question_id,title,body,tags
4378516,Game holder is always losing money in the St. Petersberg Paradox?,"The St. Petersberg Paradox is described as follows: A gambler pays an entry fee $M$ dollar to play the following game: A fair coin is tossed repeated until the first head occurs and you win $2^{n-1}$ amount of money where $n$ is the total number of tosses. And the question is what is the fair amount of $M$ ? By some simple probability knowledge we can get the ""Expected Winning Money"" is $$E(2^{n-1})=\sum\limits_{k=1}^\infty 2^{k-1}\times\frac1{2^k}=\infty$$ But Bernoulli claims that $M$ is not worth infinity because of utility. And log-utility is considered here, which intuitively means that 1000 dollars are not equally significant to a pauper and a rich man. But I am confused that if so, then $M$ will be set to some finite amount. But if we only focus on the number itself, since the ""Expected Winning Money"" is infinity, then the game holder is 100% certain to lose money to hold this game(Or to be more precise, the game holder will definitely lose money when the number of people to play this game is large enough)? Can anyone help me? Thank you!!","['expected-value', 'utility', 'probability', 'economics']"
4378529,Solve $I=\int_{0}^{1}\int_{0}^{1}\frac{xy}{\sqrt{x^2+y^2}}\arctan{\sqrt{x^2+y^2}}\arctan{\frac{y^2}{x^2}}dxdy$,"I tried to solve this integral by using polar coordinate system, but it becomes harder: $$I=\int_{0}^{1}\int_{0}^{1}\frac{xy}{\sqrt{x^2+y^2}}\arctan{\sqrt{x^2+y^2}}\arctan{\frac{y^2}{x^2}}dxdy$$ After transformation $$I=2\int_{0}^{\frac{\pi}{4}}\arctan{(\tan^2\theta)}\sin\theta \cos\theta d\theta\int_{0}^{\frac{1}{cos\theta}}r^2\arctan{(r)}dr$$ And after IBP, $$\int r^2\arctan{(r)}dr=\frac{1}{6}\left(2r^3\arctan(r)-r^2+\ln(r^2+1)\right)$$ and replace upper and lower limits, it becomes a mess.
Did I misdirect? Need some help from everyone, thank you very much.","['integration', 'multivariable-calculus', 'multiple-integral']"
4378618,Finding roots in higher order polynomials using binary search,"I could successfully implement a root finder, by recursively: finding the zeroes of the nth derivative which gives the extrema of the (n-1)th derivative which provides alternating sign values to be used to find the zeroes of the (n-1)th derivative using binary search. What is the name of this method? The wikipedia article has a paragraph on ""Newton's method (and similar derivative-based methods)"" that doesn't mention it. Can it be expanded to support multivariate polynomials? I couldn't find a way to do it by myself, and I couldn't search for it because I couldn't find the name of this method.","['multivariable-calculus', 'derivatives', 'roots']"
4378630,A couple of basic questions on contact structures,"Consider the following definition of contact structures: A contact structure on an n dimensional smooth manifold is a codimension 1 tangent distribution $\xi$ whose first curvature $\beta:\xi\times \xi\to TM/\xi$ , defined by $\beta_p:=[-,-]_p\mod \xi_p$ , is a non singular at every point. Can we automatically infer that the manifold is odd-dimensional? What is the proof of that? Can we define contact structures simply as bracket-generating codimension 1 distributions? If the distribution is weakly regular, meaning that the Lie flag consists of subbundles, it seems an equivalent definition. Am I wrong?","['contact-geometry', 'differential-topology', 'contact-topology', 'differential-geometry']"
4378695,Expressing algorithmic problem as a mathematical problem,"I'm working in an algorithmic problem, which can surely be solved without any math involved, however I'm practicing (translating problems to formal definitions). I've done some baby steps however I need some expert help. algorithmic problem: Given Two stacks a and b , stack a contains some random amounts of numbers and stack b is empty. and given exactly 11 operations , sort the
numbers with the lease amount of operations. ~ sa : swap the first 2 elements at the top of stack a . ~ sb : same as above for b . ~ ss : sa and sb at the same time. ~ pa : take the first element of b and put it at the top of a . ~ pb : same as
above for b . ~ ra : shift up all elements of stack a by 1 The first element becomes the last one. ~ rb :
same as above for b . ~ rr : ra and rb at the
same time. ~ rra : shift down all elements of stack a by
one The last element becomes the first one. ~ rrb : same as above for b . ~ rrr : rra and rrb at the
same time. one cannot compare without comparison and in the algorithmic problem its allowed because its not a movement per se, in algo problem you can compare what you're at, and you're arrive at what you're at by looping, so I thought lets just say cab : compare any or all elements in a and b any time. my baby steps: given a sequence $A$ of $n$ numbers $\left\langle a_{1}, a_{2},
   ...a_{n} \right\rangle $ and an empty sequence $B$ . produce a
permutations (reordering) of the input $\left\langle a_{1}, a_{2},
   ...a_{n} \right\rangle $ so that $\left\langle a_{1} \le  a_{2} \le
   a_{n} \right\rangle $ My main problem is how to formalize these operations ( sa , sb ...) in math entities, one idea is functions, but  I'm not that good to complete the formalization. What do you think ?","['permutations', 'sorting', 'functions', 'algorithms', 'sequences-and-series']"
4378706,Is it legal build two sets that recursively depend on each other?,"For instance, I would like to describe two sets the following way $S_1 = \{x \in N | x = 100 \lor x + 5 \in S_2\}$ $S_2 = \{x \in N | x + 5\in S_1\}$ is it legal?",['elementary-set-theory']
4378716,Matrices with a more general indexing than by integers,"A matrix is usually understood as an rectangular array of objects (for the most part numbers) arranged in rows and columns. If the objects belong to a set $\mathcal S$ , then one can write $M(m,n;\mathcal S)$ for the set of $(m \times n)$ -matrices $A$ with entries in $\mathcal S$ . Such a matrix $A$ has $m$ rows and $n$ columns. The entry of $A$ occurring in row $i$ and column $j$ is often denoted as $a_{ij}$ and one writes $A = (a_{ij})$ . In linear algebra matrices are used to represent linear maps $f : V \to W$ between finite-dimensional vector spaces $V,W$ with respect to bases $\mathfrak V  =\{v_1,\ldots, v_n\}$ of $V$ and $\mathfrak W  =\{w_1,\ldots, w_m\}$ of $W$ . We have $f(v_j) = \sum_{i=1}^m a_{ij}w_i$ with unique $a_{ij}$ and therefore $f(\sum_{j=1}^n\lambda_jv_j) =  \sum_{j=1}^n \lambda_jf(v_j) = \sum_{i,j} \lambda_j a_{ij}w_i$ . The $(m\times n)$ -matrix $(a_{ij})$ is the matrix representation of $f$ with respect to $\mathfrak V, \mathfrak W$ . For such matrix representations we work only with bases indexed by sets of the special form $I_k = \{1,\ldots,k\}$ . Here is my question: Wouldn't it be more flexible to allow arbitrary finite index sets ? That is, to consider matrix sets of the form $M(I, J; \mathcal S) = \mathcal S^{I \times J}$ with arbitrary finite sets $I, J$ . A matrix $A \in M(I, J; \mathcal S)$ is then an indexed collection $A 
  =(a_{(i,j)}) \in \mathcal S^{I \times J}$ . We can still regard it as a rectangular array of objects of $\mathcal S$ arranged in rows and columns, although these do not have integer row numbers and column numbers . Of course this concept is not a big innovation. But does is occur somewhere in the literature? This question was motivated by the answers to Chain rule for differentiation yields conflicting dimensions . The question deals with a matrix valued function $f : \mathbb R \to M(n,n;\mathbb R)$ . Clearly $M(n,n;\mathbb R)$ is isomorphic to $\mathbb R^{n^2}$ , but it does not have a canonical basis indexed by $\{1\ldots,n^2\}$ . Instead it has a natural base consisting of the matrices $(E_{ij}) \in M(n,n;\mathbb R)$ where the $E_{ij}$ have an entry $1$ in row $i$ and column $j$ , all other entries being $0$ . The index set of this natural basis is $I_{m,n} = \{1,\ldots,m\} \times \{1,\ldots,n\}$ . When considering the derivative $Df \mid_x$ at $x \in \mathbb R$ , which naturally is a linear map $\mathbb R \to M(n,n;\mathbb R)$ , the mentioned answers are wavering around by saying that we have to ""flatten matrices"" or to ""identify $M(n,n;\mathbb R)$ with $\mathbb R^{n^2}$ "" to get a matrix in $M(n^2,1;\mathbb R)$ . One can do this, but I think it is uncessary and may even cause confusion (the OP of the above question seems to mix up $M(n^2,1;\mathbb R)$ with $M(n,n;\mathbb R)$ when flattening $Df\mid_x)$ ). In my opinion it is much more transparent to say that the Jacobian of $f$ at $x$ is a matrix in $M(I_{m,n},1;\mathbb R)$ .","['matrices', 'reference-request']"
4378787,"Is there a way to integrate $ \int_{0}^{2\pi}\frac{(\cos (t/2))^2 \sqrt{1+\sin t}}{(1+\sin t+K)^{3/2}}\,d t$ by hand?","I came across the following integral $$
\int_{0}^{2\pi}\frac{(\cos (t/2))^2 \sqrt{1+\sin t}}{(1+\sin t+K)^{3/2}}\,d t\tag1
$$ where $K>0$ is a constant. I don't see a way to handle this integral, however Wolfram Mathematica gives an exact answer for it symbolic computation, so it must be a way to solve it. With the change of variable $e^{it}=z$ I get $$
\begin{align*}
(\cos (t/2))^2&=\frac1{4}(z^{1/2}+z^{-1/2})^2=\frac1{4}(z+z^{-1}+2)=\frac{(z+1)^2}{4z}\\
\sqrt{1+\sin t}&=\left(1+\frac{z-z^{-1}}{2i}\right)^{1/2}=\frac {z+i}{(2iz)^{1/2}}\\
(1+K+\sin t)^{3/2}&=\left(K+\frac{(z+i)^2}{2iz}\right)^{3/2}\\
dz&=d(e^{it})=ie^{it}dt\implies dt=-iz^{-1}dz
\end{align*}\tag2
$$ Therefore $$
\mathrm{(1)}=\frac1{2}\oint (z+1)^2(z+i)z^{-1}(2izK+(z+i)^2)^{-3/2}\,d z\tag3 
$$ However $(2izK+(z+i)^2)^{-3/2}$ is not analytic in a neighborhood of zero, so I cannot use residue theorem here. Someone have some idea to try to tackle this?","['integration', 'calculus', 'contour-integration', 'definite-integrals']"
4378799,Strong vs Weak solution to one-dimensional elliptic PDE,"Consider the elliptic PDE \begin{align}
&-\frac{\mathrm{d}}{\mathrm{d} x} \left(a(x) \frac{\mathrm{d}}{\mathrm{d} x}u(x)\right) = 1, \qquad 0 < x < 1,\\
&u(0) = u(1) = 0.
\end{align} Here $a \in L^\infty(0,1) \cap C^0(0,1)^C$ is defined as $$
a(x) = a_1, \text{ if } x \leq 1/2, \quad a(x) = a_2, \text{ if } x > 1/2,
$$ where $a_1$ and $a_2$ are positive real scalars. I am having troubles with the difference between a strong and a weak solution of this problem. Since $a$ is not continuous, I would expect $u\in H_0^1(0,1) \cap H^2(0,1)^C$ , so in particular (since we are in dimension one) I would expect $u \in C^0(0,1) \cap C^1(0,1)^C$ . Actually somewhere in between, e.g. Hölder continuous. I tried to solve the equation ""by hand"". In particular, I define $u$ piecewise as $$
u(x) = u_1(x), \text{ if } x \leq 1/2, \quad u(x) = u_2(x), \text{ if } x > 1/2,
$$ Then, one can impose that $u_1$ and $u_2$ solve the equation in the strong sense on each side of $x=1/2$ and obtain $$
u_1(x) = -\frac{1}{2a_1} x^2 + C_1x + C_2, \quad u_2(x) = -\frac{1}{2a_2} x^2 + D_1x+D_2.
$$ Now we can impose $u_1(0) = 0$ to obtain $C_2 = 0$ , and $u_2(1) = 0$ to obtain $$
D_1 + D_2 = \frac{1}{2a_2}.
$$ We expect the equation to be continuous, so $u_1(1/2) = u_2(1/2)$ , which gives the condition $$
\frac{C_1}{2} - \frac{D_1}{2} - D_2 = \frac{1}{8a_1} - \frac{1}{8a_2}.
$$ We now have three unknowns ( $C_1$ , $D_1$ , and $D_2$ ), and two conditions. We could impose moreover that $u_1'(1/2) = u_2'(1/2)$ so that the solution $u \in C^1(0,1)$ . This yields the condition $$
C_1 - D_1 = \frac{1}{2a_1} - \frac{1}{2a_2}.
$$ The three linear equations above are solvable and thus define a solution $u$ on the whole domain which is $C^1$ . I tried to compare the solution (obtained by solving the linear equation and fixing $a_1 = 0.5$ , and $a_2 = 2$ ) to a FEM solution on $1000$ elements. The FEM solution converges to the weak solution $u \in H_0^1$ such that $$
\int_0^1 a(x) u'(x) v'(x) \, \mathrm{d}x = \int_0^1 v(x) \, \mathrm{d}x,
$$ for all $v \in H_0^1(0, 1)$ . The result is in the picture below. The FEM solution (in blue) behaves a bit more ""similarly"" to what I expected. In particular, there is a discontinuity in the derivative at $x = 1/2$ and the solution is ""only"" Hölder continuous. In particular, the FEM solution is different than the strong solution (in red). I think there's something wrong with my reasoning that yields the strong solution, but I cannot see why or where. In particular, since the weak solution exists and is unique for this equation, it should be equal to the strong solution when it exists. I somehow trust more the FEM solver in this case.","['finite-element-method', 'analysis', 'partial-differential-equations']"
4378834,"De Morgan's saying: ""I was x years old in the year x^2""","Augustus de Morgan, when asked about his age, remarked: I was $x$ years old in the year $x^2$ . He died in $1871$ . Examining possible squares, we find $41^2 = 1681$ , $42^2 = 1764$ , $43^2 = 1849$ , $44^2 = 1936$ . Clearly, the only one which makes sense in this problem would be $43^2 = 1849$ . So de Morgan was $43$ in $1849$ . Therefore, he was born in $1806$ . Is there a systematic way to answer such a question without trial and error?",['algebra-precalculus']
4378870,What is the general solution of $\frac{df(x)}{dx} = f(x-a)$? [duplicate],"This question already has answers here : How to solve differential equations of the form $f'(x) = f(x + a)$ (2 answers) Closed 2 years ago . Let $a \in \mathbb{R}$ be a constant. What is the general solution of the following delay differential equation (DDE)? $$\frac{df(x)}{dx} = f(x-a)$$ For example, for $a = - \frac{\pi}{2}$ , $$\begin{split}
\frac{d(\sin x)}{dx} &= \cos x \\
&= \sin\left(x + \frac{\pi}{2}\right)
\end{split}$$","['delay-differential-equations', 'ordinary-differential-equations']"
4378888,"If $X$ is an infinite set, then $|X|=|F(X)|$, where $F(X)$ is the free group on $X$","I am struggling to prove that for an infinite set $|X|=|F(X)|$ , where $F(X)$ is the free group on $X$ . I can see the obvious injection $X \to F(X)$ but struggling to see the converse.
Any help would be much appreciated!","['elementary-set-theory', 'group-theory', 'free-groups', 'infinite-groups']"
4378897,A manifold admits a non-vanishing top form iff it has an oriented atlas. Do we need connectedness here?,"I appreciate anyone who can provide help. This is a theorem in sec. 20.4 of Loring Tu's book ""An introduction to manifolds"" The theorem is the following, A manifold admits a non-vanishing top form iff it has an oriented atlas. I manage to show one direction. The part that confuses me is that if we assume the top form, we want to show there is an oriented atlas. Say we have a top form $\omega$ , then pick a chart $(U,x)$ , we know that $$\omega=fdx^1\wedge...\wedge dx^n$$ on the coordinate open set $U$ . But then the book say either $f>0$ or $f<0$ . I cannot see why this is the case without connectedness assumed.","['multivariable-calculus', 'differential-topology', 'differential-geometry']"
4378911,Why do these two sums define functions that are asymptotically so close in value?,"Let $$f_n(\epsilon)=(2n+1)e^{-n(n+1)\epsilon},\qquad Z_\text{odd}(\epsilon)=\sum_{p\ge0}f_{2p+1}(\epsilon),\qquad Z_\text{even}(\epsilon)=\sum_{p\ge0}f_{2p}(\epsilon).$$ (These functions are relevant to describing rotation of diatomic molecules in statistical physics.) When $\epsilon\searrow0$ , these functions are extremly close in value; numerically: $$Z_\text{even}(0.1)-Z_\text{odd}(0.1)\simeq 3\,10^{-9},\qquad
Z_\text{even}(0.05)-Z_\text{odd}(0.05)\simeq 2\,10^{-19}.$$ Is there a reason for this? What is an asymptotic equivalent of the difference? I tried to obtain a small $\epsilon$ expansion using the Euler-MacLaurin expansion and Mathematica, and I found that they both have the same expansion. Notice that Euler-MacLaurin involves derivatives of $f$ at 0 for $Z_\text{odd}$ and derivatives of $f$ at 1 for $Z_\text{even}$ , so it is not obvious how this should lead to the same expansion. [Paragraph added in an edit] In fact, to compare the expansions, it is easier to write $Z_\text{even}(\epsilon)-Z_\text{odd}(\epsilon)=2Z_\text{even}(\epsilon)-Z_\text{total}(\epsilon)$ , where $Z_\text{total}(\epsilon)=\sum_{n\ge0} f_n(\epsilon)$ . Then, applying Euler-MacLaurin to order $m$ to $Z_\text{even}$ and $Z_\text{total}$ , one obtains $$Z_\text{even}(\epsilon)-Z_\text{odd}(\epsilon)=\frac{f_0(\epsilon)}2+\sum_{p=0}^m \frac{b_{2p}(2^{2p}-1)}{(2p)!}f^{(2p-1)}(\epsilon)+R_m(\epsilon).$$ The first terms in this summation are $\frac12$ , $\frac14(-2+\epsilon)$ , $\frac1{48}(-12\epsilon+12\epsilon^2-\epsilon^3)$ , $\frac1{480}(-120\epsilon^2+180\epsilon^3-30\epsilon^4+\epsilon^5)$ , $\frac{17}{80\,640}(-1680 \epsilon ^3+3360 \epsilon ^4-840 \epsilon ^5+56 \epsilon ^6-\epsilon ^7)$ . Notice how each new term cancels the lowest power of $\epsilon$ remaining. Finally, the result is the remainder term $R_m(\epsilon)$ plus a polynomial of $\epsilon$ with powers going from $\epsilon^{m+1}$ to $\epsilon^{2m+1}$ . I find it quite fascinating, but maybe the Euler-MacLaurin approach is not the good one here. [End of paragraph added in the edit] A last remark: this result is not universal. If I pick the not-so-different function $\tilde f_n(\epsilon)=2n e^{-n^2\epsilon}$ , then the corresponding $\tilde Z_\text{odd}$ and $\tilde Z_\text{even}$ differ by a quantity of order $\epsilon$ . So, what is so special about my choice of $f_n(\epsilon)$ ? Why are my two $Z$ functions so close? How can I compute the asymptotic equivalent of $Z_\text{even}(\epsilon)-Z_\text{odd}(\epsilon)$ as $\epsilon\searrow0$ ?","['euler-maclaurin', 'asymptotics', 'sequences-and-series']"
4378919,Algorithm behind Sin Function [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question The relationship between the hypothenuse and the opposite cathetus of an angle can be described by $$\sin\theta=\frac{a}{h}$$ , where $a$ is the opposite cathetus, and $h$ is the hypothenuse. But I am curious about the sine  function, it is not a variable and my math teacher defines it as a function. So if it is a function, could anyone kindly tell me how this function was found and what variable terms it is composed as? Thanks",['trigonometry']
4378977,"Proving that the tangent space, considered as a set of equivalence classes of contours, is a vector space.","[Note: Before you simply paste a link, I've already read several other similarly-phrased posts on this site, none of which quite answer my question or phrase it in a way I understand.] Wikipedia defines the tangent space $T_xM$ of an $n$ -manifold $M$ at $x\in M$ as follows.  Let $\varphi:U\rightarrow \mathbb{R}^n$ a chart, where $U\ni x$ is an open subset of $M$ , and define $$X\varphi_x(\gamma):=\frac{d}{dt}\left[\varphi\circ \gamma\right]\Big\rvert_{t=0}$$ over the set $C_{U,x}$ of contours $\gamma:I_{\gamma}\rightarrow U$ , $I_{\gamma}$ a sufficiently small interval around 0, such that $\gamma(0) = x$ and $\varphi\circ\gamma$ is smooth.  Also, wlog, assume that $\varphi(x) = 0$ for simplicity. Then $T_xM$ is the set $C_{U,x}$ modulo $\sim$ , where $$\gamma_1\sim\gamma_2\quad\Leftrightarrow\quad X\varphi_x(\gamma_1) = X\varphi_x(\gamma_2).$$ (It is straightforward to show that this definition does not depend on the choice of $\varphi$ or $U$ .) Now, Wikipedia says that $X\varphi_x(\gamma)$ induces a bijective map from $T_xM$ to $\mathbb{R}^n$ , which gives a vector space structure to $T_xM$ .  I understand why the map is well-defined and injective, but I do not understand how to prove surjectivity without using the fact that $T_xM$ is a vector space, a fact that (so Wikipedia suggests) should follow from surjectivity of $X\varphi_x$ and not the other way round.  Here's my approach so far: Define $\gamma_k(t):= \varphi^{-1}(t\cdot e_k)$ , where $e_k$ is the unit vector in $\mathbb{R}^n$ with a $1$ as its $k^{th}$ component.  Then $$X\varphi_x(\gamma_k[t]) = e_k.$$ If $T_xM$ is defined as the free vector space with base $\{\gamma_1, \dots, \gamma_n\}$ and $X\varphi_x:T_xM\rightarrow\mathbb{R}^n$ is assumed to be a linear map, then the conclusion that $X\varphi_x$ is surjective follows from the universal property of free modules.  But as I write above, this seems like putting the cart before the horse, proving that the map is surjective from the fact that $T_xM$ is a vector space and not v.v. ; and even if this approach is correct, it's not clear that we are allowed to assume that $X\varphi_x$ is linear as a function of $T_xM$ (unless we extend it to be the unique homomorphism guaranteed by the universal property ). indeed, why does the map $X\varphi_x$ matter at all if we can show that $T_xM$ is a vector space another way? What am I missing?  Can I prove that $T_xM$ is a vector space using only the tangent vector definition ( i.e. , without also using the definition in terms of derivations)?","['differential-topology', 'vector-spaces', 'tangent-spaces', 'differential-geometry']"
4379190,Is conditional probability axiomatic? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Is the conditional probability $P(A|B) = P(A \cap B)/P(B), P(B) \ne 0$ an axiom in the Kolmogorov scheme or is it derived from the other axioms?","['conditional-probability', 'math-history', 'probability-theory', 'axioms']"
4379229,Symmetry of Conditional Distribution,"Suppose that $X_1, X_2, ..., X_n$ are independently and identically distributed random variables. What is $E(X_1 \mid X_1 + ... + X_n=t)$ ? The solution first tells me that By Symmetry, $$E(X_i \mid X_1 + ... + X_n=t) = E(X_j \mid X_1 + ... + X_n=t)$$ for all $i\neq j$ . Hence, $$E(X_1 \mid X_1 + ... + X_n=t) = \frac{1}{n}E(X_i \mid X_1 + ... + X_n)=t$$ May I know what the question means by symmetry here? My understanding of symmetry is when random variables are generally normally distributed on either side of the mean. Why would the condition of this distribution be symmetrical? Or did I miss out on some other concept? Any help would be greatly appreciated! Thank you!","['expected-value', 'statistics', 'conditional-expectation', 'probability']"
4379239,Using matrix to analyze an ODE system $\begin{cases}\dot a_k(t)=2(b_k^2-b_{k-1}^2)\\\dot b_k(t)=b_k(a_{k+1}-a_{k})\end{cases}$ with $b_0(t)=b_n(t)=0$.,"Let $a_k(t), b_k (t) \in \mathbb{R} \ (k = 1, 2, \dots, n)$ satisfy the following differential equations: $$\begin{aligned} \frac{d}{dt}a_k(t) &= 2 \left( b_k^2 - b_{k-1}^2 \right) \\ \frac{d}{dt} b_k(t) &= b_k \left( a_{k+1} - a_{k} \right)\end{aligned} \qquad k = 1, 2, \dots , n $$ where $b_0(t) = b_n(t) = 0$ . Consider the $n \times n$ tri-diagonal matrix $$L(a,b)=\begin{bmatrix}
    a_1 & b_1 & 0 & \dots  & 0 &0\\
    b_1 & a_2 & b_2 & \dots  & 0 &0\\
    \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\
0 & 0 & 0 & \dots  & a_{n-1} & b_{n-1}\\
    0 & 0 & 0 & \dots  & b_{n-1} & a_n
\end{bmatrix}$$ show that: The eigenvalues of $L(t) = L(a(t), b(t))$ are independent of $t$ . $\lim_{t \to \infty}b_k(t) = 0$ for $k=1,2,\dots,n-1$ . For P1, Let $L(a,b)=\begin{bmatrix}
    0 & b_1 & 0 & \dots  & 0 &0\\
    -b_1 & 0 & b_2 & \dots  & 0 &0\\
    \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\
0 & 0 & 0 & \dots  & 0 & b_{n-1}\\
    0 & 0 & 0 & \dots  & -b_{n-1} & 0
\end{bmatrix}$ , then all above becomes $\frac{dL}{dt}=PL-LP$ . Since $L$ exists and is unique, $P$ also exists and is unique. Hence there exists only one $U$ such that $\frac{d}{dt}U(t,s)=P(t)U(t,s),U(s,s)=I$ . Since $U(t,s)L(s)U(t,s)^{-1}$ is also the solution to $\frac{dL}{dt}=PL-LP$ , we must have $L(t)=U(t,s)L(s)U(t,s)^{-1}$ and P1 is solved. However now I'm stucking on P2, which is equivalent to prove $\lim_{t\rightarrow \infty}P=0$ or $L$ tends to be diagonal, in which the solution of ODE tends to be constant, and I don't know how to handle with analyzing matrix ODE. Any help, hint or solution on Problem 2 would be appreciated!","['ordinary-differential-equations', 'matrices', 'limits', 'derivatives', 'tridiagonal-matrices']"
4379345,Instability of a parameter varying system whose parameters belong to a compact set,"Suppose, there is a system $$\dot{x}=f(t, \gamma_p(t), x)$$ with $x\in\mathbb{R}^2$ .
For my specific case, parameter vector $\gamma_p$ is a scalar and known monotonic function with a compact image set (for example $\gamma_p(t) = e^{-t}$ , thus $\gamma_p \in [1,0)$ ). I would like to show instability of this system.
Well known techniques (specifically, averaging) are applicable to show system's instability for all frozen parameter values, that is a simplified system $\tilde{f}$ with $$\dot{x}=f^\star(t, x) = f(t, \gamma_p(t^\star), x) \quad \forall \; t^\star \in [0, \infty).$$ Can instability of all frozen parameter cases let me conclude anything about the instability of the original system? Specifics of my problem $f$ takes the form $$ f(t,\gamma_p(t),x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t))}{\gamma_p(t)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t)+\alpha x_2\right)$$ where $\varepsilon > 0$ is a small parameter of the system, $\alpha$ is a positive coefficient and $L$ is a $2\pi$ periodic function over only the first argument $t$ (thus not necessarily periodic when $\gamma_p$ is changing) with $$\mathrm{sign}\left[\int_{0}^{2\pi}L(t, x_2, \underbrace{(x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2}_{y(t^\star)})\mathrm{d}t\right] = \mathrm{sign}\left[y(t^\star)\right]\quad \forall \; t^\star \in [0,\infty).$$ Substituting $\gamma_p(t^\star)$ gives the frozen parameter system $$ f^\star(t,x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix} L\left(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2\right)$$ With $\gamma_p(t^\star)$ now fixed, dynamics become periodic with period $2\pi$ which allows me to apply averaging. Specifically, we can determine the stability/instability through the averaged system $$ \dot{x}\approx\tilde{f}^\star(x) = \varepsilon \begin{bmatrix}\tfrac{\alpha(1-\gamma_p(t^\star))}{\gamma_p(t^\star)} \\ 1\end{bmatrix}\int_{0}^{2\pi}L(t, x_2, (x_1-\alpha x_2)\gamma_p(t^\star)+\alpha x_2)\mathrm{d}t$$ Using further details of the system, I can show the system grows unboundedly in the direction $[\gamma_p(t^\star), (1-\gamma_p(t^\star)]$ in the space spanned by $(x_1,x_2)$ .","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'stability-theory', 'nonlinear-dynamics']"
4379359,Why is $ab$ likely to have more divisors than $(a-b)(a+b)$?,"Consider the two numbers $ab$ and $(a-b)(a+b), \gcd(a,b) = 1, 1 \le b < a$ . On an average, which of these two numbers has more distinct prime factors? All the prime factors of $a$ and $b$ divide $ab$ and similarly all the prime factors of $a-b$ and $a+b$ divide $(a-b)(a+b)$ . So one number does not seem to have an obvious advantage over the other. However if we look at the data than we see that $ab$ dominates. Let $f(x)$ be the average number of distinct prime factors in all such $ab, a \le x$ and $g(x)$ be the average number of distinct prime factors in all such $(a-b)(a+b), a \le x$ . Update : Experimental data for the first $6.1 \times 10^{9}$ pairs of $(a,b)$ shows that $f(x) - g(x) \sim 0.30318$ . Instead of distinct prime factors, if we count the number of divisors than $f(x) - g(x) \sim 0.848$ . Question : Why is $ab$ likely to have more distinct prime factors or divisors than $(a-b)(a+b)$ and what is the limiting value of $f(x) - g(x)$ ?","['divisibility', 'elementary-number-theory', 'number-theory', 'limits', 'prime-numbers']"
4379364,Evaluate $\int_{0}^{1}\int_{0}^{1}\int_{0}^{1}\sqrt{x^2+y^2+z^2}\arctan{\sqrt{x^2+y^2+z^2}}dxdydz$,"I am trying to evaluate this integral: $$\displaystyle\int_{0}^{1}\int_{0}^{1}\int_{0}^{1}\sqrt{x^2+y^2+z^2}\arctan{\sqrt{x^2+y^2+z^2}}dxdydz$$ I coverted the region by using polar coordinate system for xy-plane and it became: $$\displaystyle\int_{0}^{1}\int_{0}^{\frac{\pi}{4}}\int_{0}^{\frac{1}{\cos\theta}}r\sqrt{r^2+z^2}\arctan{\sqrt{r^2+z^2}}drd\theta dz+\displaystyle\int_{0}^{1}\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\int_{0}^{\frac{1}{\sin\theta}}r\sqrt{r^2+z^2}\arctan{\sqrt{r^2+z^2}}drd\theta dz$$ After IBP $$\int r\sqrt{r^2+z^2}\arctan{\sqrt{r^2+z^2}}dr=\frac{1}{3}(r^2+z^2)^{\frac{3}{2}}\arctan{\sqrt{r^2+z^2}}-\frac{1}{6}(r^2+z^2)+\frac{1}{6}\ln(r^2+z^2+1)+C $$ and replace upper and lower limits, it is scary-looking.
My experience in solving this tyle of integral is still weak, I think there is a way to get rid of one variable from the beginning integral, but i can't figure it out. Hope everyone can help me, thanks.","['integration', 'multivariable-calculus', 'calculus']"
4379373,Proving that $\frac{1}{n^2} - \frac{1}{(n+1)^2} \approx \frac{2}{n^3}$ when $n$ is very large,"This is an example from Mathematical Methods in the Physical Sciences, 3e, by Mary L. Boas.
My question is, \begin{equation}
\frac{1}{n^2} - \frac{1}{(n+1)^2} \approx \frac{2}{n^3}
\end{equation} can also be written as, \begin{equation}\frac{1}{(-n)^2} - \frac{1}{(n+1)^2} \approx \frac{2}{n^3} \end{equation} so that $\Delta n = dn = -n-(n+1) = -2n-1$ . For $f(n) = 1/n^2$ , $f'(n) = -2/n^3$ , and \begin{equation}
df = d(\frac{1}{n^2}) = f'(n)dn
\end{equation} \begin{equation}
df = \frac{(2)(2n+1)}{n^3}
\end{equation} Now, for very large $n$ , $2n+1 \approx 2n$ . Thus, \begin{equation}
df = \frac{4}{n^2}
\end{equation} But, $4/n^2$ is not approximately equal to $2/n^3$ (required ans.) even if $n$ is very large. So, please point out my mistake(s). Thanks in advance (;","['partial-derivative', 'derivatives', 'partial-differential-equations']"
4379428,How to solve $\ddot{\vec{u}} = \vec{u} \times \hat{k}$,"How do I solve?: $$\ddot{\vec{u}} = \vec{u} \times \hat{k}$$ I have tried solving a simpler version of this, $\dot{\vec{u}} = \vec{u} \times \hat{k}$ . This one was easy: the head of the vector rotates around the $z$ -axis with constant angular speed. However, I have no idea on the second-order one. It doesn't seem to have any relation to the simpler one.. or does it?..","['cross-product', 'vectors', 'ordinary-differential-equations']"
4379467,"Clarifying the rule: ""R-squared is invalid for nonlinear regression""","I'm a novice trying to determine if the use of R-squared is valid for developing an exponential regression (of the form $y=a+b\,e^{cx}$ ). I've come across several articles online that say R-squared is invalid for nonlinear regression . That claim makes sense to me. But I've also noticed that lots of people still use R-squared for  nonlinear regression. Several of my statistics books suggest that R-squared can be used for all regression analysis, including nonlinear regression. Most statistics software produces a R-squared value for nonlinear regression. People that are far more educated than me continue to use R-squared for nonlinear regression. So, I can't help but think that something doesn't add up here. Why do so many people still do it? Are they really all wrong, and I'm right? My uneducated thoughts are: Sure, it's a common misconception that R-squared is valid for all types of regression, including nonlinear regression. But also, I'm wondering if the wording of ""R-squared is invalid for nonlinear regression"" might be an oversimplification . Does the rule need clarification? For example: R-squared can be used for some nonlinear regressions, such exponential regression, if the equation is first flattened by taking the natural logarithm of both sides. [removed - misleading] There are models that appear to be nonlinear, but in this context, they're actually considered to be linear. Examples: parabolic and polynomial. So using R-squared for them is valid. As mentioned, I'm a novice, I'm not a mathematician. Are clarifications 1-3 above correct? Or have I misunderstood? Note: I've intentionally posted this question on Math Stack Exchange, instead of on Cross Validated, because I find I get better answers here.",['statistics']
4379513,Find rotation angle (line tangential to ellipse),"I want to find the rotation angle $\theta$ so that a line with given slope $m$ and y-intersect $t$ is tangential to an ellipse with a given minor axis $a$ , major axis $b$ and center $h,k$ . The rotation axis is the global z-axis. Given: The blue ellipse with $a$ , $b$ in a given reference state (center: $h=r+a, k=0$ ) The green line with $m$ , $t$ . Todo: Rotate the green line around the origin until it is tangential to the blue ellipse (result is grey line) OR Rotate the blue ellipse until it is tangential to the green line (result is grey ellipse) What I know: (1) Equation of a straight line $$y=m*x+t$$ (2) Rotation around z-axis $$x'=x*\cos(\theta)-y*\sin(\theta)$$ $$y'=x*\sin(\theta)+y*\cos(\theta)$$ (3) Equation of a ellipse in the given reference state (blue ellipse) $$\frac{(x-h)^2}{a^2}+\frac{(y(x)-k)^2}{b^2}-1$$ I can find the implicit derivative of the ellipse $$\frac{d}{dx}y(x)=-\frac{b^2*(2*h-2*x)}{2*a^2*(k-y(x))}$$ and I know that this must be equal to the slope $m$ of the line. But here comes the problem. Not with the slope $m$ of the green line but with the slope $m$ of the rotated line (which is unknown, since I don't know the rotation angle.) Can anybody help me to find what I'm missing? I think somehow I have to use equation 1 and 2 but I don't see how to solve for the angle $\theta$ . I would appreciate help very much.","['trigonometry', 'calculus', 'conic-sections', 'geometry']"
4379575,"Given $\varphi$ is golden ratio, how do I prove this $\sum \limits_{j=1}^{\infty}\frac{(1-\varphi)^j}{j^2}\cos{\frac{3j\pi}{5}}=\frac{\pi^2}{100}$?","Given $  \varphi$ is golden ratio, how do I prove this: $ \displaystyle \tag*{}\sum \limits_{j=1}^{\infty}\frac{(1-\varphi)^j}{j^2}\cos{\frac{3j\pi}{5}}=\frac{\pi^2}{100}$ My approach: We can reduce the sum into simpler parts by using the property of golden ratio, we have: $\displaystyle \tag*{} \phi^2 - \phi - 1 = 0  \Leftrightarrow 1- \phi = - \dfrac{1}{\phi}$ And I also found the values of $\cos \dfrac{3j\pi}{5}$ in terms of $\varphi$ : $\displaystyle \tag*{} \begin{align} \cos \dfrac{3\pi}{5} &= \dfrac{-1}{2 \varphi} \\\\ \cos \dfrac{6\pi}{5} &= \dfrac{-\varphi}{2} \\\\ \cos \dfrac{9\pi}{5} &= \dfrac{\varphi}{2} \\\\ \cos \dfrac{12\pi}{5} &= \dfrac{1}{2 \varphi} \\\\ \cos \dfrac{15\pi}{5} &= {-1} \end{align}$ And this repeats, periodically with alternate opposite signs. I don't know how to connect these information I found to prove the question. Maybe my approach is wrong. Any help would be appreciated. Thanks.","['trigonometry', 'irrational-numbers', 'geometry', 'summation']"
4379583,Is there any identity for $\sum_{k=1}^{n}\tan\left(\theta+\frac{k\pi}{\color{red} {2n+1}}\right)$?,"Is there any identity for $\sum_{k=1}^{n}\tan\left(\theta+\frac{k\pi}{\color{red} {2n+1}}\right)$ or $\sum_{k=1}^{n}\tan\left(\frac{k\pi}{\color{red} {2n+1}}\right)$ ? I thought maybe wrongly that given: $\sum_{k=1}^{n}\cot\left(\frac{k\pi}{\color{red} {2n+1}}\right)=\sum_{k=1}^{n}-\tan\left(\frac{\pi}{ {2}}+\frac{k\pi}{\color{red} {2n+1}}\right)$ and since $n \cot(nx)$ is the logarithmic derivative of $\sin(nx)$ and $\cot\left(x+\frac{\pi k}{n}\right)$ is the logarithmic derivative of $\sin\left(x+\frac{\pi k}{n}\right)$ , I tried manipulating the identity: $2^n \prod_{k=1}^n \sin\left(\frac{k\pi}{2n+1}\right)=\sqrt{2n+1}$ but I kept getting stuck. The variation $\sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n}\right)=−n\cot\left(\frac{n\pi}{2}+n\theta\right)$ is seen to work quite nicely.","['summation', 'complex-analysis', 'calculus', 'sequences-and-series', 'trigonometry']"
4379643,Any derivation map is a linear combination of directional derivatives,"Fix a point $\mathbf p\in \mathbb R^n$ . Given $\mathbf v\in \mathbb R^n$ , define the directional derivative $\nabla_{\mathbf v}$ of a function $f$ at the point $\mathbf p$ as $$\nabla_{\mathbf v}f(\mathbf p)=\lim_{t\to 0}\frac{f(\mathbf p+t\mathbf v)-f(\mathbf p)}t$$ Note that this is a map $\nabla_{\mathbf v}:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n)$ where $C^\infty(\mathbb R^n)$ is the set of all real valued smooth functions on $\mathbb R^n$ . Now, let us have a map $D:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n)$ that satisfies the following properties- $D(f+g)(\mathbf p)=D(f)(\mathbf p)+D(g)(\mathbf p)$ $D(cf)(\mathbf p)=cD(f)(\mathbf p)$ $D(fg)(\mathbf p)=fD(g)(\mathbf p)+gD(f)(\mathbf p)$ Such a map is called a derivation at $\mathbf p$ . Prove that any such derivation maps can be written as a linear combination of $\{\nabla_{\mathbf e_i}\}_{i=1}^n$ . I have proved that the directional derivative map $\nabla_{\mathbf v}$ satisfies properties (1),(2) and (3). Using that, we can also very easily show that any linear combination of $\{\nabla_{\mathbf e_i}\}_{i=1}^n$ also satisfies properties (1),(2) and (3). As a part of a previous exercise (which may be of use in this part), I have proved $$\nabla_{\mathbf {v}+\mathbf{w}}=\nabla_{\mathbf v}\\\nabla_{c\mathbf v}=c\nabla_{\mathbf v}$$ But, I have no idea how to show that any derivation is a linear combination of directional derivatives.","['geometry', 'multivariable-calculus', 'calculus', 'derivatives', 'differential-geometry']"
4379682,Compute $Var[\frac{1}{n}\sum_{i=1}^n (X_i-E[X])^2]$,"Let $X_1,\ldots, X_n$ be positive random variables in the range $[a, b]$ and $\sum_i^n X_i = 1$ . I want to compute \begin{eqnarray}
Var[\frac{1}{n}\sum_{i=1}^n (X_i-E[X])^2] 
&=& \frac{1}{n^2}Var[\sum_{i=1}^n (X_i-E[X])^2]\\
&=& \frac{1}{n^2}\sum_{i=1}^n Var[X_i^2] - 4Var[X_i E[X]] + Var[E[X]^2]\\
\end{eqnarray} Here I got stuck. Are there any approximations I can use to go further or to at least estimate an upper bound for the variance?","['statistical-inference', 'statistics', 'descriptive-statistics', 'probability-theory', 'probability']"
4379737,Continuous $f:\mathbb{R}\rightarrow\mathbb{R}^n$ with dense image is surjective?,"Let $f:\mathbb{R}\rightarrow\mathbb{R}^n$ be continuous with dense image. Must $f$ be surjective? Intuitively it seems true, I can't picture a curve in $\mathbb{R}^2$ satisfying this without covering everything. In the case $n=1$ , I can prove it quite easily using intermediate value theorem.","['calculus', 'general-topology', 'analysis', 'real-analysis']"
4379750,Help needed solving logistic differential equation with initial conditions [duplicate],"This question already has answers here : How do you solve the Initial value probelm $dp/dt = 10p(1-p),    p(0)=0.1$? (3 answers) Logistic equation involving population (1 answer) Closed 2 years ago . Given information: Solve $$\frac{dP}{dt} = P(10 - 2P)$$ with initial conditions of $P(0) = 1$ . I have separated the variables, integrated, and simplified so I am now at $\ln(p) - \ln(p-5) = 10t + c$ I need help determining firstly, if that´s correct thus far, and secondly, where to go from here as everything else I have tried has not worked.",['ordinary-differential-equations']
4379754,Differentiable But Not Continuous Example,"In the context of maps between Banach spaces, a map $f:E\to F$ is differentiable at $x\in E $ if it is continuous at $ x $ and there exists a linear map $ T:E\to F $ such that $$\lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.$$ The continuity assumption here then implies that the linear map $ T $ is bounded. Also, if one assumes in the definition that $ T $ is a bounded linear map, then $ f $ will be continuous at $x$ . I was wondering if someone had in mind an example of Banach spaces $ E, F $ , a map $f:E\to F $ , which isn't continuous at $ x\in E $ , and an unbounded linear map $ T:E\to F $ such that $$\lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.$$ Such an example would motivate the necessity of including either $ f$ continuous at $ x $ or $ T $ bounded in the definition of differentiability.","['banach-spaces', 'continuity', 'definition', 'functional-analysis', 'derivatives']"
4379800,Visual and conceptual intuition for diffeomorphisms,"I have recently learned the notion of diffeomorphism in the context of defining regular surfaces as those that are locally diffeomorphic to a plane. I have read the answers on this great question , but for improving upon my intuition for diffeomorphisms and what kind of connections they represent, I came to ask more on the subject. Of course, I feel my discussion is different from this linked question. Is there a way for getting a feel for diffeomorphic structures? Even visually, for surfaces embedded in $\mathbb{R}^3$ , can I look at two surfaces and see if they diffeomorphic? Because of the motivation for the definition of regular surfaces, in the case of seeing if, for example, a plane and a conic surface are diffeomorphic, I can say that at the tip of the cone for example it shouldn't be, because it's like a ""sharp turn"". Likewise for a self intersecting surface. But that's only when comparing general surfaces to planes. Is there a kind of, ""trademark"" local property that visually or even geometrically only diffeomorphic structures have?","['diffeomorphism', 'differential-geometry']"
4379840,Solve double integral $\iint_E\frac{1 + x}{1 + 2x^2 + 3y^2} dA$ ellipstic area $E$,"I am trying to solve the integral $$\iint_E\frac{1 + x}{1 + 2x^2 + 3y^2} dA$$ where E is the elliptical area given by $$2x^2 + 3y^2\leq 6$$ I have tried substitution with $(x, y)\mapsto(\sqrt{3} r\cos\theta, \sqrt{2}r\sin\theta)$ , where the new ranges are $r\in [0, 1]$ and $\theta\in [0, 2\pi]$ . Then I substitute that into the integral and solve it with the new limits. Is this the right way to go? I get an answer of $\pi\ln 7$ and that seems a bit weird but I am pretty unsure. Can someone guide me in the right direction in this problem? Thanks!","['integration', 'multivariable-calculus', 'calculus', 'elliptic-equations']"
4379844,"How to show $f:\mathbb{Z}^{+} \rightarrow \mathbb{Z}^{+}, f(n) = n!$ is one-to-one?","I've given this a good effort but I'm pretty stuck. How to show $f:\mathbb{Z}^{+} \rightarrow \mathbb{Z}^{+},$ $$f(n) = n!$$ is one-to-one? I'm quite sure the function is one-to-one as $0$ is not an element of the domain, so $f(0)=f(1)$ is not a concern.
However doing something like setting $f(m)=f(v)$ where $m,v$ are arbitrary elements of the domain doesn't really work, since $n!$ doesn't have an inverse. I was able to get something working algebraically by also doing $f(m+1)=f(v+1)$ , but I'm quite sure it is circular to do something like that. Any nudge in the right direction would be greatly appreciated.","['functions', 'discrete-mathematics']"
4379858,Connectedness in logic?,"Topological compactness has a logical interpretation when we consider the theorem of compactness which says that if a set of sentences has a model, then every finite subset of it also has a model, and vice-versa. Is there some similar result when we consider connectedness?","['general-topology', 'logic']"
4379871,How does this sequence of expressions continue?,"I have the following sequence of expressions (question marks denote unknown numbers): $1^k x$ $2^k x² − (1 × 1^k +  1 × 2^k) x$ $3^k x³ − (2 × 2^k +  3 × 3^k) x² + ( 2 × 1^k +  2 × 2^k +  2 × 3^k) x$ $4^k x⁴ − (3 × 3^k +  6 × 4^k) x³ + ( 6 × 2^k +  9 × 3^k + 11 × 4^k) x² − (6 × 1^k + 6 × 2^k + 6 × 3^k + 6 × 4^k) x$ $5^k x⁵ − (4 × 4^k + 10 × 5^k) x⁴ + (12 × 3^k + 24 × 4^k + 35 × 5^k) x³ − (24 × 2^k + 36 × 3^k + 44 × 4^k + 50 × 5^k) x² + (24 × 1^k + 24 × 2^k + 24 × 3^k + 24 × 4^k + 24 × 5^k) x$ $6^k x⁶ − (5 × 5^k + 15 × 6^k) x⁵ + (20 × 4^k + 50 × 5^k + 85 × 6^k) x⁴ − (60 × 3^k + 120 × 4^k + 175 × 5^k + 225 × 6^k) x³ + (120 × 2^k + 180 × 3^k + 220 × 4^k + 250 × 5^k + 274 × 6^k) x² − (120 × 1^k + 120 × 2^k + 120 × 3^k + 120 × 4^k + 120 × 5^k + 120 × 6^k) x$ $7^k x⁷ − (6 × 6^k + 21 × 7^k) x⁶ + (30 × 5^k + 90 × 6^k + 175 × 7^k) x⁵ − (120 × 4^k + 300 × 5^k + 510 × 6^k + 735 × 7^k) x⁴ + (360 × 3^k + 720 × 4^k + 1050 × 5^k + 1350 × 6^k + 1624 × 7^k) x³ − ( ? × 2^k +  ? × 3^k +  ? × 4^k +  ? × 5^k +  ? × 6^k +  ? × 7^k) x² + (720 × 1^k + 720 × 2^k + 720 × 3^k + 720 × 4^k + 720 × 5^k + 720 × 6^k + 720 × 7^k) x$ The Problem I want to know how to continue this sequence. That is, how to fill in the missing numbers in the seventh expression in the sequence, and how to construct the eighth and ninth and twelfth and so on expressions in the sequence. Some patterns immediately stand out: The expressions are polynomials in terms of $x$ , with coefficients composed of sums of numbers in the form in the form $(a × b^k)$ , where $a$ and $b$ are positive integers The coefficient for the $m$ th term in all expressions is a sum of $(m + 1)$ numbers in the form $(a × b^k)$ The $n$ th expression is $n$ terms long, with highest degree $n$ and containing terms of all lower degrees down to and including 1, with zero constant term The zeroth term in the $n$ th expression is $n^k × x^n$ The first term in the $n$ th expression is $−\left((n − 1) × (n − 1)^k + \frac{n² − n}{2} × n^k\right) × x^{n − 1}$ The second term in the $n$ th expression is $\left((n² − 3 n + 2) × (n − 2)^k + \frac{n³ − 4 n² + 5 n − 2}{2} × (n − 1)^k + \frac{3 n⁴ − 10 n³ + 9 n² − 2 n}{24} × n^k\right) × x^{n − 2}$ The third term in the $n$ th expression is $−\left((n³ - 6 n² + 11 n - 6) × (n − 3)^k + \frac{n⁴ - 8 n³ + 23 n² - 28 n + 12}{2} × (n − 2)^k + \frac{3 n⁵ - 25 n⁴ + 79 n³ - 119 n² + 86 n - 24}{8} × (n − 1)^k + \frac{n⁶ - 7 n⁵ + 17 n⁴ - 17 n³ + 6 n²}{48} × n^k\right) × x^{n − 3}$ The $(n − 1)$ th term (the last non-zero term) in the $n$ th expression is $(−1)^{n − 1} × (n − 1)! × (1^k + 2^k + 3^k + \, ... + (n − 1)^k + n^k) × x$ The terms alternate positive and negative — the zeroth is always positive, the first is always negative, the second is always positive, and so on The bases $b$ of the exponential numbers $(a × b^k)$ in the $m$ th term in the $n$ th expression are one higher than the bases of the corresponding numbers in the $m$ th term of the $(n − 1)$ th expression To break it down, here are the coefficients of the zeroth terms in all the expressions: $1^k$ $2^k$ $3^k$ $4^k$ $5^k$ $6^k$ $7^k$ $n^k$ Here are the coefficients of the first terms in all the expressions: $0 × 0^k +  0 × 1^k$ $1 × 1^k +  1 × 2^k$ $2 × 2^k +  3 × 3^k$ $3 × 3^k +  6 × 4^k$ $4 × 4^k + 10 × 5^k$ $5 × 5^k + 15 × 6^k$ $6 × 6^k + 21 × 7^k$ $(n − 1) × (n − 1)^k + \frac{n² − n}{2} × n^k$ Here are the coefficients of the second terms in all the expressions: $ 0 × −1^k +  0 × 0^k +   0 × 1^k$ $ 0 ×  0^k +  0 × 1^k +   0 × 2^k$ $ 2 ×  1^k +  2 × 2^k +   2 × 3^k$ $ 6 ×  2^k +  9 × 3^k +  11 × 4^k$ $12 ×  3^k + 24 × 4^k +  35 × 5^k$ $20 ×  4^k + 50 × 5^k +  85 × 6^k$ $30 ×  5^k + 90 × 6^k + 175 × 7^k$ $(n² − 3 n + 2) × (n − 2)^k + \frac{n³ − 4 n² + 5 n − 2}{2} × (n − 1)^k + \frac{3 n⁴ − 10 n³ + 9 n² − 2 n}{24} × n^k$ Here are the coefficients of the third terms in all the expressions: $  0 × −2^k +   0 × −1^k +   0 × 0^k +   0 × 1^k$ $  0 × −1^k +   0 ×  0^k +   0 × 1^k +   0 × 2^k$ $  0 ×  0^k +   0 ×  1^k +   0 × 2^k +   0 × 3^k$ $  6 ×  1^k +   6 ×  2^k +   6 × 3^k +   6 × 4^k$ $ 24 ×  2^k +  36 ×  3^k +  44 × 4^k +  50 × 5^k$ $ 60 ×  3^k + 120 ×  4^k + 175 × 5^k + 225 × 6^k$ $120 ×  4^k + 300 ×  5^k + 510 × 6^k + 735 × 7^k$ $(n³ − 6 n² + 11 n − 6) × (n − 3)^k + \frac{n⁴ − 8 n³ + 23 n² − 28 n + 12}{2} × (n − 2)^k + \frac{3 n⁵ − 25 n⁴ + 79 n³ − 119 n² + 86 n − 24}{24} × (n − 1)^k + \frac{n⁶ − 7 n⁵ + 17 n⁴ − 17 n³ + 6 n²}{48} × n^k$ Here are the coefficients of the fourth terms in all the expressions (note that I don't know the pattern for the last number in each coefficient): $  0 × −3^k +   0 × −2^k +    0 × −1^k +    0 × 0^k +    0 × 1^k$ $  0 × −2^k +   0 × −1^k +    0 ×  0^k +    0 × 1^k +    0 × 2^k$ $  0 × −1^k +   0 ×  0^k +    0 ×  1^k +    0 × 2^k +    0 × 3^k$ $  0 ×  0^k +   0 ×  1^k +    0 ×  2^k +    0 × 3^k +    0 × 4^k$ $ 24 ×  1^k +  24 ×  2^k +   24 ×  3^k +   24 × 4^k +   24 × 5^k$ $120 ×  2^k + 180 ×  3^k +  220 ×  4^k +  250 × 5^k +  274 × 6^k$ $360 ×  3^k + 720 ×  4^k + 1050 ×  5^k + 1350 × 6^k + 1624 × 7^k$ $(n⁴ − 10 n³ + 35 n² − 50 n + 24) × (n − 4)^k + \frac{n⁵ − 13 n⁴ + 65 n³ − 155 n² + 174 n − 72}{2} × (n − 3)^k + \frac{3 n⁶ − 43 n⁵ + 249 n⁴ − 745 n³ + 1212 n² − 1012 n + 336}{24} × (n − 2)^k + \frac{n⁷ − 14 n⁶ + 80 n⁵ − 242 n⁴ + 419 n³ − 416 n² + 220 n − 48}{48} × (n − 1)^k + ??? × n^k$ However, I have been unable to figure out patterns for constructing the terms that fall between the third and last terms in the expressions. Where the Expressions Come From The expressions generate numbers to be added as constants when developing the piecewise equations for an Irwin-Hall distribution (IHD), a probability distribution of a number $n$ of independent uniformly-distributed random variables. The probability density function (PDF) of an IHD of $n$ variables is a piecewise polynomial function consisting of $n$ segments of degree $(n − 1)$ . The Cumulative Distribution Function (CDF) of an IHD of $n$ variables is a piecewise polynomial function consisting of $n$ segments of degree $n$ . Wikipedia gives this expression for the PDF of an IHD of $n$ variables: $$f_X(x; n) = \frac{1}{2(n − 1)!} \sum_{k = 0}^n {\left((−1)^k {n \choose k} (x − k)^{n − 1} \operatorname{sgn}(x − k)\right)}$$ However, this does not directly give the piecewise polynomial form of the PDF. I want to find a way to generate the piecewise polynomial directly. This question is part of my efforts to do so. The $(n − 1)$ th derivative of the $m$ th segment in the PDF (and the $n$ th derivative of the $m$ th segment in the CDF) of an IHD of $n$ variables is a constant term of the form $\frac{(−1)^m (n − 1)!}{m! (n − 1 − m)!}$ . For example, in the case of four variables, the 3rd derivatives of the four segments of the PDF are: $\frac{ (4 − 1)!}{0! (4 − 1 − 0)!} = \frac{ 3!}{0! 3!} =  1$ $\frac{−(4 − 1)!}{1! (4 − 1 − 1)!} = \frac{−3!}{1! 2!} = −3$ $\frac{ (4 − 1)!}{2! (4 − 1 − 2)!} = \frac{ 3!}{2! 1!} =  3$ $\frac{−(4 − 1)!}{3! (4 − 1 − 3)!} = \frac{−3!}{3! 0!} = −1$ In the case of five variables, the 4th derivatives of the five segments of the PDF are: $\frac{ (5 − 1)!}{0! (5 − 1 − 0)!} = \frac{ 4!}{0! 4!} =  1$ $\frac{−(5 − 1)!}{1! (5 − 1 − 1)!} = \frac{−4!}{1! 3!} = −4$ $\frac{ (5 − 1)!}{2! (5 − 1 − 2)!} = \frac{ 4!}{2! 2!} =  6$ $\frac{−(5 − 1)!}{3! (5 − 1 − 3)!} = \frac{−4!}{3! 1!} = −4$ $\frac{ (5 − 1)!}{4! (5 − 1 − 4)!} = \frac{ 4!}{4! 0!} =  1$ The full PDF of an IHD of $n$ variables can be obtained by integrating these base numbers $(n − 1)$ times, adding a new constant term each time. The full CDF of an IHD of $n$ variables can be obtained by continuing the process one additional time. The constant terms to be added follow a specific pattern for each segment. I obtained the sequences of constant terms for each segment by trial and error, making a guess and then refining it until it lined up with the previous segment. I then used WolframAlpha to find the expressions that produce those constant terms for a given number $n$ of variables after $k$ integrations, giving rise to the sequence of equations that is the topic of this question. The constant we add to the integrations of the zeroth segment is always zero The constant we add to the integrations of the first segment is given by the sequence of expressions $\{n, \frac{−n}{2}, \frac{n}{6}, \frac{−n}{24}, \frac{n}{120}, \frac{−n}{720}, \, … \} = \frac{(−1)^k n}{0! × (k + 1)!}$ The constant we add to the integrations of the second segment is given by the sequence of expressions $\{−(n² − 2n), \frac{2n² − 3n}{2}, \frac{−(4n² − 5n)}{6}, \frac{8n² − 9n}{24}, \frac{−(16n² − 17n)}{120}, \frac{32n² − 33n}{720}, \, … \} = \frac{(−1)^{k + 1} (2^k n² − (1 + 2^k) n)}{1! × (k + 1)!}$ The constant we add to the integrations of the third segment is given by the sequence of expressions $\{\frac{n³ − 5n² + 6n}{2}, \frac{−(3n³ − 13n² + 12n)}{4}, \frac{9n³ − 35n² + 28n}{12}, \frac{−(27n³ − 97n² + 72n)}{48}, \frac{81n³ − 275n² + 196n}{240}, \frac{−(243n³ − 793n² + 552n)}{1440}, \, … \} = \frac{(−1)^k (3^k n³ − (2 × 2^k +  3 × 3^k) n² + 2! (1 + 2^k + 3^k) n)}{2! × (k + 1)!}$ The constant we add to the integrations of the fourth segment is given by the sequence of expressions $\{\frac{n⁴ − 9n³ + 26n² − 24n}{6}, \frac{−(4n⁴ − 33n³ + 83n² − 60n)}{12}, \frac{16n⁴ − 123n³ + 281n² − 180n}{36}, \frac{−(64n⁴ − 465n³ + 995n² − 600n)}{144}, \frac{256n⁴ − 1779n³ + 3641n² − 2124n}{720}, \frac{−(1024n⁴ − 6873n³ + 13643n² − 7800n)}{4320}, \, … \} = \frac{(−1)^k (4^k n⁴ − (3 × 3^k + 6 × 4^k) n³ + (6 × 2^k + 9 × 3^k + 11 × 4^k) n² − 3! (1 + 2^k + 3^k + 4^k) n)}{3! × (k + 1)!}$ An Example of the Process The equation for the third segment ( $m = 3$ ) after three integrations ( $k = 3$ ) is: $$\int \left( \int \left( \int \left( \frac{(−1)^3 (n − 1)!}{3! (n − 1 − 3)!} \right) + \frac{n³ − 5n² + 6n}{2! × 1!} \right) − \frac{3n³ − 13n² + 12n}{2! × 2!} \right) + \frac{9n³ − 35n² + 28n}{2! × 3!}$$ In the case of four variables ( $n = 4$ ), that simplifies to: $$\int \left( \int \left( \int \left( −1 \right)  + 4 \right) − 8 \right) + \frac{32}{3}$$ Which expands into: $$\frac{−x³}{6} + 2x² − 8x + \frac{32}{3}$$ Which can be re-written as: $$\frac{−x³ + 12x² − 48x + 64}{3!}$$ Doing this for the other three segments in the PDF of the IHD of four variables gives the piecewise polynomial: $$f_X(x; 4) = \begin{cases}
\frac{x³}{3!} & : 0 ≤ x ≤ 1 \\
\frac{−3x³ + 12x² − 12x + 4}{3!} & : 1 ≤ x ≤ 2 \\
\frac{3x³ − 24x² + 60x − 44}{3!} & :  2 ≤ x ≤ 3 \\
\frac{−x³ + 12x² − 48x + 64}{3!} & :  3 ≤ x ≤ 4 \\
0 & : \text{otherwise}
\end{cases}$$ Back to the Problem The expressions for the $m$ th segment are all divided by $((m − 1)! × (k + 1)!)$ . When $m$ is even, the expressions are multiplied by $(−1)^{k + 1}$ . When $m$ is odd, the expressions are multiplied by $(−1)^k$ . Removing these known factors and replacing the $n$ s with $x$ s leaves the polynomials that are the basis of this question. Here are the numbers I've worked out for the seventh expression in the sequence, which is as far as I can get with WolframAlpha: $1 x⁷ −  27 x⁶ +  295 x⁵ −  1665 x⁴ +  5104 x³ −  8028 x² +  5040 x$ 1, 0, 0, 0, 0, 0, 0, 8, 63, 280, 924, 2520, 6006, 12936, 25740, ... $7 x⁷ − 183 x⁶ + 1915 x⁵ − 10185 x⁴ + 28678 x³ − 39672 x² + 20160 x$ 1, −2, 0, 0, 0, 0, 0, 64, 495, 2170, 7084, 19152, 45318, 97020, 192060, ... …","['generating-functions', 'recreational-mathematics', 'recurrence-relations', 'sequences-and-series']"
4379890,Why must differential forms be antisymmetric?,"A differential $n$ -form is defined as a totally antisymmetric $(0,n)$ -tensor field on the surface of a manifold. Why must they be antisymmetric? I understand this has something to do with asking which kinds of tensors can you define a derivative without any more structure.","['tensors', 'differential-forms', 'differential-geometry']"
4379914,What field do eigenvalues live in?,"Given a vector space $V$ over scalar field $F$ , and given a linear transformation $T : V → V$ , the definition of eigenvalues and eigenvectors of $T$ is: A nonzero vector $\mathbf{v} \in V$ is an eigenvector of $T$ iff there exists a scalar $\lambda \in F$ such that $T(\mathbf{v}) = \lambda \mathbf{v}$ . $\lambda$ is then said to be an eigenvalue of $T$ corresponding to $\mathbf{v}$ . But it seems some eigenvalues escape this definition. For example, let $F = \mathbb{R}$ , $V = \mathbb{R}^2$ , and $T = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ . Then some computation reveals that the eigenvalues are $i$ and $-i$ , which do not live in $F$ . If $V$ is finite-dimensional, I can at least confidently say that all eigenvalues live in $\overline{F}$ , the algebraic closure of $F$ . But what for infinite-dimensional cases?","['linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
4379930,Prove Bergman's kernel formula by theorem of residues,"I am having trouble with exercise $5$ , $4.5.3$ in Ahlfors's complex analysis. I was asked to prove the Bergman's kernel formula: \begin{equation}
f(\zeta)=\frac{1}{\pi}\iint\limits_{|z|<1}\frac{f(z)\mathrm{d}x\mathrm{d}y}{(1-\bar{z}\zeta)^{2}}
\end{equation} under the conditions f(z) is bounded and analytic in the unit disk, moreover $\zeta$ lies in the disk. Two solutions of this problem are available on this website by using series expansion or Green's formula. However Ahlhors suggested another approach by using polar coordinates first, then transforming the inside integral into a line integral which can be evaluated by theorem of residues. I really don't know how to proceed in this way. I haved tried as following: \begin{align}
\iint\limits_{D}\frac{f(z)\mathrm{d}x\mathrm{d}y}{(1-\bar{z}\zeta)^{2}}&=\int_{0}^{1}\mathrm{d}r\int_{0}^{2\pi}\frac{f(z)r}{(1-\bar{z}\zeta)^{2}}\mathrm{d}\theta\\
&=\int_{0}^{1}r\mathrm{d}r\oint\limits_{|z|=r}\frac{f(z)}{(1-\bar{z}\zeta)^{2}z}\mathrm{d}z\\
\text{Since $z \bar{z}=r^{2}$}\\ &= \int_{0}^{1}r\mathrm{d}r\oint\limits_{|z|=r}\frac{f(z)z}{(z-r^{2}\zeta)^{2}}\mathrm{d}z
\end{align} At this stage, I have observed that if $\zeta=0$ , then by Cauchy's theorem, the equality holds. If not, I use Residues to evaluate the contour integral and get the value $f(r^{2}\zeta)+f'(r^{2}\zeta)r^{2}\zeta$ . But I stacked here because it seems impossible to get an explicit result of the integral with respect to r. Then I spent a huge amount of time trying to construct a special change of variable for the inside contour integral by a fractional linear transformation from a unit disk onto the smaller disk. But I just can not find the right linear transformation. Maybe I am on the wrong track.","['complex-analysis', 'residue-calculus']"
4379953,Kempf's Proof that Projective Varieties are Complete,"The following proof is taken from ""Algebraic Varieties"" by Kempf. There seems to be way too many $n$ 's in the proof. The projective variety is $\mathbb{P}^n$ , the number of polynomials is $n$ , there seems to be a linear map $\beta_n$ for each $n$ . How does one clean this up? Additionally, Kempf says $P_j$ is the space of homogenous polynomials of degree $a_j$ , then he write $P_{n-a_j}$ , does he mean the space of homogenous polynomials of degree $n-a_j$ ? And does $P_n$ refer to polynomials of degree $n$ or degree $a_n$ ? Do you also find this proof to be highly confusing?","['proof-explanation', 'algebraic-geometry', 'projective-varieties']"
4380012,Integrating $\sqrt{1-x^2}$ in an unusual way.,"We have $$
\int \sqrt{1-x^2}\,\mathrm dx.
$$ The integral above can be easily done using trig-sub. But a question rose during class: ""Can it be done without using trig sub?"" And here 's the work. Even without using trig sub, I understand this way of work is not optimal. But I'm not looking for the best way to solve it. I'm looking for why this answer is wrong . At first, I suspected that it is the correct answer but just written in different form, probably by some trig identities. But when I graphed both correct result and this result, they look different. Furthermore, when I find the derivative, I don't get the given integral, so it's obviously wrong, but I'm having trouble finding what step is wrong. Any help would be appreciated. Edit: Below's the summary. For a side scratch work, I hope you can find it from the link as it's hard to show below, and I hope you understand. From $$\int\sqrt{1-x^2}dx$$ ,
multiply both the numerator and the denominator by $\sqrt{1-x^2}$ , and get $$=\int\frac{1-x^2}{\sqrt{1-x^2}}dx$$ . Using integration by parts with choices $u=1-x^2$ and $dv=\frac{1}{\sqrt{1-x^2}}dx$ , it becomes $$=(1-x^2)\sin^{-1}x+2\int x\sin^{-1}x\,dx$$ Let $I_1=\int x\sin^{-1}x\,dx$ and rewrite it as $$=(1-x^2)\sin^{-1}x+2I_1$$ For $I_1$ , do another integration by parts with choices $u=\sin^{-1}x$ and $dv=x\,dx$ . $$I_1=\frac{1}{2}x^2\sin^{-1}x-\frac{1}{2}\int\frac{x^2}{\sqrt{1-x^2}}\,dx$$ Let $I_2=\int\frac{x^2}{\sqrt{1-x^2}}\,dx$ , and rewrite it as $$I_1=\frac{1}{2}x^2\sin^{-1}x-\frac{1}{2}I_2$$ $$I_2=\int\frac{x^2}{\sqrt{1-x^2}}\,dx=\int x\cdot\frac{x}{\sqrt{1-x^2}}\,dx$$ Use substitution with $u=1-x^2$ , which means $-\frac{1}{2}\,du=x\,dx$ and $x=\sqrt{1-u}$ if $x>0$ or $x=-\sqrt{1-u}$ if $x<0$ .
If $x>0$ , then $$I_2=-\frac{1}{2}\int\sqrt{1-u}\cdot\frac{1}{\sqrt{u}}\,du$$ For the integral, use integration by parts with choices $a=\frac{\sqrt{1-u}}{\sqrt{u}}$ and $dv=du$ . Then, $$=-\frac{1}{2}\bigg(u\sqrt{\frac{1-u}{u}}+\frac{1}{2}\int\frac{u}{u^{3/2}\sqrt{1-u}}\,du\bigg)$$ $$=-\frac{1}{2}u\sqrt{\frac{1-u}{u}}-\frac{1}{4}\int\frac{1}{\sqrt{u}\sqrt{1-u}}\,du$$ $$=-\frac{1}{2}\sqrt{u}\sqrt{1-u}-\frac{1}{4}\cdot I_3$$ ,
where $I_3=\int\frac{1}{\sqrt{u}\sqrt{1-u}}\,du$ .
Similarly, if $x<0$ , $$I_2=\frac{1}{2}\sqrt{u}\sqrt{1-u}+\frac{1}{4}\cdot I_3$$ For $I_3$ , use substitution with choice $z=\sqrt{u}$ . Then $$I_3=2\sin^{-1}\sqrt{u}+C$$ Substitute $I_3$ into $I_2$ , and get $$I2=
 \begin{cases} 
      -\frac{1}{2}\sqrt{u}\sqrt{1-u}-\frac{1}{2}\sin^{-1}\sqrt{u}+C, & x>0 \\
      \frac{1}{2}\sqrt{u}\sqrt{1-u}+\frac{1}{2}\sin^{-1}\sqrt{u}+C, & x<0
   \end{cases}
$$ $$=
 \begin{cases} 
      -\frac{1}{2}\sqrt{1-x^2}\sqrt{x^2}-\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x>0 \\
      \frac{1}{2}\sqrt{1-x^2}\sqrt{x^2}+\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x<0
   \end{cases}
$$ But $\sqrt{x^2}=x$ if $x>0$ , and $-x$ if $x<0$ . Thus, $$=
 \begin{cases} 
      -\frac{1}{2}x\sqrt{1-x^2}-\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x>0 \\
      -\frac{1}{2}x\sqrt{1-x^2}+\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x<0
   \end{cases}
$$ Substitute this into $I_1$ and get $$I_1=
 \begin{cases} 
      \frac{1}{2}x^2\sin^{-1}x+\frac{1}{4}x\sqrt{1-x^2}+\frac{1}{4}\sin^{-1}\sqrt{1-x^2}+C, & x>0 \\
      \frac{1}{2}x^2\sin^{-1}x+\frac{1}{4}x\sqrt{1-x^2}-\frac{1}{4}\sin^{-1}\sqrt{1-x^2}+C, & x<0
   \end{cases}
$$ Substitute this and combine like terms and finally get $$\int\sqrt{1-x^2}\,dx=
 \begin{cases} 
      \sin^{-1}x+\frac{1}{2}x\sqrt{1-x^2}+\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x>0 \\
      \sin^{-1}x+\frac{1}{2}x\sqrt{1-x^2}-\frac{1}{2}\sin^{-1}\sqrt{1-x^2}+C, & x<0
   \end{cases}
$$","['integration', 'calculus', 'trigonometry']"
4380023,Prove that a relation isn't transitive,"Let \begin{eqnarray*}
M_{R}=
 \begin{pmatrix}
  1 & 1 & 0\\
  1 & 1 & 1\\
  0 & 1 & 0
 \end{pmatrix} 
\end{eqnarray*} Where $M_{R}$ is the relation matrix for a relation $R$ . Is $R$ reflexive, symmetric, antisymmetric or transitive? I find that is Symmetric but isn't reflexive and antisymmetric, To verify if $M_{R}$ is transitive. I compute the Boolean product \begin{eqnarray}
 \begin{pmatrix}
  1 & 1 & 0\\
  1 & 1 & 1\\
  0 & 1 & 0
 \end{pmatrix} 
  \odot
 \begin{pmatrix}
  1 & 1 & 0\\
  1 & 1 & 1\\
  0 & 1 & 0
 \end{pmatrix}
=
\begin{pmatrix}
  1 & 1 & 1\\
  1 & 1 & 1\\
  1 & 1 & 1
 \end{pmatrix}  
\end{eqnarray} That means that $M_{R}\odot M_{R}\neq M_{R}$ , So $M_{R}$ isn't transitive. This is correct?","['relations', 'discrete-mathematics']"
4380040,Looking for a direct way to evaluate $\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx$,"The following integral $$I=\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx=-\frac{13}{24}\zeta(3)$$ has been evaluated in different ways ( see here , here , here , and here ) but these four solutions involve many arguments of polylogs and much simplifications are required to reach the final closed form. I am wondering if there is a simpler direct way to prove it. I tried some tricks but none worked out. Here is my best try: Following @Zacky's idea , lets denote: $$ I(a)=\int_0^1\frac{\ln(x)\ln(1+a(1+x))}{1+x}dx$$ and note that $I(1)=I$ and $I(0)=0,$ $$I'(a)=\int_0^1\frac{\ln(x)}{1+a(1+x)}dx=\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}.$$ Integrate both sides from $a=0$ to $1$ , $$ \int_0^1 I'(a)=I(a)|_0^1=I(1)-I(0)=I-0=I=\int_0^1\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}da$$ Integrate by parts, $$I=\int_0^1\frac{\ln(a)\ln(1+2a)}{a(1+a)}da-\int_0^1\frac{\ln(a)\ln(1+a)}{a(1+a)}da$$ $$=\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3).$$ To finish the proof, we need to show $$\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=-\frac76\zeta(3)$$ and i am stuck here. I also added $I$ to both sides: $$2I=\int_0^1\frac{\ln(x)\ln\left(\frac{2+x}{1+2x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx$$ then used the subbing $x\to (1-x)/(1+x)$ for the first integral: $$2I=\int_0^1\frac{\ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{3+x}{3-x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx$$ and I made it even worse. Any other thoughts ( without complicated arguments of polylogs ) ? thanks, Bonus: Let's use the relation we obtained above: $$I=\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3)$$ $$I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx-\underbrace{\int_1^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx}_{x\to 1/x}+\frac{5}{8}\zeta(3)$$ $$I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+I-\int_0^1\frac{\ln^2(x)}{1+x}dx+\frac{5}{8}\zeta(3)$$ $$\Longrightarrow \int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=\int_0^1\frac{\ln^2(x)}{1+x}dx-\frac{5}{8}\zeta(3)=\frac32\zeta(3)-\frac58\zeta(3)=\frac78\zeta(3).$$","['integration', 'calculus', 'alternative-proof', 'definite-integrals']"
4380162,Probability that second of $20$ watches is broken given $2$ are broken and after discarding first based on $80\%$-correct expert advice? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question The problem goes this way. There are $20$ watches, $2$ of which are broken. And there is an expert who can say if the watch is broken. But his verdict is only correct with probability $0.8$ - in both cases, if the watch is broken and if it is not. So, then I choose one watch, show it to the expert and he says: it's broken. I choose then another watch of these $20$ . What is the probability that this second watch is broken? Thanks in advance. PS. I dont really understand how this information can help, but let it be. My math background: PhD, but it is 20 years ago, so I'm not an active mathematician anymore. The source of the problem: a student who i know. What I tried: naturally, Bayes theorem. But I still cannot solve it. PPS. I found the solution. It goes without Bayes but with the law of total probability. Its rather technical.","['conditional-probability', 'probability']"
4380163,Relation of trace and matrix derivatives,"I've been trying to calculate the gradients of a neural network for backpropagation and I used chain rule which didn't work because of dimensions of the matrices. Then I found that the trace function along with differantials can be used to calculate the correct derivatives, I tried and succesfully found the correct ones as below: $$A:B=tr(A^TB)$$ $$O = J(W_2^TW_1^TX)$$ $$\eqalign{\frac{\partial O}{\partial W_1} &= \nabla J : dO \cr
&= \nabla J : dW_2^T(W_1^TX) + W_2^T(dW_1^TX + W_1^TdX) \cr
&= \nabla J : dW_2^TW_1^TX + W_2^TdW_1^TX + W_2^TW_1^TdX \cr
&= \nabla J : dW_2^TW_1^TX + \nabla J : W_2^TdW_1^TX + \nabla J : W_2^TW_1^TdX \cr
&= \nabla JX^TW_1 : dW_2^T + W_2\nabla JX^T : dW_1^T + W_1W_2\nabla J : dX \cr
&= (W_2\nabla JX^T : dW_1^T)^T \cr
&= X\nabla J^TW_2^T: dW_1 \cr
&= X\nabla J^TW_2^T}$$ I can calculate it but I am lost looking for an explaination . Why and how is trace related to matrix derivatives?","['matrices', 'calculus', 'matrix-calculus', 'partial-derivative', 'derivatives']"
4380190,The sum of $\sum_{n=1}^{\infty}\frac 1{2n^2-1}$ using the beta function,"I am trying to find the sum of the series $$
\sum_{n=1}^{\infty}\frac 1{2n^2-1}
$$ using definite integrals. After several substitutions I get $$
\sum_{n=1}^{\infty}\frac 1{2n^2-1}=\dots=\frac{\sqrt 2}{4}\int_0^1\frac{(1-t)^{-\frac{\sqrt 2}{2}}-(1-t)^{\frac{\sqrt 2}{2}}}{t}\,\mathrm dt
$$ Now I am in stuck. It's like the combination of two beta functions $B(0,1-\sqrt 2/2)$ and $B(0,1+\sqrt 2/2)$ , however they are not defined at $0$ in the first input. Is there any way of handling with the integral? Thank you in advance. Derivation of the integral: \begin{align*}
\sum_{n=1}^{\infty}\frac 1{2n^2-1}
&=\frac 12\sum_{n=1}^{\infty}\left(\frac 1{\sqrt 2n-1}-\frac 1{\sqrt 2n+1}\right)
=\frac 12\sum_{n=1}^{\infty}\int_0^1\left(x^{\sqrt 2n-2}-x^{\sqrt 2n}\right)\mathrm dx\\[12pt]
&=\frac 12\int_0^1\sum_{n=1}^{\infty}\left(x^{\sqrt 2n-2}-x^{\sqrt 2n}\right)\mathrm dx
=\frac 12\int_0^1\frac{x^{\sqrt 2-2}-x^{\sqrt 2}}{1-x^{\sqrt 2}}\,\mathrm dx\\[12pt]
&=\left[\text{subst. $x=y^{\sqrt 2}$}\right]\ \frac {\sqrt 2}2\int_0^1\frac{y^{-\sqrt 2}-y^{\sqrt 2}}{1-y^2}\cdot y\,\mathrm dy\\[12pt]
&=\left[\text{subst. $t=1-y^2$}\right]\ \frac {\sqrt 2}4\int_0^1\frac{(1-t)^{-\frac{\sqrt 2}{2}}-(1-t)^{\frac{\sqrt 2}{2}}}{t}\,\mathrm dt
\end{align*}","['beta-function', 'definite-integrals', 'sequences-and-series']"
4380226,Kernel of Bounded Operator on Hilbert Space,"I encountered this question on an exam recently and was not able to solve it. Suppose you have a bounded operator $T$ on a Hilbert space $\mathcal{H}$ such that $I-T$ is compact, where $I$ is the identity operator. Then $ker(T)$ is finite-dimensional. I'm not really sure how to even start with this problem. We know that since $I-T$ is compact, the image of the unit ball is compact. I thought it was true that $(I-T)\mathcal{H} = I\mathcal{H} - T\mathcal{H} = \mathcal{H} - T\mathcal{H} = (T\mathcal{H})^\perp$ . However, this is not true, although I do not quite understand why. I would be grateful for some tips on how to solve this, and maybe someone can tell me why the equation above does not hold.","['hilbert-spaces', 'compact-operators', 'functional-analysis']"
4380271,Tightness of random variables,"in my probability theory course, we defined a sequence of random variables $(X_i)_{i=1}^{\infty}$ to be tight if for all $\epsilon >0$ , there is a constant $M$ s.th. $P(|X_n|>M) < \epsilon$ for all $n \in \mathbb{N}$ . I have seen the following criteria for tightness/non-tightness and I was wondering whether they are true or not: $(X_i)_{i=1}^{\infty}$ tight if there exists $M$ s.th. lim $_{n\to \infty} P(|X_n|>M) = 0$ $(X_i)_{i=1}^{\infty}$ not tight if for all $M$ lim $_{n\to \infty} P(|X_n|>M)>0$ I am quite sure that the first one is right (we get the condition that the probability is smaller than $\epsilon$ for all but finitely many n and can take the maximum of all the remaining $M$ necessary for the finitely many n). For the second one I am not so sure, I was thinking that maybe we need some uniform bound, i.e. lim $_{n\to \infty}P(|X_n|>M) \geq \epsilon>0$ . I know that the criterion is right if the limit is equal to 1.","['measure-theory', 'probability-theory']"
4380298,The most generic radicals-solvable quintic,"It's well known that it is impossible to solve a generic quintic equation in terms of radicals involving its coefficients. However: what's the ""most generic"" quintic equation that is still possible to solve using radicals? If we think of a quintic as $ax^5+bx^4+cx^3+dx^2+ex+f = 0$ then ""the most generic"" here would mean such an equation that describes the largest solvable subset of quintics. There certainly exist some subsets of quintics that are solvable, a good example is $ax^5+b=0$ , but I want to know what's the most generic look of a solvable quintic is.","['algebra-precalculus', 'quintics']"
4380388,"Evaluate $\int_0^{\pi}\log\left(5-4 \cos x\right) \, \mathrm{d}x$.","The problem is $$\int_0^{\pi}\log\left(5-4 \cos x\right) \, \mathrm{d}x$$ What I tried was using standard limit formulas like changing $\cos(x)$ to $\sin(x)$ and I also tried integration by parts on it to no avail.
Could anyone help?
By the way, the method is different from $\int_0^{\pi}\log\left(1-\cos x\right) \, \mathrm{d}x$ , otherwise I would have solved it.","['integration', 'definite-integrals', 'logarithms', 'calculus', 'trigonometry']"
4380414,The value of the sum $\sum_{n=1}^\infty \frac{1}{n\sinh(\pi n /4)}$,"Can we compute the exact value of the sum $$S = \sum_{n=1}^\infty \dfrac{1}{n\sinh(\pi n /4)}.$$ WolframAlpha spits out $S = 1.4667$ . But I have no clue how it obtains this and I suspect this may be an approximation? It may be useful to know that $$
\dfrac{1}{n\sinh(\pi n /4)} = \dfrac{|\Gamma(in/4)|^2}{4\pi},
$$ where $\Gamma$ is the gamma function.","['hyperbolic-functions', 'analysis', 'complex-analysis', 'calculus', 'sequences-and-series']"
4380415,Galois group of a function field over finite field,"I have a question about the structure of this Galois group that I can't understand:
suppose that $p>2$ is prime and $q$ is any power of p, and we have these two function fields: $$K=\mathbb{F}_{p}(t^{1/d^{'}},\mu_{d^{'}})$$ $$F=\mathbb{F}_{q}(t^{1/d})$$ where $\mu_{d^{'}}$ is the $d^{'}$ -th root of unity and $d$ is some integer that divides $d^{'}=p^{f}+1$ (f is a non-negative integer). I know that $K_{d^{'}}/F$ is a Galois extension with this Galois group: $$Gal(K/F)\cong(d\mathbb{Z}/d^{'}\mathbb{Z})\rtimes \langle 
Fr_{q}\rangle.$$ ( $\langle Fr_{q}\rangle$ is a cyclic group with order $[\mathbb{F}_{q}(\mu_{d^{'}}):\mathbb{F}_{q}]$ generated by $q$ -power Frobenius) but I can't understand how?","['finite-fields', 'field-theory', 'function-fields', 'galois-extensions', 'group-theory']"
4380431,"One red, one green and one blue ball in a box","I asked myself a question about probabilities and got pretty confused about it. Let's say there is a box with 1 red ball, 1 green ball and 1 blue ball inside.
I pick randomly a ball from the box, then put it back, and repeat the process two more times (so three times at all). So, from the first pick, my chance of picking the red ball is 1/3. And during the second picking, my chance of grabbing the ball is also 1/3. Same for the third time. But if I ask myself this next question : what is my total chance of picking one time the red ball during the three random picks combined ? And here I got confused. Shouldn't my chance of picking the red ball increase everytime I pick randomly one ball ?
Let's say the first time is 1/3, then the second time should be 1/3 (from the first time) +1/3 (from the second time) = 2/3 ?
And the third time 3/3 ? But if it's 3/3 it means that I have 100% chance to pick the ball after 3 times, which is not realistic in the real life : I could pick 3 times the green ball, or two times the green ball and one time the blue one, etc. I hope you will be able to explain this to me.
Thank you!",['probability']
4380443,"Translate every predicate expression in plain English, use number theory conects whenever possible. Prove or disprove the statements.","So far I've translated the A , B, C to plain English as I understand it but I believe there are mistakes. A) There exists a value a0 for the set of natural numbers where there exists a value d for the set of natural numbers ( in which d is greater than 0 and for all/each value k which belongs to a set of natural numbers for all/each value m which belongs to a set of natural numbers in which value m is true that in which a0 + value k times value j implies that value m is equal to 1 or value m is equal to value a0 + value k times d. B) For all / for each value p which belongs to a set of natural numbers there exists value a which belongs to a set of natural numbers in which P is Prime and not 2 such that value p implies that value a is not equal to 1 and a with the power of value p minus 1 divided by 2 is equivalent to 1 mod of value p. C) For all/for each value k which belongs to the set of natural numbers, in which k is greater or equal to 2 implies that for all values n which belong to a set of natural numbers that for all / for each value j which belongs to a set of natural numbers in which value j is less than value k it implies that for all/for each values m which belongs to a set of natural numbers in which 1 is less than value m or value m is less than value k which implies that not value m such that value n plus value j. Maybe there is also a way to prove or disprove A, B and C statements?","['predicate-logic', 'elementary-number-theory', 'logic', 'solution-verification', 'discrete-mathematics']"
4380475,How to solve derivative/limit of $f(x)=x\sqrt{4-x^2}$,"I'm trying to differentiate $x\sqrt{4-x^2}$ using the definition of derivative. So it would be something like $$\underset{h\to 0}{\text{lim}}\frac{(h+x) \sqrt{4-\left(h^2+2 h x+x^2\right)}-x \sqrt{4-x^2}}{h}$$ I was trying to solve and I just can end up with something like $$\underset{h\to 0}{\text{lim}}\frac{(x+h)\sqrt{4-x^2-2xh-h^2}-x\sqrt{4-x^2}}h \cdot \frac{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}$$ $$\underset{h\to 0}{\text{lim}}\frac{-3x^2h-3xh^2+4h-h^3+\sqrt{4-x^2}-\sqrt{4-x^2-2xh+h^2}}{h\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}$$ Now if I group on h, I will have some tricky 3 instead of 2.
The idea is I should have something like $h(2x^2+4)$ that would cancel up. I'm quite stuck can I ask a little of help? I know wolframalpha exists but it refuses to create the step by step solution with the error ""Ops we don't have a step by step solution for this query"". The final result shall be $$-\frac{2 \left(x^2-2\right)}{\sqrt{4-x^2}}$$",['limits']
4380528,Autocorrelation of sum of sinusoids.,"Consider a signal that is a sum of sinusoids, e.g. $x(t)=Asin(at)+Bcos(bt)$ Is there an easy and general way to get an analytical solution for the autocorrelation of $x(t)$ ? Is the best way to simply plug $x(t)$ into the autocorrelation formula? $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}x(t+\tau)x(t)dt$$ $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}[Asin(a(t+\tau))+Bcos(b(t+\tau))][Asin(at)+Bcos(bt)]dt$$ $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}[A^2sin(a(t+\tau))sin(at) +ABsin(a(t+\tau))cos(bt) + ABcos(b(t+\tau))sin(at) + B^2cos(b(t+\tau))cos(bt)]dt$$ Now evaluating each one independently, $$ R_{xx}(\tau)=\frac{A^2}{2}cos(a\tau)+\frac{AB}{2}sin(a\tau)-\frac{AB}{2}sin(b\tau)+\frac{B^2}{2}cos(b\tau)$$ The algebra can get pretty messy if we have a larger sum of sinusoids like $x(t)=Asin(at)+Bcos(bt)+Csin(ct)+Dcos(dt)+...$ Do I just have to bite the bullet and do the all the algebra? Or is there some general formula?","['time-series', 'statistics', 'stationary-processes', 'correlation']"
4380581,Can all covector field be written as a product of a function and a differential of another function?,"Is it available to write every covector field $$
\alpha = \sum_{i=1}^{n}\alpha^i\mathrm dx_i
$$ on a manifold into the form $$
\alpha = f\mathrm{d}g,
$$ where $f$ and $g$ are smooth functions? Or, can we do this at least locally?","['differential-forms', 'multivariable-calculus', 'smooth-manifolds', 'differential-geometry']"
4380606,Two ropes problem in measure theory,"Given two ropes, each 1 ft long. You can burn them however you like.  For the first rope, you have $\Omega_{1}= [1,2]$ , i.e.  1ft, and for the second one you have $\Omega_{2}= [3,4]$ , i.e.  1ft. Consider $\Omega:= \Omega_{1}\cup \Omega_{2}$ , and $(\Omega,\mathcal{B}(\Omega),\mu)$ a measure space where $\mu$ measures how long each measurable set takes to burn.  Each rope takes one minute to burn, i.e., $\mu(\Omega_{1}) =\mu(\Omega_{2}) = 1$ .  Note that the Radon-Nikodym derivative ofμwith respect to the Lebesgue measure is unknown to us. The goal is to use the two ropes by burning them in such a way to measure 45 seconds or 50 seconds described here https://www.youtube.com/watch?v=z7LhQrZdpIc . I have several questions: How can we mathematically prove that burning both sides of a rope takes only 30 seconds in this setting? How can we show that it only takes 50 seconds in the limit by keeping the 3 flames rules in the video?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4380704,How can construction of the gamma function be motivated?,"The gamma function extends the factorial function. This can be proved inductively using integration-by-parts. But if you didn't already know that the gamma function had this property, and you wanted to construct a continuation of factorial a priori, how could you construct the gamma function? How could you motivate an exploration which would lead to this answer?","['gamma-function', 'motivation', 'factorial', 'real-analysis']"
4380710,"The divergence and convergence about integration $ \int^\infty_0 \frac{x^x}{e^{x^\lambda}}\, dx$ when $\lambda>1$","I wonder whether this integration is convergent or divergent when $\lambda>1$ \begin{equation}
\displaystyle \int^\infty_0 \dfrac{x^x}{e^{x^\lambda}}\,dx =\displaystyle \int^\infty_0 {x^x}{e^{-x^\lambda}}\,dx 
\end{equation} I can prove that when $\lambda\leq1$ the integration diverges, but when $\lambda>1$ it's difficult for me to decide whether the integration diverges or not. I tried to split the integration at $x= 1,e,\lambda$ these places, but it doesn't help. Also, I tried to use limit comparison at $x=\infty$ , but I can't find a suitable function to do that. Then I tried to find whether there exists a function greater or less than the integrand, but it also doesn't seem to be useful. I noticed that $x^x$ and $x^\lambda$ are kind of suspicious, but I also don't know if this is a key observation for obtaining the answer. Thus, any suggestions or help on this? Thanks!","['integration', 'convergence-divergence', 'analysis']"
4380756,Why are all sets we can construct Borel sets?,"It is said in the textbook https://measure.axler.net/ However, any subset of $\mathbb R$ that you can write down in a concrete fashion is a Borel set. How should we understand this sentence? Here are some ideas I can think of I have once heard that we can suppose the Borel sets includes all subsets of $\mathbb R^n$ if we don't admit the axiom of choice and it does not conflict with ZF set theory. But I am not sure whether this can explain the universality of Borel sets. Also, it seems that Borel set is somehow alike ""Computable numbers"" proposed by Alan Turing, but I am not sure how to draw this analogy.","['borel-sets', 'measure-theory', 'set-theory', 'real-analysis']"
4380766,Evaluate $\int_0^\infty\frac{\cos(x+\ln{x})}{x^2+1}dx$ and $\int_0^\infty\frac{\sin(x+\ln{x})}{x^2+1}dx$,"I recently ran across the following integrals $$I=\int_0^\infty\frac{\cos(x+\ln{x})}{x^2+1}dx$$ $$J=\int_0^\infty\frac{\sin(x+\ln{x})}{x^2+1}dx$$ They both converge according to WolframAlpha. I am curious whether there is a closed form. I tried combining these integrals while using Euler’s identity. $$I+Ji=\int_0^\infty\frac{e^{i(x+\ln{x})}}{x^2+1}dx=\int_0^\infty\frac{x^ie^{ix}}{x^2+1}$$ I wanted to use contour integration at this step, but I didn’t know how to deal with $x^i$ since it is a multi-valued function. I don't know what type of branch cut that would require. For instance, $i^i$ is equivalent to $e^\frac{\pi}{2}$ , $e^\frac{3\pi}{2}$ , $e^\frac{5\pi}{2}$ , and so on. I am willing to accept any type of answer, complex analysis or not. ADDENDUM I think the best way to go about this is to evaluate the following considering only real values for $a$ $$I(a)=\int_0^\infty\frac{x^ne^{ix}}{x^2+1}dx$$ $$I=\Re[I(i)]$$ $$J=\Im[I(i)]$$ I tried integrating $$f(z)=\frac{z^ae^{iz}}{z^2+1}$$ along a semicircle contour in the upper half with a dent around zero. I managed to find the following: $$\int_0^\infty\frac{z^a}{z^2+1}(e^{iz}+e^{a\pi i}e^{-iz})dz=\frac{\pi}{e}e^{a\pi i/2}$$ I assumed $a$ was real in my calculation, but if we plug in $i$ $$\int_0^\infty\frac{z^ie^{iz}}{z^2+1}+e^{-\pi}\int_0^\infty\frac{z^{i}e^{-iz}}{z^2+1}dz=\pi*e^{-\pi/2-1}$$ Just like in the comments. I don't know how to use this","['complex-analysis', 'improper-integrals', 'calculus', 'definite-integrals']"
4380771,"How many length-6 lists can be made from the symbols {A,B,C,D,E,F,G} if repetition is allowed","I am trying to solve this question from The Book of Proof, and I solved it in a different way than the author. The question is as follows: How many length-6 lists can be made from the symbols $\{A,B,C,D,E,F,G\}$ if repetition is allowed and the list is in alphabetical order? (Ex. BBCEGG, but not BBBAGG). My Solution Since we want this to be in alphabetical order we need to think of what the starting letter is. If the starting letter is $A$ then we are free to choose $A$ as many times after that. If we choose $B$ to be our first letter than we can never choose $A$ in that list. Using ""stars and bars,"" let's say we choose $A$ to be our first letter, then we have $5$ more spots to from our selection of symbols. Thus, we have $5$ stars and $6$ bars, $\binom{11}{5}$ . Using the same idea for choosing $B$ being the first letter, we have $5$ more spots to fill in, but we cannot choose $A$ . This leads to the consequence of loosing a bar. So, we have $5$ stars and now $5$ bars leading to $\binom{10}{5}$ Continuing with this choosing the first letter from $A$ to $G$ we get: $$\binom{11}{5} + \binom{10}{5} + \binom{9}{5} + \binom{8}{5} + \binom{7}{5} + \binom{6}{5} + \binom{5}{5} = 924$$ Authors Solution: Any such list corresponds to a 6-element multiset made from the
symbols $\{A,B,C,D,E,F,G \}$ . For example, the list AACDDG corresponds to the multiset $[A,A,C,D,D,G]$ . Thus the number of lists equals the number of multisets, which is $\binom{12}{6}$ . My Question: Is my solution a correct way to use Stars and Bars? The way the author solved it, how can you enforce the idea that the lists are in alphabetical order without explicitly telling it so?","['solution-verification', 'combinatorics', 'discrete-mathematics']"
4380794,Gaussian Curvature for Poincare upper half plane embedded in Minkowski,"I started with the mapping of Poincaré half plane in Minkowski space $\mathbb{R}^{(1,2)}$ by the mapping $f(x,y)\to(X_0,X_1,X_2)$ via $$
X_0=\frac{x^2+y^2+1}{2 y}\notag\\
X_1=\frac{x}{y}\notag\\
X_2=\frac{x^2+y^2-1}{2 y}.\notag\\
$$ The metric on $\mathbb{R}^{(1,2)}$ is the Minkowski metric given by $$
ds^2=-dX_0^2+dX_1^2+dX_2^2.
$$ The induced metric on the Poincaré half plane model is $$
ds^2=\frac{dx^2+dy^2}{y^2}.
$$ The orthonormal frame choosen for $\mathbb{R}^{(1,2)}$ is given by $$
e_1=\{\frac{X_1}{(X_0-X_2)},1,\frac{X_1}{(X_0-X_2)}\}\notag\\
e_2=\{\frac{(1-X_1^2-(X_0-X_2)^2)}{2(X_0-X_2)},-X_1,\frac{(1-X_1^2+(X_0-X_2)^2)}{2(X_0-X_2)}\}\notag\\
n=\{\frac{(1+X_1^2+(X_0-X_2)^2)}{2(X_0-X_2)},X_1,\frac{(1+X_1^2-(X_0-X_2)^2)}{2(X_0-X_2)}\}\notag\\
$$ where $e_1,e_2$ are tangent to the disk and are spacelike while the normal $n$ is chosen as timelike ( $\langle n,n\rangle=-1$ ). The shape operator is given by $S(e_a)=-(\nabla_{e_a}n)^T$ . For this case $\nabla=\partial$ the operator is given by $$
S(e_1)=-e_1,~~~S(e_2)=-e_2.
$$ Thus the shape operator is proportional to the opposite identity matrix $S=-I$ . The determinant of the shape operator is the Gaussian Curvature. For this case it is coming as $K=1$ . But according to the literature it should be $K=-1$ . I haven't found this discussion of embedding in space with signature elsewhere. Hence I am posting it here. As I am not very much familiar with this area any comment will be helpful.","['riemannian-geometry', 'semi-riemannian-geometry', 'geometry', 'curvature', 'differential-geometry']"
4380799,Isomorphism of meromorphic function fields implies Riemann surfaces are isomorphic,"This is about from Forster, Lectures on Riemann Surfaces, Exercise 8.1. The exercise is the following: ( $\mathcal{M}(X)$ represents the field of meromorphic functions on $X$ , and likewise for $Y$ .) Suppose $X$ and $Y$ are compact Riemann surfaces such that $\mathcal{M}(X)$ and $\mathcal{M}(Y)$ are isomorphic as $\mathbb{C}$ algebras. Prove that $X$ and $Y$ are isomorphic. There's a hint which suggests representing $X$ and $Y$ as the Riemann surfaces of algebraic functions of the same polynomial $P$ from $\mathcal{M}(\mathbb{P}^1)[T]$ and the fact (which is proved later) that the meromorphic functions separate points, over a compact Riemann surface. To get started, it seems like I need a branched holomorphic covering map $\pi:X\to \mathbb{P}^1$ . One candidate seems like using an arbitrary non constant element of $\mathcal{M}(X)$ and consider that as a surjective map into $\mathbb{P}^1$ . However I'm not quite sure how to proceed. To that end I would be very grateful if someone could give me a hint where to start. (I am aware that something along these lines has already been asked here If two meromorphic function fields on compact riemann surfaces isomorphic, then they are isomorphic , but it doesn't shed much light on how I might use the hint with whatever Forster has developed so far.)","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'riemann-surfaces']"
4380816,Why is differentiation of $x^2$ equal to $2x$ and not $2x + 1$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Tbh, I know very little about differentiation and so this should most probably be a dumb question, but I was just trying out a few examples and saw that $d/dx$ of $x^2$ actually appears to be $2x + 1$ and that of $x^3$ appears to be $3x^2 + 3x + 3$ (for an increase of $1$ ).
Then why do we use only $2x$ and $3x^2$ instead? Is this for simplification?
Thanks. Examples: $2^2 \rightarrow  3^2$ is actually an increase of $7 (2 * 2 + 1)$ . $3^3 \rightarrow 4^3$ is actually an increase of $37 (3 * 3^2 + 3 * 3 + 1)$","['calculus', 'derivatives']"
4380836,Show that $f(B_t)$ is a martingale iff $f(x)=a+bx$.,"One question from Durrett's 5th Probability textbook is the following. Suppose that $f\in C^2$ . Show that $f(B_t)$ is a martingale iff $f(x)=a+bx$ . For "" $\Leftarrow$ "" direction, if $f(x)=a+bx$ , then for s<t $$
E[f(B_t)|\mathcal{F}_s]=E[a+bB_t|\mathcal{F}_s]=a+bB_s
$$ For "" $\Rightarrow$ "" direction, I want to use the Martingale representation theorem that if $M_t$ is a martingale and $M_t\in L^2$ , then there exists a unique stochastic process $g(s,\omega)$ s.t. $$
M_t=EM_0+\int_0^tg(s,\omega)dB(s)
$$ is a martingale w.r.t. $\mathcal{F}_t$ . It suffices to take the derivative of $f$ that $$
df(B_t)=f'(B_t)dB_t+\frac{1}{2}f''(B_t)dt
$$ Then $$
f(B_t)=f(0)+\int_0^tf'(B_s)dB_s+\frac{1}{2}\int_0^tf''(B_s)ds
$$ Can we get $f''(B_s)=0$ from the martingale representation theorem? That implies $f=a+bx$ ?","['real-analysis', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4380885,"Inequality of *Problem From The Book* 19.20, but Using Integrals","The problem is here: Let $a_1,a_2,\dots,a_n$ be positive real numbers and let $S=a_1+a_2+\dots+a_n$ . Prove that $$\frac 1n \sum_{1=1}^n\frac 1{a_i}+\frac{n(n-2)}S\ge\sum_{i\ne j}\frac1{S-a_i+a_j}$$ The chapter of this problem is Solving Elementary Inequality Using Integrals . After I typed the problem, I noted that this problem is already asked here . The answer is using  Karamata's inequality, which... does not really fit the title of the chapter. This chapter suggested a technique: using $\int_0^1 x^{t-1}\mathrm dx=\frac 1t$ to transform the problems involving the fractions into polynomials. I tried this technique, let $y_i=x^{a_i-1/n}$ , and we have, before integral as $\int_0^1 \dots\mathrm dx$ , the inequality should be $$\sum_{i=1}^n y_i^n+n(n-2)y_1y_2\dots y_n\ge y_1y_2\dots y_n\sum_{i\ne j}\frac{y_i}{y_j}$$ and I don't know  how to continue from here... or, is there another integration method other than the techneque above? Many thanks!","['integration', 'inequality']"
4380909,Does $\frac1{n^\alpha}\sum_{k=1}^n kX_k$ converge in probability to a non-zero random variable?,"Suppose $(X_n)_{n\ge 1}$ is a sequence of independent random variables such that $X_n=\frac1n$ with probability $\frac34$ and $X_n=-\frac1n$ with probability $\frac14$ for every $n\ge 1$ . I am trying to find whether the sequence $Y_n=\frac1{n^\alpha}\sum_{k=1}^n kX_k$ converges in probability to a non-zero random variable. I found $E(Y_n)=\frac1{2n^{\alpha-1}}$ and $\operatorname{Var}(Y_n)=\frac3{4n^{2\alpha-1}}$ , from which I can say $Y_n\to 0$ in probability as long as $\alpha>1$ . So if $Y_n$ converges to a non-zero random variable, then it must happen for $\alpha\le 1$ . But I could not narrow down this search of $\alpha$ . I can see that $X_n\to 0$ in probability but not sure if this helps. Any suggestion would be great.","['convergence-divergence', 'probability-theory']"
4380924,"Prove $\left(\frac{a-b}{a+b}\right)^{11}+\left(\frac{b-c}{b+c}\right)^{11}+\left(\frac{c-a}{c+a}\right)^{11}\leq 1$ where $a,b,c >0$","I have to prove: $ \displaystyle \tag*{} \left(\frac{a-b}{a+b}\right)^{11}+\left(\frac{b-c}{b+c}\right)^{11}+\left(\frac{c-a}{c+a}\right)^{11}\leq 1$ where $a,b,c >0$ My approach: $ \displaystyle \tag*{}x \mapsto a-b \\\\ y\mapsto b-c \\\\ z \mapsto c-a$ We get, $x+y+z=0$ and then the inequality becomes: $\displaystyle \tag*{} \left(\frac{x}{x+2b}\right)^{11}+\left(\frac{y}{y+2c}\right)^{11}+\left(\frac{z}{z+2a}\right)^{11}\leq 1$ I don't know how to proceed from this; Can we use AM-GM inequality to prove? Any hints would be appreciated, thanks.","['number-theory', 'elementary-number-theory', 'real-analysis', 'solution-verification', 'inequality']"
4380934,Show that $E \sup_n |\frac{\xi_n}{n}|<\infty \Leftrightarrow E|\xi_1| \log^+ |\xi_1|<\infty$.,"Problem: Let $\xi_1,\xi_2,...$ be a sequence of independent ientically distributed random variables. Show that $E \sup_n |\frac{\xi_n}{n}|<\infty \Leftrightarrow E|\xi_1| \log^+ |\xi_1|<\infty$ . This is a problem from Shiryaev's Probability Theory. Kolmogorov's Law of Large numbers: If $\xi_1,\xi_2,...$ be independent identically distributed random variables, then $E |\xi_1|<\infty \Leftrightarrow n^{-1} S_n \rightarrow E \xi_1$ and $E |\xi_1|=\infty \Leftrightarrow \lim \sup n^{-1} S_n=+\infty$ . Back to the problem, for $\Rightarrow$ , $E |\xi_1| \log^+ |\xi_1|<\infty$ , we get $E |\xi_1|<\infty$ . From Kolmogorov's law of large number, we get $\frac{\xi_n}{n} \rightarrow E \xi_1$ . So $\frac{\xi_n}{n}=\frac{S_n-S_{n-1}}{n}=E\xi_1-E\xi_1=0$ . So $P(|\xi_n|>n i.o)=0$ and $\sup_n |\frac{\xi_n}{n}| \leq 1$ , and $E \sup_n |\frac{\xi_n}{n}|<\infty$ . For $\Leftarrow$ ,I am stuck here. I think the approach is based on Kolmogorov's Law of Large number again, but I am not sure how to apply $E|\xi_1|<\infty$ to get $E|\xi_1| \log^+ |\xi_1|<\infty$ , and how to apply $E \sup_n |\frac{\xi_n}{n}|<\infty$ to get $\frac{S_n}{n}$ to converge. Thanks in advance!","['statistics', 'law-of-large-numbers', 'probability']"
4380938,Measurability conditions in the definition of Brownian motion,"I am trying to study stochastic integration from Kuo, Introduction to stochastic integration. I have two small questions about the definition of Brownian motion. First question: Kuo presents it as a measurable function defined on the product space $[0,\infty)\times \Omega=T\times \Omega$ , where $\Omega$ is a measuable space $(\Omega, \mathbf{F},P)$ , with several characteristics. I guess $T$ comes endowed with the Borel sigma algebra and the Lesbegue measure (I call it here $\mathbf{B}$ the Borel sigma algebra). Kuo observes (even if not in the definition) that: for each $\omega \in \Omega, X(\cdot,\omega)$ is a measurable function (called a sample path) I was wondering if this is an assumption or not. E.g. to have $X(\cdot,\omega)$ $\mathbf{B}$ -measurable, we should have, taken a borel set $B$ , that the set $\{t|X(t,\omega)\in B \}\subset T$ should be $\mathbf{B}$ -measurable for a fixed $\omega$ . But how we can deduce if from the fact that $\{(t,\omega)|X(t,\omega)\in B \}\subset T \times \Omega$ is $\mathbf{B} \times \mathbf{F}$ -measurable ? (this we know because $X$ is defined to be measurable in the product space) Maybe I am missing some basic property of product sigma algebras ? Second question: One of the conditions is that: $P(\omega | X(\cdot,t) \text{is continuous})=1$ I guess here the probability measure is $P$ . But how can we be sure that the set $\{ \omega | X(\cdot,t) \text{is continuous} \}$ is $\mathbf{F}$ -measurable, to be able to declare its probability? Is it an other assumption or follows from some argument? I hope the questions make sense, otherwise feel free to point out inconsistencies.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'measurable-functions']"
4380940,"The number of closed paths in the square lattice $\mathbb{Z}^2$ with length $n$ and starting and ending points at $(0,0)$.","I'm thinking about this problem right now. Problem :Consider a lattice point consisting of $\mathbb{Z}^2$ points.
If $n$ is even, i.e., $n=2p$ , then
Show that the number of closed paths in the square lattice $\mathbb{Z}^2$ with length $n$ and starting and ending points at $(0,0)$ is $\binom{2p}{p}^2$ . I'm thinking about this problem right now. The policy is consider four different ways of arranging the arrows: up, down, right, and left. In order to come back to the origin, the number of ups and downs must be the same, so if the number of ups and downs is $2s$ (but $s\in\mathbb{N}$ , the number of ups and downs is $s$ each), then out of the total $2p$ , we only need to choose $2s$ , so $\binom{2p}{2s}$ .
Furthermore, since there are $\binom{2s}{s}$ many ways to arrange up and down, and the same argument can be made for right and left, I thought the number of combinations in this case would be $\binom{2p}{2s}\binom{2s}{s}\binom{2p-2s}{p-s}$ . (Note that the number of right and left can be expressed as $2p-2s$ .)
I figured if I summed this over $s=0,1,\dots,p$ , I would get the number of combinations I wanted. Is this argument wrong? Because no matter how I calculate this, the form I should be looking for is not $\binom{2p}{p}^2$ .
I would like to know if this is the correct argument.","['graph-theory', 'integer-lattices', 'cayley-graphs', 'combinatorics']"
4380951,Determining whether a sequence of averages converges almost surely at all,"Consider the problem presented in this post: An example: The average of a sequence of random variables does not converge to the mean of each variable in the sequence almost surely , where it is determined that the sequence of averages $(S_n)_{n=2}^\infty, S_n = \frac{1}{n - 1}\sum_{m=2}^nX_m$ , $P(X_m = m) = P(X_m = -m) = \frac{1}{2m\log(m)}, P(X_m = 0) = 1 - \frac{1}{m\log(m)}$ does not converge almost surely to $0$ . But what do we know about the general behaviour of the said sequence $(S_n)_{=2}^\infty$ ? How could we determine whether or not the sequence converges almost surely to any constant $c \in \mathbb{R}$ ?","['measure-theory', 'probability-theory', 'probability', 'real-analysis']"
4380992,From homogeneous to non-homogeneous linear recurrence relation,"I'm trying to do the following exercise: Find a non-homogeneous recurrence relation for the sequence whose general term is $$a_n = \frac{1}{2}3^n - \frac{2}{5} 7^n$$ From this expression we can obtain the roots of the characteristic polynomial $P(x)$ , which are $3$ and $7$ , so $P(x) = x^2 - 10x + 21$ and $a_n = 10a_{n-1} - 21a_{n-2} \; \forall \; n \ge 2, \; a_0 = \frac {1}{10}, \; a_1 = -\frac{13}{10}$ . Now I don't know how to obtain a non-homogeneous recurrence relation given this homogeneous recurrence relation.","['recurrence-relations', 'discrete-mathematics']"
4381013,A symplectic ball embedding into $\mathbb{C}P^2$,"I want to show that the following map is a symplectic ball-embedding inside $\mathbb{C}P^2$ . The mapping is defined for every $s<1$ , $i:B^4(s)\longrightarrow \mathbb{C}P^2$ as follows, $$i(z_1,z_2):= \bigg[\sqrt{1-|z_1|^2-|z_2|^2}:z_1:z_2\bigg]$$ This is a map inside the chart $U_0$ , where the symplectic form on the chart is defined as $\tilde{\omega}=i\partial \bar{\partial}f$ where $f(v)=log(1+|v|^2)$ . So in order to show that the above embedding is symplectic, I have to show that the following map is symplectic, \begin{align*}
\theta_k: B^4(s)&\longrightarrow \mathbb{C}^2 \\
(z_1,z_2)&\mapsto \bigg(\frac{z_1}{\sqrt{1-|z_1|^2-|z_2|^2}}, \frac{z_2}{\sqrt{1-|z_1|^2-|z_2|^2}}\bigg)  
\end{align*} which is to say that $\theta_k^*(\tilde{\omega})= \omega_0$ , where $\tilde{\omega}$ is already and $\omega_0$ is the standard symplectic form on $\mathbb{C}^2$ . Now for the lack of a better way, I simply started calculating by hand, and my calculations led me to the following expression: $$\theta_k^*(\tilde{\omega})=i \partial \bar{\partial}f(\theta_k^*f)= i\partial \bar{\partial}(f\circ \theta_k)= i\bigg\{ \sum_{k=1}^2 \frac{dz_k \wedge \bar{z}_k}{(1-|z_1|^2-|z_2|^2)}+ \sum_{i,k=1}^2 \frac{z_i\bar{z}_k (dz_k\wedge d\bar{z_i}) }{(1-|z_1|^2-|z_2|^2)^2}  \bigg\}$$ and I am stuck here. Can anyone suggest me a simpler way to do this, or tell me if I am doing something wrong here? Any help would is hugely appreciated as I have been stuck at this for days.","['symplectic-geometry', 'differential-geometry']"
4381041,Proof of Bourbaki's Fixed Point Theorem,"I am studying GTM 139 and troubling about the proof of Bourbaki's fixed point theorem. To quote from that book: Let $X$ be a poset such that every well ordered subset has an lub in $X$ . If $f: X \rightarrow X$ is such that $f(x) \geq x$ for all $x \in X$ , then $f$ has a fixed point. Pick an element $x_{0} \in X .$ Let $\mathbf{S}$ be the collection of subsets $Y \subset X$ such that: (1) $Y$ is well ordered with least element $x_{0}$ and successor function $\left.f\right|_{Y-\{\text { lub } Y\}}$ . (2) $x_{0} \neq y \in Y \Rightarrow \operatorname{lub}_{X}\left(\operatorname{IS}_{Y}(y)\right) \in Y$ . For example, $\left\{x_{0}\right\} \in \mathbf{S},\left\{x_{0}, f\left(x_{0}\right)\right\} \in \mathbf{S}$ , etc. We need the following sublemmas (A) and (B): (A) If $Y \in \mathbf{S}$ and $Y^{\prime} \in \mathbf{S}$ , then $Y$ is an initial segment of $Y^{\prime}$ or vice versa. To prove (A) let $V=\left\{x \in Y \cap Y^{\prime} \mid \mathrm{WIS}_{Y}(x)=\mathrm{WIS}_{Y^{\prime}}(x)\right\} .$ Suppose first that $V$ has a last element $v .$ If $v$ is not the last element of $Y$ then $\operatorname{succ}_{Y}(v)=f(v)$ . If $v$ is not the last element of $Y^{\prime}$ then $\operatorname{succ}_{Y^{\prime}}(v)=f(v) .$ Hence if neither of $Y, Y^{\prime}$ is an initial segment of the other then $f(v) \in V$ , whence $f(v)=v$ and we are done. If, on the contrary, $V$ has no last element, let $z=\operatorname{lub}_{X}(V) .$ If $Y \neq V \neq Y^{\prime}$ then it follows from $(2)$ that $z \in Y \cap Y^{\prime}$ (because if $y=\inf (Y-V)$ then $V=\operatorname{IS}_{Y}(y)$ and therefore $z=\operatorname{lub}_{X}\left(\operatorname{IS}_{Y}(y)\right) \in Y$ by $\left.(2)\right) .$ Therefore, $z \in V$ , a contradiction, proving (A). (B) The set $Y_{0}=\bigcup\{Y \mid Y \in \mathbf{S}\}$ is in $\mathbf{S}$ . To prove (B) note that if $y_{0} \in Y \in \mathbf{S}$ then it follows from (A) that $\left\{y \in Y_{0} \mid y<y_{0}\right\}=\operatorname{IS}_{Y}\left(y_{0}\right)$ and so this subset is well ordered with successor function $f$ . This implies immediately that $Y_{0}$ is well ordered and satisfies (1). Also lub $_{X}\left(\operatorname{IS}\left(y_{0}\right)\right) \in Y \subset Y_{0}$ which gives condition (2) for $Y_{0}$ . Thus (B) is proved. Now we complete the proof. Let $y_{0}=l u b_{X}\left(Y_{0}\right)$ . If $y_{0} \notin Y_{0}$ then $Y_{0} \cup\left\{y_{0}\right\} \in \mathbf{S}$ and so $y_{0} \in Y_{0}$ after all. If $f\left(y_{0}\right)>y_{0}$ then $Y_{0} \cup\left\{f\left(y_{0}\right)\right\} \in \mathbf{S}$ contrary to the definition of $Y_{0}$ . Thus $f\left(y_{0}\right)=y_{0}$ as desired. First, I had no idea about what is happening. Could someone give me any general ideas about how this proof works? Second, I've tried using some concrete examples to help me understand, but they induced even more questions. For example, let $X=\{1,2,3,4,5\}$ (with normal partial order) and $x_0=1$ . Consider $Y=\{1,2,3\}$ and $Y'=\{1,2,4,5\}$ , then $V=\{1,2\}$ and has the last element $v=2$ . However, $\operatorname{succ}_{Y'}(2)=4\neq 3=f(2)$ . It contradicts the proof and I don't know where is the problem.","['elementary-set-theory', 'order-theory', 'fixed-point-theorems', 'set-theory']"
4381052,How to be rigorous?,"I am a computer science major currently self-studying mathematics. Today I was studying topology and found out that I don't even know elementary facts like $(A \times B) \subset (C \times D)$ doesn't imply $A \subset B$ and $C \subset D%$ . I want to make my proof rigorous, but it is not easy. Do math majors also make mistakes like above? And if they don't, Can I be like them without undergraduate education in mathematics? How? Thanks in advance.","['elementary-set-theory', 'self-learning', 'soft-question']"
4381062,Does the condition $(\nabla_XJ)Y=(\nabla_YJ)X$ imply $\nabla J=0$?,"Let $(M,g,J)$ be an Almost Hermitian manifold and $\nabla $ the Levi-Civita connection. If $$(\nabla_XJ)Y=(\nabla_YJ)X$$ for any $X,Y\in \Gamma(TM)$ , can we get $\nabla J=0$ , i.e., $(M,g,J)$ is Kahlerian? The condition says that $\nabla J$ is symmetric, and I don't think it follows from the symmetry of $\nabla J$ that $\nabla J=0$ . However, I can't an example to illustrate it.","['complex-geometry', 'almost-complex', 'kahler-manifolds', 'differential-geometry']"
4381068,Pigeonhole principle problem - avoiding a sum in consecutive sets.,"We have 48 golf balls and 30 golf holes, the holes are labeled from 1 to 30. Prove whether or not it is possible to distribute these balls into the holes while satisfying the following conditions: 1.) There must be at least one ball in each hole. 2.) The sum of balls in any number of consecutive holes cannot be 11 nor 18. (So if the fourth hole had 5 balls, fifth hole 3 balls, and sixth hole 3 balls, this would not satisfy the rule.) I tried many different approaches (and I think that it is not possible to distribute the balls that way). First, I realized that I only need to distribute 18 balls as the 30 mandatory balls are already determined to be in the their respective holes. If I distributed 18 balls each into a different hole, at least 12 of the 30 holes would only have one ball inside. That means that in any case, there will be at least 12 holes with one ball only. But this is where I got stuck. I am not asking you to solve this problem, but I think I need a nudge towards the right way.","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4381090,Can $\mathbb{R}^3$ with Hadamard product be represented as matrices?,"It is known that $\mathbb{R}^2$ with Hadamard product, represented as pairs or numbers $(a,b)$ with element-wise operations is isomorphic to real matrices of the form $\left(
\begin{array}{cc}
 \frac{a+b}{2} & \frac{a-b}{2} \\
 \frac{a-b}{2} & \frac{a+b}{2} \\
\end{array}
\right)$ , and also to the split-complex numbers. But I wonder, whether it is possible to represent $\mathbb{R}^3$ with Hadamard product and element-wise operations as real matrices in a similar way?","['matrices', 'ring-theory', 'hypercomplex-numbers']"
4381101,"Show that $(\mathbb{Z}*\mathbb{Z})/[\mathbb{Z}*\mathbb{Z},\mathbb{Z}*\mathbb{Z}]\cong \mathbb{Z}\oplus\mathbb{Z}$","Im trying to show that $$(\mathbb{Z}*\mathbb{Z)}/[\mathbb{Z}*\mathbb{Z},\mathbb{Z}*\mathbb{Z}]\cong \mathbb{Z}\times\mathbb{Z}$$ and my first thought was to use the first isomorphism theorem. I've been trying to use the homomorphism $$\phi:\mathbb{Z}*\mathbb{Z}\to \mathbb{Z}\times \mathbb{Z}$$ defined by $$\phi(a^{n_1}b^{m_1}\cdots a^{n_k}b^{m_k})=(a^{n_1}\cdots a^{n_k},b^{m_1}\cdots b^{m_k})$$ Where we are considering $\mathbb{Z}*\mathbb{Z}$ to be the free product of two infinite cyclic groups generated by a and b respectively. I have shown that this is a homomorphism and is surjective, but i am really struggling to show that the commutator is the kernel. I have that the commutator is contained in the kernel but i cannot show the other containment. I am beginning to suspect that it is in fact not equal to the kernel, but if that is the case I am stumped on how to proceed. I was hoping someone would be able to shine some light on the problem.","['group-theory', 'group-isomorphism', 'free-product']"
4381109,Number of ways from top of the black square on chess board to any bottom black square when moving downwards diagonally to black squares only.,"I have attempted this problem by drawing out grid like this: \begin{array}{|c|c|c|c|c|c|c|c|}
\hline1&&1&&1&&1&\\\hline&2&&2&&2&&1\\\hline2&&4&&4&&3&\\\hline&6&&8&&7&&3\\\hline6&&14&&15&&10&\\\hline&20&&29&&25&&10\\\hline20&&49&&54&&35&\\\hline&69&&103&&89&&35\\\hline
\end{array} This gives me $69+103+89+35=296$ $ways$ . However this method is obviously long and will not be applicable for bigger grids like 20×20 so is there a more systematic way to solve this using combinatorics. So far i have tried something like : $4$ ways to choose black square on top row. Then $(4×2)-1=7$ ways for choosing a black square on second row (I subtracted $1$ because one of the black square don't lead to 2 black squares but one). Similarly, $(7×2)-1=13$ ways to choose on third row. But for forth row it should be (according to the way I am going) $(13×2)-1=25$ ways but there are 24 ways. So my method stops working. What would be the way to go about in this question?","['chessboard', 'combinatorics']"
4381171,How to prove $\frac{d}{dx}\sinh x=\cosh x$ when $\sinh$ and $\cosh$ are defined by an integral?,"Define $\sinh$ and $\cosh$ by $$x=\int_0^{\sinh x}\frac{dt}{\sqrt{t^2+1}},\, x\in\mathbb{R}$$ $$x=\int_1^{\cosh x}\frac{dt}{\sqrt{t^2-1}},\, x\ge 0$$ and define $\cosh (-x)=\cosh x$ for $x\lt 0$ . By the inverse function theorem, we have $$\frac{d}{dt}\sinh^{-1}t=\frac{1}{\sqrt{t^2+1}}\implies \frac{d}{dx}\sinh x=\sqrt{\sinh^2 x+1},$$ $$\frac{d}{dt}\cosh^{-1}t=\frac{1}{\sqrt{t^2-1}}\implies \frac{d}{dx}\cosh x=\operatorname{sgn}(x)\sqrt{\cosh^2 x-1}.$$ How can I show that $\frac{d}{dx}\sinh x=\cosh x$ and $\frac{d}{dx}\cosh x=\sinh x$ ? I think I should use the identity $\cosh^2 x-\sinh^2 x=1$ but I don't know how to prove that identity from the integral definition. I also managed to prove $$\frac{d^2}{dx^2}\sinh x=\sinh x$$ but that doesn't seem to really help.","['definite-integrals', 'hyperbolic-functions', 'real-analysis', 'functions', 'derivatives']"
4381278,find $\lim_{n\to\infty}x_n= \left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right)$,"I want to determine the limit of the sequence $$x_n=\left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right)$$ Since the sequence is of the form $x_n=\frac{v_n}{v_n}$ with $v_n$ a positively divergent sequence, I thought to apply the Stolz criterion and find the limit of $$x_n=\frac{{v_{n+1}}-u_n}{v_{n+1}-v_n}$$ However I don't get anything concrete that allows me to find the limit, any help please?","['limits', 'calculus', 'sequences-and-series']"
4381349,Isogenous elliptic curves with the same j-invariant are isomorphic,"Let $K$ be a field. Consider two elliptic curves $E,E^{\prime}$ over $K$ isomorphic over $K^{\textrm{sep}}$ or equivalently having the same $j$ -invariant. Suppose there exists a non-zero $K$ -isogeny $E^{\prime}\to E$ . Is it always true that $E$ is isomorphic to $E^{\prime}$ over $K$ ? I am particularly interested in the case when $K$ is a number field. The statement turned out to be false for curves with complex multiplication though it is true for non CM-curves.","['number-theory', 'algebraic-geometry', 'elliptic-curves', 'arithmetic-geometry']"
4381386,What is the distribution of $W$?,"Let $W=\int_0^t B_sds$ . Find $EW$ and $EW^2$ . What is the distribution of $W$ ? From the definition, we have $$
EW=\int_0^t EB_sds=0
$$ and $$
EW^2=\int_0^t EB^2_sds=\int_0^t sds=t^2/2.
$$ But how to describe the distribution of $W$ ? Does it sound like $W\sim N(0, t^2/2)$ ?","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4381414,"Is there a more general version of ""handshakes"" formula n*(n-1)/2? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I tried to solve ""birthdays paradox"" using the ""direct way"" but not the one mentioned here in one of the topics - tried my own, and as one of the solutions I figured out that there was a need to consider cases where not only couples share same birthday but maybe groups of three four five etc. Well, there is a well known formula to count unique handshakes (unique couples) amount when we have as given n (people amount): n*(n-1)/2 My question: Is there more general formula that I can use if I need to know how many unique groups of three, or unique groups of four, or five etc...are there for a given n
Thank you","['statistics', 'combinatorics', 'probability']"
4381456,Show that $x_n$ is monotone and bounded: $x_{1} = 1$ and $x_{n+1}$ = $\sqrt{4x_n -1}$,"A sequence is given by $X_{1} = 1$ and $X_{n+1}$ = $\sqrt{4x_n -1}$ for $n \in \mathbb{N}$ . Show that $x_n$ is monotone and bounded. Find $\lim n \to \infty$ $x_n$ . Proposed solutions: I claim that $|x_n| \leq 4$ for $n \in \mathbb{N}$ By induction: Base case: $x_1 = 1 < x_2 = \sqrt{3} < 4 $ Induction hypothesis: $|x_n| \leq 4$ , prove that $|x_{n+1}| \leq 4$ $|x_{n+1}| = |\sqrt{4x_n -1}| \leq |\sqrt{4x_n}| = 2 |\sqrt{x_n}| \leq 4$ Induction step: $|x_n| \leq 4$ for all $n \in \mathbb{N}$ Now, show that the sequence is increasing??? $\lim n \to \infty$ $x_n = \lim n \to \infty \sqrt{4x_n -1} = L$ $\lim n \to \infty$ $4x_n - 1 = L^2$ $4L - 1 = L^2$ $L^2 - 4L + 1 = 0$ $L = 2 + \sqrt{3}$ (by the quadratic formula)","['limits', 'solution-verification', 'analysis', 'real-analysis']"
4381485,Where does my proof of Milman-Pettis theorem break down?,"I'm trying to prove Milman-Pettis's theorem. Let $E$ be a uniformly convex Banach space. Then $E$ is reflexive. Clearly, my attempt is not correct because I have not used the uniform convexity of $E$ . Could you elaborate on where my logic fails? Let denote by $E_w$ the set $E$ together with the weak topology $\sigma(E, E')$ . Let denote $E''_w$ the set $E''$ together with the weak $^\star$ topology $\sigma(E'', E')$ . Let $J:E_w \to E''_w, x \mapsto \hat x$ be the canonical injection. Then $J$ when restricted to its image is a homeomorphism which sends open/close/compact set to open/close/compact set. By Kakutani's theorem, it's enough to prove that $B_E:= \{x\in E \mid |x| \le 1\}$ is compact in $\sigma(E, E')$ . Because $J$ (restricted to its image) is a homeomorphism, it suffices to show that $J[B_E]$ is compact in the weak $^\star$ topology $\sigma(E'', E')$ . By Banach–Alaoglu theorem, $B_{E''} := \{\varphi \in E'' \mid \| \varphi \| \le 1\}$ is compact in $\sigma(E'', E')$ . Clearly, $J[B_E] \subseteq B_{E''}$ , so it suffices to show that $J[B_E]$ is closed in $\sigma(E'', E')$ . However, this is true because $B_E$ is closed in $\sigma(E, E')$ and $J$ (restricted to its image) is a homeomorphism. Update: I added a proof that $J$ (restricted to its image) is a homeomorphism. Let $F:= J[E]$ . We denote by $F_w$ the set $F$ with the subspace topology $\tau$ that $\sigma(E'', E')$ induces on $F$ . Let's prove that $J:E_w \to F_w$ is a homeomorphism. Clearly, $J$ is bijective. Lemma: Let $(X, \tau)$ be a topological space, $A \subseteq X$ , and $\tau_A$ the subspace topology of $A$ . Let $a\in A$ and $(x_d)_{d \in D}$ is a net in $A$ . Then $x_d \to a$ in $\tau$ if and only if $x_d \to a$ in $\tau_A$ . Let $x,x_d\in E$ . To prove that $J, J^{-1}$ are continuous, we need to prove that $x_d \to x$ in $\sigma(E, E')$ if and only if $\hat x_d \to \hat x$ in $\tau$ . By our Lemma , we need to prove $x_d \to x$ in $\sigma(E, E')$ if and only if $\hat x_d \to \hat x$ in $\sigma(E'', E')$ . This equivalence is indeed true because $\hat x_d \to \hat x$ in $\tau$ if and only if $f(x_d) \to f(x)$ for all $f\in E'$ . $\hat x_d \to \hat x$ in $\sigma(E'', E')$ if and only if $\hat x_d (f) \to \hat x (f)$ for all $f\in E'$ if and only if $f(x_d) \to f(x)$ for all $f\in E'$ .","['banach-spaces', 'reflexive-space', 'functional-analysis', 'weak-topology', 'general-topology']"
4381499,Show $\lim \limits_{n \to \infty} \frac{a_{n+1}}{a_n} = \|f\|_{\infty}$ for $f \in L^{\infty}$,"I have a question that I need help with getting started (possibly I would be back for more help). I have a measure space $(X,A,\mu)$ that is finite, and $f \in L^{\infty}(\mu)$. Also, defined is $a_n = \int_X\,|f|^n\,d\mu$. I need to show that the limit is: $$\lim_{n\to\infty}\,\frac{a_{n+1}}{a_n} = \|f\|_{\infty} .$$ I am stuck on getting started, anybody have any suggestions? thanks much","['measure-theory', 'real-analysis']"
4381504,"Minimal Sufficient Statistic for $f(x) = e^{-(x-\theta)}, \; \theta < x < \infty, \; x \in \mathbb{R}$","My question comes from Exercise 6.9(b) of Statistical Inference by Casella and Berger: 6.9: Find a minimal sufficient statistic for $\theta$ (b) $f(x|\theta) = e^{-(x-\theta)}, \quad \theta < x < \infty, \quad -\infty < \theta < \infty$ . This exercise appears to be a straighforward application of the following theorem: Theorem 6.2.13. Let $f(\mathbf{x}|\theta)$ be the pmf or pdf of a sample $\mathbf{X}$ . Suppose there exists a function $T(\mathbf{x})$ such that, for every two sample points $\mathbf{x}$ and $\mathbf{y}$ , the ratio $f(\mathbf{x}|\theta)/f(\mathbf{y}|\theta)$ is constant as a function of $\theta$ if and only if $T(\mathbf{x}) = T(\mathbf{y})$ . Then $T(\mathbf{X})$ is a minimal sufficient statistic for $\theta$ . A little algebra shows that $$\frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)} = \exp\left(\sum_{i=1}^{n} (y_i - x_i) \right) \frac{I_{(\theta,\infty)}(\mathbf{x}_{(1)})}{ I_{(\theta,\infty)}(\mathbf{y}_{(1)})}$$ for all $\mathbf{x},\mathbf{y} \in (\theta,\infty)^n$ . Now it's clear that the desired implication "" $f(\mathbf{x}|\theta)/f(\mathbf{y}|\theta)$ is constant as a function of $\theta$ if and only if $T(\mathbf{x}) = T(\mathbf{y})$ "" depends solely upon the above ratio of indicator functions. If $\mathbf{x}_{(1)} = \mathbf{y}_{(1)}$ , then $\frac{I_{(\theta,\infty)}(\mathbf{x}_{(1)})}{ I_{(\theta,\infty)}(\mathbf{y}_{(1)})} = 1$ for all $\theta \in (-\infty, \mathbf{y}_{(1)})$ (and undefined on $[\mathbf{y_{(1)}},\infty)$ ), so the ratio is constant in $\theta$ for all $\theta$ where it is defined. And if $\mathbf{x}_{(1)} < \mathbf{y}_{(1)}$ , then \begin{align*}
				\frac{I_{(\theta,\infty)}(\mathbf{x}_{(1)})}{ I_{(\theta,\infty)}(\mathbf{y}_{(1)})} &=
				\begin{cases}
					1 & \text{ if } \theta \in (-\infty,\mathbf{x}_{(1)}) \\[2pt]
					0 & \text{ if } \theta \in [\mathbf{x}_{(1)},\mathbf{y}_{(1)})
				\end{cases}
			\end{align*} (and is undefined for $\theta \geq \mathbf{y}_{(1)}$ ), so in this case the ratio clearly depends on $\theta$ .  But if $\mathbf{y}_{(1)} < \mathbf{x}_{(1)}$ , then $\frac{I_{(\theta,\infty)}(\mathbf{x}_{(1)})}{ I_{(\theta,\infty)}(\mathbf{y}_{(1)})} = 1$ for all $\theta < \mathbf{y}_{(1)}$ (and undefined everywhere else). If the ratio is constant as a function of $\theta$ if and only if $\mathbf{x}_{(1)} = \mathbf{y}_{(1)}$ , then we can straightforwardly conclude that $T(\mathbf{X}_{(1)})$ is a minimal sufficient statistic. But in the case where $\mathbf{y}_{(1)} < \mathbf{x}_{(1)}$ , is the ratio of indicator functions considered to be constant as a function of $\theta$ ? Why or why not? Any feedback would be appreciated. Edit 2/23/22: As @Henry mentioned in the comments, the situation is clarified if we change the part of the theorem that says the ratio $f(\mathbf{x}|\theta)/f(\mathbf{y}|\theta)$ is constant as a function of $\theta$ to the new statement there exists some $k(\mathbf{x},\mathbf{y}) > 0$ (a strictly positive function which does not depend on $\theta$ ) such that $f(\mathbf{x}∣\theta)=k(\mathbf{x},\mathbf{y})f(\mathbf{y}∣\theta)$ for all $\mathbf{x}, \mathbf{y}$ in the sample space and all $\theta \in \Theta$ . I would just like some ""official"" confirmation of this in the form of a theorem in a textbook. Any relevant references would be greatly appreciated.","['statistical-inference', 'statistics', 'sufficient-statistics']"
4381550,Well-defineness of Chern roots?,"Let $\mathcal{E} \to X$ be a rank $n$ (complex) vector bundle on a space $X$ (possibly with some other mild conditions to make the splitting principle holds). According to the splitting principle, there is a space $X'$ and a continuous function $f: X' \to X$ inducing an injection on the cohomology groups $H^k(X;\mathbb{Z}) \to H^k(X';\mathbb{Z})$ and such that the pullback bundle $f^*\mathcal{E} \to X'$ is a direct sum of line bundles. In that case, the total Chern class $c_\bullet(f^*\mathcal{E})$ can be written as $$(1+x_1) \cdots (1+x_n) \in H^\bullet(X';\mathbb{Z})$$ for some $x_i \in H^2(X';\mathbb{Z})$ (and the product is the cup product). Then we call $x_1, \cdots, x_n$ the Chern roots of $\mathcal{E}$ . Here is the question I can't understand: aren't Chern roots, by their definition, dependent on the space $X'$ that is chosen in the splitting principle? If that's the case, then why do people only talk about "" the Chern roots"" of $\mathcal{E}$ ? Are Chern roots unique in some sense?","['characteristic-classes', 'vector-bundles', 'algebraic-geometry', 'algebraic-topology']"
4381570,Finding information about the combinatorial concepts (arriving from Music Theory),"In my music theory PhD work on scales, I've come across certain classes of musical scales, which I believe might have parallels within mathematics and I hope you can help me learn more about these classes from a mathematical point of view (I am quite ignorant in Mathematics). In my work, I represent scales as sets on a necklace of cardinality $12$ . The nodes of the necklace are therefore marked $0$ through $11$ . Below is the set [0 2 5 7 10] The structure can manifest in any of $12$ manifestations: I have found (musically) that certain slices of a structure can reveal the different amounts of information about its rotation. As you can see above, the combination [0 2] can appear in 3 of the 12 possible rotations of the scale, while the combination [0 4] can only appear in one rotation. Therefore, with respect to the set [0 2 5 7 10] , [0 4] is a: Diagnostic Combination The scale in question [0 2 5 7 10] has $7$ such diagnostic combinations: [0 4]
[0 2 4]
[0 4 7] 
[0 4 9] 
[0 2 4 7]
[0 2 4 9] 
[0 4 7 9] Since all of these combinations contain [0 4] , I say that [0 4] is the: Identity Fragment for that set. However, there are some sets that while they have Diagnostic Combinations, they don't have an identity fragment.
For instance, the set [0 1 2 3 5] has many diagnostic combinations (for instance [0 1 5] and [0 2 3] ), but no fragment is common to them all. So this has no Identity fragment. And yet, there's another class of sets, sets that have an Identity Fragment (a fragment that appears in all diagnostic combinations), but where the Identity Fragment in itself isn't diagnostic. Such an example is the set [0 1 3 5 6 8 10] where all the diagnostic combinations contain [0 6] , but [0 6] by itself is not diagnostic (has $2$ possible manifestations). I am wondering if there's well-established work on similar concepts within math: a so-called Diagnostic Combination a so-called Identity Fragment And the cases where the Identity Fragment is and isn't also a diagnostic combination. My gut is telling me that either Combinatorics, and / or Information Theory can help me deepen my exploration of those musical structures. Am I right?","['music-theory', 'cyclic-groups', 'combinatorics', 'information-theory']"
4381651,Formulating an alternating sum of product combinations,"Consider some list $A=(a_1,a_2,\cdots,a_n)$ . I'd like to find a closed form for the following operation. $$f(A)=\sum_{k=1}^n(-1)^{k-1}s_k= s_1-s_2+\cdots(-1)^{n-1}s_n.$$ Where $s_k$ is the sum of all combinations of products of $k$ unique elements of $A$ . For example, if $A=(x,y,z)$ , then $$s_1=x+y+z,$$ $$s_2=xy+xz+yz,$$ $$s_3=xyz.$$ Thus $f(A)=(x+y+z)-(xy+xz+yz)+(xyz)$ . What is the ""nicest"" way to formulate this sum of products? Is this operation known/common in combinatorics? The best I can come up with is $$\sum_{k=1}^n(-1)^{k-1}\prod_{a\in\mathcal{P}_n(A)}a$$ where $\mathcal{P}_n(A)$ denotes the set of all subsets in $A$ with cardinality $n$ . However, it would be nice to have this in some ""standard form"" without the use of powersets (let alone the generalization of such). My binomial theorem alarm is going off; thus I imagine there is some nicer closed form which makes use of binomial coefficients. Any insight or advice would be greatly appreciated.","['summation', 'inclusion-exclusion', 'combinatorics', 'products']"
4381664,Solving $\sqrt{2x+1}-\sqrt{2x-1}=2$,"I solved $\sqrt{2x+1}-\sqrt{2x-1}=2$ by squaring both sides. $$\sqrt{2x+1}=\sqrt{2x-1}+2$$ $$2x+1=(2x-1)+4\sqrt{2x-1}+4$$ $$-1=2\sqrt{2x-1}$$ $$1=4({2x-1})$$ $$x=\frac{5}{8}$$ Now when I put $x=\frac{5}{8}$ to the given equation,I get $\sqrt{\frac{5}{4}+1}-\sqrt{\frac{5}{4}-1}=1 \neq 2$ What am I missing ?","['algebra-precalculus', 'quadratics']"
4381696,From where does the dualizing sheaf come from?,"I have been going through basic Algebraic Geometry, and although I have been using Serre duality for quite a lot of time, I still do not understand how one came up with the dualizing sheaf for a Projective scheme $X \hookrightarrow \mathbb{P}^N.$ The way we prove Serre duality in Hartshorne is like this: (I) First prove it for Projective Space (II) Define dualizing sheaf, and then prove that for a projective scheme $\mathcal{E}xt(i_*\mathcal{O}_X,\omega_{\mathbb{P}^N})$ is a dualizing sheaf. (III) Then for the duality we need ample $\mathcal{O}(1)$ and Cohen-Macaulayness. The proof went smooth, but I did not understand how Hartshorne came up with that dualizing sheaf. I know how in derived categories we look for the right adjoint of $f_*$ to prove Serre duality, but how does that influence the definition of the dualizing sheaf in Hartshorne is still not clear to me. Any comments are welcome. I am aware of almost all the questions on dualizing sheaf and Serre duality on StackExchange, but none of them seemed to clear my confusion. Thank you for your time.",['algebraic-geometry']
4381726,Proving Jordan measurability from integrability of a function,"I'm working with the next exercise and I don't know how to solve it. Let $f:A\subseteq\mathbb{R}^{n}\to\mathbb{R}$ an integrable function over $A$ where $A$ is a bounded set. Prove that if there exist $c>0$ such that for all $x\in A$ we have that $f(x)\geq c$ then $A$ is Jordan measurable set. Recall that a set $A$ is Jordan measurable iff the indicator function of $A$ is integrable over a rectangle containing $A$ iff the boundary of $A$ has zero content. My first idea was to prove that $h(x)=c\chi_{A}(x)$ is integrable over $R$ (and therefore $\chi_A(x)$ ), i.e., consider a rectangle $R$ such that $A\subseteq R$ and proving that the differente of upper and lower sums are small. As $f$ is integrable over $A$ , the zero extension of $f$ to all rectangle, called $f_A$ (i.e., $f_A$ is defined as $f$ if $x\in A$ and $0$ if $x\in R\setminus A$ ), is integrable. Then, there exist a partition $P$ of $R$ such that $U(f_A,P)-L(f_A,P)$ is less than a small number. But now, if we take a rectangle $R_i$ generated by $P$ we can see the next relation between the infimum and supremum of $f_A$ and $h$ over $R_i$ : $$\inf(h(x),R_i )\leq \inf(f_A,R_i) \ \text{and} \ \sup(h(x),R_i)\leq \sup(f_A,R_i)$$ This is not a good inequality because with this I can't bound (or not directly) $U(c\chi_A,P)-L(c\chi_A,P)$ . So, I don´t know how to continue... My next idea was to use the Lebesgue criterion of integrability to prove directly that $h$ is integrable. But, again, clearly, the set of discontinuities of $c\chi_A$ is $\operatorname{bd}(A)$ (boundary of $A$ ). If I knew that the boundary of $A$ has zero content or zero Lebesgue measure (they are equivalents as $A$ is bounded) I have finished, but this is equivalent to Jordan measurability of $A$ . So, again, I'm stucked. Any hint or idea? I really appreciate. Thanks!","['multivariable-calculus', 'calculus', 'lebesgue-measure', 'riemann-integration']"
