question_id,title,body,tags
4534917,Can chess moves be identified as a meaningful algebraic structure?,"I have been contemplating for some time if one can give the movements of chess pieces on a chess board an algebraic structure. I will give my ideas so far and hopefully someone can help me complete it. To begin, here is a picture of a chessboard: In the set which I put the algebra over,  would be the set of moves themself. I am not sure how many operations should be given over the set, but till now I have been considering only one which is the composition of moves. So, let me give an example. The bishop b1 to c3 could be represented as a move $\phi$ and pawn $c2$ to $c3$ could be represented as another move $\chi$ . Now, in my set, the element $\phi$ followed by $\chi$ or $\chi$ followed by $\phi$ denoted as $\chi \phi$ and $\phi \chi$ does not exist because more than one piece can't be in a certain spot of the chess board. Essentially multiplication means doing one move after another. However if Bishop g1 to f3 is denoted by $\alpha$ then $\chi \alpha$ and $\alpha \chi$ is in my set because they don't cause the two pieces occupying one spot issues. In essence, closed under certain multiplications but not others. It seems so generally that I don't have associativity or commutativity. One nice property is that except for moves regarding pawns, everything in the set is actually invertible. Is there anything more which can be said in this direction or is there another direction which gives a meaningful algebraic structure on the chess board? Thanks. Note: At the moment I am thinking of free movement on the Chess Board. I am not accounting for the turn based system which is there when two people play. So, for instance black can move two turns in a row (legal to multiply black moves). However, if there is interesting results which occur from restricting it into only alternating colors moving, I'd be interested in that too. The main restriction here is that I still keep the movement rules for pieces and between pieces. For example Bishop can only move in Ls, pawns one step at a time ( unless unmoved then two) etc. Also that of one chess square being occupied by only one piece.","['abstract-algebra', 'chessboard']"
4534988,compute the Hilbert series of a free product of algebras,"We work over a field K. Assume there are two connected graded algebras $B, C$ . And $A=B * C$ .
The Hilbert series of A is $A(x) = \sum_{n \geq 0} dim(A_n) x^n$ Why the Hilbert series satisfy the equation: $\frac{1}{A(x)} = \frac{1}{B(x)} + \frac{1}{C(x)} -1$ This question comes from an article ""Combinatorial Hopf algebras and generalized Dehn–Sommerville relations"" (6.4). It says this is a well-known fact. But I don't know how to prove it.","['abstract-algebra', 'combinatorics']"
4535027,Does there exist an infinite class two graph with no leaves?,"In this question I will be talking about both finite and infinite graphs. All graphs are assumed to be simple (i.e. undirected and contain no loops or double edges) and connected. It is also assumed for the entirety of this question that every graph $G=(V(G),E(G))$ satisfies the property that $\Delta(G):=\sup\{\mathrm{deg}(v):v \in V(G) \}<\infty$ , where $\mathrm{deg}(v)$ denotes the degree of a vertex $v \in V(G)$ . An edge colouring of a graph is called proper if no two adjacent edges are assigned the same colour. Given a graph $G$ , the chromatic index of $G$ , denoted $\chi'(G)$ , is the minimum number of colours required for a proper edge colouring of $G$ . By Vizing's theorem, if $G$ is a finite graph, then either $\chi'(G) = \Delta(G)$ or $\chi'(G) = \Delta(G)+1$ . Now suppose that $G$ is an infinite graph. By the De Brujin-Erdős theorem, if every finite subgraph of $G$ can be properly edge coloured with $k$ colours, then $G$ can also be properly edge coloured with $k$ colours. Thus, when $G$ is infinite, by combining the theorems of Vizing and De Brujin-Erdős, we also have that $\chi'(G) = \Delta(G)$ or $\chi'(G) = \Delta(G) +1$ . For an arbitrary (finite or infinite) graph $G$ , $G$ is called class one if $\chi'(G) = \Delta(G)$ , and is called class two if $\chi'(G)=\Delta(G)+1$ . In the context of my own research, which is studying locally compact groups acting on infinite graphs, one typically wants to study such actions which are 'highly transitive' in some sense. Thus regular infinite graphs arise naturally in this theory, and although it may not be obvious why, proper edge colourings of such graphs also arise naturally in this context. At the time of writing this, I am not aware of any examples of infinite graphs that are class two, and I cannot find much research on the topic that is useful for finding an example of such a graph. It is also known that the proportion of finite graphs of order $n$ that are class two converges to $0$ as $n \rightarrow \infty$ , so this may suggest that finding a class two infinite graph could be challenging, especially if I am looking in the wrong direction. I thus pose the following two questions: (i) Does there exist an infinite class two graph with no leaves? (ii) Does there exist an infinite regular class two graph? If the answer is yes to one or both of these questions, I would like to know an example of a graph satisfying the specified properties.","['graph-theory', 'combinatorics', 'coloring', 'infinite-graphs']"
4535063,Is restriction of smooth functions to an open submanifold epimorphic?,"I learned recently that restricting smooth functions from a manifold $M$ to a closed submanifold $X$ is a surjective ring homomorphism, which is reminiscent of how (to my understanding) closed subschemes of $\operatorname{Spec} A$ are defined as quotient rings of $A$ , i.e surjections out of $A$ . Also for affine schemes, restricting regular functions from $\operatorname{Spec} A$ to an open subscheme is not a ring surjection but is always a localisation homomorphism, and hence an epimorphism. My question is, does a similar result hold for open submanifolds? That is, does restricting smooth functions from a manifold $M$ to some open $U$ give an epimorphism - either in general or for some class of manifolds? To be honest I'm not sure what the most appropriate category to use would be, e.g I can imagine the restriction being an epimorphism in $\mathbb{R}$ -Alg but not CRing or something. Any insight would be appreciated.","['manifolds', 'algebraic-geometry', 'schemes', 'differential-geometry']"
4535100,"If metric circles on a surface are genuine circles, must the surface be sphere?","A moment's thought (or some careful examination of maps) reveals that any circle drawn on a globe is in fact a circle in real life.  The same claim holds for circles drawn on flat surfaces (obviously). Are these the only examples? Formally, let $M$ be any Riemannian surface, (isometrically) embedded in $\mathbb{R}^n$ .  Moreover, suppose that any metric circle $C$ (i.e., a locus of points with constant geodesic distance from some fixed point) is in fact a circle in $\mathbb{R}^n$ .  Then must $M$ have constant nonnegative Gaussian curvature?","['circles', 'spherical-geometry', 'differential-geometry']"
4535154,Is it feasible to use Operator Calculus to solve for $f(t)=\underbrace{\exp(\exp(\dots\exp}_{t}(0)\dots))$ over $\mathbb{R}$,"Consider the function $f(t)=\exp^{(t)}(0)$ where $\exp^{(0)}(0)=0$ and $\exp^{(t+1)}(0)=\exp(\exp^{(t)}(0))$ . That is, $$f(t)=\underbrace{\exp(\exp(\dots\exp}_{t}(0)\dots)).$$ Such a function is not short of prior study on this website before, as finding such an analytic interpolation allows for construction of functions like $h(x)$ , where $$h(h(x))=\exp(x),$$ by taking $h(x)=f\left(\frac{1}{2}+f^{-1}\left(x\right)\right)$ , as described in this answer . There are infinitely many continuous constructions for $f$ over the reals, but AFAIK, an analytic, or even an infinitely smooth example is elusive. As inspired by this video , my question is, using Operator Calculus , is it possible to solve for a (potentially analytic) solution to $f$ ? My idea comes from the fact that, by definition, $$f(x+1)=\exp(f(x))\iff\tau f(x)=\exp(f(x)),$$ where $\tau$ is the Shift Operator . Hence, using Operator Calculus, we find the expression $$e^D f(x) = e^{f(x)}\iff\sum_{n=0}^\infty \frac{D^n f(x)}{n!}=\sum_{n=0}^\infty \frac{(f(x))^n}{n!},$$ where $D=\frac{d}{dx}$ . Furthermore, $f$ also has the property that $$\frac{f'(x)}{f(x)}=f'(x-1)\iff \tau\frac{Df(x)}{f(x)}=Df(x).$$ From these facts, is it possible to solve for a solution? Update: Using the aforementioned method, I was able to deduce the inductive relation: $$D^{n+1}f(1)=\sum_{k=0}^n{n \choose k} (D^{n-k+1}f(0))(D^{k}f(1)).$$ Thus, assuming $D^{m}f(0)=m!c_m$ for $m\in\mathbb{N}$ when $$f(x)=\sum_{n=0}^\infty c_n x^n,$$ we find that $$D^{n+1}f(1)=\sum_{k=0}^n \frac{n!}{k!}(n-k+1)c_{n-k+1}(D^{k}f(1)).$$ With this, given that $$D^{n}f(1)=\sum_{k=n}^{\infty}\frac{k!}{(k-n)!}c_k x^{k-n},$$ I assume we have enough information to solve for the coefficients $(c_k)_{k\in\mathbb{N}}$ . However, this seemingly surmounts to solving a countably infinite system of equations, to which I am dumbfounded at where to begin.","['differential-operators', 'functional-equations', 'calculus', 'derivatives']"
4535177,Example of Bayes Theorem...,"In the paternity suit, the mother's blood type is A, the man's blood type is B, and the child's blood type is AB. According to various circumstances, the possibility that the man identified as a real father is 50:50. Based on the given blood type data, find the probability that he is a real father, if the father's blood type is A or O (86% of the total population), the child cannot be AB, and if the father's blood type is B or AB (14% of the total population), the probability that the child is AB is 1/4. If the man is not the father, the father is considered the representative of the general population. I'm trying to solve it with Bayes Theorem. Below are the events I defined. P(F): Event that he is a real father. We know that the prior is 1/2 by the second paragraph. P(F $^c$ ) : Complement of P(F) which is 1/2 as well P(AB|F) : Given that he is a real father, probability of the child's blood type is AB P(AB|F $^c$ ) : Given that he is not a real father, probability of the child's blood type is AB P(F|AB) : Posterior that I would like to know. P(F|AB) = $\frac{P(AB|F)P(F)}{P(AB|F)P(F) + P(AB|F^c)P(F^c)}$ Here's the thing, I'm not sure about P(AB|F $^c$ ). Did I define the events wrong or Did I miss something...(I don't think I understand the meaning of the last paragraph => ""If the man is not the father, the father is considered the representative of the general population"") Can anyone help me with this? Thank you.","['statistics', 'bayes-theorem', 'probability']"
4535182,Is there any closed form for the integral $\int_0^{\frac{\pi}{2}} \frac{\ln ^n(\sin x) \ln ^m(\cos x)}{\tan x} d x?$,"Inspired by the question in the post , I started to generalise the integral $$
\begin{aligned}
\int_0^{\frac{\pi}{2}} \frac{\ln^n (\sin x) \ln (\cos x)}{\tan x} d x =& \frac{1}{n+1} \int_0^{\frac{\pi}{2}} \ln (\cos x) d\left(\ln ^{n+1}(\sin x)\right) \\
=& \frac{1}{n+1}\left[\ln (\cos x) \ln ^{n+1}(\sin x)\right]_0^{\frac{\pi}{2}}+\frac{1}{n+1} \int_0^{\frac{\pi}{2}} \frac{\sin x \ln ^{n+1}\left(\sin x\right)}{\cos x} d x\\\stackrel{IBP}{=} &\frac{1}{2^{n+2}(n+1)} \int_0^{\frac{\pi}{2}} \frac{\ln ^{n+1}\left(\sin ^2 x\right)}{\cos ^2 x} d\left(\sin ^2 x\right)\\=& \frac{1}{2^{n+2}(n+1)} \int_0^{1} \frac{\ln ^{n+1} y}{1-y} d y \quad \textrm{ where }y=\sin^2 x\end{aligned}
$$ We then deal the last integral using an infinite series on the integral $$
\begin{aligned}
\int_0^1 \frac{y^a}{1-y} d y=\sum_{k=0}^{\infty} \int_0^1 y ^a\cdot y^k d y =\sum_{k=1}^{\infty} \frac{1}{a+k}
\end{aligned}
$$ Differentiating both sides w.r.t. $a$ by $n$ times yields $$
\int_0^1 \frac{\ln^{n+1} y}{1-y} d y=\left.\frac{\partial^{n+1}}{\partial a^{n+1}} \int_0^1 \frac{y^a}{1-y} d t\right|_{a=0} =(-1)^{n+1}(n+1) ! \sum_{k=1}^{\infty} \frac{1}{k^{n+2}}= (-1)^{n+1}(n+1) !\zeta(n+2)
$$ Then we can conclude that $$\boxed{\int_0^{\frac{\pi}{2}} \frac{\ln^n (\sin x) \ln (\cos x)}{\tan x} d x = \frac{(-1)^{n+1} n !}{2^{n+2}} \zeta(n+2)} $$ For examples, $$
\begin{aligned}
&I_1=\frac{1}{8} \zeta(3);\quad  I_2=-\frac{1}{16} \cdot 2 \cdot \zeta(4)=-\frac{\pi^4}{720} ;\quad  I_{12}=-\frac{12 !}{2^{14} }\zeta(12)=-\frac{691 \pi^{12}}{21840}
\end{aligned}
$$ Similarly, I want to generalise the integral further as $$I(n,m)=\int_0^{\frac{\pi}{2}} \frac{\ln ^n(\sin x) \ln ^m(\cos x)}{\tan x} d x =\frac{m}{2^{n+m}(n+1)} \int_0^1 \frac{\ln ^{n+1} y \ln ^{m-1}(1-y)}{1-y} d y $$ Noticing that $$
\begin{aligned}
\int_0^1 y^a(1-y)^bd y =& \int_0^1 y^a(1-y)^{b} d y =B(a+1, b+1)
\end{aligned}
$$ Therefore $$
\boxed{I(n, m)= \frac{1}{2^{n+2}(n+1)} \left.\frac{\partial^{n+1}}{\partial a^{n+1}} \frac{\partial^{m-1}}{\partial b^{m-1}}\left(\frac{\Gamma(a+1) \Gamma(b+1)}{\Gamma(a+b+2)}\right)\right|_{a=0,b=-1}}
$$ My question: Is there a closed form for the last high derivative? Your comments and suggestions are highly appreciated.","['integration', 'improper-integrals', 'gamma-function', 'calculus', 'trigonometric-integrals']"
4535197,Find all real solutions of the equation $x^{10} - x^8 + 8x^6 - 24x^4 + 32x^2 - 48 = 0$,"I have been able to factorize the polynomial as follows: $$(x^2 - 2)(x^8 + x^6 + 10x^4 - 4x^2 + 24)$$ from which $\sqrt2$ and $-\sqrt2$ are obvious solutions. My guess is that $x^8 + x^6 + 10x^4 - 4x^2 + 24 = 0$ does not have any real solutions, but how would I show that?","['algebra-precalculus', 'roots', 'polynomials']"
4535213,Conditional expectation and an independent $\sigma$-algebra,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $\mathcal{G},\mathcal{H} \subseteq \mathcal{F}$ be sub- $\sigma$ -algebras of $\mathcal{F}$ and let $X$ be a real-valued random variable that is independent of $\mathcal{H}$ . Does $$
\mathbb{E}[X\mid\sigma(\mathcal{G},\mathcal{H})]=\mathbb{E}[X\mid\sigma(\mathcal{G})]
$$ hold? Intuitively, I think this should hold, by I can not think of a proof for this. I would be grateful for help.","['conditional-probability', 'conditional-expectation', 'expected-value', 'probability-theory', 'probability']"
4535215,Proof that $\frac{d}{dx} \tan^{-1} x = \frac{1}{1+x^2}$using implicit differentiation,"Proof that $\frac{d}{dx} \tan^{-1} x = \frac{1}{1+x^2}$ using implicit differentiation My workings: $y=\tan^{-1} x$ , $x= \tan y$ $\frac{d}{dx} (x) = \frac{d}{dx} \tan y$ $1 = \sec^2 y \frac{dy}{dx}$ $\frac{dy}{dx} = \frac{1}{\sec^2 y} = \cos^2 y$ How do I carry on from here?",['derivatives']
4535253,Existence of a non vanishng vector field on a manifold,"I have some doubts on a question related to the existence of a never vanishing smooth vector field on a manifold. First of all I have proved that $M=F^{-1}(1)$ is a regular submanifold of $\mathbb{R}^4$ where $F$ is defined as: \begin{equation}
F(x,y,z,t)=x^2+y^2+z^2-t^2.
\end{equation} Then I proved that $M$ is diffeomorphic to $S^2\times\mathbb{R}$ using the following map; since for $(x,y,z,t)\in M$ we have $\sqrt{x^2+y^2+z^2}\neq 0$ , for a fixed $t$ I can define: \begin{equation}
F_t:S^2(\sqrt{1+t^2})\to S^2(1) \ \ \ F_t(x,y,z)=\bigg(\dfrac{x}{\sqrt{x^2+y^2+z^2}},\dfrac{y}{\sqrt{x^2+y^2+z^2}},\dfrac{z}{\sqrt{x^2+y^2+z^2}}\bigg).
\end{equation} Then I simply notice that $F\colon M\to S^2\times\mathbb{R}$ defined as $F(x,y,z,t)=(F_t(x,y,z),t)$ is a diffeomorphism because both $F_t$ and the identity are diffeomorphism. Finally I have the problematic question: does there exist a never vanishing vector field on $M$ ? I know that to answer it, the Hairy ball theorem (telling us that on $n$ -spheres with $n$ even, there is no never vanishing vector field) should be useful; but how can I use this statement?","['submanifold', 'vector-fields', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
4535321,Generalizing an incomplete Gamma function to many variables.,"One of the topics of interest in random matrix theory is the joint distribution of eigenvalues of Gaussian random matrices in the Wishart ensemble, see Harnad, John (ed.) , Random matrices, random processes and integrable systems , CRM Series in Mathematical Physics. Berlin: Springer (ISBN 978-1-4419-9513-1/hbk; 978-1-4419-9514-8/ebook). xviii, 524 p. (2011). ZBL1215.15002 ., for example. Now, in order to obtain the density of eigenvalues there is a need to integrate over simplices  with  the integrand being a product of power functions and an exponential. This leads to   a generalization of the Gamma function in $d $ dimensions  and as such motivates the following definition below. Let $d \ge 2$ be an integer.  Let $\vec{n}:= (n_1,n_2,\cdots,n_{d-1}) \in {\mathbb N}_+^{d-1} $ where $n_j \ge 1 $ for $j=1,\cdots,d-1$ . Finally let $\lambda_d \ge 0 $ be real. Definition 1: The multivariate lower, incomplete Gamma function  is defined as follows: \begin{equation}
\gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(-)}(\lambda_d)}
\left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot 
e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi
\end{equation} where $\Delta^{(-)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | 0 \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \lambda\right\}$ . Result 1: By using elementary integration and induction in the dimension $d$ and then differentiation with respect to parameters we have found the following formula below: \begin{equation}
\gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1}  \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}} \left.\left(
\sum\limits_{j=1}^d (-1)^{d-j} \frac{e^{-(\sum\limits_{\xi=j}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{j-1} (a_\xi+\cdots+a_{j-1}) \cdot \prod\limits_{\xi=j}^{d-1}(a_j+\cdots+ a_\xi)}
\right)\right|_{(a_\xi)_{\xi=1}^d:>1} \tag{1a}
\end{equation} Definition 2: The multivariate upper, incomplete Gamma function  is defined as follows: \begin{equation}
\Gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(+)}(\lambda_d)}
\left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot 
e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi
\end{equation} where $\Delta^{(+)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | \lambda \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \infty\right\}$ . Result 2: Using the same technique as above  we have found the following formula below: \begin{equation}
\Gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1} \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}}
\left.
\left(
\frac{e^{-(\sum\limits_{\xi=1}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{d-1} (a_\xi+\cdots+a_{d-1})}
\right)
\right|_{(a_\xi)_{\xi=1}^d:>1} \tag{2a}
\end{equation} The Mathematica code snippet below  verifies the results above: In[772]:= d = RandomInteger[{2, 6}];
L = RandomReal[{0, 5}];
n = RandomInteger[{1, 5}, d - 1];
(*Lower incomplete*)
NIntegrate[
 Product[l[xi]^(
   n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],
  Evaluate[
  Sequence @@ 
   Table[{l[j], If[j == 1, 0, l[j - 1]], L}, {j, 1, d - 1}]]]
D[Sum[(-1)^(Total[n] - j - 1)/1 Exp[-Sum[a[eta], {eta, j, d}] L]/
    Product[Sum[a[eta], {eta, xi, j - 1}], {xi, 1, j - 1}] 1/
    Product[Sum[a[eta], {eta, j, xi}], {xi, j + 0, d - 1}], {j, 1, 
    d}], Evaluate[
   Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /. 
 a[j_] :> 1

(*Upper incomplete*)
NIntegrate[
 Product[l[xi]^(
   n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],
  Evaluate[
  Sequence @@ 
   Table[{l[j], If[j == 1, L, l[j - 1]], Infinity}, {j, 1, d - 1}]]]
(-1)^(Total[n] - d + 1)
   D[Exp[-Sum[a[eta], {eta, 1, d}] L]/1 1/
    Product[Sum[a[eta], {eta, xi, d - 1}], {xi, 1 + 0, d - 1}], 
   Evaluate[
    Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /. 
 a[j_] :> 1
  
  

Out[775]= 0.0295689

Out[776]= 0.0295689

Out[777]= 0.00111378

Out[778]= 0.00111378 Now, having said all this the question would be twofold. Firstly, can we evaluate those derivatives in $(1a)$ and $(2a)$ in order to obtain some neat closed form expressions. Secondly, can we generalize those results to the case when the components of the vector $\vec{n}$ are real?","['gamma-function', 'random-matrices', 'multivariable-calculus', 'eigenvalues-eigenvectors']"
4535372,"(reference request) Simple(st) proof of construction of the probability measure on $\{0,1\}^{\mathbb N}$ with the product $\sigma$-algebra","For each $n\ge 1$ , consider the fair coin probability space $(\Omega_i,\mathcal F_i, P_i)$ with $\mathcal \Omega_i=\{0,1\}$ , sigma-algebra $\mathcal F_i = \{\emptyset, \{0\}, \{1\}, \Omega_i\}$ and $P_i(\{0\})=1/2 = P_i(\{1\})$ . Let $\Omega=\{0,1\}^{\mathbb N}=\prod_i \Omega_i$ be the set of binary sequences, and let $\mathcal F$ be the product $\sigma$ -algebra, which is as usual the $\sigma$ -algebra generated by sets of the form $\prod_{i=1}^\infty A_i$ where $A_i\subset \{0,1\}$ and $|A_i|=1$ only for finitely many $i$ . I am looking for a reference for a simple self-contained proof of the construction of the product probability measure on $\Omega=\{0,1\}^{\mathbb N}$ , without relying on extension theorems, and without relying on Tonelli's theorem either. One paper cited in a related question is ""A Proof of the Existence of Infinite Product Probability Measures"" by Sadahiro Saeki. This paper does not rely on Tonelli theorem. I am wondering if the very simple structure of $\Omega$ here (countable product, each $\Omega_i$ with cardinality 2) allows for a simple proof that bypasses the more general arguments used in extension theorems. The context is for teaching probability on a rigourous footing with $\sigma$ -algebras but without the full measure theoretic machinery of extension theorems: For most probability problems before stochastic processes, one only requires a countable sequence of independent real random variables with prescribed distribution, and $\Omega$ above is sufficiently rich for that purpose.","['measure-theory', 'probability-theory', 'probability']"
4535383,Characteristic function in central limit theorem,"I have difficulties following the proof of the CLT. I know that there are essentially three steps: Using Characteristic functions, Using Taylor's Theorem, Using Lévy's continuity theorem. In the proof of the classical CLT on Wikipedia they state that $\varphi_{Y_1} = 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right), \frac{t}{\sqrt n}\to0$ . $\frac{t}{\sqrt n}\to0$ as $n$ tends to $\infty$ is obvious to me. But how do I arrive at $\varphi_{Y_1} = 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right)$ and what is the characteristic function before using Taylor's theorem. Where do we use the fact, that the variance and mean of $\frac{S_n - \mathbb E(S_n)}{\mathbb V(S_n)}$ are $1$ and $0$ respectively. Thank you","['taylor-expansion', 'characteristic-functions', 'measure-theory', 'probability-theory']"
4535404,Analytic guess for integral $\int_0^\infty \frac{\ln^2(\sqrt{x^2+1}+x) }{\sqrt{x^2 + 1}} \ln \frac{\cosh w+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } dx$,"(I was directed here from mathoverflow.) I am not able to do the following integral analytically, but after numerical evaluations was able to guess the result for all real $w$ values. Not only is the full $w$ -dependence clear numerically, the actual coefficients are also correct to very high precision. $$\int_0^\infty \frac{1}{\sqrt{x^2 + 1}} \log^2(\sqrt{x^2+1}+x) \log\left(\frac{\cosh(w)+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } \right) dx = \frac{1}{12} \left( 2\pi^2 w^2 + w^4 \right)$$ The fact that the result is a simple low order polynomial in $w$ is surprising enough to me. I tried all kinds of tricks (new variables, partial integrations, etc.) to prove the above but was not able to. How would I go about proving it analytically?","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4535419,Distribution of Minimum Distance between repeated integers in a sequence,"Let $a$ be an integer sequence containing unique consecutive integers $(1, 2, ..., N)$ Let $b$ be any random permutation of $a$ . For example, $(N, 1, ..., 2)$ . Assume $b$ is chosen uniformly from the set of all  permutations of $a$ . Let $c$ be the sequence of $b$ appended to $a$ . Using the two lines above as an example, $c=(1, 2, ..., N, N, 1, ..., 2)$ Let $D$ be the minimum distance between any two repeated elements of $c$ . In the example above, since $N$ immediately follows $N$ , $D=1$ . If $c$ were instead $(1, 2, 3, 2, 3, 1)$ , $D$ would be $2$ since both twos (and threes) are two elements apart in the sequence. Given $D$ varies with the randomly selected permutation of $b$ , $D$ is a random variable. What is the distribution of $D$ as a function of $N$ ? Edit: I wrote some python code to calculate the empirical distribution up to N = 11, in case this helps identify some patterns. Here is the code and the output: def mindist(alist, blist):
  min_dist = 9999999999
  for item in alist:
    b_ind = blist.index(item) + len(alist)
    delta = b_ind - alist.index(item)
    if delta < min_dist:
      min_dist = delta
  return min_dist

import itertools
for N in range(3, 11 + 1):
  distribution = {}
  print(N)
  aa = list(range(1, N + 1))
  permutations = list(itertools.permutations(aa))
  for bb in permutations:
    md = mindist(aa, list(bb))
    if md in distribution.keys():
      distribution[md] += 1
    else:
      distribution[md] = 1
  print(distribution)
  print('-'*80) Output: 3
{3: 1, 2: 3, 1: 2}
--------------------------------------------------------------------------------
4
{4: 1, 3: 7, 2: 10, 1: 6}
--------------------------------------------------------------------------------
5
{5: 1, 4: 15, 3: 38, 2: 42, 1: 24}
--------------------------------------------------------------------------------
6
{6: 1, 5: 31, 4: 130, 3: 222, 2: 216, 1: 120}
--------------------------------------------------------------------------------
7
{7: 1, 6: 63, 5: 422, 4: 1050, 3: 1464, 2: 1320, 1: 720}
--------------------------------------------------------------------------------
8
{8: 1, 7: 127, 6: 1330, 5: 4686, 4: 8856, 3: 10920, 2: 9360, 1: 5040}
--------------------------------------------------------------------------------
9
{9: 1, 8: 255, 7: 4118, 6: 20202, 5: 50424, 4: 80520, 3: 91440, 2: 75600, 1: 40320}
--------------------------------------------------------------------------------
10
{10: 1, 9: 511, 8: 12610, 7: 85182, 6: 276696, 5: 558120, 4: 795600, 3: 851760, 2: 685440, 1: 362880}
--------------------------------------------------------------------------------
11
{11: 1, 10: 1023, 9: 38342, 8: 353850, 7: 1481784, 6: 3723720, 5: 6502320, 4: 8542800, 3: 8749440, 2: 6894720, 1: 3628800}
--------------------------------------------------------------------------------","['permutations', 'probability-distributions', 'combinatorics', 'discrete-mathematics', 'probability']"
4535470,"What is the density of ""powerful"" or ""squareful"" numbers?","A number is powerful if every prime divisor has multiplicity $\ge2$ . I.e. if $n=\prod p_i^{k_i}$ , then $n$ is powerful if all $k_i\ge2$ . I wanted to know if the the density of these numbers tends to $0, 1$ , or something in-between? I found a lot of other info about them online but nothing about the density in particular. Obviously the density of the perfect squares tends to $0$ but these powerful (or ""squareful"") numbers include more than just perfect squares. In fact, since every powerful number has a unique representation as $n=a^2 b^3$ , it seems like the perfect squares are an infinitely small portion of the powerful numbers. I also read that the sum of their reciprocals tends to a finite value: $$\sum_{a,b}\frac{1}{a^2b^3}=\prod_p(1-\frac{1}{p(p-1)}=\frac{315}{2\pi^4}\zeta(3)=1.943...$$ which I feel intuitively would indicate a density of zero since other sets, like the primes, have density zero but are frequent enough that the sum of their reciprocals diverges. I'm also not sure I understand the product above but guess it has something to do with powerful numbers being multaplicative with the squares and cubes of primes as primitive elements.","['number-theory', 'square-numbers', 'prime-numbers', 'density-function']"
4535530,What is the scope of a variable in set-building notation?,"Let's take a set builder notation, if I define a set { $x$ | $x$ ∈N} Is the scope of the variable supposed to be limited to the set-builder notation? Would using $x$ again imply that the second use of $x$ is new variable, and different to the first, because the scope of $x$ in set builder notation is limited to it? Is it more formally correct to define new variables for use in set-builder notation? Would this imply that re-using symbols is a sort of abuse of notation?","['elementary-set-theory', 'logic']"
4535555,"Indexed Sets, a notation question","Consider a collection of sets $B=\{A_i: i\in \mathbb{N}\}$ and assume I want to work with some some set in this collection that satisfies property $P$ . Assume also that at least one set in $B$ has property $P$ . Which of the following is appropriate? Choose some $A_i \in B$ such that $P$ holds. Choose some $A_j \in B$ such that $P$ holds. The first option re-uses the indexing variable $i$ . Is this allowed? Is this conveying that I am working with any element in the collection satisfying property $P$ without fixing which set I am working with exactly? The second option introduces a new indexing variable $j$ . Is this appropriate since it tells the reader that I am fixing a specific $j\in\mathbb{N}$ and am now working with the fixed set $A_j$ . (Though all that is known about $A_j$ is that is an element of $B$ satisfying property $P$ . I have seen this done multiple ways and it is always clear what is being conveyed. My goal is to figure out what, if anything, is an abuse of notation, and what is technically correct. Also, I want to know any implications of re-using the index variable. I am looking for more than just ""You interpret both statements the same"" unless there is a formal reason for it (perhaps referencing the logic notion of variable substitution). Finally, if my notion of what it means to ""fix"" a set with a given property when multiple such sets may exist is not well-defined, please elaborate on that idea.","['elementary-set-theory', 'notation', 'index-notation']"
4535557,Prove that $\sin^n (2x) + (\sin^n x - \cos^n x)^2 \leq 1$,"Prove that $\sin^n (2x) + (\sin^n x - \cos^n x)^2 \leq 1$ . Let $a = \sin x, b = \cos x.$ Then we want to show $2^n a^n b^n +a^{2n}-2a^n b^n + b^{2n}\leq 1,$ which follows once we show that $(2^n-2)a^n b^n \leq \sum_{j=1}^{n-1}{n\choose j} a^{2(n-j)}b^{2j}$ . I think the AM-GM inequality is useful, but I'm not sure how to apply it in this case. I don't think that for all j we have $a^{2(n-j)} b^{2j}\ge a^n b^n$ ; for instance if $a<b,$ then $a^{2(n-j)}b^{2j} < a^nb^n$ .","['algebra-precalculus', 'trigonometry', 'inequality']"
4535605,How to solve this 2nd order ODE with Dirac delta?,"I need to solve the following ODE $$ f''(x) - \zeta f(x) + \zeta\delta(x-b) = 0,$$ where $x\in(-\infty,\infty)$ and where $f(x)\rightarrow 0$ as $x \rightarrow \mp \infty$ . The solution ignoring the Dirac impulse is given by $$f(x) = c_1 e^{\sqrt{\zeta}x} + c_2 e^{-\sqrt{\zeta}x}.$$ Since I have a Dirac impulse at $x=b$ , I should be solving for two ODEs, one below $x=b$ and another above $x=b$ . Then I have to put together both solutions at $x=b$ . This is where I am confused, how can I do this part? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at $x=b$ and dissipates both to the left and right of $x=b$ . Then, mass anywhere in $x\in(-\infty,\infty)$ is taken out at a rate $\zeta$ and reinjected back to $x=b$ .","['stochastic-processes', 'dirac-delta', 'ordinary-differential-equations', 'partial-differential-equations']"
4535607,How to get the following integral which is the Laplace transform of the modified Bessel function?,"How to get the following integral $$\int_0^\infty E_{\lambda\sim \mu}[\lambda e^{2 r\lambda}]e^{-4r}dr$$ where $\mu$ is the semi-circle law $d\mu(x)=\frac{1}{2\pi}\sqrt{4-x^2}1_{|x|\le 2}dx$ . Note that if define the moment generating function $M(t)=E[e^{t\lambda}]$ and then $E_{\lambda\sim \mu}[\lambda e^{2 t\lambda}]=M'(2t)$ . Also, we can express it by the modified Bessel function $$
M'(2t)=\frac{I_1(2t)}{t}
$$ So that integral becomes the Laplace transform of the modified Bessel function $$
\int_0^\infty \frac{I_1(2r)}{r}e^{-4r}dr
$$ But how to get the result?","['integration', 'probability-distributions', 'laplace-transform', 'real-analysis', 'bessel-functions']"
4535642,"Does the Cauchy-Binet theorem simplify for matrices AWB, where W is a square matrix?","Suppose I have two matrices $A$ and $B$ , where $A$ is $m\times n$ and $B$ is $n\times m$ . The Cauchy-Binet theorem gives a way to calculate $\det(AB)$ : $$\det(AB) = \sum_S\det(A_S)\det(B_S),$$ where the sum is over all length- $m$ subsets of $\{1,2,...,n\}$ , $A_S$ is the $m\times m$ matrix whose columns are the columns in $A$ with indices $S$ , and $B_S$ is the $m\times m$ matrix whose rows are the rows in $B$ with indices $S$ . Now suppose I have a square $n\times n$ matrix $W$ . Is there a comparatively simple way to calculate $\det(AWB)$ ? It would be extra convenient if there is an expression for this in terms of $\det W$ , so it could be extended to $N$ square matrices, something like $\det(A\prod_k^NW_kB)$ . I expected to find a corollary to the Cauchy-Binet theorem for linear transformations but haven't come across anything. Maybe it's obvious and my linear algebra is just too rusty?","['determinant', 'matrices', 'linear-algebra', 'linear-transformations', 'matrix-equations']"
4535717,$\{\log n-n(1-na_n)\}$ is convergent for $a_{n+1}=a_n(1-a_n)$ and $0<a_1<1$ (without using asymptotic analysis),"Let a sequence $\{a_n\}_{n=1}^\infty$ satisfy $0<a_1<1$ and $a_{n+1}=a_n(1-a_n)$ for $n\geq 1$ . Prove that $\{\log n-n(1-na_n)\}$ is convergent (without using asymptotic analysis). This problem came to me when I was preparing my exercise lessons on Analysis(I) course, as a teaching assistant. The original problem asks students to prove $\lim_{n\to\infty}na_n=1$ . I'm curious about the asymptotic behaviour of this sequence. Using the method of asymptotic analysis, I found that $$a_n= \frac1n-\frac{\log n}{n^2}+\frac C{n^2}+o\left(\frac1{n^2}\right),\qquad n\to\infty,$$ where $C$ is a constant probably depending on the value of $a_1$ . After that, I tried to prove this asymptotic relation, by directly proving the following three properties: $\displaystyle\lim_{n\to\infty}na_n=1$ . $\displaystyle\lim_{n\to\infty}\frac{n}{\log n}(1-na_n)=1$ . $\displaystyle\lim_{n\to\infty}\log n\left(1-\frac{n}{\log n}(1-na_n)\right)=\lim_{n\to\infty}\left(\log n-n(1-na_n)\right)$ exists. We can easily show that $a_n$ is decreasing to $0$ . The first two properties can be proved using $\frac1{a_{n+1}}=\frac1{a_n}+\frac1{1-a_n}$ and Stolz–Cesàro theorem : for the first one, we apply Stolz–Cesàro theorem to $\frac{n}{1/a_n}$ ; and for the second one, we apply Stolz–Cesàro theorem to $$\frac{\frac{1-na_n}{a_n}}{\log n}=\frac{\frac1{a_n}-n}{\log n}.$$ However, I don't know how to prove the third one. I don't believe that we can prove it by directly using Stolz–Cesàro theorem. Because Stolz–Cesàro theorem will give a limit not depending on $a_1$ . Any help would be appreciated!","['recurrence-relations', 'analysis', 'real-analysis', 'sequences-and-series', 'limits']"
4535742,"Let $x = u\cos(v)$, and $y = u\sin(v)$, and assume $f(u,v)$ is given. Determine $f_x$ and $f_y$ in terms of $u, v, f_u$, and $f_v$.","Let $x = u \cos(v)$ and $y = u \sin(v)$ , and assume $f(u, v)$ is given. Determine $f_x$ and $f_y$ in terms
of $u$ , $v$ , $f_u$ , and $f_v$ . I thought of chain rule like $$
\frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dx} 
+ \frac{df}{dv} \cdot \frac{dv}{dx}. 
$$ But in order to find $du/dx$ , I have to find $u$ in terms of $v$ and $x,y$ . Is there anyway to solve this?","['inverse-function-theorem', 'jacobian', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
4535752,(Grafakos 2.2.12) Prove that all Schwartz functions $f$ on $\Bbb R^n$ satisfy this inequality,"Let $1 \leq p \leq \infty $ and let $p'$ thus $\frac{1}{p} + \frac{1}{p'}=1$ . $\mathcal{S}(\mathbb R^n)$ is the Schwartz space defined as the class of all $C^\infty$ functions $\varphi:\Bbb R^n\rightarrow \mathbb C$ such that $$\sup_{x\in\mathbb R^n}|x^\beta \partial^\alpha \varphi(x)|<\infty,$$ for all multi-indices $\alpha, \beta\in\mathbb N^n$ Prove that all Schwartz functions $f$ on $R^n$ satisfy this inequality $$\|f\|_{\infty}^2 \leq \sum_{|\alpha + \beta|=n} \|\partial^\alpha f\|_p\|\partial^\beta f\|_{p^{'}}$$ where the sum is taken over all pairs of multi-indices $\alpha$ and $\beta$ whose sum has size $n$ . My try: I have already proved that $\|f\|_{\infty}^2 \leq 2\|f\|_p \|f\|_{p^{'}}$ but I don't know how to continue. Could someone help me? I am allowed to use only the theorems a definitions of that chapter of the book.","['harmonic-analysis', 'fourier-analysis', 'analysis', 'real-analysis']"
4535778,Cardinality of Image of Injection equals Cardinality of its Domain?,"Preliminaries: (1) I know that a mapping is bijective if and only if it is injective and surjective. (2) I also know that a mapping $f:X\rightarrow Y$ is surjective if and only if $f(X)=Y$ , where $f(X)$ denotes the image of $X$ under $f$ . (3) I've defined two sets $X$ and $Y$ to be equinumerous, written $X\sim Y$ , iff there is a bijective mapping $f:X\rightarrow Y$ . Question: In my notes I have the following statement (I can't remember where it's coming from, it's been a while...). Let $A$ and $B$ be sets. A Mapping $f:A\rightarrow B$ is injective if and only if for all subsets $X\subseteq A$ we have $X\sim f(X)$ , i.e., if for all $X\subseteq A$ there is a bijective map $g:X\rightarrow f(X)$ , where $f(X)$ denotes the image of $X$ under $f$ . I've tried to come up with a proof for this statement but so far I didn't really succeed and so I've started to wonder if that theorem is actually true or not. I think I can proof the implication "" $f$ is injective $\Longrightarrow$ $X\sim f(X)$ "", though. Proof $(\Longrightarrow)$ Let $f:A\rightarrow B$ be injective. Then because of (2) $\tilde{f}:X\rightarrow f(X)$ is surjective and therefore, because of (1), bijective. Then, from definition (3), it follows that $X\sim f(X)$ . I'm not quite sure about the first step in my proof though. When $f$ is injective, why is a map $\tilde{f}$ injective too? Furthermore, what about the $(\Leftarrow)$ direction of the proof? Anyone got an idea or should I rewrite the Theorem and make it an implication instead of an equivalence?","['cardinals', 'functions']"
4535793,Integral: $\int_{0}^{2\pi}\arctan\left(\frac{1+2\cos x}{\sqrt{3}}\right)dx$,"(Context) While working on an integral for fun, I stumbled upon the perplexing conjecture: $$\int_{0}^{2\pi}\arctan\left(\frac{1+2\cos x}{\sqrt{3}}\right)dx = 2\pi\operatorname{arccot}\left(\sqrt{3+2\sqrt{3}}\right).$$ (Attempt) I tried multiple methods. One method that stuck out to me was using the formula $$\arctan(\theta) = \frac{1}{2i}\ln{\left(\frac{1+i\theta}{1-i\theta}\right)}$$ so that my integral becomes $$\frac{1}{2i}\int_{0}^{2\pi}\ln\left(1+i\left(\frac{1+2\cos x}{\sqrt{3}}\right)\right)dx-\frac{1}{2i}\int_{0}^{2\pi}\ln\left(1-i\left(\frac{1+2\cos x}{\sqrt{3}}\right)\right).$$ Both of these look similar to the integral $$\int_{0}^{2\pi}\ln\left(1+r^2-2r\cos(x)\right)dx=\begin{cases}
0, &\text{for}\; |r|<1,\\
2\pi\ln \left(r^2\right), &\text{for}\; |r|>1,
\end{cases}\tag{2}$$ and its solution can be found here . I tried to get my integrals to ""look"" like the above result but to no avail. Not wanting to give up, I searched on this site for any ideas, and it seems like a few people have stumbled upon the same kind of integral, such as here and here . In the first link, the user @Startwearingpurple says, ""Now we have \begin{align}
4\sqrt{21}\pm i(11-6\cos\varphi)=A_{\pm}\left(1+r_{\pm}^2-2r_{\pm}\cos\varphi\right)
\end{align} with $$r_{\pm}=\frac{11-4\sqrt7}{3}e^{\pm i\pi/3},\qquad A_{\pm}=(11+4\sqrt7)e^{\pm i\pi /6}.""$$ I tried to replicate his method but even after doing messy algebra, I couldn't figure out how to manipulate the inside of my logarithm such that it looked like what he did. I also tried letting $\operatorname{arg}\left(1+i\left(\frac{1+2\cos x}{\sqrt{3}}\right)\right) \in \left(-\pi/2, \pi/2\right)$ , if that helps. (Another method I tried was noticing that the original integral's function is periodic, so I tried using residue theory by letting $z=e^{ix}$ , but I wasn't able to calculate the residues.) (Question) Can someone help me approach this integral (preferably finding a closed form)? Any methods are absolutely welcome. And if someone could figure out how to get my logarithms to look like $\ln{\left(1+r^2-2r\cos{(x)}\right)}$ , that would be nice. (Edit) After using @SangchulLee's integral, $$ \int_{0}^{\pi} \arctan(a + b\cos\theta) \, d\theta = \pi \arg \left(1 + ia + \sqrt{b^2 + (1+ia)^2}\right), $$ found here , I was able to deduce that $$\int_{0}^{2\pi}\arctan\left(\frac{1+2\cos x}{\sqrt{3}}\right)dx\ =\ 2\pi\operatorname{arccot}\left(\sqrt{3+2\sqrt{3}}\right).$$ I still have no idea how they proved it though.","['integration', 'definite-integrals', 'complex-analysis', 'calculus', 'complex-numbers']"
4535843,How to reasonably (numerically) estimate $\int_0^1 (1 - x) \sqrt{2\over\pi} e^{x^2/2}dx$?,"As the question suggests, I came across the following integral in a calculation: $$\int_0^1 (1 - x) \sqrt{2\over\pi} e^{x^2/2}dx.$$ According to Wolfram Alpha, this equals $$\text{erfi}\left({1\over{\sqrt{2}}}\right) - (\sqrt{e} - 1)\sqrt{2\over\pi} \approx 0.435834.$$ However, I'm wondering if anyone can give a reasonable numerical estimate for this integral from first principles (pencil and paper) without using a calculator or Wolfram Alpha. I've tried but made little to no progress, and two PhD students I consulted didn't know either, so I'm asking here.","['estimation', 'normal-distribution', 'real-analysis', 'probability', 'random-variables']"
4535886,Express the Induced Curvature Forms on the Pullback Bundle through the connection forms,"This question came up while reading Differential Geometry by Loring W.Tu. Suppose $\pi: E \to M$ is a $\mathcal{C}^{\infty}$ vector bundle. And $f:N \to M$ a $\mathcal{C}^{\infty}$ map of manifolds. Let $\nabla$ be a connection on $E$ with connection matrix $\omega_e$ relative to a local frame $e$ for $E$ . Then there is a unique connection $\nabla'$ on the pullback bundle $f^{*}E \to N$ whose connection matrix relative to the frame $f^{*}e$ is $f^{*}(\omega_e)$ . That makes sense so far. The author then writes:
The induced curvature form on $f^{*}E$ is therefore $f^{*} \Omega_e = f^{*} \omega_e + \frac{1}{2} [f^{*} \omega_e, f^{*} \omega_e]$ . Now I don't understand why the induced curvature forms are $f^{*} \Omega_e$ or why this formula holds.","['riemannian-geometry', 'connections', 'curvature', 'pullback', 'differential-geometry']"
4535926,"How to minimize a determinant, without a computer?","I was asked to find the minimum value of a determinant of order $3\times 3$ having elements $-1$ and $1$ , and after trial and  error, was able to come up with this $$
\begin{vmatrix}
-1 & 1 & -1 \\
 1& 1 & 1 \\
-1 & - 1& 1 \\
\end{vmatrix}
= -4$$ However, this seems like a brutalist approach, and was looking for simpler methods. I have come across answers like this on maximizing and minimising a 3 by 3 matrix , however, as this is for school, I can't use computers for more complicated problems","['matrices', 'maxima-minima', 'determinant', 'linear-algebra']"
4535963,Problem in understanding sigma algebra in infinite product space,"$\textbf{(1)}$ Okay, So I understand the concept of product measure when we have two measure spaces, let's suppose $(\Omega_1, F_1,\mu_1)$ and $(\Omega_2,F_2,\mu_2)$ then we defined $(\Omega,F,\mu)$ where $\Omega=\Omega_1\times\Omega_2$ , $F=F_1\times F_2$ and $\mu=\mu_1\times \mu_2$ . what we did here is that we proved that $F_1\times F_2$ is a semi-algebra and then we defined a set function $\mu: F_1\times F_2\to[0,\infty]$ and then we proved that it is countably additive hence it is a measure and then using caratheodory extension theorem we extended this to sigma-algebra generated by $F_1\times F_2$ which is $\sigma(F_1\times F_2)$ . But I am stuck in infinite product space $\textbf{(2)}$ Let us have $(\Omega_i, F_i,\mu_i),i\in I$ be a collection of measure space.If I follow the same steps as above in the case of two measure spaces then $\Omega=\Pi_{i\in I}\Omega_i$ , My problem is in the case of two measure space I was able to visualize that with the help of rectangles but in the infinite case, I am finding it difficult to visualize it. The book I am reading they have defined the sigma-algebra on $\Omega$ as follows Let $\pi_{i}:\Omega\to \Omega_{i}$ be a projection map then the sigma-algebra $F$ is the smallest sigma-algebra such that $\pi_{i}$ is measurable for all $i\in I$ and $F$ = $\sigma\{\pi_{i}^{-1}(E_i):E_i \in F_i,i\in I\}$ My question: Can someone explain to me why we have defined the sigma-algebra like this in the infinite case and how can I connect this with the sigma-algebra generated in the case when we had two measure space as we have defined in $\textbf{(2)}$ , also if possible suggest some books where I can find this topic to understand it in a better way.Thanks","['measure-theory', 'probability-theory', 'real-analysis']"
4535966,What is the formal definition of the breakdown value of a statistic,"On page 482 of Statistical Inference (Second Edition) by Casella & Berger, the authors define the breakdown value as follows: Defintion 10.2.2 Let $X_{(1)} < \dots < X_{(n)} $ be an ordered sample of size $n$ , and let $T_n$ be a statistic based on the sample. $T_n$ has a breakdown value $b$ , $0 \leq b \leq 1$ , if, for every $\epsilon > 0$ , $\lim_{X_{(\{(1-b)n\})} \rightarrow \infty} T_n < \infty$ and $\lim_{X_{(\{(1-(b+\epsilon))n\})} \rightarrow \infty} T_n = \infty$ where the round brackets $\{\cdot \}$ indicate rounding to the closest integer. Now on the next page Casella & Berger state that the breakdown value of the mean is $0$ , which is generally accepted, I think. But if I apply the definition, both of the limits would go to infinity, would they not? I would appreciate if anybody could point out my error in understanding or provide a different formal definition. I am aware that the breakdown value is the proportion of the sample that can be changed without changing the statistic (very generally speaking).","['robust-statistics', 'statistics', 'probability-theory', 'probability']"
4536000,What is a closed form of this limit? (product of areas in circle with parabolas),"I am looking for a closed form of $L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \left(\left(6+\frac{4n}{\pi}\left(\sin{\frac{\pi}{2n}}\right)\left(\cos{\frac{(2k-1)\pi}{2n}}\right)\right)^2-\left(\frac{32n}{\pi}\left(\sin{\frac{\pi}{4n}}\right)\left(\cos{\frac{(2k-1)\pi}{4n}}\right)\right)^2\right)$ The limit seems to exist. Calling the product $P_n$ , we have $P_1\approx10.062$ $P_{10}\approx18.409$ $P_{100}\approx19.788$ $P_{200}\approx19.869$ $P_{300}\approx19.896$ $P_{400}\approx19.909$ $P_{500}\approx19.917$ $P_{570}\approx19.919$ (Then desmos stops working.) Context I came up with the following question. $4n$ $(n\in\mathbb{Z}^+)$ points are uniformly distributed around a circle. $2n$ parabolas are drawn in the manner shown below with example $n=3$ . The parabolas share a vertex, that is one of the designated points, and a line of symmetry. Each parabola passes through two more of the designated points on the circle. One of the parabolas is degenerate and looks like a line segment. Assume that, for every value of $n$ , the average (arithmetic mean) area of the $4n$ regions is exactly $6$ . Find $\lim\limits_{n\to\infty}(\text{product of areas})$ . I have worked out that the answer is $L^2$ where $L$ is the limit shown above. So I am looking for a closed form of $L$ . If the average area is $5.999$ the product approaches $0$ ; if the average area is $6.001$ the product diverges. I have tested my expression for $P_{n}$ with some small values of $n$ , and I think it is correct. This question was inspired by other questions about the product of areas in a circle, such as this , this and this . I have tried to apply the techniques from the answers to those questions, to no avail. For the limits in those other questions, the closed forms (if not $1$ ) involve $\text{cosh}$ , so I would not be surprised if the closed form of $L$ in this question also involves cosh. Maybe $L=2\cosh{\left(\pi e^{\frac{1}{\pi}-\frac{1}{e}}\right)}\approx19.929$ ? UPDATE: From @Claude Leibovici's answer, it seems that $\left(\dfrac{P_{4n}}{P_{2n}}-1\right)\to\dfrac{1}{2}\left(\dfrac{P_{2n}}{P_{n}}-1\right)$ as $n\to\infty$ . Then based on his value of $P_{60000}$ , I project that $L\approx 19.950955$ . UPDATE2: I believe the number of points around the circle does not have to be a multiple of $4$ . Any positive integer will do. If the average area of the regions is $6$ , then $\lim\limits_{n\to\infty}(\text{product of areas})=L^2$ .","['area', 'circles', 'closed-form', 'infinite-product', 'limits']"
4536083,Where does $\ln x$ come from here?,"There's a simple formula for the antiderivative of any function of the form $x^n$ ; it's $\frac{x^{n+1}}{n+1}$ . But in the case of $n=-1$ , and only in that case, you get $\ln x$ instead. Even if you go really close to $-1$ , for instance $n=-0.9999$ , you still get $\frac{x^{0.0001}}{0.0001}$ , not anything logarithm-y. It makes sense that the formula would break, because the formula would then spit out $\frac{x^0}{0}=\frac{1}{0}$ , and of course you can't divide by zero. But why specifically $\ln x$ ? I understand that it can be derived from the standard Riemann sum definition of an integral; that's not what I'm looking for, I could do that myself. I want an intuitive understanding of why you would get anything well-defined at all here where the formula spits out a division by zero, let alone why it would be specifically a nice function like $\ln x$ as opposed to some horrible mess.","['integration', 'calculus']"
4536114,Proving Rate of Convergence,"I am investigating the following coupled sequence: \begin{align*}
y_0 &= 1\\
x_{n+1} &= \sqrt{1 + \frac{1}{y_n}}\\
y_{n+1} &= \sqrt{1 - \frac{1}{x_{n+1}}}\\ 
\end{align*} I am trying to show \begin{align*}
\lim_{n \to \infty} \frac{\left |x_{n+1} - \varphi\right |}{\left |x_n - \varphi\right |} &= \frac{1}{4}\\
\lim_{n \to \infty} \frac{\left |y_{n+1} - \varphi^{-1}\right |}{\left |y_n - \varphi^{-1}\right |} &= \frac{1}{4}
\end{align*} where $\varphi = \frac{-1 + \sqrt{5}}{2}$ . So far, I've shown: \begin{align*}
        x_{n+1}^2 - \varphi^2 &= 1 + \frac{1}{y_n} - \varphi^2\\
        &= \sqrt{1 + \frac{1}{x_n - 1}} - \varphi\\
        &= \frac{1 + \frac{1}{x_n - 1} - \varphi^2}{\sqrt{1 + \frac{1}{x_n - 1}} + \varphi}\\
        &= \frac{\frac{1}{x_n - 1} - \frac{1}{\varphi - 1}}{\sqrt{1 + \frac{1}{x_n - 1}} + \varphi}\\
        &= \frac{\varphi - x_n}{\left (\varphi - 1\right )\left (\sqrt{x_n^2 - x_n} + \varphi\left(x_n - 1\right ) \right )}\\
        \frac{x_{n+1} - \varphi}{\varphi - x_n} &= \frac{\varphi}{\left (x_{n+1} + \varphi\right )\left (\sqrt{x_n^2 - x_n} + \varphi\left(x_n - 1\right )\right )}\\
    \end{align*} This result (which is true when the limits are substituted in) could be promising but I've come up on to a dead-end it seems. Any help would be great.","['nested-radicals', 'continued-fractions', 'sequences-and-series', 'real-analysis']"
4536122,"Why is it assumed that the mode, in a grouped frequency distribution, lies in the modal class(the class with maximum frequency)?","I am in 10th standard. We are given a formula to find the mode mode = l +[(f1-f0)/(2f1-f0-f2)]h
Where l is the lower limit, F1 the modal class i.e the class with most frequencies, f0 the frequency of class preceding the modal class, F2 the frequency of class succeeding the modal class. Here it is said that it the mode lies in the modal class. But why? Why do we assume that the mode lies in the modal class, even if the actual mode maybe far off.","['central-tendency', 'statistics']"
4536160,Monty's choice of door to open when the car is behind the door initially chosen by the contestant,"In the Monty Hall game, suppose that whenever the car is behind the door initially chosen by the contestant (so that Monty Hall may open either one of the remaining two doors), he chooses to open the door labeled with the smaller number. Is the probability of winning upon switching still 2/3 or is it 1/2? My considerations suggest that not only is it still 2/3, but also it is 2/3 no matter how Monty chooses a door to open when he does have a choice. Am I wrong? If computer simulations (Monte Carlo) confirm or contradict the assertion that it is still 2/3, I would like to know. I am not in a position to carry out simulations myself. Available applets demonstrate the 2/3 probability but they may be programmed to make Monty choose either door with probability 1/2.","['monty-hall', 'recreational-mathematics', 'discrete-mathematics', 'probability']"
4536180,Sums of independent random variables cannot accumulate at a point,"Suppose that we have independent random variables $X_1, X_2, \ldots$ , each of which has finite mean. I am interested in the condition $$\underset{n\to\infty}{\lim} \sup_{x\in \mathbb{R}} \mathbb{P}\Bigl(\sum_{i=1}^n X_i=x\Bigr)=0. \tag{A}$$ What are some necessary conditions which guarantee that condition $(A)$ holds? What are some sufficient conditions which guarantee that condition $(A)$ holds? What are some necessary conditions which guarantee that condition $(A)$ holds, under the additional hypothesis of identical distributions? My hope is that iid and non-constant would give $(A)$ . What are some sufficient conditions that guarantee that condition $(A)$ holds, under the additional hypothesis of identical distributions? The low hanging fruit here is the assumption of finite variance, since then CLT gives the conclusion. I'm hoping for something more general which could apply under the assumption of finite first moments, without necessarily assuming finite $1+\delta$ moments for any $\delta>0$ .","['statistics', 'probability-theory', 'probability']"
4536189,A (fake) proof that $\limsup\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|>1\Rightarrow \sum_{n=m}^{\infty}a_n\ \text{ diverges}$,"Let $(a_n)_{n=m}^{\infty}$ be a sequence of nonzero real numbers.
Then we know that the implication $$\limsup\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|>1\Rightarrow \sum_{n=m}^{\infty}a_n\ \text{ diverges}$$ is false
(a counterexample is the sequence $(a_n)_{n\geq 1}:=\begin{cases}\frac{1}{n^2} & \text{if } n \text{ is even }\\ \frac{2}{n^2} & \text{if }n \text{ odd}\end{cases}$ which has $\limsup\limits_{n\to\infty} |\frac{a_{n+1}}{a_n}|=2>1$ but nonetheless converges (as can be easily seen by using Comparison Test with the series $\sum_{n=1}^{\infty}\frac{2}{n^2}$ ). This notwithstanding I haven't been able to see where the following proof of the false statement, which I had come up with before stumbling upon the counterexample I described above, is wrong, so I would appreciate if someone could point it out to me. Thanks. Let $L=\limsup\limits_{n\to\infty} \left| \frac{a_{n+1}}{a_n} \right|$ ; then there exists a subsequence $\left(b_k\right)_{k\in\mathbb{N}}=\left(\left|\frac{a_{n_k+1}}{a_{n_k}}\right|\right)_{k\in\mathbb{N}}$ such that $\lim\limits_{k\to\infty}b_k=L$ so if we take $\varepsilon:=\frac{L-1}{2}$ we have that there exists $K\in\mathbb{N}$ such that $|b_k-L|\leq\varepsilon$ for every $k\geq K$ hence $b_k\geq\frac{L+1}{2}>1$ for every $k\geq K.$ So if $k>K$ we have $|a_{n_k}|=\left|\frac{a_{n_k}}{a_{n_{k-1}}}\right|\cdot \left|\frac{a_{n_{k-1}}}{a_{n_{k-2}}}\right| \cdot \dots \cdot \left|\frac{a_{n_{K+1}}}{a_{n_K}} \right|\cdot |a_{n_K}|>\left(\frac{L+1}{2}\right)^{k-K}|a_{n_K}|=A\left(\frac{L+1}{2}\right)^k$ , where $A:=\left(\frac{L+1}{2}\right)^{-K}|a_{n_K}|$ which implies that $\lim\limits_{k\to\infty}|a_{n_k}|\geq\lim\limits_{k\to\infty} A\left(\frac{L+1}{2}\right)^k=+\infty$ i.e. $\lim\limits_{k\to\infty}|a_{n_k}|=+\infty$ hence $\lim\limits_{k\to\infty} |a_{n_k}|\neq 0$ thus $\lim\limits_{n\to\infty}a_n\neq 0$ and therefore the series $\sum\limits_{n=0}^{\infty}a_n$ cannot converge, as desired. $\square$","['limsup-and-liminf', 'sequences-and-series', 'fake-proofs', 'real-analysis']"
4536191,Three different results for $\frac{\partial}{\partial p}\left((-\gamma+1)(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1})\right)^{1/(-\gamma+1)}$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question I need to differentiate this equation wrt. $p$ : $$\frac{\partial}{\partial p}\left((-\gamma+1)\left(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1}\right)\right)^{1/(-\gamma+1)}$$ See problem as image My supervisor gets this result: $$(-γ+1)^\frac{γ}{-γ+1}\left(v+e^{ω}\frac{p^{ε+1}}{ε+1}\right)^{\frac{γ}{-γ+1}}e^{ω}p^{ε+1}$$ See supervisor's result as image Wolfram Alpha gets this result: See Wolfram Alphas result as image I get this result: $$\left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1}$$ See my result as image Here are my calculations: Steps:
1: Split up all parentheses (multiple inside and then add the potency of the outside parenthesis) $$\frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}}+(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right)$$ 2: Remove $(-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}}$ as it is a constant $$\frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right)$$ 3: Use rule of derivatives (derivative wrt x: $x^n = nx^{n-1}$ ) $$\left(\frac{ε+1}{-γ+1}\right)(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}}p^{\frac{ε+1}{-γ+1}-1}$$ 4: Convert the first fraction to factors by using potency rules and use the new form to multiply similar term's potences and isolate the part of the p that doesn't have the same potency in common (sorry if some terms sound strange, I didn't learn math in English) $$(-γ+1)^{\frac{1}{-γ+1}-1}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}-1}p^{\frac{1}{-γ+1}-1}p^{ε+1}$$ 5: Put all with same potency under same potency $$\left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1}$$ Which one is right? What did I do wrong? Any pointers to what method I should be using? See calculations as image","['problem-solving', 'calculus', 'derivatives', 'algebra-precalculus']"
4536193,Rough path expected signature vs cumulant-generating function / characteristic function,"What is the point of using rough path expected signature to characterize the law of а stochastic process when the cumulant generating function is known ( $\log\mathbb{E}[e^{i\theta X(t)}]$ )? Since an average of group-like elements is not a group-like element, an expected signature (of a sample of paths) is not a signature, which seems to imply that expected signatures do not enjoy a nice geometric interpretation as signatures do. It seems that two main applications of expected signatures are the characterization of the law of a process (if moments completely describe the distribution) and the approximation of continuous functions on a collection of paths as linear functions on the expected signature of that collection of paths. If one doesn't need the latter use case, how is the description of a stochastic processes in terms of expected signatures qualitatively different from the classic characteristic function description? Is there something else that expected signatures can tell besides the moments? For example, in case of Levy processes, characteristic function of a process has some very useful information about the process besides moments in light of Levy–Khintchine representation. It tells what parts this Levy process is composed of, namely its linear drift, Brownian motion, and Lévy jump process. By expected signature I mean $ \Big( \mathbb{E} \Big[ \int dX^{\otimes n} \Big]  \Big)_{n\geq 0} = \Big( \mathbb{E} \Big[\int_0^T \int_0^{r_n} \cdots \int_0^{r_2} dX_{r_1} \otimes \cdots  \otimes dX_{r_n} \Big]\Big)_{n\geq 0} $ described in [LM22] (Chapters 6 and 13), [CO22] , [LSDBL21] , [BDMN20] . END OF THE ORIGINAL POST EDIT: Reply to JeremyFR Thank you, @JeremyFR, for your response! However, I still struggle to see advantages of expected signatures vis-à-vis CGF when applied to common 1-D Levy processes. Let us start with Brownian motion. Let B be a standard one-dimensional Brownian motion calculated up to some fixed time T > 0. Then B has the following expected signature (Fawcett’s formula): \begin{align*}
	\mathbb{E}[S(B_{[0,T]})] &= \exp(\frac{T}{2}  e_i \otimes e_i ) \\
       &= 1+ \frac{T}{2} e_i ^ {\otimes 2}   +    \frac{1}{2!} \frac{T^2}{4} e_i ^ {\otimes 4}  + \dots 
\end{align*} It is also known that it’s moment-generating function is \begin{align*}
	M_{B_T}(\theta) &= \exp( \frac{T}{2}  \theta^2) \\
		             &= 1 + \frac{T}{2} \theta^2 + \frac{1}{2!} \frac{T^2}{4} \theta^4 + \dots
\end{align*} It seems that both expressions carry identical information once we take apart the notation. In the case of Brownian motion, it seems that one hardly can argue that one approach is better than the other because the results are semantically equal. Now let’s move on to a more general case of d-dimensional Lévy process with triplet (a, b, K). It is shown in [FS14] (Theorem 43), that the expected signature of such process is awfully similar to Lévy–Kintchine formula, \begin{align*}
\mathbb{E}[S(X)_{[0,T]}]  =\exp\Big [  T \Big (b + \frac{1}{2} a + \int (exp(y) – 1 – y 1 _{|y| < 1}) K(dy) \Big ) \Big] \in T((\mathbb{R}^d)).
\end{align*} In light of this result, one cannot help but wonder what is the point of using expected signatures for time series analysis from the practitioner’s perspective in applications involving common types of time series, which are usually 1D Levy Processes, that frequently occur in finance and energy / load research, given that the information in expected signature is essentially identical to the one provided by characteristic function? Granted, there are more exotic cases that require more general expression for expected signature [FHT21] but that hardly looks like something arising frequently in day-to-day applications.","['stochastic-processes', 'statistics', 'rough-path-theory', 'probability']"
4536223,"What is the ""Natural"" Product Measure on $\mathbf{R}^n$?","It seems like most of the times when we talk about product measures $\mathbf{R}^n$ , we are talking about Lebesgue measure on $\mathbf{R}^n$ . I will denote this measure space as $(\mathbf{R}^n, \mathcal{L}(\mathbf{R}^n), m_{\mathbf{R}^n})$ . On the other hand, one may consider the canonical product measure on $\mathbf{R}^n$ defined as product of the measure spaces $(\mathbf{R}, \mathcal{L}(\mathbf{R}), m_{\mathbf{R}})$ , namely, the measure space $(\mathbf{R}^n, \otimes_n \mathcal{L}(\mathbf{R}), \pi)$ , where $\pi: \otimes_{n} \mathcal{L}(\mathbf{R}) \to [0, +\infty]$ is the product measure extended by the Catheodory Extension Theorem from the premeasure $\pi_0: \{ \bigcup_{finite} A_i \times B_i: A_i, B_i \in \mathcal{L}(\mathbf{R}) \} \to [0, +\infty]$ . At the beginning I thought it would make sense that we have $(\mathbf{R}^n, \otimes_n \mathcal{L}(\mathbf{R}), \pi) = (\mathbf{R}^n, \mathcal{L}(\mathbf{R}^n), m_{\mathbf{R}^n})$ . However, I have not yet succeeded in proving the equality. In particular, I am stuck at showing $$
\otimes_n \mathcal{L}(\mathbf{R}) = \mathcal{L}(\mathbf{R}^n).
$$ We can consider this in the case where $n = 2$ and conclude by induction. Now I believe it is true that $$
\mathcal{L}(\mathbf{R}) \otimes \mathcal{L}(\mathbf{R}) \subseteq \mathcal{L}(\mathbf{R^2})
$$ since the cartesian product of two Lebesgue measurable sets are measurable: Is the product of two measurable subsets of $R^d$ measurable in $R^{2d}$? . I am not so sure about the converse inclusion: $$
\mathcal{L}(\mathbf{R}^2) \subseteq \mathcal{L}(\mathbf{R}) \otimes \mathcal{L}(\mathbf{R})
$$ Perhaps we need to argue through Borel $\sigma$ -algebra and talk about the completion of measure spaces? Or is there a $\lambda$ - $\pi$ system argument here? In fact, I fear this might not even be true. If this is not true, why do we introduce product measure as it is? What would be the motivation?","['measure-theory', 'lebesgue-measure', 'real-analysis', 'solution-verification', 'product-measure']"
4536297,Do any exotic smooth structures on $\mathbb R^n$ leave addition and negation smooth?,"Let $V$ be a finite-dimensional real vector space with the standard topology. We can equip $V$ with the ""standard smooth structure,"" which includes as charts every linear isomorphism $\phi:V\to\mathbb R^n$ . With this smooth structure, $V$ becomes a Lie group because $(x, y)\mapsto x+y$ and $x\mapsto -x$ are smooth. Do any nonstandard/exotic smooth structures on $V$ also make these operations smooth? I'm wondering if perhaps the solution to Hilbert's fifth problem (or more basic principles) rule this out, but I'm too much of a novice in this area to understand what those results entail. There are also some other posts regarding the relationship between exotic smooth structures and Lie groups [1] [2] [3] , but again I'm too much of a novice to see whether those questions are directly relevant to this one.","['topological-groups', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4536308,A deck of $52$ cards is divided into $13$ piles of $4$ cards each. [duplicate],"This question already has an answer here : An application of Hall's marriage theorem. (1 answer) Closed 1 year ago . I have the following question in a Discrete Mathematics course I am taking. We are currently studying Intro to Graph Theory. A standard deck of $52$ cards (no Jokers) is divided into $13$ pile of $4$ cards. Prove that by choosing one card from each pile you can choose all of the numbers ( $2$ - $10$ , Ace, K, Q, J). I thought of making a graph $G=(V,E)$ such that $V$ is the group of all cards and that $\forall u,v
\in V\colon \{a,b\}\in E \iff a\text{ and }b \text{ have the same number or are in the same pile.}$ I can show that if you a connected component exists that has only $4$ vertices than they are all of the same number, and we can just pick a card from that pile of cards, no matter which. It is also pretty simple to show that a connected component of more than one pile, containing $4k$ for $1<k\le 13$ , it must contain $k$ cards with different numbers. I am struggling to show that for a connected component containing more than one pile, you can choose all different of $k$ cards with different numbers. Thanks in advance.","['graph-theory', 'discrete-mathematics']"
4536311,Construct an Injective and onto unbounded operator.,"i was study functional analysis and i found a interesting problem. Let $X$ an infinite dimensional normed space. Construct an operator $T: X \rightarrow X$ such that $T$ is injective and onto. Also, we can define a new norm on $X$ given by $\| x \|_{T} = \| T(x) \| $ . Prove that $(X,\|  \cdot\|)$ , $(X,\| \cdot \|_{T})$ are isometric. My attemp:
I was trying to define the operator $T$ in the usual form of the books, take a Hamel Basis $(x_{i})_{i \in I}$ ,and WLOG suppose $\|x_{i}\|=1$ for any $i \in I$ then take countable subsequence $J = \{j_{1},j_{2},\dots\}$ of $I$ . And define then as follows: $T(x_{ji}) = ix_{ji}$ , and $T(x_{i})=0$ if $i \in I-J$ . Then we can extend the function and we see that is unbouded. The problem is that, in this way $T$ is neither injective nor onto. At this point i do not how to redefine the function to be bijective.
Any help is welcome, thanks!","['operator-theory', 'normed-spaces', 'analysis', 'functional-analysis', 'unbounded-operators']"
4536328,combinatorics: deck of cards and suits,"I feel confused about selecting suits and cards that each suit corresponds to. Suppose we have a standard deck of cards, and we want to form a 6 hands with at most 3 suits. I understand this needs to be spliced into 3 cases to discuss, which are 1 suit case, 2 suits case, and 3 suits case. But I feel confused when it comes to select cards. For example, I am not sure if I understand this correctly, but I feel like if we only need to form cards with a single suit, then this can be done by $4(13 * 12 * 11 * 10 * 9 * 8)$ ways, since each suit has 13 cards, and we have 4 suits here. And when it comes to the 2 suits case, I feel like we only need to draw from the total of 26 cards to form such hands, and there are 6 ways to do this, but I am not sure if my understanding about the 26 cards is right or what. And for the 3 suits case, I feel like it's just choose 3 suits out of 4, and draw cards from total of 39 cards. Is my understanding towards the total number of cards in each case correct? If not, why so?","['statistics', 'combinatorics', 'card-games']"
4536334,"Vector that maximizes $(x^TAx)(x^TBx)$ subject to $x^Tx=1$ ? (A,B symmetric, positive definite)","I've been playing with eigenvector-type optimisation problems where a vector maximally projects onto two matrices. The sum version of this problem (maximize $x^TAx + x^TBx$ ) is fairly straightforward (leading eigenvector of $A+B$ ), but the product version in the title has completely stumped me. To restate the problem, I aim to find a vector $x$ such that $x^Tx=1$ that maximizes the quantity $(x^TAx)(x^TBx)$ . Here A and B can be taken to be symmetric and positive definite matrices. My attempted solution involved a substitution $p=B^{-1/2}x$ , such that the quantity to be maximized is $(p^TB^{-1/2}AB^{-1/2}p)(p^Tp)$ , but $p^Tp=x^TB^{-1}x$ is unknown, so we are back to square one. I also considered that, as A and B are positive definite, perhaps the vector that maximizes the sum also maximizes the product. This of course doesn't hold though, as either of the terms may be <1. Any pointers would be received gratefully.","['optimization', 'nonlinear-optimization', 'linear-algebra', 'eigenvalues-eigenvectors']"
4536351,Continuity at a point vs. interval—contradicton or not?,"Let $f(x)=\lfloor x \rfloor $ and imagine posing the following questions. Is $f(x)$ continuous at $x=0$ ? Is $f(x)$ continuous on $[0,1)$ ? For the first question, since $\displaystyle \lim_{x\rightarrow 0} f(x)$ does not exist, we must answer no . For the second question, since $\displaystyle \lim_{x\rightarrow 0^+} f(x)$ exists and $\forall a \in (0,1) : \displaystyle \lim_{x\rightarrow a} f(x) = f(a)$ , we should answer yes . These are the answers to these two questions based on my understanding of what it means to be continuous at a point and what it means to be continuous on a(n) (closed) interval. However, in retrospect, this seems bizarre to me given that we are saying that $f(x)$ is not continuous at $0$ , while $f(x)$ is continuous on $[0,1)$ even though $0 \in [0,1)$ . Is this really the case?","['continuity', 'calculus', 'step-function', 'ceiling-and-floor-functions']"
4536373,Question about the definition of the outer measure,"From Rudin's Principles of Mathematical Analysis , we define the outer measure as so: Definition $11.7$ : Let $\mu$ be additive, regular, nonnegative, and finite on $\mathcal{E}$ . Consider countable coverings of any set $E \subset \mathbb{R}^n$ by open elementary sets $A_n$ : $$E \subseteq \bigcup_{n=1}^\infty A_n.$$ Define $$\mu^\ast(E) = \inf \sum_{n=1}^\infty \mu(A_n),$$ the infimum being taken over all countable coverings of $E$ by open elementary sets. $\mu^\ast(E)$ is called the outer measure of $E$ , corresponding to $\mu$ . I apologize beforehand as I know there are a multitude of questions regarding the outer-measure, but I have a specific issue that makes the definition a little unclear to me and I hope someone can help me understand it better. I posted the definition from baby Rudin above for reference. Now my issue is this, it appears we did not put any restrictions on the coverings of $E$ . I am not talking about openness or countability. Rather, the way I am reading the definition is this: As long as $E \subset \bigcup_{n=1}^{\infty} A_n$ , then its outer measure is $$\mu^{*}(E) = \inf \left\{\sum_{n=1}^{\infty} \mu(A_n) \right\} $$ So basically, you can give me a finite cover for a finite compact set $E$ , but then add $\mathbb{R}$ to the union of its covers $\bigcup_{n=1}^{N} A_n$ , $(N < \infty)$ , which is true, then $$\mu^{*}(E) = \inf \left\{\sum_{n=1}^{N} \mu(A_n) + \mu\mathbb{R} \right\} = inf\{\infty \} = \infty$$","['measure-theory', 'lebesgue-measure', 'outer-measure', 'analysis', 'real-analysis']"
4536378,Stuck showing that Milnor's topological intersection multiplicity agrees with the algebraic definition,"I am trying to solve problem 3 from Appendix B from Milnor's “Singular Points of Complex Hypersurfaces” (p. 115). This shows that Milnor's topological definition of intersection multiplicity agrees with another more algebraic one. Let $f = (f_1, \dots, f_n)$ be holomorphic functions with an isolated zero at the origin in $\mathbb C^n$ . Milnor (p. 59) defines the intersection multiplicity $\mu$ of $f_1, \dots, f_n$ at the origin as the degree of the map $$g : S^{2n-1} \to S^{2n-1}, \qquad g(x) = \dfrac {f(tx)} {\Vert f(tx) \Vert}$$ for small enough $t > 0$ . Milnor also shows (p. 113) that, for “most” values $p$ near the origin, the fiber $f^{-1}(p)$ has exactly $\mu$ points. Actually, I am not trying to solve the problem from scratch, but rather heavily reusing a proof in Ebeling's book “Functions of Several Complex Variables and Their Singularities” (pp. 150-151), which, if it goes through, then it solves the problem. By hypothesis, $f_1, \dots, f_n$ form a system of parameters of the local ring $B = \mathbb C \{ x_1, \dots, x_n \}$ . Then $B / \langle f_1, \dots, f_n \rangle$ is a finite-dimensional $\mathbb C$ -vector space, say, of dimension $d$ . The goal is to show that $d = \mu$ . Let $A = \mathbb C \{ f_1, \dots, f_n \}$ . By the Weierstrass preparation theorem for modules (p. 97), $B$ is a finite $A$ -module, minimally generated by $d$ elements. By miracle flatness (p. 109, or also Stacks Project 10.128), $B$ is freely generated by these $d$ elements. In particular, $B$ is an integral extension of $A$ . Let $K = \mathrm{Frac}(A)$ and $L = \mathrm{Frac}(B)$ . Then $L = K(x_1, \dots, x_n)$ is an algebraic extension of $K$ of degree $d$ . By the primitive element, we may write $L = K(x_0)$ , where $x_0$ is a $K$ -linear combination of $x_1, \dots, x_n$ . Then $x_0$ 's minimal polynomial $g_0 \in K[T]$ has degree $d$ . Multiplying $x_0$ times the least common denominator of $g_0$ 's coefficients, we obtain another element $y_1 \in L$ such that $L = K(y_1)$ , but whose minimal polynomial is in $A[T]$ . This implies that $y_1 \in B$ . In fact, $y_1$ is in $B$ 's maximal ideal. So far so good, but then I do not understand the next step. Ebeling claims (p. 151) that, after a change of coordinates, we may assume $x_1 = y_1$ . Equivalently, there exist $y_2, \dots, y_n \in B$ such that $y_1, \dots, y_n$ is a system of parameters that generates the whole maximal ideal $\langle x_1, \dots, x_n \rangle$ . Equivalently, $y_1$ has order $1$ . Why is this the case? Moving on to the rest of the proof. Let $g \in A[T]$ be $y_1$ 's minimal polynomial. Using Hensel's lemma (p. 73), one can show (p. 101) that $g$ 's non-leading coefficients are in $A$ 's maximal ideal. The inclusion ring map $A \to A[y_1]$ corresponds geometrically to the restriction of $$\pi : U \times \mathbb C \longrightarrow U, \qquad (f_1, \dots, f_n, y_1) \longmapsto (f_1, \dots, f_n)$$ to the hypersurface $g = 0$ . Away from the subset of $U$ where the discriminant $\Delta g$ vanishes, the fibers $\pi^{-1}(p)$ have exactly $d$ points on this hypersurface. IMO, Ebeling handwaves the last part, so I am going to try to “unhandwave” it. The inclusion ring map $A[y_1] \to B$ is a normalization map, because $A[y_1]$ and $B$ have the same field of fractions, and $B$ is integrally closed. Moreover, one constructs the normalization $B$ by adding $y_2, \dots, y_n$ , so, if we invert the product $h = y_2 \cdots y_n$ , then the localization $A[y_1]_h \to B_h$ is actually an isomorphism . In other words, geometrically, the restriction of $$\iota : V \longrightarrow U \times \mathbb C, \qquad (y_1, \dots, y_n) \longmapsto (f_1, \dots, f_n, y_1)$$ is “almost an isomorphic embedding” into the hypersurface $g = 0$ , and it actually becomes an isomorphism after we remove the points where $h = 0$ from both the domain and the codomain. Is the highlighted idea right? Finally, the composition $\pi \circ \iota : V \to U$ agrees with the original map $f$ , and it has fibers of size $d$ on an open subset of $U$ whose closure contains the origin. Therefore, $d$ agrees with Milnor's intersection multiplicity $\mu$ . Change of strategy: Instead of considering the ring extension $A \to B$ in one go, I will set $A_j = A[x_1, \dots, x_j]$ and consider the tower of extensions $$A = A_0 \longrightarrow A_1 \longrightarrow A_2 \longrightarrow \dots \longrightarrow A_n = B$$ Similarly, I will set $K_j = \mathrm{Frac}(A_j) = K(x_1, \dots, x_j)$ and consider the tower of extensions $$K = K_0 \longrightarrow K_1 \longrightarrow K_2 \longrightarrow \dots \longrightarrow K_n = L$$ By construction, $K_j = K_{j-1}(x_j)$ , hence $x_j$ 's minimal polynomial $g_j \in K_{j-1}[T]$ has degree $d_j = [K_j : K_{j-1}]$ . Geometrically, the inclusion $A_{j-1} \to A_j$ corresponds to the restriction of $$(f_1, \dots, f_n, x_1, \dots, x_j) \longmapsto (f_1, \dots, f_n, x_1, \dots, x_{j-1})$$ to the zero locus of $g_1 = \dots g_j = 0$ . Away from the subset of the codomain where the discriminant $\Delta g_j$ vanishes, the fiber over each point has exactly $d_j$ points. Therefore, the general fiber of $$(f_1, \dots, f_n, x_1, \dots, x_n) \longmapsto (f_1, \dots, f_n)$$ has exactly $\mu = d_1 \cdots d_n$ points. Finally, the map $$(x_1, \dots, x_n) \longmapsto (f_1, \dots, f_n, x_1, \dots, x_n)$$ is an isomorphism, because it is a parametrization of the graph of $f(x)$ , so we are done. Does this idea work?","['local-rings', 'complex-geometry', 'algebraic-geometry', 'power-series', 'commutative-algebra']"
4536382,Is $\frac{1}{6}\ln(x+4)^6$ equivalent to $\ln(x+4)$?,"I was doing a question and was wondering if bringing the coefficient back into the exponent was allowed? So, for example: $$\frac{1}{6}\ln(x+4)^6$$ Is it possible for me to do: $$\ln(x+4)$$ Are those two equivalent? I am struggling on what to do because I am doing a question where it is asking me to simplify to a single logarithm: $$\frac{1}{6}\ln(x+4)^6 + \frac{1}{4}[\ln x-\ln(x^2+6x+8)^4]$$ If I can put them back into the exponent area, then I could do something such as: $$\ln(x+4)+\ln x^\frac{1}{4}-\ln(x^2+6x+8)$$ After that, I could apply the basic adding and subtracting rules to get: $$\ln\left(\frac{(x+4)x^\frac{1}{4}}{x^2+6x+8}\right)$$ Finally, I could factor the bottom and reduce the numerator and denominator: $$\ln\left(\frac{(x+4)x^\frac{1}{4}}{(x+4)(x+2)}\right)$$ $$\ln\left(\frac{x^\frac{1}{4}}{x+2}\right)$$ Is this way of thinking right or am I not allowed to put the exponent back into the exponent area?","['algebra-precalculus', 'exponential-function', 'logarithms']"
4536427,How to solve this logarithm problem,If $x$ and $y$ are positive real numbers and $x^{\log _yx}=2$ And $y^{\log _xy}=16$ Find $\log _yx$ and find $x$ . The way I tried to solve it was to take $\log _x$ on the base $x$ one and $\log _y$ for the $y$ one. Then something really messy came up. I got that $\log _yx=\sqrt[3]{\dfrac{1}{4}}$ but I don't know if it's correct or not.,"['algebra-precalculus', 'logarithms']"
4536439,Roll two balanced dice until the sum of the faces equals 7 appears for the first time. Determine the expected value of tosses in this experiment.,"Roll two balanced dice until the sum of the faces equals 7 appears for the first time. After that, roll the same two dice until some face 3 appears for the first time. Determine the expected value of tosses in this experiment. All the possibilities are bellow: (1,1)(2,1)(3,1)(4,1)(5,1)(6,1) (1,2)(2,2)(3,2)(4,2)(5,2)(6,2) (1,3)(2,3)(3,3)(4,3)(5,3)(6,3) (1,4)(2,4)(3,4)(4,4)(5,4)(6,4) (1,5)(2,5)(3,5)(4,5)(5,5)(6,5) (1,6)(2,6)(3,6)(4,6)(5,6)(6,6)\ So, there are 6 possibilities for the sum to be 7. So the probability is $6\over36$ $=$ $1 \over 6$ .\ And the probability of face three is going to be $11 \over 36$ . But how do I determine the number of expected tosses?","['probability-theory', 'probability']"
4536447,Mathematical Logic: Having trouble understanding this inference,"So here I wanted for fun to infer that: $$(\lvert a\rvert<\varepsilon) \rightarrow (a > -\varepsilon)
\land (a < \varepsilon)$$ In other words, if it is true that the absolute value of $a$ is less than $\varepsilon$ , then $a$ is in between $-\varepsilon$ and $\varepsilon$ . So I started by the definition of $\lvert a \rvert$ . Which says that: $$if\; a > 0 \; then \; \lvert a\rvert=a$$ $$if\; a < 0 \; then \; \lvert a\rvert=-a$$ Which I know them to be true (by definition). So then I do the following logical steps: $$suppose\;\lvert a\rvert<\varepsilon$$ $$a>0 \rightarrow \;a<\varepsilon$$ $$a<0 \rightarrow \;-a<\varepsilon$$ from here I am literally stuck. I do not know how to infer/conclude that $a > -\varepsilon 
\land a < \varepsilon $ . Where I get confused is in here: If I suppose that $a$ is greater than $0$ , then I get that $a<\varepsilon$ . If I also suppose that $a$ is less than $0$ , then I get $-a<\varepsilon$ or $a>-\varepsilon$ . But how can $a$ be both positive and negative? In other words how can this statement: $a>0\,\land\;a<0$ be true? I want them to be both true so that through modus ponens I infer $a > -\varepsilon 
\land a < \varepsilon $ . So long story short, I'd like to see this proof done by only involving logical steps.",['discrete-mathematics']
4536454,"Potential solution to Sloane's ""Powers of 2"" problem?","Neil Sloane gave a problem in a recent Numberphile video here . It seems like there's a solution. But Sloane said it's unsolved and very hard so maybe not. But I'll try to outline the idea for the solution and I'm hoping to know: would an approach along these lines work? The problem is to find four distinct integers, $a,b,c,d\in \mathbb{Z}$ such that as many pair-wise sums as possible are powers of two. For instance $a,b,c,d=-1,3,5,11$ gives four such pair-wise sums $$-1+3=2\quad 3+5=8\quad -1+5=4\quad 5+11=16 $$ out of a possible six (the powers of two themselves don't need to be distinct). Sloane said four sums is the best if $a,b,c,d$ are bounded but that five sums (though highly suspected impossible) was not disproven for the unbounded case. Here's an approach that would seem to be capable of proving five sums impossible. Firstly, five sums would imply that some three of $a,b,c,d$ give powers of two for all pair-wise sums (since a graph on 4 elements with 5 distinct edges has at least one 3-clique). Without loss of generality we can suppose $$a+b=2^p\quad a+c=2^q\quad b+c=2^r$$ for non-negative integers $p,q,r\in\mathbb{N}$ . But this also gives us linear expressions for $a,b,$ and $c$ in terms of powers of two: $$2^p+2^q-2^r=2a\quad 2^p-2^q+2^r=2b\quad -2^p+2^q+2^r=2c$$ And we can divide through by $2$ getting $a,b,$ and $c$ alone $$a=2^{p-1}+2^{q-1}-2^{r-1}\quad b=2^{p-1}-2^{q-1}+2^{r-1}\quad c=-2^{p-1}+2^{q-1}+2^{r-1}$$ Any distinct choice of $p,q,r$ gives us three of the five desired pair-wise sums. To achieve five, we need $d$ to sum to a power of 2 with two of $a,b,$ and $c$ . Without loss of generality, say with $a$ and $b$ : $$a+d=2^{p-1}+2^{q-1}-2^{r-1}+d=2^m$$ $$b+d=2^{p-1}-2^{q-1}+2^{r-1}+d=2^n$$ And rearranging to get $d$ alone, we get an equality of powers of two: $$d=2^m-2^{p-1}-2^{q-1}+2^{r-1}=2^n-2^{p-1}+2^{q-1}-2^{r-1}$$ Cancelling terms (like $-2^{p-1}$ ) and rearranging again, we get $$2^m+2^r=2^n+2^q$$ This, of course, implies that $\{m,r\}=\{n,q\}$ . This is probably the jump in logic where this approach fails. The intuition is that we can think of a sum of powers of two like a binary number, with the exponents indicating the indices of the $1$ 's in the binary digit expansion of the number. So the aforementioned equation could be read as ""we have two equal binary numbers with two $1$ 's in each binary digit expansion"" -- wouldn't it follow that the $1$ 's have to be in the same positions in both such numbers? The analogy fails a bit if $m=r$ . But in that case, wouldn't it still be apt to determine the LHS as a binary number with a single $1$ digit? But even in that case $n=q=m=r$ is the only solution and we still have $\{m,r\}=\{n,q\}$ . Anyways, there are two ways to realize $\{m,r\}=\{n,q\}$ . Case 1 is that $m=n$ and $r=q$ . In this case, $a=b=2^{p-1}$ and thus our $a,b,c,d$ are not distinct. Case 2 is that $m=q$ and $r=n$ . In this case $d=-2^{p-1}+2^{q-1}+2^{r-1}=c$ and again our $a,b,c,d$ are not distinct. Thus the assumption that there exist distinct $a,b,c,d\in\mathbb{Z}$ such that five of their pairwise sums are powers of two was incorrect. Am I missing something? There's no way something Neil Sloane calls ""very hard"" has a solution this straight-forward right? He did talk about extending the problem to more than four integers; perhaps he meant they don't know the optimal in general? EDIT: Looks like I wasn't the first to think of this. Someone named T. Ranha suggested the same approach in a comment on the video: ""... Bringing 2^B to the other side we get: 2^C+2^D=2^E+2^B. Now lets look at the problem in binary. A power of 2 in binary is a single 1 at some place in the number. The sum of two different powers of 2 are therefore two 1s somewhere in a string of 0s. In the case of equal powers of 2, there is only one 1. In both cases, there is a 1:1 correspondence between the terms and the sums. Therefore we can conclude that B=C or B=D. The first one leads to c=d and the second one leads to a=b. Both are contradictons and therefore the assumption that it is possible, was wrong.""","['algebra-precalculus', 'combinatorics', 'perfect-powers', 'diophantine-equations']"
4536517,Find the equation of the function graphed below.,"Find the equation of the rational function from the image. I know that there are vertical asymptotes at $x=-1$ and $x=2$ . There are also x intercepts at $x=1$ and $x=3$ . Since the horizontal asymptote is at $y=1$ , the degrees of the numerator and denominator have to be the same. Thus I got $$f(x)= \frac{(x-1)^3(x-3)}{(x+1)^2(x-2)^2}.$$ However, the y-intercept of graph is at $(0,1)$ while the function I created has a y-intercept at $(0,\frac{3}{4})$ . How do I change my function to make it have a y-intercept of $(0,1)$ ? Also, I am not sure if the cube is supposed to be attached to the $(x-1)$ binomial. I just stuck the cube to any binomial to make the degrees of the numerator and denominator the same. How do I know where the cube is supposed to go?","['algebra-precalculus', 'rational-functions']"
4536518,"$-\Delta u=\lambda u$ in $\Omega$, $u=0$ in some ball, then $u\equiv 0$","Let $-\Delta u=\lambda u$ in $\Omega\subset\mathbb{R}^n$ , $\lambda>0$ . Suppose $u=0$ in a ball $B\subset\Omega$ . I want to prove that $u\equiv0$ in $\Omega$ . If $\lambda\leq 0$ , integrating by part solves this problem. But if $\lambda>0$ , things become more subtle. Besides, I think $\Omega$ must be connected. (am I right?) Any hints will be appreciated a lot!","['analysis', 'partial-differential-equations']"
4536524,Prove that the given set is measurable and has measure $0$.,"I was doing a problem given in ISI exam.It is a problem on measure theory.The problem is as follows: Let $(\Omega,\mathcal F,\mu)$ be a measure space and $(E_n)$ be a sequence in $\mathcal F$ such that $\sum\limits_{n=1}^\infty \mu(E_n)<\infty$ ,then show that the set $A=\{\omega\in \Omega: \omega\in E_n$ for infinitely many $n\}$ is measurable and has measure $0$ . I solved the problem as follows: Notice that, $A=\bigcap_{k=1}^\infty\bigcup_{n=k}^\infty E_n$ which is measurable because countable union and intersection of measurable sets is measurable.Again observe that $A\subset \bigcap_{k=1}^N\bigcup_{n=k}^\infty E_n=\bigcup\limits_{n=N}^\infty E_n$ so that $\mu(A)\leq \sum\limits_{n=N}^\infty \mu(E_n)\to 0$ as $N\to \infty$ because $\sum\limits_{n=1}^\infty \mu(E_n)<\infty$ .Is my approach correct?","['measure-theory', 'problem-solving']"
4536534,Show that random sum of ergodic processes is not ergodic,"We say that a mean stationary stochastic process $(X_t)_{t \in \mathbb N}$ - i.e. $E[X_t]= \mu_X$ for all $t$ -  is ergodic mean if \begin{equation}\tag{I}
\frac 1 T \sum_{t=1}^T X_t  \overset {pr} \longrightarrow \mu_X, \quad (T \to \infty)
\end{equation} It staightforward to show that the finite sum of ergodic processes is ergodic:  Let $(X_{t;j})_{1\leq j \leq n}$ be a finite sequence of ergodic process and $Y_{t;n} = \sum_{j=1}^n X_{t;j}$ . So: $$\frac 1 T \sum_{t=1}^T Y_{t;n}  \overset {pr} \longrightarrow \mu_n, \quad (T \to \infty)$$ where $\mu_j= E[X_{t;j}]$ and $\mu_n = \sum_{j=1}^n \mu_j$ . But, how about a random sum? More specifically, Let $(X_t)_{t \in \mathbb N}$ be a ergodic process with $\mu_X = E[X_t]$ as (I) above.  Consider $$Y_t = \sum_{j=1}^N X_{t;j}, \quad N\sim \hbox {Poisson} (\lambda)$$ where $X_{t,1}, X_{t,2},..., X_{t,j},...  \overset {\mathrm{i.i.d}} \sim  X_{t}$ for all $t$ (copies) and independent of $N$ . Note that this process is not the Compound Poisson Process. This is just a stationary process such that $Y_t$ is a Compound Poisson random variable, for all $t$ , according with this . It straightforward to show that $(Y_t)_{t\in \mathbb N}$ is mean stationary $$E[Y_t]= \lambda \mu_X, \quad \forall\, t \in \mathbb N.$$ How to show that \begin{equation}%\tag{I}
\frac 1 T \sum_{t=1}^T Y_t  \overset {pr} \longrightarrow \lambda \mu_X, \quad (T \to \infty) \quad ??
\end{equation} My first attempt is try to show that $P\left(\left| \frac 1 T \sum_{t=1}^T Y_t - \lambda \mu_X \right| > \epsilon \right) \to 0$ , as $T \to \infty$ , for all $\epsilon >0$ . Note: \begin{aligned}
P\left(\left| \frac 1 T \sum_{t=1}^T \sum_{j=1}^N X_{t;j}, - \lambda \mu_X \right| > \epsilon \right) &= \sum_{n=1}^\infty P\left(\left| \frac 1 T \sum_{t=1}^T \sum_{j=1}^n X_{t;j} - \lambda \mu_X \right| > \epsilon \right)P(N=n)\\
= \sum_{n=1}^\infty& P\left(\left| \frac 1 T \sum_{t=1}^T \sum_{j=1}^n X_{t;j} - n \mu_X + n \mu_X + \lambda \mu_X \right| > \epsilon \right)P(N=n)
\end{aligned} So. I am trying to use the ergodicity of $ \sum_{j=1}^n X_{t;j}$ , i.e.: $P\left(\left| \frac 1 T \sum_{t=1}^T \sum_{j=1}^n X_{t;j} - n \mu_X  \right| > \epsilon \right)  \overset {pr} \longrightarrow 0$ as $T \to \infty$ . How to conclude?  Is there another way to show this?","['self-learning', 'ergodic-theory', 'asymptotics', 'stochastic-processes', 'probability-theory']"
4536608,General formula for $\prod_{i<j} (a_i + b_j)$,"I want to expand the product of a sum into a sum of products $$
  \prod_{i<j}^n (a_i + b_j) = \sum_{\text{sets } A,B} ~ \prod_{i\in A} a_i \prod_{j\in B} b_j.
$$ With the result from this post General formula for $\prod (x+a_i)$ , it follows that $$
  \prod_{i<j} (a_i + b_j) = \prod_{j=2}^n \prod_{i=1}^{j-1} (a_i + b_j) = \prod_{j=2}^n \Big( \sum_{A \subset \{1, .., j-1\}} b_j^{j-1-|A|} \prod_{i\in A} a_i \Big).
$$ However, I don't see how to continue from here. There is also this post: expand the product $\prod_{i=1}^n(a_i+b_i)$ , but again it does not help in this generalized case.","['sumset', 'algebra-precalculus', 'products']"
4536624,How to find a system of ODEs that give a specific function,"I have the given system of equations: \begin{equation}
x^2 + y^2 - z = C_1 \\
x y z=C_2
\end{equation} and I want to find  a system of ODEs that yield the equation above as a solution. The ODEs must be in the form: \begin{equation}
\frac{dx}{dt}=f_1(x,y,z)\\
\frac{dy}{dt}=f_2(x,y,z)\\
\frac{dz}{dt}=f_3(x,y,z)\\
\end{equation} First I would sum the two at the top to one and get $x^2+y^2-z+xyz=C_1+C_2$ Then it appears that the system of ODEs is really the gradient of the function f with respect to $t$ . So I would just integrate this with respect to t for each function and divide them all by three, so their sum is equal to the equation at the top and get: \begin{equation}
\frac{dx}{dt}=x^2+xyz-C_1\\
\frac{dy}{dt}=y^2-C_2\\
\frac{dz}{dt}=-z
\end{equation} Integrating I get: \begin{equation}
x=t(x^2+xyz-C_1)\\
y=t(y^2-C_2)\\
z=-tz\\
\end{equation} But something tells me this is not right. Any ideas? Thanks",['ordinary-differential-equations']
4536658,"Compute $\int_{0}^{2\pi}\frac{e^{ikt}}{|e^{it}-e^{it_0}|^m}~\text{d}t$, where $k\in\mathbb{Z}$, $t_0\in\mathbb{C}$, and $m=1,3,5,\dots$","I want to compute $$
\int_{0}^{2\pi}\frac{{\rm e}^{{\rm i}kt}}{\,\left\vert\,{{\rm e}^{{\rm i}t} - {\rm e}^{{\rm i}t_{0}}}\,\,\right\vert^{\, m}\,}\,{\rm d}t,
$$ where $k\in\mathbb{Z}$ , $t_{0}\in\mathbb{C}$ , and $m = 1,3,5,\ldots$ . The integrand experiences a singularity at $t=t_0$ and $t = \bar{t}_{0}$ . We may assume that $\left\vert{\,{\rm e}^{{\rm i}t_{0}}\,}\right\vert < 1$ . However, if someone is able to make any progress for any $m$ and e.g. for only $k > 0$ , it would be of much help. You may also consider the integral rewritten into a contour integral over the unit circle $S$ by letting $\xi\left(t\right)={\rm e}^{{\rm i}t}$ and $\mu = {\rm e}^{{\rm i}t_0}$ , $$
\frac{1}{i}\oint_{S}\frac{\xi^{k - 1}}{\left\vert\xi-\mu\right\vert^{m}}\,{\rm d}\xi.
$$ Another reformulation is to let $t_{0} = a_{0}+ {\rm i}b_{0}$ and $\alpha = {\rm e}^{-b_{0}}$ . Then, the integral can be found as $$\int_{0}^{2\pi}
\frac{\cos\left(kt\right) + {\rm i}\sin\left(kt\right)}
{\,\left\{\left(1 + \alpha\right)^{2} -
2\alpha\left[1 - \cos\left(t - a_{0}\right)\right]\right\}^{m/2}\,}~\text{d}t,$$ where one may treat the two integrals separately. I have not been able to make any significant progress on any of these three formulations.","['integration', 'branch-points', 'complex-analysis', 'contour-integration', 'fourier-series']"
4536659,"If $\int_{-\infty}^{+\infty} |f(t)|dt < \infty$, show that $\int_{-\infty}^{+\infty}f(t)dt$ exists","Let $f: \mathbb{R}\to \mathbb{R}$ be a function such that $f$ and $|f|$ are Riemann-integrable on each closed interval. If $\int_{-\infty}^{+\infty} |f(t)|dt < \infty$ , show that $\int_{-\infty}^{+\infty}f(t)dt$ exists. Attempt: We need to show that $\int_0^{+\infty} f(t)dt:= \lim_{x\to \infty}\int_0^x f(t)dt$ exists. Let $\{x_n\}_n$ be a non-decreasing sequence with $x_n\nearrow \infty$ . Then for $n \le m $ , we have $$\left|\int_0^{x_m}f(t)dt-\int_0^{x_n} f(t)dt\right|\le \int_{x_n}^{x_m}|f(t)|dt = \int^{x_m}_0 |f(t)|dt - \int_0^{x_n}|f(t)|dt \stackrel{m,n \to \infty}\longrightarrow 0$$ so $\lim_{n \to \infty }\int_0^{x_n} f(t) dt$ exists. Now, we use the following fact: Let $g: \mathbb{R}\to \mathbb{R}$ a function. If for all $\{x_n\}$ non-decreasing sequences with $x_n \nearrow +\infty$ , we have that $\lim_n g(x_n)$ exists, then $\lim_{x\to \infty} g(x)$ exists. Similarly, we prove that $\int_{-\infty}^0 f(t)dt$ exists. Hence, $$\int_{-\infty}^{+\infty}f(t)dt$$ exists. Is my attempt correct?","['integration', 'solution-verification', 'riemann-integration', 'real-analysis']"
4536673,Expected number of fixed points of a random permutation,"I am working on this problem and do not know how to proceed. Let $X_n$ be the number of fixed points of a given permuation of $\{1,2\dots ,n\}$ . Show that for $k\in \{1,2\dots,n\}$ , $E[X_n(X_n -1) \dots (X_n- k +1)] = 1$ . It is clear to me to show in the case of $k = n$ , but otherwise unclear. Any pointers on how to proceed?","['probability-distributions', 'probability-theory', 'probability']"
4536699,How to show that any continuous map $f:\mathbb{C}^*\to \mathbb{C}^*$ is of the form $f(z)=z^me^{g(z)}?$,"Given a nowhere vanishing continuous map $f:\mathbb{C}^*\to \mathbb{C}^*$ , how do I show that there exist an integer $m\in \mathbb{Z}$ and a continuous map $g:\mathbb{C}^*\to \mathbb{C}$ such that $f$ is of the form $$f(z)=z^me^{g(z)}, ~~~~~ \forall z\in \mathbb{C}^*.$$ Where $\mathbb{C}^*= \mathbb{C}\setminus\{0\}.$ I come across this problem while trying to show that the cokernel of the exponential map in the following sequence $$0\to \mathbb{Z}\stackrel{2\pi i}{\rightarrow} \mathcal{O}(\mathbb{C}^*)\stackrel{exp} {\rightarrow}\mathcal{O}^*(\mathbb{C^*})\to 1$$ is isomorphic to $\mathbb{Z}=H^1(\mathbb{C}^*, \mathbb{Z}).$ Where $\mathcal{O}(\mathbb{C^*})$ denotes the group of all continuous maps from $\mathbb{C}^*$ to $\mathbb{C}$ and $\mathcal{O}^*(\mathbb{C^*})$ denotes the group of all continuous maps from $\mathbb{C}^*$ to $\mathbb{C}^*$ . I have proved that for all integers $m\in \mathbb{Z}$ , the function sending $z\mapsto z^m$ belongs to distinct classes of the quotient group $\frac{\mathcal{O}^*(\mathbb{C^*})}{exp\left(\mathcal{O}(\mathbb{C^*})\right)}$ and I want to show that these are the only classes in the quotient. For that I have to solve the above problem. Is there an elementary proof for this problem where I dont have to use cohomology of the punctured complex plane. Thanks in advance.","['complex-geometry', 'complex-analysis', 'continuity', 'sheaf-theory', 'exponential-function']"
4536702,Non associativity of Cartesian product,"I am trying to prove that if $$(A \times B) \times C = A \times (B \times C)$$ Then either $A = \emptyset, B = \emptyset$ or $C = \emptyset$ . So far I start of by assuming the equivalence and that $A \neq \emptyset, B \neq \emptyset$ and $C \neq \emptyset$ . This gives $a \in A, b \in B, c \in C$ such that $((a,b), c) \in (A \times B) \times C $ and $((a,b), c)\in A \times (B \times C)$ , whence $(a, b) \in A \times B$ and $(a, b) \in A$ and $c \in C$ and $c \in B \times C$ , so that $A \times B = A$ and $B \times C = C$ . I understand on an intuitive level that $A \times B \neq A$ , unless they are both empty (and likewise for $B \times C = C$ , but what rules it out formally (in particular, what axiom or chain of reasoning in ZFC would rule it out)?",['elementary-set-theory']
4536705,Do we have $\prod_{n=1}^{\infty}\left(1+\frac{\tan\left(n\right)+1}{n\left(\tan\left(n+1\right)+1\right)}\right)=^?0$,"It seems too good to be true so let me propose it: Do we have: $$\prod_{n=1}^{\infty}\left(1+\frac{\tan\left(n\right)+1}{n\left(\tan\left(n+1\right)+1\right)}\right)=^?0$$ I try to plot the equation: $$1+\frac{\left(\tan\left(x\right)+1\right)}{x\left(\tan\left(x+1\right)+1\right)}=0$$ And the root seems to be always a non integer. On the other hand: $$\prod_{n=1}^{10000}\left(1+\frac{\tan\left(n\right)+1}{n\left(\tan\left(n+1\right)+1\right)}\right)\simeq −3.5722175948×10^{−11}
$$ Using WolframAlpha: So is my closed form true? If yes how can we show it ?","['examples-counterexamples', 'closed-form', 'products', 'trigonometry', 'convergence-divergence']"
4536720,How to find two small matrices $M_1$ and $M_2$ such that $M_1 M_2 A \approx M A$?,"If we have a matrix $M$ and we want to find its least squares approximation as the product of two smaller (as in less rows or columns) matrices $M_1M_2$ of a given size, we can simply run SVD and pick the entries with the biggest singular values. It's not true, however, that we can do the same to approximate $MA$ as $M_1M_2A$ . For instance, if the rows of A have a mean of $0$ and $M$ is the identity, we need to run PCA on $A$ (instead of $M$ ) to find $M_1$ and $M_2$ . So, short of running gradient descent, is there a simple way of finding the solution? My current best guess is $(MV)(V^T)A$ , with $V$ being the top principal components of $A$ . But that is obviously suboptimal. It would also be very nice if interpreting the columns of $A$ as data samples there was a natural way of applying regularization, so that for few samples or strong regularization the solution was approximately the SVD of $M$ .","['statistics', 'svd', 'principal-component-analysis', 'linear-algebra', 'matrix-decomposition']"
4536726,How to solve the ODE $ f(x)f''(x)-f(x)f'(x)-{f'(x)}^2=0$?,"Solve the differential equation $$f(x)f''(x)-f(x)f'(x)-f'(x)^2=0$$ with $f(0) = 0 = f'(x)$ . my attempt:- Let $f(x)f'(x)=z$ so we have $f'(x)^2+f(x)f''(x)= \frac{dz}{dx}$ , we get substituting in :- $ f(x)f''(x)-f(x)f'(x)-{f'(x)}^2=0$ ,we get $$f(x)f''(x)-z-\left(\frac{dz}{dx}-f(x)f''(x)\right)=0 ,$$ beyond which I'm stuck. Could someone help, please?","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
4536746,Shorter way to prove $|\underset{x \in A}{\text{inf}}f(x)-\underset{x \in A}{\text{inf}}g(x)|\leq \underset{x \in A}{\text{sup}} |f(x)-g(x)|$,"Here is a result I found in a step of a proof (not even a lemma, and I can't even find its justification): $$|\underset{x \in A}{\text{inf}}f(x)-\underset{x \in A}{\text{inf}}g(x)|\leq \underset{x \in A}{\text{sup}} |f(x)-g(x)|$$ Here're my questions: Is my proof correct? (I've attached it below) Is there a more intuitive way to do it? What's the intuition behind? Can I understand the result without the concept of taking limit? My attempt to prove it: Denote $sup= \underset{x \in A}{\text{sup}} |f(x)-g(x)|$ (assume it's finite), and assume both $\text{inf} f(A)$ and $\text{inf} g(A)$ are real. For all $x \in A$ , $|f(x)-g(x)| \leq sup$ . By the definition of infimum, we can find a sequence $(f(x_n))$ , where $x_n \in A$ , such that $$\lim_{n \to \infty }f(x_n)=\underset{x \in A}{\text{inf}}f(x):= inf_f$$ So for all $x \in A$ we have: $$|inf_f-g(x)|\leq sup$$ Similarly for $g$ we get $|inf_f-inf_g|\leq sup$ . Am I correct?","['elementary-set-theory', 'supremum-and-infimum', 'real-analysis']"
4536769,"Proof of the Meyers–Serrin theorem (the ""$H=W$"" theorem)","I'm working on the proof of the "" $H=W$ "" theorem in the book SOBOLEV SPACES by Adams and Fournier,
and there is an argument that seems quite strange to me. Let me introduce some conventions first. $H^{m,p}(\Omega)$ is defined as the completion of $$\{u\in C^m(\Omega):\lVert u\rVert_{m,p}<\infty\}$$ w.r.t. the norm $$\lVert u\rVert_{m,p}=\left(\sum_{|\alpha|\leq m}\lVert D^\alpha u\rVert_p^p\right)^\frac{1}{p},$$ where $D^\alpha$ is understood in the weak sense, while $W^{m,p}(\Omega)$ is defined by weak derivatives in the usual way. As to completions, the authors mentioned in the preliminary chapter that every normed space $X$ is either a Banach space or a dense subset of a Banach space $Y$ called the completion of $X$ whose norm satisfies $$\lVert x\rVert_Y=\lVert x\rVert_X$$ for every $x\in X$ . Finally, throughout our discussion, $\Omega$ denotes a nonempty open subset of the Euclidean space. Now let us see the proof to be understood. The argument underlined with red is much annoying. I don't see any relevance between completeness of $W^{m,p}(\Omega)$ and the extension to the asserted isometric isomorphism. If the identity operator on $S$ means the inclusion of $S$ in $W^{m,p}(\Omega)$ , how could I extend it to the isometric isomorphism? Before digging into the proof, I have reviewed the preliminary chapter, so I know the normed space $S$ is isometrically isomorphic to a dense subspace of a Banach space $H^{m,p}(\Omega)$ ? But this doesn't seem to be any helpful. How could I use this fact to build the extension. Help is much needed. Thank you for your precious time. Update 1: I remember that if the codomain $T$ of a uniformly continuous map $f:A\subseteq M\to T$ is complete, then we can uniquely extend $f$ to the closure $\bar{A}$ , preserving uniform continuity. But this seems to give nothing if I extend the inclusion of $S$ in $W$ . Update 2: Though it may be obvious to people who know about completions of normed spaces, I'd like to emphasize for once that every normed space $X$ is in fact isometrically isomorphic to a dense subspace of a Banach space and it is this Banach space that is defined as the completion of $X$ . Then, by identifying isomorphic spaces, the authors conclude that every normed space is a dense subset of its completion, I guess.","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4536778,Without any software and approximations prove that $\sec(52^{\circ})-\cos(52^{\circ})>1$,"Without any software and approximations prove that $$\sec(52^{\circ})-\cos(52^{\circ})>1$$ We can use some known trig values like $18^{\circ}$ , $54^{\circ}$ ,etc My try: I considered the function: $$f(x)=\sec(x)-\cos(x)-1,\: x\in \left (0, \frac{\pi}{3}\right)$$ We have the derivative as: $$f'(x)=\sec x\tan x+\sin x >0$$ so $f$ is Monotone increasing. So we have: $$f(52^{\circ})>f(45^{\circ})=\frac{1}{\sqrt{2}}-1$$ but not able to proceed","['monotone-functions', 'algebra-precalculus', 'trigonometry', 'inequality']"
4536791,Can every manifold with torus boundary be cut?,"Let $\mathcal{M}$ be a compact, oriented and connected $3$ -manifolds, whose boundary satisfies $\partial\mathcal{M}\cong T^{2}$ , where $T^{2}:=S^{1}\times S^{1}$ denotes the $2$ -torus. If I ""cut"" through the manifold $\mathcal{M}$ , do I always end up  with a well-defined manifold $\mathcal{M}^{\prime}$ whose boundary is a $2$ -sphere? For example, if $\mathcal{M}$ is the solid torus (the genus 1 handlebody), then we can just cut along an embeded disk, whose boundary circle lies purely in $\partial\mathcal{M}$ . If $\mathcal{M}$ is a manifold obtained by performing the connected sum of the solid torus with some closed $3$ -manifold, we can apply the same logic. But it is also true more generally? In general, manifolds with torus boundary can have a highly non-trivial bulk topology, for example manifold with incompressible boundary, etc.","['manifolds', 'general-topology', 'low-dimensional-topology']"
4536838,Are derivatives defined on functions or variables?,"My question is, is differentiation done on functions or on variables/values? For example take $z=f(x,y)$ , in this case we can 'fix' one of the variables to the value $y_0$ , the question then becomes, is the derivative of $z_0=f_y(x,y)$ defined here? $y$ is now constant, however if our derivative is defined on $f$ the partial derivative should exist. Does it make sense to talk about the rate that $f(x,y_0)$ is changing? can we for example write $\frac{dz_0}{dx}$ ? or $\frac{d(f(x,y0))}{dx}$ ? to talk about the rate the value $f(x,y_0)$ changes? Or can we only discuss the function 's partial derivatives $f_x'(x,y_0)$ and $f_y'(x,y_0)$ ? Another thing is that if we apply $f$ to two arguments who depend on each other, we provide the total derivative, which is actually different depending on their relation, if the function is defined independently how can this be the total derivative of a function? If we take $\frac{df(x^2)}{dx}$ , it seems we take the 'derivative' of the value $f(x^2)$ , but perhaps this notation can be seen as the 'derivative of the function whose value is $f(x^2)$ ', but seems a bit strange. Do we take the derivative of a 'value' or variable or is it better to keep Euler 'notation'.","['partial-derivative', 'calculus', 'functions', 'derivatives']"
4536956,Convergence of $p^{\text{th}}$ quantile estimator for sample from Exponential distribution,"Convergence in probability of $p^{\text{th}}$ quantile estimator for iid sample $X_1, \ldots X_n$ from
Exponential distribution given by $f(x, \lambda) = \lambda e^{-\lambda x}$ The $p^{\text{th}}$ is given by $\theta = F^{-1}(p)$ where $p\in(0,1)$ and $F$ is the CDF. I found that for exponential distribution, $\theta = -\frac{\ln(1-p)}{\lambda}$ and the MLE estimator $\hat{\theta}  = -\ln(1-p) \bar{x}$ . How can I use this to show that $\hat{\theta} \to^{P} \theta$ Is the following method correct? $$\bar{x} \to^P E(X_1) = \frac{1}{\lambda} \;\;\;\;\text{(From Weak Law of Large Numbers)}$$ $$\Rightarrow \bar{x} \to^{d} \frac{1}{\lambda} \;\;\;\;\text{(Convergence in probability implies convergence in distribution)}$$ $$\Rightarrow -\ln(1-p)\bar{x} \to^{d} -\frac{\ln(1-p)}{\lambda} \;\;\;\;\text{(Using Slutky's theorem)}$$ $$\Rightarrow -\ln(1-p)\bar{x} \to^{p} -\frac{\ln(1-p)}{\lambda} \;\;\;\;\text{(Convergence in distribution to a constant implies convergence in probability)}$$ I'm not sure about the last step, is it correct that $-\frac{\ln(1-p)}{\lambda} $ is a constant?","['probability-limit-theorems', 'quantile-function', 'exponential-distribution', 'probability-theory', 'probability']"
4536997,Compute the area of Quadrilateral $ABCD$,"As title suggests, the question is to solve for the area of the given convex quadrilateral, with two equal sides, a side length of 2 units and some angles: I have solved the problem with a synthetic geometric approach involving some angle chasing. However, I believe my solution (which I will post in a comment below since I don't want to clutter the question) is a little messy and not efficient. Are there any better ways to do this? Geometric and/or trigonometric approaches are all welcomed! EDIT: I have posted my solution below!","['euclidean-geometry', 'trigonometry', 'geometry']"
4537024,Semisimple Lie group topologically generated by two finite order elements,"Let $ G $ be a semisimple (algebraic) Lie group.
Do there always exist two finite order elements of $ G $ which generate a dense subgroup? This is partially inspired by https://mathoverflow.net/questions/59213/generating-finite-simple-groups-with-2-elements which shows that every finite simple group is 2-generated. Indeed even every finite quasisimple group is 2-generated https://mathoverflow.net/questions/254164/is-every-finite-quasi-simple-group-generated-by-2-elements Note that $ G=U_1 $ is not a counterexample because $ U_1 $ is not semisimple. The universal cover of $ SL_2(\mathbb{R}) $ is  not a counterexample since it is not algebraic. Example: $$
\frac{i}{\sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
$$ and $$
\begin{bmatrix} \overline{\zeta_{16}} & 0 \\ 0 & \zeta_{16} \end{bmatrix}
$$ generate a dense subgroup of $ SU_2 $ . Update: A lot of simple compact Lie groups have Lie primitive subgroups (not contained in any proper positive dimensional closed subgroups) which are finite simple groups (or at least quasisimple). Since quasi simple groups are 2-generated you can take $ a,b $ generators for the Lie primitive (quasi)simple group and then randomly pick a 3rd finite order element $ c $ and almost surely $ a,b,c $ will generate a dense subgroup of $ G $ . Indeed if you pick a maximal Lie primitive subgroup $ \Gamma $ , that is also (quasi)simple, then any choice of a finite order element $ c \not \in \Gamma $ is guaranteed to topologically generate $ G $ . Papers like https://arxiv.org/abs/math/0502080 list (quasi)simple finite subgroups of many simple Lie groups.","['group-theory', 'topological-groups', 'lie-groups']"
4537036,What is the Lebesgue measure of Luzin's non-Borel set of reals?,"On Wikipedia there's a nice example of a non-Borel set due to Luzin. For completeness, I'll summarize it here. For $x\in[0,1]$ , let \begin{align}
x=a_0 + \cfrac{1}{a_1 + \cfrac{1}{a_2 + \cfrac{1}{a_3 + \cfrac{1}{\ddots\,}}}}
\end{align} be the continued fraction expansion of $x$ . Let $A$ be the set of numbers $x\in[0,1]$ whose corresponding sequence $a_0,a_1,a_2,\cdots$ admits an infinite subsequence $a_{k_1},a_{k_2},a_{k_3},\cdots$ such that for each $i$ , $a_{k_{i+1}}$ is divisible by $a_{k_i}$ . Then $A$ is not Borel measurable. However, $A$ is an analytic set, and in particular it is Lebesgue measurable. What is the Lebesgue measure of $A$ ? My thinking is that it should be zero, since the existence of such an infinite subsequence strikes me as an improbable coincidence. However, I've forgotten whatever little I knew about continued fractions, so that's far from a proof. I tried searching this site before asking, of course. I found only one mention, here (last sentence), where $A$ is again conjectured to have measure zero.","['measure-theory', 'lebesgue-measure', 'descriptive-set-theory']"
4537038,"Considering that the two selected balls are white, what is the probability that urn III was chosen?","Consider three numbered urns. Urn I contains 6 white balls and 2 red balls, urn II contains 2 white balls and 6 red balls, and urn III contains 4 white balls and 4 red balls. To choose an urn, roll a balanced die and consider: If faces 1, 2 appear, choose urn I, if faces 3, 4, 5 appear, choose urn II and if face 6 appear, choose urn III. Then 2 balls are chosen at random, one after the other, without replacement, from the selected urn. Considering that the two selected balls are white, what is the probability that urn III was chosen? To solve this question I have used the Bayes Theorem for Conditional probability: $P(C_j|A)$ = ${P(C_j)*P(A|C_j)} \over {\sum P(C_i)*P(A|C_i)}$ So, since the first ball is white, let's calculate the probability of urn III. $P(UrnIII|White)$ = ${P(UrnIII)*P(White|UrnIII)}\over{P(UrnI)*P(White|UrnI)+P(UrnII)*P(White|UrnII)+P(UrnIII)*P(White|UrnIII)}$ P(UrnIII) = $1\over 6$ P(White|UrnIII) = $4 \over 8$ P(UrnI) = $2\over 6$ P(White|UrnI) = $6\over 8$ P(UrnII) = $3 \over 6$ P(White|UrnIII) = $ 2 \over 8$ Replacing those values in the equation we get: $P(UrnIII|White)$ = $2 \over 11$ Now, I am supposed to to the same thing for the second ball. However, I am not quite sure of how I am going to evaluate considering that the first ball was not put back in the urn. I thought about dividing the second part in cases. First, assume that the white ball was selected from the urnI. And recalculate all the probability in bayes theorem for the second ball. Then, I would consider the first white ball was selected from urnII. And lastly, consider the first white ball was selected from urnIII and calculate another probability. However, this thought doesn't seem right to me, and I thought I could get some help on thoughts of how to solve this problem.","['polya-urn-model', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4537059,"If $(f,g) \in R$ then $( (f \circ f),(g \circ g) )\in R$","I was doing the exercise nº 17-(b) on chapter 5.3 from the book ""How To Prove It"" by D.J. Velleman and I got stuck. The exercise is as follows: Suppose $A$ is a set, and let $F = \{ f | f: A \rightarrow A \}$ and $P = \{ f \in F | f \text{ is one-to-one and onto } \}$ . Define a relation $R$ on $F$ as follows: $R = \{ (f,g) \in F \times F | \exists h \in P (f = h^{-1} \circ g \circ h ) \}$ . Prove that if $(f,g) \in R$ then $((f \circ f), (g \circ g)) \in R$ . As I understand it, we know that $R(f) = g$ and we are asked to prove that $R(f \circ f) = R(h^{-1} \circ g \circ g \circ h) = g \circ g$ . I'm having an hard time ending this proof.","['proof-writing', 'functions']"
4537061,Does any set of reals with positive outer measure always contains a non-measurable set?,"I need to prove that if $A\subseteq\mathbb{R}^n$ is measurable with positive measure, then it contains a non-measurable set. I produced a proof but I think it is wrong since I didn't use the fact that $A$ is measurable, only that its outer measure is positive. Here it is. Let $Q_j$ be the collection of closed unit cubes with integer base points, i.e. of the form $[a_1,a_1+1]\times\dots\times[a_n,a_n+1]$ for $a_i$ integers. Then $A = \bigcup (A\cap Q_j)$ . Now suppose that $m_*(A\cap Q_j)=0$ for all $j$ . Then, by subadditivity of the outer measure, $m_*(A) = 0$ . In our case, since $m_*(A) > 0$ , there must exists a unit cube $K$ such that $m_*(A\cap K) > 0$ . By translating the set $A$ , we can assume wlog that $K$ is the unit cube at the origin $[0,1]^n$ . Now it's just an adaptation of the Vitali set argument. Consider the classes of $\mathbb{R}^n/\mathbb{Q}^n$ that intersect $A\cap K$ . Take exactly one representative from each of these classes and use them to form the set $V$ . Clearly it's possible to choose the representatives so that $V$ is contained in $A\cap K$ . Now let $\{r_k\}$ be an enumeration of the rationals in $[-1,1]^n$ and let $V_k = V + r_k$ . For each $k$ , we have $V_k \subseteq K+r_k$ , so $V_k\subseteq [-1,2]^n$ . We also have that $A\cap K \subseteq \bigcup V_k$ because if $x\in A\cap K$ , then there exists a $x_\alpha\in V$ such that $x\sim x_\alpha$ and therefore $x\in V_k$ for some $k$ . Putting these inclusions together and using monotonicity of the outer measure, $0 < m_*(\bigcup V_k)\leq 3^n$ where the strict inequality is because $m_*(A\cap K)>0$ . If we assume that $V$ is measurable, we have that $0 < \sum m_*(V) \leq 3^n$ , which is impossible. Since $V$ is contained in $A\cap K$ , we have produced a non-measurable set contained in $A$ . Is this proof valid? Did I use the measurability of $A$ without knowing it?","['measure-theory', 'solution-verification', 'lebesgue-measure']"
4537092,How does one find the GCD of three numbers with large exponent values? [duplicate],"This question already has answers here : gcd and lcm from prime factorization proof [closed] (2 answers) Find $d=\gcd(a,b)$ if $a=p_{1}^{k_{1}}p_{2}^{k_{2}} \cdots p_{s}^{k_{s}}$ and $b=p_{1}^{l_{1}}p_{2}^{l_{2}} \cdots p_{s}^{l_{s}}$ (2 answers) $\gcd(a,b,c)=\gcd(\gcd(a,b),c)\,$ [Associative Law for GCD, LCM] (6 answers) Closed 1 year ago . I have a problem which is to find the GCD of three numbers, each of which have large exponents: $$ GCD( 2^{300}, 3^{200}, 2^{200})$$ What I have tried: So far, I think there are two main steps here: (1) address the fact that its three numbers, and (2) attempt to get the GCD given the exponents. So first, I believe I can use Euclids algorithm: Now, I would be working with only two values (which is what we have practiced so far). Now, I would have: $$ GCD( 3^{200}, 2^{300})$$ What I am getting stuck is on the following step, and how to proceed, assuming this first step is correct. What I am inclined to do based on what we practice so far is to find the primes they have in common, but where I get stuck is how to do that for exponents, when searching for the GCD. Any guidance would be appreciated!","['gcd-and-lcm', 'discrete-mathematics']"
4537123,Diophantine equation $x^2y^2+y^2z^2+z^2x^2=N^2$,"The equation in integers $$x^2y^2+y^2z^2+z^2x^2=N^2$$ has solutions such as $$(x,y,z,n)\in \{(1,1,2,3),(1,3,4,13),(5,6,11,91) \}$$ Can one find a family of infinitely many?
The problem is related with the tetrahedron of vertices $$(x,0,0),(0,y,0),(0,0,z),\ \text{and}\ \ (0,0,0)$$ having faces of integer/rational areas (Heronian faces). This is almost a Heronian tetrahedron. https://en.wikipedia.org/wiki/Heronian_tetrahedron","['number-theory', 'geometry']"
4537133,Is a graphs is determined by its and its complement's multiset of spanning trees?,"As shown by this post, a graphs is not determined by its multiset of spanning trees . In fact, the two graphs below have the same multiset of spanning trees, but are non-isomorphic. Lets call such graphs tree-equivalents . Notice, however, that their complements are not tree-equivalents. In fact the following graph appears $5$ times in the multiset of the left graph's complement, but $11$ in the multiset of the right graph's complement. So my question is: are there two non-isomorphic tree-equivalent graphs whose complements are tree-equivalents as well? I understand that might be a hard question, so I would also like to know what properties graphs satisfying this condition must share. For instance, it's easy to show that the maximum degree of the graph is the maximum of the maximum degrees of it's spanning trees, so two tree-equivalent graphs must share their maximum degrees . If their complements are tree-equivalent as well, then that must also be true for their minimum degree. An analogous argument can be made to prove that two tree-equivalent graphs must share the length of its maximum path , but I don't know if this statement implies something nice for its complements, like in the previous case.","['graph-theory', 'graph-isomorphism', 'combinatorics']"
4537145,How many types of pizza combinations are possible?,"Before I begin, this question is from Algebra Nation's ACT prep subject, section 10: Statistics and Probability, and topic 5: Counting techniques. It is given that: Anthony's pizza offers 4 different types of cheese, 3 different kinds of meat toppings, 6 different kinds of vegetable toppings, and 2 different types of sauce. Each type of pizza on the menu has a combination of exactly 4 ingredients: 1 cheese, 1 meat, 1 vegetable, and 1 sauce. How many types of pizzas are possible? The answer choices are as given: A. 15 B. 63 C. 120 D. 144 E. 720 What I Tried I've never really come across a problem like this, but I used my knowledge to try and come up with an answer. I started by adding the different cheeses, meats, vegetables, and sauce types. 4+3+6+2=15 Since there are 4 ingredients I divided 4 by 15, but I immediately knew this would leave me with an answer that is not valid. Dividing 4 by 15 would give me a decimal, but I continued to go further into solving the problem. ⁴⁄₁₅=0.26 This answer was not applicable, so I tried multiplying 0.26 by 15. 0.26×15= 3.9 3.9 was nowhere near the values of the answer choices, which concludes that my method of solving the problem was not applicable. I am unsure where I made my mistake exactly. But there is a chance that the whole problem was solved incorrectly and that neither of the steps was performed accurately.","['combinations', 'combinatorics']"
4537166,Prove that $\lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0$,"Let $f\in\mathcal{C}^0[0,1]\cap\mathcal{C}^2(0,1)$ and satisfy Boundary Condition: $f(0)=0=f(1)$ Positive: $f(r)>0,\quad r\in(0,1)$ Finite Energy: $\int_0^1\left(r(f')^2+\dfrac{f^2}{r}\right)dr<\infty$ I want to prove the 2nd problem. I include the first to show what I am able to do. $\lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=0$ $\lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0$ . I am able to show the first one is true via a contradiction. However, the approach does not work for the second one. Here is the proof for the first one. Suppose 1 is not true, but the limit exists, i.e., $$\lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=L>0.$$ By the definition of the limit, for any $\epsilon>0$ there is an $r_0\in(0,1)$ such that $$\big|rf(r)|f'(r)|-L\big|<\epsilon\quad\forall r\in(0,r_0).$$ We then have $$L-\epsilon<rf(r)|f'(r)|<L+\epsilon\quad\forall r\in(0,r_0).$$ Now, we can choose $\epsilon=\dfrac{L}{2}>0$ to get $$\dfrac{L}{2}<rf(r)|f'(r)|\quad\forall r\in(0,r_0).$$ Then dividing by $r$ and integrating on $(0,r_0)$ , we arrive at $\infty=\int_0^{r_0}\dfrac{L}{2r}dr\leq\int_0^{r_0}f(r)|f'(r)|dr\leq\left(\int_0^{r_0}\dfrac{(f(r))^2}{r}dr\right)^{1/2}\left(\int_0^{r_0}r(f')^2dr\right)^{1/2}<\infty$ , a contradiction to the finite energy condition. If I approach the second one via a contradiction, it is clear that $f'(r)\rightarrow\infty$ as $r\rightarrow 1$ and, consequently, should have a contradiction to the finite energy condition. But, I am unable to establish rigorous proof.","['functional-analysis', 'analysis']"
4537172,Exponent of noncyclic finite abelian group,"Prove that a finite abelian group $G$ of order $n$ which is not cyclic has an exponent $m$ with $m < n$ . Here is what I have so far: Suppose that $G$ is a finite abelian group which is not cyclic.  We know that for all $g \in G$ , the order $o(g)$ of $g$ is a divisor of $n$ and is strictly less than $n$ , since otherwise $g$ would generate the entire group $G$ and then $G$ would be cyclic.  We also know that $o(g)$ is the smallest positive integer $k$ such that $g^k = e$ .  Since $G$ is finite, write $G = \{g_1, g_2, ..., g_n\}$ and let $n = p_1^{\alpha_1}p_2^{\alpha_2}...p_j^{\alpha_j}$ be the prime factorization of $n$ . Since $o(g)$ divides $n$ for each $g \in G$ , we can write $$
o(g_1) = p_1^{\beta_{1,1}}p_2^{\beta_{2,1}}...p_j^{\beta_{j,1}}, \quad o(g_2) = p_1^{\beta_{1,2}}p_2^{\beta_{2,2}}...p_j^{\beta_{j,2}}, \quad ..., \quad  o(g_n) = p_1^{\beta_{1,n}}p_2^{\beta_{2,n}}...p_j^{\beta_{j,n}},
$$ where $\beta_{l,k} \leq \alpha_l$ for all $l,k$ and where we don't have equality in all of the exponents (this would give that the order of some group element was $n$ ).  I know that if I choose $m = \text{lcm}(o(g_1), o(g_2), ..., o(g_n))$ then $m$ will have the property that $g^m = e$ , and since each of $o(g_i)$ divide $n$ we will have $m \leq n$ . My problem is that I can't seem to prove that there is a strict inequality $m < n$ , since in taking the least common multiple we consider the maximum exponent for each of the primes $p_1, p_2, ..., p_j$ in the prime factorization of $n$ .  So, it seems very plausible that we could have $m = n$ .  What am I missing?","['cyclic-groups', 'finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4537211,Volume comparison for scalar curvature,"I'm reading Gromov's article "" Four lectures on scalar curvature "". On the page 7, he states a volume comparison theorem for small balls without proof. I want to know how to prove it or where I can find its proof. The article can be found on arXiv:1908.10612v6. I would like to restate this theorem here: If the scalar curvature of $n$ -dimensional manifolds $X$ and $X'$ at some points $x\in X$ and $x'\in X'$ are related by the strict inequality $$Sc(X)(x)<Sc(X')(x')$$ then the Riemannian volumes of the $\varepsilon$ -balls around these points satisfy $$vol(B_x(X,\varepsilon))>vol(B_{x'}(X',\varepsilon))$$ for all sufficiently small $\varepsilon>0.$","['differential-topology', 'reference-request', 'riemannian-geometry', 'differential-geometry']"
4537231,"Provided that $A$ is a $n \times n$ real matrix with $A^2＝0$, is it true that $\operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A)$?","Question: Provided that $A$ is a $n×n$ real matrix with $A^2＝0$ , is it true that $\operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A)$ ?
I believe it’s true. I’ve tried matrix operations, Jordan standard form and dividing $A$ into column vectors, but all failed.
Now I’m aware that linear mapping might be a good choice, because $Ax$ and $A^\text{T}x$ are perpendicular, so we can prove that $\ker(A+A^\text{T})=\ker(A)\cap\ker(A^\text{T})$ .
Then we need to prove that $\dim(\ker(A+A^\text{T}))=n-2 \operatorname{rank}(A)$ , but I don’t know how.","['matrices', 'matrix-rank', 'linear-algebra']"
4537258,"an example of tangent equation calculation, from a book, that I do not understand","I am in trouble with an example of an equation of a tangent from a book. Here's what my book is writing (in french) : I translate it (summarizing a bit) : take a T(X,Y) point on the tangent, the slope between M and T is $\frac{Y - f(x_{0})}{X - x_{0}}$ This slope is also the derived number on M, $f'(x_{0})$ : $\frac{Y - f(x_{0})}{X - x_{0}} = f'(x_{0})$ The relationship between the coordinates (X,Y) of T are thus $Y - f(x_{0}) = f'(x_{0}).(X - x_{0})$ But here is the sample given, that troubles me : What the equation of the tangent in $\mathbf{x_{0} = 2}$ to the parabole of equation $\mathbf{y = x^2}$ ? ( this drawing isn't from the author, it's mine, to figure what is $y = x^2$ , and what $x_{0} = 2$ or $x_{0} = 3$ would then mean ) On $x_{0} = 3, y_{0} = 9$ ; the derivative being $y' = 2x$ , we have $y_{0}' = 6$ First question : why does the author assign $x_{0} = 3$ if he said he is looking for $x_{0} = 2$ the line just before? reassigning an $x_{0}$ looks strange to me. According to 8.7, the equation to the tangent on $x_{0} = 2$ Here, $x_{0}$ returns to its previous assignment : $x_{0} = 2$ ... to the parabole is : $Y - 9 = 6(X - 3)$ or $Y = 6X - 9$ It's very troublesome. Especially because when I check with M(2,4), $y_{m} = 6x_{m} - 9$ with $x_{m}=2$ gives $y_{m} = 6 \times 2 - 9 = 12 - 9 = 3$ which is not on the curve, and I am supposed to be on the tangent equation. If with $y = x^2 $ M has for coordinates M(2,4), why this tangent equation is returning me a point of coordinates (2, 3) for it?","['tangent-line', 'derivatives']"
4537265,Matlab code for finding exact solution,"I am trying to find an exact solution to this problem $$y''=\frac18(32+2x^3-yy')$$ and I get ""Warning: Unable to find symbolic solution."" What is my mistake? % define symbolic variables
syms y(x)
eqn = diff(y,2) == (1/8)*(32+(2*x^3)-y*diff(y))
% Solution without initial conditions
sol = dsolve(eqn)
% Define initial conditions
conds = [y(1) == 17, y(3) == 43/3]
% Get solution with initial conditions
sol_conds = dsolve(eqn, conds)","['matlab', 'ordinary-differential-equations']"
4537277,Clarifications on differentiability and continuity,"If there is a function $f(x)$ such that f'(x) has a jump discontinuity: $f(x)=\begin{cases}
        2x & 0 \leq x \leq 1 \\
        3x-1 & 1 < x \leq 2 \\
    \end{cases} $ then what is the derivative at $x=1$ ? The graph looks like this: (orange is $f(x)$ and purple is $f'(x)$ )","['continuity', 'calculus', 'derivatives']"
4537332,What is the probability of a random line passing through the unit disk meet the y-axis?,"This is a question that I came up myself. So it might have some of the errors pointed in the comments below. Consider the open unit disk centered at (0,0) and the set of all straight lines of the plane that intersect the y-axis. Hence, we are consedering the set of straight lines that have the form: y=lx+m, $l\in\mathbb{R}, m\in\mathbb{R}$ . What is the probability of a randomly selected line that passes through the unit disk to intesect the y-axis inside the unit disk? I don't really know how to approach this kind of problems. At first I try to see which lines pass through the unit disk. The coefficients must obey $|m|<\sqrt{l^2+1}$ . To intersect the y-axis inside the unit disk we must have $|m|<1$ . Thus, for a fixed $l\in\mathbb{R}$ the proportion of the lines that meet the y-axis inside the unit disk is $1/\sqrt{l^2+1}$ (lenght of (-1,1)/ length of $(-\sqrt(l^2+1),\sqrt(l^2+1))$ . Now the probability must be the mean value of $1/\sqrt{l^2+1}$ for $l\in(-\infty,+\infty)$ , which is the integral of this function over that interval. But the integral $$\int\limits_{-\infty}^{\infty}\dfrac{1}{\sqrt{l^2+1}}=\infty.$$ Hence, this reasoning - which seems right to me - doesn't lead to an answer. I don't really know how to approach this kind of problems.
Do you have any hints? Suggestions? Or maybe an answer?","['euclidean-geometry', 'probability']"
4537339,Glivenko-Cantelli-type question: For what *non-independent* random variables $(X_i)$ does $\| \hat F_n - F\|_\infty \overset{P}{\longrightarrow} 0$?,"Setup Let $\mathbf X = (X_1, X_2, \dots)$ be a sequence of identically distributed random variables on a probability space $(\Omega, \mathcal F, P)$ with the common marginal distribution (function) $F$ . Set the empirical distribution function of $X_1, \dots, X_n$ as $\hat F_n: (\Omega, \mathbb R) \to [0,1]$ , given by $$
F_n^\omega (t) := \frac 1n \sum_{j=1}^n \mathbf{1} \big\{ X_j(\omega) \leq t \big\}.
$$ The Glivenko-Cantelli theorem tells us that if the $X_i$ are independent and identically distributed (i.i.d.), then $$
\| \hat F_n - F \|_\infty \overset{\text{def}}{=} \sup_{t \in \mathbb R} \Big | \hat F_n(t) - F(t) \Big| \overset{\text{a.s.}}{\longrightarrow} 0. 
$$ Question Are there known weaker assumptions on $\mathbf X$ than independence of the $X_i$ under which $\hat F_n$ converges to $F$ uniformly in probability? That is: $$
\| \hat F_n - F \|_\infty \overset{P}{\longrightarrow} 0.
$$ Note: I don't necessarily need a proof here. References would suffice.","['probability-limit-theorems', 'probability-distributions', 'probability-theory']"
4537356,How do I find the second order moment with a MGF?,"Let $X$ be a random variable such that $M_X(t)=e^t M_X(-t)$ . Find $E(X)$ and $E\left(X^2\right)$ . I know the general procedure that to find the $n$ th moment, the $n$ th order derivative needs to be taken and then t must be set to 0. $M_X(t)^{\prime}=e^t M_X(-t)-e^t M_X^{\prime}(-t)$ The solution is $E[X] = \frac{1}{2}$ Question 1: For the second order moment: \begin{aligned}
&M_X^{(2)}(t)=e^t\left(M_X^{(2)}(-t)-2 M_X^{(1)}(-t)+M_X(-t)\right) \\
&M_X^{(2)}(0)=M_X^{(2)}(0)-2 M_X^{(1)}(0)+M_X(0)
\end{aligned} If I differentiate two times, It seems like there is no expression for the second derivative. Does this mean that $E[X^2]$ is undefined?","['moment-generating-functions', 'probability']"
4537364,"$(2,p)$ generation of alternating group $A_p$","Let $p>7$ be a prime, $a=(1,2,3,...,p)$ , $b$ is the product of two arbitrary transpositions such as $b=(2,6)(3,5)$ . I found that when $p=11,13,17,19$ , the group $\langle a,b \rangle$ is always $A_p$ (use Magma), I want to know whether it is right for all prime numbers $p$ . I tried the Jordan's theorem but can't find a short cycle for all situations. Can you prove it or find a counterexample for it?
Thanks a lot for any help!","['permutations', 'finite-groups', 'abstract-algebra', 'group-theory', 'group-actions']"
4537367,$\lim_{n\to\infty}\int_0^\infty \frac{f(x)}{x^2}\sin nx dx=\frac{\pi}{2}f'(0)$,"Let $f$ be twice continuously differentiable on $[0,\infty)$ , and bounded, with $f(0)=0$ . Show that $\lim_{n\to\infty}\int_0^\infty \frac{f(x)}{x^2}\sin nxdx=\frac{\pi}{2}f'(0).$ My attempts: Using $\int_0^\infty \frac{\sin t}{t}dt=\frac{\pi}{2}$ to find $$\left|\int_0^\infty \frac{f(x)}{x^2}\sin nxdx-\frac{\pi}{2}f'(0)\right|
=\left|\int_0^\infty \left[\frac{f(t/n)-f(0)}{t/n}-f'(0)\right]\frac{\sin t}{t}dt\right|$$ For the part $t\leq A$ for fixed $A>0$ it is OK, but for $t>A$ , any ideas? Another try is $$\left|\int_0^\infty \frac{f(x)}{x^2}\sin nxdx-\frac{\pi}{2}f'(0)\right|
=\left|\int_0^\infty \frac{f(x)-xf'(0)}{x^2}\sin nxdx\right|$$ The function $\frac{f(x)-xf'(0)}{x^2}$ has limit $f''(0)/2$ as $x\to0^+$ . Oh, it seems that the assumptions are not sufficient, any help? What condition should we add to prove this above limit? And the proof?","['integration', 'limits', 'calculus']"
4537401,Compact simple group which is not a Lie group,What is an example of compact topological group $ G $ which is abstractly simple ( $ G $ has no proper nontrivial normal subgroups) but is not a Lie group?,"['simple-groups', 'group-theory', 'topological-groups', 'examples-counterexamples']"
