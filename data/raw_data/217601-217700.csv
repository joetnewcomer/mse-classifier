question_id,title,body,tags
4438369,Equivalence of conditional expectations wrt two sigma-algebras,"Let $X$ be a random variable and let $\mathcal{G}$ , $\mathcal{H}$ be two sub- $\sigma$ -algebras. Consider the equation $$\mathbb{E}(\mathbb{E}(X|\mathcal{G})|\mathcal{H}) = \mathbb{E}(X|\mathcal{G}\cap\mathcal{H}) \text{ almost surely.}$$ I am tasked to show that the above holds in the following three cases: $\mathcal{G}\subseteq\mathcal{H}$ , $\mathcal{H}\subseteq\mathcal{G}$ , $\mathcal{G}$ and $\mathcal{H}$ are independent. If we let $Y = \mathbb{E}(\mathbb{E}(X|\mathcal{G})|\mathcal{H})$ and take $A\in\mathcal{G}\cap\mathcal{H}$ , it appears (unless I am making a mistake) that $\mathbb{E}(Y\mathbb{I}(A))=\mathbb{E}(X\mathbb{I}(A))$ (forgive the ugly indicator notation) holds generally, not just in the three cases above. Thus it remains to show that $Y$ is $(\mathcal{G}\cap\mathcal{H})$ -measurable in the three cases, but I only see how to do this for the case $\mathcal{H}\subseteq\mathcal{G}$ , as then $\mathcal{H}=\mathcal{G}\cap\mathcal{H}$ and $Y$ is of course $\mathcal{H}$ -measurable. Any help for the other two cases would be greatly appreciated! Also perhaps an example of why this does not hold generally would be useful if someone could provide one.","['measure-theory', 'conditional-expectation', 'expected-value', 'probability-theory', 'random-variables']"
4438370,"prove or disprove: for every function $f : \mathbb{N}\to \mathbb{N}, 0\leq f(n) \leq n,$ the graph of $f$ has an infinite set of collinear points","Let $\mathbb{N} = \{0,1,\cdots\}$ . Prove or disprove: for every function $f : \mathbb{N}\to \mathbb{N}, 0\leq f(n) \leq n,$ the graph of $f$ has an infinite set of collinear points. I think the claim is false and a counterexample is $f(x)=\lfloor \sqrt{x}\rfloor$ . Clearly for all n in $\mathbb{N}$ , $0\leq f(n)\leq \sqrt{n}\leq n$ . Intuitively, one can't draw a line passing through infinitely many point of the function. I think it might be useful to use a proof by contradiction. Assume such a line $L$ does exist. Clearly $f(x)$ takes the same value for $x\in [k^2, (k+1)^2 - 1]$ for some nonnegative integer $k$ . Let the equation of $L$ be $y=mx +b$ . Then by assumption there are infinitely many points so that $\lfloor \sqrt{x}\rfloor = mx + b$ . Could one derive a contradiction by showing that $\dfrac{\lfloor \sqrt{x}\rfloor - b}x = m$ has only finitely many solutions in positive integers, and if so, how?","['functions', 'geometry']"
4438418,Hermitian metrics on the Associated Vector Bundle of a Principal $U(1)$-bundle,"If a complex line bundle $L$ over some manifold $M$ has a Hermitian metric, then $M$ has a frame bundle, which is a principal $U(1)$ -bundle over $M$ . Now reverse this, if we have a principal $U(1)$ -bundle over $M$ , we then have an associated line bundle $L$ . Does this induce a Hermitian metric on $L$ ? Is it canonical?","['principal-bundles', 'vector-bundles', 'differential-geometry']"
4438429,Are there non-surjective functions in ZFC theory?,"If I understand correctly, in ZFC set theory, a function is just a set of ordered pairs with certain properties. The codomain set is just the set of all its 2nd co-ordinates (ordinates). There is no distinction made between the codomain and what is commonly called the range of a function. So, every function in ZFC set theory would be surjective. Is that correct?","['elementary-set-theory', 'functions']"
4438474,Big-$O$ confusion,"$f(x) = O(g(x))$ means that $\frac{f(x)}{g(x)}$ is bounded. But $x=O(x^2 + 1),\ x\in  \mathbb R$ while $x\ne O(x^2)$ . Is there a human friendly explanation of what $O$ is?
the definitions I saw are as follows: f, g: E → R, a is a limit point of E. If there are $\dot{U}_a$ and a function ϕ: E → R, f(x)=ϕ(x)⋅g(x) and ϕ(x) is bounded in $\dot{U}_a$ ∩E , then f = O(g) f, g: E→R. ∃ C> $0$ : |f(x)| ≤ C ⋅ |g(x)| ⇒ f=O(g) in E
and examples: sin x = O(x) $x \in R$ cos x ≠ O(x) $x \in R$ x ≠ O(sin x) $x \in R$ x = O( $x^2+1$ ) $x \in R$ x ≠O( $x^2$ ) $x \in R$",['analysis']
4438497,Negative Tetrations?,"To start, I'll say that for this post I'll be using Rudy Rucker notation for tetration. That being $^2$ x= $x^x%$ , which means the number raised to the left means how many times one would exponentiate x. Ok, so now to see my logic I'll start with a pattern: $^4x$ = $x^{x^{x^x}}$ = $x^{x^{x^{x^1}}}$ $^3x$ = $x^{x^{x}}$ = $x^{x^{x^1}}$ $^2x$ = $x^x$ = $x^{x^1}$ $^1x$ = $x$ = $x^1$ Take note how for each tetration we go down by one, the previous exponent just reverts to a one, so this pattern must be true for the $0$ th tetration of x. $^0x$ = $1$ = $1^1$ Now, my question, what about for $^{-1}x$ . I didn't even know if this was possible, after jut a quick scour on the internet I ended up with zilch. So I began looking for a nice formula to help. Then I came across this nice formula: $\frac{d}{dx}$ [ $^nx$ ]=( $^nx$ ) $\left(\frac{^{n-1}x}{x}+\frac{d}{dx}[^{n-1}x]ln(x)\right)$ Assuming n=0, and plugging in, we know from before that $^0x$ =1 therefore $\frac{d}{dx}$ [ $^0x$ ]= $0$ , one returns that: $0$ = $^{(n-1)}x$ + $\frac{d}{dx}$ [ $^{(n-1)}x$ ] * ln $(x)$ * $x$ To solve this problem I just substituted $u=^{(n-1)}x$ . So my questions arise, Am I allowed to make this substitution? Even if I do, do I change the variable of differentiation? If this method I am using is not viable, is there some other method that I can use, or is it truly impossible and I just wasted a few days of my life?","['calculus', 'tetration', 'derivatives']"
4438522,(convergence and probability) If $X_n$converges on the probability in $X$. Prove...,"If $X_n$ converges on the probability in $X$ . Prove a) (using only the definition of convergence with probability) For every $ \epsilon_k \to 0$ when $k \to \infty$ , that there exists a $n_k$ such that $P(|X_n − X| \gt \epsilon_k) \lt \frac{1}{2^k}$ by $n \gt n_k$ b)(using a) and Borell-Cantelli thm 1. Prove that $X_{n_k}$ converges on $X$ with probability $1$ . a) The definition of convergence with probability $\lim_{}P(|X_n − X| \gt \epsilon) = 0$ so we can choose any $\epsilon_k$ such that $P(|X_n − X| \gt \epsilon_k) = 0\ \text{more specifically}\ k=1 \to P(|X_n − X| \gt \epsilon_1) \lt \frac{1}{2^1} \to 0 \lt \frac{1}{2}$ b) If the sum $P(A_n) \lt \infty$ then $P(|X_n − X| \gt 1 = A_n) \lt \frac{1}{2^k} \to \sum A_n \le \sum \frac{1}{2^k}$ That was all I was able to think. I know it's incomplete,but what I've done so far is correct? any thought from anyone. Thanks.","['borel-cantelli-lemmas', 'probability-limit-theorems', 'solution-verification', 'probability-theory']"
4438526,Finding roots of a polar trigonometric equation,"I have an equation which I am using to describe the squared distance from a polar point $(r_1, \theta_1)$ to a function $g(\theta)$ which is $C_1$ smooth over the period $0-2\pi$ . $$r = r_{1}^{2}+g\left(\theta\right)^{2}-2r_{1}g\left(\theta\right)\cos\left(\theta-\theta_{1}\right)$$ To find local minima and maxima we differentiate w.r.t. $\frac{dr}{d\theta}$ and find the roots of this derivative. I believe this is: $$r = 2g\left(\theta\right)g'\left(\theta\right)-2r_{1}\left(g\left(\theta\right)\cdot-\sin\left(\theta-\theta_{1}\right)+g'\left(\theta\right)\cos\left(\theta-\theta_{1}\right)\right)$$ How do I find roots for trig functions which don't boil down to textbook trig identities? I looked into Chebyshev Polynomials but it looks like those are still only helpful for rootfinding if they are free of trigonometric functions. Notes: $g(\theta) = sin(z\theta)+s$ , though I'd like to stay general to any function which is $C_1$ smooth and has a period of $2\pi$ I'd also like to only find local minima, so I believe this will require the second derivative, but at least not require root finding on that derivative.","['trigonometry', 'roots', 'polar-coordinates']"
4438530,"Uniform converges of $f_n(x)=x-\frac{x^n}{n!} , x\in [-a,a] ,a>0$","I have 2 questions  : 1.with the given following function series : $f_n(x)=x-\frac{x^n}{n!}$ I need to show that for $x\in [-a,a]$ $ ,a>0 $ there is $P,Q>0$ such that : $|f_n(x)-f_{n-1}(x)|\leq PQ^{n-1}\frac{|x|^n}{n!}$ and I stack at this point : $$\begin{align} |f_n(x)-f_{n-1}(x)|&=\left|x-\frac{x^{n+1}}{(n+1)!}-\left(x-\frac{x^{n}}{n!}\right)\right|\\&=\left|x-\frac{x^{n+1}}{{n+1}!}-x+\frac{x^{n}}{n!}\right|\\&=\left|\frac{x^n}{n!}-\frac{x^{n+1}}{(n+1)!}\right|\\&=\left|\frac{x^n}{n!}\left(1-\frac{x}{n+1}\right)\right|\\&=\left|\frac{x^n}{n!}\right|\left|1-\frac{x}{n+1}\right|\\&=\frac{|x|^n}{n!}\left|1-\frac{x}{n+1}\right|\end{align} $$ so I need to prove that : $|1-\frac{x}{n+1}|\leq PQ^{n-1}$ then I tried : $|1-\frac{x}{n+1}|\leq 1+|\frac{x}{n+1}|\leq 1+\frac{|x|}{n+1} \leq 1+\frac{a}{n+1} $ from here I stack. My second question is to show that $f_n(x)$ converges unifomly at $x\in [-a,a]$ $ ,a>0 $","['functions', 'uniform-convergence']"
4438531,Why can't we use Yoneda Lemma to get a representation theorem for Rings?,"I'm new to category theory. I am trying to understand how Yoneda Lemma is a generalization of representations theorems in algebra. Cayley theorem can be interpreted as a instance for the case where the category $\mathcal{C}$ is the one with only one element $*$ and  arrows are iso arrows that correspond to the elements of $G$ (i.e., it is a grupoid. I am using this wiki-page as reference) . From the Yoneda Lemma we can establish the Cayley theorem. But there is another result from algebra that says that there isn't a representation theorem for Rings in general. (I don't understand why this is the case, I just heard it). There is only a representation theorem if we restrict to the context of Boolean algebras (the Stone Theorem). Why we can't just apply Yoneda lemma in some way to get a general result for Ring theory? (Not the Stone theorem) I mean, in the context of groups we've used the grupoid category. Why we can't consider something like it in Rings and apply the Yoneda? Some category like a ``ringoid'' (sorry for this word). Maybe there is to much happening here, but I think some intuition of this might help understand better the nature of Yoneda Lemma. Edit: the discussion in the comments and in this link pointed me that the result I mentioned is false: there is a Cayley on Rings. I will not edit the original text question. So now I think the question is: How do we get this version of Cayley on Rings with Yoneda? I still very confused with all this new information, and I didn't understand the answer there.","['abstract-algebra', 'category-theory']"
4438533,Inaccuracy in Schilling Proposition 6.5,"I found an inaccuracy in the Proposition 6.5 of the book ""Measures, Integrals and Martingales"" From René Schilling [EDIT: I'm talking about the first edition of the book] and I would like to correct this error. To be sure that I'm clear in asking the question I will list the definitions I use in this post so that there are no notational misunderstandings. I will try to make sure that even those unfamiliar with the book can answer the question. But obviously those who have studied from that book are more likely to be able to answer the question. I organized this post in the following way: In the first part there's the question. In the second part there is the list of definitions and notations that I use, so if you come across a symbol or a definition that you do not know go to read the second part. In the third part there is the reason that led me to formulate the question, how the question is related to proposition 6.5 and what was my attempt to answer the question. FIRST PART: THE QUESTION [Edit: as suggested I changed the name of the ""Theorem 0.2"" calling it ""Statement 0.2""] Statement 0.2 : Let $X$ be a set, $\mathcal F$ a semi-ring on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a pre-measure on $X$ if and only if [ \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset , A \cup B \in \mathcal F \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} ] End of the Statement 0.2 MY QUESTION IS : is Statement 0.2 True? And what is the proof of the implication ""from the right to the left""? If this implication of Statement 0.2 is not true what is an example of a set $X$ with a semi-ring $\mathcal F$ and a function $\mu$ on $\mathcal F$ which satisfies properties (i), (ii), (iii'') of Statement 0.2 but such that $\mu$ is not a pre-measure on $X$ ? I could only prove the implication ""from the left to the right"" of theorem 0.2 but i couldn't prove the viceversa. If you want to see a sketch of the proof of the implication i proved, you can find it in the third part of the post. SECOND PART: Notations and  definitions I use if $X$ is a set and $\mathcal F \subseteq \mathcal P (X)$ then we define: $\mathcal F$ is a ring on $X$ if and only if (by definition) [ \begin{split} 
	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\
	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies B \setminus A \in \mathcal{F} \\
	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cup B  \in \mathcal{F} 
\end{split} ] $\mathcal F$ is a semiring on $X$ if and only if (by definition) [ \begin{split} 
	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\
	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies \exists A_1,A_2,\dots,A_n \in \mathcal{F} \text{ pairwise disjoint } : \,\; B \setminus A = \bigcup_{k=1}^{n}{A_k}\\
	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cap B  \in \mathcal{F} 
\end{split} ] it is easy to prove that a ring on $X$ is also a semiring on $X$ If $X$ is a set, $ \mathcal F \subseteq \mathcal P (X) : \emptyset \in  \mathcal F$ and $\mu :  \mathcal F \to [0,+\infty] $ then $ \mu $ is a pre-measure on $(X,  \mathcal F )$ [or simply on $X$ ] if and only if (by definition) [ \begin{split} 
	&1)  \hspace{0.5cm}\mu(\emptyset) = 0 \\
	&2) \hspace{0.5cm} A_1,A_2,\dots \in \mathcal{F} \text{ pairwise disjoint , } \bigcup_{n\in\mathbb{N}}{A_n} \in \mathcal{F} \implies \mu\bigg( \bigcup_{n \in \mathbb{N}}{A_n}\bigg) = \sum_{n=1}^{\infty}{\mu(A_n)}\\
\end{split} ] so basically a pre-measure is the same as a measure with the only differences that a pre-measure isn't necessarily defined on a $\sigma$ -agebra and the $\sigma$ -additivity holds only when the union of the sets is in the domain of the function. Moreover $\mathcal J ^n$ is the set whose elements are the subset of $\mathbb R ^n$ in the form $[a_1,b_1) \times ... \times [a_n,b_n)$ where $a_i,b_i \in \mathbb R \forall i = 1,...,n$ THIRD PART: Correlation of the question with proposition 6.5 and my attempt to answer the question Now i will list some theorems of the Schilling required to understand the problem with proposition 6.5 and how it is related to Statement 0.2 Theorem 4.4 : Let $X$ be a set, $\mathcal F$ a $\sigma$ -algebra on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a measure on $X$ if and only if [ \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N : A_n \subseteq A_{n+1} \forall n \in \mathbb N  \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N   \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} ] end of the Theorem 4.4 Then the book states that Theorem4.4 is still valid in a more general case, regarding pre-measures instead of measures. But, due to the fact that the proof of the Theorem 4.4 requires that the family $\mathcal F$ is closed under finite intersection, union and difference of sets, then this more general theorem is true in the hypotesis that $\mathcal F$ is a ring on $X$ . I could easily prove this more general theorem, I'll copy the statement below under the name of Theorem 0.1 Theorem 0.1 :  Let $X$ be a set, $\mathcal F$ a Ring on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a pre-measure on $X$ if and only if [ \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} ] End of theorem 0.1 Now the problems begin. The book defined the function $\lambda ^n : \mathcal J ^n \to [0,+\infty)$ that associates each element of $\mathcal J ^n $ to its volume. The book also proved that $\mathcal J ^n$ is a semi-ring on $\mathbb R^n$ (Proposition 6.4) My problem with the Proposition 6.5 is that the book wants to prove that $\lambda ^n$ is a pre-measure on $\mathcal J ^n$ using some sort of Theorem 0.1 but the problem is that $\mathcal J ^n$ is not a Ring on $\mathbb R^n$ but only a semi-ring. Let me be clearer,
this is the proposition6.5 of the book: Proposition 6.5 : $\lambda ^n$ is a pre-measure on $\mathcal J ^n$ Proof : the proof of the book consists in showing that the properties (i) (ii) (iii'') of the theorem 0.1 holds for $\lambda ^n$ and i'm ok with this proof (obviously you need to modify the property (ii) requiring that $A \cup B \in \mathcal J ^n$ ) but the problem is that then he uses a not well specified version of the theorem 0.1 to conclude that $\lambda ^n $ is a pre-measure on $\mathbb R^n$ . But, again, you can't use theorem0.1 because $\mathcal J ^n$ is not a Ring on $\mathbb R^n$ but only a semi-ring. What was my attempt to solve this issue? Firstly I have formulated a version of the theorem suitable for semi-rings which goes under the name of Statement 0.2 (You've read it in the first part of the post) I will shortly show how i proved the first implication of the Statement 0.2 (The implication ""from the left to the right""): First of all I proved the following theorems: Theorem 0.3 : Let $X$ be a set and $\mathcal F \subseteq \mathcal P (X)$ then it does exist the smallest Ring on $X$ containing $\mathcal F$ and it's denoted with the simbol $R(\mathcal F)$ Theorem 0.4 : Let $X$ be a set and $\mathcal F$ a semi-ring on $X$ then $R(\mathcal F)$ is the set of all the finite disjoint unions of elements of $\mathcal F$ Theorem 0.5 : Let $X$ be a set and $\mathcal F$ a semi-ring on $X$ and $\mu : \mathcal F \to [0,+\infty] $ a pre-measure on $X$ then there exists and is unique an extension of $\mu$ to $R(\mathcal F)$ such that this extension is a pre-measure. Proof of the implication from the left to the right of Statement 0.2 : if $\mu$ is a pre-measure on the semi-ring $\mathcal F$ then denoted the unique extension of $\mu$ to $R(\mathcal F)$ still with $\mu$ we get the assertion thanks to Theorem0.1","['measure-theory', 'solution-verification', 'analysis', 'real-analysis']"
4438559,"Does there exist a polynomial $P(x,y)$ which detects all non-squares?","Problem. Does there exist a two-variable polynomial $P(x, y)$ with integer coefficients such that a positive integer $n$ is not a perfect square if and only if there is a pair $(x, y)$ of positive integers such that $P(x, y)=n$ ? Context. The answer is positive for polynomials in 3 variables! This appeared as a problem in USA Team Selection Test in 2013. It turns out that the polynomial $P(x, y, z)=z^2\cdot (x^2-zy^2-1)^2+z$ enjoys the following property: a positive integer $n$ is a not a perfect square if and only if $P(x, y, z)=n$ has a solution in positive integers $(x, y, z)\in \mathbb{N}^{3}$ . This construction works nicely due to Pell's equation. If $n$ is not a perfect square, then Pell's equation $x^2-ny^2=1$ has a solution in positive integers $(x_0, y_0)$ , and so we get $P(x_0, y_0, n) = n$ . Conversely, if $P(x, y, z)=n$ , then one can show that $n$ cannot be a perfect square because $n=z^2(x^2-zy^2-1)^2+z$ can be squeezed between two consecutive perfect squares: $$
 (z(x^2-zy^2-1))^2 < n < (z(|x^2-zy^2-1|+1)^2
$$ Remark. It is clear that there is no single-variable polynomial $P(x)$ which could achieve the desired property. Indeed, there are arbitrary number of consecutive non-squares, and a polynomial $P(x)$ of degree $n>1$ cannot output a consecutive list of $n+1$ numbers. This last claim itself is a nice problem; for a solution, see Example 2.24 in page 11 of Number Theory: Concepts and Problems by Andreescu, Dospinescu and Mushkarov.","['contest-math', 'number-theory', 'polynomials']"
4438581,Perfect semi direct products,"Let $ \pi,V $ be a representation of a perfect group $ G $ . I'm interested in sufficient conditions for a semi direct product like $ V \rtimes_\pi G $ to be perfect. Requiring that $ \pi $ is faithful and or irreducible isn't enough because of trivial counterexamples using abelian groups. I'm especially interested in the condition that $ \pi $ is fixed point free (the only vector $ v $ fixed by all of $ G $ is the zero vector). Is that sufficient for $ V \rtimes_\pi G $ to be perfect? Motivation: Let $ V $ be the $ n-1 $ dimensional vector space of length $ n$ binary vectors whose entries sum to 0. There is a natural permutation representation $ \pi $ of the alternating group $ A_n $ on $ V $ . This representation is fixed point free, in fact it acts transitively on the nonzero vectors. For $ n \geq 5 $ the semdirecrt product $ V \rtimes A_n $ is perfect. Consider the Weil representation of $ Sp_{2n}(p) $ on $ V=\mathbb{F}_p^{2n} $ for $ p $ an odd prime. This representation is fixed point free, in fact it acts transitively on the nonzero vectors. $ V \rtimes  Sp_{2n}(p) $ is perfect (except when $n=1,p=3 $ in which case the symplectic group is not perfect). Is it good enough for $ \pi $ to be fixed point free? Or do we need that $ G $ is transitive on the nonzero elements of $ V $ ? If you have some totally different sufficient condition for a semi direct product to be perfect feel free to share, these are just some thoughts. And more generally I'm interested in any condition on a (possibly non split) extension of a perfect group $ G $ $$
1 \to V \to E \to G \to 1
$$ Which is sufficient to imply that $ E $ is perfect. EDIT: Really nice sufficient condition from Derek Holt: Let $$
1 \to V \to E \to G \to 1
$$ be a SES with $ V $ abelian and $ G $ perfect. Then if $ V $ is a (nontrivial) irreducible $ G $ module then $ E $ must be perfect. The statement for semi direct products is just the special case when the sequence splits. Requiring nontriviality of the module just avoids the case that $ E $ is a direct product of $ G $ with a cyclic group of order $ p $ , which is technically an irreducible $ G $ module.","['representation-theory', 'finite-groups', 'group-extensions', 'semidirect-product', 'group-theory']"
4438587,Doubt with Socle and O'Nan-Scott Theorem.,"The following is the statement of O'Nan-Scott Theorem. Theorem : Let $G$ be a finite primitive group of degree $n$ , and let $H$ be the socle of G. Then either (a) $H$ is a regular elementary abelian $p$ -group for some prime $p$ , $n=p^m:=|H|$ , and $G$ is isomorphic to a subgroup of the affine group ${\rm AGL}_m(P)$ ;  or (b) H is isomorphic to a direct power $T^m$ of a nonabelian simple group $T$ and one of the following holds: (i) m = 1 and G is isomorphic to a subgroup of Aut(T); (ii) $m\geq2$ and $G$ is a group of ""diagonal type"" with $n = |T|^{m-1}$ . (iii) $m\geq2$ and for some proper divisor $d$ of $m$ and some primitive group $U$ with a socle isomorphic to $T^d$ , $G$ is isomorphic to a subgroup of the wreath product $U\,\wr\, {\rm Sym}(m/d)$ with the product action, and $n=l^{m/d}$ , where $l$ is the degree of $U$ . (iv) $m\geq6$ , $H$ is regular, and $n=|T|^m$ . Question: In the Case (b), the socle of $G$ which is $H$ is isomorphic to a direct power $T^m$ of a nonabelian simple group $T$ . In the case (b)(iii), does any bounds on the order of $H$ in terms of $n$ is known ? (e.g. in part(b)(ii), $|T|^{m-1}=n)$ . Note: Trivial bound is $|H| \leq |G| \leq n^{(\sqrt n)\log n}$ . (Thus I am asking about some nontrivial one like $|H|\leq n^c$ or $n^{poly(\log n)}$ ). Edit: See page 126 of the book ""Finite permutation group"" , where it is mentioned that (last line of the first paragraph), the Groups of types (b)(ii) and (b)(iii), are generally distinguished as having a small orders of socle with respect to their degree.","['permutations', 'socle', 'group-theory', 'finite-groups']"
4438612,How to show $\int_0^1 \frac{\ln\frac{1+a}{1+x}}{a-\frac1a{x^2}}dx=\frac{\pi^2}{24}-\frac14\text{Li}_2\left[\left(\frac{a-1}{a+1}\right)^2\right] $,"I came across this polylogrithmic integral that evaluates to a close form of dilogarithmic value $$\int_0^1 \frac{\ln\frac{1+a}{1+x}}{a-\frac1a{x^2}}dx=\frac{\pi^2}{24}-\frac14\operatorname{Li}_2\bigg[\bigg(\frac{a-1}{a+1}\bigg)^2\bigg]
$$ for $a>1$ . So far, I am not able to derive it. Knowing the form of the result, I expect to utilize the identity $$\operatorname{Li}_2(z)+\operatorname{Li}_2(-z)= \frac12\operatorname{Li}_2(z^2)$$ But, I was not able to manipulate the integrand to fit it into the desired sum of dilogarithm integrals. It may not be as straightforward as I expected.",['integration']
4438657,Conflicting inflection point conditions in my book,"My 12th grade calculus book mentions that an inflection point is a point that: • Function $f$ in point $(c, f(c))$ has a tangent line. • Function $f$ 's concavity shall change at point $(c, f(c))$ . Although this has been mentioned, in an example, it says that function $\sqrt[3]x$ has an inflection point at $x = 0$ . We know that $\sqrt[3]x$ 's derivative is: $1/(3\sqrt[3](x^2))$ that at point $x = 0$ would not be possible and it would be a Vertical Tangent . Now, I understand that Vertical Tanget is a tangent line for our function but it's not in our derivative's domain. I believe that my problem is with understanding the meaning of an available tangent line cause I thought that if a point has a tangent line, it's probably in the derivative's domain and as a Vertical Tangent wouldn't be available at our target point, the first condition wouldn't be satisfied. What am I missing here? Now based on this, if we say that a function has a tanget line at a certain point, it's possible that the specified point isn't in the derivative's domain, is that correct?","['calculus', 'functions', 'derivatives']"
4438701,"Expectation/mean and variance of change in $x,y$ given noisy polar coordinates","I am working on a robotics project in which I must characterize the uncertain changes in a drone's location as it makes noisy movements in a plane according to some polar coordinates. A precise description of the problem follows... Let $\epsilon_\alpha$ ~ $N(0,\sigma_\alpha)$ and $\epsilon_d$ ~ $N(0,\sigma_d)$ be independent, normal random variables representing the Gaussian noise exhibited whevenever a drone intends to turn (in place) by an angle $\alpha$ and then move forward some distance $d$ . Of course, due to the error embedded in these actions, the actual angle turned and distance moved is $\alpha + \epsilon_\alpha$ and $d + \epsilon_d$ , respectively. Now, bear in mind that whenever the drone turns, it does so with respect to a particular orientation to the $x$ -axis, denoted as $\theta$ , so that the new orientation after the turn is $\theta + \alpha + \epsilon_\alpha$ . Since the turn and distance moved are noisy, the changes in the $x$ and $y$ coordinates of the drone are necessarily noisy and modeled by the random variables $\Delta x = (d+\epsilon_d)\cos(\theta+\alpha+\epsilon_\alpha)$ $\Delta y = (d+\epsilon_d)\sin(\theta+\alpha+\epsilon_\alpha)$ In order to better estimate the drone's location over repeated movements, I can weight estimates of $\Delta x$ and $\Delta y$ at each time step by their respective variances. Thus, I need to figure out What is the expectation and variance of the the random variables $\Delta x$ and $\Delta y$ ? Ultimately, I want to use this information in a GraphSLAM algorithm to assign weights to the drone's movements and better estimate changes in the drone's location.","['statistics', 'probability-distributions', 'multivariable-calculus', 'trigonometry', 'probability-theory']"
4438721,Compute the following integral: $\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx$,"Question: Compute: $\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx$ . This is from Folland's Real Analysis book. If we can find an integrable majorant, then the integral will equal $0$ , since $\lim_{n\rightarrow\infty}\sin(x/n)=0$ .  We can use the Binominal Theorem to show $(1+\frac{x}{n})^n\geq 1+x+(\frac{1}{2}-\frac{1}{2n})x^2$ , by noticing that all the terms will be positive since $x$ only takes nonnegative values, so we just ""cut it off"" after those couple of terms.  Next, since $|\sin x|\leq 1$ , we can say $|\frac{\sin(\frac{x}{n})}{(1+\frac{x}{n})^n}|\leq \frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2}$ , and $\int_0^\infty\frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2}<\infty$ , so we have our integrable majorant, and by Dominated Convergence Theorem we can pass the limit inside the integral giving us a value of $0$ . I am curious if there are any (unseen by me) holes in this argument, or, maybe, are there any other ways of using DCT to solve this?  Thank you! EDIT:
Just a quick note, I wanted to use Bernouli's inequality to get $\frac{1}{(1+\frac{x}{n})^n}\leq\frac{1}{1+n\frac{x}{n}}\leq\frac{1}{1+x}$ ... but $\int_0^\infty\frac{1}{1+x}dx$ doesn't converge.","['lebesgue-integral', 'analysis', 'real-analysis', 'solution-verification', 'convergence-divergence']"
4438732,Inequalities stated in the solution for Brezis Exercise 5.2,"Here are two inequalities stated in the solution given by Brezis for Exercise 5.2 and I am not seeing why this is true: I know that when $p < 2$ , we have $\frac{2}{p} > 1$ and hence we have $$
2(\alpha + \beta)^{\frac{2}{p}} > \alpha^{\frac{2}{p}} + \beta^{\frac{2}{p}}
$$ as $\alpha^{\frac{2}{p}} < (\alpha + \beta)^{\frac{2}{p}}$ and $\beta^{\frac{2}{p}} < (\alpha + \beta)^{\frac{2}{p}}$ . However, this does not seem to be the way to see these two inequalities as there are no constant factors presented here. Moreover, I have no idea how the second inequality is true knowing $\frac{2}{p} < 1$ . It probably has to do with the convexity/concavity of the function $|x|^p$ when $p > 1$ and $p < 1$ . However, I do not see how to justify this. Update: Here is another proof on top of the accepted answer: We first show that $$(\alpha + \beta)^{\frac{2}{p}} > \alpha^{\frac{2}{p}} + \beta^{\frac{2}{p}}$$ for $\alpha, \beta > 0$ if $p < 2$ . In particular, we have $\frac{2}{p} > 1$ . Equivalently, we wish to show that $$
1 > \left(\frac{\alpha}{\alpha + \beta}\right)^{\frac{p}{2}} + \left(\frac{\beta}{\alpha + \beta}\right)^{\frac{p}{2}}.
$$ Equivalently, this is the same as showing the following statement:
If $x, y \in \mathbf{R}$ , $x + y = 1$ , $0 < x, y < 1$ and $r > 1$ , then $$
x^r + y^r < 1.
$$ To see this, simply note that we have $x < 1$ and $r > 1$ . Therefore, we have $x^r < x$ . Moreover, we have $y < 1$ and $r > 1$ . Therefore, we also have $y^r < y$ . This implies that we have $$
x^r + y^r < x + y = 1.
$$ This shows the first inequality. Now to show the second inequality,  for the same reason as above, it is sufficient to show the statement: If $x, y \in \mathbf{R}$ , $x + y = 1$ , $0 < x, y < 1$ and $r < 1$ , then $$
x^r + y^r > 1.
$$ Now since $0 < x < 1$ and $r < 1$ , we have $x^r > x$ . Similarly, since $0 < y < 1$ and $r < 1$ , we have $y^r > y$ . Therefore, we have $$
x^r + y^r > x + y = 1.
$$ Therefore, we are done.","['proof-explanation', 'real-analysis', 'solution-verification', 'functional-analysis', 'inequality']"
4438741,Expected amount of overlap of binary vectors,"Suppose one has $n$ binary vectors $X_1,\ldots,X_n$ of length $G$ , each containing a contiguous series of ones of length $R\ll G$ which have been positioned uniformly at random within the vector, and zeros everywhere else. Suppose we sum these vectors, $Y=\sum_{i=1}^nX_i$ , what is the expected number of entries of $Y$ that are strictly greater than $1$ ? After thinking about this problem for a while without making much headway, I considered the following approximation, $${\bf exercise}(G,n,R)\approx{\bf exercise}(G\;/\;R^2,n,1)$$ Initially, I thought the correct reduction in vector length would be $\frac{G}{R}$ , however after testing it empirically, division by $R^2$ appears to be the correct scaling factor. The solution to ${\bf exercise}(G\;/\;R^2,n,1)$ can be formulated in terms of binomial random variables $B_i$ with probability of success given by $\frac{1}{H}=\frac{R^2}{G}$ , \begin{align}
E\big[\sum_{i=1}^H[B_i>1]\big]=\sum_{i=1}^HE[B_i>1]&=\sum_{i=1}^H\sum_{b_i=0}^n[b_i>1]\binom{n}{b_i}\big(\frac{1}{H}\big)^{b_i}\big(1-\frac{1}{H}\big)^{n-b_i}\\
&=H\Big(1-\sum_{b=0}^1\binom{n}{b}\big(\frac{1}{H}\big)^b\big(1-\frac{1}{H}\big)^{n-b}\Big)\\
&=H\Big(1-\big(1-\frac{1}{H}\big)^n-n\big(\frac{1}{H}\big)\big(1-\frac{1}{H}\big)^{n-1}\Big)
\end{align} Can anyone provide some rigorous justification for this approximation? Or provide a solution to the original problem?","['solution-verification', 'probability-theory', 'probability']"
4438745,Unified explanation for parametrization independence,"Let $\gamma:[a,b]\rightarrow\mathbb{R}^n$ be a $C^1$ curve. We have the following different kinds of line integrals: (i) $\displaystyle\int_a^bf(\gamma(t))|\gamma'(t)|dt$ where $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is a scalar field; (ii) $\displaystyle\int_a^bF(\gamma(t))\cdot\gamma'(t)dt$ where $F:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is a vector field; (iii) $\displaystyle\int_a^bf(\gamma(t))\gamma'(t)dt$ where $n=2$ and $f:\mathbb{C}\rightarrow\mathbb{C}$ is a complex function (identify $\mathbb{C}$ with $\mathbb{R}^2$ ). These are all parametrization independent, in the sense that if we replace $\gamma$ by $\eta=\gamma\circ\alpha$ , where $\alpha:[c,d]\rightarrow[a,b]$ is a $C^1$ increasing diffeomorphism, then the integral does not change. I wonder if there is some ""high-tech"" way to explain them all at once. My attempt: both the dot product $\mathbb{R}^n\times\mathbb{R}^n\rightarrow\mathbb{R}$ and complex multiplication $\mathbb{R}^2\times\mathbb{R}^2\rightarrow\mathbb{R}^2$ are bi-linear. I feel like the following general form is true. If $B:\mathbb{R}^m\times\mathbb{R}^n\rightarrow\mathbb{R}^k$ is a bi-linear map, then $\displaystyle\int_a^bB(f(\gamma(t)),\gamma'(t))dt$ is parametrization independent for $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ ; actually if $G:[a,b]\rightarrow\mathcal{L}(\mathbb{R}^n;\mathbb{R}^k)$ is continuous, then $\displaystyle\int_a^bG(t)(\gamma'(t))dt$ is parametrization independent . If this works then it unifies (ii) and (iii), but I don't see how to also cover (i). Could this be explained using differential form and density (I am still trying to understand their definitions)? Are there generalizations to higher dimensions?","['complex-analysis', 'analysis', 'differential-geometry']"
4438754,"why does the area ""inside"" the graph of $y=\frac{\sqrt{r^{2}-x^{2}}}{\sin x}$ look like a circle","I decided to graph this ratio because both functions have to do with circles and thought the graph may look interesting, but I can't connect the dots.","['trigonometry', 'circles']"
4438779,Understanding $\lambda$ in the definition of Poisson distributions,"I am trying to understand the meaning of $\lambda$ in Poisson distributions. I know that it is the average rate of occurrences of the event, but I have not been able to fully understand what that means. In ""A First Course in Probability"" by Sheldon Ross, the author says that a Poisson distribution may be used as an approximation for a binomial distribution with parameters $(n,p)$ when $n$ is large and $p$ is small enough so that $np$ is of moderate size. What does $np$ being of moderate size mean? What is considered as of moderate size? Why does $np$ have to be of moderate size? Why can a Poisson distribution not be used as an approximation for a binomial distribution if $np$ is too big or too small? Also, in other books, I read that a Poisson distribution is the limiting case of a binomial distribution when $\lambda=np$ is constant as $n\to\infty$ . Under what conditions, is $\lambda$ constant as $n\to\infty$ ? I am new to probability. If someone can provide the intuition behind Poisson distributions (specifically about $\lambda$ ), I would greatly appreciate it.","['poisson-distribution', 'probability-distributions', 'poisson-process', 'probability']"
4438814,An efficient algorithm to generate a set of tuples satisfying a given upper bound for a distance between two arbitrary elements,"Let $T_i^n$ denote a particular tuple of $n$ natural numbers (here $i < n!$ and we assume that the tuple contains all elements of the set $\{0, 1, \ldots, n-2, n-1\}$ , i.e. there are no duplicates). For example, $$\begin{array}{l}
T_0^4 = (0,1,2,3),\\
T_1^4 = (0,1,3,2),\\
 \ldots \\
T_{23}^4 = (3,2,1,0).
\end{array}$$ Assuming that $(x, y)$ are two different elements of a given tuple $T$ , let $d(T, x, y)$ denote the number of elements between $x$ and $y$ in $T$ . For example, if $T = (3, 0, 1, 4, 2)$ , we have $$d(T, 3, 0) = d(T, 1, 0) = d(T, 2, 4) = d(T, 4, 2) = 0, d(T, 3, 2) = 3.$$ Question: given an arbitrary natural number $n > 2$ and a natural number $k$ such that $0 \leq k \le n - 3$ , is there an efficient algorithm to generate a set $$S = \{T_{j_0}^n, T_{j_1}^n, \ldots, T_{j_m}^n\}$$ of tuples such that for any pair $(x, y)$ of elements of the set $\{0, 1, \ldots, n-2, n-1\}$ there exists an element $T_{j_z}^n$ of $S$ such that $d(T_{j_z}^n, x, y) \leq k$ , yet the cardinality of $S$ is as small as possible ? The algorithm is expected to operate as follows: start with $T_0^n$ ; permute $T_0^n$ to obtain $T_{j_0}^n$ ; permute $T_{j_0}^n$ to obtain $T_{j_1}^n$ , etc. (to avoid storing a large number of tuples in memory).","['permutations', 'combinatorics', 'discrete-mathematics', 'algorithms', 'discrete-optimization']"
4438862,"How homeomorphic are noninjective images of $[0,1]$ to $[0,1]$?","I have a continuous function $f:[0,1]\rightarrow \mathbb{R^2}$ . If $f$ we injective, we'd know the image is homeomorphic to $[0,1]$ . Lets consider $S:=\{f(t) : f \text{ injective at }  
t\}$ (i.e $t$ such that $f^{-1}\big(\{f(t)\}\big)=\{t\}$ .) What can we say about the homeomorphism type of S as a subspace of $\mathbb{R^2}$ ? Is $S$ homeomorphic to a subset of $[0,1]$ ? In particular, if $S$ is dense in the image of $f$ , must $S$ be homeomorphic to a dense subset of $[0,1]$ ? Clearly the same theorem doesnt apply since $S$ need not be compact. We could choose big compact subsets of $S$ and show that almost all of $S$ is homeomorphic to some union of compact intervals, therefore embeds in $[0,1]$ nicely.","['general-topology', 'analysis', 'real-analysis']"
4438888,First Order Differential Equation -Variation of Parameters Method,"I'm trying to solve arbitrary current equation for neuron membrane from Gerstner's book. Here it begins: Question: Assuming that before a given time t0 the current is null and the membrane potential is at rest,
derive the general solution to Eq. (1) for arbitrary I(t). My Solution: Variation of Parameters method is used to solve differential equation: $$\tau\frac{du}{dt} = \ - (u(t) - u_{\text{rest}}) + RI(t)\quad \tag1 $$ $$\tau\frac{du}{dt} +u(t) =0 \implies \tau\frac{du}{dt}=-u(t)\implies \int \frac{du}{u}=-\int\frac{dt}{\tau}$$ $$u(t)= e^{-t/{\tau}}\cdot C \implies u(t)=C(t)\cdot e^{-t/{\tau}}$$ $$ u^{'}(t)=C^{'}(t)\cdot {e^{-t/{\tau}}-\frac{1}{\tau}e^{-t/\tau}}\cdot C(t) $$ Inserting into  Eq. $(1)$ : $$ \tau\left(C^{'}(t)\cdot {e^{-t/{\tau}} - \frac{1}{\tau}e^{-t/\tau}}\cdot C(t)\right)= -C(t)e^{-t/\tau} +u_{\text{rest}}+RI(t) $$ $$ \tau\cdot C^{'}(t)\cdot e^{-t/{\tau}} =u_{\text{rest}}+RI(t) $$ $$ \int C^{'}(t)=\int \frac{1}{\tau} \left(u_{\text{rest}}+RI(t)\right)\cdot e^{t/{\tau}}dt \implies C(t)= C_{1}+ \frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt $$ $$ u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right) $$ In solution, it says before a given time $t_{0}$ the current is zero and the $ut_{0} = u_{\text{rest}}$ . With this information I can't apply it to find $C_{1}$ . In some resources, they say applying initial condition results that integration part of equation equals to zero. So I tried that: $$ u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right) $$ Initial condition $ut_{0} = u_{0}$ $$u_{0} =e^{-t0/{\tau}}(C_{1} + \text{ (they specified integration equals to zero)) } \implies C_{1}=u_{0}\cdot e^{t0/{\tau}}   $$ $$u(t)=e^{(t0-t)/{\tau}}\cdot u_{0} + \frac{e^{-t/{\tau}}}{\tau}\int_{t_{0}}^t(u_{\text{rest}}+RI(t))e^{s/{\tau}}ds $$ I couldn't proceed from that point. Book's Solution: $$\tau\frac{du}{dt} +u(t) =0 \iff \frac{du/dt}{u}=-\frac{1}{\tau} \implies u(t)=ke^{-t/{\tau}} $$ A particular solution can be obtained by the ""variation of parameters"" method: we write $u(t)=k(t)e^{-t/{\tau}}$ and replace it in Eq. (1). $$\tau\left(\frac{dk(t)}{dt}-\frac{1}{\tau}k(t)\right)e^{-t/{\tau}} + k(t)e^{-t/{\tau}} =u_{\text{rest}}+RI(t)  $$ $$\frac{dk(t)}{dt}=\frac{1}{\tau}(u_{\text{rest}}+RI(t))e^{t/{\tau}} $$ Integrating, we find $k(t) = k_{2}+\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(s))e^{s/{\tau}}ds $ where k2 is a new integration constant. Denoting the initial condition by $u_{0} = u_{to}$ , we obtain: $$u(t) = u_{\text{rest}} + (u_{0}-u_{\text{rest}})\cdot e^{-(t-t0)/{\tau}} +\frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds $$ Using the particular initial condition $u(t_{0}) = u_{\text{rest}}$ , the equation simplifies to: $$ \bbox[yellow]{ u(t) = u_{\text{rest}} + \frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds \implies \text{The real solution}}  $$ As you can realize, my solution is wrong. How can I achieve the correct solution. Also some sources say, these equations can be solved by laplas and convolution methods. I searched this, but this method is used solve second degree diff equations. If someone wish to solve by laplas, I will be appreciated. Pls try to help me, I am very desperate right now. Best regards.","['laplace-transform', 'ordinary-differential-equations']"
4438905,Sampling distribution with intelligence quotient,"The Intelligence Quotient of adults follows normal distribution with mean $m = 100$ and standard deviation $σ = 10$ . Consider a sample of 27 adults and calculate the probability for the IQ of the sample to be less than 98. What I've tried so far: $Z = \frac{\bar X - m}{σ/\sqrt{n}} = \frac{98 - 100}{19.245} = 0.1039$ . So from z-score tables the required probability is $0.555$ . I don't know if it's correct and I also need some explanation on what exactly we compute by this.
I am very new to statistics and distributions. Thank you.","['statistics', 'normal-distribution', 'probability']"
4438917,Spinor bundle $\mathbb{S}(M)$ for connected sum $\mathbb{S}(M_1 \#M_2)$?,"I was just reading This question Wherein the OP wants to know what the tangent bundle looks like in a general connected sum $$T\left(M_{1}\#M_{2}\right)$$ (where the connected sum is along some submanifold $V$ ). The only answer provided (first comment) is that at points interior to $M_{i}$ with $V$ removed, the tangent bundle looks like $T(M_{i})$ . I'm interested in the analog for spin bundles on a connected sum. In general, a spin manifold might be the connected sum of several non-spin manifolds. The previous comment won't apply in this case since there is no spinor bundle for some $M_i $ . In that case how do we find an isomorphism that could apply to our boundary space? I had a hunch that might involve generalized spin structures $spin_G $ which I've also asked a question about. If anyone could maybe clarify how this might work I'd be very thankful! To be more specific, I am particularly interested in the induced spin structure on $V$ . It would seem like one orientation would have one form and the opposite orientation would have another? My thought was there must be some kind of spin statistics requirement on the boundary of the connected manifolds. Consider for example the connected sum of the complex projective plane and say $S^3 \times S^1 $ . The former admits a $Spin_c $ structure while the latter admits a standard spin structure, how to they meet and agree on $V$ ?","['general-topology', 'spin-geometry', 'riemannian-geometry']"
4438968,Sup norm of iterated convolution on $\mathbb{R}$,"Let $f\in L^\infty(\mathbb{R})$ be a bounded function satisfying $0\leq f\leq 1$ and $\int f = 1$ .  I am curious whether the bound $$
\| f^{\ast k} \|_{L^\infty} \leq C k^{-1/2}
$$ holds for the $k$ -fold iterated convolution $f^{\ast k}$ .  This seems like a continuous version of the Littlewood-Offord problem, but I don't know how to make the connection precise.","['fourier-analysis', 'probability-theory', 'real-analysis']"
4438997,Can the Cauchy product of a conditionally convergent series with itself be absolutely convergent?,"If $\sum_{n\ge 0} a_n$ and $\sum_{n\ge 0} b_n$ are two series, their Cauchy product is defined as $\sum_{n\ge 0} c_n$ , where $c_n = \sum^n_{k=0} a_k b_{n-k}$ . As this question points out, finding two conditionally convergent series whose Cauchy product is absolutely convergent is quite hard, but examples do indeed exist. I also learned that the Cauchy product of a divergent series with itself can be absolutely convergent (see here ). So do there exists a conditionally convergent series $\sum_{n\ge 0} a_n$ such that the Cauchy product of $\sum_{n\ge 0} a_n$ with itself is absolutely convergent?","['conditional-convergence', 'convergence-divergence', 'cauchy-product', 'real-analysis']"
4439012,Understanding why do we need $\delta>0$ in Lyapunov's condition?,"I am showing that Lyapunov's condition $$
\exists\delta>0:\sum_{m=1}^ns_n^{-(2+\delta)}\mathbb{E}[(X_m)^{2+\delta}]\to0
$$ implies the Lindeberg's condition $$
\forall\epsilon>0:\sum_{m=1}^n \mathbb{E}\left[\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\right]\to0.
$$ However, I do not see why the $\delta>0$ is needed. If we have $$
\sum_{m=1}^ns_n^{-2}\mathbb{E}[(X_m)^{2}]\to0
$$ then apparently Lindeberg's condition holds since $\left(\frac{X_m}{s_n}\right)^2>\left(\frac{X_m}{s_n}\right)^2\mathbb{1}_{\{X_m\geq\epsilon s_n\}}\geq0,\forall\epsilon>0$ . I cannot identify the problem in the argument above. I am aware of a duplicate of this question ( here ) but I fail to see why the answerer suggests that ""The $\dfrac{1}{c\delta}$ with $\delta>0$ is needed to make this go to 0"" and the counterexample (it doesn't satisfy the Lyapunov's condition in first place). May someone help me identify the errors I made in my arguments and understand how $\delta>0$ is needed? Any input is appreciated.","['convergence-divergence', 'central-limit-theorem', 'probability']"
4439028,Challenge: Solve $x^x = \frac{1}{256}$ without the use of the Lambert W function,"As stated above: $x^x = \frac{1}{256}$ , solve for $x$ . Since $256$ is power of $2$ , I let $x = 2^n$ , where $n \in R$ . So: $2^{n^{(2^n)}} = 2^{-8}$ $n*2^n = -8$ $-n = 2^{3-n}$ $\log_2$$(-n) = 3-n$ $\log_2$$((-n)^{\frac{1}{3-n}}) =1$ $(-n)^{\frac{1}{3-n}}= 2$ I am currently stuck at this point with no idea on how I should progress forward. My last option is to solve it numerically but I would like to see if there are any other algebraic ways(other than Lambert W function).","['exponentiation', 'algebra-precalculus', 'functions', 'logarithms']"
4439057,How to create a trace table for a recursive function,"So, I know how to trace an algorithm in discrete mathematics when you have e.g., a for or while loop and then output. But, for some unknown reason, I am lost when it comes to tracing a recursive function calling itself. def func(n):
    if n = 1 then:
       func <- 3 + 6
    else
       func <- (-n) + 2 * func(n - 1) trace table: func(5) -------------------------------
|  step  |  n  |  r  | output |
-------------------------------
|   1.   |  5  |  -  |   -    |
|   2.   |  5  |  0  |   -    | This question, is not concerned with counting, because I've done this already by paper and hand. I even written the method in Python, and received the same output. def func(n):
  r = 0;
  if n == 1:
    r = 3 + 5
  else:
    r = (-n) + 2 * func(n-1)
  return r

print(func(1)) =  8
print(func(2)) = 14
print(func(3)) = 25 
print(func(5)) = 87",['discrete-mathematics']
4439066,Sums of ratios of a set’s sums to its products,"Let $S = \{1, 2, 3, ..., 8\}$ .
Let $A \subseteq S$ and $A \neq \varnothing$ . $F(X) = \text{sum of all elements in } X.$ $G(X) = \text{product of all elements in }X$ . Calculate $\left\lfloor{\sum_{A ⊆ S}^\  \frac {F(A)} {G(A)}}\right\rfloor$ . My approach was looking for a pattern, so I calculated the first three terms and found out the sum could be $\frac {N^3-(N-1)^2} N$ . Can someone help me finding the real solution? Thanks in advance.","['number-theory', 'functions', 'combinatorics', 'sequences-and-series']"
4439093,Solve for $x$ where $\sin(x)^{\cos(x)}$= $2$,"I found this question in a mathematics book (some kind of challenge questions book). I was just trying to solve this question for fun. At the moment I am a bit confused. So by now, I have tried two things 1st of all is to sub $cos(x)$ as $\frac{1}{\pm \sqrt{1+\tan^2(x)}}$ and $\pm \sqrt{\sin^2(x)-1}$ . However, I haven't made any progress. I graphed both equation son desmos and Graph of both sides of the equation . I also got this solution from a math computation website solution .
Any help will be highly appreciated.","['contest-math', 'trigonometry', 'graphing-functions']"
4439121,"Why are mean, median, and mode called central tendency?","Is there any difference between the two terms ""Central tendency"" and ""representative values""? Why are mean, median, and mode called central tendency or representative values?","['statistics', 'median', 'central-tendency', 'means', 'terminology']"
4439123,L^2 function also bounded almost everywhere?,"Let $B$ be a bounded subset of $\mathbb{R}^n$ and suppose we have $f\in L^2(B;\mathbb{R})$ such that $$||f||^2=\int\limits_B(f(x))^2dx < \infty.$$ How can I show that $f$ is pointwise bounded almost everywhere, i.e. there exists $a,b\in \mathbb{R}$ such that $a\leq f(x)\leq b$ for almost all $x\in B$ ?","['measure-theory', 'analysis']"
4439131,Counter-example for Lipschitz function,"Is the following statement true or false? Let $f : [0,1] \to \mathbb{R}$ be a continuous function such that $|f(x)-f(0)|\leq |x|$ for all $x\in[0,1/2]$ For all $\varepsilon >0$ there exists $C_{\varepsilon}>0$ such that $|f(x)-f(y)|\leq C_{\varepsilon}|x-y|$ for all $x, y \in [\varepsilon , 1]$ . Then $f$ is Lipschitz on $[0,1]$ . I think that the statement is false but I'm not able to produce a counter-example. Here is some intuition: Since $C_{\varepsilon}$ may go to infinity as $\varepsilon \to 0$ , then it is not useful when proving that $f$ is Lipschitz in a neighborhood of $0$ . And the condition $|f(x)-f(0)|\leq |x|$ alone is not powerful enough: take $x=1/n$ and $y=1/n+1/n^2$ , we have $|x-y|=\frac{1}{n^2}$ but $|f(x)-f(y)|\leq |f(x)-f(0)|+|f(y)-f(0)|\lesssim \frac{1}{n}$ which is too big.","['lipschitz-functions', 'analysis', 'real-analysis']"
4439136,On some congruence in cyclotomic field,"Let $k = \mathbb{Q}(\zeta_5)$ the $5^{th}$ cyclotomic field and $p$ a prime number verify $p\equiv 1 \pmod 5$ . Let $\lambda = 1-\zeta_5$ the unique prime of $k$ above $5$ , precisely $5 = (1-\zeta_5)^4 = \lambda ^4$ . We have that $p$ splits in $k$ as $p=\pi_1\pi_2\pi_3\pi_4$ with $\pi_i$ are primes in $k_0$ . I need to know what is the exact congruence of $\pi_i$ modulo $\lambda^5$ ? is it $\pi_i \equiv\,\, 1  \pmod {\lambda^5}$ or $\pi_i \not\equiv \,\,1\,\, \pmod {\lambda^5}$ . In the case when $p\equiv 1 \pmod {25}$ we see that $\pi_1\pi_2\pi_3\pi_4 \equiv\,\, 1  \pmod  {\lambda^5}$ so we should have $\pi_i \equiv\,\, 1 \pmod {\lambda^5}$ in order to have the congruence ? and what happen if $p \not\equiv 1 \pmod {25}$","['number-theory', 'arithmetic', 'cyclotomic-polynomials', 'cyclotomic-fields']"
4439154,Jacobian for Conversion Between Polar and Cartesian Resembles a Rotation Matrix,"In matrix form, supposing $z=f(x,y)$ where x,y represent the cartesian coordinate system, one can write the chain rule: $ \begin{bmatrix}
z_x \\
z_y 
\end{bmatrix} = \begin{bmatrix}
r_x & \theta_x \\
r_y & \theta_y 
\end{bmatrix}   
 \begin{bmatrix}
z_r \\
z_\theta 
\end{bmatrix}$ Which turns out to be $ \begin{bmatrix}
z_x \\
z_y 
\end{bmatrix} =  \begin{bmatrix}
\cos\theta & -\frac{\sin\theta}{r} \\
\sin\theta & \frac{\cos\theta}{r} 
\end{bmatrix}  
 \begin{bmatrix}
z_r \\
z_\theta 
\end{bmatrix}$ Why does this middle matrix resemble a rotation and a scaling by 1/r so much?","['matrices', 'multivariable-calculus', 'linear-algebra']"
4439155,On the continuity of the funciton $w \mapsto \mu(\{x \in X \mid w^\top v(x) > t\})$,"Let $X$ be a measurable space and let $\mu$ be a probability measure on $X$ (assumed to be non-atomic , in case that helps). Let $v:X \to \mathbb R^k$ be a $\mu$ -integrable function and for every $t \in [0,b]$ (with $b>0$ fixed), consider the function $G_t:\mathbb R^k \to [0,1]$ defined by $$
G_t(w):= \mu(\{x \in X \mid w^\top v(x) > t\}).
$$ Question. Under what minimal conditions on $\mu$ and $v$ are the functions $G_t$ continuous for (almost) all $t \in [0,b]$ ?","['measure-theory', 'set-valued-analysis', 'functional-analysis', 'analysis']"
4439237,Radon-Nikodym derivative of pushforward measures and Girsanov theorem,"Let $\mu$ and $\nu$ be two measures on a measure space $(\Omega, \Sigma)$ , and $\mu$ is absolute continuous w.r.t. $\nu$ . Also let $X\colon \Omega \to H$ be a measurable functions mapping to another measure space $(H, Z)$ . Suppose that we known the Radon-Nikodym derivative $$
\omega \mapsto \frac{d \mu}{d \nu}(\omega).
$$ How to then find the Radon-Nikodym derivative of their pushforward measures? That is, $$
\frac{d \mu_X}{d \nu_X}\colon H \to [0, \infty),
$$ where $\mu_X$ and $\nu_Y$ are the pushforward measures of $\mu$ and $\nu$ of the functions $X$ , respectively. A concrete example is given as follows. Let $B(t, \omega)$ be a Brownian motion under measure $\nu$ . If $$
Z_T(\omega) = \exp\bigg(\int^T_0 a(B(s, \omega)) dB(s, \omega) - \frac{1}{2}\int^T_0 a^2(B(s, \omega)) ds\bigg)
$$ satisfies certain conditions, then we can define the Radon-Nikodym derivative $$
\frac{d \mu}{d \nu}(\omega) := Z_T(\omega),
$$ so that the process $B$ under the measure $\mu$ created from this derivative is now a weak solution to the SDE $d X(t) = a(X(t)) dt + d\overline{B}(t)$ , where $\overline{B}$ is another Brownian motion under measure $\mu$ . This is the famous Girsanov theorem applied on SDEs. The purpose that I want to obtain $\frac{d \mu_B}{d \nu_B}$ is to get the (finite-dimensional) distribution $\mu_B$ of $B$ , since the distribution of Brownian motion $\nu_B$ is easy to compute. I am purely guessing (by change of variable formula), is it true that $$
\frac{d \mu_B}{d \nu_B}(f) = \exp\bigg(\int^T_0 a(f(s)) df(s) - \frac{1}{2}\int^T_0 a^2(f(s)) ds\bigg)?
$$","['measure-theory', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus', 'radon-nikodym']"
4439279,Exponential bound for tail of standard normal distributed random variable,"Let $X\sim N(0,1)$ and $a\geq 0$ . I have to show that $$\mathbb{P}(X\geq a)\leq\frac{\exp(\frac{-a^2}{2})}{1+a}$$ I have no problem showing that $\mathbb{P}(X\geq a)\leq \frac{\exp(\frac{-a^2}{2})}{a\cdot\sqrt{2\pi}}$ which can be done by computation of the integral, but I haven't found a way to prove the inequality above. I tried to use the fact that $$\mathbb{P}(X\geq a) = \mathbb{P}(h(X)\geq h(a))\leq \frac{\mathbb{E}[h(X)]}{h(a)}$$ but I haven't found a suitable function $h$ , yet. Can anybody give me an advice or an idea how to go forward?","['statistics', 'normal-distribution', 'upper-lower-bounds', 'inequality', 'probability-theory']"
4439285,"What does it mean to ""augment"" a set?","I am reading a book Hausdorff Compactifications and I don´t understand the sentence in bold. It is part of proof of the Theorem above (claim 1). I dont undersand in particular, what is meant by ""augmenting"" the open cover with open set. Thank you for your help! Theorem 1.11 from Chandler´s book: Closed subsets of compact spaces are compact. Compact subsets of
Hausdorff spaces are closed. If $f : X \rightarrow Y$ is continuous
and $X$ is compact, then $f(X)$ is compact. If $f : X \rightarrow Y$ is one-to-one and continuous, $X$ is compact, and $Y$ is Hausdorff
then $f$ is a homeomorphism onto $f(X)$ . Proof of 1. Choose a compact space $X$ and its closed subset $F$ . Let $\{\mathcal{O}_\alpha \}_{\alpha \in A}$ be an open cover of $F$ . We augment $\{\mathcal{O}_\alpha \}_{\alpha \in A}$ with the open set $ X \setminus F$ to obtain an open covering of the compact set $X$ .","['general-topology', 'compactness', 'terminology']"
4439406,"Forming rational numbers using unique Egyptian fractions, all but one of whom have coprime denominators","Question: For a given rational number $r\in (0,1)$ , does there exists a finite $S\subset \mathbb{N}$ such that every pair of elements of $S$ are coprime and $$r-\sum_{n\in S}\frac{1}{n}=\frac{1}{b}$$ for some $b\in\mathbb{N}$ ? Example: For example, for $r=\frac{3}{11}$ we have $$\frac{3}{11}-\frac{1}{5}-\frac{1}{14}=\frac{1}{770}$$ For $r=\frac{3}{13}$ we have $$\frac{3}{13}-\frac{1}{5}-\frac{1}{33}=\frac{1}{2145}$$ For $r=\frac{4}{13}$ we have $$\frac{4}{13}-\frac{1}{4}-\frac{1}{19}-\frac{1}{199}-\frac{1}{28089}-\frac{1}{502057679}=\frac{1}{2772681042969479772}$$ Work so far: I've verified that this works for all fractions with denominator less than or equal to $13$ using a combination of a greedy algorithm as well as brute force. The greedy algorithm is described below Step 1: Check whether $r=\frac{1}{b}$ for some $b\in\mathbb{N}$ . If yes, set $S=\emptyset$ and you are done. If not go to step $2$ . Step 2: Define $S_1=\{\lceil r^{-1} \rceil\}$ and go to step 3 while setting $i=1$ . Step 3: Check if $$r-\sum_{n\in S_i}\frac{1}{n}=\frac{1}{b}$$ for some $b\in\mathbb{N}$ . If yes, set $S=S_i$ . If not go to step 4. Step 4: Consider $$m= \left\lceil \left(r-\sum_{n\in S_i}\frac{1}{n}\right)^{-1}\right\rceil$$ and define $S_{i+1}$ to be $$S_{i+1}=S_i\cup\{\text{Least }n\geq m: \gcd(n,a)=1\text{ for all }a\in S_i\}$$ Go back to step 3 with $i$ incremented to $i+1$ . I actually believe that this greedy algorithm will always end since the numerators of $$r-\sum_{n\in S_i}\frac{1}{n}$$ seem to 'stay close' to $1$ , but unfortunately the sizes of the denominators grow too fast to easily use it to calculate sets $S$ for a significant number of possible $r$ . Motivation: If answered in the affirmative, then this would settle a conjecture I asked about here . The conjecture is:  Let $r$ be a given positive rational and let $T$ be a finite set of rational numbers in $(0,r)$ . Does there exist a finite set $S\subset \mathbb{N}$ such that $$\sum_{n\in S}\frac{1}{n}=r$$ but $$\sum_{n\in R}\frac{1}{n}\not\in T$$ for all $R\in P(S)$ (the powerset of $S$ )? EDIT: As the bounty is over halfway done, I'll add this addendum which might be easier to solve. A well known algorithm for egyption fractions is the greedy algorithm . In terms of the algorithm presented above (in step 4), it would define $$S_{i+1}=S_i\cup\{m\}=S_i\cup\left\{\left\lceil \left(r-\sum_{n\in S_i}\frac{1}{n}\right)^{-1}\right\rceil\right\}$$ That is, simply pick the smallest integer such that you don't overshoot the rational number you are trying to reach. This algorithm is well known to terminate in all cases. In a similar manner, we could instead modify the algorithm above (again in step 4) to be $$S_{i+1}=S_i\cup \{\text{Least }k\geq m:k\not|\prod_{n\in S_i}n\}$$ As you can see, this allows for many more terms to be added to $S$ as it includes all possible relatively prime numbers. This alternative algorithm also has the added benefit that it is super easy for a computer to test. With it, I checked it terminated at every rational between $0$ and $1$ with denominator less than or equal to $1000$ .","['number-theory', 'gcd-and-lcm', 'egyptian-fractions']"
4439489,How to solve a long system of equations?,"We know that there are lots of ways to solve a system of equations, however I can't seem to find a way to solve a long system of equation just by hand without using a computer. the system I'm focused on is this: $$\begin{cases}
x_0+0^1x_1+0^2x_2+0^3x_3+...+0^nx_n=c_0 \\
x_0+1^1x_1+1^2x_2+1^3x_3+...+1^nx_n=c_1 \\
x_0+2^1x_1+2^2x_2+2^3x_3+...+2^nx_n=c_2 \\
.\\
.\\
.\\
x_0+n^1x_1+n^2x_2+n^3x_3+...+n^nx_n=c_n \\
\end{cases}$$ we can see that there is a pattern in the coefficients, but I wonder if there is a general form that I can use to find each variable in function of the other coefficients $$x_0=f_0(c_0, c_1,...,c_n)\\x_1=f_1(c_0, c_1,...,c_n)\\..\\x_n=f_n(c_0, c_1,...,c_n)$$ so far I tried the reduced row echelon form (RREF) method, but the time needed to solve this kind of system depends on ""n"". my desired outcome is to find (if there is) a general form to solve this kind of systems.","['algebra-precalculus', 'systems-of-equations']"
4439536,Path of the sun across the sky in a 4D world,"Someone asked a question on worldbuilding about navigating by the stars on a 4D planet. In thinking about it I came up with a question that seems appropriate to ask here, as it's purely a maths question. Suppose you're at a point on the surface of a sphere in 4 dimensions. There is a stationary point in the sky, infinitely far away - call it the sun. The sphere you're standing on rotates at a constant rate and you move with it, so from your point of view the stationary point moves across the sky. Now suppose we take your view of the sky and project it via a stereographic projection into a 3D Euclidean space. The question is, what does the path traced out by the sun look like in this three dimensional space? We can assume without loss of generality that the rotation of the sphere preserves the $xy$ plane and the $uv$ plane. The answer to this question will depend on the location of the stationary point and of the observer, and also on the ratio of the two rotation rates that are needed to specify such a rotation in 4D. I'm interested in knowing the general answer, i.e. what kinds of paths can be traced out depending on the values of these parameters?","['euclidean-geometry', 'stereographic-projections', 'geometry', 'rotations']"
4439537,Flipping $n$ coins until they're all heads,"Suppose you have $n$ (fair) coins: You flip them until they're all heads in the following sense: Suppose you flip all of them first and get $k$ heads. This is round $1$ You remove these heads from the board and now you flip the remaining $n- k$ heads. This is round $2$ . Suppose you get $k_1$ heads. You remove these heads and flip the remaining $n- k - k_1$ heads. This is round $3$ . And so on. Continue doing this until you've got an empty board/all the coins have reached heads. Now there's two quantities I want to observe: Expected value of total number of flips performed from start to finish. 2. The number of rounds. In particular I want to see that with high probability the # of rounds is $\leq c \log n$ for some constant $c$ (This is the one I'm more interested in) I sort of have an idea for 1 and am struggling with 2. For $1$ we define $X_i = \#$ of flips coin $i$ needs to get to heads. So $\mathbb{P}[X_i = j] = 0.5(1-0.5)^{j-1} = 0.5^j$ and $\mathbb{P}[X_i \leq j] = \sum_{k = 1}^j0.5^k = 1 - 0.5^{j+1}$ Then the thing we're looking for is $X = \max X_i$ and so $\mathbb{P}[X \leq j] = (1-0.5^{j + 1})^n$ (independence). From this finally we get that $\mathbb{P}[X = j] = (1- 0.5^{j + 1})^n - (1-0.5^j)^n$ . This I think is right, but given this expression I can't find a neat way to express $\mathbb{E}[X]$ . Is this the best I can hope for? But what I'm more interested in is 2 I want to see that with high probability the number of rounds is $\leq c \log n$ . I'm not sure where to begin with this one. Is it just a matter of saying that roughly every round we half the number of coins on the table and so we take $\log_2 n$ turns? Is this a way to make this more rigorous (""with high probability"")","['probability-distributions', 'expected-value', 'combinatorics', 'probability-theory', 'probability']"
4439560,Is there a product identity for $ a\cos\left(\alpha\right)+b\cos\left(\beta\right) $ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The product identity of $ \cos\left(\alpha\right)+\cos\left(\beta\right) $ is $ 2\cos\left(\frac{\alpha+\beta}{2}\right)\cos\left(\frac{\alpha-\beta}{2}\right) $ Is there any formula that can generalize to $ a\cos\left(\alpha\right)+b\cos\left(\beta\right) $ ?",['trigonometry']
4439584,How to prove that $\sup(A-B) = \sup(A) - \inf(B)$?,"How to prove that $\sup(A-B) = \sup(A) - \inf(B)$ ? My attempt:
Let $c \in A-B$ and define $c= a-b$ , where $a \in A$ and $b \in B$ . Then $a-b \leq \sup(A) - \inf(B)$ . Hence, $\sup(A-B) \leq \sup(A) - \inf(B)$ . Moreover, for any $\epsilon >0$ , $\sup(A) \leq a + \epsilon$ and $\inf(B) \geq b - \epsilon$ . This implies that $$\sup(A) + b - \epsilon \leq \inf(B) + a + \epsilon$$ $$\sup(A) - \inf(B) \leq a-b + 2\epsilon \leq \sup(A-B)$$ Is this proof correct or logical?","['general-topology', 'supremum-and-infimum', 'analysis']"
4439599,Uniform Convergence of Difference Quotient to Derivative using Compactness,"I have a question about a proof to a question in another discussion. This is regarding the uniform convergence of the difference quotient to the derivative of a function. Here is a link to the discussion. The question in the discussion was: Let $f:[0,1] \to \mathbb{R}$ be continuously differentiable. Prove that, for every $\epsilon > 0$ , there exists $\delta > 0$ such that $0 < |h| < \delta$ implies $\left| \frac{f(x+h) - f(x)}{h} - f'(x) \right| < \epsilon$ for all appropriate $x$ . And the answer provided by Hagen von Eitzen was: Let $\epsilon>0$ be given.
For $\delta>0$ let $$U_\delta = \left\{a\in [0,1]\colon 0<|h|<\delta\Rightarrow \left|\frac{f(a+h)-f(a)}h-f'(a)\right|<\epsilon\right\}$$ Clearly, $\delta<\delta'$ implies $U_{\delta'}\subseteq U_\delta$ .
By continuity of $f$ and $f'$ , $U_\delta$ is open and by definition of $f'$ , $$[0,1]=\bigcup _{\delta>0}U_\delta.$$ Since $[0,1]$ is compact, there is a finite subcover, i.e. there is a single $\delta>0$ such that $[0,1]=U_\delta$ . My question is: why is $U_{\delta}$ open? I tried creating some examples and this does appear to hold for those examples, but I couldn't see how to extend it to a general proof. I do understand how all of the other parts in the proof work though. It would also be appreciated if the use of the mean value theorem can be avoided as this was requested in the discussion when the question was asked. However, if no other reasonable approach is known to show that $U_{\delta}$ is open, then the use of the mean value theorem or other similar theorems is welcomed.","['derivatives', 'uniform-convergence']"
4439605,"Approximation of $\int_0^\pi \big[x(\pi-x)\csc (x)\big]^k\,dx \quad \forall k$","A recent post addressed the problem of the closed form of $$I(k)=\int_0^\pi \Bigg[ \frac {x(\pi-x)} {\sin(x)}\Bigg]^k \,dx$$ When $k$ is a positive integer, they seem to be known except that they require a lot of computer ressources as soon as $k\geq 10$ . Just for the fun of it , I tried to obtain approximate values of these integrals for any exponent. Because ot the analogy with Bhaskara I's sine approximation formula , my leading idea was to first approximate the integrand $$x(\pi-x)\csc (x) \sim a -bx(\pi-x)$$ and the coefficient $(a,b)$ where obtained minimizing the norm $$\Phi(a,b)=\int_0^\pi \Big[x(\pi-x)\csc (x)-\big[a -bx(\pi-x) \big] \Big]^2\,dx$$ which is analytic. The optimal values are $$a=\frac{18 \left(14 \pi ^2 \zeta (3)-155 \zeta (5)\right)}{\pi ^3}\qquad \qquad b=\frac{30 \left(49 \pi ^2 \zeta (3)-558 \zeta (5)\right)}{\pi ^5}$$ $$\Phi(a,b)=\frac{6 \left(\pi ^4 \left(\pi ^2-2009 \zeta (3)\right) \zeta (3)+45570 \pi ^2
   \zeta (3) \zeta (5)-259470 \zeta (5)^2\right)}{\pi ^5}=1.903\times 10^{-4}$$ Notice that these values are relatively close to $a=\frac{5 \pi ^2}{16}$ and $b=-\frac 14$ given using 2 ; however, the current norm is four times smaller. All of that leads to $$J(k)=\int_ 0^\pi \big[a -bx(\pi-x) \big]^k\,dx$$ $$\color{blue}{J(k)=\pi \,\left(\frac{3(930 \zeta (5)-77 \pi ^2 \zeta (3))}{2 \pi ^3}\right)^k\, _2F_1\left(\frac{1}{2},-k;\frac{3}{2};-\frac{5 \left(49 \pi ^2 \zeta (3)-558 \zeta (5)\right)}{930 \zeta
   (5)-77 \pi ^2 \zeta (3)}\right)}$$ Just as for $I(k)$ (see @user64494's comment), $\lim_{k\to \infty } \, \frac{J (k+1)}{J(k)} =\pi^-$ . When $k$ is an integer, $\, _2F_1\left(\frac{1}{2},-k;\frac{3}{2};z\right)$ are quite simple polynomials with interesting patterns $$\left(
\begin{array}{cc}
k & \, _2F_1\left(\frac{1}{2},-k;\frac{3}{2};z\right)\\
 1 & 1-\frac{1}{3}z\\
 2 & 1-\frac{2 }{3}z+\frac{1}{5}z^2 \\
 3 & 1-\frac{3 }{3}z+\frac{3 }{5}z^2-\frac{1}{7}z^3 \\
 4 & 1-\frac{4 }{3}z+\frac{6 }{5}z^2-\frac{4}{7}z^3+\frac{1}{9}z^4 \\
 5 & 1-\frac{5 }{3}z+\frac{10 }{5} z^2-\frac{10 }{7}z^3+\frac{5}{9}z^4-\frac{1}{11}z^5
\end{array}
\right)$$ This seems to lead to decent approximations $$\left(
\begin{array}{ccc}
k & J(k) & I(k) \\
 1 & 8.41440 & 8.41440 \\
 2 & 22.6580 & 22.6582 \\
 3 & 61.3506 & 61.3546 \\
 4 & 167.057 & 167.093 \\
 5 & 457.510 & 457.738 \\
 6 & 1260.20 & 1261.41 \\
 7 & 3491.25 & 3497.00 \\
 8 & 9743.95 & 9752.77 \\
 9 & 27314.7 & 27360.7 \\
10 & 76794.7 & 77205.2
\end{array}
\right)$$ What looks interesting (at least to me) is that this works for non integer values of $k$ $$\left(
\begin{array}{ccc}
k & J(k) & I(k) \\
 1.234 & 10.6042 & 10.6042 \\
 2.345 & 31.9297 & 31.9303 \\
 3.456 & 96.8040 & 96.8156 \\
 4.567 & 295.557 & 295.662 \\
 5.678 & 908.812 & 909.529 \\
 6.789 & 2814.49 & 2818.65 \\
 7.890 & 8688.35 & 8709.72
\end{array}
\right)$$ and even for complex values $$\left(
\begin{array}{ccc}
k & J(k) & I(k) \\
 1+i   & +4.61973 +7.00583 \,i & +4.61977 +7.00576 \,i \\
 1+2 i & -3.27763 +7.65210 \,i & -3.27715 +7.65218 \,i \\
 1+3 i & -8.08205 +1.46086 \,i & -8.08159 +1.46231 \,i \\
 2+i   & +12.3359 +18.9310 \,i & +12.3353 +18.9310\,i \\
 2+2i  & -9.04768 +20.5009 \,i & -9.04680 +20.4992 \,i \\
 2+3i  & -21.8087 +3.57177 \,i & -21.8039 +3.57352 \,i \\
 3+i   & +33.1122 +51.4419 \,i & +33.1083 +51.4458 \,i \\
 3+2i  & -25.1135 +55.2172 \,i & -25.1179 +55.2074 \,i \\
 3+3i  & -59.1718 +8.66441 \,i & -59.1523 +8.65622 \,i
\end{array}
\right)$$ My questions : do the $\, _2F_1\left(\frac{1}{2},-k;\frac{3}{2};z\right)$ correspond to known polynomials ? could it be possible to develop a recurrence rekation for $\, _2F_1\left(\frac{1}{2},-k;\frac{3}{2};z\right)$ could we find better approximations ? could we obtain decent and rather detailed asymptotic formulae for $Ik$ and/or $J(k)$ ?","['definite-integrals', 'approximation', 'polynomials', 'trigonometry', 'hypergeometric-function']"
4439678,How to proof this lemma using Hoeffding Inequality?,"This question follows from a previously asked question . I am trying to understand the proof of Lemma 2.1 in the paper ""A Universal Law of Robustness via isoperimetry"" by Bubeck and Sellke. We start with a lemma showing that, to optimize heyond the noise level one must necessarily correlate with the noise part of the labels. In what follows we denote $g(x)=\mathbb{E}[y \mid x]$ for the target function, and $z_{i}=y_{t}-g\left(x_{i}\right)$ for the noise part of the observed labels (namely $y_{1}$ is the sum of the target funetion $g\left(x_{i}\right)$ and the noise term $z_{1}$ ).
Lemma 2.1. One has $$
\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n}\left(y_{t}-f\left(x_{i}\right)\right)^{2} \leq \sigma^{2}-\epsilon\right) \leq 2 \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)+\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right) z_{i} \geq \frac{\epsilon}{4}\right).......(A^{1})
$$ Proof. The sequence $\left(z_{1}^{2}\right)$ is i.i.d., with mean $\sigma^{2}$ , and such that $\left|z_{i}\right|^{2} \leq 4$ . Thus Hoeffding's inequality yielde: $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)......................(1)
$$ On the other hand the sequence $\left(z_{1} g\left(x_{1}\right)\right)$ is i.i.d., with mean $0\left(\right.$ since $\left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right)$ , and such that $\left|z_{1} g\left(x_{1}\right)\right| \leq 2$ . Thus Hoeffding's inequality yields: $$
\mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{i} g\left(x_{i}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)$$ I am typically got stuck in the 1st step of proof of this lemma.
I have read about Hoeffding's inequality ,but unable to sync. For reference Theorem $12.3$ (Hoeffding inequality) Given independent $\left(X_{1}, \ldots, X_{n}\right)$ with $X_{i} \in\left[a_{i}, b_{i}\right]$ a.s., $$
\operatorname{Pr}\left[\frac{1}{n} \sum_{i}\left(X_{i}-\mathbb{E} X_{i}\right) \geq \epsilon\right] \leq \exp \left(-\frac{2 n^{2} \epsilon^{2}}{\sum_{i}\left(b_{i}-a_{i}\right)^{2}}\right)
$$ Edit 1 Theorem 4 (Hoeffding's inequality). Let $Z_{1}, \ldots, Z_{n}$ be independent bounded random variables with $Z_{i} \in[a, b]$ for all $i$ , where $-\infty<a \leq b<\infty$ . Then $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \geq t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ and $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ for all $t \geq 0$ . Using the 2nd one I am able to reach here , $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(z_{i}^2-\mathbb{E}\left[z_{i}^2\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2-\frac{1}{n} \sum_{i=1}^{n}\mathbb{E}\left[z_{i}^2]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ as $$\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[z_{i}^2] = \sigma^2(mean) $$ Replacing the value of it $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2- \sigma^2 \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
$$ Thus comparing with Eq $1$ , I am getting the  value of $t$ as $\epsilon/6$ . And it is given $\left|z_{i}\right|^{2} \leq 4$ .
So We can rewrite it as $ -4 \leq z_{i} ^{2} \leq 4$ . So $a$ = -4 and $b$ = 4 Plugging it into RHS , $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n \epsilon^{2}}{(4+4)^{2}*36}\right)
$$ $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(4+4)^{2}*18}\right)
$$ $$
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(8)^{2}*18}\right)
$$ But RHS is not matching with $Eq 1$ . I am unable to figure it out , where am I wrong. Can anyone help me out? (Solved) Edit 2 $$
\mathbf{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right).....................(2.1)
$$ On the other hand the sequence $\left(z_{1} g\left(x_{1}\right)\right)$ is i.i.d., with mean $0\left(\right.$ since $\left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right)$ , and such that $\left|z_{1} g\left(x_{1}\right)\right| \leq 2$ . Thus Hoeffding's inequality yields: $$
\mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{1} g\left(x_{1}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)...........(2.2)
$$ Let us write $Z=\frac{1}{\sqrt{n}}\left(z_{1}, \ldots, z_{n}\right), G=\frac{1}{\sqrt{n}}\left(g\left(x_{1}\right), \ldots, g\left(x_{n}\right)\right)$ , and $F=\frac{1}{\sqrt{n}}\left(f\left(x_{1}\right), \ldots, f\left(x_{n}\right)\right)$ . We claim that if $\|Z\|^{2} \geq \sigma^{2}-\frac{\epsilon}{6}$ and $\langle Z, G\rangle \geq-\frac{\epsilon}{6}$ , then for any $f \in \mathcal{F}$ one has $$
\|G+Z-F\|^{2} \leq \sigma^{2}-\epsilon \Rightarrow\langle F, Z\rangle \geq \frac{\epsilon}{4}......(2.3)
$$ This claim together with $(2.1)$ and $(2.2$ ) conclude the proof of $A^{1}$ How?? And how they have written the eq ( $2.3$ ) I have clearly understood Eqn 2.1 and 2.2 but after that how they conclude the proof I am not getting, Kindly help me to understand","['statistics', 'proof-explanation', 'inequality', 'probability-theory', 'random-variables']"
4439722,Inverse of a function related area problem,My solution is based on the following diagram which I will illustrate in details Now ${A_1} = \frac{5}{4};{A_3} = 1$ $g\left( {2x} \right) = 2f\left( x \right)$ $g\left( 2 \right) = 2f\left( 1 \right) = 2 \times 1 = 2$ $g\left( 4 \right) = 2f\left( 2 \right)$ $\int\limits_1^8 {xf'\left( x \right)dx}  = \left. {xf\left( x \right)} \right|_1^8 - \int\limits_1^8 {\frac{d}{{dx}}x\left( {\int {f'\left( x \right)dx} } \right)dx} $ $\int\limits_1^8 {xf'\left( x \right)dx}  = 8f\left( 8 \right) - f\left( 1 \right) - \int\limits_1^8 {f\left( x \right)dx} $ Not able to proceed from here,"['area', 'functions', 'inverse']"
4439796,A question about Constant sectional curvature between the Riemannian and Kähler manifold,"According to the corollary (3.6) on An Introduction to Differentiable Manifolds and Riemannian Geometry, Def . A Riemannian manifold $M$ is isotropic at $p \in M$ if the curvature is the same constant at $k_p$ on every section at $p$ .
And if $M$ is isotropic at every point $p$ , then the manifold $M$ is
called isotropic . Corollary 3.6 Let $M$ be a $\underline{Riemannian ~manifold}$ . If $p \in M$ is isotropic and every point $p\in M$ ,  and $(U, \varphi)$ is a coordinate neighborhood
with coordinate frames $E_1,....E_n$ and Riemannian metric $g_{ij}=(E_i,E_j)$ , then $$R_{ijkl}= -k_p(g_{ik}g_{ij}-g_{il}g_{jk})
$$ at $p\in M$ I think it is better to see the above corollary as : Corollary3.6' Let $M$ be a Riemannian manifold and $(U, \varphi)$ be a coordinate
neighborhood  with coordinate frames $E_1,....E_n$ and Riemannian
metric $g_{ij}=(E_i,E_j)$ . If $p\in M$ is isotropic, then $$R_{ijkl}=
 -k_p(g_{ik}g_{ij}-g_{il}g_{jk}) $$ at $p\in M$ But, how about the converse statement of Corollary 3.6(') ? In other words, if $-{R_{ijkl}}/(g_{ik}g_{ij}-g_{il}g_{jk})$ is constant at each point $p\in M$ (or constant whenever pick a point $p \in M$ ),  then the manifold $M$ is a isotropic. (and naturally, $-{R_{ijkl}}/(g_{ik}g_{ij}-g_{il}g_{jk})=k_p$ ) The main reason I think the converse statement of Corollary3.6 is just Kähler manifold :
In case of Kähler manifold $M$ , there exists a holomorphic sectional curvature at $p \in M$ and also one can think that this curvature is isotropic at $p\in M$ and isotropic (Kähler manifold) as well. However, on the following PDF , the term ""isotropic manifold"" on the Boothby's textbook seems to correspond ""CHSC"", short for a space of Constant Holomorphic Sectional Curvature. Anyway, according to Theorem 1.3.8 on the following PDF , Theorem 1.3.8 The following identities are equivalent . $(1)$ A $\underline{Kähler~ manifold} ~M$ is CHSC with constant $c$ ; $(2)$ (mumble, mumble) $(3)$ For any $U,V,W,X \in T^{(1,0)}M$ , $$R(U,\bar{V},W,\bar{X})=-\frac{c}{2}(g(U,\bar{V})g(W,\bar{X})+g(U,\bar{X})g(W,\bar{V}))$$ is introduced. In this case, however, the converse statement $(3)\Rightarrow (1)$ holds. To sum up, the statement of Corollary 3.6 in $\underline{Riemannian~ manifold}$ is nearly analogous to Thm1.3.8 in $\underline{Kähler~manifold}$ , but why the converse of Corollary 3.6 does not be mentioned(I doubt that the converse of this corollary would be false), whereas the converse of Thm1.3.8 holds? Here is the very question I wonder.","['complex-geometry', 'differential-geometry']"
4439808,An inequality for inner product space: $\|a+b\|+\|b+c\|+\|c+a\|\le\|a\|+\|b\|+\|c\|+\|a+b+c\|$,"In an inner product space show that the following inequality holds: $$\|a+b\|+\|b+c\|+\|c+a\|\le\|a\|+\|b\|+\|c\|+\|a+b+c\|$$ I figured LHS and RHS are two different upper bounds on $\|2a+2b+2c\|$ using the triangle inequality in two different ways, but I can neither prove it, nor find anything like it.","['inner-products', 'inequality']"
4439819,"Distributing $n$ distinct objects into $m$ types of urns with $k_1,k_2...k_m$ urns of each type","I came accross this (rather complex?) combinatorial problem: I have $18$ distinct objects, $3$ red urns, $7$ blue urns, and $11$ green urns. In how many ways can I distribute the objects into those
urns? The urns of one colour are identical to each-other. I haven't actually solved it yet, and yet I have decided to generalize: I have $n$ distinct objects, $k_1,k_2,k_3,...,k_m$ urns of types $1,2,3,...,m$ . In how many ways can I distribute the objects into
those urns? I think the solution is strongly related to both partitions of $n$ into $m$ parts where order matters, and the stirling numbers of the second kind. But whichever way I take it somewhat reaches a dead-end, or becomes very complex. How do I approach this?","['integer-partitions', 'set-partition', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory']"
4439842,Filtrations of stopping times.,"My textbook introduces this deffinition. Let $(\Omega, \mathcal{F} , \mathcal{F}_t \mathbb{P}) $ be a filtered probability space. Let $\tau$ be a stopping time, Then define $\mathcal{F}_{\tau} : = \{A \in \mathcal{F} : \forall t $ $ \{ \tau \leq t\}  \cap A \in \mathcal{F}_t\}$ It states  "" $\mathcal{F}_{\tau}$ is the $\sigma$ -algebra representing the information available up to the random time $\tau$ "" I was having a hard time understanding the worded definition so decided to come up with a simple enough (discrete) example with coins. Consider we toss two coins , then letting $\Omega = \{ H,T\}^2$ and $\mathcal{F}_0 = \{ \emptyset, \Omega\}$ , $\mathcal{F}_1 = \{\emptyset,\Omega , \{H,T \} \cup \{ H,H\} , \{T,H \} \cup \{ T,T\}  \}$ and $\mathcal{F}_2 = \mathcal{P}(\Omega)$ we see that the $\mathcal{F}_t$ for $t = 0,1,2$ represent the natural filtration. Consider $\tau$ the hitting time of the first head tossed. What would $\mathcal{F}_\tau$ be? What is an intuitive idea of what it contains? I do not understand the worded definition given to me. I know so far that $\mathcal{F}_\tau$ contains $\emptyset, \Omega, TT , TH $ and I believe also $HT \cup HH$","['stopping-times', 'probability-theory', 'filtrations']"
4439869,Intuition regarding sequence functions,"I'm working my way through Halmos and am struggling with his explanation of transfinite recursion.  Specifically, I'm getting stuck on his definition of a ""sequence function,"" and I was hoping someone here could help me build some intuition around it. Halmos says: A sequence function of type $W$ in $X$ is a function $f$ whose domain consists of all sequences of type $a$ in $X$ , for all elements $a$ in $W$ , and whose range is included in $X$ .  Roughly speaking, a sequence function tells us how to ""lengthen"" a sequence; given a sequence that stretches up to (but not including) some element of $W$ we can use a sequence function to tack on one more term. Now, based on Halmos's earlier explanation, a ""sequence function of type $W$ in $X$ "" is a function from some initial segment of $a$ (where $a$ $\in$ $W$ and $W$ is well-ordered) into $X$ .  So, as I understand it, if W is a well-ordered set $\{a,b,c,d\}$ where the ordering is alphabetical, and $X$ is an arbitrary set, say $\{1,2,3,4,5\}$ , then an example of ""a sequence of type $c$ "" would be a function $f$ such that: $$
f(a)=1, f(b)=3, f(c)=5
$$ Similarly, another example of a ""sequence of type $c$ "" would be a function g such that $$
g(a)=2, g(b)=3, g(c)=4
$$ This is where I get stuck.  How do you go from here to a ""sequence function of type $W$ in $X$ "".  I see that the domain is supposed to be $\{ f, g, etc...\}$ where ""etc..."" is any other sequences of type $W$ in $X$ , and the domain is some $Y$ where $Y \subseteq X$ .  But I don't really have any intuition of what that function is or how it can be used to ""lengthen"" a sequence. I've also read through this answer a few times and I believe this bit is capturing the same point: Now, 𝐼 is a method for extending a partial function ""one step further"": if I feed 𝐼 a map 𝑝:𝛽→𝑋 for some 𝛽<𝛼, 𝐼 tells me what 𝑓(𝛽) ""ought"" to be given that 𝑝=𝑓↾𝛽. That is, if I've defined 𝑓 for the first 𝛽-many inputs, 𝐼 tells me how to define 𝑓 for the next input. But here too I similarly am not understanding the ""one step further"" notion. Can someone provide me some examples that clarify how ""sequence functions"" work? EDIT: I'm still struggling with the ""lengthen"" or ""tack on"" concept behind sequence functions, but I think I have a better understanding of what a sequence function is . Later on in the chapter on transfinite recursion, Halmos provides an example of a sequence function in proving the comparability theorem for well-ordered sets.  Specifically, he defines that, for two well ordered sets $X$ and $Y$ , if $x \in X$ and $t$ is a sequence of type $x$ in $Y$ , then let $f$ be a function from $t$ to the supremum of the range of $t$ , if one exists, otherwise, to the least member of $Y$ . $f$ , he explains, is a sequence function. So building on the example above, if $X = \{ a, b, c, d, e, f, g\}$ (ordered alphabetically) and $Y = \{1, 2, 3, 4, 5, 6, 7 \}$ (ordered numerically), we can give an example of a sequence function as follows: Let $M$ be a function from $X$ onto $Y$ that simply maps $a$ to $1$ , $b$ to 2, etc. Let $t$ be a sequence of type $x$ in $Y$ defined as the restriction of $M$ to the initial segment of $x$ for any $x \in X$ .  So, for example, $t$ for $c$ would be $\{(a,1),(b,2),(c,3)\}$ and $t$ for $d$ would be $\{(a,1),(b,2),(c,3),(d,4)\}$ Let $f$ be a function from any $t$ to to the supremum of the range of $t$ , so, for example, $f(t$ for $c) = 3$ and $f(t$ for $d) = 4$ etc. As I understand it, $f$ is a sequence function because it maps any ""sequence of type $x$ "", e.g., $t$ for $a$ , $t$ for $b$ , etc., into $Y$ .  Or, to use Halmos's language, the domain of $f$ is all sequences of type $x$ in $Y$ and the range of $f$ is a subset of $Y$ . That all makes sense to me (did I get it right?).  And it's probably enough to keep going in Halmos.  But I would still love to understand the intuition behind this notion that $f$ lengthens or tacks-on to a sequence.  I just don't see it in this example. $f$ doesn't seem to me to lengthen any sequence, or even to go from one sequence to another.  It simply maps a sequence to an element of $Y$ .  What am I missing?","['elementary-set-theory', 'intuition']"
4439907,Show that $p^k \mid \mid (x-y)$ iff $p^k \mid \mid (x^6-y^6)$.,"Definition. Let $n>1$ be an integer and $p$ be a prime. We say that $p^k$ fully divides $n$ and write $p^k \mid \mid n$ if $k$ is the greatest positive integer such that $p^k \mid n$ . Let $p>6$ be a prime. Let $x$ and $y$ be two distinct integers such that $p\nmid x, p\nmid y,$ and $p\mid (x-y)$ . Show that $p^k \mid \mid (x-y)$ iff $p^k \mid \mid (x^6-y^6)$ . Attempt: $(\implies)$ Let $p^k \mid \mid (x-y)$ . Then $k$ is the greatest positive integer such that $p^k \mid (x-y)$ . Write $x-y=p^km$ for some $m \in \Bbb Z$ . Notice that $$x^6-y^6 = (x-y)(x^5+x^4y+\cdots+y^5) = p^km(x^5+x^4y+\cdots+y^5)=p^k(m(x^5+x^4y+\cdots+y^5)).$$ Hence, $p^k \mid (x^6-y^6)$ and then $p^k \mid \mid (x^6-y^6)$ . Does this approach correct? If not, how to approach it? And how to approach the reverse direction?
Any ideas? Thanks in advanced.",['number-theory']
4439919,"Proving that out of all the possible n-gons that exist in the unit circle, the one with the maximum possible perimeter is the regular $n$-gon.","So in the context of my Convex Analysis studies, I have come across this problem: First I have to prove that $ - \sin x $ is convex over $[0, \pi]$ . That's easy enough using the second derivative theorem. Now, using the answer from above, I have to prove that  out of all possible $n$ -gons that can be specified on the unitary circle, the one with the maximum possible perimeter is the normal $n$ -gon. My geometric intuition is not good, and I cannot figure out how to do this using the tools from Convex Analysis. Any help would be greatly appreciated.","['analytic-geometry', 'calculus', 'convex-analysis']"
4439933,"Is there a generalized solution to the ""Freshman's dream"" of derivatives equation?",Michael Penn shows that $(f(x)g(x))^{\prime} = f^{\prime}(x) g^{\prime}(x)$ has solutions of the form $$f(x) = C \exp{\int \frac{g^{\prime}(x)}{g^{\prime}(x)-g(x)}dx}$$ for a given function $g(x)$ . Taking a generalization of this freshman's dream to be $$\left( \prod_{j=1}^n f_j (x)\right)^{\prime} = \left( \prod_{j=1}^n f_j^{\prime} (x)\right)$$ is there analogously a collection of formulae for constructing a solution? Blackpenredpen also considered the case of two functions.,"['integration', 'derivatives']"
4439948,A combinatorial question for divergent sequences,"Suppose $C=\{ a_{i,j} \}_{i,j \in \Bbb{Z}}$ is a symmetric complex bisequence (which means $a_{i,j}=a_{j,i}$ ) such that $\sum_{i \le j} \lvert a_{i,j} \rvert$ diverges. Call $P \subseteq \Bbb{Z}^2$ square-symmetric if $P=F \times F$ for some finite set $F \subseteq \Bbb{Z}$ . Denote by $S$ the collection of $\sum_{(i,j) \in P} a_{i,j}$ for all square-symmetric $P$ 's. Must $S$ be unbounded? The square-symmetric restriction really annoyed me and I don't know where to start. Even a result about the case $a_{i,j}=\pm 1$ is of great help to me! PS : The problem can be stated in a matrix-manner: Suppose the entry-wise $l^1$ -norm of a countably-infinite symmetric complex matrix is unbounded, must the entry-sums of its finite principal minors be also unbounded?","['matrices', 'convergence-divergence', 'combinatorics', 'sequences-and-series']"
4439953,Probability of stopping time being finite.,"Let $X$ be a continuous non-negative local martingale with $X_0=1$ and $X_t\to0$ almost surely as $t\to\infty$ . For $a>1$ , let $\tau_a=\inf\{t\geq0:X_t>a\}$ . I am tasked with showing that $\mathbb{P}(\tau_a<\infty)=\mathbb{P}(\sup_{t\geq0}X_t>a)=1/a$ . The hint given is to compute the expected value of $X_{t\wedge\tau_a}=a1_{\tau_a\leq t}+X_t1_{\tau_a>t}$ , but I still don't really have any idea how to proceed, so any advice would be greatly appreciated!","['expected-value', 'local-martingales', 'martingales', 'stopping-times', 'probability-theory']"
4440041,"Find non-zeroes $a$ and $b$ such that $\lim[a^n(u_n - 1)] = b$ where $u_1 = 0$ and $4u_{n + 1} = u_n + \sqrt{6u_n + 3}, \forall n \in \mathbb Z^+$.","Consider sequence $(u_n)$ , defined as $\left\{ \begin{aligned} u_1 &= 0\\ u_{n + 1} &= \dfrac{u_n + \sqrt{6u_n + 3}}{4}, \forall n \in \mathbb Z^+ \end{aligned} \right.$ . Knowing that $a$ and $b$ are two real numbers not equal to zero such that $\lim[a^n(u_n - 1)] = b$ , calculate the value of $a + b$ . [For context, this question is taken from an exam whose format consists of 50 multiple-choice questions with a time limit of 90 minutes. Calculators are the only electronic device allowed in the testing room. (You know those scientific calculators sold at stationery stores and sometimes bookstores? They are the goods.) I need a solution that works within these constraints. Thanks for your cooperation, as always. (Do I need to sound this professional?) By the way, if the wording of the problem sounds rough, sorry for that. I'm not an expert at translating documents.] Why was this question there? WHY WAS THIS IN MY MID-SEMESTER EXAM? (>_<) I don't need answers, I need to be angered . (This is a joke, please take it with a grain of salt. I do actually need answers to this problem.) Here are my observations. According to the WolframAlpha , the general term of the sequence above is $$u_n = \left(1 - \dfrac{1}{2^{n - 1}}\right)\left(1 - \dfrac{2 - \sqrt 3}{2^{n - 1}}\right), \forall n \in \mathbb Z^+$$ If we define sequences $(v_n)$ and $(w_n)$ as $v_n = 1 - \dfrac{1}{2^{n - 1}}, \forall n \in \mathbb Z^+$ and $w_n = 1 - \dfrac{2 - \sqrt 3}{2^{n - 1}}, \forall n \in \mathbb Z^+$ respectively, then it can be obtained that $u_n = v_nw_n, \forall n \in \mathbb Z^+$ and $$\left\{ \begin{aligned} v_1 &= 0\\ v_{n + 1} &= \dfrac{v_n + 1}{2}, \forall n \in \mathbb Z^+ \end{aligned} \right. \text{ and } \left\{ \begin{aligned} w_1 &= -1 + \sqrt 3\\ w_{n + 1} &= \dfrac{w_n + 1}{2}, \forall n \in \mathbb Z^+ \end{aligned} \right.$$ That means $$\begin{aligned} v_{n + 1}w_{n + 1} = \dfrac{v_nw_n + \sqrt{6v_nw_n + 3}}{4} &\iff \dfrac{(v_n + 1)(w_n + 1)}{4} = \dfrac{v_nw_n + \sqrt{6v_nw_n + 3}}{4}\\ &\iff v_n + w_n + 1 = \sqrt{6v_nw_n + 3}\\ &\iff w_n = (2v_n - 1) + \sqrt 3(1 - v_n), \forall n \in \mathbb Z^+ \end{aligned}$$ Anyhow, about the sequence $(u_n)$ itself, it is strictly increasing bounded. More specifically, we have that $u_n \in [0; 1), \forall n \in \mathbb Z^+$ . The same goes for sequences $(v_n)$ and $(w_n)$ . Actually, $v_{n + 1} = \dfrac{v_n + 1}{2}$ and $w_{n + 1} = \dfrac{w_n + 1}{2}$ , those look familiar, hmmm~ Of course, I forgot. If we let $v_n = \cos a_n, \forall n \in \mathbb Z^+$ and $w_n = \cos b_n, \forall n \in \mathbb Z^+$ , then it can be obtained that $$\left\{ \begin{aligned} a_1 &= \dfrac{\pi}{2}\\ \cos(a_{n + 1}) &= \cos^2\dfrac{a_n}{2}, \forall n \in \mathbb Z^+ \end{aligned} \right. \text{ and } \left\{ \begin{aligned} b_1 &= \arccos(-1 + \sqrt 3)\\ \cos(b_{n + 1}) &= \cos^2\dfrac{b_n}{2}, \forall n \in \mathbb Z^+ \end{aligned} \right.$$ Nevermind, that didn't work as well as I had thought. What was I doing this entire time? You might be wondering. Well, I'm trying to find the general term of sequence $(u_n)$ without the need of a laptop, since you can't take that into the testing room. Anyhow, for the second part of the problem, first of all, let $\lim u_n = m$ , then we have that $m = \dfrac{m + \sqrt{6m + 3}}{4} \iff m = 1$ . Again, the same goes for sequences $(v_n)$ and $(w_n)$ . Futhermore, $$\begin{aligned} \left\{ \begin{aligned} 2^n(v_n - 1) &= -2\\ 2^n(w_n - 1) &= 2\sqrt 3 - 4 \end{aligned} \right. &\iff \left\{ \begin{aligned} 2^n(v_n + w_n - 2) &= 2\sqrt{3} - 6\\ 4^n(v_n - 1)(w_n - 1) &= 8 - 4\sqrt{3} \end{aligned} \right.\\ &\implies 4^n\left[u_n - \left(\dfrac{2\sqrt{3} - 6}{2^n} + 2\right) + 1\right] = 8 - 4\sqrt{3}\\ &\iff 4^n(u_n - 1) = (8 - 4\sqrt{3}) - 2^n(6 - 2\sqrt{3})\\ &\iff 2^n(u_n - 1) = \dfrac{(8 - 4\sqrt{3})}{2^n} - (2\sqrt{3} - 6), \forall n \in \mathbb Z^+\\ &\implies \lim[2^n(u_n - 1)] = 2\sqrt{3} - 6 \end{aligned}$$ In conclusion, $a + b = 2 + (2\sqrt{3} - 6) = 2\sqrt{3} - 4$ . My question is more focused on the first part of the problem, on how the general term of sequence $(u_n)$ . As always, thanks for reading (and even more if you could help~) By the way, the options were $-1, \sqrt 3 - 1, 2\sqrt 3 - 4$ and $4 - 2\sqrt 2$ .","['limits', 'sequences-and-series']"
4440065,A sigma algebra with countably infinite atoms does not exist,"I am self-studying Rene Schilling's Measure Theory book: Show that there cannot be a σ-algebra $A$ which contains countably infinitely many sets.
The outline for the proof goes as follows: We first show that such the set $A$ contains | $\mathbb{N}$ | disjoint atoms. (this is the bulk of the proof) Then, the solution manual says: ""Since $A$ contains all countable unions of sets from $A_0$ [the set of all atoms], and since there are more than countably many such unions, it is clear that $|A| > |\mathbb{N}|$ ."" (At which point the proof concludes, since it's a contradiction, as the sigma algebra itself must have cardinality of $|\mathbb{N}|$ ) But it is not clear to me that the countable unions of countably infinite atoms are uncountably infinite. In fact, it seems to me an extension of the proof that shows a bijection from the rationals to the naturals:
I could list out the atoms in a grid, and take the union of the first atom and the second atom, then the union of the first atom and the third atom, and then I could enumerate all $|\mathbb{N}\times\mathbb{N}|$ two-atom unions this way. Next, I would use an inductive argument to count all the three-atom unions, and so on and so forth. In the end, I would still end up with a countably infinite number of sets, wouldn't I? Thanks a bunch for the help--I wish I had a prof. that I could ask questions of, but Math Stack Exchange is a great second!","['measure-theory', 'combinatorics']"
4440066,Computing the area of a cone section bounded by a plane,"Compute the area of the section of the $x^2=y^2+z^2$ in the first octant and bounded by the plane $y+z=a>0.$ source: Demidovich, task 2215 My thoughts: I tried applying the following formula: Suppose $Q\subset\Bbb R^k$ is a cube and $\boldsymbol F\in C^1(Q,\Bbb R^n)$ is injective. Then the $k-$ dimensional area of $\boldsymbol F$ is defined as $\nu(\boldsymbol F)=\int_Q\sqrt{\det\nabla F(\boldsymbol u)^T\nabla F(\boldsymbol u)}d\boldsymbol u.$ I think this section, call it $S$ can be written as $\{(x,y,z)\in\Bbb R^3\mid x^2=y^2+z^2\}\cap\bigcup_{v\in[0,1]}\{(x,y,z)\in\Bbb R^2 y+z=av\}.$ I tried parametrizing each of the intersection in the union as follows: $$(x,y,z)=\boldsymbol F(u,v)=(f(y,z),y,z)=\left(\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac\pi2u\right).$$ $\sin\left(\frac\pi2+\frac\pi4\right)$ arose from the polar equation of the line $y+z=av,$ which yields $r=\frac{av}{\sin\varphi+\cos\varphi}.$ Now, $$\nabla F(u,v)=\begin{bmatrix} -\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)} & \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u))}& \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\ \frac\pi2 &0\end{bmatrix},i=1,2$$ where the partials in the first column are obtained as $\frac{\partial F_i(u,v)}{\partial u}=-\frac{av\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2\left(1-\cos\left(\pi u+\frac\pi2\right)\right)}=-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)}.$ Proceeding in a similar manner, using the formulae $$\sin^2\alpha=\frac{1-\cos(2\alpha)}2\\\cos^2\alpha=\frac{1+\cos(2\alpha)}2\\\cot\alpha=\frac{\sin(2\alpha)}{1-\cos(2\alpha)}$$ I got $$\nabla F(u,v)^T\nabla F(u,v)=\begin{bmatrix}\frac\pi4+a^2v^2\pi^2\frac{1-\sin(\pi u)}{(1+\sin(\pi u))^2}&-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}\\-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}&\frac{2a}{1+\sin(\pi u)}\end{bmatrix}$$ but the determinant doesn't seem quite operable. Is there any elegant way? Should I sum the length of the curves that are intersections instead?","['integration', 'multivariable-calculus']"
4440071,Is my understanding about this complex contour integral correct?,I have to evaluate $$\int_{\gamma} \sin(z)\cos(2z)dz$$ over the above contour. My question is: Is the info given sufficient to solve this problem (just the contour image and integral) and can I take $1$ and $-0.5$ as my endpoints/bounds to solve this integral?,"['complex-analysis', 'multivariable-calculus', 'calculus']"
4440078,"Evaluate $\lim_{n\to\infty}\int_1^n\frac{\ln x}{c_n+x\ln x}\,dx$","Let $c_n$ be an unbounded sequence: $c_n\to\infty$ when $n\to\infty$ Find the value of the limit: $$\lim_{n\to\infty}\int_1^n\frac{\ln x}{c_n+x\ln x}\,dx$$ What I managed to find was the fact that $$\lim_{n\to\infty}\frac{\ln x}{c_n+x\ln x}=0$$ I was thinking  that the limit is going to be $0$ , but I have to prove it still. Maybe it is not and I am wrong I do not know for sure. I denoted $$I(n)=\int_1^n\frac{\ln x}{c_n+x\ln x}\,dx$$ I was thinking of finding a recurrence formula for this integral, Though, I could not be able to make any significant progress. What should I do?","['limits', 'real-analysis']"
4440117,Apparent complex integration paradox,"I encountered the following integral : $$ 
\int_{\gamma} \frac{dz}{z-1-i},
$$ which has to be integrated along two straight-line contours : $\gamma_{1} : 2i$ to $3$ $\gamma_{2} : 3$ to $0$ and then from $0$ to $2i$ . The integral will be $\log (z-1-i)$ , of course. Considering principal values of complex logarithms for uniqueness we get the following results : $\log(2-i) - \log(i-1)$ $\log(i-1) - \log(2-i)$ . All good (not really). Now, consider adding the two integrals above. This will form a closed loop with a simple pole at $1+i$ so the result should be $2\pi i$ by Cauchy Integral formula. However, adding the above two gives zero. How come? I thought I was doing the integrals incorrectly so I checked with WolframAlpha as well but apparently I can't find any issues. Would appreciate any help. Thanks!","['integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
4440233,Find all the functions $f:\mathbb{Z}^+ \to \mathbb{Z}^+$ such that $f(f(x)) = 15x-2f(x)+48$.,"Find all the functions $f:\mathbb{Z}^+ \to \mathbb{Z}^+$ such that $f(f(x)) = 15x-2f(x)+48$ . If $f$ is a polynomial of degree $n$ , we have that $\deg(f(f(x))) = n^2$ and $\deg(15x-2f(x)+48)=n$ . Therefore, the only possible polynomials that satisfy the condition have degree $0$ or $1$ . Let $f:\mathbb{Z}^+ \to \mathbb{Z}^+$ be a function that holds the condition of the problem given by $f(x)=ax+b$ for some constants $a$ and $b$ . Since $$f(f(x)) = f(ax+b) = a(ax+b)+b = a^2x + (a+1)b$$ and $$15x-2f(x)+48 = 15x-2(ax+b)+48 = (15-2a)x+(48-2b),$$ it follows that $$a^2+2a-15=0 \quad\text{and}\quad (a+1)b=48-2b.$$ From the first equation, we get that $a=-5$ or $a=3$ . If $a=-5$ , from the second equation we get that $b=-24$ , and it contradicts that $f[\mathbb{Z}^+]\subseteq \mathbb{Z}^+$ . If $a=3$ , then $b=8$ . Therefore, $f(x)=3x+8$ is the only polynomial that satisfies the condition of the problem.  I guess that this is the only solution, but I do not know how to prove it. Edit: I was trying to prove that the iterations of any function $f$ that satisfies the problem have the same behaviour. For instance, by iterating $f$ we have that $2f^3(x)+f^4(x)-15f^2(x)=48$ , so this functions are almost the same except for constant terms. Is this usefull this idea to complete the problem?","['contest-math', 'number-theory', 'functional-equations']"
4440239,Showing that a function of two brownian motions is a martingale.,"Let $B$ be a standard Brownian motion, let $f$ be a smooth function taking values in $[a,b]$ where $0<a<b<\infty$ and assume that the derivative $f^\prime$ is bounded. For $t\in[0,1]$ and $x\in\mathbb{R}$ , let $$U(t,x)=\mathbb{E}\{f(x+B_{1-t})^2\}.$$ Let $M_t$ = $U(t,W_t)$ where $W$ is a Brownian motion independent of $B$ . I am tasked with showing that $M$ is a martingale with respect to the filtration generated by $W$ . The question suggests that I should do this by ""directly computing conditional expectations and the definition of Brownian motion"". I am quite unsure how to do this - particularly the part regarding the filtration generated by W. Any advice would be greatly appreciated! Thank you.","['expected-value', 'stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory']"
4440240,Making Figure-8 into a manifold,"So I am using the map in Lee's Smooth manifold book $\beta(t) = (\sin 2 t, \sin t)$ . This is an injective immersion where $t \in (-\pi, \pi).$ Now the image is not a manifold of $\mathbb{R}^2$ because an open ball around near the origin gives a picture of ""X"" and that doesn't look like $\mathbb{R}$ ? Can we make it a 1-manifold in $\mathbb{R}^2$ ? Or would that require the ""X"" to look like $\mathbb{R}$ ? Which is not possible. Is that the problem with self-intersections on $\mathbb{R}^2$ ? Because it would create pictures like ""X"" or "" $\bot$ "" and those don't look like $\mathbb{R}$ ? Next, I've been told that if we look use the induced topology by $\beta$ from $(-\pi, \pi)$ , this can be made into a manifold (of $\mathbb{R}$ ?) 3.Even if I look near $0$ with an open interval in the image set, I am still going to get an ""X"" shape no? Or do they mean that if I delete the intersection point, a neighborhood near the center would ""ignore"" the other three pieces of the ""X""?","['intuition', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
4440278,Proof verification: every injection $A \to A$ is surjective,"I'm trying to prove that if $A$ is a finite set, then every injection $f: A \to A$ is surjective. Here is my attempt. Let $f: A \to A$ be an injection. For each $a \in A$ , let $N(a)$ denote the cardinality of the collection $\{f^{-1} (\{a\}\}$ of preimages for $a$ under $f$ . Notice that, because $A$ is finite, $f^{-1} (\{a\})$ is finite for each $a$ , so $N(a)$ is well-defined. As $f$ is injective, for each $a$ , $N(a) \leq 1$ ; otherwise, for some $a \in A$ , there exist distinct preimages $b,b' \in A$ such that $f(b) = a = f(b')$ , contradicting injectivity of $f$ . Furthermore, as the fibres of $f$ partition its domain, we have $\sum\limits_{a \in A} N(a) = |A|$ . Suppose for the sake of contradiction that $f$ is not surjective. Then $A \setminus f(A) \neq \emptyset$ , so there exists $t \in A \setminus f(A)$ . Then $N(t) < 1$ , i.e., $N(t) = 0$ . We then have \begin{align*}
\sum\limits_{a \in A} N(a) = \sum\limits_{a \in A \setminus \{t\}} N(a) + N(t) \leq \sum\limits_{a \in A \setminus \{t\}} 1 + N(t) = |A\setminus \{t\}| + 0 < |A|
\end{align*} since $A \setminus \{t\}$ is a proper subset of $A$ , contradicting surjectivity of $f$ . Therefore, there exists no such $t$ , and we have $A \setminus f(A) = \emptyset$ , hence $A = f(A)$ , so $f$ is surjective. How does this look? I worry that my assertion in the final step is a leap of logic. Even though it seems obvious that $A \setminus \{t\}$ has cardinality strictly less than $A$ , I think this is what I'm trying to prove.","['elementary-set-theory', 'solution-verification']"
4440294,Fresnel Integral Proof (without rigorous complex analysis),"I’d like to present my dodgy proof of the Fresnel Integrals, which I wrote before I knew anything about complex analysis. It takes some… liberties; yet, it still managed to produce the right value for both integrals, so I thought it might be worth sharing. Perhaps, with some input, I could change it into something more rigorous? I also wanted to leave a fun teaser at the end. The Proof So, the goal is to evaluate both $\int_0^\infty \cos{\left(x^2\right)}dx$ and $\int_0^\infty \sin{\left(x^2\right)}dx$ , the Fresnel Integrals. Start by using Euler’s identity: $$e^{ix} = \cos{x} + i\sin{x}$$ Substituting $x$ for $-x^2$ , this leaves: $$e^{-ix^2} = \cos{\left(x^2\right)} - i\sin{\left(x^2\right)}$$ $$\implies \int_0^\infty e^{-ix^2}dx = \int_0^\infty\cos{\left(x^2\right)}\,dx - i\int_0^\infty\sin{\left(x^2\right)}\,dx$$ Notice how the left-hand side looks almost exactly like the Gaussian integral? Using that observation, I used the substitution $ix^2 = u^2 \implies u =\pm\sqrt{i} \cdot x$ . Since this integral is taken within the interval $[0, \infty)$ , we can take only the positive branch from the square root. When $x = 0, u = 0$ ; as $x\to\infty, u\to\infty$ . $dx = \frac{du}{\sqrt{i}} = -i\sqrt{i} \cdot du$ . $$I = -i\sqrt{i}\int_0^\infty e^{-u^2}du$$ $\sqrt{i} = \frac1{\sqrt{2}} + i\frac1{\sqrt{2}}$ (taking only the positive branch), $\implies -i\sqrt{i} = \frac1{\sqrt{2}} - i\frac1{\sqrt{2}}$ . The new integral $\int_0^\infty e^{-u^2}du = \frac{\sqrt{\pi}}{2}$ . All of this leaves: $$I = \left(\frac1{\sqrt{2}} - i\frac1{\sqrt{2}}\right)\frac{\sqrt{\pi}}{2}$$ $$ = \sqrt{\frac{\pi}8} - i\sqrt{\frac{\pi}8} $$ With this, all that’s left to do is compare the real and imaginary parts of $I$ : $$\Re{\left(I\right)} = \sqrt{\frac{\pi}{8}} = \int_0^\infty\cos{\left(x^2\right)}dx$$ $$\Im{\left(I\right)} = -\sqrt{\frac{\pi}{8}} = -\int_0^\infty\sin{\left(x^2\right)}dx$$ $$\therefore  \int_0^\infty\cos{\left(x^2\right)}dx = \int_0^\infty\sin{\left(x^2\right)} = \sqrt{\frac{\pi}{8}}$$ The Problem The most blaring issue here is my assumption that $\lim_\limits{x\to\infty} \sqrt{i} \cdot x = \infty$ . This is nonsense when you look at it this idea of the limit on the complex plane. The complex function $f(x) = \sqrt{i} \cdot x$ can be expressed as $f(x) = \frac{x}{\sqrt2} + i\frac{x}{\sqrt2}$ . To help visualize this, I plotted this on a graphing calculator, which gives the following: As you can see, as this x approaches infinity, this function does approach an infinity; the function approaches the corner of the real-imaginary plane $\infty + i\infty$ . However, this is not the same as $\infty$ , which represents approaching the edge of the horizontal real number line. To put this more rigorously, this picture shows that $\lim_\limits{x\to\infty}\sqrt{i}\cdot x \neq \lim_\limits{x\to\infty}x$ . But why did this proof still work? It clearly returned the right value for both integrals. Is this step really as nonsensical as it seems? And if not, what kind of implications could that carry? The Teaser — The Icing on the Cake For positive n, $$\int_0^\infty e^{-ax^n}dx = a^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)}$$ You could derive this yourself with the substitution $u = ax^n$ . But all this thinking about limits to complex infinities has me thinking: why not plug in $a = i$ ? Is that valid? This would yield: $$\int_0^\infty e^{-ix^n}dx = i^{-\frac1{n}}\Gamma{\left(1+\frac1{n}\right)}$$ Using the same logic as before, this would not make sense, since as $x\to\infty$ in the substitution above, $u\to -i\infty$ , which is not the same as $u$ approaching normal infinity. But it managed to work before—could it work now? With $n = 2$ —our original case—it does seem to return the same value as before.","['complex-analysis', 'fresnel-integrals']"
4440308,"How do you calculate combinations with fractions (eg, ""$\frac13$ choose $2$"")? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question I was given "" $\frac13$ choose $2$ "" and asked to compute. I checked the answer and it's $-\frac19$ , but I have no clue how to arrive at this answer. How do you deal with fractions in the combination formula?","['combinations', 'binomial-coefficients', 'discrete-mathematics']"
4440379,Is Brownian motion a semimartingale?,"I have read an article about semimartingales on Wikipedia and it says that: ""A Brownian motion is a semimartingale"" . However, it is hard for me to find any proof of this statement, and I really doubt about its accuracy. Can any one verify if the statement is true, or show me some counter examples if the statement is wrong? Here is the link of the article: https://en.wikipedia.org/wiki/Semimartingale","['martingales', 'brownian-motion', 'probability-theory']"
4440391,"Given $v\in\mathbb R^3$, what are the solutions $A\in\mathbb R^{3\times 3}$ of $A^2=I+vv^{T}$?","The following question arose while deriving the explicit form of symmetric Lorentz transformations: Given $v\in\mathbb R^3$ , what can we say about the set \begin{equation}
\{A\in\mathbb R^{3\times 3}:A^2=I+vv^{T}\}\ 
\end{equation} (with $I\in\mathbb R^{3\times 3}$ being the identity matrix)? Are there any analogies to the case where $A$ and $v$ are real numbers, i.e. are there exactly two solutions which differ by a minus sign?","['matrices', 'linear-algebra']"
4440406,Is there any closed form for $\frac{d^{2 n}( \cot z)}{d z^{2 n}}\big|_{z=\frac{\pi}{4}}$?,"Latest Edit Thanks to Mr Ali Shadhar who gave a beautiful closed form of the derivative as $$\boxed{S_n = \frac { \pi ^ { 2 n + 1 } } { 4 ^ { n + 1 } ( 2 n ) ! } | E _ { 2 n } | }, $$ where $E_{2n}$ is an even Euler Number . In the post , I had found the sum $$
\sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k+1)^{3}}= \frac{\pi^{3}}{32},
$$ and want to investigate it in a more general manner, $$
S_{n}=\sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k+1)^{2 n+1}}
$$ where $n\in N.$ $$
\begin{aligned}
S_{n}&= \lim _{N \rightarrow \infty} \sum_{k=0}^{N} \frac{1}{(4k+1)^{2 n+1}}-\sum_{k=0}^{N} \frac{1}{(4 k+3)^{2 n+1}} \\
&=\frac{1}{4^{2 n+1}} \lim _{N \rightarrow \infty} \left[\sum_{k=0}^{N} \frac{1}{\left(k+\frac{1}{4}\right)^{2 n+1}}-\sum_{k=0}^{N} \frac{1}{\left(k+\frac{3}{4}\right)^{2 n+1}}\right] \\
&=\frac{1}{4^{2 n+1}} \lim _{N \rightarrow \infty} \left[\sum_{k=0}^{N} \frac{1}{\left(k+\frac{1}{4}\right)^{2 n+1}}+\sum_{k=0}^{N} \frac{1}{\left(-k-\frac{3}{4}\right)^{2 n+1}}\right] \\
&=\frac{1}{4^{2 n+1}} \lim _{N \rightarrow \infty} \left[\sum_{k=0}^{N} \frac{1}{\left(k+\frac{1}{4}\right)^{2 n+1}}+\sum_{k=-N}^{-1} \frac{1}{\left(k+\frac{1}{4}\right)^{2 n+1}}\right] \\
&=\frac{1}{4^{2 n+1}}\left[\lim _{N \rightarrow \infty} \sum_{k=-N}^{N} \frac{1}{\left(k+\frac{1}{4}\right)^{2 n+1}}\right] 
\end{aligned}
$$ Using the Theorem: $$(*):\pi \cot (\pi z)=\lim _{N \rightarrow \infty} \sum_{k=-N}^{N} \frac{1}{k+z} ,\quad \forall z \not \in Z.$$ Differentiating (*) w.r.t. $z$ by $2 n$ times yields $$
\begin{aligned}
& \lim _{N \rightarrow \infty} \sum_{k=-N}^{N} \frac{(-1)^{2 n}(2 n) !}{(k+z)^{2 n+1}}=\frac{d^{2 n}}{d z^{2 n}}[\pi \cot (\pi z)] \\
\Rightarrow & \lim _{N \rightarrow \infty} \sum_{k=-N}^{N} \frac{1}{(k+z)^{2 n+1}}=\frac{\pi}{(2 n) !} \frac{d^{2 n}}{d z^{2 n}}[\cot (\pi z)] 
\end{aligned}
$$ Now we can conclude that $$\boxed{\sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k+1)^{2n+1}}=\left.\frac{\pi^{2n+1}}{4^{2 n+1}(2 n) !} \frac{d^{2 n}}{d z^{2 n}}[\cot z]\right|_{z=\frac{\pi}{4}}}$$ My Question: Is there any closed form for $\displaystyle \left.\frac{d^{2 n} (\cot z)}{d z^{2 n}}\right|_{z=\frac{\pi}{4}}$ ?","['integration', 'trigonometry', 'power-series']"
4440413,On the order of matrix groups.,"I have two subgroups $A,B$ of $\text{GL}_2(\mathbb{C})$ such that $A \subset B$ and $[B:A]$ is finite. Then you can define the subgroups $A^1=\{a \in A: \det a =1\}$ and $B^1=\{b\in B: \det b =1\}$ . Do we always have that $[B^1:A^1]$ is finite? The problem I am trying to prove is that for any two orders $O,O'$ in a quaternion algebra $B$ over the rationals (or maybe even number fields) I want $\Gamma^1(O)$ and $\Gamma^1(O')$ to be commensurable. Here $\Gamma^1(O)$ is defined as the embedding of the units with norm $1$ into $M_2(\mathbb{C})$ as a group under multiplication, and then we mod out by $\{\pm 1\}$ . If we let $N:=\{\det b : b\in B\}$ and define for $n\in N$ that $B_n:=\{b\in B: \det b=n\}$ then we have a bijection between all pairs $B_n,B_{n'}$ for $n,n'\in N$ so I feel like $[B^1:A^1]\leq [B:A]$ should be true, but I am not sure about the details. Can anyone help?","['matrices', 'group-theory']"
4440463,"Does the ""physicist common knowledge"" that ""solenoidal vector fields have closed integral curves"" have any mathematical foundation?","I remember having heard some physicist claiming that the integral curves of the magnetic field have to be closed, or ""closed at $\infty$ "", due to the fact that the magnetic field is solenoidal, i.e., that its divergence is zero. I'm looking for a mathematical proof of this fact. Here's a (possible) mathematical formulation of the problem. Suppose $F:\mathbb{R}^3 \to \mathbb{R}^3$ is a $C^1$ solenoidal (i.e., such that $\mathrm{div}(F) = 0$ ) vector field. Let $I$ be an open interval and suppose that $\gamma:I\to\mathbb{R}^3$ is an integral curve for $F$ , i.e., $\gamma$ is a $C^1$ curve such that $\forall t\in I, \gamma'(t)=F\big(\gamma(t)\big)$ . Suppose that $(\gamma,I)$ is maximal, in the sense that if $J$ is another open interval such that $I \subset J$ and $\delta: J \to \mathbb{R}^3$ is another $C^1$ curve such that $\forall t\in J, \delta'(t)=F\big(\delta(t)\big)$ and $\delta \mid_I = \gamma$ , then it holds that $J=I$ . Suppose that $I = (a,b)$ , where $-\infty \le a < b \le +\infty$ . Is it true that one of the following must hold: $\lim_{t \to a} \|\gamma(t)\| = \lim_{t \to b} \|\gamma(t)\| = +\infty $ (i.e., the curve is closed at $\infty$ ), $\exists c\in(a,b), \exists d\in(c,b), \gamma(c) = \gamma(d)$ (i.e., the curve is closed)? Any proof (or pointer to a mathematical proof in the literature) or counterexample is very welcome.","['mathematical-physics', 'vector-fields', 'ordinary-differential-equations', 'dynamical-systems']"
4440479,Find the value of P[X>Y<Z>U] from a given joint distribution,"Suppose the joint distribution is given by some function $f(x, y, z, u)$ . Then find the value of $P[X>Y<Z>U]$ assuming that $f$ is non-zero only for $x > 0, y>0, z>0, u>0$ My approach: Simply integrate the joint distribution function, taking care of appropriate limits for all the possible cases. However, I keep getting some form of "" circular limits "" on the integral. For example, one arrangement of the values for $x, y, z, u$ can be $y < u < x < z$ and the integral looks like $\int_{0}^{z}\int_{u}^{\infty}\int_{0}^{x}\int_{y}^{\infty}f(x, y, z, u)dx dy dz du$ Here, the limits for $z$ and $u$ are dependent on each other's value. Similarly for $x$ and $y$ . What is the way to deal with such integral limits? Forgive me if this a silly doubt, but I've been out of practice for a while. Please note that the RV are not independent. Thanks","['statistics', 'definite-integrals', 'probability']"
4440501,"Efficiency curves, what is this called?","I am studying a set of functions. This seems like something other people would have studied too, and I'd like to know what other people are calling this and how to read up on it. These ""efficiency curve"" functions are: $$f(d_1, d_2, ..., d_n) \in \{0, 1\} $$ where $$d_i \in \mathbb{N} $$ and $$f(..., d_i, ...) = 1, d_i > 1 \implies f(..., d_i-1, ...) = 1$$ In two dimensions, this is one-to-one relatable with a monotonic decreasing function. Examples in two dimensions (green is 1, red is 0): You can easily count the number of 2d efficiency functions that fit in some boundary ( $b_1$ , $b_2$ ). It is: $$\frac{(b_1 + b_2)!}{b_1! b_2!} $$ Because looking at the function diagonally from the northeast, and distinguishing the tops and the rights of squares, you will always see $d_1$ tops and $d_2$ rights in some order, and every order is a valid and distinct efficiency curve. In three dimensions you can think of this as placing sugar cubes into a box such that gravity applied down, left, or back does not cause the cubes to move. You can look at this diagonally too and that picture uniquely defines the curve. Counting those is harder. Here is an enumeration of all the efficiency functions in three dimensions with boundaries $(2,2,2)$ : I'm interested in studying this problem in $n$ dimensions. Specifically, I'd like to count how many efficiency functions exist in $n$ dimensions with the boundaries $b_1, b_2, ..., b_n$ . But again, I'm not asking you to do my homework (and this is not homework). I'm just asking what other people call this problem and a reference to anybody else that has written about this.","['curves', 'functions', 'combinatorics', 'dynamic-programming', 'functional-analysis']"
4440503,Moore-Penrose pseudoinverse solves the least squares problem (SVD framework) [duplicate],"This question already has answers here : Prove uniqueness of solutions of different OLS matrix cases (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved I am a computer science researcher who has to learn some numerical linear algebra for my work. I have been struggling with the SVD and Moore-Penrose pseudoinverse as of late. I am trying to solve some problems to get more comfortable with what should probably be routine manipulations. First of all, I have gone through similar questions on Stack Exchange but I believe they were more general and are not equivalent. I am working in the framework where $A^{\dagger} = V\Lambda^{\dagger}U^T$ . So, basically, I am using SVD's. The matrix $A$ of course is identified with $U\Lambda V^T$ Problem Consider the matrix equation $Ax=y$ , where $A\in R^{m\times n}$ . The corresponding least squares problem is to find a least squares solution $x_{\text{LS}}$ that minimizes the Euclidean norm of the residual, i.e., $$\|Ax_{\text{LS}}-y\| = \min_{x \in \Bbb R^n} \|Ax-y\| = \min_{z \in \mbox{Ran}(A)}\|z-y\|$$ a) Show that $A^{\dagger}y$ is a least-squares solution and satisfies the normal equation $A^TAx=A^Ty$ . Why is this solution special? b) Show that $\ker(A^TA) = \ker(A)$ . c) Use the above results to deduce that $x \in \Bbb R^n$ is a least-squares solution if and only if it satisfies the normal equation. Help on any or all of these parts is appreciated. I'd also appreciate links to relevant posts. Like I said, I've read similar questions but did not understand them as they were in a more general framework. Edit: I have solved b). It didn't depend on a) as I had initially thought and it is pretty straightforward to solve, see eg. here: Prove that for a real matrix $A$, $\ker(A) = \ker(A^TA)$ Edit: I realize that part a) might be more involved than I had expected... Assuming parts a) and b), can someone help me with part c)?","['svd', 'matrices', 'pseudoinverse', 'least-squares', 'linear-algebra']"
4440528,How to find $E(\tau)$?,"Question. Let $X_1,X_2,...$ be an i.i.d sequence of random variable with $P(X_i=0)=P(X_i=1)=1/2$ . Let $\tau$ be the waiting time until the appearance of the six consecutive $1's$ . That is $$
\tau = \inf\{k \geq 6 : X_{k-5}=1,X_{k-4}=1,..,X_{k}=1\}
$$ Find $E(\tau)$ . My attempt : I am thinking to apply Optional Stopping theorem to find $E(\tau)$ . Firstly, I have shown that $P(\tau < \infty)=1$ . Let $$
Y_i = 2^61_{\{X_i=1,X_{i+1}=1,..X_{i+5}=1\}}
$$ Then I define a random variable $$
Z_n= \sum_{i =1}^n E[Y_i|F_n]-n
$$ I proved the $Z_n$ is a martingale w.r.t to $F_n$ where $F_n= \sigma(X_1,....X_n)$ . If I will show that $\tau$ is a stopping time, then
by Optional Stopping theorem , $$E(Z_{\tau})=E(Z_1)$$ . I got $E(Z_1)=0$ , this implies $$ E(\tau)= \sum_{i=1}^\tau E(E(Y_i|F_{\tau})$$ I am not sure how to conclude the last part of this question, can anyone help me with this? Can anyone also suggest how do I claim $\{\tau \leq n\}\in F_n$ to prove $\tau$ is a stopping time?","['random-walk', 'stochastic-processes', 'martingales', 'stopping-times', 'probability-theory']"
4440536,Finding all square roots of a matrix when it has distinct eigenvalues,"According to https://en.wikipedia.org/wiki/Square_root_of_a_matrix#Matrices_with_distinct_eigenvalues ,
""An n×n matrix with $n$ distinct nonzero eigenvalues has $2^n$ square roots."" And we can clearly see that they are given by decomposing $A$ into $SDS^{-1}$ where $D$ is diagonal and taking square roots of the eigenvalues; this generates $2^n$ pairwise distinct square roots. But how did we prove that these are the only matrices $B$ such that $B^2 = A$ ?","['matrices', 'linear-algebra']"
4440563,"What functions satisfy $\int_a^b f(x) c(x) \, dx \ge 0$ for all convex functions $f$?","This is an attempt to generalize Prove that $\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0$ for every $k \geq 1$ given that $f$ is convex. Inspired by that question and the given answers, I have the following Conjecture: Let $c:[a, b] \to \Bbb R$ be a continuous function. Then $$ \tag{1} \int_a^b f(x) c(x) \, dx \ge 0 $$ holds for all convex functions $f:[a, b] \to \Bbb R$ if and only if $$ \tag{2} \int_a^b c(x) \,dx = \int_a^b x c(x) \, dx = 0 \, . $$ The “only if” direction is easy: If $(1)$ holds for the four convex functions $x \mapsto \pm 1$ , $x \mapsto \pm x$ then $(2)$ holds. So the interesting part is the “if” direction. In the above mentioned Q&A this has been proven for the functions $c(x) = \cos(k x)$ (on the interval $[0, 2 \pi]$ ). Some of the proofs given there use the fact that $$
\int_0^{2\pi}  \cos (x) \, dx = \int_0^{2\pi} x \cos (x) \, dx = 0 \, ,
$$ but all proofs use also symmetries of the cosine function, trigonometric identities, or where the cosine is positive and negative in $[0, 2 \pi]$ . My conjecture is that these additional properties of the cosine are not needed, and that $(2)$ alone is sufficient to prove $(1)$ for all convex functions $f$ . As pointed out in the comments, the following is wrong: There is a simple proof for the “if” direction under the additional assumption that $f$ is twice continuously differentiable: Let $c_1$ be an antiderivative of $c$ , and $c_2$ be an antiderivative of $c_1$ . By adding a constant to $c_2$ , if necessary, we can assume that $c_2(x) \ge 0$ on $[a, b]$ . If $(1)$ holds then $c_1(a) = c_1(b)$ and $c_2(a) = c_2(b)$ , and for all convex functions $f$ on $[a, b]$ is, using integration by parts (twice): $$  \int_a^b f(x) c(x) \, dx  = \int_a^b f''(x) c_2(x) \, dx  \ge 0 \,. $$ What I am looking for is a proof of the conjecture (the “if” direction) which works for all convex functions $f$ , without additional assumptions on differentiability.","['convex-analysis', 'integral-inequality', 'real-analysis']"
4440574,Show that $\mathbb{E}\Big[\mathrm{ln}\big(\frac{X}{\mathbb{E}[X]}\big)\Big] < 0$,We know that $X > 0$ and that $\mathbb{E}[X] < \infty$ . Show that $\mathbb{E}\Big[\mathrm{ln}\big(\frac{X}{\mathbb{E}[X]}\big)\Big] < 0$ . Could someone show me a way to prove it? Thanks.,"['statistics', 'proof-writing', 'expected-value', 'probability-theory', 'probability']"
4440616,Why in Green's theorem can we not simply integrate $y \mathrm{d} x$ instead of $y \mathrm{d} x - x \mathrm{d} y$?,"According to the multidimensional Stoke's theorem, in order to evaluate the integral of a form $\omega$ , I just have to find a one-form $\alpha$ so that $d\alpha=\omega$ and then use $\int_{\partial \Omega} \alpha=\int_{\Omega} \omega$ say $\omega$ is just the area form, $dx \wedge dy$ . Then, according to Stokes' theorem I just have integrate $-x dy$ around the contour. However, according the Green's theorem I should choose rather the one form $y dx-x dy$ . I can see the second one is the right choice but I can't see why the first one is the wrong one. Can anyone tell me the error in my reasoning please?","['differential-forms', 'greens-theorem', 'vector-analysis', 'differential-geometry']"
4440617,How to calculate $\lim\limits_{n\rightarrow \infty }(n-\sum^n_{k=1}\cos\frac{2k}{n\sqrt{n}}) $?,"I tried this: $\lim\limits_{n\rightarrow \infty }(n-\sum^n_{k=1}\cos\frac{2k}{n\sqrt{n}}) $ = = $ \lim\limits_{n\rightarrow \infty }(1-\cos\frac{2}{n\sqrt{n}}) $ + $ \lim\limits_{n\rightarrow \infty }(1-\cos\frac{4}{n\sqrt{n}}) $ + ... + $ \lim\limits_{n\rightarrow \infty }(1-\cos\frac{2n}{n\sqrt{n}}) $ = = $ \lim\limits_{n\rightarrow \infty }\frac{(1-\cos\frac{2}{n\sqrt{n}})(1+\cos\frac{2}{n\sqrt{n}})}{1+\cos\frac{2}{n\sqrt{n}}} $ + $ \lim\limits_{n\rightarrow \infty }\frac{(1-\cos\frac{4}{n\sqrt{n}})(1+\cos\frac{4}{n\sqrt{n}})}{1+\cos\frac{4}{n\sqrt{n}}} $ + ...+ $ \lim\limits_{n\rightarrow \infty }\frac{(1-\cos\frac{2n}{n\sqrt{n}})(1+\cos\frac{2n}{n\sqrt{n}})}{1+\cos\frac{2n}{n\sqrt{n}}} $ = = $ \lim\limits_{n\rightarrow \infty }\frac{\sin^2\frac{2}{n\sqrt{n}}}{1+\cos\frac{2}{n\sqrt{n}}} $ + $ \lim\limits_{n\rightarrow \infty }\frac{\sin^2\frac{4}{n\sqrt{n}}}{1+\cos\frac{4}{n\sqrt{n}}} $ + ... + $ \lim\limits_{n\rightarrow \infty }\frac{\sin^2\frac{2n}{n\sqrt{n}}}{1+\cos\frac{2n}{n\sqrt{n}}} $ = ... , but from here I don't know what to do.","['limits', 'trigonometry']"
4440634,Discrete Probability Question With No Named Distribution,"I've got a real world probability problem that I have been able to solve easily using simulation but that I am struggling to find (out of pure interest) an analytic solution to. For privacy reasons, I have changed both the context and the actual numbers. A vendor is selling tickets. Customers either purchase 1 ticket with probability 60%, 2 tickets with probability 30%, or 3 tickets with probability 10%. If the vendor needs to sell 10 tickets, how many customers on average does he need to sell to (assuming customer purchases are iid)? A simulation produces an average of 6.6 customers, but I am curious what an analytic solution to this might look like (if there exists a convenient one) as there is no named distribution I can think of that corresponds to this problem.",['probability']
4440653,Can a relation that doesn't pass the vertical line test be considered a function from R to a subset of R?,"I understand that a function $f: \mathbb{R}\to \mathbb{R}$ cannot provide more than one value per input without failing to be a function, but what about $g: \mathbb{R} \to X$ or $h: X \to X$ where $X$ is the set of subsets of $\mathbb{R}$ .  Can $\sqrt{x}$ be considered a function from $\mathbb{R} \to X$ in this circumstance without restricting its range to only positive numbers since a subset of $\mathbb{R}$ is still a single member of $X$ ?  I'm not sure if I'm missing something or not, but for example when the vertical line test is talked about regarding a relation being a function or not it seems like it's specifically about a function from $\mathbb{R} \to \mathbb{R}$ .  What am I not understanding?",['functions']
4440662,"provided $f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n}$, what is f(x)?","$$
f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n}
$$ Here is one of the solution from my workbook: $$
 \begin{array}{l}
f( x+1) \ =\ \lim _{n\rightarrow \infty }\left[\left( 1+\frac{2+x}{n-2}\right)^{\frac{n-2}{2+x}}\right]^{n\left(\frac{2+x}{n-2}\right)} =\ e^{2+x} =e^{1+( x+1)}\\
f( x) \ =\ e^{1+x}
\end{array}
$$ but is a bit preplexing for me :(","['limits', 'proof-explanation', 'real-analysis']"
4440691,"$\sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0$ and $\nabla f(0)=0$ implies constancy of $f$ in $B_1(0)$","Let $B_1(0)$ be the unit ball in $\mathbb R^n$ centered at the origin. Assume that the function $f\in  C^2(B_1(0))$ . Prove that $1)$ If $f$ satisfies $$\sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0$$ on $B_1(0)$ , and $\nabla f(0) = 0$ , then $f$ is constant in $B_1(0)$ . $2)$ If $f$ satisfies $$x_i\frac{\partial f}{\partial x_j}-x_j\frac{\partial f}{\partial x_i}=0,i,j=1,\cdots,n$$ on $B_1(0)$ ,then $f$ is constant on the sphere $\{x:x\in B_1(0),\vert x\vert=\frac{1}{2}\}$ . My intuition for the first problem is that if letting $L=\sum_{i=1}^{n}x_i\frac{\partial}{\partial x_i}$ , then we have $L^2f=Lf$ . But I don't know if this will proceed next, I think this maybe associated with the Hessian matrix or the maximum principle, but what property of Hessian matrix should I use here? Also this seems not the exactly the form of maximum principle so I don't know how to extend it. I don't know how to use the condition to proceed. Are there any solutions or suggestions of proceeding? Thanks in advance!","['partial-derivative', 'hessian-matrix', 'derivatives', 'analysis']"
4440762,Quadratic Gauss sum and a polynomial $ \equiv -1 \pmod p$,"I'm trying to solve problem 6.22 in Ireland and Rosen's A Classical Introduction to Modern Number Theory (second edition). The exercise is about the sign of the Gauss sum $g = \sum_{j=1}^{p-1} \left(\frac{j}{p}\right)\zeta^j$ , where $p$ is a fixed prime and $\zeta = e^{2\pi i/p}$ . In the chapter this problem is reduced to showing that $\varepsilon = 1$ makes the following polynomial equality hold and $\varepsilon = -1$ doesn't (those are the only values $\varepsilon$ can take and we know one of them works). $$\sum_{j=1}^{p-1}\left(\frac{j}{p}\right)(1+t)^j-\varepsilon\prod_{k=1}^{(p-1)/2}((1+t)^{2k-1}-(1+t)^{p-(2k-1)})=((1+t)^p-1)h(t)$$ Here $h(t)\in\mathbb{Z}[t]$ is some polynomial we're not interested in. Following the book's approach I looked at the coefficients of $t^{(p-1)/2}$ in both sides. On the left-hand side we have $$\sum_{j=(p-1)/2}^{p-1}\left(\frac{j}{p}\right)\binom{j}{(p-1)/2}-\varepsilon\prod_{k=1}^{(p-1)/2}(4k-p-2)$$ and on the right-hand side we have a multiple of p, so equating mod $p$ and multiplying by $((p-1)/2)!$ yields $$\sum_{j=(p-1)/2}^{p-1}\left(\frac{j}{p}\right)j(j-1)...(j-(p-1)/2+1) \equiv \varepsilon((p-1)/2)!\prod_{k=1}^{(p-1)/2}(4k-2) \equiv -\varepsilon \ (\textrm{mod}\ p).$$ The second congruence is not hard to justify. However, I have no idea how to show the first expression is -1. I've tried to interpret it as the polynomial $$f(x) = \sum_{j=1}^{p-1}\left(\frac{j}{p}\right)(x+j)(x+j-1)...(x+j-(p-1)/2+1)$$ evaluated at $x=0$ . I've expanded it for small primes and got that $f(x)$ is identically congruent to -1, but I don't know how to prove that in general. Any help with the polynomial or any alternative approaches to the problem would be appreciated. Thanks in advance :)",['number-theory']
4440783,About an equivalent condition for convergence of a scalar series,So I came across this problem a while back when one of my friends suggested it. The question asks to show that the scalar series $\sum_{n=1}^\infty a_n$ converges if and only if $$\sum_{n=1}^\infty \left(1-\frac1{2^2}+\cdots+\frac{(-1)^n}{n^2}\right)a_n$$ converges. The scalar field under consideration is real or complex. We were able to show the result holds if $a_n>0$ . But the general case is eluding us. Any ideas on how to approach this?,"['complex-analysis', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4440793,Difference in Prior and Posterior Probability,"I'm a bit puzzled by the answer I'm getting in a quite simple problem. $4$ athletes simultaneously shoot at the target. It is known that the probability of hitting the first hunter is $0.4$ , second - $0.6$ , third - $0.7$ , fourth - $0.8$ . As a result, one of the athletes did not hit the target. What is the probability that it was the second athlete? I'll give my solution: Event $A$ - the second athlete did not hit the target Event $B$ - $3$ out of $4$ athletes hit the target Based on the condition of the problem, I thought that we need to find $P(A|B)$ . So, $P(A|B)=\frac{P(B|A) \cdot P(A)}{P(B)} =\frac{(0.4 \cdot 0.4 \cdot 0.7 \cdot 0.8) \cdot 0.4}{0.4 \cdot 0.6 \cdot 0.7 \cdot 0.2 + 0.4 \cdot 0.6 \cdot 0.3 \cdot 0.8 + 0.4 \cdot 0.4 \cdot 0.7 \cdot 0.8 + 0.6 \cdot 0.6 \cdot 0.7 \cdot 0.8 } = 0.0937$ It seems that the found probability turned out to be extremely small. Have I correctly defined what is asked to be found in this problem in terms of elementary events? It also confuses me that the sum of the posterior probabilities of not hitting the target of each athlete comes out to be significantly less than one. A posteriori knowledge that only one of the shooters did not hit the target, in my opinion, should greatly increase the probability of not hitting for the ""weakest"" shooters - the first and second. But the two of them together will account for about 40%.","['probability-theory', 'probability']"
4440848,"Derivative(s) of Cantor Measure (Donald L. Cohn ch. 6.2, exercise 6.2.4, related to lemma 6.2.5)","First, context: I'm doing a course on measure/integration theory following the book by Donald L. Cohn. In section 6.2, he defines the (upper and lower) derivates of a finite Borel measure $\mu$ on $\mathbb{R}^d$ as $$ (\overline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\sup\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\} $$ and $$ (\underline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\inf\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\}, $$ where $\mathscr{C}$ is the collection of all cubes in $\mathbb{R}^d$ (i.e. products of closed intervals of common length), $e(C)$ is the length of the edges of the cube $C$ , and $\lambda$ is the Lebesgue measure. In exercise 6.2.4, Cohn asks to confirm that certain assumptions in the following lemma are required: Lemma 6.2.5. Let $\mu$ be a finite Borel measure on $\mathbb{R}^d$ such that $\mu \ll \lambda$ , let $a$ be a positive real number, and let $A$ be a Borel set of $\mathbb{R}^d$ such that $(\underline{D}\mu)(x)\leq a$ holds at each $x\in A$ . Then $\mu(A)\leq a\lambda(A)$ . In particular, the exercise asks to show that the requirement that $\mu\ll\lambda$ is indeed necessary, so there should be some $\mu$ such that $\mu\not\ll\lambda$ , but $\underline{D}\mu$ is bounded on some set $A$ where the statement $\mu(A) \leq \text{constant}\cdot\lambda(A)$ fails. I have an extremely meager attempt at a solution to this: I know that any ""continuous"" looking measure cannot work, since any ""naturally"" constructed one will be absolutely continuous w.r.t. the Lebesgue measure, and any discrete measure will have either zero derivative (where it does not produce a counter example), or it'll have infinite derivative, which will also not work. Therefore, one needs to use some very strange measure, probably singular. Therefore, I suspect that the Cantor measure works, i.e. the measure induced by integrating next to the Cantor function. However, I have no idea how to calculate the upper and lower derivatives of this at the points where it would be non-zero (i.e. points of the Cantor set), nor can I even really show that the derivatives ought to be bounded. So I am stuck. Is this the correct approach, and if so, how should I proceed? If it isn't, what is?","['measure-theory', 'lebesgue-measure', 'analysis', 'singular-measures', 'derivatives']"
4440872,Maximum probability without the Optimal strategy,"I've been stuck on this problem for a good 2 days now, I don't feel any closer to solving it. It reads: ""Pick a natural number between $0$ and $100$ inclusive; denote this number as $\mathit{n}$ . Define a process that terminates at either $0$ or $100+$ : For each iteration of this process, define $\mathit{l}$ to be a positive integer which is at most the current $\mathit{n}$ . Then, with probability $\frac{1}{5}$ , increment $\mathit{n}$ by the chosen $\mathit{l}$ instead. With probability $\frac{4}{5}$ , decrement $\mathit{n}$ by the chosen $\mathit{l}$ . Given that the strategy used to raise $\mathit{n}$ to $100+$ is optimal, find the probability that the process terminates at $100+$ rather than $0$ when you start at $\mathit{n}= 20$ ."" I believe I understand what this problem is asking, but I don't quite know where to begin! More specifically, I'm not sure about two things: What the optimal strategy might be, and how I would calculate probability from there. I would love to share what I've tried, but I've been so lost that I have next to nothing to share. If I let $p(\mathit{n})$ be the probability that I get to $100$ or more from $\mathit{n}$ , I've reasoned that it's related to $\frac{1}{5}p(n+l)+\frac{4}{5}p(n-l)$ . Considering that I don't know how they're related, you could say that I'm pretty stumped!",['probability']
4440902,The solutions of the equation $ \sin{x} + \sin{3x} = \frac{8}{3\sqrt{3}} $ are?,"I tried this: $$ \sin{x} + \sin{3x} = \frac{8}{3\sqrt{3}} $$ $$ 2\sin{2x}\cos{x} = \frac{8}{3\sqrt{3}} $$ $$ 4\sin{x}\cos{x}\cos{x} = \frac{8}{3\sqrt{3}} $$ $$ \sin{x}(1-\sin^2{x}) = \frac{2}{3\sqrt{3}} $$ Here, I tried to set $\sin x = t$ $$ t(1-t^2) = \frac{2}{3\sqrt{3}}, $$ but I don't know to resolve this.",['trigonometry']
