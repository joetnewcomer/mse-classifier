question_id,title,body,tags
4688146,Number of ways to complete a partial Young tableau,"Suppose we have a Young tableau with missing entries. Then there can be many number of ways we can complete the Young Tableau. Is there any specific method to find the number of ways we can complete a partial Young tableau? For example if we have the following partial Young tableau, how many ways we can complete it? Need some hints. Thanks.","['integer-partitions', 'integers', 'combinatorics', 'discrete-mathematics']"
4688195,In the proof of the Gauss-Bonnet Formula ( John Lee's Riemannian Manifold ),"I'm reading the John Lee's Introduction to Riemannian Manifold, p.273, proof of Theorem 9.3. and stuck at understanding some statement : Theorem 9.3 ( The Gauss-Bonnet Formula ). Let $(M, g)$ be an oriented Riemannian 2-manifold. Suppose $\gamma$ is a positively oriented curved polygon in $M$ , and $\Omega$ is its interior. Then $$ \int_{\Omega}KdA + \int_{\gamma}\kappa_Nds +\Sigma_{i=1}^{k} \epsilon_i = 2\pi,$$ where $K$ is the Gaussian curvature of $g$ , $dA$ is ita Riemannian volume form, $\epsilon_1 , \dots , \epsilon_k$ are the exterior angles of $\gamma$ ( c.f. his book p.271 ), and the second integral is taken with respect to arc length ( Problem 2-32 ). In the proof of the theroem, he argues as follows : Proof. Let $(a_0 , \dots ,a_k)$ be an admissible partition of $[a,b]$ , and let $(x,y)$ be oriented smooth coordinates on an open set $U$ containing $\bar{\Omega}$ ( by the definition of $\Omega$ , refer to definition of curved polygon ; his book p.271 ). Let $\theta : [a,b] \to \mathbb{R}$ be a tangent angle function for $\gamma$ ( c.f. his book p.272 ). Using the rotation index theorem ( his book Lemma 9.2. ) and the fundamental theorem of calculus, we can write $$ 2\pi = \theta(b) - \theta(a) = \Sigma_{i=1}^{k}\epsilon_i + \Sigma_{i=1}^{k}\int_{a_{i-1}}^{a_i}\theta'(t)dt$$ Q. And why this is true? Why the term $\Sigma_{i=1}^{k}\epsilon_i$ appears ? An issue that makes me confusing is, $$ \Sigma_{i=1}^{k}\int_{a_{i-1}}^{a_i}\theta'(t)dt = \Sigma_{i=1}^{k} ( \theta(a_i) - \theta(a_{i-1})) = \theta(b) - \theta (a) ?$$ What is a point that I made misunderstood? What is the definition of $\int_{a_{i-1}}^{a_i}\theta'(t)dt$ for each $i$ ? In page p.272, the author defined a tangent angle function for $\gamma$ as a piecewise continuous function $\theta : [a,b] \to \mathbb{R}$ that satisfies $$ T(t) = \cos\theta(t) E_1 |_{\gamma(t)} + \sin\theta(t) E_2|_{\gamma(t)}$$ ( where $T(t)$ is the unit tangent vector field of $\gamma$ , c.f. p.271 ) at each $t$ where $\gamma'$ is continuous, and that is continuous from the right and satisfies (9.1) and (9.2) at vertices. Here the (9.1) and (9.2) in his book are as follows : $$ \theta(a_i) = \lim_{t \nearrow a_i}\theta(t) + \epsilon_i,$$ $$ \theta(b) = \lim_{t \nearrow b}\theta(t) + \epsilon_k $$ Can we try to use these ? Can anyone helps?","['riemannian-geometry', 'differential-geometry']"
4688209,Why can the del operator cross product a triple integral be placed inside the triple integral?,"Consider the (electric) vector field $$\pmb{E}(\pmb{r})=k_e\iiint_V \frac{\rho(\pmb{r_s})}{\lVert \pmb{r}-\pmb{r_s} \lVert^2}\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert}d\tau\tag{1}$$ where $\pmb{r_s}$ is the position vector of a source charge, $\rho(\pmb{r_s})$ is the charge-per-unit-volume at $\pmb{r_s}$ , and $\pmb{r}$ is the fixed position vector of a field point. Suppose we want to calculate $\nabla\times \pmb{E}$ . Here is one way that I saw in a physics book $$\nabla\times\pmb{E}=\iiint_V\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3}\rho(\pmb{r_s})d\tau\tag{2}$$ Here is the step I don't know how to justify $$=k_e \iiint_V \nabla\times \left ( \frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3} \right )\rho(\pmb{r_s})d\tau\tag{3}$$ $$=0\tag{4}$$ I am fine with the final step: it results from the fact that $\nabla\times (r^n\hat{r})=0$ . But why can the cross product pass through the triple integral?","['integration', 'cross-product', 'curl', 'multivariable-calculus', 'vector-analysis']"
4688215,Is FC-center of a topologically finitely generated profinite group closed?,"Let $ G $ be a group. A element $ g\in G $ is an FC-element if it has only finitely many conjugates in $ G $ . The set $ \Delta(G) $ of FC-elements of $ G $ is a characteristic subgroup of $ G $ , and it is called the FC-center of $ G $ . Question: Suppose that $G$ is a topologically finitely generated profinite group. Is $ \Delta(G) $ closed in $G$ ? Note that it's not true without the condition ""topologically finitely generated"", see Page 1281 in Profinite Groups with Restricted Centralizers . Since $G$ is topologically finitely generated,   the topology on $G$ should be determined by the algebraic structure, cf. On finitely generated profinite groups, I: strong completeness and uniform bounds . Hence, it seems that the answer should be Yes in our case. Any references would be appreciated. All I know is the following: Firstly, note that $\Delta(G)$ is just the union of all centralisers $C_G(H)$ of all (abstract) subgroup $H$ of finite index. Here each $H$ must be open since $G$ is topologically finitely generated. Also, one can show that each $C_G(H)$ is closed. For each natural number $n$ , it's well known that the number of open subgroups of $G$ of index $n$ is finite. What's the next step?","['group-theory', 'profinite-groups', 'reference-request']"
4688260,what is the Cardinality of subsets of Z of size 3,"I need to find out the cardinality of the subsets of Z of size 3, |N| = א0
|R| = א, thats how we defined it in class. My idea is to build a ZxZ matrix, we can see that the subsets of size 2 are all under the main diagonal.
now we can build a path and for each step we name the current element we standing on the number of the step, we get a 1:1 and onto function from N --> ZxZ and we can conclude that the cardinality of the subsets of Z of size 2 is א0, now we take all those ZxZ pairs we walked on and place them on another matrix Z x (ZxZ),
and now we can make a new path, but the problem I encounter is that there are many many repetitions and elements such as (-1,-1,0) that cant be a subset. can i ignore them and say : we number only valid and not seen already subsets?
or is this approach not logical in the first place? Thanks in advance for the help","['elementary-set-theory', 'cardinals', 'combinatorics']"
4688264,Set such that $x \in A \implies x\in B$,"I'm a littlle confused right now, I'm not sure if this is some sort of trick question, but basically I am given 3 sets: $A = \{1, 2, 3, 4\}$ and $B =\{3, 4, 5, 6\}$ and $C = \{x \in  \mathbb{N} : x \in A \implies x\in B\}$ .
I'm supposed to list the elements of $C$ . Well at first I thought the set $\{3, 4\}$ , but after analyzing it for a little longer I realized that it can also be the set: $\{0, 3, 4, 5, 6, 7,...\}$ , so which one is the right answer and why? Thanks!",['elementary-set-theory']
4688274,A quite tricky problem about mean value theorem.,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be twice differentiable. Suppose $f'(0)=f'(1)=2$ and $\forall x \in [0,1]$ , $|f''(x)|\leq 4$ . Prove that $|f(1)-f(0)|\leq3$ . I know how to prove that when the bound is 4. It simply needs taylor expansion that $f(1)=f(0)+f'(0)+\frac{f''(c)}{2}$ $f(1)-f(0)\leq2+\frac{f''(c)}{2}$ And the proof can be obtained immediately.
But when the bound is 3 it really confuses me. Thank you for any help or advice.","['analysis', 'real-analysis', 'calculus', 'taylor-expansion', 'mean-value-theorem']"
4688288,Marginal density function involving exponential and uniform distributions,"for my probability homework I was given the following question. Let $Y \sim$ Exponential(2) and let $X \sim$ Uniform $(0, e^{Y})$ . This means that $(X, Y)$ is a bivariate continuous random variabele with joint density function $f_{X, Y}: \mathbb{R}^{2} \rightarrow \mathbb{R}$ given by: \begin{align*}
f_{X, Y}(x, y) =
\begin{cases}
2e^{-3y}, & 0 < x < e^{y}, y > 0; \\
0, & \text{otherwise}. 
\end{cases}
\end{align*} For the third question I had to determine the marginal density function $f_{X}(x)$ . I know that I have to integrate the above function over the support of $Y$ , however, they gave the hint that I have to distinguish two different cases, which makes me unsure about what to do. Originally I would just integrate from 0 to $\infty$ , but now I am not sure. I tried to make a plot of the bounds and thought I would have to approach two different cases involving $x$ between 0 and 1, and $x$ > 1, but I am not sure how I would approach this in the integral. I would greatly appreciate if anybody could help!.","['probability-distributions', 'probability']"
4688300,$f(x_0) + f''(x_0) = 2f'(x_0)$,"f $\in$ C $^2$ [a,b], and f has at least three distinct roots in [a,b]. I'm required to show that there's a point x $_0$ in [a,b], such that f(x $_0$ ) + f''(x $_0$ ) = 2f'(x $_0$ ). I concluded that there are points y $_1$ and y $_2$ such that f'(y $_1$ ) and f'(y $_2$ ) are zero.
and a z $_1$ such that f''(z $_1$ ) = 0. I used the taylor formula at these points, I also considered to define a function g(x) = f(x) - f'(x) so that I have to show that g(x $_0$ ) = g'(x $_0$ ), but it didn't help. I also tried to find some counter examples, but didn't succeed. What should I do?","['mean-value-theorem', 'rolles-theorem', 'taylor-expansion', 'analysis']"
4688316,Calculate the integral over the half ellipse,Calculate the integral $\displaystyle \iint_E x^2 dA$ where E is the half ellipse given by the inequalities $x^2 + 2y^2 \le 2$ and $y \ge 0$ . This is what I did: $$\begin {align}\iint_E x^2 dA &=\iint r^3 \cos^2(\theta) dr d\theta\\ &= \int_1^{\sqrt2}r^3 \times \int_0^{\pi} \frac12(\cos(2\theta)+1) dr d \theta\\ &= \left[\frac{r^4}4\right]_1^{\sqrt2}\left[\frac{\sin(2\theta)}2+ \frac{\theta}2\right]_0^{\pi}\\ &= \left(0+\frac{\pi}2-0\right)\left (\frac44 - \frac14\right)\\ &= \frac{3\pi}4\end{align}$$ This is wrong and the correct answer is $\dfrac{\sqrt{2\pi}}4$,"['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'partial-derivative']"
4688348,When is true that $\int_0^1 \int_0^1 f(xy)\ dx\ dy=\ -\int_0^1 \log(x) f(x)\ dx$,"Saw in a post this identity $$\int_0^1 \int_0^1 f(xy)\ dx\ dy=\ -\int_0^1 \log(x) f(x)\ dx$$ then the author says ""notice that $f$ is under some restrictions"", and concludes the post, without saying what restrictions! I'd like to know what these restrictions are, since in the proof he provided I couldn't find any. Here is the proof: Let $xy=t$ , so $dx=\frac{dt}{y}$ , and the integral becomes: $$\int_0^1 \int_0^y f(t)\ \frac{dt}{y}\ dy$$ = $$\int_0^1 \frac{1}{y} \int_0^y f(t)\ dt\ dy$$ now integrate by parts with $$u=\int_0^yf(t)\ dt\hspace{2cm} dv=\frac{1}{y}\ dy$$ and get $$\left[\log(y)\int_0^yf(t)\ dt\right]_{y=0}^{y=1} -\int_0^1 \log(y) f(y)\ dy=$$ $$=\ 0\ -\ 0\ -\int_0^1 \log(y) f(y)\ dy=$$ $$=-\int_0^1 \log(y) f(y)\ dy$$ and the proof is concluded. Please help with the restrictions on $f$ .","['integration', 'calculus', 'definite-integrals']"
4688370,"IF $f:\mathbb N\to\mathbb N$ and for any natural number $f(n+1)>f(n)$ and $f(f(f(n)))=4n$ ,find $f(2023)$","Function $f:\mathbb N\to\mathbb N$ is defined.If for any natural number $f(n+1)>f(n)$ and $f(f(f(n)))=4n$ , $f(2023)=?$ I attempted to solve it as shown below: $f(f(f(1)))=4$ and let's say $f(f(1))=x$ then $f(x)=4$ and as $f:\mathbb N\to\mathbb N$ it must be $x\leq4$ .Then I tried out these $4$ possible ways and concluded that only $x=3$ works.Because if so $f(3)=4$ , $f(f(1))=3$ . From these I found $f(1)=2$ , $f(2)=3$ . And after that I wanted to find other terms but it doesn't give the answer.Is my attempt wrong?  If so how to solve this? NOTE: This question is adapted from a competition that took place in january $2023$ .",['functions']
4688386,"Are open sets and closed sets fundamentally different, considering only theorems that apply to every metric space?","I'm going through a real analysis course, and i noticed that every theorem we've seen about open sets can be transformed into a theorem about closed sets and vice versa. Here's an example: Let $(X, d)$ be a metric space and consider $A \subset X$ , $B \subset X$ .
Then: $$
 Int(A) \cap Int(B) = Int(A \cap B)
$$ $$
 Int(A) \cup Int(B) \subset Int(A \cup B)
$$ Now, if you consider the complement of both relations: $$
 Int(A) \cap Int(B) = Int(A \cap B)
$$ $$
\Rightarrow (Int(A) \cap Int(B))^c = (Int(A \cap B))^c
$$ $$
\Rightarrow Cl(A^c) \cup Cl(B^c) = Cl(A^c \cup B^c)
$$ $$
Int(A) \cup Int(B) \subset Int(A \cup B)
$$ $$
\Rightarrow (Int(A) \cup Int(B))^c \supset (Int(A \cup B))^c
$$ $$
\Rightarrow Cl(A^c) \cap Cl(B^c) \supset Cl(A^c \cap B^c)
$$ You end up with a very similar theorem but with closure of sets: $$
 Cl(A) \cup Cl(B) = Cl(A \cup B)
$$ $$
 Cl(A \cap B) \subset Cl(A) \cap Cl(B)
$$ Another example is that $Int(A)$ is the maximum open set contained on $A$ , while $Cl(A)$ is the minimum closed set that contains $A$ . One last example is continuity of metric spaces. A function from a metric space $(X,d)$ to another metric space $(Y,\lambda)$ is continuous if and only if the preimage of every open set on Y is open on X. The same function is also continuous if and only if the preimage of every closed set on Y is closed on X. This duality, at least with the theorems i've seen, seems to tell that open sets and closed sets really behave the same way. I know that, if you consider continuous functions on real numbers, each function will have a maximum and a minimum on a closed interval, but this doesn't always apply on open intervals. However, the real numbers along with absolute value is only one metric space, in which axioms of real numbers apply. This isn't true for every metric space. So, i want to consider only definitions and theorems that apply to every metric space. This is (informally speaking because i don't know about mathematical logic, if i'm wrong correct me or ignore this), i want to consider only the axiomatic system of metric spaces and it's theorems, not theorems that only some models verify along with other axioms. Since you can't use real number axioms on every metric space, the extreme value theorem isn't a counter-example to my question, it's not intrinsic to metric spaces in general. Finally, the question is: Considering only statements that are true for every metric space, do open and closed sets actually behave the same way?","['general-topology', 'metric-spaces', 'real-analysis']"
4688429,"If $\sum_ix^i\partial_if=0$, then $f$ is constant?","Let $f$ be a real-valued function defined on a neighborhood $D$ of $0\in\mathbb R^n$ - is it true that $\sum_ix^i\partial_if=0$ implies that $f$ is constant? Intuitively, I'd say yes: Consider the radial vector field $R(x):=x$ , then $\sum_i x^i\partial_if=\mathrm df(R)$ and we get that $f$ is constant on each ray emanating from the origin. Because of continuity, $f$ should be constant everywhere. Is my reasoning correct? What properties does $D$ need to have such that everything works out? (My guess would be that $D$ needs to be star-shaped w.r.t. the origin.) Motivation: As far as I understand, this is used in the proof of Lemma $4.13$ of Heat Kernels and Dirac Operators .","['partial-derivative', 'derivatives', 'differential']"
4688432,Can I bound the spectral radius (or maximum eigenvalue) of a positive definite matrix?,"This is a problem motivated by spatial autoregressive models. Assume $A$ is an $n\times n$ non-negative matrix with the sum of each row equals to $1$ . Let $I$ be an $n\times n$ identity matrix and $c\in\mathbb{R}$ . Define \begin{align*}
A_1 &= (I-cA)^{-1}(I-cA^\top)^{-1},\\
A_2 &= (I-cA)^{-1}(I-cA^\top)^{-1}(A^\top+A-2cA^\top A)(I-cA)^{-1}(I-cA^\top)^{-1}.
\end{align*} When $n$ goes to infinity, can we upper bound the spectral radius $\rho(A_1)$ (or the largest eigenvalue $\lambda_{\max}(A_1)$ ) and $\rho(A_2)$ by adding some appropriate assumptions (maybe on $A+A^\top$ or $A^\top A$ )? This is my consideration. Assume $\lambda_i$ s are the eigenvalue of $A+A^\top - cA^\top A$ for $1\leq i\leq n$ . Then $A_1$ 's eigenvalue should be $(1-c\lambda_i)^{-1}$ . So I guess we should avoid every $\lambda_i$ to be close to $c^{-1}$ , but I have no idea how to describe it mathematically. For $A_2$ , we want to show $\lambda_{\max}(U^{-1}VU^{-1})\leq K$ for some K>0, where $U = (I-cA^\top)(I-cA)$ and $V = A^\top+A-2cA^\top A$ . I believe It suffices to show that \begin{align*}
\quad & \lambda_{\max}(U^{-1}VU^{-1})\leq K\\
\Leftrightarrow & \frac{y^\top U^{-1}VU^{-1} y}{y^\top y} \leq K \quad (\text{for all }y\neq 0)\\
\Leftrightarrow & x^\top V x\leq K x^\top U^2 x\quad (x = U^{-1}y)\\
\Leftrightarrow & V\leq KU^2
\end{align*} in Lowner order. I am not sure if it is the right way.","['eigenvalues-eigenvectors', 'matrices', 'spectral-radius', 'linear-algebra', 'upper-lower-bounds']"
4688437,Intersection of intersection of families of sets is intersection of union of families of sets,"I am going through Velleman's ""How to prove it"" and I am stuck on exercise 13b of section 2.3, where I have to show: $$(\cap \mathcal{F})\cap (\cap \mathcal{G})=\cap(\mathcal{F}\cup \mathcal{G})$$ I don't see how the union comes into place here. Here's what I have got: $$x\in (\cap \mathcal{F})\cap (\cap \mathcal{G})$$ $$x\in (\cap \mathcal{F})\land x\in(\cap \mathcal{G})$$ $$(\forall i \in I (x\in A_i)) \land (\forall i \in I (x\in B_i))$$ $$\forall i \in I ((x\in A_i) \land (x\in B_i))$$ $$x\in \cap (A_i\cap B_i)$$ Something must be wrong here, somehow the i dissappeared. Hope someone can help.","['elementary-set-theory', 'logic']"
4688492,Let A and B are square matrices of order 2 with real elements such that AB = $A^2B^2 - (AB)^2$,"A and B are 2 x 2 matrices with real elements and AB = $A^2B^2 - (AB)^2$ and |B| = 3, Find the value of |A+2B| - |B+2A|. Using the given relation, i managed to get |A| = 0 by the following; AB = $A^2B^2 - (AB)^2$ A = $A^2B - ABA$ A(I - AB - BA) = 0 [ taking determinant on both sides ] |A||I - AB - BA| = 0 It's here I was stuck on and later on seeing the solution it said this; "" For 2x2 matrices A and B, the following relationship holds true for some scalar x,
|A+xB| = |A| + $x^2|B|$ + x[Tr(A)Tr(B)-Tr(AB)] "" Could anyone explain how this relationship was derived or any alternates to solve this problem.","['matrices', 'determinant', 'linear-algebra']"
4688537,Show that $(1+x)^{1+x} \ge 1 + x + x^2 + \frac{x^3}{2}$ for $x \ge -1$,"Show that for every $x\ge -1$ we have the inequality $$(1+x)^{1+x} \ge 1 + x + x^2  + \frac{x^3}{2}$$ with equality only if $x = 0$ . Notes: One of Bernoulli's inequalities  : $(1+x)^a \ge 1 + a x$ if $x\ge -1$ and $a\ge 1$ , so $(1+x)^{1+x}\ge 1 + x + x^2$ for $x \ge 0$ . We can further improve it as follows $(1+x)^{1+x} \ge 1 + x + x^2 + \frac{x^3}{2} + \frac{x^4}{3} + \frac{x^5}{12} $ for $x \ge -1$ . One could look at the Taylor expansion of $(1+x)^{1+x}= \exp( (1+x) \log (1+x))$ Any feedback would be appreciated!","['calculus', 'exponential-function', 'taylor-expansion', 'inequality']"
4688562,$A$ is dense in $A''$ in the strong topology (Murphy's book),"In Lemma 4.1.4. Murphy is doing something strange that I don't seem to understand. for each $x\in H$ we find $v_n(x)$ such that it converges to $u(x)$ . we wish to prove that is holds for EVERY $x$ . Murphy goes on to make this construction on $H^n$ so that in the end he can take $W$ (neighbourhood of $u$ in the strong topology) and show that there there is at least some $v_n(x)$ so it is indeed the sot closure.
What I don't understand is why he uses this $x_1,...x_n$ what is he trying to say? surely not that $W$ is a sot neighbourhood of $u$ if $|(t-u)x_j|<\epsilon$ for finitely many $x_j$ ?! Can someone illuminate this argument, please?","['von-neumann-algebras', 'c-star-algebras', 'operator-theory', 'functional-analysis']"
4688644,"What conditions ensure that $\lim_{x \to 0+} \log(x) \int_0^x f(t) \, dt = 0$?","In a recent post , a question arose as to the requirements on $f:[0,1] \to \mathbb{R}$ in order to ensure that $$\tag{1}\lim_{x \to 0+}\log(x) \int_0^x f(t) \, dt = 0$$ I showed that Riemann integrability of $f$ is a sufficient condition. Assuming $f$ is bounded (and all Riemann integrable functions must be bounded) we have $f(t) \leqslant M$ for some $M \geqslant 0$ and, hence, $$\tag{2}0 \leqslant \left|\log(x) \int_0^x f(t) \, dt \right|= -\log(x)\left|\int_0^x f(t) \, dt\right|\leqslant -\log(x)\int_0^x|f(t)| \, dt\leqslant -Mx\log(x)\underset{x \to 0+}\longrightarrow 0$$ My questions is what is the most general class of functions for which (1) holds? It clearly holds for some unbounded functions like $x \mapsto x^{-1/2}$ and I believe it is even true for $x \mapsto x^{-1}\sin x^{-1}$ which is not Lebesgue integrable. Furthermore, since $x \log(x) \to 0$ as $x \to 0+$ , it really is a question of the convergence or boundedness of $\frac{1}{x}\int_0^x f(t) \, dt$ and perhaps an application of the Lebesgue differentiation theorem.","['integration', 'riemann-integration', 'lebesgue-integral', 'real-analysis']"
4688654,"Can the ""structural"" equivalence between partitions and equivalence relations be expressed as an isomorphism of categories?","I have not taken a formal course in category theory, but as part of my abstract algebra class, I was given a brief introduction to subject of categories, so that I better understand and appreciate the recurring themes within abstract algebra (e.g. sub-objects, homomorphisms, isomorphisms, quotient-objects, etc.). As part of my abstract algebra course, I learned about several instances of ""isomorphism of categories"", and came to appreciate them as essentially expressing the ""structural"" equivalence between two types of objects. For instance, I learned that the fact that abelian groups and $Z$ -modules (where $Z$ is the ring of integers) have the same ""structural"" information can be expressed as an isomorphism of categories (i.e. between the category of abelian groups Ab and the category of $Z$ -modules $Z$ -Mod). But I have also noticed similar types of ""structural"" equivalences occurring outside of abstract algebra, even in as simple of a subject as elementary set theory: From elementary set theory, I know that equivalence relations and partitions are essentially the same, in that the information present within each equivalence relation is just enough to uniquely construct the corresponding partition, and vice versa. This seems very similar to the instances of ""isomorphism of categories"" I encountered within abstract algebra. Thus, I am wondering whether this is indeed another instance of an isomorphism of categories (between a ""category"" of equivalence relations and a ""category"" of partitions)?","['elementary-set-theory', 'abstract-algebra', 'category-theory']"
4688682,Find the maximum number of elements in the set $M$ such that no two elements have a difference of $5$ or $8$,"$\textbf{Question :}$ Let $M$ be a set of positive integers less than or equal to $2000$ with the property that no two elements can have a difference of either $5$ or $8$ then what is the maximum number of elements in $M$ ? $\textbf{My Attempt :}$ Choosing the the numbers as following $(1,2,3,4,5); (14,15,16,17,18); ...; (1990,1991,1992,1993,1994)$ we get a total of $770$ numbers but this is way off the given answer which is $924$ .","['elementary-set-theory', 'elementary-number-theory', 'combinatorics', 'extremal-combinatorics']"
4688719,"Evaluate $\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$.","Evaluate $\displaystyle\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$ . Which of the following are true? (a) If $b<a$ , $x\to 0$ as $n\to ∞$ (b) If $a < b$ , $x\to 0$ as $n\to ∞$ (c) If $a = b$ and $c > 0, x \to ∞$ as $n \to ∞$ (d) If $a = b$ and $c < 0, x\to ∞$ as $n \to ∞$ I was stuck for a long time, with this problem. I searched this site, to find, whether this was previously asked or not (as I saw, duplicates are closed according to the site policy). I found a question asked some years ago. Here's the link: How do I find $\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}n^c\right ]$ for real $a,b,c$ and $n\geq 1$? I looked at all the answers. But, somehow I feel some informations are very much lacking in all the answer and the question as well. I was particularly interested in the answer of a user named Qwerty. I quote the answer hereby: Standard limit rule says $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ So result is $$\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}$$ $b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall $ finite $c$ $b=a\implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty\ \ $ if $c\gt 0$ , $1$ if $c=0$ and $0$ if $c\lt 0$ $b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty$ This seems to be the most appealing to me. But I have a few questions (or to be definite, I am having a hard time with $3$ questions, regarding this question and the solution.): The user says : Standard limit rule says $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ So result is $$\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}.$$ Now, the thing is if this was a standard fact, it was completely unknown to me. I was searching for a proof of this result. To my surprise, in that link (above) , a user named, avz2611 posts a proof of this apparent new result. I quote his proof as well: $$=\frac{n^c}{(1+\frac{2}{n^a})^{n^b}}$$ now let $$l=(1+\frac{2}{n^a})^{n^b}$$ $$\ln l={n^b}\ln({1+\frac{2}{n^a}})=\frac{2n^b}{n^a}\frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=\frac{2n^b}{n^a}.1$$ $$\therefore l=e^ {\frac{2n^b}{n^a}}$$ therefore original limit is $$=\frac{n^c}{e^{2n^{b-a}}}$$ now exponential function raises a lot faster than a finite polynomial thus limit would $\infty$ if $b<=a$ and $0$ if $b>a$ assuming $c$ is positive. I did a little modification as I used italics to highlight the proof of the above identity. Nevertheless, this proof, looks good only if, we assume $a>0$ in the 4th step, which appears to be using the formula $\lim_{x\to 0}\frac{ln(1+x)}{x}=1,$ and if, say, $a<0$ , then in the 4th step, $$\frac{\ln({1+\frac{2}{n^a}})}{2/n^{a}}=1$$ , is not true, as $\frac{2}{n^a}$ does not tend to (or approach) $0$ if, $a$ is negative, it rather tends to infinity. So, the assumption that should have been taken, is that $a>0$ in order to use the identity, $$\lim_{n\to \infty}\left [\left (1+\frac{2}{n^a}\right )^{-n^b}\right]=e^{-2n^{b-a}}$$ as in Qwerty's answer. It seems, each and every answer is making use of this identity. But I am not quite convinced about it's usage, unless it is done under the assumption, $a>0.$ Am I missing something? Further, is this identity a very standard fact, that can be used as a theorem ? My next question, is, how does , say, for example, in Qwerty's answer, how was it concluded, "" $b>a\implies\lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}}\to 0 \ \forall $ finite $c$ "" ? I got convinced by this statement, by intuitively guessing maybe, that, $e^{2n^{b-a}}$ increases more rapidly than, $n^c$ , without no reasoning so as to back up this intution . My question, is, was this fact stated solely based on intuition ? The next question, also is similar to the previous one. In the quoted answer, it is mentioned, "" $b<a \implies \lim\limits_{n\to \infty}{n^c\over e^{2n^{b-a}}} \to \infty$ "". Again, was this fact, solely stated based upon intuition and appeals the reader to just convince themselves, that this indeed holds, by using intuitions ? These are the $3$ questions, that I am having a hard time dealing with. I am looking for an elementary answer, to these questions, because, I don't seem to have an adequate knowledge in real analysis as of now. So, to be precise, an answer to these questions of mine, that uses only elementary calculus is the most optimal or better say, acceptable judging this scenario. To be honest, I dont know, whether these sort of posts are allowed here or not. But I found that majority of users advocate for posts asking for clarifications for a particular solution, to be acceptable.","['limits', 'calculus', 'proof-explanation']"
4688720,Please tell me good introductory Lebesgue integral books which include a rigorous proof of the change of variables theorem.,"I am reading ""Measure, Integration & Real Analysis"" by Sheldon Axler. I have read up to p. 59 of this book. I love this book and I guess this book is the best introductory book about Lebesgue integral in the world. On the page which is nearly equal to $100\sqrt{2}$ (I guess it means p. 141), the author wrote the following: To evaluate this integral, switch to the usual polar coordinates that you learned about in calculus ( $d\lambda_2=rdrd\theta$ ), getting $\cdots$ So, I guess the author didn't write the change of variables theorem in this book. Please tell me good introductory Lebesgue integral books which include a rigorous proof of the change of variables theorem.","['measure-theory', 'lebesgue-integral', 'book-recommendation', 'real-analysis', 'change-of-variable']"
4688731,A complex analysis proof of the extremal case of Bernstein's inequality?,"Bernstein's inequality states: Given any degree- $n$ polynomial $P(z)$ with complex coefficients, we have \begin{align}
\sup_{|z| \leq 1}  |P'(z)|  \leq  n\,\sup_{|z| \leq 1}  |P(z)|.  \label{Binequality}
\end{align} Moreover, the equality holds if and only if $P(z)=\lambda z^n$ for some nonzero constant $\lambda$ . There is a simple proof of the inequality by Rouch'e and Gauss-Lucas theorem, see here . My understanding is that the above proof is not sufficient to conclude the equality case. Instead, there is a proof which makes use of the Fejer kernel of complex Fourier series to relate $P(z)$ and $P'(z)$ , it proves both the inequality and the extremal case, see Theorem 14.1.1 on p.508 of the following book. Rahman, Q. I.; Schmeisser, G.. Analytic theory of polynomials. London Mathematical Society Monographs. New Series, 26. The Clarendon Press, Oxford University Press, 2002. Question: Is there a complex analysis proof of the extremal case? I am new to Bernstein's inequality and must miss some references on this famous inequality. I would appreciate anyone who suggests a (complex analysis) proof or some references.","['complex-analysis', 'polynomials', 'real-analysis']"
4688737,Prove that the any Riemann surface of genus zero is isomorphic to the Riemann sphere,"Let $X$ be a compact Riemann surface, and suppose that there exists a meromorphic function $f$ on $X$ with one simple pole and no other poles. Show that $f$ is an isomorphism between $X$ and the Riemann sphere. Using this, prove that the any Riemann surface of genus zero is isomorphic to the Riemann sphere. Let $(f) = (P) - (Q)$ . Since the degree of $f$ is zero, it must be that $P \neq Q$ . I am not sure how I can prove that $f$ is surjective, other than the general instruction that I can use the Riemann-Roch theorem. I know that the sum of the divisor of any meromorphic function is equal to zero, and that for any divisor $K$ of $X$ , the degree of $K$ is equal to $2g - 2$ where $g$ is the genus. EDIT: Thanks to @DJ Dowd, I proved the second part: let $Q$ be any point in $X$ . Then $\dim L((Q)) \geq 2$ by Riemann Roch, so that there exists a nonconstant function $f \in L((Q))$ and it can be shown that $f$ can have only one simple zero and no other poles by using the fact that the degree of $f$ is zero. It remains to prove the first part. Because the function $f - c$ has a simple zero at $c = f(R)$ for arbitrary $R \in X$ , we have injectivity. Thus it remains to prove surjectivity.","['complex-analysis', 'riemann-surfaces', 'algebraic-geometry', 'meromorphic-functions']"
4688789,Find the local extemum of $x^{2}-2y^{2} - \ln (x + y)$,"I have to find the local extrmum of the function $$f(x, y) = x^{2}-2y^{2} - \ln (x + y).$$ I found the first derivatives of $$f'_x = 2x-\frac{1}{x+y}$$ $$f'_y = -4y-\frac{1}{x+y}$$ I made the system $f'_x=0$ and $f'_y = 0$ . I found points $A (-1, \frac{1}{2})$ and $B (1, -\frac{1}{2}).$ After that, I found second derivatives $f''_x = 2 + \frac{1}{(x+y)^{2}}$ , $f''_y = - 4 + \frac{1}{(x+y)^{2}}, $ , $f''_{xy} = \frac{1}{(x+y)^{2}}$ , $f''_{yx} = \frac{1}{(x+y)^{2}}$ .
Then I made The Hessian matrix $\begin{bmatrix}
2 + \frac{1}{(x+y)^{2}} & \frac{1}{(x+y)^{2}}\\ 
\frac{1}{(x+y)^{2}} & -4 + \frac{1}{(x+y)^{2}}
\end{bmatrix}$ . The Problem is that the determinant is $\frac{-8x^{4}-32x^{3}y-48x^{2}y^{2}-2x^2-32xy^3-4xy-8y^4-2y^2}{(x+y)^4}$ . How I can find the local extremum then?","['hessian-matrix', 'functions', 'derivatives']"
4688809,"Finding the minimum of $\int 1/|Du \cdot v|^2$ over $[0,1]^2$ for some $v \in \mathbb{R}^2$ when $\int Du = I$","I've run into the question of finding the minimum (or infimum) of this integral over $C^2([0,1]^2)$ , subject to the conditions that... $$\det (Du) \neq 0 \,\,\text{everywhere} \qquad \text{and} \qquad \int_{[0,1]^2} Du(x)dx = I_{2\times2} $$ Let $u: \mathbb{R}^2 \to \mathbb{R}^2$ with $u\in C^2([0,1]^2)$ . Let $v \in \mathbb{R}^2\setminus \{0\}$ . Find the infimum of $J$ with respect to $u$ . $$J(u) = \int_{[0,1]^2} \frac{1}{|Du(x) \cdot v|^2}dx$$ The idea is that $u$ is some deformation of the unit square. My conjecture is that the minimisers $u^*$ will be linear, and so I attempted to try and use some sort of convexity of $f(y) = \frac{1}{|y|^2}$ , but I struggled to get anywhere with that strategy. Is there a standard method for problems like this? Examples of similar solved problems would be helpful too if you have them. Context: I'm using a continuum approximation of a system of point particles with some interaction and this is the integral which describes their total interaction energies up to some asymptotically small error term.","['integration', 'multivariable-calculus', 'calculus-of-variations']"
4688836,Coercivity and spectral gap: understanding the equivalence,"I am referring to this paper , p. 21. First, there is the following definition of coercivity : Let $L$ be an unbounded operator on a Hilbert space $\mathcal{H}$ with kernel $\mathcal{K}$ and let $\tilde{\mathcal{H}}$ be another Hilbert space continuously and densely embedded in $\mathcal{K}^\perp$ , endowed with a scalar product $\langle\cdot,\cdot\rangle_{\tilde{\mathcal{H}}}$ and a Hilbertian norm $\Vert\cdot\Vert_{\tilde{\mathcal{H}}}$ . The operator $L$ is said to be $\lambda$ -coercive on $\tilde{\mathcal{H}}$ if $$
\forall h\in\mathcal{K}^\perp\cap D(L),\quad\Re\langle Lh,h\rangle_{\tilde{\mathcal{H}}}\geq\lambda\Vert h\Vert^2_{\tilde{\mathcal{H}}},
$$ where $\Re$ stands for the real part. The operator $L$ is said to be coercive on $\tilde{\mathcal{H}}$ if it is $\lambda$ -coercive on $\tilde{\mathcal{H}}$ for some $\lambda>0$ . Afterwards, it is said that [..] the most standard situation is when $\tilde{\mathcal{H}}=\mathcal{K}^\perp\simeq \mathcal{H}/\mathcal{K}$ . Moreover, in this particular case, it is said that [..] it is equivalent to say that $L$ is coercive on $\mathcal{K}^\perp$ , or that the symmetric part of $L$ admits a spectral gap . Could you please explain to me why this equivalence holds? As far as I understand, spectral gap means that there exists some $c>0$ such that $$
\sigma(L)\subset\{0\}\cup [c,\infty)
$$ However, from this, I can neither conclude that $L$ is coercive on $\mathcal{K}^\perp$ (nor vice versa). My ""feeling"" is that this has something to do with the relationship between the spectrum of $L$ and its numerical range (but I only know this for self-adjoint bounded operators and this seems to be another situation here).","['coercive', 'kinematics', 'operator-theory', 'functional-analysis', 'spectral-theory']"
4688848,Limit $\lim _{t\to \infty} \int_t^\infty t \frac {\sin^2 (u)}{u^2} du$,"Does the limit $\displaystyle{\lim _{t \to \infty}  t \cdot \int_t^\infty \frac {\sin^2 (u)}{u^2} \mathrm{d}u}$ exist? Intuitively, since $\sin(t) > \frac 1 2$ holds for at least half of a period, I would assume that the integral $\int_t^\infty \frac {\sin^2 (u)}{u^2} \mathrm{d}u$ behaves approximately as $\int_t^\infty \frac {1}{u^2} \mathrm{d}u$ . But the latter is $\frac 1 t$ , so it seems that the limit diverges. Is it true?","['limits', 'improper-integrals']"
4688853,Numerical derivative of a matrix with respect to a vector,"I have an expression $$ \mathbf{A}\mathbf{s}
$$ where $\mathbf{A}$ is an $n \times n$ matrix and $\mathbf{s}$ is a $n \times 1$ vector. The matrix $\mathbf{A}$ is itself a function of $\mathbf{s}$ $$ \mathbf{A} = \mathbf{f}(\mathbf{s})$$ I am wondering how I firstly, compute the derivative of this expression and secondly, numerically estimate the derivative at some vector $\mathbf{s}_0$ . My attempt has been to apply the product rule in some way $$ \frac{d}{d\mathbf{s}} \mathbf{f}(\mathbf{s})\mathbf{s} = \mathbf{f}(\mathbf{s}) + \mathbf{f}'(\mathbf{s})\mathbf{s} $$ noting that $\mathbf{f}'(\mathbf{s})$ is a $(n \times n) \times n$ matrix so that the derivative above is a matrix of dimension $n \times n$ if we consider the term $\mathbf{f}'(\mathbf{s})\mathbf{s}$ as an $n \times n$ matrix expressed with its columns placed below each other. I am not completely sure whether my approach is correct and additionally, how to proceed from here in terms of numerically evaluating this derivative at a particular vector. I plan to use Matlab to compute a numerical derivative. Any assistance or input would be greatly appreciated!","['matrices', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'numerical-methods']"
4688862,Show that $AC=AD+BD$ in the triangle below,"The triangle $ABC$ in the figure below is isosceles with base $AC$ , knowing that $AD$ is bisector of $\angle BAC$ and $\angle ABC=100^o$ , prove that $AC=AD+BD$ . $\frac{AB}{BD}=\frac{AC}{CD}$ $\frac{BD}{DE}=\frac{AB}{AE}$ $\frac{AB}{AF}=\frac{BC}{CF}$ $AD =\sqrt{AB.AC-BD.CD}$","['euclidean-geometry', 'geometry', 'plane-geometry']"
4688870,Hypergeometric distribution with multiple observations,"This is my maiden question on Math stack exchange so do forgive me if I am not following any community norms or not presenting my questions as clear as it should be. I am trying to find probabilities with hypergeometric distribution. I will be working with multiple random and independent observations being made at the same time and I am unclear of how to handle it in the manner that is presented to me. The usual probability mass function ("" PMF "") is this: $$P(x)=\frac{\binom{k}{x}\cdot\binom{N-k}{n-x}}{\binom{N}{n}}$$ For the purposes of this post, I shall provide a hypothetical question here to illustrate the issue, Suppose a certain disease affects 400 ( $k$ ) people in a population of 20,000 people ( $N$ ). If a random sample of 1,000 people ( $n$ ) is taken and tested without replacement what is the probability that exactly 20 ( $x$ ) of them have the disease if people were taken and tested, one at a time? 10 at a time? 7 at a time? For scenario 1 If I am making one observation at a time, I can proceed with using the PMF as per normal such that, $$P(x=20|N=20,000, n=1,000, k=400)$$ $$P(x=20)=\frac{\binom{400}{20}\cdot\binom{20,000-400}{1,000-20}}{\binom{20,000}{1,000}}$$ For scenario 2 If I am making 10 observations at a time, my understanding is that, all I would need to do is to divide all the variables $(x, N, n, k)$ by 10. This means that $P(x=20|N=20,000, n=1,000, k=400)$ adjusted for 10 events being observed at a time would give us, $$P(x=2|N=2,000, n=100, k=40)$$ $$P(x=2)=\frac{\binom{40}{2}\cdot\binom{2,000-40}{100-2}}{\binom{2,000}{100}}$$ For scenario 3 Now, this is where things get hairy. If I am making 7 observations at a time, I cannot simply divide all the variables by 7 like the way I did in scenario 2. I will have remainders for everything, especially for $n$ where I will have a remainder of 6 and I have no idea what to do with it. In summary I am asking whether, dividing all variables $(x, N, n, k)$ by the number of observations being made at the same time a mathematically sound way to handle multiple observations in hypergeometric distributions? (aka is my method in scenario 2 correct) if so, how do I go about handling remainders in $n$ ? how would you handle scenario 3? I have attempted to work with ChatGPT on this issue. However, it is, to say the least, rather unreliable at math. Thus, it took me some time to discover this community and to ask this question.","['statistics', 'probability-distributions', 'probability']"
4688891,A direction that is far from any n directions of the vertices of a hypercube,"$C_n \stackrel{\text{def}}{=} \left\{\pm \frac{1}{\sqrt{n}} \right\}^n$ Question : Is the following true? For all $K>0$ , $n$ large enough and $r_1,..,r_n \in C_n$ , there exists a unit vector $c \in \mathbb{R}^n$ such that $|c\cdot r_i| \leq \frac{1}{K\sqrt{n}}$ for all $i \in [n]$ . Discussion : Since any $(n-1)$ -subset of $\mathbb{R}^n$ is not a spanning set, it can be shown that: For any $r_1,..,r_{n-1} \in C_n$ , there exists a unit vector $c \in \mathbb{R}^n$ such that $|c\cdot r_i| = 0 \left( \text{hence }\leq \frac{1}{K\sqrt{n}} \right)$ for all $i \in [n-1]$ . For the $n$ -subsets of $C_n$ that I checked manually (and with Mathematica), the conjecture is true.","['combinatorial-geometry', 'combinatorics', 'geometry']"
4689009,Find a derivation for$\{\varphi \Rightarrow (\psi \land \phi)\} \vdash \psi \to (\varphi \Rightarrow \phi)$,"I was given the following problem: Find a derivation for $\{\varphi \Rightarrow (\psi \land \phi)\} \vdash \psi \Rightarrow (\varphi
\Rightarrow \phi)$ The derivation is to be made using natural deduction. I came to the following derivation: However, I am unsure of the last step ( $\Rightarrow I$ ). It is the case that $\psi$ was found to be true; namely, when we arrived to $\psi \land \phi$ . So the conclusion seems correct from an intuitive point of view. But I think the formal application of the rule is incorrect. The rule for $\Rightarrow I$ states that If $[\varphi] \ldots \phi $ is a derivation, then $\varphi \Rightarrow \phi$ . In our case, I should have gotten a derivation of the form $\psi \ldots (\varphi \Rightarrow \phi)$ , but what I have is a derivation of the form $(\psi \land \phi) \ldots (\varphi \Rightarrow \phi)$ , which is not identical to what is stated in the rule. Of course, another rule is $\land E$ , by virtue of which $\psi$ follows from $\psi \land \phi$ , but I have never applied this rule explicitly for $\psi$ (only for $\phi$ ).","['natural-deduction', 'propositional-calculus', 'logic', 'discrete-mathematics']"
4689026,Show that the order of a finite subgroup of $\mathrm{SL}_n(\mathbb{Z})$ divides the order of $\mathrm{SL}_n(\mathbb{F_3})$,"This problem comes from Johns Hopkins University Spring 2020 algebra qualifying. Let $G$ be a finite subgroup of $\mathrm{SL}_n(\mathbb{Z})$ . Prove that the order of $G$ divides $$
\frac{1}{2}\left(3^n-1\right)\left(3^n-3\right) \ldots\left(3^n-3^{n-1}\right) .
$$ Hint: Use reduction modulo 3. My idea: Assuming $\pi_p:\mathrm{SL}_n(\mathbb{Z}) \rightarrow \mathrm{SL}_n(\mathbb{F_3})$ performed by mod 3 ,so it is sufficient to prove $\pi_p$ provides a injection between finite subgroups  of $\mathrm{SL}_n(\mathbb{Z})$ to $\mathrm{SL}_n(\mathbb{F_3})$ , for which I cannot give a proof. I have found it have been solved on https://en.wikipedia.org/wiki/Congruence_subgroup ,where it provides a huge paper concerning some advanced method. But I havn't learn algebraic number theory. So I wonder some simple method for graduates level students.","['group-theory', 'abstract-algebra', 'linear-algebra']"
4689027,How can I show that $\sup_{0\leq t\leq 1} |B_t| \stackrel{d}{=} \frac{1}{\sqrt{\tau_1}}$?,"Let $(B_t)_{0\leq t\leq 1}$ be a Brownian motion. I want to show that $$\sup_{0\leq t\leq 1} |B_t| \stackrel{d}{=} \frac{1}{\sqrt{\tau}}$$ where $\tau:=\inf\{t>0: |B_t|\geq 1 \}$ . My idea was to pick an arbitrary $q\in \Bbb{R}$ and show that $\Bbb{P}\left(\sup_{0\leq t\leq 1} |B_t|\leq q\right)=\Bbb{P}\left( \frac{1}{\sqrt{\tau}}\leq q\right)$ . But $$\begin{align}\Bbb{P}\left( \frac{1}{\sqrt{\tau}}\leq q\right)&=\Bbb{P}\left( \frac{1}{\tau}\leq q^2\right)\\&=\Bbb{P}\left(\sup\left\{\frac{1}{t}: t\in A\right\}\leq q^2\right)\end{align}$$ where $A:=\{t>0: |B_t|\geq 1\}$ . But now I don't get further, can someone help me?","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4689065,What does it mean to take the first Chern class of a sheaf?,"I know that if I have a divisor $D$ on a Riemann surface $X$ , there is a line bundle associated to $D$ , that I write as $[D]$ following the terminology in Griffiths & Harris (Principles of Algebraic Geometry). Now I know that the expression $c_1([D])$ makes sense, as $c_1$ associates an element of $H^1(X,\mathcal{O}^*)\cong \textrm{Pic}(X)$ to a class in $H^2(X,\mathbb{Z})$ . However in reading about the Hirzebruch-Riemann-Roch theorem, I saw the following assertion: $$\int_Xc_1(\mathcal{O}(D))=\deg(D),$$ where I think $\mathcal{O}(D)$ is the sheaf of holomorphic sections of the associated line bundle $[D]$ . How am I supposed to understand $c_1(\mathcal{O}(D))$ since $\mathcal{O}(D)$ is a sheaf and not a line bundle (albeit it is a sheaf associated to a line bundle)? EDIT: If $s$ is a meromorphic section of a line bundle $L$ , then the fact is that $[(s)]=L$ where $(s)$ is the divisor of the section, i.e. $$(s)=\sum_{p\in X}\textrm{ord}_p(s)\cdot p.$$ Therefore for any such section $s$ of $\mathcal{O}([D])$ I just take $[(s)]=[D]$ and $c_1(\mathcal{O}(D))$ must be taken to mean $c_1([D])$ . Can someone confirm this?","['divisors-algebraic-geometry', 'algebraic-geometry']"
4689103,Determine the orthogonal projection of $f$ onto $K := \{u \in H : |u| \le h \text{ a.e.}\}$,"Let $(\Omega, \mathcal F, \mu)$ be a $\sigma$ -finite measure space. Let $h:\Omega \to [0, \infty)$ be $\mu$ -measurable. Let $H:= L^2(\Omega)$ and $$
K := \{u \in H : |u| \le h \text{ a.e.}\}.
$$ Then $K$ is non-empty closed convex subset of $H$ . I'm trying to solve below exercise Fix $f \in H$ and let $u$ be the orthogonal projection of $f$ onto $K$ . Determine $u$ . Could you verify my below attempt? Let $\Omega_1 := \{ |f| \le h \}, \Omega_2 := \{ f < -h \}$ and $\Omega_3 := \{ f > h \}$ . We define $u$ by $$
u (\omega) :=
\begin{cases}
f(\omega) &\text{if} \quad \omega \in \Omega_1, \\
-h(\omega) &\text{if} \quad \omega \in \Omega_2, \\
h(\omega) &\text{if} \quad \omega \in \Omega_3.
\end{cases}
$$ Then $u \in K$ . Fix $v \in K$ . It suffices to prove that $\int_\Omega (f-u)(v-u) \le 0$ . First, $\int_{\Omega_1} (f-u)(v-u) =0$ . We have $f+h<0$ on $\Omega_2$ and $v+h \ge 0$ , so $$
\int_{\Omega_2} (f-u)(v-u) = \int_{\Omega_1} (f+h)(v+h) \le 0.
$$ We have $f-h>0$ on $\Omega_3$ and $v-h \le 0$ , so $$
\int_{\Omega_3} (f-u)(v-u) = \int_{\Omega_1} (f-h)(v-h) \le 0.
$$ This completes the proof.","['hilbert-spaces', 'lp-spaces', 'functional-analysis']"
4689114,"Given $U$ open and connected, show that $U^*$ is open and closed.","Hello I am working on an exercice which aims to show that any complete connected locally connected metric space $E$ is path-connected. One of the question is showing that some set is open and closed. I would like to know if my answer is correct, and if so if there's any better way to do it (my proof feels a bit overcomplicated to me). For $U\subseteq E$ open and connected, $a\in U$ and $\epsilon > 0$ they define $U^*$ as the set of $x\in U$ such that there exists connected open sets $V_0,..,V_n$ satisfying: $a\in V_0$ and $x\in V_n$ $\overline V_i \subset U$ and $\text{diam}(V_i) < \epsilon$ for $i=0,1,2,...,n$ $V_{i-1} \cap V_i \neq \emptyset$ for $i=1,2,...,n$ Show that $U^*$ is open and closed (openness) For $x \in U^*$ we have the associated sequence $V_0,..,V_n$ with $x\in V_n$ and $V_n$ open so $\exists r>0$ such that $B(x,r) \subset V_n$ and naturally any $y\in B(x,r)$ is an element of $U^*$ . Hence $U^*$ is open. (closedness) We show that the closure of $U^*$ is a subset of $U^*$ . Let $x\in \overline U^*$ , $E$ a metric space so there's a sequence $(x_n)$ of elements of $U^*$ that converges towards $x$ . $E$ is locally connected so there is $V$ a connected neighborhood of $x$ and by convergence $\exists n\in\mathbb{N}$ s.t. $x_n \in V$ . We have $x_n \in U^*$ so there exists sets $V_0,..,V_n$ satisfying the three conditions. Note that $V\cap (E\setminus\overline V_n)$ and $V\cap V_n$ form a disjoint open cover of $V$ . Since $V$ is connected and $V \cap V_n$ isn't empty because $x_n \in V \cap V_n$ we have that $V\cap (E\setminus\overline V_n) = \emptyset$ hence necessarily $x\in\overline V_n$ Therefore, \begin{gather}
x\in\overline V_n \subset U
\end{gather} Now we wish to construct a connected open set $V'$ such that $V_0,..,V_{n-1},V'$ has the three desired properties so we can conclude that $x\in U^*$ . $U$ is open so there exists $0<r<\epsilon - \text{diam}(V_n)$ such that $B(x,r)\subset U$ . Now since $E$ is a metric space it is regular so there is a neighborhood $\Gamma$ of $x$ such that $x\in \overline \Gamma \subset B(x,r)$ . Finally since $E$ is locally connected there is an open connected neighborhood $C\subset\Gamma$ of $x$ such that \begin{gather}
x\in C\subset\overline\Gamma\subset B(x,r)
\end{gather} Now consider \begin{gather}
V' = C \cup V_n
\end{gather} It's open since it's the union of two open sets. Note that $V_n$ and $C$ are both connected and $C\cap V_n \neq \emptyset$ since $C$ is a neighborhood of $x\in\overline V_n$ . Hence $V'$ is connected. Concerning the three conditions, $x\in V'$ since $x\in C$ $\overline V' = \overline C \cup \overline V_n$ and $\overline C \subset \overline \Gamma \subset U$ since $\overline V_n \subset U$ we have $\overline V' \subset U$ . The condition on the radius of the ball implies that $\text{diam}(C) < \epsilon - \text{diam}(V_n)$ so we easily get $\text{diam}(V') < \epsilon$ by triangular inequality. And obviously $V' \cap V_{n-1} \neq \emptyset$ . Therefore $V_0,...,V_{n-1},V'$ respect all the desired properties and thus $x\in U^*$ . Consequently $U^*$ is closed.","['general-topology', 'solution-verification', 'metric-spaces']"
4689130,"Lebesgue measurable function and set, $\forall t\in (0,1), $there exists $ E_t \subset E$ such that $\int_{E_t} f(x) dx = t \int_{E} f(x) dx$.","The question is: Let f be Lebesgue measurable on $R$ and $E\subset R$ be measurable so that $0<A=\int _E f(x)dx < \infty$ . Show that for every $t\in (0,1)$ there exists a measurable set $E_t \subset E$ so that $\int_{E_t} f(x) dx = t A$ . My thoughts to solve this question and my question is in step 3: Since $f$ is Lebesgue measurable on $R$ , so $f$ can be approximated by simple functions. So if I can show the result is true for $f(x) = \chi _{B}(x)$ where $\chi _{B}(x)$ is a simple function. Then the result is true for Lebesgue measurable $f$ . Now $\int _{E} \chi _{B}(x) dx = m(E\cap B)= A$ ( $m$ is Lebesgue measure). Now I have to show that there exists a measurable set $E_t \subset E$ such that $m(E_t\cap B)=t \cdot m(E\cap B)= tA$ . To show that $m(E_t\cap B)=t \cdot m(E\cap B)= tA$ , here is the trouble for me if the above steps are correct. After searching posts here, I think I can define a $g(x) = m(E\cap B\cap (-\infty, x))$ . I need to show $g(x)$ is continuous and then that could lead to the conclusion that there will be $t_0$ such that $0<g(t_0) = t \cdot m(E\cap B)<m(E\cap B)$ . I would like to hear about your comments about the above steps and see if there is any error. Thanks!","['measure-theory', 'lebesgue-measure', 'real-analysis', 'measurable-sets', 'measurable-functions']"
4689192,Evaulate $\int_{-\infty}^{\infty} \frac{\ln(x^2+1)}{x^4+x^2+1}dx$,"I'm really stuck on this integral. I want to believe there is a closed form for it but I'm really unable to find it. WFA cannot find one either. I think it may be possible to use Feynman's Trick and reduce it to something easier but I can't find a good parameterisation that gives something workable. So far, I have factored the denominator to be $(x^2+x-1)(x^2+x+1)$ and have tried to parametrise like $$I(a) = \displaystyle \int_{-\infty}^{\infty} \frac{\ln(a(x^2+1))}{(x^2+x-1)(x^2+x+1)}dx$$ Which looks promising but after differentiating and integrating (via symbolic math software) I get $I'(a)=\frac{\pi}{\sqrt{3}a}$ and I can't do anything to retrieve $I(1)$ since $I(0)$ is not defined. Performing partial fraction decomposition gives $$\int_{-\infty}^{\infty} \frac{x\ln(x^2+1)+\ln(x^2+1)}{2(x^2+x+1)} - \int_{-\infty}^{\infty} \frac{x\ln(x^2+1)-\ln(x^2+1)}{2(x^2-x+1)}$$ I am unsure where to go from there. Can anyone give me a hint at the paramterisation if that even is the right approach or if a closed form for this integral even exists? WFA says the decimal expansion is 0.743763...","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'calculus']"
4689221,How do you represent a transformation between bivectors?,"Say one has 2 vectors in 2D $v_1, v_2$ . One can construct the bivector $b_1 = v_1 \wedge v_2$ . Say that there is a linear map $L$ that relates 2 other vectors as follows: $L(v_1) = u_1, L(v_2) = u_2$ . $L$ acting on vectors this way implies the existence of a map acting on the bivectors themselves as: $M(b_1)=L(v_1)\wedge L(v_2) = u_1\wedge u_2 = b_2$ If one has a coordinate representation of $v, u, L$ , is there a coordinate represnetation of $M$ ? Is it different from $L$ ?","['linear-algebra', 'geometry', 'clifford-algebras']"
4689314,How fast does $f(x)=f(x)^{\frac{x-1}{x}}+x$ grow?,"Say $f(x)\in\mathbb{R}$ is defined by $$f(x)=f(x)^{\frac{x-1}{x}}+x$$ For example, $f(95)$ is around $1306$ since $1306^{94/95}+95\approx1306$ . A friend found this and was trying to find its growth rate. Just something like $f(x)\sim c\cdot x^k$ . Just from eyeballing the growth rate for small $x$ , it looks like $k$ is a bit under $2$ but I couldn't find any way to prove this. I did notice that $$``f'(x)""\approx f(x+1) - f(x)=f(x+1)^{\frac{x}{x+1}}-f(x)^{\frac{x}{x+1}\cdot\frac{x^2-1}{x^2}}+1$$ which makes me want to say $f$ 's derivative is simply $1$ since $\frac{x^2-1}{x^2}$ tends to $1$ . But this isn't rigorous or even well defined. Part of me feels like the Lambert-W function needs to get involve since directly substituting $f(x)=c\cdot x^k$ leaves us with $x$ to the power of $x$ (or at least, expressions involving $x$ ). EDIT: forgot to add, $f(x)$ can be equivalently defined with $$f(x)^{-1}=\Big(1-\frac{x}{f(x)}\Big)^x$$ which I didn't find to be particularly helpful. The equation looks like a weird variant of $e$ 's limit definition though and is interesting.","['functions', 'exponential-function', 'asymptotics', 'real-analysis']"
4689316,"Evaluate $I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k$","With the interests of $$
I(a,b)=\int_{0}^{1}k^a(1-k^2)^bK(k)\text{d}k,
$$ where $K(k)$ represents the complete elliptic integral with modulus $k$ and $K^\prime(k)$ its complementary,
many $I(a,b)$ for rational $a,b$ are evaluated. The methodologies are as follows: $\textbf{1.Hypergeometric Representations}$ Expanding $K(k)$ , we obtain(conditions omitted) $$I(a,b)=\frac{\pi}{4} \frac{\Gamma(b+1)\Gamma\left ( \frac{a+1}{2}  \right ) }{
\Gamma\left ( \frac{a+3}{2}+b  \right ) }
{}_3F_2\left ( \frac{1}{2},\frac12,\frac{a+1}{2};\frac{a+3}{2}+b,1;1    \right ).$$ Note that $I(a,b+1)=I(a,b)-I(a+2,b)$ . Apparently, supposing $\frac{a+3}{2}+b=\frac12$ i.e. $b=-1-\frac{a}{2}$ , it simplifies to $$
I\left ( a,-1-\frac{a}{2}  \right ) 
=\frac{\cos\left ( \frac{\pi a}{2}  \right ) }{4\pi} 
\Gamma\left ( -\frac{a}{2}  \right )^2\Gamma\left ( \frac{a+1}{2}  \right )^2.
$$ Also, $$
I(1,b)=\frac{\pi}{4} \frac{\Gamma\left ( b+1 \right )^2 }{
\Gamma\left ( b+\frac32 \right )^2}.
$$ Some hypergeometric transformations are applicable to the given ${}_3F_2$ . For instances, from here we have $$I(s,0)+I(-s-1,0)
=-\frac{\pi}{4}\tan\left ( \frac{\pi s}{2}  \right ) 
\frac{\Gamma\left ( -\frac{s}{2} \right )^2 }{
\Gamma\left ( \frac{1-s}{2}  \right )^2 }.$$ (add on May 20th, 23 ) Applying Dixon's ${}_3F_2$ Theorem, that allowed us to proceed further. Explicitly, as $a+b=-1/2$ , $$
I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{2\pi} } 
\frac{\Gamma\left ( \frac{1+a}{2}  \right ) \Gamma\left ( \frac12-a \right )
\Gamma\left ( \frac14-\frac{a}{2} \right )  }{\Gamma\left ( \frac34-\frac{a}{2}  \right )
\Gamma\left ( \frac{1-a}{2}  \right )  }.
$$ If $a-2b=1,\Re(a)>0$ , $$
I(a,b)=\frac{\Gamma\left ( \frac14 \right )^2}{8\sqrt{\pi} }
\frac{\Gamma\left ( 1+\frac{a}{2}  \right )\Gamma\left ( \frac{1+a}{2}  \right )^3}{
\Gamma(1+a)\Gamma\left ( \frac{3}{4}+\frac{a}{2}   \right )^2 },
$$ which gives the evaluation $$
I\left ( -\frac{5}{6},-\frac{11}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot2^{2/3}}.
$$ $\textbf{2.Contour Integration}$ We have $$
I(a,b)+\cos(b\pi)I(-a-2b-1,b)
+\sin(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right ) 
+\sin\left ( \frac{\pi a}{2}  \right )I\left ( a,-\frac{a}{2}-b-1  \right )=0
,$$ and $$
\cos(b\pi)I\left ( 2b+1,-\frac{a}{2}-b-1  \right ) 
=\cos\left ( \frac{\pi a}{2}  \right )
I\left ( a,-\frac{a}{2}-b-1 \right )+\sin\left ( \pi b \right )
I(-a-2b-1,b).
$$ Setting $a=-5/6,b=-11/12$ , producing \begin{aligned}
I\left ( \frac{5}{3},-\frac{11}{12}   \right ) 
& = \sqrt{3}\left ( \sqrt{3}+1  \right ) I\left ( -\frac56,\frac13 \right )\\
& =\frac{3^{1/4}\left ( \sqrt{3}+1  \right )\Gamma\left ( \frac14 \right )^4  }{4\pi\cdot 2^{1/6}},
\end{aligned} where the second equality is owing to the former identity for $a+b=-1/2$ . Another less obvious one is, $$
\int_{0}^{1} \frac{k^{2/3}K(k)}{\left ( 1-k^2 \right )^{2/3} }
\text{d} k=\frac{3^{3/4}\Gamma\left ( \frac14 \right )^4 }{24\pi\cdot2^{1/6}  }.
$$ $\textbf{3.Modular Forms(1)}$ The basic idea is to construct a modular form expressed by Jacobi $\vartheta$ functions and compute its $L$ -value. For example, setting $q=\exp(-\pi K^\prime(k)/K(k))$ $$
f(q)=\sum_{m,n\in\mathbb{Z}}
(-1)^m\left [ 3\left ( m+\frac16 \right )^2-n^2 \right ]
q^{3\left ( m+\frac16 \right )^2+n^2 }.$$ We have $$f(q)=\frac{2^{2/3}k^{1/6}(1-k^2)^{1/12}(1-2k^2)}{
3\pi^3}K(k)^3.$$ And $$\int_{0}^{\infty}xf(q)\text{d}x
=\frac{1}{\pi^2} \sum_{m,n\in\mathbb{Z}}
\frac{(-1)^m}{\left ( \sqrt{3}\left ( m+\frac16 \right )+ni   \right )^2 }.$$ Therefore it's sufficient to show that $$\int_{0}^{1} \frac{(2k^2-1)K(k)}{k^{5/6}(1-k^2)^{11/12}}\text{d}k
=\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\pi^2\cdot2^{1/3}  },$$ and therefore, $$
I\left ( \frac{7}{6},-\frac{11}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
32\pi^2\cdot2^{1/3}  }.
$$ Which also gives $$
I\left ( -\frac{5}{6},\frac{1}{12}   \right )
=\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot2^{2/3}}
-\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
32\pi^2\cdot2^{1/3}  },
$$ with $$
\int_{0}^{1}\left ( 1-k^{12} \right )^{\frac{1}{12}}
K\left ( k^6 \right )\text{d}k =\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac14 \right )^4  }{48\pi\cdot2^{2/3}}
-\frac{3^{3/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{64\pi^2\cdot2^{1/3}  }.
$$ However, this usually only gives linear equation among two $I(a,b)$ . Apart from $$I\left(-\frac34,0\right)=\frac{\left ( 3+\sqrt{2}  \right )
\Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{
48\pi\sqrt{2} },$$ $$
I\left(-\frac14,0\right)=\frac{\left ( 3-\sqrt{2}  \right )
\Gamma\left ( \frac18 \right )^2\Gamma\left ( \frac38 \right )^2  }{
48\pi\sqrt{2} },$$ they can be: $$
2I\left ( -\frac13,-\frac{11}{12} \right )
-I\left ( \frac53,-\frac{11}{12} \right ) 
=\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
8\cdot2^{5/6}\pi^2  },
$$ $$
4I\left ( -\frac23,-\frac{17}{24} \right )
+I\left ( \frac43,-\frac{17}{24} \right ) 
=\frac{\left ( \sqrt{2}-1  \right )^{\frac32}
\left ( \sqrt{3} +\sqrt{2}  \right )^{\frac32}\cdot3^{\frac14}
\left ( 1+\left ( 2-\sqrt{3}  \right )
\left ( \sqrt{3} -\sqrt{2}  \right )   \right )^2 }{8\cdot2^{\frac13}\pi}
\Gamma\left ( \frac{1}{24} \right )
\Gamma\left ( \frac{5}{24} \right )
\Gamma\left ( \frac{7}{24} \right )
\Gamma\left ( \frac{11}{24} \right ).
$$ Gathering all information we know about $I(a,b)$ , we obtain two additional
values: \begin{aligned}
&I\left ( -\frac13,-\frac{11}{12} \right )
=\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\cdot2^{5/6}\pi^2  },\\
&I\left ( -\frac13,\frac{1}{12}  \right )
=-\frac{3^{1/4}\left ( 1+\sqrt{3} \right )\Gamma\left ( \frac14 \right )^4  }{8\pi\cdot 2^{1/6}}
+\frac{3^{7/4}\left ( 1+\sqrt{3}  \right )\Gamma\left ( \frac13 \right )^6  }{
16\cdot2^{5/6}\pi^2  }.
\end{aligned} $\textbf{4.Modular Forms(2)}$ It's also possible to prove that \begin{aligned}
&\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{(1-2k^2)K(k)}{\sqrt{k}(1-k^2)^{3/4}}\text{d}k
=2^{2s}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)+L_{-8}(s)L_{8}(s-2)]},\\
&\color{purple}{\left(\frac2\pi\right)^2\int_{0}^{1} \left ( \frac{K^\prime}{K}  \right )^{s-1}
\frac{\sqrt{k}K(k)}{(1-k^2)^{1/4}}\text{d}k
=2^{2s-1}\pi^{-s}\Gamma(s)[L_8(s)L_{-8}(s-2)-L_{-8}(s)L_{8}(s-2)]}.
\end{aligned} Also note that $a=-\frac12,b=-\frac34=-1-\frac{a}{2}$ , we derive \begin{aligned}
&I\left ( \frac32,-\frac34 \right ) 
=\frac{\pi^2}{4\sqrt{2} }+\frac{\Gamma\left ( \frac14 \right )^4 }{8\pi\sqrt{2} },\\
&I\left ( \frac12,-\frac14 \right ) 
=\frac{\pi^2}{4\sqrt{2} }.
\end{aligned} $\textbf{5.Fourier-Legendre Expansions}$ Similarly to here . But most results aren't newly-created. The questions come here: Whether we can find more closed-forms for single $I(a,b)$ ? Can we come up with more ways to cope with $I(a,b)$ ? Remark : The remained question is $I\left(\frac34,-\frac38\right)$ , which seems to be unable to prove in ways listed above.","['integration', 'definite-integrals', 'real-analysis', 'elliptic-integrals', 'hypergeometric-function']"
4689321,Evaluate the integral $\int\limits_0^\infty {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}$,"I found this integral on Instagram $$I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}$$ My first try is: $$\begin{array}{l}
\displaystyle I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}  = \sum\limits_{n = 0}^\infty  {\int\limits_n^{n + 1} {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx} } \\
\displaystyle {\rm{Let}}:t = x - n \Rightarrow dt = dx \Rightarrow I = \sum\limits_{n = 0}^\infty  {\int\limits_0^1 {{{\left( {\frac{e}{{t + n}}} \right)}^{t + n}}\Gamma \left( {t + n} \right)\sin \left( {2\pi t} \right)dt} } 
\end{array}$$ But I don't know how to process further from this step. May I ask for some advices? Thank you very much.","['integration', 'calculus']"
4689353,"Possible arrangements of the letters of the word ""Polyunsaturated"" in which the vowel order is preserved","How many possible arrangements can we sort the letters of the ""Polyunsaturated"" so that the vowel order is preserved in this word? I think $p,l,y,n,s,t,r,t,d$ have $\frac{9!}{2!}$ possible arrangements, because "" $t$ "" have $2$ repetition. $o,u,a,u,a,e$ must sit in $10$ places between $p,l,y,n,s,t,r,t,d$ and left and right of that. But I don't know how count them so that preserved this order.","['combinatorics', 'discrete-mathematics']"
4689368,"Proof of the measurability of functions in Theorem 1.10 (Product measure) in ""Analysis"" by Lieb and Loss","I am confronted with the following problem: Let $\left(\Omega_1,\Sigma_1,\mu_1\right)$ and $\left(\Omega_2,\Sigma_2,\mu_2\right)$ be two sigma-finite measure spaces. Let $A$ be a measurable set in $\Sigma_1\times\Sigma_2$ , where $\Sigma_1\times\Sigma_2$ is the smallest sigma-algebra containing all rectangles defined by $ A_1\times A_2 = \left\{ \left(x_1,x_2\right) {}:{} x_1\in A_1,\quad x_2\in A_2 \right\},\quad A_1\in\Sigma_1,\quad A_2\in\Sigma_2 $ . Next, for every $x_2\in\Omega_2$ , set $f\left( x_2\right) :=\mu_1\left( A_1\left(x_2\right)\right)$ and, for every $x_1\in\Omega_1$ , $g\left( x_1\right) :=\mu_2\left(A_2\left(x_1\right)\right)$ , where $ A_1\left( x_2\right) = \left\{x_1\in\Omega_1\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_1 $ and $ A_2\left( x_1\right) = \left\{x_2\in\Omega_2\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_2 $ . Then $f$ is $\Sigma_2$ -measurable, $g$ is $\Sigma_1$ -measurable and $\left(\mu_1\times\mu_2\right)(A) :=\int_{\Omega_2}f\left( x_2\right)\mu_2\left(\mathrm{d}x_2\right) =\int_{\Omega_1}g\left(x_1\right)\mu_1\left(\mathrm{d}x_1\right).$ Now the first part of the proof says: The measurability of $f$ and $g$ parallels the proof of the section
property in Section 1.2 and uses the Monotone Class Theorem, while the section property says that for every $A\in\Sigma_1\times\Sigma_2$ the following applies: $A_1\left(x_2\right)\in\Sigma_1\quad\forall x_2\in\Omega_2\quad$ and $\quad A_2\left(x_2\right)\in\Sigma_2\quad\forall x_1\in\Omega_1.$ To show the section property, you define a class of sets which have the section property and show that this class is a sigma-algebra. Then, this class is also the smallest sigma-algebra containing all rectangles by definition - as far as I understood it, at least. Now, to show that $f$ and $g$ are measurable, I think that I have to define a class $\mathcal{M}$ of sets,  for which $f$ is $\Sigma_2$ -measurable and $g$ is $\Sigma_1$ -measurable, and show that $\mathcal{M}$ is a monotone class. The Monotone Class Theorem then states, that this class is also a sigma-algebra and therefore I should infer that the measurability of $f$ and $g$ holds. But my problem is that I am having a hard time to construct such a class $\mathcal{M}$ . Do you have any ideas for an approach? I'd appreciate the help.","['monotone-class-theorem', 'product-measure', 'measurable-functions', 'analysis']"
4689382,Advanced methods to explain indeterminate forms,"I know this post may be covering a subject that is considered 'low quality' but I wanted to try and cover it in a more advanced manner (before writing I also searched if there were duplicate posts). I have to teach some students about analysis, but I already wanted to give them tricks to solve limits quickly, just not to be the usual teacher explaining the usual things that everyone can find online. Having to start the course by explaining the indeterminate forms, I wanted a confirmation regarding a notion that I know, but that I have not found anywhere. When we have indeterminate forms $\left[\dfrac{0}{0}\right],\;[0^0],\; [1^{\infty}],[0\cdot\infty], [\infty^0]$ , we can build the following tables: ( $\textbf{ind.}$ =indeterminate form, n.d.=not defined) $$\begin{array}{|c|c|c|c|}\hline
\left[\frac{0}{0}\right]&\frac{0^+}{\cdot}&\frac{0}{\cdot}&\frac{0^-}{\cdot}\\\hline
\frac{\cdot}{0^+}&\textbf{ind.}&0&\textbf{ind.}\\\hline
\frac{\cdot}{0}&\text{n.d.}&\text{n.d.}&\text{n.d.}\\\hline
\frac{\cdot}{0^-}&\textbf{ind.}&0&\textbf{ind.}\\\hline
\end{array}\qquad
\begin{array}{|c|c|c|c|}\hline
[0^{0}]&(0^+)^{\cdot}&(0)^{\cdot}&(0^-)^{\cdot}\\\hline
(\cdot)^{0^+}&\textbf{ind.}&0&\textbf{ind.}\\\hline
(\cdot)^{0}&1&1&1\\\hline
(\cdot)^{0^-}&\textbf{ind.}&\text{n.d.}&\textbf{ind.}\\\hline
\end{array}$$ $$
[1^{\infty}]\;\Rightarrow\;\begin{cases}
(1^+)^{\infty}&\textbf{ind.}\\
(1)^{\infty}&1\\
(1^-)^{\infty}&\textbf{ind.}\\
\end{cases}\qquad [0\cdot\infty]\;\Rightarrow\begin{cases}0^{+}\cdot\infty&\textbf{ind.}\\
0\cdot\infty&0\\
0^{-}\cdot\infty&\textbf{ind.}
\end{cases}\qquad[\infty^0]\;\Rightarrow\begin{cases}\infty^{0^+}&\textbf{ind.}\\
\infty^{0}&1\\
\infty^{0-}&\textbf{ind.}
\end{cases}$$ The sense of these schemes is to show that the indeterminate forms are not all the forms where $0$ and $1$ appear brutally, but only the cases in which the numbers tend to $0$ and $1$ . My doubt is the fact that I have never found these tables in books but honestly they seem very valid, especially in cases where it's asked ""If I multiply $1$ infinite times by itself, shouldn't it give $1$ ?"", even to give a sense of the value of $0^0$ . P.s. For all those who could write me "" $0^0$ is indeterminate because $0^0=\exp(0\ln(0))$ "" I point out that this example is not valid since with the same method I could say that $0=0^2=\exp(2\ln(0))$","['indeterminate-forms', 'limits', 'education', 'real-analysis']"
4689398,Questions about Stein's Real Analysis Chapter 3 Exercise 3,"I'm trying to solve the following problem in Stein's Real Analysis (exercise 3 Chapter 3) and here is the partial proof that I found difficult to understand. As far as I understand, the proof says $E \cap (-E)$ has positive measure, so $E$ has a sequence $\{x_n\}$ and the proof ends. What I'm uncertain about is how the fact that a set has a positive measure guarantees the existence of a sequence in the set? Also, the exercise asks us to show the existence of a sequence that converges to $0$ . But it seems the proof above doesn't prove it. Any comments about how the proof makes sense would be appreciated. Also, If there is a better idea about this exercise, that would also be welcome. Thank you.","['proof-explanation', 'measure-theory', 'real-analysis']"
4689427,"Can this given $f: S^1\to \mathbb C$ be extended to a continuous $F: \overline{\mathbb D}\to \mathbb C, F$ is holomorphic on $\mathbb D$?","Suppose that $f: \mathbb S^1\to \mathbb C$ is continuous such that $f(z)=f(\bar z)$ for all $z\in \mathbb S^1$ . Can it be extended to a continuous $F: \overline{\mathbb D}\to \mathbb C$ such that $ F$ is holomorphic on $\mathbb D$ ? If $f$ is constant, then the result is obviously true. So suppose that $f$ is not constant. Suppose that such an extension $F$ exists. Take any $r\in (0.2,1)$ . For any $z\in D_{1/10}(0)$ , we have $$F(z)=\int_{|z|=r}F(u)/(u-z) \,du\tag 1$$ $(1)$ gives $F(z)=\sum_{k\ge 0}\left(\int_{|z|=r}\frac{F(u)}{u^{k+1}}\right)z^k=\sum_{k\ge 0} a_k(r) z^k$ , where $(r)$ denotes that $a_k$ depends upon $r$ .
Calculations show that $2\pi r^ka_k(r)=\color{blue}{\int_0^{2\pi} F(re^{-i\theta})e^{ki\theta} \,d\theta}$ . By continuity of $F,F(re^{-i\theta})\overbrace{\to}^{r\to 1-} F(e^{-i\theta})$ $$\implies 2\pi a_k(r)\overbrace{\to}^{r\to 1-}\int_0^{2\pi} \color{blue}{F(e^{-i\theta})}e^{ki\theta} \,d\theta=\int_0^{2\pi} \color{blue}{F(e^{i\theta})}e^{ki\theta} \,d\theta=i\int_{|z|=1}F(z)z^{k-1}dz=0\,\forall k\ge 1$$ It follows that $a_k(r)\to 0$ as $r\to 1-$ . $\color{red}{\text{Since $z$ is fixed and power series representation is unique, we must have $a_k(r)=$ constant.}}$ Hence $a_k(r)=0$ for all $k\ge 1$ . It follows that $F$ is a constant in $D_{1/10}(0)$ , whence by identity theorem it follows that $F$ is constant which contradicts our assumption. So such an extension is not possible. Is this correct? I am not sure about validity of the read part. Can anyone please help me justify or disprove the red colored part? Thanks.","['complex-analysis', 'continuity', 'power-series', 'cauchy-integral-formula']"
4689453,Prove that for any $x$ the inequality is true.,"Prove that for any $x \left(x \neq \dfrac{\pi}{2}k, k \ \text{- integer}\right)$ the inequality $\displaystyle{\left(1+\dfrac {1}{\left(\sin{x}\right)^2}\right) \left(1+\dfrac{1}{\left(\cos{x}\right)^2}\right)} \geq 9$ .
Let's transform the left side of the inequality: $$\left(1+\dfrac{1}{\left(\sin{x}\right)^2}\right) \left(1+\dfrac{1}{\left(\cos{x}\right)^2}\right) = \left(1+\dfrac{\left(\sin{x}\right)^2 + \left(\cos{x}\right)^2}{\left(\sin{x}\right)^2}\right) \left(1+\dfrac{\left(\sin{x}\right)^2 + \left(\cos{x}\right)^2}{\left(\cos{x}\right)^2}\right) = 9+ 2 \left( \left(\tan{x}\right)^2 - 2 \tan{x} \cdot \cot{x} + \left(\cot{x}\right)^2\right) = 9 + 2 \left( \tan{x}-\cot{x}\right)^2.$$ Consequently $$9 + 2 \left( \tan{x}-\cot{x}\right)^2 \geq 9.$$ Which also had to be proven. I thought that the example could be solved faster using substitution $\sin{x}=a, \cos{x}=b$ and using the main trigonometric identity: $\sin^2{x}+\cos^2{x}=1$ or $a^2+b^2=1$ . So we have $$\left(1+ \dfrac{1}{a^2}\right) \left(1+ \dfrac{1}{b^2}\right) \geq 9.$$ $$\dfrac{\left(1+a^2\right)\left(1+b^2\right)}{a^2b^2} = \dfrac{1+b^2+a^2+a^2b^2}{a^2b^2}=\dfrac{2+a^2b^2}{a^2b^2}.$$ And from that moment on, no idea what to do next. Maybe the solution is too simple? I will be grateful for any hint. Thanks! Thanks to @yanruijie I got such answer $$\displaystyle{\left(1+\dfrac{1}{a^2}\right) \left(1+\dfrac{1}{b^2}\right)} = \dfrac{\left(a^2+1\right)\left(b^2+1\right)}{a^2b^2} = \dfrac{a^2b^2+a^2+b^2+1}{a^2b^2}=1+\dfrac{2}{a^2b^2} \geq 1+2 \cdot \dfrac{4}{\left(a^2+b^2\right)^2}=1+2 \cdot \dfrac{4}{1}=9.$$ Thanks everyone, great solutions!","['trigonometry', 'inequality']"
4689454,Variance of Order Statistics,"I have a question about bounding the variance of order statistics. Given that for $i \in \{1,\cdots,\lambda\}$ , denote $Bin(s,\frac{1}{n})$ to be a binomial random variable with success probability $\frac{1}{n}$ and $s$ independent experiments. Define $W_i=Bin(n-s,\frac{1}{n})-Bin(s,\frac{1}{n})$ , where $n \in \mathbb{N}$ is some constant w.r.t s and $\lambda$ . All $W_i$ are i.i.d random variables. Is there any estimate that I can use to upper bound the variance of the largest order statistics $Z= \max_{i \in [\lambda]}\{W_i\}$ , i.e. $Var(Z)\leq U$ ? I am aware that we can use $\max_{i \in [\lambda]}\{W_i\}\leq \sum W_i$ to bound the variance $Var(Z)\leq \sum Var(W_i)=\lambda * n \frac{1}{n}(1-\frac{1}{n})= \lambda(1-\frac{1}{n})$ . But I wonder whether we can derive any tighter upper bound in this case. So we can bound the variance within some lower order term in $\lambda$ or even constant $O(1)$ .
Thank you!","['statistics', 'analysis', 'order-statistics', 'probability-theory', 'probability']"
4689461,Probability of taking black ball out of k-th bag,"There are $k ≥ 3$ bags, each containing m black and n white marbles. A marble is randomly chosen from
the first bag and transferred to the second bag; then a marble is randomly chosen from the second bag and transferred to the third bag, and so on until a marble is randomly chosen from the kth bag.
Find the probability that the marble chosen from (i) the 3rd bag is black; (ii) the $k$ -th bag is black. I don't understand how to proceed in this. I thought of trying recursion but it doesn't seem to be working. For the 3 bags case I think it is easy to solve by individually considering each case but for the kth part I have zero idea. Kindly help.
The way I tried recursion:
Suppose $P_k$ is the probability of drawing black ball from $k$ -th bag.
Then, $$P_k=P_k-1*(m+1)/(n+m+1)+(1-P_k-1)*m/(n+m+1)$$ I don't see a way of proceeding further, Plus this seems to be a bit too complicated. I was wondering if there is a simpler solution. Thank you!","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4689470,"Consider the natural numbers x, y, z that simultaneously satisfy the conditions:","Consider the natural numbers $x, y, z$ that simultaneously satisfy the conditions: i) $x,y,z \in \{2000, 2001, 2002, \ldots, 2025\}$ : ii) $|y-z| \le 2$ iii) $\sqrt{1+x\sqrt{yz+1}}=2023$ Show that one of the numbers $x, y, z$ is equal to $2023$ . MY IDEAS Let $x \le y \le z$ $z-y$ must be $\le 2$ then $z-y$ can be $0, 1$ or $2$ . If $z-y=0$ , then $z=y$ The equality will become $\sqrt{1+x\sqrt{{z}^{2}+1}}=2023$ ${z}^{2}+1$ is irrational and will make all the equation irrational. But 2023 isn't irrational, which makes this case impossible. I tried doing the same for $z-y=1$ and $z-y=2$ but got nowhere. Like I don't know what to do. Hope one of you can help me. Any ideas are welcome! Thank you!","['calculus', 'linear-algebra', 'radicals', 'square-numbers']"
4689499,Regular cardinals exponentiation.,"I am struggling with an exercise regarding cardinal exponentiation when one of the two is regular. The exercise reads: Let $k$ , $\lambda$ be cardinals, with $k$ regular and $\lambda < k$ . Prove that $k^{\lambda} = \sum_{\alpha<k}|\alpha|^{\lambda}$ . I still have to get familiar with cofinalities and regular cardinals so I may be missing something very obvious, but I don't know where to start. Any help is very much appreciated :)","['elementary-set-theory', 'cardinals', 'set-theory']"
4689529,Need help understanding triangle properties problem,"I am struggling with a problem that involves triangle properties, and I am hoping someone can help me understand it better. Here is the problem: Two altitudes of a triangle do not intersect, and the acute angle between their extensions is $45^{\circ}$ . Then, (a) one of the angles of the triangle is $45^{\circ}$ ; (b) one of the angles of the triangle is $135^{\circ}$ ; (c) it is impossible to determine; (d) there is no such triangle. I have attempted to solve the problem by drawing a triangle and its two altitudes, but I'm not sure where to go from there. I think I need to use the fact that the product of the lengths of the two altitudes is equal to the product of the sides of the triangle, but I'm not sure how that helps me determine the angles. Can someone please help me understand this problem better? Thank you in advance!","['euclidean-geometry', 'trigonometry', 'problem-solving', 'geometry']"
4689530,Must a self-adjoint operator be surjective when it's injective?,"Let $V$ be a Hilbert space, $\mathcal{A}  : V \rightarrow V$ be a bounded linear operator satisfying that $(\mathcal{A}u,v) = (u,\mathcal{A}v)$ , and $(\mathcal{A}u,u)>0$ if $u \neq 0$ . I know that $\mathcal{A}$ is injective because $\mathcal{A}u = 0$ implies $u = 0$ . Does it hold that $\mathcal{A}$ is surjective? (Maybe use the fact that $\mathcal{A}$ is self-adjoint?) It's easy to see that $\mathcal{A}$ induces another inner product $(u,v)_{\mathcal{A}} = (\mathcal{A}u,v)$ . For any bounded linear operator $\mathcal{B}: V \rightarrow V$ with the original inner product, I am not sure the existence and the uniqueness of the adjoint operator: $$
(B^*u,v)_{\mathcal{A} }= (u, \mathcal{B}v)_{\mathcal{A} }.
$$ I know the answer is yes when $V$ is finite-dimensional.","['operator-theory', 'adjoint-operators', 'functional-analysis']"
4689560,Combinatorial proof understanding [duplicate],"This question already has an answer here : Requesting deeper understanding of binomial coefficient (1 answer) Closed last year . Reference: Appendix C.1-11, CLRS 4th edition. Argue that for any integers $n,j,k \geq 0$ and $j+k \leq n$ $$\binom{n}{j+k} \leq \binom{n}{j}\binom{n-j}{k}$$ . I can see why this inequality holds from algebraic proof clearly.
From combinatorial proof, I've converted statement into this funny way, Suppose a boy is given option to choose $j+k$ chocolate from $n$ chocolates and on RHS a boy is asked to choose first $j$ chocolates from $n$ and then $k$ chocolates from remaining $n-j$ chocolates. Question: At the end boy will choose $j+k$ chocolates then why number of ways of choosing $j+k$ chocolates together are less than choosing them in parts (first $j$ and then $k$ from remaining $n-j$ )? I have a brief argument to it when boy is choosing $j$ then $k$ he'll have to to look up or consider options again in $n-j$ chocolates which he may have seen in first $n \choose j$ expression. I don't know whether it makes any sense or not.","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4689580,History of Frobenius normal $p$-complement theorem,"It is well-known that Burnside’s and Frobenius’ normal $p$ -complement theorems are the very foundations of modern theories relating to groups. I wanted to sort out the history of theses theorems. In Wikipedia, we can find that the one due to Burnside can be located at section 243 of his book (1911, 2nd edition). However, Wikipedia does not give a clear reference addressing the resource of Frobenius’ theorem. I looked up the theorem in several group theory books, but they always state it without citing any book or article of Frobenius. Any comment or answer that can help one obtain more clues is appreciated.","['group-theory', 'abstract-algebra', 'finite-groups', 'math-history']"
4689594,Why are Jacobian varieties important?,"I've recently met Jacobian varieties, in the context of elliptic curves and modular curves. I'm still very new to them and not super comfortable with the machinery, so please forgive me any mistakes here. One convenient thing to do is to let $\varphi: X_0(N) \to E$ be a modular parameterization. Then letting $J(\cdot)$ stand for taking the Jacobian, $\varphi$ ought to transfer to a map $\varphi_*: J(X_0(N)) \to J(E)$ which is both a morphism of varieties and a group homomorphism, right? The group structure of the Jacobian lets you do things that aren't really possible to do directly with $\varphi$ on $X_0(N)$ and since $E \cong J(E)$ by $P \mapsto [P-O]$ , it's easy to move from $J(E)$ to $E$ itself. One question is about what this induced map ought to look like. Given $D = \sum a_i P_i$ a degree- $0$ divisor, is $\varphi_* D = \sum a_i \varphi(P_i)$ (which is again degree $0$ )? (And you have to check this makes sense when modding out the principal divisors.) I'm a bit unsure of what's going on because don't you usually induce maps on divisors going the other way? Furthermore, in the case of $\varphi:X_0(N) \to E$ , what's the relationship between ""summing up points in divisors"" and ""summing up points according to the group law on $E$ ""? I do know there is a lemma (from Silverman's Arithmetic of Elliptic Curves) that says a divisor on $E$ is principal if and only if it is degree $0$ and the points in it sum to $O$ under $E$ 's group law. One other fact I know is that in the case of curves over $\mathbb{C}$ (i.e. Riemann surfaces), there is a very concrete description of the Jacobian, as periods modulo homology, giving a $g$ -dimensional torus. Most of my exposure to Jacobians is through Diamond and Shurman's ""A First Course in Modular Forms."" I apologize for a question that's a bit all over the place, but what's the ""main thing"" I should take away from Jacobian varieties? How do their basic properties work, and what is the underlying reason they are useful for doing number theory and arithmetic/algebraic geometry?","['elliptic-curves', 'number-theory', 'algebraic-geometry', 'modular-forms', 'arithmetic-geometry']"
4689597,Rank of free groups,"In Johnson's 'Topics in the Theory of Group Presentations', one can find this theorem after the definition of free groups using the universal property. Theorem. Free groups of different ranks are not isomorphic. Proof. Let $F$ be free on a subset $X$ with $|X|= \omega$ , and let $G$ be any group. Then it is the burden of definition of free group by the universal property that the mappings $X \to G$ are in one-to-one correspondence with the homomorphisms $F \to G$ . Thus, there are exactly $2^{\omega}$ homomorphisms from $F$ to $\mathbb{Z}_2$ . Since this number is invariant under isomorphism, we see that $2^{\omega}$ , and hence the rank $\omega$ , is determined by the isomorphism class of $F$ . Isn't the author assuming that $2^{|X|}=2^{|Y|} \Rightarrow |X| = |Y|$ ? I have seen this post over here, https://mathoverflow.net/questions/67473/equality-of-cardinality-of-power-set , which says that this statement is independent from ZFC. So is the proof wrong?","['combinatorial-group-theory', 'group-theory', 'free-groups']"
4689642,Does a set of positive outer measure contain a *measurable* set of positive measure?,"Given a complete measure space $(X, \mathcal{X}, \mu)$ , and a subset $A \subseteq X$ of positive outer measure, does there necessarily exist a subset $E$ of a $A$ which is measurable for which $\mu(E) > 0$ ? This feels intuitively like it should be true, but I don't see how to show it, since it involves inner approximations and outer measure is generally defined by reference to outer approximations. One thought I had was to exploit the idea of inner regularity for Borel measures, but I think that definition is typically only stated as the ability to approximate measurable sets from within. However, I want to find a positive-measure and measurable subset of a potentially non -measurable set. Thanks in advance for your help! EDIT: Since somebody asked, by the outer measure I mean $$\mu^*(A) : = \inf \left\{ \sum_{n = 1}^\infty \mu(E_n) : E_n \in \mathcal{X}, A \subseteq \bigcup_{n = 1}^\infty E_n \right\} . $$","['measure-theory', 'outer-measure', 'borel-measures']"
4689643,Uniform convergence of this particular limit functional,"Let $\{h_n(x)\}_{n\in\mathbb{N}}$ a family of infinitely differentiable functions, defined on the entire real line, vanishing outside $[a,b]$ . Let $F(x)$ be any function defined on $\mathbb{R}$ with the property that $F$ and all its derivatives are $O(|x|^{-N})$ as $|x|\to\infty$ , for every $N$ . By hypothesis, the following limit: $$\lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x)\mathrm{d}x$$ exists for any $F$ of the type specified above. Is it true or false that $\lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x+\tau)\mathrm{d}x$ converge uniformly w.r.t. $\tau\in[\tau_1,\tau_2]\subset\mathbb{R}$ ? I tried with Cauchy criterion, but without success...
If the answer is ""no"", with which hypothesis the answer could become ""yes""?","['limits', 'calculus', 'uniform-convergence']"
4689651,Can someone give examples where a non monotonic function is invertible?,I tried to figure out about examples of non monotonic functions that are invertible but I only got to know that it should be discontinuous to be invertible but could not find any such examples.,"['continuity', 'functions', 'monotone-functions', 'inverse-function']"
4689678,How to compute the exterior derivative of a 1-form on projectivization of a vector space,"Let $V$ be a complex vector space, and let $\mathbb{P}(V)$ denote the projectivization of $V$ (i.e. space of 1-dimensional subspaces, i.e. 1st Grassmanian). Suppose further that $V$ is endowed with a non-degenerate two-form $\omega$ . We know that the tangent space $T_v\mathbb{P}(V)$ is canonically isomorphic to $V/\left<v\right>$ . We can thus define a 1-form $\alpha$ by the maps $$\alpha_v(\cdot)=\omega(v,\cdot),$$ which is well defined since $\omega(v,v)=0$ . My question is how I can explicitly compute the exterior derivative $d\alpha$ . The reason I ask this is that I am trying to do Exercise 10.1 in Voisin's ""Hodge theory and complex algebraic geometry i"", where they ask to show that the top dimensional form $\alpha \wedge (d\alpha)^{n-1}$ does not vanish at any point, and I feel that I can't do the exercise if I don't know how to compute $d\alpha$ . EDIT: Below is the exercise in question. Let $V$ be a complex vector space endowed with a non-degenerate 2-form $\omega$ . Recall that $T_{\mathbb{P}(V),v}$ is isomorphic to $V/\left<v\right>$ . Show that the form $\alpha$ defined (up to multiplicative coefficient) by $$\alpha_v(\cdot)=\omega(v,\cdot)$$ provides a contact structure on $\mathbb{P}(V)$ Contact structures are defined above as follows: Let $X$ be a complex manifold of dimension $2n-1$ a contact structure on $X$ is determined by the local datum of a holomorphic $1$ -form $\alpha$ which is well defined up to multiplication by an invertible holomorphic function and which satisfies the condition that the $(2n-1)$ -form $$\alpha\wedge (d\alpha)^{n-1}\in K_X$$ does not vanish at any point.","['hodge-theory', 'complex-geometry', 'differential-geometry']"
4689745,"If you have an inflection point, is there a three-term A.P. in the domain that the function maps to a three-term A.P.?","Let $f:\mathbb{R}\to\mathbb{R}$ be a twice-differentiable function with a point of inflection
at $\ x=0.\ $ Specifically, $\ f''(0)=0,\ $ and there exists $\ b>0, c>0\ $ such that $\
 f''(x) < 0\ $ on $\ (-b,0),\ $ and $\ f''(x) > 0\ $ on $\ (0,c).\ $ Does there exist $\ a<0,\ d > 0,\ $ such that $\ f(a), f(a+d),
 f(a+2d)\ $ forms an arithmetic progression, in particular, $\ f(a+d) -
 f(a) = f(a+2d) - f(a+d)\ ?$ I think the answer is yes. In particular, I think there exists $\ \mu < 0\ $ such that the result is true for every $\ a\in [\mu,0).\ $ However, I don't know how to prove this. Instinctively I think we can apply the Intermediate Value theorem on $\ f'(x),\ $ but I'm not sure how to implement this.","['mean-value-theorem', 'calculus', 'derivatives', 'real-analysis']"
4689789,There exists an element in a group whose order is at most the number of conjugacy classes,"Suppose $G$ is a finite group and there are $c$ conjugacy classes. I want to show that there is a non-identity element whose order is at most $c$ . I've tried using the class equation and seeing if there's a divisibility argument. But I'm not sure where to go from there. I also know conjugacy classes consist of elements whose orders are equal, so a contradiction argument might be possible. I would appreciate any hints.","['group-theory', 'finite-groups']"
4689809,Calculating $\;\int_{-\infty}^{\infty}\frac{1}{(e^x-1-x)^2+4\pi^2}~dx$,"I would like to calculate the following integral: $\displaystyle\int_{-\infty}^{+\infty}\frac{1}{\big(e^x-1-x\big)^2+4\pi^2} ~dx$ My attempts: I have tried to use the complex analysis, in particular the Cauchy’s residue theorem which is a powerful tool to evaluate line integrals of analytic functions over closed curves, but the denominator $\big(e^x-1-x\big)^2+4\pi^2=\big(e^x\!-\!1\!-\!x\!+\!2\pi i\big)\big(e^x\!-\!1\!-\!x\!-\!2\pi i\big)$ has infinitely many complex zeros, moreover I cannot get their exact values, but only an approximation of them. I have also tried to put a parameter $b$ in the integral: $I(b)=\displaystyle\int_{-\infty}^{+\infty}\frac{b}{\left(e^x-1-x\right)^2+b^2}~\mathrm dx$ and find a relation between $I(b)$ and its derivative $I’(b)$ , but in this way I get $I’(b)=\dfrac1bI(b)-\displaystyle\int_{-\infty}^{+\infty}\frac{2b^2}{\left[\left(e^x-1-x\right)^2+b^2\right]^2}~\mathrm dx$ and I do not know how I can solve it in order to obtain $I(b)$ . Another attempt I made is to write the integrand function as a series and then integrate it, but I did not even manage to do it. By using numerical methods I got that the result is approximately $\,0.33333\,,$ but I would like to obtain the result exactly which should be $\dfrac13\,.$ Could anyone give me a hint to calculate the integral ?","['improper-integrals', 'definite-integrals', 'real-analysis']"
4689822,Finding the UMVUE for $g(\lambda) = \lambda \cdot \exp(-\lambda)$,"Given the random sample $X_1, \ldots, X_n$ of the random variable $X \sim \textrm{Poisson}(\lambda)$ , find the UMVUE for $g(\lambda) = \lambda \exp(-\lambda)$ . We know through the exponential family that $\sum X_i$ is a sufficient and complete statistic for $\lambda$ . Define $W(X)$ as $$
W(X) =
\begin{cases} 
1, & \text{if $X_1 = 1$} &&  \\
0, & \text{otherwise} 
\end{cases}
$$ Then $$ E[W(X)] = \sum_{w = 0}^{\infty} w P(W = w) = 1 \cdot P(X_1 = 1) = \lambda e^{-\lambda}  $$ Using Lehmann-Scheffé $$ T^{*}(X) = E[W(X) | T(X)] = \sum w P(W(X) = w  |  T(X) = t ) = P(X_1 = 1 | \sum_{i = 1}^{n} X_i )$$ we obtain $$ \lambda \left(\frac{n-1}{n} \right)^{\sum_{i = 1}^{n} X_i} $$ Is the above reasoning correct?","['statistical-inference', 'statistics']"
4689863,Is my proof for the single variable chain rule correct?,"I have studied the proof for the Chain Rule given by Spivak in Calculus but I found the argumentation convoluted, so I tried proving the Chain Rule on my own. I feel like what I've written is watertight but I figured I'd ask here to see if some people stronger than me in math could find any issues with it. Begin with the statement $\lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h}$ . Add and subtract $g(a)$ so that we get: $=\lim_{h \to 0} \frac{f(g(a)+g(a+h)-g(a))-f(g(a))}{h}$ Assume that the function $g$ is not locally constant. If it is locally constant then by default, $\frac{d}{dx}f(g(x)) = 0$ . Then, we multiply by $\frac{h}{h}$ inside the function to get: $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h}$ Then multiply the whole limit expression by $1$ again with $\frac{\frac{g(a+h)-g(a)}{h}}{\frac{g(a+h)-g(a)}{h}}$ : $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\frac{g(a+h)-g(a)}{h}$ Since the limit of a product of functions is the product of the limits of each function, $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\lim_{h \to 0}\frac{g(a+h)-g(a)}{h}$ . The right hand expression is just $g'(a)$ . The left hand expression remains to be evaluated. Here I introduce the secant slope function as well as the derivative: $\phi(a, h) = \begin{cases} \frac{g(a+h)-g(a)}{h} & h \neq 0 \\ g'(a) & h = 0\end{cases}$ Substituting, $\frac{d}{dx}f(g(x)) = [\lim_{h \to 0} \frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)}]g'(a)$ Proceed by evaluating the remaining limit expression. Call it $L$ , and write a $\delta-\epsilon$ statement of the existence of the limit: $\forall\epsilon>0, \exists\delta>0: 0<|h|<\delta \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon$ We massage the $\delta$ expression by multiplying it by $|\phi(a, h)|$ : $\forall\epsilon>0, \exists\delta>0: 0<|h\phi(a, h)|<\delta|\phi(a, h)| \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon$ The point of this was to construct a new $\delta^*$ and allow a substitution to do some cleanup work: $k = h\phi(a, h)$ , so that we construct a derivative expression: $\forall\epsilon>0, \exists\delta^*>0: 0<|k|<\delta^* \implies |\frac{f(g(a)+k) - f(g(a))}{k} - L|<\epsilon$ Clearly, $L = f'(g(a))$ . So now I can write the chain rule: $\lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h} = f'(g(a))g'(a)$ . Or, $\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)$ I will greatly appreciate any feedback on this!","['calculus', 'solution-verification', 'derivatives']"
4689889,"In general, will the ILP be solved faster if the number of variables is smaller?","This question might be a little bit vague. Suppose I have a variable $x$ in an ILP formulation such that $x$ can choose $\{0,1,2,3,4\}$ . Now I use four binary variables to replace $x$ , ( $x_1+x_2+x_3+x_4$ to replace $x$ ). Will the new ILP be solved slower than the original one because we have more variables? In general, is there any relationship between the speed of solving ILP and the number of variables or the number of constraints? Thanks!","['discrete-optimization', 'integer-programming', 'discrete-mathematics', 'linear-programming']"
4689920,Upper bounds on the Stirling approximation of binomial coefficients,"The Stirling approximation for binomial coefficients gives $$
\log \binom{n+k}{k} \leq (n+k) \log (n+k) - n \log n - k \log k =: f(n, k) \text{.}
$$ I want to obtain a nice upper bound on the right-hand side $f(n, k)$ to make it easier to handle (to be more specific, I want to use the resulting upper bounds to evaluate the integral $\int_0^1 \sqrt{f(n, 1/x)} \, dx$ ). A naive approach would be the following.
Setting $g(x) = x \log x$ for $x > 0$ , we can rewrite $$
f(n, k) = g(n + k) - g(n) - g(k) \text{.}
$$ Noting that $g$ is a convex function, we can use the Jensen inequality to get $$
g(n) + g(k) \geq 2 g\left( \frac{n + k}{2} \right) = 
2 \cdot \frac{n + k}{2} \log \left( \frac{n + k}{2} \right) = 
g(n + k) - (n + k) \log 2
\text{.}
$$ Therefore we have $$
f(n, k) \leq (n + k) \log 2 \text{.}
$$ However, this is too crude: the resulting bound is $$
\binom{n + k}{k} \leq 2^{n+k}
$$ So what we have just done is nothing more than $$
\text{(# of ways to select $k$ items out of $n + k$)}
\leq 
\text{(# of ways to select some items out of $n + k$)}
\text{.}
$$ Question : Is there any sophisticated way of upper-bounding $f(n, k)$ ?","['statistics', 'combinatorics', 'probability']"
4689985,"How can I find all possible values of $a, b \in \mathbb{R}$ such that $\lim_{x \to 1} \frac{ax^2 - 1}{x - 1} = b$?","I am trying to tackle a ""continuity""-related question that goes as ""Let $f$ be the function defined by $$
f(x) = \begin{cases} 
\frac{ax^2 - 1}{x - 1} & \text{if } x \neq 0; \\
b & \text{if } x = 1;
\end{cases}
$$ Find all possible values of $a, b \in \mathbb{R}$ such that $f$ is continuous at $x = 1$ ."" I understand that for $f$ to be continuous at $x = 1$ , its limit as $x \to 1$ has to equate its value at $x = 1$ , that is $$
f \text{ continuous at $x = 1$} \longleftrightarrow \lim_{x \to 1} f(x) = f(1).
$$ Hence, the equation $$
\lim_{x \to 1} \frac{ax^2 - 1}{x - 1} = b
$$ needs to be true. This is where I'm stuck. I know that I need to factor out the denominator $(x - 1)$ so that I may be able to evaluate the limit through direct substitution, but obviously I can't directly factorise the numerator as it is - I understand that I am lacking the knowledge of some property. As this is a past exam paper, I referred to the answer key, and it says: ""In order for $f$ to be continuous at $1$ , $\lim_{x \to 1} f(x)$ must first exist (not be infinite). This forces the numerator to be zero: $$
\lim_{x \to 1} ax^2 - 1 = 0.
$$ Hence, $$
a - 1 = 0 \text{ or } a = 1.""
$$ So, another question I have is, why is the numerator ""forced"" to be zero for the limit to exist at $x = 1$ ? Thank you so much for reading. I would be sincerely grateful to anyone who is able to help.","['limits', 'calculus', 'continuity']"
4690057,Solve the differential equation $(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0.$,"Solve the differential equation $(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0.$ I tried solving the problem like this: We assume $M=(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)$ and $N=(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x}).$ Now, we observe, $\frac{\partial  M}{\partial y}=12x^2+4xy-12y^2+2e^{2x}-e^y$ and $\frac{\partial N}{\partial x}=6x^2y+12x^2-12y^2-e^y+2e^{2x}.$ We find, that if we considered $M=12x^2y+4x^3-4y^3+2ye^{2x}-e^y$ and $N=4x^3-12xy^2+3y^2-xe^y+e^{2x}$ then $\frac{\partial  M}{\partial y}=12x^2-12y^2+2e^{2x}-e^y=\frac{\partial  N}{\partial x}.$ So, we consider or rather write the given equation, as $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+2xy^2dx+2x^3ydy=0.$ Now, we consider the differential equation, $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0.$ This equation is an exact differential equation. The solution is of this differential equation is $$\int (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+\int 3y^2dy=c_1\implies 4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3=c_1.$$ Next we try to find the solution of the differential equation, $2xy^2dx+2x^3ydy=0.$ We write this differential equation as $\frac{dy}{dx}=\frac{2xy^2}{2x^3y}=\frac{y}{x^2}\implies \frac{dy}{y}=\frac{dx}{x^2}.$ On integrating, both sides of $\frac{dy}{y}=\frac{dx}{x^2},$ we obtain, $\int\frac{dy}{y}=\int\frac{dx}{x^2}\implies \log y=-\frac 1x+c_2\implies \log y+\frac 1x=c_2.$ Now, we add the solutions, of both of the differential equation, $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0$ and $2xy^2dx+2x^3ydy=0,$ to get the solution of the given differential equation, $$(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0$$ i.e $$4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c_1+c_2=c.$$ So, the solution of the given differential equation is $4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c.$ It seems, that when I approach the problem like this, we get a solution of the given equation. But is this a legit way? I don't understand, this this way of solving differential equations is justified. Finally, I want to know whether the solution, I calculated is valid or not?","['alternative-proof', 'solution-verification', 'ordinary-differential-equations']"
4690075,"How to ""invert"" the argument of the Heaviside function?","How can I go from $\theta (x-x_0)$ to $\theta(x_0 - x)$ analytically?. In the first, the Heaviside function gives 1 when $x > x_0$ . I need it to give $1$ when $x < x_0$ . Do I explain myself? (1st question in this forum).","['functions', 'dirac-delta']"
4690120,Find the volume by using triple integral,"Calculate the volume of the contents of the bowl $K$ , which is given by $x^2 + y^2 \leq z \leq 1$ . $$x^2+y^2\leq z\leq 1$$ $$x^2+y^2=1\iff y=\pm\sqrt{1-x^2}$$ $$-1\leq x\leq 1$$ $$-\sqrt{1-x^2}\leq y\leq\sqrt{1-x^2}$$ $$\iiint 1~\mathrm{d}x~\mathrm{d}y~\mathrm{d}z=\iint\left(\biggl[z\biggr]_{x^2+y^2}^y\right)~\mathrm{d}x~\mathrm{d}y=\iint\left(-x^2+y^2)~\mathrm{d}y\right)~\mathrm{d}x=\int\biggl[y-yx^2+\frac{y^3}{3}\biggr]_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}=\int\left(\sqrt{1-x^2}-\sqrt{1-x^2}x^2+\frac{\sqrt{1-x^2}^3}{3}\right)-\left(-\sqrt{1-x^2}+\sqrt{1-x^2}x^2-\frac{\sqrt{1-x^2}^3}{3}\right)=\int_{-1}^12\sqrt{1-x^2}~\mathrm{d}x-\int_{-1}^12\sqrt{1-x^2}x^2+\int_{-1}^1\frac{\sqrt{1-x^2}}{3}$$ I don't know if I did right so far but as you can see I got something that is kinda hard to integrate and I'm pretty sure I wasn't meant to solve it this way so can someone please explain?","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'partial-derivative']"
4690200,"How can $\int_0^1 \lfloor ax^2+bx+c \rfloor \,\Bbb dx$ be generalized?","Let $$I = \int_0^1 \lfloor -8x^2+6x-1 \rfloor \,\Bbb dx$$ (where $\lfloor \cdot \rfloor$ represents the floor function/greatest integer function) which on solving gives the value $$I = \frac{\sqrt{17}-13}{8} = -0.876.$$ Solving this integral took me a lot of time (manually graphing the quadratic and breaking it at integral values) and I was wondering if there can be a generalized result possible for, let $a$ , $b$ , $c$ be integers, $$J = \int_0^1 \lfloor ax^2+bx+c \rfloor \,\Bbb dx$$ If evaluating $J$ is not conceptually possible/correct, can there be generalized solution for [1] or [2]? [1] $\displaystyle\int \lfloor ax^2+bx+c \rfloor \,\Bbb dx$ [2] $\displaystyle\int_n^m \lfloor ax^2+bx+c \rfloor \,\Bbb dx$ ( $m$ and $n$ are integers)","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4690296,Importance of Cameron-Martin space for abstract Wiener space,"I have been reading a wikipedia entry on abstract Wiener space where they give as a motivation the necessity to define $$
\frac{1}{Z}\int_H f(v) e^{-\frac{1}{2} \Vert v\Vert^2} Dv, \tag{1}
$$ which they say is an integral often met in physics. Here $H$ is some Hilbert space. The problem apparently is that it is hard to introduce a suirable measure $Dv$ on such space. For example, if one follows a construction of cylinder measures, one can get a nice behaving Gaussian set function, but only on an algebra, and it does not extend well to the $\sigma$ -algebra generated by those cylinder sets. Please correct me if my understanding is wrong here. Instead an idea is to consider a Banach space such that $i:H \to B$ makes $i(H)$ dense in $B$ and yet $B$ is sufficiently large so that the very same (?) cylinder set procedure with measure actually yeilds a nice behaving Gaussian measure $\gamma$ on the whole Borel $\sigma$ -algebra of $B$ . This is apparently a procedure authored by Leonard Grass, who also mentions something like: it is acutally $H$ that $\gamma$ very much depends on, not $B$ . This space $H$ is referred to as the Cameron-Martin space. I am very much lost here and hope that you could help my clarify understanding of this fact. My familiarity is with the classical Wiener space, let's say a Brownian motion law $\gamma_W$ on the Banach space $B_W = C_0(\Bbb R_+,\Bbb R)$ of all continuous function from $\Bbb R_+$ to $\Bbb R$ that start at $0$ . Such construction was done using finite-dimensional distributions, Kolomogorov's extensions theorem and Kolmogorov's continuity theorem. An in particular it so happened that $\gamma_W(C_0^1) = 0$ , i.e. a probability of getting a function with continuous derivative is $0$ . Yet, in the wikipedia article one uses $H_W = L^{2,1}_0(\Bbb R_+, \Bbb R)$ which is the Hilbert space of all continuously differentiable square integrable functions starting at $0$ with the inner product being $$
\langle \sigma_1, \sigma_2 \rangle_{L_0^{2,1}} := \int_0^\infty \dot{\sigma}_1 (t) \dot{\sigma}_2 (t) \, dt.
$$ Now what I don't understand at all is: How exactly does $H_W$ help in constructing $\gamma_W$ ? I mean, in the end we'll even end up having $\gamma_W(H_W) = 0$ . How can it matter at all then? In the wikipedia article they use intergal $(1)$ as a motivation to formally define measures on Hilbert spaces. In the end the defined measure gives $0$ weight to $H$ , casting all integral to be $0$ as well. How was this useful then for the original problem?","['stochastic-processes', 'probability-theory', 'functional-analysis', 'measure-theory']"
4690318,Why doesn't this short exact sequence of sheaves split?,"I feel this is somewhat naïve but it's always good to clear up confusion. Let $X$ be a topological space, $Z$ a closed subspace, $U$ its complementary open, $i$ the inclusion of $Z$ into $X$ and $j$ the inclusion of $U$ into $X$ . For a sheaf $\mathcal{F}$ on $X$ , there is a well-known short exact sequence $$0\to j_!(\mathcal{F}|_U)\to \mathcal{F}\to i_*(\mathcal{F}|_Z)\to 0.$$ This doesn't split in general, so what is wrong with the following argument? We can define a surjective map of sheaves $\mathcal{F} \to j_!(\mathcal{F}|_U)$ by defining a map on the stalks which is an isomorphism over $U$ and zero otherwise.  The kernel of this map is a sheaf on $X$ supported on $Z$ , whose stalks are isomorphic to those of $X$ over $Z$ and zero otherwise, which implies that the kernel is $i_*(\mathcal{F}|_Z)$ . Thanks in advance!","['homological-algebra', 'algebraic-geometry', 'sheaf-theory']"
4690359,Consider the triangle ABC in which $AC(AB+ AC)={BC}^{2}$ Show that angles $BAC = 2\cdot ABC$. [duplicate],"This question already has answers here : Finding a triangle angle based on side length equality (2 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Consider the triangle ABC in which $AC(AB+ AC)={BC}^{2}$ Show that angles $BAC = 2\cdot ABC$ . MY IDEAS MY DRAWING So I processed the equality that was given $AC(AB+ AC)={BC}^{2}$ $AC=\frac{{BC}^{2}}{AB+AC}$ As you can see, I put a point AA' that bisects the angle CAB. I will apply the bisector theorem in triangle CAB with the bisector AA'. $\frac{BA'}{A'C}=\frac{AB}{AC}$ $\frac{BA'+A'C}{A'C}=\frac{AB+AC}{AC}$ $\frac{BC}{A'C}=\frac{AB+AC}{AC}$ $AB+AC=\frac{BC\cdot AC}{A'C}$ Then we are switching $AB+AC$ with what we discover it equals. ${AC}^{2}\cdot BC={BC}^{2}\cdot A'C$ $AC=\sqrt{BC\cdot A'C}$ This seems like the reciprocal of the leg theorem.
I don't know what to do forward. All ideas are welcome. Hope one of you can help me. Thank you!","['geometry', 'fractions', 'triangles', 'radicals', 'bisection']"
4690392,How can we reconcile the exponential function with the fundamental theorem of algebra?,"For the purposes of this question let's consider the exponential function to be an infinite polynomial (the limit as $n$ goes to infinity of an $n$ th degree polynomial). It stands to reason then that we might expect an infinite polynomial to have infinitely many roots, and this does hold for many other infinite series like the sine and cosine. Let us define: $$\exp_n(x) = \sum_{k=0}^n \frac{x^k}{k!}$$ so that we have $\exp(x) = \exp_{\infty}(x)$ . All $\exp_n(x)$ for finite $n$ are $n$ th-degree polynomials with $n$ -complex roots, so  let us write: $$\exp_n(x) = \frac{1}{n!} \prod_{i=1}^n (x-r_i)$$ My question is this: as $n$ $\to$ $\infty$ , what happens to these $r_i$ ? Numerically I have determined that only odd $n$ have real roots, and as we take the odd partial sums of the exponential function, the real root gets closer and closer to $-\infty$ , which concurs with what I expect. But how do we account for the multiplicities? Do all complex roots of the partial sums go to $-\infty$ , and if so, how can we show that (bonus points for a visualization if possible)?","['complex-analysis', 'taylor-expansion', 'sequences-and-series']"
4690424,Using a rotation matrix to rotate the graph of a function?,"I understand where the following matrices come from and how they geometrically rotate a point (x, y) counterclockwise/clockwise by $θ$ $\begin{bmatrix}\cosθ&-\sinθ\\\sinθ&\cosθ\end{bmatrix} $ for counterclockwise and $\begin{bmatrix}\cosθ&\sinθ\\-\sinθ&\cosθ\end{bmatrix} $ for clockwise For example, I want to rotate the graph ${y} = x^{2}$ by 45° counterclockwise. By appying this matrix to any point (x, y), $\begin{bmatrix}\cos45°&-\sin45°\\\sin45°&\cos45°\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y\\\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y\end{bmatrix}$ So, $x$ moves to $(\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y)$ and $y$ moves to $(\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y)$ If I plug these values into ${y} = x^{2}$ , it yields the following graph: As you can see, despite plugging in the counterclockwise-rotated points, the graph was rotated clockwise by 45°. So my question is: Why does plugging in the counterclockwise-rotated points result in a graph that is rotated clockwise? I know that in order to rotate the graph counterclockwise you just have to input the points rotated counterclockwise, but I am having trouble understanding why, conceptually.","['graphing-functions', 'matrices', 'linear-algebra', 'linear-transformations', 'rotations']"
4690455,How to find residue at $z=0$ for $f(z) = \frac{e^{1/z}}{z+a}$,"Let $a \in \mathbb C$ be a complex number, and consider the function $$ f(z) = \frac{e^{1/z}}{z+a}$$ Compute the residues of $f(z)$ at each of its singularities. So I know there are singularities at $z=0$ and $z=-a$ , and I know $\text{Res}(f(z), z_0) = g(z_0) / h'(z_0)$ can be used for the singularity at $z=-a$ to find the residue to be $e^{-1/a}$ , but am not sure how to compute the residue at $z=0$ . Any help would be appreciated.","['complex-analysis', 'residue-calculus', 'singularity']"
4690556,Show that $\Phi(n) = \phi(1) +\phi(2) + \dots + \phi(n) =\frac{n^2}{2 \zeta(2)}+O(n \log n)$,"Set $\Phi(n) = \phi(1) + \phi(2) + \cdots + \phi(n)$ .
I want to understand why $$
\Phi(n) = \frac{n^2}{2\zeta(2)} + O(n\log n).
$$ I am going to show the proof that I have, and I would be very grateful if someone could help me to express this in terms of big $O$ notation. Thank you. Using the formula $$
\phi(n)=n \sum_{d \mid n} \frac{\mu(d)}{d},
$$ we have \begin{align*}
\Phi(n) = \phi(1) +\phi(2) + \phi(3) + \dots + \phi(n) & =\sum_{m=1}^n \left(m  \sum_{d \mid m} \frac{\mu(d)}{d}\right) = \sum_{m=1}^n \left(\sum_{dd^{\prime}=m} d^{\prime} \mu(d) \right)\\
    &=\sum_{d d^{\prime} \leqslant n} d^{\prime} \mu(d) =\sum_{d=1}^n \left(\mu(d) \sum_{d^{\prime}=1}^{[n / d]} d^{\prime}\right)
    \\ &=\sum_{d=1}^n \mu(d)\left(\dfrac{\left[\frac{n}{d}\right]^2+\left[\frac{n}{d}\right]}{2}\right)\\
    &=\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left(\left[\frac{n}{d}\right]^2+\left[\frac{n}{d}\right]\right)\\
    &=\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left[\frac{n}{d}\right]^2+\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left[\frac{n}{d}\right]
    \end{align*} Remember the definition. Writing the integer part $\left[x\right]$ = $x - \{x\}$ , where $\{x\}$ is the fractional part, we obtain. \begin{align*}
&=\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left[\frac{n}{d}\right]^2 + \dfrac{1}{2}\\
&=\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left(\dfrac{n^2}{d^2} + O(\dfrac{n}{d}) \right) + \dfrac{1}{2}\\
 &= \dfrac{1}{2}\sum_{d=1}^n \mu(d)\left(\dfrac{n^2}{d^2}\right) + \dfrac{1}{2}\sum_{d=1}^n \mu(d)O(\dfrac{n}{d}) + \dfrac{1}{2}\\
&=\dfrac{1}{2}\sum_{d=1}^n \mu(d)\left(\dfrac{n^2}{d^2}\right) + O(n \sum_{d=1}^n \dfrac{1}{d}) + \dfrac{1}{2}\\
&=\dfrac{n^2}{2}\sum_{d=1}^n \dfrac{\mu(d)}{d^2} + O(n \log n) + \dfrac{1}{2}\\
&=\dfrac{n^2}{2}\sum_{d=1}^\infty \dfrac{\mu(d)}{d^2} + O\left(n^2 \sum_{d=n+1}^\infty \dfrac{\mu(d)}{d^2} \right)+ O(n \log n) + \dfrac{1}{2}\\
&=\dfrac{n^2}{2 \zeta(2)} + O(n) + O(n \log n) + \dfrac{1}{2}
\end{align*} I can't understand how I can express these terms in Big O, that is, I don't understand how it works. Below is the demonstration of the book that I used, but I want to better explain each step, so I did the above. https://www.chrishenson.net/static/article_files/zeta/hardy_intro.pdf Theorem 330 I hope someone can help me, thank you very much","['number-theory', 'mobius-function', 'asymptotics']"
4690584,Gaining intuition for a property of linear functions: why is $f((1-\lambda)a+\lambda b) \equiv (1-\lambda)f(a)+\lambda f(b)$?,"I am told that if we have some linear function $f$ defined over an interval $[a,b]$ , then the fact that $f$ is linear implies that , for all $\lambda$ between 0 & 1 exclusive, the following property holds: $$f((1-\lambda)a+\lambda b) \equiv (1-\lambda)f(a)+\lambda f(b)$$ Why is this the case? How can I see that the two expressions are equivalent to each other? My issue isn't in understanding what's being said here, but rather in understanding why it's true . For context, I am trying to understand the definition of concave & convex functions, and this property is given as a minor step in the build up towards the definition, with no further elaboration. EDIT: In the answer below I am told that this property is taken as the definition of a linear function.. but in the resource I'm using, it tells me that it is due to $f$ being a linear function that this property holds, so I feel quite confused. Wouldn't it be possible to show that this property is implied by a more immediately intuitive definition?
Am I thinking about this in the wrong way? How should I view this property/definition? Any help in clearing up my confusion would be greatly appreciated.","['calculus', 'functions', 'intuition']"
4690635,Proof of Lemma 3.49 in the Diamond-Darmon-Taylor Fermat's Last Theorem Notes,"We start with a semistable elliptic curve $E/\mathbb Q$ such that such that its mod 3 representation $\overline{\rho}_{E,3}$ is reducible. We wish to show that $\overline{\rho}_{E,5}$ is modular. This follows immediately from the following lemma from the DDT notes: Lemma 3.49 : There is a semistable elliptic curve $A/\mathbb Q$ such that $A[5] \simeq E[5]$ as $G_\mathbb Q$ -modules. $A[3]$ is an irreducible $G_\mathbb Q$ -module. Proof: Let $Y'(5)$ be the $\mathbb Q$ -curve classifying elliptic curves $A$ together with an isomorphism $E[5] \simeq A[5]$ compatible with Weil pairings. Elliptic curves over $\mathbb Q$ satisfying 1) correspond to points in $Y'(5)(\mathbb Q)$ . Adjoining finitely many points to $Y'(5)$ gives its compactification $X'(5)$ which is a twist of $X(5)$ . We know that $X(5)$ has genus $0$ as a $\mathbb C$ -curve. Since $X'(5)$ has a point $x_0$ over $\mathbb Q$ corresponding to $E$ , it is isomorphic to $\mathbb P^1$ over $\mathbb Q$ . The rational points of $Y'(5)$ therefore give a large supply of elliptic curves satisfying 1). Consider the curve $Y'(5,3)$ classifying elliptic curves $A$ satisfying 1) and equipped with a subgroup of order 3. One quickly checks that its compactification $X'(5,3)$ has genus $> 1$ and hence has only finitely many rational points by Faltings' Theorem. Hence only finitely many points in $Y'(5)(\mathbb Q)$ are in the image of $Y'(5,3)(\mathbb Q)$ under the natural map $Y'(5,3) \to Y'(5)$ . Hence for all but finitely many $x \in Y'(5)(\mathbb Q)$ , the corresponding $A$ satisfies 2) since it has no rational subgroup of order 3. Choose $x$ arbitrarily close in the 5-adic topology to $x_0$ to find an elliptic curve $A$ (associated to $x$ ) which is semistable and satisfying 1) and 2). Hence the Lemma. I fail to understand the last line of the proof: why does choosing $x$ 5-adically close to $x_0$ give us a semistable elliptic curve $A$ with the desired properties? What does the 5-adic topology have to do with anything? Correct me if I am wrong here: it seems we have established a) and b) in the preceding lines of the proof, so it seems to me that this 5-adic topology argument must be being used to establish semistability. Is this true? If so, how/why is this true? If not, where am I going wrong? I have seen this exact argument given in other sources, and I feel it bears at least a little explanation.","['elliptic-curves', 'number-theory', 'proof-explanation', 'representation-theory', 'modular-forms']"
4690657,canonical divisor from canonical sheaf,"Let $X$ be a complete intersection of $m$ hypersurfaces in $P^n$ over some field $k$ . I have computed that the canonical sheaf is $\omega_X=\mathcal{O}_X(\sum d_i-n-1)$ , where $d_i$ are the degrees of the hypersurfaces. I also know that the canonical divisor of a hypersurface $Y$ is $K_Y=(d-n-1)H$ if $Y$ is a hypersurface of degree $d$ . How should I compute the canonical divisor of a complete intersection of hypersurfaces? It is probably something simple that I didn't observe. Thanks in advance.","['divisors-algebraic-geometry', 'algebraic-geometry', 'geometry', 'projective-space']"
4690677,Maximum of $\sum_{t = 0}^s \binom{m}{t} p^t (1-p)^{m-t}$?,"Let $p \in (\epsilon, 1]$ where $\epsilon > 0$ . Then for some natural $m,s$ s.t $s < \frac{m}{2}$ , let $$f(p) = \sum_{t = 0}^s \binom{m}{t} p^t (1-p)^{m-t}  $$ Then is the maximum of $f$ at $p = \epsilon$ ? I tried to show that $\frac{d}{dp} f(p) < 0$ , but all the terms in the remaining summation are not negative and got stuck there. I even observed that $\sum_{t = 0}^s \binom{s}{t} p^t (1-p)^{m-t} = (1 - p)^{m-s}$ , which is decreasing wrt p; additionally $f(p) = \sum_{t=0}^s \frac{\binom{m}{t}}{\binom{s}{t}} \cdot \Big( \binom{s}{t} p^t (1-p)^{m-t} \Big) $ , but can't proceed from here. I don't know if the result is true or if I have misinterpreted something from the paper I am reading. Can someone help me?","['optimization', 'functions']"
4690832,How to prove that the $\angle CED$ of the triangle below is equal to $\frac{1}{2} \angle\alpha\;?$,"Here is the whole problem: In the triangle $ABC$ , it is known that $AC > AB$ , and the angle at the vertex $A$ is equal to $\alpha$ . On the side $AC$ , point $M$ is marked so that $AB=MC$ . Point $E$ is the midpoint of the segment $AM$ , point $D$ is the midpoint of the segment $BC$ . Find the angle $CED$ . The textbook answer to the problem is that $\angle CED = \dfrac{\alpha}{2}$ . The tip on how get to it is to mark a point $K$ on the $AB$ such, that $AK = KB$ , draw the midline $KD$ of the triangle $ABC$ and then prove that the $KDE$ triangle is isosceles. At that point I've tried everything(even chapgpt, which, as it turned out, is bad at math), the best I've gotten so far is if I mark a midpoint F on the $CM$ , then I get parallelogram $EKDF$ $\bigg(KD = \dfrac{1}{2}AC = EF$ , and $KD\parallel AC\bigg)$ . And triangles $KDE$ and $FED$ are congruent, but it's not even close to what I need to prove. How would you do that ? Here is the final figure: Please, take into account that this is a problem from an $\mathbf{8^{th}}$ grade math textbook, the topic is ""Midline of a triangle"", which means I'm not allowed to use any angle functions or similar features that were not introduced yet in the curicullumn. EDIT : The textbook's hint seems to be wrong, thanks @Vasili for the solution. The actual triangle that needs to be proven to be isosceles is another one, not the $KDE$ .","['triangles', 'proof-writing', 'geometry']"
4690833,Expected number of non-uniform draws until collision?,"Edit May 9 -- high-level summary of the issue here . $R$ gives a good proxy for estimating collision time, with a slight undercount. Random matrices and graphs give distributions with longer time until collision than what you'd expect by looking at their values of $R$ Suppose I do IID draws from multinomial distribution with probabilities $p_1,\ldots,p_n$ . Is there a nice approximation for the $x(p)$ , the expected number of draws from $p$ until collision, ie, drawing some $i$ more than once? In the case of $p_1=p_2=\dots=p_n$ , this was shown to be the following $$x(p)=\sqrt{\frac{\pi n}{2}}$$ From simulations , the following appears to be a lower bound $$x(p)\approx \sqrt{\frac{\pi R}{2}}$$ where $R=\frac{1}{\|p\|^2}$ and $\|p\|^2=p_1^2+\ldots+p_n^2$ is the probability of observing a collision in two IID draws from $p$ . Distributions were taken to be of the form $p_i\propto i^{-c}$ with $c$ varying","['expected-value', 'multinomial-distribution', 'probability']"
4690903,Reference request: When is the set of critical values of $f : \mathbb{R}^n \mapsto \mathbb{R}^m$ a countable set?,"Given a continuously differentiable function $f : U \subset \mathbb{R}^n \mapsto \mathbb{R}^m$ (where $U$ is an open set), we say that $x$ is a critical point of $f$ if $Df(x)$ is not full rank. Let $C_f$ represent the set of all critical points of $f$ . From Sard's theorem, we know that when $f$ satisfies certain regularity conditions, then $ f(C_f) $ has measure $0$ in $\mathbb{R}^{m}$ . A generalisation of this result states that (again, under suitable regularity) if $ A_r = \{ x \in \mathbb{R}^n \mid \text{rank}Df(x) < r \} $ , then the Hausdorff dimension of $ f(A_r) $ is at most $r$ , but it can be arbitrarily close to $r$ . My question is, (*) what 'nice' functions are known to have the property that $ f(C_f) $ is actually always a countable set? For example, I think that this (*) would hold when $f : U \mapsto \mathbb{R}^m $ is analytic and $C_f \neq U$ . Also, in the case when $m=1$ and $C_f$ is a connected set, although we might expect that $f$ will be constant on $C_f$ , Whitney constructed a real-valued $C^1$ function of $2$ variables such that $C_f$ is an arc and $f(C_f)$ is not a constant and in fact contains an open set of $\mathbb{R}$ . The same paper mentions that when $f$ is 'smooth enough', then "" $f$ must be constant on any connected critical set, as shown by
M. Morse and A. Sard in an unpublished paper "". But in my view, this shows that it is quite difficult to come up with such functions and I would expect 'generic' functions (in some appropriate sense) to be constant on each connected set of critical points, even given lower regularity. Any thoughts or references regarding this are welcome!","['multivariable-calculus', 'differential-topology', 'reference-request']"
4690933,Maximum and minimum value of an algebraic expression when provided corresponding equation,"My mathematics textbook lists the following example: If $\ a,b,c\in ℝ\ $ and $\ a^2+b^2+c^2=1\ $ then the minimum and maximum value  of $\ ab+bc+ca\ $ is: (A) $\ \frac{1}{2}, 1\ $ (B) $\ 1, 4\ $ (C) $\ \frac{1}{2}, \frac{3}{2}\ $ (D) $\ -\frac{1}{2}, 1\ $ To which, it says: Given, $\ a^2+b^2+c^2=1$ We know that $\ (a+b+c)^2\geq0\ $ $\ \implies a^2+b^2+c^2+2ab+2bc+2ca\geq0$ $\ \implies 1+2(ab+bc+ca)\geq0$ $\ \implies ab+bc+ca\geq\frac{-1}{2}$ Also, $\ (b-c)^2+(c-a)^2+(a-b)^2\geq0\ $ $\ \implies 2(a^2+b^2+c^2)-2(ab+bc+ca)\geq0$ $\ \implies 2(1)\geq2(ab+bc+ca)$ $\ \implies ab+bc+ca\leq1$ $\therefore\ $ (D) $\ -\frac{1}{2}, 1\ $ is the correct answer. However, I fail to understand how that puts us in the confidence that no other interval exists for $\ ab+bc+ca\ $ . Simply put, just like simplifying $\ (a+b+c)^2\ $ and re-factoring $\ (b-c)^2+(c-a)^2+(a-b)^2\ $ yield two different inequality expressions, how do we know that factoring some other way won't result in an interval that might affect the maximum or minimum possible value?","['maxima-minima', 'algebra-precalculus']"
4690945,Verify the solution to a Bessel's equation has finite length or not,"Consider the following Bessel's equation $$\ddot{x}(t) + \frac{3}{t}\dot{x}(t) + x(t) = 0$$ with initial condition $x(0)=1$ and $\dot{x}(0)=0$ . From WolframAlpha, we know the solution is given by $$x(t) = \frac{2J_1(t)}{t}$$ where $J_1(t)$ is the Bessel's function of the first kind with degree $1$ . It is easy to see that $x(t)\to 0$ as $t\to\infty$ . My question is that can we prove the trajectory of the solution $x(t)$ has finite length, i.e., $$\int_0^\infty |\dot{x}(t)| dt <\infty$$ I find that the trajectory is oscillating, so it seems that the trajectory length is pretty long, but since we have extra $1/t$ factor, maybe it makes the solution converge fast enough so that the length is indeed finite. Any ideas to prove or disprove would be helpful. My trial: \begin{align}
\int_0^\infty |\dot{x}(t)| dt &= \int_0^\infty \left|\frac{2J_1'(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt \\
&= \int_0^\infty \left|\frac{J_0(t)-J_2(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt
\end{align} Thus, a sufficient condition for the above quantity to be finite is $$\frac{J_0(t)}{t},\; \frac{J_2(t)}{t} \text{ and } \frac{J_1(t)}{t^2} \text{ are integrable on } \mathbb{R}_+.$$ From WolframAlpha I know $|J_1(t)|\leq 1$ for all $t\in\mathbb{R}_+$ , so the last term above is obviously integrable. It suffices to check the first two are integrable. I guess $J_0(t)$ and $J_2(t)$ will have similar behavior at infinity, but I don't know how that implies integrability of $J_0(t)/t$ and $J_2(t)/t$ .","['stability-in-odes', 'ordinary-differential-equations', 'bessel-functions']"
4690961,"In characteristic $2$, can a function have a non-zero second derivative?","Let $\mathbb F$ be a field with characteristic $2$ , and with a non-trivial absolute value. (The simplest example is $\mathbb F_2(T)$ , the field of formal rational expressions with coefficients in $\mathbb F_2$ . The absolute value of a polynomial is $|p(T)|=2^{-n}\in\mathbb R$ , where $p(T)$ has a factor of $T^n$ but not $T^{n+1}$ .) Let $f:\mathbb F\to\mathbb F$ be a function. Limits and derivatives are defined as usual: $\lim_{x\to a}f(x)=b$ means that for any real (or rational) $\varepsilon>0$ there exists $\delta>0$ such that, for all $x\in\mathbb F$ where $0<|x-a|<\delta$ , $|f(x)-b|<\varepsilon$ . And $$f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}.$$ Is it possible that $f''(a)$ exists in $\mathbb F\setminus\{0\}$ ? If $f=g\cdot h$ where $g$ and $h$ are twice differentiable, the product rule gives $$f''=g''h+2g'h'+gh''$$ $$=g''h+gh''.$$ If $f=g\circ h$ , the chain rule gives $$f''=(g''\circ h)\cdot h'^2+(g'\circ h)\cdot h''.$$ Any polynomial function $p(x)=\sum a_nx^n$ has second derivative $p''(x)=\sum a_nn(n-1)x^{n-2}=0$ because $n(n-1)$ is always even. The derivatives of the reciprocal $f(x)=1/x$ are $f'(x)=-1/x^2$ and $f''(x)=2/x^3=0$ . It follows from the above properties that any rational function $f(x)=p(x)/q(x)$ also has $f''(x)=0$ . The square root function satisfies $f(x)^2-x=0$ ; differentiating this gives $-1=0$ , so it must not be differentiable. I'm not sure about general algebraic functions. I thought of using the symmetric second derivative formula, but I don't think it's valid: $$f''(a)\overset?=\lim_{x\to0}\frac{f(a+x)-2f(a)+f(a-x)}{x^2}=\lim_{x\to0}\frac{0}{x^2}$$ Well, let's apply the definition directly and see what happens: $$f''(a)=\lim_{x\to0}\frac{f'(a+x)-f'(a)}{x}$$ $$=\lim_{x\to0}\frac{\lim_{y\to0}\frac{f(a+x+y)-f(a+x)}{y}-\lim_{y\to0}\frac{f(a+y)-f(a)}{y}}{x}$$ $$=\lim_{x\to0}\lim_{y\to0}\frac{f(a+x+y)-f(a+x)-f(a+y)+f(a)}{xy}$$ We can't take $y=x$ here to get $f''=0$ , because it's not a single limit $(x,y)\to(0,0)$ but two nested limits. An equivalent description of differentiability is that there's some continuous function $h$ with $h(0)=0$ such that $$f(a+x)=f(a)+xf'(a)+xh(x).$$ So the second derivative is $$f''(a)=\lim_{x\to0}\lim_{y\to0}\frac{(x+y)h(x+y)-xh(x)-yh(y)}{xy}$$ $$=\lim_{x\to0}\lim_{y\to0}\left(\frac{h(x+y)-h(x)}{y}+\frac{h(x+y)-h(y)}{x}\right)$$ $$=\lim_{x\to0}\left(\lim_{y\to0}\frac{h(x+y)-h(x)}{y}+\frac{h(x)}{x}\right)$$ $$=\lim_{x\to0}\left(h'(x)+\frac{h(x)}{x}\right).$$ Note that all of these limits must exist, in order for $f''(a)$ to exist. But we can't simplify this further, since $h'$ may be discontinuous or undefined at $0$ . We may even consider the special case where $f$ has a $2$ nd-order Peano derivative: there's some continuous function $g$ with $g(0)=0$ such that $$f(a+x)=f(a)+xf'(a)+x^2f^\ddagger(a)+x^2g(x).$$ Then the second derivative is $$f''(a)=2f^\ddagger(a)+\lim_{x\to0}xg'(x).$$ Is it possible that this limit exists in $\mathbb F\setminus\{0\}$ ?","['field-theory', 'derivatives', 'analysis', 'positive-characteristic']"
4691026,"Proof that $L^2 (Ω, F, P)$ is a Hilbert space","I'm working on the probabilistic space of square random variables ( $L^2 (Ω, F, P)$ ). I need to prove that it is a Hibert Space (the exercise doesn't say with which norm).
The usual inner product is $<X,Y>=\mathbb{E}[XY]$ , it verifies the necessary properties. So I decide to consider the norm $\lVert X\rVert_2=\sqrt{\mathbb{E}[X^2]}$ . So I know that I need to prove that this space is complete with relation to that norm, and that's what I can't seem to do.
I need to consider a Cauchy sequence $\{X_n\}$ in $L^2 (Ω, F, P)$ . So for every $\epsilon>0$ , there is a N such that for all $n,m>N$ we have : $\lVert X_n-X_m\rVert_2=\sqrt {\mathbb{E}[(X_n-X_m)^2]}\leq \varepsilon$ . I need to prove that there is a X in $L^2 (Ω, F, P)$ such that $\lim_{n\to\infty}\lVert X_n-X\rVert_2=0$ And now from there I don't really know where to go, I saw things about the dominated convergence theorem but I'm unsure about how to use it in this siuation. I hope my question is not too dumb.","['hilbert-spaces', 'probability']"
4691043,Is there ever a reason not to take the completion of a measure space?,"It seems to me like whenever we have a measure space $(X, \mathcal{A}, \mu)$ , it's pretty much always more convenient to work with $(X, \mathcal{X}, \mu)$ , where $\mathcal{X}$ is the completion, i.e. $$\mathcal{X} = \left\{ A \cup E : A \in \mathcal{A} \land \exists N \in \mathcal{A} \; \left( E \subseteq N \land \mu(N) = 0 \right) \right\} .$$ This has several advantages, e.g. if two functions agree almost everywhere, and one is measurable, then so is the other. Wikipedia also mentions that Maharam's Theorem applies to complete measure spaces, as well as for other classes of measure spaces, but doesn't mention general measure spaces. But is there ever a reason why we'd specifically want to stick to $\mathcal{A}$ without making the passage to $\mathcal{X}$ ? Hopefully this isn't too broad or subjective a question.",['measure-theory']
4691082,Calculus: Differentiable and Continuous Reasoning,"Let $f(x)=x^{5/3}+b$ when $x<1$ and $f(x)=ax^{4/3}$ when $x\geq 1$ . This question wants to know for what values of $a$ and $b$ is $f(x)$ differentiable for all values of $x$ . When I solved this problem, I set $\frac{d}{dx} [x^{5/3}+b]=\frac{d}{dx}ax^{4/3}$ when $x=1$ . So, $\frac{5}{3}=\frac{4}{3}a\implies a=\frac{5}{4}$ . This implies that $\lim_{x\to 1^-}f'(x)=\lim_{x\to 1^+}f'(x)=c$ for some $c\in \mathbb{R}$ to make the function be differentiable when $x=1$ . When the left hand derivative and the right hand derivative at a point are equal, then the function is said to be differentiable at that point. When $f(x)$ is differentiable at $x=1$ , then we know $f(x)$ is continuous at $x=1$ . This means $\forall b\in \mathbb{R}$ , $f(x)$ should be continuous at $x=1$ . However, this is not the case for certain values of $b$ . So, what is wrong with my reasoning here... Now, the answer requires that $b=\frac{1}{4}$ because the function should be continuous at $x=1$ . This is because when $1^{5/3}+b=a1^{4/3}$ and $a=\frac{5}{4}$ implies $1+b=\frac{5}{4}\implies b=\frac{1}{4}$ .","['continuity', 'derivatives']"
4691099,"Asymptotics of $\int^{\pi/2}_0 xf(x)(\cos x)^n\,dx$","The following is an old Analysis qualifying problem I am tried to solve but I have not been able to solve rigorously. Suppose $f$ is continuous in $[0,\pi/2]$ . A simple application of dominated convergence shows that $$I_n=\int^{\pi/2}_0 x f(x) (\cos x)^n\, dx \rightarrow0\quad\text{as}\quad n\rightarrow\infty$$ The problem asks to show that $$I_n\sim \frac{1}{n}f(0)+o(1/n)$$ This integral seems to have similarities with integrals where the Laplace method can be use. The function $\cos$ has maximum at $x=0$ in $[0,1]$ , so it is not surprise that the values of $x\in[0,\pi/2]$ that contribute to $I_n$ are those near $0$ . On the other hand, $\sin x\sim x$ as $x\rightarrow0$ . This suggest that $$I_n\sim \int^{\pi/2}_0f(0)\sin x(\cos x)^n\,dx=\frac{1}{n+1}f(0)$$ I can't find a way to put things into a  rigorous argument. Any help is appreciated. Thank you!","['integration', 'asymptotics', 'real-analysis']"
4691104,The area between parabolic lines inside a square,"The following square has edges of size $1$ and I'm trying to find the area of the blue region trapped between the parabolic curves created by the straight lines (number of lines is technically infinite). I assume that the simplest way to go about this is to use integration based on the parabolic equations that the lines create, but is there any more fundamental way of solving this maybe using infinite series and limits? Or at least explaining the integral solution using differentiation and limits. P.S. You can imagine that a line is drawn at the point $\dfrac 1n$ of one edge to the point $1-\dfrac1n$ of the other edge where $n$ varies from $1$ to infinity.","['integration', 'analytic-geometry', 'envelope', 'conic-sections', 'geometry']"
