question_id,title,body,tags
4469155,A question on two self-adjoint operators with same spectrum,"What could be an example of two self-adjoint operators $T_1,T_2\in \mathcal B(E)$ ,where $E$ is a hilbert space,such that both of them have cyclic vectors and have same spectrum but they are not unitarily equivalent? What if one of them is compact and injective? Are two operators unitarily equivalent then? I know that, $T_1$ and $T_2$ both are unitarily equivalent to some multiplication operator on $L^2(\sigma(T_i),\mu)$ . So,I was thinking in terms of multiplication operators but could not come up with an example. Any help or hint would be appreciated. Thanks in advance.","['compact-operators', 'self-adjoint-operators', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
4469171,Fourier transform of $\frac{x}{\sinh(x)}$,"I was given to calculate the Fourier Transform of $\frac{x}{\sinh(x)}$ . So, the problem is to calculate the integral $$
\int_\mathbb{R} \frac{x}{\sinh(x)}e^{-i \omega x} dx
$$ I know such an integrals can be evaluated using complex analysis, but I don't know how to take the proper contour. Function under integral has a removable singularity at $x=0$ and poles at $x=\pi i k, 0 \ne k \in \mathbb{Z}$ I've tried this contour: But it doesn't work since the $x/\sinh(x)$ is not even bounded on the half-circle, so Jordan Lemma is inapplicable. And this one: But I don't quite see how to integrate over any side of a rectangle. Any ideas?","['complex-analysis', 'contour-integration', 'fourier-transform', 'residue-calculus']"
4469203,Two players toss a coin; probability game doesn't end in 100 tosses?,"Player A and B alternate when flipping a coin. If the number of heads is K more than the number of tails, A wins, if the number of tails is K more than heads, B wins. What is the probability that the game is not over after 100 coin tosses? I started by considering simpler cases like winning if there are 2 more heads/tails than tails/heads in 100 tosses and it is clear the the game finishes at most on the 3rd toss with probability 1, hence it makes sense that with 50 more heads/tails than tails/heads in 100 tosses the game finishes in at most 99 tosses? For more than that I would need to compute the permutations of the different scenarios for winnings and divide by the total number of posibilities for each K, not feasible. I was thinking maybe I could solve it via recursion or some sort of conditioning Need help with ideas, hints and/or intuition please! Also how would that change if the players toss the coins (no alternation) separately and the one to have k more heads/tails than the other wins?","['conditional-probability', 'probability-theory', 'probability']"
4469205,"Calculate $D^3f $ if $ f(x,y)= e^x cos(y)$","I want to know if my answer is correct, please. Let $f(x,y)=e^x cos(y)$ , calculate $D^3f$ . Solution: We have that $Df = [\frac{\partial f}{\partial x} \space \space \frac{\partial f}{\partial y}]$ , then: \begin{equation}
Df=
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
\end{bmatrix}
\end{equation} \begin{equation}
D^2f=D(Df)=
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\
\end{bmatrix} = 
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
\end{bmatrix}
\end{equation} And finally: \begin{equation}
D^3f=D(D(Df))=
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\
\frac{\partial f_3}{\partial x} & \frac{\partial f_3}{\partial y} \\
\frac{\partial f_4}{\partial x} & \frac{\partial f_4}{\partial y} \\
\end{bmatrix} =
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}cos(y) & e^{x}sin(y) \\
\end{bmatrix}
\end{equation} Therefore, \begin{equation}
D^3f=
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}cos(y) & e^{x}sin(y) \\
\end{bmatrix}
\end{equation}","['multivariable-calculus', 'calculus', 'solution-verification']"
4469236,Applying theorems and properties to solve indeterminate limits,"I have been trying to solve the following limit for my university Mathematical Analysis class: $$\lim \limits_{x \to +\infty}\sin(x)(\ln{(\sqrt{x}+1})-\ln(\sqrt{x+1}))$$ I know that the answer is 0, but I am not sure why. Since $\lim \limits_{x \to +\infty}\sin(x)$ is indeterminate, I immediately discarded substitution as a possibility, especially considering that the logarithms would also be indeterminate with f(x) as a whole being $\sin(\infty)(\ln(\infty)-\ln(\infty))$ . I have tried rewriting the problem as $$\lim \limits_{x \to +\infty}\frac{\ln{(\sqrt{x}+1})-\ln(\sqrt{x+1})}{\frac{1}{\sin(x)}}$$ to use L'Hopital theorem, which I simplified to $$\lim \limits_{x \to +\infty}\frac{\frac{1}{2x+2\sqrt{x}}-\frac{1}{2x+2}}{-\frac{\cos(x)}{\sin^2(x)}}$$ But this also seemingly has no solution, and just complicates the problem a lot more that is likely required to solve it. I ""know"" how to use Landau symbols, so there probably is a way to apply them here, but I have not thought of it yet. I am not aware of any theorems or properties that could help solve this problem. Any help would be appreciated.","['indeterminate-forms', 'limits', 'calculus']"
4469237,"Does a subset $S$ of $\mathbb{Q}$ with the following conditions, equal $\mathbb{Q}$?","Let $S$ be a subset of $\mathbb{Q}$ such that: $0 \in S$ ; If $x\in S$ , then $(x-1) \in S$ and $(x+1) \in S$ ; If $x \in S \setminus\{0,1\}$ , then $\frac{1}{x(x-1)} \in S$ . Does $S = \mathbb{Q}$ ?",['abstract-algebra']
4469238,"Difficult integral from general relativity $\int_{a}^{0} \frac{1+mu}{\sqrt{u-a} \sqrt{b-u}} \,du$","When solving for the angular deflection of a light ray passing close to a star in general relativity, one needs to compute an elliptic integral. By some clever factoring and application of a relevant approximation, one reduces the problem to $$\Delta \phi = -2\int_{a}^{0} \frac{1+mu}{\sqrt{u-a} \sqrt{b-u}} \,du$$ I know (and have verified with Mathematica) that this has exact solution $$\Delta \phi = 2m\sqrt{-ab}+[4+2m(a+b)]\arctan\left(-\sqrt\frac{a}{b}\right),$$ but do not see how to replicate this solution. I've tried reverse engineering this in the case integration by parts was used but can't quite figure it out. Also, is there a complex contour integration approach - perhaps on the dogbone contour? Any hint or point to relevant literature would be appreciated. Thanks.","['integration', 'complex-analysis']"
4469245,"Prove that $\int_{-\infty}^\infty\frac{e^{-tx^2}}{\cosh\pi x}\,dx\ge\frac{e^{t/2}}{t+1}$ for all $t\in[0,1]$","This question is taken directly from Showing that $z^2 e^{-z^2/2} \int \frac{\phi^2(x)}{\cosh(xz)} \, dx \geq \frac{1}{2\sqrt{\pi}} \frac{z^2}{z^2+1}$ which unfortunately turned out to be untrue for all $z\ge0$ . By plotting the functions, it appears that the inequality is true for $0\le z\le1.$ That is, conjecturally, $$\int_{-\infty}^\infty\frac{e^{-u^2}}{\cosh zu}\,du\ge\frac{e^{z^2/2}\sqrt\pi}{z^2+1},\quad\forall z\in[0,1].$$ To simplify this, we can invoke the elegant identity $$\int_0^\infty\frac{e^{-u^2}}{\cosh\alpha u}\,du=\frac{\sqrt\pi}\alpha\int_0^\infty\frac{e^{-u^2}}{\cosh(\pi u/\alpha)}\,du$$ to obtain the equivalent $$\int_{-\infty}^\infty\frac{e^{-u^2}}{\cosh(\pi u/z)}\,du\ge\frac{ze^{z^2/2}}{z^2+1}.$$ Note the identity can be used as the integrand is an even function. Substituting $x=u/z$ and $t=z^2$ yields $$\int_{-\infty}^\infty\frac{e^{-tx^2}}{\cosh\pi x}\,dx\ge\frac{e^{t/2}}{t+1}.\tag1$$ Can $(1)$ be proven analytically for all $t\in[0,1]$ ?","['integration', 'inequality']"
4469246,Triple integral over the region bounded by a parabolic cylinder and three planes,"In chapter 13 of Stewart's calculus the integral $$\iiint_A 3y \, dx \,dy \,dz,$$ where $A$ is the region bounded by the parabolic cylinder $z = 1-x^2$ and the planes $z = 0,$ $y = 0$ and $y+z=2,$ is computed by projecting $A$ on the $XZ$ plane, which yields $184/35$ as a result. Now, I've tried to compute the integral by using the projection on the $XY$ plane, but I get a different result. I guess the way I've set up the integral is mistaken, but I can't see why. It seems to me that the projection of $A$ on the $XY$ plane is $\pi_{XY}(A) = [-1, 1] \times [0, 2]$ and that, for each $(x, y) \in \pi_{XY}(A),$ the corresponding section is $$ A_{(x, y)} = \begin{cases} [0, 1-x^2] &\text{ if } 0\leq y \leq 1, \\
                              [0, 2-y] &\text{ if } 1< y \leq 2.\end{cases}$$ Hence $$ \iiint_A 3y \, dx \, dy \, dz = \int_{-1}^1 \int_0^1 \int_0^{1-x^2}3y \, dz \, dy \, dx+\int_{-1}^1 \int_1^2 \int_0^{2-y} 3y \, dz \, dy \, dx.$$ Edit: There must definitely be something wrong with the limits of integration because my section is not continuous at $y=1,$ but I still can't picture the situation clearly.","['multivariable-calculus', 'multiple-integral']"
4469256,How can I prove that $\Bbb{P}(X>0)\geq \frac{\Bbb{E}(X)^2}{\Bbb{E}(X^2)}$,"I have the following problem: Let $X$ be a nonnegative random variable such that $\Bbb{E}(X^2)<\infty$ Then show that $$\Bbb{P}(X>0)\geq \frac{\Bbb{E}(X)^2}{\Bbb{E}(X^2)}$$ I would like to get some hints, because I did the following: Let me define $U:=X$ and $V:=\Bbb{1}_{\{X>0\}}$ . Then clearly $U\in L^2$ . But also $V\in L^2$ since $$\int_\Omega (\Bbb{1}_{\{X>0\}})^2 \Bbb{P}(d\omega)=\int_\Omega \Bbb{1}_{\{X>0\}}\Bbb{P}(d\omega)=\Bbb{P}(X>0)\leq 1$$ In addition we have that $\Bbb{E}(V^2)=\Bbb{E}((\Bbb{1}_{\{X>0\}})^2)=\Bbb{E}(\Bbb{1}_{\{X>0\}})=\Bbb{P}(X>0)$ . Now applying the Cauchy-Schwarz inequality we get $\Bbb{E}(|UV|)^2\leq \Bbb{E}(U^2)\cdot\Bbb{E}(V^2)=\Bbb{E}(U^2)\cdot\Bbb{P}(X>0)$ so $$\Bbb{P}(X>0)\geq \frac{\Bbb{E}(|\Bbb{1}_{\{X>0\}}X|)^2}{\Bbb{E}(X^2)}$$ Now I see that $\Bbb{E}(|\Bbb{1}_{\{X>0\}}X|)=\Bbb{E}(\Bbb{1}_{\{X>0\}}X)=\Bbb{E}(X)$ since the lebesgue mesure doesn't see the nullsets. Is this correct like this? Thanks for your help.","['stochastic-calculus', 'expected-value', 'cauchy-schwarz-inequality', 'probability-theory', 'probability']"
4469268,Infinite symmetrical matrix sum (discrete Lyapunov equation),"I have 2 symmetrical matrices ( $A$ and $B$ ) and I am looking to find the sum $S$ : $$S=A+BAB+B^2AB^2+\ldots$$ Or in summation format: $$S=\sum_{i=0}^\infty B^iAB^i$$ We know that the absolute magnitude of all the eigenvalues $\lambda_i$ of $B$ are less than $1$ so the sum converges. I tried to use the same trick as for a geometric sum but I can't factor out both $B$ 's at the same time. To simplify, we can assume that $B$ is diagonal but I assume that the result would hold for any shape. EDIT: It seems to be similar to the the discrete Lyapunov equation, with: $$BSB-S+A=0$$ as all matrices are symmetrical here. https://en.wikipedia.org/wiki/Lyapunov_equation","['matrices', 'summation', 'geometric-series']"
4469276,Every contractive self-adjoint operator is the weak limit of projectors,I try to show that every self-adjoint operator $A$ on Hilbert space $H$ with $\|A\| \leq 1$ is the weak limit of the projectors. My teacher told me that the most important part is to show that for one-dimensional operator and I have done that. But how can I obtain the general case form one-dimensional? I have no idea. Any help is appreciated. Thanks in advance.,"['self-adjoint-operators', 'operator-theory', 'functional-analysis', 'weak-convergence']"
4469281,"Reference Request: Does there exist a definition for a ""total finite difference"" operator?","I was just wondering if there is a total finite difference operator defined in a way that is analogous to the total derivative operator. As we know, for a function of one variable, we might define the finite difference by $$\Delta f(x_n) = f(x_{n + 1}) - f(x_n),$$ and we could conceivably treat this as a derivative operation if we consider an equally spaced grid so that $$\frac{\Delta_h f(x)}{\Delta_h\,x} = \frac{f(x + h) - f(x)}{(x + h) - x}.$$ My question is if anyone has extended this to a ""Total Finite Difference"" operator in the same way that we have a total derivative for $f(x,y)$ given by something like $$D_{\Delta} f(x_n,y_m) = f_{\Delta x} \Delta x_n + f_{\Delta y}\Delta y_m.$$ where $f_{\Delta x} = f(x_{n + 1}, y) - f(x_n)$ and $\Delta x_n = x_{n + 1} - x_n$ (or something like this). If this does exist, does anyone know if this type of operation plays well with summation by parts? Any help is appreciated. Thank you!","['finite-differences', 'derivatives', 'discrete-mathematics', 'discrete-calculus']"
4469344,Metric defined on k tensors,"If $g$ is a riemannian metric on $M$ and $f$ is a real valued smooth function on $M$ , Then, why does $g(\nabla^kf,\nabla^kf)$ make sense? How does one interpret this expression? Can one view $\nabla^kf$ as a a smooth vector field on $M$ ? For answers, try to avoid coordinates. I am interested in how one views $g$ in this case. $g$ is a map $\mathfrak{X}(M)\times \mathfrak{X}(M)\rightarrow C^{\infty}(M)$ , whereas $\nabla^kf$ is a smooth linear map $\bigoplus_{i=1}^k \mathfrak{X}(M)\rightarrow C^{\infty}(M)$ For reference: Look at the answer to the following question: Understanding iterated covariant derivatives to define Sobolev spaces on manifolds","['functional-analysis', 'riemannian-geometry']"
4469362,A norm $\|\cdot\|$ on $\mathbb R^N$ such that $\|x\|^2$ is $C^2$ near $x=0$ is induced by an inner product.,"Let $\|\cdot\|$ be a norm on $\mathbb R^N$ and define $f: \mathbb R^N\to\mathbb R$ by $f(x)=\|x\|^2$ . Suppose that $f$ is $C^2$ near $x=0$ . Prove that there is an inner product $(\cdot,\cdot)$ on $\mathbb R^N$ such that $\|x\|^2=(x,x)$ for all $x\in \mathbb R^N$ . Here is my attempt. I guess that the inner product is given by a positive-definite matrix. Therefore, I define $a_{ij}=\frac12\frac{\partial^2f}{\partial x_i\partial x_j}(0)$ for all $1\leq i,j\leq N$ and let $A=(a_{ij})$ . Since $f$ has a minimum at $x=0$ and is $C^2$ near $x=0$ , I deduced that the matrix $A$ is positive-definite. It suffices to show that $f(x)=x^TAx$ for all $x\in \mathbb R^N$ ; if so, then $(x,y):= x^TAy$ gives the desired inner product. But I don't know how to prove $f(x)=x^TAx$ . Any hints are welcome！","['linear-algebra', 'functional-analysis', 'analysis']"
4469404,Different formulations of vague topology,"Given a locally compact Hausdorff space $X$ , I have seen at least two ways to define vague topology on the space of signed finite Radon measures $M(X)$ . They use the fact that $M(X)$ , endowed with the total variation norm, is isomorphic to the dual of both $C_c(X, \mathbb R)$ , the set of real-valued, continuous and compactly supported functions on $X$ , and its closure (in the uniform metric) $C_0(X, \mathbb R)$ , the set of real-valued continuous functions on $X$ that vanish at infinity. The first isomorphism allows us to use the weak*-topology on $C_c(X, \mathbb R)^*$ to say that a sequence $\{\mu_n\} \subset M(X)$ converges vaguely to a limit $\mu \in M(X)$ iff $$
\int_X f \, \mathrm{d}\mu_n \to \int_X f \, \mathrm{d}\mu \quad \text{for all } f \in C_c(X, \mathbb R). \tag{1}
$$ On the other hand, the second isomorphism allows us to use the weak*-topology on $C_0(X, \mathbb R)^*$ to say that a sequence $\{\mu_n\} \subset M(X)$ converges vaguely to a limit $\mu \in M(X)$ iff $$
\int_X f \, \mathrm{d}\mu_n \to \int_X f \, \mathrm{d}\mu \quad \text{for all } f \in C_0(X, \mathbb R). \tag{2}
$$ I know a relation between these two formulations of vague convergence. Suppose we call the first formulation c-vague and the second formulation 0-vague . Then 0-vague convergence of $\{\mu_n\}$ to $\mu$ is equivalent to c-vague convergence of $\{\mu_n\}$ to $\mu$ and $\{\mu_n\}$ being uniformly bounded. I am new to functional analysis, and so, can somebody shed more light on the relation between these two formulations ? In analysis literature, the 0-vague formulation is used, for example, in Riesz representation theorem. While in probability literature, the c-vague formulation is used. This question is probably the closest to what I am asking.","['measure-theory', 'real-analysis', 'functional-analysis', 'general-topology', 'probability']"
4469416,"Why is $k[x,t]/(x^n-t)\simeq k[t]\oplus k[t]\cdot x\oplus\ldots\oplus k[t]\cdot x^{n-1}$ as $k[t]$-modules?","I stumbled across this isomorphism on the Wikipedia article for finite morphisms: $$k[x,t]/(x^n-t)\simeq k[t]\oplus k[t]\cdot x\oplus\ldots\oplus k[t]\cdot x^{n-1}$$ as $k[t]$ -modules. I'm really struggling to understand what this quotient looks like and am wondering how one would construct this isomorphism.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4469477,On the definition of weak and weak-* topologies,"I have been studying topological vector spaces, and despite going over numerous resources, the definitions of weak and weak-* topologies have been causing me some confusion. I am having trouble visualizing and understanding these topologies. Suppose $X$ is a normed vector space. Then the weak topology on $X$ is the topology generated by $X^*$ , in other words the weakest topology making $x \mapsto f(x)$ continuous for all $f \in X^*$ . Similarly, the weak-* topology is the weakest topology making the maps $x \mapsto f(x)$ continuous for all $x \in X$ . I see the big difference here is that one is generated by the dual and the other by the original vector space. However, I have three points of confusion. I suspect part of my difficulty may be due to not properly visualizing topologies generated by a collection of seminorms. The dual space $X^*$ is defined as the set of bounded linear functionals from $X$ to the underlying field. However, I recall reading that the boundedness of a linear map is equivalent to the map being continuous, so I fail to see what sets we are excluding in this new, weaker topology. What do the open sets (or more simply, the basis sets) look like in these two topologies? The resources I am learning this from often note a relation between the double dual $X^{**}$ and the weak-* topology, what is the relation between these spaces exactly?","['topological-vector-spaces', 'real-analysis', 'weak-topology', 'functional-analysis', 'general-topology']"
4469482,Probability of buying a defective laptop,"There are two brand shops - A and B. A manufactures 1000 laptops, of which 100 are defective. B manufactures 100 laptops, of which 10 are defective.
When I go to buy at a shop of any brand, they simply pick a laptop at random and give it to me. Which shop should I buy from - A or B? I was given this problem by a friend who's into tricky puzzles, so I'm skeptical that this will be straightforward. I'm thinking that in both cases there's a 10% chance of getting a defective laptop. So I guess both shops are equally fine to buy from? Am I missing something here and is there a more involved solution to this?",['probability']
4469507,"Prove equation $E(h(X)\,e^{-Y})=E(e^{-Y})\, E(h(X-\operatorname{Cov}(X,Y)))$","Prove: $$
E\bigl(h(X)\, e^{-Y}\bigr) 
= E\bigl(e^{-Y}\bigr) \, E\bigl(h(X-\operatorname{Cov}(X,Y))\bigr)
$$ when $X$ , $Y$ are both normally distributed, $h(X)$ is a function of $X$ . I think it can be proved by definition and maybe conditional distribution. But I am stuck in the calcultation of conditional expected value of $h(X)$ given $Y$ and cannot get result I want to see. Maybe this idea is wrong. Can you solve this problem or give a reachable idea? Thank you very much!","['expected-value', 'probability-distributions', 'normal-distribution', 'probability']"
4469565,Value of $x^6 - 2\sqrt{3}x^5 - x^4 + x^3 - 4x^2 + 2x - \sqrt{3}$ given $x = \frac{1}{2-\sqrt{3}}$?,It is given that $x = \frac{1}{2-\sqrt{3}}$ . Find the value of $x^6 - 2\sqrt{3}x^5 - x^4 + x^3 - 4x^2 + 2x - \sqrt{3}$ . Well I tried rationalising and I came to know that $x = 2 + \sqrt{3}$ . And I know that directly putting the values wont help either but I am not able to factorize the polynomial or manipulate it to help me. I would be grateful if anybody could help me.,"['radicals', 'algebra-precalculus', 'polynomials']"
4469575,Period map for products of K3 surfaces,"If $X$ is a K3 surface, it carries a unique (up to scaling with $\mathbb{C}^*$ ) holomorphic 2-form $\sigma$ , determined by the complex structure. Let $\Lambda=3U\oplus-2E_8$ be the K3-lattice. The period domain of a K3 surface is given by $$
\Omega=\{[\sigma]\in \mathbb{P}(\Lambda\otimes\mathbb{C})\colon \sigma\cdot\sigma=0, \sigma\cdot\overline{\sigma}>0\}.
$$ A marked K3 surface is a pair $(X,\phi)$ , with $X$ K3 and where $\phi$ is an isometry $\phi:H^2(X,\mathbb{Z})\to \Lambda$ . Let $M_1$ be the set of equivalence classes of Marked K3 surfaces, and define the Period map $\tau:M_1\to \Omega$ by $\tau([X,\phi])=[\phi(\sigma_X)]$ . Now suppose $Y$ is another K3 surface, and consider the product of K3 surfaces $X\times Y$ . I want to define the period map for this product. By Künneth's theorem, the natural map $$H^2(X,\mathbb{Z})\oplus H^2(Y,\mathbb{Z})\to H^2(X\times Y,\mathbb{Z})$$ is an isomorphism, so this endows $H^2(X\times Y,\mathbb{Z})$ with the natural lattice structure, isomorphic to $\Lambda\oplus\Lambda$ . Let $\phi:H^2(X\times Y,\mathbb{Z})\to \Lambda\oplus\Lambda$ . The product $X\times Y$ carries two nowhere vanishing holomorphic 2-forms: $\sigma_1=\pi^*_X\sigma_X$ and $\sigma_2=\pi^*_Y\sigma_Y$ . The new period domain would be $$
\Omega'=\{[(\sigma_1,\sigma_2)]\in\mathbb{P}((\Lambda\oplus\Lambda)\otimes\mathbb{C})\colon \sigma_1\cdot\sigma_1=0,\sigma_1\cdot\sigma_2=0,\sigma_2\cdot\sigma_2=0,\sigma_1\cdot\overline{\sigma_2}=0, \sigma_1\cdot\overline{\sigma_1}>0,\sigma_2\cdot\overline{\sigma_2}>0\},
$$ and I would like to define $\tau':M_1'\to \Omega'$ by $\tau'([X\times Y, \phi])=[(\phi(\sigma_1),\phi(\sigma_2)]$ . However, this is not really well-defined. Is there a better way to define the period map for the product of K3 surfaces? Edit: At first, I thought that $M_1'=M_1\times M_1, \Omega'=\Omega\times \Omega$ and $\tau'=\tau\times\tau$ . However, this does not seem to work dimensionwise, so I am interested in any ideas!","['complex-geometry', 'algebraic-geometry', 'k3-surfaces', 'differential-geometry']"
4469582,Union Bound of two events?,"I am trying to understand the assumption proof of Theorem 2(Page - $7$ ) in the paper ""A Universal Law of Robustness via isoperimetry"" by Bubeck and Sellke. Inequality 1 \begin{align}
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(f\left(x_{i}\right)-\mathbb{E}[f]\right) z_{i} \geq \frac{\epsilon}{8}\right) \leq 2 \exp \left(-\frac{\epsilon^{2} n d}{9^{4} c L^{2}}\right)
\end{align} Since we assumed that the range of the functions is in $[-1,1]$ we have $\mathbb{E}[f] \in[-1,1]$ and hence: Inequality 2 $$
\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[f] z_{i} \geq \frac{\epsilon}{8}\right) \leq \mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} z_{i}\right| \geq \frac{\epsilon}{8}\right)
$$ By Hoeffding's inequality, the above quantity is smaller than $2 \exp \left(-n \epsilon^{2} / 8^{3}\right)$ (recall that $\left.\left|z_{i}\right| \leq 2\right)$ . Thus we obtain with an union bound: Inequality 3 $$
\begin{aligned}
\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right) z_{i} \geq \frac{\epsilon}{4}\right) & \leq|\mathcal{F}| \cdot \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(f\left(x_{i}\right)-\mathbb{E}[f]\right) z_{i} \geq \frac{\epsilon}{8}\right)+\mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} z_{i}\right| \geq \frac{\epsilon}{8}\right) \\
& \leq 2|\mathcal{F}| \cdot \exp \left(-\frac{\epsilon^{2} n d}{9^{4} c L^{2}}\right)+2 \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)
\end{aligned}
$$ I am not getting how Union bound is getting happened using Ineq 1 and 2. Can anyone help me with that how they able to reach last inequality?","['expected-value', 'empirical-processes', 'solution-verification', 'upper-lower-bounds', 'probability']"
4469600,Munkres Topology preparation for Algebraic Topology by Hatcher,"I am planning to take a graduate topology class that uses Algebraic Topology by Hatcher. In order to prepare for that class, would going over chapter 1-8 of Munkres Topology be sufficient enough to prepare for the class? By that time, I would already prepared with the algebra prerequisites.","['general-topology', 'algebraic-topology']"
4469620,Proof of Lyapunov instability theorem,"I'm reading through my university notes and I've noticed we only proved the Lyapunov stability and asymptotical stability theorems, and haven't touched upon instability. I'm familiar with the concept of the Lyapunov function, I just find it a bit strange that even on the internet I haven't been able to find a detailed proof, or even the formal statement of the instability theorem. I'm wondering if anyone knows where can I find a detailed proof of the Lyapunov instability theorem on the internet. Thanks!","['lyapunov-functions', 'ordinary-differential-equations']"
4469662,Property of Fitting subgroup: Let $G$ be a finite group and $C:= C_G(F(G))$. Then $O_p(C/C\cap F(G))=1$ for every prime $p$.,"I'm trying to understand the proof of the following which is stated in Kurzweil and Stellmacher: Let $G$ be a finite group and $C:= C_G(F(G))$ . Then $$O_p(C/C\cap F(G))=1$$ for every prime $p$ . Here, $F(G)$ is the Fitting subgroup of $G$ , i.e., the product of all nilpotent normal subgroups of $G$ and $O_p(H)$ denotes the intersection of all Sylow $p$ -subgroups of a group $H$ . Also, a group is defined to be nilpotent if all its subgroups are subnormal. My attempt: Let $H=C\cap F(G)$ and let $\pi:C\to C/H$ be the quotient map. Let $P=\pi^{-1}(O_p(C/H))$ . If $P\subseteq H$ , then $\pi(P)= 1$ and so $O_p(C/H)=1$ . Therefore it suffices to show that $P\subseteq H$ . As $P\subseteq C$ , we are only left to show that $P\subseteq F(G)$ . As the $F(G)$ is the largest normal nilpotent subgroup of $G$ , it suffices to show that $P$ is normal in $G$ and $P$ is nilpotent. Normal: As $C$ is a normal subgroup of $G$ , so if $P$ is a characteristic subgroup of $C$ , then $P$ is also a normal subgroup of $G$ ......? Nilpotent: As subgroups of nilpotent groups are nilpotent, we only need to show that $C$ is nilpotent. However, as $H\subseteq Z(C)$ , we know that $C$ is nilpotent if and only if $C/H$ is nilpotent..... This is where I'm stuck. I don't know how to show that $P$ is a characteristic subgroup of $C$ and how to show that $C/H$ is nilpotent. Edit: Okay, I made some attempt on the nilpotent part: $Z(C)$ is a characteristic subgroup of $C$ and $C$ is a normal subgroup of $G$ , so $Z(C)$ is a normal subgroup of $G$ . Also, $Z(C)$ is nilpotent. Therefore $Z(C)\subseteq F(G)$ . Of course, $Z(C)\subseteq C$ . It follows that $H=Z(C)$ . So, we need to show that $C/Z(C)$ is nilpotent.... I don't know if this is any help.","['finite-groups', 'nilpotent-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4469697,Diffeomorphism between two norms,"If I consider the $L^2$ and $L^4$ norms on $\mathbb R^n$ , i.e. $||x||_2 = (\sum_{i=1}^n x_i^2)^{1/2}$ and $||x||_4 = (\sum_{i=1}^n x_i^4)^{1/4}$ , I know these two norms are equivalent in the sense we can find constants $c, C$ such that $c||x||_2 \leq ||x||_4 \leq C||x||_2$ , but are they equivalent as smooth (away from zero) functions in the sense I can find a change of coordinates/diffeomorphism sending one norm to the other? I can definitely find a composition which sends one norm to the other: if $$\varphi(x_1, \cdots, x_n) = (x_1^2, \cdots, x_n^2), \quad \psi(x) = \sqrt{x}$$ then $\psi \circ ||x||_2 \circ \varphi = ||x||_4$ . I would like to know whether I can find a single function $\Phi: \mathbb R^n \rightarrow \mathbb R^n$ such that $||x||_2 \circ \Phi = ||x||_4$ and whether $\Phi$ can have bounded Jacobian. Also, in general, can I do this for any pair of norms on $\mathbb R^n$ and not necessarily $L^p$ ? If not, can we do it with two functions $\varphi, \psi$ instead? I have some intuition: I think I ""want to send a $L^2$ -ball to an $L^4$ -ball"" in order for $||x||_2\circ\Phi=||x||_4$ to hold. And if I have a map $\Phi_0$ sending one ball to another $B_{L^2}(1) \rightarrow B_{L^4}(1)$ I think I can extend this to a map $\Phi$ on the entire space by taking a point, scaling it back to the unit ball, mapping it under $\varphi$ , then rescaling. The only downside of this is that I imagine is the scaling: I think I would end up needing to divide (and multiply) by a norm and I can't imagine this having a nice derivative. This makes me think maybe I'm hoping too much. The idea of this application might be the integration of radial functions - is there a nice change of coordinates (i.e. bounded Jacobian) from one norm to another?","['normed-spaces', 'functional-analysis', 'real-analysis']"
4469710,"Constant $A$: $|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$ for all $m\in \mathbb{N}_{>0}$, $\theta \in [0, 2\pi)$","Problem : Prove that there exists some real constant $A$ such that $$|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$$ for any natural numbers $m \geq 1$ and any real number $0 \leq \theta < 2\pi$ . Context:
I am currently working on my bachelor's thesis and trying to show that the following contour integral tends towards zero for fixed $\Re(z) < 0$ for large natural numbers $m$ : $\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^w - 1}dw$ . I could already find the following: \begin{equation}
 |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}| 
\end{equation} but to continue I am trying to show the following inequality for any natural numbers $m \geq 1$ and any real number $0 \leq \theta < 2\pi$ : $$|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$$ for some real constant $A$ . I do not need to know the specific constant but just that there is one. With this I could conclude: \begin{equation}
 |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}| 
    \leq  \thinspace2\pi (2m+1)\pi \frac{((2m+1)\pi)^{\Re(z)-1}}{A}
    =  \thinspace 2\pi \frac{((2m+1)\pi)^{\Re(z)}}{A} \xrightarrow{m\rightarrow\infty} 0.
\end{equation} The only problem is that I have no clue where to start, could someone give me an idea of how to maybe find something? I know that graphically $A \approx 1$ but besides that I am stuck. Edit: I tried approaching this with the inverse triangle inequality but that won't get us there because if we write: $|e^{(2k+1)\pi e^{i\theta}}-1|\geq |e^{(2k+1)\pi \cos\theta}-1| $ then we can no longer find such an $A$ since $\theta = \frac{\pi}{2}$ will result in |1-1| = 0. So I am asking if someone maybe knows a different approach.","['complex-analysis', 'inequality', 'absolute-value', 'complex-numbers']"
4469726,Is this formula from Wolfram wrong?,"I tried to calculate the length of the side $a'$ of the contact triangle of $\Delta ABC$ (the side corresponding the the side $BC$ ), and I found $(-a+b+c)\sin \frac {A}2$ (if we denote the incenter by $I$ , the ends of $a'$ by $E$ and $F$ , and $M$ the midpoint of $EF$ , we have $a'=EF=2FM=2\cdot AF\cdot \frac{FM}{AF}=2\cdot (s-a)\cdot \sin \angle FAM=(-a+b=c)\sin\frac{A}2$ ). But Wolfram say this is $(-a+b+c)\cos \frac{A}2$ . Am I wrong, or Wolfram is?","['euclidean-geometry', 'geometry']"
4469764,"What is my error in this $\nabla_{\vec{v}} f(x,y,z)$ at $\vec{a} = (-1, -1, 4)$ and $\vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2})$ problem","I want to find gradient of $f(x,y,z) = \sqrt{xyz}$ in the direction of $\vec{v}$ at a point $\vec{a}$ . That is, $\nabla_{\vec{v}} f(x,y,z)$ at $\vec{a} = (-1, -1, 4)$ and $\vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2})$ I computed gradient of $f(x,y,z)$ along $\vec{v}$ to be $\left(\begin{matrix} \sqrt{\frac{yz}{4x}} \\ \sqrt{\frac{xz}{4y}} \\ \sqrt{\frac{xy}{4z}} \end{matrix}\right)$ . So my answer for the value of gradient at $\vec{a}$ is $\left(\begin{matrix} 1 \\ 1 \\ \frac{1}{4}\end{matrix}\right)$ . But the answer given is $\left(\begin{matrix} \frac{yz}{\sqrt{4xyz}} \\ \frac{xz}{\sqrt{4xyz}} \\ \frac{xy}{\sqrt{4xyz}} \end{matrix}\right)$ . So the accepted answer for value of gradient at $\vec{a}$ is $\left(\begin{matrix} -1 \\ -1 \\ \frac{1}{4}\end{matrix}\right)$ . Why is my answer wrong?","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
4469767,Prove that $\lim_{n\to\infty} n^2 \int _0^{1/n}{x^{x+1}}dx = {1\over2}$.,"Prove that $$\lim_{n\to\infty} n^2 \int _0^{1/n}{x^{x+1}}dx = {1\over2}.$$ My Attempt: When I saw the integral it was tempting to see the integrand as $x^x x$ and to think of $lim_{x\to0}$ $x^x$ which when evaluated using L-Hopital's gives 1, will it be helpful in reaching a desired conclusion?","['integration', 'limits', 'definite-integrals']"
4469770,"Finding pdf of $Y=\min\{X_1,X_2,...,X_n\}$","Let $X_1,X_2,...,X_n$ random indepedent variables  that  follow the same distribution with a pdf $f$ and possibility function $F$ .Find the pdf of $Y=\min\{X_1,X_2,...,X_n\}$ . My solution: $F_Y(a)=P(Y<a)=P(\min\{X_1,X_2,...,X_n\}<a)$ ,if we let $X_k$ with $k \in \{1,2,...,n\}$ be the min then $F_Y(a)=P(X_k<a)=F_{X_k}(a)$ . So it follows that $f_X(y)=f_{X_k}(x)=f_{X_i}(x)$ for every $i \in \{1,2,...,n\}$ since $X_i$ follow the same pdf. I dont really thing that answers the question but it is an obvious note. Could someone help ? Thank you in advance !","['probability-distributions', 'order-statistics', 'probability-theory', 'probability', 'random-variables']"
4469792,Prove or disprove: $\lim_{x\rightarrow \infty} g(x) = 0$.,I am trying to determine whether the following statement is true or false: Let $f(x)$ be an unbounded non decreasing function. Define $g(x) =  f(x) - \frac{f(x)}{\cos \left(\tfrac{1}{f(x)}\right)}$ . Prove or disprove: $\lim_{x\rightarrow \infty} g(x) = 0$ . After trying to find counter examples I believe the statement is true. I have tried to prove the statement using the squeeze theorem but it didn't got me much further. Any hints will be appericiated.,"['limits', 'calculus', 'real-analysis']"
4469798,log - derivative transformation.,"That problem actually came out from Advanced Macroeconomics exercise, where one needs to transform an elasticity of substitution between capital and effective labor in the gross production function. Formulating problem mathematically, there is a function $F = F(x,y)$ . Then one constructs a derivative/function: $$\frac{\partial(x/y)}{\partial(F_x/F_y)}\cdot\frac{F_x/F_y}{x/y} = \frac{\partial \log(x/y)}{\partial \log(F_x/F_y)},$$ and ""transform"" this derivative into: $$\frac{\partial \log(x/F)}{\partial \log(F_x)}.
$$ Here we assume that the function has all the needed partial derivatives and they are continuous in a point of interest. So the question is how one may prove this transformation, and, probably, which fact one may use to prove this transition? I would appreciate any idea on how to proceed. corrections: Function $F$ should be homothetic .","['derivatives', 'analysis', 'real-analysis']"
4469861,"find the complete statistic for Uniform($\theta,\theta+1$)","Assume $x_1x_2,\dots,x_n$ are i.i.d Uniform( $\theta,\theta+1$ ), want to know if $x_{(n)}$ , the order statistic, is a complete statistic. This appears in a course slides, but I don't know if the result is right, since for $n=1$ the result is wrong. (by using the definition of complete statistic and the function $f(t)=sin(2\pi t)$ ). any hint would be appreciated!","['statistical-inference', 'statistics']"
4469924,What is the name of this closure operation?,"For any relation $R$ , let $\overline{R}$ denote its transitive closure, and define the function $\Omega_R:\wp(\operatorname{fld}(R))\to\wp(\operatorname{fld}(R))$ by $$\Omega_R:P\mapsto{P}\;\cup\;\{\;y\in\,\operatorname{fld}(R):\exists{x, z}\in{P}\,[x\overline{R}y\,\wedge\,y\overline{R}z]\;\}$$ Question: Given $P\subseteq\operatorname{fld}(R)$ , is there a standard name for the derived set $\Omega_R(P)$ ? In the question title, I called this a ""closure"" operation because I think that $\forall\,P\subseteq\operatorname{fld}(R)\;[\;\Omega_R(\Omega_R(P)) = \Omega_R(P)\;]$ . It reminds me of the definition of the ""convex hull"" of a set of points in, say, a Euclidean space.  Here, ""in-between-ness"" is defined through a relation $R$ (or, if you prefer, through its transitive closure $\overline{R}$ ), rather than through a convexity condition equation (i.e. $y = tx + (1 - t)z$ , for all ${x, z}\in{P}$ and $t\in[0, 1]$ ). Notation and terminology: $\operatorname{fld}(R)$ is the smallest set $S$ such that $R\subseteq{S\times{S}}\;.$ $\wp(X)$ is the powerset of $X$ . A relation $R$ is said to be transitive iff $\forall{x, y, z}\;[\,(xRy\,\wedge\,yRz)\,\to\,xRz\,]\;.$ If $R$ is a relation, the transitive closure $\overline{R}$ of $R$ is the smallest transitive relation that contains $R$ .","['elementary-set-theory', 'relations', 'terminology']"
4469943,"Is there a closed, convex subset $C$ of $\mathbb{R}^3$, such that any proper, closed, convex shape in the plane is a section of $C$?","To be precise, by a section of $C$ I mean the intersection of $C$ with a hyperplane. I want every proper, closed, convex subset of the plane, up to translation and rotation, to be appear as sectiond. So, for example, I want both the unit disc and the disc of radius $8.45$ to appear as sections. It was recently brought to my attention that there is a closed convex cone in $\mathbb{R}^3$ such that the set of conic sections is ""dense"" amongst the proper, closed, convex sets of the plane (dense in exactly metric, I do not know). This astonished me and motivated the above question. I would be very surprised if the answer is yes, and am kind of expecting some argument which shows that such a $C$ obviously cannot exist. But as I've just learnt, convex subsets can be highly exotic. If the answer is positive a bonus question would be if it can be arranged so that every closed, convex subset of the plane, up to translation and rotation, appears exactly once as a section.","['convex-analysis', 'geometry']"
4469996,How should I find $a_n$ knowing that $a_n = a_{n-1} + a_{n-3}$,"I tried using a quadratic formula by using the constants of the recursive formula. Then when I get the solutions of the quadratic function, I would insert the $x$ values gotten to $a_n = a_1 \cdot (x_1)^n+a_2 \cdot (x_2)^n$ . After, I would get some initial values such as $a_0$ and $a_3$ and make a system to solve. Unfortunately the $a_n$ formula gotten didn’t work.","['relations', 'recurrence-relations', 'discrete-mathematics']"
4470019,proving that $g(x)\in A$ for almost every $x$,"Let $B$ be a (Lebesgue) measurable set with $m(B) < \infty$ and let $g \in L^1(B)$ . That is, $\int_B |g| < \infty$ . Let $A\subseteq \mathbb{C}$ be a closed set such that $\frac{1}{m(F)} \int_F g dm\in A$ for every $F\subseteq A, m(F) > 0$ . Prove that $g(x) \in A$ for almost every $x\in B.$ A hint was the following: let $a \in A^c$ and $r > 0$ be small enough so that the closed ball of radius $r$ centered at $a, \overline{B}_r(a),$ is contained in $A^c$ . Show it suffices to prove $m(F) = 0$ , where $F = g^{-1}(\overline{B}_r(a))$ . Show that if $m(F) > 0,$ then $\frac{1}{m(F)}\int_F g dm \in A^c$ . I know that a countable union of measure zero sets has measure zero, so maybe I could obtain the countable union using $F$ somehow? Also, I'm not sure how to show that if $m(F) > 0,$ then $\frac{1}{m(F)} \int_F g dm\in A^c$ . How can I conclude that the integral is not in $A$ ? Should I use some sort of contradiction?","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4470083,Doubt regarding validity of answer in a mod equation question,"$$|x^2-2x+2|-|2x^2-5x+2|=|x^2-3x|$$ Find the set of values of $x$ . The answer given is $[0,\frac12]\cup [2,3]$ . What I don't get is how is the solution a range? I'm getting the solution as $0,\frac 12, 2 ,3, \frac 25$ . That makes sense to me as the first expression is always positive, $$\because x^2-2x+1+1=(x-1)^2+1> 0$$ and the other two expressions give two cases which on being solved gives 4 solutions in total as it's a quadratic. $$x^2-2x+2-|(x-2)(2x-1)|-|(x-0)(x-3)|=0$$ Using wavy curve on the two expressions inside mod, we get that they are negative for $x\in (\frac 12,2)$ and $x\in [0,3]$ respectively. So, we can conclude that if the first expression is negative then the second expression is negative too as $[\frac 12,2]$ is in $[0,3]$ . So we can get 3 cases: Both are positive, or both are negative, or the first one is positive and the second one is negative. Solving for each gives the solutions which I said I've received earlier in the post. So where is the range coming from? Am I interpreting the mod operator wrong? Or is my book wrong?","['algebra-precalculus', 'quadratics', 'inequality']"
4470094,Lemma 6 in the proof of Schwartz's theorem,"I'm reading Chapter 10 of Sotomayor's book on Schwartz's theorem on flows on a compact two-dimensional surface, which follows the original proof . My first question is here: The following statement is found in either of the two links above. Letting $W$ be open in $(-1,1)$ and such that $G\subset W\subset \overline{W}\subset V$ , we summarize te properties of $f$ : $G=(-1,1)-\displaystyle\bigcup_{i=1}^\infty (a_i,b_i)\subset W$ $f(G)=G$ $f^k(v)=v$ and $v\in G$ implies $k=0$ $G$ is minimal for $f$ , in the following sense that $G$ does not contain any closed proper subset $K$ such that $f(K)=K.$ I manage to understand the test that is done in Sotomayor's book until before the red box, I just don't understand that very well. I know that there must exist a neighborhood $\tilde{V}$ of $\varphi(t_0,p)$ such that $\tilde{V}\cap\tilde{K_0}=\emptyset$ . But I don't know how to get to that part of red. Just that part of red is what leads to a contradiction. On the other hand, I also have problems understanding the proof of Lemma 6: There exists a interval $(a,b)$ such that $a<b;\;a,b\in G$ and $f^k((a, b))\subset W$ for all $k\geq 0$ . Assuming that $f$ is monotone. At one point in the demonstration they mention the following, $$
s_0\in G-K_1\Leftrightarrow\text{ for all }\epsilon>0\text{ we have }\;G\cap (s_0-\epsilon,s_0)\neq\emptyset\wedge G\cap (s_0,s_0+\epsilon)\neq\emptyset
$$ I understand its use but I don't know how to get to test that if and only if mentioned above. Thanks to anyone who can help me understand this demo. Please be clear and if possible detailed to understand the essence of this demonstration.","['ordinary-differential-equations', 'dynamical-systems']"
4470101,"In an equilateral triangle, infinite line segments connect a vertex to the opposite side. If the product of the lengths converges and >0, what is it?","In an equilateral triangle, line segments connect a vertex to $n$ uniformly distributed points on the opposite side, including points at the ends. Assuming $\lim_{n\to\infty}\text{(product of lengths of line segments)}$ exists and is positive, what is the value of the limit? My attempt: Let $x=\text{side length}$ . The limit expression is: $$L=\lim_{n\to\infty}\prod_{k=0}^n{x\sqrt{\frac{3}{4}+\left(\frac{k}{n}-\frac{1}{2}\right)^2}}$$ The idea is to find $x$ so that the limit exists and is positive, and then evaluate the limit. I took the log of the product to change it to a series of logs, then used the Maclaurin expansion of $\ln{(1+x)}$ and the fact that $\sum_{k=0}^n{k^p}=\frac{1}{p+1}n^{p+1}+\frac{1}{2}n^p+...$ , but the series turned into a mess. I conjecture that $x=e^{1-\frac{\pi}{2\sqrt{3}}}\approx{1.098}$ and that (surprisingly) $L=x$ . (The basis of my conjecture is that I solved a similar but easier question: In a $45^{\text{o}}$ right triangle, line segments connect a $45^{\text{o}}$ vertex to $n$ uniformly distributed points on the opposite side, including points at the ends. Assuming $\lim_{n\to\infty}\text{(product of lengths of line segments)}$ exists and is positive, what is the value of the limit? Using the method I described above, I found that the length of the leg of the triangle is $(2^{-1/2})e^{1-\frac{\pi}{4}}\approx0.876$ and the limit is $(2^{-1/4})e^{1-\frac{\pi}{4}}\approx1.042$ . Then I used desmos to work out my conjecture.) Context: I was thinking about infinite products of areas and lengths in various shapes, and I came up with this question. My background: high school math teacher.","['geometry', 'real-analysis', 'triangles', 'plane-geometry', 'limits']"
4470136,Solving $\sqrt{x+3}+\sqrt{5-x}-2\sqrt{15+2x-x^2}=-4$,"Solve the following equation: $\sqrt{x+3}+\sqrt{5-x}-2\sqrt{15+2x-x^2}=-4$ Since real solutions are to be found, the domain of $x$ is $[-3; 5]$ . I immediately found that $15+2x-x^2$ can be factored to $(5-x)(x+3)$ , this gave: $\sqrt{x+3}+\sqrt{5-x}-2\sqrt{(5-x)(x+3)}=-4$ Squaring both sides was too complicated so I tried to substitute $a=\sqrt{5-x}$ for $a \in [0; \sqrt{8}]$ and $b=\sqrt{x+3}$ for $b \in [0; \sqrt{8}]$ , this gave the new multivariable equation: $a+b-2ab=-4$ Another thing I noticed was $a^2+b^2=8$ , together with the above equation, I got this system of equations: $\left\{ \begin{array}{l}
a + b - 2ab =  - 4\\
{a^2} + {b^2} = 8
\end{array} \right.$ . Solving this by elimination is quite difficult for me. This problem needs to be solved using algebra so I wonder how do I continue with this or are there any better way to solve this algebraically?","['algebra-precalculus', 'systems-of-equations', 'radicals']"
4470168,Differentiate under the integral sign,"Let $\Sigma\subset \Bbb{R}^n$ be some compact hypersurface without boundary in $\Bbb{R}^n$ ,fixed $v = -H$ where $H$ is mean curvature vector, assume we have a family of hypersurface $$\Sigma_{s} = \{x+sv(x)\mid x\in \Sigma\}$$ Prove the differentiation under the integral sign formula $$\frac{d}{dt}\int_{\Sigma_t} u = \int_{\Sigma_t} u_t -\int _{\Sigma_t}u|H|^2$$ where $H$ is the mean curvature vector. My attempt using the differentiation under the integral sign formula in calculus we have $$\frac{d}{dt}\int_{\Sigma_t}u\ dV_g=  \int_{\Sigma_t}u_t  \ d V_g + \int_{\partial\Sigma_t} u \langle v,n\rangle\ dS$$ Correct where $v$ is the velocity direction of the change of $\Sigma _t$ and $n$ is the out normal direction of $\Sigma_s$ . However, as $\Sigma_t$ does not has boundary, the second term on the right hand side is zero?Sorry for the silly question, I can't see where goes wrong with my reasoning?","['manifolds', 'multivariable-calculus', 'differential-geometry', 'real-analysis']"
4470270,What is the flaw in this approach?,"$12$ delegates exists in three cities $C_1,C_2,C_3$ each city having $4$ delegates. A committee of six members is to be formed from these $12$ such that at least one member should be there from each city. My method: I first selected one from each city which can be done in $4 \times 4 \times 4=64$ ways and then remaining $3$ can be chosen from the left over $9$ in $\binom{9}{3}=84$ ways. So the total ways is $64 \times 84=5376$ . I know its wrong but where did I go wrong?","['permutations', 'binomial-coefficients', 'combinatorics']"
4470297,"Find all power series $f$ such that $\mathbb{C}[[x, y]]/(f(x, y))\cong \mathbb{C}[[x, y]]/(xy)$","Find all power series $f$ such that $\mathbb{C}[[x, y]]/(f(x, y)) \cong  \mathbb{C}[[x, y]]/(xy)$ . The isomorphism $\phi$ should be identical on $\mathbb{C}$ , i.e., $\forall c \in \mathbb{C} \subseteq \mathbb{C}[[x, y]]/(f(x, y))$ , $\phi(c) = c$ . This problem is the third problem of the 2018 Alibaba math contest (final) —— Algebra track. It is not an ongoing contest. My only attempt is to show the power series of the Right Hand Side (RHS) are in the form $\sum\limits_{i = 0}a_ix^i + \sum\limits_{j = 0}b_iy^j (xy = 0)$ but I do not know how to proceed. Does this problem require some advanced theories or theorems? Is the complex field $\mathbb{C}$ anything special?","['contest-math', 'ring-theory', 'abstract-algebra']"
4470325,Find the image of $D=\{z\in \mathbb{C} : |z|\leq1 \ \text{and}\ 0\leq \text{arg}\ z <\pi\}$ under $f (z)=z^2$.,"Let $f(z) = z^2$ . Find the image of $D=\{z\in \mathbb{C} : |z|\leq1 \ \text{and}\ 0\leq \text{arg}\ z <\pi\}$ under $f$ . My attempt: If we draw $D$ then we get $D = \ \text{the upper half disk}\ \setminus [-1,0)$ . Let $z = re^{i\theta}$ where $r\in [0,1]$ and $\theta \in [0, \pi)$ . Hence, $w=z^2 = r^2 e^{2i\theta}$ . Take $\rho= r^2$ and $\alpha = 2\theta$ . We clearly see that $\rho \in [0,1]$ and $\alpha  \in [0, 2\pi)$ . Hence the image would be $\bar{\Delta} = \{z\in \mathbb{c} : |z|\leq 1\}$ . Am I correct? Thanks.","['complex-analysis', 'functions']"
4470330,Scalar curvature calculating problem,"I have a problem understanding calculating Ricci and scalar curvature. By definition: for every $p \in M$ and $x,y \in T_pM$ with respect to any orthonormal basis { $e_1..e_n$ } Ricci curvature is $$Ric(x,y)=\sum_{j=1}^{n} g(R(e_j,x)y,e_j)$$ And scalar curvature is $$\tau(p)=\sum_{j=1}^{n} Ric(e_j,e_j)$$ For example, Riemannian metric $g$ on torus is given with $ds^2=dx^2+(3+\cos(x))^2dy^2$ .
After calculating Christoffel symbols I have: $\Gamma_{12}^{2}=- \frac{\sin(x)}{3+\cos(x)}$ , $\Gamma_{22}^{1}=\sin(x)(3+\cos(x))$ and others are zero. Calculating gives: $$R(e_1,e_1)=g(\frac{\cos(x)}{3+\cos(x)}e_1,e_1)$$ $$R(e_2,e_2)=g(\cos(x)(3+\cos(x))e_2,e_2)$$ and for orthonormal basis $g(e_j,e_j)=1$ so $$\tau(p)=R(e_1,e_1)+R(e_2,e_2)=\frac{\cos(x)}{3+\cos(x)}+\cos(x)(3+\cos(x))$$ and this have be twice as Gaussian curvature, but it isn't. It looks like term $\cos(x)(3+\cos(x))$ should be multipled with $g^{22}$ but why?","['curvature', 'riemannian-geometry', 'differential-geometry']"
4470334,Is there a darboux fuction that doesn't have a primitive?,"We know that if f is a differentiable fuction, then f' is a Darboux fuction due to the Darboux theorem. However, the Darboux theorem isn't know as the ""characterization of fuctions with primitive theorem"" so I guess there have to be at least one fuction that is Darboux that doesn't have a primitive. I'm trying to find an example but I don't get anything. Is this an still open problem? Is there any property in between ""darboux"" and continuous that can caracterizate if a fuction has a primitive?","['integration', 'calculus', 'derivatives']"
4470356,"Is any function on $(\mathbb S^1)^4$ which is invariant under isometries, invariant under permutations?","$\newcommand{\S}{\mathbb{S}^1}$ This might be silly, but here it goes: Let $E:(\mathbb{S}^1)^4 \to \mathbb{R}$ be a smooth function. Let $f \in \text{Iso}(\mathbb{S}^1)$ be an isometry of $\mathbb{S}^1$ . $f$ induces a map $\tilde f:(\mathbb{S}^1)^4 \to (\mathbb{S}^1)^4$ , given by $$
\tilde f(x_1,x_2,x_3,x_4):=\big(f(x_1),f(x_2),f(x_3),f(x_4)\big).
$$ Suppose that $E \circ \tilde f = E$ for any $f \in \text{Iso}(\mathbb{S}^1)$ . Is it true that $$
E(x_1,x_2,x_3,x_4)=E(x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)},x_{\sigma(4)})
$$ for any permutation $\sigma \in S^4$ and any $(x_1,x_2,x_3,x_4) \in (\mathbb{S}^1)^4$ ? I guess that the answer is negative, but I am not sure.","['symmetric-functions', 'examples-counterexamples', 'multivariable-calculus', 'symmetry', 'differential-geometry']"
4470370,Chern's definition of G-structure,"Section 4, page 10 of The geometry of G-structures by S. S. Chern,  Bull. Amer. Math. Soc. 72(2): 167-219 (March 1966), the definition of G-structure is somewhat vague, and I have have the impression that it is wrong. Let $T$ be an $n$ -dimensional real vector space and let $T^*$ be its dual space. Denote their pairing by $\langle y, \xi \rangle \in R$ , $y \in T$ , $\xi \in T^*$ . We let $GL(n,R)$ act on $T$ on the left and on $T^*$ on the right, so that the following relation holds: $$\langle gy, \xi \rangle = \langle y, \xi g \rangle, \quad \quad g \in GL(n,R).$$ The tangent bundle over $M$ has the local charts $(x,y_U)$ , $x \in U$ , $y_U \in T$ , which are the local coordinates of the tangent vectors relative to $U$ . The local coordinates $(x, y_U)$ and $(x, y_V)$ in $U \cap V$ define the same tangent vector if and only if $y_U = g_{UV}(x)y_V$ , where $g_{UV} : U \cap V \to GL(n,R)$ . Consider a subgroup $G$ of $GL(n,R)$ ; we say that the structural gropu of the tangent bundle is reduced to $G$ , if all $g_{UV}(x) \in G$ . Such a reduction will simply be called a $G$ -structure. I cannot relate this definition to other ones found elsewhere (using the notion of principal G-bundles). Is it complete? Correct? What am I missing? The article by Chern is in the reference list of Wikipedia's article G-Structures on a manifold",['differential-geometry']
4470379,Average number of the maximum amount of fixed points of permutation in a partition of $S_n$,"Consider the symmetric group $S_n$ and, for each $\sigma \in S_n$ , let $f(\sigma)$ be the number of fixed points of $\sigma$ . Now let $g$ be
the permutation such that $g(i)=i+1$ for $i=1,...,n-1$ and $g(n)=1$ . We can partition $S_n$ in $(n-1)!$ sets of the form $$\sigma _g = \{\sigma, \sigma \circ g ,
 \sigma \circ g ^2 ,..., \sigma \circ g ^{n-1}\}$$ for all $\sigma \in
 S_n$ such that $\sigma(1)=1$ . Let $$h(\sigma _g) = \max \{f(\pi) \mid \pi
 \in  \sigma _g\}. $$ Show that the average of $h(\sigma _g)$ for all $\sigma \in S_n$ such that $\sigma(1)=1$ is $3$ if $n\rightarrow
 \infty$ . The only thing I can do is show that the average of $f(\sigma)$ is $1$ if $n\rightarrow
 \infty$ using the formulas in [1]. I also tried to adapt the approaches in [2], but with no success. At last, I attempted to rewrite the problem in terms of permutation matrices, where $f$ is the trace of the matrix and apply some properties, also without success. EDIT: so, from Carl Schildkraut's comment and also [3], it seems that this value explodes (although very slowly) as $n\rightarrow \infty$ , as it's most likely $$\left( \frac{\log(n)}{\log(\log(n))} \right)$$ asymptotically, because, in each set $\sigma_g$ , the distribution of the number of fixed points likely is $n$ Poisson distributions, so the expected maximum, according to [3], should be like that.
So this question is most likely wrong and this limit may well be unbounded. [1] https://en.wikipedia.org/wiki/Rencontres_numbers [2] Fixed points of permutation groups [3] https://arxiv.org/pdf/0903.4373.pdf","['permutations', 'permutation-cycles', 'combinatorics', 'symmetric-groups', 'group-theory']"
4470382,Can one separate variables as a sum in PDE?,"I always see solutions for some PDEs (e.g; wave equation) decomposed as a product, i.e; $u(x,y)=X(x)Y(y)$ for example. But I have never seen solutions written as $u(x,y)=X(x)+Y(y)$ for example. Is it possible to assume solutions of this kind and also construct the general solution using the superposition principle?","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
4470473,Inverse Laplace transform of $\dfrac{e^s}{s(e^s+1)}$ [duplicate],This question already has answers here : inverse laplace transform of $\frac{1}{s(e^s+1)}$ (2 answers) Closed 1 year ago . The original problem is to solve $$\mathcal{L}^{-1}\left\lbrace\frac{e^s}{s(e^s+1)}\right\rbrace.$$ Doing partial fractions $$\frac{e^s}{s(e^s+1)}=\frac{1}{s}-\frac{1}{s(e^s+1)}$$ the problem reduces to solve the inverse Laplace transform of the last member. Using Heaviside function the problem is the same. Any ideas?,"['inverse-laplace', 'laplace-transform', 'ordinary-differential-equations']"
4470490,Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$?,"Let $F\subset\Bbb{R} $ intersect every uncountable $\mathcal{F}_{\sigma}$ set. $B\subset \Bbb{R}$ is said to have the property of Baire if $B=U\triangle M$ where $U$ is open and $M$ is meager. Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$ defined above ? Edit $1$ : $F$ intersects every uncountable $F_{\sigma}$ set iff F intersects every uncountable closed set. Edit $2$ : If $f:\Bbb{R}\to \Bbb{R}$ is additive i.e $f(x+y) =f(x) +f(y) $ then $f$ is $\Bbb{Q}$ -linear.In other words $f$ is a linear function if we consider $\Bbb{R}$ as a vector space of $\Bbb{Q}$ . $f(x+y) =f(x) +f(y) $ $f(qx) =qf(x) $ $\forall x, y\in\Bbb{R} $ and $\forall q\in\Bbb{Q}$ $\underline{ \text{Case }  1}$ : $( f \text{ is } \Bbb{R} \text{ linear}) $ Then $f(x) =ax$ for some $a\in \Bbb{R}$ . Hence clearly $f$ is additive onto map $($ and moreover $f$ is a linear homeomorphism / topological isomorphism $)$ . But $f$ fails to hold the third property mentioned above. For an example ,if we  take $F=\mathcal{B}( \text{Bernstein set})$ then $f(F) =a\mathcal{B}$ doesn't have the property of Baire. $\boxed{\text{ Required function can't be $\Bbb{R}$ -linear}}$ $\underline{\text{Case }2} $ : $f$ is $\mathbb{Q}$ -linear but not $\Bbb{R}$ -linear. Points to be considered: If $g:\Bbb{R}\to \Bbb{R}$ defined by $g(x) =qx , q\in\Bbb{Q}$ is continuous then $g=f$ on $\Bbb{Q}$ implies $g=f $ on $\Bbb{R} $ i.e any continuous $\Bbb{Q}$ -linear map extends linearly on $\Bbb{R}$ . $\boxed{ \text{ So $g$ can't be a continuous on $\Bbb{Q}$}}$ A linear map is completely determined by it's action on the basis. So our task is reduced to construct a discontinuous linear map from the vector space $\Bbb{R}_{\Bbb{Q}}$ to $\Bbb{R}$ . A non-continuous solution of an additive function ( called ugly function) is non-measurable. Conjecture $1$ : $f:\Bbb{R}\to \Bbb{R} $ is $\Bbb{Q}$ -linear and non-measurable function. Then $\exists F\subset \Bbb{R}$ closed uncountable set such that $f(F) $ doesn't have the property of Baire. There exists a discontinuous additive function $f:\Bbb{R}\to\Bbb{R}$ satisfying Darboux property ( It can be shown by defining $f$ linealy on a hamel basis of $\Bbb{R}_{\Bbb{Q}}$ and then extending additively on $\Bbb{R}$ ) Conjecture $2$ : The Darboux property of $f$ ( discontinuous $\Bbb{Q}$ -linear function) is sufficient to conclude that $f(F) $ have property of Baire for every closed uncountable set $F$ . Any second category (non meager) subset of $\Bbb{R}$ contains a set that fails to have the property of Baire. $[$ Suppose $ A\subset \Bbb{R}$ non meager. Then $A\cap \mathcal{B} $ or $A\cap \mathcal{B}^c$ atleast one of the set doesn't have the B.P otherwise both would be meager and eventually $A$ would be meager $]$ $f:\Bbb{R}\to \Bbb{R}$ is an additive onto function such that $f$ is not injective then $f^{-1}(y)$ is dense in $\Bbb{R}$ for all $y\in\Bbb{R}$ . Question: Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$ defined above ? Here is the MO post of this question.","['descriptive-set-theory', 'real-analysis', 'functional-analysis', 'axiom-of-choice', 'general-topology']"
4470542,Good reference for self study of Gauge theory [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question I am looking for the shortest way possible to study basic gauge theory. I am looking for some inspiring survey notes like this one: Christian Bär, Gauga Theory (rather than the great books by Kobayashi & Nomizu or Steenrod). Dealing, in particular, with: Problem of lifting 'basic' paths to horizontal paths. Why the reduction of the structural group is important. Links between: trivial bundle, flat bundle, $G$ -local system","['fibration', 'reference-request', 'holonomy', 'gauge-theory', 'differential-geometry']"
4470549,Finding a unit vector $v$ that makes only one quadratic form vanish,"I was reading a proof on the non-convexity (even locally) of loss landscape in high-dimensional neural networks. Specifically, in the paper , it seems like the proof of proposition 2 at some point uses the following fact: Let $A,B \in \mathbb{R}^{m\times m}$ be symmetric matrices. Suppose that $\operatorname{rank}(A) \leq n$ and that $\operatorname{rank}(B) > 2n$ . Then we can always find a unit vector $v$ such that $v^{\intercal}Av = 0$ but $v^{\intercal}Bv \neq 0$ . Is there a reason we should expect such a $v$ to exist? If so, is the condition $\operatorname{rank}(B) > 2n$ necessary? Or is it just sufficient? Simultaneous vanishing of quadratic forms seems to be similar but not quite the same thing, and I wasn't able to find anything else about this. Any insight into this would be appreciated! Edit: Here's what I've come up with thus far. The problem is equivalent to showing that there exists a vector $v$ such that $v\in null(A)\cap col(B)$ . This is equivalent to showing that there is a zero eigenvalue to the generalized eigenvector problem: $$    Av = \lambda Bv   $$ With $\lambda = 0$ . Let $U = null(A)$ , $V = col(B)$ . Since $rank(A) \leq n$ , $dim(U) > m-n$ and $dim(V) > n$ . Thus, $$    dim(U+V) = dim(U) + dim(V) - dim(V\cap U)
$$ Since $dim(U + V) \leq m$ and $dim(U) + dim(V) > m$ , then $dim(V \cap U) \geq 1$ , showing that $V \cap U \neq \emptyset$ .","['generalized-eigenvector', 'linear-algebra', 'quadratic-forms']"
4470561,Is (Hausdorff $\implies$ Unique Limits) intuitionistic?,"It's well known that in Hausdorff topological spaces, limits of convergent sequences are unique and essentially the only proof I know (which appears very natural) is: Suppose $(x_i)$ is a convergent sequence with distinct limits $x,y$ in a Hausdorff top. space $(X, \mathcal{T})$ . Then there must be disjoint open sets $U,V\in \mathcal{T}$ with $x\in U$ and $y \in V$ . But by definition of convergence, the sequence then must eventually lie (and stay) in $U$ and also eventually lie in $V$ and this is a contradiction. Now notice that this appears to use the law of double-not-reduction or non-intuitionistic logic: if $P$ represents the statement that ""Sequences in $X$ have unique limits"" then our proof proceeds by assuming $\neg P$ and deducing a contradiction so we obtain $\neg \neg P$ , which is classically equivalent to $P$ . Notice also that using contradiction in this way seems naively unavoidable as to apply Hausdorffness we must assume $x\not = y$ ... Is it possible to prove the uniqueness of limits in Hausdorff spaces intuitionisticly or does this implication not hold in general in non-classical logics?","['general-topology', 'intuitionistic-logic']"
4470584,"If F < E are fields, how is it possible for a representation X, to be irreducible as an F-representation, but reducible as an E-representation?","Studding Character theory, and been bouncing back and forth between reading Dummit and Foote, and Character theory of finite groups by Martin Issacs. In section 18.1 of Dummit and Foote, we are given a Bijection: $$\{V: V  \text{ is an } FG-\text{module}\} \leftrightarrow \{ (\phi, V): \text{is an } F-\text{ represention from } G \rightarrow GL(V)\}$$ at the start of chapter 2 of Character theory of finite groups, It states that it is possible to have fields $F < E$ , and an irreducible $F$ -representation $X$ , such that $X$ is reducible as an $E$ -representation. I don’t understand how $X$ can be irreducible as an $F$ -representation and reducible as an $E$ -representation. Since if $X$ was a representation with corresponding module $V$ , and if $\phi$ and $\psi$ were $E$ -representations with corresponding $E$ -modules $W$ and $U$ , such that $X = \phi + \psi$ . Then shouldn’t $W$ and $U$ also be $F$ -modules and $V = W + U$ , so there would have the be $F$ -representations that sum up to $X$ ? Somone gave me the example: "" $X : C_3 \rightarrow GL_2(\mathbb{R})$ ,  where $C_3 = \langle g \rangle$ is cyclic of order $3$ and $X(g)$ is the matrix whose first row is $[ 0 \ \ 1 ]$ and
whose second row is $[ -1 \ \  -1 ]$ .  The eigenvalues of this matrix are not real (they are the two primitive cube roots of $1$ ). Therefore, $X$ is irreducible as an $\mathbb{R}$ -representation of $C_3$ . Since $X(g)$ is diagonalizable over the complex numbers $\mathbb{C}$ , it follows that as a representation, $X : C_3 \rightarrow GL_2(\mathbb{C})$ reduces.  So, over $\mathbb{C}$ ,  the representation is equivalent to a block diagonal sum of two linear representations of $C_3$ I understand the example, but I'm still not seeing where and why my logic fails?","['characters', 'abstract-algebra', 'representation-theory']"
4470585,"Where's the error in my ""proof"" about iterated multivariable functions?","I've tried to prove a property of a kind of iterated multivariable functions I am playing with and I thought I did it, but now I've found a lot of counter examples. I'm still going to school, but I hope the formalism isn't to horrible. I started by defining injectivity, surjectivity and bijectivity with respect to a single variable for multivariable functions: Def : A funtion $f(x,y):\mathbb{R}²\to\mathbb{R}$ is injective/surjective/bijective  with respect to $x$ if and only if $f_y(x)=f(x,y)$ is injective/surjective/bijective for all $y\in\mathbb{R}$ . I thought that I proved the Proposition that $f(x_0,y_0)=x_0$ was true if $f(f(x_0,y_0),y_0)=x_0$ is true and $f$ is bijective w.r.t. x and continuous, but I found some counter examples. One of them is $f(x,y)=\cos(y)-x³$ . I wanted to illustrate this with some pictures, but sadly I haven't gotten any reputation yet, so I can't post any. First I ""proved"" a Lemma : If the the system of equations $$f(x)=y\\f(y)=x$$ has any solutions and $f$ is continuous, x=y is true for at least one of them (this seems kind of obvious, but I couldn't find a proof online). Proof by contradiction: assume $f(x)\neq x$ for all $x\in\mathbb{R}$ . It follows from continuity that $f(x)<x$ for all $x\in\mathbb{R}$ or $f(x)>x$ for all $x\in\mathbb{R}$ . For $f(x)<x$ this means that $f(f(x))<f(x)<x$ , but the original system of equations means that $f(f(x))=x$ . This is a contradiction. The proof is exactly the same for the case $f(x)>x$ . this proves the Lemma. Now I tried to prove the Proposition . Proof : Assume $f$ is bijective w.r.t $x$ and continuous and $f(f(x_0,y_0),y_0)=x_0$ . It follows from bijectivity that there is exactly one $x':=f(x_0,y_0)$ that solves this with $f(x',y_0)=x_0$ . Fix $y_0$ and set $f_y(x)=f(x,y_0)$ . Then we have $f_y(x_0)=x'$ and $f_y(x')=x_0$ . It follows from the Lemma that $x_0=x'$ for at least one solution, but since $f_y$ is bijective this is the only solution. This means that $f(x_0, y_0)=x_0.$ q.e.d. But as I said earlier there are some counter examples, though it does seem like $f(x_0, y_0)=x_0$ is really true when $f$ is bijective w.r.t. $x$ and continuous and $f(f(f(x_0,y_0),y_0),y_0)=x_0$ , though I haven't proved that yet. Thank you for your help in advance.","['multivariable-calculus', 'fake-proofs', 'analysis']"
4470600,How to show that controllability and observability are not affected by replacing $A$ with $(A+αI)$?,"I found a thread with the same question. The answer there: ""Controllability: there does not exist a left-eigenvector for $A$ that is orthogonal for $B$ . For your problem, for any $x$ such that $xA=λx, \  xB≠0$ . Since the eigenvectors of A are also the eigenvecors of $A+αI$ , i.e., $x(A+Iα)=x(λ+α)$ , you have $xB≠0$ ."" I don't understand what is the connection here. Why is it obvious that the eigenvecors of $A$ are also the eigenvecors of $A+αI$ ? Even if that is true, why does it mean that $x(A+Iα)=x(λ+α)$ ? I have read that replacing $A$ with $(A+αI $ ) can ruin stabilizability. But why can it ruin that and not controllability and observability?","['matrix-rank', 'eigenvalues-eigenvectors', 'control-theory', 'analysis', 'linear-control']"
4470620,Eigenvalue bounded by sum of row and column,"Let $A =(a_{ij})$ be a matrix and $R_i=\sum_j |a_{ij}|$ and $C_j=\sum_i |a_{ij}|$ Let $\lambda$ be an eigen value of $A$ . Prove that there exists a $k$ such that $$|\lambda| \leq \sqrt{R_k C_k}$$ Any hints? Edit: What I tried so far we have $AX=\lambda X$ where $||X||=  X^tX=1$ $$|\lambda|^2 = X^tA^tAX = \sum_i\sum_j x_i(A^tA)_{i,j}x_j = \sum_p\sum_ix_ia_{ip}\sum_jx_ja_{pj} \leq \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{R_p}\sqrt{\sum_j|x_j|^2|a_{pj}|}\sqrt{C_p} \text{     (Cauchy schwartz)}$$ Continuing: Let $R_kC_k=\max_p{R_pL_p}$ $$\begin{align} |\lambda|^2 &\leq \sqrt{R_kC_k} \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{\sum_j|x_j|^2|a_{pj}|} \\
 &\leq \sqrt{R_kC_k}\sqrt{\sum_p\sum_i|x_i|^2|a_{ip}|\sum_p\sum_j|x_j|^2|a_{pj}|}\\ &=\sqrt{R_kC_k}\sqrt{\sum_i|x_i|^2\sum_p|a_{ip}|\sum_j|x_j|^2\sum_p|a_{pj}|} \\
&\leq...
\end{align}$$ I'm still stuck here...","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4470623,"Vector identity for $\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi(\vec r(t),t) $","Suppose I have a scalar field $\varphi\left(\vec r(t), t\right)$ where $\vec r$ is a vector in $\mathbb R^3$ that depends on the parameter $t$ . I'd like to figure out what $\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi$ is. I start as follows, $$\nabla \varphi = \partial_i \varphi \hat{e}^i $$ (which I attempt to write in Einstein notation) From there, I take the time derivative component-wise and using the directional derivative I get $$\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi = \left( v_j \partial _j \partial_i \varphi   +\partial _t \partial _i \varphi  \right)\hat{e}^i$$ where $v^\nu = \dfrac{\mathrm d r^\nu }{\mathrm dt} $ . I can rewrite this as (where $^T$ denotes the transpose) $$\dfrac{\mathrm d\nabla\varphi }{\mathrm dt} = \pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }\vec v+\dfrac{\partial}{\partial t}\nabla \varphi$$ and am wondering if there is a way for me to simplify this further, in particular if there is a notation or name for the $\pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }$ matrix.","['multivariable-calculus', 'calculus', 'linear-algebra']"
4470627,the quotient of a sphere by a contractible closed subset in it,let $A \subsetneq S^n$ be a contractible closed subset of the $n$ -dimensional sphere. my visual intuition for the two-dimensional sphere tells me there should be a homeomorphism $S^n / A \cong S^n$ . is it true? how can one prove this?,['general-topology']
4470652,"What does ""$A^b \bmod c$, where $A$ is a square matrix"" mean? What is the modulus of a matrix?","I was reading Wikipedia's ""Modular exponentiation"" entry . It made sense to me until I got to the part about Matrices . What does "" $A^b \bmod c$ , where $A$ is a square matrix"" mean? What is the modulus of a matrix?","['matrices', 'modular-arithmetic']"
4470657,how to calculate the area of ​the painted circle?,The problem begins by saying that it is a square with side $a$ and ...blah blah blah. They ask me for the area of ​​the small circle but I cannot find its radius. manage to find that the radius of the larger circle is $$\frac{3}{8}a$$ and  the height at which the two quarter circle intersect is $$\frac{\sqrt{3}a}{2}$$ (forming a system of equations with the circles) I can't think of how to calculate a small piece of the segment that remains between the intersection of the quarter circle and the small circle to find. Can you help me?,"['euclidean-geometry', 'algebra-precalculus', 'geometry']"
4470665,Miklos Schweitzer 1968 P11 by Renyi,"Source : Miklos Schweitzer 1968, Problem 11 Let $ A_1,...,A_n$ be arbitrary events in a probability field. Denote by $ C_k$ the event that at least $ k$ of $ A_1,...,A_n$ occur. Prove that $$\prod_{k=1}^n P(C_k) \leq  \prod_{k=1}^n P(A_k).$$ Attempt : I believe the first step to tackle this would be to employ Bayes rule: $$P(C_n)=P(A_1\cap\cdots\cap A_n)=P(A_2\cap\cdots\cap A_n|A_1)P(A_1)$$ as here we accrue a $P(A_1)$ term times something less than or equal to one which is promising for the inequality to show. Then my guess would be to continue to establish similar correspondences with the other $P(A_i)$ terms. I try that by further employing Bayes to get $$P(C_n)=P(A_1)P(A_2|A_1\cap A_3\cap\cdots\cap A_n)P(A_3\cap\cdots\cap A_n|A_1).$$ However this seems to lead to a dead end (at least with $C_n$ ) because I'm conditioning the other $A_k$ terms on some measurable subset of the sample space, so I can't obtain the other $P(A_k)$ terms unconditioned without magic. Another approach or follow-up that seems messy is to use sub-additivity of $P$ to look at the $C_k$ terms for $k<n$ : $$P(C_k)=P\left(\bigcup\limits_{i_1\neq i_2\neq\cdots\neq i_k} A_{i_1}\cap\cdots\cap A_{i_k}\right)\leq \sum\limits_{i_1\neq i_2\neq\cdots\neq i_k} P\left(A_{i_1}\cap\cdots\cap A_{i_k}\right)$$ but um, aside from applying Bayes here to get $P(A_k)$ times some constant $\leq 1$ , this doesn't help as we are adding these factors, not multiplying. Question : I would appreciate tactics/hints here (preferably related to my line of reasoning if possible), not full blown solutions - thanks!","['contest-math', 'probability-theory', 'probability']"
4470678,"Prove that $A =\{x \in X\mid f(x) \ge 2022\}$ is closed in $X$, by means of sequences.","Let $f:X\to \mathbb{R}$ be a continuous function, where $(X,d)$ is a metric space, and $\mathbb{R}$ has the usual metric, $|x-y|$ . Prove that the set $A=\{x\in X:f(x)\geq 2022\}$ is a closed set in $X$ , by means of sequences. I was thinking of proving that the complement of $A$ is open. So, I did the following: Let $0<r<x_0-2022$ . If $x\in B(x,r)\implies x\in A^c$ $$d(x,x_0)=|x-x_0|<r \implies x_0-r<x<r+x_0$$ But $$\begin{align*}
0<r<x_0-2022
&\implies -r>2022-x_0 \\
&\implies -r-x_0>2022\\
&\implies x>2022\\
&\therefore A^c \text{ is open} \\
&\therefore A \text{ is closed}
\end{align*}$$ But I think I am not using the sequences.","['continuity', 'sequences-and-series', 'metric-spaces', 'real-analysis']"
4470682,Definite integral $ \int _{0}^{\infty } x\cdotp \tanh( 2x) \cdotp \ln(\coth x)\mathrm{d} x$,"I want to show that $\displaystyle \int\limits _{0}^{\infty } x\cdotp \tanh( 2x) \cdotp \ln(\coth x)\mathrm{d} x=\frac{\pi ^{2} \cdotp \ln( 2)}{2^{4}}\tag*{}$ I tried integration by parts, Feynman trick with no luck. It just gets complicated.","['integration', 'improper-integrals', 'definite-integrals', 'hyperbolic-functions', 'calculus']"
4470736,Question about definition of normal bundle from the quotient space,"According to Wikipedia, the definition of normal bundle is defined as, Defintion $1$ . [Normal bundle] Let $(M,g)$ be a Riemannian manifold, and $S\subset M$ a Riemannian submanifold. For a given $p \in S$ , a vector $n \in T_pM$ to be  normal to $S$ if whenever $g(n,v)=0$ for all $v \in T_pS$ . Then the set $N_pS$ of all such $n$ is called the normal space to $S$ at $p$ . $NS:= \coprod _{s \in S}N_pS$ is called the total space of normal bundle to $S$ at $p$ . ............. $(*)$ From the above definition, I understand $$N_p S := \left\{ n \in T_pM : g(n,v)=0~ for~ all~~ v ~\in T_pS \right\}$$ For example, let $M=\mathbb{R}^2$ and $S=S^1$ and pick a point $p\in S$ . Then, since $S$ is embedded in $M$ , naturally $p\in M$ . and $T_pM$ is also well defined. then since the vector $n$ is easily constructed ( clearly the every vector $v$ who lives in $T_pM$ is perpendicular to $n$ , literally, $g(n,v)=0$ . ) and like the second bullet, $NS$ is also well defined for any point $p \in M$ . (The following image is just visualizing my description. the pink vector is one of element of $N_pS $ ) Meanwhile, how about the formal definition of normal bundle? Even though such definition is slightly different from each source, essentially, the basic idea seems to use a quotient space ,  However, I think that the formal definition contradicts the first definition , $(*)$ . To begin with, based on wikipedia description, one can define a normal bundle of $N$ in $M$ , by at each point of $N$ ,
taking the quotient space of the tangent space on $M$ by the tangent
space on $N$ . For a Riemannian manifold one can identify this quotient
with the orthogonal complement, but in general one cannot... I will define for convenience, Defintion2. [General definition of normal bundle] Let $(M,g)$ be a Riemannian manifold, and $S\subset M$ a Riemannian submanifold. Then the quotient space $TM/TS$ is called a normal bundle to $S$ at $p$ . For example, also consider $M=\mathbb{R}^2$ and $S=S^1$ and metric tensor still is Riemannian metric, $g$ . if I pick two vector bundle $v,w \in TM$ , the equivalence relation ~ is given, $$v\sim w  ~if~and~only~if~ v-w \in TS  ...... (**)$$ Then we can viusalize both vector and $v$ and $w$ like the below picture, and since $\sim$ is equivalent relation, we consider a representation $\nu \in TM/TS $ . Then the representation $\nu$ would be a normal bundle. Of course, when considering $(**)$ , the representation is descirbed $$\nu=\left\{ w + \alpha u : w \in TM , \alpha \in \mathbb{R}, u \in TS \right\}$$ However, when comparing to the first definition, $\nu $ clearly does not represent normal vector . Obviously, for any point $p \in S$ , then $w+u$ is not perpendicular to the tangential vector $T_pS$ Therefore, I cannot understand why the formal definition of normal bundle , Defintion2 ,  is a reasonable statement when considering Definition1 .",['differential-geometry']
4470751,Proving a result in maximum likelihood inference,"I need help proving a result shown in a paper. I am reading Assessing the Quadratic Approximation to the Log Likelihood Function in Nonnormal Linear Models by Salomon Minkin. The paper defines several concepts that are used in the argument, so I'll state those here: Definitions Let $\beta \in \mathbb{R}^m$ . Let $L(\beta;x)$ be a a log-likelihood function and $\hat{\beta}$ be the maximum likelihood estimator (MLE) of the parameter $\beta$ given data $x$ ( $x$ is a vector of observations) Let $Q(\beta;\hat{\beta},x)$ be the quadratic approximation to $L(\beta;x)$ around the $\hat{\beta}$ (i.e., the second degree Taylor approximation about $\hat{\beta}$ ). The argument uses the ""drop"" in the loglikelihood functions as you move away from their common maximizer at $\hat{\beta}$ Let $D_L(\beta):=D_L(\beta;\hat{\beta},x)=L(\hat{\beta};x) - L(\beta;x)$ be the drop in the $L$ at point $\beta$ . Similarly, $D_Q(\beta):=D_Q(\beta;\hat{\beta},x)=Q(\hat{\beta};\hat{\beta},x) - Q(\beta;\hat{\beta},x)$ be the drop in the quadratic approximation to $L$ at $\beta$ . Proof Plan The author then uses $D_L,D_Q$ in the main part of the argument, reproduced below: To me it's not clear/obvious that $\beta\in R_L(a_0-d) \implies \beta \in R_Q(a_0) \;\;\forall a_0 \leq a$ so I tried to prove this to myself. Let's start with $a_0=a$ . Here are the series of implications I am trying to prove (there may be a more straightforward approach). The first implication (marked with $(*)$ ) is the one I'm not sure about, but if I can prove it the rest of the proof here is pretty straightforward. I basically assumed it was true and continued on the proof, now coming back to shore up $(*)$ . $(*): \beta \in R_L(a-d) \implies \beta \in R_A(d)$ $(1): (*) \implies D_Q(\beta) < D_L + d$ $(2): \beta \in R_L(a-d)  \implies D_L(\beta) \leq a - d $ $(3): (2) \wedge (1) \implies D_Q(\beta) < D_L + d \leq (a-d) + d \implies D_Q(\beta) < a$ $(4): (3) \implies \beta \in R_Q(a) \implies R_L(a-d) \subset R_Q(a)$ So, assuming $(*)$ we've shown $\beta \in R_L(a-d) \implies \beta \in R_Q(a)$ . The generalization to $a_0\leq a$ is also straightforward due the definition of $R_Q$ . For any decreasing sequence of values $s_1>s_2>s_3...>s_n$ defines a decreasing sequence of sets in $\mathbb{R}^{m}$ (ellipsoids) $R_Q(s_1)\supset R_Q(s_2)\supset R_Q(s_3)...\supset R_Q(s_n)$ Substituting $a_0 < a$ for $a$ into $(*)$ and proceeding with $(1)-(4)$ gets us the general result stated in the paper: $$\beta \in R_L(a_0-d) \implies D_Q(\beta) < a_0 \implies \beta \in R_Q(a_0) \implies R_L(a_0-d) \subset R_Q(a_0) \;\;\forall a_0 \leq a$$ $\square$ Proving $(*)$ The jumping off point of my proof is $(*)$ , which I am not sure I can rigorously prove. I feel it needs to use the fact that $R_Q(a) \subset R_A(d)$ and that the regions $R_L(z)$ are connected (as mentioned in the above passage). My attempt $R_Q(a) \subset R_A(d) \implies |D_Q(\beta)-D_L(\beta)| < d\;\; \forall \beta \in R_Q(a)$ by the definition of $R_A(d)$ To show $R_L(a-d) \in R_A(d)$ , there are two cases: Case 1: $R_L(a) \setminus R_Q(a) = \emptyset$ $R_L(a) \setminus R_Q(a) = \emptyset \implies R_L(a) \subset R_Q(a) \implies R_L(a) \subset R_A(d)$ . By the definition of $R_L(z)$ , $R_L(a-d) \subset R_L(a) \implies R_L(a-d) \subset R_A(d)$ . Case 2: $R_L(a) \setminus R_Q(a) \neq \emptyset$ Let $G = R_L(a) \setminus R_Q(a)$ In this case, we know that $D_L(g) < D_Q(g)\;\;\forall g \in G$ : Proof : Assume $g \in G: D_L(g)-D_Q(g) = k > 0$ . This means that $D_L(g) = a+k$ which contradicts the fact that $g \in R_L(a)\;\;\;\square$ Since $D_L(g)< D_Q(g)$ over $G$ , and also that $R_Q(a) \in R_A(d)$ , then $\exists c<d:R_L(a-c) \subset R_Q(a)$ . By definition, $R_L(a-d) \subset R_L(a-c) \subset R_Q(a) \subset R_A(d)\;\; \square$ Turns out this appears to simultaneously prove the overall point I was trying to understand. Is this correct? Am I missing something?","['statistics', 'real-analysis', 'solution-verification', 'general-topology', 'probability-theory']"
4470791,Basis of solutions of $x^{2}y''-4xy'+6y=0$?,"What is the basis of solution space spanned by the solutions of the homogeneous ODE given by $$x^{2}y''-4xy'+6y=0$$ My work:
The solution is of the type $x^{m}$ . By solving the auxiliary equation for this ODE, which is $m(m-1)-4m+6=0$ implies $m=2,m=3$ .
Hence, the two solutions are $x^{2}$ and $x^{3}$ and thus the basis is { $x^{2}$ , $x^{3}$ }. But according to the solution I have the basis is { $x^{2}$ , $x^{3}$ , $x^{2}|x|$ }, but I actually want to know why $x^{2}|x|$ is explicitly specified as a solution?","['homogeneous-equation', 'ordinary-differential-equations']"
4470820,How to find $\int_{0}^{\pi} \ln (b \cos x+c)$ without using Feynman’s integration technique?,"I shall find the integral by Feynman’s Technique Integration on a particular integral $\displaystyle I(a)=\int_{0}^{\pi} \ln (a \cos x+1) d x,\tag*{} $ where $-1\leq a \leq 1.$ $\displaystyle \begin{aligned}I^{\prime}(a) &=\int_{0}^{\pi} \frac{\cos x}{a \cos x+1} d x, \\&=\frac{1}{a} \int_{0}^{\pi} \frac{(a \cos x+1)-1}{a \cos x+1} d x \\&=\frac{\pi}{a}-\frac{1}{a} \int_{0}^{\pi} \frac{d x}{a \cos x+1} \\&\stackrel{t=\tan \frac{x}{2}}{=} \frac{\pi}{a}-\frac{1}{a} \int_{0}^{\infty} \frac{1}{1+\frac{a\left(1-t^{2}\right)}{1+t^{2}}} \cdot \frac{2 d t}{1+t^{2}} \\&=\frac{\pi}{a}-\frac{2}{a} \int_{0}^{\infty} \frac{d t}{(1-a) t^{2}+(1+a)} \\&=\frac{\pi}{a}-\frac{2}{a \sqrt{1-a^{2}}} \tan^{-1}\left[\frac{\sqrt{1-a} t}{\sqrt{1+a}}\right]_{0}^{\infty} \\&=\frac{\pi}{a}-\frac{\pi}{a \sqrt{1-a^{2}}}\end{aligned}\tag*{} $ Integrating both sides w.r.t. $a$ yields \begin{aligned}\int I^{\prime}(a) d a &=\pi\int\left(\frac{1}{a}-\frac{1}{a \sqrt{1-a^{2}}}\right) da \\& \stackrel{a=\sin \theta}{=} \pi\int\left(\frac{1}{\sin \theta}-\frac{1}{\sin \theta \cos \theta}\right) \cos \theta d \theta \\&=\pi\int \frac{\cos \theta-1}{\sin \theta} d \theta\\&I(a) =\pi \int \frac{-\sin ^{2} \theta}{\sin \theta(\cos \theta+1)} d \theta\\&=\pi \ln (1+\cos \theta) +C\end{aligned} Putting $a=0$ gives $C=-\pi\ln 2$ and hence $$
\boxed{\int_{0}^{\pi} \ln (a \cos x+1) d x =\pi \ln \left[1+\cos \left(\sin ^{-1} a\right)\right]= \pi \ln \left(\frac{1+\sqrt{1-a^{2}}}{2}  \right)}
$$ I now want to generalize it to $$I(b,c)=\displaystyle \int_{0}^{\pi} \ln (b \cos x+c),\tag*{} $$ where $c\neq 0$ and $-1\leq \frac{b}{c} \leq 1.$ $$
\begin{aligned}
I(b,c)&=\int_{0}^{\pi} \ln (b \cos x+c) \\
&=\int_{0}^{\pi} \ln \left[c\left(\frac{b \cos x}{c}+1\right)\right] \\
&=\pi \ln c+\int_{0}^{\pi} \ln \left(\frac{b}{c} \cos x+1\right) d x \\
&=\pi \ln c+I\left(\frac{b}{c}\right)
\end{aligned}
$$ Putting $a=\frac{b}{c}$ yields $$\boxed{\int_{0}^{\pi} \ln (b \cos x+c) = \pi\left[\ln c+\ln \left(1+\sqrt{1-\frac{b^{2}}{c^{2}}}\right)\right] = \pi \ln \left(\frac{c+\sqrt{c^{2}-b^{2}}}{2}\right)}
$$ For example, $$
\int_{0}^{\pi} \ln (\cos x+1)=\pi \ln \left(\frac{1}{2}\right)=-\pi \ln 2;
$$ $$
\int_{0}^{\pi} \ln (\sqrt{3} \cos x+2) d x=\pi\ln \frac{3}{2} 
$$ Is there any  method other than Feynman’s integration technique?","['integration', 'calculus', 'definite-integrals']"
4470834,In how many ways can an engineering student select and schedule three technical electives in his final four semesters?,"I've the following question: An engineer needs to take exactly three technical electives sometime during
his final four semesters. The three are to be selected from a list often. In how many ways can he schedule those classes, assuming that he never wants to take more than one technical elective in any given term? I tried to solve it in the following way: Total number of ways $3$ electives can be chosen from a list of $10$ electives: $10P3$ Total number of was $3$ electives can be arranged in $4$ semesters: $4P3$ So, total number of permutation= $4P3 \cdot 10P3 = 24 \cdot 720 = 17,280$ . Is my calculation correct?","['permutations', 'combinatorics']"
4470835,Combinatorial proof of a simple inequality: $\left(\frac{n+1}{2}\right)^n \ge n!$,"I want to prove the following inequality combinatorialy $$\left(\frac{n+1}{2}\right)^n \ge n! ,n \in \mathbb{N} $$ my attempts in this direction so far have been $$\left(\frac{n+1}{2}\right)^n \ge n! \iff (n+1)^n \ge 2^n n!$$ then i tried to show a injection from some set with $2^nn!$ elements to a set with $(n+1)^n$ elements but this is where i am not able to come up with a setup that shows this clearly. I would greatly appreciate any kind of help pointing to such a setup with the injective mapping. Thank you","['inequality', 'combinatorics', 'combinatorial-proofs', 'factorial']"
4470924,Is a monoid structure on the set of knots useful?,"Under the operation of connected sum, the set of knots become a monoid. The unknot is the identity, and associativity is given by the definition of connected sum. There is no way to find inverses on this set under the operation, so this is not a group. In a lot of the texts I've read, authors seem to gloss over this fact, and move on to talk about using knot concordance and other methods to create a group structure.
Is this monoid really not useful at all? It seems quite a fundamental thing, so I thought that it must have use in classifying knots. I also thought that the connected sum was quite a natural way to combine knots, because it is how we decompose into prime knots. However, the fact that you need to modify the operation so much in order to get a group structure makes me doubt this. It seems like there is a bit more going on than I initially assumed. So my question is: Is there any use for this monoid, and if not, does that mean that the connected sum isn't really the most natural operation on the set of knots?","['knot-theory', 'group-theory', 'algebraic-topology']"
4470937,Trying to model the Flyby anomaly,"Thinking about it, the following non-linear ordinary differential equation crossed my mind: $$
\frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0
$$ Apparently, I've been trying to modify and simplify an equation for Orbit al motion. The physical meaning of the variables is: $r=$ position , $t=$ time , $(H,\mu)=$ constants. The constant $H$ may be considered as being very small. An obvious simplification is $H=0$ : $$
\frac{d^2 r}{dt^2} = - \frac{\mu}{r^2}
$$ An equation which can be solved as follows. Physicists will recognize kinetic and potential energy. $$
\frac{d^2 r}{dt^2}\frac{dr}{dt} = - \frac{\mu}{r^2}\frac{dr}{dt} \\
\frac{1}{2} \frac{d}{dt}\left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{\mu}{r}\right) \\
v^2 = \left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + C
$$ With $C$ as an integration constant. It can be determined by assuming that speed $v=V$ at infinity $r\to\infty$ is given: $$
\left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + V^2 = \mu \left( \frac{2}{r} - \frac{1}{a} \right)
$$ The result is written in the latter form because it resembles (not at all by coincidence)
the Vis-viva equation , with $a = -\mu/V^2 \lt 0$ for a hyperbola. Calculations can be continued eventually for finding the position as a function of time.
Which is not quite easy, and not interesting too, for our purpose. I think that the one-dimensional model may be acceptable as the simplification of a rather straight hyperbolic trajectory: Because in that case the asymptotes of the hyperbola could replace the hyperbola itself as a first approximation
(except for the singularity near $r=0$ ). But let us return to the core of the question, which is my tentative non-linear ODE for the Flyby anomaly .
Problem is that I have not a clue how to solve it. $$
\frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0
$$ Or equivalently: $$
\left(\frac{d}{dt} - H \right) \left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{2\mu}{r}\right)
$$ Which doesn't help me any further. Any ideas? I'm satisfied with a solution for $\,v=dr/dt\,$ only, even if it's approximate.","['ordinary-differential-equations', 'calculus', 'nonlinear-system', 'physics', 'celestial-mechanics']"
4470941,Conditional expectation over sum of function,"I am trying to understand how conditional expectation works when it is done over a sum of a function. Such as is the case in the following gain function g with the following properties: The gain function of policy $\pi$ is a mapping $g^{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ defined as $$
g^{\pi}(s):=\lim _{N \rightarrow \infty} \frac{1}{N} \mathbb{E}^{\pi}\left[\sum_{t=1}^{N} r\left(s_{t}, a_{t}\right) \mid s_{1}=s\right] .
$$ where $\mathbb{E}^{\pi}$ indicates expectation over trajectories generated by $\pi$ . $g^{\pi}(s)$ measure the per-step reward obtained in a steady state under $\pi$ starting from $s$ . The limit may not exist for all policies. For all $\pi$ and $s$ : $$
\left|g^{\pi}(s)\right| \leq R_{\max },
$$ where $R_{\max }$ is an upper bound on the rewards. In this case can I then rewrite expression such that it looks like this: $g^{\pi}(s):=\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{x}\left[ p\left(X = \left[\sum_{t=1}^{N} r\left(s_{t}, a_{t}\right) \mid s_{1}=s\right]\right) \cdot\left[\sum_{t=1}^{N} r\left(s_{t}, a_{t}\right) \mid s_{1}=s\right]\right]$ According to the definition of expectation as: $E[X]=\sum_{x} x \cdot P(X=x)$ , where in this case $x =  \left[\sum_{t=1}^{N} r\left(s_{t}, a_{t}\right) \mid s_{1}=s\right]$ And in that case can I then rewrite the conditional expectation, such that it represents a number? In neither case I am using $s_1$ when summing up. Nor do I really know if complications arise due to the N and the lim in the expression.","['conditional-probability', 'functions', 'conditional-expectation']"
4471104,There exists a subsequence of every $n$ consecutive natural numbers whose sum is divisible by $n(n+1)/2$,"This is my first question here, hence I apologize beforehand for any mistakes I make. The statement is very simple. For any natural $n$ , show that for all sequences of $n$ consecutive natural numbers, there exists a subsequence with non-zero length whose sum is divisible by $(1+2+.. +n)$ . I really can't find any way to proceed with this question. I thought of double induction since this looks kind of analogous to $n$ consecutive natural numbers being divisible by $n!$ . But I couldn't proceed anywhere with that. It looks like a question meant to be solved by utilization of pigeonhole principle, but I can't make my pigeonholes either. Any hint on how to proceed would be very appreciated.","['combinatorics', 'modular-arithmetic']"
4471106,Conditions and correct interpretation of Borel summation,"Hello to the community. In my line of research (theoretical particle physics) it is customary to apply the strategy of Borel summation to infinite power series in order to find closed forms and/or numerical estimates. However, I have the feeling things are often not done very rigurously, so I want to see if the mathematicians here can help me understand how it works. I have a power series $R(x)\equiv\sum^{\infty}_{n=1}r_nx^n$ that I rewritte using the integral definition of the gamma function as $$R(x)=\sum^{\infty}_{n=1}\frac{r_{n+1}}{n!}x^{n+1}\int_{0}^{\infty}e^{-t}t^n.$$ So far so good, but now people do two things. The sum and the integral cannot be in general interchanged, so they define the Borel sum of the series as $$
R_B(x)\equiv\int_{0}^{\infty}e^{-t}\sum^{\infty}_{n=0}\frac{r_{n+1}}{n!}x^{n+1}t^n.
$$ They are only interested in $x>0$ , so they define $u\equiv xt$ and rewrite $R_B$ as (the integration limits remain unaltered) $$
R_B(x)\equiv\int_{0}^{\infty}e^{-u/x}\sum^{\infty}_{n=0}\frac{r_{n+1}}{n!}u^n.
$$ The series in the integrand is referred to as the Borel transform of the original series and denoted as $B[R(x)](u)$ . In my field, this strategy is tipically used for series where $r_n$ diverges factorially for large $n$ . Since the series is (or is related to) a physical magnitude, we assume that, despite divergent, the series must constitute an asymptotic expansion of some unknown function. We perform the Borel transform of the series, sum the result up, and then perform the integral, and the result is interpreted as an estimate of the said unknown function. My questions are: When is $R_B(x)$ equal to $R(x)$ ? This is, when can the sum and the integral be interchanged? If $B[R(x)](u)=\sum^{\infty}_{n=0}\frac{r_{n+1}}{n!}u^n$ converges for $|u|<r<\infty$ can we still integrate the result from $u=0$ to $\infty$ ? If the original series has a finite convergence radius, how is $R_B(x)$ going to carry that information? There is an example in wikipedia on how Borel summation is applied to the geometric series $\sum_{n=0}^\infty x^n$ . The original series is converges to $1/(1-x)$ for $|x|<1$ , and the integral returns the same expression but under the weaker condition $\mathrm{Re}(x)<1$ . It says then that Borel summation provides ""an analytical contnuation of the known result "" $1/(1-x)$ "". What does this mean exactly? Thanks in advance!","['asymptotics', 'real-analysis', 'complex-analysis', 'sequences-and-series', 'power-series']"
4471123,Compute $f^{(2020)}(0)$,"Problem : Let $$f(x)=\frac{x}{(x+1)(1-x^2)}$$ Then find $f^{(2020)}(0)$ . My Attempt : From partial fraction decomposition, $$f(x)=\frac{1}{4(1-x)}+\frac{1}{4(x+1)}-\frac{1}{2(x+1)^2}$$ and, $$\frac{1}{1-x}=\sum_{n=0}^\infty x^n,\quad \frac{1}{1+x}=\sum_{n=0}^\infty (-1)^nx^n, \quad\frac{1}{(1+x)^2}=\sum_{n=1}^\infty (-1)^{n+1}nx^{n-1}=\sum_{n=0}^\infty(-1)^n(n+1)x^n$$ From these, $$f(x)=\sum_{n=0}^\infty \frac{x^n}{4}+\sum_{n=0}^\infty \frac{(-1)^nx^n}{4}+\sum_{n=0}^\infty\frac{(-1)^{n+1}(n+1)x^n}{2} \\ =\sum_{n=0}^\infty\frac{x^n+(-1)^nx^n+(-1)^{n+1}(n+1)x^n}{4} \\ =\sum_{n=0}^\infty\frac{(1+(-1)^n+(-1)^{n+1}(n+1))x^n}{4}$$ $f^{(2020)}(0)=(2020)!\times a_{2020}$ where $f(x)=\sum a_nx^n$ In this case, $a_n = 1+(-1)^n+(-1)^{n+1}(n+1), a_{2020}=-2019$ but there is no same answer. Where did I make a mistake?","['power-series', 'calculus', 'sequences-and-series']"
4471125,Solving $\tan(2x)\tan(x)=1$ two ways gives different solutions,"While solving this Equation by 2 ways I am getting different solution $$\tan(2x)\tan(x)=1$$ So, I solved it in two ways. Method-1 This gives solution as : x=nπ ± π/6 Which is also given as solution Method-2 Where did I go wrong And how to solve correctly using Method-2",['trigonometry']
4471157,Extreme points of Hilbert cube,"Define the set $$Q:=\prod \limits _{n=1}^\infty \left [0,\frac{1}{n}\right ]$$ as a subspace of $X:=\mathbb{R}^{\mathbb{Z}_{>0}}$ with the product topology. Clearly $Q$ is a convex compact subset of $X$ , hence by the Krein-Milman Theorem we have that $Q$ is the closed convex hull of the set of its extreme points $\mathcal{E}(Q)$ . In the book of Bühler and Salamon it is stated that $$\mathcal{E}(Q)=\prod \limits _{n=1}^\infty \left \{0,\frac{1}{n}\right \},$$ that the convex hull of any finite subset of $\mathcal{E}(Q)$ is nowhere dense in $Q$ and that therefore the convex hull of $\mathcal{E}(Q)$ is not equal to $Q$ . It also says that the last fact follows from Baire Category Theorem. I proved that $\displaystyle \mathcal{E}(Q)=\prod \limits _{n=1}^\infty \left \{0,\frac{1}{n}\right \}$ and that $Q$ is a complete metric space, hence if I could express the convex hull of $\mathcal{E}(Q)$ as a countable union of convex hulls of finite subsets of $\mathcal{E}(Q)$ and prove that each of said convex hulls is nowhere dense in $Q$ , then all the assertions would have been proved. However, I couldn't prove that the convex hull of a finite subset is nowhere dense and I couldn't find a way of expressing the convex hull of $\mathcal{E}(Q)$ as a countable union (it is clearly equal to the union of the convex hulls of all of its subsets, but I don't know which subsets I should discard in order to obtain a countable union). How can I prove these two facts?","['functional-analysis', 'convex-hulls']"
4471230,Three-term arithmetic progressions ending with a given prime,"A student has asked me whether the following is true: If p is prime and bigger than 37, there exists a three-term arithmetic progression of primes ending in p . For smaller odd primes one can find 3-APs except for 3, 5, 13, and 37. For those the best one can do is (1,(p+1)/2,p). The next prime such that (p+1)/2 is prime is 61, but we can find a 3-AP (13,37,61). (Edit) I have tested all primes up to 10^5 and found no other exceptions. Any hints?","['number-theory', 'arithmetic-progressions', 'prime-numbers']"
4471257,"Let $L_0 = (-i)^m \frac{d^m u}{dx^m}$. How can I prove, that $L_0$ is a non-negative operator?","The minimal operator $L_0 = (-i)^m \frac{d^m u}{dx^m}, \ u \in \mathcal{C}_0^{\infty} (a,b)$ for $- \infty \le a < b < \infty$ induced in $L_2 (a,b)$ by the differential form $l[u] = (-i)^m \frac{d^m u}{dx^m}$ is non-negative A self-adjoint extension $A$ of $L_0$ is given by $$Au = (-i)^m \frac{d^{2n} u}{dx^{2n}}, \ u \in dom A$$ where $dom A$ consists of all $u \in W_2^m (a,b)$ such, that $$f(a) = f' (a) = ... = f^{m-1} (a) = 0 \ if \ a > - \infty$$ and $$f(b) = f' (b) = ... = f^{m-1} (b) = 0 \ if \ b < \infty$$ My attempt We want to prove, that $\langle L_0 u, u \rangle \ge 0$ . We know, that if it has to be non-negative, then it has to be real, thus $\langle L_0 u, u \rangle = \overline{\langle L_0 u, u \rangle} = \langle L_0^* u, u \rangle$ . This again means, that $L_0 = L_0^*$ . $$\langle L_0 u, u \rangle = (-i)^m \int_a^b \frac{d^m u}{dx^m} \overline{u} dx = (-i)^m (\frac{d^{m-1}u}{dx^{m-1}} \overline{u} \ |_a^b - \int_a^b \frac{d^{m-1}u}{dx^{m-1}} \overline{u'}dx) = ... $$ Because the support of $u$ here is $(a,b)$ , this means that ultimately, we get: $$... = (-i)^m (-1)^m \int_a^b u \overline{\frac{d^m u}{dx^m}}dx = i^m \int_a^b u \overline{\frac{d^m u}{dx^m}}dx = \int_a^b u \ \cdot \  \overline{(-i)^m \frac{d^m u}{dx^m}}dx = \langle u, L_0 u \rangle$$ I proved that indeed $\langle L_0 u, u \rangle$ is real-valued. But how do I prove that it is non-negative, too? My idea would be to use integration by parts, too. But the problem here is that it's the $2n$ -th derivative, and not the $m$ -th derivative. So that's where I'm stuck","['integration', 'operator-theory', 'functional-analysis', 'inner-products']"
4471304,"Is ""Popping Popcorn"" a Stochastic Process?","While placing a bag of popcorn in the microwave today, I noticed the following thing: In the 30 seconds, the time difference between (e.g. treat ""time"" as a random variable) which any two kernel of popcorn can pop has some probability distribution function (e.g. the n-th kernel pops at time T1 and the n_1-th kernel pops at time T2 : T2 - T1 has some probability distribution In the next 30 seconds, the time difference between which any two kernels of popcorn can pop has some other probability distribution - naturally, the time difference between successive kernel pops is obviously shorter In the last 30 seconds, the time difference between which any two kernels of popcorn can pop has another probability distribution - naturally, the time difference between successive kernel pops is longer than this same time difference in the previous 30 seconds. This brings me to my question - could ""popping popcorn"" be modelled as some Stochastic Process (such as a Inhomogeneous Poisson Arrival Process, Markov Switching Process or a Brownian Motion ) ? For instance, suppose we knew that 43 kernels of popcorn have already popped - we also know that time at which each kernel popped (e.g. kernel_1 = 12 seconds, kernel_2 = 19 seconds, kernel_3 = 21 seconds .... kernel_42 = 81 seconds, kernel_43 = 85 seconds). This means we also know the time differences between each successive pair of popcorn kernels (e.g. Kernel_2_1 = 19 - 12 = 7 seconds, Kernel_3_2 = 21 - 19 seconds = 2 seconds, etc.). With this information (and historical data we collected about popcorn kernels popping) , could we make some inference as to when the 44th kernel of popcorn is expected to pop? Thanks! Note 1: I feel a very naive way to approach this problem would be to just fit some regression model through historical data - here I have shown this using the R programming language. library(dplyr)
    #first 30 seconds
    popping_times = abs(rnorm(40,10,3))
    index = 1:40
    data_1 = data.frame(index, popping_times)

#next 30 seconds
popping_times = abs(rnorm(80,5, 2))
index = 1:80
data_2 = data.frame(index, popping_times)

#next 30 seconds
popping_times = abs(rnorm(30,7,3))
index = 1:30
data_3 = data.frame(index, popping_times)

total_data = rbind(data_1, data_2, data_3)
total_data$index = 1:nrow(total_data)

total_data = data.frame(total_data %>%  mutate(popping_times_diff = abs(popping_times - lag(popping_times))))

total_data[is.na(total_data)] <- 0 The data looks something like this: index popping_times popping_times_diff
1     1     10.380158         0.00000000
2     2      8.874496         1.50566244
3     3      8.966483         0.09198782
4     4      9.312913         0.34642958
5     5     11.094466         1.78155287
6     6      9.707844         1.38662218 And the plots look something like this: plot(total_data $index, total_data$ popping_times, main=""Popping Times of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time (Seconds)"", type = ""b"")

plot(total_data $index, total_data$ popping_times_diff, main=""Popping Time Difference of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time Difference (Seconds)"", type = ""b"") Note 2: If we aggregated the data into ""number of kernels that popped within each 10 second interval"" - we might be able to make a time series model (e.g. ARIMA) that predicts how many kernels will pop in the next 10 seconds ... but this is a slightly different problem from the one I am interested in","['stochastic-processes', 'probability']"
4471324,"Compute $(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle$ using the Fundamental Theorem of Finitely Generated Abelian Groups.","I'm trying to get the hang of factor group computations so I made up a problem for myself to try to solve. For practice, I tried computing $(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle$ using the Fundamental Theorem of Finitely Generated Abelian Groups. Here's my solution: First, we see that $\langle(2,4)\rangle=\{(2,4),(0,0)\}$ , so $|\langle(2,4)\rangle|=2.$ Therefore, we have that $$|(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle|=\frac{|\mathbb{Z}_4\times\mathbb{Z}_8|}{|\langle(2,4)\rangle|}=\frac{32}{2}=16, $$ so according to the Fundamental Theorem, our factor group $(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle$ is isomorphic to $\mathbb{Z}_{16},\mathbb{Z}_2\times\mathbb{Z}_8,$ or $\mathbb{Z}_4\times\mathbb{Z}_4$ . Consider the coset $(0,1)+\langle(2,4)\rangle$ . This coset has order 8 since $8(0,1)=(0,0).$ Since $1$ generates $\mathbb{Z}_8,$ $(0,1)+\langle(2,4)\rangle$ is the element with the largest possible order in $(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle$ . Since $\mathbb{Z}_2\times\mathbb{Z}_8$ is the only group out of the possibilites presented whose elements have a maximum order of $8$ , we conclude $(\mathbb{Z}_4\times \mathbb{Z}_8)/\langle(2,4)\rangle\cong\mathbb{Z}_2\times\mathbb{Z}_8.$ Did I get the right answer, and is my thought process correct here?","['abelian-groups', 'group-theory', 'abstract-algebra', 'solution-verification']"
4471335,Function identifying consecutive integers,"I am looking for a function $f(a,b,c)$ defined on triplets of integers so that $f(a,b,c)=0$ if and only if $a$ , $b$ , $c$ form a set of 3 consecutive integers. Is there such function in closed form?","['functions', 'integers', 'integer-sequences']"
4471347,Non-elementary examples of compact Anosov manifolds,"Anosov manifolds (Riemannian manifolds whose geodesic flow is Anosov) are a natural generalisation of negatively curved compact manifolds. I'm wondering how good the generalisation is. In particular, is there any Anosov manifold that has some positivity in its curvature?","['dynamical-systems', 'riemannian-geometry', 'differential-geometry']"
4471349,Discrete-time dynamical systems with variable state space dimensions (or output space dimensions),"I am trying to figure out how to formalize a dynamical system whose state vector can change dimensions from one step to the next. For example, I have a process (a discrete-time dynamical system, if you could call it) that at time step $t=k$ has a state vector $x(k)$ that is an $n$ -dimensional vector and at time $t=k+1$ , the state vector $x(k+1)$ can become an $(n+1)$ -dimensional vector depending on the input. In the context of dynamical systems, this is an unusual construct that I am not familiar with. Has anyone ran into this? Any clues as to how one might capture/describe the state space for a process like this? Addendum: An alternate form would be if the variable dimensional state, $x(k)$ , is re-defined to represent the output $y(k)$ of another system, one whose SS is a fixed dimensional vector space: $x(k+1)=F(k,x(k),u(k))$ , $x(0)=x_0 \in \mathbb{R^n}$ $y(k) = C(k,x(k),u(k))$ , $y(k) \in \mathbb{R}^{d_k}$ In the context of dynamical systems, it seems like this is a more natural construct to capture variability of the dimension: via the output space rather than the internal SS. This removes the challenges introduced with time incremental change in SS dimension, i.e. $x(k+1) \in \mathbb{R^{n}}$ has the same dimension as $x(k) \in \mathbb{R^{n}}$ , but now the output is free to change  dimension via an output transition mapping, $C$ , in $y(k)=C(k,x(k))$ or more generally $y(k)=C(k,x(k),u(k))$ where $u$ is the input to the system. But now, of course the devil is in figuring out the mapping $C$ that gets us to $x$ . I am hoping there are more examples/hits on this form of the problem?","['topological-vector-spaces', 'discrete-mathematics', 'discrete-time', 'nonlinear-dynamics', 'dynamical-systems']"
4471378,The Making of a Ring — How can we choose 1?,"I am an undergraduate studying Algebra: Chapter 0 by Paolo Aluffi in my free time (Note that because of this whenever I refer to 'rings' I am speaking of 'rings with identity'). While learning about the basics of rings, it became clear to me that we define multiplication by associativity and distributivity because these are the properties of group homomorphisms. Ring multiplication distributes over addition in the underlying abelian group because of the homomorphism condition—i.e. for a group homomorphism $\varphi: G \to G$ , we have $\varphi(a+b) = \varphi(a) + \varphi(b)$ . Similarly the associativity of multiplication is due to the associative nature of morphism composition. Therefore it seems natural to me to view the multiplication operation as a convenient abstraction encapsulating the data we obtain by studying an abelian group alongside its group endomorphisms. Aluffi brings as Proposition 2.7 the following: Let $R$ be a ring. Then the function $r \mapsto \lambda_r$ [where $\lambda_r$ is the group endomorphism representing left ring multiplication by $r$ ] is an injective ring homomorphism $$\lambda: R \to \mathrm{End}_{\mathrm{Ab}}(R).$$ (Algebra: Chapter 0, §III.2.5) Thus the image of $\lambda$ is a subring of $\mathrm{End}_{\mathrm{Ab}}(R)$ isomporphic to $R$ . If we can pick $\mathrm{im} \ \lambda$ out of $\mathrm{End}_{\mathrm{Ab}}(R)$ without a predefined ring structure, we can obtain a ring structure on $R$ using an isomorphism $\mathrm{im} \ \lambda \to R$ given by $\lambda_r \mapsto \lambda_r(1_R)$ . So given an abelian group $G$ and an element designated $``1_G""$ , I think that we need only to study how a certain subgroup of $\mathrm{End}_{\mathrm{Ab}}(G)$ interacts with $1_G$ to determine a ring structure on $G$ . Therefore my question is: What are the precise consequences of differing choices of $1$ when defining a ring structure on an abelian group? My thoughts on this so far:
Clearly different choices of $1$ lead to different ring structures. If, for example, we let $G := (\mathbb{Z}\backslash 4\mathbb{Z}, +)$ , the natural choice of $1_G$ being 1 gives rise to a ring of characteristic $4$ . Now, I struggled to actually carry out this example, but we can easily imagine that if we could choose $1_G$ to be 2, then the induced ring would be of characteristic 2, since $2 + 2 = 2 \cdot 1_G = 0$ . Furthermore, since 3 is the additive inverse of 1 in this group, we should be able to let $1_G = 3$ and come up with a sort of 'backwards' version of the usual ring structure. In this case we have a ring with $G$ as the underlying abelian group, isomorphic to the version with $1_G = 1$ and multiplication defined by: $\ast$ 3 2 1 3 3 2 1 2 2 0 2 1 1 2 3 So is every element of an arbitrary abelian group $G$ a candidate for $1_G$ when defining a ring structure? If so, what are the consequences of different choices, and if not what are the necessary group-theoretic requirements? Note: I am a beginner at this, having only studied this subject independently throughout my senior year of highschool and freshman year of college. With that in mind please let me know if anything I said is off-base and try to explain things as simply as possible. Thank you.","['ring-theory', 'group-theory', 'abstract-algebra']"
4471390,"Books for upper-undergraduate, higher level real analysis","My university does not offer a real analysis course for math major students in the later stages of their undergrad; these are reserved for honours students. I had an excellent professor in my final introductory analysis course and I wish to study some higher level real analysis on my own. I was looking for a book that might help me in this endeavour. In my analysis courses, we covered sequences and series, limits including $\limsup$ and $\liminf$ , continuity, differentiability, and Riemann integration (Darboux's approach only). Our main textbook was Abbott's Understanding Analysis , though I found Bartle's Introduction to Real Analysis much more helpful. I remember reading a bit of Rudin's Principles of Mathematical Analysis , but it went a bit too fast for me. I looked at the syllabi for the honours courses and they involve topics like point-set topology, some introductory measure theory, and Lebesgue integration, with a brief foray into functional analysis and Fourier analysis only at the end of the year. Some potential books that I looked at were Royden's Real Analysis , Carothers' Real Analysis , and Axler's Measure, Integration, & Real Analysis . Royden seems to be used by my university, but upon a quick glance I may need a bit more mathematical maturity before I attempt to self-study from it. Carothers and Axler both seem to match my pace, but I am interested as to your thoughts on either the books I mentioned or some other books outside of these three that you believe might suit me best.","['book-recommendation', 'real-analysis']"
4471408,Conjecture: $\frac1\pi=\sum_{n=0}^\infty\left((n+1)\frac{C_n^3}{2^{6n}}\sum_{k=0}^n(-1)^k{n\choose k}{\frac{(n-k)(k-1)}{(2k-1)(2k+1)}}\right)$,"Let $C_n$ denote the $n$ -th Catalan  number defined by $${\displaystyle C_{n}={\frac {1}{n+1}}{2n \choose n}=\prod \limits _{k=2}^{n}{\frac {n+k}{k}}\quad \left(n\geqslant 0\right).}$$ Next, we define the sequence $${\displaystyle A_{n}={\frac {C_{n}^{3}}{2^{6n}}}\sum _{k=0}^{n}(-1)^{k}{n \choose k}{\frac {(n-k)(k-1)}{(2k-1)(2k+1)}}}.$$ I've numerically managed to verify that \begin{equation}\tag{1}\label{pi}
{\displaystyle \sum _{n=0}^{\infty }(n+1)A_{n}={\frac {1}{\pi}}}.
\end{equation} Is it possible to prove the relation in (\ref{pi})? If yes, then how could we go about it? Also, is this series already known or studied in the literature? If yes, then any references will be highly appreciated. Thanks!","['catalan-numbers', 'number-theory', 'pi', 'generating-functions', 'sequences-and-series']"
4471425,Inequality with discriminants,"If $x^2-ax+1-2a^2>0$ for all $x \in {R}$ ,
find range of $a$ The solution to this takes the discriminant of the expression in terms of $a$ , i.e., $$\implies D={a^2-4(1-2a)}>0(\because x \in R)$$ and then figures out the range from the inequality in $a$ . But what I dont understand is, as per my knowledge, the discriminant is taken from a quadratic equation, so in order for $D$ to be as it is in the solution, we must first consider $$x^2-ax+1-2a^2=0$$ What I mean to say is, Say $x^2-ax+1-2a^2=y$ $$\implies x^2-ax+1-2a^2-y=0$$ $$D=a^2-4(1-2a^2-y)>0$$ But the solution has considered only $a^2-4(1-2a^2-y)>0$ , which means that they have considered $y=0$ , but the question says that $y>0$ . The question is clearly stating that the expression is greater than and not equal to $0$ . So how can we work with the discriminant here?","['algebra-precalculus', 'quadratics', 'polynomials', 'inequality']"
4471461,Find the maximum perimeter of a right angled triangle with hypotenuse 1,"This is a question from the grade 7 Math Competition. I can solve it by considering one of the angles, say $\theta$ , besides the right angle. We have the perimeter given by $1+\cos\theta+\sin\theta = 1+\sqrt{2}\cos(\theta-\pi/4 )$ and the rest is easy. However, I believe there should be an elementary but elegant way to solve it. Any inspiration?",['trigonometry']
4471469,Is the space of distinct triples homeomorphic to a union of products?,"$\newcommand{\S}{\mathbb{S}^1}$ Let $M=\{(x,y,z) \in  (\S)^3 \, |\,\, x,y,z \,\,\text{are distinct}\}$ . Is $M$ homeomorphic to a finite union of products of one-dimensional manifolds? I think $M$ is not connected, so it cannot be homeomorphic to a product. $M$ is not connected, since if $x-y-z$ are ordered clockwise, you cannot move to $x-z-y$ being clockwise. So I think that there are two connected components. The case where considering pairs follows easily from the group structure of $\mathbb{S}^1$ .","['symmetry', 'topological-groups', 'general-topology', 'differential-topology', 'algebraic-topology']"
4471490,A proper approach to learn Koopman Operator Theory,"Edit : Rephrasing my post to make my statement clearer. In $1931$ , Bernard Koopman known for his work in ergodic theory, proposed a linear operator that describes the evolution of scalar observables (i.e., measurement functions of the states) in an infinite dimensional Hilbert space. In other words, if we have a non-linear system then we can shoot it to an infinite-dimensional function space where the evolution of the original system becomes linear. This has now become known as Koopman Operator Theory (KOT) . This is a relatively new (for me) topic $80$ years later his work resurged and became an important contribution in the study of stability analysis and in providing alternative formalism for study of dynamical systems mainly in non-linear control systems with applications ranging in adaptive control, non-linear control, system identification, deep learning, reinforcement learning which is the source of my motivation (to model non-linear dynamics using advanced and rigorous mathematics). I myself have no rigorious understanding in much of functional analysis except from an engineering point of view in topics such as signal processing, control theory, but I questioned how can non-linear systems from an engineering perspective be dealt in an ""infinite""-dimensional space. With that being said, I am looking for a formal roadmap that can help me establish a proper and rigorous understanding of Koopman Operator Theory. While being obvious that this topic is just a fraction of what operator theory contains, I wish to know what concepts are needed in measure theory, functional analysis to reach the level of understanding required to deal with KOP from a mathematician perspective. I am asking this question due to my engineering background with a rich background in applied mathematics but limited knowledge in analysis. Therefore, this question targets individuals who have knowledge in functional analysis, hilbert space,... or individuals who are active in the research community in particular in KOP theory.","['operator-theory', 'reference-request', 'functional-analysis', 'koopman-operator', 'dynamical-systems']"
4471503,Finding a bound on a certain number of sums,"Let $x_1, ..., x_{2n}$ be real numbers with $|x_i
| ≥ 1$ for all $i$ , and let $I ⊂ R$ be an
arbitrary open interval of length $2$ . I want to: (a) Prove that the number of sums $\sum_{i=1}^{2n}ε_ix_i$ , where $ε_i ∈ \{−1, +1\}$ , which fall in the interior
of $I$ does not exceed $\binom{2n}{n}$ . (b) Show that for a closed interval $I$ of length $2$ the statement is not necessarily true. I can't see why binomial coefficients come into the picture when we are dealing with intervals. I thought we could try a proof by contradiction; assume that the number of sums is greater than $\binom{2n}{n}$ , and find a contradiction. But I cannot see what true statement would be contradicted.","['combinatorics', 'discrete-mathematics']"
4471505,How to understand the Riemann surface of $\log z$ using the formal definition of Riemann surface?,"I am a beginner studying Riemann surface. First things first, I learned about the definition of Riemann surface as follows. Definition : A Riemann surface $X$ is defined by the following data: a connected Hausdorff topological space $X$ ; an open cover $\{U_\alpha\}_{\alpha\in A}$ of $X$ ; for each $\alpha\in A$ , a homeomorphism $\phi_\alpha:U_\alpha\rightarrow V_\alpha$ to an open subset $V_\alpha\subset\mathbb{C}$ in such a way that for each $\alpha,\beta\in A$ , if $U_\alpha \cap U_\beta\neq \emptyset$ , the transition functions $$\phi_\beta\circ\phi^{-1}_\alpha:\phi_\alpha(U_\alpha\cap U_\beta)\rightarrow\phi_\beta(U_\alpha\cap U_\beta),$$ is bi-holomorphic, namely, holomorphic with inverse holomorphic. We know the famous Riemann surface for $\log z$ , which has the following shape How can I describe such surface with the formal definition above? In other words, I need to construct a precise $U_\alpha$ , $\phi_\alpha$ and transition map to describe the Riemann surface for $\log z$ , not using intuitive words such as ""gluing"", ""rotate"", ""visualized like parking lot"" etc... Thanks very much for your help.","['complex-analysis', 'riemann-surfaces']"
4471583,System of inequations,"If $p$ , $q$ , $r$ , $s$ and $t$ are real numbers such that $q+r<s+t$ , $r+s<t+p$ , $s+t<p+q$ and $p+q<r+s$ , then find the largest and the smallest term among them. This is how I solved it: $$p+q<r+s$$ $$(+)-p-t<-s-r$$ Gives $q<t$ $$p+q<r+s$$ $$(+)-p-q<-s-t$$ Gives $t<r$ $$s+t<p+q$$ $$(+)-s-t<-q-r$$ Gives $r<p$ So we have $$p>r>t>q>s$$ or $$p>r>s>t>q$$ or $$p>r>t>s>q$$ So it's either $s$ or $q$ which is the smallest. Now I've to check which is smaller. For that, $$q+r<s+t$$ $$(-)s+r<p+t$$ Gives $q-s<s-p$ But $p<s$ as we have already seen before. So, $s-p<s-s$ , hence, $q-s<s-s=0$ Gives $q<s$ . So $p$ is largest, $q$ is smallest. Please show any better or shorter or more elegant or unexepected ways to solve such a question.","['algebra-precalculus', 'systems-of-equations', 'inequality']"
