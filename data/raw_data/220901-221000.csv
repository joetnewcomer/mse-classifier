question_id,title,body,tags
4521232,Are manifolds with the same constant scalar curvature locally isometric?,"I was reading through Wald's General Relativity book and he mentions in chapter 5 that if two manifolds have the same dimension, metric signature, and constant curvature, they are locally isometric. I believe I should be interpreting ""curvature"" here as the sectional curvature, for which I can a proof in Lee's Intro to Riemannian Manifolds in Corollary 10.15. If I assume constant scalar curvature I am not so sure. For instance, if the manifolds are 2 dimensional, this follows from Minding's Theorem. But if they are 3-dimensional then I doubt this claim is true. For instance, any solution to the Einstein vacuum equations $R_{ab} = 0$ will have constant $0$ scalar curvature, yet the Schwarzschild metric cannot be locally isometric to flat space because the norm of its Riemann tensor, the Kretschmann invariant, is nowhere vanishing. If this is correct then I can accept Wald's claim since he is working with isotropic 3-dimensional foliations of spacetime, and an isotropic 3-dimensional manifold has constant sectional curvature. Otherwise, maybe all vacuum solutions are locally flat?","['general-relativity', 'differential-geometry']"
4521241,Let $C \in \mathbb{R}^{4 \times 4}$ satisfy $C^3 + 6C = 5C^2$. Prove that $C$ is diagonalizable over the real numbers,"Question : Let $C \in \mathbb{R}^{4 \times 4}$ satisfy $C^3 + 6C = 5C^2$ .  Prove that $C$ is diagonalizable over the real numbers. My Attempt : If $C$ satisfies $C^3 + 6C = 5C^2$ then $C$ is a root of the polynomial $p(x) = x^3 - 5x^2 + 6x = x(x-2)(x-3)$ .  The minimal polynomial $m(x)$ for $C$ then must divide $p(x)$ .  Hence $$
m(x) \in \{x, x-2, x-3, x(x-2), x(x-3), (x-2)(x-3), x(x-2)(x-3) \}.
$$ In any of these instances, the minimal polynomial for $C$ splits into linear factors over $\mathbb{R}$ , and each of the possibilities for $m(x)$ has distinct roots.  This implies that the matrix $C$ will always have distinct eigenvalues, and that $C$ can be diagonalized. Follow up question : How does the value of $n$ in $\mathbb{R}^{n \times n}$ affect the proof of this solution?  For instance, if $C$ were a 5 by 5 matrix instead, would the proof still hold?","['matrices', 'abstract-algebra', 'linear-algebra', 'solution-verification', 'diagonalization']"
4521243,evaluation of $\int\tan^{8}{x}dx$,"Compute the indefinite integral $$\displaystyle \int\tan^{8}{x}dx$$ My Attempt:(proposed solution) $$\displaystyle \tan{x}=z\Rightarrow dz=(1+\tan^{2}{x})dx$$ $$\displaystyle dx=\frac{dz}{1+z^{2}}$$ $$\displaystyle z^{8}=(1+z^{2})(z^{6}-z^{4}+z^{2}-1)+1$$ $$\displaystyle \int\tan^{8}{x}dx=\int((z^{6}-z^{4}+z^{2}-1))dz+\int dx$$ $$\displaystyle =\frac{\tan^{7}{x}}{7}-\frac{\tan^{5}{x}}{5}+\frac{\tan^{3}{x}}{3}-\tan{x}+x+c$$ Thank you (I learned a lot thanks to you, thank you)","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
4521259,When is the leaf of a foliation the level set of a function?,"Suppose I have a smooth (say $C^1$ ) codimension one foliation of $P^n$ (open subset of $R^n$ consisting of vectors with all positive components) arising from a smooth $(n-1)$ -plane field satisfying the Frobenius conditions. Now take a single leaf $L$ ; I would like to show that $L$ is the graph of a function defined on some open subset of $P^{n-1}$ . If I knew that $L$ was the level set of some function $F$ on $P^n$ , say $F(x_1,\ldots,x_n) = 0, \forall x \in P^n$ , and I knew that $F_n(p) > 0 \; \forall p \in L$ , then I could use the ""basic"" implicit function theorem to conclude that there exists a function $u$ such that $F(x_1,\ldots,x_{n-1},u(x_1,\ldots,u_{n-1})) = 0$ locally, then proceed to extend it since I know $F_n(p) > 0$ everywhere on $L$ . My question is: Do I know that $L$ is the level set of some function $F$ ? For reasons outside the scope of this question I do know that the normal vector to the tangent plane to $L$ , at every point of $L$ , has $n$ th component $> 0$ (I can compute this manually since I know the vector fields giving rise to the distribution), but to me that doesn't seem to make things directly amenable to an application of the implicit function theorem. I have a feeling that there is some way to look at this which makes the answer to my question very obvious (and the question itself silly), but I have tried to work this out and being very obsessive about my proofs I wanted to ask here. Any info or advice appreciated. Edit to add information. : I understand that $L$ , being an integral manifold, is a graph, locally, say on some domain $U$ . I believe my condition that the last component of the normal to the tangent plane, at every point $p \in L$ , means that $U$ can be extended to a maximal domain $D$ by means of the implicit function theorem. I just am searching for the proper application of that theorem given that I don't know that $L$ can be  given by a function of the form $F(x_1,\ldots,x_n) = 0$ . Edit to clarify my objective I want to show that any leaf $L$ of a particular foliation (arising from a specific $n$ -plane distribution on $P^n$ ) is the graph of some function $u : D \rightarrow P$ where $D$ is an open subset of $P^{n-1}$ . This specific distribution gives me the condition on the normals mentioned in the previous edit. I believe the way to prove that is to show that any neighborhood $U$ on which we know $L$ is a graph (there has to be one since an integral manifold is locally a graph) can always be extended to a maximal domain unless $u$ goes to infinity or zero on its boundary, and I believe the way to do that is via the implicit function theorem, utilizing the condition on the normals. I just don't know how to prove it this way. Edit to give specifics : Let $n=3$ ; I have two strictly positive, $C^2$ functions on $P^3$ , $m^1(x,y,z)$ and $m^2(x,y,z)$ . The vector fields are $X^1 = \partial/\partial x - m^1 \partial/\partial z$ and $X^2 = \partial/\partial y - m^2 \partial/\partial z$ and I impose  the following condition on the derivatives of $m^1,m^2$ : $ -\partial m^2/\partial x + \partial m^1/\partial y + m^1 \partial m^2/\partial z - m^2 \partial m^1/\partial z = 0$ . This should make the $2$ plane distribution generated by $X,Y$ involutive and so by Frobenious there is a smooth foliation $\mathscr{F}$ of $P^3$ by integral manifolds of the distribution. The normal vector to the plane tangent to a leaf $L$ at a point $p$ can be computed as $\pm(m^1(p),m^2(p),1)$ , I take the positive normal. My understanding is that under these conditions (in particular, that the normal has strictly positive last component at every point of $P^3$ ) each leaf $L \in \mathscr{F}$ is the graph of some function $u : D \rightarrow P$ , where $D$ is some open set of $P^2$ . I am looking for details on proving (if possible) my understanding, and my idea was to use the implicit function theorem.","['foliations', 'differential-geometry']"
4521266,"Differential fails to exist in a certain point in a surface, does this means the surface is not regular?","I am trying to answer the following problem: Show that the two-sheeted cone, with its vertex at the origin, that is, the
set $\{(x,y,z)∈R^3 :x^2+y^2−z^2=0\},$ is not a regular surface. I am trying the following: I rewrite it as $(x,y,\pm \sqrt{x^2+y^2})$ and then compute the differential: $$\begin{pmatrix}
{1}&{0}\\ 
{0}&{1}\\ 
{\frac{x}{\sqrt{x^2+y^2}}}&{\frac{y}{\sqrt{x^2+y^2}}}
\end{pmatrix}$$ In the definition of regular surface, we ask that: For each $q$ , the differential $d\textbf{x}_q$ is one to one. I think I can use that. The trouble for me is: The differential does not exist at $(0,0,0)$ , does this also means that the surface is non regular?",['differential-geometry']
4521270,inverse functions when solving trig equations.,"I am in Pre-calc this semester, and I have been given the problem: $\cos(\theta) = -\frac12$ The answers I get are $30^o$ and $300^o$ but the answers I get from the homework key are $120^o $ and $240^o$ . Why do the inverse equations give me answers that arent correct? Is there a better way to solve these problems? My work is as follows: $\cos(\theta)=x$ $x= -\frac12$ $x^2+y^2=1$ $(-\frac12)^2+y^2=1$ $\frac14+y^2=1$ $y^2=\frac34$ $\sqrt y^2=\pm\sqrt\frac34$ $y=\pm\frac{\sqrt3}{2}$ $\sin(\theta)=y$ $\sin(\theta)=\frac{\sqrt3}{2}$ $\sin(\theta)=-\frac{\sqrt3}{2}$ $\sin^{-1}(\frac{\sqrt3}{2})=\theta$ $\sin^{-1}(-\frac{\sqrt3}{2})=\theta$ $\theta=60^o,300^o$","['algebra-precalculus', 'inverse-function', 'trigonometry']"
4521272,Confusion with homogenous ODE: why $[\ln (y)]'=y'/y$?,"I have a question which is quite basic, however, I am unfortunately having trouble understanding the explanation. I am utilizing a textbook which states the following: $$\frac{dy}{dt}+a(t)y=0.$$ It then states that ""divide both sides of the equation by y and rewrite it in the form"" $$\frac{\frac{dy}{dt}}{y}=-a(t).$$ .However, it then states: $$\frac{\frac{dy}{dt}}{y}=\frac{d}{dt}\ln |y(t)|.$$ I am confused as to how this could be the case, as $$\frac{d}{dt}\ln |y(t)|=\frac{1}{y(t)},$$ not $$\frac{\frac{dy}{dt}}{y}=\frac{dy}{dt}\frac{1}{y}.$$ I would truly appreciate any and all help regarding this.",['ordinary-differential-equations']
4521282,Sum of subset converges to an irrational,Let $(a_n)$ be a sequence such that $a_n>0$ for all $n$ and $\displaystyle \sum \limits _{n=1}^\infty a_n$ converges. Is it true that there exists some irrational number $0<\alpha <L$ and a subsequence $(a_{n_k})$ such that $\displaystyle \sum \limits _{k=1}^\infty a_{n_k}=\alpha$ ? Can we generalise this result and say that we can find such a subsequence for every number $0<\alpha <L$ large enough?,"['summation', 'sequences-and-series', 'real-analysis']"
4521288,Probability that both red and blue cards exist in a pack of 10 cards?,"You are given a pack of 10 cards. Each card has 0.05 probability of being a red card and also 0.05 probability of being a blue card (red and blue cards are rare). What is the probability that the pack has at least 1 red card and at least 1 blue card? I tried solving this problem by enumerating all possible red and blue cards combination: $$\mathbb{P}(\geq 1 \text{ red card and } \geq 1 \text{ blue card}) = \sum_{r=1}^9 \sum_{b=1}^{10 - r} {10 \choose r} {10 - r \choose b} 0.05^r 0.05^b (1 - 0.05 - 0.05)^{10-r-b}$$ However, this is very difficult to compute, especially if the number of cards in the pack gets larger (more than 10). Is there a more clever, simpler way to solve this problem?","['contest-math', 'statistics', 'puzzle', 'discrete-mathematics', 'probability']"
4521318,An oriented compact smooth manifold with boundary does not have a retraction onto its boundary,"The following is Problem 16-4 of Lee's Introduction to Smooth manifolds. Suppose $M$ is an oriented compact smooth manifold with nonempty boundary. Show that there does not exist a retraction of $M$ onto its boundary. [Hint: if the retraction is smooth, consider an orientation form on $\partial M$ .] Suppose there is a smooth retraction $r:M\to\partial M$ and let $\iota: \partial M\hookrightarrow M$ be an inclusion so that $r\circ\iota = 1$ . Let $\omega$ be an orientation form of $\partial M$ . Then by Stoke's theorem, \begin{align*}
\int_{\partial M} r^*\omega = \int_M d(r^*\omega) = \int_M r^*(d\omega) = 0.
\end{align*} where the last equality follows from the fact that $d\omega = 0$ . Since $r\circ\iota = 1$ , we have \begin{align*}
\int_{\partial M}r^*\omega = \int_{\partial M}\iota^*r^*\omega = \int_{\partial M}(r\circ\iota)^*\omega = \int_{\partial M}\omega.\end{align*} Hence, we get $$\int_{\partial M}\omega = 0.$$ which is a contradiction as $\omega$ is an orientation form of $\partial M$ . Now, suppose $\tilde{r}:M\to\partial M$ is just continuous. Then by Smooth approximation theorem, $\tilde{r}\simeq r$ where $r:M\to\partial M$ is smooth. Then $\tilde{r}\circ\iota = 1\simeq r\circ\iota$ . Since $1$ and $r\circ\iota$ are smooth, we can choose a smooth homotopy $H:M\times I\to \partial M$ such that $H(x,0) = x$ and $H(x,1) = r\circ\iota(x)$ . Now, $$\int_{\partial (M\times I)}H^*\omega = \int_{M\times I}d(H^*\omega) = \int_{M\times I}H^*(d\omega) = 0.$$ I'm stuck in here. I need to express $\partial (M\times I)$ as a finite union of manifolds with boundary but can't see.","['smooth-manifolds', 'differential-geometry']"
4521327,Dimension of Zariski Tangent over $\mathbb{Z}[2i]$,"I'm trying to compute the dimension of the Zariski Tangent space of the affine scheme $\operatorname{Spec} A$ , where $A = \mathbb{Z}[2i] \cong \mathbb{Z}[x]/(x^2 + 4)$ , at the point $(2, 2i)$ . Using the isomorphism in the previous sentence, we associate this point to the maximal ideal $(2, x)$ so that our residue field is simply $\kappa(p) = \mathbb{Z}_2$ . I know that the Zariski Tangent space in this case may be identified with the set of morphisms $$
\operatorname{Spec} \mathbb{Z}_2[\epsilon]/(\epsilon^2) \to \operatorname{Spec} A
$$ mapping the unique point $[(\epsilon)]$ to $p$ . Any such map of affine schemes should correspond to some $\mathbb{Z}_2$ -linear map of global sections $$
\mathbb{Z}[x] / (x^2 + 4) \to \mathbb{Z}_2[\epsilon]/(\epsilon^2)
$$ but now I think I'm starting to get a bit lost in the details of $\mathbb{Z}_2$ -linearity, where our generators should go and what the dimension as a $\mathbb{Z}_2$ vector space is. Is there any clean way to get the dimension of $T_{X,p}$ from this? Thanks for any help in advance",['algebraic-geometry']
4521337,Are there curves other than circles such that the line through two points on the curve is parallel the line tangent to the curve at midpoint?,"In a circle, if we pick any two distinct points $p_1$ and $p_2$ and draw a line passing through $p_1$ and $p_2$ that line is parallel to the line tangent to the circle at the midpoint of the arc with endpoints $p_1$ and $p_2$ . Are there curves other than cicles such that is true? What about parabolas?","['plane-curves', 'euclidean-geometry', 'geometry', 'plane-geometry']"
4521350,How to Find the Limit Correctly?,"I am refreshing my Calculus memory, and bump into this example (need feedback): $$f(x)= 
		\begin{cases}
			-x,		  & \text{if } x < 0 \\
			x,        & \text{if } 0 \le x < 1 \\
			1 + x,    & \text{if } x \ge 1
		\end{cases}$$ I have to find the limits / state if it does not exist. Correct me if I am wrong: $\lim_{x \rightarrow 0} f(x)$ does not exist $\lim_{x \rightarrow 1} f(x)$ does not exist $f(1) = 2$ $\lim_{x \rightarrow 1^{+}} f(x) = 2$ Are my answers correct? This is the plot",['limits']
4521384,The Hasse invariant at cuspidal elliptic curves in a generically ordinary family,"For an elliptic curve of the form $y^2 = f(x)$ where $f(x) \in \mathbb F_q[x]$ is a cubic polynomial with distinct roots, it is known (from Silverman's book, say) that the curve is supersingular precisely when the coefficient of $x^{p-1}$ in $f(x)^\frac{p-1}{2}$ is $0$ . My question is about the behavior of this coefficient in families of curves. Consider the following families of elliptic curves in characteristic $p>3$ . $E_1:y^2 = f_1(x,t) =  x(x-1)(x-t)$ and $E_2:y^2 = f_2(x,t) = x^3+3tx^2+3x+1$ where we can think of $t$ as an element of $\overline{\mathbb F}_q[t]$ . We can plug in any value $\alpha$ from $\overline {\mathbb F}_q$ for $t$ and so long $f_i(x,\alpha)$ doesn't obtain roots with multiplicity $>1$ , we still get an elliptic curve. The key difference between the above two families is that the former has node at $t=0,1$ and the latter has a cusp singularity at $t=1$ . Now I want to study the $x^{p-1}$ coefficient of $f_i(x,t)^{\frac{p-1}{2}}$ as an element of $\overline{\mathbb F}_q[t]$ . Let us call it $H_i(t)$ . Now, by doing some experiments in Sage (you can play around with these equations in the link) , I have seen that $H_1(t)$ does not vanish at $t=0,1$ (and can prove it using an old argument of Igusa) but $H_2(t)$ does seem to vanish at $t=1$ with a very large and predictable multiplicity: $\frac{p-1}{6}$ if $p\equiv 1 \pmod 6$ and $\frac{p+1}{6}$ if $p\equiv 5 \pmod 6$ . My question is why does this happen? So I've managed to tweak the equation of $E_2$ a bit so that the curve retains it's cusp but makes bookkeeping easier and have found a combinatorial proof of this high order vanishing. But I am looking for something more geometric that explains it. Somehow the polynomial vanishes on $\mathbb G_a$ but not on $\mathbb G_m$ ? This feels very much like the mod- $p$ modular form that goes by the name of the Hasse invariant or at least it's pullback under the map to the moduli space of elliptic curves. Is $\mathbb G_a$ ""very supersingular"" while $\mathbb G_m$ is ""ordinary"" under a suitable interpretation? Can someone give an explanation or direct me to a book/paper on what is going on and what the relationship of this polynomial is to the different types of singularities? NB: This phenomenon is clearly not unique to Elliptic Curves: Even for some higher genus cases, I see very similar patterns happening: If there are cusps or higher order singularities for a curve in a family of curves $C_t$ at a point $a$ , this large order of vanishing at $a$ is happening for an appropriate analog of the coefficient studied above.","['elliptic-curves', 'positive-characteristic', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4521385,Expected number of coin streaks,"Source: Quant interview. You have a coin which comes heads with probability $1/5$ , and you
toss it $700$ times. If there are multiple coins in a row (could be just
1), we call them/it a streak. For example, if we get HTHTT, we
have 4 streaks: first H, second T, third H, and last two T. What is the expected number of streaks in $700$ tosses? Attempt : I interpreted the number of streaks as the number of switch between either H to T or T to H in the sequence + 1. The problem has a markov chain structure and I was able to obtain the following equations, given $S(n)$ number of strike in sequence with n coins: $S(n|H)=0.2S(n-1)+0.8(1+S(n-1))$ and $S(n|T)=0.8S(n-1)+0.2(1+S(n-1))$ This implies $S(n) = S(n|T)p(T) + S(n|H)p(H) = S(n-1) + 8/25$ and therefore $E[S(700)]=224$ . This solution matches some numerical simulations but always underestimates a bit (e.g. with $10^6$ iterations I get $224.64$ and also smaller $n$ tend to underestimate).","['statistics', 'markov-chains', 'probability']"
4521441,Can you introduce the identity as $e\cdot e = e$ for groups?,"I have been reading The Number System by Thurston.  In this book he introduces a thing he calls a 'hemigroup' which I think would more conventionally be an 'abelian cancellative monoid'.  Hemigroups are $(G,\cdot)$ such that $(x\cdot y)\cdot z = x \cdot (y\cdot z)$ ; $x\cdot y = y\cdot x$ ; if $x\cdot y = x\cdot z$ then $y = z$ ; there is $e$ such that $e\cdot e = e$ . Well, it's then easy to show that $e$ is an identity: $x\cdot e = x\cdot(e\cdot e) = (x\cdot e)\cdot e$ and then by cancellativity $x\cdot e = x$ . My question then is: can you introduce an identity for groups the same way ?  So instead of defining a group as $(G,\cdot)$ where $(x \cdot y)\cdot z = x \cdot (y\cdot z)$ ; there is an $e$ such that $e\cdot x = x$ for all $x$ ; For all $x$ there is an $x'$ such that $x'\cdot x = e$ ; can you replace the second axiom above by 'there is an $e$ such that $e\cdot e = e$ ' and derive that $e$ is an identity? (I think you probably can not, but I'm not sure.  Certainly for groups it's easy to show that if $a\cdot a = a$ then $a = e$ , but you do this by using that $e$ actually is a (left-)identity.  However I am very bad at finding these kind of proofs or showing there are none: I can follow them, but I am terrible at finding new ones.)","['group-theory', 'abstract-algebra']"
4521459,Permuting subgroups with the same finite index in free abelian group,"Let $H$ be a subgroup of $\mathbb{Z}^2$ with finite index $m$ . Let $\phi$ be an automorphism on $\mathbb{Z}^2$ . Then $\phi$ corresponds to a matrix in $ \operatorname{GL}(2, \mathbb{Z})$ . Question : What is the bound for the smallest $n \in \mathbb{N} \setminus \{0\}$ , such that $\phi ^n(H) = H$ ? Is it possible to get a linear or even sublinear bound? Thoughts so far : Let $a(m)$ be the number of subgroups in $G$ with index $m$ , then $a(m)$ is bounded by a polynomial but not a linear function in $m$ . It follows that $n$ is bounded by a polynomial in $m$ . I was wondering if this bound can be improved,  it seems unlikely for the automorphism to go through all the subgroups of this index. Any references for this question would be really appreciated, thanks for reading.","['permutations', 'matrices', 'linear-algebra', 'geometric-group-theory', 'group-theory']"
4521461,In the proof of showing $\operatorname{Ad}_{*} =\operatorname{ad} $ (adjoint representations of Lie Group(Algebra).,"I'am reading the Lee's Introduction to smooth manifolds, 2nd edition, p.534, Theorem 20.27 and stuck at understanding some equality : Why the underlined equality is true? Here, $\operatorname{ad} : \mathfrak{g} \to \mathfrak{gl(g)}$ is the adjoint representation of $\mathfrak{g}$ and $\operatorname{Ad}_{*} :\mathfrak{g} \to \mathfrak{gl(g)}$ is the induced Lie algebra representation which is defined as follows (His book p.195) : Here, the 'L' in the above underlined definition of $Y$ is defined as follows (His book, p.191) : Let $G$ be a Lie group. Let $v\in T_{e}G$ be arbitrary. Then define a (left invariant) vector field $v^{L}$ on $G$ by $$ v^{L}|_g = d(L_g)_{e}(v)$$ . Note that by the proof of his book, Theorem 8.37 (p.191), the correspondence $L$ is an isomorphism $T_eG \to \operatorname{Lie}(G) =: \mathfrak{g}$ with inverse $\epsilon : \operatorname{Lie}(G) \to T_eG$ , given by $\epsilon(X) := X_e$ . It seems that our question is somewhat basic but I don't know how to establish the equality rigorously. What should I notice? First attempt : Let $H:=GL(\mathfrak{g})$ and let $\mathfrak{h}:=\mathfrak{gl(g)}$ be its Lie algebra. Let $\gamma : t \mapsto \operatorname{exp}tX$ . Then $$\frac{d} {dt} |_{t=0} (\operatorname{Ad}(\operatorname{exp}tX)) 
= (\operatorname{Ad} \circ \gamma)^{'}(0) = d(\operatorname{Ad})_{\gamma(0)}(\gamma^{'}(0)) = d(\operatorname{Ad})_{e}(X_e) \in T_eH$$ . And, $$\operatorname{Ad}_{*}(X) = (d(\operatorname{Ad})_{e}(X_e))^{L} \in \operatorname{Lie}H =: \mathfrak{h} = \mathfrak{gl(g)} $$ And why the difference appear?
I think that for the first equality, more correct form is, $$(\operatorname{Ad}_{*}X)Y = (\frac{d} {dt} |_{t=0} (\operatorname{Ad}(\operatorname{exp}tX)))^{L}(Y) $$ Did I make point out well? If so, it remains(the second equality) to show that $$(\frac{d} {dt} |_{t=0} (\operatorname{Ad}(\operatorname{exp}tX)))^{L}(Y) = \frac{d} {dt} |_{t=0}(\operatorname{Ad}(\operatorname{exp}tX)Y) $$ How can we show this? How the passage of $Y$ into the derivative $\frac{d}{dt}|_{t=0}$ ( ; i.e., the second equality) possible? If we can use the Chain rule, how can we apply the chain rule more rigorously? Can anyone helps?","['lie-algebras', 'lie-groups', 'differential-geometry']"
4521465,Approximating the product of two positive elements in a $C^\star$ algebra by a positive element,"Let $\mathcal{A}$ be a (unital) $C^\star$ algebra, and $a,b$ be two positive elements of $\mathcal{A}$ . We know that $ab$ is not positive in general (in fact, it is positive iff $a$ and $b$ commute), but it is always true that $\sigma(ab)\geq 0$ . I was wondering if it would be possible to approximate $ab$ by a positive element in $\mathcal{A}$ , that is, if for all $\epsilon>0$ there exists some $c>0$ in $\mathcal{A}$ such that $\lVert ab - c \rVert < \epsilon$ . My obvious first attempt was with $c=\frac{ab+ba}{2}$ . But the noncommutativity of $a$ and $b$ blocks this from working. I was wondering if functional calculus techniques might work, but I am not sure. I know that the set of hermitian elements is not dense in $\mathcal{A}$ , so the approximation might actually not work as well, but I don't have a counterexample at hand. I would be ineterested in both finite and infinite dimensional settings. (Of course, the problem is trivial if $\mathcal{A}$ is commutative.)","['c-star-algebras', 'positive-semidefinite', 'functional-analysis', 'operator-algebras']"
4521471,Solving $y' = \frac{x^2 + 2xy - y^2 - 2}{x^2 - 2xy - y^2 + 2}$ without tricks,"I would like to solve the equation: $y' = \frac{y^2 - 2xy - x^2 + 2}{y^2 + 2xy  - x^2 - 2} \tag{1}$ From that, we have: $y'(y^2 + 2xy  - x^2 - 2) = y^2 - 2xy - x^2 + 2 \implies $ $\frac{(2x + 2yy')(x+y) - (x^2 + y^2 + 2)(1 + y')}{(x+y)^2} = 0 \implies $ $(\frac{x^2 + y^2 + 2}{x + y})' = 0 \implies $ $\frac{x^2 + y^2 + 2}{x + y} = C$ Where the last part agrees with the solution in the book. The ""trick"" is that I first looked at the solution in the book, differentiated it, and then applied here the process in reverse, so it looks like a solution. I was obviously not happy with that approach, so I looked online for a better solution. Unfortunately, I could not see a generic pattern in neither of the two existing solutions [solution 1 , solution 2] I found. Does there exists a more standard solution to the equation? Are the two existing solutions common, and are not considered as ""tricks""? Why would they be less of a trick than the one I described above? Thanks!",['ordinary-differential-equations']
4521490,"Is this last statement a convergence in probability, why does this converse holds?","Theorem 7.6 I can see why does almost sure convergence implies convergence in probability, and I don't see why does this converse also holds. Thank you for any help Thank you all for the posts, here it does not say anything about the nested sequence, so I did not think about it this way. If increasing sequence is provided, is that sufficient prove this solely by the continuity of probability?","['convergence-divergence', 'analysis', 'probability-theory', 'probability']"
4521497,Proving mean value theorem in case of vector valued functions,"Let $f:U\to \mathbb R^n$ be differentiable on an open subset $U\subset \mathbb R^m$ . Suppose that $a,b\in U$ are such that the set $L:=\{a+(b-a)t: t\in [0,1]\}\subset U.$ Then, the following holds: $\|f(b)-f(a)\|\le \sup_{z\in L}\|Df_z\|_{\text{op}}\,\|b-a\|$ , where op stands for the operator norm. Proof: If $f(b)=f(a)$ , then there is nothing to prove so assume that $f(b)\ne f(a)$ . Define $u:=f(b)-f(a)$ and $g(t):=u\cdot f(a+t(b-a)), t\in [0,1]$ and $\cdot$ in definition of $g$ is the usual dot product. $g$ is differentiable on $(0,1)$ and continuous on $[0,1]$ .  So by mean value theorem, there exists a $\rho\in (0,1)$ such that $$g(1)-g(0)=g'(\rho)=u\cdot \color{blue}{f'(a+\rho(b-a); b-a)}=u\cdot (f(b)-f(a))\tag 1$$ The blue colored term in $(1)$ is the directional derivative of $f$ at $a+\rho(b-a)$ along $b-a$ . Define $\alpha(t):=a+t(b-a)$ . It follows by $(1)$ that $$u\cdot (f(b)-f(a))=u\cdot Df_{\alpha(\rho)}(b-a)\tag 2$$ It follows by $(2)$ that $\|u\|^2=\|u\cdot Df_{\alpha(\rho)}(b-a)\|\le\|u\|\,\|Df_{\alpha(\rho)}\|_{\text{op}}\, \|b-a\|\tag 3$ The desired inequality follows by $(3)$ . Explanation for: Para before $(1)$ : I used the following lemma: If $h:\mathbb R\to \mathbb R^n$ is a function such that $\lim_{x\to a} h(x)$ exists for some $a$ , then for any $u\in \mathbb R^n$ , $\lim_{x\to a} u\cdot h(x)=u\cdot (\lim_{x\to a}h(x)).$ $(3)$ : For any linear map $T:\mathbb R^m\to \mathbb R^n$ , $\|Tx\|\le \|T\|_{\text{op}}\,\|x\|$ for all $x\in \mathbb R^m$ . Is my proof correct? Thanks.","['multivariable-calculus', 'calculus', 'solution-verification', 'real-analysis']"
4521547,How can I increase the value of a multiplier the larger it gets?,"I'm creating an app that turns fitness data into ""scores"". An example will better describe the problem I'm having... Let's say you do 5 sets of Squats as an example. Below are your sets (weight and reps) 1. 20 kg x 20 reps
2. 30 kg x 15 reps
3. 35 kg x 12 reps
4. 40 kg x 10 reps
5. 45 kg x  8 reps I create a ""score"" for each set by multiplying the weight x reps . In this case 1. 20 kg x 20 reps = 400
2. 30 kg x 15 reps = 450
3. 35 kg x 12 reps = 420
4. 40 kg x 10 reps = 400
5. 45 kg x  8 reps = 360 I want the weight to have more of a ""value"" the larger it gets. Preferably, in this example, I'd want the score of the 5th set to be greater than the score of the 4th set.
The best solution I've found so far is to square the weight value like so $weight^2*reps$ 1. 20 kg x 20 reps = 8000
2. 30 kg x 15 reps = 13500
3. 35 kg x 12 reps = 14700
4. 40 kg x 10 reps = 16000
5. 45 kg x  8 reps = 16200 The actual numeric values are arbitrary because I use them to create a percentage score so it doesn't matter that it's in the thousands. It's mostly the difference in between the values that I'm interested in. However this causes too great an increase as the weight value gets higher. The difference between set 1 and set 5 is too drastic. Is there another way I can cause an exponential increase with a flatter curve? Should I instead be using the initial weight value as a base value or something along those lines? Apologies for the long question, my math knowledge is limited and I've been mulling this over for a while but still can't seem to word the problem correctly.","['data-analysis', 'statistics', 'exponential-function']"
4521575,Invariant description of fibre bundles,"The question: What is the (or an) invariant definition of fibre bundle with fibre F and structure group G, without reference to a fixed cover of the base and transition functions for local trivializations? The context: In Steenrod's ""Topology of fibre bundles"", Part I, Section 2, he gives the definition of a fibre bundle with structure group G and coordinate system (local trivializations in which the transition functions live in some fixed subgroup G of homeomorphisms of the fibre). A fibre bundle is then an equivalence class of such (where you get to change local trivializations in a controlled way). This is all standard and written down in many other places, for example in Husmoller's book ""Fibre bundles"". Then, at the end of Section 6, Steenrod says that Ehresmann (Sur la théorie des spaces fibrés) has a more invariant alternative definition of fibre bundle with structure group G, phrased in terms of an associated principle bundle. I wasn't able to get a hold of the reference, and the references of Ehresmann that I did find didn't seem any more invariant.","['algebraic-topology', 'differential-geometry']"
4521594,How to derive the polar equation of a circle.,"I have learnt that the standard equation of a circle whose centre is at (a,b) is $$(x-a)^2 + (y-b)^2 = c^2
$$ I am trying to derive the polar equation of this circle but I am unfortunately stuck. $$\begin{align}(x-a)^2+(y-b)^2&=c^2\\
(r\cos\theta-a)^2+(r\sin\theta-b)^2&=c^2\\
r^2\cos^2\theta-2ra\cos\theta+a^2+r^2\sin^2\theta-2rb\sin\theta+b^2&=c^2\\
r(r-2a\cos\theta-2b\sin\theta)&=c^2-a^2-b^2\end{align}$$ The following is stated in James Stewart's book on Precalculus but there is no proof stated in this book. So I am trying to reach a similar result by beginning at the standard equation of a circle.","['analytic-geometry', 'circles', 'trigonometry', 'polar-coordinates', 'algebra-precalculus']"
4521597,Radical of Infinite Product of Primes,"I somehow stumbled across this thing. $$\lim_{n \to \infty}({p_1\times p_2\times p_3\times\cdots \times p_n})^{\frac{1}{p_n}}$$ Where $p_i$ is the $i$ -th prime. I wanted to know if this converges or not. So I wrote a python program, and computed it up to $n = 300000$ and got this: $2.7168952344276653$ , with $p_n = 4256249$ It seems to approach $e$ , but that doesn't make sense, $e$ doesn't have anything to do with prime numbers. It was converging extremely slowly, and the program was also very slow, so I wasn't able to compute it for higher values of $n$ . I want to know what this limit approach?","['analytic-number-theory', 'infinite-product', 'limits', 'radicals', 'prime-numbers']"
4521654,"Convex polygons - For $n>3$, $\sum_{i=1}^{n} \cos\theta_i < \cos\left(\sum_{i=1}^{n} {\theta_i}\right)$","It seems that for any convex polygon $P$ with $n>3$ sides and $n$ interior angles $\theta_i$ , $$\sum_{i=1}^{n} \sin\theta_i > \sin\left(\sum_{i=1}^{n} {\theta_i}\right)$$ $$\sum_{i=1}^{n} \cos\theta_i < \cos\left(\sum_{i=1}^{n} {\theta_i}\right)$$ The first inequality is really straightforward to prove. As $\sum_{i=1}^{n} {\theta_i}=(n-2)180^°$ , thus $\sin\left(\sum_{i=1}^{n} {\theta_i}\right)=0 \space \forall n>2$ . The fact that $\sin \theta_i>0 \space \forall \theta_i\neq (k*180)^°$ concludes the proof. However, I have not been able to prove the second inequality. We have that $\cos\left(\sum_{i=1}^{n} {\theta_i}\right)=-1 \space \forall n=2m+1, \space m\in \mathbb N$ and $\cos\left(\sum_{i=1}^{n} {\theta_i}\right)=1 \space \forall n=2m, \space m\in \mathbb N$ . Also, $\cos \theta_i\geq 0 \space \forall \theta_i\leq 90^°$ and $\cos \theta_i< 0 \space \forall 90^°<\theta_i<180^°$ . Any hint, help or reference proving the second inequality would be welcomed. Thanks! EDIT After working on a proof of the second inequality using induction, by trial-and-error experiments, it seems that, for triangles, $\sum_{i=1}^{3} {\cos\theta_i}\leq 1.5$ ; for quadrilaterals, $\sum_{i=1}^{4} {\cos\theta_i}< 0.5$ ; for pentagons, $\sum_{i=1}^{5} {\cos\theta_i}< -0.5$ ; for hexagons, $\sum_{i=1}^{6} {\cos\theta_i}< -1.5$ ; for heptagons, $\sum_{i=1}^{7} {\cos\theta_i}< -2.5$ , ... Thus, the second inequality could be improved to affirm that for any convex polygon $P$ with $n$ sides and $n$ interior angles $\theta_i$ , $$\sum_{i=1}^{n} \cos\theta_i \leq 4.5-n$$ , with equality only when $n=3$ . The pattern arises as any triangle maximizes the sum of the cosines of interior angles when it has three acute angles equal to $60^º$ ; any quadrilateral maximizes the sum of the cosines of interior angles when it has three acute angles tending below to $60^º$ , and one obtuse angle tending above to $180^º$ ; and for $n>3$ , this pattern continues: any convex polygon maximizes the sum of the cosines of interior angles when it has three acute angles tending below to $60^º$ , and the rest of angles are obtuse angles tending above to $180^º$ . A proof by induction could then be constructed, proving the triangle case, and showing that adding an additional vertex to form some convex $(n+1)$ -polygon anexes a triangle of angles $\alpha,\beta$ and $(180^º-(\alpha+\beta))$ with one shared side with the $n$ -gon, so the sum of the cosines of interior angles of the $(n+1)$ -gon is $\cos\theta_{1}+\cos\theta_{2}+...+\cos(\theta_{n-1}+\alpha)+\cos(\theta_{n}+\beta)+\cos(180^º-(\alpha+\beta))$ . Applying the trigonometric identities, we need to show that $$\cos\theta_{1}+\cos\theta_{2}+...+(\cos\theta_{n-1}\cos\alpha-\sin\theta_{n-1}\sin\alpha)+(\cos\theta_{n}\cos\beta-\sin\theta_{n}\sin\beta)-\cos(\alpha+\beta)\leq \cos\theta_{1}+\cos\theta_{2}+...+\cos\theta_{n-1}+\cos\theta_{n}-1$$ Or, simplifying, $$(\cos\theta_{n-1}\cos\alpha-\sin\theta_{n-1}\sin\alpha)+(\cos\theta_{n}\cos\beta-\sin\theta_{n}\sin\beta)-\cos(\alpha+\beta)\leq \cos\theta_{n-1}+\cos\theta_{n}-1$$ , where at most one of $\alpha,\beta$ or $(\alpha+\beta)$ is equal or greater than $90^º$ . Proving the above inequality would finish the proof. However, I have not been able to prove it yet.","['trigonometry', 'angle', 'geometric-inequalities', 'polygons']"
4521701,How to remember trig identities?,"Suppose I have a trig function $T: \Bbb{R} \rightarrow \Bbb{R}$. I want to be able to derive four basic properties: $$T(x) \cdot T(y)$$
$$T(x) + T(y)$$
$$T(x+y)$$
$$T(cx)$$ where $c$ is some scalar. I know there are a bunch of identities: reciprocal, quotient, Pythagorean,  co-function, even-odd. And then some formulas: product-to-sum, sum-to-product, sum-difference, double angle, half-angle/power-reducing. Here is a list. That's a lot to memorize and some of them seem to overlap. Why do the four basic properties I mentioned  require so many identities to learn? It is a bit cumbersome. Is there an easier way to learn how to do the basic arithmetic operations with trig functions?","['trigonometry', 'mnemonic']"
4521710,Is there a general approach to solving trigonometric equations? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question We've been solving equations in school that mainly include forms of sin, cos and tan but usually I don't know how to approach them. It's not like solving a quadratic where there's just a solution formula.","['trigonometry', 'geometry']"
4521724,"Solving $\cos3x=-\sin3x$, for $x \in [0,2\pi]$","For my first step, I would choose to either divide both sides or add sin3x to both sides. My professor in Pre-Calc told me I shouldn't divide any trig functions from either side. Despite this, I can only find a solution this way. $$\frac{\cos3x}{-\sin3x}=1$$ $$-\tan3x=1$$ $$\tan3x=-1$$ Which would lead me to my method of solving trig equations which is: $$-\tan\,\epsilon\,Q2\quad-\tan\,\epsilon\,Q4$$ $$\tan x=1 \, at\,\frac{\pi}{4}$$ $$Q2:\pi-\frac{\pi}{4}=\frac{3\pi}{4}$$ $$Q4:2\pi-\frac{\pi}{4}=\frac{5\pi}{4}$$ However, this is only for $\tan x=-1$ , my professor told me to apply the ""modifications of x"" after. $$3x=\frac{3\pi}{4}=\frac{3\pi}{4}\div3=\frac{\pi}{4}$$ $$3x=\frac{5\pi}{4}=\frac{5\pi}{4}\div3=\frac{5\pi}{12}$$ Is this the correct way to solve this equation? I believe dividing out $\sin3x$ leads to domain or range errors. However, I tried solving by adding $\sin3x$ to both sides first but could not figure out how to isolate the trig functions. I will clarify anything needed in the comments.","['algebra-precalculus', 'problem-solving', 'trigonometry']"
4521729,Smooth/Lipchitz surjection from torus to sphere that preserves the invariant measure,"Let $\mathbb{T}^2$ be the flat torus with Haar measure $\mu_{\mathbb{T}^2}$ , and $\mathbb{S}^2$ the sphere with the spherical (rotationally-invariant) measure $\mu_{\mathbb{S}^2}$ . Is is possible to construct a smooth (or even just Lipchitz-continuous) surjection $\Theta:\mathbb{T}^2\to \mathbb{S}^2$ such that $\mu_{\mathbb{T}^2}\circ \Theta^{-1}=\mu_{\mathbb{S}^2}$ ? i.e. for any measurable set $A\subseteq \mathbb{S}^2$ , $\mu_{\mathbb{T}^2}(\Theta^{-1}(A))=\mu_{\mathbb{S}^2}(A)$ . It is indeed possible to construct smooth surjections from the torus to the sphere (e.g. this question ). It seems to me that those surjections cannot possibly preserve the measures. For example, a natural map is to send the torus to a cylinder, and then map the cylinder into a sphere, but the equal-area projection from a cylinder into the sphere is not Lipchitz continuous at the poles. I would appreciate some references on the matter, thanks!","['measure-theory', 'differential-topology', 'haar-measure', 'topological-groups']"
4521733,"Do ""equi-homomorphic"" group elements lie in the same orbit?","Given a structure (of pretty much any type) $\mathfrak{A}$ , there is a natural ""sameness"" relation on the elements of $\mathfrak{A}$ : we set $a\sim b$ iff there is an automorphism, in whatever sense is appropriate, of $\mathfrak{A}$ sending $a$ to $b$ . This is often called the orbit equivalence relation , and its classes are called orbits . I'm interested in the following a priori coarsening of the orbit relation: set $a\approx b$ iff there are homomorphisms (again, in whatever sense is appropriate) $f,g$ with $f(a)=b$ and $g(b)=a$ . In general it's not hard to see that $\approx$ can be much coarser than $\sim$ , even when $\mathfrak{A}$ is finite. However, I don't know of any particularly natural examples of finite structures where $\sim$ and $\approx$ separate. Question : Is there a finite group $G$ such that $\sim$ and $\approx$ do not coincide in $G$ ? If $G$ is abelian, then the answer is negative by brute-force application of the classification of finite abelian groups. However, I don't see any way to extend this to non-abelian groups. I vaguely recall seeing that the answer remains negative for all groups, but that was long ago and I can't recall how the proof (supposedly) went. Separately, I'm interested in any sources on $\approx$ . In particular, does it too have a snappy name?","['group-homomorphism', 'automorphism-group', 'universal-algebra', 'finite-groups', 'group-theory']"
4521747,Why is average defined as sum of the data divided by the total number of data?,"Wherever I search on the internet, the definition of average is more like just giving a formula for it- sum the values and then divide the result by the number of values, you
get a number, that number is the average. Okay, but what does it represent? A central value? Okay but how did mathematicians know that this central value was sum of the values divided by the number of values? In a nutshell, I'm curious to know what led to the formula of average as sum of the data / number of data. I believe the answer lies in knowing the significance of the concept of average.","['average', 'statistics', 'probability']"
4521783,Is $X$ (independent variable) considered random in linear regression? Does correlation make sense as an unbiased estimator?,"Basically my question comes from two parts. In simple definitions of LR, we consider X to be a known constant. Thus $$var(Y) = Var(b_0+ b_1x + e) = Var(e)$$ . Great. However, a great deal of linear regression has to do with Cov(X,Y). This is used to estimate our parameters, and it assumes that X is a random variable. How do we distinguish between these two schools of thought? I'm ultimately trying to show that sample correlation between X and Y for simple linear regression is an unbiased estimator of true correlation between X and Y. Does this question even make sense, given that sample correlation of $$Corr(X,\hat Y) = Corr(X, b_0 + b_1 X) = Corr(X, b_1 X)=1$$ always? But even if this doesn't make sense, aren't we still able to make claims in linear regression about the correlation between X and Y because $$E[b_1] = E[\beta_1] \\ b_1 = r\frac{\sigma_y}{\sigma_x}$$","['linear-regression', 'self-learning', 'statistics', 'covariance']"
4521784,Is any symmetric matrix of which its diagonalvector equals its eigenvalues a diagonal matrix?,"Let $A$ be an $n \times n$ a real, positive semidefinite and symmetric matrix. Suppose we know that the diagonal of this matrix is equal to its eigenvalues. (So if we have eigenvalues $\lambda_1, ... \lambda_n$ , then $diag(A)$ is a permutation of $(\lambda_1, ..., \lambda_n)$ .) Can we prove from these assumptions that $A$ is a diagonal matrix? I.e. all non-diagonal entries are $0$ ? And if so, how? It's equal to this questio n, only the counterexample given there is not symmetric. Ideas so far: Proving it the other way around is easier. If a matrix is diagonal and $n \times n$ , then its entries are equal to its eigenvalues. But this doesn't help with proving it the other way around. I know that since $A$ is real, and positive semidefinite, we can do a diagonalization by eigendecomposition to find $A = VBV^{T}$ with $V$ an orthonormal matrix and $B$ a diagonal matrix with eigenvalues on the diagonal. I've tried to prove from here that the $diag(A)$ being a permutation of $diag(B)$ must mean that $V$ is a permutation matrix, i.e. $V$ 's orthonormal rows consist of $n-1$ zeros and a single 1, but I couldn't get the math working in cases when there are two equal eigenvalues on $B$ 's diagonal.","['diagonalization', 'linear-algebra', 'eigenvalues-eigenvectors']"
4521825,"The hardest geometry question with ""a triangle"" and ""a circle"" - Circle intersecting triangle equally in 5 parts","I received this question long time ago from one of my old friends who is mathematician/physicist. He called it the hardest geometry question with ""a triangle"" and ""a circle"". I am not sure if he know the answer or not. Here it is. Note: These picture are just the example of possible answer. The actual answer might look like one of these or not like any of these. The circle intersects the triangle and divides the area into 5 parts. Each 5 parts of the red-fill areas have equal area. The radius of circle is 1. That is all the information the question gives. Then the question asks these two things. Find the distance from the center of the circle to the centroid of the triangle. If there are more than 1 possible solutions, find longest and shortest possible the distance from the center of the circle to the centroid of the triangle. Find length of outer perimeter of the new intersecting shape. If there are more than 1 possible solutions, find longest and shortest possible length of outer perimeter of new intersecting shape. Note : If you think that there is only 1 possible solutions that the triangle is isosceles, you also have to proof that there is really only 1 possible solutions. I have to say that I don't even know how to start to solve this question. The question doesn't even specify the type of the triangle. I think the triangle has to be isosceles triangle, but I remember that the question doesn't specify it. Edit: I realize where I get this question from. And for now, I doubt if the triangle has to be isosceles or not. Edit(2) : Due to some technical reason, the account of the real original poster is merged to my account. That is why there is a delay of the update on this question. I do contact and take to the real original poster. Here is why he edit this question. After sometimes, I decide to change the original question a bit by adding more picture. The original question is actually tricky that give only the first image and trick anyone who try to solve it that that is how the answer look like. R. J. Mathar and John Bentin answer is good but that isn't what the original question ask for. Edit(3) : The question might not have analytical solution. So, if there is no analytical solution, numerical solution is acceptable.","['area', 'circles', 'geometry', 'calculus', 'triangles']"
4521870,What is probability that 10th toss of a weighted coin is a head given that the first 9 tosses were heads?,"Question : We are given a coin which lands on head with probability p such that $ P(p \le x) = x^5$ . Given that the first 9 tosses of the coin lands on head, what is the probability that the 10th toss is a head? I have been practicing several probability problems, but having read this one I have no idea how to start. I am wondering if there is a way to determine a range on the value of p, but I do not have any idea how to use the information on the  previous tosses. I would appreciate someone giving me some hints/some resources I can use to learn about how to solve these types of question.","['conditional-probability', 'statistics', 'probability']"
4521872,"$f'$ decreasing, is $f'$ continuous?","Suppose $f:(0,\infty)\rightarrow \mathbb R$ is positive and differentiable and $f'$ is decreasing to $0$ . Is $f'$ continuous? I know that $f'$ has to have intermediate value property so it can not have any discontinuous jumps, but is the assumption that $f'$ is decreasing sufficient for the continuity? Proof or counterexample would be great","['calculus', 'derivatives', 'real-analysis']"
4521900,"In the field $No$ of surreal numbers, does $\underbrace {\frac 1 \omega + \frac 1 \omega + ...}_{\omega\text{ times}}= 1?$","The question is in the title.  It is known that $\omega$ $\cdot$ $\frac 1 {\omega}$ = 1, but can the expression on the left-hand side be replaced by the infinite sum in the title?  If so then by the fact that $No$ is a field suggests that $\frac 1 {\omega}$ can be thought of as a probability measure.  On the other hand, consider the following examples given by Prof. Conway (onpp.43-44 of On Numbers and Games : It is interesting to note that our definitions of infinite sums have in a certain sense to be ""global"", rather than as limits of partial sums, because limits don't seem to work. For instance, the limit of the sequence 0, $\frac 1 2$ , $\frac 2 3$ , $\frac 3 4$ ,... ( $\omega$ terms) is not 1, at least in the ordinary sense, because there are plenty of numbers in between.  A simpler, but sometimes less convincing, example of the same phenomenon is given by the sequence 0,1,2,3,... of all finite ordinals, which one would expect to tend to $\omega$ , but obviously can't, since there is a whole Host of numbers greater than every finite integer but less than $\omega [here Prof. Conway gives us his favorites of such numbers--my comment].","['probability', 'surreal-numbers']"
4521910,"Advanced Functions, How to simplify $\tan \frac{5\pi}{12}$?","I was asked to find the exact value of $\,\tan \left(\frac{5\pi}{12}\right)$ , so I did up until this point where I got completely stuck. I split the ratio into two, so $\;\dfrac{5\pi}{12} = \dfrac{\pi}4 + \dfrac{\pi}6\,.$ using the formula, $\tan(x+y) = \dfrac{\tan x+\tan y}{1-\tan x\tan y}$ : $$\tan(\pi/4 + \pi/6) = \frac{1+1/\sqrt 3 }{1-(1)(1/\sqrt 3)
}$$ ... $$\tan(\pi/4 + \pi/6) = \frac{3+\sqrt3}{ 3-\sqrt3 }$$ I'm stuck here, I looked up the answer online, and apparently, you have to times numerator and denominator both by $\,3+\sqrt3\,,\,$ but it's cheating, I don't understand why you have to do that.","['trigonometry', 'functions']"
4521927,Union and difference between two sets using only Symmetric Difference and Intersection operation.,"Write the union and difference of two sets $A$ and $B$ using only the symmetric difference $\bigtriangleup$ and intersection $\cap$ operations. NOTE: For $A$ and $B$ subsets of $\Omega$ , we define $$
A - B = A \cap \neg B
$$ ( $\neg B$ is the complement set of $B$ ) I can't actually think of a solution to this problem that use only those two operations. My answer to the union was $$
(A \bigtriangleup B) \cup (A \cap B), 
$$ but the question states that we are supposed to use only two operations defined. For the difference I been thinking about a way to solve the problem but it's not clear to me.","['elementary-set-theory', 'set-theory']"
4521933,Inability to divide because of initial value,"I apologize in advanced for my confusion, however, in my ODE textbook, it gives the following example: $$\frac {dy}{dt} = (1+y)t$$ with initial value of $y(0) = -1$ . I am confused with this question because although from my understanding, there is definitely a solution to this equation, $$y=ke^\frac{t^2}{2}-1,$$ taken by dividing both sides by $(1+y)$ , multiplying both sides by $dt$ , and finally integrating both sides. With $k=0$ , $$y=0e^\frac{t^2}{2}-1.$$ Then, utilizing $y(0)=-1$ , solving the equation, $$y=0e^\frac{0^2}{2}-1 = 0(1)-1 = -1.$$ Given this, ultimately the solution to the initial-value is $$y = -1.$$ However, my textbook states that even dividing both sides of the differential equation by $(1+y)$ is impossible because then the differential equation would be undefined at $y(0) = -1$ , however, even looking the solution up online, it seems that the solution does indeed exist? I would appreciate any and all help on this. Thanks so much.",['ordinary-differential-equations']
4521948,prove that is big-Oh [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Show that $2\sqrt n + n^{1/3} \log_2(n) = O(\sqrt n)$ . I need help understanding how to solve this limit: $$ \lim_{n\to \infty}\frac{2\sqrt n + n^{1/3} \log_2(n)}{\sqrt n}$$ thanks","['limits', 'asymptotics']"
4521959,Exponentiating only the rotational component of SE(3),"I'm studying the SE(3) group for an engineering application and I'm curious about the difference between $SE(3)$ and $SO(3) \times \mathbb{R}^3$ . Practically, I'd like to optimize in the tangent space of $SE(3)$ and differentiate through its $\mathrm{Exp}$ operator. I'm wondering if I can get away with just differentiating through the $\mathrm{Exp}$ of $SO(3)$ and leaving the translation component untouched. The paper I linked has the following phrase on page 44 SE(3) is diffeomorphic to SO(3) × R3 as a manifold, where each element is described by 3 · 3 + 3 = 12 coordinates (see §7.2). SE(3) is not isomorphic to SO(3) × R3 as a group, since the group multiplications of both groups are different. It is said that SE(3) is a semidirect product of the groups SO(3) and R3. I'm not sure what the implication is of ""diffeomorphic as a manifold"" and ""not isomorphic as a group"". Is $\mathrm{Exp}(SO(3)) \times \mathbb{R}^3$ still a valid retraction of $SE(3)$ ? Why?","['rigid-transformation', 'lie-algebras', 'group-theory', 'exponential-function', 'lie-groups']"
4521961,Topological solution for combinatorial geometry problem,"Let $X$ be a finite set of points in Euclidean plane. It is given that those points are not colinear. i.e. there is no line such that includes all points of $X$ . Show that for any function $f \colon X \to \{0,1\}$ (coloring the points with 2 color), there is a line $L$ on the plane such that satisfies (a) $L$ contains at least 2 points in $X$ . Namely, $|L \cap X| \geq 2$ . (b) $\forall a,b \in L \cap X$ , $f(a) = f(b)$ (all points of $X$ on $L$ have same color). I reduced this geometric problem in terms of equivalence relation. Let $Y$ be a set of 2-subsets of $X$ , and define $Z = \{\{a,b\} \in Y | f(a) = f(b) \}$ . Give equivalence relation $\sim$ on $Y$ to be $\{a,b\} \sim \{c,d\}$ iff $a,b,c,d$ are colinear. If $\mathcal{P}$ is a partition induced by $\sim$ , then the problem is asking for the existence of $P \in \mathcal{P}$ such that $P \subseteq Z$ . When presenting this problem, my instructor said that combinatorial proof is very complicated, but there is a simple topological proof. Indeed I cannot figure out what to do after the reduction. What are the topological properties that I can exploit to attack this problem? How can I lead to a proof with those properties? Thanks in advance for every help, hint, or solution.","['combinatorial-proofs', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics', 'general-topology']"
4522002,Intuition behind the Law of large numbers,"For a random variables $X_1,X_2,\ldots, X_n$ , the law of large numbers says that the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed, i.e., $$\lim_{n\rightarrow\infty}\sum_{i=1}^n\frac{X_i}{n}=\bar{X},\tag{1}$$ where $\bar{X}$ is the expected value. But the same doesn't hold for the following: $$\sum_{i=1}^nX_i-n\bar{X}.\tag{2}$$ In fact, $(2)$ tends to increase in absolute value as $n$ increases. I am finding it very surprising and hard to understand. Because I expected that if the average tends to expected value, then the sum should tend to $n\times$ expected value. Let $Y=\sum_{i=1}^nX_i-n\bar{X}$ , then $$\frac{Y}{n}=\frac{\sum_{i=1}^nX_i}{n}-\bar{X}\tag{3}.$$ The way I try to convice myself is as follows: when $n\rightarrow\infty$ , the LHS of $(3)$ tends to zero, thus $(1)$ will hold. But $(2)$ will not converge to zero, because the sum of the fluctuations around the expected value sum up and increase, thus forming divergent series. Any help on obtaining a better intuition behind the differences between $(1)$ and $(2)$ is appreciated.","['statistics', 'law-of-large-numbers', 'probability-theory']"
4522020,Show that the function $f(x)=x+\sqrt{x}$ is one-to-one,"Show that the function $f(x)=x+\sqrt{x}$ is one-to-one. I know that for showing that a function is one-to-one I have to prove that if $f(a)=f(b)$ then $a=b$ . Then I'm trying that in here but I get stuck. $$f(a)=f(b)$$ $$a+\sqrt{a}=b+\sqrt{b}$$ $$a-b=\sqrt{b}-\sqrt{a}$$ How to do I show from here that $a=b$ ? I've tried square both sides, completing the square and haven't worked. :( I will appreciate a detail to understand, thanks in advance.",['functions']
4522048,"Let $r,s,t$ are roots of the cubic equation $x^3+bx^2+cx+d=f(x)$ then write down $D=((r-s)(r-t)(s-t))^2$ in terms of $b, c, d$","Let $r,s,t$ are roots of the cubic equation $f(x) = x^3+bx^2+cx+d$ then write down $D=((r-s)(r-t)(s-t))^2$ in terms of $b,c,d$ . Is there any clever way to solve it? We know $r+s+t=-b, rs+st+rt=c, rst=d.$ I also have the feeling that $D$ is actually defined as a discriminant of the cubic so there must be a discriminant of a matrix or some tricky way to prove that this is in particular ${\displaystyle b^{2}c^{2}-4c^{3}-4b^{3}d-27d^{2}+18bcd\,.}$ I am just not getting the answer without making my hands dirty i.e. expanding both sides.","['contest-math', 'cubics', 'roots-of-cubics', 'polynomials', 'algebra-precalculus']"
4522054,Proof regarding Angle Bisector [duplicate],"This question already has answers here : Given two lines in 2D, how to select the angle bisector related to the smallest angle between the lines? (5 answers) Closed 1 year ago . If $c_1, c_2$ are positive then the angle bisector of the acute angle between the lines $a_1x+b_1y+c_1=0$ and $a_2x+b_2y+c_2=0$ is $\frac{a_1x+b_1y+c_1}{\sqrt{a_1^2+b_1^2}}=\frac{a_2x+b_2y+c_2}{\sqrt{a_2^2+b_2^2}}$ , if $a_1a_2+b_1b_2\lt0$ $\frac{a_1x+b_1y+c_1}{\sqrt{a_1^2+b_1^2}}=-\frac{a_2x+b_2y+c_2}{\sqrt{a_2^2+b_2^2}}$ , if $a_1a_2+b_1b_2\gt0$ I wonder what the proof for this is. I understand the perpendicular distances from a point to the lines are being equated. My doubt is regarding the signs taken. If the angle between the lines is $\theta$ then the angle between the bisector and either of the lines is $\frac\theta2$ . Using the formula for angle between two lines, if $|\tan\frac\theta2|$ comes out to be less than $1$ then the acute angle is being bisected by the line in concern. Or if $|\tan\frac\theta2|\gt1$ then the obtuse angle. But how to find that direct relation with $a_1a_2, b_1b_2$ ? My Attempt: Let the slopes of the given lines be $m_1, m_2$ . Thus, $m_1=-\frac {a_1}{b_1}$ and $m_2=-\frac {a_2}{b_2}$ If $m_1m_2=-1$ then the lines are perpendicular. If $m_1m_2\lt-1$ then can we say the angle between them is acute? If yes, we get $-\frac{a_1}{b_1}\cdot-\frac{a_2}{b_2}\lt-1$ , thus, $a_1a_2+b_1b_2\lt0$ But from this, how do we conclude that the angle bisector would be obtained by taking a plus sign in the formula?","['coordinate-systems', 'angle', 'geometry']"
4522091,"What is the meaning of ""without referring back to the ambient space $R^3$ where the surface lies""?","I'm reading Do Carmo's Differential Geometry book, here: What is the meaning of ""without referring back to the ambient space $R^3$ where the surface lies""? Does it mean that we can compute it directly without appealing to some inverse mapping? If so, why don't we need to refer back? This concerns me because it seems that there are situations where we need to refer back to the ambient space and situations where we don't and up to now, It's not clear where we need it and where we don't, I just accepted the definitions and constructions without deeper thought because I still can't. This gives me a lot of curiosity, for example: In a definition I mentioned in a previous question: I noticed that this is defined referring back to a map between open sets in $R^2.$ Can we make this definition without this?",['differential-geometry']
4522118,Calculation of limit with sandwich theorem,I want to calculate the limit $$\lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1}$$ I have done the following : $$\left |\frac{n+\sin(n)}{n^2+n+1}\right |=\frac{|n+\sin(n)|}{|n^2+n+1|}=\frac{|n+\sin(n)|}{n^2+n+1}\leq \frac{|n|+|\sin(n)|}{n^2+n+1}=\frac{n+|\sin(n)|}{n^2+n+1}\leq \frac{n+n}{n^2+n+1}=\frac{2n}{n^2+n+1}\leq \frac{2n}{n^2}=\frac{2}{n}$$ So since $\left |\frac{n+\sin(n)}{n^2+n+1}\right |\leq \frac{2}{n}$ we get $$-\frac{2}{n}\leq \frac{n+\sin(n)}{n^2+n+1}\leq \frac{2}{n}$$ and since $\lim_{n\rightarrow \infty}\left (-\frac{2}{n}\right )=\lim_{n\rightarrow \infty}\left (\frac{2}{n}\right )=0$ and so we get $\lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1}=0$ . Is this correct? Or could we get an other easier inequality to use the sandwich theorem?,"['limits', 'calculus', 'analysis']"
4522223,What is the simplest function to generate this curve shape?,"I'm looking for the most straightforward function that generates this shape: The best I've found so far is: $\frac{1}{1+(\frac{x}{\alpha})^\beta}$ , where $\alpha$ is the middle point at which the function moves from 1 to 0 (0.25 in the illustration) and $\beta$ controls how steep is that transition (200 in the illustration). What I don't like about my current solution is that I need to set $\beta$ to a high arbitrary number to get a steep transition, which is what I need for my application. Any suggestions? Thanks! Edit : The function and its derivative need to be continuous.","['functions', 'soft-question']"
4522270,"How can the Lebesgue-Measure be an ergodic measure on [0, 1]?","In my class on ergodic theory there was a theorem, that all ergodic $F$ -invariant measures $\mu$ for a Borel-function $F: X \rightarrow X$ are extreme points of the set $M_F(X) =$ {all $F$ -invariant probability-Borel-measures $\mu$ on X}. Vice versa, all $\mu \in ex(M_F(X))$ are ergodic measures. Finally, the extreme points of probability-Borel-measures on X are all Dirac measures $\delta_x, x\in X$ .  (The extreme points of $M_F(X)$ should only be Dirac-measures as well, right?) My question is, how can it be that e.g. the Lebesgue-Measure is ergodic for $T_2: [0, 1]\rightarrow [0, 1]$ , where $T_2$ is the tent map defined by \begin{equation}
T_2(x) = 
\left\{
    \begin{array}{lr}
        2x, & \text{if }  x \in [0, 1/2]\\
        2-2x, & \text{if } x\in [1/2, 1]
    \end{array}
\right\},
\end{equation} since the Lebesgue measure is not a Dirac-Measure, but a $T_2$ -invariant probability-Borel-measure on $[0,1]$ .
I'm sure I'm missing something obvious but maybe someone can help me point it out.","['chaos-theory', 'measure-theory', 'lebesgue-measure', 'ergodic-theory', 'dynamical-systems']"
4522281,"Prove/disprove that $f : (0 , \infty) \rightarrow \mathbb R$ , $f(x) = x \sin x$ is onto","I need to find whether the function $f : (0 , \infty) \rightarrow \mathbb R$ , $f(x) = x \sin x$ is onto or not. Here is my approach: $f$ is an elementary function, and therefore continuous. It is continuous everywhere because it is defined everywhere in $\mathbb R$ . Now, I think the function has no limit. It diverges and is not bounded. I am not sure how to ""prove"" it mathematically and formally. But because it is unbounded (neither from above or below), then for every $y$ , there exists $x_1$ such that $f(x_1) < y$ . Similiarly, there exists $x_2$ such that $f(x_2) > y$ . Hence $f(x_1) < y < f(x_2)$ . Again, $f$ is continuous, then by the intermediate value theorem, for every such $y$ there must exist $x_0$ such that $f(x_0) = y$ . Therefore, the function $f$ is onto. But I think there is a flaw in my proof, because the intermediate value theorem requires a closed interval. How can I fix this? Is this approach valid and correct? If it is, I am struggling in writing in in formal, mathematical writing, specifically in proving it is not bounded (from both above and below), and that it has no limit (not even an infinite one)","['real-analysis', 'continuity', 'calculus', 'functions', 'limits']"
4522306,Prove that a nontrivial quadrangulation cannot have two adjacent vertices with degree 2 in one quadrilateral,"Given a planar quadrangulation $Q$ I am trying to show that there cannot exist two adjacent vertices in any quadrilateral of the quadrangulation that are both of degree 2 (we will label these vertices as $v_1$ and $v_2$ ). I am not taking into consideration just a simple $4$ -cycle, rather I am considering quadrangulations of 5 or more vertices. My idea is the following: assume that there exists a quadrangulation of 5 or more vertices that contains two adjacent vertices, both of degree 2. By definition, this quadrangulation must contain a $4$ -cycle as a subgraph. Then it seems like we cannot join this subgraph with only one vertex to create another subgraph of said quadrangulation, because this would have to contain one of these vertices of degree 2. Thus we must add at least two new vertices, both of which are now adjacent with the two vertices that are allowed to have more than 2 neighbors. However, now this is not a quadrangulation, because the outer face is incident with 6 distinct vertices. I am quite sure one would need some type of a recursive argument here. It would be nice to have a general proof for this, not necessarily considering several different cases.","['graph-theory', 'combinatorics', 'geometry', 'discrete-mathematics']"
4522313,Nature of stationary points of reciprocal functions,"I have solved a problem but I'm not sure if the proof is right. The problem is:
Show that if the curve $y=f(x)$ has a maximum stationary point at $x=a$ then the curve $y=\frac{1}{f(x)}$ has a minimum stationary point at $x=a$ where $f(a)\neq 0$ Here is what I've done: I know that $f'(a)=0$ and $f''(a)<0$ and $$\begin{equation}[\frac{1}{f(x)}]'' = -\frac{f(x)^2f''(x)-2f(x)f'(x)^2}{f(x)^4}\end{equation}$$ assuming $y=\frac{1}{f(x)}$ has a minimum stationary point at x=a then $$\begin{equation}-\frac{f(a)^2f''(a)-2f(a)f'(a)^2}{f(a)^4}>0\end{equation}$$ which leads to $$\begin{equation}f(a)^2f''(a)-2f(a)f'(a)^2<0\end{equation}$$ because $f'(a)=0$ we get $$\begin{equation}f(a)^2f''(a)<0\end{equation}$$ because $f(a)\neq 0$ we get $$\begin{equation}f''(a)<0\end{equation}$$ which is true and therefore $y=\frac{1}{f(x)}$ does have a minimum stationary point at $x=a$ . My question is why would this show that $y=\frac{1}{f(x)}$ does has a minimum stationary point at $x=a$ just because I assumed that it's true and deduced something true from it?","['calculus', 'solution-verification', 'derivatives']"
4522317,Question about multiple scales approximation for a system of ODEs,"I have been trying to apply the method of multiple scales to a certain set of partial differential equations, but I keep having problems.  I would appreciate if someone could point out my mistake, whether in my application of the method or in my interpretation of the results.  I apologize about the length of the post, but I wanted to include my full derivation. The relevant equations may be written as \begin{gather}
  \frac{\partial f}{\partial x} - i\epsilon a(x)g = 0, \\
  \frac{\partial g}{\partial x} - i\epsilon b(x)f = 0,
\end{gather} where $\epsilon\ll1$ and $a(x)$ and $b(x)$ are statistically homogeneous, positive and integrable functions (e.g., they could take the form $a(x)=1+\sin(kx)/4$ ).  If I assume that $f(x)=f_0(x_0,x_1,x_2)+\epsilon f_1(x_0,x_1,x_2) + \epsilon^2 f_2(x_0,x_1,x_2)$ and similarly for $g(x)$ , where $x_n=\epsilon^n x$ , and if I further assume that $f_n$ and $g_n$ are $O(1)$ , I get an series of sets of differential equations.  The $O(1)$ equations may be written as \begin{gather}
  \frac{\partial f_0}{\partial x_0} = \frac{\partial g_0}{\partial x_0} = 0,
\end{gather} and so we find that the largest-scale approximation of $f$ and $g$ does not depend on the smallest length scale, $x_0$ .  The $O(\epsilon)$ equations are \begin{gather}
  \frac{\partial f_1}{\partial x_0} + \frac{\partial f_0}{\partial x_1} - i a(x_0)g_0 = 0, \\
  \frac{\partial g_1}{\partial x_0} + \frac{\partial g_0}{\partial x_1} - i b(x_0)f_0 = 0.
\end{gather} Since $f_0$ and $g_0$ do not depend on $x_0$ , we may integrate, yielding \begin{gather}
  f_1 + x_0\frac{\partial f_0}{\partial x_1} - i g_0\int a(x')dx' + c_1 = 0, \\
  g_1 + x_0\frac{\partial g_0}{\partial x_1} - i f_0\int b(x')dx' + c_2 = 0,
\end{gather} where $c_1$ and $c_2$ are constants of integration.  These constants may be set equal to zero without loss of generality, as they may be incorporated into $f_0$ and $g_0$ .  We need to remove secular terms.  Clearly, the $x_0$ terms are secular, but the integrals over $a$ and $b$ are also secular.  If we decompose $a(x_0)$ and $b(x_0)$ such that $a(x_0)=\langle a\rangle + \Delta a(x_0)$ , where $\langle a\rangle$ is the average of $a(x_0)$ , and $b(x_0)=\langle b\rangle + \Delta b(x_0)$ , we may then write \begin{gather}
  f_1 + x_0\left[\frac{\partial f_0}{\partial x_1} - ig_0\langle a\rangle\right] - i g_0\int_0^{x_0} \Delta a(x_0)dx_0 = 0, \\
  g_1 + x_0\left[\frac{\partial g_0}{\partial x_1} - if_0\langle b\rangle\right] - i f_0\int_0^{x_0} \Delta b(x_0')dx_0' = 0.
\end{gather} Now the integrals are not secular, and we may set the terms in the square brackets equal to zero.  Thus, we may write \begin{gather}
  f_1 = i g_0\int_0^{x_0} \Delta a(x_0)dx_0, \\
  g_1 = i f_0\int_0^{x_0} \Delta b(x_0)dx_0, \\
  \frac{\partial f_0}{\partial x_1} - i\langle a\rangle g_0 = 0, \\
  \frac{\partial g_0}{\partial x_1} - i\langle b\rangle f_0 = 0.
\end{gather} Thus, I have equations for $f_0$ and $g_0$ that may be explicitly solved as a function of $x_1$ .  I can then write the solutions as \begin{gather}
  f_0(x_1,x_2) = A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} + B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}, \\
  g_0(x_1,x_2) = \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} - \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}.
\end{gather} To this point I do not think there is an issue, but I would like to extend the results to higher orders.  The $O(\epsilon^2)$ equations are given by \begin{gather}
  \frac{\partial f_2}{\partial x_0} + \frac{\partial f_1}{\partial x_1} + \frac{\partial f_0}{\partial x_2} - i a(x_0)g_1 = 0, \\
  \frac{\partial g_2}{\partial x_0} + \frac{\partial g_1}{\partial x_1} + \frac{\partial g_0}{\partial x_2} - i b(x_0)f_1 = 0.
\end{gather} Substituting in the solutions to $f_1$ and $g_1$ and their $x_1$ gradients then yields \begin{gather}
  \frac{\partial f_2}{\partial x_0} + \frac{\partial f_0}{\partial x_2} + \left[ a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0' - \langle b\rangle \int_0^{x_0} \Delta a(x_0')dx_0' \right] f_0 = 0, \\
  \frac{\partial g_2}{\partial x_0} + \frac{\partial g_0}{\partial x_2} + \left[ b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0' - \langle a\rangle \int_0^{x_0} \Delta b(x_0')dx_0' \right] g_0 = 0.
\end{gather} We may integrate with respect to $x_0$ again, and eliminating the secular terms leads to the conclusion \begin{gather}
  \frac{\partial f_0}{\partial x_2} + \left[ \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle \right] f_0 = 0, \\
  \frac{\partial g_0}{\partial x_2} + \left[ \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle \right] g_0 = 0.
\end{gather} These equations may also be solved explicitly.  If we let \begin{gather}
  \alpha \equiv \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle, \\
  \beta \equiv \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle,
\end{gather} then the solutions are given by \begin{gather}
  f_0(x_1,x_2) = f_{01}(x_1)e^{-\alpha x_2}, \\
  g_0(x_1,x_2) = g_{01}(x_1)e^{-\beta x_2}.
\end{gather} Combining this result with the $O(\epsilon)$ result leads to the conclusions \begin{gather}
  A(x_2) = B(x_2) = C_0e^{-\alpha x_2}, \\
  A(x_2) = B(x_2) = D_0e^{-\beta x_2}.
\end{gather} However, both of these statements can only be true if $\alpha=\beta$ , which is not always the case, or if $C_0=D_0=0$ , which is the trivial solution.  I know that a non-trivial solution can exist for the original equations, even when $\alpha\ne\beta$ . Did I make a mistake somewhere?  Is there a restriction on the method of multiple scales that makes may analysis invalid?  Any assistance would be greatly appreciated.","['systems-of-equations', 'ordinary-differential-equations', 'perturbation-theory']"
4522332,Show inequality for positive real numbers,"If $x,y$ are positive real numbers then we have that $$ \frac{1}{\sqrt{x+y}}<\frac{1}{\sqrt{x}}+\frac{1}{\sqrt{y}}$$ right? But how can we show that? I have tried the following but I don't think that this is the way we should go. \begin{align*}\frac{1}{\sqrt{x + y}} < \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{y}}=\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}} & \iff \left(\frac{1}{\sqrt{x + y}}\right)^2 < \left(\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}}\right)^2 \\ & \iff \frac{1}{x + y} <\frac{x+y+2\sqrt{xy}}{xy} \\ & \iff  xy<(x+y)(x+y+2\sqrt{xy}) \\ & \iff xy<x^2+xy+2x\sqrt{xy}+xy+y^2+2y\sqrt{xy}\\ & \iff 0<x^2+2(x+y)\sqrt{xy}+xy+y^2\end{align*}","['calculus', 'analysis', 'inequality']"
4522344,Can't understand why cosh(1/x) changes the result for this limit,"I have this limit: $$\lim_{x \to -\infty} x^2\left({(x^2+1)\cosh{\frac 1x}\over x^2}-1\right)$$ If I solve this limit by hand, I get 1 $\require{cancel}$ $$\lim_{x \to -\infty} x^2\left({(x^2+1)\cosh{\cancelto{0}{\frac 1x}}\over x^2}-1\right)=$$ $$\lim_{x \to -\infty} x^2\left({(x^2+1)\cancelto{1}{\cosh{0}}\over x^2}-1\right)=$$ $$\lim_{x \to -\infty} \cancel{x^2}\left({\cancel{x^2}+1\cancel{-x^2}\over \cancel{x^2}}\right)=1$$ However, both Wolfram Alpha and the exercise say the correct result is ${\frac 32}$ $$\lim_{x \to -\infty} x^2\left({(x^2+1)\cosh{\frac 1x}\over x^2}-1\right) = \frac 32$$ However, if I remove $\cosh{\frac 1x}$ , which for ${x \to -\infty}$ tends to $1$ , suddenly Wolfram Alpha says the result is 1: $$\lim_{x \to -\infty} x^2\left({(x^2+1)\over x^2}-1\right) = 1$$ I'm at a complete loss. Wolfram Alpha uses L'Hopital's rule to solve the first limit, so it doesn't help me undersand the discrepancy","['limits', 'calculus', 'limits-without-lhopital', 'hyperbolic-functions']"
4522358,Weak convergence of sum implies convergence of summands,"Suppose $X_n$ and $Y_n$ are independent, $X_n \overset d \to X$ and $X_n + Y_n \overset d \to Z$ for some random variable $Z$ . Is it necessary, that $Y_n$ converges in distribution? I tired using characteristic functions and Levy-Cramer theorem, but I'm missing one thing. Here is what I have: $$
\forall t \in \mathbb R \quad
\varphi_{X_n}(t) \to \varphi_X(t), \quad \varphi_{X_n}(t)\varphi_{Y_n}(t) \to \varphi_Z(t)
$$ Since $\varphi_{Y_n}$ is bounded, it must converge for all $t$ such that $\varphi_{X}(t) \neq 0$ . For those $t$ we would have $$
\varphi_{Y_n}(t) \to \frac {\varphi_Z(t)}{\varphi_{X}(t)}
$$ Which is continuous at $0$ and equal to $1$ there. If $\varphi_{Y_n}$ converged in every point, then $Y_n$ would have to converge in distribution. Can I show that $\varphi_{Y_n}$ converges everywhere? Or in other way show that $Y_n$ converges in distribution? Counterexamples are welcome too.","['characteristic-functions', 'weak-convergence', 'independence', 'convergence-divergence', 'probability-theory']"
4522360,"Why are we allowed to impose a constraint when using the method of variation of parameters to solve second order, linear ODEs","We never actually covered the method of variation of parameters in class as it was marked as optional so the lecturer said it's our choice whether we cover it or not. I wanted to cover it and have no problems using this method for first order ODEs but upon reading the example in the notes that shows using variation of parameters to solve second order ODEs I see something that I don't understand. The example provided is: Find the general solution of $$y''+y'-2y=e^x$$ Finding the homogeneous solution is self explanatory giving: $$y=C_1(x)y_1(x)+C_2(x)y_2(x)=C_1e^{-2x} + C_2e^{x}$$ $$y'=C^{'}_1e^{-2x}-2C_1e^{-2x}+C^{'}_2e^x+C_2e^x$$ Then the example says ""...we impose on $C_1(x)$ and $C_2(x)$ a constraint:"" $$C^{'}_1e^{-2x} + C^{'}_2e^{x}=0$$ I cannot for the life of me understand why we are allowed to do this and where it comes from. I asked my lecturer and he said: With two unknown constants, we can impose two constraints on them to find a unique solution. One constraint follows from the ODE itself, leaving us the freedom to select the other constraint a convenient. And the choice of Lagrange is convenient indeed! I replied: Forgive me if it is obvious but I am still unsure as to why we know that the constraint is equal to zero – I assume it somehow follows from the ODE like you said but I cannot see how. Then finally he said: We do not KNOW that — we DECIDE to impose such a constraint because it helps to solve the problem. The other constraint follows from the equation — no independent decisions there. Unfortunately this does not make anything clearer so if anyone could help explain it to me I would be very grateful!",['ordinary-differential-equations']
4522368,Are there interesting combinatorial proofs which use more sophisticated grouping than sign-reversing involutions?,"There are many combinatorial proofs which establish interesting identities by designing suitable ""sign-reversing involutions"" on a set of relevant signed objects. For example, Benjamin and Quinn showcased many such arguments for simplifying alternating sums with their Description-Involution-Exception (DIE) method and Zeilberger highlighted many such arguments for proving identities involving the determinant . Are there examples of interesting combinatorial identities which involve more sophisticated grouping? In particular, are there identities which simplify sums of the form $$\sum_{s\in S}\omega^{f(s)}g(s),$$ where $S$ is some group of interesting combinatorial objects, $g$ is some weight function, $f$ is some sort of sign function, and $\omega$ is a primitive $p^{\text{th}}$ root of unity? This is a bit of a vague, ill-posed question since the space of combinatorial proofs is vast, and what counts as ""interesting"" is unclear in this context is unclear. But I'm curious to see if there are examples of identities people know of which are proved combinatorially by designing some sort of order $p$ map which shifts weights by $\omega$ (similar to how the linked papers have proofs which work by designing an order $2$ map which shifts weights by $-1$ ).","['combinatorial-proofs', 'matrices', 'combinatorics', 'algebraic-combinatorics', 'discrete-mathematics']"
4522399,"Does a discriminant condition on $f(x,y)$ imply that $f$ is weighted homogeneous?","[I cross-posted this question as https://mathoverflow.net/questions/431454 . (That version is also slightly updated.)] Let $f = \sum_{i=m}^n f_iy^i \in \mathbb{C}[x,y]$ be a polynomial (where $f_i \in \mathbb{C}[x]$ with $f_m,f_n \ne 0$ and $n > m$ ), and let $\operatorname{Disc}_y(x) \in \mathbb{C}[x]$ be the discriminant of $f$ with respect to $y$ (i.e., for every $a \in \mathbb{C}$ , $\operatorname{Disc}_y(a)$ is the discriminant of $f(a,y)$ ). I got the impression that $f$ is weighted homogeneous (with possibly negative weights) if and only if the following three conditions hold: $f_m$ is a (non-zero) monomial in $x$ $f_n$ is a (non-zero) monomial in $x$ $\operatorname{Disc}_y$ is a non-zero monomial in $x$ The direction "" $\Rightarrow$ "" should not be too hard to prove: 1 and 2 are clear, and I think that 3 follows from the formula expressing the discriminant in terms of the coefficients of a polynomial. My question is: Is "" $\Leftarrow$ "" also true? This seems more surprising, given that it yields strong conditions on $f$ ; since the question seems rather elementary, I am hoping that this might be known. Some thoughts/remarks (which didn't yield a solution yet): If one thinks of $f$ as defining a variety $V \subset \mathbb{C}^\times \times \mathbb{P}^1\mathbb{C}$ , then 1 and 2 say that $V \subset \mathbb{C}^\times \times \mathbb{C}^\times$ , and 3 says that the projection to the first coordinate is unramified. Consider $g := y \cdot \partial f/\partial y$ . It is not hard to verify that 1, 2 and 3 together are equivalent to: the subvarieties of $\mathbb{C}^\times \times \mathbb{P}^1\mathbb{C}$ defined by $f$ and $g$ have empty intersection. Applying a version of Bezout's Theorem for subvarieties of $\mathbb{P}^1\mathbb{C} \times \mathbb{P}^1\mathbb{C}$ thus yields a lot of intersections above $0$ and $\infty$ . If $n$ (the degree in $y$ ) is 2, then one can easily prove the claim using the formula for the discriminant. I was also able to prove the claim when $f$ consists of only three monomials.","['algebraic-geometry', 'discriminant', 'toric-varieties']"
4522421,Prove that the group generated by sum of two subgroups is isomorphic with their cartesian product,"While studying abstract algebra I encountered the following question: Given an abelian group $G$ and its two sub-groups $H_1, H_2$ , such that $H_1 \cap H_2=\{e\}$ , prove that the group generated by $\langle H_1 \cup H_2 \rangle$ is isomorphic with the group $H_1 \times H_2$ (their Cartesian product, with the operation being the one from $G$ along the coordinates, so $(a,b)(c,d)=(ac, bd)$ ). From what I understand I should suggest a bijection that preserves the operations of these groups. However I don't quite know how to go about this. I was thinking of the following: for each element $a$ of $\langle H_1 \cup H_2 \rangle$ we first express is at a linear combination of elements from $H_1 \cup H_2$ (so $a = (H_1)_1^{k_1}(H_1)_2^{k_2}...(H_2)_1^{q_1}(H_2)_2^{q_2}...$ ), separate the elements from $H_1$ and $H_2$ into two expressions ( $R_1 = (H_1)_1^{k_1}(H_1)_2^{k_2}..., R_2=(H_2)_1^{q_1}(H_2)_2^{q_2}...$ ), and place the results of these expressions into their respective coordinates of the pair from the Cartesian product group (so $(R_1, R_2)$ ). Since $G$ is abelian, we don't have to worry about the order of the operations. Does such proposition hold any water?","['direct-product', 'group-isomorphism', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4522434,"If $f(x)=\sqrt x-\sqrt {4-x}+2$, what is the domain of $\sqrt{f(x)-f^{-1}(x)}$?","If $f(x)=\sqrt x-\sqrt {4-x}+2$ , what is the domain of $\sqrt{f(x)-f^{-1}(x)}$ ? We have $f(x)\ge f^{-1}(x)$ . Since the graph of $f^{-1}(x)$ and $f(x)$ are symmetric along the line $y=x$ , intuitively I realized that in order to $f(x)\ge f^{-1}(x)$ we should have $f(x)\ge x$ , $$\sqrt x-\sqrt{4-x}+2\ge x \Rightarrow \sqrt x-\sqrt{4-x} \ge x-2$$ Here I'm not sure how to continue. Squaring doesn't help a lot (since it is valid only for $x\ge2$ and after simplification and squaring again one would get a fourth degree polynomial). I noticed both functions $g(x)=\sqrt x-\sqrt {4-x}$ and $h(x)=x-2$ are strictly increasing but I don't know how to use this fact.","['algebra-precalculus', 'functions', 'inverse-function', 'inequality']"
4522440,"How could we prove ""there exists"" a linear transform that converts convolution to multiplication?","Let's pretend the Fourier transform was never invented. Well, how could we prove there exists a linear transform $L$ over complex numbers such that, for two convolved integrable functions $f*g$ , that $L[(f*g)(t)] = k \cdot L(f)L(g)$ for some normalization constant $k$ ? I don't know much about abstract algebra, but this almost looks like the definition I see of a homomorphism. Are there circumstances under which we can ignore $k$ ?","['abstract-algebra', 'convolution']"
4522462,(no) Continuous Injection from Torus $S^1 \times S^1$ into $S^2$,"I'm trying to show there is no injection $f$ between the 2-torus $ S^1 \times S^1$ ( Henceforth "" The Torus"") and the 2-sphere $S^2$ My ideas so far: The Torus is compact, the 2-sphere is Hausdorff. Then $f: T^2 \rightarrow f(T^2) \subset S^2 $ is a homeomorphism, implying the Torus can be embedded in the 2-Sphere. not clear that's enough.(It's been a while since I saw this material. Hope I'm not too far off) This question : No continuous injective function from 2-sphere to torus is similar but goes in the opposite direction As a map $f: T^2 \rightarrow S^2, f$ is an element of $\pi_2(T^2)$ , which by Kunneth is trivial: ( Edit: As pointed out, this is not the case; rather the other way round; it's the group of homotopy classes of maps from S^n into X ) $\pi_2 (T^2)=\pi_2(S^1 \times S^1)= \pi_2(S^1)\times \pi_2(S^1)=0$ (*) So that every map in $\pi_2(T^2)$ is homotopically trivial. But I can't tell if any such injective map violates this triviality.
Thanks for your suggestions. Can this be done without Invariance of Domain? I vaguely remember it, but I never got a good feel for it. *Up to some type of isomorphism.","['general-topology', 'homotopy-theory', 'algebraic-topology']"
4522464,Show that an empirical mean converges to zero,"Let $(X_i)_{i\geq 1}$ and $(Y_i)_{i\geq 1}$ be sequences of two real-valued random variables, where the support of $X_i$ is bounded. I assume that $X_1, \dots, X_n$ are independent conditionally on $\mathcal{I}_n = \{Y_1, \dots, Y_n\}$ . I want to show that $$\displaystyle S_n = \dfrac{1}{n}\sum_{i = 1}^n \left(X_i - \mathbb{E}(X_i|\mathcal{I}_n)\right)$$ converges in probability to $0$ . Is the following proof correct? We have $\mathbb{E}(S_n) = 0$ and $\mathbb{V}(S_n) = \mathbb{E}_{\mathcal{I}_n} \left(\dfrac{1}{n^2}\sum_{i = 1}^n\sigma^2_{i}\right)$ , where $\sigma^2_{i} = \mathbb{V}(X_i|\mathcal{I}_n)$ . As $X_i$ have bounded supports, $\sigma^2_{i}$ is bounded and $\mathbb{V}(S_n) \to 0$ as $n$ grows to infinity. Using Chebyshev's inequality, I come to the result that $$\mathbb{P}(|S_n| > \varepsilon) \leq \dfrac{\mathbb{V}(S_n)}{\varepsilon^2},$$ for any $\varepsilon > 0$ .","['probability-limit-theorems', 'probability-theory']"
4522493,Distance of the product of two nocommuting positive operators from normal operators,"This question is inspired by problem . Given two noncommuting positive operators $A,B\in B(\mathcal{H}).$ Let $\mathcal{N}$ denote the subspace of all normal operators, i.e. the operators $N$ such that $NN^*=N^*N.$ Determine $$d(AB,\mathcal{N}):=\inf\{\|AB-N\|\,:\,N\in \mathcal{N}\}$$ My contribution:
In one of the answers to problem it was shown
that $$\inf\{\|AB-N\|\,:\,N=N^*,\ N\in B(\mathcal{H})\}={1\over 2}\|AB-BA\|$$ hence $$d(AB,\mathcal{N})\le {1\over 2}\|AB-BA\|$$ Moreover $AB$ is not normal. Indeed, since $AB\neq BA,$ the operator $AB$ is not self-adjoint.
Furthermore
we have $$\{0\}\cup \sigma(AB)=\{0\}\cup \sigma(A^{1/2}BA^{1/2})\subset [0,\infty)$$ Therefore $$\sigma(AB)\subset [0,\infty)$$ If $AB$ was normal then $AB$ would be self-adjoint by this , a contradiction. Once we know that $AB$ is not normal, the number $d(AB,\mathcal{N})$ is strictly positive as the subspace $\mathcal{N}$ is closed in $B(\mathcal{H}).$ I wonder if $$d(AB,\mathcal{N})={1\over 2}\|AB-BA\|$$ or there can be the strict inequality.","['linear-algebra', 'functional-analysis']"
4522501,"Geometric interpretation of the Lie bracket: Sign of the ""gap""?","The short of it: Concerning Lie brackets of vector fields: Who's right with the sign of the ""gap vector""? Does $[V=d/d\mu, W=d/d\lambda]$ point from $A$ to $B$ or from $B$ to $A$ ? Adapted from Schutz's ""Geometrical methods in mathematical physics"": But in other sources the sign differs, e.g. Hehl 2007 : Who is right? The long of it: I'm reading Bernard Schutz's ""Geometrical methods in mathematical physics"", which -- being a physicist myself -- strikes a great balance for me between explanatory detail and rigor. I'd like to prefix my question with some things that I think I understand, so that it's easier for others to point to where I might have erred. I added two figures to illustrate what I'm trying to get to. I cannot find an answer to my specific question, though I have seen similar arguments as the following posted. I can understand how vector fields can be seen as operators acting on functions. The coordinate representation of the Lie bracket $[V, W] = \left(V^j \frac{W^i}{\partial x_j} - W^j \frac{V^i}{\partial x_j} \right)\partial_i$ is clear to me and using the product rule, I can derive the ""product rule"" $[fX, gY] = fg[X, Y] + fX(Y) - gY(f)X$ . Schutz describes the geometric interpretation of the Lie bracket by using the exponentiation of vector fields that describe finite motions along integral curves: Let $V = d / d\lambda$ and $W = d / d \mu$ be vector fields. Starting at a point $P$ , moving $\Delta \lambda = \epsilon$ along the V curve through $P$ to a point $R$ , and then moving $\Delta \mu = \epsilon$ along a W curve, ending at a point $A$ . Then start again at $P$ , go first $\Delta \mu$ along $W$ and then $\Delta \lambda$ along $V$ ending at a point $B \neq A$ . The vector from $A$ to be $B$ is $\epsilon^2[V, W]$ , to lowest order in $\varepsilon$ , since $x^i(R) = \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P \\
x^i(A) = \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P$ and analogously $x^i(B) = \exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right)x^i|_P$ . Using the Taylor series of the exponentials yields $x^i(B) - x^i(A) = \left[\exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right), \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)\right]x^i|_P \\
 = \epsilon^2\left[V, W\right]|_P$ . This makes sense to me, the exponentials act from the left on $x^i$ so the rightmost exponential acts first.  You from $P$ to $A$ via $WV$ and you get from $P$ to $B$ via $VW$ . So all of this should be easy to verify with an example, right? Lets take polar coordinates $(r, \phi)$ in 2d and three vector fields: $V := \cos(\phi) \partial_x + \sin(\phi) \partial_y = \frac{x}{r} \partial_x + \frac{y}{r} = \partial_r\\
W := -r\sin(\phi) \partial_x + r \cos(\phi) \partial_y = -y \partial_x + x \partial_y = \partial_{\phi}\\
Z := -\sin(\phi) \partial_x +  \cos(\phi) \partial_y = \frac{1}{r} \partial_{\phi}$ The vector fields $V$ and $W$ are the usual ones for polar coordinates and form a coordinate basis, since they commute with each other: $[V, W] = 0$ (just plug in the definition of the Lie bracket and use the product rule for the Lie bracket together with commuting partial derivatives $[\partial_x, \partial_y] = 0$ . Ok. Using the product rule for Lie brackets yet again yields straightforwardly: $[V, Z] = [V, \frac{1}{r} W] = \frac{1}{r} [V, W] + \frac{\partial r^{-1}}{\partial_r} \partial_\phi = -\frac{1}{r^2} \partial_\phi= -\frac{1}{r} Z \neq 0$ I get the same result by separately computing $V(Z) = ... = 0$ and $Z(V) = \frac{1}{r^2}\partial_\phi$ and again $[V, Z] = -\frac{1}{r^2} \partial_\phi$ . So the vector field $[V, Z]$ points opposite to $\partial_\phi$ . Finally my question: How does this connect to $[V, Z]$ pointing from $A$ to $B$ ? Following the above reasoning of following the flows, I get to $A$ by following first $V$ and then $Z$ , so as an operator it's $ZV$ ; and I get to $B$ by following first $Z$ and then $V$ , so as an operator it's $VZ$ . Sketching this for small $\epsilon$ (or large radii so that the circle segments look almost like straight lines), starting at $P = (x=1, y=0)$ results in $B$ being almost directly above $A$ , so $\vec B - \vec A = \epsilon^2 [V, Z] + O(\epsilon^4)$ points almost completely in positive $y$ direction. But from my calculations I see that $[V, Z] = - \frac{1}{r^2}\partial_\phi$ so evaluated at $P = (1, 0)$ this yields $-\partial_y$ . Exactly opposite of what I expected! I can follow the derivation of why the Lie bracket points from $A$ to $B$ , but the example yields the opposite! Where am I going wrong? Sorry for the rather long post, but I think my question may be unclear without context. Thanks for your help!","['vector-fields', 'visualization', 'lie-derivative', 'differential-geometry']"
4522519,How do you integrate a curl operators?,"I have this proof that I need to do and I don't know how to execute a certain step.
The part that I don't get is this; $$-\nabla \times E = \nabla \times (w \times B)$$ The notes just say, ""Integrating we get""; $$E = (-w \times B) + (-\nabla \psi)$$ Where $-\nabla \psi$ is the constant of integration. How do we integrate a curl operator? and what does that even mean?","['integration', 'curl', 'vector-analysis']"
4522532,Characteristic function of a random variable with symmetric distribution is real,"Show characteristic function of a distribution is real iff the a probability distribution is symmetric. I am trying to show the first direction, in which a symmetric distribution implies a real characteristic function. We have that $$\varphi(t)=\int_{-\infty}^\infty e^{itx}d\mu(x) = \int_{-\infty}^\infty \cos(tx)  d\mu(x) + i\int_{-\infty}^\infty \sin(tx)  d\mu(x)$$ I need to show that $$\int_{-\infty}^\infty \sin(tx)  d\mu(x) = 0$$ This would be trivial under the Lebesgue measure, but I need to do it for an arbitrary symmetirc measure over the line. I think one may be able to do this using a convergence theorem and a sequence of simple functions and the dominated convergence theorem, but is there a better way to do this?
Is there a way to show this without resorting to a discretization of the $\sin$ function?","['measure-theory', 'probability-distributions', 'analysis', 'probability-theory', 'probability']"
4522551,How to prove $\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How to prove this identity? $$\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0$$ This is George Casella statistical inference textbook exercise 1.27 (c). I have no idea to prove.","['statistics', 'binomial-coefficients', 'combinatorics', 'summation']"
4522642,Limit of $(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2}$,"Find $a,b$ of: $$\lim_{x \to \infty}(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2}$$ I can't use L'hopital, I tried multiplying by the conjugate, and solving it, $$\lim_{x \to \infty}(\sqrt{4x^2+2x+1}-(ax+b))\cdot \frac{\sqrt{4x^2+2x+1}+(ax+b)}{\sqrt{4x^2+2x+1}+(ax+b)}$$ $$=\lim_{x \to \infty}\frac{4x^2+2x+1-(ax+b)^2}{\sqrt{4x^2+2x+1}+(ax+b)} = \lim_{x \to \infty}\frac{x^2\left(4+\frac{2}{x}+\frac{1}{x^2}-a^2-\frac{2ab}{x}-\frac{b^2}{x^2}\right)}{x^2\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)}$$ Applying limit on the numerator and denominator $$\frac{\lim_{x \to \infty}\left(4-a^2+\frac{2-2ab}{x}+\frac{1-b^2}{x^2}\right)}{\lim_{x \to \infty}\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)}$$ It can be seen that the denominator tends to $0$","['limits', 'calculus', 'limits-without-lhopital']"
4522660,Double integral $\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$,"I'm trying to evaluate this double integral: $$I=\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$$ Here is the closed-form: $$\frac{91π^4}{11520}+\frac{21}{32} ζ(3)-\frac{7}{8} ζ(3)\log{2}-\text{Li}_4 (\frac{1}{2})+\frac{π^2}{24}\log^2{2}-\frac{π^2}{16}\log{2}-\frac{1}{24}\log^4{2}$$ My try: because of its symmetry we can rewrite it as: $$2I=\int_{0}^{1}\int_{0}^{1}\frac{(x^2y^3+x^3y^2)\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$$ Then use the substitution: $x\to \frac{1}{x}$ and $y\to \frac{1}{y}$ , now it becomes: $$I=\int_{1}^{\infty}\int_{1}^{\infty}\frac{\log{(x)}\log{(y)}}{x(1-x^2)(1-y^2)(x^2y^2-1)}dxdy$$ Seem I could get rid of the power of $x$ and $y$ from numerator, but after splitting: $$I=\int_{1}^{\infty}\frac{\log{(x)}}{x(1-x^2)}dx\int_{1}^{\infty}\frac{\log{(y)}}{(1-y^2)(x^2y^2-1)}dy$$ The latter integral gives (Mathematica): $$\fbox{$-\frac{-2 x \text{Li}_2\left(x^2\right)+8 x \text{Li}_2(x)+2 x \log ^2(-x)-2 x \log ^2(x)+\pi ^2}{8 \left(x^2-1\right)}\text{ if }\Im(x)\neq 0\lor -1<\Re\left(\frac{1}{x}\right)<1$}$$ Now, I don't know how to process further. Maybe I misdirected at the beginning. Can you guys help me with this? Thank you very much.",['multivariable-calculus']
4522743,Use of Fatou's lemma to prove almost everywhere convergence,"I was trying to solve point 4. of this question based on an excercise taken from Probability Theory by A. Klenke (3rd version). Here is the link for completeness, but you can continue without reading as I am reporting here the main features below: Proof of Cramér-Lundberg inequality I am not sure about my use of Fatou's lemma to show that $\lim_{n\to\infty} f_n=0$ a.e. Let $f_n$ be a positive function (like $Z^\theta_n$ of the exercise in the link). We know that $\lim_{n\to\infty} E[\sqrt{f_n}]=0$ (also in the exercise). Moreover, $f_n$ is a martingale (as suggested by @jakobdt) Using the Fatou's lemma and the hypothesis about the expected value in the last equality I get: $0\leq\int \liminf_{n\to\infty} \sqrt{f_n} d\mu\leq\liminf_{n\to\infty}\int \sqrt{f_n}d\mu=0$ . So from $\int \liminf_{n\to\infty} \sqrt{f_n} d\mu = 0$ follows that $\liminf_{n\to\infty}\sqrt{f_n}=0 $ and that $\liminf_{n\to\infty}f_n=0$ a.e. Up to this point, is it right? How can I continue to show $\lim_{n\to\infty} f_n=0$ a.e.? Thank you.","['convergence-divergence', 'probability-theory', 'real-analysis']"
4522772,"If $b^2=p^2+q^2$ and $2b^2=r^2+s^2$ (all values positive integers), then is it necessary that $r=p+q$ and $s=p-q$ (or vice versa)?","Here's the question: For positive integers $b$ , $p$ , $q$ , $r$ , $s$ , if $$b^2=p^2+q^2 \quad\text{and}\quad 2b^2=r^2+s^2$$ then is it necessary that $r=p+q$ and $s=p-q$ (or vice versa)? I can easily show that it these are sufficient conditions, but can't figure out whether they are necessary.",['number-theory']
4522810,Integral of borel function wrt to measure,"Let $(\Omega,\mathscr{F},v)$ be some measure space. Consider the non-negative simple function $l:\Omega \rightarrow R$ given by: $$l(\omega)=\sum _{i=1}^k a_i I_{A_{i}}(\omega)$$ Where $a_i\in R_+, A_i\in\mathscr{F}$ , and $I_{A_{i}}$ is the indicator function for $A_i$ Definition 1 : Integral of a non-negative simple function $l$ wrt to a measure $v$ is given by: $$\int l~dv = \sum_{i=1}^k a_i v(A_i)$$ Definition 2 : Integral of some non-negative Borel measurable function $f:\Omega\rightarrow R$ with respect to measure $v$ is given by: $$\int f~dv= \sup\left\{\int l~dv: l \in S_f \right\}$$ Where $S_f$ is a collection of non-negative simple functions satisfying $l(\omega) \leq f(\omega)$ $\forall\omega\in\Omega$ . Consider some non-negative simple function $l'$ . I am aware all simple functions as defined above are Borel measurable. So how do I reconcile definition 1 and 2? That is, how do I prove that: $$\sup\left\{\int l~dv: l \in S_{l'} \right\} = \sum_{i=1}^{k'} a_i' v(A_i')$$ Where $l'(\omega)=\sum _{i=1}^{k'} {a_i'} I_{A_{i}'}(\omega), a_i'\in R_+, A_i'\in\mathscr{F}$ and $S_{l'}$ is the collection of non-negative simple functions satisfying $l(\omega)\leq l'(\omega)$ $\forall \omega\in \Omega$","['measure-theory', 'statistics']"
4522847,Inverse Trig Identity Proof under different conditions,"I am trying to understand how the inverse trigonometric identities below work/are derived: $$\tan^{-1}(x)+\tan^{-1}(y)=\tan^{-1}\left(\frac{x+y}{1-xy}\right)\\ xy<1$$ The above identity has a proof that depends on the identity for $\tan(x)+\tan(y)$ , which I have seen in my textbook, and I understand. $$\tan^{-1}(x)+\tan^{-1}(y)=\pi + \tan^{-1}\left(\frac{x+y}{1-xy}\right)\\xy>1$$ This one is mentioned in the book, but not proved. I can understand the logic behind it, since if the angles related to $x$ and $y$ are such that the angle related to $x+y$ will be in the second quadrant, then it is not possible for the $\tan^{-1}$ to ever give a correct output For $x>0,y>0$ if $xy>1$ , then $1-xy<0\Rightarrow\frac{x+y}{1-xy}<0$ , and that will give an answer in the fourth quadrant, whereas we likely want one in the second quadrant. However, this is just intuition. Some questions I have: where does the $xy>1$ come in? How is it established/proved? where does the $\pi$ factor come in? is it possible to see a complete proof for both cases of the property.","['trigonometry', 'proof-writing', 'inverse', 'inverse-function']"
4522865,Function satisfying $f(z) = 1 + z f\left(\frac{z}{1+z}\right)$,"Iterate the following equation to obtain an explicit formula for $f( z)$ : $$
\begin{align*}
f( z) = 1 + z f\left( \frac{z}{1 + z}\right)
.\end{align*}
$$ Iterating this equation one obtains $$
\begin{align*}
f( z) = \sum_{n \geq 0}^{} \frac{z^{n}}{\prod_{k = 0}^{n - 1} ( 1 + kz)}
.\end{align*}
$$ I wonder know whether this is already ""explicit"" enough or whether it can be simplified
even further.","['summation', 'recurrence-relations', 'calculus', 'combinatorics', 'discrete-mathematics']"
4522893,"Palka, An Introduction to Complex Analysis pp. 97-98, drawing a connection between differentiability in the real sense and complex differentiability","The book in question is An Introduction to Complex Function Thoery by Bruce Palka. Palka defines differentiability in the real sense as that a continuous function $f:A\to \mathbb{C}$ is differentiable at $z = x + iy$ in the real sense at $z_0 \in A$ if there exists complex numbers $c, d \in \mathbb{C}$ such that $f(z) = f(z_0) + c(z - z_0) + d(\overline{z} - \overline{z_0}) + E(z)$ where $E(z) \to 0$ as $z \to z_0$ . Palka's definition of complex differentiability is that (assume the same function $f$ ) $f$ has a complex derivative at $z_0 \in A$ if the limit $\lim_{z\to z_0}\frac{f(z) - f(z_0)}{z - z_0}$ exists. Palka discusses the general philosophy between these two notions in pages 97-98 by noting that the derivative of $f$ is in the real sense the matrix $\begin{bmatrix}\alpha & \beta\\ \gamma & \delta\end{bmatrix}$ with $\alpha = u_x, \beta = u_y, \gamma = v_x, \delta = v_y$ when $f = u + iv$ giving the $f$ the linear approximation form  at $z_0$ as $$f(z) = f(z_0) + \begin{bmatrix}\alpha & \beta\\\ \gamma & \delta\end{bmatrix}\begin{bmatrix}x - x_0\\\ y - y_0\end{bmatrix} + E(z)$$ when a complex number $z$ is viewed as a two dimensional vector. Then Palka states (without any computation) that the $c$ and $d$ w.r.t. the elements of the said matrix are $c = \frac{1}{2}\left(\alpha + \delta + i\left(\gamma - \beta\right)\right)$ and $d = \frac{1}{2}\left(\alpha - \delta + i\left(\gamma + \beta\right)\right)$ when $x = \frac{z + \overline{z}}{2} , y = \frac{z - \overline{z}}{2i}$ . Unfortunately I cannot seem to be able to derive these same equalities. Namely after applying the matrix to the vector $\begin{bmatrix}x - x_0\\\ y - y_0\end{bmatrix}$ and substituting the form of $x, x_0, y, y_0$ , we get $$\frac{1}{2}\begin{bmatrix}(z - z_0)(\alpha - i\beta) + (\overline{z} - \overline{z}_0)(\alpha + i\beta)\\\ (z - z_0)(\gamma - i\delta) + (\overline{z} - \overline{z}_0)(\gamma + i\delta)\end{bmatrix}$$ and I can't see any clear connection between $c$ and $d$ from this. What should I do? Thanks!",['complex-analysis']
4522974,Why is this condition sufficient for a topological space to be irreducible?,"I am studying an article about the space of valuation rings by Koji Sekiguchi and while trying to establish a morphism of ringed spaces, he used the fact that the space $Zar(K\mid A) $ of valuation rings  contained in a fixed field K and containing a ring A, equipped with the basis of topology: $$
\Sigma = \{Zar(K\mid E) \mid E \ \text{is a finite subset of} \ K\}  \quad \quad (1)
$$ is irreducible. The justification he used is a result about singletons in this space: $$
\overline{\{R\}} = \{R^\prime \in Zar(K \mid A) \mid R^\prime \subset R \} \quad \quad (2)
$$ I cannot see how $(2)$ would lead to the fact that $Zar(K\mid A)$ is irreducible. (a topological space is said to be irreducible if it cannot be expressed as a union of two proper closed subets.) Here's a citation for the article: Sekiguchi, Koji , Prüfer domain and affine scheme , Tokyo J. Math. 13, No. 2, 259-275 (1990). ZBL0726.14001 .","['commutative-algebra', 'algebraic-geometry', 'sheaf-theory', 'general-topology', 'schemes']"
4523004,Example of independent and identically distributed random variables,"Consider the probability space $(\Bbb [0, 1], \mathcal{B}, \mu),$ where $\mathcal{B}$ denotes the Borel sigma algebra in $[0, 1],$ and $\mu$ is the standard Lebesgue measure restricted to $[0, 1]$ . Could you please provide an example (mathematically defined) of two absolutely continuous random variables $X_1, X_2 : ([0, 1], \mathcal{B}, \mu) \rightarrow \Bbb R$ that are independent and identically distributed?","['measure-theory', 'independence', 'probability-theory', 'random-variables']"
4523032,Is $\frac {e^{\frac 1x} + 1} {e^{\frac 1x}}$ the derivative of some function,"I need to determine if there exists a differentiable function $f$ in $\mathbb R$ such that for every $x \neq 0$ , $f'(x)=\frac {e^{\frac 1x} + 1} {e^{\frac 1x}}$ I've been pondering this for quite a while, but I really have absolutely no idea how to approach this question. I would really like to hear how one tackles this, what the thought process is, and about different methods/approaches/theorems. EDIT: For clarification, this question is part of Calculus I. Although I know about integrals to some extent (from high-school), they haven't been discussed in the course. So this needs to be solved without them. And, according to some comments, I believe what I am after is a closed form ""primitive"", elementary function.","['calculus', 'functions', 'derivatives', 'real-analysis']"
4523048,$I-T : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ is not surjective,"Let $p > 1$ . Define $T : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ by $Tf(x) := \int_0^1 f(x+y)\,dy$ . Prove that $I-T$ is not surjective, where $I : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ is the identity map. Using Jensen's inequality, one can show that $\|Tf\|_p \leq \|f\|_p$ , so in particular, $T$ is well-defined, bounded linear operator with $\|T\| \leq 1$ . Beyond this, I don't have much idea how to proceed. I would appreciate any hint or reference.","['lp-spaces', 'functional-analysis', 'real-analysis']"
4523058,How to find the measure of an angle in degrees/radians from inverse trigonometric functions?,"OK, so it's relatively quite simple to say that if $$ \tan \alpha = \frac{3}{4} $$ Then $$ \alpha = {\tan}^{-1}(3/4) $$ Which of course means $\alpha  \approx 37° $ or $36.8698976458°$ to be more precise. But how do you get there? You can represent a $45°$ angle as $\tan^{-1}(1) $ or $\sin^{-1}(1/\sqrt{2})$ and you can have inverse trigonometric functions for all angles. But how do you calculate the measure of an angle (in degrees or radians) from a given inverse trigonometric function? I mean of course I can just pick up a scientefic calculator and enter $\arctan(\sqrt3)$ to get the value of $60°$ or its radian equivalent $\pi/3$ , i.e., $1.0471975512$ . But how do you do that manually? I mean the calculators nowadays are usually written all in High Level Languages, but the system does do the required calculation do give the result of $\arctan(\sqrt3)$ I have just begun with inverse trigonometry and I've got intrigued by this problem and would appreciate a simple explanation. And yes, this question applies to the other way round as well. What are the calculations required to find the trigonometric functions for any given angle? Sines, cosines, and other trigonometric functions for some common angles like 30°, 45°, 60°, 90° and thus some other values like 75°, 15°, 300°, etc. can easily be derived but how is that done for all angles? My current mathematical level is of early high school (class 11 CBSE), So it would be heavily appreciated if the explanation  is kept simple and detailed. Please let me know if it is beyond my current knowledge (or beyond the level I may be able to grasp) if you feel so.",['trigonometry']
4523100,Does the relation $\sinh(iz) = i\sin(z)$ have anything to do with a rotation of the complex plane?,"Ok, I recently learned about the following relation in complex analysis: $$i\sin(z) = \sinh(iz)$$ Now, let $\sin(z)$ be the image $I_1$ of the complex plane $\mathbb{C}$ , and $\sinh(iz)$ be another image $I_2$ of the complex plane $\mathbb{C}$ . Since the images are sets of vectors if we think of the complex plane as $\mathbb{R}^2$ , then since the relation above holds, does this mean that every vector in $I_2$ is rotated by $\pi/2$ radians CCW in relation to the corresponding vector in $I_1$ . So, in essence, does this mean that if we just decided to rotate the second image on our complex plane by $\pi/2$ radians, we would get the first image? How can one understand this geometrically, if that's the case so to speak. What does really happen here, and is there any visualization behind this. Thanks.",['complex-analysis']
4523119,Confused about Logic Machine in The Lady or the Tiger by Smullyan,"In The Lady or the Tiger by Raymond Smullyan, on p.172, it says: ""I am working in a language that contains names of various sets of numbers--specifically, positive integers. There are infinitely many sets of numbers nameable in this language. For example, we have a name for the set of even numbers, one for the set of odd numbers, one for the set of prime numbers, one for the set of all numbers divisible by 3--just about every set that number-theorists are interested in has a name in the language. Now, although there are infinitely many nameable sets, there are no more nameable sets than there are positive integers . And to each positive integer n is associated a certain nameable set A_n. We
can thus think of all the nameable sets arranged in an infinite sequence A_1, A_2,..., A_n..."" I'm confused about the bolded part. Wouldn't the number of nameable sets in this situation be the power set of the positive integers? If so, wouldn't it be uncountable and therefore have a larger cardinality than the positive integers? And wouldn't that mean that these nameable sets couldn't be arranged in a ""1, 2, ..., n..."" sequence? I haven't read the rest of the book, and despite being a lover of logic puzzles, I don't really understand Godel's Incompleteness Theorems or have formal education in advanced mathematics. I was wondering if I'm missing something or interpreting something wrong. Anyone care to provide enlightenment on the situation? Edit: I just realized something. Does it have to do with the number of expressible permutations in a phonetic language? Or whatever other restrictions a language would probably have?","['elementary-set-theory', 'incompleteness', 'puzzle']"
4523122,Area of set in complex plane,"Let $\Omega_1$ and $\Omega_2$ be two bounded open set in the complex plane and $f:\mathbb C \to \mathbb C$ an analytic function. We then define $$ A = \bigcup_{y \in \Omega_1}\{ (z,x) \in \mathbb C\times \Omega_2;  z^2 = y f(x) \}.$$ Can we give a formula for the volume of the set $A$ in terms of $f$ and the volume of $\Omega_1$ ?","['integration', 'complex-analysis', 'calculus', 'real-analysis']"
4523135,Showing a particular function is measurable,"I'm trying to prove the following. Let $X$ be a measurable space. Given that the the real-valued function f on $X$ is a measurable function, show that the function $f_{A}(x) = \begin{cases}|f(x)|,& |f(x)|\leq A\\ A,& f(x)>A\\ -A,& f(x)<-A\end{cases}$ is measurable given that A is some nonnegative real number.
My attempt is as follows: $\{x\in X\mid f_{A}(x)>\alpha\} = \{x\in X\mid |f(x)|>\alpha\}\cup\{x\in X\mid f_{A}(x) = A>\alpha\}\cup\{x\in X\mid f_{A}(x) = -A>\alpha\}$ . Now since $f$ is measurable and every constant function is measurable, each of these sets in the union is a measurable set and so our function $f_{A}$ is in turn a measurable set. I want to know if this attempt is correct, especially if this $\{x\in X\mid f_{A}(x)>\alpha\} = \{x\in X\mid |f(x)|>\alpha\}\cup\{x\in X\mid f_{A}(x) = A>\alpha\}\cup\{x\in X\mid f_{A}(x) = -A>\alpha\}$ is true. NB: I am not looking for a solution to the problem as I already have it, I just want to know if my attempt is correct, or if it makes sense at least. Any help is much appreciated.","['measure-theory', 'solution-verification']"
4523141,Does a sequence with no limit points eventually become monotonic?,"A common definition for the limit point of a sequence is that a number $x$ is a limit point of the sequence $(x_n)_{n\in \mathbb{N}}$ if there exists a subsequence that converges to $x$ . Intuitively, it appears that if a sequence has no limit points then for some $N$ the sequence is monotonic for all $n\geq N$ . Is this true? Formally speaking, by the Bolzano Weierstrass Theorem $\#\{n \in \mathbb{N}:x_n \in [-K,K]\}<\infty$ for every $K>0$ . Is this fact sufficient to imply that $x_n$ must explode in a monotone manner eventually?","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4523161,$\cos(a)+\cos(b)-\cos(a+b)\geq 1$,"I am trying to prove that $$\cos(a)+\cos(b)-\cos(a+b)\geq 1$$ For $a,b \geq 0$ and $0\leq a+b\leq 180^°$ I have checked in Wolfram Alpha that the inequality is true, but I am not able to prove it. The trigonometric identitiy I have tried to apply (basically, $$\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b))$$ does not seem useful as it is, so if there is some other one that you think it could help... any hint would be welcomed. Thanks!","['trigonometry', 'angle', 'geometric-inequalities']"
4523193,Are there infinitely many primes of the form [X]? We probably don't know.,"Are there infinitely many primes of the form [expression]? (We probably don't know. Sorry.) This question appears pretty often, with any number of various expressions. The sad reality is that the answer, more likely than not, is that we don't know. What we don't know about the prime numbers vastly outnumbers what we do know. Related questions regarding gaps between primes are common as well. For instance: Common unanswerable questions posted to M.SE Are there infinitely many primes of the form $n^2+1$ ? Or any other order-2 or higher polynomial. We think there probably are, but it's unproven, and going to be hard to prove. Are there infinitely many primes of the form $2^k + a$ ? Or any other exponential expression. Are there infinitely many prime gaps of the form [ expression ]? Does [ this formula ] generate all the prime numbers? (Though this is often ""No."") Note : There are other questions that ask ""What do we know about primes of the form [ expression ]?"" This is a very different question, and can generate good discussion! They're also rarer. I felt we could use an article that could be pointed to, edited, referenced, and the like, for questions of this type. Hopefully the community will find this useful. This post will organize some answers--both positive and negative--to the question for various expressions, and provide informative links and proofs where proofs are available. Please edit in more information if you think it's useful! Note that the questions and answers here are not intended to deal with error terms, sieve bounds, asymptotics, etc. We're just keeping it nice and simple. Even the Prime Number Theorem is outside the scope here. Terminology used below: The gap function, $g(p)$ , is the difference between two consecutive primes. That is, $g(p_i) = p_{i+1} - p_i$ . The prime-counting function, $\pi(x)$ , counts the number of primes equal to or less than $x$ . ""ATIM"": ""Are There Infinitely Many""","['number-theory', 'elementary-number-theory', 'prime-gaps', 'prime-numbers', 'faq']"
4523224,"Is there a hypergeometric function form of the quadratic formula that converges for all a,b,c?","After reading this question about a general formula for roots, I was curious about the simple case of $N=2$ . As is well known to school children, if $ ax^2 + bx + c = 0 $ , then the roots are $ \frac{-b \pm \sqrt{b^2-4ac}}{2a} $ Shuffling the binomial series around yields $$  -\frac{c}{b} \sum_{k=0}^{\infty} \binom{-\frac{1}{2}+k}{k} \left( \frac{4ac}{b^2} \right)^k \frac{1}{(k+1)} = -\frac{b}{2a} +  \frac{ \sqrt{b^2-4ac}}{2a} $$ but this doesn't converge for simple cases such as $ a=1,b=1,c=-1 $ since the radius of convergence is $<\frac{1}{4}$ . Is there a hypergeometric series that converges for all $a,b,c$ (where there are roots of course)?","['algebra-precalculus', 'roots', 'sequences-and-series']"
4523226,Understanding the weak* topology on the space of distributions,"I am studying distributions from Folland's book on real analysis. In this text he says A distribution on $U$ is a continuous linear functional on $C_C^\infty(U)$ (where $U$ is an open subset of $\mathbb{R}^n$ ). The space of all distributions on $U$ is denoted by $\mathcal{D}$ '(U). We impose the weak* topology on $\mathcal{D}'(U)$ , that is, the topology of pointwise convergence on $C_C^\infty(U)$ . I have been trying to better understand this topology and why we would impose this in the first place. Part of my troubles could be that I have not used the weak* topology much and so I am not too comfortable/experienced with it. To my knowledge, for a topological vector space $X$ , we look at the maps $\mathscr{X}_x \in X^{**}$ that are associated to some $x \in X$ defined as $\mathscr{X}_x(f) = f(x)$ for all $f \in X^*$ , and require all these maps to be continuous. This is the weak* topology. If this picture is correct, what interest is this topology in the context of distributions? Is it simply because it is weak enough to allow certain convergence results? Is there some better picture to have in mind?","['topological-vector-spaces', 'distribution-theory', 'real-analysis', 'functional-analysis', 'duality-theorems']"
4523260,Proof: product of σ-algebras in separable metric spaces,"Let $(X_1, d_1)$ ,..., $(X_n, d_n)$ be separable metric spaces, and define $X = X_1 \times ... \times X_n$ to be the produce space with the metric $$d((x_1,...,x_n),(y_1,...,y_n)) = \sqrt{d_1^2 (x_1,y_1)+...+ d_n^2 (x_n, y_n)}$$ Prove that $\mathcal{B}(X) = \mathcal{B}(X_1) \times...\times\mathcal{B}(X_n)$ Some toolsets I have are: Definition: The $\sigma$ -algebra $\mathcal{F}_1 \times ... \times \mathcal{F}_n$ is defined as the minimal $\sigma$ -algebra which contains all the sets of the form $A_1 \times ... \times  A_n$ , where $A_i \in \mathcal{F}_i , 1 \leq i \leq n$ . If $X$ is separable, then any open set can be represented as a countable union of open balls. Thus, for a separable space $X$ , we could define the Borel $\sigma$ -algebra  of $X$ as $\sigma (\mathcal{A})$ , with $\mathcal{A}$ being the family of all open balls.","['probability-theory', 'probability']"
4523275,Understanding the Kummer extension of semi-abelian varieties,"Currently, I am reading D. Bertrand's paper Galois representation and transcendental numbers ( here is the link to the paper), and I have a question about Theorem 2 (on Kummer theory of semi-abelian varieties). Theorem 2. Let $G = A \times L$ , and let $P$ be a point in $G(k)$ . For all prime numbers $\ell$ , the group $\chi_{(\ell)}(P)$ is commensurable with $T_\ell(G^o_P)$ , and coincides with $T_\ell(G^o_P)$ when $\ell$ is sufficiently large. The proof of Theorem 2 follows Ribet's method [60] (see also [37],[38]): since the representations $\rho_{(\ell)}$ , $\rho_\ell$ are semi-simple and satisfy Tate's conjecture [30], and in view of the finiteness or vanishing of the cohomology groups $H^1(\mathcal{G}_{(\ell)}, T_\ell(G))$ ([62], [21], [66]), one is reduced to showing that an endomorphism $\alpha$ of $G_P$ sends $P$ to a highly divisible point in $G_P(k)$ if and only if $\alpha$ itself is highly divisble in $\text{End}(G_P$ ; in practice, this will mean that $\alpha = 0$ . An effective version of this statement requires a precise description of the groups $G_P$ , and we return to this point in Theorem 8 below. He said: ""one is reduced to showing that ..."". I do not understand how to reduce the theorem to this claim. Do you have any hints or references? He also said that this Theorem can be proved using Ribet's method (in the paper Kummer theories on extensions of abelian varieties by tori, you can see the paper here ), but as I understand, in Ribet's paper, he proves claims involving almost all primes, while Theorem 2 involves every prime. Thank you. Edit: I have found a paper M .Hindry here where he sketches a proof of a more general result (appendix 2 in the paper).","['algebraic-number-theory', 'algebraic-geometry', 'kummer-theory', 'abelian-varieties']"
4523327,"Find the generating function, $f(x)$ of the squence $a_n =\binom{-3}{n}$","As you can expect, $f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3}$ by the ""generalized binomial theorem"". But I tryied to find $f(x)$ without using that formula, Binomial thm. I took $g(x) = \sum_{n=0}^{\infty} (-x)^n = \frac{1}{x+1}$ . Since $a_n = (-1)^n(n+1)(n+2)$ , I have to do only finding $f(x) = \frac{d^2}{dx^2}(x^2g(x))$ . The reason why claim like is $ \frac{d^2}{dx^2}(x^2g(x)) = \sum_{n=0}^{\infty} (-1)^n(n+1)(n+2)x^n$ . Therefore the answer is $f(x) = \frac{2}{(1+x)^3}$ When I used the formula $f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3}$ But without using the theorem, $f(x) = \frac{2}{(1+x)^3} (\neq (1+x)^{-3})$ . What the point did I missed? I can't find my errors in my solution. Regards.","['generating-functions', 'solution-verification', 'discrete-mathematics', 'sequences-and-series']"
4523356,Find the value of $x(t)$ when $x'(t)=0$ given $x''(t) = a\cdot x'(t) / x(t)$ and some initial conditions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am considering the differential equation $$ x''(t) = a\frac{x'(t)}{x(t)} $$ where $a>0$ is an arbitrary constant. I cannot solve this equation directly, but I am wondering if there's still any way I could answer the following below. Given an initial condition $x(0) = x_{0}$ and $x'(0) = -v_{0}$ where $x_{0}, v_{0}>0$ , how can I determine the position $x(T)$ for which $x'(T) = 0$ ? I don't care what the time $T$ is, just what is $x(T)$ ? Maybe there is a clever method to figure out the answer. If a simple formula can't be found, then I am wonder if there are any numerical technique, even approximate ones at that.","['ordinary-differential-equations', 'approximate-integration', 'nonlinear-system', 'numerical-methods', 'dynamical-systems']"
4523375,Estimate involving $L^{\infty}$ norm of function in $H^2(\mathbb{R}) \cap H^{-1/2}(\mathbb{R})$,"I have a question concerning a Gagliardo-Nirenberg type inequality involving the $L^{\infty}$ -norm on the unbounded domain $\mathbb{R}$ : To be more precise, I have encountered the following estimate $ \| f \|_{L^{\infty}} \leq C \| f \|_{L^2}^{1/2} \| f_x \|_{L^2}^{1/2}$ for some constant $C$ depending only on $f$ , where $f$ is an element of $H^2(\mathbb{R}) \cap \dot{H}^{-1/2}(\mathbb{R})$ , the latter being the homogeneous fractional Sobolev space in terms of Fourier transforms and $f_x$ denoting the (first) derivative . For a bounded domain $I \subset \mathbb{R}$ , the analogous statement follows immediately from the Gagliardo-Nirenberg inequality which may be found in Brezi's Functional Analysis book (Chapter 8, Comment 1 (iii), Equation (42)) and the Poincaré inequality. I, however, am interested in the inequality for the unbounded domain $\mathbb{R}$ . Is anyone aware of general interpolation inequalities taking care of this case?
Help is much appreciated! Thank you!","['lp-spaces', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
