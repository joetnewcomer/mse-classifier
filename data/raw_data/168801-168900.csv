question_id,title,body,tags
2961877,"Let $a\mid bc $ then prove or disprove $a\mid (a,b)c$","Prove or disprove: Let $a\mid bc$ then $a\mid (a,b)c$ Here is my approach, but I am not sure if I am doing this correctly or efficiently. Let $a\mid bc$ . It follows that either $a\mid b$ Proof: $b=ar, a\mid bc => (ar)c = a \rightarrow a(rc)=a \rightarrow a|a(rc)$ $a\mid c$ . Since $rc$ is an integer. $a\mid bc$ . Similar for $(2). a|c $ Let $a\mid (a,b)c$ . Using: the definition of $\gcd(a,b)=1=ax+by$ if $\exists x,y \in Z$ then we can rewrite it as $a\mid dc$ . This is as far as I go. I can't manipulate it so that I show that $a\mid (a,b)c$ . Does this mean that I would have to disprove $a\mid bc$ then $a\mid (a,b)c$ ? Any help would be appreciated.","['elementary-number-theory', 'gcd-and-lcm', 'abstract-algebra', 'divisibility']"
2961884,A visually guided proof of the fundamental theorem of algebra?,"A complex root of a polynomial $P(z)$ is a pair of real numbers $u,v$ that simultaneously make the real part and the imaginary part of $P(z)$ zero. The zeros of the real part and the imaginary part are given by two curves in the complex plane $$\operatorname{Re}(P(u +iv)) = 0$$ $$\operatorname{Im}(P(u+iv)) = 0$$ The zeros of the polynomial are the intersection points of these two curves. For the sake of specifity here are the two curves for $P(z) = z^5 + z^3 + z^2 + z + 1$ : $u^5-6u^3v^2-4v^2u^3+5uv^4+ u^3-v^2u-2uv^2+u^2-v^2+u+1 = 0 $ $v^5+5vu^4-10u^2v^3+3u^2v-v^3+2uv+v=0$ The fundamental theorem of algebra says that such curves always do intersect. A proof of the fundamental theorem might go like this: The curves $\operatorname{Re}(P(z)) = 0$ (red) and $\operatorname{Im}(P(z)) = 0$ (blue) – which are tightly related – are such-and-such, so they must intersect at least once and at most $n$ times (for $n$ the degree of the polynomial). What can be seen is, that the curves always come in $n$ branches which extend to infinity and for some reason must intersect. $x^2 + x + 1$ $x^3 + x^2 + x + 1$ $x^4 + x^2 + x + 1$ $x^5 + x^3 + x^2 + x + 1$ How could such a proof be spelled out?","['proof-writing', 'roots', 'abstract-algebra', 'polynomials', 'complex-numbers']"
2961899,Sparsest similar matrix,"Given a square matrix A (say with complex entries), which is the sparsest matrix which is similar to A? I guess it has to be its Jordan normal form but I am not sure. Remarks: A matrix is sparser than other if it has less nonzero entries. Two square $n \times n$ matrices $A,C$ are similar if there exists and invertible matrix $P$ such that $A = P^{-1}CP$","['matrices', 'sparse-matrices', 'linear-algebra']"
2961971,Does the series $\sum_{n=1}^\infty\frac{(2n)!}{2^{2n}(n!)^2}$ converge or diverge.,$$\sum_{n=1}^\infty\frac{(2n)!}{2^{2n}(n!)^2}$$ Can I have a hint for whether this series converges or diverges using the comparison tests (direct and limit) or the integral test or the ratio test? I tried using the ratio test but it failed because I got 1 as the ratio. The integral test seems impossible to use here.,"['calculus', 'sequences-and-series']"
2962012,"How many 5 digit numbers are there, whose i digit is divisible by i?","For example in the number 34567, the second digit (4) can be divided by 2, but the third digit (5) can't be divided by 3. I thought that the first digit can be divided by any number as 1,2,3,4,5,6,7,8,9 are all divisible by 1. The second digit can only be divided by 2, which is 2,4,6,8. The third digit can only be divided by 3, which is 3,6,9. The fourth digit can only be divided by 4, which is 4 or 8. The fifth digit can only be divided by 5.",['combinatorics']
2962013,Equation of Simple Harmonic Oscillator,"So I know that the differential equation of a simple harmonic oscillator is $\dfrac{d^2x}{dt^2}=-\omega^2x$ and it's solution is $y = A\sin(\omega t+\phi)$ . Now I learned for a solution with $n$ independent constants, the differential equation will be of order $n$ . In this case, there are 3 independent constants, $A, \omega, \phi$ , so shouldn't the differential equation be of order three? How is it two? Edit: Also when I start with the solution itself $y=A\sin(\omega t + \phi)$ $y'=A\omega\cos(\omega t + \phi)$ $\dfrac{\omega^2y^2}{A^2\omega^2}+\dfrac{y'^2}{A^2w^2} = 1$ $\omega^2\cdot2yy'+2y'y''=0$ $\omega^2=-\dfrac{y''}{y}$ $0=\dfrac{yy'''-y''y'}{y^2}$ $yy'''=y''y'$","['calculus', 'ordinary-differential-equations']"
2962073,Show that $\int\limits_0^{\frac{\pi}2}4\cos^2(x)\log^2(\cos x)~\mathrm dx=-\pi\log 2+\pi\log^2 2-\frac{\pi}2+\frac{\pi^3}{12}$,"Within this collection of definite integrals, number $30.$ is given by $$\int_0^{\frac{\pi}2}4\cos^2(x)\log^2(\cos x)\mathrm dx~=~-\pi\log 2+\pi\log^2 2-\frac{\pi}2+\frac{\pi^3}{12}\tag1$$ I figured out a way to evaluate the integral using a substitution and further the values of the derivatives of the Beta Function. My approach can be found on the bottom of the post. Another solution can be found here within the original post of the $(1)$ . I was just curious whether there is an easier attempt. First of all I thought about applying a Weierstraß Substitution to get rid of the trigonometry and instead solving an algebraic integral. Anyway it did not worked out quite well since the substitution made the new integrals even more complicated. Hence the logarithm is squared I guess series expansions $-$ although the linked answer invoked the Fourier Series Expansion of $\log(\cos x)$$-$ or IBP are not the right way to approach to this integral either. I am asking for interesting or elegant ways to evaluate $(1)$ . Please provide a different attempt than the two I suggested within this post if you are aware of one. Thanks in advance! Own evaluation First of all rewrite the integral sligthly $$\small\int_0^{\frac{\pi}2}4\cos^2(x)\log^2(\cos x)\mathrm dx=\int_0^{\frac{\pi}2}\cos^2(x)\log^2(\cos^2 x)\mathrm dx$$ To notice that the subsititution $\cos^2 x=y$ is suitable in this case. Computing $\mathrm dy$ and changing the borders of integration yields to $$\begin{align}
\small\int_0^{\frac{\pi}2}\cos^2(x)\log^2(\cos^2 x)\mathrm dx&=\small\int_1^0 (y)\log^2(y)~\left(\frac12\frac{-\mathrm dy}{\sqrt{y(1-y)}}\right)\\
&=\small\int_0^1\frac{\sqrt{y}\log^2(y)}{\sqrt{1-y}}\mathrm dy
\end{align}$$ One may note the familiar structure similiar to one given by the second derivative of the Beta Function. Therefore this integral can be written as $$\small\int_0^1\frac{\sqrt{y}\log^2(y)}{\sqrt{1-y}}\mathrm dy=\left.B_{xx}\left(x,\frac32\right)\right|_{x=\frac12}$$ The second derivative of the Beta Function is given in terms of the Beta Function connected with the Digamma and Trigamma Function as $$\small B_{xx}(x,y)=B(x,y)[(\psi^{(0)}(x)+\psi^{(0)}(x+y))^2+\psi^{(1)}(x)-\psi^{(1)}(x+y)]$$ Plugging in the values $x=\frac12$ and $y=\frac32$ yields to $$\begin{align}
\small\left.B_{xx}\left(x,\frac32\right)\right|_{x=\frac12}&=\small B\left(\frac12,\frac32\right)\left[\left(\psi^{(0)}\left(\frac12\right)+\psi^{(0)}\left(\frac32+\frac12\right)\right)^2+\psi^{(1)}\left(\frac12\right)+\psi^{(1)}\left(\frac12+\frac32\right)\right]\\
&=\small\frac14\pi\left[\left(\left(\psi^{(0)}\left(\frac12\right)+\psi^{(0)}\left(2\right)\right)^2\right)+\psi^{(1)}\left(\frac12\right)+\psi^{(1)}\left(2\right)\right]\\
&=\small\frac14\pi\left[(-\gamma-2\log 2-1+\gamma)^2+\frac{\pi^2}2-\frac{\pi^2}6+1\right]\\
&=\small\frac14\pi\left[4\log^2 2+4\log 2+\frac{\pi^2}3+2\right]\\
&=\small -\pi\log 2+\pi\log^2 2-\frac{\pi}2+\frac{\pi^3}{12}
\end{align}$$ Form hereon the desired equality follows $$\small\therefore~\int_0^{\frac{\pi}2}4\cos^2(x)\log^2(\cos x)\mathrm dx~=~-\pi\log 2+\pi\log^2 2-\frac{\pi}2+\frac{\pi^3}{12}$$","['integration', 'alternative-proof', 'definite-integrals', 'closed-form']"
2962278,Show that $\mathbb{R}$ is Hausdorff.,"Show that $\mathbb{R}$ with standard topology is Hausdorff. For any $x,y\in\mathbb{R}$ it is possible to define $\mathscr{U}_x=(x-\epsilon,x+\epsilon)$ for $\epsilon>0$ and $\mathscr{U}_y=(y-\delta,y+\delta)$ so that $\mathscr{U}_x\cap\mathscr{U}_y=\emptyset$ . Question : Is this proof right? If not. How should I answer the question? What tools should I use? Thanks in advance!","['general-topology', 'proof-verification', 'proof-writing', 'real-analysis']"
2962290,Contour integration and the central binomial coefficients,"I am trying to compute the integral $$\int_{-\infty}^\infty \frac{x^{2n}}{(x^2 + 1)^{n + 1}}\ dx.$$ From computational evidence, it's very obvious that $$\int_{-\infty}^\infty \frac{x^{2n}}{(x^2 + 1)^{n + 1}}\ dx = \frac{\pi}{4^n} {2n \choose n}.$$ Indeed, I can prove this via the generating function for the central binomial coefficients. However, I want to prove this via contour integration. With $f(z) = z^{2n} / (z^2 + 1)^{n + 1}$ , we can integrate over the semicircle of radius $R$ in the upper half-plane. Call this contour $\gamma_R$ . The integral over the arc of $\gamma_R$ goes to zero as $R \to \infty$ , which leaves $$\int_{-\infty}^\infty \frac{x^{2n}}{(x^2 + 1)^{n + 1}}\ dx = 2\pi i \operatorname{res}_i f.$$ The residue of $f$ at $i$ is $g^{(n)}(i) / n!$ , where $$g(z) = \frac{z^{2n}}{(z + i)^{n + 1}}.$$ Thus, we should have $$g^{(n)}(i) = \frac{-i n! {2n \choose n}}{2^{2n + 1}}.$$ How can I show that this equality holds? Or, more generally, How can I compute the residue of $f$ at $i$ ? I tried using the series $$\frac{1}{(1 - z)^{n + 1}} = \sum_{k \geq 0} {k + n \choose k} z^k,$$ but couldn't really make it work. Edit: I was asked to explain why my evaluation is ""obvious."" This is from using a computer algebra system to directly evaluate $g^{(n)}(i)$ for a few dozen $n$ . This gives some rational expressions which, when looked up in the OEIS , suggest the closed form I have given here. Then it is a trivial matter to estimate the integral numerically and compare it for hundreds of terms.","['complex-analysis', 'contour-integration', 'derivatives']"
2962301,"perimeter of sets. How do $P(E,\Omega)$ and $P(E\cap \Omega,\mathbb{R}^n)$ relate?","Let $\Omega$ be an open set in $\mathbb{R}^n$ and $E$ a Borel set. The relative perimeter of $E$ w.r.t. $\Omega$ is defined as $$P(E,\Omega)=\sup\left\{\int_{\Omega}\chi_E(x) \mathrm{div}\boldsymbol{\phi}(x) \, \mathrm{d}x : \boldsymbol{\phi}\in C_c^1(\Omega,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\Omega)}\le 1\right\},$$ see here https://en.wikipedia.org/wiki/Caccioppoli_set . Suppose that $E\cap \Omega$ is non-empty. Is $P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n)$ ? Or is $P(E,\Omega)=P(E\cap \Omega,\mathbb{R}^n)$ +some (n-1)-dimensional Hausdorff measure? It is $$P(E\cap \Omega,\mathbb{R}^n)=\sup\left\{\int_{\mathbb{R}^n}\chi_{E\cap\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ $$=\sup\left\{\int_{\mathbb{R}^n}\chi_{E}\chi_{\Omega}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ $$=\sup\left\{\int_{\Omega}\chi_{E}(x) \mathrm{div}\boldsymbol{\psi}(x) \, \mathrm{d}x : \boldsymbol{\psi}\in C_c^1(\mathbb{R}^n,\mathbb{R}^n),\ \|\boldsymbol{\phi}\|_{L^\infty(\mathbb{R}^n)}\le 1\right\}$$ From there I conclude $P(E,\Omega)\le P(E\cap \Omega,\mathbb{R}^n)$ . However, does $P(E,\Omega)\ge P(E\cap \Omega,\mathbb{R}^n)$ hold? Or how do the perimeters relate?","['integration', 'geometric-measure-theory', 'analysis', 'bounded-variation']"
2962306,"Given that f(x) is a quadratic function and that f(x) is only positive when x lies between -1 and 3, find f(x) if f(-2) = -10.",What i did: $f(x)=ax^2+bx+c$ $f(-2)=4a-2b+c=-10$ $f(0) =c  > 0$ $f(1) =a+b+c   > 0$ $f(2) =4a+2b+c  > 0$ I thought using $b^2-4ac = 0$ for $f(-2)$ but its wrong since I am getting c = -6. ANS : $-2x^2+4x+6$,"['functions', 'quadratics', 'polynomials']"
2962311,Induction: How to prove each number can be written as sum of natural numbers such that sum of inverses of these numbers will be 1.,"For every $n>32$ , prove that $n$ can be written as sum of a number of natural numbers such that sum of inverses of these numbers will be 1. Suppose the statement is correct for $33, 34, ..., 73$ . I want to prove this by strong induction. if $n$ is even, we know $n=2+2k$ . Also $2k=k+k$ and we know $k=\sum{a_i}$ such that $\sum1/a_i$ is 1. 
Therefore $2k=\sum(2a_i)$ and for inverses we have: $\sum1/(2a_i) = 1/2*\sum1/a_i = 0.5$ Now since $n = 2 + 2k$ , $n = 2 + \sum 2a_i$ and $(1/2 + \sum 1/(2a_i) = 0.5 + 0.5 = 1)$ I'm not sure how can i prove the statement for odd numbers though. Any ideas? Thanks in advance.","['induction', 'discrete-mathematics']"
2962351,Surjective and Injective function,"Let $N=\{1,2,3...\}$ be the set of natural numbers and $F:N \times N \rightarrow N$ be such that $f(m,n)=(2m-1)*2^n.$ (A)F is Injective. (B)F is Surjective. (C)F is Bijective. (D)None of the above. I can see that F can never be surjective because 1 does not have a pre-image. And it seems injective to me because $2m-1$ term would always be odd, $2^n$ term would always be even, and hence the product will always be Even, and for different values of m and n, we would get a different even number. Is my Reasoning correct?",['functions']
2962352,"$(0,\infty)$ has Measure Zero with Given Measure","I've been trying to work through the following problem. Let $\mu$ be a measure defined on $(\mathbb{R},\mathscr{L})$ , where $\mathscr{L}$ is the $\sigma$ -algebra of all Lebesgue measurable subsets of $\mathbb{R}$ . Suppose that there exists a $K<\infty$ such that $\int_{\mathbb{R}}e^{nx}~d\mu(x)\leq K$ for each $n\in\mathbb{Z}^{+}$ . Prove that $\mu((0,\infty))=0$ . I'm really not sure what I should be aiming for here. I was thinking of defining a sequence $f_{n}(x)=\int_{\mathbb{R}}e^{nx}~d\mu(x)$ for each $n\in\mathbb{Z}^{+}$ and possibly seeing where one of the convergence theorems takes me? However, I don't see how this would recover the measure of the given interval. Any help is greatly appreciated!","['measure-theory', 'lebesgue-integral', 'real-analysis']"
2962385,Integral of a function with respect to the measure $\nu(A)=\int_{A}fd\mu$,"Question: Given measure space $(X,\mathcal(A),\mu)$ and non-negative measurable function $f:X\to[0,\infty]$ , consider the measure $\nu(A) = \int_{A}fd\mu$ defined for all $A\in\mathcal{A}$ .  Prove that for any $\mu$ -measurable function $g$ , $$ \int_{X}gd\nu = \int_{X}fgd\mu $$ Attempted Solution: I am thinking we can use (1) the fact that any measurable function $g$ is a pointwise limit of simple functions, and (2) monotone convergence theorem, to extend the result for simple functions So let $\varphi$ be a simple function with canonical form $\sum_{k}a_k \chi_{a_k}$ .  Then $$ \int_{X}\varphi d\nu = \sum_{k}a_{k}\nu(A_{k}) = \sum_{k}a_{k}\int_{A_{k}}fd\mu = \sum_{k}\int_{A_{k}}a_{k}fd\mu  $$ And hereI am stuck....","['integration', 'measure-theory']"
2962394,Bounding survival probability of an asymmetric random walk by a symmetric one,"Consider two random walks that start from point $x=0$ and time $t=0$ and move either to right $x+1$ or left $x-1$ : 1) Walker 1's first move is with equal probability to the right/left. However, any other move depends on the previous move: if he moves to the right in the $k$ th step, he is more likely to move to the left in the $(k+1)$ th step. Denote its location after the $n$ th step by $X_n$ . 2) Walker 2 is a symmetric random walker that in every step goes to either right or left with equal probability. Denote its location after the $n$ th step by $Y_n$ . Prove that for all $m,n\geq 0$ : $$\mathbb{P}\left(\max\limits_{1 \leq k \leq n}X_k\geq m\right) \leq \mathbb{P}\left(\max\limits_{1 \leq k \leq n}Y_k\geq m\right)\tag{1}$$ Intuitively, the Walker 1's location is more centered around the origin since everytime it moves to the right, it has a tendency to go to the left in the next step, but I found it hard to prove. I have verified it using simulation. I thought about induction: Base case: Let $n=m$ . Eq. (1) follows immediately since for $n=m$ means the Walkers have to make $m$ steps to the right, which is less likely for the Walker 1. Now, assume (1) is true for $m,n$ . We want to conclude (1) for $m,n+1$ . Observe: $$\mathbb{P}\left(\max\limits_{1 \leq k \leq n+1}X_k\geq m\right)= \mathbb{P}\left(\max\limits_{1 \leq i \leq n}X_n\geq m\right)\\+  \mathbb{P}\left(\left\{\max\limits_{1 \leq i \leq n}X_n\leq m-1\right\} \cap\{ X_{n-1}=m-1\} \cap \{X_{n-1}=m\} \right).$$ The first term on the right-hand side of above immediately is proved from (1), but the second term cannot be easily bounded. Any idea?","['stochastic-processes', 'probability-theory', 'random-walk']"
2962402,Weird notation for function,"I was going through my textbook today and I found this question which defined a function $f(x)$ as $$f(x)=\int_0^{x}\sin{(\lfloor{2x}\rfloor)}dx$$ and was asking to calculate $f(\frac{\pi}{2})$ . I get that the question wants me to evaluate, $$f(x)=\int_0^{\frac{\pi}{2}}\sin{(\lfloor{2x}\rfloor)}dx$$ But this notation is bothering me because of the $x$ inside the integral. Shouldn't the proper notation be, $$f(x)=\int_0^{x}\sin{(\lfloor{2t}\rfloor)}dt$$ Am I wrong? Thanks for the help.","['integration', 'functions', 'notation']"
2962432,What are $f\circ\emptyset$ and $\emptyset\circ f$ if $\circ$ is function composition and $f$ is any function?,"My guess is that both are $\emptyset$ because if $g\circ f=\{(x,z)\mid \exists y\in \text{Im}f:(x,y)\in f\land (y,z)\in g\}$ then if $f$ or $g$ are the empty set then it doesn't exist any $y$ with such condition, so the set is empty. Am I right?",['functions']
2962444,Any Point in a Square is Covered by Finitely Many Disks,"I'm working on the following problem. Let $A$ be a square of side length 1 in $\mathbb{R}^{2}$ . Let $B(x_{k},r_{k})$ be disks centered at points $x_{k}\in A$ of radius $r_{k}$ . Suppose that $\sum_{k=1}^{\infty}r_{k}^{2}<\infty$ . Show that, with respect to the Lebesgue measure on $\mathbb{R}^{2}$ , almost every point of $A$ is covered by only finitely many disks. My attempt at a solution : For each $k$ , we note that $B(x_{k},r_{k})$ is contained in the square $a_{k}$ of side length $2r_{k}$ . Moreover, $$\sum_{k=1}^{\infty}m_{2}(a_{k})=4\sum_{k=1}^{\infty}r_{k}^{2}<\infty$$ by the assumed condition on the radii $r_{k}$ . Let $A_{\infty}$ denote the set of points in $A$ which lie in infinitely many of the squares $a_{k}$ . We must show that $m_{2}(A_{\infty})=0$ . Define $f(x)=\sum_{k=1}^{\infty}\chi_{a_{k}}(x)$ , where $\chi_{a_{k}}$ is the characteristic function of $a_{k}$ . For each $x\in A$ , each term of $\sum_{k=1}^{\infty}\chi_{a_{k}}(x)$ is either $0$ or $1$ . In particular, $x\in A_{\infty}$ if and only if $f(x)=\infty$ . Now, since each $\chi_{a_{k}}$ is a measurable function and $f$ is the infinite sum of these functions, we have $$\int_{A}f(x)~dx=\sum_{k=1}^{\infty}\int_{A}\chi_{a_{k}}(x)~dx<\infty.$$ Thus, $f(x)<\infty$ almost everywhere. Thus, each $x\in A$ lies in at most finitely many of the $a_{k}$ , and hence finitely many of the $B(x_{k},r_{k})$ . Does this proof look okay? I think my logic is sound, but I just want to make sure that it doesn't have any gaps. Thanks in advance for any help!","['lebesgue-measure', 'measure-theory', 'proof-verification', 'lebesgue-integral']"
2962447,Detail in Perelman's proof of the Soul Conjecture: why $O(\delta^2)$ and not $O(\delta)$?,"Referring to G. Perelman, Proof of the soul conjecture by Cheeger and Gromoll . Given a distance-nonincreasing retraction $P$ from an open complete manifold of nonnegative curvature onto its soul $S$ , one wants to prove that $P(\exp_xt\nu)=x$ for every $x\in S$ and $t\geq 0$ . Suppose this is true up to $t=l$ , and consider the function $$f(r)=\max\{|xP(\exp_x(l+r)\nu)|\mid x\in S, \nu\in SN_x(S)\}$$ where $SN(S)$ is the unitary normal bundle to $S$ . It seems to me that the only way to interpret the notation $|xP(\exp_x(l+r)\nu)|$ is ""the Riemannian distance between the two points"". But later in the proof, Perelman does the following: he has a geodesic $\gamma(u)\in S$ , a parallel normal field $\nu$ along $\gamma$ , and two ""vertical"" geodesics related to a variation around $\gamma$ , namely $\sigma_{u_0}(t)=\exp_{\gamma(u_0)}t\nu$ and $\sigma_{u_1}(t)=\exp_{\gamma(u_0)}t\nu$ , $t$ varying. He then proves that the two-dimensional strip between these two $\sigma_{u_0}$ and $\sigma_{u_1}$ is flat and totally geodesic, and that all the
  ""horizontal"" geodesics $\gamma_t(u)=\exp_{\gamma(u)}t\nu,
> u\in[u_0,u_1]$ are minimizing of constant length $l(\gamma|_{[u_0,u_1]})$ . Then he claims that, for small $\delta$ , $$f(r-\delta)\geq |\gamma(u_1)P(\sigma_{u_1}(l+r-\delta))|\geq|P(\sigma_{u_0}(l+r)\gamma(u_1)|-|P(\sigma_{u_0}(l+r))P(\sigma_{u_1}(l+r-\delta))|\geq|P(\sigma_{u_0}(l+r))\gamma(u_1)|-|\sigma_{u_1}(l+r-\delta)\sigma_{u_0}(l+r)|\geq|P(\sigma_{u_0}(l+r)\gamma(x_1)|-|\sigma_{u_0}(l+r)\sigma_{u_1}(l+r)|-O(\delta^2).$$ Recall that $\gamma(u)=\sigma_u(0)$ and think that you can just put $l=0$ for simplicity. The first inequalities are easy applications of the triangle inequality and the fact that $P$ does not increase distances. Now consider the last inequality of the chain just written. It seems to me that he is applying the triangle inequality to the term with sign ""-"" and then says that one can estimate the distance $\rho(\sigma_{u_0}(l+r-\delta),\sigma_{u_0}(l+r))$ as $O(\delta^2)$ . I wonder why. Shouldn't it be $O(\delta)$ ? Moreover, he says that he uses in that last in equality the fact that I put under citation. Where does he use it? Does it allow to gain the $O(\delta)$ ? I don't think so, since in the case of $\mathbb R^2$ that distance is precisely $\delta$ . Another hypothesis was the following: he is not using the distances, but their squares. This would provide the $\delta^2$ , but what about the triangle inequality? That should not be true anymore... So I am surely missing something important. Can someone help me? Thank you in advance. EDIT: Question solved. One can use flatness to use Pitagoras's theorem and then that $\sqrt{1+a^2}= 1+a^2/2+O(a^2)$ .","['curvature', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2962478,Trace inequality for a product of p.s.d. matrices and their pseudo inverse.,"Let $A, B_i$ be positive semidefinite real matrices. Let $\dagger$ stand for the Moore-Penrose generalized inverse. I managed to prove that if $\operatorname{Ran}B_1\subseteq\operatorname{Ker}B_2$ then $$\operatorname{trace}\left((A + B_1 + B_2)^\dagger B_1 \right) \leq\operatorname{trace}\left(( A + B_1)^\dagger B_1\right) $$ Does it still hold without this assumption?","['positive-semidefinite', 'matrices', 'linear-algebra', 'generalized-inverse', 'positive-definite']"
2962502,How many solutions of $3z^5 + z^2 + 1=0$ have in $1<|z|<2$.,I used Rouche's theorem. I got $5$ solutions in $1<|z|<2$ . Is my approach correct?,['complex-analysis']
2962530,Symmetric function on set of size four,"Let $A=\{1,2,3,4\}$ , $\mathcal{A}$ be the set of all nonempty subsets of $A$ , and $\mathcal{B}$ be the set of all subsets of $A$ of size $1$ or $2$ . Is there a function $f:\mathcal{A}\times\mathcal{A}\rightarrow\mathcal{B}$ such that for any $X,Y\in\mathcal{A}$ : $f(X,Y)=f(Y,X)$ , $f(X,Y)$ contains at least one element of $X$ , and if $f(X,Y)$ contains exactly one element of $X$ , then $f(X',Y)$ contains no more than one element of $X$ for any $X'\in\mathcal{A}$ ? From the given conditions, if $X$ and $Y$ are disjoint, $f(X,Y)$ contains exactly one element of both $X$ and $Y$ . Moreover, we may deduce some values of $f$ . For example assume that $f(12,3)=13$ . (This is shorthand for $f(\{1,2\},\{3\})=\{1,3\})$ . Then by the last condition, $f(12,13)=13$ . Applying the last condition again, $f(2,13)=23$ , which by the first condition means that $f(13,2)=23$ . Similarly we can find the values of $f(1,23),f(12,23),f(13,23)$ .","['elementary-set-theory', 'functions', 'combinatorics']"
2962545,"If $\langle g \rangle$ is the only subgroup of order $p$, what's the order of elements $xg$?","Somewhat getting distracted from an exercise, I noticed that if a group $G$ has only one subgroup of order $p$ where $p$ is a prime (so our subgroup is wlog generated by $g$ ), any conjugation $$xgx^{-1}$$ must have order p as well, i.e. be representable as $g^k$ for $1\leq k < p$ . Question: Is there any simple criterion for the order of $xg$ ? My observations By induction, we can prove Lemma Let $g, x\in G$ with $xg=g^kx$ with $1<k$ . Then $x^ng = g^{k^n}x^n$ , and furthermore, $$
  (xg)^l = g^{k\left(k^l-1\right)/{(k-1)}} x^l
$$ Now, if we demand $1<k<p$ , we can note that \begin{align}
  e=g^{[\cdots]} &\iff p \vert k \frac{k^l-1}{k-1} \\
  &\iff p \vert k \:\vee\: p\vert \frac{k^l-1}{k-1} \\
  &\iff p(k-1)\vert k^l-1 \\
  &\iff p \vert k^l-1 \:\wedge\: (k-1)\vert k^l-1 \\
  &\iff p \vert k^l-1 \iff k^l \equiv 1 \mod p \\
  &\iff l \vert p-1
\end{align} Where we used properties like $p$ being prime and $p$ and $k-1$ being coprime. We get an immediate corollary of that: Observation If $l\vert p-1$ and $\operatorname{ord}x\vert l$ , we have $(xg)^l=e$ , i.e. $\operatorname{ord} (xg) \mid l$ . Is there any way to strengthen this assertion? Assuming the best case scenario, can we get an explicit expression for the order of $xg$ ? Proof of the commutation rule First we'll prove $x^ng = g^{k^n}x^n$ .  The case $n=1$ is clear, so assume it holds for $n$ . We have $$
x^{n+1}g = xx^ng = xg^{k^n}x^n = g^{k\cdot(k^n)}xx^n = g^{k^{n+1}}x^{n+1}.
$$ Similarly, assume $(xg)^l = g^{k\left(k^l-1\right)/{(k-1)}} x^l$ (the base case $l=1$ is clear again).  We have $$
  (xg)^{l+1} = g^{k\left(k^l-1\right)/{(k-1)}} x^{l+1} g\\
  = g^{k\left(k^l-1\right)/{(k-1)}} g^{k^{l+1}} x^{l+1}\\
  = g^{k\left(k^l-1\right)/{(k-1)}} g^{k\left(k^{l+1}-k^l\right)/{(k-1)}} x^{l+1}\\
  = g^{k\left(k^{l+1}-1\right)/{(k-1)}} x^{l+1}.
$$","['group-theory', 'finite-groups', 'divisibility']"
2962567,"$x\in\triangle_{i=1}^{n}A_i$ if and only $x$ belongs to an odd number of $A_1,A_2,\dots,A_n$","Is the following argument correct? Proposition . Given an arbitrary $x$ and sets $A_1,A_2,\dots,A_n$ , we have $x\in A_1\triangle A_2\triangle \dots\triangle A_n$ if and only if $x$ belongs to an odd number of $A_1,A_2,\dots,A_n$ , here $A_1\triangle A_2 = (A_1\backslash A_2)\cup(A_2\backslash A_1)$ . Proof. We contruct the proof by recourse to Mathematical-Induction, the proof of the base case is trivial. We therefore proceed with the inductive step. Assume for an arbitrary $k\in\mathbf{N}$ that given any $k$ sets $A_1,A_2,\dots,A_k$ , $x\in A_1\triangle A_2\triangle \dots\triangle A_n$ if and only if $x$ belongs to an odd number of $A_1,A_2,\dots,A_n$ . Now let $X_1,X_2,\dots,X_k,X_{k+1}$ be any $k+1$ arbitrary sets. We prove both conditionals as follows $(\Rightarrow).$ Assume $x\in (\triangle_{j=1}^{k}X_j)\triangle X_{k+1}$ , then either $x\in(\triangle_{j=1}^{k}X_j)\backslash X_{k+1}$ in which case the inductive hypothesis readily implies that $x$ belongs to an odd number of of $X_1,X_2,\dots,X_k$ , the claim in question is the evident, or $x\in X_{k+1}\backslash(\triangle_{j=1}^{k}X_j)$ but then $x\not\in \triangle_{j=1}^{k}X_j$ , and thus by inductive hypothesis $x$ belongs to  an even number of $X_1,X_2,\dots,X_k$ which together with $x\in X_{k+1}$ , implies that $x$ belongs to an odd number of $X_1,X_2,\dots,X_k,X_{k+1}$ . $(\Leftarrow).$ Now assume that $x$ belongs to an odd number of $X_1,X_2,\dots,X_{k},X_{k+1}$ . Now either $x\in X_{k+1}$ , in which case $x$ belongs to an even number of $X_1,X_2,\dots,X_k$ which by inductive hypothesis implies that $x\not\in\triangle_{j=1}^{k}X_j$ and by extension $x\in X_{k+1}\backslash(\triangle_{j=1}^{k}X_j)\subseteq X_{k+1}(\triangle_{j=1}^{k+1}X_j)$ , or $x\not\in X_{k+1}$ , but then $x$ belongs to an odd number of $X_1,X_2,\dots,X_k$ and by inductive hypothesis $x\in \triangle_{j=1}^{k}X_j$ , thus $x\in (\triangle_{j=1}^{k}X_j)\backslash X_{k+1}\subseteq \triangle_{j=1}^{k+1}X_j$ . $\blacksquare$","['elementary-set-theory', 'proof-verification']"
2962641,approximate identities for convolution with measure,"Convolution makes $L^1(\mathbb{R}^n, \mathbb{C})$ into an associative algebra that has no identity, but that does have an ""approximate identity"" in the sense that for any sequence $\varphi_1, \varphi_2,...$ of nonnegative $L^1$ functions with $\int \varphi_k~d\lambda=1$ and $$\lim_{k\rightarrow \infty}\int_{|x|>\epsilon}\varphi_k ~~d\lambda(x)  =0~~~\text{for any }\epsilon>0$$ and any $f\in L^1$ , we have $ \varphi_k \ast f \rightarrow f$ in $L^1$ (where $\lambda$ is Lebesgue measure). Is the same true (and in what sense?) if we replace $f$ with a Borel probability measure which is not necessarily absolutely continuous w.r.t. Lebesgue measure? I.e. with $\varphi_k$ as above, define $$(\varphi_k \ast \mu)(y) := \int_{\mathbb{R^n}} \varphi_k(y-x)d\mu(x).$$ Do we have $$(\varphi_k \ast \mu) \lambda \rightarrow \mu$$ in some sense? Still another way to ask it is whether we have $$(\varphi_k \lambda) \ast \mu \rightarrow \mu$$ in some sense (where this $\ast$ means convolution of measures)?","['analysis', 'real-analysis']"
2962644,Number of real zeroes of iterated polynomial: $x^3-2x+1$,"If $P(x)=x^3-2x+1$ , define $z_n$ as the number of real roots of the polynomial $P^{\circ n}(x)$ , where the superscript denotes $n$ -fold composition. Can we find a general formula for $z_n$ , or perhaps a recurrence of some sort? The sequence begins $3,7,15,27,47,...$ and is not in the OEIS. I have solved this problem for the polynomial $Q(x)=x^3-3x+1$ , and determined that the number of real zeroes of $Q^{\circ n}$ is equal to $2^{n+1}-1$ . However, this problem was much easier because the maximum values of $Q$ occur at integer values of $x$ and the zeroes of $Q$ are irrational, allowing one to break the real line into intervals of the form $[k,k+1]$ and kept track of which intervals $Q$ maps onto one another. Can anyone figure out how to do this with $P(x)$ ? This problem has puzzled me for a while, so I am willing to offer a $+50$ bounty for a satisfactory answer or analysis of the problem (as soon as the rules of MSE will allow me to offer it). It would also be helpful if anyone could provide a large list of values of $z_n$ , since all of the values I have were obtained by counting by hand. Cheers! EDIT: Should a closed-form formula or recurrence elude any potential answerers, it would also be nice to obtain a (proven) asymptotic formula for $z_n$ instead of a closed-form formula.","['combinatorics', 'polynomials', 'function-and-relation-composition', 'roots']"
2962695,Are Baire class functions closed under pointwise limits?,"I am confused about the notion of Baire functions (real or complex valued) on a compact space $X$ . The set of Borel functions on $X$ , $Bo(X)$ is defined to be the set of those functions $f$ for which $f^{-1}(U)$ is a Borel set, when $U$ is open. On the other hand, the Baire functions of class $\alpha$ , $Ba_{\alpha}(X)$ , where $\alpha$ is a ordinal less than $\omega_1$ , are defined iteratively as the pointwise limit of functions in the previous Baire classes, with $Ba_{0}(X)=C(X)$ , continuous functions on $X$ (see here ). Then the set of all Baire functions is defined to be $Ba(X):=\cup_{\alpha<\omega_1}Ba_\alpha$ Clearly, $Bo(X)\supset Ba(X)$ . Now, this article assumes my definition of Baire functions, and this article (see ""Comparison with Baire functions""), says that the Baire functions are the smallest set of those real functions closed under pointwise limits and containing continuous functions. This however, seems to imply that $Ba(X)$ is closed under pointwise limits. Is this obvious?  I cannot seem to show this for $Ba(X)$ . (If we restrict ourselves to bounded functions, is the statement true for uniform limits of Baire functions?) Any help is much appreciated!","['banach-spaces', 'general-topology', 'functional-analysis', 'descriptive-set-theory']"
2962720,Having trouble showing a function is continuous: $x/(1+|x|)$,"I am having trouble showing that a function is continuous using the $\epsilon, \delta$ definition. My function $f:\mathbb{R}\rightarrow(-1,1)$ is defined as $f(x) = \frac{x}{1+|x|}$ . Looking at the graph, the function is clearly continuous, but when I expand I don't get anywhere: $ |f(x) - f(x_0)| = |\frac{x}{1+|x|} - \frac{x_0}{1+|x_0|}| 
= |\frac{x(1+|x_0|) - x_0(1+|x|) }{(1+|x|)(1+|x_0|)}|$ This does not seem to simplify much at all, I cannot find a $\delta$ that satisfies: $\forall \epsilon > 0, \exists \delta > 0, \forall x,y, \in \mathbb{R}, |x-x_0| < \delta \Rightarrow |f(x) - f(x_0)| < \epsilon $ Am I missing something here? Any advice is greatly appreciated.","['continuity', 'epsilon-delta', 'analysis', 'real-analysis']"
2962735,Matrix logarithm not in Lie algebra,"In Hall's Lie Groups, Lie Algebras, and Representations: An Elementary Introduction , he defines the Lie algebra of a matrix Lie group $G$ as the set $\mathfrak{g}$ of all matrices $X$ such that $e^{tX}\in G$ for all $t\in\mathbb{R}$ . Here the exponential of matrices is defined using the power series. Similarly, he defines the logarithm of a matrix $A$ using power series: \begin{equation}
\log A=\sum_{m=1}^{\infty}(-1)^{m+1}\frac{(A-I)^m}{m}.
\end{equation} It is known that this series converges when $\|A-I\|<1$ , where $\|\cdot\|$ is the Hilbert-Schmidt norm. Now, in Exercise 3.7, we are asked the following question: Given an $A$ in a matrix Lie group such that $\|A-I\|<1$ (so that the
series above converges), is it always true that $\log A$ is in $\mathfrak{g}$ ? Prove or give a counterexample. My idea is the following: We know that the exponential map $exp:\mathfrak{g}\to G$ is a local diffeomorphism between a small neighbourhood $U$ of $0$ in $\mathfrak{g}$ and a small neighbourhood $V$ of $I$ in $G$ . However, $V$ may be very small such that it may not contain some $A$ that satisfies $\|A-I\|<1$ (that is, although $A$ is already closed to $I$ , it may still not in $V$ ). In this case, $\log A$ may not necessarily inside $\mathfrak{g}$ . But then when I tried to find some counterexamples, they are all outside the radius $1$ ball of $I$ (i.e., these examples $A$ are such that $\|A-I\|>1$ ). Thus, I am lost again. Any hint, suggestion, comment and answer are much appreciated.","['matrices', 'linear-algebra', 'lie-algebras', 'lie-groups']"
2962749,Why are these two definitions of Markov property equivalent?,"Question Suppose that $S$ is a finite or a countable subset of $\mathbb R$ and $(\xi_n)_{n\in\mathbb N}$ is an $S$ -valued sequence of random variables. Then are these two definitions of Markov property equivalent? 1. For all $n\in\mathbb N$ and all $s\in S$ , $P(\xi_{n+1}=s|\xi_0,\ldots,\xi_n)=P(\xi_{n+1}=s|\xi_n)$ . 2. For all $n\in\mathbb N$ and for all $s_0,\ldots,s_{n+1}\in S$ , $P(\xi_{n+1}=s_{n+1}|\xi_0=s_0,\ldots,\xi_n=s_n)=P(\xi_{n+1}=s_{n+1}|\xi_n=s_n)$ . Here, $P(A|\xi_0,\ldots,\xi_n):=P(A|\sigma(\xi_0,\ldots,\xi_n))=E(1_A|\sigma(\xi_0,\ldots,\xi_n))$ for all $A$ in the given $\sigma$ -algebra of the probability space and all random variables $\xi_0,\ldots,\xi_n$ . How did I come to the question I'm reading Brzezniak, & Zastawniak. ""Basic Stochastic Processes."" In the book it defines Markov chain as follows(Note that (5.10) is same as 1 in my question): Definition Suppose that $S$ is a finite or a countable set. Suppose also that a probability space $(\Omega,\mathcal F,P)$ is given. An $S$ -values sequence of random variables $\xi_n$ , $n\in\mathbb N$ , is called an $S$ -valued Markov chain or a Markov chain on $S$ if for all $n\in\mathbb N$ and all $s\in S$ $$P(\xi_{n+1}=s|\xi_0,\ldots,\xi_n)=P(\xi_{n+1}=s|\xi_n).\tag{5.10}$$ Here $P(\xi_{n+1}=s|\xi_n)$ is the conditional probability of the event $\{\xi_{n+1}=s\}$ with respect to random variable $\xi_n$ , or equivalently, with respect to the $\sigma$ -field $\sigma(\xi_n)$ generated by $\xi_n$ . Similarly, $P(\xi_{n+1}=s|\xi_0,\ldots,\xi_n)$ is the conditional probability of $\{\xi_{n+1}=s\}$ with respect to the $\sigma$ -field $\sigma(\xi_0,\ldots,\xi_n)$ generated by the random variables $\xi_0,\ldots,\xi_n$ . Property (5.10) will usually be referred to as the Markov property of the Markov chain $\xi_n$ , $n\in\mathbb N$ . The set $S$ is called the state space and the elements of $S$ are called states . But in the proof of the proposition that follows, it says: ... A similar line of reasoning shows that $\xi_n$ is indeed a Markov chain. For this we need to verify that for any $n\in\mathbb N$ and any $s_0,s_1,\ldots,s_{n+1}\in S$ $$P(\xi_{n+1}=s_{n+1}|\xi_0=s_0,\ldots,\xi_n=s_n)=P(\xi_{n+1}=s_{n+1}|\xi_n=s_n).$$ ... It's asserting that 2 in my question implies 1. It gives no proof for that. I'm pretty sure that 1 also implies 2 because if you see the 'definition' in this link: https://en.wikipedia.org/wiki/Markov_property there is a formulation which is the same as 2. My attempt I really don't see a way to start. For 1 $\to$ 2, I put $\zeta_0=P(\xi_{n+1}=s_{n+1}|\xi_0,\ldots,\xi_n)$ , $\zeta_1=P(\xi_{n+1}=s_{n+1}|\xi_n)$ . I aslo put $A=\{\xi_{0}=s_0\}\cap\cdots\cap\{\xi_n=s_n\}$ , $B=\{\xi_n=s_n\}$ . I found that $\int_A\zeta_0dP=\int_A\zeta_1dP=P(\xi_0=s_0,\ldots,\xi_{n+1}=s_{n+1})$ and $\int_B\zeta_0dP=\int_B\zeta_1dP=P(\xi_n=s_n,\xi_{n+1}=s_{n+1})$ . But I don't know how to continue or if this even helps.","['conditional-expectation', 'stochastic-processes', 'probability-theory', 'markov-chains']"
2962753,The cyclic quadrilateral and the slopes of its sides,"Suppose a plane quadrilateral ABCD (convex, concave or crossed) no side of which is parallel to y-axis, and let $m_1, m_2, m_3, m_4$ be the slopes of the equations of  sides AB, BC, CD, DA. Having made these definitions, now we may state the following theorem: ABCD is a cyclic quadrilateral iff $$(m_1m_3-1)(m_2+m_4)=(m_2m_4-1)(m_1+m_3)$$ I can prove this theorem using the theory of circumscribing conics, but I would appreciate if someone could show me a different proof. Proof based on the theory of conics: Let $L_1\equiv m_1x -y +r_1=0$ , $L_2\equiv m_2x -y +r_2=0$ , $L_3\equiv m_3x -y +r_3=0$ , $L_4\equiv m_4x -y +r_4=0$ be the equations of lines $AB$ , $BC$ , $CD$ , $DA$ . Then all the conics which circumscribe the quadrilateral ABCD can be given by the equation $\lambda L_1L_3+\mu L_2L_4=0$ . Therefore all the conics circumscribing the quadrilateral are given by the equation $$\lambda(m_1x -y +r_1)(m_3x -y +r_3)+\mu(m_2x -y +r_2)(m_4x -y +r_4)=0,$$ $$(m_1m_3\lambda +m_2m_4\mu)x^2-((m_1+m_3)\lambda+(m_2+m_4)\mu)xy+(\lambda+\mu)y^2+...=0$$ If ABCD is a cyclic quadrilateral, then there is a circle circumscribing it, represented by an equation whose coefficients of $x^2$ and $y^2$ are equal and whose coefficient of $xy$ vanishes. Therefore $$\begin {cases}
(m_1m_3-1)\lambda +(m_2m_4-1)\mu=0\\
(m_1+m_3)\lambda +(m_2+m_4)\mu=0 \\
\end {cases}
$$ As this system has to have a solution distinct from the trivial one $(0,0)$ , $$\begin{vmatrix} (m_1m_3-1) & (m_2m_4-1)\\ (m_1+m_3) & (m_2+m_4))\\\end{vmatrix}=0, $$ $$(m_1m_3-1)(m_2+m_4)=(m_2m_4-1)(m_1+m_3),$$ QED. Conversely, if $$(m_1m_3-1)(m_2+m_4)=(m_2m_4-1)(m_1+m_3),$$ then the system above has a solution $(\lambda,\mu)$ distinct  from the trivial one $(0,0)$ . Therefore there is an ordered pair $(\lambda,\mu)\neq (0,0)$ which renders the following equation of a conic circumscribing the quadrilateral ABCD: $$(\lambda +\mu)x^2 +0.xy+(\lambda +\mu)y^2+...=0$$ As $(\lambda +\mu)^2>0$ , the conic given by this equation must be an ellipse (and a real one and non degenerate, because this conic passes through four real points), and, more precisely, a circle, because of the equal coefficients of $x^2$ and $y^2$ , hence ABCD is a cyclic quadrilateral, QED. Note: $(\lambda +\mu)^2\neq0$ for two reasons (one algebric, the other geometric). First, because if $(\lambda +\mu)^2=0$ , then $(\lambda +\mu)=0$ , then $\lambda=-\mu$ , then $m_1m_3=m_2m_4$ and $m_1+m_3=m_2+m_4$ , then $m_1=m_2$ and $m_3=m_4$ (absurd!), or $m_1=m_4$ and $m_3=m_2$ (absurd!).
Second, because if $(\lambda +\mu)^2=0$ , then $(\lambda +\mu)=0$ , then the equation wouldn´t have second degree terms, degrading to an equation of a straight line passing through four non collinear points (absurd!) Is anyone acquainted with another proof?","['analytic-geometry', 'quadrilateral', 'conic-sections', 'geometry', 'alternative-proof']"
2962773,"I found another pattern in the Fibonacci sequence, can anyone explain why?","I think I stumbled on something and I just wanted to ask why this pattern occurs. I’m not a mathematician I put the sequence in excel (if anyone wants it here’s a link https://oeis.org/A000045/b000045.txt ) and from 20th number onwards a pattern emerges with the largest digit. 6765 so the 6 10946 the 1 17711 the 1 28657 the 2 46368 the 4 75025 the 7 121393 the 1 196418 the 1 317811 the 3 514229 th 5 832040 the 8 1346269 the 1 2178309 the 2 3524578 the 3 5702887 the 5 9227465 the 9 14930352 the 1 24157817 the 2 39088169 the 3 The number 6 then begins again on the 39th number, , and the 40th begins with 1 , and the 41st begins with 1 , and the 42nd begins with 2 , and the 43rd begins with 4 , and the 44th begins with 7 , and the 45th begins with 1 so again the 6112471.... begins to appear. I thought I'd write out everytime I see this 6 in the sequence begins again. List of line where the 6 sequence repeats. 20 (as shown with first example), 39, 63, 87, 106, 130, 154, 173, 197, 221, 240, 264, 288, 307, 331, 355, 374, 398, 422, 441, 465, 484, 508 ,532 , 551, 575, 599, 618, 642, 666, 685, 709, 733, 752, 776, 800, 819, 843, 867, 886, 910, 934, 953, 977, 996, 1020, 1044, 1063, 1087, 1111, 1130, 1154, 1178, 1197, 1221, 1245, 1264, 1288, 1312, 1331, 1355, 1379, 1398, 1422, 1441, 1465 As you will see below this pattern repeats (0r the 6 1 1 2 4 7 1) begins again every 19, 24, 24 times respectively 6 times. 39 - 20 = 19 63 - 39 = 24 87 - 63 = 24 106 - 87 = 19 130 - 106 = 24 154 - 130 = 24 173 - 154 = 19 197 - 173 = 24 221 - 197 = 24 240 - 221 = 19 264 - 240 = 24 288 - 264 = 24 307 - 288 = 19 331 - 307 = 24 355 - 331 = 24 374 - 355 = 19 398 - 374 = 24 422 - 398 = 24 441 - 422 = 19 465 - 441 = 24 484 - 465 = 19 508 - 484 = 24 532 - 508 = 24 551 - 532 = 19 575 - 551 = 24 599 - 575 = 24 618 - 599 = 19 642 - 618 = 24 666 - 642 = 24 685 - 666 = 19 709 - 685 = 24 733 - 709 = 24 752 - 733 = 19 776 - 752 = 24 800 - 776 = 24 819 - 800 = 19 843 - 819 = 24 867 - 843 = 24 886 - 867 = 19 910 - 886 = 24 934 - 910 = 24 953 - 934 = 19 977 - 953 = 24 996 - 977 = 19 1020 - 996 = 24 1044 - 1020 = 24 1063 - 1044 = 19 1087 - 1063 = 24 1111 - 1087 = 24 1130 - 1111 = 19 1154 - 1130 = 24 1178 - 1154 = 24 1197 - 1178 = 19 1221 - 1197 = 24 1245 - 1221 = 24 1264 - 1245 = 19 1288 - 1264 = 24 1312 - 1288 = 24 1331 - 1312 = 19 1355 - 1331 = 24 1379 - 1355 = 24 1398 - 1379 = 19 1422 - 1398 = 24 1441 - 1422 = 19 1465 - 1441 = 24 (excel reached limit after this, could not continue)
The 6 sequence repeats 19, 24, 24 interval 6 times respectively for 19 cosecutive times. Does anyone have an explanation? Also I know numbers shouldn’t be looked at from it’s largest number alone but as a whole, but it’s just something I noticed Thanks in advance","['integers', 'sequences-and-series']"
2962774,Combinatorics - Sequences with repetition and restrictions,"Question: How many sequences of five elements with repetition allowed can be created from elements of the set $\{1,2,3,4,5,6\}$ in which the last digit is equal to any of the previous digits? My Answer: Let's use the inclusion-exclusion principle. First we need to know the number of all sequences, that is: $6^5$ since we're allowing repetition. Then let's define the sets for $k\in\{1,2,3,4,5,6\}$ : $$
C_k = \{ \text{sequences with last digit equal k and previous digits different than k}\}
$$ Clearly the sets $C_k$ are disjoint, since there is no sequence equal to another one with the last digits being different. Therefore, the answer is: $$
6^5 - |C_1\cup C_2 \cup C_3 \cup C_4 \cup C_5 \cup C_6| = 6^5 - 6\cdot 5^4
$$ Is my answer correct? Any help is highly appreciated! Thanks!","['proof-verification', 'combinatorics']"
2962817,Gradient of a complex valued matrix function but with real domain,"Let $f: \mathbb{C}^{N\times M}\rightarrow \mathbb{R}$ and $g: \mathbb{R}^{N\times M}\rightarrow \mathbb{C}^{N \times M}, N\geq M $ and $F = f \circ g$ . I am trying to compute the gradient of $F$ w.r.t. $\mathbf{X} \in \mathbb{R}^{N\times M}$ , i.e., $\nabla_\mathbf{X} f(g(\mathbf{X}))$ but I am struggling with the chain rule because of the complex domain. What is the dimension of the final gradient matrix? As an example, I have: $g(\mathbf{X})=e^{i\mathbf{X}}$ and $f(\mathbf{Y})=|| \mathbf{A}-\mathbf{YB}||_F^2$ ( $\mathbf{A}$ and $\mathbf{B}$ complex as well). Thank you in advance.","['complex-analysis', 'matrix-calculus', 'linear-algebra']"
2962834,How to find the final angle of a car after steering?,"I'm doing a car simulator. The car makes a turn with maximum steering 30 degrees . With a distance of 4 meters between the two axes of the car, its
turning radius is $\frac{4}{\tan 30}$ = 6.93 meters . The total steering time (from 0 to 30 degrees or from 30 than 0
degrees is 2 seconds ). If the car moves at 10 m/s , its angular velocity will be $\frac{10}{6.93}$ = 1.443 radians (or 82.68 degrees per second ) My question is: knowing that the car takes 2 seconds to steer from 30 to 0 degrees, and that it is on a steady curve at maximum steering (30 degrees), at a speed of 10 m/s, if the car starts to steer back when it reaches the zero degree of the curve , what will be the final degree of the car when its wheel is fully aligned with the car (0 degrees)? This might seem simple, but the problem is that as the car decreases the steering, the angular velocity will also shifting and changing everything else. In the animation below, the car has the same specifications above. The steering is at 30 degrees and when the car arrives from the bottom of the curve (0 degrees), it reverses the steering from 30 to 0 degree in 2 seconds. The final angle of the car is 78.54 degrees.",['geometry']
2962836,Understanding a Proof in Topology of the Reals,"Good evening fellow math-friends (or morning, depending on where you are), I am having trouble understanding a proof in the topology of the reals, i.e. a subset F of the reals is closed if and only if the limit of every convergent sequence in F belongs to F. In particular, I was trying to prove that ""if the limit of every convergent sequence in F belongs to F, then F is closed. I was trying to do a proof by contradiction, and then for some help I looked at the proof here (under proposition 5.18): https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/ch5.pdf I don't really understand why they say to assume $x \in F^c$ , and $x$ has to have a neighbourhood belonging to $F^c$ otherwise $\forall n \in N, \exists x_{n} \in F$ such that $x_{n} \in (x - \frac{1}{n}, x + \frac{1}{n})$ , so $x = \lim x_{n}$ and $x$ is the limit of a sequence in $F$ . I don't really follow through with the ""otherwise"" bit or see how it is a contradiction, may someone clarify this or further explain it to me? Thank you in advance.","['general-topology', 'sequences-and-series', 'real-analysis']"
2962837,Use of Mergelyan's theorem to approximate a function that is nearly vanishing on unit circle,"Let $K$ be a proper compact subset of the unit circle in $\mathbb{C}$ . Prove that for any $\epsilon > 0$ , there exists a polynomial $p(z)$ such that $$|p(z)| \leq \epsilon$$ for $z \in K$ and $p(0)=1$ . My thoughts: Mergelyan's Theorem came to mind, but I could not maintain the condition $p(0)=1$ . That is, even if I could construct a function $f(z)$ continuous on $K$ such that $|f(z)|<\epsilon/2$ (for instance) whenever $z \in K$ , there is no guarantee that the polynomial approximation $p(z)$ to $f(z)$ has to satisfy $p(0)=1$ . How to maintain this condition when Mergelyan is invoked?",['complex-analysis']
2962862,"Free,Undamped Mechanical Vibrations","In the case of free, undamped vibrations,the differential equation is $mu''+ku=0$ and solution to this differential equation is \begin{align} \tag{1}
u(t)=c_1\cos{(\omega_0 t)}+c_2\sin{(\omega_0 t)}
\end{align} Now this has been written by the author in the following form, \begin{align} \tag{2}
u(t)=R\cos{(\omega_0 t-\delta)}
\end{align} where he says R is the amplitude of displacement $u(t)$ and $\delta$ is the phase shift or phase angle of displacement $u(t)$ Now why did he assume that both the equations $(1)$ and $(2)$ are equivalent and on what ground?","['trigonometry', 'homogeneous-equation', 'ordinary-differential-equations']"
2962896,Confusion with the formula for harmonic conjugate,"According to here , the harmonic conjugate of a harmonic function $u$ is given by $$v(z)=\int_{z_0}^z u_xdy-u_ydx+C$$ where $C$ is a constant, while in here , the harmonic conjugate is given by $$v(z)=\int u_xdy-\int u_ydx-\iint u_{xx}dxdy$$ where the integral is an indefinite integral. The last term above is no way a constant (since it is an indefinite integral). I am wondering whether there is any relation between the line integral and the indefinite integral. In conclusion, my question is: why are the above two formulas equivalent?","['complex-analysis', 'harmonic-functions']"
2962904,Solution of differential equation $\frac{dy}{dx}+Py=Q$,"If $\ y_1$ and $\ y_2$ be the solution of the differential equation $\frac{dy}{dx}+Py=Q$ where P and Q are the function of x alone and $\ y_2=z\ y_1$ , then prove that $z=1+ae^{{-\int\frac{Qdx}{y}}}$ , where a is being an arbitrary constant. My approach is as follow
Step 1: $\ y_2-\ y_1=z(\ y_1-1)$ Step 2: $\frac{dy_1}{dx}+Py_1=Q$ & $\frac{dy_2}{dx}+Py_2=Q$ Step 3: $\frac{dy_2}{dx}-\frac{dy_1}{dx}=z\frac{dy_1}{dx}$ Step 4: $\frac{dy_2}{dx}-\frac{dy_1}{dx}+P(\ y_2-\ y_1)=0$ or $\frac{dy_2}{dx}-\frac{dy_1}{dx}+Pz(\ y_1-1)=0$ Step 5: $z\frac{dy_1}{dx}+Pz(\ y_1-1)=0$ AFter this step I am confused",['ordinary-differential-equations']
2962920,Problem understanding a solenoidal vector field that is not a curl.,"Problem In Apostal's calculus volume 2 , there is an example which shows that a solenoidal vector field that is not a curl. Example states that proof is difficult at this stage . Can anyone please me some understanding why this can happen. That is on what kind of open sets a solenoidal vector field is always a curl of some other vector field in that set? NB-Currently a sophomore .","['multivariable-calculus', 'calculus']"
2962937,"Given 10 digits, how many ways can they be arranged so that two odds cannot be adjacent?","Given $10$ digits, where each digit can be an integer from $0$ to $9$ , how can I determine the number of ways to arrange the numbers so that two odds are not adjacent? Repetition of digits is not allowed. So far, I have figured out the total number of possibilities: $$10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2 \cdot 1 = 10!$$ Then I had planned to subtract the number of bad possibilities from the total number of possibilities. $$10! - X$$ Where $X$ is all the bad possibilities, which means $X$ is all the possibilities where two odds could be next to each other in the $10$ digits. I know that for each number, $5$ odds can be selected, how can I use this information to figure out the answer to the question?",['combinatorics']
2962990,What is the number of arrangements of all the seven letters of the word “EXAMPLE” in which the vowels are all separated?,What is the number of arrangements of all the seven letters of the word $EXAMPLE$ in which the vowels are all separated? I know that $2520$ is the number of arrangements if there are no restrictions. But what I am asking is how many arrangements are there when no two vowels are next to each other. Thanks.,"['permutations', 'combinations', 'discrete-mathematics']"
2963026,Bifurcation Diagram-confusion,"I am a bit confused about how the Bifurcation Diagram of a parametric autonomous system $x'=f(x,μ)$ is defined. For the one dimentional case, I think it is more obvious to me, but still not clear enough: 
For example, if $$x'=μ-x^2$$ then the equilibria are: For $μ=0$ is only the $0$ which is unstable For $ 0\ltμ$ there are two equilibria $\sqrt{μ}, - \sqrt{μ} $ with the first one unstable and the second stable. For $ μ\lt0$ there are no equilibria. Now, should the bifurcation diagram be the graph of the functions? $x=0$ , $x=\sqrt{μ}$ , $x= - \sqrt{μ} $ ? dotted where the $μ$ gives unstable equilibria? In my book I have a diagram like the following: Furthermore, what the bifurcation diagram should look like if the system $x'=f(x,μ)$ is planar? My confusion here is what the axis $x$ then should represent...
Thanks.","['ordinary-differential-equations', 'dynamical-systems']"
2963030,Evaluate $\int_0^1 \log^n(x^a)\log^m(1-x^{\color{red}{\alpha}})x^b(1-x^{\color{red}{\beta}})^t\mathrm dx$ with $\alpha\ne\beta$,"While dealing with algebraic integrals composited of logarithms and polynomials, I learned about using the derivatives of the Beta Function in order to evaluate them. Applying this knowledge I was able to show that $$\int_0^1 \log^n(x^a)\log^m(1-x^b)x^c(1-x^b)^t\mathrm dx~=~\frac{a^n}{b^{n+1}}\left.\frac{\partial^{n+m}}{\partial x^n\partial y^m}B(x,y)\right|^{x=\frac{c+1}{b}}_{y=t+1}\tag1$$ Which is not difficult at all since it is just the application of the substitution $u=x^b$ . The explicit evaluation gets more and more complicated while $n$ and $m$ are increasing. Furthermore I can only apply this formula for the case $n,m\in\mathbb{N}$ . Anyway now I thought about the following integral $$\int_0^1 \log^n(x^a)\log^m(1-x^{\color{red}{\alpha}})x^b(1-x^{\color{red}{\beta}})^t~dx\tag2$$ With $\alpha\ne\beta$ . For this case the simple substitution $u=x^{\alpha}$ or $u=x^{\beta}$ , respectively, does not work out since it produces a term of the type $(1-x^{\alpha/\beta})$ which does not fit within the integral representation of the Beta Function and its derivatives. Therefore, I think it is maybe not the right approach at all but I could not figure out a different way to get started with $(2)$ . IBP or a different subsititution seem pointless to me as well as a series expansion of the logarithm because of the powers $m$ and $n$ respectively. How can one tackle $(2)$ ? I would be interested in a general formula similar to $(1)$ if possible. I would be glad about an attempt concerning small values of $n$ and $m$ . My priority lies within the evaluation of the composition of $\log(1-x^{\alpha})$ and $(1-x^{\beta})^t$ therefore the powers of the logarithm are of minor matter. Thanks in advance!","['integration', 'definite-integrals', 'logarithms', 'closed-form', 'beta-function']"
2963036,"A relation concerning the ""sum of squares"" counting function $r_2(n)$","Let $r_2(n)$ denote the number of ways in which a positive integer $n$ can be expressed as the sum of squares of two integers. Here the sign as well as order of summands matters. Also by convention we set $r_2(0)=1$ . G. H. Hardy mentions the following formula in his book Ramanujan : Twelve Lectures on Subjects Suggested by His Life and Work (see page $82$ ) $$\sum_{0\leq n<x} \frac{r_2(n)}{\sqrt{x-n}}=2\pi\sqrt {x} +\sum_{n=1}^{\infty} \frac{r_2(n)}{\sqrt{n}}\sin 2\pi\sqrt{nx} \tag{1}$$ This is preceded by mention of another formula of Ramanujan $$\sum_{n = 0}^{\infty}\frac{r_{2}(n)}{\sqrt{n + a}}e^{-2\pi\sqrt{(n + a)b}} = \sum_{n = 0}^{\infty}\frac{r_{2}(n)}{\sqrt{n + b}}e^{-2\pi\sqrt{(n + b)a}}\tag{2}$$ which is proved here . Next Hardy says that the above formula of Ramanujan is valid when $\sqrt{a}, \sqrt{b} $ have positive real parts. Putting $a=xe^{it} $ for $x>0, x\notin\mathbb{Z} ,0<t<\pi$ in $(2)$ and letting $t\to\pi$ followed by equating imaginary parts and setting $b=0$ the relation $(1)$ is obtained. And then comes the remark ""this deduction, of course, is not a proof of $(1)$ and I do not know that there is any proof standing in the literature"". Has a proof of $(1)$ been found since? If so a reference would be greatly appreciated. Can the deduction mentioned above be fixed by making some modification? Any other approaches to prove $(1)$ are also welcome.","['number-theory', 'theta-functions', 'reference-request', 'sequences-and-series']"
2963044,How to find a function that is defined by its integral and its derivative?,"I was trying to solve a physics problem. The question is: ""How long will it take for a boat to cross a river if its velocity is always directed towards a fixed point on the opposite side of the river bank, assuming the velocity of the boat is greater than the water's velocity?"".
I came to the conclusion that it would be easier to describe the boat's movement if we took the point towards which the boat is directed as moving at a constant velocity equal to the current's velocity.
Thinking about the problem this way, the function describing the boat's $y$ position would satisfy: $$f'\left(x\right)=\left(\frac{1}{d-x}\right)\ast\left(\frac{v_{river}}{v_{boat}}\ast\int_{0}^{x}{f\left(x\right)\ dx}-f(x)\right)$$ Where $d$ is the width of the river.
We also know that $f(0) = 0$ , $f'(0)=0$ , and $F(0)=0$ .
Is there any way to deduce $f(x)$ knowing this information? Or is there a simpler way to think of the problem in the first place?","['integration', 'functions', 'physics', 'derivatives', 'mathematical-physics']"
2963069,Integers which are squared norm of 2 by 2 integer matrices,"Question : Which integers are of the form $\Vert A \Vert^2$ , with $A \in M_2(\mathbb{Z})$ . The code below provides the first such integers: $0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26$ . By searching this sequence on OEIS, we find: ""Numbers that are the sum of 2 squares"" A001481 . Are these integers exactly those which are the sum of two squares ? Research First, $\Vert A  \Vert^2$ is the largest eigenvalue of $A^*A$ , so for $A = \left( \begin{matrix} a & b \cr c & d \end{matrix} \right)$ and $a,b,c,d \in \mathbb{Z}$ , so we get: $$\Vert A  \Vert^2 = \frac{1}{2} \left(a^2+b^2+c^2+d^2+\sqrt{(a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2}\right)$$ Obviously, every sum of two squares is of the expected form , because by taking $c=d=0$ , we get $\Vert A \Vert^2=a^2+b^2$ . Then it remains to prove that there is no other integer (if true). Now, recall that: Sum of two square theorem An integer greater than one can be
  written as a sum of two squares if and only if its prime decomposition
  contains no prime congruent to 3 (mod 4) raised to an odd power. By taking $c=ra$ and $d=rb$ , we get that $\Vert A  \Vert^2 = (r^2+1)(a^2+b^2)$ , which is also a sum of two square because the following equation occurs ( proof here ): $$r^2 \not \equiv -1 \mod 4s+3$$ A necessary condition for $\Vert A  \Vert^2$ to be an integer, is that $(a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2$ must be a square $X^2$ , so that $(X,2(ad-bc),a^2+b^2+c^2+d^2)$ is a Pythagorean triple , so must be of the form $(k(m^2-n^2),2kmn,k(m^2+n^2)$ , and then $\Vert A  \Vert^2 = km^2$ . So it remains to prove that $k$ must be a sum of two squares. sage: L=[]
....: for a in range(-6,6):
....:     for b in range(-6,6):
....:         for c in range(-6,6):
....:             for d in range(-6,6):
....:                 n=numerical_approx(matrix([[a,b],[c,d]]).norm()^2,digits=10)
....:                 if n.is_integer():
....:                     L.append(int(n))
....: l=list(set(L))
....: l.sort()
....: l[:20]
....:
[0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26, 29, 32, 34, 36, 37]","['matrices', 'elementary-number-theory', 'linear-algebra']"
2963178,Can't we compute $\lim_{n \to \infty}\frac{1+ \cdots +n}{n^2}=\lim_{n\to\infty}\frac{1}{n^2}+\cdots +\lim_{n\to\infty}\frac{n}{n^2}=0$?,"I just learned the definition of limits, and I learned that if $\{a_n\}, \{b_n\} $ converges, then $$\lim_{n\to \infty} (a_n+b_n)=\lim_{n \to \infty} a_n+\lim_{n \to \infty}b_n$$ holds. And my teacher said that $\lim_{n \to \infty}\frac{1+2+3+ \cdots +n}{n^2}=\lim_{n\to \infty}\frac{\frac{n(n+1)}{2}}{n^2}=\frac{1}{2}$ . But can't we compute like $$\lim_{n \to \infty}\frac{1+2+3+ \cdots +n}{n^2}=\lim_{n\to\infty}\frac{1}{n^2}+\lim_{n\to\infty}\frac{2}{n^2}+\lim_{n\to\infty}\frac{3}{n^2}+\cdots +\lim_{n\to\infty}\frac{n}{n^2}=0+0+\cdots+0=0$$ ?","['limits', 'calculus']"
2963241,How to see the real projective plane is a Möbius band glued to a disk?,"I am seeking an easily comprehended, convincing explanation 
that ${RP}^2$ is topologically the same as gluing the circle
boundary of a disk to the edge of a Möbius band.","['geometric-topology', 'general-topology']"
2963243,Induction on Fibonacci numbers,"For a homework problem, I need to prove $f_0f_1+f_1f_2+...+f_{2n-1}f_{2n}=f_{2n}^2$ for $n\geq1$ with induction. So far, using my basis step, I have $$\sum_{i=1}^{k+1} f_{2(k+1)-1}f_{2(k+1)}=$$ $$\left(\sum_{i=1}^{k}f_{2k-1}f_{2k}\right)+f_{2k+1}f_{2k+2}=$$ $$f_{2k}^2+f_{2k+1}f_{2k+2}$$ However, I'm stuck here. I've tried breaking down the terms into their recursive terms and FOIL-ing/factoring, etc, and I can't seem to get to the end goal of $f_{2k+2}^2$ . I might be missing something obvious... Any help in the right direction would be greatly appreciated.","['fibonacci-numbers', 'induction', 'proof-writing', 'discrete-mathematics']"
2963251,The set of points reached exactly $n$ times is measurable,"Let $p:X \to Y$ be a measurable surjection and assume that for each $y \in Y$ the set $p^{-1}(y)$ is at most countable. Define $Y_n$ (for $n \in \mathbb{N} \cup \{ \infty \}$ ) to be the set of those $y \in Y$ for which there are exactly $n$ distinct $x \in X$ such that $p(x)=y$ . Is it clear that each $Y_n$ is measurable? EDIT: I forgot to add, $X,Y$ are assumed to be standard Borel spaces: i.e. sigma algebras are the sigma algebras of Borel set and $X$ and $Y$ are assumed to be complete, separable metric spaces","['measurable-functions', 'measure-theory', 'descriptive-set-theory', 'analysis']"
2963260,Signed measure of uncountable set,"I have a question and hope some of you can help me :) Consider a signed measure $\nu$ on $(\Omega, \bf{A})$ and let be $P_i \in \bf{A}$ positive sets, such that $ \forall B \subset P_i: \nu(B) \geq0 $ . Then we state that $\bigcup_{i \in I} P_i$ is also a positive set. If $I$ is countable ( $|I| \leq |\mathbb{N}|$ ) this seems to be true.
To show this I considered an arbitrary $M \in \bigcup_{i \in I} P_i $ . I know that there exists $M_i := M \cap P_i \subset P_i$ . So we get $\nu(M) = \nu \left( \bigcup_{i \in I}M_i  \right) = \sum_{i \in I} \nu(M_i) \geq 0$ . The last $=$ follows due to sigma-additivity of $\nu$ and the $\geq$ follows due to $M_i \subset P_i$ and $P_i$ is positive. Now I wonder, what happens, if $I$ is uncountable ( $|I| > |\mathbb{N}|$ ). I think there must be a counterexample, because the sigma-additivity doesn't hold anymore, but I can't find one. Does anybody have a counterexample for me, or is my assumption wrong, and one can proof, that the statement is still true for overcountable I's. Thanks a lot!","['measure-theory', 'signed-measures']"
2963264,Notation - Functions in $\mathbb{R}^n$,"In an example I am given that following function $F: \mathbb{R}^N \rightarrow \mathbb{R}^N$ is given by \begin{equation}
F(x_1, x_2, ... , x_N)= \bigg( \sum\limits_{i=1}^{N} x_i^2 \bigg)(x_1,x_2,...,x_N)
\end{equation} I am a bit confused with the notation. If I expand the function is it given by the following: $F(x_1, x_2, ... , x_N)= x_1^2+x_2^2+...+x_N^2$ ?
I am given that it is a function from $\mathbb{R}^N$ to $\mathbb{R}^N$ so I think that is wrong but don't understand how the term $(x_1,x_2,...,x_N)$ at the end works. Could it be that: \begin{equation}
F(x_1, x_2, ... , x_N)=
\begin{pmatrix}
  x_1^2+x_2^2+...+x_N^2 \\
  x_1^2+x_2^2+...+x_N^2 \\
  \vdots \\
  x_1^2+x_2^2+...+x_N^2 \\
\end{pmatrix}
\end{equation}","['notation', 'multivariable-calculus']"
2963287,Proving that $\sum_{n=1}^\infty \frac{\sin\left(n\frac{\pi}{3}\right)}{(2n+1)^2}=\frac{G}{\sqrt 3} -\frac{\pi^2}{24}$,"Trying to show using a different approach that $\int_0^1 \frac{\sqrt x \ln x}{x^2-x+1}dx =\frac{\pi^2\sqrt 3}{9}-\frac{8}{3}G\, $ I have stumbled upon this series: $$\sum_{n=1}^\infty \frac{\sin\left(n\frac{\pi}{3}\right)}{(2n+1)^2}$$ The linked proof relies upon this trigamma identity . Now by rewriting the integral as: $$I=\int_0^1 \frac{\sqrt{x}\ln x}{x^2-2\cos\left(\frac{\pi}{3}\right)x+1}dx$$ And using that: $$\frac{\sin t}{x^2-2x\cos t+1}=\frac{1}{2i}\left(\frac{e^{it}}{1-xe^{it}}-\frac{e^{-it}}{1-xe^{-it}}\right)=\Im \left(\frac{e^{it}}{1-xe^{it}}\right)=$$ $$=\sum_{n=0}^{\infty} \Im\left(x^n e^{i(n+1)t}\right)=\sum_{n=0}^\infty x^n\sin((n+1)t)$$ $$I=\frac{1}{\sin \left(\frac{\pi}{3}\right)}\sum_{n=0}^\infty  \sin\left(\frac{\pi}{3} (n+1) \right)\int_0^1 x^{n+1/2} \ln x dx$$ $$\text{Since} \  \int_0^1 x^p \ln x dx= -\frac{1}{(p+1)^2}$$ $$I=-\frac{2}{\sqrt 3} \sum_{n=0}^\infty \frac{\sin\left((n+1)\frac{\pi}{3}\right)}{(n+1+1/2)^2}=-\frac{8}{\sqrt 3}\sum_{n=1}^\infty \frac{\sin\left(n\frac{\pi}{3}\right)}{(2n+1)^2} $$ And well by using the previous link we can deduce that the series equals to $\frac{G}{\sqrt 3} -\frac{\pi^2}{24}$ , where $G$ is Catalan's constant. I thought this might be a coefficient of some Fourier series, or taking the imaginary part of $\left(\sum_{n=1}^\infty \frac{e^{i\frac{n\pi}{3}}}{(2n+1)^2}\right)$ , but I was not that lucky afterwards. Is there a way to show the result without relying on that trigamma identity? Another approach to the integral would of course be enough.","['integration', 'summation', 'closed-form']"
2963320,I wake up in a random class and hear 6 biology-related words. How certain should I be that I'm in Biology class?,"Suppose I'm sleeping in some class. I wake up and I hear 6 topic-specific words that seem related to biology. I'm asked to guess whether I'm in Biology class? How confident should I be? I think this can be presented with the following Bayesian network, with one parent node and 6 children nodes. Suppose that $$P(word_1|biology)=0.6$$ $$P(word_2|biology)=0.6$$ $$P(word_3|biology)=0.7$$ $$P(word_4|biology)=0.7$$ $$P(word_5|biology)=0.8$$ $$P(word_6|biology)=0.8$$ Suppose that I think there's some chance I could hear these words in some other class, such as chemistry. Hence, let $P(word_i|\neg biology)$ be $P(word_i|biology)-0.1$ : $$P(word_1|\neg biology)=0.5$$ $$P(word_2|\neg biology)=0.5$$ $$P(word_3|\neg biology)=0.6$$ $$P(word_4|biology)=0.6$$ $$P(word_5|\neg biology)=0.7$$ $$P(word_6|\neg biology)=0.7$$ My prior credence of being in biology class is $0.1$ . How do I update to form a posterior after hearing these 6 words? Upon hearing word 1, using Bayes rule I update as follows: $$P(class=bio|word_1)=\frac{p(word_1|bio)*p(bio)}{p(word_1|bio)*p(bio)+p(word_1|\neg bio)*p(\neg bio)}=\frac{0.6*0.1}{(0.1*0.6)+(0.5*0.9)} \approx 0.1176$$ Do I keep updating like this sequentially for each word, plugging in the previous posterior as the next prior? Such as, $$P(class=bio|word_2)=\frac{p(word_2|bio)*p(bio)}{p(word_2|bio)*p(bio)+p(word_2|\neg bio)*p(\neg bio)}=\frac{0.6*0.1176}{(0.1176*0.6)+(0.5*0.8824)} \approx 0.1378$$ And so on... Is that correct?","['bayes-theorem', 'bayesian-network', 'bayesian', 'probability']"
2963367,"Construction of an Open, Dense, Connected Set in the Plane","I'm stumped with the following problem. Let $\varepsilon>0$ be given. Prove that there exists an open, dense, and connected set $G\subset \mathbb{R}^{2}$ such that $m_{2}(G)<\varepsilon$ , where $m_{2}$ is the Lebesgue measure on $\mathbb{R}^{2}$ . My thoughts: I'm thinking that I need to use some sort of construction with a Cantor-like set in $\mathbb{R}^{2}$ and then take a set complement. However, I haven't worked with Cantor sets outside of $\mathbb{R}$ , so I'm not sure what constitutes a ""Cantor-like set"" in higher dimensions (if this is even defined or a valid construction) Is this roughly what I should want to do? Otherwise, I'm not sure where I should start. Thanks in advance for any help!","['measure-theory', 'lebesgue-measure', 'real-analysis']"
2963403,Estimate on Limit of Recursive Sequence,"How can I estimate (via a lower bound) the limit of the recursive sequence $$P_{n+1}=P_n-\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})}$$ where $0<C<1$ and $1<P_0<2$ . Let $P_{\infty}=\lim_{n\to\infty}P_n$ . The best estimate I can get is $$\begin{aligned}
P_{\infty}-P_0 &=-\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{(2^n+C)(P_n+C2^{-n})} \\
&> -\sum_{n=0}^{\infty}\frac{C(P_n-1)^2}{2^nP_n} \\
&> -\frac{(P_0-1)^2}{P_0}\sum_{n=0}^{\infty}\frac{C}{2^n}
\end{aligned}$$ where from line 2 to 3 I use the fact that $(P_n)$ is decreasing. Hence $$P_{\infty}> P_0-2C\frac{(P_0-1)^2}{P_0}$$ This is a fairly decent bound when looking at the absolute difference, but for my application I need as much precision as possible so I'm hoping to find a better lower bound. Special functions are acceptable if they arise. Edit : As per Wolfram, we have that $$\begin{aligned}
&\sum_{n=0}^{\infty}\frac{C(P_0-1)^2}{(2^n+C)(P_0+C2^{-n})} \\
=&\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big)
\end{aligned}$$ which gives $$P_{\infty}>P_0-\frac{P_0-1}{\ln(2)}\Big(\psi_2(\log_2(-1/C))-\psi_2(\log_2(-P_0/C))\Big)$$","['limits', 'estimation', 'recurrence-relations', 'sequences-and-series']"
2963426,What is the expectation of $X$ given $X$,"Hi im trying to understand conditional expectation and conditional probability based on sigma algebras. Therefore an answer in that flavour would be most useful. So in a physical sense I can see what it means to condition on an event $A$ (i.e we know if this event happens or not). Then what does it mean to condition on a collection of events i.e a sigma algebra? and what is the meaning of asking for 1) $E[X|X]$ 2) to go further, $E[X|f(X)]$ p.s I know conditioning on a r.v is just conditioning of the sigma algebra generated by that r.v. So I guess im just asking what it means physically to condition on a collection of events (and why when you do so you get something random back, unless those events were just { $A,A^{c},\Omega$ ,empty set}","['conditional-expectation', 'measure-theory', 'probability-theory', 'probability']"
2963478,Characterisation of matrices whose real eigenvalues are positive,"My question is the following Is there a characterisation of $n\times n$ matrices with real entries whose real eigenvalues are positive? I am interested in this question because I am analysing some matrices where I know the determinant up to a sign, and I want to show that the sign must be positive. These matrices often have complex eigenvalues, but because the entries are real these eigenvalues will appear in conjugate pairs. So when computing the determinant as the product of the eigenvalues, these complex conjugate pairs will multiply together to make positive quantities (zero is never an eigenvalue of the matrices I am studying). So I want to show that the remaining real eigenvalues must be positive. I understand that there is a possibility that some of the real eigenvalues could be negative and the determinant would still be positive, but I suspect that they are all going to be positive. In order to see if this is true, I am looking for a characterisation of such matrices (as in my question). Any ideas or references to known results about this question are much appreciated. Edit: I found a sufficient condition (although, I'm fairly certain it is not necessary): If for each $0\leq k\leq n$ the sum of the determinants of the principal $k\times k$ minors is positive, then the matrix has no negative eigenvalues.","['matrices', 'determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
2963491,Quick way for the expected first hitting time for a 2D Brownian Motion,"Let $\{W_t\}_{t\ge 0}$ be a standard 2D Brownian motion starting at $(1,1)\in\Bbb R^2$ . What's the probability that $W_t$ hits the positive half of $x$ -axis before it hits the negative part? There are standard ""tricks"" to solve similar problems in 1D. For example, if $B_t$ is a standard 1D BM starting at 0 and we want to compute $\Bbb P(B_t\,\text{hits 3 before hitting -5})$ , then we let $\tau=\inf\{t\ge 0\mid B_t=3\vee B_t=-5\}$ and since $\tau$ is a.s. bounded we have, by the optional stopping theorem, that $$0=B_0=\Bbb E(B_\tau)=\Bbb E(B_\tau I(B_\tau = 3))+\Bbb E(B_\tau I(B_\tau = -5))=3\Bbb P(B_\tau=3)-5(1-\Bbb P(B_\tau = 3)).$$ Simple as above. We don't have to explicitly work out the distribution or expectation of $\tau$ at all. Now back to the 2D case. Do similar elegant tricks exist? If not, what'd be the quickest possible way to solve it? (For what it's worth, this problem was asked in a Morgan Stanley quant interview, so I expect there to be at least one trick that can solve it in no more than a few minutes.)","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
2963516,Solve the equation $y' + (xy)^2 = -2/(x^4)$ knowing that $y_1 = 1/(x^3)$ is the particular solution,$$y' + (xy)^2 = -\frac2{x^4}\text{ with }y_1 = \frac1{x^3}$$ I have tried using various differential equations methods to solve but it appears to be very challenging.,['ordinary-differential-equations']
2963525,"Dose the word ""fiber"" in ""fibered category"" and ""fiber bundle"" mean something similar?","For a fiber bundle, a point in the base space relates to a fiber in the total space, in some sense the total space is more complex and has more information than the base space. Recently I'm studying fibered category, it's definition is quite different from fiber bundle's and I'm still puzzled. Does the word ""fiber"" in fibered category also mean it somehow has more information than the category it fibers over?","['algebraic-geometry', 'category-theory', 'differential-geometry']"
2963557,How do we consider the 'content' of a statement in propositional logic?,"From what I understand about formal logic, we are not concerned with the statements' content or our intuitive understanding of the content for that matter, but we are concerned with the statement forms or the way that the statements relate to each other. Now please take a look at the picture taken from a book: (Discrete Maths & Its Applications by Susanna) In the highlighted portions, the author is asserting that the statements are true or false based on the content alone. She is using the content's understanding of numbers to determine the truth values. But isn't it against the principal of formal logic? I mean we can not determine the truth values or derive conclusions based on the content, right? Please explain what is happening here.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2963575,7 dancers on a circle,"7 dancers are going to participate in a contest. They are initially placed in their positions basis the initial letter of their surname. 
  At the second part of the contest, they are given random positions around the circle, which are determined by a draw. What is the probability that none of the dancers are in their initial positions or their neighboring? It seems very simple to me but obviously isn't. For each dancer, the probability is $\frac47$ so for all 7 it is $\frac{4^7}{7^7}$ – very simplistic, no?","['combinatorics', 'probability']"
2963614,"$T_{f}(a,x)$ is absolutely continuous in $[a,b]$ whenever $f$ is absolutely continuous $[a,b]$. (Proof verification)","Let $$T_{f}(a,x) = \sup \sum_{j=1}^{n}|f(t_{j}) - f(t_{j-1})|$$ be the total variation of $f$ on $[a,x]$ . I want show that: $T_{f}(a,x)$ is absolutely continuous in $[a,b]$ whenever $f$ is absolutely continuous $[a,b]$ . My attempt. We need to show that for each $\epsilon > 0$ , there is a $\delta>0$ such that: $$\sum_{1}^{m}|T_{f}(a,b_{k}) - T_{f}(a,a_{k})| < \epsilon$$ whenever $$\sum_{1}^{m}|b_{k} - a_{k}|<\delta.$$ Note that $$\sum_{1}^{m}|T_{f}(a,b_{k}) - T_{f}(a,a_{k})| = \sum_{1}^{m}T_{f}(a,b_{k}) - T_{f}(a,a_{k}) = \sum_{1}^{m}T_{f}(a_{k},b_{k}).$$ Choose $\delta$ that satisfies the absolutely continuity of $f$ on $[a,b]$ and $\displaystyle \tilde{\epsilon} < \min\left\{\frac{\epsilon}{m+1},\frac{\epsilon}{2}\right\}$ . So, $$\sum_{1}^{n}|t_{j}-t_{j-1}|<\delta$$ implies $$\sum_{1}^{n}|f(t_{j}) - f(t_{j-1})|<\tilde{\epsilon}.$$ for any partition of $[a_{k},b_{k}]$ . Therefore, $$T_{f}(a_{k},b_{k}) = \sup \sum_{1}^{n}|f(t_{j}) - f(t_{j-1})|<\frac{\epsilon}{m}.$$ Thus, $$\sum_{1}^{m}T_{f}(a_{k},b_{k}) <\epsilon.$$ That makes sense?","['measure-theory', 'absolute-continuity', 'real-analysis']"
2963656,Is the matrix $A = I - \frac{J}{n+1}$ idempotent?,"I am supposed to figure out if the following statement is false or true. If $I$ is the $n \times n$ identity matrix, and $J$ is an $n \times n$ matrix consisting entirely of ones, then the matrix $$A = I - \frac{J}{n+1}$$ is idempotent (i.e., $A^{2} = A$ ). I understand obviously what $I$ and $J$ are, my issue is with the $A = I - \frac{J}{n+1}$ . I searched my textbook and found no reference to it. What does it mean?","['matrices', 'linear-algebra', 'idempotents']"
2963687,Generalisation of the norm of bounded linear operators II,"Let $E$ be a complex Hilbert space, with inner product $\langle\cdot\;, \;\cdot\rangle$ and the norm $\|\cdot\|$ and
let $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$ . Let $M\in \mathcal{L}(E)^+$ (i.e. $\langle Mx\;, \;x\rangle \geq0,\;\forall x\in E$ ), we consider the following subspace of $\mathcal{L}(E)$ : $$\mathcal{L}_M(E)=\left\{A\in \mathcal{L}(E):\,\,\exists c>0 \quad \mbox{such that}\quad\|Ax\|_M \leq c \|x\|_M ,\;\forall x \in \overline{\mbox{Im}(M)}\right\},$$ with $\|x\|_M:=\|M^{1/2}x\|,\;\forall x \in E$ . If $A\in \mathcal{L}_M(E)$ , the $M$ -semi-norm of $A$ is defined us $$\|A\|_M:=\sup_{\substack{x\in \overline{\mbox{Im}(M)}\\ x\not=0}}\frac{\|Ax\|_M}{\|x\|_M}$$ According to this answer , for $A\in \mathcal{L}_M(E)$ , we have $$\|A\|_M=\displaystyle\sup_{\|x\|_M\leq1}\|Ax\|_M=\displaystyle\sup_{\|x\|_M=1}\|Ax\|_M.$$ Let $A\in \mathcal{L}_M(E)$ , I see in a paper that it is straightforward that $$\|A\|_M=\sup\left\{|\langle Ax, y\rangle_M|;\;x,y\in \overline{\mbox{Im}(M)} ,\;\|x\|_{M}\leq1,\|y\|_{M}\leq 1\right\},$$ where $\langle Ax, y\rangle_M=\langle MAx, y\rangle.$ How can I prove this result? Thank you everyone !!!","['operator-theory', 'functional-analysis']"
2963721,Example of an ODE with initial conditions,"I was given the following example as an introduction towards ODEs. But I have an issue with it: Let $$-u'' + pu' + qu = 0,\ p,q \in \mathbb{R} \\
u(t_0) = u_0 \\
u'(t_0) = v_0, \\ u_0,v_0\in \mathbb{R}
$$ If we say, that $$u = e^{\lambda(t-t_0)} u_0 
$$ We fulfill the first initial condition. Then we can write the equation as $$(-\lambda^2 +p\lambda + q )u = 0$$ If we solve $-\lambda^2 +p\lambda + q = 0$ for $\lambda$ , we might have either a single or a two-fold zero. Which is way too restricted to let us fulfill the second initial conditon $u'(t_0)=v_0$ , because $$u'(t) = \lambda e^{\lambda (t-t_0)}u_0$$ and so $$u'(t_0) = \lambda u_0 \stackrel{!}{=} v_0 \Rightarrow \lambda = v_0 / u_0$$ But we are already forced to choose $\lambda$ from the at most two solutions to the 2nd degree polynomial.","['examples-counterexamples', 'ordinary-differential-equations']"
2963746,"Prove that the paraboloid in $R^3$, defined by $x^2 + y^2 - z^2 =a$ is a manifold if $a>0$. why does not $x^2 + y^2 -z^2 =0$ define a manifold?","Prove that the paraboloid in $R^3$ , defined by $x^2 + y^2 - z^2 =a$ is a manifold if $a>0$ . why does not $x^2 + y^2 -z^2 =0$ define a manifold? Could anyone give me a hint of the solution of this question? EDIT: I found this solution: But I need more details about why it is a manifold and why when $a = 0$ it is not, could anyone explain this to me in a better way?","['geometric-topology', 'differential-topology', 'riemannian-geometry', 'differential-geometry']"
2963754,"How to show that $\int_0^1 \sin \pi t ~ \left( \zeta (\frac12, \frac{t}{2})-\zeta (\frac12, \frac{t+1}{2}) \right) dt=1$?","I've been trying to prove Fresnel integrals by real methods and encountered  an interesting problem. Let's start with the known result: $$\int_0^\infty \sin y^2 dy = \sqrt{\frac{\pi}{8}}$$ Can we prove it without complex methods? I have tried to do the following: $$\int_0^\infty \sin y^2 dy =\frac{1}{2} \int_0^\infty \sin x \frac{dx}{\sqrt{x}} =\frac{1}{2} \sum_{n=0}^\infty (-1)^n \int_0^\pi \sin x \frac{dx}{\sqrt{x+\pi n}}$$ This directly follows from the properties of the sine, except the series only converges conditionally, not absolutely, so there's a question of if we can bring it inside the integral. I will do it without proper justification for now, but if anyone has a comment on this, I would appreciate it. So we obtain, after a simple change of variables: $$\int_0^\infty \sin y^2 dy = \frac{\sqrt{\pi}}{2} \int_0^1 \sin \pi t ~ dt \sum_{n=0}^\infty \frac{(-1)^n}{\sqrt{n+t}} $$ Wolfram gives for this series: $$\sum_{n=0}^\infty \frac{(-1)^n}{\sqrt{n+t}}=\frac{1}{\sqrt{2}}\left( \zeta \left(\frac12, \frac{t}{2}\right)-\zeta \left(\frac12, \frac{t+1}{2}\right) \right)$$ Which is easy enough to show using the definition of Hurwitz zeta function . Surprisingly enough, this brings us exactly the known value of the Fresnel integral as a coefficient: $$\int_0^\infty \sin y^2 dy =\sqrt{\frac{\pi}{8}} \int_0^1 \sin \pi t ~ \left( \zeta \left(\frac12, \frac{t}{2}\right)-\zeta \left(\frac12, \frac{t+1}{2}\right) \right) dt$$ Which means that we need to prove the identity in the title of the question: $$\int_0^1 \sin \pi t ~ \left( \zeta \left(\frac12, \frac{t}{2}\right)-\zeta \left(\frac12, \frac{t+1}{2}\right) \right) dt=1$$ Can we prove this by real methods, not using the Fresnel integral? Mathematica confirms this identity numerically.","['integration', 'zeta-functions', 'definite-integrals', 'fresnel-integrals']"
2963802,A simple proof of this equality: $\det_{\mu\nu}\left(\frac 1 2 \text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu \right]\right)=1$,"Question Let $A\in \text{SL}(2,\mathbb{C})$ , so $\det(A)=1$ . Define the following (Pauli) matrices: $$\begin{align}
\sigma_0=\begin{pmatrix}-1 & 0 \\ 0 & -1 \end{pmatrix} & &\sigma_1=\begin{pmatrix}0 & 1 \\ 1 & 0 \end{pmatrix} & \\
&\\
\sigma_2=\begin{pmatrix}0 & -i \\ i & 0 \end{pmatrix} & &\sigma_3=\begin{pmatrix}1 & 0 \\ 0 & -1 \end{pmatrix} & 
\end{align}
$$ Now define the following $4\times 4$ matrix. $$L_{\mu\nu}\equiv \frac{1}{2}\text{Tr}\left[A\sigma_\mu A^\dagger \sigma_\nu\right]$$ What I am trying to prove is $\det (L)=1$ , that's it. But I am having so much trouble. I have verified that it is true via Mathematica. The following is know exactly: $$\det (L)=\left|\det(A)\right|^4=1$$ Actually, it would be sufficient for my current purposes to show $\det(L)\geq 0$ , but even that is very hard for me to show. Mathematica Code. ClearAll[s0, s1, s2, s3, s, A]; (* Define the Pauli-Matrices and A \
matrix *)
s0 = {{-1, 0}, {0, -1}};
s1 = {{0, 1}, {1, 0}};
s2 = {{0, -I}, {I, 0}};
s3 = {{1, 0}, {0, -1}};
s = {s0, s1, s2, s3};
A = {{a, b}, {c, d}};

L00 = 1/2*
  Tr[A.s0.A\[ConjugateTranspose].s0];  (* Define L(A) through the \
equation LSubscript[(A)^\[Mu], \[Nu]] = 1/2Tr[Subscript[A\[Sigma], \
\[Mu]]A\[ConjugateTranspose]Subscript[\[Sigma], \[Nu]]] *)
L01 = 
 1/2*Tr[A.s0.A\[ConjugateTranspose].s1];
L02 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s2];
L03 = 1/2*Tr[A.s0.A\[ConjugateTranspose].s3];
L10 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s0];
L11 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s1];
L12 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s2];
L13 = 1/2*Tr[A.s1.A\[ConjugateTranspose].s3];
L20 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s0];
L21 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s1];
L22 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s2];
L23 = 1/2*Tr[A.s2.A\[ConjugateTranspose].s3];
L30 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s0];
L31 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s1];
L32 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s2];
L33 = 1/2*Tr[A.s3.A\[ConjugateTranspose].s3];
L = {
   {L00, L01, L02, L03},
   {L10, L11, L12, L13},
   {L20, L21, L22, L23},
   {L30, L31, L32, L33}
   };

TraditionalForm[
 FullSimplify[
  Det[L]]]  (* Evaluate the determinant explicitly, and put it in \
legible form *)","['physics', 'linear-algebra', 'mathematical-physics']"
2963810,Find the range of $A$ if $A=\sin^{20}x+\cos^{48}x$,"Find  the range of $A$ if $$A=\sin^{20}x+\cos^{48}x$$ $$
A'=20\sin^{19}x\cos x-48\cos^{47}x\sin x=0\implies5\sin^{19}x\cos x=12\cos^{47}x\sin x\\
\implies5\sin^{18}x=12\cos^{46}x
$$ How do I proceed further and prove that $A\in(0,1]$ ? Is it possible to find the range of $A$ without using differentiation ? Note: $\sin^2 x,\cos^2 x\in[0,1]\implies A\in[0,2]$ but $2$ is not ""the"" maximum value of $A$","['maxima-minima', 'trigonometry', 'polynomials', 'inequality']"
2963832,Symmetry of function and symmetry of level sets.,":)  I've been thinking about the following:  if $f: \mathbb{R}^n \to [0,\infty)$ is an even and continuous function, then it's super level sets $S_t= \{x : f(x)\geq t\}$ are symmetric about the origin for every $t>0$ .  This seems to be intuitively clear, but I worry that I am wrong. Any additional opinions would be appreciated!","['elementary-set-theory', 'calculus', 'real-analysis']"
2963882,$a_1^k+\cdots+a_n^k=0$ for all $1\leq k\leq n$,"Prove that for $a_i\in F$ where $F$ is an algebraically closed field (Edit:with characteristic $0$ ), say $\mathbb C$ , if $a_1^k+\cdots+a_n^k=0$ for all $1\leq k\leq n$ , then $a_1=\cdots=a_n=0$ . For now the answer I know either use Newton's identity or Vandermonde determinant, which both seem overkill. I think maybe we could argue that the equations are linearly independent so it has at most one solution, or use induction somehow. Any thoughts?","['matrices', 'abstract-algebra', 'linear-algebra']"
2963886,"What does this statement (and another) mean in discrete math? Let $P:\Bbb{Z}\times\Bbb{Z}\to\{T,F\}$, where $P(x,y)$ denotes ""$x+y=5$""","What do these statements mean in discrete mathematics? Example 1: Let $P:\mathbb{Z}\times \mathbb{Z}\to \{T,F\}$ , where $P(x,y)$ denotes "" $x+y=5$ "". Example 2: Let $B=\{T,F\}$ . Let $P(p,q,r,\ldots )$ be a proposition. Then, $$P:=(p\rightarrow q)\rightarrow r:B\times B\times B\to B$$",['discrete-mathematics']
2963892,"$C(\mathbb{R})$ is complete under $d(f,g)= \sum^{\infty}_{k=0} 2^{-k} w(\text{sup}_{[-k,k]} |f(x)-g(x)|)$?","Let $C(\mathbb{R})$ be the set of continuous functions on $\mathbb{R}$ . Let $w(t) = \frac{t}{1+t}$ for $t \geq 0$ . Define $$d(f,g)= \sum^{\infty}_{k=0} 2^{-k} w(\text{sup}_{[-k,k]} |f(x)-g(x)|).$$ Show that $C(\mathbb{R})$ is complete with this metric. I believe it comes down to constructing a limit function which converges in $\mathcal{C}(\mathbb{R})$ under the supremum norm, but since the function need not be bounded over all of $\mathbb{R}$ , I am having trouble constructing such a function","['metric-spaces', 'complete-spaces', 'functional-analysis', 'real-analysis']"
2963914,Does integrating by parts in two variables works the same as one variable?,"I will give a example that I tried to integrate by parts in $x$ : \begin{align}
\int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy
\end{align} Using the rule of integration by parts: $f= ye^x \implies f^{(n)} = ye^x$ $g'= \frac{d^n}{dx^n}x^n \implies g = x^n$ I found the $n$ th derivative and integrated $n$ times. \begin{align}
&\int_0^1 \int_0^1 ye^x \frac{d^n}{dx^n} x^n\:\: dxdy = \\
&fg\vert_0^1 - \int_0^1 \int_0^1 f'g \: \:dxdy=\\
& ye^x   x \vert_0^1 - \int_0^1 \int_0^1 ye^x x^n \:\:dxdy=\\
& y e  -  \int_0^1 \int_0^1 ye^x x^n \:\:dxdy\\
\end{align} The left side is a function of $y$ and the right side is a function of $n$ . Since $n$ a natural number we can evaluate the right side but why is the left side a function of $y$ ? Shouldn't be  the integral a function of $n$ ? Is this how we integrate by parts in two variables ?","['integration', 'multivariable-calculus']"
2963952,Independent increments of the Bernoulli claim arrival process,"Let $B_{1},...,B_{n}\sim \mathrm{Bernoulli}(p)$ be independent random variables. \begin{alignat}{1}
T_{0}&:=0,\\
T_{1}&:= \min\big\{k \geq 1 | B_{k}=1 \big\},\\
T_{2}&:=\min\big\{k > T_{1} | B_{k}=1 \big\},\\
&\vdots\\
T_{n}&:=\min\big\{k > T_{n-1} | B_{k}=1 \big\}.
\end{alignat} How to prove that increments $T_{1}-T_{0},T_{2}-T_{1},...,T_{n}-T_{n-1}$ are independent? This task was in my uni exercise sheet. To be honest I have real doubts about whether it is true because for me intuitively at least $T_{2}-T_{1}$ and $T_{1}$ have to be dependent. I don't understand how these two variables can be independent. I've been staring at this exercise for more than 2 hour and still cannot understand the trick. If you do, I would really appreciate any help. By the way, I know how to prove that $T_{n}\sim\mathrm{NBinom}(n,p)$ .  And because every increment has geometric distribution with parameter $p$ the fact that sum of increments equals $T_{n}$ must lead us to a thought that they are independent according to properties of negative binomial distribution. But I just don't get it. How can $T_1$ have no influence on $T_{2}-T_{1}$ ?","['stochastic-processes', 'probability-distributions', 'probability-theory', 'probability']"
2963972,Why is the standard deviation preferred over the mean deviation?,"To me, the mean deviation, which is the average distance that a data point in a sample lies from the sample's mean, seems a more natural measure of dispersion than the standard deviation; Yet the standard deviation seems to dominate in the field of statistics. I rarely see the mean deviation reported in studies; generally only the sample mean or median and the standard deviation are provided. Question: Why is the standard deviation preferred over the mean deviation as a measure of dispersion?",['statistics']
2963995,Are there any general guidelines for proving limits of multivariable functions?,"Today I was trying to prove that $$\lim_{(x, y) \to (0, 0)}\dfrac {x^2y^2}{x^2+y^2} = 0 $$ I got really lucky because the AM-GM inequality directly applies here to give us $$\dfrac {x^2y^2}{x^2+y^2} \le \dfrac {x^4 + y^4}{x^2+y^2} \le \dfrac {(x^2+y^2)^2}{x^2+y^2} = x^2+y^2$$ And thus we may choose $\delta = \sqrt \epsilon$ . However, this was really lucky to turn out so cleanly. My question is, when it isn't so clean, are there any general guidelines? For example: Do you look at the $|f(x, y)- L| < \epsilon$ and try to manipulate it? What exactly is the goal when you try to manipulate this? Do you look the $\sqrt {(x-a)^2 + (y-b)^2} < \delta$ term and try to manipulate it? What exactly is the goal when you try to manipulate this? Etc. Thank you.","['analysis', 'real-analysis', 'calculus', 'inequality', 'soft-question']"
2964068,Characteristic classes of exotic 4-manifolds,"Let $M,M'$ be homeomorphic smooth, closed, simply connected 4-manifolds. Is it necessarily true that $w_2(TM)=w_2(TM')$ and $p_1(TM)=p_1(TM')$ ? If so, the comment on this post , shows that $TM$ and $TM'$ are topologically isomorphic as vector bundles. If the above is false, how does the statement fail, i.e. do we have $w_2(TM)\neq w_2(TM')$ , or $p_1(TM)\neq p_1(TM')$ , or both?","['differential-topology', 'smooth-manifolds', 'algebraic-topology', 'differential-geometry']"
2964153,Magnitude of gradient is rotationally invariant,"Consider an image with edges, which we can take a gradient of (i.e. by subtracting pixels). Suppose we have a point $(x,0)$ on an edge of the image. Now suppose we rotate this point by an angle counterclockwise $\alpha$ to yield a new point $(x\cos(\alpha), x \sin(\alpha))$ . I'm trying to figure out how to prove that the magnitude of the gradients at the point before and after are equal. Here's what I've tried: We can see that the magnitude of the gradient of the point before only has a $y$ component. Thus it's magnitude is the partial of the image $\dfrac{\partial f(x,y)}{\partial y}$ . Now for the rotated image, we are looking for $\sqrt{({{\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial x}}})^2 + {\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial y}})^2}$ . We can see that the second squared term under the square root goes away by the chain rule so the expression becomes $\cos(\alpha)\sin(\alpha)\dfrac{\partial f(x,y)}{\partial x}$ ...which is clearly not what we want. Can anyone tell me what I'm doing wrong?","['multivariable-calculus', 'chain-rule']"
2964172,"find the answer in terms of $a$ and $b$ only ($a, b$ are roots of $\ x^4 + x^3 - 1 = 0$ [duplicate]","This question already has answers here : If $a$ and $b$ are two roots of $x^4 + x^3 - 1 = 0$ prove that $ab$ is a root of $x^6 + x^4 + x^3 - x^2 - 1$. (3 answers) Closed 5 years ago . If $a$ and $b$ are the two solutions of $\ x^4 + x^3 - 1 = 0$ , what is the solution of $\ x^6 + x^4 + x^3 - x^2 - 1 = 0$ ? Well I am not able to eliminate or convert $\ x^6$ . Please help.","['algebra-precalculus', 'roots', 'polynomials']"
2964175,"If $H$ and $K$ are normal subgroups of $G$ and $H\bigcap K = \{e\}$, prove that $G$ is isomorphic to a subgroup of $G/H \times G/K$","I tried proving this in the following manner, but I am not confident with these types of problems so any verification would be appreciated. Thank you. Let $A = \{(gH, gK): g \in G\}$ Define $\phi$ : $G$ $\rightarrow$ $A$ by $\phi(g)=(gH,gK)$ First we'll show $\phi$ is a homomorphism: $\phi(gg')=(gg'H,gg'H)=(gHg'H,gHg'H)=(gH,gH)(g'H,g'H)=\phi(g)\phi(g')$ Now we'll show $\phi$ is bijective, and thus an isomorphism: The codomain of $\phi$ is { $(gH,gK):g \in G$ } so $\phi$ is clearly onto. Now suppose $g_1\not= g_2$ and $\phi(g_1)=\phi(g_2)$ . Then $(g_1H,g_1K)=(g_2H,g_2K)$ $\Rightarrow$ $g_1H=g_2H$ and $g_1K=g_2K$ $\Rightarrow$ $g_2^{-1}g_1\in H$ and $g_2^{-1}g_1 \in K$ , a contradiction since we assumed $H\bigcap K = \{e\}$ $\square$","['group-isomorphism', 'proof-verification', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
2964179,Character table and conjugacy class of cyclic subgroups,"Let $G$ be a non-abelian finite group. Let $C_1$ and $C_2$ be distinct conjugacy classes in $G$ with following conditions: 1) $C_1$ and $C_2$ contain of elements of same prime order $p$ . 2) For every complex irreducible character of $G$ , its values on $C_1$ and $C_2$ are complex conjugates (in case they are real, they should be same). Can we conclude that the cyclic subgroup generated by any element of $C_1$ and one such by any element of $C_2$ are conjugate in $G$ ? (If yes, how? and if not, what is example?) In the book on Finite Simple Groups (First proceeding) an article mentions such fact concerning $G$ to be a Janko group and $C_1$ and $C_2$ certain conjugacy classes of elements of same prime order.","['group-theory', 'finite-groups', 'characters']"
2964181,Factorization of operators of second ODE,"$x^2 y'' +(2-x^3)y' - (2x+x^2)y=0$ How can I write this ODE in a factored form? I know who to factor second ODE with constant coefficient or Euler equation, but who can I factor this type?",['ordinary-differential-equations']
2964184,General solution of ODE of a constant $3\times 3$ matrix,"Determine the general solution of the system $y'=Ay$ , where $A$ is a constant matrix, defined by $$A = \begin{pmatrix}-5&-8&4\\2&3&-2\\6&14&-5\end{pmatrix}$$ After attempting to find the eigenvalues of the system, I end up with eigenvalues $\lambda=-1,-3,-3$ , where $-3$ has a multiplicity of $2$ . Then, finding the corresponding vector for $\lambda=-1$ , $$
u=
\begin{pmatrix}3\\-1\\1\end{pmatrix},
$$ for $\lambda=-3$ , $$v=
\begin{pmatrix}-2\\1\\1\end{pmatrix},
$$ and for the second $ \lambda =-3$ the vector $$w=\begin{pmatrix}-1-2t\\\frac{1}{2}+t\\t\end{pmatrix}.$$ I understand how to get the first two eigevectors $u$ and $v$ , but how did they get the third eigenvector to be in terms of $t$ ? Is it because it has a multiplicity of $2$ , so there is a second solution that can be described in terms of a variable? If so, what is the process to finding that third eigenvector? And does this process generalize for an eigenvalue that would have perhaps a multiplicity of $3$ ? The general solution of the system is $$y(t) = C_{1}e^{-t}u + C_{2}e^{-3t}v + C_{3}e^{-3t}w$$ where $u$ , $v$ , and $w$ are defined above.","['matrices', 'proof-explanation', 'ordinary-differential-equations', 'fundamental-solution']"
2964186,How to swap rows in square a matrix algebraically,"Is there some way to achieve swapping of rows of a 3x3 square matrix(for example exchanging rows 0 and 2) by using matrix algebra? Or is it something that cannot be done with algebra? What about row operations in general like multiplying a row with a constant, can it be expressed in terms of matrix algebra?","['matrices', 'linear-algebra']"
2964219,Riemann Sums of Improper Integrals on Unbounded Domain,"Assume that $f:[0,\infty)\rightarrow[0,\infty)$ is decreasing (nonincreasing), continuous, and in $L^{1}[0,\infty)$ , it is not hard to see that the following holds: \begin{align*}
\lim_{n\rightarrow\infty}\dfrac{1}{n}\sum_{i=1}^{\infty}f\left(\dfrac{i}{n}\right)=\int_{0}^{\infty}f(t)dt.
\end{align*} The problem that I have is that, if we let $\{0=x_{0}^{N}<x_{1}^{N}<\cdots\}$ be a partition of $[0,\infty)$ such that $x_{i}^{N}-x_{i-1}^{N}<1/N$ , $i=1,2,...$ , here $N=1,2,...$ is fixed and we consider the (infinite) Riemann sum $S_{N}$ at the stage $N$ of the form \begin{align*}
S_{N}=\sum_{i=1}^{\infty}f(x_{i}^{N})(x_{i}^{N}-x_{i-1}^{N}),
\end{align*} must $S_{N}\rightarrow\displaystyle\int_{0}^{\infty}f(t)dt$ as $N\rightarrow\infty$ ? Obviously $S_{N}\leq\displaystyle\int_{0}^{\infty}f(t)dt$ , but how to proceed further then? For the special case that all $x_{i}^{N}$ are equal distributed with length $1/n$ , we can easily get the lower bound as $\displaystyle\int_{1/n}^{\infty}f(t)dt$ and then Squeeze Theorem just finishes the job. But now I have no idea how to estimate if $x_{i}^{N}$ are not necessarily equal distributed. Further thoughts: Writing that \begin{align*}
S_{N}(x)=\sum_{i=1}^{\infty}f(x_{i}^{N})\chi_{[x_{i-1}^{N},x_{i}^{N})}(x),
\end{align*} we have $S_{N}(x)\leq f(x)$ and it seems that $S_{N}(x)\rightarrow f(x)$ because of the continuity, if so, then we are ready to conclude. Edit: The continuity implies the result. Now the question is, what if continuity is not given?","['measure-theory', 'analysis', 'real-analysis']"
2964220,Finding possible p's for signed measure,"Im new here at StackExchange and hope some of you can help me with a problem I'm dealing with. We are considering a signed measure $\nu$ on $(\mathbb{N},2^{\mathbb{N}})$ , which is given by $\nu(\{k\}):= k^p (-1)^k$ with $k \in \mathbb{N}$ . Now I have to find out, which values of $p \in \mathbb{R}$ are possible? I know, that signed measures satisfy $\nu( \emptyset ) = 0$ . Due to there is not given, what happens with empty sets, I would associate this with inserting zero for k (assuming, that $0 \notin \mathbb{N}$ ). So this doesn't give me additional information to determine possible $p$ 's Furthermore for countable $I$ and a set of pairwise disjoint $A_i$ 's holds $\nu(\bigcup_{i\in I}A_i) = \sum_{i \in I} \nu(A_i)$ . I guess this is one information, that might be used to determine possible $p$ 's. But my problem is, that I don't really know, how to use this, due to there is no real definition, what happens, if I put something different than singletons in $\nu$ . Is there anything, that I have overseen, or is there something missing in the indication? Or is there a reason, why we can claim, that $\nu(\{k,m\}):= (k+m)^p (-1)^{k+m} = \nu(\{k+m\})$ ? Thank you very much for your help!","['measure-theory', 'signed-measures']"
2964244,Show that a specific map is a submersion of $O(3)$ in $S^2$,"Denoting the components of the $3\times3$ matrix $A \in O(3)$ as $a_{ij}$ , show that $$ F: O(3) \rightarrow S^2, a_{ij} \mapsto a_{1j} $$ is a submersion. (The map is well defined since for $A \in O(3)$ it is true that $a_{11}^2 + a_{12}^2 + a_{13}^2 = 1$ .) From what I understand to show that the map is a submersion, I have to show that the differential map $$ dF: T_AO(3) \rightarrow T_{F(A)}S^2 $$ is onto for every $A \in O(3)$ . Now if I were given a tangent vector $X\in T_AO(3)$ then from what I understand the map simply is $dF: x_{ij} \mapsto x_{1j}$ , where $x_{ij}$ denote the components of $X$ . However, I fail to see that this is surjective at every point $A\in O(3)$ . Indeed at the identity $A = I$ I know that a basis of tangent vectors is given by the antisymmetric matrices with one non-zero number in the upper triangular part and that then the map is something like $X \mapsto (0, s, t)$ and arguably two parameters are enough to span the tangent space of $S^2$ . I fail to see how this is true for tangent vectors $X$ at an arbitrary point $A$ with the vector being given by $$ X = \left.\frac{\mathrm{d}}{\mathrm{d}t}\right|_{t=0} \gamma(t)$$ where $\gamma: \mathbb{R} \rightarrow O(3)$ with $\gamma(0) = A$ .",['differential-geometry']
2964247,Integers which are squared norm of 3 by 3 integer matrices,"Let $n$ be a positive integer. Let $E_n$ be the set of integers which are the sum of $n$ squares. Let $F_n$ be the set of integers of the form $\Vert A \Vert^2$ with $A \in M_n(\mathbb{Z})$ .  Then $E_n \subseteq F_n$ because: $$\left\| \pmatrix{a_1&0& \cdots\\ \vdots & \vdots&  \\ a_n&0& \cdots} \right\|^2  = \sum_{i=1}^n a_i^2.$$ Note that the case $n=3$ is exceptional, because $E_n= F_n$ $\forall n \neq 3$ , whereas $E_3 \subsetneq F_3$ : obviously $E_1=F_1$ , it is proved here that $E_2=F_2$ , for $n \ge 4$ , $E_n=F_n$ because $E_4 = \mathbb{N}$ , by Lagrange's four square theorem , finally, $E_3 \subsetneq F_3$ because $\forall n \le 2000$ , $n \in F_3$ (by computation below), whereas: Legendre's three-square theorem A natural number can be represented as the sum of three squares of integers if and only if it is not of the form $4^n(8m+7)$ for integers $n,m \ge 0$ . Question : Which integers are contained in $F_3$ ? The computation suggests that $F_3$ contains every natural number, so (if it is true) we are reduced to prove that it contains those of the form $4^n(8m+7)$ , by Legendre's three-square theorem. Computation sage: L=[]
....: for a2 in range(33):
....:     for a4 in range(33):
....:         for a5 in range(33):
....:             for a7 in range(33):
....:                 for a8 in range(33):
....:                     n=numerical_approx(matrix([[0,a2,0],[a4,a5,0],[a7,a8,0]]).norm()^2,digits=10)
....:                     if n.is_integer():
....:                         L.append(int(n))
....: l=list(set(L))
....: l.sort()
....: l[2095]
....:
2095","['number-theory', 'elementary-number-theory', 'normed-spaces', 'matrices', 'linear-algebra']"
2964248,"Showing that $Y^c$ is dense in $X$, if $ Y \subset (X, \| \cdot \|)$","Exercise : Let $X$ be a normed space $(X, \| \cdot \|)$ and $Y$ be a proper subspace of $X$ , $Y \subset X$ . Show that the complement set $Y^c$ is dense in $X$ . Question : I'm totally at loss on how to start this exercise. I know that a normed space means it's a linear space carrying the definition and properties of the norm, while on the other hand, a subset $A$ of a topological space $X$ is called dense (in $X$ ) if every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$ ; that is, the closure of $A$ is constituting the whole set $X$ . But how would I proceed to showing rigorously the statement of the exercise ?","['normed-spaces', 'functional-analysis', 'real-analysis']"
2964275,Solve $\sqrt{\frac{4-x}x}+\sqrt{\frac{x-4}{x+1}}=2-\sqrt{x^2-12}$,The equation is $$\sqrt{\frac{4-x}x}+\sqrt{\frac{x-4}{x+1}}=2-\sqrt{x^2-12}$$ I tried squaring both left side and right side then bringing them to same numerator but got lost from there ... any ideas of how should this be solved? I got to: $$2\cdot\sqrt{-\frac{(x-4)^2}{x^2+x}}+4\cdot\sqrt{x^2-12}=x^2-8-\frac{4-x}{x^2+x}$$,"['algebra-precalculus', 'radical-equations', 'radicals', 'inequality']"
2964328,How to show that the following sets are equinumerous,"This is my first question on this website, so please do give me feedback where nessecary. I got stuck on a question in Computability and Logic 5th edition by Boole i.a.
It's question 2.10 and the question is as follows: 2.10 Show that the following sets are equinumerous: a) the set of all pairs of sets of positive integers b) the set of all sets of pairs of positive integers c) the set of all sets of positive integers. It has been shown that the set of all sets of positive integers is not enumerable and since the sets in a) and b) contain more elements, these must be non-enumerable too. But I don't think that is enough to show that the sets in a), b) and c) are equinumerous. What would prove it, is if I could find two bijections between two unequal pairs of these three sets. But I couldn't think of any. Also, I could show that all of these three sets are equinumerous with the real numbers. For c), this was already shown in the text. a) is clearly equinumerous with R^2 and R^2 is equinumerous with R, as is explained well in this post: Examples of bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$ . So a) must be equinumerous with  c). But then how do I find a bijection between b) and either a) or c)? Thanks in advance.","['elementary-set-theory', 'computability', 'real-analysis']"
2964346,Pove that the summation of iid sequence satisfies $\frac{S_n}{n\log n}\rightarrow c\quad \text{in probability}$,"Suppose that $X_1,X_2,\cdots,X_n$ are iid sequence with pdf $\frac{2}{\pi (1+x^2)}\cdot 1_{(0,+\infty)}(x)$ . Denote $S_n$ as $S_n:=X_1+X_2+\cdots+X_n$ . Prove that there exits $c>0$ such that $$\frac{S_n}{n\log n}\rightarrow c\quad \text{in probability}$$ and calculate $c$ . My solution : Choose another iid sequence $Y_1,Y_2,\cdots,Y_n$ such that $X_n,Y_n$ are independent and have same distribution. Let $X_n^s:=X_n-Y_n$ . I have proved  that $$\frac{X_1^s+X_2^s+\cdots+X_n^s}{n\log n}\rightarrow 0\quad \text{in probability}.$$ Therefore, $$\frac{S_n}{n\log n}-m_n\rightarrow 0\quad \text{in probability}$$ where $m_n$ is the medin of $\frac{S_n}{n\log n}$ . I want to show $m_n\rightarrow c$ but I can't.","['law-of-large-numbers', 'probability-theory']"
2964353,"If $f$ is a polynomial, how does $f(\frac{d}{dt})$ act on $y$?","If $f\left(\frac{d}{dt}\right)=a_n\frac{d^n}{dt^n}+\dots+a_1\frac{d}{dt}+a_0$ , then whether $$f\left(\frac{d}{dt}\right)(y)=a_n\frac{d^n}{dt^n}y+\dots+a_1\frac{d}{dt}y+a_0y$$ or $$f\left(\frac{d}{dt}\right)(y)=a_n\frac{d^n}{dt^n}y+\dots+a_1\frac{d}{dt}y+a_0$$ I am confused.",['ordinary-differential-equations']
2964356,Is the Euclidean Distance and the Euclidean Norm the same thing?,"I understand that a norm is defined as: $$||\space.||:V\to \mathbb{R}$$ which is a notion of distance defined on a vector space to give the magnitude of a vector (distance from the origin). Is it true that this distance can be any type of distance you want to define it to be? For example, taking the norm to be the Manhattan (Taxi Metric) distance, where $$||x||=\sum_{i=1}^{n}{|x_{i}|^{2}} \text{ where }x\in\mathbb{R}^{n}$$ or you could simply define the norm to be the absolute value of the first component of $x$ . I also understand that distance is a function $$d:V\times V\to \mathbb{R}$$ which measures between two points.
So as the Euclidean distance is defined to be: $$dist(x,y)=\bigg(\sum_{i=1}^{n}{(x_{i}-y_{i})^2}\bigg)^{\frac{1}{2}}$$ and the Euclidean Norm is defined to be $$|x|=\bigg(\sum_{i=1}^{n}{x_{i}^2}\bigg)^{\frac{1}{2}}$$ then the norm is simply the Euclidean distance between a vector $x$ and $0$ so can you essentially think of them as the same thing? Is it true to say that a norm is a type of distance? Or would it be the other way around to say that the distance is a type of norm? Thanks!","['euclidean-geometry', 'normed-spaces', 'multivariable-calculus', 'functions', 'vector-analysis']"
