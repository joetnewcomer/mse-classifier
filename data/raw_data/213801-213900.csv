question_id,title,body,tags
4328530,What is the probability that a five-card poker hand contains all 4 suits?,"I know that my answer is wrong but I don't know why it is wrong. What is the probability that a five-card poker hand contains all 4 suits? What I did was I this: $$\frac{C(4,1)\,C(13,1)\,C(3,1)\,C(13,1)\,C(2,1)\,C(13,1)\,C(1,1)\,C(13,1)\,C(4,4)\,C(48,1)}{C(52,5)}.$$ My logic for $C(4,1)$ was to choose one of the four suits, then the $C(13,1)$ was to pick one of the cards of that suit, and similarly for $C(3,1)$ subtracting one because there is one less suit to choose from. Same logic applies for $C(2,1)$ and $C(1,1)$ and then $C(4,4)$ and $C(48,1)$ lets me choose any of the remaining $48$ cards from all suits because the last card doesn't matter. Then answer ends up being something ridiculously big (not a fraction) and I don't know where I went wrong because I believe my logic is sound. What would be the correct answer and why?","['discrete-mathematics', 'poker', 'combinatorics', 'probability']"
4328531,Prove instability using Lyapunov function,"Consider the system: \begin{align}
x' &= x^3 + xy \\
y' &= -y + y^2 + xy - x^3 \\
\end{align} I want to prove the origin is an unstable point by using the Lyapunov function $V(x,y) = \tfrac{x^4}{4} - \tfrac{y^2}{2}$ (this is a hint provided in the exercise). In order to use Chetaev instability theorem I would like to prove that there is a domain $U$ in a punctured neighborhood of $0$ such that $$V'(x,y) = \frac{\partial E}{\partial x} (x^3 + xy) + \frac{\partial E}{\partial y} (-y + y^2 + xy - x^3) = x^6 + x^4y + y^2 - y^3 -xy^2 + x^3y$$ is strictly positive, which Wolfram Alpha confirms (indeed, that function is strictly positive in a disk around $0$ ). I have tried to show that $0$ is a local minimum, but the Hessian test is not conclusive as the Hessian is positive semidefinite. I have also tried to bound the expression below by $0$ without any luck. Could you give me any hint to proceed? Thanks in advance","['ordinary-differential-equations', 'dynamical-systems']"
4328588,Yankee swap without stealing: Prob of last player ending with his/her own gift,"Our group did a yankee swap yesterday, which got me thinking about a probability question, re: a simplified version without actual ""stealing"". Players numbered $1, 2, ..., N$ each brings a wrapped gift. Then in order, player $1, 2, ..., N-1$ each uniformly-randomly picks a wrapped gift which is NOT the one he/she brought, and unwraps it.  (By unwrapping it, this gift cannot be picked by any subsequent player.) Question: What is the probability $p$ that the last wrapped gift was brought by the last player, i.e. $N$ ?  (Or in yankee swap terminology, the last player would be forced to unwrap the gift he/she brought.) Further thoughts: Clearly $1/N$ is not the answer since $N=2 \implies p=0$ (player $1$ is forced to pick the gift brought by player $2$ ). A bit of calculation shows $N = 3 \implies p=1/4$ : Player $1$ can pick $2$ or $3$ , with prob $1/2$ each. If player $1$ picked $3$ , then player $2$ must pick $1$ and player $3$ will pick $2$ . If player $1$ picked $2$ , then player $2$ can pick $1$ or $3$ , with conditional probability $1/2$ each. Therefore, the only case of gift $3$ being the last wrapped gift is $1$ picks $2$ and $2$ picks $1$ , which happens with probability $1/2 \times 1/2 = 1/4$ . I would think for large $N$ the probability approaches $1/N$ , but stays slightly below.  I.e. I imagine $p = 1/N - \epsilon(N)$ where the error term $\epsilon(N) > 0$ and approaches $0$ very very fast (much faster than $1/N$ ). For any $N$ , we can solve this exactly using a system of recurrences (involving $3N$ variables, I think...), but I'm wondering if there is a smarter way.  E.g. if my second bullet above is correct, is there a good way to calculate or bound the error term?","['recreational-mathematics', 'probability']"
4328613,Proving monotonicity of a equation.,"I am trying to prove the following: Let $f : (a,b) \rightarrow \mathbb{R}$ be a differentiable function and let $x_1,x_2$ be two points in $(a,b).$ Then $f'$ assumes all real values between $f'(x_1)$ and $f'(x_2).$ Hint: Consider $x_1 < x_2$ and a singular value $c$ between $f'(x_1)$ and $f'(x_2),$ set $g(x) = f(x) - cx,$ show that $g(x)$ is not monotonic. My work: WLOG let $x_1 < x_2$ and $f'(x_1) < c < f'(x_2).$ Setting $g(x) = f(x) - cx.$ Then $g'(x) = f'(x) - c$ and thus $g'(x_1) = f'(x_1) - c < 0$ and $g'(x_2) = f'(x_2) - c > 0$ from the inequalities. So $\implies g'(x_1)g'(x_2) < 0.$ My problem: How do I proceed to show that $g(x)$ is not monotonic ? Am I overlooking something ? P.S. I do NOT want a full solution , I want a hint to solve it myself.","['calculus', 'functions', 'derivatives']"
4328618,"Is it true that, if $f$ is uniformly continuous in $(a,b),$ then the limits $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist?","Let $f$ be a continuous function in $(a,b)$ . I know that:
If the limits $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist, then $f$ is uniformly continuous in $(a,b)$ . Is the inverse also true?","['limits', 'calculus', 'uniform-continuity', 'continuity']"
4328632,"What is the formal, rigorous definition of a differential equation?","In college classes, we solve differential equations. However, I have never seen a book give a rigorous definition of what a differential equation actually is. Can someone give me a rigorous definition of a differential equation? I want a definition in terms of set theory.","['definition', 'ordinary-differential-equations']"
4328727,Find all complex numbers $z$ such as $z$ and $2/z$ have both real and imaginary part integers,"I am really struggling to solve this one. I feel like I am missing the key part of the solution, so I would like to see how it's done. Find all complex numbers $z=x+yi$ such as $z$ and $2z^{-1}$ have both real and imaginary part integers This is what I thought: $$2z^{-1} =\frac{2}{z} = \frac{2}{x+yi}= \frac{2}{x+yi}\cdot \frac{x-yi}{x-yi} = \frac{2x-2yi}{x^2+y^2}.$$ In order to $2z^{-1}$ have its imaginary part $\in \mathbb Z$ , we should equal $2y$ to $0$ $$2y=0 \Rightarrow y=0$$ $yi=0$ is an integer. $x$ must also be an integer. We simply assume $x \in \mathbb Z$ (no matter what value $x$ has, as long as it's an integer, we are good). We do the same for $z$ and find out the same values $yi=0$ and $x \in \mathbb Z$ . Therefore, the set $A=\{(x,y)\in \mathbb C|x\in \mathbb Z\text{ and }y=0\}$ is the set of all complex numbers whose real and imaginary part are integers.","['elementary-number-theory', 'integers', 'solution-verification', 'discrete-mathematics', 'complex-numbers']"
4328729,How to show that $D^{-1}A$ and $L^{T}D^{-1}L$ have same Eigenvalues,"Let $A$ be a symmetric positive definite matrix, I want to prove that $D^{-1}A$ and $L^{T}D^{-1}L$ have same Eigenvalues where $D=\text{diag}(\text{diag}(A))$ and $L$ is a lower-triangular matrix such that $A=LL^{T}$ . I observed that $D^{-1}A=D^{-1}LL^{T}$ and thus we can write $L^{T}D^{-1}A=(L^{T}D^{-1}L)L^{T}$ .How can we proceed from here? I would hope for some hints.
I noticed that $D^{-1}$ and $L^{T}D^{-1}L$ are similar given this form. Therefore, they both must have same eigenvalues but how can I show that $D^{-1}$ and $D^{-1}A$ have same eigenvalues to complete this proof by substitution as it appears? Update: As mentioned in the comments, this does not work since $L^{T}D^{-1}L$ and $D^{-1}$ are congruent and not similar. Therefore, I would really hope for some help in finding an approach to prove the claim. Remark: $D:=\text{diag}(\text{diag}(A))$ means that $D$ is a diagonal matrix whose diagonal entries are the diagonal entries of $A$ . This is based on MATLAB's notation.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4328755,$(a\otimes b)^z = a^z \otimes b^z$ for elements in a $C^*$-algebra and $z\in \mathbb{C}$.,"Let $A$ and $B$ be unital $C^*$ -algebras. Let $a\in A$ and $b \in B$ be positive and invertible elements, so that in particular we can use the continuous functional calculus to define $a^z$ and $b^z$ and $(a\otimes b)^z$ where $z\in \mathbb{C}$ . I want to show that $$(a\otimes b)^z = a^z\otimes b^z$$ as elements of the $C^*$ -algebra $A\otimes B$ (say equiped with the minimal norm, but I don't think the choice of norm matters). Attempt : Let $n \in \mathbb{N}\setminus \{0\}$ . Then clearly $(a\otimes b)^{1/n} = a^{1/n} \otimes b^{1/n}$ by uniqueness of the positive $n^{th}$ root of a positive element. Hence, $(a\otimes b)^{m/n}= (a^{1/n}\otimes b^{1/n})^m = a^{m/n}\otimes b^{m/n}$ for all $m,n \in \mathbb{N}\setminus \{0\}$ . Applying the same argument with $a$ replaced by $a^{-1}$ and $b$ by $b^{-1}$ yields $(a\otimes b)^q = a^q \otimes b^q$ for all $q\in \mathbb{Q}$ . By density of $\mathbb{Q}$ in $\mathbb{R}$ and using the fact that $x \mapsto x^z$ is uniformly continuous on compacts, I can then deduce that $(a\otimes b)^r = a^r \otimes b^r$ for all $r\in \mathbb{R}$ . How can I deduce that the equality holds for all complex numbers? Perhaps we must prove that $$z \mapsto (a\otimes b)^z, \quad z \mapsto a^z \otimes b^z$$ are analytic functions $\mathbb{C}\to A\otimes B$ that agree on $\mathbb{R}$ , so that they agree everywhere?","['c-star-algebras', 'operator-algebras', 'operator-theory', 'complex-analysis', 'tensor-products']"
4328757,Is there a general method to operate the reduction of a rational expression to a sum : $\frac {1+2x}{1-3x} \rightarrow 1+5x+\frac{15x^3}{1-3x}$,"Source : page $146$ https://archive.org/details/academicalgebraf00bowsrich/page/n5/mode/2up The question I am asking does not deal with partial fraction decomposition ( as far as I can tell). In Edward Bowser's Academic Algebra , I find the following exercise ( I chose : $\frac {1+2x}{1-3x} \rightarrow  1+5x+\frac{15x^3}{1-3x}$ as a mere example). First I observe that if I were told to do the exercise in the reverse sense, that is , from the decomposed form ( sum of terms) to the composed one, I would not encounter much difficulty. This is simply applying the rule governing the addition of fractions. But the other direction is not as easy as the other one. (1) how does one know the number of terms into which the fraction can be decomposed? (2) should one use a method akin to the partial  fraction decomposition technique, I mean , should one write something like this : $\frac {1+2x}{1-3x}= \frac {A}{1-3x} + \frac {B}{1-3x}+\frac {C}{1-3x}$ and then try to find the appropriate numerators $A, B$ and $C$ ? (3) finally, under what name is this kind of decomposition known nowadays?","['fractions', 'algebra-precalculus', 'soft-question', 'rational-functions']"
4328814,Categories of modules and the correct “language”,"Let $R$ be a ring, two $R$ -modules are said to be isomorphic if there exists a linear bijective map between the modules. But the first assumption is the two modules are over the same ring. Is it possible to generalize the term of isomorphic modules into the next case? Let $S$ ( $S\ncong R$ ) be a ring, $M$ be a $R$ module, and $N$ be a $S$ module; if $M$ is isomorphic as groups to $N$ , then of course $M$ heirs a structure of a $S$ module and $N$ heirs a structure of a $R$ module. But although $S\ncong R$ , is it possible to think of $M\cong N$ as modules in some categories? For example, if $M=\Bbb{C}[x,y] \big /\langle x^2+y^2-1 \rangle$ is a $R=\Bbb{C}[x+y]$ module, of course $M\cong \Bbb{C}[x,x^{-1}]$ as groups (moreover as rings but it doesn’t matter), is it true now to think of $M$ as a $\Bbb{C}[x+x^{-1}]$ module?, if not, are there some categories such that $M\cong \Bbb{C}[x,x^{-1}]$ as modules?","['abstract-algebra', 'category-theory']"
4328818,Determining characteristics of the differential equation based on the behavior of its solutions,"I am trying to solve this problem, and I have some ideas of how to do it, but I can't quite pin it down. All the solutions to the equation: $$x''+p(t)x'+q(t)x=0$$ tend to $0$ as $t$ tends to $0$ . What can be said about $p(t)$ ? I have tried drawing all the different ways I can think of that a function can tend to zero (decreasing oscillations, or something like $e^{-x}$ or $-e^{-x}$ .) From there, I can find some relationships between $x$ , $x'$ , and $x''$ . For example, if the function is positive, it must at some point start to decrease, but that decrease must eventually slow down, or else the function becomes negative. And if it's negative, it must at some point start to increase and then the increase slows down. From that sort of analysis, I think I've concluded at least that $p(t)$ is bounded from below, but I'm not sure if that's sufficient, and I haven't proved it rigorously at all, I've just drawn some pictures. Does anyone have an idea of a theorem that might be applicable here, or of a more rigorous way of approaching this? I have a sense that Gronwall's Inequality might be applicable, but I don't know exactly how.",['ordinary-differential-equations']
4328928,Proving the inequality $\sin{\theta} \leq \theta \leq \tan{\theta} $,"In the Squeeze Theorem proof of $\lim\limits_{\theta\to 0}\frac{\sin{\theta}}{\theta} $ , the appeal is made to the obvious triangle inclusion in the image on the unit circle below to say that $$\frac{\sin{\theta}}{2} \leq \frac{\theta}{2} \leq \frac{\tan{\theta}}{2} $$ While it is indeed obvious that these triangles share this inequality relationship, I was wondering how it can be rigorously proven.  For the purposes of answering, it's fine to prove it for just the upper right quadrant of the unit circle, i.e. when $0 < \theta < \frac{\pi}{2}$ .","['limits', 'calculus', 'trigonometry', 'inequality']"
4328979,"Did I prove $\;{^{n}2} \equiv {^{n-1}2} $ $\,$ mod $n\;\;\;$ for $\;n \geq 2\;$?","Let's first state some properties of tetrations : They are one of the basic arithmetic functions, 4. hyperoperation to
be exact. $\;{^{n}2}$ is in its basic, most elemental form. So we can't really
simplify it or break it down. Tetrations also aren't associative, the ""agreement"" is that we exponentiate from right-to-left. So e.g. $\; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536 $ Not e.g. $\; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256 $ I stated these properties, because when I first started dealing with this problem, I had no idea that a thing like tetrations exists and did a lot of unnecessary work and faulty adjustments. The idea that helped me, hopefully, prove it came with the use of modular exponentiation. Modular exponentiation $a^b\;$ mod $c \; \equiv \; ((a \;$ mod $c)^b)\;$ mod $c$ Often used to calculate mod for large values of $b$ , which is exactly what I had to deal with. The use of it, illustrated by an example: For $n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536}$ Mod for ${^{n}2}$ : $\;{2^{2^{16}}}$ mod $5=\;{4^{2^{15}}}$ mod $5=\;{16^{2^{14}}}$ mod $5=\;{1^{2^{14}}}$ mod $5=\;1$ Mod for ${^{n-1}2}$ : $\;{2^{2^{4}}}$ mod $5=\;{4^{2^{3}}}$ mod $5=\;{16^{2^{2}}}$ mod $5=\;{1^{2^{2}}}$ mod $5=\;1$ So generally: $\;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\;$ ...and so on, until the basis is big enough to use mod Now from what I can tell, there are two or three scenarios depending on how you look at it. Stable and decisive results, such as $1$ or $0$ where, the function ends E.g. $n = 5\;$ so ${^{5}2}$ mod $5=1\;$ and $n = 8\;$ so ${^{8}2}$ mod $8=0$ Stable results, where if you square it and then subtract the mod, you'll land again on the same number E.g. $n = 6\;$ so ${^{6}2}$ mod $6=4\;$ and E.g. $n = 10\;$ so ${^{10}2}$ mod $10=6\;$ Looping/recurring results, that again will trap the function E.g. $n = 7\;$ so ${^{7}2}$ mod $7= 2\; or \;4\;$ and $n = 11\;$ so ${^{11}2}$ mod $11= 4\; or\; 5\; or\; 3\; or \;9$ From the three, the only one that could cause trouble, would be the looping one. The worry could be that the result will land on a different number for $\;{^{n}2}\;$ and $\;{^{n-1}2}$ But $\;{^{n}2}\;$ goes through the same proces as $\;{^{n-1}2}$ just twice. $\;{^{n-1}2}$ will finish first, but the result won't be changed while $\;{^{n}2}\;$ does the ""x more laps"" because it will be out of exponents to get it into the loop again and since the $\;{^{n}2}\;$ has exactly x times the exponents it will finish on the same result out of the loop. This bit was a bit informal, but I'm not sure how to write it down properly. Let's for take $n=5\;$ again For $\;{^{n}2}\; = {2^{2^{16}}}=2^{65536}$ For $\;{^{n-1}2}\; = {2^{2^{4}}} =2^{16} $ And since $\;65536 = 2^{16}\;$ is of course divisible by $\;16 = 2^{4}\;$ it makes sense, that they'll meet at the same result. Does it make sense ? Are there any gaps, that I forgot to cover ? Did I actually prove it ? I'm well aware that, my method and my record of it is far from optimal. If you have any suggestions on how to write it more formally and correctly, please tell me. Thanks a lot, any sort of advice and help is appreciated. Czech version ( needed it for my homework sorry ): Dokaž $\;{^{n}2} \equiv {^{n-1}2} $ $\,$ mod $n\;\;\;$ for $\;n \geq 2\;$ Jde o Tetratace Důležité vlastnosti tetratace: Jde o základní aritmetickou operaci. Konkrétně 4. Hyperoperaci. $\;{^{n}2}$ je poměrně jednoduchá operace co se tetrací týče. Tedy je ve své základní, nejvíce elementární formě. Takže jí upravit, zjednodušit už moc nemůžeme. Tetratace nejsou asociativní. Záleží z jakého směru exponujeme, jako standart se bere z prava do leve, nebo-li zhora dolů. Tedy e.g. $\; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536 $ Ne e.g. $\; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256 $ Tyto vlastnosti jsem uvedl, protože když jsem tento problém začal řešit, tak jsem nevěděl, že tetratace existují a kvůli neznalosti jejich vlastností jsem si nadělal spoustu zbytečné práce a nadělal spoustu chybných úprav ve snaze tvar zjednodušit. Myšlenka která mi pomohla problém dokázat, byla skrytá v Modulárním umocňování Modulárním umocňování $a^b\;$ mod $c \; \equiv \; ((a \;$ mod $c)^b)\;$ mod $c$ Často používané na počítání modula pro velké čísla, exponenty $b$ , což je přesně případ se kterým se potýkáme u této úlohy. Její použití illustruji na případu: Pro $n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536}$ Mod pro ${^{n}2}$ : $\;{2^{2^{16}}}$ mod $5=\;{4^{2^{15}}}$ mod $5=\;{16^{2^{14}}}$ mod $5=\;{1^{2^{14}}}$ mod $5=\;1$ Mod pro ${^{n-1}2}$ : $\;{2^{2^{4}}}$ mod $5=\;{4^{2^{3}}}$ mod $5=\;{16^{2^{2}}}$ mod $5=\;{1^{2^{2}}}$ mod $5=\;1$ Takže obecně: $\;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\;$ ...a takhle dál, dokud základ není dostatečně velký na to aby jsme použili modulo. Výsledky $\;{^{n}2}$ mod $n\;$ různých $\;n\;$ můžeme rozdělit do dvou či tří kategorii. Stabilní a konečné výsledky, tedy $1$ nebo $0$ kde, funkce skončí E.g. $n = 5\;$ takže ${^{5}2}$ mod $5=1\;$ a $n = 8\;$ takže ${^{8}2}$ mod $8=0$ Stabilní výsledky, které po mocnění dvojkou a odečtení modula, skončí zpět na čísle které jsme umocňovali, tedy funkci uvězní v smyčce o jedné otáčce. E.g. $n = 6\;$ takže ${^{6}2}$ mod $6=4\;$ a E.g. $n = 10\;$ takže ${^{10}2}$ mod $10=6\;$ Opakující se výsledky, tedy smyčka o více otáčkách, které funkci také uvězní. E.g. $n = 7\;$ takže ${^{7}2}$ mod $7= 2\; nebo \;4\;$ a $n = 11\;$ takže ${^{11}2}$ mod $11= 4\; nebo\; 5\; nebo\; 3\; nebo \;9$ Z těchto tří kategorií výsledků, jediná která by mohla tvrzení vyvrátit, jsou opakující se výsledky. Zdá se, že by problém mohl nastat kdyby funkce $\;{^{n}2}\;$ a $\;{^{n-1}2}$ skončily na různých číslech smyčky. Ale stačí si uvědomit, že $\;{^{n}2}\;$ prochází stejným procesem jako $\;{^{n-1}2}$ jen vícekrát. $\;{^{n-1}2}$ sice skončí první, ale její výsledek se nezmění už nezmění a počká, než $\;{^{n}2}\;$ také doběhne. Použil jsem analogii s okruhy, jako na běžecké trati, obě funkce mají stejný cíl, v oku matematiky tedy i start. Funkce $\;{^{n}2}\;$ jen běží více okruhů a to přesně $x$ krát. Má více exponentů na to aby proběhla víckrát. Uznávám, že tento segment byl poměrně dost neformální, ale pro mě to byla užitečná vizualizace. Také jsem si nebyl jistý jak to formálně zapsat. Pro přesnost ukáži na příkladě: Vezmeme si $n=5\;$ Pro $\;{^{n}2}\; = {2^{2^{16}}}=2^{65536}$ Pro $\;{^{n-1}2}\; = {2^{2^{4}}} =2^{16} $ A protože $\;65536 = 2^{16}\;$ je jasně dělitelné $\;16 = 2^{4}\;$ dává smysl, že dopadnou na stejný výsledek. $\;{^{n}2}\;$ akorát udělá $\;65536 \, : \, 16 \,=\, 4096 \,= \,2^{12}\; $ více kol. FJFI DIM1","['modular-arithmetic', 'elementary-number-theory', 'solution-verification', 'discrete-mathematics', 'tetration']"
4328990,Show that a set $A \subset \mathbb{R}^2$ of positive Lebesgue measure contains the vertices of an equilateral triangle,"I have the following problem for which I'm trying to figure out a solution, and I'm a bit stuck. Any hints or insights would be appreciated. Suppose $A \subseteq \mathbb{R}^2$ is a Lebesgue measurable set of positive Lebesgue measures. Show that it contains the vertices of an equilateral triangle. Here is what I've done so far. First, we can write $$\displaystyle A =\bigcup_{i = 1}^\infty \left(A \cap ((B(0, i) \backslash B(0, i-1))\right)$$ where this is a disjoint union. If, for all $i$ , $$m\left(A \cap ((B(0, i) \backslash  B(0, i-1))\right) = 0$$ then $m(A) = 0$ , which is a contradiction. Hence, we can take $A$ to be bounded. Furthermore, since, by the regularity of measure, we can approximate $A$ by a closed subset $F$ such that $m(A \backslash F) < \epsilon$ for any $\epsilon > 0$ , it suffices to show that $F$ contains the vertices of an equilateral triangle. Hence, we can take our set to be compact. The above so far I believe to be correct. What follows is the rest of my thought process, which I don't believe is correct, but I wonder if anyone has any idea how to fix the gap in it. Since $F$ is compact, we can cover it with finitely many balls of radius $\epsilon$ , so we can write $$F = \bigcup_{i = 1}^n (F \cap B(x_i, \epsilon))$$ Hence, by the countable subadditivity of measure, we have $$m(F) \leq \sum_{i = 1}^n m(F \cap B(x_i, \epsilon))$$ Dividing by the measure of a ball of radius $\epsilon$ , we have $$\frac{m(F)}{\pi \epsilon^2} \leq \sum_{i = 1}^n \frac{m(F \cap B(x_i, \epsilon)}{m(B(x_i, \epsilon))}$$ Now, by Lebesgue's density theorem, taking the limit as $\epsilon \to 0$ , the right hand side is finite, which implies that $m(F) = 0$ , a contradiction. Now, one problem with this is that $n$ actually depends on $\epsilon$ , so we can't actually move the limit inside the finite sum. I do think solving this problem is supposed to make use of Lebesgue's density theorem though. Any hints or insights would be appreciated! Thank you!","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4329000,Can we use bounded convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$?,"I know how to use dominated convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$ ? Can we use bounded convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$ ? The bounded convergence theorem statement is as follows: Let $\{f_n\}$ be a sequence of measurable functions on a set of finite measure $E.$ Suppose $\{f_n\}$ is uniformly pointwise bounded on $E,$ that is, there is a number $M \geq 0$ for which $$|f_n| \leq M $$ on $E$ for all $n.$ If $\{f_n\} \rightarrow f$ pointwise on $E,$ then $\lim_{n \rightarrow \infty} \int_E f_n = \int_E f.$ Could anyone help me in this please?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
4329023,"For $v_1,\dots ,v_n\in\mathbb{C}^n$ define $A=(\langle v_i,v_j\rangle)_{i,j=1}^n$. Prove that $A$ is a non negative linear operator on $\mathbb{C}^n$.","Consider $\mathbb C^n$ as an inner product space with the standard inner product $\langle \cdot, \cdot \rangle$ . For $v_1,\dots ,v_n\in \mathbb{C}^n$ define the $n\times n$ matrix $$A=(\langle v_i,v_j \rangle)_{i,j=1}^n.$$ Prove that $A$ is a non negative linear operator on $\mathbb C^n$ . Does the converse hold? Prove or give a counterexample. This link that Vincent pointed out in the comments seems to answer the non-negativity part. I am copying the necessary part of that answer here- Let $x$ be the matrix $$
x=\begin{bmatrix}v_1&v_2&\cdots&v_n\end{bmatrix}.
$$ Then $$
x^*x=\begin{bmatrix}
v_1^*v_1&v_1^*v_2&\cdots&v_1^*v_n\\
v_2^*v_1&v_2^*v_2&\cdots&v_2^*v_n\\
\vdots & \vdots & \ddots & \vdots \\
v_n^*v_1&v_n^*v_2&\cdots&v_n^*v_n\\
 \end{bmatrix}
=\begin{bmatrix} 
\langle v_1, v_1 \rangle & \langle v_1, v_2\rangle & \cdots &\langle v_1, v_n \rangle \\
\langle v_2, v_1 \rangle & \langle v_2, v_2\rangle & \cdots &\langle v_2, v_n \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle v_n, v_1 \rangle & \langle v_n, v_2\rangle & \cdots &\langle v_n, v_n \rangle
 \end{bmatrix}.
$$ As $x^*x$ is positive-semidefinite, $\det x^*x\geq0$ . Also, the linearity of $A$ trivially follows from the linearity of matrix multiplications. For the converse, I'm not sure how to proceed. I guess, the converse of the given statement is something like ""if $A$ is a non-negative linear operator on $\mathbb C^n$ , then it's entries are of the form $\langle v_i,v_j \rangle$ for $v_1,\dots ,v_n\in \mathbb{C}^n$ "". I feel like it's not true, but I can't produce any counterexamples. Please help me.","['matrices', 'inner-products', 'linear-algebra', 'linear-transformations']"
4329046,"Given $G=\langle a\rangle$, $|G|=n$ and $d\mid n$, show $G$ has a unique subgroup of order $d$.","Given $G=\langle a\rangle$ , $|G|=n$ and $d\mid n$ , show $G$ has a unique subgroup of order $d$ . Proof: (Existence) : $|\langle a\rangle|=|a|=n$ and $ |a^\frac{n}{d}|=d$ . Then, $H_d=\langle a^\frac{n}{d}\rangle$ is a subgroup of $G$ of order $d$ . (Uniqueness) : Suppose, $H\le G$ and $|H|=d$ . Claim: $H=H_d$ . It is enough to prove one-sided set inclusion $H\subseteq H_d$ because $H\le H_d$ and $|H|=|H_d|$ implies $H=H_d$ . Choose $b\in H$ . Then $|H|=d  \implies b^d=e$ and $b\in H \implies b \in G$ . Hence $b=a^k$ for some $k\in \mathbb{Z}$ . Also $e=b^d =(a^k)^d =a^{kd} $ and $|a|=n $ implies $n|kd$ . Hence $kd=nk'$ for some $k'\in \mathbb{Z}$ and $k=(\frac{n}{d})k'$ Hence $b=a^k =a^{{(\frac{n}{d})}k'}$ for some $k' \in \mathbb{Z}$ implies $b\in H_d$ . Hence $H\subseteq H_d$ and then $H=H_d$ . Note: $|G|$ : order of the group $G$ . Is the proof correct ? Is there any mistake?","['cyclic-groups', 'group-theory', 'abstract-algebra', 'solution-verification', 'abelian-groups']"
4329047,"Why is $\hat f(\xi)/\xi\in L^1(\mathbb R)$ when $f\in L^1$ is odd,$\hat f'(0)$ exists, and $\hat f(\xi)$ always has the same sign as $\xi$?","I need to prove: If $f \in L^1(\mathbb{R})$ is an odd function such that its Fourier Transform $\hat{f}$ is differentiable at $\xi = 0$ and $\hat{f} \geq 0$ when $\xi \geq 0$ , then $\hat{f}(\xi)/\xi \in L^1(\mathbb{R})$ . My work so far: I have established (using only $f$ being odd) that we have $$\Gamma = \sup_{\alpha \geq 1} \left|\int_1^\alpha \frac{\hat{f}(\xi)}{\xi} \, d\xi\right| < \infty.$$ Now, $$\left\|\frac{\hat{f}(\xi)}{\xi}\right\| = \int_{-\infty}^{\infty} \left|\frac{\hat{f}(\xi)}{\xi}\right| \, d\xi= 2 \int_0^\infty \frac{\hat{f}(\xi)}{\xi} \, d\xi \\= 2 \left(\int_0^1  \frac{\hat{f}(\xi)}{\xi} \, d\xi + \int_1^\infty  \frac{\hat{f}(\xi)}{\xi} \, d\xi\right) \leq 2\int_0^1  \frac{\hat{f}(\xi)}{\xi} \, d\xi+2\Gamma.$$ I am unsure how to proceed from here. I have not made use of assumption that $\hat{f}'(0)$ exists, but I am not sure how to use it. As a follow-up: seems like a lot of questions on here have talked a lot about functions being odd and considering the Fourier Transform. Is there a particular reason, or is the theory for even functions simplified?","['integration', 'fourier-analysis', 'even-and-odd-functions', 'fourier-transform', 'real-analysis']"
4329081,Characterization of a joining over a common subsystem.,"Given a joining measure $\lambda$ on $X\times Y$ , where $(X,\mu)$ and $(Y,\nu)$ are two probability measure space, let $\lambda=\int (\lambda_y \times \delta_y)\,d\nu(y)$ be the disintegration of $\lambda$ over $\nu$ . We denote by $P_{\lambda}:L^2(X,\mu)\rightarrow L^2(Y,\nu)$ the conditional expectation operator given by $$(P_{\lambda}f)(y)=\mathbb{E}_{\lambda}^Y(f)(y)=\int_xf(x)\,d\lambda_y(x) \text{ for } \nu\text{-a.e.} y.$$ In Theorem $6.8$ of the book 'Ergodic theory via joining' by Glasner, the author says that if $\lambda=\mu\times_{\eta}\nu$ , where $(Z,\eta)$ is the common factor of $(X,\mu)$ and $(Y,\nu)$ determined by $\lambda$ , then the operator $P_{\lambda}:L^2(X,\mu)\rightarrow L^2(Y,\nu)$ is the projection onto $L^2(Z,\eta)$ . My doubts are the following: What does the author mean by $(Z,\eta)$ to be the common factor of $(X,\mu)$ and $(Y,\nu)$ determined by $\lambda$ ? How can we show that if $\lambda=\mu\times_{\eta}\nu$ , then $P_{\lambda}=P_Z$ , i.e. the projection onto $L^2(Z,\eta)$ ? $\big($ Note that, if $\mu=\int_z\mu_z\,d\eta (z)$ and $\nu=\int_z \nu_z\,d\eta (z)$ be the disintegrations of $\mu$ and $\nu$ , respectively with respect to $\eta$ , then $\mu\times_{\eta}\nu:=\int_z(\mu_z\times\nu_z)d\eta(z)$$\big)$ . Thanks in advance for any help.","['measure-theory', 'lebesgue-measure', 'ergodic-theory', 'conditional-expectation', 'functional-analysis']"
4329103,Calculation about Chern character in a special setting,"I'm confused with working out the Chern character in the following special setting. Let $E$ be a spinor bundle $$S=P_{Spin(2n)}(S^{2n})\times_\rho \mathbb{C}^{2n}$$ over sphere $S^{2n}$ , where $\rho$ is the natural left action. We all know that there is a splitting $$S=S^+\oplus S^-$$ by the volume element. Now I want to compute the Chern character $\rm {ch}(S^+)$ of $S^+$ via Euler class $e(TS^{2n})$ of $S^{2n}$ . My thought is as follow: By splitting principle, we may assume $$TX=V_1\oplus \cdots\oplus V_n , \qquad V_j=L_j\oplus \overline{L}_j  $$ here $V_j$ is a 2-plane bundle over $S^{2n}$ and $L_j$ is a line bundle and $\overline{L}_j$ is its conjugate line bundle. Then the spinor bundle has the splitting $$S(TS^n)=S(V_1)\oplus S(V_2)\oplus \cdots \oplus S(V_n),$$ So the half spinor bundle has the splitting $$S^+(TS^n)=S^+(V_1)\oplus S^+(V_2)\oplus \cdots \oplus S^+(V_n)$$ And it's not difficult that we have $$L_j=S^+(V_j)\otimes S^+(V_j), \quad \overline{L}_j=S^-(V_j)\otimes S^-(V_j)$$ Therefore $$\rm{ch}(S^+(V_j))=\left(\rm {ch}(L_j)\right)^{1/2}=e^{x_j/2},\quad \text{here the Chern root $x_j=c_1(L_j)$} $$ $$\rm {ch}(S^+(TS^{2n}))=\sum_{j=1}^n \rm{ch} (S^+(V_j))=\sum_{j=1}^n e^{x_j/2}=\sum_{j=1}^n\sum_{k=0}^{\infty}\dfrac{1}{2^kk!}x_j^k\in H^{*}(S^{2n},\mathbb{Z})$$ and the sphere only has $0$ -th cohomology and the top cohomology, so the above equation just remains that $$\sum_{j=1}^n\left(1+\dfrac{1}{2^n n!}x_1\cdots x_n\right)=n+\dfrac{1}{2^n (n-1)!}x_1\cdots x_n$$ In additionally, We have easily that $$e(TS^{2n})=\rm x_1\cdots x_n$$ Hence $$ \rm {ch}(S^+(TS^{2n}))=n+\dfrac{e(TS^{2n})}{2^n (n-1)!} $$ And it also tells us that $$c_n(S^+(TS^{2n}))=\dfrac{\chi(S^{2n})}{2^n}$$ I'm not sure whether my calculation is right. If not, what's the expression of $\rm {ch}(S^+(TS^{2n}))$ via $e(TS^{2n})$ ? Could you please tell me this stuff?  Thanks in advance","['riemannian-geometry', 'complex-geometry', 'geometry', 'characteristic-classes', 'differential-geometry']"
4329148,Find largest ellipse in basin of attraction,"Consider the following system: $$
\begin{align}
x' &= -x+y^2 \\
y' &= -2y + 3x^2 \\
\end{align}
$$ I am asked to find the largest ellipse of the form: $$ V(x,y) = \tfrac{x^2}{2} + \tfrac{y^2}{4} = r$$ contained in the region of attraction of the system. We can use $V(x,y)$ as a Lyapunov function of the system and we get: $$V'(x,y) = xx' + \tfrac{y}{2}y' = -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y$$ I deduced we are looking for $r$ such that: \begin{align}
r &= \min_{(x,y) \neq (0,0)} \tfrac{x^2}{2} + \tfrac{y^2}{4}\\
&\text{ s.t } -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y=0 
\end{align} I have not been able to solve this problem analytically using the Lagrangian (a 3rd-degree polynomial comes up). Wolfram Alpha solution seems to be right ( $r \simeq 0.4782$ ) when plotted though: Is this way of reasoning sound? Is there some simplification that allows us to get an analytical solution?
Thanks in advance EDIT: I have found the answer in the book, apparently the correct result is $\frac{1}{9}$ . So there must be an error in my reasoning I cannot find. The book offers no solution, so I still don't know how to correctly solve the exercise.","['ordinary-differential-equations', 'dynamical-systems']"
4329155,An application of Baire Category Theorem,"I am trying to prove a proposition that $BV[a.b]\cap C[a.b]$ equipped with the $||\cdot||_\infty$ is Baire 1 category set, which will tell us that $E=\{f:V(f)=\infty, f\in C[a,b]\}$ is a dense Baire 2 category set in $C[a.b]$ . My attempt: I define $F_n=\{f: V(f)\leq n, f\in C[a,b]\}$ , then we know that $\cup_{n=1}^{\infty}F_n=BV[a.b]\cap C[a.b]$ . I am trying to show that this is a Baire 1 category set, then we are done. In order to show that, we just need to prove the following: 1. $F_n$ is closed.
2. $F_n$ has no interior point for every n. I have figured out the second claim by using sawtooth functions, but I have some problems when i try to prove the first claim. We suppose $f_n\rightarrow f$ uniformly, then by the definition and some easy calculation, we know that for every $\epsilon>0$ , there exists a $m_0$ such that $V(f)\leq V(f_{m_0})+2n\epsilon$ , where $n$ is the number of partition (where $a=x_0\leq x_1\leq \cdots\leq x_{n}=b$ ). So when n goes larger and larger, we can't give an estimation for $V(f)$ , this is why i get confused. My questions: $F_n$ is closed or not? if so, how to prove that? if not so, how do we prove the proposition at first? Any help will be truly grateful.","['bounded-variation', 'baire-category', 'functional-analysis', 'real-analysis']"
4329161,Integrating $\int \frac{dx}{x^{11}\sqrt{1+x^4}}$,"Find $$I=\int \frac{dx}{x^{11}\sqrt{1+x^4}}$$ With $x^2=t$ , we get $I=\int _{ }^{ }\frac{d\ \sqrt{t}}{\left(\sqrt{t}\right)^{11}.\sqrt{1+t^2}}=\frac{1}{2}\int _{ }^{ }\frac{dt}{t^6\sqrt{1+t^2}}$ Now, with $t= \tan k$ we get $I=\frac{1}{2}\int _{ }^{ }\frac{\cos ^5k.\ dk}{\sin ^6k}$ . At this step, I can not do anything more. Can you show me the way to solve this? I try to go with this way but it doesn't work.","['integration', 'calculus', 'trigonometry']"
4329166,When is the projection of an ellipsoid a circle?,"Consider an ellipsoid in the three dimensional Euclidean space, say $$\frac{x^2}{a^2}+\frac{y^2}{b^2} + \frac{z^2}{c^2} =1 $$ where $a$ , $b$ , $c$ are positive reals.
I'm counting the number of planes through the origin so that the image is a perfect circle. There may be divergent cases if we consider the case that some of $a$ , $b$ , $c$ are coincide. But at first, let us focus on the case that $a$ , $b$ , $c$ are all different, say $a>b>c$ . I guess the answer would be $4$ . I have made many efforts but failed.
What I have observed is the that at least two such planes exists and the radius of the circle is $b$ . Just consider rotating plane possesses $y$ axis and apply intermediate value theorem. Causion! We are concerning projection, not intersection. PS. Now I guess there are infinitely many... PS2. According to one suggested answer, there are just two such planes for the non-degenerate case. I'm checking if it is correct. PS3. Another opinion appeared that the selected answer may have fault. And it seems making sense. I think somewhat stronger analysis is required. PS4. The above PS3 is about another answer which now have disappeared.","['euclidean-geometry', 'linear-algebra']"
4329175,linear 4×4 system of nonhomogeneous differential equations with repeated eigenvalues.,"I'm trying to find the general solution of this system of linear Nonhomogeneous differential equation: $$X'=\begin{bmatrix}
-2 &1  &1 &-1\\ 
-1 & -1 & 1 &0\\ 
0 &0  &-1 &0\\
0 &-1  &0 &0
\end{bmatrix}X + 
\begin{bmatrix}
e^t \\ 
2t \\ 
e^{-2t} \\
0 
\end{bmatrix}
$$ as an aproach, I found the eigenvalue $-1$ with multiplicity of 4.
And I found two eigenvectors: $$V^{(1)}=\begin{bmatrix}
1 \\ 
0 \\ 
1 \\
0 
\end{bmatrix}$$ and $$V^{(2)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}$$ so i got two answers for the Corresponding homogeneous equation: $$X^{(1)}=\begin{bmatrix}
1 \\ 
0 \\ 
1 \\
0 
\end{bmatrix}e^{-t}$$ and $$X^{(2)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}e^{-t}$$ since we have 2 eigenvectors and one eigenvalue with multiplicity of 4, I tried to find the other two answers by assuming them as $X=vte^{-t} + ηe^{-t}$ and I got: $X^{(3)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}te^{-t} + \begin{bmatrix}
0 \\ 
0 \\ 
1 \\
1 
\end{bmatrix}e^{-t}$ $X^{(4)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}te^{-t} + \begin{bmatrix}
-1 \\ 
-1 \\ 
0 \\
0 
\end{bmatrix}e^{-t}$ I wanna know if the Answers I found are correct.
if Yes, we have: $$X=\begin{bmatrix}
e^{-t} &0  &0 &-e^{-t}\\ 
0 & e^{-t} & te^{-t} &te^{-t}-e^{-t}\\ 
e^{-t} &0  &e^{-t} &0\\
0 &e^{-t}  &te^{-t}+e^{-t} &te^{-t}
\end{bmatrix}$$ using the Variation of Parameters method, we have to inverse X to find the answer but A is singular $(detA=0)$ :( where am I going wrong? can someone help me?","['systems-of-equations', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4329197,"For which $p \in [1, \infty]$ the sequence is Cauchy","Let $f_n (x) = \frac{n}{1+n\sqrt{x}}$ , for $x \in (0,1)$ . I am asked to find for which $p \in [1, \infty]$ $(f_n)_n$ is a Cauchy sequence with respect to $L^p$ norm. First of all, I observed that $(L^p(0,1), ||\cdot||_{L^p})$ is a Banach space for all $1 \le p \le \infty$ . Thus, $(f_n)_n$ is a Cauchy sequence $\Leftrightarrow$ $(f_n)_n$ is convergent in $L^p(0,1)$ . Moreover, since $\lim_{n \rightarrow \infty} f_n (x) = f(x) := \frac{1}{\sqrt{x}} \in L^p(0,1), 1 \le p < 2$ , for all $x \in (0,1)$ , the strong limit - if it exists - must coincide with $f$ . The case $p=1$ is easy, as it is possible to show that $||f_n - f||_{L^1} = \int_{0}^{1} \frac{1}{\sqrt{x} + nx} dx = \frac{2 log(n+1)}{n} \rightarrow 0$ as $n$ approaches infinity. Hence, the sequence being convergent in $L^p(0,1)$ is Cauchy. For the case $1 < p < \infty$ (and similarly for $p=\infty$ ), I proceeded in a similar way. However, in this case I end up with an integral that does not converge for all $n \in \mathbb{N}$ , that is $||f_n - f||_{L^1} = \int_{0}^{1} \frac{1}{\sqrt{x} + nx}^p dx = \left.\frac{1}{n} \frac{(t + n t^2)^{1-p}}{p-1}\,\right|_{0}^{1} \rightarrow \infty$ as $x \rightarrow 0$ . Is it correct to state that in this case the sequence does not converge (and thus it can't be a Cauchy sequence)? My doubt here concerns the fact that I am not able to estimate this integral with something that is convergent as $x$ approaches $0$ and hence I cannot take the limit for $n \rightarrow \infty$ and conclude. If anyone could give me a feedback on my reasoning or suggest a more straight-forward way to solve the problem, it would be greatly appreciated.","['cauchy-sequences', 'lp-spaces', 'functional-analysis']"
4329232,In how many ways can 4 males and 4 females sit together in a round table with such that there are exactly 3 males who are seated next to each other?,"I have tried using this method:
first I took 4C3 to choose 3 men to be seated together, then I multiplied by 3! for the 3 men to switch among themselves. After which I grouped the 3 men into one unit. Counting the other 4 women and the remaining 1 men, I should have 6 units. Since this is a round table, I took (6-1)! to arrange this 6 units. I realised that this gives the possibility of including the case where 3 and 4 men could be seating next to each other since that remaining 1 men can be seating next to one of the 3 men in the rotation, so I subtracted the case where there are exactly 4 men who sits next to each other. However, I do not get the answer. Can anyone advise? below is my working: $4C3 \times 3! \times (6-1)!-(5-1)! \times 4! = 2304$ . The correct answer is $1728$ . I tried other methods and I got the answer but I don't understand why this does not work.","['combinations', 'solution-verification', 'combinatorics']"
4329343,Can the product of some matrices equal the identity matrix?,"Let $A=\begin{pmatrix} 1 & 2 \\ 0 & 1\end{pmatrix}$ and $B=\begin{pmatrix} 1 & 0 \\ -2 & 1\end{pmatrix}$ . Can a product $X_1 X_2...X_n$ be equal to the identity matrix if every factor $X_i$ equals either $A$ or $B$ ? I believe that the answer is negative, but I don't really know how to prove it. I thought of two approaches: We could try doing this by induction. The base case is trivial since neither of $A$ nor $B$ are the identity matrix. However, I don't know how to go from $n$ to $n+1$ . Maybe we should use the fact that if $U, V$ are two square matrices such that $UV$ is the identity, then $VU$ is also the identity. I guess that we should somehow shuffle the order of the factors using this observation, but then again, I don't know how to use this.","['matrices', 'linear-algebra', 'contest-math']"
4329422,Maximize the variance of a function of random variable,"I have a function $f(X)=\exp\left(\frac{-\gamma^2}{a^2X+b^2}\right)$ where $X \sim \mathrm{Binomial}(n,p)$ .  I am interested in finding the value of $\gamma^2$ which maximizes the variance of $f(X)$ for given values of $a$ and $b$ . Definition of variance : $$
\operatorname{var}(f(X))=\mathrm{E}\left(f^2(X)\right)-\left(\mathrm{E}\bigl(f(X) \bigl) \right)^{2}
$$ From Expectation of Function of Discrete Random Variable: $$E\left(f^2(X)\right)=\sum_{k = 0}^{n} \exp\left(\frac{-2\gamma^2}{a^2k+b^2}\right) {n \choose k} p^{k} (1-p)^{n-k}$$ $$E\left(f(X)\right)=\sum_{k = 0}^{n} \exp\left(\frac{-\gamma^2}{a^2k+b^2}\right)  {n \choose k} p^{k} (1-p)^{n-k}$$ To me, it seems NOT straightforward how to solve $\mathrm{d}[\operatorname{var}(f(X))]/\mathrm{d}\gamma=0$ to find an expression for optimal $\gamma^2$ . Hence, I am thinking of at least finding a numerically accurate solution through curve-fitting. My aim is to come up with a ""working formula"" for optimal $\gamma$ in terms of the other parameters. Also, I have MATLAB and I am wondering how formulate this problem and find an expression using MATLAB as well. Some context on $f(X)$ : $f(X)$ comes from the complementary CDF of an exponential distribution. There are $X$ signal components with power $a^2$ and a non-signal component with power $b^2$ . Also, $\gamma^2$ is a threshold at which we evaluate the function.","['probability-distributions', 'optimization', 'numerical-methods', 'probability', 'random-variables']"
4329429,Proof of formula involving the Haar measure of SU(2).,"I would like to verify that $$\int_{\mathrm{SU}(2)}\mathrm{d}g\,\delta(g)=1$$ where the ""delta-function"" is defined via $$\delta(g):=\sum_{j\in\mathbb{N}_{0}/2}(1+2j)\chi^{j}(g)$$ where $\chi^{j}$ are the characters of the irreducible unitary spin-j representation of $\mathrm{SU}(2)$ with dimension $2j+1$ . My attempt: The normalized Haar measure of SU(2) can be written in terms of Euler angles $\alpha,\beta,\gamma$ as $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,f(g)=\frac{1}{8\pi^{2}}\int_{0}^{2\pi}\,\mathrm{d}\alpha\,\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\int_{0}^{2\pi}\,\mathrm{d}\gamma\,f(\alpha,\beta,\gamma).$$ Furthermore, the characters $\chi_{j}$ can be written as $$\chi^{j}(g(\alpha,\beta,\gamma))=\frac{\mathrm{sin}((2j+1)\beta/2)}{\mathrm{sin}(\beta/2)}$$ according to this wikipedia article . Hence, we have to show that $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg )\overset{!}{=}1$$ (Here, I also rescaled the sum 2j $\to$ j). Now, since I am a physicist, I am allowed to change the infinite sum with the integral (maybe the error lies there; maybe I should check with dominated convergence theorem). Let us look at the integrals appearing in the expression above: $$\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}=\begin{cases}2 &\text{if j=0}\\ 4\frac{1+j-\mathrm{cos}(j\pi/2)}{j(2+j)} &\text{else}\end{cases}$$ Hence, I get something like $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=1+\sum_{j=1}^{\infty}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)}$$ However, the series on the right-hand side is clearly not convergent: A quick mathematica calculation shows $$\sum_{j=1}^{N}\frac{j+1}{2}\frac{4(1+j-\mathrm{cos}(j\pi/2))}{j(2+j)}=\begin{cases}\frac{241}{11}\cong 22 &\text{N=10}\\\frac{1040350}{5151}\cong 202 &\text{N=100}\\\frac{1004003500}{501501}\cong 2002 &\text{N=1000}\\\vdots\end{cases}$$ I think the error lies indeed in exchanging the series with the integral. If I look at the series in $$\int_{\mathrm{SU}(2)}\,\mathrm{d}g\,\delta(g)=\int_{0}^{\pi}\,\mathrm{d}\beta\,\mathrm{sin}(\beta)\,\bigg (\sum_{j=0}^{\infty}\frac{j+1}{2}\frac{\mathrm{sin}((j+1)\beta/2)}{\mathrm{sin}(\beta/2)}\bigg )$$ we have that $$\sum_{j=0}^{\infty}(j+1)\mathrm{sin}((j+1)\beta/2)=\sum_{j=1}^{\infty}j\cdot \mathrm{sin}\bigg(j\cdot\frac{\beta}{2}\bigg )$$ The series on the right-hand side seems to be zero, when evaluated with Mathematica, but I can't see why this is the case...","['integration', 'definite-integrals', 'representation-theory', 'haar-measure', 'lie-groups']"
4329439,Relation between absolute continuity with respect to lebesgue measure and integral,"Let $f:X \to \mathbb{R}$ be a continuous function on a compact metric space $X$ . Assume that a Borel probability measure $\mu$ is absolutely continuous with respect to Lebesgue measure $\text{Leb}.$ Is it true that if $f(x)<0$ for $\text{Leb}$ a.e. $x$ , then $\int f d\mu<0?$ I think it should be true as $\mu << \text{Leb}$ . Attempt: Assume that $\int fd\mu\geq 0$ . Then $f\geq 0$ $\mu$ a.e. $x$ . That means $\mu(\{x: f(x)>0\})>0$ . That implies $\text{Leb}((\{x: f(x)>0\})>0$ which is not true.","['measure-theory', 'functional-analysis']"
4329467,"Let $A_1$, $A_2$, $P$ be CPOs and let $\psi: A_1 \times A_2 \to P $ be a map, then $\psi$ is continuous $\iff$ it is so in each variable separately","From the exercise 8.7 by B. A. Davey, H. A. Priestley Introduction to lattices and order Let $A_1$ , $A_2$ , $P$ be CPOs and let $\psi: A_1 \times A_2 \to P $ be a map. $\forall a \in A_1$ and $\forall b \in A_2\,\,$ let $\,\,\,\psi^a:A_2 \to P\,\,$ s.t. $\,\,\psi^a(v)= \psi(a,v)\,\,\,$ and $\,\, \psi_b:A_1 \to P\,\,$ s.t. $\,\, \psi_b(u) = \psi (u,b)$ . $\,\,\,\,$ Prove that $\psi$ is continuous (that is iff it preserves the suprema of directed sets) $\iff \psi^a$ and $\psi_b$ are continuous. Proving this $\Rightarrow$ direction is quite easy, but I am not sure how to conclude the opposite $\Leftarrow$ direction: let $D \in A_1 \times A_2$ be a directed set, then $D = D_1 \times D_2$ , with $D_1$ directed in $A_1$ and $D_2$ directed in $A_2$ . If $\bigvee D = (d_1, d_2) = \bigvee(D_1 \times D_2)$ , we have: $\psi(\bigvee D) = \psi(d_1,d_2) = \psi^{d_1}(d_2) = \psi_{d_2}(d_1)$ , where $$\psi^{d_1}(d_2) = \psi^{\bigvee D_1}(\bigvee D_2) = \bigvee \psi^{\bigvee D_1}(D_2)= \bigvee \psi(\{\bigvee D_1\}\times D_2)$$ and, similarly, $$\psi_{d_2}(d_1) = \psi_{\bigvee D_2}(\bigvee D_1) = \bigvee \psi_{\bigvee D_2}(D_1)= \bigvee \psi(D_1\times \{\bigvee D_2\})$$ how to conclude that $\psi(\bigvee D) = \bigvee \psi(D)$ ?","['lattice-orders', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4329477,"If $a^2 = b^2$ in a field, then $a = b$ or $a = -b$","I'm trying to prove that if $a^2 = b^2$ in a field , then $a = b$ or $a = -b$ I know that a field is a commutative, division ring by definition. Hence if $a^2 = b^2$ in a field, then we have $a^2 - b^2 = 0$ where we can say $a^2 - b^2 = (a - b)(a+b)$ because $$(a-b)(a+b) = a^2 +ab - ba - b^2 = a^2 - b^2$$ where $ab = ba$ by commutativity. Hence we can say that $(a - b)(a + b) = 0$ , which is only true if $a - b = 0$ or $a + b = 0$ . If the first is true, then $a = b$ and if the second is true $a = -b$ . Hence proved. Is this the correct approach?","['field-theory', 'ring-theory', 'abstract-algebra']"
4329478,Existence of “sufficiently rich” unions,"Let $m$ be a positive integer and define $M\equiv\{1,\ldots,m\}$ . Let $A_1,\ldots,A_m$ be (not necessarily disjoint and potentially empty) finite sets. Moreover, let $c_1,\ldots,c_m$ be non-negative integers. For each $i\in M$ , $A_i$ may or may not contain precisely $c_i$ elements, but assume that the following is true (here, $\#$ denotes cardinality): $$\#\left(\bigcup_{i\in M} A_i\right)=\sum_{i\in M}c_i.$$ Conjecture: There exists a non-empty index subset $T^*\subseteq M$ with the following properties: $\#\left(\bigcup_{i\in T} A_i\right)\geq\sum_{i\in T}c_i$ whenever $T\subseteq T^*$ ; and $\#\left(\bigcup_{i\in T} A_i\right)\geq\sum_{i\in T}c_i$ whenever $T\supseteq T^*$ . In words, I am looking to establish the existence of an index set $T^*$ such that if the index set $T$ is either a subset or a superset of $T^*$ , then the union of the $A_i$ ’s over $T$ contains at least as many elements as the sum of the $c_i$ ’s over $T$ . Any references to a proof or hints about a counterexample would be appreciated. UPDATE: Cases for a small number of sets are simple enough to be checked “manually.” When $m=1$ or $m=2$ , the conjecture is trivial and very easy, respectively, to prove. When $m=3$ , a systematic analysis of all seven ( $2^3-1$ ) non-empty subsets of $\{1,2,3\}$ reveals that the conjecture holds true as well. I am still struggling to come up with a proof or counterexample when $m\geq 4$ .","['elementary-set-theory', 'combinatorics', 'discrete-mathematics']"
4329499,Example of a sequence on an infinite-dimensional vector space with respect to different norms [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am stuck with the following problem: Give example of an infinite dimensional vector space V and two norms $\theta$ and $\rho$ on V and the sequence $\{x_{n}\}_{n\geq 1}$ of V such that: The sequence $\{x_{n}\}_{n\geq 1}$ is cauchy in $(V, \rho)$ The sequence $\{x_{n}\}_{n\geq 1}$ is not cauchy in $(V, \theta)$ It is easy to find a vector space, a norm, and a sequence that meet one of the items but I do not know how to proceed. Any help is appreciated.","['cauchy-sequences', 'analysis']"
4329664,Behaviour of the solutions of a system of differential equations at infinity.,"Consider the following system of differential equations: $$
\begin{cases}
u'=-u+uv\\
v'=-2v-u^2
\end{cases}
$$ I'm able to prove that solutions must tend to $0$ if $t\to 0$ by the use of Lyapunov function $L(x,y)=x^2+y^2$ but I'm unable to prove that the function: $$
I(t)=\frac{x^2(t)+2y^2(t)}{x^2(t)+y^2(t)}
$$ must admit limit for $t\to+\infty$ Any hint for a solution? Thanks in advance.","['lyapunov-functions', 'systems-of-equations', 'ordinary-differential-equations', 'real-analysis']"
4329711,Does adding to degree n polynomial terms with negative exponents still have n roots?,"A polynomial has n roots. You can add terms less than n and still have a polynomial with n roots. Does this continue if you take negative exponents? Eg. $$ax^2 + bx^1 + cx^0 + dx^{-1}...$$ Edit: I guess this isn't a polynomial per se, but does it still have n roots?","['algebra-precalculus', 'roots', 'polynomials']"
4329736,Light bulbs in a high dimensional grid,"Consider a $n \times n \times n$ array of light bulbs. In each step, one can flip lights from on to off and off to on along a 1d row in the $x$ -, $y$ - or $z$ -axis. Suppose one has a configuration that can switch all the lights off. Show that one can do so in $3\text{(# of lights that are on)}/n$ steps. More generally, for a $d$ -dimensional array of light bulbs with width $n$ , if one again can flip a 1d row of lights, show that one can switch off in $d\text{(# of lights that are on)}/n$ steps. === I’m 90% confident that this is true from examples. Note that this bound is tight in the sense that for all $d$ , there exists $n$ and a configuration where one needs to perform $d\text{(# of lights that are on)}/n$ steps. For example, when $n=2$ and 2 lights are on are at $0^d$ and $1^d$ , one needs to perform $d$ flips. There are other examples too.","['puzzle', 'combinatorics', 'coding-theory']"
4329766,"If $A\cap B=\emptyset$ and $A$ is an open set, then $\overline A\cap \operatorname{Int} (\overline B)=\emptyset$??","A and B are subsets of a topological space. How do i prove that if $A \cap B = \emptyset$ and A is an open set, then $\overline A \cap Int(\overline B) = \emptyset $ ? Here's my part of proof: $A \cap B = \emptyset \rightarrow A \cap \overline B = A \cap (B \cup \partial B) = (A \cap B) \cup (A \cap \partial B) = \emptyset \cup (A \cap \partial B) = A \cap \partial B$ $Int (A \cap \overline B) = Int (A \cap \partial B) = Int A \cap Int (\partial B) =$ (since A is open) $= A \cap \emptyset = \emptyset$ $Int (A \cap \overline B) = Int A \cap Int (\overline B) = A \cap Int (\overline B)$ $A \cap Int(\overline B) = \emptyset$ $\overline A \cap Int(\overline B) = (Int (A) \cup \partial A) \cap Int(\overline B) = (A \cup \partial A) \cap Int(\overline B) = (A \cap Int(\overline B)) \cup (\partial A \cap Int(\overline B)) = \emptyset \cup (\partial A \cap Int(\overline B)) = \partial A \cap Int(\overline B)$",['general-topology']
4329773,How do you write $\sin(2x)$ in terms of $\tan x$?,"I want to write $\sin 2x$ in terms of $\tan x$ . Here is my progress so far: $$\begin{align}\sin 2x &= 2\sin x\cos x\\
&=2\tan x\cos^2x\\
&=2\tan x \cdot \frac{1+\cos2x}2\\
&=\tan x(1+\cos2x)\\
&=\tan x\cot^2x \cdot (1-\cos2x)\\
&=\frac{1-\cos2x}{\tan x}\end{align}$$ As it seems, I cannot seem to fully convert everything into $\tan x$ . These are not all my steps, as I try many other things, but it always results in extra $\cos2x$ , $\cos^2x$ , or some other trig function that wont easily convert into $\tan x$ . I need some insight on how to continue.","['algebra-precalculus', 'trigonometry']"
4329818,A question about sigma-algebras: $\sigma(Y+f(X))= \sigma(Y+X)$ if $\sigma(f(X))= \sigma(X)$?,"Let $X,Y$ be two independent real random variables. Let $f$ be an injective mesurable real function, which means $\sigma(f(X))= \sigma (X)$ , see When do we have $\sigma(X)= \sigma (f(X))$? Is it true that we also have $\sigma(Y+f(X))= \sigma(Y+X)$ ? Does the fact that we have independence of $X$ and $Y$ play a role ?","['measure-theory', 'probability-theory', 'probability', 'real-analysis']"
4329857,Why does Inverse of Linear/Linear Function have same structure as Inverse of Matrix,"The inverse of a $\frac{\text{linear}}{\text{linear}} $ function $$f(x)=\frac{ax+b}{cx+d}\implies f^{-1}(x)=\frac{dx-b}{-cx+a}, ad-bc\neq0$$ has a very similar structure to that of the inverse of a matrix (one is a multiple of the other) $$X=\begin{bmatrix}a & b\\c & d\end{bmatrix}\implies X^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\-c & a\end{bmatrix}, ad-bc\neq0$$ I understand these two properties, and their proofs, individually .I understand how to use matrices as representations of systems of linear equations, but am not sure that is the correct interpretation here. What would be an appropriate interpretation of the matrix, and what causes this connection?","['contest-math', 'projective-geometry', 'linear-algebra', 'inverse-function']"
4329875,On the complex matrix equation $AX-XA=B$,"I want to show that there exists solution to the matrix equation $AX-XA=B$ if and only if $$
\begin{pmatrix}
A&0\\
0&A
\end{pmatrix},
\begin{pmatrix}
A&B\\
0&A
\end{pmatrix}
$$ are similar, where all matrices ( $A,B,X$ ) are complex and $A,B$ are $n\times n$ . The sufficiency is easy to show, while I could only manage to show the necessity in some special cases. I also noticed that this is in fact a special case of a much more general Roth's theorem, however I am more interested in this particular question (i.e. how is this equation special). Is there a proof to show the necessity without (over) generalizing?","['matrices', 'jordan-normal-form', 'matrix-equations', 'linear-algebra']"
4329877,Taylor expansion of $\sin \pi z$ at $z = -1$.,"Taylor expansion of $\sin \pi z$ at $z = -1$ is $$\sin\pi z = -\sin(\pi(z+1)) = -\sum_{n=0}^\infty \frac{(-1)^n\pi^{2n+1}}{(2n+1)!}(z+1)^{2n+1}$$ so that $$\sin\pi z = \sum_{n=0}^\infty \frac{(-1)^{n+1}\pi^{2n+1}}{(2n+1)!}(z+1)^{2n+1}. \tag{$\dagger$}$$ But if I try this \begin{align}
\sin\pi z  & = \sum_{n=0}^\infty \frac{(-1)^n\pi^{2n+1}}{(2n+1)!}z^{2n+1} \\& = \sum_{n=0}^\infty \frac{(-1)^n\pi^{2n+1}}{(2n+1)!}(z+1-1)^{2n+1}  \\ & = \sum_{n=0}^\infty \frac{(-1)^n\pi^{2n+1}}{(2n+1)!}\left(\sum_{k=0}^{2n+1}\binom{2n+1}{k}(z+1)^k(-1)^{2n-k+1}\right).\tag{$\dagger^*$}
\end{align} In this case, how can I reduce $(\dagger^*)$ as the above form $(\dagger)$ ? Just direct calculation?","['trigonometry', 'taylor-expansion']"
4329911,How to interpret Carathéodory's theorem for a twice differentiable function?,"Let $ f(x) = |x|^3 $ , compute $f'(0),f''(0),f'''(0)$ . I was trying to use Carathéodory's theorem to prove that the function is differentiable. Finding $f'(0)$ We know that the function is differentiable at $x=0$ if there exists a $f_1(x)$ continuous at $x=0$ such that $$f(x) - f(0) = f_1(x)(x-0) \cdots (*)$$ Here $f(0) = 0$ . Also we know that $f(x) = x^3, x \ge 0 $ and $f(x) = -x^3,x < 0 $ Then $f_1(x) = x^2 ; x \ge 0 $ and $f(x) = -x^2; x < 0$ . This function $f_1(x)$ is continuous at $x = 0$ and also satisfies $(*)$ Hence $f'(0) = f_1(0) = 0$ Finding $f''(0)$ This is where I am confused. To show that $f''(0)$ exist using the Carathéodory's theorem should I show that there is a function $f_2(x)$ continuous at $x= 0 $ such that $f_1(x) - f_1(0) = f_2(x)(x-0)$ or should I show that the function $f_2(x)$ satisfies $f'(x) - f'(0) = f_2(x)(x-0) ?$","['derivatives', 'real-analysis']"
4329936,"For each number field $K$ with place $\mathfrak{p}$, prove that $K_{\mathfrak{p}}^{\mathrm{ab}} = K_{\mathfrak{p}} K^{\mathrm{ab}}$","As described in the title, I want to show that for each number field $K$ with place $\mathfrak{p}$ , $K_{\mathfrak{p}}^{\mathrm{ab}} = K_{\mathfrak{p}} K^{\mathrm{ab}}$ . Here $K_{\mathfrak{p}}$ is the completion of $K$ with respect to the place $\mathfrak{p}$ , and the superscript ${}^{\mathrm{ab}}$ means taking maximal abelian extension. I was given the hint to use (5.6) and (5.8) (in Chapter VI) in Neukirch's book Algebraic number theory . (5.6) If $L|K$ is an abelian extension and $\mathfrak{p}$ is a place of $K$ , then the diagram \begin{array}{c}
K_{\mathfrak{p}}^{\times} & \xrightarrow{(-, L_{\mathfrak{p}}|K_{\mathfrak{p}})}
) & \mathrm{Gal}(L_{\mathfrak{p}}|K_{\mathfrak{p}}) \\
 \downarrow{[-]} & & \downarrow{\mathrm{incl.}} \\
 C_K = \mathbb{I}_K/K^{\times} & \xrightarrow{\varphi} & \mathrm{Gal}(L|K)
\end{array} is commutative. Here the horizontal maps are norm residue symbols, and the left vertical one is the canonial injection $$[-]: K_{\mathfrak{p}}^{\times} \rightarrow C_K; \quad\quad a_{\mathfrak p} \mapsto \text{ the class of }(1, \ldots, 1, a_{\mathfrak p}, 1, \ldots, 1).$$ (5.8) : For every finite abelian extension, one has $\mathcal{N}(C_L) \cap K_{\mathfrak{p}}^{\times} = \mathcal{N}_{\mathfrak{p}} L_{\mathfrak{p}}^{\times}$ . Here we identify $K_{\mathfrak{p}}^{\times}$ with its image in $C_K$ under the map $[-]$ . We use the abbreviations $\mathcal{N} = N_{L|K}$ and $\mathcal{N}_{\mathfrak{p}} = N_{L_{\mathfrak{p}} | K_{\mathfrak{p}}}$ . The two propositions provide the local-global compatibility of class field theories. My question is how to prove $K_{\mathfrak{p}}^{\mathrm{ab}} = K_{\mathfrak{p}} K^{\mathrm{ab}}$ . My attempts : After discussing with a friend of mine, we have a vague idea: We start from any norm group $N$ (i.e. open normal subgroups of finite index) in $K_{\mathfrak{p}}^{\times}$ , then via the local existence theorem, there exists an extension of $K_{\mathfrak{p}}$ with norm group $N$ . Now applying $[-]$ to the global, we obtain a norm group in the global $C_K$ , which by global existence theorem corresponds to an extension $L$ of $K$ . Then maybe we can use (5.8) to say that the local field correspond to $N$ is the completion $L_{\mathfrak p}$ . Then somehow we have a correspondence between the abelian extensions of $K_{\mathfrak p}$ and the abelian extensions of $K$ . But we don't know how to carry on (or use (5.6) ). Meanwhile, the above attempts are quite vague at some points. So how to clean this up? Thank you all for answering and commenting! I'm a new learner on class field theory and sorry for such a possiblely trivial question. By the way, I'm still quite puzzled how to really play with CFT well. Though I have gone through the proof of CFT via Milne's note, it seems that I have learnt nothing except calculating various cohomology groups and chasing on diagrams (Of course, Milne's note is awesome! It is only the silly of me). When given an algebraic number theory problem, I still cannot handle them with simply the statements of CFT. It makes me feel quite frastrated. :(","['class-field-theory', 'number-theory', 'p-adic-number-theory', 'algebraic-number-theory']"
4329937,"Second Order Differential Equations $ay''+by'+cy=0$, without complex numbers","How to solve the following equation without using Second Order Differential Equations formulas or Power Series: $$af^{''}(x)+bf^{'}(x)+cf(x)=0$$ where $b^2-4ac<0$ . I know that the soltution is something like this: $$f(x)=A\sin (qx)+B\cos (qx)$$ In any event, I would like to know how to solve this equation without that well-known formula that we find in any Differential Equations Course. I would like an elementary solution to this equation and without complex numbers. For instance, if $b^2-4ac>0$ then we can solve it by using something like that: $$(e^{kx}g(x))^{'}=0$$ . Can't we do something similar as well? The second question I want to ask is if I want to solve the equation with Power Series is it correct the following approach: I prove by induction that $f$ is infinitely derivable and after that I assume: $$f(x)=\sum_{n=0}^\infty a_n x^n$$ I know that not all infinitely derivable can be written in this form, for example: $e^{-\frac{1}{x^2}}$ .
So it shoul be wrong. Nonetheless, there are several people who provide solutions to this problem by using this fact which for me seems to be blatantly erroneous.","['ordinary-differential-equations', 'real-analysis']"
4329949,Conditions for a linearly independent sequence with dense linear span to be a Schauder basis for a Banach space,"Let $ (e_n)_{n \in \mathbb{N}} $ be a linearly independent sequence in a Banach space $X$ such that $$ X = \overline{\operatorname{span}} \{e_n : n \in \mathbb{N}\} $$ In particular, $X$ is separable. However, $(e_n)$ may not be a Schauder basis for $X$ , unless $X$ is a Hilbert space. The easiest counter-example is the sequence $ (1, x, x^2, \dots)$ in $ C(\mathbb{R}) $ . Even worse, $X$ may not have a Schauder basis at all. Questions: What are the conditions for $(e_n)$ to be a Schauder basis for $X$ ? Even if $(e_n)$ is not a Schauder basis, what are the conditions for $X$ to have a Schauder basis?","['banach-spaces', 'schauder-basis', 'functional-analysis', 'real-analysis']"
4329959,Calculating complex integral for different values of $n$.,"I don't know how to compute the following integral. I think it might be possible to use the Residue Theorem to compute it, but the $\sin\theta$ inside the cosine function is throwing me off. $$I_{n}=\int_{0}^{2 \pi} d \theta e^{-\cos (\theta)} \cos (n \theta+\sin(\theta))$$ I am not very sure how to manipulate the integrand so that it becomes somewhat easier. Maybe something like this could work because we could split the integral in several smaller integrals, but the fact that there are still nested trig functions bothers me because I do not know how to deal with them: $$\cos(n\theta + sin(\theta)) = \cos (\sin (\theta)) \cos (\theta n)-\sin (\sin (\theta)) \sin (\theta n)$$ (Where $n \in \mathbb{Z}$ ). I need to give the result for all $n$ . Any help would be very appreciated. Update . I simplified the integrand a little. From the last expression, one can see that $\int_0^{2\pi}\sin(\sin\theta) = 0$ because the function $\sin(\sin\theta)$ is periodic in $\theta$ with period $2\pi$ and for both $\theta = 0$ and $\theta = 2\pi$ it equals zero. Therefore the integrand can be reduced to: $$I_n = \int_0^{2\pi} e^{-\cos\theta}\cos(n\theta)\cos(\sin\theta)$$ Which still seems kinda difficult to compute.",['complex-analysis']
4330034,Does this group construction preserve finite presentability?,"Suppose $G$ is a group. Consider the set $G^G$ of all functions $G \to G$ , which forms a group under elementwise multiplication. Now, for all $g \in G$ let’s define $c_g \in G^G$ as the constant function $c_g(x) \equiv g$ , and $id \in G^G$ , as the identity map $id(x) = x$ . Now, consider the subgroup $E(G) = \langle \{c_g | g \in G\} \cup \{id\} \rangle$ . $E(G)$ preserves several “finiteness” properties of $G$ : If $G$ is finite then $E(G)$ is also finite. Proof : $|E(G)| \leq |G^G| = |G|^{|G|}$ If $G$ is finitely generated then $E(G)$ is also finitely generated. Proof : If $A$ is a generating set of $G$ , then $\{c_g | g \in A\} \cup \{id\}$ is a generating set of $E(G)$ . If $G$ is finitely approximated then $E(G)$ is also finitely approximated. Proof :  Consider the following class of maps $\pi_g: E(G) \to G, f \mapsto f(g)$ for all $g \in G$ . All $\pi_g$ are homomorphisms and each non-trivial element of $E(G)$ , maps to a non-zero element of $G$ under some of $\pi_g$ . The rest follows from finite approximated ness of $G$ . However, there is also a fourth “finiteness” property I am interested in but do not know how to deal with: If $G$ is finitely presented, does that mean that $E(G)$ is also finitely presented? I suspect, it should be, but have no idea how to prove it.","['group-presentation', 'combinatorial-group-theory', 'finitely-generated', 'abstract-algebra', 'group-theory']"
4330039,Question on Baby Rudin theorem 10.7,"This is the definition which is usable in the proof . https://i.sstatic.net/W6UoC.png . Here is the theorem:
Suppose $F$ is a $\mathscr C'$ - mapping ( that means continuously differentiability) of an open set E $\subset R^n$ into $R^n$ , $0 \in E $ , $F(0) = 0$ , and $F'(0)$ is invertible. Then there is a neighborhood of $0$ in $R^n$ in which a representation: $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ . is valid.
with each $\mathbf{G}_i$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of $0$ , $\mathbf{G}_i(\mathbf{0})=0$ , and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. Here is the proof: Put $F = F_1$ . Assume $1 \leq m \leq n - 1,$ and make the following induction hypothesis ( which evidently holds for $m$ = 1): $V_m$ is a neighborhood of $0$ , $F_m$ $\in$ $\mathscr C'(V_m)$ , $F_m(0)$ = $0$ , $F_m'(0)$ is invertible, and $$P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m).  (1)$$ by $(1)$ , we  have: $$F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i$$ where $\alpha_m,...,\alpha_n$ are real $\mathscr C'$ -functions in $V_m$ . Hence $F_m'(0)$$e_m$ = $\sum_{i=m}^n$ $(D_m\alpha_i)(0)$$e_i$ . ( Mark this equality by ( $\oplus$ )). Since $F_m'(0)$ is invertible, the left side of $(\oplus)$ is not $0$ , and therefore there is a $k$ such that $m$ $\leq$ $k$ $\leq$ $n$ and ( $D_m$$\alpha_k$ )( $0$ ) $\neq$ $0$ . Let $B_m$ be the flip (the definition of this is in the first link) that interchanges $m$ and this $k$ ( if $k = m$ , $B_m$ is the identity ) and define $G_m(x)$ = $x$ + [ $\alpha_k(x)$ - $x_m$ ] $e_m$ ( $x$ $\in$ $V_m$ ). Then $G_m$ $\in$ $\mathscr C'(V_m),$ $G_m$ is primitive and $G_m'(0)$ is invertible, since ( $D_m$$\alpha_k$ )( $0$ ) $\neq$ $0$ . The inverse function theorem shows therefore that there is an open set $U_m$ , with $0$ $\in$ $U_m$ $\subset$ $V_m$ , such that $G_m$ is a $1-1$ mapping of $U_m$ onto a neighborhood $V_{m+1}$ of $0$ , in which $G_m^{-1}$ is continuously differentiable. https://i.sstatic.net/m7BkJ.png ( it's the inverse function theorem). Define $F_{m+1}$ by $F_{m+1}$ ( $y$ ) = $B_m$$F_m$ $\circ$ $G_m^{-1}$ ( $y$ ). ( $y$ $\in$ $V_{m+1}$ ). ( We mean composition in $\circ$ ) Then $F_{m+1}$ $\in$ $\mathscr C'(V_{m+1})$ , $F_{m+1}$ ( $0$ ) = $0$ , and $F_{m+1}'$ ( $0$ ) is invertible. I don't understand how do we get the ( $\oplus$ ) and I also don't understand why is $F_{m+1}(0)$ equal of $0$ . Any help would be appreciated.","['linear-algebra', 'analysis', 'real-analysis']"
4330073,What is known about sums of the form $\sum_{n=-\infty}^{\infty} \operatorname{sinc} (n^{p})$?,"Recently, I've become fascinated with the whole 'sum = integral' concept. The sinc function harbours some great examples. For instance, the authors R. Bailie, D. Borwein and J. Borwein described in their paper “Surprising Sinc Sums and Integrals” (p. 8) that the equality $$\sum_{n=1}^{\infty} \operatorname{sinc}(n)^{N} = - \frac{1}{2} + \int_{0}^{\infty} \operatorname{sinc}(x)^{N} dx $$ holds for $1 \leq N \leq 6$ . There are many other examples in the paper and in this MSE question. Currently, I'm looking at a related expression involving the sinc function. Instead of having the powers outside the function, I wonder what happens once they get inside the sinc. In other words, I'm looking at sums of the form $$\sum_{n=-\infty}^{\infty} \operatorname{sinc} (n^{p}),\tag{*}$$ where $p \in \mathbb{Z}_{>1}$ . It appears that integrals of the form $$\int_{-\infty}^{\infty} \operatorname{sinc}(x^{p})dx $$ can be found, as we have for instance $$\int_{-\infty}^{\infty} \operatorname{sinc}(x^{2})dx = \sqrt{2 \pi} ,$$ and $$\int_{-\infty}^{\infty} \operatorname{sinc}(x^{3})dx = \frac{\Gamma(-2/3)}{\sqrt{3}} ,$$ and : $$\int_{-\infty}^{\infty} \operatorname{sinc}(x^{4})dx =  \frac{2}{3} \cos(\pi/8)\Gamma(1/4). $$ However, I haven't been able to find closed forms for sums of the form $(*)$ . One thing that probably has to do with this is that the Fourier transform of $\operatorname{sinc}(x^{2})$ amounts to a Fresnel integral, and that the Fourier transform of the sinc of univariate polynomials of higher powers does not have an expression in terms of known mathematical functions. Therefore, one can't apply the Poisson Summation Formula to these types of sums. Question : Can a closed form be obtained for sums of the type $(*)$ ? References are much appreciated.","['definite-integrals', 'trigonometric-series', 'reference-request', 'trigonometric-integrals', 'sequences-and-series']"
4330076,Show $|f(0)| \le e$ for holomorphic function satisfying $ |f(e^{i\pi t})|\leq e^{t}$,"Let f be a holomorphic function on the closed unit disk such that : $$ | f(e^{i\pi t})| \leq e^{t}, \forall t \in [0,2] \, .$$ Show that $$ | f(0)| \leq e \, .$$ I tried to use this relation to use the average value of $f$ in $0$ $$ f(0)=\frac{1}{2\pi}\int_{0}^{2\pi}f(e^{it})dt= \frac{1}{2}\int_{0}^{2}f(e^{i\pi t})dt$$ so $$ | f(0)| \leq \frac{e^2-1}{2}$$ which is far from the desired inequality.",['complex-analysis']
4330086,Confused about reducing order of non-linear homogenous ODE,"I have two problems for which I know the answers (and working) but am still confused about the method used to solve them. The equations are $yy^{(2)}=(y^{(1)})^2$ and $x^{(2)}+(x^{(1)})^2=0$ In my notes it instructs that in the case of the independent variable missing from the equation (x and t, respectively), the substitution to reduce the order is made as follows: $$ p = y^{(1)} $$ $$ y^{(2)}=\frac{dp}{dx}=\frac{dp}{dy}\frac{dy}{dx}=p\frac{dp}{dy} $$ Then, for the first equation: $yy^{(2)}=(y^{(1)})^2 \implies yp\frac{dp}{dy}=p^2$ But for the second equation, the substitution is made as: $x^{(2)}+(x^{(1)})^2=0 \implies \frac{dp}{dt}=-p^2$ and not $p\frac{dp}{dx}=-p^2$ I've tested this with online calculators and even they compute these two differential equations in the two different ways. Any explanation of where I'm going wrong would be greatly appreciated.",['ordinary-differential-equations']
4330127,Orthogonal transformation of multivariate Bernoulli-Gaussian distribution,"Recently, I studied multivariate Bernoulli-Gaussian distribution which is very useful for sparse signal processing. Suppose $X = (X_{1}, \cdots, X_{n})$ are i.i.d BG( $p, \sigma^{2}$ ), we can know that $\mathbb{E}(X) = \mathbf{0}$ and Cov( $X$ ) = diag( $p \sigma^{2}$ ). Suppose $A$ is an orthogonal matrix with proper size and $Y = AX$ , we can also know that $\mathbb{E}(Y) = \mathbf{0}$ and Cov( $Y$ ) = diag( $p \sigma^{2}$ ). Is it possible for us to know the distribution of $Y$ and is it possible for us to know $Y = (Y_{1}, \cdots, Y_{n})$ are i.i.d? Thank you.","['statistics', 'independence', 'probability-distributions', 'correlation', 'signal-processing']"
4330147,Why can we use Taylor series for evaluating limits? [duplicate],"This question already has answers here : Is it okay to ""ignore"" small numbers in limits where $x$ approaches infinity? (6 answers) Closed 2 years ago . I was trying to solve a limit with Taylor series today, which I never had a problem with, but now I suddenly don't understand why it's ok to do it in that way. For example, the limit $$\begin{align}
&\lim_{x \to 0} \frac{x - x^3/6 - \sin{x}}{x^5} \\
&= \lim_{x \to 0} \frac{(x-x^3/6) - (x - x^3/6 + x^5/5! - \text{higher order terms} )}{x^5} \\
&= \lim_{x \to 0} \frac{-x^5/5! + \text{higher order terms}}{x^5} \\
&= \lim_{x \to 0} {\frac{-1}{5!} + \text{higher order terms}} = - \frac{1}{120}
\end{align}$$ But why can we say that the sum of higher order terms goes to zero? I understand that each of the terms goes to zero, but why does their infinite sum go to zero? How can I justify that? I think that although the summands are small, their infinite sum can hypothetically still be big?","['limits', 'calculus', 'taylor-expansion']"
4330151,"How to create multiplication table (""Cayley table"") for an algebra or class of algebras?","I am studying universal algebra and getting familiar with the concept of variety of algebra. As far as I understand, a variety is just a class of all algebras satisfying given set of identities. Also, a variety is always closed under homomorphic images, subalgebras and direct products of its members. I am also familiar with definition of free algebra. However, I dont know how to start with this exercise from Bergman´s Fundamentals of Universal Algebra . Exercise 5 (b) from Exercise set 4.34 Let $\mathcal{V}$ be the variety of algebras $(A, ·)$ satisfying the identities $x \ast x \approx x$ and $(x \ast y) \ast z \approx (z \ast y) \ast x.$ Let $\mathcal{W}$ be the subvariety of V defined by the additional identity $y \ast (x \ast y) \approx x$ . Determine $\textbf{F}_\mathcal{W}(x, y)$ . Write out a Cayley table. My thoughts I would just create a multiplication table with x, y, z and start generating the entries according to the operations. My attempt is this: $$\begin{array}{|c|c|c|c|} \hline *& x & y & z\\ \hline x & x & ? & ?\\ \hline y & ? & y & ?\\ \hline z & ? & ? & z\\ \hline \end{array}$$ The problem is, I dont know, how to proceed, when I have identity with three different elements, but on the table, I can only combine two (one on row, on on column). But even if I generate the complete table, I dont know, how to proceed with the free algebra generated by this. (The $\textbf{F}_\mathcal{W}(x, y)$ ). I appreciate any advice in this problem or even how to determine a free algebra generaly. Thank you!","['universal-algebra', 'abstract-algebra', 'cayley-table']"
4330153,Proof of Cartan lemmma : why are the coefficients smooth?,"I am new to differential geometry and I am struggling on the proof of the Cartan's lemma. The version I am trying to prove is the following. Let $M$ be a smooth $n$ -manifold, $\omega^1,...,\omega^k,\alpha^1,...,\alpha^k$ be smooth 1-forms such that The $(\omega^i)$ are linearly independant $\sum_{i=1}^k \alpha^i \wedge \omega^i= 0$ . Then the $(\alpha^i)$ are smooth linear linear combinations of the $(\omega^i)$ . I don't see how to prove the ""smooth"" part. Here is how I managed to prove the fact that each $\alpha^i$ is linearly generated by the $(\omega^i)$ . In the second assumption take the wedge product with $\omega^1 \wedge ... \wedge \omega^{k-1}$ to get $$
\alpha^k \wedge \omega^k \wedge \omega^1 \wedge ... \wedge \omega^{k-1} = 0.
$$ I know that for covectors this means they are linearly dependant, so for all $p \in M$ , $\alpha_p^1 , \omega^1_p,...,\omega^k_p$ are linearly dependant. Let $\lambda, \mu_1,...,\mu_k$ be reals not all null such that $$
\lambda \alpha_p^k + \sum_{i=1}^k \mu_i\omega_p^i = 0.
$$ We cannot have $\lambda = 0$ because then by the first assumption $\lambda = \mu_1 = ... = \mu_k = 0$ . That is $\alpha_p^k \in \langle \omega_p^i \rangle_{i=1}^k$ . The same work can be done up to a change of index to prove that $\alpha_p^i \in \langle \omega_p^i \rangle_{i=1}^k$ for $i = 1,...,k$ arbitrary. Now I don't know how to prove the coefficients are smooth, my idea would be to get a formula and see if it is smooth on $p$ but I don't see how to perform this operation. Any hint in this direction, or a general fact about differential geometry that guaranty the coefficients to be smooth, would be appreciated.","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
4330291,Milne's proof of weak lefschetz,"I was reading Milne's proof of Weak Lefschetz theorem from his book on Étale cohomology [VI.7] and certain parts of theorem 7.9 did not make sense to me. I shall give some definitions first. For any finite type morphism $\pi:Y\rightarrow X$ and any $y\in Y$ with image $x\in X$ we define $\delta(y) = d(x) + \operatorname{trdeg}_{k(x)} k(y)$ , where $d(x)$ is the dimension of the closure of the point $x$ . And for any étale sheaf $F$ on $Y$ , he defines $\delta(F)=\operatorname{sup}\{\delta(y)|F_{\overline{y}}\ne 0\}$ Now, in the proof of [7.9] he takes $X$ to be the spectrum on a strictly local ring and $F$ a torsion sheaf on $\mathbb{A}^1_X$ such that $\delta(F)\leq d$ (assume $d\geq 2$ ). Let $j:\mathbb{A}^1_X\rightarrow \mathbb{P}^1_X$ be an embedding. Then Milne claims that $H^i(\mathbb{P}^1_X, j_*F)=0$ for all $i> 2$ as a consequence of proper base change theorem. I believe the proper map being considered here is $\mathbb{P}^1_X\rightarrow X$ , but I don't see how this can be used to prove that the cohomology is zero! Could someone please help me with that? A second question I had in the same proof was towards the end. Let $g:\mathbb{A}^{n}_X\rightarrow \mathbb{A}^{n-1}_X$ be the morphism that identifies $\mathbb{A}^{n}_X$ with the affine line over $\mathbb{A}^{n-1}_X$ and let $F$ be a torsion sheaf on $\mathbb{A}^{n}_X$ with $\delta(F)\leq d$ . Then using the case $n=1$ shown in the first half of the proof at the level of stalks helps solve the general case. I agree with this as long as we can show that $\delta(R^jg_*F)\leq d-j$ . But why should $\delta(R^jg_*F)\leq d-j$ ? Thanks in advance for any help!","['etale-cohomology', 'algebraic-geometry']"
4330301,Solving the trigonometric equation $\sqrt{2}\sin(⁡2x)=-\sqrt{3\sin(⁡x) + 3\cos(⁡x) + 8\cos^4⁡(x-\pi/4)}$,"My problem is to solve the following equation: $$\sqrt{2}\sin(⁡2x)=-\sqrt{3\sin(⁡x) + 3\cos(⁡x) + 8\cos^4⁡(x-\pi/4)}$$ I've narrowed it down to $$3\sin(⁡x) + 3\cos(⁡x) + 2 + 8\sin(⁡x)\cos(⁡x) = 0.$$ That's as far as I can get. It seems so simple, but I just can't find the next step(s).",['trigonometry']
4330381,Exact definition of algebraic set,"Let $k$ be a (algebraically closed) field, as far as I know, the algebraic sets in $k^n$ are the sets $S\subset k^n$ sustaining $V(I(S))=S$ , but I also heard that algebraic sets are the sets sustaining $S=V(T)$ for some $T\triangleleft k[x_1,x_2,...x_n]$ ; It is obvious that the first definition implies the second one, but for the second one implies the first one I'm not sure if my proof is correct: $$V(I(S))=V(I(V(T)))=V(\sqrt{T})$$ It is known that $I:\{\text{algebraic sets (by the second definition)}\}\to\{\text{radical ideals of } k[x_1,...,x_n]\}$ is injective, hence because: $$I(V(T))=\sqrt{T}=I(V(\sqrt{T}))$$ We get $V(T)=V(\sqrt{T})$ , therefore $$V(I(S))=V(I(V(T)))=V(\sqrt{T})=V(T)=S$$ Therefore $S$ is algebraic by the first definition. $\blacksquare$","['algebraic-geometry', 'solution-verification']"
4330385,A prime generating algorithm,"I was trying to explain the famous proof of infinitude of primes to a young one, and I tried to explicitly show some examples. So, I said something like Let the only primes be $2,3,5$ . Then $$N=2\times 3\times 5+1=31$$ which is a prime. So, let the only primes be $2,3,5,31$ . This time $$N=2\times 3\times 5\times 31+1=931=7^2\times 19$$ which introduces two more ""new primes"" in the list. But, this lead me to a different question. In both the mentioned cases, as is in general, if we start with the first $k$ primes, the ""new prime"" is the list will not be the $(k+1)$ -th prime. So, my question is, if we start with a finite number of primes, and go on repeating this algorithm, are we bound to hit all the primes? If not, then what are the primes that we may hit or miss? So, let me frame the question once again in a more mathy way Let $P=\{p_1,p_2,\dots ,p_k\}$ be a finite set of primes. Apply the following algorithm- Define $N=\prod_{i=1}^kp_i+1$ If $N$ is prime, add $N$ to the set $P$ , i.e., take $P=P\cup \{N\}$ . If $N$ is not prime, let $N=q_1^{\alpha_1}q_2^{\alpha_2}\dots q_m^{\alpha_m}$ where $q_i<q_{i+1}\forall i\in\{1,2,\dots ,m-1\}$ . Add $q_1$ to $P$ , i.e., take $P=P\cup \{q_1\}$ Repeat steps 1,2,3 using updated $P$ . Euclid's proof guarantees that this algorithm will never stop. The question is, for what initial ""seeds"" $P$ is this algorithm guaranteed to hit some given prime $p$ in a finite number of steps (if that's possible)? If it indeed does, then how many steps will it take? If not, then for some given initial seeder $P$ , what are the primes that we can be sure to miss? What changes (if any) will we notice if we change the 3rd step of the algorithm to ""take $P=P\cup \{q_1,q_2,\dots q_m\}$ "" (i.e., instead of updating the list with the least new prime, we are updating it with all the new primes)? Although the question apparently seems to be quite elementary, I don't see any obvious way to proceed. I just feel like we need some analytic tools to answer this. I would love to know your thoughts on it. This link pointed out by Steven Clark and this one by Gerry Myerson may be of some help. This question is now also in MathOverflow .","['analytic-number-theory', 'number-theory', 'distribution-of-primes', 'prime-numbers']"
4330401,Taking constants out of indefinite integrals,"In the case of definite integrals, the linearity property implies that constants can be taken out of the integrals, $$\int_{a}^{b} \alpha f(x) d x=\alpha \int_{a}^{b} f(x) d x \tag{1}$$ However, in the case of indefinite integrals, this leads to contradictory results in the case $\alpha=0$ , since $$\int 0\,dx = \int 0 \cdot 1 \,dx = 0 \int 1 \,dx = 0·(x+C) = 0$$ while the derivative of any constant equals $0$ , so $\int 0\,dx =C$ .
Therefore, can't constants be taken out of indefinite integrals?","['indefinite-integrals', 'calculus']"
4330403,Prove that $\lim_{m\to\infty} \sum_{n=0}^{m} \frac{(-1)^n}{n!} \binom{m}{n}=0$.,"The problem from the statement is the hardest part of the problem A. 810 from KöMaL contest November 2021 (the deadline was 10 December).
After the deadline, I noticed that if $r_0=1$ : $$\begin{align}  \sum_{n=0}^m r_n&=\sum_{t=0}^m\sum_{n=t}^m\frac{(-1)^t}{(t+1)!}\binom{n}t\\&=
\sum_{t=0}^m\frac{(-1)^t}{(t+1)!}\sum_{n=t}^m
\binom{n}{t}\\&=\sum_{t=0}^m\frac{(-1)^t}{(t+1)!}\binom{m+1}{t+1}. \end{align} $$ Therefore the KöMaL problem is equivalent to $$\begin{align}  &\lim_{m\to\infty}\sum_{n=0}^m r_n=1\Leftrightarrow\\&\lim_{m\to\infty}\sum_{t=0}^m\frac{(-1)^t}{(t+1)!}\binom{m+1}{t+1}=1\Leftrightarrow\\
&\lim_{n\to\infty}\sum_{t=1}^{m+1}\frac{(-1)^t}{t!}\binom{m+1}t=-1 \end{align} $$ which equivalent to $\displaystyle \lim_{m\to\infty} \sum_{n=0}^{m}
\frac{(-1)^n}{n!} \binom{m}{n} = 0$ and I cannot prove this affirmation. My ideas: To study the function $f_m:\mathbb{R}\to\mathbb{R},$ $f_m(x)=\sum\limits_{n=0}^m
(-1)^n\binom{m}{n}\frac{x^{n}}{n!}$ . To use Cauchy product of two series.","['integration', 'contest-math', 'binomial-coefficients', 'sequences-and-series', 'limits']"
4330526,Solve $X''(t) + f(g(t)) X(t) = 0$ using the solutions of $X''(t) + f(t)X(t) = 0$,"I want to solve the ODE $$X''(t) + f(g(t)) X(t) = 0$$ And I know the solutions of $$X''(t) + f(t) X(t) =0$$ Is there a way I can find the solutions of the first ODE using the second? I think I can't in general, they seem to be completely different problems even though the equations are similar. But for $g(t) = t$ , the solutions are the same, obviously. Or if $g(t) = -t$ , then $\varphi(-t)$ is a solution for the first if $\varphi(t)$ is a solution for the second. So its possible in some cases. I would like to know if it is possible to do the same for other $g$ . I notice if $F(r,s) = A(r)B(s)$ , where $A$ is a solution to the first ODE and $B$ is a solution to the second, then $F_{rr}(t,g(t)) = F_{ss}(t,g(t))$ . I don't know if this helps. Any ideas/hints/references are welcome (I am a newbie in differential equations). Thanks","['ordinary-differential-equations', 'partial-differential-equations']"
4330529,Why does the bounds of this integral not consider both equalities?,"I'm trying to show that this function is a joint probability density function. $$f(x,y)=\begin{cases}1/x&:& 0<y<x<1\\0&:&\text{otherwise}\end{cases}$$ To do this, I need to integrate the function over the bounds of x and the bounds of y and check that the area equals one. According to the problem, 0 < y < x, and y < x < 1. However, in the solution , only the inequality for y is used when integrating over the bounds for y (from 0 to x). However, the bounds for the integral with respect to x is from 0 to 1. According to the problem specifications though,y < x < 1 and thus x > y, so I thought that the bounds of the outside integral with respect to x would be from y to 1. Why is this not the case?","['calculus', 'statistics', 'probability']"
4330550,The map $q: \mathbb{R}^3 / \{0\} \to S^2$ where $q(x) = \frac{x}{|x|}$ is a quotient map,"In exercise 3.64 in Lee's topology, he claims that the fibers of $q$ are open rays in $ \mathbb{R}^3 / \{0\}$ , which makes sense. He then says that it is easy to check that $q$ takes open saturated sets to open sets, which seems intuitive, but I can't quite put into words why exactly that would be. Intuitively, open saturated sets in the domain are open ""pie slices"" (and their unions) with the tip in the origin, and $q$ will just project those to the unit ball in a very straightforward sense. How do I formalize this reasoning? Thank you.",['general-topology']
4330600,"Differentiability at (0,0).","I always get stuck when I've to show something is differentiable,like in the following question: $f(x,y) = \begin{cases} \dfrac{x^3y^3}{x^4+y^4} & \text{if $(x,y)\neq(0,0)$} \\ 0 & \text{if $(x,y)=(0,0)$} \end{cases}$ show that f is differentiable at (0,0) ... First I thought of showing that partial derivatives exist for the
point (0, 0). $$\frac{\partial f}{\partial x}(0,0)=\lim_{x\rightarrow 0}\frac{f(x,0)-f(0,0)}{x-0}=\lim_{x\rightarrow 0}\frac{\frac{0}{x^4}}{x}=\lim_{x\rightarrow 0}\frac{{0}}{x^5}= 0$$ later in relation to y: $$\frac{\partial f}{\partial y}(0,0)=\lim_{y\rightarrow 0}\frac{f(0,y)-f(0,0)}{y -0}=\lim_{y\rightarrow 0}\frac{\frac{0}{y^4}}{y}=\lim_{y\rightarrow 0}\frac{{0}}{y^5}= 0$$ alright both partial derivatives exists and are equal,so now we have to show that :they are continuous near (0,0).. but that doesn't prove the limit is differentiable, what can I do? I thought of using the following formula for a corollary. $$E(h,k)=f(x_0 +h, y_0 +k)-f(x_0, y_0)-\frac{\partial f}{\partial x}(x_0,y_0)h -\frac{\partial f}{\partial y}(x_0,y_0)k$$ however: $$E(h,k)=\frac{(x_0+h)^3(y_0+k)^3} {(x_0 + h)^4 + (y_0 +k)^4}  -\frac{x^3 y^3}{x^4 + y^4}-0h -0k$$",['multivariable-calculus']
4330611,What is a twisted symmetric group?,"I am trying to calculate the total number of subgroups for each subgroup in $S_5$ . I am working on subgroups of order $6$ . From this website , I have stumbled upon a subgroup called ""twisted $S_3$ "". I understand its generating set of a representative subgroup in the context of $S_5$ : it is a $3$ -cycle in $S_5$ , and a double transposition constructed through selecting two elements in the $3$ -cycle and two elements not in the $3$ -cycle (hence why there are $\frac{{5 \choose 2}2!}{2}\frac{{3 \choose 2}1!}{3}\frac{{2 \choose 2}1!}{1}= 10$ elements). What is the twisted symmetric group in general, though?","['symmetric-groups', 'group-theory', 'abstract-algebra', 'definition']"
4330619,multivariable calculus divergence theorem help,"I am stuck on a problem: Use the Divergence Theorem to evaluate $\iint \mathbf{F} \cdot d\mathbf{S}$ , where $$\mathbf{F}(x,y,z)=z^2x\mathbf{i}+(\frac{1}{3}y^3+\tan(z))\mathbf{j}+(x^2z+y^2)\mathbf{k}$$ and $S$ is the top half of the sphere $x^2+y^2+z^2=1$ . [Hint: Note that $S$ is not a closed surface. First compute integrals over $S_1$ and $S_2$ , where $S_1$ is the disk $x^2+y^2\le 1$ , oriented downward, and $S_2 = S \cup S_1$ .] my working process: $\iint \mathbf{F} \cdot d\mathbf{S} = \iiint\limits_E div\mathbf{F} \cdot d\mathbf{V} 
= \iiint\limits_E (x^2+y^2+z^2) d\mathbf{V}$ . for $S_2$ , parametrise $x=r\sin(\phi)\cos(\theta)$ , $y=r\sin(\phi)\sin(\theta)$ , $z=r\cos(\theta)$ and $0\le r \le 1,\;\; 0\le \theta \le 2\pi,\;\; 0\le \phi \le \pi.$ = $\iiint\limits_{S_2} (r^2\cos^2(\theta)+r^2\sin^2(\phi)) dV+\iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S}$ = $\frac{14\pi}{15}$ + $\iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S}$ (the correct answer is $\frac{13\pi}{20}$ ) could you point out the mistake? Thank you very much!!!","['divergence-theorem', 'multivariable-calculus']"
4330623,What is the correct way to denote empty n-ary intersection of sets?,"I'm trying to prove the statement: Show that a finite intersection of open subsets in a metric space is open. If I'm able to enumerate the sets as $U_1, \ldots, U_n$ and consider $U := \bigcap\limits_{i=1}^n U_i$ , I have no problem writing the proof. But ""finite"" can also mean empty, so by listing the sets out like this, I think I'm sacrificing generality. What I'm really doing is defining $U := \bigcap\limits_{i \in I_n} U_i$ , where $I_n$ is an indexing set of size $n \geq 0$ . If $n = 0$ , $U = X$ by definition, which is open. But the notation $\bigcap\limits_{i=1}^n U_i$ doesn't make sense in the case where $n = 0$ , unless $\bigcap\limits_{i=1}^0 U_i$ means ""empty intersection."" Is there a way to get around this subtlety?","['elementary-set-theory', 'index-notation']"
4330624,"Can $\sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n$ for distinct $(x_i)_{i\in \mathbb{N}}$ from interval $(0,1)$?","Can we have an example of two distinct power series $\sum_{n=0}^\infty a_nx^n$ and $\sum_{n=0}^\infty b_n x^n$ with the radius of convergence equal $1$ and there exists a sequence $\{x_i\}_{i\in\mathbb{N}}$ in $(-1,1)$ so that $$\sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n$$ for $i\in \mathbb{N}$ ? Theorem 8.5 in Rudin's Principles of Mathematical Analysis says we don't have such an example if $\{x_i\}_{i\in\mathbb{N}}$ has a limit point in $(-1,1)$ . What if $x_i\nearrow 1$ ?","['power-series', 'analysis', 'analytic-functions']"
4330683,"In layperson's terms, what is a general affine group?","I am trying to calculate the total number of subgroups for each subgroup in $S_5$ . One subgroup in $S_5$ is the general affine group $GA(1, 5)$ . The same website provides a definition of the general affine group. For someone new to group theory, the page is not too beginner friendly. The Wikipedia page isn't too much better. I have tried picturing a Cayley graph, but I do not know where to begin. What is a general affine group in simple terms?","['symmetric-groups', 'group-theory', 'abstract-algebra', 'definition']"
4330695,Hard problem to show almost sure boundedness,"Let us assume that $X_i = \theta t_i + e_i, \ \ i=1,2,...,n$ where $\theta \in \Theta$ which is unknown and $\Theta$ is a closed subset of $\mathbb{R}$ . Suppose $e_i$ 's are i.i.d. on the interval $[-\tau,\tau]$ with unknown $\tau >0$ and $E(e_i)=0$ . Given $t_i$ 's are fixed constants let $$T_n= S_n(\tilde{\theta})=\min_{\gamma \in \Theta} S_n(\gamma)$$ where $$S_n(\gamma) = 2 \max_{i \le n} \frac{|X_i - \gamma t_i|}{\sqrt{1+\gamma^2}}.$$ If we assume that $\sup_{i} |t_i| < \infty$ and $\sup_i t_i - \inf_i t_i > 2\tau$ show that $\{\tilde{\theta_n}, n =1,2,...\}$ is bounded almost surely. I have really no clue how to tackle this one as there is too much detail. Judging by the expression of $S_n(\gamma)$ , it looks like the perpendicular distance of a straight line from a point and it feels that out of $t_1,...,t_n$ , I need to find that $t_i$ which is the most distance from the straight line $X_i = \gamma t_i$ . Can anyone have a crack at this? Another follow-up question : Under the same assumptions mentioned, can we show that $T_n$ is a strongly consistent estimator of $\nu = \min_{\gamma \in \Theta} S(\gamma)$ , where $S(\gamma)=\lim_{n \rightarrow \infty} S_n(\gamma)$ almost surely? By strong consistency of an estimator, you can think of almost sure convergence of the estimator to the parameter of interest. Again, I could show that for any sequence $\theta_n \rightarrow \theta$ , $S_n(\theta_n)-S_n(\theta)=O(|\theta_n-\theta|)$ almost surely. But how can I use this fact to show the almost sure convergence of $T_n$ to $\nu$ ?","['statistical-inference', 'statistics', 'asymptotics', 'probability-theory', 'random-variables']"
4330720,Geodesic equation parametrized by arc length,"Below is a problem from Do Carmo: If the geodesic equations (i) and (ii) are parametrized by arc length, then (i) implies (ii), except in the case of coordinate curves. $(i): u''+\Gamma_{11}^1(u')^2+2\Gamma_{12}^1u'v'+\Gamma_{22}^1(v')^2=0\\(ii): v''+\Gamma_{11}^2(u')^2+2\Gamma_{12}^2u'v'+\Gamma_{22}^2(v')^2=0$ If we take the the following approach to this problem: Suppose the geodesic is parametrized by $\gamma=X(u(s),v(s)).$ By construction of the geodesic equation, we know $\gamma''=(i)X_u+(ii)X_v+(*)N$ , and also $\gamma'=u'X_u+v'X_v$ . So $0=\langle \gamma'',\gamma'\rangle=(i)u'E+((i)v'+(ii)u')F+(ii)v'G$ and if $(i)=0$ we have $(ii)(u'F+v'G)=0$ . Now if we can show $u'F+v'G$ is nonzero, we are done. But the question is: is it possible to do so?","['geodesic', 'differential-geometry']"
4330745,"$P$ poset. $x = \bigvee(\downarrow x\cap U)\Rightarrow \forall x, y \in P$, with $y \lt x$, $\exists a\in U$ s.t. $a \le x $ and $a \nleqslant y$","Le $P$ be a partially ordered set, $U \subseteq P$ and $\downarrow x = \{y \in P : y \le x\}$ (a down set). Show that if $\,\,\forall \,\,x \in P\,\,$ we have $\,\,x = \bigvee(\downarrow x\cap U)\,\,\Rightarrow\,\,\forall x, y \in P$ , with $y \lt x$ , then $\exists \,\,a\in U\,\,$ s.t. $\,\,a \le x \,\,$ and $a \nleqslant y$ . Prove also that if $P$ is a complete lattice then also $\Leftarrow $ is true. When $P$ is a finite lattice everything works fine, in fact : let $L$ be a finite lattice and $x, y \in L\,\,$ s.t. $x \nleq y$ .  Let be $S := \{z \in L : z \le x\ , z \nleq y \}$ , since $L$ is finite and $x \in S$ $\,\,$ I can pick $a \in S$ which is minimal in $S$ . $\,\,$ If $\,\,\,a = c \vee d$ for some $c, d,\in L$ , with $c<a$ and $d <a$ , then $c, d \notin S$ because of the minimality of $a$ , thus: $c, d \le y$ with $c<a\le x$ and $d<a\le x \Rightarrow a = c\vee d \le y$ , a contraddiction. So $a$ has to be join-irreducible $\Rightarrow a \in J(L)\,\,\,\,$ (actually $\,\,a = \bigvee\{z \in J(L) : z \le a \} = \bigvee(\downarrow a \cap J(L)\,\,\,)$ but how to prove it for a poset without hypothesis on its cardinality?","['lattice-orders', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4330777,Upper bound for chromatic number of partitioned graph.,"Let $G=(V,E)$ be a graph that satisfies: $(1)$ $V= \dot\cup_{k=1}^n V_k.$ $(2)$ For all $i,j \in \{1,...,n\}$ there exists a vertex in $V_i$ and a vertex in $V_j$ that are not an edge of $G$ . Then it holds that $\chi(G) \leq v-n+1,$ where $v$ denotes the number of vertices of $G$ .
My idea was to use induction on $n$ , where the base case $n=1$ follows since $\chi(G)$ has to be less or equal than $v$ .
Now for the induction step, I can consider the parition $V=(\dot\cup_{k=1}^n V_k) \dot\cup V_{n+1}.$ Using the induction hypothesis yields $\chi(G') \leq v(G') - n+1$ and $\chi(G'') \leq v(G''),$ where $G'$ denotes the induced Graph on $V_1,...,V_k$ and $G''$ denotes the Graph induced by $V_{n+1}$ . However, I am neither sure how to combine the two colorings to obtain the desired statement nor if this is a good approach. Thanks in advance for any help.","['graph-theory', 'combinatorics', 'coloring']"
4330788,Relation between $\eta\wedge \bar{\eta}\wedge \omega^{n-1}$ and $||\eta||^2\omega^n$,"Let $X$ bet a compact Kahler manifold of dimension $\dim_{\mathbb{C}}(X)=n$ with Kahler form $\omega$ . Let $\eta\in\Omega^{1,0}(X)$ be a differential form. I would like to know if there is a way to relate the forms $\eta\wedge \bar{\eta}\wedge \omega^{n-1}$ and $||\eta||^2\omega^n$ or in a weaker way the integrals $\int_X\eta\wedge \bar{\eta}\wedge \omega^{n-1}$ and $\int_X||\eta||^2\omega^n$ .","['kahler-manifolds', 'complex-geometry', 'differential-forms', 'differential-geometry']"
4330833,Conditional expectations with special structure. Counterexample?,"We have: three random variables $X_1,X_2,X_3$ , three $\sigma$ -fields $\mathcal{G}_1, \mathcal{G}_2, \mathcal{G}_3$ , three random variables $Y_{1,2}$ , $Y_{2,3}$ , $Y_{3,1}$ , such that: $X_1=\mathbb{E}(Y_{1,2}|\mathcal{G_1})$ and $X_2=\mathbb{E}(Y_{1,2}|\mathcal{G_2})$ , $X_2=\mathbb{E}(Y_{2,3}|\mathcal{G_2})$ and $X_3=\mathbb{E}(Y_{2,3}|\mathcal{G_3})$ , $X_3=\mathbb{E}(Y_{3,1}|\mathcal{G_3})$ and $X_1=\mathbb{E}(Y_{3,1}|\mathcal{G_1})$ . (all the above hold a.s.) My question is: does this imply, that there exists a random variable $Z$ , such that $$X_i=\mathbb{E}(Z|\mathcal{G}_i),$$ for $i=1,2,3$ ? Let us  assume that there exists a random variable $U\sim \mathcal{U}(0,1)$ which is independent from all $X_i, Y_{i,j}$ . I have been thinking about this a bit, and I am rather convinced, that this is not true. Even though,  I do not have a good idea on how to construct a counterexample. I will be glad for any help.","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4330851,"$L$ finite and distributive lattice, then $\mathcal{J}(L)$ (join-irreducible's) is isomorphic, as poset, to $\mathcal{M}(L)$ (meet-irreducible's)","Show that for any finite distributive lattice $L$ , $\mathcal{J}(L)$ , that is the associated poset of join-irreducible elements of $L$ , is isomorphic to $\mathcal{M}(L)$ , the associated poset of meet-irreducible elements of $L$ . Obviously $\mathcal{J}(L)$ and $\mathcal{M}(L)$ are poset with the order $\le$ of $L$ .
To show that $\mathcal{J}(L) \cong \mathcal{M}(L)$ I need to find an order isomorphism between the two poset. Now, if $L$ is a finite (not necessarily distributive), it is easy to see that $\forall \,\, x \in L$ we have $x = \bigvee\{a\in\mathcal{J}(L)\,\,| \,\,a \le x\}$ , in fact if $H :=\{a \in \mathcal{J}(L)\,\,|\,\,a\le x\}$ , then $x \in H^u$ by definition of $H$ . Moreover if $y \in H^u$ with $x \nleq y$ then $\exists \,\, a\in \mathcal{J}(L)$ s.t. $a \le x $ and $a \nleq y$ , but $a \in H$ and $y \in H^u \Rightarrow a \le y$ , a contradiction. By duality we have also that $x = \bigwedge\{b \in\mathcal{M}(L)\,\,| \,\,b \ge x\}$ , that is, $\forall x \in L$ : $$x = \bigvee\{a\in\mathcal{J}(L)\,\,| \,\,a \le x\} = \bigwedge\{b \in\mathcal{M}(L)\,\,| \,\,b \ge x\}$$ When $L$ is a finite and also a distributive lattice then, for the Birkhoff's representation theorem , we have that there is a distributive lattices isomorphism between $L$ and $\,\,\mathcal{O}(\mathcal{J}(L))$ (the lattice of down-sets of $\mathcal{J}(L)$ ordered by inclusion), by putting $x \to \{a\in\mathcal{J}(L)\,\,| \,\,a \le x\}$ . Dued to the duality defining $x \to \{b \in\mathcal{M}(L)\,\,| \,\,b \ge x\}$ gives a distributive lattice isomorphism between $L$ and $\,\,\mathcal{O}(\mathcal{M}(L))$ . How to deduce that $\mathcal{J}(L) \cong \mathcal{M}(L)$ as poset, by all of that?","['lattice-orders', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4330870,Inverse Mapping Theorem implies Open Mapping Theorem,"I need to assume that the Inverse Mapping Theorem is true and deduce from it the Open Mapping Theorem . It is immediate to show that the OMP implies IMP. The converse however seems harder. This is since somehow we should use IMP on a bounded linear function $T$ which we should assume is surjective, but not injective. (If it is injective then we can easily see that OMP follows.) For concreteness let $T: X \to Y$ be a surjective continuous linear map between Banach spaces. To actually use the statement, I suppose we should restrict $X$ to some set $M$ such that $T$ there is a bijection. Hence by the IMP, $T$ is indeed open. Other than that I don't see how can we show it is still open in the complement of $M$ . Can someone help me prove this implication?","['banach-spaces', 'functional-analysis']"
4330875,In the Garling's proof of 'AC implies Zorn's lemma',"$
\newcommand{\powset}{\mathcal{P}}
\newcommand{\emtset}{\varnothing}
$ (The post became longer than I expected... but help me) Garling showed 'AC $ \Rightarrow$ Zorn's lemma' in Theorem 1.9.1 in his book A Course in Mathematical Analysis Volume 1: Foundations and Elementary Real Analysis (2013). Assume AC. For any nonempty poset $(X, \le)$ , if every nonempty chain
of $X$ has an upper bound, then there is a maximal element in $X$ . Notations: For a subset $A$ of $X$ , $A'$ denotes the set of strict upper bounds of $A$ . A subset $A$ of a totally ordered set $(S, \le)$ is an initial segment of $S$ if for all $x \in S$ and all $y \in A$ , $x\le y$ implies $x \in A$ . A choice function $c: \powset(X)\setminus \{\emtset\} \to X$ exists such that for all nonempty subset $A$ of $X$ , $c(A) \in A$ by AC. He defined an interesting set: $T$ is the set of chains $C$ in $X$ such that if $D$ is an initial
segment of $C$ , and $D \neq C$ , then $c(D')$ is the least element of $C\setminus D$ . The definition of $T$ is clear but the proofs using it make me really confused. Following his proof, I could't really understand some of his statements appearing in the proposition: Lemma A.1.6 Suppose that $C, D \in T$ , and that $C$ is not contained
in $D$ . Then, $D$ is an initial segment of $C$ . To show this, he defined the set $$
E = \{ x \in C \cap D ~|~ \text{if $y \in x$, then $y \in C$ iff $y \in D$}\}
$$ I could easily show that $E$ is an initial segment of both $C$ and $D$ . Moreover, because $E \subseteq D$ and $\neg C \subseteq D$ , we have $E \neq C$ . Therefore, because $C \in T$ , $c(E')$ is the least element of $C\setminus E$ . There's no problem so far. The confusing point is the following: Suppose $E \neq D$ . Then, $c(E')$ is the least element of $D\setminus E$ .
But this implies that $c(E') \in E$ , giving a contradiction. How on earth could $c(E')$ be an element of $E$ ? I can't find any clue of that conclusion. First, $E'$ and $E$ are disjoint and $c(E') \in E'$ by definition. The condition that $c(E')$ is the least element of $C \setminus E$ also does not imply that $c(E') \in E$ . I've been holding this for whole two days. I am sticking to his proof because it seems to use only elementary mathematical notions I can understand for now.","['elementary-set-theory', 'axiom-of-choice']"
4330884,Solve $(y+1)dx+(x+1)dy=0$,Solve $(y+1)dx+(x+1)dy=0$ $$\frac{dy}{y+1}=-\frac{dx}{x+1}$$ then we get $\ln|y+1|=-\ln|x+1|+c$ $$\ln(|(y+1)(x+1)|)=c$$ $$|(y+1)(x+1)|=e^c=c_1$$ but answer is $y+1=\frac{c}{x+1}$ can you help to find where is my mistake?,"['calculus', 'ordinary-differential-equations']"
4330921,Structure ring of constant group scheme.,"For the finite abelian group $G$ , the group scheme $G_{\mathrm{Spec}\,{\Bbb Z}} = {\mathrm{Spec}}\,{\cal O}_G$ over ${\mathrm{Spec}}\,{\Bbb Z}$ is defined as follows $\colon$ $$
{\cal O}_G = {\Bbb Z}e_0 \oplus {\Bbb Z}e_1 \cdots \oplus {\Bbb Z}e_{g}
$$ with $$
e_g^2 = e_g, e_g e_{g'} = \delta_{g,g'}, m(e_g) \colon= \!\!\!\underset{g',g''| g' + g'' = g}{\Sigma} e_{g'} \otimes e_{g'}, \iota(e_g) \colon= e_{-g}, \epsilon(e_g) = \delta_{0,g}. 
$$ Q. Does it hols that $\pi \colon {\cal O}_G \cong k[G]$ , where $k[G] = k[u_g ; g \in G]$ ? How can I make an explicit isomorphism $\pi$ ?","['number-theory', 'group-schemes', 'algebraic-geometry', 'schemes', 'picard-scheme']"
4330968,Prove that the incircle is the smallest circle which passes through the three sides of a triangle,"Can somebody think of a proof (if it is geometric, better) that the incircle of a triangle is the smallest circle which passes through the three sides of a triangle? By proving this, one can demonstrate that the circumradius of a triangle is at least twice the inradius without calculating the distance between the circumcenter and incenter. (This is because the circumradius of the median triangle is half the circumradius of the original triangle and its circumcircle is a circle which passes through the three sides of the triangle.)","['euclidean-geometry', 'geometry']"
4330984,Binomial distribution exact,"From a box containing 20 balls, 14 of them black and 6 white, we are drawing four balls with replacements. What is the probability, that black ball will be drawn twice? is my solution right? $n = 4, k = 2, p = 7/10$ using the formula: i got $P(X = 2) = 0,2646$","['statistics', 'solution-verification', 'binomial-distribution', 'probability']"
4331007,Verification of Proof of Law of Total Covariance,"If $X,Y,Z$ are 3 random variables, then prove that the following holds: $$\mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))$$ Would the following be a good proof? I know that $\mathrm{Cov}(X,Y|Z)=\mathbb{E}(XY|Z)-\mathbb{E}(X|Z)\mathbb{E}(Y|Z)$ . So taking expectations over $Z$ , and using linearity, yields, $$\mathbb{E}(\mathrm{Cov}(X,Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)].$$ Also, using the definition of covariance gives that $$\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)].$$ So, adding the two equations gives $$\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)].$$ Now by the Tower law, the RHS is $\mathrm{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$ . Thus, we have that $$\mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z)),$$ as required.","['conditional-probability', 'solution-verification', 'covariance', 'probability']"
4331069,Relating Dirac mass with surface integral,"I know that for a smooth function $g:\mathbb{R}^n\to\mathbb{R}$ with $\nabla g(x)\neq 0$ , there holds the formula $$\int_{\mathbb{R}^n}\delta(g(x))f(x)\,dx=\int_{g=0}\frac{1}{|\nabla g(x)|}f(x)\,d\sigma(x)$$ where $\,d\sigma$ is the surface measure induced by $[g=0]$ and $\delta$ the one dimensional Dirac mass. I wonder, what is the equivalent expression for $g:\mathbb{R}^n\to\mathbb{R}^m$ for $m<n$ , when $Dg(x)\neq 0$ ? In other words what is the scaling factor in the surface integral to compensate for the $m$ -dimensional Dirac mass? Is it something like $|Dg(x)|^m$ ?","['integration', 'dirac-delta', 'analysis', 'real-analysis', 'differential-geometry']"
4331194,Finding a random vector with a pre-specified distribution,"Consider a bivariate probability distribution $G$ such that: (a) $G$ has full support. (b) The marginals of $G$ are identical (c) The marginals of $G$ are symmetric around 0. Can we always find a random vector $(X_1,X_2,X_3)$ such that $$
(1) \quad \begin{pmatrix}
X_1-X_3\\
X_1-X_2
\end{pmatrix}\sim \begin{pmatrix}
X_2-X_3\\
X_2-X_1
\end{pmatrix}\sim \begin{pmatrix}
X_3-X_1\\
X_3-X_2
\end{pmatrix}\sim G \quad ?
$$ The symbol "" $\sim$ "" means ""distributed as"". Let me give you an example. Suppose $G\sim \mathcal{N}\Big(\begin{pmatrix} 0\\0 \end{pmatrix}, \begin{pmatrix} 2 & 1\\ 1 & 2 \end{pmatrix}\Big)$ . Observed that $G$ satisfies (a)-(c). For such a $G$ , we can find $(X_1,X_2,X_3)$ satisfying (1). For instance, we can take $(X_1,X_2,X_3)\sim \mathcal{N}\Big(\begin{pmatrix} 0\\0\\0 \end{pmatrix}, \begin{pmatrix} 1& 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{pmatrix}\Big)$ . However, I'm wondering whether there may be other $G$ satisfying (a)-(c) (but not necessarily Normal, as in my example) which do not allow us to find $(X_1,X_2,X_3)$ satisfying (1). Can we exclude so?","['probability-distributions', 'probability-theory', 'probability']"
4331243,Find $p$ such that the integral is finite,"Let $X \subset \mathbb{R}^{n}$ . For which $p \in[1, \infty)$ it holds that $f \in L^{p}(X)$ when $f(x)=|x|^{-1}$ and $X=B(0,1)$ $X=\mathbb{R}^{n} \backslash B(0,1)$ $X=\mathbb{R}^{n}$ . My attempt : Suppose $0\leq a<b\leq \infty$ and we consider the annulus $E_{a,b}:=\{x\in\Bbb{R}^n\,:\, a<|x|<b\}$ . Then, for any $p\in\Bbb{R}, $ we have $\int_{E_{a,b}}\frac{1}{|x|^{p}}\,dx=\int_a^b\frac{1}{r^{p}}A_{n-1}r^{n-1}\,dr=A_{n-1}\int_a^b\frac{1}{r^{p+1-n}}\,dr$ , where $A_{n-1}$ is the surface area of the unit sphere $S^{n-1}\subset\Bbb{R}^n$ $A_{n-1} \frac{-(p+1-n)}{r^{p+2-n}}\bigr\vert_{a}^{b}$ In the first case, $a = 0$ and $b = 1$ , in the second case $a = 1$ and $b = \infty$ , and in the third case $a = 0$ and $b = \infty$ . In the first case the integral is finite when $p +2 ≤ n$ , in the second case the integral is finite when $p + 2 ≥ n$ , and in the third case $p + 2 = n$ . Is my attempt correct?","['lp-spaces', 'sobolev-spaces', 'functional-analysis']"
4331274,How can I show that this stochastic process satisfies the heat equation?,"I have the following stochastic equation: $u(x,t)=e^{\rho t}\mathbb{E}[f(x+W(t))]$ For some function $f(x)$ and $W(t)$ is a Brownian motion. In my notes it says that this equation satisfies the heat equation: $\frac{\partial u}{\partial t}=\rho u+\frac{1}{2}\frac{\partial^2 u}{\partial x^2}$ I believe this identity comes from the backward Kolmogorov equation, but can't seem to find exactly this result. In particular, my signs seem to be wrong when just applying the backward Kolmogorov equation. Any ideas?","['stochastic-integrals', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4331278,The size of the subset that maximizes variance,"I have a set $S =\{p_i\}_{i=1}^N$ data points. Each data point has dimensionality of $D$ . Now for each subset $L \subset S$ , I compute the sum of variance of each dimension of $L$ , namely: $
\text{var}(L) = \sum_i \sigma_i^2, i\in {1...D}
$ I figure out (by testing with code) that the subset that has the maximum variance $\text{var}(L)$ always has the size (number of data points) $\leq D+2$ . Might be my observation is wrong, but in all of my test cases, it's true. Is there a proof for that? UPDATED : Here is my code for testing: import numpy as np
from itertools import combinations
from tqdm import tqdm

# testing parameters
N = 1000  # number of runs
D = 3  # datapoint dimensionality
K = 10  # number of data points

print('Number of run: ', N)
print('Dimensionality: ', D)
print('Number of subsets in each run ', 2 ** K)


# function to calculate the spreading metric
def spreading_metric(s):
    return np.var(s, axis=0).sum()


for i in tqdm(range(N)):
    points = np.random.rand(K, D)
    size_max = 0
    v_max = -1
    for k in range(2, K + 1):
        for comb in combinations(range(points.shape[0]), k):
            rows = list(comb)
            v = spreading_metric(points[rows])
            if v > v_max:
                v_max = v
                size_max = len(rows)
    assert size_max <= D + 3, f'wrong theory caught for size_max {size_max} !!!'

print('Test passed!') In fact, after sometimes testing, $D+3$ seems to be the good bound. Still, if you could provide proof or a better bound, I really appreciate it. Here is one example for $D=2$ . >>> points
array([[0.22092705, 0.40871716],
       [0.29761191, 0.10001131],
       [0.92967956, 0.50780717],
       [0.0432983 , 0.28589581],
       [0.47457283, 0.44952723],
       [0.05990606, 0.91755492],
       [0.76807522, 0.96215681],
       [0.38944321, 0.226995  ],
       [0.66338676, 0.57872981],
       [0.23038586, 0.4326604 ]]) The best subset (var = 0.249) is: array([[0.29761191, 0.10001131],
       [0.92967956, 0.50780717],
       [0.0432983 , 0.28589581],
       [0.05990606, 0.91755492],
       [0.76807522, 0.96215681]])","['optimization', 'combinations', 'combinatorics']"
4331345,Does this function ever have the same value twice?,Consider the function from a real value to a complex value: $$f(x) = \cos(\sqrt{2} x ) + i \sin(\sqrt{3} x)$$ My contention is it never has the same complex value for two different real values of $x$ . i.e. $f(x)=f(y) \implies x=y$ Is this true? Is there a proof? My second contention is that it has no definable inverse. $f^{-1}(z)$ from a complex number to a real number. Edit: I realised this must be wrong! But I think it is correct for this function to a quaternion: $$g(x) = \cos(\sqrt{2} x ) + i \sin(\sqrt{2} x) + j\cos(\sqrt{3} x ) + k \sin(\sqrt{3} x)$$,['complex-analysis']
4331358,"Suppose $G$ and $G'$ are grobner bases for the ideal $I$. Show that $\overline{f}^{G} = \overline{f}^{G'}$ for $f \in k[x_1, \cdots, x_n]$","Suppose $G$ and $G'$ are grobner bases for the ideal $I$ . Show that $\overline{f}^{G} = \overline{f}^{G'}$ for $f \in k[x_1, \cdots, x_n]$ By the division algorithm and Proposition $1$ , we may write $f \in k[x_1, \cdots, x_n]$ as: $$
f = \underbrace{q_1 g_1 + q_2 g_2 + \cdots + q_n g_n}_{qg} + r
$$ such that the remainder $r \in k[\overline{x}]$ is unique. We may also do the exact division process with $G'$ : $$
f = \underbrace{q_1' g_1' + \cdots + q_n' g_n'}_{q'g'} + r'
$$ and as before $r' \in k[\overline{x}]$ is unique by proposition $1$ . We wish to show the two remainder must be the same. Therefore, consider their difference: $$
f - f = qg - q' g' = r - r' = 0 \in k[\overline{x}]
$$ Note that $qg$ and $q'g'$ are both contained in the ideal. Thus, $r - r'$ must be contained in the ideal and therefore $\overline{(r-r')}^{G,G'}$ is zero. But I know that the leading terms $LT(g)$ do NOT divide $r$ . Likewise, the leading terms $LT(g')$ do not divide $r'$ . What can I say about the leading term of $r - r'$ ? My attempt: There are three cases: $LT(r-r') = r$ , $LT(r -r') = r'$ , and $LT(r -r') = \text{ something other than } r,r'$ . Case 1. Suppose $LT(r-r') = r$ . Since $r-r' \in I$ , then $LT(r-r') \in I$ . So $r \in I$ . But by Proposition $1$ , $LT(g)\nmid r$ . This is a contradiction ? But I am unsure of its implications.","['algebraic-geometry', 'groebner-basis']"
4331387,"Is there a single equation describing a triangle in the Cartesian plane, using only arithmetic operations? (Yes.)","Is there a single equation describing a triangle in the Cartesian plane which includes only arithmetic operations? The below uses the square root---but, in this case, the square root is itself may be computed using only arithmetic. This post is in response to Is there an equation for a triangle? by ""Stand-up Maths"" (2021-12-01). Therein, a single equation for a triangle in the Cartesian plane is presented (at about minute 13). However, that equation is not valid if any of the sides of the triangle are parallel to the y-axis. I present the following as an alternative. See also "" Is there any equation for triangle? "" where the questioner starts off with the concept of slope, which is undefined for ""vertical"" lines. Let there be three mutually non-parallel lines $l_1$ , $l_2$ , $l_3$ in the Cartesian Plane: $$c_1 + a_1 x + b_1 y = 0$$ $$c_2 + a_2 x + b_2 y = 0$$ $$c_3 + a_3 x + b_3 y = 0$$ respectively, $c_k, a_k, b_k \in \mathbf{Z}$ (i.e., the integers); $k \in \left\{ 1, 2, 3\right\}$ Let $P$ be an arbitrary point $(x, y); x, y \in \mathbf{Q}$ (i.e., the rational numbers). Let $Q_1$ , $Q_2$ , and $Q_3$ be the quadrances (""square distances"") from $P$ to lines $l_1$ , $l_2$ , and $l_3$ , respectively. $P$ is on at least one of the three lines if and only if $$Q_1 Q_2 Q_3 = 0$$ Recall that $0 \le Q_1 Q_2 Q_3$ . Let $z_1$ , $z_2$ , and $z_3$ be points on two lines (a.k.a., the
intersection of the two lines): $$z_1 = l_2 l_3,$$ $$z_2 = l_3 l_1,$$ $$z_3 = l_1 l_2.$$ Let triangles $T_1$ , $T_2$ , and $T_3$ be the triangles formed by: $P, z_2, z_3$ ; $P, z_1, z_2$ ; and $P, z_3, z_1$ ; respectively. Let triangle $T$ be the
triangle formed by $z_1$ , $z_2$ , and $z_3$ (i.e., by $l_1$ , $l_2$ , and $l_3$ ). Let $A_1$ , $A_2$ , $A_3$ be the quadreas (""square areas,"" basically) of triangles $T_1$ , $T_2$ , $T_3$ . Let $A$ be the quadrea of triangle $T$ . Then $P$ is either on triangle $T$ or in the triangular region
bounded by $T$ if and only if $$\sqrt{A_1}
+\sqrt{A_2}
+\sqrt{A_3}
-\sqrt{A} = 0$$ (Note that the expression on the left hand side cannot be
negative. Note that each of the terms is rational.) Then an equation of a triangle in the Cartesian Plane is: $$Q_1 Q_2 Q_3  +
\sqrt{A_1}
+\sqrt{A_2}
+\sqrt{A_3}
-\sqrt{A} = 0$$ or $$Q_1 Q_2 Q_3 =
\sqrt{A}
-\sqrt{A_1}
-\sqrt{A_2}
-\sqrt{A_3}$$ Either of the above equations could be expanded into an equation in $c_k$ , $a_k$ , $b_k$ , $x$ , and $y$ only and, with the exception of the square root, would include only arithmetic operations. Moreover, I claim that, under the constraints laid out above, the "" $\sqrt{}$ "" terms are arithmetic, since there exists at least one arithmetic process for determining exactly the square root of any rational number. Addendum I: The ""Is there an equation for a triangle?"" equation for a triangle In response to @Blue 's 2021-12-13 comment, ""You should include the solution presented by Stand-up Maths,"" that solution is: $$(\mathrm{sign}(m_1x-y)+1)(\mathrm{sign}(m_2x-y)-1)(a_1x+b_1-y)\\+(\mathrm{sign}(m_2x-y)+1)(\mathrm{sign}(m_3x-y)+1)(a_2x+b_2-y)\\+(\mathrm{sign}(m_3x-y)-1)(\mathrm{sign}(m_1x-y)-1)(a_3x+b_3-y)=0$$ (transcribed from ""Is there an equation for a triangle?"" [about minute 13]) ).","['trigonometry', 'triangles', 'plane-geometry', 'rational-numbers']"
4331486,Do carmo: theorem of turning tangents --- notational confusion,"Theorem statement Let $\mathbf x: U \subseteq \mathbb R^2 \to S \subseteq \mathbb R^3$ be a parametrization compatible with the orientation of $S$ . Assume further that $U$ is homeomorphic to the open disk in the plane. Let $\alpha: [0, l] \to \mathbf x(U) \subseteq S$ be a simple, closed, piecewise regular parametried curve with vertices $\alpha(t_i)$ and external angles $\theta_i, i = 0,\dots,k$ . Let $\phi: [t_i, t_{i+1}] \to R$ be differentiable functions which measure at each $t \in [t_i, t_{i+}]$ the positive angle from $\mathbf x_u$ to $\alpha'(t)$ . Theorem of turning tangents : With the above notation: $$
\sum_{i=0}^k (\phi_i(t_{i+1}) - \phi_i(t_i)) + \sum_{i=0}^k \theta_i = \pm 2 \pi
$$ Questions What is the quantity $\phi_i$ ? I don't follow from the definition what $\mathbf x_u$ , and what is it trying to capture by considering the angle between this $\mathbf x_u$ and $\alpha'(t)$ ? Where can I find a proof of this exact theorem? Do Carmo states this without proof. I would like to find a proof of this exact statement of the theorem --- I have found other proofs that invoke the gaussian curvature. Picture for reference","['differential-topology', 'surfaces', 'differential-geometry']"
4331487,Is there a simply connected region with only two (or 1<N<$+\infty$) boundary points?,"My textbook states the Riemann mapping theorem as follows: If D is a simply-connected domain on the extended complex plane that has at least two boundary points ... (translated) I'm wondering what a simply-connected region with only two boundary points would be. Definition of simply-connected domain: For every simple closed curve C in domain D, all points in the interior of C are also in D, where the ""interior"" means: a simple closed curve in the plane divides the plane into two regions, one exterior, one interior. What I know: There are both points belonging and not belonging to the set in any neighborhood of the boundary point. The complement of a simply-connected region is a connected region. PS. I major in physics. I don't know much about this problem and the translation maybe not very clear. The textbook is specially written for physics students as well. Thanks a lot.","['complex-analysis', 'connectedness']"
4331537,Rewriting $\sqrt{h(h+1)}-h$ into $\frac{h(h+1)-h^2}{\sqrt{h(h+1)}+h}$,I need help with how to rearrange the equation $\sqrt{h(h+1)}-h$ . In the solutions booklet the answer to this question is $$\frac{h(h+1)-h^2}{\sqrt{h(h+1)}+h}$$ however I got $$\frac{h(h+1)-h^2}{\sqrt{h(h+1)}-h}$$ instead (difference is the +/- sign.) Here's how I did it: $$\sqrt{h(h+1)}-h$$ = $$(h(h+1))^{1/2})^2-h^2$$ = $$(\sqrt{h(h+1)}+h)(\sqrt{h(h+1)}-h)$$ = ${h(h+1)-h^2}$ . At this point I divided ${h(h+1)-h^2}$ by the original equation because I raised it to a power 2. So I did $$\frac{h(h+1)-h^2}{\sqrt{h(h+1)}-h}$$ This solution doesn't fit the one in the textbook. I'm not sure where to get the $\sqrt{h(h+1)}+h$ in the denominator.,['algebra-precalculus']
4331559,"How to handle the differential of a vector field, ${\rm d}X:TM\to TTM$, in terms of (equivalence classes of) curves?","Given a generic smooth function $f:M\to N$ , we know that its differential is a smooth function $\mathrm df:TM\to TN$ such that $$\mathrm df(p,[\gamma'(0)])\equiv (f(p),\underbrace{\big[\partial_t\big|_0 f(\gamma(t))\big]}_{\in \eta\circ T_{f(p)}N}),$$ for any path $\gamma:I\to M$ with $\gamma(0)=p$ and $\gamma\in[\gamma'(0)]$ , and denoting with $[\gamma'(0)]$ the equivalence class of paths identified by their first derivative in some chart, and similarly denoting with $\partial_t\big|_0 [f(\gamma(t))]$ the corresponding equivalence class of paths $I\to N$ .
I'm also denoting with $\eta$ the map sending an element in $TN$ into its second component, to clarify that the second element in the expression above is an equivalence class of curves (the notation is from Tao's notes I believe). On the other hand, a vector field is also a smooth function $X:M\to TM$ , and it should therefore make sense to talk about its differential, which is then a map $\mathrm dX:TM\to TTM$ , with $TTM$ tangent bundle of the tangent bundle of $M$ . If I try to unravel the same definition used above in this case I get a bit lost in the notation, however. In particular, we should have $$\mathrm dX(p,[\gamma'(0)]) = \big(\underbrace{X(p)}_{\in T_pM}, \underbrace{\big[\partial_t\big|_0X(\gamma(t))\big]}_{\in \eta\circ (T_{X(p)}TM)}
\big).$$ So far, so good. But then we also know that $X(p)=(p,[\partial_t\big|_0\Phi_X^t(p)])$ , where $t\mapsto \Phi_X^t(p)$ is a curve representing the tangent curve corresponding to $X(p)$ (there might be a better notation for this, I'm not sure). Using this, I'd get $$\mathrm dX(p,[\gamma'(0)]) = \big(
\underbrace{(p,\,\,[\partial_t|_0 \Phi_X^t(p)])}_{\in T_pM}, \,\,
\big[\partial_t\big|_0\big\{\underbrace{(\gamma(t),\,\,[\partial_s|_0 \Phi_X^s(\gamma(t))])}_{X(\gamma(t))}\big\}\big]
\big).$$ On the RHS we are now dealing with equivalence classes of paths in $TM$ . My question is, is there a way to simplify this expression to have just a tuple of four numbers? Naively, I would be tempted to just rewrite this as $$\mathrm dX(p,[\gamma'(0)]) = \big(
  p,\,\,
  [\partial_t|_0 \Phi_X^t(p)],\,\,
  [\gamma'(0)],\,\,
  \Big[\partial_t|_0\big[\partial_s|_0 \Phi_X^s(\gamma(t))\big]\Big]
\big),$$ but I'm not sure whether this is legit, as I'm pretending that I can simply add pointwise the components of the tangent bundle. I'm sort of taking a curve $\tilde\gamma:I\to TM$ and writing it as $\tilde\gamma(t)=(\gamma_1(t),\gamma_2(t))$ for some pair of curves $\gamma_1:I\to M$ and $\gamma_2:I\to\eta\circ TM$ . Locally, we can do this via the trivialisation of the bundle, but is it legit to write this sort of expression more in general?
Furthermore, is there a way to further rewrite the last bit of this expression, the one with the multiple derivatives, more explicitly? Addendum: Perhaps a more expressive, if less standard, notation for the above equations would be as follows. Given any curve $\gamma:I\to M$ , define the map $D_t$ as sending smooth curves defined at $t\in\mathbb R$ to the corresponding equivalence class of curves defined by their slope at $t$ , so that $D_t[\gamma]\subset\mathrm{Curves}(M)$ for any $\gamma\in\mathrm{Curves}(M)$ . With this, we can write for any $f:M\to N$ , $$\mathrm df(p,D_0[\gamma]) = (f(p), D_0[f\circ\gamma]).$$ For vector fields, we then have $$X(p)=(p,D_0[\Phi_X(p,\cdot)]),
\qquad
X(\gamma(t)) = (\gamma(t), D_0[\Phi_X(\gamma(t),\cdot)]),
$$ and thus $$\mathrm dX(p,D_0[\gamma])
= (
    X(p),
    D_0[X\circ\gamma]
) \\
= (
    p, D_0[\Phi_X(p,\cdot)], \,\,
    D_{t=0}\big[(\gamma(t), D_{s=0}[\Phi_X(\gamma(t),s)])\big] \,\,
),$$ where I also had to write $D_{t=0}$ and $D_{s=0}$ to remark which curve/functional relation ought to be used as input to the map $D_0$ .","['differential', 'vector-fields', 'vector-bundles', 'tangent-bundle', 'differential-geometry']"
