question_id,title,body,tags
3782196,"Quadratic P.S.D. differential operator that is invariant under $\textrm{SL}(2, \mathbb{R})$","Given some function $f \in L^2(\mathbb{R}^2)$ , I'm interested in finding a positive semi-definite differential operator $\mathcal P: L^2(\mathbb{R}^2) \rightarrow L^2(\mathbb{R}^2)$ that is quadratic in $f$ and invariant under the the action of $\textrm{SL}{(2, \mathbb{R})},$ such that $\forall A \in \textrm{SL}(2, \mathbb{R})$ and $\forall {\bf x} \in \mathbb{R}^2,$ $$ {\mathcal P} f(A {\bf x}) = [{\mathcal P} f] (A {\bf x} ).$$ After thinking for some time, I've come up with two operators that are invariant and P.S.D, but not quadratic. For example, suppose we consider the operator $${\mathcal P} = \left( \frac{\partial^2}{\partial x^2} \frac{\partial^2}{\partial y^2} - \left[\frac{\partial^2}{\partial x y}\right] \right)^2,$$ which is the squared determinant of the Hessian. It's clear that it is P.S.D. and invariant under transformations in $\textrm{SL}(2, \mathbb{R})$ , though it is quartic in $f$ . Furthermore, letting $H$ denote the Hessian and $J \in \textrm{SO}(2)$ be a rotation by $90^\circ$ , the operator $$ {\mathcal P} = \left(\nabla^T  J^T H  \ J \ \nabla\right)^2,$$ is also invariant and P.S.D., but is not quadratic. I'm asking this question in the hope that someone might know of such a quadratic P.S.D. differential operator that is invariant under $\textrm{SL}(2, \mathbb{R})$ (if it even exists) or be able to point me toward a with a few other ideas I could try. Some possibly related question(s): Projective invariant differential operator Classification of diffeomorphisms by association of differentials with Lie groups Proof that $a\nabla u = b u$ is the only homogenous second order 2D PDE unchanged/invariant by rotation","['differential-operators', 'invariance', 'functional-analysis', 'lie-groups', 'differential-geometry']"
3782213,How to choose the order of a Runge-Kutta method?,"I have seen that Runge-Kutta's methods are a family of methods used to approximate the solution of an initial value problem. I have also seen that they are classified depending on their order (with the second-order R-K being the Euler's Modified method, and the fourth-order R-K being the most used among them). So, given a first-order ODE $y'=f(x,y)$ with an initial condition $y(x_0)=y_0$ , what is the criteria to follow to choose the order of the Runge-Kutta method to be used?","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
3782277,Prove: $\int_0^1 \int_0^1 \frac{\ln{\left(2+yx\right)}}{1+yx} \; \mathrm{d}y\; \mathrm{d}x = \frac{13}{24} \zeta(3)$,"Prove: $$\int_0^1 \int_0^1 \frac{\ln{\left(2+yx\right)}}{1+yx} \; \mathrm{d}y\; \mathrm{d}x = \frac{13}{24} \zeta(3)$$ My attempt: \begin{align}
\int_0^1 \int_0^1 \frac{\ln{\left(1+(1+yx)\right)}}{1+yx} \; \mathrm{d}y\; \mathrm{d}x \\
\int_0^1 \int_0^1 \sum_{n=1}^{\infty} \frac{{(-1)}^n{(1+xy)}^{n-1}}{n} \; \mathrm{d}y\; \mathrm{d}x \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n}{n} \int_0^1 \int_0^1 {(1+xy)}^{n-1}\; \mathrm{d}y\; \mathrm{d}x \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n}{n} \int_0^1 \frac{(y+1)^n-1}{yn}\; \mathrm{d}y \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n}{n} \int_0^1 \sum_{i=1}^{n} \frac{{n \choose i} y^{n-1}}{n}\; \mathrm{d}y \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n}{n} \sum_{i=1}^{n}  \frac{{n \choose i}}{n}\int_0^1  y^{n-1}\; \mathrm{d}y \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n}{n^3} \sum_{i=1}^{n} {n \choose i} \\
\sum_{n=1}^{\infty} \frac{{(-1)}^n\left(2^n-1\right)}{n^3} \\
\end{align} This diverges.  I realize now that this is because $1+xy$ here is outside of the radius of convergence. Credit to @Varun Vejalla:
The double integral equals $$-\sum_{m=1}^{\infty} \frac{1}{m \cdot 2^m} \sum_{n=1}^m \frac{{(-1)}^n}{n^2}$$ How can we prove this equals the result?","['integration', 'real-analysis', 'calculus', 'sequences-and-series', 'indefinite-integrals']"
3782304,A Sobolev space on a perforated domain with extension by a solid vector field,"Presentation : Let $\Omega \subset \mathbb{R}^3$ an open bounded regular set and $B=B(a,r)$ a ball such as $\bar{B} \subset \Omega$ . I'm studying the following space : $$V=\{v|_{\Omega \setminus B} \in (H^1(\Omega \setminus B))^3 \ | \ \operatorname{div}(v) = 0 \text{ in } \Omega \setminus B \ | \ \gamma_0(v)=0 \text{ a.e on }  \partial \Omega \ | \ \exists (b,c) \in \mathbb{R}^3 \times \mathbb{R}^3 \text{ such as } v|_{B}(x)=b + c \wedge (x-a) \}$$ where $\gamma_0 : (H^1(\Omega \setminus B))^3 \rightarrow (L^2(\partial \Omega \cup \partial B))^3$ is the sobolev trace application on $\Omega$ and the divergence condition should be understand in the distribution sense : $$\int\limits_{\Omega \setminus B} v \cdot \nabla \varphi=0, \ \forall \varphi \in C^\infty_c(\Omega \setminus B).$$ Concretely, the function $v \in V$ is a divergence free element in $(H^1(\Omega \setminus B))^3$ , which is worth $0$ on $\partial \Omega$ , that has been extended in the ball $B$ by a solid vector field $x \mapsto b + c \wedge (x-a)$ so as to get a function $v$ which is at least in $L^2(\Omega)$ . I want to study the property of this extension. My question I was wondering if it were possible to show that an element $v \in V$ is  in $(H^1_0(\Omega))^3$ and is divergence free in $\Omega$ . My attempt so far I want to show that the weak derivatives of $v$ are elements of $L^2(\Omega)$ . However, since I have a jump at the interface $\partial B$ , i'm not sure I will be able to have this kind of regularity. For the divergence free condition, I know that the divergence of a solid vector field $x \mapsto b + c \wedge (x-a)$ is $0$ pointwise, so the weak divergence is equal to $0$ both in $\Omega \setminus B$ and $B$ , but again I have an issue with the jump at $\partial B$ . Any help or references wich deal with such extension are welcomed, as always :). Feel free to ask me questions.","['distribution-theory', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'weak-derivatives']"
3782315,"Real-life math problem , how much faster did i run?","So I was running today and I tried to run faster than usual.
I always workout for $30$ minutes, which contains $25$ minutes of running and $5$ minutes of walking.
But today, I ran $20$ minutes and walked for $10$ .
The distance in both workouts was the same, but I'm not sure of the absolute value of it. I'm assuming the same walking speed for both workouts.
So I was wondering, is it possible to know how much faster was I running today in percentage or in absolute value?
Meaning get an answer in form of $V_1=aV_2$ or $V_1=V_2+a$ I figured there are not enough data, so I tried to assume $6$ km/hour, walking speed, but still couldn't get a result. Bonus:
This really happened to me","['physics', 'algebra-precalculus']"
3782318,Product of $L^p$-convergent sequences are Cauchy,"I'm working on showing: If $\|f_n - f\|_p \to 0$ with $1â‰¤p<\infty$ and $h_n \to h$ pointwise with $|h_n|<M$ for all $n$ , then $\{f_nh_n\}$ is Cauchy in $L^p$ I've shown that $\|h_n-h\|_p \to 0$ , but I'm having a hard time showing the product is Cauchy. I tried to manipulate the terms in the norm in additon to using Minkowski's inequality, but it hasn't worked thus far.","['measure-theory', 'functional-analysis', 'real-analysis']"
3782320,Understanding the Lebesgue's Decomposition Theorem,"In his book Bauer proves the Lebesgue's decomposition theorem . Actually he proves it only in the case where $\mu$ and $\nu$ are finite, leaving the $\sigma$ -finite case as an exercise. Looking at the proof however I don't see anywhere where the finiteness of $\mu$ is used. For the $\sigma$ -finite case I did the following: Since $\nu$ is $\sigma$ -finite we can find a partition $(A_n)$ of $\Omega$ into sets of finite $\nu$ measure.  For each $n$ , let $\nu_n$ denote the finite measure defined by $\nu_n(A):=\nu(A\cap A_n)$ for measurable $A$ . From the finite measure case there is a unique decomposition $$\nu_n=\nu_n^c+\nu_n^s \hspace{0.7cm}\nu_n^c\ll\mu \hspace{0.7cm} \nu_n^s\perp\mu.$$ For each $n$ , let $N_n$ be such that $\mu(N_n)=0=\nu_n^s(N_n^c)$ and let $N=\bigcup_{n\in\mathbb{N}}N_n$ . Define the measures $\nu_c$ and $\nu_s$ by $\nu_c(A):=\nu(A\cap N^c)$ and $\nu_s(A):=\nu(A\cap N)$ for measurable $A$ . We see that $\nu=\nu_c+\nu_s$ and $\nu_s \perp\mu$ . Also $\nu_c\ll\mu $ , since $\mu(A)=0$ implies $$\nu_c(A)=\nu(A \cap N^c)=\sum_{n=1}^\infty \nu_n(A\cap N^c)=\sum_{n=1}^\infty \nu^c_n(A\cap N^c)+\sum_{n=1}^\infty \nu^s_n(A\cap N^c)$$ $$\leq \sum_{n=1}^\infty \nu^c_n(A)+\sum_{n=1}^\infty \nu^s_n( N_n^c)=0$$ This shows existence as well as uniqueness, because if $\nu=\nu'_c+\nu'_s$ is any such decomposition, then defining the measures $\nu_n^{'c}(A):=\nu'_c(A\cap A_n)$ and $\nu_n^{'s}(A):=\nu'_s(A\cap A_n)$ for measurable $A$ and $n\in\mathbb{N}$ we get $$\nu_n=\nu_n^c+\nu_n^s=\nu_n^{'c}+\nu_n^{'s} \hspace{0.7cm}\nu_n^{'c}\ll\mu \hspace{0.7cm} \nu_n^{'s}\perp\mu $$ which implies $\nu_n^c=\nu_n^{'c}$ and $\nu_n^s=\nu_n^{'s}$ for each $n$ by uniqueness. Hence $\nu'_c=\sum_{n=1}^\infty \nu_n^{'c}$ and $\nu'_s=\sum_{n=1}^\infty \nu_n^{'s}$ are completely determined. Again I don't see where the $\sigma$ -finiteness of $\mu$ is needed in the argument. I also get confused by the hint Bauer gives for the exercise: Hint. For the existence proof use 17.6 . For the uniqueness proof choose a sequence $(A_n)$ of measurable sets with $A_n \uparrow \Omega$ and $\mu(A_n),\nu(A_n)$ finite for each $n$ , and consider the measures $\nu_n(A):=\nu(A\cap A_n)$ for mesurable $A$ and $n\in\mathbb{N}$ . EDIT: I think the requirement that $\mu$ be $\sigma$ -finite is due to the fact that Lebesgue's decomposition theorem is usually proven in conjunction with the Radon-Nikodym theorem. See my answer below. Still I don't get how to use the hint provided to prove existence and uniqueness. Any help is appreciated.","['measure-theory', 'lebesgue-measure', 'radon-nikodym']"
3782348,What does $f:\mathbb R \rightarrow \mathbb R$ mean?,"This is simply a basic notation question: what is the meaning of $$f:\mathbb R \rightarrow \mathbb R$$ I imagine it's some sort of function to do with the set of real numbers, perhaps some sort of mapping. Until now I've only encountered functions of the form $$f(x)=...$$ or $$f:x\mapsto...$$ Thanks in advance.","['notation', 'real-numbers', 'functions']"
3782394,"How is the function $\psi(c) = (c\bmod m,c\bmod n)$, given $\gcd(m, n) = 1$, a bijection?","I came across this lemma in a textbook, and found something in the proof I did not understand: Proof that $\psi$ is injective: Assume otherwise, that there exists a value in the co-domain that has more than one pre-image: $$\psi(c) = \psi(c')$$ By its definition, $\psi(c) = (c\mod m,c\mod n)$ , given $\gcd(m, n) = 1$ , so it follows: $$c \bmod m = c' \bmod\ m \iff c \equiv c' \pmod m$$ $$c \bmod n = c' \bmod\ n \iff c \equiv c' \pmod n$$ By definition of a congruence modulo x: $$c \equiv c' \pmod m \iff m\ |\ (c - c')$$ $$c \equiv c' \pmod n \iff n\ |\ (c - c')$$ Given that $m, n$ are relatively-prime ( $\gcd(m,n)=1$ ), it follows that: $$mn \mid(c - c')\iff c \equiv c' \pmod {mn}$$ Then, the text concludes that $c = c'$ from the above congruence, which I don't understand. Given that $c, c' \in \mathbb N$ , I don't see why $c - c'$ must equal $0$ . I even have a counterexample to this reasoning. If we take $m=2, n=3$ , which are relatively prime, we see that $\psi(7) = \psi(13) = \psi(6k + 1) = (1, 1)$ , showing that $(1,1)$ in the co-doman has multiple pre-images. If the proof is indeed incorrect, is there any way to show if $\psi$ is a bijection?","['elementary-set-theory', 'elementary-number-theory', 'proof-explanation', 'modular-arithmetic']"
3782469,Convergence of $\frac{1}{n^{1/p}}\sum_{k=1}^n\sin(kx)X_k$,"Let $(X_n)_n$ be a sequence of i.i.d random variables. Prove that the following statements are equivalent: a) $X_1 \in L^1.$ b) $\forall x \in \mathbb{R},\frac{1}{n}\sum_{k=1}^n\sin(kx)X_k$ converges a.s. c) $\exists x \in \mathbb{R}-\pi\mathbb{Z}; \frac{1}{n}\sum_{k=1}^n \sin(kx)X_k$ converges a.s a) Prove that $X_1 \in L^2$ if and only if $\exists x \in \mathbb{R}-\pi\mathbb{Z} $ ans a sequence $(x_n)_n$ of real numbers such that $\frac{1}{\sqrt{n}}\sum_{k=1}^n\sin(kx)X_k-x_n$ converges in distribution. Hint : Use characteristic functions. You may consider the random variables $Y_n=\frac{1}{\sqrt{n}}\sum_{k=1}^n\sin(kx)(X_{2k+1}-X_{2k}),$ for the converse. b) Deduce that $\forall p>2,x \in \mathbb{R},\frac{1}{n^{1/p}}\sum_{k=1}^n\sin(kx)X_k$ diverges a.s. For the first question, suppose that $X_1 \in L^1,$ I used Kolmogorov two series theorem, to prove that $\sum_{n}\frac{1}{n}\sin(nx)(X_{n}-E[X_1])1_{|X_n-E[X_1]| \leq n}$ converges a.s, so let $Y_n=\frac{1}{n}\sin(nx)(X_{k}-E[X_1])1_{|X_n-E[X_1]| \leq n},$ $\sum_nP(Y_n \neq \frac{1}{n}\sin(nx)(X_n-E[X_1]))=\sum_{n}P(|X_1-E[X_1]| \geq n)<+\infty,$ $\sum_n Var(Y_n-E[Y_n]) \leq \sum_n \frac{1}{n^2}E[(X_1-E[X_1])^2 1_{|X_1-E[X_1]| \leq n}]<+\infty,$ since $E[|X_1-E[X_1]|]<+\infty$ which implies that $\sum_{n}(Y_n-E[Y_n])$ converges a.s, further $\sum_{n}|E[Y_n]| \leq \sum_n P(n<|X_1-E[X_1]| \leq n+1)\sum_{k=1}^n\frac{|\sin(kx)|}{k} \leq x\sum_{n}nP(n<|X_1-E[X_1]| \leq n+1)<+\infty$ so $\sum_n\frac{\sin(nx)}{n}(X_n-E[X_1])$ converges a.s and since $\sum_n \frac{1}{n}\sin(xn)$ converges, we conclude that $\sum_{n}\frac{1}{n}\sin(nx)X_n,$ converges a.s, then using Kronecker lemma $\frac{1}{n}\sum_{n}\sin(nx)X_n$ converges a.s to $0$ . To prove that $X_1 \in L^1,$ I took a subsequence $n_k=\left \lfloor{\frac{5\pi}{6}+2\pi k}\right \rfloor$ such that $\frac{1}{2} \leq \sin(n_kx)$ and then we will have $\frac{1}{n}X_{n_k}$ converges a.s to $0$ , which means that $X_1 \in L^1.$ Needing help for 2) a), part b) is a conclusion from a). Appreciating any ideas.","['measure-theory', 'probability-limit-theorems', 'probability-theory']"
3782499,Is there a reason it is so rare we can solve differential equations?,"Speaking about ALL differential equations, it is extremely rare to find analytical solutions. Further, simple differential equations made of basic functions usually tend to have ludicrously complicated solutions or be unsolvable. Is there some deeper reasoning behind why it is so rare to find solutions? Or is it just that every time we can solve differential equations, it is just an algebraic coincidence? I reviewed the existence and uniqueness theorems for differential equations and did not find any insight. Nonetheless, perhaps the answer can be found among these? A huge thanks to anyone willing to help! Update: I believe I have come up with an answer to this odd problem. It is the bottom voted one just because I posted it about a month after I started thinking about this question and all you're inputs, but I have taken all the responses on this page into consideration. Thanks everyone!","['calculus', 'derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
3782501,What is the sum of $\sum_{n=3}^\infty \frac{\tan(\frac{\pi}{n})}{n}$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What is the sum of $$\sum_{n=3}^\infty \frac{\tan\left(\frac{\pi}{n}\right)}{n}\quad ?$$ I was playing a little bit with geometry things, and I got this, this would be the sum of the inverse areas of all regular polygons of length 1, it's a silly thing, but I'm interested in if there's a way to calculate the exact value of this sum.
(Actually the sum have a 4 multiplying, but I guess it is not relevant for the question).","['trigonometry', 'geometry', 'sequences-and-series']"
3782539,If the boundary of a manifold is $C^2$ then the distance to it is also $C^2$ in a neighborhood of the boundary.,"This question comes from the the local property of pseudoconvexity. Pseudoconvexity is is determined by the behaviour of the function of distance to the boundary. In fact it is determined by the differentially geometric property of the boundary in the view of Levi form . As in Demailly's note on complex analytic geometry , assume the boundary of an open set $\Omega \subset \mathbb{C}^n$ has $C^2$ boundary. In order to generate Levi form we should make sure $\delta(z) = \mathrm{d}(x, \mathsf{C}\Omega)$ is also $C^2$ . But how can I make sure this is actually $C^2$ . One of my thought is using tubular neighbourhood. Locally the map $(x , t) \mapsto x+vt$ where $v$ is the unit normal vector is a diffeomorphism satisfying the nearing point on the boundary to $x+vt$ is $x$ . But if the boundary is $C^2$ , this map seems to be $C^1$ and so is the inverse $\pi$ of the map. And $\delta(z) = |\pi(z) - z|$ is also $C^1$ .","['complex-geometry', 'differential-geometry']"
3782559,"Proving a positive function doesn't exist with the condition $f(x+y)\geq yg(f(x)) \ \ , \ \ x>0 \ \ , \ \ y>0$","This is an old question from Nieuw Archief voor Wiskunde 23(1975) p.242 . I don't have access to this journal but I would really like to see the solution. Here is the question: Let $g$ be a positive and continuous function on $(0,\infty)$ with the property that $$\int_1^\infty\frac{ds}{g(s)}<\infty$$ Prove that there exists no positive and continuous function $f$ on $(0,\infty)$ such that $$f(x+y)\geq yg(f(x)) \ \ , \ \ x>0 \ \ , \ \ y>0$$ Any help would be appreciated, even if it is digging the original journal and the solution posted there.","['functions', 'problem-solving', 'real-analysis']"
3782578,Epitrochoids and adjacent loop touching,"Consider the pair of parametric equations which describe a (simplified) epitrochoid: \begin{align}
x(t) &= \cos (t) - a \cos (\alpha t)\\
y(t) &= \sin (t) - a \sin (\alpha t).
\end{align} Here $a \in \mathbb{R}$ while $\alpha$ is a positive integer such that $\alpha \geqslant 3$ (so that the epitrochoid will have two or more loops). Selecting a given $\alpha$ , how is the value for $a$ found such that adjacent loops just touch one another? When the loops touch one another such points on the curve are self-tangential. It seems these self-tangential points can be found by setting $y (t) = 0$ and $\frac{dy}{dt} = 0$ , but I do not understand why this ought to be the case. As an example, when $\alpha = 5$ , adjacent loops in the corresponding epitrochoid (which has 4 loops) touch when $a = \pm \frac{4}{5}$ . Edit I have added two curves to show what I exactly mean for my example when $\alpha = 5$ .  For the curve on the left $a= \frac{3}{5}$ and none of the loops touch their adjacent loops. For the curve on the right $a = \frac{4}{5}$ and each of the loops just touch their adjacent loops.","['curves', 'trigonometry', 'parametric']"
3782594,Curvature Flow and Green's Functions,"I'm trying to find out if there is a research area with associated literature linking the solution of Laplace's and Helmholtz's equations in 3D to curvature flow. If you look at the solution of Laplace's and Helmholtz's equations for a cylinder or sphere it appears there is a relationship with mean curvature and maybe Gaussian curvature and the Taylor expansion of the Green's functions for a line and point source respectively. This has potential applications in computational geometry.
For Helmholtz, the sphere/point Green's function is $\frac{e^{jk |x-x_0| }}{4 \pi |x-x_0|}$ and the cylinder/line Green's function is $\approx C\frac{e^{jk |x-x_0| }}{\sqrt{|x-x_0|}}$ . The ""damped"" versions are: $\frac{e^{-\beta |x-x_0| }}{4 \pi |x-x_0|}$ and $\approx C\frac{e^{-\beta |x-x_0| }}{\sqrt{|x-x_0|}}$ , respectively. It appears the denominator encodes the curvature information. Any input is appreciated.","['mean-curvature-flows', 'partial-differential-equations', 'differential-geometry']"
3782616,Prove $ \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}$.,"Prove $ \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}$ .
I know how to do it using calculus. But I want to use Cauchy's integral formula. First, consider $ \oint_C e^{-z^2} dz$ along a contour C consisting of a line along the x-axis from $-R$ to $R$ and the semicircle $\Gamma$ above the x-axis having this line as diameter. By Cauchy's Integral formula $ \oint_C e^{-z^2} dz=0$ which implies $ \int_{-R}^{R} e^{-x^2} dx + \int_{\Gamma} e^{-z^2} dz=0$ . When $R \to \infty$ , $ \int_{-\infty}^{\infty} e^{-x^2} dx + \int_{\Gamma} e^{-z^2} dz=0$ . Now to compute $\int_{\Gamma} e^{-z^2} dz$ , consider $z=Re^{i \theta}$ , $dz=iRe^{i \theta}$ . I am stuck here.","['complex-analysis', 'contour-integration']"
3782629,"Calculate: $\int_0^\infty [x]e^{-x} \, dx$ where $[x]:=\max \{k\in\mathbb{Z}:k\leq x\}$","Calculate: $\int_0^\infty [x]e^{-x} \, dx$ where $[x]:=\max \{k\in\mathbb{Z}:k\leq x\}$ Solution: $$\int_0^\infty[x]e^{-x} \, dx = \sum_{k=0}^\infty \int_k^{k+1}[x]e^{-x} \, dx = \sum_{k=0}^\infty k\int_k^{k+1}e^{-x} \, dx = \sum_{k=0}^\infty k(-e^{-1}+1)e^{-k}$$ It is right? How can I solve the series? Thank you.","['calculus', 'improper-integrals', 'sequences-and-series']"
3782663,Determining whether the function is exponential?,"Here's a screenshot of the problem: Because the $x$ -value is an exponent, then this must be an exponential function. By definition, an exponential function is where the independent variable (the $x$ -value) is the exponent. To write the function in the form $K(x) = ab^x$ , I first converted the radical into its exponent form and moved the denominator over to the numerator to get $K(x) = (3^x)(3^{-\frac{1}{2}})(6^{-x})$ I then simplified it further, following the exponent rules of $a^ma^n = a^{m + n}$ $K(x) = (3^{x - \frac{1}{2}})(6^{-x})$ However, as shown, my answers were incorrect. What did I do wrong?","['algebra-precalculus', 'exponential-function']"
3782670,Calculate $\lim_{h\to 0} \frac{\cos(x-2h)-\cos(x+h)}{\sin(x+3h)-\sin(x-h)}$,Calculate $$\lim_{h\to 0} \frac{\cos(x-2h)-\cos(x+h)}{\sin(x+3h)-\sin(x-h)}$$ If I take the limit it results in undefined value. I try to change the formation using identity $\sin A + \sin B$ $$\lim_{h\to 0} \frac{-2\sin\frac{(2x-h)}2\sin(-3h/2)}{2\cos(x+h)\sin(2h)}$$ How do I actually evaluate the limit? With and without derivative?,"['limits', 'calculus']"
3782695,Quetion about this irrationality proof.,"I found this proof on mathoverflow. It's about the irrationality of $(\arcsin 1/4)/\pi$ . My question is: Assuming that we don't know the value of $(\arcsin 1/2)$ , and only knew that $\sin(\arcsin 1/2) = 1/2$ , then this proof wouldn't work just the same to establish the irrationality of $(\arcsin 1/2)/\pi = 1/6?$","['number-theory', 'irrational-numbers', 'proof-explanation']"
3782715,"(BAMO $2013/3$) $ABH$, $BCH$ and $CAH$ is congruent to $ABC$.","Let $H$ be the orthocenter of an acute triangle $ABC$ . Prove that the triangle formed by the circumcenters of triangles $ABH$ , $BCH$ and $CAH$ is congruent to $ABC$ . I have already seen many answers on MSe But my doubt is diffferent, In this solution (first one ) https://artofproblemsolving.com/community/c618937h1628954_problem_320_bamo_20133 To prove $O_AB || O_BA$ , we can do some angle calculations: $\angle O_ABC = 90 - A$ , and $\angle C A O_B = 90 - B$ how he got $\angle O_ABC = 90 - A$ , and $\angle C A O_B = 90 - B$ ?
I tried some angle chasing but did not able to get this ..thankyou","['contest-math', 'euclidean-geometry', 'circles', 'geometry', 'triangles']"
3782778,Prove that $\mathcal A$ is a $\sigma$-algebra of subsets of $\Bbb R^2.$,"Let $\textbf {x} = (x,y) \in \Bbb R^2.$ For any subset $E \subseteq \Bbb R^2$ define $\textbf {x} E$ as follows $:$ $$\textbf {x} E : = \left \{(ax,by)\ |\ (a,b) \in E \right \}.$$ Prove that $\mathcal A = \left \{E \in \mathcal L_{\Bbb R^2}\ |\ \textbf {x} E \in \mathcal L_{\Bbb R^2} \right \}$ is a $\sigma$ -algebra of subsets of $\Bbb R^2.$ It is easy to see that $\varnothing, \Bbb R^2 \in \mathcal A.$ Also $\mathcal A$ is closed under countable unions. For that let us take a sequence $\{E_n \}_{n = 1}^{\infty}$ of elements in $\mathcal A.$ Need to show that $E = \bigcup\limits_{n=1}^{\infty} E_n \in \mathcal A.$ For that we need only to show that $\textbf {x} E \in \mathcal L_{\Bbb R^2}.$ Claim $:$ $\textbf {x} E = \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n.$ Let $y \in \textbf {x} E.$ Then $\exists$ $(a,b) \in E$ such that $y = (ax,by).$ Since $(a,b) \in E,$ $\exists$ $i \in \Bbb N$ such that $(a,b) \in E_i.$ But then $y = (ax,by) \in \textbf {x} E_i.$ Hence $y \in \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n.$ This shows that $\textbf {x} E \subseteq \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n.\ \ \ \ \ \ \ \ (1)$ To prove the reverse inclusion let us take any $z \in \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n.$ Then $\exists$ $j \in \Bbb N$ such that $z \in \textbf {x} E_j.$ So $\exists$ $(p,q) \in E_j$ such that $z = \textbf {x} (p,q) = (px,qy).$ But since $E_j \subseteq \bigcup\limits_{n=1}^{\infty} E_n = E,$ it follows that $(p,q) \in E.$ This shows that $z=(px,qy) = \textbf {x} (p,q) \in \textbf {x} E.$ Hence $\bigcup\limits_{n=1}^{\infty} \textbf {x} E_n \subseteq \textbf {x} E.\ \ \ \ \ \  \ \ (2)$ From $(1)$ anf $(2)$ it follows that $\textbf {x} E = \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n.$ This proves the claim. Now since $E_n \in \mathcal A,$ for all $n \geq 1,$ it follows that $\textbf {x} E_n \in \mathcal L_{\Bbb R^2},$ for all $n \geq 1.$ Since $\mathcal L_{\Bbb R^2}$ is a $\sigma$ -algebra of subsets of $\Bbb R^2,$ we have $\textbf {x} E = \bigcup\limits_{n=1}^{\infty} \textbf {x} E_n \in \mathcal L_{\Bbb R^2}.$ This shows that $E = \bigcup\limits_{n=1}^{\infty} E_n \in \mathcal A.$ Hence $\mathcal A$ is closed under countable unions. The last thing what we have to show is that $\mathcal A$ is closed under complimentation. For that let $E \in \mathcal A.$ Then $\textbf {x} E \in \mathcal L_{\Bbb R^2}.$ Now I'm trying to prove that $\left (\textbf {x} E \right )^c = \textbf {x} E^c.$ But I don't think it's true. For instance let $E = [0,1] \times [0,1]$ and $\textbf {x} = (1,0).$ Then $\textbf {x} E = \{(t,0)\ |\ t \in [0,1] \}.$ Now $\left (\frac  1 2,2 \right ) \in E^c.$ Therefore $\textbf {x} \left (\frac  1 2,2 \right ) = \left (\frac 1 2,0 \right ) \in \textbf {x} E^c.$ Therefore $\textbf {x} E^c \cap \textbf {x} E \neq \varnothing.$ Hence $\textbf {x} E^c \subsetneq \left (\textbf {x} E \right )^c.$ The equality can only occur if the both the components of $\textbf {x}$ are non-zero. So how do I prove that $\mathcal A$ is closed under complimentation in case both the components of $\textbf {x}$ are not simultaneously non-zero? Any help in this regard will be highly appreciated. Thanks in advance. Source $:$ https://youtu.be/CjewMbxZzEQ","['measure-theory', 'proof-writing']"
3782864,"Derive a field from gradient expression that is linear in the field variable, $\textrm{grad}\,F \propto aF$","Not a mathematician.
I have got a problem of finding by integration a field $F$ from the expression of its gradient.
It is a physical problem in continuous space $x,y$ and time $t$ and with real-valued smooth functions. I first show the vanilla case I coped with; please do correct where applicable. The gradient relationship has the simple form $$
\textrm{grad} F =a \{\cos (\omega t), \sin(\omega t)\}  
$$ I consider the differential form for the spatial variation: $$
dF = \frac{\partial F}{\partial x} dx + \frac{\partial F}{\partial y} dy
$$ Since the gradient components are independent of space, and of $F$ , in my case the formula above becomes the relationship $$
dF = a \cos(\omega t)\, dx + a \sin(\omega t)\,dy
$$ I integrate both sides along an arbitrary line on the plane $(x,y)$ and the result is $$
F -F_0 = a \cos(\omega t)\, (x-x_0) + a \sin(\omega t)\,(y-y_0)
$$ where $F_0 = F(x_0,y_0)$ , which I can set. The surface of $F$ is a rotating plane. The case that I found difficult is when the gradient depends on $F$ , say as simply as this form $$
\textrm{grad} F =aF \{\cos (\omega t), \sin(\omega t)\}  
$$ The same differential form as above becomes $$
dF = aF \cos(\omega t)\, dx + aF \sin(\omega t)\,dy
$$ If I tread the same steps as in the vanilla case above, I do not know how to derive a closed form for $F$ any longer. Would someone please show and explain the way to derive $F$ from this second gradient expression?","['multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
3782930,regular Borel probability measure implying countable basis,"Peter Walters' An Introduction to Ergodic Theory (page 10) says If $X$ is a metric space and ( $\mathscr{B}$ is the $\sigma$ -algebra of Borel subsets of $X$ ... and $m$ is any probability measure on $(X, \mathscr{B})$ then $(X, \mathscr{B},m)$ has a countable basis. (This follows from Theorem 6.1.) Therefore most of the spaces we shall deal with have $L^2(X, \mathscr{B}, m)$ separable. I do not see why $(X, \mathscr{B},m)$ has a countable basis. Here, $(X, \mathscr{B},m)$ is said to have a countable basis if there exists a sequence of elements $\{E_n\}_n^\infty\subset \mathscr{B}$ such that for every $\epsilon>0$ and every $B\in\mathscr{B}$ with $m(B)<\infty$ there is some $n$ with $$
m(B\triangle E_n)=m(B\setminus E_n)+m(E_n\setminus B)<\epsilon.
$$ Theorem 6.1: (Theorem 6.1) A Borel probability measure $m$ on a metric space $X$ is regular (i.e., $\forall B\in\mathscr{B}(X)$ and $\forall \epsilon>0$ $\exists$ an open set $U_\epsilon$ , and a closed set $C_\epsilon$ , with $C_\epsilon\subset B\subset U_\epsilon$ and $m(U_\epsilon\setminus C_\epsilon) < \epsilon$ ). (Thoughts) For any $\epsilon>0$ and for any $B\in\mathscr{B}(X)$ , we can take $U_\epsilon$ , $C_\epsilon$ as above such that $$
m(B\setminus C_\epsilon)\leq m(U_\epsilon\setminus C_\epsilon) < \epsilon
$$ and $$
m(C_\epsilon\setminus B)=m(\emptyset)=0
$$ but I do not see how only countably many $C_\epsilon$ would suffice. What is still confusing is that on page 45 Walters say If $X$ is a metric space with a countable topological base and $\mathscr{B}$ is the $\sigma$ -algebra of Borel subsets of $X$ then $(X, \mathscr{B},m)$ has a countable basis for any probability measure $m$ on $(X,\mathscr{B})$ . This follows from Theorem 6.1. Now we have an extra condition on $X$ ...","['general-topology', 'metric-spaces', 'measure-theory']"
3782938,Why is the Conway 'Look and Say' sequences constant defined by this polynom?,"In his work on 'Look and Say' sequences,for instance beginning with $1$ . $$1//
11//
21//
1211//
111221//
312212$$ If $L_n$ is the length of the $n-th$ sequences, then it follows from Conway work that : $$\lim_{n\to\infty} \
\frac{L_{n+1}}{L_n} =\lambda=1.303577269034... $$ where $\lambda$ is the unique real, stricly positive root of \begin{align}
  x^{71} - x^{69}   - 2x^{68}  - x^{67}   + 2x^{66}  + 2x^{65}  + x^{64}   - x^{63} \\
- x^{62}  - x^{61}   - x^{60}   - x^{59}   + 2x^{58}  + 5x^{57}  + 3x^{56}  - 2x^{55}  - 10x^{54} \\
- 3x^{53}- 2x^{52}  + 6x^{51}  + 6x^{50}  + x^{49}   + 9x^{48}  - 3x^{47}  - 7x^{46}  - 8x^{45}  \\
- 8x^{44} + 10x^{43} + 6x^{42}  + 8x^{41}  - 5x^{40}  - 12x^{39} + 7x^{38}  - 7x^{37}  + 7x^{36}  \\
+ x^{35}  - 3x^{34}  + 10x^{33} + x^{32}   - 6x^{31}  - 2x^{30}  - 10x^{29} - 3x^{28}  + 2x^{27}  \\
+ 9x^{26} - 3x^{25}  + 14x^{24} - 8x^{23}   - 7x^{21} + 9x^{20}  -3x^{19} - 4x^{18}  \\
- 10x^{17} - 7x^{16} + 12x^{15} + 7x^{14}  + 2x^{13}  - 12x^{12} - 4x^{11}  - 2x^{10}  + 5x^9     \\
+ x^7      - 7x^6    + 7x^5     - 4x^4     + 12x^3    - 6x^2     + 3x       - 6
\end{align} My question is: why that polynom? How did Conway manage to get it? Is it an approximation of the experimental values of $\lambda$ he got? If there exists any paper, I would appreciate to read it. Thanks for your help.","['algebraic-number-theory', 'polynomials', 'sequences-and-series']"
3782955,Continuity of retraction on $r : \text{Int }\Omega^c \to B$,"I'm reading Milnor's Morse Theory and I have difficulty verifying some claim (which is easy according to Milnor) on page $88$ ,  section $\S 16$ in the book. Here's the setup for my question. In the end, I only ask about how to show that a certain map between metric spaces is continuous. Let $\Omega= \Omega(M;p,q)$ be the set of piecewise smooth path $\omega : [0,1]\to M$ from $p$ to $q$ in the connected, complete Riemannian manifold $M$ . This set equipped with metric function $d : \Omega \times \Omega \to \mathbb{R}$ defined as $$
d(\omega,\omega') = \max_{t \in [0,1] } \rho\big( \omega(t), \omega'(t)\big) + \sqrt{\int_0^1 \Big( \|\dot{\omega}(t)\| - \|\dot{\omega}'(t)\| \Big)^2 dt },
$$ where $\rho : M \times M \to \mathbb{R}$ is the topological metric of $M$ coming from its Riemannian metric. I already showed that the Energy function $E : \Omega \to \mathbb{R}$ , $E(\omega) = \int_0^1 \|\dot{\omega}\|^2 dt $ is continuous. So for some $c>0$ we have open subset $\text{Int }\Omega^c :=  E^{-1}([0,c))$ . For a subdivision $0 = t_0 < t_1 < \cdots < t_k=1$ ,  let $\Omega(t_0,\dots,t_k)$ be a subspace of $\Omega$ consisting of paths $\omega : [0,1] \to M$ such that segment $\omega|[t_{i-1},t_i]$ is a geodesic for each $i=1,\dots,k$ . Finally we have subspace $$
B := \text{Int }\Omega^c \, \cap \, \Omega(t_0,\dots,t_k). 
$$ It is shown in the text that we can define a map $r : \text{Int }\Omega^c \to B$ such that each piecewise-smooth curve $\omega \in \text{Int }\Omega^c = E^{-1}([0,c))$ maped to unique broken geodesic $r(\omega) \in B$ formed by joining the end points $\omega(t_{i-1})$ to $\omega(t_i)$ by minimal geodesic. The detail as follows : for each $i=1,\dots,k$ , the couple $\omega(t_{i-1}),\omega(t_i)$ contained in a neighbourhood $W_i$ of a point $x_i \in M$ such that $W_i \times W_i \subset F(U' \times B_{\delta}(0))$ , where $F : U' \times B_{\delta}(0) \to M \times M$ is the map $F(x,v)=(x,\text{exp}_x(v))$ which map $U'\times B_{\delta}(0)$ diffeomorphic onto its image. Therefore the couple $\omega(t_{i-1}),\omega(t_i)$ contained in the image $F(U'\times B_{\delta}(0))$ which means that there is a unique minimal geodesic from $\omega(t_{i-1})$ to $\omega(t_i)$ . Therefore broken geodesic $r(\omega)$ uniquely determined. $\color{blue}{(\star)}$ Question : How to show that the map $r : \text{Int }\Omega^c \to B$ is continuous? I decided to show this by sequence criteria for continuous function. That is if $\omega_n \to \omega$ as $n \to \infty$ then $\gamma_n=r(\omega_n) \to \gamma=r(\omega)$ as $n \to \infty$ . Here is my thought so far: The sequence $\omega_n \to \omega$ says that I can make the distance $$
d(\omega_n,\omega) = \max_{t \in [0,1] } \rho\big( \omega_n(t), \omega(t)\big) + \sqrt{\int_0^1 \Big( \|\dot{\omega}_n(t)\| - \|\dot{\omega}(t)\| \Big)^2 dt }, \qquad (1)
$$ as small  as i like by letting $n$ large enough. Let $\epsilon>0$ be the challenge, i have to show that for $n$ large enough, the distance $$
d(\gamma_n,\gamma) = \max_{t \in [0,1] } \rho\big( \gamma_n(t), \gamma(t)\big) + \sqrt{\int_0^1 \Big( \|\dot{\gamma}_n(t)\| - \|\dot{\gamma}(t)\| \Big)^2 dt }, \qquad (2)
$$ will be small than $\epsilon$ . I think I can show that $ \max_{t \in [0,1] } \rho\big( \gamma_n(t), \gamma(t)\big) $ can be made small as I like since I can control the term $\max_{t \in [0,1] } \rho\big( \omega_n(t), \omega(t)\big)$ . My problem is to control the integral term in $(2)$ . I know that the $\gamma$ 's are broken geodesics, so on each segement $[t_{i-1},t_i]$ , the integrand $\Big( \|\dot{\gamma}_n(t)\| - \|\dot{\gamma}(t)\| \Big)^2$ on the integral term in $(2)$ , is constant. So if i can show that on each segment that $\|\dot{\gamma}_n(t)\| \to \|\dot{\gamma}(t)\| $ as $n \to \infty$ for some fix $t \in [t_{i-1},t_i]$ , then the whole integral goes to zero. To show this, i plan to use the continuity of exponential map $(q,v) \to \text{exp}(p,v)$ that define each geodesic segment $\gamma_n$ and $\gamma$ . Am I on the right track ? Any help will be appreciated. Thank you. Update Here is the detail of my idea in the paragraph above this: lets concentrate on a particular segment $[t_{i-1},t_i]$ . Since i already showed that $\max_{t \in [0,1]} \rho\big( \gamma_n(t),\gamma(t) \big) \to 0$ we have $$
\rho\big( \gamma_n(t_{i-1}),\gamma(t_{i-1}) \big) \to 0, \quad \text{and} \quad \rho\big( \gamma_n(t_{i}),\gamma(t_{i}) \big) \to 0.
$$ Therefore if $W_i$ is the neighbourhood of a point $x_i \in M$ such that $\omega(t_{i-1}) = \gamma(t_{i-1})$ and $\omega(t_i) = \gamma(t_i)$ both contained in $W_i$ (as described in $\color{blue}{(\star)}$ above), then for $n$ large enough the end points $\gamma_n(t_{i-1})$ and $\gamma_n(t_i)$ also contained in $W_i$ . Since $W_i \times W_i \subset F(U'\times B_{\delta}(0))$ with $F(x,v):=(x,\text{exp}_x(v))$ and $F$ diffeomorphic to its image, then $$
F(\gamma_n(t_{i-1}),v_n) = (\gamma_n(t_{i-1}), \gamma_n(t_i)), \quad \text{and} \quad F(\gamma(t_{i-1}),v) = (\gamma(t_{i-1}), \gamma(t_i))
$$ for some tangent vectors $v_n$ and $v $ at the starting points. But since the curves $\text{exp}_{\gamma(t_{i-1})}(tv_n)$ with domain $[0,1]$ is just reparametrization of geodesic segment $\gamma_n|[t_{i-1},t_i]$ , then the initial velocities related by a constant as $v_n = \lambda  \|\dot{\gamma}_n\|$ . Similarly for $\gamma$ we have $ v = \lambda \| \dot{\gamma} \|$ . Since $F$ diffeomorphism (onto its image) we can write $$
(\gamma_n(t_{i-1}),v_n) = F^{-1}(\gamma_n(t_{i-1}), \gamma_n(t_i)), \quad \text{and} \quad (\gamma(t_{i-1}),v) = F^{-1}(\gamma(t_{i-1}), \gamma(t_i)).
$$ Now by continuity of $F^{-1}$ , the convergence $\gamma_n(t_{i-1}) \to \gamma(t_{i-1})$ and $\gamma_n(t_i) \to \gamma(t_i)$ implies $v_n = \lambda  \|\dot{\gamma}_n\| \to v = \lambda  \|\dot{\gamma}\| $ . Therefore $\|\dot{\gamma}_n\| \to \|\dot{\gamma}\|$ .","['riemannian-geometry', 'morse-theory', 'continuity', 'general-topology', 'differential-geometry']"
3782959,"If $A$ is a $2\times2$ integer matrix with $\det(A)=1$ and $|\text{tr}(A)|>2$, then $A^n\neq I$.","If $A \in \Bbb Z^{2\times 2}$ with $\det(A)=1$ and $\left| \text{tr} (A)\right|>2$ , then $A^n\neq I$ for all $n\in \mathbb{N}$ . I tried to prove this by induction and by contradiction, but I'm quite a bit lost here because it seems that it all reduces to obtain a contradiction about the numbers. I don't see a way around that. Any suggestions?","['matrices', 'linear-algebra']"
3782983,Infinite Cartesian product on a finite family of sets (example),"everyone! I am struggling to see how two definitions of Cartesian product (for finite and infinite cases) are equivalent practically. Imagine I have two sets, $X_{1}=\left\{ 3,4\right\} ,X_{2}=\left\{ 5,6\right\}.$ Their Cartesian product is the set of all ordered pairs : $\left\{ (3,5),(3,6),(4,5),(4,6)\right\}$ such that $x\in X_{1},y\in X_{2}.$ Now, by the definition of the infinite Cartesian product, it must be a set of maps $f:I\rightarrow\bigcup_{i\in I}X_{i}$ such that $f(i)\in X_{i}\,\forall i\in I.$ Here $I=\left\{ 1,2\right\}.$ How does this set look like? I know by definition, that the function is a subset of Cartesian product, so it must be sth like $f_{1}(1)=3, f_{2}(1)=4, f_{1}(2)=5,f_{2}(1)=6,$ so $\left\{ (1,3),(1,4),(2,5),(2,6)\right\}.$ Those two definitions does not produce the same result at all, where am I mistaken?","['elementary-set-theory', 'products']"
3782985,Show $1+\frac{8q}{1-q}+\frac{16q^2}{1+q^2}+\frac{24q^3}{1-q^3}+\dots=1+\frac{8q}{(1-q)^2}+\frac{8q^2}{(1+q^2)^2}+\frac{8q^3}{(1-q^3)^2}+\dots$.,"Show that $$1+\frac{8q}{1-q}+\frac{16q^2}{1+q^2}+\frac{24q^3}{1-q^3}+\dots=1+\frac{8q}{(1-q)^2}+\frac{8q^2}{(1+q^2)^2}+\frac{8q^3}{(1-q^3)^2}+\dots$$ where $|q|<1$ (q can be complex number). The hint is to convert the left side to a double series. One can see that the nominators on the left side is double, triple, ... of those on the right side, while denominators on the right side are square of those on the left side. Both sides have 1, which seems redundant. I'm not sure how to proceed from these observations. Any advice will be helpful. I can try to solve it with some suggestion and if I still can't solve it I will then explain what puzzles me. If anyone gives the answer I guess I will look at only part of it and work from that. Edits: I post my answer inspired by the comments and some other thoughts about the question below. Is there any other ways to solve it?","['complex-analysis', 'double-sequence', 'q-series', 'sequences-and-series']"
3783000,Challenging limit: $\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right)$,Here is a challenging limit proposed by a friend: $$\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right)$$ and he claims that the closed form for this limit is really pleasant. I am not good at limits so I am not going to show any work and just leave it to those who find it interesting. Addendum: A similar problem proposed by the same person: $$\lim_{\alpha\to0^{+}}\left(\frac{2}{3\alpha^3}-\frac{4\pi}{3\alpha}\int_1^\infty\frac{x\cosh(\pi\alpha x)}{\sinh^2(\pi\alpha x)\sqrt{x^2-1}}dx\right)$$,"['integration', 'improper-integrals', 'real-analysis', 'closed-form', 'limits']"
3783034,Prove that $\left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1$.,"Let $f\in C^{3}[0,1]$ such that $f(0)=f'(0)=f(1)=0$ and $\big|f''' (x)\big|\le 1$ .Prove that $$\left|30240\int_{0}^{1}x(1-x)f(x)f'(x)dx\right|\le1 .$$ I couldn't make much progress on this problem. I thought that maybe I should try using polynomial interpolation since I have a bound for $|f'''|$ , but I can't determine the interpolation polynomial and I am quickly stuck (there is also the problem that I am dealing with both $f$ and $f'$ under the integral). Apart from this, I don't think that there is much we can do, the solution probably relies on this technique, but I can't make further progress.","['definite-integrals', 'real-analysis', 'calculus', 'integral-inequality', 'inequality']"
3783057,(Dummit and Foote) Group of order 105 with $n_3 = 1$ must be abelian,"I was working on this problem: Let $G$ be a group of order $105 = 3\times 5\times 7$ . Assume it has a unique normal Sylow 3-subgroup. Then prove that $G$ is abelian. I worked out the following from Sylow's theorem: $n_5 = 1$ or $n_5 = 21$ $n_7 = 1$ or $n_7 = 15$ and by showing that a homomorphism from $G$ into $\operatorname{Aut}(P_q)$ must be trivial if $q-1$ is coprime to $|G|$ : Since $n_3 = 1$ the Sylow 3-group lies in the center. if $n_5 = 1$ the Sylow 5-group lies in the center. and counting elements of order $q$ : $n_5 = 21$ would mean the Sylow 5-subgroups contribute 84 elements of order 5. $n_7 = 15$ would mean the Sylow 7-subgroups contribute 90 elements of order 7. This implies we cannot have both, one of them must be a unique normal subgroup. Is this correct so far? How can I continue from here and finish the proof? Is there a way to avoid splitting into two different cases?","['group-theory', 'sylow-theory', 'abelian-groups']"
3783058,Evaluating $\int_0^1\frac{\arctan x\ln\left(\frac{2x^2}{1+x^2}\right)}{1-x}dx$,"Here is a nice problem proposed by Cornel Valean $$
I=\int_0^1\frac{\arctan\left(x\right)}{1-x}\,
\ln\left(\frac{2x^2}{1+x^2}\right)\,\mathrm{d}x =
-\frac{\pi}{16}\ln^{2}\left(2\right) -
\frac{11}{192}\,\pi^{3} +
2\Im\left\{%
\text{Li}_{3}\left(\frac{1 + \mathrm{i}}{2}\right)\right\}
$$ My Trial: By subbing $x=\frac{1-t}{1+t}$ we have $$I=\int_0^1\frac{\left(\frac{\pi}{4}-\arctan x\right)\ln\left(\frac{(1-x)^2}{1+x^2}\right)}{x(1+x)}dx$$ $$=2\underbrace{\int_0^1\frac{\left(\frac{\pi}{4}-\arctan x\right)\ln(1-x)}{x(1+x)}dx}_{x\to (1-x)/(1+x)}-\int_0^1\frac{\left(\frac{\pi}{4}-\arctan x\right)\ln(1+x^2)}{x(1+x)}dx$$ $$=2\int_0^1\frac{\arctan x\ln(\frac{2x}{1+x})}{1-x}dx-\int_0^1\frac{\left(\frac{\pi}{4}-\arctan x\right)\ln(1+x^2)}{x(1+x)}dx$$ and got stuck here. Any idea? thanks.","['integration', 'real-analysis', 'harmonic-numbers', 'polylogarithm', 'closed-form']"
3783061,Logic expressions for an English verse.,"Question: This is an assignment question that I am having trouble with. The question goes like this. What I did: I made the predicate logic expression as follows. $$\forall x\ LOVES(x, MYBABY)\ \wedge\ \forall x\ (x=ME \Longleftrightarrow LOVES(MYBABY, x))$$ Please tell me if this is correct. Also, I can't seem to find any logic on how to end up with $ME=MYBABY$ , so please help with that as well. Sorry if this question seemed too naÃ¯ve or seemed like I am getting an assignment done from you. Thanks for the attention.","['predicate-logic', 'logic', 'discrete-mathematics']"
3783102,Are finitely generated modules over a commutative ring always a direct sum of cyclic submodules?,"Let's first motivate my question by looking at a finitely generated $k$ -algebra $A$ over a field $k$ . Then $A$ in general does not have the form $k[a_1,a_2,\ldots,a_n]$ where $\{a_1,a_2,\ldots,a_n\}$ is a generating set for $A$ . For example consider the two-dimensional irreducible representation $V$ of the quarternion group $Q_8$ , then the ring of invariants is finitely generated by Hilbert's finiteness theorem, but  the algebra of invariants, which is a subalgebra of a polynomial algebra in two variables holds the form $$ \mathbb{C}[V]^{Q_8} = \dfrac{\mathbb{C}[f,g,h]}{(h^2-f^2g+4g^3)},$$ where $f$ and $g$ are invariant polynomials of degree 4, and $h$ is of degree 6. The reason is that the generating polynomials are not algebraically independent. Now consider a commutative ring $R$ , and $M$ a finitely generating $R$ -module, and $\{m_1,m_2,\ldots,m_n\}$ a generating set for $M$ , I want to know whether it is true that $M$ holds the form $$ M = \bigoplus_{i=1}^n Rm_i$$ I think this is not true, but this is true if and only if $M$ is a finitely generated $\textit{free}$ module over $R$ . Can someone enlighten me?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra', 'modules']"
3783104,Rigorously showing that a process is Markov (edit made),"Let $S_n = X_1 + \dots + X_n$ , where the increments $X_i$ are not i.i.d. Let $\tau$ be some random time (not necessarily a stopping time) and assume $$(\tau, X_1, \dots, X_\tau)\quad  \text{ and } \quad (X_{\tau + 1}, X_{\tau + 2}, \dots) \qquad (1)$$ to be independent. I want to show with rigor that the process $(S_{\tau + n}- S_\tau)_{n \geq 1}$ is a Markov chain (satisfies the Markov property). For convenience, let us write $\tilde{S}_n = S_{\tau + n} - S_\tau$ . As $\tilde{S}_n$ is independent of $\tau$ and $S_\tau$ it makes absolutely sense that the process is Markov but I would like to see the argument in more detail. Edit: Possibly, the assumption (1) is not enough. Let us in addition to (1) assume that $$ (X_1, X_2, \dots) \overset{d}{=} (X_{\tau + 1}, X_{\tau +2}, \dots).$$ Maybe this is now enough to prove the claim that $\tilde{S}_n$ is a Markov chain. Edit2: I am starting to think that the assumptions are not necessary. Simply by the structure of $S$ and in view of $\tilde{S}_n = X_{\tau + 1 } + \dots + X_{\tau + n}$ we have should have that $\tilde{S}_n$ conditioned on the values of $\tilde{S}_1, \dots , \tilde{S}_{n-1}$ only depends on $\tilde{S}_{n-1}$ . Is this true / why is it not that easy?","['stochastic-processes', 'markov-process', 'probability-theory', 'markov-chains']"
3783141,Functoriality of the module of KÃ¤hler differentials,"In Eisenbud's Commutative Algebra , at the start of Chapter 16, he describes the module of KÃ¤hler differentials: given a ring $R$ and an $R$ -algebra $S$ , we have the associated $S$ -module $\Omega_{S/R}$ . This comes equipped with an $R$ -module homomorphism $d: S \to \Omega_{S/R}$ , called the universal $R$ -linear derivation , which satisfies an associated universal property. He goes on to state that the module of KÃ¤hler differentials is functorial in the following sense: given a commutative diagram of rings $\require{AMScd}$ \begin{CD}
R @>>> R'\\
@V{}VV @VVV\\
S @>>> S',
\end{CD} where $S$ is an $R$ -algebra, and $S'$ is an $R'$ -algebra, there is a commutative square of abelian groups $\require{AMScd}$ \begin{CD}
S @>>> S'\\
@V{d}VV @VV{d}V\\
\Omega_{S/R} @>>> \Omega_{S'/R'},
\end{CD} where $S \to S'$ is the associated $R$ -algebra homomorphism, $\Omega_{S/R} \to \Omega_{S'/R'}$ is an $S$ -module homomorphism, and $d$ denotes the universal derivation in each context. As Eisenbud notes, this is quite complicated to state. I am curious if this can be rephrased in a simpler way. My question can be stated concisely as follows: As the module of KÃ¤hler differentials is functorial, we should be able to understand it as a functor of the form $\Omega_{-/-}: \mathscr{C} \to \mathscr{D}$ . In this context, what are the categories $\mathscr{C}$ and $\mathscr{D}$ ? Once question 1 is answered, how do you understand the universal property of the module of KÃ¤hler differentials in this categorical framework?","['abstract-algebra', 'category-theory', 'commutative-algebra', 'modules']"
3783155,Question about symmetry,"What is the area of the largest trapezoid that can be inscribed in a semi-circle with radius $r=1$? Here in @Hagen von Eitzen answer He mentioned: ""By symmetry alone the largest hexagon that
can be inscribed in a circle is regular hexagon."" What is the meaning of symmetry here? and how to  prove the statement that largest n-gon inscribed in a circle is regular n-gon. I never saw this method before.So I am looking for some references or similar geometry problems like this (for example I think the problem that asks for largest rectangle that can be inscribed in a quarter circle has something to do with symmetry) but I couldn't find anything can you send some links for that? Thank you.","['symmetry', 'geometry']"
3783159,"Is an entire function ""determined by"" its maximum modulus on each circle centered at the origin?","Let $f$ be an entire function, and $$M_f(r)=\max_{|z|\leq r}|f(z)|$$ denotes its maximum modulus on the circle centered at the origin with radius $r>0$ . It's clear that for any entire functions $f(z)$ and $g(z)=e^{i\varphi}f(e^{i\theta}z)$ , $M_f\equiv M_g$ , where $\varphi$ and $\theta$ are real constants. For $f(z)$ and $h(z)=\overline{f(\overline z)}$ , we also have $M_f\equiv M_h$ . My question is, for any entire functions $f(z), g(z)$ that satisfying $M_f\equiv M_g$ , could we always transform $f$ into $g$ by the rotation and reflection(or some composition of both) as above? If not, is there any other universal transforms that keeps $M_f$ ?","['complex-analysis', 'entire-functions', 'transformation']"
3783186,"Proving the integral inequality $2â‰¤\int_{-1}^1 \sqrt{1+x^6} \,dx â‰¤ 2\sqrt{2} $","I am trying to prove that $$2â‰¤\int_{-1}^1 \sqrt{1+x^6} \,dx â‰¤ 2\sqrt{2} $$ I learned that the equation $${d\over dx}\int_{g(x)}^{h(x)} f(t)\,dt = f(h(x))h'(x) - f(g(x))g'(x)  $$ is true due to Fundamental Theorem of Calculus and Chain Rule, and I was thinking about taking the derivative to all side of the inequality, but I am not sure that it is the correct way to prove this. Can I ask for a  help to prove the inequality correctly? Any help would be appreciated! Thanks!","['integration', 'calculus', 'derivatives', 'inequality']"
3783199,Structure of the Mapping Cylinder of a Continuous Function,"Let $X,Y$ be topological spaces and $f : X \to Y$ a continuous function. In his book Algebraic Topology , Hatcher constructs the mapping cylinder of $f$ to be the quotient space $M_f = ((X \times I) \sqcup Y)/\sim$ , where $(x,1) \sim f(x)$ for all $x \in X$ . I'm trying to prove the intuitively obvious claim that $\pi|_Y : Y \to \pi(Y)$ is a homeomorphism, where $\pi : (X \times I) \sqcup Y \to M_f$ is the quotient map. Here are my thoughts: Clearly the map is continuous and a bijection onto its image, so we just need to show that it's open. For any $U \subset Y$ open, we know that $f^{-1}(U) \subset X$ is open since $f$ is continuous. Then $f^{-1}(U) \times I$ is open in $X \times I$ which implies that $V := (f^{-1}(U) \times I) \cup U$ is open in $(X\times I) \sqcup Y$ . $V$ is also saturated with respect to the quotient map $\pi$ , so $\pi(V)$ is open in $M_f$ . It follows that $\pi(V) \cap \pi(Y)$ is open in $\pi(Y)$ , but it's easy to see that $\pi(V) \cap \pi(Y)$ is just $\pi(U)$ , and the proof is complete. I'm pretty sure this reasoning is correct (let me know if it isn't), but I'm trying to figure out if the continuity of $f$ is necessary for the claim to hold. My gut tells me that it shouldn't---even if $X \times I$ is joined to $Y$ by some crazy, discontinuous function, the gluing should still leave $Y$ intact. Is there a way to prove this without relying on the continuity of $f$ , or is the claim even true in this case?","['continuity', 'general-topology', 'algebraic-topology']"
3783224,Measure on $\mathcal B(\Bbb R)$ having uniformly bounded central moments,"Consider $\Bbb R$ with Borel $\sigma$ -algebra $\mathcal B(\Bbb R)$ . Let $\mathcal \mu:\mathcal B(\Bbb R)\to[0,\infty]$ be a measure such that for all $p\in [1,\infty)$ there exists $M>0\ ($ indepedent of $p)$ with $$\int_\Bbb R\big|x\big|^p\ d\mu(x)<M.$$ Now, one such measure is $$\sum_{n=1}^\infty\frac{\delta_{c_n}}{n^2}$$ where each $c_n\in [-1,1]$ . I want to find out all such $\mu$ having uniformly bounded central moments. Is it possible? Note that by Chebyshev's inequality if $\mu$ is such a measure then $\mu\big((-\infty,\epsilon]\cup [\epsilon,\infty)\big)\leq \frac{M}{\epsilon}$ for all $\epsilon>0$ .","['measure-theory', 'moment-problem', 'real-analysis']"
3783255,Let $\alpha>0$. Show that $\sum_{n=1}^\infty {\sin nx\over n^\alpha}$ converges for all $x\in\Bbb{R}$ and examine continuity of the limit function.,"First of all I have proved that this series of functions $$\sum_{n=1}^\infty {\sin nx\over n^\alpha}$$ converges $\forall x\in\Bbb{R}$ .
Let $f_n(x)=\sin nx$ and $g_n(x)=\frac{1}{n^\alpha}$ .
Let us fix $x\in\Bbb{R}$ .
Now, let $a_n=f_n(x)$ and $b_n=g_n(x)$ . Now if $x=2m\pi$ for some $m\in\Bbb{Z}$ , $a_n=0$ , hence the series $\sum a_nb_n$ converges to $0$ . Now we assume $x\ne2m\pi$ for any $m\in\Bbb{Z}$ .
Then the partial sums of $(a_n)$ to be $A_n=\sum_{k=1}^{n}a_k$ . Then $$ \left| A_n \right| = \left| \sin x + \sin 2x + \cdots + \sin nx \right| = \left|\frac{\sin{\frac{nx}{2}}\sin {\frac{(n+1)x}{2}}}{\sin{\frac{x}{2}}}\right| \le \frac{1}{\left|\sin{\frac{x}{2}}\right|} $$ and $\frac{1}{\left|\sin{\frac{x}{2}}\right|} \in \Bbb{R}$ since $x \ne 2m\pi$ .
So, partial sums of $(a_n)$ are bounded. Again $b_n={1\over n^\alpha}$ is monotone decreasing and converges to $0$ .
Hence by Dirichlet's test the series $\sum_{n=1}^\infty a_nb_n$ converges. Thus the series of functions $\sum_{n=1}^{\infty} \frac{\sin nx}{n^\alpha}$ converges for all $x\in\Bbb{R}$ . Let $$ f(x) = \sum_{n=1}^{\infty} \frac{\sin nx}{n^\alpha} $$ My target is to find where the function $f$ is continuous. Let us choose a point $a\in\Bbb{R}$ such that $a\ne2m\pi$ for any $m\in\Bbb{Z}$ .
Now we can always find a compact interval $I$ containing $a$ such that $\sin \frac{x}{2} \ne 0$ $\forall x\in I$ since we have chosen $a\ne2m\pi$ . Since, $I$ is compact interval and $\sin$ function is continuous we have $\min_{x\in I}\lvert \sin \frac{x}{2} \rvert = m > 0 $ . We will show that the series of functions $\sum f_n g_n$ converges uniformly on $I$ by using Dirichlet's test for the series of functions.
Let $(F_n)$ denotes the sequence of partial sums of $(f_n)$ .
Then for all $x\in I$ , $\left|F_n(x)\right|\le \frac{1}{\lvert\sin\frac{x}{2}\rvert}\le\frac{1}{m}$ , hence the sequence of functions $(F_n)$ is uniformly bounded over $I$ .
Again, the sequence of function $g_n(x)={1\over n^\alpha}$ is monotone decreasing and converging uniformly to $0$ function.
Thus by Dirichlet's Test for series of functions the series of functions $\sum f_n(x)g_n(x) = \sum_{n=1}^\infty \frac{\sin nx}{n^\alpha}$ converges uniformly over $I$ .
Since the functions $f_n g_n$ are continuous on $I$ , the limit function $f$ is continuous on $I$ .
In particular since $a\in I$ , $f$ is continuous at $a$ . So I found $f$ is continuous on the set $\Bbb{R}\setminus \{2m\pi \mid m\in\Bbb{Z}\}$ . But what about the points like $2m\pi$ ? I observed that $f$ is $0$ at those points.
I am getting no idea how to check continuity at those points. Can anyone help me in this regard? Thanks for your help in advance.","['sequence-of-function', 'continuity', 'sequences-and-series', 'real-analysis']"
3783291,If $ A $ is a $ 2 \times 2 $ real matrix such that $ \det (A) = 1 $ and $ A^n = I$ show that $ A ^tA = I $,"If $ A $ is a $ 2 \times 2 $ real matrix such that $\det (A) = 1 $ and $ A^n = I$ show that $ A ^tA = I $ IDEA: Since $ \text{det}(A) = 1 $ according to the Cayley-Hamilton theorem, it is true that $$A^2-\text{tr}(A)A+\text{det}(A)=0$$ then $A^{-1}=\text{tr}(A)I-A$ , just show that $ A^ {-1} = A^{t}$ , another way is to show that the columns of $ A $ form an orthonormal system of $\mathbb{R} ^ 2 $ but I don't see a way to test Can anyone give a suggestion ? Thank you.","['matrices', 'orthogonal-matrices', 'cayley-hamilton', 'linear-algebra']"
3783304,If $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ then $E[X] < \infty$?,"Let $X$ be a positive random variable. Suppose that $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ Does this implies that $X$ has finite expectation? that is $E[X] < \infty $ I know that if $E[X] < \infty$ $\Rightarrow$ $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ (For any positive random variable see: Expected value as integral of survival function ) , so I was wondering if the converse is true. I have also tried to think in a counterexample but unfortunately I have not been successfull. I would really appreciate any hints or suggestions with this problem.","['measure-theory', 'probability-distributions', 'real-analysis', 'probability-theory', 'probability']"
3783337,"Prove that if a finite simple graph $G$ has exactly $|V(G)| - |E(G)|$ components, then $G$ is a forest","Problem If a finite simple graph $G$ has exactly $|V(G)| - |E(G)|$ components, then $G$ is a forest. Prove this by induction on the number of vertices. Solution I reached a solution to this problem; however, I wasn't able to reach a conclusion using induction. First, I'm going to show the solution without using induction. Then, I will show the beginning of a solution that attempts to use induction, but in which I got stuck. Without using induction In this solution, I'm going to use, without proof, the following theorem about finite forests, found on the textbook I'm using: Theorem . A finite forest $F$ cosists of exactly $|V(F)| - |E(F)|$ trees. Let $G$ be any finite simple graph with $n$ vertices and exactly $|V(G)| - |E(G)|$ components. Then, there is a set $E \subseteq E(G)$ of edges on cycles of $G$ such that removing these edges from $G$ leaves an acyclic graph $H = G - E$ . By definition, $H$ is a forest. Then, by the above Theorem, it has $|V(H)| - |E(H)|$ connected components. Removing edges on cycles from $G$ doesn't change the number of connected components, because, for each edge $e$ that was removed from $G$ , there is a path between the endpoints of $e$ that doesn't include $e$ . So, $G$ and $H$ have the same number of connected components: $$|V(G)| - |E(G)| = |V(H)| - |E(H)|$$ Since $V(H) = V(G)$ and $E(H) = E(G) - |E|$ , substituting these values in the above expression yields: $$|V(G)| - |E(G)| = |V(G)| - (|E(G)| - |E|)$$ $$|V(G)| - |E(G)| = |V(G)| - |E(G)| + |E|$$ $$|E| = 0$$ So, $G$ had no cycles to begin with. Therefore, $G$ is a forest. Using induction Induction hypothesis : $P(n)$ := if a finite simple graph $G$ with $n$ vertices has exactly $|V(G)| - |E(G)|$ components, then $G$ is a forest. Base case ( $n = 1$ ): A graph with only 1 vertex has $|V(G)| - |E(G)| = 1$ component, and it's a forest. So, $P(1)$ is true. Inductive step ( $n \geq 1$ ): Let $G$ be a graph with $n + 1$ vertices and $|V(G)| - |E(G)|$ components. Let $v$ be a $k$ -degree vertex of $G$ . Remove $v$ and all its $k$ incident edges, leaving a subgraph $H$ with $|V(G)| - 1$ vertices and $|E(G)| - k$ edges. I got stuck at this point. Any hints on how to continue the inductive step?","['graph-theory', 'solution-verification', 'discrete-mathematics']"
3783348,Calculating $ \lim_{x \to 0} (\frac{x\cdot\sin{x}}{|x|}) $,"To solve $ \lim_{x \to 0} (\frac{x\cdot\sin{x}}{|x|}) $ I have separated the function in two limits: \begin{align}
\lim_{x \to 0} (\frac{x\cdot\sin{x}}{|x|}) =
\lim_{x \to 0} \frac{x}{|x|} \cdot \lim_{x \to 0} \sin{x}
\end{align} Now solving independently: \begin{align}
\lim_{x \to 0} \frac{x}{|x|} =  \lim_{x \to 0} \text{sign}(x) \, \, \, \text{does not exist}
\end{align} And: \begin{align}
\lim_{x \to 0} \sin{x} = 0
\end{align} I've checked graphically and I'm aware that the limit is $ 0 $ , however, I'm not sure that multiplying ""undefined"" by $ 0 $ is something allowed. Can I do this? If not, what could be another way of solving the limit? Edit: Just realized I can split the function in a different way, knowing that $ \lim_{x \to 0} \frac{\sin{x}}{|x|} = 1 $ \begin{align}
\lim_{x \to 0} (\frac{x\cdot\sin{x}}{|x|}) =
\lim_{x \to 0} (\frac{|x|\cdot\sin{x}}{x}) =
\lim_{x \to 0} |x| \cdot \lim_{x \to 0} \frac{\sin{x}}{x} = 0 \cdot 1 = 0
\end{align} How about this second approach?","['limits', 'calculus', 'solution-verification']"
3783358,Definition of the tangential gradient,"Let $d\in\mathbb N$ and $M\subseteq\mathbb R^d$ be bounded and open such that $\partial M$ is of class $C^1$ (i.e. a $(d-1)$ -dimensional embedded $C^1$ -submanifold of $\mathbb R^d$ ). If $f:\partial M\to\mathbb R$ is $C^1$ -differentiable, we can find the following definition of the ""tangential gradient"" of $f$ in Shapes and Geometries: Metrics, Analysis, Differential Calculus, and Optimization, Second Edition (p. 492) $^1$ : Why is it important to consider a $C^1$ -extension $F$ of $f$ on a tubular neighborhood (or even on that specific one)? Why can't we take any $C^1$ -extension of $f$ , i.e. any $\tilde f\in C^1(O)$ , where $O$ is an $\mathbb R^d$ -open neighborhood of $\partial M$ , with $$f=\left.\tilde f\right|_{\partial M}?\tag1$$ Is it needed to show that $g(F)$ is well-defined, i.e. independent of the choice of $F$ ? In any case, how can we show that it actually is well-defined? EDIT : Meanwhile, I've found others references which consider arbitrary $C^1$ -extensions. But it's still not clear to me why the definition of the tangential gradient is independent of the choice of the extension. $^1$","['tangent-spaces', 'smooth-manifolds', 'vector-analysis', 'differential-topology', 'differential-geometry']"
3783375,What is the application/use to know about the order and degree of a differential equation?,"A degree of a polynomial tells us the maximum number of possible roots of the equation. But what is the use to know about the order and degree of the differential equation. Like, what does it really tell us about the equation we are working with? For Example: If I am working with a polynomial of degree 3, and I tell that to you, you will be expecting, at most 3 roots of that polynomial. Degree of a polynomial only tells us about the number of roots, and so, x 3 and x 3 + 3x 2 + 5 both are equation of degree 3 and tells it will have a maximum of 3 roots. But what does telling you that I am working with a differential equation of order 3 and degree 2 really tell you about the equation because you can not predict what the equation is, but then what can you predict with the data I give you? I am new in the Journey of Differential Equations and wanted to know about the things (and reasons) that I am learning. Thanks in advance for your answer. :)",['ordinary-differential-equations']
3783445,Lagrange formula for solving linear differential equations,"I am studying control systems, and my textbook uses ""Lagrange's formula"" for solving time-continuous linear systems in ""state-space"". Below are the equations presented: $$\dot{x}(t) = Ax(t) + Bu(t)$$ $$y(t) = Cx(t) + Du(t)$$ where A,B,C and D are matrices of coefficients, but let's assume they are all singular to make things simpler. Assuming that $t > t_0$ and $x(t_0) = x_{t_0}$ ,the formula used to calculate $x(t)$ is the following: $$x(t) = e^{A(t-t_0)}x_{t_0} + \int_{t_0}^t e^{A(t-\tau)}Bu(\tau) d\tau $$ This formula is very similar to another formula I learned in calc 2 for an identical purpose, albeit this next formula is defined for all values of t and contains an indefinite integral in place of the definite integral in the formula above $$ x(t) = e^{-At} \int Bu(t)e^{At} dt $$ It is obvious to me that these formulas are strongly connected, but whereas I understand how the second one is derived, I cannot say the same about the first one. And what is up with using $\tau$ as the variable for intergration? I've been told it is a ""dummy variable"", but it was presented as a fact, rather than a proven result of calculus. I've also tried to google this so-called ""Lagrange formula"", but unfortunately I haven't had any success with it. Can anybody please help me understand this?","['calculus', 'control-theory', 'linear-control', 'ordinary-differential-equations']"
3783462,Show that a transformation is linear if and only if its restriction to subspaces of dimension 2 is linear.,"Let $V$ be a vector space over a field $\mathbb{K}$ with $\dim_\mathbb{K} \geq 3$ . Show that a transformation $T : V \rightarrow V $ is linear if and only if the restriction of $T$ to each subspace of dimension $2$ of $V$ is linear. (->) If $T$ is linear in $V$ then it's clear that it's also linear in any subspace of $V$ . (<-) Suppose $\dim_\mathbb{K} = n \geq 3$ and that $T$ is linear in any subspace of dimension $2$ of $V$ . Let $\{b_1,b_2, \cdots, b_n\} \subset V$ be a basis for $V$ . Now consider the following subspaces of $V$ : $$
W_i = \text{span}(\{b_i, b_{i+1}\})
$$ Now let $v = \big(\sum_{i=1}^n \alpha_i \cdot b_i\big) \in V$ . Therefore: $$
v = \sum_{i=1}^n \alpha_i \cdot b_i = \underbrace{(\alpha_1 b_1 + \alpha_2 b_2)}_{\in W_1} + \underbrace{(\alpha_3 b_3 + \alpha_4 b_4)}_{\in W_3} + \cdots + \underbrace{(\alpha_{n-1} b_{n-1} + \alpha_n b_n)}_{\in W_{n-1}}
$$ And from that it follows that if $n$ is even, then $$
V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-1}
$$ and if $n$ is odd, then: $$
V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-2} \oplus \text{span}(\{b_n\})
$$ It's clear to see that the sum is direct since $W_i \cap W_{i+2} = \{0\}$ . Now I need to prove the linearity of $T$ in $V$ , so let $v = \sum_{i=1}^n \alpha_i \cdot b_i$ , $u = \sum_{i=1}^n \beta_i \cdot b_i$ and $\lambda \in \mathbb{K}$ . So it remains to prove that $T(u+v) = T(u) + T(v)$ and $T(\lambda \cdot u) = \lambda \cdot T(u)$ . $$
\begin{align*}
T(u+v) = T\big( \sum_{i=1}^n \alpha_i \cdot b_i + \sum_{i=1}^n \beta_i \cdot b_i \big) = 
T\big( \sum_{i=1}^n (\alpha_i + \beta_i) \cdot b_i \big) = \cdots
\end{align*}
$$ And now I'm stuck because for me ""the restriction of $T$ to each subspace of dimension $2$ of $V$ is linear"" means is that $T$ is going to be linear in each of those $W_i$ that I've defined. That means that if $w = \alpha b_i + \beta b_{i+1} \in W_i$ then $T(w) = \alpha \cdot T(b_i) + \beta \cdot T(b_{i+1})$ . But that does not implies that $$
T(w_1 + w_3 + \cdots + w_{n-1}) = T(w_1) + T(w_3) + \cdots + T(w_{n-1})
$$ where $w_i \in W_i$ . Any help is highly appreciated. Thanks!","['solution-verification', 'linear-algebra', 'linear-transformations']"
3783476,Show that there does not exist a holomorphic function $h(z)$ such that $\exp(h(z)) = z$ on the punctured plane without using complex integration?,"We must show that there does not exist a holomorphic function $h(z)$ on the domain $\mathbb C - \{0 \}$ such that $\exp(h(z)) = z$ on the complex plane. Can we do this without using complex integration? I know the proof of the fact that there exists no function whose exponential is the identity function $id(z) = z$ on the complex plane which uses complex analysis. We proceed by assuming such a  holomorphic $h(z)$ exists. This means that $e^{h(z)} = z$ . Differentiating, we get $1 = e^{h(z)} h'(z) = z h'(z)$ . This means that $1 = z h'(z)$ , or $h'(z) = 1/z$ . Now we compute $\oint h'(z)$ around the countour $c(t) = e^{2 \pi i t}$ in two different ways: $\oint h'(z) = h(1) - h(1) = 0$ , by using FTC and that the countor starts and ends at the same point: $1$ . $\oint h'(z) = \oint 1/z = 2 \pi i$ by the Residue Theorem Thus, we get $0 = 2 \pi i$ which is absurd. However, this proof seems to rely on a lot of the machinery of complex integration to get things done. Is there no ""simpler"" proof? Can we show that to prove this fact, we somehow ""need to"" invoke facts about complex integrals? Ideally, I would want an answer that only uses elementary properties of complex numbers, and properties of the complex exponential, and complex differentiation, but not complex integration .",['complex-analysis']
3783516,Special name for matrices with the same singular values?,"Suppose there exist matrices $A \in \Bbb C^{m \times n}$ and $B = U_1 A U_2$ , where $U_1 \in \Bbb C^{m \times m}$ and $U_2 \in \Bbb C^{n \times n}$ are unitary matrices (not necessarily related to each other). Then, $A$ and $B$ have the same singular values. The reason for this is because $A^H A$ and $B^H B$ are related the following way ( $A^H$ is the Hermitian transpose of $A$ ): $$B^H B = (U_1 A U_2)^H (U_1 A U_2) = U_2^H A^H U_1^H U_1 A U_2 = U_2^H A^H A U_2 = U_2^{-1} (A^H A) U_2$$ Since $A^H A$ and $B^H B$ are similar matrices (by definition), they share the same eigenvalues. Since the singular values of any matrix $M$ are the positive square-roots of the eigenvalues of $M^T M$ , $A$ and $B$ have the same singular values. Is there a special name relating these types of matrices (just like ""similar matrix"" relates $A$ and $B$ with the same eigenvalues)? I know that orthogonally equivalent matrices $C$ and $D = U C U^H$ are kind of similar to this, but $C$ and $D$ are always square, and the two unitary matrices are related as the inverse of one another ( $U^H = U^{-1}$ ). In my problem statement, $A$ and $B$ can be rectangular, and the 2 unitary matrices $U_1$ and $U_2$ don't have to be related to each other (in fact, even their dimensions may differ).","['matrices', 'singular-values', 'linear-algebra', 'terminology']"
3783546,Is the set of all random $n$-vectors with arbitrary expectations an inner product space?,"Let $(\Omega, \mathcal A, Prob)$ be a probability space. Let $V$ be a set of all random $n$ -vectors $X$ of the form $X = [x_1 \cdots x_n]^T$ , where each $x_i: \Omega \rightarrow \mathbb R$ is in turn an arbitrary random variable with finite second moment, i.e., $e\left[X^T X\right] < \infty$ . Is $V$ a real inner product space without requiring that expectation $E[X] = 0$ for each element $X$ of $V$ ? If yes, what is the simplest inner product definition that does the job? (Note: $E$ is the expectation operator on $V$ whereas $e$ is the expectation operator on the vector space of all random variables $x: \Omega \rightarrow \mathbb R$ .) It is clear to me that $V$ satisfies the axioms of a real vector space. I can also see that if $V$ were to consist of only those random $n$ -vectors $X$ that have zero expectation, then the inner product definition $\langle X, Y \rangle = e\left[X^TY\right]$ transforms $V$ into a real inner product space. However I am unsure about the general case of $V$ as mentioned in the question. Would appreciate some insight, and a simple example of an inner product definition, if the answer to the question is ""yes"". Thanks.","['statistics', 'inner-products', 'vector-spaces', 'probability-theory', 'random-variables']"
3783565,"If $f: \mathbb{R}^2 \to \mathbb{R}$ is a continuous function with zero integral over every rectangle of area $1,$ then $f = 0.$","If $f: \mathbb{R}^2 \to \mathbb{R}$ is a continuous function with zero
integral over every rectangle of area $1,$ prove $f = 0.$ Using induction and a shifting argument for the base case, I managed to show that if $ABCD$ is a rectangle with integer area, then $$f(A) + f(C) = f(B) + f(D) \, \, (*)$$ I think this almost solves the problem, I just need to draw the right diagram now. Unfortunately, I'm having difficulties coming up with a diagram that works. I drew a $2 \times 2$ square with vertices $A, B, C, D, E, F, G, H, I,$ applied $(*)$ on all sorts of squares within this square, summed the results, and then tried to cancel as many terms on both sides as possible. However, no matter which sums I took, I would always end up with either $0 = 0$ or a trivial result of the form $(*).$ How do I find the right diagram? I tried drawing $3 \times 3, \sqrt{2} \times \sqrt{2}$ and even $\sqrt{3} \times \sqrt{3}$ squares, but ran into the same problem (at one point, I thought I had proven that $f(x,y) = f(x+\sqrt{3},y) = f(x+\sqrt{2},y),$ which would show $f$ is constant after employing the density of $\{a\sqrt{2}+b\sqrt{3} : a, b \in \mathbb{Z}\}$ in $\mathbb{R},$ but it turned out I made a mistake when adding terms in both cases). Any ideas?","['integration', 'multivariable-calculus', 'real-analysis']"
3783577,Chernoff bound for binary cross entropy loss with finite bracketing entropy,"Consider the binary cross entropy loss of the posterior $\eta(x) = \mathbb{P}(Y=1 | X=x)$ : $$\mathcal{L}_n(\eta) = \frac{1}{n} \sum_{i=1}^n Y_i\log(\eta(X_i)) + (1-Y_i)\log(1-\eta(X_i))$$ Assume that $\eta$ has finite bracketing entropy. Specifically, assume that for every $\eta$ there exists $\eta_L \leq \eta \leq \eta_U$ with $\eta_L, \eta_U \in \mathcal{F_\delta}$ where $\mathcal{F_\delta}$ is a finite class of functions and $\eta_L, \eta_U$ are close in the sense that: $\mathbb{E}|\eta_L- \eta_U| \leq \delta$ . Show the following upper bound on the deviation of the empirical likelihood from the expectation for all $\varepsilon < \varepsilon_0$ where $\varepsilon_0$ is some positive constant that may depend on the $\eta$ being considered: $$\mathbb{P}(\mathcal{L}_n - \mathbb{E}[\mathcal{L}_n] > \varepsilon) \leq e^{-n\varepsilon^2/16}$$ I expect I should be able to apply a Chernoff bound here but I am having trouble evaluating or bounding the moment generating function of $\mathcal{L}_n$ : $$\mathbb{P}(\mathcal{L}_n - \mathbb{E}[\mathcal{L}_n] > \varepsilon) \leq \inf_t \frac{\mathbb{E}\exp{t\mathcal{L}_n}}{\exp{t\epsilon}}$$ In this case I don't know anything about $\eta$ other than it's range. In particular, $\eta(x)$ can be $0$ for some values of $x$ , in which case $\log(\eta(x))$ is unbounded. How do I deal with this? If it's not possible to show such a bound, what additional assumptions do I need to make so that such a bound is possible? EDIT: I've tried to apply a symmetrization argument, but I am still missing a bound on the deviation of each term in the summation. As described in the question, we will use a chernoff bound: $$\mathbb{P}(\mathcal{L}_n - \mathbb{E}[\mathcal{L}_n] > \varepsilon) \leq \inf_t \frac{\mathbb{E}\exp{t(\mathcal{L}_n - \mathbb{E}[\mathcal{L}_n])}}{\exp{t\epsilon}}$$ Let's focus on the numerator. By Jensen's: $$\mathbb{E}\exp{t(\mathcal{L}_n - \mathbb{E}[\mathcal{L}_n])} \leq \mathbb{E}\exp{t(\mathcal{L}_n - \mathcal{L}_n')}$$ where $\mathcal{L}_n'$ is an independent sample that is identically distributed to $\mathcal{L}_n$ . Now, since $\mathcal{L}_n - \mathcal{L}_n'$ is distributed as $\sigma(\mathcal{L}_n - \mathcal{L}_n')$ where $\sigma$ is a rademacher random variable we can take the expectation conditonal on $\sigma$ and apply the fact that $\sigma$ is subgaussian: $$\leq \Pi_i \mathbb{E}\exp{(t/n)^2(Z_i - Z_i')^2/8}$$ where $Z_i, Z_i'$ are individual terms in the summation forming $\mathcal{L}_n, \mathcal{L}_n'$ respectively. All that remains to do is to upperbound $(Z_i - Z_i')$ and then optimize $t$ to get the upper bound. However, it's not clear to me that I can bound this difference. EDIT 2: It should also be possible to apply Theorem 2 from https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/ by showing that each term in the likelihood has bounded variance. EDIT 3: I'm pretty sure we need additional conditions on the class of bracketing functions (either bounded second moment or bounded a.s.) but I am not sure how to show that those conditions are required. For example, for the weak law of large numbers we use the condition of finite variance and arrive at a similar subgaussian convergence rate (we could also arrive at the same rate with absolute integrability condition by applying the truncation method). Although in this case, the convergence is subexponential since we only need to specify some $\epsilon_0$ where the condition holds. This question is from problem 15.4 in [1]. References [1] Devroye, Luc, LÃ¡szlÃ³ GyÃ¶rfi, and GÃ¡bor Lugosi. A probabilistic theory of pattern recognition. Vol. 31. Springer Science & Business Media, 2013","['statistics', 'concentration-of-measure', 'probability']"
3783584,Showing that increasing N (a parameter) causes the function below to shift to the right,"I've been trying to show that increasing $N$ (where $N\in\{2,3,...\}$ ) shifts the following function rightward: $$P(x;N,k):= \frac{k(N-1)e^{-kx}}{\left(1+(N-1)e^{-kx}\right)^2}, \ (k>0).$$ I made a graph in Desmos just in case it's helpful: https://www.desmos.com/calculator/fi698ohyds . Note/Fun(?) Fact: For those who are interested, this relates to the ""Contest Success Function (CSF) in differences "" discussed in Hirschleifer (1989) ; these are used to model the probability of winning a winner-takes-all ""contest"" (e.g. a conflict). The function above corresponds to $\frac{\partial}{\partial C_1} p_1(\mathbf{C})\bigg|_{\mathbf{C} = (1,0,...,0)}$ , where $\mathbf{C}=(C_1,...,C_N)$ and $p_1$ is as in equation (5) of that paper. Anyway, it's a neat paper, I think :)","['recreational-mathematics', 'functions']"
3783607,An upper bound for $\sum_{n=1}^{\infty} n^{r-2} P\{|\sum_{k=1}^{n} \sum_{i=-\infty}^{\infty} a_{i} X_{i+k}|>\varepsilon n\}$,"Let $1 \leq r<2$ and let $\left\{X, X_{i},-\infty<i<\infty\right\}$ be a sequence of pairwise i.i.d. random variables. Let $\left\{a_{i},-\infty<i<\infty\right\}$ be a sequence of real constants such that $\sum_{i=-\infty}^{\infty}\left|a_{i}\right|^{\theta}<\infty,$ where $\theta \in(0,1)$ if $r=1$ and $\theta=1$ if $1<r<2$ . Assume additionally that $EX = 0$ and $E|X|^r<\infty$ . for every $\varepsilon > 0$ , show that : \begin{array}{c}
\sum_{n=1}^{\infty} n^{r-2} P\left\{\left|\sum_{k=1}^{n} \sum_{i=-\infty}^{\infty} a_{i} X_{i+k}\right|>\varepsilon n\right\} \\
\leq \sum_{n=1}^{\infty} n^{r-2} P\{|\sum_{i=-\infty}^{\infty} a_{i} \sum_{j=i+1}^{i+n} X_{j} I(|X_{j}|>n)|>\varepsilon n / 2\} \\
+c \sum_{n=1}^{\infty} n^{r-2} P\left\{\left|\sum_{i=-\infty}^{\infty} a_{i} \sum_{j=i+1}^{i+n}\left(X_{j} I\left(\left|X_{j}\right| \leq n\right)\right.\right.\right. 
\left.\left.-E X_{j} I\left(\left|X_{j}\right| \leq n\right)\right) |>\varepsilon n / 4\right\}
\end{array} where $c$ is a constant. In case it helps, I've already proven that : $$\sum_{k=1}^{n} \sum_{i=-\infty}^{\infty} a_{i} X_{i+k}=\sum_{i=-\infty}^{\infty} a_{i} \sum_{j=i+1}^{i+n} X_{j}$$ and that : $$n^{-1}\left|E \sum_{i=-\infty}^{\infty} a_{i} \sum_{j=i+1}^{i+n} X_{j} I\left(\left|X_{j}\right| \leq n\right)\right| \to 0$$ this inequality that I'm stuck with is used to proved a lemma that is helpful in proving a variant of the strong law of large numbers for pairwise i.i.d random variables. any tips or comments will be greatly appreciated, thanks ! Edit : it's from this paper, page 507/7","['law-of-large-numbers', 'probability-theory', 'random-variables']"
3783616,"Why is $ \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right) \neq \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) $","Known Definition : $$ \tan{\left(\frac{\pi x}{2}\right)} = \frac{4}{\pi} \sum_{n=0}^{\infty} \frac{x}{(2n+1)^{2}-x^2} $$ I integrate both sides with respect to $x$ to obtain : $$ \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right)= \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) $$ However, this appears to NOT be true due to the series on the RHS diverging, why does this occur? and where am I making the mistake? Thank you kindly for your help and time.","['integration', 'sequences-and-series']"
3783639,"$\frac{1}{d_1} + \dots + \frac{1}{d_k} = 1,$ and $\gcd(d_i,d_j)>1 \, \forall i,j$ implies $\gcd(d_1, \dots, d_k) > 1$ for distinct $d_i.$","Conjecture: If $d_i \in \mathbb{N}$ are distinct, $\frac{1}{d_1} + \dots + \frac{1}{d_k} = 1,$ and $\gcd(d_i,d_j)>1 \, \forall i,j,$ then $\gcd(d_1, \dots, d_k) > 1.$ Motive: In the process of solving a question related to covering $\mathbb{N}$ with arithmetic progressions, I found that the following statement would allow me to finish my proof by contradiction. Clearly, the conjecture is true if any of $d_1, \dots, d_k$ are prime powers. If none of them are prime powers, then they must be among $6, 10, 12, 14, 15, 16, 18, 20, 21, \dots$ Since $S = \frac{1}{6}+ \frac{1}{10}+ \frac{1}{12}+ \frac{1}{14}+ \frac{1}{15}+ \frac{1}{16}+ \frac{1}{18}+ \frac{1}{20}+ \frac{1}{21} + \frac{1}{22}+\frac{1}{24}+\frac{1}{26}+\frac{1}{28}+\frac{1}{30}+\frac{1}{33}+\frac{1}{34}+\frac{1}{35} < 1 < S + \frac{1}{36},$ any counterexample must have $k \ge 18.$ But including a number with $2$ distinct prime divisiors severely restricts the numbers we are allowed to take afterwards, so we could perform further casework and make the bound for a potential counterexample higher and higher. Warning: You cannot rely solely on analysis because if $=1$ is replaced with $\ge 1,$ the result is false. There are $2$ constructions that demonstrate this: $\{6, 10, 15\} \cup \{30a : a \le n\}$ for $n$ large enough. $\{2 \cdot 3 \cdot 5 \cdot 7 = 210\} \cup S'$ where $S'$ is a finite subset of $S = \{2^a 3^b 5^c : \text{at most one of } a, b, c \text{ is zero}\}$ with sum $\ge 209/210.$ This is possible since $\sum\limits_{s \in S} \frac{1}{s} = 1.$","['number-theory', 'combinatorics', 'egyptian-fractions']"
3783658,"Let $\frac{\tan A}{1-\tan^2A}=\sin^220^\circ-\sin160^\circ\sin220^\circ+\sin^2320^\circ$, find $\tan6A$","Let $\dfrac{\tan A}{1-\tan^2A}=\sin^220^\circ-\sin160^\circ\sin220^\circ+\sin^2320^\circ$ , find $\tan6A$ My attempt : \begin{align*}
\dfrac{\tan2A}{2}=\sin^220^\circ-\sin20^\circ\sin40^\circ+\sin^240^\circ\\
\tan2A=2(\sin^220^\circ-\sin20^\circ\sin40^\circ+\sin^240^\circ)
\end{align*} and \begin{align*}
\tan6A&=\tan(2A-60^\circ)\tan2A\tan(2A+60^\circ)\\
&=(\dfrac{\tan2A-\sqrt{3}}{1+\sqrt{3}\tan60^\circ})(\tan2A)(\dfrac{\tan2A+\sqrt{3}}{1-\sqrt{3}\tan60^\circ})
\end{align*} give $$\tan6A=(\dfrac{2(\sin^220^\circ-\sin20^\circ\sin40^\circ+\sin^240^\circ)-\sqrt{3}}{1+\sqrt{3}\tan60^\circ})(2(\sin^220^\circ-\sin20^\circ\sin40^\circ+\sin^240^\circ))(\dfrac{2(\sin^220^\circ-\sin20^\circ\sin40^\circ+\sin^240^\circ)+\sqrt{3}}{1-\sqrt{3}\tan60^\circ})$$ This method is incredibly long, there may be better way to deal with this problem.",['trigonometry']
3783690,Changing variables in integration over spheres,"Suppose we would like to change variables in the integral $$I:=\int_{\mathbb{S}^{n-1}}f(\omega_1,\omega_2,...,\omega_{n})d\sigma_{n-1},$$ where $\mathbb{S}^{n-1}$ is the standard unit sphere in $\mathbb{R}^{n}$ , $n\geq 2$ , $d\sigma_{n-1}$ is the surface measure induced by the Lebesgue measure on $\mathbb{R}^{n}$ , and $\left(\omega_{1}(\theta_{1},...,\theta_{n-1}),
\omega_{2}(\theta_{1},...,\theta_{n-1}),...,\omega_{n}(\theta_{1},...,\theta_{n-1})\right)$ is a unit vector that gives the parametric spherical representation of every point $(x_1,...,x_n)$ that lies on the sphere.
So, for example, every $(x,y)\in\mathbb{S}^{1}$ has the representation $(x,y)=(\omega_1,\omega_2)=(\cos{\theta_{1}},\sin{\theta_1})$ , $\theta_{1}\in [0,2\pi]$ , and every $(x,y,z)\in\mathbb{S}^{2}$ has the representation $(x,y,z)=(\omega_1,\omega_2,\omega_3)=(\sin{\theta_{1}}\cos{\theta_2},\sin{\theta_1}\sin{\theta_{2}},\cos{\theta_{1}})$ , $\theta_{1}\in[0,\pi], \theta_{2}\in[0,2\pi]$ . Question: How to change variables in the integral $I$ ? My question is about the Jacobian. Precisely, if we change variables $\omega_{i}=\phi_{i}(\omega_{1},\omega_{2},...,\omega_{n})$ where $\phi_{i}$ are continuously differentiable and invertible, is it correct that $$I=\int_{\cup_{\theta_1,\theta_2,...,\theta_{n-1}}{(\phi_{1},...,\phi_{n})}} f(\phi_{1},...,\phi_{n})\det\left(\frac{\partial(\omega_1,...,\omega_n)}{\partial(\phi_1,...,\phi_n)}\right)\,d\sigma_{n-1} ?$$","['multivariable-calculus', 'change-of-variable', 'real-analysis']"
3783731,Why am I getting derivative of $y = 1/x$ function as $0$?,"I was finding the derivative of the function: $y = 1/x$ . I did the followed steps: \begin{align*}
\frac{\frac{1}{x+dx} - \frac{1}{x}}{dx} &=\left(\frac{1}{x+dx} - \frac{1}{x} \right) \frac{1}{dx} \\
&= \frac{1}{x dx + (dx)^2} - \frac{1}{x dx}.
\end{align*} Since, $(dx)^2$ would be extremely small, I removed it, so $$\frac{1}{x dx} - \frac{1}{x dx}$$ which is equal to zero. why am I getting the derivative of $y = 1/x$ as $0$ ?","['calculus', 'derivatives']"
3783743,Equivalent condition to an operator $T$ on a complex Hilbert space being compact,"Let $H$ be a complex Hilbert space and let $T:H\rightarrow H$ be a bounded linear map. The problem asks to show $T$ is compact if and only if for any sequence $x_n$ weakly converging to $0$ , $\langle Tx_n,x_n \rangle\rightarrow 0$ . While the forward implication seem to be relatively straightforward, I'm having trouble proving the reverse implication. By reflexivity of $H$ it suffices to show that $\langle Tx_n,x_n\rangle \rightarrow 0$ implies $\|Tx_n\|\rightarrow 0$ . Also because the problem sort of suggests that something might be different for real Hilbert spaces, I tried using the polarization identity (but couldn't make it work). Any help would be appreciated. Thanks in advance!","['hilbert-spaces', 'compact-operators', 'functional-analysis', 'weak-convergence']"
3783806,Evaluate :- $\frac{(2020^2 - 20100)(20100^2 - 100^2)(2000^2 + 20100)}{10(2010^6 - 10^6)}$,"Evaluate :- $\frac{(2020^2 - 20100)(20100^2 - 100^2)(2000^2 + 20100)}{10(2010^6 - 10^6)}$ What I Tried :- I couldn't think of any ways to factorise this expression . The denominator can be written as $10(2016^3 - 10^3)(2016^3 + 10^3)$ , but I can't understand how it will help here . I have absolutely no idea how to factorise the numerator except that it can be $(2020^2 - 20100)(20100 - 100)(20100 + 100)(2000^2 + 20100)$ , other than that I got no idea, and it seems to me the only way to get it is to open the brackets , which will contain a lot of calculations . Wolfram Alpha gives the answer to be $10$ . But I am looking for some clever way so that this expression gets factorised and I can get my answer in less calculations . Can anyone help?","['proof-writing', 'factoring', 'polynomials', 'algebra-precalculus', 'problem-solving']"
3783827,Show that for a properly embedded submanifold the manifold and topogoical boundary coincide,"Let $d\in\mathbb N$ and $M\subseteq\mathbb R^d$ be a $d$ -dimensional properly embedded $C^1$ -submanifold of $\mathbb R^d$ . Let $\partial M$ and $M^\circ$ denote the manifold boundary and interior and $\operatorname{Bd}M$ and $\operatorname{Int}M$ denote the topological boundary and interior of $M$ , respectively. How can we show that $\partial M=\operatorname{Bd}M$ and $M^\circ=\operatorname{Int}M$ ? Note that $M$ being properly empedded into $\mathbb R^d$ is equivalent to $M$ being $\mathbb R^d$ -closed. So, $\operatorname{Bd}M=M\setminus\operatorname{Int}M$ . Let $x\in\partial M$ . In order to prove $x\in\operatorname{Bd}M$ , all we need to show is that every neighborhood of $x$ has a nonempty intersection with $M^c$ . There is a $C^1$ -diffeomorphism from an $M$ -open neighborhood $\Omega$ of $x$ onto an open subset $U$ of $\mathbb H^d:=\mathbb R^{d-1}\times[0,\infty)$ and $$u:=\phi(x)\in\partial\mathbb H^d=\mathbb R^{d-1}\times\{0\}\tag1.$$ Since $U$ is $\mathbb H^d$ -open, $$U=V\cap\mathbb H^d\tag2$$ for some open subset $V$ of $\mathbb R^d$ and since $V$ is $\mathbb R^d$ -open, $$B_\varepsilon(u)\subseteq V\tag3$$ for some $\varepsilon>0$ . Now, clearly, $$B_\varepsilon(u)\cap\left(\mathbb R^d\setminus\mathbb H^d\right)\ne\emptyset\tag4.$$ But how can we conclude? Note that $$\phi=\left.\tilde\phi\right|_\Omega\tag5$$ for some $\tilde\phi\in C^1(O,\mathbb R^d)$ for some $\mathbb R^d$ -open neighborhood $O$ of $\Omega$ .","['submanifold', 'manifolds-with-boundary', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
3783834,Do these generalization of riemannian/semi-riemannian geometry have names?,"In Riemannian geometry we require that the metric $g$ be a positive definite real matrix and in semi-Riemannian geometry we relax this requirement and instead ask that the metric be a non degenerate symmetric matrix. If we relax further and simply ask that the matrix be non degenerate, that is, we allow for asymmetric matrices with non zero determinant, what kind of object do we get? Does this object have a name? Similarly, if we generalize further and allow the matrix to be any complex matrix with non zero determinant, does this have a name? Finally, what if we allow the matrix to be complex valued but require it to be self adjoint, what is this called? I would like to know if any of these objects have been studied and if so what they are called. Any references are greatly appreciated. If you need clarification please let me know.","['semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
3783860,number of solutions of $f(f(f(f(x))))$,"Let $f(x)={x^2+10x+20}$ then the number of real solutions of $f(f(f(f(x))))=0$ My try: I first tried finding the roots of $f(x)=0$ they came out to be $-5+\sqrt{5}$ and $-5-\sqrt{5}$ . Let these roots be $\alpha$ and $\beta$ thus $f(\alpha)=0$ also if $f(f(f(f(x))))=0$ then $f(f(f(x)))=\alpha$ I was able to go only till this. I skipped the method of actually finding the composite function as it would be too cumbersome. Any ideas, helps will be appreciated.","['contest-math', 'functions', 'quadratics']"
3783876,Let $1\in S\subseteq G$ such that the left cosets $aS$ with $a \in G$ partition the group $G$. Prove that $S\le G$.,"I am a complete beginner at group theory and I was looking at the following problem. Let $S$ be a subset of a group $G$ that contains the identity element $1$ and such that the left cosets $aS$ with $a \in G$ partition $G$ .Prove that $S$ a is a subgroup of $G$ . I tried the following: If we want to show that $S$ is a subgroup of $G$ , then we need to satisfy the following: $S \subseteq G$ $1 \in S$ $a,b\in S \implies ab \in S$ $\forall a\in S,\, \exists a^{-1} \in S,\, aa^{-1}=a^{-1}a=1$ I know that $S$ is a subset of $G$ so the first requirement is satisfied. It is also given that $1 \in S$ , so the second requirement is satisfied. To prove closure under composition (3), suppose $p,q \in S$ . Then $ap,aq \in aS$ . We want to show that $a(pq) \in aS$ as well, for some arbitrary $a \in S$ . If we let $b = ap$ and $c = aq$ , we have $a^{-1}b = p$ and $a^{-1}c = q$ . Left multiplying the equations gives $pq = a^{-1}ba^{-1}c$ . since $a^{-1} \in G$ , I can say that $pq$ is in another partition $a^{-1}S$ , and so it must be the case that $pq \in a^{-1}S$ . Now this is where I'm stuck because I want to get rid of that $a^{-1}$ in front of the $S$ , but I don't know how. I've also read this question here Let $S$ be a subset of a group $G$ that contains the identity element $1$ and such that the left cosets $aS$ with $a$ in $G$, partition $G$. , but I can't understand what the top answer is trying to argue at all, even after reading the comments below it. Can I please have some help with this problem?","['group-theory', 'abstract-algebra']"
3783889,Hard problem :Prove that $f(x)<\frac{3}{2}x-\frac{1}{2}$,Define the function $1<x<\frac{103}{100}$ : $$f(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}+(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ Prove that : $$f(x)<\frac{3}{2}x-\frac{1}{2}$$ I strongly believe that it is a problem of tangent . So the natural way is to use derivative but first there is a limit that I cannot evaluate $$\lim_{x\to 1^{+}}(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}=0$$ I have tried power series at the first order of the Gamma function ( $x=1$ ) we have : $$\Gamma\Big(\frac{1}{1-x}\Big)=1-\gamma\Big(\frac{1}{1-x}-1\Big)+O\Big(\Big(\frac{1}{1-x}-1\Big)^2\Big)$$ But it's ineffective unfortunately . Why this limit ? Because it implies that if: $$g(x)=(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ Then : $$\lim_{x\to 1^{+}}g'(x)=0$$ Then we have just to evaluate the derivative of : $$h(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ As $x\to 1^{+}$ An conclude with the formula : $$y=f'(x_0)(x-x_0)+f(x_0)$$ My question : How to prove the strict inequality ? Thanks in advance for your help !,"['gamma-function', 'power-series', 'limits', 'inequality', 'derivatives']"
3783900,$\sigma$-algebra generated by open balls,"We know that the $\sigma$ -algebra generated by open balls in $\mathbb{R}^n$ is the Borel $\sigma$ -algebra (with the Euclidean metric). In general, does the set of open balls of a metric space generate the Borel $\sigma$ -algebra?  If not, is there a sufficient condition for this to be the case?","['borel-sets', 'measure-theory']"
3783980,Find the greatest integer less than $\frac{1}{\sin^2(\sin(1))}$ without calculator.,"Find the greatest integer less than $$\frac{1}{\sin^2(\sin(1))}$$ This was on one of my tests. All angles in radians. Here's my work: $$0<1<\frac{\pi}{3}<\frac{\pi}{2}$$ Since $\sin(x)$ is increasing in the first quadrant, $$0<\sin(1)<\sin\left(\frac{\pi}{3}\right)=\frac{\sqrt{3}}{2}<\sin\left(\frac{\pi}{2}\right)=1<\frac{\pi}{2}$$ $\sin(1)$ radians and $\frac{\sqrt{3}}{2}$ radians also lie in the first quadrant, and $\sin^2(x)$ is increasing in the first quadrant $$\sin^2(\sin(1))<\sin^2\left(\frac{\sqrt{3}}{2}\right)<\frac{3}{4}$$ $$\frac{1}{\sin^2(\sin(1))}>\frac{4}{3}$$ But I cannot find an upper bound on my expression. Any help?","['trigonometry', 'number-comparison', 'ceiling-and-floor-functions', 'inequality']"
3784056,Is $\frac{\cos^2 (x)\sin(nx)}{\sin(x)}$ a trigonometric polynomial?,Is $\frac{\cos^2 (x)\sin(nx)}{\sin(x)}$ trigonometric polynomial of degree at most $n$ ? I need this result to show that for any trigonometric polynomial $T(t)$ of degree $n$ exists $a$ such that $$T(t)=a\cos(nt)+\frac{\cos(nt)}{2n}\sum\limits_{k=1}^{2n}(-1)^kT(t_k)\cot\left(\frac{t-t_k}{2}\right)$$ Where $$t_k=\frac{2k-1}{2n}\pi$$,"['trigonometry', 'interpolation', 'polynomials']"
3784074,Confused with this SVD problem: Does it matter which singular vectors you choose?,"I am trying to decompose the following matrix using the Singular Value Decomposition (SVD): $$A = \begin{bmatrix}
    4 & 4\\
    -3 & 3\\
    \end{bmatrix} = U\Sigma V^T$$ Here is my work (I know this is far from the most efficient way to do SVD, but please follow along my way): Finding $\Sigma$ and $V$ : $$A^T A = \begin{bmatrix}25 & 7\\7 & 25\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, v_1 = \begin{bmatrix}1/\sqrt2\\1/\sqrt2\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, v_2 = \begin{bmatrix}1/\sqrt2\\-1/\sqrt2\\\end{bmatrix}$$ So, $$V = \begin{bmatrix}1/\sqrt2 & 1/\sqrt2\\1/\sqrt2 & -1/\sqrt2\\\end{bmatrix} \quad \text{and} \quad \Sigma = \begin{bmatrix} \sqrt{32} & 0 \\ 0 & \sqrt{18}\\\end{bmatrix} $$ Finding $U$ : $$A A^T = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix} \quad\text{with } \lambda_1 = 32, u_1 = \begin{bmatrix}1\\0\\\end{bmatrix} \quad\text{and } \lambda_2 = 18, u_2 = \begin{bmatrix}0\\1\\\end{bmatrix}$$ So, $$U = \begin{bmatrix}32 & 0\\0 & 18\\\end{bmatrix} $$ However, $$U \Sigma V^T = \begin{bmatrix}4 & 4\\3 & -3\\\end{bmatrix} \neq A$$ Did I do something wrong? Next attempt: This time, I used $u_2 = \begin{bmatrix}0\\-1\\\end{bmatrix}$ instead of $\begin{bmatrix}0\\1\\\end{bmatrix}$ . So, $U = \begin{bmatrix}1 & 0\\0 & -1\\\end{bmatrix}$ . Now, it seems to work: $$U \Sigma V^T = \begin{bmatrix}4 & 4\\-3 & 3\\\end{bmatrix} = A.$$ So my question is: Does it matter which singular vectors you choose for $U$ and $V$ ? In other words, if you find a singular vector $x$ with unit length, how do you know to choose $x$ or $-x$ ? I know that in  Eigenvalue decomposition, it didn't matter because you can change the diagonal matrix $\Lambda$ accordingly. What about in SVD?","['matrices', 'linear-algebra', 'svd', 'eigenvalues-eigenvectors']"
3784094,"""Real Line and Complex plane are the same set just with different topologies"" explanation","I was reading the ""topology"" article on Wikipedia and They stated the following: ""For instance, the real line, the complex plane, and the Cantor set can be thought of as the same set with different topologies."" My question is, which set and what different topologies generate the real line and the complex plane? I don't see how this is possible. Because clearly $\mathbb R \neq \mathbb C$ , so how do we have a set $K$ such that $(K,\tau_1) = \mathbb R$ , but $(K,\tau_2) = \mathbb C$ ?",['general-topology']
3784109,Ratio of the sum of the four circles to the area of the original right triangle,"A right triangle is divided into two smaller right triangles by the altitude $CD$ to its hypotenuse $AB$ as shown in the diagram. Circle $O$ with radius $r$ is inscribed in the $\unicode {0x25FA} BCD$ . The $\unicode {0x25FA} CAD$ contains 3 circles which are tangent to each other and also to the sides of the triangle as depicted in the diagram. All four circles are congruent . What is the ratio of the sum of the areas of the four circles to the area of the original $\unicode {0x25FA} ABC$ ? My solution: Say, $BC = a, CA = b, AB = c$ . As $\unicode {0x25FA} BCD \sim \unicode {0x25FA} CAD$ , it is easy to see that $\angle OBC = \angle ACE$ and $\angle OCB = \angle CAG$ . Given that the circles are congruent, $BH = CI, CH = AJ, IJ = 4r$ . So we get, $b-a = 4r$ . Given $\unicode {0x25FA} BCD \sim \unicode {0x25FA} ABC$ , the ratio of their inradius will be the ratio of their hypotenuse (or other sides). So, $r \times c = a \times \dfrac{a+b-c}{2}$ or $c(b-a) = 2a(a+b-c)$ or $(a+b)(2a-c) = 0$ . So, $c = 2a$ and hence $\angle A = 30^0, \angle B = 60^0$ . $BC = 2BD = 2r(1+cot30^0) = 2r(\sqrt3+1)$ $AC = BC+4r = 2r(3+\sqrt3)$ Area of $\unicode {0x25FA} ABC = \dfrac{1}{2} \times AC \times BC = 4r^2(2\sqrt3 + 3)$ So, the ratio of the sum of the areas of 4 circles to the triangle ABC $= \dfrac{4 \pi r^2}{4r^2(2\sqrt3+3)} = \dfrac{\pi}{3} (2\sqrt3-3)$ Coming to the purpose of posting the question here - With the arrangement of the congruent circles and division of the triangle $ABC$ , we come to the conclusion that $c = 2a$ . I am getting to it with some calculation. Is there a more obvious way to get to the conclusion or is there any theorem which establishes it? Is there a better and faster solution to the problem than what I already have?",['geometry']
3784195,Intuition for the invariance lemma,"The invariance lemma, as stated below, is commonly used to prove Lieâ€™s theorem. Lemma (Invariance lemma). Let $\mathbb{k}$ be a field of characteristic zero.
Let $\mathfrak{g}$ be a $\mathbb{k}$ -Lie algebra, let $M$ be a finite-dimensional representation of $\mathfrak{g}$ and let $I$ be an ideal of $\mathfrak{g}$ .
Let $\lambda$ be an element of the dual space $I^*$ and let $$
  M_\lambda
  :=
  \{
    m \in M
    \mid
    \text{$x m =â€¯\lambda(x) m$ for all $xâ€¯\in I$}
  \}
$$ be the corresponding $I$ -weight space of $M$ .
This weight space is already a $\mathfrak{g}$ -subrepresentation of $M$ . Every time Iâ€™m reading through my old notes on representation theory of Lie algebras Iâ€™m wondering where this lemma comes from.
I have no intuition for why it should be true, and if I hadnâ€™t seen a proof of it I would probably even suspect it to be false. What is the intuition behind the invariance lemma?","['abstract-algebra', 'representation-theory', 'lie-algebras', 'intuition']"
3784235,Is there a proof for $\frac{\mathrm{d}y}{\mathrm{d}x} = - \frac{\frac{\partial }{\partial x}}{\frac{\partial }{\partial y}}$? [duplicate],"This question already has an answer here : Proving $f' = - \frac{\partial{F}/\partial{x}}{\partial{F}/\partial{y}}$ (1 answer) Closed 3 years ago . I've read that $$\frac{\mathrm{d}y}{\mathrm{d}x} = - {\frac{\frac{\partial}{âˆ‚x}}{\frac{âˆ‚}{âˆ‚y}}}$$ For example, if $f(x,y) = 2xy + y^2,$ $$\frac{\partial f(x,y)}{\partial x} = 2y \ \ \text{ and }\ \ \frac{\partial f(x,y)}{\partial y} = 2x+2y, $$ so $\displaystyle \frac{\mathrm{d}y}{\mathrm{d}x} = -\frac{2y}{2x+2y}$ Is there a proof for this assertion? I can't seem to find one.","['multivariable-calculus', 'implicit-differentiation', 'derivatives']"
3784278,What's the problem with differentiating $y = \sin(x^2)$ by applying the limit definition of a derivative directly?,"I was taking the derivative of $y = \sin(x^2)$ . I know that we can solve it by applying chain rule, but i tried without any rules, just like a normal method. This is what i did: $$\frac{\sin((x + h)^2) - \sin((x)^2)}{h}$$ Is this method correct? If not, then why? Because wherever I search for the derivative of $y = \sin(x^2)$ , none did like this. And also I am not able to come to the proper answer which is $2x\cos(x^2)$ through that method. Can somebody help me!","['calculus', 'derivatives']"
3784326,Logic puzzle - All people born in Italy are citizens of EU,I need to choose a logically equivalent statement to the title out of the following list (A to E). A. All EU citizens are born in Italy. B. All non-EU citizens were not born in Italy. C. Some non-EU citizens may have been born in Italy. D. Those not born in Italy are not EU citizens. E. Those born in Italy are not necessarily EU citizens. I thought about it and I think it's D because the premise is that all people born in Italy are citizens of EU. Is it correct?,"['solution-verification', 'logic', 'discrete-mathematics']"
3784351,"Differential of $\langle f,g \rangle $, if $f,g: E\subset \mathbb{R}\to \mathbb{R}^{m}$ are differentiable functions.","If $f,g: E\subset \mathbb{R}\to \mathbb{R}^{m}$ are differentiable real functions. Prove that $$\frac{d}{dt}\left\langle f(t),g(t) \right\rangle=\left\langle f(t), \frac{d}{dt}g(t) \right\rangle +\left\langle \frac{d}{dt}f(t), g(t) \right\rangle$$ How can I prove that? I know that if $f,g: U\subset \mathbb{R}^{n}\to \mathbb{R}$ are differentiable functions, so $$d(fg)=fd(g)+gd(f)$$ Can I use it?","['multivariable-calculus', 'real-analysis']"
3784368,Problems with Set Function,"(Questions are at the bottom of the post. Iâ€™ve added two more doubts since the recent answer.) Introduction Consider $f:A\to\mathbb{R}$ where $A\subseteq[a,b]$ , $a,b \in \mathbb{R}$ and $S$ is a fixed subset of $A$ . Before mentioning my set function, it is important to know I need it to compute my average. The average should satisfy the following Positivity: If $f>0$ , $\operatorname{average}(A,f)>0$ . Linearity: $\operatorname{average}(A,f+g)=\operatorname{average}(A,f)+\operatorname{average}(A,g)$ , and $\operatorname{average}(A,cf(x))$ is $c \times\text{average}(A,f(x))$ . As $f\to\text{constant function}$ , $\operatorname{average}(A,f)\to\text{constant}$ The $\operatorname{average}{(A,f)}$ should give a defined, unique value when $f$ is defined on a measurable set. Take for example functions defined on Lebesgue measurable sets. The average of $f$ should satisfy $\inf f \le \operatorname{average} (A,f) \le \sup f$ when $f$ is defined on measurable sets. The Average defined by the Lebesgue Measure and Integral does not fit all the requirements. If $\lambda(A)$ is the Lebesgue Measure of $A$ and $\lambda(A)=0$ , then the average of $f$ is undefined breaking rules 4 and 5. I wish to define an average that matches all the rules and gives the following. When $\lambda(A)>0$ , the average should be the Lebesgue Average. When $A$ is finite the average of $f$ should be $$\frac{1}{|A|}\sum_{x\in A}f(x)$$ When $A$ is infinite and $\lambda(A)=0$ , divide $[a,b]$ into $r$ equal sub-intervals. Take the average of $f$ over the infimum of all sub-intervals that meet $A$ . Call this the lower average. Take the average of $f$ over the supremum of all sub-intervals that meet $A$ . Call this the upper average. As $r\to\infty$ , if the lower and upper average meet at the same value, call this the total average. When the total average is defined it should equal the average I want to define. There are cases of $f$ where neither of these averages can give a defined value. Rather we generalize these definitions into my average and see that supports rules 4. and 5. The problem may not be well-defined since an average may be unable to exist on all zero Lebesgue Measure sets. However, before working on my average, I would like to work on my set function which I shall use to compute my average. Note the set function I am defining may not be a measure. It could be finitely additive but give positive values for sets with zero Lebesgue measure. Definition of Outer Set Function $\mu^{*}(c,S)$ If we define the following: $I=[a,b]$ $a,b\in\mathbb{R}$ $\left(I_k\right)_{k=1}^{m}$ are $m$ open sub-intervals of $I$ $\ell(I)=b-a$ is the length of $I$ $\ell(I_k)=c\in\mathbb{R}^{+}$ is the length of $I_k$ for $k=1,...,m$ $$\Omega(S\cap I_k)=\begin{cases} 
0 & S\cap I_k \ \text{is countable}\\
1 & S\cap I_k \ \text{is uncountable} \\ 
\end{cases}$$ then $\mu^{*}(c,S)$ is the outer piece-wise set function defined as \begin{align*}
& \mu^{*}(c,S)=  \begin{cases}\inf\limits_{m\in\mathbb{N}}\left\{ \sum\limits_{k=1}^{m}c\ \Omega\left(S\cap I_{k}\right): S\subseteq\bigcup\limits_{k=1}^{m}I_{k}\right\} & A \ \text{is uncountable}\\
\inf\limits_{m\in\mathbb{N}}\left\{\sum\limits_{k=1}^{m}c : S\subseteq\bigcup\limits_{k=1}^{m}I_{k} \right\} & A \ \text{is countable}\\
\end{cases}
\end{align*} Explanation of $\mu^{*}(c,S)$ (Skip if you understand my definition) The parameter $c$ must remain a variable throughout the computation of $\mu^{*}(c,S)$ . When $A$ is uncountable, $\mu^{*}(c,S)$ should equal $cm^{\prime}_{\text{min}}$ , where $m^{\prime}_{\text{min}}$ is the minimum number of $I_k$ that cover $S$ for $k$ where $S\cap I_k$ is uncountable. The reason we set some $I_k$ to lengths zero is that countable subsets of $A$ , such as countable $S\cap I_k$ , are regarded to be as small as the null set compared to the uncountable set. Hence we ""prevent"" corresponding $I_k$ from covering this part of $S$ by setting their ""lengths"" to zero instead of $c$ . When $A$ is countable $\mu^{*}(c,S)$ should equal $cm_{\text{min}}$ where $m_{\text{min}}$ is the minimum number of $I_k$ that can cover $S$ . Here $S\cap I_k$ can be countable (instead of uncountable). When $A$ is countable, we do not set the length of any $I_k$ to be zero, since countable subsets of countable $A$ , should be covered by $I_k$ such that the total length of $I_k$ should have a positive proportional value to $\mu^{*}(c,A)$ . Inner Set Function $\mu_{*}(c,S)$ and Total Set Function $\mu(c,S)$ where $\mu(c,S)=\mu^{*}(c,S)=\mu_{*}(c,S)$ The inner set function should be $$\mu_{*}(c,S)=\mu^{*}(c,A)-\mu^{*}(c,A\setminus S)$$ Making the total set function $\mu(c,S)$ defined for values of $c$ where, $$\mu(c,S)=\mu^{*}(c,S)=\mu_{*}(c,S)$$ I added this so the total set function would be as rigorous as the Lebesgue Measure. I'm not sure whether this is necessary. The problem is for most values of $c$ , $\mu^{*}(c,S)\neq \mu_{*}(c,S)$ , so to fix this I set the limit as $c\to 0$ . $$\lim_{c\to 0}\mu(c,S)=\lim_{c\to 0}\mu^{*}(c,S)=\lim_{c\to 0}\mu_{*}(c,S)$$ Questions What is $\lim\limits_{c\to 0}\mu(c,\mathbb{Q})$ , $\lim\limits_{c\to 0}\mu(c,\ln(\mathbb{Q}_{>0}))$ , and $\lim\limits_{c\to 0}\mu(c,\mathbb{Q}\cup\ln(\mathbb{Q}_{>0}))$ , where $A=\mathbb{Q}\cup\ln(\mathbb{Q}_{>0})$ ? My guess is all the answers would be $\lim\limits_{c\to 0}c\left\lceil\frac{b-a}{c}\right\rceil$ since they are countable and dense in $[a,b]$ . If I am correct, here is the problem... If my set function is finitely additive: $$\lim_{c\to 0}\mu(c,\mathbb{Q}\cup\ln(\mathbb{Q}_{>0}))=\lim_{c\to 0}\left(\mu(c,\mathbb{Q})+\mu(c,\ln(\mathbb{Q}_{>0}))\right)=2(b-a)$$ However, $\lim\limits_{c\to 0}\mu(c,\mathbb{Q}\cup\ln(\mathbb{Q}_{>0}))=\lim\limits_{c\to 0}c\left\lceil\frac{b-a}{c}\right\rceil=b-a$ The finite additivity on my set function doesn't give this, hence either my calculations are wrong or my set function is not finitely additive. Which is true? What should be $\mu^{*}(c,\mathbb{Q})$ and $\mu^{*}(c,\ln(\mathbb{Q}_{>0}))$ instead? Edit: According to a recent answer, the set function is not well-defined for this specific case. Here are a few more things I need resolved. When $A=[a,b]$ , does my set function of $S$ equal the Lebesgue measure of $S$ . If $A=\mathbb{Q}$ could we solve $\lim_{c\to 0} \mu(c,S)$ by doing the following: Take elements of $S$ that have $n$ decimal places. Take $\lim\limits_{c\to 0}\mu$ of these elements as $n\to\infty$ . If we cannot do this, how can we change my definition of $\lim\limits_{c\to 0}\mu(c,S)$ so this is possible.","['amenability', 'measure-theory', 'outer-measure', 'means', 'average']"
3784420,Are partitions of unity needed in manifolds to define a measure coinciding with the Lebesgue measure under coordinate systems?,"Consider a Hausdorff smooth manifold $M^n$ . In the book Riemannian Geometry by Gallot, Hulin and Lafontaine, proposition 6.1.4 reads as follows: If $M$ is compact (or locally compact and countable at infinity), there always exists a density. (...) In such context, I understand that the idea for defining a density would be: let $\{\rho_i\}$ be a partition of unity strictly subordinate to a smooth atlas $\{(U_i,\phi_i)\}$ . Given a local chart $(U_i,\phi_i)$ and a Lebesgue measurable $E \subset \phi_i(U_i)$ , define $\mu_i(E)=\sum_j \int_{\phi_i(U_i \cap U_j)} \rho_j \chi_E dm=m(E)$ , where m is the Lebesgue measure on $\mathbb{R}^n$ . I understand that such definition allows us to prove the second axiom of densities. (i.e., the Change of Variables Theorem holds for continuous functions supported on the intersection of local charts) On the other hand, it does not seem clear to me whether the definition $\mu_i(E)=m(E)$ would likewise define a density for a non-paracompact $M$ . In other words, my question is: could we use such definition and obtain a density in a non-paracompact manifold? Edit: the problem was that I guessed the idea for defining the density wrongly! The correct definition for a measurable $E \subset \phi_i(U_i)$ would be $\mu_i(E)=\sum_j \int_{\phi_j(U_j)} \chi_{\phi_j \circ \phi_i^{-1}(E)} (\rho_j \circ \phi_j^{-1}) dm \neq m(E)$ , which is reasonable, as it takes into account how the volume is deformed through the diffeomorphisms between charts.","['measure-theory', 'smooth-manifolds']"
3784422,Understanding the Topology in a Proof about Ergodic/Recurrent Transformations,"A portion of Lemma 3.7.2 of Silva's ""Invitation to Ergodic Theory"" states the following: Lemma. Let $(X, \mathcal{S}, \mu)$ be a $\sigma$ -finite measure space and let $T: X \to X$ be a measure-preserving transformation. Then, the following two statements are equivalent If $A$ and $B$ are sets of positive measure, then there exists an integer $n > 0$ such that $$T^{-n}(A) \cap B \neq \emptyset.$$ If $A$ and $B$ are sets of positive measure, then there exists an integer $n > 0$ such that $$\mu(T^{-n}(A) \cap B) > 0.$$ Clearly, only (1) $\implies$ (2) needs to be shown, and the proof is short: Let $A, B$ be sets of positive measure. For the sake of contradiction, suppose $\mu(T^{-n}(A) \cap B) =0 $ for all $n$ . Let $A_0 = A \backslash \bigcup_{n = 1}^\infty T^{-n}(A) \cap B$ . Then $\mu(A_0) > 0$ but $T^{-n}(A_0) \cap B = \emptyset$ , a contradiction. I'm having trouble seeing the argument for $$A_0 := A \backslash \bigcup_{n = 1}^\infty T^{-n}(A) \cap B \implies T^{-n}(A_0) \cap B = \emptyset.$$ What exactly is the reasoning for this claim? Any hints, discussion, and solutions are appreciated.","['general-topology', 'ergodic-theory', 'measure-theory']"
3784423,Exercise 11.1.G Vakil FOAG,"I am trying to solve Ex 11.1.G from Ravi Vakil's FOAG. It says if $X$ is an affine scheme over $k$ , a field and $K|_k$ is an algebraic field extension, then $X$ is of pure dimension $n$ iff $X_K:=X\times_k K$ is of pure dimension $n$ . Here's my attempt at a solution. Let $X=\operatorname{Spec}A$ . Then $X_K=\operatorname{Spec}(A\otimes_k K)$ . The canonical map $X_K \rightarrow X$ , in this case corresponds to the homomorphism of $k$ -algebras namely $A\rightarrow A\otimes_k K$ ; $a\mapsto a\otimes 1$ Since $K|_k$ is an integral extension, so is $A\otimes_k K|_A$ . Thus the Going-Up theorem holds true. Also since $K$ is a free $k$ -module, $A\otimes_k K$ is a free $A$ -module and faithfully flat. Since $A\hookrightarrow A\otimes_k K$ is a flat ring extension, the Going-Down theorem holds true. Now say $P$ be a minimal prime ideal of $A\otimes_k K$ . Then $p:=P \cap A$ is a minimal prime ideal of $A$ by Going-Down theorem. Conversely if $p$ is a minimal prime of $A$ and $P$ lies over $p$ (since it is an integral extension) then $P$ is a minimal prime ideal of $A\otimes _k K$ by Incomparability. Assume $X$ is of pure dimension $n$ . Then for any minimal prime $p$ of $A$ , we have $\operatorname{coht}p=\dim X$ . If $P$ is a minimal prime of $A\otimes _k K$ , then $\operatorname{coht} P =\operatorname{coht} P\cap A=\dim X $ by Going-Up Theorem and Incomparability. Thus $X_K$ is of pure dimension $n$ as well. Now assume $X_K$ is of pure dimension $n$ . Then if $p$ is a minimal prime ideal of $A$ , then $p=P\cap A$ where $P$ is a minimal prime of $A\otimes_k K$ . Again by Going-Up and Incomparability, we get $\dim X_K=\operatorname{coht}P=\operatorname{coht} P\cap A $ . Thus $X$ is of pure dimension $n$ . I hope this is a correct solution. I am pretty sure there exists alternative solution along the lines of Vakil's hints or preferably without using flatness since Vakil does not develop it upto this point but I simply can't get it. To be more precise, Vakil has first asked to reduce the problem to $A$ being an integral domain. I am stuck with this part without using the Going-Down theorem for flat ring extensions. Any help/insight/suggestion will be most welcome.","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'flatness', 'dimension-theory-algebra']"
3784451,Is there more than one answer to trig identities? Can the answer be both a pythagorean identity and a double angle?,I struggle with trig identities. It always seems that I make the wrong choices on the final step. For example: Will it be a Pythagorean identity or a double angle identity? Is there a principle I'm missing? Here is an example: $$h(u) =\frac{\sin x+\cos x}{\sin x-\cos x}$$ We are supposed to find the first derivative. The final step I got to was: $$\frac{-2\sin^2x-2\cos^2x}{(\sin x-\cos x)^2}$$ . I decided to use the double angle identity to make it - $$\frac{2\cos2x}{(\sin x-\cos x)^2}$$ The answer given is $$\frac{-2}{(\sin x-\cos x)^2}$$ . Was I wrong and what am I missing?,"['calculus', 'trigonometry']"
3784455,Finding the maximum value of $\sin\left((2^n)^\circ\right)$ where $n \in \Bbb N$,"Found this question in my math textbook where it asked us to find $\max\sin\left((2^n)^\circ\right)$ where $n \in \Bbb N$ and $2^n$ expressed in god forbid degrees. It's in the arithmetic section but the problem seems analytic at first.
I tried finding if $n$ exists such that $2^n=90+360k$ for some $k\in \Bbb N$ but I can't quite get some progress with it, then I tried using functions where I minimize the difference between $2^n-90-360k$ to get close to the max of the $\sin$ function that we all know is $1$ but not luck there either.
Any help would be appreciated!
PS: I know how to study functions using basic calculus, I just graduated high school. I wouldn't mind a little advanced math to explore and learn.
EDIT : just found out that $\sin(2^{96})=0.99..\gt \sin(2^6)$ which contradicts Ben's answer, maybe $2^n\mod360$ isn't periodic ?","['trigonometry', 'arithmetic']"
3784462,Equivariant Diffeomorphism $S^2 \times S^3$ to itself with respect to the following $\mathbb{Z}_4$ action,"Let $\mathbb{Z}_4$ be the cyclic group generated by $(R,j)$ where $R \in$ SO $(3)$ is the rotation matrix $R = 
\begin{pmatrix}
-1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 
\end{pmatrix}$ Consider the $\mathbb{Z}_4$ action on $S^2 \times S^3$ generated by $(x,q) \mapsto (Rx,q\overline{j})$ . Here we are thinking of $S^2$ as the standard subset of $\mathbb{R}^3$ with elements represented as column vectors and $S^3$ as unit quaternions. I am looking for an equivariant diffeomorphism $S^2 \times S^3 \rightarrow S^2 \times S^3$ that takes the $\mathbb{Z}_4$ action above, and moves the action to the $\mathbb{Z}_4$ action generated by $(x,q) \mapsto (x,q\overline{j})$ . My issue is that you cannot just use the diffeomorphism $(x,q) \mapsto (R^{T}x,q)$ because $R$ is only being multiplied by the fist coordinate when half of the elements of $\mathbb{Z}_4$ are acting.","['equivariant-maps', 'riemannian-geometry', 'group-actions', 'algebraic-topology', 'differential-geometry']"
3784470,Are all non-associative (not necessarily associative) finite division rings finite fields?,"According to the Artinâ€“Zorn theorem, any finite alternative division ring is a finite field, but I'm interested in the general non-associative (i.e. not necessarily associative) case. Are there any non-associative finite division rings different from finite fields, or are all non-associative finite division rings finite fields? Edit: We define a not necessarily associative division ring to be a set $S$ equipped with two binary operations $+$ and $\cdot$ such that $S$ with $+$ is an abelian group $S/\{0\}$ with $\cdot$ is a loop (unital quasigroup) $\cdot$ distributes over $+$ . This comes from the following defintion of a division ring: a set $S$ equipped with two binary operations $+$ and $\cdot$ such that $S$ with $+$ is an abelian group $S/\{0\}$ with $\cdot$ is a group (associative loop) $\cdot$ distributes over $+$ .","['quasigroups', 'ring-theory', 'abstract-algebra']"
3784472,Find equilibrium points when given polar coordinates,"Consider the following system given in polar coordinates $\dot{r}=-r^3+r+r\sin{(2\theta)}/2$ and $\dot{\theta} = 1+\cos^2{\theta}$ . Find all equilibria and show that there are no invariant circles centered at the origin. I know that when a system is given in cartesian coordinates then we convert to polar coordinates. And to find equilibria, we just look at values of $x$ and $y$ such that $\dot{x}$ and $\dot{y}$ are $0$ . But here, we are given only polar coordinates. Can we convert it back to cartesian coordinates and then find equilibrium points? Otherwise, using polar coordinates, I can think of only origin as the equilibrium point since $r=0$ will yield $\dot{r}=0$ . And no other point will yield $\dot{\theta}=0$ . So would origin be the only equilibrium point? And how do we show there are no invariant circles centered at the origin?",['ordinary-differential-equations']
3784475,Fixed point theorem with one-sided assumption,"The problem is stated as follows: Assume that $f:\mathbb{R} \rightarrow \mathbb{R}$ is differentiable.
(a) If there is an L < 1 such that for each $x \in \mathbb{R}$ we have $f '(x) < L$ , prove that there exists a unique point x such that $f(x) =x$ .
(b) Show by example that (a) fails if $L=1$ . I find this problem interesting because, since the derivative has no lower bound, it is not a contraction mapping problem nor is the function necessarily Lipschitz continuous. I think I understand the essence of the problem in that since the function grows at a rate strictly slower than 1 (and a rate not approaching 1), then the graph of the function must necessarily intersect the graph of $y = x$ . Only, I have no idea how to prove this mathematically. Any help with a starting point for proof of this theorem would be greatly appreciated. I tried analyzing the function $h(x) = f(x) - x$ and even $h(x)=f(x) - (x + f(0))$ , but couldn't really see how this lead to a solution.","['fixed-point-theorems', 'real-analysis', 'continuity', 'functions', 'derivatives']"
3784515,LindelÃ¶f in terms of filters,The following characterisation of compactness is well known. Let $X$ be a space. Then $X$ is compact if and only if every filter on $X$ has a cluster point if and only if every ultrafilter on $X$ converges. Now recall that a space is LindelÃ¶f if every open covering of it has a countable subcovering. My question is whether there is a characterisation of the LindelÃ¶f property in terms of filters similar to the statement given above for compactness.,"['filters', 'general-topology']"
3784555,Calculating a FrÃ©chet derivative,"Let $I$ be a set, and let $B(I)$ be the space of bounded, real-valued functions on $I$ equipped with the sup-norm. Let $\phi: \mathbb R \to \mathbb R$ be bounded and continuously differentiable everywhere. Finally, let $S: B(I) \to \mathbb R$ be linear and continuous. Define $\Phi: B(I) \to \mathbb R$ by $\Phi(x) = S(\phi \circ x)$ . Is $\Phi$ FrÃ©chet differentiable at every $x \in B(I)$ , and if so is it the case that $\Phi'(x) = S(\phi' \circ x)$ ? I can show that the function $x \mapsto S(\phi' \circ x)$ is linear and bounded, using the corresponding facts about $\phi'$ and $S$ , but I'm not sure I can show that this function satisfies the definition of the FrÃ©chet derivative. I have to show, for every $x \in B(I)$ , that $$\lim_{\| h \|_\infty \to 0} \frac{| \Phi (x + h) - \Phi(x) -  S(\phi' \circ h) |}{\| h \|_\infty} = 0,\tag{1}$$ where $\| \cdot \|_\infty$ is the sup-norm on $B(I)$ . Now, by the definition of $\Phi$ and the linearity of $S$ $$ \frac{\Phi (x + h) - \Phi(x) -  S(\phi' \circ h)}{\|h\|_\infty} =  \frac{S(\phi \circ (x+h)) - S(\phi \circ x) -  S(\phi' \circ h)}{\|h\|_\infty}  = S\Big(\frac{[\phi \circ(x+h)] - [\phi \circ x] - [\phi' \circ h]}{\|h\|_\infty} \Big).$$ From here I would like to argue that as $\|h\|_\infty \to 0$ , $$\frac{[\phi \circ(x+h)] - [\phi \circ x] - [\phi' \circ h]}{\|h\|_\infty} \to 0, \tag{2}$$ and then use the continuity of $S$ to conclude. I haven't convinced myself that (2) holds however.","['frechet-derivative', 'derivatives', 'functional-analysis', 'real-analysis']"
3784563,Maximizing An Integral Using Stokes' Theorem,"Here's a question I've been stuck on: Find a piecewise smooth, simple, closed, oriented curve $C$ which maximizes $$
\int\limits_{C} \vec{F} \,\mathrm{d}\vec{x},\quad \vec{F}(x,y,z)=\big(-y(z+1),x(z+1),0\big)
$$ among all curves $C$ restricted to lie on the three-dimensional unit sphere. I began with computing $$\nabla \times F=\big(-x,-y,2(z+1)\big)$$ which doesn't really simplify the question a lot. Next, I restricted myself to deal with situations where $z$ is held constant. In this situation, by the Stokes' Theorem, and by using Polar Coordinates, I can write out, $$\int\limits_{C} \vec{F}\, \mathrm{d}\vec{x}=\iint\limits_S \big((\nabla \times \vec{F})\cdot \vec{n}\big) r\,\mathrm{d}r\,\mathrm{d}\theta \quad\vec{n}=(0,0,1), r\in(0,a],\theta\in(0,2\pi), z^2=1-a^2 $$ Upon solving this, I got that the integrand is maximized when, $$a=1 \implies z=0\implies x^2+y^2=1$$ I'm trying to generalize this approach to any curve which lies on the unit sphere. However, the primary issue I'm encountering is a lack of a neat expression for the unit normal (for any arbitrary surface satisfying the constraints) which appears in the Stokes' Theorem. I'm not really sure how to proceed any further. Update: 23rd August 2020 So, based on some comments and help from other forums, I got the following idea. The integrand, after the application of Stokes' Theorem, reduces to: $$\iint\limits_S 3z^2+2z-1 dA$$ If you consider the function in the integral on the domain $$z\in[-1,1]$$ , you'll see that the function is non-negative when $$\frac{1}{3}\leq z<1$$ , and non-positive otherwise. Therefore, in order to maximize the integral, we need to consider the surface on the sphere enclosed between the planes $$z=\frac{1}{3}, z=1$$ I used the following parameterization: $$(x,y,z)=(\cos(\theta)\sin(\phi), \sin(\theta)\sin(\phi), \cos(\phi)), 0\leq \theta \leq 2\pi, 0 \leq \phi \leq \frac{\pi}{2}-\arctan{\frac{1}{2\sqrt{2}}}$$ Then, $$\iint\limits_S 3z^2+2z-1 dA=\iint\limits_S (3\cos^2(\phi)+2\cos(\phi)-1)\sin(\phi)d\phi d\theta = \frac{64\pi}{27}$$ I believe this is the maximum...","['integration', 'stokes-theorem']"
3784585,About the definition of the indexed set of a family,"I'm a computer science student currently studying universal algebra by reading the book Foundations of Algebraic Specification and Formal Software Development by Donald Sannella and Andrzej Tarlecki. I'm having a hard time trying to understand the subtleties of indexed families. As far as I know, families are just maps between two sets: $I$ , the index set; and $A$ , the indexed set. That means that $|A|_{i_0}$ , which is the element of $A$ indexed by $i_0$ could be basically anything, like a number, a set, a collection, etc. And that the particular case where $I$ is $\mathbb{N}$ it is called a sequence. However, the definition of the product of an indexed family is: $ \prod_{i\in I} A_i=\{f:I\to \bigcup_{i\in I} A_i: (\forall i_0\in I)(f(i_0)\in A_{i_0})\}$ Considering the definition of a family, then the product could be also defined as the set of all families $ (a_i)_{i \mathop \in I}$ with $|a|_{i_0} \in |A|_{i_0}$ for each $i_0 \in I$ But then, it is implied that the indexed set ( $A$ ) must be a collection (a set of sets), because otherwise, $|A|_{i_0}$ could be an element like a number thus the union of $|A|_i$ would make no sense, since it only works for sets. So why does the definition of a family (at least the ones I've read on several math textbooks) does not require the indexed set to be a collection rather than just a set? Also, would this mean that the elements of a sequence must be sets (containing just one or more elements)? I'm use to thinking about elements of a sequence, specially when studying convergence, as just real numbers. And finally, I will give an example of what I understand so that you could tell me what I'm missing: I will define a family of countries indexed by their currency name. So: $I$ , the index set would be $I = \{pound, dollar,euro\} $ $A$ , the indexed set would be $A = \{\{Spain, Italy, France\},\{UK\},\{US, Canada\}\}$ Then, the family would be a mapping defined as: $(A_i)_{i \in I}= \{pound \rightarrow \{UK\},dollar \rightarrow \{US, Canada\}, euro \rightarrow \{Spain, Italy, France\} \} $ Finally, the product of the family $\prod_{i\in I} A_i$ will consist on a set of mappings (families) where each one of them has $I$ as index set, and maps each index to a set containing just one of the elements of the corresponding subset of $A$ . So in that way, each of these families will map the indexes to one of the six combinations possibles by picking one element from each subset of $A$ . Then the product of the family will be a set of exactly 6 families, somewhat similar to the Cartesian product of the subsets of $A$ .","['elementary-set-theory', 'definition', 'universal-algebra']"
3784607,"Why does $A_s = k[U,T,S]/(UT-S) \otimes_{k[S]} k[S]/(S-s)$ simplify to $ A_s = k[U,T] / (UT-s)$?","When reading algebraic geometry (on the technique of base change) in the book Algebraic Geometry 1 - Schemes by Ulrich Gortz, et.al, I came up with the following tensor product: $$
A_s = k[U,T,S]/(UT-S) \otimes_{k[S]} k[S]/(S-s), \quad \quad (\star)
$$ and the author claimed that $$
A_s = k[U,T] / (UT-s).      \quad \quad (\star\star)
$$ My question is: how to simplify $(\star)$ to $(\star\star)$ ? More on this question: I have read Atiyah and MacDonald's Comm. Algebra and know what a tensor product is. Yet I have not been familiar with the concrete calculation of tensor products (though I know the universal property of tensor products, its relation with localization, its exactness and etc.) So beside the above question, I hope to know that what is going on in your mind when calculating the tensor product ? For example, when calculating the quotient ring $k[x,y]/(y-x^2)$ , we can imagine that $y-x^2 = 0$ and hence $y=x^2$ , then in the ring $k[x,y]$ , we can make $y$ be $x^2$ and the quotient ring is isomorphic to $k[x]$ . For example, when calculating the quotient ring $k[x,y]/(1-xy)$ , we can imagine that $xy=1$ and hence $y=1/x$ , then in the ring $k[x,y]$ , we can make $y$ be $1/x$ and the quotient ring is isomorphic to $k[x, 1/x]$ , or the localization $k[x]_{x}$ . Then, when calculating the tensor product, is there a way like these above in mind to help us calculate these? Thank you all! :)","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'tensor-products', 'commutative-algebra']"
3784641,Proof of interior gradient estimate for Laplace's equation,"I have a question about the proof of the estimate $$
|\nabla u(x_0)| \leq \frac{n}{R} \max_{\bar{B}_R(x_0)} |u|
$$ where $u$ is assumed to be harmonic. Since $u_{x_i}$ is harmonic, by the mean value property and integration by parts, $$
u_{x_i}(x_0) = \frac{r}{\omega_n R^n}\int_{B_R(x_0)} u_{x_i}(y) dy = \frac{n}{\omega_n R^n}\int_{\partial B_R(x_0)} u(y) \nu_i dS_y.
$$ Taking the absolute value, we obtain $$
|u_{x_i}(x_0)| \leq \frac{n}{\omega_n R^n} \int_{\partial B_R(x_0)} |u(y)| dS_y \leq \frac{n}{R}\max_{\bar{B}_R(x_0)} |u|.
$$ I understand the preceding steps. What I don't understand is how this obviously proves the desired result. This is my attempt at obtaining the desired result: \begin{align*}
|\nabla u(x_0)|^2 &= u_{x_1}^2(x_0) + \cdots + u_{x_n}^2(x_0) \\
&\leq \underbrace{\frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2 + \cdots + \frac{n^2}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2}_{\text{$n$ times}}\\
&=\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2.
\end{align*} Taking the square root, $$
|\nabla u(x_0)| \leq \left(\frac{n^3}{R^2}(\max_{\bar{B}_R(x_0)} |u|)^2\right)^{1/2} = \frac{n^{3/2}}{R}\max_{\bar{B}_R(x_0)}.
$$ I'm not sure where my logic is wrong and I am aware this must be something simple...","['multivariable-calculus', 'harmonic-functions', 'partial-differential-equations']"
3784731,Finding the common ratio of the geometric progression,"There is a question in my book stating that A geometric progression consists of an even number of terms. If the sum of all the terms is five times the sum of terms occupying odd places then find the common ratio. I solved it correctly but I want to ask what if we have an odd number of terms and rest of the data are left undisturbed? I tried but couldn't find the solution because in the earlier problem the number of odd terms is exactly half of the total terms but in this new part it is one more  than the number of terms on even place. My attempt : There are for sure one more number at odd place than at even place . So first term is $a$ , the common ratio is $r^2$ and the number of terms is $n+1$ ( if total number of terms in the original G.M. are $2n+1$ ). So I did this but couldn't proceed further \begin{align}
\frac{r^{2n+1}-1}{r-1} &= 5\frac{(r^2)^{n+1}-1}{r^2-1}\\
(r+1)(r^{2n+1}-1)&= 5(r^{2n+2}-1)\\
r^{2n+2} -r + r^{2n+1}-1 &=5r^{2n+2}-5 \\
4r^{2n+2} -r^{2n+1}+r-4 &= 0. 
\end{align}","['geometric-progressions', 'sequences-and-series']"
3784741,Probability inequality for independent random variables with symmetric distributions,"I'm trying to solve the following question, in particular part (b): My solution so far: Denote $\max_{k < n} |S_k| \equiv M_n$ and $A_k \equiv \{|S_k| > x, |S_j| \leq x \quad \forall j < k\}$ .  Notice that the $A_k$ form a disjoint collection whose union is $\{M_n >x\}$ .  Further, on $A_k$ , we have $2x < 2|S_k| = |S_n + X_1 + ... + X_k - X_{k+1} - ... - X_n| \leq |S_n| + |S_n'(k)|$ where $S_n'(k) = X_1 + ... + X_k - X_{k+1} - ... - X_n$ and this implies that at least one of $|S_n|, |S_n'(k)|> x$ For part (a), we have $$P(M_n > x) = \sum_{k = 1}^n P(A_k) \leq \sum_{k=1}^n P(A_k, |S_n| > x) + P(A_k, |S_n'(k)| > x) \\ = 2 \sum_{k=1}^n P(A_k, |S_n| > x) \leq 2P(|S_n| >x)$$ where the second equality follows from the fact that $(X_1, ..., X_n)$ and $(X_1, ..., X_k, -X_{k+1}, ..., -X_{n})$ are equal in distribution. I have no idea where to even begin for part (b).  Any help would be massively appreciated.","['inequality', 'probability-theory']"
