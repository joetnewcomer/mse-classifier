question_id,title,body,tags
1197537,"Prove that $PSL(2,\mathbb{Z})$ is free product of $C_2$ and $C_3$","Prove that $PSL(2,\mathbb{Z})=C_2 \star C_3$. Now $C_2 \star C_3=\langle a,b\ |\ a^2, b^3 \rangle$ i.e. the free product. But how do I show that presentation of $PSL(2,\mathbb{Z})$ is this?","['combinatorial-group-theory', 'group-theory', 'free-product', 'free-groups']"
1197582,Measure theory question involving a $\sigma$ finite measure space,"Q/ Let $f: X \rightarrow \bar{\mathbb{R}}$ be measurable on a $\sigma$-finite measure space $(X,\mathscr{A},\mu)$. Show that the set $\{x\in \mathbb{R} :\mu(f^{-1}(x))>0\}$ is countable. So since we know we can write $X=\cup_1^{\infty} A_i$ with $A_i \in \mathscr{A}$ and $\mu(A_i)<\infty$ for all i. I spent a while looking at it without really getting anywhere, I can't really see the connection between the measurability of f and the sigma finiteness of the measure space, I am assuming that there is more to the measurability of f than just the fact it means that set makes sense. I thought possibly about writing f as the limit of simple measurable functions as they only have a finite number of values they each put out but couldn't really make it go anywhere. It was only a brief thought. I don't really have time to spend ages looking at it no matter how much I want to so a small to medium push in the right direction would be appreciated.","['real-analysis', 'measure-theory']"
1197590,Is the following set (path) connected?,"This is a homework question. $d,n\ge 2$. Let $L=\{(x_1,...,x_n)\in (\mathbb{R}^d)^n: x_i\in \mathbb{R}^d,  x_i\ne x_j \forall i\ne j\}$. I tend to think it is not path connected because if you assume $d=1$, this is the case. But I cannot prove for $d\ge 2$. Any help will be appreciated! :)","['euclidean-geometry', 'connectedness', 'general-topology']"
1197596,What is the intuition behind the definition of the differential of a function?,"What is the intuition behind the definition of a differential of a function in differential geometry? i.e. $$df(p)(v_{p}) =v_{p} (f)(p) $$ where $v_{p} \in T_{p} M$ is a vector in the tangent space to the point $p\ M$. Is it just that the notion of an infinitesimal change in a function is not mathematically rigorous (in the ""traditional"" sense that it's presented in elementary calculus). So in order to make the notion mathematically rigorous we note that a quantity that captures the notion of an infinitesimal change in a function should itself be a function of all the possible infinitesimal changes of the point that the function is evaluated at, i.e. it should be a function of the all possible tangent vectors in the vector space tangent to that point (as such quantities describe the possible directions that a function can pass through a point and all the possible ""speeds"").  In this sense the differential change in the function as it passes through the point in a given direction should be equal to the directional derivative of the function along the particular vector describing that direction?!","['differential-geometry', 'differential-forms', 'intuition', 'vectors']"
1197598,Determining te probability that a message can not be corrected,"A bit error occurs with probability $10^{-7}$  . A message consists of 8000 bits. Upto three bit errors can be corrected at the receiver with FEC (Forward Error Correction) code in the message. Determine the probability that a message can not be corrected at the receiver. I thougt that a message contain three or less error with probability $$\begin{align}
p & = \binom{8000}{3}\left(\frac{1}{10^7}\right)^3\left(1-\frac{1}{10^7}\right)^{7997} + \binom{8000}{2}\left(\frac{1}{10^7}\right)^2\left(1-\frac{1}{10^7}\right)^{7998} + \binom{8000}{1}\left(\frac{1}{10^7}\right)\left(1-\frac{1}{10^7}\right)^{7999} + \left(1-\frac{1}{10^7}\right)^{8000}
\end{align}$$ so probability that a message can not be corrected is 
$$
1-p
$$ Is this answer right?","['solution-verification', 'probability', 'combinatorics']"
1197604,Properties of holomorphic functions (demonstration),"I don't know how to do this demonstration: ""If f is an holomorphic function, and M $\in \mathbb{R}^+$, such that for $z \in \mathbb{C}$, $|f(z)| \leq M(1+ |z|^n)$, then f is a $n$ or less degree polynomial"" Thanks for your attention!","['analyticity', 'complex-analysis']"
1197626,Surjective and Not Injective Problem,"for this data structures homework I am having a hard time figuring out this one problem on surjection and injection. Here is the problem: Show that each function $f\colon \mathbb{N} \rightarrow \mathbb{N}$ has the listed properties. $f(x) = \operatorname{ceiling}(\log_2 (x+1))$ is Surjective and not injective. For problems dealing with injection I know you are supposed to show $f(x) = f(y)$ so that $x = y$ where each element in $x$ points to a specific element in $f(x)$, I think? I am just not sure how to simplify this problem down since it is dealing with ceilings and logs to show that $f(x) \neq f(y)$ since this problem is not injective. As far as showing surjection, I am not clear on how to do that. My professor is not very clear. Any tips on what to do or any explanation on how to calculate surjection? Thanks","['discrete-mathematics', 'functions']"
1197636,Compact universal covering spaces,Let $X$ be a topological compact space admitting a universal covering $C$. When is $C$ again compact? Thanks.,"['differential-topology', 'algebraic-topology', 'general-topology']"
1197640,Elementary proof of topological invariance of dimension using Brouwer's fixed point and invariance of domain theorems?,"http://people.math.sc.edu/howard/Notes/brouwer.pdf https://terrytao.wordpress.com/2011/06/13/brouwers-fixed-point-and-invariance-of-domain-theorems-and-hilberts-fifth-problem/ These papers give fairly elementary proofs of Brouwer's fixed point and invariance of domain theorems. Having established these tools, is it possible to prove that an open subset of $ \mathbb{R}^n $ and an open subset of $ \mathbb{R}^m $ can't be homeomorphic unless $ n = m $ without refering to all those more advanced things which are normally used here, such as homology theory and algebraic topology?",['general-topology']
1197656,R (Stats) - Read anova table and prove Ha,"Don't down-vote me for not including more context--You don't need it (unless you're curious for more info). I have a dataset and have the following output in the anova table: (the stats are based on this study: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC164340/ ) I understand that the F value is the test statistic and Pr(>F) denotes the associated p-value. Using this data, I've been asked to ""report the conclusion at the 0.05 significance level for Ho : no difference..."" Do we reject Ho because of the significantly small p-value? After doing a little research, I've found that statisticians look down on making decisions as to the significance of studies/results solely based on the p-value. Is there some other comparison that can/should be made, or should our decision solely be based on the p-value?","['probability', 'statistics']"
1197670,Winding number (demonstration),"How could I explain mathematically, that the winding number of a closed curve $\gamma$  around $a$ ($a \notin \gamma$) gives always an integer value.
$$
W(\gamma,a)=\frac{1}{2\pi i} \int_{\gamma} \frac{dw}{w-a}
$$ 
where $W(\gamma,a)\in \mathbb{Z}$","['complex-analysis', 'winding-number']"
1197680,Is the derivative formula itself complex differentiable?,"Let $f$ be complex differentiable on an open set $U\subset\mathbb{C}$ (and therefore holomorphic on said set, according to my teacher, since in my class we consider this implies the derivative is continuous), show that : $g(z)=
\left\{
\begin{array}{cl}
\displaystyle{\frac{f(z)-f(z_0)}{z-z_0}} & z\neq z_0\\
f'(z_0) & z=z_0\\
\end{array}
\right.$ is complex diffentiable on $U$. I have already managed to show that the derivative exists if $z\neq z_0$: $g'(z)=\displaystyle{\lim_{h\rightarrow0}\frac{g(z+h)-g(z)}{h}=\frac{f'(z)-g(z)}{z-z_0}}$ But I get stuck when trying to find the derivative for $z=z_0$ : $\begin{array}{rcl}
g'(z_0) & = & \displaystyle{\lim_{z\rightarrow z_0}\frac{g(z)-g(z_0)}{z-z_0}}\\
& = & \displaystyle{\lim_{z\rightarrow z_0}\frac{\frac{f(z)-f(z_0)}{z-z_0}-f'(z_0)}{z-z_0}}\\
& = & \displaystyle{\lim_{z\rightarrow z_0}\frac{f(z)-f(z_0)}{(z-z_0)^2}-\frac{f'(z_0)}{z-z_0}}\\
\end{array}$ I felt like I could conclude after the second line because $\displaystyle{\lim_{z\rightarrow z_0}\frac{f(z)-f(z_0)}{(z-z_0)}}=f'(z_0)$ and then I'd have 0, but one of my classmates told me that I couldn't do that, because of continuity issues.
How do I calculate the derivative for $z=z_0$ then ? Should I prove that $g'(z)$ is continuous ?",['complex-analysis']
1197682,Discrete and Essential spectrum of Laplacian in $\mathbb R_{+}$ (with weird boundary conditions),"I am given on Hilbert Space $\mathcal H=L^2(\mathbb R_{+})$ $$
Af(x)=-f''(x)
$$
 and Domain of A is 
$$
D(A)=\{f\in H_2(\mathbb R_{+})\;\;| \;\;f'(0)+\alpha f(0)=0\}
$$ for some $\alpha \in \mathbb R$ I need to find discrete and essential spectrum of this unbounded operator. I only know one way of finding the spectrum of Laplacian, that is by Fourier transform (and its variant for half line). I do not know how to handle this situation.","['analysis', 'spectral-theory', 'operator-theory']"
1197708,Jacobi Triple product cases,Hello I want some guidance how can I prove these identities of Jacobi Tripple Product $$\sum_{n=-\infty}^\infty q^{2n^2+n}=\prod_{n=1}^\infty\frac{(1-q^{2n})^2}{(1-q^n)}$$ $$\sum_{n=-\infty}^\infty q^{n^2}=\prod_{n=1}^\infty\frac{(1-(-q)^n)}{(1+(-q)^n)}$$ $$\sum_{n=-\infty}^\infty q^{n(n+1)}=\prod_{n=1}^\infty\frac{(1-q^{4n})}{(1-q^{4n-2})}$$,"['number-theory', 'combinatorics']"
1197732,Prove that $HK$ is a subgroup iff $HK=KH$. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $H$ and $K$ be subgroups of a group $G$, and let $HK=\{hk: h
\in H, k \in K\}$, $KH=\{kh: k \in K, h \in H\}$. How can we prove that $HK$ is a subgroup iff $HK=KH$?","['abstract-algebra', 'group-theory']"
1197737,Composition of real-analytic functions is real-analytic,"Suppose $f,g: \mathbb{R} \to \mathbb{R}$ are real analytic, i.e, locally given by convergent power series. Then $g \circ f$ is real-analytic as well. How do I prove this? I guess the ""standard"" proof would be to extend $f$ and $g$ into some open subsets of $\mathbb{C}$ in the natural way via their power series, then notice that $g \circ f$ is complex differentiable, hence complex-analytic, and hence real-analytic when restricted to $\mathbb{R}$. But is there another proof of this, one that doesn't use complex-analytic extensions? I want a proof which can be extended to the multivariate case, i.e, if $f: \mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$ are both analytic (i.e, their components are locally given by multivariate power series), then $g \circ f$ is also real-analytic. Is the standard proof for the multivariate case via complex-analytic extensions as well? Does anyone know a good book for this subject?","['power-series', 'analyticity', 'real-analysis']"
1197752,"Is $T=\{U\cup (V \cap \mathbb{Q}), U,V$ open in $\mathbb{R} \}$ a topology on $\mathbb{R}$?","Let $T$ be the set of all subsets of $\mathbb{R}$ of the form $U\cup (V \cap \mathbb{Q})$ where $U$ and $V$ are open in the usual topology on $\mathbb{R}$. Is $T$ a topology? Is it Hausdorff? I attempted going through the 3 conditions that a topology must satisfy, but I think I may have run into some issues. Any union of elements of T is an element of T Can I take two arbitrary elements of $T$ (i.e. they are of the form $U\cup (V \cap \mathbb{Q})$) and show that their union is of the same form? Then because set union is associative the union of many elements in $T$ will still be of that form. I know this would work for finite union, but is it still logically valid in this case? Any intersection of finitely many elements of T is an element of T I feel like the above reasoning would work in this case, but I'm having trouble with the set algebra. I'm not sure where to go with something that looks like $(U_i \cup (V_i \cap \mathbb{Q}))\cap (U_j \cup (V_j \cap \mathbb{Q}))$. And for Hausdorffness... $T$ is Hausdorff if distinct points in $T$ have distinct neighbourhoods I've been stumbling here with the distinction between points in $T$ and sets in $T$. Do we choose our arbitrary points from $\mathbb{R}$? Or is there more restriction on the initial points we choose?",['general-topology']
1197797,Showing when a permutation matrix is diagonizable over $\mathbb R$ and over $\mathbb C$,"For a permutation $\sigma$ of the set $\{1,...,n\}$, and consider the $n \times n$ matrix $A_\sigma$, where the $i^{\text{th}}$ column is the standard vector $e_{\sigma (i)}$.  For which $\sigma$ is $A_\sigma$ digonizable over $\mathbb{C}$, and which values for $\mathbb{R}$? I don't exactly know what I should do.  I think I should use the fact that a map is diagnizable iff the characteristic polynomial of t factors in linear factors over the field I'm in, and if $m_g(\lambda)=m_a(\lambda)$ for all eigenvalues If anyone could start me up or help me out or do one of the examples it would be appreciated, thanks! Small edit: I don't even know what the matrix looks like, maybe it's just me but I don't find it's clear","['linear-algebra', 'diagonalization', 'matrices']"
1197829,"Finding axis of ellipse described by $x=a\cos t+ h\sin t$,$ y=b\sin t + g\cos t$","Hi I am in need of help here for my project. Basically I have managed to obtain this form of equation. Example: $a=-181,h=33,b=185.9$ and $g=18.3$. When I plot it on a graphing program, it looks like a ellipse. Basically I need to find all the different parameters of the ellipse. Initially I thought that a general rotation would solve the problem. Basically $x=A\cos t$ and $y=B\sin t$ would be a normal parametric equation of ellipse, and alpha is the angle I have rotated. Hence that would allow me to find the major and minor axis easily. However, as you can see solving the 2 equation actually yields 2 equations for $\alpha$ which are $\tan\alpha ={ -h\over b}$ and $\tan\alpha ={ g\over a}$.
Obviously this cannot be true. Does anyone know of what to do? Is there a more general equation of ellipse that I am unaware of? Please advise, thank you very much.","['geometry', 'linear-algebra', 'analytic-geometry', 'conic-sections']"
1197832,critical numbers of a complex function $ f(x) = 2 x - 5 x^{\frac{2}{5}} $,"Critical numbers of $ f(x) = 2 x - 5 x^{2/5} $ are 1,$0$ How come $0$ is also a critical number? I found the critical number 1 this way: $f'(x) = 2 - 5 \ \frac {2}{5}  \ x^{-\frac{3}{5}}$ $ x^{\frac{3}{5}} = 1$","['calculus', 'functions']"
1197865,"Definite integral over $(0,1)$ rather than $[0,1]$","I'm wanting to calculate a simple definite integral (but haven't done so since high-school), and am seeking some clarification. $$f(x)=\begin{cases}x^{2}(x-1) & 0<x<1\\ 0 & \text{otherwise}\end{cases}$$ I am interested in the area under the curve that is between $0$ and $1$. However, I can't just calculate $$\int_{0}^{1}x^{2}(x-1)\;dx$$ as the function isn't $x^{2}(x-1)$ for $0\leq x\leq1$, correct? Is there a method that I've completely forgotten about that will allow me to get what I'm after? Thanks for your patience in answering what I'm sure is a very basic question.","['definite-integrals', 'integration']"
1197875,Partial derivative with respect to $y$ of $(y/x)$?,"I'm just starting partials and don't understand this at all. I'm told to hold $y$ ""constant"", so I treat $y$ like just some number and take the derivative of $\frac{1}{x}$, which I hope I'm correct in saying is $-\frac{1}{x^2}$, then multiply by $y$, getting $-\frac{y}{x^2}$. But apparently the correct answer is $\frac{1}{x}$. What am I missing?","['partial-derivative', 'multivariable-calculus']"
1197888,Invariant Subspace of Two Operators [duplicate],"This question already has answers here : Invariant Subspace of Two Linear Involutions (4 answers) Closed 9 years ago . Let $S$ , $T$ be linear operators on a finite-dimensional vector space $V$ over $\mathbb{C}$ . Suppose $$S^2 = T^2 = I.$$ Show that there exists either a $1$ -dimensional or $2$ -dimensional subspace of $V$ which is  invariant under $S$ and $T$ . Ok so, since $S^2 = T^2 = I$ ,  either $1$ or $-1$ are Eigenvalues of S and T. i,e  The Minimal Polynomial $M_T$ and $M_S$ satisfy $$M_T \; | \;  (x+1)(x-1)$$ $$M_S \; | \;  (x+1)(x-1)$$ Edit : Found the same question elsewhere. For those of you who are interested in the answers. Invariant Subspace of Two Linear Involutions","['vector-spaces', 'linear-algebra']"
1197898,"Hatcher's Algebraic Topology, Example 1.35","Hatcher considers the mapping cylinder A from $S^{1}$ to $S^{1}$ under the function $z \rightarrow z^m$. He claims without explanation that the universal cover of A is homeomorphic to a product $C_m \times \mathbb{R}$ where $C_m$ is the graph that is a cone on $m$ points. I don't understand where that came from. Here is a link to Hatcher's book, chapter 1. The example can be found on page 65. http://www.math.cornell.edu/~hatcher/AT/ATch1.pdf","['algebraic-topology', 'general-topology']"
1197901,What is the derivative of the square root of a function f(x)?,"I'm trying to figure out if the $\frac{d}{dx} \sqrt{f(x)} = \frac{f'(x)}{2\sqrt{f(x)}}$
If possible can you give me the proof for the function?","['calculus', 'derivatives']"
1197927,Is the sum of an algebraic and transcendental complex number transcendental?,"I was wondering if the sum of an algebraic and transcendental complex number is transcendental. I was thinking if a is algebraic, and b is transcendental, then if a+b is algebraic, then a+b-a is also algebraic since algebraic numbers are closed under additive inverses, but b is transcendental. Is this a correct approach?","['transcendental-numbers', 'abstract-algebra', 'field-theory', 'galois-theory']"
1197934,How many matrices can commute with a given matrix?,"I'm trying to learn linear and abstract algebra on my own and have been attempting textbook exercises and problem sets I find online. I've been doing okay so far but I found this problem and I'm having a lot of trouble with it: Let $A$ be an $n \times n$ complex matrix. a) Show that the set of matrices commuting with $A$ is a subspace. b) What is the dimension of this subspace? I think I got the first part. It wasn't that bad.  But I'm having trouble with the second part. I feel like this is supposed to be an easy question, but I just don't know how to start it. I was thinking about using Jordan form somehow. If $A$ ~ $J_A$ and $B$ ~ $J_B$ , is it true that if $J_A J_B = J_B J_A$ then $AB = BA$ ? If it is, then we'd only have to look at the Jordan blocks of these and see when those commute with each other. Then the problem wouldn't be so bad, I think. I'd love some hints.","['linear-algebra', 'matrices']"
1197941,What does $[L]=[I]^{-1}[II]$ mean?,"I have a question about one of the equations in my notes. Matrix representations of Weingarton map, first fundamental form and second fundamental form satisfies $[L]=[I]^{-1}[II]$ According to my understanding, $[II]$ is really the matrix representation of a bilinear map from $T_pM\times T_pM \to R$. And so is $[I]$, so $[I]^{-1}$ represents a map from $R \to T_pM \times T_pM$? But then $L$ is a map that takes in one vector, and outputs anothe vector. So the left hand side and the right hand side doesn't seem to be the same thing at all. Not sure if I'm confusing myself. Any idea?","['bilinear-form', 'differential-geometry', 'linear-algebra']"
1197949,Difference between $F[x]$ and $F(x)$,"Notation wise, what is the difference between $F[x]$ and $F(x)$ ? Is $F[x]$ the ring of polynomials with coefficients in $F$ , and $F(x)$ the field of rational functions with coefficients in $F$ ? I am asking because I am trying to determine if this statement is true or false: An element of the field $F(x)$ of rational functions is transcendental
over $F$ if and only if it is not in $F$ and I'm not sure what an element of $F(x)$ not being in $F$ means. Thanks for your help.","['abstract-algebra', 'field-theory']"
1197952,"Is there a set of $4$ vectors in $\mathbb{R}^3$, any $3$ of which form a linearly independent set?","I have an exercise in my last assignment for linear algebra: Is there a set of $4$ vectors in $\mathbb{R}^3$, any $3$ of which form
  a linearly independent set? Prove. My answer intuitively is no for one reason. If we have 4 vectors in $\mathbb{R}^3$, then, if we consider then all together, one of them is a linear combination of another (or others), it's a multiple of another. From this set of $4$ vectors, we can pick $3$ vectors to check if they are linear independent, but we are going to have at least one group of $3$ vectors where we have a vector and its multiple. Does my reasoning make some sense? How could I prove what they are asking?","['linear-algebra', 'proof-writing', 'matrices']"
1197959,Subtlety in the Definition of Limit Point,"In my time studying mathematics I have always found some subtle confusion with the definition of a limit point.  I know it's possible for different definitions to yield the same results, and the issues with this might be syntactical rather than anything else, but this subtlety is really bothering me. Given a topological space $(X, \mathcal{T})$ and a subset $A \subseteq X$, I've seen definitions that a limit point is any element $p \in X$ such that every neighborhood of $p$ intersects $A$ at some point other than $p$.  But I've also seen the definition that $p$ is a limit point provided that if $U$ is a neighborhood of $p$ then $U \cap A \neq \emptyset$, where the latter can admit the possibility that $A \cap U = \{p\}$. To illustrate a case where these competing definitions might get confusing, consider $\mathbb{R}$ with the particular point topology centered at $0$ (that is, any $A \in \mathcal{P}(\mathbb{R})$ containing $0$ is open).  The interval $(1,2)$ is closed in this topology since its complement $(-\infty,1)\cup (2,\infty)$ is open.  Notice however that for any $x \in \mathbb{R}$ the set $\{0,x\}$ is open in this topology.  We expect that since $(1,2)$ is closed that it should contain all of its limit points.  If we go with the second definition of limit point over the first, we find that if $x$ is a limit point of $(1,2)$ then we should have $\{0,x\} \cap (1,2) \neq \emptyset$, but this only occurs when $x \in (1,2)$.  This shows however that $x$ has a neighborhood intersecting $(1,2)$ that doesn't contain any other point of $(1,2)$, so $x$ can't possibly be a limit point of this interval if we adhere to the first definition.  Furthermore $(1,2)$ has no limit points according to the first definition. I do want to acknowledge that the first definition is there to make sure that isolated points aren't mistaken for limit points.  For example, in the Euclidean topology on $\mathbb{R}$ we would never look at the set $\{0\}\cup(1,2)$ and say $0$ is a limit point of this set.  Is there something I'm missing here?  Is there truly some ambiguity with these definitions or am I just not seeing it?  Looking forward to the discussion.","['definition', 'general-topology']"
1197963,Number of finite extensions of $p$-adic number field of given degree $n$,"Let $p$ be a prime number, $\mathbb{Q}_p$ the $p$-adic number field.
We fix an algebraic closure $\Omega$ of $\mathbb{Q}_p$.
Any algebraic extension of $\mathbb{Q}_p$ is assumed to be a subfield of $\Omega$.
Let $n$ be a positive rational integer. Is the number of finite extensions of $\mathbb{Q}_p$ of degree $n$ finite?
  If yes, is there an algorithm to construct all of them? The motivation is as follows.
Let $p$ be an odd prime number.
I came up with the following result using Hensel's lemma. The number of quadratic extensions of $\mathbb{Q}_p$ is $3$.
  They are $\mathbb{Q}_p(\sqrt a)$, $\mathbb{Q}_p(\sqrt{ap})$, $\mathbb{Q}_p(\sqrt p)$,
  where $a$ is a quadratic non-residue rational integer mod $p$. $\mathbb{Q}_p(\sqrt a)$ (resp. $\mathbb{Q}_p(\sqrt{ap})$) does not depend on the choice of $a$.","['abstract-algebra', 'algebraic-number-theory']"
1197966,Deriving the Poisson Integral Formula from the Cauchy Integral Formula,"If $f$ is analytic inside and on the unit circle $\gamma$, show that for $0<|z|<1$,
$$2\pi if(z)=\int_\gamma \frac{f(w)}{w-z}dw-\int_\gamma \frac{f(w)}{w-1/\bar{z}}dw$$
and then derive the Poisson Integral Formula:
$$f(re^{i\theta})=\frac{1}{2\pi}\int_0^{2\pi}\frac{1-r^2}{1-2rcos(\theta-t)+r^2}f(e^{it})dt, \qquad 0<r<1.$$ I know the Cauchy Integral Formula is $f(a)=\frac{1}{2\pi i}\int_\gamma \frac{f(w)}{w-a}dw$ for $a$ inside $\gamma$, and I also know its counterpart for derivatives, which is $f^{(n)}(a)=\frac{n!}{2\pi i}\int_\gamma \frac{f(w)}{(w-a)^{n+1}}dw$ for $a$ inside $\gamma$. The former looks promising in terms of deriving the first equation, if only I can find $a$ such that $\frac{f(w)}{w-a}=\frac{f(w)}{w-z}-\frac{f(w)}{w-1/\bar{z}}$. I think a key point is working out where $1/\bar{z}$ lies in relation to $z$ but I can't seem to visualise it, thanks to the function at $0$ being a singularity. Is the idea that the first fraction has a zero at $w=z$ and the second at $w=\frac{1}{\bar{z}}$? Putting the two fractions over a common denominator gives:
$$\frac{f(w)(w-1/\bar{z})-f(w)(w-z)}{(w-z)(w-1/\bar{z})}=\frac{zf(w)-(f(w)/\bar{z})}{(w-z)(w-1/\bar{z})}=\frac{(z-1/\bar{z})}{w^2-w(z+1/\bar{z})+z/\bar{z}}f(w)$$
which does look a little like the final equation I am trying to find if I let $z=re^{i\theta}$. What am I missing? EDIT: Okay, letting the second fraction in the first equation become $\frac{\bar{z}f(w)}{w\bar{z}-1}$, I can get the second, however it is negative. I.e. I get
$$f(re^{i\theta})=\frac{1}{2 \pi}\int_0^{2 \pi}\frac{r^2-1}{2rcos(\theta-t)+r^2+1}f(e^{it})dt$$
is this okay? I am still struggling with getting the first equation, however. Is it the case that whenever $z$ is in $\gamma$, then $1/\bar{z}$ wont be and vice versa? In which case, why do you subtract them from each other?","['complex-analysis', 'complex-integration']"
1197975,A question on Derangement Combinatorics,"Six cards and six envelopes are numbered $1$ , $2$ , $3$ , $4$ , $5$ , $6$ and cards are to be placed in envelopes so that each envelope contains exactly one card and no card is placed in the envelope bearing the same number and moreover the card numbered $1$ is always placed in envelope numbered $2$ . Then, what is the number of ways this can be done? Since card one is already fixed, I tried directly applying the Derangement formula for $5$ things. But it didn't work. I guess it's not so simple since the envelope no. $2$ is already occupied. I just need a hint on how to proceed.","['arithmetic-combinatorics', 'derangements', 'combinatorics', 'algebraic-combinatorics']"
1197985,Why is $\ker\omega$ integrable iff $\omega\wedge d\omega=0$?,"Suppose $\omega$ is a nonvanishing $1$-form on a $3$-manifold $M$. It's known that $\ker\omega$ is an integral distribution iff $\omega\wedge d\omega=0$. I'm trying to understand this, but I don't get why $\ker\omega$ integrable implies $\omega\wedge d\omega=0$. I worked out so far: suppose $\omega\wedge d\omega=0$. Let $X,Y$ be vector fields in $\ker\omega$. Then
$$
d\omega(X,Y)=X(\omega(Y))-Y(\omega(X))-\omega([X,Y])
$$
so $\omega([X,Y])=-d\omega(X,Y)$. Since $\omega$ is nonvanishing, we can find $Z$ such that $\omega(Z)\neq 0$. Then
$$
0=(\omega\wedge d\omega)(X,Y,Z)=c\omega(Z)d\omega(X,Y)
$$
for some $c\neq 0$. Thus $d\omega(X,Y)=0$, so $\omega([X,Y])=0$, so $[X,Y]\in\ker\omega$. Then $\ker\omega$ is involutive, so by Frobenius, it is integrable. On the other hand, if $\ker\omega$ is integrable, then the annihilator ideal $I(\ker\omega)$ is closed under $d$. Clearly $\omega\in I(\ker\omega)$, so by assumption $d\omega\in I(\ker\omega)$. I'm trying to show $\omega\wedge d\omega(X,Y,Z)=0$ for any three vector fields, but it looks like a dead end. Is it possible to assume $X,Y\in\ker\omega$ without loss of generality some how? Because that would make the calculation work out.",['differential-geometry']
1197989,Absolute convergence of $\sin(n)/(n^2)$,"Prove that $$\sum_{n=1}^{\infty}  \frac{\sin(n)}{{n}^{2}}$$ is either absolutely convergent, conditionally convergent or divergent. Note that $$\sin(n) \in [-1,1] \text { for} 
\left| \sum_{n=1}^{\infty} \frac{\sin(n)}{{n}^{2}} \right|$$ is bound between $ 0 \text { and }1$.
So we have
$$0\leq \frac{\sin(n)}{{n}^{2}} \leq \frac{1}{{n}^{2}}$$
$$\lim_{n\to\infty} \frac{1}{{n}^{2}}=0 $$
and since
$$\frac{\sin(n)}{{n^{2}}}$$
is bounded between 0 and 0, it converges. 
I haven't proved the non-absolute of the series but I'd like to know if I'm in the right direction. Edit:
Dam what is going wrong with LaTeX. I don't know how I got into this mess.","['absolute-convergence', 'sequences-and-series', 'divergent-series']"
1198044,Show that there are only finitely many subgroups of $F$ in which $H$ can be of finite index.,"Result - Let $H$ be a finitely generated subgroup of the free group $F$. Show that there are only finitely many subgroups of $F$ in which $H$ can be of finite index. I encountered a similar result in Group theory Book by Bogopolski (pg 120), B- ""The number of subgroups of a finite index n in a finitely generated group is finite."" Does the Bogopolski result ( B ) also implies Result , the way Bogopolski proved B does not help in proving Result . So what should be the approach to prove Result .","['abstract-algebra', 'group-theory']"
1198052,Expectation of the inverse of sum of Bernoullis,"Say we have \begin{equation}
 B = \sum\limits_{i=1}^n b_{i}
\end{equation} where $b_i$ is a bernoulli random variable with probability $p$ of being $1$ for all $i$. How to calculate \begin{equation}
\mathbb{E}\{  (1 + B)^{-1} \}
\end{equation} Thanks in advance.",['probability-theory']
1198087,prove increasing/decreasing sequence,"Are these two statements true? If so, how does one prove them? 1) For each integer k (positive or negative), the sequence $a_n = (1 + k/n) ^ n$ (1) is increasing (at least after a certain number n). 2) For each integer k (positive or negative) the sequence $a_n = (1 + k/n) ^{n+1} $ (2) is decreasing (at least after a certain number n). Note that I have no problem proving that the two sequences are convergent and to find their limits but I have really hard time proving formally that these are monotonic (after certain n). In fact for statement (1) seems one can prove it this way. Note that: $(1 + \frac{k+1}{n} )^n = \frac{(1 + \frac{k}{n+1})^{n+1}(1 + \frac{1}{n})^{n}}{1+\frac{k}{n+1}}$  (3) Now apply induction on k. Based on (3), the (k+1)-sequence ${a_n}$ must be increasing because on the right hand side we have: the increasing k-sequence ${a_{n+1}}$ multiplied by the increasing 1-sequence ${a_n}$, and then divided by a decreasing sequence. So the left hand side must be increasing too. Is this proof correct? Actually I think it only works for k>0 otherwise the sequence in the divisor (1 + k/(n+1)) is not decreasing but is increasing. Also, what is the proof for (2)? Should be something analogous, I guess, like the above proof for (1).","['sequences-and-series', 'calculus', 'limits']"
1198109,Question about definition of degree of a morphism,"Studing the theory of Riemann surfaces i've found the definition of the degree of a holomorphic map between compact Riemann Surfaces. If i take two complex algebraic projective surfaces and a morphism between them, is there a way to extend the definition of degree between Riemann Surfaces to degree between projective surfaces?",['algebraic-geometry']
1198114,"Good article (or book) about coordinate-dependent linear algebra, for those already familiar with coordinate-free aspects.","I have a decent understanding of coordinate- free linear algebra. For example: (not-necessarily-finite-dimensional) vector spaces, linear transforms, (possibly infinite) products of vector spaces, (possibly infinite) coproducts, biproducts, free vector spaces, the concepts of ""basis"" and dimension, subspaces, quotient spaces, multilinear maps, tensor products of vector spaces, the tensor-hom adjunction, canonical self-enrichment of $\mathbf{Vect}_K$, and dual spaces. At the same time, there's a lot of holes in my knowledge: Matrix normal forms are a subject I know almost nothing about If you say ""positive definite matrix"" I will stare at you blankly If you say ""special linear group"" I will stare at you blankly Matrix similarity / congruence / equivalence, um what? I feel like I have no understanding of bilinear mappings into $\mathbb{R}$, despite that they're ""the same"" as matrices. (Given a matrix $A,$ we get a bilinear mapping into $\mathbb{R}$ given by $y,x \mapsto y^T Ax.$ This process is an isomorphism of vector spaces.) These are mainly things that can be looked up on wikipedia, of course (except for the last dot point), but its tough to get the ""big picture"" and/or the geometric meaning without the help of a good article or book. Question. Can anyone recommend an article or book that specifically deals with coordinate- dependent linear algebra, such as matrices, in a sophisticated, mathematically-mature way, and which preferably takes abstract linear algebra for granted, and even uses it to help to express and clarify the coordinate dependent stuff?","['reference-request', 'linear-algebra', 'vector-spaces', 'matrices']"
1198188,Why is quadratic integer ring defined in that way?,"Quadratic integer ring $\mathcal{O}$ is defined by \begin{equation}
   \mathcal{O}=\begin{cases}
      \mathbb{Z}[\sqrt{D}] & \text{if}\ D\equiv2,3\ \pmod 4\\
      \mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right]\  & \text{if}\ D\equiv1\pmod 4
   \end{cases}
\end{equation} where $D$ is square-free. I understand $\mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right]$ is not closed under multiplication if $D\equiv2,3\pmod 4$ . But still, isn't it more natural to define $\mathcal{O}=\mathbb{Z}[\sqrt{D}]$ for all square-free $D$ ? (In that case, it really seems like 'quadratic integer'.) I wonder what is the motivation of this definition.","['abstract-algebra', 'quadratic-integer-rings', 'number-theory', 'algebraic-number-theory']"
1198200,What does a well-posed problem imply?,"A well-posed problem in the sense of Hadamard states that: A solution exists The solution is unique The solution's behavior changes continuously with the initial conditions. Now in order to prove this kind of results there are several techniques that are used separately or in combination with each other. For example, using energy estimates, using $C^{0}$ semigroups, proving the existence and uniqueness of a Green function,etc. My question is: How much can be said if we start with a well-posed problem: Can we guarantee that a Green function exist and is unique? Is there always an operator $A$ that generates a $C_{0}$ semigroup? Is there always an energy inequality giving the correct control so if one started with those then one will be able to prove at least part of the well-possessedness? I am more familiar with hyperbolic PDE but I am interested in this kind of converses in elliptic and parabolic problems also if there are any result available. For the sake of concreteness I am most interested in the problem:
$\Box_{g}\phi=0$ 
where $\Box_{g}$ is the wave operator in a Lorentzian manifold.","['functional-analysis', 'partial-differential-equations']"
1198224,Show $a - b \mid f(a) - f(b)$ for $f(x)$ an integer coefficient polynomial [Polynomial Factor Theorem],"I have seen this lemma elsewhere. Suppose $A$ is a domain, and $f \in A[X]$. Prove that $$a - b \mid f(a) - f(b)$$ I need to prove this. $$f(a) - f(b) \equiv 0 \pmod{a-b}$$ basically. Let, $a - b = c$ $$f(a) - f(b)/(a-b) = f'(\xi)$$ for Some $\xi \in (a, b)$. But I dont see it showing divisibility","['number-theory', 'calculus', 'modular-arithmetic', 'elementary-number-theory']"
1198233,"Gaussian, measurability","I have a quesition about an isonormal Gaussian process and measurability. Let $\mathcal{H}$ be a real separable Hilbert space with inner product $\langle \cdot,\cdot \rangle$ and norm $\|\cdot\|=\langle\cdot,\cdot\rangle^{1/2}$. Let $X=\{X_{h}:h \in \mathcal{H}\}$ be an isonormal Gaussian process over $\mathcal{H}$ i.e. $X$ is a collection of jointly Gaussian random variables defined on a probability space $(\Omega,\mathcal{F},P)$ and such that $E \left[X_{h}X_{g} \right]=\langle h,g\rangle$ for all $h,g \in \mathcal{H}$, $E\left[X_{h} \right]=0$ for all $h \in \mathcal{H}$. We shall assume $\mathcal{F}=\sigma[X]$. My question Let $(h_{j})_{j =1}^{\infty}$ be an orthonormal basis of $\mathcal{H}$ and $\mathcal{F}_{m}\,(m\in\mathbb{N})$ be the $\sigma$-algebra generated by $X_{h_{1}},\ldots,X_{h_{m}}$. In this case can we deduce $\mathcal{F}=\sigma \left[ \bigcup_{m=1}^{\infty} \mathcal{F}_{m} \right]$? My attempt: I think it is enough to show that $\forall h \in \mathcal{H}$, $X_{h}$ is $\sigma \left[ \bigcup_{m=1}^{\infty} \mathcal{F}_{m} \right]$-measurable. Since $h=\lim_{n \to \infty} \hat{h}_{n}$ in $\mathcal{H}\quad(\hat{h}_{n}=\sum_{j=1}^{n}\langle h,h_{j} \rangle h_{j})$, \begin{align*}
E \left[\left|X_{h}-X_{\hat{h}_{n}} \right|^{2} \right]=\|h-\hat{h}_{n}\|^{2}\to0\quad(n \to \infty)
\end{align*} Therefore $X_{h}=\lim_{k \to \infty}X_{\hat{h}_{n_{k}}}$ $P$-a.s. but not in pointwise. If we could prove $X_{h}(\omega)=\lim_{k \to \infty}X_{\hat{h}_{n_{k}}}(\omega)$ for all $\omega \in \Omega$, $X_{h}$ is $\sigma \left[ \bigcup_{m=1}^{\infty} \mathcal{F}_{m} \right]$-measurable. (since $X_{\hat{h}_{n}}$ is $\sigma \left[ \bigcup_{m=1}^{\infty} \mathcal{F}_{m} \right]$-measurable.) Thank you in advance.","['probability-theory', 'functional-analysis', 'measure-theory']"
1198238,Which sets occur as boundaries of other sets in topological spaces?,"Which sets occur as boundaries of other sets in topological spaces? Of course the boundary of a set is closed. But is every closed set in a topological space, the boundary of some set in that space? It is tempting to assert that boundaries have empty interiors, but this is not true, as is shown by the fact that the boundary of $\mathbb{Q}$ in $\mathbb{R}$ is $\mathbb{R}$. In fact it can be seen in general that the boundary of a dense set with empty interior is the whole space. Thus the only boundary in an indiscrete space is the whole set. (This is sort of complementary to the comment of Daniel Fischer below.) However the intuitive feeling comes right for open sets (and then for closed sets as well): The boundary of an open set cannot contain an open set. This question concerns all subsets of a topological space. An alternative question shall be to characterise all topological spaces in which every closed set occurs as a boundary. Note: I have now asked this on MathOverflow.",['general-topology']
1198242,Gluing construction of the projective space scheme.,"When constructing the projective space scheme $\mathbb{P}_R^n$ for a ring $R$, we may take the subrings
$$
A_i = R\left[\tfrac{X_0}{X_i}, \ldots, \widehat{\tfrac{X_i}{X_i}}, \ldots, \tfrac{X_n}{X_i}\right],
\; i = 0, \ldots, n,
$$
of the ring $A^{(0)}$of homogenous elements of $A = R[X_0, \ldots, X_n, X_0^{-1},\ldots, X_n^{-1}]$ and glue the affine schemes $U_i = \text{Spec}(A_i)$ together along the open subschemes $U_{ij} \subseteq U_i$ with
$$
U_{ij} = D_{U_i}\bigl(\tfrac{X_j}{X_i}\bigr) \cong \text{Spec}\left(A_i\left[\left(\tfrac{X_j}{X_i}\right)^{-1}\right]\right)
$$ I read that we can use the identity to glue, but why (and how) is
$$
A_i\left[\left(\tfrac{X_j}{X_i}\right)^{-1}\right]
\cong
A_j\left[\left(\tfrac{X_i}{X_j}\right)^{-1}\right]
$$
for $i \neq j$?","['projective-geometry', 'algebraic-geometry', 'projective-space', 'commutative-algebra']"
1198261,Expected value of number of sorted elements in a permutation,"Consider the obvious algorithm for checking whether a list of integers is sorted: start at the beginning of the
list, and scan along until we first find a successive pair of elements that is out of order. In that case, return
false. If no such pair is found by the time we reach the end of the list, return true.
Our elementary operation is a comparison between two integers. I'm trying to find the average case running time of this algorithm. For a permutation with the first $k$ elements sorted and the first $k+1$ unsorted the algorithm must make $k$ comparisons. For each combination of $k+1$ elements out of $n$, there is one in which these $k+1$ elements are sorted. The number of possible permutations of $n$ elements without repetition is $n!$. Therefore the probability that at least the first $k+1$ elements are sorted is: $$\binom{n}{k+1}\cdot\frac{1}{n!}$$ If $K$ is a discrete random variable over the natural numbers 1 to $n-1$ (our event space, since the best case was $k=1$ and the worst case was $k=n-1$), this probability is $\mathbb{P}(K\ge k+1)=\mathbb{P}(K>k) $. We have that: $$\mathbb{E}[K]=\sum\limits_{k=1}^{n-1}\mathbb{P}(K>k)$$ We also have that:
$$\sum\limits_{k=0}^{n}\binom{n}{k}=2^n$$
$\binom{n}{0}=1$ for all $n$, so $\sum\limits_{k=1}^{n}\binom{n}{k}=2^n-1$.  Therefore, if $K$ is the distribution of running times of our algorithm:
$$\mathbb{E}[K]=\sum_{k=1}^{n-1}\Bigg[\binom{n}{k+1}\cdot\frac{1}{n!}\Bigg]=\frac{1}{n!}\sum_{k=1}^{n-1}\binom{n}{k+1}=\frac{1}{n!}\sum_{k=2}^{n}\binom{n}{k}=\frac{1}{n!}\Bigg(\sum_{k=1}^{n}\binom{n}{k}-\binom{n}{1}\Bigg)=
$$
$$
=\frac{1}{n!}\Bigg(2^n-1-\frac{n!}{1!(n-1)!}\Bigg)=\frac{2^n-n-1}{n!}
$$ However, this gives nonsensical values. It should be that for all $n<0$ and $n\in\mathbb{N}, \mathbb{E}[K]\ge1$, since the algorithm must check at least the first pair of elements. However, this function for the expected value does not satisfy this. I can't see where I've gone wrong, but I must have. What have I done incorrectly?","['probability-distributions', 'discrete-mathematics', 'permutations', 'random-variables', 'algorithms']"
1198271,Putnam 2009 B1 (rational number as factorial),"Show that every positive rational number can be written as a quotient of products of factorials of (not necessarily distinct) primes. For example, $ \frac{10}9=\frac{2!\cdot 5!}{3!\cdot 3!\cdot 3!}.$ I used the idea: $$\frac{a}{b} = \frac{\prod x_{k}!}{\prod y_{j}!} \implies a = \frac{(b)\cdot\prod x_{k}!}{\prod y_{j}!}$$ Then I said suppose it holds for: $$\varphi = \{1, 2, 3.... a-1\}$$ Then since for $a > 1$: $$\frac{a}{2} < a-1$$ It follows from strong induction, $$\frac{a}{2!} = \frac{(b)\cdot\prod x_{k}!}{\prod y_{j}!}$$ $$\frac{a}{b} = 2! \cdot \frac{ \prod x_{k}!}{\prod y_{j}!}$$","['analysis', 'calculus', 'real-analysis', 'contest-math']"
1198294,Proof of Wald's Equation: Why do we need this statement?,"Let $(X_i)_{i \geq 1}$ be a sequence of i.i.d. random variables with $\mathbb{E}[X_i]= \mu$ and $Var[X_i]= \sigma^2$. Let $\tau$ be a non-negative integer-valued random variable independent of $X_i$. Now consider $S_n:=\sum_{i=1}^n X_i$. We want to show that $\mathbb{E}[S_{\tau} \mid \tau] = \mu \tau$ a.s. for $\mathbb{E}[\tau]< \infty$. In the proof there first is shown that $\mathbb{E}[|S_{\tau}|]\leq \mathbb{E}[|X_i|]\mathbb{E}[\tau]$ using the monotone convergence theorem. Then we calculate $\mathbb{E}[S_{\tau} \mid \tau] = \mathbb{E}[S_k] \mid_{k=\tau}=\mu k \mid_{k = \tau}= \mu \tau$. What confuses me now is: do we really need that statement above about the absolute value derived using the monotone convergence theorem? Under which circumstances is the second calculation (with the conditional expectation) valid? I would suggest that we calculated this to justify the use of dominated convergence, but where do we use dominated convergence in the following? Then it is mentioned that we can use the tower property of conditional expectation to derive that $\mathbb{E}[S_{\tau}]= \mu \mathbb{E}[\tau]$. Do we need the above statement for this?","['probability-theory', 'conditional-expectation', 'expectation']"
1198295,Prove that function is continuous without knowing the function explicitly,"Let $f\colon \mathbb R^+\to\mathbb R$ be a function that satisfies the following conditions:
$$\tag1 \lim_{x\to 1}f(x)=0 $$
$$\tag2f(x_1)+f(x_2)=f(x_1x_2)$$
Show that $f$ is continuous in its domain. I managed to show that $f$ is continuous at $x=1$, but I have no idea how to continue from there. Here's what I've done so far: Because $\lim_{x\to 1}f(x)=0$, for every ϵ>0 there exists a δ>0 so that
$$0<|x - 1|<δ⇒|f(x)-0|<ϵ$$ To prove continuity at $x=1$ it's enough to show that $f(1)=0$ using the condition 2):
$$f(1)+f(1)=f(1 ·1)$$
$$f(1)=f(1)-f(1)$$
$$f(1)=0$$
So now we have the definition of continuity at $x=1$:
$$|x - 1|<δ⇒|f(x)-f(1)|<ϵ$$","['continuity', 'real-analysis', 'functional-equations']"
1198329,Equivalent metrics define the same topology proof.,"Let $X$ be a set and $d_{1},d_{2}$ be two metrics on $X$. Define a metric to be equivalent if convergence of a sequence in one metric implies the convergence in the other. I am having difficulty understanding a particular line of one version of the proof. The overall strategy is to show that if $U \subset X$ is open with respect to $d_{1}$, that through a contradiction, show that there must be a $d_{2}$ ball contained by $U$. The proof begins by defining $u\in U$ and supposing $U$ is open in $d_{1}$. Suppose there is no ball of radius $r$ in the $d_{2}$ metric that is contained in $U$. Therefore, for every natural number $n$ there is a $x_{n}\in X\setminus U$ such that $d_{2}(x_{n},u) < 1/n$ and hence $x_{n}$ converges to $u$ in $d_{2}$. I get that $d_{2}(x_{n},u) < 1/n$ implies convergence in $d_{2}$. I don't understand why there is necessarily a sequence in $x_{n}\in X\setminus U$ that has this property or why it is worded in this particular manner. If this is a standard Euclidean $n$-ball over $\mathbb{R}^{2}$, and I naively draw a diagram I can accept this, but for an arbitrary set and two arbitrary metrics, I am having trouble convincing myself.","['analysis', 'general-topology']"
1198347,Find a function with certain requirements,"I'm trying to find a function $y=f(x)$ that can be described as follows: $f(x) = g(x) + c/(x-x_a)$. With $f(x)$ I want to design a function with the following properties: $f(0) = 0$; $f(x)$ has a maximum at $x_1, \quad x_1>0$; the maximum of $f(x)$ is $\max(f(x))=f(x_1)=y_\max$ $f(x_2) = 0, \quad x_2>x_1$; $f(x)$ has a vertical asymptote at $x=x_a, \quad x_a<0$; $f(x)$ has a second asymptote which is a function $g(x)$; Thus: I want to set $x_1, x_2$ and $y_\max$ and provide a function $g(x)$ to create a function with the listed requirements. I have tried to implement this by solving a system of non-linear equations, but I could not achieve a function with all the listed conditions being met. I used MATLAB function fsolve to solve the unknown parameters of $g(x), c$ and $x_a$, by solving $f(x)$ for the unknown parameters manually (this could be done using the symbolic math toolbox as well). An example, to give you a better feeling for what I'm looking for, is shown in the figure below. In the example, the asymptote function I used was $g(x) = ax+b$, so $a$ and $b$ are additional parameters to solve. From the figure you can see that conditions $f(0)=0$ and $f(x_2)=0, x_2=7$ have been met, but in this case I didn't manage to set the desired value of the maximum ($y_\max$) at the desired $x$-value ($x_1$). How do I do this (by hand and/or using MATLAB)? Do I need to solve $\mathrm{d}f(x)/\mathrm{d}x=0$ for the parameters involving the peak height and $x$-location, or can this be done in a different or more efficient way?","['interpolation', 'parameter-estimation', 'matlab', 'functions']"
1198364,"Seminorms in distribution theory are norms, right?","In distribution theory the seminorms that you use there $p_m( \phi) := \max_{|\alpha| \le  m}  \sup_{x \in \Omega} |(\partial^{\alpha}(\phi) (x)|, \phi \in C_c^{\infty}(\Omega)$. Those guys are norms and not just seminorms, right?- I am just wondering because I have a book that keeps on calling them seminorms, so I just wanted to be sure that I am not making a stupid error over and over again","['normal-distribution', 'distribution-theory', 'real-analysis', 'functional-analysis', 'analysis']"
1198366,Exchanging $\lim$ and $\inf$?,"Suppose we have a sequence of functions $f_n(x)$ that converge to a limiting function $f(x) = \lim_{n \to \infty} f_n(x)$ for $\forall x \in [a,b]$. I was wondering under what conditions the following holds?
\begin{equation*}
\lim_{n \to \infty} \inf_{x \in [a,b]} f_n(x) = \inf_{x \in [a,b]} f(x).
\end{equation*}","['optimization', 'calculus', 'limits', 'convergence-divergence']"
1198373,Monotonicity of function of two variables,"I have a function of two variables, which I wish to check for monotonicity in the entire function domain. I cant find any formal definition of increasing or decreasing function for multi variable case. Can anybody please guide?
Thanks in advance.","['multivariable-calculus', 'real-analysis']"
1198377,Proof that the Gaussian Curvature is a Ratio of Areas,"Let $S \subset \mathbb{R}^3 $ be a smooth surface, and let $S^""$ be the unit sphere, and let $n: S \to S^2$ be a given Gauss map. I want to prove that the Gaussian curvature $K(p)$ at a point $p \in S$ is the limit of the ratio of areas we get when a neighbourhood $U$ shrinks around $p$ :
$$ K(p) = \lim_{U \to p}\frac{\text{Area}(n(U))}{\text{Area}(U)} $$ I have shown already that
$$\lim_{U \to p} \frac{\text{Area}(n(U))}{\text{Area}(U)} \le K(p) $$
letting $U = F(V)$ for some local parameterization $F$ from $V \subset \mathbb{R}^2$ to $U \subset S$, and using my definition:
$$
\text{Area}(U) := \int_V ||\partial_x F \times \partial_y F || dx dy.
$$
and the triangle inequality. Is a lower bound obvious? or do we need to use the (reverse) triangle inequality or something?","['geometry', 'multivariable-calculus', 'differential-geometry']"
1198379,Changes in singular values of matrix when rows are added,"I know that if a column is added to a matrix then the matrix largest signular value increases and the smallest singular value decreases. That is:
Given matrix $A \in R^{m \text{x} n}$, $m>n$, and $z \in R^{m}$ then
$$\sigma_{max}([A |z]) >= \sigma_{max}(A),$$
and
$$\sigma_{min}([A |z]) <= \sigma_{min}(A),$$ But how do I show that when a row is added, the singular values of $A$ also change as follows: ($w \in R^{n}$) $$\sigma_{n}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])>=\sigma_{n}(A)$$ and 
$$\sigma_{1}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])<=\sqrt{||A||_2^2 + ||w||_2^2}$$","['svd', 'linear-algebra', 'matrices']"
1198386,Determine the number of solutions of $x^p\equiv 1\mod p^h$ using primitive roots,"So the problem is to determine the number of solutions of the congruence $x^p\equiv 1\mod p^h$, where $p$ is an odd prime and $h\geq2$. We are asked to establish the result using primitive roots. We know that for any primitive root $g_1$ of $p$, $g=g_1+kp$ is, for some $k$, a primitive root mod $p^h$. Also, if $x$ is a solution of the congruence, then there is some $r$ such that $x \equiv g^r \mod p^h$. The congruence then becomes
$$(g^r)^p\equiv1\mod p^h,$$
from which we can infer $\phi(p^h)∣rp$. This especially means $p^{h−2}(p−1)∣r$. All in all, we know now that $r$ has to be even and only take values $p^{h−2}(p−1)\leq r\leq p^{h−1}(p−1)$. I've played around with a few numbers a bit, and found that $r$ likely has to be of the form $kp^{h−2}(p−1)$ for $1\leq k\leq p$, which means that there are exactly $p$ distinct solutions to the congruence, as was expected (by asking WolframAlpha to solve the original congruence). These solutions are given by $$x=g^{kp^{h-2}(p-1)}.$$
We can check this and it looks like it fits. But I really don't know how to show that the number of solutions is $p$. Why does $r$ have to be of that specific form? That's only a thing I observed but am not able to prove. Any hints would be greatly appreciated","['number-theory', 'primitive-roots']"
1198399,Fibered product of Hopf Fibrations,I am wondering: what is the vector bundle associated to the Hopf-fibration $S^{3}\to S^{2}$? What is the fibered product of two Hopf-fibrations? Thanks.,"['differential-geometry', 'algebraic-topology', 'fibration', 'fiber-bundles']"
1198414,Pull back image of maximal ideal under surjective ring homomorphism is maximal,"Let $f :R \to S$ be a surjective ring homomorphism , $M$ be a maximal ideal of $S$ , I am writing a proof showing $f^{-1}(M)$ is a maximal ideal of $R$ , Please verify whether it is correct or not . Proof :- Let $J$ be an ideal such that $f^{-1}(M) \subset J \subseteq R$ , we want to show $J=R$ i.e. $R \subseteq J$. Now as  $f$ is surjective , $M=f(f^{-1}(M)) \subseteq f(J) \subseteq S$ . As $f$ is surjective , $f(J)$ is an ideal of $S$ . Now if it were possible that $M=f(J)$ , then $x \in J \implies f(x) \in f(J)=M \implies x \in f^{-1}(M)$ , so $J \subseteq f^{-1} (M)$  ,contarry to our assumption $f^{-1}(M) \subset J$ . Thus $M \ne f(J)$ , $f(J)$ is an ideal of $S$ containing $M$ ; since $M$ is maximal ideal in $S$ , so $f(J)=S$ . Then $x \in R \implies f(x) \in S=f(J) \implies \exists j \in J : f(x)=f(j) \implies f(x-j)=0_S \in M $ $\implies x-j \in f^{-1} (M) \subset J \implies x=x-j+j \in J$ , so $R \subseteq J$ Is the proof correct . Please comment .","['ring-theory', 'maximal-and-prime-ideals', 'ideals', 'functions']"
1198427,"zeros of $x^*Ax$, a quadratic form","The question hopefully says it all! We have a Hermitian matrix $A=A^* \in \mathbb{C}^n$ and a quadratic form: $f(x)=x^*Ax,~x\in \mathbb{C}^n$ We want to find the solution of $f(x) = x^*Ax = 0$ When the matrix is positive semi definite (p.s.d.), the solution seems to be null-space of the matrix of $A$. This I found by diagonalising $A$. But suddenly I became helpless when $A$ has negative Eigen values too! Please note that when the matrix is not p.s.d., the solution space contains the null space. (I.e. null space is always a solution)","['matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'quadratic-forms', 'linear-algebra']"
1198457,"If I remove a remove a subset of a uniform distribution, will it remain uniform?","suppose that I have a long sequence of random numbers coming from a uniform distribution. If I remove a small subset from the sequence (either part of the sequence, or randomly pick some elements), will the remaining sequence still be a uniform distribution? Thank you! edit: all of the elements are generated from the same distribution. For example generate 10,000 values randomly from a uniform distribution with a range of [0,1]","['statistics', 'probability-distributions']"
1198467,Looking for at least one surjective ring homomorphism from $M_n(R)$ to $R$,"Let $R$ be a ring , I am looking for a surjective ring homomporphism from $M_n(R)$ to $R$ . Please help . Thanks in advance .","['ring-theory', 'functions', 'soft-question', 'matrices']"
1198479,Solve integral (convolution) equation,"Given a function:
$u(t) = \exp\left( -\frac{At^2}{1+t}\right),$
$A>0, t>0,$ and an equation:
$\frac{d u(t)}{dt} = \int^{t}_0 \phi(t-\tau) u(\tau) d \tau .$ How to find a closed expression for $\phi(t)$?","['integral-equations', 'ordinary-differential-equations', 'laplace-transform', 'convolution', 'partial-differential-equations']"
1198485,Evaluate $\int \frac{1}{x^3+3x+1}dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Evaluate $$\int \frac{1}{x^3+3x+1}dx$$
I tried to evaluate it but I couldn't  do .","['calculus', 'indefinite-integrals', 'integration']"
1198494,Solving $\sum\limits_{k=0}^n k2^k \binom{n}{k}$,"I'm trying to solve $\sum\limits_{k=0}^n k2^k \binom{n}{k}$. By binomial theorem, $\sum\limits_{k=0}^n x^k \binom{n}{k}=(1+x)^n$, I observe that $\sum\limits_{k=0}^n kx^k \binom{n}{k}=\left (\sum\limits_{k=0}^n x^{k+1} \binom{n}{k} \right )'$, where ' indicates the first derivative.
But $\left (\sum\limits_{k=0}^n x^{k+1} \binom{n}{k} \right )'=(x(1+x)^n)'=(1+x)^{n-1}(1+x+nx)$.
Therefore $\sum\limits_{k=0}^n kx^k \binom{n}{k}=(1+x)^{n-1}(1+x+nx)$. Evaluating in $x=2$, I have the quantity $3^{n-1}(3+2n)$.
Is my argument correct?",['combinatorics']
1198496,Finitely many isomorphism classes for finite index subgroups,"I'm looking for a classification/reference/theory for groups with the following property: every finite index subgroup of $G$ lies in one of finitely many isomorphism classes. I would also be interested to see this as a straightforward consequence of some other more well known classification. A simple example, beyond the obvious abelian ones, would be the infinite dihedral group $D_\infty$. All the finite index subgroups are either isomorphic to $D_\infty$ or isomorphic to $\mathbb{Z}$. Classes of examples that satisfy the property are also of interest.",['group-theory']
1198507,If $\alpha$ and $\beta$ are ordinals then $\alpha \in \beta \Leftrightarrow \alpha \subsetneq \beta$,Def 1. $x$ is $\underline{transitive}$ if $\forall y \forall z (z \in y \in x \Rightarrow z \in x)$. Def 2. $x$ is $\underline{ordinal}$ if $x$ is transitive and all elements of $x$ are transitive. Def 3. $S(x) = x \cup \{x\}$. If $\alpha$ and $\beta$ are ordinals then $\alpha \in \beta \Leftrightarrow \alpha \subsetneq \beta$. $\Rightarrow$. By transitivity. $\Leftarrow$. It seems that we have to take $\alpha$ and prove that $S(\alpha) \subset \beta$. $S(\alpha)$ is also ordinal if $\alpha$ was ordinal. If we can do that it will imply that ${\alpha} \in \beta$. But how to make it precisely?,"['ordinals', 'elementary-set-theory']"
1198518,Testing hypothesis that means are same using t-test and confidence intervals give different results,"I have two samples and I want to test the null hypothesis that the means of the two samples are the same at a 95% level of confidence interval. When I use a t-test my p value is 0.023 and so I reject the null hypothesis that the means are the same, and conclude there is a significant difference between the means. However when I calculate the 95% confidence intervals of each sample individually the confidence intervals overlap, which suggests to me that we do not have enough evidence to reject the null hypothesis and conclude that the means are different. Is it possible to get different conclusions using these two methods, and if so which one should I trust more? Or have I done something wrong somewhere? Thanks in advance","['hypothesis-testing', 'statistics']"
1198520,Integrating reciprocals of functions with known antiderivatives,"If
$$\int_{}^{} f(x)\,dx$$
is known, is there a way to directly find
$$\int_{}^{} \frac{1}{f(x)}\,dx$$",['calculus']
1198527,Uniform convergence and composition,"Suppose we have a sequence of analytic functions $f_j:(1,M) \to (1,M)$ for some $M \in \mathbb{R}^+$. We are guaranteed that there is an $n \in \mathbb{N}$ such that its $n$'th iterate $$g_j(x) =f_j( f_j(...(n\,times)...(f_j(x)) = (f_j \circ f_j \circ ...(n\,times).. \circ f_j)(x)$$ converges uniformly, $g_j \to g$. Does this guarantee that $f_j \to f$ uniformly? If so how would one go about showing this? I think a proof by contradiction would work, if $f_j \not\to f$ uniformly then $g_j \not\to g$ uniformly. Is this sufficient enough of an argument? Any help or suggestions would be greatly appreciated. Thanks.",['analysis']
1198536,Proving combinatorially $\sum_{k=0}^n k \binom n k ^2=n\binom{2n-1}{n-1}$,"Prove with a combinatorial proof (story): $\displaystyle\sum_{k=0}^n k \binom n k ^2=n\binom{2n-1}{n-1}$ My attempt: Let's make it easier to work with (it's very easy to show that identity): $\displaystyle\sum_{k=0}^n \frac kn \binom n k ^2=\binom{2n-1}{n-1}\Rightarrow \sum_{k=0}^n \binom {n-1}{k-1} \binom n k =\binom{2n-1}{n-1}$ RHS: we want to form a group of size $n-1$ from $2n-1$ people. LHS: The group has $n$ men and $n-1$ women. We'll choose the men and women such that there will always be $1$ men more than women: No men - no way to form such group. $1$ person - $n=\binom {n-1} 0 \binom {n} 1$. $3$ people - $\binom{n-1} 1\binom n 2$ ... $n$ people -   $\binom{n-1} {n-1}\binom n n$ And if we sum up all the different cases we get: $\displaystyle\sum_{k=0}^n \binom {n-1}{k-1} \binom n k$ I'm not convinced with my story for the LHS, picking the groups such that there will always be $1$ men more seems too arbitrary. Also I'm pretty sure the first and last cases are wrong. Any tips on how fix it? Note: no integrals nor use of other identities without proving them nor generating functions.","['binomial-theorem', 'combinatorics']"
1198549,Can a differential k-form be integrated on a manifold that is not k-dimensional?,"For example, can you integrate a 2-form on some curve, a 1-dimensional manifold, or some 3-dimensional manifold? I know that Stokes's Theorem states that if you integrate $\omega \in \mathcal A^{k-1}(M)$ or a (k-1)-differential form when integrated over the (k-1)-dimensional boundary of the k-dimensional manifold $M$, it is equal to integrating $d\omega \in \mathcal A^{k}(M)$ over the k-dimensional manifold $M$. I.e. $\int_{\partial M} \omega = \int_{M} d\omega$. I'm not very confident in all this because I'm new to learning university math, I am a novice in it and I'm doing this for fun as I am in Grade 11 still so I am not really forcing myself to learn all the details which can be bad. If you can recommend an article or book that explains my question that is suitable for my level, I would appreciate it a lot. I'm studying from Professor Shifrin's lectures on Multivariable Calculus. I was recently on one of his lectures on Stoke's Theorem and it was interesting because it seemed that all the dimensions of the forms and the manifolds (or its boundaries) matched up. Therefore, I was curious if I can integrate k-forms on manifolds of dimension less than k or bigger than k, as I can integrate at least some k-forms on k-dimensional manifolds according to Stokes's Theorem. I have edited this question as I had no details whatsoever which can seem rude so I tried to put some context into it but I'm new to this site so please bear with me.","['differential-forms', 'manifolds', 'multivariable-calculus']"
1198564,Is this a theorem or a conjecture?,"A few months ago, I read The Irrationals by Julian Havil. I remember reading that if you order the rationals between 0 and 1 in this way: $\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{2}{4},\frac{3}{4},\frac{1}{5},\cdots$ and then construct intervals, the first one being $[\frac{1}{3},\frac{1}{2}]$ and the following ones being constructed from the next two (not necessarily consecutive) terms in the sequence that are included in the last constructed interval, you get smaller and smaller intervals converging to a single point: $\sqrt2 - 1$. My question is whether this result was arrived at numerically or is there a formal proof? [and if the latter, what is the proof or where can I find it?] The wording in the book did not make this very clear, if I recall correctly.","['sequences-and-series', 'number-theory', 'limits']"
1198592,Radon-Nikodym Derivatives between Ito Processes,"I am curious about the following problem: Let $B_t$ be a standard Brownian motion on $(\Omega, \mathcal F, \mathcal F_t, \mathbb P_a)$, where the filtration is generated by $B_t$. On a finite interval $[0,T]$ we define $X_t$ as the one solving the SED
$$\mathrm dX_t=\mu_a\,\mathrm dt+\sigma\,\,\mathrm dB_t.$$
For some other measure $\mathbb P_b$, we define $X_t$ as the solution to
$$\mathrm dX_t=\mu_b\,\mathrm dt+\sigma\,\mathrm d B_t',$$
where $B_t'$ is a Brownian motion under $\mathbb P_b$, $\mu_a\ne\mu_b$ being two different real numbers, and $\sigma>0$ being a constant. Hence, the difference between the two diffusion processes lies only in the drift. My question is: what is the Radon-Nikodym derivate (as a function of $t$ and $X_t$)
$$\frac{\mathrm d\mathbb P_a}{\mathrm d\mathbb P_b}$$ 
on $\{\mathcal F_t\}$? What I know so far is the answer to a special case: $\mu_a=0$, where the answer can be derived explicitly. Is it possible to generalize the special case? Many thanks!","['probability-theory', 'brownian-motion', 'radon-nikodym', 'stochastic-processes']"
1198604,Two definitions of curvature,"my question is about the compatibility of two definitions of curvature of a Riemannian manifold. In particular I refer to the one from algebraic geometry and the one from differential geometry. Reading books like ""Principles of algebraic geometry"" by Griffiths-Harris or ""Complex algebraic geometry"" by Claire Voisin we can find the curvature of a manifold defined as the second iteration of the connection. More specifically we define the connection
$\nabla$ as a function from $\Gamma(TM)$ to $\Gamma(TM)\otimes\Gamma(\Omega_M)$. If $\{\frac{\partial}{\partial x_i}\}_i$ is a local base for $\Gamma(TM)$ we write $\nabla(\frac{\partial}{\partial x_i})=\sum_j\theta_{ij}\frac{\partial}{\partial x_j}$ and $(\theta_{ij})$ is the connection matrix. So the curvature is simply defined as $\nabla^2$ from $\Gamma(TM)$ to $\Gamma(TM)\otimes\Gamma(\Omega_M^2)$, in particular in local coordinates $\nabla^2(\frac{\partial}{\partial x_i})=\sum_{l,f}(d\theta_{if}+\theta_{if}\wedge\theta_{fl})\frac{\partial}{\partial x_f}$. Reading a differential geometry book (for example Do Carmo) we can find the curvature $R$ defined as $R:\Gamma(TM)\times\Gamma(TM)\times\Gamma(TM)\rightarrow \Gamma(TM)$, $(X,Y,Z)\mapsto R(X,Y)Z=\nabla_Y\nabla_XZ-\nabla_X\nabla_YZ+\nabla_{[X,Y]}Z$.
I know that in local coordinates $[\frac{\partial}{\partial x_k},\frac{\partial}{\partial x_j}]=0$, so locally $R(\frac{\partial}{\partial x_k},\frac{\partial}{\partial x_j})\frac{\partial}{\partial x_i}=\nabla_{\frac{\partial}{\partial x_j}}\nabla_{\frac{\partial}{\partial x_k}}\frac{\partial}{\partial x_i}-\nabla_{\frac{\partial}{\partial x_k}}\nabla_{\frac{\partial}{\partial x_j}}\frac{\partial}{\partial x_i}$.
Doing the calculations it comes out $\nabla_{\frac{\partial}{\partial x_k}}\frac{\partial}{\partial x_i}=\sum_l\theta_{il}(\frac{\partial}{\partial x_k})\frac{\partial}{\partial x_l}$ and $\nabla_{\frac{\partial}{\partial x_j}}\nabla_{\frac{\partial}{\partial x_k}}\frac{\partial}{\partial x_i}=\sum_{l,f}d(\theta_{if}(\frac{\partial}{\partial x_k}))(\frac{\partial}{\partial x_j})+\theta_{if}(\frac{\partial}{\partial x_k})\theta_{fl}(\frac{\partial}{\partial x_j}))\frac{\partial}{\partial x_f}$ which is coherent with what i wrote for $\nabla^2(\frac{\partial}{\partial x_i})$, but (QUESTION 1) why should I consider also the term $\nabla_{\frac{\partial}{\partial x_k}}\nabla_{\frac{\partial}{\partial x_j}}\frac{\partial}{\partial x_i}$? Also, when considering the question globally, (QUESTION 2) why should I consider the term $\nabla_{[X,Y]}Z$? Thank you very much.","['algebraic-geometry', 'differential-geometry', 'connections']"
1198621,"Compactness, continuity and the discrete topology","Assume that $X, Y$ are compact metric spaces, and that there is a map $$ \mu :  X \to \Delta (X \times Y)$$ such that $\mu$ is continuous , where $\Delta (\Omega)$ denotes the set of probability measures over a generic $\Omega$ . Endow $X \times Y$ with the product topology , and $ \Delta (X \times Y)$ with the topology of weak convergence . The continuity of $\mu$ tells us that when we have an open (resp. closed ) subset $G$ of $ \Delta (X \times Y)$, we are ensured that the preimage $\mu^{-1} (G)$ is open (resp. closed ). However, I have a problem with the following situation. Let $X$ be finite, hence compact. Let $G = \{ \delta_{(x,y)} \}$, where $\delta$ denotes the Dirac measure for some elements $x \in X$ and $y \in Y$. Thus $\mu^{-1} (\{ \delta_{(x,y)} \} )$  maps to some element $ x \in X$. But now, how can $\mu$ really be continuous in this case? Is it continuous because we are implicitly endowing the finite $X$ with the discrete topology? Any feedback or answer is most welcome. Thank you for your time. PS: To the moderators, this questions looks fairly close to this previous one . However, they are different in spirit, because that question is not well written (too many questions into one). Hence, I decided to ""unzip"" it, starting from this one. Regarding this issue, I think it would be wise to close that linked question (I don't know how to do it).","['self-learning', 'real-analysis', 'general-topology', 'measure-theory']"
1198630,Show that the following are equivalent:,"If $f$ is a continuous function on a bounded set $S$ , show that the following are equivalent: (a) the function $f$ is uniformly continuous on $S$ . (b) it is possible to extend $f$ to a continuous function on the set $\text{cl}(S)$ .","['analysis', 'continuity', 'uniform-continuity', 'functions']"
1198663,How many degrees of freedom does a line have on a plane?,"Using $y=ax+b$ I can get a line for every point $(a,b)$ but there are still some lines left $(x=c)$. So it seems that lines are more than points so they have more degrees of freedom than $2$. How many?","['algebraic-geometry', 'linear-algebra']"
1198682,Reference Text that develops Linear Algebra with Knowledge of Abstract Algebra,"Background: Due to some unfortunate sequencing, I have developed my abstract algebra skills before most of my linear algebra skills. I've worked through Topics in Algebra by Herstein and generally liked his approach to vector spaces and modules. Besides a very elementary course in linear algebra (where most of the time went towards matrix multiplication), I have not developed any other linear algebra skills. But it seems that it is now necessary for me to do so. Most of the topics that I am looking at now require some background of linear algebra and I still lack understanding of ideas like: bilinear forms, invariant subspaces, eigenvalues, requirements for diagonalization of a matrix and so forth. This brings me to my question (provided there are any) Question: What are some good texts that develop the theory of linear algebra from a perspective of general algebra? Are there any texts that develop the key (elementary) ideas of linear algebra in an abstract setting? Thank you for the help!","['soft-question', 'reference-request', 'abstract-algebra', 'book-recommendation', 'linear-algebra']"
1198702,Expression from generators of Special Linear Groups II,"I wonder whether one can generate this $t$ matrix form the $A_1$ and $A_2$ matrix below. Here
$$
t=\begin{pmatrix} 1& 1& 0\\ 0& 1& 0\\ 0& 0& 1 \end{pmatrix}
$$
from:
$$
A_1=\begin{pmatrix} 0& 0& 1\\ 1& 0& 0\\ 0& 1& 0 \end{pmatrix},
\text{ 
and   }\;\; 
A_2=\begin{pmatrix} 0& 1& 0\\ -1& 0& 0\\ 0& 0& 1 \end{pmatrix}.
$$ Question: How to generate $t$ from $A_1$ and $A_2$? i.e. So what is the exact expression to make $t=\dots A_1 \dots A_2$ as a product of $A_1$ and $A_2$ matrices? Notice that $(A_1)^3=1$ and $(A_2)^4=1$. I learned the basics about those generators and SL(N,Z) from the book Coxeter and Moser on "" Generators and relations for discrete groups "" (published by Springer, 1957). Linear algebra and special-linear group experts please help. Thank you! :o)","['discrete-mathematics', 'integer-lattices', 'abstract-algebra', 'group-theory', 'linear-algebra']"
1198722,"In a linear program, how to add a conditional bound to x?","I am working with a standard linear program: $$\text{min}\:\:f'x$$
$$s.t.\:\:Ax = b$$
$$x ≥ 0$$ Goal: I want to enforce all nonzero solutions $x_i\in$ x to be greater than or equal to a certain threshold ""k"" if it's nonzero. In other words, I want to add a conditional bound to the LP: if any $x_i$ is > 0, enforce $x_i$ ≥ k. Main issue: Is there a way to set up this problem as an LP? Any alternate approaches? Any input would be appreciated and I'm happy to provide any additional info as needed! Thanks!","['optimization', 'linear-programming', 'linear-algebra', 'matlab']"
1198729,Row vector vs. Column vector,"I'm a student in an elementary linear algebra course. Without bashing on my professor, I must say that s/he is very poor at answering questions, often not addressing the question itself. Throughout the course, there have been multiple questions that have gone unanswered, but I must have the answer to this one question. ""Why are some vectors written as row vectors and others as column vectors?"" I understand that if I transpose one, it becomes the other. However, I'd like to understand the purpose behind writing a vector in a certain format. Taking examples from my lectures, I see that when I'm trying to prove linear independence of a group of vectors, the vectors are written as column vectors in a matrix, and the row reduced form is found. Other times, like trying to find the cross product or just solving a matrix, I see the vectors written as row vectors. My professor is very vague on the notations and explanations, and it bugs me as a person who needs to know the reason behind every small thing, why this variation occurs in the format. Any input is greatly appreciated.","['vectors', 'linear-algebra']"
1198735,Combinatorial proof of $\sum_{k=1}^n k k!=(n+1)!-1$,"Prove: $\displaystyle\sum_{k=1}^n k k!=(n+1)!-1$ (preferably combinatorially) It's pretty easy to think of a story for the RHS: arrange $n+1$ people in a row and remove the the option of everyone arranged to height from shortest to highest, but it doesn't hold up for the LHS. Alternatively, trying to visualize the LHS, I noticed that it's like a right angle tetrahedra: 1 2!+2! 3!+3!+3! ... But it doesn't help to see a connection to the RHS. Note: no integrals or gamma function nor use of other identities without proving them nor generating functions.","['factorial', 'summation', 'induction', 'combinatorics']"
1198751,Eigenvalues of a 2x2 matrix A such that $A^2$=I,"I have no idea where to begin. Let A be a $2\times 2$ matrix such that $A^2= I$, where $I$ is the identity matrix.  Find all the eigenvalues of $A$. I know there are a few matrices that support this claim, will they all have the same eigenvalues?","['linear-algebra', 'matrices']"
1198754,Find All Dimensions such that Volume of Box = Surface Area,"A rectangular prism has integer edge lengths. Find all dimensions such that its surface area equals its volume. My Attempt at a Solution: Let the edge lengths be represented by the variables $l, w, h$. Then $$lwh = 2\,(lw +lh + wh) \implies lwh = 2lwh\left(\frac{1}{l} + \frac{1}{w} + \frac{1}{h}\right).$$ Dividing both sides of the equation by $lwh$ yields $$1 = 2\left(\frac{1}{l} + \frac{1}{w} + \frac{1}{h}\right)$$ Or, $$\frac{1}{l} + \frac{1}{w} + \frac{1}{h} = \frac{1}{2}$$ Though perhaps a bit unnecessary, I used some algebraic deduction and number theory to find all the possible ordered triple pairs for the dimensions of the rectangular prism in the cases where all the dimensions are the same and two of the dimensions are the same. My answers were: $(6,6,6),(5,5,10),(8,8,4),(12,12,3)$ I have a hunch that no ordered pair exists where all three values are distinct, but is there a way to rigorously prove this? Note: By the AM-GM Inequality, $lwh \geq 216$. (I haven't been able to make use of this fact, but I just noted it here in case) Edit : After researching a little more about Egyptian Fraction Analysis, inspired by marty cohen's answer, I found from a Mathworld Wolfram page about Egyptian Fractions that a unit fraction can be (infinitely) split into two more unit fractions: $$\frac{1}{a} = \frac{1}{a+1} + \frac{1}{a(a+1)}.$$
I then used this idea to successfully generate a few ordered triplet solutions ($l,w,h$). WLoG, assume $l \leq w \leq h$. Then the following ordered triplets ($l,w,h$) are solutions of the equation $\frac{1}{l} + \frac{1}{w} + \frac{1}{h} = \frac{1}{2}$. $$(4,6,12), (3,7,42), (3,8,24)$$ But, I suspect there are many more solutions since you can mix and match the fractions that you decompose. This results in countless more combinations of ($l,w,h$), and I don't know how to use casework or an otherwise more organized approach to solve this problem. But, I do believe it can still be solved using this approach. If anybody has any idea on how to solve the problem using the ""splitting unit fraction"" method, or has any other working solution, I would greatly appreciate it if you shared it with me. Apologies for the long post and thanks for reading.","['fractions', 'egyptian-fractions', 'number-theory', 'algebra-precalculus']"
1198764,Is there a Linear Map to represent transpose?,"For example,
I want $AB = A^T$, is there such a matrix $B$ that does this? Where $A$ is $n \times m$ and $B$ is $m \times n$.","['matrix-equations', 'linear-algebra', 'matrices']"
1198797,Multivariable/Vector Calculus Textbook Recommendation Please! [duplicate],"This question already has answers here : References for multivariable calculus (13 answers) Closed 6 years ago . S.E friends, I am a college sophomore with a major in mathematics.  I am trying to self-study multivariable and vector calculus (they means the same, right?) and prepare for Summer course on multivariable calculus.  Our university uses a course packet for the multivariable calculus which is not theoretical enough to satisfy my curiosity.  I am seeking a textbook that covers both theories and applications, with more emphasis on theories. I have been searching the forum and it seems there are sook good books on multivariable calculus:  Hubbard/Hubbard's Vector Calculus, Linear Algebra, and Differential Forms; Marsden/Tromba's Vector Calculus, Collier's Vector Calculus, and Lang's Multivariable Calculus.  I want to pick only one from them.  Could you help me?","['self-learning', 'book-recommendation', 'calculus', 'multivariable-calculus']"
1198805,duality of $L^p$ spaces with non $\sigma$ finite measure,"Why is the condition that $\mu$ (measure) is $\sigma$-finite is important for the relation
$(L^1)^{*} = L^{\infty}$? This condition is added while proving that $(L^p)^{*} = L^{q}$ where $p, q$ are conjugate exponents and $1 \leq p < \infty$. Could someone provide a counter example using counting measure and show that in the case of the measure being not $\sigma$-finite $(L^1)^{*} \neq L^{\infty}$.","['real-analysis', 'functional-analysis']"
1198830,Trace operator is basis independent,"Let $H$ be a Hilbert space and call suppose $A:H\rightarrow H$ is positive. How do you show Tr$(A)=\sum_{n}(Ae_n,e_n)$, does not depends on the orthonormal basis $e_n$. I was thinking about using an approximation of $A$ as $A_k$ where $A_k$ is finite rank operator, so $Tr(A_k)$ is the sum of diagonal entries of an $\infty\times n$ matrix, which is basis independent. Then use $Tr(A_k)\rightarrow Tr(A)$ to conclude the proof. Is this the right approach?","['real-analysis', 'functional-analysis']"
1198831,Slightly changing the formal definition of continuity of $f: \mathbb{R} \to \mathbb{R}$?,"I'm curious for some perspectives on why it would be wrong to change the definition of continuity of $f: \Bbb R \to \Bbb R$ in the following way: Original definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \epsilon > 0$ $\exists \delta > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . Altered definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \delta > 0$ $\exists \epsilon > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . The altered definition is more in line with what I think when I think about continuity intuitively: nearby points are sent to nearby points.  It only makes sense to me to be able to choose ""nearness"" in the domain (i.e., $\forall \delta > 0$ ) and show there is nearness in the codomain (i.e., $\exists \epsilon > 0$ ) to prove intuitively that ""nearby points are sent to nearby points"". Similarly , if $X, Y$ are topological spaces, we say $f: X \to Y$ is continuous if the preimages of open sets are open.  What would be wrong about changing the definition to say that a map is continuous if the images of open sets are open (i.e., $f$ is continuous if it is an open map)?  This is more inline with the intuitive idea of ""nearby points being sent to nearby points"" -- you pick nearness in the domain (i.e., an arbitrary open set) an show nearness in the codomain (i.e., the image is open). Does anyone have any useful remarks?","['continuity', 'real-analysis', 'general-topology']"
1198841,Plücker Relation: misunderstanding?,"I'm trying to understand exterior algebra better by gaining some ""bare hands"" understanding of the exterior powers $\Lambda^k(X)$ in more detail when $\dim(X)$ is small. I think so far I understand the cases $\dim(X) = 1,2,3$ quite well. The next case $\dim(X) =4$ is giving me more trouble. So, let us take $X = \mathbb{R}^4$. We have, as usual $\Lambda^1(\mathbb{R}^4) = \mathbb{R}$ and $\Lambda^1(\mathbb{R}^4) = \mathbb{R}^4$. The exterior square $\Lambda^2(\mathbb{R}^4)$ is more interesting because $4$ is the smallest dimension such that there exist elements in $\Lambda^2(\mathbb{R}^4)$ besides those of the form $u \wedge v$ for $u,v \in \mathbb{R}^4$. Notably, there are the ""symplectic elements"" like $e_1 \wedge e_2 + e_3 \wedge e_4$. I can see that they cannot be obtained as wedges of two vectors. Also, I think I can see that they are all congugate under the action of $GL(4)$ on $\Lambda^2(\mathbb{R}^4)$. Initial Question : It seems to me that there are precisely two types of elements in $\Lambda^2(\mathbb{R}^2)$, each of which constitutes an orbit of the $GL(4)$-action. The ""symplectic elements"" and the ones which are wedges of two vectors. Is this correct? Or is there some intermediate type of element I am missing? In trying to confirm the answer to the above question was ""yes"" I spend some time browsing through wikipedia articles like these ones on Plücker coordinates. A lot of what is there is superfluous to my needs, but I think I correctly understood that there is a mapping
$$ \{ \text{planes in }\mathbb{R}^4 \} \longrightarrow \{ \text{lines in }\Lambda^2(\mathbb{R}^4)\}$$
which sends
$$ \mathrm{span}\{u,v\} \to \mathrm{span}( u \wedge v).$$ In terms of coordinates, this map sends the column space of a rank $2$ matrix 
$$ A =  \begin{bmatrix}
u_1 & v_1 \\
u_2 & v_2 \\
u_3 & v_3 \\
u_4 & v_4 \\
\end{bmatrix} $$
to 
$$ X_{12} e_1 \wedge e_2 + X_{13} e_1 \wedge e_3  + X_{14} e_1 \wedge e_4 +
X_{23} e_2 \wedge e_3  + X_{24} e_2 \wedge e_4  + X_{34} e_3 \wedge e_4 $$
where $X_{ij}$ is the determinant of the $2 \times 2$ submatrix of $A$ consisting of Rows $i$ and $j$. It's not too hard to see this map is well defined. It suffices to check performing elementary operations on the matrix can only scale the Plücker coordinates. Indeed: Switching the  columns of $A$ changes the sign of each $X_{ij}$. Adding a multiple of one column of $A$ to the other leaves each $X_{ij}$ invariant. Scaling a column of $A$ results in scaling each $X_{ij}$ by the same amount. Now from this section , I gathered that you are supposed to be able to detect precisely which elements of $\Lambda^2(\mathbb{R}^4)$ are wedges of two vectors by checking whether they satisfy the Plücker Relation:
$$X_{12}X_{34} − X_{13}X_{24}+ X_{23}X_{14} = 0$$
So, what I tried to do next was verify directly that, given an element $w =  \sum_{i < j} X_{ij} e_i \wedge e_j \in \Lambda^2(\mathbb{R}^4)$: If $w$ is a wedge of two vectors, then $w$  satisfies the Plücker Relation. If $w$ does not satisfy the Plücker Relation, then $w$ is a ""symplectic element"". However, I was confused to discover that (1) does not seem to hold Main Question: Am I going crazy? Or does the Plücker relation
  $$X_{12}X_{34} − X_{13}X_{24} + X_{23}X_{14} = 0$$ not generally hold where
  $$
X_{ij} = 
\det \begin{bmatrix}
u_i & v_i \\
u_j & v_j \\ 
\end{bmatrix}$$
  and $u,v \in \mathbb{R^4}$ are linearly independent? I have computed this about 4 times now. Possibly I am making some mistake, but at this point I think it is more likely that I am not understanding what the Plücker relations are supposed to do properly. Sorry for the long question. I could have been more concise, but I also wanted to record my thought process so that I could recall it later.","['determinant', 'exterior-algebra', 'multilinear-algebra', 'abstract-algebra', 'linear-algebra']"
1198895,"Relationship between eigenvalues of two related, Euclidean distance matrices","If $X=\{x_1,\ldots,x_N\}$ is a set of points in $\mathbb{R}^n$ then one can generate a Euclidean distance matrix $D = [d_{ij}]$ where $d_{ij}=\Vert x_i-x_j\Vert_2^2$ is the square of the Euclidean distance from point $i$ to point $j$.  Note that $X$ is real and symmetric (all real eigenvalues). Suppose that I remove an element from $X$ to create $X^{\prime}$.  The corresponding matrix $D^{\prime}$ is related to $D$.  It is formed from $D$ by deleting the row and column corresponding to the element removed from $X$. My question is: is there any relationship between the (non-zero) eigenvalues of $D$ and the non-zero eigenvalues of $D^{\prime}$?  Certainly, counting up multiplicities, $D^{\prime}$ has one fewer eigenvalue.  It is also known that any such Euclidean Distance Matrix has (at most) $n+2$ non-zero eigenvalues; it has exactly $n+2$ non-zero eigenvalues whenever $N>n$ and all the points in $X$ do not lie in some $N-1$ dimensional subspace. EDIT: If it matters, in the case that I'm looking at, $n=3$ and $N$ is much bigger (on the order of 100s).","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1198898,The Laurent series of $1/(z^2+1)^2$ in the annulus $0<|z-i|<2$,I can't figure it out how to solve this problem: Find the Laurent Series of the function $$f(z)=\frac{1}{(z^2+1)^2}$$ valid in $A=\{z \in \mathbb{C} : 0 < |z-i|<2\}$ I think that it is impossible because we have the $-i$ singularity that restrict $A$.,"['laurent-series', 'complex-analysis']"
1198919,Theorem 1 in chapter II.4 of Mumford's Red Book,"While reading Mumford's wonderful Red Book , I arrived to a Theorem where I don't understand the proof. So Theorem 1 in chapter II.4 says Let $X_0$ be a prescheme over $k_0$ , let $X= X_0 \times_{k_0} k$ , and let $p: X \rightarrow X_0$ be the projection. Assume that $k$ is an algebraic closure of $k_0$ . Then $p$ is surjective and both open and closed. For all $x, y \in X$ , $p(x)=p(y)$ if and only if $x = \sigma_X (y)$ for some $\sigma \in$ Gal $(k/k_0)$ . Moreover, $p^{-1}(x)$ is a finite set. Ok, I understand the proof until the ""moreover statement"". There, he takes a prime ideal $P \subset R \otimes_{k_0}k$ and says that it is generated by $f_1, \ldots, f_m$ . So the question is: why can he assume that $P$ is finitely generated? Is he assuming somewhere that the prescheme is noetherian? For example, if we take $R = \Bbb Q[X_1, X_2, \ldots]$ and $P = \left<X_1 \otimes \alpha_1, X_2 \otimes \alpha_2, \ldots\right>$ , where the $\alpha_i$ 's are algebraic numbers, we can't find such a finite set. Even worse, we could make the set of all $\sigma$ 's which leave the $\alpha_{ij}$ 's fixed be just the identity, (since $\Bbb Q^{al}$ is countable) and therefore it would not be a subgroup of finite index. Thank you!","['algebraic-geometry', 'commutative-algebra']"
1198935,Uncountable subset of first uncountable ordinal set,"Given $g: \omega_1\rightarrow \omega_1$ is a function such that if $x\neq 0$, then $g(x)<x$ ($g$ is not necessarily continuous). Prove that there exists $t\in \omega_1$ such that $\ f^{-1}(t)$ is uncountable My attempt By using Pressing-down lemma, we know that there exists a stationary subset $\ K\subset \omega_1$ such that $f(K)$ is constant. Since $K$ = subset that has nonempty intersection with EVERY closed and unbounded sets in $\omega_1$, and there are uncountable such closed and unbounded sets, there must be uncountable intersections with $K$. The union of such uncountable intersections is uncountable , so $K$ must be uncountable. Can anyone help review my proof above to see if it has any error?","['ordinals', 'elementary-set-theory']"
1198941,"Fibers of a scheme over $\text{Spec}\,\mathbb{Z}$","How to construct connected scheme $X$ over $\text{Spec}\,\mathbb{Z}$ such that for $p\neq0$ the fiber $X_p=X\times_{\text{Spec}\,\mathbb{Z}}k(p)$ over the prime ideal $(p)$ contains precisely $p$ points over $\mathbb{F}_p$? The same question for $p+1$ and $p-1$.",['algebraic-geometry']
1198974,Consequences of Collatz Conjecture being true,"Collatz conjecture has been conjectured for a long time and I think there are some evidence showing that it should be true. Similar to $P \neq NP$ conjecture, is there some interesting consequence if we assume that Collatz conjecture is true? Is there some crazy consequence if we assume that the conjecture is false? Any reference would be appreciated.","['collatz-conjecture', 'number-theory', 'reference-request']"
1198976,Differential equation - Green's Theorem,"I want to find the solution of the following initial value problem: 
$$u_{tt}(x, t)-u_{xt}(x, t)=f(x, t), x \in \mathbb{R}, t>0 \\ u(x, 0)=0, x \in \mathbb{R} \\ u_t(x, 0)=0, x \in \mathbb{R}$$ using Green's theorem but I got stuck... $$$$ I found the following example in my notes: 
$$u_{tt}-c^2u_{xx}=f(x, t), x \in \mathbb{R}, t>0 \\ u(x, 0)=0, x \in \mathbb{R} \\ u_t(x, 0)=0, x \in \mathbb{R}$$ $$\iint_{\Omega}[u_{tt}(x, t)-c^2u_{xx}(x, t)]dxdt=\iint_{\Omega}f(x, t)dxdt=\int_0^{t_0} \left (\int_{x_0-ct_0+ct}^{x_0+ct_0-ct}f(x, t)dx\right )dt \tag 1$$ $$\iint_{\Omega}\left [\frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{t}}\right ]dxdt=\int_{\partial{\Omega}}Pdx+Qdt$$ $$Q(x, t)=-c^2u_x \\ P(x, t)=-u_t$$ $$\iint_{\Omega}\left [u_{tt}(x, t)-c^2u_{xx}(x, t)\right ]dxdt=\int_{\partial{\Omega}}\left [-u_t(x, t)dx-c^2u_x(x, t)dt\right ]=\int_{C_1} [ \ \ ]+\int_{C_2} [ \ \ ]+\int_{C_3} [  \ \ ]$$ $(\int_{C_1} [ \ \ ]=cu(x_0, t_0), \int_{C_2} [ \ \ ]=cu(x_0, t_0), \int_{C_3} [  \ \ ]=0)$ $$\int_{C_3}[-u_t(x, 0)dx-c^2u_x(x, 0)dt], \text{ where } u_t(x, 0)=0, u_x(x, 0)=0$$ $$C_1: x+ct=x_0+ct_0 \Rightarrow dx+cdt=0$$ 
$$\int_{C_1}(-u_tdx-c^2u_xdt=\int_{C_1}-u_t(-cdt)-c^2u_x\left (-\frac{dx}{c}\right )=\int_{C_1}cu_tdt+cu_xdx=c \int_{C_1}u_tdt+u_xdx=c\int_{C_1}du=c(u(x_0, t_0)-u(x_0+ct_0, 0))\overset{ u(x_0+ct_0, 0)=0 }{ = }cu(x_0, t_0) \ \ \ \ \ (2)$$ $$2cu(x_0, t_0)=\int_0^{t_0}\int_{x_0-ct_0+ct}^{x_0+ct_0-ct}f(x, t)dx$$ $$u(x_0, t_0)=\frac{1}{2c}\iint_{c(x_0, t_0)}f(x, t)dxdt$$ I got stuck at the following: Could you explain to me the first graph?? Why are the limits of the integral at the relation $(1)$ the following: $x_0-ct_0+ct$ and $x_0+ct_0-ct$ ?? Why does it stand at the relation $(2)$ that $u(x_0+ct_0, 0)=0$ ?? $$$$ EDIT: So, for the problem  $$u_{tt}(x, t)-u_{xt}(x, t)=f(x, t), x \in \mathbb{R}, t>0 \\ u(x, 0)=0, x \in \mathbb{R} \\ u_t(x, 0)=0, x \in \mathbb{R}$$ do we have the following?? Let $P=(x_0, t_0)$. The two characteristics are $x=x_0$ and $x+t=x_0+t_0$. The characteristics intersect the line $t=0$ at the points $A(x_0, 0)$ and $B(x_0+t_0, 0)$. So, we get the following region of influence: $$\iint_{\Omega}[u_{tt}(x, t)-u_{xt}(x, t)]dxdt=\iint_{\Omega}f(x, t)dxdt=\int_0^{t_0} \left (\int_{x_0}^{x_0+t_0-t}f(x, t)dx\right )dt$$ $$\iint_{\Omega}\left [\frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{t}}\right ]dxdt=\int_{\partial{\Omega}}Pdx+Qdt$$ $$Q(x, t)=-u_t \\ P(x, t)=-u_t$$ $$\iint_{\Omega}\left [u_{tt}(x, t)-u_{xt}(x, t)\right ]dxdt=\int_{\partial{\Omega}}\left [-u_t(x, t)dx-u_t(x, t)dt\right ]=\int_{C_1} [ \ \ ]+\int_{C_2} [ \ \ ]+\int_{C_3} [  \ \ ]$$ $$C_1: x+t=x_0+t_0 \Rightarrow dx+dt=0 \Rightarrow dx=-dt$$
$$\int_{C_1} \left [-u_t(x, t)dx-u_t(x, t)dt\right ]=\int_{C_1} \left [u_t(x, t)dt-u_t(x, t)dt\right ]=0$$ $$C_2: x=x_0 \Rightarrow dx=0$$ 
$$\int_{C_2} \left [-u_t(x, t)dx-u_t(x, t)dt\right ]= \int_{C_2} \left [-u_t(x, t)dt\right ]=-\int_{C_2} \left [du\right ]=u(x_0, t_0)-u(x_0, 0)=u(x_0, t_0)$$ $$C_3: t=0 \Rightarrow dt=0$$ 
$$\int_{C_3} \left [-u_t(x, 0)dx-u_t(x, 0)dt\right ]=0$$ So, we have $$u(x_0, t_0)=\int_0^{t_0} \left (\int_{x_0}^{x_0+t_0-t}f(x, t)dx\right )dt$$ Is this correct?? Could I improve something??","['greens-theorem', 'ordinary-differential-equations', 'partial-differential-equations']"
