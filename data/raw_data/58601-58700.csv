question_id,title,body,tags
646296,Why can matrix exponentiation be done by squaring?,"Matrix multiplication is not communative: A*B != B*A Then why can matrix exponentiation be done by squaring? I have tried searching for special cases where this rule did not apply, but from what I've understood, none of it seemed to apply specifically to powers of two equal matrices. For example a matrix M is raised to a 12, then which of the following is true? M 4 * M 8 = M 12 OR M 8 * M 4 = M 12 ? Do we need to follow a particular order while calculating matrix exponentiation by squaring ? If yes then how is the order determined and why does it lead to the correct exponentiation. If not then why doesn't the fact that matrix multiplication is not communative does not affect it? Can this be explained this to me in plain english? Thanks in advance.","['matrices', 'exponentiation']"
646297,Finding $ \lim_{x \to 0^+} (\frac{\arctan (x)}{x})^{\frac{1}{x^2}} $ without power series.,"I have to find: $\lim_{x \to 0^+} (\frac{\arctan (x)}{x})^{\frac{1}{x^2}}$ ""Minimized"" the problem to finding: $ \lim_{x \to 0^+} \frac {\ln(\frac{\arctan x}{x})} {x^2}
$ L'Hôpital's rule once is not enough, second L'Hôpital's rule seems worse. any help? Edit: I cannot use power series expansion. that is provided on the next semester.","['calculus', 'real-analysis', 'limits']"
646319,"prove that $(n^2)!$ is divisible by $(n!)^{n+1}$, where $n\in \mathbb{N}$","(1) How can we prove that $(kn)!$ is divisible by $(n!)^k$, where $n,k\in \mathbb{N}$ (2) How can we prove that $(n^2)!$ is divisible by $(n!)^{n+1}$, where $n\in \mathbb{N}$ (3) How can we prove that $(k!)!$ is divisible by $(k!)^{(k-1)!}$, where $k\in \mathbb{N}$ $\bf{My\; Try}::(1) $ We know that product of $n$ consecutive integer is divisible by $n!$ So Here  $1.2.3.4........................................n$ is divisible by $n!$ Similarly $(n+1)\cdot(n+2)\cdot(n+3)...........(2n)$ is divisible by $n!$ Similarly $(2n+1)\cdot (2n+2)\cdot ..............(3n)$ is divisible by $n!$ Similarly $(3n+1)\cdot(3n+2)......................(4n)$ is divisible by $n!$ ............................................................. ............................................................. Similarly $\{(k-1)n+1\}\cdot \{(k-1)n+2\}\cdot..................\{(k-1)n+n\}$ is divisible by $n!$ So we can say that $(kn)!$ is divisible by $(n!)^k$ Now I did not understand how can i  solve $(II)$ one and $(III)$ one Help Required Thanks",['combinatorics']
646359,"Find the limits of ""Almost Divergent"" Series","Find the following limits:
$ \lim_{\varepsilon\rightarrow 0}\sum_{n=0}^{+\infty}\frac{(-1)^n}{1+n\epsilon} $ $ \lim_{\varepsilon\rightarrow 0}\sum_{n=0}^{+\infty}\frac{(-1)^n}{1+n^{2}\epsilon} $",['sequences-and-series']
646375,Wald's equation example controversy,"I'm trying to get a grip of Wald's equation , applying it to the following example. Suppose, we have a simple sequence of fair coin flips, where heads wins us a dollar, while tails means loss of a dollar: $$\mathbb{P}(X_i=1)=\frac{1}{2}, \mathbb{P}(X_i=-1)=\frac{1}{2}$$ Suppose, that we're planning to gamble, tossing the coin until we win 3 dollars, that's our condition on stopping time N: $$S_N = \sum_{i=1}^{N}X_N = 3$$ According to Wald's equation $$E(S_N) = E(X_i) \cdot E(N)$$ As we know, expectation of our fortune at stopping time is $E(S_N) = 3$, expectation of  a fair coin is zero: $E(X_i) = 0$, so I thought that Expectation of the stopping time $E(N)$ should grow to infinity. But seemingly it doesn't. Our process is described by the following Markov chain: $$
\begin{pmatrix}
\mathbb{P}(S_{i+1}=2) \\
\mathbb{P}(S_{i+1}=1) \\
\mathbb{P}(S_{i+1}=0) \\
\mathbb{P}(S_{i+1}=-1) \\
\mathbb{P}(S_{i+1}=-2) \\
\mathbb{P}(S_{i+1}=-3) \\
\dots
\end{pmatrix}
=
\begin{pmatrix}
0 & 0.5 & 0 & 0 & 0 & 0 & \dots\\
0.5 & 0 & 0.5 & 0 & 0 & 0 & \dots\\
0 & 0.5 & 0 & 0.5 & 0 & 0 & \dots\\
0 & 0 & 0.5 & 0 & 0.5 & 0 & \dots\\
0 & 0 & 0 & 0.5 & 0 & 0.5 & \dots\\
0 & 0 & 0 & 0 & 0.5 & 0 & \dots\\
0 & 0 & 0 & 0 & 0 & 0.5 & \dots\\
\dots & \dots & \dots & \dots & \dots & \dots & \dots
 \end{pmatrix} \cdot 
\begin{pmatrix}
\mathbb{P}(S_i=2) \\
\mathbb{P}(S_i=1) \\
\mathbb{P}(S_i=0) \\
\mathbb{P}(S_i=-1) \\
\mathbb{P}(S_i=-2) \\
\mathbb{P}(S_i=-3) \\
\dots
\end{pmatrix}
$$ Here each vector of $S_i(x)$ is infinite ($x \in (-\infty, 2]$, $x \in \mathbb{Z}$), but we can set a highly improbable lower bound (say, -20$) and the resulting 23x23 matrix will approximate our process well. We will calculate the Eigenvalues and Eigenvectors of that matrix to calculate the expected stopping time. The probability of our fortune to be e.g. in state -1 dollar at step $i$ is approximated by $$\mathbb{P}(S_i = -1) = C \cdot \lambda^i \cdot V(-1)$$ where $\lambda$ is the main eigenvalue, $C$ is its quotient in eigenvalue decomposition and $V(-1)$ is the coordinate of main eigenvector, corresponding to $\mathbb{P}(S_i = -1)$. The probability of stopping at the moment of time $i$ is $\mathbb{P}(S_{i}=3) = \mathbb{P}(S_{i-1}=2) \cdot 0.5$, thus expectation of stopping time $$E(N) \approx (1 + 2 \cdot \lambda + 3 \cdot \lambda^2 + 4 \cdot \lambda^3 + \dots) \cdot C \cdot V(2) \cdot 0.5 = \frac{d(\lambda + \lambda^2 + \lambda^3 + \dots)}{d\lambda} \cdot C \cdot V(2) \cdot 0.5 = \frac{d(\frac{\lambda}{1-\lambda})}{d\lambda} \cdot C \cdot V(2) \cdot 0.5 = \frac{1}{(1-\lambda)^2} \cdot C \cdot V(2) \cdot 0.5 < \infty$$ So, $E(S_N) = E(X_i) \cdot E(N)$ means 3 = 0 * C, which is wrong. Can you see, what's wrong?","['probability-theory', 'stochastic-processes', 'markov-chains', 'stopping-times']"
646386,The dual of $L^1(G)$ for a locally compact group $G$,"I might be missing something, but most literature on topological groups and harmonic analysis that I've encountered mention that $L^\infty(G)$ can be naturally identified with the dual of $L^1(G)$ by means of the isomorphism $$\begin{array}{ccc}L^\infty(G)&\longrightarrow&L^1(G)^*\\f&\mapsto&\int fg\, \mathrm d\mu\end{array}$$ where $\mu$ is a fixed Haar measure. However, when dealing with measure spaces $(X,\mu)$ in general, a lot of literature also states that the above map is an isomorphism of $L^\infty(X)$ onto $L^1(X)$ if the measure $\mu$ is $\sigma$ -finite. Are Haar measures always $\sigma$ -finite (I would think not), and if not, then why does so much literature insist that the above identification works?","['topological-groups', 'measure-theory', 'harmonic-analysis', 'functional-analysis', 'locally-compact-groups']"
646389,Is Gradient really the direction of steepest ascent?,"I want to intuitively understand why the gradient gives you the direction of the steepest ascent of a function. Apart from the already posted questions, my confusion arises from the fact that we form the gradient vector from the derivative of each dimension separately . Then take the vector consisting of both (for 2D) derivatives take it as the steepest ascent. What if in both directions the derivative is say $5$, so our vector will be $45$ degrees from both axis, But in that direction specifically the function goes down ? If it's not clear what I'm confused with, consider this function represented as an image : $$ \begin{pmatrix}100&5&-100\\0&\textit{0}&5\\0& 0&   0\end{pmatrix}$$ at 0 , it makes sense that the derivative is $5$ in $x$ and in $y$, but a vector of $(5,5)$ goes to a direction that's not a steepest ascent. Does this have to do with the differentiability of the function ? what am I missing ?",['multivariable-calculus']
646391,Frobenius actions and Frobenius groups.,"I am trying to solve the following exercise (this is the exercise 248 from John Rose's A Course on Group Theory): Let $G$ be a finite group. We make the following definitions: (a) Suppose that $G$ acts on the set $X$. The action is said to be a Frobenius action if it is transitive and there exists at least an element $x\in X$ with a nontrivial stabiliser, $|X|>1$, and whenever $x_1, x_2$ are distinct elements of $X$, $\operatorname{Stab}_G(x_1)\cap \operatorname{Stab}_G(x_2)=1$. (b) $G$ is said to be a Frobenius group if it has a nontrivial proper subgroup $H$ such that $N_G(H)=H$ and whenever $H^{g_1}, H^{g_2}$ are distinct conjugates of $H$ in $G$ (with $g_1, g_2\in G$), $H^{g_1}\cap H^{g_2}=1$. Any such subgroup is called a Frobenius complement in $G$. Prove the following statements: (i) $G$ has a Frobenius action on some set if and only if $G$ is a Frobenius group. (ii) If $G$ is a Frobenius group and $H$ is a Frobenius complement in $G$ then $|G:H|\equiv 1 \mod |H|$. (iii) If $G$ is a Frobenius group then $Z(G)=1$. (iv) Let $n$ be a positive integer, and consider the natural action of $S_n$ on the set $\{1,...,n\}$. This is a Frobenius action if and only if $n=3$. (v) Let $n$ be an integer, $n\geq 3$. Then $D_{2n}$ is a Frobenius group if and only if $n$ is odd. For the first question, I think I need to use the following fact: let $X$ be a transitive $G$-space, $Y$ a transitive $H$-space. If $(G,X)\cong (H,Y)$ then for any $x\in X, y\in Y$, $\operatorname{Stab}_G(x)\cong \operatorname{Stab}_H(y)$. My attempts at solution: (i) $\Rightarrow$ Suppose that $G$ has a Frobenius action on the set $X$. Let $x\in X$ be an element with a nontrivial stabiliser, this is my candidate for a Frobenius complement. Denote $H=\operatorname{Stab}_G(x)$, now consider two distinct conjugates $H^{g_1}, H^{g_2}$. Note that $H^{g_1}=\operatorname{Stab}_G(g_1\cdot x)$, this is due to the fact that every element in $H^{g_1}$ has the form $g_1 h g^{-1}$ for $h\in H$ which stabilises $x$, now consider the action
$$
g_1 h g^{-1}_1\cdot (g_1 \cdot x)=g_1 h\cdot (g_{1}^{-1}g\cdot x)=g_1 \cdot (h\cdot x)=g_1 \cdot x
$$
Similarly, $H^{g_2}=\operatorname{Stab}_G(g_2\cdot x)$. From there I think I need to show that $g_1\cdot x\neq g_2\cdot x$, so that the stabilisers intersect trivially and therefore $H^{g_1}\cap H^{g_2}=1$. For the normaliser, we always have that $H\subseteq N_G(H)$, if $n\in N_G(H)$ then $nhn^{-1}\in H$ for every $h\in H$. Now, $nhn^{-1}\in \operatorname{Stab}_G(n\cdot x)$. We either get that $\operatorname{Stab}_G(x)=\operatorname{Stab}_G(n\cdot x)$ or $\operatorname{Stab}_G(x)\cap \operatorname{Stab}_G(n\cdot x)=1$, I think that the second case is not possible, so we only get the first case. Not sure where to from here. $\Leftarrow$ Now suppose that $G$ is a Frobenius group with a Frobenius complement $H$. Consider the action of $G$ on the set $G/H$, it is transitive and $H$ is an element with nontrivial stabiliser. I am not sure how to show the part about stabilisers. From then, I want to show that every Frobenius action is equivalent to the action defined above, so it is enough to consider only this action. (ii) I am not sure what to do here, maybe count the orbits under the action? (iii) I think the previous exercise needs to be used here, but I am not sure how exactly. (iv) In case $n=4$, we get that $\begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 1 & 3 & 4 \end{pmatrix}\in \operatorname{Stab}_{S_4}(3)\cap\operatorname{Stab}_{S_4}(4)$, this counterexample works for $n\geq 4$ essentially. If $n=3$, then this can be done by computing stabilisers for all three elements. (v) I know that $Z(D_{2n})=1$ only for $n$ odd, but I am not so sure how to go about the other direction. If $n$ is odd, how do I show that $D_{2n}$ is Frobenius?","['group-theory', 'abstract-algebra']"
646399,Symmetry in partial derivatives.,"I was wondering how the relationship $$x_j \partial_i f(x) = x_i \partial_j f(x)$$ means, that a function has rotational symmetry? I mean with rotational symmetric, that the value of $f$ at a point $x$ depends only on $\|x\|_2$. Or more precisely: Is there any $C_1(\mathbb{R}^n \backslash \{0\}; \mathbb{R})$ function, that has no rotational symmetry, but fulfills this equation for all $x \in \mathbb{R}^n\backslash\{0\}$?","['calculus', 'partial-differential-equations', 'real-analysis', 'analysis', 'derivatives']"
646409,What is the group of $k$-rational points of an algebraic group?,"Let $k$ be a field and $G$ a linear algebraic group over $k$. What is the group of $k$-rational points of $G$? By definition, $G$ is an algebraic variety. Suppose that $G$ is defined by polynomials $f_1, \ldots, f_n$. Then the set of $k$-rational points of $G$ is $A=\{a=(a_1, \ldots, a_n)\in k^n: f_1(a)=\ldots = f_m(a)=0\}$. Is $A = G$? In $GL_n(k)$ case, what are the group of $k$-rational points of $GL_n(k)$? Thank you very much.","['algebraic-geometry', 'algebraic-groups']"
646419,What is the maximum number of pieces that a pizza can be cut into by 7 knife cuts? (NBHM 2005),I am seeing this question very first time and do not know any formal way to solve it. Which part of mathematics it is related to? What is the maximum number of pieces that a pizza can be cut into by 7 knife cuts? A pizza is usually round shaped. So I was cutting a circle into pieces by seven straight lines. Minimum number is eight. But no maximum I am getting. It is better to add complete answer with references. Thank you for your help.,"['geometry', 'discrete-mathematics', 'contest-math', 'combinatorial-geometry', 'reference-request']"
646420,Find the Vector Equation of a line perpendicular to the plane.,"Question: Find the vector equation $r(t)$ for the line through the point $P = (-1, -5, 2)$ that is perpendicular to the plane $1 x  - 5 y + 1 z = 1$.
Use $t$ as your variable, $t = 0$ should correspond to $P$, and the velocity vector of the line should be the same as the standard normal vector of the plane. This one is really giving me a hard time.  I know that to find the plane perpendicular to the line I can use the vector n between two points on the line and and the plane.  I cannot wrap my mind around how to reverse this process, particularly because the plane is equal to 1 and not zero. Any help would be greatly appreciated.","['calculus', 'vector-analysis']"
646427,Evaluating Lebesgue integral,"What techniques are there to evaluate the Lebesgue integral of an integrable function $f$ on the whole $\mathbb{R}$? I am following a class on Measure Theory and these questions make my head go blank, whenever there is not a obvious sequence of simple functions which converges to $f$ I am clueless. The convergence theorems are known, but is this the only possible route of attack? For example: $\int_{\mathbb{R}}\left|2ye^{-y^2}\right|dy$","['lebesgue-integral', 'measure-theory']"
646445,Probability with Calculus,"A point is chosen randomly in the region bounded by the curve $y = x^2$ and the line $y = 4$. Find the probability that the $y$-coordinate is less than $a$ for any $a$ in[0,4]. I think that I need to use calculus to find the area under the curve. However, since $y$ is bounded from 0 to 4, it seems like the answer should be $\frac{1}{2}$. Please help guide me in the right direction to get started, even if my intuition is correct.","['calculus', 'probability']"
646476,Shift Operator not continuous,"Let $X=L^1(\mathbb{R}^n)$ and $T:\mathbb{R} \rightarrow L(X)$, such that $(T(\tau)f)(t):=f(t+\tau)$. The question is: Is $T$ continuous? Well, my idea was the following: $|\tau_1-\tau_2| \le \delta \rightarrow ||T(\tau_1)-T(\tau_2)|| = sup_{f,||f||=1} \int |f(t+\tau_1) - f(t+\tau_2)|$ But the problem is: We could take any function with compact support(for example $[0,1]$), such that on intervals of length $|I|=\frac{\delta}{2}$, $f$ would be +1 on $[0,\frac{\delta}{2}]$ and negative one on $[\frac{\delta}{2},\delta]$ and so on. So the integral could become arbitrarily large. So I guess the shift-operator $T$ is not continuous, is this right?","['calculus', 'integration', 'real-analysis', 'analysis', 'lebesgue-integral']"
646477,Hyperplane sections on projective surfaces,"I am studying Beauville's book ""Complex Algebraic Surfaces"".
At page 2 he defines the intersection form (.) on the Picard group of a surface.
For $L, L^\prime \in Pic(S)$ $$(L.L^\prime)=\chi(\mathcal{O}_S)-\chi(L^{-1})-\chi(L^{\prime-1})+\chi(L^{-1}\otimes L^{\prime-1})$$ Why the self-intersection (i.e $(H.H)=H^2$) of an hyperplane section $H$ on $S$ is always positive?","['intersection-theory', 'algebraic-geometry', 'surfaces']"
646527,Measure extension via inner measure,"Is it possible to use ""inner measure"" in the proof of the Caratheodory extension theorem ? I'm trying to understand why we prefer outer one instead, although definitions of outer and inner measures are dual. N. B.: I mean $\mu_*(A) = \sup \{\sum \mu(E_i) | E_i\mbox{ disjoint}, E_i \subset A\}$ is the ""inner measure,"" generated by some pre-measure $\mu$, defined on some ring $\mathcal R$. It's not in general use. So let me explain the problem: We can proof superadditivity of $\mu_*$ defined above. Then we consider $\mathcal M (\mathcal R) = \{A \;|\; \forall E  \in \mathcal R \;\;\mu_*(E\cup A) + \mu_*(E\setminus A) = \mu_*(E)\}.$ Now it's not hard to show that $\mathcal M(\mathcal R)$ closed under finite union and completion. But I can't prove that $\mathcal M(\mathcal R)$ closed under countable union. Maybe it's not true in general? What is the difference in such symmetrical concepts of inner in outer measure?",['measure-theory']
646558,Second order homogeneous differential equation with non-constant coefficients,"How can I solve a 2nd order differential equation with non-constant coefficients like the following? $$ty''-(t+1)y'+y=0$$ If I'm not wrong, I have only seen methods (apart from the reduction of order method) for finding a solution when the coefficients are constant. How can I do this?",['ordinary-differential-equations']
646569,Expected Value - Uniform distribution over infinite interval,"Question: The probability that an error is introduced into a packet is $\alpha$. Messages, consisting of one or more packets, are received at a node. Given that a message has been received free of errors, what is the expected number of packets in it? Assume that packets are never lost and that messages of all lengths are equally likely. I know I need to find the expected value and use the uniform distribution, but since it could range from length = 1 to infinity I'm unsure.  What I'm starting with:
$$E[X]=\sum_{k=1}^\infty\alpha^kf_X(k)$$ Since its a uniform distribution I thought $f_X(k)$ would be a constant so I have:
$$E[X]=f_X(k)\big(\sum_{k=1}^\infty\alpha^k\big)$$
$$E[X]=f_X(k)*\frac{\alpha}{1-\alpha}$$ Any hints on what to do next (how to compute the pdf when infinity is involved) or corrections would be appreciated.","['uniform-distribution', 'probability']"
646582,How to find a center/axis of rotation?,"I have a 3d model (M1) consisted of several points. I know all their coordinates. I also have another model (M2). M2 and M1 are the same, but M2 is a model after rigid transformation. I don't know the axis or the center of transformation. The only thing I know is the type of transformation - translation or rotatation. So my question is - how can I find the center or the axis of rotation, and the the value of angle?",['geometry']
646584,"If $A,B,C$ are the angle of a triangle, then show that $\sin A+\sin B-\cos C\le \dfrac3 2$","If $A,B,C$ are the angle of a triangle, then show that $\sin A+\sin B-\cos C\le \dfrac32$ I tried substituting $C=180^\circ-(A+B)$ and got stuck. I also tried using the formula $\sin A+\sin B=2\sin \frac{A+B}{2}\cos \frac{A-B}{2}$ without any progress. Please help!","['geometry', 'triangles', 'trigonometry']"
646610,Determine if $\sum_{n=1}^{\infty}\frac{(-1)^nn^2+n}{n^3+1}$ converges or diverges.,"Another series I found I'm struggling with. Determine if the following series converges or diverges.$$\sum_{n=1}^{\infty}\frac{(-1)^nn^2+n}{n^3+1}$$ Ratio test and n-th root test are both inconclusive, Leibniz - criterion cannot be applied since the sequence given is not in the form of $(-1)^na_n$. I am sure the problem can be solved with the limit comparison test, though, the $n^{(...)}$ look pretty inviting after all. Let $a_n:=\frac{(-1)^nn^2+n}{n^3+1}$ then $|a_n|= \frac{n^2+n}{n^3+1}$. A try showing that the series diverges using the divergence of $\sum\frac{1}{n}$. For $n≥1$ it is clear that
$$ \frac{n^3+n^2}{n^3+1} ≥ 1 $$ dividing by $n$ yields:
$$ \frac{n^2+n}{n^3+1} = |a_n| ≥ \frac{1}{n}$$
Thus the series diverges. (?) EDIT: Hints you gave me yield:
$$\sum_{n=1}^{\infty}\frac{(-1)^nn^2+n}{n^3+1} = \sum_{n=1}^{\infty}(-1)^n\frac{n^2}{n^3+1}  +
\sum_{n=1}^{\infty}\frac{n}{n^3+1}$$ With the first series converging by Leibniz-theorem and the second by limit comparison test with $\frac{1}{n^2}$.","['convergence-divergence', 'sequences-and-series', 'summation', 'real-analysis']"
646643,maxima and minima of 2 variable function,"How can I show that $f(x,y)=e^x cos(y)$ doesn't have maxima nor minima in the unit circle? Because $f_x = f_{xx} =0$ when $x=0$ and $y=\frac{\pi}{2} +n\pi, n\in \Bbb{Z}$. and isn't $(0,\frac{\pi}{2},0)$ in the unit circle? if we try to find the maximas or minimas with derivates. $$f_{y}=-\frac{e^{x+iy}-e^{x-iy}}{2i}$$ And when $x,y=0\to f_y(x,y)=0$. isn't $(0,0,0)$ in the circle..?",['multivariable-calculus']
646705,Counting integer partitions of n into exactly k distinct parts size at most M,"How can I find the number of partitions of $n$ into exactly $k$ distinct parts, where each part is at most $M$? The number of partitions $p_k(\leq M,n)$ of $n$ into at most $k$ parts, each of size at most $M$, is given by the generating function:
$$
 \binom{M+k}{k}_{x} = \prod_{j=1}^{k}\frac{1-x^{M+k-j+1}}{1-x^j}= \sum_{n=0}^{kM} p_{k}(\leq M,n) x^n
$$ For the number of the partitions $p_k(\mathcal{D},n)$ of $n$ into at most $k$ parts there is the recurrence relationship:
$$
  p_{k}(\mathcal{D},n) = p_{k}(\mathcal{D},n-k) + p_{k-1}(\mathcal{D},n)
$$ But what, if I want to count only the partitions with distinct parts and restricted number of parts and restricted part size? Update : Now I know the generating function for the number of distinct restricted partitions $p_k(\leq M, \mathcal{D},n)$ of $n$ into exactly $k$ distinct parts, all at most $M$ is
$$
   \prod_{j=1}^{M} (1+xq^{j}) = \sum_{k,n=0}^{\infty}p_k(\leq M, \mathcal{D},n)x^{k}q^{n}
$$
and there is also a recurrence relation
$$
   p_k(\leq M, \mathcal{D},n) =    
    p_{k-1}(\leq M-1, \mathcal{D},n-k) +    
    p_k(\leq M-1, \mathcal{D},n-k)
$$
How can I prove this? Could you recommend a book, where I could read about this?","['integer-partitions', 'generating-functions', 'combinatorics']"
646715,Solve $ax^t = by^t$ for $ t$,I use to know how to do this but a friend of mine asked for help and I cannot remember. Can anyone help me solve $ax^t = by^t$ for $t$?,['algebra-precalculus']
646777,"Any positive linear functional $\phi$ on $\ell^\infty$ is a bounded linear operator and has $\|\phi \| = \phi((1,1,...)) $","This is a small exercise that I just can't seem to figure out. When I see it I'll probably go 'ahhh!', but so far I haven't made any progress. I'd like to prove that any linear functional $\phi$ on $\ell^\infty$ that is positive, i.e. for any $(x_n) \in \ell^\infty$ such that $x_n \geq 0$ for all $n \in \mathbb{N}$ we have $\phi((x_n)) \geq 0$, is a bounded linear operator and has $\|\phi \| = \phi((1,1,...))$. Nothing I have tried worked so far, so any hints would be appreciated, thanks!","['normed-spaces', 'functional-analysis', 'lp-spaces']"
646780,How to tell if two 3D vectors are in the same direction?,Given: $$AB=\left( \begin{array}{ccc}2\\1\\3\end{array} \right) \;\;\;\; \text{and}\;\;\;\; CD=\left( \begin{array}{ccc}4\\3\\6\end{array} \right).$$ Justify if $AB$ has the same direction as $CD$ -- give the explanation.,"['linear-algebra', 'vectors']"
646826,Jordan's decomposition,"I have a matrix $A\in R^{n,n}$. $A=
\begin{bmatrix}
1&0&-2&0&0&\dots&0\\
0&1&0&-6&0&\dots&0\\
0&0&1&0&-12&\dots&0\\
\vdots&\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\
0&0&0&0&1&0&-(n-2)(n-1)\\
0&0&0&0&0&1&0\\
0&0&0&0&0&0&1
\end{bmatrix}$ $A$ is a matrix with $1$ on diagonal and two spots to the right is $-i(i+1)$, where $i$ is the line number. It has a characteristic polynomial $p_A(t)=(1-t)^n$. So jordan's form of this matrix has $1$ on diagonal, and $0$ or $1$ above it. EDIT: $J=
\begin{bmatrix}
J_1 & 0 & 0 &\dots& 0\\
0&J_2&0&\dots&0\\
\vdots & \vdots &\ddots & \dots &0\\
0&0&0&\dots&J_n
\end{bmatrix}$ Where 
$J_i=
\begin{bmatrix}
1 & 1 & 0&0 & \dots & 0\\
0 &1 &1&0&\dots&0\\
0&0&1&1&\dots&0\\
\vdots&\vdots&\dots&\ddots&\ddots&\vdots\\
0&0&0&0&1&1\\
0&0&0&0&0&1
\end{bmatrix}$ How do I know how many blocks $J_i$(and of what size) are used in $J$ matrix?","['jordan-normal-form', 'matrices', 'linear-algebra']"
646835,Difference between graded ring and graded algebra,"Wikipedia says that a graded $A$-algebra is just a graded $A$-module that is also a graded ring. Question: when one says then ""finitely generated graded $A$-algebra"", does one mean that every element $s$ can be written as $s=\sum^N_{i=0} a_i g_i$, where $\{g_i\}_{i=1}^N$ is a generating set, or that it can be written as $s=\sum^N_{i=0} a_i \prod_{j\in I} g_j$? Ravi Vakil proposes in an exercise: 4.5.D. (a) Show that a graded ring $S_\bullet$ over $A$ is a finitely generated graded ring (over $A$) iff $S_\bullet$ is a finitely generated graded $A$-algebra, i.e., generated over $A=S_0$ by a finite number of homogeneous elements of positive degree. What I don't understand is how putting the word ""algebra"" instead of ""ring"" makes the difference that the former is supposed to mean generated over homogeneous elements and the latter by ""any elements"". Doesn't, in general, an $A$-algebra simply mean a ring over $A$, i.e. a ring into which $A$ maps (possibly not injectively), so that an $A$-action is defined?","['algebraic-geometry', 'graded-modules', 'commutative-algebra', 'terminology', 'graded-rings']"
646856,Commutativity of integration and Taylor expansion of the integrand in an integral,"I am baffled with a seemingly a straightforward problem. Suppose we are given the following integral: \begin{equation}
 f(a)\,=\,\int_{0}^{\infty} \frac{x^4}{x^4+a^4} e^{-x},
\end{equation}
and we want to determine the dependence of $f(a)$ on $a$ when $a\ll 1$. Apparently this integral can be solved using Mathematica. Taylor expanding the result, which is a Meijer G-function, it turns out that $f(a)$ is analytic in $a$. In the specific case of this integral, it's possible to use a trick so that one can directly Taylor expand the integrand (Taylor expanding the integrand of $f(a)-f(0)$ after $x\to x'=a x$). But I'm not interested in this particular integral and am mentioning this as a simple example. Now here is what I find paradoxical: Let's try to do this in a more pedestrian way by breaking up the integration range and Taylor expanding the exponential when x is small and the rest of the integrand when x is large. Interchanging the integration and summation is justified by Fubini's theorem (if I'm not mistaken, $\int \sum |c_n(x)| <\infty$ or $\sum\int |c_n(x)|<\infty$). Now, breaking up the integral can be done in two ways. Either, \begin{equation}
f(a)=\int_{0}^{1} \frac{x^4}{x^4+a^4} e^{-x} + \int_{1}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,,
\end{equation}
or \begin{equation}
f(a)=\int_{0}^{2a} \frac{x^4}{x^4+a^4} e^{-x} + \int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,.
\end{equation}
$\frac{x^4}{x^4+a^4}$ can be Taylor expanded and the integration ranges are within the convergence radius in both cases. The Taylor expansion in both cases results in a series that's uniformly convergent and therefore one should be able to interchange integration and summation. The former case, where the integration range is broken up at $1$, gives an analytic result in $a$. Curiously, the latter (breaking up the integral at $2a$) gives non-analytic terms (see below) and I cannot figure out how to reconcile this with the exact result. The lower integration ranges in both cases give analytic expressions in $a$. \begin{equation}
\int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,=\, \sum_{n=0}^{\infty} \int_{2a}^{\infty} \frac{(-1)^n a^{4n}}{x^{4n}}e^{-x} \,=\, \sum_{n=0}^{\infty}(-1)^{n}a^{4n}\Gamma(1-4n,2a).
\end{equation}
Using the series expansion of the upper incomplete $\Gamma$-function, there will be terms of the form $\frac{-(-1)^n}{(4n-1)!} a^{4n} \ln(a)$. I would like to know whether the Taylor expansion is not justified (if so, why precisely), or, although hard to imagine, is it that somehow these non-analytic terms sum up to an analytic result. Thanks.","['analyticity', 'real-analysis', 'analysis']"
646902,estimate the max of a function using p-norm of the function and its derivative,"Prove the following interesting, elementary but tricky problem :
Let f be a smooth function defined on the interval $[a,b]$ and $p\in[1,\infty]$. Then there exists a constant $C_p$ such that for any $\mu\gt0$,$\max|f|\le C_p(|f(a)|+\mu^{1/p-1}||f'||_p+\mu^{1/p}||f||_p)$. There is a hint for this exercise: using fundamental theorem of calculus and Holder inequality. I started this problem by writing f(t) as f(a)+$\int_a^tf'(x)dx$. Then I can use Holder to get the $p$-norm of the derivative $f'$. However, I don't know how to relate this expression with the $p$-norm of f and $\mu$.","['lebesgue-measure', 'functional-analysis', 'real-analysis']"
646912,determinant of the Fubini-Study metric,"Is there any easy way to compute the determinant of the Fubini-Study metric, given by: $g_{\alpha\bar{\beta}}=\frac{1}{1+\bar{z}z}\left(\delta_{\alpha\bar{\beta}}-\frac{\bar{z}_\alpha z_{\bar{\beta}}}{1+\bar{z}z}\right)$ ? I've tried using the relation of the determinant and the Levi-Civita connections $\Gamma^\gamma_{\bar{\beta}\gamma}$ (since my goal at the end would be to compute the Ricci curvature tensor $R_{\alpha\bar{\beta}}$). The Ricci curvature tensor is given by: $R_{\alpha\bar{\beta}}=-\partial_\alpha\partial_{\bar{\beta}}\left(\log(\det g_{\gamma\bar{\delta}})\right)=-\partial_\alpha\left(g^{\gamma\bar{\delta}}\partial_\bar{\beta}g_{\gamma\bar{\delta}}\right).$ Where I got the last equality by using the Levi-Civita connection do solve the determinant. Now I'm still having problems with the calculations of the last term, any hints on that one ? Any help would be most welcome! A solution to the determinant would be most helpfull since I think I could compare this with the Kähler-potential given by $K(z,\bar{z})=\log(1+z\bar{z})$, for which we have that $g_{\alpha\bar{\beta}}=\partial_\alpha\partial_{\bar{\beta}}K(z,\bar{z})$. Edit, since I've seen people asking this question before. I'm working on a complex manifold with coordinates $z^a=\{z^\alpha,\bar{z}^\bar{\alpha}\}$, where $a=1,...,2n$ and $\alpha=1,...,n$. So when I write Greek indices, I've split up the 2n linear independent (denoted by an $a$) into two groups, the holomorphic (denoted by $\alpha$) and anti-holomorphic (denoted by $\bar{\alpha}$) coordinates.","['metric-spaces', 'kahler-manifolds', 'differential-geometry', 'determinant']"
646919,$n^{th}$ root of a matrix.,What conditions do I need on a matrix $A$ in order to know an $n^{th}$ root exists. In other words there is a matrix $B$ such that $B^n=A$ for $n \in \mathbb{Z}^+$.,"['matrices', 'linear-algebra']"
646929,Revolve a 3D shape around an axis to create a 4D shape (and so on and so fourth),"You can revolve a 2 dimensional shape around an axis to make it a 3 dimensional shape, and finding the volume of this shape is pretty simple using the disk method. What I want to know is if it is viable to use a similar method to find the space taken up by a 4 dimensional shape created by revolving a 3 dimensional shape about an axis. Then would I be able to revolve that shape to create a 4 dimensional shape? What form would the equation need to take if it is possible? The shapes I want to do this with have been obtained using the disk method out of a 2D shape, so they are infinitely rotationally symmetrical about an axis (let's go with the y axis). Basically what I want to do is take a 2D shape and revolve it into n dimensions. As an example we can take a circle going to a sphere going to a 4-sphere going to a 5-sphere and so on. Is this possible or am I thinking madness?","['geometry', 'calculus']"
646980,Question on Nakayama?,"In reading a certain proof on the stacks project "" http://stacks.math.columbia.edu/tag/00NV "", I can't see how Nakayama's lemma is used to make the following conclusion: ""Assume M is finitely presented and flat.......Pick any prime p and $x_1,…,x_r\in M$ which map to a basis of M⊗Rκ(p). By Nakayama's Lemma these elements generate $M_g$ for some g∈R, g∉p. ""","['algebraic-geometry', 'projective-module', 'abstract-algebra', 'modules', 'commutative-algebra']"
646984,Mathematical Induction of $\frac{n(n-1)(n-2)}{6}$,"Show that the number of triples that can be chosen from $n$ items is precisely $$\frac{n(n−1)(n−2)}{6}.$$ Suppose n = k+1, We want $\frac{(k+1)k(k-1)}{6}$ therfore, $\frac{k(k-1)(k-2)}{6}$ + (k+1) and then solve the rest. What am I doing wrong here?",['discrete-mathematics']
646995,Show that in a quasi-compact scheme every point has a closed point in its closure,"Vakil 5.1 E Show that in a quasi-compact scheme every point has a closed point in its closure Solution:
Let $X$ be a quasi-compact scheme so that it has a finite cover by open affines $U_i$. Let $z \in X$, and $\bar z$ its closure. Consider the (finite) sub-collection of $\{U_i\}_{i=1}^N$ that intersect $\bar z$. Because $U_1$ is just the spec of a ring, we can pick a closed point $z_1 \in \bar z \cap U_1$. If $z_1$ is also closed in all other $U_i$ that contain it, we're done, but if it is not closed in some $U_i$ then it is not closed in $X$. However, in that case, we can pick another $z_i \in \bar {z_1} \cap U_i$. Certainly, $z_i$ does not lie in $U_1$, because if it did $z_1$ wouldn't have been a closed point in $U_1$ in the first place. Now we proceed in the obvious way until we get to a point $z_n$ that is closed in all the open affines that contain it. Notice that this procedure terminates because once we move from $z_i$ to $z_{i+1}$ we can no longer have $z_{i+1}$ contained in any open affine where some $z_{j\le i}$ was closed, because if we did then $z_{i+1} \in \bar{z_j} \cap U_j = \{z_j\}$.","['algebraic-geometry', 'schemes', 'solution-verification']"
647025,Why $\frac{|1-z|}{1-|z|}\le K$ corresponds to the region defined by the Stolz angle?,"In his presentation of Abel's theorem , Ahlfors mentions that for a fixed positive number $K$, the region defined by \begin{equation}
\frac{|1-z|}{1-|z|}\le K
\end{equation} corresponds to the region inside the unit circle and in a certain angle with vertex $1$, symmetric around the x-axis. This should not be too difficult, but I cannot actually see how that inequality is related to that geometric picture. Can someone give a hint? Thanks!",['complex-analysis']
647058,"If $N$ is a normal subgroup of a group $G$, and $M$ is a characteristic subgroup of $N$, then $M$ is a normal subgroup of $G$.","Prove that: If $N$ is a normal subgroup of a group $G$ , and $M$ is a characteristic subgroup
of $N$ , then $M$ is a normal subgroup of $G$ . Here what I am seeing is that $M$ is normal in $N$ and $N$ normal in $G$ . But normality is not transitive property. So how to go?","['group-theory', 'abstract-algebra', 'normal-subgroups']"
647070,Example of a certain locally univalent function,"I'm looking for an example of a non-quadratic analytic function $f\colon \mathbb{C}\to \mathbb{C}$ (a power series with infinite radius of convergence) that has the following three properties: $f(0)=0$. $f'(\mathbb{R})=\mathbb{R}$. $f''(x)\ne 0$ for all $x\in \mathbb{C}$. I'm embarrassed, I can't find one! nor prove that such a function exists. Edit notes: Notice the following: $f$ will not be locally univalent because $f'(x)=0$ for some $x\in \mathbb{R}$. $f'$ is required to be locally univalent because $f''(x)\ne 0$ everywhere in $\mathbb{C}$. the restriction of $f'$ to $\mathbb{R}$ must be either increasing or  decreasing by (3) and (2). if $f''$ is constant, then $f$ is quadratic. $f$ cannot be polynomial of degree greater than two because of (3). from the Big Picard Theorem in the non-constant $f''$ case $f''$ attains all the complex values with the exception of zero (as in $e^x$). In summary my hunch is that any example will be a composition involving $e^x$.","['complex-analysis', 'real-analysis']"
647075,"Extending continuous, densely-defined linear maps between locally convex spaces","Let $X$ and $Y$ be locally convex topological vector spaces, say over $\mathbb{C}$. To set the stage a bit, I'll say that the topology on $X$ is given by a separating family of semi-norms $(p_i)_{i \in I}$ and, similarly, the topology on $Y$ is given by a family $(q_j)_{j \in J}$. Now, suppose that $V \subset X$ is a dense subspace of $X$ and $T : V \to Y$ is a continuous linear map. Does there exist, then, a (unique) continuous extension $\overline T : X \to Y$ of $T$? The answer is yes for Banach spaces, and this is an incredibly useful fact. The norm seems pretty crucial to the proof though. Any easy counterexamples?","['topological-vector-spaces', 'locally-convex-spaces', 'functional-analysis']"
647085,"How prove exists a sequence $\{a_{n}\}$ of real numbers such that $\sum_{n=1}^{\infty}a^2_{n}<\infty,\sum_{n=1}^{\infty}|a_{n}b_{n}|=\infty$","Suppose that the series $\displaystyle\sum_{n=1}^{\infty}b^2_{n}$ of postive numbers diverges. Prove that
 there exists a sequence $\{a_{n}\}$ of real numbers such that
$$
\sum_{n=1}^{\infty}a^2_{n}<\infty
\quad\text{and}\quad
\sum_{n=1}^{\infty}|a_{n}b_{n}|=\infty.
$$ My try: maybe this Cauchy-Schwarz inequality have usefull
$$\Big(\sum_{n=1}^{\infty}a^2_{n}\Big)\Big(\sum_{n=1}^{\infty}b^2_{n}\Big)\ge
\Big(\sum_{n=1}^{\infty}a_{n}b_{n}\Big)^2$$",['sequences-and-series']
647090,Find $2\times 2$ matrices $A$ and $B$ such that $AB=0$ but $BA$ does not equals to $0$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find $2\times 2$ matrices $A$ and $B$ such that $AB=0$ but $BA$ does not equals to $0$ (please show working and the concepts used) Thanks :)",['matrices']
647149,Applications of the Kuratowski closure-complement theorem,"I crossed with the Kuratowski closure-complement theorem while learning Munkres's Topology (Problem 21 in Section 17; Page 102, 2nd edition). The following description is from B.J. Gardner and M. Jackson . The Kuratowski Closure-Complement Theorem: If $(X, \tau)$ is a topological space and $A \subseteq X$ then at most 14 sets can be obtained from $A$ by taking closures and complements. As stated in B.J. Gardner and M. Jackson , this remarkable theorem and related phenomena have been the concern of many authors. However, being not a researcher in Topology, My Problem: I would expect to see more interesting applications of the Kuratowski theorem in other fields, even in some non-mathematical fields (such as computer science ). I failed to find related literatures through googling.","['general-topology', 'applications', 'reference-request']"
647156,Can the following integral be computed?,Can the following integral be computed?,"['sequences-and-series', 'convergence-divergence', 'calculus', 'real-analysis']"
647168,Find $\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx$ [duplicate],"This question already has an answer here : Evaluate $\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x$ (1 answer) Closed last year . I want to find the sum: $$\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx$$ I start with finding the antiderivative of the integrant, which is: $$\frac{1}{2\sqrt{2}}[\ln(x^2-\sqrt{2}x+1)-\ln(x^2+\sqrt{2}x+1)+2\arctan(\sqrt{2}x-1)+2\arctan(\sqrt{2}x+1)]$$ Then I use the fundamental theorem of calculus to evaluate the integral. It turns out that the result is really ugly and I have no idea how to hanled it. Is there any tricks to tackle this?","['sequences-and-series', 'integration', 'real-analysis']"
647184,$(X \supset A)\wedge (X \supset B)|(Y\supset A) \wedge (Y \supset B)\to Y\supset X$|Prove that $X=A \cup B$.,"I've just got this question from Elon Lages' Curso de Análise Vol 1 . Given the sets $A$ and $B$ , let $X$ be a set with the following properties: $1.$ $(X \supset A)\wedge (X \supset B)$ . $2.$ $(Y\supset A) \wedge (Y \supset B)\to Y\supset X$ Prove that $X=A \cup B$ . Reading the property $1$ , I know that $A$ and $B$ are in $X$ but it doesn't give the certainty that only $A$ and $B$ are in $X$ . When reading $2$ , it states that if $Y$ contains both $A$ and $B$ , it also contains $X$ , this property gives me the certainty that $X$ contains only $A$ and $B$ , hence $A \cup B =Y=X$ . Now $A$ and $B$ could be disjoint, they could have an intersection but $A\neq B$ and it could be that $A=B$ . Then the formula that would encompass all these possiblities would be $X=A \cup B$ . Would this work as a proof?",['elementary-set-theory']
647235,"Counterexample to ""Measurable in each variable separately implies measurable""","Some fellow classmates are preparing for a qualifying exam on real analysis, and asked me for help on the following question: Let $ \ f:[0,1]^2\longrightarrow\mathbb{R}$ be such that: (i) $\ f(x,\cdot)$ is measurable for each fixed $x\in[0,1]$ ; (ii) $\ f(\cdot,y)$ is continuous for each fixed $y\in[0,1]$ . Show that $\ f$ is measurable.  If we only assume that $\ f(\cdot,y)$ is measurable for each $y\in[0,1]$ , rather than continuous, can we still conclude that $f$ is measurable? I was able to come up with the following simple proof (at least I hope it is a proof) of the first statement using some standard arguments: Proof. $ \ $ Define a sequence of functions $\{f_n:[0,1]^2\longrightarrow\mathbb{R}\}_{n\geq 1}$ by: $$ f_n(x,y)=f\bigg(\frac{i}{n},y\bigg), \qquad \text{for } x\in\bigg[\frac{i}{n},\frac{i+1}{n}\bigg], \ i=1,\cdots,n $$ Moreover, since $ \ f(\cdot,y)$ is continuous at $x=\tfrac{i}{n}$ for each $y$ , for each $\epsilon>0$ there is a $\delta_y>0$ so that $$ \bigg| \ x-\frac{i}{n}\bigg|<\delta_y \qquad\Rightarrow\qquad \bigg| \ f(x,y)-f\bigg(\frac{i}{n},y\bigg)\bigg|<\epsilon; $$ therefore, for any $n>\tfrac{1}{\delta_y}$ and any fixed $(x,y)\in[0,1]^2$ with $x\in\big[\tfrac{i}{n},\tfrac{i+1}{n}\big]$ , we have $$ \bigg| \ x-\frac{i}{n}\bigg|\leq \frac{1}{n}<\delta_y $$ and so $$ | \ f_n(x,y)-f(x,y)|=\bigg| \ f\bigg(\frac{i}{n},y\bigg)-f(x,y)\bigg|<\epsilon. \tag{$\ast$} $$ In other words, $ \ f_n\longrightarrow f$ pointwise.  Furthermore, we clearly have that $$ A_n:=\{(x,y)\in[0,1]^2 \ | \ f_n(x,y)>\alpha\} \qquad\qquad\qquad $$ $$ \qquad\qquad \ =\bigcup_{i=1}^{n-1}\bigg(\bigg[\frac{i}{n},\frac{i+1}{n}\bigg)\times \bigg\{y\in[0,1] \ \bigg| \ \ f\bigg(\frac{i}{n},y\bigg)>\alpha \bigg\}\bigg). $$ Since $ \ f\big(\tfrac{i}{n},\cdot\big)$ is measurable for each $i=1,\cdots,n$ , the latter expression is a union of measurable sets.  This means that $ \ A_n$ is measurable, and therefore $ \ f_n$ is measurable for each $n\geq 1$ .  Finally, as the limit of measurable functions is measurable, we conclude that $ \ f$ is measurable. $\hspace{6.25in}\square$ I'm fairly certain that the above proof is correct ( anyone care to confirm? ); however, that still leaves the following: Question:  If $ \ f$ is merely measurable in each variable separately, is it measurable? The proof I gave above uses the continuity assumption in $(\ast)$ , so obviously the same proof will not work in this case, but I cannot come up with any counterexamples.  Any suggestions?","['examples-counterexamples', 'measure-theory', 'proof-verification', 'real-analysis']"
647257,Write the following in the form of AX = B,"Write the following system of equations in the form $AX = B$, and calculate the solution using the equation $X = A^{-1}B$. $$2x - 3y = - 1$$ $$-5x +5y = 20$$ I'm not the strongest at linear algebra but I don't understand what the question is asking me over here or how to even go about solving this.","['matrices', 'linear-algebra', 'inverse', 'systems-of-equations']"
647270,Independent set of events that their union happen in prob.1 implies the events are happening i.o,"I'm pretty clueless with a proof in probability and i'll be happy for some help. Let ${A_n}$ be an infinite set of independent events with $P(A_n) < 1$ for every n.
I want to prove that if $P(\bigcup _{n=1}^\infty A_n) = 1$ it follows that $P(A_n\space i.o.)=1$. Thanks,
Ohad",['probability-theory']
647301,Essential range and integral means,"I am looking at the notion of essential range, which is defined for a function $f:(X,\mu)\rightarrow \mathbb{C}$ as the set $R_f$ of points $z\in\mathbb{C}$ satisfying $$\forall\epsilon>0, \mu(\{x\in X:|z-f(x)|<\epsilon\})>0.$$ I consider the case where $f\in L^{\infty}(\mu)$. In this case, let's consider the set $A_f$ of complex numbers of the form $$\frac{1}{\mu(E)}\int_E f\ d\mu,$$ where $E$ is a measurable set satisfying $0<\mu(E)<\infty$. I conjecture that $A_f$ is included in the convex hull of $R_f$. I have first proved that $\mu(X-f^{-1}(R_f))=0$, so that each element of $A_f$ is actually a ''mean'' over a set $E\cap f^{-1}(R_f)$, comprised of elements mapped into $R_f$. This convinces me that $A_f$ is included into the convex hull of $R_f$, but I can't manage to prove it. Is there a simple or elegant way to come to the result? NB: I also proved that $R_f$ is compact with supremum $\|f\|_{\infty}$, may it be useful...",['measure-theory']
647311,If the dual spaces are isometrically isomorphic are the spaces isomorphic?,"Let $X$, $Y$ be Banach spaces such that the duals $X^\ast$ and $Y^\ast$ are isometrically isomorphic. Are $X$ and $Y$ necessarily isomorphic? The answer to the question whether $X$ and $Y$ are automatically also isometrically isomorphic is no as the example $c$, $c_0$ which both have dual $\ell^1$ but are not isometrically isomorphic shows.","['normed-spaces', 'functional-analysis', 'banach-spaces']"
647324,Fubini on path integrals?,"I've been given an exercise in more steps where I need to prove that the Riemann $\zeta$ function extends to a meromorphic function on all $\mathbb{C}$ with a single simple pole in $z=1$. To prove it I've been given the equality:  $sin(\pi s)\Gamma(s)\Gamma(1-s)=\pi$ which I now plug in in a formula which i derived before which leads to the following result: $$\zeta(s)=-\frac{\Gamma(1-s)}{2\pi i}\int_\gamma\frac{(-z)^{(s-1)}}{e^z-1}$$ We know that $\zeta(s)$ is holomorphic for $\Re(s)>1$ and we want now to check the behaviour for $\Re(s)\le1$. For $\Re(s)\le1$ we know that $\Gamma(1-s)$ is meromorphic with a simple pole at $s=1$. And now the idea was to check that the integral represents an holomorphic function, hence I wanted to use Fubini as shown below. Let $\gamma_s$ be a closed path in $\mathbb{C}$. Then: $$\int_{\gamma_s} \underbrace{\int_\gamma \frac{(-z)^{s - 1}}{e^z - 1} dz}_{f(s)} ds \overbrace{=}^{Fubini} \int_\gamma \int_{\gamma_s} \underbrace{\frac{(-z)^{s - 1}}{e^z - 1}}_{\text{is holomorphic in s on $\mathbb{C}$}} ds dz \overbrace{=}^{\text{Cauchy Theorem}} \int_\gamma 0 dz = 0$$ Is this last step allowed a priori or do I need more assumptions to be able to swap paths? Thank you",['complex-analysis']
647358,Binomial thinning of geometric distribution,"I need to show that a random variable follows a specific law, how could I do that? Let $\{A_{i,j}\}$ be an infinite iid array of Bernouilli($\psi$) variables and $\eta_1,\ldots,\eta_n$ be iid geometric($p$) random variables. Let $X_1,\ldots, X_n$ be iid random variables defined by:
$$X_j=\sum_{i=1}^{\eta_j}A_{i,j}, \quad j=1,\ldots, n.$$
How could I show that
$$X\sim \mbox{geometric}\left(\frac{p}{\psi(1-p)+p}\right).$$ Thanks a lot!","['statistics', 'probability-distributions', 'probability']"
647362,"Prove that $\int_{D}\nabla u\cdot\nabla vdx=\int_{D}uv\,dx=0$","Let $D$ be the open bounded subset in $\mathbb{R}^{n}$ with smooth boundary, $\alpha$ and $\beta$ be different  non-null real numbers, and $u$ and $v$ be in $W_0^{1,2}(D)\setminus\left\{ 0\right\} $ such that $\Delta u=\alpha u$ and $\Delta v=\beta v$ in weak solutions sense. Prove that $$\int_{D}\nabla u\cdot\nabla v\,dx=\int_{D}uv\,dx=0$$ I don't understand what ""in weak solutions sense"" means. Can anyone tell me what it means so I can solve the problem. Thanks in advanced.","['sobolev-spaces', 'partial-differential-equations', 'real-analysis', 'analysis', 'functional-analysis']"
647423,The distribution of the inner product of a random complex normal vector.,"Good day! I would like to find the distribution of the inner product of a random complex normal vector with: some constant vector; random gaussian vector. Let's assume a vector $\vec{z}$ which has a complex normal distribution with location parameter $\mu$, covariance matrix $\mathbf{\Gamma}$ and the relation matrix $\mathbf{C}$:
$$\vec{z}=\{\Re{\vec{z}},\Im{\vec{z}}\}^{T}=\{z_1,z_2\ldots z_n\}^{T}\sim \mathit{CN}(\mu_{\vec{z}},
\mathbf{\Gamma},\mathbf{C})$$
So the pdf of $\vec{z}$ is:
$$w_{\vec{z}}(z_1,z_2\ldots z_n)= \frac{1}{\pi^k\sqrt{\det(\mathbf{\Gamma})\det(\mathbf{\Gamma}-\mathbf{C^{H}}\mathbf{\Gamma^{-1}}\mathbf{C})}}\times\\ \times \exp\!\left[\!-\frac12 \begin{pmatrix}
(\vec{z}-\mu_{\vec{z}})^{\ast}\!\!\!&\!\!\!(\vec{z}-\mu_{\vec{z}})\end{pmatrix}
    \begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}^{\!\!-1}\! \!\!                                 \begin{pmatrix} \vec{z}-\mu_{\vec{z}} \\ \vec{z}^{\ast}-\mu_{\vec{z}}^{\ast}\end{pmatrix}
\right] $$
and a complex constant vector $\vec{a}=\{\Re{\vec{a}},\Im{\vec{a}}\}^{T}=\{a_1,a_2\ldots a_n\}^{T}$. I'd like to find the joint distribution of $(\Re{U},\Im{U})$ where $U=\vec{z}.\vec{a}^{\mathbf{H}}$ My attempt. First I can form a matrix $\mathbf{A}=\text{diag}\{\vec{a}^{\mathbf{H}}\}$ and using the property that
$$Z\ \sim\ \mathit{CN}(\mu,\,\mathbf{\Gamma},\, \mathbf{C}) \quad\Rightarrow\quad AZ+b\ \sim\ \mathit{CN}(A\mu+b,\, A\mathbf{\Gamma} A^{\mathbf{H}},\, A\mathbf{C}A^{\mathbf{T}}) $$
and rearranging $U=\vec{z}.\vec{a}^{\mathbf{H}}$ as $U=\mathrm{tr}\left(\mathbf{A}\vec{z}\right)$ I can say that $\mathbf{A}\vec{z}\sim \mathit{CN}(\mathbf{A}\mu_{\vec{z}},\, \mathbf{A}\mathbf{\Gamma} \mathbf{A}^{\mathbf{H}},\, \mathbf{A}\mathbf{C}\mathbf{A}^{\mathbf{T}})$.
Then I tried to rewrite covariance matrix of $\mathbf{A}\vec{z}$ as:
$$    \begin{pmatrix}\mathbf{A}\mathbf{\Gamma} \mathbf{A}^{\mathbf{H}}&\mathbf{A}\mathbf{C}\mathbf{A}^{\mathbf{T}}\\(\mathbf{A}\mathbf{C}\mathbf{A}^{\mathbf{T}})^{\mathbf{H}}&(\mathbf{A}\mathbf{\Gamma} \mathbf{A}^{\mathbf{H}})^{\ast}\end{pmatrix}=\begin{pmatrix}\mathbf{A}&\mathbf{0}\\\mathbf{0}&\mathbf{A}^{\ast}\end{pmatrix}\begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}\begin{pmatrix}\mathbf{A}^{\ast}&\mathbf{0}\\\mathbf{0}&\mathbf{A}\end{pmatrix}
$$
And since $\mathbf{A}$ is diagonal
$$\det{\begin{pmatrix}\mathbf{A}\mathbf{\Gamma} \mathbf{A}^{\mathbf{H}}&\mathbf{A}\mathbf{C}\mathbf{A}^{\mathbf{T}}\\(\mathbf{A}\mathbf{C}\mathbf{A}^{\mathbf{T}})^{\mathbf{H}}&(\mathbf{A}\mathbf{\Gamma} \mathbf{A}^{\mathbf{H}})^{\ast}\end{pmatrix}}=\det{\begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}}\prod^n_{i=1}|a_i|^4$$
So the pdf of $\mathbf{A}\vec{z}$ would look like:
$$
w_{\mathbf{A}\vec{z}}(x_1,x_2\ldots x_n)= \frac{1}{\pi^k\sqrt{\det{\begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}}}\prod^n_{i=1}|a_i|^2 }\times\\ \!\!\times \!\!\exp\!\left[\!-\frac12 \begin{pmatrix}
(\vec{x}-\mathbf{A}\mu_{\vec{z}})^{\ast}\!\!\!&\!\!\!(\vec{x}-\mathbf{A}\mu_{\vec{z}}\! )\end{pmatrix}
   \!\!\begin{pmatrix}\mathbf{A^{-1}}&\mathbf{0}\\\mathbf{0}&\mathbf{A^{-1}}^{\ast}\end{pmatrix}\! \!\!\begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}^{\!\!-1}\! \!\!\begin{pmatrix}\mathbf{A^{-1}}^{\ast}&\mathbf{0}\\\mathbf{0}&\mathbf{A^{-1}}\end{pmatrix}\! \!\!                                 \begin{pmatrix} \vec{x}-\mathbf{A}\mu_{\vec{z}} \\ \vec{x}^{\ast}-\mathbf{A}^{\ast}\mu_{\vec{z}}^{\ast}\end{pmatrix}\! \!\!
\right] 
$$ UPDATE Using the notation of augmented matrices and vectors I can put it in a different form.
Let's call $$\mathbf{\alpha}=\begin{pmatrix}\mathbf{A}^{\ast}&\mathbf{0}\\\mathbf{0}&\mathbf{A}\end{pmatrix}, \ \mathbf{\alpha^{-1}}=\begin{pmatrix}\mathbf{A^{-1}}^{\ast}&\mathbf{0}\\\mathbf{0}&\mathbf{A^{-1}}\end{pmatrix}, \ \mathbf{R}=\begin{pmatrix}\mathbf{\Gamma}&\mathbf{C}\\\mathbf{C^{H}}&\mathbf{\Gamma}^{\ast}\end{pmatrix}, \ \underline{\vec{x}}=\begin{pmatrix} \vec{x}\\ \vec{x}^{\ast}\end{pmatrix}$$
Then the pdf can be rewritten as
$$
w_{\mathbf{A}\vec{z}}(x_1,x_2\ldots x_n)= \frac{1}{\pi^{k}\prod^n_{i=1}|a_i|^{2}\sqrt{\det{\mathbf{R}}}}\exp\!\left[\!-\frac12 
(\underline{\vec{x}}-\mathbf{\alpha}\underline{\mu_{\vec{z}}})^{\mathbf{H}}
   \mathbf{\alpha^{-H}}
\mathbf{R}^{\!\!-1}\mathbf{\alpha^{-1}} (\underline{\vec{x}}-\mathbf{\alpha}\underline{\mu_{\vec{z}}})
\right] 
$$
then multiplying matrices inside the exponent I get
$$
w_{\mathbf{A}\vec{z}}(x_1,x_2\ldots x_n)= \frac{1}{\pi^{k}\prod^n_{i=1}|a_i|^{2}\sqrt{\det{\mathbf{R}}}}\exp\!\left[\!-\frac12 
(\mathbf{\alpha^{-1}}\underline{\vec{x}}-\underline{\mu_{\vec{z}}})^{\mathbf{H}}
  \mathbf{R}^{\!\!-1}(\mathbf{\alpha^{-1}}\underline{\vec{x}}-\underline{\mu_{\vec{z}}})
\right] \!\!\!\tag{1}
$$ But next I should find the distribution of the  sum of elements of $\mathbf{A}\vec{z}$ and here I'm stuck. In the second case one can set $\vec{a}$ to be gaussian with augmented mean vector $\underline{\mu_{\vec{a}}}$ and augmented covariance matrix $\mathbf{R_a}$ and pdf
$$w_{\vec{a}}(a_1,a_2\ldots a_n)= \frac{1}{\pi^{k}\sqrt{\det{\mathbf{R_a}}}}\exp\!\left[\!-\frac12 
(\underline{\vec{a}}-\underline{\mu_{\vec{a}}})^{\mathbf{H}}
  \mathbf{R_a}^{\!\!-1}(\underline{\vec{a}}-\underline{\mu_{\vec{a}}})
\right] $$
Then one can treat $(1)$ as the conditional distribution of $\mathbf{A}\vec{z}$ given $\vec{a}$, so the unconditional distibution would be 
$$
w_{\mathbf{A}\vec{z}}(x_1,x_2\ldots x_n)=\int w_{\mathbf{A}\vec{z}|\vec{a}}(x_1,x_2\ldots x_n|\vec{a})w_{\vec{a}}(a_1,a_2\ldots a_n)\mathrm{d \vec{a}}$$
Which (up to the scale factor) will look like
$$
w_{\mathbf{A}\vec{z}}(x_1,x_2\ldots x_n)\!\!\propto \!\!\int \!
\!\frac{1}{\prod^n_{i=1}|a_i|^{2}} 
\!\exp\!\left[\!-\!\frac{(\mathbf{\alpha^{-1}}\underline{\vec{x}}\!-\!\underline{\mu_{\vec{z}}})^{\mathbf{H}}
  \mathbf{R}^{\!\!-1}(\!\mathbf{\alpha^{-1}}\!\underline{\vec{x}}\!-\!\underline{\mu_{\vec{z}}}\!)\!+\!
(\!\underline{\vec{a}}\!-\!\underline{\mu_{\vec{a}}})^{\mathbf{H}}
  \mathbf{R_a}^{\!\!-1}(\!\underline{\vec{a}}\!-\!\underline{\mu_{\vec{a}}}\!)}{2}
\right]\!
\mathrm{d \!\vec{a}} 
$$
It is not clear to me how to cope with this integral (especially since \vec{a} is complex then it should be treated as some contour integral or not?) So my questions are: How to complete the trace operation in $(1)$ in order to get inner product with a constant vector? How to perform integration in the last equation in order to get inner product with a gaussian vector?","['multivariable-calculus', 'normal-distribution', 'probability-distributions', 'integration']"
647428,Showing $(1 + \sin x)^{-1} = \sec^2 x - \sec x \tan x$,I was asked to show the following: $$\frac {1}{1 + \sin x} = \sec^2 x - \sec x \tan x$$ I proceeded to factorize and simplify.. $\sec x (\sec x - \tan x)$$ $\sec x (\frac {1}{\cos x} - \frac {\sin x}{\cos x}) $ $\sec x (\frac {1 - \sin x}{\cos x}) $ $\frac {1 - \sin x}{\cos ^ 2 x}$ $\frac {1 - \sin x}{1 - \sin ^ 2 x} $ and then i'm lost.. Any help will be much appreciated!,['trigonometry']
647436,Fundamental theorem of calculus of Banach-space valued functions,"Let $f:[a,b]\rightarrow E$ be a  continuous function from the interval $[a,b]$ to a Banach space $E$. Let $F(x)=\int_a^xf(t)\text{ }dt$ where the integral is the Bochner integral. I have to prove that $F'(x)=f(x)$. The first thing I tried to do was try to calculate $F(x+h)-F(h)=\int_x^{x+h}f(t)\text{ }dt$. If $F'(x)=f(x)$ then this integral must equal $hf(x)+o(|h|)$, but the only thing I can say about this integral is $||\int_x^{x+h}f(t)\text{ }dt||_E\leq h||f||_{\infty}$. I am kind of stuck. I need a hint. Thanks.","['calculus', 'functional-analysis', 'banach-spaces']"
647493,Prove that $f(x)=x$ if the following holds true [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Let $f\colon\mathbb R \to \mathbb R$ be a continuous odd function such that 1) $f(1+x)=1+f(x)$ 2) $x^2f(1/x)=f(x)$ for $x\ne0$. Prove that $f(x)=x$.","['calculus', 'functions']"
647520,"let $A$ be a set of $n+1$ natural numbers between $1$ and $3n$. Show that there are $a,b \in A$ such that $n \leq a-b \leq 2n$","I'm having difficulties solving this question and would appreciate a nudge in the right direction. 
I think this is best solved with pigeonhole, but what are the pigeons and what are the holes?","['pigeonhole-principle', 'combinatorics']"
647561,Multiplication of Set Discrete math,"I ran into some problems.
Do you guys have any ideas about multiplication of set? For example: $A = \{1,2,3\},\quad B = \{x,y\},\quad C=\{0,1\}$ What will you do if I want to see: $A\times(B\times C)$ Can you do the binary method?","['discrete-mathematics', 'elementary-set-theory']"
647573,The two possible structures on a triple point,"Let $k$ be an algebraically closed field. I would like to prove that there are only two possible $k$-scheme structures on a triple point, namely that of a $2$-jet $\textrm{Spec }k[x]/(x^3)$, and that of $\textrm{Spec }k[x,y]/(x^2,xy,y^2)$. A triple point ""is"" a local Artin $k$-algebra $(A;\mathfrak m)$ of dimension $3$. So I have to show that $A$ is either isomorphic to a $2$-jet ring, or to $k[x,y]/(x^2,xy,y^2)$. I do not know how to do it. The only thing I know is that $A/\mathfrak m=k$, so by $\dim_k A=3$ we see that as a $k$-vector space, $\mathfrak m$ has dimension $2$. If I assume $\mathfrak m$ is generated by ""vectors"" $x,y$, then (since $\mathfrak m^2=0$) we have relations $x^2=0,xy=0,y^2=0$. This is not precise at all, though. There is a sort of intuition that I cannot make precise. It is as follows: I have a supporting point $[\mathfrak m]\in\textrm{Spec }A$ and I have to give two tangent vectors; I have essentially two choices: either I choose them to be linearly independent, or linearly dependent. Can I somehow make this precise and get the result? Thanks in advance.",['algebraic-geometry']
647592,Arclength does not change with reparametrization,"Recall that the length of a curve $\alpha : [a,b] \rightarrow \mathbb{R}^3$ is given by $L(\alpha) = \int |\alpha'(t)|  dt$. Let $\beta(r): [c,d] \rightarrow \mathbb{R}^3$ be a reparametrization of $\alpha$ defined by taking a map $h: [c,d] \rightarrow [a,b]$ with $h(c) = a, h(d)=b$ and $h'(r) \geq 0$ for all $r \in [c,d]$. Show that the arclength does not change under this type of reparametrization. I believe I have to use the chain rule. I initially set $\beta(r) = \alpha(h(r))$. Then using the chain rule, I get $\beta '(r) = \alpha '(h(r)) \cdot \frac{dh}{ds} (r)$. Does this imply that the magnitudes are the same as well? Because then that would be sufficient to prove that the arclength is the same.","['multivariable-calculus', 'differential-geometry']"
647600,"$\gcd(a,b,c)\!=\!1\Rightarrow \gcd(az+b,c)\!=\! 1$ for some $z$ [Coprime Dirichlet Theorem]","I can't crack this one. Prove: If $\gcd(a,b,c)=1$ then there exists $z$ such that $\gcd(az+b,c) = 1$ (the only constraint is that $a,b,c,z \in \mathbb{Z}$ and $c\neq 0)$","['elementary-number-theory', 'divisibility', 'abstract-algebra', 'gcd-and-lcm']"
647617,Maximum value of the lowest sum in a set of numbers,"Last year in a maths contest held in Catalonia called Cangur it was posed the following qüestion: We write numbers 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10, in a certain order around a circumference. Then we sum each number with its two neighbors and we get 10 sums. Which is the maximum value that can have the lowest of these sums? a) 14      
  b) 15      
  c) 16      
  d) 17      
  e) 18 My first approach has been to add up all the triplets. By doing that, I will get a number that won't change independently from the order of the numbers around the circumference. $$
(1+2+3)+(2+3+4)+(3+4+5)+\dots+(10+1+2)=
\sum\limits_{n=1}^{10} 3n=165
$$ Therefore, I can deduce that the average sum of every triplet is $16.5$ which automatically discards $17$ and $18$ from being the correct answer. That's because if one of the triplets added up $17$, in order to keep the average at $16.5$, we would need some other triplet to add up less than $17$, which would imply that this triplet adding up $17$ wasn't the lowest triplet. And the same explanation would work for $18$. I can also discard $16$ because in order to keep the average at $16.5$ we would need more or half of the triplets to add up $16$. And, although I'm not 100% sure about this assertion, I think it can be proven that there aren't 5 possible ways of adding up $16$ that can be written around the circumference. In any case, even if my last guess was true, I'm not sure how to proceed at this point. I don't know how to choose between $14$ and $15$ and I don't like to think that the only way of finding the correct answer is to try out and see if it works. So if some of you have found any other ways of facing the problem I'd really appreciate it if you shared it. Thanks! Edit : In order to understand better what I meant by these 10 sums of triplets, here's a sketch that shows that visually for the first three sums. I would also want to make clear that the order of the numbers in the drawing isn't the order in which they should be to solve the problem.","['optimization', 'sequences-and-series', 'contest-math']"
647647,Calculate Dq(x),"Let A be a symmetric $m \times m$ matrix, and $q(x)=x\cdot Ax$ a quadratic form on $\mathbb{R}^m$. Question:
Calculate $Dq(x)$; write your answer in vector notation. Does anyone knows the answer on this question? I don't know what they mean with a quadratic form on $\mathbb{R}^m$. I know that if $A$ is the unit matrix, $q(x)$ would look like $x_{1}^{2}+\dots +x_{m}^{2}$ but i don't know if $A$ is an unit matrix.","['convex-analysis', 'derivatives']"
647668,Derivatives of multivariable functions,"I would like to make a few statements about a simple object - the derivative of a univariate function - and apply and relate its features and my understaning of them to multivariate functions. Univariate functions. A derivative of a real function $f: {\mathrm R} \to {\mathrm R}$ at the point $a \in {\mathrm R}$ is the slope of the function at this point; that is how much the function value changes with respect to the change in variable, or $$f'(a) = \lim_{h\to0} \frac{f(a + h) - f(a)}{h}.$$ The derivate of this a real function $f: {\mathrm R} \to {\mathrm R}$ is the function $$f': a \mapsto f'(a)$$ that maps a point to the slope of the funtion $f$ at that point. The derivative at a point is not itself the tangent to the function graph at that point, but it is closely related to it. The tangent at the point $a$ can be expressed as $$t(x) = f(a) + f'(a)(x-a),$$ which happens to be the best linear approximation of the function $f$ around $a$, or the first-degree Taylor polynomial $T^{f,a}_1$. The function $f'(a)(x-a)$ is linear in $x$. Multivariate functions. Let $f: {\mathrm R}^n \to {\mathrm R}^m$, where $m,\ n \in {\mathrm N}$. We can consider partial derivatives of $f$ at $a \in {\mathrm R}^n$, defined for example as $${\partial f\over\partial x_i} = \lim_{h \to 0} \frac{f(a + h{\bf e^i}) - f(a)}{h},$$ that is the derivative of the function at $a$ with respect to $x_i$ and other variables held constant, where ${\bf e^i} = (0, \dots, 0, 1, 0, \dots, 0)$ ($1$ is $i$-th from the left). These are derivatives of single-variable partial functions and therefore the same applies to them what I have written in the first section. The gradient of the function at a point is the vector of partial derivatives at that point, i.e. $$\nabla f(a) = \Big({\partial f\over x_1}, \dots, {\partial f\over x_n}\Big).$$ Its geometrical meaning is that it points in the direction of the steepest growth while its value is the growth in that direction. The equivalent of the derivative at a point seems to be what is called the total differential at that point. If $L$ is total differential at $a \in {\mathrm R}^n$, then $$\lim_{\bf h \to 0} \frac{||f(a + h) - f(a) - L(h)||}{||h||} = 0,$$ where $||\cdot||$ is the Euclidian norm, which means that $L$ has the ""approximative property"" - it appoximates the difference $f(a + h) - f(a)$ locally. If the total differential exists, it can be expressed as $L(h) = \nabla{f}(a) \cdot h$, where $\cdot$ is the dot product. (I think: the total differential does not approximate the function itself - this resembles ""derivative of a function at a point"" for univariate functions.) I come to understand that the derivative at a point of a multivariate function can be defined exactly the same way as the total differential at a point. (For some reason we have only defined the total differential for functions ${\mathrm R}^n \to {\mathrm R}$. Is this related to math, or is it a problem of terminology?) When I try to look at the derivative $f'$ of the function f , I should see that: $f': {\mathrm R}^n \to {\mathscr L}(\mathrm{R}^n, \mathrm{R}^m)$ - and I do, this simply states that the first derivative at a point is a local linear approximation of the original function f, but also that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$, which is driving me crazy. I would like to ask: Why is the total differential not called simply the derivative ? Why is it true that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$? I need an intuitive way of understanding what a ${\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$ is. Thanks!","['multivariable-calculus', 'partial-derivative', 'derivatives', 'real-analysis']"
647697,Tricks - Prove Homomorphism Maps Identity to Identity - Fraleigh p. 128 Theorem 13.12(1.),"Let $\phi$ be a homomorphism of a group G into a group G'. If $e =$ the identity element in G, then $\phi(e) =$ the identity element in G'. Is this what Sharkos is trying to answer: About homomorphisms, you only know $\phi(a) \phi(b) = \phi(ab)$. ab is more complicated than anything we want to think about, hence just presuppose $b = e$. Then $ \begin{align} \phi(a)\phi(e) & = \phi(a\color{magenta}{e}) \\ & = \phi(a) \end{align} $ Left multiply the last equation  by $\color{green}{\phi(a)^{-1}}$: $\quad \phi(e) =  id_{G'}.$ (1.) How do you predestine to rewrite $a$ as   $ a = a\color{magenta}{e}$? Or to 'presuppose $b = e$' ? I understand neither tricks. (2.) What's the intuition?","['intuition', 'group-theory']"
647733,Linear combination of symmetric matrices,"Let $A, B, C, D$ be four linearly independent symmetric 3 x 3-matrices over $\mathbb K$ . Show that some linear combination of these matrices is a matrix of
rank 1. I know it is supposed to be a question in Algebraic Geometry, so it must be interesting :)","['matrices', 'linear-algebra', 'algebraic-geometry']"
647739,Solution of a partial differential equation.,"Find $u=u(x,y)$ satisfying
$$\dfrac{\partial^2 u}{\partial x^2} = 6xy, \,\,\,u(0,y) = y, \,\,\,\dfrac{\partial u}{\partial x}(1,y)=0.$$ I have tried by laplace transformation $$\displaystyle s^2\bar{u}(s,y)-su(0,y)-\frac{\partial u}{\partial x}(0,y) = \frac{6y}{s^2}
\,\,\,\Longrightarrow\,\,\,
\bar{u}(s,y)=sy +\frac{ 6y}{s^2}+\frac{\partial u}{\partial x}(0,y).$$ Please tell me how to proceed.","['multivariable-calculus', 'calculus', 'partial-differential-equations', 'partial-derivative', 'analysis']"
647777,Why does the Fourier series of $x$ not seem to give the right value?,"I'm reading a lecture about Fourier series , and it says that you can represent any continuous function as Fourier series. There's a given example: Let $f(x) = x$. $f(x) \approx \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(nx).$ So I can understand that $5 = f(5) = \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(5n)$ According to wolframalpha , the sum of this series is $\approx -1.283$ What is wrong here ..?","['fourier-series', 'linear-algebra']"
647779,Continuity via Closure Operator,"I'm having a rather simple question: Lets say a function preserves neighborhoods iff: $N\in\mathcal{N}_x \Rightarrow f^{-1}(N)\in\mathcal{M}_x$ and a function preserves closeness iff: $x\parallel A \Rightarrow f(x)\parallel f(A)$ I want to show that, in fact, these are equivalent. So far the second property is the same as: $f(\overline{A})\subseteq \overline{f(A)}$ I already passed a proof that a function preserves neighborhoods iff it is continuous in the usual sense: $V\in\mathcal{T} \Rightarrow f^{-1}(V)\in\mathcal{S}$ ...while $x\parallel A$ is meant to mean $x\in\overline{A}$.",['general-topology']
647790,Unequal circles within circle with least possible radius?,"It is the classical will-my-cables-fit-within-the-tube-problem which lead me to the interest of circle packing .  So basically, I have 3 circles where r = 3 and 1 circle where r = 7 and I am trying to find the least r for an outer circles of these 4 smaller circles. After a couple of hours of thinking and some sketches with a compass I am getting close to the actual result.
But how can I calculate this?
With what formula? EDIT: Thanks for the great answer.
And then I come to wonder.
What happens if you add another of the small circles, so you have four circles with r = 3?  It is very close to 11.7","['geometry', 'inequality', 'circles', 'packing-problem']"
647794,"Help with complex integration problem: $\left| \int_{[\sqrt2,\sqrt2i]}\frac{1}{z-(1+i)}dz\right| \le 2(\sqrt2+1)$","I found a math problem (of a 2002 exam) I can't seem to solve, that is simply stated as: Show that
$$\left| \int_{\left[\sqrt2,\sqrt2i\right]}\frac{1}{z-(1+i)}dz\right| \le 2(\sqrt2+1)$$ I looked around and found two properties that might help me in this case. The first one is $$
\left| \int_a^bf(t)dt\right| \le  \int_a^b\left|f(t)\right|dt
$$ Parameterezing  $[ \sqrt2,\sqrt2i] $ as $\sqrt2(1-t)+\sqrt2it, 0\le t \le 1$, a and b become 0 and 1, respectively, and this can be solved by substitution($f(z)\rightarrow f(\sqrt2(1-t)+\sqrt2it)\cdot(\sqrt2(1-t)+\sqrt2it)'$, wielding, hopefully, $2(\sqrt2+1)$. However, I've been unable to solve the integral on the right, which leads me to the second property, which, at first glance, seems a bit easier and more directly applied, but, again, I'm not getting the results I hoped. It follows: Let $\gamma : [a,b] \rightarrow D$ a path, and f be a complex function, so that f is continuous in $tr(\gamma)$ (? - see note) (or  $\varphi(t)=f(\gamma(t))$ is continuous in [a,b]). If there is $M \in R^+$, such that $\left|f(z)\right|\le M, \forall z \in tr(\gamma)$, then: $$
\left|\int_\gamma f(z)dz\right|\le Ml(\gamma)
$$
where $l(\gamma)$ is the length of the path $\gamma$. In this case, $\gamma$ is simply a line segment, so $l(\gamma)$ is quite easy to calculate, $l(\gamma)=|\sqrt2i-\sqrt2| = \sqrt{\sqrt2^2+\sqrt2^2}=\sqrt4=2$, so all that's left is to show that $\left|f(z)\right|\le (\sqrt2+1)$, which I, again, seem to be unable to do. Through derivation, I was able to find the minimum of $|z-(1+i)|$ (and then the maximum of $|f(z)|$ ), and got the result $\left|f(z)\right|\le \frac{2}{\sqrt{3-2\sqrt2}}$, which, technically, shows what was asked (since $\frac{2}{\sqrt{3-2\sqrt2}}=0.41<\sqrt2+1$, but I really doubt that was the intended result. (Edit: As Daniel Fischer noted in his answer below, this calculation (that wields 0.41) is actually incorrect. I leave the error unedited, as a cautinary tale for the use of Windows calculator) Any help you can provide that at least points me in the right direction, or highlights something I've missed and/or misinterpreted, will be greatly appreciated. Note: The exercise and properties mentioned were not in english, and I translated them myself, so there may be some errors, particularly in notation/nomenclature. If you spot such errors, please either let me know by comment or edit the answer. Thank you.",['complex-analysis']
647817,Orthogonal complement examples,"I am looking for an example such that in a pre-Hilbert space $H$ we have for a subspace $U$ that (i) $\bar{U} \oplus U^\perp \neq H$ (ii) $ \bar{U} \neq U^{\perp \perp}$ 
Since finite and closed subspaces will probably not do it, I am not good at inventing nice examples. Does anybody here have a few at hand?","['calculus', 'functional-analysis', 'real-analysis', 'analysis']"
647818,"norm of differential operator on $P^n[0,1]$","Consider the space $P^n[0,1]$ of polynomials of degree $\leq n$ on $[0,1]$, equipped with the sup norm. Now, this is a finite dimensional space, so all linear operators have to be continuous, hence bounded. My question is: what is the norm of the differential operator $d/dx : P^n[0,1] \rightarrow P^{n-1}[0,1]$? I can't see the relation between the supremum of a polynomial $p(x)=a_0+\dots+a_nx^n$ and the supremum of $p'(x)=a_1 + 2a_2 x +\dots+n a_n x^{n-1}$.","['operator-theory', 'normed-spaces', 'functional-analysis', 'polynomials']"
647829,All almost complex structures on a manifold,"I read the statement of the Newlander-Nirenberg theorem, which says that ""any integrable almost complex structure is induced by a complex structure"". To make sense of the statement, I was wondering how many almost complex structures there are in the first place, i.e. if there is any way to determine all almost complex structures on a given manifold. The sphere admits a complex structure as $\mathbb P^1$, but I know that the complex structure on $\mathbb P^1$ is rigid, i.e. cannot be deformed. As a next non-trivial example I considered $\mathcal O(k)$, which in real terms would be a twisted rank 2 bundle over the sphere. My question is now, how can I determine all almost complex structures on $\mathcal O(k)$?","['complex-manifolds', 'complex-geometry', 'almost-complex', 'differential-geometry']"
647835,Why does finding the $x$ that maximizes $\ln(f(x))$ is the same as finding the $x$ that maximizes $f(x)$?,"I'm reading about maximum likelihood here . In the last paragraph of the first page, it says: Why does the value of $p$ that maximizes $\log L(p;3)$ is the same $p$ that maximizes $L(p;3)$. The fact that it mentions that log is an increasing function does not help me understand it at all. Does this method work for any increasing function? I've covered single and two variable optimization but have not come across this fact until now. I'd like to understand why it must be true and read more about its explanation. So, I would also appreciate it if someone can provide some links to it (I don't know what term to search).","['optimization', 'calculus', 'estimation']"
647845,"$95\,\%$ confidence interval for geometric distribution","I am analyzing data with a geometric distribution. Using maximum likelihood estimation, I can estimate $p$ to be $\displaystyle \hat p_{MLE} = \frac{N}{\sum_{i=1}^N x_i}$, where $N$ is the number of datapoints and each $x$ is the number of trials necessary for the first success, in each separate experiment. However, it is not clear to me how 'accurate' this value is, and thus I'd like to construct a $95\,\%$ confidence interval. But my searches have been rather unsuccessful, as I can't find any worked out versions of a suitable confidence interval for this particular distribution. I'm pretty sure there has to be something out there, and I would be very thankful if someone could guide me towards it!",['statistics']
647866,"In $\ell^p$, if an operator commutes with left shift, it is continuous?","Our professor put this one in our exam, taking it out along the way though because it seemed too tricky. Still we wasted nearly an hour on it and can't stop thinking about a solution. What we have: The left shift $L : \ell^p \to \ell^p$
$$L(x_1,x_2,x_3,\ldots) = (x_2,x_3,\ldots)$$ and another operator $T$. We should prove that if $TL=LT$, then $T$ is continuous. We had defined subspaces
$$ X_k = \{ (x_i) : x_i = 0 \text{ for } i>k \} $$
and seen that these are $T$-invariant and the restrictions $T : X_k \to X_k$ continuous (obvious). The hint was to use closed-graph-theorem to show that $T$ is continuous. Of course we can truncate any sequence to then lie in $X_k$, however I do not see how convergence of the truncated sequences relates to convergence of the images under $T$. Any help please?","['lp-spaces', 'functional-analysis', 'banach-spaces']"
647874,Valuative criteria with varieties,"Let $X$ be an algebraic prevariety over an algebraically closed field $k$ (I.e. an integral  scheme of finite type over $k$). In the valuative criteria for separatedness and properness of $X$ over $k$, is it enough to consider only DVR's of transcendence degree $1$ over $k$? If so, is it also enough for determining the separatedness and properness of a morphism of prevarieties over $k$? For both, please point me in the direction of a proof/counterexample.","['algebraic-geometry', 'schemes']"
647898,"Find the critical points for $F(x,y,z)=-x^{3}-y^{2}+2xy+x+2z$",I started by taking the first order partial derivatives: $F_{x}=-3x^{2}+2y + 1$ $F_{y}=-2y+2x $ $F_{z}=2 $ Now I would try to solve it for $F_{x}=F_{y}=F_{z}=0$ but $F_{z}=2$. How can I proceed or this means that $F$ doesn't have critical points?,"['multivariable-calculus', 'partial-derivative']"
647912,Prove inequality of generalized means,"Consider the generalized (power) mean of positive numbers $a_1, \dotsc, a_n$ $$M_p(a_1, \dotsc, a_n)=\left(\frac{a_1^p + \dotsb + a_n^p}{n}\right)^{1/p}\qquad p\in \mathbb{R}$$ where for $p=0$ we use the geometric mean. The generalized mean inequality says that  $$ p < q \implies M_p(a_1, \dotsc, a_n) \leq M_q(a_1, \dotsc, a_n),$$ with equality holding iff $a_1 = \dotsb = a_n$. On Wikipedia it says that one can prove this by differentiating with respect to $p$ and using Jensen's inequality, and noting that $\partial_p M_p(a_1, \dotsc, a_n)>0$. When I do so, I get: $$\partial_p \left(\left(\frac{a_1^p + \dotsb + a_n^p}{n}\right)^{1/p}\right)\\= \left(\frac{a_1^p + \dotsb + a_n^p}{n}\right)^{1/p} \partial _p \left( \frac1p \log (a_1^p + \dotsb + a_n^p) - \frac1p \log n  \right),$$ which is positive iff $\partial _p \left( \frac1p \log (a_1^p + \dotsb + a_n^p) - \frac1p \log n  \right)$ is positive. $$\partial _p \left( \frac1p \log (a_1^p + \dotsb + a_n^p) - \frac1p \log n  \right) \\= \left(\frac{-1}{p^2}\right) \log (a_1^p + \dotsb + a_n^p)+ \frac1p \frac{a_1^p \log a_1+ \dotsb + a_n^p \log a_n }{a_1^p + \dotsb + a_n^p}+\frac{1}{p^2}\log n.$$ I'm trying to show that this is positive. When I think about what Jensen's equality would tell me, I note that $$\frac{a_1^p\log a_1 + \dotsb + a_n^p \log a_n}{a_1^p + \dotsb + a_n^p} \\ \leq  \log \left(  \frac{a_1^p}{a_1^p + \dotsb + a_n^p}a_1 + \dotsb +  \frac{a_n^p}{a_1^p + \dotsb + a_n^p}a_n \right) \\ = \log \left(  \frac{a_1^{p+1} + \dotsb + a_n^{p+1}}{a_1^p + \dotsb + a_n^p} \right),$$ or we could try to bring the factor $\frac1p$ of the term in question into the mix and make a similar estimate. Anyone have an idea?","['average', 'calculus', 'derivatives']"
647917,"Decrease of $L_1$ norm of piecewise constant functions after some ""averaging""","In the course of a project, I ended up looking at what happens to the $L_1$ of real-valued piecewise-constant functions after some particular king of smoothing — but am currently stuck at a particular step. Essentially, my question boils down to the following general
statement:
Let $f$ be a piecewise-constant function over $[0,N]$ with $\int_{[0,N]} f = 0$, and let $\mathcal{I}$ be a
partition $\mathcal{I}=(I_1,...,I_k)$ of $[0,N]$ with $I_j=[x_j, x_{j+1})$ proving it.
For any other partition $\mathcal{J}=(J_1,...,J_\ell)$ (with $J_j=[y_j, y_{j+1}))$, let
$\bar{f}_\mathcal{J}$ be the ""flattening"" of $f$ (resp. g) wrt $\mathcal{J}$,
that is
$$
\bar{f}_{\mathcal{J}}(x) = \sum_{j=1}^\ell \mathbb{1}_{J_j}(x) \frac{1}{y_{j+1}-y_j}\int_{y_j}^{y_{j+1}} f(t)dt
$$
($y_1=0$, $y_{\ell+1}=N$)
It is not hard to show that the $L_1$ norm $\lVert \bar{f}_{\mathcal{J}} \rVert_1$ is at most $\lVert f \rVert_1$.
However, can we be more specific and get either an exact explicit relation, depending on all the variables $x_i$'s,
$y_j$'s, etc) between $\lVert \bar{f}_{\mathcal{J}} \rVert_1$ and $\lVert f \rVert_1$? an exact explicit relation for specific $\mathcal{I}$ and $\mathcal{J}$, i.e. both of the
form $\operatorname{length}(I_j) = N\frac{ (1+a)^{j+1}-1 }{(1+a)^k-1}$ and $\operatorname{length}(J_j) = N\frac{ (1+b)^{j+1}-1 }{(1+b)^\ell-1}$ for $1 > b > a > 0$ (as the partition proposed in [Birge87])? same as above, but only an upperbound of the form
$$ \lVert \bar{f}_{\mathcal{J}} \rVert_1 < (1-\phi(a,b))\lVert f \rVert_1 $$
for some positive function $\phi$? (if you have any idea or pointers -- the latter in case it has been
studied already...) Thank you, Clément [Birge87] Birge, Lucien. On the Risk of Histograms for Estimating
Decreasing Densities. The Annals of Statistics 15 (1987), no. 3,
1013--1022. doi:10.1214/aos/1176350489. http://projecteuclid.org/euclid.aos/1176350489 .","['functions', 'real-analysis']"
648002,$\int_{0}^{\infty}\frac{\cos2\pi x}{x^4+x^2+1}dx=-\frac{\pi}{2\sqrt{3}}\mathrm{e}^{-\pi\sqrt{3}}$,"Can somebody help me out with the following integral? Prove that: $\int_{0}^{\infty}\frac{cos2\pi x}{x^4+x^2+1}dx=\frac{-\pi}{2\sqrt{3}}e^{-\pi\sqrt{3}}$ I have already determined the singularities: $z=e^{\frac{\pi i}{3}}$  and $z=e^{\frac{2\pi i}{3}}$. These are poles of order one. But determining the integral with the residue theorem doesn't work. Can anybody help me? For the first residue I find: $Res(f,z)=\frac{e^{iz}}{4z^3+2z}$  (with z the first value above) and for the second in the same way.","['residue-calculus', 'complex-analysis', 'contour-integration']"
648004,Double integral -- tricky?,"If $f(x,y) = x^2+y^2$ and $D=\{(x,y)\in\mathbb{R}^2:x^2+y^2\geq1, x^2+y^2-2x\leq0 \text{ and } y\geq0\}$, find $\displaystyle\int\displaystyle\int_D f$. $D$ looks like the intersection between two circumferences, as I draw below: Polar coordinates seems a obvious choice, but using them I'd have unwanted bits, this is, if I split the region in two sections (by $x=\frac{1}{2}$) after computing the red and blue region I would have twice $R1$ and $R2$, a problem that could be solved computing each one of them and then substract them of the final answer -this is, after adding the integral of the red and blue sections-. I believe the answer would be given by $$\displaystyle\int \displaystyle\int_D f = \displaystyle\int\displaystyle\int _{\text{RED}} f + \displaystyle\int\displaystyle\int _{\text{BLUE}} f - \displaystyle\int\displaystyle\int _{\text{R1}} f - \displaystyle\int\displaystyle\int _{\text{R2}} f$$ $R1$ and $R2$ could be described as follows: $$R1 = \{(x,y):0\leq y\leq \sqrt{3}x,\;0\leq x\leq 1/2\}$$
$$R2 = \{(x,y):0\leq y\leq \sqrt{3}-\sqrt{3}x,\;1/2 \leq x\leq 1\}$$ Which gives
$$\displaystyle\int\displaystyle\int _{\text{R1}} f =\displaystyle\int_0^{1/2}\displaystyle\int_0^{\sqrt{3}x} (x^2+y^2)\; dy dx = 2\sqrt{3}\displaystyle\int_0^{1/2}x^3 = \displaystyle\frac{1}{52\sqrt{3}}.$$ $$\displaystyle\int\displaystyle\int_{R2} f = \displaystyle\int_{1/2}^1\displaystyle\int_0^{\sqrt{3}-\sqrt{3}x}(x^2+y^2)\; dydx \\ = \displaystyle\int_{1/2}^1(x^2(\sqrt{3}-\sqrt{3}x)dx + \frac{1}{3}\int_{\frac{1}{2}}^1 (\sqrt{3}-\sqrt{3}x)^3 dx \\ = \left[\sqrt{3}\displaystyle\int_{1/2}^1 x^2 -\sqrt{3}\displaystyle\int_{1/2}^1 x^3\right]  + \frac{1}{3}\int_{\frac{1}{2}}^1 (\sqrt{3}-\sqrt{3}x)^3 dx \\ = \sqrt{3}\left[\left(\displaystyle\frac{1}{3}-\displaystyle\frac{1}{24}\right)+\left(\displaystyle\frac{1}{4}-\displaystyle\frac{1}{64}\right) \right] + \displaystyle\frac{1}{3}\left(\displaystyle\frac{1}{2}\right) \\= \frac{\sqrt{3}}{16}+\frac{1}{6} = \frac{1}{48}(3\sqrt{3}+8).$$ Now changing to polar coordinates $x=r\cos \theta$, $y= r\sin \theta$ to compute the red and blue: $$\displaystyle\int\displaystyle\int _{\text{RED}} f = \displaystyle\int_0^{\pi/3}\displaystyle\int_0^1 r^3\;drd\theta = \frac{1}{4}\displaystyle\int_0^{\pi/3}d\theta = \frac{1}{12}\pi$$ Setting auxiliary axis $u = x-1, v = y$ I have $\displaystyle\frac{\partial (x,y)}{\partial (u,v)}=1$ and after with $u=r\cos \theta$, $v=r\sin \theta$ $$\displaystyle\int\displaystyle\int _{\text{BLUE}} f = \displaystyle\int_{2\pi/3}^{\pi}\displaystyle\int_0^1 r^3\;drd\theta = \frac{1}{12}\pi.$$ Finally 
$$\displaystyle\int \displaystyle\int_D f = \frac{1}{6}\pi -\displaystyle\frac{1}{52\sqrt{3}} -\frac{1}{48}(3\sqrt{3}+8) .$$ Now I'd like to know if the solution is right, but fundamentally, can this integral be computed without splitting the $D$? P.S: Legitimate question, but couldn't resist the pun.","['definite-integrals', 'multivariable-calculus', 'integration']"
648013,Proof of Gaussian elimination/Why does it work,"I have just had a class on linear algebra and the professor explained how to solve matrixes. While he could explain how to solve them by using Gaussian's elimination, he failed to explain how does that work. Why does matrix before doing any operations have the same solutions as the matrix after ""changing"" a row with Gaussian elimination? Where can I read the proof?",['linear-algebra']
648027,Differential Equations; A prerequisite to Differential Geometry?,"Is thorough knowledge in ODEs/PDEs and solution techniques to be considered a prerequisite to the study of Differential Geometry (Specifically Riemannian Geometry)? If not, how would one describe the relation between the subjects?","['ordinary-differential-equations', 'differential-geometry']"
648029,Verifying the subspace topology with a topology defined in terms of neighborhoods,"In M.A. Armstrong's Basic Topology , he introduces the concept of a topological space by first defining it in terms of neighborhoods (rather than open sets). Here is his formulation (p. 13): We ask for a set $X$ and for each point $x$ in $X$ a nonempty collection of subsets of $X$ , called neighborhoods of $x$ . These neighborhoods are required to satisfy four axioms: (a) $x$ lies in each of its neighborhoods (b) The intersection of two neighborhoods of $x$ is itself a neighborhood of $x$ . (c) If $N$ is a neighborhood of $x$ and if $U$ is a subset of $X$ which contains $N$ , then $U$ is a neighborhood of $x$ . (d) If $N$ is a neighborhood of x and if $\mathring{N}$ denotes the set $\{z\in N |  N$ is a neighborhood of $z\}$ , then $\mathring{N}$ is a neighborhood of $x$ . This whole structure is called a topological space . He then defines the subspace topology as follows: Let $X$ be a topological space and let Y be a subset of X . We can define a subspace topology on Y as follows. Given a point $y\in Y$ take the collection of its neighborhoods in the topological space $X$ and intersect each of these neighborhoods with $Y$ . The resulting sets are the neighbourhoods of $y$ in $Y$ . One of the exercises in this section is to veryify that the subspace topology (as defined above) satifies axioms (a)-(d) and is, hence, a topology on Y . I was able to verify axioms (a)-(c) fairly easily; however, I can't seem to finish the verification of axiom (d). This is what I have so far: To verify axiom (d), suppose $y\in Y$ , $N_Y$ is a neighborhood of $y$ in $Y$ , and let $$\mathring{N_Y}=\{z\in N_Y | N_Y \text{ is a neighborhood of } z\}.$$ Observe that $\mathring{N_Y}\subset N_Y \subset Y$ and, by definition, $N_Y=Y\cap N_X$ for some neighborhood $N_X$ of $y$ in $X$ . Now let $$\mathring{N_X}=\{z\in N_X | N_X \text{is a neighborhood of }z\}.$$ By axiom (d), $\mathring{N_X}$ is a neighborhood of $y$ in $X$ . We will show that $\mathring{N_Y}=Y\cap \mathring{N_X}$ , which will show that $\mathring{N_Y}$ is a neighborhood of $y$ in $Y$ . I was able to prove that $(Y\cap \mathring{N_X}) \subset \mathring{N_Y}$ , but I have not been able to establish that $\mathring{N_Y} \subset (Y\cap \mathring{N_X})$ . If I take an arbitrary $z\in \mathring{N_Y}$ , then I know that $N_Y=Y\cap N_X$ is a neighborhood of $z$ in $Y$ , but I do not see how to deduce from this that $N_X$ is a neighborhood of $z$ in $X$ (and hence that $z\in \mathring{N_X}$ ). I feel like there is just something obvious that I am missing, or, perhaps, my intuition was wrong and $\mathring{N_Y}\neq Y\cap \mathring{N_X}$ , and axiom (d) is verified in some other way. Any hints/suggestions would be much appreciated. Thanks.",['general-topology']
648036,Is it true that $H(X|Y)=H(Y|X)$?,"I have some difficulties with the question whether $H(X|Y)=H(Y|X)$? From my knowledge $I(X;Y)=H(X)-H(X|Y) = H(Y)-H(Y|X)$ so $H(X|Y)=H(Y|X)$ only when $H(X)=H(Y)$ The question is whether it's the last step, can I make a further assumption about the distribution of $X$ and $Y$ or $H(X)=H(Y)$ is a last step and no further conclusions.","['statistics', 'information-theory']"
648043,integral to infinity + imaginary constant,"A proof I'm reading tries to evaluate the integral (where $i$ is the regular imaginary unit) $$\int_{-\infty}^{\infty} e^{-(x-\alpha i)^2}\mathrm{d}x$$ by doing a substitution $u=x-\alpha i$. Normally, one would also have to change the bounds of integration. $$\int_{-\infty+\alpha i}^{\infty+\alpha i} e^{-x^2}\mathrm{d}x$$ But this proof leaves the bounds as +/- infinity. $$\int_{-\infty}^{\infty} e^{-x^2}\mathrm{d}x$$ Why is this valid?","['definite-integrals', 'complex-analysis']"
648058,Multidimensional Fourier transform of the laplacian,"In my course on electromagnetic field theory we use the Fourier transform to simplify Maxwell's equations, for example: $$\frac{\partial ^2\vec E(\vec r,t)}{\partial t^2} \rightleftharpoons -\omega^2\mathcal{F}_t\vec E(\vec r,\omega)$$ I haven't officially studied multidimensional fourier transforms but I've been exposed to them through MIT's set of courses ""The fourier transform and its applications"" (Brad Osgood). I tried transforming things like the Laplacian of a function and this is what I got: Start by considering the $\mathbb{R}^3$ FT of the second x-derivative of some nice function f and manupulate the integral to be in terms of the one dimensional FT with respect to x:
$$\mathcal{F}_{xyz}\{\frac{\partial ^2f(\vec r)}{\partial x^2}\}=\int\limits_{\mathbb{R}^3}e^{j\vec\xi\cdot\vec r}\frac{\partial ^2f(\vec r)}{\partial x^2}dxdydz=\int\limits_{\mathbb{R}^2}e^{j(\xi_2y+\xi_3z)}\mathcal{F}_x\{\frac{\partial ^2f(\vec r)}{\partial x^2}\}dydz$$
and so, using a well known identity for the FT of a derrivative:
$$ \mathcal{F}_{xyz}\{\frac{\partial ^2f(\vec r)}{\partial x^2}\}= \int\limits_{\mathbb{R}^2}e^{j(\xi_2y+\xi_3z)}(j\xi_1)^2\mathcal{F}_x\{f\}dydz=-\xi_1^2\mathcal{F}_{xyz}\{f\}$$
and since that is the case we can conclude that:
$$\mathcal{F}_{xyz}\{\Delta f\}= -(\xi_1^2+\xi_2^2+\xi_3^2)\mathcal{F}_{xyz}f= -\|\vec\xi \|^2\mathcal{F}_{xyz} f $$ The reason I have for doubting this is that if I use this to transform the homogeneous wave equation for the electric field:
 $$ \Delta \vec E-\mu\epsilon\frac{\partial ^2\vec E}{\partial t^2}=0$$
I get:
$$-( \|\vec\xi \|^2-\omega^2\mu\epsilon)\mathcal{F}\vec E=0 \Rightarrow \mathcal{F}\vec E=0$$
This seems to say that $\vec E$ is equal to zero for all points in space and time (which is definitely false). So either I've got my multidimensional FT wrong (maybe a $\mathcal{F}f=0$ does not imply $f=0$ like in the one dimensional case?) or I've got my electromagnetics wrong in which case this isn't the right forum. Anyway, I'm interested to hear anything you have to say on the subject, I've had a hard time finding anything relating the Fourier transform and vector calculus so any pointers on where to look is appreciated as well.","['multivariable-calculus', 'fourier-analysis', 'vector-analysis']"
648101,"Evaluate the contour integral $\int_{\gamma(0,1)}\frac{e^z+e^{-z}}{z^n}dz \hspace{10mm} n=1,2,3,\cdots .$","Let $\gamma(z_0,R)$ denote the circular contour $z_0+Re^{it}$ for $0\leq t \leq 2\pi$. Evaluate 
$$\int_{\gamma(0,1)}\frac{e^z+e^{-z}}{z^n}dz \hspace{10mm} n=1,2,3,\cdots .$$ Using Cauchy's formula:
\begin{align*}
\int_{\gamma(0,1)}\frac{e^z+e^{-z}}{z^n}dz & = 2\int_{\gamma(0,1)}\frac{e^z+e^{-z}}{2z^n}dz \\
& = 2\int_{\gamma(0,1)}\frac{\cosh}{z^n} \\
& = \frac{4\pi i}{(n-1)!}\left(\frac{d^{n-1}}{dz^{n-1}}\cosh(z) \right).
\end{align*}
Then evaluating at $0$ gives
$$\frac{4\pi i}{(n-1)!}\sinh(0)\hspace{10mm} \text{if $n$ is even}$$
and
$$\frac{4\pi i}{(n-1)!}\cosh(0) \hspace{10mm} \text{if $n$ is odd}.$$ Please let me know if I have made a mistake.","['complex-analysis', 'contour-integration']"
648126,Why is the value of this line integral constant,"Consider the line integral given by $$\int_C \frac{(x+y)\,dx-(x-y)\,dy}{x^2+y^2}$$ where $C$ is any simple closed curve around the origin. Can someone explain, without using complex analysis, why this is always $-2 \pi$?","['multivariable-calculus', 'calculus', 'integration']"
648132,"Darboux's Integral vs. the ""High School"" Integral","The definition of the integral below is what I usually call the ""High School definition,"" because that's usually where I've seen it in use. Take a partition $\Delta = \{ x_0, x_1, x_2, \ldots, x_n\}$, where $$a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b$$ of an interval $[a, b]$, such that $\Delta x_i = x_i - x_{i - 1}$ is constant, and define the norm of this partition $||\Delta||$ by $$||\Delta||=\frac{b-a}{n}.$$ Let $c_i$ be any point in $[x_{i-1}, x_i]$. If the following limit exists, then we call it the definite integral of a function $f$ on $[a, b]$. 
  $$\lim_{||\Delta|| \to 0}\sum_{i=1}^nf(c_i)\Delta x_i = \int_{a}^bf(x)\ dx.$$ The integral often used in more advanced textbooks (that aren't advanced enough to into Riemann-Stieltjes integrals or Lebesgue integration), which I believe is called the ""Darboux integral"", I've seen defined roughly as follows. Take a partition $\Delta$ of an interval $[a, b]$, in the same way as above, and define $$m_i=\inf\{f(x) : x_{i-1} \leq x \leq x_i \},$$ $$M_i = \sup\{ f(x) : x_{i - 1} \leq x \leq x_i \}.$$
  Then, the lower and upper sums of $f$ for the partition $\Delta$ are defined as $$L(f, \Delta) = \sum_{i = 1}^n m_i(x_i - x_{i - 1}),$$ $$U(f, \Delta) = \sum_{i = 1}^n M_i(t_i - t_{i - 1}).$$ After the hassle of proving (I'm not going to type it up, as it's lengthy and irrelevant) that for any two partitions $\Delta_1, \Delta_2$, that we have $$L(f, \Delta_1) \leq U(f, \Delta_2),$$ it is stated that if we have $$\sup\{ L(f, \Delta) : \Delta\ \mathrm{is\ a\ partition\ of\ } [a, b] \} = \inf\{ U(f, \Delta) : \Delta\ \mathrm{is\ a\ partition\ of\ } [a, b] \}$$ then we define this common value to be $$\int_{a}^{b} f(x)\ dx.$$ It seems that the ""High School definition"" is significantly simpler than the Darboux integral, so my question is: why does the latter definition often appear in more advanced texts (e.g. Spivak's Calculus )? Are these two definitions equivalent, or does each have specific advantages over the other? I assume that the first definition is preferred in most lower-level classes because it avoids the need for $\sup$'s and $\inf$'s, but this still does not explain why the second definition is used in other texts (especially because it seems to take considerably more time to construct).","['education', 'calculus', 'integration']"
648158,Euclidean space as a sum of non-intersecting and non-parrarel lines,Is it possible to represent $\mathbb{R^{3}}$ as a union of countable non-intersecting and non-coplanar lines?,['general-topology']
648180,Constant of motion in a high dimensional analogue of the Lotka-Volterra system.,"Suppose I would extend the Lotka Volterra system to the the $n$-dim first order ODE \begin{eqnarray*}
\dot{x}_{1} &=& x_1(x_2-\alpha_1)   \\
\dot{x}_{2} &=& x_2(x_3-\alpha_2)   \\
 \vdots  \; \;  &=&  \; \; \vdots \\ 
\dot{x}_{n}  &=&  x_n(x_1-\alpha_n)   
\end{eqnarray*} where $\alpha_i$ are constants and $n \geq 2$. Note that for $n=2$ we get the Lotka Volterra system upto a scaling. In the sense of Lotka-Volterra the above model is a  population model for $n$-different species where the population of each species depends on one other and where finally the last is coupled back to the first. For $n=2$ finding the constant of motion  ( a non-trivial function $f$ such that for all solution curves $x$ we have that $f(x)=$ constant) is straightforward. However, I am not completely sure how to to this for the $n$-dimension case or if it even exists. Any help is welcome!","['dynamical-systems', 'ordinary-differential-equations']"
648182,Homogenous polynomial and partial derivatives,"I'm struggling to understand this part in a book I'm reading: Let $F$ be a projective curve of degree $d$ with $P\in F$. Wlog,
  suppose $P=(a:b:1)$. Let's look the affine chart $(a,b)\mapsto
 (a,b,1)$. Let $f$ be the deshomogenization of $F$, we can write $f$ in this way: (WHY?) $$f=F(x,y,1)=f_1(x-a,y-b)+\ldots +f_d(x-a,y-b)$$ Where $f_l$ is a homogeneous polynomial of degree $l$ and we have (WHY?) $$f_l=\sum_{i+j=l}\frac{1}{i!j!}\frac{\partial^lf}{\partial^ix\partial^jy}x^iy^j$$ I know this should be a silly question, but I'm a beginner in this subject and I really need help, if anyone could help me I would be grateful. Thanks EDIT I'm thinking about Taylor's formula, but The formula of the post doesn't match with the Taylor's  one of several variables, see for example this link , maybe there is some mistake in the formula of my post?","['algebraic-geometry', 'algebraic-curves', 'abstract-algebra']"
648204,Why do Matrices work the way they do? [duplicate],This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 7 years ago . You get taught about matrices and how they work but nobody ever tells you WHY they work in the way that they do. What was the idea that sparked the creation of matrices?,"['matrices', 'soft-question']"
648211,Find minimal polynomial of this element?,"Let $f(x)=x^3+x+1$. $\alpha_1, \alpha_2, \alpha_3 - $ roots of $f$. The task is to determine the minimal polynomial of $\frac{\alpha_1}{\alpha_2}$ over $\Bbb Q $ and $\Bbb Q(\alpha_1)$. My thoughts (not sure if everything is correct) There is only one real root of $f$, since $f'=3x^2+1>0$. So $x^3+x+1=(x-b)(x-w)(x-\overline{w})$ for some $w \in \Bbb{C} \setminus \Bbb{R}$ and $b \in \Bbb{R}$. If we open brackets then we get $$ \begin{cases}b=-w-\overline{w}\\ b(w+\overline{w})+w\overline{w}=1\\aw\overline{w}=-1\end{cases}$$ This set of equations might somehow lead to the fact that $\Bbb Q (\frac{\alpha_1}{\alpha_2}) = \Bbb Q(\alpha_1, \alpha_2, \alpha_3)$ and $[\Bbb Q (\frac{\alpha_1}{\alpha_2}):\Bbb Q] = 6$ (which should be if we take a look at first equation), regardless what roots we call $\alpha_1, \alpha_2, \alpha_3$. So we get that the minimal polynomial of $\frac{\alpha_1}{\alpha_2}$ has degree 6. And since $[\Bbb{Q}(\alpha_1):\Bbb{Q}]=3$, then the minimal polynomial over $\Bbb{Q}(\alpha_1)$ must have degree 2. But how can I figure out what are they? Thanks in advance!","['abstract-algebra', 'field-theory']"
