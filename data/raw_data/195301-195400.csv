question_id,title,body,tags
3759536,"Let $b \in [0,1)$. Prove that $\frac{b}{1-b} \in [0,\infty)$","Can someone check my solution for this problem? It seems to me that it’s incomplete, and I’m not sure. Problem: Let $b \in [0,1)$ . Prove that $\frac{b}{1-b} \in [0,\infty)$ . Solution: We know that $b \in [0,1)$ , so $0 \leq b < 1$ . From here we can also deduce that $ 0 < 1-b \leq 1$ . So $\frac{1}{1-b} \geq 1$ . Multiplying by $b$ we obtain that $\frac{b}{1-b} \geq b$ . Since $b \geq 0$ we conclude that $\frac{b}{1-b} \geq 0$ . Therefore $\frac{b}{b-1} \in [0,\infty)$ .","['algebra-precalculus', 'solution-verification', 'inequality', 'real-analysis']"
3759552,Purely geometric proof of inverse trigonometric functions derivatives,"Can you compute the derivatives of $\sin^{-1}(x),\cos^{-1}(x),$ and $\tan^{-1}(x)$ using only geometry? I know how to use geometry to find the derivatives of $\sin x$ and $\cos x$ like this: We can use the fact that we know the tangent of the circle to show that $\frac{d}{dx}\cos(x)=\sin(x)$ . Wondering if you can do the same with the inverse functions.","['trigonometry', 'calculus', 'proof-writing', 'geometry']"
3759558,Negative expected value for iid standard normal random variables,"I would like to show analytically that: $$\Bbb{E}\left[\frac{(X_2-X_1)(Y_2-Y_1)}{|X_2+Y_2-X_1-Y_1|}\right] < 0$$ I would be happy just showing that this is true when $(X_1,Y_1,X_2,Y_2)$ are iid standard normal random variables, but a more general demonstration would be even better. Montecarlo integration shows the expectation to be strongly negative (much less than zero) for pretty much any distribution governing the $X_1, Y_1, X_2, Y_2$ .","['integration', 'probability']"
3759561,Understanding Complex Form of Green's Theorem,"I'm reviewing complex analysis for the GRE. I've never taken a course in complex analysis before, but I do know vector calculus. I'm trying to understand the statement of the complex version of Green's theorem, which has a few symbols that I've never seen before. $$ \oint_C F(z,\overline{z}) dz = 2i \iint_{R} \frac{\partial F}{\partial \overline{z}} dA $$ For example, what is $F(z,\overline{z})?$ How is $\partial F/\partial \overline{z}?$ defined? Also, if there is an obvious connection to Green's theorem on a plane (the real version) I would appreciate an explanation!","['complex-analysis', 'greens-theorem', 'complex-integration', 'vector-analysis']"
3759579,How many elements of order $2$ does Sym $6$ have?,"First, I will answer the following question: ''How many elements of order $2$ does Sym $5$ have?'' The answer is: $(12),(13),(14),(15),(23),(24),(25),(34),(35),(45),(12)(34),(12)(35),(12)(45),(13)(24),(13)(25),(13)(45),(14)(23),(14)(25),(14)(35),(15)(23),(15)(24),(15)(35), $ that is, there are 22 elements of order $2$ does Sym $5$ have. I omitted 3 products of two transpositions; the correct number for S5 is 25. Thanks @BrianM.Scott How many elements of order $2$ does Sym $6$ have? I can compute as a manual but it will be too long. Is there any easy method to find it? Thanks...","['permutations', 'group-theory']"
3759622,"For a complex number $\alpha $ which is algebraic over $\Bbb Q$, determining whether $\bar{\alpha}\in \Bbb Q(\alpha)$ or not","Let $\alpha =3^{1/3}+3^{5/4}i$ , which is clearly algebraic over $\Bbb Q$ . How can we determine whether $\Bbb Q(\alpha)$ contains $\bar{\alpha}$ or not? This would be certainly true if $\Bbb Q(\alpha)$ is normal (hence Galois) over $\Bbb Q$ , because $\bar{\alpha}$ is a root of the minimal polynomial of $\alpha$ over $\Bbb Q$ . But in this case it doesn't seem easy to determine whether $\Bbb Q(\alpha)$ is normal over $\Bbb Q$ , neither computing the minimal polynomial of $\alpha$ over $\Bbb Q$ .","['field-theory', 'galois-theory', 'abstract-algebra', 'extension-field', 'galois-extensions']"
3759675,How many ways can this hexagon be tiled by 11 rhombuses of unit side length?,"I came across a question in an exercise booklet for Mathematic Olympiad for primary school students in Australia. The question is shown in the following picture: I barely have any clue how this sort of questions can be approached. Would anyone please help solve this question and possibly suggest a general solution, if any, to this kind of problems. Thank you very much!","['combinatorics', 'geometry']"
3759693,Finding the parameters of curves of rotations in rotation space,"I asked this a while ago: For more background information... .  Not required for this question... 2020/07/24 - changes at end I have a vector, it contains 3 curvatures. A curvature is a change in angle over time, it is either an unit circle/sphere arc-length or angle. because this is purely about curvature, 'axis' will often be referred to as 'axle', since each component represents the curvature of the perpendicular plane around the axle. The sum of the absolute value of the 3 components is the total curvature. (divided by 2 somewhere) For a specific ( X , Y , Z ) vector there is a specific orientation; this can be demonstrated using the Show Basis Map or Show Inverse Basis Map options. $$\begin{matrix}theta= (|X|+|Y|+|Z|)/2 * d_T\\
q_r = cos(theta)\\
q_i = sin(theta) * X / \sqrt(XX+YY+ZZ)\\
q_j = sin(theta) * Y / \sqrt(XX+YY+ZZ)\\
 q_k = sin(theta) * Z / \sqrt(XX+YY+ZZ)\end{matrix}$$ \begin{bmatrix}1-2q_{j}^{2}-2q_{k}^{2}&2(q_{i}q_{j}-q_{k}q_{r})&2(q_{i}q_{k}+q_{j}q_{r})\\2(q_{i}q_{j}+q_{k}q_{r})&1-2q_{i}^{2}-2q_{k}^{2}&2(q_{j}q_{k}-q_{i}q_{r})\\2(q_{i}q_{k}-q_{j}q_{r})&2(q_{j}q_{k}+q_{i}q_{r})&1-2q_{i}^{2}-2q_{j}^{2}\end{bmatrix} $$ up = 2(q_iq_j-q_kq_r, 1/2-q_i^2-q_k^2, q_jq_k+q_iq_r) $$ For a specific ( $X$ , $Y$ , $Z$ ) vector there is a curve of related ( $X_n$ , $Y_n$ , $Z_n$ ) curvatures that share the same normal.  This demonstration is just showing the curve of rotation vectors that share the same normal as some specified input (controlled by the X , T , and Z sliders; T is the y rotation... the existing Y slider controls the target rotation; which is the destination point around the curve) The curve is formed by linear interpolation from the intersection points where the unit circle centered at the origin, and the normal of the curve of rotations that share the same normal(up/green) : $((X,Y,Z)/\sqrt(XX+YY+ZZ)) × up(d_T=1/2)$ .  This substitution results in a double-angle sin/cos substitution, so the /2 of the normal is not used; but IS used anyway because of the 0.5 dT scalar... (represented with *0.5 instead of /2 ) $ 		x = { X \over \sqrt(XX+YY+ZZ) }$ , $ 		y = { X \over \sqrt(XX+YY+ZZ) } $ , $ 		z = { X \over \sqrt(XX+YY+ZZ) } $ , $ t = { |X|+|Y|+|Z| } * 0.5 $ $  curve_{up_x} = {z * ( 1 - ( 1/2 - \cos(t) ) ( zz - xx  - yy ) ) - \sin(t) * yx } $ $		curve_{up_y} = {    \sin(t) * ( xx - zz ) } $ $		curve_{up_z} = { x * ( 1 - ( 1/2 - \cos(t) ) ( zz -  xx - yy ) ) - \sin(t) * yz }$ (untested; algebraic substitution) the tangent(right/red) is : $up(d_T=1/2)$ the bi-tangent(forward/blue) is : $normal × tangent$ These are the plane crossings where the slope changes from one crossing to the next... There are a maximum of 6 plane crossings and a minimum of 4, if the curve lies exactly in a plane (has a 0 curvature component for that axle).  The x y and z plane intersections are... ( $plane_{coordinate}$ ) $x_x=0$ ; $x_y =2* -curve_{up_z}/(|curve_{up_y}|+| curve_{up_z}|)$ ; $x_z =2* curve_{up_y}/(|curve_{up_y}|+ |curve_{up_z}|)$ $y_x =2* curve_{up_z}/(|curve_{up_z}|+|curve_{up_x}|)$ ; $y_y=0$ ; $ y_z = 2* -curve_{up_x}/(|curve_{up_z}|+|curve_{up_x}|)$ $z_x =2* -curve_{up_y}/(|curve_{up_x}|+| curve_{up_y}|)$ ; $z_y =2* curve_{up_x}/(|curve_{up_x}|+|curve_{up_y}|)$ ; $z_z=0$ These can also be computed for the square normal - that lies in the circle formation instead of the linear; though I expect updating the unit X , Y and Z components will be easier with a linear adder. (A) For each step around the curve, there is a certain total rotation of that system, this is shown as a graph on the X / Y plane that has a characteristic sin/cos wave to it, and the max+min rotation is always 2π. The scalar of this curve is a component I miss. $ Y = |X|+|Y|+|Z|, X = T $ (B) for each step, the angle around the curve changes by a varying amount, this is shown on the X / Y plane, and shows the angle change per step of unit angle around the curve; this $d_{Angle}/d_T$ is one component I miss.  This curve is created with the normal of the rotation... $acos(Rotation_t · Rotation_{t-1} )$ as $t$ steps around the curve. Question - what are the functions that generate (A) and (B); right now all I have is a plot, and all the numbers that went into generating those points on the curve. Reason for the question; Given any arbitrary axis of rotation, these functions will generate the angle-angle-angle updates without translating to a matrix and making assumptions about the reverse translation. Additional Notes: There are distinct intervals or 'octaves' which for a spin of -2pi to 2π, 2π to 6π, -6pi to -2pi, etc... the relative rotations to stay within that octave are themselves distinct curves... There is a separate sort of calculation to reduce or increase curvature from one octave to another.... This is an overview.
The axles are labeled with their color in script font X , Y , Z . There is a graph (labeled in gold), which is the total sum of the rotation vector: Question part 1) What parameters define this curve?
There is another graph (labeled in olive/green), which is the angle step around the curve for each step of twist: Question part 2 ) what parameters define this curve? https://d3x0r.github.io/STFRPhysics/3d/index2.html https://github.com/d3x0r/STFRPhysics/blob/master/3d_index2_html.png (explainer 1) https://github.com/d3x0r/STFRPhysics/blob/master/3d_index2_html-2.png (explainer 2) Edit: Added options to enable/disable single curves, I realized others will not be as familiar with the character of a curve to be able to immediately identify it. Reverse tangent and bi-tangent colors; looking at the graph, because Three.JS is reversed, and is left-handed, the 'right' should always appear 'left' which means the blue and red previously defined for the forward/right of the rotation curve were backwards. (attached images still are incorrect) Modified cross product of rotation with up(1/2); it should be the square-normal of the coordinates 2020/07/24 new rotation explorer version... https://d3x0r.github.io/STFRPhysics/3d/index3.html https://github.com/d3x0r/STFRPhysics/blob/master/Twister.md The new demo includes sliders to set the axis of rotation within the specified rotation frame.  This starts of as (0,N,0) which is similar to the original rotate around yaw() operation, but now the actual rotation vector can be controlled.  For very high rotation values, the octave will auto-trigger to jump rings. Partial Rodriguez Implementation Inputs to this are $yaw_{angle}$ , $rotation_{axis}$ , and a q which has {x,y,z} as the primary values and the following partial products.. {nx,ny,nz} as the normalized spin values (square axis of rotation), {nL} which is the linear normal or |x|+|y|+|z| , {nR} which is the square normal or sqrt(xx+yy+zz) , {qw,s} are the cos/sin of nL/2 (qw represents the exp() partial result... converting from log-quaternion to quaternion sets w of the quaternion as cos(theta/2) , so qw is the pre-computed quaternion partial w ); and as a side note - the 'w' part of rotation log-quaterions is always 0 (again, exp(0)=1 ). Was reading https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#The_composition_of_spatial_rotations and saw $$\cos\frac{\gamma}{2} = \cos\frac{\beta}{2}\cos\frac{\alpha}{2} - 
\sin\frac{\beta}{2}\sin\frac{\alpha}{2} \mathbf{B}\cdot \mathbf{A}$$ which, before that goes a step too far, is just perfect.  This is a very long expression: $$cos_{over2} = \cos(|x|+|y|+|z|/2) * \cos(yaw_{angle}/2) + \sin(|x|+|y|+|z|/2) * \sin(yaw_{angle}/2) * (x*basis_{up_x}+y*basis_{up_y}+z*basis_{up_y})$$ And then compute the result angle... $$result_{angle} = 2*arccos( cos_{over2} )$$ the Dot Product is also the cos(angle between axles of rotation).  Basis up can be retrieved from above matrix. [ax,ay,az] is the normalized, axis of rotation to rotate around. $$\begin{matrix} [ax,ay,az] = ||rotation_{axis}||\\
as = sin(yaw_{angle})\\ ac = cos(yaw_{angle})\\ q.s = sin( original_{angle})\\ q.qw=cos(original_{angle}) \end{matrix} $$ $$ \begin{matrix} Cx = as * q.qw * ax + q.s * ac * q.nx + q.s*as*(ay*q.nz-az*q.ny)\\
	Cy = as * q.qw * ay + q.s * ac * q.ny + q.s*as*(az*q.nx-ax*q.nz)\\
	Cz = as * q.qw * az + q.s * ac * q.nz + q.s*as*(ax*q.ny-ay*q.nx)\\
sin_{angle} = \sin(result_{angle}/2) \\
C_{norm} = sin_{angle}*(|Cx/sin_{angle}|+|Cy/sin_{angle}|+|Cz/sin_{angle}|);\\
result_{x_{Angle}} = Cx/C_{norm}*result_{angle}\\
result_{y_{Angle}} = Cy/C_{norm}*result_{angle}\\
result_{z_{Angle}} = Cz/C_{norm}*result_{angle}
\end{matrix}  $$ The remaining results can be delay computed later from the above result values, however, these values are already just laying around from the above computation anyway... $$\begin{matrix}
 result_{normal_x} = Cx/sin_{angle}\\
result_{normal_y} = Cy/sin_{angle}\\
result_{normal_z} = Cz/sin_{angle} \\
result_{normal_{linear}} = result_{angle}/2\\
result_{normal_{rect}} = sin_{angle}/C_{norm}*result_{angle}\\
result_{qw} = cos_{over2}\\
result_{s} = sin_{angle} \\
\end{matrix}  $$ Using this transformation, any arbitrary axle can be specified, the axis can have the log-quaternion applied to put the axle relative to the rotation, and then use, or the axles to rotate around can be gotten from any log-quaternion basis for roll , pitch and yaw operation.  Any of these axles of rotation have a smooth curve in rotation space which is their (square normal(result_n?) * total angle (|x|+|y|+|z|) ). I expect the parameters of curvature is like the derivative of the above; which I probably don't need now that the rotations result in at least a good coverage of the rotation space naturally. This error correction code can apply, so the result, which resembles( arccos(cos(a+b)) which should be approximately a+b , if the result angle is far from the input angles, attempt to correct the octave. let fix = ( result_angle - (q.nL+yaw_angle))
    // - result_angle is always -π to π
    // - q.nL is always positive (the individual x/y/z signs determine directions)
    // - yaw_angle is often small, but may be positive or negative...
    //
    // this correction USUALLY just increments the angle to match the 
    // lnQ's octave, but the yaw_angle may change octaves...
    while( fix < -Math.PI*4 ){
        ang += Math.PI*4;
        fix += Math.PI*4;
    }
    while( fix > Math.PI*4 ){
        ang -= Math.PI*4;
        fix -= Math.PI*4;
    }","['differential-topology', 'rotations', 'differential-geometry']"
3759707,Deletion of given matrix entry via matrix product,"Consider a matrix $A \in \mathbb{R}^{n \times n}$ . Is it possible to define an operator $\phi(i,j)$ which, when applied to matrix $A$ , will delete the $ij$ -th element, so that $[ \phi(i,j) A ]_{i,j} = 0$ ? If so, is it possible to define this operator in terms of the standard matrix product or the Kronecker product? If not, is there a proof that such an operator does not exist?","['matrices', 'linear-algebra', 'linear-transformations']"
3759710,"If $p$ is an odd prime, exactly half the elements of $\mathbb{U}_p$ are squares. [duplicate]","This question already has answers here : Nonzero integers mod $p$ - showing exactly half are perfect squares (3 answers) Closed 10 months ago . I'm trying to prove the following: If $p$ is an odd prime, then exactly half the elements of $\mathbb{U}_p$ are squares. Where $\mathbb{U}_p$ refers to the units of $\mathbb{Z}_p$ . It seems somewhat intuitive that no more than half could be squares, but I'm struggling to prove that exactly half are squares. Any ideas/hints?","['number-theory', 'elementary-number-theory', 'abstract-algebra', 'elementary-set-theory', 'prime-numbers']"
3759724,"Show that an element of $GL_2(\mathbb{Z})$ has order $1,2,3,4,6$ or $\infty$.","Show that an element of $GL_2(\mathbb{Z})$ has order $1,2,3,4,6$ or $\infty$ . My idea: $GL_2(\mathbb{Z})=\{A|A_{2 \times 2}$ & $det(A)\neq0\}$ . Let $A\in GL_2(\mathbb{Z})$ , and order of $A$ is $n\geq 2$ . $A^n=I$ , the characteristic polynomial is $x^n-1=0$ . $x^n-1=(x-1)(x^{n-1}+x^{n-2}+......+x+1)=0$ . I know that the cyclotomic polynomial is an irreducible monic polynomial in $\mathbb{Z}[x]$ of degree $\phi(n)$ . I confused how do I conclude that $n=2,3,4,6$ or $\infty$ ? Can anyone suggest me some hint?","['group-theory', 'abstract-algebra']"
3759725,"Manipulate $f(a,b,c,d) = \frac{a}{b} - \frac{c}{d}$ so that the resulting expression can be seen as $g(a-c, b-d)$","Is there a clever way to manipulate the difference $$f(a,b,c,d) = \frac{a}{b} - \frac{c}{d}$$ in such a way that the resulting expression can be seen as $g(a-c, b-d)$ , i.e. a function of just  the difference $a-c$ (or $c-a$ ) and $b-d$ (or $d-b$ )?","['algebra-precalculus', 'functions']"
3759762,"If $A^m = 0$, then $\mbox{rank}(A) \leq \frac{m-1}{m}{n}$ [duplicate]","This question already has answers here : Upper bound for the rank of a nilpotent matrix , if $A^2 \ne 0$ (4 answers) Closed 3 years ago . Let $A$ be a $n \times n$ real matrix. Show that if $A^m = 0$ , then $\mbox{rank}(A) \leq \frac{m-1}{m}{n}$ My attempt: If $m=1$ , then $A=0$ so $\mbox{rank}(A)=0$ . If $m=2$ , we have $\mbox{im}(A) \subset \ker(A)$ so $2\operatorname{rank}(A) \leq \dim \mbox{im}(A) + \dim \ker(A)=n$ For arbitrary $m$ , I want to use induction. $B=A|_{\mbox{im}(A)}$ satisfies $B^{m-1}=0$ so $\mbox{rank}(B) \leq \frac{m-2}{m-1}\mbox{rank}(A)$ . Thus $\dim \ker B \geq \mbox{rank}(A)- \frac{m-2}{m-1}\mbox{rank}(A) =\frac{1}{m-1}\mbox{rank}(A)$ by rank-nullity theorem. Thus $n=\dim \ker A + \mbox{rank}(A) \geq  \dim \ker B +\mbox{rank}(A)\geq \frac m {m-1}\mbox{rank}(A)$ Is this ok?","['inequality', 'solution-verification', 'linear-algebra', 'matrix-rank']"
3759771,Find all automorphisms of the multiplicative group mod $n$,"The question is to determine all pairs of integers $(m, n)$ such that $f (a) = a^m$ maps a reduced system of residues modulo $n$ bijectively to itself. I have only found those trivial automorphisms given by $m\equiv1 \mod \phi(n)$ , where $\phi$ is the Euler Totient Function. However, I'm not sure if there is any non-trivial automorphisms given by such $f$ . Any help and I would be grateful.","['number-theory', 'group-theory', 'abstract-algebra']"
3759779,How many 6 digit numbers can be formed from two sets of digit?,"There are two sets of digit : $ \text{set 1 :} \{~0,1,2,3,4~\}$ $ \text{set 2 :} \{~5,6,7,8,9~\}$ Now how many 6 digit number can we make by taking numbers from these two sets ? From $\text{set 1}$ repetition is permitted but from $\text{set 2}$ repetition is prohibited . Solution :
The possible combinations are : \begin{array}{c|cccc}
\ combo &\text{group 1}&  \text{group 2} & \text{without considering 0} &\text{considering 0}&\text{result}\\
\hline
1 & 6 & \fbox0 & 5^6 & 5^6 - 5^5 & 12,500\\
2 & 5 & \fbox1 & 5 \times 5 \times 5 \times 5 \times 5 \times \fbox 5& 5 \times 5 \times 5 \times 5 \times 5 \times \fbox 5  - 5^5 & 12,500\\
3 & 4 & \fbox2 & 5 \times 5 \times 5 \times 5 \times \fbox {$5 \times  4 $}& (5 \times 5 \times 5 \times 5 \times \fbox {$5 \times  4 $})- (5 \times 5 \times 5 \times \fbox {$5 \times  4 $}) & 10,000\\
4 & 3 & \fbox3 & 5 \times 5 \times 5 \times \fbox {$5 \times  4 \times  3 $}& (5\times 5 \times 5 \times \fbox {$5 \times  4 \times  3 $})-(5 \times 5 \times \fbox{$5 \times  4 \times  3 $}) & 6000 \\
5 & 2 & \fbox4 & 5 \times 5 \times \fbox {$5 \times  4 \times  3 \times  2$} & (5 \times 5 \times \fbox {$5 \times  4 \times  3 \times  2$}) - ( 5 \times \fbox {$5 \times  4 \times  3 \times  2$}) & 2400\\
6 & 1 & \fbox5 & 5 \times \fbox {$5 \times 4 \times  3 \times  2 \times  1$} & (5 \times \fbox {$5 \times 4 \times  3 \times  2 \times  1$}) - (\fbox {$5 \times 4 \times  3 \times  2 \times  1$}) & 480\\
\end{array} So,total number of possible combinations is $= 43880$ Is this procedure correct ?","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
3759812,"Give an example of distribution $u \in \mathcal{D}'((0,+\infty)) $ that is not extendible to $\mathbb{R}$","i.e. find a $u \in \mathcal{D}'((0,+\infty))$ , such that for any $v \in \mathcal{D}'(\mathbb{R})$ , $v|_{(0, +\infty)} \neq u$ . In order to find such an example, my question: I tried to prove $e^{1/x^2}$ is an example, i.e. there is a distribution u such
that $ u|_{(0, +\infty)} = e^{1/x^2}$ , but I cannot find a contradiction.","['distribution-theory', 'functional-analysis', 'partial-differential-equations']"
3759838,"When does $\mathrm{Cov}[g(X),h(X)] \ge 0$ hold for all nondecreasing $g$ and $h$ on $\mathbb R^n$?","If $X$ is a random variable then the covariance of two nondecreasing transformations of $X$ is nonnegative (see here ). This fact is very intuitive but it turns out that it does not immediately generalize to random vectors. I am wondering what is known about the class of random vectors for which this condition holds. To formalize this question, assume from now on the following: A function $g: \mathbb R^n \to \mathbb R$ is called nondecreasing if it is nondecreasing in each component. Let $$X=(X_1,\dots,X_n): (\Omega, \mathcal A) \to (\mathbb R^n,\mathcal B^n)$$ be a random vector defined on the probability space $(\Omega, \mathcal A, P)$ . Let $g, h: \mathbb R^n \to \mathbb R$ be measurable functions with $E[g(X)^2]<\infty$ , $E[h(X)^2]<\infty$ . Theorem 1: If $g$ and $h$ are nondecreasing and $n=1$ then $$\mathrm{Cov}[g(X),h(X)] \ge 0.$$ Theorem 2: If $g$ and $h$ are nondecreasing and $n\ge 2$ then $\mathrm{Cov}[g(X),h(X)] \ge 0$ does not necessarily hold. Question: For $n\ge 2$ , what is known about the class of random vectors $X$ for which $$\mathrm{Cov}[g(X),h(X)] \ge 0\tag{*}$$ holds whenever $g$ and $h$ are nondecreasing? Some observations: Note that $(*)$ is the same as $$E[g(X)h(X)] \ge E[g(X)]E[h(X)].$$ If $X$ is such that $(*)$ holds for any nondecreasing $g$ and $h$ and $X$ possesses finite second moments then all components of $X$ are nonnegatively correlated. To see this, simply choose $g(x) = x_i$ and $h(x) = x_j$ for $i, j = 1,\dots, n$ . Theorem 3: $(*)$ holds for any nondecreasing $g$ and $h$ if the components of $X$ are stochastically independent. Proof of Theorem 1: See here . Proof of Theorem 2: Let $n = 2$ and suppose $X=(X_1, X_2)$ takes on the values $(1,0)$ and $(0,1)$ with probability $0.5$ each. Define measurable, nondecreasing functions by $g(x_1, x_2) = x_1$ and $h(x_1, x_2) = x_2$ . Then \begin{align}
\mathrm{Cov}[g(X), h(X)] &= E[g(X)h(X)]-E[g(X)]E[h(X)]\\
                         &= E[X_1 X_2]-E[X_1]E[X_2]\\
                         &= 0-0.5\times0.5\\
                         &= -0.25.
\end{align} Proof of Theorem 3: We proceed by induction on the number of components $n$ . If $n=1$ we can simply apply Theorem 1. Now suppose $X$ has $n+1$ components. Write $X=(X_1, Y)$ where $Y$ consists of the last $n$ components. Then $$E[g(X_1,Y)h(X_1,Y)|Y=y] \ge E[g(X_1,Y)|Y=y]\,E[h(X_1,Y)|Y=y]$$ since $g(\cdot,y)$ and $h(\cdot,y)$ are nondecreasing functions for any $y$ . Therefore \begin{align}
E[g(X)h(X)] &= E[\,E[g(X_1,Y)h(X_1,Y)|Y]\,] \\
            &\ge E[\,E[g(X_1,Y)|Y]\,E[h(X_1,Y)|Y]\,].
\end{align} further, since $Y$ and $X_1$ are independent, $$\tilde g(y) := E[g(X_1,Y)|Y=y] = E[g(X_1,y)]$$ is a nondecreasing function of the $n$ -dimensional vector $y$ . Hence, by applying the induction assumption, \begin{align}
E[\,E[g(X_1,Y)|Y]\,E[h(X_1,Y)|Y]\,] &= E[\,\tilde g(Y)\,\tilde h(Y)\,]\\
                                    &\ge E[\,\tilde g(Y)]\,E[\tilde h(Y)\,] \\
                                    &= E[g(X)]\,E[h(X)].
\end{align} It follows that $E[g(X)h(X)] \ge E[g(X)]E[h(X)].$","['measure-theory', 'real-analysis', 'soft-question', 'probability-theory', 'probability']"
3759842,"All solutions of $\frac{n}{2z} = \sum\limits_{i=1}^n \frac{1}{z-c_i}$ lie on the unit circle given that $|c_i| = 1, 1 \le i \le n.$","Prove that all solutions of $\frac{n}{2z} = \sum\limits_{i=1}^n
\frac{1}{z-c_i}$ lie on the unit circle given that $|c_i| = 1$ for $1
\le i \le n.$ If $n=1,$ then $z = -c_1.$ If $n=2,$ then $z = \pm \sqrt{c_1 c_2}.$ Trying to prove the statement through brute force for $n \ge 3$ quickly becomes unfeasible. I tried a proof by contradiction by assuming that $|z| < 1$ or $|z| > 1$ : $\frac{n}{2|z|} = \left|\sum\limits_{i=1}^n \frac{1}{z-z_i}\right| \le \sum\limits_{i=1}^n \frac{1}{|z-z_i|} \le \frac{n}{|1-|z||}.$ If $|z| > 1,$ this gives $-1 \le |z|,$ which we already know. If $|z| < 1,$ this gives $|z| \ge 1/3,$ which is not helpful enough. Thus, we must take into account the argument of the LHS and RHS of the original equation. But assuming that a point lies off of the unit circle does not place any constraints on its argument, so we can't get a contradiction that way either. We must somehow consider the argument and magnitude of $z$ simultaneously. How do we do this? Any approaches, hints, or ideas? What would be the motivation behind these approaches? Is there something obvious I'm missing?","['complex-analysis', 'polynomials', 'complex-numbers']"
3759843,Write down an expression in the form $ax^n$ for: $\lim_{h\to 0} \frac{\sqrt{x+h}-\sqrt{x}}{h}$,Write down an expression in the form $ax^n$ for $$\lim_{h\to 0} \frac{\sqrt{x+h}-\sqrt{x}}{h}$$ What I have tried so far: multiplying by the conjugate to give: $$\lim_{h\to 0} \frac{\sqrt{x+h}-\sqrt{x}}{h} \cdot \frac{h}{\sqrt{x+h}+\sqrt{x}}$$ so we cancel out $h$ : $$\lim_{h\to 0} \frac{\sqrt{x+h}-\sqrt{x}}{\sqrt{x+h}+\sqrt{x}}$$ as $\lim_{h\to 0}$ : $$\lim_{h\to 0} \frac{\sqrt{x+0}-\sqrt{x}}{\sqrt{x+0}+\sqrt{x}}$$ giving: $$\lim_{h\to 0} \frac{\sqrt{x}-\sqrt{x}}{\sqrt{x}+\sqrt{x}}$$ So I'm not sure how you get the numerator to equal $1$ which would give the correct given answer: $0.5x^{-0.5}$ I think I'm missing a simpler method to obtain this answer - any hints would be much appreciated!,"['limits', 'calculus', 'solution-verification', 'radicals']"
3759940,Taking derivative with function of multiple variables?,"Suppose we are studying the function $$ 
f(x,y) = xy + ax^2 + bx^2y^2,
$$ We want to find the maximum $x$ satisfying the equation $$ 
f(x,y) = c,
$$ where $a, b, c$ are constants. Somebody suggested to make use of the following auxiliary function $$ 
g(x,y) = xy,
$$ so that $$ 
g + ax^2 + bg^2 = c.
$$ Isolating $x$ , $$
x^2 = \frac{c-g-bg^2}{a}.
$$ Now he says that the same condition as $ \frac{dx}{dy} = 0 $ is $$ \frac{dx^2}{d g(x,y)} = 0$$ Why? Edit : I saw this trick here .","['calculus', 'derivatives']"
3760008,Proving that the solution to a nonlinear oscillator equation is bounded,"I am working on a fluid-structure-interaction evolution problem: let us suppose that the motion of the rectangular body $B$ , with boundary $\partial B=S$ , immersed in an unbounded 2D channel $\mathbb{R}\times (-L,L)$ , is governed by a nonlinear oscillator equation with restoring force $f=f(h)$ , and forced by the fluid lift exerted on $B$ : \begin{equation}\label{eq:ODE}
  \ddot{h}+f(h)=-\hat{e}_2\cdot{\langle \mathbf{T}\cdot\hat{n},1\rangle}_S \qquad \text{in}\,\,(0,T),\qquad (1)
\end{equation} where $\mathbf{T}= \mathbf{T}(u,p)=-p\mathbf{I}+\mu[\nabla u+(\nabla u)^T]$ . We suppose that $u\in L^2(0,T; H^1(\Omega))$ , velocity field of the fluid, is $\textit{known}$ and thus \begin{equation}
 \mathbf{T}(u,p)\cdot\hat{n}\in L^2(0,T;W^{-\tfrac{2}{3},\tfrac{3}{2}}(S)),
\end{equation} In (1), ${\langle \cdot,\cdot\rangle}_S$ precisely labels the duality between $W^{-\tfrac{2}{3},\tfrac{3}{2}}(S)$ and its dual space $W^{\tfrac{2}{3},3}(S)$ . We assume that $f(h)$ satisifes the following hypotheses: $f\in C^1(-L+1,L-1)$ and \begin{equation}
f'(h)>0\, \forall h\in (-L+1,L-1), \quad \lim_{|h|\to L-1}\frac{|f(h)|}{(|L-1|-|h|)^{3/2}}=+\infty
\end{equation} The ODE (1) enjoys existence and uniqueness of solutions $h\in H^2(0,T)$ . How do I prove that this solution is also $\textit{bounded}$ ?","['ordinary-differential-equations', 'fluid-dynamics']"
3760025,Does the category of $O_X$-Modules have exact products,"I was wondering if the category of $O_{X}$ -Modules over a scheme $X$ has exact products. I've searched on the internet and found that it is a Grothendieck category (something that i am not familiar with but I figured that this doesn't imply the exactness of the direct products). Also, I would like to know, if this is not the case, whether the category of quasi-coherent $O_{X}$ -Modules has exact products. Since in this post it is clear that this is not always true, I claim that $X$ should be quasi-compact,quasi-separated but I can't prove anything and I am stuck. Any help or recommended reference would be very helpful.","['homological-algebra', 'category-theory', 'algebraic-geometry', 'infinite-product', 'soft-question']"
3760115,Derivative of vector with vectorization,"I have these constraints on a cost function $$
c = A+Bx=A+B\text{vec}\ (q^*q^\top),
$$ where $(c,A)\in\mathbb{R}^{100}$ , $B\in\mathbb{C}^{100\times 81}$ , $x\in\mathbb{C}^{81}$ and $q\in\mathbb{C}^9$ . So $x=\text{vec}\ (q^*q^\top)$ , which is the vectorization operator. I want to speed up my optimizer and therefore i require the gradient of the constraints (with respect to $q$ ). This is how far i have come: $$
\begin{aligned}
dc = Bdx &= Bd\text{vec}\ (q^*q^\top)\\
&=B\text{vec}\ (q^*dq^\top+dq^*q^\top) \\
&=B\text{vec}\ (q^H:dq)+B\text{vec}\ (q^\top:dq^*)
\end{aligned}
$$ However, i cannot seem to get rid of the $\text{vec}$ operator. If i ""matricize"" the left side to remove the vectorization at the right side, i cannot get to $\frac{\partial c}{\partial q}$ anymore. Anyone got some brilliance for me? Update : The last line of my derivation is incorrect i think. $q^H\in\mathbb{C}^{12}$ while $dq\in\mathbb{C}^{1\times 12}$ , so you cannot use the Frobenius product here.","['matrix-calculus', 'linear-algebra', 'vector-analysis', 'optimization', 'derivatives']"
3760164,Partial fractions zero coefficients,"In partial fractions, there can be terms in the coefficients in the partial form that turn out to be zero. One case of this is when the variable is ""linear"" in a power of $x$ , for example, found in this similar question $\frac{1}{x^2(x^2+4)}=\frac{A}{x^2}+\frac{B}{x}+\frac{Cx+D}{x^2+4}$ , and because of this, we intuitively know C is zero. This is sufficient in this case, but not necessary. For example, $\frac{3x+3}{(x^2+x+2)(x^2+4x+5)}=\frac{Ax+B}{x^2+x+2}+\frac{Cx+D}{x^2+4x+5}$ . Solving, $A=0\ B=1\ C=0\ D=-1$ . Is there, either a necessary condition in general or other sufficient conditions?","['algebra-precalculus', 'partial-fractions']"
3760190,differentiate integral in t by x,can somebody explain to me why the derivative of $ h(x) = \int_1^x \ln\lvert \cos(e^\sqrt{t}) + 2 \rvert dt $ is: $ h(x)' = \ln\lvert \cos(e^\sqrt{x}) + 2 \rvert $ Why does only the variable change? Thanks,"['definite-integrals', 'derivatives']"
3760290,Calculating the $\dfrac{d}{dx} \arccos(x)$ with derivative definition.,"I was asked to find the derivative of $\arccos$ $x$ with the definition of derivative . I know I have to form this limit. $f^{'}(c)= $ $\displaystyle{\lim_{h\to0}\dfrac{f(h+c)-f(c)}{h}}$ or $f^{'}(c)= $ $\displaystyle{\lim_{x\to c}\dfrac{f(x)-f(c)}{x-c}}$ which $-1<c<1$ (two limits are actually the same) I formed the first limit which is $\displaystyle{\lim_{h\to0}\dfrac{\arccos(h+c)-\arccos(c)}{h}}$ and the second limit which is $\displaystyle{\lim_{x\to c}\dfrac{\arccos(x)-\arccos(c)}{x-c}}$ I tried to use this equation: $$\arccos(x)+\arccos(y)=\arccos\left(xy-\sqrt{(1-x^2)(1-y^2)}\right) $$ but I failed
and except that, I have literally NO idea how to calculate these limits.","['derivatives', 'real-analysis']"
3760313,Closed form of $\sqrt{2 + \sqrt{2 + ... + \sqrt{2 + \sqrt{x}}}}$,"How to prove the following formula, $$\sqrt{2 + \sqrt{2 + ... + \sqrt{2 + \sqrt{x}}}} = 
    \begin{cases} 
          2 \cos\Big[\frac{1}{2^{n}}\Big(\pi + \arctan{\frac{\sqrt{x(4-x)}}{x-2}}\Big)\Big] & 0 < x < 2 \\
          2 \cos\Big[\frac{1}{2^{n}}\arctan{\frac{\sqrt{x(4-x)}}{x-2}}\Big] & 2 < x \leq 4 \\
          2 \cosh\Big[\frac{1}{2^{n}}\text{arctanh}{\frac{\sqrt{x(x-4)}}{x-2}}\Big] & x \geq 4 
       \end{cases}
    \
$$ where $x \in \mathcal{R}$ and $n$ is the number of time the square root symbol $\sqrt{\text{ }\text{ }}$ appears. This formula is a generalization of the following formula, which can be found here and here , and  which can be obtained by taking the above general formula in the limit $x = 2$ . $$\sqrt{2 + \sqrt{2 + ... + \sqrt{2 + \sqrt{2}}}} = 2\cos\Big(\frac{\pi}{2^{n+1}}\Big)$$","['nested-radicals', 'trigonometry', 'closed-form']"
3760316,Combinations similar to sectional curvature,"Suppose we have a 3-d manifold $M$ and a point $p$ in $M$ , and let $T,N,B\in T_pM$ be three orthonormal tangent vectors. We know that combinations like $(T,N,T,N)$ and $(T,B,T,B)$ and so on are sectional curvatures (I'm using DoCarmo's notation here), and they have a very clear geometrical meaning. But what about expressions like $(T,N,T,B)$ and so on? Do they have a clear geometrical interpretation, possibly related to sectional or Ricci curvature?","['riemannian-geometry', 'differential-geometry']"
3760363,Finding the Center of Mass of a disk when a part of it is cut out.,"From a uniform disk of radius $R$ a circular disk of radius $\frac{R}{2}$ is being cut out. The center of the ""cut out"" disk is at $R/2$ from the venter of the original disk. We have to find the center of mass of leftover body. I thought that we should set up a coordinate system with the center of original disk as the origin. The formula for center of mass is $$
\vec{R}_{CM} = \frac{1}{M_{tot}} \int \vec{r} ~dm$$ I thought of creating another identical region (identical to what is being cut out) on the left of $O$ . Like this By symmetry, any position vector $\vec{r}$ outside the encircled region (on the left) will have it's counter-part and hence it will cancel up. So, we need to worry only about the integral inside the circular region, even there by symmetry we know that $\vec{R}_{CM}$ will lie on the axis joining their centers (let's call the line joining all three centers as $x$ -axis and the line perpendicular to this line as $y$ -axis). If we use the polar coordinate then we have $$
R_{CM} = \frac{1}{M_{tot}} 2\int \int r \cos \theta \sigma   dA\\
\text{(I have written $2r\cos \theta$ because that's the thing we would get when we add any two}\\
 \text{vectors in that encircled region, $\sigma$ is the mass per unit area, and $dA$ is the area element)}\\
R_{CM} = \frac{1}{M_{tot}}2 \sigma \int_{r=0}^{R} \int_{\theta=0} r \cos \theta ~dA
$$ But the problem is that I don't know the upper limit of $\theta$ , I worked hard but it seemed a little different in this case. We can use our ordinary cartesian system, $$
R_{CM} = \frac{1}{M_{tot}} 2\sigma \int \int x dx dy$$ the limit of $x$ will be (I think) $0$ to $R$ and we can get $y$ as $$
(x-R/2)^2 + y^2 = R^2/4 \\
y= \sqrt{ x^2 - Rx}$$ So, we have $$
R_{CM} = \frac{1}{M_{tot}} 2\sigma \int_{x=0}^{R} \int_{y=0}^{\sqrt{x^2-Rx}} x dy dx\\
R_{CM} = \frac{1}{M_{tot}} 2\sigma \int_{x=0}^{R} x\sqrt{x^2-Rx} ~dx
$$ I don't know how to carry out that last integral. The answer to this question is "" $R/6$ to the left of O"" but where am I mistaking? Can someone help me out?","['integration', 'physics', 'multivariable-calculus', 'multiple-integral']"
3760368,Read off from second fundamental form if submanifold is contained in a larger totally geodesic submanifold?,"Let $Z$ be a Riemannian manifold and consider submanifolds $X\subset Y\subset Z$ . Denote by $N_{X,Y}$ the normal bundle of $X$ in $Y$ and $N_{X,Z}$ of $X$ in $Z$ and $N_{Y,Z}$ of $Y$ in $Z$ . Let $\Pi_{X,Y},\Pi_{X,Z},\Pi_{Y,Z}$ be the respective second fundamental forms. When pulling back/restricting the bundles to $X$ the metric on $Z$ gives a splitting $N_{X,Z}=N_{X,Y} \oplus N_{Y,Z}$ . With this identification, I have convinced myself that $\Pi_{X,Z}=\Pi_{X,Y}+\Pi_{Y,Z}$ . In other words, if $Y$ is totally geodesic then $\Pi_{X,Z}$ takes values in a proper subbundle of $N_{X,Z}$ . My question is: Is the converse also true? Ie. if $\Pi_{X,Z}$ takes values in a proper subbundle, is there a totally geodesic submanifold $Y\subset Z$ such that $X\subset Y$ .
I suppose all we can hope for is a local statement but maybe more can be said when some of the submanifolds are compact and simply-connected or something like that?","['submanifold', 'riemannian-geometry', 'differential-geometry']"
3760388,General formula for eigenvectors of a 3x3 matrix,"Sorry if this is a dumb question but given a general 3x3 matrix $A = \begin{pmatrix} a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix} $ and assuming it has 3 distinct eigenvalues $\lambda_1, \lambda_2, \lambda_3$ , is there a general (analytical) formula for the eigenvectors of this matrix?","['matrices', 'linear-algebra']"
3760396,"How many unordered pairs of positive integers $(a,b)$ are there such that $\operatorname{lcm}(a,b) = 126000$?","How many unordered pairs of positive integers $(a,b)$ are there such that $\operatorname{lcm}(a,b) = 126000$ ? Attempt: Let $h= \gcd(A,B)$ so $A=hr$ and $B=hp$ , and $$phr=\operatorname{lcm}(A,B)=3^2\cdot 7\cdot 5^3 \cdot 2^4\,.$$ Let $p = 3^a5^b7^c2^d$ and $r = 3^e 5^f 7^g 2^s$ . Notice, that given $p$ and $r$ , $h$ is determined, so we can count $p$ and $r$ . Multiplying $p$ and $r$ we get $$pr = 3^{(a+e)} 5^{(b+f)} 7^{(c+g)} 2^{(d+s)}\,,$$ and so $a+e = 0,1,2$ . For the first case we have $0+1 = 1$ possibility, similarly $2$ and $3$ for the other cases, so the total number is $6$ . For $b+f$ we have $b + f = 0,1,2,3$ giving $10$ options. Similarly for $c + g$ we have $3$ choices and for $d + h$ we have $$1+2+3+4+5 = 15$$ choices. Multiplying these together, we get $$15\cdot 3\cdot 6\cdot 10 = 60\cdot 45 = 2700\,,$$ which is not equal to the given answer of $473$ . Edit: sorry for the weird variables. I think I've fixed everything, if not, please do point it out","['contest-math', 'elementary-number-theory', 'gcd-and-lcm', 'combinatorics', 'algebra-precalculus']"
3760415,Simplify $\tan^{-1} ( \frac{x-\sqrt{1-x^2}}{x+\sqrt{1-x^2}} )$ with trigonometric substitution,"I will explain my approach, help me with the last step please! $$ \tan^{-1} {\left(\frac {x - \sqrt {1-x^2}}{x + \sqrt {1-x^2}}\right)}$$ substituting x = $\sin \theta$ (as learnt from book) and solving 1- $\sin^2 \theta$ = $\cos^2 \theta$ $$ \tan^{-1} {\left(\frac {\sin \theta - |\cos \theta|}{\sin \theta + |\cos \theta| }\right)}$$ For solving modulus, it was important to determine range of $\theta$ , therefore  I defined it (as it is my variable,i can define it my way) for [- $\pi$ /2 , $\pi$ /2] so that sine covers all values from $-1$ to $1$ (as , $ -1 \le x \le 1 \,$ , from domain  ) and $\cos \theta$ is positive , and hence $|\cos \theta| = \cos \theta$ . $$ \tan^{-1} {\left(\frac {\sin \theta - \cos \theta}{\sin \theta + \cos \theta }\right)}$$ = dividing by $\cos \theta$ $$ \tan^{-1} {\left(\frac {\tan \theta - 1}{\tan\theta + 1 }\right)}$$ = by formula of $\tan (\theta - \pi/4)$ $$ \tan^{-1}( \tan{\left(\theta - \pi/4\right)})$$ That's where I am stuck ,as according to the identity, $\quad$ $tan^{-1} ( \tan \alpha) = \alpha$ $\quad$ only when $\, -\pi/2 <\alpha < \pi/2$ . But here $$ -3\pi/4 \le \,(\theta-\pi/4) \, \le \pi/4 $$ Therefore, I am not going to get ( $ \,\theta - \pi/4 $ ) out of the expression. What  i get will be based on that graph of $\bf {\tan^{-1} (\tan x)}$ . $$ (\theta - \pi/4) +\pi \,$$ for $\,-3\pi/4 \le \, (\theta -\pi/4) \, < -\pi/2 \,\,$ and $$\theta -\pi/4$$ for $\,-\pi/2 < \, (\theta -\pi/4) \, \le \pi/4 \,\,$ My teacher just cancelled arctan and tan and wrote $\theta - \pi/4$ and he didn't even include that modulus function over $\cos \theta$ . So what will be the exact answer because if everyone decide $\theta$ as per they like then there will not be a finite answer. Everyone will have their own answers and in each answer they have multiple cases as I just discussed above. So please help me, very hopefully I signed up in stackexchange! Found Solution :- I was confused because  I was thinking that there can be many solutions differing person to person, but even if you choose any value of $\theta$ , you are going to get two solutions which are in the asked question above. The problem resolves when we write $\theta$ in terms of $sin^{-1} x$ as then we would not simply write like $$ \theta = \sin^{-1} x $$ we would write an equation, $$ \sin^{-1} x = \sin^{-1} (\sin \theta)$$ ,
now if $\theta$ is not in range of $-\pi/2$ and $\,\pi/2$ , then there would be some constant in $\pi$ (like , $\pi/4 , 2\pi$ etc. we would have to add or subtract according to the graph of 'sin inverse sin' and when we would put that value of $\theta$ , we would end with the solutions as answered by people.
(I write the answer in this edit  to help anyone who will reach here after searching  web , thanks to everyone for answers)","['angle', 'trigonometry', 'inverse-function']"
3760443,Barycenter of the union of two parametric curves,"Let $\gamma_1:t\in[a,b]\rightarrow (x_1(t),y_1(t))\in \mathbb{R}^2$ and $\gamma_2:t\in[a,b]\rightarrow (x_2(t),y_2(t))\in \mathbb{R}^2$ two parametrized curves what is the barycenter of the curve $\gamma=\gamma_1\cup\gamma_2$ ?","['curves', 'differential-geometry']"
3760455,I am trying to understand how sets are generally defined using ZF set theory.,"I've been reading about ZF set theory.  Using the book, Introduction to Set Theory by Hrbacek and Jech it starts with these axioms (informal definitions here). Axiom of Existence: There exists a set which has no elements. Axiom of Extensionality: If two sets contain the same elements they are equal. Axiom Schema of Comprehension: Let $S,T$ be sets. Let $x$ be an object and $P$ be a property of $x$ . Then for any set $T$ there exists a set $S$ such that $x ∈ S$ if and only if $x ∈ T$ and $P$ is true. Axiom of Pair: Given any two sets $x$ and $y$ there exists a set $S$ such that $x,y ∈ S$ . Axiom of Union: Given a set $S$ there exists a set $T$ such that $x ∈ T$ if an only if $x ∈ X$ for some $X ∈ S$ . At this point I am curious how ZF actually gets sets that are not based off the empty set.  That is, we know the empty set exists by the Axiom of Existence so we can have two sets $A = \emptyset, B=\emptyset$ .  By the Axiom of Pair we can have set $C = \{A, B\} = \{\emptyset, \emptyset \}$ .  However, how would we get a set such as $D=\{\text{Apple}, \text{Pear}\}$ . Would we have to use the Axiom Schema of Replacement to actually bring this set into existence?  This would allow us to create a mapping between the sets C and D.","['elementary-set-theory', 'axioms', 'set-theory']"
3760465,Normalizing constant of an exponential family of distributions with spherical harmonics,"I am interested in modeling a distribution on a sphere with a series expansion in terms of the spherical harmonics $Y_l^m (x)$ where $x \in \mathbb{S}^2$ is a point on the unit sphere.
From the paper Harmonic analysis and distribution-free inference for spherical distributions by Jammalamadaka and Terdik I learned that there are multiple ways of achieving this.
The following formulation appears to be suitable in my setting: An exponential family of distributions for directional data was introduced in [4] and on p. 82 of [36].
Apart from the normalizing constant, the density has the form $$f_e(x) \propto \exp \sum_{l=0}^{\infty} \sum_{m=-l}^{l} c_l^m Y_l^m (x) $$ where $c_l^{m \ast} = (− 1)^m c_l^{−m}$ . The normalizing constant
corresponds to $c_0^0$ and depends on the rest of the parameters $c_l^m$ as well since the integral of $f_e$ must be 1. I am interested in this normalization constant $c_0^0$ .
Following the references in the paper, I could not find an expression for $c_0^0$ .
Can anybody point me to a paper or book in which an analytical form (or approximation) is given?","['integration', 'statistics', 'spherical-harmonics']"
3760485,Let $f$ be an entire function s.t. $F(z) = \lim_{n\to\infty} f^{(n)}(z)$ exists for all $z$ with local uniform convergence. What can we say about $F$?,"I have stumbled into this problem, without a given answer. Let $f$ be an entire function such that $F(z) = \lim\limits_{n\to\infty} f^{(n)}(z)$ exists $\forall z \in \mathbb{C}$ with local uniform convergence. What can you say about the function $F$ ? What can you say about the function $f$ ? I have sort of convinced myself that $F(z) =Ce^z$ and thus $f(z)=F(z)$ but i am very doubtful about this and even if it is correct I have no idea how to prove it, and there is probably more information you have to provide about the given functions. Such that $F$ is analytic implies that $f$ is analytic.","['complex-analysis', 'uniform-convergence']"
3760491,Is there a story proof behind the combinatorial identity $(n-2k)\binom{n}{k} = n\left[ \binom{n-1}{k} - \binom{n-1}{k-1} \right]$?,"Is there a ""story proof""/combinatorial proof for the following combinatorial identity: $$(n-2k)\binom{n}{k} = n\left[ \binom{n-1}{k} - \binom{n-1}{k-1} \right]\tag1$$ I know that this identity can be proved by using the following identities: $$k\binom{n-1}{k} = (n-k)\binom{n-1}{k-1}\tag2$$ $$k\binom{n}{k} = n\binom{n-1}{k-1}\tag3$$ but is there a ""story proof"" for equation $(1)$ ? Edit 1: I do know the story proofs for equations 2 and 3. But 'sewing them together' is the problem! $$\text{RHS} \stackrel{\text{i}}{=}  n\left[ \binom{n-1}{k} - \binom{n-1}{k-1} \right] \stackrel{\text{ii}}{=}  \frac{n}{k}\left[ k\binom{n-1}{k} - k\binom{n-1}{k-1} \right] \stackrel{\text{iii}}{=} \frac{n}{k}\left[ (n-k)\binom{n-1}{k-1} - k\binom{n-1}{k-1} \right] \stackrel{\text{iv}}{=} \frac{n}{k}\binom{n-1}{k-1}\left[ (n-k) - k \right] \stackrel{\text{v}}{=} (n-2k)\binom{n}{k}$$ Precisely, how do you formulate a story proof for step (iv)? i mean the term $\binom{n-1}{k-1}$ is being taken common in step iv. What could a story proof for taking a term common be?","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
3760523,How to mathematically explain why the median kill/death ratio is always lower than the mean in battle royale games?,"Firstly this is a theory that I'm pretty certain must be true at least under certain conditions, but I don't know how to explain it mathematically. I understand there are many variables I'm not accounting for. FYI: In a battle royale game (BR) players drop in to a specified area that closes over time, fighting till the last man standing, with no respawning. Outside of game-specific mechanics like fall-deaths, the total kill-death ratio (kdr) is a fraction below 1, because the last player doesn't die. Here's my logic now: From the stats I've seen, as well as experience, the median kd should be lower than the mean, I'd guess around 0.7-0.85 for most BR games. It's quite obvious why that is. The top players can sometimes kill 1/4 of the lobby. But I wanted to explain it more mathematically. If you convert the battle royale into a 1v1 single elimination tournament, the distribution is obvious. If there are 8 players, 4 get no kills, 2 get 1, 1 gets 2, and 1 gets 3. From my understanding you can't calculate the mean kd from this because the winner doesn't die, but the median is 0.5. To get the mean I'd add a second game, where all the players who didn't get a kill get on the scoreboard. The KDs would now be 4 with 0.5, 2 with 1 and 2 with 3 (the mean being 1.5 and the median being 0.75). So even with the skill levels being completely flattened (the placing reversed after the first game), the median is still 50% of the mean. If you added skill, and assumed the same players would place highly most of the time, the median would be even lower. Obvious counters to this as a relevant model: yes, players don't fight each other one at a time, and some players will camp for placement. IMO the former would actually accentuate the skewedness of the distribution, as good players are more able to take advantage of third parties. The latter would have no/little effect on kd distribution, as camping till the end to win won't help you if you have to fight good players, and doesn't necessarily improve your kd or harm a high-kd player's anyway. So here's an alternative model: players kill each other in a circle, p1 kills p2, p8 kills p1... and p3 kills p4 to win. Playing twice assuming equal skill you would end up with a distribution like {0, 0.5, 1, 1, 1, 1, 2, 3} the mean being 1.1875 and the median being 1}. Even though I think this is less similar to how BRs play out, it still would give a lower mean than median. I was wondering how I could improve upon the logic and make it more useful.  Is it a convincing argument? Is there a way I could combine the first and second models? And also if there are any general theorems that would apply.","['statistics', 'infinite-games', 'probability', 'ratio']"
3760537,"If $f:\mathbb{R}\setminus\{0,1\}\to\mathbb{R}$ satisfies $f(x)+2f\left(\frac 1 x\right)+3f\left(\frac x {x-1}\right)=x$, then $8f(4)=?$","Let $f: \Bbb{R}\setminus\{0,1\}\to \Bbb{R}$ be a function such that $$f(x)+2f\left(\frac 1 x\right)+3f\left(\frac x {x-1}\right)=x\text{ for all }x\neq 0,1\,.$$ Then, $8f(4)=$ ? My Attempt. Is it correct?","['contest-math', 'functional-equations', 'functions', 'solution-verification']"
3760579,Do functions with the same gradient differ by a constant?,"Let $f,g:\mathbb{R}^n\to\mathbb{R}$ be such that $\nabla f=\nabla g$ . I believe this implies that $f$ and $g$ only differ by a constant, like in the one-dimensional case. But I'm not sure how to prove it. If it's indeed true, can you give me a hint? Thanks!","['multivariable-calculus', 'vector-analysis']"
3760594,Set notation : Composing set of sets,"Is there a proper notation to compose sets and produce a set of sets? ( I am referring to this as compose due to ignorance of a proper manner to call it ) To illustrate what I want, let me suppose that $\otimes$ does the job, so that \begin{align}
\{1\} \otimes \{2\} &\rightarrow \{ \{1\} , \{2\} \}\\
\{1\} \otimes \{1,2\} &\rightarrow \{ \{1\} , \{1,2\} \}
\end{align} Also, how can we write a composition for a finite number of sets? Say that $U_i = \{i\}$ (trivial example) is there something that can make (again using $\otimes$ ): \begin{equation}
\bigotimes_{i=1}^N U_i = \{ \{1\} , \{2\}, \ldots , \{N\} \}
\end{equation}","['elementary-set-theory', 'notation']"
3760598,What loops are possible when doing this function to the rationals?,"What loops are possible when doing this function to the rationals? Let's define this function on a simplified fraction $\frac{a}{b}$ . $$f\left(\frac{a}{b}\right)=\frac{a+b}{b+1}$$ I started this with $f(\frac{2}{3})=\frac{5}{4}$ then i did the function again and got this sequence of numbers $\frac{2}{3},\frac{5}{4},\frac{9}{5},\frac{7}{3},\frac{5}{2},\frac{7}{3},\dots$ I saw that is starts to loop with $\frac{7}{3},\frac{5}{2}$ Another loop is $\frac{1}{1}$ , a one cycle. Another loop I found was $\frac{2}{1},\frac{3}{2},\frac{5}{3}$ . My first question is: from starting from any rational number does it all ways end in a loop or does it ever go to infinity? And my second question is: what sizes of loops are possible? If the three loops I stated are the only loops prove it Dark made a post related What are the possible loops when doing this a type of function to the rationals?","['number-theory', 'sequences-and-series']"
3760614,Trace norm of a trace class operator exercise in Conway,"Exercise in Conway's Functional Analysis book: Let $T$ be a trace class operator on a Hilbert space ${\cal H}$ . Prove: $$\sup\{|\mbox{tr}(CT)|:\ C\ \mbox{is compact}, ||C||\leq 1\}=||T||_1.$$ Here, $||T||_1=\mbox{tr}[(T^*T)^{\frac{1}{2}}]$ is the trace norm . I can prove that $\leq$ holds. I can prove the equality in the finite dimensional case using polar decomposition. This led me to believe that a polar decomposition argument should also work for the infinite dimensional case. However, I am not sure how to use the compactness assumption. Any hints for the $\geq $ inequality?",['functional-analysis']
3760622,Characteristic functions are not in $H^{1/2}$.,"This is an exercise from Lieb, Loss.
I need to show that no characteristic function of a set $A\subset\mathbb{R}^n$ with a finite positive measure is in $H^1(\mathbb{R}^n)$ or even in $H^{\frac{1}{2}}(\mathbb{R}^n)$ . I know that that the distributional partial derivatives of such a function must be zero. But I don't see the contradiction since in that case the derivatives are in ${\rm L}^2(\mathbb{R}^n)$ . I would also like some help with the second part. According to the book, a function is in $H^{\frac{1}{2}}(\mathbb{R}^n)$ if and only if $\int\limits_{\mathbb{R}^n}(1+2\pi|k|)|\hat{f}(k)|^2{\rm d}k$ is finite. But for an arbitrary $A$ , how can I know enough about $\chi_A$ to show that this integral isn't finite?","['measure-theory', 'distribution-theory']"
3760642,Small-angle approximation of $ \frac{\sin^2 x}{x^2 \sqrt{1-\frac{\sin^2 x}{3}}} $,"I need to show the following: $$ \frac{\sin^2 x}{x^2 \sqrt{1-\frac{\sin^2 x}{3}}} \approx 1-\frac{x^2}{6} $$ when $ x $ is small. I think this problem is trickier than most other questions like it because in the original source there is comment saying ""if you got $ 1+\frac{x^2}{6} $ [what I got] then think again!"". My attempt was: When $ x $ is small, $ \sin x \approx x $ so $$ \frac{\sin^2{x}}{x^2 \sqrt{1-\frac{\sin^2 x}{3}}} = \frac{x^2}{x^2 \sqrt{1-\frac{x^2}{3}}} = \left ( 1 - \frac{x^2}{3} \right )^{-\frac{1}{2}} $$ Then using the binomial series approximation, $$ \left ( 1 - \frac{x^2}{3} \right )^{-\frac{1}{2}} \approx 1 - \frac{1}{2}\left ( -\frac{x^2}{3} \right ) + ... = 1 + \frac{x^2}{6} $$ ...and so it looks like I've fallen into whatever trap the question set. Where is my error?","['limits', 'algebra-precalculus']"
3760676,Probability becomes nonzero conditioned on zero-probability event?,"Several other questions have noted the fact that the standard definition of conditional probability, $$P(A\mid B) = \frac{P(A \cap B)}{P(B)},$$ is undefined when $P(B) = 0$ . Wikipedia avoids dividing by $0$ using any decreasing sequence of events $B_n \rightarrow B$ with $P(B_n) > 0$ , and taking $$P(A|B) = \lim_{n \rightarrow \infty} P(A|B_n).$$ However, this methods defies my intuition when $A$ is also a zero-probability event. If $P(A) = 0$ , then clearly the above limit equals $0$ . Yet it seems that some zero-probability events should have positive probabilities conditional on other zero-probability events. For instance, let $X \sim \text{Unif}[0, 9]$ . Clearly, $\{X \in \mathbb{Z}\}$ and $\{\sqrt{X} \in \mathbb{Z}\}$ are both zero-probability events. Yet it seems obvious that if the realization of $X$ is integral, then $\sqrt{X}$ has a nonzero chance of also being integral. Indeed, based on the fact that $0, 1, 4,$ and $9$ have integral square roots, I'd expect $$P(\sqrt{X} \in \mathbb{Z} \mid  X \in \mathbb{Z}) = \frac{4}{10}.$$ Is there a way to formalize this?","['measure-theory', 'probability-limit-theorems', 'conditional-probability', 'probability-theory', 'probability']"
3760696,L' Hospital's Rule with general measurable function,"I come up with this question when I read L'Hospital's rule and think about non-continuous function case, more specifically, assume $0\leq g(x)\leq 1$ , not necessarily continuous but a measurable function w.r.t Lebesgue measure and $$\lim_{\delta\rightarrow 0} \frac{1}{\delta}\int_0^\delta g(x)dx = C$$ for some real number C. The integration is defined in the Lebesgue sense. For another function $f(x)$ , defined on $[0,1]$ , continuous and monotonic increasing, $f(0) = 0$ , could we have the following hold: $$\lim_{\delta\rightarrow 0} \frac{\int_0^\delta f(x)g(x)dx}{\int_0^\delta f(x)dx} = C$$ I raise this question because if $g(x)$ is continuous, then by L'Hospital's rule, $g(\delta)\rightarrow C$ as $\delta\rightarrow 0$ , and the above limit is clearly held by applying of the L'Hospital's rule again. But here what if there is no such good regularity of $g(x)$ ? For example, $g(x)$ could have no limit when $x$ goes to $0$ . If the second limit formula does not hold for all $f(x)$ function with the mentioned conditions, then could we impose more regularity of $f(x)$ to make this true? Thank you so much!","['continuity', 'measure-theory', 'real-analysis']"
3760716,Is there an explicit construction of this bijection?,"As part of my answer to another question , I needed the following fact: if $S = \{1, \ldots, n\}$ and $k \leq n/2$ , then there is a bijection $f : {S \choose k} \to {S \choose k}$ such that $t \cap f(t) = \emptyset$ for all $t \in {S \choose k}$ . Here $n$ and $k$ are positive integers, and ${S \choose k}$ denotes the family of all size- $k$ subsets of $S$ . Here's the proof I found for that fact. Let $p = \left\lvert{S \choose k}\right\rvert = {n \choose k}$ , and write ${S \choose k} = \{t_1, \ldots, t_p\}$ . Construct a bipartite graph $G$ on partite sets $A = \{a_1, \ldots, a_p\}$ and $B = \{b_1, \ldots, b_p\}$ by drawing an edge $a_ib_j$ whenever $t_i \cap t_j = \emptyset$ . Observe that $G$ is an ${n-k \choose k}$ -regular bipartite graph, where ${n-k \choose k} > 0$ , and therefore has a perfect matching $M$ , by Hall's Theorem. Now for each $i \in \{1, \ldots, p\}$ we have $a_ib_j \in M$ for exactly one value of $j$ , and we get the desired bijection just by taking $f(t_i) = t_j$ for the corresponding value of $j$ . Unfortunately, the proof above doesn't give an explicit construction of the bijection $f$ , which makes it hard to naturally use this bijection in a combinatorial proof. When $n = 2k$ , the function $f(t) = S-t$ is an easy example of a bijection with this property. Is there a nice explicit construction of such a bijection for general $k$ ? Some partial thoughts: it's tempting to try to build on the $n=2k$ case by modifying the function $f(t) = S-t$ , say by taking the function $f$ to be ""take the $k$ least elements of $S-t$ "", but it seems that the natural approaches to modifying that function end up failing to be injective (hence also fail to be surjective). For example, the "" $k$ least elements of $S-t$ "" function fails at $n=5$ and $k=2$ because it yields $f(\{3,4\}) = f(\{3,5\}) = \{1,2\}$ . When $k=1$ this is just asking for a derangement of $\{1, \ldots, n\}$ , and a function like $f(\{i\}) = \{i+1 \mod n\}$ works, where $x \mod n$ is the residue of $x$ modulo $n$ . When $k=2$ and $n \geq 4$ , I believe the following function works, where $\{x,y\} + i \mod n$ is shorthand for $\{x+i \mod n, y+i \mod n\}$ : $f(\{i, j\}) = \begin{cases} \{i, j\} + 2 \mod n, & \text{if $i-j \equiv \pm 1 \pmod{n}$} \\
\{i, j\} + 1 \mod n, & \text{otherwise.}\end{cases}$ This suggests that in a general construction, maybe we can just assign an integer $r_t$ for each $t \in {S \choose k}$ and use a map of the form $t \mapsto t+r_t \bmod{n}$ , with the values of $r_t$ chosen cleverly to ensure bijectivity and disjointness. However, this approach is doomed to fail when $t$ is a difference set for $\mathbb{Z}_n$ . To use an example of such a set due to Jungnickel, Pott, and Smith, when $n = 11$ and $t = \{1,3,4,5,9\}$ , it is easy to check that $t + r_t \mod 11$ intersects $t$ regardless of the choice of $r_t$ . So this approach cannot work in general either. Relevant external literature I've found so far: The $n = 2k+1$ case appears to have been solved by Kierstead and Trotter (1988) , in a superficially-different but equivalent formulation. Kai Jin (2019) refers to the problem of finding an explicit $1$ -factorization of the related ""bipartite Kneser graphs"" (equivalent to the graph $G$ described in the proof above) as a ""challenging open problem"", but we are only looking for an explicit description of one matching in a bipartite Kneser graph, not an entire $1$ -factorization.","['combinatorics', 'combinatorial-proofs']"
3760717,Is the product of graph Laplacians positive semidefinite?,"For any weighted directed graph $G = (V,E,w)$ we can define the weighted Laplacian matrix $L=D-A$ , where $A=[a_{ij}]$ is the adjacency matrix and $D=\text{diag}( d_1, ..., d_n )$ is the in-degree matrix with $d_i = \sum_j a_{ij}$ . However, $L$ is not in general positive semi-definite. If we include the out-degree through the out-Laplacian matrix $L^o = D^o - A^\top$ , where $D^o=\text{diag}( d_1^o, ..., d_n^o )$ is the out-degree matrix with $d_i^o = \sum_j a_{ji}$ , the quadratic form $x^\top ( L + L^o )x$ is positive semidefinite since, $$ x^\top ( L + L^o )x = \sum_i\sum_j a_{ij}(x_i-x_j)^2 \geq 0.$$ If we define $Q=L+L^o$ , I want to proof if $(Qx)^\top(-Lx) \leq 0$ . This is equivalent to \begin{align*}
(Qx)^\top(-Lx) \leq 0 &\iff -x^\top QLx \leq 0 \\
&\iff x^\top QLx \geq 0 \\
&\iff x^\top ( QL + L^\top Q )x \geq 0 \\
&\iff QL + L^\top Q \succeq 0
\end{align*} Some properties that I have found $Q=Q^\top$ is positive semidefinite. By Gershgorin's Theorem we know that all the eigenvalues of $L$ have positive real part. Both, $L$ and $Q$ are diagonally dominant. $QL + L^\top Q \succeq 0$ is very similar to Lyapunov equation. This can be useful since $-L$ is a Hurwitz matrix and $Q\succeq 0 $ .","['positive-semidefinite', 'graph-theory', 'matrices', 'linear-algebra', 'graph-laplacian']"
3760721,"Evaluating $\int_{0}^{2\pi}\frac{1}{\cos^2(\theta)+1}\, d\theta$","What would be the values of this definite Integral? $$\int_{0}^{2\pi}\frac{1}{\cos^2(\theta)+1}\, d\theta$$ So, I have solved this definite integral using the substitution method, taking $u=\tan(\theta)$ . After some simplification, the solution to the definite integral I get is follows: $$\frac{1}{\sqrt{2}} \, \tan^{-1}\left(\frac{\tan{\theta}}{\sqrt{2}}\right) \Biggr|_{0}^{2\pi}$$ Whenever I am evaluating the above result at the limits of the integration I am getting an answer of $0$ . My simplification, $$=\frac{1}{\sqrt{2}} \, \left[ \tan^{-1}\left(\frac{\tan{2\pi}}{\sqrt{2}}\right) - \tan^{-1}\left(\frac{\tan{0}}{\sqrt{2}}\right) \right]$$ $$=\frac{1}{\sqrt{2}} \, \big[ \tan^{-1}(0) - \tan^{-1}(0) \big]$$ $$=\frac{1}{\sqrt{2}} \, \big[ 0-0]$$ $$=0$$ However, using Mathematical/Integral calculator, the value of this Integral is $$2\pi$$ I am probably doing something very silly, as I can't figure out what I am doing wrong. Any help would be appreciated. Thanks!","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
3760724,What is the probability space of typical real univariate probability distributions?,"Postscript to the question below. In trying to learn from the answers below, all of which I am grateful for, I read a historical article on the origins and legacy of Kolmogorov's Grundbegriffe .  This article helped me understand what basic things people were struggling with when this theory was developed. In particular, the long-term trend towards abstraction and foundation in terms of measure theory, and the early-days focus on the connection between the real world and the probabilistic model.  I then re-read the answers and comments. I made a comment that started We can choose $Ω=\Re$ because the domain of the distribution function is $\Re$ . This is wrong because the domain of the distribution function is not necessarily mentioned in the declaration of the probability space .  I made the convention that random variables $X: \Omega \rightarrow \Re$ .  So the domain of the distribution function is $\Re$ by my convention, but that doesn't have anything to do with the probability space. $\Omega$ is a kind of index set .  Suppose we are reasoning about the saturation of the color red in grapes.  In that case we are thinking about say a color level in $S=[0,255)$ . Nowhere in the definition of a probability space $(\Omega,\mathcal A,P)$ to support reasoning about $S$ do we need to specify $S$ .  We do need to demonstrate that there is a 1-1 mapping between $\Omega$ and $S$ , i.e. that $\Omega$ can enumerate $S$ . Once we have ""built"" $(\Omega,\mathcal A,P)$ , we can put it to work and re-use it for any $S$ which $\Omega$ can enumerate. The probability space $(\Omega,\mathcal A,P)$ is a kind of indexing structure. That for me is the key realization.  The key cognitive error comes from labelling $\Omega$ as the sample space, and $\mathcal A$ as the event space.  The common sense meaning of those terms implies a connection with the actual samples being reasoned about, when that does not have to be the case.  A far less misleading terminology would be to label $\Omega$ as the sample index space or just index space, and $\mathcal A$ as the index set space.  This kind of thing is clearly understood in programming languages, where if I have an array $A$ , then $(i,j)$ is an index and I don't confuse $(i,j)$ with $A[i,j]$ , and I don't confuse the purpose of arrays with the purpose of array indices, but in some contexts I can identify $A[i,j]$ with $(i,j)$ . Short version of the question: How do we formally and correctly define the probability space of the reals which supports the definition of the typical/usual univariate continuous probability distributions, such as uniform and exponential? Short restatement of the core question that I have :  I am hung up on p. 3 section 1.1B of the KPS text .  They start with an unspecified probability space $(\Omega,\mathcal A,P)$ .  Two distinct random variables $V$ , $V \in Exp(\lambda)$ and $V \in U[a,b]$ , are said to have distribution functions $F_V=P_V((-\infty,x))=P(\{\omega \in \Omega: V(\omega)<x\})$ .  These are distinct and solved separately as $F_{U[a,b]}(x) = \mathcal H(x-a) \mathcal H(b-x) \frac{x-a}{x-b} + \mathcal H(x-b)$ and $F_{Exp(\lambda)}=\mathcal H(x) (1-e^{-\lambda x})$ , where $\mathcal H(x \geq 0) = 1, \mathcal H(x<0)=0$ . My key question is: What is a solution for the $P$ shared by $X$ and $Y$ ? Note: Here are some similar questions on Math Stack Exchange Probability space of a Gaussian , unanswered, from 2016. What are the sample spaces when talking about continuous random variables? , asked 9 years ago and answered as $[0,1]$ .  The accepted answer starts out by saying ""You can take it to be a subset of $\Re$ or, more generally, $\Re^n$ .""  But then the solver gets to $[0,1]$ . Comment: I was mistakenly assuming that the text above was taking $\Omega=\Re$ because I saw a similar statement somewhere to the effect of saying ""for purposes of discussion let's say the sample space for continuous random variables is $\Re^d$ "".  The cited answer to 2nd question above starts that way but then gets to $[0,1]$ .   So: I now understand that the $[0,1]$ is the ""best fit"" sample space, along with Lebesgue measure.  So the ""right"" probability space that I was looking for is the Steinhaus space $([0,1],\mathscr B([0,1]), \mu)$ where $\mu$ is the Lebesgue measure restricted to $[0,1]$ .  99.999% of my confusion came from Not recognizing that $[0,1]$ is a ""big enough"" space to enumerate the domain of a continuous map into $\Re$ .  So it's ""as good as"" $\Re$ . Making the assumption that the convention, was, somehow somewhere, to identify the sample space for $d$ -dimensional continuous random variables with $\Re^d$ , when the ""best fit"" answer is $[0,1]^d$ . Longer version of the question: Following this text , Let $\Omega$ be a nonempty set, the sample space. Let set $\mathcal F$ of subsets of $\Omega$ be a $\sigma$ -algebra so that $\Omega \in \mathcal F$ $\Omega \setminus F \in \mathcal F$ if $F \in \mathcal F$ $\bigcup_{n=1}^{\infty} F_n \in \mathcal F$ if all $F_i \in \mathcal F$ Let $P: \mathcal F \rightarrow [0,1]$ be a probability measure so that $P(\Omega) = 1$ $P(\Omega \setminus F) = 1-P(F)$ $P(\bigcup_{n=1}^{\infty} F_n) = \sum_{n=1}^\infty P(F_n)$ We call the triple $(\Omega, \mathcal F, P)$ a probability space . Suppose $X:\Omega\rightarrow \Re$ .  We say $X$ is a random variables if $\{\omega \in \Omega : X(\omega) \leq a\}$ is in $\mathcal F$ for every $a \in \Re$ . Then the probability distribution function $F_X : \Re \rightarrow \Re$ is defined for all $x \in \Re$ as $$F_X(x) = P(\{\omega \in \Omega : X(\omega) < x\})$$ Note that $P$ appears unsubscripted in the definition of $F_X$ . $P$ does not depend on the particular random variable $X$ whose distribution we are defining.  So in that sense it should be possible for the same probability space $(\Omega, \mathcal F, P)$ to underly probability distribution function constructions for multiple distinct random variables $X$ and $Y$ , $X \neq Y$ , for the same probability space. For example, let $$\Omega = \{0,1\}$$ $$\mathcal F = \{\emptyset, \{0\}, \{1\}, \{0,1\}\}$$ $$P = \begin{cases}
\emptyset &\mapsto& 0 \\
\{0\} &\mapsto& \frac{1}{2} \\
\{1\} &\mapsto& \frac{1}{2} \\
\{0,1\} &\mapsto& 1
\end{cases}$$ Let $X,Y: \Omega\rightarrow \Re$ and be random variables fully defined by $$X = \begin{cases}
0 &\mapsto& 17 \\
1 &\mapsto& 17
\end{cases}$$ $$Y = \begin{cases}
0 &\mapsto& 42 \\
1 &\mapsto& 42
\end{cases}$$ Then the probability distributions of $X$ and $Y$ are $$F_X(x) = P(\{\omega\in\Omega:X(\omega)<x\}) = \begin{cases}
 x < 17 &\mapsto& 0 \\
x \geq 17 &\mapsto& 1
\end{cases}$$ $$F_Y(x) = P(\{\omega\in\Omega:Y(\omega)<x\}) = \begin{cases}
 x < 42 &\mapsto& 0 \\
x \geq 42 &\mapsto& 1
\end{cases}$$ Clearly $X \neq Y$ and $F_X \neq F_Y$ .  In the above discrete example, if I understand the language correctly, there is a single probability space $(\Omega,\mathcal F,P)$ with a single probability measure $P$ which underlies or supports two distinct probability distributions $F_X$ and $F_Y$ for two distinct random variables $X$ and $Y$ . Now let $(\Omega, \mathcal F, P)$ be a probability space underlying random variables $X$ and $Y$ where: Random variable $X: \Omega \rightarrow \Re$ is such that $X$ has the uniform distribution $F_X: \Re \rightarrow [0,1]$ such that $$F_X(x) = P(\{\omega\in\Omega:X(\omega)<x\}) = \begin{cases}0  &:& x < a \\
\frac{x-a}{b-a}  &:& a \leq x \leq b \\
1 &:& b < x
\end{cases}$$ Random variable $Y: \Omega \rightarrow \Re$ is such that $Y$ has the exponential distribution $F_Y: \Re \rightarrow [0,1]$ such that $$F_Y(x) = P(\{\omega\in\Omega:Y(\omega)<x\}) =  \begin{cases}0  &:& x < 0 \\
1-e^{-\lambda x} &:& x \geq 0
\end{cases}$$ Also, per comment below, one distribution can be supported by multiple probability spaces.  (The key understanding here for me is that probability space and probability distribution are separate constructions.) My questions are (and some answers that I take from my reading of the solutions below): Q1. Is $(\Omega, \mathcal F, P) = (\Re, \mathcal B(\Re), \mu)$ where $\mathcal B(\Re)$ is the Borel set of the reals and $\mu$ is the Lebesgue measure a probability space which underlies $X$ and $Y$ ? Answer : No, but the Steinhaus $([0,1], \mathcal B([0,1]), \mu)$ is good. Q2. Is it correct to call $(\Re, \mathcal B(\Re), \mu)$ the standard probability space of the reals?  Is there some other standard notation or language for the probability space underlying the usual continuous probability distributions ? Answer : No, but the Steinhaus space is a standard space in the Wikipedia sense . Q3. Is it correct to say that the notion of probability space is independent of and complementary to the notion of probability distribution , and that the notion of probability distribution is always associated with a particular random variable $X$ presented with a supporting probability space $(\Omega, \mathcal F, P)$ ? Answer : Kind of.  One distribution can be accompanied by many probability spaces.  One probability space can be accompanied by many distributions.  I'm using ""accompanied"" because the worked ""supported"" may be overloaded in math.  I'm looking for some compact synonym of ""independent and complementary"".  The main thing is to demonstrate through examples that the relationship is many-to-many.","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'category-theory', 'probability-theory']"
3760730,Generalizing trig-sum to product using complex exponentials,"Consider, $$ \sin A \sin B$$ Using exponential definition of sine, $$ \frac{ e^{iA} - e^{-iA} }{2i} \cdot \frac{ e^{iB} - e^{-iB} }{2i}$$ $$ =\frac{1}{-4}  ( e^{ i (A+B) } - e^{i (A-B)}  -e^{ -i(A-B) } + e^{ - i(A+B)})$$ $$ =\frac{-1}{4} ( 2\cos(A+B) - 2 \cos(A-B) )$$ or, $$ \frac{1}{2} ( \cos(A-B) - \cos(A+B) )$$ Now, I want to generalize this trick for like $$ \sin A \sin B \sin C \sin D....$$",['trigonometry']
3760731,How can I determine the radius of 4 identical circles inside an equilateral triangle $ABC$?,"How can determine the radius $r$ of 4 identical circles inside equilateral triangle $ABC$ of side $a$ ? My attempt : $r$ is radius of each of four identical circles. $a$ is side of equilateral triangle ABC. Joined the centers of three circles P, Q and R to obtain $\Delta PQR$ I have $$PQ=PR=r+r=2r$$ then dropped perpendicular PT to the side $AB$ . the length of $PT$ will be equal to the inradius of triangle ABC $$PT=\frac{a}{2\sqrt3}$$ $QM=RN=r$ radius of each circle. so I can get perpendicular in small right $\Delta$ $$PT-r=\frac{a}{2\sqrt3}-r$$ Now, I can use Pythagorean theorem in right $\Delta $ but I am stuck to find the length of $QR$ or $MN$ . Help me to find the radius $r$ of the circle in terms of side $a$ of equilateral $\Delta ABC$ . Thanks.","['analytic-geometry', 'circles', 'geometry', 'triangles', 'trigonometry']"
3760741,Find the singularities of $f(z) =\frac{1}{(2\sin z - 1)^2}$.,"Find the singularities of $f(z) =\frac{1}{(2\sin z - 1)^2}$ .
I am just learning about singularities and I was wondering if someone could give me feedback on my work. So I think, for this function, that there are singularities at $z=\frac{\pi}{6}+2k\pi,\frac{5\pi}{6}+2k\pi$ , would this be correct? Additionally, I am classifying them as essential and nonremovable. Is this also correct? To determine that they were nonremovable, I tool the limit of the function approaching the singularities and found that they tended towards infinity.","['complex-analysis', 'singularity', 'complex-numbers']"
3760767,MSE for MLE of normal distribution's ${\sigma}^2$,"So I've known $MLE$ for ${\sigma}^2$ is $\hat{{\sigma}^2}=\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2$ , and I'm looking for $MSE$ of $\hat{{\sigma}^2}$ . But I'm having trouble to get the result. What I tried goes like below: By definition, $MSE$ = $E[(\hat{{\sigma}^2}$ - ${\sigma}^2$ ) $^2$ ], which is = $Var(\hat{{\sigma}^2} - {\sigma}^2)+(E(\hat{{\sigma}^2} - {\sigma}^2))^2$ = $Var(\hat{{\sigma}^2})-Var({{\sigma}^2})+(E(\hat{{\sigma}^2} - {\sigma}^2))^2$ . From here, I tried to find $Var(\hat{{\sigma}^2})$ , which is = $Var(\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2$ ) = $\frac{1}{n^2}Var(\sum_{i=1}^{n} X_{i}^2 -n\bar{X}^2)$ = $\frac{1}{n^2}(\sum_{i=1}^{n} Var (X_{i}^2) -n^2Var(\bar{X}^2))$ But I'm not sure how to get $Var (X_{i}^2)$ and $Var(\bar{X}^2)$ . I tried $Var (X_{i}^2)$ = $E(X_i^4) - (E(X_i^2))^2$ , But I'm not quite sure what $E(X_i^4)$ would be. Could anyone help me with this? Am I on the correct path to solve this? Thanks in advance!","['statistics', 'maximum-likelihood']"
3760772,What are the possible loops when doing this a type of function to the rationals?,"I have John Hilbert to thank for his wonderful question that made me ask more questions What loops are possible when doing this function to the rationals? .
I thought well why not try instead of $f(\frac{a}{b})=\frac{a+b}{b+1}$ I thought of a general version $f_n(\frac{a}{b})=\frac{a+b}{b+n}$ . I started with $\frac{1}{1}$ and found a quick loop $\frac{1}{1},\frac{2}{3}$ . Then I tried a random number $\frac{1}{3}$ and got a series I proved never loops. you can find out for the x $^{th}$ iteration you can write it as $\frac{x^2}{2x+1}=\frac{x}{2} + \frac{1}{(4 (2 x + 1))} - \frac{1}{4}$ and the two values $x^2$ and $2x+1$ are always coprime. after thinking about it I think that for John's original problem the terms can't go to infinity but that's just a hunch. My question is for all n greater than 1 a series that never loops. and my question is for $f_n(\frac{a}{b})$ are there only finite examples of loops?","['number-theory', 'sequences-and-series']"
3760794,Prove that if the only singularities of a function are poles then the function must be rational.,"Prove that if the only singularities of a function are poles then the function must be rational. So originally this was an iff statement, but I have solved the other direction of the proof. I'm just not quite sure how to prove it going this direction. I'm thinking to assume the only singularities of a function are poles, but I'm not sure how to work this down to being only for rational functions. I toyed with the idea of contradiction, but still wasn't quite sure how I should go about proving this.","['complex-analysis', 'singularity', 'complex-numbers']"
3760838,"Integral $ \int_{-\infty}^{+\infty} \frac{\cos(4x)}{x^2 + 2x + 2} \, dx$","I am trying to evaluate this very long definite integral given below: $$ \int_{-\infty}^{+\infty} \frac{\cos(4x)}{x^2 + 2x + 2} \, dx$$ The direction it can go is by decomposing the denominator to $(x−i+1)$ and $(x+i+1)$ and then taking partial fraction $$ \frac{i\cos(4x)}{2(x−i+1)} - \frac{i\cos(4x)}{2(x−i+1)}.  $$ The online mathematical integral solvers follow this procedure, which ends up with a long and ugly solution to this integral. Is there a better way to go about this integral? And are there any elegant solutions? Thanks for your time!","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'trigonometric-integrals']"
3760884,Prove that $a$ commutes with each of its conjugates in $G$ if and only if $a$ belongs to an abelian normal subgroup of $G$.,"Let $a$ be an element of a group $G$ . Prove that $a$ commutes with each of its conjugates in $G$ if and only if $a$ belongs to an abelian normal subgroup of $G$ . My attempt: firstly, suppose that $N$ is an abelian normal subgroup of $G$ . Since, $a$ belongs to $N$ and $N$ is a normal subgroup, this implies $gag^{-1}\in N$ , for all $g \in G$ . Now, $a \in N$ , and $gag^{-1}\in N$ , this implies $(a)(gag^{-1})=(gag^{-1})(a)$ (because, $N$ is a normal subgroup of $G$ ). This shows that $a$ commute with all of each conjugates of $a$ . Converse:Let $a$ commute with all of its conjugate. i.e $(a)(gag^{-1})=(gag^{-1})(a)$ for all $g \in G$ . Let $N=\langle gag^{-1}\mid  g \in G\rangle$ . Clearly, $N$ is a normal subgroup of $G$ . Abelian: Let $gag^{-1},hah^{-1} \in N$ , then $(gag^{-1})(hah^{-1})=ga(g^{-1}hah^{-1}g)g^{-1}=g(g^{-1}hah^{-1}g)ag^{-1}=(hah^{-1})(gag^{-1})$ . This implies $N$ is a normal abelian subgroup of $G$ . Is my proof correct?","['group-theory', 'abstract-algebra', 'solution-verification', 'normal-subgroups', 'abelian-groups']"
3760892,What exactly does the definition of a nilpotent group mean?,"I'm studying nilpotent and solvable group and find it pretty hard to tell what the definition of a nilpotent group is after. For example, a group is solvable iff it has a solvable series (that is, a subnormal series such that each factor is abelian). This equivalent definition tells something clearly about the structure of the group for me. Then what about a nilpotent group? Since it's a condition stronger than solvable, in which part does it strengthen the equivalent defination above? Is there a true proposition like ""a group is nilpotent iff it has a subnormal series such that each factor is abelian and something else"" ?","['nilpotent-groups', 'group-theory', 'definition']"
3760895,How to convert $P \cdot (1+\frac{r}{m})^{m \cdot t}$ to $P_0 \cdot e^{k \cdot t}$?,"Future value formula is: $A=P \cdot (1+\frac{r}{m})^{m \cdot t}$ where, $A$ is resulting amount $r$ is annual interest $P$ is present value $n$ is number of compound periods per year $t$ is time (in years) And, exponential growth function is: $P(t) = P_0 \cdot e^{k \cdot t}$ The question is: A retirement account is opened with an initial deposit of $8,500 and earns 8.12% interest compounded monthly. What will the account be worth in 20 years? What if the deposit was calculated using simple interest? Could you see the situation in a graph? From what point one is better than the other? So to calculate the account worth in 20 years with exponential growth formula: $P_0$ is $8,500$ and $k$ is $0.812$ , months in 20 years is $P(240)$ and so: for the account worth in 20 years is: $P(240)=8500 \cdot e^{0.812 \cdot 240} = 3.67052\dots E88$ After calculating with future value formula, the answer is different: $A = 8500 \cdot (1+\frac{0.812 \cdot 12}{12})^{12 \cdot 20} = 7.71588\dots E65 $ I see different values when I calculate with exponential growth functions and future value formula. How to achieve this calculation correctly with exponential growth function? Is it possible?","['algebra-precalculus', 'exponential-function']"
3760959,Solve $2x^2+y^2-z=2\sqrt{4x+8y-z}-19$,"I am trying to solve the following equation. $$
2x^2+y^2-z=2\sqrt{4x+8y-z}-19
$$ To get rid of the square root, I tried squaring both sides which lead to $$
(2x^2+y^2-z+19)^2=16x+32y-4z
$$ which was too complex to deal with. Also, I have tried some substitutions to simplify the equation, but none of them were working. I believe that the equation could be solved with a appropriate substitution and factorization, yet I have no idea what to do. Any hint or help is appreciated.","['systems-of-equations', 'proof-writing', 'sum-of-squares-method', 'algebraic-equations', 'algebra-precalculus']"
3760974,Problem related to real monic quadratic polynomial,"Let $f(x)$ be a real monic quadratic polynomial. If ${x_1},{x_2},{x_3},{x_4},{x_5}$ be the $5$ points where $g(x) = |f(|x|)|$ is non-differentiable and $\sum_{i = 1}^5 {\left| {{x_i}} \right| = 8} $ then   find the value of $\frac{1}{5}\mathop {\lim }\limits_{x \to \infty } \frac{{{x^2} - f\left( x \right)}}{x}$ . My approach is as follow, real monic quadratic means that leading coefficient viz. the value of $a$ in $ax^2+bx+c=0$ is $1$ . That is the equation is of the form $x^2+bx+c$ , for real case $b^2-4c\ge0$ . But not able to approach.","['real-analysis', 'calculus', 'polynomials', 'limits', 'quadratics']"
3761005,Infinite number of apples in a box [duplicate],"This question already has answers here : A strange puzzle having two possible solutions (6 answers) Closed 3 years ago . Imagine I have an infinite source of numbered apples and a box of an infinite size. I take two apples from the source, put them in the box, then take one apple out: Step $1$ : apples $1$ and $2$ in - apple $1$ out Step $2$ : apples $3$ and $4$ in - apple $2$ out Step $3$ : apples $5$ and $6$ in - apple $3$ out ... Step $N$ : apples $(2N-1)$ and $2N$ in - apple $N$ out ... With each step the number of apples $S$ in the box increases, so: $$ \lim_{N\to\infty}S=\infty  $$ However, once I have gone though the infinite number of steps, the box ends up empty: $$ S_{N=\infty}=0 $$ Proof: If the box is not empty, it must contain at least one apple numbered $N$ . This is impossible, because I removed this apple in the step number $N$ according to the above procedure. Where is a flaw in this logic and how is this apparent paradox resolved?","['number-theory', 'infinity']"
3761014,"Let $P$ be a $3\times 3$ matrix such that all the entries are from $\{–1, 0, 1\}$. What is the maximum possible value of the determinant of $P$?","Let $P$ be a matrix of order $ 3 \times 3$ such that all the entries in $P$ are from the set $\{–1, 0, 1\}$ . What is the maximum possible value of the determinant of $P$ ? The original question This question is from JEE ADVANCED 2018, paper 2 My Attempt: $det(P)$ = $$
    \begin{vmatrix}
    a_1 & a_2 & a_3 \\
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    \end{vmatrix}$$ $$a_1(b_2c_3 – b_3c_2)– a_2(b_1c_3 – b_3c_1) + a_3(b_1c_2 – b_2c_1)\le6$$ value can be 6 only if $a_1 = 1, a_2 = \ –1, a_3 = 1 ,  b_2c_3 = b_1c_3 = b_1c_2 = 1 , b_3c_2 = b_3c_1 = b_2c_1 = \ – 1$ and $(b_2c_3) (b_3c_1) (b_1c_2) = \ – 1$ and $ (b_1c_3)(b_3c_2) (b_2c_1) = 1$ i.e $b_1b_2b_3c_1c_2c_3 = 1$ and $– 1$ hence not possible
Similar contradiction occurs when $ a_1 = 1, a_2 = \ –1, a_3 = 1, b_2c_2 = b_3c_1 = b_1c_2 = 1, b_3c_2 = b_3c_1 = b_2c_1 =\ – 1$ Now for value to be $5$ one the terms must be zero but that will make $2$ terms zero which means answer
cannot be $5$ Now, $$
    \begin{vmatrix}
    1 & 0 & 1 \\
    -1 & 1 & 1 \\
    -1 & -1 & -1 \\ 
    \end{vmatrix}
=4$$ Hence max value = $4$ I need the best way to solve this question(the process through which this question could be done in the least time possible and it should not be similar to my  approach (i.e any approach other than substitution and checking)","['linear-programming', 'matrices', 'maxima-minima', 'linear-algebra', 'discrete-optimization']"
3761015,When is $\det (X+Y) = \det X + \det Y$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $X$ be a symmetric $n\times n$ matrix and $Y$ an $n\times n$ matrix. Are there conditions which forces $\det (X+Y) = \det X + \det Y$ . Any comments, references are appreciated. Thank you.","['matrices', 'determinant', 'linear-algebra']"
3761054,How to evaluate $\int_0^{\pi/2} \frac{\sin x}{\sin^{2n+1}x +\cos^{2n+1}x} dx$?,"I have an exercise to evalute the following integral for all $n\geq 1 $ $$I(n)=\int_0^{\frac{\pi}{2}} \frac{\sin x}{\sin^{2n+1} x+\cos^{2n+1} x}dx$$ I attempted to find the closed form for the integral above in the following manner, where I used the integral identity $\int_a^bf(x)=\int_a^b f(a+b-x)dx$ . $$I(\bar{n})=\int_0^{\frac{\pi}{2}}\frac{\cos x}{\cos^{2n-1} x+\sin ^{2n-1} x}dx$$ adding $I(n)$ and $I(\bar{n})$ its reduces to $$\frac{1}{2}\int_0^{\frac{\pi}{2}}\frac{\cos x +\sin x}{\cos^{2n+1}x +\sin^{2n+1}x}dx$$ using the algebraic identity $a^n+b^n=(a+b)(a^{n-1}-a^{n-2}b+a^{n-3}b^2+\cdots +b^{n-1})$ for odd integers $n$ , I get $$\frac{1}{2}\int_{0}^{\frac{\pi}{2}}\frac{1}{\cos^{2n}x-\cos^{2n-1}\sin x+\cdots +\sin^{2n}x}dx $$ I'm now stuck here. How can I continue now?. Thanks in advance.","['integration', 'trigonometric-integrals', 'definite-integrals']"
3761061,$\prod_{k=1}^{m}\sin(kt)$ related identity,"Investigating the product $$\prod_{k=1}^{m}\sin(kt)$$ And became to the following enough trivial identity, but have a feeling that I am missing something here. Any hint about the following is appreciated. \begin{align}
\sum _{k=1}^m \log \sin (k t) &=
  \sum _{k=1}^m \left(-\sum _{n=1}^{\infty } \frac{\cos (2 k n t)}{n}-\log 2\right)=\\
  &=-\sum _{n=1}^{\infty } \sum _{k=1}^m \frac{\cos (2 k n t)}{n}-m \log 2=\\
  &=-\sum _{n=1}^{\infty } \frac{\csc (n t) \sin (m n t) \cos ((m+1) n t)}{n}-m \log 2
\end{align} So the final result would be: $$\prod_{k=1}^{m}\sin(kt)=2^{-m} \prod _{k=1}^{\infty } \exp\left(-\frac{\csc (k t) \sin (k m t) \cos (k (m+1) t)}{k}\right)$$","['products', 'trigonometry', 'complex-numbers', 'sequences-and-series']"
3761071,Prove that $\tan^{-1}\frac{\sqrt{1+x^2}+\sqrt{1-x^2}}{\sqrt{1+x^2}-\sqrt{1-x^2}}=\frac{\pi}{4}+\frac 12 \cos^{-1}x^2$,Let the above expression be equal to $\phi$ $$\frac{\tan \phi +1}{\tan \phi-1}=\sqrt{\frac{1+x^2}{1-x^2}}$$ $$\frac{1+\tan^2\phi +2\tan \phi}{1+\tan^2 \phi-2\tan \phi}=\frac{1+x^2}{1-x^2}$$ $$\frac{1+\tan^2\phi}{2\tan \phi }=\frac{1}{x^2}$$ $$\sin 2\phi=x^2$$ $$\phi=\frac{\pi}{4}-\frac 12 \cos^{-1}x^2$$ Where am I going wrong?,"['algebra-precalculus', 'solution-verification', 'trigonometry', 'inverse-function']"
3761096,Sets of infinite Hausdorff dimension in a second countable metric space,I am wondering if there exists an example of a second countable metric space $X$ containing a set $A$ with infinite Hausdorff dimension.,"['measure-theory', 'hausdorff-measure', 'metric-spaces', 'dimension-theory-analysis']"
3761103,Simple question related to Correlation & Covariance,"$X$ and $Y$ are two random variables with $\Bbb E[X] = \Bbb E[Y] = 1$ and $\Bbb E[X^2] = \Bbb E[Y^2] = 2$ . Which of the following is not possible: $\Bbb E[XY] > 0$ $\Bbb E[XY] < 0$ $\Bbb E[XY] = 0$ $\Bbb E[XY] \le 2$ I reached the following conclusions: $\operatorname{Cov}(X,Y) = \Bbb E[XY] - \Bbb E[X]\Bbb E[Y] = \Bbb E[XY]-1$ $\operatorname{Var}(X) = \Bbb E[X^2] - \Bbb E[X]^2 = \operatorname{Var}(Y) = 1$ $\operatorname{Corr}(X,Y) = \operatorname{Cov}(X,Y)$ I noticed that this value is the maximum value for both correlation and covariance since $$\operatorname{Corr}(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}$$ and $\operatorname{Corr}(X,Y) = \Bbb E[XY]-1$ but I still did not reach a conclusion regarding $\Bbb E[XY]$ . Any help is appreciated!","['statistics', 'covariance', 'correlation', 'random-variables']"
3761120,Separability of a polynomial by reducing to residue fields,"Let $A$ be a commutative ring, and $k(p)$ denote the residue field of $A$ at a prime $p$ , i.e. $A_p/p A_p$ .
Then say a polynomial $f(t) \in A[t]$ is separable if $(f,f')=1$ . In Milne's text on Étale Cohomology, Example 3.4, pg. 22, it is claimed that $(f,f')=1$ iff $(\bar f, \bar f')=1$ for all prime $p$ , where $\bar f$ is the image of $f$ in $k(p)[t]$ . I could only see $\Rightarrow$ . How does one argue the converse?","['algebraic-geometry', 'commutative-algebra']"
3761141,Why is $\mathbb{R}-\mathbb{Q}$ an uncountable set and how can I prove it?,"I am now starting to prepare for a discrete mathematics class. On a test, I came across the following question: Which of the following sets are countable? $$\mathbb{Z},\mathbb{R}, \mathbb{R-Q}, \{31,2,2019\} $$ The only countable sets are: $\mathbb{Z}$ (easily proved) and $\{31,2,2019\}$ as it is a finite set.
Using Cantor's method we prove that there is not a bijective function, such that $\mathbb{R}$ is countable. So there is only $\mathbb{R-Q}$ , which is the set of irrational numbers. Can anyone suggest a proper way for me to prove that this is a uncountable set?",['discrete-mathematics']
3761150,Holomorphic maps preserve Hausdorff dimension.,"In a paper I read there is the following claim: Let $f:\mathbb{C}\to \mathbb{C}$ be a non-constant entire transcendental function(essential singularity at infinity) and $A\subset \mathbb{C}$ a set in the complex plane. Then $f^{-1}(A)$ , $A$ and $f(A)$ have the same Hausdorff dimension. I know that bi-Lipschitz maps preserve Hausdorff dimension but I dont see why entire maps in the complex plane should too. Perhaps because entire maps are locally bi-Lipschitz away from critical points. But do locally bi-Lipschitz maps preserve the dimension? Can someone prove this or provide a reference for a proof?","['complex-analysis', 'dimension-theory-analysis', 'geometric-measure-theory']"
3761162,Power Series and Analyticity of a complex function,"I was studying about the connection of analytic function and their power series representation. Finally, I came to an understanding that, if I am given with an function, analytic at some point 'a', then I will be able to write a power series representation of that function, where that power series representation is convergent in some circle centered around that 'a'.
Now, what about the behavior points outside this circle of convergence? Can the function remain analytic at those points? In short, is it true if a function having a power series representation about a point is not convergent at a point outside the radius of convergence, then we cannot say about the analyticity of that function at that point. Is my understanding correct? Or Am I still missing the essence of the power series expansion?","['complex-analysis', 'power-series', 'analytic-functions']"
3761164,"If $H$ is a subgroup of a group $G$, is there a standard name for subsets of the form $xHy$?","Let $H$ be a subgroup of a group $G$ . The right (resp. left) cosets of $H$ are the subsets of $G$ of the form $Hg$ (resp. $gH$ ) for some $g \in G$ . Question . Is there a standard name for the subsets of the form $xHy$ (with $x, y \in G$ )? These sets occur naturally in the study of the power monoid of $G$ (the monoid of all subsets of $G$ under the product $XY = \{xy \mid x \in X, y \in Y\}$ ). I thought of double cosets or bilateral cosets but it seems to be used for other purpose.","['group-theory', 'terminology']"
3761182,Alternate way to solve $\lim\limits_{x \to 0} (\sin x) ^x$?,"My solution: $$ \lim_{x \to 0} (\sin x)^x = \lim_{x \to 0} e^{(x)(\ln \sin x)} = \exp \left( \lim_{x \to 0} (x) (\ln \sin x) \right)$$ Now we have $\lim_{x \to 0} (x) (\ln\sin x)$ . Now  we  can say that the limit is $0$ as $\ln$ , $\sin x$ decreases more slowly than $x$ . Question : Is there any other method to solve the above limit without using the arguments saying one function decreases slower?","['limits', 'calculus']"
3761254,"What is the closed form of the sequence $\{-\frac12,\frac19,\frac{13}{100},\frac{71}{588},\frac{71}{648},\frac{1447}{14520},\frac{617}{6760},...\}$?","I am trying to find the closed form of the following sequence: $$ \left\lbrace -\frac{1}{2} , \frac{1}{9} , \frac{13}{100} , \frac{71}{588} , \frac{71}{648} , \frac{1447}{14520} , \frac{617}{6760} , \frac{1061}{12600} , ... \right\rbrace \tag{1}$$ These form the coefficients $a_k$ with $k=1,3,5,...$ of a power series expansion of the following form: $$f(x) = \sum_{k=1,3,5,...}^\infty a_k x^k = a_1 x + a_3 x^3 + a_5 x^5 + ..., \tag{2}$$ for which I wish to find a closed form. I could not find anything in Wolfram Alpha nor in OEIS . Here is a plot of $(1)$ : Note that these coefficients were found numerically, and the rational forms given above agree up to at least 15 significant digits. If needed I can probably obtain any desired coefficient, although the precision may decrease. EDIT : Here is some context, as suggested in the comments. This sequence arises when trying to solve the following integral: $$I(x) = \frac{1}{2^9\pi^5} \int_{-\infty}^\infty d\tau \left\lbrace \frac{1}{1+\tau^2} \arctan^2 \frac{\tau}{x} + \frac{x}{x^2 + \tau^2} \arctan^2 \tau \right\rbrace \tag{3}$$ with $x>0$ . I managed to obtain the following form numerically: $$I(x) = \frac{1}{2^{13} \pi^4} + \frac{1}{2^{10}\pi^6} \tanh^{-1}(x) \left\lbrace \log(x) - \tanh^{-1}(x) + 2\log 2 \right\rbrace + \frac{1}{2^9 \pi^6} \sum_{k=1,3,5,...} a_k x^k. \tag{4}$$ So finding a closed form for $(1)$ would give me the exact solution of the integral $I(x)$ . Note that the integral exhibits the curious symmetry $I(x) = I(1/x)$ .","['closed-form', 'sequences-and-series']"
3761264,"Distance between set and point, confused of partial derivatives.","Let $H = \{(x,y,z)\ \in \mathbb{R}^{3}: x^2+y^2 - z^2 + 4 = 0$ Compute the shortest distance between H and point $p=(2,4,0)$ . I am a bit confused because I tried a direct approach. $$ x^2+y^2 + 4 =  z^2$$ Let $D(H,p) = \sqrt{(2-x)^{2}+(4-y)^{2} + x^{2} + y^{2}+4}$ So I tried compute $$\frac{\partial D}{\partial x} = (\sqrt{2} (-1 + x))/\sqrt{12 - 2 x + x^2 - 4 y + y^2}$$ $$\frac{\partial D}{\partial y} = (\sqrt{2} (-2 + y))/\sqrt{12 - 2 x + x^2 - 4 y + y^2}$$ It seems not nice to compare with zero. Do you have another idea?","['maxima-minima', 'derivatives']"
3761273,"Show that $E\left[\|V\|^2| (V+U,U) \in C \times C \right] < E\left[\|V\|^2 \right]=3$ where $V$ and $U$ are standard normal","Let $C\in \mathbb{R}^3$ be a cone. Specifically, assume that $C$ is given by \begin{align}
C=\{(x_1,x_2,x_3):  x_1 \le x_2 \le x_3 \}.
\end{align} I am interested in the  quantity \begin{align}
E\left[\|V\|^2| (V+U,U) \in C \times C \right]
\end{align} where $V\in \mathbb{R}^3$ and $U\in \mathbb{R}^3$ are independent  and standard normal. Question : Can we show that \begin{align}
E\left[\|V\|^2| (V+U,U) \in C \times C \right] < E\left[\|V\|^2 \right]=3.
\end{align} Things  that I have tried: Cauchy-Schwarz: \begin{align}
E\left[\|V\|^2| (V+U,U) \in C \times C \right] &=\frac{E\left[\|V\|^2 1_{ (V+U,U) \in C \times C} \right] }{ E[1_{ (V+U,U) \in C \times C}]}\\
& \le  \frac{ \sqrt{E\left[\|V\|^4 \right] E\left[ 1_{ (V+U,U) \in C \times C} \right]} }{ E[1_{ (V+U,U) \in C \times C}]}\\
&=\frac{\sqrt{15}}{\sqrt{ E[1_{ (V+U,U) \in C \times C}]}}.
\end{align} However, the above is large than $3$ . Re-writing : I was thinking that we can define $W=V+U$ in which case we have that \begin{align}
E\left[\|W-U\|^2| (W,U) \in C \times C \right],
\end{align} but this also didn't lead anywhere. I think the approach has to use the fact that $C$ is a cone, but I am not sure how to use it.  I also tried to move the problem into spherical coordinates but didn't get anywhere.","['conditional-probability', 'expected-value', 'probability-theory', 'conditional-expectation']"
3761278,Confirming solution to a Langevin equation using Fourier series,"Consider the following Langevin equation $$\frac{d^2 x}{dt^2}+\omega_n^2x=\eta(t),$$ where $\eta(t)$ has a gaussian probability distribution with mean zero and correlation $$\langle \eta(t) \eta(t')\rangle = D\delta(t-t'),$$ where $\delta(t-t')$ is the dirac delta function. In this post they show that the solution is $$\langle x(t) \rangle = 0$$ $$\langle x^2(t) \rangle = \frac{D}{2\omega_n^3}[\omega_n t - \sin(\omega_n t) \cos(\omega_n t)].$$ I want to confirm this result using a Fourier series method.
Let $$\eta(t) = a_0 X_0 + \sum_{k=1}^\infty a_k X_k \cos(\omega_k t) + b_k Y_k \sin(\omega_k t),$$ where $X_k$ , $Y_k$ are independent random gaussian variables with mean 0 and variance 1. By solving the differential equation for a sinusoidal driver we get $$x(t) = \sum_{k=0}^\infty x_k(t),$$ where $$x_k(t) = \begin{cases}
\frac{a_k X_k\omega_n[\cos(\omega_n t) - \cos(\omega_k t)] + b_k Y_k[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]}{\omega_n(\omega_k^2 - \omega_n^2)}, & \omega_k \ne \omega_n \\
\frac{a_k X_k\omega_n t \sin(\omega_n t) + b_k Y_k[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]}{2\omega_n^2}, & \omega_k = \omega_n.
\end{cases}$$ Therefore, the soltion should be given by $$\langle x(t) \rangle = 0,$$ $$\langle x(t)^2 \rangle = \sum_{k=0}^\infty\langle x_k(t)^2 \rangle,$$ where $$\langle x_k^2(t) \rangle = \begin{cases}
\frac{a_k^2\omega_n^2[\cos(\omega_n t) - \cos(\omega_k t)]^2 + b_k^2[\omega_k\sin(\omega_n t) - \omega_n\sin(\omega_k t)]^2}{\omega_n^2(\omega_k^2 - \omega_n^2)^2}, & \omega_k \ne \omega_n \\
\frac{a_k^2\omega_n^2 t^2 \sin^2(\omega_n t) + b_k^2[\sin(\omega_n t) - \omega_n t \cos(\omega_n t)]^2}{4\omega_n^4}, & \omega_k = \omega_n.
\end{cases}$$ Therefore, letting $a_k=b_k=D$ , shouldn't the formulas for the variances agree? I tried to confirm this in python but they are not the same. Here is the code and outputted figures: import numpy as np
import matplotlib.pyplot as plt

def white_noise(t):
    eta = a[0] * np.random.normal()
    for k in range(1,N+1):
        eta += a[k] * np.random.normal() * np.cos(omega[k] * t)
        eta += b[k] * np.random.normal() * np.sin(omega[k] * t)
    return eta

def variance_exact(t):
    return D / 2 / omega_n ** 3 * (omega_n * t - np.sin(omega_n * t) * np.cos(omega_n * t))

def variance_fourier(t):
    var_for = variance_fourier_term1(t, 0)
    for k in range(1,N+1):
        if omega[k] != omega_n:
            var_for += variance_fourier_term1(t, k)
        elif omega[k] == omega_n:
            var_for += variance_fourier_term2(t, k)
    return var_for

def variance_fourier_term1(t, k):
    term1  = a[k] ** 2 * omega_n ** 2 * (np.cos(omega_n * t) - np.cos(omega[k] * t)) ** 2
    term1 += b[k] ** 2 * (omega[k] * np.sin(omega_n * t) - omega_n * np.sin(omega[k] * t)) ** 2
    term1 = term1 / omega_n ** 2 / (omega[k] ** 2 - omega_n ** 2) ** 2
    return term1

def variance_fourier_term2(t, k):
    term2  = a[k] ** 2 * omega_n ** 2 * t ** 2 * np.sin(omega_n * t) ** 2
    term2 += b[k] ** 2 * (np.sin(omega_n * t) - omega_n * t * np.cos(omega_n * t)) ** 2
    term2 = term2 / 4 / omega_n ** 4
    return term2

P = 20
N = 1000
k_array = np.arange(N+1)
omega = np.pi * k_array / (2 * P)
omega_n = 1
D = 1

t_min = 0
t_max = P
nt = 1024
t = np.linspace(t_min, t_max, nt)

a = np.full(N+1, D)
b = np.full(N+1, D)
b[0] = 0

fig = plt.figure()
fig_size = fig.get_size_inches()
fig_size[0] = 2 * fig_size[0]
fig.set_size_inches(fig_size)

ax = fig.add_subplot(121)
ax.plot(t, variance_exact(t))
ax.set_xlabel('t')
ax.set_title(r' $\langle x^2(t)\rangle$ - Exact')

ax = fig.add_subplot(122)
ax.plot(t, variance_fourier(t))
ax.set_xlabel('t')
ax.set_title(r' $\langle x^2(t)\rangle$ - Fourier')

fig.savefig('temp_figures/variance_exact_vs_fourier.png', bbox_inches = 'tight')

plt.show(block = False) Edit: Fixed bug on line 25 of code.","['ordinary-differential-equations', 'noise', 'stochastic-differential-equations', 'physics', 'signal-processing']"
3761300,Linear transformation of a function vector space,"I'm having a bit of trouble solving a homework problem. I've given the specific problem below, but general answers are 100% welcome: Consider the following three bases for a vector space 𝑉, which is a subspace of $C^\infty(-\infty,\infty):$ $$B_1 = \{ 1, e^x, e^{2x}\}$$ $$B_2 = \{ 1, 1+e^{2x}, e^x -e^{2x}\}$$ $$B_3 = \{ 1+e^x, 1+e^{2x}, 1+e^x+e^{2x}\}$$ Obtain the following transition matrices: $$P_{B_1 \to B_2}, P_{B_2 \to B_1}, P_{B_2 \to B_3}$$ I've read about change of basis and done problems for questions like this in finite vector spaces ( $\mathbb{R}^n$ and $M_{nm}$ , etc). But the way we're given to solve equations involves utilizing the inverse of the basis, which I'm not certain on calculating. At first observation, I can see that I could pull out $\vec{x} = \{1,e^x,e^{2x}\}$ in order to at least simplify what I'm looking at: $$B_1 = A_1\vec{x} = \begin{bmatrix} 1&0&0\\0&1&0\\0&0&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ $$B_2 = A_2\vec{x} = \begin{bmatrix} 1&0&0\\1&0&1\\0&1&-1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ $$B_3 = A_3\vec{x} = \begin{bmatrix} 1&1&0\\1&0&1\\1&1&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ This allows me to view each basis at least in the sense of something which isn't one dimensional. I know from my text that a transition vector can be expressed as $P_{A \to B} = [B]^{-1}[A]$ (where A and B are bases), as well as: $$AA^{-1} = I\\
IA = A\\
(AB)^{-1} = B^{-1}A^{-1}\\
[B] = [\vec{u}_1| \vec{u}_2| ...| \vec{u}_n]\\
A\vec{x}=\vec{b}\\
\vec{x}=A^{-1}\vec{b}$$ With the final two holding true if A is invertible. I've expressed all the bases in the form $\vec{b} = A\vec{x}$ , so following the above logic (since every A from our bases is invertible), I believe the following should hold true: $$
P_{B_1 \to B_2} = B_2^{-1}B_1\\
... = (A_2\vec{x})^{-1}(A_1\vec{x})\\
... = (\vec{x})^{-1}A_2^{-1}A_1\vec{x}\\
\vec{x}P_{B_1 \to B_2} = A_2^{-1}A_1\vec{x}$$ However, I'm having trouble getting rid of or simplifying that $\vec{x}$ around. I'm also not certain if it's even appropriate to denote a non-square $\vec{x}^{-1}$ . But I feel like there must be a way to simplify $(\vec{x})^{-1}M\vec{x} \to M$ . Failing that, I also tried: $$rank(B_{1...3}) = 3\\
\therefore dim(V) = 3\\
\therefore \forall \vec{v} \in V, \vec{v} \in \mathbb{R}^3$$ So by this, I believe we can determine that while $C^\infty$ is an infinite-dimensional vector space, $V$ is actually a 3-dimensional subspace described by our bases. But I suppose this also leaves me a littlle confused, as it would mean e.g. $1 + e^x$ (from $B_{3,1}$ ) is actually a third-dimensional vector, which is what lead me to breaking down the bases given based on the variable inputs over addition. In short: Is my approach flawed or incorrect? If so, how? I'm a bit short on terminology since I'm learning, is there a name for what I did in that first step? Most importantly: How do I generate the transition vector for this subspace?","['matrices', 'change-of-basis', 'linear-algebra']"
3761303,"If $x$ and $y$ are integer variables that have value $0$ or $1$, then what does the expression $x + y - xy$ mean?","Answer Options: $1.$ Logical AND $2.$ Logical OR $3.$ Nothing, it makes no sense $4.$ Logical implies I'm stuck between Logical OR and ""Nothing, it makes no sense"" because when I create a truth table for $x  + y - xy$ , it is equivalent to Logical OR. But also, "" $-$ "" doesn't really mean anything in boolean logic, right? Please help me out here.","['logic', 'discrete-mathematics']"
3761342,Abelian finite groups and their subgroups,"I have asked a similar question about 40 days ago, it did not generate any answers perhaps because it was not well formulated. So I have deleted it. Here is another attempt. Consider pairs $(G,H)$ where $G$ is a finite abelian $p$ -group of exponent $p^n$ , $H<G$ . The direct product is defined as $(G,H)\times (G',H')=(G\times G', H\times H')$ . How many directly indecomposable pairs are there depending on $p, n$ ? Certainly if $G$ is cyclic $p$ -group then any pair $(G,H)$ is indecomposable. But I have found an indecomposable pair for $n=6$ and any $p$ where $G$ is not cyclic.","['abelian-groups', 'group-theory', 'finite-groups', 'p-groups']"
3761348,"What does it mean for $A(t)$ to be continuous, where $A(t)$ is a matrix?","A theorem I encountered in my differential equations class (during the section on systems of DEs, where linear algebra is used a lot) is: Let $A(t)$ be a continuous ( $n\times n$ )-matrix on an open interval $I$ . If $\vec x_1(t),\dotsc,\vec x_n(t)$ are linearly independent solutions to the homogenous system $\vec x'(t) = A(t)\vec x(t)$ on $I$ , then every solution has the form $\vec x(t) = c_1 \vec x_1(t) + \dotsb + c_n \vec x_n(t)$ What does it mean for a matrix to be continuous? This doesn't seem to be a common term because I couldn't find a thing about this online...","['systems-of-equations', 'ordinary-differential-equations', 'matrices', 'calculus', 'linear-algebra']"
3761357,Is it possible to solve $\frac{dy}{dx}=1+ax^my^2$?,"I have a problem with the following equation, $\frac{dy}{dx}=1+ax^my^2$ I had solution for $m=1$ using Mathematica. I need to find a general solution for $m$ . The equation can be rewritten by parametrizeing $y=\frac{u}{u'}$ as $u''+ax^mu=0$ ; i want declare that problem i introduced is a simplified form from  such equation! I need absolute solution (not approximate) for the general case. And its OK if the solution is in special functions (because, actually, its the only way to do it). Can anyone here help me please? Thank you guys","['calculus', 'ordinary-differential-equations']"
3761379,Functoriality and inverse images of sheaves,"I'm now learning sheaf theory from Gortz and Wedhorn's Algbraic Geometry 1 - Schemes and trying to understand the direct image and the inverse image of sheaves. In the book, the authors said that ...... Again the construction of $f^{+}\mathcal{G}$ and hence of $f^{-1} \mathcal{G}$ is functorial in $\mathcal{G}$ . Therefore we obtain a functor $f^{-1}$ from the category of presheaves on $Y$ to the category of sheaves on $X$ ....... Now let me clarify the notations. In the quoted sentences, $f: X \rightarrow Y$ is a continuous map and $\mathcal{G}$ is a presheaf of $Y$ . A presheaf on $X$ is defined by $$
U \mapsto \mathrm{colim}_{V \supset f(U), V \subset Y \,  \text{open}}  \mathcal{G}(V)
$$ and the restriction maps are induced by the restriction maps of $\mathcal{G}$ . We denote this presheaf by $f^{+}\mathcal{G}$ , and the sheafification of it by $f^{-1}\mathcal{G}$ . This is the inverse image of $\mathcal{G}$ under $f$ . My question is: What does the word "" functorial "" mean in the quoted sentences? I have looked up the book Categories for the Working Mathematician by Mac Lane, and only find the definition of natural when discussing natural transformations. It seems that this is the same as the word functorial ? Actually I'm lost in checking from the definition and finding what I need to verify (to show that $f^{-1} \mathcal{G}$ is functorial in $\mathcal{G}$ ). I've read the question and answers on What exactly is functoriality? but still feeling hard to find out what to check. Thank you for your helps!","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
3761385,How can I prove that $\int_{a}^{b} f(x)g(x) dx = g(a)\int_{a}^{c} f(x) dx +g(b)\int_{c}^{b} f(x) dx$? [duplicate],"This question already has an answer here : proof of the second generalized mean value theorem for integrals (1 answer) Closed 3 years ago . Let $f$ be a continuous function on $[a, b]$ . If $g$ is decreasing monotone and bounded in $[a, b]$ using the mean value theorem, prove that there exists $c \in [a, b]$ such that: $$\int_{a}^{b} f(x)g(x) dx   = g(a)\int_{a}^{c} f(x) dx +g(b)\int_{c}^{b} f(x) dx$$ I tried to do $h(x) = g(x)-g(a)$ but failed","['integration', 'calculus', 'real-analysis']"
3761387,Closure of range of injective compact operator on a Hilbert space,Let $\mathcal{H}$ be a separable Hilbert space and $C$ a compact operator on $\mathcal{H}$ . Assume that $C$ is injective. Is it then true that the closure of the range of $C$ is $\mathcal{H}$ ?,['functional-analysis']
3761393,Is Student's t-distribution valid when samples themselves have uncertainty - such as quantisation errors?,"NB: I was gonna post on physics stack exchange, not really sure where this fits in. But I'm only a lowly Engineer so please go easy on the notation if you can Using Student's t-distribution I can infer the parameters ( $\mu,\sigma^2$ ) of a probability distribution based on $n$ samples of data that I assume will fit a gaussian prior. However in all the examples I've seen, the $n$ samples are all simple values. How can I infer a probability distribution based on samples of data with uncertainty; if my $n$ samples are not simple values but probability distributions themselves? What is the effect of measurement uncertainty on the shape of the inferred distribution? Context I'm trying to measure how long some code takes to run on a computer. The timer is low resolution - similar order of magnitude to the duration I'm trying to measure - and so the true timestamps are quantized into 100 ms bins. Assuming a uniform rectangular probability distribution within these bins, then the time differences have a triangular probability distribution. i.e. A task starting at $142ms$ and ending at $331 ms$ when quantised will appear to start at $100\pm50ms$ and end at $300\pm50ms$ . Then the difference will be a triangular probability distribution, centered on $200ms$ and with a width of $\pm 100ms$ . I have several of these triangular timespan measurements, and I'd like to use them to determine the parameters of a distribution. As I say, I could just ignore the quantisation errors in my samples, and plug the modal (centre) values into the t-distribution, but surely those errors will increase the uncertainty ( $\sigma$ ) of my inferred gaussian?","['statistical-inference', 'statistics', 'probability-distributions', 'probability']"
3761405,How can $\sin(2nx)$ be expressed as a polynomial?,"Let $P_n (u)$ be a polynomial in $u$ of degree $n$ . Then, prove that for every positive integer $n$ , $\sin(2nx)$ is expressible as $(\cos x)(P_{2n-1} (\sin x))$ for some $P_{2n-1}$ . I tried by starting as $\sin(2nx)=2\sin(nx)\cos(nx),$ and ta-da, it satisfies for $n=1,2,3$ . But I can't figure it in general. P.S. Complex exponent is not allowed.","['trigonometry', 'polynomials']"
3761436,Comparison between SO(n) and Spin(n) representation theory [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question We know that $Spin(n)/\mathbb{Z}_2=SO(n)$ . The $SO(n)$ and $Spin(n)$ have the same Lie algebra. When it comes to the representation of $SO(n)$ and $Spin(n)$ , does it make any difference? $Spin(2n)$ has a $2^n$ -dimensional reducible spinor representation, and a $2^{n-1}$ -dimensional irreducible spinor representation. Does $SO(2n)$ have the same? How about other representations? $Spin(2n+1)$ has a $2^n$ -dimensional irreducible spinor representation (correct me if I was wrong). Does $SO(2n+1)$ have the same? How about other representations? Since Spin group is a double cover of SO group , how does this global structure being reflected in the case of representation? (if their representations are the same? or differed also by a double cover? perhaps the parameters of Lie group are ""doubled"" in some way?) Am I correct to say that SO group has integer spin representations , while Spin group has both integer and half-integer spin representations ? For example, the SO(3) group has a trivial representation, and other odd-rank dimensional matrix representation: $$
1,3,5,7,\dots.
$$ In contrast, the Spin(3) group has a trivial representation, and other odd and even-rank dimensional matrix representation: $$
1,2,3,4,5,6,7,\dots.
$$ The odd and even-rank dimensional matrix representation is related to what physicists call the integer and half-integer spin representations. Do $SO(n)$ and $Spin(n)$ both have complex and real representations ? How about quaternion representation?","['spin-geometry', 'representation-theory', 'group-theory', 'lie-groups', 'differential-geometry']"
3761451,Confusion about definition of Germ in The Rising Sea which seems circular,"I'm going to write down the definitions as from The Rising Sea: Foundations of AG - Nov 18 2017 draft , starting from 2.2.3. I feel that the definition of a germ is circular. The setting: We have a topological space $(X, \tau)$ and a function $F: (U: \tau) \rightarrow \operatorname{Diff}(U)$ , which assigns to each open set $U \in \tau$ , the set of differentiable functions over $U$ . Sections of a presheaf $F$ over an open set $U$ : For each open set $U \in \tau$ , we have a set $F(U)$ . The elements of $F(U)$ are called as the sections of $F$ over $U$ . Restriction Map: For each inclusion $U \hookrightarrow V$ ( $U \subseteq V$ ),
we have a restriction map $Res(V, U): F(V) \rightarrow F(U)$ . Identity Restriction: The map $Res(U, U)$ is the identity map. Restrictions Compose: If we have $U \subseteq V \subseteq W$ , we must have $Res(W, U) = Res(W, V) \circ Res(V, U)$ . Germ at a point $p$ (1): A germ of a point $p$ is any section over any open set $U$ containing $p$ .
That is, the set of all germs of $p$ is formally $\operatorname{Germs}(p) \equiv \{ F(U_p) : p \in U_p \in \tau \}$ .
We sometimes write the above set as $\operatorname{Germs}(p) \equiv \{ (f, U_p) : f \in F(U_p), p \in U \in \tau \}$ .
This way, we know both the function $f$ and the open set $U_p$ over which it is defined. Stalk at a point $p$ : A stalk at a point $p$ , denoted as $F_p$ ,
consists of equivalence classes of all germs at a point, where two germs are
equivalent if the germs become equal over a small enough set.
We state that $(f, U) \sim (g, V)$ iff there exists a $W \subseteq U \cap V$ such that
the functions $f$ and $g$ agree on $W$ : $Res(U, W)(f) = Res(V, W)(g)$ . Germ of $f$ at $p$ (2): If $p \in U$ and $f \in F(U)$ , then the image of $f$ in $F_p$ , as in, the value
that corresponds to $f$ in the stalk is called as the germ of $f$ at $p$ . This last definition doesn't make sense. We have already defined the germ at a point $p$ (1) before. Now we are re-defining the germ at point $p$ with definition (2). The definition (2) is an equivalence class of elements of definition (1). So when someone says ""germ"", which definition do they really mean? This feels quite circular.","['algebraic-geometry', 'sheaf-theory']"
3761475,Show that $P(A\mid B) > P(A\mid B^{c}) \implies P(B\mid A) > P(B\mid A^{c})$,"My initial thought is to start from $P(A\mid B)$ and $P(A\mid B^{c})$ and transform them to expressions involving $P(B\mid A)$ and $P(B\mid A^{c})$ respectively. $P(A\mid B) = \dfrac{P(A)P(B\mid A)}{P(B)}$ $P(A\mid B^{c}) = \dfrac{P(A) - P(B) + P(A^{c})P(B\mid A^{c})}{P(B^{c})}$ The expressions get quite messy when I try to compare them, and I am wondering if there is a better a way.","['conditional-probability', 'probability']"
3761477,Solving $\left(x-c_1\frac{d}{dx}\right)^nf(x)=0$ for $f(x)$,"I'm given that $$\left(x-c_1\frac{d}{dx}\right)^nf(x) =  0$$ I have to solve for $f(x)$ in terms of $n$ . For $n=0$ : $$f(x)=0 \tag{0}$$ For $n= 1$ : $$\begin{align}
xf(x) - c_1f'(x) &= 0 \\
\quad\implies\quad f(x) &= c_2\exp\left(\frac{x^2}{2c_1}\right) \tag{1}
\end{align}$$ For $n=2$ : $$\begin{align}
\left(x-c_1\frac{d}{dx}\right)^1(xf(x) - c_1f'(x)) &= 0 \\[4pt]
\quad\implies\quad x^2f(x) -xc_1f'(x) -c_1(f(x)+xf'(x)) +c_1^2f''(x) &=0 \\[4pt]
\quad\implies\quad f(x) = k_1\exp\left(\frac{-x^2}{2c_1}\right)
+ k_2x\exp\left(\frac{-x^2}{2c_1}\right) & \tag{2}
\end{align}$$ The case for $n= 3$ gets so complicated that I haven't put up the solution. The solution is based on hermitian polynomials.","['integration', 'ordinary-differential-equations', 'calculus', 'derivatives', 'hermite-polynomials']"
3761516,"Prove that for every integer $x$, if $x$ is odd then there exists an integer $y$ such that $x^2=4y+1$.","Prove that for every integer $x$ , if $x$ is odd then there exists an integer $y$ such that $x^2=4y+1$ . Let $x$ be an odd integer. Then, there exists an integer m such that $x=2m+1$ .  But $x^2=4m^2+4m+1=4(m^2+m)+1$ .
Case 1: $m$ is odd. Then there exists a $k$ such that $m=2k+1$ . Since $m^2=4k^2+4k+1$ , $m^2$ is odd. Hence, $m^2+m$ is even and $m^2+m=2y$ for some $y$ . Thus, $4(m^2+m)+1=4(y)+1$ as required.","['elementary-number-theory', 'algebra-precalculus', 'solution-verification', 'discrete-mathematics']"
3761518,"If $T_t$ is a diffeomorphism and $t\mapsto T_t(x)$ is differentiable, can we find a map $v$ with $v(t,T_t(x))=\frac{\partial T}{\partial t}(t,x)$?","Let $d\in\mathbb N$ , $\tau>0$ , $U\subseteq\mathbb R^d$ be open and $T_t$ be a $C^1$ -diffeomorphism from $U$ onto an open subset of $\mathbb R^d$ for $t\in[0,\tau)$ with $T_0=\operatorname{id}_U$ . Note that $$V:=\bigcup_{t\in[0,\:\tau)}T_t(U)$$ is open. Assume $[0,\tau)\ni t\mapsto T_t(x)$ is differentiable for all $x\in U$ . Can we show (under suitable additional assumptions, if necessary) that there is a $v:[0,\tau)\times V\to\mathbb R^d$ with $$v\left(t,T_t(x)\right)=\frac{\partial T}{\partial t}(t,x)\tag1$$ for all $t\in[0,\tau)\times V$ ? If $U=\mathbb R^d$ (and hence $V=\mathbb R^d$ ), we may simply set $$v(t,x):=\frac{\partial T}{\partial t}\left(t,T_t^{-1}(x)\right)\tag2.$$ EDIT 1 : I want to choose $v$ such that it is (jointly) continuous. By assumption, $$[0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3$$ is partially differentiable in both the first and the second variable. So, it should be differentiable and hence (jointly) continuous. EDIT 2 : I wonder whether any differentiability properties of $v$ with respect to the second variable carry over to $v$ . I've found the following excerpt in a book , which seems to indicate this, but I actually don't understand how they conclude (2.76):","['diffeomorphism', 'ordinary-differential-equations', 'inverse-function-theorem', 'real-analysis', 'implicit-function-theorem']"
3761523,Why is the Right Hand Rule for the vector product $\vec{a}\times\vec{b}$ true?,"Why is the Right Hand Rule true? The only thing that I'm searching for is its justification. Remember that $$\vec{a}\times\vec{b}=\begin{vmatrix}
a_{2} & a_{3}\\
b_{2} & b_{3}
\end{vmatrix}\hat{i}-\begin{vmatrix}
a_{1} & a_{3}\\
b_{1} & b_{3}
\end{vmatrix}\hat{j}+\begin{vmatrix}
a_{1} & a_{2}\\
b_{1} & b_{2}
\end{vmatrix}\hat{k}$$","['vectors', 'geometry']"
3761525,Does full rank matrix have a null space? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The null space is defined as all vector that is set to null by matrix $A$ , where $Ax = 0$ . If the matrix $A$ is full rank, does it mean that it has no null space?",['linear-algebra']
