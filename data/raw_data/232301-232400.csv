question_id,title,body,tags
4836528,Find all positive integers $n$ such that $n$ divides $9^{n-1}+3^{n-1}+1$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question Find all positive integers $n$ such that $n$ divides $9^{n-1}+3^{n-1}+1$ . I suspect there are no solutions other than $n=1$ . Assuming $n$ divides it gives $3^{3(n-1)} \equiv_n 1$ , from which one might start arguing using orders mod $n$ ? I'm unsure of how to proceed with this observation.","['number-theory', 'divisibility', 'elementary-number-theory']"
4836550,Finding the convergent interval of $z^{{2z}^{{3z}\ldots^{{nz}\ldots}}}$,"I was trying to find an exact solution to $x=a^x$ , and naturally derived a solution defined by infinite tetrations of $a$ . I first defined a recursion equivalent to the definition of tetrations and then used invariant points to show that $$\lim_{n \to \infty} (^nx)$$ converges for $x \in [e^{-e}, e^{\frac{1}{e}}]$ . Now, I am curious if I could extend the definition of tetrations and find the convergence interval of the following function. $$f(x)=x^{{2x}^{{3x}^{{...}}}}$$ I find this to be hard because I cannot, as in the case of normal tetrations, define a recursion that can be applied to define $f(x)$ . Any tips on how to proceed will be appreciated. Thanks a lot!","['real-analysis', 'complex-analysis', 'sequences-and-series', 'convergence-divergence', 'tetration']"
4836551,Solution of integral $\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{ 2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx dy$,"I am working on a simulation experiment on light where I concluded that the magnitude of intensity at each pixel is given by the equation - $$C_\text{total}=\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{
2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx\space dy$$ where $x$ and $y$ are the variables and rest of the symbols are constants. Problem and Motivation I am unable to solve the double integral expression on the RHS. The solution of this expression will enable achieving highest possible level of accuracy in the simulation. Current Efforts In present, I am using the Monte-Carlo method for evaluation of integrals. But, it is compute intensive. Greater the precision, greater the amount of computation required since more random sample points are required for precise evaluation of integrals. Considering my requirement of precisions up to $10^{-9}$ , this method is not enough to produce satisfactory accuracy in simulation since computing power is practically limited. Expectations To achieve satisfactory level of accuracy, I expect to find an analytical solution (rather than the numerical one being used presently). The best case is to have a proper solution, but, concepts like power-series expansion of integral would also be great.","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4836555,What is the minimum number of axioms necessary to define a (not necessarily commutative) ring (that doesn't have to have a one)?,"Motivation: According to this , the minimum number of axioms required to define a group is one. What can we say about rings (that are not necessarily commutative nor do they have to have a one)? The Question: What is the minimum number of axioms necessary to define a (not necessarily commutative) ring (that doesn't have to have a one)? Details: This is in the spirit of the link above: you can have as many operations as you like ( e.g. , $*$ and $(\cdot)'$ for an abelian group, under the axiom $$\forall x\forall y\forall z, ((x * y) * z) * (x * z)' = y,$$ as cited), as long as you use just one axiom. (I guess the closure of each operation is given.) I'm afraid I can't define this more rigorously. Thoughts: My guess is that two or three might suffice. Why? I don't know. I have no experience in this sort of thing, so please pitch your answers at an introductory level, perhaps for a bright undergraduate, if possible. This seems to be the domain of universal-algebra . I have included the tag logic , too, because it might attract people who are interested.","['universal-algebra', 'axioms', 'logic', 'ring-theory', 'abstract-algebra']"
4836658,Uniform Integrability and Conditional Expectations,"Let $X=\{X_n:n\in \mathbb{N_0}\}$ be a supermartingale or a submartingale on a filtered space $(\Omega,\mathcal{F},\{\mathcal{F}_{n}\},\mathbf{P}).$ Suppose the sequence $\{X_n\}_{n\in\mathbb{N_0}}$ is uniformly integrable $\left( \text{i.e.} \lim \limits_{\lambda \rightarrow \infty} \sup _{n \in \mathbb{N}_0} E\left(\left|X_{n}\right| \mathbf{1}_{\left\{\left|X_{n}\right|>\lambda\right\}}\right)=0\right)$ ,then  we have $\sup _{n \in \mathbb{N}_0} E\left(\left|X_{n}\right|\right)<\infty$ .By Doob's Martingale Convergence Theorem (discrete case),there exits a random variable $X_{\infty}$ on $(\Omega,\mathcal{F},\mathbf{P})$ ,such that $\lim \limits_{n \rightarrow \infty} X_{n}=X_{\infty}\text{ a.s.}$ .Since uniformly integrability of $\{X_n\}_{n\in\mathbb{N_0}}$ ,then $\lim \limits_{n \rightarrow \infty}\left\|X_{n}-X_{\infty}\right\|_{1}=0$ , that is, the sequence $\{X_n\}_{n\in\mathbb{N_0}}$ converges to $X_{\infty}$ in the $L^{1}$ norm. For any fixed $k\in\mathbb{N_0},$ can we have $$
\boxed{\mathbf{E}(X_\infty\big|\mathcal{F_{k}})=\lim \limits_{n \rightarrow \infty}\mathbf{E}(X_n\big|\mathcal{F_{k}})\text{ a.s.}\quad?}
$$ I doubt the validity of this equation.Surely, $X_n \to X_{\infty}$ in $L^1$ implies $\mathbf{E}(X_n\big|\mathcal{F_{k}})\to\mathbf{E}(X_{\infty}|\mathcal{F_{k}})$ in $L^1$ .But from this we only know that there exists a subsequence $\left\{\mathbf{E}(X_{n_{\ell}}\big|\mathcal{F_{k}}): \ell \in \mathbb{N}_{0}\right\}$ such that $$\mathbf{E}(X_\infty\big|\mathcal{F_{k}})=\lim \limits_{\ell \rightarrow \infty}\mathbf{E}(X_{n_{\ell}}\big|\mathcal{F_{k}})\text{ a.s.}\quad$$ I feel the condition $\lim \limits_{n \rightarrow \infty} X_{n}=X_{\infty}\text{ a.s.}$ doesn't contribute to deducing $$\mathbf{E}(X_\infty\big|\mathcal{F_{k}})=\lim \limits_{n \rightarrow \infty}\mathbf{E}(X_n\big|\mathcal{F_{k}})\text{ a.s.}$$ How to construct a counterexample to show its invalidity, or prove its correctness ?","['conditional-expectation', 'stochastic-processes', 'probability-theory', 'martingales']"
4836690,"What did Rota mean by ""one can define cumulants relative to any sequence of binomial type""?","$\newcommand{\E}{\mathbb{E}}$ Near the end of ""Finite Operator Calculus"" (1976), G.C. Rota writes: Note that one can define cumulants relative to any sequence of binomial type, e.g. the
factorial cumulants (Kendall and Stuart). The book he makes reference to is listed in the bibliography as ""M. G. Kendall and A. Stuart, “The Advanced Theory of Statistics”, Vol. I, Griffin, London, 1963"". I was only able to find the 1945 version of the book in abysmal quality, and I wasn't able to find any reference to ""factorial cumulants"" there (for instance, there is no reference to ""factorial cumulants"" or ""cumulants, factorial"" in the index). Perhaps, they were added in the 1963 edition. I am in a great need to understand this phrase by Rota, since it may have a direct connection to my current research topic, so any references or explanations would be greatly appreciated, especially if there is an established direct connection with free probability. Edit: My own guess is the following (based on the definition of factorial cumulants found in this article: https://arxiv.org/pdf/1012.0750.pdf ). Let $b_{k}(z)$ be a polynomial sequence of binomial type (for instance, lower (falling) factorial sequence, as in the article). Define as $$M_{b}(z)[X]:=\sum_{k=0}^{\infty}\frac{\mathbb{E}[b_{k}(X)]}{k!}z^k$$ the  moment generating function of $X$ . Then define $$S_b(z)[X]=\ln M_{b}(z)[X]$$ as the new cumulant generating function, such that $$\frac{d^{k}}{dz^{k}}\Bigg|_{z=0}S_{b}(z)[X]=\kappa_{b}(X)$$ and name these new objects cumulants associated to a given binomial type sequence. However, why does the sequence have to be of binomial type at all? Edit 2: I suppose, the reasoning behind this is the following: let $M_{b}(z)[X], M_{b}(z)[Y]$ be generating functions for the same binomial type sequence. Then their product is $$M_{b}(z)[X]M_{b}(z)[Y]=\sum_{n=0}^{\infty}z^{n}\sum_{k+l=n}\frac{\E[b_k(X)]\E[b_{l}(Y)]}{k! l!}=$$ $$=\sum_{n=0}^{\infty}\frac{z^n}{n!}\sum_{k=0}^{n}{n \choose k}\E[b_k(X)]\E[b_{n-k}(Y)]$$ If we denote $m_{j}(X)=\E[X^j]$ , then $\E[b_k(X)]=\sum_{l}{b_{kl}m_{l}(X)}$ where $b_k(x)=\sum_{l}b_{kl}x^l$ , and $m_{k}(X)$ is itself a binomial type sequence of sort, in the sense that for independent $X,Y$ $$m_{n}(X+Y)=\sum_{k=0}^{n}{n \choose k}m_{k}(X)m_{n-k}(Y)$$ so $\E[b_{k}(X)]$ is an ""umbral composition"" $b_{k}(\vec{m}(X))$ . Then $$M_{b}(z)[X]M_{b}(z)[Y]=\sum_{n=0}^{\infty}\frac{z^n}{n!}\sum_{k=0}^{n}{n \choose k}b_{k}(\vec{m}(X))b_{n-k}(\vec{m}(Y))\cong $$ $$\cong \sum_{n=0}^{\infty}\frac{z^n}{n!}b_{n}(\vec{m}(X+Y))=M_{b}(z)[X+Y]$$ (I write $\cong$ in the last transition because I am not entirely sure in the whole calculation and reasoning, and not yet comfortable with using umbral calculus language). If this reasoning is correct, then the virtue of generalized binomial type moments is the following: just like for regular moments, sum of independent random variables corresponds to multiplication of their moment generating functions. Based on this, I guess, the next step (besides checking the rigor of the computation above) is to prove that generalized binomial type cumulants are linear with respect to addition of independent random variables, and that corresponding central limit theorem exists for every binomial type sequence. But I've also read that a separate notion of independence of random variables can be devised from alternative cumulants, so I am a bit confused by now. Edit 3: I cross-posted this to MathOverflow.","['reference-request', 'moment-generating-functions', 'combinatorics', 'cumulants', 'probability-theory']"
4836696,Finding the value of $\int_0^{\frac{\pi}{2}} \frac{1}{\cos^6x + \sin^2x} dx$,"Happy New Year $2024$ everyone! This is my first post on this site. I was attempting this question: $$ \int_0^{\frac{\pi}{2}} \frac{1}{\cos^6x+\sin^2x} dx$$ I am relatively new to advanced calculus so I could only think of a couple of things to do with this. First try: substitute $\sin^2x =t$ . Didn't quite lead me anywhere, since the integrand now got even weirder. Second try: Try to substitute $\tan x =t$ and simplify the integrand. Doing that, if I am not wrong, the expression becomes: $$\int_0^{\infty} \frac{(t^2+1)^2}{t^2(t^2+1)^2+1}dt = \int_0^{\infty} \frac{t^4+2t^2+1}{t^6+2t^4+t^2+1} dt$$ I was unable to do anything after this. Someone please help me compute this integral. Thanks in advance!","['integration', 'calculus', 'trigonometric-integrals']"
4836697,Is the isometry group of a finite volume negatively curved manifold finite?,Is the isometry group of a finite volume negatively curved manifold finite? If $ M $ is compact then this is true see Why is the isometry group of a closed negatively curved manifold finite? But what if $ M $ is finite volume noncompact? Then does negative curvature still imply finite isometry group?,"['lie-groups', 'riemannian-geometry', 'differential-geometry']"
4836698,One point (non-Hausdorff) compactification of compact space,"A compactification of a space $X$ is an embedding $f:X \to Y$ so that (1) $Y$ is compact, (2) $f(X)$ is dense in $Y$ . If furthermore, $Y\setminus f(X)$ is a single point, we say it is a one point compactification. If we assume $X$ is a compact and Hausdorff space (thus LCH), does there exist a (non-Hausdorff) $Y$ , such that there exists $f:X \to Y$ that is a one-point compactification of $X$ ? If we require $Y$ to be Hausdorff, this is impossible see here . However, since compact does not imply closed in general, I'm not sure the case when we relax this condition. In particular, even if compactification exists, is it unique up to homeomorphism? $S^{1}$ is an example, but I cannot think of a compactification. (Edit: I think taking $S_1 \sqcup S_1$ and quotient it out by a space so that every two points on the circle are combined besides 1 point might be an example, but I need to think more carefully, it might not work)","['general-topology', 'compactification', 'compactness']"
4836789,Find all the numbers obtainable via $+$ and $\times$ on 10 to 1,"Happy New Year 2024!
I got to know that $$(10 + (9 + 8 \times 7) \times 6) \times 5 + 4 \times 3 \times 2 \times 1 = 2024.$$ I am wondering how many numbers can be represented by similar arithmetic operations ( $+$ and $\times$ ) on the numbers from 10 to 1. We can definitely write code to enumerate all the possible cases, but can we have better ways to find the numbers? More specifically, I mean $$10 \operatorname{op}_1 9 \operatorname{op}_2 8 \operatorname{op}_3 7 \operatorname{op}_4 6 \operatorname{op}_5 5 \operatorname{op}_6 4 \operatorname{op}_7 3 \operatorname{op}_8 2 \operatorname{op}_9 1$$ with legal parentheses,
where each $\operatorname{op}_i \in \{+, \times\}$ (we can consider "" $-$ "" and "" $/$ "" too! but less us stay with $\{+, \times\}$ first). Naive idea: If I am correct, we have at most $9! \times 2^9 = 185794560$ possible situations, where $9!$ is the number of different orders of operators that might be changed by parentheses, and $2^9$ is the number of combinations of operators. Reduce the cases: Ideas and techniques for improving the efficiency of enumeration are also appreciated!
For example, we can find all the full binary trees with $10$ leaves and use reserve Polish to compute the numbers.
Each case can be represented by a full binary tree, where the $10$ leaves are numbers and the $9$ non-leaves are operators.
Hence, the number of cases can be further bounded by $$C_9 \times 2^9 = 2489344,$$ where $C_9 = 4862$ is the $9$ -th Catalan number. Moreover, even finding the maximum possible number seems interesting and nontrivial. Related question: When considering all four arithmetic operations, the question would be equivalent to an existing question on the site ( Construct numbers using digits $123456789$ and the operations $+,-,×,÷$ ).
When only considering two operations ( $+$ and $\times$ ), the question accepts unique techniques mentioned in the answer by @Misha Lavrov, which is also mentioned by @Benjamin Wang.","['arithmetic', 'combinatorics']"
4836821,What’s the probability for this problem?,in class we were working on a question regarding our seating plan. The question was: we are a class of 32 students sitting in pairs of 2. What is the probability of one or more pairs existing in an original seating plan and after shuffling all students around? I made a small simulation in php that gives me an approximation of 40.3% but I don’t know how to calculate it. If anybody is interested in the simulation and wants to look for an error there please say so and I’ll add it later,['probability']
4836823,Find Uniform Minimum Variance Unbiased estimator (UMVU) using Lehmann Scheffé - showing statistic is complete,"Let $X_1,...,X_n$ be independent copies of a real-valued random variable $X$ where $X$ has Lebesgue density \begin{align*}
p_\theta(x) = \begin{cases} \exp(\theta-x),\quad x>\theta  \\ 
0, \quad\quad\quad\quad\;\ x\leq \theta, \end{cases}
\end{align*} where $\theta\in \mathbb{R}$ is an unknown parameter.
Let $S:=\min(X_1,...,X_n)$ . Find the Uniform Minimum Variance Unbiased (UMVU) estimator of $\theta$ . I already know that $S$ is sufficient for $\theta$ and that $T:=S-1/n$ is an unbiased estimator of $\theta.$ My idea is to apply the Lehmann-Scheffé thm. since then the UMVU is given by \begin{align*}
\mathbb{E}[T|S]=\mathbb{E}[S-1/n|S]=S-1/n.
\end{align*} Is this the correct approach?
If yes, for applying Lehmann-Scheffé, I would also need that S is a complete statistic. How do I show this properly? Edit : I tried to show completeness by definition, i.e. I setup the equation $\mathbb{E}_\theta[g(S)]=0 \;\forall \theta$ for some function $g$ and now want to show that $g(S)=0 \; \mathbb{P}_\theta$ -a.s. for all $\theta$ .
Since the $X_i$ are iid it is easy to see that the cdf is $F_S(x)=1-(1-P_\theta(x))^n$ , where $P_\theta(x)$ is the cdf of $X_i$ . By taking the derivative we get the pdf for $S$ : $f_S(x)=n\cdot p_\theta(x)(1-P_\theta (x))^{n-1}$ . $P_\theta (x)$ can be easily calculated and we get \begin{align*}
f_S(x)=n\cdot e^{n(\theta-x)}.
\end{align*} Hence, $\mathbb{E}_\theta[g(S)]=\int_\theta^\infty g(x)ne^{n(\theta-x)}dx$ has to be $0$ . Is it now enough to say that $g(S)=0 \; \mathbb{P}_\theta$ -a.s. for all $\theta$ , since the exponential function is always positive? Or is there a more rigorous way to show it?","['statistical-inference', 'statistics', 'parameter-estimation', 'exponential-distribution']"
4836828,"Am I using the condition ""$f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$"" correctly (Calculus on Manifolds by Michael Spivak)","I am reading ""Calculus on Manifolds"" by Michael Spivak. An open cover $\mathcal{O}$ of an open set $A\subset\mathbb{R}^n$ is admissible if each $U\in\mathcal{O}$ is contained in $A$ . If $\Phi$ is subordinate to $\mathcal{O}$ , $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ , $\{x:f\text{ is discontinuous at }x\}$ has measure $0$ , then each $\int_A \varphi\cdot |f|$ exists. The author wrote $\int_A \varphi\cdot |f|$ exists without a proof. So I forced to check $\int_A \varphi\cdot |f|$ exists. At first, it was necessary to consider why the above conditions are required. One of the above conditions is "" $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ "". I am new to such a property of functions. So, I want you to check if I am using the condition "" $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ "" correctly. I used the following theorems to check $\int_A \varphi\cdot |f|$ exists. 3-8 Theorem. Let $A$ be a closed rectangle and $f:A\to\mathbb{R}$ a bounded function. Let $B=\{x:f\text{ is not continuous at }x\}$ . Then $f$ is integrable if and only if $B$ is a set of measure $0$ . 3-11 Theorem. Let $A\subset \Bbb R^n$ and let $\mathcal{O}$ be an open cover of $A$ . Then there is a collection $\Phi$ of $C^\infty$ functions $\varphi$ defined in an open set containing $A$ , with the following properties: (1). For each $x \in A$ we have $0 \leq \varphi(x) \leq 1$ . (2). For each $x \in A$ there is an open set $V$ containing $x$ such that all but finitely many $\varphi \in \Phi$ are $0$ on $V$ . (3). For each $x \in A$ we have $\sum_{\varphi \in \Phi}\varphi(x)=1$ (by (2) for each $x$ their sum is finite in some open set containing $x$ ). (4). For each $\varphi \in \Phi$ there is an open set $U$ in $\mathcal{O}$ such that $\varphi = 0$ outside of some closed set contained in $U$ . (A collection $\Phi$ satisfying (1) to (3) is called a $C^\infty$ partiion of unity for $A$ . If $\Phi$ also satisfies (4), it is said to be subordinate to the cover $\mathcal{O}$ . In this chapter we will only use continuity of the functions $\varphi$ .) Even if we change ""closed"" in (4) with ""compact"", Theorem 3-11 still holds. About this, please see https://math.stackexchange.com/a/243671/1226161 . Let $\varphi\in\Phi$ . Then by (4), there is an open set $U\in\mathcal{O}$ such that $\varphi=0$ outside of some compact set $C$ contained in $U$ . Since $\mathcal{O}$ is admissible, $C\subset U\subset A$ . Since $\varphi$ is continuous, $\varphi(x)=0$ for $x$ on the boundary of $C$ . Let $a$ be an any point on the boundary of $C$ . For any positive $\varepsilon$ , there exists a neighborhood $V_1$ of $a$ such that $|\varphi(x)|=|\varphi(x)-\varphi(a)|<\varepsilon$ for any $x\in V_1$ since $\varphi$ is continuous at $a$ and $\varphi(a)=0$ . For any positive $\varepsilon$ , there exists a neighborhood $V_2$ of $a$ and $M$ such that $||f|(x)|<M$ for any $x\in V_2$ since $|f|$ is bounded in some open set around each point of $A$ . Then, $|\varphi\cdot |f|(x)-\varphi\cdot |f|(a)|=|\varphi(x) |f|(x)|=|\varphi(x)|\cdot |f(x)|<\varepsilon\cdot M$ for any $x$ in some neighborhoood of $a$ . So, $\varphi\cdot |f|$ is continuous at each $a$ on the boundary of $C$ . Since $\varphi\cdot |f|(x) = 0$ if $x\in A-C$ , so $\int_A \varphi\cdot |f|=\int_C \varphi\cdot |f|$ . Let $B$ be a closed rectangle such that $C\subset B$ . Let $g:B\to\mathbb{R}$ be a function such that $g(x) = \varphi\cdot |f|(x)$ if $x\in C$ and $g(x) = 0$ if $x\in B-C$ . Then, $g$ is continuous at each point $a$ in $B-C$ and $g$ is continuous at each point $a$ on the boundary of $C$ and $\{x\in C:\varphi\cdot |f|\text{ is discontinuous at }x\}\subset \{x\in A:f\text{ is discontinuous at }x\}$ has measure $0$ . So, $\int_C \varphi\cdot |f|$ exists. So, $\int_A \varphi\cdot |f|$ exists. I wrote $\int_A \varphi\cdot |f|$ as if I knew what $\int_A \varphi\cdot |f|$ means. But we don't know what $\int_A \varphi\cdot |f|$ means if $A$ is not bounded. So, I want to defne as follows: An open cover $\mathcal{O}$ of an open set $A\subset\mathbb{R}^n$ is admissible if each $U\in\mathcal{O}$ is contained in $A$ . Let $\mathcal{O}$ be admissible. Let $\Phi$ be subordinate to $\mathcal{O}$ . By (4) in Theorem 3-11, there is an open set $U\in\mathcal{O}$ such that $\varphi=0$ outside of some compact set $C_{\varphi}$ contained in $U$ . Since $\mathcal{O}$ is admissible, $C_{\varphi}\subset U\subset A$ . If $f:A\to\mathbb{R}$ is bounded in some open set around each point of $A$ , $\{x:f\text{ is discontinuous at }x\}$ has measure $0$ , then each $\int_{C_{\varphi}} \varphi\cdot |f|$ exists. The value of $\int_{C_{\varphi}} \varphi\cdot |f|$ does not depend on the choice of $C_{\varphi}$ . We define $\int_A \varphi\cdot |f|:=\int_{C_{\varphi}} \varphi\cdot |f|$ .","['multivariable-calculus', 'solution-verification', 'improper-integrals']"
4836841,Finding the value of $\beta(5)$ via a definite integral,"I was trying to compute the integral: $$ I = \int_0^1 \frac{(\operatorname{ln}x)^4}{1+x^2} dx$$ This is not a very difficult integral to evaluate if one knows the standard procedure used in evaluating such types of integral. I substituted $-\operatorname{ln}x = t$ and the integral becomes: $$ I = \int_0^{\infty} \frac{t^4 \cdot e^{-t}}{1+e^{-2t}} dt$$ $$ I = \int_0^{\infty} t^4 \cdot e^{-t} \cdot \sum_{n=0}^{\infty} (-1)^n \cdot e^{-2nt} dt$$ $$ I = \sum_{n=0}^{\infty} (-1)^n \cdot \int_0^{\infty} t^4 \cdot e^{-(2n+1)t} dt$$ $$ I = \sum_{n=0}^{\infty} (-1)^n \cdot \frac{(4!)}{(2n+1)^5} = 24\beta(5)$$ When I checked the value of $I$ in WolframAlpha, it suggested that it is equal to $\frac{5 \pi^5}{64}$ . So that must mean $$ \beta(5) = \frac{5 \pi^5}{24 \cdot 64} = \frac{5 \pi^5}{1536}$$ Which is indeed true. What I want to know is - Is there any other way to compute $\beta(5)$ , like how we compute $\zeta(2)$ ? Note that the $\beta$ function in this question refers to the Dirichlet beta function , not to be confused with the Eulerian Beta function.","['integration', 'definite-integrals', 'special-functions', 'calculus', 'sequences-and-series']"
4836849,Is a topology recoverable from its set of neighborhoods?,"Let $(X, \tau)$ be a topological space and $\tau$ be the set of opens. Let's call a pair of the form $(x, W)$ where $x$ is a point in $X$ and $W$ is a (not necessarily open) neighborhood of $x$ a neighborhood pair . Let's call the collection of all such neighborhood pairs $N$ . It seems pretty clear that we can convert back and forth between $(X, \tau)$ and $(X, N)$ without losing any information. An open set $U$ is a set that is a neighborhood of each of its points. The same token, a set $T$ is a neighborhood of $x$ if and only if it contains an open set $U$ that contains $x$ . I'm wondering what happens when you strip away the information about what point a neighborhood is a neighborhood of, i.e. $$ N \rightsquigarrow \{ W : (x, W) \in N \} $$ Call $N$ the set of neighborhoods corresponding to the topological space in question. Can we recover our topology from just the set of neighborhoods, neglecting completely which points they are neighborhoods of? . We definitely have a way of getting an answer by just intersecting a ton of topologies. Given a family of topologies $F$ over a shared point set $X$ , $\cap F$ is a topology. For proof, see here . Alternatively, let's check the axioms one at a time. $\varnothing$ is in $\cap F$ . $X$ is in $\cap F$ . if $A$ is in $\cap F$ and $B$ is in $\cap F$ , then $A$ and $B$ are in each $\tau \in F$ individually, thus $A \cap B$ is in each $\tau$ individually, thus $A \cap B$ is in $\cap F$ . Suppose $E \subset \cap F$ , then $E \subset \tau$ for each $\tau$ in $F$ . Thus $\cup E$ is in each $\tau$ , thus $\cup E$ is in $\cap F$ . Suppose our set of neighborhoods is $M$ . Let $\Lambda$ be the family of all topologies on $X$ . Let $L \subset \Lambda$ be the family of all topologies on $X$ whose set of neighborhoods is a superset of $M$ . $L$ is not empty. The discrete topology on $X$ will always be in $L$ . Let's call $\cap L$ $\sigma$ . $\sigma$ is a topology. There's no guarantee though that the set of neighborhoods of $(X, \sigma)$ will be $M$ though and no guarantee that $(X, \sigma)$ will be the topology we started with to produce $M$ . This construction always succeeds, regardless of whether it's possible to get a topology whose set of neighborhoods is $M$ or not. If there are multiple topologies whose set of neighborhoods is $M$ , this construction will give us the coarsest one.",['general-topology']
4836901,"If $(a,b,c)$ are the sides of a triangle, what is the probability that $ac>b^2$?","Let $a \le b \le c$ be the sides of a triangle inscribed inside a fixed circle such that the vertices of the triangle are distributed uniformly on the circumference. Question 1 : Is it true that the probability that $ac > b^2$ is $\displaystyle \frac{1}{5}$ . I ran a simulation by generating $1.75 \times 10^9$ triangle and counting the number of times $ac > b^2$ . The experimental data seems to suggest that probability converges to about $0.2$ . Note : For any triangle with $a \le b \le c$ , the triangle inequality implies $b < a+c < 3b$ . Now the condition $ac > b^2$ implies that $2b < a+c < 3b$ ; here the lower bound follows from AM-GM inequality. Hence all triangles for which $b < a+c < 2b$ are ruled out. For our problem, the condition $2b < a+c < 3b$ is necessary but not sufficient. Update : Changed the title in light of the comments and answer that relaxing the condition $a\le b \le c$ is easier to handle Related question: If $(a,b,c)$ are the sides of a triangle and $x \ge 1$ , what is the probability that $a+b > cx$ ?","['geometric-probability', 'geometry', 'triangles', 'inequality', 'probability']"
4836955,Determine all positive integers $n$ such that: $n+d(n)+d(d(n))+\dotsb=2023$.,"For a positive integer number $n>1$ , we say that $d(n)$ is its superdivisor if $d(n)$ is the largest divisor of $n$ such that $d(n)<n$ . Additionally, we define $d(0)=d(1)=0$ . Determine all positive integers $n$ such that: $$n+d(n)+d(d(n))+\dotsb=2023.$$ I didn't even understand this problem when I began solving it. I just now realized that if $d(n)<n$ it must mean that $d(d(n))<d(n)$ , so the values of the sum must be decreasing, that means that after a finite number of steps we are going to have $d(d(\dotso d(n))\dotso)=d(1)=0$ , so then I tried by saying that $n$ cannot be prime, because the smallest prime smaller than $2023$ is $2017$ , so for $n=2017$ we have $$n+d(n)+d(d(n))+\dotsb=2017+1++0+0+0+0+\dotsb=2018,$$ so $n$ cannot be prime, but I am not sure what to do from there.","['number-theory', 'summation', 'divisor-sum', 'elementary-number-theory']"
4837031,Let $A\subset\mathbb{R}$ be an uncountable set of irrational numbers. Does there exist a finite $B\subset A$ such that $\sum_{x\in B} x\in\mathbb{Q}?$,"Let $A\subset\mathbb{R}$ be an uncountable set of irrational numbers. Does there exist a nonempty finite subset $B\subset A$ such that $\displaystyle\sum_{x\in B}x \in \mathbb{Q}\ ?$ If we change ""uncountable"" to ""countable"" then the answer is trivially no, as $\ A=\{ q\pi: q\in\mathbb{Q}_{>0} \}\ $ is a counter-example. I believe there is no analogue to this counter-example to my question above. I am unsure how to answer the question, although I sense that maybe the Baire Category Theorem could be helpful, but I have poor familiarity with this theorem and it's applications.","['general-topology', 'irrational-numbers', 'examples-counterexamples', 'real-analysis']"
4837065,Fixed point property and compactness,"A topological space $X$ has fixed point property if for every continuous map $f:X\to X$ there exists $x\in X$ such that $f(x) = x$ . Its easy to see that fixed point property is a topological property, so that its preserved by homeomorphisms. If $X$ has fixed point property, then it has to be connected, since if $X$ is not connected we can find open non-empty disjoint sets $U, V\subseteq X$ such that $U\cup V = X$ , and taking $x\in U, y\in V$ and letting $f(z) = \begin{cases} y & z\in U\\ x & z\in V\end{cases}$ we see that $f$ is a continuous map without a fixed point. But it doesn't have to be path-connected, since topologist's sine curve is an example of a space with fixed point property that's not path-connected. Most examples of spaces with fixed point property are spaces like $[0, 1]^n$ , topologist's sine curve or Warsaw circle. All of those spaces are compact. Is every space with fixed point property also compact? If not, can we obtain any partial results that make this true?","['general-topology', 'fixed-points', 'compactness']"
4837081,Multivariable function having partial derivatives almost everywhere are necessarily measurable?,"Let $f : \mathbb{R}^n \to \mathbb{R}$ be a function such that partial derivatives \begin{equation}
(\partial_i f)(x) := \lim\limits_{h \to 0} \frac{f(x + h e_i)-f(x)}{h}
\end{equation} exists for all $i=1,\cdots,n$ and almost every $x \in \mathbb{R}^n$ . Then I wonder if $f$ itself is Lebesgue-measurable on $\mathbb{R}^n$ . Here, $n \geq 2$ is arbitrary. Could anyone please help me? I think it is true at least for $n=2$ but cannot generalize further.","['partial-derivative', 'measure-theory', 'measurable-functions']"
4837102,Find the max of this function,"$$f(x) = x(2+x)e^{-x}$$ I have to find $\max_{\mathbb{Q}} \{ f(x) \}$ but I don't know how to proceed. I started with $f'(x) = (-x^2+2)e^{-x}$ whence the stationary points are given when the $x$ coordinate is $x = \pm \sqrt{2}$ Yet those points do not belong to $\mathbb{Q}$ . I thought like ""I hav to find the closest possible point to $\sqrt{2}$ , but it doesn't exist. I can always find a closer rational number from the one I found (call it $p$ ) and $\sqrt{2}$ . My conclusion would be that $f(x)$ has no maximum in $\mathbb{Q}$ , but it does on $\mathbb{R}$ or if we want to be fancy, it even does on $\mathbb{R}\backslash\mathbb{Q}$ . Please correct my wrongs.","['analysis', 'maxima-minima', 'calculus', 'optimization', 'rational-numbers']"
4837166,Rolling a dice until the average of the outcome hits a specific value.,"I roll the dice until the average of the outcomes is $2.5$ . Once the average is $2.5$ , I will stop rolling. For example, if I get a sequence of 2, 6, 4, 1, 1, 1 outcomes, I will stop rolling the dice at the sixth trial. Let $E$ be the event of stopping rolling the dice and $E_n$ be the event of getting the average $2.5$ after rolling the dice exactly $n$ times.
Then, $E = E_1 \cup E_2 \cup E_3 \cup E_4 \cup \cdots$ . Since it will not end with odd trials, I can rewrite it as $E = E_2 \cup E_4 \cup \cdots$ . Since ${E_{2n}}^\complement \supseteq \bigcup_{k=2n}^{\infty} E_{2k}$ , $E_{2k}$ 's are disjoint. Thus, $Pr(E) = \sum_{k=1}^{\infty} Pr(E_{2k})$ . However, I am stuck with calculating the exact probability of each event $E_{2k}$ for large $k$ . How can I calculate the probability of each event $E_{2k}$ ( $Pr(E_{2k})$ for $k>1$ ) and finally the probability of stopping rolling the dice $Pr(E)$ ? It seems that $Pr(E_{2k})$ goes to zero as $k\rightarrow \infty$ . Then, would the $Pr(E)<1$ ? Or, is the probability of stopping rolling the dice eventually one? Also, I wonder if there is any related theorem that I can use to calculate such events.","['statistics', 'dice', 'probability']"
4837178,How to prove that $f(x)=\frac{ \cot(\frac{\pi}{x+1}) }{ \cot(\frac{\pi}{x}) }\cdot\frac{x}{x+1}$ is strictly deceasing for $x>2$?,"I read an article that claimed that among two regular polygons with equal perimeters, the one with more sides has a larger area, in other words $\frac{  \cot(\frac{\pi}{n+1})  }{ \cot(\frac{\pi}{n}) }\cdot\frac{n}{n+1} >1$ for $n \ge 3$ . I became curious about $f(x):=\frac{  \cot(\frac{\pi}{x+1})  }{ \cot(\frac{\pi}{x}) }\cdot\frac{x}{x+1}$ and decided to graph it. It seemed to be strictly decreasing on $(2,\infty)$ $$f'(x)=\frac{\cot(\frac{\pi}{x}) \cdot \csc(\frac{\pi}{x+1})\cdot \frac{\pi}{(x+1)^2}- \cot(\frac{\pi}{x+1}) \cdot \csc(\frac{\pi}{x})\cdot \frac{\pi}{x^2}}{\cot^2(\frac{\pi}{x}) }\bigg(1 -\frac{1}{x+1} \bigg) +\frac{  \cot(\frac{\pi}{x+1})  }{ \cot(\frac{\pi}{x}) }\cdot\frac{1}{(x+1)^2}
$$ I tried to prove this observation, but I failed. Finding the derivative of the function and then showing that $f′(x)<0$ for $x>2$ seemed impossible and dreadful, as the derivative involved trigonometric and rational functions. I also want to know how to prove that $\frac{\cot(\frac{\pi}{x})}{x}$ is strictly increasing for $x\ge 3$ or in other words  among two regular polygons with equal perimeters, the one with more sides has a larger area","['calculus', 'derivatives', 'monotone-functions', 'inequality']"
4837192,"If $f\in L^1(\Bbb R)$, what is $\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = ?$","Given $f\in L^1(\Bbb R)$ , compute the limit $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx.$$ My work. Of course, $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx \le 2\|f\|_1 < \infty.$$ We know that step functions are dense in $L^1(\Bbb R)$ . My first thought is to find an expression for $f = \mathbf{1}_{(a,b)}$ , i.e., the indicator function of an interval. For simplicity, take $a = 0, b= 1$ . Then, $$
\begin{align*}
\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx &= \int_0^1 1\, dx + \int_t^{t+1} 1\, dx = 2 = 2\|f\|_1 
\end{align*}$$ as $\|f\|_1 = 1$ . It is easy to see that a similar equality holds for $f = \mathbf{1}_{(a,b)}$ in general. If I can show that $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = 2\|f\|_1$$ for functions $f = \sum_{i=1}^n a_i \mathbf{1}_{E_i}$ where $E_i$ 's are disjoint intervals, then I can hope to use an approximation argument for the result. Suppose the result holds for all step functions. Then, given $f\in L^1(\Bbb R)$ , there is a sequence $\{f_n\}_{n\ge 1}$ of step functions such that $\|f - f_n\|_1 \to 0$ . We have $$\lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = 2\|f_n\|_1$$ for every $n\ge 1$ . Certainly, $\|f_n\|_1 \to \|f\|_1$ as $n\to \infty$ . The hard part is showing that $$\lim_{n\to\infty} \lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx.$$ Thanks for your help!","['analysis', 'real-analysis']"
4837248,Max Eigenvalue Norm,"Let $X$ be a vector space over $\mathbb{C}$ . The set $\mathcal{L}(X)$ of all linear mappings $A: X \to X$ is a vector space over $\mathbb{C}$ . Prove, or disprove with a counterexample, the following assertion: \begin{align*}
\rho(A) = \textrm{max}\{|\lambda| \in \mathbb{R} : \lambda \textrm{ is an eigenvalue of $A$}\}
\end{align*} defines a norm on $\mathcal{L}(X)$ . It seems to me that this is not a norm. For a counterexample, consider any nonzero nilpotent operator $N$ . Since $N$ is nilpotent, its only eigenvalues are 0, hence $\rho(N) = 0$ . However, for $\rho$ to be a norm, $\rho(X) = 0$ if and only if $X = 0$ , so $\rho$ is not a norm. Is there something I'm missing here?","['normed-spaces', 'linear-algebra']"
4837284,"Showing $\int^\infty_{-\infty} \frac{\sin(x/n)}{x} \prod_{i=1}^n \cos(x/i)dx>c/n$, for some $c,N>0$ and all positive integers $n>N$","I’d like to show for some $c,N>0$ and all positive integer $n>N$ that $$\int^\infty_{-\infty} \frac{\sin(x/n)}{x} \prod_{i=1}^n \cos\left(\frac xi\right)dx>\frac cn$$ If I just look at the integral near $0$ , I can get that for $x$ much less than $1$ , the cosine terms are positive and approximately quadratic and the sin term is odd and approximately linear,  so the integrand is positive and around $(1-\pi^2 x^2/6)/n$ which when integrating from $-1$ to $1$ gives a positive value proportionate to $1/n$ , but I’m having trouble bounding the potentially large oscillations of the tails. I have noticed that $x$ and $2\pi k n! -x$ give the same value for the cosine product and opposite signs for the sine, so this could lead to a significant amount of cancellation, but making this rigorous seems tricky. This problem came up while I was looking at the characteristic function of the partial sums of a random variant of the harmonic series while digging into this problem: How often do we expect a random walk with decreasing step size to cross $0$?","['integration', 'inequality', 'trigonometry', 'trigonometric-integrals']"
4837286,How am I factorizing the $\frac{d}{dx}$ operator while trying to solve for the equation of a catenoid bubble?,"I've been working on my Math IA for a while, its a project part of the IB course which requires me to write a ~20 page math paper investigating and exploring something within maths. I chose to write my paper on using the Euler Lagrange equation to solve for the function which minimizes the surface area of a bubble between two rings, aka a catenoid. I did all the math for the paper a while ago, and I'm revisiting it to write the introduction and conclusion. While rereading the math, something didn't make sense to me. My final answer was correct, but a step didn't make sense or just didn't seem right. A section of my working out (which I took from a couple sources) starts off with this equation, from which we have to solve for $f$ . $$f'\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0$$ and from here we can replace the first $f'$ with $\frac{d}{dx}f$ , giving us: $$\frac{d}{dx}f\sqrt{1+(f')^2} - f'\frac{d}{dx} \left( \frac{ff'}{\sqrt{1+(f')^2}} \right)=0$$ and then for some reason, I factorise the operator $\frac{d}{dx}$ to get: $$\frac{d}{dx} \left[ f\sqrt{1+(f')^2} - \left( \frac{f(f')^2}{\sqrt{1+(f')^2}} \right) \right]=0$$ and from there I just integrate, solve for $f'$ , and solve the separable diff equation to solve for $f$ in terms of $x$ ( $\cosh$ ). The step I am confused about is when I factorize out the $\frac{d}{dx}$ , since its an operator. The math works because I got the correct answer, but why does it work, to me that shouldn't work but for some reason it does. Is there a reason for why this works? If so what is it.","['integration', 'ordinary-differential-equations', 'euler-lagrange-equation', 'calculus', 'derivatives']"
4837296,How to find the area between two overlapping quarter circles?,"Angle BAC = Angle YXZ = 90º. AB = XY = 6. Angle BXY = 60º. X is the midpoint of AB. Find the shaded areas. I coloured the area I could get red. I'm asking on any advice/solutions to the blue area. Solution for red area: Angle XAS = 90º (supplementary angles)
XS = 6
XA = 3
therefore triangle XAS is a scaled {1, √3, 2} triangle.
therefore AS = 3√3 Angle PXA = 30º (supp. angles)
XA = 3
therefore AP = 3/√3
= √3 SP = AS - AP
= 3√3 - √3
= 2√3 red area = Sector XZS - triangle XPS
Sector XZS = 36π(30º/360º)
= 3π
Triangle XPS = 2√3 * 3 / 2
= 3√3
therefore red area = 3(π - √3) I don't know how to start on the blue area, as mentioned before. I don't know trig that well yet as I'm still in middle school, but I know enough to understand solutions using it. Some ways that I tried are mirroring it on AB and solving for sector (XR'R - AQ'Q) adding and subtracting the other neccesary areas, and using a circumscribed square minus the unshaded areas, but I couldn't get far with those.","['area', 'circles', 'geometry']"
4837337,Recovering a binary function on a lattice by studying its sum along closed paths,"I have a binary function $f:\mathbb N^2\rightarrow\{0,1\}$ . While I do not known $f$ explicitly, I have a ""device"" located at the origin $(1,1)$ which can do the following: Given an even number $m$ , the device runs over all closed walks of length $m$ that start and end at the origin. After going through all possible walks, the device gives me a list of all sums of the values of $f$ along each of these walks (without saying which sum corresponds to which walk). This list could potentially have repetitions of course (if I get the same sum from more than one walk). To clarify, for me a closed walk of length $m$ is an ordered list of $m+1$ vertices, such that each two vertices in the list are adjacent to one another (and two lattice points are adjacent if they differ by 1 in exactly one of their components). In our case, the first and last vertices on the list are the origin. Here is an example (showing only part of the infinite lattice): Here, red points correspond to the value $1$ and blue points to the value $0$ . If I take $m=0$ , the only closed walk at the origin is the one starting and ending there without moving (and the sum along this walk is $1$ ), and so the device would return the list $(1)$ . This measurement allows me to deduce that $f(1,1)=1$ . If $m=2$ , I have two closed walks of length $2$ - going up and back, or going right and back. The list of sums along these walks are $(3, 2)$ (but again - I don't know which sums corresponds to which walk). Since I know that the value at the origin is $1$ , I can tell that either $f(1,2)=1, f(2,1)=0$ or vice versa. Personally I only care about $f$ up to reflection, so I'm fine with assuming WLOG the first option and continuing. The question is, can I continue this process in a way which will definitely allow me to recover $f$ at an arbitrary point away from the origin? (up to reflection) The difficulty comes from the fact that this list of sums becomes more and more complicated (you have more and more closed walks, and they are more complex). Since I know only sums (up to multiplicity) but not which closed walks a given sum corresponds to, it's hard to recover the value at a specific point. Even when looking at closed walks of length $m=6$ and more I am not sure how to recover the values of $f$ at each point (if that's even possible). So the question is - can this be done? Is there an algorithm? Will it help if I can add finitely many ""devices"" at points that I choose? (not just one at the origin) Hard mode - can you take the lattice $\mathbb Z^2$ ? Can you do this for dimension higher than $2$ ? Any help would be appreciated. Thanks in advance.","['interpolation', 'inverse-problems', 'combinatorics', 'discrete-mathematics', 'algorithms']"
4837347,An exercise about structure sheaf of product of two algebraic varieties,"I am interested in the Exercise 5.5.8 of this lecture notes . I have my own solution for this exercise but I need someone here to verify if there are any flaws in my arguments. To recall, this exercise requires you to show that (i) Given two algebraic varieties $X,Y$ and a fixed point $P_0 \in X$ , then the map $\varphi: Y \to X \times Y, Q \mapsto (P_0,Q)$ is a morphism, (ii) Furthermore, if $\mathcal{O}_X(X)=k$ , then $\mathcal{O}_{X \times Y}(X \times Y) \cong \mathcal{O}_Y(Y)$ . My ideas are (i) Each algebraic variety can be written as a union of (not necessarily finite) open affines, so we write $X=\bigcup_{i \in I} U_i$ and $Y=\bigcup_{j \in J} V_j$ , where $U_i$ and $V_j$ are open affines. Then $P_0$ must live in some $U_{i_0}$ and similarly, a random element $Q \in X$ must live in some $V_{j_0}$ . Now $\varphi(V_{j_0})=\{P_0\} \times V_{j_0} \subseteq U_{i_0} \times V_{j_0}$ and the restriction $\varphi_{|V_{j_0}}: V_{j_0} \to U_{i_0} \times V_{j_0}$ determined by $Q \mapsto (P_0,Q)$ is a morphism by Prop 4.3.9 in this lecture notes. (ii) I use the map in part (i) to prove this part. Let $\pi_Y$ is the projection from $X \times Y \to Y$ , it is not hard to see that $\pi_Y \circ \varphi$ is the identity over $Y$ . Then $(\pi_Y \circ \varphi)^*=\varphi^* \circ \pi_Y^*$ is also the identity over $\mathcal{O}_Y(Y)$ , which tells that $\varphi^*$ is an isomorphism having the inverse $\pi_Y^*$ , and thus an isomorphism between the two $k$ -algebras mentioned in the statement. What I am concerned about is that I did not use the assumption $\mathcal{O}_X(X)=k$ at all, so I think my arguments for part (ii) are wrong. However I cannot figure out where I made the mistakes. Also, we knew that $\mathcal{O}_{X \times Y}(X \times Y) \cong \mathcal{O}_X(X) \otimes_k \mathcal{O}_Y(Y)$ if both $X$ and $Y$ are affine varieties, but this lecture notes does not tell if this isomorphism is also true in the general case. I also read this MO post but it does not satisfy me as well. Any helps are really appreciated. Happy New Year 2024 to everyone!","['morphism', 'algebraic-geometry']"
4837363,"Motivation behind locally convex spaces, seminorms, and Frechet spaces","I am looking for some motivation behind the definition of locally convex spaces, seminorms, and Frechet spaces. Since all three concepts are related I have grouped them as one question. I am familiar with the technical definitions but I don't see what would lead one to defining them. What is so special about locally convex spaces that we wish to focus are analysis only on them? I am aware that seminorms are generalizations of norms but with the condition $\|x \| = 0 \implies x = 0$ dropped. But why do we care about such objects? The idea of angles and inner products would naturally lead one to define a Hilbert space and similarly length would lead one to define Banach spaces. Is there any similar idea that a Frechet space captures?","['topological-vector-spaces', 'frechet-space', 'functional-analysis', 'locally-convex-spaces', 'soft-question']"
4837412,Simplify a formula with 449 terms - Radical circle,"Context The other day I wanted to answer this question . Which is now closed so doesn't accept answers (but this isn't the important part). Since I didn't know the topic I went to look it up but I saw that there isn't much about it online, so out of curiosity I wanted to see what the formula for the center and the radical radius was . I consider 3 generic circles in the Cartesian plane: $$c_i: (x-x_i)^2+(y-y_i)^2=r_i^2\qquad\text{for }i=1,2,3$$ By doing a few steps I arrived at these formulas: $$\mathbf{C}=(x_C,y_C)=\left(\frac{\begin{vmatrix}
y_2y_3+x_1^2-r_1^2&y_1&1\\
y_3y_1+x_2^2-r_2^2&y_2&1\\
y_1y_2+x_3^2-r_3^2&y_3&1
\end{vmatrix}}{2\begin{vmatrix}
x_1&y_1&1\\x_2&y_2&1\\x_3&y_3&1
\end{vmatrix}}, \frac{\begin{vmatrix}
x_1&x_2x_3+y_1^2-r_1^2&1\\
x_2&x_3x_1+y_2^2-r_2^2&1\\
x_3&x_1x_2+y_3^2-r_3^2&1
\end{vmatrix}}{2\begin{vmatrix}
x_1&y_1&1\\x_2&y_2&1\\x_3&y_3&1
\end{vmatrix}}\right)$$ and $$r=\begin{cases}\sqrt{\left(x_C-x_{1}\right)^{2}+\left(y_C-y_{1}\right)^{2}-r_{1}^{2}}\\
\sqrt{\left(x_C-x_{2}\right)^{2}+\left(y_C-y_{2}\right)^{2}-r_{2}^{2}}\\
\sqrt{\left(x_C-x_{3}\right)^{2}+\left(y_C-y_{3}\right)^{2}-r_{3}^{2}}\end{cases}$$ Question The 3 formulas for calculating the radius are all equivalent and correct, but I would like them not to depend so directly on a specific center. In simple words: I would like a formula that is ""regular/symmetrical"" like the one for the center and that it does not directly depend on the fact of necessarily having to calculate $x_C$ and $y_C$ first, because I'm pretty sure it can be written as the ratio of two determinants of some matrix (the denominator matrix is ​​that of the determinant of the coordinates of the centers). Obviously I tried to develop the formulas but the formulas are excessively long (only the term $(x_C-x_1)^2$ alone contains 224 monomials , so in total there are 449 terms), could anyone help me in some way to obtain the formula efficiently? Essentially there would be this calculation to do: $$r=\sqrt{\left(\frac{\begin{vmatrix}
y_2y_3+x_1^2-r_1^2&y_1&1\\
y_3y_1+x_2^2-r_2^2&y_2&1\\
y_1y_2+x_3^2-r_3^2&y_3&1
\end{vmatrix}}{2\begin{vmatrix}
x_1&y_1&1\\x_2&y_2&1\\x_3&y_3&1
\end{vmatrix}}-x_{1}\right)^{2}+\left(\frac{\begin{vmatrix}
x_1&x_2x_3+y_1^2-r_1^2&1\\
x_2&x_3x_1+y_2^2-r_2^2&1\\
x_3&x_1x_2+y_3^2-r_3^2&1
\end{vmatrix}}{2\begin{vmatrix}
x_1&y_1&1\\x_2&y_2&1\\x_3&y_3&1
\end{vmatrix}}-y_{1}\right)^{2}-r_{1}^{2}}$$","['euclidean-geometry', 'analytic-geometry', 'circles', 'geometry', 'calculus']"
4837421,"In a triangle, the semiperimeter $\le$ sum of the medians $\le$ the perimeter","Let $EFG$ be a triangle such that $P$ , $N$ and $M$ are the midpoints of $EF$ , $EG$ and $FG$ respectively. The problem is to show that: $$Q/2< EM+FN+GP<Q,$$ where $Q$ is the perimeter of the triangle $EFG$ . My approach: I figured that I'd try using the triangle inequality in the triangles formed inside, but that didn't lead me to anything. So any hint or help are much appreciated.","['triangles', 'inequality', 'geometry']"
4837427,Simplifying $\arctan\biggl(\frac{\sqrt{2+\sqrt{3}}}{\sqrt{2}}\biggr)-\arcsin\biggl(\frac{3}{2\sqrt{4+\sqrt{3}}}\biggr)$,"After some calculations I have $$\arctan\biggl(\frac{\sqrt{2+\sqrt{3}}}{\sqrt{2}}\biggr)-\arcsin\biggl(\frac{3}{2\sqrt{4+\sqrt{3}}}\biggr).$$ Mathematica simplyfies this to $\pi/12$ and I wonder how it is done. Edit: The results comes from calculating $x$ in the figure below. There might be simple ways to do it than mine. I add my solution: Cosinus Theorem: $$
|AB|^2
=1^2+(\sqrt{3}\,)^2-2\cdot1\cdot\sqrt{3}\cdot\cos(120°)
=1+3-2\sqrt{3}(-\tfrac{1}{2})
=4+\sqrt{3}.
$$ Pythagoras: $$
|AC|^2+|AB|^2=|BC|^2
\quad\Leftrightarrow\quad
|AC|^2
=|BC|^2-|AB|^2
=4+\sqrt{3}-(\sqrt{2}\,)^2
=4+\sqrt{3}-2
=2+\sqrt{3}
$$ i.e. $$\textstyle|AC|=\sqrt{2+\sqrt{3}}.$$ Sinus Theorem $$
\frac{\sin(y)}{\sqrt{3}}=\frac{\sin(120°)}{\sqrt{4+\sqrt{3}}}
\quad\Leftrightarrow\quad
\frac{\sin(y)}{\sqrt{3}}=\frac{\frac{\sqrt{3}}{2}}{\sqrt{4+\sqrt{3}}}
\quad\Leftrightarrow\quad
\sin(y)
=\frac{3}{2\sqrt{4+\sqrt{3}}}
$$ i.e. $$
y=\arcsin\biggl(\frac{3}{2\sqrt{4+\sqrt{3}}}\biggr)
$$ Further, $$
\tan(x+y)
=\frac{|AC|}{|AB|}
=\frac{\sqrt{2+\sqrt{3}}}{\sqrt{2}}
$$ i.e. $$
x+y=\arctan\biggl(\frac{\sqrt{2+\sqrt{3}}}{\sqrt{2}}\biggr).
$$ and $$
x=\arctan\biggl(\frac{\sqrt{2+\sqrt{3}}}{\sqrt{2}}\biggr)-\arcsin\biggl(\frac{3}{2\sqrt{4+\sqrt{3}}}\biggr)
$$",['trigonometry']
4837442,Nonsymmetric matrix has real eigenvalues,"Consider a $n\times n$ symmetric matrix $A=\{a_{i,j}\}_{i,j}$ with nonnegative values, i.e., $a_{i,j}\ge0$ . It is known that $A$ must have real eigenvalues. Now, consider numbers $s_1,\dots, s_n\ge0$ . Is it true that $\{s_ia_{i,j}\}_{i,j}$ has real eigenvalues? For a better visualization of the matrix: \begin{align*}
\{s_ia_{i,j}\}_{i,j}=\begin{bmatrix}
    s_1 a_{11} & s_1 a_{12} & s_1 a_{13} & \dots  & s_1 a_{1n} \\
    s_2 a_{21} & s_2 a_{22} & s_2 a_{23} & \dots  & s_2 a_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    s_n a_{n1} & s_n a_{n2} & s_n a_{n3} & \dots  & s_n a_{nn}.
\end{bmatrix}
\end{align*} In the two dimensional case $n=2$ , it is not hard to see that indeed there cannot be complex eigenvalues. However, calculations become immediately very messy in the three dimensional case. Some numerical tests in the 3-dimensional case suggests the conjecture should be true.","['symmetric-matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4837452,$A$ is diagonalizable $\iff A^{-1}$ is diagonalizable.,"I used that proposition while doing an exercise and later realized it hadn't been demonstrated in class so i proceeded trying to prove it myself as it follows: Let $A\in M_n(\Bbb{K})$ be invertible, than exists $A^{-1}\in M_n(\Bbb{K})$ ; Let then  V be a vector space and $L_A=:f\in End(V)$ . Now if $A$ is diagonalizable there exist a base $\mathscr{B}=\left( v_1,\dots,v_n \right)$ for V such that $$M_{\mathscr{B}}\left(f\right)=
\begin{pmatrix}
\lambda_1 & 0 & \dots & \dots & 0 \\
0 & \lambda_2 & 0 & \dots & 0 \\
\vdots & 0 & \ddots & & \vdots \\
\vdots & \vdots & & \ddots & 0\\
0 & 0 & \dots & 0 & \lambda_n \\
\end{pmatrix}
$$ Now since $f\circ f^{-1}=Id_V$ , it must be true that $M_{\mathscr{B}}\left(f\right) \cdot M_{\mathscr{B}}\left(f^{-1}\right)=Id_n$ ; therefore $$ M_{\mathscr{B}}\left(f^{-1}\right)=
\begin{pmatrix}
1\over\lambda_1 & 0 & \dots & \dots & 0 \\
0 & 1\over\lambda_2 & 0 & \dots & 0 \\
\vdots & 0 & \ddots & & \vdots \\
\vdots & \vdots & & \ddots & 0\\
0 & 0 & \dots & 0 & 1\over\lambda_n \\
\end{pmatrix} 
$$ and so we found that $A$ being diagonalizable implies $A^{-1}$ is diagonalizable. Now, since the inverse of $A^{-1}$ is actually $A$ , for the same argument we can conclude that $A^{-1}$ being diagonalizable implies $A$ is diagonalizable and therefore the thesis. Is my argument correct? Could I prove it quicker? Because I believe my proof is correct as much as I believe it could be done in a smarter way.","['matrices', 'diagonalization', 'linear-algebra']"
4837510,positively homogeneous function that differentiable at $0^k$,"Let $f:\Bbb R^k \to \Bbb R$ be continious function and positively homogeneous function with degree $\alpha$ in $\Bbb R^k \setminus {0^k} $ (the definition of  positively homogeneous function in our course is $f(\lambda\cdot x)=\lambda^\alpha \cdot f(x)$ for $\lambda > 0$ and x $\neq 0^k$ ) if $\alpha > 1$ and $f(0^k)=0$ prove that f is differentiable at $0^k$ note :
the definition of differentiability at $0^k$ in our course is $lim_{h\to 0^k}$ $f(0^k+h)-f(0^k)-\nabla f(0^k)\cdot h \over |h|$ = $0$ I succeeded in proving that $∇f(0^k) = 0^k$ , which means that with the data $f(0^k) = 0$ What is left to prove is $lim_{h\to 0^k}$ $f(0^k+h) \over |h|$ = $0$ The question is taken from an old exam in Calculus 3","['multivariable-calculus', 'special-functions', 'real-analysis']"
4837514,Can a ring have only principal ideals but not be a domain?,"This question might be a bit simple for many of those reading this text, but I was wondering why, given the fact that $\mathbb{Z}_8$ is not even an integral domain (and therefore not a PID), all the ideals of this ring are generated by one element. Is there any ideal in this ring generated by two elements? If the answer is negative, doesn't it imply that $\mathbb{Z}_8$ is a PID? Can a ring have only principal ideals but not be a domain?","['ring-theory', 'abstract-algebra']"
4837534,Injective holomorphic function is surjective,"I know my problem is similar to this question , but I'm not allowed to use Picard's theorem here, so this question does not answer mine. Also, I'm looking to finish the proof I've already started. Let $f : \mathbb{C} \rightarrow \mathbb{C}$ be an injective holomorphic function. I want to prove $f$ is surjective. We know any holomorphic function has its image dense in $\mathbb{C}$ and since $f$ is injective we also have $f' \neq 0$ which means, by the inverse function theorem, that there is a neighborhood around each point in which $f$ is invertible. I want to show this inverse is the same for all points in the plane, that is we have a unique inverse of $f$ making it bijective. I'm stuck here. Any idea would be welcome.",['complex-analysis']
4837553,Prove that there is only one global solution to an ODE,"Consider the following ODE $$y\left(x\right)=\frac{x-ay'\left(x\right)}{1+b y'\left(x\right)^{2}},$$ where $a,b>0$ (a d'Alembert's / Lagrange equation). Let $k>0$ denote the unique solution to $bk^{3}+k-1=0$ . Then, conjecturing a linear solution to the ODE, we find that this ODE has a unique, global solution given the boundary condition $y(0)=-\frac{ak}{1+bk^{2}}$ . I would like to show that for any other boundary condition, any smooth solution to the ODE is not global. This appears to be true numerically -- solutions converge to imaginary values of y -- but I am not sure how to prove this.",['ordinary-differential-equations']
4837566,Gamma Distribution parameters as function of n,"I have a line with $n$ spaces initially empty, and in each iteration, I randomly choose one of them to insert an element. If it already has 1, the iteration doesn't add anything to the space. Eventually, after enough amount of iterations, all spaces will be full. Then, to visualize the number of iterations needed to finish the process (fill all spaces), I repeat the process with a fixed number of spaces and come up with the following histogram: The above graph is the result of 1 million process repetitions with $n=10$ , showing a distribution of the number of iterations for the specific size. As you can see, I tried to fit the histogram with a Gamma Distribution. My question is, how can I create an expression that outputs the parameters $k$ and $\theta$ of the gamma distribution as a function of $n$ ? I also tried to fit the histogram with a Negative Binomial distribution too, as the probability of success (inserting element) in each iteration changes over time, but it seems a better option to continue with the Gamma distribution since it has a continuous nature.","['statistics', 'probability-distributions', 'coupon-collector', 'discrete-mathematics', 'probability']"
4837573,"Bacteria either die or double depending on whether it turns left or right. If we start with $n$ bacteria, what's the best long-term survival strategy?","Start with $n\geq 4$ bacteria. At each round i.e. turn , you get to choose how many bacteria go left; the remaining bacteria must go right.
Each bacteria either dies, or, survives and reproduces creating another healthy bacteria, depending on whether it goes left or right (assume bacteria do not overlap or any other shenanigans). In other words, either left is certain death (for all the bacteria who go left) and right is certain reproduction (for all the bacteria who go right), or right is certain death (for all the bacteria who go right) and left is certain reproduction (for all the bacteria who go left). Each scenario has a $50$ % chance of happening each round/turn, independently of all other rounds/turns. What is the best or approximately the best strategy to maximise the
likelihood that there are $k>n$ bacteria at the end of $t$ rounds/turns? [The ""left or right"" is arbitrary. I could also say ""Either goes through door $1$ or door $2$ "", or any other binary decision.] For example, Start with $n=1000$ bacteria, and let $A_j$ be the number of bacteria at the $j$ -th turn. We want to maximise the chance that there are at least $k=10,000$ bacteria by the $50$ th round/turn, i.e. maximise the chance that $A_{50}\geq 10,000.$ A strategy that is guaranteed to fail is to make $500$ go left and $500$ go right for every round/turn, for then the population will always be $0 + 500\times 2 = 1000,$ contrary to our aim of reaching a population of $10,000.$ We are better off taking some risk in the first round/turn, for example make $550$ go left and $450$ go right. Then there is a $50$ % chance we increase the population up to $0 + 550\times 2 = 1100,$ and we are closer to our goal of $10,000.$ On the other hand, there is a $50$ % chance we decrease the population to $450\times 2 + 0 = 900,$ taking us further away from our goal. Obviously in order to risk increasing the bacteria population, you must risk reducing the number of bacteria by the same amount (I guess it's a zero sum game), and since we only care about reaching $10,000$ bacteria in total, all possible strategies have a high probability of failure. My suspicion is that either all strategies that try to be successful have the same likelihood of working, or, the best strategy is something like: Send $0.45A_t$ bacteria left and the remaining $0.55A_t$ right at every round/turn. But maybe sending all bacteria left for three rounds/turns until there are $8,000,$ (with $12.5$ % probability) and then doing something like in the second half of the previous sentence for up to $10,000?$ Edit: maybe we require Markov chains to help, although I have not studied the topic of Markov chains before.","['statistics', 'normal-distribution', 'markov-chains', 'game-theory', 'probability']"
4837647,"Strong law of large numbers for $\mathrm{Bin}(n, p_n)$ variables","Massive edit to simplify the central question Suppose $X_n\sim \mathrm{Bin}(n, p_n)$ be a collection of independent random variables such that $np_n\to \infty$ . Can we say that $Y_n:=X_n/np_n\to 1$ almost surely? It is easy to show using Chebyshev, for example, that indeed $Y_n\stackrel{p}{\to}1$ . I tried using the fourth moment bound which is one of the elementary methods to prove almost sure convergence, but I need summability of $\sum_n 1/(np_n)^2$ which is certainly not true for all choices of $p_n$ , for example if $p_n=\log(n)/n$ . What would be a general approach to solve such kinds of questions? Thanks in advance! Question: Is it possible to somehow use Kronecker's lemma adapted to triangular sequences?","['law-of-large-numbers', 'binomial-distribution', 'almost-everywhere', 'convergence-divergence', 'probability-theory']"
4837744,How did Feynman produce solutions in under a minute?,"As detailed here and elsewhere, Feynman and others at Los Alamos could calculate many problems to 10% accuracy in minutes: When I was at Los Alamos I found out that Hans Bethe was absolutely topnotch at calculating... A few minutes later we need to take the cube root of $2 \frac 1 2$ ... and he says, ""It's about $1.35$ .""... I had a lot of fun trying to do arithmetic fast, by tricks, with Hans... I announced, ""I can work out in sixty seconds the answer to any problem that anybody can state in ten seconds, to 10 percent!"" ... People started giving me problems they thought were difficult, such as integrating a function like $\frac 1 {1 + x^4}$ , which hardly changed over the range they gave me. The hardest
one somebody gave me was the binomial coefficient of $x^{10}$ in $(1 + x)^{20}$ ; I got that just in time.... [Paul Olum] says, $\tan 10^{100}$ . I was sunk: you have to divide by $\pi$ to $100$ decimal places! What methods do Feynman, Bethe, Olum, use to do these? Or, since we can't really know the answer to that: What methods can we use to easily approximate calculations within 10% error? Now, one might simply respond: Wolfram Alpha .  But we do this not for lack of a calculator!  For example, Sanjay Mahajan requires his students to show ""number sense"" by Without a calculator, estimate $\sqrt{1.3}, \sqrt[3]{1.6}, \sin 7,$ and $1.01^{100}$ . What methods can we use to do this?  I'll post my collection as an answer below.  Surprisingly, while I've found good methods for estimating logs, exponents, and trig, approximate long division (in less steps than the real thing) proves to be the hardest!","['approximation', 'real-analysis', 'calculus', 'taylor-expansion', 'numerical-methods']"
4837758,Some properties in Projective Geometry,"I am trying to understand the following two Propositions in James Oxley's book ""Matroid Theory"" Prop. 6.1.3 Let $M$ be a simple rank-r matroid and $\mathbb F$ be a field. The following statements are equivalent: (i) $M$ is $\mathbb F$ -representable. (ii) $PG(r - 1, \mathbb F)$ has a finite subset $T$ such that $M \cong PG( r - 1, \mathbb F)|T.$ (iii) For some $m \geq r,$ there is a finite subset $S$ of $PG(m - 1, \mathbb F)$ such that $M \cong PG( m - 1, \mathbb F)|S.$ So, my understanding is that $(ii)$ and $(iii)$ are saying almost the same thing i.e., $(ii)$ is a special case of $(iii)$ when $m = r.$ Am I correct? Prop. 6.1.4 Let $k$ be a non-negative integer. When $\mathbb F$ is the finite field $GF(q):$ $(i)$ The number of rank-k flats in $PG(r - 1, q)$ is the Gaussian coefficient ${r \brack k}_q.$ $(ii)$ The number of $k$ -element independent sets in $PG(r - 1, q)$ is $$\frac{1}{k!}\frac{q^r - 1}{q - 1}\frac{q^r - q}{q - 1} \dots \frac{q^r - q^{k-1}}{q - 1}.$$ $(iii)$ The number of $k$ -element circuits in $PG(r - 1, q)$ is 0 for $k <3$ and is $$\frac{1}{k!(q - 1)}(q^r - 1)(q^r - q)\dots (q^r - q^{k-2} \text{   for  } k \geq 3.$$ I want to: Describe $PG(r - 1, q)/I$ for a $k$ - element independent set $I$ in $PG(r - 1, q).$ My guess is: Using the first Prop.(ii),if we denote all the independent sets of $PG(r - 1, q)$ by $\mathcal{I}$ then $PG(r - 1, q)|(\mathcal{I} - I)$ is isomorphic to a simple matroid of rank r  - r(I). I can also say that by (ii) in the second proposition   The number of $k$ -element independent sets in $PG(r - 1, q)$ is $$\frac{1}{k!}\frac{q^r - 1}{q - 1}\frac{q^r - q}{q - 1} \dots \frac{q^r - q^{k-1}}{q - 1} - 1.$$ I am not sure if what I am saying above is correct or no, could someone explain this to me please?","['projective-geometry', 'matroids', 'algebraic-graph-theory', 'discrete-mathematics', 'affine-geometry']"
4837767,Question about exc. $15.2.21.$b in Dummit and Foote.,"This is an excerpt (paraphrased) from Dummit and Foote (I believe 3rd edition): Let $V \subset \mathbb{A}^n$ be an algebraic set and let $f \in k[V]$ . If $J$ is the ideal generated by $I(V)$ and $x_{n+1}f-1$ in $k[x_1,\ldots,x_{n+1}]$ , show that $J = I(Z(J))$ . The $J \subset I(Z(J))$ always holds, it is the $I(Z(J)) \subset J$ direction I am having trouble with. I think have shown that $Z(J) = \{(v,\frac{1}{f(v)}):v \in V_f\}$ is but I don’t know what conclusions to draw from this. Here, $V_f = \{v \in V: f(v) \neq 0\}$ . Possibly, one can observe that $g = g+x_{n+1}f-1$ on $Z(J)$ . Also, if we fix $\frac{1}{f}$ in the $x_{n+1}$ coordinate we will get a rational fractions of polynomials in $k(x_1,\ldots,x_n)$ , $g(x_1,\ldots,x_n,\frac{1}{f})$ that vanishes on $V_f$ . Still not sure how to proceed. Clarification: $k$ is not neccessarily algebraically closed.","['zariski-topology', 'algebraic-geometry']"
4837818,Let $f$ be a differentiable function satisfying $\log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R$ and $f'(0)=1$. Find the value of $[f(3)]$.,"Let $f$ be a differentiable function satisfying $\log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R$ and $f'(0)=1$ . Find the value of $[f(3)]$ , where $[k]$ denotes greatest integer less than or equal to $k$ . Solution: $\log_2(f(3x))=x+\log_2(3f(x))$ $\frac{f(3x)}{3f(x)}=2^x$ $\frac{f(x)}{3f(\frac x3)}\cdot\frac{f(\frac x3)}{3f(\frac x{3^2})}\cdot\frac{f(\frac x{3^2})}{3f(\frac x{3^3})}\cdot\cdot\cdot\frac{f(\frac x{3^{n-1}})}{3f(\frac x{3^n})}=2^{\frac x3}\cdot2^{\frac x{3^2}}\cdot\cdot\cdot2^{\frac x{3^n}}$ $\frac{f(x)}{3^n\cdot f(\frac x{3^n})}=2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)}$ $\lim_{n\to\infty}\frac{f(x)\cdot\frac x{3^n}}{x\cdot f(\frac x{3^n})}=\lim_{n\to\infty}2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)}$ $\frac{f(x)}{x\cdot f'(0)}=2^{\left(\frac {\frac x3}{1-\frac13}\right)}$ $f(x)=x\cdot2^{\frac x2}$ $\therefore f(3)=3\cdot2^{\frac32}=6√2≈8.4$ $\implies[f(3)]=8$ My doubt: They seem to have written $\lim_{n\to\infty}\frac{f(\frac x{3^n})}{\frac x{3^n}}=f'(0)$ I don't understand this step. My Attempt: $f'(0)=\lim_{h\to0}\frac{f(0+h)-f(0)}{h}$ Replacing $h$ by $\frac x{3^n}$ $f'(0)=\lim_{n\to\infty}\frac{f(\frac x{3^n})-f(0)}{\frac x{3^n}}$ I don't think $f(0)$ is zero because that's not in the range.","['contest-math', 'limits', 'functions', 'derivatives']"
4837822,BBP-like formula for $\arctan5$,"Question I'm collecting some Bailey–Borwein–Plouffe formula as algorithm tests. But I did not find the formula related to $\arctan5$ . Does this formula exist? $$
\begin{aligned}
\arctan{1}&=\frac{1}{4}∑_{n=0}^{∞}\frac{1}{16^n}\left(\frac{4}{8n+1}-\frac{2}{8n+4}-\frac{1}{8n+5}-\frac{1}{8n+6}\right)\\
\arctan{2}&=\frac{1}{4}∑_{n=0}^{∞}\frac{1}{16^n}\left(\frac{4}{8n+2}+\frac{4}{8n+3}+\frac{4}{8n+4}+\frac{1}{8n+6}-\frac{1}{8n+7}\right)\\
\arctan{3}&=\frac{1}{4}∑_{n=0}^{∞}\frac{1}{16^n}\left(\frac{4}{8n+1}+\frac{4}{8n+2}-\frac{2}{8n+4}-\frac{1}{8n+5}-\frac{2}{8n+6}\right)\\
\arctan{4}&=\frac{1}{32}∑_{n=0}^{∞}\frac{1}{16^{2n}}\left(\frac{64}{16n+1}-\frac{64}{16n+4}-\frac{16}{16n+5}-\frac{16}{16n+6}+\frac{4}{16n+9}-\frac{1}{16n+13}-\frac{1}{16n+14}\right)\\
\arctan{5}&=\,?\\
\arctan{6}&=\frac{1}{32}∑_{n=0}^{∞}\left(\frac{27}{64}\right)^n\left(\frac{48}{6n+1}-\frac{27}{6n+5}\right)\\
\arctan{7}&=\frac{1}{4}∑_{n=0}^{∞}\frac{1}{16^n}\left(\frac{4}{8n+1}+\frac{4}{8n+3}+\frac{2}{8n+4}-\frac{1}{8n+5}+\frac{1}{8n+6}-\frac{1}{8n+7}\right)\\
\end{aligned}
$$ Related I searched $5k+i$ until $±2^{31}$ but did not find a valid coefficient. According to the formula given by wiki, there is $$
\begin{aligned}
\arctan{5}=\frac{π}{2}-\underset{n=0}{\overset{\infty}{\sum}}5^{-4n-3}\left(\frac{25}{4n+1}-\frac{1}{4n+3}\right)
\end{aligned}
$$ But $π$ does not have a formula for $5^{-4n}$",['number-theory']
4837843,Stochastic Calculus : Dangers of Incorrectly Calculating Derivatives,"I am trying to understand the importance of Ito's Lemma in Stochastic Calculus. When I learn about some mathematical technique for the first time, I always like to ask questions such as : "" Is this complicated approach truly necessary - and what happens if I were to incorrectly persist with a simpler approach? How much trouble can I land myself into by persisting within the incorrect and simpler approach? Are there some situations where this is more of a problem compared to other situations where this might be less of a problem? "" Part 1: For example, consider the following equation: $$f(t, B_t) = X_t = \mu t + \sigma \log(B_t)$$ Where: $X_t$ is the stochastic process $\mu$ is the drift term $\sigma$ is the volatility term $B_t$ is a geometric Brownian motion. When it comes to taking the derivative of this equation, there are 3 approaches that come to mind: Approach 1: Basic Differencing We can do this by simulating $X_t$ and evaluates consecutive differences: $$df(t, B_t) = f(t, B_t) - f(t_{-1}, B_{t-1}) $$ or $$dX_t = X_t - X_{t-1} $$ Approach 2: Basic Calculus (Incorrect): If this was a basic calculus derivative, for some generic function $f(x,y)$ , I could use chain rule to determine: $$\frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$$ $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$$ Thus, applying the logic of basic calculus incorrectly to stochastic calculus, I would incorrectly determine that: $$f(t, B_t) = \mu t + \sigma \log(B_t)$$ $$df(t, B_t) = \frac{\partial}{\partial t} f(t,B_t) dt + \frac{\partial}{\partial B_t} f(t, B_t) dB_t$$ $$df(t, B_t) = \mu dt + \sigma \left(\frac{1}{B_t}\right) dB_t$$ Approach 3: Ito's Calculus (Correct): Just to recap, in a Taylor Expansion of a non-stochastic function, we can show that a first order Taylor Expansion of a function equals to its first derivative (in limit), since all other higher order terms are negligible: $$f(x + \Delta x) = f(x) + (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ... $$ $$(\Delta x) f'(x) = f(x + \Delta x) - f(x) - 0.5(\Delta x)^2 f''(x) + ... $$ $$f'(x) = \frac{f(x + \Delta x) - f(x)}{\Delta x} - 0.5 \Delta x f''(x) + ...$$ $$\lim_{\Delta x \to 0} f'(x) = \lim_{\Delta x \to 0} \left( \frac{f(x + \Delta x) - f(x)}{\Delta x} \right) - \lim_{\Delta x \to 0} \left(0.5 \Delta x f''(x)\right) + \lim_{\Delta x \to 0} (....) $$ $$f'(x) = f'(x) - 0$$ $$f'(x) = f'(x) $$ However, this is not true in the Taylor Expansion of a stochastic function: $$df = \left(\frac{dB_t}{dt} f'(B_t)\right) dt$$ Since the Brownian Motion is not smooth and not differentiable in any interval (i.e. if you zoom into a very small part, there is still ""more Brownian Motion"" happening). Thus, $dB_t$ is defined, but $\frac{dB_t}{dt}$ is not defined. In the non-stochastic case, we could have written the Taylor Series this way with all higher order terms dropping off: $$\Delta f = f(x + \Delta x)  - f(x) =  (\Delta x) f'(x) + \frac{(\Delta x)^2}{2} f''(x) + ... $$ But, for some stochastic function of a Brownian Motion $f(B_t)$ , if we were to write the same Taylor Series: $$\Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta B_t)^2}{2} f''(B_t) + ...$$ We know that $\Delta B_t$ (i.e. two differences in a Brownian Motion) is equal to a Weiner Process, i.e. $$B_{t+s} - B_s = W_t \sim N(0,  t)$$ $$\Delta B_t = B_{t+\Delta t} - B_t = W_t \sim N(0, \Delta t)$$ $$\text{Var}(\Delta B_t^2) = E(\Delta B_t^2) - E(\Delta B_t)^2 = \Delta t - 0 = \Delta t $$ $$E(\Delta B_t^2) = \Delta t $$ Going back to the Taylor Series of the stochastic function, we can now make this replacement for $\Delta t$ using the Expected Value: $$\Delta f = f(B_t + \Delta B_t) - f(B_t) = (\Delta B_t) f'(B_t) + \frac{(\Delta t)^2}{2} f''(B_t) + ...$$ Now (for some reason that I don't understand), the second order term is not negligible anymore. Using this information, we can now formally write Ito's Lemma as : $$df(t, B_t) = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial B_t} dB_t + \frac{1}{2} \frac{\partial^2 f}{\partial B_t^2} (dB_t)^2$$ Now, going back to our original problem, for the function $f(t, B_t) = X_t = \mu t + \sigma \log(B_t)$ , we can write: $\frac{\partial f}{\partial t} = \mu$ $\frac{\partial f}{\partial B_t} = \frac{\sigma}{B_t}$ $\frac{\partial^2 f}{\partial B_t^2} = -\frac{\sigma}{B_t^2}$ Thus, the final answer is: $$df(t, B_t) = \mu dt + \frac{\sigma}{B_t} dB_t - \frac{1}{2} \frac{\sigma}{B_t ^2} dt$$ Note that $ Var(B_t) = E(B_t^2) - E(B_t)^2 = t - (0)^2 = t$ Part 2: Looking at Part 1 (provided I have done everything correctly), it is not immediately clear to me what are the ""dangers"" of incorrectly calculating the derivative (i.e. how much more ""wrong"" would the incorrect approach be compared to the correct approach). To get a better understanding of this, I tried to make a computer simulation to look at this (using the R programming language): library(ggplot2)

set.seed(123)
# parameters
n <- 1000
dt <- 0.01
mu <- 0.05
sigma <- 0.2
t <- seq(0, (n-1)*dt, dt)
Bt <- exp(cumsum(rnorm(n, 0, sqrt(dt))))
Xt <- mu * t + sigma * log(Bt)

# Original stochastic process: Xt = mu*t + sigma * log(Bt)
p1 <- ggplot(data.frame(Time = t, Xt = Xt), aes(Time, Xt)) +
    geom_line() +
    labs(title = paste(""Original Equation: Xt = mu*t + sigma * log(Bt)\nmu ="", mu, "", sigma ="", sigma)) +
    theme_bw()

# Approach 1: Basic Differencing
# dXt_1 = Xt[i+1] - Xt[i]
dXt_1 <- c(0, diff(Xt))
p2 <- ggplot(data.frame(Time = t, dXt_1 = dXt_1), aes(Time, dXt_1)) +
    geom_line() +
    labs(title = paste(""Approach 1: Basic Differencing\nXt[i+1] - Xt[i]\nmu ="", mu, "", sigma ="", sigma)) +
    theme_bw()

# Approach 2: Incorrect Derivative using basic calculus
# dXt_2 = mu*dt + sigma*(1/Bt)*dBt
dBt <- c(0, diff(Bt))
dXt_2 <- mu * dt + sigma * (1 / Bt) * dBt
p3 <- ggplot(data.frame(Time = t, dXt_2 = dXt_2), aes(Time, dXt_2)) +
    geom_line() +
    labs(title = paste(""Approach 2: Incorrect Derivative\nmu*dt + sigma*(1/Bt)*dBt\nmu ="", mu, "", sigma ="", sigma)) +
    theme_bw()

# Approach 3: Correct Derivative using Ito's Lemma
# dXt_3 = mu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(Bt^2))*dt
dXt_3 <- mu * dt + (sigma / Bt) * dBt - 0.5 * (sigma / (t)) * dt
p4 <- ggplot(data.frame(Time = t, dXt_3 = dXt_3), aes(Time, dXt_3)) +
    geom_line() +
    labs(title = paste(""Approach 3: Correct Derivative using Ito's Lemma\nmu*dt + (sigma/Bt)*dBt - 0.5*(sigma/(t))*dt\nmu ="", mu, "", sigma ="", sigma)) +
    theme_bw() In the above graphs, the results of all 3 approaches look quite similar to one another. I then compared the absolute differences between Approach 1 and Approach 3, and between Approach 2 and Approach 3: abs_diff_1_3 <- abs(dXt_1 - dXt_3)
abs_diff_2_3 <- abs(dXt_2 - dXt_3)

p5 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_1_3), aes(Time, AbsDiff)) +
    geom_line() +
    labs(title = ""Absolute Difference between Approach 1 and 3"") +
    theme_bw()

p6 <- ggplot(data.frame(Time = t, AbsDiff = abs_diff_2_3), aes(Time, AbsDiff)) +
    geom_line() +
    labs(title = ""Absolute Difference between Approach 2 and 3"") +
    theme_bw() Again, all results look quite similar to one another. My Question: Based on this exercise, it seems like whether you take the correct derivative (via Ito's Lemma) or the incorrect derivative (basic calculus), the final answer looks very similar. Thus, is Ito's Lemma more of a theoretical consideration with little added value compared to the incorrect derivative? Or perhaps there are much bigger differences for the derivatives of other functions (and perhaps for stochastic integrals)? Thus, for stochastic functions, is there any ""real danger"" in incorrectly calculating their derivatives and integrals using basic calculus methods? Thanks! Note: The one thing that comes to mind is perhaps Ito Calculus is really needed when you need to take the derivative or integral of a stochastic function in an intermediate step for some math problem (e.g. first passage time). In such cases, perhaps the ""danger"" of propagating an incorrect result is much higher compared to these basic simulations. References: https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/ef2c66c8079ba656210ad1fd4a5e2fa8_MIT18_S096F13_lecnote18.pdf Distribution of Brownian Motion squared https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_%28Siegrist%29/18%3A_Brownian_Motion/18.01%3A_Standard_Brownian_Motion","['motivation', 'calculus', 'brownian-motion', 'stochastic-calculus']"
4837846,Why is the binomial coefficient $\binom{\text{even}}{\text{odd}}$ always even?,"I recently came across the following: Fix an odd $k\in\mathbb{N}$ . Then for all even $n\in\mathbb{N}, \binom{n}k \text{ is even.}$ I'd like to complete my attempted proof by induction (see below), although other arguments are equally welcomed. Proof attempt Let $k\in\mathbb{N}$ with $k$ odd. $\underline{\text{Base Case:}}$ [n=2] Then $\binom{n}k=\binom{2}k$ . If $k=1$ , then $\binom{2}1=2$ is even. If $k>1$ , then $\binom{2}k=0$ , which is also even. $\underline{\text{Induct:}}$ Let $n\in\mathbb{N}$ be even. Assume that $\binom{n}k$ is also even. Say, $\binom{n}k=2l$ for some $l\in\mathbb{N}$ .  We'll show $\binom{n+2}k$ is even. Consider, $$
\begin{aligned}
\binom{n+2}k&=\frac{(n+2)!}{k![(n+2)-k]!}\\[10pt]
&=\frac{(n+2)(n+1)n}{k![(n-k)+2][(n-k)+1](n-k)!}\\[10pt]
&=\frac{(n+2)(n+1)}{[(n+2)-k][(n+1)-k]}\cdot\frac{n!}{k!(n-k)!}\\[10pt]
&=\frac{(n+2)(n+1)}{[(n+2)-k][(n+1)-k]}\cdot 2l
\end{aligned}
$$ I suppose if one can show that $\frac{(n+2)(n+1)}{[(n+2)-k][(n+1)-k]}$ is a natural number, they'd be done. However, I'm unsure how to do this, and I'm not entirely convinced it's always true. Suppose $n=6$ and $k=3$ , then: $$\frac{(n+2)(n+1)}{[(n+2)-k][(n+1)-k]}=\frac{8\cdot 7}{5\cdot 4}=\frac{14}{5}$$ which is certainly not a natural number. This leads me to believe there's an error in my reasoning, but I cannot see where I went wrong.","['elementary-number-theory', 'binomial-coefficients', 'combinatorics']"
4837849,How far can I downsample two sets A and B to compute the Jaccard Index of A and B?,"Let be $A$ and $B$ two sets such that $|A|=N_A$ and $|B|=N_B$ .
Let be $(A_n)_{1\leq n \leq N_A}$ a sequence of sets such that $A_n \subset A$ , $|A_n|=n$ . Analogously, consider the sequence $(B_n)_{1\leq n\leq N_B}$ . Denoting $J$ the Jaccard Index, $J(E,F) = 2 \frac{|E \cap F|}{|E| + |F|}$ ,
how to choose $r$ and $s$ such that $\mathbb{P}(|J(A_r, B_s) - J(A, B)| \leq \varepsilon) > 1 - \eta$ ? Note that, $N_A \gg 1$ and $N_B \gg 1$ , I don't know if it may help. But that's the case I am considering. Moreover, $J(A,B)$ is known. I could compute $J(A_r, B_s)$ as well, but the value of this quantity depends on how $A_r$ and $B_s$ are defined. I would assume that $A_r$ and $B_s$ are built by randomly downsampling $A$ and $B$ (without replacement), and I hope to be able to estimate the probability numerically with some Monte-Carlo simulations, because I don't think we can come up with an analytic estimation. Does it make sense ? Can you help me to better formulate this probabilistic problem ? Feel free to suggest anything to clarify this question.","['convergence-divergence', 'probability-theory', 'discrete-mathematics']"
4837865,"In a normalization, does the choice of isomorphism with function field break uniqueness?","This is from Vakil's Foundations of Algebraic Geometry, 10.7.I Let $X$ be an integral scheme with function field $K$ . Let $L$ be an algebraic extension of $K$ . Then, the normalization of $X$ in $L$ is a scheme $v : \widetilde{X} \rightarrow X$ with $\widetilde{X}$ normal, integral and having function field $L$ , $v$ is a dominant morphism that induces the inclusion $i : K \rightarrow L$ and is universal wrt this property. That is: for any scheme $f :Y \rightarrow X$ also satisfying the previous properties, there is a unique morphism $f' : Y \rightarrow \widetilde{X}$ such that $v \circ f' = f$ . If $X = Spec(R)$ is affine, then I expect $\widetilde{X}$ to be the integral closure of $R$ inside $L$ , but this seems to not be the case as the then the factorization $v \circ f' = f$ is not unique, as it seems to depend on the choice of isomorphism $K(Y) \rightarrow L$ . For example, if $R = \mathbb{Z}, K = \mathbb{Q}, L = \mathbb{Q}[\sqrt{2}]$ , then $\widetilde{R} = \mathbb{Z}[\sqrt{2}]$ . If $Y$ is also $Spec(\mathbb{Z}[\sqrt{2}])$ , then we get different morphisms $Y \rightarrow Spec(\widetilde{R})$ depending on if we take the isomorphism $K(Y) \simeq  \mathbb{Q}[\sqrt{2}] \rightarrow L$ to be identity or conjugation. They correspond to identity or conjugation maps $\widetilde{R} = \mathbb{Z}[\sqrt{2}] \rightarrow O_Y(Y) = \mathbb{Z}[\sqrt{2}]$ . Thus, the $f'$ is no longer unique. How do we fix this? I think the only way to fix this problem is to choose a specific isomorphism $j : K(Y) \simeq L$ , and require that the $f'$ induce $j$ , so different $j$ 's will result in different $f'$ 's.","['algebraic-geometry', 'schemes', 'commutative-algebra']"
4837867,Does functions for change in functions exists,"Ok, so we define derivatives as functions that gives us almost instantaneous rate of change.
Using limits we calculate the derivatives as single functions.
Like in the case of finding average velocity between t1 and t2,
Avg v = [s(t2)-s(t1)]÷[t2-t1] but here we have two independent variables t1 and t2, we get rid of it during finding derivatives by using h (h approaches 0).
So we can also write the average change as a function by defining a constant 'a' such that average velocity AROUND TIME 't' can be calculated as [s(t+a)-s(t-a)]÷2a. I was wondering if this already is a thing in mathematics or it is just a Bunch of nonsense.","['calculus', 'functions', 'derivatives']"
4837871,Asymptotic solution of ODE,"I have the following system of ODEs $$
-4 (u a(u)+1) f'(u)=2 f(u) \left(u a'(u)+a(u)\right)+\left(u \left(3 a(u)^2+\frac{1}{4}\right) a'(u)+\left(a(u)^3+\frac{a(u)}{4}\right)\right),$$ $$a'(u)=\frac{1}{2} u f'(u),$$ $$a(0)=-1/2,
f(0)=0.$$ I need to find the behaviour of solutions at $u\rightarrow\infty$ . Numerical experiments indicate that $f(u)\approx 3/8$ and $a(u)\approx -1/u$ . But how can it be rigorously shown? I would be even happy to be able to show that $\lim_{u\rightarrow\infty}a(u)=0.$ How to achieve that? These equations have some relevance for Hubbard model of strongly correlated system (2-sites case). $-1/2\le a(u)\le 1/2$ implies that some approximate method fulfils the Pauli exclusion principle.","['asymptotics', 'ordinary-differential-equations']"
4837901,Prove directly from the definition of the Wiener process: $\mathbb P \left( \{W_{n^2}\}_{n=1}^{\infty} \text{ is a bounded sequence } \right) = 0$,"Let $W_t$ - standard Wiener process. Prove directly from the definition of the Wiener process: $$\mathbb P \left( \{W_{n^2}\}_{n=1}^{\infty} \text{ is a bounded sequence } \right) = 0$$ Tip: If $(W_{n^2})_n$ is  bounded then also $(W_{n^4})_n$ is bounded. When the sequence $\{W_{n^2}\}_{n=1}^{\infty}$ was limited then exist $M>0$ such that $|W_{n^2}|\le M$ for all $n\ge 1$ . From the definition of Wiener process: $$\forall_{0 \le s <t} W_t-W_s \sim \mathcal N(0,t-s)$$ so also $$W_{(n+1)^2}-W_{n^2} \sim \mathcal N(0,2n-1)$$ However, these observations don't seem to help me and I don't know what to use to prove my thesis.","['stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4837945,Is $X^4-3X^2+2X+1$ irreducible over the rationals?,"I want to check whether the following polynomials is irreducible over the rationals $f=X^4-3X^2+2X+1$ . I think I found that it is irreducible but my solution is really complicated, quite long and I am not sure whether this is correct. I would really appreciate it if someone could point out any mistakes or tell me whether my reasoning is correct. Sadly, I cannot use the classical theorems like Eisenstein (there is no common divisor of the coefficients $-3,2$ and $1$ and neither I have found a good reduction modulo some prime $p$ . I have tried $p=2$ , which gave me $X^4+X^2+1$ which is reducible and $p=3$ which gave me $X^4+2X+1$ for which $-1$ is a root thus also reducible.
Out of options, I resorted to doing it by hand.
Since $f$ is normed it is primitive. Thus, $f$ irreducible over $\mathbb{Q}[X]$ if and only if it is irreducible over $\mathbb{Z}[X]$ which follows by Gauss.
Then, I checked whether the polynomial had any roots in $\mathbb Z$ . It does not because since every root  of a polynomial must a divisor of the constant term, here $1$ only $\pm 1$ are possible options for roots. Since $f(\pm 1)\neq 0$ I deduced that f does not have rational roots.
Thus, if $f=gh$ for some polynomials $g,h\in \mathbb{Z}[X]$ , then both $g$ and $h$ need to be quadratic. So I made the following ansatz \begin{align}
f&=gh=(\alpha_1X^2+\beta_1X+\gamma_1)(\alpha_2X^2+\beta_2X+\gamma_2)\\
&=\alpha_1\alpha_2X^4+(\alpha_1\beta_2+\alpha_2\beta_1)X^3+(\beta_1\beta_2+\gamma_1\alpha_2+\gamma_2\alpha_1)X^2+(\beta_1\gamma_2+\beta_2\gamma_1)X+\gamma_2\gamma_1
\end{align} with $\alpha_i,\beta_i,\gamma_i\in\mathbb{Z}$ for $i=1,2$ . Then, I can start doing some simplification by comparing this to the original f.
Obviously, $\alpha_1=\alpha_2=\pm1$ , from which we can deduce by looking at the term proportional to $X^3$ that $\alpha_1\beta_2+\beta_1\alpha_2=\pm(\beta_1+\beta_2)=0$ , it follows $\beta=\beta_1=-\beta_2)$ . However, at the same time $\gamma_1=\gamma_2=\pm 1$ . Then, by looking at the term proportional to $X$ , we see that $\beta_2\gamma_1+\beta_1\gamma_2=\pm(\beta_1+\beta_2)=2$ which is a contradiction. Thus, f must the irreducible.
I would really really appreciate any help on this, I have been stuck on this for hours and I have no idea whether what I did was correct. Thank you so much in advance!","['irreducible-polynomials', 'abstract-algebra', 'polynomials']"
4837960,"Multiindex partial derivative higher order product rule, i.e. formula for $\partial^\alpha(fg)$","I want to prove the product rule for higher order partial derivatives. It is given on Wikipedia under the name ""General Leibniz rule"": $$\partial^\alpha(fg)=\sum_{\beta\leq \alpha}\binom\alpha\beta(\partial^\beta f)(\partial^{\alpha-\beta}g),$$ where $\alpha$ and $\beta$ are multiindices. My first attempt is to make a recursion by computing (here $i$ is just an index) $$\partial^{\alpha}\big( \partial^i(fg) \big)=\sum_{\beta\leq\alpha}\binom{\alpha}{\beta}\Big(  (\partial^{(\alpha,i)-\beta}f)(\partial^{\beta}g)+(\partial^{\beta}f)(\partial^{(\alpha,i)-\beta}g) \Big).$$ I'm not sure how to continue at this point. The index $i$ can be one of the components of the multiindex $\alpha$ . I dont'see how to manage the combinatorics... related on MathOverflow: Multivariable Higher Order Chain Rule","['partial-derivative', 'multivariable-calculus', 'multinomial-coefficients']"
4838012,Finding the smallest set of values that make an exponent of a known value. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 months ago . Improve this question I need some help with figuring out an equation or process? Not sure what the correct term is? Full disclosure: Unfortunately I’m not even remotely a mathematician so I’m grateful for your patience. If I misuse any vocabulary or am unclear please let me know and I’ll try tor clarify what I’m trying to convey. The Question. The question is simple. I want to figure out the exponent of a value. But there is caveat: Only the total is known. And out of all the possible exponents that could be used: I want to find the smallest possible base with the smallest possible power. That is to say: If I was to take the digit of the base number $x$ and the digit of the exponent power $y$ , that both these numbers would be as small as possible without making the other larger. Essentially the resulting pair of numbers would essentially be the smallest difference of values and/ or the smallest sum total. Moreover I need to “ prove ” that it is the smallest combination of numbers possible. Unfortunately it can’t be “close enough.” It needs to be certain that this pair of numbers are the smallest pair possible values that can form the exponent total. Here is an example that I hope illustrates what I am trying to achieve: (But remember, I’m not just trying to find the answer. Rather I want to know if there is a mathematical process to finding the smallest possible pair in the first place?) Let’s say that the equation is $x^{y} = 64$ . The most simple exponent is $64^{1}$ . But it is also the largest number I could use as a base and so would not be considered a “successful” answer, (even if the exponent is only $1$ ). $2^{6}$ would be considered more “successful” exponent. As the numbers are $2$ and $6$ .
It doesn’t matter if the base is smaller than the exponent. The only criteria is that they are made of the smallest numbers as viewed as a decimal value. $4^{3}$ I believe to be the most successful. The base may have significantly increased but we have reduced the exponent by half its value. $8^{2}$ would be less of a successas it has still gained a higher number. Whilst it is the smallest exponent possible aside from $1$ , the base has risen to $8$ . Now, the first question would be how do I find $4^{3}$ out of all the exponents I could possibly use? Then secondly how can I be certain that there is no smaller combination of numbers that could be used to create an exponent that results in $64$ ?. Or put another way: Is there a method of mathematically finding the smallest pair of numbers without simply wading through every possible exponent that makes the original sum and comparing the total sum and/or difference of every pair? Thank you in advance! I know I’m probably over complicating this but unfortunately I don’t know what I don’t know. edit: Here is the link @SohamSaha post using positive integers in case anyone should need it: Minimising $x+y$ in $x^y=a$","['lambert-w', 'optimization', 'calculus']"
4838019,Gradient zero implies lipschitz with arbitrary constant,"I can't work out if this is true or not: Let $f:\mathbb{R}^N\rightarrow \mathbb{R}$ be differentiable at zero with $\nabla f(0)=0$ and $f(0)=0$ . Is it true that for any $\delta > 0$ , there is a neighbourhood of $0$ over which $f$ is Lipschitz with constant $\delta$ ? i.e. does this hold: $$\forall \delta > 0, \exists r > 0 \text{ such that } \forall x, y \in B_r(0), |f(x)-f(y)|<\delta |x-y|$$ where $B_r(0)$ is the $r-$ ball around $0$ . I think it is true but I am struggling to prove it. I thought maybe I could use a proof by contradiction by assuming the opposite of the above and finding $x_n,y_n\rightarrow 0$ such that $|f(x_n)-f(y_n)|>\delta|x_n-y_n|$ to then somehow reach that $|\nabla f|>\delta$ but I can't get this to work. If it isn't true, I can't manage to make a counterexample. Note, I do not want to assume differentiability of $f$ for $x\neq 0$ .","['limits', 'derivatives', 'lipschitz-functions', 'epsilon-delta']"
4838053,Evaluate 2D trigonometric integration of $\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{\cos(x)^3}{\cos(x)-\cos(y)}dydx$,"I have the following example of an integral: $$ I = \frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{\cos^3 x}{\cos x - \cos y} \ dy \ dx $$ I can determine the exact value of this integral by leveraging the symmetry properties of integrals: $$ I = \frac{1}{2}\left(\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{\cos^3 x}{\cos x - \cos y} dy dx + \frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{\cos^3 y}{\cos y - \cos x} dx dy\right) $$ $$ = \frac{1}{2}\left(\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{\cos^3 x - \cos^3 y}{\cos x - \cos y} dy dx\right) $$ $$ = \frac{1}{2}\left(\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}(\cos^2 x + \cos^2 y + \cos x\cos y) dy dx\right) = \frac{1}{2} $$ In my case, I need to evaluate the first integral with respect to $y$ : $$ I(x) = \frac{1}{2\pi}\int_0^{2\pi}\frac{1}{\cos x - \cos y} dy $$ and then integrate over $x$ : $$ I = \frac{1}{2\pi}\int_0^{2\pi}I(x)\cos^3 x dx $$ However, the results are not valid. What mistake did I make?","['integration', 'improper-integrals', 'singular-integrals', 'trigonometric-integrals', 'trigonometry']"
4838089,Integrate of diferential of modulus of a complex function.,"Let $u(x,t):\mathbb{R}^2\rightarrow\mathbb{C}$ be a diferentiable function square integrable. So I want to calculate $$\int_{-\infty}^\infty \frac{d}{dt}|u|^2dx.$$ I know that the diferential of the modulus is not well defined in that case, but if we define $|u|^2=u\bar{u}$ we have: $$\int_{-\infty}^\infty \frac{d}{dt}(u\bar{u})=\int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx,$$ my questions is: why is this well defined (many books use that)? If $u$ in infinity goes to zero I can use integration by parts to conclude that: $$\int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx=\int_{-\infty}^{\infty}u_t\bar{u}-u_t\bar{u}dx=0?$$","['integration', 'derivatives']"
4838115,Showing that if $\sin x +\sin y\ge\sqrt{3}$ then $\cos x +\cos y\le 1$,"Let $x$ and $y$ be the measures of two acute angles, such that: $$\sin x +\sin y\ge\sqrt{3},$$ the problem is to show that: $$\cos x +\cos y\le 1$$ My approach: I've tried squaring both sides of the given inequality, and I got $\sin^2 x + \sin^2 y + 2\sin x\sin y\ge 3$ , then I substituted in $1-\cos^2 x$ and $1-\cos^2 y$ to obtain: $$1-\cos^2 x +1-\cos^2 y+2\sin x\sin y\ge 3$$ But then,  I  got stuck and couldn't keep any further. Any hint or help is much appreciated.","['inequality', 'trigonometry']"
4838154,Norm upper bound on difference of inverted matrices,"I feel like I have seen this bound before but no longer can recall its source: Suppose $A, B$ are square matrices with $||A^{-1} (A-B) || < 1$ , i.e. their perturbation is relatively minimal. Then, what is an upper bound on $|| A^{-1} - B^{-1} ||$ ? In my notes, I have: $$
|| A^{-1} - B^{-1} || \leq \frac{||A^{-1}||^2 ||A - B||}{1 - ||A^{-1} (A - B)||}
$$ Is this true? And is there a source?",['matrices']
4838215,"How to evaluate $\int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da $ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question $$
\Omega = \int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da
$$ $$
\Omega_1 = \int_0^{\frac\pi4} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \quad \text{(let } t = \tan x \text{)}
$$ $$
= \int_0^1 \frac{a^2 + (a^2 + b^2) t^2}{b^2 + a^2 + b^2 t^2} \cdot \frac{t}{1 + t^2} \, dt
$$ $$
= \frac{1}{2} \int_0^1 \frac{a^2 + (a^2 + b^2) t}{(a^2 + b^2 + b^2 t)(1 + t)} \, dt
$$ $$
= \frac{a^4 + b^4 + a^2 b^2}{2a^2 b^2} \int_0^1 \frac{b^2}{a^2 + b^2 + b^2 t} \, dt - \frac{b^2}{2a^2} \int_0^1 \frac{1}{1 + t} \, dt
$$ $$
= \frac{1}{2} \left( \left(\frac{b}{a}\right)^2 + \left(\frac{a}{b}\right)^2 + 1 \right) \log \left(1 + \frac{\left(\frac{b}{a}\right)^2}{1 + \left(\frac{b}{a}\right)^2} \right) - \frac{\log(2)}{2} \left(\frac{b}{a}\right)^2
$$","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4838217,Understanding the Solid torus,"I have asked this same question and it was closed in less than two minutes, I really don't understand why is it very stupid? (sorry for being very stupid). I've come across the following exercise: Prove that the solid torus denoted $\tau$ which is obtained by rotating the disc $\lbrace(x-1)^2 +z^2 \leq \frac{1}{4}, y= 0 \rbrace$ around the axis $\lbrace  x=y=0\rbrace$ . Prove that $\tau$ is diffeomorphic to $D^2 \times S^1.$ I couldn't start thinking of this exercise since I didn't understand the description of the solid torus as it is mentioned in the exercise, I hope that some could give more explicit explanation of what it is please. I am self studying, and any comment would so much appreciated! $\textbf{Edit}$ : what I mean by explicit explanation is as follows: for example if I take a point $z$ in the disc $D^2$ or in the circle $S^1$ I know that it satisfies $\vert z\vert \leq 1 $ or $\vert z \vert = 1$ respectively. However if I take a point in the solid torus I don't know which equation it satisfies ?",['general-topology']
4838235,Dodecahedral number visualization,"The dodecahedral numbers, 0, 1, 20, 84, 220, 455, 816, 1330, 2024, 2925, 4060, ... numbers of the form ${3 n \choose 3}$ ( A006566 ). Does anyone have a good visualization of these?  In particular, I'd like to see 2024 as a dodecahedral number. Here's the corresponding tetrahedral number 2024:","['polyhedra', '3d', 'visualization', 'geometry', 'oeis']"
4838238,"Show that $\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2}$","Show that $$\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2}$$ My try : $$\Omega = \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \int_{0}^{\infty} \frac{\log(1 + x^4)}{(1 + x^2)^2} \, dx$$ $$\int_{0}^{\infty} \frac{x^2 \left\{ \log(1 + x^4) - 4 \log(x) \right\}}{(1 + x^2)^2} \, dx$$ $$2\Omega = \int_{0}^{\infty} \frac{\log(1 + x^4)}{1 + x^2} \, dx - 4 \int_{0}^{\infty} \frac{x^2 \log(x)}{(1 + x^2)^2} \, dx$$ $$\Omega = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \, dx + 2 \int_{0}^{\infty} \frac{\log(x)}{(1 + x^2)^2} \, dx$$","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4838255,"What are the extremes of the function $ f\left(x,y\right)=xy\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}} $?","$ f\left(x,y\right)=xy\sqrt{1-\frac{x^2}{a^2}-\frac{y^2}{b^2}} $ ; where a,b are constants. I have tried to differentiate the equation, but I get stuck in a very hard system after this. The answer of the problem is: $ f_{max}=\frac{ab}{3\sqrt{3}};f_{min}=-\frac{ab}{3\sqrt{3}} $","['multivariable-calculus', 'analysis', 'extreme-value-analysis']"
4838260,Lagrange-mean like inequality,"I had this problem on an assignment a while ago, but I don't quite understand the formulation of the problem nor the purpose: Let $f : [a,b]→\mathbb{R}$ be a function that admits a derivative (not necessarily
finite!) at any point of $[a,b]$ . Prove that there exists $x_0 \in [a,b]$ such that: $|\frac{f(b)-f(a)}{b-a}|\leq |f'(x_0)|$ The original solution was a bit convoluted, and was by constructing some nested intervals, but can't we just say that if the derivative is infinite at some point then we're done, otherwise $f$ is differentiable and we're done by Lagrange's mean theorem? What's wrong with this?",['analysis']
4838265,How do you solve equations of $ f'(t)=f(t)^a$?,"I was doing some math for fun and I came across equations of this form: $f'(t)=f(t)^a $ . I understand how to solve it for case $a=1$ -it's just $f(t)=Ae^t+C$ . And I've hear that for $a=2$ the solution is $f(t)=-\frac{1}{t-c}$ . But I haven't been able to find a general method for solving all cases. I'm mostly self taught and I've only take up to Calc 2 at my university. If anyone knows how to do it, I'd greatly appreciate it.","['nonlinear-system', 'ordinary-differential-equations']"
4838293,How do you construct an everywhere inward/outward-pointing vector field?,"I've been recently doing some problems in John Lee's book Introduction to Smooth Manifolds (Second Edition) , and I wanted to ask how one would approach the following problem. Let $M$ be a smooth manifold with boundary. Show that there exists a global smooth vector field on $M$ whose restriction to $\partial M$ is everywhere inward-pointing, and one whose restriction to $\partial M$ is everywhere outward-pointing. After doing some digging in the book, I found the definition (see Page 118) that Lee is referring to when he says ""inward/outward-pointing"", but it's not necessarily for vector fields. Definition : If $p\in M$ , a vector $v\in T_pM\backslash T_p\partial M$ is said to be inward pointing if for some $\epsilon>0$ there exists a smooth curve $\gamma:[0,\epsilon)\to M$ such that $\gamma(0)=p$ and $\gamma'(0)=v$ , and it is outward pointing if there exists such a curve whose domain is $(-\epsilon,0]$ . My first question is how to adapt this to a vector field, and make sure that it is inward/outward-pointing everywhere. Here is my proposed definition (as far as I'm aware, it's not in the book). Definition (Own) : Let $M$ be a smooth manifold with boundary, and let $X$ be a smooth vector field on $M$ . Furthermore, let $A\subseteq \partial M$ be a closed subset. Then $X$ is said to be inward-pointing everywhere on $A$ if each $X_p\in T_pM$ is inward-pointing for all $p\in A$ . We say that $X$ is outward-pointing everywhere on $A$ if $X_p\in T_pM$ is outward-pointing for all $p\in A$ . I assume that an outward/inward-pointing vector field is defined similarly. So now that all of the above groundwork has been laid out, here is my ultimate question. Question(s) : Is the above definition the correct interpretation of the problem? If so, I was thinking of just applying Lemma 8.6, or trying to use Proposition 8.7 to prove the claim, but I was unsure if this was the correct approach. For those who don't have the book, these say the following. (Lemma 8.6). Let $M$ be a smooth manifold with or without boundary, and let $A\subseteq M$ be a closed subset. Suppose that $X$ is a smooth vector field along $A$ (meaning $X_p\in T_pM$ for each $p\in A$ ). Given any open subset $U$ containing $A$ , there exists a smooth vector global vector field $\widetilde{X}$ on $M$ such that $\widetilde{X}|_A=X$ and $\text{supp}~\widetilde{X}\subseteq U$ . (Proposition 8.7). Let $M$ be a smooth manifold with or without boundary. Given $p\in M$ and $v\in T_pM$ , there is a smooth global vector field $X$ on $M$ such that $X_p=v$ . Any ideas for how to solve this problem would be greatly appreciated!","['vector-fields', 'geometry', 'vector-analysis', 'differential-geometry']"
4838307,What else can we conclude about $f$ given this information?,"Consider a function $f: \mathbb{R} \mapsto \mathbb{R}  $ , with the following property: Consider the sequence $x, f(x), f(f(x)), f(f(f(x))), ...$ . This sequences converges for all values of $x$ . For completeness, this sequence is $a_0=x, a_{n+1}=f(a_n)$ . For all $x \in \mathbb{R}$ , there exists a $y \in \mathbb{R}$ such that $y = \displaystyle \lim_{m \to \infty} a_m$ . What else can we conclude about $f$ given this information? For example, $f$ might be a constant function. It might even be, $f(x)=x/2$ or $x/100$ .",['functions']
4838339,"Proving $n!k! \le (n+k-1)!$ for positive integers $n \ge 1$, $k \ge 1$","The Question Does the following inequality hold: $$n!k! \le (n+k-1)! \quad \text{for positive integers} \quad n \ge 1,\\ k \ge 1  \text{.}$$ I suspect it does but I'm at a loss on how to prove (or disprove ) this. What I've tried I've been able to show that the simpler inequality: $ n!k! \le (n+k)!\\ $ holds by dividing both sides by $k!$ so that $n$ -terms remain on both sides: $ \prod_{p=1}^{n} p \le \prod_{p=1}^{n} (p+k) $ which is clearly true for $k \ge 1$ . Applying the same process for the original inequality yields $ \prod_{p=1}^{n} p \le \prod_{p=1}^{n-1} (p+k) $ which feels inconclusive due to the right-hand-side having one fewer terms than the left-hand-side. I don't have much of a background in combinatorics so any help would be appreciated.","['inequality', 'combinatorics']"
4838362,Other functions on eigenvalues besides trace and determinants,"Given a linear transformation $\mathscr{A}: V \to V$ , where $V$ is a finite-dimensional vector space with the underlying field being $\mathbb{R}$ or $\mathbb{C}$ . Suppose $\mathscr{A}$ has eigenvalues $\lambda_{1},\dots,\lambda_{n}$ . We can define $\operatorname{tr}(\mathscr{A}) = \sum_{i} \lambda_{i}$ , and $\det(\mathscr{A}) = \prod_{i}\lambda_{i}$ . These are two commonly studied functions, and one can ask many interesting questions about them (which goes beyond linear algebra for instance in convex analysis ). However, it seems odd to me that I've never heard people using other different functions on eigenvalues. In particular we can define $\operatorname{oper_{1}}(\mathscr{A}) = \sum_{1\leq i,j\leq n}\lambda_{i}\lambda_{j}$ , or similarly $\operatorname{oper_{2}}(\mathscr{A}) = \sum_{1\leq i,j,k\leq n}\lambda_{i}\lambda_{j}\lambda_{k}$ , etc. In this case we do not have $\lambda_{1},\dots,\lambda_{n}$ ordered, but if we do. Say in decreasing order, we can define something like $\operatorname{oper} (\mathscr{A}) = f(\lambda_{1},\dots,\lambda_{n})$ , where $f$ is some continuous function from $\mathbb{R}^{n}$ ( $\mathbb{C}^{n}$ ) to $\mathbb{R}$ ( $\mathbb{C}$ ). So are there any results on such functions. If no, why not? Even beyond linear algebra, it might be useful in analysis.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'functional-analysis', 'convex-analysis']"
4838399,Looking for help solving a second-order linear differential equation and guidance on a specific method,"I am currently working on the differential equation: $$
x \frac{d^2y}{dx^2} + \frac{1}{2} \frac{dy}{dx}+ \frac{1}{4} y = 0
$$ During a brief discussion with an expert, I was advised to try expressing the solution in the form $y=f\cdot g$ , where $f=x^n$ . The suggestion was to rewrite the equation in terms of $g$ . However, I am struggling to find relevant literature or references on this method. I would greatly appreciate any insights, guidance, or references that can help me understand and apply this approach. If anyone is familiar with this method or has encountered similar problems, kindly share your knowledge and recommend any books or resources that may assist in solving such equations. Thank you in advance for your assistance!",['ordinary-differential-equations']
4838403,How many words with 4 different consonants and 3 different syllables can be created if we have 9 consonants and 6 syllables available such that...,"We have $9$ different consonants and $6$ different vowels available. How many words (not necessarily actual words) can be created using $4$ different consonants and $3$ different vowels such that no two vowels stands next to each other? Here's what I've got so far: The number of words with 4 different consonants and 3 different vowels is $\binom{9}{4}.\binom{6}{3}.7!$ To solve for the case where we don't want any two vowels next to each other, we want to put every vowel between at least two consonants. First we choose our $4$ consonants and shuffle them. That can be done in $\binom{9}{4}.4!$ ways. Let's labels our consonants $C$ . Then there are $5$ gaps (labeled $X$ ) where we can potentially place our vowel. (X C X C X C X C X) Choosing $3$ gaps out of $5$ and choosing $3$ consonants out of $6$ + shuffling them can be done in $\binom{5}{3}.\binom{6}{3}.3!$ ways. Hence the answer to the question is: $\binom{9}{4}.\binom{5}{3}.\binom{6}{3}.4!3!$ Could that be a correct approach? I've considered using principle of exclusion and inclusion but I couldn't figure iut what would the sets themselves represent.","['permutations', 'combinatorics', 'discrete-mathematics']"
4838407,Why we Use Partitions of Unity. Specific Questions,"While trying to better understand why we use partitions of unity I stumbled across this post . There are a few points I do not understand: This is exactly how to compute integrals in practice. But if we use it as our definition in theory, it becomes messy Why exactly does it become messy? The definition pointed at in the post seems to be the following: Let $\{(U_i,\phi_i)\}$ be an atlas for $S$ up to a null set consisting of disjoint charts. We define the integral of $\omega$ over $M$ as $$\int_S\omega := \sum_i\int_{U_i}\omega|_{U_i} := \sum_i\int_{V_i} (\phi_i^{-1})^*\omega|_{U_i}$$ where, for the integrands in the sum to be well-defined, we need the guarantee that $(\phi_i^{-1})^*\omega|_{U_i}$ is compactly supported in some rectangle $V_i\supseteq\phi_i(U_i)$ . $(\phi_i^{-1})^*\omega|_{U_i} = f\ dx^1\wedge dx^2$ for some bounded function $f$ . Is there anything else we would need to make this definition proper? If I understand the author of the post correctly, each of the atlases he suggests for the unit sphere has their own issues. What exactly are these? As far as I can tell these are: Spherical coordinates: as $\phi(S) = \mathbb{R}^2$ , our pullback form fails to be compactly supported. However, could we not 'patch' our previous definition so as to include a limit process as follows: $$\int_{U_i}\omega|_{U_i} := \lim_{r\to\infty}\int_{B(r)}(\phi^{-1})^*\omega|_{\phi^{-1}(B(r))}\ ?$$ where $B(r)$ is, say, the box $[-r,r]^2$ . I've seen similar ideas used to attribute a value to integrals such as $$\int_0^1\frac{1}{\sqrt{x}}dx := \int_{(0,1)}\frac{1}{\sqrt{x}}dx$$ as opposed to, although possible, to define them in terms of partitions of unity. By northern and southern hemispheres: the atlas is given by $$\Big\{\Big(S\cap\{x^3<0\},\phi_1\Big),\Big(S\cap\{x^3>0\},\phi_2\Big)\Big\}
\ \ \text{where} \ \ \phi_i:(x,y,z)\mapsto (x,y) \ \ \text{ and } \ \ \phi_i^{-1}:(u,v)\mapsto \left(u,v,\pm\sqrt{1-u^2-v^2}\right).$$ Here the issue is that $f$ would become unbounded ( why? ): convert a bounded integrand to an unbounded one -- look at the example with the square root. Spherical coordinates: here I simply fail to see what the issue would be. What is it? Perhaps that 'our integrands will have discontinuities at the boundaries of the patches'? If so, how would this cause trouble?","['integration', 'intuition', 'definition', 'smooth-manifolds']"
4838462,Extending a $G$-torsor/vector bundle over a codimension 2 locus,"I want to prove the following statement: Let $X$ be a scheme over a field $k$ locally of finite presentation, and let $S$ be a closed subset of $X$ of codimension $\geq 2$ . If $\mathcal{E}$ is a locally free sheaf of rank $n$ on $X-S$ , then one can extend $\mathcal{E}$ to $X$ uniquely. I think the following argument should be able to show the statement: $\mathcal{E}$ is described by a cocycle $(g_{ij})$ where $g=g_{ij}$ is an element of $\mathrm{GL_n}(\Gamma(U,\mathcal{O}_{X-S}))$ . Here $U$ is an open of $X-S$ that is a restriction of an affine open $\mathrm{Spec}(B)$ of $X$ , i.e., $\mathrm{Spec}(B)|_{X-S}=U$ . At this point, if I assume $X$ is also normal , then I can use Algebraic Hartog to deduce $\mathrm{Spec}(B)=\Gamma(U,\mathcal{O}_{X-S})$ . Therefore, the same cocycle $(g_{ij})$ defines the unique extension to $X$ . However, in this argument, I still need to assume $X$ is normal. How can I remove this condition? I also want to ask a more general question. For a group scheme $G$ over $X$ (with some further mild assumptions on $X$ if necessary), when can we still extend a $G$ -torsor on $X-S$ to $X$ uniquely?","['algebraic-geometry', 'schemes', 'sheaf-theory']"
4838463,How can I find the periods of difference of the sine waves that has irrational coefficients,"I'm working on a project right now. And now I need to find periods of difference of the sine waves and i'm stuck. In few resources I found that I can find the periods of summed or differenced sine waves by using LCM(Least Common Multiple) and HCF(Highest Common Factor) of sine waves' periods.
But the problem is: I'm dealing with sine waves that has irrational coefficients. And when we try to find the period of any sine wave we deal with the coefficient of ""x"". So when I jump into LCM and HCF it doesn't give any result I want. After that I thought that I can make some approximations to the irrationals I deal with. Then I can use LCM and HCF. But I don't have a clue to how to do that either. And I don't have a idea for how to apply LCM and HCF of rational approximations to irrational numbers either. Is it possible to find the periods for such functions like this? If it's how can we? I'm waiting for the answers thank you Example for the sine functions I mentioned: $$\sin(x)-\sin(\sqrt[7]{2}x).$$ Edit: Couple graph images from the function type I want $$\sin(x)-\sin(2^{12/12}x).$$ $$\sin(x)-\sin(2^{7/12}x).$$ $$\sin(x)-\sin(2^{1/12}x).$$ As you can see these type of functions actually look like periodic functions. You can check out by yourself in any graph calculator. Actually if you have knowledge about musical intervals,you can see that im dealing with sound waves of intervals between musical notes. And notes that have ""perfect"" harmony such as octave $$(2^{12/12}).$$ or perfect fifth $$(2^{7/12}).$$ has much more smaller periods than the other intervals such as tritone etc. *Of course when I say periodic its just a hypothesis.","['approximation', 'periodic-functions', 'gcd-and-lcm', 'music-theory', 'trigonometry']"
4838478,Convergence of this Doob Martingale Sequence,"Let $U$ be a Uniform $[0,1]$ variate and conditioned on $U$ , let $\{X_{n}\}_{n\geq 1}$ be iid $\operatorname{Bern}(U)$ variates. Then  show that $E(U|\sigma(X_{1},...,X_{n}))\xrightarrow{a.s.} U$ My attempt(s): Firstly, I can immediately notice that $Y_{n}=E(U|\sigma(X_{1},...,X_{n}))$ is a Doob Martingale sequence that is uniformly bounded (by $1$ ) in $L^{\infty}$ . So, by Martingale convergence theorem $Y_{n}\xrightarrow{a.s\, ,\, L^{p},\,p<\infty} X $ for some $X\in L^{p}$ for all $p<\infty$ . But I don't know how to show that $X$ is equal to $U$ . I also tried to use the method of moments as $U$ is compactly supported. I tried to show that $E(U^{m})=E(E(U^{m}|X_{1},...,X_{n}))\to E(X^{m})$ but the issue is that I need to consider $\bigg(E(U|X_{1},...,X_{n})\bigg)^{m}$ instead of $E(U^{m})$ and that we only have $\bigg(E(U|X_{1},...,X_{n})\bigg)^{m}\xrightarrow{L^{m}} X^{m}$ . This does not give us that $X^{m}$ has the same moments as $U^{m}$ . I also know from the Convergence Theory that $Y_{n}=E(X|X_{1},...,X_{n})$ which would mean that $E(X|X_{1},...,X_{n})=E(U|X_{1},...,X_{n})$ . From this can we conclude that $X=U$ as we have that $\int_{A} (X-U)\,dP=0$ for all $A\in \sigma(X_{1},...,X_{n})$ . I am very unsure about this as I am not using any property of $X_{1},...,X_{n}$ let alone the fact that conditioned on $U$ , they are iid Bernoulli $(U)$ variates. Since $X_{n}$ are iid, I tried to think of using Kolmogorov's Zero-One law but I couldn't see how that could help. Can anyone provide a hint or help me with this?","['uniform-distribution', 'conditional-expectation', 'martingales', 'probability-theory', 'probability']"
4838494,Show that $\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5}$,"There is numerical evidence that $I=\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5}$ . How can this be proved? I was trying to answer another question , and I got it down to this integral. I tried integration by parts, substitution and Maclaurin series, but it seems that this integral requires more advanced techniques. Wolfram does not evaluate the indefinite integral. Here is the graph of $y=\arccos(2\sin^2 x-\cos x)$ for $0\le x\le \frac{\pi}{3}$ . It intersects the axes at $(\frac{\pi}{3},0)$ and $(0,\pi)$ . Not sure if this helps, but apparently we also have $\int_0^{\pi/3}\arccos^{\color{red}{2}}(2\sin^2 x-\cos x)\mathrm dx=\frac{19\pi^3}{135}$ .","['integration', 'definite-integrals', 'calculus', 'closed-form', 'trigonometry']"
4838513,Limit of a sequence does not converge to zero,I want to prove that $$\lim_{n\to \infty}\frac{n}{n+2}\ne 0.$$ I proved it as followed $$|L-a_n|\ge\epsilon$$ $$|0-\frac{n}{n+2}|\ge\epsilon$$ $$\frac{n}{n+2}\ge\epsilon$$ $$n\ge\epsilon n+2\epsilon$$ $$n(1-\epsilon)\ge2\epsilon$$ $$n\ge\frac{2\epsilon}{1-\epsilon}.$$ Is this proof correct? Because I can just switch the sign $\geq$ to $<$ it looks like the series converges to $0$ since $$|L-a_n|\lt\epsilon$$ $$|0-\frac{n}{n+2}|\lt\epsilon$$ $$\frac{n}{n+2}\lt\epsilon$$ $$n\lt\epsilon n+2\epsilon$$ $$n(1-\epsilon)\lt2\epsilon$$ $$n\lt\frac{2\epsilon}{1-\epsilon}.$$,"['limits', 'calculus', 'sequences-and-series']"
4838538,"Can we solve $A \cos (\theta + \alpha) = \sin 2 \theta$ where $\theta \in [0, 2\pi)$ and $A, \alpha$ are constants?","What are solutions to $$A \cos (\theta + \alpha) = \sin 2 \theta$$ where $\theta \in [0, 2\pi)$ and $A, \alpha$ are constants? Graphically, this seems to have 2, 3, or 4 solutions, but I don't know how to find them analytically. In fact, my goal isn't to find $\theta$ , but rather characterize for which regions of $A, \alpha$ there are 4 solutions.  I believe this will be one means of solving how many normals of a given ellipse intersect a given point. Can we simplify via trig identities? $$q = A \cos \alpha \\
r = A \sin \alpha \\
q \cos \theta - r \sin \theta - 2 \sin \theta \cos \theta = 0$$ What about expanding the Taylor Series?","['calculus', 'geometry', 'transcendental-equations', 'real-analysis']"
4838595,Calculating an integral depending on a parameter,"I have to prove as an excercise of my Measure Theory lessons that for $p > -1$ it is true that: $$\int_0^1 \frac{-\ln(x) x^p}{1-x} dx = \sum_{n \geq 1} \frac{1}{(n+p)^2}$$ As an advice, the excercise says that there is some series of part of the expression bellow the integral that helps to prove the result. I am trying to use that $x\in (0,1)$ so $\frac{1}{1-x} = \sum_{n \geq 0} x^n$ . Applying to this excercise we have: $$\int_0^1 \frac{-\ln(x) x^p}{1-x} dx = \int_0^1 -\ln(x) \cdot \sum_{n \geq 0} x^{n+p} dx = \sum_{n \geq 0} \int_0^1 -\ln(x) \cdot x^{n+p} dx $$ The thing is that the last integral diverges: $$
\int_0^1 -\ln(x) \cdot x^{n+p}=\left [\dfrac{{x}^{n+p+1}}{(n+p+1)^2}-\dfrac{\left(n+p+1\right)\,{x}^{n+p+1}\,\ln\left(x\right)}{(n+p+1)^2} \right]_{x \rightarrow 0} ^{x=1}
$$ and $\lim_{x \rightarrow 0} \
\ln(x) = -\infty$ , so I am getting stuck and do not know how to proceed. Thanks in advance :)","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4838604,Show that $\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi$,"Show that $$\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi$$ My try : We know that $$\sum_{n=1}^{\infty} \binom{2n}{n} (H_{2n} - H_{n}) t^n = - \frac{1}{\sqrt{1 - 4t}} \log\left(\frac{1 + \sqrt{1 - 4t}}{2}\right)$$ In particular we have $$\sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n} t^{2n} = - \frac{1}{\sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right)$$ ....(1) Next, dividing both sides of (1) by $t ^ 2$ and then integrating from 0 to 2, we get $$\sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n (2n - 1)} x^{2n - 1} = - \int_{0}^{x} \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt$$ Making the change of variable $t=sin(\theta)$ in $$J = \int \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt$$ $$J = \int \frac{1}{\sin^2 \theta \cos \theta} \log\left(\frac{1 + \cos \theta}{2}\right) \cos \theta \, d\theta$$","['integration', 'summation', 'definite-integrals', 'harmonic-numbers', 'calculus']"
4838611,A sequence of coin flips of random length has n heads. What is the expected value of the sequence's length?,"More rigorously, let's say you are generating sequences of coin flips with a maximum length l, with equal probability of any length from 1 to l. You generate a sequence with n heads. What is the expected value of the sequence's length as l goes to infinity? I tried this for n=1, where I obtained the probability density function P(x) = x*2^(-x) and an expected value of 3. For general value of n, I obtained P(x) = x*C(x,n)*2^(-x). Not sure if the sum converges for these, or what the value is. SOLVED E(0) = 1/2 + 1/4 + 1/8... = 1 E(n)+1 = E(n)/2+E(n+1)/2 => E(n+1) = E(n) + 2 Therefore: E(n) = 2n+1","['statistics', 'probability-distributions', 'probability']"
4838638,Prove that a Banach space cannot be reflexive if some strict closed subspace of its dual space separates its points [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question Let $X$ be a Banach space and let $Z$ be a closed subspace of $X^*$ such that $Z\neq X^*$ . Suppose $Z$ separates the points in $X$ , that is, if $x \in X$ and $x^*(x) = 0$ $\forall x^* \in Z$ then $x = 0$ .
Prove that $X$ is not reflexive, i.e. that the canonical injection $J_X$ : $X$ $\rightarrow$ $X^{**}$ is not surjective. How can I do that?","['hahn-banach-theorem', 'functional-analysis', 'reflexive-space']"
4838697,"Will the range of an inverse trigonometric function change, if the domain of the initial function is changed?","For example, I write a function $$y=\sin x$$ and set its domain to be $\frac{\pi}{2}<x<\pi$ . Now, suppose I want to express $x$ from it, so the result is $$x=\arcsin y$$ But the domain for $x$ was $\frac{\pi}{2}<x<\pi$ , while $\arcsin y$ outputs values between $\left[-\frac{\pi}{2};\frac{\pi}{2}\right]$ . Does it mean that I just changed the range of $\arcsin y$ to be $\left(\frac{\pi}{2};\pi\right)$ , since $x$ and $\arcsin y$ are equal to each other in this scenario? I thought $\arcsin$ or any other inverse trigonometric function has a fixed range, yet here it feels I am redefining its range by setting a specific domain for the initial trigonometric function, which feels wrong, but I cannot explain why. Thank you in advance!","['algebra-precalculus', 'functions', 'trigonometry']"
4838700,Expressing a cubic equation,"Suppose I have a cubic equation $y=ax^3+bx^2+cx+d$ , how could I express it as a Bézier curve in parametric form? That is, how could I find the 4 points $P_i$ for the parametric form $$Bézier(t)=\sum_{i=0}^3\binom{3}{i}\times (1-t)^{3-i}\times t^i\times P_i$$ such that $Bézier (t)$ traces out the same cubic equation? I have already done this for quadratic fairly easily, because 2 points are already on the curve, and I found the third one using the turning point and symmetry of the function. However, I don't know what to do for cubic functions.",['functions']
4838710,Evaluate the limit $\lim_{n\to\infty}\prod_{r=1}^n\frac{4r}{4r+3}$,Evaluate the limit $$\lim_{n\to\infty}\prod_{r=1}^n\frac{4r}{4r+3}$$ My Attempt: I am trying to do by Squeeze theorem and then idea of telescopic product but not able to get anywhere. Using $A.M\geq G.M$ we get $$\frac{2r+1+2r+2}{2}\geq \sqrt{(2r+1)(2r+2)}$$ $$\frac{4r+3}{4r}\geq \frac{\sqrt{(2r+1)(2r+2)}}{2r}$$ $$\frac{4r}{4r+3}\leq \frac{2r}{\sqrt{(2r+1)(2r+2)}}<\frac{2r+1}{\sqrt{(2r+1)(2r+2)}}$$ $$\frac{4r}{4r+3}< \sqrt{\frac{2r+1}{2r+2}}$$ Some clever manipulation or trick is required here There is something here though Limit as $n\to+\infty$ of $\prod_{k=1}^{n} \frac{2k}{2k+1}$ but no general method as such,"['limits', 'calculus', 'real-analysis']"
4838819,Deciphering the Ratio Challenge: Inscribe Circle in Quadrilateral $ABCD$,"I trust this message reaches you well. I am reaching out to request your assistance in solving an intriguing geometry problem that I came across in a recent competitive exam. Despite my best efforts, I have been unable to find a solution. I am eager to gain insights that will surely improve my understanding of this geometric challenge. $ \textbf{Problem Description:} $ What is the ratio of the length of segment $OA$ to the length of segment $OC$ in a geometric figure where a circle is inscribed within a quadrilateral $ABCD$ ? The quadrilateral has vertices labeled as points $A, B, C,$ and $D$ , with point $O$ as the center of the inscribed circle. Given the lengths of line segments $AB (4)$ , $AD (9)$ , $CD (12)$ , and $BC (7)$ , determine the value of $\frac{OA}{OC}$ . My Efforts: I've tried various analytical approaches, but my results have been inconsistent. A more systematic strategy involving geometric properties, algebraic manipulation, or other mathematical tools seems necessary. I made a note of this method, and then I tried to determine the values of $x$ , $y$ , $z$ , and $p$ , but it turned out that there are a countless number of solutions... I would be extremely grateful for any help or advice in figuring out the intricacies of this problem. Thank you for your expertise and support.","['analytic-geometry', 'circles', 'geometry']"
4838851,Converse of Viviani theorem : Statement [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 months ago . The community reviewed whether to reopen this question 4 months ago and left it closed: Original close reason(s) were not resolved Improve this question In a math paper of MAA(Mathematical Association of America) by Z.Chen, he proved that-
""If, inside $∆ ABC$ , there is a circular region $ R$ for which the sum of the
distances from a point $P$ in $R$ to the three sides of the triangle is independent of the
position of $P$ , then $∆ABC$ is equilateral."" Why did he mention ""circular region $R$ ""? Also, I think the theorem is also true for isoceles triangles. (The Converse of Viviani's Theorem https://maa.org/sites/default/files/Chen-CMJ-2006.pdf )","['euclidean-geometry', 'analytic-geometry', 'geometry']"
4838869,Finding Lamé coefficients (scale factors) for the curves,"I am having trouble finding the Lamé coefficients for the following curves. I have specified all the necessary theory, however I am not sure how to practically compute it. $$
\begin{cases} 
x = \frac{1}{2}(v_1^2 - v_2^2), & v_1 = \text{const} \\ 
y = v_1v_2, & v_2 = \text{const} 
\end{cases}
$$ Here is some background:
A point $P$ in $3$ -D space (or its $\mathbf{r}$ ) can be defined using Cartesian coordinates $(x,y,z)$ by $\mathbf{r} = x \mathbf{e}_x + y\mathbf{e}_y + z\mathbf{e}_z$ , where $\mathbf{e}_x,\mathbf{e}_y, \mathbf{e}_z$ are the standard basis vectors. In the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point $P$ with respect to the local coordinate $$
\mathbf{e}_{x} = \frac{\partial\mathbf{r}}{\partial x}; \quad
\mathbf{e}_{y} = \frac{\partial\mathbf{r}}{\partial y}; \quad 
\mathbf{e}_{z} = \frac{\partial\mathbf{r}}{\partial z}.
$$ Applying the same derivatives to the curvilinear system locally at point $P$ defines the natural basis vectors: $$
\mathbf{h}_1 = \frac{\partial\mathbf{r}}{\partial q^1}; \quad 
\mathbf{h}_2 = \frac{\partial\mathbf{r}}{\partial q^2}; \quad 
\mathbf{h}_3 = \frac{\partial\mathbf{r}}{\partial q^3}.
$$ we define the Lamé coefficients (after Gabriel Lamé) by $$
h_1 = |\mathbf{h}_1|; \quad 
h_2 = |\mathbf{h}_2|; \quad 
h_3 = |\mathbf{h}_3|.
$$ Additional note: Lamé  coefficients are also known as scale factors.","['systems-of-equations', 'coordinate-systems', 'curvilinear-coordinates', 'multivariable-calculus', 'calculus']"
4838885,Why aren't integrals invariant over coordinate transformations?,"I am reading chapter 16 of Introduction to smooth manifolds by Lee and at the beginning of the chapter it is stated that  an integral is clearly not invariant under coordinate transformations. Why is that? It looks to me that it is the opposite I mean for instance if I change from  cartesian to polar coordinates the integral should be the same, no matter what coordinates I use","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
4838925,Can you help improve the writing of my proof of $\sin \arctan x$?,"In order to improve my proof writing, I proved a well established, but less frequently encountered, identity (whose proof is Spivak Problem 15.16). Is the proof rigorous? Did I handle the issue of different quadrants (plus/minus) correctly? Is the exposition clear (while still succinct)? How could it be improved? All feedback and suggestions are in scope. Note: Proofs of this identity of course exist.  This question is about proof writing and how to write this proof. Proposition: For all $x$ , $$\sin \arctan x = \pm \frac x {\sqrt{1+x^2}} \\ \cos \arctan x = \pm \frac 1 {\sqrt{1+x^2}}.$$ Proof: Let $$t = \pm \frac x {\sqrt {1 + x^2}}$$ so $$\sqrt{1-t^2} = \pm \frac 1 {\sqrt{1 + x^2}} \\ x = \frac t {\sqrt {1 - t^2}}.$$ Since $t^2 + (\sqrt{1 - t^2})^2 = 1$ , there exists an $\alpha \in (- \frac \pi 2, \frac \pi 2)$ such that $$t = \sin \alpha \\ \sqrt{1 - t^2} = \cos \alpha \\ x = \tan \alpha$$ which completes the proof.","['calculus', 'solution-verification', 'trigonometry', 'proof-writing']"
4838962,Computing Asymptotic Expansion of Coefficients of Implicitly Defined Power Series,"Background :
I've been studying the text ""Analytic Combinatorics"" (amazing book!) and related papers in an effort to intuit the methods therein. My tangentially related background in spectral graph theory has provided a point of entry. However, I've needed to engage in a good bit of self-Flajolet-tion to learn the complex analysis and algebra (resultants, Newton–Puiseux methods) required to understand and execute asymptotic expansions of coefficients of implicitly defined power series associated with polynomial equations. The sticking point right now is section VI.7. Given the implicitly defined power series for $y$ , $$
\begin{align}
y&=ze^y, 
\end{align}
$$ one can solve the characteristic equation $\phi\left(\tau\right)-\tau\phi^{\prime}\left(\tau\right)=0$ to find $\rho=\frac{\tau}{\phi\left(\tau\right)}$ , the singularity of $y\left(z\right)$ closest to the origin and hence the radius of convergence of its series representation in powers of $z$ at the origin. One can then develop an asymptotic expansion of the coefficients of $y\left(z\right)$ from its series representation in powers of $(1-\frac{z}{\rho})^{\frac{1}{2}}$ about $\rho$ . Setting $y=T$ to denote Cayley trees, we see that the Table VI.14 on page 406 lists a few graph generating functions built from this class, i.e., $$
e^{T-\frac{1}{2}T^2},\quad \log{\frac{1}{1-T}}, \quad\frac{1}{\sqrt{1-T}}, \quad\frac{1}{\left(1-T\right)^m}.
$$ These, however, do not conform to the hypothesis that $y=z\phi\left(y\right)$ . Question :
How does one develop a full asymptotic expansion of the coefficients of these given knowledge of $T$ ? Is there a unified method?","['complex-analysis', 'analytic-combinatorics', 'combinatorics', 'generating-functions']"
4838963,Prove that $g(U_n)$ converges in probability to $0$,"Let $(\Omega,\Sigma,\mathbb{P})$ be a probability space and $\{U_n\}_{n\in\mathbb{N}}$ a collection of random variables such that $U_n\sim \text{Unif}(0,n)$ (uniform distribution) for all $n\in\mathbb{N}$ . Prove that if $g:\mathbb{R}\to \mathbb{R}$ is a Borel measurable function satisfying $\lim_{x\to \infty}|g(x)|=0$ , then $g(U_n)$ converges in probability to $0$ . This is an exercise that can be found in the page 47 of this PDF . I know that $\mathbb{P}(|g(U_n)|>\varepsilon )=n^{-1}\int _\mathbb{R} \mathbf{1}_{\{|g|>\varepsilon \}}(x)\mathbf{1}_{[0,n]}(x)dx$ for any $\varepsilon >0$ and $n\in\mathbb{N}$ , however I don't know how to continue. Thank you for your attention!","['uniform-distribution', 'probability-theory', 'probability']"
4838970,"Terminology - domain, codomain, image, and why no ""pre-image"". [duplicate]","This question already has answers here : If there is a need to distinguish between 'image' and 'codomain', why not do this on the input side of a function? (3 answers) Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved If a function $f$ is defined as $$f:X\to Y$$ We say the domain is $X$ , and the codomain is $Y$ . However, it seems the image is distinct from codomain, and only coincides with $Y$ if $f$ is surjective. Question 1: Why is there this distinction? Why is it convenient to define a function's ""target"" as a super-set of its strict image? For example, why would we say $f(x)=x^2$ is $f:\mathbb{R} \to \mathbb{R}$ and not $f: \mathbb{R} \to \mathbb{R}^{+}$ ? Question 2: Similarly, if we are content to make a distinction between image and codomain, why do we insist the domain is actually the pre-image and not a superset? That is, why do we say $f(x)=\log(x)$ is $f:\mathbb{R}^{+} \to \mathbb{R}$ and not $f:\mathbb{R} \to \mathbb{R}$ ? This question asks the same but there is no marked answer, and reading through it, none seemed convincing. The same for this one too.",['functions']
4838989,A suggested proof of the uncountability (sic) of rational numbers,"[Note: I initially made the mistake of posting this over at MathOverflow, where it was promptly closed. There are however some comments there. Link. ] At MathOverflow, I found a very interesting comment to this question here , about proving the uncountability of the real numbers. I observed that it can be modified to a contradictory statement about rational numbers. (Jump to the end to get to the point.) (I'm sorry, writing this on a smartphone I find it hard to produce a nicely named link, a good qoute, and a good credit to the quoted author...) Quoting: I haven't seen the following proof mentioned, which I learned from Hai
Dang at Mississippi State. Suppose the reals are countable, and let $a_1, a_2, a_3, \dots$ be an
enumeration. For each $j$ , let $I_j$ be an interval centered at $a_j$ and
having length $1 / 2^j$ . Since the sequence $\{a_j\} $ enumerates the reals, it follows that $$
 \bigcup_{j=1}^\infty I_j = \mathbb{R}. 
 $$ But since the sum of the
lengths of the $ I_j $ is the geometric series $$ \sum_{j=1}^\infty
 \frac{1}{2^j} = 1, $$ this is nonsense. Please carefully consider the following modification. Let $ a_1, a_2, a_3, \dots $ be a complete enumeration of all rational numbers. For each $ j $ , let $ I_j $ be an interval of real numbers centered at $ a_j $ and having length $ 1 / 2^j $ . Now, since the rational numbers are dense , and every interval has length $ >0 $ , every such interval $ I_j $ includes an infinite number of rational numbers to both sides of $ a_j$ . Since every rational number is being enumerated and assigned an interval, it is inevitable that all intervals will overlap with other intervals in both ends, and any gaps are impossible. It follows that $$
\bigcup_{j=1}^\infty I_j = \mathbb{R}.
$$ This is the interval $(-\infty, \infty)$ , clearly of infinite size or length (using the standard measure on real intervals). But at the same time, the sum of the lengths of the $ I_j $ is the sum of the geometric series $$ \sum_{j=1}^\infty\frac{1}{2^j} = 1,$$ and furthermore due to the overlaps, the actual accumulated length will be $ <1 $ . Contradiction! Accumulated length of intervals is infinite and within $[1/2, 1)$ . It seems to me that above is a proof by contradiction that the rational numbers are not countable after all. Or at least, that any set of numbers cannot be both countable and dense. Comments? Or rather, where is the error? Bonus material: additional food for thought, based on comments to the first post. Consider putting an $\epsilon$ -surrounding (a symmetric interval of size $2\epsilon, \epsilon>0$ ) around every rational number. Let's say $\epsilon=0.1$ for a start. It should be clear that this covers the real number line completely. Now, is there a limit for this $\epsilon$ where it becomes small enough that the complete coverage is no longer true? I would say no, based on the density of rational numbers and what makes the Dedekind cuts work.","['elementary-set-theory', 'fake-proofs', 'real-analysis']"
4839008,Prove that $T:B\mapsto AB+BA$ is bijective if and only if $\det(A)$ and $\text{tr}(A)$ are nonzero.,"I am trying to solve the following problem: Let $A$ be a $2\times 2$ complex matrix. Show that the linear map $$B\longmapsto AB + BA$$ is a bijection if and only if $\det(A)$ and $\text{tr}(A)$ are nonzero. My attempt: Let us call $T$ the linear map. The Jordan canonical form of $A$ can be of the form $\begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}$ or $\begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix}$ . Suppose that $A$ is diagonalizable. Then, there exist vectors $v_1$ and $v_2$ in $\mathbb{C}^2$ such that $Av_1 = \lambda_1 v_1$ , $Av_2 = \lambda_2 v_2$ , and $\{v_1,v_2\}$ is a basis of $\mathbb{C}^2$ . It follows that the set of the matrices $$X_1 = \begin{pmatrix} v_1 & 0  \end{pmatrix}, ~~X_2 = \begin{pmatrix} 0&v_1  \end{pmatrix},~~ X_3 = \begin{pmatrix} v_2 & 0  \end{pmatrix},~~X_4 = \begin{pmatrix} 0 & v_2  \end{pmatrix}$$ is a basis of $M_2(\mathbb{C})$ . Notice that $$T(X_1) = AX_1 + X_1 A = \begin{pmatrix} \lambda_1v_1 & 0  \end{pmatrix} + \begin{pmatrix} a_{11}v_1 & a_{12}v_1  \end{pmatrix} = \begin{pmatrix} (\lambda_1+a_{11})v_1 & a_{12}v_1  \end{pmatrix} = (\lambda_1+a_{11})X_1 + a_{12}X_2$$ and $$T(X_2) = AX_2 + X_2 A = \begin{pmatrix} 0 & \lambda_1v_1  \end{pmatrix} + \begin{pmatrix} a_{21}v_1 & a_{22}v_1  \end{pmatrix} = \begin{pmatrix} a_{21}v_1 & (\lambda_1+a_{22})v_1  \end{pmatrix} = a_{21}X_1 + (\lambda_1+a_{22})X_2.$$ Likewise, we get that $T(X_3) = (\lambda_2+a_{11})X_3 + a_{12}X_4$ and $T(X_4) = a_{21}X_3 + (\lambda_2+a_{22})X_4$ . Thus, the representation of $T$ on the basis $\{X_1,X_2,X_3,X_4\}$ is given by the matrix: $$
\begin{pmatrix}
\lambda_1+a_{11} & a_{21} & 0 & 0 \\
a_{12}& \lambda_1+a_{22} & 0 & 0 \\
0 & 0 & \lambda_2+a_{11} & a_{21} \\
0 & 0 & a_{12} & \lambda_2+a_{22} \\
\end{pmatrix}.
$$ Since this matrix is diagonal by blocks, we have that $$\det(T) = \det\begin{pmatrix}
\lambda_1+a_{11} & a_{21} \\
a_{12}& \lambda_1+a_{22} 
\end{pmatrix}  \det\begin{pmatrix}
\lambda_2+a_{11} & a_{21} \\
a_{12} & \lambda_2+a_{22} 
\end{pmatrix} = (\lambda_1^2+\text{tr}(A)\lambda_1+\det(A))(\lambda_2^2+\text{tr}(A)\lambda_2+\det(A)).$$ Because $\lambda_1$ and $\lambda_2$ are roots of the polynomial $x^2-\text{tr}(A)x+\det(A)$ , it follows that $$\det(T) = (2\text{tr}(A)\lambda_1)(2\text{tr}(A)\lambda_2) = 4\text{tr}(A)^2\det(A).$$ Hence, $\det(T)=0$ if and only if $\det(A)=0$ or $\text{tr}(A)=0$ , which completes the proof of this case. However, I cannot apply the same idea to prove the statement when $A$ is not diagonalizable, so I am stuck. Can you give me some advice to complete the exercise? Thanks in advance. Edit: I just realized that for the case $J_A = \begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix}$ we can use a similar idea. Indeed, there exists a basis $\{v_1,v_2\}$ of $\mathbb{C}^2$ such that $Av_1 = \lambda v_1$ and $Av_2 = v_1 + \lambda v_2$ . Repeating the previous construction with this basis, we get the following representation for T: $$
\begin{pmatrix}
\lambda+a_{11} & a_{21} & 1 & 0 \\
a_{12}& \lambda+a_{22} & 0 & 1 \\
0 & 0 & \lambda+a_{11} & a_{21} \\
0 & 0 & a_{12} & \lambda+a_{22} \\
\end{pmatrix}.
$$ This matrix also has determinant equal to $4\text{tr}(A)^2\det(A)$ , and so the problem is solved. I am still looking for a shorter approach.","['matrices', 'diagonalization', 'linear-algebra']"
