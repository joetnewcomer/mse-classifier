question_id,title,body,tags
4820155,Prove that $\sin^2x \cos^2x = \frac{1}{8}(1-\cos(4x))$,"I'm struggling to show that $\sin^2x\cos^2x = \frac{1}{8}(1-\cos(4x))$ . Knowing that $\sin^2x = \frac{1}{2}(1-\cos(2x))$ $\cos^2x = \frac{1}{2}(1+\cos(2x))$ It follows that: $\sin^2x \cos^2x = \frac{1}{2}(1-\cos(2x))\frac{1}{2}(1+\cos(2x))$ Combining like terms: $\sin^2x \cos^2x = \frac{1}{4} (1+\cos^2(2x))$ Eq2 can be used again to reduce the power, here $\sin^2x \cos^2x = \frac{1}{4} (1 + \frac{1}{2}(1 + \cos(4x)))$ And combining like terms $\sin^2x \cos^2x = \frac{1}{4} (1 + (\frac{1}{2}+ \frac{1}{2}\cos(4x))$ $\sin^2x \cos^2x = \frac{1}{4} +  \frac{1}{8}+ \frac{1}{8}\cos(4x)$ $\sin^2x \cos^2x = \frac{1}{8} ( 3 + \cos(4x)) $ But I seem to get nowhere...",['trigonometry']
4820210,Compute the Googolplex-th Fibonacci number modulo $10^9+7$,"Is it possible to find this in a reasonable amount of time? The best theoretical approach that I’m aware of is using the Fibonacci Matrices, but even at a complexity of $O(\log n)$ , the logarithm of a googolplex is already huge. Is there a more efficient algorithm to solve this? Can Binet's formula be used in some manner?","['number-theory', 'computational-complexity', 'computer-science']"
4820216,Computation of $\left.\dfrac{\text d^{100}}{\text dz^{100}}\cos(z^2)\right|_{z=0}$,"I was solving the following contour integral, $$\oint_{|z|=r,\forall r>0}\dfrac{\cos(z^2)+\sin z}{z^{101}}\text d z=\dfrac{2\pi i}{100!}f^{(100)}(0),$$ where $f(z)=\cos(z^2)+\sin z$ . Sine's derivative is direct by induction: $$\left.\dfrac{\text d^{100}}{\text dz^{100}}\sin(z)\right|_{z=0}=\left[\sin(z)\right|_{z=0}=0.$$ But I'm not too sure about $\cos(z^2)$ .","['complex-analysis', 'calculus', 'derivatives']"
4820245,"If $\{f_n\}$ is uniformly integrable, then there's a subsequence $(f_{k_n})$ of $(f_n)$ such that $\big(\int _Ef_{k_n}d\mu \big)$ is a Cauchy sequence","The page 20 of the book "" Topological Fixed Point Theory for Singlevalued and Multivalued Mappings and Applications "" (written by Ben Amar and O'Regan) has the following lemma. Lemma: Let $(X,\Sigma,\mu )$ be a finite measure space and $\{f_n\}_{n\in\mathbb{N}}\subseteq \mathcal{L}^1_\mathbb{R}(\mu )$ .
Suppose that $\sup_{n\in\mathbb{N}}\Vert f_n\Vert _{L^1}<\infty$ $(\forall \varepsilon >0)(\exists \delta >0)(\forall E\in \Sigma )\left(\mu (E)<\delta\Rightarrow \sup_{n\in\mathbb{N}}\int _E|f_n|d\mu
 <\varepsilon \right)$ . Then there's a subsequence $(f_{k_n})_{n\in\mathbb{N}}$ of $(f_n)_{n\in\mathbb{N}}$ such that $\left(\int _Ef_{k_n}d\mu
 \right)_{n\in\mathbb{N}}$ is a Cauchy sequence for all $E\in \Sigma$ . Unfortunately, the book doesn't give any tips on how to prove this lemma. My question is: how can I prove that lemma? I couldn't do anything worth mentioning, but I know that the conclusion of that lemma is true if and only if there's a subsequence $(f_{k_n})_{n\in\mathbb{N}}$ of $(f_n)_{n\in\mathbb{N}}$ that converges weakly. Thank for your attention! EDIT: Please don't use the Dunford-Pettis Theorem because I want to use that lemma to prove this theorem. Please don’t use either the Eberlein-Smulian Theorem, because I want to avoid advanced theorems of functional analysis.","['uniform-integrability', 'measure-theory', 'lp-spaces', 'measurable-functions']"
4820328,"If $g(t,x) \rightarrow 0$ as $x \rightarrow \infty$, then decays uniformly in $t$?","Assume we have continuous $g:[0,T]\times \mathbb{R}\rightarrow \mathbb{R}$ such that $$
\lim_{x \rightarrow \pm \infty} g(t,x)=0
$$ for every $t \in [0,T]$ . Then let $(t_k)_{k\in\mathbb{N}}\subseteq [0,T]$ converge to some $t \in [0,T]$ and $(x_k)_{k \in \mathbb{N}}\subseteq\mathbb{R}$ diverge to $\infty$ . Is it then true that $$
\lim_{k \rightarrow \infty} g(t_k,x_k) =0?
$$ Initially, I thought yes. I tried to use continuity to make $\lvert g(t_k,x_k)-g(t,x)\rvert$ small. But continuity is not uniform, such that there is noch guarantee the above expression goes to $0$ as $k\rightarrow \infty$ . So there might be a counterexample
, although I do not see it…","['limits', 'continuity', 'examples-counterexamples', 'real-analysis']"
4820356,Convergence analysis of a quotient of two sequences $x_{n+1}^2 = x_n^2 + \frac{c}{x_n^2}$.,"Given two recurrence relations $x_{n+1}^2 = x_n^2 + \frac{c_1^2}{x_n^2}$ and $y_{n+1}^2 = y_n^2 + \frac{c_2^2}{y_n^2}$ for $c_1, c_2\in \mathbb{R}^+$ and $x_1=y_1=c_3\in\mathbb{R}^+$ . We want to study the convergence properties of $r_n := \frac{y_n^2 c_1}{x_n^2 c_2}$ as $n\rightarrow \infty$ . I strongly suspect that $r_n$ converges to 1 as $n\rightarrow \infty$ but am unable to prove it yet. I was also able to show that 1 is its the only fixed point and an attractor in the sense that $r_n > 1 \Rightarrow r_{n+1} < r_n$ and $r_n < 1 \Rightarrow r_{n+1} > r_n$ . I was then able to prove that the $r_n$ does converge, as $r_n$ is strictly monotonous for $n\geq 2$ . I would like to show that $r_n$ cannot asymptotically near another limit than 1 or find a counter-example for it.","['recurrence-relations', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4820372,Inequalities for the solution of $x = (x-a) e^{x+a}$.,"Let $a > 0$ . The equation $$(x-a) \, e^{x+a} = x $$ must be solved for $x > 0$ . Since the solution does not have a closed form, I would like to obtain bounds for the solution. Until now, I was able to show that $$ a < x < \sqrt{a}\sqrt{a + 1}. $$ The lower bound comes from $(x-a) e^{x+a} = x > 0$ and the upper bound comes from $e^{x+a} > 1+x+a$ . I'm interested in any better bounds than the ones I provided. However, to keep the question objective, I'm interested in the proof of the better upper bound $$x < \sqrt{a}\sqrt{a+e^{-a}},$$ which is very tight. I have tried to compare derivatives and use other inequalities but to no avail. I also obtained the power series $$ x =\sqrt{a} - \frac{a}{4} + \frac{31 a^{3/2}}{96} +\frac{a^2}{16} - \frac{229 a^{5/2}}{92160} + \frac{a^3}{320} + \mathcal{O}(a^{7/2})$$ as $a \to 0^+$ for the solution; however, I'm after a simple bound.","['elementary-functions', 'algebra-precalculus', 'nonlinear-analysis', 'upper-lower-bounds']"
4820376,Probability of getting a correct Bit,"I have a probability problem that goes like this: I want to sent a bit across a channel that has a certain error rate. The probability of getting a bit wrong is $0.3$ , and so to increase the chances of sending a correct message, I resend the bit $n$ times, and the receiver interprets the bit as the most common bit in the sequence, so $n$ is odd. The error rate is not independent though, if a bit is incorrect, the bit after that has a probability of being incorrect of $0.7$ . My question is, what is the minimum $n$ (number of resent bits) that I need to send to guarantee a probability of getting the correct bit interpreted of $0.9$ ? I can't seem to find a closed form solution for the general probability.","['combinatorics', 'probability-theory', 'probability']"
4820378,Bayes' theorem and card colors,"This is an expansion/generalization of a previous question I've asked here . Some of the simplifications I made in the original question turned out to be too simplifying, so I'm trying again. The most general case involves millions of cards and hundreds of front/back colors, if that's important for any reason. I have 10 cards. The color of the front of the card is either pink, yellow, or green. The color of the back of the card is orange, blue, or red. Here’s what I know: 5 of the cards have pink fronts, 3 have yellow fronts, and 2 have green fronts. 5 of the cards have orange backs, 1 has a blue back, and 4 have red backs Cards with an orange back can have either a yellow or pink front, but not green Cards with a red back can have either a green or pink front, but not yellow Cards with a blue back can have any color front If you're handed a new card with a pink/yellow/green front, what are the odds that the back is orange/blue/red given that the new card follows the same probability distribution as the 10 old cards? I've simulated the probabilities, and I'm reasonably confident the simulation is accurate, but I don't know how to get the closed-form solutions. The general idea is to create all possible decks that satisfy the constraints, and then count the number of occurrences of each card type in all decks. Simulation code (python): from itertools import permutations

fronts = [""pink""] * 5 + [""yellow""] * 3 + [""green""] * 2
backs = [""orange""] * 5 + [""blue""] * 1 + [""red""] * 4

decks = [list(zip(x, backs)) for x in permutations(fronts, len(backs))]
permitted_decks = [d for d in decks if (""green"", ""orange"") not in d and (""yellow"", ""red"") not in d]

unique_decks = []
for deck in permitted_decks:
    sorted_deck = sorted(deck)
    if sorted_deck not in unique_decks:
        unique_decks.append(sorted_deck)
print(f""{len(unique_decks)} unique decks"")

counts = {f: {} for f in set(fronts)}

for deck in unique_decks:
    for front, back in deck:
        counts[front][back] = counts[front].get(back, 0) + 1

for front, back_counts in counts.items():
    back_total = sum(back_counts.values())
    for back_color, back_count in back_counts.items():
        for quantity in [front, back_color, back_count, back_total, round(back_count/back_total, 10)]:
            print(quantity, end=""\t"")
        print("""")

>>> 3 unique decks
>>> yellow  blue    1   9   0.1111111111    
>>> yellow  orange  8   9   0.8888888889    
>>> pink    orange  7   15  0.4666666667    
>>> pink    red     7   15  0.4666666667    
>>> pink    blue    1   15  0.0666666667    
>>> green   red     5   6   0.8333333333    
>>> green   blue    1   6   0.1666666667 Any help would be appreciated. Thanks! EDITED:
If you run the simulation and do the counts for permitted_decks instead of unique_decks , by replacing for deck in unique_decks with for deck in permitted_decks , you get different numbers: yellow  orange  604800  691200  0.875   (7/8)
yellow  blue    86400   691200  0.125   (1/8)
pink    orange  547200  1152000 0.475   (19/40)
pink    red     518400  1152000 0.45    (9/20)
pink    blue    86400   1152000 0.075   (3/40)
green   red     403200  460800  0.875   (7/8)
green   blue    57600   460800  0.125   (1/8) So can I infer that even though there are three unique decks, the number of ways I can arrange the sides to create each unique deck is not the same for all decks, so the deck orientations are not equally likely? Here are the counts: 86400 [('green', 'red'), ('green', 'red'), ('pink', 'orange'), ('pink', 'orange'), ('pink', 'orange'), ('pink', 'red'), ('pink', 'red'), ('yellow', 'blue'), ('yellow', 'orange'), ('yellow', 'orange')]
********************
86400 [('green', 'red'), ('green', 'red'), ('pink', 'blue'), ('pink', 'orange'), ('pink', 'orange'), ('pink', 'red'), ('pink', 'red'), ('yellow', 'orange'), ('yellow', 'orange'), ('yellow', 'orange')]
********************
57600 [('green', 'blue'), ('green', 'red'), ('pink', 'orange'), ('pink', 'orange'), ('pink', 'red'), ('pink', 'red'), ('pink', 'red'), ('yellow', 'orange'), ('yellow', 'orange'), ('yellow', 'orange')]","['combinatorics', 'bayes-theorem', 'probability']"
4820471,Is the dual norm of an induced norm an induced norm?,"I think this Q&A gets close to answering this, but does not provide a full response. If you have some induced matrix norm $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}$ induced by the vector norm $\|\cdot\|'$ , I'm wondering if is it the dual of this norm (i.e. $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ ) also an induced norm? Furthermore, if $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ is an induced norm, is there a relationship between the vector norm $\|\cdot\|'$ and the vector norm which would induce $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ ? I was considering trying to work with what I think is just an unnamed Theorem, $$
\|A\|_{\|\cdot\|, \|\cdot\|} = \|A^*\|_{\|\cdot\|^D, \|\cdot\|^D},
$$ but that doesn't seem to be getting me anywhere.","['matrices', 'normed-spaces', 'linear-algebra', 'matrix-norms']"
4820497,Two sample test - distribution of pooled variance estimator,"I am attending a statistics course this semester and although it is offered by the math department the precise assumptions underlying the main theorems are not provided, let alone the proofs. That being said, I would like to get a reference or at least discuss the proof of the following fact: Suppose that we are given two normal iid samples $(X_{11},\ldots,X_{1n_1})$ and $(X_{21},\ldots,X_{2n_2})$ with standard derivations $\sigma_i$ and means $\mu_i$ and set $$S^2_i:=S^2_{X_i}:=S_{X_iX_i}:=\sum_{j=1}^{n_i}(X_{ij}-\bar{X}_i)^2$$ as well as $$S^2:=S_1^2+S_2^2$$ and finally $\nu:=n_1+n_2-2$ .
Then it is claimed that $$T:=\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{S^2/\nu}{n_1}+\frac{S^2/\nu}{n_2}}}\sim t_\nu$$ Note that if $\bar{X}_1\perp\bar{X}_2$ then $^1$ $$Z:=\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma^2}{n_1}+\frac{\sigma^2}{n_2}}}\sim N(0,1)$$ and it can easily be shown that $$T=\frac{Z}{\sqrt{W/\nu}}$$ with $W=S^2/\sigma^2$ , so perhaps we can show that $W_1:=S_1^2/\sigma^2$ and $W_2:=S_2^2/\sigma^2$ are independent, i.e. $W_1\perp W_2$ and $W\perp Z$ as this would yield the desired result. $^{2}$ Is this indeed the usual strategy? Where can I look this up? $^1$ If $(X_1,\ldots,X_n)$ is iid with $X_i\sim N(\mu,\sigma^2)$ , then $\bar X\sim N(\mu,\sigma^2/n)$ and hence $$\bar X_1-\bar X_2\sim N(\mu_1-\mu^2,\sigma_1^2/n_1+\sigma_2^2/n_2)$$ (just look up ""linear combination of independent normal variables""). $^2$ It is well known that $W_i\in\chi^2(n_i)$ and hence $W_1+W_2\in\chi^2(n_1+n_2)$ if $W_1\perp W_2$ . Furthermore $Z/\sqrt{W/n}\in t_n$ if $Z\in N(0,1)$ , $W\in\chi^2(n)$ and $W\perp Z$ .","['statistics', 'probability-distributions', 'confidence-interval', 'probability-theory']"
4820524,Formulas for Trace expressions like $\operatorname{Tr}(AA^TA)$ for Gaussian $A$,"Suppose $A$ is an $n\times n$ matrix with IID standard normal entries and I'm given a sequence like $A,A^T,A,A,A,A^T$ . How would I obtain the formula for $E\operatorname{Tr}(AA^TAAAA^T)$ in terms of $n$ ? I've gotten a few formulas by computing expectations for a few $n$ and extrapolating, but this doesn't scale beyond 6","['combinatorics', 'probability-theory']"
4820589,Does there exist an $f: \mathbb{R} \to \mathbb{R}$ such that $f(f(x)) = x^3 + 1$?,"This (question 6) was from the November $2022$ NZ maths olympiad workshop (I couldn't find the online one, even on the Wayback Machine so I'm emailing them). All I know is that $f$ is bijective (injective & surjective). Assume that $f$ is not injective, ie there exist $x_1 < x_2$ such that $f(x_1) = f(x_2)$ $$
\begin{align}
  f(x_1) &= f(x_2) \\[1ex]
  f(f(x_1)) &= f(f(x_2)) \\[1ex]
  x_1^3 + 1 &= x_2^3 + 1 \\[1ex]
  x_1^3 &= x_2^3 \\[1ex]
  x_1 &= x_2 \ \large{\unicode{x21af}}
\end{align}
$$ The last line is true since $x \mapsto x^3$ is an injective function. (it is strictly monotonic) Assume that $f$ is not surjective, ie there exists a $y_0$ such that $\forall x : f(x) \neq y_0$ $$
\begin{align}
  f(x) &\neq y_0 \\[1ex]
  f(f(x)) &\neq f(y_0),\quad (f \text{ is injective})  \\[1ex]
  x^3 + 1 &\neq f(y_0) \ \large{\unicode{x21af}}
\end{align}
$$ This is a contradiction since $x^3 + 1$ spans the reals, so $f$ is bijective. I have a feeling that $f$ is discontinuous, although I can't prove it. I find it hard to even find specific values of $f$ , like I only know that $$f(\alpha) = \alpha$$ where $\alpha$ is the real solution to $x = x^3 + 1$ . Trying to evaluate $f$ at other points results in other unknown values. Below I have rearranged the functional equation (Wolfram Alpha couldn't solve it). Since $f$ is bijective there must be an inverse. $$
\begin{align}
  f(f(x)) &= x^3 + 1 \\[1ex]
  f(x) &= f^{-1}\left(x^3 + 1 \right) \\[1ex]
  f(x) &= \left[ f^{-1}(x) \right]^3 + 1 \ \text{(from eqn 1)}\\[1ex]
  f^{-1}\left(x^3 + 1 \right) &= \left[ f^{-1}(x) \right]^3 + 1 \\[1ex]
\end{align}
$$ Alternatively one could rearrange the equation in terms of $f$ $$
\begin{align}
  f(x) &= f^{-1}\left(x^3 + 1 \right) \\[1ex]
  f\left( \sqrt[3]{x - 1} \right) &= f^{-1}(x) \\[1ex]
  \left[ f\left( \sqrt[3]{x - 1} \right) \right]^3 + 1 &= \left[ f^{-1}(x) \right]^3 + 1 \\[1ex]
  f(x) &= \left[ f\left( \sqrt[3]{x - 1} \right) \right]^3 + 1 \\[1ex]
  f\left(x^3 + 1 \right) &= \left[ f(x) \right]^3 + 1
\end{align}
$$","['contest-math', 'functional-equations', 'functions']"
4820591,"Evaluate $\int_{0}^{1}\{1/x\}^2\,dx$","Evaluate $$\displaystyle{\int_{0}^{1}\{1/x\}^2\,dx}$$ Where {•} is fractional part My work $$\displaystyle{\int\limits_0^1 {{{\left\{ {\frac{1}{x}} \right\}}^2}dx}  = \sum\limits_{n = 1}^\infty  {\left( {\int\limits_{1/\left( {n + 1} \right)}^{1/n} {{{\left\{ {\frac{1}{x}} \right\}}^2}dx} } \right)} }$$ For $\displaystyle{x \in \left( {\frac{1}{{n + 1}},\frac{1}{n}} \right]}$ $$\displaystyle{n \leqslant \frac{1}{x} < n + 1 \Rightarrow \left\{ {\frac{1}{x}} \right\} = \frac{1}{x} - n \Rightarrow \int\limits_{1/\left( {n + 1} \right)}^{1/n} {{{\left\{ {\frac{1}{x}} \right\}}^2}dx}  = \int\limits_{1/\left( {n + 1} \right)}^{1/n} {{{\left( {\frac{1}{x} - n} \right)}^2}dx}  = }$$ $$\displaystyle{ = \int\limits_{1/\left( {n + 1} \right)}^{1/n} {\left( {\frac{1}{{{x^2}}} - \frac{2}{x}n + {n^2}} \right)dx}  = 1 + \frac{n}{{n + 1}} - 2n\ln \frac{{n + 1}}{n} \Rightarrow \boxed{\int\limits_0^1 {{{\left\{ {\frac{1}{x}} \right\}}^2}dx}  = \sum\limits_{n = 1}^\infty  {\left( {1 + \frac{n}{{n + 1}} - 2n\ln \frac{{n + 1}}{n}} \right)} }}$$ $$\displaystyle{\sum\limits_{n = 1}^\infty  {\left( {1 + \frac{n}{{n + 1}} - 2n\ln \frac{{n + 1}}{n}} \right)}  = \sum\limits_{n = 1}^\infty  {\left( {2 - \frac{1}{{n + 1}} - 2n\ln \frac{{n + 1}}{n}} \right)}  = \mathop {\lim }\limits_{N \to \infty } \sum\limits_{n = 1}^N {\left( {2 - \frac{1}{{n + 1}} - 2n\ln \frac{{n + 1}}{n}} \right)} }$$","['integration', 'summation', 'calculus', 'closed-form', 'indefinite-integrals']"
4820613,"Double integral $\int_{0}^{1}\int_{0}^{1}\frac{x^{a-1}y^{b-1}}{(1+xy)\ln(xy)}\,dx\,dy$","Double integral $$\int_{0}^{1}\int_{0}^{1}\frac{x^{a-1}y^{b-1}}{(1+xy)\ln(xy)}\,dx\,dy$$ $\displaystyle{a,b>0}$ my work $$I\left( {a,b} \right) = \int\limits_0^1 {\int\limits_0^1 {\frac{{{x^a}{y^b}}}{{\ln \left( {xy} \right)}}dx\,dy} }  = \frac{{\ln \left( {\dfrac{{1 + b}}{{1 + a}}} \right)}}{{a-b}}$$ $$\displaystyle{I\left( {a,b} \right) = \int\limits_0^1 {\int\limits_0^1 {\frac{{{x^a}{y^b}}}{{\ln \left( {xy} \right)}}dx\,dy} }  = }$$ $$\displaystyle{ = \int\limits_0^1 {\int\limits_0^1 {{x^a}{y^b}\left( { - \int\limits_0^\infty  {{e^{t \cdot \ln \left( {xy} \right)}}dt} } \right)dx\,dy} }  =  - \int\limits_0^\infty  {\left( {\int\limits_0^1 {\int\limits_0^1 {{x^a}{y^b}{e^{t \cdot \ln \left( {xy} \right)}}dx\,dy} } } \right)dt}  =  - \int\limits_0^\infty  {\left( {\int\limits_0^1 {\int\limits_0^1 {{x^a}{y^b}{{\left( {xy} \right)}^t}dx\,dy} } } \right)dt}  = }$$ $$\displaystyle{ =  - \int\limits_0^\infty  {\left( {\left( {\int\limits_0^1 {{x^{a + t}}dx} } \right)\left( {\int\limits_0^1 {{y^{b + t}}dy} } \right)} \right)dt}  =  - \int\limits_0^\infty  {\left( {\frac{1}{{a + 1 + t}} \cdot \frac{1}{{b + 1 + t}}} \right)dt}  = \frac{1}{{b - a}}\int\limits_0^\infty  {\left( { - \frac{1}{{a + 1 + t}} + \frac{1}{{b + 1 + t}}} \right)dt}  = \frac{{\ln \left( {\dfrac{{1 + b}}{{1 + a}}} \right)}}{{a - b}}}$$ $$\displaystyle{I = \int\limits_0^1 {\int\limits_0^1 {\frac{{{x^{a - 1}}{y^{b - 1}}}}{{\left( {1 + xy} \right)\ln \left( {xy} \right)}}dx\,dy} }  = \int\limits_0^1 {\int\limits_0^1 {\frac{{{x^{a - 1}}{y^{b - 1}}}}{{\ln \left( {xy} \right)}}\left( {\sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{{\left( {xy} \right)}^k}} } \right)dx\,dy} }  = \sum\limits_{k = 0}^\infty  {\left( {{{\left( { - 1} \right)}^k}\left( {\int\limits_0^1 {\int\limits_0^1 {\frac{{{x^{a + k - 1}}{y^{b + k - 1}}}}{{\ln \left( {xy} \right)}}dx\,dy} } } \right)} \right)} }$$","['integration', 'indefinite-integrals', 'calculus', 'closed-form']"
4820747,What exactly is the unique union of a family of sets?,"It's quite an elementary question, but I couldn't find anything relevant to the query online. In Velleman's book ""How To Prove It"", 3.6.5, this excerpt can be found 1 . It defines ""a new set $∪!F$ by the formula $∪!F = \{x \mid ∃!A (A ∈ F ∧ x ∈ A)\}$ "".  I frankly don't really know what it would mean. Is this even called a unique family of sets? Not sure, that's just what I believe it must be known as. For example, if we let a family of sets $F = \{\{1, 2, 3, 4\}, \{2, 3, 4, 5\}, \{3, 4, 5, 6\}\}$ , then the $∪F$ would obviously be $\{1,2,3,4,5,6\}$ . What would $∪!F$ be? In my opinion, it would be the same, but I'm not sure since I don't fully understand the concept.","['elementary-set-theory', 'logic', 'first-order-logic']"
4820763,"Showing that, in Pascal's triangle, the product of the numbers along each median is always the same.","On Pascal's triangle with any number of rows, draw the three medians . For each median, calculate the product of the numbers that zig-zag along that median. The three products are always equal! (proof below) Note that, on the vertical median, every other number is either just to the left, or just to the right of the median; it makes no difference. On the oblique medians, every other number is just below the median (this rule applies when the number of rows is even or odd). Here is an example with a triangle with $10$ rows (the top $1$ is row $0$ ): $1\cdot 1\cdot 2\cdot 3\cdot 6\cdot 10\cdot 20\cdot 35\cdot 70\cdot 126\cdot 252=560105280000$ $1\cdot 6\cdot 15\cdot 35\cdot 35\cdot 56\cdot 28\cdot 36\cdot 9\cdot 10\cdot 1=560105280000$ This is such a simple result, so I wonder if there is an intuitive explanation. My question is: Is there an intuitive explanation for this result? To give an idea of what I mean by ""intuitive explanation"", here are some examples: The sum of the numbers in the $n$ th row is $2^n$ , because $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}+\cdots+\binom{n}{n}=(1+1)^n=2^n$ . $\sum_{k=0}^n \binom n k ^2 = \binom {2n} n$ has an intuitive combinatorial proof. In the left-justified Pascal's triangle, any square matrix that shares a border with the left edge of the triangle has determinant $1$ ; this has an intuitive proof . My non-intuitive proof I prove the case when the number of rows is even, $2n$ . The proof with an odd number of rows is very similar. $P_v=$ product along vertical median $=\prod\limits_{k=1}^n\binom{2k-1}{k-1}\binom{2k}{k}$ $P_o=$ product along oblique median $=\prod\limits_{k=1}^n\binom{n+k}{2k-1}\binom{n+k}{2k}$ $\begin{align}
\dfrac{P_v}{P_o}&=\prod\limits_{k=1}^n\dfrac{(2k-1)!^2 (2k)!^2 (n+1-k)!(n-k)!}{(k-1)!k!^3 (n+k)!^2}\\
&=\prod\limits_{k=1}^n \dfrac{(2k-1)!^2(2k)!^2}{k!^2 (n+k)!^2}\\
&=\prod\limits_{k=1}^n \dfrac{(2k)!^4}{k!^2 (n+k)!^2 (2k)^2}\\
&=\dfrac{(2!4!6!\dots (2n)!)^4}{(1!2!3!\dots n!)^2((n+1)!(n+2)!(n+3)!\dots (2n)!)^2(2\cdot 4\cdot 6\dots (2n))^2}\\
&=\dfrac{(2!4!6!\dots (2n)!)^2}{(1!3!5!\dots (2n-1)!)^2 (2\cdot 4\cdot 6\dots(2n))^2}\\
&=1
\end{align}$ Context I have been playing with Pascal's triangle, investigating some of its mysterious , geometrical and humourous properties.","['factorial', 'binomial-coefficients', 'combinatorics', 'intuition', 'products']"
4820820,"Evaluate $\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx$ [duplicate]","This question already has answers here : How to evaluate $\int_{0}^{\pi }\theta \ln\tan\frac{\theta }{2} \, \mathrm{d}\theta$ (6 answers) Closed 7 months ago . Evaluate $$\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx$$ First we will work out the complex integral of the function $$\displaystyle{f\left( z \right) = \frac{{{z^2}}}{{{e^z} - 1}},{\text{ }}z \ne 0} and \displaystyle{f\left( 0 \right) = 0}$$ in the rectangle $\displaystyle{c:{\text{ }}OABC}$ with $$\displaystyle{O\left( {0,0} \right){\text{, }}A\left( {R,0} \right){\text{, }}B\left( {R,i \pi } \right){\text{ \& }}D\left( {i\pi ,0} \right)}$$ Obviously f(z) is analytic without poles in the above rectangle, so $\displaystyle{\int\limits_c {f\left( z \right)dz}  = 0}$ For $$\displaystyle{z = R + iy{\text{  with  }}y \in \left[ {0,\pi } \right]:\mathop {\lim }\limits_{R \to \infty } \left| {f\left( z \right)} \right| = \mathop {\lim }\limits_{R \to \infty } \left| {\frac{{{{\left( {R + iy} \right)}^2}}}{{{e^{R + iy}} - 1}}} \right| \leqslant \mathop {\lim }\limits_{R \to \infty } \frac{{\left| {{{\left( {R + iy} \right)}^2}} \right|}}{{{e^R} - 1}}\xrightarrow{{R \to  + \infty }}0}$$ And $$\displaystyle{0 = \int\limits_c {f\left( z \right)dz}  = \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_\infty ^0 {\frac{{{{\left( {x + i\pi } \right)}^2}}}{{{e^{x + i\pi }} - 1}}dx}  + i \cdot \int\limits_\pi ^0 {\frac{{{{\left( {iy} \right)}^2}}}{{{e^{iy}} - 1}}dy}  \Rightarrow }$$ $$\displaystyle{ \Rightarrow \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} + 1}}dx}  - {\pi ^2}\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  + 2i\pi \int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  - i \cdot \int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = 0}$$ $$\displaystyle{\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  =  - \left[ {\ln \left( {1 + {e^{ - x}}} \right)} \right]_0^\infty  = \ln \left( 2 \right)}$$ $$\displaystyle{\int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{x{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  = \int\limits_0^\infty  {x\left( {\sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^{k + 1}}{e^{ - kx}}} } \right)dx}  = \sum\limits_1^\infty  {{{\left( { - 1} \right)}^{k + 1}}\int\limits_0^\infty  {x{e^{ - kx}}dx} }  = \sum\limits_1^\infty  {\frac{{{{\left( { - 1} \right)}^{k + 1}}}}{{{k^2}}}}  = .. = \frac{{{\pi ^2}}}{{12}}}$$ $$\displaystyle{\int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{2{{\sin }^2}\left( {\dfrac{y}{2}} \right) - 2i \cdot \sin \left( {\dfrac{y}{2}} \right)\cos \left( {\dfrac{y}{2}} \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{ - 2i\sin \left( {\dfrac{y}{2}} \right)\left( {\cos \left( {\dfrac{y}{2}} \right) + i\sin \left( {\dfrac{y}{2}} \right)} \right)}}dy}  = }$$ $$\displaystyle{ = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}\left( {\cos \left( {\dfrac{y}{2}} \right) - i\sin \left( {\dfrac{y}{2}} \right)} \right)}}{{\sin \left( {\dfrac{y}{2}} \right)}}dy}  = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}}}{{\tan \left( {\dfrac{y}{2}} \right)}}dy}  + \frac{1}{2} \cdot \int\limits_0^\pi  {{y^2}dy}  = \mathop  = \limits^{y/2 = x}  = 4i \cdot \int\limits_0^{\pi /2} {\frac{{{x^2}}}{{\tan \left( x \right)}}dx}  + \frac{{{\pi ^3}}}{6}}$$","['integration', 'calculus', 'closed-form', 'residue-calculus', 'indefinite-integrals']"
4820832,A lemma to the escape lemma (Lee' Intro to smooth manifolds Lemma 9.19),"I am trying to prove the following lemma, that is used to prove the escape lemma (Lee's Intro to smooth manifolds  Lemma 9.19) Lemma Suppose $X$ is a smooth vector field on a smooth manifold $M$ . Let $\gamma : J \to M$ a
maximum integral curve of $X$ such that $b := $ sup $(J)$ is finite. Let $t_0 \in J$ ,
and $K \subseteq M$ compact. Suppose $\gamma([t_0, b)) \subseteq K$ . Suppose $U$ and $V$ are relatively compact open subsets of $M$ such that $K \subseteq
U$ and $\bar U \subseteq V$ . Let $\psi \in C^\infty(M )$ such that $\psi|_ \bar U \equiv 1$ and supp $(\psi) ⊂ V$ . Prove that there is a $\varepsilon > 0$ such that $(t_0 − \varepsilon, b) \subseteq J$ and $\gamma|_{(t_0−\varepsilon,b)}$ an is an integral curve of $\psi X$ . So I have no idea how to start, how can I prove this ? The following proposition can be used Proposition Let $X$ be a smooth vector field on a smooth manifold $M$ . Let $p \in M$ ,
and $\gamma_p : J_p \to M$ the maximum integral curve of $X$ with $\gamma_p(0) = p$ . Let $\gamma : J \to M$ be another integral curve of $X$ with $\gamma(0) = p$ . Then $J \subseteq J_p$ and $\gamma = \gamma_p|_J$ .","['vector-fields', 'smooth-manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
4820852,Integration of rational functions. Integration techniques. Hermite-Ostragradski method.,"By the fundamental theorem of algebra a real nonconstant polynomial $Q$ has factorisation into real prime factors $$Q=g_1^{k_1}g_2^{k_2}\cdots g_l^{k_l}$$ The prime factors $g_j$ , all distinct, are first degree polynomials or second degree polynomials without real roots. The exponents are positive integers. The polynomial $Q$ is called square-free if all the exponents $k_j$ equal 1. Exercise: Let $\psi$ be square-free polynomial of degree $d$ and $P$ have degree less than $2d$ and no prime factor in common with $\psi$ . Show that the integral $\int\frac{P}{\psi^2}$ is rational if and only if $\psi$ divides $P'\psi'-P\psi''.$ My sratch work: I have noticed that $$(\frac{P}{\psi'})'=\frac{P'\psi'-P\psi''}{(\psi')^2}$$ So I decided use this pattern in the given integral bu using integration by parts : $$\int\frac{P}{\psi^2}=\int(\frac{P}{\psi'})(\frac{1}{\psi^2}\psi')=\frac{P}{\psi'}(\frac{-1}{\psi})+\int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}$$ Meanwhile the first sum in the right is rational we should care about the rightside integral. Lets consider $$\int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A}{B}$$ Where A and B are polynomials. So we get $$\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A'B-B'A}{B^2} $$ but how judge after I couldn't proceed. Maybe, another approach is using Hermite-Ostragradski method of integration. In this case the integral becomes $$\int \frac{P}{\psi^2}=\frac{P_1}{\psi}+\int \frac{P_2}{\psi}$$ The integral on the right hand side
is transcendental  function since the denominator $\psi$ is square free and the numerator has lower degree than the denominator. In order integral to be rational the rightside integral should be equal $0$ .
So we get an equality $$\int \frac{P}{\psi^2}=\frac{P_1}{\psi}$$ and diferrentiate both side : $$\frac{P}{\psi^2}=\frac{P_1'\psi-P_1\psi'}{\psi^2}$$ However I am still unable to proceed further.
Could anybody help me please.","['integration', 'indefinite-integrals', 'calculus', 'real-analysis']"
4820874,"The number of triangles in the fractal, which is formed from a square by the additional construction of isosceles right triangles","There is an algorithm for constructing a fractal: Take a square with a side of size 1; An isosceles right triangle is completed on each side; GOTO step 2. There is an example of that shape: How can I find the number of triangles in the i-th iteration of the algorithm? Initially, I thought it was just a power of 2, but the construction of the 5th and 6th generations showed that this was not the case. (Since the figure tends to the octagon and at some iterations it becomes one).","['euclidean-geometry', 'programming', 'geometry', 'fractals']"
4820905,Creating a new mean,"I was wondering: are there some necessary criteria to be respected and fulfilled for creating a new statistical mean? This question came up to my mind while studying arithmeticl mean, gometric mean and harmonic mean. I couldn't notice that, for example, geometric mean cannot accept zero or negative inputs. So I thought: let's create a new mean (in the same spirit, ish, of someone creating a new distance in mathematics, despite a distance must respect well given properties). So I thought of those trivial criteria, but I wonder if there is something necessary. Let's call $\theta$ the new mean, acting on $n$ variables. If $a, b > 0$ then $\theta(a, b) > 0$ $\theta(a, b) < a + b$ $\theta(a, b) < \max\{ a, b \}$ $\theta(a, b) > \min \{a, b \}$ Is there anything else one would expect?","['statistics', 'means', 'analysis']"
4820918,Reference for the PDF of Gamma distribution,"Is there a well-known reference in statistics to cite that state the PDF of a gamma random variable $$f(x)= \frac{1}{\Gamma{(k) \theta^k}} x^{k-1} e^{-\frac{x}{\theta}}$$ with $\text{mean}= k \theta$ and $\text{variance} = k \theta^2$ also a reference for random variable transformation for the PDF and CDF transformation of variables, for example this relation: $$f_Y(y) = f_X(g^{-1}(y)) \left| \frac{\mathrm d}{\mathrm dy} g^{-1}(y) \right|$$ I am searching for a well known and cited references for these topics.","['statistics', 'probability-distributions', 'distribution-theory', 'reference-request', 'probability']"
4820935,"If each term in a sum of positive integers divides that sum, then there must be one term that divides another.","Let $\begin{align} \{ x_i  \}_{i=1}^n \end{align}$ be a finite sequence such that $x_i\in\mathbb{N}$ . Prove that if $x_i$ divides $\sum_{i=1}^n x_i$ , $\forall{ 1\le i\le n}$ then there are $1\le j\neq h\le n$ such that $x_j$ divides $x_h$ . I know that I should avoid asking questions without trying to solve on my own, but I don't know where to start - I tried with induction but except the basis case of $n=2$ . I couldn't reach anything. Let $x_1,x_2\in\mathbb{N}$ such that $x_1|(x_1+x_2)$ and $x_2|(x_1+x_2)$ so $\exists k_1,k_2$ such that $$k_1\cdot x_1=x_1+x_2 \implies (k_1-1)\cdot x_1=x_2\implies x_1|x_2 \\k_2\cdot x_2=x_1+x_2\implies(k_2-1)\cdot x_2=x_1\implies x_2|x_1\\ \Longrightarrow x_1=x_2 $$ Probably induction won't help, but I also thought that maybe the pigeonhole principle might come in handy, but could not think about anything.
Please help","['number-theory', 'induction', 'elementary-number-theory']"
4820993,Is $\sin n/\sin m$ irrational for nonzero integers $n\neq m$?,"As shown in Is sin(x) necessarily irrational where x is rational? , the sine of any rational except for zero is transcendental. But what about the ratio of two such numbers, at least for integers? I thought about expanding $\sin n$ in terms of $\sin 1$ and $\cos 1$ , but maybe there's already been some prior work on this?","['irrational-numbers', 'integers', 'ratio', 'transcendental-numbers', 'trigonometry']"
4821013,On the diophantine equation $x^5+y^5=z^5+t^5$,"It is known that the diophantine equation $x^5+y^5=z^5+t^5$ ,
has infinity many trivial solutions.
Has this equation been proven to have other integer solutions?","['number-theory', 'algebraic-number-theory', 'diophantine-equations']"
4821017,How do we prove that $(x^2 + y^2)^2 + x^2 + y^2 < 1$ is a disk?,"How do I see that $(x^2 + y^2)^2 + x^2 + y^2 < 1$ is a disk? I plotted it in wolfram alpha and it looks like a disk, but I don't know how to show it algebraically. Even if we write in polar coordinates, $r^4 + r^2 < 1$ , I don't know why this is a disk. Isn't a disk $r^2 < 1$ ?","['multivariable-calculus', 'algebra-precalculus']"
4821034,Show $\sum_{i=0}^n{i\frac{{n \choose i}i!n(2n-1-i)!}{(2n)!}}=\frac{n}{n+1}$,"How can this identity be proved? $$\sum_{i=0}^n{i\frac{{n \choose i}i!n(2n-1-i)!}{(2n)!}}=\frac{n}{n+1}$$ I encountered this summation in a probability problem, which I was able to solve using alternative methods. However, I am curious if there is a direct way to prove this identity using algebraic or combinatorial techniques. Note. The identity was verified by Wolfram|Alpha and it agrees with the answer obtained from the other methods.","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'algebra-precalculus']"
4821052,Perturbing a measure $\mu$ so that the integral $\int fd\mu$ becomes nonzero,"Let $X$ be a compact subset of $\mathbb{R}^d$ , let $f\in L^2(X)$ be an unknown function with $\lVert f\rVert_2=1$ for which we may assume suitable regularity (e.g. Lipschitz, $C^1$ ), and let $\mu$ be a Borel probability measure on $X$ . Suppose $\int fd\mu=0$ . Is there an efficient (in an algorithmic sense, time complexity not exponential in $d$ ) way to find a perturbation $\mu'$ close to $\mu$ such that $\int fd\mu'\neq 0$ ? For example, we could divide the domain into $N^d$ hypercubes $A_1,A_2,\cdots$ and try the density proportional to $\mu+\epsilon 1_{A_i}$ until we hit a region of nonzero $f$ . Then the integral value is guaranteed to change due to Lipschitzity for large enough $N$ . However, this incurs the curse of dimensionality in $d$ . For a similar reason, random perturbations also seem to require exponential time to detect nonzero $f$ . I am unsure if this is inevitable. For context, I am studying the stability of gradient dynamics of a functional on the space of probability measures, and trying to come up with a scheme that will always find an unstable direction to escape to, if one exists. This can be quantified by the magnitude of the perturbation in the direction of an unstable eigenfunction $f$ .","['measure-theory', 'calculus-of-variations', 'functional-analysis', 'perturbation-theory', 'optimization']"
4821086,How far are the Mode and the Median of the Log-Normal distribution from behaving as Linear functions?,"How far are the Mode and the Median of the Log-Normal distribution from behaving as Linear functions? Intro_______________ Recently I made a question where later I figure out I was requiring that the Mode $\nu[x]$ of a distribution were behaving as it was a linear function $\nu\left[\sum_i^N a_iX_i\right]=\sum_i^N a_i\nu[X_i]$ , which I know is not true in general. But also, if the random variables $X_i$ belong to the same symmetrical distribution , then I will have that $\nu = \mu = m$ with "" $\mu[x]$ "" the mean value and ""m[x]"" the Median of the distribution ( $m$ is the value such it split the probabilities as $P(X\geq m) = P(X\leq m) = \frac12$ ). Since the mean value is a linear operator $\mu\left[\sum_i^N a_iX_i\right]=\sum_i^N a_i\mu[X_i]$ , under this ""symmetric"" scenario I think it should be true also that $\nu\left[\sum_i^N a_iX_i\right]=\sum_i^N a_i\nu[X_i]$ and $m\left[\sum_i^N a_iX_i\right]=\sum_i^N a_im[X_i]$ (this because $\mu$ , $\nu$ , and $m$ are homogeneous of degree $1$ ), so at least there are some conditions where they could be split as a weighted sum. Question__________________ If the variables $X_i$ belongs each to some non-necessarily identical Log-Normal distributions with some parameters $X_i \sim \text{Lognormal}(\mu_i,\ \sigma_i)$ (so each of them have their individual parameters $\nu[X_i]=\nu_i$ and $m[X_i]=m_i$ ), and for some real-valued weights $0\leq a_i\leq 1$ such as $\sum\limits_{i=1}^N a_i = 1$ , I want to know How far are from behaving as a linear function each of: $$\nu\left[\sum_{i=1}^N a_i X_i\right] \overset{?}{\approx}\sum_{i=1}^N a_i\ \nu\left[X_i\right]=\sum_{i=1}^N a_i\ \nu_i$$ $$m\left[\sum_{i=1}^N a_i X_i\right] \overset{?}{\approx}\sum_{i=1}^N a_i\ m\left[X_i\right]=\sum_{i=1}^N a_i\ m_i$$ Could we say something about the Left-Hand Sides (LHS) being always bigger/lower than the Right-Hand Sides (RHS)? Are there any inequalities setting bounds of how much spread could have the LHS from the RHS? Are there any ways of split somehow the LHS expressions? Like having a known formula? Since the Log-Normal distribution is already positively skewed (so non-symmetric): Are there any conditions under the RHS could be considered as an approximation of the LHS? PS: If you are currently a undergraduate student, I will really appreciate if you could share this question with your probability/statistic teachers. Added later After the answer by @Amir I realized that without
without assuming anything regarding independence, correlations, or unimodality, one could do the following: $$\begin{array}{r c l}
|\sum a_i m[x_i]-m\left[\sum a_i x_i\right]| & = & |\sum a_i m[x_i]+E\left[\sum a_i x_i\right]-E\left[\sum a_i x_i\right]-m\left[\sum a_i x_i\right]| \\
& \overset{\text{triangle ineq.}}{\leq} & |E\left[\sum a_i x_i\right]-m\left[\sum a_i x_i\right]|+|\sum a_i m[x_i]-E\left[\sum a_i x_i\right]| \\
& \overset{|\mu - m|\leq \sigma}{\leq} & \sqrt{\text{Var}\left[\sum a_i x_i\right]}+\left|\sum a_i\left(m[x_i]-E[x_i]\right)\right|
\end{array}$$ due the linearity of the expected value. All the values from the RHS could be taken from the individual variable distributions. From this last result could be inferred conditions for having both terms near one from each other, and also tells that the mistaken formula from this another question could be useful at last as a selection figure since it will be penalizing strongly those variables which makes both $\sum a_i m[x_i]$ and $m\left[\sum a_i x_i\right]$ drift apart (note it was done from the mode, but identical construction could be made for the median). Do you think this formula could be improved for LogNormal distributed variables without assuming independence? I already know it could be improved if assuming unimodality, but would be also better to not taking it as assumption.","['measure-theory', 'statistics', 'stochastic-processes', 'probability-theory', 'probability']"
4821088,Does ZFC -Union +UniqueUnion prove the axiom of union?,"The unique union $\mathcal{U}(F)$ is defined as $\{x \mid [\exists! A \in F](x \in A)\}$ . I saw this question earlier today, and I was wondering what one might reasonably use the unique union construction for. I think a natural first question to ask is whether ZFC -Union +UniqueUnion is equivalent to ZFC. Let $A \oplus B$ be $\mathcal{U}(\{A, B\})$ , which exists by pairing. We can define the binary union $A \cup B$ using $A \oplus B \oplus A\cap B$ , noting that $A \cap B$ is $\{x \mid x \in A \land x \in B \}$ , which exists by comprehension. By a result quoted in this answer (which I do not understand at all), it is consistent with ZFC -Union that there exist two sets $x$ and $y$ whose union does not exist. Although, by a result quoted in this other answer to the same question , it cannot be the case that $x$ and $y$ are both finite. ZFC -Union +UniqueUnion does rule out the possibility of two sets whose union doesn't exist, so it is stronger than ZFC -Union. This makes sense. It seems intuitively reasonable that the existence of the unique union is not a theorem of the other axioms. How do ZFC and ZFC -Union +UniqueUnion compare, though?",['elementary-set-theory']
4821185,Why does $\sqrt{\cos x+1} = \sqrt{2}\left|\cos\left(\frac{x}{2}\right)\right|$?,"While playing around with equations on desmos, I noticed that: $$\sqrt{\cos x+1} = \sqrt{2}\left|\cos\left(\frac{x}{2}\right)\right|$$ which may suggests an connection between the square root of a cosine function and its absolute function evaluation. Is this something that is known and/or meaningful? Has this been used for any significant proof?","['trigonometry', 'real-analysis']"
4821200,"Prove that $\exists c \in [a,b] \: \text{s.c}\: f(x)-f(a)- \frac{f(b)-f(a)}{b-a}(x-a)=\frac{(x-a)(x-b)}{2}f''(c)$","Given function $f$ differentiable on $[a,b]$ , and has derivative $f''(x)$ on $[a,b]$ , prove that for all $x\in[a,b]$ , we have at least one $c\in[a,b]$ such that $$f(x)-f(a)- \frac{f(b)-f(a)}{b-a}(x-a)=\frac{(x-a)(x-b)}{2}f''(c)$$ I tried using Rolle theorem to no avail, but I think I will need to derive something else from here. If I have $$\theta(x)=f(x)-f(a)- \frac{f(b)-f(a)}{b-a}(x-a)-\frac{(x-a)(x-b)}{2}f''(c)$$ $$\longrightarrow \theta'(x)=f'(x)- \frac{f(b)-f(a)}{b-a}-\lambda \left(x-\frac{a+b}{2}\right), \lambda = f''(c)$$ Of which still nothing came across my mind. Trying to rearrange gives $$\underbrace{\left[\frac{f(x)-f(a)}{x-a}\right]}_{(I)}-\underbrace{\left[\frac{f(b)-f(a)}{b-a}\right]}_{(II)}=\frac{(x-b)}{2}f''(x)$$ of which I see that $(I)\rightarrow f'(c_1)$ for $c_1\in [a,x]$ and $(II)=f(c_2)$ for $c_2 \in [a,b]$ . But still, there are no clue to proceed. My request here is to trying to find a way to resolve this problem, while also trying to understand the notion of what the proof really means,and what exactly is being examined here. So: How can I prove the above results? What is the meaning of the expressions and what is happening inside? What is (potentially) the geometrical intepretation and representation of the problem?","['calculus', 'functions', 'derivatives', 'real-analysis']"
4821204,Can $\ln$ be written as a ratio of polynomials?,"Is it possible that $\ln(x)=\frac{p(x)}{q(x)}$ for all $x>0,$ where $p$ and $q$ are polynomials with real coefficients? I think the answer is no. Suppose two such polynomials did exist. Take the limit as $x$ goes to infinity. This gives that $\deg(p)>\deg(q),$ as $\lim_{x\to\infty}\ln(x)=\infty.$ Let $m=\deg(p)$ and $n=\deg(q).$ Differentiate both sides to get $$\frac{1}{x}=\frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}.$$ Rearrange (this step is valid as $x>0$ and $q(x)^2>0$ by hypothesis) to get $q(x)^2=xp'(x)q(x)-xp(x)q'(x).$ Let the leading coefficient of $p$ be $a$ and the leading coefficient of $q$ be b. The leading coefficient of $xp'(x)q(x)$ then, is $amb.$ Similarly, the leading coefficient of $-xp(x)q'(x)$ is $-bna.$ Suppose their sum were $0.$ Then, $ab(m-n)=0.$ But, $ab≠0$ (as $a$ and $b$ are leading coefficients). So, $m-n=0.$ This contradicts $m>n.$ Hence, the coefficient of $x^{m+n}$ in the RHS is non-zero. Now, compare degrees to get $2n=m+n.$ This contradicts $m>n.$ Is my approach right? What other methods can we use to show this?","['contest-math', 'calculus', 'real-analysis']"
4821246,Does $A^{m} \le A^{n}$ hold for operators when $A \ge 1$ and $m \le n$?,"Let $A$ be a symmetric bounded linear operator on a Hilbert space $\mathscr{H}$ . Suppose that $A \ge 1$ , in the sense that $\langle \psi, A\psi\rangle \ge \|\psi\|^{2}$ , for all $0 \neq \psi \in \mathscr{H}$ .Question: if $m \le n$ with $m$ and $n$ being nonnegative rational numbers, does it follow that $A^{m} \le A^{n}$ in the sense that $\langle \psi, A^{n}\psi\rangle \ge \langle \psi, A^{m}\psi\rangle$ ? It seems reasonable to think the answer is yes , but I was not able to prove it myself.","['operator-theory', 'functional-analysis', 'analysis']"
4821247,The existence of a matrix $B$ with $AB=BA^c$ implies nilpotence of $A^r-I$.,"My friend and me were studying some group theory, and we thought of the following problem. Let $n$ be a positive integer, and let $A\in \text{GL}_n(\mathbb{Z})$ be a matrix with integer entries that is invertible over $\mathbb{Z}$ (i.e. $\text{det}(A)=\pm 1$ ). Question: Does the existence of a matrix $B\in \text{GL}_n(\mathbb{Q})$ and an integer $c>1$ such that $$AB = BA^c$$ imply that there exists some pair of integers $r,s>0$ such that $(A^r-I)^s=0$ ? Or equivalently that $A^r-I$ is nilpotent? This implication seems a bit too random to be true, does anyone know a counterexample (or a proof)? What we have thought of so far: The matrix $B$ needs to be invertible, as else $B=0$ and for example the matrix $$A=\left(\begin{matrix} 2 & 5 \\ 
          1 & 3\end{matrix}\right)$$ has eigenvalues that are not roots of unity, but it is actually diagonalizable, showing that $A^r-I=S(D^r-I)S^{-1}$ will never be nilpotent (its determinant is nonzero). We are not really interested in the case when $A$ is not invertible, but when $A$ has determinant $|\text{det}(A)|>1$ the equation $AB=BA^c$ will fail for any $c>1$ by taking the determinant on both sides. We will need $c>1$ , as else we may take the commuting matrices $$A = \left(\begin{matrix} 1 & 5 \\ 1 & 6 \end{matrix}\right); B= \left(\begin{matrix} 2 & 5 \\ 1 & 7 \end{matrix}\right).$$ That is, we have $AB=BA$ , but $A$ has eigenvalues that are not roots of unity, showing that $A^r-I$ will never be nilpotent for $r>0$ with the same argument as before. We have only tried $2\times 2$ matrices, as it seems incredibly difficult to check if such a matrix $B$ exists in any case. To show that there is some support for the question, we may take $$A = \left(\begin{matrix} 1 & -3 \\ 1 & -2 \end{matrix}\right)$$ and $$B= \left(\begin{matrix} -6 & -3 \\ -7 & 6 \end{matrix}\right).$$ This example has $AB=BA^2$ , the matrix $A$ is diagonalizable and the eigenvalues are roots of unity (implying that $A$ is of finite order, so $A^r-I=0$ for some $r>0$ , in this case $r=3$ ). These were the final thought we had before submitting this question: only assuming the relation $AB=BA^c$ does not imply nilpotence of $A^r-I$ for some $r>0$ as we have seen above. So we do need the invertibility of $A$ and $B$ . Moreover, somehow $A$ having determinant $\pm 1$ makes it more plausible for this to be true; the ""excess"" of the relation $AB=BA^c$ is something along the lines of $A^{c-1}$ . This is a positive power, so we think this might somehow imply that the eigenvalues of $A$ must be roots of unity. Can this imply the statement?","['matrices', 'nilpotent-groups', 'group-theory', 'linear-algebra']"
4821277,"What does ""dx"" mean in ∫𝑓(𝑥)𝑑𝑥? [duplicate]","This question already has answers here : What does multiplying the integrand by $dx$ mean in an indefinite integral? (3 answers) Closed 7 months ago . The community reviewed whether to reopen this question 7 months ago and left it closed: Original close reason(s) were not resolved I have this confusion while studying indefinite integrals. Is dx a derivative of x or a notation? If it is just a notation,how can we explain that the transformation from dx to du satisfies the operational law of differentiation?","['indefinite-integrals', 'analysis']"
4821295,"Suppose $f(x)=(\tan x)^{\frac{3}{2}}-3\tan x+(\tan x)^{\frac{1}{2}}$. Then, how can we compare the given integrals?","Let $f(x)=(\tan x)^\frac32-3\tan x+\sqrt{\tan x}$ . Consider the integrals $$I_1=\int_0^1f(x)dx$$ $$I_2=\int_{0.3}^{1.3}f(x)dx$$ $$I_3=\int_{0.5}^{1.5}f(x)dx$$ Then, prove that $I_1>I_3>I_2$ I tried out a lot of things. This question has been repeated twice at this site but no proper solution was found. Some found solutions using graphs. However, This is a question from a high school exam that does not permit the use of graphing softwares. Here are a few of my approaches that failed: Creating a function $g(x)$ such that: $$ g(x)=\int_{x}^{x+1} f(t)dt$$ and differentiate it using Newton-Leibnitz. Nothing came out.It gives $g(x)=f(x+1)-f(x)$ . The derivative of $f(x)$ does not reveal much to comment about $f(x+1)$ and $f(x)$ . Integrating the expression and finding out the primitive. I tried substituting $t=\sqrt{\tan x}$ and proceeding further using algebraic twins. I did find a primitive and here it is: $$2(\ln(\frac{\sqrt{\tan x}+\sqrt{\cot x}-\sqrt{2}}{\sqrt{\tan x}+\sqrt{\cot x}+\sqrt{2}})+ \sqrt{\tan x})-3\ln{\sec x}$$ Go ahead and put the limits and compare them if you dare. I won't stop you. If you find something out,please do tell me. Graphing by converting it into a reduced cubic and seeing if one or more areas are comparable enough.(It is a hand graph, so no use).But I got to see something interesting. I found out that the roots of the reduced cubic are $\frac{3+\sqrt{5}}{2}$ and $\frac{3-\sqrt{5}}{2}$ which can be written as $4cos^2 \frac{\pi}{10}$ and its reciprocal. But in the end the, it did not help me. This is a problem from Bstat paper of ISI $2009$ and this problem has got me drunk.","['integration', 'calculus']"
4821302,Finding the smallest positive integer $n$ such that $f_n(x)=\cos(x)\cos(2x)\cdots\cos(nx)$ satisfies $|f_n''(0)|>2023$. (Putnam 2023 A1),"Putnam 2023, Problem A1: For a positive integer $n$ , let $f_n(x)=\cos (x) \cos (2 x) \cos (3 x) \cdots \cos (n x)$ . Find the smallest $n$ such that $|f_n''(0)|>2023$ . Note: Here, $f_n''(x)$ denotes the double derivative of $f_n(x)$ . My Attempt : Observe that we can write $$f_n(x)=\cos (x) \cos (2 x) \cos (3 x) \cdots \cos (n x)=\frac{1}{2^n}. \prod \limits_{k=1}^n ({e^{ikx} + e^{-ikx}})$$ But how can I get from here that $$|f_n''(0)| = \frac{n(n+1)(2n+1)}{6} > 2023 \quad\Longrightarrow\quad n \geq 18$$ Any help would be appreciated.","['inequality', 'real-analysis', 'calculus', 'trigonometry', 'derivatives']"
4821385,Notation for orbits in Ergodic Theory,"Consider a map $T:X\to X$ from a set to itself. In ergodic theory, we say that the orbit of a point $x\in X$ is the set \begin{equation} \{x,Tx,T^2x,T^3x,\cdots\}.\end{equation} Is there any standard nomenclature for such a set? I expect there is, since for example in group theory there is notation to describe an orbit, but I can't find any ergodic theory analogue.","['measure-theory', 'ergodic-theory', 'group-actions', 'probability-theory', 'dynamical-systems']"
4821408,Vector spaces and hypercube vertices,"The following statement is taken from Kanter & Sompolinsky (1987). It is stated without a complete proof. Let $\xi_{i}^{\mu}=\pm1$ be $p$ spin vectors chosen uniformly at random in $N$ dimensions. Thus the indices $\mu=1,\dots,p$ , and $i=1,\dots,N$ . Let $\mathcal{V}_{p}$ denote the linear subspace spanned by these $p$ vectors. What is the probability that $\mathcal{V}$ contains another spin vector $\in\{\pm 1\}^{N}$ , different from the $p$ vectors $\{\xi_{i}^{\mu}\}$ and their negatives? Kanter & Sompolinsky (1987) claim that this probability vanishes like: $$P(p) \sim 4\binom{p}{3} \left( \frac{3}{4} \right)^{N}$$ as $N\to\infty$ , for all $p/N < 1 - 7\ln(2)/\ln(N)$ . They cite an unpublished paper (see references below), and give the following sketch of an argument: The origin of the result (2.18) is the fact that the dominant contribution to $P$ comes from the probability of linear combinations of three of the patterns. In order that $\{\xi_i^1,\xi_i^2,\xi_i^3\}$ span another $(\pm 1)$ vector the $N$ triplets $(\xi_i^1,\xi_i^2,\xi_i^3)$ must contain only six out of the eight different triplets, yielding the result (2.18). But I can't reconstruct a full proof. Any suggestions? References: Kanter, I., and Haim Sompolinsky. ""Associative recall of memory without errors."" Physical Review A 35.1 (1987): 380. A. Odlyzko (unpublished).","['linear-algebra', 'vector-spaces', 'discrete-mathematics']"
4821442,"Exercise 5.22 in Isaacs' ""Character Theory of Finite Groups""","Let $G$ be a finite group and let $\chi$ be a rational character of $G$ . A theorem due to Artin states that $|G| \chi$ is a $\mathbb{Z}$ -linear combination of characters of the form $(1_H){\uparrow}^G$ , where $H \leq G$ is a cyclic subgroup. It's a fact (see, for instance, exercise 5.20 of the same book by Isaacs) that the set $P(G)$ of $\mathbb{Z}$ -linear combinations of characters of the form $(1_H){\uparrow}^G$ , for $H \leq G$ (not necessarily cyclic here) is a ring. The problem I'm trying to solve is as follows: Let $n_G(\chi)$ the the smallest non-zero natural number such that $n_G(\chi)\chi \in P(G)$ and let $N = \ker \chi$ , so that $\chi$ may be considered a character of $G/N$ . Show that $n_G(\chi) = n_{G/N}(\chi)$ . The following hint is also provided: Hint: Writing $n\chi = \sum a_H (1_H){\uparrow}^G$ , show $n\chi = \sum a_H (1_{NH}){\uparrow}^G$ by obtaining $(1_H){\uparrow}^G = (1_{NH}){\uparrow}^G + \xi_H$ , where $N$ isn't contained in the kernel of any of the constituents of $\xi_H$ . My attempt Assuming the hint as given: It's trivial (from the hint) that $n_{G/N}(\chi) \leq n_G(\chi)$ . Furthermore, if $m\chi = \sum a_\overline{H} (1_\overline{H}){\uparrow}^G$ , the correspondence theorem yields $m\chi = \sum a_{NH} (1_{NH}){\uparrow}^G$ . Thus, $n_{G/N}(\chi) \geq n_G(\chi)$ and we are done. Proving the hint: I first tried to show that every constituent of $(1_{NH}){\uparrow}^G$ is also a constituent of $(1_{H}){\uparrow}^G$ . This was relatively simple: $$\langle (1_{NH}){\uparrow}^G, \eta \rangle > 0 \iff \langle 1_{NH}, \eta {\mid}_{NH} \rangle > 0 \iff \eta {\mid}_{NH} = 1_{NH} + \sum \eta_i$$ Restricting $\eta$ to $H$ , we get at least one component of $1_H$ , which gives me the result. That said: How do I know all these constituents appear in $(1_H){\uparrow}^G$ the same number of times they do in $(1_{NH}){\uparrow}^G?$ In principle, some of the $\eta_i$ may be different from $1_{NH}$ but also restrict to $1_H$ - and this can't happen if the hint is true... Also: How do I show that every constituent of $(1_H){\uparrow}^G$ containing $N$ in its kernel has to be a constituent of $(1_{NH}){\uparrow}^G$ ? The idea used before doesn't work here, since inducing $\psi{\mid}_H$ to $NH$ doesn't necessarily yield the restriction $\psi{\mid}_{NH}$ . Thanks in advance!","['group-theory', 'abstract-algebra', 'finite-groups', 'characters']"
4821468,Show $(X_n)$ is a sequence of i.i.d. random variables such that $S_n/n\to -\infty$,"$(X_n)_{n\geq 1}$ is a sequence of i.i.d. random variables such that $E{X_n}^+<\infty$ and $E{X_n}^-=\infty$ . I want to show $\frac{S_n}{n}\to -\infty$ almost surely. The problem is that I cannot apply SLLN since $E|X_1|=\infty$ . My immediate thought is that I have to construct a new sequence of i.i.d. random variables from the original sequence apply SLLN to this sequence and relate it back to what happens to the original sequence. The problem is, I am having trouble finding a good candidate sequence. Any hints would be appreciated.","['convergence-divergence', 'probability-limit-theorems', 'probability-theory']"
4821499,Is there an injective function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ that maps circles to $n$-gons?,"Fix $n\geq 3$ . Does there exist an injective function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ such that the image of any circle in $\mathbb{R}^2$ under the map $f$ is a (simple) $n$ -gon in $\mathbb{R}^2$ ? A friend of mine showed me this problem a few days ago without an answer, and we have been trying to answer it for the better part of 3 days with no luck. We don't really have an intuition about whether or not a function like this exists, but we have managed to prove some partial results. If we do not require that $f$ is an injection then the answer is positive. Let $g:\mathbb{R}^2\rightarrow\mathbb{R}$ be the projection map $g(x,y)=x$ and let $P$ be any $n$ -gon in $\mathbb{R}^2$ . Since $|P|=|\mathbb{R}|=|\mathbb{R}/\mathbb{Q}|$ , we can choose a bijection $\varphi:\mathbb{R}/\mathbb{Q}\rightarrow P$ , and define $h:\mathbb{R}\rightarrow P$ by $h(x)=\varphi(x+\mathbb{Q})$ . If we define $f=h\circ g$ , then $f(C)=P$ for every circle $C\subseteq\mathbb{R}^2$ . If we require that $f$ is continuous and injective then the answer is negative. Assume $f$ exists for a contradiction. Let $C$ be a circle in $\mathbb{R}^2$ , and let $p\in\mathbb{R}^2$ such that $f(p)$ lies on an edge (not a vertex) of $f(C)$ . Since $f$ is continuous, then its restriction $f|_C$ to the closed interior of $C$ is uniformly continuous. Take a sequence $D_m$ of circles in the tangent to $C$ at $p$ in the interior of $C$ so that $D_m$ converges to $C$ (in the Hausdorff metric). Since $f|_C$ is uniformly continuous, then $f(D_m)$ converges to $f(C)$ . Since $f(D_m)$ and $f(C)$ are $n$ -gons, it's clear that $\text{vertex}(f(D_m))$ converges to $\text{vertex}(f(C))$ . Since each $D_m$ is tangent to $C$ at $p$ , then $f(D_m)\cap f(C)=f(D_m\cap C)=\{f(p)\}$ , so since $f(p)$ is not a vertex of $f(C)$ then $f(p)$ must be a vertex of $f(D_m)$ , since otherwise $f(D_m)\cap f(C)$ would be infinite. The fact that $f(p)\in \text{vertex}(f(D_m))$ for all $m$ contradicts the fact that $\text{vertex}(f(D_m))$ converges to $\text{vertex}(f(C))$ . We also had a proof that there is no bijective $f$ , but I may have found an error as I was typing this. Anyways, this brings us no closer to figuring out our actual problem. If anyone has any ideas, we would be happy to hear them.","['general-topology', 'geometry', 'examples-counterexamples']"
4821575,Calculating the probability of a set of 4 players with at least one hand with more than D doubles (in double 6 dominoes),"I'm new when it comes to statistics, so apologies if my math is wrong or even not relatable. Assuming each player gets 7 dominoes in a field of 28. Player 1 picks 7, then player 2, and so on... I suspect, to calculate the probability of a single person receiving ""D"" doubles in ""P"" picks is: $$(\frac{7!}{(7-D)!})(\frac{21!}{(21-P+D)!})(\frac{(28-P)!}{28!})$$ When P = 7 and D = 7, I get the probability of you getting all doubles which is 1 in 1184040. I believe 28C7 is 1184040 so this appears to be right since there's only 1 hand that has all doubles. My instinct was to find the probability of 21 picks and generate the table below (P = 21) ""D"" Doubles 1 in # Hands 7 1184040 6 169148.6 5 56382.86 4 33829.71 3 33829.71 2 56382.86 1 169148.6 0 1184040 I find the data a bit suspicious. I find it strange that it is as likely for you to be left with a hand of non-doubles after the three players have picked theirs, as it is to be left with the only set of doubles. While I think the calculation makes sense, I'm not 100% sure I'm getting the calculation I want. As the title states I'm looking for the probability of at least one player getting D doubles.","['statistics', 'probability']"
4821604,What are the chances of finding all the balls?,"Given $x$ distinguishable balls (say they have different colors), sample with replacement repeatedly until all the balls that have been sampled have been sampled at least twice.  I am interested in $$P(\text{number of distinct balls sampled} = x).$$ That is, the probability that you would have seen all the balls when you stop. You can of course stop before then, if for example you sample the same ball the first two times and $x > 1$ . @leonbloy gives the different values of $x$ from $2$ to $20$ : 2   0.5
3   0.4444444
4   0.4479167
5   0.4689333
6   0.4958611
7   0.5240295
8   0.5512568
9   0.5765074
10  0.5993544
11  0.6197125
12  0.6376863
13  0.6534778
14  0.6673292
15  0.6794889
16  0.6901919
17  0.6996503
18  0.7080495
19  0.7155483
20  0.7222809 Calculating by hand, the correct probability for $x = 3$ is in fact $4/9$ . Bounty Is it possible to compute what the probabilities are in general, either exactly or at least what they are asymptotic to, and do they tend to $1$ as $x$ tends to infinity?",['probability']
4821605,Is there a well-ordering of the reals whose initial segments are all measurable?,"Pretty much the question in the title: can there be a well-ordering the reals $\langle r_\alpha\mid \alpha<\kappa \rangle$ , all of whose initial segments $I_\beta := \{r_\alpha\mid \alpha<\beta<\kappa\}$ are Lebesgue-measurable? Observation: if the Continuum Hypothesis holds, then any well-ordering of the reals in ordertype $\omega_1$ will have this property. So it's consistent that such a well-ordering exists.","['measure-theory', 'lebesgue-measure', 'set-theory', 'real-analysis']"
4821639,Does central limit theorem hold for random stopping times?,"Let $\{X_k\}_{k\geq 1}$ be a sequence of iid $L^2$ random variables. Let $\mathcal{A}_k$ be defined as the pullback algebra of the random vector $(X_1,X_2,...,X_k)$ for every positibve integer $k$ . For every $n\geq 1$ , let $\tau^n:\Omega\rightarrow \{1,2,3,...\}$ be a stopping time with respect to the filtration $\mathcal{A}_1\subseteq \mathcal{A}_2\subseteq\mathcal{A}_3\subseteq...$ . It is also given that $\lim_{n\rightarrow \infty}\tau^n=\infty$ . Define $S_k$ to be $X_1+X_2+...+X_k$ . Question: Must the random variable $$\frac{S_{{\tau}^n}-E(S_{{\tau}^n})}{\sqrt{VAR(S_{{\tau}^n})}}$$ converge to the standard normal distribution as $n\rightarrow \infty$ ? If answer is yes, does the answer remain yes if the hypothesis that $\tau^n$ is a stopping time is omitted but the hypothesis that $\tau^n$ diverges as $n\rightarrow \infty$ is kept","['stopping-times', 'probability-theory', 'central-limit-theorem']"
4821684,Inequality appearing at theorem 1.3.5 in Hormander's book,"Theorem 1.3.5 p19 in ""the analysis of linear partial differential operators
I"" by Hormander states that for any positive sequence $a_{0}\geq a_{1}\geq
\cdots $ such that $a=\sum_{k=0}^{+\infty }a_{k}<+\infty $ , if $%
u_{k}=H_{a_{0}}\ast \cdots \ast H_{a_{k}}$ (where $H_{c}(x)=c^{-1}$ when $%
0\leq x\leq c$ and $H_{c}(x)=0$ otherwise), then $u_{k}\in $ $%
C_{0}^{k-1}\left(\mathbb{R}
\right) $ has support in $\left[ 0,a\right] $ and converges as $k\rightarrow
+\infty $ to a function $u\in C_{0}^{\infty }\left( 
\mathbb{R}\right) $ with support in $\left[ 0,a\right] $ such that $\int udx=1$ and $%
\left\vert u^{(k)}(x)\right\vert \leq 2^{-1}\int \left\vert
u^{(k+1)}(x)\right\vert dx\leq 2^{k}(a_{0}...a_{k})^{-1},k=0,1,\ldots $ In the proof of the theorem, the author shows clearly that $\left\vert
u^{(k)}(x)\right\vert \leq 2^{k}(a_{0}...a_{k})^{-1}$ and $2^{-1}\int
\left\vert u^{(k+1)}(x)\right\vert dx\leq 2^{k}(a_{0}...a_{k})^{-1}$ , but,
if I am not wrong, he doesn't give any hint for the inequality $\left\vert
u^{(k)}(x)\right\vert \leq 2^{-1}\int \left\vert u^{(k+1)}(x)\right\vert dx$ . I have tried to state it myself but all I have found is the inequality : $ \int \left\vert u^{\left( k\right)}\left(x\right)-u^{\left( k\right)}\left(x-a_{k}\right)\right\vert dx  \leq 2^{-1}\int \left\vert u^{(k+1)}(x)\right\vert dx$ and nothing further. Please help me clarify this point, as I am totally stuck. Thanks in advance.","['proof-explanation', 'inequality', 'functional-analysis', 'convolution']"
4821691,Six touching circles inside a seventh imply $a+b+c=r$,"This is from another question which I started answering but which has been closed before I could finish my answer. Right now there is still a gap in my answer, so it is now my turn to ask about this problem in an attempt to fill that gap. We have a ring of six circles with radii $a,b,c,a,b,c$ , each of them touching both the previus and the next one from the outside. They also all touch the inside of a circle of radius $r$ . Show that $a+b+c=r$ . The original question described this as a sangaku, so I'll keep that tag. But I don't know a source where this came from. (Update: See comment from brainjam for a source .) Below I'll show my approach for how to prove this fact, but there is one gap that I don't have a good handle for. Also, my approach used a lot of computer algebra, so there is a high chance someone has a more intuitive approach which would be better suited to a paper and pencil computation, or perhaps even avoid computations altogether. The whole configuration has to be point symmetric with respect to the center of the circumcircle. Let's use $O$ to denote that center, $A,B,C$ for the centers of the first three circles, and $A',B',C'$ for those of their point reflections in $O$ . Define $\alpha:=\measuredangle AOB, \beta:=\measuredangle BOC, \gamma:=\measuredangle COA'$ . Due to the point symmetry, $\alpha+\beta+\gamma=\pi$ . As we want to use the cosine rule eventually, let's first use angle sum formulas to rewrite this in terms of trigonometric functions. \begin{align*}
\alpha+\beta+\gamma&=\pi \\
\cos(\alpha+\beta+\gamma) &= -1 \\
\cos(\alpha+\beta)\cos\gamma-\sin(\alpha+\beta)\sin\gamma &= -1 \\
\cos\alpha\cos\beta\cos\gamma-\sin\alpha\sin\beta\cos\gamma
-\sin\alpha\cos\beta\sin\gamma-\cos\alpha\sin\beta\sin\gamma &= -1
\end{align*} Now the sines are in the way here. So let me combine $\sin^2\alpha+\cos^2\alpha=1$ with the equation above to eliminate $\sin\alpha$ , then do the same with $\beta$ and $\gamma$ . I used Sage and resultants for this elimination. The result I got was some polynomial of total degree $12$ in the cosines of the angles, but factoring it returned just a single factor with exponent $4$ which represents the following equivalent equation: $$2\cos\alpha\cos\beta\cos\gamma + \cos^2\alpha+\cos^2\beta + \cos^2\gamma = 1$$ Now we can apply the cosine rule to find a relation between these cosines of the angles and the radii of the circles. That is because of the triangles seen in the figure above, the edges of which are sums or differences of radii. \begin{align*}
(a+b)^2 &= (r-a)^2 + (r-b)^2 - 2(r-a)(r-b)\cos\alpha \\
(b+c)^2 &= (r-b)^2 + (r-c)^2 - 2(r-b)(r-c)\cos\beta \\
(c+a)^2 &= (r-c)^2 + (r-a)^2 - 2(r-c)(r-a)\cos\gamma
\end{align*} I can use the first of these to eliminate $\cos\alpha$ , the second to eliminate $\cos\beta$ and the third to eliminate $\cos\gamma$ , again using resultants. When I factor the resulting degree $12$ polynomial I get: $$
(a + b + c - r)\cdot r^2\cdot (a - r)^2 \cdot (b - r)^2 \cdot (c - r)^2 \cdot (4abc - ar^2 - br^2 - cr^2 + r^3) = 0
$$ The first of these factors is the condition $a+b+c=r$ we are trying to prove. So we need to show that all the other factors are non-zero to conclude that the first one has to be zero. This will hopefully boil down to some reasonable non-degeneracy assumptions. $r\neq 0$ is reasonable to assume since otherwise we don't have a proper circumcircle anyway. $r\neq a, r\neq b, r\neq c$ follow from the assumption that the inner circles are actually smaller, so their radius can't be equal to that of the circumcircle. But how do we know $4abc - ar^2 - br^2 - cr^2 + r^3\neq 0$ ? Why can we rule that out? Does that term have any intuitive geometric interpretation? Also, do you have any alternative approaches to suggest for this whole problem? Update: Rewriting that last factor as $4abc - (a+b+c-r)r^2$ (as some of the proposed answers did) we can see that if the assumption holds, that parenthesis becomes zero and the whole term becomes positive. So we might want to prove that $4abc > (a+b+c-r)r^2$ holds without building that proof on the assumption $a+b+c=r$ . Perhaps some clever chain of well-argued inequalities can achieve that. I'm convinced that $\frac r6\le a,b,c\le\frac r2$ but my reasoning for the lower bound currently involves the assumption we try to prove, and anyway doesn't lead to the desired inequality in an obvious way. Update 2: After reading the answers it becomes clear that $a+b+c=r$ can be concluded if we assume that the six inner circles don't overlap, don't have any inner points common to two of them. That condition obviously holds in the example figure, but hasn't been explicitly stated in my original wording of the problem statement.","['sangaku', 'geometry']"
4821709,Conjecture: If $\sum_{i=1}^n \frac{1}{x_i}=1$ then $x_i | x_j$,"Let $x_1,x_2,x_3,\cdots , x_n\in\mathbb{N}$ prove that if $\sum_{i=1}^n\frac{1}{x_i}=1$ , then there exist $x_j,x_i$ such that $x_j | x_i$ .
I read that I should avoid the no clue questions, but this is a conjecture I came up with and need to know if it's true to continue my work on some project. All I could figure out is that it's true for $n=2$ - and then $x_1=x_2=\frac{1}{2}$ Edit The user @deif proved the conjecture if there exist atleast one $x_i$ which is prime.
So to prove the conjecture entirely(if it's true) we just need to prove it with the assumption that $x_1,x_2,\cdots ,x_n$ are not primes!","['number-theory', 'conjectures', 'egyptian-fractions']"
4821771,Finding flux of a vector field through a hemisphere [duplicate],"This question already has an answer here : Stokes theorem normalization (1 answer) Closed 7 months ago . This question is given in a (publicly shared) past exam at my university: Let S be the upper hemisphere of $x^2 + y^2 + z^2 = 4$ with normal vector pointing toward the origin, and $\vec F = z \vec x / |\vec x|$ where $\vec x = <x, y, z>$ . Compute $\iint\vec F \cdot d\vec S $ The answer is given as $-8\pi$ ; I understand that it can be obtained by parameterizing the sphere and computing $\vec t_u \times \vec t_v = <x/z, y/z, 1>$ as the normal vector where $z = \sqrt{4 - x^2 - y^2}$ , then computing the integral. However, I am confused by this as I have read that the flux across a sphere can be obtained using $\vec r / | \vec r |$ as the normal, where $\vec r = <x, y, z>$ . This results in a different answer, albeit not far off from $-8\pi$ . Which is the proper normal to use in a flux surface integral where S is a sphere?","['multivariable-calculus', 'calculus']"
4821776,"How to prove $f \in L^p(0,1)$","Let $f \ge 0$ , $f\in L^1(0,1)$ , monotone decrease. Suppose $\forall a,x \in (0,1) $ such that $0<x-a<x+a<1$ , $$\int_{x-a}^x f(t)dt<\frac{5}{4} \int_{x}^{x+a}f(t)dt.$$ Prove $f\in L^p(0,1)$ , $\forall 1\le p\le \frac{\ln 2}{\ln 3-\ln 2}$ . I can get a weaker bound $1\le p<\frac{\ln 2}{\ln25 - \ln16}$ by dividing the interval of integration by $[2^{-n},2^{-n+1}]$ and getting $f(2^{-n})<\frac{25}{16}f(2^{-n+1})$ , but I don't know how to get a finer bound. My attempt, $$
2^{-n}f(2^{-n})<\int_0^{2^{-n}} f(t)dt<\frac{5}{4}\int_{2^{-n}} ^{2^{-n+1}} f(t)dt<\frac{25}{16}\int_{2^{-n+1}} ^{2^{-n+2}} f(t)dt<2^{-n}f(2^{-n+1}),
$$ thus, $$f(2^{-n})<\frac{25}{16}f(2^{-n+1})<\left(\frac{25}{16}\right)^nf(1).$$ Then, $$
\int_0^1 f(t)^p<\sum_{n=0}^{+\infty}2^{-n}f(2^{-n})^p<f(1)^p\sum_{n=0}^{+\infty}\left(\frac{25}{16}\right)^{np}\cdot2^{-n},
$$ if $\frac{1}{2}\left(\frac{25}{16}\right)^p<1$ , we can get $\int_0^1 f(t)^p<+\infty$ .","['analysis', 'real-analysis']"
4821904,Prove that the general formula for a sequence $a_n$ is $\frac{(-1)^n}{n!}$,"Here is a sequence $a_n$ where the first five $a_n$ are: $a_1=-\frac{1}{1!}$ $a_2=-\frac{1}{2!}+\frac{1}{1!\times1!}$ $a_3=-\frac{1}{3!}+\frac{2}{2!\times1!}-\frac{1}{1!\times1!\times1!}$ $a_4=-\frac{1}{4!}+\frac{2}{3!\times1!}+\frac{1}{2!\times2!}-\frac{3}{2!\times1!\times1!}+\frac{1}{1!\times1!\times1!\times1!}$ $a_5=-\frac{1}{5!}+\frac{2}{4!\times1!}+\frac{2}{3!\times2!}-\frac{3}{3!\times1!\times1!}-\frac{3}{2!\times2!\times1!}+\frac{4}{2!\times1!\times1!\times1!}-\frac{1}{1!\times1!\times1!\times1!\times1!}$ For each term in $a_n$ ​: 1.The denominator enumerates all combinations that sum to $n$ . 2.The numerator represents the number of permutations possible for that combination. 3.If the number of elements in the combination is odd, the term is negative. I have computed the first five $a_n$ mentioned above, and it appears that $a_n=\frac{(-1)^n}{n!}$ ​. However, I am struggling to provide a proof for this formula. Could someone please assist me?","['integer-partitions', 'combinatorics', 'sequences-and-series']"
4821909,The Gaussian width of the unit ball,"The Gaussian width of a subset $T \subseteq\mathbb{R^n}$ is defined as follows. $$w(T)=\mathbb{E} \hspace{0.04 in} \sup \langle g,x\rangle$$ where $g$ has the distribution $\mathcal N(0,I_{n})$ and the $\sup$ is taken over all $x$ in $T$ . Now, I want to show that $$w(B_{1}^{n})= \mathbb{E}\ \lvert\lvert g \rvert\rvert_{2}$$ where $B_{1}^{n}$ is the unit ball. I am not good at probability so my idea might be completely wrong. I tried to use Cauchy-Schwarz, but we should have the equality form of that inequality to get the result, and $g$ is a random vector so the linearly dependence of $g$ and $x$ is not meaningful, to my understanding.",['probability-theory']
4821941,Let $A$ be an abelian group of order $2^{100}$. Prove that $A$ is not a subgroup of $S_n$ for $n<200$.,Let $A$ be an abelian group of order $2^{100}$ . Prove that $A$ is not a subgroup of $S_n$ for $n<200$ . I was thinking of showing that there is no subgroup of order $2^{200}$ by using an argument using lagrange to solve this question by I can't find a way to verify how many $2$ s are there in $200!$ and also this is a stronger claim so it seems unprobable. Does anybody have some other ideas? Edit: I realized we are dealine with $2^{100}$ I made a mistake the question is about order $2^{100}$,"['symmetric-groups', 'abelian-groups', 'group-theory', 'finite-groups']"
4821952,System of integral equations describing probability,"During my work on my thesis, I've stumbled upon the following problem: Let $f_1$ and $f_2$ be some arbitrary PDFs with support $\mathbb{R}$ . Does there exist a joint bivariate distribution $f_r(x, y)$ , such that the marginal distribution of both $X, Y \sim f_r$ is $f_1$ ,and the distribution of $X+Y$ is $f_2$ ? Writing this problem down as a system of integral equations, I've come up with this : $$
\begin{cases} 
f_1(x) = \int_{-\infty}^{\infty} f_r(x, y) dy \\
f_1(y) = \int_{-\infty}^{\infty} f_r(x, y) dx \\
f_2(z) = \int_{-\infty}^{\infty} f_r(t, z-t) dt
\end{cases} $$ However, I don't have sufficient knowledge in solving PDEs to move this system anyway forwards. If it is not solvable in general, I would be interested in the solution for some example distributions, e. g. normal or Cauchy. Numerical solution would also be sufficient. Please note that we cannot assume the independence of $X$ and $Y$ , since then $f_r$ uniquely determines all marginals. If some additional requirements on $f_1,f_2$ need to be assumed, it's allowed. EDIT: thanks to the comment of @geetha290krm, we must assume that $\mathbb{E}_{f_2} (X) = 2\mathbb{E}_{f_1} (X)$","['systems-of-equations', 'integral-equations', 'probability-theory', 'partial-differential-equations']"
4821959,Exercise about Brownian motion,"Consider a probability space $(\Omega, \mathcal{F}, \mathbf{P})$ ,a standard(one-dimensional) Brownian motion or Wiener process ,denoted by $B=\{B_t:t\in [0,\infty)\}$ ,is a stochastic process on $(\Omega, \mathcal{F}, \mathbf{P})$ satisfying the following properties: 1 . $\mathbf{P}(\{\omega:B_0(\omega) = 0\}) = 1.$ 2. For every choice of nonnegative real numbers $0 \le t_1<t_2<\cdots <t_{n-1}<t_{n}<\infty$ , the increments $B_{t_1}-B_{t_0}, B_{t_2}-B_{t_2},..., B_{t_n} - B_{t_{n-1}}$ are mutually independent random variables.( $B$ has independent increments ) 3. For each $0\le s<t<\infty$ ,the increment $B_t - B_s\sim\mathcal{N}(0,t-s).$ ( $B$ has stationary increments ) 4. $\mathbf{P}(\{\omega: t \mapsto B_t(\omega) \textrm{ is not continuous at each $t$}\}) = 0.$ $\textbf{Exercise.}$ Show that $\textbf{(a)}$ .For any $\delta\in\mathbb{R}^{+},\Gamma\in\mathscr{B}(\mathbb{R}),$ $$\mathbf{P}\left(\underset{u\in [s,t]}{\text{inf}}B_{u}\in \Gamma \bigg| B_{t}=\alpha,B_{s}=\beta \right)=\mathbf{P}\left(\underset{u\in [s+\delta,t+\delta]}{\text{inf}}B_{u+\delta}\in\Gamma \bigg| B_{t+\delta}=\alpha,B_{s+\delta}=\beta\right),$$ where $0\le s<t<\infty;\alpha,\beta\in \mathbb{R}.$ $\textbf{(b)}$ .For any $\Delta\in\mathbb{R},\Gamma\in\mathscr{B}(\mathbb{R}),$ $$\mathbf{P}\left(\underset{u\in [s,t]}{\text{sup}}B_{u}\in \Gamma \bigg| B_{t}=\alpha,B_{s}=\beta \right)=\mathbf{P}\left(\underset{u\in [s,t]}{\text{sup}}B_{u}\in\Gamma+\Delta\bigg| B_{t}=\alpha+\Delta,B_{s}=\beta+\Delta\right),$$ where $0\le s<t<\infty;\alpha,\beta\in \mathbb{R}.$ Below are some of my own thoughts： Fixed $0\le t<u<s<\infty$ and $\alpha ,\beta\in \mathbb{R}$ , the conditional distribution of $B_{u}$ given $B_{s}=\alpha,B_{t}=\beta$ is normal: $$\left(B_u\bigg|  B_s = \alpha, B_t = \beta\right)\sim \mathcal{N}\left(\alpha+(\beta-\alpha)\frac{u-s}{t-s},\frac{(t-u)(u-s)}{t-s}\right)$$ From above, ${\color{Blue} {\textsf{the time-homogeneous property of $B$}}}$ and ${\color{Blue} {\textsf{the spatially homogeneous property of $B$ }}}$ acquired without much effort. $\textbf{(c)}.{\color{Blue} {\textsf{The time-homogeneous property of $B$}}}$ : For any $\delta\in\mathbb{R}^{+},\Gamma\in\mathscr{B}(\mathbb{R}),$ $$\mathbf{P}\left(B_{u}\in \Gamma \bigg| B_{t}=\alpha,B_{s}=\beta\right)=\mathbf{P}\left(B_{u+\delta}\in \Gamma \bigg| B_{t+\delta}=\alpha,B_{s+\delta}=\beta\right),$$ where $0\le s<u<t<\infty;\alpha,\beta\in \mathbb{R}.$ $\textbf{(d)}.{\color{Blue} {\textsf{The spatially homogeneous property of $B$ }}}$ : For any $\Delta\in\mathbb{R},\Gamma\in\mathscr{B}(\mathbb{R}),$ $$\mathbf{P}\left(B_{u}\in \Gamma \bigg| B_{t}=\alpha,B_{s}=\beta\right)=\mathbf{P}\left(B_{u}\in \Gamma+\Delta \bigg| B_{t}=\alpha+\Delta,B_{s}=\beta+\Delta\right),$$ where $0\le s<u<t<\infty;\alpha,\beta\in \mathbb{R}.$ $\\$ I want to deduce $\textbf{(a),(b)}$ from $\textbf{(c),(d)}$ respectively, but how to proof that for each $u\in \mathcal{U}$ (an uncountable set in $[0,\infty)$ ) have the same conditional distributions,then those ""infimum & supremum"" also have identical conditional distribution? By the way,I speculate that a random process $X=\{X_t:t\in [0,\infty)\}$ (not limited to Brownian motion) with stationary independent increments ,then $\textbf{(c)}$ and $\textbf{(d)}$ are very likely to be true,(however,the validity of $\textbf{(a)} $ and $\textbf{(b)}$ still requires further confirmation).","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4821979,"If a deck of 54 cards (including 2 jokers) is evenly split into 3 groups of 18, what is the probability that any one group contains both jokers?","I was just asked this interview question on combinatorics: A deck of 54 cards (includes 2 jokers) are split into 3 equal groups
of 18. What is the probability of any single group having both jokers? I only had time to think of a solution post-interview, but please check whether my answer is correct: $$
P = \frac{\text{Ways to form group with 2 jokers} \times \text{Ways to form 1st non-joker group} \times \text{Ways to form 2nd non-joker group}}{\text{Ways to form 3 equal groups}}
$$ $$
P = \frac{{2 \choose 2}{52 \choose 16} \times {36 \choose 18} \times {18 \choose 18}}{{54 \choose 18}{36 \choose 18}{18 \choose 18} / 3!}
$$ I'm not sure whether I need to divide by $3!$ in the denominator, since it seems the numerator will also have this and it cancels out?","['combinatorics', 'card-games', 'probability']"
4821986,Determine stability of non-hyperbolic stationary point,"Given the system $$\begin{align*} \dot{x_1} &= x_2+x_1^2-x_1^3 \\
\dot{x_2} &= -x_2+\mu x_1^2
\end{align*} $$ determine the stability of the stationary point in the origin for $\mu = \{-1,0, 1\}$ . Attempt First step of determining stability is to find the Jacobian-matrix: $$\mathbf{D}\mathbf{f}(x,y) = \begin{bmatrix} 2x_1-3x_1^2 & 1 \\
2\mu x & -1 \end{bmatrix} $$ Next, we determine the eigenvalues of the Jacobian-matrix evaluated in the origin. $$\mathbf{D}\mathbf{f}(0,0) = \begin{bmatrix} 0 & 1 \\
0 & -1 \end{bmatrix} \Rightarrow \text{eigenvalues}: \lambda=\{0,-1\}$$ To me, it seems the eigenvalues in the origin are independent of $\mu$ . Furthermore, the origin is non-hyperbolic because one of the eigenvalues are $0$ . Question How do I determine the stability of the origin why one of the eigenvalues are $0$ , and how can I see the dependence of $\mu$ on the stability. Is it possible to solve this problem via heuristics instead of by computation?","['jacobian', 'stability-theory', 'stationary-point', 'ordinary-differential-equations']"
4822007,Conjecture about integer partitions,"I formulated this conjecture after reading this related question . Let $\mathcal{P}(n) = \{P_1(n), P_2(n), \ldots \}$ be the set of all integer partitions of a positive integer $n$ , and $p(n)=\vert \mathcal{P}(n) \vert$ the number of those partitions. Note that the $P_k(n)$ are multisets, i.e. elements can be repeated. Let $f(P_k(n))$ be the number of distinct permutations of the elements of $P_k(n)$ . Is it true that for $n \ge 2$ : $$\sum_{k=1}^{p(n)} f(P_k(n))(-1)^{\vert P_k(n) \vert} = 0 \space ?$$ I verified it for $2 \le n \le 6$ , while it evaluates to $-1$ for $n = 1$ . EDIT 1 Searching the OEIS and precisely the comments section of OEIS A111786 it seems this is a well known fact. I haven't found the proof yet... EDIT 2 For example, for $n=4$ , $\mathcal{P}(4) = \{\{4\},\{3,1\},\{2,2\},\{2,1,1\},\{1,1,1,1\}\}$ , $f(\{4\})(-1)=-1$ , $f(\{3,1\})(-1)^2=2$ , $f(\{2,2\})(-1)^2=1$ , $f(\{2,1,1\})(-1)^3=-3$ , $f(\{1,1,1,1\})(-1)^4=1$ .","['integer-partitions', 'combinatorics']"
4822026,"Problem 3-32 in ""Calculus on Manifolds"" by Michael Spivak. What are ""considerably weaker hypotheses""?","Problem 3-32. Let $f:[a,b]\times [c,d]\to\mathbb{R}$ be continuous and suppose $D_2f$ is continuous. Define $F(y)=\int_a^b f(x,y) dx$ . Prove Leibnitz's rule: $F'(y)=\int_a^b D_2f(x,y) dx$ . Hint: $F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx$ . (The proof will show that continuity of $D_2f$ may be replaced by considerably weaker hypotheses.) I solved this problem as follows. Since $D_2f$ is continuous on $[a,b]\times [c,d]$ , $D_2f$ is uniformly continuous on $[a,b]\times [c,d]$ . So, for arbitrary positive real number $\varepsilon$ , there is a positive real number $\delta$ such that $(x_1,y_1)\in [a,b]\times [c,d]$ and $(x_2,y_2)\in [a,b]\times [c,d]$ and $|(x_1,y_1)-(x_2,y_2)|<\delta\implies |D_2f(x_1,y_1)-D_2f(x_2,y_2)|<\frac{\varepsilon}{b-a}$ . By Fubini's theorem, $F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C$ , where $C$ is some real number. Let $(x,y)\in [a,b]\times [c,d]$ . Let $z\in [c,d]$ and $|z-y|<\delta$ . Then $|(x,z)-(x,y)|=|z-y|<\delta$ . So, $|D_2f(x,z)-D_2f(x,y)|<\frac{\varepsilon}{b-a}$ . So, $\left|\int_a^b D_2f(x,z)dx-\int_a^b D_2f(x,y)dx\right|\leq\int_a^b \left|D_2f(x,z)-D_2f(x,y)\right|dx<\int_a^b \frac{\varepsilon}{b-a}dx=\varepsilon$ . So, $G$ such that $G(y):=\int_a^b D_2f(x,y)dx$ is continuous on $[c,d]$ . So, $F(y)=\int_c^y G(y)dy+C$ is differentiable at $y\in [c,d]$ and $F'(y)=G(y)=\int_a^b D_2f(x,y)dx$ . The author wrote ""The proof will show that continuity of $D_2f$ may be replaced by considerably weaker hypotheses."". What are ""considerably weaker hypotheses""? My attempt: Is there a function $f:[a,b]\times [c,d]\to\mathbb{R}$ which satisfies the following conditions? $D_2f$ exists on $[a,b]\times [c,d]$ . $D_2f$ is integrable on $[a,b]\times [c,d]$ . $D_2f$ is not continuous on $[a,b]\times [c,d]$ . For any $\varepsilon>0$ and any $y\in [c,d]$ , there is $\delta>0$ such that $|D_2f(x,z)-D_2f(x,y)|<\varepsilon$ if $x\in [a,b]$ and $z\in [c,d]$ and $|y-z|<\delta$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto f(x,y)\in\mathbb{R}$ is integrable on $[a,b]$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto\int_c^y D_2f(x,y)dy\in\mathbb{R}$ is integrable on $[a,b]$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto D_2f(x,y)\in\mathbb{R}$ is integrable on $[a,b]$ . Suppose such $f$ exists. By 5, $F(y)=\int_a^b f(x,y)dx$ exists for any $y\in [c,d]$ . By 4, $\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx$ . By 6 and 5, $\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C$ , where $C$ is some real number. By 2 and 4 and 7 and Fubini's theorem, $\int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C$ . By 4, $G$ such that $G(y):=\int_a^b D_2f(x,y)dx$ is continuous on $[c,d]$ . So, $F(y)=\int_c^y G(y)dy+C$ is differentiable at $y\in [c,d]$ and $F'(y)=G(y)=\int_a^b D_2f(x,y)dx$ . But by 3, $D_2f$ is not continuous on $[a,b]\times [c,d]$ .","['integration', 'multivariable-calculus', 'fubini-tonelli-theorems', 'partial-derivative']"
4822043,How can I show that this difference-differential equation has a unique solution?,"Consider the following difference-differential equation $$ (1+\alpha t)y_k'(t) - (k\beta+r) y_k(t) = y_{k-1}(t) $$ with $\alpha,\beta,r\in\mathbb R$ and the initial conditions $y_k(0)=0$ for $k\geq1$ and $y_0(t)=(1+\alpha t)^{r/\alpha}$ . In a paper I'm currently reading it is said, that this differential equation has a unique solution under the above conditions and I'm wondering why. If I could rewrite the equation to $y_k'(t)=f(t,y_k)$ I could probably use the existence and uniqueness theorem, unfortunately the equation also contains $y_{k-1}(t)$ so I can just rewrite the equation as $$ y_k'(t) = f(t,y_k(t),y_{k-1}(t)). $$ How can I show that there exists a unique solution to this type of differential equation?","['initial-value-problems', 'ordinary-differential-equations']"
4822044,"Problem 3-33 in ""Calculus on Manifolds"" by Michael Spivak. What is $g$? I cannot understand the author's intention for this problem at all.","Problem 3-33. If $f:[a,b]\times [c,d]\to\mathbb{R}$ is continuous and $D_2f$ is continuous, define $F(x,y)=\int_a^x f(t,y)dt$ . (a) Find $D_1F$ and $D_2F$ . (b) If $G(x)=\int_a^{g(x)} f(t,x)dt$ , find $G'(x)$ . I solved (a) as follows: $D_1F(x,y)=f(x,y)$ by a famous theorem in one variable calculus. $D_2F(x,y)=\int_a^x D_2f(t,y)dt$ by Problem 3-32 . But I could not solve (b). At first, what is $g$ ? I guess $g$ is a function from $[c,d]$ to $[a,b]$ and $g$ is differentiable on $[c,d]$ . $G(x)=F(g(x),x)$ . $G'(x)=D_1F(g(x),x)\cdot g'(x)+D_2F(g(x),x)=f(g(x),x)\cdot g'(x)+\int_a^{g(x)} D_2f(t,x)dt$ . I cannot understand the author's intention for this problem at all.","['integration', 'multivariable-calculus', 'partial-derivative']"
4822069,What is the probability to get a value of 11 or 12 in 10 rolls or less?,"Suppose I have a 3-sided die with each side contains different number of 1, 2, and 3. In this problem, each time I roll the die I will add up the number that shows up with previous rolls. The target number I need is either 11 or 12. Once it reaches either of those number I won't roll the die again. What is the probability to get a value of 11 or 12 in 10 rolls or less? I tried to calculate myself but the result is more than 100%. Sum of 11 or 12 in 4 rolls: 0,061728395
Sum of 11 or 12 in 5 rolls: 0,308641975
Sum of 11 or 12 in 6 rolls: 0,366255144
Sum of 11 or 12 in 7 rolls: 0,195244627
Sum of 11 or 12 in 8 rolls: 0,057613169
Sum of 11 or 12 in 9 rolls: 0,010211858
Sum of 11 or 12 in 10 rolls: 0,001100781
Total: 1,000795949","['dice', 'combinatorics', 'probability']"
4822121,Approximating $\sqrt{x}$ by a rational function in the complex plane,"Newman (1963) proved the following. Theorem 1. Let $d \in \mathbb{N}$ . Define $$p(x) = \prod_{k=0}^{d-1} \left(x+\exp\left(\frac{-k}{\sqrt{d}}\right)\right)$$ and $$r(x) = \frac{\sqrt{x} \cdot (p(\sqrt{x}) - p(-\sqrt{x}))}{p(\sqrt{x}) + p(-\sqrt{x})}.$$ Then $r(x)$ is a rational function of degree $\lceil d/2 \rceil$ with real coefficients. (Specifically, $r(x)$ is the ratio where the numerator is a polynomial in $x$ of degree $\lceil d/2 \rceil$ and the denominator is a polynomial in $x$ of degree $\lfloor d/2 \rfloor$ .)
Furthermore, $$\sup_{x \in [0,1]} |r(x)-\sqrt{x}| \le 3 \cdot \exp(-\sqrt{d}).$$ Subsequent work has obtained optimal constants and generalized to powers other than the square root;  see Stahl (1993) and references therein. I'm interested in extending to the complex plane. I.e., rather than $x \in [0,1]$ , I want the approximation guarantee to hold for all $x \in \mathbb{C}$ with $|x-\frac12|\le\frac12$ .
The following gives a specific strong conjecture. Conjecture 2. There exists a universal constant $c>0$ such that the following holds.
Let $d \in \mathbb{N}$ .  There exists a rational function $r(x)$ of degree $\le d$ with real coefficients such that $$\sup \left\{ \left| r(x) - \sqrt{x} \right| :  x \in \mathbb{C}, \left|x-\frac12\right|\le\frac12 \right\} \le \exp(-c \cdot \sqrt{d} +1/c).$$ (Some technicalities: Implicit in Conjecture 2's conclusion is that $r(x)$ has no poles with $|x-\frac12|\le\frac12$ . We take the principal branch of the square root so that it matches Newman's result on the positive real interval $[0,1]$ and the square root is continuous in the region of interest, since the branch cut is on the negative real axis.) Numerically, it seems that Newman's construction works on the complex plane without modification. Here is a plot of the error as we go around the boundary of the disc for varying degree. Here is a plot showing that the error is maximal at the boundary. That is, we vary the radius of the circle centered at $\frac12$ and compute the maximum error on that circle. Finally, here is a plot showing how the error decreases with degree. This seems consistent with $\exp(-c\sqrt{d})$ asymptotic error. For my application, it would suffice to prove something weaker than Conjecture 2 above. In particular, we can shrink the disc slightly, as in Conjecture 3 below. It would also suffice to prove some kind of weighted average error bound. But the statement becomes messy, so I'll leave it at this. Conjecture 3. There exists a universal constant $c>0$ such that the following holds.
Let $d \in \mathbb{N}$ .  There exists a rational function $r(x)$ of degree $\le d$ with real coeficients such that $$\sup \left\{ \left| r(x) - \sqrt{x} \right| :  x \in \mathbb{C}, \left|x-\frac12\right|\le\frac12-\varepsilon \right\} \le \varepsilon,$$ where $\varepsilon = \exp(-c \cdot \sqrt{d} +1/c)$ . This seems like a question that should have been studied.  Any pointers or suggestions would be greatly appreciated.","['complex-analysis', 'radicals', 'rational-functions', 'approximation-theory']"
4822166,Complex integral:- $ \int_0^\infty \frac{1}{x^3+1} dx$ using an unusual contour,"Given the integral $ I:= \int_0^\infty \frac{1}{x^3+1} dx$ . There are various ways of solving it such as Cauchy's residues theorem, partial fraction.
I am interested in solving it using the residues theorem but choosing a different contour than the ones i was able to find here ( Integrating $\int_0^{\infty} \frac{dx}{1+x^3}$ using residues. ). In addition, I would focus only in the part in which i can spot i made the mistake. Now lets jump into the problem: The contour i want to use is half circle centered at z=R and let R approach infinity as shown in the picture. The integral over the contour is equal to the sum of the integrals over $c_1$ and $c_2$ as shown in the picture. The integral over $c_1$ is simply I as defined above. For the integral over $c_2$ lets use z=R+R $e^{i\theta}$ , $\theta$ goes from 0 to $\pi$ ,dz= $Rie^{i\theta}d\theta$ hence the integral over $c_2$ (lets name it $I_2$ ) becomes $I_2=\int_0^\pi \frac{Rie^{i\theta}}{R^3(1+e^{i\theta})^3+1} d\theta$ , as R approaches infinity $I_2$ approaches zero leaving us with the integral over the contour equals I. Now lets use the residues theorem to obtain I= $2{\pi}iRes(\frac{1}{z^3+1})$ . I have calculated the residues and used it in a different calculation using a different contour and got the correct answer which means the mistake is somewhere in my contour choosing but i dont know where. I would like to know what i did wrong. Thanks in advance.","['integration', 'complex-analysis', 'residue-calculus']"
4822300,Group is to ring as ring is to...,"I'm wondering if there is a structure that builds on the ring axioms analogously to how rings build on the group axioms. For example, $(X, +)$ is a group and $(X, +, \times)$ is a ring with identity. Is there a structure $(X,+,\times,\wedge)$ that adds another operator and additional constraints to the ring structure? For example: $\times$ is commutative $\wedge$ is associative $\wedge$ is distributive across $\times$ The first thing I thought of as a possible example is the ring $(\mathbb{Z},+, \times)$ with exponentiation as the third operator. But of course exponentiation is not associative and it only distributes on the right over multiplication. I don't see this kind of structure in eg. Grillet, Abstract Algebra , or in the Wikipedia list of algebraic structures . Or perhaps I'm not recognizing some common structure as being essentially the kind of thing I'm asking about?","['universal-algebra', 'ring-theory', 'abstract-algebra', 'category-theory']"
4822302,Function that equals its own Mellin transform [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I know that the Gaussian distribution is its own Fourier transform.
There was a similar post about what function is its own Laplace transform that was answered.
I was wondering if there exist a function such that $$
f(s) = \int_0^\infty f(x) x^{s-1}dx
$$ I don't have any use for finding such a function but I am just curious about it.","['complex-analysis', 'calculus', 'functions', 'mellin-transform']"
4822358,Find the derivative of $𝑥𝑒^{x\sin(𝜋/𝑥)}$ at $x = 0$,"I am not sure what the correct answer is and I am stuck on a problem. I will list the question below and provide my reasoning. Please let me know where or if there is something incorrect in my process. Here is the question: Consider the function $$
    f(x)= 
\begin{cases}
    xe^{x\sin \big( \frac{\pi}{x} \big)} & \text{if } x\neq 0\\
    0              & \text{if } x = 0 
\end{cases}
$$ Determine $f'(0)$ . Write DNE if the derivative is not defined.
(Note: The expression in the exponent is $x\sin(\frac{\pi}{x})$ .) My reasoning To show that $f$ is differentiable at all $x \in \mathbb{R}$ , we must show that $f'(x)$ exists at all $x \in \mathbb{R}$ . Recall that $f$ is differentiable at $x$ if $\lim_{x\to h} \frac{f(x+h) - f(x)}{h}$ exists. The derivative at $x=0$ is given by the limit: $$f'(0) = \lim_{h\to 0} \frac{f(h) - f(0)}{h} = \lim_{h\to 0} \frac{he^{h\sin \big( \frac{\pi}{h} \big)} - 0}{h} = \lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)}$$ Now the range of sine is also [−1, 1], so $$-1 \leq \sin \Big( \frac{\pi}{h} \Big) \leq 1 $$ Taking $e^{h}$ raised to both sides of an inequality does not change the inequality, so $$(e^{h})^{-1} \leq (e^{h})^{\sin \Big( \frac{\pi}{h} \Big)} \leq (e^{h})^{1} $$ $$e^{-h} \leq e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq e^{h}$$ $$\lim_{h\to 0} e^{-h} \leq \lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq \lim_{h\to 0} e^{h}$$ So, our original function is bounded by $e^{-h}$ and $e^{h}$ , and since $$ \lim_{h\to 0} e^{-h} = \lim_{h\to 0} e^{h} = 1$$ Then, by squeeze theorem, $$\lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} = 1. $$ So, $f'(0) = 1$ . When look up the limit on a calculator, I find $\lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)} = $ DNE. The answer key said $f'(0) = 0$ . Where did I go wrong?","['limits', 'calculus', 'derivatives']"
4822386,Distribution of $X(X^\top X)^{-1}X^\top$ when $X$ is a matrix of iid normals,"I'm looking at a sparse linear regression model. For the problem I'm working on, I would like to show that there are many possible choices of the independent variables that are wrong but achieve low residual error. In particular, I have a setup of $Y=X\beta^*+W$ , where $Y$ is $n\times 1$ , $X$ is $n\times p$ , with $X_{ij}$ distributed i.i.d. as $N(0,\sigma^2_x)$ , and $W$ is i.i.d $N(0,\sigma^2_w)$ . The OLS coefficient is $\beta_{OLS}=(X^\top X)^{-1}X^\top Y$ . This gives a predicted $\hat{Y}=X(X^\top X)^{-1}X^\top Y$ . I want to consider the case that the true $\beta^*$ is actually 0, and bound the correlation between $Y$ and $\hat{Y}$ , i.e. to see if $$\frac{Y\hat{Y}}{\lVert Y \rVert^2}$$ could be close to 1 even though $X$ is independent of $Y$ . I would therefore like to determine the distribution of $X(X^\top X)^{-1}X^\top$ when $X$ is an $n\times p$ matrix of i.i.d. normals, i.e. $X_{ij}$ are i.i.d. $N(0, \sigma^2)$ for all $i,j$ . I know that in this case $X^\top X$ is distributed as a Wishart $W_p(\sigma^2 I_p,n)$ but I haven't got any further than this.","['statistics', 'normal-distribution', 'probability', 'random-variables']"
4822415,"Calculating Volume of ""fat sphere"" defined by $x^8+y^8+z^8 = 64$","A young friend of mine had a homework problem involving a surface integral over the surface of the equation $x^8+y^8+z^8=64$ .  The homework problem was solvable using the divergence theorem, but he was curious how to calculate the volume of the bounded complementary component of this surface using triple integrals and I don't remember enough of these tricks to figure it out. Is there a nice formula for calculating volumes of these 'fat spheres' defined by $x^{2n}+y^{2n}+z^{2n} = a$ ? We tried spherical coordinates but the mess was prodigious.","['integration', 'definite-integrals', 'volume', 'multivariable-calculus', 'calculus']"
4822424,Using Chebyshev Inequality to find lower bound of an interval,"Given a random variable $X$ with $\mu = 200$ and $\sigma = 10$ . Find a lower bound for $P(130 \leq X \leq 270)$ using Chebyshev Inequality. What I've tried was: The inequality is $P(|X - E[X]| \geq k\sigma) \leq \frac{1}{k^2}$ , so for $P(130 \leq X \leq 270)$ we have $P(|X-200| \geq 700) \leq \frac{1}{70^2} = P(|X-220| \leq 700) \geq 1 - \frac{1}{70^2} = P(|X-200| \leq 700) \geq 0.9997$ But in the solutions the answer is $\frac{48}{49} = 0.98$ What am I doing wrong?","['statistics', 'probability']"
4822479,Axler Proof of Real vs. Complex Spectral Theorem,"In Professor Axler's Linear Algebra Done Right , he separates the proof of the spectral theorem for normal operators on a complex v.s. from the spectral theorem for self-adjoint operators over a real v.s., and I am wondering if each proof also works for the other case. To my understanding, sketches of the proofs are as follows: Normal operator $T$ on a complex v.s.: $T$ has an eigenvalue $\lambda$ . Schur's theorem says we can express $T$ as an upper triangular matrix (apply the inductive hypothesis of Schur's theorem to the range of $T - \lambda I$ and complete the basis). Result that $||Tx|| = ||T^*x||$ for normal operators gives the result. Self-adjoint operator $T$ on a real v.s.: $T$ has an eigenvalue $\lambda$ . Apply the inductive hypothesis of the spectral theorem to the orthogonal complement of the span of an eigenvector (allowed to do this because this is a $T$ -invariant subspace) to get result. It seems to me like the key result is showing the existence of eigenvalues, and each proof would work for the other case (to apply (2) to normal operators, we need to verify that the complement of an eigenspace is $T$ -invariant if $T$ is normal). I just want to make sure I'm not missing some subtlety that explains why Professor Axler separated these 2 cases. Thank you!","['spectral-theory', 'linear-algebra']"
4822506,Central limit theorem on small scales,"Say $X_1,\ldots,X_n$ are iid, bounded,  symmetric random variables with  mean $0$ , variance 1 and a smooth density. At what scale does $\frac{X_1+\cdots+X_n}{\sqrt{n}}$ ""look like"" the normal distribution? If the $X_i$ are uniform random variables on $[-1,1]$ then it seems that a modification of the characteristic function proof of Berry-Esseen gives that $$\mathbb{E}f\left(\frac{X_1+\cdots+X_n}{\sqrt{n/3}}\right) = \mathbb{E}f(G)+ O(n^{-1} \||f\||_1) + O(e^{-cn} \||D^2f\||_2)\,,$$ where $G$ is the standard unit Gaussian on $\mathbb{R}$ . This suggest that $\frac{X_1+\cdots+X_n}{\sqrt{n}}$ ""looks"" gaussian down to exponential scales, however I am unable to find a reference addressing this situation. The standard examples showing sharpness of Berry-Esseen usually involve discrete random variables. But this calculation suggests that an improved Berry-Esseen holds on much smaller scales when the distribution of $X_i$ is smooth. Thanks in advance.","['central-limit-theorem', 'probability-limit-theorems', 'probability-theory', 'weak-convergence']"
4822524,Does this recursive journey through Pascal's triangle always reach $1$?,"Let $p(k)$ be the $k^\text{th}$ number in Pascal's triangle, numbered from left to right in each row, going down the rows. So, for example, $p(1)$ to $p(10)$ are $\binom{0}{0},\space \binom{1}{0},\space \binom{1}{1},\space \binom{2}{0},\space \binom{2}{1},\space \binom{2}{2},\space \binom{3}{0},\space \binom{3}{1},\space \binom{3}{2},\space \binom{3}{3}$ , respectively. Now define sequence $a_n$ as: $$a_1\text{ is a positive integer}$$ $$a_{n+1}=p(a_n)$$ Is the following conjecture true or false: Conjecture: $\lim\limits_{n\to\infty}a_n=1$ for all $a_1\in\mathbb{Z^+}$ . For example, with $a_1=150$ we have: $a_2=p(150)=560$ $a_3=p(560)=32$ $a_4=p(32)=35$ $a_5=p(35)=7$ $a_6=p(7)=1$ $a_n=1$ for $n\ge 6$ With $a_1=100$ we have: $a_2=p(100)=1287$ $a_3=p(1287)=37353738800$ $a_4=p(37353738800)=\space ?$ If an $a_n$ value is very large, then $a_{n+1}$ is likely to be much larger than $a_n$ . However, even if the terms become very large, it is possible that eventually one of them will be small enough so that the sequence will become an endless stream of $1$ s. I don't know how the sequence will play out. Here and here is the sequence $p(k)$ , except their index is shifted down $1$ from mine. So for example, my $p(5)$ is numbered as the $4$ th term is the linked sequence. Context: I have been playing with Pascal's triangle, investigating some of its mysterious , geometrical and humourous properties.","['conjectures', 'recurrence-relations', 'binomial-coefficients', 'sequences-and-series', 'limits']"
4822554,"Proving $\frac{a+b+c}{a^2b+b^2c+c^2a+9}\ge \frac{abc+6}{abc+27}$ for $a, b, c\ge 0$; $ab + bc + ca = 3$","I came up with the inequality accidentally so there is no original proof so far. It would be great if you can give some useful help to prove it. Problem. Given non-negative real numbers $a,b,c$ satisfying $ab+bc+ca=3.$ Prove that $$\frac{a+b+c}{a^2b+b^2c+c^2a+9}\ge \frac{abc+6}{abc+27}.$$ Equality occurs when $a=b=c=1$ or $a=3;b=1;c=0$ and its permutations. I thought that $BW$ might be helpful for this case. See more about this method AOPS. Firstly, I tried to verify the inequality as $$abc(a+b+c)+27(a+b+c)\ge abc(a^2b+b^2c+c^2a)+9abc+54+6(a^2b+b^2c+c^2a).$$ After homogenization, it's $$\frac{abc(a+b+c)(ab+bc+ca)}{3}+\sqrt{3}(a+b+c)\sqrt{(ab+bc+ca)^5}$$ $$\ge abc(a^2b+b^2c+c^2a)+\sqrt{3}abc\sqrt{(ab+bc+ca)^3}+2(ab+bc+ca)^3+\frac{2\sqrt{3}(a^2b+b^2c+c^2a)\sqrt{(ab+bc+ca)^3}}{3}.$$ Or $$3 abc(a^2b+b^2c+c^2a)+6(ab+bc+ca)^3-abc(a+b+c)(ab+bc+ca)\le \sqrt{3(ab+bc+ca)^3}\cdot\left[3(a+b+c)(ab+bc+ca)-3abc-2(a^2b+b^2c+c^2a)\right].$$ It's not hard to see that both side is non-negative sign. Hence, it's enough to show a $12$ th degree cyclic polynomial $$3(ab+bc+ca)^3\cdot\left[3(a+b+c)(ab+bc+ca)-3abc-2(a^2b+b^2c+c^2a)\right]^2\ge \left[3 abc(a^2b+b^2c+c^2a)+6(ab+bc+ca)^3-abc(a+b+c)(ab+bc+ca)\right]^2.$$ I attach the result by Wolfram. BW I did not check all coefficient. In case it is true, the proof is still non-human proof, I think. Also, I found an interesting fact which is simpler $$\boxed{\color{green}{\frac{9(a+b+c)}{2}-\frac{3abc}{2}-9\ge a^2b+b^2c+c^2a}.}$$ This inequality saves desired occuring equality. I try to use it for the starting inequality without success. Indeed, we'll prove $$\color{black}{\frac{a+b+c}{\dfrac{9(a+b+c)}{2}-\dfrac{3abc}{2}}\ge \frac{abc+6}{abc+27}}$$ which is already wrong when $a=b\rightarrow 1^{-}.$ For this kind of inequalities, I'd like to study more about the relations of $a^2b+b^2c+c^2a, a+b+c,ab+bc+ca, abc.$ It is very useful if you can give some relevant links to topic. I truly hope we can find some nice ideas for my inequality. Thank you for sharing.","['multivariable-calculus', 'algebra-precalculus', 'inequality']"
4822581,Finding $E[Z^2\Phi(Z)]$ using law of the unconscious statistician,"Question: Random Variable $Z$ follows a standard normal distribution with c.d.f. $\Phi$ . Find $E[Z^2\Phi(Z)]$ using the law of the unconscious statistician. I am thinking of using some substitution to work it out, for example, using $\phi'(z)=-z\phi(z)$ can help to solve $E[Z\Phi(Z)]$ , but I don’t know how to apply this trick for finding out $E[Z^2\Phi(Z)]$ .","['expected-value', 'statistics']"
4822693,Definition and computation of concrete blow-up on projective plane,"I have trouble finding a definition for the blow of of a point on the projective plane (or any projective space), and a projective plane curve. So first of all if you have a reference treating this I would appreciate.
Hartshorne talks about affine varieties or blow-up coherent sheaves of ideals in general on a noetherian scheme but I didn't find anything on the concrete case of a point in a projective space. Gathman briefly says that on any algebraic variety we can consider an open covering of affine schemes, we can blow-up these and glue them. He also says  we can define it to be the closure of the graph $(x,f_1(x),\dots,f_r(x))$ if $f_1,\dots,f_r$ are homogeneous polynomials defining our variety but it's not clear for me what this thing is supposed to be. In his last comment on his answer Jérémy Blanc defined the blow-up of $\mathbb P^n$ at $[1:0:\dots:0]$ to be $$X=\{([x_0:\dots:x_n],[y_1:\dots:y_n])\in\mathbb P^n\times\mathbb P^{n-1}\mid x_iy_j=x_jy_i \forall i,j\in[1,n] \}$$ which I guess is a particular case of more general definition where we consider $[1:a_1:\dots:a_n]$ . Now consider the projective curve in $\mathbb P^2_{\mathbb C}$ defined by $f_{\lambda}=y^2z^3-x^5+5\lambda xz^4-4\lambda z^5$ with $\lambda\neq0,1$ . It has a singular point at $[0:1:0]$ and so I want to blow it up at this point. I would guess that the total inverse image is $$\{([x:y:z],[t:u])\mid xu-tz=0,f_{\lambda }(x,y,z)=0\}.$$ Now if as in the affine case I consider the chart $U_t$ we would get $xu-y=0$ but replacing $y$ by $xu$ in $f_{\lambda}$ breaks homogeneity and so $[x:y:z]$ being a root of $f_{\lambda}$ doesn't make sense does it ? Do we consider charts of $\mathbb P^1,\mathbb P^2$ silmunateously ? That is on the chart $U_x\times U_t$ for example we would get the that the total inverse image is $$\{([1:y:z],[1:u])\mid (y,z,u)\in V(z-u,y^2z^3-1+5\lambda z^4-4\lambda z^5)\}$$ which I'm not sure of. Is it the correct approach or am I off road ? And how to extract the strict transform from this ? Finally if for example I consider the chart $\{y\neq 0\}$ , I can blow-up as in the affine case. It seems to be enough (and simpler) to do this affine blow-up to characterize the blow up in $\mathbb P^2$ , it it the case ?","['algebraic-curves', 'projective-geometry', 'blowup', 'algebraic-geometry', 'projective-space']"
4822700,"Particular Integral, with sequence","Subject: Seeking Help for a Computer Science Contest - Integral Estimation Hello everyone, I hope this message finds you well. I am currently preparing for an ongoing computer science contest, and I have encountered a challenging problem related to estimating integrals. The specific problem involves calculating the integral: $ I_n = \int_0^1 \frac{x^n}{6 + x - x^2} \,dx $ The initial values are given as $ I_0 = \frac{2}{3}\ln\left(\frac{3}{2}\right) $ and $ I_1 = \frac{1}{5}\ln\left(\frac{3}{2}\right) $ . The task is to show that for $ n \geq 2 $ , $ I_n $ can be expressed through the following recurrence relation: $ I_n = \alpha I_{n-1} + \beta I_{n-2} + \gamma_n $ where $ \alpha + \beta  $ are constants to be determined, and $ \gamma_n $ is a sequence that needs to be explicitly defined. I've been struggling to prove this recurrence relation, and any guidance or assistance would be greatly appreciated. Additionally, I am curious to determine the values of $\alpha,\beta$ , and the explicit form of $ \gamma_n \ $ for this recurrence relation. Thank you in advance for your help!","['integration', 'definite-integrals', 'approximation', 'sequence-of-function', 'linear-approximation']"
4822777,Minimum number of terms to approximate $\pi=4\sum_{n=1}^\infty\frac{(-1)^{n-1}}{2n-1}$,"Consider the Leibniz formula for $\pi$ $$
\pi=4\sum_{n=1}^\infty\frac{(-1)^{n-1}}{2n-1}.
$$ What is the minimum number of terms needed to calculate $\pi$ accurate to $k$ decimal places? My attempt: Let's consider $k=2$ decimal places for example and set $a_n=\frac{4}{2n-1}$ . One way to think about this is to consider the remainder of the series and simply use $$
|R_n|\leq a_{n+1}\Leftrightarrow |R_n|\leq \frac{4}{2n+1}\leq 10^{-2}
$$ which holds for $n\geq 200$ . Alternatively, Calabrese's error bound yields $$
\frac{a_{n+1}}{2} < |R_n| < \frac{a_n}{2}\Leftrightarrow \frac{2}{2n+1} < |R_n| < \frac{2}{2n-1}
$$ which leads to $99.5<n<100.5$ and thus $n=100$ , a refined number. Indeed, either term order satisfies $$
\begin{align}
R_{200}&=4\sum_{n=201}^\infty\frac{(-1)^{n-1}}{2n-1}\simeq 0.004999968751 \leq 10^{-2}\\
R_{100}&=4\sum_{n=101}^\infty\frac{(-1)^{n-1}}{2n-1}\simeq 0.009999750031 \leq 10^{-2}
\end{align}
$$ However, the partial sums give $$
\begin{align}
S_{200}&=4\sum_{n=1}^{200}\frac{(-1)^{n-1}}{2n-1}\simeq 3.136592685\\
S_{100}&=4\sum_{n=1}^{100}\frac{(-1)^{n-1}}{2n-1}\simeq 3.131592904
\end{align}
$$ which are not accurate to two decimal places ( $\pi\simeq 3.14...$ ). In fact, the minimum value of $n$ I found (computationally) that gives an accuracy to two decimal places was $n=119$ . Indeed, $$
\begin{align}
S_{119}&=4\sum_{n=1}^{119}\frac{(-1)^{n-1}}{2n-1}\simeq 3.149995867\\
R_{119}&=4\sum_{n=120}^{\infty}\frac{(-1)^{n-1}}{2n-1}\simeq -0.008403213004
\end{align}
$$ Is there an analytical way of determining the minimum number of terms for any accuracy $k$ ? Comment: Out of curiosity, I have calculated the first $10$ minima for ""accuracies"" $k=0,...,9$ . Respectively, I got $$
(3,19,119,167,10794,136121,1530012,18660304,155973051,1700659132)
$$ On a log plot, we get","['approximation', 'calculus', 'pi', 'taylor-expansion', 'sequences-and-series']"
4822818,How could I prove that $g$ is differentiable?,"Let $x_1, \dots, x_{k - 1} \in \mathbb{R}^n$ be fixed and let $f \colon U \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ a $k$ times differentiable function. I want to prove that $g \colon U \longrightarrow \mathbb{R}^m$ given by $g(z) = D^{k - 1}f(z)\left(x_1, \dots, x_{k - 1}\right)$ is differentiable and that $Dg(z)\left(x_k\right) = D^kf(z)(x_k, x_1, \dots, x_{k - 1})$ . I think the proof I've got has a mistake. Here it is: $g$ can be written as the composition of $$D^{k - 1}f \colon U \longrightarrow \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right)$$ and $$\lambda \colon \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right) \longrightarrow \mathbb{R}^m,$$ where $\lambda(T) = T\left(x_1, \dots, x_{k - 1}\right)$ . Since $\lambda$ is linear, by the chain rule we can write, for $z \in U$ , $$Dg(z) = D\left(\lambda \circ D^{k - 1}(f)\right)(z) = D\lambda\left(D^{k - 1}f(z)\right) \circ D\left(D^{k - 1}f\right)(z) = \\$$ $$= \lambda \circ D\left[D^{k - 1}f\right](z) \in \mathcal{L}\left(\mathbb{R}^n, \mathbb{R}^m\right).$$ Consequently, for $x_k \in \mathbb{R}^n$ , $D\left[D^{k - 1}f\right](z)\left(x_k\right) \in \mathcal{L}\left(\mathbb{R}^n, \mathcal{L}^{k - 1}\left(\mathbb{R}^n; \mathbb{R}^m\right)\right) \equiv \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right)$ , and we can write $$Dg(z)\left(x_k\right) = \lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right) = D\left(D^{k - 1}f\right)(z)\left(x_k\right)\left(x_1, \dots, x_{k - 1}\right) = \\$$ $$= D^kf(z)\left(x_k, x_1, \dots, x_{k - 1}\right)$$ (we identify $D(Df)(z)(h)(k)$ with $D^2f(z)(h, k)$ ). I don't understand the last equalities. I don't really understand what is $\lambda\left(D\left(D^{k - 1}f\right)(z)\left(x_k\right)\right)$ , I mean, is that even defined? $D\left(D^{k - 1}f\right)(z)\left(x_k\right) \in \mathcal{L}^k\left(\mathbb{R}^n; \mathbb{R}^m\right)$ , so $\lambda$ shouldn't be defined there. It is really confusing, but could you help me? Thanks!","['calculus', 'derivatives', 'real-analysis']"
4822843,Convex Independence,"I have a difficulty in understanding a given definition for convex independence: A set of beliefs $\beta$ of an agent $i$ satisfy convex independence if beliefs of no type $t_i$ can be represented as a convex combination of other types $t'_i \neq t_i$ I read the Wikipedia article about convex combinations and I think I get what it means. Thus, also the definition of convex independence should be clear but I can't make sense of the given example: example of convex independence why does convex independence fail for the bottom example? I can't see how I can create a convex combination (of the red row?) to match another row (belief) what is the fastest way to check for convex independence in such a case? Checking each row vs. the other ones would take quite some time Notation: $\beta_i(.) = \pi_j(.)$ : belief of player i (expressed as a probability) about player $j$ being of a certain type given his own type $\theta_1^i$ : type „ $\theta_1$ “ of player $i$","['statistics', 'independence', 'economics', 'probability-theory', 'probability']"
4822854,"Computing the surface integral of $\ G(x,y,z) = xyz $ over the triangular surface with vertices $(1,0,0),\,(0,2,0)$ and $(0,1,1)$.","I was attempting to compute the surface integral of $\ G(x,y,z) = \ xyz $ over the triangular surface with vertices at $\ (1,0,0)$ , $\ (0,2,0)$ and $\ (0,1,1)$ . Clearly, the first step is to parametrize the region. I thought of two approaches : The first approach was choosing $\ x$ and $\ y $ as my parameters and rewriting $\ z $ as a function of $\ x$ and $\ y$ , then projecting the above triangular surface onto the $\ xy $ plane to find the upper and lower bounds of the required surface integral. The surface is given by the equation: $$\ 2x + y + z = 2 $$ Then, we can use the formula: $$ \ dS = \sqrt(1+f_x^2+f_y^2) \cdot dx dy $$ Since the normal vector of our projection is along the positive z direction. So, I expressed $\ z$ in terms of $\ x$ and $\ y$ , substituted $\ z(x,y) $ into G and obtained the double integral in terms of $\ x$ and $\ y$ . But I am having some trouble identifying the bounds of the double integral. It is obvious that 0 <= $\ x $ <= 1, but what should be the bounds for y? I tried sketching the shadow of the triangular region on the $\ xy $ plane by moving the point $\ (0,1,1)$ to $\ (0,1,0)$ and computing the bounds for $\ y $ for a given $\ x$ . Doing that, I got $\ (1-x) <= y <= (2-2x) $ but I am unable to obtain the correct answer via this method. So then, I thought of another method. I thought of doing the same thing with the $\ x-z $ plane instead of the $\ xy $ plane. The triangular surface remains the same and the integrand also remains the same if we simply swap out $\ y$ with $\ z$ . But I am also confused about the bounds in this case. If I project the point $\ (0,1,1)$ onto $\ (0,0,1)$ and the point $\ (0,2,0)$ to $\ (0,0,0)$ , I get the bounds of z as $\ 0$ <= $\ z$ <= $\ (1-x) $ . I am yet again getting an incorrect answer with this. It is possible that the source I am checking for answers is incorrect itself. The answers I am obtaining via both methods is $\frac{\sqrt6}{30}$ , while the answer given is $\frac{\sqrt6}{15}$ . Can someone please clarify the appropriate procedure to find the correct bounds in this question, and the correct answer if possible?","['multivariable-calculus', 'surface-integrals']"
4822881,An example of a function presented by Riemann in one of his paper,"I came across this function which Riemann has constructed in his paper 'On the representation of a function by a trigonometric series' to prove that a function that is dicontinuous in infinitely many points could be integrable.
he wrote: Since these functions have never been considered before, it is well to start
from a particular example. Designate, for brevity, $(x)$ to be the excess of $x$ over the closest integer, or if x lies in the middle between two (and thus the
determination is ambiguous) the average of the two numbers $1/2$ and $-1/2$ ,
hence zero.
Furthermore, let $n$ be an integer and $p$ an odd integer, and form the series $$f(x) = (x) + (2x)/2 + (3x)/9 + \cdots $$ It is easy to see that the series converges for each value of $x$ . When the argument
continuously decreases to $x$ , as well as when it continuously increases
to $x$ , the value always approaches a fixed limit. Indeed, if $x = \frac{p}{2n}$ (where $p$ and $n$ are relatively prime) $$
f(x + 0) = f(x) - \frac{\pi^2}{16n^2}
$$ $$
f(x - 0) = f(x) + \frac{\pi^2}{16n^2}
$$ in all other cases $f(x + 0) = f(x)$ and $f(x - 0) = f(x)$ . Could you please help me understand how he got to these results in detail? I found it puzzling.","['integration', 'measure-theory', 'analysis']"
4822902,Example of a uncountable compact metric space with countably infinitely many isometries,"For a metric space $(X,d)$ , let $\DeclareMathOperator{\Iso}{\operatorname{Iso}} \Iso(X, d)$ be a set of all isometries on $X$ . (Function $f:X \rightarrow X$ is isometry if $d(f(x), f(y)) = d(x, y)$ , for all $x,y \in X$ ). Is there an example of compact metric space $(X,d)$ such that $X$ is uncountable set and $\operatorname{card}(\Iso(X,d)) = \aleph_0$ ?","['general-topology', 'metric-spaces', 'examples-counterexamples']"
4822969,Integral $\int_{-\infty}^\infty \frac{\mathrm dx}{(a^2 - x^2)^2 + (bx)^2}$ [duplicate],This question already has answers here : How might I go about solving this definite integral? (4 answers) Closed 7 months ago . I'm trying to find the following integral: $$\int_{-\infty}^\infty \frac{\mathrm dx}{(a^2 - x^2)^2 + (bx)^2}$$ The result is $\pi/a^2b$ but somehow I can't recover that. I tried the residue theorem but it gave me an awfully long square root of square root residue. Mathematica is giving me also an exact integral with tan $^{-1}$ that I can't get close to the expected result and that is equally complex. Any help is appreciated.,"['integration', 'complex-analysis', 'calculus', 'indefinite-integrals']"
4823011,exchanging limit and integral of a function which tends to infinity in some points,"I am studying Young's Introduction to nonharmonic Fourier series and I am currently stucked in the proof of Theorem 4 from Chapter 3 (after Levinson). The question is: why can we state that, for $n \in \mathbb{N} $ and $\alpha < 0$ , $$\int_{-\pi}^{\pi} (1+e^{it})^{\alpha}e^{int} \,dt = \lim_{r \to 1^-} \int_{-\pi}^{\pi} (1+re^{it})^{\alpha}e^{int} \,dt \,?$$ If $\alpha \geq 0$ , it is pretty clear that it is possible to do so, as the function would be bounded. In this case I am struggling to find a proper bound for the function in order to apply the dominated convergence theorem , since $$\lvert 1 + re^{it} \rvert^2 = \left( 1 + r\cos t \right)^2 + \left( r\sin t \right)^2 = 1 + 2r\cos t + r^2$$ can be arbitrarily close to $0$ when $t = -\pi,\pi$ . Note: in the original text $\alpha = 2(\frac{1}{2p} + \varepsilon) - 1$ , where $\varepsilon >0$ and $p \in (1,\infty)$ .","['integration', 'lebesgue-integral', 'calculus', 'limits', 'binomial-theorem']"
4823057,Computing $ I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x $,"After reading the reduction formula in the post , I am curious about the closed form of the integral $$
I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x
$$ I first resolve the integrand into two partial fractions as $$
I_n=\frac{1}{5}  \left[\underbrace{\int_0^1 \frac{x^n}{3-x} d x}_{J_n} +\underbrace{\int_0^1 \frac{x^n}{2+x} d x}_{K_n} \right]
$$ Then I tackle the two integrals in general, $$
\begin{aligned}
\int_0^1 \frac{x^n}{a-x} d x & =\int_{a-1}^a \frac{(a-x)^n}{x} d x \\
\\
& =\int_{a-1}^a\left(\frac{a^n}{x}-\sum_{k=1}^n\left(\begin{array}{c}
n \\
k
\end{array}\right) a^{n-k} x^{k-1}\right) dx \\
& =a^n \ln \left(\frac{a}{a-1}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{a^{n}-a^{n-k}(a-1)^k}{k}
\end{aligned}
$$ Hence $$J_n =3^n \ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{3^{n}-3^{n-k}2^k}{k} $$ and $$K_n= -\left[(-2)^n \ln \left(\frac{2}{3}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{(-2)^{n}-(-1)^n2^{n-k}3^k}{k} \right]$$ We can now conclude that the closed form of the integral is $$
\begin{aligned}I_n=  &\ \frac{1}{5} \left[ 3^n\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1-\left(\frac{2}{3}\right)^k}{k}\right]+ (-2)^{n}\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1-\left(\frac{3}{2}\right)^k}{k}\right]\right]\\=& \frac{3^n+(-2)^n}{5} \ln \left(\frac{3}{2}\right) -\frac{1}{5}\left[\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1}{k}\left(3^n\left(1-\left(\frac{2}{3}\right)^k\right)+(-2)^n\left(1-\left(\frac{3}{2}\right)^k\right)\right]\right.
\end{aligned}
$$ My question: Is there any alternative method? Your comments and alternative methods are highly appreciated.","['integration', 'calculus', 'partial-fractions', 'reduction-formula']"
4823106,Cartesian parametrization of the Greek Meandre,"As the title suggests, I've been trying to find a parametric equation to describe the Greek meander pattern that's seen in a lot of historical architecture. I've created a series of X and Y coordinates that traces it out: And I have both a set of quasiperiodic data describing the X values: And a set of periodic data describing the Y values: How would I find the equation that describes these? At first I thought I had to do a Fourier  series but I wouldn't even know where to start with a function like this. Please help!","['parametric', 'fourier-analysis', 'geometry']"
4823158,"If $X$ and $Y$ have same distributions and $X \leq Y$ almost surely, does $X=Y$ almost surely?","If $X$ and $Y$ have same distributions and $X \leq Y$ almost surely, does $X=Y$ almost surely? Here is a specific problem I met. Assume $f(\omega)$ be a real-valued random variable which is defined on a metric dynamical system $(\mathcal{\Omega},\mathcal{F},\theta_{t},\mathbb{P})$ and $f(\theta_{t}\omega)\leq f(\omega)$ $\mathbb{P}$ -a.s. Can we cliam that $f(\theta_{t}\omega)=f(\omega)$ $\mathbb{P}$ -a.s. ? Recall that a metric dynamical system $(\mathcal{\Omega},\mathcal{F},\theta_{t},\mathbb{P})$ means that $(\mathcal{\Omega},\mathcal{F},\mathbb{P})$ is a probability space. $\theta_t\circ\theta_s=\theta_{t+s}$ , $(t,\omega)\mapsto\theta_{t}\omega$ is measurable. And $\mathbb{P}$ is measure-preserving, that is, $\mathbb{P}(B)=\mathbb{P}(\theta_{t}B)$ for all $B\in\mathcal{F}$ . Here is my option. Clearly, $f(\theta_{t}\omega)$ and $f(\omega)$ has same distribution since $\mathbb{P}$ is measure-preserving. Thus we have $\mathbb{P}\{\omega:f(\theta_{t}\omega)\in[0,f(\theta_{t}\omega]\}=\mathbb{P}\{\omega:f(\omega)\in[0,f(\theta_{t}\omega]\}$ . Since $\mathbb{P}\{\omega:f(\theta_{t}\omega)\in[0,f(\theta_{t}\omega]\}=1$ , we have $\mathbb{P}\{\omega:f(\omega)\in[0,f(\theta_{t}\omega]\}=1$ , that is $f(\omega)\leq f(\theta_{t}\omega)$ $\mathbb{P}$ -a.s. Similarly, $\mathbb{P}\{\omega:f(\omega)\in[0,f(\omega]\}=\mathbb{P}\{\omega:f(\theta_t\omega)\in[0,f(\omega]\}$ . And $\mathbb{P}\{\omega:f(\omega)\in[0,f(\omega]\}=1$ . Hence $\mathbb{P}\{\omega:f(\theta_t\omega)\in[0,f(\omega]\}=1$ , that is $f(\theta_{t}\omega)\leq f(\omega)$ $\mathbb{P}$ -a.s. However, We don't use the assumption $f(\theta_{t}\omega)\leq f(\omega)$ almost surely, we directly follow the conclusion $f(\theta_{t}\omega)=f(\omega)$ almost surely from the validity of the same distribution of $f(\theta_{t}\omega)$ and $f(\omega)$ . It is not ture in general. Could you tell me where I deduced wrong? Thanks!","['probability-distributions', 'probability-theory', 'almost-everywhere', 'random-variables']"
4823165,"If $T$ is self-adjoint, $K$ is bound and $KT$ is symmetric, does $K$ commute with $T$?","This is a question from an undergraduate-level course on operator theory on Hilbert spaces (this is not technically homework, it's half of one of 30 statements that us students may be asked to prove during our final exam which we where given ahead of schedule for the very purpose of not having to prove them on the spot; we were encouraged to use any and all possible methods to find a solution apart from asking the professor or his collaborator directly; in the very real possibility that he's reading this, we salute him as per usual). Let $T:D(T) \subset H \rightarrow H $ , where $H$ is a Hilbert space. If $T$ is self-adjoint (i.e. densely defined, hermitian on its domain and $D(T)=D(T^*)$ ) and $K$ is a bound operator on $H$ such that $KT$ is symmetric (i.e. densely defined and hermitian on its domain), show that $K \subset \{T\}'\colon=\{A\in \mathcal{B}(H)\mid A(D(T)) \subset D(T) \land \forall f \in D(T): ATf=TAf\} $ If $K$ is bound, then it has a unique extension from its domain to the whole Hilbert space, so I can always consider $K$ to be everywhere defined. From this, I can deduce that $ D(KT)=\{f \in D(T) \mid Tf \in D(K)=H\} = D(T) $ .
In general $ T^*K^*\subset(KT)^* $ . Because $K$ is bound and $T$ is self-adjoint, I have that $ TK^*=T^*K^*=(KT)^* $ , which implies that $K^*(D(T^*)) \subset D(T^*)$ . From the symmetry of $KT$ : $KTf=(KT)^*f, \forall f\in D(T)$ , which implies that $TK^*=(KT)^*=KT$ on $D(T)$ . Now, if $K$ was self-adjoint, then the proof would be concluded. As
@geetha290krm pointed out in the comments, if $T = 0$ then all the hypoteses of the proposition are satisfied (because $0$ is self-adjoint and $KT = 0$ is surely symmetric) but $K$ can be any operator, even a not self-adjoint one. So, now, the question is: how can the statement be proven? P.S. I'm aware that there's another fairly similar question on here that was asked a week ago: that was done by a classmate of mine, but because he didn't get any answer we decided to rewrite it in a more complete form. P.P.S. Any suggestion that would exceed the level of this course is still more than welcome","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4823241,Finding the Inverse of a Function on the Sphere,"I am attempting to find the inverse of the function: $f:S^2 \to S^2$ , defined as: $f(x,y,z)=(x\cos(z)+y\sin(z), x\sin(z)-y\cos(z),z)$ My approach so far has been to use linearity and express the function as a sum of scaled basis vectors: $f(x,y,z)=x(\cos(z),\ sin(z),0)+y(\sin(z), -\cos(z),0)+z(0,0,1)$ However, I'm unsure how to proceed from this point. Any guidance or assistance would be greatly appreciated! Thanks.","['multivariable-calculus', 'calculus', 'functions', 'linear-algebra']"
4823257,"Counting ""good"" strings of octal numbers of length $n$?","My professor asked me to create a combinatorics problem. I have created one, i think it is a good problem, but I am having trouble solving it. Can anyone help me? A string of octal numbers of length $n$ is called “good” if $k=n(0)+n(7)$ and $l=n(3)+n(4)$ are odd numbers, where $n(i)$ , $i=0,1,\ldots,7$ represents the number of $i$ ’s found in the string. How many strings of octal numbers of length $n$ are “good”, where $n$ is an element of $\mathbb{N}$ ? For example: 00334221, isn't a ""good"" octal string because $k=2$ is even number. But 007334221 is a ""good"" octal string, because $k=3$ and $l=3$ are both odd numbers. We know that $a_1=0$ , $a_2=8$ , $a_3=96$ , $a_4=896$ . Where $a_n$ is the number of ""good"" octal string of length n. I have tried in this method $a_n=4a_{n-1}+4b_{n-1}$ . Where $b_n$ is string of octal numbers of length $n$ where $k$ is even and $l$ is odd (due to the symmetry of $k$ and $l$ ). But couldn't eliminate $b_n$ .","['combinations', 'combinatorics']"
4823347,"Struggling with a Calculus III problem :$\iint x+y\ dS,$ where $S(u,v)=2\cos(u)\vec i+2\sin(u)\vec j+v \vec k$ and $0\le u\le \pi/2;\;0 \le v \le9.$","So I have had Calc III many moons ago but I cannot seem to solve this problem for my son who is taking it now.  Worked it four ways and got four different answers.  Hoping someone here can set me right. Problem: Solve $\iint f(x,y)\,dS,$ where $$f(x,y) = x + y$$ and $$S(u,v) = 2\cos(u)\vec i + 2\sin(u)\vec j + v \vec k$$ $$0\le u \le \pi/2;\;0 \le v \le 9. $$ My solution is: $$\frac{\partial S}{\partial u} = -2\sin(u) \vec i + 2\cos(u) \vec j$$ $$\frac{\partial S}{\partial v} = \vec k$$ $$\frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} = 2\cos(u) \vec i - 2 \sin(u) \vec j $$ $$\left\lvert\frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} \right\rvert = \sqrt{4\cos^2(u) + 4 \sin^2(u)}=2$$ $$\iint(x+y)\,dS =\iint(x+y)2\,dA$$ $$\int^{\pi/2}_0\int^2_0(r\cos\theta +r\sin\theta) 2r\,dr\,d\theta =$$ $$\int^{\pi/2}_0\int^2_02r^2(\cos\theta +\sin\theta) \,dr\,d\theta =$$ $$\int^{\pi/2}_0(16/3)(\cos\theta +\sin\theta) \,d\theta =$$ $$32/3.$$ His online program doesn’t like this answer.  I got the same answer by staying in rectangular coordinates.  Feel like I’m setting something up wrong. Thanks in advance for any help.","['integration', 'cylindrical-coordinates', 'surface-integrals', 'multivariable-calculus', 'calculus']"
4823357,"$e$ is hidden in Pascal's (binomial) triangle. What is hidden in the trinomial triangle, in the same way?","In Pascal's triangle, denote $S_n=\prod\limits_{k=0}^n\binom{n}{k}$ . It can be shown that $$\lim_{n\to\infty}\frac{S_{n-1}S_{n+1}}{{S_n}^2}=e$$ What is the analogous result for the trinomial triangle ? That is, denote $T_n=\prod\limits_{k=-n}^n\binom{n}{k}'$ where $\binom{n}{k}'$ are the numbers in the $n$ th row of the trinomial triangle*. $$\lim_{n\to\infty}\frac{T_{n-1}T_{n+1}}{{T_n}^2}=\space ?$$ Numerical investigation using A027907 gives: $\dfrac{T_{0}T_{2}}{{T_{\color{red}{1}}}^2}=12$ $\dfrac{T_{1}T_{3}}{{T_{\color{red}{2}}}^2}=15.75$ $\dfrac{T_{2}T_{4}}{{T_{\color{red}{3}}}^2}=18.1555$ $\dfrac{T_{9}T_{11}}{{T_{\color{red}{10}}}^2}=22.4713$ $\dfrac{T_{49}T_{51}}{{T_{\color{red}{50}}}^2}=24.2722$ $\dfrac{T_{73}T_{75}}{{T_{\color{red}{74}}}^2}=24.4284$ $\dfrac{T_{97}T_{99}}{{T_{\color{red}{98}}}^2}=24.5087$ suggesting a limit of approximately $24.7$ . I found that the trinomial coefficients have a closed form expression involving Gegenbauer polynomials , but I am unable to work out the limit. *The article uses the notation $\binom{n}{k}_2$ for trinomial coefficients, but I thought that may be confusing ( $2$ for trinomial?), so I decided to use $\binom{n}{k}'$ .","['products', 'binomial-coefficients', 'closed-form', 'eulers-number-e', 'limits']"
4823360,Sufficient condition for multiplication to be continuous,"Given a commutative ring $(R,+,\cdot)$ , and a topology $\tau$ on $R$ such that for any $a\in R$ the maps \begin{align}
\cdot a: &R\to R \\
&x\mapsto a\cdot x \\\\ &\mbox{and} \\\\
+a: &R\to R \\ 
&x\mapsto x+a 
\end{align} Are continuous, is it necessarily true that $(R,+,\cdot,\tau)$ is a topological ring? What I have tried:
We need to show that the maps \begin{align}
\cdot : &R^2\to R \\
&(x,y)\mapsto x\cdot y \\\\ &\mbox{and} \\\\
+: &R^2\to R \\ 
&(x,y)\mapsto x+y
\end{align} are continuous. To do so, let $V \in \tau$ , and let $(x,y) \in \cdot^{-1}(V)$ , then since $\cdot y$ is continuous, there is some $U_x \in \tau$ such that $$U_x \subseteq \cdot y ^{-1}(V) \subseteq \pi_1\left(\cdot^{-1}(V)\right)$$ and similarly there is some $U_y$ such that $$ U_y \subseteq \cdot x^{-1}(V) \subseteq \pi_2\left(\cdot^{-1}(V)\right) $$ Where $\pi_1, \pi_2$ are the canonical projections, now one would be tempted to conclude that $$U_x\times U_y \subseteq \cdot^{-1}(V)$$ But this is not obviously true, since it is not true in general that $$\pi_1(X) \times \pi_2(X) \subseteq X$$ However my intuition from metric spaces says that we should still be able to choose $U_x$ and $U_y$ to be ""small enough"" to guarantee that their product is contained in $\cdot^{-1}(V)$ , but this intuition might be leading me astray. I suspect that the claim is in fact false, since it would almost surely be listed on Wikipedia as an alternative characterization, but yet I cannot find a counterexample. I have found some examples showing that this is not true for general continuous functions, but it is more tricky to come up with examples where the functions also turn the underlying set into a ring.","['general-topology', 'ring-theory', 'topological-rings']"
4823388,Bridgeland flops and non-projective flop,"In this paper , Bridgeland showed that for a projective 3-fold $X$ with Gorenstein and terminal singularity and a crepant resolution $f:Y \to X$ , there exists $g:W \to X$ such that $f$ is the flop of $g$ (i.e., for a divisor $D$ on $W$ if $-D$ is $g$ -nef, then $D$ is $f$ -nef) and moreover there is an exact equivalence $D^bCoh(W) \simeq D^bCoh(Y)$ . On the other hand, it seems to be known (e.g. https://mathoverflow.net/a/369756/177839 ) that this works when $X$ is not necessarily projective but just proper (while $f$ is still a projective morphism?). So, my questions are: Under which condition the extension to non-projective case is true? Is there any reference in such a case? Thank you in advance.","['derived-categories', 'algebraic-geometry', 'birational-geometry']"
4823407,"The inverse of the ""Given vertices find area of polygon in complex plane"" problem","Let $\omega_0, \ldots, \omega_{n-1}$ be the $n$ -th roots of unity, and $a_0, \ldots, a_{n-1}$ be real numbers in the $(0, 1]$ interval.
Define $z_i = a_i \omega_i$ as the vertices of a polygon on the complex plane, and $s_i$ as the area of a triangle whose vertices are $(0, z_i, z_{i+1})$ , indices taken modulo $n$ . For the case illustrated below, with $n=4$ , is it possible to determine the values of $(a_0, \ldots, a_3)$ given the values of $(s_0, \ldots, s_3)$ ? What I suspect: for odd $n$ , this problem can be reduced to a circulant linear system with a unique answer. For even $n$ , the same approach is not adequate because it leads to an underdetermined system. Find vertices $z_i$ given areas $s_i$","['complex-analysis', 'linear-algebra', 'circulant-matrices']"
4823418,Find all $a$ for a unique solution of $y'=[\cos(x+2y)+ay]^{\frac{4}{5}}$ with $y(0)=2\pi$,"Find all values of $a\in \mathbb{R}$ for which the IVP $y'=[\cos(x+2y)+ay]^{\frac{4}{5}}$ with $y(0)=2\pi$ has solution in some neighbourhood of $(x_0,y_0)=(0,2\pi)$ . For which values of $a$ is this solution unique? Attempt Let $D=\{(x,y):|x|\leqslant A,~|y-2\pi|\leqslant B\}$ and $f(x,y)=[\cos(x+2y)+ay]^{\frac{4}{5}}$ is continuous on $D$ as long as $\cos(x+2y)+ay\geqslant 0$ for all $(x,y)\in D$ . In that case,  theorem of Peano guarantees the existence of a solution, as wanted. To find all $a$ though, we have $a\geqslant -\min_{(x,y)\in D}\frac{\cos(x+2y)}{y}$ , which implies two issues: $\bullet$ how could we evaluate the above minimum? $\bullet$ even if we evaluate the min above, it would depend on the values $A,B$ of the rectangle $D$ . I would expect an answer independent of $A,\,B$ , meaning that we could work for all regions of $(x_0,y_0)=(0,2\pi)$ . For the next part of course, we could apply the Picard - Lindelof theorem, as long as $f_y(x,y)=\frac45\,[\cos(x+2y)+ay]^{-\frac{1}{5}} [a-2\sin(x+2y)]$ is bounded on $D$ , that is $\cos(x+2y)+ay>0$ for all $(x,y)\in D$ . Thanks for the help.",['ordinary-differential-equations']
