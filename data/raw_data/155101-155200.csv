question_id,title,body,tags
2622717,Linear recurrence analog of fundamental matrix,"Consider the following first order linear ODE:
$$\vec{x}'(t) = A(t)\vec{x}(t)+\vec{f}(t).$$
Let $\Psi(t)$ be a fundamental matrix, i.e. a matrix whose columns are linearly independent solutions to the above equation when $\vec{f} \equiv \vec{0}$. By making the ansazt $\vec{x}(t)=\Psi(t)\vec{c}(t)$ for the solution subject to the initial condition $\vec{x}(t_0)=\vec{x}_0$, it is trivial to show that we get
$$\vec{x}(t)=\Xi(t,t_0)\vec{x}_0+\int_{t_0}^{t}\Xi(t,\tau)\vec{f}(\tau)d\tau$$
where we have defined $\Xi(t,\tau)=\Psi(t)\Psi^{-1}(\tau)$. I now want to consider the linear recurrence analog $$\vec{x}(k+1) = A(k)\vec{x}(k)+\vec{f}(k). $$ Again, let $\Psi(k)$ be a fundamental matrix, i.e. a matrix whose columns are linearly independent solutions to the above equation when $\vec{f} \equiv \vec{0}$. It appears natural to make the ansatz $\vec{x}(k) =\Psi(k)\vec{c}(k)$. We note that $\Psi(k+1)=A(k)\Psi(k)$, so we get $\vec{x}(k+1) = \Psi(k+1)\vec{c}(k+1) = A(k)\vec{x}(k)+A(k)\Psi(k)[\vec{c}(k+1)-\vec{c}(k)] = A(k)\vec{x}(k)+\vec{f}(k)$, whence $\vec{c}(k+1) = \vec{c}(k)+\Psi(k)^{-1}A(k)^{-1}\vec{f}(k).$ However, this doesn't seem quite right since we shouldn't have to assume that $A(k)$ is invertible. My guess is that the final formula will look something like
$$\vec{x}(n) = \Xi(n,n_0)\vec{x}_0+\sum_{\eta = n_0}^{n-1}\Xi(n,\eta)\vec{f}(\tau)d\tau$$
if we prescribe the initial condition $\vec{x}(n_0)=\vec{x}_0$. Here we should have $\Xi(n+1,\eta) = A(n)\Xi(n,\eta)$ as well as $\Xi(n,n)=I$. Indeed, with these properties, we get $\vec{x}(n+1)=A(n)\vec{x}(n)+\vec{f}(n)$. In particular, if $A(n)=A$ is constant, we have $\Xi(n,\eta)=A^{n-\eta}, \ n \geq \eta$. However, I am not sure hot to derive it. Any suggestions or comments?","['recurrence-relations', 'discrete-mathematics']"
2622719,Show that $2n\choose n$ is divisible by $2n-1$,"I have found many questions asking to prove $2n\choose n$ is divisible $2$, but I also observed by trying the first few ""$n$"" that $2n\choose n$ is divisible  by $2n-1$. It sure seems true when $2n-1$ is prime, but is it true in general for all $n$?","['combinatorics', 'divisibility', 'elementary-number-theory']"
2622775,"Nice neighborhoods of each ""piece"" in a manifold connected sum","This is problem 4.19 in Lee's Topological Manifolds .  I have been working on this problem for a couple days now, and just a need a hint in the right direction. If $M$ and $N$ are two $n$-manifolds, and $B_1\subset M$ and $B_2\subset N$ are two open, regular coordinate balls (definition below), the connected sum $M\#N$ is the quotient of the disjoint union $(M-B_1)\displaystyle\sqcup (N-B_2)$ by the relation that identifies points on the spherical boundaries of each component via some homeomorphism $h$. Now I want to show that there are two open sets $U,V\subset M\#N$, such that: $U\cong M-\{p\}$ and $V\cong N-\{q\}$, for some points $p\in M$ and $q\in N$ $U\cap V\cong S^{n-1}\times\mathbb{R}$ $U\cup V=M\#N$ Here's my intuition: take a larger coordinate ball $D$ in $N$, containing $B_2$, which works because $B_2$ is regular. $(M-B_1)\sqcup (D-B_2)$ is a saturated open set, so its image in $M\#N$ is open. This is what I want to be $U$. Now I've managed to define a homeomorphism $g$ from $D-B_2$ to $\overline{\mathbb{B}}_1(0)-\{0\}$ (the punctured closed unit ball). If I can then map that punctured unit ball to $\overline{B}_1-\{p\}$, I can try and show the map $f:\ (M-B_1)\sqcup(D-B_2)\rightarrow M-\{p\}$ $$ f(x)=\begin{cases}
x& x\in M-B_1\\
g(x)& x\in D-B_2\\
\end{cases} $$ is a quotient map, and then use the uniqueness of quotients to show $U$ is homeomorphic to $M-\{p\}$.  However, this is too hard for me to do, because I need to somehow incorporate $h$ in the mix, so that the identifications of $f$ match those of $h$. Can someone provide a hint here? I feel like this is a lot of work for one problem, and I understand intuitively what to do here, but getting all the details right is proving to be too much. I'm also open to hints about part 2. as well (3. is easy). Definition : A coordinate ball $B\subset M$ is called regular if it has a neighborhood $B'\supset B$, such that there is a homeomorphism $k: B'\rightarrow \mathbb{B}_s(0)$ (this is the open ball of radius $s$ around $0$). Under this homeomorphism, $B$ goes to $\mathbb{B}_r(0)$ for some $0<r<s$, and $\overline{B}$ goes to $\overline{\mathbb{B}}_r(0)$.","['manifolds', 'general-topology', 'quotient-spaces']"
2622788,When are curves irreducible?,"This question might be too vague, but we know lots of things about irreducible curves, but when I'm given a curve I often can't tell if it's irreducible, so I don't know if those things apply to my curve. I wonder if there's any way of telling if a (projective) curve is irreducible, without trying to factor the polynomial. I read somewhere that a cubic regular curve in the complex projective plane is irreducible. Is there a generalization of this, e.g. for non-cubic curves, in a projective space over some other field, or in a higher-dimensional projective space? And is there some other similar results?","['algebraic-curves', 'algebraic-geometry']"
2622810,Why such stark contrast between the approach to the continuum hypothesis in set theory and the approach to the parallel postulate in geometry?,"In geometry, each of the 3 versions of the parallel postulate (the Euclidean, Hyperbolic and Elliptic) can be used in conjunction with the first 4 Euclid's axioms to form the axiomatic basis of each of 3 respective self-consistent formal systems: Euclidean geometry, Hyperbolic geometry and Elliptic geometry. In set theory, either the continuum hypothesis (CH) or its negation can be used in conjunction with the ZFC axioms to forms the axiomatic basis of each of 2 respective self-consistent formal systems: (ZFC + CH) and (ZFC + ¬CH). In geometry, no scholar would even think of arguing that the Euclidean version of the parallel postulate is either its only ""true"" version or a ""truer"" version than the Hyperbolic and Elliptic, and that therefore Euclidean geometry is either the only ""true"" geometry or a ""truer"" geometry than the Hyperbolic and Elliptic. Why, in contrast, do set theory scholars argue about whether CH or ¬CH is ""true""? Why don't they approach CH and ¬CH just as geometry scholars approach the 3 versions of the parallel postulate and study the respective consequent self-consistent formal systems? Taking as an example this question in mathoverflow , it would be unthinkable to ask geometry scholars ""What is the general opinion on the parallel postulate?"" I am aware that Hamkins 2011 introduced and argued for a multiverse view in set theory, which is clearly consistent with Mark Balaguer's ""plenitudinous Platonism"" (*) position in philosophy of mathematics, as argued explicitely in Fuchino 2012 . What I find remarkable is that said view was proposed that late in the development of set theory and that it seems to be still a minority position. (*) Which can just be ""plenitudinous fictionalism"", as Balaguer himself is agnostic between Platonism and fictionalism, the important notion being ""plenitudinous"".","['axioms', 'set-theory', 'geometry']"
2622831,Find $f'(0)$ if $\frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64$,Assume f is a differentiable function with $f(0)=0$ satisfying the equation $$\frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64$$ then find $f'(0)$. This is easy using L'Hopital rule. How can one compute this without using L'Hopital?,"['real-analysis', 'limits', 'calculus', 'limits-without-lhopital', 'analysis']"
2622908,Where am I making a mistake in showing that countability isn't a thing? [duplicate],"This question already has an answer here : Why there are real numbers with infinite digits, but no such natural numbers (or another reason why real numbers are uncountable) (1 answer) Closed 6 years ago . I am not a mathematician so I might not be entirely accurate in my mathematical depictions, but I will correct them if I can. My problem is like that:
I have $f_n:\mathbb R ^+\to\mathbb R ^+$ with $f_{n}(x)=x_{m_{n}}$
For the moment I will explain the function using a $n=10$ as $n$ is the basis in which the number is written and ""processed"" by my function. So given the normal decimal basis I can write the function as: $f(\sum c_i*10^i)=\sum c_i*10^{-1-i}$. I think the sum goes from $-\infty$ to $\infty$ for any given number as the digits that are not written are zeros. So what my function does (or at least if think it does) is basically a mirroring of digits over the decimal point (ex.: $f(23.45)=54.32$). I should be easy to prove that the function is bijective as $f\circ f=1_{\mathbb R ^+}$. Now given the established function my problem comes when I restrict the function to the interval $(0;1)$ as i believe that $ Im_f(0;1)=\mathbb N ^* $ which seems to indicate that ${\mathbb R}$ could be countable. EDIT: Since this was a commonly raised problem about my question I will add as many forms as I have encountered it in with what I hope are satisfying answers: $\mathbb N$ has ""finitely many digits"": Note the ""finite number of digits"" with $k$. Take $10^{k+1}$. $\mathbb R^+$ can not have numbers with infinite digits (to the left of the decimal point) then $lim_{x\to\infty}\int log_{10}x\ dx=?$ any other proof of something is finite using induction is also flawed from a simple perspective: $\mathbb N$ is not finite to begin with the argument about the lack of a successor and a predecessor holds no ground. If i can't add or subtract 1 from a decimal number knowing all the digits involved (at least in theory) I would have to go and start school again from the first grade.","['real-numbers', 'irrational-numbers', 'functional-analysis', 'elementary-set-theory', 'definition']"
2622931,Show that the inequality is valid for infinite terms of a sequence,"This question comes from a Brazilian book of real analysis, which is ""Introdução a Análise"" (Introduction to Analysis) of Antonio Caminha. The problem is: Let $(a_n)_{n \in \mathbb{N}}$ be a sequence of positive real numbers. Show that the inequality $$ 1 + a_n > 2^{1/n}a_{n-1} $$ is true for infinity $n \in \mathbb{N}$.","['real-analysis', 'inequality', 'real-numbers', 'sequences-and-series']"
2622965,Compare the variance of two unbiased estimators,"Two unbiased estimators of $Y_i\sim N(\beta x_i, \sigma^2)$ with $\sigma$ known are $\tilde\beta=\dfrac{S_{xy}}{S_{xx}}=\dfrac{\sum_{i=1}^n(x_i-\overline x)Y_i}{\sum_{i=1}^n(x_i-\overline x)x_i}$ and $\tilde{\tilde\beta}=\dfrac{\overline Y}{\overline x}$. How can I show $\operatorname{Var}(\tilde B)\le \text{Var}(\tilde{\tilde\beta})$? I have gotten as far as determining $\text{Var}(\tilde\beta)=\dfrac{\sigma^2}{\sum_{i=1}^n(x_i-\overline x)^2}=\dfrac{\sigma^2}{\sum_{i=1}^n(x_i-\overline x)x_i}$ and $\operatorname{Var}(\tilde{\tilde\beta})=\dfrac{\sigma^2}{n\cdot\overline x^2}=\dfrac{n\sigma^2}{\left(\sum_{i=1}^nx_i\right)^2}$ but am unsure how to make a proper comparison.","['descriptive-statistics', 'statistics']"
2622969,References for endomorphism bundle and adjoint bundle,I am trying to understand what are endomorphism bundle(of a vector bundle) and adjoint bundle(of a principal bundle) but could not find any references on google. Searching  Adjoint bundle gives https://en.m.wikipedia.org/wiki/Adjoint_bundle which does not really say much. But searching endomorphism bundle does not give anything. Any reference is welcome. Endomorphism bundle came in context when discussing about curvatures. It has been said that Curvature can be seen as section of some endomorphism bundle.,"['principal-bundles', 'reference-request', 'vector-bundles', 'differential-geometry']"
2622973,Graph Theory: Connected graph and vertex degrees,"- Background Information: I am studying graph theory in discrete mathematics. I have come across this question with the provided solution from my professor, but I cannot understand part of the solution. I appreciate any clarification, thanks. - Question and Solution: - My questions: Could you please explain... the part that says ""Thus each component will have at-least 1 + 1/2(n-1) vertices"". Where is + 1 (the front one) coming from in the equation 1 + 1/2(n-1)? why do we have only two of (1+1/2(n-1)) in the equation (1+1/2(n-1)) + (1+1/2(n-1)) = (n-1)+2 = n+1 ? What do those two (1+1/2(n-1)) + (1+1/2(n-1)) represent? - My thinking: Assuming n = 3, then deg(v) >= 1/2( 3 - 1) , so weget deg(v) >= 1, that means that each vertex of graph G has atleast degree of 1. This indicates that there can be more vertices that the vertex can be connected to, so that is why we use 1 + 1/2(n - 1). Having two (1+1/2(n-1)) + (1+1/2(n-1)) is probably for the number of components that we have. I am not sure, maybe my professor is assuming the graph to have two components when it is disconnected during the process of proof by contradiction.","['graph-theory', 'discrete-mathematics']"
2622983,How to deal with extended real valued functions in optimization?,"In optimization, often times people like to define a so-called extended (real) valued function, that is, a function that takes on the value $\infty$ outside of its domain. But I have noticed that people tend to downplay the distinction between an extended valued function and its unextended counterpart. For example, in Boyd's text, on page 68, it reads: In this book we will use the same symbol to denote a convex function and its
extension, whenever there is no harm from the ambiguity.  convex functions are implicitly extended, i.e., are defined as $\infty$
outside their domains. However, in the rest of the text, extended value function rarely comes up and functions are almost always denoted as  $f: \mathbb{R}^n \to \mathbb{R}$ instead of $f: \mathbb{R}^n \to \mathbb{R}\cup\{\infty\}$. (why not??) I wonder if it would be better to always stick with the un-extended version instead to avoid this ambiguity. Here is another argument as to why I think it would be better just not to talk about the notion of the extended value function. These are some pros and cons of using extended valued functions from my understanding: Pros: Do not have to specify domain of variables in definition of convexity Can represent certain functions such as the indicator function $I_{\mathcal{C}}$ Cons: Infinite arithmetic when defining convex functions The extended real line lacks good properties as compared to $\mathbb{R}$ in a topological sense It seems to infinite arithmetics is a heavy price to pay for using the extended valued functions. We are extending the arithmetic system, albeit a trivial and intuitively acceptable extension... So are there strong arguments as to why one should or should not use extended value functions? How and when should one use this concept?","['optimization', 'convex-analysis', 'functions', 'convex-optimization', 'definition']"
2622992,Complex Definite Integral: $\int _0^1\frac{dx}{\left(1+\sqrt{x}\right)^4}$,"Compute the following definite integral: $$\int _0^1\frac{dx}{\left(1+\sqrt{x}\right)^4}$$ This is what I did: $$\int _0^1\frac{1}{\left(1+\sqrt{x}\right)^4} \, dx$$ $$u = \sqrt{x}$$ $$\frac{du}{dx}=\frac{1}{2}x^{-\frac{1}{2}} \, dx$$ $$du = \frac{1}{2\sqrt{x}} \, dx$$ And after this I just got stuck. How exactly am I supposed to write $du$ in terms of the initial integral? I can't double it and nor can I leave it as is because of the $+1$. Am I supposed to make $u = 1 + \sqrt{x}$ instead or is there a way to do it with the current $du$? Any help?","['integration', 'definite-integrals', 'calculus']"
2623004,What is the meaning of the word parameter in the context of statistics?,"Question: 
The per capita consumption of bottled water in the United States amounted to 27.6 gallons in 2009. Bottled water is drinking water (carbonated or still) sold in plastic or glass water bottles. Per capita consumption of bottled water in the United States has continued to steadily increase. In 2012, it amounted to 30.8 gallons. In 2015, the per capita consumption was 36.5 gallons.† The per capita consumption of bottled water in the United States is which of the following? (a).biased statistic (b). categorical variable (c). numerical variable (d). parameter The answer is (d). 
Please can someone explain why the answer is parameter? I don't feel like I truly understand the meaning of parameter in context of statistics. I just think of a parameter like a variable or a placeholder for something else. But this seems to be different in statistics.",['statistics']
2623016,Jacobian matrix of time dependant system,"Suppose we have a differential equation of the form $x^{'}(t)=(x^{'}_{1}(t),x^{'}_{2}(t))=f((x_{1}(t),x_{2}(t))$  where $f : R^2 \rightarrow R^2$ Does the Jacobian matrix of $f$ get affected by $t$?","['ordinary-differential-equations', 'vector-analysis']"
2623072,How does the classification using the 0-1 loss matrix method work? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 months ago . Improve this question In this machine learning lecture the professor says: Suppose $\mathbf{X}\in\Bbb R^p$ and $g\in G$ where $G$ is a discrete
  space. We have a joint probability distribution $\Pr(\mathbf{X},g)$. Our training data has some points like: $(\mathbf{x_1},g_1)$, $(\mathbf{x_2},g_2)$, $(\mathbf{x_3},g_3)$ ...
  $(\mathbf{x_n},g_n)$ We now define a function $f(\mathbf{X}):\Bbb R^p \to G$. The loss $L$ is defined as a $K\times K$ matrix where $K$ is the
  cardinality of $G$. It has only zeroes along the main diagonal. $L(k,l)$ is basically the cost of classifying $k$ as $l$. An example of $0-1$ loss function: \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} $\text{EPE}(\hat{f}) = \text{E} [L(G,\hat{f})]$ (where $\text{EPE=
 Expected Prediction Error}$) $=E_\mathbf{X} E_{G/\mathbf{X}} \{L[G,\hat{f}]|\mathbf{X}\}$ $\hat{f}(\mathbf{x})=\text{argmin}_g\sum_{k=1}^{k}L(k,g)\text{Pr}(k|\mathbf{X}=\mathbf{x})=\text{argmax}_g\text{Pr}(g|\mathbf{X=x})$ $\hat{f}(\mathbf{x})$ is the Bayesian Optimal Classifier. I couldn't really follow what the professor was trying to say in some of the steps. My questions are: Suppose our loss matrix is indeed: \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} What it the use of this matrix? What does classifying $k$ as $l$ even mean? Then how do we read off the loss for (say) a certain input $\mathbf{x_i}$ from the matrix? I couldn't understand what $\hat{f}$ and $\text{EPE}(\hat{f}\mathbf{(x)})$ stand for. Could someone please explain it with a simple example?","['matrices', 'machine-learning', 'statistics', 'probability']"
2623097,Why can't we simply substitute the input variable into the output of a function?,"Sorry if this question has been asked before. I'm not sure how to phrase this question properly (hence I couldn't find any fruitful results on Google). We know that the following holds true for $|x| < 1$, $$
\frac{1}{1-x} = \sum_{i=0}^\infty x^i = 1 + x + x^2 + \ldots
$$ If we substitute $x^2$ as the variable into both sides of the equation, we obtain: $$
\frac{1}{1-x^2} = \sum_{i=0}^\infty x^i = 1 + x^2 + x^4 + \ldots
$$ which is also true. However, if we try the same with integration, we will get a different outcome. The following holds true by basic law of integration: $$
\int x dx = \frac{x^2}{2} + C
$$
where C is an arbitrary constant. If we substitute $x^2$ as the variable into both sides of the equation, we obtain: $$
\int x^2 dx = \frac{x^4}{2} + C
$$ which is clearly wrong. My question is, $ x $ is meant to be an arbitrary input for a function. Hence, (I think) we could substitute $ x $ with anything and simply plug the new variable into the output as well. Although this is evidently false in the integration example, it works for the example using geometric series. Why does this claim hold true sometimes and fail in other situations (just like in the 2 examples given above)?",['functions']
2623107,Does a random variable come from a probability distribution or is it vice-versa?,"I am curious to know which statement is correct: a random variable come from a probability distribution
OR a probability distribution is created from observing the behavior of a random variable","['random', 'probability', 'probability-distributions']"
2623121,Finding the marginal density when the conditional density is given to us.,"Let the conditional probability density of $X$ given $Y=y$ is as follows: $f(x|y)= {e^{y-x}, x>y}$ where $y>0$. and let the density of $Y$ has been provided as exponential distribution with mean $\frac{1}{\lambda}$. then what is the marginal density of the $X$. I have obtained the joint density of the $X$ and $Y$ by multiplying the given densities and obtained:
$f(x,y)=\lambda e^{-x} e^{-\lambda y+y}$ where $y<x$. To obtain the density of $X$, i calculated the following integral: $\int_{0}^{x} \lambda e^{-x} e^{-\lambda y+y} dy$ which after a little simplification turned out to be: $f(x)=\frac{\lambda}{1-\lambda} [e^{-\lambda x}+e^{-x}]$ I am not sure whether my density of $X$ is correct or not. My textbook is showing some different answers. Also, is there any quick method to the given problem because the definite integral took some time to solve. Thanks in advance.","['statistics', 'probability-distributions']"
2623180,Problem regarding cumulative distribution function,"Let $X$ be a random variable with the following cumulative distribution function: $$F(x)= 
\begin{cases}
0 & \quad x<0\\
x^2 & \quad 0\leq x<\frac{1}{2}\\
\frac{3}{4} & \quad \frac{1}{2}\leq x<1\\
1 &\quad x\geq1.
\end{cases}$$ Then what is the value of $P(\frac{1}{4}<X<1).$ I am trying to solve this problem as: $P(\frac{1}{4}<X<1)=P(\frac{1}{4}<X\leq1)$, since for continuous random variable probability at a point is always zero.
  Thus $P(\frac{1}{4}<X<1)=P(\frac{1}{4}<X\leq1)=F(1)-F(\frac{1}{4})=\frac{15}{16}=0.9375$. But the answer didn't match. Answer is 0.68. So where did i wrong. Any suggestion or solution regarding this should be highly appreciated.","['probability-theory', 'probability-distributions']"
2623229,Proving statement regarding infimum of image of set with monotonically increasing function,"I'm having difficulty completing a proof of a statement (my attempt is written after the statement): Given a monotonically increasing function $f:\mathbb R \rightarrow \mathbb R$, and a bounded set $A\subset \mathbb R$, I am trying to prove that $\inf(f(A))=f(\inf(A))$ where the $f(A)$ is the set defined by $f(A)=\{f(a) : \,a\in A\}$. I managed to prove that $\inf(f(A)) \geq f(\inf(A))$: Let $a\in A$. Then by definition of $\inf$: $$a\geq \inf(A)$$ Because $f$ is monotonically increasing, for any $a,b\in \mathbb R$ such that $a\geq b$, $f(a) \geq f(b)$. So: $$\forall a\in A\quad f(a)\geq f(\inf(A))$$ Which means that $$\inf(f(A))\geq f(\inf(A))$$ I am struggling with showing the reverse inequality (or show a contradiction of the strong inequality). I tried using the definition of $\inf$ with $\epsilon$ but I didn't see anything that might help with the proof. So, how might I prove the rest of the statement? (And is my proof correct, up till this point?) I have searched around and haven't found something about the image of a set (mapped by a monotone function). EDIT: It seems that the statement isn't true (an answer gives a counterexample).
So, I suppose if $f$ is continuous then it is true, but I haven't checked.","['real-analysis', 'supremum-and-infimum', 'functions']"
2623243,"Find lengths of tangents drawn from $(3,-5)$ to the Ellipse","Find lengths of tangents drawn from $A(3,-5)$ to the Ellipse $\frac{x^2}{25}+\frac{y^2}{16}=1$ My try: I assumed the point of tangency of ellipse as $P(5\cos a, 4 \sin a)$ Now Equation of tangent at $P$ is given by $$\frac{x \cos a}{5}+\frac{ y \sin a}{4}=1$$ whose slope is $$m_1=\frac{-4 \cot a}{5}$$ Also slope of $AP$ is given by $$m_2=\frac{4 \sin a+5}{5 \cos a-3}$$ So both slopes are equal , with that we get $$12 \cos a-25 \sin a=20 \tag{1}$$ Now distance $AP$ is given by $$AP=\sqrt{(3-5 \cos a)^2+(5+4 \sin a)^2}=\sqrt{75-30 \cos a+40 \sin a} \tag{2}$$ Now using $(1)$ we have to find $\cos a$ and $\sin a$ and then substitute in $(2)$ which becomes lengthy. Any better way?","['algebra-precalculus', 'conic-sections', 'tangent-line', 'geometry']"
2623247,"Computing $\lim\limits_{x\to0}\int_0^x\frac{t^2}{(x-\sin x)\sqrt{a+t}}\,dt$ without L'Hopital","I am computing the following limit
$$\lim_{x\to0}\int_0^x\frac{t^2}{(x-\sin x)\sqrt{a+t}}\,dt$$
where $a$ is a parameter. the case $a= 0$ has been resolved. But yet, for the case $ a\neq 0$ I want to  compute it without using L'Hopital rule.","['real-analysis', 'limits', 'calculus', 'limits-without-lhopital', 'analysis']"
2623248,Derivation of Variance and Mean of an Indicator Function,"Consider $Y_1, \ldots, Y_n$ random variables such that $n^{1/2}(\bar{Y}_{n} - \mu ) \xrightarrow{D} \text{iid}~ \mathcal{N}(0,\,\sigma^{2}).$ It holds that $$g_{n}(x_n(c))=1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \xrightarrow{D} 1_{[Z \leq c]}, \quad
Z \sim \mathcal{N}(0,1)$$ where $c$ is some deterministic value. Show: \begin{align}
E[g_{n}(x)] &\to P[Z \leq c] = \Phi(c)\quad (n \rightarrow \infty) \\
V[g_{n}(x)] &\to \Phi(c)(1-\Phi(c)) \quad(n \rightarrow \infty)
\end{align} Problem: How do I arrive at those expressions? Heuristically it is somehow clear due to the indicator function (Bernoulli variable). I am interested in the exact formal derivation. My Attempt: \begin{align}
\lim_n E[g_{n}(x)] &= \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \right] = \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] = P[Z \leq c] = \Phi(c) \\
\lim_n V[g_{n}(x)] &= \lim_n E \left[ 1^2_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right] - \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right]^2 \\
&= \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] - \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right]^2 \\
&= P(Z \leq c)-P(Z \leq c)^2= \Phi(c)(1- \Phi(c))
\end{align} (I think I cannot decompose the variance and look at each term separately, can I?) For clarification, I am interested if I have used the convergence in distribution property right. In particular, looking at the variance, I decompose it and apply to each term separately (3rd equality) that the distribution functions converge (due to convergence in distribution of the predictor). 
Can I do this (if yes why): $\lim_n P[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]^2 = P(Z \leq c)^2$ ?","['probability-theory', 'integration', 'statistics']"
2623261,How to divide $n$ people into two groups to maximize the probability of finding a lost boy,"A small boy is lost coming down Mount Washington. The leader of the search team estimates that there is a probability $p$ that he came down on the east side and a probability $1 − p$ that he came down on the west side. He has $n$ people in his search team who will search independently and, if the boy is on the side being searched, each member will find the boy with probability $u$. Determine how he should divide the $n$ people into two groups to search the two sides of the mountain so that he will have the highest probability of finding the boy. How does this depend on $u$? No idea where to start. The answer has couple $\log$s in it. That's from a chapter of the book where I've learned this: https://i.sstatic.net/sbGj0.png . Perhaps I'm supposed to use some kind of derivative over $p$ somewhere along the way? But then I can't really come up with what to differentiate.",['probability']
2623272,Find coefficient of $x^{20}$,"Find the coefficient of $x^{70}$ in the expansion $$(x-1)(x^2-2)(x^3-3)(x^4-4)\cdots (x^{12}-12)$$ $\mathcal {\text {Now I have solved this question}}$. What I did was I noticed that the highest power that can be formed in this expansion  would be $78$ . Hence I need to find the sum of Distinct numbers that on addition would form $8$ and I have to multiply them among themselves .
Hence the answer would be simply $$(-8) + [(-1)(-7) + (-2)(-6) + (-3)(-5)] + [(-1)(-3)(-4) + (-1)(-2)(-5)]  = 4$$ $\mathcal {\text {But what if something like coefficient of $x^{20}$ would have been asked}}$. Because the way I used would expend a lot of time to solve it.  I want to know if there are any standard methods to solve it.","['algebra-precalculus', 'combinatorics', 'binomial-coefficients', 'polynomials']"
2623293,Moser invariant curves in discrete dynamical systems and how they give stability,"I have seen a theorem which says that under certain hypothesis, given an elliptic fixed point $P$ for particular discrete dynamical systems ($X_{k+1}=S(X_k)$ with $S$ a conservative diffeomorphism not depending on time), there are invariant curves for every neighborhood of $P$ called Moser curves. Then it is concluded that if I have these curves, then the point is stable. I don't understand something though. If I consider the region between two such invariant curves, why should the internal points into internal points of the image? I mean, the boundary given by these curves stays the same because of invariance, but why does the interior points get mapped to interior points?
If I had a continous dynamical system I could say that every solution would be bound to stay in that set because otherwise it should pass through a Moser curve, violating the uniqueness of solution. But for discrete systems, why does this happen? Is it true that a diffeomorhphism sends interior points to interior points? Or is it because there is also the hypothesis of the diffeomorphism being conservative?","['diffeomorphism', 'dynamical-systems', 'chaos-theory', 'discrete-calculus', 'discrete-mathematics']"
2623306,Let $a_{n+1}=\sqrt{a_n+n}$ with $a_1=1$ then find $\lim_{n \to \infty} (a_n -\sqrt{n})$,"Let $a_{n+1}=\sqrt{a_n+n}$ with $a_1=1$ then find 
  $\displaystyle \lim_{n \to \infty} (a_n -\sqrt{n}).$ My Try:
$$\lim_n( \sqrt{a_{n}+n}-\sqrt{n}) \times \dfrac{( \sqrt{a_{n}+n}+\sqrt{n}) }{( \sqrt{a_{n}+n}+\sqrt{n})}=\lim_n \dfrac{1}{( \sqrt{\dfrac{1}{a_{n}}+\dfrac{n}{a_{n}^2}}+\sqrt{\dfrac{n}{a_{n}^2}})}.$$
Now what do I do?","['sequences-and-series', 'calculus']"
2623315,Restrict $g$ to a dense subclass in $\|f\|_p=\text{sup}\{|\int_Xfg|:\|g\|_{p'}\leq 1\}$,"We know that if $1\leq p \leq \infty$ and $f \in L^p(X)$, then $\|f\|_p=\text{sup}\{|\int_Xfg|:\|g\|_{p'}\leq 1$} where $p'$ is the dual exponent of $p$. We want to show that the same expression holds when $g$ is restricted to be a simple function (indicator functions with infinite measure support are alowed). Here is my attempt: Let $S$ be the space of space of simple functions which are  $p'$ integrable for all $1\leq p\leq\infty$. We know that $S$ is dense in $L^{p'}$. Let $g_n\in S$ be such that $g_n\rightarrow g$. We can choose $g_n$ such that $\|g_n\|\leq1$. We know that $\int_X fg_n\rightarrow \int_X fg$. This is because $\|\int_X f(g-g_n)\|_1\leq\|f\|_p\|g-g_n\|_{p'}$. Hence, $|\int_X fg_n|\rightarrow |\int_X fg|$ and the two supremums should be equal. Is this correct? 
The text which I am following says that we need to argue the case $p=1$ more carefully by using the $\sigma$-finite hypothesis, which is why I am thinking that I am doing something wrong.","['functional-analysis', 'harmonic-analysis', 'measure-theory', 'dual-spaces']"
2623352,Why not define lines in a metric space using a locus?,"If we want to talk about geometry in a metric space, we go through the following procedure to define geodesics. We define the length of a path in the (complete) space as the length given by integrating over infinitesimal distances on the path. The intrinsic metric of the space is then given by the infimum of the lengths of all paths between two points. If the intrinsic metric agrees with the metric we call the space a 'length space'; If furthermore there is a unique path between any two points whose length is the same as the distance between the points, we call that path the geodesic between the two points. We can from here go on and talk about geometry. The above definition uses that property of straight lines in the euclidean plane, that they are the shortest paths between two points, to generalize to abitrary metric spaces. This is fine and also nicely connects with the usual definition of geodesics in Riemannian geometry, however it's only applicable in a restricted class of metric spaces (length spaces with unique geodesics), and seems to rely on further analytic properties instead of propeties related only to the metric itself. However, here is an alternative defintion of straight lines in general metric spaces that seems to me much more general, applicable to many more metric spaces and also relies only on simple properties of of the metric. A further property of straight lines in the euclidean plane, besides the fact that they form the shortest path between two points, is that they are the locus of all points equidistant from two given points. Let us, then, define a straight line in a metric space as the locus of all points equidistant from two given points; I played with some equations and it seems to me that at least in taxicab space it gives the 'correct' lines. What is the problem with this definition? Why is it not used? Which properties of straight lines and the geometry are generalized to general metric spaces if we use this defintion, and which properties are lost?","['analytic-geometry', 'metric-spaces', 'metric-geometry', 'geometry']"
2623374,Is infinitely small error the same as $0$ error?,"I am really confused over the idea. If I have a circle I can approximate its area by using a regular polygon inside of it, with $n$ sides, and I can just split that polygon into triangles and compute the area. If I want better accuracy then I can use a polygon with more sides. The accuracy becomes better since the area between the polygon and circle (the ""error"") decreases as $n$ increases. So it stands to reason that if we could keep adding sides our answer would keep getting better since the error would get closer to $0$. I don't know what it means to ""add infinitely many sides"" because to me the idea doesn't make sense. No matter how many sides there are we can always add one more. But it does make sense to ask what's the value we never actually reach but get closer and closer to? For that we use the concept of a limit. The error becomes ""infinitely small"" but I don't understand what this really means. Is ""infinitely small"" the same as $0$? Because by definition it's never quite getting to $0$. But conceptually then how can we say something with nonzero error gives us an exact answer as if it had $0$ error?","['definition', 'approximation', 'calculus', 'limits']"
2623404,Prove the convergence of $\sum\limits_{k=2}^\infty \frac 1 {(2k+1)(2k+5)}$,"Hints preferred: I need to prove convergence and determine the limit of following series: $$ \sum_{k=2}^\infty \frac 1 {(2k+1)(2k+5)}$$ I need to use partial fraction decomposition to simplify the term, which I attempted with following: $$
\frac 1 {(2k+1)(2k+5)} = \frac a {(2k+1)} + \frac b {(2k+5)} \\~\\
\frac 1 {(2k+1)(2k+5)} = \frac {a(2k+5)} {(2k+1)(2k+5)} + \frac {b(2k+1)} {(2k+1)(2k+5)}  \\~\\$$ Then we have: 
$$
k: 2a + 2b = 0 \\
k^0: 5a + b = 1 \\~\\
a = \frac 1 4 ~ \land b = -\frac 1 4
$$ So we have: 
$$
\frac 1 {(2k+1)(2k+5)} = \frac { \frac 1 4 } {(2k+1)} - \frac { \frac 1 4} {(2k+5)} = \\ \frac { 1 } {4(2k+1)} - \frac { 1 } {4(2k+5)} 
$$ Which can be defined as:
$$
\frac 1 4 (\frac { 1 } {2k+1} - \frac { 1 } {2k+5} )
$$ I'm not sure how to turn the result into a telescopic series (if the partial fraction decomposition is right in the first place).","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2623440,Monotonicity of function $f(x)=\sqrt[3]{(x+1)^2}-\sqrt[3]{x^2}$,"I'm examining the properties of the function $$f(x)=\sqrt[3]{(x+1)^2}-\sqrt[3]{x^2}$$ and I'm stuck at monotonicity. I know that monotonicity depends from first derivative (if first derivative is grater, equal or lower than zero). So I found the first derivative and got $$f'(x)=\frac{2}{3}\left(\frac{1}{\sqrt[3]{x+1}}-\frac{1}{\sqrt[3]{x}}\right)$$ 
and I know it's correct. Now I need the zeroes of the derivative so I can see important points where monotonicty potentially changes. When I try to find zeroes I get: $$\frac{2}{3}\left(\frac{1}{\sqrt[3]{x+1}}-\frac{1}{\sqrt[3]{x}}\right)=0$$
$$\frac{1}{\sqrt[3]{x+1}}-\frac{1}{\sqrt[3]{x}}=0$$
$$\frac{1}{\sqrt[3]{x+1}}=\frac{1}{\sqrt[3]{x}}$$
$$x+1=x$$ which makes no sense... What's the problem here?
How can I find monotonicity of $f(x)$ when I can't find the zeroes? Is my method of finding zeroes wrong? I did this with other similar functions and was able to find zeroes without a problem and monotonicity was correct after. Edit : To clarify, $\sqrt[3]{x}$ is defined on $\Bbb R$, and the whole function is defined on $\Bbb R$.","['radicals', 'monotone-functions', 'functions']"
2623481,Arzelá-Ascoli Theorem precompact sets,"Is the Arzelá-Ascoli Theorem true in a precompact subset of $\mathbb{R}^n$?. If $S \subset \mathbb{R}^n$ is precompact and we have a sequence $(f_n)$ of functions in $C(S)$ (Space of bounded and continuous functions $f: S \to \mathbb{R}$) which is equicontinuous and uniformly bounded, is there a subsequence of $(f_n)$ uniformly convergent? I think that if $S=(a,b)$, with $a,b \in \mathbb{R}$, we can extend continuously each function $f_n$ to $[a,b]$ and apply Arzelá-Ascoli Theorem in $C([a,b])$. Thanks in advance.","['functional-analysis', 'sequence-of-function', 'real-analysis', 'arzela-ascoli']"
2623500,"Let $\dot{x}=\arctan(x(t)\cdot t)$, $x(t_0)=x_0$ be an IVP. Prove that if $x_0<0$, then $x(t)<0$ for all $\mathbb{R}$","This is a statement of a problem with $4$ sections. I've solved the other ones, but this I couldn't solve it: Let $\dot{x}=\arctan(x(t)\cdot t)$, $x(t_0)=x_0$ be an IVP. Prove that if $x_0<0$, then $x(t)<0$ for all $\mathbb{R}$. Can we say something about the solution without actually having it? How can I say how's the solution if I can't find it?","['ordinary-differential-equations', 'initial-value-problems']"
2623515,Schwartz kernel theorem and dual topologies,"We define the space of linear and continuous operators from $\mathcal{S}(\mathbb{R}^d)$ to $\mathcal{S}'(\mathbb{R}^d)$ as $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$. The Schwartz kernel theorem says that there is an isomorphism between  $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$ and   $\mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$, thanks to the relation that associated to a ""kernel"" $ K \in \mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$ the operator $\mathcal{K}$ defined by 
$$\langle \mathcal{K}\{\varphi_1\} , \varphi_2 \rangle = \langle \varphi_1\otimes \varphi_2 , K \rangle $$
for any $\varphi_1, \varphi_2 \in \mathcal{S}(\mathbb{R}^d)$. It is often only implicit that the adequate topology on the dual is the weak*-topology. Is there a good reason to consider this topology for the dual? Is the result false if one endows $\mathcal{S}'(\mathbb{R}^d)$ with the strong topology?","['functional-analysis', 'distribution-theory', 'schwartz-space']"
2623561,Find $\lim_{n\rightarrow\infty} \frac{\log(2 + 3^{n})}{2n}$,Find $$\lim_{n\rightarrow\infty} \frac{\log(2 + 3^{n})}{2n}$$ Hint: $$\log(2+3^{n}) = \log(3^{n}) + \log\left(\frac{(2 + 3^{n})}{3^{n}}\right)$$ Attempt: If I apply the hint to the expression and do a little simplification I arrive at: $$\frac{n \log(3)}{2n} + \frac{\log\left(\frac{\log(2 + 3^{n})}{3^{n}}\right)}{2n}$$ Now if I take the limit of this expression: $$\lim_{n\rightarrow\infty} \frac{ \log(3)}{2} + \lim_{n\rightarrow\infty} \frac{\log\left(\frac{\log(2 + 3^{n})}{3^{n}}\right)}{2n}$$ Here is where I am stuck. I feel I can argue that the limit of the second term goes to 0 because the denominator will go to infinity faster than the numerator. As such I am left with $\frac{\log(3)}{2}$ as my answer. My only concern is that I could've used that same  train of thought with the original expression. Advice?,"['real-analysis', 'limits', 'logarithms', 'calculus', 'sequences-and-series']"
2623602,The need for rigorous proofs in Sets,"I am a $10$th grade student who is studying Elementary Set Theory. In a lot of questions, I come across, the solution or rather the proof requires being proved by identities or by assuming an element of a set, i.e $a \in \{A\}$ and so on. What I am unable to understand is why we require rigorous proofs for Sets Theory and to broaden the scope of the question why do we require rigorous proofs at all in Mathematics?","['soft-question', 'elementary-set-theory']"
2623608,"Differential Equation : $\frac{dy}{dx} +\int_0^5{y\,dx}=27$","Given $y=f(x)$ , is twice differentiable, passes through the origin and satisfies the equation, $$\frac{dy}{dx} +\int_0^5{y\,dx}=27$$What is the probability that $2$ randomly chosen variables $a$, $b$ from the set $S=\{1,2,3,4\}$ lies on the curve as $(a,b)$? My Attempt: As $\int_0^5{y\,dx}$ is a constant. $$\frac{d^2y}{dx^2}=0$$ Therefore, $y=ax+b$, As curve passes through the origin, $b=0$, so $y=ax$. On putting this in the equation, we get $a=2$. Therefore, $$y=2x.$$ Is my approach right? And also how else can we attempt this question?","['integration', 'probability', 'ordinary-differential-equations']"
2623708,Express $2\tan^{-1}x$ in terms of $\sin^{-1}$ and $\cos^{-1}$,"$$
2\tan^{-1}x=\begin{cases}\tan^{-1}\frac{2x}{1-x^2} &\mbox{ if $|x|<1$}\\
\pi+\tan^{-1}\frac{2x}{1-x^2} &\mbox{ if $|x|>1$}
\end{cases}
$$ Similarly, $$
2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2} &\mbox{ if $|x|\leq1$}\\
\text{____________} &\mbox{ if $|x|>1$}
\end{cases}
$$ and $$
2\tan^{-1}x=\begin{cases}\cos^{-1}\frac{1-x^2}{1+x^2} &\mbox{ if $x\geq0$}\\
\text{____________} &\mbox{ if $x<0$}
\end{cases}
$$ How do I derive the missing cases ? My Attempt: Take $\tan^{-1}x=A\implies x=\tan A$ where $\frac{-\pi}{2}<A<\frac{\pi}{2}\implies -\pi<2A<\pi$. Case 1 : $$
\cos2A=\frac{1-\tan^2A}{1+\tan^2A}=\frac{1-x^2}{1+x^2}=\cos\Big(\cos^{-1}\frac{1-x^2}{1+x^2}\Big)\\
\implies \cos^{-1}\frac{1-x^2}{1+x^2}=2n\pi\pm2A=2n\pi\pm2\tan^{-1}x
$$
As $0\leq\cos^{-1}\leq\pi$, If $0\leq2A\leq\pi$
$$
\cos^{-1}\frac{1-x^2}{1+x^2}=2A=2\tan^{-1}x
$$ If $-\pi<2A<0$
$$
\cos^{-1}\frac{1-x^2}{1+x^2}= \color{red}{\text{?}}
$$ Case 2 : $$
\sin2A=\frac{2\tan A}{1+\tan^2A}=\frac{2x}{1+x^2}=\sin\Big(\sin^{-1}\frac{2x}{1+x^2}\Big)\\
\implies \sin^{-1}\frac{2x}{1+x^2}=n\pi+(-1)^n.2A=n\pi+(-1)^n.2\tan^{-1}x
$$
As $\frac{-\pi}{2}\leq\sin^{-1}\leq\frac{\pi}{2}$, If $\frac{\pi}{2}\leq2A\leq{\pi}$
$$
\sin^{-1}\frac{2x}{1+x^2}= \color{red}{\text{?}}
$$ what value should $\cos^{-1}\frac{1-x^2}{1+x^2}$ and $\sin^{-1}\frac{2x}{1+x^2}$ take and how do I proceed further ? My Understanding: For,
$$
\tan2A=\tan\big(\tan^{-1}\frac{2x}{1-x^2}\big)\implies\tan^{-1}\frac{2x}{1-x^2}=n\pi+2A
$$
If $\frac{\pi}{2}<2A<{\pi}$ I would take
$$
\tan^{-1}\frac{2x}{1-x^2}=-\pi+2A=-\pi+2\tan^{-1}x
$$","['trigonometry', 'inverse-function']"
2623777,Limit: $\lim_{x\to2} \frac{1}{x-2}\cdot \sin\left(\frac{x-2}{x+2}\right)$,"Can someone help me understand how the solution for the following limit is $1/4$? I've been trying to solve it but I always end up in a 'dead end' with an 
indetermination. If someone could help me, that would be awesome. $$\lim_{x\to2} \frac{1}{x-2}\cdot \sin\left(\frac{x-2}{x+2}\right)$$","['calculus', 'limits']"
2623796,Prove that $A \smallsetminus (A \smallsetminus B) = A \cap B$,"$A$ and $B$ are any sets, prove that $A \smallsetminus (A \smallsetminus B) = A \cap B.$ This formula makes sense when represented on a Venn diagram, but I am having trouble with proving it mathematically. I have tried letting $x$ be an element of $A$ and continue from there, but it doesn't seem to work out as a valid proof anyways. Could anyone please point me in the right direction? Many thanks.",['elementary-set-theory']
2623856,Determine the distribution function of this density function,"Given is the density function $f(x)=\left\{\begin{matrix} \frac{1}{\pi}\frac{1}{1+x^2}\text{ if }x\geq 0\\  \frac{1}{2}e^x \;\;\;\;\text{ else} \end{matrix}\right.
\;\;\;\;$ Determine the (cumulative) distribution function from this density
  function. I'm not quite sure how this is done correctly but I need to know it because I need this for another thing I wanted calculate :p If I understood correctly, you determine the distribution function by taking the integral of the density function. So we have that $$F(x) = \int_{-\infty}^{x}f(t) dt$$ Now we need to cover all cases: $x < 0$: $$F(x) = \int_{-\infty}^{0}\frac{1}{2}e^t dt = \left[\frac{1}{2}e^t\right]_{-\infty}^{0}=\frac{1}{2}-(0)=\frac{1}{2}$$ $x \geq 0$: $$F(x)=\int_{0}^{\infty}\frac{1}{\pi} \cdot \frac{1}{1+t^2}dt = \left[\frac{1}{\pi} \cdot \arctan(t)\right]_{0}^{\infty}=\left(\frac{1}{\pi} \cdot \frac{\pi}{2}\right)- \left(\frac{1}{\pi} \cdot 0\right)=\frac{1}{2}$$ Is it really correct like that? Because if this is wrong my next calculation will be wrong too! :(","['statistics', 'probability', 'random-variables', 'probability-distributions']"
2623906,If $[K(\alpha):K]=p\neq q=[K(\beta):K]$ then $[K(\alpha+\beta):K]=pq$,"I am having some trouble with the following problem: Let $K\subseteq L$ be fields. Suppose $\alpha,\beta\in L$ are algebraic elements over $K$ of degrees $p,q$ respectively, where $p$ and $q$ are distinct primes. Show that $\alpha+\beta$ is algebraic over $K$ with degree $pq$. I have shown that $\alpha+\beta$ is algebraic over $K$, but I am having trouble showing that the degree is $pq$. I have previously shown that $[K(\alpha,\beta):K]=pq$, so I have been trying to use this result. I thought that if I could show that $K(\alpha,\beta)=K(\alpha+\beta)$ then I'd be done. To show this I first noted that clearly $K(\alpha+\beta)\subseteq K(\alpha,\beta)$. Then we have
$$pq=[K(\alpha,\beta):K]=[K(\alpha,\beta):K(\alpha+\beta)][K(\alpha+\beta):K]$$
so that $[K(\alpha+\beta):K]=1,p,q,pq$. I tried looking at the cases when it is equal to $1,p,q$ and deriving a contradiction, but have been unsuccessful. Another way I thought of solving this is to show that $\alpha,\beta\in K(\alpha+\beta)$, which would allow me to conclude that $K(\alpha+\beta)=K(\alpha,\beta)$, but I am not sure how to complete this either. I am looking for some assistance to show that $[K(\alpha+\beta):K]\neq 1,p,q$, or that $\alpha,\beta\in K(\alpha,\beta)$. If you have any other solutions to this problem I would like to see those too.","['abstract-algebra', 'extension-field', 'field-theory']"
2623910,Coefficients of the stirling's series expansion for the factorial.,"Knowing the Stirling's approximation for the Gamma function (factorial) for integers:
$$\Gamma(n+1)=n!\approx \sqrt{2\pi n}n^ne^{-n}\bigg(1+\frac{a_1}{n}+\frac{a_2}{n^2}+\cdots\bigg)$$
Using the above approximation one can write:
$$(n+1)!=\sqrt{2\pi(n+1)}(n+1)^{n+1}e^{-(n+1)}\bigg(1+\frac{a_1}{n+1}+\frac{a_2}{(n+1)^2}+\cdots\bigg)$$
We know that following recursion holds:
$$(n+1)!=(n+1)n!$$
One can rewrite this:
$$(n+1)!=(n+1)\sqrt{2\pi n}n^ne^{-n}\bigg(1+\frac{a_1}{n}+\frac{a_2}{n^2}+\cdots\bigg)$$
All this comes from: https://www.csie.ntu.edu.tw/~b89089/link/gammaFunction.pdf (Page 8-9)
Then the author gives this expansion to calculate the $a_k$ coefficients when $n$ becomes large.
Comparing these two expressions for $(n+1)!$ gives
$$1+\frac{a_1}{n}+\frac{a_2}{n^2}+\cdots=\bigg(1+\frac{1}{n}\bigg)^{n+1/2}e^{-1}\bigg(1+\frac{a_1}{n+1}+\frac{a_2}{(n+1)^2}+\cdots\bigg)$$
Then he says, that after ""classical series expansion"" this equals:
$$1+\frac{a_1}{n}+\frac{a_2-a_1+\frac{1}{12}}{n^2}+\frac{\frac{13}{12}a_1-2a_2+a_3+\frac{1}{12}}{n^3}+\cdots$$
I don't understand how he got there. Only thing that came to my mind was, that as $n\to\infty$ $\big(1+\frac{1}{n}\big)^n$ goes to $e$ but then we are left with $\big(1+\frac{1}{n}\big)^{1/2}$. When i expand this into binomial series, i get:
$$\bigg(1+\frac{1}{2n}-\frac{1}{8n^2}+\frac{1}{16n^3}\cdots\bigg)\cdot\bigg(1+\frac{a_1}{n+1}+\frac{a_2}{(n+1)^2}+\cdots\bigg)$$
And I'm stuck here. Or is there any other elementary way how to compute the coefficients $a_k$ of the stirling's series expansion for factorial/Gamma function?","['asymptotics', 'sequences-and-series', 'factorial', 'power-series', 'gamma-function']"
2623936,"How to prove that $|\ln(2+\sin(x)) - \ln(2+\sin(y))| <= |x-y| \space \forall \space x,y \in \mathbb R$","Question states Prove that for all $x$ and $y$ $\in R$, the following inequality is true: $\lvert \ln(2+\sin(x)) - \ln(2+\sin(y))\rvert \le \lvert x-y\rvert$ i've gotten to the point that $\frac{y-x}{2+\sin(c)}  = \ln\frac{2+\sin(y)}{2+\sin(x)}$  (y-x divided by 2+sin(c) is my f dash c from the mean value theorem I asked my teacher that i should use mean value theorem here so please don't use anything other than this, but i have no idea how to push this problem further. Also this is my first post so i'm sorry for the f dash (c) thing, mathjax is hard","['derivatives', 'inequality', 'calculus']"
2623952,Complex Analysis - Limits,"Does the following limit exist? 
$$\lim_{z\rightarrow 0}\frac{Re(z)^2}{|z|^2}$$
Here, Re means the real part of the function. This is what I have so far: In order for the limit to exist, z must be allowed to approach $0$ from any direction. With z approaching $0$ through values $z=x+i0$:
$$\lim_{x\rightarrow 0}\frac{x^2-0}{x^2-0}=1$$
With z approaching $0$ through values $z=iy$ :
$$\lim_{y\rightarrow 0}\frac{-y^2-0}{y^2-0}=-1$$
Since the two limits are different, the limit does not exist. Does this logic make sense?","['complex-analysis', 'limits']"
2623966,How is the equation $z=xy$ a hyperbolic paraboloid?,The general equation for a hyperbolic paraboloid is $z - z_{0} = \frac{(x - x_{0})^{2}}{a^{2}} -\frac{(y - y_{0})^{2}}{b^{2}}$. How is the equation $z=xy$ a hyperbolic paraboloid? Is there any algebra that can be applied to it to get its general form?,"['multivariable-calculus', 'hyperbolic-geometry']"
2623978,Relative volume of $\delta$-fattening (neighborhood) of a compact set,"For a non-empty, compact set $A \subseteq \mathbb{R}^n$, the $\delta$-fattening of $A$, $A_\delta$, is defined to be the set
$$
A_\delta = \cup_{a \in A} B_{\delta}(a),
$$
where $B_\delta(a)$ denotes the closed ball centered at $A$ with radius $\delta$. Is it possible to establish an upper bound on $\mu(A_\delta)$ in terms of $\mu(A)$? In a previous post , it was claimed that: Claim Let $A$  be a nonempty compact subset of $\mathbb R^n$ with $\mu(A)>0$. Then for all $\delta>0$ $$\mu(A_\delta)\le \left(1+\delta\,\frac{\lambda(\partial A)}{n\,\mu(A)}\right)^n\mu(A)\tag1,$$ $A_\delta$ is the $\delta$-fattening of the set $A$, and $\lambda(\partial A)$ is the Minkowski content $$\lambda(\partial A)=\liminf_{\delta\to 0}\delta^{-1}(\mu(A_\delta)-\mu(A))\tag2.$$
However, the proof utilized the fact that $f(\delta) = \left(\mu(A_\delta)/\mu(A)\right)^{1/n}$ is concave, which per this post is not true. Can the claim above or a similar inequality be established?","['volume', 'lebesgue-measure', 'geometric-measure-theory', 'measure-theory', 'general-topology']"
2623989,Are the canonical representatives of the Hilbert space $L^2$ basis-dependent?,"The space $\mathcal{L}^p(\mathbb{R}^n)$ of functions $f$ such that $\int |f(x)|^p\, d^nx$ converges is only a seminormed rather than a normed vector space, because any function $f$ whose support has Lebesgue measure zero has $||f||_p := \int |f(x)|^p\, d^nx = 0$, even if $f$ is not identically zero. In order to turn it into a normed vector space $L^p(\mathbb{R}^n)$ , we need to mod out by the kernel of the $p$-norm, which is the set of functions whose support has Lebesgue measure zero (or equivalently, we need to identify functions that agree almost everywhere). This raises the natural question of whether this quotient space $L^p(\mathbb{R}^n)$ has a natural section ; that is, whether for every equivalence class $[f] \in L^p(\mathbb{R}^n)$ there is a natural canonical representative square-integrable function $f \in \mathcal{L}^p(\mathbb{R}^n)$. Is there generally a natural section of $\mathcal{L}^p(\mathbb{R}^n)$ space? (I'm using the word ""natural"" loosely, not in any kind of mathematically precise sense.) For the Hilbert space $L^2(\mathbb{R}^n)$ with a given orthonormal basis $\{\phi_n(x)\}$, there does seem to be a natural canonical representative $f_c \in [f]$, given by the generalized Fourier series for $[f]$:
$$f_c(x) := \sum_{n=0}^\infty \langle f, \phi_n \rangle \phi_n, \qquad \qquad \langle f, \phi_n \rangle := \int_{\mathbb{R}^n} f(x)\, \phi_n(x)\, d^nx.$$
Does this choice of section depend on the choice of basis $\{ \phi_n \}$? (Obviously it would be ""nicer"" if it didn't.)","['fourier-analysis', 'hilbert-spaces', 'functional-analysis', 'lp-spaces', 'lebesgue-integral']"
2623990,Flow invariance of submanifold implies that tangent subspace is unvariant under the action of differential map,"Suppose that you have a system of ODEs $$\dot{x} = F(x), \, x \in \mathbb{R}^n,$$
and $x_0$ is a an equilibrium point of this system, i.e. $F(x_0) = 0$. Suppose you have a locally invariant smooth submanifold $\mathcal{M}$ that passes through $x_0$. By locally invariant I mean that for any point $p \in \mathcal{M}$ there exists an $\varepsilon > 0$ such that $\varphi^{t}(p) \in \mathcal{M}$ for $\lvert t \rvert < \varepsilon$, where $\varphi^t$ is the flow defined by a system of ODEs. I believe that if these conditions are met, then $DF(x_0) (T_{x_0}\mathcal{M}) \subset T_{x_0}\mathcal{M}$; $DF(x_0)$ is a Jacobi matrix for $F(x)$ at point $x_0$. It feels like I had encountered similar statement somewhere before, but I can't remember the exact reference. I can't find a counter-example or a proof for this statement from the get-go, though I definitely want to crack that puzzle if there is no reference to it.  If this statement was true, it would much simplify one of my proofs, so that's the reason why I'm so interested in it. So, mostly I'm interested in the reference where this (or very similar) statement had been proved. If the statement is false and you know a counter-example, that would be very useful (although a bit heartbreaking). ADDED LATER I have a sketch of the proof for this statement (kind of). If we pick any vector $v \in T_{x_0}\mathcal{M}$ there exists a curve $\gamma_v(s)$ such that $\gamma_v'(0) = v, \gamma_v(0) = x_0,$ and which lies in $\mathcal{M}$. If we choose some $\tau$ and apply $\varphi^{\tau}$ to $\gamma_v(s)$ we will obtain some curve $\tilde{\gamma}_v(s)$ that still lies on $\mathcal{M}$ and $\tilde{\gamma}_v(0) = x_0$. Thus tangent vector to this curve has to lie in $T_{x_0}\mathcal{M}$. But this tangent vector is nothing but $DF(x_0)(v)$ and this implies that $\forall v \in T_{x_0}\mathcal{M}$ follows $DF(x_0)(v) \in T_{x_0}\mathcal{M}$.","['reference-request', 'differential-geometry', 'dynamical-systems']"
2623991,Why is it $|f(x) - L| < \epsilon$ in the definition for a limit and not $0 < |f(x) - L| < \epsilon$?,"In the epsilon-delta definition of a limit it says $0 < |x - a| < \delta$ must exist, i.e. the distance between $x$ and $a$ must be positive. And then this leads to the implication that $|f(x) - L| < \epsilon$ holds where $\epsilon > 0$. But why isn't it $0 < |f(x) - L| < \epsilon$? I thought limits were all about getting closer to a $y$ as you narrow in on an $x$. This definition implies that it is possible to bring $x$ near $a$ and yet somehow the limit $L$ is can possibly equal $f(x)$ as opposed to just getting closer and closer to it.","['definition', 'epsilon-delta', 'calculus', 'limits']"
2623999,Find out if $\hat{\tau}$ is an unbiased estimator,"I got given a pdf: $$f(x)=\tau x \exp\left(\frac{-\tau x^2}{2}\right)$$
$x,\tau >0$
I found $$E(X)=\sqrt{\frac{\pi}{2\tau}}$$
And I used the method of moments method to find:
$$\hat{\tau}=\frac{\pi}{2\bar{x}^2}$$
Now, from what I found out is that an estimator is unbiased if $E(\hat{\tau})=\tau$, but I have:
$$E(\hat{\tau})=E\left(\frac{\pi}{2\bar{x}^2}\right)=\frac{\pi}{2}E\left(\frac{1}{\bar{x}^2}\right)$$ and I have no idea how to calculate $E\left(\frac{1}{\bar{x}^2}\right)$","['means', 'statistics']"
2624005,"Find the points that are closest and farthest from $(0,0)$ on the curve $3x^2-2xy+2y^2=5$","Find the points that are closest and farthest from $(0,0)$ on the curve $3x^2-2xy+2y^2=5$ My attempt: So, I'm looking to find global extrema of the function $f=x^2+y^2$ (since square root is a motonous function it has extrema at same points as this function). The set $S = \{ (x,y)\in \mathbb{R} : 3x^2 -2xy + 2y^2 -5 = 0\}$ is not compact, therefore I cannot guarantee that $f$ will have a minimum or a maximum on $S$. Anyways by applying the usual theorem, gradient of $f$ is colinear with the gradient of $g$. (gradient of g is $0$ only if $x=y=0$ and that point is not in $S$ therefore not relevant so the set containing only gradient of $g$ is linearly independent).Let the coefficient be noted by $\lambda$. I'm not sure how to go about this from now though. I have 3 equations that each contain $x,y,xy,\lambda x,\lambda y$: (if my calculations were right) $$\lambda (-3x+y) + x=0$$ $$\lambda (x-2y)+y=0$$ $$3x^2 -2xy + 2y^2 - 5 = 0$$ I'm stuck here and I'm not sure what to do.Hints would help! Thanks in advance!","['optimization', 'discriminant', 'multivariable-calculus', 'lagrange-multiplier', 'quadratics']"
2624023,"Does every equation involving $+,-,\times,\div,\sqrt{},\mathbb Q$ only have solutions in the algebraic numbers","Let's say you have an equation $f(x)=0$ where $f:\mathbb R \to \mathbb R$ is composed of $$+,-,\times,\div,\sqrt{},n \in \mathbb Z$$
If $f$ has only finitely many solutions, are all the solutions algebraic? I've tried (structural) induction. The cases $n$, $\div$ and $\times$ are easy. $+$ and $-$ are not. I've been trying to reduce it to a polynomial equation. Problem is that doing $\sqrt{A}=B \implies A=B^2$ makes the RHS more complicated. If the RHS contains sums of square roots then this doesn't make progress. (Maybe I could do more work here?) I don't see how to use the fact the algebraic numbers are closed under the above operations. I've been trying to think of a counterexample.","['number-theory', 'field-theory']"
2624025,"""Continuity"" of volume function on hyperbolic tetrahedra","Consider a sequence $T_i$ of tetrahedra in $\mathbb H^3$ whose
  vertices tend to the vertices of a regular ideal tetrahedron $T$ in
  $\partial \mathbb H^3$. Then $$Vol(T_i)\to v_3.$$ This should follow from Lebesgue dominated convergence if $T_i\subseteq T_{i+1}$ for (almost) all $i$, since, calling $\nu$ the volume form on $\mathbb H^3$, $$|\nu\chi_{T_i}|\leq|\nu\chi_T|$$ so the integrals converge. I think one can always suppose to be in this case by moving the $T_i$ by isometries: is this true? Is there a formally satisfying way to see it? Thank you in advance.","['hyperbolic-geometry', 'differential-geometry', 'geometric-topology']"
2624061,Inequality involving independent random variables [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question We have independent random variables $X$ and $Y$. Why does the following equality hold?
$$P(X>Y)=\int P(X>Y\mid Y=z)f_Y(z) \, dz,$$
where $f_Y$ is the density of $Y$. Any ideas?","['probability-theory', 'statistics']"
2624078,"Looking for a ""soft"" book on second semester real analysis?","I am starting my $2^{nd}$ undergrad analysis class in which we will cover chapters 1-4 of Stein's book (includes Measure Theory, Lebesgue Integration, Dominated Convergence Theorem, Hilbert Spaces, ...). I have looked at the textbook and it looks traditional; this is good, but I am looking for a secondary book which is more informal/visual/intuitive which will help me understand the material better and build more intuition. The following are examples of books from other fields which are comparable to what I'm looking for now: Visual Complex Analysis by Tristan Needham for Complex Analysis, Understanding Analysis by Stephen Abbott for basic Real Analysis, and The Art and Craft of Problem Solving by Paul Zeitz. Thank you. Edit: One explicit characteristic that I'm looking for is that the book gives details on how one might have come up with a proof, not just gives the most polished version of the proof.","['real-analysis', 'reference-request', 'book-recommendation', 'measure-theory', 'soft-question']"
2624127,Are functions considered continuous at endpoints?,"Consider a function $f(x)$ that has no jump, infinite, or removable discontinuities in the middle anywhere -- but maybe the domain is limited: Would the endpoint $[a,$ be considered continuous? What about the endpoint $(a,$? I ask because I often see ""a function is continuous if we can draw it without lifting up the pencil"" but I didn't know to what extent this applies to the endpoints and whether or not it matters if the points themselves are defined.","['continuity', 'definition', 'calculus', 'limits']"
2624212,Why is my work incorrect?,"As a homework question, I am asked to answer the following question: In how may ways can $8$ people be seated in a row if $(a)\,$ $5$ are women and they must sit next to one another? $(b)\,$ there are $4$ married couples and each couple must sit together? $\mathbf{(\mathit{a})}$
 The way I approached the problem is by considering the 5 women seating next to each other as a single element, call it $W$, which leaves me with $3$ other elements $M_1,M_2$ and $M_3$. Therefore, by defining $k$ as the set containing the above elements, I have:
$$k=\left\{W,M_1,M_2,M_3\right\}.$$
The number of ways that the elements in $k$ can be arranged is $4!$. Now, defining $W=\left\{W_1,W_2,W_3,W_4,W_5\right\}$, the women can be arranged with each other in $5!$ ways. Therefore, the solution, in my opinion, should be $5!\cdot4!=2880$. However, according to the instructor it should be $5760$. $\mathbf{(\mathit{b})}$ With the same logic as before, the $4$ married couples can be arranged with each other in $4!$ ways and each couple can be arranged in $2!$ ways. Being that there are $4$ couples, the solution is $4!\cdot2!\cdot2!\cdot2!\cdot2!=4!\cdot2^4=384.$ Once again, my solution does not match with the one given to me, which is $192$. It is not clear to me what I am doing wrong since I thought of these problmes as being very trivial. Any help is appreciated! Thank you!",['combinatorics']
2624227,Asymptotic behavior of zeroes of $\ln(t)-t+n$,"This is related to this question. Consider the sequence of functions $(f_n)_{n\geq 2}$ defined on $[1,\infty)$ by
$$f_n(t)=\ln(t)-t+n.$$
Those functions are decreasing on $[1,\infty)$ with $f_n(1)=n-1$ and $\lim_{t\to \infty}f_n(t)=-\infty$. So by intermediate value theorem, there exists a sequence of zeroes located in $[1,\infty)$, that is, $t_n>1$ and $f_n (t_n)=0$ for all $n\geq2$. I am interested in the asymptotic behavior of the sequence $(t_n)_n$, specially finding an equivalent of this sequence at infinity. Here's what I tried: $\ln(t_n)-t_n+n=0$ and $t_n>1$ is equivalent to 
$$t_n=e^{t_n-n}>1$$ which implies that $t_n>n$, so already the sequence diverges to infinity. Now again
$$t_n=e^{t_n-n}>n$$
which implies that
$$t_n>n+\ln(n)$$
repeating this argument we can find that
$$t_n>n+\ln(n+\ln(n+\ln(n+\dots)))$$
I don't know if this path leads to something. I still feel I need an upper bound to find an equivalent.","['limits', 'logarithms', 'asymptotics', 'calculus', 'sequences-and-series']"
2624273,Finding the limit of a multivariable function,"So I'm trying to find this limit:
$$\lim_{(x,y)\to(0,0)}\frac{\sin(x^2y^2)}{(x^2+y^2)^{3/2}}$$ What I've tried so far is setting the value up for the sandwich theorem: $$0\leq \Big|\frac{sin(x^2y^2)}{(x^2+y^2)^{3/2}}\Big|\leq\Big|\frac{sin(x^2y^2)}{(2x^2y^2)^{3/2}}\Big|$$
(using AM-GM inequality) What troubles me is that I can't find a way to prove that this is less then/equal to zero. Thanks in advance.",['limits']
2624285,"Does this Riemann integral over $[0,1]^2$ exist?","While waiting for responses to this question , I did some research and came across this function on $[0,1]^2$: $f(x,y) = 0$ if $x$ or $y$ is irrational and $f(x,y) = 1/q$ if $x$ and $y$ are rational and $x = p/q$ in lowest terms. Its claimed that the double Riemann integral $\int_{[0,1]^2}f $ exists since $f$ is continuous almost everywhere, but if $x$ is rational then $\int_0^1 f(x,y)dy $ does not exist as a Riemann integral. I understand the second part since $f(x,y)$ looks like the Dirichlet function (when $x =p/q$ fixed) alternating between $1/q$ and $0$ for rational and irrational $y$.  Just because $f$ alternates between $0$ and a variable non-zero value off and on a rational grid does not make it completely obvious  about the continuity. So I would like to see how to prove the first part directly using Darboux sums.","['real-analysis', 'riemann-integration', 'multiple-integral', 'multivariable-calculus', 'integration']"
2624300,Complex Analysis - Harmonic function as real part of holomorphic function [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How would I prove the following? If $\Omega$ is simply connected, in $\mathbb C$, and if $u$ is a harmonic function in $\Omega$ , prove that there exists a holomorphic function $f$ in $\Omega$ such that $Re(f) = u$.","['complex-analysis', 'holomorphic-functions', 'harmonic-functions']"
2624306,Show that a function is increasing,"I have the following question: Show that if j > 0, then the function f (m) = (1 + j/m) m is an increasing function of m . Clearly, I have to derive the function and I obtain f '(m) = (j/m + 1) m [log(j/m + 1) - j/(m (j/m + 1))] However, I can't see how to show that this is strictly positive, thanks for your help.","['calculus', 'functions']"
2624348,Do unitarily equivalent operators have the same spectrum?,"First, some definitions: We say that two bounded operators on Hilbert spaces $T:H_1\to H_1$ and $S:H_2\to H_2$ are unitarily equivalent if there is an unitary operator $U:H_1\to H_2$ such that $T=U^*SU$ (where the supscribed $*$ represent the Hilbert adjoint). I'd already proved the following fact: If $T$ and $S$ are unitarily equivalent self-adjoint operators, then they have the same spectrum. Now... what if they are not self-adjoint? I was looking for some sufficient conditions (such as this ) and, more important, counterexamples : two unitarily equivalent operators that don't share the same spectrum. I tried to look for some simple examples (finite ranked and even compact infinite ranked), but had no success.","['functional-analysis', 'spectral-theory', 'operator-theory', 'adjoint-operators']"
2624411,Mathematicians' Tensors vs. Physicists' Tensors,"It seems, at times, that physicists and mathematicians mean different things when they say the word ""tensor."" From my perspective, when I say tensor, I mean ""an element of a tensor product of vector spaces."" For instance, here is a segment about tensors from Zee's book Einstein Gravity in a Nutshell : We already saw in the preceding chapter
  that a vector is defined by how it transforms: $V^{'i}
 = R^{ij}V^j$ . Consider a collection of “mathematical
  entities” $T^{ij}$ with $i , j = 1, 2, . . . , D$ in $D$-dimensional space. If they transform
  under rotations according to
  $T^{ij} \to T^{'ij} = R^{ik}R^{jl}T^{kl}$ then we say that $T$ transforms like a tensor. This does not really make any sense to me. Even for ""vectors,"" and before we get to ""tensors,"" it seems like we'd have to be given a sense of what it means for an object to ""transform."" How do they divine these transformation rules? I am not completely formalism bound, but I have no idea how they would infer these transformation rules without a notion of what the object is first . For me, if I am given, say, $v \in \mathbb{R}^3$ endowed with whatever basis, I can derive that any linear map is given by matrix multiplication as it seems the physicists mean. But, I am having trouble even interpreting their statement. How do you derive how something ""transforms"" without having a notion of what it is? If you want to convince me that the moon is made of green cheese, I need to at least have a notion of what the moon is first. The same is true of tensors. My questions are: What exactly are the physicists saying, and can someone translate what they're saying into something more intelligible? How can they get these ""transformation rules"" without having a notion of what the thing is that they are transforming? What is the relationship between what physicists are expressing versus mathematicians? How can I talk about this with physicists without being accused of being a stickler for formalism and some kind of plague?","['tensor-products', 'differential-geometry', 'linear-algebra', 'geometry']"
2624439,Riemannian metric with specified totally geodesic submanifolds,"Here's a motivating question: suppose $N_1, N_2$ are transversely intersecting submanifolds of $M$ and let $p$ be a point in the intersection. Does there exist a chart $(U, \varphi)$ of $M$ at $p$ such that the diffeomorphism $\varphi : U \to \Bbb R^n$ takes $N_1 \cap U$ and $N_2\cap U$ to two transverse subspaces of $\Bbb R^n$, i.e., $\varphi(N_i \cap U) = H_i$ where $H_i \subset \Bbb R^n$ are transverse subspaces of $\Bbb R^n$ with $\dim H_i = \dim N_i$? This is more or less asking if, similar to the immersion and submersion theorem which are essential to differential topology, transversality also have a local model based on the picture $(\Bbb R^n, H_1, H_2)$ consisting of two transverse subspaces $H_1,H_2 \subset \Bbb R^n$. There are probably easier proofs of this fact, but my sketch was as follows. Suppose we impose a Riemannian metric $g$ on $M$ so that $N_1$ and $N_2$ becomes totally geodesic. Consider the exponential map $\exp_p : T_p M \dashrightarrow M$. Since $N_1$ and $N_2$ are transverse submanifolds, there's a decomposition $T_pM = T_pN_1 + T_pN_2$, and $\exp_p$ restricts to the two factors of this decomposition (for any $v \in T_p N_i$ in the domain of definition, $\exp_p(tv)$ is the geodesic starting at $p$ with initial vector $v$. Under the induced Riemannian metric there is also a geodesic $\gamma$ of $N_i$ starting at $p$ with initial vector $v$ - since $N_i$ is totally geodesic $\gamma$ is also a geodesic of $M$, so by uniqueness we have to have $\gamma(t) = \exp_p(tv)$, forcing $\exp_p(v)$ to map to $N_i$.) Since $\exp_p$ is a local diffeomorphism at the origin, one can choose a ball $B$ around $0$ of radius $\varepsilon$ in $T_pM$ so that $\exp_p : B \to M$ is a diffeomorphism which sends the transverse subspaces $H_i := T_p N_i \cap B$ diffeomorphically to $N_i$, as desired This brings me to the question of the title: Given a finite family $N_\alpha$ of mutually transversely intersecting submanifolds of $M$, when is it possible to impose a Riemannian metric $g$ on $M$ such that $N_\alpha$ are totally geodesic submanifolds of $(M, g)$? For a single submanifold $N \subset M$, this should be doable as follows: take the normal bundle $\pi: E \to N$. Choose a metric $g_N$ on the base and a bundle metric $\tilde{g}$ for $\pi$. Moreover choose a connection on $\pi$ compatible with the bundle metric so that there's a canonical decomposition $TE \cong H \oplus V$ into horizontal and vertical subbundles. Define the bundle metric on $d\pi : TE \to TM$ as $g_E((x_h, x_v), (y_h, y_v)) = g_N(x_h, y_h) + \tilde{g}(x_v, y_v)$ where the subscripts denote the horizontal($h$) or vertical($v$) coordinates respectively. This makes the total space into a Riemannian manifold $(E, g_E)$ and indeed one so that the map $\iota : E \to E$ defined by $\iota((x, v)) = f(x) \cdot v$ for a chosen scalar function $f : N \to\Bbb R$ is an isometry. This is nothing but scaling fiberwise by a constant, which leaves the zero section $E_0 \subset E$ fixed. Fixed point loci of isometries of a Riemannian manifold are totally geodesic submanifold, so this guarantees $E_0 \subset E$ is totally geodesic. But by the tubular neighborhood theorem, there is a neighborhood $U$ of $N \subset M$ such that $U$ is diffeomorphic to a neighborhood of the zero section $E_0 \subset E$. Therefore $U$ admits a metric $g_U$ obtained from pulling back $g_E$ in which $N$ is a totally geodesic submanifold of $(U, g_U)$. Let $V := M \setminus N$ and give $V$ an arbitrary Riemannian metric. Use partition of unity on the cover $\{U, V\}$ of $M$ to glue these two metrics to a metric $g$ on $M$ such that $N \subset (M, g)$ is totally geodesic. One immediately runs into trouble trying to generalize this argument for multiple transverse submanifolds of $M$ as the glued metrics would change near the points of intersection, no longer assuring total geodesicity. Is this true in general? If so, what would be a proof?","['riemannian-geometry', 'differential-geometry', 'differential-topology']"
2624440,How to Find Moore Penrose Inverse,"I have a matrix: 
$$A=
    \begin{bmatrix}
    -1 & 0 & 1 & 2 \\
    -1 & 1 & 0 & -1 \\
    0 & -1 & 1 & 3 \\
    0 & 1 & -1 & -3 \\
    1 & -1 & 0 & 1 \\
    1 & 0 & -1 & -2 \\
    \end{bmatrix}
$$
I want to find a Moore Penrose generalized inverse for it, but I only know how to do this using computer software. 
My attempt: I think the inverse is supposed to be of the form 
$$C'(CC')^{-1}(B'B)^{-1}B'$$
where $A$ has dimensions $m\times n$, $B$ has dimensions $m\times k$, $C$ has dimensions $k\times n$, and all three have rank $k$. I just don't understand how to actually find this inverse matrix.","['matrices', 'pseudoinverse', 'linear-algebra', 'inverse']"
2624466,Tail Value at Risk of Normal Distribution,"For a random variable $X$, Tail-value-at-risk is denoted as $\operatorname{TVaR}_p(X) = \operatorname E(X \mid X>\pi_p) = \dfrac{ \int_{\pi_p}^\infty xf(x) \, dx}{1-F(\pi_p)}$, where $\pi_p=\operatorname{VaR}_p=$ the value-at-risk $=$ the value such that $P(X>\pi_p)=1-p$. While I was reading through my book, I stumbled upon this page: But I struggle to see how the second result was proved.  I'm either not sure how to do the calculus or I don't know what tricks they used to get there, but I can't figure it out.  Do you understand how they got there?","['distribution-tails', 'normal-distribution', 'probability-distributions', 'probability', 'actuarial-science']"
2624501,The set of morphism from a tensor product to an certain ring,"When we try to prove the existence of fibre product in affine case, we use the following formula stacks project :
$$\newcommand{\Hom}{\mathrm{Hom}}\newcommand{\Mor}{\mathrm{Hom}}\newcommand{\Spec}{\mathrm{Spec}}
\begin{eqnarray}
\Mor(X, \Spec(A \otimes_R B))& = &
\Hom(A \otimes_R B, \mathcal{O}_X(X)) \\
& = &\Hom(A, \mathcal{O}_X(X))
\times_{\Hom(R, \mathcal{O}_X(X))}
\Hom(B, \mathcal{O}_X(X)) \\
& = & \Mor(X, \Spec(A))\times_{\Mor(X, \Spec(R))}\Mor(X, \Spec(B))
\end{eqnarray}
$$ My questions are: I don't know how do we derive the second formula. Moreover, some part of the third equality is also confusing to me. I know $$\Hom(A, \mathcal{O}_X(X))=\Mor(X, \Spec(A))$$
But what does the ""fibre product"" (in category of sets) notation mean?","['category-theory', 'abstract-algebra', 'algebraic-geometry', 'tensor-products']"
2624508,Trouble understanding the Lindeberg-Feller Central Limit Theorem,"I am having trouble understanding the notation in the following Lindeberg-Feller CLT. The notation isn't defined anywhere else in the text so I would like to clarify a few things. Firstly, what is $y_{ni}$? Is $i$ indexing the observation? E.g., if there are 5 observations in the sample, then we have $y_{n1}, \cdots, y_{n5}$? What does the subscript $n$ mean? How is the sample average $\overline{y}$ defined? Is it $\frac{1}{m}\sum_{i=1}^{m} y_{ni}$ where $m$ is the total number of observations?","['statistics', 'probability', 'central-limit-theorem']"
2624518,If $dx$ is just syntax and not an infinitesimal then why do we apply operations to it?,"So apparently my understanding of this concept is either old, outdated, or nonstandard, etc, but I was under the assumption that in an integral, $dx$ represented the ""infinitesimal change in $x$"", analogous to $\Delta x$, the ""width"" of an approximating rectangle under the curve, where $f(x)$ is the height of that rectangle. But now I hear it's just syntax to let you know the variable you're integrating. But if this is true then why, during things like $u$-substitution, do we still manipulate $dx$ as if it were a real quantity and we're just changing the units? It's like as if we were doing dimensional analysis for example. So what exactly is $dx$ if it's not a real thing but is still something we apparently manipulate in certain situations such as $u$-substitution? And moreover if $dx$ is just syntax when what exactly is the integral doing if not implicitly taking the sum of infinitely many ""rectangles"" with infinitely small width?","['limits', 'calculus', 'number-theory', 'integration', 'infinitesimals']"
2624524,The Probability of the Sample Space is $1$: $P(\Omega) = 1$.,"Let's say we denote the sample space as $\Omega$. One of the axioms of the probability function is that the probability of something happening is equal to $1$. Ok, fine, this makes sense. But, notationally, this is represented as $P(\Omega) = 1$. This is saying that the probability of the sample space is equal to $1$. In my mind, it seems that there is a subtle difference between these two statements: One says that some event must occur and the other says that the entire sample space must occur. However, obviously, it is not necessary that the entire sample space occur, right? Rather, at least one outcome in the sample space must occur . I must be thinking about this incorrectly, so I would appreciate it if people would please take the time to clarify this. And the sample space is not a set of sets, right? It's simply a set of elements (outcomes/endpoints). So we cannot justify this by saying that the sample space represents the union of all its elements, since its elements are not sets and the union operator only operates on sets.",['probability-theory']
2624574,Does there exist a non-increasing sequence with these properties?,"Following on from Does there exist a sequence with these properties? Does there exist a non-increasing sequence $a_n\in \mathbb{R}^{>0}$ such that $$\liminf_{N\rightarrow \infty}~~~ \frac{1}{N}\sum_{n=1}^Na_n >0$$ and $$\sum_{n=1}^\infty \frac{1}{a_n\cdot n^2} = \infty~~?$$ Note that the first obvious guesses - $a_n = 1/n$, $a_n = \log(n)/n$ don't work.","['real-analysis', 'sequences-and-series', 'analysis']"
2624583,"For independent, symmetric random variables $(\xi_n)$, $E\left(\left(\sum\limits_n\xi_n\right)^2\land1\right)\le\sum\limits_nE(\xi_n^2\land1)$","The following problem appears as an exercise in the Russian version of Probability , Shiryaev, 2003 edition(it seems that no English version containing this problem is available yet). Let $\xi_1,\xi_2,\ldots$ be independent, symmetrically distributed random variables. Then
  $$\mathsf E\left(\left(\sum_n\xi_n\right)^2\land1 \right)\le\sum_n\mathsf E(\xi_n^2\land1)$$ To avoid discussing convergence issue, I would like to assume that the summation is finite. It's not quite clear from context what ""symmetrically distributed"" means, but it's reasonable to guess this means $\xi_n$ and $-\xi_n$ has the same distribution. In this sense, I tried to write,
$$\mathsf E\left(\left(\sum_{n=1}^N\xi_n\right)^2\land1 \right)=\int(x^2\land1)dF_{S_N}(x)=2\int_0^\infty(x^2\land1)dF_{S_N}(x)$$
and
$$\mathsf E(\xi_n^2\land1)=\int(x^2\land1)dF_{\xi_n}(x)=2\int_0^\infty(x^2\land1)dF_{\xi_n}(x)$$
and, after integrating by parts, reduce the problem to proving
$$\int_0^1x\left(\mathsf P(S_N\ge x)-\sum_{n=1}^N\mathsf P(\xi_n\ge x)\right)dx\le0$$
The problem would be settled if
$$\mathsf P\left(\sum_{n=1}^N\xi_n\ge x\right)\le\sum_{n=1}^N\mathsf P(\xi_n\ge x),\quad 0\le x\le1$$
which is, unfortunately, not true in general. My question: Is my approach above completely nonsense? Is it possible to turn it into a proof? If not, how to prove this inequality? Edit. By induction, it suffices to prove for $N=2$, i.e. $\mathsf E((\xi_1+\xi_2)^2\land1)\le\mathsf E(\xi_1^2\land1)+\mathsf E(\xi_2^2\land1)$. This should be easier, but still not quite obvious for me.","['probability-theory', 'inequality', 'expectation']"
2624586,Find cube root of $63$ to three decimal places,Find $\sqrt[3]{63}$ to three decimal places (without calculus or a calculator). I am unable to find it. How to get the result by hand calculation? Please help.,['algebra-precalculus']
2624661,"How to multiply on a calculator which only allows add, subtract and reciprocal [duplicate]","This question already has answers here : Can the product $AB$ be computed using only $+, -,$ and reciprocal operators? (4 answers) Closed 6 years ago . Assume we have a calculator with the following flaw. The only operations can be done by it are $+$ and $-$ and $\dfrac{1}{x}$ i.e. you only can add or subtract two numbers and also calculate the reciprocal but you can't multiply or divide. Using this calculator how can you multiply two numbers? I twiddled with lots of formulas but I got nowhere. Sorry if  I can't add any further information or idea. I appreciate any solution on this....","['algebra-precalculus', 'puzzle']"
2624671,How to prove the space of divergence-free vector fields on a manifold is infinite dimensional?,"Let $M$ be a closed manifold, equipped with a volume form $\omega$. I understand that the vector space $$ \{ X \in \Gamma(TM) \, | \, L_X\omega=0 \}$$ is always infinite-dimensional. Is there an elementary argument showing this? We can assume of course $\omega$ comes from a Riemannian metric $g$, and then $L_X\omega=0$ if and only if $\text{trace}(\nabla X)=0$.","['volume', 'smooth-manifolds', 'differential-forms', 'divergence-operator', 'differential-geometry']"
2624702,"Evaluate the integral $ I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin \left(\frac{1}{z+2}\right)dz$,","Evaluate the integral $$ I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin\left(\frac{1}{z+2}\right)dz$$ where $R \geq 4$ My work: Here the zeros of $\sin\left(\frac{1}{z+2}\right)$ is $\frac{1}{n \pi}-2$, so $-2$ is non-isolated singularity. But after I am stuck, Please help. Thanks.","['complex-analysis', 'complex-integration']"
2624760,Is the determinant of a covariance matrix always zero?,"My understanding is that given matrix X, I can find its corresponding covariance matrix by: finding the means of each column subtracting each mean from each value in its respective column and multiplying the resulting matrix by its own transpose. Let's call this matrix C. Here is what it would look like in Python: Y = X - numpy.mean(X, axis = 0) 
C = numpy.dot(Y, Y.T) If I do this, I can prove mathematically (and experimentally using some simple Python code) that det(C) = 0 always. However, a colleague tells me that using the inverse of a covariance matrix is common in his field and he showed me some R code to demonstrate. > det(cov(swiss))
[1] 244394171542 I notice that R has several ways of calculating the covariance matrix that leads to different results. I also notice from Googling that some people say the covariance matrix is always singular (eg here ) whereas others say it is not. So, my question is: why the differences of opinion and what's the true answer? EDIT : I discovered that the determinant is only zero if the matrix is square. If anybody knows the proof for this or can throw some further light on the matter, I'd be grateful.","['statistics', 'linear-algebra']"
2624813,Limit of chi square distribution.,Question in my exercise is written as $$\lim_{n \to \infty}\bigg(\dfrac{1}{2^{\frac{n}{2}}\Gamma{\frac{n}{2}}}\int_{n+\sqrt{2n}}^{\infty}e^{\frac{-t}{2}}t^{\frac{n}{2}-1}dt\bigg)$$ equals : $(A)=.5$ $(B)=0$ $(C)=.0228$ $(D)=.1587$ As sample size increase chi square approaches normal distribution.(I am not sure if i wrote this statement correct so please correct me and give me little explanation on that). Using this fact i calculated $P(X>n-\sqrt{2n}) = \Phi(-1) = .1587$. Did i do everything correct using this intuition ?,"['statistical-inference', 'sampling', 'probability-distributions', 'statistics', 'chi-squared']"
2624826,A somewhat pathological function,"For educational purposes, I need a function $f:\Bbb R\to \Bbb R$ that meets the following properties, or a proof that it doesn't exist: Continuous. For every $x\in\Bbb R$, $f^{-1}(\{x\})$ is infinite and bounded. I have been thinking on the problem for a while, and intuition tells me that such a function doesn't exist, but I'm not sure where to begin to find a proof. Hints for a proof or a counterexample (or a reference for it) are welcome.","['general-topology', 'reference-request', 'real-analysis']"
2624856,"Trigonometry problem, Evaluate: $\frac {1}{\sin 18°}$","My problem is, Evaluate: $$\frac {1}{\sin 18°}$$ I tried to do something myself. It is obvious, $$\cos 18°= \sin 72°$$ I accept $\left\{18°=x \right\}$ for convenience and here, $\sin (x)>0$ $$\cos (x)=\sin (4x)$$ $$\cos (x)=2× \sin(2x) \cos (2x)$$ $$\cos (x)=2× 2\sin(x) \cos (x)×(1-2\sin^2(x)), \cos(x)>0$$ $$8\sin^3(x)-4\sin(x)+1=0$$ $$(2\sin(x)-1)(4\sin^2(x)+2\sin (x)-1)=0$$ $$4\sin^2(x)+2\sin (x)-1=0, \sin(x)≠\frac 12$$ $$4t^2+2t-1=0$$ $$t_{1,2}=\frac {-1±\sqrt 5}{4}$$ $$t=\frac {\sqrt5-1}{4} ,t>0$$ $$\sin 18°=\frac {\sqrt5-1}{4} .$$ Finally, $$\frac {1}{\sin 18°}=\frac {4}{\sqrt5-1}=\sqrt5+1$$ Is this way correct and is there a better/elegant way to do it? As always, it was an ugly solution. Thank you!","['radicals', 'trigonometry', 'proof-verification', 'algebra-precalculus', 'proof-writing']"
2624880,Find mistake in simplifying rational expression,"Why can't I simplify the following expression?
$$\frac{-(-\frac{\sqrt2}{2})}{(-\frac{\sqrt2}{2})^2}$$
I did this:
$$\frac{-(-\frac{\sqrt2}{2})}{(-\frac{\sqrt2}{2})^2}=\frac{-1(-\frac{\sqrt2}{2})}{(-\frac{\sqrt2}{2})^2}=\frac{-1}{(-\frac{\sqrt2}{2})}=\frac{2}{\sqrt2}$$
However, the correct way to do is first raise a square and then simplify:
$$\frac{-(-\frac{\sqrt2}{2})}{(-\frac{\sqrt2}{2})^2} = \frac{-(-\frac{\sqrt2}{2})}{\frac{2}{4}}=\frac{\sqrt2}{2}\cdot\frac{4}{2}=\sqrt2$$","['algebra-precalculus', 'radicals']"
2624900,Verify the Riemann Hypothesis for first 1000 zeros.,"I'm trying to verify the Riemann hypothesis for the first 1000 zeros using the Euler-MacLaurin expansion of $\zeta(s)$. But here's where my problems begin, firstly my python code doesn't seem to be working, I'm essentially using the method outlined in Mike Rubinstein - ""Computational Methods and Experiments in Analytic Number Theory"" [ https://arxiv.org/pdf/math/0412181v1.pdf] and here is the code I have: def CalculateRZ(s, tol):
    A = int(0)
    while (2*A - 1 < tol + 0.5 * math.log10(abs(s + 2*A - 2))):
        A = A + 1
    B = int(2 * A)
    N = int(math.ceil((10 / 2 * pi) * abs(s + B - 2)))

    sum1 = 0.0
    for n in range(1, N + 1):
        sum1 = sum1 + n ** (-s)

    ksum = 0.0
    for k in range(1, B + 1):
        ksum = ksum + Binomial(s + k - 2, k - 1) * (bernoulli(k) / k) * N ** (-s - k - 1)

    sum2 = (N ** (1 - s)) / (s - 1) + ksum

    EulerApx = sum1 + sum2

    return EulerApx where Binomial(n, k), and bernoulli(k) are exactly what they look like. This seems perfectly correct to me, however when I set x = 1/2 + 14.134725141734693790457251983562470270784257115699243175685567460149 * 1j (i.e., the first zero) and run CalculateRZ(x, 100) I get as an output $\zeta(x) = -0.008282095160420333 + 0.009227302793957781j$ which may be kind of close to zero but certainly not to 100 digits of accuracy and likely not enough accuracy to verify the Riemann Hypothesis. Further to this, once I have this code working I'm unsure how to go about verifying the Riemann Hypothesis afterwards. Would I be better off creating a new function for $Z(t)$ then using Newton Raphson to count the zeros? If so how would I go about computing $Z(t)$? Should I just make it as a function return $\zeta(1/2+1t)e^{i\theta(t)}$ using my version of zeta and a function that just straight up calculates $\theta(t)$? Where should I set my start points for N-R method to ensure I hit every zero once and only once? Once the zeros are counted I understand I can then integrate $\zeta$ over some rectangle, and divide that number by $2\pi$ which will give me the number of singularities which should match up to the number of zeros found using Newton Raphson, therefore showing that all zeros lie on the line $Z(t)$, and therefore verifying the RH. But how would this integral be calculated? I know this is a huuge question so any help would be appreciated even if it's not an answer to the full question.","['number-theory', 'riemann-hypothesis', 'riemann-zeta', 'python']"
2624913,Solution to a wierd differential equation,"I solved this problem: find the area of the triangle formed by the $x$ and $y$ axis and the tangent line to the curve of $f(x) = \frac 1x$ at any point $(x_0,f(x_0))$.
I found out that the area was equal to $2$ for every point.
I then wondered what kind of function can have that particular area being independent of the point we choose.
And I saw that for the area to be equal to a constant $c$, the function $y(x) $must verify this differential equation:
$$y'x^2 + \frac 1{y'}y^2 - 2xy = 2c$$
Taking $C = 2$ Area gives $C = 2c$ to make it simpler.
But I can't find the solutions. I really need a help.",['ordinary-differential-equations']
2624926,Computing $\lim_{x \to 1}\frac{x^\frac{1}{5}-1}{x^\frac{1}{6} -1}$,"I cannot figure out how to
get around the zero numerator and denominator in order to compute the limit below: $$\lim_{x \to 1}\frac{\left(x^\frac{1}{5}\right)-1}{ \left( x^\frac{1}{6}\right) -1}$$ I tried: $$       \lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{1}{6} - 1) (x^\frac{1}{6}  + 1)        }     $$ $$\lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{2}{6} - 1)   } $$","['radicals', 'limits', 'factoring', 'calculus', 'fractions']"
2624971,How to compute $\sum_{k=1}^{+\infty}kq^{k-1}$? [duplicate],"This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 6 years ago . Another task from former tests of my basic course on probability... Each time a cup of coffee is requested, a coffee express: Brews coffee with probability $p_c=0.9$; Pours hot water to the cup with probability $p_w=0.09$; Gets out of order and refuses this and any subsequent requests with probability $p_f=0.01$. Let $X$ denote the amount of coffees the express prepares before it gets out of order. Compute $\operatorname{E}X$. My solution attempt: On $k$th request, assuming the express is not broken, let $c_k$ denote the event that it prepares coffee, $w_k$ mean that it prepares boiling water and $f_k$ mean that it fails. We have $\operatorname{P}(c_k|\neg f_k)=\frac{\operatorname{P}(c_k\wedge\neg f_k)}{\operatorname{P}(\neg f_k)}=\frac{p_c}{1-p_f}$ Let $F=k$ mean that the express went out of order while handling the $k$th request. Now by the law of total expectation we have: $\operatorname{E}X=\sum^{+\infty}_{k=1}\operatorname{P}(F=k)\operatorname{E}(X|F=k)$ $F$ has geometric distribution, so we have $\operatorname{P}(F=k)=(1-p_f)^{k-1}p_f$ $X|F=f$ has binomial distribution with $k-1$ experiments, each having probability of success equal to $\operatorname{P}(c_k|\neg f_k)=\frac{p_c}{1-p_f}$; thus $\operatorname{E}(X|F=k)=(k-1)\frac{p_c}{1-p_f}$ Therefore: $\operatorname{E}X=\sum^{+\infty}_{k=1}\operatorname{P}(F=k)\operatorname{E}(X|F=k)=\sum^{+\infty}_{k=1}(1-p_f)^{k-1}p_f(k-1)\frac{p_c}{1-p_f}=\\p_cp_f\sum_{k=1}^{+\infty}(1-p_f)^{k-2}(k-1)=p_cp_f\sum_{k=1}^{+\infty}(1-p_f)^{k-1}k$ Here is were I'm stuck. How to compute this sum? I've really forgotten much from discrete mathematics :(","['probability', 'sequences-and-series', 'discrete-mathematics']"
2624980,IMO 2017 problem #1,"This clearly means that the sequence repeats itself(not necessarily from beginning, i.e from $a_0$) after a certain number of terms. So we can visualize the sequence in the following way: After a certain number of terms, we get a perfect square, the next term becomes its square root, then after a while we get the same perfect square, the next term again is its square root, and this repeats. Thus we see that the square root of the perfect square must follow
$$n + 3k = n^2,k\in\mathbb N_0\Rightarrow k=\frac{n(n-1)}3$$ So $$n\equiv\{0,1\}\bmod3$$
Now if $n$ is of form $3k+1$ then its square is also the same and also $a_0$ Now we will prove that this $3k+1$ form of $n$ will not do. We can use this to show that if $n$ is of form $3k+1$ then we will get a perfect square before reaching $n^2$ and it will never repeat. Only I have to prove that $a_0$ of form $3k$ will do. Can you do it for me?","['number-theory', 'contest-math', 'elementary-number-theory']"
2624986,The meaning behind $(X^TX)^{-1}$,"In linear algebra, we learn that the inverse of a matrix ""undoes"" the linear transformation.  What exactly is the meaning of the inverse of $(X^TX)^{-1}$? $X^TX$ we know as being a square matrix whose diagonal elements are the sums of squares.  So what are we doing when we take the inverse of this?  I have always used this property in my calculations but would like to understand more of the meaning behind it.","['regression', 'statistics', 'linear-algebra']"
2624989,About Algebraic Statistics,"Recently, I find that the website of professor Bernd Sturmfels in Berkeley mentions Algebraic Statistics. I am quite interested in both Algebra and Statistics. But I have never know the cross field between them. Can anyone introduce some reccent work about Algebraic Statistics ?","['statistics', 'algebraic-geometry']"
2625014,Derivative of a function $y(x)= \sum\limits_{n=1}^\infty nx^{-n}$,"Can anyone show and explain how to obtain the derivative of the following function,
  $$y(x)= \sum_{n=1}^\infty nx^{-n}$$ which is, for $n=3$, equivalent with, $$y(x)=x^{-1}+2x^{-2}+3x^{-3}$$ now, what is the derivative of $x$ with respect to $y$, i.e. $\dfrac{dx}{dy}$ for $n=\infty$? Can this kind of problem be solved analytically?","['derivatives', 'real-analysis', 'geometric-series', 'calculus']"
2625074,Proof for a graph has Euler tour iff each vertex has even degree,"This is the proof: I was able to understand this proof except the last part. They consider the edge $\{u, v_i\}$ to prove that if $W$ is not the Euler tour then it is not the longest walk as well, as it can be extended a new walk $W^{'}$ can be formed. However they state: $W^{'}=u,v_i,v_{i+1},..., v_k, v_1, v_2,..., v_i $ and this contradicts our statement that each node has even degree, as in this case $v_i$ has odd degree. So isn't it wrong to consider it? (As $W^{'}$ is not even possible in our scenario of nodes with even degree)","['eulerian-path', 'graph-theory', 'discrete-mathematics']"
2625217,"Show that if $A,B$ are $2\times2$ matrices, then $(AB-BA)^2$ commutes with all $2\times2$ matrices.","I tried to write it all out, but it becomes really messy... Is there a more elegant way to do it? Note that I don't know about dimensions, vector spaces & bases yet","['matrices', 'linear-algebra', 'linear-transformations']"
2625231,"Difficulty in understanding ""Find $E[N]$ , where $N=\min\{n>0: X_n=X_0\}$""","It is Question 30 on page 168 in Ross's book (Introduction to Probability Models-11th edition) Let $X_i, i\ge 0$ be independent and identically distributed random variables with probability mass function $p(j)=P(X_i=j), j=1,\ldots,m, \sum_{j=1}^m P(j) = 1$ Find $E[N]$, where $N=\min\{n>0: X_n = X_0\}$ The same question was here, but I don't have enough credit to comment on. Find $E[N]$, where $N = \min\{n>0: X_n = X_0\}$ Here are my questions: What is the sample space for it? Can anyone give me a simple example? or any references? What does $X_n = X_0$ mean for 2 random variables with the same distribution? Is it possible to resolve it with conditional probability? Are there any applications of such a question? What is the point behind the question? Any help would be greatly appreciated. Update 1: The Solution says: E[N] = $\sum_{j=1}^m$ E[N|Xo = j] * p(j) = $\sum_{j=1}^m (1/p(j)) * p(j) = m$ I am totally confused, so I want to confirm a few things. $N=\min\{n>0: X_n = X_0\}$: Is this the least index of indices of Xi that are equal to X0 (having the same j with X0)?","['conditional-expectation', 'probability']"
2625253,Differential Equation with Euler multiplier,"I want to solve the following D.E. $$xy'+(lnx)y =lnx$$ First I need to bring it to the form $y'+a(x)y=b(x)$, so it becomes:
$$y'+\frac{lnx}{x}y=\frac{lnx}{x} $$ After this, I need to calculate the $\int a(x)dx$ which will give me:
$$\int{\frac{lnx}{x}dx} = \frac{(lnx)^2}{2} +c$$ So the Euler multiplier is $e^\frac{(lnx)^2}{2}$. Now I multiply my equation with this, at each side and get on the left side the product rule: $$(ye^\frac{(lnx)^2}{2})'=e^\frac{(lnx)^2}{2}\frac{lnx}{x}$$ So the question that I have is this: Am I allowed to do the following? $$ye^\frac{(lnx)^2}{2}=\int{e^\frac{(lnx)^2}{2}\frac{lnx}{x}}dx$$ If no, why? Edit: If this is correct, then if I simplify I get: $$ yx=\int{lnx}dx $$ and by computing the integral I would end up with this: $$yx= xlnx -x +c \Rightarrow y=lnx -1+c$$ But if it is done otherwise, like this: $$(ye^\frac{(lnx)^2}{2})'=(e^\frac{(lnx)^2}{2})'  \Rightarrow ye^\frac{(lnx)^2}{2}=e^\frac{(lnx)^2}{2}$$ I get this result: $$ y= 1 +c *e^{-\frac{(lnx)^2}{2}} $$ which is different from the other result.",['ordinary-differential-equations']
2625440,Simulate a random variable by the given density,"Given is the density $$f(x)=\begin{cases}
\frac{3}{4}\left(2x-x^2\right)&\mbox{if }x \in (0,2) \\ 
0&\mbox{else}\end{cases}$$ Find a random variable for the density. There are probably several methods for doing this but I only know of inverse transform and I like to try to use it here. Firstly, we need the distribution function of the density. It is (just assume this is correct): $$F(x)=\begin{cases} \frac{3}{4}x^2-\frac{1}{4}x^3&\mbox{if }0<x\leq 2 \\
1 &\mbox{if }x>2\\ 
0 &\mbox{if }x<0\end{cases}$$ Now we need to inverse this distribution function:
$$y=\frac{3}{4}x^2-\frac{1}{4}x^3$$ I used a software for this one because it got very complicated when I tried to do it by hand :( $$x= \sqrt[3]{2\sqrt{y^2-y}-2y+1}+\frac{1}{\sqrt[3]{2\sqrt{y^2-y}-2y+1}}$$ While the other party of the distribution function didn't need any change except for the cases, so for the inverse we have $$F^{-1}(y)=\begin{cases} \sqrt[3]{2\sqrt{y^2-y}-2y+1}+\frac{1}{\sqrt[3]{2\sqrt{y^2-y}-2y+1}}&\mbox{if }\\
1 &\mbox{if }\\ 
0 &\mbox{if }\end{cases}$$ I didn't expect it will end up that complicated, I don't even know what cases I need to use and how the inverse of $0$ will be treated as it doesn't seem to be defined : / Maybe there is an easier way of solving the problem or did I miss an important step / did a mistake somewhere? :s","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
2625467,Algorithm to tell if an infinite sequence is random or not?,"Background I recently came up with an algorithm which creates patterns. Consider the famous sequence of the primes. They have considerable amount of ""structure"" but no ""obvious pattern"". I will illustrate ""row math"" making a pattern emerge with the example of primes: Row math \begin{equation}
2,3,5,7,11,13,17,19,23,\dots
\end{equation} Now we will take the modulus/absolute difference of every element with its adjacent neighbor.
\begin{equation}
2,3,5,7,11,13,17,19,\dots
\end{equation}
\begin{equation}
1,2,2,4,2,4,2,\dots
\end{equation} Now we will take the difference (not the absolute difference) of the $2$'nd row with the first: \begin{equation}
2,3,5,7,11,13,17,19,\dots
\end{equation}
\begin{equation}
1,2,2,4,2,4,2,\dots
\end{equation}
\begin{equation}
1,1,3,3,9,9,15,\dots
\end{equation} Again now we repeat the process of taking the absolute difference between adjacent neighbors: \begin{equation}
2,3,5,7,11,13,17,19,\dots
\end{equation}
\begin{equation}
1,2,2,4,2,4,2,\dots
\end{equation}
\begin{equation}
1,1,3,3,9,9,15,\dots
\end{equation}
\begin{equation}
0,2,0,6,0,6,\dots
\end{equation} Again now we take the difference between this foremost row and the one preceding it: \begin{equation}
2,3,5,7,11,13,17,19,\dots
\end{equation}
\begin{equation}
1,2,2,4,2,4,2,\dots
\end{equation}
\begin{equation}
1,1,3,3,9,9,15,\dots
\end{equation}
\begin{equation}
0,2,0,6,0,6,\dots
\end{equation} Alternating between these two procedures indefinitely on sees a pattern emerge: \begin{equation}
2,3,5,7,11,13,17,19,\dots
\end{equation}
\begin{equation}
1,2,2,4,2,4,2,\dots
\end{equation}
\begin{equation}
1,\textbf{1},3,3,9,9,15,\dots
\end{equation}
\begin{equation}
0,\textbf{2},0,6,0,6,\dots
\end{equation}
\begin{equation}
\textbf{1},\textbf{-1},3,-3,9,3,\dots
\end{equation}
\begin{equation}
\textbf{2},\textbf{4},6,12,6,\dots
\end{equation}
\begin{equation}
\textbf{-1},\textbf{-5},-3,-15,3,\dots
\end{equation}
\begin{equation}
\textbf{4},\textbf{2},12,18,\dots
\end{equation}
\begin{equation}
\textbf{-5},\textbf{-7},-15,-33,\dots
\end{equation}
\begin{equation}
\textbf{2},\textbf{8},18,\dots
\end{equation}
\begin{equation}
\textbf{-7},\textbf{-15},10,\dots
\end{equation}
\begin{equation}
\textbf{8},\textbf{25},\dots
\end{equation}
\begin{equation}
\textbf{-15},-40,\dots
\end{equation}
\begin{equation}
\textbf{25},\dots
\end{equation} We note that in the number $1$ in bold in rowrepeats again in row. Similarly we see the bold $2$ repeats in another row. And so on for every bold number. Hence, as a pattern has emerged the sequence contains structure. Algebraic Representation The above manipulations can also be represented algebraically using a nilpotent matrix. Consider the following example of primes, again. But first some definitions: Let $x = 1 - \epsilon $
 where 1 represents the identity matrix and epsilon is a nilpotent matrix such that $\epsilon^2 = 0$. We define $y$ satisfying the properties:
$ xy =1$ and $x^\lambda + y^\lambda = 2$ for any $\lambda$ being an integer. \begin{equation}
K = s x^2 + s^2 x^3 + s^3 x^5 + \dots
\end{equation} Multiplying $s$ both sides we get: \begin{equation}
Ks = 0+ s^2 x^2 + s^3 x^3 + s^4 x^5 + \dots
\end{equation} Subtracting the equations one gets: \begin{equation}
K(s-1) = -s x^2 + s^2 x^2(1-x) + s^3 x^3(1-x^2 ) + \dots
\end{equation} Note in the above procedure this is quite similar to taking absolute difference between the primes. 
Now, using $x^\lambda + y^\lambda = 2$: \begin{equation}
K(s-1) = -s x^2 + s^2 x^2(y-1) + s^3 x^3(y^2-1 ) + \dots
\end{equation} Defining $K_1$ as a sequence with only positive coefficients: $K_1 = s x^2 + s^2 x^2 + s^3 x^3 + \dots$ and using $ xy =1$. Hence, \begin{equation}
K(s-1) + K_1 = s^2 x + s^3 x + \dots
\end{equation} Note the above step is similar to subtraction between rows! Hence, we can represent all the manipulations algebraically! = Questions What are some non random sequences row math does not make a pattern emerge (I've tried it for Fibonacci, geometric, etc)? And a pattern emerges everytime (for some reason). Why does a pattern emerge to begin with?","['dynamical-systems', 'computer-science', 'number-theory', 'algorithms', 'sequences-and-series']"
2625476,"If a function preserves integrability, must it have linear growth?","This question is the converse of Sufficient conditions on $F$ such that $F(X)\in\mathcal{L}^{p}$ for all $X\in\mathcal{L}^{p}$ . Let $F : \mathbb{R} \to \mathbb{R}$ be a Borel function with the following property: for every Borel probability measure $\mu$ on $\mathbb{R}$ having finite first moment (i.e. $\int |x|\,d\mu < \infty$), we have $\int |F|\,d\mu < \infty$.  (In probability language, this says that whenever $X$ is an integrable random variable on any probability space, then $F(X)$ is integrable too; let $\mu$ be the law of $X$.) Must $F$ have linear growth?  That is, does there necessarily exist a constant $C$ such that $|F(x)| \le C(1+|x|)$ for all $x \in \mathbb{R}$? I thought about a closed graph theorem argument, but here the map $X \mapsto F(X)$ is a nonlinear mapping of $L^1$.","['real-analysis', 'integration', 'measure-theory', 'probability-theory']"
