question_id,title,body,tags
3963924,How to build taylor series for infinite dimensional objects?,"Given an operator $O(f(z)): \left(\mathbb{C} \rightarrow \mathbb{C}\right) \rightarrow \left(\mathbb{C} \rightarrow \mathbb{C}\right) $ We can define its functional derivative up to a perturbation $\lambda$ as $$ \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{O(f+\epsilon \lambda) - O(f)}{\epsilon} $$ This can be (with a slight abuse of notation expressed as) $$ \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial f'}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial f^{(n)}}\lambda^{(n)} $$ We will adjust this notation to the leibniz style (for reasons that will be obvious in the next next step): $$ \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial \frac{df}{dz}}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial \frac{d^nf}{dz^n}} \frac{d^n \lambda }{dz^n} $$ You then get some nice identities (I call them the taylor extractants): $$ \frac{\partial O}{\partial f} = \delta_{1}O \\ \frac{\partial O}{\partial f'} = \delta_xO-x\delta_1 O \\ \frac{\partial O}{\partial f''} =  \delta_{\frac{1}{2}x^2}O-x\delta_{x}O+\frac{1}{2}x^2\delta_1O$$ etc... Now it would be nice to assume that all such operators $O$ can be written as an infinite sum of terms of the form $$a(z) f^{n_1}\left(\frac{df}{dz}\right)^{n_2}\left(\frac{d^2f}{dz^2}\right)^{n_2}...  = a(z) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k} \right)^{n_k}\right]$$ Much like how all holomorphic functions can be written as an infinite sum of terms of the form $$ a_n z^n$$ We then get a definition of an ""infinite dimensional taylor series"" as a series of the form $$O(f) =  \sum_{g \in \left( \mathbb{N}_0 \rightarrow \mathbb{N}_0 \right)} \left[ \mathfrak{a}(g) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k}\right)^{g(k)} \right] \right]$$ Where $\mathfrak{a}: (\mathbb{N}_0 \rightarrow \mathbb{N}_0) \rightarrow (\mathbb{C} \rightarrow \mathbb{C})$ assigns complex function coefficients to our terms, which are products of the derivatives of different orders of $f$ . This is a direct generalization of the usual expression of a taylor series as $$ f(z) = \sum_{k \in \mathbb{N}_0} a_k z^k $$ With this set up, I was hoping then to try to decompose the operator $O(f) = f(f)$ (ex: it sends $x^2$ to $x^4$ and $e^z$ to $e^{e^z}$ ) into this taylor series framework. But I unfortunately ran into some problems. It seems that besides the expressions $\frac{\partial^n O}{ \partial f^n}$ all other ""functional-partial-derivatives"" including the mixed derivative terms equal $0$ . Which means that my infinite dimensional taylor series for $f(f)$ CANNOT be made to converge except on a single function. I.E. it offers no utility in extrapolating the value of $f(f)$ to new functions. Now this isn't super shocking to me, because sometimes taylor series have radius of convergence zero, see for example $e^{-\frac{1}{x^2}}$ . So I'm guessing that $f(f)$ just happens to have $0$ -radius of convergence infinite dimensional space but i'm not ENTIRELY sure if that might mean I forgot to look at it from the correct angle (much like how $e^{-\frac{1}{x^2}}$ has a useful laurent series, could it be that f(f) has a useful infinite-dimensional-laurent series/some other generalization thereof?) Has anyone else worked with/seen similar infinite-dimensional taylor series, and am I potentially missing important terms here? Visualization tool: Visualizing the infinite dimensional sum is not easy, as you are attempting to visualize a commutative free group with countably infinitely many generators. One approach for ""seeing"" the picture is via the partition sum representation $$ O =  c_0(z) + a_0(z)f  \begin{matrix} a_1(z)f' \\ + \\  a_2(z)f^2 \end{matrix}   \begin{matrix} a_3(z) f'' \\ + \\  a_4(z) ff' \\ + \\ a_5(z)f^3 \end{matrix}  + \begin{matrix} a_6(z) f''' \\ + \\  a_7(z) f''f \\ + \\ a_8(z)f'f^2 \\ + \\ a_9(z)(f')^2 \\ + \\ a_{10}(z)f^4 \end{matrix}  + ... $$ Where the height of each column is equal to the number of partitions (in the combinatorial/number theoretic sense) of the index of the column. Update: It turns out that I made a mistake. Seeing the ramifications now. FWIW I was able to successfully derive $f(x+1) = f + f' + \frac{1}{2}f'' + \frac{1}{6}f''' + ... $ using this framework So far: Unforunately still stuck Observe that for the case of $O(f) = f(f)$ we have $$ \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{f(f+\epsilon \lambda)+\epsilon\lambda(f+\epsilon \lambda)-f(f)}{\epsilon} = f'(f)\lambda+\lambda(f)$$ $$ \frac{\partial O}{\partial f} = \delta_1 O =  f'(f)+1$$ $$ \frac{\partial O}{\partial f'} = \delta_x O - x\delta_1O = xf'(f)+f - x(f'(f)+1) = f-x$$ $$ \frac{\partial O}{\partial f''} = \delta_{\frac{1}{2}x^2} O - x\delta_xO + \frac{1}{2}x^2\delta_1O= \frac{1}{2}x^2f'(f)+\frac{1}{2}f^2- x^2 f(f)-xf+\frac{1}{2}x^2f'(f)+\frac{1}{2} = \frac{1}{2}(f-x)^2$$ Generally then we can extract the rest of the terms this way so we find that $$ f(f) =_{expected} (f'(f)+1)f + (f-x)f'(x) + \frac{1}{2}(f-x)^2f''(x) + ...$$ But in reality we all know $$ f(f) = f(x) + f'(x)(f-x)+ \frac{1}{2}f''(x)(f-x)^2 + ... $$ So something here is quite off.","['functional-equations', 'operator-theory', 'functional-analysis', 'calculus-of-variations']"
3963931,Why does a cumulative distribution have the quality that $P(X<b)=\lim_{n\to\infty}\left[P\left(X\leqslant b-\frac1n\right)\right]$?,"I came across the following passage in Ross' ""First Course in Probability"": If we want to compute the probability that X is strictly less than b , we can apply the continuity property to obtain: $$P\bigl(X<b\bigr)=\lim_{n\to\infty}\left[P\left(X\leqslant b-\frac1n\right)\right].$$ The continuity Ross is referring to, is the right continuity of the cumulative function but the property he mentions seems to me to be a property of a left continuity since the sequence $b-\dfrac1n$ , where $n$ goes to infinity, is an increasing sequence that converges to $b$ from left to right.
Will be grateful for any enlightening remarks on this.","['cumulative-distribution-functions', 'probability-theory', 'probability']"
3963991,Integration is zero implies $g$ is continuous,"Let $g$ be a monotone function such that $$ \int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y)=0$$ where $f(x,y)= 1$ if $x-y \in \mathbb{Z}$ otherwise it is $0$ . The integration is w.r.t. Riemann-Stieltjes sense. How can we show that $g$ is continuous? My try: If $g$ is not continuous at $c$ , $g$ can have only jump discontinuity at $c$ . I am not being able to use this fact.
Any help or hint will be appreciated. Thanks in advance. Update:
If $f$ is not continuous at a point $c \in (0,1)$ .
Let us choose $\epsilon>0$ such that, $(c-\epsilon, c+ \epsilon) \subset (0,1)$ .
Then, $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)=
 \int_0^1 \int_0^1  \,dg(x) \,dg(x) = [  \int_0^1  \,dg(x) ]^2 \geq [  \int_{c -\epsilon}^{c+ \epsilon}  \,dg(x) ]^2 = [g(c+\epsilon)- g(c-\epsilon)]^2 >0$$ , which is a contradiction. Thus $g$ is continuous at $c\in (0,1)$ . If $g$ is not continuous at $0$ , then $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [g(\epsilon)- g(0)]^2 >0$$ , a contradiction. If $g$ is not continuous at $1$ , then $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [ g(1)- 
g(1-\epsilon)]^2 >0$$ , a contradiction. Thus, $g$ is continuous on [0,1]. I want  to justify the step $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)$$ with proof and hope the other parts are correct. Any help or hint will be appreciated. Thanks in advance.","['complex-analysis', 'functional-analysis', 'analysis', 'real-analysis']"
3964021,Prove $|f(x)-p_{3}(x)| \leq \frac{1}{384} \max\limits_{0 \leq x \leq 1}|f^{(4)}(x)|$,"Given $f(x)\in C^4[0,1]$ , and a polynomial $p_3(x)$ of degree $3$ s.t. $p_3(0)=0,p_3'(0)=f'(0),p_3(1)=f(1),p_3'(1)=f'(1)$ . Prove that, $\left|f(x)-p_{3}(x)\right| \leq \dfrac{1}{384} \max\limits_{0 \leq x \leq 1}\left|f^{(4)}(x)\right|$ . I thought it may relate to Simpson's rule , but it seems to be not the case. Also I thought it may relate to Orthogonal Projection on a Polynomial Space , but I don't know how to proceed either. Can anyone help? And, are there any further suggestions on these kind of problems(i.e. given a function, and a polynomial whose values and derivates are related to the function, then estimate)?","['analysis', 'polynomials', 'numerical-methods', 'inequality', 'derivatives']"
3964038,Proof of relative complement identity,"I often see this identity for set theory, $A-B=A\cap B^c$ It is easy to believe this identity by considering the venn diagram for a relative complement, If we consider $B^c$ informally as the space not inside $B$ then the space of set $A$ not intersectiong with set $B$ would be included in $B^c$ . So logically, $A\cap B^c$ would be the shaded area, which is the same space as $A-B$ . But how would one go about proving this identity purely from set theory and symbolic logic?","['elementary-set-theory', 'discrete-mathematics']"
3964041,Exercise 1.7 from Silverman's AEC,"I'm working through Silverman's Arithmetic of Elliptic Curves Exercise 1.7 (3 parts) and I have a question about the last part. (a) In $\mathbb{P}^1$ at least one of $S$ or $T$ is non-zero, so the map is well-defined at all $[S,T] \in \mathbb{P}^1$ . Therefore, $\phi$ is a morphism. (b) You can easily check that $\psi: [X,Y,Z]\mapsto [Y,X]$ works. (c) The only problematic point on $\psi$ is $[0,0,1]$ . I tried to do a bit of the usual manipulation, but was not able to make $\psi$ well-defined on $[0,0,1]$ . For instance: $$ [Y,X] = [YX^2,X^3] = [YX^2,Y^2Z] = [X^2,YZ] $$ This leads me to believe that $\psi$ is not actually a morphism. However, for $\phi$ to be an isomorphism there just has to exist an inverse morphism $\phi^{-1}$ to $\phi$ , and this does not necessarily have to be $\psi$ as I have defined it. I'm wondering how can I (formally) show that $\phi$ is not an isomorphism? EDIT: I realize that $Y^2Z=X^3$ has a cusp at $[0,0,1]$ . So I guess can someone show me how to prove that an isomorphism of projective varieties must preserve singularities?","['algebraic-geometry', 'projective-geometry', 'elliptic-curves']"
3964065,What qualities of an algebraic torus make it like a torus?,"Why do we call it an algebraic torus? What qualities of an algebraic torus make it like a torus? D_S gives the following classical definition of an algeraic torus here Formal Definition of an Algebraic Torus : ""Clasical definition: Let $H \subseteq \operatorname{GL}_n(\overline{k})$ be a connected linear algebraic group over $k$ .  So $H$ is a Zariski closed subgroup of $\operatorname{GL}_n(\overline{k})$ which is irreducible, and the polynomials of the radical ideal $I(H)$ corresponding to $H$ can be chosen to have coefficients in $k$ , rather than just $\overline{k}$ .   We say that $H$ is an algebraic torus if there exists a $g \in \operatorname{GL}_n(\overline{k})$ such that $gHg^{-1}$ consists of diagonal matrices.""","['algebraic-geometry', 'abstract-algebra']"
3964074,How to use comparison lemma to solve the differential inequality $\dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})}$,"I am reading a paper https://ieeexplore.ieee.org/document/7782377 : (p.3864 top) Control Barrier Function Based Quadratic Programs for Safety Critical Systems The comparison Lemma says: Consider the scalar differential equation $$\dot{u} = f(t,u), \ \ \  u(t_0)=u_0,$$ where $f(t,u)$ is continuous in $t$ and locally Lipschitz in $u$ , for all $t \geq 0$ and all $u\in J \subset \mathbb{R}$ . Let $[t_0,T]$ be the interval of existence of the solution $u(t)$ . Let $v(t)$ be a differentiable function  whose derivative $Dv(t)$ satisfies the differential inequality $$Dv(t) \leq f(t, v(t)), \ \ \ \ v(t_0) \leq u_0$$ with $v(t)\in J$ for all $t\in [t_0,T)$ . Then, $v(t)\leq u(t)$ for all $t\in[t_0,T)$ . So suppose we have $$\dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})},$$ compared to the Lemma, we have $$Dv(t)=\frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})},$$ how to obtain the result $$h(x(t,x_0))\geq \frac{1}{-1+\exp\bigg(\sqrt{2\gamma t + \log^2\Big(\frac{h(x_0)+1}{h(x_0)}\Big)}\bigg)}?$$ Thanks in advanced.","['inequality', 'derivatives', 'ordinary-differential-equations']"
3964090,How many $k$-letter words are there such that the letters A and B are not next to each other?,"I spent the better part of a long car ride today thinking about the following problem: Consider an alphabet of $n$ letters. How many $k$ -letter words of this alphabet are there such that two certain letters (say A and B) are not next to each other? A word may contain the same letter multiple times. My thoughts: There are $n^k$ words with length $k$ over the alphabet. We need to find and subtract away the number of words where A and B are next to each other. I modeled any such word as one with the string AB or BA at an index $i$ : $$x_1x_2x_3\cdots x_{i-1}ABx_{i+2}\cdots x_k$$ The $x_j$ here are all letters of the alphabet. We can think of the above word as an $(i-1)$ -letter word, followed by AB, followed by a $(k-i-2)$ -letter word. Thus, there are $n^i \cdot n^{k-i-2} = n^{k-2}$ words where AB or BA is at an arbitrary index $i$ . Summing over all $i$ , we get that there are $(k-2)n^{k-2}$ words containing AB and $2(k-2)n^{k-2}$ words containing AB or BA. Our final expression for the number of $k$ -letter words such that A and B are not adjacent is $$n^k - 2(k-2)n^{k-2}$$ There’s an issue though. I’m over-counting words that contain AB or BA by a lot. I ran a simulation, and at $n=k=10$ , for example, the correct value is $8441614754$ , and my answer is $8400000000$ . By this method, any word that contains AB or BA more than once is counted more than once. Is there any way I can modify this approach to yield the correct result? If not, how should I proceed?","['combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
3964105,"Find $\sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right)$ [duplicate]","This question already has answers here : Maximize $\mathrm{tr}(Q^TCQ)$ subject to $Q^TQ=I$ (5 answers) Closed 3 years ago . Let $$
A:=\left[\begin{array}{llll}
3 & 1 & 0 & 0 \\
1 & 3 & 0 & 0 \\
0 & 0 & 6 & 2 \\
0 & 0 & 2 & 6
\end{array}\right]
$$ Find $\sup _\limits{Q \in M_{4\times 2} (\mathbb{R}), Q^{T} Q=I_{2}} \operatorname{tr}\left(Q^{T} A Q\right)$ , where $M_{4 \times 2}(\mathbb{R})$ represents the set of all matrices of size $4\times 2$ . I know that $\mathrm{tr}A=\sum _i A_{ii}$ , but how can we deal with this upper bound? It is obvious that $Q^T AQ$ is a $2\times 2$ matrix, but I don't know how does the condition $Q^TQ=I_2$ help. Also, are there any backgrounds for this problem? I seldom see (linear algebra) problems asking the uppper bound for a trace and I hope I could get further information about these kind of problems (if possible).","['matrices', 'orthogonal-matrices', 'linear-algebra', 'upper-lower-bounds']"
3964121,How do we prove That $\mathbb{P}(A\cap B) - \mathbb{P}(A)\mathbb{P}(B)\leq 1/4$? [duplicate],"This question already has an answer here : Measure of Dependence Between Two $\sigma$-algebras (1 answer) Closed 3 years ago . How do we prove That $\mathbb{P}(A\cap B) - \mathbb{P}(A)\mathbb{P}(B)\leq 1/4$ ? I have tried in this way: \begin{align*}
\begin{cases}
\mathbb{P}(A)\geq \mathbb{P}(A\cap B)\\\\
\mathbb{P}(B)\geq \mathbb{P}(A\cap B)
\end{cases} \Rightarrow \mathbb{P}(A)\mathbb{P}(B)\geq[\mathbb{P}(A\cap B)]^{2}
\end{align*} But I am unable to prove the required result.","['statistics', 'probability-theory', 'probability']"
3964156,"Mistake when substituting constraint $4x^2+y^2=1$ into a function $f(x,y)=x^2+y^2$ in extrema problem","Consider the function $f(x,y)=x^2+y^2$ under the constraint $4x^2+y^2=1$ . The extrema of $f$ under that constraint can be easily found with Lagrange multipliers, and they are attained for $(0,1)$ , $(0,-1)$ for maximum and $(1/2,0)$ , $(-1/2,0)$ for minimum; however, if we isolate $y^2=1-4x^2$ and we substitute it in the function, we get a wrong result (in particular, the one variable function $g(x)=1-3x^2$ obtained has only a maximum for $x=0$ and Weierstrass theorem assures that the are both maximum and minimum for $f$ ). Can someone explain me why this fails?","['optimization', 'multivariable-calculus', 'analysis']"
3964181,Is this Markov chain recurrent or transient?,"A Markov chain $X_n$ $(n\ge1)$ with state space S=0,1,2,3,... has the following transition probabilities: $$p_{i,i+1}=\frac{(i+1)^2}{2i^2+2i+1},\ p_{i,i-1}=\frac{i^2}{2i^2+2i+1}, \ p_{0,1}=1, \ and \ i\ge 1$$ I am trying to find if it is recurrent or transient. What I have is the following: Obviously, $p_{i,i+1}$ and $p_{i,i-1}$ depends on $i$ , it means that each state will have a different probability. For example, when $i=1$ , then $p_{1,2}=4/5$ and $p_{1,0}=1/5$ . So, it seems that $p_{i,i+1}$ is decreasing as $p_{i,i-1}$ increases on each step. $$T=\pmatrix{0&1&0&0&...&0&0&0\\1/5&0&4/5&0&...&0&0&0\\0&4/13&0&9/13&...&0&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\0&0&0&0&...&1/2&0&1/2}_{nxn}$$ According to wiki , a state i is recurrent if and only if the expected number of visits to i is infinite: $$ \sum_{n=0}^{\infty}p_{ii}^{(n)}=\infty\\ p_{ii}=Pr(X_n=i|X_0=0)\ for\ n\ge 1$$ for a recurrent state i, the mean hitting time is defined as: $$M_i=E\left[ T_i \right]=\sum_{n=1}^{\infty}nf_{ii}^{(n)}$$ State i is positive recurrent (or non-null persistent) if Mi is finite; otherwise, state i is null recurrent (or null persistent). If this Markov chain had a finite state space: I think that I should be positive recurrent because it is possible to
get to any state from any state. How does the infinite state space change this fact? I haven't found good information about it. What about this? If we start from zero, it seems that the MC may return to zero almost surely with probability 1. Thank you!","['ergodic-theory', 'markov-chains', 'probability']"
3964224,How is the formula for the volume of a body in $\mathbb{R}^n$ derived?,"Here is a formula for the volume of a general body in $\mathbb{R}^n$ I came across, that uses spherical polar coordinates. I'm not sure how it came about: Here, $K$ is a body with $0$ in its interior, and for each direction $\theta \in S^{n-1}$ , let $r(\theta)$ be the radius of $K$ in that direction. $$ \text{vol}(K) = nv_n \int_{S^{n-1}} \int_{0}^{r(\theta)} s^{n-1}ds d\sigma = v_n \int_{S^{n-1}} r(\theta)^n d\sigma(\theta)$$ $S^{n-1}$ is the sphere of radius $1$ in $n-1$ dimensions, and $v_n$ is the volume of the Euclidean ball of radius $1$ in $n$ -dimensions. I understand how the first integral goes to the second, but how did this formula come about in the first place? I'm trying to start off with the regular Cartesian integral for volume which looks like $$\int \int ... \int dx_1 dx_2...dx_n$$ but I'm not able to proceed from here. It has been a long while since I posted this. Could someone please add a detailed explanation? It would be really helpful.","['integration', 'volume', 'geometry', 'analysis', 'spherical-coordinates']"
3964284,MAP estimation for discriminative models,"I have some problems in understanding the MAP estimation for discriminative models.
I will use the notation used in the very first two pages of this paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-Valencia-07.pdf As far as I understand, the posterior distribution of a discriminative model is $p(\theta|X, C)$ .
Where $X = \{x_1,x_2,\dots,x_n\}$ is the training set while $C=\{c_1,c_2,\dots,c_n\}$ are the corresponding labels. As usual the posterior is split into the prior and likelihood. $$p(\theta|X,C) \overset{?}{=} \frac{p(\theta)L(\theta)}{p(C|X)} = \frac{p(\theta)p(C|X,\theta)}{p(C|X)}$$ The step that I do not understand is the one marked by the ""?"". Moreover, in the same paper, it is also noted that: $p(\theta,C|X)=p(\theta)L(\theta)$ .
However, if we go a bit further: \begin{aligned}
p(\theta,C|X)=p(\theta)L(\theta) \implies \\
p(\theta,C|X)=p(\theta)p(C|X,\theta) \implies \\
\frac{p(\theta,C,X)}{p(X)} = \frac{p(\theta)p(\theta,C,X)}{p(X,\theta)} \implies \\
p(X)p(\theta) = p(X,\theta)
\end{aligned} Therefore, it seems that $X$ and $\theta$ are independent but I fail to see why.
Given such independence, it would be fairly simple also proving the step marked with ""?"".","['machine-learning', 'statistical-inference', 'statistics']"
3964285,Rod partitioning in n pieces,"A straight rod of length n units (obviously n is an integer) is to be partitioned into n pieces of random length. All pieces are to be used to form triangular constructions, so we are looking for the boundaries for the length of the largest piece, to ensure that this will be possible with any 3. If the largest piece is $n_i$ units long, and its largest and smallest possible lengths are $x_i$ and $y_i$ , then $y_i \leq n_i \leq x_i$ and then for all other pieces, it will be: $n_j \leq y_i$ . Also $\sum n_i \leq \sum y_i = n*y_i$ . Therefore for any 3 pieces, we will have: $y_i \leq y_1 \leq n_1 \leq x_1 \leq x_i$ and same for $n_2$ and $n_3$ .
Wlg we consider $n_1 \leq n_2 \leq n_3$ . In order for the 3 pieces to form a triangle, it must be $n_1+n_2 \leq n_3$ , $n_1+n_3 \leq n_2$ and $n_2+n_3 \leq n_1$ . Also $n_3-n_2 \geq n_1$ , $n_2-n_1 \geq n_3$ and $n_3-n_1 \geq n_2$ . An obvious solution would be to have all segments equal to 1 unit, so we could form $\frac {n}{3}$ equilateral triangles but obviously this is not the general case. I have seen several solutions to similar problems but with very advanced math (integrals etc) and I can't follow the solutions. I wonder if there is any solution with basic calculus (11th or 12th grade) because I don't have any maths background. FYI this is not homework or assignment. Thank you very much.","['calculus', 'inequality']"
3964309,Probability of getting a piece longer than $1/2$ on cutting a rope of length $1$ at two randomly chosen points,"Here's a bootstrap approach I used to find the answer (python 3). def num_pieces(num,lenght):
    ot = list(range(1,lenght+1))[::-1]
    nig = []
    for i in range(lenght-1):
        n = random.randint(1, num-ot[i])
        nig.append(n)
        num -= n
    nig.append(num)
    r = []
    for i in nig:
         r.append(i/10)
    return r

res = []
n1 = 1000
n2 = int(n1/10)

for i in range(n1):
    a = []
    for i in range(n2):
        r = num_pieces(10,3)
        if r[0] > 0.5 or r[1] > 0.5 or r[2] > 0.5:
            a.append(1)
        else:
            a.append(0)
    res.append(sum(a)/n2)

pd.DataFrame(res).hist(); And here's the result If my code is correct (I believe it is), the probability is about 50% Question - I do not know how to solve this task using plain math. In other words - what is the math base under the bootstrap result? UPDATE After receiving those awesome answers below, I checked the code and fixed the num_pieces generator. It worked incorrectly. All other code works just fine. So, here's the final solution def get_random():
    a = [random.random(), random.random()]
    c = 1 - (max(a))
    b = max(a) - min(a)
    a = min(a)
    return [a,b,c]
res = []
n1 = 1000
n2 = int(n1/10)
for i in range(n1):
    a = []
    for i in range(n2):
        r = get_random()
        if r[0] > 0.5 or r[1] > 0.5 or r[2] > 0.5:
            a.append(1)
        else:
            a.append(0)
    res.append(sum(a)/n2)
pd.DataFrame(res).hist(); The probability is 75% indeed. Thank you all for answering!",['probability']
3964310,"All real numbers $(p,q)$ such that $|\sqrt{1-x^{2}}-p x-q| \leq \frac{\sqrt{2}-1}{2}$ holds for every $x \in[0,1]$","Find all pairs of real numbers $(p, q)$ such that the inequality $|\sqrt{1-x^{2}}-p x-q| \leq \frac{\sqrt{2}-1}{2}$ holds for every $x \in[0,1]$ Originally I thought to rephrase it in geometric terms, seeing one degree terms I think about equation of a line. I also have that $\sqrt{1-x^{2}}-\frac{\sqrt{2}-1}{2} \leq p x+q \leq \sqrt{1-x^{2}}+\frac{\sqrt{2}-1}{2}$ I think that a geometric solution might be possible, please help me to proceed. Thanks.","['contest-math', 'coordinate-systems', 'geometry', 'calculus', 'inequality']"
3964333,Absolute value of sub-gaussian,"Let $\eta_1 , \eta_2, \cdots, \eta_t$ be 1-subgaussian independent random variables with mean 0(but not necessarily identical). Now we know several nice equations about $|\sum_{s=1}^t \eta_s|$ . How about $\sum_{s=1}^t |\eta_s|$ ? Is there any hints about $\mathbb{E}|\eta_s|$ ? I believe $\sum_{s=1}^t |\eta_s|\leq O(t \log t)$ with high probability. Is it true? (Surprisingly, there is no 'sub-gaussian' tag.......)","['concentration-of-measure', 'normal-distribution', 'distribution-tails', 'gaussian', 'probability-theory']"
3964386,2 dimensional symmetric random walk,"Let $\left\{\left(X_{n}, Y_{n}\right)\right\}_{n=0}^{\infty}$ be a 2 -dimensional symmetric random walk. Namely, this is a Markov chain where $\left(X_{n+1}, Y_{n+1}\right)$ takes one of the following 4 values with equal probability: $$\left(X_{n}+1, Y_{n}\right),\left(X_{n}-1,Y_{n}\right),\left(X_{n}, Y_{n}+1\right),\left(X_{n}, Y_{n}-1\right)$$ Suppose that $X_{0}=Y_{0}=0$ . Define $T:=\inf \left\{n \geq 0: \max \left(\left|X_{n}\right|,\left|Y_{n}\right|\right)=3\right\} .$ I want to find the value of $\mathbb{E}[T]$ and $\mathbb{P}\left(X_{T}=3, Y_{T}=0\right)$ .
Thanks for your help!","['random-walk', 'stochastic-processes', 'markov-chains', 'probability']"
3964390,Combinatorics problem with students choosing elective courses. (Prove by assuming the opposite). [duplicate],"This question already has an answer here : Non-empty intersection. (1 answer) Closed 3 years ago . There are 2009 different elective courses in the university. Each one of the courses has exactly 45 students in it. Every 2 courses has exactly 1 student in common. Prove that there is a student enrolled in all courses. I think it should be proven by assuming the opposite.
Maybe there is a connection that 2009 / 45 is less than 45 but I'm not sure about that.","['combinatorics', 'discrete-mathematics']"
3964397,Number theory problem with sets of elements - $x = (1 + 3 + \dots + m)p + (m + 2) + (m + 4) + \dots + n$,"Given $p \in \mathbb{N}$ , $p > 1$ and the set: $$A_p = \{x \ | \ (\exists) \ m ,n \in 2\mathbb{N} + 1, x = (1 + 3 + \dots + m)p + (m + 2) + (m + 4) + \dots + n\}$$ Prove that, if $x, y, \in A_p$ , then $xy \in A_p$ . Source : Romanian magazine in mathematics. Attempt: Note that $1 + 3 + \dots + k = z^2$ , if $k \in 2\mathbb{N} + 1$ . So, the set $A_p$ may be redefined as: $$A_p = \{x \ | \ x = p\alpha^2 + \beta^2 - \alpha^2, \alpha, \beta \in \mathbb{N}^* \}$$ Therefore, $x = (p - 1)\alpha^2 + \beta^2$ and $y = (p - 1)\gamma^2 + \delta^2$ . So, $xy$ should be written as $(p - 1)\varepsilon^2 + \varphi^2$ , but multiplying the $x$ and $y$ equations, the general form doesn't look good. I'm also thinking at induction, as $p$ natural. We may start with $p = 2$ (it may be simpler) and than a general $p \to p + 1$ case.","['sums-of-squares', 'number-theory', 'elementary-number-theory', 'elementary-set-theory', 'induction']"
3964468,Dirichlet's Integral to find Volume with gamma function values,"Evaluate volume $\iiint_D dx dy dz$ over a domain $D$ where $D$ is the region bounded by $x\ge 0$ , $y\ge 0$ and $z\ge 0$ and ellipsoid $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2} = 1$ . My approach: I used the RMS $\ge$ AM inequality (since $x,y,z\ge 0$ and $a,b,c>0$ without loss of generality) to arrive at $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}\le\sqrt{3}$ and then did a substitution $X=\frac{x}{a}$ , $Y=\frac{y}{b}$ and $Z=\frac{z}{c}$ And I got answer to be $\frac{abc(\cdot\Gamma(1)\Gamma(1)\Gamma(1)\cdot(\sqrt{3})^3)}{\Gamma(4)}$ ie $abc\sqrt{3}/2$ My friend's approach: Direct substitution of $X=\frac{x^2}{a^2}$ , $Y=\frac{y^2}{b^2}$ and $Z=\frac{z^2}{c^2}$ and his answer was $(\frac{abc}{8})\frac{(\Gamma(\frac{1}{2})\Gamma(\frac{1}{2})\Gamma(\frac{1}{2}))}{\Gamma(\frac{5}{2})}$ ie $\frac{abc\cdot\pi}{6}$ Professor says mine is wrong and friend's method is right but I am not satisfied without the reason.","['integration', 'volume', 'gamma-function', 'multivariable-calculus', 'inequality']"
3964527,Why velocity and accelleration must be linearly independent?,"I am struggling to understand the definition of biregular curve. Definition A curve $I \ni t \mapsto P(t) \in \mathbb{R}^3$ is said to be biregular when the velocity and accelleration vectors are linearly independent, that is when their cross product is different from zero $$dP/dt \land d^2P/d^2t \neq 0$$ Q. I read that this is equivalent to the fact that $$d \underline{t}/ds \neq 0$$ and this is what I am having trouble understanding. Why is so? Recall that $$s= \int_0^t \|dP/dt\| \, dt,$$ is the curvilinean coordinate, and $$\underline{t}(s)=dP/ds$$ so that $$dP/dt= dP/ds \  \dot{s}= \underline{t} \ \dot{s}$$ where $$\dot{s}=ds/dt.$$ Moreover $$d^2P/d^2t= d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s}.$$ Hence we have $$dP/dt \land d^2P/d^2t = \underline{t} \ \dot{s} \land (d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s})$$ $$ = \dot{s}^3(\underline{t}\land d\underline{t}/ds)+\ddot{s}(\underline{t}\land \underline{t})=\dot{s}^3(\underline{t}\land d\underline{t}/ds).$$ So, the curve is biregular $\iff$ $\dot{s}=\|dP/dt\|\neq 0$ and $\underline{t}\land d\underline{t}/ds \neq 0.$ How is this equivalent to the fact that $d \underline{t}/ds \neq 0$ ? Why is it not possible that $d\underline{t}/ds$ and $\underline{t}$ are both different from zero and yet linearly dependent?","['curves', 'real-analysis', 'multivariable-calculus', 'differential-topology', 'differential-geometry']"
3964528,Using the pigeonhole principle to show that there are sequences with the same sum,"We have $a_1,a_2,...,a_n$ , and $b_1,b_2,...,b_m$ , all positive integers, with $a_i < m+1$ for all i, and $ b_j < n+1$ for all j. It is known that $m>n$ , and that the sum of $b_1,..,b_m$ is strictly larger than the sum of $a_1, a_2,...,a_n$ . Show that there is a subset of $a_1,..,a_n$ whose sum is equal to the sum of a subset of $b_1,...,b_m$ . I know this should be solvable using the pigeonhole principle on several sequences, but I just can't seem to find the sequence that works. I tried using sequences that excluded one of the values, but I think that since there are so many possible sequences, this just won't work, and using all possible sums seems pretty hard to do, since there can be multiple occurrences of the same number. I would greatly appreciate any hints, thanks!","['pigeonhole-principle', 'combinatorics']"
3964557,Show that $\mathbb{E}\left|\hat{f_n}-f \right| \leq \frac{2}{n^{1/3}}$ where $\hat{f_n}$ is a density estimator for $f$,"Question Suppose we have a continuous probability density $f : \mathbb{R} \to [0,\infty)$ such that $\text{sup}_{x \in \mathbb{R}}(\left|f(x)\right| + \left|f'(x)\right|) \leq 1. \;$ Define the density estimator: $$\hat{f_n} = \frac{1}{n^{2/3}}\sum^n_{i=1}\mathbf{1}\{-1/2 \leq n^{1/3}(x-X_i) \leq 1/2\}, \: x \in \mathbb{R}$$ Show that, for every $x \in \mathbb{R} \:$ and every $n \in \mathbb{N},$ $$\mathbb{E}\left|\hat{f_n}(x)-f(x)\right| \leq \frac{2}{n^{1/3}}$$ Attempt It's easy to show that $$\mathbb{E}\hat{f_n} = \frac{F(x+\frac{1}{2}n^{-1/3}) - F(x-\frac{1}{2}n^{-1/3})}{n^{-1/3}}$$ where $F$ is the CDF of $f$ . From here, using the condition $\:\text{sup}_{x \in \mathbb{R}}(\left|f(x)\right| + \left|f'(x)\right|) \leq 1 \:$ and in particular, $\:\text{sup}_{x \in \mathbb{R}}\left|f'(x)\right|\leq 1\:$ we can show (geometrically) that, $$\left|\mathbb{E}\hat{f_n}-f\right| \leq \frac{1}{4n^{1/3}}$$ However, I'm not sure how to get a bound on $\mathbb{E}\left|\hat{f_n}-f\right|$ .","['statistics', 'expected-value', 'upper-lower-bounds', 'probability-theory', 'density-function']"
3964572,count increasing functions on natural numbers.,"I want to count all functions with these two conditions:
it is from {1,2,....,7} to {1,2,....,5}.
it is increasing . I have some idea to start with f(1) different values but do not know how to organize solution.","['functions', 'combinatorics']"
3964607,Checking exercise on uniform convergence and differentiation,"I have solved the following exercise and I would like to know if there are any mistakes. Thank you. Consider the sequence of functions defined by $g_n(x)=\frac{x^n}{n}$ . (a) Show $(g_n)$ converges uniformly on $[0,1]$ and find $g=\lim g_n$ . Show that $g$ is differentiable and compute $g'(x)$ for all $x\in [0,1]$ . (b) Now, show that $(g_n')$ converges on $[0,1]$ . Is the convergence uniform? Set $h=\lim g_n'(x)$ and compare $h$ and $g'$ . Are they the same? My solution: (a) Let $\varepsilon>0$ : then $|g_n(x)-0|=|\frac{x^n}{n}|\leq\frac{1}{n}$ for $x\in [0,1]$ so it suffices to choose $N>\frac{1}{\varepsilon}$ to guarantee that $|g_n(x)-0|<\varepsilon$ for all $n\geq N$ and $x\in [0,1]$ so $(g_n)$ converges uniformly to $0\equiv g(x)$ which, being a constant function, is differentiable and has $g'(x)\equiv 0$ too. (b) $g_n'(x)=x^{n-1}$ converges pointwise to $0$ for $x\in [0,1)$ and to $1$ for $x=1$ , so $h(x)=\lim g'_n(x)=\begin{cases}
                                0 & \text{if $x\in [0,1)$}\\
                                1 & \text{if $x=1$}
\end{cases}\neq g'(x)$ and the convergence is not uniform: by the Continuous Limit Theorem) since $(g'_n)$ is a sequence of continuous functions, if it converged uniformly its limit $h$ would be continuous too and that is not the case.","['solution-verification', 'derivatives', 'uniform-convergence', 'real-analysis']"
3964622,Uniformly Integrable Second Moments and Weak Convergence.,"Let $\mathcal{P}(\mathbb{R}^d)$ be the space of Borel probability measures, and let $\mathcal{P}_2(\mathbb{R}^d)$ be the space of Borel probability measures with finite second moment. Let $\{\mu_n\} \subset \mathcal{P}_2(\mathbb{R}^d)$ have uniformly bounded second moments, and assume $\mu_n\to \mu \in \mathcal{P}(\mathbb{R}^d)$ weakly. Is it true that $\mu \in \mathcal{P}_2(\mathbb{R}^d)$ ?","['probability-distributions', 'expected-value', 'convergence-divergence', 'probability-theory', 'probability']"
3964625,Inverse of Multivariable Functions on Manifolds,"Consider the unit circle, $S=$ { $(x,y) | x^2 + y^2 =1$ } with the stereographic charts $(U_N,\phi_N)$ , $(U_S,\phi_S)$ i.e. $U_N=S$ \ { $(0,1)$ }, $\phi_N((x,y))=\frac{x}{1-y}$ $U_S=S$ \ { $(0,-1)$ }, $\phi_S((x,y))=\frac{x}{1+y}$ How would I find ${\phi_N}^{-1}$ and ${\phi_S}^{-1}$ ?
These functions are bijective on their domains ( $U_N$ and $U_S$ respectively) so their inverses exist. For ${\phi_N}^{-1}$ , I have considered a point $z$ in $\phi_N(U_N)$ and set $\frac{xy}{1-y}=z$ . I believe I need to write both x and y in terms of $z$ to get something like ${\phi_N}^{-1}(z)=(f(z), g(z))$ where I need to find $f(z)$ and $g(z)$ . And then similarly for $\phi_S$ . How would I do this? Thank you for any help!","['manifolds', 'multivariable-calculus', 'inverse-function', 'stereographic-projections']"
3964637,Mean Number of Samples from Uniform Distribution to Achieve a Desired Total,"Problem Statement I'm interested in the mean number of samples from a given uniform distribution of integers I would need to take to reach a desired total or more. For example, a six-sided dice is a uniform distribution of integers from $1$ through to $6$ . The number of times you have to roll a six-sided dice ( $r$ ) to achieve a desired total or greater ( $t$ ) produces a distribution. For example, here is a graph produced by sampling dice rolls, aiming for a total of $80$ per run: Graph of six-sided dice rolls with a target of $80$ The mean of this distribution is approximately $23.332448$ . This is the number I am interested in deriving. Simply 23 would be too imprecise for what I need. Right now I am determining this by simply sampling large numbers of dice rolls and then calculating the average of that data set, however, I would like to instead understand the statistics involved and compute the mean, or a reasonable approximation of the mean. One might try to derive the mean of the number of rolls needed $P(r=t)=0.5$ by simply using the mean of the uniform distribution, and dividing the total $t$ by that mean: $$\frac{t}{(max-min)/2+min} = \frac{80}{(6-1)/2+1} \approx 22.857$$ I will call this the 'naive' approximation. As you can see, this result can be significantly off the desired result. Sometimes, it happens to be close to the right answer, however, I am interested in many cases where it is unacceptably imprecise. Ultimately, I want to have a method or formula that takes an arbitrary continuous uniform distribution of integers (such as a die) and the desired total, which tells me the mean number of samples from that distribution I would need to take in order to reach that total. Prior Research I've looked at a few posts that talk about this subject, however, none of them were able to provide the complete answer to me. Here are two of the things I've found and read: A Similar Question on This Forum Number of dice rolls taken to reach a certain sum This was useful in creating an approximation of the actual distribution I'm trying to find. I used this to turn the following recursive function into the following code (in rust, dependency on the cached crate for memoizing the recursive results to improve performance to something usable): $$P(S_t=s) = \sum_{i=1}^6P(S_{t-1} = s-i)/6$$ With the following terminators: $P(S_0=0) = 1$ $P(S_0<0) = P(S_{t>0}=0) = 0$ #[cached]
pub fn p(t: i32, s: i32) -> f64 {
    if s <= 0 && t <= 0 {
        return 1.0;
    }

    if s <= 0 || t <= 0 {
        return 0.0;
    }

    (1..(std::cmp::min(6, s) + 1)).map(|i| p(t - 1, s - i)).sum::<f64>() / 6.0
} Which, when run, can be used to create a graph comparing the frequency of the sampled results to the frequency predicted by that algorithm. In the graph, the blue line is the result of the algorithm, while the orange line is sampled data from a simple PRNG using a uniform distribution (the same data as in the first graph). Note that to get this graph, you have to divide the results of this algorithm by the inverse of the mean of the uniform distribution (so $3.5^{-1}$ in this example) The author of the original answer suggests dividing by $P(S_t=36)$ (where $36$ was their target) however fails to realise that this is just an approximation of the inverse of the mean. I'm not actually sure why the mean of the uniform pops out here, so that's an interesting tidbit. This looks pretty good, however, it does not provide the mean number of rolls as best I can tell. The author of the answer to the linked question states that the mean in their example is ""about $10.5238577$ "" , however, they don't state how they derived that number and I haven't been able to figure it out. It's also not generalised to any uniform, however, I don't believe it would be difficult to generalise. I don't anticipate this to be an obstacle if someone finds out how to derive the mean from that answer. A Similar Question on Reddit On average, how many rolls of a fair die would it take to get a cumulative score above n? This was useful in creating a different approach to solving this problem, where I iterated $r$ (the number of rolls) upwards from 1, each time producing an approximation of the distribution of the combined rolls using a normal distribution. The first normal distribution to have a cumulative distribution function for the given target of less than $0.5$ indicated when the integer mean number of rolls was being made. Again, however, like the previous solution, this only produced the integer number of rolls. I need to know the mean number of rolls more precisely than this. There are a number of other answers than the top up-voted answer on this Reddit post with differing amounts of clear information. The answer from the user u/possiblywrong shows an interesting cumulative distribution function which I wonder if it would solve my problem. However, I am unable to figure out how to reproduce their ideas as code or fully understand them. For completeness sake, here is an image of that response in case it is deleted in the future . Thanks Thank you for any time you spend on this, this is doing my head in. It feels like it should be so easy to model, and yet I've lost an entire day researching it only to find vague answers that are either too complicated for me, or are simply missing information that I need to understand them. Hopefully, you can help.","['dice', 'probability-theory', 'probability']"
3964678,Calculating expected value of non-empty chambers. What is wrong in my solution?,"Instering $5$ different balls to $12$ different chambers, all the options are equally probabable. Let $X$ be the number of non-empty chambers, calculate $\mathbb{E}[X]$ I know that $\mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t)$ so I just need to calculate $\mathbb{P}(X=t)$ for every $1\leq t \leq 5$ $$\mathbb{P}(X=1)= \frac{1}{12^4}$$ $$\mathbb{P}(X=2)= \frac{11 \cdot 2^3}{12^4}$$ $$\mathbb{P}(X=3)= \frac{11\cdot 10\cdot 3^2}{12^4}$$ $$\mathbb{P}(X=4)= \frac{11\cdot 10 \cdot 9 \cdot4}{12^4}$$ $$\mathbb{P}(X=5)= \frac{11\cdot 10 \cdot 9 \cdot8}{12^4}$$ now I just substitute this values in the expression $$\mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t)$$ but I am not getting the right answer. I assume I have mistake in develop the expressions above. The options are: A. $\displaystyle \frac{161051}{20736}$ B. $\displaystyle \frac{161051}{248832}$ C. $\displaystyle \frac{87781}{20736}$ D. $\displaystyle \frac{87781}{248832}$","['expected-value', 'solution-verification', 'probability']"
3964688,Application of Lang' theorem about finite groups,"Let $G$ be a connected algebraic group defined over the finite field $\mathbb{F}_q$ , and let $F: G \to G$ be the Frobenius morphism. Show that $G^F = \{g \in G\mid g = F(g)\}$ is a finite group; For $x\in G^F$ , let $Z_G(x) = \{g\in G\mid gx = xg\}$ . Show that $Z_G(x)$ is $F$ -stable, i.e., $g\in Z_G(x)$ implies $F(g) \in Z_G(x)$ ;  and if $Z_G(x)$ is connected, $y\in G^F$ is conjugate to $x$ in $G$ , then $y$ is conjugate to $x$ in $G^F$ . I have proved 1. and the first half of 2. About the 2nd half Let $h \in G$ , s.t. $hyh^{-1} = x$ , hence I want to prove $\{F^n(h)\}_{n \geq 0} \cap G^F \neq \emptyset$ . But I have no idea using the connectivity of $Z_G(x)$ to prove this. Any help will be appreciate.","['group-theory', 'abstract-algebra', 'finite-groups', 'algebraic-groups']"
3964731,Problem in proof that $ \lim_{x\to0}\frac{\sin(x)}{x}$=$1$,"I'm having some confusion on understanding the proof that $$\lim_{x\to 0}\frac{\sin x}{x}=1.$$ In the proof on khanacademy and on other various sources we always end up with the inequality $$\cos(x)\le\frac{\sin(x)}{x}\le1\
$$ And then we apply the squeeze theorem. However in the proof we always say that it is valid for any non zero x however if this is true how can we ever say that the inequality is less than or equal to each other my question is why isn't the inequality this instead $$\cos(x)<\frac{\sin(x)}{x}<1
$$ No one seems to explain how they make the less than or equal to statement true so which one is correct?","['limits', 'calculus']"
3964736,"Count the anagrams from ARARAQUARA, in such a way that there are not 2 As adjacent?","How many anagrams can be made from the word ARARAQUARA, in such a way that  there are not 2 As adjacent? My proposed solution: There are 10 letters, 5 As, 3 Rs,1 Q and 1 U. The general approach will be positioning the 5 As with spaces separating them (or before or after them) and counting the ways to fill these spaces with one or two letters. I divided the problem in 2 cases: (a) there is only 1 letter different from A between A's. There are two cases here, starting with non-A, or finishing with non-A. Total number of possibilities for the 2 cases, considering the 3 repeated Rs: $$2\times \frac{5!}{3!}=40.$$ (b) there is one pair of non-A letters between 2 A's (only one pair is possible). Cases for the pairs (considering order): RR, RQ, QR, RU,UR, QU, UQ. So the anagram possibilities for each pair is given by:
RR - $4!=24$ possibilities;
RQ - $4!/2!=12$ possibilities;
QR - $4!/2!=12$ possibilities;
UR - $4!/2!=12$ possibilities;
RU - $4!/2!=12$ possibilities;
UQ - $4!/3!=4$ possibilities; and,
QU - $4!/3!=4$ possibilities. So the total number of anagrams considering restrictions is: 40+24+4 12+2 4=120. My problem: the answer given is 128. Am I missing cases? or the answer is wrong?","['solution-verification', 'combinatorics']"
3964818,Analytic continuation of the Kronecker Delta,"The Kronecker Delta can be written as the integral of the complex function $$f(n,z)=\frac{1}{2\pi i} z^{n-1} \ ,$$ where $n\in \mathbb{Z}$ and $z\in\mathbb{C}$ on a closed path $\mathcal{C}$ enclosing the origin $$ \delta_{n,0}= \oint_\mathcal{C} f(n,z) dz \ .$$ This can be seen as a trivial consequence of the Cauchy integral theorem $$ \delta_{n,0}= \oint_\mathcal{C} dz \frac{1}{2\pi i} z^{n-1}  = \mathrm{Res}_{z=0}(z^{n-1}) \ .$$ The same function computed for $n=i\eta$ with $\eta\in\mathbb{R}$ and integrated on the real axis $$ \int_0^\infty dy \, f(i \eta,y) =\int_0^\infty dy \frac{1}{2\pi i} y^{i\eta-1}  = \int_{-\infty}^\infty dx \frac{1}{2\pi i} e^{i \eta x} = -i \delta(\eta) \ \ .$$ My question is the following. Is there a master function $$ G(n) \equiv \int_\gamma dz f(n,z)
$$ defined as the integral on some path $\gamma$ such that $G(n)\propto \delta_{n,0}$ and $G(i \eta)\propto \delta(\eta)$ ? What are the path and the proportionality constants? I do not know if it is useful but I noticed that $z^{n-1}$ has a pole also at infinity.","['complex-analysis', 'kronecker-delta', 'dirac-delta', 'analytic-continuation']"
3964846,Existence of two normal vector fields with derivatives lying in $TM$,"Given a 1-codimensional manifold $M^n\hookrightarrow\mathbb{R}^{n+1}$ and any local normal vector field $\nu$ it's straightforward to prove that $\partial_j\nu\in TM$ , simply by differentiating the relation $\nu\cdot\nu=1$ . In higher codimension, say $M^n\hookrightarrow\mathbb{R}^{n+2}$ , this is not always true (it's enough to consider Frenet equations for a curve in the space).
I was wondering if it is always possible to construct a basis for $TM^\perp$ with this property, that is, such that $$
\partial_j\nu_k\in TM\quad\forall j=1,\dots,n \text{ and }k=1,2.
$$ I made the following attempt. In a local chart for $M$ let $e_{1},\dots,e_{k}$ be a local orthonormal basis for $TM$ and $\nu_{1},\nu_{2}$ be an oriented orthonormal basis for $TM^{\perp}$ . First, notice that differentiating the relations $\nu_{k}\cdot\nu_{k}=1$ we obtain $$\partial_{i}\nu_{k}\cdot\nu_{k}=0$$ so that in general we can write $$\partial_{i}\nu_{1}=\alpha_{i}^{k}e_{k}+\beta_{i}^{1}\nu_{2}$$ $$\partial_{i}\nu_{2}	=\gamma_{i}^{k}e_{k}+\beta_{i}^{2}\nu_{1}$$ Moreover, differentiating $\nu_{1}\cdot\nu_{2}=0$ we find $$0=\partial_{i}\left(\nu_{1}\cdot\nu_{2}\right)=\partial_{i}\nu_{1}\cdot\nu_{2}+\nu_{1}\cdot\partial_{i}\nu_{2}=\beta_{i}^{1}+\beta_{i}^{2}$$ so that $\beta_{i}^{1}=-\beta_{i}^{2}=:\beta_{i}$ and we rewrite $$\partial_{i}\nu_{1}	=\alpha_{i}^{k}e_{k}+\beta_{i}\nu_{2}$$ $$\partial_{i}\nu_{2}	=\gamma_{i}^{k}e_{k}-\beta_{i}\nu_{1}.$$ In general any oriented orthonormal basis for $TM^{\perp}$ can be written as a $SO(2)$ -transformation of $\{\nu_{1},\nu_{2}\}$ , so we set $$\mu_{1}	=\cos\theta\nu_{1}-\sin\theta\nu_{2}$$ $$\mu_{2}	=\sin\theta\nu_{1}+\cos\theta\nu_{2}$$ with $\theta$ being a smooth function defined locally on $M$ . We compute \begin{align}
\partial_{i}\mu_{1}	&=-\sin\theta\partial_{i}\theta\nu_{1}+\cos\theta\partial_{i}\nu_{1}-\cos\theta\partial_{i}\theta\nu_{2}-\sin\theta\partial_{i}\nu_{2}
	\\&\in TM-\left(\partial_{i}\theta-\beta_{i}\right)\sin\theta\nu_{1}-\left(\partial_{i}\theta-\beta_{i}\right)\cos\theta\nu_{2}
\partial_{i}\mu_{2}	\\&=\cos\theta\partial_{i}\theta\nu_{1}+\sin\theta\partial_{i}\nu_{1}-\sin\theta\partial_{i}\theta\nu_{2}+\cos\theta\partial_{i}\nu_{2}
	\\&\in TM+\left(\partial_{i}\theta-\beta_{i}\right)\cos\theta\nu_{1}-\left(\partial_{i}\theta-\beta_{i}\right)\sin\theta\nu_{2}
\end{align} So we get to the result if we find a function $\theta$ such that $d\theta=\beta$ , where $\beta=\beta_{i}dx^{i}$ . By Poincaré lemma, such $\theta$ exists if $\beta$ is a closed form, so the claim becomes $$\partial_{i}\beta_{j}=\partial_{j}\beta_{i}\quad\forall i,j=1,\dots,n.$$ Now, to prove this I used that $\beta_i=\partial_i\nu_1\cdot\nu_2$ and hence $$
\partial_j\beta_i=\partial_{ji}\nu_1\cdot\nu_2+\partial_i\nu_1\cdot\partial_j\nu_2
$$ Similarly $$
\partial_i\beta_j=\partial_{ij}\nu_1\cdot\nu_2+\partial_j\nu_1\cdot\partial_i\nu_2
$$ Taking the difference we find $$
\partial_{j}\beta_{i}-\partial_{i}\beta_{j}=\partial_i\nu_1\cdot\partial_j\nu_2-\partial_j\nu_1\cdot\partial_i\nu_2.
$$ From here I'm stuck. I tried several times applying different orthogonality conditions but I can't show that the r.h.s. is $0$ . I suspect there may be some relation between $\nu_1$ and $\nu_2$ that I'm missing.
I'm interested especially in the case where $M$ is a surface in $\mathbb{R}^4$ . Also, could the minimality of $M$ be of any help here? Any hint is very appreciated.","['vector-fields', 'orthogonality', 'geometry', 'differential-forms', 'differential-geometry']"
3964860,Infinite lists in Lambda calculus....,"I have been learning Haskell for a number of months now and am now trying to understand the underpinning $\lambda$ -calculus,  but I have run into a bit of a mental block! How would I go about writing a $λ$ -term corresponding to an infinite list (for example $[0, 1, 2, \dots]$ )?","['lambda-calculus', 'computability', 'discrete-mathematics', 'computer-science']"
3964873,Maximum number of permutations not repeating smaller permutations,"There are $n$ soldiers, lining up every morning for their military service. The commander demands that the morning lineup of these soldiers be arranged differently for every next day according to the following rule: ""Any $m$ soldiers cannot be lined up next to each other in the same order for other days."" What is the maximum number of days $f(n,m)$ that this can happen? This is a generalization of 8 soldiers lining up for the morning assembly ( $n=8,m=3$ ). The generalized problem can be rewritten in terms of graph theory, Define a graph $G$ whose $|V|=n!$ vertices are $n$ -permutations on $n$ symbols $V=S_n$ . Any two vertices (permutations) are connected by an edge iff they share a $m$ -permutation. What is the independence number of graph $G$ ? For example, in the case $(n,m)=(3,2)$ graph $G$ consists of two $C_3$ cycle components. The independence number of $C_3\simeq K_3$ is one. Therefore, the answer is $f(3,2)=1+1=2$ . The problem can also be stated in terms of integer linear programming, Let binary decision variable $x_v\in\{0,1\}$ indicate whether vertex (permutation) $v\in V$ is used on one of the days or not. Let $V_t\subset V$ be the set of all permutations that contain a specific $m$ -permutation $t\in T$ . $$
\text{Maximize}\quad \sum_{v\in V} x_v\quad \text{subject to}\quad \sum_{v\in V_t} x_v \le 1, t\in T.$$ Notice that this is a special case of the set packing problem , with lot of symmetry. My question is, Can we find a closed form for $f(n,m),m\le n$ that answers the generalization? It is trivial that $f(n,1)=1$ and that $f(n,n)=n!$ . Each day contributes $(n-m+1)$ of $m$ -permutations giving an upper bound $$f(n,m)\le \frac{n!}{(n-m+1)!}.$$ I've implemented the linear programming formulation and computed: $$\begin{array}{c|ccccccccc}
n\overset{\LARGE\setminus}{\phantom{.}}\overset{\Large m}{\phantom{l}}&1&2&3&4&5&6&7&8&9\\
\hline
1&1&-&-&-&-&-&-&-&-\\
2&1&2&-&-&-&-&-&-&-\\
3&1&\color{red}{2}&6&-&-&-&-&-&-\\
4&1&4&12&24&-&-&-&-&-\\
5&1&\color{red}{4}&20&\color{red}{48}&120&-&-&-&-\\
6&1&6&30&120&360&720&-&-&-\\
7&1&7&42&?&?&\color{red}{2160}&5040&-&-\\
8&1&8&56&?&?&?&20160&40320&-\\
9&1&9&?&?&?&?&?&\color{red}{161280}&362880\\
\end{array}$$ Notice that for some cases $(n,m)\in\{(3,2),(5,2),(5,4),(7,6),(9,8),\dots\}$ , the result $\color{red}{f(n,m)}$ does not reach the upper bound. Can we determine all cases that do not reach the upper bound? The $?$ cases haven't been computed yet. Can we calcuate these in reasonable time? Edit: Alternatively, let soldiers be labelled as $\{0,1,2,\dots,n-1\}$ . Can we find $f(n,m)$ by explicitly constructing solutions? For example, if $m=2$ and $n+1$ is a prime number, then following $n$ permutations $$
P_{(n,2)}=\left\{ \left( (ka-1)\bmod{(n+1)}  ,a=1,2,\dots,n \right) , k=1,2\dots,n\right\}
$$ Satisfy the problem. Therefore, if $n+1$ is prime then $f(n,2)=n$ . E.g. $P_{(6,2)}=\{(012345), (135024), (251403), (304152), (420531), (543210)\}$ . Can we construct $n$ permutations for $m=2$ when $n+1$ is not prime? Can we do the same for all $(n,m)$ cases which reach the upper bound?","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-optimization', 'recreational-mathematics']"
3964910,"If a normalized sequence of finite measures converges weakly, does the same hold true for the nonnormalized one?","Let $E$ be a metric space, $(\mu_n)_{n\in\mathbb N}$ be a sequence of finite nonnegative measures on $\mathcal B(E)$ and $\mu$ be a probability measure on $\mathcal B(E)$ s.t. $$\frac{\mu_n}{\mu_n(E)}\xrightarrow{n\to\infty}\mu\tag1$$ with respect to the to the topology of weak convergence of measures . Assuming that $c:=\sup_{n\in\mathbb N}\mu_n(E)<\infty$ , are we able to deduce that the nonnormalized sequence $(\mu_n)_{n\in\mathbb N}$ is convergent as well? If not, does the other direction of this assertion hold, i.e. can we infer that the normalized sequence (i.e. the left-hand side of $(1)$ ) is convergent from knowing that the nonnormalized sequence is convergent? It's easy to see that $(\mu_n(E))_{n\in\mathbb N}$ is convergent in that case (since the function constantly equal to $1$ is bounded and continuous). So, this should be possible to obtain. Remark : Without assuming $c<\infty$ , the first implication is clearly wrong. We simply can consider any finite nonnegative measure $\nu$ on $\mathcal B(E)$ and set $\nu_n:=n\nu$ for $n\in\mathbb N$ .","['measure-theory', 'probability-theory', 'weak-convergence']"
3964925,How can we show that there is a zero of $f$,"Let $$f(x)= x^2+2020x+1$$ a polynomial.
We define $F_n(x)$ to be $f(f(f...f(x)))$ applied $n$ times .
Show that for all positive integer $n\ge1$ there exists a real $x$ such that $F_n(x)=0$ Please help me, I've tried induction but it doesn't seem to work. $n=1$ is trivial but then it's hard. I don't know much about polynomials. Any ideas?","['contest-math', 'functions', 'polynomials']"
3964948,Cutting space by hyperplanes: a direct proof,"The number of regions formed by cutting $k$ generic hyperplanes out of $\mathbb R^n$ is: $$C_n(k)=\binom{k}{0}+\binom{k}{1}+\dots+\binom{k}{n}$$ The usual proof is by induction, counting how many new regions are formed by adding the $k^\text{th}$ plane and utilizing the formula $C_n(k)=C_n(k-1)+C_{n-1}(k-1)$ . This formula resembles Pascal's rule $\binom{n}k=\binom{n}{k-1}+\binom{n-1}{k-1}$ and indeed, it can be proven by rearranging sums and applying Pascal's rule. But we can also view $C_n(k)$ as the number of subsets of $\{1,2,\dots,k\}$ of size $\leq n$ . Then the formula $C_n(k)=C_n(k-1)+C_{n-1}(k-1)$ admits essentially the same combinatorial proof as Pascal's rule, where we break up $C_n(k)$ depending on whether or not the subset contains $k$ . To better understand the relationship between $C_n(k)$ and hyperplane cutting, I wonder if there is a direct proof (i.e. not using induction) of the above-mentioned result? Since there are $k$ hyperplanes, it seems reasonable that $C_n(k)$ might count subsets of these hyperplanes of size $\leq n$ . But the question that currently stumps me is: how does one then biject these subsets of the planes with the regions?","['euclidean-geometry', 'combinatorics', 'combinatorial-proofs']"
3964989,Prove that a recurrent sequence $u_n$ is convergent if $\lim \limits_{n \to \infty}(u_{n+1}-u_n)=0$,Let $f:[a;b] \to [a;b]$ be a continuous function ( $a$ and $b$ real numbers) and $(u_n)$ a sequence defined by $u_0 \in [a;b]$ and $u_{n+1}=f(u_n)$ Prove that if $\lim \limits_{n \to \infty}\left(u_{n+1}-u_n\right)=0$ then $(u_n)$ converges I tried using subsequences and the bolzano weierstrass theorem but couldn't even get close. Any help would be welcome.,['sequences-and-series']
3964997,Non-projective variety which is topologically homeomorphic to closed subset of $\mathbb{P}^n_k$,"Definition by Mumford's red book : An affine variety is defined as an irreducible algebraic subset of $\mathbb{A}^n$ with a sheaf of rings of $k$ -valued functions. A projective variety is defined as an irreducible algebraic subset of $\mathbb{P}^n$ with a sheaf of rings of $k$ -valued functions (page 28). A variety is defined as a separated prevariety (page 37) and a prevariety is a locally ringed space which is locally isomorphic to an affine variety as locally ringed space . Question : If we have a non-projective irreducible variety, then is it possible that it is topologically homeomorphic to a closed subset of $\mathbb{P}^n_k$ in the Zariski topology?","['algebraic-geometry', 'commutative-algebra', 'projective-varieties']"
3965032,Primes in the form $n^{k}+n-1$,"Lately I've been (very casually) toying with primes in the form $n^k+n-1$ , as a very far-reaching generalization of Fermat primes. (you get a Fermat prime when you set $k=2^m$ and $n=2$ ). I have little training in number theory, so mainly I've been just typing stuff into Wolfram and searching for patterns. One thing I started doing was looking for the smallest $k$ such that $n^k+n-1$ is prime for any particular $n$ . I've been able to find solutions for $n \leq 106$ . Obviously $k=1$ and $k=2$ do a lot of heavy lifting, but there are some remarkable numbers here. The one that stunned me the most upon finding it was $32^{108}+31 \approx 3.6 \times 10^{162}$ , but there's an even bigger smallest solution: $80^{194}+79 \approx 1.58 \times 10^{369}$ . For $n=107$ , there are no solutions for $k \leq 495$ , which is about as far as I can reliably check.  Obviously any $k$ that works must be divisible by $4$ . So, three things I wonder about are: Is any solution for $n=107$ known? Does it exist for sure? In general, has it been shown that there is a solution for any $n$ ? For any $k$ not in the form $6m+5$ , is there $n$ such that $k$ is the smallest solution? EDIT: Let $f(n)$ be any polynomial $\mathbb N \rightarrow \mathbb N$ . Let $\text{GCD}[f]$ denote the greatest common divisor of $\{f(n): n \in \mathbb N\}$ . There is this long-standing conjecture by Bouniakowsky that $f$ is irreducible iff there are infinitely many numbers $n$ such that $\frac{f(n)}{\text{GCD}[f]}$ is prime. If we suppose this is true, then the following statement implies a positive answer to question 3: For any $k: k \not \equiv 5 \pmod 6$ there are natural numbers $\alpha,\beta$ such that $\text{GCD}[(\alpha n+\beta)^k+(\alpha n+\beta)-1]=1$ , but $\text{GCD}[(\alpha n + \beta)^m+(\alpha n + \beta)-1] \neq 1$ for any $m<k: m \not \equiv 5 \pmod 6$ . Example: for $k=3$ , one can let $\alpha = 15, \beta = 2$ . To my untrained eye, this seems approachable.","['number-theory', 'prime-numbers']"
3965057,$\frac{d}{dx}(\sin(x^{\frac{1}{3}}))$ from first principle,"The question contains a hint: at the appropriate point use the result $a^{3} - b^{3} =
\left(a - b\right)\left(a^{2} + b^{2} + ab\right)$ . $$
\frac{{\rm d}}{{\rm d}x}\sin\left(x^{1/3}\,\right)
$$ My attempt: I did not use the hint as it was not immediately obvious to me what the simplification was. Instead I wrote with binomial expansion that $(x+h)^{\frac{1}{3}}=x^{\frac{1}{3}}(1+\frac{h}{x})^{\frac{1}{3}} \approx x^{\frac{1}{3}}(1+(\frac{1}{3})\frac{h}{x})$ , used $\sin(()+())=\sin()\cos()+\sin()\cos()$ and applied $\cos() \approx 1$ , $\sin() \approx ()$ for small $h$ which gave the correct answer. But I do not see the point of using the cubes difference formula given in the question? Could this be a typo because it would make more sense to apply the identity to something like e.g. $\sin({x})^{\frac{1}{3}} (\text{not }\sin(x^{\frac{1}{3}}))\to \sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}} \iff (\sin(x+h)-\sin(x)=(\sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}})(\sin(x+h)^2+\sin(x)^2+\sin(x+h)\sin(x))$ Or am I missing something here?","['trigonometric-series', 'calculus', 'derivatives', 'trigonometry']"
3965070,Integration and periodic divergent series,"Consider the divergent infinite series $$ \sum_{n=1}^\infty 5^n \cos(nt) $$ where $t \in \mathbb{R}$ .  I don't believe it has a closed form.  However, if I take the integral with respect to $t$ : $$ \int_{-\pi}^\pi \sum_{n=1}^\infty 5^n \cos(nt) \, dt = \sum_{n=1}^\infty 5^n \int_{-\pi}^\pi \cos(nt) \, dt$$ $$ = \sum_{n=1}^\infty 5^n \cdot 0 = 0$$ So even though the series is divergent it still has a finite integral.  This seems to be a consequence of the fact that it's periodic over the integration domain: the infinities sort of cancel each other out. This can extend also to the product of series: $$ \int_{-\pi}^\pi \left( \sum_{n=1}^\infty 5^n \cos(nt) \right) \left(\sum_{n=1}^\infty \frac 1 {7^n} \cos(nt) \right) \,dt$$ $$ = \pi \sum_{n=1}^\infty \left( \frac 5 7 \right)^n = \frac 5 2 \pi$$ I'm trying to understand the underlying properties at work here.  Specifically, is there a way to come up with a ""closed form"" for a periodic infinite divergent series which behaves the same under integration (for instance, something I could use with integration by parts) without actually being the limit of the partial sums of $f(t)$ ?  That is, something like a proxy function with the infinities sort of preemptively normalized out?","['integration', 'divergent-series', 'periodic-functions', 'sequences-and-series']"
3965088,$\frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k])$ converges a.s. to $0$,"Let $X_1,X_2,\dots$ be a sequence of independent random variables, such that the series $$\sum_{n=1}^\infty\frac{\operatorname{Var}(X_n)}{n^2}$$ converges. Show that as $n\to\infty$ , $$\frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k])$$ converges almost surely to $0$ . There is a quick solution via the martingale convergence theorem: we have that $$Y_n=\sum_{k=1}^n\frac{X_k-\mathbb E[X_k]}{k}$$ is martingale, and $\sup_n\mathbb E[|Y_n|]$ is finite, so $Y_n$ converges almost surely to some random variable $Y$ , and we can finish with Kronecker's lemma. I'm interested though in any approaches avoiding the heavy machinery of the martingale convergence theorem. I feel like defining the $Y_n$ 's as I did above could be fruitful. For example, the Kolmogorov inequality gives the bound $$\mathbb P\left(\max_{1\leq i\leq n}|Y_i|>\varepsilon\right)\leq\frac{1}{\varepsilon^2}\mathbb E\left[Y_n^2\right]=\frac{1}{\varepsilon^2}\cdot\sum_{k=1}^n\frac{\operatorname{Var}(X_k)}{k^2}.$$ But I'm unsure what to make from all this.","['probability-theory', 'probability']"
3965134,"A problem of ""guessing number""","The problem statement: I have two people A and B, and think of a natural number n. Then I
give the number n to A and the number n + 1 to B. I tell them that
they have both been given natural numbers, and further that they are
consecutive natural numbers. However, I don't tell A what B's number
is and vie versa. I start by asking A if he knows B's number. He says
\no"". Then I ask B if he knows A's number, and he says \no"" too. I go
bak to A and ask, and so on. A and B an both hear each other's
responses. Do I ever get a \yes"" in response? If so, who responds first
with \yes"" and how many times does he say \no"" before this? Assume
that both A and B are very intelligent and logical. I understand we should start from a simple case where $A = 1$ , and $B = 2$ ---- obviously number of ""no"" from $A$ is $0$ . Then for $A = 2, B = 3$ case, $A$ responds with ""no"" because $B$ could be $1$ or $3$ , then $B$ responds with ""no"", so $A$ knows $B$ is not $1$ , then in the second respond he says ""yes"". So on and so forth. However, it's not clear to me how should I extend the case to $n$ and $n+1$ ? Maybe better to have a recursive relation between $(n,n+1)$ and $(n-1,n)$ case?",['algebra-precalculus']
3965164,The equation formula for a circle laying in an arbitrary plane,"I know the standard and expanded forms of the equation of the circle in the simple 2d space, ${(x-a)}^2+{(y-b)}^2=r^2$ $x^2-2ax+y^2-2by=c$ So in 3d space what are the equations for a circle laying in an arbitrary plane,
and what is the 3d version of the polar form of the circle equation $$(a+rcos(t),b+rsin(t))$$ How the two equations (that I mentioned up) look in 3d space and are there more abstractive formulae for the circle equation in more than 3 dimensions?","['linear-algebra', 'geometry', 'circles', '3d']"
3965192,Eigenfunction expansion of Gaussian kernel over a closed interval,"$\newcommand{\Xc}{\mathcal{X}}$ Let $\Xc=[-1,1]$ and consider a Gaussian kernel $k(x,t)\propto \exp(-(x-t)^2/2\sigma^2)$ for some $\sigma>0$ on $\Xc$ .
I am looking for an eigenfunction expansion of the induced integral operator $\mathbf{K}\colon L^2(\Xc)\to L^2(\Xc)$ .
That is, I wish to find a countable set of functions $\{\phi_n\colon \Xc\to \mathbb{R}\}_{n=1}^\infty$ such that $$
k(x,t) = \sum_{n=1}^{\infty} \lambda_n \phi_n(x)\phi_n(t)
$$ and $\int_{\Xc}\phi_m(x)\phi_n(x)d x=\delta_{mn}$ . Here are some related results I am aware of: Hermite polynomials characterize the $L^2(\rho)$ -orthonormal eigenfunctions of Gaussian kernels over $\mathbb{R}$ for a Gaussian weight $\rho$ [See Sec. 6.2] . Spherical harmonics characterize the $L^2(\Xc)$ -orthonormal eigenfunctions of Gaussian kernels over the hypersphere $\Xc=S^{d-1}$ (for a uniform weight) [See Thm. 2.] .","['reproducing-kernel-hilbert-spaces', 'gaussian', 'functional-analysis', 'eigenfunctions']"
3965203,Constructing non-isomorphic groups such that the counts of elements of given order match,"In one of the simplest ways of proving that two groups aren't isomorphic, we can compare the orders of each element of both groups. If we find an order $\ell$ such that one of the groups has more elements of this order than the other group, we're done. I am interested in finite groups where this fails --- ones that aren't isomorphic, but the counts for each order are the same: $$\DeclareMathOperator\ord{ord} G \not\cong H \quad \land \quad \forall\ell.\ |\{g \in G \mid \ord g = \ell\}| = |\{h \in H \mid \ord h = \ell\}|$$ With some SageMath code, I have computed that the smallest groups where this happens are of order 16. However, I am more interested in how one could methodically find such a counterexample, apart from just trying out groups you can think of until something sticks. How would you find such a pair of groups without a computer?","['group-theory', 'group-isomorphism', 'finite-groups']"
3965204,Square of the Error Function,"The author defines the probability integral as follows $$\Phi(z)=\frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2}\mathrm{d}t$$ .
(S)he instructs the reader to derive the following integral representation of the square of the probability integral by transforming it to polar coordinates $$\Phi(z)^2=1-\frac{4}{\pi}\int_0^1\frac{\exp(-z^2(1+t^2))}{1+t^2}\mathrm{d}t$$ .
I am uncertain how transforming the integral from cartesian coordinates to polar coordinates would yield the integral representation. Can someone explain this to me? $$I^2=\frac{4}{\pi}\int_{0}^{z}\int_{0}^{z}e^{-(x^2+y^2)}dydx=-\frac{2}{\pi}\int_{0}^\frac{\pi}{2}\int_{0}^{z}-2re^{-r^2}drd\theta$$ ?","['multivariable-calculus', 'special-functions']"
3965224,Proof $\overline X_n \rightarrow \mu$ a.s. given $\sqrt{n}(\overline X_n - \mu) \rightarrow Z$ in distribution,"Given $X_1...X_n...$ i.i.d. random variables with $E|X_1|<\infty$ and $\sqrt{n}(\overline X_n - \mu) \rightarrow Z$ in law for a certain random variable Z and a certain real number $\mu$ (not necessarily equal to $EX_1$ ), I need to proof that $\overline X_n \rightarrow \mu$ a.s. It is very similar to the central limit theorem but the problem is I don’t know if the variance is finite, so I cannot use it. Any hint on how can I approach this problem?","['probability-theory', 'probability']"
3965247,"Power set of $\{\varnothing, \{ \varnothing\}\}$","The power set of $\{\varnothing, \{ \varnothing\}\}$ is defined as follows: $$\{\varnothing, \{ \varnothing\}, \{\{\varnothing\}\}, \{\varnothing, \{\varnothing\}\}\}$$ Why do we have the 3rd element in the answer $\{\{\varnothing\}\}$ ?","['elementary-set-theory', 'discrete-mathematics']"
3965311,Can I use divergence theorem right after using Stokes' theorem?,"Stokes’ theorem converts a line integral to a surface integral. Divergence theorem relates a surface integral to a triple integral. So is it possible to take the result from Stokes’ theorem, and apply the divergence theorem to it? I’m not good at using latex, else I would’ve written down what I meant.","['divergence-theorem', 'multivariable-calculus', 'calculus', 'stokes-theorem', 'vector-analysis']"
3965314,show this $\lceil \frac{n}{1-a_{n}}\rceil =n+1$,"let $a_{n}$ be squence such $a_{1}=2-\dfrac{\pi}{2}$ , and $$a_{n+1}=2a_{n}+\dfrac{a_{n}-1}{n},n\in N^{+}$$ show that $$f(n)=\lceil \dfrac{n}{1-a_{n}}\rceil =n+1$$ I try:since $$f(1)=\lceil \dfrac{1}{\dfrac{\pi}{2}-1}\rceil=2$$ and $$a_{2}=2a_{1}+a_{1}-1=3a_{1}-1=5-\dfrac{3}{2}\pi$$ so $$f(2)=\lceil \dfrac{2}{\dfrac{3\pi}{2}-4}\rceil=3$$ and so on ,if we want prove $$f(n)=\lceil \dfrac{n}{1-a_{n}}\rceil =n+1$$ or $$n<\dfrac{n}{1-a_{n}}\le n+1$$ or $$0<a_{n}\le\dfrac{1}{n+1}$$ if we use induction to prove it.then $$a_{n+1}=(2+\dfrac{1}{n})a_{n}-\dfrac{1}{n}\le (2+\dfrac{1}{n})\cdot\dfrac{1}{n+1}-\dfrac{1}{n}=\dfrac{1}{n+1}$$ is wrong,becasue we want to prove $a_{n+1}\le\dfrac{1}{n+2}$",['sequences-and-series']
3965324,Deriving Generating function for centered trinomial coefficients,"Let $c_n$ denote the $n$ -th center trinomial coefficient (OEIS sequence here ). It appears they cannot be generated by a linear recurrence relation, so how should I go about to find the generating function $G(x)$ for the sequence? $$G(x)=\sum_{n=0}^{∞}c_nx^n=1+x+3x^2+7x^3+19x^4+51x^5+141x^6+...$$ The geometric ratio appears to have a limit close to $$\lim_{n\to ∞}\frac{c_{n+1}}{c_n}=2.95...$$ (these are the successive ratios of the last two listed terms in the OEIS sequence). Also, what is the interval of convergence (and divergence)? Based on the geometric limit, it seems that $G(1/3)$ will converge. Edit: The generating function is $$G(x)=\frac{1}{\sqrt{1-2x-3x^2}}$$ Any idea as to how this answer is derived?","['limits', 'multinomial-coefficients', 'generating-functions', 'sequences-and-series']"
3965361,Why does the cube have the fewest facets among (centrally) symmetric polytopes in $\mathbb{R}^n$?,"""A body like the cube, which is bounded by a finite number of flat facets, is called a
polytope. Among symmetric polytopes , the cube has the fewest possible facets, namely $2n$ ."" I am looking for rigorous proof and also intuition behind why this must be true. I'm thinking of doing something by contradiction, i.e. assuming less than $2n$ facets for a symmetric polytope and then going ahead from there? I don't have much to add here in terms of how I approached this , because this isn't a problem - it is something I came across while reading some lecture notes! I could not find much on this online, and I'd appreciate any help. Edit: It seems the author is talking about centrally symmetric polytopes, i.e. $x\in K$ whenever $-x\in K$ .","['convex-geometry', 'polytopes', 'geometry']"
3965384,On finite separable morphisms,"Let $f:X\longrightarrow Y$ be a finite separable morphism of smooth projective varieties over an algebraically closed field $k$ . Let $d$ be the degree of $f$ . We have an exact sequence $$0\longrightarrow\mathcal{O}_Y\longrightarrow f_\ast\mathcal{O}_X\longrightarrow \mathcal{F}\longrightarrow 0$$ When $\operatorname{char} k\nmid d$ the above exact sequence splits via $\frac{1}{d}Tr:f_\ast\mathcal{O}_X\longrightarrow \mathcal{O}_Y$ , hence $H^0(Y,\mathcal{F})=0$ . Does the latter equality hold also in the case $\operatorname{char} k\mid d$ ? I'm particularly interested in the case of curves.","['separable-extension', 'algebraic-geometry', 'positive-characteristic']"
3965402,Differentiability of $fg$ when one differentiable and other not but product differentiable.,"I conjectured the following theorem : Let $I$ be an open interval and let $a\in I.\,\,$ If $f,g:I\to\mathbb{R}$ such that $f$ is differentiable at $x=a$ with $f(a)=f'(a)=0$ and $g(x)$ is a bounded function over $(a-h,a+h)$ for some $h>0$ , then $f(x)g(x)$ is differentiable at $x=a.$ Proof: Let $\phi(x)=f(x)g(x)$ and let $\exists\,\, M>0\text{  such that }|g(x)|<M\,\,\forall \,\,x\in(a-h,a+h)$ $\lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=\lim_{h\to 0}\frac{f(a+h)g(a+h)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}g(a+h)$ Now $-M\left |\frac{f(a+h)-f(a)}{h}\right | \le \left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le M\left |\frac{f(a+h)-f(a)}{h}\right |$ $\implies 0\le \lim_{h\to0}\left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le 0$ $\implies \lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=0$ $\implies \phi(x)$ is differentiable. Is this working correct. Can we further generalised it ? any similar results most welcome?","['calculus', 'derivatives', 'analysis']"
3965426,A square appears randomly within a square of ten time its area. What is probability that the smaller square contains the larger square's center?,"The alignment relative to the larger square can be anywhere between 0° to 90° (0° or 90° being identical orientation to the larger square, 45° being all the way skewed), and the probability distribution of the alignment is based on percentage of total possibility given the constraint guaranteeing that it will fit within the larger square. The largest discrete probability, for an alignment of 0°=90°, should be simple enough to figure-out‡. The smallest discrete probability, for alignment of 45°, is a bit trickier but straightforward enough. Edit :The off-kilter version would be more likely to contain the center, since it has less possible places to exist and a larger proportion of them are the center. Intuitively this should correspond directly (though not necessarily uniformly) to what I am totally clueless about, which is: how to determine the probability  distribution for between 0° to 45°, ‡ 2. how this continuum corresponds to the probability of the original problem (e.g. {1/17?} for 0° up to {2/9?} for 45° back down to {1/17?} for 90°, along finite part of some sort of curve function presumably), and 3. how then to apply these equations formulated into the original problem to find the final probability that a square true-randomly generated wholly within a larger square with sides √(10)-times longer would surround (or contain exactly on an edge or corner) the center of the larger square. ‡ I haven't calculated the maximum or minimum ‘discrete’ probabilities (let alone any inbetween) yet. Those guesstimate values are placeholders. I am especially interested in the processes required to solve this, and appreciate any insight or clues.","['analytic-geometry', 'geometry', 'trigonometry', 'problem-solving', 'probability']"
3965440,Technique for finding cardinalites in combinatorics,"In combinatorics I see the following technique used a lot: Let A and B be sets. We try to find a function $f:A→B$ such that all the fibers of the elements of $B$ under $f$ have the same cardinality $\kappa$ . If we manage to do that, we conclude that $|A|=\kappa|B|$ . Example: Find the number of subsets of $\{1,2,3,4\}$ whose cardinality is $2$ . Let A be the set of ordered pairs that have two distinct elements from $\{1,2,3,4\}$ . Let B be the set of subsets of $\{1,2,3,4\}$ whose cardinality is $2$ . Let $f:A→B$ , $f(〈x,y〉)=\{x,y\}$ . The fiber of each $\{x,y\}∈B$ under $f$ has exactly $2$ elements: $〈x,y〉$ and $〈y,x〉$ . From that, we conclude that $|A|=2|B|$ , and we know that $|A|=4⋅3=12$ , so we get $|B|=6$ . My questions: Is there a name for this technique? Is there a name for this type of function?","['functions', 'combinatorics']"
3965513,Limit points of the set $\{\frac {\varphi(n) }n : n\in \mathbb{N}\}$,"Let $\varphi(n)$ denotes the cardinality of the set $\{a | 1\le a\le n, (a,n)=1\}$ where $(a , n)$ denotes the gcd of $a$ and $n$ . Which of the following is NOT true ? $(a)$ There exist infinitely many $n$ such that $\varphi (n)\gt \varphi(n+1)$ $(b)$ There exist infinitely many $n$ such that $\varphi (n)\lt \varphi(n+1)$ $(c)$ There exist $N\in \mathbb{N}$ such that $N\gt 2$ and for all $n\gt N$ , $\varphi (N)\lt \varphi(n)$ $(d)$ The set $\{ \frac {\varphi (n)}{n} : n\in \mathbb{N} \} $ has finitely many limit points. My Thinking: $(a)$ is true  by selecting $n=p$ ( an odd prime greater than $3$ ) Justification: Since $p$ is odd, $p+1$ is even. Now $\varphi(p)=p-1$ and $\varphi(p+1)\le p+1-\frac{p+1}{2}=\frac{p+1}2$ So $\varphi(p+1)\le \frac{p+1}2 \lt p-1=\varphi(p)$ By the infinitudes of primes, the result follows. $(b)$ Is true by taking $n=p-1$ where $p$ is an odd prime greater than $3$ and following the same line of reasoning. $(c)$ This statement (if true ) means that the set $\{\varphi(n) : n\gt N\}$ is bounded below. I think it is true My argument : Let $N=6$ . Then $\varphi(6)=2$ If $n\gt 6$ and is prime  , then clearly $\varphi(n)\gt 2$ Otherwise, if $n$ is composite, then let $n=p_1^{r_1} p_2^{r_2}...p_k^{r_k}$ . $\varphi(n)=p_1^{r_1-1}p_2^{r_2-1}...p_k^{r_k-1}(p_1-1)(p_2-1)..(p_k-1)$ If $\phi(n)=2$ , then the distinct primes can be just $2$ and $3$ . So the above equality reduces to $2=\phi(n)=2^{r_1}3^{r_2-1}$ when both the factors $2$ and $3$ are present . This gives $r_1=r_2=1$ and so $n=6$ When only $3$ present ,  then we get $n=3$ and if the factor is only $2$ , then $n=4$ . So the result follows as $N=6$ works $(d)$ This is NOT true. Justification :- Let $\{p_i\}$ be the enumeration of primes. Let an index $k$ be fixed and $i$ be the running index. Then if the only prime factors of $n_i$ are $p_k, p_i$ , then $\frac{\varphi(n_i)}{n_i}=\big( 1-1/p_k\big)\big(1-1/p_i\big)$ So ,letting $i\to \infty$ , we have $\lim_{i\to \infty}\frac{\varphi(n_i)}{n_i}=1-1/{p_k}$ Since $p_k$ is arbitrary, there are infinitely many limit points (maybe uncountably many) limits points of the given set. Please check if my work is correct or the answers can be shortened in any way. Thanks for your time.","['limits', 'solution-verification', 'elementary-number-theory']"
3965528,Proving the Alternate Series Test,"I am self-learning Real Analysis from Understanding Analysis by Stephen Abbott. I am finding it difficult to come up with a proof for the Alternating Series Test for an infinite series. I only have some initial ideas, so I'll put those down. I'd like to ask if somebody could help, by providing any hints (please do not write the complete proof), on how to think about this. Exercise 2.7.1 Proving the Alternating Series Test amounts to showing that the sequence of partial sums \begin{align*}
	s_n = a_1 - a_2 + a_3 - \ldots \pm a_n
\end{align*} converges. Different characterizations of completeness lead to different proofs. (a) Produce the Alternating Series Test by showing that $(s_n)$ is a Cauchy sequence. (b) Supply another proof for this result using the Nested Interval Property. (c) Consider the sequences $(s_{2n})$ and $(s_{2n+1})$ , and show how the Monotone Convergence Theorem leads to a third proof for the Alternating Series Test. Proof. (a)Let $(a_n)$ be a sequence satisfying (i) $a_1 \ge a_2 \ge a_3 \ge \ldots \ge a_n \ge a_{n+1} \ge$ (ii) $(a_n) \to 0$ Our claim is that, the alternating series $\sum_{n=1}^{\infty}(-1)^n a_n$ converges. Let $(s_n)$ be sequence of the partial sums, where \begin{align*}
s_n = a_1 - a_2 + a_3 - a_4 + a_5 - a_6 + \ldots + (-1)^{n+1}a_n
\end{align*} We are interested to show that $(s_n)$ is a Cauchy sequence. Let's see if can find a simple expression for $\lvert s_n - s_m \vert$ . If $n > m$ , \begin{align*}
\lvert (s_n - s_m) \rvert &= \lvert (-1)^{m+2}a_{m+1} + (-1)^{m+3}a_{m+2} + (-1)^{n+1}a_{n} \rvert\\
&= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert
\end{align*} A consequence of condition(i) is that $(a_n)$ is a decreasing sequence and has a lower bound zero, so $a_n \ge 0$ for all $n \in \mathbf{N}$ . Since $a_{m+2} \ge 0$ , $a_{m+4} \ge 0$ , $a_{m+6} \ge 0, \ldots$ we can write, \begin{align*}
\lvert (s_n - s_m) \rvert &= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert\\
&\le \lvert {a_{m+1} + a_{m+3} + a_{m+5} + \ldots + a_{n}} \rvert \\
&\le \vert {a_{m+1} + a_{m+1} + a_{m+1} + \ldots + a_{m+1}} \rvert
\end{align*} But this gives me a variable number of terms (whereas I need to prove that the distance $\lvert s_n - s_m \rvert$ becomes smaller than fixed $\epsilon$ or $k\epsilon$ . I am not quite sure, where to go from here. I know that the original sequence $(a_n)$ converges, so given any $\epsilon > 0$ , $\exists N \in \mathbf{N}$ , such that $\lvert a_n \rvert < \epsilon$ for all $n \ge N$ . (b) I am thinking of constructing a sequence of closed nested intervals $I_1 \supseteq I_2 \supseteq I_3 \supseteq \ldots \supseteq I_n \supseteq I_{n+1} \supseteq \ldots$ , in such a way that, $I_1=\{s_1,s_2,s_3,\ldots\}$ , $I_2 = I_1 - \{s_1\}$ and in general $I_{k+1} = I_k - \{s_k\}$ , as I am interested in the tail of the sequence of partial sums. Then, I can apply the nested interval property to show that $\bigcup_{k\ge 1}I_k$ is not empty. The one thing I don't know, is whether $(s_n)$ is bounded. If and only if this set is bounded, one may construct a closed interval. I think this is important for the construction. (c) Again, I need some bounds for $(s_n)$ , I guess.","['sequences-and-series', 'real-analysis']"
3965530,ODE and separations of variables: what about $0$?,"Considering the following ODE: $$ y'=f(x)g(y)$$ as long as $g(y) \neq 0$ , I can divide by $g(y)$ , and obtain $$ \frac{y'}{g(y)}=f(x)$$ and by solving $$ \int \frac{dy}{g(y)}= \int f(x)dx$$ I eventually find a solution $y$ on a domain $I$ (for the sake of clarity, let's put $I=(a,b)$ ) such that $g(y(I)) \neq 0$ . But what if, for example, $y(a)=0$ is well defined, $y$ in $a$ is continuous and with a continuous derivative such that $y'(a)=0$ , and $g(y(a))=0$ ? Can it even be possibile? In this case the method above is not working anymore in $a$ , but since our original equation was $y'=f(x)g(y)$ , would $y$ be a solution on $[a,b)$ and not only on $(a,b)$ ? These may sound like trivial questions, but I'm currently studing ODE and their resolution methods and I'm having an hard time grasping all the concepts. Hopefully everything makes sense and it's written in proper English. Thanks everyone!","['analysis', 'ordinary-differential-equations', 'real-analysis']"
3965658,Does the branch of the initial value determine the branch for the whole curve?,"Let $\Omega$ be a connected and simply-connected open subset of $\mathbf{C}$ which does not contain the origin. Let $\gamma :[0,1] \to \Omega$ be a simple smooth curve and let $\zeta := \gamma(0)$ . Let $\phi:[0,1] \to \mathbf{C}$ be a smooth map such that $e^{\phi(t)} = \gamma(t)$ for all $t \in [0,1]$ (i.e., at each point $\phi$ is a logarithm of $\gamma$ ). Since $\Omega$ is connected and simply-connected, there exists a unique single-valued branch $""f""$ of the logarithm defined on $\Omega$ such that $f(\zeta) = \phi(0)$ . Does it necessarily follow that $\phi$ is the restriction of $f$ to the curve $\gamma$ ?","['complex-analysis', 'riemann-surfaces', 'analytic-continuation']"
3965673,Find constants in system of equations,"In the below system of equations, find the values of constants $q_1, q_2, q_3 \in \mathbb R$ so that the system has no solution. $$\begin{cases}
2x&+y-z&+2s &= q_1\\
3x&+y-2z&+s &= q_2\\
x&+y+3z&-s &= q_3
\end{cases}$$ In order to eliminate some of the unknowns, we add the 1st with the 3rd and subtract the 2nd, so we get: $y+4z = q1+q3-q2$ By similar more ways (multiplying, adding and subtracting) we get some more such equations. I don't see how this system can have no solution; plus that we are one equation short (3 equations - 4 unknowns). Any assistance is much appreciated. EDIT: (Apologies but I am not familiar with linear algebra!!) What I believe I must do is the following: Reduce the unknowns by one (don't see how), so as to have a 3x3 system. Calculate the determinant, for which it must be $D \neq 0$ .
Then calculate $D_x, D_y, D_z$ assuming we have eliminated s.
Then, for the system NOT to have a solution, it must be $D=0$ and $D_x, D,y, D_z \neq 0$ . Any further help?","['calculus', 'linear-algebra']"
3965693,"Calculate $\int_0^{1/2}\, x\sin(\pi x)\Gamma(\frac{d-1}{2} + x)\Gamma(\frac{d-1}{2} - x) \mathrm dx$ for even $d\ge 4$","I would like to evaluate the integral of form: \begin{align}
I(d) \equiv \int_0^\frac{1}{2}\,d x\, x\,\sin(\pi\,x)\,\Gamma\left(\frac{d-1}{2} + x\right)\,\Gamma\left(\frac{d-1}{2} - x\right) \ ,
\end{align} for even $d\ge 4$ . ( $I(2)$ diverges due to the pole of the gamma function.) After some explicit calculation, I speculate it is given by a sum of the Riemann zeta functions: \begin{align}
I(d) = \sum_{n=1}^{\frac{d}{2}-1}\,c_{n,d}\,\zeta(2n+1) \ .
\end{align} Is it possible to determine the coefficients $c_{n,d}$ ?","['integration', 'definite-integrals', 'special-functions', 'gamma-function', 'riemann-zeta']"
3965700,Generating function of total sales on a given day,"Given the problem below: The number of customers that shop at Elmo’s World of Food on a given day is geometrically distributed with parameter $p_1$ . The number of items a customer buys at Elmo’s is geometrically distributed with parameter $p_2$ . The distribution of the cost of each item is exponentially distributed with parameter $\lambda$ . Assume that the amount customers buy and the amount of prices of items are independent. Let $X$ denote the total sales on a given day. Find the generating function of $X$ . In my attempt on solving this I let $N_c$ denote the number of customers that shop on a given fay, $N_i$ denote the number of items a customer buys, and $C$ cost of each item. Then we can write $X = \sum_{k=1}^{N_c} N_i\cdot C$ . To find the generating function I used the MGF: $E[e^{tX}]$ and to compute it I conditioned on $N_c$ first and then on $C$ . However, my work became very tedious and I'm stuck at simplifying the expression I got. Therefore, I was wondering if there is a different approach I could use in finding the generating function of $X$ ? This is just a practice problem as I study for my exam.","['statistics', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
3965702,"How many passwords can we create that contain at least one capital letter, a small letter and one digit? [duplicate]","This question already has answers here : How many ways can you create a password of 10 characters long that has at least one lowercase letter (a-z) and at least one number ($0-9$)? (2 answers) Closed last year . I have a problem in combinatorics.
How many passwords can we create such as: Each password has length $n$ when $n\ge3$ . Each password must contain at least one capital letter (there are 26 letters in English), and at least one small letter, and at least one digit (there are 10 possible digits) I tried solving this problem like this: There are $62^{n}$ possible passwords without any restrictions There are $2*26^{n}$ passwords with only capital letters or small letters. There are $10^{n}$ passwords with only digits. There are $52^{n}$ passwords with only capital letters and small letters. There are $36^{n}-10^{n}-26^{n}$ passwords with only capital letters and digits. There are $36^{n}-10^{n}-26^{n}$ passwords with only small letters and digits. So to get the ""right answer"" I subtracted all of them from $62^{n}$ and got $62^{n}-52^{n}-2*36^{n}+10^{n}$ Is this the right method/answer? or did I miss something silly?
I am new to combinatorics and I want to make sure, I am on the right track.
Thanks in advance guys!","['permutations', 'combinations', 'combinatorics']"
3965787,Finding the “root” of a monotone function (in the sense of composition),"Let $f:[0,\infty)\rightarrow [0,\infty) $ be a smooth and monotone function s.t $f(0)=0$ . Let $N\in\mathbb{N}$ . Can we find a function $g: [0,\infty) \rightarrow [0,\infty) $ s.t $g\circ\cdots\circ g$ ( $g$ composed with itself $N$ times) equals $f$ ? Can we say something about $g$ ‘s monotonicity? Its smoothness? I cannot come up with any basic answers. Thanks in advance to the helpers.","['calculus', 'functional-analysis', 'real-analysis']"
3965790,"How to prove $\int _0^\infty \operatorname{si}(x) \operatorname{Ci}(x) \, dx=\ln 2$","How to prove the integral $$\begin{align}&\int _0^\infty \operatorname{si}(x) \operatorname{Ci}(x) dx\\=&\int_0^\infty\left (\int_x^\infty\frac{\sin t}{t}dt \int_x^\infty\frac{\cos t}{t}dt\right)dx\\=&\ln 2\end{align}$$ I try to use integration by parts $$\begin{align}
&\int _0^\infty \operatorname{si}(x) \operatorname{Ci}(x) dx\\
=&\int_0^\infty \int_x^\infty\frac{\sin t}{t}dt \int_x^\infty\frac{\cos t}{t}dt\\
=&x\left.\int_x^\infty\frac{\sin t}{t}dt \int_x^\infty\frac{\cos t}{t}dt\right|_0^\infty+\int_0^\infty \cos x \int_x^\infty\frac{\sin t}{t}dx+\int_0^\infty \sin x  \int_x^\infty\frac{\cos t}{t}dx
\end{align}$$ But the last two integrals are divergent.","['integration', 'trigonometric-integrals', 'improper-integrals']"
3965805,"When we write $y'' + y' + y = x$, for a function $y$, do we really mean $y''(x) + y'(x) + y(x) = x$?","I have recently started exploring differential equations at a graduate level (partial and ordinary) and really started thinking about why we write them the way we do. Let (since this seems to be the common symbol of choice) $y: \mathbb{R} \to \mathbb{R}$ such that $x \mapsto y(x)$ . Just to be fussy, we will consider $y \in C^{\infty}(\mathbb{R})$ . Now, for example, consider the second order differential equation $$y'' + y' + y = x \tag{i}$$ My question is in regards to this notation: Why do we write the function and it's derivatives this way in the equation rather than writing: $$y''(x) + y'(x) + y(x) = x \tag{ii}$$ using the function values? When we write (i), do we really mean (ii)? This may be a bit pedantic, but I just want to be thorough.","['notation', 'dynamical-systems', 'ordinary-differential-equations', 'partial-differential-equations']"
3965855,Show that surface integral of absolute vorticity is constant over time,"In a rotating frame the (unforced, incompressible) Euler equation is $$
\frac{∂\vec{u}}{∂t}+\vec{u}\cdot\nabla\vec{u}=-\nabla\left(\frac{p}{\rho_0}\right)-2\vec{\Omega}\times\vec{u}+\nabla\left(\frac{1}{2}|\vec{\Omega}\times\vec{x}|^2\right)
$$ where $\vec{\Omega}$ is a constant vector (the angular velocity). Taking the curl of this equation (and using suitable vector identities), I can show that the vorticity $\vec{\omega}=\nabla\times\vec{u}$ satisfies $$
\frac{∂\vec{\omega}}{∂t}=\nabla\times\left(\vec{u}\times\vec{\omega}\right)+2\nabla\times\left(\vec{u}\times\vec{\Omega}\right)
$$ Now I want to show that , if $S_t$ is a material surface within the fluid, then the following is true $$
\frac{d}{dt}\int_{S_t}{\left(\vec{\omega}+2\vec{\Omega}\right)\cdot d\vec{S}}=0
$$ I can write $$
\vec{\omega}+2\vec{\Omega}=\nabla\times\vec{u}+\nabla\times\left(\vec{\Omega}\times\vec{x}\right)
$$ and use Stokes' theorem to rewrite this as a line integral $$
\frac{d}{dt}\int_{S_t}{\left(\vec{\omega}+2\vec{\Omega}\right)\cdot d\vec{S}}=\frac{d}{dt}\int_{∂S_t}\left(\vec{u}+\vec{\Omega}\times\vec{x}\right)\cdot d\vec{l}
$$ but here is where I am stuck . I am given the hint (for any material curve $\gamma_t$ ) $$
\frac{d}{dt}\int_{\gamma_t}\vec{a}\cdot d\vec{l}=\int_{\gamma_t}\frac{D\vec{a}}{Dt}\cdot d\vec{l}+\int_{\gamma_t}a_i\left(∂_ju_i\right)dl_j
$$ But when I substitute $\vec{a}=\vec{u}+\vec{\Omega}\times\vec{x}$ here nothing seems to become clearer.","['integration', 'multivariable-calculus', 'vector-analysis', 'mathematical-physics', 'fluid-dynamics']"
3965911,Prove that $\mathcal F_\tau$ is a $\sigma -$algebra,"Let $(\Omega ,\mathcal F,\{\mathcal F_t\},\mathbb P)$ a probability filtred space and $\tau$ a stopping time. I'm trying to prove that $$\mathcal F_\tau=\{A\in \mathcal F\mid A\cap \{\tau\leq t\}\in \mathcal F_t\}.$$ I could prove that $\Omega ,\varnothing \in \mathcal F_\tau$ , and that $\mathcal F_\tau$ is stable by countable union. But I have difficulties to prove that it's stable by complementary. Let $A\in \mathcal F_\tau$ . So, $A\cap\{\tau\leq t\}\in \mathcal F_t$ . How can I prove that $A^c\cap \{\tau\leq t\}$ ?","['measure-theory', 'probability-theory']"
3965925,Bound on $\mathbb P[X=0]$ using Paley-Zygmund and Chebyshev,"In an exercise I am asked to show that if a non negative random variable $X$ satisfies $\frac{\mathbb E [X^2]}{\mathbb E[X]^2}\ge 2$ and $\text{Var}(X)<\infty$ , then Paley-Zygmund gives a tighter upper bound on $\mathbb P[X=0]$ than Chebyshev. Paley-Zygmund: $\mathbb P[X\ge\theta \mathbb EX]\ge (1-\theta)^2\frac{\mathbb E[x]^2}{\mathbb E[X^2]}$ holds for any $\theta\in[0,1]$ . Chebyshev: $\mathbb P[|X-\mathbb E X|\ge t]\le\frac{\text{Var}(X)}{t^2}$ The way to use Chebyshev is to set $t= \mathbb E X$ and get $\mathbb P[X=0]\le \mathbb P[|X-\mathbb E X|\ge \mathbb E X]\le \frac{\text{Var}(X)}{\mathbb E[X]^2}=\frac{\mathbb E[X^2]}{\mathbb E[X]^2}-1$ For P-Z, since it is true if we replace $\ge$ by $>$ , and by setting $\theta=0$ , we can write $\mathbb P[X=0]=1-\mathbb P[X>0]\le 1-\frac{\mathbb E[X]^2}{\mathbb E[X^2]}$ But in both cases we cannot use the $\frac{\mathbb E [X^2]}{\mathbb E[X]^2}\ge 2$ assumption. We could use it if the inequality was flipped (and it would give 0.5 and 1 respectively for P-Z and Chebyshev)... unless I am doing something wrong. Also, can we use $\theta>1$ if $\frac{(\mathbb E X)}{\sqrt{\mathbb E(X^2)}}<\frac{1}{\theta -1}$ ?","['inequality', 'solution-verification', 'probability-theory']"
3965947,Count the numbers in which the product of the digits is 48 (1 not allowed),"Count the numbers in which the product of the digits is 48 (1 not allowed as digit). Proposed solution: I addressed the problem by brute force, first noticing that $$48=2^4\times 3.$$ Then I found the possible groups of digits to be permuted for each number: (1) 2,2,2,2,3; (2) 2,2,2,6; (3) 4,2,2,3; (4) 4,2,6; (5) 4,4,3; (6) 8,2,3; (7) 8,6. By adding the permutations with repetition for each case, I've found 38 possible numbers. Which agrees with the answer provided in the source. My problem: I found this solution is not elegant and I'm wondering if there is a better way to address problems like these. By brute force it is easy to undercount terms (as I did as first solving the problem). Any help?","['contest-math', 'combinatorics']"
3965958,Stereographic projection and complex polynomial. (Do Carmo 2-3-16),"$\newcommand{\set}[1]{\left\lbrace #1 \right\rbrace} \newcommand{\suchthat}{\;\big\vert\;}$ Let $ \mathbb{R}^{2} = \set{(x,y,z)\in\mathbb{R}^{3} \suchthat z=-1} $ be identified with the complex plane $ \mathbb{C} $ by setting $(x,y,-1)=x+iy=\zeta\in\mathbb{C}$ . Let $ P: \mathbb{C}\to\mathbb{C} $ be the complex polynomial \begin{equation*}
P(\zeta) = a_{0}\zeta^{n} + a_{1}\zeta^{n-1} + \cdots + a_{n}, \quad a_{0}\neq 0,\, a_{i}\in\mathbb{C},\, i=0,\dots,n.
\end{equation*} Denote by $ \pi_{N} $ the stereographic projection of $ S^{2}=\set{(x,y,z)\in\mathbb{R}^{3} \suchthat x^{2}+y^{2}+z^{2}=1 } $ from the North pole $ N=(0,0,1) $ onto $ \mathbb{R}^{2} $ . Prove that the map $ F: S^{2}\to S^{2} $ given by \begin{align*}
F(p) &= \pi_{N}^{-1}\circ P \circ \pi_{N}(p), \quad \text{if } p\in S^{2}-\set{N}, \\
F(N) &= N
\end{align*} is differentiable. My attempt By using Do Carmo's hint, $ F $ is differentiable in $ S^{2}-\set{N} $ as a composition of differentiable maps. To prove that $ F $ is differentiable at $ N $ , consider the stereographic projection $ \pi_{S} $ from the South pole $ S=(0,0,-1) $ and set $ Q=\pi_{S} \circ F \circ \pi_{S}^{-1}: U \subset \mathbb{C}\to\mathbb{C} $ (of course, we are identifying the plane $ z=1 $ with $ \mathbb{C} $ ) Now, I will show that $ \pi_{N}\circ\pi_{S}^{-1}: \mathbb{C}-\set{0}\to\mathbb{C} $ is given by $ \pi_{N}\circ\pi_{S}^{-1}(\zeta) = 1/\overline{\zeta} $ . Let $ \mathbf{u}=(u,v,1) $ be a point in plane $ z=1 $ (i.e., $ \mathbb{C} $ ); $ \ell_{S}(t) $ be the line through $ S $ with director vector $ \mathbf{u}-S=(u,v,2) $ , then the parametrization of $ \ell_{S} $ is \begin{equation*}
\ell_{S}(t) = (0,0,-1) + t(u,v,2) = (tu,tv,2t-1)
\end{equation*} and the intersection point $ (x,y,z) $ between $ \ell_{S} $ and $ S^{2} $ (distinct of $ S $ , i.e., $ t \neq 0 $ ) is given by the value of $t$ that satisfies \begin{equation*}
(tu)^{2} + (tv)^{2} + (2t-1)^{2}= 1 \quad \implies \quad t^{2}u^{2} + t^{2}v^{2} + 4t^{2} - 4t + 1 = 1 \quad \implies \quad tu^{2} + tv^{2} + 4t = 4
\end{equation*} then \begin{equation*}
t=\dfrac{4}{u^{2} + v^{2} + 4}
\end{equation*} yields \begin{equation*}
(x,y,z) = \left( \dfrac{4u}{u^{2} + v^{2} + 4}, \dfrac{4v}{u^{2} + v^{2} + 4}, \dfrac{4-u^{2}-v^{2}}{u^{2} + v^{2} + 4} \right)
\end{equation*} therefore \begin{equation}
\pi_{S}^{-1}(u,v) = \left( \dfrac{4u}{u^{2} + v^{2} + 4}, \dfrac{4v}{u^{2} + v^{2} + 4}, \dfrac{4-u^{2}-v^{2}}{u^{2} + v^{2} + 4} \right)
\tag{1}
\end{equation} Now, to calculate $ \pi_{N}\circ\pi_{S}^{-1} $ , take (1) as a point in $ S^{2} $ and let $ \ell_{N}(t) $ be the line trough $ N $ with director vector $ \pi_{S}^{-1}-N $ , then \begin{align*}
\ell_{N}(t) &= (0,0,1) + t\left[ \pi_{S}^{-1} - (0,0,1) \right] \\[2ex]
 &= (0,0,1) + t\left[ \left( \dfrac{4u}{u^{2} + v^{2} + 4}, \dfrac{4v}{u^{2} + v^{2} + 4}, \dfrac{4-u^{2}-v^{2}}{u^{2} + v^{2} + 4} \right) - (0,0,1) \right] \\[2ex]
 &= (0,0,1) + t\left( \dfrac{4u}{u^{2} + v^{2} + 4}, \dfrac{4v}{u^{2} + v^{2} + 4}, \dfrac{-2(u^{2}+v^{2})}{u^{2} + v^{2} + 4} \right)
\end{align*} And for this point to belong to the plane $ z=-1 $ (i.e., $ \mathbb{C} $ ) clearly we need that \begin{equation*}
\dfrac{-2(u^{2}+v^{2})}{u^{2} + v^{2} + 4}t+1=-1 \quad \implies \quad t=\dfrac{u^{2} + v^{2} + 4}{u^{2} + v^{2}}
\end{equation*} thus \begin{equation*}
(x,y,-1) = \left( \dfrac{4u}{u^{2}+v^{2}}, \dfrac{4v}{u^{2}+v^{2}}, -1 \right)
\end{equation*} therefore \begin{equation}
\pi_{N}\circ\pi_{S}^{-1}(u,v) = \left( \dfrac{4u}{u^{2}+v^{2}}, \dfrac{4v}{u^{2}+v^{2}}, -1 \right)
\tag{2}
\end{equation} where $ (u,v-1) = u + iv = \zeta $ . \
But the inverse of conjugate of $ \zeta $ should be \begin{equation}
1/\overline{\zeta} = \left( \dfrac{u}{u^{2}+v^{2}}, \dfrac{v}{u^{2}+v^{2}}, -1 \right)
\tag{3}
\end{equation} My Question . Where is my mistake? Do Carmo's book says the equation should be like (3). I'm confused. Next step is conclude that \begin{equation*}
 Q(\zeta) = \dfrac{\zeta^{n}}{\overline{a_{0}} + \overline{a_{1}}\zeta + \cdots + \overline{a_{n}}\zeta^{n}}   
\end{equation*} We have the next relations (if $ F \neq N $ ) \begin{align*}
Q(\zeta) &= \left( \pi_{S}\circ F \circ\pi_{S}^{-1} \right) (\zeta) = \left( \pi_{S}\circ \pi_{N}^{-1} \circ P \circ\pi_{N} \circ\pi_{S}^{-1} \right) (\zeta) \\
&= \left( \pi_{S}\circ \pi_{N}^{-1} \circ P \right) \circ \left( \pi_{N}\circ\pi_{S}^{-1}(\zeta) \right) \\
&= \left( \pi_{S}\circ \pi_{N}^{-1} \circ P \right) (1/\overline{\zeta}) \\
&= \left( \pi_{S}\circ \pi_{N}^{-1} \right) \circ P(1/\overline{\zeta}) \\
&= \left( \pi_{S}\circ \pi_{N}^{-1} \right) \left( a_{0}\left(1/\overline{\zeta}^{n}\right) + a_{1}\left(1/\overline{\zeta}^{n-1}\right) + \cdots + a_{n} \right) \\
&= \left( \pi_{S}\circ \pi_{N}^{-1} \right) \left(  \left( 1/\overline{\zeta}^{n} \right)\left( a_{0}+a_{1}\overline{\zeta} + \cdots + a_{n}\overline{\zeta}^{n} \right) \right)
\end{align*} Since $ \pi_{S}\circ \pi_{N}^{-1} = \left( \pi_{N}\circ\pi_{S}^{-1} \right)^{-1} $ should be $ \pi_{S}\circ \pi_{N}^{-1} = 1/\overline{\zeta} $ as well. Thus \begin{align*}
Q(\zeta) &= \left( \pi_{S}\circ \pi_{N}^{-1} \right) \left(  \left( 1/\overline{\zeta}^{n} \right)\left( a_{0}+a_{1}\overline{\zeta} + \cdots + a_{n}\overline{\zeta}^{n} \right) \right) \\
 &= \dfrac{\zeta^{n}}{\overline{a_{0}} + \overline{a_{1}}\zeta + \cdots + \overline{a_{n}}\zeta^{n}}   
\end{align*} Hence, $ Q $ is differentiable at $ \zeta=0 $ . Thus, $ F = \pi_{S}^{-1}\circ Q \circ\pi_{S} $ is differentiable at $ N $ .","['stereographic-projections', 'differential-geometry']"
3966143,"Is there a $\triangle ABC$, with $D$ on $BC$, such that $AB$, $BC$, $CA$, $AD$, $BD$, $CD$ are all integers, but only one is even?","This is just a question I came up with while working through another problem. If we have a triangle $ABC$ and we draw a cevian from $A$ to $D$ on side $BC$ , is it possible to assign integer values to lengths $AB$ , $BC$ , $AC$ , $AD$ , $BD$ or $CD$ such that only one of the listed lengths has an even valued length? If it is possible, what are these lengths? I have done some parts of this problem on my own. I figured out that this even length could only be $BC$ , $CD$ , or $BD$ , with $BD$ and $CD$ being the same case. With $BC$ being the only even length I computed for $BC$ using Stewart's Theorem and got $$BC = \frac{(AC^2)(BD)+(AB^2)(CD)}{(BD)(CD)+(AD)^2}$$ where every length but $BC$ is odd. The equation seems to work but I don't know how to obtain its integer solutions. Similarly with $BD$ / $CD$ case, I applied Stewart's theorem and got $$BD=\frac{(AB^2)(CD)-(BC)(AD^2)}{(CD)(BC)-(AC)^2}$$ Once again this seems to work, but I don't know how to obtain its integer solutions. Pretty much all I need help is verifying whether or not my work is right, and if it  is how to find the integer solutions of my provided equations. If there is a better way or my work has flaws please let me know.","['number-theory', 'geometry', 'diophantine-equations', 'triangles', 'trigonometry']"
3966166,How to transform a Cauchy-Euler equation into a constant coefficients equation?,"I have doubts with a problem that goes like this: Let's consider the second order linear differential equation $ y''(x)+R(x)·y'+S(x)·y=Q(x) $ a) How must the change of variable $z = z(x)$ be so that the equation
is transformed into one of constant coefficients? b) Apply the above result to solve the equation: $ y''(x)+(1+4e^{x})·y'+3e^{2x}·y=e^{2(x+e^{x})} $ I understand what the problem ask I don't know at all how to do it. I even wonder if the statement is right because the condition I get it's a bit abstract. It's a Cauchy-Euler differential equation, so that: $$ \frac {dy}{dx} = \frac {dy}{dz}· \frac {dz}{dx} = \dot yz'$$ $$ \frac {d^2 y}{dx^2} = ... = \ddot{y}z'^{2}+ \dot yz'' $$ And the differential equation turns to: $$ \ddot{y}z'^{2}+ \dot yz'' + R(x)·\dot y z' + S(x)y = Q(x) $$ $$ \ddot{y}z'^{2}+ \dot y (z'' + R(x)z') + S(x)y = Q(x) $$ If the equation should transformed into a constant coefficients equation, I think it means: $$ z'' + R(x)z' = K $$ But I don'tknow what to do from here.. I supposse (in case this result is fine) I can't make K=0 (because it's just a special case) but this is the only value that let me develop a little but more this condition. Please, anybody know whether this result is right or it has some errors?
How could I solve the problem please? (Not solve it completely, of course. Just a tip to let me continue doing it). Thanks a lot.",['ordinary-differential-equations']
3966188,Show $\frac52\cot\frac{4\pi}7+\frac34\cot\frac\pi7\sec\frac{2\pi}7+\cot\frac{2\pi}7\cos\frac\pi7=\sqrt7$,"While solving a definite integral in different ways, I found that the trigonometric expression involving the heptagonal angles below simply reduces to $\sqrt7$ $$\frac52\cot\frac{4\pi}7+\frac34\cot\frac\pi7\sec\frac{2\pi}7+\cot\frac{2\pi}7\cos\frac\pi7$$ I verified it numerically, but could not resist trying to do the same by hand. Unlike the familiar ones, such as $4\sin\dfrac{2\pi}7-\tan \dfrac\pi7 =\sqrt7$ , I have not come across any resembling patterns anywhere and would like to find a clean path to derive it analytically.","['algebra-precalculus', 'trigonometry']"
3966194,"How are integer sets of ""all odd positive"" or ""all even negative"" or ""all negative"" described?","The infinite set of all positive integers, i.e., evens and odds combined (sometimes including, sometimes without, $0$ ), is called the ""natural numbers"", denoted by $\mathbb{N}$ . What about with a specific half of them omitted, or all of them converted to their additive inverse? And does this work the same way for finitely-defined sets? Slightly off-topic but how might ""all complex numbers"" or ""all rational numbers"" (two cases also with special abbreviations) or ""all natural numbers"" or cetera be described in terms of a subset of ""all numbers"" (i.e. all numerical values conceivable/possible) without using their respective abbreviations?","['elementary-set-theory', 'notation']"
3966201,Conjecture regarding the sum of prime factors of $x!$ and $(x-1)!$?,I think using dodgy means I can show the following: Let $\lambda(x)$ be the sum of  primes in $x$ . For example: $$ \lambda(2) = 2$$ $$ \lambda(4) = 2 + 2 = 4$$ $$ \lambda(6) = 3 + 2 = 5$$ Then for large $n$ : $$ \frac{\lambda (n!)}{\lambda ((n-1)!)} \sim \frac{\ln (n!)}{\ln (n-1)!}$$ Or more precisely substracting $1$ : $$ \frac{\lambda (n)}{\lambda ((n-1)!)} \sim \frac{\ln (n)}{\ln (n-1)!}$$ Or even better: $$ \frac{\lambda (n)}{\lambda ((n-1)!)} - \frac{\ln (n)}{\ln (n-1)!} = o(1)$$ Can someone prove or disprove this conjecture? For example: $$ \frac{\lambda(8!)}{\lambda(7!)} = \frac{31}{26} \approx 1.23$$ And $$ \frac{\ln(8!)}{\ln(7!)} \approx 1.244 $$ Substracting $1$ : $$ .23  \approx .24$$,"['number-theory', 'conjectures', 'asymptotics', 'prime-numbers']"
3966206,How often should I compound my Algorand (or other proof of stake crypto assets)?,"Note to editors/mods: Please help me tag this appropriately! I'm not really a math wiz... is it multivariable calculus? See this note from the Algorand FAQ about how collecting rewards on staked assets works with compounding. The participation rewards are calculated automatically, however the compounding effect is not automatic. This is because the rewards are calculated from the last recorded balance on the blockchain, the easiest way to force rewards compounding is to send a zero Algo payment transaction to the target address on a frequent, recurring basis. This transaction will trigger the commit of all accrued rewards and record them to the on-chain balance of the account. As of this question, the network fee of a 0.00 ALGO transaction to myself is .001 ALGO, and the APR for rewards is ~5.827% (converted from ~6% APY). Let's say I have some amount of ALGO principal, P , and I decide to claim my rewards every d days. It stands to reason then that every time I claim my rewards, the resulting interest, I is: I = P * ( d * .05827 /365)-.001 Every d days, P goes to P + I . How do we find, for a given value P , which value d is the optimal compound interval such that we maximize the rate that P increases? My suspicion is that as P increases sufficiently high, it will make sense to compound much more frequently. It's been 11 years since I took Calculus 1 and 2, and this seems like an interesting problem. It seems like there's some recursive nature to this that makes it challenging.","['multivariable-calculus', 'exponential-function']"
3966216,"there is a positive constant $c$ such that for any $n$ real numbers $a_i$ with $\sum a_i^2=1$, $\mathbb P[|\sum \epsilon_i a_i|\le1]\ge c$","Question (4.8.2 from the Probabilistic Method 4th edition by Alon and Spencer): Show that there is a positive constant $c$ such that for any $n$ real numbers $a_1,\dots,a_n$ satisfying $\sum\limits_{i=1}^na_i^2=1$ , if $(\epsilon_1,\dots,\epsilon_n)$ is a random vector of iid random variables uniformly distributed on $\{\pm1\}$ then $\mathbb P[|\sum \epsilon_i a_i|\le1]\ge c$ . I let $X=\sum\limits_{i=1}^n \epsilon_ia_i$ , and I have computed $\Bbb E[X]=0$ and Var $(X)=1$ . By Chebyshev's inequality we get $\Bbb P[|X|\le 1]=1-\Bbb P[|X|>1]>1-\frac{\text{Var}(X)}{1}=0$ which is useless... I guess I need a hint. One other thing that is true from Cauchy Schwarz inequality is $$|X|^2=|\langle \epsilon,a\rangle|^2\le\sum\epsilon_i^2\sum a_i^2=n\cdot1=n\implies |X|\le \sqrt{n}$$ and it is true not matter how $\epsilon $ is chosen. So the density of $|X|$ is supported on $[0,\sqrt{n}]$ .","['probabilistic-method', 'probability-theory']"
3966246,Non continuous function with directional derivatives zero in all directions,"I was asked the following question:
A function f(x,y) has directional derivatives in all directions, and they are 0 in all directions. Does f have to be continous? I thought I proved it by: \begin{align}\lim_{(x,y)\to (0,0)}f(x,y)-f(0,0)&=\lim_{r\to0}f(r\cos\theta,r\sin\theta)-f(0,0)\\&=\lim_{r\to0}r\cdot (f(r\cos\theta,r\sin\theta)-f(0,0))/r\\&=\lim_{r\to0}r\cdot \lim_{r\to0}(f(r\cos\theta,r\sin\theta)-f(0,0))/r\end{align} The right limit is the directional derivative in direction $\theta$ which is said to be zero, and the left limit is zero and we get: $$\lim_{(x,y)\to (0,0)}f(x,y)=f(0,0)$$ Is there a mistake in this proof? Because the solution said that f does not have to be continuous.
Thank you!","['continuity', 'multivariable-calculus', 'derivatives']"
3966262,If $f$ is continuous then $f$ is uniformly continuous iff $|f|$ is uniformly continuous,"If $f:\Bbb R^n \to \Bbb R$ is continuous then $f$ is uniformly continuous iff $|f|$ is uniformly continuous. A map $f$ from a metric space $M=(M,d)$ to a metric space $N=(N,\rho)$ is said to be uniformly continuous if for every $\epsilon>0$ , there exists a $\delta>0$ such that $\rho(f(x),f(y))<\epsilon$ whenever $x,y \in M$ satisfy $d(x,y)<\delta$ . Clearly if $f:\Bbb R^n \to \Bbb R$ is uniformly continuous then $|f|$ is uniformly continuous as $|f|(x)-|f|(y)|\leq |f(x)-f(y)|$ but I am having a real trouble showing the converse part. In the region where $f$ is always positive or negative, we will not have any problem but how to deal with the points where $f$ is changing sign. If the zeros of $f$ are finite then also we can take a minimum of all $\delta$ s and conclude the result. What will happen if zeros of $f$ are infinite?","['uniform-continuity', 'metric-spaces', 'real-analysis', 'continuity', 'multivariable-calculus']"
3966305,partitioning a set into denumerable subsets,"Let $A$ and $B$ be sets. If there exists an injection $f : A\to B,$ then write $|A|\leq |B|,$ where $|A|$ is the size of the set $A$ . Let $A$ be a nonempty set. Show that $|\mathbb{N}|\leq |A|$ iff $A$ can be partitioned into denumerable sets. That is, there is a set of sets $\{K_i\}_{i\in I} \subseteq \mathcal{P}(A)$ so that Each $K_i$ is denumerable $K_i\cap K_j=\emptyset$ for $i\neq j$ $A = \cup_{i\in I} K_i$ Suppose $A$ can be partitioned into denumerable sets. Then $A = \cup_{i\in I} K_i$ for some index set $I$ , where the $K_i$ 's are disjoint and denumerable. In particular, one can find a bijection $f : \mathbb{N}\to K_{\alpha}, \alpha \in I.$ This is clearly an injection from $\mathbb{N}$ to $A$ (as $K_\alpha \subseteq A$ ), so $|\mathbb{N}|\leq |A|.$ Suppose there is an injection $f : \mathbb{N}\to A.$ Then $A$ is clearly infinite, since its size is at least the size of an infinite set. Also, $A$ has a denumerable subset. I'm not really sure how to show that the given family exists, though I think Zorn's lemma should be useful. However, I'm not really sure which family of sets to consider. For instance, would it be useful to consider the set of all denumerable subsets of $A$ ? And if so, how would I define the partial order?",['elementary-set-theory']
3966361,What is the asymptotic behavior of large polyominoes? How many of them tile the plane?,"The free polyominoes on $n$ cells can be classified into three categories: those with holes, those that tile the plane, and those without holes that do not tile the plane. (No polyomino with holes tiles the plane, so the fourth logical category has no members.) I am curious about the asymptotic behavior of this distribution for large $n$ . For $n\le 6$ , every polyomino on $n$ cells tiles the plane. However, as $n$ grows larger, this ceases to be the case. Using the values given at this page , I have plotted the ratio of polyominoes in each category for $n\le 25$ : Emprically, the ratio of tiling polyominoes appears to decay exponentially, roughly halving at every increase in $n$ ; the ratio $1:x$ where $x=2^{n-11}$ matches the observed data quite well. I also suspect that the polyominoes with holes dominate for very large $n$ , in that their ratio approaches 1; one would expect a ""generic"" large polyomino to have many holes, though I'm not sure how to formalize this intuition and obtain a formal proof. I also don't know what the rate of approach is. Likewise, it seems obvious that a ""generic"" large polyomino has no hope of tiling the plane, even conditioned on being simply connected; again, I am unsure how to go about formalizing this intuition into something resembling a proof or a bound on growth. The relevant OEIS sequences ( A000105 , A054359 , A054360 , A001419 ) do not offer much insight in their comments, either proven results or conjectures. It is mentioned that the total number of polyominoes of order $n$ is known to grow exponentially, with a rate of growth between $3.98$ and $4.64$ , but no comments about the growth rates of these subsets are given. One can at least show that the fraction of size- $n$ polyominoes that tile the plane does not shrink any faster than exponentially; for $n=2k>2$ , there are at least $2^{k-2}$ tiling polyominoes which tile by translation, given by placing shifted dominoes in a row: This bounds the asymptotic decay of the ratio of tiling polyominoes by $\left(\frac{\sqrt{2}}{4.64}\right)^n$ . Some questions: Can we place nontrivial upper bounds on the number of tiling polyominoes? In particular, can it be proven that they occupy an exponentially-decreasing fraction of the polyominoes, and what bounds can be put on the associated constant? Can we do the same for the fraction of simply connected polyominoes which tile the plane? Can it be rigorously shown that ""most"" polyominoes have holes in the limit? What can be said about the rate at which the fraction of hole-y polyominoes approaches 1? (For the latter question, I would be interested in heuristic arguments to expect a certain growth rate as well, even if the associated bound cannot be proven.) Is there previous work on this or related questions that can be found?","['geometry', 'asymptotics', 'reference-request', 'polyomino', 'tiling']"
3966378,Do we count only distinct roots in Descartes' rule of signs?,"Descartes' rule of signs states that numbet of positive roots of a polynomial $P(x) = a_nx^n + ... + a_1x + a_0$ is not greater than the number of sign changes of coefficients. By Gauss we know that number of complex roots of $P(x)$ is $n$ and therefore, number of real (in this case positive) roots is certainly $\leq n$ and we can count same roots more than once if $P(x)$ indeed has multiple roots. Okay, suppose that number of signs changes of $P(x)$ is $k$ and also suppose that we have set $S$ of all positive roots of $P(x), S = \{x_1, x_2, ..., x_m\}, m \leq n$ and some $x_i, x_j$ can be equal. Now, take that same set and eliminate those equal elements (eliminate all $x_i = x_j$ when $i \neq j$ ), that new set call $S'$ and let's say $|S'| = r.$ Does Descartes' rule of signs tells us that $m$ cannot be greater than $k$ or that $r$ cannot be greater than $k$ ? Does this rule counts multiple roots more than once or it counts just disctint roots?
I am asking this because the geometric proof given here https://math.hmc.edu/funfacts/descartes-rule-of-signs/ has sense to me if we count only distinct roots (because that's number of intersections of polynomial with the $x$ -axis) but no otherwise. Thanks I saw in the comments that the rule counts multiple roots (say $x_0$ ) more than once. Since the proof given in the link is geometric, does polynomial bounces near the crossing point $(x_0, 0)$ ? How exatcly the number of $x_0$ 's we counted reflect on the behaviour of $P(x)$ near that point? ($) My hypothesis is that if that number is even polynomial has parabolic shape near that point and if it is odd polynomial has shape simillar to the curve $x^3$ (and since the parity of sign changes and positive roots (counted with multiplicity) are equal, then this does not change number of ups and downs of our curve - now proof has sense but please answer #)","['algebra-precalculus', 'polynomials']"
3966394,"If I'm given the distances of some point $p$ from $n$ coplanar points, can I tell whether $p$ is also coplanar?","I've been thinking for some time about this problem, however I'm not yet sure how to approach it. There are a few ways to think about this, however I'll give it exactly as in the original applied problem I'm trying to solve. Consider a network of $n$ coplanar nodes in $2$ -dimensions, with known coordinates, and a signal with velocity $v$ , with arbitrary coordinates $(x,y)$ and emitted at an unknown time $t$ . I've written an algorithm which is able to reconstruct the coordinates of the signal source $(x,y)$ given only the signal velocity $v$ and the time of arrival at each node, assuming the source is also coplanar with the nodes. The Problem: Given the time at which the signal reached each node (or, if you like, all $\frac{n(n-1)}{2}$ time of arrival differences), how can we determine whether it may correspond to a source coplanar with the nodes (a source in $2$ -dimensions). Or, if you like, how can we flag input as ""invalid"" before passing it to the algorithm? My Thinking: I know that the minimum possible time of arrival difference can be found by computing the centroid of the $n$ nodes, and taking a signal source at that location, then computing the time of arrival for each node from this point. I also know that the maximum possible time of arrival difference will be for the two furthest nodes (the ""diameter"" of the set, if you like, or the diameter of the minimum bounding sphere). Will restricting input to lying between this minimum and maximum be sufficient enough to exclude input that corresponds to sources outside of the plane formed by the nodes? There will be $\infty$ possible source locations in $2$ -dimensions, but (I think) an even larger $\infty$ of locations in $3$ -dimensions. I could also phrase this as ""given the distance of some point $p$ from $n$ coplanar points, how can I determine whether $p$ is also coplanar with the $n$ points by examining the distances? I'm interested not only in solving this as an applied problem, but also understanding the deeper meaning that must exist behind this. How can I look at $n$ times of arrival, and tell whether it definitely corresponds to a source in $3$ -dimensions? Or, at least, something close to this. EDIT: this looks like it may be useful, though I don't quite follow yet (and it seems a bit overkill).","['optimization', 'geometry', 'euclidean-geometry']"
3966395,"Name of ""divided difference"" transform $\frac{f(x)-f(x_0)}{x-x_0}$ and special case $\frac{e^x - 1}{x}$?","Given an analytic function / formal power series $$\displaystyle f(x)=\sum _{n=0}^{\infty }\frac{f^{(n)}(x_0)}{n!}\left(x-x_{0}\right)^{n}=f(x_0)+f'(x_0)(x-x_{0})+ \tfrac{1}{2}f''(x_0)(x-x_{0})^{2}+\ldots$$ we can construct another analytical function via the ""divided difference"" transformation $$ R(f)(x) := \frac{f(x)-f(x_0)}{x-x_0} := f'(x_0) + \tfrac{1}{2}f''(x_0)(x-x_0) +  \tfrac{1}{6}f'''(x_0)(x-x_0)^2 + \ldots $$ Note that this is similar but not equal to the derivative operator $$ D(f)(x) = f'(x_0) + f''(x_0)(x-x_0) +  \tfrac{1}{2}f'''(x_0)(x-x_0)^2 + \ldots $$ Does this transformation have a name in the literature? What are its properties? In the discrete case it is known as divided differences, however here I am explicitly interested in this transform as a map $$T\colon A(D)\to A(D),\, f\mapsto R(f)$$ between the space of analytic functions $A(D)$ on an open interval $D\subset \mathbb R$ . Does the function we get by applying this transformation to the exponential function, i.e. $\frac{e^x-1}{x}$ have a name in the literature? The latter appears in the integrand of the exponential integral; is apparently related to the derivative of the exponential map from Lie groups and occurs in the solution of inhomogeneous linear ODEs, e.g. $\dot x = a x+b, x(t_0)=x_0$ has the solution $  x^*(t) = e^{a(t-t_0)}x_0 + \frac{e^{a(t-t_0)} -1}{a} b$ ; and in the multivariate case $\dot x(t) = A \cdot x(t) + b ,  x(t_0) = x_0$ we get the analogous $  x^*(t) = e^{A(t-t_0)}x_0 + \frac{e^{A(t-t_0)}-I}{A}b$ . Note that $\frac{e^{A(t-t_0)}-I}{A}$ exists even when $A$ is singular, hence it would be useful to drop this notation and give the function a name instead.","['special-functions', 'ordinary-differential-equations', 'exponential-function', 'terminology', 'analytic-functions']"
3966401,Minimal polynomials and Galois extensions,"I'm back again with a question, but this time I am only curious about one thing. Here's how it goes: Let $K$ be a Galois extension of a field $F$ . By the theorem of the primitive element, we know that $K = F(\alpha_1)$ for some $\alpha_1 \in K.$ Suppose that $f(X)$ is the minimal polynomial of $\alpha_1$ over $F$ . Now $K$ is the splitting field for $f(X)$ as $K$ is separable and normal. We also know that $F(\alpha_1, \alpha_2, \ldots, \alpha_n)$ for the distinct roots $\alpha_i$ of $f(X)$ is a splitting field for $f(X)$ . This means that $F(\alpha_1, \alpha_2, \ldots, \alpha_n) = F(\alpha_1)$ . Since $f(X)$ has $n = deg f(X)$ distinct roots it must be the case that all roots of $f(X)$ are linear combinations of $\alpha_1$ . This also means that $F(\alpha_k) = F(\alpha_d)$ for some $k, d \leq n$ . However, I am not sure if my arguments are correct. I am also quite afraid of drawing my own conclusions as I am currently self-studying Galois theory with basically no prior knowledge of algebra. I hope that someone can correct me if I am wrong!","['galois-theory', 'minimal-polynomials', 'abstract-algebra']"
3966409,Deriving exponential generating function for central trinomial coefficients,"A recent question ( link ) asked for a derivation of the (ordinary) generating function for the central trinomial coefficients $\{T_n\}$ . But the OEIS page ( A002426 ) also lists an exponential generating function $$\sum_{n=0}^\infty T_n \frac{x^n}{n!}=e^x I_0(2x)$$ where $I_0(x)$ is the zeroth Bessel function. How is this derived? I'll take a stab myself at showing this using the tools of analytic combinatorics, but I wanted to open this up to more knowledgeable folks as well.","['special-functions', 'analytic-combinatorics', 'combinatorics', 'generating-functions', 'sequences-and-series']"
3966410,Asymptotic formula for roots of a polynomial-like function,"Let $\sigma \in (0, 1)$ and $n \geq1$ be given. How can one rigorously show that the positive root $x_0 \in (0, n)$ of \begin{equation}
f(x) = (n^2-x^2-\sigma n^2 x)-(n^2-x^2+\sigma n^2x)e^{-4x}
\end{equation} is of the form \begin{equation}
x_0 = x^*-\varepsilon_n
\end{equation} for some $\varepsilon_n$ such that $\varepsilon_n \rightarrow 0$ as $n\to+\infty$ , and
where $$x^* := \frac12(n\sqrt{n^2\sigma^2+4}-n^2\sigma)$$ is the positive root of $f_1(x) = (n^2-x^2-\sigma n^2x)$ ? This question is related and complementary to Roots of polynomial/exponential function .","['calculus', 'functions', 'polynomials', 'real-analysis']"
3966420,Reconstructions of Groups From Category of $G-\mathbf{Sets}$; Construction of a Group Homomorphism [duplicate],"This question already has answers here : Why is $\text{Aut}(F)$ of the forgetful functor $F$ on $G$-sets isomorphic to $G$? (4 answers) Closed 3 years ago . I try to come up with a proof of the following statement, but I find it a little difficult. I hope that I can get some help from someone on this site. I think this is what they give a proof of, on Ncatlab - Tannakian Duality (at the section $G-\mathbf{Sets}$ ). But I can't really follow that proof: https://ncatlab.org/nlab/show/Tannaka+duality#ForPermutationRepresentations . Statement. Let $F:G-\mathbf{Sets}\to\mathbf{Sets}$ be the forgetful functor, where $G-\mathbf{Sets}$ is the category of sets equipped with a group action by the group $G$ . I'm trying to understand the proof of the following fact $$\operatorname{Aut}(F)\cong G.$$ What I have done I have managed to construct a map $$\varphi:G\to\operatorname{Aut}(F)$$ This was done by the following rule $\varphi(g)=\eta^g$ , where $\eta_S^g:S\to S$ is defined by $\eta_S^g(s)=s\cdot g$ . It is straightforward to check that this gives a natural transformation from $F$ to $F$ and that it also is a group homomorphism. However , the other way is more problematic for me. I want to find a map $$\psi:\operatorname{Aut}(F)\to G.$$ That is, given a natural transformation $\eta$ , I want to assign it to a group element $g\in G$ . The natural transformation $\eta$ is defined by the following commutative diagram $\require{AMScd}$ $$
\newcommand{\ra}[1]{\!\!\!\!\!\!\!\!\!\!\!\!\xrightarrow{\quad#1\quad}\!\!\!\!\!\!\!\!}
\newcommand{\da}[1]{\left\downarrow{\scriptstyle#1}\vphantom{\displaystyle\int_0^1}\right.}
%
\begin{array}{llllllllllll}
F(X) & \ra{\eta_X} & F(X) \\
\da{F(f)} & & \da{F(f)} \\
F(Y) & \ra{\eta_Y} & F(Y) & \\
\end{array}
$$ where $\eta_X$ is a morphism in $\mathbf{Sets}$ and $f:X \to Y$ is a morphism in the category $G-\mathbf{Sets}$ . Since $F$ is just the forgetful functor, the above diagram reduces to $$
\newcommand{\ra}[1]{\!\!\!\!\!\!\!\!\!\!\!\!\xrightarrow{\quad#1\quad}\!\!\!\!\!\!\!\!}
\newcommand{\da}[1]{\left\downarrow{\scriptstyle#1}\vphantom{\displaystyle\int_0^1}\right.}
%
\begin{array}{llllllllllll}
X & \ra{\eta_X} & X \\
\da{f} & & \da{f} \\
Y & \ra{\eta_Y} & Y & \\
\end{array}
$$ Concerns and Questions In the definition of natural transformation - I have that - given any $G-\text{Set}$ $X$ , $\eta_X:F(X)\to F(X)$ is a morphism. A natural $G-\text{Set}$ is simply to take $X=G$ and to let it act on itself through the group structure: $$\varphi: G\times G\to G \\ (g,s)\mapsto g\cdot s.$$ So the commutative diagram now becomes $$
\newcommand{\ra}[1]{\!\!\!\!\!\!\!\!\!\!\!\!\xrightarrow{\quad#1\quad}\!\!\!\!\!\!\!\!}
\newcommand{\da}[1]{\left\downarrow{\scriptstyle#1}\vphantom{\displaystyle\int_0^1}\right.}
%
\begin{array}{llllllllllll}
G & \ra{\eta_G} & G \\
\da{f} & & \da{f} \\
Y & \ra{\eta_Y} & Y & \\
\end{array}
$$ Remark 1. I remember a professor told me that the morphism $\eta_G$ is totally understood by what it does to the identity element $e\in G$ (from which I should be able to understand how to construct the group homomorphism), $$e\mapsto \eta_G(e).$$ I don't really understand what the above means. I think I have misunderstood something about the forgetful functor. When I think about the forgetful functor $F:A\to B$ , I think that the functor forgets everything that is present in $A$ , but isn't present in $B$ . In our case, it forgets the structure of group actions. And so, in particular, I cannot use the property of being a $G$ -equivariant map. Only the properties of being a set-theoretic map. Question 1. If $\eta_G(e)=s$ , and if I would like to make sense of what the professor told me, I think I would reason something as follows $$\eta_G(g)=\eta_G(e\cdot g)=\eta_G(e)\eta_G(g)=s\eta_G(g).$$ where I in the second equality used the property of being a group homomorphism. But on the other hand, if I want to treat it as a group homomorphism, then I think I had to do it to start with. That is, $\eta_G$ must map identities to identities (in order to be consistent in my reasoning). So I think my argument fails. My question is: What does he mean? I don't think what I did above makes any sense. But I think I have seen others using the properties of the morphisms in the category $A$ , after having applied the forgetful functor, hence my reasoning. Once again, I am not really sure what I am doing. So I may very well be wrong. Question 2. How does this tell me where to map a natural transformation? Given a $\eta\in\operatorname{Aut}(F)$ , where do I map it? Do I map it as follows $$\eta\mapsto \eta_G(e)?$$ Doing so, do I know that I have exhaustively told where to map every natural transformation? Question 3. I guess I also, somehow, have to use the commutative diagram in the definition of the natural transformation when I construct the group homomorphism, which I haven't done? I guess my suggestion above is not the correct way to do it. Do you have any ideas how I can construct the map? I would be really happy I could have any help from someone on this site to understand this better. Because I am really lost, and confused. Best wishes, Joel","['category-theory', 'forgetful-functors', 'group-theory', 'group-actions', 'natural-transformations']"
3966439,Confusion on notation of multivariable differentiation in n-dimensional space for matrices (gradients),"I am learning gradients on my own. One of the rules given in the textbook I am reading states this (without proof): $$∇_\mathbf{x}\mathbf{A}\mathbf{x} = \mathbf{A}^⊤$$ and then proceeds onwards as if it's trivial to see. I haven't seen this before so I need to go prove that it is correct. I do not know if the above equation is equal to $$∇f(\mathbf{x}) = \mathbf{A}^⊤$$ where $$f(\mathbf{x}) = \mathbf{A}\mathbf{x}$$ I decided to go ahead with this anyways and see where it takes me. I figure doing this with a simple $\mathbb{R}^{2 \times 2}$ like $\mathbf{A} = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ and seeing where it takes me when $\mathbf{x} = \begin{bmatrix}i\\j\end{bmatrix}$ . This means $\mathbf{A}\mathbf{x}$ is: $$\begin{bmatrix}ai + bj\\ci + dj\end{bmatrix}$$ The gradient would be (for $\mathbf{Ax}$ as I'm passing that in for the parameter): $$∇f(\mathbf{Ax}) = \begin{bmatrix} \frac{\partial f(\mathbf{Ax})}{\partial i} \frac{\partial f(\mathbf{Ax})}{\partial j}\end{bmatrix}^⊤$$ Which means we want to evaluate $∇_\mathbf{x} \begin{bmatrix}ai + bj\\ci + dj\end{bmatrix}$ . By the definition of a gradient, then we should get $∇f(\mathbf{Ax}) = \begin{bmatrix} \begin{bmatrix}a\\c\end{bmatrix} \\ \begin{bmatrix}b\\d\end{bmatrix} \end{bmatrix}$ This however does not look right, or is it? Is that equal to $\begin{bmatrix}a&c\\b&d\end{bmatrix}$ ? I wrote it out that way because I suspect that when we ask for $a_{21}$ we are saying ""get the 2nd row in the first column, and in a programming fashion, A[2][1] (or A[1][0] for most languages) would get $b$ from the result above. This would make sense because I'd get the transpose I was looking for in the primitive example. My linear algebra class only covered vectors, then jumped to matrices and showed us matrix multiplication without covering matrices as if they were a vector of vectors, so if there is a fundamental gap in my knowledge here and what I've described above is correct... then we've found an issue. Moving to an n-dimensional proof after figuring out what is wrong (if anything) seems like it'd be straight forward with more rigorous variable and index naming.","['partial-derivative', 'derivatives', 'linear-algebra']"
3966444,Is the shortest distance between a point and a curve always normal to the curve?,"Suppose we have some point $P$ and some differentiable function $f$ . Then let $P_C$ be a point on $f$ such that $|P-P_C|\le |P-f(t)|$ for any $t$ . In other words, $P_C$ is one of the points closest to $P$ and lies on $f$ . Intuitively, it seems to me that the line normal to $f$ at $P_C$ will always intersect $P$ . However, I don't know of a proof for this. Is my intuition correct? Is there a proof of this statement? Is this true for all dimensions? If it's not true for all differentiable functions, is there a subset of functions for which it is true?","['euclidean-geometry', 'calculus', 'geometry', 'differential-geometry']"
3966450,"Is my proof right? (Serge Lang ""Calculus of Several Variables 3rd Edition"", Potential Theory)","I am reading ""Calculus of Several Variables 3rd Edition"" by Serge Lang. Let $G(x,y) := (\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2})$ . Then, $\phi(x,y):=\arg(x+iy)$ is a potential function for the vector field $G$ on the plane from which the shaded region has been deleted. (Please see the picture below.) I proved the above fact. But I am not sure that I am right or not. Is my answer right? My solution is here: If $0\leq\arg(x+iy)<\frac{\pi}{2}$ , then $\phi(x,y)=\arctan(\frac{y}{x})$ . $\nabla \phi = G(x,y)$ . If $\arg(x+iy)=\frac{\pi}{2}$ , then $\phi(x,y)=\frac{\pi}{2}$ . $\lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})-\frac{\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{\pi}{2}-\frac{\pi}{2}}{h} = 0$ . So, $\nabla \phi(0,y) = G(0,y)$ . If $\frac{\pi}{2}<\arg(x+iy)<\frac{3\pi}{2}$ , then $\phi(x,y)=\arctan(\frac{y}{x})+\pi$ . $\nabla \phi = G(x,y)$ . If $\arg(x+iy)=\frac{3\pi}{2}$ , then $\phi(x,y)=\frac{3\pi}{2}$ . $\lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+2\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{3\pi}{2}-\frac{3\pi}{2}}{h} = 0$ . So, $\nabla \phi(0,y) = G(0,y)$ . If $\frac{3\pi}{2}\leq\arg(x+iy)\leq 2\pi-c$ , then $\phi(x,y)=\arctan(\frac{y}{x})+2\pi$ . $\nabla \phi = G(x,y)$ .","['potential-theory', 'solution-verification', 'derivatives']"
