question_id,title,body,tags
2967587,What is the difference between differentiability of a function and continuity of its derivative?,"I am sort of confused regarding differentiable functions, continuous derivatives, and continuous functions. And I just want to make sure I'm thinking about this correctly. (1) If you have a function that's continuous everywhere, then this doesn't necessarily mean its derivative exists everywhere, correct? e.g., $$f(x) = |x|$$ has an undefined derivative at $x=0$ (2) So this above function, even though its continuous, does not have a continuous derivative? (3) Now say you have a derivative that's continuous everywhere, then this doesn't necessarily mean the underlying function is continuous everywhere, correct? For example, consider $$
f(x) = \begin{cases}
1 - x \ \  \ \ \ x<0 \\
2 - x \ \ \ \ \ x \geq 0
\end{cases}
$$ So its derivative is -1 everywhere, hence continuous, but the function itself is not continuous? So what does a function with a continuous derivative say about the underlying function?","['calculus', 'derivatives']"
2967615,"How can I prove that eventually one function will overtake another, and find when?","Given for example 2 functions, $\ n^{100} $ and $\ 2^n$ . I know that $\ 2^n$ grows faster and that therefore there is some $\ n$ where it will eventually overtake $\ n^{100} $ but how can I prove this, and also maybe find that $\ n$ ?","['limits', 'functions', 'asymptotics']"
2967616,Generalized convex combination using matrices,"A convex combination between two points $x_1$ and $x_2$ in $\mathbb{R}^N$ is defined as: $$ x(\lambda) = \lambda x_1 + (1-\lambda)x_2, \qquad \lambda \in [0,1].$$ Here $x(0) = x_2$ , $x(1) = x_1$ , and the set $x(\lambda)$ (when $\lambda$ varies) can be interpreted as the line segment between $x_1$ and $x_2$ , which is itself a convex set. This could be generalized to matrix combinations, for example: $$ x(A) = A x_1 + (I-A)x_2, \qquad  0 \preceq A \preceq I,$$ where $I$ is the identity matrix, $A \preceq B \Leftrightarrow B-A$ positive semi-definite, and similarly here $x(0) = x_2$ and $x(I) = x_1$ . Question : what can be said about the set $S = \{A x_1 + (I-A)x_2 | 0 \preceq A \preceq I\}$ ? In particular, is there any geometrical interpretation of S? More generally, for $M$ point $\{x_i\}_{1,...,M}$ , one could consider: $$\left\{\sum_{i=1}^M A_ix_i \;\Bigg| \;\sum_{i=1}^M A_i = I, \, \quad 0 \preceq A_i \preceq I\right\},$$ which simplifies to the convex hull of $\{x_i\}_{1,...,M}$ when $A_i = \lambda_i I$ , $\sum \lambda_i = 1$","['convex-geometry', 'linear-algebra']"
2967688,"what is the geometry picture of Riemann tensor identity $R(X\wedge Y,V\wedge W) = R(V\wedge W, X\wedge Y)$","For symmetries in Riemann tensor of $R(X,Y)V:=\nabla_X\nabla_YV-\nabla_Y\nabla_XV-\nabla_{[X,Y]}V$ , there are excellent explanations on the intuition behind it like this relevant question . If we think $R(X,Y)V$ measures the change of tangent vector $V$ along $X$ and $Y$ direction, it is easily understood that $R(X,Y) = -R(Y,X)$ . But along this line, how can we see through the skew symmetry of the latter pair $ R(.,.,V,W) = -R(.,.,W,V) $ and the symmetry of exchange pair of $X,Y$ and $V,W$ beyond algebraic proof? $$R(X\wedge Y,V\wedge W) = R(V\wedge W, X\wedge Y)$$","['riemannian-geometry', 'curvature', 'geometric-algebras', 'general-relativity', 'differential-geometry']"
2967732,An interesting number theory question for math contest training $x^2+x=2y^3$ (finding integer solutions),"This question is in one of my old notebook and I marked it as a solved problem. However, somehow I can't remember the proof or maybe my solution in the past was wrong. The first thing I tried is to muptiply both sides by 4 and then plus to get the equation to the form: $(2x+1)^2=(2y+1)(4y^2-2y+1)$ . There are 2 cases, if $2y+1$ and $4y^2-2y+1$ are coprime then the rest is easy. However, if $gcd(2y+1,4y^2-2y+1)=3$ , I have tried multiple ways and failed to finish the proof. Any help would be greatly appreciated.","['contest-math', 'number-theory', 'elementary-number-theory']"
2967751,How to introduce a CW structure on RP^n?,"My first course in topology is going extremely fast, and does not seem like rigorous mathematics. Last lecture, we were given the definition of CW-structures, but did not do any examples. Yet we were assigned us homework problems about them. Here is the definition (verbatim) he gave:
A CW-space $X$ is a Hausdorff space such that $$ X = \bigcup_{i=0}^\infty X_i $$ where $X_0$ is a discrete space, and $X^{i+1}$ is obtained by attaching the disjoint union of $i+1$ disks to $X^i$ along a continuous map. I have no intuition about what this is supposed to mean, or how to use it. There is no reference book for the course, and all the resources I have found are at the advanced graduate level, so they use terminology that I am unfamiliar with. One problem I am supposed to solve is: Give a CW-structure on $\mathbb{S}^2$ that corresponds to a CW-structure on $\mathbb{RP}^2$ . To my understanding, I need to introduce a CW-structure on both of these spaces, then send the structure on $\mathbb{S}^2$ through the quotient map to $\mathbb{RP}^2$ , and verify this is also a CW-structure. Next, I am supposed to introduce a CW-structure on $\mathbb{RP}^n$ . The definition seems like an inductive construction, so once I understand the definition, and how to introduce a CW-structure on $\mathbb{RP}^2$ , this problem should become much easier. Please help - I have no idea what I got myself into by taking this course. Edit: Here is the picture I think I am supposed to draw? Comment: A ""proof by pictures"" is not proof to me. This does not seem like mathematics. I want to go back to analysis - epsilon was my friend.","['geometric-topology', 'cw-complexes', 'general-topology', 'algebraic-topology', 'projective-space']"
2967781,Prove inequality for all complex numbers,"Prove following inequality for all complex numbers: $
\lvert z\rvert \le \lvert z \rvert ^2 + \lvert z-1 \rvert
$ It is obvious for $\lvert z\rvert \gt 1 $ but what about the rest ? Any hints would be appreciated.","['complex-analysis', 'inequality', 'complex-numbers']"
2967796,Your evil probability professor has an urn with 9 balls.,"Your evil probability professor has an urn with 9 balls:  2 red, 3 white and 4 blue.  He draws two balls from the urn without replacement.  Let X be the number of red balls drawn and Y the number of white balls. a) Determine the joint probability mass function of X and Y. b) Are X and Y independent random variables? c) Compute the covariance between X and Y. For point A : $P(0,0)=\frac{4}{9} \cdot \frac{3}{8} = \frac{1}{6}$ That is correct for the solution. $P(0,1)=\frac{4}{9} \cdot \frac{3}{8} + \frac{3}{9} \cdot \frac{4}{8} = \frac{1}{3}$ That is correct for the solution. $P(1,0)=\frac{2}{9} \cdot \frac{4}{8} + \frac{4}{9} \cdot \frac{2}{8}= \frac{2}{9}$ That is correct for the solution. $P(1,1)=\frac{2}{9} \cdot \frac{3}{8} +  \frac{3}{9} \cdot \frac{2}{8}= \frac{1}{6}$ That is correct for the solution. $P(2,0)=\frac{2}{9} \cdot \frac{1}{8} = \frac{1}{36}$ That is correct for the solution. $P(0,2)=\frac{3}{9} \cdot \frac{2}{8} = \frac{1}{12}$ That is correct for the solution. For point B to check the independancy I have just to check if for example $P(X=0,Y=0) = P(X=0) \cdot P(Y=0)$ . $\frac{1}{6} \neq (\frac{7}{9} \cdot \frac{6}{8}) \cdot (\frac{6}{9} \cdot \frac{5}{8})$ . So X and Y are not independent. For point C I know that the covariance $Cov(X,Y)=E[X \cdot Y]-E[X]\cdot E[Y]$ , but how can compute the expectations, do I have to figure put with distribution is? How can I do it? Can someone help me?
Thanks in advance, Fabio!",['probability']
2967831,"Singular continuous measures ""in nature""","According to the Lebesgue decomposition theorem, there are 3 basic kinds of measures on $\mathbb R^n$ : continuous measures (those with a density), discrete measures, and singular continuous measures (those supported on a Lebesgue-null set, but with a continuous cdf). The first two kinds are obviously encountered in countless contexts. However, the only examples of the third kind that I am familiar with are rather artificial (e.g., the Cantor measure). Would a practicing statistician ever have occasion to work with a singular continuous measure?","['statistics', 'probability-theory', 'measure-theory']"
2967909,$PGL_2(\Bbb R)$ as a scheme,"How is $PGL_2(\Bbb R)$ a scheme? Here is my thought process $GL_2(\Bbb R)=Spec(\Bbb{R}[w,x,y,z,q]/((wz-xy)q-1))$ We want $PGL_2(\Bbb R)=GL_2(\Bbb R)/\Bbb{G}_m(\Bbb R)$ somehow. We can find $PGL_2(\Bbb R)$ as the open subset of $\Bbb RP^3$ $$\{[w:x:y:z]\in\Bbb RP^3\mid wz-xy\ne 0\}$$ We can find $PGL_2(\Bbb R)$ as the closed subset of $\Bbb RP^5$ given by $$\{[w:x:y:z:q:a]\in\Bbb RP^5\mid (wz-xy)q-a^3=0\}$$ Perhaps then we can conclude that $PGL_2(\Bbb R)$ is the scheme: $$Proj(\Bbb{R}[w,x,y,z,q,a]/((wz-xy)q-a^3)))$$ Is this correct? Or do I need to make sense of $PGL_2(\Bbb R)$ as a categorical quotient or something else?","['group-schemes', 'algebraic-geometry', 'algebraic-groups']"
2967921,Proof of the derivative of a quadratic form [duplicate],This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I was reading this pdf and on page 6 proposition 8 states: I don't really understand the steps that bring from $$\alpha = \sum_{j=1}^n\sum_{i=1}^n a_{ij} x_{i} x_{j}$$ to its derivative $$\frac{\partial \alpha}{\partial \bf{x}} = \sum_{j=1}^n a_{kj} x_J + \sum_{i=1}^n a_{ik}x_i$$ and then back to the final result: $$\frac{\partial \alpha}{\partial \bf{x}} = \bf{x}^T A^T + \bf{x}^T A$$ Can someone please help me?,"['matrices', 'calculus', 'matrix-calculus', 'linear-algebra', 'derivatives']"
2967964,Existence of derivative of series at $x = 0$,"I want to determine if the following function is differentiable for $x \in [0,\infty)$ $$F(x) = \sum_{j=0}^\infty \frac{e^{-jx}}{j^2 +1}$$ The series is uniformly convergent on $[0,\infty$ ) by the M-test since $\displaystyle \left|\frac{e^{-jx}}{j^2 +1} \right| < \frac{1}{j^2}$ and the series of termwise derivatives has uniform convergence on any interval $[\alpha,\infty)$ since $\displaystyle \left|\frac{je^{-jx}}{j^2 +1}\right|< \frac{e^{-\alpha j}}{j^2}.$ This proves that $F$ is differentiable for any point $x \in (0,\infty)$ . I know that the series of termwise derivatives does not converge  for $x = 0$ , so the derivative $F'(0)$ may not exist, but I would like to know how to show this directly. There are examples where a derivative of a series may exist even though it can't be obtained by termwise differentiation.","['derivatives', 'sequences-and-series']"
2967986,"Integrating $\iint_{R} \frac{y}{x^2+y^2}\,dA$ where $R$ is bounded by $y=x,y=2x,x=2$","So I'm supposed to set an integral up for both orders of integration and evaluate using the ""nicer"" of the two $$\iint_{R} \frac{y}{x^2+y^2} \,dA$$ for $R$ bounded by $y=x, y = 2x$ , and $x=2$ . So what I have tried to do so far was figuring out the region which I'm integrating over which is So this is what my two integrals are: $$\int_{0}^{2}\int_{y=x}^{y=2x} \frac{y}{x^2+y^2} \,dy\, dx$$ $$\int_{0}^{2}\int_{x=\frac{y}{2}}^{x=y} \frac{y}{x^2+y^2}\, dx\, dy$$ I believe the easier integral would be the second one when you integrate with respect to $dy$ first, because since $y$ would be a constant,  you can factor out $x$ and integrate $\frac{1}{x^2+y^2}$ .   I was wondering if my thought process is correct, and if it does can you help me integrate this?","['integration', 'multivariable-calculus']"
2968006,$\mathbb{R}^2 \setminus \mathbb{Q}^2$ connected?,"To show that it is connected, I have assumed for contradiction that it is not. It can therefore be separated into two disjoint, proper clopen subsets $A \sqcup B.$ I have shown that it is not possible that $A$ is both closed and open, for if it is closed it either (1) has boundary points and contains all of them, or (2) it has no boundary points. I have shown that $A$ cannot have a boundary point, for that would contradict the assumption that it is open (no neighborhood of a boundary point is fully contained within the set). So, $A$ has no boundary points. Now, I'm trying to show that $A$ cannot be proper if it has no boundary points. HERE is my difficulty. I know $\mathbb{R}^2$ doesn't ""have"" infimums and supremums, but is there a way I can use the infimums and supremums along each dimension to show that it is bounded? That is, if $A \subset \mathbb{R}^2$ is bounded along each of its dimensions, then it itself is bounded. However, I am having trouble formalizing this part. My argument comes across clunky. Is there a simple way to do this, or will a different direction prove better?","['general-topology', 'real-analysis']"
2968028,Proof verification of $x_n = \sqrt[3]{n^3 + 1} - \sqrt{n^2 - 1}$ is bounded,"Let $n \in \mathbb N$ and: $$
x_n = \sqrt[^3]{n^3 + 1} - \sqrt{n^2 - 1}
$$ Prove $x_n$ is bounded sequence. Start with $x_n$ : $$
\begin{align}
x_n &= \sqrt[^3]{n^3 + 1} - \sqrt{n^2 - 1} = \\
&= n \left(\sqrt[^3]{1 + {1\over n^3}} - \sqrt{1 - {1\over n^2}}\right)
\end{align}
$$ From here: $$
\sqrt[^3]{1 + {1\over n^3}} \gt 1 \\
\sqrt{1 - {1\over n^2}} \lt 1
$$ Therefore: $$
\sqrt[^3]{1 + {1\over n^3}} - \sqrt{1 - {1\over n^2}} \gt 0
$$ Which means $x_n \gt 0$ . Consider the following inequality: $$
\sqrt[^3]{n^3 + 1} \le \sqrt{n^2 + 1} \implies \\
\implies x_n < \sqrt{n^2 + 1} - \sqrt{n^2 - 1}
$$ Or: $$
x_n < \frac{(\sqrt{n^2 + 1} - \sqrt{n^2 - 1})(\sqrt{n^2 + 1} + \sqrt{n^2 - 1})}{\sqrt{n^2 + 1} + \sqrt{n^2 - 1}} = \\ 
= \frac{2}{\sqrt{n^2 + 1} + \sqrt{n^2 - 1}} <2
$$ Also $x_n \gt0$ so finally: $$
0 < x_n <2
$$ Have i done it the right way?","['inequality', 'proof-verification', 'upper-lower-bounds', 'sequences-and-series', 'algebra-precalculus']"
2968055,Mean of two numbers by infinite sequences,"Consider two numbers $a$ and $b$ , and the following sequence alternating between even and odd positions: $$
a+2b+3a+4b+5a+6b\ldots,
$$ If we ''normalize'' $$
\frac{a+2b+3a+4b+\ldots}{1+2+3+4+\ldots},
$$ it turns out this ratio approaches the mean value of $a$ and $b$ : $(a+b)/2$ . In general $$
\frac{a+2^n b+3^n a+4^n b+\ldots}{1^n+2^n+3^n+4^n+\ldots}=\frac{a+b}{2}
$$ for $n\geq1$ . However if we use exponential functions instead powers: $$
\frac{m^1 a+m^2 b+m^3 a+m^4 b+\ldots}{m^1 +m^2 +m^3 +m^4 +\ldots}
$$ for some $m>1$ , this ratio oscillates and does not approach any number. Could someone explain why convergence to the mean is obtained by using sequences of powers, and the ratio diverges for sequences of exponentials?","['limsup-and-liminf', 'limits', 'probability', 'sequences-and-series']"
2968062,Painted cube probability,"Each face of a cube is painted either red or blue, each with
  probability 1/2. The color of each face is determined independently.
  What is the probability that the painted cube can be placed on a
  horizontal surface so that the four vertical faces are all the same
  color? The correct answer is $5/16$ . I don't understand why my solution (below) is incorrect. I will compute the probability that the four vertical faces are painted red and then multiply by two to cover the symmetric blue case. To compute this value, I will condition on the number of faces painted red. Case 1: All faces are painted red. This occurs with probability $1/64$ and guarantees the four vertical faces to be red. Case 2: Five faces are painted red. This occurs with probability $1/32$ and given this event occurs, there is a ${5\choose 4}/{6\choose 4} = 1/3$ chance of choosing four red-painted sides to be the vertical sides. Case 3: Four faces are painted red. This occurs with probability $1/16$ and given this event occurs, there is a ${4\choose 4}/{6 \choose 4} = 1/15$ chance of choosing four red-painted sides to be the vertical sides. Note that I don't need to consider the cases where there are less than $4$ red-painted sides, since their probabilities will equal $0$ . Therefore, my final answer is $2 \cdot \left((\frac{1}{64} \cdot 1) + (\frac{1}{32} \cdot \frac{1}{3}) + (\frac{1}{16} \cdot \frac{1}{15})\right) = 0.0604166666 \neq 5/16.$","['probability-theory', 'probability']"
2968102,"If $G$ is a simple group of order $60$, how can we there exists some $G\to A_6$ not trivial without using the fact that $G\cong A_5$?","If $G$ is simple, any homomorphism out of $G$ must be trivial or injective since the kernel of a homomorphism is a normal subgroup. If $G$ is simple of order $60$ , how can I show that there exists an injective map from $G\to A_6$ without using the fact that $G\cong A_5$ ?","['simple-groups', 'group-theory', 'abstract-algebra']"
2968119,Finding the residue with a Laurent series expansion.,"I have a question about the following problem: ""Detect the error in the following argument. The function $f(z)=\frac{1}{z(z-1)^2}$ has an isolated singularity at $z=0$ . The Laurent series is $f(z)=\frac{1}{(z-1)^3}-\frac{1}{(z-1)^4}+\frac{1}{(z-1)^5}-...$ for $|z-1|>1$ . Apparently $z=1$ is an essential singularity with residue 0."" Now I know that the error is that one has to compute the Laurent series on the annulus $0<|z-1|<1$ , but why is this the case? Do you always have to take the inner annulus to compute the residue? A clear explanation would be much appreciated!","['complex-analysis', 'residue-calculus', 'laurent-series']"
2968156,Likelihood Ratio Test for binomial proportions,"Suppose we wish to preform a simple likelihood ratio test for the parameters of two binomial distributions. Where the null hypothesis is that the two parameters are equal versus the alternative they are not. Now for the following example, to construct a normal test or T-test would be straightforward. However I am interested in comparing this to using the likelihood ratio test and chi-square distribution. Say we know for example $A∼Bin(40,p_{A})$ and $B∼Bin(20,p_{B})$ and we observe data from A and we observe $24$ successes hence we can then estimate $p=0.6$ and from B we observe data and observe $15$ successes hence $p=0.75$ How would one go about forming the likelihood test statistic?","['statistics', 'probability', 'hypothesis-testing']"
2968252,On Grassmann manifold,"In most references about Grassmann manifold, we usually introduce the following map: suppose that $S$ is a fixed subspace of codimension $k$ , and let $\Omega_S \subset Gr(k,\mathbb{K})$ be all subspaces $W$ such that $W \cap S = \{0\}$ . $\Omega_S$ is an open set. Fix some $U \in \Omega_S$ . One can show that $W \in \Omega_S$ if and only if there exists some linear map $\beta_W \in Hom(U,S)$ such that $W = \{x + \beta_W x : x \in U \}$ . From there, one can easily check that there exists a bijection $\Omega_S \simeq Hom(U,S)$ . I'm interested in show that this bijection is actually an homeomorphism of topological spaces! So far, I haven't been able to make a lot about this claim. What bugs me is that the topologies at stake are not necessarily intuitive to work with: $Gr(r, \mathbb{K})$ is endowed with the quotient topology, and $Hom(U,S)$ is really just an euclidean space, so might just take some random norm/distance. Any help greatly appreciated ! EDIT : Look here Lee's page 22 for more informations.","['differential-topology', 'differential-geometry']"
2968273,How to prove $ \frac{n}{2}^\frac{n}{2} \leq n! $ using induction,"While proving that $n! \leq n^n $ using induction is easy, and showing that $ \frac{n}{2}^\frac{n}{2} \leq n! $ by comparing each term of the product does work, I've struggled to come up with a successful way of showing the latter through induction $$ \frac{k+1}{2}^\frac{k+1}{2} \leq (k+1)! $$ $$ \sqrt{\frac{k+1}{2}} \cdot \frac{k+1}{2}^\frac{k}{2} \leq (k+1)k! $$ $$ \frac{k+1}{2}^\frac{k}{2} \leq \sqrt{2k+2} \cdot k! $$ This is what's seemed to me to be the most promising but I can't seem to take it any further, or find any productive way to apply the inductive assumption Following @gimusi's hint, by dividing both sides by $ \sqrt{\frac{k+1}{2}}$ , which then leads to the result of having to prove: $$ (2k+2)^\frac{1}{k} \leq \frac{k+1}{k} $$ which would be easy to show if it was $ \frac{k}{k+1} $ since it would be smaller than 1, but as it is I can't find a straightfoward way to progress","['induction', 'discrete-mathematics']"
2968327,Path connectedness of graph and domain implies continuity?,"I'm trying to produce an example to show that a path-connected graph $G_f$ of a function $f$ whose domain is also path-connected does not imply the function is continuous. Does this example work: Consider The function $f:[\theta,2\pi] \rightarrow \mathbb{R}^2$ where $f$ is defined in the following way: $f(\theta) = (cos(\theta),sin(\theta))$ for $0 < \theta < 2\pi,$ $f(0) = (0,1),$ and $f(2\pi) = (0,-1).$ The graph produced is a circle without the point $(0,1).$ Moreover, it is has a jump discontinuity at $\theta = 0$ and $\theta = 2\pi.$ However, the domain $[0,2\pi]$ and the graph are path connected. Is there an error somewhere? I doubt myself...","['general-topology', 'real-analysis']"
2968337,"Probability of missing dart throw given uniform random radius (0,1) of board","Given that the radius of a dart board is uniformly distributed between 0 and 1, I throw a dart uniformly at random within a circle of radius 1. What is the probability of not hitting the dart board? I have calculated the expected area of the dart board E(A) = π/3. Would it be correct to assume (1 - π/3)/π = 2/3 is the probability of not hitting the dart board? Or would I need to use integration given the continuous probability?","['statistics', 'uniform-distribution', 'probability']"
2968359,"IMO 2017: Determine all functions $f: \mathbb{R} \to\mathbb{R}$ such that, for all real numbers $x$ and $y$, $f(f(x)f(y)) + f(x +y) = f(xy)$.","EDİT : I think I've repair the error in the solution. I want to know if I'm fixing it properly. I'm just a student, not a mathematician. Please focus on the ""backbone"" of my writing. I want to prove that, the following substitution is correct: $$ x\mapsto f^{-1}(x)$$ Statement : For a function to have an inverse, each element $y ∈ Y $ must correspond to no more than one $x ∈ X; $ a function $f$ with this property is called one-to-one or an injection. The function $f(x)$ is an injective and $f$ is an invertible function. In other words, if $f(0)≠0$ , for function $f(x)$ , the inverse function $f^{-1}(x)$ is exist. $f(x)=f(0)-\frac {x}{f(0)} \Rightarrow f^{-1}(x)=f(0)(f(0)-x)$ and $f(f^{-1}(x))=x$ for all $x\in\mathbb {R}$ . For this reason, we can applying the substitution $ x\mapsto f^{-1}(x), x\in\mathbb{R}.$ For example: If $f(f(x))=f(x)-5 , x\in\mathbb{R}$ , then $f(x)=x-5$ must be. Because,  the function $f(x)$ is an injective and $f$ is an invertible function. Applying $x\mapsto f^{-1}(x)$ we have $f(x-5)=x-10 \Rightarrow f(x)=x-5.$ I hope you can understand what I mean. Can you tell me that this approach is wrong or true? Problem: Let $\mathbb{R}$ be the set of real numbers. Determine all functions $f:  \mathbb{R} \to\mathbb{R}$ such that, for all real numbers $x$ and $y$ , $$f \big(f(x)f(y)\big) + f(x +y) = f(xy).$$ I ask You to confirm that the solution is sufficient / insufficient / missing / incorrect or correct. Here is my attempts: Are there any problems in my substitutions?","['contest-math', 'functional-equations', 'proof-writing', 'proof-verification', 'functions']"
2968360,Using $ \sum_{k=0}^{\infty}\frac{(-1)^{k}}{k!}=\frac1e$ evaluate first $3$ decimal digits of $1/e$.,"Using the series $\displaystyle \sum_{k=0}^{\infty}\frac{(-1)^{k}}{k!}=\frac{1}{e}$ , evaluate the first $3$ decimal  digits of $1/e$ . Attempt. In alternating series $\displaystyle \sum_{k=0}^{\infty}(-1)^{k+1}\alpha_n$ , where $\alpha_n \searrow 0$ , if $\alpha$ is the sum of the series then $$|s_n-\alpha|\leq \alpha_{n+1}.$$ So, in our case  we need to find $n$ such that $|s_n-1/e|<0.001$ , where $\displaystyle s_n=\sum_{k=0}^{n-1}\frac{(-1)^{k}}{k!}$ and it is enough to find $n$ such that $\dfrac{1}{n!}<0.001$ , so $n\geq 7$ . Therefore: $$s_7=\sum_{k=0}^{6}\frac{(-1)^{k}}{k!}=0.36805\ldots$$ so I would expect $\dfrac{1}{e}=0.368\ldots$ . But: $\dfrac{1}{e}=0.36787944\ldots$ . Where am I missing something? Thanks in advance.","['calculus', 'exponential-function', 'sequences-and-series']"
2968361,$\int_0^{2\pi} e^{e^{i\theta}}d\theta$,This came up during my GRE prep.  Typically these questions have a trick which allows for a solution in <3 mins.  Either the ''trick'' or a hint or a worked out solution would be nice.  So far I tried expanding via EGF to little success.  I have no background in complex analysis. Thank you in advance for your time.,"['complex-analysis', 'complex-integration']"
2968451,What are the transformations that preserve cross ratios on a sphere in higher dimensions?,"If we have four points $x,y,z,w$ on a sphere, then the cross ratio is $\frac{|x-z|}{|x-w|}\frac{|y-w|}{|y-z|}$ . If we consider $S^1 \subseteq \mathbb{C}$ , then the transformations of $\mathbb{C}$ which preserve the cross ratios on the circle are precisely the Mobius transformations which map the open unit disc to itself. Is there a nice classification for transformations preserving cross ratios of spheres in higher dimensions?","['complex-analysis', 'projective-geometry', 'hyperbolic-geometry', 'mobius-transformation']"
2968455,Existence of solution of a 3-dimensional linear PDE.,"Maybe this is a canonical result but I'm facing difficulties to find a reference. It is well known the following theorem about Linear Partial Differential Equations: Theorem: Let $\Omega \subset \mathbb{R}^2$ be an open set and $\Gamma \subset \Omega$ a $\mathcal{C}^1$ curve. Let $\gamma (s) = (\alpha(s),\beta(s))$ be a $\mathcal{C}^1$ parametrization of $\Gamma$ , defined in the interval $I \subset \mathbb{R}$ . Suppose that $a,b,c$ $\in$ $\mathcal{C}^1 (\Omega , \mathbb{R})$ and $f\in\mathcal{C}^1 (I,\mathbb{R})$ , such that $a$ and $b$ never simultaneously vanish in $\Omega$ and satisfy $$\text{det} \begin{bmatrix}
\alpha'(s) & a(\alpha(s),\beta(s)) \\
\beta'(s) & b (\alpha(s),\beta(s)) 
\end{bmatrix} \neq 0, \ \forall \  s \in I.  $$ Then, there is a solution of the Cauchy Problem \begin{align*}
\left\{\begin{array}{l}
a(x,y)u_x + b(x,y) u_y = c(x,y),\quad &\text{if}\ (x,y) \in \Omega, \\
u(\alpha(s),\beta(s)) = f(s)\ &\text{if} \ s \in I.  
\end{array}\right.
\end{align*} defined in a neighborhood of $\Gamma$ . However, I wasn't able to find a 3-dimensional version of this theorem. 
 All the books in which I looked for such a theorem only presented the 2-dimensional version. I think that a 3-dimensional version of this theorem would look like the following Possible Theorem: Let $\Omega \subset \mathbb{R}^3$ be an open set and $\Sigma \subset \Omega$ be a $\mathcal{C}^1$ surface. Let $\sigma (u,v) = (\sigma_1(u,v),\sigma_2(u,v),\sigma_3(u,v))$ a $\mathcal{C}^1$ parametrization of $\Sigma$ , defined in the square $I\times I \subset \mathbb{R}^2$ . Suppose that $a,b,c,d$ $\in$ $\mathcal{C}^1 (\Omega , \mathbb{R})$ and $f\in\mathcal{C}^1 (I,\mathbb{R})$ , such that $a$ , $b$ and $c$ never simultaneously vanish in $\Omega$ and satisfy $$\text{det} \begin{bmatrix}
\frac{\partial \sigma_1}{\partial u}(u,v) & \frac{\partial \sigma_1}{\partial v}(u,v) & a(\sigma(u,v))\\
\frac{\partial \sigma_2}{\partial u}(u,v) & \frac{\partial \sigma_2}{\partial v}(u,v) &b(\sigma(u,v))\\
\frac{\partial \sigma_3}{\partial u}(u,v) & \frac{\partial \sigma_3}{\partial v}(u,v) & c(\sigma(u,v))
\end{bmatrix} \neq 0, \ \forall \  (u,v) \in I\times I.  $$ Then, there is a solution of the Cauchy Problem \begin{align*}
\left\{\begin{array}{l}
a(x,y,z)u_x + b(x,y,z) u_y + c(x,y,z)u_z = d(x,y,z) ,\quad& \text{if}\ (x,y,z) \in \Omega, \\
u(\sigma(u,v) ) = f(u,v)\ &\text{if} \ (u,v) \in I\times I. \\
\end{array}\right.
\end{align*} defined in a neighborhood of $\Sigma$ . Does anyone know if this theorem is true and could indicate me a reference?","['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'differential-topology', 'differential-geometry']"
2968521,What is the convergence of the explicit formula for $\frac{\zeta'(s)}{\zeta(s)}$?,"The explicit formula for $\frac{\zeta'(s)}{\zeta(s)}$ illustrated in (3) below was derived from the relationship illustrated in (1) below using the explicit formula for $\psi(x)$ defined in (2) below. (1) $\quad\frac{\zeta'(x)}{\zeta(s)}=\frac{s}{1-s}-s\int\limits_1^{\infty}x^{-s-1}(\psi(x)-x)\,dx\,,\quad \Re(s)\ge 1\quad(\Re(s)>\frac{1}{2}\text{ assuming RH})$ (2) $\quad\psi_o(x)=x-\sum\limits_\rho\frac{x^{\,\rho}}{\rho}-\log(2\,\pi)-\sum\limits_{n=1}^\infty\frac{x^{-2\,n}}{-2\,n}\,,\quad x>1$ (3) $\quad\frac{\zeta'(s)}{\zeta(s)}=\frac{s}{1-s}+\log(2\,\pi)-\frac{1}{2}H_{\frac{s}{2}}+s\sum\limits_\rho\frac{1}{\rho\,\left(s-\rho\right)}$ My understanding is the validity of relationship (1) above for $\Re(s)\ge 1$ is predicted by the Prime Number Theorem, and the validity of relationship (1) above for $\Re(s)>\frac{1}{2}$ is predicted by the Riemann Hypothesis (RH). Contrary to these theoretical convergences, formula (3) above seems to exhibit observational evidence of convergence everywhere I've evaluated it. Some example plots are illustrated following the question below. Question : What is the convergence of the explicit formula for $\frac{\zeta'(s)}{\zeta(s)}$ illustrated in (3) above? It's interesting to contrast formula (3) above for $\frac{\zeta'(s)}{\zeta(s)}$ with formula (4) below for $\frac{\zeta'(s)}{\zeta(s)}$ . Note formula (3) above can also be derived from formula (4) below by subtracting $\frac{\zeta'(0)}{\zeta(0)}$ (see section 3.2 of ""Riemann's Zeta Function"" by H. M. Edwards copyright 1974). (4) $\quad\frac{\zeta'(s)}{\zeta(s)}=\frac{1}{1-s}+\frac{1}{2}\log(\pi)-\frac{1}{2}\psi^{(0)}\left(\frac{s}{2}+1\right)+\sum\limits_\rho\frac{1}{s-\rho}$ The following plot illustrates formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ (orange curve) and the reference function (underlying blue curve) evaluated for $s\in\mathbb{R}$ where formula (3) is evaluated over the first 100 pairs of non-trivial zeta zeros. Figure (1): Illustration of formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ (orange curve) evalualated for $s\in\mathbb{R}$ The following two plots illustrate the real and imaginary parts respectively of formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ (orange curves) and the corresponding parts of the reference function (underlying blue curves) evaluated along the critical line $s=\frac{1}{2}+i\,t$ where formula (3) is evaluated over the first 100 pairs of non-trivial zeta zeros. Figure (2): Illustration of the real part of formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ (orange curve) evaluated along the critical line $s=\frac{1}{2}+i\,t$ Figure (3): Illustration of the imaginary part of formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ (orange curve) evaluated along the critical line $s=\frac{1}{2}+i\,t$ I don't believe the vertical orange lines at the location of the non-trivial zeta zeros illustrated in Figure (3) above indicate a lack of convergence of formula (3), rather I believe the vertical lines are a result of Mathematica connecting two adjacent evaluation points where the first evaluation point is slightly to the left of a non-trivial zeta zero and the next evaluation point is slightly to the right of the non-trivial zeta zero. Formula (3) above clearly illustrates a pole at each non-trivial zeta zero. The following table illustrates the imaginary part of formula (3) for $\frac{\zeta'(s)}{\zeta(s)}$ evaluates fairly closely to the imaginary part of the reference function when evaluated at $\rho_1\pm\epsilon$ where formula (3) was evaluated over the first 100 pairs of non-trivial zeta zeros. (5) $\quad\begin{array}{ccccc}
 \epsilon  & \Im\left(\frac{\zeta'(\rho_1-\epsilon)}{\zeta(\rho_1-\epsilon)}\right) & \Im\left(\frac{\zeta'(\rho_1+\epsilon)}{\zeta(\rho _1+\epsilon)}\right) & \Im(\text{(3)}@(\rho_1-\epsilon)) & \Im(\text{(3)@}(\rho_1+\epsilon)) \\
 0.1 & 9.91199 & -10.0762 & 9.82459 & -10.1649 \\
 0.01 & 99.9173 & -100.082 & 99.8293 & -100.17 \\
 0.001 & 999.918 & -1000.08 & 999.83 & -1000.17 \\
 0.0001 & 9999.92 & -10000.1 & 9999.83 & -10000.2 \\
 0.00001 & 99999.9 & -100000. & 99999.8 & -100000. \\
 \text{1.$\grave{ }$*${}^{\wedge}$-6} & 1.\times 10^6 & -1.\times 10^6 & 1.\times 10^6 & -1.\times 10^6 \\
 \text{1.$\grave{ }$*${}^{\wedge}$-7} & 1.\times 10^7 & -1.\times 10^7 & 1.\times 10^7 & -1.\times 10^7 \\
 \text{1.$\grave{ }$*${}^{\wedge}$-8} & 1.\times 10^8 & -1.\times 10^8 & 1.\times 10^8 & -1.\times 10^8 \\
 \text{1.$\grave{ }$*${}^{\wedge}$-9} & 9.99995\times 10^8 & -1.\times 10^9 & 1.\times 10^9 & -1.\times 10^9 \\
 \text{1.$\grave{ }$*${}^{\wedge}$-10} & 1.00001\times 10^{10} & -9.9997\times 10^9 & 1.\times 10^{10} & -1.\times 10^{10} \\
\end{array}$","['number-theory', 'riemann-hypothesis', 'riemann-zeta', 'mellin-transform', 'prime-numbers']"
2968533,"Proving $f : (-1, 1) \rightarrow \mathbb{R}$ has a property","I'd like to answer the two questions below. I believe my solution to the first one is right, but it'd help if someone could please help verify this, as I am working through this book independently through self-study. I don't know how to do the second one, which is a continuation of the first problem. I'm pretty sure both questions utilize a Theorem stated in my book, so I've provided that as well. Suppose $f : (-1, 1) \rightarrow \mathbb{R}$ has $n$ derivatives and its $n^{\text{th}}$ , suppose its $n^{\text{th}}$ derivative $f^{(n)} : (-1, 1) \rightarrow \mathbb{R}$ is bounded. Finally, suppose we have $$f(0) = f'(0) = f''(0) = \cdots f^{(n - 1)}(0) = 0.$$ Prove there exists a positive number $M$ such that $$|f(x)| \leq M|x|^{n}.$$ Second question: Suppose $f : (-1, 1) \rightarrow \mathbb{R}$ has $n$ derivatives and there is a positive number $M$ such that $$|f(x)| \leq M|x|^{n}.$$ Prove $$f(0) = f'(0) =  \cdots f^{(n - 1)}(0) = 0.$$ Also, here's the theorem I was referring to: Theorem: Let $I$ be an open interval and $n$ be a natural number and suppose that the function $f : I \rightarrow \mathbb{R}$ has $n$ derivatives.  Suppose also at the point $x_{0}$ in $I$ , $$f^{(k)}(x_{0}) = 0$$ for $0 \leq k \leq n - 1$ . Then, for each point $x \neq x_{0}$ in $I$ , there is a point $z$ strictly between $x$ and $x_{0}$ at which $$f(x) = \frac{f^{(n)}(z)}{n!}(x - x_{0})^{n}.$$ Here's my attempt at problem $1$ : By the Theorem above, for each $x \in (-1, 1)$ , $x \neq 0$ , there's a point $z$ between $x$ and $0$ such that $$f(x) = \frac{f^{(n)}(z)}{n!}x^{n}.$$ Using the fact that $f^{(n)}$ is bounded, we know there exists a bound $N$ such that $|f^{(n)}(x)| \leq N$ for all $x$ in $(-1, 1)$ .  Therefore, $$|f(x)| = \left|\frac{f^{(n)}(z)}{n!}x^{n}\right| \leq |\frac{N}{n!}x^{n}| = (N/n!)|x|^{n} $$ So setting $M = N/n!$ suffices. Is this correct? How do I do the next one?","['functions', 'real-analysis']"
2968583,Infinitely many number fields,"I wonder if it is known that there are $\textit{infinitely}$ many number fields $F$ (up to isomorphism) with fixed degree $[F:\mathbb{Q}]=n$ and fixed a transitive group $G$ of $S_n$ such that $G=\textrm{Gal}(F^c/\mathbb{Q})$ (if we assume inverse Galois holds for $G$ ). Obviously, there are finitely many number fields $F$ if we make a restriction for $F$ such that $|d_{F}|<M$ for some $M>0$ . This is from Hermite's theorem.","['algebraic-number-theory', 'field-theory', 'galois-theory', 'abstract-algebra', 'galois-extensions']"
2968584,Let $G$ be a finite group such that $|G| = p$,"Let $G$ be a finite group such that $|G| = p$ where $p$ is  prime. Let $g,h$ be elements of $G$ . What are the possible orders of $\langle g \rangle \cap \langle h \rangle$ ? My current thought: I know if $\langle g \rangle \cap \langle h \rangle = m$ , $g|m$ , and $h|m$ , but how can I determine $g$ and $h$ ?","['group-theory', 'abstract-algebra']"
2968588,Pigeonhole Principle Issue five integers where their sum or difference is divisible by seven.,"Given any five integers, there will be two that have a sum or difference divisible by 7. I'm trying to solve this using the Pigeonhole Principle, but it is not making sense to me, how this principle holds true in this case. Aren't the ""pigeons"" in this case ""five"" and the ""holes"" are ""seven""? I'm so confused.","['pigeonhole-principle', 'proof-writing', 'combinatorics', 'discrete-mathematics']"
2968618,What is the expected number of infected people?,"I was recently in an interview and got asked an interesting problem which I want to know the answer to. Suppose we're in a party with $n$ people. At minute $t=0$ , there is $1$ person who has a contagious disease that is transferable by shaking hands. Every minute, every person in the party shakes hands with someone they haven't shook hands with yet randomly (i.e. if there are $k$ people he/she hasn't shaken hands with, then probability of shaking hands with any of them is $\frac{1}{k}$ ). Let $S(t)$ be the random variables of the number of people who have disease at minute $t$ . What is $S(t), \mathbb{E}(S)$ ? A followup question was if you could choose who shakes hands with who, how can you maximize $t^*$ , the minute where everyone is infected. In the interview, it was $n=1000$ , and I did some approximations manually and reached $664$ for the first problem until minute $t=10$ . Couldn't really answer the followup question that well. I am not even sure if this is a known probability distribution or not. Would appreciate an answer, thanks!","['stochastic-processes', 'probability-distributions', 'reliability', 'probability']"
2968638,Integration with respect to the Lebesgue-Stieltjes measure associated with floor function,"For two different measures, I am trying to identify $L^1(\mu)$ (the set of integrable functions, i.e. $\{f:\int \lvert f\rvert du<\infty\}$ , and trying to compute $\int fd\mu$ for $f\in L^1(\mu)$ . on $\mathbb R, \mu=\mu_F$ , the Lebesgue-Stieltjes measure with $F(x)=\lfloor x\rfloor$ . Attempts I know that the unique measure derived from the floor function would measure the number of integers between the end points, or in the set. But I'm not sure how to build on that to use to calculate the integral for a general $f$ , or what would make a function integrable. Update : $$\int 1_{E}(x)d\mu(x)=\mu(E)=\sum_{n\in\mathbb Z}1_E(n)$$ should be true for any characteristic function. And for this integral to be finite, in any Borel set $E$ , there should only be finite number of integers in the set. But I'm not sure how to bootsrap up from this point to simple functions, and then functions in general. Any help would be greatly appreciated!","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
2968657,Basic proof that $\frac{d}{dx}(\sin(nx))=n\cos(nx)$,"Could someone provide a basic proof that $\frac{d}{dx}(\sin(nx))=n\cos(nx)$ ? I'm using $n$ to be broad, and so this can be searched easier, though if it's easier to provide an example, then replace $n$ with $3$ . My teacher has just taught us the Chain Rule, and he showed us an example like this: $$f(x)=(\sin(3x))^3$$ $$f'(x)=3(\sin(3x))^2(\cos(3x))(3)$$ $$f'(x)=9\sin^2(3x)\cos(3x)$$ I understand that the derivative of $\sin(x)$ is $\cos(x)$ and I understand that the derivative of $3x$ is $3$ , but I just don't see how this makes sense, because it feels like you're taking the derivative of $\sin(3x)$ which as far as I can tell should be $\cos(3x)$ , so where did the 3 come from? I was told that the 3 was taken as the derivative of the 3x inside the $\cos$ ... but why? Isn't the $\cos(3x)$ already differentiated? My teacher made a few different attempts to explain this to me, and I just don't get it, this is why I've asked for a proof. Now, I ask for a basic proof, because I don't want one of these super textbook like answers. Feel free to cut corners like not always putting $f(x)$ in front of each line or something like that, I just want to wrap my head around this concept! Oh and, I've already graphed $\sin(3x)$ and $3\cos(3x)$ , which so far is the only way I was able to actually see that, that is indeed the derivative.","['calculus', 'derivatives', 'trigonometry', 'chain-rule']"
2968754,Evaluate $\lim_{n\to\infty}\frac{x_n}{\sqrt n}$,"Question: $x_1>0$ , $x_{n+1}=x_n+\dfrac1{x_n}$ , $n\in\Bbb N$ . Evaluate $$\lim_{n\to\infty}\frac{x_n}{\sqrt n}.$$ What I know now is that $\dfrac1{x_n}\to\dfrac12$ when $n\ge2$ , $\{x_n\}$ is monotonically increasing， $x_n\ge 2$ when $n\ge 2$ . I have tried to use the Stolz theorem, and I found I could not use Squeeze theorem. Could you please give some instructions? Thank you!","['limits', 'analysis', 'real-analysis']"
2968822,pacing between iid r.v. : why introduce a scaling $\frac{1}{Np_X(x)}$ in proving $\hat p(\hat s)\sim e^{\hat s}$,"Let $\{X_1,...,X_n\}$ iid r.v. We denote $p_X(x)=P_{X_i}(x)$ the PDF of $X_i$ for all $i$ . In the section 2.3 page 12 of this paper , we have Let $p_N(s\mid X_i=x)$ is the probability that given $X_j=x$ , there is a r.v. variable $X_k$ ( $k\neq j$ ) s.t. $X_{k}=x+s$ and there is no other r.v. lie between. We can prove that $$p(s\mid X_j=x)=p_X(x+s)[1+F(x)-F(x+s)]^{N-2},$$ where $F$ is the CDF of the $X_i$ 's. From this, we can conclude that $$p(s\mid any\ X=x)=\sum_{j=1}^N p_N(s\mid X_j=x)\mathbb P\{X_i=x\}=Np_N(s\mid X_i=x)p_X(x),$$ and finally $$p_N(s)=N\int_\sigma  p_N(s\mid X_j=x)p_X(x)dx,$$ where $\sigma $ is the support of the $X_i's$ . Now, we are interested is to compute the spacing distribution function when we have an infinite number of r.v. ,i.e. for $\{X_i\}_{i=1}^\infty$ iid. What they do is that they introduce the substitution $s=\frac{\hat s}{Np_X(x)}$ , and says that this distribution is given by $$p(s)=\lim_{n\to \infty}\hat p_N(\hat s)=\lim_{N\to \infty}p\left(s=\frac{\hat s}{Np_X(x)}\right)\frac{ds}{d\hat s}=...=e^{\hat s}.$$ Question : I don't understand the argument here ! 1) First, why the scaling $\frac{\hat s}{Np_X(x)}$ ? 2) What does mean $\lim_{N\to \infty}p\left(s=\frac{\hat s}{Np_X(x)}\right)\frac{ds}{d\hat s}$ ? I guess it's $$\lim_{N\to \infty}p_N\left(\frac{\hat s}{Np_N(x)}\right)\frac{d}{d\hat s}\left(\frac{\hat s}{Np_X(x)}\right)=\lim_{N\to \infty}p_N\left(\frac{\hat s}{Np_N(x)}\right)\frac{1}{Np_X(x)},$$ but I've never see such a thing : why introducing the term $\frac{d s}{d\hat s}$ ?","['random-matrices', 'measure-theory', 'probability']"
2968838,$a^n-a + 1 $ divisible by $n$,"Problem . Given $a$ is a positive integer greater than 3, are there infinitely many positive integers $n$ satisfying $a^n-a + 1 $ divisible by $n$ ?","['number-theory', 'divisibility', 'elementary-number-theory']"
2968842,Help find the mistake in this problem of finding limit (using L'Hopital),"Evaluate $$\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right).$$ Attempt \begin{align*}
&\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right)\\
= &\lim_{x \to 0} \left(\frac{1}{x}-\cot{x}\right)\left(\frac{1}{x}+\cot{x}\right)\\
= &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right)\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right)\\
= &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right).
\end{align*} Both the terms are in $\frac00$ form. So applying L'Hopital on both the limits we have, $$= \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\sin{x}}{x\cos{x}+\sin{x}}\right).$$ The second term is in $\frac00$ form. So applying L'Hopital on the second limit we have, \begin{align*}
= &\lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\
=& \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\
=& 1
\end{align*} The correct answer is $\dfrac23$ which can be found using series expansion. But I think I'm making a conceptual mistake in one of the above steps. Could you please point out to the specific step where I've committed a mistake in above solution?",['limits']
2968844,Dimension of $W_{2}$?,"Let $A = \begin{bmatrix} 1 & -1 & -5 & 1 & 4\\ -1 & 2 & 8 & -3 & -4\\ 3 & -1 & -9 & 0 & 4 \\ 2 & 2 & 2 & -5 & -10\\ 0&-3&-9&5&13\end{bmatrix}$ Now we define the subspace $W_{1},W_{2}$ of $A$ as follows - $W_{1} = \{X \in M_{5 \times 5}| AX = 0\}$ $W_{2} = \{Y \in M_{5 \times 5} | YA =0\}$ I can see that $W_{1}$ is the nullspace of $A$ using rank nullity theorem I got Nullity of $A$ as $2$ since we have the rank of matrix $A$ to be 3. Now I am thinking about the dimension of $W_{2}$ ? As from the comments and we know that row rank = column rank, hence dim $(W_{2}) = 2$ But Now I am thinking about the dimension of $W_{1} \cap W_{2}$ and $W_{1} + W_{2}$ ? Any ideas?","['matrices', 'matrix-rank', 'linear-algebra']"
2968851,Functional equation of $\sum_{n=0}^\infty \chi(n)x^n$,"Denote $f(x)$ the analytic continuation of $\sum_{n=0}^\infty \chi(n)x^n$ , where $\chi$ denotes the Dirichlet character $\operatorname{mod} k(k>1)$ . Show that $$f^2(x)=\ f^2\left(\frac1x\right).$$ Attempt Knowing that $\chi$ has a period $k$ , $$f(x)=\sum_{m=0}^\infty\sum_{n=0}^{k-1}\chi(n)x^{mk+n}\\
=\sum_{n=0}^{k-1}\sum_{m=0}^\infty\chi(n)x^{mk+n}\\
=\sum_{n=0}^{k-1}\chi(n)\frac{x^n}{1-x^k}$$ $$f\left(\frac1x\right)=\sum_{n=0}^{k-1}\chi(n)\frac{x^{-n}}{1-x^{-k}}\\=-\sum_{n=0}^{k-1}\chi(n)\frac{x^{k-n}}{1-x^{k}}$$",['number-theory']
2968911,Tightness of a family of probability measures.,"Let $\mathscr{A}$ be a family of probability measures then this family is tight iff there exists a function $f\in C(\mathbb{R^n})$ such that $f(x)\to\infty$ as $|x|\to\infty$ and $$\sup_{\mu\in\mathscr{A}}\int f\,\mathrm{d}\mu<\infty.$$ I think the function needs to be nonnegative. If that is the case I can prove the tightness of the family but again I am unable to show the existence of such a function from the tightness. Any help will be appreciated.","['measure-theory', 'probability-theory']"
2968940,Elementary functions equivalent to the prime counting function,"Could there be an elementary function $f(x)$ such that $$ \pi(x) = f(x) $$ where $\pi(x)$ represents the number of the prime numbers up to $x$ ? (By elementary I mean, functions those are differantiable)","['functions', 'prime-numbers']"
2969016,Show that a closed curve formed by two disjoint paths contains the corner of a unit square?,"I am looking for a reference, or a topologically/analytically rigorous way of showing the following: Consider two injective paths in $\mathbb{R}^2$ parametrized as $p_1(t)$ , $p_2(t)$ , $0 \leq t \leq 1$ respectively, such that $p_1$ and $p_2$ are disjoint everywhere except at the endpoints (i.e. $p_1(x) = p_2(x)$ iff $x \in \{0,1\}$ ). Let $S$ be a unit square somewhere in the plane. Suppose that $p_1$ enters $S$ through one of its edges and exits through a different one, and that $p_1$ enters $S$ exactly once. Further suppose that $p_2$ never touches any point of $S$ . Show that the interior or boundary of the closed curve formed by $p_1$ and $p_2$ contains at least one corner of $S$ . This seems completely obvious to me, but I am not sure how one would show this rigorously. If it helps, one may assume $p_1$ and $p_2$ are composed of polygonal lines.","['plane-curves', 'general-topology', 'geometry']"
2969053,Can i determine if a function will increase in the future?,"Is it possible to determine whether a function will have increased in the future relative to starting points, given a sample of the first $m$ points? For example, given the 4 first values of a function $(f(1), f(2), f(3), f(4) )= (2, 1, 4, 5 )$ can I with some probability determine whether the $f(n)$ will be larger than all $f(1)$ to $f(4)$ . I apologize if this is too vague. Any answers appreciated EDIT:
Thanks for commenting. If i let my function be something like a ratio of how many white and black marbles i have in my collection, and i keep adding marbles (non-fair with no known probability), can i then pursue something?",['functions']
2969076,How to determine the number of possible combinations of letters that contain a degenerate substring,"I've been racking my brain for a couple of days to work out a series or closed-form equation to the following problem: Specifically: given all strings of length $N$ that draws from an alphabet of $L$ letters (starting with 'A', for example {A, B}, {A, B, C}, ...), how many of those strings contain a substring that matches the pattern: 'A', more than 1 not-'A', 'A'. The standard regular expression for that pattern would be A[^A][^A]+A . The number of possible strings is simple enough: $L^N$ . For small values of $N$ and $L$ , it's also very practical to simply create all possible combinations and use a regular expression to find the substrings that match the pattern; in R: all.combinations <- function(N, L) {
    apply(
        expand.grid(rep(list(LETTERS[1:L]), N)),
        1,
        paste,
        collapse = ''
    )
}

matching.pattern <- function(N, L, pattern = 'A[^A][^A]+A') {
    sum(grepl(pattern, all.combinations(N, L)))
}

all.combinations(4, 2)
matching.pattern(4, 2) I had come up with the following, which works for N < 7: $$M(N, L) = \sum_{g=2}^{N-2} (N - g - 1) \cdot (L - 1)^g \cdot L^{(N - g -2)}$$ Unfortunately, that only works while N < 7 because it's simply adding the combinations that have substrings A..A, A...A, A....A, etc. and some combinations obviously have multiple matching substrings (e.g., A..A..A, A..A...A), which are counted twice. For example: when N = 4 and L = 2 ({A, B}), there are $L^N = 16$ possible strings: AAAA, BAAA, ABAA, BBAA, AABA, BABA, ABBA, BBBA, AAAB, BAAB, ABAB, BBAB, AABB, BABB, ABBB, BBBB . Only one of those strings matches the pattern 'A', more than 1 not-'A', 'A': ABBA . If you are familiar with regular expression syntax, the pattern is expressed as A[^A][^A]+A . When N = 4, and L = 3 ({A, B, C}), there are $L^N = 81$ possible strings, and there are 4 strings that match the pattern: ABBA, ABCA, ACBA, ACCA . When N = 6, and L = 2, there are $L^N = 64$ combinations, and 17 strings that match the pattern: ABBAAA, AABBAA, BABBAA, ABBBAA, ABBABA, AAABBA, BAABBA, ABABBA, BBABBA, AABBBA, BABBBA, ABBBBA, ABBAAB, AABBAB, BABBAB, ABBBAB, ABBABB Any suggestions? For what it is worth, here's the number of combinations, and matching combinations for some values of N and L that are tractable to determine by generating all combinations and doing a regular expression match: N  L  combinations  matching
--  -  ------------  --------
 4  2            16         1
 5  2            32         5
 6  2            64        17
 7  2           128        48
 8  2           256       122
 9  2           512       290
10  2          1024       659
 4  3            81         4
 5  3           243        32
 6  3           729       172
 7  3          2187       760
 8  3          6561      2996
 9  3         19683     10960
10  3         59049     38076
 4  4           256         9
 5  4          1024        99
 6  4          4096       729
 7  4         16384      4410
 8  4         65536     23778
 9  4        262144    118854
10  4       1048576    563499","['combinatorics', 'probability']"
2969081,the expected amount of time that the 3:00 appointment spends at the doctor’s office.,"A doctor has scheduled two appointments, one at 2:00 P.M. and the other at 3:00 P.M. The amounts of time that appointments last are independent exponential random variables with mean 60 minutes. Assuming that both patients are on time, find the expected amount of time that the 3:00 appointment spends at the doctor’s office. I tried to find my own solution by using conditional expectation where : $T_1 (\text{time appointment 2:00 P.M leaves before 3:00 P.M})$ $T_2 (\text{time appointment 2:00 P.M leaves after 3:00 P.M})$ $\mathbb{E}(T_1)=\mathbb{E}(T_2)=60.$ My solution is: $$\mathbb{E}(\text{T spent time})=\mathbb{E}(T{|}T_1\le 60)\mathbb{P}(T_1\le 60)+\mathbb{E}(T{|}T_2\ge 60)\mathbb{P}(T_2\ge 60)$$ $$=\mathbb{E}(T_1)\int _0^{60}\:\frac{1}{60}e^{-\frac{1}{60}t_1}dt_1+\mathbb{E}(T_2)\int _{60}^{\infty }\frac{1}{60}e^{-\frac{1}{60}t_2}dt_2$$ $$=60(1-e^{-1})+60(e^{-1})=60.$$ I see this is wrong answer , and I'm not sure if $$\mathbb{E}(T{|}T_1\le 60)=\mathbb{E}(T_1)$$ $$\mathbb{E}(T{|}T_2\ge 60)=\mathbb{E}(T_2)$$","['exponential-distribution', 'stochastic-processes', 'statistics', 'conditional-expectation']"
2969086,Basic maths: Interview question getting proportion from averages,"It's probably simple but I was given this question in a video interview recently and I spent ages coming up with two different answers. (please let me know if this is the wrong place for this, I'm not entirely sure where is best suited for this type of question) Question as follows: A mobile app is on both iPhone and Android. Overall, there are 600,000
  app users who log into the app an average of 11 times a month. The
  average iPhone user logs in 7 times a month. The average Android user
  logs in 13 times a month. What proportion of users access the app with
  an iPhone? My two answers: Answer one $a$ : proportion of iphone users $$\frac{7}{20\cdot a} = 11$$ $$a=\frac{11 \cdot 20}{7}$$ $$=\frac{220}{7}$$ $$31.43% $$ Answer two Answer: $33.33\%$ Are either of these right or have I completely lost the plot?","['statistics', 'percentages', 'recreational-mathematics']"
2969098,Sum of two velocities is smaller than the speed of light,"Using the Lorentz transformation from special relativity, we get that the sum of two velocities can be expressed as $$u=\frac{u'+v}{1+\frac{u'v}{c^2}}.$$ Given that $|u'|,|v| \le c$ , I want to prove that $|u| \le c$ , ie. that the velocity never exceeds $c$ . However I am struggling to produce this bound. I have tried to bound the denominator from above but this produces zero and have tried a case wise approach but this has got me no where either.","['special-relativity', 'real-analysis']"
2969141,Evaluate $\lim\limits_{n \to \infty}\frac{n}{\sqrt[n]{n!}}$.,"Solution Notice that $$(\forall x \in \mathbb{R})~~e^x=1+x+\frac{x^2}{2!}+\cdots+\frac{x^n}{n!}+\cdots.$$ Let $x=n$ where $n\in \mathbb{N_+}$ . Then we obtain $$e^n=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\cdots>\frac{n^n}{n!}.$$ Thus, we obtain $$e>\frac{n}{\sqrt[n]{n!}}.\tag1$$ Moreover, we can find that, for $k=0,1,\cdots,n-1.$ $$\frac{n^k}{k!}< \frac{n^n}{n!}.$$ Thus \begin{align*}
e^n&=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)(n+2)}+\cdots\right]\\
&< (n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)^2}+\cdots\right]\\
&=(n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot n\\
&=(2n+1)\cdot \frac{n^n}{n!},
\end{align*} which shows that $$\frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}.\tag2$$ Combining $(1)$ and $(2)$ , we have $$e>\frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}\to e(n \to \infty).$$ By the squeeze theorem, $$\lim_{n \to \infty}\frac{n}{\sqrt[n]{n!}}=e.$$ Please correct me if I'm wrong. Many thanks.","['limits', 'proof-verification', 'sequences-and-series', 'real-analysis']"
2969153,$p$-th power roots of unity: why $p$ prime?,"Let $p$ be prime, and let $Z=\{z\in \Bbb C: z^{p^n}=1 \text{ for some } n\in\Bbb Z^+\}$ . I see that $Z$ is indeed group, but, I wonder if we can replace ' $p$ is prime' with ' $p$ any positive integer'? I proved $Z$ is indeed a group by showing that it is subgroup of group of roots of unity: $Z$ indeed contains identity, if $x,y$ are in $Z$ , then $x^{p^k}=1$ and $y^{p^l}=1$ for some $k,l\in\Bbb Z^+$ $(xy)^{\text{lcm} (p^k,p^l)}=1$ and $x^{-1}$ lies indeed in $Z$ . I thought that it was ${\text{lcm} (p^k,p^l)}=1$ where ' $p$ is prime' was needed, to see that ${\text{lcm} (p^k,p^l)}$ is indeed positive power of $p$ . But I was wrong. If taking $p$ is positive integer works, then why do we cherish the ' $p$ prime' version","['group-theory', 'abstract-algebra', 'complex-numbers']"
2969161,Can $n+1$ distinct positive vectors in $\mathbb{R}^n_{>0}$ agree on $n$ distinct weighted $p$-norms?,"Consider $n$ distinct positive numbers $\{p^{(1)},...,p^{(n)}\}\subset [1,\infty)$ along with weights $\{w^{(1)},...,w^{(n)}\}\subset\Delta^{n-1}$ and scalars $\{q^{(1)},...,q^{(n)}\}\subset(0,\infty)$ . Let $X$ be the set of positive vectors defined by the $n$ weighted weighted $p$ -norms, $$X=\left\{\vec x\in (0,\infty)^n\ \middle|\ \left(\sum_{i=1}^n w^{(k)}_i (x_i)^{p^{(k)}}\right)^{1/p^{(k)}}\!\!\!\!=q^{(k)}\text{ for all } k=1,...,n\right\}.$$ Is it true that $|X|\leq n$ ? Update: Connor points out that a simple permutation over components always maintains all $p$ -norms, so obviously the statement is wrong as written for $n\geq 3$ . But are there less `special' counterexamples? More formally, if you consider all the sets $X\subset (0,\infty)^n$ of cardinality $|X|=n+1$ that satisfy the definition above for some sets of $w$ , $p$ and $q$ , and you put them all into a `superset' $Y$ ... does $Y$ have zero Lebesgue measure as a subset of $((0,\infty)^n)^{n+1}$ ? My ultimate goal is to establish the following: If I take a random countable collection of positive $n$ -dimensional vectors (in the Lebesgue sense), then for almost all $p>0$ , at most $n$ of those vectors will lie in the same weighted $l_p$ -sphere (for any weights $w$ , and any radius $q$ ).","['systems-of-equations', 'normed-spaces', 'algebraic-geometry', 'lp-spaces', 'polynomials']"
2969163,Interchanging the order of summation in a double sequence,"I encountered the following change of summation $\sum_{i=1}^ \infty \sum_{j=1}^ \infty  a_{i,j} = \sum_{j=1}^ \infty \sum_{i=1}^ \infty  a_{i,j} $ , in Ahtreya and Lahiri's ""Measure theory and Probability theory"", page 16, where it says that this is allowed since the summands $a_{i,j}$ are non-negative. I have encountered the same argument in other books of Measure theory, but never saw a proof in the appendix neither a reference of it. From a web search I made, it seems that Fubini's theorem for double integrals in $R^n$ , or, for a measurable function on a product measure space, is proved, and resembles very much my question. However, I cannot find a proof exactly for double sequences, as is my original question. This is quite strange, since this should be considerably easier to prove. Can you kindly point me to one? Could you outline the proof, alternatively? Thanks a lot.","['analysis', 'sequences-and-series']"
2969209,Set is a sigma-algebra,"Let $A \subset \mathbb R$ be denoted as: $-A := \left\{-x : x \in A\right\}$ . Why is $E:= \{ A \subset \mathbb R : A = -A\}$ a $\sigma$ -algebra on $\mathbb R$ ? Here is my proof: the empty set belongs to $E$ because it holds true that $- \emptyset = \emptyset $ $\mathbb R \in E$ because $\mathbb R^c$ is the empty set and therefore belongs to $E$ if for $B \in E $ it holds that $B=-B$ , then $B=(B^c)^c = -(B^c)^c$ . Therefore, $B^c \in E$ if $E_n = -E_n$ for all $n$ , then $\cup E_n = - \cup E_n$ . If $E_n^c = - E_n^c$ for some m, then, since $E_m \subset \cup E_n$ , we have $(\cup E_n)^c \subset A_m^c$ and $-(\cup E_n)^c \subset E_m^c$ . Then $(\cup E_n)^c = -(\cup E_n)^c$ and, by my third point, $\cup E_n = - \cup E_n$ . Is this proof correct? Or what would you write in other terms / words? And how? EDIT: corrected proof: the empty set belongs to E since $\emptyset$ as well as $- \emptyset$ are in E by definition [is this ok?] $\mathbb R$ is in E since $\mathbb R$ as well as $- \mathbb R$ are in E by def. if $B \in E$ then: $B^c = f(B)^c = f(B^c) = -B^c$ --> E is closed under complements if $B_a \in E $ for all $a \in \mathbb N$ , then: $\cup B_a = \cup f(B_a) = f(\cup B_a) = - \cup B_a$ showing that E is closed under arbitrary unions. Is this acceptable? :)","['elementary-set-theory', 'measure-theory', 'real-analysis']"
2969249,Why can't we treat $\text{d}y/\text{d}x$ as a fraction? [duplicate],"This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) When can we not treat differentials as fractions? And when is it perfectly OK? (9 answers) Closed 5 years ago . Disclaimer - I'm not a mathematician, I'm a dirty physicist. My work often involves performing calculus on various things without thinking about what I'm doing too much (I leave the proof of various identities etc for the pure mathematicians to worry about). However I've often noticed that mathematicians get a little upset when I do tricks such as treating the differential $\frac{\text{d}y}{\text{d}x}$ as if it were a fraction. The simplest example I can think of is how I think about the chain rule: $$\frac{\text{d}y}{\text{d}x} = \frac{\text{d}y}{\text{d}u}\frac{\text{d}u}{\text{d}x}$$ In my head, I imagine the $\text{d}u$ terms cancelling, which is why this works.  Indeed, this is how I'll explain to others how to use the chain rule when asked about it. My question is the following: Is is dangerous to think about differentials in this way? After all, one of the very first examples of calculus I've ever seen (back when I was a baby in high-school) was the derivative of $y=x^2$ calculated from first principles in the following way: \begin{align}
y&=x^2\\
y+\delta y&=(x+\delta x)^2\\
y+\delta y&=x^2+2x\delta x+\delta x^2\\
\require{cancel}\cancel{x^2}+\delta y&=\cancel{x^2}+2x\delta x+\delta x^2\\
\delta y&=2x\delta x+\delta x^2\\
\frac{\delta y}{\delta x}&=2x+\delta x\\
\text{Now let }&\delta x\rightarrow0\text{ leaving}\\
\frac{\delta y}{\delta x}&=2x\\
\frac{\text{d}y}{\text{d}x}&=2x
\end{align} And to me, this is just treating $\frac{\delta y}{\delta x}$ as a fraction. I know that technically you're doing $0/0$ if you think about it, but are there any examples where treating $\text{d}y/\text{d}x$ is really inappropriate?","['fractions', 'derivatives']"
2969282,$ \begin{vmatrix} f(a) & g(a) & h(a) \\ f(b) & g(b) & h(b) \\ f'(c) & g'(c) & h'(c) \\ \end{vmatrix} =0 $,"If $f(x),g(x),h(x)$ have derivatives in $[a,b]$ , show that there exists a value $c$ of $x$ in $(a,b)$ such that $$
    \begin{vmatrix}
    f(a) & g(a) & h(a) \\
    f(b) & g(b) & h(b) \\
    f'(c) & g'(c) & h'(c) \\
    \end{vmatrix} =0
$$ I am getting an idea of using generalized mean value theorem, but not able to proceed. Need help!","['derivatives', 'analysis', 'real-analysis']"
2969303,How do I calculate the maximum velocity of a CSS Bezier curve?,"I tried to do calculate if it's possible to get the top velocity of a co-ordinate point on a CSS Bezier curve. Below is my working process. Calculate the top velocity point in a bezier curve (4 control points): A Bezier curve can be described using a mathematical formula. $B(t) = (1−t)³P₀ + 3(1−t)²tP₁ + 3(1−t)t²P₂ + t³P₃$ In CSS timing function, $P₀$ is $(0, 0)$ and represents the initial time and the initial state, $P₃$ is $(1, 1)$ and represents the final time and the final state. $P$ is a vector. In other words, we can put $x$ and $y$ instead of $P$ to get corresponding coordinates. $X = (1−t)³X₀ + 3(1−t)²tX₁ + 3(1−t)t²X₂ + t³X₃$ $Y = (1−t)³Y₀ + 3(1−t)²tY₁ + 3(1−t)t²Y₂ + t³Y₃$ Since $P₀$ is $(0, 0)$ and $P₃$ is $(1, 1)$ , $X = 3(1−t)²tX₁ + 3(1−t)t²X₂ + t³$ $Y = 3(1−t)²tY₁ + 3(1−t)t²Y₂ + t³$ If I customise my curve to use $P₁ (0.4, 0)$ and $P₃ (0.2, 1)$ , $P₁ = (0.4, 0) P₂ = (0.2, 1)$ $X = 1.6t³ - 1.8t² + 1.2t$ $Y = -2t³ + 3t²$ Calculate the rate of change of $Y$ , $dy/dt = -6t² + 6t$ $dy²/dt² = -12t + 6$ $-12t + 6 = 0$ I get $t = 0.5$ Does that make sense?","['cubics', 'bezier-curve', 'ordinary-differential-equations']"
2969329,Is the solution of every differential equation CONTINUOUS in it's given domain?,"I am a first reader of Differential Equation. I was solving a differential equation whose solution is $|f(x)|= c$ . Now my question is can I write that the solution is $f(x)= k$ . If $f(x)$ is continuous then I can  remove the mod. But I am not sure whether the function $f(x)$ is continuous or not. (Here $c$ and $k$ are constants) So that's why my question is the following.
 Is the solution of every differential equation CONTINUOUS  in it's given domain? Can anyone please help me to understand this?","['continuity', 'ordinary-differential-equations', 'real-analysis']"
2969346,Example 9.6 Tu text on manifolds,"I am trying to understand example 9.6 from Tu text on manifolds. The problem is trying to expresss $S^2 = \{(x,y,z) \in \mathbb{R}^3 | x^2 + y^2 + z^2 = 1\}$ as a zero set. Thus, we define the equation $f(x,y,z) = x^2 + y^2+z^2-1=0$ and then clearly, $S^2 = f^{-1}(\{0\})$ . This is the part I don't quite get, the text says: since $\frac{\partial f}{\partial x} = 2x$ , $\frac{\partial f}{\partial y} = 2y$ , $\frac{\partial f}{\partial z} = 2z$ , the only critical point of $f$ is $(0,0,0)$ . I know that a critical point $p$ is a point where the differential of a map fails to be surjective. But I really see no connection between this definition and the quick conclusion that TU arrived at. The way I am trying to understand this is as follows: since $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ , then the differential of $f$ , will also be a map from a $3$ dimensional vector space to a 1 dimensional vector space (well the tangent spaces). Thus, the only way this map has rank less than 1 (and so would not be a surjective map because the map doesn't have maximal rank) happens when we consider the point $(0,0,0)$ . Thus, the point $(0,0,0)$ is the only critical point. Is this reasoning correct for this type of problems or how can it be improved/made correct? Thanks for your help!","['differential-topology', 'differential-geometry']"
2969360,Why is $f(n) = n + 6 \mod{1729}$ not injective?,"The exercise was to determine if the function $f : \mathbb{Z} \rightarrow \mathbb{Z}_{1729}, f(n) = n + 6 \mod{1729}$ is injective or not. My thinking was that $f(-7) = 1728\\ 
f(-6) = 0\\ 
f(-5) = 1\\
\vdots\\
f(0) = 6\\ 
f(1) = 7\\ 
\vdots\\
f(1722)=1728\\ 
f(1723) = 0$ and so on, we will get all numbers in $\mathbb{Z}_{1729}$ . However, this turned out to be wrong, but the solution doesn't say why, though. Why is this function not injective?","['elementary-number-theory', 'functions', 'discrete-mathematics']"
2969372,Simple example of product not preserving coequaliser in $\mathbf{Top}$,"In the category of topological spaces ( $\mathbf{Top}$ ), products do not always preserve colimits. If they did then $\mathrm{Hom}_\mathbf{Top}(-\times X,S)$ would be representable and hence $\mathbf{Top}$ would be Cartesian closed (which it isn't ). I think that products do preserve coproducts, so it must be that there's some coequaliser which products don't preserve. I'm trying to understand why this is in more concrete terms, but I've struggled to find a simple example that I can examine in detail. What are some simple spaces $A$ , $B$ and $X$ and maps $f,g:A\to B$ in $\mathbf{Top}$ such that the product of $X$ with the coequaliser is different from the coequaliser of the products? The same question for the category of locales is here .","['general-topology', 'limits-colimits', 'category-theory']"
2969453,First order deformation isomorphic to the tangent space of moduli?,"Let $\mathcal M$ be the moduli space of all smooth hypersurface of degree $d$ and dimension $n$ . Is it true that $$T_{[X]} \mathcal M \cong Def(X):=H^1(X,T_X)$$ holds for any such hypersurface $X$ ? Any reference about this would be helpful.","['algebraic-geometry', 'deformation-theory', 'moduli-space', 'reference-request']"
2969482,Examples of functions that are not Borel measurable that will be accessible to 1st year undergraduates,"While teaching statistics, I've grown sick of using terms such as ""under (very) weak regulatory conditions..."" and would like to be able to provide some solid examples of when these conditions are not met. For example, it's well known to any stats student that for a continuous random variable X and for a ""sufficiently nice"" function $g:\mathbb{R} \to \mathbb{R}$ , we have E[ $g$ (X)]= $\int_{-\infty}^\infty g(x)f_X(x) \ \mathrm{dx}$ . However, any stats student who knows their measure theory will know that, in this case, ""sufficiently nice"" means 'Borel Measurable'. This leads me to the gist of my question. Are there any examples of non-Borel measurable functions that are simple enough to be explainable to someone who may have only just learned (or may not even know) set theory but are general enough to stress that the condition of 'g must be Borel measurable' is so weak that they won't really need to worry about it?","['education', 'borel-sets', 'statistics', 'measure-theory']"
2969575,Direct proof of sequential characterization of limits,In my studies of real analysis the proof of sequential characterization of limits is done by contradiction. The statement is $\lim\limits_{x\to c}f(x)$ iff for every sequence $\{x_n\}\subseteq\mathrm{dom}(f)\setminus\{c\}$ such that $\{x_n\}\to c$ then $\{f(x_n)\}\to L$ . Typically proving the “if every sequence $\{x_n\}$ ... then $\lim\limits_{x\to c}f(x)=L$ ” is done by supposing $\lim\limits_{x\to c}f(x)\neq L$ . I don’t really like this proof by contradiction so I’m wondering if there’s a better direct proof out there.,"['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
2969592,Request help verifying the Taylor series expansion of $\text{ln}(1 + e^{2ix})$,"To Prove: $\;\text{ln}(1 + e^{2ix})\; =\; \sum_{k=0}^{\infty}\dfrac{(-1)^k}{k+1}\;e^{2ix(k+1)}$ My Work: I have worked through ""Calculus Volume 1, 2nd Ed."", 1966, by Tom Apostol.  I am familiar with Taylor series expansions applied to the real variable $x.$ In particular, I understand that when $\;|x|<1, \;\dfrac{1}{x+1} = 1 - x + x^2 - x^3 + \cdots,\;$ with $\;x^n \rightarrow 0\;$ as $\;x \rightarrow \infty.$ From this I can say that when $\;|x|<1,\; \text{ln}(1+x) \;= \;\int_0^x \dfrac{1}{1+t}dt 
\;= \int_0^x (1 - t + t^2 - t^3 + \cdots) dt$ $= x - \dfrac{x^2}{2} + \dfrac{x^3}{3} - \dfrac{x^4}{4} +\cdots
\;= \sum_{k=0}^{\infty}\dfrac{(-1)^k}{k+1} \;x^{(k+1)}.$ I know little about complex analysis.  My intuition suggests that when complex $\;|z|<1,\;$ the same reasoning holds, so that $\;\text{ln}(1+z) \;= \;\sum_{k=0}^{\infty}\dfrac{(-1)^k}{k+1}\;z^{(k+1)}.\;$ However, $|e^{2ix}| \;= \;|\text{cos}(2x) + i\times \text{sin}(2x)| \;= \;\sqrt{\text{cos}^2(2x) + \text{sin}^2(2x)} \;= \;1.\;$ Therefore, when complex $\;z = e^{2ix},\;$ my reasoning does not hold. My Real Question: Why does the formula hold even though $\;|e^{2ix}| \;= \;1?\;$ I recognize that
for an in-depth understanding, I'll need to attack complex analysis from the 
ground up, via a book.  However, all I'm looking for is an elementary 
(?over-simplified?) explanation that covers this particular issue. My Research: Searching on this forum for ""complex taylor"" showed many articles.  I browsed a few of them but did not find any questions/comments/answers that dealt with my issue. My Attempt To Answer The Question: It just occured to me that the case of complex $\;|z| = 1\;$ needs special handling.  In this case, it could be argued that if $\;z\neq -1,\;$ then it becomes irrelevant that $\;z^n\;$ does not approach zero, since (in the formula), $\;\frac{z^{k+1}}{k+1} \rightarrow 0\;$ as $\;k\rightarrow \infty.\;$ I am on very shaky ground here and would like a math pro to weigh in.","['complex-analysis', 'taylor-expansion']"
2969596,Find the value of $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}$,"Suppose $(a, b, c)\in\Bbb R^3$ , $a,b,c$ are all nonzero, and we have $
\sqrt{a+b}+\sqrt{b+c}=\sqrt{c+a}$ . Find the value of $$\frac{1}{a}+\frac{1}{b}+\frac{1}{c}.$$ Here is my attempt: $$\frac{1}{a}+\frac{1}{b}+\frac{1}{c}=\frac{bc+ac+ab}{abc}.$$ I am having trouble in figuring out the best approach to simplify $
\sqrt{a+b}+\sqrt{b+c}=\sqrt{c+a}$ so that I can find the value of $
\frac{1}{a}+\frac{1}{b}+\frac{1}{c}$ . Hope somebody has an idea.","['contest-math', 'algebra-precalculus']"
2969633,Solve the following autonomous ODE,"Show that the following ode has a maximum of one solution. $$x'(t) = |x(t)| +\sin(x^2(t)+t^2) \quad \text{with} \quad  x(0)=0$$ and that this solution satisfy $x(t)\leq  e^t-1$ for $t \geq 0$ We got the hint: compare with $x'(t)= |x(t)|+1$ I solved the ODE of the hint and got 
for $x(t) \geq 0 $ the solution $ x(t)=c_{1}e^t-1  ,c_{1} \in \mathbb{R}$ for $x(t) < 0 $ the solution $ x(t)=c_{2}e^{-t}+1  ,c_{2} \in \mathbb{R}$ Now  I´m not sure what to do. I know that if $c_{1}=1$ I get $x(t)\leq  e^t-1$ for $t \geq 0$ . What have i to do next? I have problems to show Lipschitz-continuity, because i cant find an L. I need some help.",['ordinary-differential-equations']
2969649,Translating Predicate Logic to English,"$E(x,y)$ : $x$ can eat $y$ $L(x,y)$ : $x$ loves eating $y$ $D$ is the domain of all dogs $S$ is the domain of all snakes Predicate Logic to English: $\forall a \in S,\sim \ \bigg[   \exists c \in S,\ a\ \ne  c  \ \wedge E(a,c)\bigg] \iff \forall b \in D, \ L(a,b)$ : All snakes, a, cannot eat any other snakes , if and only if, all snakes, a, loves eating all dogs. Any thoughts on if this is accurate and/or a way to condense this statment?","['boolean-algebra', 'predicate-logic', 'logic', 'discrete-mathematics', 'logic-translation']"
2969665,Is every measure $0$ set a set of discontinuities of a Riemann integrable function?,"Let $f:[a,b]\rightarrow\mathbb{R}$ be bounded, and let $D$ be its set of discontinuities.  Then Lebesgue's criterion states that $f$ is Riemann-integrable if and only if $D$ has Lebesgue measure $0$ . My question is, for any subset $D$ of $[a,b]$ with Lebesgue measure $0$ , does there exist a Riemann integrable function $f:[a,b]\rightarrow\mathbb{R}$ whose set of discontinuities is $D$ ?  Would the characteristic function of $D$ suffice, or is more complicated than that?","['measure-theory', 'lebesgue-measure', 'real-analysis', 'continuity', 'riemann-integration']"
2969682,Help understanding the complex matrix representation of quaternions,"Using the basis $ B = \{1, j\}$ , one can show that quaternions can be represented by 2x2 complex matrices as follows: \begin{pmatrix}
z & w\\
-\bar{w} & \bar{z}\\
\end{pmatrix} I would like some help to understand this. Lets say $z = a + bi, w = c + di$ Then we can represent the quaternion $h = a + bi + cj + dk$ as $z + wj$ . I would have thought that to find the matrix representation of complex numbers we would see what would happen if we multiply $z + wj$ by the basis elements. This would mean the first column of our matrix would be $(z + wj)(1) = z + wj = 
         \begin{bmatrix}
           z \\
           w \\
         \end{bmatrix}$ And the second column would be $(z + wj)(j) = zj + wj^{2} = zj - w =          \begin{bmatrix}
           -w \\
           z \\
         \end{bmatrix}$ So I would have thought the matrix representation would be \begin{pmatrix}
z & -w\\
w & z\\
\end{pmatrix} I have a feeling I have some large misunderstanding about what I'm doing, I'm just following the same approach I did to find matrix representations of complex numbers with real 2x2 matrices and matrix representations of quaternions with real 4x4 matrices. e.g. with complex numbers, using a basis of $B = \{1, i\}$ , and a complex number $a + bi$ where $a$ and $b $ are : $(a + bi)(1) = a+ bi = \begin{bmatrix}
           a \\
           b \\
         \end{bmatrix}$ $(a + bi)(i) = ai -b = \begin{bmatrix}
           -b \\
           a \\
         \end{bmatrix}$ Which gives us the matrix \begin{pmatrix}
a & -b\\
b & a\\
\end{pmatrix} Which is correct. The same approach worked for me for quaternions and real 4x4 matrices.","['vectors', 'matrices', 'linear-algebra', 'complex-numbers', 'quaternions']"
2969749,Surjectivity of the sum of two bounded operators,"Let $H$ be a Hilbert space. Let $A\in B\left( H\right) $ be surjective, and let $Q\in B\left( H\right) $ be quasi-nilpotent such that $AQ=QA$ Prove that $A+Q$ is surjective. Thank you.","['operator-theory', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
2969828,Is the empty function differentiable?,"Given the empty function $\varnothing: \emptyset \to X$ where $X$ is a set, is $\varnothing$ a differentiable function? If so, what is its derivative? Also, is the empty set a differentiable manifold? If so, what is its dimension?","['calculus', 'functions', 'differential-topology', 'real-analysis']"
2969882,Smoothness of a parametric curve,"Suppose that f(t) and g(t) are differentiable on [a, b]. What can be said about the smoothness of the curve parameterized by x = f(t) and y = g(t) on [a, b]? (Smooth here is used in the sense of having no corners or cusps.) A) It must be smooth
B) It might or might not be smooth.
C) It cannot be smooth. How is the answer B? I thought it would always be smooth when you differentiate a function. Can someone explain?",['multivariable-calculus']
2969907,Understanding definite integrals with functions as bounds [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I can't quite grasp the meaning behind definite integrals defined on two bounds, which appear as functions. 
For instance, $$\int_{x^2}^{\cos x}t^2dt$$ What is this notation telling me? What does it mean that the lower bound is $x^2$ , and the upper bound is $\cos x$ ? Where does the definite integral ""stop"" and ""end"", if $x^2$ and $\cos x$ are not single values, but a collection of values? Wouldn't these $x$ values then overlap...? Moreover, when I wish to take the derivative of such an integral, how do I know that $0$ (or any specified constant of integration for that matter) exists ""between"" $x^2$ and $\cos x$ ? Very confused, my apologies. Thanks for any clarification you can provide.",['derivatives']
2969942,Laplace Transform of Lambert W function,"Does there exist a Laplace transform of $W(at)$ that can be expressed in terms of elementary functions and the Lambert W function $W(x)$ ? If such a transform exists, how is it expressed in terms of $s$ , and how is it derived? A couple properties of the productlog $W(x)$ to note include: $$
e^{W(x)}=\frac{x}{W(x)}\\
W'(x)=\frac{W(x)}{x\left(1+W(x)\right)}$$ and $$\int W(ax)=x(W(ax)−1)+\frac{1}{a}e^{W(ax)}+C.$$","['integration', 'improper-integrals', 'laplace-transform', 'lambert-w']"
2969961,Is the set rational points on the unit circle isomorphic to $\mathbb{Q}$ as affine varieties?,"I will denote by $X$ the set of rational points on the unit circle, i.e., $$X := \{ (x,y) \in \mathbb{Q}^2: x^2 + y^2 = 1 \}.$$ Viewing both $\mathbb{Q}$ and $X$ as an affine varieties, then every morphism $f: \mathbb{Q} \to X$ is given by two (global) regular functions on $\mathbb{Q}$ . To be more specific, $$f(t) = \left(f_1(t),f_2(t)\right)$$ such that both $f_1$ and $f_2$ are rational functions of the form $\frac gh$ , where $g$ and $h$ are polynomials on $\mathbb{Q}$ , and $h$ never vanishes on $\mathbb{Q}$ . My question is: does there exist an isomorphism $f: \mathbb{Q} \to X$ ? On one hand, I know that the rational parametrization of the unit circle $$ t \mapsto \left( \frac{1-t^2}{1+t^2}, \frac{2t}{1+t^2} \right) $$ is a morphism from $\mathbb{Q}$ to $X$ , but it's not an isomorphism. However, this doesn't mean there cannot be an isomorphism $f: \mathbb{Q} \to X$ . On the other hand, if we equip $\mathbb{Q}$ and $X$ with the usual topologies (i.e., the subspace topologies from $\mathbb{R}$ and $\mathbb{R}^2$ , respectively), then $X$ is a countable metrizable space without isolated points, and hence homeomorphic to $\mathbb{Q}$ , by a theorem of Sierpinski whose proof can be found here . So my question is indeed asking whether a homeomorphism $f: \mathbb{Q} \to X$ can be in the form of two rational functions $f_1$ and $f_2$ as mentioned above.",['algebraic-geometry']
2970014,Show that $z_n$ is a perfect square if $z_0 = z_1 = 1$ and $z_{n+1} = 7z_n âˆ’ z_{nâˆ’1} âˆ’ 2$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $z_0 = z_1 = 1$ and $$z_{n+1} = 7z_n âˆ’ z_{nâˆ’1} âˆ’ 2$$ for all positive integers $n$ . How is it possible to show that $z_n$ is a perfect square for all $n$ ?",['combinatorics']
2970021,Alternating Sequence of Conditional Expectations,"Suppose that we have a sequence of random variables ${X_n}, n \in \mathbb{N}_0$ .  Suppose we have two sub-σ-fields that are not nested, $\mathcal{G}_1$ and $\mathcal{G}_2$ .  We have some random variable $X$ and define this sequence as, $X_0 = X$ $X_{k+1} = \mathbb{E}[X_{k}|\mathcal{G}_1]$ if $k$ is even $X_{k+1} = \mathbb{E}[X_{k}|\mathcal{G}_2]$ if $k$ is odd For visualization here is the 4th and 5th element of the sequence, $X_4 = \mathbb{E}[\mathbb{E}[\mathbb{E}[\mathbb{E}[X|\mathcal{G}_1]|\mathcal{G}_2]|\mathcal{G}_1]|\mathcal{G}_2]$ $X_5 = \mathbb{E}[\mathbb{E}[\mathbb{E}[\mathbb{E}[\mathbb{E}[X|\mathcal{G}_1]|\mathcal{G}_2]|\mathcal{G}_1]|\mathcal{G}_2]\mathcal{G}_1]$ Is there anything we can say about the convergence of such a sequence?  How about required conditions when this converges?
I'm quite unfamiliar with martingales and the such, but if at least one has resources that they can recommend for me to understand problems specifically of this type I would be happy as I recently am dealing with quite a few problems of this type.","['measure-theory', 'conditional-probability', 'conditional-expectation', 'stochastic-processes', 'martingales']"
2970024,Permutation representation: why is G isomorphic to a subgroup of the normalizer of $(1\:\ldots\:q)$ in $S_q$?,"This question appears in Dummit and Foote, which I am self-studying.  It asks the student to show that, given a non-abelian group $G$ with $|G| = pq$ where $p$ and $q$ are prime and $p < q$ , there exists a nonnormal subgroup $H \leq G$ with $|G:H| = q$ , and that this guarantees the existence of an injective homomorphism $\phi:G \to S_q$ (i.e., the permutation representation associated with the action of left multiplying left cosets of $H$ by elements of $G$ ).  I have managed to accomplish this, albeit with a slightly different argument than the one given in the linked question. There is, however, a final part to the question: we are asked to deduce that $G$ is isomorphic to a subgroup of the normalizer of $(1\:\ldots\:q)$ in $S_q$ .  Here I am stuck, which leads me to believe that there is something I am failing to understand about permutation representations generally. We certainly know that $G$ is isomorphic to some subgroup of $S_q$ , as $\ker \phi = 1$ and left multiplication by $G$ is permuting $q$ elements; we further know that this action is transitive--that is, given any two left cosets $aH, bH$ of $H$ , there exists $g \in G$ such that $g\cdot aH = bH$ . But I do not understand where the cycle $(1\:\ldots\:q)$ enters the picture; I therefore do not understand why its normalizer makes an appearance.  I'm not sure I need a complete solution--I'm more interested in a gentle nudge toward understanding the situation, so that I can work out the details myself.",['group-theory']
2970026,Partial derivative of $\sin(xz)$: Is this making sense and is it a proper use of notation?,"The exercise here is to calculate $\dfrac{\partial}{\partial x} \sin(xz)$ The usual way is just considering $z$ as a constant and deriving, so it would be $z\cos(xz)$ . However, I want to solve it this way:
Define $a=xz$ , then $\sin(a) = \sin(xz)$ $\dfrac{\partial}{\partial x} \sin(a) = \dfrac{\partial}{\partial a} \sin(a) \dfrac{\partial}{\partial x} xz = z\cos(a) = z\cos(xz)$ . Is this second way correct? My friend said it is not making sense and not a proper use of notation. But I do not understant why...","['partial-derivative', 'multivariable-calculus', 'calculus']"
2970039,What sets are obtained by adding $\aleph_1$ unions and intersections to the Borel algebra?,"The Borel sigma algebra on $\mathbb{R}$ is obtained by starting with open sets in $\mathbb{R}$ and repeatedly applying the operations of complement, countable union, and countable intersections.  My question is, what would happen if you don't just allow countable unions and intersections, but also allow unions and intersections of $\aleph_1$ many sets? Now obviously if we assume the Continuum Hypothesis, the answer is ""we obtain all subsets of $\mathbb{R}$ "". But we just assume $ZFC$ , then what sets do we know we can obtain?  Can we at least obtain all Lebesgue measurable sets?  Can we still obtain all subsets of $\mathbb{R}$ ?","['borel-sets', 'measure-theory', 'descriptive-set-theory', 'set-theory']"
2970046,Is the Borel sigma algebra complete under some measure?,"A measure space $(X,F,\mu)$ is complete if whenever $\mu(E)=0$ , every subset of $E$ is an element of $F$ .  Every incomplete measure space has a completion , which extends the sigma algebra to more sets and extends the measure to those sets. Most famously, the Borel algebra on $\mathbb{R}$ is incomplete with respect to Lebesgue measure, but it can be extended to a bigger sigma algebra, the Lebesgue signma algebra, which is complete with respect to Lebesgue measure. But question is, does there exist a measure under which the Borel sigma algebra on $\mathbb{R}$ is complete?  Or is it incomplete under every measure?","['measure-theory', 'lebesgue-measure', 'borel-measures', 'real-analysis', 'borel-sets']"
2970083,How to convert the normal vector of a point in some polygonal surface transformed onto the surface of a triangle in space?,"It's complicated to explain what the transformation I'm dealing with is but basically it is a transformation that takes a 3D model and maps it to the surface of one polygon on another model. The issue is that I'd prefer to not have to recompute the normal vectors of the post-transformation model at construction time (which are needed for lighting). There are actually two versions of the transform. I'm not sure which I will pick so ways to convert normals for each will be needed. First transform: Given a point $v = (x,y,z)$ in space and the triangle $(p_1, p_2, p_3)$ in space with point normals $(N_1, N_2, N_3)$ the transformation is $\Omega v = p_1x + p_2y + p_3(1 - x - y) + z(N_1x + N_2y + N_3(1 - x - y))$ . Second transform: Given a point $v = (x,y,z)$ in space and the triangle $(p_1, p_2, p_3)$ in space with point normals $(N_1, N_2, N_3)$ and triangle normal $N_t$ the transformation is $\Omega v = p_1x + p_2y + p_3(1 - x - y) + z(N_1x + N_2y + N_3(1 - x - y)) \frac {1}{<N_t,N_1x + N_2y + N_3(1 - x - y)>}$ . If $v$ has some normal vector associated with it, then what would it map to geometrically in the case of each transform? Note: $<a,b>$ is the dot product between $a$ and $b$ .","['transformation', 'geometry']"
2970091,When is a scheme topologically simply connected?,"Let $X$ be a path connected scheme (we have an argument that this is implied by connected and Noetherian). When is the topological fundamental group of $X$ trivial? This is straightforward for certain special cases like that of an integral scheme, but we haven't been able to come up with a more general characterization.","['algebraic-geometry', 'algebraic-topology']"
2970305,"Is it true that if the set $A=\{x:f(x) =c\}$ is measurable for every $c$ in $\Bbb R$, then $f$ is measurable?","Let $f: [0;1] \to \Bbb R$ . Is it true that if the set $A=\{x:f(x) =c\}$ is measurable for every $c$ in $\Bbb R$ , then $f$ is measurable? I have this counter example $f(x) =x$ if $x$ belong to $P$ , $f(x) =-x$ otherwise, where $P$ is a non measurable set on $[0,1]$ . $A$ is measurable but $f$ is not measurable function. I had proved that $f$ is not measurable, but how can I prove that $A$ is measurable and what is $A$ in this example ?","['measure-theory', 'examples-counterexamples']"
2970381,How to say $x=f(y)$ is similar to $y=g(x)$?,"I am writing a statement for a model that: The relationship between two independent variables $V_{1}$ and $V_{2}$ , a linear model that accounts for errors in from both $x-$ and the $y-$ axis must be defined, such that the model satisfies both $V_1(V_2)$ and $V_2(V_1)$ relationships. Here, I am using Deming regression where it considers errors from both axes, instead of only in $y-$ axis in OLS. My intention is to so that it does not matter which variable will be the dependent or predictor variables. In this case, is $V_1(V_2)$ and $V_2(V_1)$ correct? or is there's another way e.g. $ f(x, y) = f(y, x)$ ?",['functions']
2970388,(non)Zero elements in an SPD matrix A lead to (non)zero elements in L where L is the Cholesky decomposition matrix.,"Given an SPD matrix $A$ and Cholesky decomposition $A = LL^T$ . Prove that $A(i,j) = 0$ means that $L(i,j) = 0$ and that $A(i,j) \neq 0$ means that $L(i,j) \neq 0$ I've been messing around with the formulas that determine the elements of the L matrix for too long now. I thought about proving this through induction and through a contradiction but nothing worked. Any help is appreciated.","['matrices', 'matrix-equations', 'linear-algebra']"
2970455,How to calculate the geodesics in polar coordinates?,"I know that: $$
\ddot{r}^2 = r\dot{\phi}^2
$$ and $$
\ddot{\phi}=-\frac{2}{r}\dot{r}\dot{\phi}
$$ and I know that using this I should be able to get the geodesic equations, but for my life I just can't, I keep trying but I end up with things I cannot solve. I tried getting $r$ from the first equation and replacing it in the second, getting $\dot{\phi}$ from the second and replacing it in the first, I even found that: $$
r^2\dot{\phi} = constant
$$ but that didn't help at all, I am completely stuck and I can't find any derivation of the geodesic equations anywhere in the internet, please help I want to understand","['general-relativity', 'ordinary-differential-equations', 'differential-geometry']"
2970464,Non-unique solution of first order PDE,"Question: $$\frac{\partial u}{\partial x} \frac{\partial u}{\partial y}=1 \qquad \qquad u=0 \; \text{ when } \; x+y=1$$ Find all possible solutions and state where each one exists. Attempt: Using the method of characteristics (Charpit's equations), we end up with $$x=s \pm t \;,\; y=1-s \pm t \;,\; \frac{\partial u}{\partial x} = \pm 1 \;,\; \frac{\partial u}{\partial y}= \pm 1 \;,\; u=2t$$ The set of all solutions is then $$u(x,y) = \pm(x+y-1)$$ However, how do you know where these solution does/doesn't ""exist""? There seems to be no problems as far as the characteristic projections are concerned (they are a set of straight lines perpendicular to the initial data), and the Jacobian is $J= \pm 2$ which is non-zero. So is $u$ just supposed to be a multi-valued function in this case?","['cauchy-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
2970481,Explanation of Proof of Second-Derivative Test for Local Extrema,"My textbook introduces the following theorem: Theorem 5 Second-Derivative Test for Local Extrema If $f : U \subset \mathbb{R}^n \to \mathbb{R}$ is of class $C^3$ , $\mathbf{x}_0 \in U$ is a critical point of $f$ , and the Hessian $Hf(\mathbf{x}_0)$ is positive-definite, then $\mathbf{x}_0$ is a relative minimum of $f$ . Similarly, if $Hf(\mathbf{x}_0)$ is negative-definite, then $\mathbf{x}_0$ is a relative maximum. It then goes on to say the following: Actually, we shall prove that the extrema given by this criterion are strict. A relative maximum $\mathbf{x}_0$ is said to be strict if $f(\mathbf{x}) < f(\mathbf{x}_0)$ for nearby $\mathbf{x} \not= \mathbf{x}_0$ . A strict relative minimum is defined similarly. Also, the theorem is valid even if $f$ is only $C^2$ , but we have assumed $C^3$ for simplicity. The proof of theorem $5$ requires Taylor's theorem and the following result from linear algebra. Lemma 1 If $B = [b_{ij}]$ is an $n \times n$ real matrix, and if the associated quadratic function $$H: \mathbb{R}^n \to \mathbb{R}, (h_1, \dots, h_n) \mapsto \dfrac{1}{2} \sum_{i, j = 1}^n b_{ij} h_i h_j$$ is positive-definite, then there is a constant $M > 0$ such that for all $\mathbf{h} \in \mathbb{R}^n$ ; $$H(\mathbf{h}) \ge M || \mathbf{h} ||^2.$$ The proof of theorem 5 is as follows: proof of theorem 5 Recall that if $f: U \subset \mathbb{R}^n \to \mathbb{R}$ is of class $C^3$ and $\mathbf{x}_0 \in U$ is a critical point, Taylor's theorem may be expressed in the form $$f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) = Hf(\mathbf{x}_0)(\mathbf{h}) + R_2(\mathbf{x}_0, \mathbf{h}),$$ where $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ . Because $Hf(\mathbf{x}_0)$ is positive-definite, Lemma 1 assures us of a constant $M > 0$ such that for all $\mathbf{h} \in \mathbb{R}^n$ $$Hf(\mathbf{x}_0)(\mathbf{h}) \ge M || \mathbf{h} ||^2.$$ Because $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ , there is $\delta > 0$ such that for $0 < || \mathbf{h} || < \delta$ $$| R_2(\mathbf{x}_0, \mathbf{h}) | < M || \mathbf{h} ||^2.$$ Thus, $0 < Hf(\mathbf{x}_0)(\mathbf{h}) + \mathbf{R}_2 ( \mathbf{x}_0, \mathbf{h}) = f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0)$ for $0 < || \mathbf{h} || < \delta$ , so that $\mathbf{x}_0$ is a relative minimum; in fact, a strict relative minimum. The proof in the negative-definite case is similar, or else follows by applying the preceding to $-f$ , and is left as an exercise. The problem I'm having with this proof is that, although I managed to follow it, I don't see how it specifically says/demonstrates anything about relative minimums or strict relative minimums. I would greatly appreciate it if people could please take the time to explain/clarify this. EDIT: For the sake of clarity, I will also include the following information: Theorem 3 Second-Order Taylor Formula Let $f: U \subset \mathbb{R}^n \to \mathbb{R}$ have continuous partial derivatives of third order. Then we may write $$f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + \sum_{i = 1}^n h_i \dfrac{\partial{f}}{\partial{x_i}}(\mathbf{x}_0) + \dfrac{1}{2} \sum_{i, j = 1}^n h_i h_j \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) + R_2(\mathbf{x}_0, \mathbf{h}),$$ where $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ and the second sum is over all $i$ 's and $j$ 's between $1$ and $n$ (so there are $n^2$ terms). Suppose that $f: U \subset \mathbb{R}^n \to \mathbb{R}$ has second-order continuous derivatives $\dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0)$ , for $i, j = 1, \dots, n$ , at a points $\mathbf{x}_0 \in U$ . The Hessian of $f$ at $\mathbf{x}_0$ is the quadratic function defined by \begin{align} Hf(\mathbf{x}_0)(\mathbf{h}) &= \dfrac{1}{2} \sum_{i, j = 1}^n \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) h_i h_j \\ &= \dfrac{1}{2} [h_1, \dots, h_n] \left[\begin{matrix}\frac{\partial^2 f}{\partial x_1^2} & \ldots & \frac{\partial^2 f}{\partial x_1\partial x_n}\\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n\partial x_1}& \ldots & \frac{\partial^2 f}{\partial x_n^2}\end{matrix}\right] \left[\begin{matrix} h_1 \\ \vdots \\ h_n
\end{matrix}\right] \end{align} A quadratic function $g: \mathbb{R}^n \to \mathbb{R}$ is called positive-definite if $g(\mathbf{h}) \ge 0$ for all $\mathbf{h} \in \mathbb{R}^n$ and $g(\mathbf{h}) = 0$ only for $\mathbf{h} = \mathbf{0}$ . Similarly, $g$ is negative-definite if $g(\mathbf{h}) \le 0$ and $g(\mathbf{h}) = 0$ for $\mathbf{h} = \mathbf{0}$ only.","['proof-explanation', 'real-analysis', 'multivariable-calculus', 'taylor-expansion', 'optimization']"
2970537,Conversion of a Vector in a Cartesian Coordinate System to a Cylindrical Coordinate System,"I'm having trouble converting a vector from the Cartesian coordinate system to the cylindrical coordinate system (second year vector calculus) Represent the vector $\mathbf A(x,y,z) = z\ \hat i - 2x\ \hat j + y\ \hat k $ in cylindrical coordinates by writing it in the form $$\mathbf A(\rho, \phi, z) = A_1(\rho, \phi, z){\hat \rho} + A_2(\rho, \phi, z){\hat \phi} + A_3(\rho, \phi, z){\hat z}$$ I know that $$x=\rho\cos(\phi) \ \ ,  \ \ y = \rho\sin(\phi) \ \ , \ \ z = z$$ and $$\rho=\sqrt{x^2+y^2} \ \ , \ \ \phi = \arctan\Big(\frac{y}{x}\Big) \ \ , \ \ z = z$$ So I put $$\mathbf A(\rho,\phi,z) = z\ \hat i - 2\rho\cos(\phi) \ \hat j + \rho\sin(\phi) \ \hat k$$ I've tried computing the tangent vectors to the curve: $$\vec \rho = \Big(\frac{\partial A_x}{\partial \rho}, \frac{\partial A_y}{\partial \rho} , \frac{\partial A_z}{\partial \rho} \Big) = (0,-2\cos(\phi),\sin(\phi))$$ $$\vec \phi = \Big(\frac{\partial A_x}{\partial \phi}, \frac{\partial A_y}{\partial \phi} , \frac{\partial A_z}{\partial \phi} \Big) = (0, 2\rho\sin(\phi), \rho\cos(\phi))$$ $$\vec z = \Big(\frac{\partial A_x}{\partial z}, \frac{\partial A_y}{\partial z} , \frac{\partial A_z}{\partial z} \Big) = (1,0,0)$$ Scale factors: $$h_\rho = |\vec \rho| = \sqrt{4\cos^2(\phi) + \sin^2(\phi)}$$ $$h_\phi = |\vec \phi| = \sqrt{4\rho^2\sin^2(\phi) + \rho^2\cos^2(\phi)}$$ $$h_z = |\vec z| = 1$$ I also know that $$\hat \rho = \frac{1}{h_\rho}\vec \rho \ \ , \ \ \hat \phi = \frac{1}{h_\phi}\vec \phi \ \ , \ \ \hat z = \frac{1}{h_z}\vec z$$ But from here I'm not sure how to proceed to the correct answer of $$\mathbf A(\rho,\phi,z) = (z\cos(\phi)-\rho\sin(2\phi))\ \hat \rho - (z\sin(\phi)+2\rho\cos^2(\phi))\ \hat \phi + \rho\sin(\phi)\ \hat z$$ Any help would be greatly appreciated, cheers!","['cylindrical-coordinates', 'multivariable-calculus', 'curvilinear-coordinates']"
2970573,How to calculate the envelope of the trajectory of a double pendulum?,"Consider a double pendulum : Background For the angles $\varphi_i$ and the momenta $p_i$ we have (with equal lengths $l=1$ , masses $m=1$ and gravitational constant $g=1$ ): $\dot{\varphi_1} = 6\frac{2p_1 - 3p_2\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)}$ $\dot{\varphi_2} = 6\frac{8p_2 - 3p_1\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)}$ $\dot{p_1} = -\frac{1}{2}\big( \dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +3\sin(\varphi_1)  \big)$ $\dot{p_2} = -\frac{1}{2}\big( -\dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +\sin(\varphi_1)  \big)$ To see the relations more clearly: $\dot{\varphi_1} = B(2p_1 + Ap_2)$ $\dot{\varphi_2} = B(8p_2 + Ap_1)$ $\dot{p_1} = -C + 3D$ $\dot{p_2} = +C + D $ with $A = -3\cos(\varphi_1 - \varphi_2)$ $B = 6/(16 - A^2)$ $C = \dot{\varphi_1}\dot{\varphi_2}\sin(\varphi_1 - \varphi_2)/2$ $D = -\sin(\varphi_1)/2$ Observations With initial angles $\varphi_1^0 = \varphi_2^0 = 0$ and different combinations of small values for $p_1^0$ , $p_2^0$ a number of intriguiung patterns can be observed when plotting the tractory of the tip of the pendulum: $p_1^0 = 1$ , $p_2^0 = 1$ $p_1^0 = 1, p_2^0 = -1$ $p_1^0 = 0$ , $p_2^0 = 1$ $p_1^0 = 0$ , $p_2^0 = 2$ $p_1^0 = 0$ , $p_2^0 = 3$ $p_1^0 = 0$ , $p_2^0 = 3.7$ $p_1^0 = 0$ , $p_2^0 = 4$ What these patterns have in common: The tip of the pendulum draws a curve which more or less slowly ""fills"" an area enclosed by a specific envelope , intermediately exhibiting seemingly regular patterns which inevitably eventually vanish. Questions Can the envelope be given in closed form, depending only on the two parameters $p_1^0, p_2^0$ ? Can the positions of the two inner cusps which can be seen clearly for $p_1=0$ , $p_2=2,3$ be given in closed form, depending only on the two parameters $p_1^0, p_2^0$ ? [The envelope for $p_1^0=0, p_2^0 = 1,2,\dots$ looks like a canoe whose bow and stern bend to each other, eventually amalgamating. Can anyone guess what's the explicit formula for this shape?]","['chaos-theory', 'ordinary-differential-equations', 'classical-mechanics', 'ergodic-theory', 'mathematical-physics']"
2970634,Function that returns a number greater than 1 from a division,"Given two real numbers $x$ and $y$ (with $x, y \neq 0$ ), is there a function that returns only the result of the divisions $x/y$ and $y/x$ which is $\geq 1$ ? $$f(x, y) = x/y \geq 1 \text{ if } x \geq y$$ and $$f(x, y) = y/x \geq 1 \text{ if } y \geq x$$ A practical application of this could be to calculate the voltage standing wave ratio VSWR from two impedances $Z_1$ and $Z_2$ : the VSWR is always $\geq 1$ , and is just the ratio of the larger to the smaller impedance. What I am looking for is in effect the multiplication equivalent of the absolute function, which returns values $\geq 0$ for subtractions. It sounds to me like that probably exists, but I couldn't find it anywhere.",['functions']
2970652,Let $T:V\to V$ be a diagonalizable linear map on a vector space $V$. Does it hold that $T^{**}:V^{**}\to V^{**}$ is diagonalizable?,"Edit: I found my own answer.  But I will be happy to see other answers as well. Question: Let $T:V\to V$ be a linear operator on a vector space $V$ . (a) Suppose that $T$ is diagonalizable. Does it hold that $T^{**}:V^{**}\to V^{**}$ is diagonalizable? (b) If $V$ has a generalized eigenspace decomposition with respect to $T$ , then does it also hold that $V^{**}$ has a generalized eigenspace decomposition with respect to $T^{**}$ ? Here, $V$ is defined over a field $F$ and $V^*$ denotes the algebraic dual $\operatorname{Hom}_F(V,F)$ of $V$ .  So, the double dual of $V$ is $V^{**}=(V^*)^*$ .  The dual map $T^*:V^*\to V^*$ is defined by $T^*(f)=f\circ T$ for each $f:V\to F$ .  Similarly, the double dual $T^{**}=(T^*)^*$ satisfies $$T^{**}\varphi(f)=(\varphi\circ T^*)(f)=\varphi(f\circ T)$$ for all $\varphi:V^*\to F$ and $f\in V^*$ . We say that $T$ is diagonalizable if $V$ has a direct sum decomposition into eigenspaces $V_\lambda$ of $T$ $$V=\bigoplus_{\lambda\in F} V_\lambda,$$ where $V_\lambda=\big\{v\in V:Tv=\lambda v\big\}$ .  If $V_\lambda\neq \{0\}$ , $\lambda$ is an eigenvalue of $T$ . We can also say that $T$ is Jordanizable if $V$ has a direct sum decomposition into generalized eigenspaces $V^\lambda$ of $T$ $$V=\bigoplus_{\lambda\in F} V^\lambda,$$ where $V^\lambda=\big\{v\in V:(T-\lambda)^dv=0\text{ for some integer }d>0\big\}$ .  If $V^\lambda\neq \{0\}$ , $\lambda$ is a generalized eigenvalue of $T$ .  A Jordan block of $T$ is a subspace $J$ of some $V^\lambda$ that is an indecomposable $F[X]$ -submodule with the action $X\cdot v=Tv$ for all $v\in V$ that is not a subspace of a larger indecomposable $F[X]$ -submodule of $V$ . Here is what is known. If $V$ is finite dimensional, then we have a natural identification $V=V^{**}$ , so $T$ and $T^{**}$ are basically the same operator under this identification.   Thus, the answers to both questions are positive. However, if $V$ is infinite dimensional, things are a little cloudy.  I know that if $T$ is diagonalizable and has finitely many eigenvalues, then so does $T^{**}$ .  Hence, the answer to (a) is yes in this case.  But what if $T$ has infinitely many eigenvalues? We can say something similar for (b).  Suppose $V$ can be decomposed into a direct sum of finitely many generalized eigenspaces of $T$ .  If for each generalized eigenvalue $\lambda$ , there exists a positive integer $d_\lambda$ such that any $(T-\lambda)^{d_\lambda}$ vanishes on the generalized eigenspace $V^\lambda$ , then $V^{**}$ also can be decomposed into a direct sum of finitely many generalized eigenspaces of $T^{**}$ .  But I don't have the answer if $T$ has infinitely many generalized eigenvalues or if $d_\lambda$ does not exist for some $\lambda$ . The idea of the proof of my partial results for (a) and (b) is that if $p(T)=0$ for some polynomial $p$ , then $p(T^{**})=0$ as well.  But this proof does not work if $T$ has infinitely many (generalized) eigenvalue or if there exist arbitrarily large Jordan blocks (in terms of dimensions).  I do not expect that the answer to (a) is yes (and so this will answer (b) as well), but I cannot find a way to construct a counterexample. In fact, let $V^{*m}$ denote the $m$ th dual of $V$ and $T^{*m}$ the $m$ th dual of $T$ (i.e., $V^{*m}=\big(V^{*(m-1)}\big)^*$ and $T^{*m}=\big(T^{*(m-1)}\big)^*$ , with $V^{*0}=V$ and $T^{*0}=T$ ).  If $T$ satisfies the polynomial equation $p(T)=0$ , then every $T^{*m}$ satisfies $p\big(T^{*m}\big)=0$ .  Hence, if $T$ is diagonalizable with finitely many eigenvalues, then so are all $T^{*m}$ .  If $V$ has a generalized eigenspace decomposition wrt $T$ with finitely many generalized eigenvalues and there is a universal upper bound on the dimension of every Jordan block, then $V^{*m}$ has a generalized eigenspace decomposition wrt $T^{*m}$ .","['eigenvalues-eigenvectors', 'vector-spaces', 'linear-algebra', 'linear-transformations', 'generalized-eigenvector']"
2970739,Show that $\left(1+\frac{1}{1^3}\right)\left(1+\frac{1}{2^3}\right)\left(1+\frac{1}{3^3}\right)\cdots\left(1+\frac{1}{n^3}\right) < 3$,"I have this problem which says that for any positive integer $n$ , $n \neq 0$ the following inequality is true: $$\left(1+\frac{1}{1^3}\right)\left(1+\frac{1}{2^3}\right)\left(1+\frac{1}{3^3}\right)\cdots\left(1+\frac{1}{n^3}\right) < 3$$ This problem was given to me in a lecture about induction but any kind of solution would be nice.And also I'm in 10th grade :)","['induction', 'products', 'infinite-product', 'inequality', 'sequences-and-series']"
2970858,Integrate$ \int x \cos(x) \sin(x)\; dx$,"Integrate: $$\int x \cos(x) \sin(x) \;dx$$ I've been trying to integrate by parts, but I can't! I know there's a trigonometric function about $\cos (2x)$ but I don't know how to integrate that function with $\sin(x)$ .","['integration', 'calculus', 'trigonometry']"
2970906,To find which of the following statistics is (are) sufficient but NOT complete,"Let $X_1,X_2,X_3,....X_n$ be a random sample from a distribution with the probability density function $$f(x|\theta) =
\begin{cases}
\dfrac{x}{\theta^2}e^{\frac{-x}{\theta}},  & \text{if $x>0$; $\theta>0$} \\
0, & \text{otherwise}
\end{cases}$$ Which of the following statistics is(are) sufficient but NOT complete? $A=\bar X$ $B=\bar X^2+3$ $C=( X_1,\sum_{i=2}^{n}X_i) $ $D=(X_1,\bar X)$ My input The joint probability density function of the sample is $f(x_1,x_3,x_2...x_n|\theta) =\dfrac{\prod_{i=1}^{n}x_i}{\theta^{2n}}\bigg(e^{\dfrac{-\sum_{i=1}^{n}x_i}{\theta}}\bigg)$ Let $T(x)=\sum_{i=1}^{n}x_i$ .We see that the pdf can be expressed as a function that depends on the sample only through $T(x)$ . Thus $T(x)$ is a sufficient statistic for $\theta$ $A$ =  is sufficient statistic because $\bar X$ is a function of $\sum_{i=1}^{n}x_i$ by dividing the whole sum by $n$ gives $\bar X$ ( I am not sure if this reasoning of mine is valid I just can't explain it to someone in brief. I need a perfect reasoning for that so someone tell me that.) $B$ = is a sufficient statistic because any one to one function of sufficient statistic T is also sufficient statistic. For example, if $x>0$ $x^2$ will be a one to one function. $C$ = Sum of these two statistics give $\sum_{i=1}^{n}x_i$ $D$ = I am not sure how to calculate sufficient statistic in this one because I am not able to derive $\sum_{i=1}^{n}x_i$ with any linear combination. So my intuition says it's not sufficient statistics. Completeness is the next step but I am stuck in calculating sufficient statistic.
I am learning these concepts  so please give me any tips ,advice before calculating these things.","['statistical-inference', 'statistics']"
2970914,Why does a finitely oscillating sequence always has at least $2$ limit points?,Why is it not possible to find a finite oscillating sequence which has only $1$ limit point or no limit point at all? Why must it always have at least $2$ ? I'm just not able to picture it. Thanks EDIT : I think I should've started out by saying how do you even picture a finite oscillating sequence? What is an intuitive way to imagine a finite oscillating sequence?,"['sequences-and-series', 'real-analysis']"
