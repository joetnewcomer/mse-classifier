question_id,title,body,tags
2263289,Elements of $\mathbb Q(2^{1/3})$,"$\mathbb R$ is an extension of $\mathbb Q$. Why the elements of $\mathbb Q(2^{1/3})$ have the form $a+b\sqrt[3]2$? More specifically why $\mathbb Q(2^{1/3})=\{a+b\sqrt[3]2+c\sqrt[3]4:a,b,c \in\mathbb Q\}$?","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2263298,Basic Calc - why can't I solve this integral?,"So a basic form of the Gauss error function is $$\int e^{x^2}\,dx$$ and apparently this is not solvable analytically. But why? It seems that I can solve it pretty easily as $$\int e^{x^2}\,dx = \frac{1}{2x}e^{x^2}$$ since $$\frac{d}{dx} \frac{1}{2x}e^{x^2} = e^{x^2}.$$ Why is this wrong?","['derivatives', 'normal-distribution', 'calculus', 'integration', 'error-function']"
2263347,(Crazy idea) Line integral and the generalization of the $x$-axis,"I've been thinking about the mathematical motivation of the definition of line integrals and a crazy idea came to my mind. First of all, let's begin with the definition of integral line of a function: If $f$ is defined on a smooth curve $\gamma:[a,b]\to \mathbb R$ , then the line integral of $f$ along $\gamma$ is $$\int_{\gamma}f(x,y)ds=\int_a^bf(\gamma(t))|\gamma'(t)|dt$$ For me this definition is simply saying that the line integration is the one where the function is being integrate over a curve instead of a $x$ -axis: Note the $|\gamma'(t)|$ is the correction of the lengths of the partitions of the curves (the partitions are in the curve instead of the $x$ -axis). Am I right?","['multivariable-calculus', 'line-integrals']"
2263363,Solve a differential equation by a Laplace transform,I have this differential equation $\frac{d^4 y}{dx^4} - y = f(x)$ Where $y $ and the first three derivatives of $y$ disappear at $x = 0$. I have shown that $$\bar{y}(s)= \int_{0}^{\infty} f(\xi) \frac{e^{-s \xi}}{s^4 -1} d\xi $$ which is the first part of the question. I need to show that $$ y(x) = \frac{1}{2} \int_{0}^{x} f(\xi) [\sinh(x - \xi) - \sin(x - \xi)] d\xi$$ By breaking $\frac{1}{s^4 -1}$ into partial fractions we have $$\frac{1}{s^4 -1} = \frac{1}{2} \int_{0}^{\infty} (\sinh x - \sin x ) e^{-sx}  dx $$ And we have a double integral. How do I get to the solution from here?,"['ordinary-differential-equations', 'laplace-transform']"
2263404,Eigenvalues of a $2\times2$ matrix,"Let $a,b$ be distinct eigen values of a $2\times2$ matrix $A.$Then which of the following statement is true? $A^2$ has distinct eigen values. $A^3=\frac{a^3-b^3}{a-b}A-ab(a+b)I$ Trace of $A^n$ is $a^n+b^n$ for every positive integer n. $A^n$ is not a scalar multiple of identity matrix for any positive integer n. I think first option is wrong because if $1,-1$ are distinct eigenvalues of $A$ but $A^2$ has eigenvalues $1,1$. Third option is correct as we have result. I tried second option. Second is also right. For fourth option, characteristic equation implies $A^n$ cannot be a scalar multiple of identity matrix for any positive integer $n$. Is it correct?","['matrices', 'linear-algebra', 'proof-verification']"
2263408,Arccot series summation,"Find the value of: $$\sum_{k=1}^{\infty}\mathrm{arccot}\left(2^{k+1}+\dfrac{1}{2^k}\right)$$ I think we need to use telescoping method, but I am not able to convert this.","['algebra-precalculus', 'summation', 'trigonometry', 'trigonometric-series']"
2263415,"Prove that even if $f'$ is not continuous, there must exist a number $c, a<c<b$, with $f'(c)=0$ [duplicate]","This question already has answers here : How to prove that derivatives have the Intermediate Value Property (5 answers) Closed 7 years ago . Let $f'$ exist for all $x$ on $[a,b]$, and suppose that $f'(a)=-1, f'(b)=1$. Prove that even if $f'$ is not continuous, there must exist a number $c, a<c<b$, with $f'(c)=0$ I'm not sure how to approach this problem. I thought about Rolle's Theorem, but can't really show it.","['derivatives', 'real-analysis', 'continuity']"
2263431,Finding minimum of trigonometric function,"Let $$v=\sqrt{a^2\cos^2(x)+b^2\sin^2(x)}+\sqrt{b^2\cos^2(x)+a^2\sin^2(x)}$$ Then find difference between maximum and minimum of $v^2$. I understand both of them are distance of a point on ellipse from origin, but how do we find maximum and minimum? I tried guessing, and got maximum $v$ when $x=45^{o}$ and minimum when $x=0$, but how do we justify this?","['inequality', 'trigonometry', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'maxima-minima']"
2263439,Upper semicontinuity on a normal space,"If $X$ is a normal Hausdorff space and $f:X \rightarrow \mathbb{R}$ is upper semicontinuous ( $\{ f < a \}$ is open for $a \in \mathbb{R}$), then how can we show $f(x) = \inf \{ g(x): g \in C(X), f \leq g\}$? It seems to me that Urysohn's Lemma is involved. I wanted to consider functions of the form $f_k = \sum_{j=1}^{k 2^k} \frac{j}{2^k} \chi_{ \{ \frac{j-1}{2^k} \leq f < \frac{j}{2^k} \} }$ and use Urysohn's Lemma on the sets $\{ \frac{j-1}{2^k} \leq f < \frac{j}{2^k} \}$, but these may not be closed.","['general-topology', 'real-analysis', 'semicontinuous-functions', 'separation-axioms']"
2263443,Deduce that $\int_{0}^{2\pi}\cos^{2n}\theta\ d\theta = {{2n}\choose{n}}\frac{\pi}{2^{2n -1}}$,"Deduce that $\int_{0}^{2\pi}\cos^{2n}\theta\ d\theta = {{2n}\choose{n}}\frac{\pi}{2^{2n -1}}$ So first we make the following substitution $z = e^{i\theta}$ and hence $\cos = \frac{z + z^{-1}}{2}$. Then we have the following; $\frac{1}{i}\int_{C(0;1)}(\frac{z+z^{-1}}{2})^{2n}\frac{1}{z}dz$; multiply ""top and bottom on the outside"" by $2\pi$, we then arrived at $2\pi$Res($(\frac{z+z^{-1}}{2})^{2n}\frac{1}{z}$, $0$). And now I'm totally stuck, I have no idea how to even ""magically"" make the combinatoric function appear. Is there some standard result I need to use here or is there a trick I'm not seeing here. Any help or insight is deeply appreciated (I really mean it :)) EDIT:
I realize there is a mistake in my working, hence it's not making sense to some. I edited it.","['complex-analysis', 'integration', 'contour-integration', 'analysis']"
2263553,Complete orthonormal and countable set in a Hilbert space,"$\{x_n\}$ is a complete orthonormal countable set in a Hilbert space and $\{y_n\}$ is a countable set. It is known that $\underset {n\ge 1} \sum ||x_n-y_n||<1 $. I need to show that $\{y_n\}$  is also complete (not necessarily orthonormal). I tried to use Parseval's identity  and another facts which are equivalent to say that an orthonormal system is complete, but no results.","['functional-analysis', 'hilbert-spaces']"
2263668,Conditional Expectation with elliptical random variables,"In a paper I am reading it is written the following: Let $X = (X_1, \dots, X_n) \sim E_n(\mu, \Sigma, \phi)$ be a
  elliptical-distributed random vector; let $S = \sum_{i = 1}^n X_i$.
  Then $$E[X_k \mid S=s] = \mu_k + \frac{\sigma_{k,S}}{\sigma_{S}^2}(s -
 \mu_S)$$
  where $\mu_k$ is the mean of $X_k$, $\mu_S, \sigma^2_S$ are the mean and variance of $S$, and $\sigma_{k,S}$ is the covariance between $X_k$ and $S$ I have tried to prove this but without luck. Can anybody help? Thoughts I tried the following: $$E[X_k \mid S=s] = \int xf_{X_k \mid S = s}(x) dx$$ Since $$f_{X_k \mid S = s}(x)  = \frac{f_{(X_k, S)}(x,s)}{f_S(s)}$$ I get $$E[X_k \mid S=s] = \frac 1{f_S(s)} \int x f_{(X_k, S)}(x,s) dx$$ I know the distribution of the vector $(X_k, S)$ (Since it is a linear transformation of the vector $X$, it is still elliptical distributed and I can compute it's mean and variance; the elliptical generator $\phi$  is still the same). But how to simplify it further to get to the result stated in the paper?","['expectation', 'probability-theory', 'integration', 'probability', 'conditional-expectation']"
2263676,A polynomial equation game,"Alice and Bob are sitting in a warm, cozy room. As cozy as the room is, there's not much to do, and they soon find themselves rather bored. As you might expect from people as uncreatively named as Alice and Bob, Alice and Bob decide to play a game. They start with a monic polynomial equation $x^n + a_1 x^{n-1} + \cdots + a_n$, where the coefficients $a_i$ are to be determined. Each turn, Alice chooses an integer, and Bob chooses which coefficient that integer is to be. After choosing and placing all $n$ coefficients, if the resulting polynomial has $n$ distinct integer solutions, Alice wins. Otherwise, Bob wins. For which $n$ does Alice have a winning strategy? For $n = 1, 2$ it is fairly easy to see she does. What happens for larger $n$?","['number-theory', 'combinatorics']"
2263690,$\frac{3}{2}$-generated groups?,"Is there a special name for groups $G$ with the following property? For every $g \in G \setminus \{1\}$ there is some $h \in G \setminus \langle g \rangle$ with $G = \langle g,h \rangle$. Which symmetric groups have this property? (I have already checked with a program that $S_3,S_5,S_6,S_7,S_8,S_9$ have this property. $S_4$ does not have this property.) Edit. $S_n$ has this property for all $ n \neq 2$ (see the accepted answer). Is there a proof for this in English?","['finite-groups', 'group-theory', 'symmetric-groups']"
2263733,More `General' Chinese Remainder Theorem,"I know of the Chinese Remainder Theorem, but I get the feeling it is only the `basic' version of it. From the Chinese Remainder Theorem, we know that, for $m,\in\mathbb{Z}$ such that $\text{gcd}(m,n)=1$, $C_{mn}\cong C_{m}\times C_{n}$. Is it then true that $C_{p_{1}^{r_{1}}\ldots p_{n}^{r_{n}}}\cong C_{p_{1}^{r_{1}}}\times\ldots\times C_{p_{n}^{r_{n}}}$, where $p_{i}^{r_{i}}$s are (distinct) prime numbers?","['finite-groups', 'abstract-algebra', 'chinese-remainder-theorem', 'cyclic-groups', 'group-theory']"
2263840,Non-compact operator with compact derivative,"Let $T:X \to Y$ be a nonlinear operator between Hilbert spaces. Is it possible for $h \mapsto T'(x)[h]$ to be a compact mapping from $X$ to $Y$ even if $T$ is not compact?  Here $T'(x)[h]$ is the directional derivative of $T$ at $x$ in the direction $h$. If so, is it possible to classify all such $T$?","['functional-analysis', 'compact-operators', 'operator-theory', 'derivatives']"
2263847,Example of function that is not Borel measurable,"Could you provide an example of a function $f$ from $[0,1]^2$ to $[0,1]$ that is not Borel measurable? Is requiring $f$ to be Borel meusurable very restrictive?",['measure-theory']
2263853,Parametrization of two surfaces $\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1$ and $\frac{x^2}{p}+\frac{y^2}{q}=2z$.,"Can someone please help me to parametrize the following surfaces in terms of  hyperbolic(for second it might not be possible but i need some more convenient set of parametric equation than mine ) and trigonometric functions $$\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1 $$ and  $$\frac{x^2}{p}+\frac{y^2}{q}=2z$$
I have tried to do but the set of parametric equations I got were too complicated as I have to use those in some further calculation which makes the result very ugly. 
For first equation the set of parametric equations is: $$x=a\sqrt{1+\frac{u^2}{c^2}}\cos v, \ \ y=b\sqrt{1+\frac{u^2}{c^2}}\sin v \ \ z=u$$ and for second: $$x=\sqrt{2pu} \cos v ,\ \ y=\sqrt{2qu} \sin v, \ \ z=u $$","['multivariable-calculus', 'differential-geometry']"
2263898,Probability of two people meeting on a non-square grid,"Suppose a person leaves from home to the health club (eight blocks east and five blocks north). Furthermore, suppose this person wants to keep the route as short as possible but likes to vary it: There are $C(13,8)$ routes. My question is what the probability would be of two people meeting if one (say, Matt) left from the home to the health club, and someone else (say, Tine) left from the health club to the home. This is similar to a question asked yesterday where there was a square grid. My thought here was that the probability of Matt and Tina meeting would be $\frac{C(13,8)}{2^{13}}$, but that doesn't seem right given that Matt and Tina can walk on different paths even though they will only ever meet 6.5 blocks into their walk. Any idea about how to deduce what the probability would be here?","['algebra-precalculus', 'combinatorics', 'probability']"
2263925,Inequality $\frac{b}{ab+b+1} + \frac{c}{bc+c+1} + \frac{a}{ac+a+1} \ge \frac{3m}{m^2+m+1}$,"Let $m=(abc)^{\frac{1}{3}}$, where $a,b,c \in \mathbb{R^{+}}$. Then prove that $\frac{b}{ab+b+1} + \frac{c}{bc+c+1} + \frac{a}{ac+a+1} \ge \frac{3m}{m^2+m+1}$ In this inequality I first applied Titu's lemma ; then Rhs will come 9/(some terms) ; now to maximise the rhs I tried to minimise the denominator by applying AM-GM inequality.But then the reverse inequality is coming
Please help.","['inequality', 'algebra-precalculus', 'contest-math', 'fractions', 'holder-inequality']"
2263932,Adding digits in this way to primes to obtain another primes,"I just created the following ""game"": Choose some prime number. If it has $k$ digits then add another $k$ digits to the right to obtain another prime number. Now we have a prime number with $2k$ digits. Now add $2k$ digits to the right of the new number to obtain a new prime number. Repeat the procedure until it is no longer possible to produce new primes in this way. To clarify, let us take for example $5$. We must add one digit to the right to obtain another prime number, it can be $53$. Now we must add two digits to the right of $53$ to obtain a new prime. It could be $5323$. Now we must add four digits to the right of $5323$ to obtain a new prime. It could be $53231117$ and so on and so on... Is it true that whatever prime we choose at the beginning that this game never ends? That is, that it is always possible to build larger primes from the smaller ones in this way?","['number-theory', 'conjectures', 'prime-numbers', 'elementary-number-theory']"
2263933,"Why does this not contradict 'Solutions of $\dot{x} = f(x)$ cannot intersect""?","Say we have $\dot{x} = cos(x^2)$ and $x(0) = 1$, and also $\dot{y} = cos(y^2)$ and $y(0) = 0.5$ Then by the MVT we know that $0.5 < \frac{y(1)-y(0)}{1 - 0} < 1$, which implies that $y(0) = 0.5 < x(0) = 1< y(1)$. How does this not contradict that solutions of an autonomous ODE, $\dot{x} = f(x)$ with $f:\mathbb{R}^n \to \mathbb{R}^n$ continuously differentiable, cannot cross? I think this may be down to a misunderstanding in general of what the above theorem means. I thought it meant that solutions of the same ODE, for different initial conditions, cannot physically cross, even if this happens at different times, but in the above we seem to have different initial conditions, and a solution physically crossing the path of another solution. Is it something to do with the solutions being 1 dimensional?","['ordinary-differential-equations', 'dynamical-systems']"
2263954,Ideal associated to a set of points of $\mathbb{P}^n$ in general position,"In The homogeneous ideal of $2n$ points in general position in $\mathbb{P}^n$ , we let $\Gamma$ be a set of $d=2n$ points in general position in $\mathbb{P}^n$, and we want to show that the associated homogeneous ideal $I(X)$ is generated by homogeneous polynomials of degree $2$. If $\Gamma$ is a set of $d\leq kn$ points of $\mathbb{P}^n$ in general position, where $k\geq 2$, how could I show that $\Gamma$ vanishes for a family of homogeneous polynomials of degree $\leq k$? The main problem I have is that I want to prove the result using Hilbert polynomials and resolutions only, my professor told me there is a very easy way to prove it using such tools but I don't see how.","['hilbert-polynomial', 'algebraic-geometry']"
2263983,Finding Lie Subalgebras and their corresponding Lie subgroups,"I'm studying for a qualifying exam and just reviewed the following theorem: Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$. Suppose $\mathfrak{h}\leq \mathfrak{g}$ is a subalgebra. Then there exists a unique connected Lie subgroup $H \leq G$ such that $T_1H=\mathfrak{h}$. I'm now trying to apply this theorem to do the following exercise: Let $H=\left\{\left( \begin{array}{ccc}
    1 & x & z  \\
    0 & 1 & y \\
    0 & 0 & 1
\end{array}\right) \Bigg\vert x,y,z \in \mathbb{R} \right\}$ and let $ Z =\left\{\left( \begin{array}{ccc}
    1 & 0 & z  \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{array}\right) \Bigg\vert z \in \mathbb{R} \right\}$ so that $Z$ is a Lie subgroup of the nilpotent Lie group $H$. ($Z$ is the center of $H$.) Find all connected $2$-dimensional Lie subgroups of $H$ that contain $Z$. Describe them as both submanifolds of $H$ and as groups. The first thing I did was find their corresponding Lie algebras:
$$\mathfrak{h}=T_1H=\left\{\left( \begin{array}{ccc}
    0 & x & z  \\
    0 & 0 & y \\
    0 & 0 & 0
\end{array}\right) \Bigg\vert x,y,z \in \mathbb{R} \right\} ~\text{ and }~\mathfrak{z}=T_1Z=\left\{\left( \begin{array}{ccc}
    0 & 0 & z  \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right) \Bigg\vert z \in \mathbb{R} \right\}$$ Next we need to find all the $2$-dimensional subalgebras of $\mathfrak{h}$ containing $\mathfrak{z}$. Here's where I lose confidence in my solution; I think that for any $a,b \in \mathbb{R}$ the following are all the $2$-dim. subalgebras (containing $\mathfrak{z}$): $\operatorname{span}\left\{ \left( \begin{array}{ccc}
    0 & 1 & 0  \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right), ~\left( \begin{array}{ccc}
    0 & 0 & 1  \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right) \right\}$ $\operatorname{span}\left\{ \left( \begin{array}{ccc}
    0 & 0 & 0  \\
    0 & 0 & 1 \\
    0 & 0 & 0
\end{array}\right), ~\left( \begin{array}{ccc}
    0 & 0 & 1  \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right) \right\}$ $\operatorname{span}\left\{ \left( \begin{array}{ccc}
    0 & a & 0  \\
    0 & 0 & b \\
    0 & 0 & 0
\end{array}\right), ~\left( \begin{array}{ccc}
    0 & 0 & 1  \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right) \right\}$ These are all subalgebras because the bracket of basis elements is the zero matrix, so by bi-linearity of the bracket they are closed under bracketing. In the proof of the theorem the corresponding Lie subgroup to a Lie algebra is the connected leaf of the resulting foliation which contains $1 \in G$. This seems hard to find so instead I think we can just ""find"" the Lie subgroups which will have the corresponding Lie algebras: $\left\{ \left( \begin{array}{ccc}
    1 & x & z  \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{array}\right) \Big\vert x,z \in \mathbb{R} \right\}$ $\left\{ \left( \begin{array}{ccc}
    1 & 0 & z  \\
    0 & 1 & y \\
    0 & 0 & 1
\end{array}\right) \Big\vert y,z \in \mathbb{R} \right\}$ $\left\{ \left( \begin{array}{ccc}
    1 & ta & z  \\
    0 & 1 & tb \\
    0 & 0 & 1
\end{array}\right) \Big\vert t,z \in \mathbb{R} \right\}$ Each of these sets are groups with multiplication. The first two are diffeomorphic to $\mathbb{R}^2$, but I'm not sure how to see what the third Lie group is as a manifold. Any corrections, comments or hints would be greatly appreciated! EDIT: Following the comment of Sunghyuk, 
$$\operatorname{exp} \left( \begin{array}{ccc}
    0 & a & 0  \\
    0 & 0 & b \\
    0 & 0 & 0
\end{array}\right) = \left( \begin{array}{ccc}
    1 & a & \frac{ab}{2}  \\
    0 & 1 & b \\
    0 & 0 & 1
\end{array}\right)$$ and we could instead write the corresponding Lie subgroup instead as $\left\{ \left( \begin{array}{ccc}
    1 & ta & z+\frac{t^2ab}{2}  \\
    0 & 1 & tb \\
    0 & 0 & 1
\end{array}\right) \Big\vert t,z \in \mathbb{R} \right\}$ which is the same Lie subgroup as 3. but perhaps gives us more intuition on how to view it as a manifold?","['differential-topology', 'manifolds', 'differential-geometry', 'lie-algebras', 'lie-groups']"
2264008,"If $\sin^4(x)=A+B\cos(2x)+C\cos(4x)$, then find $A$, $B$, and $C$.","If $\sin^4(x)=A+B\cos(2x)+C\cos(4x)$, then find $A, B$, and $C$. Would I use half-angle identities for this problem?",['trigonometry']
2264029,The nth finite difference of the nth powers of consecutive real numbers yields n!,"I was recently shown the following observation: if we have the set $$[1^2,2^2,3^2] = [1,4,9]$$ and we form a new set with two elements by subtracting each element from the element to its right, that is to say we form the new set: $$[2^2-1^2, 3^2-2^2] = [3,5]$$ 
and then we repeat this process, we get $$[5-3] = [2] = [2!]$$
More generally, it was seen that for any real starting number ""a"" followed in the original set by a+1, a+2, this would hold. That is to say: starting with the set: $$[a^2,(a+1)^2,(a+2)^2]$$  and performing the process described one always arrives at the one-element set $$[2!]$$ after 2 repitions of the subtraction process described above. Further generalizing, this holds for any power, not just 2. Stated generally the above observed rule is then starting with the initial set $$[a^n,(a+1)^n,(a+2)^n, (a+3)^n,...,(a+(n-1))^n,(a+n)^n ]$$ and performing the subtraction process described above n times yields the single-element set $$[n!]$$ Immediately evident was that if I used this process n times on a set with n+1 elements, let's say: $$[a_0,a_1,a_2,a_3,...,a_{n-2},a_{n-1},a_n]$$  the following would result:
$$\sum_{k=0}^{n} \binom nk a_{n-k}(-1)^k$$ However, this result did not make the n factorial result clear. Going on, I used the easily demonstrated fact that $$(x+1)^n-x^n = \sum_{k=1}^n \binom nkx^{n-k}$$ to find an alternative expression which consisted of a series of nested sums. First I will prove that the above expression is indeed correct. By the binomial theorem, we have that $$(x+1)^n = \sum_{k=0}^n \binom nk  x^{n-k} = \binom n0x^n+\sum_{k=1}^n \binom nk  x^{n-k} = x^n+\sum_{k=1}^n \binom nk  x^{n-k}$$ thus, subtracting $$x^n$$ we get $$(x+1)^n-x^n = \sum_{k=1}^n \binom nkx^{n-k}$$ Since all of the elements in our set differ by 1, this result is particularly useful. Starting again with the set $$[a^n,(a+1)^n,(a+2)^n, (a+3)^n,...,(a+(n-1))^n,(a+n)^n ]$$ we then find that subtracting two consecutive terms, means subtracting $$ (a+c)^n$$ from $$ ((a+c)+1)^n$$ where c is a positive integer that runs from 0 to n-1. From the earlier demonstrated equality, we have that  $$((a+c)+1)^n-(a+c)^n = \sum_{k=1}^n\binom nk(a+c)^{n-k}$$ Thus performing the process once yields the new set: $$[\sum_{k=1}^n\binom nk(a+0)^{n-k},\sum_{k=1}^n\binom nk(a+1)^{n-k},\sum_{k=1}^n\binom nk(a+2)^{n-k},\sum_{k=1}^n\binom nk(a+3)^{n-k},...,\sum_{k=1}^n\binom nk(a+(n-2))^{n-k},\sum_{k=1}^n\binom nk(a+(n-1))^{n-k}]$$ Performing the operation again, we can ignore the outer summation and the binomial coefficients since they are the same for all the terms, and we are then faced with the problem as the original, but with c now running from 0 to (n-2). Thus with the subtraction performed twice, we have produced the set $$[
\sum_{k=1}^n(\binom nk \sum_{j=1}^{n-k}\binom {n-k}j(a)^{n-k-j}),
\sum_{k=1}^n(\binom nk \sum_{j=1}^{n-k}\binom {n-k}j(a+1)^{n-k-j}),
\sum_{k=1}^n(\binom nk \sum_{j=1}^{n-k}\binom {n-k}j(a+2)^{n-k-j}),...,
\sum_{k=1}^n(\binom nk \sum_{j=1}^{n-k}\binom {n-k}j(a+(n-3))^{n-k-j})
\sum_{k=1}^n(\binom nk \sum_{j=1}^{n-k}\binom {n-k}j(a+(n-2))^{n-k-j})
]$$ Clearly by repeating this process ""b"" times, we increase the number of nested summations to b, while decreasing the number of terms by b. We will have to introduce b ""dummy iterators"" (like k and j) which iterate over the sums. The most internal sum will have the sum of these variables up to the bth one. Clearly c always starts at 0 and ranges from 0 to n-b (Verify this in the above 2 iterations of the process). In mathematical terms, if I introduce a list of dummy variables which I will call $$d_i$$ Then by repeating the subtraction process b times I produce the new set in which each term is of the form 
$$\sum_{d_1=1}^n\binom n{d_1} \sum_{d_2=1}^{n-d_1}\binom {n-d_1}{d_2} \sum_{d_3=1}^{n-d_1-d_2}\binom {n-d_1-d_2}{d_3}...\sum_{d_b=1}^{n-{\sum_{i=1}^{b-1}{d_i}}}\binom {n-{\sum_{i=1}^{b-1}{d_i}}}{d_{b}}(a+c)^{n-{\sum_{i=1}^{b}{d_i}}}$$ 
where c runs from 0 to n-b to give us the elements of the set from left to right. Since the subtraction process decreases the number of terms in the set by 1 each iteration, and we started with n+1 terms in the set, to get our single term resulting series, we should decrease the number of elements by 1 n times, that is to say, perform our subtraction process n times. That means with have now have an expression for our one-element set in terms of the nested summations, by letting $$b=n$$By our ealier formula it is also evident that c now ranges from 0 to n-n, so we can say that $$c=0$$ and eleminate it from the formula entirely. Plugging in n to our earlier formula with b, we get the one element set: $$\sum_{d_1=1}^n\binom n{d_1} \sum_{d_2=1}^{n-d_1}\binom {n-d_1}{d_2} \sum_{d_3=1}^{n-d_1-d_2}\binom {n-d_1-d_2}{d_3}...\sum_{d_n=1}^{n-{\sum_{i=1}^{n-1}{d_i}}}\binom {n-{\sum_{i=1}^{n-1}{d_i}}}{d_n}a^{n-{\sum_{i=1}^n{d_i}}}$$ Notice that when each of these dummy iterators is at their starting value, 1 $$\sum_{i=1}^n{d_i} = n $$ and $$\sum_{i=1}^{n-1}{d_i} = n-1 $$This means that with all d's at a value of 1, the final imbedded summation is $$\binom {n-(n-1)}1a^{n-n}= 1a^0=1$$ What happens if you want to increase one of the d's though? Take for example, $$d_1=2$$Now$$\sum_{i=1}^n{d_i} = n+1 $$ and $$\sum_{i=1}^{n-1}{d_i} = n $$ Therefore the value of the final imbedded term is $$\binom {n-n}1a^{n-n}= \binom 0 1a^{-1}=0$$ Clearly, if we increase any of the d's, we get a binomial coefficient that is ""a number less than 1 choose 1"", which is get 0. Therefore, the only non-zero term in the entire embedded summation exists when all d's are equal to 1, so the summations evaporate leaving only a product of binomial coefficients of the form: $$\binom n{d_1} \binom {n-d_1}{d_2} \binom {n-(d_1+d_2)}{d_3}...\binom {n-{\sum_{i=1}^{n-1}{d_i}}}{d_n}a^{n-{\sum_{i=1}^n{d_i}}}$$ which, by the definition of the binomial coefficients, is equal to: $$\frac {n!}{(n-d_1)!{d_1}!} \frac {(n-d_1)!}{(n-(d_1+d_2))!{d_2}!}\frac {(n-(d_1+d_2))!}{(n-(d_1+d_2+d_3))!{d_3}!}...\frac {(n-\sum_{i=1}^{n-1}{d_i})!}{(n-\sum_{i=1}^{n}{d_i})!{d_n}!}$$Since all d's are now 1, the d!'s on in the denominator are all 1 and can be removed from the expression. Additionally, and quite conveniently, all of the factorials cancel except for $$n!$$ in the numerator and $$(n-\sum_{i=1}^{n}{d_i})!$$ in the the denominator. Recall that since all d's are equal to 1, $$\sum_{i=1}^{n}{d_i} = n$$ so we can simply our final expression to $$ \frac {n!}{(n-n)!} = \frac {n!}{0!} = \frac {n!}1 = n!$$ I would like feedback on whether or not this is a valid demonstration of this fascinating observation...please let me know what you think! Also if there is a name for this phenomenon or this subtraction process I would appreciate knowing that too. I am aware that this is strongly linked to the stirling numbers of the second kind, for which reason I have tagged this post with the tag ""stirling numbers"".","['stirling-numbers', 'factorial', 'finite-differences', 'discrete-mathematics']"
2264035,Definition of CW complex: weak topology vs quotient,"In the definition of a CW complex we construct a topological space by gluing  $n$-cells to the $n-1$-skeleton, and then we give to it another topology (it already has one: the one given by the gluing operation which is a quotient topology) choosing a particular family of closed sets as fundamental covering (or equivalently the initial topology given by the  characteristic maps). I imagine that the two topologies (the quotient and the weak one) are different (otherwise why call it ""weak""? It would be enough to define the CW complex as a gluing of cells), but I would make good use of an example. Can anyone help me?
Thanks","['algebraic-topology', 'general-topology', 'cw-complexes']"
2264136,$\int_{0}^{\infty} {x^{\alpha} \over \left(x + 1\right)}dx$,I need some help to evaluate the following integral. $$\int_{0}^{\infty} {x^{\alpha} \over \left(x + 1\right)}dx$$ I know you need to use a branch cut but not sure how to start. Any help is always appreciated. Edit: Need Help to Establish the Cauchy Principle for the improper integral if that provides any direction.,['complex-analysis']
2264179,equivalence classes of $x^2 \equiv y^2 \pmod{7}$ for all integers,"How do I denote the equivalence classes for $x^2 \equiv y^2 \pmod{7}$ for all integers.
I know the possible results are $0, 1, 2$ and $4$. $$x\equiv0 \pmod{7}, \space x^2\equiv0^2  \equiv 0 \pmod{7}$$
$$x\equiv1 \pmod{7}, \space x^2\equiv1^2  \equiv 1 \pmod{7}$$
$$x\equiv2 \pmod{7}, \space x^2\equiv2^2  \equiv 4 \pmod{7}$$
$$x\equiv3 \pmod{7}, \space x^2\equiv3^2  \equiv 2 \pmod{7}$$ But I do not understand how to denote it properly.","['equivalence-relations', 'discrete-mathematics']"
2264203,Wasserstein distances metrize weak convergence,"Let $(X,d)$ be a metric space. Let $P(X)$ denote the space of probability measures on $X$. The (first) Wasserstein distance is a distance on $P(X)$ given by:
$$
d_W(p,q):= \inf_{m\in\Gamma(p,q)} \int_{X\times X} d(x,y)\, dm(x,y)\;,
$$
where $\Gamma(p,q)$ is the set of measures on $X\times X$ which have marginals $p$ and $q$ respectively on the first and second component. In the literature (for example, Villani, ""Topics in optimal transportation""), I've always read that if $X$ is Polish , then $d_W$ metrizes the weak convergence. However, is separability of $X$ really necessary?","['reference-request', 'optimal-transport', 'weak-convergence', 'measure-theory', 'metric-spaces']"
2264213,Using Normal distribution for a random sample.,"The weights of a group of people are distributed on a normal curve with $\mu = 172$, $\sigma = 30$. What is the probability that the average weight of a sample of $9$ people is less than $177$? My attempt: Define $X_A = \frac{X_1+X_2+X_3+X_...+X_9}{9}$, where $X_i$ are random variables representing the weights of each man. Then $E[X_A] = 172$, and $Var[X_A] = \frac{30^2}{9}$. Then $Z = \frac{X_A-172}{10} $, The answer is the area to the left of $0.5$ in the standard normal curve, which is $0.6915$ (from a table). Is this right?","['statistics', 'probability', 'probability-distributions']"
2264239,"eliminate A,B and C","How to eliminate $A$, $B$, $C$ from the given three equations:
$$X \cos A+ Y \cos B + Z \cos C=0$$
$$X \sin A+ Y \sin B + Z \sin C=0$$
$$X \sec A+ Y \sec B + Z \sec C=0$$
Answer is 
$$(X^2+ Y^2 - Z^2 )^2 = 4 X^2 Y^2$$
I was squaring and adding 1st and 2nd eqn .But could not use the 3rd one.Please anybody help.","['proof-writing', 'trigonometry', 'systems-of-equations']"
2264265,divisibility of polynomials over a finite field,Let $K$ be a finite field. Let $f$ be an irreducible polynomial over $K$ of degree $n\ge 2$. Let $P_i$ be the product of all polynomials of degree $i$ over $K$. Prove that $f$ divides $P_1\cdot\ldots\cdot P_{n-1}-1$.,"['finite-fields', 'abstract-algebra', 'irreducible-polynomials', 'polynomials']"
2264288,Why is the period of $\sin(2x)$ is $\pi$?,"In my textbook, it is stated that the period of $\sin(2x)$ is $\pi$ . The author justifies this using a mathematical statement which I cannot understand.
He writes that, since $\sin(2x) = \sin(2x+2\pi) = \sin(2(x+\pi))$ the period of $\sin(2x)$ is $\pi$ . Though my intuition tells me that the period of $\sin(2x)$ is $\pi$ , I just cannot understand this reasoning. To me the period of $\sin(2x)$ appears to be to $2\pi$ since $\sin(2x)=\sin(2x+2\pi)$ . I would be very thankful if someone could explain this reasoning to me.",['trigonometry']
2264329,Prove $f(x) = f(0) e^{\lambda x}$ if $f'(x) = \lambda f(x)$,"I'm to show that if $f:\mathbb{R} \to \mathbb{R}$ is differentiable and $f'(x) = \lambda f(x)$ for $\lambda \in \mathbb{R}$, then $f(x) = f(0) e^{\lambda x}$. I'm not really sure where to start with proving this. I was trying to start with the case that $f(x)$ is constant, so $f'(x) = 0 \implies f(x)=c = c \exp(0) = c$. I'm just not sure how to start this proof.","['real-analysis', 'ordinary-differential-equations']"
2264343,Solve the Differential equation $x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2}$,Solve the Differential equation $$x^3 \frac{dy}{dx}=y^3+y^2\sqrt{x^2+y^2}$$ i reduced the equation as $$x^3\frac{dy}{dx}=y^3\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right)$$ $\implies$ $$\frac{x^3}{y^3}\frac{dy}{dx}=\left(1+\sqrt{1+\left(\frac{x}{y}\right)^2}\right) \tag{1}$$ Next put $$\frac{x}{y}=v$$ we get $$ x=vy$$ then $$ \frac{dx}{dy}=v+y\frac{dv}{dy}$$ Then $(1)$ becomes $$\frac{v^3}{v+y\frac{dv}{dy}}=1+\sqrt{1+v^2}$$ Reciprocating we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{1}{\sqrt{1+v^2}+1}$$  Rationalizing RHS we get $$\frac{v+y\frac{dv}{dy}}{v^3}=\frac{\sqrt{1+v^2}-1}{v^2}$$ Rearranging we get $$\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\frac{dy}{y}$$ EDIT: i am posting here the clue given by paul using Substitution $v=\tan z$: $$\int\frac{dv}{v \times \left(\sqrt{1+v^2}-2\right)}=\int\frac{\sec^2 z\: dz}{\tan z(\sec z-2)}=\int\frac{dz}{\sin z(1-2\cos z)}$$ So $$\int\frac{dz}{\sin z(1-2\cos z)}=\int \frac{\sin z\: dz}{\sin^2 z(1-2\cos z)}$$ Put $\cos z=t$ and use partial fractions,"['algebra-precalculus', 'ordinary-differential-equations', 'calculus']"
2264393,Permutations preserving sequential order,"Say we have a sequence ""abc123"".  I am trying to count the sequences that are permutations of this, yet preserve the order such that the 3 will never come before 1 or 2, b won't come before a, etc.
So ""a1b23c"" is acceptable but ""a2b31c"" is not. I attempted to count cases by hand and got the following. $ abc123, ab1c23, ab12c3, a12bc3, a12b3c, a1b23c, a1b2c3 ,a123bc, a1bc23 ,ab123c $
and
$123abc, 12a3bc, 12ab3c, 1ab23c, 1ab2c3, 1a2bc3, 1a2b3c, 1abc23, 1a23bc, 12abc3$ (I essentially reasoned that the first value had to be an $a$ or 1, so I organized them grouped based on their starting value.)
This gives me 20 total permutations as a result found by brute force, and I am not sure if I am under counting or if there is a more mathematical way of approaching this permutations problem with regards to ""choosing"" positions and permuting the contents of the sequence.","['permutations', 'combinatorics']"
2264494,Closed form for $\sum_k \frac{1}{\sqrt{k}}$,"I would like to know if there's any equivalence to: $$\sum_{k=1}^n \frac{1}{k} = \log n + \gamma + \frac{1}{2n} - \sum_{k=1}^\infty \frac{B_{2k}}{2kn^{2k}}$$ But to define $E(n)$ in: $$\sum_{k=1}^n \frac{1}{k^{1/2}} =2  \sqrt{n} + \zeta(\frac{1}{2}) + \frac{1}{2\sqrt{n}} + E(n)$$ (For $n$ an integer $n>1$) I would like to express the error term as a sum/series rather than an integral. I did not find anything on the Internet but error terms using big-O notation. The only exact formula I found is: $$\sum_{k=1}^n \frac{1}{k^{1/2}} = \zeta(\frac{1}{2}) - \zeta (\frac{1}{2}, n+1)$$ I do not know hoy to get from there to $E(x)$. Any help? Thank you.","['summation', 'sequences-and-series', 'closed-form']"
2264515,Finding Bayes estimator with inverse-gamma prior and uniform likelihood.,"Consider an i.i.d. sample of data from a population given by 
$$Y_i|\Psi \sim \text{Unif}(0, \Psi), \quad i = 1,2,\ldots, n,$$
with $\Psi$ having a prior distribution given by $$\Psi \sim \text{Inv-}\Gamma(\alpha, \beta),$$
where Inv-$\Gamma$ is the inverse Gamma distribution with pdf 
$$f_{\Psi}(z) = \frac{1}{\beta^\alpha\Gamma(\alpha)}\left(\frac{1}{z}\right)^{\alpha+1}\text{exp}\left\{-\frac{1}{\beta z}\right\}\quad \alpha,\beta > 0.$$ I know that the largest order statistic $Y^{(n)}$ is a sufficient statistic for $\psi$. If I want to find the Bayesian estimator of $\psi$ under the squared error loss, I know that this can be found via the following (rather heavy) integral:
$$\hat{\psi}_B = \int_{Y^{(n)}}^{\infty}\psi\left[\frac{\left(\frac{1}{\psi}\right)^{\alpha + n + 1}e^{-1/({\beta\psi)}}}{\int_{Y^{(n)}}^{\infty}\left[\left(\frac{1}{\psi}\right)^{\alpha + n + 1}e^{-1/({\beta\psi)}}d\psi\right]}\right]d\psi$$ This can be somewhat simplified to 
$$\hat{\psi}_B = \int_{Y^{(n)}}^{\infty}\left[\frac{\psi^{-(\alpha + n
)}e^{-1/({\beta\psi)}}}{\int_{Y^{(n)}}^{\infty}\left[\psi^{-(\alpha + n + 1)}e^{-1/({\beta\psi)}}d\psi\right]}\right]d\psi.$$ Directly solving this integral would be pretty hard I feel. But, the resource I am trying to learn this from claims that now this integral can be expressed as 
$$\hat{\psi}_B = \frac{1}{\beta(\alpha + n -1)}\frac{Pr\left(\chi_{2(\alpha + n -1)}^2 < 2/(\beta y^{(n)})\right)}{Pr\left(\chi_{2(\alpha + n)}^2< 2/(\beta y^{(n)})\right)},$$ where the probabilities above are the probability that a $\chi^2$ distributed random variable with the given degrees of freedom is less than $2/(\beta y^{(n)})$ (where $y^{(n)}$ is the maximum of the sample). I am generally familiar with the fact that the INVERSE-chi-squared distribution has pdf $$f(x|k) = \frac{2^{-k/2}}{\Gamma(k/2)}x^{k/2 -1}e^{-1/2x}.$$ The large and messy integral from above can be rewritten in the form of the Inverse-chi-squared distribution as follows using the transformation $\psi =\frac{\beta x}{2}$ in the pdf of the Inverse-chi-squared distribution given above (this final form is with a lot of factoring out and cancelling of Gamma functions, powers of $\beta$, etc.): $$\hat{\psi}_B = \beta(\alpha + n -1)\int_{\frac{\beta Y^{(n)}}{2}}^{\infty}\left[\frac{x^{-(\alpha + n
)}e^{-1/({\beta x)}}}{\int_{\frac{\beta Y^{(n)}}{2}}^{\infty}\left[x^{-(\alpha + n + 1)}e^{-1/({\beta x)}}dx\right]}\right]dx.$$ It seems that this is tantalizingly close to being able to conclude that this is equivalent to $$\frac{1}{\beta(\alpha + n -1)}\frac{Pr\left(\chi_{2(\alpha + n -1)}^2 < 2/(\beta y^{(n)})\right)}{Pr\left(\chi_{2(\alpha + n)}^2< 2/(\beta y^{(n)})\right)},$$
but I am not quite sure of the details. What route would I need to take to bridge the gap between the integral expression I have and the ratio of probabilities I want to achieve?","['probability-distributions', 'bayesian', 'gamma-distribution', 'statistics', 'probability']"
2264524,Can I use prime factorization to prove that the rationals are countable?,"I know the classic argument for countability of $\mathbb{Q}$ is the zig-zagging traversal, but could I also prove that the rationals are countable using prime factorization of the natural numbers? For instance, defining an injective function $f: \mathbb{Q} \to \mathbb{N}$ where $f(\frac{a}{b}) = 2^{a}3^{b}$. This should work, unless I'm missing something.",['elementary-set-theory']
2264543,Showing that $d$ is a metric,"For any two random variables $X$ and $Y$, denote by $d(X,Y)$ the infimum of such numbers $\epsilon >0$ that $P(|X-Y|>\epsilon)<\epsilon$.Show that $d$ is a metric on the set of classes of equivalent rv's (random variables $X$ and $Y$ are equivalent if $X=Y$ with probability 1). I was able to show the positivity and transitivity of $d$ since it's quite obvious. But how should I show that $d(X,Z)\le d(X,Y)+d(Y,Z)$? Can I assume $P(|X-Y|)=P(|X-Z|)=P(|Y-Z|)$ since $X,Y,Z$ are equivalent?","['probability-theory', 'metric-spaces', 'random-variables']"
2264548,Prove that $\lim_{n\to\infty}\int f_n=\int f$.,"Let $(f_n)$ be a sequence of non-negative Lebesgue measurable functions such that $f_n$ converges to $f$ and $f_n\leq f\ \forall n\in \mathbb{Z^+}$. It is needed to prove that $\lim_{n\to\infty}\int f_n=\int f$. The following is my solution. By Fatou's lemma $\int f=\int (\liminf f_n)\leq \liminf\int f_n$. Since $f_n\leq f\ \forall n\in \mathbb{Z^+}$, $\int f_n\leq\int f$. Therefore $\limsup\int f_n\leq \int f$. Therefore $\limsup\int f_n\leq \int f\leq \liminf\int f_n$. Hence $\lim_{n\to\infty}\int f_n=\int f$. Could someone tell me if my proof is correct? Thanks.","['self-learning', 'lebesgue-integral', 'measure-theory', 'proof-verification']"
2264554,Significance of adding zero rows and columns to a matrix.,"Suppose I have a $n\times n$ matrix to which I add a row of zeros and a column of zeros, somewhere in the matrix, to make it $(n+1)\times (n+1)$. When I multiply the matrix by itself, or multiply it with other matrices where I have inserted a row and column of zeros in the same way, it seems to behave as though the extra row/column were not there. Is there a way to think about the added row/column, and why it does not affect the product? What have I done to this matrix? I think (but not sure at all) that it is like I have put my matrix in a higher matrix space but it only spans the subset where it previously existed?","['matrices', 'linear-algebra']"
2264565,"Pattern ""inside"" prime numbers","Update $(2020)$ I've observed a possible characterization and a possible parametrization of the pattern, and I've additionally rewritten the entire post with more details and better definitions. It remains to prove the observed possible characterization, and then complete it by finding the sequences for locations of the ""parabolic shapes"" that live in the characterized regions. Table of contents: Visualization of factorizations Inside prime visualizations The pattern and the question Pattern parametrization Visualization of factorizations Here I will explain how we can visualize positive integers in an interesting way which also has the property that the factorization of $n$ is encoded into it in the form of a fractal-like pattern. $2$ -digit palindromes represent factorizations We say that a positive integer is palindromic in some integer number base ( number-system ) $b\ge 2$ if when written in that number base, its digits are read the same forward and backwards. The $2$ -digit palindrome is an integer $n=(a,a)_b=ab^1+ab^0=a(b+1),a\lt b,a\ge 1$ , where in the $(n_1,n_2,\dots)_b$ notation, $n_1,n_2,\dots$ stand for digits of $n$ in the integer base $b\ge 2$ . Factorizations of positive integers $n$ are related to $2$ -digit palindromes. That is, if $n$ can be factorized as $n=p\cdot q$ where $p\lt q-1$ , this means that $n$ is a $2$ -digit palindrome in the number base $q-1$ , which we write as: $$
n=p q=p(q-1)+p=(p, p)_{q-1}, p<q-1,
$$ where $(p,p)$ are digits of $n=pq$ in the number base $q-1$ . Representing $n\in\mathbb N$ as a sum of $n\times n$ images (matrices) First we define a ""matrix of point"", then we define the ""sum"" of these matrices. Let $x_0,y_0$ be nonnegative integers. Let $P_n(x_0,y_0)$ be a $n$ by $n$ matrix ""of point $(x_0,y_0)$ "" whose top left entry is denoted with $P_n(x_0,y_0)(0,0)$ and whose bottom right entry is denoted with $P_n(x_0,y_0)(n-1,n-1)$ . It is defined for $x,y\in[0,n)$ as: $$
P_n(x_0,y_0)(x,y)=
\begin{cases}
1, & \text{if }x+nx_0\text{ is a two digit palindrome in base }y+ny_0\\
0, & \text{else}
\end{cases}
$$ We can visualize these ""point"" matrices as images by coloring different entries with different colors.  For example, "" $P_{100}(x_0,y_0)$ ""'s for $(x_0,x_0)=(0,0),(1,0),(2,0),(3,0)$ are: Where the green pixels represent the matrix entries that are $1$ . Notice that $P_n(x_0,y_0)$ in point $(x_0,y_0)=(0,0)$ contains ""dotted lines"" $L_t$ of points from $$\{(t^2-1+(t-1)r,t+r),t\ge 2, r\ge 0\}$$ and that if we look at $P_n(x_0,y_0)$ in some other point, we get continuations of these lines. Visualization of the ""image of $n$ "" matrix We define $N_n(y_0)=\sum\limits_{k=0}^{\infty} P_n(k,y_0)$ as ""entry-wise sum of $P_n$ 's along the $x$ -axis at level $y_0$ "". Notice that this sum always sums finitely many matrices because for large enough $k=x_0$ , all entries of the matrix are $0$ . To visualize $N_n(y_0)$ we will color all $0$ entries white, and all other entries blue. This visualization remains the same for all $y_0\ge 1$ except in the top left entry which is zero only for $y_0=1$ . If we choose $y_0=0$ , then the visualization is ""degenerated"". This motivates us to write $N_n=f(N_n(1))$ and call $N_n$ the ""image of $n$ "", where $f$ returns the same matrix but all nonzero entries are set to ""blue"" and all zero entries are set to ""white"". For example, below is the image showing the $N_n$ 's for $n=7,\dots,31$ . Notice that the prime numbers are solid blue squares with no additional details, and that other numbers have visualizations that correspond to their factorizations. Patterns in $N_n$ based on factorizations of $n$ For example, notice that even semi-primes $n=2p$ where $p$ is prime, all have the following look: Or for example, notice that squares of primes $n=p^2$ have the least details: On the other hand, numbers with a lot of factors such as factorials like $4!=2\cdot3\cdot4 = 24$ or primorials like $p_3\#=2\cdot 3\cdot 5 = 30$ have much more details (factors), and numbers such as powers of primes like $27=3^3$ and $32=2^5$ have regular fractal-like patterns. One might ask if we can for $n$ in general compute the matrix $N_n$ more efficiently than calculating and summing up all those $P_n$ 's. That is, is there an efficient test to get a $N_n(x,y)$ entry directly? $-$ This question does not need to be answered, it's just my thought on the current chapter. Inside prime visualizations The $N_p$ where $p$ is a prime number resembles a solid blue square and looks boring. But, we can ""look inside"" this solid blue square by replacing our $f$ function. Recall that we are WLOG observing $y_0=1$ because all $y_0\ge 1$ yield equivalent images. ""Heat-image"" matrix of prime number $p$ We will write $\overline{N}_p=h(N_p(1))$ and call $\overline{N}_p$ the ""heat-image of prime $p$ "", where $h$ assigns distinct colors to distinct values of entries of $N_p(1)$ and returns such matrix. Let the zero value be ""black"", the smallest nonzero value be ""red"" and the second-smallest nonzero value be ""yellow"". Other values can be ""white"" or represented in shades of ""grey"". For example, here are $\overline{N}_p$ 's of primes $p=101,103,109$ : At first glance these appear to be different sizes of the same image, but this is not the case. Within different $M_p$ 's we find different patterns in certain regions of the image. Thanks to Hyperplane's comment , we can ""enhance"" these regions by removing the needless details. That is, if we ignore the last row then these $\overline{N}_p$ 's appear to be symmetric along the central horizontal line. In other words, if an entry $(x,y)$ is red then the mirrored entry $(x,|p-y-2|)$ is yellow, and vice versa. However, this is not true for all entries. This motivates us to observe the ""non-mirrored"" entries, and set the other ones to $0$ . The pattern inside prime number $p$ We will observe the irregularities in the ""heat-image"". That is, we define a new matrix $M_p$ : $$
M_p(x,y)=
\begin{cases}
1, & \text{if }N_p(1)(x,y) = N_p(1)(x,|p-y-2|)\\
0, & \text{else}
\end{cases}
$$ We will represent these entries by coloring $1$ 's and $0$ 's with white and black, respectively. For example, here are $M_p$ 's of primes $p=101,103,109$ : Observe the white pixels near the central horizontal line. Notice that there we can distinguish between a ""thick dotted parabola"" and a ""thin dotted parabola"". For example, zooming into the central horizontal line of $109$ shows that the ""thick dotted parabola"" one is on the left side, and that the ""thin dotted parabola"" one is on the right side. It is the same for the prime $101$ , but not for the prime $103$ which has it the other way around. It turns out that primes of the form $p=4k+1$ are ""left-handed"" (have the ""thick"" parabola on the left side) and that the primes of the form $p=4k-1$ are ""right-handed"" (have the ""thick"" parabola on the right side). It also turns out that these are not the only ""parabolic"" groups of pixels that exist. I've tried tracing the prime $p=101$ from different groups of pixels and found a set of three distinct parabolas. Observe the last (fourth) image below. These have one of the following ""shapes"" of pixels at their ""head"": The first (""green parabola"") has $I_{2}$ - ""I shape of height of 2 pixels"" The second (""blue parabola"") has $J_{2,1}$ - ""J shape of height of $2+1$ pixels"" The third (""red parabola"") has $I_{3}$ - ""I shape of height of $3$ pixels"" Note that these ""shapes"" are mirrored below/above the central horizontal line. All $6$ permutations of these ""shapes"" can appear in some prime number $p$ (in some $M_p$ ). Depending on the permutation that appears in the image (the matrix), we can determine if the prime number is of form $p=18k+m$ where $m\in\{-1,1,5,7,11,13\}$ . For example, all primes of the form $p=18k+11$ have the same $(I_{2},J_{2,1},I_{3})$ permutation of these parabolic patterns as the prime $101=18\cdot 5+11$ , in that region of $M_p$ . The larger the prime $p$ , the more of these regions we can find. For example, in primes as large as $p=997$ I've highlighted four of these regions.  Some regions repeat multiple times. For example, notice that there are four instances of the yellow region highlighted. If the picture above isn't clear, we can alternatively color (highlight) the regions by connecting the pixels (entries) that represent (""belong to"") corresponding ""parabolic"" shapes: In every region $R$ , we can observe a set of ""parabolic"" shapes. Different permutations of those shapes will appear in different primes $p$ (in different $M_p$ 's), depending on the remainder $m$ from division of $p$ by some constant $c_R$ . The question is now, can we characterize every such region and its shapes? The pattern and the question I think I'm having a hard time characterizing all these regions, because I'm actually not sure how to properly define them. Essentially, it appears $M_p$ contains ""regions $R_s$ "" and within those regions we can find one of the possible permutations $P=(p^{(s)}_1,p^{(s)}_2,\dots,p^{(s)}_s)$ of $s$ ""parabolic shapes"". It seems that region $R_s$ contains some  permutation $P_m$ if and only if $p\equiv m\pmod{c_s}$ where $c_s$ is some constant related to the $s$ th region. Question. $1.$ Can we characterize all regions $R_s$ and their $P_m$ 's and $c_s$ ? The ""pattern inside a prime $p$ "" would be the collection of all regions $R_s$ of the matrix $M_p$ . Note that this is related to Moiré pattern's , as this user pointed out . Below are the regions I've observed so far. Characterization of regions $R_s$ for small $s$ I will be using $P=(1,2,\dots,s)$ as a shortened notation for $P=(p^{(s)}_1,p^{(s)}_2,\dots,p^{(s)}_s)$ . Also, the interpretation of this notation is that $p^{(s)}_1$ is in the leftmost part of the region, and that $p^{(s)}_s$ is in the rightmost part of the region. $$ (s=1) $$ In every $M_p$ , if you look either at the very top or at the very bottom, you will find the same ""half-parabola"" shape $p^{(1)}_1$ . This gives the trivial region: $$
R_1 : P=(1) \iff p \equiv 1\pmod{2}
$$ That is, there is only one possible permutation within $R_1$ , and every prime $p$ has it. $$ (s=2) $$ Given a $M_p$ and looking around the central horizontal line we can find $2$ distinct parabolic shapes. These are the ""thick parabola"" $p^{(2)}_1$ and the ""thin parabola"" $p^{(2)}_2$ . I've observed that: $$\begin{align}
R_2 : P=(1,2) &\iff p \equiv 1\pmod{4} \\
R_2 : P=(2,1) &\iff p \equiv 3\pmod{4} \\
\end{align}$$ This region splits the primes into two sets. $$ (s=3) $$ In the previous chapter we've observed $R_3$ to contain parabolic shapes characterized by a ""short I shape"" = $p^{(3)}_1$ , a ""long I shape"" = $p^{(3)}_2$ and a ""J shape"" = $p^{(3)}_3$ . It appears that: $$\begin{align}
R_3 : P=(1,2,3) &\iff p \equiv 1\pmod{18} \\
R_3 : P=(2,1,3) &\iff p \equiv 5\pmod{18} \\
R_3 : P=(2,3,1) &\iff p \equiv 7\pmod{18} \\
R_3 : P=(1,3,2) &\iff p \equiv 11\pmod{18} \\
R_3 : P=(3,1,2) &\iff p \equiv 13\pmod{18} \\
R_3 : P=(3,2,1) &\iff p \equiv 17\pmod{18} \\
\end{align}$$ This region splits the primes into six sets. Unlike $R_2$ that appears only once, this $R_3$ region appears twice in the image: above and (mirrored) below the central horizontal line. $$ (s=4) $$ The region $R_4$ is similar to $R_3$ but of course has more permutations. This region contains parabolic shapes characterized by following shapes (groups of pixels): ""3 long pixels"" = $p^{(4)}_1$ , ""2 short pixels"" = $p^{(4)}_2$ , ""2 long pixels"" = $p^{(4)}_3$ and ""3 short pixels"" = $p^{(4)}_4$ . The corresponding congurences are: $$\begin{align}
R_4 : P=(1,2,3,4) &\iff p \equiv 1\pmod{16} \\
R_4 : P=(1,4,3,2) &\iff p \equiv 3\pmod{16} \\
R_4 : P=(4,1,2,3) &\iff p \equiv 5\pmod{16} \\
R_4 : P=(2,1,4,3) &\iff p \equiv 7\pmod{16} \\
R_4 : P=(3,4,1,2) &\iff p \equiv 9\pmod{16} \\
R_4 : P=(3,2,1,4) &\iff p \equiv 11\pmod{16} \\
R_4 : P=(2,3,4,1) &\iff p \equiv 13\pmod{16} \\
R_4 : P=(4,3,2,1) &\iff p \equiv 15\pmod{16} \\
\end{align}$$ Interestingly, not all permutations of four shapes are possible to appear in this region. That is, we can see that only $8$ out of theoretical $24=4!$ appear. Also notice that we alternate even and odd numbers (""short"" and ""long"" shapes) in all of these permutations. $$ (s\ge 5) $$ Unexpectedly, unlike $R_3$ and $R_4$ that appear only once (on each mirrored side), we can say that the $R_5$ appears twice (on each mirrored side). But, in each of those two mirrored pairs we have different parabolic shapes. Therefore, we can also say that there are two different $R_5$ regions. So far, I've been manually tracing pixels in many consecutive primes to observe these regions and their permutations. Can we characterize these regions more efficiently? I've examined the $R_5$ region(s) and found that $c_5=50$ , which gives $20$ possible permutations. I've also examined $R_6$ and found $c_6=36$ , which gives $12$ possible permutations. Notice that a pattern emerges here. The $c_s $ appears to follow the sequence: $$
c_s=2, 4, 18, 16, 50, 36,\dots
$$ Which corresponds to only one OEIS sequence, namely the A137933(s) = $c_s = \DeclareMathOperator{\lcm}{lcm}\lcm(s^2,2)$ . If this is true, then the number of possible permutations is $s\phi(s)$ , which is A002618. In other words, it appears that only the permutations (of ""parabolic"" shapes $\{1, 2, 3, \dots, s\}$ ) that are ""in arithmetic progressions modulo $s$ "" can exist within the $R_s$ region! Can we prove that this holds for all $s$ ? Beside finding how the individual parabolic shapes permute within corresponding regions, we also need to find their exact locations within those regions, to complete the characterization. Partition of pixels (entries) into regions $R_s$ An interesting question that we can ask is, can we color every pixel (entry) $(x,y)$ such that it belongs to exactly one (or two neighbouring) regions $R_s$ ? That is, $(x,y)$ should belong to a region if it is a part of a shape that is being permuted in that region, or if it ""extends"" one of those shapes up to the border of a neighbouring region. I've decided to color $R_1,R_2,R_3,R_4$ with $\color{purple}{\text{purple}},\color{red}{\text{red}},\color{green}{\text{green}},\color{blue}{\text{blue}}$ , respectively. I'll use white to color pixels that ""overlap"" (could be in either region) or those that ""are forming next region"". Prime numbers $p=2,3,5$ seem too small so we can start with $p=7,11,13,17,19,23$ . The first three regions are already visible in these small primes. Observing the next few primes $p=29,31,37,41$ , we see that the fourth region emerges too, but still intersects the third region at some parts. It is at primes $p=43,47,53$ when the fourth region should be clearly visible for the first time. The fifth region and beyond will appear at a bit larger primes. If we want to be able to color larger examples, we need to first characterize the regions (answer my first question) and then we need to be able to additionally determine their exact locations within the corresponding region. Question. $2.$ Can we determine locations for the parabolic shapes for given $R_s$ region? I've looked at $R_2$ (first nontrivial region), and it is not hard to see where exactly the two corresponding parabolic shapes will appear. This allowed me to come up with the parametrization of the entire pattern (matrix $M_p$ ) and is presented in the following chapter. Pattern parametrization The central region $R_2$ can be used to generate the entire matrix $M_p$ . This is much more efficient than computing it and might shed some light on the pattern. That is, all regions $R_s$ emerge from the following parametrization. Parametrization of $M_p$ To start, we locate the start (the ""head"") of the ""thin"" and the ""thick"" parabola. The starting $y$ will be given by $y=y_P$ for both, where the starting $x=x_L$ of the leftmost one and the starting $x=x_R$ of the rightmost one are: $$\begin{align}
y_P&=\frac12(p-3)\\
x_L&=\frac14\left(p-(p\bmod{4})_{\in\{-1,1\}}\right)\\
x_R&=p-x_L
\end{align}$$ where the ""mod"" takes values in $\{-1,1\}$ as noted. To generate $M_p$ , we start with a matrix of $0$ 's and fill in the $1$ 's with the following sequences, where the ""mod"" now takes nonnegative values: $$\begin{align}
\left(x^{(i)}_n,y^{(i)}_n\right)&=
\left(
\left(\left(x^{(i)}_{n-1}+(2n-1)\right)\bmod{p}\right),
y^{(i)}_{n-1}+1
\right), i\in\{1,2\},\\
\left(x^{(i)}_n,y^{(i)}_n\right)&=
\left(
\left(\left(x^{(i)}_{n-1}+(2n-1)\right)\bmod{p}\right),
y^{(i)}_{n-1}-1
\right), i\in\{3,4\},\\
\left(x^{(5)}_n,y^{(5)}_n\right)&=
\left(
\left(\left(x^{(5)}_{n-1}+2n\right)\bmod{p}\right),
y^{(5)}_{n-1}+1
\right),\\
\left(x^{(6)}_n,y^{(6)}_n\right)&=
\left(
\left(\left(x^{(6)}_{n-1}+2n\right)\bmod{p}\right),
y^{(6)}_{n-1}-1
\right),\\
\end{align}$$ where the starting conditions are: $$\begin{align}
\left(x^{(i)}_0,y^{(i)}_0\right)&=\left(x_a,y_P+(i\bmod{2})\right),i\in\{1,2,3,4\},\\
\left(x^{(i)}_0,y^{(i)}_0\right)&=\left(x_b,y_P+(i\bmod{2})\right),i\in\{5,6\},\\
\end{align}$$ where $x_a,x_b\in\{x_L,x_R\}$ are the $x$ 's of the ""head""'s of ""thick"",""thin"" parabolas, respectively. A sequence terminates after its $y$ coordinate leaves the $[0,p)$ interval. The only correction that needs to be done is that $M_p(0,0)=M_p(0,p-2)=M_p(0,p-1)=0$ and that $M_p(0,y)=1$ for $y=1,2,\dots,p-3$ . The rest of entries $M_p(x^{(i)}_n,y^{(i)}_n)=1$ are correctly generated by sequences $i=1,2,3,4,5,6$ and $n$ 's up until the $y$ escapes $[0,p)$ . These sequences represent extensions of the ""thick"" and ""thin"" parabolas from $R_2$ , that loop around the right and left edges of the image (matrix) up until the top (bottom). As these sequences intertwine, they will produce all other regions $R_s,s\ne 2$ . That is, all regions emerge from these sequences of pixels (entries). Can we use this to help us predict the characterizations of regions $R_s$ ?","['prime-numbers', 'number-theory', 'visualization', 'palindrome', 'number-systems']"
2264614,Continued Cosine Product. [duplicate],"This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 7 years ago . Is there a way to evaluate, $$
\large \cos x \cdot \cos \frac{x}{2} \cdot \cos \frac{x}{4} ... \cdot \cos \frac{x}{2^{n-1}} \tag*{(1)}
$$ I asked this to one of my teachers and what he told is something like this, Multiply and divide the last term of $(1)$  with  $\boxed{\sin \frac{x}{2^{n-1}}}$ So,
$$
\large \frac{\cos \frac{x}{2^{n-1}} \cdot \sin \frac{x}{2^{n-1}}}{\sin \frac{x}{2^{n-1}}} \\ \tag*{(2)}
$$
$$
\large \implies \frac{\sin (2x)}{2^n \cdot \sin \frac{x}{2^{n-1}}} \\
$$
$$
\large \implies \frac{\sin (2x)}{2^n \cdot \frac{\sin \frac{x}{2^{n-1}}}{\frac{x}{2^{n-1}}} \cdot \frac{x}{2^{n-1}}} \tag*{(3)}
$$ Now, as $n \to \infty$ , we have $x \to 0$,
Using this, $\lim$ we have, $$
\boxed{ \lim_{x \to 0} \frac{\sin x}{x} = 1}
$$ Using this in $(3)$, we have, 
$$
\large \boxed{\frac{\sin (2x)}{2x}} \tag*{(4)}
$$ All the steps sort of make sense. My doubts are, How do I do this for other trigonometric ratios? How does the step 2 happen? I need help looking into it more intuitionally. Please provide necessary reading suggestions. Regards.","['algebra-precalculus', 'trigonometry']"
2264627,A matrix is a product of nilpotent matrices iff its not invertible,"I just completed a homework problem which proves the following result: a matrix (with coefficients in some field) is a product of nilpotent matrices iff its not invertible. The proof was broken into several parts and was quite involved. I'm wondering if there's a very simple way to demonstrate this result (just the implication non-invertible then its a product of nilpotent matrices , as the other implication is trivial).","['matrices', 'linear-algebra']"
2264645,Intersection with paraboloid,"This problem feels really easy but I've been having a really hard time with it. I'm given an equation of a paraboloid $z=x^2+4y^2$ and told that an unknown plane, perpendicular to the $xy$ plane has a point $(2,1,8)$ in common with the paraboloid. The intersection between the plane and the paraboloid is a parabola with slope $0$ at the given point. I'm told to find the equation of the plane. I've tried using the gradient vector but I found out that my approach is wrong. I tried to explicitly find the intersection between a plane with an equation $y=ax+d$ and the paraboloid equation, then differentiate it once to find out what $a$ and $d$ are so that the slope in $(2,1,8)$ is $0$, looking at the graphs in Mathematica it seems that I've got it wrong with both approaches. Looking for any suggestions on this, I'm really lost. Edit: some information on the gradient approach. I calculated $\nabla z(x,y)=(2x,8y)$, then substituted $x$ and $y$ for $2$ and $1$ respectively. This should be perpendicular to the level curve $8=x^2+4y^2$, if I'm thinking correctly. Therefore, I can define a plane using $\nabla z(2,1)=(4,8)$ and using the fact that we know the plane is orthogonal to $xy$, therefore we use $(4,8,0)$ as the normal vector to the plane. So, by my chain of thought, the plane equation should be $4x+8y+d=0$, substituting $x$ and $y$ for $2$ and $1$ we get $d=-16$. Unless I messed up my Mathematica plot lots of times, this isn't right...",['multivariable-calculus']
2264667,Calculate $\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}$,"$$\displaystyle\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}=I_t=?$$ Attempt 1: If we apply substitution $x=1/u$, then our integral will be like following; $$\displaystyle\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}=-\displaystyle\int_{t}^{1/t}\dfrac1{u^2}\dfrac{du}{\left(\dfrac{1+u^2}{u^2}\right)\left(\dfrac{1+u^t}{u^t}\right)}=\displaystyle\int_{1/t}^t\dfrac{du}{\frac{(u^2+1)(u^t+1)}{u^t}}$$ But this doesn't make any sense: $I_t=\displaystyle\int_{1/t}^t\dfrac{u^tdx}{(u^2+1)(u^t+1)}=\displaystyle\int_{1/t}^t\dfrac{x^tdx}{(x^2+1)(x^t+1)}=^?\displaystyle\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}$ Attempt 2: $$\dfrac{1}{(x^2+1)(x^t+1)}=\dfrac{Ax+B}{(x^2+1)}+\dfrac{h_{t-1}x^{t-1}+h_{t-2}x^{t-2}+...+h_1x+h_0}{(x^t+1)}$$$$\to$$ $$A+h_{t-1}=0\\B+h_{t-2}=0\\B+h_0=0\\A+h_1=0$$
other $h_i$ are $0$;
$$\displaystyle\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}=\displaystyle\int_{1/t}^t\left(\dfrac{Ax+B}{(x^2+1)}+\dfrac{h_{t-1}x^{t-1}+h_{t-2}x^{t-2}+...+h_1x+h_0}{(x^t+1)}a\right)$$$$\to$$$$ \displaystyle\int_{1/t}^t\dfrac{dx}{(x^2+1)(x^t+1)}=\displaystyle\int_{1/t}^t\left(\dfrac{Ax+B}{(x^2+1)}-\dfrac{Ax^{t-1}+Bx^{t-2}+Ax+B}{(x^t+1)}\right)$$ After this, I tried to apply substitution of trigonometric forms etc., but I failed.","['integration', 'definite-integrals', 'calculus']"
2264695,How small the probability $\Bbb{P}(X_1+X_2 +\dots +X_n < n + 1)$ can be if $\Bbb{E}(X_i) = 1$?,"Let $X_1, X_2, \dots, X_n$ be $n$ nonnegative independent identically
  distributed random variables with the same expectation:  $$\forall 1
\le i \le n: \quad \Bbb{E}(X_i) = 1$$ How small the probability $\Bbb{P}(X_1+X_2 +\dots +X_n < n + 1)$ can
  be? Ideally, for the fixed $n$, we need to find an infimum of this probability over all possible distributions of $X_i$ and then see if the lower bound can ever be achieved. The natural idea is to rewrite the probability in question in the following form:
$$\Bbb{P}\left(\dfrac{X_1+X_2 +\dots +X_n}{n} < 1 + \dfrac{1}{n}\right)$$
Here appears the average of $X_1, X_2, \dots, X_n$, but I'm not sure what can be done next. I'm also interested in solving the generalized version of the problem for random variables that are not necessarily identically distributed. Any suggestions, hints on how to approach this problem, and useful comments would be greatly appreciated.","['probability', 'optimization', 'upper-lower-bounds']"
2264702,Shortest distance from point to curve,"I could use some help solving the following problem. I have many more like this but I figured if I learn how to do one then I can figure out the rest on my own. Thanks in advance! A curve described by the equation $y=\sqrt{16x^2+5x+16}$ on a Cartesian plane. What is the shortest distance between coordinate $(2,0)$ and this line?","['algebra-precalculus', 'linear-algebra', 'calculus']"
2264703,differential equation with reciprocal and a linear function in the derivative,"My question is how to solve
$$ \frac{dx(at+b)}{dt} =\frac{c}{x(t)} $$ I'm aware that when $a=1, b = 0$, the solution to this problem is $x(t)=\sqrt{2(ct+k)}$ But in general, does this differential equation have an solution when $a \neq 1$ and $b \neq 0$?",['ordinary-differential-equations']
2264710,Is this the odd expansion of the function $f(x)=x$ if $0\leq x<1$ and $f(x)=1$ if $1\leq x<2$?,"We have the function $f(x)=\left\{\begin{matrix}
x &, 0\leq x<1 \\ 
1 &, 1\leq x<2 
\end{matrix}\right.$ I want to compute the Fourier sinus series with period 4. Is the odd expansion of the function the following? $h(x)=\left\{\begin{matrix} 
-1 &, -2<x<-1 \\ 
x& , -1<x<0 \\ 
0 & , x=0,2 \\ 
x &, 0\leq x<1 \\ 
1 &, 1\leq x<2 
\end{matrix}\right.$ The graph of that function is this: I think that it is wrong, since a periodic function must be the same for each period. But in this case the line segment $y=x$ is getting higher at each interval. Can that be?","['fourier-series', 'analysis']"
2264765,$f= e^{-x}$ is not a contraction but $f^2$ is,Let $f: \mathbb R \rightarrow \mathbb R$ and $f(x)=e^{-x}$ $a)$ Show that f is not a contraction $b)$ Show that $f^2$ given by $f(x):=e^{-e^{-x}}$ is a contraction My attempt $f$ is a contraction if we can find a $c<1$ such that $$|f(x)-f(y)|<c|x-y|$$ $a)$ Let $|f(x)-f(y)|=|e^{-x}-e^{-y}|$ If $x=0$ and $y=-3$ then $$|e^0-e^{-(-3)}|>|0-(-3)|$$ Which is a counter-example $b)$ Not sure what to do here...,"['general-topology', 'real-analysis', 'metric-spaces']"
2264774,Showing that a discontinuous function is or is not borel measurable.,"So my question is this. Please let me know if my answer is sufficient. Let
$$ 
f(x) = 
     \begin{cases}
      \sin(\frac{1}{x})  &\quad\text{if } x \neq 0,\\
       \text{5} &\quad\text{if } x = 0\\
     \end{cases} 
$$
be defined on the whole real line. Is $f$ Borel measurable. My way of thinking is to use the definition of measurable function; Suppose we have a set X together with a sigma-algebra $\Sigma$. Function $f:X \rightarrow \mathbb{R}$ is measurable or $\Sigma$-measurable if the set $\{x:f(x)>a\}$ belongs to $\Sigma$ for all $a \in \mathbb{R}$. Therefore, from the question, I obtained the two following facts; If $a \geq 5$ then $\{x \in \mathbb{R}: f(x) > a\}=\emptyset$ Surely, this is all that is required to show that it is not Borel measurable as the empty set doesn't generate a sigma algebra.","['borel-sets', 'lebesgue-measure', 'measure-theory']"
2264776,"Prove that for graph $G$, $|E(G) \ge |V(G)|−1 $ implies that $G$ is connected","How do we prove that for graph $G$, If $G$ is acyclic and $|E(G)| \ge |V(G)|−1$ it implies that $G$ is
  connected? I saw that there is a similar question about a proof going the other way here: Show that any connected graph $G$ satisfies $\lvert E(G)\rvert \geq \lvert V(G)\rvert -1 $ I would think that the proof would be similar, but I'm having a very hard time with it. Another question that I have is how to prove that for graph $G$, if $G$ is connected and $|E(G)| \ge |V(G)|−1$ it implies that $G$ is acyclic? I would assume that this should be very similar, but I'm not really sure.","['graph-theory', 'discrete-mathematics']"
2264823,Black magic behind Padé approximation,"In condensed matter physics one usually calculates the so-called Matsubara Green's function in the set of discrete points $G (i \omega_n)$, where
$$
\omega_n = (2n+1) \pi T
$$ Physically significant Green's function, however, is retarded related as follows
$$
G_R (\omega) = \lim_{\varepsilon \to 0} G (\omega + i \varepsilon)
$$
assuming that the unique analytical continuation $G (z)$ exists (it seems it does) with the property $G (z) \equiv G_R (z)$ in the upper half-plane and $G (z) \equiv G_A (z)$ in the lower half-plane. The task usually is: given set of values $G (i \omega_n)$ (computed in some previous simulation), determine $G (z)$ (and all the other Green's functions, if that's one's desire). The underlying theorem about existence and uniqueness of such continuation is of Baym, Mermin in 1961 paper Determination of Thermodynamic Green's Functions in Journal of Mathematical Physics. The theorem, however useful, does not give the answer to ""how to practically obtain such continuation?"" What physicist do is the Pade approximation. Recipe would be: define the function $P (z)$ as follows
$$
P (z) \equiv \frac{p_0 + p_1 z + \cdots + p_n z^n}{q_0 +q_1 z + \cdots + q_m z^m}
$$ Now set up appropriate number of equations in the form
$$
G (i \omega_0) = P (i \omega_0), \quad G (i \omega_1) = P (i \omega_1), \quad \cdots
$$ Solve for the unknown coefficients $\left\{ p, q \right\}$. Plugging the real frequency with a small imaginary part into $P$ should yield the result for the retarded and advanced Green's function
$$
G_R (\omega) = P (\omega + i 0^+), \quad G_A (\omega) = P (\omega - i 0^+)
$$ And here is where the magic comes: it works. This might sound silly, but recently I've been playing with this in the following way. I made up a very ugly function $A(x)$ (e.g. sum of 20 Gaussians with random parameters). Then I pretended that this is the spectral function for some Green's function, so I calculated the Matsubara Green's function
$$
G (i \omega_n) = \int \limits_{-\infty}^\infty \frac{\mathrm{d} \omega \, A (\omega)}{i \omega_n - \omega}
$$ Then I calculated the Pade approximation $P (z)$, obtaining $G_R (\omega)$ and finally getting back my original function by the means of Sokhotskij-Plemelj theorem:
$$
A (x) = - \frac{1}{\pi} \lim_{\varepsilon \to 0^+} G_R (\omega + i \varepsilon)
$$ The ""data"" $G (i \omega_n)$ had to be calculated VERY precisely - at least 20 or 30 significant digits. The number of equations for the Pade approximation had to be at least 40 to reproduce the original function. What baffles and astounds me most is, that for every temperature, i.e. very small like $10^{-5}$, or big, like $1$, this worked, provided the data $G (i \omega_n)$ is precise enough and the number of equations is sufficient. This is quite strange, as for the very low temperatures, what you really approximate is very small portion of the Matsubara Green's function, imagine a function like $1/(1+x^2)$ and now take the first 50 Matsubara points, however at the temperature $10^{-5}$, so what you see is almost a constant. Yet the recipe works. Now imagine some large temperature, like $1$ and do the same. It still works. If someone wants to play with that, I share my Mathematica code with you. Copy it to your notebook and save the notebook somewhere. This generates some funny function $A(x)$ and plots it: n = 20;
SetDirectory[NotebookDirectory[]];
sig = Table[RandomReal[] + 0.1, {i, 1, n}];
x0 = 4 Table[RandomReal[] - 0.5, {i, 1, n}];
h = 2 Table[RandomReal[] - 0.5, {i, 1, n}];
A[x_] := Sum[h[[i]] Exp[-((x - x0[[i]])^2/sig[[i]]^2)], {i, 1, n}]
Plot[A[x], {x, -5, 5}, PlotRange -> {-5, 5}] The following code generates data for Matsubara Green's function at temperature $T$ T = 1/1000; w[l_] := (2 l + 
    1) Pi T; wmax = 1; k = 100; acc = 10;
Monitor[MatsG = 
   Table[{w[l], 
     NIntegrate[A[x]/(
      I w[l] - x), {x, -Infinity, Infinity}, 
      WorkingPrecision -> acc, PrecisionGoal -> acc, 
      AccuracyGoal -> acc, MaxRecursion -> 500]}, {l, 1, 2 k}];, l] This creates the continuation GComplex (I'm using $m = n+1$, because Green's function must approach zero for $|z| \to \infty$ and I've set $q_m = 1$, so I don't make it an eigenvalue problem). Just keep the number acc high enough so Mathematica doesn't complain about the bad conditioned matrix. $2k$ is the number of equations, you can tweak that to see what it does: k = 30; acc = 300;
ClearAll[p]
f[w_] := Subscript[p, 0] + Sum[Subscript[p, j] w^j, {j, 1, k - 1}];
g[w_] := Subscript[q, 0] + Sum[Subscript[q, j] w^j, {j, 1, k - 1}] + 
   w^k;
vars = Flatten@
   Table[{Subscript[p, i], Subscript[q, i]}, {i, 0, k - 1}];
eqns = Table[
   SetAccuracy[
    f[I MatsG[[i + 1, 1]]] == 
     MatsG[[i + 1, 2]] g[I MatsG[[i + 1, 1]]], acc], {i, 0, 2 k - 1}];
sol = Solve[eqns, vars, WorkingPrecision -> acc];
GComplex[w_] := f[w]/g[w] /. First@sol; And finally comparing $A(x)$ with the newly generated function: Plot[{A[x], -(1/Pi) Im[GComplex[x]]}, {x, -6, 6}, 
 PlotRange -> {-5, 5}] I'm very sorry if this is considered an off-topic here, but I'm really surprised this works in every case of the trial function $A(x)$ I did this with and I want to know what theory is behind this black magic box. Does anyone know? Thank you.","['complex-analysis', 'analytic-functions', 'analytic-continuation']"
2264867,Prove that if $P(A^c∩B)=P(A^c)P(B)$ then $P(A∩B)=P(A)P(B)$.,"Please help me to prove that if $P(A^c∩B)=P(A^c)P(B)$ then $P(A∩B)=P(A)P(B)$. This is what I have now: $$P(A∩B) = P(B) - P(A^c∩B)$$
$$P(A∩B) = P(B) - P(A^c)P(B)$$
$$P(A∩B) = P(B)(1-P(A^c))$$","['proof-writing', 'probability', 'discrete-mathematics']"
2264907,Products of distributions in QFT,"In Quantum Field Theory quantum fields are operator valued distributions. Namely, given the Schwartz space $\mathcal{S}(M)$ defined on Minkowski spacetime $M$, fields are continuous linear maps $\phi : \mathcal{S}(M)\to \mathcal{L}(\mathcal{H})$ which gives back for each function one operator in some Hilbert space. When just free fields are considered this is just fine. Now, when one considers interactions, the issue that appears is that we have to deal with products of quantum fields. For instance, the $\phi^4$ theory has the interaction given by $\lambda \phi^4$. The scalar Yukawa theory also has products of fields. The field equations become non-linear in the fields. The issue now is that if quantum fields are distributions, then necessarily we can't deal with these terms, because products of distributions is not defined. Physicists just ignore this most of the time. My question here is: is there a rigorous known way to deal with this? Can we make this rigorous someway, even if we need to go into approximate theories? Or there is no way currently known to make this rigorous? Edit: For more information on this matter, I've asked on Physics.SE about the issues with interacting QFT and found out the main problems. This product problem seemed the worst, and since the point here is about whether or not a mathematical construct can be made rigorous I thought that Math.SE could be a good place to discuss the matter further.","['functional-analysis', 'distribution-theory', 'mathematical-physics', 'quantum-field-theory']"
2264922,"Novel(?) method of generating Motzkin numbers, Catalan partial sums, and other sequences","For solutions to the equations (1/X)+(1/Y)=K  and (X+Y)=1+(1/k) the answer is a pair of points. The X and Y values of a point represent stable points in a two-cycle orbit of the logistic equation using K as input. For example, for K=3.14159 the results are .781031... and .53727... which are indeed the two values obtained by plugging 3.14159 into the logistic equation. For large values of K these solutions often generate sequences embedded into the decimal representation of the points. For example, for k=10)        .111251780630393896979680584629184521

k=100)      0.1010102040921523031574757410327e-1

k=1000)     0.1001001002004009021051127323837e-2

k=10000)    0.00010001000100020004000900210051 as K increases by powers of 10 the number of padding zeros increases but more terms of the sequence are observable. In this case the sequence is: 1,1,1,2,4,9,21,51,127,323,835... which is equivalent to Sloane's A001006 , the Motzkin numbers (with an additional leading '1').
 For K= 99999999999999999999999999999999999999999 one of the points results in a sequence of: 1,2,4,9,23,65,197,626,2056... which is Sloane's A014137 , partial sums of Catalan numbers. As you might guess, various other large K repdigits and near repdigits create their own sequences. In short, can anyone point me to references on this or related items? I've had little luck looking online and I'm not a mathematician so my knowledge of the literature is limited. P.S. I generate solutions using Maple: proc (z) -> solve({x+y = 1+1/z, 1/x+1/y = z}, {x, y}) end proc;","['number-theory', 'combinatorics', 'reference-request']"
2264936,Is there a straight forward way to prove that $\frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{31} > 3$,"Using brute force, it is straight forward to calculate that  $\frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{31} > 3$ Is there a more elegant way to demonstrate this? What if I want to find the smallest $n$ such that $\frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} > 4$ Is there a standard way to solve for $n$ without using brute force?","['combinatorics', 'inequality', 'harmonic-numbers']"
2265030,Let $f$ entire funtion such that $\left|f(z)\right|=1$ when $|z|=1$. Then $f(z)=Cz^{m}$ for some $m\in \mathbb{N}$ and $C\in\mathbb{C}$ with $|C|=1$.,Let $f:\mathbb{C}\rightarrow\mathbb{C}$ entire function such that $\left|f(z)\right|=1$ when $|z|=1$ . Show that $f(z)=Cz^{m}$ for some $m\in \mathbb{N}$ and $C\in\mathbb{C}$ with $|C|=1$ . My attempt :  Since $f$ is entire we have that power series at $0$ $$f(z)=\sum_{n=0}^{\infty}c_{n}z^{n}$$ is convergent in all $\mathbb{C}$ . I show that $|c_{n}|\leq 1$ for all $n\in\mathbb{N}$ .,"['complex-analysis', 'entire-functions', 'complex-numbers']"
2265092,"Why is the set of all functions mapping $\{0,1\}\to\Bbb N$ countable, but the set of all functions mapping $\Bbb N\to\{0,1\}$ uncountable?","Is there an intuitive way to think about this question? I understand that for the set of all functions mapping $\{0,1\}\to\mathbb{N}$, you can think of each function as a tuple of $(0, 1),(1,1),(0,2),\ldots$. However, doesn't think same logic apply to the set of all functions mapping $\mathbb{N}\to\{0,1\}$, where you can think of each function as a tuple (some natural number, $0$ or $1$)?","['cardinals', 'elementary-set-theory']"
2265122,A question regarding formulating a PDE for a minimization problem.,"Question : I'd like to formulate a pde for the following minimization problem. Let $\Omega$ be a convex, closed, compact set in $\mathbb{R}^d$ with a smooth boundary. Given a data $(x_i,d_i)$, $x_i \in \Omega^{\mathrm{o}} $ ,$d_i \in \mathbb{R}$, $i = 1,2,3...N$, $N>d$ and $\sum\limits_{i=1}^N d_i = 0$. Also given that, there are always $d$ vectors in $\{x_i\}$ which are linearly independent. Let $A = \int_{\Omega}dx$ I want to find a continuous function $f:\Omega \to \mathbb{R}$, such that, $\int_{\Omega}f(x)dx = 0$ and $C(f)$ is minimum, where $$C(f) = \frac{A^{\frac{1}{d+1}}}{N}\left\{\sum\limits_{i=1}^N |f(x_i)-d_i|^{d+1}\right\}^{\frac{1}{d+1}} +\|f\|_{L^{d+1}}+ A^{\frac{1}{d+1}} \||\nabla f|\|_{L^{d+1}}$$ The minimum exists, and is unique and is atleast Holder continuous with $\alpha = \frac{1}{d+1}$, due to Sobolev embedding theorem and Morrey's inequality Reference :  This Q&A from math.stackexchange. Motivation : I am interested in a stronger result, that is the minimum $f_{min}$ is atleast Lipschitz. We havent leveraged the fact that it is a minimum, while arriving at the weaker result that it is Holder continuous.","['multivariable-calculus', 'real-analysis', 'calculus-of-variations', 'partial-differential-equations']"
2265141,"Is $\mathbb{R}/\mathord{\sim}$ a Hausdorff space if $\{(x,y)\!:x\sim y\}$ is a closed subset of $\mathbb{R}\times\mathbb{R}$?","Let $\sim$ be an equivalence relation on a topological space $X$ such that $\{(x,y)\!:x\sim y\}$ is a closed subset of the product space $X\times X$.
It is known that if $X$ is a compact Hausdorff space then the quotient space $X/\mathord{\sim}$ is normal, and hence Hausdorff. It is also known that $X/\mathord{\sim}$ need not to be Hausdorff if $X$ is not normal. But what about $X=\mathbb{R}$? Is non-normality a necessary condition in the last example? Question: Is the quotient space $\mathbb{R}/\mathord{\sim}$ Hausdorff if $\sim$ is an equivalence relation on $\mathbb{R}$ such that $\{(x,y)\!:x\sim y\}$ is a closed subset of $\mathbb{R}\times\mathbb{R}$? If an equivalence relation $\sim$ is closed in $X\times X$ then each equivalence class is closed, and hence $X/\mathord{\sim}$ is $T_1$.
The Hausdorffness of $X/\mathord{\sim}$ is equivalent to the property that any two points of $X$ can be separated by open saturated neighborhoods (a set $A\subseteq X$ is saturated iff $(x\in A\wedge x\sim y)\Rightarrow y\in A$ holds true for all $x,y\in X$). First attempt to find a counterexample failed.
Assuming that $\sim$ is closed in $X\times X$, one can show that $X/\mathord{\sim}$ is Hausdorff if $X$ is Hausdorff and the quotient mapping $q\colon X\to X/\mathord{\sim}$ is open ( here ), or if $X$ is normal and $q$ is closed (this is essentially proved here ). It is not difficult to find an equivalence relation $\sim$ that is closed in $\mathbb{R}\times\mathbb{R}$ and the quotient mapping $q\colon\mathbb{R}\to\mathbb{R}/\mathord{\sim}$ is neither open nor closed. An example is $x\sim y$ iff $x=y\vee(\sin x=\sin y\le 0)\vee(\sin x=\sin y=1)$, but $\mathbb{R}/\mathord{\sim}$ is Hausdorff in this case. I will appreciate any advices, suggestions, or references.","['general-topology', 'quotient-spaces']"
2265143,(Geometric intuition) Line integral over vector fields,"I'm trying to understand the geometric intuition behind the definition of the line integrals over vector fields. The definition is given below: Definition: Let $\vec{F}$ be a continuous vector field defined on a smooth curve $\gamma$ given by a vector function $r(t)$ . Then the line integral of $\vec{F}$ along $\gamma$ is $$\int_{\gamma}\vec{F}\cdot d\vec{r}=\int_{\gamma} \vec{F}\cdot\vec{T} ds$$ Where $T$ is the unit tangent. So the line integral of the vector field $\vec{F}$ along $\gamma$ is defined as line integral over a scalar field. The geometric interpretation of this one  can be found here . So using the geometric interpretation of line integrals over scalar fields, I'm trying to understand this one over vector fields. In the definition above the scalar product $\vec{F}\cdot \vec{T}$ is a function $\alpha(x,y)$ which takes a point in the curve $\gamma$ and gives out a point with $\alpha(x,y)=|\vec{T}|$ (Since $T$ is a unit vector, $\vec{F}\cdot \vec{T}$ is the length of the projection of the vector $\vec{F}(x,y)$ over the tangent). So using the geometric interpretation of the line integral over scalar fields, is the integral $\int_{\gamma}\vec{F}\cdot d\vec{r}$ the area below the curve $\alpha$ ? If yes, why is this geometric relevant?","['multivariable-calculus', 'intuition', 'geometric-interpretation', 'line-integrals']"
2265149,"Prob. 3, Chap. 5 in Baby Rudin: Any positive real number and any real function with bounded derivative together give rise to an injective function","Here is Prob. 3, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $g$ is a real function on $\mathbb{R}^1$, with bounded derivative (say $| g^\prime | \leq M$). Fix $\varepsilon > 0$, and define $f(x) = x + \varepsilon g(x)$. Prove that $f$ is one-to-one if $\varepsilon$ is small enough. (A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.) My Attempt: Suppose $x, y \in \mathbb{R}$ such that $x < y$. Then $g$ is continuous on $[x, y]$ and differentiable on $(x, y)$. So, by the Mean Value Theorem, there is some point $p \in (x, y)$ such that $$ g(y) - g(x) = (y-x) g^\prime(p).$$
  And so, 
  $$ f(y) - f(x) = y-x + \varepsilon \left( g(y) - g(x) \right) = (y-x)\left( 1 + \varepsilon g^\prime(p) \right).$$
  Since $-M \leq g^\prime(t) \leq M$ for all $t \in \mathbb{R}$ and since $y-x > 0$, therefore, 
  $$ (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right). $$
  So if we choose $\varepsilon$ so that $$0 < \varepsilon < \frac{1}{M},$$ then we note that 
  $$0 < (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right) $$
  for any real numbers $x$ and $y$ such that $x < y$; that is, $f(x) < f(y)$ for any real numbers $x$ and $y$ such that $x < y$, which implies that $f$ is strictly increasing and thus one-to-one. Is this proof correct?","['derivatives', 'real-analysis', 'calculus', 'analysis']"
2265166,"Prob. 4, Chap. 5 in Baby Rudin: The existence of a solution in $(0, 1)$","Here is Prob. 4, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $$ C_0 + \frac{C_1}{2} + \cdots + \frac{C_{n-1}}{n} + \frac{C_n}{n+1} = 0,$$ where $C_0, \ldots, C_n$ are real constants, prove that the equation 
  $$ C_0 + C_1 x + \cdots + C_{n-1}x^{n-1} + C_n x^n = 0$$ 
  has at least one real root between $0$ and $1$. My Attempt: Let $f$ be the real function defined on $[0, 1]$ by the equation $$f(x) \colon=  C_0 x + \frac{C_1 x^2 }{2} + \cdots + \frac{C_{n-1} x^n}{n} + \frac{C_n x^{n+1} }{n+1}. $$
  Then $f(0) = 0$, and also $f(1) = 0$, by our hypothesis. Moreover, $f$ is continuous on $[0, 1]$ and differentiable on $(0, 1)$. So by the Mean Value Theorem there is a point $p \in (0, 1)$ such that 
  $$ f(1) - f(0) = (1-0) f^\prime(p),$$
  that is, there is a point $p \in (0, 1)$ such that 
  $$ f^\prime(p) =  C_0 + C_1 p + \cdots + C_{n-1} p^{n-1} + C_n p^n = 0,$$ 
  as required. Is this proof correct?","['derivatives', 'real-analysis', 'calculus', 'analysis']"
2265169,What motivated Cantor to show the reals are uncountable?,"When you study measure theory, the importance of distinguishing different kinds of infinite sets becomes apparent. You want to be able to measure ""reasonably nice"" subsets of the real line in such a way that $[0,1]$ has measure one.  If you have an infinite collection $E, E', ...$ of disjoint subsets, you want the measure of their union to be the sum of their measures.  The measure should be translation invariant. Without the knowledge that there exist uncountable subsets of the real line, this problem would seem impossible: all points have the same measure by translation invariance, and must all have measure zero in order for infinite sets to have finite measure.  But then the measure of $[0,1]$ should be zero, and not one, because its measure is the sum of the measures of the individual points. So, cardinality can be strongly motivated by some concepts in measure theory.  But abstract measure theory did not come about until after Georg Cantor.  What concepts or problems in the late nineteenth century could have motivated Cantor to come up with his results about uncountable sets?","['math-history', 'cardinals', 'measure-theory']"
2265176,Find $\lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \ln\left(\frac{k}{n} + \epsilon_n\right)$ if $\epsilon_n>0$ and $\epsilon_n\to0$,"Let $\epsilon_{n}$ be a sequence of positive reals with $\lim\limits_{n \rightarrow \infty} \epsilon_{n}=0$ . Then find $$\lim_{n \rightarrow \infty} \frac{1}{n} \sum_{k=1}^{n} \ln\left(\frac{k}{n} + \epsilon_n\right)$$ My doubt here is that , can I introduce the limit within the summation?
Then it can be easily found by converting the sum into an integral.","['sequences-and-series', 'limits']"
2265178,"Prob. 5, Chap. 5 in Baby Rudin: If $f^\prime(x) \to 0$ as $x \to +\infty$, then $f(x+1) - f(x) \to 0$ as well.","Here is Prob. 5, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined and differentiable for every $x > 0$, and $f^\prime(x) \to 0$ as $x \to +\infty$. Put $g(x) = f(x+1)-f(x)$. Prove that $g(x) \to 0$ as $x \to +\infty$. Here is an attempt of mine. As $$\lim_{x \to +\infty} f^\prime(x) = 0,$$ 
  so, given any real number $\varepsilon > 0$, we can find a real number $r$ such that $$\left\vert f^\prime(x) - 0 \right\vert < \varepsilon  \tag{1} $$
  for all real numbers $x$ which satisfy $x > r$. Let  $x$ be a real number such that $x > r$. Then as $f$ is continuous on $[ x, x+1]$ and differentiable on $(x, x+1)$, so by the Mean Value Theorem there is some point $p \in (x, x+1)$ for which 
  $$g(x) = f(x+1) - f(x) = \left( \ (x+1) - x \ \right) f^\prime(p) = f^\prime(p), $$ 
  and as $p > x > r$, so by (1) we can conclude that 
  $$\left\vert g(x) \right\vert = \left\vert f^\prime(p) \right\vert < \varepsilon.$$ 
  Thus, given a real number $\varepsilon > 0$, we can find a real number $r$ such that 
  $$\left\vert g(x) - 0 \right\vert < \varepsilon$$ 
  for all real numbers $x$ which satisfy $x > r$. Hence $$\lim_{x \to +\infty} g(x) = 0.$$ Is this proof correct?","['derivatives', 'real-analysis', 'calculus', 'analysis']"
2265192,Solve for n for loan amortization,Can someone help with rearranging the following formula to find n with explanation? $$PMT =   \frac{PV}{\frac{1}{r} \cdot(1- \frac{1}{(1+r)^{n}})}$$,"['algebra-precalculus', 'finance']"
2265236,Compactness properties of rational sequence topology,"For each irrational ${x}$ we take a sequence of rational numbers ${x_k}$ with the property that ${x_k}$ converges to it in the Euclidean topology. The rational sequence topology is given by defining each rational number singleton to be open, and using ${\displaystyle U_{n}(x):=\{x_{k}:n\leq k<\infty \}\cup \{x\}}$ as a basis for irrational point ${x}$ . Proof that this topology is not metacompact but countably metacompact. Will it be countably paracompact ? (this example is example #65 in Steen, L. A.; Seebach, J. A. (1995), Counterexamples in Topology )",['general-topology']
2265248,About definition of Fuchsian group derived from a quaternion algebra,"In Katok's book, we have a defintion as following: If $\Gamma$ is a subgroup of finite index of some
  $\Gamma(A,\mathcal{O})$, then we call $\Gamma$ a Fuchsian group
  derived from a quaternion algebra $A$. We know that subgroup of a Fuchsian group is also a Fuchsian group. So, what is the motivation of ""subgroup of finite index"" in that definition?","['hyperbolic-geometry', 'quaternions', 'group-theory']"
2265255,Mapping a 3D point inside a hexahedron to a unit cube,"Let A 4x8 be a hexahedron defined by 8 points in 3D homogeneous coordinates. M 8x8 is a map that transforms the hexahedron to a unit cube, B 4x8 . If A = B . M , then M = B -1 . A How can I map a 3D point, P 4x1 in A to B using M ? Example: A = \begin{bmatrix}
-0.909198 & -0.717041 & -0.745330 & -0.938879 & -1.435481 & -1.196454 & -1.231644 & -1.472402 \\
0.867548 & 0.893142 & 0.633169 & 0.615845 & 1.071125 & 1.102962 & 0.779577 & 0.758028 \\
2.259478 & 2.391340 & 2.435814 & 2.300889 & 2.942843 & 3.106868 & 3.162191 & 2.994355 \\
1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0
\end{bmatrix} B = \begin{bmatrix}
0.0 & 1.0 &	1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0\\
1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0\\
0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0\\
1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0
\end{bmatrix} P = \begin{bmatrix}
-1.052645\\
0.833213\\
2.661413\\
1.0
\end{bmatrix}","['matrices', 'solid-geometry']"
2265260,Is the fractional part of Brownian motion equidistributed almost surely?,"Let $B(t)$ be a standard Brownian motion on $\mathbb{R}$, starting at $0$. Let $\pi: \mathbb{R} \to [0,1)$ be the fractional part of a real number, $\pi(1.2) = .2$, etc. I morally identify this with the map $exp : \mathbb{R} \to S^1$. Question: Is it true that, almost surely, the fraction part of Brownian motion will traverse $[0,1)$ in an asymptotically equidistributed fashion? Precisely... Let $A \subset [0,1)$ be measurable. If we denote by $B(A) = \lim_{N \to \infty} \frac{ |t : \pi (B(t)) \in A| }{N}$, is $B(A)$ the same as the Lebesgue measure of $A$ (almost surely)? Here $|t : \pi B(t) \in A|$ refers to the Lebesgue measure of the set of $t$ so that $\pi B(t) \in A$. Based on some computer experiments (with a truncation of the Levy construction for Brownian motion) it seems true but I can't prove it. I want to prove it like this: 1) The limiting distribution shouldn't care about the starting point. 2) Thus the limiting distribution on $S^1$ is invariant under the action of $\mathbb{R}$. 3) Therefore by uniqueness it must be the Haar measure. I can't verify 1)... also its not clear that the formula $B(A)$ is even well defined, so it's not clear that it gives rise to a distribution on $S^1$, invariant or not. (Though I think once $B(A)$ is shown to be well defined it should be clear that it is a probability measure... total measure one is clear, non-negativity is clear, countable additive would follow from analogous properties of the Lebesgue measure on the reals...)","['brownian-motion', 'probability', 'lebesgue-measure']"
2265267,Nilpotent with infinite degree?,"Consider the ring of polynomials over a field or ring: $R[x]$.  We can define a formal differentiation operator: $D$ which maps $x^n$ to $nx^{n-1}$ etc. For any particular element of $R[x]$, sufficient applications of $D$ will result in $0$.  However, there is no $n$ such that $D^n$ maps all elements to $0$. So, do we call $D$ nilpotent?  If we do then do we say that it is nilpotent with infinite degree?  If we don't then is there another standard term? (Not homework, just an old man trying to exercise his ageing brain.)","['abstract-algebra', 'ring-theory', 'nilpotence']"
2265280,Show that $f$ is univalent if in unit disk $|f'(z) - f'(0)|< |f'(0)|$.,"Let $D = ${$z \in \mathbb{C}: |z|\leq 1$}, $f$ analytic on $D$ such that
  $|f'(z) - f'(0)|< |f'(0)|$ for all $z\in D$. Show that for any $a \neq b$, $f(a) \neq f(b)$. I really need some hint or insight on how to start the proof. In all my attempt, I could not find a way to make use of the inequality given, thus I know I'm on the wrong track. Any help or insights is deeply appreciated. Edit: So turns out I have a solution, so do help to verify: Suppose not, that is $a\neq b$ but $f(a) = f(b)$ consider $L$ to be the line segment from $a$ to $b$. $|\int_L f'(z) - f'(0)dz| <|b-a||f'(0)|$ by M-L inequality and the conditions given. This means $|f(b) - f(a) -(b-a)f'(0)|<|b-a||f'(0)|$
but since we assume $f(b) = f(a)$ $|b-a||f'(0)| <|b-a||f'(0)|$ contradiction.","['complex-analysis', 'proof-verification']"
2265281,Homeomorphism Between Spectra: Atiyah Macdonald Ch. 3 Ex. 21,"I checked if anything on this questions exists on MSE, but I found nothing. I did find two solutions available via google, but was skeptical of both. This is Exercise 21 - i) from Chapter 3 of Introduction to Commutative Algebra by M.F. Atiyah and I.G. Macdonald. The exercise states: Let $A$ be a ring, $S$ be a multiplicatively closed subset of $A$, and $\phi \colon A \to S^{-1}A$ the canonical homomorphism. Show that $\phi^{*} \colon \mathrm{Spec}(S^{-1}A) \to \mathrm{Spec}(A)$ is a homeomorphism of $\mathrm{Spec}(S^{-1}A)$ onto its image in $X = \mathrm{Spec}(A)$. Let this image be denoted by $S^{-1}X$. I am satisfied with 3/4'ths of my solution. Namely, we know that $\phi^{*}$ is continuous by Exercise 21 - i, Chapter 1 . We also know this restriction mapping is surjective, trivially. Further $\phi^{*}$ is injective by applying Exercise 20 - ii, Chapter 3 that every prime ideal of $S^{-1}A$ being an extended ideal is a sufficient condition for infectivity. We know the hypothesis that every prime ideal of $S^{-1}A$ is extended is satisfied since Proposition 3.11 - iv) gives a bijective correspondence, $\mathfrak{p} \longleftrightarrow S^{-1}\mathfrak{p},$ where $\mathfrak{p}$ are the prime ideals of $A$ that don't meet $S$ and $S^{-1}\mathfrak{p}$ represents $\mathfrak{p}^{e}$, thus every prime ideal in the localization is extended. My issue is with the final step, showing that the inverse map is continuous. To be honest, I have been struggling to decided what precisely the rule defining the inverse function would be? I believe it is simply the action of $\phi$? I decided to follow the same outline for how this text had us show that $\phi^{*}$ it self was continuous, so here is the proof I came up with. I am very skeptical however as I don't believe I am using the follow power of my hypothesis: Proof:
To show ${\phi^{*}}^{-1}$ is continuous we show ${\phi^{*-1}}^{-1}(X_{f}) = Y_{\phi^{-1}(f)}$, apologies for the atrocious notation on the left hand side, I simply mean the pullback of the open basis set $X_f$ in $\mathrm{Spec}(S^{-1}A)$ under the inverse of $\phi^{*}$. If successful this obviously satisfies the usual topological definition of continuity since $Y_{\phi^{-1}(f)}$ is open in $S^{-1}X \subseteq \mathrm{Spec}(A).$ The argument is $$\mathfrak{p} \in {\phi^{*-1}}^{-1}(X_{f}) \iff \phi^{*-1}(\mathfrak{p})\in X_{f}$$
$$\iff \phi(\mathfrak{p}) \in X_{f}$$ but since $\mathfrak{p}$ is some contracted ideal, $\phi(\mathfrak{p})$ is just some $\mathfrak{q}$ in $S^{-1}X$, then $$\iff f \notin \phi(\mathfrak{p})$$ $$\iff \phi^{-1}(f) \notin \mathfrak{p}$$ $$\iff \mathfrak{p} \in Y_{\phi^{-1}(f)}.$$","['localization', 'zariski-topology', 'algebraic-geometry', 'commutative-algebra']"
2265284,What is the present value of an annuity consisting of monthly payments of an amount C.,What is the present value of an annuity consisting of monthly payments of an amount C continuing for n years? Express the answer in terms of the effective rate re. I know that the effective rate $re = (1 + \frac {r}{m}) ^ m $. Where m is the amount of payments per year. But apart from that I don't understand how to solve this question.,"['statistics', 'finance']"
2265341,Calculating dimension of the intersection of two subspaces,"Q. Let $U$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2),(2,9,0,-5,-2)\}$, and let $V$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. What is the vector space dimension of $U \cap V$? I found that the basis of $U$ is just $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2)\}$ and that of $V$ is $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. This means $\dim U=2$ and $\dim V=3$. I know that the dimension of $U \cap V$ is either $0,1$ or $2$ since $\dim (U \cap V) \le \dim U$. Attempt : Using the method of top answer from here , I got this equation $a(1,3,-3,-1,-4)+b(1,4,-1,-2,-2)-x(1,6,2,-2,3)-y(2,8,-1,-6,-5)-z(1,3,-1,-5,-6)=0$ then solving the arising system of linear equations and reducing to row-echelon form I get the following system of linear equations :
$$
\left\{ 
\begin{array}{c}
a+b-x-2y+z=0 \\ 
b-3x-2y=0 \\ 
x-y-2z=0
\end{array}
\right. 
$$ Which gives me null space as $[-2y-5z \;\;\; 5y+6z \;\;\; y+2z \;\;\; y \;\;\; z]^T$. Setting $y=-\frac 65$ and $z=1$ we get a vector in null space as $(-\frac {13}5,0,\frac 45,-\frac 65,1)$ Hence our $\textbf {v}=-\frac {13}5 (1,3,-3,-1,-4)=\frac 45(1,6,2,-2,3)+\frac {-6}5 (2,8,-1,-6,-5)+(1,3,-1,-5,-6)$. Does this mean that $\{(1,3,-3,-1,-4)\}$ is a basis of $U \cap V$ implying that $\dim (U \cap V)=1$? Have I done it right through out? Thanks. PS : How I set $y=-\frac 65,z=1$ ? I let $5y+6z=0 \; \text{in null space} \; \Rightarrow y=-\frac {6z}5$. Which gives me $[-\frac {13z}5 \;\;\; 0 \;\;\; \frac {4z}5 \;\;\; -\frac {6z}5 \;\;\; z]$. Then I took $z=1$. If I vary $z$ all over the $\Bbb R$ then, $U \cap V=\{-\frac {13z}5 (1,3,-3,-1,-4)\ : z \in \Bbb R\}$.","['systems-of-equations', 'linear-algebra', 'proof-verification', 'vector-spaces']"
2265348,Functional Derivative of a function,"I want to know the steps involved for the functional derivative that has been taken for the below objective function And the derivative for above objective function is give below These equations are from Generative Adversarial Networks, http://arxiv.org/pdf/1701.00160.pdf can be found on page 46 and 47. Please help me with the steps for the functional derivative of the objective function.","['derivatives', 'machine-learning', 'frechet-derivative']"
2265349,Distribution of sum of exponentially distributed variables with arbitrary rates,"Consider independent random variables $X_1, \ldots, X_n$, each of them exponentially distributed with parameters $\lambda_1, \ldots, \lambda_n$. Let us consider the sum $Y= X_1 + \ldots + X_n$. (1) In the case where all the rates are equal ($\lambda_i = \lambda$ for all $i$), we know that the distribution is a Gamma distribution :
$$
f_Y(t) = \lambda e^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1)!}.
$$
(2) In the case where all the rates are different $(\lambda_i \neq \lambda_j $ for all $i,j$), we can also calculate the distribution of $Y$. This time it is given by the hypoexponential distribution :
$$
f_Y(t) = \sum_{i=1}^{n} \left( \prod_{j\neq i} \frac{\lambda_j}{\lambda_j - \lambda_i} \right) \lambda_i e^{-\lambda_i t},
$$
where the product is over all $1 \leq j \leq n$, $j \neq i$. For an induction proof see Ross . (3) Now consider the case in which some rates are equal to others but they are not all equal, i.e. the number of unique rates is between $2$ and $n-1$. Is there a general formula for the distribution of the sum in this case? I tried approaching the problem by computing convolutions and trying to find a pattern in the distributions. For example, for two variables $X_1, X_2$ I would calculate $f_{X_1+X_2}(t) = \int\limits_{0}^{t} f_{X_1}(s) f_{X_2}(t-s) ds$, and then do this iteratively for more and more variables where some but not all rates are similar. From the results I obtained I came up with the following conjecture : Assume that there exist $i,j$ such that $\lambda_i = \lambda_j$ and that there exist $m,n,$ such that $\lambda_m \neq \lambda_n$. Define $m$ as the number of distinct rates, $2 \leq m \leq n-1$. Define $n_j$ as the number of variables with rate $\lambda_j$, $1 \leq n_j \leq n$ and $1 \leq j \leq m$. Hence the system is specified by $(m, n_1, \ldots, n_m)$ and rates $(\lambda_1, \ldots, \lambda_m)$. Then the distribution of $Y$ is given by
$$
f_Y(t) = \sum_{i=1}^{m} \left[ \prod_{j\neq i} \left(\frac{\lambda_j}{\lambda_j - \lambda_i}\right)^{n_j} \right] \lambda_i^{n_i} \left[ \sum\limits_{k=1}^{n_i} \frac{t^{k-1}}{(k-1)!} \right] e^{-\lambda_i t},
$$ I was not able to proof this by induction or verify it in some other way than by checking results for a small number of variables. Could you prove or disprove the conjecture I made above ? Linked question Distribution of sum of exponential variables with different parameters : a similar question, but it only considers a special case with parameters $(\lambda, \lambda/2, \lambda/3, \ldots, \lambda/n)$, which belongs to case (2) above. I would like to know the general case for arbitrary $\lambda_i$'s.","['probability', 'calculus', 'probability-distributions']"
2265367,Prove that $\Bbb{Z}[i]/J$ is not isomorphic to $\Bbb{Z}$,"If $J$ is a prime ideal, then prove that $\Bbb{Z}[i]/J$ is not isomorphic to $\Bbb{Z}$. My attempt: Since $\mathbb{Z}[i]$ is a PID, then $J$ is a maximal ideal, then  $\mathbb{Z}[i]/J$ is a field. Hence the result follows. I just want some alternative approaches.","['abstract-algebra', 'ring-theory']"
2265407,Explanation about identity $\sum\limits_{R=0}^N\binom{R}r\binom{N-R}{n-r} = \binom{N+1}{n+1}$ in Jaynes' Probability theory chapter 6,"Equation from Jaynes Probability Theory chapter 6. $$\sum_{R=0}^N\binom{R}{r} \times \binom{N-R}{n-r} = \binom{N+1}{n+1}.$$ The following is my proof. For
$$\sum_{R=0}^N\binom{R}{r} \times \binom{N-R}{n-r}$$
$$\binom{R1}{r} \times \binom{N-R1}{n-r} = \binom{N}{n}$$
Then get
$$(N+1) \times \binom{N}{n}$$ I don't know which step is wrong. Help, thanks very much.","['combinations', 'probability-theory']"
2265429,Ten and eleven sided polygons with circles.,"I am having trouble with the question below: Given a not necessarily convex 10 sided polygon, draw circles with its sides as diameters. Is it possible that all these circles pass through a point which is not a vertex of this 10 sided polygon? Do the same for an 11 sided polygon. I've tried simply drawing sketches of various situations and am currently thinking that it is not possible",['geometry']
2265477,A counting problem of traversing a circular arrangement,"Let there be $n$ chairs, $C_1,C_2,\ldots,C_n$ around a circular table. A cat starts jumping from $C_1$ and after first jump it reaches to $C_3$, then after second jump it reaches to $C_6$ and so on. That is at the $k$th jump the cat skips $k$ chairs. Now my question: Is there a bound on the number of jumps (depending on $n$) or an exact number of jumps such that the cat visits all the chairs at least once? N.B I have proved that if $n$ is odd then there will at least be a chair that will not be visited ever. Thus $n$ must be even. Also there are instances of even $n$'s such that some chairs are not going to be visited ever. Thus I'm looking for a bound on the minimal number of jumps, whence the scenario is feasible for some even $n$.","['number-theory', 'combinatorics', 'elementary-number-theory']"
2265500,Is there a metric space on $\mathbb{R}$ such that open sets of Euclidean metric space are not open?,"If $X$ is metric space together with a metric function $d$, we call $U \subseteq X$ open if for all $u \in U$ there is some $\epsilon > 0$ with $B_{\epsilon}(u) \subseteq U$ where $B_{\epsilon}(u) = \{v \in X|d(u,v) < \epsilon \}$. So, my question is that can we have a metric on $\mathbb{R}$ such that open intervals (of Euclidean metric) are not open in this particular metric we provide? Or, can we prove that open intervals are always open in any other metric? In latter case, of course open intervals can be closed and open.","['real-analysis', 'metric-spaces']"
2265519,Showing that a recursion for approximating the square root converges,"Let the numbers $x_n$ be defined by the recursion $$x_1=k,\quad x_{n+1}=\frac{1}{2}\bigg(x_n + \frac{k}{x_n}\bigg)\space (n\ge 2).$$ To show the numbers defined by the recursion approach $\sqrt{k}$, is it really sufficient to argue that if $x_n$ approaches $\sqrt{k}$ then so does $x_{n+1}$? Do we not have to take into account for how fast the sequence is approaching $\sqrt{k}$?","['approximation', 'discrete-mathematics']"
2265526,Calculate $\lim\limits_{n\to\infty}\dfrac{\sum\limits_{k=0}^n\log\binom{n}{k}}{n^2}$,"How to prove if the following limit exists? If it exists, what's the value? $$\lim\limits_{n\to\infty}\dfrac{\sum\limits_{k=0}^n\log\binom{n}{k}}{n^2}$$ Thanks!","['combinatorics', 'limits']"
2265533,Unbiased estimate for $\mu$ and standard error for Poisson random variables [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Suppose that computer failure data $x_1, x_2, \ldots, x_n$ are modelled as observations of i.i.d. Poisson random variables $X_1, X_2, \ldots, X_n$ with common mean $\mu$. Recalling that, for this data, $n = 104$ and $\bar{x} = 3.75$, calculate an unbiased estimate for $\mu$, together with its standard error, under the Poisson model.","['statistics', 'poisson-distribution']"
2265535,Evaluate $\displaystyle \lim_{n \to \infty} n \int_0^1 (\cos x - \sin x)^n dx$,Evaluate $$\lim_{n \to \infty} n \int_0^1 (\cos x - \sin x)^n dx$$ The answer should be $1$. I tried to solve it similar to how one user solved this integral Limit of integral with cos and sin but it seems like it doesn't work because the upper bound will become $1 - \pi/4$.,"['integration', 'limits']"
2265536,Existence and Uniqueness of Geodesics,"I'm reading the theorem 4.10 of the book ""Riemannian manifolds"" by John Lee. In proof, page 58, the author refers to the existence and uniqueness theorem for first-order ODEs [Boo86, Theorem IV.4.1]. Also in page 60, theorem 4.12 is about the Existence and Uniqueness for Linear ODEs. I want to know if this two theorem are the same? Is there any difference between them? Thanks!","['manifolds', 'riemannian-geometry', 'differential-geometry', 'smooth-manifolds']"
2265571,Is there an error in my proof of existence of a solution to this integral equation?,"I have this proof to show that $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ has a unique integral solution in $C[0,1]$. I was looking at someone who had posted an identical question to this here: Show that the integral equation has a solution on a suitable subset of $C[0,1]$ where someone answered that you need to be careful about which subsets of $C[0,1]$ you choose. I didn't take any care about subsets  at all in my own proof so I was wondering if someone could tell me precisely where I went wrong in the following sketch of my proof. Help is truly appreciated. Sketch of proof at first is that I show that for all $t\in [0,1]:$ $$\frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds $$ is continuous with respect to $t$. (and thus locally Lipschitz given the closed bounded interval). Then I define $$T_x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ which has a fixed point at: $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ Then from the lemma above that the integral is continuous, $T$ is also continuous and thus we have that $T: C[0,1] \rightarrow C[0,1]$. Then I make the following argument that $T$ is a contraction map. $$|T_{x_1}(t) - T_{x_2}(t)| = \frac{1}{5} \Big|\int ^1_0 e^{-t} \cos^2 (ts) (x_1(s)-x_2(s))ds\Big| \leq \frac{1}{5} \text{max}_{s\in[0,1]} \Big|x_1(s)-x_2(s)\Big|$$ Thus $\Big\| T_{x_1} - T_{x_2} \Big\| \leq \frac{1}{5} \Big| x_1 - x_2 \Big|$, so $T$ is a contraction map, and since $C[0,1]$ is complete then $T$ has a unique fixed point. Thus there is a unique solution. If this is correct or if I am missing something fundamental? Thanks for the time.","['functional-analysis', 'ordinary-differential-equations', 'analysis']"
2265615,Finding $\lim_{n \to \infty} x_n$,Let $x$ be a positive real number. A sequence $x_n$ of real numbers is defined as follows: $$x_1=\frac 12\left(x+\frac5x\right)$$ and $$x_{n+1}=\frac 12\left(x_n+\frac 5{x_n}\right) \quad\forall n\geq 1$$ The question is to show that $$\frac{x_n-\sqrt 5}{x_n+\sqrt 5}=\left(\frac{x-\sqrt 5}{x+\sqrt 5}\right)^{2^{n}}$$ and hence or otherwise evaluating $$\lim_{n \to \infty} x_n$$ To show first part I tried using induction. It is clear that $n=1$ is true. Let it be true for $n=k$. So $$\frac{x_n-\sqrt 5}{x_n+\sqrt 5}\left(\frac{x-\sqrt 5}{x+\sqrt 5}\right)^2=\left(\frac{x-\sqrt 5}{x+\sqrt 5}\right)^{2^{n+1}}$$ I tried manipulating this but could not proceed. Any ideas? Thanks.,"['induction', 'sequences-and-series', 'convergence-divergence', 'limits']"
2265621,Is there any other special numbers?,"$34$ is quite an interesting number. It's the product of 2 different prime numbers: $2$ and $17$. Also, $34-1$ and $34+1$ also the products of 2 different prime numbers, which are $(3)$$(11)$ and $(5)$$(7)$ respectively. We have a definition: Definition: A positive integer n is called special if each of n, n-1 and n+1 is the product of 2 distinct prime numbers. Question: Is there any special number beside $34$? Note: I have figured out that if a number is special , then it must be even, but I don't khow what to do next.","['number-theory', 'discrete-mathematics', 'recreational-mathematics', 'elementary-number-theory']"
2265651,Finding the value of $f(1)$ from a given functional equation,"A function $f: \mathbb{Q}^+ \cup \{0\} \to \mathbb{Q}^+ \cup \{0\}$ is defined such that $$ f(x) + f(y) + 2xyf(xy) = \frac{f(xy)}{f(x+y)}$$
  Then what is the value of $\left[f(1)\right]$ (where $[.]$ denotes the greatest integer function)? I proceeded this way: Putting $x=y=0$ I got $f(0) = \frac{1}{2}$ (assuming $f(0) \neq 0$) Again putting $y=0$ I got $f(x) + f(0) = \frac{f(0}{f(x)}$ which gave 2 values of $f(x)$ as $-1$ and $\frac{1}{2}$. As $f(0)$ was equal to $\frac{1}{2}$ so I assumed $f(x)$ as a constant function having value $\frac{1}{2}$ for all $x$. When $f(0) = 0$ then I got $f(x) = 0$ for all $x$. But the answer was given to be equal to $1$ which means $f(1) \in [1,2)$. Where am I wrong?",['functions']
2265694,Proof to the properties of sinusoidal functions,"A sinusoidal function is defined if $f(x)$ can be written as $a\sin [b(x+c)]+d$ where $a$ is the amplitude of the curve, $b$ is related to the period, $c$ is the horizontal shift and $d$ relents the vertical shift of the curve. But can anyone explain me how $a,b,c,d$ represents those?",['trigonometry']
2265701,How Ricci Flow makes room to find Enistein metrics?,"I am studding a lecture note entitled ""Topics in Riemanian Geometry"" by Jeff. Viaclovsky. See the below phrase  in lecture 12: ""In order to find Einstein metrics, one would first think of looking at the gradient Ricci flow on the space of Riemannian metrics. This is
\begin{equation}
{{\partial g}\over {\partial t}}=-2Ric_g, \quad g_0=g(0).
\end{equation} I want know the idea behind this flow, and how the flow makes room to obtain an Einstein metric on underlying manifold?",['differential-geometry']
2265745,Prove that $\int_{0}^{\frac{\pi }{2}}\sin(\cos(x))+\cos(\sin(x))dx\leq \frac{\pi ^{2}}{4}$,"Let $f:\left [ 0,\frac{\pi }{2} \right ]\rightarrow \mathbb{R}, f(x)=\sin(\cos(x))+\cos(\sin(x))$. Prove that $$\int\limits_{0}^{\pi/2}f(x)\,\mathrm{d}x\leq \frac{\pi ^{2}}{4}\,.$$ I've managed to find that $f$ is decreasing on $\left [ 0,\frac{\pi }{2} \right ]$, hence: 
$$
\int\limits_{0}^{\pi/2} f(x)\,\mathrm{d}x
     \leq \int\limits_{0}^{\pi/2} f(0)\,\mathrm{d}x
      =   \frac{\pi }{2}\cdot (1+\sin(1))$$ 
But $1+\sin(1)\geq \frac{\pi }{2}$, thus my inequality isn't enough to prove the problem statement.","['inequality', 'definite-integrals', 'calculus']"
2265788,"If $A \subset X$ is contractible, then is $X \setminus A$ homotopy equivalent to $X \setminus a_0$?","As in the title: let $A \subset X$ be contractible and let $a_0 \in A$. Are $X \setminus A$ and $X \setminus a_0$ homotopy equivalent? I tried to prove that it is but honestly i have no idea what a homotopy inverse to the inclusion $X \setminus A \rightarrow X \setminus a_0$ would look like. This is pretty difficult for me to visualize so any intuitive proof/counterexample would be greatly appreciated, or any general advice on looking for a homotopy inverse.","['algebraic-topology', 'general-topology', 'homotopy-theory']"
