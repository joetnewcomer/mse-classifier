question_id,title,body,tags
3524445,How to approximate the expected minimum Hamming distance with $N \gg n$ strings,"If we sample $N \gg n$ binary strings of length $n$ , uniformly and
  independently, what is the expected minimum Hamming distance between the closest pair? It seems likely this is hard to compute exactly so a good approximation would be very welcome. This related question asks specifically about the case where $N = 3$ . In a very nice answer @joriki sets out that the mean minimum Hamming distance in this case is approximately: $$
\boxed{\frac n2-\frac34\sqrt\frac n\pi}\;.
$$ In order to have a concrete value to aim at: If $N=2^{12}$ and $n=50$ then the mean minimum Hamming distance is approximately $7.3$ . If $N=2^{12}$ and $n=64$ then the mean minimum Hamming distance is approximately $11.7$ . If $N=2^{14}$ and $n=50$ then the mean minimum Hamming distance is approximately $5.9$ . If $N=2^{14}$ and $n=150$ then the mean minimum Hamming distance is approximately $40.2$ .","['combinatorics', 'probability']"
3524448,Is topology just the minimal mathematical structure you need to define on a set just to define continuous functions?,"I looked for a lot of explanations for the idea of defining a topology for a set to make a topological space, I found a lot of clever explanations but most of them seem like just ""concrete explanations that may seem like the original idea but actually has nothing to do with it"", and the worst of them who try to make an analogy between a topological space and a metric space by explaining them with a notation of semi-measures? which I guess has nothing to do with the actual idea behind the topology. I think ""but not quite sure that's why I'm asking"" that the core idea of topology is in its definition of continuous functions, like if we take the donut and cup definition of two shapes: two shapes are the same topologically if there is no cuts needed to transform one of them to the other , and put two 2D surfaces in $\mathbb{R}^3$ , it's easy to see that they will satisfy this property if there is a kind of a continuous function and a continuous inverse between them, so in order to generalize that you need a minimal structure to define for sets so that you can define continuous functions, so is that it? EDIT: It seems that the easiest way to make sense of topologies and there axioms at the beginning of studying the subject is basically through a generalization of continuous functions $f:\mathbb{R}^n\to \mathbb{R}^m$ , in the simplest case of them $f:\mathbb{R}\to \mathbb{R}$ we can easily see that a function is continuous everywhere if the preimage of any open interval is also an open interval, if $f$ is discontinuous at some point say $x$ it's easy to see that you can make an open interval containing $f(x)$ but it's preimage will be half closed half open interval so it fails the condition of making the preimage of any open subset open, then the axioms of a topology can be seen as the properties of the set of open intervals, the union of 2 opens is open ...etc , so a topology is a generalization of the set of open intervals so we can define continuous functions for any set with such a structure topological spaces , this intuition seems also to be the direction of Munkres's topology book when he first introduce continuous functions.",['general-topology']
3524457,Find the function $f(x)$ if $f(x+y)=f(x-y)+2f(y)+xy$,"I'm suffering from the procedure I found, which is contradictory. The original question is: Let $f$ be differentiable. For all $x, y \in \mathbb R$ , $$f(x+y)=f(x-y)+2f(y)+xy$$ Suppose $f'(0)=1$ and find $f(x)$ . My procedure was: Put $x=y=0$ : $$f(0)=f(0)+2f(0)+0$$ $$f(0)=0$$ Put $x=y=\frac{h}{2}$ : $$f(h)=2f\left(\frac{h}{2}\right)+\frac{h^2}{4}$$ Substituting $x=x+\frac{h}{2}, y=\frac{h}{2}$ brings: $$f'(x)=\lim_{h\to0} \frac{f(x+h)-f(x)}{h}=\lim_{h \to 0}\frac{2f\left(\frac{h}{2}\right)+\frac{h}{2}\left(x+\frac{h}{2}\right)}{h}=\lim_{h\to0}\frac{f(h)+\frac{h}{2}x}{h}\\=\lim_{h\to0}\frac{f(h)-f(0)}{h-0}+\frac{x}{2}=\frac{x}{2}+1$$ $$f(x)=\frac{x^2}{4}+x+c$$ where $c$ is constant.
  Since $f(0)=0,\space\space c=0$ However, when we substitute $x=0$ to the given condition, $$f(y)=f(-y)+2f(y)+0(y)$$ $$-f(y)=f(-y)$$ which informs that $f$ is odd: contradiction! Could you please help me to find the error? Thanks.","['functional-equations', 'calculus', 'functions']"
3524470,How can we count different coloring in 2 cycle with one edge in common,"There is a question that asked what is the counts of different coloring in $C_5$ and $C_6$ when it has one edge in common : I have used fundamental reduction in cycles in order to get a recursive formula (I know there is a closed formula for that too), but when i try use the same in this particular question, it becomes very complicated. How can I find a formula for this kind of graph? Im looking for Chromatic Polynomial.","['graph-theory', 'coloring', 'discrete-mathematics']"
3524473,Isomorphic groups vs. isomorphic subgroups,"Let's consider the following diagram: $$
\newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex}
\newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\las}[1]{\kern-1.5ex\xleftarrow{\ \ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}}
\begin{array}{c}
  I_n &  & \color{red}{I_m}     &   &    \color{red}{I_m}  & & I_n    \\
\da{f} &    &  \color{red}{\da{\hat f}}  &  &  \color{red}{\da{\bar f}}  & & \da{f}  \\
G & \las{\hat\epsilon}   &   \color{red}{H} & \color{red}{\ras{\psi}} &  \color{red}{K} & \ras{\bar\epsilon} & G \\
\da{\theta}  &   &  \color{red}{\da{\hat\theta}} & & \color{red}{\da{\bar\theta}} & & \da{\theta}  \\
S_G & \las{\hat\iota}   &   \color{red}{S_H} & \color{red}{\ras{\varphi^{(\psi)}}} & \color{red}{S_K} & \ras{\bar\iota} & S_G  \\
\da{\varphi^{(f)}} & & \color{red}{\da{\varphi^{(\hat f)}}} & & \color{red}{\da{\varphi^{(\bar f)}}} & & \da{\varphi^{(f)}}         \\
S_n &    &   \color{red}{S_m} &  & \color{red}{S_m} & & S_n  \\
\end{array}
$$ where: $m,n$ are positive integers $I_x:=\{1,\dots,x\}$ , for $x=m,n$ $\hat f$ is a bijection $G$ is a finite group of order $n$ , and $H,K<G$ with $H\ne K, H \cong K$ $\psi$ is an isomorphism $\bar f:=\psi \hat f$ $S_X:=\operatorname{Sym}(X)$ , for $X=G,H,K$ $\theta,\hat\theta,\bar\theta$ are Cayley embeddings $S_x$ is the symmetric group of degree $x$ , for $x=m,n$ $\hat\epsilon,\hat\iota,\bar\epsilon,\bar\iota$ are embeddings such that: $$\hat\iota\hat\theta=\theta\hat\epsilon, \quad \bar\iota\bar\theta=\theta\bar\epsilon \tag 0$$ given two sets $A,B$ and a bijection $\alpha\colon B \rightarrow A$ , the map $\varphi^{(\alpha)}\colon S_A \rightarrow S_B$ is the isomorphism defined by $\sigma \mapsto (b \mapsto (\alpha^{-1}\sigma\alpha)(b))$ . If we single out the red-coloured part of the diagram, and interpret $H$ and $K$ as independent entities, then this answer has already shown that: $$\varphi^{(\hat f)}\hat\theta \hat f=\varphi^{(\bar f)}\bar\theta \bar f \tag 1$$ Namely: two isomorphic (abstract) groups of order $m$ can embed into one same subgroup of $S_m$ . In this sense, ""they need not be distinguished from the standpoint of group theory"" (see this other answer ). Now, I would like to see what differences get in if $H$ and $K$ are no longer ""independent entities"", but rather subgroups of the parent group $G$ (whole diagram). In particular, can $H$ and $K$ embed into one same subgroup of $S_n$ via $S_G$ ? Equivalently: Does there exist a bijection $f\colon I_n \to G$ such that: $$\varphi^{(f)}\theta\hat\epsilon \hat f = \varphi^{(f)}\theta\bar\epsilon \bar f \tag 2$$ ? Hoping to have posed well this kind of question.","['group-theory', 'abstract-algebra', 'finite-groups', 'group-isomorphism']"
3524493,Does my proof show that the sequence of measures defined using mollifiers with shrinking support converges weakly to the Dirac measure?,"Following Evan's PDE book, Appendix C4, PP. 629, let's define the function: $\eta(x):=C\exp\left(\frac{1}{\|x\|^2 - 1} \right) \forall x \in \mathbb{R}^d$ when $\|x\|\leq 1$ and $0$ otherwise, where $C$ is a positive constant so that the integral of $\eta$ is $1$ . Define the mollifier $\eta_\varepsilon$ by : $\eta_\varepsilon(x): \frac{1}{\varepsilon^d}\eta(\frac{x}{\varepsilon})$ . Next, let's define the measure $P_\varepsilon$ by $dP_\varepsilon(x):=\eta_\varepsilon(x) \, dx$ , where $dx$ denotes the Lebesgue measure on $\mathbb{R}^d$ . I'm trying to prove (if true?!), that: $P_\varepsilon \to \delta_0$ weakly as $\varepsilon \to 0$ , where $\delta_0$ denotes the Dirac measure at $0$ . How do we prove this, if possible? So to start, I want to prove that $\forall f$ continuous, bounded on $\mathbb{R}^d$ , we must have: \begin{align}
& \int_{\mathbb{R}^d} f(x)\, dP_\varepsilon(x) = \int_{\mathbb{R}^d} f(x)\eta_\varepsilon(x) \, dx \\
\to {} & f(0)=\int_{\mathbb{R}^d} f(x) \, d\delta_0(x)
\end{align} as $\varepsilon \to 0$ . To achive this, I've done as follows: \begin{align}
& \int_{\mathbb{R}^d}f(x) \eta_{\varepsilon}(x)\,dx - f(0) \\
= {} & \int_{\mathbb{R}^d}(f(x)-f(0)) \eta_\varepsilon(x) \, dx \\
= {} & \int_{B(0;\varepsilon)}(f(x)-f(0)) \eta_\varepsilon(x) \, dx,
\end{align} as the support of $\eta_\varepsilon = \bar{B(0;\varepsilon)}$ . Fix any $\eta > 0$ . Using the continuity of $f$ at $0$ , we must have a $\varepsilon > 0$ so that for all $x$ with $\|x\|\leq \varepsilon$ , $|f(x)-f(0)|\leq \eta$ . Next, after we've taken the absolute values: \begin{align}
& \left|\int_{B(0;\varepsilon)}(f(x)-f(0)) \eta_\varepsilon(x) \, dx \right| \\
\leq {} & \int_{B(0;\varepsilon)}|f(x)-f(0)| |\eta_\varepsilon(x)| \, dx \\
= {} & \int_{B(0;\varepsilon)}|f(x)-f(0)| \eta_\varepsilon(x) \, dx \\
\leq {} & \eta \int_{B(0;\varepsilon)} \eta_\varepsilon(x)\,dx \\
\leq {} & \eta \int_{\mathbb{R}^d} \eta_\varepsilon(x) \, dx \\
= {} & \eta
\end{align} I think this finishes the proof, but if you could verify if it's correct, then it'd be greatly appreciated :)","['measure-theory', 'dirac-delta', 'probability-distributions', 'functional-analysis', 'probability']"
3524515,Find all functions $f$ satisfying $f(2x)=2f(x)^2 -1$,"Problem: Find all functions $f:[0,2]\to (0,1]$ such that $f'(0)$ exists and $f$ satisfies $$
\forall x \in [0,1],\quad f(2x)=2f(x)^2 -1.
$$ My attempt: Motivated from the identity $\cos 2\theta = 2\cos^2 \theta -1,$ let us define $g(x)=\cos^{-1} f(x)$ for $x\in [0, 2].$ We can show from the given functional equation that $g(x)=2g(x/2)$ holds for every $x\in [0, 2].$ On the other hand, we find that $f(0)=1$ and $f'(0)=0.$ Now, if we somehow show that $g'(0)$ exists, then letting $n\to\infty$ in the identity $$
\forall n\ge 1,\quad \frac{g(x)}{x}=\frac{ g(x/2^n)}{x/2^n}
$$ would give us $g(x) = g'(0) x$ for every $x.$ In other words, $f(x) = \cos kx$ for some constant $k.$ Note that the range is given to be $(0,1],$ which restricts $k$ to lie in the interval $(-\frac{\pi}{4}, \frac{\pi}{4}).$ Now my question is, how to show that $g'(0)$ exists? The usual chain rule does not apply here, because $\cos^{-1}$ does not have a finite derivative at $f(0)=1$ .","['functional-equations', 'real-analysis', 'calculus', 'functions', 'derivatives']"
3524550,"Why are lines in $\mathbb{R}^3$ all congruent to one another, but circles in $\mathbb{R}^3$ are not?","Lines in $\mathbb{R}^3$ are all congruent to one another, but circles in $\mathbb{R}^3$ are not all congruent to one another (because two different circles may have different radii).  Visually, this is completely obvious.  However, I would like a group-theoretic explanation for this. I am thinking of $\mathbb{R}^3$ as the homogeneous space $\mathbb{R}^3 = \frac{G}{G_0} = \frac{\text{SE}(3)}{\text{SO}(3)}$ , where $G = \text{SE}(3)$ is the group of (orientation-preserving) rigid motions and $G_0 = \text{SO}(3)$ is the stabilizer of the origin. A line in $\mathbb{R}^3$ is an orbit of a point in $\mathbb{R}^3$ by a subgroup $H \leq G$ that is conjugate to the subgroup $\{ (x_1, x_2, x_3) \mapsto (x_1 + t, x_2, x_3) \colon t \in \mathbb{R}\}$ of translations by the vector $(1,0,0)$ . A circle in $\mathbb{R}^3$ is an orbit of a point in $\mathbb{R}^3$ by a subgroup $K \leq G$ that is conjugate to the subgroup $\{ (x_1 + ix_2, x_3) \mapsto (e^{i\theta}(x_1 + ix_2), x_3) \colon e^{i\theta} \in \mathbb{S}^1\}$ of rotations around the $x_3$ -axis. Two subsets $S_1, S_2$ of $\mathbb{R}^3$ are congruent if there exists $g \in \text{SE}(3)$ such that $S_2 = g \cdot S_1$ . Given these definitions of ""line"" and ""circle"" --- as orbits of subgroups --- how could we have known that all lines in $\text{SE}(3)/\text{SO}(3)$ are congruent, but not all circles in $\text{SE}(3)/\text{SO}(3)$ have this property? In other words: What are the relevant aspects of the subgroups $H$ , $K$ , and $G_0$ that explain the $G$ -equivalence of $H$ -orbits in $G/G_0$ , as opposed to the non- $G$ -equivalence of all $K$ -orbits in $G/G_0$ ?","['geometry', 'group-theory', 'homogeneous-spaces', 'group-actions', 'differential-geometry']"
3524626,"$ A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 $ where A,B are complex matrices of odd size","Let $A, B$ be square matrices (with complex entries) of size $2n+1$ , where $n$ is a positive integer. I need help proving the following: $$A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 $$ I've tried using characteristic polynomials, properties of eigenvalues, however to no avail. I feel like this kind of problem needs a little bit of experience in working with ranks of matrices (I've tried using Sylvester in more ways) and I would appreciate some help.","['matrix-rank', 'determinant', 'matrices', 'characteristic-polynomial', 'matrix-equations']"
3524641,KKT-Conditions in a functional setting,"Let $F:L^2([0,1])\rightarrow \mathbb{R}$ be a convex functional.
Consider the minimization problem \begin{align}
   \underset{f(\cdot) \in L^2([0,1])}{\min} F(f)\,\,\text{ subject to }
     \|f(\cdot) \|_2\leq \lambda,
\end{align} with $\lambda>0$ and $\|f(\cdot) \|_2=\int_0^1 f\left(t\right)^2\text{d} t  $ . Are there some kind of KKT conditions for the minimizer of $F$ ? You can even consider $F$ as linear if that‘s easier.","['optimization', 'functional-analysis', 'karush-kuhn-tucker']"
3524686,Find max/min of the following function,"Find the minimum e max distance (in $R^2$ )
) between the point $Q =
(
3/
2
, −
3/
2
)$ and the set $$B = \{(x, y) ∈ R^2
: yx = 1, x ≥ 0, y ≥ 0\}$$ In other words I have to find max /min points of the function $${(x-3/2)^2 + (y+3/2)^2}$$ The set $B$ is clearly neither bounded nor convex. If I use Lagrange I can only find local min/max, but how do I show they are global? In a previous question : Does the following function admit a maximum? you suggested me to maximize/minimize the $x$ component and the $y$ component independently, but it is not clear to me if I can do it in this exercise as well. It seens to me that choosing $x=3/2$ , which clearly minimizes the first component of the sum, would restrict the choice of $y$ as $yx = 1 $ . Is there any way to show that the point found using lagrange is a global min? Possibly without using the bordered hessian method?","['qcqp', 'lagrange-multiplier', 'maxima-minima', 'multivariable-calculus', 'optimization']"
3524693,"Is it possible to set up a probability space with sample space $\Omega=\{1,2,\dots\}$ to model a 'uniformly chosen positive integer'?","$\textbf{The Problem:}$ Is it possible to set up a probability space with sample space $\Omega=\{1,2,\dots\}$ to model a 'uniformly chosen positive integer'? $\textbf{My Thoughts:}$ Suppose that we can indeed set up such a probability space $(\Omega,\mathcal F,P)$ . Observing that outcomes must be equally likely due to the uniformly chosen hypothesis, we may assume that there exists some $0<\varepsilon<1$ such that for all $n\in\Omega$ we have that $P(\{n\})=\varepsilon$ . Then since $\bigcup_{n\in\mathbb N}\{n\}=\Omega$ is a union of pairwise disjoint sets, countable additivity implies that $$P(\Omega)=P\left(\bigcup_{n\in\mathbb N}\{n\}\right)=\sum_{n=1}^\infty\varepsilon=\infty>1,$$ and we have a contradiction. Therefore, no such probability space can be set up. Could anyone please provide some feedback on the proof above? Any comments are much appreciated, and thank you very much for your time.","['solution-verification', 'uniform-distribution', 'probability-theory', 'probability']"
3524702,I'm trying to understand proof by induction rigorously. Is it based on assumptions?,"My understanding of the reason for using proof by induction is to see if the expression used to calculate what the nth term in a sequence is, always holds or not. A proof by induction requires a basis step.  It's not explicitly stated why the basis step is important when learning this.  I hear analogies that proof by induction is like a ladder, like dominoes, like stairs, so I think to myself what is similar about those objects.  The segments of a ladder, or stairs all look identical to each other going all the way up the ladder or all the way up the stairs. This leads me to believe there is an assumption that an equation performs an identical action on each of the numbers inputted into it. Which seems reasonable to me. An equation performs the same action on the number 2, whether it be scaling it, adding to it, etc, that it would perform on the next number, say 3. Some expressions are hard to see exactly what the pattern would be, but by looking at the a few terms in the pattern we notice certain pattern, sometimes that pattern does break and we discover the actually equation that would hold that pattern forever is different from what we thought it was originally. So this is where the distinction that we assume the expression we are given originally is correct, in the induction hypothesis we use the logical expression know as implication, ""If p then q""  if you recall the truth table for that expression it can only be proven to be false when p is true and q is false. So the truth of p is actually irrelevant, we are checking to see that if p were true than would q hold. We test the induction hypothesis by setting the original equation on one side of an equals symbol, adding the k+1 last term to it, then we put the expression with the k+1 replacing every instance of k. 
We massage the equations to see if they look identical, and if they do we can see our equality holds. I'm not really sure why we bother doing all of this in the first place, 
If we are assuming our propositional statement is true to begin with, and if we know from the onset that our equation behaves like a ladder or stairs, can't we just infer from the very beginning that k+1 holds . . I'm not too certain what the point of the proof really is. It still seems circular to me. I must be missing some really important insight. I don't want to just route memorize this. I get some of the basic ideas of the proof and I think I understand what it's trying to accomplish, it just doesn't seem rigorous like proof by contradiction or proof by contra positive.","['induction', 'discrete-mathematics']"
3524770,Why must delta be defined in terms of epsilon only?,"I'm a mathematics tutor trying to get a better understanding of the theory of epsilon-delta limit proofs. I can prove linear and constant epsilon delta proofs easily, and I understand the proof form and definitions, but nonlinear ones stump me. For example, take Problem 4., Page 3 here: $$
\lim_{x \rightarrow 2} {x^2 + x - 2} = 4
$$ The paper works through it as follows: \begin{align}
|f(x)-L| < \epsilon &\implies |(x^2+x-2) - 4| < \epsilon \\
&\implies |(x^2+x-6)| < \epsilon \\
&\implies|x+3||x-2| < \epsilon \\
&\implies |x-2| < \frac{\epsilon}{|x+3|}
\end{align} Now, this is the point where I would add $$
\text{let} \ \delta = \frac{\epsilon}{|x+3|}
$$ However, no source I've seen does that. This is where I get a little confused. The epsilon-delta definition means that the expression $$
\lim_{x\rightarrow c }f(x) = L
$$ is equivalent to $$
\forall\epsilon >0 \ \exists\delta>0 \ s.t. 0 < |x-c| < \delta \implies |f(x) - L|<\epsilon.
$$ Now, the definition has no qualifications for $x$ . $x$ is just the input of the function. However, the paper goes on to say ""In general delta must be
in terms of epsilon only, without any extra variables."" Why? The proof, when done ""forward"", goes through just fine for $\epsilon: \epsilon(\delta, x)$ . And intuitively, this makes sense: for some parts of $f(x)$ , the limits on epsilon will be different. However, every source I've seen finds limits on $|x+3|$ in some region and uses a constant in its place. Why must this be done? Why not leave $|x+3|$ in the denominator and be done with it?","['limits', 'calculus', 'epsilon-delta']"
3524797,Coordinate Vector Fields,"I am reading the following notes about coordinate systems in $\mathbb{R}^n$ and it lists four properties that coordinate vector fields should have. For the $(x,y)$ system and the coordinate vectors $\mathbf{i},\mathbf{j}$ , they are: They are orthogonal. They are unit vectors. $\mathbf{i}$ points in the direction in which $x$ increases and $y$ is constant, and $\mathbf{j}$ points in the direction in which $y$ increases and $x$ is constant. $\mathbf{i}$ and $\mathbf{j}$ represent differentiation of a function by $x$ and $y$ . What this means is that if $f$ is a differentiable function, then $$
\frac{\partial f}{\partial x} = D_{\mathbf{i}} f \text{ and } \frac{\partial f}{\partial y} = D_{\mathbf{j}} f
$$ In the first paragraph on the second page, it's written Unfortunately, for an arbitrary
coordinate system it is impossible to satisfy all four of the properties above. (It is a fairly
deep theorem in differential geometry that the only coordinate systems with coordinate vector fields that satisfy all four properties are Euclidean coordinates.) My question is, which theorem is referenced here? Is it related to the necessity of having a vanishing Lie bracket, i.e two linearly independent vector fields $X$ and $Y$ must satisfy $[X, Y] = 0$ in order to find coordinates with $X = x_u$ and $Y = x_v$ ? So is it related to symmetry of higher-order derivatives? Thanks.","['multivariable-calculus', 'partial-differential-equations', 'differential-geometry']"
3524833,Limit: ratio of the digit product and the number itself,"Compute: $$\lim_{n\to\infty}\frac{a_n}{n},a,n\in\mathbb N$$ Where $a_n$ equals the product of the digits of $n$ in base $10$ . source Math Analysis 1 exam, 2012 My attempt:
The first idea that came to my mind: $$a_n=0\;\forall\;n,k\in\mathbb N, s.t. n=10k$$ There certainly are some convergent subsequences $(a_{p(n)})$ : $$\lim_{n\to\infty}a_{p(n)}=0$$ I thought of writing $n$ either in this polynomial form: $$n=\sum_{i=0}^nd_i10^i$$ or in Horner's algorithm: $$\begin{align*}
n&=d_{0}+10\left(d_{1}+10\left(d_{2}+10\left(d_{3}+\cdots+10\left(d_{k-1}+10 d_{k}\right) \cdots\right)\right)\right),\\
k&=\lfloor\log n\rfloor+1
\end{align*}$$ (In my country we denote Briggs logarithm with $\log$ ) The first option seemed better. Then I decided to express $a_n$ this way: $$\prod_{i=0}^{\lfloor\log n\rfloor}d_i$$ I got this: $$\lim_{n\to\infty}\frac{\displaystyle\prod_{i=0}^{\lfloor\log n\rfloor}d_i}{\displaystyle\sum_{i=0}^nd_i10^i}$$ but I stuck here not knowing how to write a concise proof. Is there a better way of solving this?","['limits', 'calculus', 'decimal-expansion', 'real-analysis']"
3524840,What’s the Absorption Method?,What’s the absorption method in extremal combinatorics ? Is it related to the absorption law from discrete math? Or is it related to renormalization groups and dimensionality reductions of graphs?,"['graph-theory', 'combinatorics', 'discrete-mathematics']"
3524868,Degree of a map when restricting it to a submanifold,"Let $X, Y$ be closed oriented smooth manifolds of dimension $(n+1)$ , where $Y$ is also connected, and let $F: X → Y$ be a smooth map. As usual, the degree $\deg{F}$ is defined as $$
    \deg{F} = ∑_{x ∈ F^{-1}(y)} ±1 \; ,
$$ where $y ∈ Y$ is any regular value of $F$ and the sign is chosen depending on whether $F$ preserves or reverses orientation at the given point $x$ . Let $N^n ⊂ Y$ be an oriented, embedded and connected submanifold such that $N$ is closed as a subset of $Y$ . Suppose $M ≔ F^{-1}(N) ⊂ X$ is also an oriented embedded submanifold of dimension $n$ . By construction, $M$ and $N$ are both compact and $N$ is connected, so the degree of the restriction $\tilde{F} ≔ F|_M: M → N$ is well-defined. Is there anything that can be said about the degree of $\tilde{F}$ ? Do we have $\deg{\tilde{F}} = ± \deg{F}$ ? Notice that if $y \in Y$ is a regular value of $F$ (and thus of $\tilde{F}$ ) and happens to lie in $N$ , then $F^{-1}(y) = \tilde{F}^{-1}(y)$ and the claim that $\deg{\tilde{F}} = \pm \deg{F}$ follows if it can be shown that: Let $x \in F^{-1}(y) = \tilde{F}^{-1}(y)$ . Then $F$ is orientation-preserving at $x$ if and only if $\tilde{F}$ is orientation-preserving (or orientation-reversing – in the case of the minus sign). If it helps, I'm particularly interested in the special case where $X, Y$ are Riemannian manifolds and $M$ and $N$ are constructed as follows: Let $p: Y → ℝ$ be a smooth map and suppose $t ∈ ℝ$ is a regular value of both $p$ and $p ∘ F$ . Then define $N ≔ p^{-1}(t)$ and $M ≔ F^{-1}(t) = (p ∘ F)^{-1}(t)$ and assume that $N$ is connected. Note that $N$ and $M$ are automatically two-sided (and thus oriented) because the gradients of $p$ and $p ∘ F$ give rise to a trivialization of their normal bundles. My issue in proving the claim even in this special case is that I'm having trouble relating the normal vectors on $M$ and $N$ to each other as I don't know what $F$ and the two Riemannian metrics are doing.","['differential-topology', 'differential-forms', 'differential-geometry']"
3524885,Irreducible representation that is not absolutely semisimple,Let $G\to \mathrm{GL}_n(k)$ be an irreducible representation of a group $G$ over a perfect field $k$ . Is it possible that the induced representation $G\to \mathrm{GL}_n(\overline{k})$ is not semisimple? I have some examples in the case of a non-perfect field. Can this happen if $G$ is finite?,"['group-theory', 'abstract-algebra', 'representation-theory']"
3524901,"If $X_2$ is independent of $\mathcal F$, can we show that $f(X_1,X_2)$ is conditionally independent of $\mathcal F$ given $X_1$?","Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $\mathcal F\subseteq\mathcal A$ be a $\sigma$ -algebra on $\Omega$ $(E_i,\mathcal E_i)$ be a measurable space $X_i$ be an $(E_i,\mathcal E_i)$ -valued random variable on $(\Omega,\mathcal A,\operatorname P)$ $f:E_1\times E_2\to E_3$ be $(\mathcal E_1\otimes\mathcal E_2,\mathcal E_3)$ -measurable $X_3:=f(X_1,X_2)$ Assuming $X_2$ is independent of $\mathcal F$ , are we able to show that $X_3$ is conditionally independent of $\mathcal F$ given $X_1$ , i.e. $$\operatorname P\left[X_3\in B_3,F\mid X_1\right]=\operatorname P\left[X_3\in B_3\mid X_1\right]\operatorname P\left[F\mid X_1\right]\;\;\;\text{almost surely}\tag1$$ for all $B_3\in\mathcal E_3$ and $F\in\mathcal F$ ? Let $B_3\in\mathcal E_3$ and $F\in\mathcal F$ . We need to prove that $$\operatorname P\left[X_1\in B_1,X_3\in B_3,F\right]=\operatorname E\left[1_{\{\:X_1\:\in\:A\:\}}\operatorname P\left[X_3\in B_3\mid X_1\right]\operatorname P\left[F\mid X_1\right]\right]\tag2.$$ What's the easiest way to show $(2)$ ? Maybe we are able to reduce the problem to the case $f^{-1}(B_3)=A_1\times A_2$ for some $A_i\in\mathcal E_i$ , but I'm missing the right argument for that. EDIT : If necessary, feel free to impose a stronger notion of measurability on $f$ .","['conditional-expectation', 'conditional-probability', 'probability-theory', 'independence']"
3524947,How to calculate the expected value of the differences between nearest ordered values?,"Imagine I generate $N$ real numbers with a uniform distribution between $0$ and $1$ . I sort them in ascending order. And I calculate the differences between each consecutive pair. For example, for $N = 3$ , it would be like this: I would like to know what is the expected value of that differences, $\Delta$ . Each pair will have a different $\Delta$ but I'm just interested on the average expected value of all $\Delta$ . As I don't know how to calculate it with equations I've done it with a simulation instead (I'm not mathematician nor statistician, I just work with computers). And what I've gotten is: if I have $N$ numbers the average distance between them is $\frac1{1+N}$ , and that's also the value between the first number and zero. I would like to know how to calculate this with equations.
Intuitively I think it's the same as calculating $E\left[|X_i-X_j|\right]$ where $X_i$ and $X_j$ are two neighboring numbers in that sample. In general the expected value is calculated as: $$E[X]=\int_{-\infty}^\infty xf(x)\,dx$$ I think here we should integrate $|X_i-X_j|$ but I don't know $f(x)$ , the distribution of the differences, because I can't assume they are independent because we have to sort them and take the nearest pairs. And the absolute value complicates calculations a little bit more. There is an apparently similar question here but they are speaking about the minimum distance among all pairs.","['expected-value', 'statistics', 'order-statistics', 'probability']"
3524950,About closed sets and sequences in the weak topology,"Let $E$ be a normed vector space. In an exercise for a homework in my Functional Analysis class, my professor defined weak convergence for sequences as $(x_n)$ is weakly convergent to $x \in E$ iff for each $f \in E^*$ - the space of continuous functionals - $\lim_{n \to \infty}f(x_n) = x$ . Then he proceeds defining a set $X \subseteq E$ to be weakly closed iff for each weakly convergent sequence $(x_n) \subseteq X$ , its limit is an element of $X$ . Now here's where I got confused: this homework was previous to the lecture in which he properly introduced the weak topology of $E$ . Bearing in mind that for each infinite dimensional normed vector space, its weak topology is never metrizable, it can't be, particularly, first countable. So the sequence criterion for characterizing closed sets is not guaranteed (and false, I suspect). So the definition of 'weakly closed set' given in the homework isn't immediately equivalent to the definition of closed set in the weak topology. Are they actually the same? The exercise also asks us to show a subset of a Banach space that is closed in the norm topology, but isn't weakly closed (in the sense of weakly convergent sequences above). I thought about the unit sphere, as its closure in the weak topology is the closed unit ball - but I still haven't related this result to his definition of weakly closed.","['banach-spaces', 'functional-analysis', 'weak-convergence', 'weak-topology']"
3525002,clarification for disjoint union from any union,"In the book ""MEASURE THEORY AND FINE PROPERTIES  OF FUNCTIONS"", the authors claim: where Can someone explain to me how can we use  the observation to construct the disjoint union.
  I used $F_1=A_1\times B_2$ , $F_2=A_2 \times B_2 -A_1\times B_1 $ , $F_3=A_3 \times B_3 -(A_1\times B_1\cup A_2 \times B_2) $ and so one. I conclude that $(F_i)$ is a disjoint sequence of element of $(A_i\times B_i)$ which equals its union. But I don't know how to use there observation.","['elementary-set-theory', 'measure-theory']"
3525059,Negative ground state energy in two dimensions,"Let $\epsilon > 0$ and $V\in L^{1+\epsilon}(\mathbb R^2) \cap L^\infty(\mathbb R^2)$ real-valued, $V \leq 0, V \neq 0$ and let $$\mathcal E(\psi) := \int_{\mathbb R^2} \lvert \nabla \psi(x) \rvert^2 \, dx + \int_{\mathbb R^2} V(x) \lvert \psi(x) \rvert^2 \, dx, \quad \psi \in H^1(\mathbb R^2) = W^{1,2}(\mathbb R^2).$$ Consider the ground state energy $$E_0 := \inf\{\mathcal E(\psi) : \psi \in H^1(\mathbb R^2), \lVert \psi \rVert _2 = 1\}.$$ which can be shown to be bounded from below. I want to show that $E_0 < 0$ . However, I did not know how to approach this problem. The usual approach taking $f \in C_c^\infty(\mathbb R^2)$ and letting $$f_n = nf(nx)$$ does not seem to work since both terms scale the same way under this dilation. Can anyone help me come up with a better trial state?","['calculus-of-variations', 'functional-analysis', 'mathematical-physics', 'real-analysis']"
3525172,"In how many ways can we choose 4 different numbers from the set ${1,2,3,...,8,9,10}$ so that no two numbers are next to each other?","I did this question using PIE and I'm confused as to why I'm not getting the right answer. My approach: Use complementary counting. There are $\binom{10}{4}$ ways to choose 4 different numbers. I then subtracted $9\cdot\binom{8}{2}$ because there are $9$ ways to choose the pair of numbers and then $\binom{8}{2}$ ways to choose the last two numbers. I then added $8\cdot\binom{8}{1}$ because I subtracted this case twice and thus have to add it in once. I then subtracted $7$ . I got a final answer of $7$ , but the correct answer is $35$ . What did I do wrong?","['inclusion-exclusion', 'combinatorics']"
3525263,Are distance functions necessarily nonsmooth on the cut locus?,"Let $(M,g)$ be a complete Riemannian manifold and fix $p\in M$ . Consider the distance function $r(x):=d(p,x)$ . It is well-known that $r$ is smooth outside $\operatorname{cut}(p)\cup\{p\}$ where $\operatorname{cut}(p)$ is the cut locus of $p$ . My question is: Is $r$ necessarily nonsmooth on every point of $\operatorname{cut}(p)$ ? It is well-known that $x\in\operatorname{cut}(p)$ if and only if either (a) there are two distinct unit-speed minimizing geodesics $\gamma_1,\gamma_2:[0,\ell]\to M$ joining $p$ and $x$ , or (b) $x$ is a critical value of $\exp_p$ . In Peter Petersen's Riemannian Geometry , the author gave a remark on this: In case (a), $\nabla r$ could be either $\gamma_1'(\ell)$ or $\gamma_2'(\ell)$ and hence does not exist; in case (b), $\operatorname{Hess}r$ is undefined since it must tend to $-\infty$ along certain
fields. I know that the part about (a) is intuitive, but is there any way to make the argument rigorous? O the other hand, I don't see why $\operatorname{Hess}r$ must blow up.","['riemannian-geometry', 'differential-geometry']"
3525321,What kind of field does define exp function?,"In Haskell programming language, Fractional typeclass effectively represents a normed field, and has Floating as a sub-typeclass. Floating is to define $\exp$ and trigonometric functions (and related constants and functions). ""Types"" that satisfies Floating include $ℝ$ and $ℂ$ . I wonder there would be other field that defines $\exp$ . I would define $\exp$ as Taylor series: $$
\exp x = \sum_{n=0}^\infty \frac{x^n}{n!}
$$ In a field, since $n \in ℤ$ , $x^n$ is well-defined, division is well-defined, and if integers are defined as repeated addition of unity, $n!$ is well-defined. The only ways to make $\exp x$ undefined would be divergence of the series, or a division by zero. Convergence and divergence is well-defined because the field is normed, making the $\epsilon$ -definition of limit of number sequence applicable. As a consequence, a notable non-example would be $\text{GF}(p^k)$ because $p ≡ 0$ , hence $p! ≡ 0$ , resulting in a division by zero. Is there an example rather than $ℝ$ and $ℂ$ ?","['field-theory', 'abstract-algebra', 'analysis']"
3525416,"Arithmetic progression within sequence: 1/2, 1/3, 1/4, 1/5, ......","A sequence: 1/2, 1/3, 1/4, 1/5, ...... Within it, Does there exist an arithmetic progression of five fractions. And is there an arithmetic progression with more than five fractions from this sequence. I found 1/2, 1/3, 1/6 is an arithmetic progression, then I halved them to get 1/4, 1/6, 1/12, and add 1/3 to the front to make an arithmetic progression of four fractions: 1/3, 1/4, 1/6, 1/12. From here, I divided the four fractions by 5 to get: 1/15, 1/20, 1/30, 1/60, and add 1/12 to the front. Then I have an arithmetic progression of five fractions: 1/12, 1/15, 1/20, 1/30, 1/60. But I do not know if there is an arithmetic progression with more than five fractions and how to make them.","['contest-math', 'arithmetic-progressions', 'sequences-and-series']"
3525565,"Find such $x$ so that set $\{\sin x, \sin2x, \sin3x\}$ coincides with the set $\{\cos x, \cos2x, \cos3x\}$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question I recently started learning set theory so my attempt might be looking silly.
I set the sums of elements to be equal: $$
\begin{split}
\sin x+\sin 2x+\sin 3x&=\cos x+\cos 2x+\cos 3x\\
\sin 2x+2\sin 2x\cos x&=\cos 2x+2\cos 2x \cos x\\
(\sin 2x-\cos 2x)(2\cos x+1)&=0\\
\end{split}
$$ 1) $\sin2x-\cos2x=0$ $\tan2x=1$ $x=\pi/8+\pi n/2$ 2) $\cos x=-1/2$ $x=\pm\pi/3+\pi k$ Turns out that the solution of 1) is the correct answer to the initial problem, but the solution of 2) isn't.","['elementary-set-theory', 'trigonometry']"
3525571,Derivative of $\dfrac{\sqrt{3-x^2}}{3+x}$,"I am trying to find the derivative of this function $f(x)=\dfrac{\sqrt{3-x^2}}{3+x}$ $f'(x)=\dfrac{\dfrac{1}{2}(3-x^2)^{-\frac{1}{2}}\frac{d}{dx}(3-x)(3+x)-\sqrt{3-x^2}}{(3+x)^2}$ $=\dfrac{\dfrac{-2x(3+x)}{2\sqrt{3-x^2}}-\sqrt{3-x^2}}{(3+x)^2}$ $=\dfrac{\dfrac{-x(3+x)}{\sqrt{3-x^2}}-\sqrt{3-x^2}}{(3+x)^2}$ $\dfrac{-x(3+x)}{\sqrt{3-x^2}(3+x)^2}-\dfrac{\sqrt{3-x^2}}{(3+x)^2}$ $\dfrac{-x}{\sqrt{3-x^2}(3+x)}-\dfrac{\sqrt{3-x^2}}{(3+x)^2}$ At this point, I want to transform this derivative into the form of $\dfrac{3(x+1)}{(3+x)^2\sqrt{3-x^2}}$ How do I do this? This form is given by Wolfram: https://www.wolframalpha.com/input/?i=derivative+%283-x%5E2%29%5E%281%2F2%29%2F%283%2Bx%29","['calculus', 'derivatives']"
3525590,Why is Standard Deviation not defined as the expected value of the distances of points from mean? [duplicate],This question already has answers here : Motivation behind standard deviation? (6 answers) Closed 4 years ago . Variance is defined as $$V(X) = \sum (x-\mu)^2 .p(x)$$ And standard deviation is $\sigma_X = \sqrt{V(x)}$ But I feel it makes more sense to define $\sigma_X$ as $\sum( |x-\mu|.p(x))$ instead because the mod takes care of negative distances and multiplication by p(x) would give us expected value of the deviation we should expect. Then why is SD defined the way it is?,"['statistics', 'variance', 'standard-deviation']"
3525591,Finite concatenation-free languages,"Suppose, $A$ is a finite alphabet. $L \subset A^*$ is a language. Let's call $L$ concatenation-free iff $\forall u, v \in L$ we have $uv \notin L$ . Does there exist some function $c: \mathbb{N} \to (0; 1)$ , such that for any finite language $L \subset A^*$ , there exists a concatenation-free sublanguage $L_0 \subset L$ , such that $|L_0| \geq c(|A|)|L|$ ? The only thing I currently know about this problem, is that we can take $c(1) = \frac{1}{3}$ . That is a direct consequence of Erdos-Sidon theorem, that states: $\forall A \subset \mathbb{Z}$ $\exists$ a sum-free $A_0 \subset A$ , such that $|A_0| \geq \frac{|A|}{3}$ However, I do not know how to deal with $|A| \geq 2$ .","['combinatorics-on-words', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics', 'formal-languages']"
3525644,Moore-Penrose pseudoinverse and the Euclidean norm,"Section 2.9 The Moore-Penrose Pseudoinverse of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says the following: Matrix inversion is not defined for matrices that are not square. Suppose we want to make a left-inverse $\mathbf{B}$ of a matrix $\mathbf{A}$ so that we can solve a linear equation $$\mathbf{A} \mathbf{x} = \mathbf{y} \tag{2.44}$$ by left-multiplying each side to obtain $$\mathbf{x} = \mathbf{B} \mathbf{y}. \tag{2.45}$$ Depending on the structure of the problem, it may not be possible to design a unique mapping from $\mathbf{A}$ to $\mathbf{B}$ . If $\mathbf{A}$ is taller than it is wide, then it is possible for this equation to have no solution. If $\mathbf{A}$ is wider than it is tall, then there could be multiple possible solutions. The Moore-Penrose pseudoinverse enables us to make some headway in these cases. The pseudoinverse of $\mathbf{A}$ is defined as a matrix $$\mathbf{A}^+ = \lim_{\alpha \searrow 0^+}(\mathbf{A}^T \mathbf{A} + \alpha \mathbf{I} )^{-1} \mathbf{A}^T. \tag{2.46}$$ Practical algorithms for computing the pseudoinverse are based not on this definition, but rather on the formula $$\mathbf{A}^+ = \mathbf{V} \mathbf{D}^+ \mathbf{U}^T, \tag{2.47}$$ where $\mathbf{U}$ , $\mathbf{D}$ and $\mathbf{V}$ are the singular value decomposition of $\mathbf{A}$ , and the pseudoinverse $\mathbf{D}^+$ of a diagonal matrix $\mathbf{D}$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix. When $\mathbf{A}$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides  the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions. When $\mathbf{A}$ has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ . It's this last part that I'm wondering about: When $\mathbf{A}$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions. When $\mathbf{A}$ has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ . What I found confusing here is that the Euclidean norms $\vert \vert \mathbf{x} \vert \vert_2$ and $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ seemingly come out of nowhere. Prior to this section, there is no discussion of the Euclidean norm -- only of the mechanics of the Moore-Penrose Pseudoinverse. And the authors then just assert this part without explanation. So I am left wondering the following: Why is it that, when $\mathbf{A}$ has more columns than rows, then using the pseudoinverse gives us the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions? Why is it that, when $\mathbf{A}$ has more rows than columns, then using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ ? And what are the mechanics involved here? I would greatly appreciate it if people would please take the time to clarify this.","['normed-spaces', 'matrices', 'machine-learning', 'pseudoinverse', 'linear-algebra']"
3525699,A question about the definition of a strictly increasing function,"So a definition of a strictly increasing function is $~x_1 < x_2 \implies f(x_1) < f(x_2)$ . Can this be extended to be a two-way implication, namely, $~x_1 < x_2 \iff f(x_1) < f(x_2)$ ? Thanks!","['definition', 'monotone-functions', 'real-analysis']"
3525734,Maximum Likelihood Estimator for Logarithmic Distribution,"I am trying to calculate the MLE for the logarithmic distribution.
It holds $$ P(X=k) = -\frac{p^k}{k \cdot \ln(1-p} $$ Thus, the ML function is $$L_p(X_1,\dots, X_n) = \prod_{i=1}^{n} -\frac{p^{x_i}}{x_i \cdot \ln(1-p)} = p^{\sum_{i=1}^n x_i} \left(\frac{1}{\ln(1-p)}\right)^n \prod_{i=1}^n -\frac{1}{x_i} $$ and the log likelihood function is \begin{align}
\log L_p(X_1,\dots,X_n)& = \sum_{i=1}^n x_i \cdot \log(p) + n \cdot \log\left(\frac{1}{\log(1-p)}\right) + \log\left(\prod_{i=1}^n -\frac{1}{x_i}\right) 
\\&= \sum_{i=1}^n x_i \cdot \log(p) + n \cdot -\log(\log(1-p)) + \log\left(\prod_{i=1}^n -\frac{1}{x_i}\right)
\end{align} So: $$\frac{\partial L_p}{\partial p} = \frac{1}{p} \sum_{i=1}^n x_i + n \frac{1}{\log(1-p)(1-p)} \overset{!}{=} 0$$ This is equivalent to: $$\frac{1}{n} \sum_{i=1}^n x_i = - \frac{p}{\log(1-p)(1-p)}$$ Now I don't know how to go on. How do I get the estimator for $p$ ? Thanks in advance, for helping !","['statistics', 'probability-distributions', 'parameter-estimation', 'maximum-likelihood']"
3525764,Intepretation of the fact that $\operatorname{stab}_G(g \cdot x) = g\operatorname{stab}_G(x)g^{-1}$,I know how to prove the fact that $\operatorname{stab}_G(g \cdot x) = g\operatorname{stab}_G(x)g^{-1}$ but I'm having trouble finding meaning behind it. Does someone know why this is useful or has an intuitive view of this matter?,"['group-theory', 'group-actions', 'intuition']"
3525859,Positive matrix with integer eigenvalues,"Is there any way of creating a positive matrix which has integer eigenvalues? Each entry $a_{ij}$ of the matrix must be strictly greater than $0$ . I get how to create a matrix with certain eigenvalues using diagonal matrices, but I do not know how to make sure the matrix is strictly positive","['matrices', 'positive-matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3525879,Convergence in distribution: Proof strategy,"Let $X_i$ and $Y_i$ , $i\in\mathbb{N}$ , be random variables. I want to show that (asymptotic normality) $$\sqrt{n}\bigg(\frac{1}{d_n}\sum_{k=1}^{d_n}X_k +Y_k\bigg)\overset{d}{\to} N(0,\sigma^2), n\to\infty.$$ The problem is that, in my case, $X_t$ is an ugly expression, and I'm struggling to  determine the form of the variance $\sigma^2$ .
Although, I know that $\sqrt{n}/d_n\sum_{k=1}^{d_n}X_k\overset{p}{\to} 0$ , i.e., this term is $o_p(1)$ . In addition, $\sqrt{n}/d_n\sum_{k=1}^{d_n}Y_k\overset{d}{\to} N(0,\sigma_1^2)$ , where $\sigma_1^2$ is completely known. Well, Slutsky's theorem says that if $Z_1,Z_2$ are random variables such that $Z_1\overset{d}{\to}Z, Z_2\overset{p}{\to}c $ then $Z_1+Z_2\overset{d}{\to}c+Z$ , for some constant $c$ . I conclude that $$\sqrt{n}\bigg(\frac{1}{d_n}\sum_{k=1}^{d_n}X_k +Y_k\bigg)\overset{d}{\to} N(0,\sigma_1^2), n\to\infty.$$ I suspect that there is something wrong with this argument since I simply ignored the dependence/covariance of $X_k$ and $Y_k$ . Can you give me feedbacks on this? Is there somthing wrong with this? Thanks in advance!","['stochastic-processes', 'statistics', 'probability-theory', 'asymptotics']"
3525889,Splitting field and Galois group of $(x^5-1)(x^2+1)$ over $\mathbb{Q}$,"Consider $p(x) = (x^5-1)(x^2+1)$ . Then, its splitting field is $\mathbb{Q}(e^{\frac{2\pi i}{5}}, i)$ . Thus, $f\in \text{Gal}(\mathbb{Q}(e^{\frac{2\pi i}{5}}, i)/\mathbb{Q})$ maps $\omega = e^{\frac{2\pi i}{5}}$ to any of $\omega^k$ for $k=1,...,4$ and $i$ to $\pm i$ . In that way, I can conclude that $|\text{Gal}(\mathbb{Q}(e^{\frac{2\pi i}{5}}, i)/\mathbb{Q})|= 4 \cdot 2 = 8$ . Now, how do I know which of the groups of order $8$ it is? It might be $\mathbb{Z}_2 \times \mathbb{Z}_4$ because of element orders but I am not sure.","['field-theory', 'galois-theory', 'abstract-algebra', 'splitting-field']"
3525912,"Find locus of $z$, where $z$ satisfies $\arg \frac{(z- z_1)(z_2 - z_3)}{(z - z_3)(z_2 - z_1)} = \pi $","How to find the locus of the point $z$ , satisfying $$\arg \frac{(z- z_1)(z_2 - z_3)}{(z - z_3)(z_2 - z_1)} = \pi $$ Can anyone please extensively describe how one should tackle these kind of problems?","['complex-analysis', 'complex-numbers']"
3525931,Can we extract back the value of a vector after convolution with another vector whose first value and absolute values are only known,"Assume we have Hadamard-Walsh matrix whose size is 4 x 4 as below: And we have random vector, for example: $h=[2, 3,   1,   5]$ . $S$ is the result of convolution operations  between vectors $h$ and one row chosen randomly from the matrix $W$ , let's choose row 2 which is $W_2 = [1, -1, 1, -1]$ . So, the result of $S = h*W_2 = [2,  1,  0,  5,  -7,  4,  -5]$ , where $*$ denote the convolution operation. What I am looking for is to estimate which row from matrix $W$ was convoluted with $h$ based on $S$ , or in other words the possibility to calculate $h$ . As known, that can be done since convolution is nothing but multiplication of toeplitz matrix with a vector. For example, we can build the toeplitz matrix of $h$ , and then calculate the value of $h$ , in case if $h$ has more than one solutions, choose the value which make the equation equal to $|W| = 1$ . For example, taken the case of Hadamard-Walsh matrix whose size is 2 x 2 as below: and $h$ is generated randomly $h = [2, 4]$ , so in that case the result of convolution between $W_2: [1, -1]$ and $h$ is : $S = [2,     2,    -4]$ , Hence, we can calculate the value of $h$ and $W_2$ based on $S$ by creating $S = toelitz_{ matrix} *  W_2$ as follows: Therefore, we should have three equations : 1- $h_1 W_{21} = 2$ ---> since $W_{21}$ is equals to 1 in all cases, so $h_1 = 2$ , $W_{21} = 1$ . 2- $h_2 W_{21} + h_1 W_{22} = 2$ ---> $W_{21}$ and $h_1$ are already known from equation 1, then the equation (2) will be $h_2 + 2 W_{22} = 2$ ---> this is equation 2 3- $h_2 W_{22} = -4$ , ---> this is equation 3 Based on equations 2 and 3, we conclude that either $h_2 = 4$ or $h_2 = -2$ , therefore, $h_2 = 4$ since we know that $|W_{22}| = 1$ , but we don't know it's sign, but based on $h_2 = 4$ and equations 3, we knew that $W_{22} = -1$ . So, as seen above, we could extract the vector $h$ based on first value of vector $W_2$ and the known absolute values of matrix $W$ . My question, Is there an easier method or known algorithm (method) can implemented in that case to avoid the higher complexity resulted in case if size matrix $W$ and length of vector $h$ are big? Or can machine or deep learning algorithm can be used to implemented that easily? Thank you in advance.","['machine-learning', 'statistics', 'signal-processing', 'linear-algebra']"
3525950,Local behavior of rational map near fundamental locus,"Assume $f: \mathbb {CP}^n \dashrightarrow\mathbb {CP}^m$ is a rational map, and $B$ is the fundamental locus (means on which $f$ cannot define). Take $b\in B$ . Is it possible that there exists $u\neq v$ in the target, such that the closures of $f^{-1}(u)$ and $f^{-1}(v)$ are both tangent to a same line $L\simeq\mathbb P^1$ passing through $b$ at $b$ ? I guess this is not possible. If $B$ is smooth, it would be clear as we can blowup along $B$ to get a morphism, say $\overline f:\overline {\mathbb {CP}^n} \to \mathbb {CP}^m$ . $\overline f^{-1}(u)$ and $\overline f^{-1}(v)$ will approach to the same point $\overline b$ at the exceptional divisor, as $f^{-1}(u)$ and $f^{-1}(v)$ approach to $b$ in the same direction. But this is not so clear if $B$ is not smooth.","['complex-geometry', 'algebraic-geometry']"
3526015,Difficult Bessel integral,"In my research involving quantum field theory and computing correlation functions in perturbation theory, I encountered the following integral that seems pretty hard to solve: $$f(x)=\int_{0}^{\infty}\frac{dz}{z(z^2+1)}\text{arcsinh}^2(z)J_0(xz)$$ I wonder if anyone has any insights towards a solution. A series expansion and it's convergence properties would be fantastic as well.","['integration', 'contour-integration', 'improper-integrals', 'mathematical-physics']"
3526047,Reference Request and Category Theoretic Interpretation of a Result on Banach Spaces,"Today I learned in class the following result, which my professor stated without proof: Given a Banach space $V$ , there exists a compact Hausdorff space $X$ such that $V$ embeds into $C(X)$ as a closed subspace. Recall that $C(X)$ is the space of all continuous complex valued functions on $X$ , which is a unital $C^*$ -algebra (the only ones in fact). First, does anyone know where I can find a proof of this result? Secondly, is there some category theoretic interpretation of this result? I won't even presume to know much category theory, but it seems to say something like the unital commutative $C^*$ -algebras are universal objects in some sense...?","['c-star-algebras', 'banach-spaces', 'functional-analysis', 'category-theory']"
3526164,Probability concept that distinguishes likelihood of sequences 0110101011101... and 000000000000...?,"Say we have a coin and want to decide if it is fair or not. We flip it many times. Consider two cases. Say the result is a sequence like 0110101011101... The result is 000000000000... In the first case the assumption that the coin is fair sounds reasonable, while in the the second the coin is obviously completely biased. However, for a truly fair coin the probabilities of the two sequences are the identical. What is then the concept that distinguishes the first one as more likely? How does one quantify that? Consider also the third case The result is 010101010101010... This actually does not look like a coin at all, but it seems to pass naive tests which boil down to comparing number of zeros to number of ones. Is there a sense in which it is less random then (1)? Or I have to invent a new rule if somebody gives me a more cleverly crafted sequence?",['probability']
3526292,$I_n=\int_0^1{\frac{x^n}{x^n+1}}$. Prove $I_{n+1} \le I_n$ for any $n \in \mathbb N$,"$$I_n=\int_0^1{\frac{x^n}{x^n+1}}$$ Prove $\lim_{n\to\infty}{I_n} = 0$ Here is what I tried.
First, I rewrite $I_n$ . $$I_n=\int_0^1{1-\frac{1}{x^n+1}}=1 - \int_0^1{\frac{1}{x^n+1}}$$ Now the limit becomes: $$L=1-\lim_{n\to\infty}\int_0^1{\frac{1}{x^n+1}}$$ Next I try to solve the limit of the integral using Squeeze Theorem, with no success.
Using the fact $0\le x \le 1$ I get to the following double inequality: $$\int_0^1\frac{1}{x^{n-1}+1} \le \int_0^1\frac{1}{x^{n}+1} \le 1 $$ $$I_{n-1} \le I_n \le 1$$ I don't know what to do next, I would greatly appreciate some help.","['limits', 'calculus', 'definite-integrals']"
3526328,Determining How Addition in New Data Point Effects Hyperparameters in Gaussian Process with Squared Exponential Kernel,"I want to determine how the inclusion of new data effects hyperparameters of the Gaussian Process kernel. For reference assuming square exponential kernels as provided here : $$K(x,x') = \sigma^2\exp\left(\frac{-(x-x')^T(x-x')}{2l^2}\right)$$ So the derivative with respect to length scale determines what the effect to the kernel when the lengthscale changes as follows: $$\frac{\partial K}{\partial l} = \sigma^2\exp\big(\frac{-(x-x')^T(x-x')}{2l^2}\big) \frac{(x-x')^T(x-x')}{l^3}$$ I however would like to determine what is the change or effect of a single new data point to the lengthscale. What should be the symbolic expression I need to evaluate the derivative of? Is it $$\frac{\partial l}{\partial \mu}$$ of the GP? where $\mu$ is the predictive mean of the GP as follows: $$\mu(x^*)=K(x^*,X)^\top[K(X,X)+\sigma_n^2\mathbf{I}]^{-1} \mathbf{y_n}$$ If so how can the derivative expression be formulated. (Initial expression atleast, I should be able to workout derivitave from there itself)","['machine-learning', 'covariance', 'derivatives']"
3526348,Finite Galois Cohomology of Abelian Variety,"Let $l/k$ be a finite galois extension let $A$ be an abelian variety over $k$ . Then $A(l)$ is a $Gal(l/k)$ -module. Hence it makes sense to study $H^1(Gal(l/k),A(l))$ . I know that for $A=\mathbb{G}_m$ , we have $H^1(Gal(l/k),\mathbb{G}_m)=0$ (okey, this is not an abelian variety). I'm wondering if there is a good set of conditions on $A$ such that this vanishes?","['algebraic-number-theory', 'algebraic-geometry', 'galois-cohomology']"
3526357,Surjectivity of x^2 - floor(x)^2,"How would I go about proving $f(x) = x^2 - \lfloor x\rfloor^2$ is surjective when $f: \mathbb{R}^{\geq 0} \to \mathbb{R}^{\geq 0} $ ? Not sure where to start...
I believe it's true but but it almost seems too easy but proving it is a challenge for me.","['proof-writing', 'functions']"
3526389,Evaluate $\int_{0}^{\infty} \frac{x^2+x+1}{x^6+x^4+1} dx$,$$\int_{0}^{\infty} \frac{x^2+x+1}{x^6+x^4+1} dx $$ Wolfram says $1.80276\ldots $ Calculating this seems complicated to me because the residues are pretty hard to find and I have tried the infinite residue method but it didn't work out.,"['complex-analysis', 'calculus', 'definite-integrals']"
3526410,Is every possible chart a member of some maximal smooth atlas?,"If $M$ is a topological $n$ -manifold (edit: that admits at least one smooth structure) and I select any open set $U \subseteq M$ , and I find that there exists some $\varphi: U \rightarrow \varphi(U)$ where $\varphi(U) \subseteq \mathbb{R}^n$ and $\varphi$ is a homeomorphism, then the pair $(U, \varphi)$ are a chart on $M$ . Is it necessarily the case that some smooth structure $\overline{\mathcal{A}}$ exists so that $(U, \varphi) \in \overline{\mathcal{A}}$ ? A little more context. My current understanding is that there are a number of ways to conceptualize a smooth structure: as a maximal smooth atlas, as an equivalence class of smooth atlases, or as a maximal set of mutually compatible charts. I've found a couple of great answers about smooth structures that consider the smooth structure from the perspective of equivalent atlases  ( Manifold and maximal atlas and Why maximal atlas ). And I also understand that given even just one smooth atlas $\mathcal{A}$ , one can essentially generate a unique maximal smooth atlas $\overline{\mathcal{A}}$ such that $\mathcal{A} \subseteq \overline{\mathcal{A}}$ . So my question is more from the perspective of the individual charts. I am almost certain that not every open set $U$ is suitable to be a domain for a chart. For example, $M$ is, itself, an open set. But certainly not every $M$ is globally homeomorphic to $\mathbb{R}^n$ . So, I think it's the case that not every $U \subseteq M$ is homeomorphic to $\mathbb{R}^n$ . But for those $U$ that are, is it certainly the case that the chart $(U, \varphi)$ is included in some smooth atlas? Thanks!","['manifolds', 'smooth-manifolds', 'differential-geometry']"
3526423,How many different figures can be formed with a regular polygon of $n$ vertices and a number $d$ of diagonals of this polygon?,"Here we have a simple, but a very hard problem: How many different figures (or graphs) can be formed with a regular polygon of n vertices and a number d of diagonals of this polygon? Consider T ( n,d ) the total number of distinct figures formed by the polygon of n vertices and d diagonals. The question is: how to determine T ( n,d )? The figure below shows some examples for the pentagon (n=5) and hexagon (n=6), to better understand the problem: OBS: Rotated or reflected figures will also be considered the same! I thought about using Polya´s Enumeration  Theory, but I didn't understand how I can do that. There seems to be a similarity with the method of counting graphs by  Polya´s Counting Theory, or still similar to the circular coloring problem solved by Polya. Does anyone have any ideas or methods to solve this problem?","['combinations', 'graph-theory', 'geometry', 'abstract-algebra', 'combinatorics']"
3526451,Uniqueness (a.s.) of regular conditional distributions,"Let $(\Omega, \mathcal{F}, \mathbf{P})$ be a probability space, $(\mathcal{X}, \mathcal{B})$ a measurable space, and $X : \Omega \to \mathcal{X}$ a random element of $\mathcal{X}$ . Also, let $\mathcal{G}$ be a sub- $\sigma$ -algebra of $\mathcal{F}$ . Question. How unique are regular conditional distributions of $X$ given $\mathcal{G}$ ? A regular conditional distribution of $X$ given $\mathcal{G}$ is a function $P : \Omega \times \mathcal{B} \to [0, 1]$ such that the following properties hold. For all $\omega \in \Omega$ , the map $B \mapsto P(\omega, B)$ from $\mathcal{B}$ into $[0, 1]$ is a probability measure on $(\mathcal{X}, \mathcal{B})$ . For all $B \in \mathcal{B}$ , the map $\omega \mapsto P(\omega, B)$ from $\Omega$ into $[0, 1]$ is $(\mathcal{G}, \mathcal{B}_{[0, 1]})$ -measurable (where $\mathcal{B}_{[0, 1]}$ denotes the Borel $\sigma$ -algebra of $[0, 1]$ ). For all $B \in \mathcal{B}$ and all $G \in \mathcal{G}$ , we have $$
\mathbf{P}(\{X \in B\} \cap G) = \int_G P(\cdot, B) \, d\mathbf{P}.
$$ (Items 2. and 3. just say that, for each $B \in \mathcal{B}$ , the random variable $P(\cdot, B)$ is a version of the conditional probability $\mathbf{P}(X \in B\mid \mathcal{G})$ .) Suppose $P$ and $Q$ are two regular conditional distributions of $X$ given $\mathcal{G}$ . On the one hand, it is not necessarily true that $P(\omega, B) = Q(\omega, B)$ for all $\omega \in \Omega$ and $B \in \mathcal{B}$ . For example, for any $\mathbf{P}$ -null set $N \in \mathcal{F}$ and any probability measure $\mu$ on $(\mathcal{X}, \mathcal{B})$ , we can define $P^\prime : \Omega \times \mathcal{B} \to [0, 1]$ by $$
P^\prime(\omega, B)
= \begin{cases}
P(\omega, B), & \text{if $\omega \notin N$,} \\
\mu(B), & \text{if $\omega \in N$.}
\end{cases}
$$ Then $P^\prime$ is another regular conditional distribution of $X$ given $\mathcal{G}$ , but it might hold that $P(\omega, B) \neq P^\prime(\omega, B)$ for some $\omega \in \Omega$ and $B \in \mathcal{B}$ . On the other hand, suppose $B \in \mathcal{B}$ is fixed. Then we have $$
\int_G P(\cdot, B) \, d\mathbf{P}
= \mathbf{P}(\{X \in B\} \cap G)
= \int_G Q(\cdot, B) \, d\mathbf{P}
$$ for every $G \in \mathcal{G}$ .
Since $P(\cdot, B)$ and $Q(\cdot, B)$ are $\mathcal{G}$ -measurable, this implies that there exists a $\mathcal{P}$ -null set $N \in \mathcal{F}$ such that $P(\omega, B) = Q(\omega, B)$ for all $\omega \in \Omega \setminus N$ .
However, this null set depends on $B$ , so we can't a priori conclude that there exists a $\mathbf{P}$ -null set $N^\prime \in \mathcal{F}$ such that $P(\omega, B) = Q(\omega, B)$ for all $\omega \in \Omega \setminus N^\prime$ and all $B \in \mathcal{B}$ . More Precise Question. Suppose $P$ and $Q$ are two regular conditional distributions of $X$ given $\mathcal{G}$ .
Does there always exist a $\mathbf{P}$ -null set $N \in \mathcal{F}$ such that $$
P(\omega, B) = Q(\omega, B)
$$ for all $\omega \in \Omega \setminus N$ and all $B \in \mathcal{B}$ ? I think I remember reading that this is true somewhere, but I can't find a proof. I'm fine with assuming that any measurable spaces in question are standard Borel, if needed.","['conditional-probability', 'measure-theory', 'probability-theory', 'probability']"
3526618,Ideal of $\mathbb Z[x] $ generated by $\mathit x^2+1$ and $ 3x+1$,"Let $\mathit I $$\,$ be the ideal of $\mathbb Z[x] $ generated by $\mathit x^2+1$ and $ 3x+1$ . 
Find the positive integer $\mathit n$ such that $\mathit I$ $\cap \mathbb Z$ = $\mathit n$$\mathbb Z$ I find $(\mathit x^2+1)(9)+ (3x+1)(-3x+1)= 10 $ , So It is expected that 'n =10'
But I can't find for n=1, 2, 5 Any hint will be appreciated.","['ring-theory', 'abstract-algebra', 'ideals']"
3526739,Evaluating $P=\frac{a^2}{2a^2+bc}+\frac{b^2}{2b^2+ac}+\frac{c^2}{2c^2+ab}$ when $a+b+c=0$,"Let $a,b,c$ such that $$a + b + c =0$$ and $$P=\frac{a^2}{2a^2+bc}+\frac{b^2}{2b^2+ac}+\frac{c^2}{2c^2+ab}$$ is defined.
  Find the value of $P$ . This is a very queer problem.","['algebra-precalculus', 'systems-of-equations']"
3526772,New wrong recurrence formula for Bell numbers,"Bell numbers are the numbers counting the total partitions on a set with $n$ distinct elements. Explanation : Consider a set like $A:=\left\{x_{1},x_{2},...,x_{n}\right\}$ A partial equivalence relation is either reflexive or it is not, so the number of partial equivalence relation on a set with cardinality $n$ is $B_n$ plus the relations which are partial equivalence relation but not reflexive, to make these relations we can consider that a set with $n$ element cannot be reflexive if at least one of the ordered pairs $(x_i,x_i)$ is not in that relation,so at first we can remove one of the ordered pairs $(x_i,x_i)$ $(0\le i\le n)$ and continue till we have $n-1$ ordered pair in the form $(x_i,x_i)$ removed (It cannot be $n$ because if we have $n$ ordered pairs then we are counting a relation which has been already counted by $B_n$ ) Also given these kind of the partial equivalence relation, they can be combined with each other to make another   equivalence relation that is not reflexive. For example for $A=\left\{1,2,3\right\}$ the total conditions that a partial equivalence relation is not reflexive is: $$\left\{\right\}\,\,\,\,\,\,\,\,\,\,\,\,\,{{3}\choose{0}}$$ $$\left\{\left(1,1\right)\right\}\left\{\left(2,2\right)\right\}\left\{\left(3,3\right)\right\}\,\,\,\,\,\,\,\,\,\,\,\,\,{{3}\choose{1}}$$ $$\left\{\left(11\right),\left(22\right)\right\}\left\{\left(1,1\right),\left(3,3\right)\right\}\left\{\left(2,2\right),\left(3,3\right)\right\}\,\,\,\,\,\,\,\,\,\,\,\,\,{{3}\choose{2}}$$ $$\left\{\left(11\right),\left(22\right),\left(12\right),\left(21\right)\right\}\left\{\left(1,1\right),\left(3,3\right),\left(13\right),\left(31\right)\right\}\left\{\left(2,2\right),\left(3,3\right),\left(23\right),\left(32\right)\right\}\,\,\,\,\,\,\,\,\,\,\,\,\,{{3}\choose{2}}{{2}\choose{2}}$$ The relations in the last row are made form combining the second and third row, the number of choosing $2$ element form the set $A$ is ${{3}\choose{2}}$ ( indeed the number of way to choose $2$ of these elements to make an ordered pair in the form $(x_i,x_i)$ ) and there is ${{2}\choose{2}}$ ways to make a new partial equivalence relation with the $2$ elements. If we sum the number of these kind of partial equivalence relations that are not reflexive with those partial equivalence relations which are reflexive we get : $$\color{blue}{9+B_3}=10+5=15=\color{blue}{B_4}$$ Which is indeed the number of partial equivalence relations on $A$ . I used this strategy and tried for $n=4$ , finally could derive the recurrence formula for Bell numbers: $$2^{n}-1+\sum_{k=2}^{n-1}\sum_{m=2}^{k}{{n}\choose{k}}{{k}\choose{m}}+B_n=B_{n+1}$$ Mapping $k-2\mapsto k$ and $m-2\mapsto m$ equivalently the formula can be rewritten as: $$2^{n}-1+\sum_{k=0}^{n-3}\sum_{m=0}^{k+2}{{n}\choose{k+2}}{{k+2}\choose{m+2}}+B_n=B_{n+1}$$ With the initial value $B_0=1$ the formula gives: $$0+B_{0}=0+1=\color{blue}{1}=\color{blue}{B_{1}}$$ $$1+B_{1}=1+1=\color{blue}{2}=\color{blue}{B_{2}}$$ $$3+B_{2}=3+2=\color{blue}{5}=\color{blue}{B_{3}}$$ $$10+B_{3}=10+5=\color{blue}{15}=\color{blue}{B_{4}}$$ $$37+B_{4}=37+15=\color{blue}{52}=\color{blue}{B_{5}}$$ $$136+B_{5}=136+52=\color{red}{188}\ne \color{red}{B_{6}}$$ The formula does not give the right number for $B_{6}$ , but I'm sure the validity of the other Bell numbers is not accidental, so why the formula gives such a wrong number? where I was wrong? Finally I should say that I came up with this calculation, because I cannot understand why the total number of partial equivalence relations on a set with cardinality $n$ is $B_{n+1}$ , so It would be really appreciated if someone explain that with more details.","['equivalence-relations', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'bell-numbers']"
3526822,"If $f : \mathbb R \to [-2 , 2]$ with $(f(0))^2 + (f'(0))^2 =85$, then there exists $x \in (-4 , 4)$ such that $f(x) +f''(x) = 0$ and $f'(x) \neq 0$.","For every twice differentiable function $f : \mathbb R \to [-2 , 2]$ with $(f(0))^2 + (f'(0))^2 =85$ there exists $x \in (-4 , 4)$ such that $f(x) +f''(x) = 0$ and $f'(x) \neq 0$ . I was trying to get the answer by constructing a function $g(x)  = f(x) ^2  + f'(x) ^2$ . But I can not proceed much. Can anyone help me?","['calculus', 'derivatives', 'real-analysis']"
3526834,A combinatorial identity - Hockey Stick generalization,"There is a well known identity (the so called ""Hockey-stick identity"") asserting that: $$\sum_{j=0}^m \binom{r+j}{j} = \binom{m+r+1}{r+1}$$ For some proofs see this . I need to prove a kind of generalization, namely: $$\sum_{j=0}^m \binom{r+j}{j}\binom{s+j}{j} = \sum_{j=0}^s \binom{r}{j}\binom{s}{j} \binom{m+r+s+1-j}{r+s+1}$$ For every $r\geq s\geq 0$ . Of course, setting $s=0$ in the latter gives the original identity. The problem is that I'm not being able to prove the second using the same ideas as those that work for the first one. Any kind of help would be very appreciated.","['summation', 'binomial-coefficients', 'combinatorics', 'balls-in-bins']"
3526850,"If $x^2 - y^2 = 1995$, prove that there are no integers x and y divisible by 3","If $x^2 - y^2 = 1995$ , prove that there are no integers $x$ and $y$ divisible by $3$ . I'm quite new to this. As in the title I would like to ask if my reasoning here is correct and if my answer would be considered acceptable. If $x$ or $y$ is divisible by $3$ , but not both, then: $\begin{align}x \equiv \pm 1 \pmod{3} \land y \equiv 0 \pmod{3} 
&\iff x^2 \equiv 1 \pmod{3} \land y^2 \equiv 0 \pmod{3}
\\
&\iff x^2 - y^2 \equiv 1 \pmod{3}\end{align}$ So if $x$ or $y$ is divisible by $3$ but not both then no values for $x$ and $y$ will ever make $x^2 - y^2$ divisible by $3$ . If $x$ and $y$ are divisible by $3$ : Since both $x$ and $y$ are divisible by $3$ they can be expressed $x = 3m$ and $y = 3n$ for some integers $m$ , $n$ . $(3m)^2 - (3n)^2 = 1995 \iff 9m^2 - 9n^2 = 1995 \iff 3(m^2-n^2) = 665$ And it's easy to see that there are no whole number solutions for m and n, and thus there are no whole number solutions for x and y such that $x^2 - y^2 = 1995$ where x or y (or both) is divisible by 3. Is this a reasonable proof? Are there more elegant ways to do it?","['algebra-precalculus', 'solution-verification']"
3526852,Transitive closure of a finite relation,"If $R$ is a relation on $A$ , we know that the relation $$T=R\cup R\circ R\cup R\circ R\circ R\cup\cdots$$ is know as the transitive closure of a relation and it is the smallest transitive relation containing $R$ . It is almost trivial to prove that it is the smallest transitive relation containing $R$ . But further it says that if $\vert A\vert =n$ , then the transitive closure of $R$ is $$T=R\cup R\circ R\cup R\circ R\circ R\cup\cdots\cup (R\circ R\circ \cdots \circ R)$$ where the last composition is $n-1$ times. Now, I am finding it difficult to prove that this relation is transitive. Please help","['elementary-set-theory', 'relations', 'discrete-mathematics']"
3526854,differentiability and continuity of $f: \mathbb{R}^2 \to \mathbb{R}$,"I have a problem with this function. I have to study where the function is continuous  and where is differentiable. The function is $f : \Bbb R^2 → \Bbb R$ : $$
f(x, y) = \begin{cases}
   x + \sin(y)& x\le y \\
       y + \sin(x) & x>y \\ 
\end{cases}
$$ Continuity (i think that is wrong):
I know that $$ sin(y) + x $$ is continuos in $\mathbb{R}^2$ because $sin(y)$ and $x$ are continuos. (for $sin(x)+ y$ is the same) My problem is about the point $(x = y)$ .
What i have to do? I tried with: $$
\lim_{(x,y)\to (a,a)}f(x,y)
$$ Differentiability I compute the Partial derivative: $$
\frac{\partial f}{\partial x} = \begin{cases}
   1& x\le y \\
       cos(x) & x>y \\ 
\end{cases}
$$ and $$
\frac{\partial f}{\partial y} = \begin{cases}
   cos(y)& x\le y \\
       1 & x>y \\ 
\end{cases}
$$ So i think that is differentiable in $\mathbb{R}^2$ \{(x,y): x=y}. For (x = y) I used the definition of differentiability: $$
\lim_{(h,k)\to (0,0)}(f(x+h,y+k) - f(x,y) - \frac{\partial f}{\partial y}(x,y) *h- \frac{\partial f}{\partial x}(x,y)*k)/\sqrt((h^2 + k^2))
$$ Is it correct?","['continuity', 'derivatives']"
3526909,"If $g$ is continous on $[a,b]$ with bounded upper and lower derivatives on $(a,b)$, will $g$ be Lipschitz?","By the Mean Value Theorem from ordinary calculus one knows that, if $f$ is continous on $[a,b]$ and differentiable on $(a,b)$ with bounded derivatives, then $f$ has to be Lipschitz on $[a,b]$ . Now I ask a more general question: If $g$ is continous on $[a,b]$ with bounded upper and lower derivatives on $(a,b)$ , will $g$ be Lipschitz? If not, what would be a counterexample? I really don't know how to proceed. One idea I had was approximating $g$ with a piecewise linear function $\phi$ , but I don't know if this gets us anywhere.","['continuity', 'derivatives', 'lipschitz-functions', 'real-analysis']"
3527001,"Basic Functional equation $f\left(\sqrt{\frac{x^2+y^2}{2}}\right)=\sqrt{f(x)\cdot f(y)},\;\forall x,y\geq0$","question: Find all continuous functions $f:[0,\infty)\to(0,\infty) $ such that $f\left(\sqrt{\dfrac{x^2+y^2}{2}}\right)=\sqrt{f(x)\cdot f(y)},\;\forall x,y\geq0$ my attempt: Since it doesn't satisfy homgeneity so, it must be some non linear function.I Tried substitutions like $(0,0), (x,x)$ but everything leads to equality i don't know how to even start solving it.Any help will be appreciated.
Try giving simple solutions which are less rigorous & intuitive. note: the range excludes $0$","['functional-equations', 'functions']"
3527074,Limit of sequence for an alternating series,"Find the limit of the sequence $y_n = \sum_{k=1}^n(-1)^k\frac{\ln{k}}{k}$ .
I have tested the limit for convergence using Lebiniz's test for alternating series and it is indeed convergent. I can't seem to figure out how to get to the answer, though.","['limits', 'sequences-and-series']"
3527080,Integrability of a polynomial under root,"Under what conditions can $$\int\sqrt[n]{a_0\cdot x^m+a_1 \cdot x^{m-1}+...+a_m}\ dx$$ be expressed in terms of elementary functions? $\sqrt{x^2+3x+5}$ can be expressed, while $\sqrt{x^3+3x+5}$ not.","['integration', 'calculus']"
3527097,How many edges can you add to a tree with $n$ vertices so it stays planar?,"How many edges can you add to a tree with $n$ vertices so it stays planar? My attempt: Any planar graph has $|E|\le 3n-6$ , and tree has $n-1$ edges, so we can add at most $2n-5$ edges. I tried a lot of examples, and I was always able to add exactly $2n-5$ edges no matter in which order I placed them. But I am stuck without proof.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3527286,Surface area of an oblate spheroid using gaussian quadrature,"I want to compute the surface area of an oblate spheroid using gaussian quadrature, the parametrization of the oblate spheroid is given by: $$x = a \cdot \sin\theta \cdot \cos \phi \\
y = a \cdot \sin\theta \cdot \sin \phi \\
z = b \cdot \cos\theta $$ Where $b<a$ , in order to compute this integral I am using a Gauss-legendre quadrature to compute the points and the weights for the integral for the first octant on the spheroid so $\theta = [0,\pi/2]$ , $\phi = [0,\pi/2]$ . So using the weights and the nodes of a Gauss-Legendre quadrature of order $n$ I can define the weights and the nodes in $\theta$ and $\phi$ by: $$X_{\theta/\phi} = \frac12 \cdot(X_{\text{Gauss-Leg}} + 1)\cdot \frac{\pi}2 \\
W_{\theta/\phi} = \frac12\cdot W_{\text{Gauss-Leg}} \cdot\frac{\pi}2$$ So having this defined, I can compute the surface integral as: $$\int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot dS $$ Where I think $dS = a^2\cdot b \cdot\rho^2 \cdot \sin\theta \cdot d\theta \cdot d\phi$ . I find a bit confusing the $\rho$ here but I have tried using the standard definition of $\rho$ as: $$\rho = \sqrt{x^2 + y^2 + z^2} = \sqrt{a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta} $$ So the expresion that I am using for solving this integral is: $$\int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot a^2\cdot b \cdot(a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta) \cdot \sin\theta $$ So I am doing something wrong since the surface of the oblate spheroid can be computed using: $$S =  2\pi \cdot \left(a^2 + \frac{b^2}{\sin(ae)} \ln\Bigl(\frac{1 + \sin(ae)}{\cos(ae)} \Bigr) \right)$$ with $ae = \arccos(b/a)$ . And the obtained result is not the same than the analytical surface. I have made an small python script that computes both results and prints them: import numpy as np
from scipy.special import roots_legendre

#Define a and b
b = 2.
a = 100.

#Compute the Weights and nodes

x_phi, w_phi = roots_legendre(150)
x_theta, w_theta = roots_legendre(100)

#Translate them
x_phi = 0.5 * (x_phi + 1.) * np.pi/2.
x_theta = 0.5 * (x_theta + 1.) * np.pi/2.
w_phi = 0.5 * w_phi * np.pi/2.
w_theta = 0.5 * w_theta * np.pi/2.

#Compute the integral

integral = 0
for i in xrange(len(x_phi)):
    for j in xrange(len(x_theta)):
        integral += w_phi[i] * w_theta[j] * a**2 * b  * (a**2 * np.sin(x_theta[j])**2 + b**2 * np.cos(x_theta[j])**2) * np.sin(x_theta[j])


print(""Estimated int: %f"" %(8*integral))

ae = np.arccos(b/a)
surface = 2*np.pi*(a**2 + b**2/np.sin(ae) * np.log((1+np.sin(ae))/np.cos(ae)))

print(""Real int: %f"" %(surface)) So what am I doing wrong? (I have to mention that this is just a simple test, what I really want to do is to compute the surface integral of any arbitrary function on this spheroid)","['multivariable-calculus', 'quadrature', 'surface-integrals', 'numerical-methods']"
3527292,Prove that $\sum_{i=1}^n\frac{x_i}{\sqrt[nr]{x_i^{nr}+(n^{nr}-1)\prod_{j=1}^nx^r_j}} \ge 1$ for all $x_i>0$ and $r \geq \frac{1}{n}$.,"Prove that, for all $x_1,x_2,\ldots,x_n>0$ and $r \geq \frac{1}{n}$ , it holds that $$\sum_{i=1}^n\frac{x_i}{\sqrt[nr]{x_i^{nr}+(n^{nr}-1)\prod \limits_{j=1}^nx^r_j}} \ge 1.$$ This is a slightly modified version of my earlier question here (which corresponds to the case $r=1$ ). The case $n=3$ and $r=\frac{2}{3}$ can be reduced to the inequality problem of IMO 2001 ( Problem 2 ). The case $r=\frac{1}{n}$ can be reduced to $$\sum_{i=1}^n\frac{x_i^{n-1}}{x_i^{n-1}+(n-1) \prod_{j\neq i}x_j}\geq \sum_{i=1}^n\,\frac{x_i^{n-1}}{x_i^{n-1}+\sum_{j\neq i}\,x_j^{n-1}}=1\,.$$ (Note that we have an equality case when $n=2$ and $r=\frac1n$ here, regardless of the values of the variables $x_i$ .)","['algebra-precalculus', 'conjectures', 'summation', 'inequality']"
3527297,"""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps.""","I was reading the Wikipedia article for complex affine spaces , which says the following: Affine geometry, broadly speaking, is the study of the geometrical properties of lines, planes, and their higher dimensional analogs, in which a notion of ""parallel"" is retained, but no metrical notions of distance or angle are. Affine spaces differ from linear spaces (that is, vector spaces) in that they do not have a distinguished choice of origin. So, in the words of Marcel Berger , ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" [1] Accordingly, a complex affine space , that is an affine space over the complex numbers, is like a complex vector space, but without a distinguished point to serve as the origin. What Marcel Berger said is what interests me: ... ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" What is meant here by ""forgetting about the origin by adding translations to the linear maps""? Can someone please explain, using basic linear algebra and geometry, and with mathematics included, what this means?","['geometry', 'vector-spaces', 'linear-algebra', 'linear-transformations', 'affine-geometry']"
3527309,How to prove that $(A \bigtriangleup B) - C = (A - C) \bigtriangleup (B - C)$?,"Let $A,B,C$ be sets. Prove that $(A \bigtriangleup B) - C = (A - C) \bigtriangleup (B - C)$ . I've already checked the Venn diagrams and it seems like said identity is true but I have no idea how to prove it using known properties of sets and axioms from set theory. It'd be awesome if you could help me, thank you!",['elementary-set-theory']
3527332,"How to solve for x, when you know quadrilateral's sides and angles created by the diagonal?","This problem was asked from 8th and 9th graders in a contest. You have an quadrilateral $ABCD$ with $AB=4x-y$ , $BC=3x-4$ , $CD=y$ , and $DA=5x-2$ . You draw a line $BD$ which makes a right triangle with $\angle CBD=36°$ and in the triangle $ABD$ $\angle DBA=126°$ . Solve for $x$ . I was able to solve this with a calculator but this problem is supposed to be solved without one. I tried to find an answer to this problem online but I couldn't find one, probably because my math vocab in English isn't that great.","['trigonometry', 'geometry']"
3527350,"Spectrum of operator on $L^2([0,1])$","I need to calculate the spectrum of the operator $T$ for $f \in L^2([0,1])$ defined by: \begin{equation}
(Tf)(x) = \int_0^1 (x+y)f(y)dy.
\end{equation} I know that $T$ is compact and self-adjoint so the residual spectrum is empty and the eigenvalues are real and a closed subset of $[-||T||, ||T||]$ . So I let $\lambda$ be an eigenvalue so I know that $\lambda f(x) = (Tf)(x)$ . By differentiating twice, I found that $\lambda f''(x) = 0$ but I don't really know how I can continue.","['spectral-theory', 'functional-analysis']"
3527365,"Given that $z = i + i^{2016} + i^{2017}$, find $|z^{10}|$.",I am told that $$z = i + i ^ {2016} + i ^ {2017}$$ and I have to find $|z^{10}|$ . This is what I tried: $$i ^ {2016} = (i^4)^{504} = 1 ^ {504} = 1$$ $$i ^ {2017} = i \cdot (i^4)^{504} = i \cdot 1 = i$$ So we have that: $$z = 1 + 2i$$ And then I worked towards finding $z ^ {10}$ . $$z = 1 + 2i$$ $$z^2 = 1 + 4i + 4i^2 = -3 + 4i$$ $$z^4 = ... = -7 - 24 i$$ $$z^5 = z^4 \cdot z = (-7-24i)(1 + 2i) = ... = 41 - 38i$$ $$z^{10} = (z^5)^2 = (41 - 38i)^2 = ... = 237 -3116i $$ So we then have: $$|z^ {10} | = \sqrt{237^2 + 3116^2} = \sqrt{56169 + 964656} = \sqrt{1020825} = 15\sqrt{4537}$$ Is this correct? Is there a better way to solve this? More efficient/faster way?,"['algebra-precalculus', 'complex-numbers']"
3527470,Compute $\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx$,"Compute $$\lim\limits_{n\to \infty} \int\limits_0^1 x^{2019} \{nx\} dx,$$ where $\{a\}$ denotes the fractional part of the real number $a$ . I firstly tried to apply the substitution $nx=t$ , but the computations didn't look nice, so I couldn't make any further progress. I also tried to use the mean value theorem for integrals, but it was also a dead end.","['integration', 'limits', 'calculus', 'definite-integrals']"
3527497,"Given a recurrence formula, evaluate $\lim\limits_{n\to \infty} n^2 x_n^3$","Define a sequence $(x_n)_{n\geq 0}$ with a fixed initial term $x_0 > 0$ such that: $$x_0 + x_1+\ldots+x_n=\frac{1}{\sqrt{x_{n+1}}}$$ Evaluate $$\lim_{n\to \infty} n^2 x_{n}^3$$ My attempt: I should define a new sequence $s_n = \displaystyle\sum_{i = 0}^nx_i$ with the recurrence formula: $$s_{n+1} = s_n+\frac{1}{s_n^2}$$ $(s_n)_{n\geq 0}$ is increasing and divergent, and I want the limit of: $$n^2x_n^3 = \frac{n^2}{s_{n-1}^6}$$ So, I can look instead for the limit: $$\lim_{n\to \infty} \frac{s^3_n}{n}$$ For $s_n^3$ , the recurrence formula gives $$s_{n+1}^3 = \left(\frac{1+s_n^3}{s_n^2}\right)^3$$ Looking at the function $f:(0, \infty) \to (0, \infty),\ f(x) = \dfrac{(x+1)^3}{x^2}$ this is increasing from a certain point forward and it feels that I should squeeze it between $3n$ and $3n+\text{something negligible}$ , but I can't give a solid argument for this.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3527712,Proving that $k(t)=\frac{|\alpha'\wedge\alpha''|}{|\alpha'|^3}.$,"I understand this question has already been asked, but I don't exactly follow that answer. I saw I had a similar process but I got lost in their explanation. Following do Carmo, Section 1.5-Exercise 12 Let $α:I→R^3$ be a regular parametrized curve (not necessarily by arc length) and let $β:J→R^3$ be a reparametrization of $α(I)$ by the arc length $s=s(t)$ , measured from $t_0∈I$ . Let $t=t(s)$ be the inverse function of $s$ and set $dα/dt=α′$ , $d^2α/dt^2=α′′$ , etc. Prove that the curvature of $\alpha$ at $t\in I$ is $$k(t)=\frac{|\alpha'\wedge\alpha''|}{|\alpha'|^3}.$$ So, my first thought it to start with what we already know $$s=s(t)=\int_{t_0}^t|\alpha'(t)|dt\implies s'(t)=|\alpha'(t)|.$$ If we rewrite this is a differentiable form, we get $$s'(t)=\frac{\alpha'(t)\cdot\alpha'(t)}{|\alpha'(t)|}.$$ Differentiating both sides of the equation we get $$s''(t)=\frac{2\alpha'(t)\cdot\alpha''(t)}{|\alpha'(t)|}\frac{dt}{ds}\implies s'''(t)=2\frac{\alpha'(t)\cdot\alpha''(t)}{|\alpha'(t)|}.$$ But now, I have no idea what I'm supposed to do? I don't know how I'm supposed to get the $k(t)$ from this equation.","['curves', 'curvature', 'differential-geometry']"
3527713,Derivative of matrix exponential $\exp(A+xB)$ at $x=0$,"Consider two (Hermitian) matrices $A$ and $B$ . Is there a nice expression for  the following? $$ \boxed{ \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} = \; ? }$$ Of course, if $A$ and $B$ commute, this is simply $B \exp{(A)}$ . One thing I tried was the Suzuki-Trotter formula: \begin{align}
\boxed{\frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0}} 
&= \frac{\mathrm d}{\mathrm d x} \left. \left( \lim_{N \to \infty} \left[ \exp\left( \frac{A}{N} \right) \exp \left( x \frac{B}{N} \right) \right]^N \right) \right|_{x=0} \\
&= \lim_{N\to \infty}  \sum_{n=1}^N \exp\left( \frac{n}{N} A \right) \frac{B}{N}  \exp\left( \frac{N-n}{N} A \right) \\
&= \left( \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N e^{\frac{n}{N}A }B\; e^{-\frac{n}{N}A } \right)  e^A \\\
&= \boxed{ \int_0^1 e^{t A} B \;e^{(1-t)A} \; \mathrm d t } \; .
\end{align} Is this as close as it gets to a closed form? One thing we can do is go to the eigenbasis of $A$ , such that we can explicitly perform the integration over $t$ . If we index the eigenvectors of $A$ by $i$ , with corresponding eigenvalues $\lambda_i$ , then we can express the answer in this basis: \begin{equation}
\boxed{ \left( \frac{\mathrm d}{\mathrm d x}  \exp\left( A + x B \right)\big|_{x=0} \right)_{ij} = \frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} B_{ij}} \;,
\end{equation} where $(\cdot)_{ij}$ are the entries of a matrix in the eigenbasis of $A$ . (Note that if $\lambda_i = \lambda_j$ , we replace $\frac{e^{\lambda_i}-e^{\lambda_j}}{\lambda_i-\lambda_j} \to e^{\lambda_i}$ , which is also consistent with l'Hopital's rule.)","['integration', 'matrix-exponential', 'matrices', 'matrix-calculus', 'derivatives']"
3527761,Monty Hall Analogue,"I am studying this variation of the Monte Hall problem. There are three prizes as before (1 car and 2 goats). The host picks two out of the three items for you at random. The host then views both items and then shows you one of the items which is a goat. At what odds will you bet with the host that the other item is the car? Consider the exact same set up as before, this time the host doesn't view any of the items and then randomly shows you one. If he shows you a goat (note this conditional, since it is possible for him to show you the car) at what odds would you bet that the other item is the car? Attempt: The first problem I am getting the same answer as the original Monty Hall problem (2/3). however this doesn't make any sense, since the option to switch is what increases my odds from 1/3 to 2/3. Nonetheless, this is how I did it. $P(car|goat shown)=P(\text{car and goat drawn})P(car )$ As for the second problem, I do not have the slightest clue how to proceed since I do not know why my odds should decrease?","['monty-hall', 'combinatorics', 'problem-solving']"
3527765,Correlation of $x + E(y\mid x)$ and $y + E(x\mid y)$ is bigger than the correlation of $x$ and $y$,"Suppose $x$ and $y$ are random variables with positive variance.  Let $\rho_{x,y} \in [-1,1]$ denote their (Pearson) correlation coefficient.  Let $\mathbb{E}(x\mid y)$ denote the conditional expectation of $x$ given $y$ and similarly $\mathbb{E}(y\mid x)$ is the conditional expectation of $y$ given $x$ .  Construct two functions: $$\lambda(x) \equiv x + \mathbb{E}(y\mid x) \text{ and } \mu(y) \equiv y + \mathbb{E}(x\mid y).$$ We can thus think of $\lambda$ and $\mu$ themselves as random variables. Assume the variances of both $\lambda$ and $\mu$ are strictly positive.
Let $\rho_{\lambda,\mu}$ be the correlation coefficient for these random variables. Conjecture: $\rho_{x,y} \le \rho_{\lambda,\mu}$ . I've spent some time trying to prove this.  It is easy to show when the conditional expectations are linear, e.g. when $x$ and $y$ are jointly normal.  In that case, $|\rho_{x,y}| = |\rho_{\lambda,\mu}|$ .  I've also used Monte Carlo to construct many examples where $x$ and $y$ are discrete.  I have not found a counterexample.  I feel like this must be a known result, but my google skills have failed me.  Any ideas are appreciated.","['correlation', 'statistics', 'probability']"
3527791,How can I calculate this (rather tricky) limit?,$$\lim\limits_{n \to \infty} \cos^{n^2} \left (\frac{2x}{n} \right)$$ Any hints and/or help is greatly appreciated.,"['limits', 'calculus']"
3527793,Portmanteau Theorem in Multiple Dimensions,"I have a significant issue in trying to prove the claim highlighted in red (see picture below), which is in Van der Vaart's ""Asymptotic Statistics"" text, where we assume $X_n = (X_{n,1}, ..., X_{n,d})^T $ and $X = (X_1, ..., X_d)^T$ are both in $\mathbb{R}^d$ and $P(X\le x) = P(X_1 \leq x_1, ..., X_d \leq x_d)$ - elementwise vector inequalities. Let $a = (a_1, ..., a_d)^T$ and $b = (b_1, ..., b_d)^T$ be in the set of continuity points for $P(X \leq x)$ with $a_i < b_i \forall i$ . $I = \prod_{i=1}^d (a_i, b_i]$ in this case. If $d=1$ , the claim is trivial (as $P(X_n \in I) = P(X_n \leq b_1) - P(X_n \leq a_1)$ which converges to the required quantity by assumption), but for $d > 1$ , I am completely lost. My attempt at proving it is as follows: $ P(X_n \in I) = P(X_n \leq b, X_n > a)$ $= P(X_n \leq b) - P(X_n \leq b \cap (\cup_{i=1}^d \{X_{n,i} \leq a_i\}))$ $= P(X_n \leq b) - P(\cup_{i=1}^d \cap_{j=1}^d \{X_{n,j} \leq b_j \} \cap \{X_{n,i} \leq a_i\})$ $= P(X_n \leq b) - P(\cup_{i=1}^d \{X_{n,i} \leq a_i \} \cap_{j \ne i} \{X_{n,j} \leq b_j \} )$ (this is all just applying definitions) Obviously the first term converges to $P(X \leq b)$ but I have no clue how to show that the second term converges to $P(\cup_{i=1}^d \{X_{i} \leq a_i \} \cap_{j \ne i} \{X_{j} \leq b_j \} )$ Can anyone offer any help?","['weak-convergence', 'statistics', 'probability']"
3527860,Edwards Calculus chapter 4 problem 81,"I can see that the terms arise from the differentiation of $\log(1+x^{2^r} )$ and tried to do it but couldn't figure it out in the end. Show that $$
\frac{1}{1+x} + \frac{2x}{1+x^2} + \frac{4x^3}{1+x^4} + \frac{8x^7}{1+x^8} + \ldots = \sum_{n=0}^{\infty} \frac{2^{n} x^{(2^n -1)}}{1+x^{(2^n)}} =  \frac{1}{1-x} \qquad \text{when}~|x|<1
$$","['calculus', 'sequences-and-series']"
3527928,Simplifying the boolean expression $AB+BC'D'+AC+AD$,"I'd like to simplify the expression $$AB+BC'D'+AC+AD$$ Logically, I understand why the AB term isn't needed, if both A and B are true, then at least one of the other terms will always be true, making the AB term redundant. However, I cannot for the life of me apply the laws of boolean algebra to actually simplify it.","['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics', 'algebra-precalculus']"
3527971,Characterizing measurability in product spaces,"Let $X,Y$ be non-empty sets.
Define $$\Phi:\mathbb{R}^{X\times Y}\to ({\mathbb{R}^{Y}})^X, f\mapsto\left(x\mapsto\left(y\mapsto f(x,y)\right)\right).$$ Let $\mathcal{F}_X$ be a $\sigma$ -algebra of subsets of $X$ and $\mathcal{F}_Y$ be a $\sigma$ -algebra of subsets of $Y$ .
Let $\mathcal{M}$ be the set of measurable functions from $(X\times Y,\mathcal{F}_X\otimes\mathcal{F}_Y)$ into $\mathbb{R}$ and $\mathcal{N}$ be the set of measurable functions from $(Y,\mathcal{F}_Y)$ into $\mathbb{R}$ . Then $$\forall f\in\mathcal{M}, \forall x\in X, \Phi(f)(x)\in\mathcal{N},$$ i.e. $$\forall f\in\mathcal{M}, \Phi(f):X\to\mathcal{N}.$$ Now, it's clear that for some choice of $(X,\mathcal{F}_X)$ (e.g. if $(X,\mathcal{F}_X)$ is $\mathbb{R}$ with its Borel $\sigma$ -algebra) there exists $f\in \mathbb{R}^{X\times Y}$ for which $\Phi(f):X\to\mathcal{N}$ while $f\notin \mathcal{M}$ . So, it seems that something is missing to characterize the measurability of $f$ from $(X\times Y,\mathcal{F}_X\otimes \mathcal{F}_Y)$ into $\mathbb{R}$ in terms of some property of $\Phi(f)$ (i.e. requiring that $\Phi(f): X\to\mathcal{N}$ is a necessary but in general not sufficient condition). My guess is that maybe we can find a a $\sigma$ -algebra $\mathcal{F}$ of subsets of $\mathcal{N}$ such that $f\in\mathcal{M}$ iff $\Phi(f)$ is measurable from $(X,\mathcal{F}_{\mathcal{X}})$ into $(\mathcal{N},\mathcal{F})$ . So the question: does there exist a $\sigma$ -algebra of subsets of $\mathcal{N}$ , say $\mathcal{F}$ , such that $$\forall f\in\mathbb{R}^{X\times Y}, (f\in\mathcal{M})\iff\left(\Phi(f) \text{ is measurable from $(X,\mathcal{F}_X)$ into $(\mathcal{N},\mathcal{F})$}\right)?$$ If not, does there exist any results in this direction assuming more, e.g. that the functions are bounded or that the underlying spaces are (separable) metric spaces? Any (also partial) result in this direction (or reference) is welcome.","['measure-theory', 'measurable-functions', 'product-space']"
3528129,Why isn't $E(\vert X\vert |\mathcal{G})=\vert E(X|\mathcal{G})\vert$?,"Let $(\Omega,\mathcal{F},P)$ be a probability space, $\mathcal{G}$ sub- $\sigma$ -algebra of $\mathcal{F}$ , and $X\in L^1(P)$ . We have $$\begin{aligned}
Y & := E(X|\mathcal{G})\\ 
  & = E(X^+ - X^-|\mathcal{G})\\
  & = E(X^+|\mathcal{G}) - E(X^-|\mathcal{G})\\
\end{aligned}
$$ As $E(X^+|\mathcal{G})$ is non-negative and $-E(X^-|\mathcal{G})$ is non-positive we have $Y^+=(E(X^+|\mathcal{G}) - E(X^-|\mathcal{G}))\vee 0 = E(X^+|\mathcal{G})$ and similarily $Y^-=E(X^-|\mathcal{G})$ . Thus, $$
\vert Y\vert = Y^+ +Y^- = E(X^++X^-|\mathcal{G}) = E(\vert X\Vert\mathcal{G})$
$$ In most texts, I just see $\vert Y \vert \leq E(\vert X\Vert |\mathcal{G})$ , so whats wrong about the above? Thank you.","['conditional-expectation', 'probability-theory', 'absolute-value']"
3528144,expected value of non-negative random variable,If $X$ is a random variable  and $X\geq 0$ then $E[X] =\int_{\mathbb {R}}xf(x)dx\geq 0$ . Does it mean that we can rewrite $E[X] = \int^\infty_{0}xf(x)dx\geq 0$ ?,"['probability-distributions', 'probability-theory']"
3528187,"Why is $U(\mathcal{H})$ a topological group, but not $GL(\mathcal{H})$?","Let $U(\mathcal{H})$ be the group of unitary operators on a (complex) Hilbert space $\mathcal{H}$ . Then $U(\mathcal{H})$ with the strong topology is a topological group with composition. In particular, composition is continuous: Consider a net $(U_i, V_i)_{i \in I} \subseteq U(\mathcal{H}) \times U(\mathcal{H})$ s.t. $(U_i, V_i) \rightarrow (U,V)$ . Then since $U(\mathcal{H}) \times U(\mathcal{H})$ has the product topology, this is equivalent to $U_i \rightarrow U$ and $V_i \rightarrow V$ , which, in turn, is equivalent to $\forall x \in \mathcal{H}: U_i(x) \rightarrow U(x), V_i(x) \rightarrow V(x)$ by definition of the strong operator topology. Now, we want to show that $U_i V_i \rightarrow UV$ . Again, by the strong operator topology, this is equivalent to point-wise convergence. So let $x \in \mathcal{H}$ be fixed, then we want to show that for any $\varepsilon > 0$ there exists a $i_0 \in I$ s.t. $\forall i \geq i_0: U_i V_i (x) \in B_{\varepsilon}(UVx)$ . Indeed \begin{align}
\Vert (U_i V_i - U V) x \Vert &= \Vert (U_i V_i - U_i V + U_i V - U V) x \Vert  \\
&\leq \Vert U_i (V_i - V) x \Vert + \Vert (U_i - U) V x \Vert \\
&\leq \underbrace{\Vert U_i \Vert}_{=1} \cdot \Vert (V_i - V) x \Vert + \Vert (U_i - U) Vx \Vert\\
&= \Vert (V_i - V)x \Vert + \Vert (U_i - U) Vx \Vert\\
\end{align} Now choose $i_0 \in I$ s.t. $(U_i,V_i) \in \{B \in \mathcal{B}(\mathcal{H}) \vert \Vert (B - U) Vx \Vert < \varepsilon/2 \} \times \{B \in \mathcal{B}(\mathcal{H}) \vert \Vert (B - V) x \Vert < \varepsilon/2 \} =: N(U,\{Vx\},\varepsilon/2) \times N(V, \{x \}, \varepsilon/2)$ , which are neighborhoods of $U$ and $V$ respectively (by definition of the strong operator topology). Then by definition we have $(\Vert (V_i - V)x \Vert + \Vert (U_i - U)x \Vert) \leq \varepsilon$ for $i \geq i_0$ . As far as I know, $GL(\mathcal{H})$ the group of invertible bounded linear operators on $\mathcal{H}$ is not a topological group, precisely because the composition is not continuous. However, I do not see how the above proof fails for $GL(\mathcal{H})$ . The only time one uses the boundedness of $U(\mathcal{H})$ is in $\underbrace{\Vert U_i \Vert}_{=1}$ . However, since $U_i$ is pointwise convergent for each $i \in I$ , this term would still be bounded above by the uniform boundedness principle. Question : Where does the proof fail for $GL(\mathcal{H})$ instead of $U(\mathcal{H})$ ? Ideally, I am also looking for a counter example.","['hilbert-spaces', 'general-topology', 'functional-analysis', 'topological-groups']"
3528242,"$X$ a random variable, where $-1\leq X\leq \frac{1}{2}, \mathbb E[X]=0$. What is the maximal value of $\mathbb E[X^2]$?","$X$ a random variable, where $$-1\leq X\leq \frac{1}{2}, \mathbb E[X]=0$$ Find the maximal value of $\mathbb E[X^2]$ . I came across a similar question, asking if the following  inequality is always true $$\mathbb E[X^2]\leq \frac{1}{4}$$ The answer is no, for example: $$\mathbb P(X=-1) = \frac{1}{3},\mathbb P(X=\frac{1}{2}) = \frac{2}{3}\implies\\ \mathbb E[X^2]= ((-1)^2)\cdot(\frac{1}{3})+(\frac{1}{2})^2\cdot(\frac{2}{3})=\\ \frac{1}{3} +\frac{2}{3\cdot 4} =\frac{1}{2}$$ I was wondering how can one find the maximum, for any discrete or continuos random variable. Remark: I'm not sure this question has a ""nice"" solution.","['expected-value', 'probability-theory', 'random-variables']"
3528273,Definition of dual representation of a finite group,"I am trying to understand the dual representation. But I am struggling to understand even the definition. Let $\rho: G \to GL(V)$ be a representation of a finite group. Then we have the dual space $V^{*}$ that consists of all linear maps from $V$ to $\mathbb{C}$ . We then get the dual representation of $G$ $$
\rho^*: G\to GL(V^{*})
$$ My question is: How is this defined? From here I see  something like $$\rho^*(g)(f) = f(\rho(g^{-1})) $$ But I don't understand this. If $f\in V^*$ , then one can't evaluate $f$ at $\rho(g^{-1})$ . From here I see something like $$
\rho^*(g) = \rho(g^{-1})^T
$$ Here there is a transpose. But this also doesn't make sense to me because $p^*(g)$ is supposed to take an element of $V^*$ to $V^*$ . I guess I am looking for a definition like $$
\rho^*(g)(f)(v) = \dots
$$ for $g\in G, f\in V^*, v\in V$ .","['representation-theory', 'duality-theorems', 'abstract-algebra', 'finite-groups']"
3528360,Number of preorder relations on a set related to the open problem about preorder relations,"Consider a set $A=\left\{1,2,3\right\}$ ,I want to count the number of preorder relations on this set, so there is two cases two consider,either the relation is symmetric or it is not, if the relation is symmetric then it's also an equivalence relation and the number of equivalence relations on this set is $B_3$ , where $B_k$ is the kth Bell number , but we also have to count the preorder relation which are not symmetric : $$\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(1,2\right)\right\}$$ $$\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(1,3\right)\right\}$$ $$\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(3,1\right)\right\}$$ these are some of examples of a preorder relations on the set $A$ that are not symmetric, here I think there should be a strategy for counting them, first we should note that since the preorder relation is reflexive, hence the ordered pairs $\left(1,1\right),\left(2,2\right),\left(3,3\right)$ appears in all of the relations, but if we want to make a preorder relation which is not symmetric hence we first should choose one of the elements in $A$ ,there are ${{3}\choose{1}}$ of them and then for each one of the chosen elements there exist $2$ ways for which of the remaining elements in the set $A$ the chosen element is going to be paired with, for example if we choose $1$ then there are $2$ ways to choose an element from the remaining ones in $A$ ( the remaining elements in $A$ are $2$ and $3$ ),that $1$ can be paired with, so now we can make two relations which are not symmetric : $$\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(1,2\right)\right\}$$ and $$\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(1,3\right)\right\}$$ The number of all these cases is : $$\color{blue}{
{{3}\choose{1}}\cdot2}\tag{I}$$ In all of these relations we just add one pair to the fixed set, for example in $\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right),\left(1,3\right)\right\}$ we just add one pair $\left(1,3\right)$ to the set, but what about adding two pairs?
Well I used the similar strategy and concluded that  there exist $$\color{red}{
{{3}\choose{1}}{{2}\choose{1}}\cdot2\cdot1+{{3}\choose{1}}\cdot2}\tag{II}$$ of them. For example assume we have the fixed set $\left\{\left(1,1\right),\left(2,2\right),\left(3,3\right)\right\}$ then we should decide which pair should be contained in this set, from above there exist $${{3}\choose{1}}\cdot2$$ relations of this from,but now we  want to add another pair, we already have chosen $1$ element from the set, so there are two elements left in the set, the same strategy can be applied so there is ${{2}\choose{1}}\cdot1$ ways to make the second pair and since each time we removed $6$ we add $6$ . For example if we choose $1$ and let it to be paired with $2$ then we have $\left(1,2\right)$ as an ordered pair,but we also going to make another ordered pair with elements $2$ and $3$ , if we choose $3$ then it can be just paired with $2$ (since if we pair $3$ with $1$ then we have a symmetric relation which already has been counted by $B_3$ ) , or if we choose $2$ then there is two elements that $2$ can be paired with, we actually removed the condition that $2$ is paired with $1$ ,and this is just one of the $${{3}\choose{1}}\cdot2$$ preorder relations , so we need to add this relation, so the number of preorder relations in this way is $$
{{3}\choose{1}}{{2}\choose{1}}\cdot2\cdot1+{{3}\choose{1}}\cdot2$$ which is the formula in $({\text{II}})$ Summing $({\text{I}})$ and $({\text{II}})$ and considering that a preorder relation can also be an equivalence relation implies the number of preorder relations on $A$ is: $$\color{blue}{{{3}\choose{1}}\cdot2}+
\color{red}{{{3}\choose{1}}{{2}\choose{1}}\cdot2\cdot1+{{3}\choose{1}}\cdot2} +B_3$$ $$=\color{blue}{6}+\color{red}{18}+5=29$$ as Wikipedia says it should be $29$ As I know there does not exist any explicit formula for counting the number of preorder relations on a set with arbitrary $n$ elements and it's one of the open problems , I just want to know does my approach has any closed form or it does not. Also it would be highly appreciated if someone explain what Wikipedia exactly has done. (please explain how can I find the number of preorder relations for $n$ arbitrary)","['equivalence-relations', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'bell-numbers']"
3528364,Bound angle between vectors on n-simplex,"I'm trying to bound from above (tight as I can find) the angle between two vectors ( $\boldsymbol{x}, \boldsymbol{y} $ ) on the standard n-simplex in $\Bbb R^{n+1}$ ( $\sum_{k=0}^{n} x_k = 1$ , $x_k\geq0$ , and the same for $y_k$ ), given that distance between them is less than a constant $\epsilon $ (i.e $\| \boldsymbol{x} -\boldsymbol{y}\| \leq \epsilon)$ . My try so far: In general, the angle between vectors is: \begin{align} 
\theta = \arccos \frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|}
\end{align} and on the stranded simplex it satisfies $ 0\leq \theta \leq \pi/2$ , since the inner proudct in non-negative for vectors on the stranded simplex. $arccos$ is monotonic decreasing, so in order to bound (tightly) the angle, it suffice to bound from below the following function: \begin{align} 
\frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|}
\end{align} under the constraints that $\boldsymbol{x}$ and $\boldsymbol{y}$ are on the simplex, and the distance between them is at most $\epsilon$ . I am not sure how to do it, or how to procced from here. Thanks in advance!","['geometry', 'multivariable-calculus', 'calculus', 'linear-algebra', 'differential-geometry']"
3528408,"Let $n, k\in\Bbb{N},n\ge 2, k\ge 2$ and $(G,\cdot)$ be of order $n$. Prove $(n,k)=1 \iff$ for all $H\le G$ we have $H=\{x\in G \mid x^k\in H\}$","Let $n, k \in \mathbb{N}, n\ge 2, k\ge 2$ and $(G,\cdot)$ be a group with $n$ elements. Prove that $(n,k)=1$ if and only if for all subgroups $H$ of $G$ we have that $H=\{x\in G \mid x^k\in H\}$ . I think that the last equality is pretty odd. For "" $\implies$ "", I observed that for any subgroup $H$ of $G$ the function $f:H\to H, f(x)=x^k$ is a bijection because $(n,k)=1$ . From here it would follow that for any $y\in H$ there exists an unique $x\in H$ such that $y=x^k$ . I am not sure if this is enough to reach our conclusion in this case.","['group-theory', 'abstract-algebra', 'finite-groups']"
3528437,How to solve the ordinary differential equation $ x^2 y'' - 2 x y' + 2y = x^4 \mathrm{e}^x $,Please tell me how to solve this differential equation. $$  x^2 y'' - 2 x y' + 2y = x^4 \mathrm{e}^x  $$ I tried to solve it but finally I stuck tell me how to go further more if anyone gives me a hint(I want to know how to find paticular integral?).I will appreciate about it.i posted my way below please tell me any hint. Here's my work:,['ordinary-differential-equations']
3528448,Equal volume iff there is a diffeomorphism,"Let $M$ be a compact oriented smooth manifold.
Let $w_1$ and $w_2$ be two volume forms.
Let integral of both these forms over $M$ be equal i.e $\operatorname{vol}(M)$ be equal wrt both forms. Show that there is a diffeomorphism $f$ from $M$ to $M$ such that $f^*(w_2)=w_1$ Of course if such an f exists then by change of variable formula the volumes shall be equal. Also it was told in class that apparently this isn't the case for symplectic manifolds and this is a global invariant. Any comments on that?","['symplectic-geometry', 'smooth-manifolds', 'multivariable-calculus', 'differential-topology', 'differential-geometry']"
3528452,"Expected number of query pings in a simple ""concentration"" game","This question is based on a recently posted clever memory game (that uses $k=16$ ) that can be explained as follows: There are $k$ cards numbered $1 \to k$ face down randomly ordered in a row.  You (the player) make one ""ping"" consisting of choosing any card you like, turning it over, and seeing its number.  If it is $1$ , the card remains face up (remains ""exposed""), but if it is any other number, the card is returned face down.  Then you repeat.  The only card that can become ""exposed"" is the one that has a value $1$ greater than the highest existing ""exposed"" card.  You continue until all $k$ cards have are ""exposed."" In short, the sequence of cards that becomes ""exposed"" must be $1, 2, \ldots, k$ . Your final score is your total number of pings.  The lower the number of pings, the better.  Thus of course you do your best to remember all cards you have turned over (but are not yet ""exposed""). Clearly the optimal score is $k$ , where by great luck you just happen to ping cards in the order $1, 2, \ldots , k$ .  But this is quite rare.  (It occurs with probability $\prod\limits_{i=1}^k \frac{1}{i}$ .) Questions Perfect memory :  If you assume you have perfect memory of any card you have seen, and you play optimally, what is your expected score (as a function of $k$ )? No memory :  If you assume you have no memory of any previous card you have seen, what is your expected score (as a function of $k$ )?  Of course you never ping an ""exposed"" card, but by ""no memory"" I mean you might indeed randomly choose a card you have previously turned over (even on the current ""round""), even if it will not become ""exposed."" No memory but systematic search :  Assume you have no memory but after each successful ping (leading to an ""exposed"" card) you repeat your sequence of pings, always starting at the left of the line of cards and pinging each available non-exposed card to the right until hitting the one card that can be ""exposed.""  (This guarantees the number of pings per exposed card becomes lower and lower as the game progresses.)","['combinatorics', 'probability']"
3528462,Evaluate $\lim\limits_{n\to\infty}\left(\sqrt [n+1]{\frac {a_{n+1}}{b_{n+1}}}-\sqrt[n]{\frac {a_n}{b_n}}\ \right)$,"Consider two sequences $(a_n)_{n\ge 1}$ and $(b_n)_{n\ge 1}$ of
  positive real numbers such that $$\lim_{n\to\infty} \frac {a_{n+1}}{n^2\cdot a_n}=x>0$$ and $$\lim_{n\to\infty} \frac {b_{n+1}}{n\cdot b_n}=y>0$$ Evaluate: $$\lim_{n\to\infty}\left(\sqrt [n+1]{\frac {a_{n+1}}{b_{n+1}}}-\sqrt[n]{\frac  {a_n}{b_n}}\ \right)$$ My thoughts: Intuitively $\dfrac{a_{n+1}}{a_n} \sim xn^2$ and $\dfrac{b_{n+1}}{b_n} \sim yn$ , so $a_n \sim x \sqrt[n]{(n!)^2}$ and $b_n \sim y\sqrt[n]{n!}$ . 
Since $\lim\limits_{n\to \infty} \dfrac{n}{\sqrt[n]{n!}} = e$ , we should get: $$\sqrt[n+1]{\frac{a_{n+1}}{b_{n+1}}} \sim \frac{x}{y}\cdot \sqrt[n+1]{(n+1)!} \sim\frac{x}{y}\cdot \frac{n+1}{e}$$ and the final answer should be $\dfrac{x}{ye}$ . But how do we write this with sound arguments?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3528490,Limit of $\frac{2^n}{n^{\sqrt{n}}}$,"$$\lim_{n\to\infty}\frac{2^n}{n^{\sqrt{n}}}$$ The way I see it is that denominator is super-exponential and the numerator is only exponential, so this must be $0$ but wolfram is saying it is infinite. How do I evaluate this limit?","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3528504,"Convexity of $\{x\in \mathbb R^n,\;x^TAx\leq t\}$ for all $t$ implies that $A$ is p.s.d.","Let $A\in \mathbb R^{n\times n}$ be symmetric and such that for all $t\in \mathbb R$ , the set $\{x\in \mathbb R^n,\;x^TAx\leq t\}$ is convex. Prove that $A$ is positive semi-definite. It suffices to prove that the eigenvalues of $A$ are $\geq 0$ . Consider $\lambda$ an eigenvalue of $A$ and $x$ an eigenvector with unit norm. Then the hypothesis rewrites as $(\lambda\leq t )\;\wedge (y^TAy\leq t) \implies \forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t$ . If $y$ is an eigenvector with unit norm associated to some other eigenvalue $\lambda'$ with $\lambda'\leq t$ , this rewrites as $(1-\mu)^2\lambda + \mu^2 \lambda'\leq t$ . This is redundant as it already follows from $\lambda\leq t$ and $\lambda'\leq t$ . I don't see how to prove $\lambda\geq 0$ from $\forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t$ .","['positive-semidefinite', 'matrices', 'symmetric-matrices', 'convex-analysis', 'quadratic-forms']"
3528558,How to prove this rotation identity in a coordinate-free way?,"Background I've been working on Exercise 1.1 in the book ""An Introduction to Finite Tight Frames"", which I paraphrase as follows: Let $u_1, u_2, u_3$ be any set of equally spaced unit vectors in $\mathbb{R}^2$ so that for a $2 \pi /3$ rotation matrix counterclockwise $R$ we have $u_2 = R u_1$ and $u_3 = R^2 u_1$ . Let $f$ be any vector in $\mathbb{R}^2$ . Show that: \begin{align*}
    f = \frac{2}{3} \left( u_1 \langle u_1, f \rangle + 
    u_2 \langle u_2, f \rangle + 
    u_3 \langle u_3, f \rangle \right) 
\end{align*} Basically, the intuition is that the sum of the projections onto three equally spaced unit vectors returns the original vector, scaled up by 3/2. The approach given in the solutions, which makes sense to me, is to pick some particular $\{u_1, u_2, u_3\}$ , form $V = [u_1, u_2, u_3]$ , and then show that for these particular $u_i$ vectors we have $V V^*= \frac{3}{2} I$ . The result then follows by noting that any rotated version $TV$ (where $T$ is a rotation matrix) of these vectors also will satisfy the above equation, as $(TV)(TV)^* = TVV^*T^* = T \frac{3}{2}I T^* = \frac{3}{2}I$ . Do we need to pick coordinates? However, I ended up picking coordinates to calculate $V V^*$ for a particular $\{u_1, u_2, u_3\}$ . I was hoping there would be a coordinate-free way to solve this problem. Letting $u_2 = Ru_1$ , $u_3 = R^2 u_1$ and $V = [u_1, u_2, u_3]$ , can we show that $V V^*  = \frac{3}{2} I$ in a coordinate-free way? An attempt at solution We can write $V V^*$ as: \begin{align}
V V^* &= u_1 u_1^* + u_2 u_2^* + u_3 u_3^*\\
&= u_1 u_1^* + Ru_1 (Ru_1)^* + R^2 u_1 (R^2 u_1)^*\\
&= u_1 u_1^* + Ru_1 u_1^* R^{-1} + R^2 u_1 u_1 ^* (R^2)^{-1}
\end{align} (Note that we have used the fact that $R$ is an orthogonal matrix). I wasn't really sure where to go from here. It might be worth noting that that if $\{I = R^0, R, R^2\}$ is the rotation group with three elements, and $\gamma_a$ denotes conjugation by $a$ , then we have: \begin{align}
V V^*  &= \gamma_{R^0} u_1 u_1^* + \gamma_{R^1} u_1 u_1^* + \gamma_{R^2} u_1 u_1^*\\
&= (\gamma_{R^0} + \gamma_{R^1} + \gamma_{R^2})   u_1 u_1^* \\
&= (\gamma_R^0 + \gamma_R^1 + \gamma_R^2) (u_1 u_1^*)
\end{align} where $u_1$ is some arbitrary unit vector. However, while this looks neat, I'm not sure how to simplify from here. Any thoughts appreciated.","['inner-products', 'group-theory', 'linear-algebra']"
3528663,"Quotient group, group action and quotient space","Let $G$ be a group acting faithfully on a topological space $X$ , and $N$ a normal and abelian subgroup of $G$ . 
Does this mean that one can define a faithful action of the quotient group $G/N$ on the quotient space $X/N$ ?","['normal-subgroups', 'group-theory', 'group-actions']"
