question_id,title,body,tags
4793183,Union of subsets.,"Let $A_1,...,A_n$ $n$ subset of $\mathbb{R}$ . Show that: $$\bigcup_{k=1}^{n}A_k=\bigcup_{k=1}^{n-1}\left(A_k\setminus A_{k+1}\right)\cup\left(
A_n\setminus A_1\right)\cup\left(\bigcap_{k=1}^{n}A_k\right)$$ We can easily show that $$\bigcup_{k=1}^{n-1}\left(A_k\setminus A_{k+1}\right)\cup\left(
A_n\setminus A_1\right)\cup\left(\bigcap_{k=1}^{n}A_k\right)\subset \bigcup_{k=1}^{n}A_k$$ For the other inclusion, i began my solution like this, let $x\in \bigcup_{k=1}^{n}A_k$ . we have two cases: the first case: $x\in\bigcap_{k=1}^{n}A_k$ in this case we acheive the result. the second case: $x\notin\bigcap_{k=1}^{n}A_k$ here we have $x\in \bigcup_{k=1}^{n}A_k$ and $x\notin\bigcap_{k=1}^{n}A_k$ but how can we find a $k\in\left\{1,...,n\right\}$ such that: $x\in A_k$ and $x\notin A_{k+1}$ with $A_{n+1}=A_1$ .",['elementary-set-theory']
4793225,How to understand the $\infty$-Wasserstein distance,"For two probability distributions $\mu$ and $\nu$ defined on $X$ , the $p$ -th Wasserstein distance between the two of them is defined as $$W_p(\mu,\nu) = \left(\inf_{\pi\in\Pi(\mu,\nu)}\int_{X\times X}c(x,x')^{p}\,\mathrm{d}\pi(x,x')\right)^{1/p}.
 $$ We also have $\infty$ -Wasserstein distance defined as the limit of p-th Wasserstein distance, i.e., $$W_\infty(\mu,\nu) = lim_{p \rightarrow \infty} W_p(\mu,\nu).$$ However, I am having a hard time understanding this $\infty$ -Wasserstein distance. We know $\infty$ -norm is the element with the largest absolute value, for $\infty$ -Wasserstein distance do we have such a simplification as well?","['measure-theory', 'probability-distributions', 'matrix-norms', 'metric-spaces']"
4793235,Well-ordering of natural numbers from axioms of real numbers,"Let $(\mathbb R,+,\times,<)$ be an ordered field that is Dedekind complete. Let $\mathbb N$ be the intersection of all additive submonoids $M \subseteq \mathbb R$ with $1 \in M$ . The question is, can we prove that every subset of $\mathbb N$ has a least element (i.e. well-ordering principle)? If not, can we ""construct"" a model of $\mathbb R$ such that $\mathbb N$ does not have the well-ordering principle (apologize for the language as I am not a model theorist)? This is inspired by the answer https://math.stackexchange.com/a/2214227/553409 , although I don't think Proposition 3 in the cited answer works. In particular, elements of $\mathbb N$ may get arbitrarily closed to each other.
[1]: https://math.stackexchange.com/a/2214227/553409","['elementary-set-theory', 'real-analysis']"
4793310,"Conditional expectation $E[X_0|X_0X_1,\ldots, X_0X_n]$.","Consider iid random variables $(X_j)_{j\in\mathbb{N}_0}$ uniformly distributed on $[0,1]$ .
For $j\in\mathbb{N}$ define $V_j:=X_0X_j$ and the recursively defined estimator $W_j:=\max (W_{j-1},V_j)$ with $W_0:=0$ . I want to compute 1. $E[X_0|\mathcal{F}_j^V]$ , the MMSE-estimator, 2. the mean square error $E[(X_0-W_j)^2]$ , 3. the convergence rate of $E[(X_0-W_j)^2]$ . My attempt: 1. With this part, I struggle most. I have done the following: From product distribution of two uniform distribution, what about 3 or more , it holds $f_{V_1}(x)=-\log(x)$ . Then for $[a,b]\subset(0,1)$ , I find $$
\begin{aligned}
E[X_01_{X_0\in[a,b]}]
=\int_a^bxdx
\end{aligned}
$$ and $$
\begin{aligned}
E[-\frac{V_1}{\log(V_1)}1_{V_1\in[a,b]}]=\int_a^b-\frac{x}{\log(x)}(-\log(x))dx=\int_a^bxdx
\end{aligned}
$$ I am not sure, if this is even useful or how to connect the integrals, such that $1_A$ shows up on both sides for $A\in\sigma(V_1,\ldots, V_j)$ . Does the MMSE-estimator even coincide with the conditional expectation? 2. With this part I am quite confident. By observation, I find $$W_j=X_0\max (X_1,\ldots, X_j),$$ and by independence of $g(X_0)=X_0^2$ from $f(X_1,\ldots,X_j)=(1-\max(X_1,\ldots, X_n))^2$ , I find $$E[(X_0-W_j)^2]=E[X_0^2(1-\max (X_1,\ldots, X_j))^2]=E[X_0^2]E[(1-\max (X_1,\ldots, X_j))^2].$$ Using Expected value of $\max\{X_1,\ldots,X_n\}$ where $X_i$ are iid uniform. , I end up with $$E[(X_0-W_j)^2]=C\frac{1}{(j+1)(j+2)}.$$ 3. the rate of convergence is therefore $j^2$ . It would be great, if someone can check over it. I can add further details, if necessary! Any hint or help is appreciated! Thank you in advance!","['conditional-expectation', 'probability-theory', 'probability']"
4793345,Calculate limit of $\frac{\sin(2x)}{x(\sin(x)-1)}$ as x goes to +infinity,"I tried like this: $$\frac{-1}{x(\sin(x)-1)}\leq\frac{\sin(2x)}{x(\sin(x)-1)}\leq\frac{1}{x(\sin(x)-1)}$$ for each $x$ in domain. And now, Wolfram-Alpha says that $\lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0$ , therefore $\lim_{x \to +\infty}\frac{\sin(2x)}{x(\sin(x)-1)}=0$ , but I do not know how to show that the following is true: $$\lim_{x \to +\infty}\frac{-1}{x(\sin(x)-1)}=\lim_{x \to +\infty}\frac{1}{x(\sin(x)-1)}=0.$$ I would be grateful for any hints.","['limits', 'calculus']"
4793359,"Differentiation through integrals, tower of expectations - Proof validation and clearing up doubts","I am interested in minimizing a loss function involving probability densities of kernel density estimation functions (KDEs). One of the terms I obtain is this one: $$\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx$$ Eventually, I would like to get an expectation that I can turn into an estimator of the gradient to train a neural network model $g$ parametrized by $\theta$ . So I try to differentiate with respect to $\theta$ : $$
\begin{align}
\nabla _{\theta }\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx
&= \int _{X} \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx \\
&= 2\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) dx\\
&=2\int _{X}\int _{Z} p( z) K( x-g_{\theta }( z)) dz\int _{Z} p( z) \nabla _{\theta } K( x-g_{\theta }( z)) dzdx\\
&=2\int _{X} E_{Z}[ K( x-g_{\theta }( Z))] E_{Z}[ \nabla _{\theta } K( x-g_{\theta }( Z))] dx\\
&=2E_{X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ E_{Z}[ \nabla _{\theta } K( X-g_{\theta }( Z))]]\\
&= 2E_{Z,X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ \nabla _{\theta } K( X-g_{\theta }( Z))]
\end{align}
$$ I am not completely certain I am correctly applying the chain rule between line $1$ and $2$ , that's the first issue. Then I am not sure if I am correct in going to the last line from the previous one. $E_{Z}[ K( x-g_{\theta }( Z))]$ should be a correct probability density function on $X$ so the previous line should be correct. However, I am not sure if I can collapse the two expectations into one like that. I don't know if it helps nor if it is correct but looking at the integral representation of the expectations, it seem to suggest that the two variables $Z$ and $E_{Z}[ K( x-g_{\theta }( Z))]$ should be independent in this context, although $Z$ participates in the computation of $E_{Z}[ K( x-g_{\theta }( Z))]$ . My intuition is that they should be independent because in this case $Z$ is ""defined"" by the inner expectation, it has no actual relation with the outer $Z$ . So if someone can clear these doubts and confirm that the proof seems correct, I would appreciate it. There is still one additional question I would like to ask: assuming the two variables are indeed independent, can I gain something by taking the same $Z$ for the computation of both variables, deliberately making them dependent?","['expected-value', 'solution-verification', 'derivatives', 'density-function']"
4793389,Friedrich mollifier on a manifold,"We say a compactly supported function $\varphi: \mathbb{R}^n \rightarrow \mathbb{R}$ is a mollifier if $\int_{\mathbb{R}^n} \varphi(x) dx = 1$ $\lim_{\epsilon \rightarrow 0} \varphi_\epsilon = \lim_{\epsilon \rightarrow 0}\epsilon^{-n}\varphi(x/\epsilon) = \delta(x)$ where $\delta$ is the delta function. Furthermore we may use $\varphi$ to smoothen any function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ using convolution: $$f_\epsilon= \int_{\mathbb{R}^n} \varphi_\epsilon(x-y) f(y) dy$$ How much of this carries over to a general smooth manifold $M$ ? I assume $M$ must be a Riemannian manifold so we have a canonical volume form to be able to integrate functions. The first criteria above can be satisfied on a Riemannian manifold, but how would one satisfy the second? There is no way to scale points on a manifold. Furthermore, if one wanted to use such a mollifier to smoothen a function $f: M \rightarrow \mathbb{R}$ , then how would the difference $x-y$ be defined when carrying out the convolution? Taylor briefly mentions this in his three volume series on PDEs but he does not go into detail.","['riemannian-geometry', 'convolution', 'smooth-manifolds', 'partial-differential-equations', 'differential-geometry']"
4793408,Why are inverse images much better behaved than forward images?,"If $f:X\to Y$ is a function, and $A\subseteq X$ , then we define the (forward) image of $A$ under $f$ as the set $f(A)=\{y\in B:(\exists x\in A)(y=f(x))\}$ ; similarly, if $B\subseteq Y$ , then the inverse image of $B$ under $f$ is defined as $f^{-1}(B)=\{x\in A:f(x)\in B\}$ . Despite the apparent duality between these two concepts, it is well known that inverse images are in general much better behaved than forward images. While unions are preserved under images (i.e. $f\left(\bigcup_i A_i\right)=\bigcup_if(A_i)$ ), the corresponding equation for intersections is false, and taking the forward image does not interact well with taking complements. By contrast, pretty much everything that you would hope that inverse images preserve is preserved. (This phenomenon is not limited to set theory. For instance, if $\phi :G\to H$ is a group homomorphism, and $N$ is a normal subgroup of $H$ , then $\phi^{-1}(N)$ is a normal subgroup of $G$ . Again, the corresponding statement for forward images is false.) Is there a deeper explanation, say using category theory, for why inverse images are in general much better behaved than forward images?","['elementary-set-theory', 'abstract-algebra', 'category-theory']"
4793417,sphere tangent to a level set of the defining function,"Suppose $D \subset \mathbb{R}^n$ is a smoothly bounded domain given by a smooth defining function $r$ .  That is $$
D = \{r < 0\}, \quad  \partial D = \{r = 0\}, \quad \text{ and } \nabla r|_{\partial D} \neq 0
$$ Given a boundary point $p \in \partial D$ , why is it that for $\epsilon > 0$ sufficiently small, the sphere of radius $\epsilon$ centered at $p$ , $$
S(p, \epsilon) = \{z: |z - p| = \epsilon\}
$$ is tangent to a level set of $r$ at exactly one point in $D$ ? I believe that one uses that in $D$ near $\partial D$ that $\nabla r$ does not vanish.  Thus, the level sets $r^{-1}(c)$ are smooth hypersurfaces for $c$ with $|c|$ small.","['analytic-geometry', 'geometry', 'differential-geometry']"
4793437,Are these fast convergent series for the lemniscate constant and $\Gamma(\frac{1}{3})$ already known?,"[ Updated on Nov.10, Dec.05.2023 and Feb.18.2024 ] Applying a similar approach followed in these MO questions, for Catalan's and Apery's Constants, i.e. Wilf-Zeilberger proofs starting from Dougall's sum, working together with nonlinear $_2F_1$ and $_3F_2$ hypergeometric transformations, I have found four convergent series for the Lemniscate Constant $s = 2\varpi$ and two for $s=\Gamma(\frac{1}{3})$ . As far as I know, they are currently the fastest series to compute such constants with arbitrary precision (they belong to a highly efficient computing class -P2B3 class- specially fitted to apply the binary splitting algorithm). More details can be found in these links (1) , (2) and (3) I am not sure if some of them has been already published, so the question is very simple.
Is any of these series known? NOTE As of Feb.18.2024 question remains open for Eqs.(1), (2), (3), (7) and (11) below. We use the following notation, where the constant $s$ is expressed as $$s=\sum_{n=1}^\infty\,\rho^n\cdot\frac{p(n)}{r(n)}\cdot\left[\begin{matrix}
 a & b & c & ... & z \\
 A & B & C & ... & Z \\ 
\end{matrix}\right]_n=\sum_{n=1}^\infty\frac{p(n)}{r(n)}\cdot\prod_{k=1}^n\frac{r(k)}{q(k)}$$ here $p(n),q(n),r(n)$ are polynomials non vanishing for $n\in\mathbb{N}$ , $q(n)$ and $r(n)$ have the same degree $d$ and the convergence ratio $|\rho|$ is the absolute value of the ratio of the leading terms of $r(n)$ and $q(n)$ . The ratio of products of Pochhammer's symbols (rising factorials) is written as $$\left[\begin{matrix}
 a & b & c & ... & z \\
 A & B & C & ... & Z \\ 
\end{matrix}\right]_n=\frac{(a)_n(b)_n(c)_n ... (z)_n}{(A)_n(B)_n(C)_n ... (Z)_n} $$ where the degree $d$ is the number of elements in a row (they are the same for both rows) and $$(w)_n = \frac{\Gamma(w+n)}{\Gamma(w)}=w(w+1)(w+2)...(w+n-1)$$ The computational speed is measured through the binary splitting cost $$ C_s = - \frac{4d}{\log|\rho|}.$$ This allows to (asymptotically) rank, classify and compare different hypergeometric-type algorithms by performance.  Series are: A. For the lemniscate constant $$2\varpi =\frac{\;\Gamma(\frac{1}{4})^2}{\sqrt{2\pi}}=5.2441151085842396209296791797822388273655...$$ a trascendental number, See Finch, Steven R. , Mathematical constants , Encyclopedia of Mathematics and Its Applications 94. Cambridge: Cambridge University Press (ISBN 0-521-81805-2/hbk). xx, 602 p. (2003). ZBL1054.00001 . (Section 6.1) I) The first one has a cost $C_s=-4\cdot8/\log\frac{1}{2^{24}}=1.9326.$ . It is $$\begin{equation*}2\varpi=\sum_{n=1}^\infty\left(\frac{1}{2^{24}}\right)^n\cdot\frac{P(n)}{R(n)}\cdot\left[\begin{matrix}
 \frac{1}{8}&\frac{3}{8}&\frac{5}{8}&\frac{7}{8}&\frac{3}{16}&\frac{7}{16}&\frac{11}{16}&\frac{15}{16}\\
\frac{1}{32}&\frac{5}{32}&\frac{9}{32}&\frac{13}{32}&\frac{17}{32}&\frac{21}{32}&\frac{25}{32}&\frac{29}{32}\\ 
\end{matrix}\right]_n\tag{1}\label{1}
\end{equation*}$$ where $$ P(n)  =  750599893155840\,n^7 - 2465512126676992\,n^6 + 3305810396971008\,n^5 - 2327426831319040\,n^4 + 919690491432960\,n^3 - 201024828994048\,n^2 
 + 22012514018112\,n - 897181286400$$ $$R(n)=(8n-1)(8n-3)(8n-5)(8n-7)(16n-1)(16n-5)(16n-9)(16n-13)$$ II) The second series gives the reciprocal of this constant with about $3.01$ decimal digits per term. It has a cost $C_s=1.5661$ : $$\begin{equation*}\frac{1}{2\varpi}=C_0\cdot\sum_{n=1}^\infty\left(-\frac{2^9}{3^9\cdot7^6}\right)^n\cdot\frac{n^2\,P(n)}{R(n)}\cdot\left[\begin{matrix}
 \frac{1}{36}&\frac{7}{36}&\frac{13}{36}&\frac{19}{36}&\frac{25}{36}&\frac{31}{36}\\
1&1&\frac{1}{3}&\frac{1}{3}&\frac{2}{3}&\frac{2}{3}\\ 
\end{matrix}\right]_n\tag{2}\label{2}
\end{equation*}$$ where $C_0=2\cdot3^{9/2}\cdot7^{5/6}$ and $$P(n) = 99446494228488\,n^5 - 296948949253092\,n^4 + 339735211540956\,n^3 - 185806427026662\,n^2 + 48479683290426n - 4840729282291$$ $$R(n)=-(36n-5)(36n-11)(36n-17)(36n-23)(36n-29)(36n-35)$$ III) The third one for this constant, performs pretty fast. It has a cost $C_s=1.5643$ : $$\begin{equation*}\frac{1}{2\varpi}=C_1\cdot\sum_{n=1}^\infty\left(\frac{2^9}{11^9}\right)^n\,\frac{n^2\,P(n)}{R(n)}\,\left[\begin{matrix}
 \frac{1}{36}&\frac{5}{36}&\frac{13}{36}&\frac{17}{36}&\frac{25}{36}&\frac{29}{36}\\
1&1&\frac{1}{3}&\frac{1}{3}&\frac{2}{3}&\frac{2}{3}\\ 
\end{matrix}\right]_n\tag{3}\label{3}
\end{equation*}$$ where $C_1 = 2\cdot3^{15/4}\cdot11^{7/4}$ and $$P(n) = 17680561647336\,n^5 - 52825631815620\,n^4 + 60473303319276\,n^3 - 33092086224942\,n^2 + 8638260598818\,n - 862864755643$$ $$R(n)=(36n-7)(36n-11)(36n-19)(36n-23)(36n-31)(36n-35)$$ IIIa) [Updated Nov.10.2023] The following fast series to compute the lemniscate constant $$\begin{equation*}2\varpi=5C_2 \cdot\sum_{n=1}^\infty\left(\frac{1}{7^4\cdot23^4}\right)^n\cdot\frac{n^2\,P(n)}{R(n)}\cdot\left[\begin{matrix}
 \frac{1}{16}&\frac{5}{16}&\frac{9}{16}&\frac{13}{16}\\
1&1&\frac{1}{2}&\frac{1}{2}\ 
\end{matrix}\right]_n\tag{4}\label{4}
\end{equation*}$$ where $C_2 = 2^{33/4}\cdot7^{7/4}\cdot23^{7/4}\cdot\pi,\,$ and $$P(n) = 6636032\,n^2 - 6636192\,n + 1659109$$ $$R(n)=(16n-3)(16n-7)(16n-11)(16n-15)$$ has a cost 0.7872 but it needs to compute $\pi$ which has a cost (Chudnovsky) 0.3670. It gives $C_s=1.1542$ . According to Jesús Guillera (pers. comm. Nov. 2023) some of these series can be bisected or trisected. Particularly Eq.(4) can be bisected  into a $\,_2F_1$ function as $$\begin{equation*}2\varpi=C_3\cdot\pi\cdot\,_2F_1\left(\frac{1}{8},\frac{5}{8};1;\frac{1}{7^2\cdot23^2}\right)\tag{5}\label{5}\end{equation*}$$ for some algebraic value $C_3$ . Eq.(5) is a fast monotone series that has a companion $\,_2F_1$ fast alternating series by applying Euler or Pfaff hypergeometric transformation. This gives $$\begin{equation*}2\varpi=C_4\cdot\pi\cdot\,_2F_1\left(\frac{1}{8},\frac{3}{8};1;-\frac{1}{2^6\cdot3^4\cdot5}\right)\tag{6}\label{6}\end{equation*}$$ being $C_4$ an algebraic value. Note that $7^2\cdot23^2=25921$ and $2^6\cdot3^4\cdot5=25920$ and Eqs. (5) and (6) are dual or twin linearly convergent fast series having practically the same performance. These linked $2\varpi\propto\pi\cdot\,_2F_1$ series are proven by purely hypergeometric methods. In fact, you can start from Eq.(2) here , a $\,_3F_2$ series, and use Clausen's formula to get the (squared) Eq.(6) above. From this, Eq.(5) is obtained applying Euler or Pfaff transformation. Note also that, if the convergence rate is fixed, Clausen's formula, whenever it can be applied, produces from a fast $\,_3F_2$ series a faster $\,_2F_1$ having a lower computational cost by a factor $\frac{1}{3}$ just by reducing the polynomial degree from $d=3$ to $d=2$ . By Googling ""25921 2F1"" Eq.(5) is found in Table 4 here where a different proof was obtained by A. Ebisu using special values of Appell's bivariate F1 function and Goursat hypergeometric transformation . Therefore the bisected series IIIa Eq.(4) is indeed already known which makes a partial answer to this post. IIIb) [New Nov.10.2023. Updated Dec.05.2023] Eqs.(5-6) allow to derive the fastest lemniscate constant $2\varpi$ Ramanujan-type series that avoid to compute $\pi$ . There are currently two fastest series to compute the lemniscate constant by binary splitting. The proofs of both series are obtained following Guillera's article ""A method for proving Ramanujan series for 1/π"" using Table 2 for level ℓ = 2, row d = 13 and also ""WZ proofs of Ramanujan-type series (via 2F1 evaluations)"". The first one was obtained by J. Guillera working from Eq.(6) above. It is found and proven in the Appendix of this last publication -Eq.(48)-. I have found the other one working from Eq.(5). This is $$\begin{equation*}\frac{1}{2\varpi}=C_5\cdot\sum_{n=0}^\infty\left(\frac{1}{7^2\cdot23^2}\right)^n\cdot(1728\,n+55)\cdot\left[\begin{matrix}
 \frac{1}{8}&\frac{5}{8}\\
1&1\\ 
\end{matrix}\right]_n\tag{7}\label{7}
\end{equation*}$$ with $C_5=5^{3/4}\cdot2^{-3/4}\cdot7^{-5/4}\cdot23^{-5/4}$ . This series has a cost $C_s=0.7872$ . The proof follows straightforward from Guillera's formula by means of the mentioned Table 2, level ℓ = 2, row d = 13, using Ebisu's formula -Eq.(5) above and Eq.(A'''.1) in Table 4 - and expanding the rhs after applying $z\frac{d}{dz}$ to both sides of Euler's hypergeometric transformation $\,_2F_1(a,b;c;z)=(1-z)^{-a}\,_2F_1(a,c-b;c;\frac{z}{z-1})$ with parameters $[a,b,c] = [\frac{1}{8},\frac{3}{8},1]$ , $z=-\frac{1}{25920}$ and $\frac{z}{z-1}=\frac{1}{7^2\cdot23^2}$ Lemniscate Eq.(7) above and Guillera's Eq.(48) have been implemented (Dec.02.2023) in y-cruncher's version 0.8.3 Build 9530 as the default algorithms to compute this constant. B. For the cube of $\Gamma(\frac{1}{3})$ constant IV) [Updated Feb.18.2024] Series is $$\begin{equation*}\Gamma\left(\frac{1}{3}\right)^3=C_6\cdot\sum_{n=1}^\infty\left(\frac{3^4}{11^4\cdot23^4}\right)^n\cdot\frac{n^2\,P(n)}{R(n)}\cdot\left[\begin{matrix}
 \frac{1}{24}&\frac{7}{24}&\frac{13}{24}&\frac{19}{24}\\
1&1&\frac{1}{2}&\frac{1}{2}\ 
\end{matrix}\right]_n\tag{8}\label{8}
\end{equation*}$$ where $C_6 = 2^{7}\cdot6^{1/2}\cdot11^{11/6}\cdot23^{11/6}\cdot\pi^2,\,$ and $$P(n) = 4097152\,n^2 - 4097536\,n + 1024535$$ $$R(n)=(24n-5)(24n-11)(24n-17)(24n-23)$$ This series has cost 0.9020 and $\pi$ (Chudnovsky) provides 0.3670 giving a total cost $C_s=1.2690$ . Following @CarP24's answer -also mentioned by Jesús Guillera (pers. comm. Nov. 2023)- Eq.(8) can be bisected into a $\,_2F_1$ function as $$\begin{equation*}\Gamma\left(\frac{1}{3}\right)^3=C_7\cdot\pi^2\cdot\,_2F_1\left(\frac{1}{12},\frac{7}{12};1;\frac{3^2}{11^2\cdot23^2}\right)\tag{9}\label{9}\end{equation*}$$ for some algebraic value $C_7$ . Eq.(9) is a fast monotone series that has a companion $\,_2F_1$ fast alternating series by applying Euler or Pfaff hypergeometric transformation. This gives $$\begin{equation*}\Gamma\left(\frac{1}{3}\right)^3=C_8\cdot\pi^2\cdot\,_2F_1\left(\frac{1}{12},\frac{5}{12};1;-\frac{3^2}{2^9\cdot5^3}\right)\tag{10}\label{10}\end{equation*}$$ being $C_8$ an algebraic value. Note that $11^2\cdot23^2=64009$ and $2^9\cdot5^3=64000$ and Eqs. (8) and (9) are dual or twin linearly convergent fast series having practically the same performance. These $\Gamma\left(\frac{1}{3}\right)^3\propto\pi^2\cdot\,_2F_1$ series are proven by purely hypergeometric methods. For instance, from Eq.(42) here , a $\,_3F_2$ series, use Clausen's formula to get the (squared) Eq.(10) above. From this, Eq.(9) is obtained applying Euler or Pfaff transformation. As @CarP24 answered, the bisected series IV Eq.(8) is already known here which makes a partial answer to this post. IVa) [New Feb.18.2024] Eqs.(9-10) allow to derive slightly faster $\Gamma\left(\frac{1}{3}\right)^3$ Ramanujan-type series that just need to compute $\pi$ instead of $\pi^2$ . There are currently two fastest series to compute this constant by binary splitting. The proofs of both series are obtained following Guillera's article ""A method for proving Ramanujan series for 1/π"" using Table 4 for level ℓ = 1, row d = 7 and also ""WZ proofs of Ramanujan-type series (via 2F1 evaluations)"". The first one was obtained by J. Guillera working from known Eq.(10) above. I have found the other one working from Eq.(9) now that @CarP24 has placed it in his answer below. This is $$\begin{equation*}\frac{\pi}{\Gamma\left(\frac{1}{3}\right)^3}=C_9\cdot\sum_{n=0}^\infty\left(\frac{3^2}{11^2\cdot23^2}\right)^n\cdot(4800\,n+147)\cdot\left[\begin{matrix}
 \frac{1}{12}&\frac{7}{12}\\
1&1\\ 
\end{matrix}\right]_n\tag{11}\label{11}
\end{equation*}$$ with $C_9=2^{-1/2}\cdot11^{-7/6}\cdot23^{-7/6}$ . The proof follows from Guillera by means of the mentioned Table 4, level ℓ = 1, row d = 7, using Hakimoglu-Brown formula -Eq.(9) above- and expanding the rhs after applying $z\frac{d}{dz}$ to both sides of Euler's hypergeometric transformation $\,_2F_1(a,b;c;z)=(1-z)^{-a}\,_2F_1(a,c-b;c;\frac{z}{z-1})$ with parameters $[a,b,c] = [\frac{1}{12},\frac{5}{12},1]$ , $z=-\frac{9}{64000}$ and $\frac{z}{z-1}=\frac{3^2}{11^2\cdot23^2}$ A non-native implementation of each series has been done for y-cruncher software (a high performance platform to compute a huge number of digits of some classical constants, having set several records in this regard). Custom input files were prepared for each formula and series were tested under this special platform against some known formulas and methods that has been used to break the $2\varpi$ and $\Gamma(\frac{1}{3})$ digits number records. The above formulas outperform providing the fastest binary splitting algorithms to compute these constants. As it is reported here Eq.(4) on Dec.12.2023 and Eq.(8) on Dec.15.2023 were used to beat up to $10^{12}$ the current record of known decimal digits of $\Gamma\left(\frac{1}{4}\right)$ and $\Gamma\left(\frac{1}{3}\right)$ respectively. Q: Excluding series IIIa Eq.(4) and IV Eq.(8), is any of these series known?","['algorithms', 'sequences-and-series']"
4793451,Functional series $\sum_{n=1}^{\infty}a_n \sin (b_n x)$ is differentiable.,"How to prove this conclusion? $$a_n,b_n\geq0,\sum_{n=1}^{\infty}a_n ＜\infty$$ And if functional series $$\sum_{n=1}^{\infty}a_n \sin (b_n x)$$ is differentiable for any $x\in \textbf{R}$ , then $$\sum_{n=1}^{\infty}a_nb_n＜\infty$$ And I have known that it may be solved by differentiating it term by term and get values on x=0 of the series $\sum_{n=1}^{\infty}a_nb_n\cos(b_nx)$ ,but I'm doubted how to prove that the order of differentiation and summation could be exchangeable,i.e. $$\sum_{n=1}^{\infty}a_nb_n=\sum_{n=1}^{\infty}a_nb_n\cos(b_nx)|_{x=0}=\frac{\mathrm{d} }{\mathrm{d} x}\sum_{n=1}^{\infty}a_n \sin (b_n x) |_{x=0}$$ Thanks for your reading and assistance.","['sequences-and-series', 'analysis', 'real-analysis']"
4793493,Decomposition for the first hitting time,"A homogeneous Markov chain $\{X_n\}_{n\in\mathbb N}$ with discrete state space $\mathcal{S}$ . Set $$\tau_{k}:=\text{inf} \left\{n\ge 0:\, X_n=k  \right\}.$$ where $\tau_{k}$ is defined to be $+\infty$ , when doesn't exist any $n$ such that $X_n=k$ . $\textbf{1}.$ Now consider the Ehrenfest model ,its state space $\mathcal{S}$ is a finite
set $\left\{0,1,2,...,N\right\}$ and one-step transition matrix is $$\left(\begin{array}{ccccccc}
0 & 1 & & & & & \\
\frac{1}{N} & 0 & \frac{N-1}{N} & & & & \\
& \frac{2}{N} & 0 & \ddots & & & \\
& & \frac{3}{N} & \ddots & \ddots & & \\
& & & \ddots & 0 & \frac{2}{N} & \\
& & & & \frac{N-1}{N} & 0 & \frac{1}{N} \\
& & & & & 1 & 0
\end{array}\right).$$ From an intuitive perspective, let $T_{ij}:=\inf\left\{n\ge 0:X_{n}=j,X_{0}=i\right\}$ , $i<j$ ,be the hitting time of reaching $j$ starting
from $i$ .Since $T_{i,N}=\sum_{k=i}^{N-1}T_{k,k+1},$ then $${\color{Red} {\text{ for any } i<N,\mathbb{E}(\tau_{N}\mid X_{0}=i)=\mathbb{E}(\tau_{i+1}\mid X_{0}=i)+\mathbb{E}(\tau_{i+2}\mid X_{0}=i+1)+\cdots+\mathbb{E}(\tau_{N}\mid X_0=N-1).（*）}}\quad$$ But how can it be proven rigorously? By linearity of expectation,we have $$\begin{align}
   \mathbb{E}\left[\tau_{N}\mid X_{0}=i\right]&=\mathbb{E}\left[\tau_{i+1}+\sum_{t=i}^{N-2}(\tau_{t+2}-\tau_{t+1})\mid X_{0}=i\right] \\
    &=\mathbb{E}\left(\tau_{i+1}\mid X_{0}=i\right)+\mathbb{E}\left(\tau_{i+2}-\tau_{i+1}\mid X_{0}=i\right)+\\&\cdots+\mathbb{E}\left(\tau_{N}-\tau_{N-1}\mid X_0=i\right).
\end{align}$$ How to prove that formally \begin{align}
    &\mathbb{E}\left(\tau_{i+2}-\tau_{i+1}\mid 
 X_{0}=i\right)=\mathbb{E}\left(\tau_{i+2}\mid 
 X_{0}=i+1\right);\\
     &\qquad\qquad\qquad\qquad\cdots\cdots \\
    &\mathbb{E}\left(\tau_{N}-\tau_{N-1}\mid X_0=i\right)=\mathbb{E}\left(\tau_{N}\mid X_0=N-1\right)
.\end{align} $\textbf{2}.$ Does this conclusion $(*)$ still hold in general homogeneous Markov chain $\{X_n\}_{n\in\mathbb N}$ with discrete state space $\mathcal{S}$ ? A modest attempt on $\textbf{2}$ ： Let the state space $\mathcal{S}$ is $\left\{0,1,2\right\}$ and one-step transition matrix is $$ \begin{pmatrix}
 1/3&  1/3&  1/3\\
 0&  1/2&  1/2\\
 0&  0&  1\\
\end{pmatrix}$$ I find that $$\mathbb{E}\left(\tau_{2}\mid X_{0}=0\right){\color{Red} \ne }\mathbb{E}\left(\tau_{1}\mid X_{0}=0\right)+\mathbb{E}\left(\tau_{2}\mid X_{0}=1\right).$$ Any help will be greatly appreciated！","['stochastic-processes', 'markov-chains', 'probability']"
4793535,Example of Kalman-filter,"I am trying to understand Example 2 in the original article of Kalman. I would like to use the notion of Theorem 2.5 in my lecture notes to determine the Kalman equations. Moreover, the example shows up as exercise in the same lecture notes. Let me repeat the problem: $x_1$ is supposed to be the position of some object, which is at time $t=0$ in the origin and moving with constant speed $x_2$ . This will be captured by $$
\begin{aligned}
x_1(t+1)&=x_1(t)+x_2(t)\\
x_2(t+1)&=x_2(t)
\end{aligned}
$$ and the initial conditions $E[x_1^2(0)]=E[x_2(0)]=0$ , $E[x_2(0)^2]=a^2>0$ . (Even though $E[x_2(0)]=0$ seems to be a bit confusing here? Possibly should read as $E[x_3(0)]=0$ ?) Then, $E[x_1^2(0)]=0$ implies $x_1(0)=0$ (start from origin), $x_2(t)=x_2(0)$ for any $t$ (constant velocity) and $$x_1(t+1)= x_1(t)+x_2(t)=x_1(t-1)+x_2(t-1)+x_2(0)=\ldots=(t+1)x_2(0),$$ which coincides with the kinetic formula for constant velocity $x(t)=v_0 t$ . Further, $$
\begin{aligned}
x_3(t+1)&=\phi x_3(t)+u_3(t)\\
y(t)&=x_1(t)+x_3(t),
\end{aligned}
$$ where $\phi$ is a constant and $E[u_3(t)]=0$ and $E[u_3^2(t)]=b^2>0$ . (Again, unfortunately no information about $E[x_3(t)]$ . Is this alright so far?) Here, one can also observe, that $x_3$ should somehow act as perturbation noise for the observation $y$ . However, I cannot really tell, why you would want to call the noise $x_3$ , since this would (from my feeling) suggest, that the noise is a signal itself? (Additionally, one can observe, that for $|\phi|<1$ noises decay over time, that for $|\phi|=1$ the noises all add up and that for $|\phi|>1$ the noises get even amplified over time?) Anyways, the example can be rewritten as $$
\begin{aligned}
x(t+1)&=\begin{pmatrix}
1 & 1 & 0\\
0 & 1 & 0 \\
0 & 0 & \phi
\end{pmatrix} x(t) + \pmatrix{0 \\ 0 \\ u_3(t)}\\
y(t)&=\begin{pmatrix}1 & 0 & 1 \end{pmatrix} x(t)
\end{aligned}
$$ On page 6 , Kalman states, that $u$ is a gaussian vector (assuming that $u=(u_1,u_2,u_3)^\top$ ), so I assume one could rewrite this (if that even makes sense) into $$
\begin{aligned}
x(t+1)&=\begin{pmatrix}
1 & 1 & 0\\
0 & 1 & 0 \\
0 & 0 & \phi
\end{pmatrix} x(t) + \pmatrix{0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 1}u(t)\\
y(t)&=\begin{pmatrix}1 & 0 & 1 \end{pmatrix} x(t)
\end{aligned}
$$ Now, my lecture notes, page 29 state, I should consider $$
\begin{aligned}
X(t)&=a_0(t)+a_1(t)X(t-1)+a_2(t)Y(t-1)+b_1(t)\varepsilon(t)+b_2(t)\xi(t)\\
Y(t)&=A_0(t)+A_1(t)X(t-1)+A_2(t)Y(t-1)+B_1(t)\varepsilon(t)+B_2(t)\xi(t)
\end{aligned}
$$ where typically $\varepsilon$ would be a noise of the signal $X$ and $\xi$ noise of the observation $Y$ , in comparison to the first pages . Then I would like to find with Theorem 2.5 the Kalman-Bucy equations $$
\begin{aligned}
P(t)&=a_1P(t-1)a_1^\top+b_1b_1^\top+b_2b_2^\top-(a_1P(t-1)A_1^\top+b_1B_1^\top+b_2B_2^\top)\cdot\\
&\quad(A_1P(t-1)A_1^\top+B_1B_1^\top+B_2B_2^\top)^{-1}(a_1P(t-1)A_1^\top+b_1B_1^\top+b_2B_2^\top)^\top\\
\hat{X}(t)&=a_0+a_1\hat{X}(t-1)+a_2Y(t-1)+(a_1P(t-1)A_1^\top+b_1B_1^\top+b_2B_2^\top)\cdot\\
&\quad(A_1P(t-1)A_1^\top+B_1B_1^\top+B_2B_2^\top)^{-1}(Y(t)-A_0-A_1\hat{X}(t-1)-A_2Y(t-1))
\end{aligned}
$$ with initial values $$
\begin{aligned}
P(0)&=Cov(X(0))-Cov(X(0),Y(0))Cov(Y(0))^{-1}Cov(X(0),Y(0))^\top\\
\hat{X}(0)&=E[X(0)]+Cov(X(0),Y(0))Cov(Y(0))^{-1}(Y(0)-E[Y(0)])
\end{aligned}
$$ Now the very first problem I have with both notions is, that Kalman states, that $x_3$ should be noise. This seems to be odd in comparison to the notion of my lecture notes, where, as far as I know, noise should be represented as $\varepsilon$ or $\xi$ . However, if I just try to do the computations with the formulas of the lecture notes, the results seem to not match up with the results of the example in the Kalman's article. So unfortunately, I am confused on both ends and I am a bit lost with my approach to understand this example. I will add more details what I have done so far later, but maybe someone has already translated this example in another notation. My attempt: $$E[X(0)]=(E[x_1(0)],E[x_2(0)],E[x_3(0)])^\top=(0,0,E[x_3(0)])^\top$$ $$Y(0)=x_1(0)+x_3(0)=x_3(0)$$ $$E[Y(0)]=E[x_3(0)]$$ $$Cov(X(0),Y(0))=Cov(X(0),x_3(0))=(0,Cov(x_2(0),x_3(0)), Var(x_3(0))^\top$$ $$Cov(Y(0))^{-1}=Var(x_3(0))^{-1}$$ $$Cov(X(0))=\begin{pmatrix}
0 & 0 & 0 \\
0 & a^2 & Cov(x_2(0),x_3(0)) \\
0 & Cov(x_2(0),x_3(0)) & Var(x_3(0))
\end{pmatrix}$$ This would result in $$\hat{X}(0)=\begin{pmatrix}
0 \\
\frac{Cov(x_2(0),x_3(0))}{Var(x_3(0))}(x_3(0)-E[x_3(0)])\\
x_3(0)
\end{pmatrix}$$ $$P_0=\begin{pmatrix}
0 & 0 & 0 \\
0 & a^2-\frac{Cov(x_2(0),x_3(0))^2}{Var(x_3(0))} & 0 \\
0 & 0 & 0 \\
\end{pmatrix}$$ I am not sure, if this is really what you want as a initial value. I do not know, how I should computer $Cov(x_2(0),x_3(0))$ and $Var(x_3(0))$ . Now, I want to rewrite $$
\begin{aligned}
x(t+1)&=\begin{pmatrix}
1 & 1 & 0\\
0 & 1 & 0 \\
0 & 0 & \phi
\end{pmatrix} x(t) + \pmatrix{0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 1}u(t)\\
y(t)&=\begin{pmatrix}1 & 0 & 1 \end{pmatrix} x(t)
\end{aligned}
$$ into the form $$
\begin{aligned}
X(t)&=a_0(t)+a_1(t)X(t-1)+a_2(t)Y(t-1)+b_1(t)\varepsilon(t)+b_2(t)\xi(t)\\
Y(t)&=A_0(t)+A_1(t)X(t-1)+A_2(t)Y(t-1)+B_1(t)\varepsilon(t)+B_2(t)\xi(t)
\end{aligned}
$$ Now, there comes a bit of guess work. I suppose $u$ is the Gaussian noise $\varepsilon$ , since it appears on the signal side. I find $$a_0=a_2=b_2=0$$ $$a_1=\begin{pmatrix}
1 & 1 & 0\\
0 & 1 & 0 \\
0 & 0 & \phi
\end{pmatrix},\quad b_1=\pmatrix{0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 1}$$ The equation for $y$ is not yet in the form it should be. One needs to consider $y(t)$ to be dependend on previous steps? $$
\begin{aligned}
y(t+1)&=x_1(t+1)+x_3(t+1)=x_1(t)+x_2(t)+\phi x_3(t) + u_3(t)\\
&=\pmatrix{1 & 1 & \phi}x(t) + \pmatrix{0 & 0 & 1}u(t) 
\end{aligned}
$$ yielding $$A_0=A_2=B_2=0$$ $$A_1= \pmatrix{1 & 1 & \phi},\quad B_1=\pmatrix{0 & 0 & 1}. $$ Defining $\alpha:=a^2-\frac{Cov(x_2(0),x_3(0))^2}{Var(x_3(0))}$ , I find $$P_1=\pmatrix{\alpha &\alpha & 0 \\ \alpha &\alpha & 0 \\ 0 & 0 & 1}-\frac{1}{1+\alpha}\pmatrix{\alpha^2 &\alpha^2 &\alpha \\ \alpha^2 &\alpha^2 &\alpha \\ \alpha &\alpha & 1 }=\pmatrix{\frac{\alpha}{1+\alpha} &\frac{\alpha}{1+\alpha} &-\frac{\alpha}{1+\alpha} \\ \frac{\alpha}{1+\alpha} &\frac{\alpha}{1+\alpha} &-\frac{\alpha}{1+\alpha} \\ -\frac{\alpha}{1+\alpha} &-\frac{\alpha}{1+\alpha} &\frac{\alpha}{1+\alpha} }$$ The only thing I could compare this to is $P^*(1)$ in the Kalman article , but it is a completely different matrix. So, I am proably not on the right track here, but do neither know, if I can adjust something to make it fit to $P^*(1)$ . Any hint or help is appreciated! Thank you in advance!","['stochastic-filtering', 'probability-theory', 'probability', 'kalman-filter']"
4793584,When is the central singular fiber $X_0$ of a proper map $X \to D$ a deformation retract of $X$?,"Let $X$ be a complex manifold, and $f:X \to D$ a proper map with connected fibers to the unit disc $D \subset \mathbb C$ . Assume that $f$ is submersive (algebro-geometric lingo: smooth) over $D^* = D \setminus \{0\}$ . Under which conditions (possibly after shrinking $D$ ) can I say that the central fiber $X_0 = f^{-1}(0)$ is a deformation retract of $X$ ? If $X_0$ is smooth, this follows easily by Ehresmann's theorem. Yesterday I thought I found a reference in Voisin's Hodge Theory and Complex Algebraic Geometry II , that this works if $f$ has only isolated critical points. But today I can't find that anymore. Is it true anyway? Is it true in more general settings? Motivation . This is one of the starting assumptions when reading about the formalism of vanishing cycles in Le formalisme des cycles évanescents , SGA 7-2 by Deligne.","['complex-geometry', 'algebraic-geometry', 'differential-topology', 'homology-cohomology', 'algebraic-topology']"
4793618,how to evaluate $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!} $,"I want to evaluate $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!} $ I was solving the integral: $$I_n:=\lim_{ n \to \infty} \int_0 ^n   \left( \frac{2nt}{t^2+n^2} \right)^n dt$$ and I did the following
let $t=n \tan(x)$ , $dt= n \sec^2{(x)} dx$ then $$I_n =\lim_{n \to \infty} n\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \sec^2{(x)}dx =\lim_{n \to \infty} n\left( \int_0 ^{\frac{\pi}{4}} \sin^n{(2x)}dx +\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \tan^2{(x)}dx \right) \geq \lim_{n \to \infty} \frac{n}{ 2} \int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx$$ There is famous formula (Wallis formula) for $\int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx$ which is $\frac{(n-1)!!}{n!!} $ if $n$ is odd and $ \frac{(n-1)!!}{n!!} \frac{ \pi}{2}$ if $n$ is even,I do believe the integral diverge to infinity i.e $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!}= \infty $ but I couldn't prove that.","['limits-without-lhopital', 'limits', 'calculus', 'factorial']"
4793631,Equations modelling a snowboarder/skier on a jump,"I am currently a highschool student elaborating an internal assessment for my mathematics AA HL course (a sort of research paper we must elaborate for the IB) and was looking into the relationship between mathematics and snowboarding. My current idea for research was investigating the feasibility of a quintuple cork given some initial conditions of the jump (angle, length, height, slope angle, etc.) and the rider (height, mass, etc.). However, I've encountered some difficulties in finding appropriate differential equations to model a snowboarder's motion during a jump. Most existing models are either too complex, relying on matrix methods and other mathematics that are beyond my current level of math, or too simplistic, relying on basic kinematic equations that wouldn't allow for a meaningful investigation. I would greatly appreciate your help in finding or developing differential equations that can accurately model a snowboarder's motion during a jump. I believe that with the right equations, I can utilize various numerical methods such as Euler and Runge-Kutta to analyze the feasibility of a quintuple cork on a specific jump. Whether you can suggest equations, recommend textbooks or research papers, or provide insights into the application of calculus and differential equations in this context, I would greatly appreciate your input. Let me make it clear I do not want my work done for me, since it would be against guidelines, rather some pointers to help me get started","['physics', 'calculus', 'ordinary-differential-equations']"
4793651,When is a proth number a perfect square?,"I am investigating when a Proth number is a perfect square. I saw a very short proof in this article (Lemma 3.1) , but I want it more number-theoretic. A Proth number $n$ is a number of the form $$
n = h\cdot 2^k + 1 \text{ with } k\in\mathbb N \text { and } h<2^k \text { odd.}
$$ So lets suppose we have $h\cdot 2^k + 1 = q^2$ for a $q\in\mathbb N$ . Then $$
h\cdot 2^k = q^2 - 1 = (q+1)\cdot (q-1).
$$ Since $h\cdot 2^k$ is even, so is $(q+1)(q-1)$ . That means that both $q+1$ and $q-1$ are even.
Since their difference is $2$ , it follows that one of these is divisible by $2$ exactly once.
And the other is divisible by $2$ exactly $2^{m-1}$ times. But how do I go on from here?","['number-theory', 'square-numbers']"
4793660,Proof that $\zeta (z)=\frac{\sigma '(z)}{\sigma (z)}$,"The Weierstrass $\sigma$ function of a lattice $\Omega$ is defined by $$\sigma (z)=z\prod_{\omega\in \Omega\setminus\{0\}}\left(1-\frac{z}{\omega}\right)\exp\left(\frac{z}{\omega}+\frac{1}{2}\frac{z^2}{\omega^2}\right).$$ The Weierstrass $\zeta$ function of a lattice $\Omega$ is defined by $$\zeta(z)=\frac{1}{z}+\sum_{\omega\in\Omega\setminus\{0\}}\left(\frac{1}{z-\omega}+\frac{1}{\omega}+\frac{z}{\omega^2}\right).$$ I want to prove that $$\zeta (z)=\frac{\sigma'(z)}{\sigma(z)}.$$ I've seen the following proof of the above theorem: $$\frac{d}{dz}\log\sigma (z)=\frac{d}{dz}\log z+\sum_{\omega\in\Omega\setminus\{0\}}\frac{d}{dz}\left(\log\left(1-\frac{z}{\omega}\right)+\frac{z}{\omega}+\frac{1}{2}\frac{z^2}{\omega^2}\right),$$ from which the identity follows. The complex logarithm is defined by $$\log z=\log |z|+i\operatorname{arg}z$$ where $\operatorname{arg}$ is the complex argument with the nonpositive real axis as a branch cut; $\operatorname{arg}z\in (-\pi,\pi]$ . Now I see $5$ problems with the above ""proof"": $\log$ is not differentiable everywhere because of the branch cut. $\log zw=\log z+\log w$ if $-\pi\lt \operatorname{arg}z+\operatorname{arg}w\le \pi$ ; the identity can fail outside this region. $\log zw=\log z+\log w$ does not imply $\log\prod=\sum\log$ where $\prod$ is an infinite product. $\log e^z=z$ for $z$ such that $-\pi\lt \operatorname{Im}z\le \pi$ ; the identity can fail outside this region. Interchanging the derivative and infinite series could be justified by uniform convergence, but how would one prove uniform convergence in this case? Can the above problems be patched? If so, how? Alternatively, can the theorem in question be proved without using logarithms? I thought that $$\mathrm{LHS}(z)=\mathrm{RHS}(z)+2\pi ik$$ and then differentiating could work, but then I realized that $k$ depends on $z$ and has jump discontinuities, so again we run into the problem of non-existent derivatives.","['complex-analysis', 'elliptic-functions', 'uniform-convergence', 'sequences-and-series']"
4793663,Bounding a Certain Derivative,"I'm self-studying the $2^{\mathrm{nd}}$ edition of Casella and Berger's Statistical Inference and I'm having trouble following a derivation they give (on pp. 72-73). Suppose we want to find a function $g(x,t)$ such that \begin{equation*}
\left[\frac{\partial}{\partial t}e^{tx}e^{-(x-\mu)^{2}/2}\right]_{t=t'} \leq g(x,t)
\end{equation*} for all $|t'-t|\leq \delta_{0}$ (where $\delta_{0}>0$ ) and such that \begin{equation*}
\int_{-\infty}^{\infty}{g(x,t)\,\mathrm{d}x} < \infty.
\end{equation*} It is easy to determine that \begin{equation*}
\left|\frac{\partial}{\partial t}e^{tx}e^{-(x-\mu)^{2}/2}\right| = \left|x\right|e^{tx}e^{-(x-\mu)^{2}/2}.
\end{equation*} They go on to define $g$ as \begin{equation*}
g(x,t) = \begin{cases}
\left|x\right|e^{(t-\delta_{0})x}e^{-(x-\mu)^{2}/2}\,&\mbox{ for }x<0\mbox{ and}\\
\left|x\right|e^{(t+\delta_{0})x}e^{-(x-\mu)^{2}/2}\,&\mbox{ for }x\geq 0.
\end{cases}
\end{equation*} Defining the function $g$ as they do, the integral over the negative real numbers involves $e^{-\delta_{0}x}$ in which $-\delta_{0}x>0$ , and the integral over the non-negative real numbers involves $e^{\delta_{0}x}$ in which $\delta_{0}x\geq 0$ , but either way it seems like $e^{-(x-\mu)^{2}/2}$ will ensure the integral is finite. It's not clear to me why it is necessary to define the function differently depending on the sign of $x$ . Is this done to ensure the inequality? Or is it done to ensure that $g(x,\cdot)$ has finite integral over $\mathbb{R}$ ? Edit : I don't think it has to do with integrability. For $x\geq 0$ , they show that one can write \begin{equation*}
g(x,t) = xe^{-\left[x-(\mu+t+\delta_{0})\right]^{2}/2}e^{-\left[\mu^{2}-(\mu+t+\delta_{0})^{2}\right]/2}
\end{equation*} so that integrating this function over $[0,\infty)$ amounts to calculating the (surely finite) mean of a normal distribution over half of its support. They do the same for $x<0$ : \begin{equation*}
g(x,t) = |x|e^{-\left[x-(\mu+t-\delta_{0})\right]^{2}/2}e^{-\left[\mu^{2}-(\mu+t-\delta_{0})^{2}\right]/2}.
\end{equation*} But the reasoning seems to hold equally well using $t-\delta_{0}$ or $t+\delta_{0}$ in either case, the difference amounting to a shift in the mean of the normal distribution.","['statistics', 'derivatives', 'real-analysis']"
4793688,Given 4 spheres is there a unique point where the distance to their surfaces is minimal?,"Problem Description: The radii $r$ of 4 spheres are $r_1,r_2,r_3,r_4>0$ . The center points of the spheres are not on a plane. The spheres can overlap. The distance to the surface of the sphere is positive in the case a sphere encloses the point. The cost-function is therefore always positive or 0 (in the case all spheres intersect at exactly one point). Intuition: There has to be a unique point where the sum of the distances from the point to the surfaces of the 4 spheres is minimal. Q1: Is there a proof that a unique minimum exists? Q2: Is the cost-function monotonically decreasing when approaching this point? Edit: In the comments it has been concluded, that the function can not be smooth. In the comments it has been concluded, that in case of equal radii the point is the Fermats point.","['optimization', 'spheres', 'geometry']"
4793708,"What is the origin of the word ""filtration"" in measure theory.","What is the origin of the word filtration in measure theory?
What is the mental image that motivated this wording? What inspired this wording outside of the mathematical world?","['measure-theory', 'math-history', 'probability-theory', 'filtrations']"
4793729,Is $\left\{\sqrt{n}\sin^{\circ n}\left(\dfrac{z}{\sqrt{n}}\right)\right\}$ locally uniformly convergent over $\mathbb{C}-\{y{\rm i}:|y|\ge\sqrt{3}\}$?,"Note $\sin^{\circ n}x$ as the $n$ -fold iteration of $\sin x$ . It is shown in this great answer that $$
\lim_{n\to\infty}\sqrt{n}\sin^{\circ n}\left(\dfrac{x}{\sqrt{n}}\right)=\dfrac{\sqrt{3}x}{\sqrt{x^2+3}}, \quad\forall x\in\mathbb{R}.
$$ I was wondering about the case where $x\in\mathbb{C}$ . Based on some numerical tests, it seems that this relation holds for all $x\in\mathbb{C}\setminus\{y{\rm i}:|y|\ge\sqrt{3}\}$ (where $\sqrt{z}$ is the principal square root defined over $\mathbb{C}-(-\infty,0]$ ). I would like to conjecture that The function sequence $\left\{\sqrt{n}\sin^{\circ n}\left(\dfrac{z}{\sqrt{n}}\right)\right\}$ is locally uniformly convergent over $\mathbb{C}\setminus\{y{\rm i}:|y|\ge\sqrt{3}\}$ . If this is true, then we know that the limit is holomophic, so it must be $\dfrac{\sqrt{3}z}{\sqrt{z^2+3}}$ by uniqueness of holomorphic functions. I was thinking that to imitate the proof in the answer to the original question, we need to find a uniform bound for $\left\{\sqrt{n}\sin^{\circ k}\left(\dfrac{z}{\sqrt{n}}\right):k\le n\right\}$ , but I have no idea of doing this for complex $z$ . So is this result of convergence correct? Any help appreciated.","['sequence-of-function', 'analysis', 'complex-analysis', 'sequences-and-series', 'limits']"
4793737,Does Galois group acts trivially on kernel of reduction?,"Let $L/K$ be a unramified quadratic extension of local fields.
Let $E/K$ be an elliptic curve over $K$ . We can consider reduction map $E(L)\to E(l)$ where $l$ is a residue field of $L$ . Suppose characteristic of $k$ is odd.
Let $E^1(L)$ be kernel of reduction map. I want to prove $Gal(L/K)$ acts trivially on $E^1(L)$ .
How can I prove this ? Suppose $\sigma \in Gal(L/K)$ acts on $P\in E^1(L)$ . $\tilde{\sigma(P)}=0$ , thus $\sigma({P})\in E^1(L)$ .Thus, $G$ acts on $E^1(L)$ , but I cannot derive any infomation on this action. Hints only is also appreciated.","['number-theory', 'algebraic-geometry', 'elliptic-curves', 'arithmetic-geometry']"
4793788,Have I discovered something or have I not been paying attention to circular or near circular geometry?,"It appears I have discovered something for myself: Take a polygon with an odd number of sides, e.g. a nonagon (9 sides for clarification.) Draw it so a horizontal side is facing down and a point is at the top. Then, draw a polygon with twice the number of sides so that the bottom side is in the same space as the bottom side of the smaller polygon. then draw a circle the size of the larger polygon (circumscribed) and its center should land right on the topmost point of the smaller polygon. When this is done with circles, where one is half the size of the other, and is tangent to the inner surface of the larger one, one end touches the circle, and the other passes through the radius. But polygons aren't exactly circular, and their heights aren't proportional in the same way circles are. Another thing is that the triangle and hexagon are pretty obvious, as one stacks to form an analog of the other, but a pentagon doesn't stack to make a decagon in the same way. If anyone else has figured this out, please explain, though I feel the explanation is rather simple. I am a stickler for knowing what is known and what is unknown, so not knowing the possibility of someone else knowing this bugs me. The phenomenon I've seen: edit Another thing I've noticed is that when I draw a line along any of the smaller polygon's sides, each line passes through at least 2 points of the larger, or at least comes very close to the latter.","['symmetry', 'geometry', 'polygons']"
4793832,Missing factor of 1/2 in a multivariable change of variables problem,"I am interested in evaluating the integral $$
\iint_{E} xy dA
$$ where $E$ is the region bounded by $xy = 3$ , $xy = 1$ , $y = 3x$ , and $y = x$ . This gives a region of integration that looks like this: Using the substitution $x = u/v$ and $y = v$ , we obtain a Jacobian of $$\frac{\partial(x,y)}{\partial(u,v)}  = 1 \cdot \frac{1}{v} - 0 \cdot \frac{-u}{v^2} = \frac{1}{v} > 0$$ which I think should turn our integral into: $$
\int_1^3\int_{1}^3 u \cdot \frac{1}{v} du dv = 4 \ln(3).
$$ However, the answer is supposed to be $2 \ln (3)$ , so I seem to be missing a factor of $1/2$ somewhere. I changed my bounds by noting that $v = y$ , and $y$ goes from $1$ to $3$ in the graph above.  Also, $x = u/v$ implies $u = xv = xy$ , and $u = xy$ goes from $1$ to $3$ as well. So where is my mistake exactly?","['jacobian', 'multivariable-calculus', 'change-of-variable']"
4793871,Why is an online poker article claiming that Straights happen more often than Three of a Kinds in a short deck?,"Six Plus Hold'em is a variant of Texas Hold'em in which a short deck is used. A short deck is a deck in which all cards from 2 to 5 are removed from a standard 52-card deck, leaving only 36 cards. Due to the change in deck composition the frequency in which hands appear are also different, which is why hand rankings are different from standard poker. An official World Series of Poker event held a few months ago swapped Full House and Flush from their usual hierarchy, leaving the others untouched. However a Wikipedia article about the variant claims the follwing about hand rankings: Flush ranks higher than full house.
In theory, three-of-a-kind ranks higher than a straight as the probability of achieving three-of-a-kind is lower than a straight in short-deck, however recent games have been ranking straight higher than three-of-a-kind which has become standard. I was unconvinced that tournaments were arbitrarily defying mathematics so I decided to do the numbers myself. Standard Deck A. Flush - Pick $5$ out of the $13$ ranks: $\binom{13}{5}=1287$ .

 - Pick a suit: $4$ .

 - Total: $1287\times 4=5148$ .

 - This figure includes Straight Flushes. B. Straight - Pick $1$ of $10$ rank combinations that constitute a Straight: A-5, 2-6, 3-7, ..., 10-A.

 - Assign a suit to each card: $4^{5}=1024$ .

 - Total: $10\times 1024=10240$ .

 - This figure includes $10\times 4=40$ Straight Flushes. C. Three of a Kind 1) Only a Three of a Kind and nothing higher

     - Pick one rank to be the Three of a Kind: $13$ .

     - Pick two other suits: $\binom{12}{2}=66$ .

     - Pick three out of four suits for the Three of a Kind: $\binom{4}{3}=4$ .

     - Assign a suit to the other two cards: $4^{2}=16$ .

     - Total: $13\times 66\times 4\times 16=54912$ .

 2) Full House

     - Pick the rank to be the Three of a Kind: $13$ .

     - Pick another rank to be the Pair: $12$ .

     - Pick three out of four suits for the Three of a Kind: $4$ .

     - Pick two out of four suits for the Pair: $\binom{4}{2}=6$ .

     - Total: $13\times 12\times 4\times 6=3744$ .

 3) Four of a Kind

     - Pick the rank to be the Four of a Kind: $13$ .

     - Pick another rank to be the lone card: $12$ .

     - Pick a suit for the lone card: $4$ .

     -Total: $13\times 12\times 4=624$ . Short Deck A. Flush - Pick $5$ out of $9$ ranks: $\binom{9}{5}=126$ .

 - Pick a suit: $4$ .

 - Total: $126\times 4=504$ .

 - Includes Straight Flushes. B. Straight - Pick $1$ of $6$ combinations: A-9, 6-10, 7-J, ..., 10-A.

 - Assign a suit to each card: $4^{5}=1024$ .

 - Total: $6\times 1024=6144$ .

 - This figure includes $6\times 4=24$ Straight Flushes. C. Three of a Kind 1) Only a Three of a Kind and nothing higher

     - Pick one rank to be the Three of a Kind: $9$ .

     - Pick two other suits: $\binom{8}{2}=28$ .

     - Pick three out of four suits for the Three of a Kind: $\binom{4}{3}=4$ .

     - Assign a suit to the other two cards: $4^{2}=16$ .

     - Total: $9\times 28\times 4\times 16=16128$ .

 2) Full House

     - Pick the rank to be the Three of a Kind: $9$ .

     - Pick another rank to be the Pair: $8$ .

     - Pick three out of four suits for the Three of a Kind: $4$ .

     - Pick two out of four suits for the Pair: $\binom{4}{2}=6$ .

     - Total: $9\times 8\times 4\times 6=1728$ .

 3) Four of a Kind

     - Pick the rank to be the Four of a Kind: $9$ .

     - Pick another rank to be the lone card: $8$ .

     - Pick a suit for the lone card: $4$ .

     - Total: $9\times 8\times 4=288$ . To summarize: Full House vs Flush(Straight Flush) Standard Deck 3744 < 5148(40) Short Deck 1728 > 504(24) Straight(Straight Flush) vs Three of a Kind(Full House/Quads) Standard Deck 10240(40) < 59280(3744/624) Short Deck 6144(24) < 18144(1728/288) As you can see the frequency of Full Houses compared to Flushes flips but Straights compared to Three of a Kinds does not, regardless of whether hands that also belong to a higher rank are deducted or not. Thus the hand ranking that WSOP used is mathematically correct. I read the reference of the Wikipedia article which claimed as follows: In Six Plus straights appear in abundance, so much so, that they rank lower than three-of-a-kind (but still higher than two pair). The article does not dive into the numbers so I have no idea about their reasoning. What method of measuring frequency could have led to this claim? Are there any errors in my logic or calculations?","['poker', 'statistics', 'card-games', 'probability']"
4793922,Integral for area under a sphere bound by a square,"I need to calculate the volume under a sphere of radius R centered on the origin bounded by a square, also centered on the origin in the X-Y plane, with sides of length A. Sphere of radius R bounded by square of size A I first tried to set it up in cylindrical and integrated $\frac{1}{8}$ of the volume by bounding as follows: $$\theta \in \left[0,\frac{\pi}{4}\right],\,r\in\left[0\,,\frac{A}{2 \cos\theta}\right]$$ then integrating as follows: $$8\int_{\theta=0}^{\frac{\pi}{4}}\int_{r = 0}^{\frac{A}{2 \cos\theta}} \sqrt{R^2-r^2} 
 r dr d\theta =$$ $$-\frac{8}{3}\int_{\theta=0}^{\frac{\pi}{4}}\left(R^2-r^2\right)^\frac{3}{2}\rvert_0^\frac{A}{2 \cos\theta}  d\theta =$$ $$-\frac{8}{3}\int_{\theta=0}^{\frac{\pi}{4}} \left(R^2-\left(\frac{A \sec\theta}{2}\right)^2\right)^\frac{3}{2} d\theta + \frac{2 R^3\pi}{3}$$ where I get stuck on how to integrate the first term. So, I tried it in spherical coordinates with bounds as follows: $$\theta \in \left[0,\frac{\pi}{4}\right],\,\rho\in[0,R],\,\phi\in\left[0\,,\arcsin\left(\frac{A}{2 R \cos\theta}\right)\right]$$ then integrating as follows: $$8\int_{r=0}^R\int_{\theta = 0}^\frac{\pi}{4} \int_{\phi=0}^{\arcsin\left(\frac{A}{2 R \cos\theta}\right)} r^2\sin\theta d\phi d\theta dr$$ $$8\int_{r=0}^R r^2\int_{\theta = 0}^\frac{\pi}{4} \sin\theta\phi\rvert_{\phi=0}^{\arcsin\left(\frac{A}{2 R \cos\theta}\right)} d\theta dr$$ $$8\int_{r=0}^R r^2\int_{\theta = 0}^\frac{\pi}{4} \sin\theta\arcsin\left(\frac{A}{2 R \cos\theta}\right) d\theta dr$$ where I get stuck again. I suspect I'm setting this integral up incorrectly. Can anyone make a suggestion? [EDIT] Cartesian is where I started (and I'll admit my integration is rusty), but the only move I could see was a substitution where $g=R^2-y^2$ and then a trig substitution $x=\sin{\theta}\sqrt{g}$ . So , the integral becomes: $$4\int_{y=0}^{\frac{A}{2}}\int_{x = 0}^{\frac{A}{2}} \sqrt{R^2-x^2-y^2} dx dy =$$ $$4\int_{y=0}^{\frac{A}{2}}\int_{\theta = 0}^{\arcsin\frac{A}{2\sqrt{g}}} g\cos^2\theta d\theta dy =$$ $$4\int_{y=0}^{\frac{A}{2}}\frac{g}{2}\int_{\theta = 0}^{\arcsin\frac{A}{2\sqrt{g}}}\left(1+\cos2\theta\right) d\theta dy =$$ $$4\int_{y=0}^{\frac{A}{2}}\frac{g}{2}\left(\theta+\frac{\sin2\theta}{2}\right)\rvert_{\theta = 0}^{\arcsin\frac{A}{2\sqrt{g}}} dy =$$ $$4\int_{y=0}^{\frac{A}{2}}\frac{g}{2}\left(\arcsin\left(\frac{A}{2\sqrt{g}}\right)+\left(\frac{A}{2\sqrt{g}}\right)\cos\left(\arcsin\left(\frac{A}{2\sqrt{g}}\right)\right)\right) dy$$ and, while I could undo the substitution and simplify slightly, I'm stuck again. I also tried it by parts with $u=\sqrt{R^2-x^2-y^2}, dv=dx$ , but the $\int{udv}$ eventually leads me back where my trig substitution did.","['integration', 'multivariable-calculus']"
4793991,Do there exist continuous maps from the Sphere to the Plane which preserve the Hausdorff Measure of all sets with some Dimension between 1 and 2?,"It's well known that there do exist area preserving maps between the sphere and the plane. It's well known there do NOT exist distance preserving maps between the sphere and the plane. So naturally one can ask if Hausdorff $2$ -measure can be preserved and Hausdorff $1$ -measure isn't preserved then is there some $1 < \theta < 2$ such that Hausdorff $\theta$ -measure can be preserved. My expectation is that either measure preserving maps exist for all $1 < \theta \le 2$ or no measure preserving maps exist for $1 \le \theta < 2$ . Any other behavior would be certainly surprising and or interesting. Some Ideas of Mine: A related question might be to ask for maps from the Sphere to itself which preserve a particular Hausdorff Measure. This question might be a bit more tractable. Suppose you fix the sphere along an axis $A$ . And divide the surface of the sphere up into slices each of which can be independent rotated. So the map of the sphere can be viewed as a map along the axis and at each point of the axis an angle is indicated for how much that particular slice rotates. We can characterize these maps from the sphere to itself by looking at functions from $[-r, r] \rightarrow [\pi, -\pi]$ As long as ALL the slices are rotated the same angle, you get an isometry which preserves $1$ -measure. Which we show below as a green function from axis-space to angle space Now the minute this green curve is no longer constant, the implied map stops preserving length. We know that for any smooth function we put on here the implied map will preserve area. I want to believe that (and I don't have a proof of this) for any continuous function with hausdorff measure $\le 2$ the implied map between spheres will preserve area. It would be beautiful but perhaps unsurprising if we stretched this idea in 2 dimensions to say that any continuous function with hausdorff measure $1 < \theta < \tau$ from axis to angle space preserves the Hausdorff $\tau$ measure on the sphere. But these ideas are far from rigorous and might even be completely wrong.","['measure-theory', 'hausdorff-measure', 'geometric-measure-theory', 'general-topology', 'differential-geometry']"
4794035,Wiping a plane clean with a rectangle (chalkboard erasing alogrithm?),"Let's say I use some green chalk to draw and fill a continous shape on a chalkboard. Let's assume the shape has no holes in it. I then use some red chalk to cover the area around my blob for a sufficient distance around my blob. Let's say I then have a chalkboard eraser of height h. Its width is negligible. To ""erase"" is defined as moving the eraser over the chalkboard in any direction (I understand some ambiguity is introduced by purely vertical movement if I'm assuming the width is negligible. In the end, for the larger problem, I'd have to consider non-negligible widths, so I'm allowing purely vertical movement now). ""Erasing"" any point removes the chalk from that point. If I go over a point with the eraser, it's ""eraser count"" is 1. For any vertical cross-section of my blob, the height of the cross-section could range from a single point up to several times h. I have two rules:
My eraser must always be vertical, but I can move it in the x and y dimensions freely.
Once I put the eraser against the board, I can't pick it up again until I am done (""done"" based on maximizing and minimizing rules below). The motion should be continuous in the x and y directions. I would like to maximize: The amount of green chalk I erase. I would like to minimize: The amount of red chalk I erase. The standard deviation of the ""eraser count"" of the set of points that have an eraser count >0, and were green originally. (Because of the rule about not being able to pick the eraser up, you would end up having to go over some areas more than once for backtracking, or to grab some horizontal strip that's <h high. I would like to equalize the number of times any area is 'erased' compared to any other area.) For areas of red chalk, I would like to minimize the eraser count somehow. Not sure how to define this any further than that (lowest maximum, lowest average or median, lowest std dev like above?). I would say the priority of these statemens is the priority that they are listed. I recognize the difficulties or impossibility of attempting to optimize too many things. My questions are: Does this type of problem have a name, or seem analagous to any other problem type? The only thing I could think of (as a layman) was tiling problems. Does an algorithm for something like this already exist? Searches for ""chalkboard eraser"" or ""paint roller"" algorithms didn't get me anywhere. What are some keywords I can look up, or areas of further study? Basically, like, what is this? What am I trying to talk about here? Am I making this too complicated? Is there an approach that is obvious that can be supported as optimal or near optimal? (I recognize there are a number of things going on here and I thank you for your patience. I am not a mathematician, and have only undergraduate education in math (up through differential equations) This is my attempt at a generalized explanation of a real world problem I am working on, and the constraints of that have influenced the rules above. I hope I have explained it clearly enough. Anyone who could say ""well, if you get rid of all of your constraints except #X and the rule about A, then it sounds like such and such,"" please do so. Much appreciated.)","['general-topology', 'area', 'tiling', 'algorithms']"
4794064,Configuration of $n$ points inside rectangle,"Given a finite set $S$ of at least $2$ points in the Euclidean plane and $x\in S$ write $$
d_S(x)=\min_{y\in S,\, y\neq x}d(x,y)
$$ and $$
d(S)=\sum_{x\in S}d_S(x)
$$ Now given a rectangle (product of closed bounded intervals) $R$ in the plane and $n\geq2$ : Which $S\subset R$ with $|S|=n$ maximizes $d(S)$ ?","['geometry', 'packing-problem']"
4794081,Is $7 + 5 \sqrt{2}$ a sum of squares in $\mathbb{Q}(\sqrt2)$?,"Is $7 + 5 \sqrt{2}$ a sum of squares in $\mathbb{Q}(\sqrt{2})$ ? I believe it is probably not. Suppose $7 + 5 \sqrt{2} = (a_1 + b_1 \sqrt{2})^2  + \ldots  + (a_n + b_n \sqrt{2})^2 $ . By factoring out the common denominator $d$ we can assume that all $a_i, b_i$ are integers and replace $7 + 5 \sqrt{2} $ with $7d + 5d \sqrt{2} $ . Then I notice that $a_1^2 + \ldots + a_n^2 + 2(b_1^2 + \ldots b_n^2)  $ is divisible by $7$ and $2a_1b_1 + \ldots + 2a_nb_n$ is divisible by $5$ . I don't see any contradictions yet, though.","['contest-math', 'field-theory', 'galois-theory', 'abstract-algebra', 'algebra-precalculus']"
4794082,"Find all real solutions for $\sqrt{1+\sin(2x)}-\sqrt{2}\cos(3x)=0$ within a domain of $[\pi, \frac{3\pi}{2}]$","Find all real possible solutions for $$\sqrt{1+\sin(2x)}-\sqrt{2}\cos(3x)=0$$ within a domain of $[\pi, \frac{3\pi}{2}]$ . This question is within a precalculus book, that is supposed to use algebra to solve the equations, for this specific problem my teachers have had problems for they need numerical methods to solve it. If you can solve the equation using algebra. I'd like to know how did you manage to do it, thank you to whoever can solve it.","['algebra-precalculus', 'trigonometry']"
4794085,Does this serie converge ? $\sum_{n=1}^{\infty} \frac{1}{n^3 - 5n}$,"Does this serie converge ? $$\sum_{n=1}^{\infty} \frac{1}{n^3 - 5n}$$ We have $$n^3 - 5n < n^3 \implies \frac{1}{n^3-5n} > \frac{1}{n^3}$$ We know $\frac{1}{n^3}$ converges because it's a p serie with $p > 1$ , but how does that help for determining the convergence of $\sum_{n=1}^{\infty} \frac{1}{n^3 - 5n}$ ? What should I do next ?","['summation', 'analysis', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4794188,"Prove that $f(x) =1/x^2$ is continuous on its domain $(0, \infty)$ [duplicate]","This question already has an answer here : Using delta epsilon to prove the continuity of a function (1 answer) Closed 8 months ago . Prove that the function $\smash{\dfrac{1}{x^2}}$ is continuous on its domain $(0,∞)$ . Proof: $$
\lim_{x \to a} \frac{1}{x^2} = \frac{1}{a^2} 
$$ After manipulating I would get $$ 
\frac{(x-a)(x+a)}{x^2 a^2} 
$$ where $x+a < 1 + 2\lvert a \rvert$ What do I do to the $\smash{\dfrac{1}{x^2 a^2}}$ ?","['epsilon-delta', 'real-analysis', 'continuity', 'calculus', 'limits']"
4794224,Determining the minor radius of an ellipse from its major axis and a tangent line,"I'm playing around with modeling an archway of a building using a solid modeler, and have come to a problem that exceeds my mathematical ability. Given an ellipse that has a known major radius, and is tangential to a line of a known length, angle, and offset on the Y-axis, can I find the ellipse's minor axis? It's been decades since I took a math class. I can handle basic trig, but I can't figure out how to get started on this one. Edit: Thanks everyone! You can see your solution in the flying buttresses of this (very preliminary) render.","['algebra-precalculus', 'conic-sections', 'trigonometry']"
4794227,"Proof of the conjugate commutator identity $ [g, h]^s = [g^s, h^s]. $","One of the five well-known commutator identities , also found on wikipedia , and apparently true for any elements $s, g, h$ of a group G, is as follows: $$ [g, h]^s = [g^s, h^s]. $$ I have failed to prove it and wonder where I have gone wrong in my proof. I also wonder where I should be looking to find proofs of this kind of elementary result, neither proofwiki nor Dummit&Foote, my usual places, seem to cover this. Attempted proof: (EDIT: in the comments it was pointed out where this goes wrong, this identity is indeed easy to prove! Though I am still curious where to find proofs of simple identities like this) $$ \begin{equation}\begin{aligned}
   {[}g, h]^s & = (g^{-1}h^{-1}gh)^s \\
            & = s^{-1}g^{-1}h^{-1}ghs
\end{aligned}\end{equation}
$$ $$ \begin{equation}\begin{aligned}
   {[}g^s, h^s] & = (g^s)^{-1}(h^s)^{-1}(g^s)(h^s) \\
                & = (s^{-1}gs)^{-1}(s^{-1}hs)^{-1}(s^{-1}gs)(s^{-1}hs) \\
                & = sg^{-1}s^{-1}sh^{-1}s^{-1}s^{-1}gss^{-1}hs \\
                & = sg^{-1}h^{-1} s^{-1}s^{-1} ghs
 \end{aligned}\end{equation}
$$","['group-theory', 'abstract-algebra']"
4794245,Probability of drawing from box B,"I have the exercise: Consider the following scenario. There are two boxes, Box A and Box B. Initially, Box A contains 3 red balls and 3 white balls; Box B contains 6 red balls. We swap the balls in the boxes via the following procedure:  We draw a ball, denoted as ball $x_A$ , uniformly at random from Box A, and draw a ball, denoted as ball $x_B$ , uniformly at random from Box B. We then place ball $x_A$ into Box B and place ball $x_B$ into Box A. After that, we select either Box A or Box B by some uniformly random procedure (like flipping a fair coin), and draw a ball from the selected box. The ball drawn was white. What was the probability that Box B was selected? I use Bayes theorem and Arrive at 1/6 But apparently the correct answer is 11/18, how is that?","['bayes-theorem', 'bayesian', 'probability']"
4794298,Generating Function $\sum_{k=1}^{\infty}\binom{2k}{k}^{-2}x^{k}$,"Closed Form For : $$S=\sum_{k=1}^{\infty}\binom{2k}{k}^{-2}x^{k}$$ Using the Series Expansion for $\arcsin^2(x)$ one can arrive at : $$\sum_{k=0}^{\infty}\binom{2k}{k}^{-1}x^{k}=\frac{4}{4-x}-4\arcsin\left(\frac{\sqrt{x}}{2}\right)\frac{1}{\sqrt{x\left(4-x\right)}}+16\arcsin\left(\frac{\sqrt{x}}{2}\right)\frac{1}{\left(4-x\right)\sqrt{x\left(4-x\right)}}\tag{1}$$ We need to add one more Binomial Term in the Denominator and I only know of two such Integrals (though there might be others?), The Beta Integral and the Odd Powers of $\sin$ . Here I will try to use the former. $$\frac{r}{2}\int_{0}^{1}t^{r-1}\left(1-t\right)^{r-1}dt=\binom{2r}{r}^{-1}$$ Then replacing $x\to t(1-t)x$ and Integrating from $0$ to $1$ leads to (after some more algebraic manipulations) : $$S=\frac{x}{16-x}+16\frac{x\arctan\left(\sqrt{\frac{x}{16-x}}\right)}{\left(16-x\right)\sqrt{x\left(16-x\right)}}+2x\frac{d}{dx}\sqrt{x}\int_{0}^{1}\left(\frac{\arcsin\left(\frac{\sqrt{\left(t\right)\left(1-t\right)x}}{2}\right)}{\left(4-\left(t\right)\left(1-t\right)x\right)\sqrt{\left(t\right)\left(1-t\right)\left(4-\left(t\right)\left(1-t\right)x\right)}}\right)dt$$ Though I haven't had any luck in Calculating the Integral present above. The variation of the Integral with only $t$ instead of $t(1-t)$ can be done : $$\int_{0}^{1}\left(\frac{\arcsin\left(\frac{\sqrt{tx}}{2}\right)}{\left(4-tx\right)\sqrt{t\left(4-tx\right)}}\right)dt=\frac{\ln\left(1-\frac{x}{4}\right)}{4\sqrt{x}}+\frac{\arcsin\left(\frac{\sqrt{x}}{2}\right)}{2\sqrt{4-x}}$$ Wolfram Alpha says ""no results found in terms of standard mathematical functions"". So is it not possible to solve this integral? Although I believe this question might have already been asked before as its the Ordinary Generating Function for Inverse Of Central Binomial Coefficient Squared but was not able to find it.","['integration', 'summation', 'definite-integrals', 'calculus', 'binomial-coefficients']"
4794304,Does this sum converge? Does it converge absolutely? $\sum_{n=1}^{\infty} \frac{(-1)^n}{n-\sqrt{n-1}}$,"Does this sum converge? Does it converge absolutely? $$\sum_{n=1}^{\infty} \frac{(-1)^n}{n-\sqrt{n-1}}$$ I first checked absolute convergence. Taking the absolute value of the term, we get $\frac{1}{n-\sqrt{n-1}}$ We know $n-\sqrt{n-1} < n \implies \frac{1}{n - \sqrt{n-1}} > \frac{1}{n}$ We know the harmonic series diverges, so our series will also diverge. So it does not converge absolutely. But for the convergence, I’m stuck. Computing $\lim_{n \to \infty} {a_{n+1}\over a_n}$ gives $1$ , so the the series can either converge or diverge, so it doesn’t seem to help. What else should I do?","['summation', 'analysis', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4794333,Problem in an inductive proof of Brouwer's fixed point theorem.,"I've been reading this article that proposes an inductive proof of Brouwer's fixed point theorem just using ""basic"" topology, avoiding the usage of homotopy groups as the usual proof goes. To do so we assume that given $n \in \mathbb{N}$ any continuous function from $C = [0,1]^n$ to $C$ has a fixed point and we prove that any continuous function $f$ from $C\times[0,1]$ to $C \times [0,1]$ has a fixed point (the base case for $n = 1$ is already proven by the intermediate value theorem). We write $f = (f_1,\dots,f_{n+1})$ where each $f_i:C\times [0,1] \rightarrow [0,1]$ is a continous function. Next it defines $\phi = (f_1,\dots,f_n)$ which is clearly a continuous function and we write $\phi_t: C \rightarrow C$ as the function $\phi_t(x) = \phi(x,t)$ , which clearly is continuous. Next by induction hypothesis we note that each $\phi_t$ has a fixed point, with this in mind we define $$X = \{(x,t) \in C \times [0,1]: \phi_t(x) = x\} = (\phi-id)^{-1}(\{0\})$$ And because this set is the preimage of a closed set under a continuous function, it's closed and nonempty because by induction each $\phi_t$ has at least one fixed point. However here's where my problem with the proof comes because next it takes $\pi_2: C\times [0,1] \rightarrow [0,1]$ as the projection onto the last coordinate. We note that $\pi_2(X) = [0,1]$ as there's at least one fixed point for each $\phi_t$ , but the problem that because clearly $\pi_2$ is an open map the proof on the article asumes that $\pi_2|_X$ is also an open map. However, restrictions of open maps are not always open, so is the article wrong or what am I missing? I've tried to prove that this map is in fact open, but I can't seem to do so, and in fact it looks that it might not be open which would prove that the article is wrong, so how do you prove that in fact this restriction is open? Or what would be a counterexample of a function $f$ such that the set we defined as $X$ is such that the restriction of $\pi_2$ isn't open?","['fixed-point-theorems', 'real-analysis', 'solution-verification', 'general-topology', 'algebraic-topology']"
4794368,Why is a finite field considered an étale Eilenberg-Maclane Space?,"The last few days I read up on étale morphisms of rings / affine schemes and the étale fundamental group. I have two closely connected questions. For $K$ a field a ring-homomorphism $K \rightarrow A$ is étale if and only if $A\cong L_1 \times ... \times L_r$ as $K$ -algebras, where all $L_i \mid K$ are finite separable field extensions. It is often stated that the étale fundamental group of a field is it's absolute Galois-group $\operatorname{Gal}(K^s,K)$ , where $K^s$ is the separable closure. I don't quite see how we can go from a product of finite separable extensions to single finite separable extensions, ie. why $$\pi_1^\text{et}(K) = \lim \operatorname{Aut}_K(L_1 \times \dots \times L_r) \;\;\text{and}\;\; \operatorname{Gal}(K^s,K) = \lim \operatorname{Aut}_K(L)$$ are supposed to give the same result, especially, since a product of finite separable extensions may not admit a morphism monomorphism into a common separable extension. Edit : There exists morphisms though . Why is a finite field $F$ considered to be an étale Eilenberg-Maclane space? When the issues in 1 are resolved, I see why $\pi_1^\text{et}(F) = \widehat{\Bbb Z}$ , but just for spaces this shouldn't be the only requirement. I tried to look for more ""evidence"", but it seems like higher étale fundamental groups are quite hard to define. So is there some other naive reason, one could say that? As always, thank you for your time and efforts!","['algebraic-geometry', 'eilenberg-maclane-spaces']"
4794374,Prove $k$ and $|G|$ is coprime when $\varphi_k(g)=g^k$ is a automorphism,"The proposition is as follows:"" Let $G$ be a finite Abelian group. Prove that $\varphi_k(g) = g^k$ is an endomorphism of $G$ . If $\varphi_k$ is an automorphism of $G$ , then $k$ and $|G|$ are coprime. "" I have already proven the endomorphism part, but I'm facing difficulties with the latter part of the proposition. I've attempted to use a proof by contradiction, trying to construct an element $g^{\frac{m}{(m,k)}}$ , where $m$ is the order of the element $g$ , in order to show that this mapping is not injective and leads to a contradiction. However, at the moment, $(m, k)$ could be $1$ , and I'm unsure how to proceed. I have seen the answer in Conditions for $x \mapsto x^k$ to be an automorphism but at this time I haven't studied neither Cauchy's theorem nor cyclic groups.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
4794479,Order of the volume growth of logarithmic spherical wedge,"I'm trying to find the volume growth of a wedge cut from a sphere in $\mathbb{R}^3$ . In particular the constraints are: $$-log(z+1)^2 \leq x \leq log(z+1)^2,$$ $$-z \leq y \leq z,$$ $$x^2+y^2+z^2\leq r^2.$$ I'm expecting the volume to be of order $\Omega(r^2)$ but I'm struggling with the computation. The problem where $x$ is bounded by a general smooth, positive, increasing function of $z$ is also of interest. ie $$-f(z) \leq x \leq f(z),$$ $$-z \leq y \leq z,$$ $$x^2+y^2+z^2\leq r^2.$$ I wonder what's an useful technique to deal with these computations?","['volume', 'multivariable-calculus', 'asymptotics']"
4794574,"Show that $h(z):=\int_X f(z,t) d\mu(t)$ is holomorphic","Let $\mu$ be a $\sigma-$ finite measure on some measure space $X$ , and $\Omega \subseteq \mathbb{C}$ be a domain. Assume that the mapping $(z,t)\in \Omega \times X \rightarrow f(z,t)\in \mathbb{C}$ is measurable for fixed $t \in X$ , the function $z \mapsto f(z,t)$ holomorphic and for every compact set $K \subseteq \Omega$ , $\int_X sup_{z \in K} |f(z,t)| d\mu(t) $ is finite Show that the function $h(z):=\int_X f(z,t) d\mu(t)$ is holomorphic in $\Omega$ Approach:
My Idea would be to use Moreas Theorem.
Thus I need to show that for every closed piecewise $C^1$ curve $\int_{\gamma}h(z)dz=0$ . $\int_{\gamma}h(z)dz=\int_{\gamma}\int_X f(z,t) d\mu(t)dz$ If I could change the order of Integration I would get $\int_{\gamma}h(z)dz=\int_X \int_{\gamma} f(z,t) dz d\mu(t)$ .
Then $\int_{\gamma} f(z,t) dz$ is the integral of a holomorphic function since $t$ is fixed in this case.
By Cauchy Theorem I would get $\int_{\gamma} f(z,t) dz=0$ , and thus $\int_X 0 d\mu(t)=0$ . My main problem seems to be that I don't know how to argue that I can interchange both integrals. I assume it has to do with 3). Question: Is my approach even correct? If yes, what is the reason to interchange the integrals?","['complex-analysis', 'measure-theory', 'analysis']"
4794578,"$\lim\limits_{n\to+\infty}\frac{1}{n}\sum_{k=1}^n|\overrightarrow{A_1A_k}|$ for $A_1,..,A_n$ vertices of a regular n sided polygon of radius one","Consider the following problem : Let $A_1,..,A_n$ be the vertices of a regular $n$ -sided polygon inscribed in a circle of radius one. Evaluate the limit $$
\lim\limits_{n \to +\infty} \frac{1}{n} \sum_{k=1}^n |\overrightarrow{A_1A_k}|.
$$ This can easily be solved by passing to complex numbers indeed we find that this is limit is equal to $$
\lim\limits_{n \to +\infty} \frac{1}{n}\sum_{k=0}^{n-1} |1-e^{\frac{2ik\pi}{n}}| = \lim\limits_{n \to +\infty} \frac{1}{n} \frac{2\cos(\frac{\pi}{2n})}{\sin(\frac{\pi}{2n})} = \boxed{\frac{4}{\pi}}.
$$ My question is, is there a geometric interpretation of this result that might make it ""obvious"" ? A colleague suggested looking into the cycloid and this seems very promising. Indeed, if I add the vectors $\frac{2\pi}{n}\overrightarrow{A_1A_k}$ one by one starting from the point $(2\pi,0)$ in geogebra it almost perfectly fits the cycloid of radius one and the fits get better with n. Since the cycloid of radius one has length $8$ I obtain the result. My problem is : I can't explain why the points (after suitable normalization) seem to almost land on the cycloid. Any ideas ? I link an image from geogebra where I take n=17. here is a link to the geogebra file (not cleaned up so I don'y know if it will be helpful) if you want to experiment with different values of $n$ and see the polygon rotate: here","['geometry', 'complex-numbers', 'polygons']"
4794641,About basic statistical definitions,"I'm having a hard time trying to understand some basic statistical definitions presented in the section II.9 of the book ""Mathematical Statistics"" written by Wiebe R. Pestman. Below I'll write the excerpt of the book that I'm trying to understand. Let $\Theta$ be an arbitrary set and let $f:\mathbb{R}\times\Theta \to
 [0,+\infty )$ be a function. For every fixed $\theta\in\Theta$ denote
the function $x\mapsto f(x,\theta)$ by $f(\bullet ,\theta)$ . If for
all $\theta\in\Theta$ this function presents a (possibly discrete)
probability density, then one talks about a family of probability
densities with parameter $\theta\in\Theta$ . [...] Now, once and for all, let $\big(f(\bullet,\Theta)\big)_{\theta\in\Theta}$ be any family of
probability densities. It is assumed that the population from which
the samples are drawn has a probability density $f(\bullet,\theta)$ ,
where $\theta\in\Theta$ . [...] Definition II.9.1. A function $\kappa:\Theta\to\mathbb{R}$ is said to be a characteristic of the population. [...] Definition II.9.2: Let $X_1,\cdots,X_n$ be a sample from a population with probability density $f(\bullet,\theta)$ , where $\theta\in\Theta$ . Suppose $\kappa$ is a characteristic of the
population. A statistic $T=g(X_1,\cdots,X_n)$ , the outcome of which is
used as an estimate of $\kappa(\theta)$ , is said to be an estimator
of $\kappa$ (based on the sample $X_1,\cdots,X_n$ ). Note that the expectation value of a statistic $T=g(X_1,\cdots,X_n)$ will, if existing, in general depend on the parameter $\theta$ . [...].
If in some context one has to count with this, one could write $\mathbb{E}_\theta(T)$ instead of $\mathbb{E}(T)$ . Definition II.9.3: A statistic $T$ is said to be an unbiased estimator of $\kappa$ if $\mathbb{E}_\theta(T)=\kappa(\theta)$ for all $\theta \in\Theta$ . My question is (EDITED): What does the definition II.9.3 mean? Does it mean that given any $\theta\in \Theta$ , any $n\in\mathbb{N}$ and any sample $X_1,\cdots,X_n$ with distribution $f(\bullet,\theta)$ there's a Borel measurable function $g:\mathbb{R}^n\to\mathbb{R}$ such that $T=g(X_1,\cdots,X_n)$ and $\mathbb{E}_\theta (T)=\kappa(\theta)$ ? I tried to interpret the content of that excerpt with the following definitions. Definition 1: We say that $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ is a statistical model if $(\Omega,\Sigma,\mathbb{P})$ is probability space, $\Theta$ is a non-empty set and $f_\theta:\mathbb{R}\to\mathbb{R}$ is a density function for all $\theta\in\Theta$ . Definition 2: Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model. We say that random variables $X_1,\cdots,X_n:\Omega\to\mathbb{R}$ is a $\theta$ -sample if $\theta\in\Theta$ , $X_1,\cdots,X_n$ are iid with respect to $(\Omega,\Sigma,\mathbb{P})$ and have common distribution $f_\theta$ . Definition 3: Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model and $X_1,\cdots,X_n$ be a $\theta$ -sample. We say that $T:\Omega\to \mathbb{R}$ is a statistic (with respect to that sample) if there's a Borel measurable function $g:\mathbb{R}^n\to\mathbb{R}$ such that $T=g(X_1,\cdots,X_n)$ . Definition 4 (EDITED): Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model and $\kappa:\Omega\to\mathbb{R}$ be any function. We say that a Borel function $g:\mathbb{R}^n\to \mathbb{R}$ is unbiased estimator of $\kappa$ if for any $\theta\in \Theta$ and any $\theta$ -sample $X_1,\cdots,X_n$ we have $\mathbb{E}[g(X_1,\cdots,X_n)]=\kappa (\theta)$ Another question: Are these definitions really a good interpretation of that excerpt? Also, are they in line with standard statistical definitions? Thank you for your attention!","['statistics', 'definition']"
4794659,To what extent smooth mappings of an affine line into a manifold determine its differentiable structure?,"If $M$ is a (real) differentiable manifold, its differentiable structure is completely determined if it is known which mappings $M\to\mathbf{R}$ are smooth. How much can be said about the differentiable structure of $M$ if it is known which mappings $\mathbf{R}\to M$ are smooth? I suspect that this is not enough to determine the differentiable structure of $M$ . If so, can there be a number $k <\operatorname{dim}M$ such that the smooth mappings $\mathbf{R}^k\to M$ completely determine the differentiable structure of $M$ ? I think my first question can be rephrased as follows: if $f\colon\mathbf{R}^n\to\mathbf{R}$ has the property that for every smooth $\gamma\colon\mathbf{R}\to\mathbf{R}^n$ , the composition $f\circ\gamma\colon\mathbf{R}\to\mathbf{R}$ is smooth, does this imply that $f$ is smooth? I conjecture that if $f\colon\mathbf{R}^n\to\mathbf{R}$ has the property that for every smooth $\sigma\colon\mathbf{R}^2\to\mathbf{R}^n$ , the composition $f\circ\sigma\colon\mathbf{R}^2\to\mathbf{R}$ is smooth, then $f$ is smooth.","['smooth-manifolds', 'manifolds', 'differential-topology', 'soft-question', 'differential-geometry']"
4794687,What matrices $M$ satisfy $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$?,"I'm trying to better understand matrices that are defined by preserving some other tensor. I recently asked a more involved question , and I realized it might be better to learn simpler examples first. As an example, consider the set of $n$ by $n$ real matrices $M$ that satisfy $M_{ik} M_{jl} \delta_{kl} = \delta_{ik}$ . Here $\delta_{ij}$ is the $n \times n$ Kronecker delta. This is merely a restatement of $M M^T = I$ which is the defining relation of orthogonal matrices. By exploring $M$ close to the identity, we can deduce that up to a reflection, $M$ will be the exponential of a real antisymmetric matrix. I want to understand generalizations where the $M$ 's instead preserve some trilinear form, not a bilinear one. Consider for example $\delta_{ijk}$ , the $n\times n \times n$ tensor that is $1$ if $i=j=k$ and $0$ otherwise. How can we characterize the set of real matrices $M$ that satisfy $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$ ? In particular, is a complete enumeration of these $M$ 's known? By inspection, one can see that if $M_1$ and $M_2$ satisfy this constraint, then so does $M_1 M_2$ . Similarly, if $M$ satisfies this constraint and is invertible, then so too does $M^{-1}$ . However, I'm finding it hard to say general statements about the properties of matrices $M$ satisfying $M_{il} M_{jm} M_{kn} \delta_{lmn} = \delta_{ijk}$ . For example, it's not even obvious to me whether all $M$ that satisfy this constraint must be invertible. By inspection, we can deduce some explicit solutions. If $M$ is a permutation matrix, it will satisfy the above equation. However, I'm having trouble guessing any more real matrices that satisfy the constraint. As an aside, if I weaken the constraint and allow $M$ to be complex, then note that I get additional solutions from the set of diagonal $M$ whose cube is the identity, but again I suspect I might be missing solutions. I suspect and hope that there are techniques that will give a complete enumeration of the kinds of matrices $M$ satisfying this constraint.","['matrices', 'orthogonal-matrices', 'tensors', 'linear-algebra']"
4794700,Contest math question about logarithmic and exponential functions,"Here's the question I'm trying to solve: For what value of $k$ is the function $f(x)=k^x$ tangent to $g(x)=\log_k(x)$ ? My Attempt: Since $f(x)$ and $g(x)$ are inverses of each other and are therefore symmetrical on $y=x$ , the tangent point must occur when $y=x$ . By setting $f(x)=x$ and $g(x)=x$ , we get $x=k^x$ and $x=\log_{k}x$ but these two equations are pretty much the same since $x=k^x \Rightarrow x=\log_{k}x$ so I can't find any unique solution. Most other approaches I've tried pretty much get me to the same two equations. I've tried taking the derivative of both functions which have to be 1; $f'(x)=\ln(k)k^x=1$ and $g'(x)=\frac{1}{\ln(k)x}=1$ and since previously $k^x=x$ we have $x \ln(k)=1$ but we get the same issue where we get two equivalent equations. After graphing it and trying to reverse engineer the answer I found that $1.4<k<1.5$ however this doesn't seem to correspond with any value one might expect either for example $\frac{e}{2}\approx 1.36$ or $\sqrt{e}\approx1.67$ or $\sqrt[3]{e}\approx 1.39$ . Note that this is a math competition question so should in theory (for this competition) be doable in about 5 minutes so I'm sure there's a trick to it that I have't figured out. (maybe Lambert W although I doubt it)","['contest-math', 'logarithms', 'functions', 'recreational-mathematics', 'exponential-function']"
4794709,Method of Steepest Descent and Contour Deformation,"In the book An Introduction to Quantum Field Theory by Peskin and Schroeder, p. 14 in section 2.1, it is stated that, in looking at the asymptotic behavior for $x^{2} \gg t^{2}$ of the integral \begin{equation}
    \int_{-\infty}^{\infty} \mathrm{d}p \; p \, \mathrm{e}^{\phi(p)}, \qquad \phi(p) = i\left( p x - t \sqrt{p^{2} + m^{2}}\right),\tag{1}
\end{equation} one can implement the method of stationary phase. However, the stationary points of $\phi(p)$ , $$p_{\pm} = \pm imx/\sqrt{x^{2} - t^{2}},\tag{2}$$ are imaginary, which would seem to indicate to me that the method of stationary phase is not applicable as is. The book then mentions that one can ""freely push the contour upward"", but exactly how and why is not clear to me. After looking around, it seems that the asymptotic behavior can be obtained by using the method of the steepest descent. However, I don't understand the following: How does one chooses a contour in the complex plane such that, through Cauchy's theorem I suppose, relates to the integral above, while at the same time passing through one of the stationary points. Should one choose only one of the stationary points? If yes, why? Any help is greatly appreciated.","['integration', 'asymptotics', 'complex-analysis', 'physics', 'stationary-point']"
4794791,Derivative of exponential of sum of non-commuting matrices,"Given two square real $m \times m$ matrices $A$ and $B$ , which will generally not commute, and given the function $$f(x)=e^{A+Bx}$$ Where $x$ is a real variable, is there an expression for the derivative $f'(x)$ ? Due to the fact that $A$ and $B$ may not commute, I can't write $e^{A+Bx}=e^A e^{Bx}$ , therefore it seems to me that the usual way to prove the expression of the derivative of the exponential for real functions does not extend to this case.","['derivatives', 'operator-theory', 'linear-algebra']"
4794800,What is the $n$th derivative of $f(x)=(1-x^3)^{-1}$?,"I am not entirely sure if this has been asked before, but I am trying to find the $n$ th derivative of: $$f(x)=(1-x^3)^{-1}=\frac1{1-x^3}$$ I first started by trying to look for a pattern: $$\begin{align}f'(x)&=-(-3x^2)(1-x^3)^{-2}=3x^2(1-x^3)^{-2}\\f''(x)&=6x(1-x^3)^{-2}+3x^2\cdot-2(-3x^2)(1-x^3)^{-3}\\&=6x(1-x^3)^{-2}+18x^4(1-x^3)^{-3}\\f^{(3)}(x)&=6(1-x^3)^{-2}+6x(-2)(-3x^2)(1-x^3)^{-3}+72x^3(1-x^3)^{-3}\\&+18x^4(-3)(-3x^2)(1-x^3)^{-4}\\&=6(1-x^3)^{-2}+108x^3(1-x^3)^{-3}+162x^6(1-x^3)^{-4}\\f^{(4)}(x)&=6(-2)(-3x^2)(1-x^3)^{-3}+324x^2(1-x^3)^{-3}+108x^3(-3)(-3x^2)(1-x^3)^{-4}\\&+972x^5(1-x^3)^{-4}+162x^6(-4)(-3x^2)(1-x^3)^{-5}\\&=360x^2(1-x^3)^{-3}+1944x^5(1-x^3)^{-4}+1944x^8(1-x^3)^{-5}\end{align}$$ But that didn't help in any way. Then I tried to tackle it in a different direction. One of the easier ways to take the nth derivative of $(1-x^2)^{-1}$ is to decompose it into $\frac1{2(x+1)}-\frac1{2(x-1)}=\frac12\left(\frac1{x+1}-\frac1{x-1}\right)$ , which is then easy to take the $n$ th derivative of as it is essentially only power rule (derivatives of both $x+1$ and $x-1$ are $1$ so chain rule won't affect anything here). My thought is to maybe decompose $\frac1{1-x^3}$ in a similar fashion, but this might result in imaginary terms popping up. Even so, is this strategy viable for calculating the $n$ th derivative of $f(x)=(1-x^3)^{-1}=\frac1{1-x^3}$ ? If not, what other strategies can I apply to solve this problem?","['calculus', 'derivatives']"
4794804,Linear algebra: What is the difference between target space of a function and the image of a function?,"I'm confused about the difference between target space of a function and image of a function. My textbook (Linear Algebra with Applications 5th edition Otto Bretscher) provided this information when introducing image, but I'm still struggling to understand conceptually the difference between target space and image. Textbook definition of image in relation to target space","['functions', 'linear-algebra']"
4794816,how to prove that $ \lim_{n \to \infty} \sum_{k=1} ^n \left( \sqrt[p] {\frac{n^p+k^{p-1}}{n^p}} -1 \right) = \frac{1}{p^2}$ for all $p \in \mathbb{R}$,"I saw this problem $L:=\lim_{n \to \infty}\sum_{k=1} ^ n \left( \sqrt {\frac{n^2+k} {n^2}} -1 \right)$ and I was able to prove that the limit is $\frac{1}{4}$ , I noticed my proof work for all $p \in \mathbb{N}$ so I generalised the result for all $p \in \mathbb{N}$ . my proof : for all $p \in \mathbb{N}-\{1\}$ ( if $p=1$ the answer is obviously $1$ ) $$L:=\lim_{n \to \infty} \sum_{k=1} ^ n\left( \sqrt[p] {\frac{n^p+k^{p-1}}{n^p}} -1 \right) =\lim_{n \to \infty} \frac{1}{n}\sum_{k=1} ^ n\left( \sqrt[p] {{n^p+k^{p-1}}} -n \right)$$ since $(a^p -b^p)=(a-b)(a^{p-1} +a^{p-2}b+a^{p-3}b^2+ \dots+ b^{p-1})  \ \ \forall p \in \mathbb{N}$ $$L =\lim_{n \to \infty} \frac{1}{n} \sum_{k=1} ^ n  \left( \frac{k^{p-1}}{\sum_{i=0} ^{p-1} (\sqrt[p] {{n^p+k^{p-1}}})^i n^{p-1-i}  }      \right) $$ $$\lim_{n \to \infty} \frac{1}{n} \sum_{k=1} ^ n  \left( \frac{k^{p-1}}{\sum_{i=0} ^{p-1} (\sqrt[p] {{n^p+n^{p-1}}})^i n^{p-1-i}  }      \right) \leq L \leq \lim_{n \to \infty} \frac{1}{n} \sum_{k=1} ^ n  \left( \frac{k^{p-1}}{\sum_{i=0} ^{p-1} (\sqrt[p] {{n^p}})^i n^{p-1-i}  }      \right) $$ $$\lim_{n \to \infty}  \left( \frac{(1+\frac{1}{n})^{\frac{1}{p}} -1}{\frac{1}{n} }\right)  \sum_{k=1} ^ n  \left( \frac{k^{p-1}}{n^{p}  }      \right)          \leq L \leq \lim_{n \to \infty}  \sum_{k=1} ^ n  \left( \frac{k^{p-1}}{ pn^{p}  }      \right) $$ $\lim_{n \to \infty} \left( \frac{(1+\frac{1}{n})^{\frac{1}{p}} -1}{\frac{1}{n} }\right)$ is the definition of derivative of $ x^{\frac{1}{p}}$ at $1$ so it is equal to $\frac{1}{p}$ and $\lim_{n \to \infty} \frac{1}{n} \sum_{k=1} ^ n  \left( \frac{k}{n   }      \right)^{p-1} $ is the Riemann sum of $ \int_0 ^1 x^{p-1}dx =\frac{1}{p} $ . By squeeze theorem $L = \frac{1}{p^2}$ my question is: can this result be generalised for all $p \in \mathbb{R}$ ? I believe that the limit will still be $\frac{1}{p^2}$ for all $p \neq 0$ but I couldn't prove that I tried to turn $\lim_{n \to \infty} \sum_{k=1} ^n \left( \sqrt[p] {\frac{n^p+k^{p-1}}{n^p}} -1 \right)$ for all $p \in \mathbb{R}$ to integral but I couldn't do that.","['integration', 'definite-integrals', 'calculus', 'sequences-and-series', 'limits']"
4794896,Maximal ideal space of the $n$-dimensional ball algebra,"Let $B_n \subset \mathbb{C}^n$ be the $n$ -dimensional open ball of radius $1$ centered at the origin. Let $A$ be the set consisting of holomorphic functions in $B_n$ which are continuous in $\overline{B_n}$ , then $A$ is a Banach space with the sup-norm and pointwise operations. Let $M$ be the maximal ideal space of $A$ , that is, $M$ is the set of non-trivial linear multiplicative functionals in $A$ . I want to prove that $M$ and $\overline{B_n}$ are homeomorphic via point-evaluations. In other words, I want to prove that the map $F: \overline{B_n} \longrightarrow M$ defined by $F(x)(\varphi)=\varphi(x)$ is an homeomorphism. Of course, in order to do so the first step is to prove that $F$ is a bijective function. Clearly $F$ is injective. My idea to prove surjectivity was the following: take any $\varphi \in M$ and let $p(z)=\sum_{|\alpha|=0}^{n}{c_{\alpha} z^{\alpha}}$ be a polynomial (here we're using the multi-index notation). For each $i, 1 \leq i \leq n$ , let $f_i:\overline{B_n} \longrightarrow \mathbb{C}$ be the map defined by $f_{i}(z)=z_i$ , then we can write $p=\sum_{|\alpha|=0}^{n}{c_{\alpha} f^{\alpha}}$ , where $f=(f_1,\ldots,f_n)$ . Therefore, since $\varphi \in M$ we have that $$
\varphi(p)=\sum_{|\alpha|=0}^{n}{c_{\alpha} \varphi(f)^{\alpha}}=p(\varphi(f))
$$ therefore, if I could prove that $\varphi(f)=(\varphi(f_1),\ldots,\varphi(f_n))$ is in $\overline{B_n}$ I would be done proving the surjectivity (since polynomials are dense in $A$ ). Of course, I can see that $|\varphi(f_i)| \leq 1$ but I don't see why would we have that $|\varphi(f)| \leq 1$ . Any suggestion in order to prove that $|\varphi(f)| \leq 1$ ?  Or, if my approach is wrong I would appreciate any hint. In advance thank you very much.","['banach-spaces', 'operator-algebras', 'banach-algebras', 'complex-analysis', 'functional-analysis']"
4794962,Prove that $\operatorname{GL}_n(k)$ and $\operatorname{SL}_n(k)$ cannot be symmetric group?,"Problem. Prove that $\operatorname{GL}_n(k)$ and $\operatorname{SL}_n(k)$ cannot be isomorphic to $S_m$ , $m\geq 4$ if $k$ is a finite field with at least two elements. I am trying to argue by looking at the order of the two linear groups and prove that they cannot be a factorial. I have computed that $$\operatorname{GL}_n(k)=(q^n-1)...(q^n-q^{n-1})$$ and $$\operatorname{SL}_n(k) = \frac{(q^n-1)...(q^n-q^{n-1})}{q-1}$$ where $q=|k|$ and I am taking a look at the special case when $q$ is a prime and conclude the factorial with $q$ occuring $n-2$ times would be much larger than the order of the general linear group. Is there a better way I could argue this?","['general-linear-group', 'group-theory', 'abstract-algebra', 'linear-groups']"
4794968,Very complicated differentiation,"I am trying to solve all steps in a economics paper , but after spending two days with the same differentiation Im losing faith. Can someone out there help me? The problem: Differentiate: $$
\begin{align}
&R_{pg}(w) = 
\left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)w \\
&+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pp} - \lambda_{gp}\right) 
\int_{w}^{\infty} W'_{p}(x)(1 - F_{p}(x)) \, dx \\
&+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pg} - \lambda_{gg}\right) 
\int_{R_{pg}(w)}^{\infty} W'_{g}(x)(1 - F_{g}(x)) \, dx
\end{align}
$$ additional info: $$
W'_{p}(x) = \left[r + \mu + \delta_{p} + \lambda_{pp} (1 - F_{p}(w)) + \lambda_{pg} (1 - F_{g}(R_{pg}(w)))\right]^{-1}
$$ $$
W'_{g}(x) = \left[r + \mu + \delta_{g} + \lambda_{gp} (1 - F_{p}(R_{gp}(w))) + \lambda_{gg} (1 - F_{g}(w))\right]^{-1}
$$ The result should be: $$
R'_{pg}(w) = \frac{r + \mu + \delta_g + \lambda_{gp}(1 - F_{p}(w)) + \lambda_{gg}(1 - F_{g}(R_{pg}(w)))}{r + \mu + \delta_p + \lambda_{pp}(1 - F_{p}(w)) + \lambda_{pg}(1 - F_{g}(R_{pg}(w)))}
$$ (I left a couple of terms out as they should not depend on w.)","['economics', 'calculus', 'derivations', 'derivatives']"
4795003,The closed-form of $1-5\left(\frac{1}{2}\right)^k+9\left(\frac{1\cdot3}{2\cdot4}\right)^k-13\left(\frac{1\cdot3\cdot5}{2\cdot4\cdot6}\right)^k+\dots$?,"( A related MSE question by P. Singh .) First define, $$F_k = 1-5\left(\frac{1}{2}\right)^k+9\left(\frac{1\cdot 3}{2\cdot 4}\right)^k-13\left(\frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6}\right)^k+17\left(\frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8}\right)^k-\cdots$$ Or more concisely, $$F_k = \sum_{n=0}^\infty\, (-1)^n\,(4n+1) \left(\frac{\Gamma\big(n+\tfrac{1}{2}\big)}{\Gamma\big(n+1\big)\;\Gamma\big(\tfrac{1}{2}\big)}\right)^k$$ The gamma quotient is also, $$ \frac{\Gamma\big(n+\tfrac{1}{2}\big)}{\Gamma\big(n+1\big)\;\Gamma\big(\tfrac{1}{2}\big)} = \frac1{2^{2n}}\frac{(2n)!}{\,n!^2}$$ Then we have the nice closed-forms, \begin{align}
F_1 &= 0\\[4pt]
F_2 &= \dfrac{2\sqrt2\,\Gamma\big(\tfrac{1}{2}\big)}{\Gamma^2\big(\tfrac{1}{4}\big)}\\[4pt]
F_3 &= \dfrac{2}{\Gamma^2\big(\tfrac{1}{2}\big)}=\dfrac{2}{\pi} \\[4pt]
F_4 &= \; \color{red}{??}\\[4pt]
F_5 &= \dfrac{2}{\Gamma^4\big(\tfrac{3}{4}\big)}
\end{align} I found $F_2$ empirically. The closed-form of $F_3=\large\frac{2}{\pi}$ was included by Ramanujan in his letter to Hardy, and $F_5$ is also by him. (I presume a version of $F_2$ may be in his Notebooks.) Alternatively, while the $F_k$ are clearly a sum of two generalized hypergeometric functions $_pF_q$ , we learn from this post that they can also be expressed as just one $_pF_q$ , \begin{align}
F_1 &= \,_2F_1\big(\tfrac12,\tfrac54;\tfrac14;-1\big)\\[4pt]
F_2 &= \,_3F_2\big(\tfrac12,\tfrac12,\tfrac54;\tfrac14,1;-1\big)\\[4pt]
F_3 &= \,_4F_3\big(\tfrac12,\tfrac12,\tfrac12,\tfrac54;\tfrac14,1,1;-1\big)\\[4pt]
F_4 &= \,_5F_4\big(\tfrac12,\tfrac12,\tfrac12,\tfrac12,\tfrac54;\tfrac14,1,1,1;-1\big)= \; \color{red}{??}\\[4pt]
F_5 &= \,_6F_5\big(\tfrac12,\tfrac12,\tfrac12,\tfrac12,\tfrac12,\tfrac54;\tfrac14,1,1,1,1;-1\big)
\end{align} and so on. Question : What is the closed-form of $F_4,\,F_6$ and others, if any?","['gamma-function', 'pi', 'closed-form', 'sequences-and-series']"
4795007,Chaces of get $b$ points throwing dices of set $D$,"I'm currently working on a casino game involving throwing different dice. To adjust the fair pay of each play, I'm trying to develop the formula that describes this scenario: Number of combinations that result in a total point sum of $b$ , knowing that I will throw the dice of a certain set $D = \left\{(d_1, d_2) \mid d_1 = \text{number of dice}, d_2 = \text{number of sides}\right\}$ . Notice that my question is not about throwing just one kind of die (which I know is already solved in this site). I want to describe the chances of a scenario where the player can choose to use any number of dice each with different number of sides in the same throw . In other words, given any combination of dice (for example, let's say I throw $n_4$ dice of $4$ sides, $n_6$ dice of $6$ sides and $n_8$ dice of $8$ sides), how can I calculate the total combinations of throws that results in a total sum of $b$ , for $b = n_4+n_6+n_8, \dots, 4n_46n_68n_8$ ? My work so far: I've noticed that the combinations follow some way of triangular pattern. For example, let's say I throw 2 dice of 6 faces. Then the combinations for each sum are: $2$ : $\{(1,1)\}$ $3$ : $\{(1,2),(2,1)\}$ $4$ : $\{(1,3),(2,2),(3,1)\}$ $5$ : $\{(1,4),(2,3),(3,2),(4,1)\}$ $6$ : $\{(1,5),(2,4),(3,3),(4,2),(5,1)\}$ $7$ : $\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\}$ $8$ : $\{(2,6),(3,5),(4,4),(5,3),(6,2)\}$ $9$ : $\{(3,6),(4,5),(5,4),(6,3)\}$ $10$ : $\{(4,6),(5,5),(6,4)\}$ $11$ : $\{(5,6),(6,5)\}$ $12$ : $\{(6,6)\}$ So for throws where there are just one kind of die (the number of sides is constant) I already have a formula. My problem is defining this rule for different types of die in the same throw. I noticed it always follows a similar triangular like distribution, but it's just not a perfect triangle (like it is when throwing all similar dices). In that case, there were three numbers for which the total number of combinations was max (instead of one or two). Is there a formula for this expression? Any help or hint will be appreciated. Edit: after reading a response from a user, I'm also interested of a solution to the problem where the number is obtained from multiplying the points obtained in each die (instead of adding).","['statistics', 'combinations', 'dice', 'combinatorics', 'probability']"
4795008,"If $S=(X, \mathcal{F})$ is a measurable space, $\mu,\nu$ two measure on S s.t. $\nu\ll\mu$, are all $\mu$-measurable functions also $\nu$-measurable?","Let $S = (X,\mathcal{F})$ be a measurable space, i.e. $X$ be a set and $\mathcal{F}$ be a $\sigma$ -algebra on $X$ . Suppose $\mu, \nu$ are two measures on $S$ such that $\nu$ is absolutely continuous with respect to $\mu$ , i.e. write $\nu\ll\mu$ to denote that $\forall A\in \mathcal{F}:\mu(A) = 0\implies \nu(A) = 0$ . Is it then true that every $\mu$ -measurable function $f$ is also $\nu$ -measurable? That is, if $(Y, \mathcal{G})$ is another measurable space and $f:X\to Y$ is a map such that $\forall A \in G:\text{$f^{-1}(A)\in \mathcal{F}$ and $f^{-1}(A)$ is $\mu$-measurable}$ . This can very well be a completely trivial question, but I don't recall many tools that allow one to tackle a question like this. As of now, I also don't know how to approach it: Canonically we would like to show that $$\forall A\in \mathcal{G}, B\subset X, : \nu(B) = \nu(f^{-1}(A)\cap B) + \nu(B\setminus f^{-1}(A))$$ and our only source of information is that $$\forall A\in \mathcal{G}, B\subset X, : \mu(B) = \mu(f^{-1}(A)\cap B) + \mu(B\setminus f^{-1}(A))$$ How should we apply the absolute continuity in this case?","['measure-theory', 'measurable-functions', 'real-analysis']"
4795110,Sum of the reciprocal of ec exponents leading to a probable prime,"Ec primes are primes of the form: $(2^n-1)\cdot 10^d+2^{n-1}-1$ where d Is the Number of decimal digits of $2^{n-1}$ .
The n's leading to a prime or probable prime are present in this vector: [2,3,4,7,8,12,19,22,36,46,51,67,79,215,359,394,451,1323,2131,3336,3371,6231,19179,39699,51456,56238,69660,75894,79798,92020,174968,176006,181015,285019,331259,360787,366770,...,541456] There could be exponents leading to a prime between 366770 and 541456 missing. Moreover for exponents greater than $10^5$ there Is no doublecheck and so It could be that some other exponent Is missing. Anyway the sum of the reciprocals of the exponents leading to a prime or probable prime comes close to $\frac{\pi^2}{6}$ , which Is the famous value of the Basel problem. The sum Is equal to $1.643688...$ Considering that exponents leading to a prime become sparser and sparser, Is It possible that the sum equals exactly $\frac{\pi^2}{6}$ ? And Is this another coincidence?",['number-theory']
4795146,Computing a limit $ \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos(\sin(\frac{1}{k})) $,"Find $$ \lim_{n \to \infty} \frac{1}{n}\sum_{k=1}^{n} \cos\left(\sin\left(\frac{1}{k}\right)\right) $$ I found the limit by approximating it from its graph. I could not get a proper solution
I drew the graph in desmos from which anyone can approximate the limit.
I also tried taylor expansion of $\cos(x)$ to its first 3 terms as higer powers will be negligible which gives $$\lim_{n \to \infty} \left(1 - \frac{1}{n} \sum_{k=1}^{n} \frac{\sin ^2{\frac{1}{k}}}{2!} + \frac{\sin ^4{\frac{1}{k}}}{4!}\right)$$ In which I think the summation goes to zero since $\sin ^2{x}$ is a bounded function","['limits', 'calculus']"
4795181,$(1 + \sqrt{2})^{2020} = a + b \cdot \sqrt{2}$ what is the value of $a^2-2b^2$?,"$a$ and $b$ are two integers such that: $$(1+\sqrt{2})^{2020} = a + b\sqrt{2}$$ What is the value of: $a^2 - 2b^2$ ?
And I know that $a^2-2b^2=(a+b\sqrt 2)(a-b\sqrt 2)$ but the problem is Idk the value of $a-b\sqrt 2$ Note that newton's binomials or anything like that is not allowed as I didn't study it,yet it's still dedicated to my level.
Thx in advance for everyone who's trying to help",['algebra-precalculus']
4795208,Have the prime Egyptian fractionary expansions of rational numbers been studied before?,"Background In a 2018 question posed by Zhi-Wei Sun, he conjectures that for any rational number $r>0$ , there are finite sets $P_r^-$ and $P_r^+$ of primes such that $$r=\sum_{p\in P_r^-}\frac1{p-1}=\sum_{p\in P_r^+}\frac1{p+1}. \tag{1}$$ Recently, Thomas Bloom has answered this question, and has published a preprint on the ArXiv in which a proof is presented that affirms this conjecture. Prime Egyptian Fractionary Expansions I'm currently considering a generalization of the representations presented in $(1)$ . We could also consider powers of every term in the sum, resulting in a representation of the form: $$q = \sum_{p\in P_q^-}\frac{1}{(p-1)^{a_p}}=\sum_{p\in P_q^+}\frac{1}{(p+1)^{b_p}}. \tag{2} $$ Here, $|P_q^-| = l $ and $ |P_q^+| = m$ . Also, $a_p$ and $b_p$ are positive integer numbers for all $p$ . Then, the positive and negative Prime Egyptian Fractionairy Expansions $^{*}$ (PEFEs) are defined as $$q_{-} =  a_{p_l} \ a_{p_{l-1}} \ \dots \ a_{p_{2}} \ a_{p_{1}}, \tag{3} $$ and $$ q_{+} = b_{p_{m}} \ b_{p_{m-1}} \ \dots \ b_{p_{2}} \ b_{p_{1}}. \tag{4}$$ Example For instance, we might have the following expansion: $$\frac{3}{8} = \frac{1}{(3-1)^{\color{red}3}} + \frac{1}{(5-1)^{\color{red}1}}. \tag{5} $$ Thus, we obtain the negative PEFE in positional notation: $$ \left( \frac{3}{8} \right)_{-} = \color{red}1 \ \color{red}3 \quad . \tag{6} $$ Also, we have $$\frac{3}{8} = \frac{1}{(3+1)^{\color{blue}1}} + \frac{1}{(7+1)^{\color{blue}1}}. \tag{7}$$ Here, we skipped the prime numbers $2$ and $5$ in the sum. We denote their corresponding $\quad$ powers as $\bar{0}$ in the (in this case, positive) PEFE: $$ \left( \frac{3}{8} \right)_{+} = \color{blue}1 \ \bar{0} \ \color{blue}1 \ \bar{0} \quad. \tag{8} $$ Questions Have such prime Egyptian fractionary expansions of the form described in $(2) - (4)$ been described in the literature before? Do they bear a relationship with the $p$ -adic numbers, and if so, how can this relationship be characterized? Note (*) this name is a contraction of Egyptian fraction and binary (or ternary, or quaternary, etc.)","['binary', 'egyptian-fractions', 'number-theory', 'reference-request', 'prime-numbers']"
4795261,When is general linear group isomorphic to special linear group?,"Can $\operatorname{GL}_n(\Bbb F_{q})$ be isomorphic to $\operatorname{SL}_m(\Bbb F_{r})$ , where $\Bbb F_q$ , $\Bbb F_r$ are finite fields with $q,r$ elements respectively? By considering the center, $$\left|\operatorname{Z}(\operatorname{GL}_n(\Bbb F_q))\right|=q-1$$ and $$\left|\operatorname{Z}(\operatorname{SL}_m(\Bbb F_r))\right|=\operatorname{gcd}(m, r-1)$$ and this helps ruling out some cases, but did not solve the problem completely. Also note that $\operatorname{SL}_m(\Bbb F_r)$ is simple when $\operatorname{gcd}(m,r-1)$ so in this case it could never to isomorphic to the general linear group, which has $\operatorname{SL}_n(\Bbb F_q)$ as a normal subgroup. As commented below, I listed the order of the two linear groups below: $$\left|\operatorname{GL}_n(\Bbb F_q)\right|=(q^n-1)\dots (q^n-q^{n-1})$$ and $$\left|\operatorname{SL}_m(\Bbb F_r)\right| = \frac{(r^m-1)\dots (r^m-r^{m-1})}{r-1}$$ I cannot see immediately that the two could not be equal though. I am guessing they could never be isomorphic except for a finite number of small orders, how could I show it?","['group-theory', 'abstract-algebra', 'group-isomorphism', 'linear-groups']"
4795342,Diffeomorphism from the unit disc to the $n$-dimensional Euclidean space,"Let $f: B^n \to \Bbb R^n$ be a map from the unit disc given by $x \mapsto \dfrac{x}{\sqrt{1-\|x\|^2}}$ . Show that this map is a diffeomorphism. So I'm trying to find a way to derive the inverse for this map, but I find it quite difficult. I've managed to solve the $1$ -dimensional case with the following: Consider $f:B^1 \to \Bbb R$ given by $x \mapsto \dfrac{x}{\sqrt{1-|x|^2}}$ . To find the inverse of $f$ we set $y =  \dfrac{x}{\sqrt{1-|x|^2}}$ from where we can solve to get $x^2+y^2|x|^2 = y^2$ . And now the crucial point, since $|x|^2 = x^2$ we get $x^2+y^2x^2 = y^2$ and so $x= \dfrac{y^2}{\sqrt{1+y^2}}$ i.e. the inverse $f^{-1}$ is given by $y \mapsto \dfrac{y^2}{\sqrt{1+y^2}}$ . The crucial point here is that due to $x$ being a real number I have $|x|^2 = x^2$ , but $x^2$ is ambiguous when $x$ is a vector so this method wont generalize. A similar approach will only lead to $$\begin{align*} y &=  \frac{x}{\sqrt{1-\|x\|^2}} \\ y(\sqrt{1-\|x\|^2})&=x \\
y^2(1-\|x\|^²)&=x^2 \\
y^2-y^2\|x\|^2&=x^2 \end{align*}$$ so $$x^2+y^2\|x\|^2=y^2$$ but this is as far as I can get as I don't know anything I could to the term $\|x\|^2$ . What could I do here?","['multivariable-calculus', 'diffeomorphism']"
4795383,"An atypical integral with arctan, log, and radical","The following integral appears in More (Almost) Impossible Integrals, Sums, and Series (2023) (evaluation details on pages $301$ - $306$ ), the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) : $$\int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x$$ $$=\frac{1}{2}\log ^2(\sqrt{2}-1)\pi-\frac{\pi^3}{8}-3\pi\operatorname{Li}_2(1-\sqrt{2}),\tag1$$ where $\operatorname{Li}_2$ represents the Dilogarithm function. Now, in the book, there are two preliminary steps needed before evaluating the preceding integral, which are as follows: $$\int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x$$ $$=\frac{5}{24}\pi ^3-\frac{1}{2}\log ^2(\sqrt{2}-1)\pi+4\pi \operatorname{Li}_2(1-\sqrt{2}) \tag2$$ and $$\small  \int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x+2 \int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}} \textrm{d}x$$ $$=\frac{1}{2}\log^2(\sqrt{2}-1)\pi-\frac{\pi^3}{24}-2\pi\operatorname{Li}_2(1-\sqrt{2}). \tag3$$ Question_1: How would we like to go differently in $(1)$ without involving the mentioned preliminary steps in $(2)$ and $(3)$ (maybe in a more direct way)? Question_2: I also find interesting the version with the squared arctan. Ideas for its evaluation? $$\int_0^1 \arctan^2 \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x,$$ or more generally, $$\mathcal{I}=\int_0^1 \arctan^n \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x, \ n\ge2,\ n \in \mathbb{N}.$$","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4795393,The normalization of the affine algebraic set $V(x^2+y^3)$,"I don't have much experience in determining the normalization of an irreducible affine algebraic set. I'm trying this example: $$F(x,y)=x^2+y^3 \in \mathbb{C}[x,y]$$ I understand that I need to find a normal algebraic set $\tilde{V}$ and a surjective morphism $\tilde{V}\to V =V(x^2+y^3)$ such that the induced morphism is injective and $\mathbb{C}[\tilde{V}]$ is isomorphic to the integral closure of $\mathbb{C}[V]$ . I know that $\varphi(s)=(s^3,s^2)$ is a normalization, but I don't understand how to get there. Could you help me, how can I try to resolve it?
What should I do initially?
Any references that can help me?","['algebraic-geometry', 'commutative-algebra']"
4795406,What should the $\mathfrak{sl}(3)$ Chebyshev polynomials be?,"Consider $\mathfrak{sl}_2$ and its fundamental weight $\lambda_1$ . The character of the simple representation $L(n\lambda_1)$ with highest weight $n\lambda_1$ is given by a polynomial in $x=\mathrm{ch}L(\lambda_1)$ . Writing $P_n(x)=\mathrm{ch}L(n\lambda_1)$ , we have a recurrence $P_n(x)=xP_{n-1}(x)-P_{n-2}$ . The first few polynomials are $1,x,x^2-1,x^3-2x\dots$ These are actually the Chebyshev polynomials of the second kind on the interval $[-2,2]$ . Consider the same question for $\mathfrak{sl}_3$ , with simple roots $\alpha_1$ , $\alpha_2$ and corresponding fundamental weights $\lambda_1$ , $\lambda_2$ . Let $x=\mathrm{ch}L(\lambda_1)$ and $y=\mathrm{ch}L(\lambda_2)$ . Let $P_{m,n}(x,y)=\mathrm{ch}L(m\lambda_1+n\lambda_2)$ . What recurrence do the $P_{m,n}(x,y)$ satisfy? Recall that the classical Chebyshev polynomials satisfy $\int_{-2}^2P_m(x)P_n(x)\sqrt{4-x^2}dx=\delta_{m,n}$ . Are the $P_{m, n}$ orthogonal in the way that the Chebyshev polynomials are orthogonal?","['chebyshev-polynomials', 'abstract-algebra', 'lie-algebras', 'recurrence-relations']"
4795421,Why can we not write the reals as a countable union of sets,"I understand that the reals are not countable, what goes wrong with this? $$\mathbb{R} = \bigcup_{n=1}^\infty (-n,n) $$","['elementary-set-theory', 'real-numbers', 'analysis', 'real-analysis']"
4795444,"Computing the variance of a ""complicated"" random variable","Let $f_{X}(x)=\frac{1}{(x+1)^2}$ when $x\geq 0$ , and $0$ otherwise. I'm being asked to compute the following expectation: $\mathbb{E}[(1+X)^2e^{-2X}].$ Now, I have decomposed the expression in this way: $$\mathbb{E}[(1+X)^2e^{-2X}]= \mathbb{E}[X^2e^{-2X}]+\mathbb{E}[2Xe^{-2X}]+\mathbb{E}[e^{-2X}].$$ Assuming the multiplication of two random variables allow you to just multiply them in the definition of the expectation, I get complicated expressions that suggest to me I'm in the wrong track or there's something I'm not seeing. For example: $$\mathbb{E}[X^2e^{-2X}]=\int_{0}^{\infty}x^2 \left(\frac{1}{(x+1)^2}\right)e^{-\frac{2}{(1+x)^2}}.$$ The similarity with the normal distribution tells me that maybe it's part of the solution. But again, I'm not sure I'm even on the right track with this. Thank you.","['expected-value', 'variance', 'probability']"
4795488,Integral relating gamma function's linear subspaces,"I'm trying to solve the equation: $$\int_0^\infty \frac{\cos(s\ln(t)+\frac{π(4n+1)}{4})}{\sqrt{t}e^t}\mathrm{d}t=0$$ For the variable $s\in\mathbb{R}$ , where $n\in\mathbb{Z}$ is any nonzero integer.
This is related to a more complicated integral equation: $$\int_0^\infty \left(\frac{\cos(s\ln(t))-\sin(s\ln(t))}{\sqrt{t}e^t}\right)\mathrm{d}t=0$$ This equation involves certain linear subspaces of the complete gamma function. More specifically, the subspace of all the vomplef numbers of the form: $a(1+i)$ where $a$ is any real number. Thanks for the help.","['gamma-function', 'indefinite-integrals', 'ordinary-differential-equations']"
4795501,Eigenvalue of the 'norm' matrix,"A week ago, I asked a question about the matrix composed with absolute difference of distinct number must have exactly only one positive eigenvalue. But I have no idea how to prove it with 'norm' matrix $A=(a_{ij})_{n\times n}$ , where $$
a_{ij}=\|\xi_{i}-\xi_{j}\|_2.
$$ Here $\xi_i\in\mathbb{R}^m$ are distinct vector and $\|\cdot\|_2$ is the Euclidean norm. Also I have verified it with MATLAB for $n=20$ and $m=5$ , but the method used for $m=1$ failed. Any advice is welcome!","['matrix-norms', 'linear-algebra', 'eigenvalues-eigenvectors']"
4795502,Restricted sum version of multinomial theorem,"The multinomial theorem states that $$
\sum_{\substack{n_1 \geq 0, \ldots, n_k \geq 0\\ n_1 + \cdots + n_k = n}} {n \choose n_1, \ldots, n_k} \, p_1^{n_1} \cdots p_k^{n_k} 
= (p_1 + \cdots + p_k)^n \,.
$$ Is there a simple closed-form expression for the closely-related sum $$
\sum_{\substack{m > n_1 \geq 0, \ldots, m > n_k \ge 0\\ n_1 + \cdots + n_k = n}} {n \choose n_1, \ldots, n_k} \, p_1^{n_1} \cdots p_k^{n_k} \,,
$$ where no summed-over integer is allowed to exceed some maximum value, in this case $m$ ? Evaluating this sum would be useful in calculating the probability that a given event $i \in \{1, \ldots, k\}$ is the mode of a sample of a multinomial distribution.","['probability-distributions', 'combinatorics', 'multinomial-coefficients', 'discrete-mathematics', 'probability-theory']"
4795520,Can't understand proof of fundamental theorem of symmetric polynomials,"I'm reading through Introduction to Abstract Algebra 4th edition by Nicholson, and I'm having trouble understanding the proof for the fundamental theorem of symmetric polynomials in section 4.5 (Never before in reading this book have I had such a hard time understanding a proof, and this is only paragraph 1 of 7!! Normally after a minute things would click when something's not clear when I'm reading a proof, but this isn't happening here.). In particular I've been stuck on the first paragraph for like 30 minutes yet the final few sentences are not very clear. Let $g = g(x_1, \dots, x_n) \neq 0$ be symmetric. If $k_1, \dots, k_m$ are the (distinct) integers that occur as degrees of monomials in $f$ , then $g = g_1 + \cdots + g_m$ , where $g_i$ is homogeneous of degree $k_i$ for each $i$ . Given $\sigma \in S$ , and a monomial $h(x_1, \dots, x_n)$ , the fact that $h(x_1, \dots, x_n)$ and $h(x_{\sigma 1}, \dots, x_{\sigma n})$ have the same degree shows that each $g_i$ is itself symmetric. Hence, we may assume that $g$ is homogeneous. Ok, I'm mainly confused with the conclusions the author is drawing here, and here are my two questions: How did the author conclude that each $g_i$ is symmetric? How does the fact that $g$ is homogeneous follow from this fact? Thanks! And please don't explain at the graduate level. I'm not a graduate. There's a reason I'm reading an introductory book. This has happened numerous times so I'm just putting this here.","['proof-explanation', 'abstract-algebra', 'symmetric-polynomials', 'polynomials']"
4795536,"Ways to tackle the integral $\int_{0}^{\frac{\pi}{4}}\operatorname{Li}_3(\tan^4 x) \, dx$","$$\boxed{J = \int_0^{\frac{\pi}{4}}\operatorname{Li}_3(\tan^4(x)) \, dx}$$ Since I had no clue about trilogarithms, tried some searching to get enough understanding to solve the above integral, I found this general relation; $$\operatorname{Li}_s(z)=\frac{\Gamma(1-s)}{2\pi^{1-s}}\left(i^{1-s}\zeta\left(1-s,\frac{1}{2}+\frac{\ln(-z)}{2\pi i}\right)+i^{s-1}\zeta\left(1-s,\frac{1}{2}-\frac{\ln(-z)}{2\pi i}\right)\right)$$ Also, some general functional equations from here , specifically, $$\operatorname{Li}_3(z)+\operatorname{Li}_3(-z)=\frac{1}{4}\operatorname{Li}_3(z^2)$$ $$\operatorname{Li}_3(z)-\operatorname{Li}_3(-z^{-1})=\frac{-1}{6}\left(\ln^3 z+\pi^2 \ln z\right)$$ Or rewriting the above as; $$\operatorname{Li}_3(z)-\operatorname{Li}_3(-z^{-1})=\frac{-1}{6}\ln^3 z-\zeta(2)\ln z$$ Understanding any other aspects about trilogarithms (or polylogarithm in general) required knowledge was beyond my scope. So I started solving the integral as follows; Using some prior experience in solving some basic dilogarithmic integrals, I substituted $\tan(x)=t$ ; $$J=\int_0^{1}\frac{\operatorname{Li}_3(t^4)}{1+t^2}\,dt$$ $$J=\int_0^{1}\frac{\operatorname{Li}_3(x^4)}{1+x^2}\,dx$$ Using the first functional equation, $$J=\int_0^{1}\frac{4\operatorname{Li}_3(x^2)}{1+x^2}\,dx+\int_0^{1}\frac{4\operatorname{Li}_3(-x^2)}{1+x^2}\,dx=J_1+J_2$$ Edit 1: $$\operatorname{Li}_n(z)=\frac{(-1)^{n-1}}{(n-1)!}\left[\int_0^1\frac{z\ln^{n-1}x}{-zx+1}\,dx\right]$$ Found the above here in the 5th integral representation. Putting $n=3$ , $$J_1=\int_0^{1}\frac{4\operatorname{Li}_3(x^2)}{1+x^2}\,dx=2\int_0^1\int_0^1\frac{1}{(1+x^2)}\frac{x^2\ln^{2}t}{(1-x^2t)}\,dx\,dt$$ Edit 2: $$J_1=2\int_0^1\int_0^1\frac{x^2\ln^{2}t}{(1-x^2t)(1+x^2)}\,dx\,dt$$ $$J_1=\int_0^1\int_0^1\frac{\ln^{2}t}{(1-x^2t)(1+x^2)}\,dx\,dt-\int_0^1\int_0^1\frac{\ln^{2}t}{(1+t)(1+x^2)}\,dx\,dt$$ Could take this further but it seems like I'm missing some identity or formula to move forward. I am interested in understanding how to solve this integral. Edit 3: The answer is $$\boxed{J=\frac{1}{8}\left(\zeta\left(4,\frac{1}{4}\right)-\zeta\left(4,\frac{3}{4}\right)\right)-\pi \left(\frac{2\pi G}{3}+\frac{27\zeta(3)}{4}\right)}$$ $G$ is Catalan's constant.","['integration', 'contest-math', 'definite-integrals', 'special-functions', 'calculus']"
4795543,Show that bounded continuous functions on $\Bbb R$ with $f(x)=\int_{x}^{x+1}f(t)\mathrm{d}t$ are constant,"I was doing my homework and one question occur to me: Let $f$ be a bounded continuous function on $\mathbb{R}$ that satisfies $f(x)=\int_{x}^{x+1}f(t)\mathrm{d}t$ for all $x\in\mathbb{R}$ . Show that $f$ is a constant, i.e. $f\equiv C$ for some $C\in\mathbb{R}$ . I didn't think too much of it, and I thought it was easy. This is my answer: By definition of $f$ it is trivial that $f\in C^\infty(\mathbb{R})$ . By taking the derivative on both side of the equation we obtain $$f^\prime(x)=f(x+1)-f(x),\forall x\in\mathbb{R}.$$ Since $f\in C^\infty(\mathbb{R})$ , it is valid to take the Taylor series of $f$ , i.e. $$f(x+1)-f(x)=\left(f(x)+\frac{f^\prime(x)}{1!}+\frac{f^{\prime\prime}(x)}{2!}+\cdots\right)-f(x)=f^\prime(x)+\frac{f^{\prime\prime}(x)}{2}+\cdots,$$ whence $f^{(k)}(x)=0$ for all $k=2,3,\cdots$ . This implies $f$ is linear. However, if $f(x)=kx+b$ with $k\ne 0$ , it will not satisfy the equation $f(x)=\int_{x}^{x+1}f(t)\mathrm{d}t$ , therefore $f$ is a constant. However, in the instructor's note the proof is very complicated. I wonder whether my proof is correct. Thanks in advance.","['integration', 'calculus', 'real-analysis']"
4795595,Doubt on the differentiability of this function over $\mathbb{R}$,"The exercise asks me to show if $f(x) = x^2\vert x \vert $ is differentiabile over $\mathbb{R}$ . Here is my work. $$\lim_{h\to 0} \dfrac{(x+h)^2\vert x+h\vert - x^2\vert x \vert}{h}$$ $$\lim_{h\to 0} \frac{x^2\vert x+h\vert + h^2\vert x+h\vert + 2xh \vert x+h\vert - x^2\vert x\vert}{h}$$ $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert) + h\vert x+h\vert(h + 2x)}{h}$$ $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert)(\vert x+h\vert + \vert x \vert)}{h(\vert x+h\vert + \vert x \vert )} + \lim_{h\to 0} \vert x+h\vert(h + 2x)$$ The second piece goes to $2x \vert x \vert$ hence it's ok. About the first one: $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert^2 - \vert x \vert ^2)}{h(\vert x+h\vert + \vert x \vert)} = \lim_{h\to 0} \frac{x^2(h + 2x)}{\vert x+h\vert + \vert x \vert}$$ Here is where I got stuck, because I don't know how to proceed exactly between those two ways: I go on, finding $\lim_{h\to 0} x^2 \frac{2x}{\vert 2x \vert}$ hence the problem is at $x = 0$ , even if there isn't actually a problem, hence it's differentiable over $\mathbb{R}$ I calculate both the limits for $h\to 0^+$ and $h\to 0^-$ , noticing the only problem occurs at $x = 0$ , but again the $x$ simplify. Seems like in both cases I get $x^2 \text{sgn}(x)$ . Which by the way completes the right derivative which is $2x\vert x \vert + x^2 \text{sgn}(x)$ Can you tell me if I'm wrong, where and how to remedy? Thank you so much!","['calculus', 'derivatives', 'analysis']"
4795610,"Prove $\sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3,$ when $a+b+c=3.$","Problem. If $a,b,c\ge 0: ab+bc+ca>0$ and $a+b+c=3,$ prove that $$\sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3.$$ It was here. Equality holds at $a=b=c=1$ and $abc=0.$ I've tried to use AM-GM but the following inequality is not true $$\sqrt{\frac{a+4bc}{4a+bc}}\sqrt{\frac{b+4ca}{4b+ac}}\sqrt{\frac{c+4ab}{4c+ab}}\le 1.$$ I hope Isolated fudging method helps. Indeed, we'll prove $$\sum_{cyc}\sqrt{\frac{\dfrac{a(a+b+c)}{3}+4bc}{\dfrac{4a(a+b+c)}{3}+bc}}\ge 3.$$ Or $$\sum_{cyc}\sqrt{\frac{a^2+ab+12bc+ca}{4a^2+4ab+3bc+4ca}}\ge 3.$$ I failed to use it. Hope you give me a hint to kill this problem. Thank you.","['inequality', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'substitution', 'convexity-inequality']"
4795654,Show that the least square estimator $\hat \beta$ for $\beta$ can be written as $\hat \beta=V D^{−1}U^TY$.,"Consider a linear model $Y=X\beta+\varepsilon$ , where $Y,\varepsilon \in \Bbb R^n,\beta\in \Bbb R^p$ and with model matrix $X \in \Bbb R^{n×p}$ of full rank, $n, p\in 
\Bbb N$ with $1\lt p\le n$ . Moreover consider the singular value decomposition of the model matrix X, i.e. $$\DeclareMathOperator{\diag}{diag}
X=UDV^T,
$$ where $U$ and $D$ is $n\times p$ and a $p\times p$ diagonal matrices and $V$ is a $p\times p$ matrix. Notes The diagonal elements of $D=\diag(\lambda_1,\lambda_2,\dots,\lambda_p)$ are the positive square roots of the eigenvalues of $X^TX$ or $XX^T$ and $U$ and $V$ contain normalized eigenvectors of $XX^T$ and $X^TX$ respectively, i.e. $ U^TU=I,V^TV=I.$ We moreover we assume that $\lambda_1\ge\lambda_2\ge \ldots\ge\lambda_p$ . Problem . Show that the least square estimator $\hat \beta$ for $\beta$ can be written as $\hat \beta=V D^{−1}U^TY$ . An alternative estimator for $\beta$ is given by $\widetilde\beta:=V D^{−1}_* U^TY$ , where $D^{−1}_*=\diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ for some $1\le k \lt p$ .
( Note :  such an estimator can e.g. be useful if some covariates are highly correlated and $X^TX$ is close to singular.) Answer : to show that the least square estimator $\hat \beta$ equals $V D^{−1}U^TY$ , we start with the linear model $Y=X\beta+\varepsilon$ Using the singular value decomposition (SVD) of the model matrix $X$ , $X=UDV^T$ we can rewrite the model as $$
Y=(UDV^T)\beta+\varepsilon.
$$ Multiplying both sides by the transpose of $U$ , we get, $$
U^TY=(U^TUDV^T)\beta+U^T\varepsilon.
$$ Since $U^TU = I$ i.e. it is the identity matrix, the term $(U^TUDV^T)$ simplifies to $DV^T\beta$ . Therefore, the equation becomes $$
U^TY=(DV^T)\beta+U^T\varepsilon
$$ To solve for $\beta$ , we multiply both sides by $V$ and get $$
VU^TY = V(DV^T)\beta+ VU^T\varepsilon.
$$ Simplifying further, since $V^TV = I$ , we have $$
VU^TY = D \beta+ VU^T\varepsilon.
$$ Subtracting $VU^T\varepsilon$ from both sides, we get $$
VU^TY - VU^T\varepsilon = D\beta.
$$ Combining the terms on the left side, we have $$
V(U^TY - U^T\varepsilon) = D\beta.
$$ Since $E$ is the vector of errors, $U^T\varepsilon = 0$ , so the equation simplifies to $VU^TY = D\beta$ .
Therefore, the least squares estimator $\beta$ equals $VD^{-1}U^TY$ . My question . Am I doing it correctly? I just need a solution verification.","['linear-regression', 'statistics', 'solution-verification']"
4795660,Investigate whether $\widetilde\beta$ is an unbiased estimator for $\beta$,"Consider a linear model $Y=X\beta+\varepsilon$ , where $Y,\varepsilon \in \Bbb R^n,\beta\in \Bbb R^p$ and with model matrix $X \in \Bbb R^{n×p}$ of full rank, $n, p\in 
\Bbb N$ with $1\lt p\le n$ .Consider further the singular value decomposition of the model matrix X, $$X=UDV^T$$ , where U is $n×p$ , D is a $p×p$ diagonal matrix and V is $p×p$ .(Note: The diagonal elements of $D=diag(\lambda_1,\lambda_2,\ldots,\lambda_p)$ are the positive square roots of the eigenvalues of $X^TX$ or $XX^T$ and U and V contain normalized eigenvectors of $XX^T$ and $X^TX$ respectively, $i.e.U^TU=I,V^TV=I.$ We assume $\lambda_1\ge\lambda_2\ge\ldots\ge\lambda_p.)$ Show that the least square estimator $\hat \beta$ for $\beta$ can be written as $\hat \beta=V D^{−1}U^TY$ . An alternative estimator for $\beta$ is given by $\widetilde\beta:=V D^{−1}_* U^TY$ , where $D^{−1}_*=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ for some $1\le k \lt p$ . (Note:  Such an estimator can e.g. be useful if some covariates are highly correlated and $X^TX$ is close to singular.) Investigate  whether $\widetilde\beta$ is  an  unbiased  estimator  for $\beta$ .   If  applicable,  calculate  its bias. Answer:2)To investigate whether the alternative estimator $\beta$ equals $VD^{-1}U^TY$ is unbiased, we need to check whether its expected value equals the true parameter vector $ẞ$ .Taking the expectation of $\beta$ , we have $\Bbb E (\widetilde{\beta}) = \Bbb E(VD^{-1}_* U^TY)$ . Since $D^{-1}_* $ is a diagonal matrix with non-zero elements only on the first k diagonal positions, we can write $D=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ Therefore, $\widetilde\beta =(VD^{-1}_* U^TY)$ can be written as $\widetilde\beta = Vdiag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ . Expanding this expression, we have $\widetilde \beta = (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ ,where $V_1, V_2,\ldots,V_p$ represent the columns of V.
Taking the expectation, we have, $\Bbb E (\widetilde{\beta})=\Bbb E (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ .
Since the expectation operator is linear, we can move it inside the product: $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
\Bbb Ediag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ .Since the diagonal elements of $D^{-1}_*$ only have non-zero values on the first k positions, $\Bbb E D^{-1}_* = diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ Therefore, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TY)$ . By the properties of the expectation operator, $\Bbb E U^TY=U^T\Bbb EY$ . But $\Bbb E (Y) = X ẞ$ , so $\Bbb E U^TY = U^T X ẞ$ .Therefore, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TX ẞ)$ . Since the matrix product diag equals zero for any matrix X, except for the first k positions, we have, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 0 $ . In  other words, the alternative estimator $\widetilde{\beta}$ is unbiased and its expected value is equal to the true parameter vector ẞ. i just try it but i am not sure so need help for the solution verification.","['regression', 'statistics', 'solution-verification', 'linear-regression']"
4795670,"In $\triangle ABC$, $AD$ is the altitude and $CE$ is the bisector. Find $\angle EDB$ if $\angle CEA = 45^\circ$.","Recently I took part in a mathematics Olympiad. There were 25 questions for 2 hours, and I couldn’t solve this problem, I’ll be glad if you help me with this: In $\triangle ABC$ , $AD$ is the altitude and $CE$ is the bisector. Find $\angle EDB$ if $\angle CEA = 45^\circ$ . this is 9 class olympic, task ball is 1.5","['contest-math', 'triangles', 'geometry']"
4795710,"On $\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx$","It seems that $$\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}.$$ Below I present a proof, but there is one problem with that ""proof"". First we use the pentagonal number theorem: $$I=\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\int_0^1 \sum_{k=-\infty}^\infty (-1)^k x^{\frac{k(3k-1)}{2}}\, dx,$$ then interchange integral and series to obtain $$I=2\sum_{k=-\infty}^\infty \frac{(-1)^k}{3k^2-k+2}.$$ Then $$I=-1+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2-k+2}+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2+k+2}$$ and we use partial fraction decomposition: $$I=-1+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k-\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k-\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right)+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k+\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k+\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right).$$ Now $$\Phi (z,s,a)=\sum_{k=0}^\infty \frac{z^k}{(a+k)^s},\quad \Re s\gt 1, |z|\ge 1.$$ If we suppose that $s$ is a positive integer then $a\notin -\mathbb{N}$ . If we suppose that $z\notin [1,\infty)$ , then the Lerch transcendent has the following integral representation: $$\Phi (z,s,a)=\frac{1}{\Gamma (s)}\int_0^\infty \frac{x^{s-1}e^{-ax}}{1-ze^{-x}}\, dx,\quad \Re s\gt 0,\Re a\gt 0.$$ Here comes the important part: If we ignore that $\Re s\gt 1$ in the series definition of $\Phi$ and if we ignore that $\Re a\gt 0$ in the integral representation, then we can write $I$ as $$\begin{align}I&=-1+\frac{2i}{\sqrt{23}}\left(\Phi \left(-1,1,-\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,-\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)+\Phi\left(-1,1,\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)\right)\\&=-1+\frac{2i}{\sqrt{23}}\int_0^\infty \frac{1}{1+e^{-x}}\left(e^{\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}+e^{-\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{-\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}\right)\, dx\\&=-1+\frac{8}{\sqrt{23}}\int_0^\infty \frac{\sin\frac{\sqrt{23}x}{6}\cosh\frac{x}{6}}{1+e^{-x}}\, dx.\end{align}$$ By utilizing Laplace transforms, this can be rewritten to $$I=\frac{4}{\sqrt{23}}\int_{-\infty}^\infty \frac{\sin\frac{\sqrt{23}x}{6}}{1+e^{-x}}e^{\frac{x}{6}}\, dx.$$ Then a routine application of the residue theorem yields $$I=\frac{4\pi}{\sqrt{23}\sinh \sqrt{23}\pi}\sum_{k=0}^5 \cosh\left(\frac{\sqrt{23}}{6}(2k+1)\pi-\sqrt{23}\pi\right)e^{(2k+1)\frac{i\pi}{6}},$$ which, after using Euler's formula, gives $$I=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}.$$ Question How to make the proof rigorous? In other words: notice how we ignored two conditions when using the Lerch transcendent. How can this be patched? As a reference for the Lerch transcendent, I used https://dlmf.nist.gov/25.14","['integration', 'definite-integrals', 'complex-analysis', 'closed-form', 'sequences-and-series']"
4795746,"Prove $\,_3F_2\left ( \frac12,\frac12,1;\frac34,\frac34;-\frac18 \right ) =\frac23+\frac{\Gamma\left ( \frac34 \right )^2}{3\sqrt{\pi} }$","With some experiments, I discover an identity, which eventually evaluates to $$
\,_3F_2\left ( \frac12,\frac12,1;\frac34,\frac34;-\frac18 \right )
=\frac23+\frac{\Gamma\left ( \frac34 \right )^2}{3\sqrt{\pi} }.
$$ This is special for the factor $-1/8$ and its simple result. I hope that there exist some ways we could attack it, and thank for your creative efforts.","['closed-form', 'hypergeometric-function', 'sequences-and-series']"
4795755,Expected rank of champion in a tournament where $P$(higher rank wins)$=\frac35$. Bracket is sorted by rank.,"This problem is from the Simon Marais Mathematics Competition Paper B which was conducted a couple of weeks ago (on October 14, 2023). Problem statement (verbatim): There are $256$ players in a tennis tournament who are ranked from $1$ to $256$ , with $1$ corresponding to the highest rank and $256$ corresponding to the lowest rank. When two players play a match in the tournament, the player whose rank is higher wins
the match with probability $\frac{3}{5}$ . In each round of the tournament, the player with the highest rank plays against the player with the second highest rank, the player with the third highest rank plays against the player with the fourth highest rank, and so on. At the end of the round, the players who win proceed to the next round and the players who lose exit the tournament. After eight rounds, there is one player remaining in the tournament and they are declared the winner. Determine the expected value of the rank of the winner. My attempt : Define a random variable $X$ denoting rank of a player. Define $P(X=n)$ to be the probability that a player with rank $n$ will be the winner. We want to determine $$\mathbb E[X]=\sum_{n=1}^{256} n\cdot P(X=n) $$ There are $2^8$ players in the first round, $2^7$ in the second round after elimination,..., $2^{9-k}$ in the $k$ -th round, and finally $2$ players in the $8$ -th i.e., final round. With this in mind, I attempted to find a probability distribution. $P(X=1)=\left(\frac{3}{5}\right)^8$ because 1 is the highest rank and this player has equal chances to win all the $8$ rounds. $P(X=2)=\left(\frac{2}{5}\right)\left(\frac{3}{5}\right)^7$ because if rank 2 can beat 1 in the first round, in following rounds, the opponents will be lower ranked. For the sake of brevity, let $a:=3/5$ and $b:=2/5$ . I computed like this till $n=10$ on my own speculating on the opponent and rounds for each player. I hope I haven't made mistake anywhere. $\displaystyle \begin{array}{ c|c|c|c|c|c|c|c|c|c|c }
\text{Rank} \ n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
\hline
P( X=n) & a^{8} & a^{7} b^{1} & a^{7} b^{1} & a^{6} b^{2} & a^{7} b^{1} & a^{6} b^{2} & a^{6} b^{2} & a^{5} b^{3} & a^{7} b^{1} & a^{6} b^{2}
\end{array}$ I could generalize that $$P(X=2^k)=a^{8-k}b^k$$ $$P(X=2^k-1)=P(X=2^k-2)=a^{8-k+1}b^{k-1}$$ I am unable to proceed further.","['contest-math', 'expected-value', 'probability']"
4795817,"Why isn't $[0,1]^{\aleph_1}$ isomorphic to $[0,1]$ as a measure space.","I was talking with a professor, and he mentioned that $[0,1]$ (with Lebesgue $\sigma$ -algebra and Lebesgue measure) isn't isomorphic as a measure space to $[0,1]^{\aleph_1}$ (with the product measure). It is intuitive, but I could not prove this, and i haven't been able to find a reference. Does anyone know how to prove it or where to find a proof?","['descriptive-set-theory', 'measure-theory', 'lebesgue-measure', 'ergodic-theory']"
4795818,Proof that the function $f(x)=x(\frac{3}{2}+\lfloor -x^2 \rfloor)$ is injective,"I've been stuck on this one for a while, what I've basically tried is showing that: $$ f(x) = f(y)\ \ \Longrightarrow\ \ x = y$$ Can anyone help?","['ceiling-and-floor-functions', 'functions']"
4795824,How can i solve $\lim_{x\to0}\frac{\log\left(1+(\sin x)^2\right)}{e^x-\cos x}$,"I'm trying to solve this limit: $$ \lim_{x\to0}\frac{\log\left(1+(\sin x)^2\right)}{e^x-\cos x}$$ I tried to do: $$ \lim_{x\to0}\frac{\log\left(1+(\sin x)^2\right)}{e^x-\cos x} = $$ $$ \lim_{x\to0}\frac{\left(o(\sin x)^2+(\sin x)^2\right)}{1+o(x)+x-(1-\frac{x^2}2+o(x^2))} = $$ $$ \lim_{x\to0}\frac{x^2+o(x^2)}{o(x)+\frac{x^2}2+x} = $$ I left out a lot of simplifications in the middle of the calculation but as far as I got it should be right. At this point I looked at the solution to the exercise, which is the following: $$ \lim_{x\to0}\frac{x^2+o(x^2)}{o(x)+x} = $$ $$ \lim_{x\to0}\frac{x+o(x)}{o(1)+1} = 0 $$ I can't understand according to which properties of the small o it is possible to carry out this simplification","['limits', 'calculus']"
4795825,"Can you map the power set of the (set of) positive integers onto $[0,1]$? Show a measure from the power set of the positive integers = $[0,1]$.","Let $\mathbb{Z}^+$ be the set of all positive integers. For any $k \in  \mathbb{Z}^+$ , let function $w(k) = 2^{-k}$ . Moreover, let $E$ denote an element of the power set of $\mathbb{Z}^+$ , that is, $E \in 2^{\mathbb{Z}^+}$ . I define the function $f(E): 2^{\mathbb{Z}^+} \rightarrow [0,1] = \sum_{k \in E} w(k)= \sum_{k \in E} 2^{-k}$ . How can I show that for every element in $[0,1]$ , say, $a$ , there exists an $E$ for which $f(E)=\sum_{k \in E} w(k)=a$ ? Here is my try (edited, based on Arturo's comment below). Could you please briefly check my proof? Every $a \in [0,1]$ has a binary representation (not necessarily unique) of the form $a = \sum_{k=1}^\infty a_k\,w_k = \sum_{k=1}^\infty a_k\,2^{-k}$ , where every $a_k \in \{0,1\}$ . Choosing the right representation, for all $a \in [0,1]$ , there exists a set $E_a=\{k \in \mathbb{Z}^+ : a_k = 1\}$ . $E_a \subset \mathbb{Z}^+$ . As we find an $E_a \in 2^{\mathbb{Z}^+}$ for every $a \in [0,1]$ , we know that $f(E)$ is onto $[0,1]$ . Moreover, $f(E) = \sum_{k=1}^\infty a_k\,2^{-k} = \sum_{k \in E} 2^{-k} = a$ . For a measure, we need two properties. (I) $f(\emptyset)= 0$ . This is the case as $f(\emptyset)=\sum_{k \in \emptyset} w(k)=0$ . (II) $F\left(\cup_{k=1}^\infty E_k \right) = \sum_{k=1}^\infty F(E_k)$ for a disjoint sequence $E_k$ .","['measure-theory', 'solution-verification', 'real-analysis']"
4795864,Proving certain triangle groups are infinite,"[Cross-posted to MathOverflow ] Consider the Von Dyck group $$ G = \langle x,y\mid x^a=y^b=(xy)^c=1\rangle $$ where $a,b,c\ge3$ . Because $G$ is infinite and residually finite, it has an infinite family of finite quotients $\{\overline{G}_n\}$ such that $|\overline{G}_n|\rightarrow\infty$ . I'm wondering if there is an expicit such family that can be constructed, giving a direct proof that $G$ is infinite. Specifically, I'm trying to use Derek Holt's argument (that I copied in this MO answer ), which can be used to show $G$ has quotients in $PSL(2,q)$ for infinitely many primes $q$ . To summarize: Let $q$ be a prime such that $q-1$ is divisible by $2a$ , $2b$ , and $2c$ . Let $\zeta$ be a primitive element of $\mathbb{F}_q^\ast$ , and define $\zeta_t=\zeta^{(p-1)/t}$ , so that $|\zeta_t|=t$ . Then define the matrixes $X,Y\in SL(2,q)$ as \begin{equation*}
\begin{aligned}
X &= \begin{pmatrix}\zeta_{2a} & 1\\ 0 & \zeta_{2a}^{-1}\end{pmatrix}
\end{aligned}\qquad
\begin{aligned}
Y = \begin{pmatrix}\zeta_{2b} & 0\\\lambda & \zeta_{2b}^{-1}\end{pmatrix}
\end{aligned}
\end{equation*} where $\lambda = (\zeta_{2c}-\zeta_{2a}\zeta_{2b})+(\zeta_{2c}^{-1}-\zeta_{2a}^{-1}\zeta_{2b}^{-1})$ . Then the images $\overline{X},\overline{Y}\in PSL(2,q)$ generate a subgroup $\langle \overline{X},\overline{Y}\rangle$ that is a quotient of $G$ . It seems to me in this case we should actually have $\langle \overline{X},\overline{Y}\rangle=PSL(2,q)$ . Is this true, and how hard is the proof? Equivalently (but perhaps easier to show), is it true that $\langle \overline{X},\overline{Y}\rangle$ contains a Sylow $q$ -subgroup of $PSL(2,q)$ ? Note : While I am mostly interested in the specific question of whether $\langle \overline{X},\overline{Y}\rangle=PSL(2,q)$ , there are other ways to exhibit an infinite family $\{\overline{G}_n\}$ . For example, by reducing to the (perhaps more tractable) case where $a,b,c$ are odd primes or $4$ . Or if there are other explicit infinite families -- especially linear -- I'd like to hear about them!","['infinite-groups', 'linear-groups', 'finite-groups', 'combinatorial-group-theory', 'group-theory']"
4795880,How do I develop an understanding of how equations were created? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 months ago . Improve this question Firstly: having a lot of difficulty figuring out how to articulate this question due to lack of general math knowledge.  There are multiple questions posed below, but I feel like if I knew more they could be condensed into a single question and am hoping someone can suggest and edit to the effect of the below.  Thank you in advance for your consideration and assistance on this point! Background I'm taking a course which includes descriptive statistics. In that course they describe the method of calculating covariance and provide that equation.  I find myself wondering - why did they choose to define it as they did (using multiplication instead of addition between the terms in the numerator - not even sure if that description is accurate). My question? Are equations for things like covariance derived from looking at phenomenon and 'cracking the code' of how those phenomenon could be described mathematically via a proof?  Or does one, at some level of mathematical skill, say: I want to model this phenomenon and I want that model to have these features and qualities to its output, therefore, I choose this particular structure to achieve that goal and then I prove that functionality through a proof? If the latter case - what progression of mathematical learning develops that skillset?
Is the same skillset used in both cases? Why I ask If I understand the motivation of the creator of the covariance equation, I could compare it to my own motivation and perhaps come up with a different approach to the same problem that better fits my own goals because maybe our goals are similar but not the same... Thank you again for any advice on how to simplify this..","['descriptive-statistics', 'statistics', 'covariance']"
4795916,Exploring Permutations in 'CORRESPONDENTS' with Three or More Adjacent Identical Letters,"Question: How many distinct permutations of the word ""CORRESPONDENTS"" exist in which at least three same letters are adjacent to each other. EXAMPLE: NN CRERE OOSS PDT ANSWER: $206\times 9!$ according to the test my solution: $$9!\times\begin{pmatrix}5\\3\end{pmatrix}\times\dfrac{8\times 9}{2!\times 2!}+9!\times\begin{pmatrix}5\\4\end{pmatrix}\times \dfrac8{2!}+9!\times\begin{pmatrix}5\\5\end{pmatrix}=201\times9!$$ I start by rearranging the nine different letters. Then, I pick three out of the five identical letters and place them next to their matching ones. After that, I add the rest of the letters to complete the arrangement. I do this for both groups of letters, one with four and the other with five adjacent letters. EDIT: With nine letters in play, we have ten potential spots to place the second 'N.' However, we need to avoid positioning it directly to the left or right of the first 'N.' To account for this, we multiply by (10−2)=8
. We repeat this process for the letter 'S,' which this time yields a total of nine options, as we've already added the second 'N,' creating more available positions. Additionally, the division by 2!
is essential to account for the fact that identical letters are not counted twice, ensuring the permutations are distinct","['combinatorics', 'discrete-mathematics']"
4795964,For every sequence of homotopy groups is there a corresponding manifold?,"Some general questions about homotopy groups: For any sequence $\{G_n\}_n$ does there exist a path-connected topological space $X$ with $\pi_n(X)=G_n$ for all $n$ ? I am aware that the answer is no if we do not impose that $G_n$ is abelian for $n\ge 2$ , so I impose this. Same question as above but for $X$ a manifold? I suspect the answer is probably no for (1) and (2) - I am just unaware of general conditions on such sequences $\{\pi_n(X)\}_n$ that must be satisfied, other than the one I mentioned.","['general-topology', 'homotopy-theory']"
4795985,Convergence in the sense of distribution and weak convergence,"A sequence of function $\{f_n\} \in L^p(\Omega)$ which converges in the sense of distribution need not be convergent in the weak topology of $L^p(\Omega)$ . A classical example being a sequence of mollifiers converging to dirac delta distribution. However, in addition suppose we know that the distributional limit is a function $f \in L^p(\Omega)$ , can we conclude that $f_n$ converges to $f$ weakly in $L^p(\Omega)$ ? A counter example or a clean proof is  greatly appreciated. Thanks in advance.","['weak-convergence', 'functional-analysis', 'analysis']"
4796129,where is the error in fake proof that a gaussian has 0 variance,"Consider the exponential family $$p_\theta(dy) = h(y)\exp(\langle \theta,T(y)\rangle - \Phi(\theta))\mu(dy)$$ with $\theta \in \mathbb R^d$ and the partition function $\Phi(\theta)<\infty$ for all $\theta \in \mathbb R^d$ .
Suppose $\nabla \Phi: (\mathbb R^d,\|\cdot\|_2) \to (\mathbb R^d,\|\cdot\|_2)$ is L-Lipschitz. Fix any $v$ on the $d-1$ dimensional sphere. Then it can be shown $X = \langle v, T(Y)\rangle$ is $2L$ sub-Gaussian. Now, consider the family $$p_\theta(dy) = \exp(-(y-\mu)^2/2)/\sqrt{2\pi}\,dy$$ This is an exponential family with $\theta = [\mu \mid \mu^2]$ and $T(y) = [y\mid -1/2]$ .
But then this implies $$X = \langle e_1,T(Y)\rangle = Y$$ is $2L$ sub-Gaussian for every $L>0$ (since $\Phi(\theta) \equiv \log \sqrt{2\pi}$ ) In a similar vein, by taking $\theta = [\mu \mid \mu^2 \mid -1/2]$ and $T(y) = [y \mid -1/2 \mid y^2]$ , one can deduce the problematic statements that $Y^2$ is 1) sub-Gaussian, and 2) has zero variance. Where is the error?","['statistics', 'fake-proofs', 'probability']"
4796143,Integrating $\int e^{\cos x} \cos(2x+\sin x)dx$ [duplicate],This question already has answers here : 2023 MIT Integration Bee Semifinals #1 Problem 1 $\int e^{\cos x}\cos(2x+\sin x)dx$ (3 answers) Closed 8 months ago . What is the result of this integral? $$\int e^{\cos x} \cos(2x+\sin x)dx$$ I tried to use integration by parts but nothing made sense to me in this given problem. What should we do to solve these kinds of integrals? This is from a high school contest so we cannot use complex integration. Thank you!,"['integration', 'problem-solving', 'contest-math']"
4796160,Are there $n>6$ such that there is exactly one non-abelian group of order $n$ up to isomorphism? This happens with $n=6$ with $S_3$,"In my algebra class, we saw that every non-abelian group of order $6$ is isomorphic to $S_3$ . I am curious as to whether this is true for higher orders. The natural answer seems that there are no such orders, but I don't know how to prove this. Thank you for your time.","['group-theory', 'abstract-algebra', 'finite-groups']"
4796187,"Example of $a_k$ such that $\sum a_k$ doesn't converge but $\lim_{x \rightarrow 1, x< 1} \sum a_k x^k$ exists.","In class we were taught that if $\sum a_k$ converges, then $\lim_{x \rightarrow 1, x < 1} \sum a_k x^k$ exists. The proof is based on the Cauchy's convergence criterion:
if the sum converges, then $\limsup_{n \rightarrow \infty} \sqrt[n]{a_n} \leq 1$ . Is there an example of $a_k$ such that the limit exists but the sum doesn't converge, i.e. is there a counter example to the opposite statement?","['power-series', 'limits', 'sequences-and-series', 'real-analysis']"
4796214,"How do we formalize the derivation of the ideal gas law from Boyle's Law, Charles's Law, and Avogadro's Law?","Given: $V \propto 1/P$ $V \propto T$ $V \propto n$ Prove that: $V \propto \frac{nT}{P}$ Here is the intuitive argument I have: We can see that $V = \langle \text{mystery meat} \rangle  1/P$ $V = \langle \text{mystery meat} \rangle  T$ , and $V = \langle \text{mystery meat} \rangle  n$ but it's possible to interpret it so that the mystery meat in (1) is actually equal to $nT$ , and the mystery meat in (2) is equal to $\frac{n}{P}$ , and the mystery meat in (3) is equal to $\frac{T}{P}$ and so it ends up looking like the blind men touching the elephant. ...But I don't know how to make this intuitive argument formal. How would this be turned into a formal proof?","['algebra-precalculus', 'proof-writing', 'education']"
4796222,Is the optimization of a symmetric function a symmetric solution?,"Under the assumption that $x^2+y^2=c^2$ I minimized the functions $P=x^{-1}+y^{-1}$ and $Q=x^{-2}+y^{-2}$ and for both the solution was $x^2=y^2=c^2/2$ . I noticed that the condition equation and the function to be optimized were both symmetric in $x$ and $y$ . Let $F(x,y)$ and $G(x,y)$ be symmetric functions. If we have the condition that $F(x,y)=C$ and we solve the equation $\dfrac{d}{dx}G(x,y)=0$ will $x=y$ be a solution?","['optimization', 'calculus', 'derivatives']"
