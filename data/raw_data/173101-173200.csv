question_id,title,body,tags
3080006,Inverse of set operations,"$A \cup B \equiv C $ then what is $A$ in terms of $B,C$ ? I tried to use $A\cup B \equiv (A-B)\cup(A \cap B)\cup(B-A) $ to find a similar expression for $A \cap B$ but got nowhere. From the elementary set theory that I did 40 or so years ago I don't recall any material on inverse of set theory operations i.e. union, intersection, complement, difference. Complement is easy as it is it's own inverse $(A^C)^C=A$ not sure if inverse of difference operator is unique, for example, one can use $(A-B)\cup A \equiv A$ to construct one inverse of difference $ -X$ . when doing algebra, inverse operations are the first tricks to learn, but was the topic of inverse operations of set theory ever mentioned?",['elementary-set-theory']
3080023,Hilbert polynomial of the graph and boundedness of Hom scheme?,"Let $X,Y$ be two projective variety over an algebraically closed field, then we know $Hom(X,Y)$ has a scheme structure by considering the embedding as a open subscheme of $Hilb(X\times Y)$ using the graph. We know for each fixed polynomial $P$ , the Hilbert scheme with Hilbert polynomial $P$ is projective hence finite type. And it seems that the graph of any $f:X\longrightarrow Y$ is isomorphic to $X$ hence has the same Hilbert polynomial viewed as a closed subscheme of $X \times Y$ . However, Hom scheme can have infinitely many component (for example consider $End(E)$ where $E$ is an elliptic curve). So where does my intuition goes wrong? If $X,Y$ are both smooth curves, can we decide all of the connected components?",['algebraic-geometry']
3080038,Calculate $\lim\limits_{ x\to \infty} \frac{\ln(x)}{x^a}$ where $ a > 0 $ [duplicate],This question already has answers here : How can I prove that $\log^k(n) = O(n^\epsilon)$? (5 answers) Evaluation $\lim_{n\to \infty}\frac{{\log^k n}}{n^{\epsilon}}$ (5 answers) Closed 5 years ago . I want to calculate $$\lim\limits_{  x\to \infty} \frac{\ln(x)}{x^a}$$ where $ a > 0 $ It looks simple because if $a>0$ then $ x^a $ it grows asymptotically faster than $ \ln(x) $ so $$\lim\limits_{  x\to  \infty} \frac{\ln(x)}{x^a} = 0$$ But I don't know how to formally justify that. I am thinking about something what I was doing in case of sequences: $$\frac{\ln(x+1)}{(x+1)^a} \cdot \frac{x^a}{\ln(x)} $$ But it have no sense because sequences was being considered in $\mathbb N$ but functions like that are considered in $\mathbb R$ I can't use there hospital's rule,"['limits', 'limits-without-lhopital', 'real-analysis']"
3080067,Combinatorics of sums,"Let's say that we have a number S that represents a sum. This sum can be broken down into a sum of terms. I want to calculate how many expressions I can write that represent that sum where terms are in the range from $ 1 $ to $ S $ . Example: $$\begin{align}    
    4 &= 1 + 1 + 1 + 1\\
    4 &= 2 + 1 + 1\\
    4 &= 1 + 2 + 1\\
    4 &= 1 + 1 + 2\\
    4 &= 2 + 2\\
    4 &= 3 + 1\\
    4 &= 1 + 3\\
    4 &= 4
\end{align}
$$ For $S=4$ we have $N=8$ . For $S=3$ , we have $N=4$ Although I figured out that I can calculate that with this formula: $N = 2^{S-1}$ I can't really tell why is that. I can count them for few sums and see the rule but is there a better way to explain this?","['combinations', 'combinatorics']"
3080121,$f+f'+f''\geq0$ implies that $f$ has a lower bound,"Let $f\in C^2(a,b)$ such that $f+f'+f''\geq0$ . Prove that $f$ has a lower bound. $My\quad Attempt$ $1.\quad$ Suppose that $f$ has no lower bound at x=b, so there is a sequence $\{x_n\}$ which converges to b( $\lim_{n\to\infty}{x_n}=b$ ), and $f'(x_n)<0,f(x_n)<-n$ . $2.\quad\forall k\in \mathbb N,\exists N\in\mathbb N,n>N,x_n>x_k$ , then prove that $$\int_{\{x|x\in(x_k,x_n)\land f(x)>0\}}{}f^2(x)dx\leq C.$$ $3.\quad$ Prove: $$0\leq\int_{x_k}^{x_n}{(f+f'+f'')}dx\leq0+f(x_k)-f(x_n)+C\to-\infty$$ $\quad\quad\quad\quad\quad\quad\quad$ This contradicts the problem So that's my idea, but I can't do it from step 2. And my idea might be wrong. Edit in 2019/2/16 I solve the question if $q=0$ . There are some Chinese characters in my answer. I hope it doesn't bother you.","['contest-math', 'real-analysis']"
3080127,Find the sum $\sqrt{5+\sqrt{11+\sqrt{19+\sqrt{29+\sqrt{41+\cdots}}}}}$,"Okay so this can be written as $$\sqrt{5+\sqrt{(5+6)+\sqrt{(5+6+8)+\sqrt{(5+6+8+10)+\sqrt{(5+6+8+10+12)\cdots}}}}}$$ Putting it as $y$ and squaring both sides doesn't seem to help, and I don't know what else can be done.","['analysis', 'sequences-and-series']"
3080164,Darboux continuity of the function $f(x) = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}}$,"Let $f : [0,1] \to [0,1]$ be a function that assigns to each $x \in [0,1]$ the following value: $$ x = 0.x_{1}x_{2}x_{3} \ \ ... \hspace{0.3cm} \text{be the binary expansion of }x $$ define $$ f(x): = \limsup_{n \to \infty} \frac{(x_{1}+...+x_{n})^{2}}{n^{2}}$$ Prove that $f$ is Darboux continuous but not continuous. Can someone give hint on this problem?
Thank you.","['binary', 'analysis', 'real-analysis', 'continuity', 'average']"
3080173,Calculate $ a_n\:=\:n^3\left(\sqrt{n^2+\sqrt{n^4+1}}-\sqrt{2}n\right) $,"I struggle for a while solving limit of this chain: $
 a_n\:=\:n^3\left(\sqrt{n^2+\sqrt{n^4+1}}-\sqrt{2}n\right)
$ I know from WolframAlpha result will be $ \frac{1}{4\sqrt{2}} $ , but step-by-step solution is overcomplicated(28 steps). Usually I solve limits like this by property $ \left(a-b\right)\left(a+b\right)=a^2-b^2 $ I made this far: $$ 
\lim _{n\to \infty }\left(n^3\left(\sqrt{n^2+\sqrt{n^{4\:}+1}}-\sqrt{2}n\right)\right)
=n^3\sqrt{n^2+\sqrt{n^4+1}}-n^4\sqrt{2}=\frac{\left(n^3\sqrt{n^2+\sqrt{n^4+1}}-n^4\sqrt{2}\right)\left(n^3\sqrt{n^2+\sqrt{n^4+1}}+n^4\sqrt{2}\right)}{\left(n^3\sqrt{n^2+\sqrt{n^4+1}}+n^4\sqrt{2}\right)} = \frac{-n^8+n^6\sqrt{n^4+1}}{n^3\sqrt{n^2+\sqrt{n^4+1}}+n^4\sqrt{2}}$$ I will appreciate every help. Thank you",['limits']
3080181,Using Partial Limit,"$$\lim_{x \to 0} \cos(\pi/2\cos(x))/x^2$$ I tried to evaluate the limit this way, $$\lim_{x \to 0} \cos(\pi/2\cdot1)/x^2$$ since $\cos0=1$ $$\lim_{x \to 0} \cos(\pi/2\cdot1)/x^2=\lim_{x \to 0} 0/x^2$$ Now apply L'Hospital's Rule twice, $$\lim_{x \to 0} 0/2(x)=\lim_{x \to 0} 0/2=0$$ So,this way the answer is zero. Can you please explain where am I doing wrong? I will be thankful for help!","['limits', 'calculus']"
3080188,Prove all roots of $p_n(x)-x$ are real and distinct,"Given a polynomial series $\{p_n(x)\}_{n=1}^{\infty}$ in $\mathbb{R}[X]$ with initial value $p_1(x)=x^2-2$ . And $p_k(x)=p_1(p_{k-1}(x))=p_{k-1}(x)^2-2,\;k=2,3,\cdots$ . Prove that for each integer $n$ , all roots of $p_n(x)-x$ are real and distinct. This is an problem in my linear algebra textbook.
I tried to figure out the relation of the roots between adjacent polynomial, but I couldn't find any useful result. Also I thought if it could be solved by induction, but it seems impracticable.","['linear-algebra', 'polynomials', 'roots']"
3080203,"Help bounding a ""norm""","In Weak Convergence and Stochastic Processes, the authors introduce the following notation: $$\|\xi\|_{2,1} = \int_0^\infty \sqrt{P(\xi > x)}\,\mathrm dx$$ They then admit that this is technically not a norm but is equivalent to one. To substantiate this, exercise 1 in the chapter is the following: Show that for any $r > 2$ and random variable $\xi$ , $$\frac12\|\xi\|_2 \leq \|\xi\|_{2,1} \leq \frac{r}{r-2}\|\xi\|_r$$ The first inequality can be shown as follows: $$\begin{align*}\|\xi\|_2^2 = \int P(\xi > x^2)\,\mathrm dx &= \int P(\xi > x)2x\,\mathrm dx \\ &\leq \int \sqrt{P(\xi > x)}\,\mathrm dx\cdot \sup_x 2x\sqrt{P(\xi>x)} \\ &\leq 2\|\xi\|_{2,1}\|\xi\|_2 \end{align*} $$ where the first inequality is Hölder and the second follows by Markov. However, I am having considerably more trouble showing the second part. My intuition has been to use Hölder (or perhaps reverse Hölder) again, but all of my efforts to do so lead to dead ends. For example, I have considered $$\|\xi\|_{2,1} = \frac{1}{(1-r)^{1/r}}\int P(\xi > x)^{1/r}(r-1)^{1/r}x\cdot \left(P(\xi>x)^{1/2-1/r}\right)\frac{1}{x} \,\mathrm dx$$ which can then be bounded by $\|\xi\|_r$ times a nasty integral by the use of Hölder's inequality. I have also considered trying something like a reverse Hölder inequality on an explicit expression for $\|\xi\|_r^r$ , also with little progress. Any thoughts on where to proceed? Is Hölder even the right approach here?","['inequality', 'lp-spaces', 'integral-inequality', 'probability']"
3080253,"If $f(\frac{x+y}{x-y})=\frac{f(x)+f(y)}{f(x)-f(y)}$, which of the following statement is correct","If $f(\frac{x+y}{x-y})=\frac{f(x)+f(y)}{f(x)-f(y)}$ for $x \ne y$ , $x$ and $y$ are integer. Which of the following statement is correct : (1) $f(0)=0$ (2) $f(1)=1$ (3) $f(-x)=-f(x)$ (4) $f(-x)=f(x)$ I thinks it's (1),(2), and (3) are the correct statement. But I don't know if I'm doing it in right way or not.
Here's my attempt : For (1) : $\frac{x+y}{x-y}=0\\
x=-y\\
f(0)=\frac{f(x)+f(-y)}{f(x)-f(-y)} 
\\f(0)=0$ (I'm not sure about this part) For (2) : $\frac{x+y}{x-y}=1\\
x+y=x-y\\
-2y=0\\
y=0\\
f(1)=\frac{f(x)+f(0)}{f(x)-f(0)} 
\\f(1)=1$ For (3) : $\frac{x+y}{x-y}=-x\\
x+y=xy-x^2\\ ??$ (I'm stuck from this part)","['functional-equations', 'functions']"
3080305,Why can't I split combinations/events?,"I was recently struggling with a problem that read as so: A club has 30 members work in business and 30 members that are professors. In how many ways can a committee of 8 be selected that has at least 3 in business and at least 3 professors? For my answer, I first took care of the requirements, then grouped the rest together to get $$\binom{30}{3}\binom{30}{3}\binom{54}{2}$$ Which ended up being wildly wrong from the correct answer which was $$\binom{30}{5}\binom{30}{3}*2 +\binom{30}{4}\binom{30}{4}$$ I once again tried splitting the last combination by case to get $$\binom{30}{3}\binom{30}{3}(\binom{27}{1}\binom{27}{1}+\binom{27}{2}+\binom{27}{2})$$ But this just turned out to be the same as my previous answer. After doing some research, I came to the conclusion that my answers were larger than the correct one because I was splitting the event into many smaller sections, and in doing so, overcounting cases, which is why $$\binom{10}{3}\neq \binom{10}{2}\binom{8}1$$ I can accept this as a general principle, as the numbers aren't equal. However, it doesn't really logically make sense to me. What am I actually overcounting by splitting up a combination such as $\binom{10}{3}$ into $\binom{10}{1}\binom{9}{1}\binom{8}{1}$ ? Why does splitting and adding events together have no effect?",['combinatorics']
3080320,Proving that the conditional entropy of a probability measure is concave,"Let $\mu$ be a probability measure on $\mathcal{X}$ and let $\mathcal{E}, \mathcal{F}$ be countable partitions of the space. Define the entropy of $\mu$ with respect to the partition $\mathcal{E}$ as $$
H(\mu, \mathcal{E}) = - \sum_{E \in \mathcal{E}} \mu(E) \log \mu(E)
$$ and the conditional entropy as $$
H(\mu, \mathcal{E} | \mathcal{F}) = \sum_{F \in \mathcal{F}} \mu(F) H(\mu_F, \mathcal{E}) = - \sum_{F \in \mathcal{F}} \sum_{E \in \mathcal{E}} \mu_{|F}(E) \log (\dfrac{1}{\mu(F)} \mu_{|F}(E)),
$$ where $\mu_{|F}(\cdot) = \mu(\cdot \cap F)$ and $\mu_F$ is the normalized restriction of $\mu$ on $F \in \mathcal{F}$ . Now it is easy to see by concavity of $x \mapsto -x \log x$ that the entropy $\mu \mapsto H(\mu, \mathcal{E})$ is a concave function, but what about the conditional entropy? How can I prove its concavity? The normalizing coefficient $\dfrac{1}{\mu(F)}$ seems to make the function slightly more complicated.","['measure-theory', 'ergodic-theory', 'probability-theory']"
3080339,Not possible to find non-zero terms of series expansion?,"I've been asked to compute the first 3 nonzero terms of a power series expansion about x=0 for two linearly independent solutions to the ODE: $$(1+x^3)y''- 6xy =0 $$ I have tried to solve this many different ways and continue to get the same solution, which does not allow me to find three nonzero terms from two linearly independent solutions I have used $$y(x) = \sum_{n=0}^\infty a_nx^n $$ $$ y'(x)=\sum_{n=1}^\infty a_nnx^{n-1} $$ $$ y''(x)=\sum_{n=2}^\infty a_nn(n-1)x^{n-2} $$ to obtain the series form $$ \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + x^3\sum_{n=2}^\infty a_nn(n-1)x^{n-2} -6x\sum_{n=0}^\infty a_nx^n$$ Add in x terms: $$ \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + \sum_{n=2}^\infty a_nn(n-1)x^{n+1} -\sum_{n=0}^\infty 6a_nx^{n+1}$$ Set all the powers of x equal to n: $$ \sum_{n=0}^\infty a_{n+2}(n+2)(n+1)x^{n} + \sum_{n=3}^\infty a_{n-1}(n-1)(n-2)x^{n} -\sum_{n=1}^\infty 6a_{n-1}x^{n}$$ Peel off terms to have all series start at n=3: $$ 2a_2x^0+6a_3x^1-6a_0x^1+12a_4x^2-6a_1x^2$$ $$+$$ $$\sum_{n=3}^\infty [a_{n+2}(n+2)(n+1)+a_{n-1}(n-1)(n-2)-6a_{n-1}]x^n=0$$ From this I have deduced: $ x^0 : a_2=0 $ $ x^1 : a_3=a_0 $ $ x^2 : a_4=\frac{a_1}{2} $ $ x^n, n\geq3 : $ \begin{align}
 & a_{n+2}=-\frac{a_{n-1}(n^2-3n+2-6)}{(n+2)(n+1)}\\
 & = -\frac{a_{n-1}(n+1)(n-4)}{(n+1)(n+2)}\\
 & = -\frac{a_{n-1}(n-4)}{n+2}\\
\end{align} Using the recursion equation above I obtained these terms: $n=3 : $ $$ a_5=\frac{a_2}{5}=0 $$ $n=4 : $ $$ a_6=a_3(4-4)=0 $$ $n=5 : $ $$ a_7=\frac{-a_4}{7}=\frac{-a_1}{14} $$ $n=6 : $ $$ a_8=\frac{-2a_5}{5}=\frac{-a_2}{20}=0$$ $n=7 : $ $$ a_9=\frac{-3a_6}{9}=0$$ $n=8 : $ $$ a_10=\frac{-4a_7}{12}=\frac{a_4}{21}=\frac{a_1}{42} $$ $n=9 : $ $$ a_11=\frac{-5a_8}{11}=\frac{a_2}{44}=0 $$ $n=10 : $ $$ a_12=\frac{-6a_9}{12}=\frac{-a_2}{2}=0 $$ $n=11 : $ $$ a_13=\frac{-7a_10}{13}=\frac{-7a_4}{273}=\frac{-a_1}{78} $$ So, every second and third term equal zer0. 
My solution is: $$ y_1(x)=a_0(1+x^3) $$ $$ y_2(x)=a_1\big(x+\frac{x^4}{2}-\frac{x^7}{14}+\frac{x^{10}}{42} - ...) $$ Can someone please tell me what I am doing wrong here? I cannot come up with three non zero terms from $y_1$ as there are only two terms, and everything else is zero. When I try to apply the ratio test to this series solution I get inconclusive results as well which further makes me think my solution is incorrect... Any help or advice would be very greatly appreciated.","['power-series', 'convergence-divergence', 'taylor-expansion', 'ordinary-differential-equations']"
3080350,Convergence of $\sigma-$algebra for converging stopping time,"Given a filtration, ${\mathcal{F}_t},t\in[0,\infty).$ Let $T_n$ be a sequence of stopping time that converges to $T$ and $T_n\le T_{n+1}.$ We have correpsonding $\sigma-$ algebra, ${\mathcal{F}_{T_n}}$ and $\mathcal{F}_T.$ Now, denote $\mathcal{F}'=\sigma(\mathcal{F}_{T_n}:n=1,2,\cdots),$ i.e., the $\sigma-$ algebra generated by all $\mathcal{F}_{T_n}$ . Q: Will $\mathcal{F}'=\mathcal{F}_T$ holds? I believe the condition that the filtration is left continuous is needed, since one can take $T_n,T$ to be constant. Let's assume that. My try: That $\mathcal{F}'\subset\mathcal{F}_T$ is trivial, since all $\mathcal{F}_{T_n}\subset\mathcal{F}_T$ by $T_n\le T$ . We left to show $\mathcal{F}_T\subset\mathcal{F}'.$ By definition, $A\in\mathcal{F}_T$ is equivalent to $A\cap\{T\le t\}\in\mathcal{F}_t$ for any $t$ . How can one deduce from here that $A\in \mathcal{F}'.$ Got stuck here. In Approximation of a unbounded stopping time and convergence of respective $\sigma$ -algebras , saz gives an approach for discrete time. For $A\in\mathcal{F}_T, A$ can be decomposed as, $$A = \cup_{n=1}^{\infty} (A\cap\{T\le n\})\cup(A\cap\{T=\infty\}) = \cup_{n=1}^{\infty} A_n\cup A_\infty.$$ Then show $A_n\in\mathcal{F}_{T_n}.$ We have $A_n\in\mathcal{F}_n, $ we require to show $A_n\cap\{T_n\le t\}\in\mathcal{F_t},$ this is so if $t\ge n$ since $\{T_n\le t\}\in\mathcal{F}_t$ . But how about $t<n?$ I was stuck here. Update:
One can define, $\mathcal{F}_{S-}=$ the $\sigma-$ algebra generated by $\mathcal{F}_{0+}=\cap_{s>0}\mathcal{F}_s$ and the sets $\{S>t\}\cap\mathcal{F_t}.$ then when $S=s$ is a constant, $\mathcal{F}_{S-}=\sigma(\mathcal{F}_u:u<s)=\mathcal{F}_{s-}.$ So this is simply the generalization of left limit. Then one can prove that, $$\mathcal{F}_{T-}=\mathcal{F}'.$$ So now, the question may become to show, $$ \mathcal{F}_{T-}=\mathcal{F}_{T} .$$ Any hint is appreciated!","['measure-theory', 'stopping-times', 'probability-theory']"
3080359,Find all three digit numbers which are divisible by groups of its digits [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How can I find all three-digit numbers which: Do not contain a $0$ digit Have different digits Are divisible by below described groups of its own digits The number passing first two conditions should be divisible by two-digit group of its own digits, which are made by omitting one of the number's digits. For example: number = $132$ It has only non-zero digits It has different digits And it should be divisible by $13$ , $12$ , and $32$ . (omitting one digit) Thanks a lot in advance for helping me finding these!","['combinatorics', 'divisibility', 'decimal-expansion']"
3080378,Is there a trick to solve $\int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)]$?,"I found this question in some old exam: Find 4 reals $a, x_1, x_2, x_3$ such that the equality $$\int_{-1}^1 \frac{P(t)}{\sqrt{1-t^2}}{\rm d}t=a[P(x_1)+P(x_2)+P(x_3)]$$ is true for all polynomial with degree less or equal 3. My problem is not to compute these reals for a particular example but rather I didn't understand the idea behind this equality. In fact there is a similar question in this exam to prove that $\int_{-1}^1 \frac{f(t)}{\sqrt{1-t^2}}{\rm d}t=\frac{\pi}{3}[f(x_1)+f(x_2)+f(x_3)]$ for all polynomial $f$ with degree less or equal 5; this what make me sure that there is a trick behind these issues. Is there any explanation?","['integration', 'polynomials', 'real-analysis']"
3080385,How do I construct this triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I was trying to draw the following triangle in latex tikz and I just could not find a way to do it with respect to the given conditions. Is it possible to construct it without trigonometry or analytical geometry? Is it possible to show that it cannot be done without trigonometry or analytical geometry? (Regrettably) I tried to do it via solving some trigonometrical equations, then I realized I am getting nowhere. So, how would I approach with trigonometry? Note 1: In the actual drawing, $c=6$ , but that does not matter, obviously.
Note 2: I am not looking for LaTeX help.","['triangles', 'trigonometry', 'geometry']"
3080459,Nine-point circle - proof using plane geometry,"I am taking a course in multivariable calculus this year & I thought it would be a good idea to brush up plane and solid geometry. I would like to prove that, for any given triangle, there is a unique circle that passes throught the three midpoints of sides, the feet of the altitudes, and the middle points on the segment joining the feet of the perpendicular and the orthocenter. Could you please give me any hints, that would lead to the correct proof? For an example of the type of work I've done ... On some results related to triangle centers, I was able to prove that the centroid $G$ divides the line joining the orthocenter $H$ and the circumcenter $K$ in the ratio $2:1$ . I wasn't sure how to prove that $H$ , $G$ , $K$ are collinear. I learnt from Kiselev's Geometry book about how to prove this result. The idea is that the circumcenter of the original triangle is the orthocenter of the medial triangle $H^\prime$ . And the medial triangle reflected about the centroid $G$ , followed by dilation of a factor $2:1$ results in the original triangle. Thus, the center $H^\prime$ moves to the point $H$ . Both triangles share the centroid $G$ . So, $H$ , $G$ , $K$ are collinear. Cheers,
Quasar.","['euclidean-geometry', 'triangles', 'geometric-transformation', 'geometry']"
3080508,Solving $(D^5-D)y = 8\sin x$ using operator methods,"This question is similar to this link but here it involves $\sin x$ and it creates a problem. I tried writing $\sin x = \Im (e^{ix})$ but that doesnt help: $$(D^5-D) y = 8\Im (e^{ix})$$ Now $(D^5-D)$ has factor $D=i$ so we need to factor that out and apply it on right side: $$(D-1)(D+i)(D+1)D y=\frac{1}{D-i}(\Im(e^{ix}))\\
\implies y = \frac{8}{(i-1)(i+1)(2i)(i)}\Im(e^{ix}\int e^{ix} e^{-ix} dx)$$ which gives $y_p = 2x\sin x$ . Another method would have been assuming $y_p = x(A\cos x + B\sin x)$ but that too would have been lengthy Is the method in this answer correct, because it is using complex numbers loosely. Is this operator method?","['calculus', 'ordinary-differential-equations']"
3080531,A question about converging derivatives,"Suppose $f \in C^{\infty}(\mathbb{R})$ and $\forall x \in \mathbb{R} \text{ } \exists \lim_{n \to \infty} f^{(n)}(x) = g(x)$ . Does this mean that $$
\exists a \in \mathbb{R} \forall x \in \mathbb{R} \text{ } g(x) = ae^x?
$$ If $f^{(n)}$ converge to $g$ uniformly, then it does, as $$
\forall x_0 \in \mathbb{R} \implies 
\begin{split}
g’(x_0) &= \lim_{x \to x_0} \frac{g(x) - g(x_0)}{x - x_0} \\
&= \lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\
&=\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\
& = \lim_{n \to \infty} f^{(n + 1)}(x_0) \\
& = \lim_{n \to \infty} f^{(n)}(x_0) \\
& = g(x_0)
\end{split}
$$ and all solutions of a differential equation $g’(x) = g(x)$ have the form $ae^x$ for some $a \in \mathbb{R}$ . However, this proof does not work in case, when $f^{(n)}$ does not converge uniformly, as in this case $$
\lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}\text{ not necessarily equals }\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}.$$ And I do not know how to proceed in this case. Any help will be appreciated.","['ordinary-differential-equations', 'real-analysis', 'calculus', 'limits', 'derivatives']"
3080572,"Finding $\phi^{-1}({id_{P(\mathbb{N})}})$, where $\phi:(\mathbb{N}\to\mathbb{N})\to(P(\mathbb{N})\to P(\mathbb{N}))$ and $\phi(f)(A) = f^{-1}(A)$","I have function $\varphi : (\mathbb{N} \to \mathbb{N}) \to (P(\mathbb{N}) \to P(\mathbb{N}))$ and $\varphi(f)(A) = f^{-1}(A)$ .
I have to find $\varphi^{-1}({id_{P(\mathbb{N})}})$ , and I don't have any idea how to find it. Any tips?
Thanks a lot.","['elementary-set-theory', 'functions', 'inverse-function']"
3080609,Probability of $2$ pairs on $5$ dice,"We throw $5$ dice. What is the probability of getting $2$ pairs ? My solution says it is $$\frac{6\cdot 5\cdot 4\cdot 5! }{6^5\cdot 2\cdot 2!\cdot 2!},$$ where as for me it's $$\frac{6\cdot 5\cdot 4\cdot 5!}{6^5\cdot 2!\cdot 2!}.$$ I do as follows: Throwing $5$ dice is the same thing as throwing one die $5$ times. What we want is $AABBC$ . We have $6$ possibilities for $A$ , $5$ possibilities for $B$ and 4 possibilities for C. Since the order doesn't count we have to multiply by $\frac{5!}{2!2!}$ . At the end, I get $$\frac{6\cdot 5\cdot 4 \cdot 5!}{6^5\cdot 2!\cdot 2!}.$$ What's wrong in my argument?","['dice', 'probability']"
3080623,Drawing balls from an urn or counting certain posets,"A  colleague of mine was curious about the number of possible start-configurations in a game. The game itself is not known to me, but the question which he formulated as urn problem was interesting. Urn problem: Assume we have an urn containing 100 balls. The balls are colored, 25 are red, 25 blue, 25 green  and  25 are black. We pick four balls without replacement and repeat this step until the urn is empty. We so obtain 25 groups of four balls each and the question is: how many configurations of this type are possible?   Thereby we may assume the order of the $4$ balls in each group is not relevant as well as the order of the $25$ groups is not relevant. A reformulation: Given an alphabet $V=\{1,2,3,4\}$ we consider $4$ -letter words $x_1x_2x_3x_4$ with $1\leq       x_1\leq x_2\leq x_3\leq x_4\leq  4$ when considered as numbers. These $4$ -letter words are building blocks of words of length $100$ . We take $25$ blocks of this kind to form a $100$ -letter word $w=b_1b_2\ldots b_{25}$ with the property that $b_j\leq b_{j+1}, 1\leq j\leq 25$ when considered as numbers. Question: How many words of this type contain exactly $25$ characters of each of the characters $j\in V, 1\leq j\leq 4$ . In general we are given an alphabet $V=\{1,2,\ldots,q\}$ with size $|V|=q$ . (a) We consider words of length $N$ and building blocks $x_1x_2\ldots x_M$ of size $M$ with $1\leq x_1\leq x_2\leq \cdots \leq x_M\leq  q$ and $M|N$ , i.e. $N$ being an integer multiple of $N$ . (b) The words are of the form $w=b_1b_2\ldots b_{N/M}$ with $b_j\leq b_{j+1}, 1\leq j \leq N/M-1$ . (c) We are looking for the number $\color{blue}{A_q(N,M)}$ , the number of words as specified in (a) and (b) which contain $N/q$ characters of each of the characters $j\in V, 1\leq j\leq q$ implying that $N$ is an integer multiple of $q$ as well. In this setting the urn problem is asking for $\color{blue}{A_4(100,4)}$ . The number of building blocks of $A_q(N,M)$ can be easily determined. It is \begin{align*}
\sum_{1\leq x_1\leq x_2\leq\cdots\leq x_M\leq q}1=\binom{M+q-1}{M}\tag{1}
\end{align*} A generating function of (1) can be easily derived. We have \begin{align*}
\sum_{M=0}^\infty\sum_{q=0}^\infty   x^My^q\binom{M+q-1}{M}&=\frac{1-x}{1-x-y}\\
&=1+y\left(1+x+x^2+x^3+x^4+\cdots\right)\\
&\qquad+y^2\left(1+2x+3x^2+4x^3+5x^4+\cdots\right)\\
&\qquad+y^3\left(1+3x+6x^2+10x^3+\color{blue}{15}x^4+\cdots\right)\\
&\qquad\cdots
\end{align*} Denoting with $[x^M]$ the coefficient of $x^M$ in a series we see for instance $[x^4y^3]\frac{1-x}{1-x-y}=\binom{6}{2}=15$ which  is  the number of valid building  blocks of size $4$ when given  a three letter alphabet $V=\{1,2,3\}$ . These $15$ building blocks are \begin{align*}
1111\quad1122\quad1222\quad1333\quad2233\\
1112\quad1123\quad1223\quad2222\quad2333\\
1113\quad1133\quad1233\quad2223\quad3333\\
\end{align*} The difficult part  (at  least for me) is to determine the number of valid words $A_q(N,M)$ which  can  be generated from  these building  blocks. I've tried to derive a    generating   function  which  describes   this  scenario,  but I  wasn't   successful up to  now. Posets: Another approach could  be  using posets based upon the approach: Start  with  an  empty  word  and  append $N/M$ times a  building  block respecting the ordering given in (b). Derive a generating function for the number of valid posets. In order to better see the situation, here is a manageable example. We are looking for $A_2(12,M)$ the number of words of length $12$ from a two-letter alphabet with different block-sizes $M$ following (a) - (c) from above. The Hasse-diagrams for $M=2,3,4,6$ are: We see graded  posets  of length $N/M$ with $A_2(12,2)=A_2(12,6)=4$ and $A_2(12,3)=A_2(12,4)=5$ indicating the symmetry \begin{align*}
A_q(N,M)=A_q(N,N/M)
\end{align*} Here is a  list  of small  values  of $A_2(N,M)$ : $$
\begin{array}{r|rrrrrr}
M&1&2\\
A_2(2,M)&1&1\\
\hline
M&1&2&4\\
A_2(4,M)&1&2&1\\
\hline
M&1&2&3&6\\
A_2(6,M)&1&2&2&1\\
\hline
M&1&2&4&8\\
A_2(8,M)&1&3&3&1\\
\hline
M&1&2&5&10\\
A_2(10,M)&1&3&3&1\\
\hline
M&1&2&3&4&6&12\\
A_2(12,M)&1&4&5&5&4&1\\
\end{array}
$$ Summary: The question  is how to find a formula for $A_q(N,M)$ or how to derive a generating function for these numbers. Alternatively is there an appropriate technique to count the number  of posets corresponding to $A_q(N,M)$ ?","['order-theory', 'combinatorics', 'generating-functions']"
3080643,"Find all $n$ such that $n/d(n) = p$, a prime, where $d(n)$ is the number of positive divisors of $n$","Let $d(n)$ denote the number of positive divisors of $n$ . Find all $n$ such that $n/d(n) = p$ , a prime. I tried this, but only I could get two solutions.
I proceeded like this - Suppose $$n = p^r \cdot p_1^{r_1} \cdot \cdots \cdot p_k^{r_k}$$ where the $p_i$ s are distinct primes. Given $$n=p\cdot d(n)=p(r+1)(r_1+1)\cdots (r_k+1)$$ $$p^r \cdots p_1^{r_1}\cdot \cdots \cdot p_k^{r_k}=(r+1)(r_1+1)\cdots(r_k+1)$$ If $k=0$ , then $p^(r-1)=r+1$ . Hence $p=2$ and $r=3$ , or $p=3$ and $r=2$ . I.e., $n=8$ , or $n=9$ . But, when I am assuming $k>0$ , I am finding no clue. For this, I need help. Thanks in advance!","['number-theory', 'divisibility', 'prime-factorization', 'prime-numbers']"
3080649,How many colors are necessary for a W-polyomino to never cover a color more than once?,"A W-polyomino is a polyomino with 2 cells in each row (except possibly the last, which may have one cell), and each row offset once cell to the right. Below are the first few W polyominoes. How many colors are necessary to color the plane, so that no matter how we place the polyomino, it never covers two cells of the same color? For even $n$ , we need $n$ colors: a coloring is given by $C_n(x, y) = (x + yn/2) \bmod n$ . This is a coloring with $n$ colors that repeat in each row, and each second row is offset by half $n$ . For example, this is the coloring $C_6$ . For odd $n$ , we can use $C_{n+1}$ to show that we need at most $n + 1$ colors. The missing piece is to prove that $n$ colors is not enough. (This is my question.) So far:
(We ignore the monomino, which is an exception.) For $n = 3$ , we need 4 (we can cover any two cells of a $2\times 2$ square). For $n = 5$ , we need 6 (a bookkeeping proof is given in Polyominoes on a Multicolored Infinite Grid ). The proof used for $n = 5$ is not very difficult, but it is not clear how it can be generalized. Background: I am looking at the general case for polyominoes. From the paper I linked above I could devise some general techniques. I recently asked the same question for rectangles , which is very relevant as we can often reduce the problem to that, or at least establish a range. Here follows some of the general principles. $H(P)$ is the rectangular hull of polyomino $P$ , and $C(P)$ the number of colors we need to color $P$ , and $|P|$ is the number of cells in $P$ . $|P| \leq C(P) \leq C(H(P))$ . (The number of colors is between the number of cells and the number of colors we need for the hull). If $P$ covers any two cells of $Q$ , then $C(P) \geq C(Q)$ . If $Q \subset P$ , then $C(P) \geq C(Q)$ . (This follows from above and is usually more convenient.) If $P$ is an L shape (a rectangle with a rectangle removed at the corner), then $C(P) = C(H(P))$ . If $P$ can fit inside $Q$ in all orientations, and $Q$ tiles the plane, $C(P) \leq |Q|$ . A typical scenario: Consider this polyomino $P$ : # ##
### It has the L-tetromino as subset, so we need at least the same as the hull of the L, which is $R(3, 2)$ , which has been shown to need 8 colors. Since the hull of $P$ is $R(4, 2)$ , which also requires 8 colors, we know we need 8 colors for $P$ too. The W-pentomino is the first polyomino that cannot be handled by the above. (It is enough for all other polyominoes with 5 or less cells, and all but 6 hexominoes, of which one is also a W-polyomino.)","['polyomino', 'combinatorics', 'tiling']"
3080659,"In an acute triangle ABC, the base BC has the equation $4x – 3y + 3 = 0$. If the coordinates of the orthocentre (H) and circumcentre (P).","In an acute triangle ABC, the base BC has 
the equation $4x – 3y + 3 = 0$ . If the coordinates of 
the orthocentre (H) and circumcentre (P) of the 
triangle are $(1, 2)$ and $(2, 3)$ respectively, then the 
radius of the circle circumscribing the triangle is $\dfrac{\sqrt m}{n}$ , where m and n are relatively prime. Find the 
value of (m+ n). (You may use the fact that the distance between 
orthocentre and circumcentre of the triangle is 
given by $R \sqrt{1 – 8\cos A\cos B\cos C}$ ) Attempt: 
I found $R$ by taking reflection ( $A$ ) of $H$ about $BC$ and then finding the distance between $P$ and $A$ . But, I cannot figure out how to solve the problem by using the hint given.","['coordinate-systems', 'triangles', 'geometry']"
3080680,How do we really get the angle of a vector from the components?,"Usually when people discuss getting the polar form of a vector $v$ , they present the following two formulas: $$\text{Magnitude}(v) = \sqrt{x^2 + y^2}$$ $$\text{Angle}(v) = \arctan \left(\frac{y}{x} \right)$$ $$ \text{ Where } \space v = \begin{bmatrix} x \\ y \end{bmatrix}$$ I believe that this formula for the angle is only partially true. I think a better and more complete formula for the angle should be: $$ \text{Angle}(v) = \begin{cases}
\arctan \left(\frac{y}{x} \right)  &; \space x \gt 0 \\
\pi +\arctan \left(\frac{y}{x} \right) &; \space x \lt 0 \\
{\begin{cases}
 \operatorname{sign}(y) \frac{\pi}{2}  &; y \neq 0 \\
\text{undefined} &; \space y = 0
\end{cases}} &; x = 0
\end{cases} $$ Is there some sort of way to simplify this or to better express this, or is this it?","['trigonometry', 'vectors']"
3080697,Does Intermediate value theorem work depending on the domain,"Does it work when the domain and codomain are subsets of the reals? For example, any continuous function from a discrete subset of reals that satisfies the least upper bound property? Is this possible? If it breaks down, why?","['continuity', 'functional-analysis', 'real-analysis']"
3080771,Proving a strict inequality in the limit,"I want to prove that $$ \lim_{k \to \infty} \left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < e .$$ Using the $AM-GM$ inequality we arrive at $$\left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < \left(\frac{k + 1 - \frac{1}{2^k} }{k} \right)^k = \left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k < \left(1 + \frac{1}{k} \right)^k < e.$$ The first inequality is strict because the terms are different.However, I know that in the limit, strict inequalities can transform into equalities. Since the limit of $\left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k$ when $k$ goes to infinity is also $e$ , how could I prove a strict inequality?","['limits', 'calculus']"
3080776,Find all Sylow 2-subgroups of $S_4$ using Sylow's theorems,"I am trying to find all the Sylow 2 subgroups of S4 using Sylow’s theorems. Now, I know that a Sylow 2 subgroup of S4 has size 8, and that there are either 1 or 3 of them (as the number of of Sylow 2-subgroups has form 1+2k and divides 3, the index). Now my lecturer states “stabilisers of the 3 different bisections of {1,2,3,4} yield 3 distinct Sylow 2 subgroups” Now my questions are: 1) What does it mean by stabilisers of 3 different bijections of {1,2,3,4}? Is that to say the elements of S4 which send {1,2} and {3,4} to {1,2},{3,4} i.e. by (12)(34)? 2) How does he know that there are 8 elements of this set? Perhaps this will become clearer after the 1st question is answered. Many thanks, group theory is hard.","['group-theory', 'sylow-theory', 'permutation-cycles']"
3080854,Can all trigonometric expressions be written in terms of sine and cosine?,"I know that sine and cosine can be rewritten in terms of the real and complex parts of the exponential function as a result of Euler's formula. My question is, can every trigonometric expression be written in terms of elementary trigonometric functions ( $\sin$ , $\cos$ )? If not, why couldn't they be? I would think that they could, although I understand that sometimes it may be prohibitive to do so since most trig identities can be derived from Euler's formula. Are there any cases when a trig expression absolutely cannot be written in terms of the elementary functions? The only potential counterexamples I could think of would include some non trigonometric terms or factors. I know that hyperbolic sine and cosine can be rewritten in terms of sine and cosine in the complex plane.","['elementary-functions', 'trigonometry']"
3080944,"$[T,S]:=TS-ST=I$ cannot holds","Let $\mathcal{B}(\mathcal{H})$ the algebra of all bounded linear operators on an infinite dimensional complex Hilbert space $\mathcal{H}$ . Let $T,S\in \mathcal{B}(\mathcal{H})$ . I want to prove that the equality \begin{equation}\label{commz}
[T,S]:=TS-ST=I \tag{1},
\end{equation} cannot hold. To see this, assume that $(1)$ holds. We shall prove by induction that \begin{equation}\label{tag2}
[T, S^n] = nS^{n - 1},\;n\in \mathbb{N}^*.
\end{equation} By assumption, $[T,S]=S^{0}=I$ . Suppose $[T,S^{n}]= nS^{n-1}$ for some $n\in \mathbb{N}^*$ . Then \begin{align*}
   [T,S^{n+1}]
  & = TS^{n+1}-S^{n+1}T\\
    & =(TS^{n}-S^{n}T)S+S^{n}TS-S^{n+1}T \\
     & = [T,S^{n}]S+S^{n}[T,S] \\
     & = nS^{n-1}S+S^{n}=(n+1) S^{n}.
\end{align*} So, \begin{equation*}\label{tag11}
TS^n - S^nT = n S^{n=1},
\end{equation*} holds for all $n\in \mathbb{N}^*$ . Hence, \begin{align*}
n\|S^{n-1}\|
& = \|TS^n - S^nT\|\\
 &\leq 2 \|T\|\cdot\|S^{n} \|\\
 &\leq 2 \|T\|\cdot\|S\|\cdot\|S^{n-1} \|.
\end{align*} If $\|S^{n-1} \|\ne 0$ , we get $n \le 2 \|T\|\|S\|$ , for all $n\in \mathbb{N}^*$ . This leads to a contradiction. So, $(1)$ cannot hold for $T,S\in \mathcal{B}(\mathcal{H})$ . Why $S^{n-1}\ne 0$ for all $n\in \mathbb{N}^*$ ?","['operator-theory', 'proof-verification', 'functional-analysis']"
3081071,$\mathbb{Z}^2$-action on product is ergodic,"I try to solve exercise 8.1.1 from Einsielder's book Ergodic Theory. It is:
""Let $(X,B_X, μ, T)$ and $(Y,B_Y , ν, S)$ be ergodic Z-actions.
Define a $\mathbb{Z}^2$ -action on the product $(X×Y, μ×ν)$ by $(m, n)\rightarrow T^m×S^n$ . Show
that this action is ergodic, but has subgroups whose action is not ergodic. The last part is clear for me (trivial subgroup), but for the first part I am struggling. I thought of taking a measurable function $f$ on $X\times Y$ which is invariant under the action. As one can write $f$ as a convergent series of characteristic functions of products $B_n\times C_n$ where $B_n \in B_X$ and $C_n \in B_Y$ , one can write $f=\sum_{k=0}^\infty a_k g_k(x) h_k(y)$ for some $a_k \in \mathbb{R}$ and measurable real-valued $g_k$ on $X$ and $h_k$ on $Y$ . If one then takes into account the invariance under the subgroups generated by $(1,0)$ and $(0,1)$ , respectively, one sees that for any $x\in X$ the function $f(x,-)$ is constant a.e. on $Y$ and for any $y\in Y$ the function $f(-,y)$ is constant a.e. on $X$ . Can one conclude that $f$ is constant almost everywhere? And if yes, how? Thanks for any hints","['measure-theory', 'ergodic-theory', 'probability-theory', 'dynamical-systems']"
3081116,Calculus on Manifolds Theorem 3-14 (Sard's Theorem),"I have a question regarding Theorem 3-14 in Spivak's Calculus on Manifolds : The proof starts by considering a closed rectangle $U\subset A$ such that all the sides of $U$ are of the same length $l$ . Let $\epsilon>0$ . Spivak claims that if $N$ is sufficiently large and $U$ is divided into $N^n$ rectangles, with sides of length $l/N$ , then for each of these rectangles $S$ , if $x\in S$ we have $$|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|\leq \epsilon\sqrt{n}(l/N)$$ for all $y\in S$ . I don't get why this is necessarily true. My attempt to prove this is as follows. Take any $\epsilon>0$ . Since $g$ is differentiable on $U$ , for all $x\in U$ there exists $\delta'_x >0$ such that for all $y$ , $$|x-y|<\delta'_x\implies|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|.$$ Since $g$ is continuously differentiable, for all $x\in U$ there exists $\delta''_x$ such that for all $y$ and $v$ , $$|x-y|<\delta''_x\implies|Dg(x)(v)-Dg(y)(v)|<\epsilon|v|.$$ Let $\delta_x=\min\{\delta'_x,\, \delta''_x\}$ . Then $\{B(x;\delta_x/2)\,|\,x\in U\}$ is an open cover of $U$ . Since $U$ is compact, a finite number of these balls cover $U$ . Say these balls are centered around $x_1,\,x_2,\,\ldots,\,x_k$ . Let $\delta=\min\{\delta_{x_1}/2,\,\delta_{x_2}/2\,\ldots,\,\delta_{x_k}/2\}$ . Pick any arbitrary $x\in U$ . Then $x\in B(x_i,\delta_{x_i}/2)$ for some $1\leq i \leq k$ . Thus $$|Dg(x_i)(x-x_i)-(g(x)-g(x_i))|<\epsilon|x_i-x|.$$ For any $y\in B(x;\delta)$ we have $y\in B(x_i,\delta_{x_i})$ , ergo $$|Dg(x_i)(y-x_i)-(g(y)-g(x_i))|<\epsilon|x_i-y|.$$ Combining these two statements gives $$|Dg(x_i)(x-y)-(g(x)-g(y))|<\epsilon(|x_i-x|+|x_i-y|).$$ Again using the fact that $x\in B(x_i,\delta_{x_i}/2)$ , we have $$|Dg(x_i)(x-y)-Dg(x)(x-y)|<\epsilon|x-y|,$$ hence $$|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon(|x-y|+|x_i-x|+|x_i-y|).$$ Unfortunately, I don't know how to progress from here. Any help would very much be appreciated.",['multivariable-calculus']
3081147,Rewrite $ \int_{\mathcal{S}}dP_X=1 $ as conditions on boxes in $\mathbb{R}^d$,"Take $r\in \mathbb{N}$ and let $d\equiv r+\binom{r}{2}$ . Consider a d-dimensional random  vector $X\equiv (X_1,...,X_d)$ . Let $P_X$ be the probability distribution of $X$ . Assume that $$
\int_{\mathcal{S}}dP_X=1
$$ where $$
\begin{aligned}
\mathcal{S}\equiv \{(b_1,b_2,..., b_d)\in \mathbb{R}^{d}: \text{ } & b_{r+1}=b_1-b_2, b_{r+2}=b_1-b_3, ...,b_{2r-1}=b_1-b_r, \\
&b_{2r}=b_2-b_3, ..., b_{3r-3}=b_2-b_r,\\
&...,\\
& b_d=b_{r-1}-b_r\}
\end{aligned}
$$ For example, when $r=2$ ( $d=3$ ) we have the surface $$
\begin{aligned}
\mathcal{S}\equiv \{(b_1,b_2,b_3)\in \mathbb{R}^{3}: \text{ } & b_3=b_1-b_2\}=\{(b_1,b_2,b_3)\in \mathbb{R}^{3}: \text{ } & b_1=b_2+b_3\}
\end{aligned}
$$ When $r=3$ ( $d=6$ ) we have $$
\begin{aligned}
\mathcal{S}\equiv \{(b_1,..., b_6)\in \mathbb{R}^{6}: \text{ } & b_4=b_1-b_2, b_5=b_1-b_3, b_6=b_2-b_3\}
\end{aligned}
$$ My final goal: I'm interested in rewriting the condition $\int_{\mathcal{S}}dP_X=1$ as a collection of zero probability measure conditions on d-dimensional ""boxes"" in $\mathbb{R}^d$ . The idea is that any box in $\mathbb{R}^d$ not intersecting $\mathcal{S}$ should have probability measure equal to zero. Therefore, if we consider enough of these boxes, we should be able to equivalently rewrite $\int_{\mathcal{S}}dP_X=1$ . When $r=2$ ( $d=3$ ), my goal is achieved by the following claim Claim: For any two real numbers $(b,c)\in \mathbb{R}^2$ , define the boxes $$B(b,c)\equiv \{(x,y,z)\text{ s.t. } x> b+c, y\leq b, z\leq c\}$$ and $$Q(b,c)\equiv \{(x,y,z)\text{ s.t. } x\leq  b+c, y>b, z>c\}$$ If $P_{X}(B(b,c))=0$ and $P_{X}(Q(b,c))=0$ $\forall(b,c)\in \mathbb{Q}^2$ , then $\int_{\mathcal{S}}dP_{X}=1$ . The proof of the claim is provided here I would like your help to generalise the claim (and possibly the proof) to any $r$ . What I find challenging is defining the relevant boxes for any $r>2$ . I really can't see how to generalise the box definitions from $r=2$ to any $r$ .","['measure-theory', 'lebesgue-measure', 'geometry', 'probability-theory', 'probability']"
3081160,Proof verification: Show that $f$ is continuous if $f(\overline{A})\subset\overline{f(A)}$.,"Let $X,Y$ be metric spaces and define $f: X\to Y$ . Show that $f$ is continuous iff $f(\overline{A})\subset\overline{f(A)}$ for each $A\subseteq X$ . My proof: $\Rightarrow$ Let $f:X\to Y$ be continuous and $A\subseteq X$ . Let $y\in f(\overline{A})$ , that is, there exist $x\in\overline{A}$ such that $f(x)=y$ with $x\in\overline{A}$ , $x\in A$ or $x$ is a limit point of $A$ . I) If $x\in A$ , as $f(x)=y$ , then $y\in f(A)\subseteq\overline{f(A)}\implies y\in \overline{f(A)}$ . II) If $x$ is a limit point of $A$ , that is, there exist $(x_{N})\subseteq A$ such that $\lim_{N\to\infty}{x_{N}}=x$ , so $f(x_{N})=y_{N}\in f(A)$ Take the limit $\lim_{N\to \infty}{f(x_{N})}=\lim_{N\to\infty}{y_{N}}$ . Since $f$ is continouos, we can exchange the limit $f(\lim_{N\to\infty}{x_{N}})=\lim_{N\to\infty}{y_{N}}\implies f(x)=\lim_{N\to\infty}{y_{N}}$ but $f(x)=y$ by hypothesis, so $\lim_{N\to\infty}{y_{N}}=y$ , then $y$ is a limit pointt of $f(A)$ . Therefore, $y\in\overline{f(A)}$ . From I) and II), we can conlcuded that $f(\overline{A})\subseteq \overline{f(A)}$ . The other direction of the proof is clear to me, so I need verification of $\Rightarrow$ proof. Question : Is this proof sufficient?  Thanks!","['functions', 'proof-verification', 'metric-spaces']"
3081206,Conjecture: $\sum\limits_{n\geq0}\left(\frac12\right)^n\prod\limits_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2$ [duplicate],"This question already has answers here : Summing the power series $\sum\limits_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{2n+1}\prod\limits_{k=1}^n\frac{2k-1}{2k} $ (5 answers) Closed 5 years ago . I am trying to solve a problem I made form myself: proving that $$\sum_{n\geq0}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2$$ The highly accurate powers of Desmos seem to confirm my hunch. But how do I prove it? I was just messing around with products and generating functions, and then I noticed that the numerical value of the sum in question was suspiciously similar to $\sqrt2$ , so I conjectured the result. Unfortunately I found this completely by accident and have absolutely no idea of how to prove it. Feeble attempt: Define $$S(x)=\sum_{n\geq0}x^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}$$ Which Wolfram says is equal to $$S(x)=\sum_{n\geq0}x^n\frac{(1/2-n)_n}{(-n)_n}$$ With $\displaystyle (x)_n=\frac{\Gamma(x+n)}{\Gamma(x)}$ . But that doesn't really make sense because $\Gamma(0)$ is undefined. So all in all I'm just confused. Could I have some help? Edit: According to the comments, it suffices to prove that $$\sum_{n\geq1}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2-1$$","['sequences-and-series', 'closed-form', 'combinatorics', 'real-analysis']"
3081210,Existence of continuous derivatives of partial functions and total differentiability,"We know that for a function $\mathbb{R}^m\to \mathbb{R}^n$ the existence and continuity of partial derivatives implies the differentiability of that function. Will this hold true for functions on product spaces of arbitrary Banach spaces? I.e., given $f\colon E_1\times E_2\to F$ ( $E_1,E_2,F$ being Banach spaces) and $(x,y)\in E_1\times E_2$ , does the existence and continuity of $(x,y)\mapsto Df(\cdot,y)(x)$ and $(x,y)\mapsto Df(x,\cdot)(y)$ locally at $(x,y)$ imply $f$ to be differentiable at $(x,y)$ ? I guess it won't because the proof in the euclidean case goes from $f(p)$ to $f(p+h)$ in finitely many steps along the coordinate axes... So, maybe it will hold in product spaces of Hilbert spaces? What would a counter-example be?","['frechet-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
3081211,General colimits and filtered colimits in the category of sets,"A category $\mathsf{I}$ is filtered if $\mathsf{Ob(I)} \neq \varnothing$ , for any $i,j \in \mathsf{Ob(I)}$ there is $k \in \mathsf{Ob(I)}$ and morphisms $f\colon i\to k$ and $g\colon j\to k$ , for any pair $f,g\colon i\to j$ of parallel morphisms in $\mathsf{I}$ there is $k \in \mathsf{Ob(I)}$ together with a morphism $h\colon j\to k$ so that $h\circ f = h\circ g$ . Let $F\colon\mathsf{I}\to\mathsf{Set}$ be a functor with $\mathsf{I}$ being small. It is known that a colimit of such a functor is the quotient $\bigsqcup_{i \in \mathsf{Ob(I)}} F(i)/\sim$ together with a colimit cocone $\lambda\colon F\Rightarrow\bigsqcup_{i \in \mathsf{Ob(I)}} F(i)/\sim$ such that for any $i \in \mathsf{Ob(I)}$ and for any $x \in F(i)$ we have $\lambda_i(x) = [(i,x)]_{\sim}$ (the equivalence class of $(i,x)$ with respect to $\sim$ ) where $\sim$ is the equivalence relation generated by the relation $\{ ((i,x),(j,y)) \in (\bigsqcup_{i \in \mathsf{Ob(I)}} F(i))\times(\bigsqcup_{i \in \mathsf{Ob(I)}} F(i)) \mid$ there is $f\colon i\to j$ so that $y = F(f)(x) \}$ , that is, such an equivalence relation on $\bigsqcup_{i \in \mathsf{Ob(I)}} $ so that for any $(i,x), (j,y) \in \bigsqcup_{i \in \mathsf{Ob(I)}} F(i)$ there is $n \in \mathbb{N}_{>0}$ and there are $(i_1,x_1), ..., (i_n,x_n) \in \bigsqcup_{i \in \mathsf{Ob(I)}} F(i)$ so that $(i_1,x_1) = (i,x), (i_n,x_n) = (j,y)$ and for any $1 \leq k < n$ there is a there is either a morphism $f\colon i_k \to i_{k+1}$ for which we have $x_{k+1} = F(f)(x_k)$ and there is a morphism $f\colon i_{k+1} \to i_k$ for which we have $x_k = F(f)(x_{k+1})$ . It is also known that if $\mathsf{I}$ is a filtered small category, then a colimit of a funtor $F\colon\mathsf{I}\to\mathsf{Set}$ is the quotient $\bigsqcup_{i \in \mathsf{Ob(I)}}F(i)/\sim$ (together with a colimit cocone $\lambda\colon F\Rightarrow\bigsqcup_{i \in \mathsf{Ob(I)}}F(i)$ such that for any $i \in \mathsf{Ob(I)}$ and for any $x \in F(i)$ we have $\lambda_i(x) = [(i,x)]_{\sim}$ ) so that we have $(i,x) \sim (j,y)$ precisely when there is $k \in \mathsf{Ob(I)}$ and there are morphisms $f\colon i\to k, g\colon j\to k$ so that $F(f)(x) = F(g)(y)$ . My question is whether we can deduce the second result (about filtered colimits in $\mathsf{Set}$ ) from the first result (about general colimits in $\mathsf{Set}$ ) by reducing the first equivalence relation to the second in the case of $\mathsf{I}$ being filtered or do we have to start from scratch.","['elementary-set-theory', 'equivalence-relations', 'limits-colimits', 'category-theory']"
3081246,Worked examples of Lie derivatives,"I'm trying to find the Lie derivative of a 2-form $\sin(\theta)d\theta \wedge d\phi$ with respect to a vector field given in a differential basis $a \partial/ \partial \phi$ and I think the way to go here is to use Cartan's formula but I'm not sure how to do that. I can't find any worked examples in any differential textbook or online, everything seems to focus on proving identities not performing actual calculations. Can somebody point me to a useful resource with some exercises or step-by-step examples?","['lie-derivative', 'differential-forms', 'differential-geometry']"
3081265,Why is it legitimate to assume that the Chapman-Kolmogorov equations hold everywhere?,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(\mathcal F_t)_{t\ge0}$ be a filtration on $(\Omega,\mathcal A)$ $E$ be a Polish space and $\mathcal E:=\mathcal B(E)$ $(X_t)_{t\ge0}$ be an $E$ -valued $\mathcal F$ -Markov process on $(\Omega,\mathcal A,\operatorname P)$ $\kappa_{s,\:t}$ be a regular version of the conditional probability of $X_t$ given $X_s$ , i.e. $\kappa_{s,\:t}$ is a Markov kernel on $(E,\mathcal E)$ with $$\operatorname P\left[X_t\in B\mid X_s\right]=\kappa_{s,\:t}(X_s,B)\;\;\;\text{almost surely for all }B\in\mathcal E\tag1$$ for $s,t\ge0$ By the Markov property and $(1)$ , $$\operatorname P\left[X_t\in B\mid\mathcal F_s\right]=\kappa_{s,\:t}(X_s,B)\;\;\;\text{almost surely for all }B\in\mathcal E\text{ and }0\le s\le t.\tag2$$ Usually, we want $(\kappa_{s,\:t}:0\le s\le t)$ to satisfy the Chapman-Kolmogorov equation $$\kappa_{r,\:t}=\kappa_{r,\:s}\kappa_{s,\:t},\tag3$$ where the right-hand side denotes the composition of transiton kernels , for all $0\le r\le s\le t$ . However, with the definition of $\kappa_{s,\:t}$ as the conditional probability of $X_t$ given $X_s$ , we've only got $^1$ $$\kappa_{r,\:t}(x,B)=(\kappa_{r,\:s}\kappa_{s,\:t})(x,B)\;\;\;\text{for all }B\in\mathcal E\text{ and }\operatorname P\circ\:X_r^{-1}\text{-almost all }x\in E\tag4$$ for all $0\le s\le t$ . However, in the literature, one is usually assuming that $(2)$ and $(3)$ hold together. Why is that possible? I could imagine that the reason is the following: Since $E$ is Polish, given $(\kappa_{s,\:t}:0\le s\le t)$ with $(3)$ there is always a Markov process $\tilde X$ on an other probability space with transition semigroup $(\kappa_{s,\:t}:0\le s\le t)$ and initial distribution $\operatorname P\circ\:X_0^{-1}$ . Is that the correct argument? $^1$ Note that there is a crucial selection of a common null set for all $B$ happening in $(4)$ . I guess this is legitimate as long as $\mathcal E$ is countably generated. Maybe someone could comment on this.","['measure-theory', 'markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory']"
3081279,Prove $a_n = \lfloor ni \rfloor$ for some irrational $i$ has no pattern,"I have a sequence $a_n = \lfloor ni \rfloor$ for an irrational $i$ and I want to prove that there is no 'pattern' to these terms. In particular, I have $1 < i <2$ and I want to prove that $\forall a,b \in \mathbb{N}, \exists k \in \mathbb{N}$ such that $a +bk \neq a_n$ for any $n$ . This makes sense to me intuitively because $a_n - a_{n-1}$ will be 1 or 2 'randomly', so the sequence must eventually 'skip over' one of the terms of $a+bk$ . My approach has been to assume that it holds, then show that $i$ must be rational for a contradiction. However, the floor is messing up the simple divisibility techniques I would use.","['irrational-numbers', 'sequences-and-series']"
3081297,Proving $\binom{n + m}{r} = \sum_{i = 0}^{r} \binom{n}{i}\binom{m}{r - i}$,"To prove $$\binom{n + m}{r} = \sum_{i = 0}^{r} \binom{n}{i}\binom{m}{r - i},$$ I demonstrated that the equality is true for any $n,$ for $m = 0, 1,$ and for any $r < n + m$ simply by fixing $n$ and $r$ and inserting $0,1$ for $m.$ Then, I proceed to induct on $m$ (and on $m$ only). I am not perfectly confident in my self, however, for I see two placeholders, $n$ and $m.$ Is this a case where double induction is needed (first on $m$ and then on $n$ )? Whether or not this proof requires double induction, may someone explain when double induction is needed? Consider any fixed $n, r \geq 0$ and the following two cases (I know that only one case is needed to complete this inductive proof). CASE 1 \begin{align}
\binom{n + 0}{r} &= \sum_{i = 0}^{r} \binom{n}{i}\binom{0}{r - i} \\ &= \binom{n}{0}\binom{0}{r} + \binom{n}{1}\binom{0}{r-1} + \cdots + \binom{n}{r}\binom{0}{0} \\ &= 0 + 0 + \cdots + \binom{n}{r} \\ &= \binom{n}{r}
\end{align} CASE 2 \begin{align}
\binom{n + 1}{r} &= \sum_{i = 0}^{r} \binom{n}{i}\binom{1}{r - i} \\ &= \binom{n}{0}\binom{0}{r} + \binom{n}{1}\binom{0}{r-1} + \cdots + \binom{n}{r-1}\binom{1}{r - (r-1)} + \binom{n}{r}\binom{1}{r - r} \\ &= 0 + 0 + \cdots + \binom{n}{r-1} + \binom{n}{r} \\ &= \binom{n}{r-1} + \binom{n}{r}
\end{align} INDUCTION Suppose it is true for $m \leq k.$ Now, consider $$\binom{n + (k + 1)}{r}.$$ It follows from Pascal's Identity that $$\binom{n + (k+1)}{r} = \binom{n + k}{r} + \binom{n + k}{r-1}$$ And, \begin{align}
\binom{n + k}{r} + \binom{n + k}{r-1} &= \sum_{i = 0}^{r} \binom{n}{i}\binom{k}{r - i} + \sum_{i = 0}^{r-1} \binom{n}{i}\binom{k}{r - 1 - i} \\ &= \binom{n}{r} + \sum_{i = 0}^{r-1} \binom{n}{i}\binom{k}{r - i} + \sum_{i = 0}^{r-1} \binom{n}{i}\binom{k}{r - 1 - i} \\ &= \binom{n}{r} + \sum_{i = 0}^{r-1} \binom{n}{i}\bigg[\binom{k}{r - i} + \binom{k}{r - 1 - i}\bigg] \\ &= \binom{n}{r} + \sum_{i = 0}^{r-1} \binom{n}{i}\binom{k+1}{r-i} \\ &= \sum_{i = 0}^{r} \binom{n}{i}\binom{k+1}{r-i}
\end{align} Hence, the equality holds for $m = k + 1.$ Given that the equality holds for $m = 0, 1,$ and that if equality holds for $m = k,$ it then holds for $m = k + 1,$ it follows that the equality holds $\forall m \in \mathbb{N}.$","['induction', 'combinatorics']"
3081300,"Find rank of AB, given that A has linearly independent columns and B has rank 2","I'm trying to prove to myself that given... Matrix A, which has linearly independent columns, and at least 2 columns... Matrix B, which has rank of 2 Their product, AB, will have rank of 2. I believe this is because... Matrix B has two linearly independent columns. Each column of AB will be a combination of the columns of A When multiplying matrix A by matrix B, each of the two independent columns of B will create a unique combination of the columns in A. Is this true? If so, can this be made more rigorous? Thanks for the help in advance!","['matrices', 'matrix-rank', 'linear-algebra']"
3081327,Eigenvalues of a block off-diagonal matrix,"Let $A_1,A_2 \in \mathbb{R}^{n \times n}$ . Construct the block matrix $A$ as follows: $$A: = \left[ {\begin{array}{*{20}{c}}
0&{{A_1}}\\
{{A_2}}&0
\end{array}} \right]$$ My observation is that matrix $A$ cannot have all its eigenvalues in the open left-half plane. In other worlds, if all eigenvalues of $A$ have non-positive real parts, then all of them are imaginary. I greatly appreciate it if someone can give me some idea/intuition why this happens.
Thanks","['matrices', 'matrix-calculus', 'linear-algebra', 'eigenvalues-eigenvectors']"
3081337,Showing $ 2 $ matrices are similar [duplicate],"This question already has answers here : How do I tell if matrices are similar? (6 answers) Closed 5 years ago . I gotta show if or if not those $ 2 $ matrices are similar: $$
    \left(\begin{matrix}
    3 & 2 & -2 \\
    1 & 4 & 0 \\
    -2 & 1 & -1 \\
    \end{matrix}\right)
$$ $$
    \left(\begin{matrix}
    1 & 3 & -1 \\
    3 & 3 & 1 \\
    -2 & 1 & 2 \\
    \end{matrix}\right)
$$ I already calculated their determinant but it's equal so nothing there. Is it possible to show it based on their polynominal?","['matrices', 'linear-algebra']"
3081369,Proving $ 2 $ angles are equal.,"Hi, so I am doing a proof but I need some help proving one part of it. I'm having trouble proving that angle $ D'Bi = $ angle $ D'iB $ . point $ i $ is the incenter of triangle ABC and D' is the point at which ray Ai intersects the circumcircle. I'm trying to prove that $ D' $ is the circumcenter of triangle $ BiC $ , however like I said before I'm having trouble proving angle $ D'Bi = $ angle $ D'iB $ .","['proof-explanation', 'analytic-geometry', 'geometry']"
3081378,"Differentiability of $f(x, y) = |xy|$ at $0$; is this a mistake by Munkres?","I am reading Analysis on Manifolds by Munkres, and question $1$ on page $54$ says Show that the function $f(x, y) = |xy|$ is differentiable at $0$ , but it is not of class $C^1$ in any neighborhood of $0$ . I know that if a function $f$ is differentiable at a point, then all its partial derivatives exist at that point. However, $$D_1f(x, y) = \begin{cases}
 |y|&\text{if}\, x>0 \\
  -|y| &\text{if}\, x<0
\end{cases}
$$ and $D_1f(0, 0)$ does not exist. How can $f$ be differentiable at $0$ ?","['multivariable-calculus', 'real-analysis']"
3081395,Differentiation under the integral sign - what transformations to use?,Need some help with this integral $$I (\alpha) = \int_1^\infty  {\arctan(\alpha x) \over x^2\sqrt{x^2-1}} dx$$ Taking the first derivative with respect to $\alpha$ $$I' (\alpha) = \int_1^\infty { dx\over (1+\alpha^2 x^2) x\sqrt{x^2-1} }$$ What transformations to use in order to solve $I'(\alpha)$ ?,"['integration', 'calculus']"
3081429,Why can't I reduce the total differential?,"I have encountered the following equation: $g: \mathbb{R}^m \rightarrow \mathbb{R}$ $u: \mathbb{R}^n \rightarrow \mathbb{R}^m$ $z = g(\mathbf{y})$ , $\mathbf{y} = u(\mathbf{x})$ then using numerator layout $\frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ translates to: $$ \left(\begin{array}{ccc}
    \frac{\partial z}{\partial x_{1}} & \cdots & \frac{\partial z}{\partial x_{n}}
    \end{array}\right)=
    \left(\begin{array}{ccc}
    \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}
    \end{array}\right)
    \left(\begin{array}{ccc}
    \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
    \end{array}\right) $$ (From https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py presented in denominator notation there) When calculating the matrix multiplication on paper I came to think that the equation must be wrong because for any element in the result vector: $ \frac{\partial z}{\partial x_i} =   
    \left(\begin{array}{ccc}
    \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}
    \end{array}\right) \left(\begin{array}{c}
    \frac{\partial y_{1}}{\partial x_i}\\
    \vdots \\
    \frac{\partial y_{m}}{\partial x_i}
    \end{array}\right)=
\sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i} \overset{chain \\rule}{=} \sum_{j=1}^m \frac{\partial z}{\partial x_i} = m \frac{\partial z}{\partial x_i}$ But maybe I'm wrong? After all I've never seen the chain rule with $\partial$ but always with $d$ . But even after research they seem to be 2 ways of expressing the same thing: $df/dx$ is the derivative of a term e.g. $2xy$ wrt. $x$ and $\partial f/\partial x$ the derivative of a function e.g. $f(x,y) = 2xy$ . Maybe this can be explained by the law of total derivatives. However this law is sometimes presented using only $\partial$ s, sometimes using a mix like this $ \frac{dz}{dx_i} = \sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{dy_j}{dx_i} $ , further implying that $d$ , $\partial$ is really just a style choice. But if it is, what's keeping me from replacing all $\partial$ with $d$ and applying the chain rule like this: $$ \frac{dz}{dx_i} = \sum_{j=1}^m \frac{dz}{dy_j} \frac{dy_j}{dx_i} = \sum_{j=1}^m \frac{dz}{dx_i} = m \frac{dz}{dx_i} $$ Questions: Is the Equation $\frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ correct? If so, is $\frac{\partial y}{\partial x}$ equivalent to $\frac{dy}{dx}$ ? If so, what am I doing wrong when applying the chain rule?","['matrix-calculus', 'derivatives', 'chain-rule']"
3081469,Is $∅ ∈ \{ \{∅\} \}$ true?,"If $ \{\emptyset\} ∈ \{\emptyset,\{\emptyset\}\} $ is true, does it mean this $ \emptyset \in \{\{\emptyset\}\} $ true ? If it is not, why it is false? Also, does $ \{\{\emptyset\}\}$ mean $\{\emptyset,\{\emptyset,\{\emptyset\}\}\}$ ?",['elementary-set-theory']
3081596,Is 2nd-order ODE with quadratic coefficients solvable?,"Consider an ODE eigensystem $$\begin{bmatrix}
0 & d_1-\mathrm id_2 \\
d_1+\mathrm id_2 & 0 
\end{bmatrix} 
\begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix} = \lambda  \begin{bmatrix}  a(y) \\ b(y)  \end{bmatrix},
$$ where $$d_1=-\mathrm i(p+qy)\partial_y+ry+s$$ $$d_2=-\mathrm i(u+vy)\partial_y+wy+t,$$ $p,q,r,s,u,v,w,t$ are just real constants, and $\mathrm i$ is the imaginary unit. Is it analytically solvable? I reduce it to a 2nd-order ODE of $b$ with coefficients quadratic in $y$ $$\alpha b''(y) + \beta b'(y) + \gamma b(y)=-\lambda^2 b(y)$$ where $$\alpha=(p+q y)^2+(u+v y)^2$$ $$\beta=p (q+2 i s-i v)+u (v+iq+2 it)+(2 i p r+q^2+2 i q s+2 i t v+2 i u w+v^2)y+2 i (q r+v w)y^2 $$ $$\gamma=-s^2-t^2+(p+i u) (w+i r)+[w (q-2 t+i v)-r (-i q+2 s+v)]y-(r^2+w^2)y^2 $$ When $u,v=0$ or $p,q=0$ , it is solvable, although the coefficients are still quadratic polynomials of $y$ . I was wondering if the more general case could be tackled as well. But I don't know how to proceed.","['sturm-liouville', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
3081624,How can I graph the derivative of 1/4th of a circle or a semicircle in a piecewise function? (Also other kinds of piecewise functions),"I'm having trouble with questions like these. In the first image, the original function is what is the two sharp lines and a semicircle in between. I understand how to find and graph the derivative of the parts that are straight lines however i don't understand how to find the derivative of the semicircle. I can see that the radius is 2 so $x^2+y^2 = 2^2 = 4$ Do I implicitly differentiate with respect to $x$ in order to get $y'$ and the graph that? If i do that I get $2x+2yy' = 0 \implies y' = \frac{-x}{y}$ Now what? Also this is a semicircle so must I differentiate $\frac{x^2+y^2}{2} = 2$ ? Similarly, in the second question I have 1/4th of a circle, how can I take the derivative of that and graph it. Finally, in the second question, I understand that the derivative of a parabola would be a linear function because (for example) the derivative of $x^2$ is $2x$ but how can I graph this with just the information that it is a parabola and I'm not given the function itself. I know that if the function is decreasing then the derivative must be negative. I understand intuitively why it must be linear but is there any graphing ""rule"" which would make me know this? EDIT: is it as simple as (for the first question) that the function is decreasing so the derivative will be below the x-axis and for the second question that the inflection point is at $x=2$ so it is decreasing from 0 to 2 so function will be negative and increasing from 2 to 4 so it will be positive there? But how does it get that shape that makes it look like an $x^3$ graph? it could look like a line, or a parabola or anything, why does it have that specific shape?","['calculus', 'derivatives', 'piecewise-continuity']"
3081649,"Understanding some proofs-without-words for sums of consecutive numbers, consecutive squares, consecutive odd numbers, and consecutive cubes","I understand how to derive the formulas for sum of squares, consecutive squares, consecutive cubes, and sum of consecutive odd numbers but I don't understand the visual proofs for them. For the second and third images, I am completely lost. For the first one I can see that there are $(n+1)$ columns and $n$ rows. I'm assuming that the grey are even and that the white are odd or vice versa? So in order to have an even amount of odds and evens you must divide by two? How can I create an image for the sum of consecutive odd numbers ( $1+3+5+...(2n-1)^2 = n^2$ )","['sums-of-squares', 'algebra-precalculus']"
3081655,"Prove that $\langle\mathbf{A}, \mathbf{C}\rangle \leq \delta$ equals with $\|\mathbf{A}\|_*\leq\delta$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Given an arbitrary matrix $\mathbf{A}\in R^{n\times n}$ and the basis matrix set $\mathbb{S}=\{\mathbf{C}\in R^{n\times n}: \mathbf{C}^T\mathbf{C}=\mathbf{I}_n\}$ .","['matrices', 'calculus', 'linear-algebra']"
3081674,Definition of ergodic map,"I ask the similar question before. About definition of Ergodic theorem .
Now just sincerely ask another fundamental problem about the definition of ergodic map. The following definition is what I read in my textbook: Let $(X,\mathcal{A},\mu)$ be a probability space. Let $T: X\rightarrow X$ is $\mu$ -invariant ( $\mu$ -preserving). Then $T$ is ergodic if for every $E\in \mathcal{A}$ with $T^{-1}(E) = E$ if and only if $\mu(E)=0$ or $1$ . In this definition, it requires $T^{-1}(E) = E$ . However, in the following paper, it seems to use $T(E) = E$ as the definition. https://www.tandfonline.com/doi/abs/10.1080/10236190601045788 (p.1154, Def 2.16, please also see "" $A$ is invariant, i.e. $T(A)=A$ on the top of that page."") My questions are: What is the difference between them? or is the definition in the paper not correct? For the definition by using $T^{-1}(E) = E$ , does it imply $E$ is not an attractor? since the only initial condition such that $T(x_0) \in E$ is $x_0 \in E$ instead of $x_0\in X\setminus E$ . So if we want to discuss $E$ being an attractor, we have to use $T(E) = E$ since this could imply the possibility that $T(x)\in E$ for $x\in X\setminus E$ . Do I make sense? Thanks in advance.","['measure-theory', 'operator-theory', 'ergodic-theory']"
3081675,Compute $ \lim\limits_{n \to \infty}\frac{\sqrt{3n^2+n-1}}{n+\sqrt{n^2-1}}$,"Compute $$ \lim\limits_{n \to \infty}\frac{\sqrt{3n^2+n-1}}{n+\sqrt{n^2-1}}$$ I did the following: $$ \lim\limits_{n \to \infty}\frac{\sqrt{\frac{3n^2}{n^2}+\frac{n}{n^2}-\frac{1}{n^2}}}{\frac{n}{n^2}+\sqrt{\frac{n^2}{n^2}-\frac{1}{n^2}}} = \frac{\sqrt{3}}{\sqrt{1}}=\sqrt3$$ However, the correct answer is different. Why am I wrong? Thank you for your help.","['limits', 'calculus']"
3081698,Creating an integral to represent the volume of the intersection of two balls in cartesian coordinates,The question states: Let $A$ be the intersection of the balls $x^2+y^2+z^2\leq 9$ and $x^2+y^2+(z-8)^2\leq 49$ I am asked to just set up the iterated triple integral that represents the volume of $A$ in cartesian coordinates. What I am able to determine so far is that for the equation $x^2+y^2+z^2\leq 9 $ : $-\sqrt{9-x^2-y^2}\leq z \leq \sqrt{9-x^2-y^2}$ $-\sqrt{9-x^2}\leq y \leq \sqrt{9-x^2}$ $-3\leq x \leq 3$ If I were to set up this integral it would be: $\int_{-3}^{3}\int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \int_{-\sqrt{9-x^2-y^2}}^{\sqrt{9-x^2-y^2}} dzdydx$ But I don't know how I'm supposed to set up the intersection of the two balls? I was thinking of setting the two equations equal to each other so that $x^2+y^2\leq 9-z^2$ and $x^2+y^2\leq 49-(z-8)^2$ so then I have $9-z^2=49-(z-8)^2$ Solving for $z$ I get $z=3/2$ but I don't know what to do with this information.,"['multivariable-calculus', 'multiple-integral']"
3081738,Exponential map is smooth,"Let $G$ be a lie group, then it is claimed that the exponential map $$\exp: Lie(G) \rightarrow G$$ is a smooth map. I want to know - what is the smooth structure of $Lie(G)$ ? I am supposing it is identified with $T_eG$ and we show that $T_eG$ is an embedded submanifold of $TG$ .","['lie-algebras', 'lie-groups', 'differential-geometry']"
3081752,Proof without words of $\oint zdz = 0$ and $\oint dz/z = 2\pi i$,"I found this visual ""proof"" of $\oint zdz = 0$ and $\oint dz/z = 2\pi i$ quite compelling and first want to share it with you. But I have a real question, too, which I will ask at the end of this post, so please stay tuned. Consider the unit circle $C_1$ in $\mathbb{C}$ with $n$ equally distributed numbers $z_k = e^{i2\pi k/n}$ . Draw from each $z_k$ an arrow to $z_k + f(z_k)$ for $f(z) = z$ resp. $f(z) = 1/z$ . To make the connection between $z$ and $1/z$ more comprehensible I plot the graphs $f_p(z) = pz + (1-p)/z$ for some values of $0 \leq p \leq 1$ . Note that $f_1(z) = z$ and $f_0(z) = 1/z$ . [click image to enlarge] Note that these plots are essentially stream plots a) restricted to the unit circle and b) displaying not only the direction of the vector field but also its magnitude. Note that by $f_p(-z) = -f_p(z)$ we have $\sum_{k=0}^{n-1}f_p(z_k) = 0$ for all $p$ which can more easily be seen when drawing $f_p(z)$ from the origin: Note the difference in color between $f_1(z) = z$ and $f_0(z) = 1/z$ which is due to different phases $\varphi = 2\pi k/n$ of the arguments. Now let's consider the factor $d_nz$ , which is the secant (for $n\rightarrow \infty$ the tangent) vector of the circle at point $z$ . Multiplying $f(z)$ by $d_nz$ shrinks and rotates $f(z)$ . Because $|d_nz|\rightarrow 0$ for $n\rightarrow\infty$ , we would have $|f(z)d_nz| \rightarrow 0$ , too, so to keep the vector $f(z)d_nz$ visible, we normalize $d_nz$ by division by $|d_nz| = 2\pi/n$ . So, only the rotation of $f(z)$ is in the focus: Now we are already done: For $f(z) = f_1(z) = z$ the vectors $f(z_k)dz$ still sum up to $0$ , while for $f(z) = f_0(z) = 1/z$ we have $n$ vectors of length $1$ all pointing into the same direction (up), so there sum is just $n\cdot i$ . Multiplying by the normalization factor $2\pi/n$ we get the desired result: $$\sum_{k=0}^{n-1}f_0(z_k)d_nz = 2\pi i$$ which holds for arbitrary $n$ . Put algebraically, one finds with $\zeta_k = e^{i2\pi k/n}$ the $n$ -th roots of unity, $d_k = \zeta_k e^{i\pi / 2}$ , and $1/\zeta_k = \overline{\zeta_k}$ : $$\sum_{k=0}^{n-1}\zeta_k d_k = \sum_{k=0}^{n-1}\zeta_k^2 e^{i\pi / 2} = i \sum_{k=0}^{n-1}\zeta_k^2 = i \sum_{k=0}^{n-1}\zeta_k = 0$$ $$\sum_{k=0}^{n-1}\overline{\zeta_k} d_k = \sum_{k=0}^{n-1}\overline{\zeta_k}\zeta_k e^{i\pi / 2} = i \sum_{k=0}^{n-1}1 = i \cdot n$$ Again, to see this more clearly, we can draw $f(z)d_nz$ from the origin: Note that the single point for $p=0$ , i.e. $f_0(z) = 1/z$ , represents in fact $n$ points, all at the same position. The proof sketched here was somehow ""synthetic"" or geometrical. It's interesting to compare it with the analytic proof: $$\oint_{C_1} dz/z = \int _0 ^{2 \pi} \dfrac{i e^{i z}}{e^{i z}} dz = \int _0 ^{2 \pi}i dz = 2 \pi i $$ and the proof by Cauchy's residue theorem $$\oint_{C_1} dz/z = 2\pi i \operatorname{I}(C_1,0)\operatorname{Res}(1/z,0)= 2 \pi i$$ with $\operatorname{I}(C_1,0) = 1$ the winding number of $C_1$ around $0$ and $\operatorname{Res}(1/z,0) = 1$ the residue of $f(z)=1/z$ at $0$ . What I don't see in the pictures above is, what the value of the integral has to do with the existence of a pole at $0$ , so my question is: By which kind of ""long-range interaction"" does the singularity at $0$ ""force"" the vectors $dz/z$ to rotate exactly the way
  they do, summing up to $2\pi i$ ? Addendum 2 Dirk asked for pictures for $f(z) = 1/z^2$ . Here they are for $z \rightarrow z + f(z)$ , $z \rightarrow z + f(z)d_nz$ , , $0 \rightarrow f(z)d_nz$ : For the sake of comparison here are the plots for $f(z) = z^k$ , $k = 1,2,3,4$ (upper rows) and $f(z) = 1/z^k$ (lower rows): Note the special character of the bottom left plot in the second picture which is the one that relates to the residue of $z^{-1}$ at $z=0$ being $1$ . All other residues of $z^{\pm k}$ at $z=0$ are $0$ . Note also how derivation acts as rotation. Addendum 1 For the sake of comparison: The same visual proof works – even though not so easy to see at a glance – for other closed curves, e.g. a cardioid $\gamma$ . In the first table you'll see $z \rightarrow z + f(z)$ , in the second $z \rightarrow z + f(z)d_nz$ , in the third $0 \rightarrow f(z)d_nz$ . With a little experience you will ""see"" that $\oint_\gamma zdz = 0$ and $\oint_\gamma dz/z = 2\pi i$ And here for the diamond curve: the tables for $z \rightarrow z + f(z)$ , $z \rightarrow z + f(z)d_nz$ , and $0 \rightarrow f(z)d_nz$ The rotation of $f(z)$ as induced by multiplication by $dz$ can be seen as a ""process"" in stop motion: Note that in these two cases – based on the circle – $f(0)$ is rotated by $\pi/2$ , $f(e^{i\pi/2})$ by $\pi$ , $f(e^{i\pi})$ by $-\pi/2$ , and $f(e^{i3\pi/2})$ by $0$ .","['integration', 'visualization', 'complex-analysis', 'contour-integration', 'residue-calculus']"
3081761,"Drawing balls of an urn, probability of one colour run out (with replacement)","I am trying to solve a probability problem but I do not manage to figure out a solution. I implemented the problem in C++ to convince myself of a solution but it didn't help (btw it shows that some colors run out with a low probability) Here is the problem: ""In a Urn, there are 17 red balls, 15 blue balls and 13 yellow balls. Each time randomly pick two balls, if they are in the same color, return both of them to the urn, if they are in different colors, they replace them with 2 balls in the third color. Will we run out of one of the color of balls?"" If you have any hint to help. I would be glad to hear any of them. Thank you EDIT: (Thanks to Daniel Mathias, adding a precision on the different outcomes: each state can still remain unchanged) Can we take an example with smaller numbers of balls and extend it to the previous problem? Let's say we have 1 red ball; 3 blue balls and 5 yellow balls. Then we have the couple (1,3,5) -- state 0 The different outcomes of the state 0 are: (0,2,7) -- game ends (0,5,4) -- game ends (3,2,4) -- state 1 (1,3,5) -- return state 0 Then the outcomes of the state 1 are: (5,1,3) -- return state 0 (2,4,3) -- return state 1 (2,1,6) -- state 2 The outcomes of the state 2 are: (4,0,5) -- game ends (1,0,8) -- game ends (1,3,5) -- return state 0 Then it is clear that the probability of one of the color of balls runs out is different from 0.","['combinatorics', 'probability']"
3081774,Second difference on the sum of extreme values of square numbers,"I notice that in a series of consecutive square number $$1,4, 9, 16, 25, 36, 49, 64, 81, 100$$ if i add up the first element to the last element as well as the second element to the second to the last element i come up with the following result: $$101, 85, 73, 65, 61$$ the eventually get the absolute difference between 2 consecutive sum i have $$16, 12, 8, 4$$ by getting the second difference of this i got $$4,4,4,4$$ is this true to all $n^{2}?$ This pattern holds true even i do not start with $1^{2}$ , that is for even number of squares, in case of odd number of squares i.e from $1^{2}$ to $9^{2}$ , i can simply multiply the median, that is $5^{2}$ , by two then do the same process the pattern still holds. I tried to prove this by letting $$n^{2}, (n +1)^{2}, (n + 2)^{2}, ..., (n+k)^{2}, (m - k)^{2}, . . .,(m - 1)^{2}, m^{2}$$ as i add both ends and perform subtraction among consecutive sums i got $$ (n+k)^{2} +(m - k)^{2} - 2((n+k)^{2} + (m - 1)^{2} + . . . +(n +1)^{2} +  (m - 1)^{2}) - n^{2} - m^{2}$$ but unfortunately i got stuck since I cant express sum of consecutive squares as single term,,, the internet say that its n(2n + 1)(n + 1)/6 but i cant connect this formula using expressions...any idea how to do this?",['number-theory']
3081790,Filtered colimits in the category of sets: an equivalence relation,"A category $\mathsf{I}$ is filtered if it is nonempty, for any $i,j \in \mathsf{I}$ there is $k \in \mathsf{I}$ and morphisms $f\colon i\to k$ and $g\colon j\to k$ , for any pair $f,g\colon i\to j$ of parallel morphisms in $\mathsf{I}$ there is $k \in \mathsf{I}$ and a morphism $h\colon j\to k$ so that $h\circ f = h\circ g$ ; equivalently, $\mathsf{I}$ is filtered if for any finite diagram in $\mathsf{I}$ admits a cocone. Let $\mathsf{I}$ be a small filtered category and let $F\colon\mathsf{I}\to\mathsf{Set}$ be a functor. I want to prove that if $\mathsf{I}$ is filtered, then the relation $\sim$ on $\bigsqcup_{i \in \mathsf{Ob(I)}} F(i)$ so that $(i,x) \sim (j,y)$ precisely when there is an object $k$ of $\mathsf{I}$ together morphisms $f\colon i\to k$ and $g\colon j\to k$ so that $F(f)(x) = F(g)(y)$ is an equivalence relation. In particular, I don't know how to prove that it is a transitive relation. Borceux in his book ""Handbook of Categorical Algebra I"" attempts to prove it this way: If $(i,x) \sim (j,y)$ and $(j,y) \sim (k,z)$ , there are $k',k'' \in \mathsf{I}$ together with morphisms $f\colon i\to k', g\colon j\to k', f'\colon j\to k''$ and $g'\colon k\to k''$ so that $F(f)(x) = F(g)(y)$ and $F(f')(y) = F(g')(z)$ . We can consider a finite category $\mathsf{J}$ consisting of objects $i,j,k,k',k''$ and morphisms $f,g,f',g'$ (aside from identity morphisms) and diagram $D\colon \mathsf{J}\to\mathsf{I}$ which the must have a cocone. I'm having trouble with his construction of the finite category $\mathsf{J}$ , as it seems to implicitly assume that neither pair of $f,g,f',g'$ is composable in $\mathsf{I}$ . However, what if, for example, we have $k' = j$ ? Or $k = k''$ ? The category $\mathsf{J}$ then would have to contain the compositions as well. If we define it as a subcategory of $\mathsf{I}$ containing said objects and morphisms, then it doesn't have to finite.","['proof-writing', 'proof-verification', 'relations', 'category-theory', 'elementary-set-theory']"
3081819,Probabilistic models problem,"Problem:
The probability of a player making a free throw is 0.6. Find the probability that the player makes the first at least 7 consecutive free throws. The problem is that I have a dilemma: the requirement sounds for me that I need to use the formula from Poisson Model, but in that case, number of successes will be 8 (because ""at least 7""), and I don't know how to find the number of trials, it will be also 8? Is it correct what I suppose?","['probability-theory', 'probability']"
3081821,Find the area of parallelogram and its missing vertex,"Given three radius-vectors: $OA(5; 1; 4), OB(6;2;3), OC(4;2;4)$ , find the missing vertex $D$ and calculate the area of obtained parallelogram. My attempt: Firstly, we are to find the vectors which form the parallelogram. $$AB = OA - OB = (-1; -1; 1)$$ $$AC=OA-OC=(1; -1; 0)$$ Since the parallelogram is spanned by $AB, AC$ , cross product and the norm of resulting vector should be then calculated. \begin{align*}
  &AB \times AC=
  {\begin{vmatrix}
    \vec{i} & \vec{j} & k \\ 
    -1 & -1 & 1 \\ 
    1 & -1 & 0 
  \end{vmatrix}} = \sqrt{1^2 + 2^2 + 1^2} = \sqrt{6}
\end{align*} $D$ is equidistant from $E$ and $B$ , where $E$ is the point of diagonal's intersection. $$x_E = \frac{x_A+x_C}{2} = 9/2$$ $$y_E = \frac{y_A+y_C}{2} = 3/2$$ $$z_E = \frac{z_A+z_C}{2} = 4$$ \begin{cases} x_B - x_E = x_E - x_D\\ y_B - y_E = y_E - y_D \\ z_B - z_E = z_E - z_D \end{cases} After plugging and solving, the final answer would be $\sqrt{6}$ and $D(3;1;5).$","['analytic-geometry', 'linear-algebra', 'geometry']"
3081828,What is the motivation of defining weak derivative as it is?,"I've been reading lately about reproducing kernel Hilbert spaces (RKHS) and Gaussian processes (GP) and during my studies I came across with the concept of weak derivative and Sobolev spaces. I have tried to get some sense from weak derivative by reading these two questions at the site: What is the intuition behind a function being 'weakly differentiable'? Intuition about weakly differentiable functions So I have learned now that: A function $u:\Omega\rightarrow\mathbb{R}$ is weakly differentiable
  with $\alpha$ th ( $\alpha\in\mathbb{N}^n$ ) weak partial derivative $v=D^{\alpha}u$ , if for all compactly supported smooth functions $\phi$ it holds that: $$\int_{\Omega} u \,D^{\alpha}\phi\,dx =
 (-1)^{|\alpha|}\int_{\Omega}v\phi\,dx.$$ Quoting earlier answers (by user Christopher A. Wong) from the links I provided above: The basic intuition is that a weakly differentiable function looks
  differentiable except for on sets of zero measure. This allows
  functions that are not normally considered differentiable at ""corners""
  to have a weak derivative that is defined everywhere on the original
  function's domain. The reason why weak derivatives ignore sets of zero
  measure is precisely because weak derivatives are defined by
  integrals, and integrals cannot see behavior on sets of zero measure. Now I'm almost satisfied with this answer, but one unclarity remains: Why is the weak derivative defined as it is? That is, when the idea of weak derivative was first considered, whoever it was, why did he select that particular equation above as the definition? Or was this definition a somewhat arbitrary choice, which simply suited to our purposes? For example, one of the reasons of using squared error in many problems of statistics or optimization is (as I've understood) because the squared error has nice analytical properties, making the math easy, when compared with e.g. absolute error. The reason why I'm asking this, is that sometimes it seems many techniques of mathematics rise like from a magicians hat. For a beginner like myself, it is difficult to picture the scenario and conditions that gave rise to that discovery or definition.","['measure-theory', 'sobolev-spaces', 'weak-derivatives']"
3081843,Definite Integration ( a little query),"$$\int_0^π \frac{xdx}{a^2\cos^2x+b^2\sin^2x} \,dx$$ Using property $$\int_a^b f(x) \,dx= \int_a^b f(a+b-x) \,dx$$ (i can't write it correctly,please check it) I get, $2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx$ On dividing numerator and denominator of R.H.S by $\cos^2x$ I get, $2I=\pi\int_0^\pi \frac{\sec^2xdx}{a^2+b^2\tan^2x} \,dx$ Now, solving by substitution method  (taking $b\tan x=t$ ) I get (i have added the image because i was not able to type this correctly) As the upper limit and lower limit on the  function are zero So, answer should be zero. But in the solution ( after getting this $2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx$ )they have used the property $$\int_0^2a f(x) \,dx= 2\left(\int_0^a f(x) \,dx\right)$$ Why they didn't ended the solution in the direction in which i did pardon for my mathjax errors","['integration', 'calculus', 'definite-integrals']"
3081844,A sequence of bounded $C^1$ functions whose derivatives are unbounded.,"What is an example of a sequence of functions, $(f_n)_{n=1}^\infty\subset C^1([a,b])$ , which are bounded in $C^1([a,b])$ under $\|\cdot\|_{\infty}$ but are such that their first derivatives $\|f'_n\|_{\infty}\to\infty$ as $n\to\infty$ ? There are many examples on this site for a single function, but I am looking for an intance when we have a sequence of functions. Are there any particularly simple instances which can be provided to demonstrate the above point?","['banach-spaces', 'normed-spaces', 'calculus', 'functions', 'functional-analysis']"
3081850,Is the following application of the CLT correct?,"I am new to probability theory and still have a problem understanding some things. I came across the following problem that I am unsure on how to solve. Here it is: Suppose that in a city with $100,000$ cars a shop sells tires for cars. It has been observed that in an interval of $3$ months the percentage of cars that come to the shop for the replacement of all their tires is $0.5$ %. What is the least number of tires that the shop has to order so that it can satisfy all its customers in a $3$ month interval with probability $\geq 95$ %? And here is my attempt: Let $X$ be the random variable of cars coming at the shop in a $3$ month interval. Then $X$ is a discrete random variable following binomial distribution $B(100,000; 0.005)$ . We would like to find the least $x\in\mathbb{N}$ satisfying $P(X\leq x)\geq95$ %. Since (I assume) every car has $4$ tires, our answer is going to be $4x$ . Now since we have a large number of cars, by the Central limit theorem, we have that $X\sim N(\mu,\sigma^2)$ , where $\mu, \sigma^2$ are the expected value and the variance of $B(100,000; 0.005)$ and therefore, by calculating, it is $\mu=500, \sigma^2=475$ . Now we have $P(X\leq x)=P\big{(}\displaystyle{\frac{X-500}{\sqrt{475}}\leq\frac{x-500}{\sqrt{475}}\big{)}=P\big{(}Z\leq\frac{x-500}{\sqrt{475}}\big{)}}$ , where $Z\sim N(0,1)$ . Now our $x$ will satisfy $\displaystyle{P\big{(}Z\leq\frac{x-500}{\sqrt{475}}\big{)}}=0.95$ , therefore $\displaystyle{P\big{(}0<Z\leq\frac{x-500}{\sqrt{475}}\big{)}}=0.45$ hence (by the tables) it is $\displaystyle{\frac{x-500}{\sqrt{475}}=1.645}$ , which yields $x=535.85..$ and since $x$ is supposed to be an integer we have that $x=536$ . therefore the least number of tires is $2144$ . Is my solution correct? if not, what should I have used? Comment: I know that the $x=$ fractional is not correct since i specified that $x\in\mathbb{N}$ , but you get the point.","['statistics', 'probability-theory', 'probability']"
3081856,Are segments in a Partially ordered space closed/open?,"I've been trying to figure out what I can say about the following definition: https://en.wikipedia.org/wiki/Partially_ordered_space A topological space $X$ with a partial order $\leq$ , is called a partially ordered space if it's graph $Gr(\leq)\subset X\times X$ , is a closed subset of $X\times X$ . Are the following sets open/closed in $X$ ?: $l(x)=\{y \in X: x\leq y  \}$ and $r(x)=\{y \in X: y\leq x  \}$ In general, for a topological space $X$ with a binary relation whose graph is closed, can I say that $l_R(x)=\{ y\in X: (x,y)\in R  \}$ or $r_R(x)=\{ y\in X: (y,x)\in R  \}$ are closed/ open?","['order-theory', 'general-topology']"
3081886,Alternative proof that $U(n^2-1)$ is not cyclic for $n>2$.,"I'm reading ""Contemporary Abstract Algebra,"" by Gallian. This is Exercise 4.84 ibid. and I want to solve it using only the tools available in the textbook so far . (A free copy of the book is available online.) Notation: The group $$(\{a\in\Bbb Z_m\mid \gcd (a, m)=1\}, \times_m)$$ of units modulo $m$ under multiplication $\times_m$ (or concatenation) modulo $m$ is denoted $U(m)$ . The Question: For every integer $n$ greater than $2$ , prove that the group $U(n^2-1)$ is not cyclic. Thoughts: I'm aware that $n^2-1=(n-1)(n+1)$ as a difference of two squares. That $U(n^2-1)$ is cyclic for $n=2$ is clear by direct computation. External methods (e.g., ideas of proofs that rely on, say, anachronistic techniques): The result that $U(m)$ is cyclic iff $m$ is $2, 4, p^k, 2p^k$ for prime $p>2$ is not yet established in the text (I think). (What I suspect is) the lemma that if a group $G$ contains at least four distinct elements $x\in G$ such that $x^2=e$ , then $G$ is not cyclic, is not clear to me; it's not established in the textbook so far and, yeah, I can sort of see why it's true (as the Klein four group is, intuitively, a (non-cyclic) subgroup of $G$ ). Please feel free to prove this lemma. It'd be sufficient, for me to understand the problem at hand. The Chinese Remainder Theorem is not established yet (but it is on page 347; I'm up to page 92). The theorem that $U(m)$ is cyclic iff $\varphi(m)=\lambda(m)$ is not established yet. (Here $\varphi$ is Euler's totient function and $\lambda$ is the Charmichael function.) In fact, I don't think it's mentioned at all (but I haven't looked very hard). Please help :)","['alternative-proof', 'group-theory', 'cyclic-groups', 'modular-arithmetic']"
3081918,A series whose convergence is equivalent to the Riemann hypothesis,"It was claimed here that the convergence of the series $$\sum_{n=2}^\infty \frac{\Lambda(n)-1}{n^{1/2}\log^3 n}\tag1$$ (where $\Lambda$ is the Von Mangoldt function ) is equivalent to the Riemann hypothesis. Is this true? That post provided a link to the Wikipedia article about the Von Mangoldt function, which does not mention this. Also, this page about the Von Mangoldt function in the context of the Riemann hypothesis makes no mention to that. If it is true that the convergence of the series $(1)$ is equivalent to the Riemann hypothesis, then I would like to have a reference for that.","['riemann-hypothesis', 'reference-request', 'sequences-and-series']"
3081930,What is the number of elements in the solution set of $(x^2-4)^2\cdot(x^2-6x-7)=0$?,"$(x^2-4)^2\cdot(x^2-6x-7)=0$ $S.S.=\{x_1,x_2,...,x_n\}$ $\Rightarrow n=?$ Answer is given as $4$ . I think it should be $6$ because of multiplicity of the roots. I debated this problem with my classmates but we can't reach a verdict. What is the number of elements in the Solution Set of this problem?","['algebra-precalculus', 'quadratics']"
3081951,On anti-derivative of functions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $f,g,h: \mathbb R \to \mathbb R$ be differentiable functions. (1) Does there necessarily exist a differentiable function $F: \mathbb R \to \mathbb R $ such that $F'=\max \{f' ,g' \}$ ? (2) Does there necessarily exist a differentiable function $G: \mathbb R \to \mathbb R $ such that $G '=\max \{f',g',h'\}$ ?","['riemann-integration', 'derivatives', 'real-analysis']"
3081960,Closed form for product of Stirling numbers of the second kind,"What does the following expression evaluate to: \begin{equation}
\sum\limits_{k=1}^n \dbinom{n}{k} \cdot k! \begin{Bmatrix} n \\ k \end{Bmatrix} \cdot k! \begin{Bmatrix} n \\ k \end{Bmatrix} 
\end{equation} We know that $k! \begin{Bmatrix} n \\ k \end{Bmatrix} = n![x^n]:(e^x-1)^k$ ,  where $[x^k]:f(x)$ represents the coefficient of $x^k$ in the power series for $f(x)$ . I was wondering if squaring $\left(\text{i.e., } k! \begin{Bmatrix} n \\ k \end{Bmatrix} \cdot k! \begin{Bmatrix} n \\ k \end{Bmatrix}\right)$ takes us to a different power series or just to a different coefficient in the same power series? I am looking for some clean closed form. A related expression: \begin{equation}
\sum\limits_{k=1}^n \dbinom{n}{k} \cdot k! \begin{Bmatrix} n \\ k \end{Bmatrix} 
\end{equation} is proven to be equal to $n^n$ in this answer https://math.stackexchange.com/q/3076350 . Note: The series $1,6,147,6940,536405,62352066, \dots$ is not on oeis.org","['power-series', 'binomial-coefficients', 'combinatorics', 'stirling-numbers']"
3081971,Resolving a rational system of equations with too many unkowns,"A little bit of context. While working a larger proof (the proof is quite related to this question I asked), I stumbled upon the following problem. The question. Can we find $x_1,\ldots,x_{24}\in\mathbb Q$ such that for all $a,b,c,d\in\mathbb Q$ : $$ (x_1a+x_2b+x_3c+x_4d)(x_5a+x_6b+x_7c+x_8d)-(x_{9}a+x_{10}b+x_{11}c+x_{12}d)(x_{13}a+x_{14}b+x_{15}c+x_{16}d)-(x_{17}a+x_{18}b+x_{19}c+x_{20}d)(x_{21}a+x_{22}b+x_{23}c+x_{24}d)$$ $$=7a^2-b^2-c^2-d^2\quad ?$$ What I tried. I have tried to do a systematic resolution of the underlying system, but this is too massive to compute. I have tried arbitrarily fixing some of the $x_i$ in order to simplify the system, but I always end up in one of the following cases: no solution, irrational solutions, complex solutions. None of this cases is acceptable. Any leads would be greatly appreciated.","['algebra-precalculus', 'systems-of-equations', 'diophantine-equations']"
3082048,Is there an embedding theorem for non-second-countable manifolds?,"It is well known although I don't know how to prove that any second-countable topological manifold of dimension $n$ can be embedded into $\mathbb{R}^{2n}$ (we consider Hausdorff manifolds only). I wonder if there is a similar result, whether positive or negative, for non-second-countable manifolds? Of course there are some very strange non-second-countable manifolds like Prüfer surface which I don't want to count in, so I quote a definition from Handbook of Set-theoretic Topology : Chapter 14, Definition 5.1: A space is $\omega$ -bounded if every countable subset has compact closure. Denote the long line by $L$ . Many basic examples of non-second-countable manifolds, such as $L$ and $L\times L$ are $\omega$ -bounded. In the same chapter
the structure theorem for $\omega$ -bounded surfaces(Theorem 5.14, ""The Bagpipe Theorem"") is proved. So it seems that $\omega$ -boundedness is a suitable condition. Question: Is there a ""nice"" space(a finite dimensional $\omega$ -bounded manifold, if possible) such that every $\omega$ -bounded manifold of dimension $n$ can be embedded into it? I have proved that the space obtained by gluing the diagonal line and $x$ -axis of the first octant of $L\times L$ cannot be embedded into $L^{n}$ for any $n$ , so possibly $L^{n}$ does not work(but the proof is ugly and may be flawed).","['manifolds', 'general-topology', 'set-theory']"
3082052,Confusing Lagrange multipliers question,"Let $a_1,a_2, \dots, a_n$ be reals, we define a function $f: \mathbb R^n \to \mathbb R$ by $f(x) = \sum_{i=1}^{n}a_ix_i-\sum_{i=1}^{n}x_i\ln(x_i)$ , in addition, we are also given that $0 \cdot \ln(0)$ is defined to be $0$ . Find the supremum of $f$ on the domain $\Omega = \{x \in \mathbb R^n: x_i \geq 0, \sum_{i=1}^{n}x_i = 1\}$ . What I did: $\Omega$ is closed and bounded and hence compact, and $f$ is continuous. It follows that $f$ admits a minimum and maximum (which confuses me as to why they asked for supremum and not maximum). Let's look for the maximum when $x_i > 0$ , by solving the system $\begin{cases}a_i-\ln(x_i)-1 = \lambda \\ \sum_{i=1}^{n}x_i = 1 \end{cases}$ . From the first $n$ equations we get $\ln(x_i) = a_i-1-\lambda$ or in other words $x_i = e^{a_i -1 -\lambda} > 0$ Plugging this to the constraint to find $\lambda$ , we get $\lambda = \ln(\sum_{i=1}^{n}e^{a_i}) - 1$ , so $x_i = e^{a_i-\ln(\sum_{i=1}^{n}e^{a_i})}$ is a potential maximum, or minimum, remember $f$ also attains a minimum. Now we need to check what happens when $x_i$ is zero. This gives us an identical problem, but one dimension smaller. After that we need to check when $(x_i,x_j)$ are zero over all pairs. Then over all triples and so forth. Surely this isn't the intended way. What's going on here?","['lagrange-multiplier', 'real-analysis', 'maxima-minima', 'calculus', 'derivatives']"
3082125,Determine if a graph exists knowing the degree of its vertices,"Problem : There is a graph on ten vertices whose degrees are $9,8,8,8,6,5,4,4,2,2$ Answer : False. My attempt : Since we know that $2 | E | = \sum _ { v \in V } d ( v )$ , $$2\cdot 28 = 9+8\cdot3+6+5+4\cdot2+2\cdot2$$ Hence $| E | = 28$ . Since we don't now more about the graph (it may not be a tree), how can I determine that such graph does not exist?","['graph-theory', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
3082128,What's the answer to $\int \frac{\cos^2x \sin x}{\sin x - \cos x} dx$?,"I tried solving the integral $$\int \frac{\cos^2x \sin x}{\sin x - \cos x}\, dx$$ the following ways: Expressing each function in the form of $\tan \left(\frac{x}{2}\right)$ , $\cos \left(\frac{x}{2}\right)\,$ and $\,\sin \left(\frac{x}{2}\right)\,$ independently, but that didn't go well for me. Multiplying and dividing by $\cos^2x$ or $\sin^2x$ . Expressing $\cos^2x$ as $1-\sin^2x$ and splitting the integral, and I was stuck with $\int \left(\frac{\sin^3x}{\sin x - \cos x}\right)\, dx$ which I rewrote as $\int \frac{\csc^2x}{\csc^4x (1-\cot x) } dx,\,$ and tried a whole range of substitutions only to fail. I tried to substitute $\frac{1}{ \sin x - \cos x}$ , $\frac{\sin x}{ \sin x - \cos x}$ , $\frac{\cos x \sin x}{ \sin x - \cos x}$ and $\frac{\cos^2x \sin x}{\sin x - \cos x},$ independently, none of which seemed to work  out. I expressed the denominator as $\sin\left(\frac{\pi}{4}-x\right)$ and tried multiplying and dividing by $\sin\left(\frac{\pi}{4}+x\right)$ , and carried out some substitutions. Then, I repeated the same with $\cos\left(\frac{\pi}{4}+x\right)$ . Neither of them worked.","['integration', 'real-analysis', 'trigonometric-integrals', 'indefinite-integrals', 'substitution']"
3082143,How do cosets form a group?,"Let G be a group and H be a Subgroup . Then if H is normal then the cosets form a group . I cannot understand what is meant by cosets form a group . To from a group , group operation on two elements must lie inside the group.So when we say that cosets form a group , group operation on any two elements from any two cosets must lie inside the two cosets chosen earlier or the set of cosets (which is group G itself) ?","['group-theory', 'abstract-algebra']"
3082161,"Given three permutations of $\{1,2,\dots,n^3+1\}$, prove two of them have a common subsequence of length $n+1$.","Let $m = n^3 + 1$ and let $\sigma_1, \sigma_2, \sigma_3$ 3 permutations of $\{{1,2,...m}\}$ . Prove that two of these permutations have same subsequence which are $n+1$ long. I have tried to use the Erdos-Szekeres theorem (every permutation of $n^3+1$ has monotonically increasing subsequence in $n^2+1$ long or monotonically decreasing subsequence in $n+1$ ) but I didn't have any idea to proceed. Any help will be appreciated.","['pigeonhole-principle', 'combinations', 'combinatorics']"
3082217,Geometric justification of a rotation matrix,"From S.L Linear Algebra: We can define a rotation in terms of matrices. Indeed, we call a linear map $L: \mathbb{R}^2 \rightarrow
 \mathbb{R}^2$ a rotation if its associated matrix can be written in
  the form: $$\begin{pmatrix} \cos(\theta) & -\sin(\theta) \\  \sin(\theta) & \, \
 \cos(\theta)  \end{pmatrix}$$ The geometric justification for this definition comes from Fig. 1. We see that: $$L(E^1) = (\cos \theta)E^1 + (\sin \theta)E^2$$ $$L(E^2) = (-\sin \theta)E^1 + (\cos \theta)E^2$$ Thus our definition corresponds precisely to the picture. When the
  matrix of the rotation is as above, we say that the rotation is by an
  angle $\theta$ . For example, the matrix associated with a rotation by an angle $\frac{\pi}{2}$ is: $$R(\frac{\pi}{2})=\begin{pmatrix} 0 & -1 \\  1 & \, \, \, 0
 \end{pmatrix}$$ Linear Transformation Perspective : I think that $L(E^1)$ and $L(E^2)$ are basis for the column space of the matrix $A$ (hence the basis for image under linear transformation $L$ ). It is known, that $L=AX$ where $A$ is the matrix associated with $L$ and $X=(x_1, x_2)$ is input of $L$ 's definition. Also $AX=b$ where $b$ is the element of 2-dimensional image subspace (correct?). On the basis thereof, I think we get: $$\begin{pmatrix}
a_{11} & a_{12} \\ 
a_{21} & a_{22}
\end{pmatrix}\begin{pmatrix}
x_1 \\
x_2 
\end{pmatrix}=\begin{pmatrix}
b_1 \\
b_2 
\end{pmatrix}$$ where $A=\begin{pmatrix}
a_{11} & a_{12} \\ 
a_{21} & a_{22}
\end{pmatrix}=\begin{pmatrix} \cos(\theta) & -\sin(\theta) \\  \sin(\theta) & \, \
 \cos(\theta)  \end{pmatrix}$ For example, $\cos(\theta)x_1 + \sin(\theta)x_2=b_1$ which seems to equivalent of $L(E^1) = (\cos \theta)E^1 + (\sin \theta)E^2$ . Geometry Perspective (problem is here) : This is where it gets confusing for me, $E_1$ and $E_2$ from the figure 1 look like unit vectors in the $x$ and $y$ direction respectively. If so, is there a proof that $||E_1||=||x_1||=1$ and that $||E_2||=||x_2||=1$ , if not, what do they represent? Furthermore, I'm aware from basic trigonometry that sine function represents a vertical leg of triangle in the unit circle, whereas cosine represents a horizontal one, does this have to do anything with the figure 1? In short: Is there any deeper explanation of geometric justification above? I'm unable to understand it completely. Thank you!","['matrices', 'trigonometry', 'linear-algebra', 'linear-transformations']"
3082233,Show that two Markov kernels almost surely agree,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(E_i,\mathcal E_i)$ be a measurable space $X_1:\Omega\to E_1$ $X_2:\Omega\to E_2$ be $(\mathcal A,\mathcal E_2)$ -measurable $\kappa$ be a Markov kernel with source $(E_1,\mathcal E_1)$ and target $(E_2,\mathcal E_2)$ with $$\operatorname P\left[X_2\in B_2\mid X_1\right]=\kappa(X_1,B_2)\;\;\;\text{almost surely for all }B_2\in\mathcal E_2\tag1$$ Now, assume $\tilde\kappa$ is another Markov kernel with source $(E_1,\mathcal E_1)$ and target $(E_2,\mathcal E_2)$ with $(1)$ . Are we able to conclude $$\tilde\kappa(x_1,B_2)=\kappa(x_1,B_2)\;\;\;\text{for }\operatorname P\circ\;X_1^{-1}\text{-almost all }x_1\in E_1\text{ and }B_2\in\mathcal E_2\tag2?$$ I guess we need to assume that $\mathcal E_2$ is countable generated, i.e. there is a $\mathcal G_2\subseteq\mathcal E_2$ with $|\mathcal G_2|\le|\mathbb N|$ and $\sigma(\mathcal G_2)=\mathcal E_2$ . Then, by $(1)$ , there is a $N\in\mathcal A$ with $\operatorname P[N]=0$ and $$\tilde\kappa(X_1(\omega),B_2)=\kappa(X_1(\omega),B_2)\;\;\;\text{for all }(\omega,B_2)\in(\Omega\setminus N)\times\mathcal G_2\tag3.$$ Let $$\tilde E_1:=\bigcap_{B_2\in\mathcal G_2}\left\{x_1\in E_1:\tilde\kappa(x_1,B_2)=\kappa(x_1,B_2)\right\}.$$ Since $\mathcal G_2$ is countable, $\tilde E_1\in\mathcal E_1$ and $$\operatorname P[X_1\in\tilde E_1]\ge\operatorname P\left[(\Omega\setminus N)\cap\left\{X_1\in\tilde E_1\right\}\right]=1\tag4.$$ Now, we know that a finite measure is uniquely determined by its values on a $\cap$ -stable generator. So, the only remaining question is: Do we find a $\cap$ -stable $\mathcal G_2$ ? I guess we can simply go over to $$\tilde{\mathcal G}_2:=\left\{\bigcap\mathcal H_2:\mathcal H_2\subseteq\mathcal G_2\text{ with }|\mathcal H_2|\in\mathbb N\right\}.$$","['markov-process', 'measure-theory', 'probability-theory']"
3082255,Limits with Taylor series around zero,"I had some problems with the following two limits, which are supposed to be calculated with Taylor series: $$
\lim_{x\to 0^+}\frac{e^\sqrt{x}-e^{-\sqrt{x}}}{\sqrt{\sin{2x}}}\quad\mbox{and}\quad
\lim_{x\to 0^+}\frac{(1-\log{x})^{\sin{x^2}}}{(\arctan{x})^{3/2}}.
$$ Although the numerators are quite simple to develop in series, I stopped when I noticed that both denominators are not derivable in $x=0$ , that is, we should not use Taylor series in $x=0$ to evaluate this functions around $0$ . I wonder if is possible to consider right derivatives only, and study the behaviour of the denominators in a right neighborhood of $0$ . Thank you in advance for your help!","['indeterminate-forms', 'limits', 'taylor-expansion']"
3082262,"Does $|f^\prime|<1$ imply that $\forall_{x,y}|f(x)-f(y)|<|x-y|$?","I have a task that I think reduces to proving that $f$ is a contraction mapping. We know that $\forall_x|f^\prime(x)|<1$ . Therefore if I could prove that $|f^\prime|<1\implies|f(x)-f(y)|<|x-y|$ then I think the task would be solved. I feel this property does hold and that it is also somewhat obvious. Unfortunately, saying that something is obvious is obviously not a valid proof. I have feeling proving this belongs to an elementary course on analysis, but it is somehow surprising how much have I forgotten from this course... How to prove this property and does it even hold?","['normed-spaces', 'derivatives', 'real-analysis']"
3082337,Is the condition $x\in\mathbb{R}$ necessary to the set statement $\{x \in\mathbb{R} \vert x> 0\}$?,"Forgive my ignorance. Is the condition $x\in\mathbb{R}$ necessary to the set statement $\{x \in\mathbb{R} \vert x> 0\}$ ? In other words, if $x$ is greater than zero, then is it not, by definition, a real number? Thank you very much!","['elementary-set-theory', 'logic']"
3082385,"Prove or find a counterexample $\forall x>0: f(2x)-f(x)<g(3x)-g(2x)$, given information about $f, g$.","Let $f,g:(0, \infty) \rightarrow \mathbb{R}$ be two functions that satisfy the following for all $x>0$ : $g'(x)>f'(x)$ and $f''(x)>0$ . Prove or find a counterexample: $$
\forall x>0: f(2x)-f(x)<g(3x)-g(2x).
$$ I've tried this for a long time, but I didn't make much progress. I tried to move everything to one side and then take derivative. If we move everything to the LHS and take derivative, we get $2f'(2x)-f'(x)+2g'(2x)-3g'(3x)$ . I don't know if this is positive or negative, and even if I knew, I don't see how it helps me. (It would help to know it if $f,g$ were defined at $0$ ). Also I did not find any counterexample. Can someone please help me? And please write in your answer what was your intuition for this problem and why I still have no intuition to determine if the claim is true or false. Thanks.","['analysis', 'real-analysis', 'calculus', 'functions', 'inequality']"
3082399,Conditional expectation for Brownian motion,"Consider two Brownian motions $(W_t)_{t\ge 0}$ with starting point $x$ and $(W'_t)_{t\ge 0}$ with starting point $y$ . Define $T:=\inf\{t\ge 0:W_t=0\}$ , the first time when $W_t$ is equal to $0$ . Show that for every $x,y,t>0$ it holds: $$P(W_{t\wedge T}\le y \mid \{W_0=x\})=P(|W'_t|\ge x \mid \{W'_0=y\}).$$ My attempt: Rewrite $W'_t=B'_t+y$ for a standard Brownian motion $(B'_t)_{t\ge 0}$ . Then $P(W'_0=y)=1$ and $1_{\{W'_0=y\}}=1_\Omega$ , such that \begin{align}
P(|W'_t|\ge x \mid \{W'_0=y\})&=\frac{1}{P(W'_0=y)}E\Big[1_{\{|W'_t|\ge x\}}1_{\{W'_0=y\}}\Big]=P\big(|B_t'+y|\ge x\big)
\end{align} Rewrite $W_t=B_t+x$ for a standard Brownian motion $(B_t)_{t\ge 0}$ . Then $P(W_0=x)=1$ and $1_{\{W_0=x\}}=1_\Omega$ , such that \begin{align}
P(W_{t\wedge T}\le y \mid \{W_0=x\})&=\frac{1}{P(W_0=x)}E\Big[1_{\{W_{t\wedge T}\le y\}}1_{\{W_0=x\}}\Big]=P\big(B_{t\wedge T}+x\le y\big)\\
\end{align} Here I get stuck. Maybe I have to use $0\le B_{t\wedge T}+x$ and the reflection principle? I would really appreciate some help! Thanks in advance!","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3082406,Gaussian curvature of surface,"Consider the set $ \Sigma = \{ (x, y, z) \in \mathbb R ^3 : xyz=1 \}
> $ . Show that $ \Sigma$ is a surface and evaluate the Gaussian
  curvature at a general point $(x,y,z) \in \Sigma $ I've managed to do the first part using the regular value theorem, that is defining the function $\Phi : \mathbb R ^3 \to \mathbb R, \; \Phi(x,y,z) = xyz $ and then demonstrating that the preimage $ \Phi^{-1}(1) $ contains no singular points. Since we have defined a surface as the level set of a function, the unit normal is given by the (normalised) gradient vector $$ N : \Sigma \to S^2, \\ N(x,y,z) = \frac{\nabla \Phi}{\| \nabla \Phi \| } = \dfrac{( yz, xz, xy )}{\sqrt{x^2y^2 + y^2z^2 + z^2x^2}}. $$ To find the Gaussian curvature, we could of course just calculate the required partial derivatives of this map to construct the shape operator and then evaluate the resulting determinant but this approach seems unnecessarily messy considering the differentiation involved, let alone expanding the determinant and I was wondering if there was an easier approach to this problem instead?","['surfaces', 'curvature', 'differential-geometry']"
3082424,"Ladybug walking on a hexagon, starts at vertex A.Every minute moves to one of the two adjacent vertices.Probability she is back at A after 10 minutes?","A ladybug is walking at random on a hexagon. She starts at vertex A. Every minute, she moves to one of the two vertices (chosen at random) adjacent to the one she's currently on. What is the probability that, after 10 minutes, she is back at A? I thought that at every even minute the ladybug would be at A, C, or E so there is a $\frac13$ chance. This was wrong and I got the following message: The probability may be close to $\frac 13,$ but since there are $2^{10}$ possible paths for the ladybug and $2^{10}$ is not divisible by $3,$ the probability cannot be exactly $\frac 13.$",['probability']
3082425,Free generators for the localization of a module,"Let $A$ a commutative ring with 1 and $M$ a finitely generated $A$ -module. Assume the localization $M_p$ is a free $A_p$ -module for some prime ideal $p$ . Is it true that there is some set of generators of $M$ whose image in $M_p$ freely generate $M_p$ ? Feel free to assume $A$ is Noetherian. Fancy way to ask the same question: is there a surjective map $A^{\oplus n} \to M$ which induces an isomorphism $A_p^{\oplus n} \to M_p$ ? The reason I'm asking this is: I was able to reduce some Exercise in Hartshorne (II.5.7) to this statement, and all solutions I've found online treat this step as it's obvious but I just don't see it.","['algebraic-geometry', 'commutative-algebra']"
3082469,Atypical way to find angle between unit vectors: $\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B}\right\|\right)$,"At my work, I have come across code with the following way of calculating the angle between two vectors. $$\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B} \right\|\right)$$ (Note the physics convention: $\hat{v}$ indicates the normalization of $v$ ; ie, $\hat{v}:=v/\|v\|$ ). I've spent some time, but I can't think of how this was derived using typical methodologies (law of sines, law of cosines, dot product, cross product). It is pretty different. So my questions is, How could this have been derived?","['trigonometry', 'linear-algebra', 'vectors']"
3082561,Does every nonzero polynomial take a nonzero value at one of its multi-indices?,"A polynomial $p$ can be specified by its coefficient function, a finitely supported function $c:\mathbb N^d_0\to\mathbb R.$ Here $\mathbb N_0=\{0,1,2,\dots\}$ and $d\in\mathbb N_0.$ The value of $p$ at a point $x\in\mathbb R^d$ is $p(x)=\sum_{\alpha\in\mathbb N^d_0}c(\alpha)x_1^{\alpha_1}\dots x_d^{\alpha_d}$ (the sum makes sense by the assumption that $c$ has finite support). We call $p$ non-zero if $c(\alpha)\neq 0$ for some $\alpha.$ For all non-zero $p$ does there exist $\alpha$ such that $c(\alpha)\neq 0$ and $p(\alpha)\neq 0$ ? Equivalently: for all finite $A\subset\mathbb N_0^d,$ is the $|A|\times |A|$ matrix defined by $M_{\alpha,\beta}=(\alpha_1^{\beta_1}\dots\alpha_d^{\beta_d})$ non-singular? (In one direction. take $A$ to be the support of a counterexample $c,$ which is then in the kernel of $M.$ In the other direction, take $c$ to be a vector in the kernel of a counterexample $M.$ ) Call $A$ ""good"" if this holds. I have checked some randomly generated sets $A$ are good. Also: If $A\subset \mathbb N_0^d$ and $B\subset \mathbb N_0^e$ are both non-empty and good then the Cartesian product $A\times B\subset\mathbb N_0^{d+e}$ is good. In terms of matrices this is because the Kronecker product of two positive-dimensional square matrices is non-singular iff the two matrices are non-singular. Let $A\subset \mathbb N_0^{d+1}.$ If the sets defined by $A_0=\{\alpha\in A\mid \alpha_{d+1}=0\}$ and $A_+=\{\alpha\in A\mid \alpha_{d+1}>0\}$ are good then $A$ is good. Proof: assume $A_0$ and $A_+$ are good and consider a polynomial $p$ with coefficients $c$ zero outside $A.$ If $p(\alpha)=0$ for $\alpha\in A_0$ then $c(\alpha)=0$ for all $\alpha\in A_0,$ because $A_0$ is good and the $A_+$ coefficients don't contribute to $p(x)$ when $x_{d+1}=0.$ So $c$ is zero outside $A_+,$ and hence $p$ must also be zero on $A_+$ because $A_+$ is good. If $A\subset \mathbb N_0^1$ then $A$ is good. Proof: by the last point we can assume $0\not\in A.$ By Descartes' rule of signs a univariate polynomial with at most $|A|$ non-zero coefficients has at most $|A|-1$ positive zeroes. $A\subset \mathbb N_0^d$ is good if it is downwards-closed, i.e. for all $\beta\in A$ and all $\alpha$ such that $\alpha_i\leq\beta_i$ for all $1\leq i\leq d$ we have $\alpha\in A.$ Proof: apply the forward difference operator $(\Delta p)(x)=p(x_1,\dots,x_{d-1},x_d+1)-p(x).$ By induction $A'=\{\alpha \in A\mid (\alpha_1,\dots,\alpha_d+1)\in A\}$ is good. If $p$ had zero coefficients outside $A$ and also vanished on $A,$ then $\Delta p$ would have zero coefficients outside $A'$ and vanish on $A',$ which forces $\Delta p$ to be the zero polynomial. This means $p$ has zero coefficients outside $A_0$ (as defined in the last point) and we can apply induction on dimension. A small variation: if $A\subset \mathbb N_0^d$ is downwards closed and $\alpha\in\mathbb N_0^d$ then the shifted set $\alpha+A=\{\alpha+\beta\mid \beta\in A\}$ is good. This follows from the same argument but using the modified forwards difference operator defined by $\Delta' p=x^\alpha \Delta x^{-\alpha} p.$ R. Zippel's "" Interpolating polynomials from their values "" calls similar questions ""zero avoidance problems"", but I couldn't find anything answering this question.","['matrices', 'algebraic-geometry', 'polynomials', 'multivariate-polynomial']"
3082642,Projecting a projective variety away from a linear subspace,"I read in Harris' book at page 148, proposition 11.37 and got slightly confused regarding the argument. Harris mentions a projective space $\mathbb{P}^{2n+1}$ and a linear subspace $\mathbb{P}^{n}$ denoted $L$ , disjoint from a projective variety $J$ , which is embedded in $\mathbb{P}^{2n+1}$ . He then denotes by $\pi_L:\mathbb{P}^{2n+1}\longrightarrow L$ , what he refers to as ""the projection from $L$ "", which is a term I both never heard of, nor ever encountered in my reading. He then proceeds to project $J$ , claiming that as $J\cap L=\emptyset$ , the projection is regular map. He finally adds that it is a general fact that the projection of a projective variety from a linear subspace disjoint from it is finite. My questions are: 1) How is the projection from a linear subspace defined? I am familiar with the notion of projection from a point. 2) Is this projection a regular morphism? 3) Why does a regular morphism in this context has finite fibers? 4) Why is the projection of a projective variety from a linear subspace disjoint from it is finite in general? I believe all my questions stem from not understanding the answer to 1). Any help is appreciated. Ciao!","['grassmannian', 'algebraic-geometry', 'projective-varieties']"
3082644,Compute the gradient of $f(x)=\|\text{diag}(x)\|$ with the chain rule,"Consider the function $f:\mathbb{R}^n\to\mathbb{R}$ given by $f(x)=\|\text{diag}(x)\|$ , where $\text{diag}(x)\in\mathbb{R}^{n\times{n}}$ is the diagonal matrix with diagonal entries $x_1,x_2,\dots,x_n$ , and $\|\cdot\|$ is the spectral norm (matrix 2-norm). Since the spectral norm of a matrix is its largest singular value, and the singular values of a (square) diagonal matrix are the absolute values of the diagonal entries, we see that $f(x)=\|x\|_\infty$ , where $\|\cdot\|_\infty$ is the (vector) sup-norm. In this form, it is easier to deduce the properties of $f$ --in particular, it is differentiable at any point $x\in\mathbb{R}^n$ where the largest element of $x$ (in absolute value) is unique. At such a point, the gradient of $f$ is given by $$
\nabla{f(x)}=\text{sgn}(x_k)e_k
$$ where $k$ is the index of the (unique) largest entry of $x$ (in absolute value), $e_k$ is the $k^\text{th}$ standard basis vector in $\mathbb{R}^n$ , and $\text{sgn}(\cdot)$ is the sign function. I want to deduce the above expression for the gradient using the chain rule applied to $f(x)=(g\circ{h})(x)$ , where $g:\mathbb{R}^{n\times{n}}\to\mathbb{R}$ is given by $g(A)=\|A\|$ , and $h:\mathbb{R}^n\to\mathbb{R}^{n\times{n}}$ is given by $h(x)=\text{diag}(x)$ . The ""Jacobian"" of $h$ is a three-dimensional object, where $$
\frac{\partial[h(x)]_{ij}}{\partial{x_k}}=\begin{cases}1,&i=j=k,\\0,&\text{else.}\end{cases}
$$ The function $g$ is differentiable at any point $A$ where $A$ has a unique largest singular value, in which case the gradient(?) is given by $$
\nabla{g(A)}=uv^\text{T},
$$ where $u$ and $v$ are the left and right singular vectors (respectively) corresponding to the (unique) largest singular value of $A$ . So I essentially have a three-dimensional object and a 2-dimensional object, and I want to apply the chain rule to get the gradient, a 1-dimensional object ( i.e. a vector). A straightforward application suggests ""multiplying them together"" (not sure that concept is even defined), which seems like it would produce a matrix. What simple thing am I missing here?","['matrix-calculus', 'derivatives', 'chain-rule']"
3082654,Different ways to express $\sqrt{a}+\sqrt{b}$,"I had been thinking about this for a long time. I can’t express $\sqrt{a}+\sqrt{b}$ in ways that are useful. (I’m not wanting a formula, but just some ways to express this.) I can only think of $\sqrt{a+2\sqrt{ab}+b}$ .","['algebra-precalculus', 'radicals']"
3082663,Perspective drawing of a train and parallel lines,"One of my students came in today with a textbook problem that ended up looking like this. It was a word problem based on the perspective drawing of a train. The vertical lines (which separated the carts in the picture) are given as parallel. The diagonals of the carts are also parallel. The problem is to find the length of C2 based on similar figures. I've tried comparing the nested triangles, but that doesn't yield anything useful. I've also tried setting up a system of linear equations, but that ends up folding back on itself. Does this have something to do with the properties of trapezoids? Is there enough information to solve this problem? I've shown it to two other math people and they can't figure it out.",['geometry']
