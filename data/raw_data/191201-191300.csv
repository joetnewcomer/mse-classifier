question_id,title,body,tags
3620031,A clarification in a para in SGA4.5,"This is a para from SGA4.5 that I have confusion in. The situation gives us an $A$ -torsor $T \longrightarrow X$ for a finite group $A$ . And we also have a group homomorphism $\rho : A \longrightarrow GL(V)$ . I am unable to make sense of the equation (1.2.2) and the text preceding it. As I understand, $T$ over $X$ has a local trivialisation in étale topology whose fibres are $A$ . This gives us a 1-cocycle (again, in étale topology) $U_i \times_{X} U_j \xrightarrow{c_{ij}} A$ , which when composed with $\rho$ gives a $V$ -torsor on $X$ . The associated sheaf is what I suppose called $\mathcal F$ . What I need help is how one obtains the map (1.2.2) in the text. According to the author, it is a map of sheaves. How is $\textrm{Isom}(V,\mathcal F)$ (``Isom"" denotes the set of isomorphisms) a sheaf? It seems to me to be covariant from the étale site on $X$ , because of the Hom. 
Thanks in advance.","['algebraic-number-theory', 'algebraic-geometry']"
3620034,How to show that $AB=BA$ if $(A-3I)(B-3I)=2B+9I$,"How to show $AB=BA$ if: $$(A-3I)(B-3I)=2B+9I$$ where $A,B\in M_n(\mathbb{R})$ I've been doing lots of arithmetical transformations but cannot see the way...","['matrices', 'linear-algebra']"
3620084,How prove this determinant is $0?$,"find the value $$A_{n}=\begin{vmatrix}
1-\dfrac{1}{(n+1)^2}&\dfrac{1}{2}&\dfrac{1}{3}&\cdots&\dfrac{1}{n+1}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\cdots&\dfrac{1}{n+2}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\cdots&\dfrac{1}{n+3}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
\dfrac{1}{n+1}&\dfrac{1}{n+2}&\dfrac{1}{n+3}&\cdots&\dfrac{1}{2n+1}
\end{vmatrix}$$ show that $\det(A_{n})=0$ I have prove $$\det(A_{1})=\begin{vmatrix}
\dfrac{3}{4}&\dfrac{1}{2}\\
\dfrac{1}{2}&\dfrac{1}{3}
\end{vmatrix}=0$$ and $$\det(A_{2})=\begin{vmatrix}
\dfrac{8}{9}&\dfrac{1}{2}&\dfrac{1}{3}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}
\end{vmatrix}=0$$ $$\det(A_{3})=\begin{vmatrix}
\dfrac{15}{16}&\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}\\
\dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}&\dfrac{!}{7}
\end{vmatrix}=0$$",['matrices']
3620085,Concativity of entropy without Jensen's inequality,"In my information theory class I need to prove that entropy is concave (which is usually done with Jensen's inequality). But I want to use only the definition of entropy. And as the result of derivations I get a wrong answer. Here what I do: I need to prove that: $$\lambda {\rm H} \left(p\right)+\left(1-\lambda \right){\rm H} \left(q\right)\le {\rm H} \left(\lambda p+\left(1-\lambda \right)q\right)$$ I use the definitions of entropy (with the summation carried out over $\textit{p}$ or $\textit{q}$ , or ( $\textit{p}$ and $\textit{q}$ )): $$\lambda {\rm H} \left(p\right)={\rm {\mathbb E}}_{p} \log \frac{1}{p^{\lambda } } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda }}$$ $$\left(1-\lambda \right){\rm H} \left(q\right)={\rm {\mathbb E}}_{q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} } $$ $${\rm H} \left(\lambda p+\left(1-\lambda \right)q\right)={\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q} $$ Then collecting everything and moving to the left-hand side: $${\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } } +{\rm {\mathbb E}}_{p,q} \log \frac{1}{q^{\left(1-\lambda \right)} } ={\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} } $$ $${\rm {\mathbb E}}_{p,q} \log \frac{1}{p^{\lambda } q^{\left(1-\lambda \right)} } -{\rm {\mathbb E}}_{p,q} \log \frac{1}{\lambda p+\left(1-\lambda \right)q} \le 0$$ Using log properties: $${\rm {\mathbb E}}_{p,q} \log \frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 0$$ So I have to prove that the ration under log is less than unity (for log to be negative): $$\frac{\lambda p+\left(1-\lambda \right)q}{p^{\lambda } q^{\left(1-\lambda \right)} } \le 1$$ Finally I get the inequality which is definitely wrong (since AM-GM says me so): $$\lambda p+\left(1-\lambda \right)q\le p^{\lambda } q^{\left(1-\lambda \right)} $$ So, where I am wrong?","['entropy', 'jensen-inequality', 'analysis', 'inequality', 'convex-analysis']"
3620210,How to prefer data vector which has a few peaks?,"In an information retrieval system, I have a vector of values which represent occurrences of a term in documents: Term is preferred when it appears more frequently in very few documents. So, I need a numerical value to prefer vector which has few high peaks. For example: TERM1 is preferred over TERM5 because it has one high peak compared to 4 high peaks

TERM1 is preferred over TERM6 because it has one high peak compared to 1 low value

TERM4 is preferred over TERM5 because it has one high peak and 3 low values compared to 4 high peaks

etc... Is there any formula or calculated value that can give higher weight to preferred vectors?
p.s. I am not a mathematician Thanks in advance","['statistics', 'soft-question']"
3620230,Conjectured continued fraction formula for Catalan's constant,"Yesterday I posted this conjecture, but then deleted it thinking it was false. Turns out Python doesn't define $a^b$ as a^b , but rather as a**b . Conjecture: Denote by $G$ Catalan's constant , then $$G=\cfrac{1}{1+\cfrac{1^4}{8+\cfrac{3^4}{16+\cfrac{5^4}{24+\cfrac{7^4}{32+\cfrac{9^4}{40+\ddots}}}}}}$$ Given the connection $G$ has with the number $8$ shown here , as well as this continued fraction reaching nearly the first five decimal places of $G$ after around $200$ iterations (vinculums), I am confident this is true. However, I do not know how to code a continued fraction on Python or Pari/GP (a friend of mine gave it a go, but also to no avail) up to an iteration $n$ without having to write it out manually, which is really tedious. Here is some python code from a friend, coding this fraction up to $12$ iterations to be $\approx 0.9151$ , reaching the first three decimal places of $G$ . The only 'local' behaviour that I can say about continued fractions is that most of them are convergent, and that they all converge via oscillation at each iteration. But, more importantly, I'd like to know that if this be true, can it be shown from here that $G$ is irrational (or even transcendental, if you are willing)? I am aware this is an unsolved problem, which was what inspired me to write $G$ in another closed form. Any thoughts? Thank you in advance.","['conjectures', 'number-theory', 'elementary-number-theory', 'sequences-and-series', 'continued-fractions']"
3620304,Kodaira dimension of a ruled surface,"I am trying to solve 21.5.J in Vakil's FOAG. The exercise is about showing that $X:=C\times_{\mathbb C} \mathbb P_{\mathbb C}^1$ is neither Fano, Calabi-Yau, nor general type where $C$ is a smooth complex curve of genus $>1$ . Since $\Omega_X = \text{pr}_1^\ast\Omega_C\oplus\text{pr}_2^\ast\Omega_{\mathbb P^1}$ , we have $K_X = \text{pr}_1^\ast K_C\otimes \text{pr}_2^\ast K_{\mathbb P^1}$ and I can show that $X$ is not Fano nor CY. To show that $X$ is not general type, I need to show that the Kodaira dimension of $X$ is not $2$ . The Kodaira dimension is defined as the least integer $m$ such that $h^0(X,K_X^j)/j^m$ is bounded for $j>0$ . Some google search tells me $H^0(X, K_X^j) = 0$ for all $j>0$ , so I tried to show this as follows. Let $s$ be a section of $K_X^j$ . Then, for any point $p\in C$ , pulling back $s$ to $p\times \mathbb P_{\mathbb C}^1$ , we have $s=0$ since we can see that $K_X=\text{pr}_1^\ast K_C\otimes \text{pr}_2^\ast K_{\mathbb P^1}$ pulls back to $\mathcal O_{\mathbb P^1}\otimes K_{\mathbb P^1}=\mathcal O(-2)$ by considering the compositions $$
p\times \mathbb P^1\to C\times\mathbb P^1 \to \mathbb P^1\\
p\times \mathbb P^1\to C\times\mathbb P^1\to C
$$ Therefore, $s$ pulls back to zero for any choice of $p\in C$ . Considering affine open subsets $\text{Spec} A, \text{Spec}B$ of $C,\mathbb P^1$ , respectively, the above translates to algebra as the following: given a $A\otimes B$ module $M$ and $s\in M$ , we have $s\in \mathfrak m M$ for all maximal ideals $\mathfrak m$ of $A$ . (i) Can we conclude that $s=0$ from here easily in general? Or since $M$ corresponds to an invertible sheaf, should I only consider the form $M=A\otimes \mathbb C[x]$ (or possibly something like $M=A\otimes \mathbb C(x)$ )? (ii) Is there a simpler way to solve this problem? (More General Question) (iii) Some google search also tells me that for two irreducible complex projective varieties $X,Y$ , the Kodaira dimension of $X\times Y$ is the sum of those of $X$ and $Y$ . How do I show this?","['algebraic-geometry', 'commutative-algebra']"
3620310,"The Four Square Theorem and Integral Apollonian Circle Packings, is there any connection?","I have been studying theta-functions and made an interesting observation which I have a question about QUESTION : Is there a more intuitive, in particular a mostly geometric way, to prove the four square theorem which is based on Integral Apollonian Circle Packings?  Alternatively, is there an ""intuitive"" proof of the four square theorem which does not involve the theta function or other analytic ""heavy machinery"" but is more elementary or geometric in nature? Motivation :
It's well known that any positive integer can be represented as the sum of at most 4 squares .  It's less well known, that any 4 integers form the basis for an Integral Apollonian Circle Packing .  It turns out that it's possible to use the theta-function to prove the four square theorem in a very direct way (see Shakarchi Stein Vol 2, Chapter 10 ); I understand the proof but it feels like there is some geometric intuition that is being lost in this process. To see the connection between the theta-function, the four square theorem, and Apollonian circle packing;  we can make use of the following rule: given any 4 integers $a,b,c,d \in \mathbb{Z}$ $$ bc - ad = 1 $$ The theta-function has a functional equation which is based on this rule, and this rule forms the basis for the Integral Apollonian Circle packing method.  These types of results are also directly related to the ""Gauss Map"" and provide a recipe for using dynamical systems theory to study continued fraction expansions. Update (further motivation): I am reading this paper Apollonian Circle Packings: Number Theory
II. Spherical and Hyperbolic Packings .  I was led to these types of circle packings a couple of months ago after I learned about the Descartes circle theorem and the the Descartes quadratic form.  One way to restate the condition of Integral Circle Packings is by using the ""four-dimensional metric"" $$2(a^2 + b^2 + c^2 + d^2)^2 - (a + b + c + d)^2 = 0$$ The observation is that this quadratic form is of the same type as a condition in probability theory for the definition of the expected value of a random variable.  This also provides an intuitive connection, at least for me, to the theory of lie algebra's which dominate the higher theory of these types of integral circle packings and their representations, because I imagine this quadratic form is a commutator between some kind of strange mathematical object I am struggling to understand (Note: please provide any feedback about how I can do a better job of posing these types of abstract questions, I struggle to communicate these types of idea's sometimes, and am working hard on improving my exposition.  Any advice or suggestions are much appreciated.)","['number-theory', 'theta-functions']"
3620313,Example of a Hamiltonian Lie group action,"I was wondering why the following Lie group action is Hamiltonian. Equip $\mathbb{C}^{k\times n}\cong\mathbb{R}^{2kn}$ with the canonical symplectic form $\omega_0$ on $\mathbb{R}^{2kn}$ . We have an action by the Lie group $G=U(k)$ on $\mathbb{C}^{k\times n}$ by matrix multiplication, which is of course smooth. However, I can't see why this action is symplectic, because I don't really see how the form $\omega_0$ works on $\mathbb{C}^{k\times n}$ . The momentum map for this action is supposed to be the map $\mu:\mathbb{C}^{k\times n}\to\mathfrak{u}(k)^*$ , $\mu(A)\xi=\frac{i}{2}\text{Tr}(AA^*\xi).$ This map is well-defined, but how do I compute $d\langle\mu,\xi\rangle$ , and show that this is equal to $\omega_0(\xi_M,\cdot)$ where $\xi_M$ is the fundamental vector field associated to the action. I have computed $\xi_M(x)=\xi x$ , but how do I work with $\omega_0$ on $\mathbb{C}^{k\times n}?$","['symplectic-geometry', 'differential-geometry']"
3620359,Show a $C^*$-algebra admits an approximate kind of identity.,"I'm reading the book ""An invitation to $C^*$ -algebra's"" by William Arveson and I'm focused on the proof of proposition 1.3.1, which says: Let $A$ be a $C^*$ -algebra and $J$ be a closed two-sided ideal of $A$ .
  Then for every $x \in J$ there is a sequence $(e_n)_n$ of
  self-adjoint elements of $J$ with $$Sp_A(e_n) \subseteq [0,1], \quad  \lim_n \Vert xe_n - x \Vert = 0$$ The author provides a proof in the case that $A$ has a unit and concludes by saying that the case where $A$ has no unit follows by adjoining a unit, leaving the details to the reader. So, suppose $A$ has no unit. Then consider the unitalisation $A_I:=A \oplus \mathbb{C}$ . Maybe I can show that $J$ is a two-sided ideal in $A_I$ ? So, let $a + \lambda 1 \in A_I$ and $x \in J$ ( $a \in A, \lambda \in \mathbb{C}$ ). Then $$(a+\lambda1)x = ax + \lambda x$$ and if I can show that $\lambda x \in J$ , I can show that $J$ is also a two-sided ideal in $A_I$ . However, I don't see why this should be true.","['functional-analysis', 'operator-algebras']"
3620399,Motivation and applications of the duality map,"The duality map is defined, according to Brezis ( Functional Analysis, Sobolev Spaces and Partial Differential Equations ) as follows, where $E$ is a Banach space and $x \in E$ : $$F(x) = \{f \in E^* \ : \ ||f|| = ||x|| \text{ and } \langle f, x \rangle = ||x||^2\}$$ It is one of the exercises to prove that $$
F(x) = \{f \in E^* \ : \ ||f|| \leq ||x|| \text{ and } \langle f, x \rangle = ||x||^2\} = \{f \in E^* \ : \ \frac12 ||y||^2 - \frac12 ||x||^2 \geq \langle f, y - x \rangle \ \forall y \in E\}.$$ My question is: Where does the duality map comes from? That, is, what motivates the definition, from a historical point of view? Why is it useful? Thanks in advance.","['duality-theorems', 'functional-analysis']"
3620409,"If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric?","If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric? It's clear to see how if $A$ = $A^T$ , they would have the same eigenvectors, but is it the only way? And how would you show it?","['eigenvalues-eigenvectors', 'vector-spaces', 'matrices', 'linear-algebra', 'linear-transformations']"
3620411,An estimate on the coefficient of bounded schlicht functions,"Let $S=\{f:\mathbb{D}\to\mathbb{C}$ be a injective holomorphic map: $f(z)=z+a_2z^2+a_3z^3+a_4z^4+...\}$ . Suppose $f\in S$ and there exists $M>1$ such that $|f(z)|\leq M,\;\forall z\in\mathbb{D}$ .Show that $|a_2|\leq2\left(1-\frac1M\right)$ and $d(0,\partial f(\mathbb{D})\geq \frac{1}{2(1+\sqrt{1-\frac1M})}$ . My Idea:If we let $g(z)=\frac{1}{f(\frac1z)}=z-a_2+\frac{a_2^2-a_3}{z}+...\in \Sigma$ , where $\Sigma=\{g:\mathbb{C}\backslash\overline{\mathbb{D}}\to\mathbb{C}\big{|}g(z)=z+b_0+\frac{b_1}{z}+\frac{b_2}{z^2}+...\}$ .By the Gronwell Area Theorem, we have $\sum\limits_{n=1}^{+\infty}n|b_n|\leq 1$ . If we apply the theorem directly to $g(z)$ , we can not achieve a satisfactory result. However, if we let $g^*(z)=\sqrt{g(z^2)}=z+b_0^*+b_1^*\frac{1}{z}+...$ . This defines a monodromy branch. It is not hard to prove $g^*(z)$ is an injective holomorphic function, and with some computation we get $b_1^*=-\frac{a_2}{2}$ . Now, the Gronwell Area Theorem implies that $|a_2|\leq 2$ . This is a way to prove the de Branges's theorem for the coefficient of $a_2$ . I do think with some improvement of this idea and use the boundedness of $f$ , the above proposition will be solved. But I just can not figure out a way. The latter part of proposition, in my opinion, is  based on $|a_2|\leq2\left(1-\frac1M\right)$ . Actually, it's some what like the Koebe- $\frac14$ Theorem. Any solution or hint is highly appreciated!",['complex-analysis']
3620416,"Prove that $f_n(B_{\tau_1 } , \dots, B_{\tau_{n-1 }}, -1) < B_{\tau_{n-1 }} < f_n(B_{\tau_1 } , \dots, B_{\tau_{n-1 }}, 1)$","I would like to prove that (almost surely) $$f_n(B_{\tau_1 } , \dots, B_{\tau_{n-1 }}, -1) < B_{\tau_{n-1 }} < f_n(B_{\tau_1 } , \dots, B_{\tau_{n-1 }}, 1)$$ Where the context is as follows: we have a martingale $(X_n )$ - with expectation equal to zero - such that for each $n \ge 1$ there exists a Borel measurable function $f_n: \ \mathbb {R } ^{-1 } \times \{-1, 1 \}  \to \mathbb R$ , and a $\{-1 , 1 \} $ - valued random variable $D_n $ such that $$X_n = f_n(X_1, \dots , X_{n-1 } , D_n )$$ Further we assume that for any $x_1, \dots, x_{n-1 } $ $$f_n(x_1, \dots, x_{n-1 } , -1 ) < f_n(x_1, \dots, x_{n-1 } , 1 )$$ Given a Brownian motion $(B_t)$ we define the stopping times $\tau_0 = 0$ and for $n \ge 1$ $$\tau_n = \inf \{t > \tau_{n-1 }: \ B_t \in \{f_n(B_{\tau_1 } ,\dots, B_{\tau_{n-1 } }, -1 ), f_n(B_{\tau_1 } ,\dots, B_{\tau_{n-1 } }, 1 ) \} \}   $$ This is what I manage to do: For $n=1 $ we have that $f_1 : \{-1, 1 \} \to \mathbb{R}$ , and since $f_1(D_1)=X_1 $ and $E[X_1]=E[X_0]=0$ , we get $$0 = E[f_1(D_1)]=f_1(-1)P[D_1=-1] + f_1(d)P[D_1=1]$$ By the assumption that $f_1(-1) < f_1(1)$ this means that $f(-1)<0<f_1(1)$ . And since $\tau_0 = 0$ and $B_0 = 0$ the claim holds for $n=1$ . For general $n $ we have again that $$E[f_n(X_1, \dots, X_{n-1 } , D_n )] = 0$$ And hence \begin{multline*}
    E[f_n(X_1, \dots, X_{n-1 } , D_n )] = \\
    = E[f_n(X_1, \dots, X_{n-1 } , -1 )1_{\{D_n = -1\}}] + E[f_n(X_1, \dots, X_{n-1 } , 1 )1_{\{D_n = 1\}}] = 0
\end{multline*} This means that one of the integrals must be negative and one positive [or both equal to zero]. But here I got stuck! How is it possible to relate the value of $f_n(B_{\tau_1 } , \dots B_{\tau_{n-1 } } , \pm 1 ) $ to $B_{\tau_{n-1 } } $ ? Much grateful for any help provided!","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
3620444,Symmetric functions written in terms of the elementary symmetric polynomials.,"[A recent post reminded me of this.] How can we fill in the blanks here: For any _____ function $f(x,y,z)$ of three variables that is symmetric in the three variables, there is a _____ function $\varphi(u,v,w)$ of three variables such that $f(x,y,z) = \varphi(x+y+z, xy+yz+zx, xyz)$ .  [Of course we can ask this for some number of variables other than three.] For example, the theorem (polynomial, polynomial) is classical, and the theorem (rational, rational) is linked above.  What others work?  (algebraic, algebraic), say, or (elementary, elementary) or (continuous, continuous) or ( $C^\infty, C^\infty$ ). Is there an elementary function $\varphi(u,v,w)$ of three complex variables such that $e^x+e^y+e^z = \varphi(x+y+z, xy+yz+zx, xyz)$ for all $x,y,z, \in \mathbb C$ ? Even if $x, y, z$ separately are not differentiable functions of $(x+y+z, xy+yz+zx, xyz)$ , could it be that $e^x+e^y+e^z$ is?","['symmetric-functions', 'galois-theory', 'elementary-functions', 'combinatorics', 'symmetric-polynomials']"
3620496,"What does ""transform like"" mean?","I read on a pdf that considering $SU(2)$ the spinor $(\xi_1, \xi_2)^T$ transform the same way as $(-\xi_2^*, \xi_1^*)^T$ . What does it mean that they transform the same way? I don't know what's the meaning of ""two things transform in the same way"".","['spin-geometry', 'linear-algebra', 'group-theory', 'transformation', 'rotations']"
3620520,"When classifying critical points of $f(x,y)$, what does a ""degenerate point"" (when $f_{xx}f_{yy}=f_{xy}^2$) mean, exactly?","In my textbook, I have the following way of classifying critical points: \begin{array} {|c|c|c|c|}\hline f_x=f_y=0 & f_{xx} < 0 & W > 0 & \text{local maximum} \\ f_x=f_y=0 & f_{xx} > 0 & W > 0 & \text{local minimum} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W < 0 & \text{saddle point} \\ f_x=f_y=0 & f_{xx} \text{ anything} & W = 0 & \text{degenerate} \\ \hline  \end{array} Notation: $f_x$ and $f_y$ are partial derivatives with respect to $x$ and $y$ . And $W=f_{xx}f_{yy}-f^2_{xy}$ is the determinant of the Jacobian of second derivatives. I understand  what minimum/maximum points are and what saddle points are. But what does a degenerate point mean, exactly?","['multivariable-calculus', 'calculus', 'derivatives', 'terminology']"
3620613,Is the reverse of the first derivative test statement true?,"The first derivative test says that if the derivative of a function is positive/negative in the open interval $(x-a,x)$ and negative/positive in the open interval $(x,x+b)$ where both $a$ and $b$ are greater than zero, then there is a local maxima/minima at $x$ . Is the reverse of this statement true ? For example, if $x$ is a local maxima of the function $f(x)$ and if the function $f(x)$ is continuous and differentiable in $(a,b)$ where $a < x < b$ , then there exists an interval to the left of $x$ , howsoever small, in which the function has a positive slope and an interval to the right of $x$ , howsoever small, in which the function has a negative slope.","['maxima-minima', 'calculus', 'derivatives']"
3620641,"Evaluating $\int \arcsin(\sqrt{x+1}-\sqrt{x})\,dx$","I found this integral here: https://www.12000.org/my_notes/ten_hard_integrals/index.htm Sites like https://www.integral-calculator.com/ , which I often find very helpful, did not successfully calculate the integral. I have found a potential answer, that seems to be correct. Using the formula $$\int f(x)\,dx=x \cdot f(x)-\left[{\int f^{-1}(y)\,dy}\right]_{y=f(x)},$$ I calculated : $$\int \arcsin(\sqrt{x+1}-\sqrt{x})\,dx = x \cdot \arcsin(\sqrt{x+1}-\sqrt{x})+\frac{3}{8}\arcsin(\sqrt{x+1}-\sqrt{x})+\frac{1}{8}\sqrt{\sqrt{x(x+1)}-x}\cdot[(1+2\sqrt{2})\sqrt{x+1}-(1-2\sqrt{2})\sqrt{x}]$$ My question is: How else could I integrate this function? Is there an easier way?
I tried to substitute $x=tan(u)^2$ but it seems that the resulting integral is an extremely long combination of sines and cosines. Thank you for your help Additional notes on my reasoning: The inverse function of $\arcsin(\sqrt{x+1}-\sqrt{x})$ is $\left(\frac{\sin^2 x-1}{2\sin x}\right)^2$ Integrating this function can be done by expanding the square : $$\int \left(\frac{\sin^2 y-1}{2\sin y}\right)^2 dy=\int \frac{1}{4}(\sin^2 y-2+\csc^2 y)\,dy$$ After having calculated that integral, we replace y by $\arcsin(\sqrt{x+1}-\sqrt{x})$ . After some simplification work, I get the result mentioned above.","['integration', 'indefinite-integrals']"
3620660,"Completitude of $L^1[0,1]\cap L^2[0,1]$ with the maximumm norm","I am trying to prove from scratch that the space $L^1[0,1]\cap L^2[0,1]$ equipped with the norm: $$\left \| f \right \|=\max\{\left \| f \right \|_1,\left \| f \right \|_2\}$$ defines a Banach space, but I am having trouble checking completitude. I have considered the Cauchy sequence $\{f_n\}$ , and found that by completitude of $L^1[0,1]$ and $L^2[0,1]$ , there must be $f_0^1, f_0^2$ such that: $$\left \| f_n -f_0^1\right \|_1\rightarrow 0$$ $$\left \| f_n -f_0^2\right \|_2\rightarrow 0$$ But I don't know how to prove that $\left \| f_0^1-f_0^2 \right \|=0$ . Any ideas?","['banach-spaces', 'measure-theory', 'lp-spaces']"
3620718,Which matrices $A\in\text{Mat}_{n\times n}(\mathbb{K})$ are orthogonally diagonalizable over $\mathbb{K}$?,"Update 1. I still need help with Question 1, Question 2' (as well as the bonus question under Question 2'), and Question 3'. Update 2. I believe that all questions have been answered if $\mathbb{K}$ is of characteristic not equal to $2$ .  The only thing remains to deal with is what happens when $\text{char}(\mathbb{K})=2$ . Let $\mathbb{K}$ be a field and $n$ a positive integer.  The notation $\text{Mat}_{n\times n}(\mathbb{K})$ represents the set of all $n$ -by- $n$ matrices with entries in $\mathbb{K}$ .  The subset $\text{GL}_n(\mathbb{K})$ of $\text{Mat}_{n\times n}(\mathbb{K})$ is composed by the invertible matrices. 
 Here, $(\_)^\top$ is the usual transpose operator.  Also, $\langle\_,\_\rangle$ is the standard nondegenerate bilinear form on $\mathbb{K}^n$ . Definition 1. A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is said to be orthogonally diagonalizable over $\mathbb{K}$ if there exist matrices $D\in\text{Mat}_{n\times n}(\mathbb{K})$ and $Q\in\text{GL}_{n}(\mathbb{K})$ where $D$ is diagonal and $Q$ is orthogonal (i.e., $Q^\top=Q^{-1}$ ) such that $$A=QDQ^{\top}\,.$$ Definition 2. A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is said to be seminormal if $$AA^\top=A^\top A\,.$$ For clarification, when $\mathbb{K}$ is $\mathbb{R}$ , seminormal matrices are the same as normal matrices.  However, when $\mathbb{K}$ is $\mathbb{C}$ , the terms seminormal and normal are different.  We have an obvious proposition. Proposition. Let $A\in\text{Mat}_{n\times n}(\mathbb{K})$ . (a) If $A$ is orthogonally diagonalizable over $\mathbb{K}$ , then $A$ is  symmetric. (b) If $A$ is symmetric, then $A$ is seminormal. The converse of (a) does not hold (but it does if $\mathbb{K}$ is $\mathbb{R}$ ).  For example, when $\mathbb{K}$ is the field $\mathbb{C}$ or any field with $\sqrt{-1}$ , we can take $$A:=\begin{bmatrix}1&\sqrt{-1}\\\sqrt{-1}&-1\end{bmatrix}\,.$$ Then, $A$ is symmetric, but being nilpotent, it is not diagonalizable.  The converse of (b) does not hold trivially (nonzero antisymmetric matrices are seminormal, but not symmetric). Here are my questions.  Crossed-out questions already have answers. Question 1. Is there a way to characterize all orthogonally diagonalizable matrices over an arbitrary field $\mathbb{K}$ ? As in Proposition (a), these matrices must be symmetric, but the counterexample above shows that this is not a sufficient condition.  Due to the answer by user277182 , I believe that this is a correct statement. Theorem. Suppose that $\text{char}(\mathbb{K})\neq 2$ .  A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is orthogonally diagonalizable over $\mathbb{K}$ if and only if (a) $A$ is symmetric and diagonalizable over $\mathbb{K}$ , and (b) there exists a basis $\{v_1,v_2,\ldots,v_n\}$ of $\mathbb{K}^n$ consisting of eigenvectors of $A$ such that $\langle v_i,v_i\rangle$ is a nonzero perfect square element of $\mathbb{K}$ for each $i=1,2,\ldots,n$ . In the case where $\mathbb{K}$ contains all of its square roots (or when $\mathbb{K}$ is algebraically closed), the condition (b) in the theorem above is redundant.  This theorem also answers Question 2' below (in the case $\text{char}(\mathbb{K})\neq 2$ ). Question 2. If a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is already known to be diagonalizable over $\mathbb{K}$ , is it also orthogonally diagonalizable over $\mathbb{K}$ ? The answer of Question 2 turns out to be no (see a counterexample in my answer below).  In light of this discovery, I propose a modified version of Question 2. Question 2'. Let $\mathbb{K}$ be an algebraically closed field.  If a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ , is it also orthogonally diagonalizable over $\mathbb{K}$ ? Bonus. If $\mathbb{K}$ is not an algebraically closed field, what is a minimal requirement of $\mathbb{K}$ such that, if a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ , it is always also orthogonally diagonalizable over $\mathbb{K}$ ?  This requirement may depend on $n$ . My guess for the bonus question is that, for every $x_1,x_2,\ldots,x_n\in\mathbb{K}$ , $x_1^2+x_2^2+\ldots+x_n^2$ has a square root in $\mathbb{K}$ .  For example, a minimal subfield of $\mathbb{R}$ with this property is the field of constructible real numbers .  Any field of characteristic $2$ automatically satisfies this condition. Edit. According to this paper and that paper , when $\mathbb{K}=\mathbb{C}$ , a symmetric matrix $A$ with an isotropic eigenvector $v$ (that is, $v^\top\,v=0$ ) is nonsemisimple (i.e., it is not diagonalizable).  Therefore, at least, when $\mathbb{K}$ is a subfield of $\mathbb{C}$ such that, for every $x_1,x_2,\ldots,x_n\in\mathbb{K}$ , $x_1^2+x_2^2+\ldots+x_n^2$ has a square root in $\mathbb{K}$ , then a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is orthogonally diagonalizable over $\mathbb{K}$ if and only if it is diagonalizable over $\mathbb{K}$ .  The result for other fields is currently unknown (to me). Question 3. As a generalization of this question , suppose that $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ . Does it hold that $A$ and $A^\top$ have the same set of eigenspaces if and only if $A$ is seminormal? Only the forward direction ( $\Rightarrow$ ) of this biconditional statement is known to be true.  It is clear, however, that when $A$ is orthogonally diagonalizable over $\mathbb{K}$ , then $A$ is symmetric, whence $A$ and $A^\top$ have the same eigenspaces.  As a result, the converse is true at least when $\mathbb{K}$ is a subfield of $\mathbb{R}$ because the seminormal (whence normal) matrices which is diagonalizable over $\mathbb{R}$ are the symmetric matrices. The answer to Question 3 is yes .  I forgot that diagonalizable matrices commute if and only if they can be simultaneously diagonalized.  See my answer in the other thread for a more detailed proof.   Therefore, I proposed a more generalized version of Question 3. Question 3'. Let $A\in\text{Mat}_{n\times n}(\mathbb{K})$ be such that all roots of the characteristic polynomial of $A$ lie in $\mathbb{K}$ .  What is a necessary and sufficient condition for $A$ and $A^\top$ to have the same set of generalized eigenspaces? Clearly, seminormality is not one such conditions.  Over any field $\mathbb{K}$ , the matrix $A:=\begin{bmatrix}0&1\\0&0\end{bmatrix}$ has the same set of generalized eigenspaces as does $A^\top$ .  (The only eigenvalue of $A$ is $0$ , and the generalized eigenspace associated to this eigenvalue is the whole $\mathbb{K}^2$ . The same goes with $A^\top$ .)  However, $$AA^\top=\begin{bmatrix}1&0\\0&0\end{bmatrix}\neq \begin{bmatrix}0&0\\0&1\end{bmatrix}=A^\top A\,.$$ In fact, any matrix $A\in\text{Mat}_{2\times 2}(\mathbb{K})$ which has an eigenvalue in $\mathbb{K}$ with multiplicity $2$ has $\mathbb{K}^2$ as its unique generalized eigenspace, and it follows immediately that $A$ and $A^\top$ have the same generalized eigenspace.","['matrices', 'orthogonal-matrices', 'field-theory', 'linear-algebra', 'diagonalization']"
3620739,How to prove Laplace transform is bounded on $L^2(\mathbb{R}_+)$?,"The Laplace transform is defined by \begin{equation}
(\mathscr{L}f)(s) \triangleq  
\int_0^\infty e^{-sx} f(x) dx, \quad s>0,
\end{equation} then how can we check that the Laplace transform $\mathscr{L}$ is bounded as an operator from $L^2(\mathbb{R}_+)$ to $L^2(\mathbb{R}_+)$ with norm $\sqrt{\pi}$ ?","['harmonic-analysis', 'operator-theory', 'functional-analysis', 'laplace-transform']"
3620768,Why is trivial intersection of groups $N$ and $H$ not required in the definition of outer semi-direct products?,"In the case of outer (synthetic) semi-direct products, we take any two groups $N$ and $H$ and a group homomorphism $\varphi: H \to \mathrm{Aut}(N)$ and effectively ""synthesize"" a new group named $(N \rtimes _\varphi H)_\text{synthetic}$ , with the underlying set as the cartesian product $N\times H$ and with a restriction imposed in form of a new group operation, by the homomorphism $\varphi$ , i.e., $$\bullet: (N \rtimes_\varphi H)_\text{synthetic} \times (N\rtimes_\varphi H)_\text{synthetic}\to (N\rtimes_\varphi H)_\text{synthetic}$$ and just like in the descriptive case $${(n_{1},h_{1})\bullet (n_{2},h_{2})=(n_{1}\varphi (h_{1})(n_{2}),\,h_{1}h_{2})=(n_{1}\varphi _{h_{1}}(n_{2}),\,h_{1}h_{2})}.$$ Say the identity element in the group is $(1_N, 1_H)$ and the inverse of an element $(n, h)$ is $(\varphi_{h^{-1}}(n^{-1}), h^{-1})$ .   Now the pairs $(n, 1_H)$ form a normal (*) subgroup $\mathcal{N} \cong N$ and the pairs $(1_N, h)$ form a subgroup $\mathcal H \cong H$ .  The descriptive semidirect product of these two subgroups $\mathcal{N} \rtimes_\varphi \mathcal{H}$ is in fact the whole artificially constructed group $(N \rtimes_\varphi H)_\text{synthetic}$ , in the same sense of inner semi-direct products. Question : Unlike in inner semi-direct product definitions, I never see the condition $N \cap H = \{1\}$ in the definition of outer semi-direct products. Why is that? Is $\mathcal{N} \cap \mathcal{H} = \{1_N, 1_H\}$ by construction ? I'm not sure how to prove it.","['semidirect-product', 'group-theory', 'abstract-algebra']"
3620825,Langley's Adventitious Angles${}$ [duplicate],This question already has an answer here : Finding an angle in a $80^\circ$-$80^\circ$-$20^\circ$ triangle (variant involving $70^\circ$ and $60^\circ$) [duplicate] (1 answer) Closed 2 years ago . I've been running in circles and couldn't give a rigorous mathematical proof that the angle is x = 20°. Any idea? This is my try: I got the answer $x=20^\circ$ using a computer program: https://www.geogebra.org/classic/qt79hpec,"['triangles', 'trigonometry', 'geometry']"
3620875,Function with the property $f(a+b)=f(a)+f(b)+2ab$,"Let $f: \mathbb{R} \to \mathbb{R}$ and let's define $f$ such that the following property holds $$f(a+b)=f(a)+f(b)+2ab \text{ for all } a,b.$$ Also let the function be differentiable at $0$ and $f'(0) = 3.$ Show that the function is differentiable everywhere and determine the derivative $f'(x)$ . Since $f(0) = 0$ and we have that $\lim_{h\to0} \frac{f(h)}{h} = 3$ . So $$\frac{f(x+h)-f(h)}{h} = \frac{f(x)+f(h)+2xh}{h} = \frac{f(h)}{h}+2xh = 3+2xh$$ so it's differentiable and the derivative is $f'(x)=3+2xh$ ? I feel like i should have gotten rid of the $h$ ?","['functional-equations', 'calculus', 'derivatives', 'real-analysis']"
3621016,What is the intuition behind Liouville's theorem in complex analysis? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 years ago . Improve this question I'm looking for an intuitive motivation for Liouville's theorem from complex analysis. 
If somebody could illustrate this with a simple example, that would be great. Thank you so much.",['complex-analysis']
3621042,"Given Gram matrix, find any set of vectors","Given Gram matrix $G_{ij}=\left \langle e_i,e_j \right\rangle=\begin{bmatrix}41 &12 &13 \\ 12 &48  &4 \\  13& 4 & 11\end{bmatrix}$ . Find any set of vectors $\left\{ e_1,e_2,e_3\right\}$ corresponding to it. My attempt: Let $A=\begin{bmatrix}
e_{11} & e_{21} &e_{31} \\ 
e_{12} &e_{22}  &e_{32} \\ 
 e_{31}&e_{32}  &e_{33} 
\end{bmatrix}$ -three columns of the  vectors. Then we have a $9$ eqution system with $9$ unknown coordinates - $A^T\cdot A=G$ , but it isn't linear.
May I ask if there is another method for solving this problem? Thank you in advance!","['matrices', 'linear-algebra']"
3621196,Notation in proof for the asymptotic equipartition property,"In the following lecture notes chapter 3, page 12-13, they state the following We begin by introducting some important notation: - For a set $\mathcal{S},|\mathcal{S}|$ denotes its cardinality (number of elements contained on the set). For example, let $\mathcal{U}=\{1,2, \ldots, M\},$ then $|\mathcal{U}|=M$ - $  u^{n}=\left(u_{1}, \ldots, u_{n}\right)$ is an $n$ -tuple of $u$ - $ \mathcal{U}^{n}=\left\{u^{n} | u_{i} \in \mathcal{U} ; i=1, \ldots, n\right\} .$ It is easy to see that $\left|\mathcal{U}^{n}\right|=|\mathcal{U}|^{n}$ - $ U_{i}$ generated by a memoryless source $U^{'}$ implies $U_{1}, U_{2}, \ldots$ i.i.d. according to $U$ (or $P_{U}$ ). That is. $$p\left(u^{n}\right)=\prod_{i=1}^{n} p\left(u_{i}\right)$$ Definition 12 . The sequence $u^{n}$ is $\epsilon$ -typical for a memoryless source U for $\epsilon>0,$ if $$
\left|-\frac{1}{n} \log p\left(u^{n}\right)-H(U)\right| \leq \epsilon
$$ or equivalently, $$
2^{-n(H(U)+\epsilon)} \leq p\left(u^{n}\right) \leq 2^{-n(H(U)-\epsilon)}
$$ Let $A_{\epsilon}^{(n)}$ denote the set of all $\epsilon$ -typical sequences, called the typical set.
  So a length- $n$ typical sequence would assume a probability approximately equal to $2^{-n H(U)}$ . Note that this applies to memoryless sources, which will be the focus on this course $^{1}$ . Theorem 13 (AEP) . $\forall \epsilon>0, P\left(U^{n} \in A_{\epsilon}^{(n)}\right) \rightarrow 1$ as $n \rightarrow \infty$ Proof This is a direct application of the Law of Large Numbers (LLN). $$
\begin{aligned}
P\left(U^{n} \in A_{\epsilon}^{(n)}\right) &=P\left(\left|-\frac{1}{n} \log p\left(U^{n}\right)-H(U)\right| \leq \epsilon\right) \\
&=P\left(\left|-\frac{1}{n} \log \prod_{i=1}^{n} p\left(U_{i}\right)-H(U)\right| \leq \epsilon\right) \\
&=P\left(\left|\frac{1}{n}\left[\sum_{i=1}^{n}-\log p\left(U_{i}\right)\right]-H(U)\right| \leq \epsilon\right) \\
& \rightarrow 1 \text { as } n \rightarrow \infty
\end{aligned}
$$ where the last step is due to the Law of Large Numbers (LLN), in which $-\log p\left(U_{i}\right)$ 's are i.i.d. and hence their arithmetic average converges to their expectation $H(U)$ My question is related to the proof.
My understanding is the following; $U^n$ is a sequence of random variables $U^n = (U_1, U_2, \ldots,U_n)$ drawn i.i.d from some distribution $p_U(u) = p(u)$ and $u^n$ is a realization of the sequence. However in the proof they switch from using $p(u^n)$ to $p(U^n)$ and I am not sure what $p(U^n)$ represents? What would be wrong by doing it as follows? $$
\begin{aligned}
P\left(u^{n} \in A_{\epsilon}^{(n)}\right) &=P\left(\left|-\frac{1}{n} \log p\left(u^{n}\right)-H(U)\right| \leq \epsilon\right) \\
&=P\left(\left|-\frac{1}{n} \log \prod_{i=1}^{n} p\left(u_{i}\right)-H(U)\right| \leq \epsilon\right) \\
&=P\left(\left|\frac{1}{n}\left[\sum_{i=1}^{n}-\log p\left(u_{i}\right)\right]-H(U)\right| \leq \epsilon\right) \\
& \rightarrow 1 \text { as } n \rightarrow \infty
\end{aligned}
$$","['statistics', 'information-theory', 'notation', 'probability-theory', 'probability']"
3621202,Probability: Drawing two marbles simultaneously from the bag,"This is a 9. grade elementary school quiz problem. There is a bag with 4 marbles. Two of them are yellow and the other two are green. You blindly pick two marbles at once. What is the probability that you get two yellow marbles? I would say the answer is $\frac{1}{6}$ . There is a single combination to draw two yellow marbles and 6 possible combinations how to draw two marbles. My second approach is: possibility of picking the first one yellow is $\frac{1}{2}$ , picking the second one yellow is $\frac{1}{3}$ , multiplying possibilities  gives me the same result $\frac{1}{6}$ . The problem is that my teacher claims that the answer is $\frac{1}{3}$ . Her argument is that there are 3 possible outcomes - two yellows, two greens and a mixed one. One outcome is positive so therefore the answer is $\frac{1}{3}$ . She also opposed to my second approach claiming that drawing marbles one by one is not the same as draw them simultaneously. From my opinion there should be no difference in result. Could someone please write what is the correct answer to this problem? Does simultaneously drawing gives the same result as drawing one by one without replacement? Finally, I think I understand the problem. Your answers help me to see it clearer. I found out that I mixed up the terminology. The problem has 6 outcomes and 3 possible events . The event that you draw the YG (mixed) combination has 4 positive outcomes while events of drawing YY or GG has a single possible outcome each. Probability is defined as a ratio between positive outcomes for specific event and all possible outcomes. Therefore the probability for drawing YY is $\frac{1}{6}$ and not $\frac{1}{3}$ . By adding more green marbles you will increase the number of all possible outcomes, but the number of events will remain 3, so therefore the probability of picking YY will decrease and not stay $\frac{1}{3}$ , what seems very logically. That was the catch I didn't see how to explain the teacher why I am correct. I think now I would be able to convince the teacher. Thank you all for your great help.",['probability']
3621256,Why I can't reduce an integral divided by another integral?,"For example, $$
\frac{\int_{0}^{1}f(x)dx}{\int_{0}^{1}dx} = \int_{0}^{1}f(x),  \space\space\space  (1)
$$ The dx disappears since $$
\frac{\int_{0}^{1}f(x)dx}{\int_{0}^{1}dx} = \lim_{N\to\infty}\left(\sum_{k=0}^{k\to N}{\frac{f(\frac{k}{N})\frac{1}{N}}{1\times \frac{1}{N}}}\right) = \lim_{N\to\infty}\left(\sum_{k=0}^{k\to N}{\frac{f(\frac{k}{N})}{1}}\right), dx = \Delta x = \frac{1}{N}  \space\space\space  (2)
$$ Above (2) is from the definition of the integral , Can I do this reduction? If NOT , why?","['integration', 'limits', 'calculus', 'definite-integrals']"
3621290,"Solving the cryptarithm ""THE+BEST+SYSTEM=METRIC""","I came across a question that I couldn't figure out. It was: What is the value of all the letters in this following cryptarithm? $$\begin{array}{ccccccc}
&&&&T&H&E\\
+&&&B&E&S&T\\
&S&Y&S&T&E&M\\
\hline
&M&E&T&R&I&C\\
\end{array}$$ The problems I can't figure out any of the letters. It is just like a bunch of letters together to form an equation. What I know The TET column and ETM column has different answers, an R and a C. In the 2 columns are 2 common letters: an E and a T. So, we can justify that the difference between E and T(or T and E) equals the difference between R and C(or C and R). Another thing I know is that the same TET column and the HSE column next to it are different and has one common letter, E. So the 2 columns will become a 2T column and an H+S column. Therefore, 2T's does not equal H+S as the answers are different. I have been staring at this question, my head is blank for an hour or so. Can I please have some help?","['puzzle', 'recreational-mathematics', 'cryptarithm', 'arithmetic', 'algebra-precalculus']"
3621291,simplify $\sqrt[3]{x \sqrt[3]{ x \sqrt[3]{x ...}} }$ -- if $x$ is negative?,"Was given this recreational problem: simplify $$\sqrt[3]{x \sqrt[3]{ x \sqrt[3]{x ...}} }$$ The solution isn't hard. Let $y = \sqrt[3]{x \sqrt[3]{ x \sqrt[3]{x ...}} }$ , then $y^3 = xy$ , $y=\sqrt{x}$ The problem didn't specify, but if $x$ is negative, would this work?","['elementary-number-theory', 'recreational-mathematics', 'complex-analysis', 'algebra-precalculus', 'complex-numbers']"
3621344,Closed form matrix derivative of $\operatorname{tr}(A\exp(X))$,"I am interested in finding a closed form expression for the following matrix derivative: $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right)$ Where we assume that $\mathbf{A}$ and $\mathbf{X}$ are both symmetric $n\times n$ matrices. By applying the following identity: $\frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\mathbf{A} \mathbf{X}^{k}\right)=\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T}$ (found in https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) I figured one can use the definition of a matrix exponential to obtain the following expression: $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A}\exp(\mathbf{X})\right)=\sum_{k=0}^\infty\frac{\partial}{\partial \mathbf{X}} \operatorname{Tr}\left(\frac{1}{k!}\mathbf{A}\mathbf{X}^{k}\right)=\sum_{k=1}^{\infty}\sum_{r=0}^{k-1}\left(\mathbf{X}^{r} \mathbf{A} \mathbf{X}^{k-r-1}\right)^{T}$ Where we skip over $k=0$ since $\frac{\partial}{\partial \mathbf{X}}\operatorname{Tr}\left(\mathbf{A})\right)=\mathbf{O}$ . I am not sure if this expression can be reduced to something which is in closed form. If you are able to suggest a method with which this might be possible, or a different method entirely, I would be helped greatly. Thanks in advance for your suggestions and help.","['scalar-fields', 'matrix-exponential', 'multivariable-calculus', 'matrix-calculus', 'derivatives']"
3621478,A curve over a finite field with apparently no points,"I am probably making a really silly mistake but I can't figure it out: Let $\mathbb F_q$ be a finite field and $a$ an element in it that is not a square. Let $E$ be the elliptic curve corresponding to the equation $y^2 = x^3 + 1$ and $E'$ it's twist $ay^2 = x^3 + 1$ . Let $f,g: E, E' \to \mathbb P^1$ be the projection to the $x$ co-ordinate and let $C = E\times_{\mathbb P^1} E'$ . Now, $C$ is a possibly singular curve and let $\pi: \tilde C \to C$ be it's normalization. This is a birational map away from finitely many points and therefore $|\tilde C(\mathbb F_{q^r})| - |C(\mathbb F_{q^r})|$ is bounded independent of $r$ . By the Weil bound, $|\tilde C(\mathbb F_{q^r})|$ (and hence $|C(\mathbb F_{q^r})|$ ) goes to infinity as $r \to \infty$ . On the other hand, a $\mathbb F_{q^r}$ point (with $r$ odd) of $C$ corresponds to a point each on $E,E'$ such that their $x$ co ordinates are equal. But if we fix a value for $x$ , then $x^3+ 1$ is either a square in $\mathbb F_{q^r} $ or it isn't and this determines whether there is a corresponding point on $E$ or on $E'$ since r is odd and so the twists continue to be non isomorphic. In particular, there are no pairs of points on $E,E'$ with the same $x$ co ordinate (apart from finitely many points at infinity independent of $r$ ) and therefore only finitely many $\mathbb F_{q^r}$ points on $C$ , independent of $r$ . I have reached a contradiction so what mistaken assumption did I make?","['arithmetic-geometry', 'finite-fields', 'algebraic-geometry', 'elliptic-curves']"
3621486,$f$ is analytic if $f$ agrees with some holomorphic function on every triplet,"I came across the following problem asked in a prelim exam. Let $f$ be a function defined on the unit disk with the property that for every triplet $a, b, c$ there exists a holomorphic function $g$ such that $g$ is bounded by $1$ on the unit disk and $g(a)=f(a), g(b)=f(b)$ and $g(c)=f(c).$ Show that $f$ is holomorphic on disk and bounded by $1$ . The fact that $f$ is bounded by $1$ is trivial. Because at every point it agrees with a function which is in turn bounded by 1. But I am trying to prove the differentiablity of $f$ and I am not able to. I was thinking to show that $f$ is differentiable at $0$ and showing that is enough to do so. I first assume that $f(0)=0.$ To prove that $f$ is differentiable at zero, I take a sequence of points $z_n\to 0$ and obtain a sequence of holomorphic functions $g_n$ such that $f(z_n)=g_n(z_n)$ . I can see that $g_n$ is normal, and hence has a convergent subsequence. I choose such a subsequence, and denote the limit by $g$ . I want to show that $f’(0)=g’(0).$ I am having problem because I can get the convergence of $g_n$ only along a subsequence, and along different subsequence I may possibly have different limits. Any suggestion is welcome.",['complex-analysis']
3621545,Calculate $\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}}$ [duplicate],"This question already has answers here : Prove $\lim\limits_{n\to\infty}n^{-1/(p+1)}(1^{1^p}\cdot 2^{2^p}\cdots n^{n^p})^{n^{-p-1}}=e^{-1/(p+1)^2}$ (2 answers) Closed 4 years ago . Calculate: $$\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}}$$ I've done some steps as follows: $$a_n:=\frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}} \iff \ln a_n=\frac{1}{n^{p+1}}\big(\sum_{k=1}^nk^p\ln k-\frac{n^{p+1}}{p+1}\ln n\big) \iff \\\ln a_n =\frac{1}{n}\sum_{k=1}^n\big[\big(\frac{k}{n}\big)^p\ln \frac{k}{n}\big]+\frac{1}{n}\sum_{k=1}^n\big(\frac{k}{n}\big)^p\ln n-\frac{\ln n}{p+1}.$$ Then, I was wondering if I could make some integrals out of it but still there are some odd terms. I think my approach isn't so good...","['limits', 'calculus', 'real-analysis']"
3621550,Properties of the sequence,"Given a sequence $\{a_i\}_{i\in \mathbb{Z}},$ consider the sequence defined by $b_i:=F(a_{i-1},a_{i},a_{i+1}),$ where $F:\mathbb{R}^3 \rightarrow \mathbb{R}$ is increasing in each of the variable, and $F(a,a,a)=a.$ Suppose total variation of the sequence $\{a_i\}_{i\in \mathbb{Z}}$ is finite i.e. $$\sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}| < \infty$$ then how to prove the following $$\sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |b_i-b_{i-1}| \leq \sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}| $$ I have an intuitive idea of the proof but some how unable to give a rigorous proof. The idea is local extremas of $b_i$ are bounded below and above by some $a_m$ and $a_n$ because of the monotonicity..How to give a rigorous proof? I could not succeed with mathematical induction either.. :(","['sequences-and-series', 'total-variation', 'analysis', 'real-analysis']"
3621668,Number of colourings of edges of a regular octahedron using n colours,"I am playing around with polyhedra symmetry, and can find the number of colourings of faces of an octahedron with ease, when rotations are considered the same colouring. I am struggling to convert this to finding the polynomial in the number of colours when trying to colour the edges of a regular octahedron with different rotations counting to be the same colouring. I have been using the orbit counting formula for this, but cannot get a polynomial which gives integer results for a number of colours. Thank you in advance.","['group-theory', 'abstract-algebra', 'combinatorics', 'geometry']"
3621749,Prove that every reflexive Banach space is weakly complete.,Here is the question: A sequence $(x_{n})$ in a normed linear space $X$ is weakly Cauchy if $(Tx_{n})$ is a Cauchy sequence for every $T \in X^*.$ The space $X$ is weakly complete if every weakly Cauchy sequence in $X$ is weakly convergent. Prove that every reflexive Banach space is weakly complete. Could anyone help me in this proof please?,"['banach-spaces', 'functional-analysis', 'weak-convergence']"
3621755,Differential geometry: non-uniqueness of the closest point on a curve,"I have looked for theorems about the closest points, but I could not find such a theorem I need to establish some other claim. The main question is expressed as a proposition as follows: Proposition:
Consider a $C^2$ naturally parametrized curve $\boldsymbol{\gamma}:\mathbb{R} \rightarrow \mathbb{R}^3$ defined in the three-dimensional Euclidean space with $\Vert \boldsymbol{\gamma}''(s) \Vert > 0$ for all $s \in [s_0, s_1]$ , $s_1 - s_0 \geq \epsilon > 0$ . Then, there is a non-empty set $A \subset \mathbb{R}^3$ such that whenever ${\bf{p}} \in A$ , ${\bf{p}}$ has at least two distinct closest points on $\boldsymbol{\gamma}$ . To help interpretation, $s$ is the arc length parameter, $\Vert \boldsymbol{\gamma}'(s) \Vert = 1$ , $\kappa := \Vert \boldsymbol{\gamma}''(s) \Vert$ is the curvature of the curve. Roughly speaking, the proposition states that if a twice continuously differentiable curve is bent on some interval, there is a region such that a point belongs to the region has always two or more closest points on the curve. A representative example is a straight line that passes through the center of a circle and is perpendicular to the plane the circle lies on. I want to show that there is such a point for more general curves. In contrast, a straight line, whose curvature is zero for the entire interval, can always have a unique closest point. Is there any theorem related to this claim? Thank you.","['curves', 'proof-writing', 'differential-geometry']"
3621775,How find the limit $\lim_{n\to\infty}\frac{\sum_{k=1}^n|\cos(k^2)|}{n}$.,"Compute $$\displaystyle\lim_{n\to\infty}\dfrac{\displaystyle\sum_{k=1}^n|\cos(k^2)|}{n}$$ . I guess is $\dfrac{2}{\pi}$ ,because the summation is essentially equal to computing the average value of $|\cos k|$ on the interval from $[0, \pi]$ , which is $\boxed{\dfrac{2}{\pi}}$ ,it's right?Thanks",['limits']
3621911,Find the minimum value of $f$,"Find the minimum value of $$f(x)=\frac{\tan \left(x+\frac{\pi}{6}\right)}{\tan x}, \qquad x\in \left(0,\frac{\pi}{3}\right).$$ My approach is as follows. I tried to solve it by segregating it $$f(x)=\frac{1}{\sqrt{3}\tan x}+\left({\sqrt{3}}+\frac{1}{\sqrt{3}}\right)\frac{1}{\sqrt{3}-\tan x},$$ but $f'(x)$ is getting more and more complicated.",['trigonometry']
3621940,Finite abelian subgroups of the Möbius group are either cyclic or isomorphic to $C_2\times C_2$,"I am meant to use the fact that Möbius transformations of finite order have exactly two fixed points. Let $G$ be the subgroup. I have observed that if $f,g\in G$ where $f$ fixes $z_0,z_1$ , then $f=gfg^{-1}$ fixes $g(z_0),g(z_1)$ , so $g$ either fixes $z_0,z_1$ or interchanges them, in which case they are fixed by $g^2$ . Other than noting that $\mathrm{ord}(fg)=\mathrm{lcm}(\mathrm{ord}(f),\mathrm{ord}(g))$ , and that maps which fix two points are conjugate to $z\mapsto az$ , I'm not sure where to go from here.","['group-theory', 'abstract-algebra', 'mobius-transformation']"
3622002,"If I bend a straight line segment such that the arc length of the line remains constant, how can I express the transformation as a function?","Imagine we have a line segment of length $1$ on the $x$ -axis. Keeping the point at $(0,0)$ fixed at $(0,0)$ , we bend the line segment into a parabola such that every point on the bent line now satisfies $y = x^2$ . This should yield a parabola starting from $(0,0)$ with arc-length $1$ . How can I express this transformation such that a point $(x, 0)$ on the original line is transformed to some point $(x', y')$ on the parabola? Is it possible to find an equation that, given x, returns $(x', y')$ ? Is it possible to obtain this for bending into any arbitrary, smooth and continuous shape?","['curves', 'geometry', 'differential-geometry']"
3622038,Probelm in solving $\sin\left(\frac{x}{2}\right) - \cos\left(\frac{3x}{2}\right) = 0$,"While solving $\sin\left(\frac{x}{2}\right) - \cos\left(\frac{3x}{2}\right) = 0$ , if I convert $\sin$ into $\cos$ , I am getting answer (i.e. $\left(n + \frac{1}{4}\right)\pi$ and $\left(2n - \frac{1}{2}\right)\pi$ . However, if I convert $\cos$ to $\sin$ , I am not getting answer (i.e. $\left(\frac{1}{2} - n\right)\pi$ and $\left(\frac{n}{2} +\frac{1}4{}\right)\pi$ ). Where am I wrong? Is there any specific way to solve these equations?",['trigonometry']
3622053,Pathwise almost sure bound of a solution to an SDE,"Assume that $X:(\mathbb R^+ \times \Omega) \to \mathbb{R}^n$ is a solution to an SDE of the form $dX = \mu(X,t)  dt + \sigma(X,t)dW$ where $\mu, \sigma$ are continuous, Lipschitz continuous in the first variable (with a Lip constant idependent of $t$ ) and satisfy $||\mu(x,t)|| \le c||x|| +1 $ and $||\sigma(x,t)|| \le c||x|| +1 $ for some $c>0$ independent of $t$ . Q: Can we deduce that from here that, for almost every $\omega \in \Omega$ there exists $p(\omega)>0$ such that $X(t, \omega) \exp(-p(\omega)t) \to 0$ as $t \to \infty$ ? In the deterministic case, this follows from the Gronwall lemma. I don't know if it is of any help, but it also resembles the exponential bound that one can obtain for semigroups of operators (via uniform boundedness), hence I assume that a proof could involve a Baire Category Theorem-like argument.","['stochastic-processes', 'stochastic-differential-equations', 'partial-differential-equations', 'probability-theory', 'stochastic-calculus']"
3622068,Show that $\mathbb P_\mu(A) = \int \mu(\mathrm dx) P_{\delta_x} (A)$,"It is known that for sufficiently ""nice"" space $S$ , and a transition kernel $p$ on $S$ : the consistent collection of finite dimensional distribution defined by $$\mathbb P_n(X_0\in B_0,\cdots,X_n\in B_n)=\int_{B_0}\mu(\mathrm dx_0)\int_{B_1}p(x_0,\mathrm dx_1)\cdots\int_{B_n}p(x_{n-},\mathrm dx_n)$$ may be extended to a probability measure $\mathbb P_\mu $ on $S^{\mathbb{N}}$ . I would like to show that $$\mathbb P_\mu(A) = \int \mu(\mathrm dx) P_{\delta_x}(A),$$ where $\mathbb P_x $ are the measures introduced similarly from $$\mathbb P_n(X_0 \in B_0, \cdots, X_n \in B_n) = \int_{B_0} \delta_x(\mathrm dx_0) \int_{B_1} p(x_0, \mathrm dx_1) \cdots \int_{B_n}  p(x_{n-1}, \mathrm dx_n).$$ That is for each $x$ we have changed the initial distribution from $\mu$ to the point mass $\delta_x$ , and then we integrate those new probabilities of $A$ over $x$ with respect to $\mu$ . Would it be sufficient to show that for any $n $ and any $B_0, \cdots, B_n$ , $$\int_{B_0} \mu(\mathrm dx_0) \int_{B_1} p(x_0, \mathrm dx_1) \cdots \int_{B_n} p(x_{n-1}, \mathrm dx_n)\\
= \int \mu(\mathrm dx) \int_{B_0} \delta_x(\mathrm dx_0) \int_{B_1} p(x_0, \mathrm dx_1) \cdots \int_{B_n} p(x_{n-1}, \mathrm dx_n)?$$ Much grateful for any help provided! EDIT: I see that I haven't written anything about the sequence $(X_n) $ above. I believe if we want the first equation to hold we may take $(X_n)$ to be some kind of canonical process?","['stochastic-processes', 'measure-theory', 'probability-theory']"
3622146,Branched cover in algebraic geometry,"I've watched a lecture on K3 surfaces (although K3 surfaces are not the point of this question) where the following example is given: Let $\pi:S\stackrel{2:1}{\to}\Bbb{P}^2$ be the branched double cover ramified over a smooth sextic curve $C\subset\Bbb{P}^2$ . Then $S$ is a K3 surface. The lecturer assumes everyone is familiar with those terms and goes on saying: ""[...] by the properties of double covers, $K_S=\pi^*K_{\Bbb{P}^2}+R$ , where $R$ is the ramification. Furthermore, $\pi^*C=2R$ "" I'm having a hard time trying to find out what ""branched cover"" and ""ramification"" mean, how the map $\pi$ is constructed, let alone figuring out why those pullback properties are true. I've looked for precise definitions in books (Hartshorne, Shafarevich, Harris, Görtz-Wedhorn, Beauville) and internet-based material (Vakil, Gathmann, wikipedia), couldn't find it anyhere. Many of these references eventually mention ""branch"" or ""ramification"" in passing or loosely, as if assuming the reader knows about it. So my questions are: What are the definitions of ""branched covering"" and ""ramification""? What is the map $\pi$ explicitly? Is there a code of ethics among algebraic geometers to make simple things harder for newcomers? I hope I won't get anyone in trouble with the ethics commitee.","['algebraic-curves', 'algebraic-geometry', 'projective-varieties']"
3622187,Find the supremum of the following set (differential inequalities),"Let $X=\Bbb{R}^{\Bbb{R}}\cap C^{2}$ that is the set of all functions $f:\Bbb{R}\to\Bbb{R}$ for which the second derivative exists on $\Bbb{R}$ .
Let $$
 A_f = \Vert f \Vert_\infty = \sup \{ |f(x)| : x \in \Bbb R \} \, ,\\
 B_f = \Vert f' \Vert_\infty = \sup \{ |f'(x)| : x \in \Bbb R \} \, , \\
 C_f = \Vert f'' \Vert_\infty = \sup \{ |f''(x)| : x \in \Bbb R \} \, .
$$ The task is to find $$
 M = \sup \left\{ \frac{B_f^2}{A_f \, C_f} : f \in X; A_f, C_f < \infty \right\} \, .
$$ Very rough approximations that ignore the limits in definitions of derivatives indicate that the answer might be $\infty$ but that doesn't sit right with my intuition and I haven't been able to find an array of functions for which $\dfrac{B_f^2}{A_f\, C_f}$ could get arbitrarily large. Any hints would be appreciated. EDIT: In Prove $\sup \left| f'\left( x\right) \right| ^{2}\leqslant 4\sup \left| f\left( x\right) \right| \sup \left| f''\left( x\right) \right| $ the upper bound $M \le 4$ is proven. An example of a function with $M=4$ would be sufficient :)","['calculus', 'functional-analysis', 'real-analysis']"
3622200,Total derivative of $\psi(x) = \frac{1}{\left \| x \right \|^p}Ax$,"I have been trying to find the Fréchet derivative of the following function: $\psi(x) = \frac{1}{\left \| x \right \|^p}Ax$ $(x \in \mathbb{R}^n, A \in\mathbb{R^{m \times n}})$ . One possibility would be to use the derivative of continuous and bilinear operators on Banach Spaces, so in this case if we could set $\psi(x)=B(f(x),g(x))$ whereas $f:\mathbb{R^n} \rightarrow \mathbb{R}, x \mapsto\frac{1}{\left \| x \right \|^p}$ and $g:\mathbb{R^n} \rightarrow \mathbb{R^m}, x \mapsto Ax $ . And $B:(\mathbb{R} \times \mathbb{R}^m) \simeq \mathbb{R}^{m+1} \rightarrow \mathbb{R}^m ,B(x,y)=xy$ $$$$ The formula I got to is the following ( $Df$ is the total derivative of $f$ ): $$D\psi(x)=g(x)Df(x)+f(x)Dg(x)$$ But then the wikipedia article on the product rule states that: $$D(f \cdot g)=Df \cdot g+f \cdot Dg(x) (""."" \text{represents scalar multiplication})$$ which is different as $Df$ and $g$ do not commute. I checked the dimensions of the objects I'm using to see where the difference lies, and I actually discovered that using the Wikipedia article the matrix multiplication dimensions do not match. Does anyone see whether there is a mistake in my reasoning, and if so then where? By the way, I am looking for an answer that does NOT use partial derivatives. I can of course solve this problem using them, but I'm looking for a more general approach that doesn't just rely on heavy calculations.","['frechet-derivative', 'multivariable-calculus', 'derivatives', 'bilinear-form']"
3622248,A morphism that is defined over $\mathbb{Q}$ and is an isomorphism over $\mathbb{C}$ may not be an isomorphism over $\mathbb{Q}$.,"Is there an example of two algebraic varieties $X,Y$ over $\mathbb{Q}$ and a morphism $f:X\rightarrow Y$ defined over $\mathbb{Q}$ , that is an isomorphism over $\mathbb{C}$ but not over $\mathbb{Q}$ ? That is, the inverse $f^{-1}$ doesn't have rational coefficients. It sounds like an easy problem but I have been doing computations (with $X=\mathbb{A}^1$ and $Y$ a plane curve) and I couldn't find any.",['algebraic-geometry']
3622285,What mistakes were made in evaluating $\int_0^{2\pi}e^{2it}\ln(a^2-2a \cos(t) + 1)dt$,"I'm trying to evaluate $$\int_0^{2\pi}e^{2it}\ln(a^2-2a \cos(t) + 1)dt$$ for $a \in (0, 1)$ . I keep getting different answers depending on the method.  First, if I split up the integrals in to real and imaginary parts I get: $$ = \int_0^{2\pi} (\cos(2t) + i \sin(2t)) \ln(a^2-2a \cos(t) + 1)dt\\
= \int_0^{2\pi} \cos(2t) \ln(a^2-2a \cos(t) + 1)dt
+ i \int_0^{2\pi} \sin(2t) \ln(a^2-2a \cos(t) + 1)dt\\
= - \frac{1}{4 a^2} \left[\begin{array}&
 (a^4 + 1) t \\
- 2 (a^4 - 1) \arctan \frac{(a + 1) \tan(t/2)}{a - 1} \\
+ 2 (a^3 + a) \sin(t) \\
+ a^2 \sin(2 t) (1 - 2 \sin(2 t) \ln(a^2 - 2 a \cos(t) + 1))
\end{array} \right]_0^{2\pi} + i * 0 \\
= - \pi \frac{a^4 + 1}{2 a^2}$$ Where I got $\int \cos(2t) \ln(a^2-2a \cos(t) + 1) dt$ from Wolfram Alpha .  This other online integral calculator gives a different but equivalent antiderivative. Second, if I do a contour integral: $$ = \int_0^{2\pi} e^{2it} \ln((1-ae^{it}) (1 - a e^{-it})) dt \\
= \int_0^{2\pi} e^{2it} (\ln(1-ae^{it}) + \ln(1 - a e^{-it})) dt \\
= \int_0^{2\pi} e^{2it} \ln(1-ae^{it}) + \int_0^{2\pi} e^{2it} \ln(1 - a e^{-it}) dt \\
= \oint \frac{z \ln(1-az)}{i}dz - \oint \frac{\ln(1 - a z)}{iz^3} dz \\
= 2 \pi i (0 - \frac{a^2}{2i}) \\
= -\pi a^2
$$ Third, if I ask an online integral calculator , I just get 0. I have no idea which, if any, answer is correct, or what mistakes I made in any of the methods.  Any help would be appreciated.","['integration', 'complex-analysis', 'contour-integration', 'definite-integrals']"
3622329,integration of a concave function,"I'm having some trouble with the following question: let $ f: [0,2]\to \Bbb R$ be a continuous nonnegative function. It is also given that $f$ is concave ( $\cap$ )
   that is for each two points $x,y\in[0,2]$ and $\lambda\in[0,1]$ sustain $$f(\lambda x + (1-\lambda)y)\ge\lambda f(x)+(1-\lambda)f(y)$$ Lets assume that $f(1)=1$ , prove that $$\int_0^2f(t)dt\ge1$$ I tried finding a linear function that this integral is greater than (thought about $X$ for example) but I didnt get any further than that","['integration', 'calculus', 'functions']"
3622331,Unique way to express a positive rational number,"In the book of G.H. Hardy, ""A course of pure mathematics"", I found this miscellaneous problem from chapter I, is the exercise 2: Prove: Any positive rational number can be expressed in one and only one way in the form $$
a_{1} + \frac{a_{2}}{2!} + \frac{a_{3}}{3!} + \cdots + \frac{a_{k}}{k!}
$$ where $a_{1},a_{2},a_{3},\cdots,a_{k}$ are integers, and $$
0 \leq a_{1},0 \leq a_{2} < 2,0 \leq a_{3} < 3, \cdots , 0 < a_{k} < k 
$$ I have left this image for reference, which is from the exercise in the book: I have some ideas how to prove this, maybe using the representation of $e^{x}$ as an infinite sum, but in general, I have no idea how to solve this problem.","['algebra-precalculus', 'analysis', 'real-analysis']"
3622348,"Help interpret exercise 4.2 in Probability Essentials, Jacod & Protter","Some years ago there was a question here regarding this exercise. I find it a bit confusing and I'm struggling to understand what is in fact being asked here. My issue is with the first part of question 4.2. I want to prove $q_k$ are the probabilities of singletons for a $Binomial(1-p,n)$ . In order to check if what is being asked can be true I computed the singleton probabilities $p_k$ for a $Binomial(0.3, 4)$ in Julia and subtracted each $p_k$ to $1$ to get the corresponding $q_k$ . I then computed the singleton probabilities of a $Binomial(1-0.3, 4)$ . As you can see results are not the same: using Distributions

n = 4
p = 0.3

d1 = Binomial(n, p)
p_ks = pdf(d1)  # returns the probabilities of singletons for a B(p=0.3, n=4)
q_ks = 1 .- p_ks

d2 = Binomial(n, 1-p)
d2_p_ks = pdf(d2) # returns the probabilities of singletons for a B(p=0.7, n=4) julia> q_ks
5-element Array{Float64,1}:
 0.7599
 0.5884
 0.7353999999999998
 0.9244
 0.9919 julia> d2_p_ks
5-element Array{Float64,1}:
 0.008100000000000003
 0.07560000000000003
 0.2646000000000001
 0.4115999999999999
 0.24009999999999992 Is the exercise wrong or am I missing something? Summing every $q_k$ doesn't even add up to 1.","['measure-theory', 'probability-theory']"
3622415,"Show that $\mathbb P[X_{n+1 }\in B|\mathcal{F}_n]=p(X_n,B)$ where $p$ is a transition kernel","Given a transition kernel $p $ on $\mathbb R$ : we may define a consistent collection of finite dimensional distribution by letting $X_i(\omega) = \omega$ and writing $$\mathbb P_n (X_0 \in B_0, \dots, X_n \in B_n ) = \int_{B_0 }  \mu(dx_0) \int_{B_1 } p(x_0, dx_1) \ \dots \int_{B_n }  p(x_{n-1 } ,d_xn)  $$ It is known [See Ionescu-Tulcea theorem] that this may be extended to a probability measure $\mathbb P_\mu $ on $\mathbb R^{\mathbb{N}} $ . I would like to show that given any transition kernel $p$ , a sequence $(X_n)$ and a probability measure $\mathbb P $ that satisfies the equation above,  that $$\mathbb P[X_{n+1 }\in B|\mathcal{F}_n]=p(X_n,B)$$ This is what I have tried to gather: For the claim to be true we should show that for any $A \in \sigma(X_0, \dots, X_n )$ and $B \in \mathcal{B}(\mathbb{R})$ $$\int _A 1_{\{X_{n+1 } \in B \} } d \mathbb P = \int_A p(X_n, B) d \mathbb P$$ [I am a little bit unsure here which measure we should integrate with respect to as I have seen some sources write this integral with respect to $\mathbb P_\mu$ but that does not make sense to me since $p(X_n, B)$ is a function from $\Omega$ if $X_n: \Omega \to \mathbb{R}$ .] If it would be sufficient to consider $A $ of the form $\{X_0 \in B_0, \dots, X_n \in B_n \} $ (which I believe it is since I think those sets generate $\mathcal{F}_n$ ?) then we would have \begin{multline}
        \int_A 1_{\{X_{n+1 } \in B \} } d \mathbb P = \mathbb P(X_0 \in B_0, \dots, X_n \in B_n, X_{n+1 } \in B) \\
        = \int_{B_0}  \mu(d x_0) \int_{B_1}  p(x_0, dx_1) \dots \int_{B_n}  p(x_{n-1 }, dx_n) p(x_n, B)
\end{multline} If we could in fact show that $$\int_{B_0}  \mu(d x_0) \int_{B_1}  p(x_0, dx_1) \dots \int_{B_n}  p(x_{n-1 }, dx_n) f(x_n) = \int_A f(X_n(\omega)) d \mathbb P $$ for bounded measurable $f$ , then (1) would follow. To prove the equality for bounded measurable $f$ it would be sufficient to prove it for an indicator function and then use the customary approximation arguments. So I tried this for $f = 1_D $ and got \begin{multline*}
        \int_A 1_D d \mathbb P = \mathbb P(A \cap D) \\
        = \int_{B_0 \cap D} \mu(dx_0) \int_{B_1 \cap D } p(x_0, dx_1) \dots \int_{B_n \cap D } p(x_{n-1 } dx_n) 
\end{multline*} Does \begin{multline*}
        \int_{B_0}  \mu(d x_0) \int_{B_1}  p(x_0, dx_1) \dots \int_{B_n}  p(x_{n-1 }, dx_n) 1_D(x_n) \\
        = \int_{B_0 \cap D} \mu(dx_0) \int_{B_1 \cap D } p(x_0, dx_1) \dots \int_{B_n \cap D } p(x_{n-1 } dx_n)
\end{multline*} ? Thus my questions are 1) the last equation. 2) If it is sufficient to consider $A$ of the form $\{X_0 \in B_0, \dots, X_n \in B_n \}$ ? and 3) If I got it right integrating w.r.t. $\mathbb P$ - the measure on the space on which $(X_n) $ is defined? Most grateful for any help provided!","['stochastic-processes', 'probability-theory', 'markov-chains']"
3622435,"What does the ""standard"" in ""standard deviation"" mean?","I know what the standard deviation is. However, I can't make sense of its name. What does the word ""standard"" refer to? Is it a synonym for mean, so that the ""standard deviation"" is the ""deviation from the mean value""? Is it a methodology, so that the ""standard deviation"" is the ""standard way of determining the deviation"" from the mean? Wikipedia says that the term standard deviation was coined by Karl Pearson in 1894. As far as I could see, his cited article Contributions to the Mathematical Theory of Evolution does indeed introduce the term, but doesn't comment on the chosen name.","['statistics', 'standard-deviation', 'terminology']"
3622466,Intuition behind topological conjugacy between the flows of two systems of differential equations,"My book defines topological conjugacy between the flows of two differential equations as follows: Suppose $X' = AX \text{ and } X' = BX$ have flows $\phi ^A$ and $\phi ^B$ . These two systems are topologically conjugate if there exists a homeomorphism $h: \mathbb{R}^2 \to \mathbb{R}^2$ that satisfies $\phi ^B (t, h(X_0))= h(\phi ^A (t,X_0))$ . I understand the idea behind homemorphism, but why is conjugacy defined the way it is? Why is $h$ of the initial condition taken on one side while h of the flow is taken on the other? Why isn't it defined like $\phi ^B (t, X_0)= h(\phi ^A (t,X_0))$ instead?","['ordinary-differential-equations', 'dynamical-systems']"
3622469,Question about $f(x)=\sum_{k=1}^\infty (-1)^{k+1}\sin (\frac{x}{k}) $,"This function is rather peculiar. It is easy to establish the following: $$f(x) =\sum_{k=0}^\infty (-1)^k A_{2k+1} \cdot x^{2k+1}, \mbox{ with } A_k=\Big(1-\frac{1}{2^{k}} + \frac{1}{3^{k}}- \frac{1}{4^{k}}+\cdots\Big).$$ Note that $A(1)=\log 2$ , and for $k>1$ , we have $$A(k)= \Big(1-\frac{1}{2^{k-1}}\Big)\zeta(k)$$ where $\zeta$ is the Riemann Zeta function. Also, $f(-x) = - f(x)$ and we have the following approximation when $x$ is large, using a value of $K$ such that $x/K < 0.01$ : $$f(x) \approx \sum_{k=1}^K (-1)^{k+1}\sin \Big(\frac{x}{k}\Big) - x\cdot\sum_{k=K+1}^\infty \frac{(-1)^{k}}{k}$$ The function is smooth but exhibits infinitely many roots, maxima and minima. I am in particular interested in the following quantity: $$g(x) = \sup_{0\leq y\leq x}f(y).$$ What is the growth rate for $g(x)$ ? Is it linear, sub-linear, or super-linear? Another question of interest is the average spacing between two roots or two extrema. Below are two plots of $f(x)$ , the first one for $0\leq x\leq 200$ , the second one for $0\leq x\leq 2000$ . Addendum: Failed attempt to solve this I used the Euler-Maclaurin summation formula to get a good approximation for $f(x)$ when $x$ is large, and this leads to $$f(x) \approx \int_1^\infty \Big(\sin\frac{x}{2u} - \sin\frac{x}{2u+1}\Big) du.$$ A closed form for this integral exists, involving the cosine integral, see WolframAlpha here . Lots of asymptotic formulas are available (see here ) but when I apply them, I end up with $f(x)$ being bounded, which is very clearly not the case based on my observations. As an illustration, below is the computation of $f(x)$ for $x = 52,000,001$ . The first chart shows $f(x)$ based on the first $n=2000$ terms in the series. Here the X-axis represents $n$ , and the Y-axis represents $f(x)$ for the particular value of $x$ in question, when using a growing number of terms. In the second chart, $n$ goes to $200,000$ . Stability is reached after adding about $4,100$ terms, and oscillations are slowly dampening then. One promising approach is this. Let $$ f_k(x)=\sum_{i=1}^k (-1)^{i+1}\sin \Big(\frac{x}{i}\Big) .$$ Define $h_k(x) =\frac{1}{2}(f_k(x) + f_{k-1}(x))$ .Then $f(x) = \lim_{k\rightarrow\infty} h_k(x)$ . The iterates $h_k$ 's are much smoother than the $f_k$ 's, and convergence is much faster.","['summation', 'real-analysis', 'maxima-minima', 'sequences-and-series', 'riemann-zeta']"
3622552,"Let S,T be sets, satisfying the property : $\forall x \in T$, $x \notin S$. Prove that $S \cap T = \varnothing$","I've been thinking about this problem for roughly half an hour. I can't think of how to use subset proof structure for the empty set. Here's my answer so far: Let $S, T$ be sets with $\forall x \in T , x \notin S$ This means that the set $S$ does not contain the element $x$ . Because all of $x \in T$ , $S$ and $T$ share no elements except the empty set. Hence, when $S$ and $T$ intersect, they always produce the empty set. Am I missing the usage of definitions? And unfortunately I may not reference theorem that depict the intersection of a set with the empty set.","['solution-verification', 'discrete-mathematics']"
3622560,Derivatives car velocity paradox,"This is my first post on the Maths section of StackExchange. I am a high school student from Greece studying for IAL Math and got a bit confused on derivatives while watching this video; specifically on minute 14:29. The author gives an example of a car traveling on the curve $x^3$ where $x \in [0, 3]$ . He then asks if the car is moving or not at $x = 0$ Let's use $x^2$ instead of $x^3$ here for simplicity. Through the use of the derivative of $x^2$ , $\frac{dy}{dx} = 2x$ we see that at $x=0$ the velocity of the car is $\frac{dy}{dx}=2(0)=0$ which suggests that the car is not moving. I can see, however, the point the author is making, arguing that the car is actually moving, since the derivative formula for $x^2$ is derived as such: $
\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}=
\lim\limits_{h \to 0} \frac{(x+h)^2-x^2}{h}=
\lim\limits_{h \to 0} \frac{x^2+2hx+h^2-x^2}{h}=
\lim\limits_{h \to 0} \frac{2hx+h^2}{h}=
\lim\limits_{h \to 0} 2x + h =
2x
$ where in the penultimate step we can see that the derivative formula is actually $2x$ plus some very very small quantity because $h$ doesn't actually ever reach zero. So is the car actually moving with its velocity being that very very small quantity? How can someone first argue that $h$ approaching zero can be neglected from the equation(last step) and subsequently contradict himself and argue that $h$ approaching zero should actually be included? Is it safe to assume that for $\frac{dy}{dx}=0$ an object is stationary? (In the context where $y$ is the distance travelled and $x$ is time) Moreover, for $x=0$ the derivative formula's result is $0$ which should also be the gradient of the line tangent to the parabola at $x=0$ And indeed, the gradient of the tangent line is $y=0$ which crosses $y=x^2$ only at point $0$ and not at point $0$ plus some very very small quantity ( Desmos ) I am very confused by this and would really appreciate help. I have also tried to make my questions as clear as possible but since I do not seem to understand calculus and limits all that well I will be more than willing to provide any clarifications.","['calculus', 'derivatives']"
3622592,De Morgan's formulas set theory,"De Morgan Formulas I can't seem to wrap my head around these two formulas. I don't know whether it's that I don't understand the union and intersection of elements of gamma being put into set A. Or either that I don't understand how the distributive property works in these kind of problems. If anyone could please help it would be greatly appreciated, thanks.",['elementary-set-theory']
3622598,Maximum number of iterations of a simple algorithm,"Suppose there is a 0-1 string of length n. We can perform the following operation on the string:
We can choose two zeros and invert the subsequence between them. The inversion includes the two zeros aswell. For example if the string is 011010, and we choose the first and fourth zeros it becomes 100110. We can also choose just one 0 and turn it into 1.
It can be proved that after some iterations the whole string will only consist of 1s. So my question is: What is the maximum number of iterations we can perform before it becomes the all 1 string. My approach was to construct a sequence of iterations that seems to be optimal, but I can't prove that it is. (Obviously the maximum can be achieved if we start from the all 0 string.) If the lenght of the string is even, so n is an even number. I would choose the middle 2 bits, and change them to 11 in two iterations $(00 \rightarrow 01 \rightarrow 11)$ . After that i would reset the middle by choosing the bits next to these two $(0110 \rightarrow 1001$ ). So I could the first step again, and so on. If n is an odd number. Then I would do almost the same. I would convert the middle one into zero, then reset it with the two bits next to it. $( 00000 \rightarrow 00100 \rightarrow 01010 \rightarrow 01110 \rightarrow 10001 \rightarrow 10101 \rightarrow 11011 \rightarrow 11111) $ We can calculate that the number of iterations for this algorithm is: \begin{cases}
2^{{\frac{n+1}{2}}}-1, & \text{for odd } n \\
2^{\frac{n}{2}}+2^{\frac{n}{2}-1}-1, & \text{for even } n 
\end{cases} So we can conclude that the maximum number of iterations is greater than this amount. But I think this is the maximum, so this sequence of iterations optimal, but I can't prove it. Could you please give me some hints on how to prove this, or if it is not true, give me a counterexample.","['recursive-algorithms', 'combinatorics', 'algorithms']"
3622615,"Estimate expected payoff of rolling a dice, with choice of rolling up to $50$ times.","This is an extended question of the classical rolling dice and give face value question. You roll a dice, and you'll be paid by face value. If you're not satisfied, you can roll again. You are allowed $k$ rolls. In the old question, if you are allowed two rolls, then the expected payoff is $E[\text{payoff}] = 4.25$ . If  you are allowed $3$ rolls, the expected payoff is $E[\text{payoff}] = 4.67$ . If you can roll up to $50$ times, you can calculate the payoff using the formula and get $E = 5.999762$ , notice that after $5^\text{th}$ roll, your expected payoff will be greater than $5$ , so you'll only stop once you roll $6$ . So my question here is, without exact calculation(using geometric process), how would you estimate how many $9$ s are there in the answer? Or another way to ask will be, is the expected payoff bigger than $5.9$ ? bigger than $5.99$ ? etc.","['expected-value', 'dice', 'probability']"
3622678,"A two-bear exercise based on the ""A bear walks one mile south, one mile east, one mile north"" puzzle.","A bear walks one mile south, one mile east, and one mile north, only to find itself where it started. Another bear, more energetic than the first, walks two miles south, two miles east, and two miles north, only to find itself where it started. However, the bears are not white and did not start at the north pole. At most how many miles apart, to the nearest $.001$ mile, are the two bears’ starting points? I am very confused. How can a bear walk south, north, east and return? Shouldn't the bear should be $1$ mile away from the starting point? For those curious, the solution which I have trouble comprehending is item 1 from the IXth Annual Harvard-MIT Mathematics Tournament, 2006 (PDF link via amazonaws.com) Reproduced here: Say the first bear walks a mile south, an integer $n > 0$ times around the south pole, and then a mile north. The middle leg of the first bear’s journey is a circle of circumference $1/n$ around the south pole, and therefore about $\frac{1}{2n\pi}$ miles north of the south pole. (This is not exact even if we assume the Earth is perfectly spherical, but it is correct to about a micron.) Adding this to the mile that the bear walked south/north, we find that it started about $1 + \frac{1}{2n\pi}$ miles from the south pole. Similarly, the second bear started about $2 + \frac{2}{2m\pi}$ miles from the south pole for some integer $m > 0$ , so they must have started at most $$3+ \frac{1}{2n\pi} + \frac{2}{2m\pi} \leq 3+ \frac{3}{2\pi} \approx 3.477$$ miles apart.","['euclidean-geometry', 'spheres', 'geometry', '3d']"
3622680,General formula for $f(n)$,"Let for $n\geq 3, C_n$ denote the $(2n) \times (2n)$ matrix such that all entries along the diagonal are $2$ , all entries along the sub- and super-diagonal are $1$ , all entries along the antidiagonal are $1$ , all entries along the diagonals directly above and below the antidiagonal are $2$ , and all other entries are zero. Let $f(n) : \mathbb{N}\to\mathbb{N}, f(n) = \det(C_n)$ for $n\geq 3.$ Prove that $$f(n) = \begin{cases}0,&\text{if }n = 3k+2,\ k\in\mathbb{N}\\
3^n,& \text{otherwise}\end{cases}.$$ I'm not sure how to go about doing this. I tried cofactor expansion along the first column, but I couldn't make much progress. I can't seem to find a recursive relationship. So I just tried converting $C_n$ to an upper triangular matrix using row operations. This results in a matrix satisfying certain patterns, but I can't seem to find a way to prove why reducing the matrix always produces these patterns (I can prove that the $k$ th diagonal entry of the resulting upper triangular matrix is $\frac{k+1}k,$ where $1\leq k\leq n$ but I can't deal with the other $n$ diagonal entries well).","['matrices', 'sparse-matrices', 'determinant', 'linear-algebra']"
3622737,Verify if the following set is open or not,"Let's consider the set $X := \{(x,\,0,\,0)\in \mathbb{R^{3}}: 0 < x < 1\}$ . Under the usual topology of $\mathbb{R^3}$ , is this set open? My guess it is not, if we sketch it, but how can one analytically prove this, in terms of open balls and?
Thanks in advance!","['metric-spaces', 'real-analysis', 'multivariable-calculus', 'calculus', 'general-topology']"
3622879,Integration by parts with a gradient operator,"I know that integration of parts can be used on a single integral: $$\int \frac{df}{dx} g   \,dx = fg - \int f\frac{dg}{dx}  \,dx$$ For a triple integral with a gradient operator, is the following operation legitimate? $$\iiint_V \nabla(f)g \,dV \,=?\, fg - \iiint_V f\nabla(g) \,dV$$ If not, how would one simplify the expression on the left? Please explain.","['integration', 'derivatives', 'vector-analysis']"
3622888,Area and center location of an ellipse generated by the intersection of an ellipsoid and plane,"I am working on a model that requires that I know the area and center coordinates of the ellipse that is created by the intersection of an ellipsoid and a plane. Specifically, for the location of the center of the ellipse I want to know the coordinates of this ellipse in Cartesian coordinates. The general equations for the ellipsoid and plane I have started with are: $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$$ $$m(x-x_0) + n(y-y_0) + k(z-z_0) = 0$$ I am writing the general forms of these equations because actually I need to be able to solve this for a number of different ellipses and planes with different orientations. One specific case that I would like to use this solution for first is the case of an ellipsoid that is defined by: $$a = 7, b  = 5, c = 6$$ and a plane defined by: $$(y+b)tan(\theta)-z + \frac{1}{2} = 0$$ Where $\theta$ is the desired angle of the plane, in this case $\theta=30^o$ is perfectly fine (arbitrary example). Please note that ""b"" is the same b used in the equation of the ellipsoid. I looked at some of the other threads that have asked about plane ellipsoid intersections. However, since I specifically need to calculate the area of the ellipse generated by this intersection and the location of its center, when I tried to use a parametric solution, I had difficulty doing this once I had the parametric equations. I would really love to learn how to solve this problem so I can include it into my model. Help is greatly appreciated. Thank you! -Christian","['conic-sections', 'geometry', 'multivariable-calculus', 'linear-algebra', 'parametrization']"
3622904,Identity of binomial coefficients: $\binom{n+1}{k+1}=\sum_{i=m}^{n+m-k}{\binom{n-i}{k-m}\binom{i}{m}}$,"I came across this equation when solving another combinatorics problem. I needed to prove the following identity: $$
\begin{aligned}
\binom{n+1}{k+1}&=\sum_{i=m}^{n+m-k}{\binom{n-i}{k-m}\binom{i}{m}}
\end{aligned}
$$ My attempt: The right hand side is selecting $k-m$ balls from a number of red balls and $m$ balls from a number of blue balls where the number of red and blue balls vary but the total is always $n$ . I imagine this is the same as lining up $n+1$ non - colored balls, then selecting $k+1$ balls from these. Then proceed to color all balls on the left side of the $(m+1)$ th selected balls blue and on the right side of it red. Thus proving the equality. My question is, does this equality or identity have a name? For research purposes.","['soft-question', 'combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
3622939,Example of a Bounded Linear Operator with Unbounded Spectrum.,"I was looking at this superb question posted on MSE Spectrum can be an arbitrary subset. . And this question raised the following more elementary question Find an example of bounded linear operator $T: X\to X$ , defined in a normed vector space $X$ , such that, $\sigma(T)$ is unbounded ( $\sigma(T)$ is the spectrum of $T$ ). I have tried to construct an example but I  have failed miserably. Moreover, I searched online but I was not able to find anything related to the question. Does anyone know an example? Just a comment. By Banach Completion we can always consider $X$ as a dense subspace of the Banach space $\widetilde X$ . Using the following question extending a bounded linear operator there exists a unique bounded linear operator $S:\widetilde X\to \widetilde X$ such that $\|T\|=\|S\|$ and $\left.S\right|_{X}=T$ . It really seems like that $\sigma(T)\subset\sigma(S)$ . If $\lambda \in \rho(S)$ (resolvent of $S$ ), then $(\lambda I-S)^{-1}$ is bounded. Therefore $$(\lambda I-S)^{-1}(\lambda I-S)=\text{Id}_{\widetilde X}, $$ if we restric the above equation to the subspace $X$ we conclude that $$(\lambda I-S)^{-1}(\lambda I-T)=\text{Id}_{X}. $$ Since $(\lambda I-S)^{-1}$ is bounded then $(\lambda I-T)^{-1}$ is bounded as well, therefore $\rho(S)\subset \rho(T)$ , then $\sigma(T)\subset \sigma(S)$ . Since $S$ is a bounded linear operator in a Banach space $\sigma(S)$ is bounded $\Rightarrow$ $\sigma(T)$ is bounded. Is this correct? (Maybe $(\lambda I-S)$ surjective  will not imply $(\lambda I-T)$ surjective).","['operator-theory', 'normed-spaces', 'examples-counterexamples', 'functional-analysis', 'spectral-theory']"
3622940,When can we use limits to overcome $0$ probability events like $\{Y=y \}$?,"I've seen the way to ""prove""/""derive"" for a continuous random variable $Y$ the probability of event $A$ given the event $\{Y=y\}$ is the intuition that the event $\{Y=y\} = \lim_{\delta \to 0} \{y - \frac{\delta}{2} \le Y\le y + \frac{\delta}{2}\}$ . 
What I've seen then is: To find $P(A|Y=y)$ is to first evaluate $$P\left(A|Y\in\left(y-\frac{\delta}{2}, y+\frac{\delta}{2}\right)\right)$$ and then take the limit as $\delta \to 0$ . But doesn't this method assume that $$\lim_{\delta\to 0}P\left(A|Y\in\left(y-\frac{\delta}{2}, y+\frac{\delta}{2}\right)\right)\overset{?}{=}P\left(A|\lim_{\delta\to 0}\bigg\{Y\in\left(y-\frac{\delta}{2}, y+\frac{\delta}{2}\right)\bigg\}\right)=P(A|Y=y) \ ?$$ I might be skirting on the edges of measure theory (which I am not familiar with) but is there an intuitive way to make this claim seem intuitive/convincing? Or is there a result which shows why this is valid to do, if it is? Addition: Thanks to @Masoud - how would you know the limit always exists, if it does? Example: I'd like to add an example which lead me to this. Suppose $N_{t_1, t_2}$ represents the number of occurrences of a phenomenon in the time interval $(t_1, t_2]$ for $0<t_1<t_2$ . Suppose you are given the last occurrence was at time $s$ . Let $X$ be the time until the next occurrence of the phenomenon, starting at time $s$ . We need to find the distribution of $X$ . (For those of you familiar this is related to the well know poison process). Then, the probability $$\mathbb P (X>t|\text{Last occurrence was time }s)=\mathbb{P}(N_{s,s+t} = 0|\text{Last occurrence was time }s)$$ and this point, since there's no way to definition the conditional null set in terms of $N_{t_1, t_2}$ , I saw the source do $$\mathbb{P}(N_{s,s+t} = 0|\text{Last occurrence was time }s) \overset{?}{=} \lim_{h\to 0} \mathbb P(N_{s+h,s+t+h} = 0| N_{s+h, s}=1)$$ On what basis can we apply such a limit? I couldn't find a definition to state when you can do something like this. In this case, the limit is being applied in both the condition to help with the null set, and the set for which we're finding the probability too! Thanks in advance","['measure-theory', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
3622945,Discriminant of a function.,"Consider the function $f(x)=kx^{2}-2kx+2.$ Find the values of $k$ if $kx^{2}+2>2kx$ for any value of $x$ . I am unsure how to start off this question. I thought it involves the 
discriminant but couldn't figure out which condition to apply. Any help 
will be appreciated.","['algebra-precalculus', 'quadratics', 'discriminant']"
3622966,How do you find the two arcs to make a path between two points and two tangents?,"Say I have two points $A$ and $B$ in an $(x,y)$ plane and tangents of the points which are normalised vectors $\mathrm{dir} A$ $\mathrm{dir} B$ , how do you find a path made of two arcs that join them together? Here is a visual of what I mean. Since the images are not clear, the grey vectors from the two tangents are equal in magnitude but the magnitude is unknown . Note: I am looking for perfect arcs not cubic bezier curves.","['trigonometry', 'circles', 'vectors']"
3623017,Dynkin index of quotient map for a simple Lie group,"Let $G_1, G_2$ be simple compact Lie groups. Then $h_i := H_3(G_i, \mathbb Z)/ \mathrm{torsion} \cong \mathbb Z$ . If $\phi : G_1 \to G_2$ is a homomorphism, then the pushforward $\phi_* : h_1 \to h_2$ carries a generator of $h_1$ to a generator of $h_2$ multiplied by some natural number $\mathrm{ind}(\phi)$ , called the Dynkin index of $\phi$ . I would like to ask if the value of the Dynkin index is known in the case that $\phi : G_1 \to G_2$ is a universal cover of $G_2$ . It is easy to see that for $G_2 = \mathrm{SO}(3)$ we have $\mathrm{ind}(\phi)=2$ . I would like to know what happens for other groups.","['homology-cohomology', 'lie-groups', 'algebraic-topology', 'differential-geometry']"
3623080,Are there geodesic triangles on surfaces with non-constant curvature with angle sum 180?,"I've been reading about differential geometry and the Gauss-Bonnet theorem to write a paper for my geometry class and am interested specifically in geodesic triangles on surfaces. I was wondering if it is possible to create a geodesic triangle on a surface with non-constant curvature such that the interior angles sum to $π$ , $$\theta _{1} + \theta_{2}+\theta_{3}= \pi,$$ even though the Gaussian curvature $K \neq 0.$ By this I mean, is it possible to place part of the triangle on a positive curvature section of the surface and part of it on a negative curvature section of the surface so that one or two vertices are affected by the positive curvature and the other two or one vertices are affected by the negative curvature, causing the interior angle sum to still be $π$ ? For example, here's a surface with positive and negative curvature from Kristopher Tapp's Differential Geometry of Curves and Surfaces. Can the top triangle be moved downward so its bottom two angles, $\theta_{1}$ and $\theta_{2}$ , are on positive curvature and the top angle $\theta_{3}$ is on negative curvature so that $\theta_{1} + \theta_{2} + \theta_{3} = \pi$ and its sides are still geodesics? If so, does anyone have any resources they know of that I could read that specifically talk about this?","['geodesic', 'triangles', 'curvature', 'differential-geometry']"
3623137,Independence of functions of order statistics when the random variables are uniformly distributed,"Let $X_1$ , $X_2$ ,…, $X_n$ be $n$ i.i.d. random variables with $f(x)$ as the pdf and $F(x)$ as the cdf in interval $[0,1]$ . Let $F$ be uniformly distributed. Let $X_{i:n}$ be the $i^{th}$ order statistic such that $X_{1:n} \leq X_{2:n} \leq ... \leq X_{n:n}$ . I wish to compute the expected value $\mathbb{E} [\frac{X_{(k-1):n} X_{i:n}}{X_{k:n}} ]$ for any $ k < i \leq n$ . So the question is are $\frac{X_{(k-1):n}}{X_{k:n}}$ and $X_{i:n}$ independent? Because if they are not, then the problem is non-trivial. Due to a standard result in theory of order statistics, we already know that for any $i \leq n$ , $\frac{X_{(i-1):n}}{X_{i:n}}$ and $X_{i:n}$ are independent.","['statistics', 'uniform-distribution', 'independence', 'order-statistics', 'probability']"
3623192,"How to formally express a function which is ""dependent upon another function""","Lets say I have the expression $ a + b + c(d) + e$ where $a,b,d,e$ are variables and $c$ is a function. I now want to write the expression as a function $y(a,b,?,e) = a + b + c(d) + e$ What replaces the ? in the arguments? Is it c(d) or some combination of the two? Is it formally wrong to just say $y = a + b + c(d) + e$ i.e. without the function arguments? I've read before that $y(\ldots) \neq y$ , where the former is strictly a value and the latter is strictly a function. I'm asking these questions because I have an expression that is very long, but I'm not 100% sure how I should write it. It's for a publication and I don't know if my own lose interpretation is going to fly. I'd rather get it right from the start.","['functions', 'linear-algebra', 'real-analysis']"
3623204,H is an abelian proper subgroup of G with $H \cap gHg^{−1} = \{e\}$ for all $ g \notin H $ then $H$ is equal to centralizer of $H$ in $G$,"Let $G$ be a group with identity $e$ . Let H be an abelian non-trivial proper subgroup of G with
the property that $H \cap gHg^{−1} =  \{e\}$ for all $ g \notin H $ . If $K = \{ g \in G : gh = hg \, \, \,    \forall h \in H\}$ . I want to prove the following. $$ \\ (1) H = K \\  (2) \text {there exists no abelian subgroup $L$ $\subset$ G such that $K$ is a proper subgroup of $L$ } $$ . I tried the following . To prove part $(1)$ it is trivial to see that $ H \subset K $ as $H$ is an abelain group. Now we have to show that $ K \subset H $ for this let $ x \in K \implies xh = hx \, \,  \forall h \in H \, \, so \, \,  xHx^{-1} = H $ . I got stuck here how to proceed further. To prove part $(2)$ i tried way of contradiction suppose there exist an abelian subgroup $L$ of $G$ such that K is a proper subgroup of $L$ . Then there exist $x \in K \text {such that} x \notin L $ so $x$ commute with all elements of $ H$ . Now how to proceed further. Thank you for help.","['group-theory', 'normal-subgroups']"
3623261,Function for the orbit of the International Space Station,"For a maths task in school, I am investigating the orbit of the International Space Station around the Earth. I understand that when the 3D movement in space is represented on a 2D  surface, the relationship is not sinusoidal, however I have created the following (simple) model, which I am unsure is the most accurate. Below you can also find my calculated values with the formula (in red), compared to the actual values from an online API. Any help would be greatly appreciated! $y=51.64*\sin(x-304)$ (This only applies for one of the curves (the one pictured below), as the wave is translated 22.5 degrees to the right after each cycle.) My data can be found in the following google doc: https://docs.google.com/spreadsheets/d/1Ac8yQn8ybdtZWK8JyKAOIw46o3UJufAoidR5unjVeHs/edit?usp=sharing This is the graph Thank you in advance!","['classical-mechanics', 'trigonometry', 'functions']"
3623345,Products of matrices in either order have the same characteristic polynomial,"Let $A, B$ be square matrices over $\Bbb C$ . Prove that matrices $AB$ and $BA$ have the same characteristic polynomial. I know it's a famous problem and found various answers. However, I am at my first year of math degree and my knowledge is very limited. I have never seen matrix which the entires of the matrix is matrices themselves. We never spoke in the class about limits of matrices (those the sort of solutions I saw online). So, this question is kind of ""challenge"" for us to prove with our basic linear algebra knowledge. If any one knows a solution (complicated as it may be as long as it dosent require more than the basic knowledge) it would help a lot. thank you very much","['matrices', 'linear-algebra', 'characteristic-polynomial']"
3623429,Difference between Directional Derivative x Chain Rule for Scalar Fields,"could you guys help me out with an issue I am having. What's the difference between the ""Directional Derivative"" and ""Chain Rule for Scalar Fields""? In meaning and the formulae ? I don't know if I got it right but both of them have the same formula: $$ g'(\vec{r(t)}) = \nabla(g(t)) \cdot  \vec{r'(t)}    \ for \ the \ Chain \ Rule$$ And $$ Derivative = \nabla (g(t)) \cdot \vec{r(t)}  \ for \ the \ Directional\ Derivative$$ And to me they both seem to have the same meaning, since when we use the chain rule, we are using a vector whose direction is defined by the parameters.","['scalar-fields', 'multivariable-calculus', 'calculus', 'derivatives', 'chain-rule']"
3623433,f(x) increasing or decreasing,"Let $f(x) = x + 2x^2\sin(1/x)$ , with $x\ne0$ ,
  and $f(x)= 0$ when $x= 0$ . Detrmine if $f(x)$ is increasing or decreasing at $x= 0$ my attempt: using first principle it is easy to see that $f'(0)=1$ however if we find derivative of $f(x)$ as $f'(x) = 1+4x\sin(1/x) -2\cos(1/x)$ ,as $f'(1/(2k\pi)) = -1$ for integral $k$ hence there is no interval around $0$ where $f(x)$ is increasing. So it shouldn't make sense that $f(x)$ is increasing at $x=0$ . so how should we handle this? Any help is greatly appreciated.","['calculus', 'functions', 'derivatives']"
3623489,How do I determine $\lim_{x\to 1}(\sqrt[3]{x}-1)/(\sqrt{x}-1)$ without L'Hopital's Rule? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question I need to determine the following limit: $$\lim_{x\to 1}\frac{\sqrt[3]{x}-1}{\sqrt{x}-1}$$ We haven't learned L'Hopital's Rule in class so I can't use it and I have tried substitution, factoring, and multiplying by conjugate but nothing seems to work. Is there a way this problem could be solved without the L'Hopital's Rule?",['limits']
3623495,Can two solutions of a PDE with different initial conditions coincide after finite time?,"Consider an ODE $$
\dot x=f(x),\quad x(0)=x_0,
$$ where $f$ is a smooth function. It is well-known that if $y$ is another solution to this ODE with different initial condition $y_0\neq x_0$ , then the trajectories of $x(t)$ and $y(t)$ do not intersect - in particular, for any finite $t$ we have $x(t)\neq y(t)$ . I wonder if the same is true for PDEs. I'm interested in the following simple example: $$
\partial_t u=\partial_{xx} u+f(t,(u(t,x)),
$$ with periodic boundary conditions. Here $x$ lives on a circle $[0,1]$ , $t\ge0$ and $f$ is a nice function. Is it true that if $u,v$ are two solutions of this PDE with $u(0)\neq v(0)$ , then $u(t)\neq v(t)$ for any $t$ ? (equivalent statement: if $u(1)=v(1)$ is it true that $u(0)=v(0)$ ?) How can we prove this? UPD: the case $f\equiv0$ can be found inn Evans' book and it involves taking second derivative of the energy. Is it possible to extend this technique to the case $f\neq0$ ?","['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
3623516,SVD (or polar decomposition) of a matrix plus a constant,"Say I have a matrix $\hat M$ with a singular value decomposition (SVD) $\hat M=\hat U\hat D\hat V ^\dagger$ . Given this SVD, is there a simple way to get the SVD of $(\hat{M}-z\hat{1})$ , for $z\in\mathbb{C}$ some constant (here $\hat 1$ denotes the identity matrix)? Equivalently, if I have a polar decomposition $\hat{M}=\hat U\hat P$ , is there a simple way to get the polar decomposition of $(\hat{M}-z\hat{1})$ ? I'm actually only interested in getting the product $\hat{U}\hat{V}^\dagger$ in the case of the SVD, or the unitary part $\hat{U}$ in the polar decomposition, so if there's a method that doesn't get me the singular values $\hat{D}$ or positive-definite part $\hat P$ , that's fine too.","['matrices', 'linear-algebra', 'svd', 'matrix-decomposition']"
3623522,Composite function. Textbook answer incorrect?,"I am practicing composite functions and I can't get the same answer as the back of the textbook but I am confident in my calculations which leads me to believe the book is wrong. Question Let a be a positive number, $f:[2,\infty)\rightarrow \mathbb{R}, f(x)=a-x $ and let $g:(-\infty,1]\rightarrow\mathbb{R}, g(x)=x^2+a$ . Find all values of $a$ for which both $f\circ g$ and $g\circ f$ exist. My attempt I know for $g\circ f$ , $Ran$ $f$ must be a subset of $dom$ $g$ and for $f\circ g$ , $Ran$ $g$ must be a subset of $dom$ $f$ therefore: $dom$ $g= (-\infty,1]$ and $Ran$ $f=(-\infty, a-2]$ $dom$ $f= [2,\infty)$ and $Ran$ $g=[1+a,\infty)$ When I solve I am left with: $a-2 \leq 1 $ and; $1+a \geq 2 $ Therefore $a \in [1,3]$ The solution in the textbook shows the answer to be $a \in [2,3]$ Can someone please help me out. Am I correct or is the textbook correct? If I have made an error can you please help me solve this. Thank you","['calculus', 'functions', 'relations', 'function-and-relation-composition']"
3623529,The number of the positive integer solutions of $n = \prod_{i = 1}^{k}(x_{i} + 1) - \sum_{i = 1}^{k}x_{i} - 1$ given $n$.,"Given $n > 0$ , consider the equation $$ \prod_{i = 1}^{k}(x_{i} + 1) - \sum_{i = 1}^{k}x_{i} - 1 = (x_1 + 1)(x_2 + 1)\cdots (x_k + 1) - (x_1 + x_2 + \cdots + x_k) - 1 = n$$ where $0 < x_{1} \leq x_{2} \leq \cdots \leq x_{k}$ . Let $S(n)$ be the number of positive integer solutions of this equation. Here are some properties of $S(\cdot)$ $S(n) \geq 1$ , since $(1, n)$ ia always a solution when $k = 2$ . $S(n) \geq \lceil d(n)/2 \rceil$ , where $d(n)$ is the number of the factors of $n$ . It is because $n = x_{1}x_{2}$ when $k = 2$ . And it leads that $$\varlimsup_{n \to +\infty}S(n) = +\infty$$ S(n) = 1 if $n =1, 2, 3, 5, 23$ and $S(n) = 3$ if $n = 4$ . My questions are as follow. Q1: Determine the set $\{ n \mid S(n) = 1\}$ , is it finite? Q2: Compute $$\varliminf_{n \to +\infty}S(n) $$ Q3: If $$\lim_{n \to +\infty}S(n) = \varliminf_{n \to +\infty}S(n) = \varlimsup_{n \to +\infty}S(n) = +\infty$$ find $l_{m} = \max \{ n \mid S(n) < m\}$","['number-theory', 'combinations']"
3623534,"For any natural number $a$, determine the value of $\lim_{n\to\infty}\int_1^ex^a(\log x)^ndx.$","Question: For any natural number $a$ , determine the value of $$\lim_{n\to\infty}n\int_1^ex^a(\log x)^ndx.$$ My solution: Firstly let $$I_n=\int_1^ex^a(\log x)^ndx, \forall n\in\mathbb{N}.$$ Now substituting $\log x=t$ in $I_n$ , we have $$I_n=\int_0^1e^{t(a+1)}t^ndt$$ Now since $$0\le t\le 1\\\implies 0\le t(a+1)\le a+1\\\implies e^0\le e^{t(a+1)}\le e^{a+1}\\\implies 1\le e^{t(a+1)}\le e^{a+1}\\\implies t^n\le e^{t(a+1)}t^n\le e^{a+1}t^n, \forall 0\le t\le 1.$$ Thus we have $$e^{t(a+1)}t^n\le e^{a+1}t^n, \forall 0\le t\le 1\implies I_n\le\int_0^1e^{a+1}t^ndt=\frac{e^{a+1}}{n+1}.$$ Now let us evaluate $I_n$ by integration by parts. Let $u=e^{t(a+1)}\implies du=(a+1)e^{t(a+1)}dt$ and $dv=t^ndt\implies v=\frac{t^{n+1}}{n+1}.$ Thus $$I_n=\frac{e^{a+1}}{n+1}-\frac{a+1}{n+1}I_{n+1}.$$ Now we have $$e^{t{(a+1)}}t^{n+1}\le e^{a+1}t^{n+1}, \forall 0\le t\le 1\\\implies I_{n+1}\le \int_0^1e^{a+1}t^{n+1}dt=\frac{e^{a+1}}{n+2}\\\implies -\frac{a+1}{n+1}I_{n+1}\ge -\frac{(a+1)e^{a+1}}{(n+1)(n+2)}\\\implies I_n=\frac{e^{a+1}}{n+1}-\frac{a+1}{n+1}I_{n+1}\ge \frac{e^{a+1}}{n+1}-\frac{(a+1)e^{a+1}}{(n+1)(n+2)}.$$ Thus we have $$\frac{e^{a+1}}{n+1}-\frac{(a+1)e^{a+1}}{(n+1)(n+2)}\le I_n\le \frac{e^{a+1}}{n+1}\\\implies \frac{n}{n+1}e^{a+1}-\frac{n(a+1)e^{a+1}}{(n+1)(n+2)}\le nI_n\le \frac{n}{n+1}e^{a+1}, \forall n\in\mathbb{N}.$$ Now $$\lim_{n\to\infty} \left(\frac{n}{n+1}e^{a+1}-\frac{n(a+1)e^{a+1}}{(n+1)(n+2)}\right)=e^{a+1}\text{ and }\lim_{n\to\infty}\frac{n}{n+1}e^{a+1}=e^{a+1}.$$ Thus by Sandwich Theorem, we have $$\lim_{n\to\infty}nI_n=e^{a+1}.$$ Is there any other method to solve this problem?","['integration', 'definite-integrals', 'real-analysis']"
3623576,Prove $\prod_{n \in \Bbb{N} } \left(\frac{1-q^n}{1+q^n}\right)^{(-1)^n} = \sum_{n \in \Bbb{Z}} q^{n^2}$,"Let $q$ be a complex number with $|q| < 1$ , prove that $$
\prod_{n \in \Bbb{N} } \left(\dfrac{1-q^n}{1+q^n}\right)^{(-1)^n} = \sum_{n \in \Bbb{Z}} q^{n^2}
$$ Not sure if this helps but the LHS can also be written as $$
\prod_{n\in \Bbb{N}\\\text{n odd}}(1+q^n)(1+q^n+q^{2n}+\cdots)\prod_{n \in \Bbb{N} \\\text{n even}} (1-q^n)(1-q^n+q^{2n}-\cdots)
$$ I've tried the method here and the method here , but they both give me nightmares. Is there any good idea?","['infinite-product', 'combinatorics', 'sequences-and-series']"
3623614,Maximal extension of domain of $\Psi(x)=\sum_{n=1}^\infty e^{\frac{\log(n)}{\log(x)}}$ using analytic continuation,Given $$ \Psi(x)=\sum_{n=1}^\infty e^{\frac{\ln(n)}{\ln(x)}}= \prod_{p~ \mathrm{prime}}\frac{1}{1-e^{\frac{\ln(p)}{\ln(x)}}}$$ What is the maximal extension of $\Psi$ ? I think that there is a barrier at $\Re(x)=0$ but don't know how to show that.,"['euler-product', 'logarithms', 'analytic-continuation', 'sequences-and-series']"
3623721,Difference over interval for periodic-like function,"Let $F:\mathbb{R}\to\mathbb{R}$ be a continuous and nondecreasing function such that $F(0)=0$ and $F(x+1) = F(x)+1$ for all $x\in\mathbb{R}$ . Moreover, $F(0.4)=F(0.5)=0.5$ and $F(0.9)=1$ . Let $A(F,a)$ be the set containing all $x\in[0,1)$ s. t. $F(x+0.4)-F(x)\geq a$ . What is the largest $a$ such that the Lebesgue measure of $A(F,a)$ is always at least $0.5$ ? Note that $A(F,0) = [0,1)$ , so the largest $a$ is nonnegative. Without the extra condition on the values of $F$ , it could be that $F$ shoots up from $0$ to $1$ around some point in $[0,1)$ , which will imply that the largest $a$ is $0$ . But this condition means that the increase in the value of $F$ needs to be somewhat spread out over the interval, which disallows such functions. A related question , where we want to find the smallest size of $A(F,0.4)$ (without the new condition).","['optimization', 'inequality', 'real-analysis']"
3623784,Spectrum of a matrix that has only one $1$ in each row,"As in the title, I'm searching for the spectrum of a matrix which has only one $1$ in each row (and zeros as other entries) and also that is not necessary a permutation matrix. For example, $$A =\begin{bmatrix}
1 & 0 &0 \\
1 &0 &0\\
0 & 1 &0
\end{bmatrix}$$ Is it true that the specrum of such a thing only contains eigenvalues $\lambda$ of $|\lambda|=1$ or $\lambda = 0$ ?","['matrices', 'stochastic-matrices', 'eigenvalues-eigenvectors']"
3623815,Generalizations of Sard-Smale Theorem,"Sard-Smale theorem holds for Fredholm maps $f:M\rightarrow B$ between separable Banach manifolds $M,N$ . There are some constrains relating the Fredholm index $\operatorname{ind}(f)$ of $f$ to its differentiablity class. More precisely, we need to require $f\in C^{r}$ , where $r>\max{(\operatorname{ind}(f),0)}$ . I would like to know what are the generalizations of this theorem into two directions: assuming weaker regularity on $f$ ; assuming weaker structure on the spaces $M,N$ . For instance, I know that the result is valid for bounded Fréchet manifolds in the case where $f$ is Lipchitz (see here ). The result is also valid for some domains with empty interior (see here ). I would also like to know if the above (or others) generalizations are maximal, in the sense that there are counterexamples in the case of weaker requirements. Any help is welcome. Edit: I posted the question on MathOverflow .","['differential-topology', 'global-analysis', 'functional-analysis', 'smooth-manifolds']"
3623833,Even Odd Functions,"Let $e(x)$ be an even function and let $o(x)$ be an odd function, such that $e(x) + x^2 = o(x)$ for all $x.$ Let $f(x) = e(x) + o(x).$ Find $f(2).$ We could write $f(x)=e(x)+e(x)+x^2.$ We want to find $f(2),$ so could we write it as $f(2)=e(2)+e(2)+2^2.$ (I'm not sure if we can.) How would I finish the problem?","['even-and-odd-functions', 'functions']"
3623860,A simpler non-calculator proof for $17^{69}<10^{85}$,"I have proved that $17^{69}<10^{85}$ by using the following inequalities: $x<\exp\left(\dfrac{2(x-1)}{x+1}\right)$ for all $x\in \left]-1,1\right[$ and $x<{\mathrm e}^{x-1}$ for all $x\in \left] 1,+\infty \right[$ , but I am looking for a simpler non-calculator proof. My proof is the following: \begin{align*}\frac{17^{69}}{10^{85}}&=\left(\frac{17^3}{2^3\cdot 5^4}\right)^{23}\cdot\left(\frac{5^3}{2^7}\right)^2\cdot\frac{5}{4}<\left(\frac{17^3}{2^3\cdot 5^4}\right)^{23}\cdot\frac{5}{4}=\left(\frac{4913}{5000}\right)^{23}\cdot \frac{5}{4}\\&<\left(\exp\left(\frac{2\left(\frac{4913}{5000}-1\right)}{\frac{4913}{5000}+1}\right)\right)^{23}\cdot\exp\left(\frac{5}{4}-1\right)\\&=\exp\left(-\frac{174}{431}\right)\cdot\exp\left(\frac{1}{4}\right)=\exp\left(-\frac{265}{1724}\right)<1.\end{align*} Could anyone find a simpler non-calculator proof without using big numbers?","['exponentiation', 'algebra-precalculus']"
3623881,Derivative Method of Proving of $\sqrt{x}>\ln x$,"I have a question about a proof I read, linked here , a proof that $\sqrt{x}>\ln x$ . The second answerer considers the derivative of the function, $f\left(x\right)=\sqrt{x}-\ln x$ .
Now the derivative is negative for $0\leq x<4$ . Does this not mean, that $\sqrt{x}<\ln x$ on that interval? I.e. The derivative being negative suggests that the function is decreasing? Why is the global minimum analysed? I must be missing something important!","['calculus', 'derivatives']"
3623947,Proof of existence of Levi-Civita connection via the Koszul Formula,"I'm currently self-studying the basic concepts of Riemannian Manifolds, and I'm stuck on the proof of the existence of the Levi-Civita connection that is presented in the book ""Semi-Riemannian Geometry"" by O'Neill, which is a coordinate-free approach. Suppose $(M,g)$ is a Riemannian manifold, and let $\nabla:\mathfrak{X}(M)\times\mathfrak{X}(M)\to\mathfrak{X}(M)$ be a connection. So far, I've been able to prove that, if $\nabla$ verifies the conditions of the Levi-Civita connection, then it must satisfy the Koszul Formula: $
\langle\nabla_{X}Y,Z\rangle=\dfrac{1}{2}(X\langle Y, Z\rangle+Y\langle Z, X\rangle-Z\langle X, Y\rangle
-\langle Y,[X, Z]\rangle-\langle Z,[Y, X]\rangle+\langle X,[Z, Y]\rangle).
$ So now, I'm trying to prove existence by using the formula: more precisely, with this result that I proved previously: If $\omega:\mathfrak{X}(M)\to \mathcal{F}(M)$ is a differential $1$ -form, then there exists a unique vector field $V$ such that for any other vector field $X$ , we have $\omega(X)=\langle V,X\rangle$ . My reasoning is the following: fix two vector fields $X,Y$ , and let $\omega_{X,Y}:\mathfrak{X}(M)\to \mathcal{F}(M)$ be the map $\omega_{X,Y}(Z)=\dfrac{1}{2}(X\langle Y, Z\rangle+Y\langle Z, X\rangle-Z\langle X, Y\rangle
-\langle Y,[X, Z]\rangle-\langle Z,[Y, X]\rangle+\langle X,[Z, Y]\rangle).$ If I were able to prove that $\omega_{X,Y}$ is $\mathcal{F}(M)$ -linear (which, knowing the Theorem to be true, it must be), then by using the preceding result I can define a unique vector field $\nabla_{X}Y$ such that $\omega_{X,Y}(Z)=\langle \nabla_{X}Y,Z \rangle$ . Thus, I would have a well defined map $\nabla$ satisfying the Koszul Formula, and it would be (after some property checking) the Levi-Civita connection. The problem is that, while it's easy to see that $\omega_{X,Y}(Z_{1}+Z_{2})=\omega_{X,Y}(Z_{1})+\omega_{X,Y}(Z_{2})$ , I haven't been able to prove that $\omega_{X,Y}(fZ)=f\omega_{X,Y}(Z)$ . In reality, what I've gotten is that $\omega_{X,Y}(fZ)=f\omega_{X,Y}(Z)-\dfrac{1}{2}\langle (Xf)Y+(Yf)X,Z \rangle$ , but I'm not sure that the second summand is $0$ . Is my attempt correct so far, or am I missing something? Thank you in advance!","['connections', 'riemannian-geometry', 'differential-geometry']"
3623953,Why study algebraic varieties over an algebraic closure of the finite field?,"Let $\mathbb{F}_q$ be a finite field and $I = \langle f_1,\ldots,f_r \rangle \subseteq \mathbb{F}_q[x_1,\ldots,x_n]$ an ideal. 
Let me write $V(I)$ for the set $\{x \in \mathbb{F}_q^n : f(x) = 0 \text{ for all } f \in I\}$ .
I am interested in computing the number of solutions to the system of equations $f_1=\cdots=f_r=0$ , which is given by $|V(I)|$ . Put $I_q = \langle x_1^q-x_1, \ldots, x_n^q-x_n\rangle$ . Since $|V(I)| < \infty$ , I know that $|V(I)| = \dim_{\mathbb{F}_q} \mathbb{F}_q[x_1,\ldots,x_n]/(I + I_q)$ . In many of the texts that I have found online, the object $V(I)$ is actually defined over an algebraic closure of $\mathbb{F}_q$ , even if the ultimate goal is to study the finite number of solutions in $\mathbb{F}_q^n$ . I have noticed that something like the Krull dimension of $V(I)$ , which is zero in the setting above, can all of a sudden be non-zero over the algebraic closure and that the dimension of $V(I)$ over an algebraic closure equals its dimension as a vector space if $f_1,\ldots, f_r$ are linear forms, which does not seem to be the case in the setting above. My question is: why move to an infinite field in order to study something that is finite? Is this simply because the theory is ""nicer"" in this case?","['finite-fields', 'algebraic-geometry']"
3623966,Framing problem : Show that $\frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1$,"Let $x,y,z\gt 0$ reals and let $x^3+y^3+z^3=4xyz$ Show that : $$\frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1$$ Can I use Cauchy inequality to solve it ?","['multivariable-calculus', 'algebra-precalculus', 'cauchy-schwarz-inequality']"
