question_id,title,body,tags
3353715,Does this system of infinite equations has an (almost) unique solution?,"Let $a_1,\dots ,a_n \in \Bbb C$ , consider the following system of equations $$\begin{cases} x_1+ \cdots+ x_n=a_1 \\ {x_1}^2+\cdots+{x_n}^2=a_2 \\ \qquad \qquad \vdots \\ {x_1}^n+\cdots+{x_n}^n=a_n \end{cases}$$ Its ""easy"" to prove this system has a unique solution up to permutations. The reason is due to Newton identities that allow us to create an equivalent system $$\begin{cases} e_1(x_1,\dots,x_n)=b_1 \\ e_2(x_1,\dots,x_n)=b_2 \\ \qquad \qquad \vdots \\ e_n(x_1,\dots,x_n)=b_n \end{cases}$$ Where $e_1, \dots ,e_n$ are the elementary symmetric polynomials and $b_1, \dots ,b_n \in \Bbb C$ are numbers which can be calculated in terms of $a_1, \dots ,a_n$ . If we consider the polynomial $$P(X)=X^n-b_1 \cdot X^{n-1}+b_2 \cdot X^{n-2} + \cdots+(-1)^n \cdot b_0$$ Then, due to Vieta's Formulas, the solution to our system are precisely the 
roots of $P$ (counted with multiplicity) which are unique up to permutations. My question is the following, let $(a_n)_{n\in \Bbb N}$ be complex numbers and consider the following system of infinite equations in $l^1(\Bbb C)$ $$\begin{cases} \sum_{n \in \Bbb N} x_n = a_1
 \\ \sum_{n \in \Bbb N} {x_n}^2 = a_2 \\ \qquad \quad \vdots \\ \sum_{n \in \Bbb N} {x_n}^k = a_k \\ \qquad \quad \vdots \end{cases}$$ Are there any necessary/sufficienct conditions over $(a_n)_{n\in \Bbb N}$ for a solution to exist? If $(x_n)_{n\in \Bbb N}$ is a solution to our system and $\sigma : \Bbb N \rightarrow \Bbb N$ is a bijection then $(x_{\sigma (n)})_{n\in \Bbb N}$ is a solution to our system so any permutation of a solution is again a solution. Also, if we take a solution $(x_n)_{n\in \Bbb N}$ and we ""add"" zeros to our sequence then we get another solution, for example, the sequence $(y_n)_{n\in \Bbb N}$ defined as $$y_{2n}=0 \qquad ; \qquad y_{2n-1}=x_n \qquad \forall n \in \Bbb N$$ Is another solution to our system of equations. My second question would be the following. If our system of infinite equations has two solutions, $(x_n)_{n\in \Bbb N}$ and $(y_n)_{n\in \Bbb N}$ , is it true that I can get $(y_n)_{n\in \Bbb N}$ by taking $(x_n)_{n\in \Bbb N}$ and adding zeros and making permutations? Inspired by the finite case, I did the following approach. Its easy to prove (again, using Newton Identities) that if $(x_n)_{n\in \Bbb N} \in l^1(\Bbb C)$ then the following limit exists for every $k \in \Bbb N_0$ $$e_k(x):=\lim_{n \to \infty} e_k(x_1,\dots ,x_n) < \infty$$ And it can be calculated in terms of $\sum_{n \in \Bbb N} x_n , \dots , \sum_{n \in \Bbb N} {x_n}^k$ so we get the following (equivalent) system of equations $$\begin{cases} e_1(x) = b_1
 \\ e_2(x) = b_2 \\ \qquad \vdots \\ e_k(x) = b_k \\ \qquad \vdots \end{cases}$$ Where $(b_n)_{n\in \Bbb N}$ are complex numbers which can be calculated in terms of $(a_n)_{n\in \Bbb N}$ . To reproduce the following step in the finite case, we will make some modifications in the approach. By Vieta's formulas, it's easy to check the following polynomial equality $$\prod_{k=1}^n (1-x_i \cdot X) = \sum_{k=0}^n (-1)^k \cdot e_k(x_1,\dots, x_n) \cdot X^k$$ With $x_1,\dots ,x_n \in \Bbb C$ . Note that the roots of this polynomial are ${x_i}^{-1}$ for every $x_i \not = 0$ . I would like to say (and have no idea how to prove) that if $(x_n)_{n \in \Bbb N} \in l^1(\Bbb C)$ then $$\prod_{k=1}^\infty (1-x_i \cdot z) = \sum_{k=0}^\infty (-1)^k \cdot e_k(x) \cdot z^k \qquad \forall \; z \in \Bbb C$$ If this is true, going back to our infinite system of equations, we could consider the series $$f(z)=1+ \sum_{k=1}^\infty (-1)^k \cdot b_k \cdot z^k$$ Wich is a function we can ""calculate"" since we know $(b_k)_{k \in \Bbb N}$ . We should ask some conditions over $(b_k)_{k \in \Bbb N}$ for this to be well defined over an open set around zero (or all over $\Bbb C$ ). Let $(r_n)_{n \in \Bbb N}$ be the roots of $f$ counted with multiplicity (Does this have a meaning???), let $x_n = {r_n}^{-1} \quad \forall n \in \Bbb N$ (Note $r_n \not = 0 \quad \forall n \in \Bbb N$ because $f(0)=1$ ), if $f$ has finite roots, we fill the sequence with zeros. I affirm that $(x_n)_{n \in \Bbb N}$ is a solution to our system. There are a LOT of gaps in my reasoning and Im not sure how to continue. My Complex analysis knowledge is really weak.","['systems-of-equations', 'complex-analysis', 'symmetric-polynomials', 'polynomials', 'sequences-and-series']"
3353767,Student claims $\lim_{x \to 0^+} \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} = \lim_{h \to 0} \lim_{x \to 0^+} \frac{f(x+h)-f(x)}{h}$. Is it justified?,"I am a tutor of an introductory calculus course in our college of engineering. The students are first year engineering undergraduates. We had a quiz where the following question was asked : Let $f : [0,1] \to \Bbb R$ be continuous on $[0,1]$ and differentiable on $(0,1)$ . If $\lim_{x \to 0^+} f'(x)=L$ for some real number $L$ , then show that $f'(0)$ exists and $f'(0)=L$ The students had been taught Lagrange's mean value theorem prior to that. So it was expected that they will use it. However, some students have written the following answer: $L=\lim_{x \to 0^+} f'(x)= \lim_{x \to 0^+}  \lim_{h \to 0} \frac {f(x+h)-f(x)}{h} = \lim_{h \to 0} \lim_{x \to 0^+} \frac {f(x+h)-f(x)}{h} = \lim_{h \to 0} \frac {f(h)-f(0)}{h}.$ This implies that $\lim_{h \to 0} \frac {f(h)-f(0)}{h}$ exists hence $f'(0)$ exists and equals $L$ . In this, they have not provided any justification for exchanging the limits. I am not able to find any justification for that step nor I am able to counter it. Can someone throw some light on this? Thanks in advance.","['proof-verification', 'real-analysis', 'calculus', 'limits', 'derivatives']"
3353803,Intersection between a curve of degree six and a conic,"I've seen this exercise in the book ""Lectures on curves, surfaces and projective varieties"" by Beltrametti and others: Show that three polynomials $\phi_2(x_1,x_2,x_3), \phi_3(x_1,x_2,x_3), \phi_4(x_1,x_2,x_3)$ homogeneous of degree $deg(\phi_i)=i$ can be chosen in a way such that $\phi_2 \phi_4 - \phi_3^2$ is the product of 6 linear forms. Under the statement the outhors put also their solution, but there is a passage that I can't really get. 
They say: in the projective complex plane, take an irreducible conic of equation $\phi_2=0$ and a cubic of equation $\phi_3=0$ that instersects the conic in exactly six distinct points $P_1,..., P_6$ . Consider the lines $l_i$ tangent to the conic at $P_i$ . Then, from the linear system of curves of equation $\phi_3^2- \lambda l_1...l_6$ take the curve $C$ that intersect the conic in an other point $Q$ distinct from all the $P_i$ . Then, $C$ has the conic as irreducible component, so the thesis follows. My question is: why does $C$ split into the conic and an other curve of degree four? I thought I could use BÃ©zout theorem, but it seems to me that the conic and $C$ have only 7 points in common, and I don't really see how one can compute the intersection multiplicity of these two curves in the points in which tey intersect. Any help would be appreciated.",['algebraic-geometry']
3353810,"Regular surface at critical value; procedure for ""find real numbers $c$ satisfying $F(x,y,z) = c$ be a regular surface. ""","First Let me state what I know, If $r$ is a regular value of $F$ and $F^{-1}(r)$ is non-empty then $F^{-1}(r)$ is a regular surface Of course I know, its converse is not true. Typical example can be found in Do Carmo as $F(x,y,z) = z^2$ . The critical points of this surface is $(x,y,0)$ so it has a critical value $F(x,y,0) =0$ . But $F^{-1}(0) = \{(x,y,0)\}$ is a plane so it is a regular surface. Now I am trying to solve following problem For which real numbers $c$ is the set of $(x,y,z)$ satisfying $x^2+y^2 + \sin(z) =c$ a regular surface? Applying the above procedure. Let $F(x,y,z) = x^2 + y^2 + \sin(z)$ , then $\nabla F = (2x,2y, \cos(z))$ so $(0,0, \frac{\pi}{2}+n\pi)$ is a critical point and thus critical value is $\pm 1$ . So $c\neq \pm 1$ . $F^{-1}(c)$ is a regular surface. Now I want to know what happen to $c=\pm 1$ . For this case $F^{-1}(\pm 1) =\{ (x,y,z) | x^2 + y^2 + \sin(z) =\pm 1 \}$ .  At this moment I am stuck with this stage.... For this do i have to compute $X(u,v) = ( \pm \sqrt{-u^2 - \sin(v)}, u, v)$ and compute $X_u \times X_v$ ? Is there any systematic way to find regularity of surface for critical value?  i.e., I want to find some general strategy to solve Find real numbers $c$ satisfying $F(x,y,z) = c$ be a regular surface.",['differential-geometry']
3353844,"Stability of non-linear, non-autonomous ODE","Consider the ODE $y'(t)=S(K(t)y(t)+b(t))$ where $K(t)$ is a matrix, $y(t), b(t)$ are vectors and $S$ applies a function $\mathbb{R}\to\mathbb{R}$ componentwise. Can the stability of this ODE be assured by chosing $K$ such that the Jacobian of the right-hand side  only has eigenvalues with negative real part at each instant ? I read a paper where this is claimed, but I cannot find a version of this criterion for non-autonomous, non-linear systems. Also why can we talk about the whole ODE being stable if generally speaking for non-linear ODEs we can only speak about stability for specific solutions ?","['stability-in-odes', 'stability-theory', 'ordinary-differential-equations', 'dynamical-systems']"
3353853,Finding limit of $a_n = \frac{1}{n|\sin n|}$,"Let $a_n = \frac{1}{n|\sin n|}$ . Find $\lim_{n\to \infty} a_n$ if it exists. It's known that $b_n = \frac{1}{n\sin n}$ is divergent but what about $a_n = \frac{1}{n|\sin n|}$ ? I tried to show that $a_n$ converges to $0$ using definition but I am stuck on $\frac{1}{|\sin n|} \lt \epsilon$ . Also attempting to show $\sum_{n=1}^\infty a_n$ converges with different tests, like comparison test , was unsuccessful.","['limits', 'sequences-and-series']"
3353860,Geometric interpretation of torsion for a non-linear (Ehresmann) connection,"In [1, eq. $7.8.10$ ] the author gives the coordinate expression for the torsion of a non-linear connection. Here a non-linear connection, in my understanding, is essentially a splitting of the double tangent bundle $TTM$ in a horizontal and vertical part (although his precise definition [1, def. $7.2.1$ ] is quite a bit more involved, but I can imagine that in the end it is equivalent). The formula for torsion in natural coordinates $(x,y)$ in the tangent bundle reads $$ T^k_{ij}(x,y) = \bar\partial_j N^k_i(x,y) - \bar\partial_i N^k_j(x,y),\qquad \bar\partial_i \equiv \frac{\partial}{\partial y^i}, $$ where $N^i_j$ are the connection coefficients. This indeed reduces to the usual expression for the torsion tensor $T^k_{ij} = \Gamma^k_{ij} - \Gamma^k_{ji}$ when the connection is linear. For linear connections my intuitive understanding of torsion has always been that it measures the degree to which an infinitesimal parallelogram does not close on itself. More precisely, given two vectors $v,w\in T_pM$ , if we infinitesimally parallel transport $u$ along $v$ and $v$ along $u$ then the difference $\delta$ between the two outcomes is given precisely by the torsion tensor $$ \delta^k = T^k_{ij}v^i w^j.$$ However, I noticed that this is not true anymore when the connection is non-linear. Indeed, for homogeneous non-linear connections, i.e. $ N(x,\lambda y) = \lambda N(x,y)$ , I find that $$\delta^k = (\bar\partial_j N^k_i(x,v) - \bar\partial_i N^k_j(x,u))u^iv^j.$$ Although the expression in between brackets looks a lot like the torsion of the non-linear connection, it is not the same, because of the additional dependence on $u$ and $v$ in $N^i_j$ . My standard geometrical interpretation of torsion is apparently not valid anymore in the non-linear realm, so my question comes down to the following: Is there an alternative geometric interpretation of the torsion of a non-linear connection? [1] Szilasi - Connections Sprays and Finsler Structures","['riemannian-geometry', 'connections', 'geometry', 'intuition', 'differential-geometry']"
3353862,Differentiation of one function with respect to another in multivariable calculus?,I want to differentiate the function say $g(x)=x^2+y^2$ with respect to the function $f(x)=xy$ .  But I haven't been able to figure it out. Though I have derived the formula $$\frac{dg}{df}=\frac{\left(\frac{\partial g}{\partial x}\right)\left(\frac{\partial g}{\partial y}\right)}{x\left(\frac{\partial g}{\partial x}\right)+y\left(\frac{\partial g}{\partial y}\right)}$$ But I don't even know whether it is correct or NOT(highlighted because that's most probable outcome),['multivariable-calculus']
3353877,Expression of $x^n+\frac1{x^n}$ by $x+\frac1{x}$ where $n$ is a positive odd number.,"There was a problem in a book: Denote that $y=x+\dfrac{1}{x}$ , express $x^7+\dfrac{1}{x^7}$ using $y$ . It's not a hard question, but I find a special sequence: $x+\dfrac{1}{x}=y\\x^3+\dfrac{1}{x^3}=\left(x+\dfrac{1}{x}\right)^3-3\left(x+\dfrac{1}{x}\right)=y^3-3y \\ x^5+\dfrac{1}{x^5}=\left(x+\dfrac{1}{x}\right)^5-5\left(x^3+\dfrac{1}{x^3}\right)-10\left(x+\dfrac{1}{x}\right)=y^5-5\left(y^3-3y\right)-10y=y^5-5y^3+5y\\x^7+\dfrac{1}{x^7}=\left(x+\dfrac{1}{x}\right)^7-7\left(x^5+\dfrac{1}{x^5}\right)-21\left(x^3+\dfrac{1}{x^3}\right)-35\left(x+\dfrac{1}{x}\right)=y^7-7\left(y^5-5y^3+5y\right)-21\left(y^3-3y\right)-35y=y^7-7y^5+14y^3-7y$ I find that the coefficient has some relationship between the Pascal Triangle, such as $y^7-7y^5+14y-7y=y^7-7 \binom{2}{0}y^5+7\binom{2}{1}y^3-7\binom{2}{2}y$ . That's strange but funny! However, I can't really prove this, or show that it is false. Hope there is someone who can answer me. Thank you!","['algebra-precalculus', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
3353912,Weak convergence + pointwise convergence on dense subspace of $L^2$,"Let $H$ be a dense subspace of $L^2(X,\mu)$ ( $X$ being a separable and complete metric space with finite measure $\mu$ ) and $(f_n)_{n\in\mathbb N}$ be a sequence in $H$ that converges to $f\in L^2(X,\mu)$ in the following sense: $$
\langle f_n - f, h \rangle_{L^2} \xrightarrow{n\to\infty} 0
\quad
\text{for all } h\in H.
$$ In addition, let $f_n$ converge pointwise $\mu$ -almost everywhere to some function $g$ , i.e. $f_n(x)\xrightarrow{n\to\infty} g(x)$ for $\mu$ -almost every $x\in X$ . I need to show that $f$ and $g$ agree $\mu$ -almost everywhere, in other words $f_n(x)\xrightarrow{n\to\infty} f(x)$ pointwise for $\mu$ -almost every $x\in X$ . I have been struggling to prove this (and also to find counterexamples), but so far I cannot find the right analytical tools. Any help will be very appreciated! Remark: I know very little about the $f_n$ , there may be no upper bound on their norm etc. Remark: The background is that I am trying to prove a convergence result for conditional mean embeddings in reproducing kernel Hilbert spaces (this is not an homework exercise).","['hilbert-spaces', 'pointwise-convergence', 'functional-analysis', 'weak-convergence']"
3353933,( Homework Help ) on Differential Equation $y^4dx+2xy^3dy=\frac{ydx-xdy}{x^3y^3}$,I am stuck with a question of differential equation. $$~y^4dx+2xy^3dy=\dfrac{ydx-xdy}{x^3y^3}~$$ My book solves it in a peculiar way by multiplying it by $~\dfrac{y}{x}~$ and forming perfect derivatives of $~x^3y^6~$ and $~\ln\left(\dfrac{y}{x}\right)~$ . I could not understand the intuition behind this rearrangement of the terms. I tried other methods like forming homogeneous equation by substitution or trigonometric substitutions with no success. So what is the exact logic behind forming these perfect differentials in the question? Also is there any other method to solve the question?,"['calculus', 'ordinary-differential-equations']"
3353980,"Dehn twist in amalgamated product, preserving a subgroup","I'm in an uncertain position where I have the following: $G=A \underset{C}{\ast} B$ and $H\leq G $ . Let $h\in H \cap C$ where $C$ is abelian. Define, $$\tau\left(g\right)=\begin{cases}
g & g\in A\\
h^{-1}gh & g\in B
\end{cases}$$ This defines an automorphism of the amalgamated product $G=A \underset{C}{\ast} B$ by the universal property. Both conditions coincide if $g\in C$ as $C$ is commutative. I hope it is the case that $\tau (H)\subseteq H$ . Obviously this follows if $H$ is contained in either factor, but I am not sure how to handle a general subgroup. But I am not sure if this is true, could anyone give leads on how to either prove this or how to come up with a nice and informative counterexample? Note that $H$ is not normal for me. If $H$ was normal in $G$ , the claim would follow easily algebraically.","['geometric-group-theory', 'group-theory', 'free-product']"
3353993,"Normalization of $\mathbb{C}[x,y]/(y^2-x^5)$","I wish to calculate the normalization of $R=\mathbb{C}[x,y]/(y^2-x^5)$ . My first two steps were to define $s=y/x, t=y/x^2$ both of which are integral over $R$ , as you can see from dividing the equation by $x^2$ , $x^4$ respectively. Now, I guess that these are all that I need, that is, that $R[t,s]$ is already integrally closed. However when calculating we get: $$R[t,s]\simeq\mathbb{C}[x,y,t,s]/(s^2-x^3,t^2-x,xs-y,x^2t-y)\simeq\mathbb{C}[t,s]/(s^2-t^6,t^2s-t^5).$$ But the latter is super reducible and thus for example not smooth and hence not normal (dimension 1). My question is: 1) Am I missing another generator for the integral closure? 2) Is my calculation of $R[t,s]$ correct? 3) How to calculate the integral closure in this case?","['algebraic-geometry', 'commutative-algebra']"
3354019,Prove that the loxodrome crosses all meridians at a constant angle,"How to prove that the loxodrome (the rhumb line) crosses all meridians at a constant angle? $$\tan\left(\frac{\pi}{4} + \frac{\psi}{2}\right) = e^{k\phi}, \quad k = \text{constant}$$ where $\psi$ is the latitude of a point on a sphere, and $\phi$ is the longitude.","['geodesy', 'curves', 'spherical-geometry', 'multivariable-calculus', 'differential-geometry']"
3354050,"If $0<\theta <180$, then $\sqrt{2+\sqrt{2+\sqrt{2+...+\sqrt{2(1+cos\theta)}}}}$=? For n number of 2s","OPTIONS A) $2\cos(\frac{\theta}{2^n})$ B) $2\cos(\frac{\theta}{2^{n-1}})$ C) $2\cos\frac{\theta}{2^{n+1}}$ D) none of these In case of $2(1+cos\theta)$ It can be written $4cos^2\frac{\theta}{2}$ . Itâs square will be $2cos\frac{\theta}{2}$ So as we go on multiply 2 and taking its square root, I donât see why there should be an ânâ inside the cos function. Yet there is, and I am wrong, so how should I solve it?",['trigonometry']
3354081,Supremum and Infimum of measures of collection of sets,"edit: I am using |*| as notation for measure, and all sets are assumed to be measurable. Define $\liminf_{k \rightarrow \infty}E_k= \cup_{k=1} \cap_{j=k} E_j$ and $\limsup_{k \rightarrow \infty}E_k= \cap_{k=1} \cup_{j=k} E_j$ . I'm wondering if there is an example where $\liminf_{k \rightarrow \infty}E_j$ is a proper subset of $\limsup_{k \rightarrow \infty}E_j$ . Also, I'm trying to prove the inequality $|\liminf_{k \rightarrow \infty}E_k| \leq \liminf_{k \rightarrow \infty}|E_k|$ I think this is an interesting inequality, because the right hand is asking for the infimum of a sequence of positive numbers, where the left is the measure of a definition of infimum defined as a set operation. Any advice is greatly appreciated! Thanks everyone!!","['elementary-set-theory', 'measure-theory', 'examples-counterexamples', 'real-analysis']"
3354203,"If the gambler's fallacy is false, how do notions of ""expected number"" of events work?","Imagine there is a fair spinner that could land on any number $1$ through $100$ . I understand that the chance of any number appearing on the next spin is $\frac{1}{100}$ , and if you spin the spinner $100$ times then the expected number of $5$ 's, for example, is $1$ . However, I have the following questions: If the spinner does not land on $5$ in the first spin then this does not make the chance of getting a $5$ on the second spin any more or less likely (to assume otherwise would imply the gambler's fallacy). This means that we now must spin the spinner a total of $101$ times in order to expect exactly one $5$ . Doesn't this cause some kind of infinite regress? If we never expect a $5$ on any given spin, and no $5$ 's have come up thus far in the first $n$ spins, then won't it take $n+100$ spins to expect a 5? I know that at some point if we examine a group of spins then we can expect something with low probability like spinning a $5$ to occur, but it seems strange and counter-intuitive to me that though we expect a $5$ to come up in a sufficiently large group of spins, on each individual spin we do not expect a $5$ . Furthermore, we do not expect any number to come up on the first spin (in that the probability of any particular number appearing is low), and yet we know for certain that some number will still come up. For there to be a greater than $50\%$ chance of spinning a $5$ , we must spin the spinner $69$ times according to the following calculation: $$P(\text{Not rolling a 5 in } n \text{ spins})=\left(\frac{99}{100}\right)^n \\ \left(\frac{99}{100}\right)^n < 0.5 \\ \log_{0.99}0.5=68.967... $$ Hence, it must be spun $69$ times for there to be a greater than $50\%$ chance of there being a $5$ . Why is this not $50$ spins, as we expect $0.5$ $5$ 's to come up in this period? Also, I have the same question that once a $5$ does not come up, don't we have to spin it $70$ times for there to a greater than $50\%$ probability, and can't this cause the same infinite regress as described above?","['definition', 'probability']"
3354279,"Functions, is compression the inverse of stretch?","In Function Transformation in the equation $Y= a[k(x-d)] + c$ We have a which is responsible for vertical stretch
.When it is $> 1$ we are told it's a stretch and then if it's $< 1$ we are told it's a compression but by the same factor. Example $Y=2x^2$ .    Has a vertical stretch by factor of $2$ $Y=1/2x^2$ .  Has a vertical compression by a factor of $1/2$ Isn't this wrong? Since compression is the inverse of stretch, shouldn't it be a compression by a factor of $2$ ? 
I say this since compression is the inverse of stretch, $1/2$ in compression is equal to $2/1$ stretch when inverses? Am I correct, I am getting varying answers from different teachers and would like a definitive answer.","['functions', 'graphing-functions']"
3354363,Evaluating $\lim\limits_{x \to 8} \frac{x^{2/3}-4}{x^{1/3}-2}$ without L'Hospital,"The problem $$\lim\limits_{x \to 8} \frac{x^{2/3}-4}{x^{1/3}-2}$$ is on my problem set due tomorrow. In class, we only addressed limits with square roots, and we would just multiply by a conjugate to solve the limit. However, here multiplying by $$\left(\frac{x^{1/3}+2}{x^{1/3}+2}\right)^2$$ only produces another fraction where the denominator is zero when x=8. We have not yet learned L'Hospital's or derivatives in general so that is not an option. Is there perhaps some way to use limit squeeze theorem on this problem, or am I forgetting about some really easy limit law that solves this problem","['limits', 'limits-without-lhopital']"
3354374,"joint pdf of $(X,X)$","Let $X$ be a continuous random variable with uniform distribution on $[0,1]$ , i.e, the probability density function of $X$ is $f(x)=1$ on $[0,1]$ . Let $Y=X$ . Then what is the joint probability density function $h(x,y)$ of $X$ and $Y$ ? It seems that $h$ is supported on the line $y=x$ , which has $0$ measure. I guess the joint pdf does not exist or equals to Dirac delta function. Can anyone confirm?","['probability-distributions', 'probability-theory']"
3354376,Proving the well ordering principle with induction,"I am wondering if the following proof of the well ordering principle is correct by induction. Well-ordering principle: Every non-empty subset of $\mathbb{N}$ has a least or smallest element. To prove this, we will prove the following lemma: Let $n\in\mathbb{N}$ and let S be a nonempty subset of $\mathbb{N}$ such that $n\in S$ . Then S has a least element. Proof by strong induction: Base: When $n=1$ , we have that $1\in S$ . Then 1 is the least element of S. Assume that when $1,2,3,...,k \in S$ then S contains a least element. Inductive step: Let $n+1\in S$ . This can be broken down into two cases. Case 1 : If any of the numbers $1,2,3,...,n\in S$ then S contains a least 
element by the induction hypothesis. Case 2 : If S doesn't contain any  of the numbers $1,2,3,...n$ but $n+1\in S$ then $n+1$ is the least element in S. We have concluded the lemma. Now using the lemma to prove the well-ordering principle: Let $S$ be a nonempty subset of $\mathbb{N}$ . This implies $\exists n \in \mathbb{N}$ such that $n\in S$ . By the previous lemma, S has a least element.","['elementary-set-theory', 'proof-verification', 'real-analysis']"
3354402,Geometric properties of variety/scheme vs. algebraic properties of the coordinate ring,"I am looking for a list comparing the geometric/topological properties of an  affine scheme $X \simeq \text{Spec}(R)$ with the algebraic properties of the corresponding ring $R$ and vice versa. Particular emphasis should be placed on affine $K$ -varieties to make the geometric picture clearer. An example of this would be: An affine $K$ -variety $X$ is irreducible iff $K[X]$ is an integral domain. eg. $\mathbb{C}[x,y]/(x)$ is an ID and consists of only a single curve, whereas $\mathbb{C}[x,y]/(xy)$ is not an ID and consists of the union of two curves. the classical points of an affine scheme $X$ lie dense iff $R$ is Jacobson An affine $K$ -variety $X$ has no ""thick subschemes"" iff $K[X]$ is reduced. Where by a ""thick subscheme"" I mean something like the thick $y$ -axis $\mathbb{R}[x,y]/(x^2)$ as opposed to $\mathbb{R}[x,y]/(x)$ As suggested by the examples I am not looking for ultra precise statements on the geometric side of things, but rather one giving justification for either why certain algebraic properties of rings are worth studying or why certain geometric properties are worth studying. If possible (or you feel the need) please provide an illustrative example (as for instance in my first example) If possible (as for instance in my second example) a proof or link to a proof or resource is appreciated. In any case, please provide some explanation, or why you think of algebraic property Y giving geometric property Z. If, in order to describe the geometric properties, you use non-standard language (as for instance in my third example) please explain. Finally, maybe think of the question like this: You read about some new class of rings and ask yourself ""Why does that definition make sense? Why should those rings be interesting?"" This question should be one place to look for in seeking an answer as, in the geometric setting, the importance may become more clear.","['affine-schemes', 'algebraic-geometry', 'soft-question', 'big-list']"
3354505,How to find a point on a line that minimizes sum of distances from three given points?,"This is a cross-post of a question on MathOverflow where it didn't get much attention: https://mathoverflow.net/questions/337892/how-to-find-a-point-on-a-line-that-minimizes-sum-of-distances-from-three-given-p Let there be given three points $x_1$ , $x_2$ , $x_3$ and a line $l$ on a plane. Does there exist an explicit method of finding such a point $p$ on $l$ , that sum of distances of $p$ from $x_1$ , $x_2$ , $x_3$ is possibly minimal? Such result for two given points and a line is trivial and known as the Heron's theorem commonly used in optics to find a path of a ray of light. All I know about the solution is that if $p$ minimizes the sum of distances, then $$\cos{\alpha}+\cos{\beta}+\cos{\gamma}=0$$ The equation above comes directly from the derivative of the function of sum of distances if we assume that the line has equation $y = 0$ : $$\frac{d}{dx} \sum_{i=1}^3 \sqrt{(x-x_i)^2+(0 - y_i)^2} = 0$$ $$\sum_{i=1}^3 \frac{x-x_i}{\sqrt{(x-x_i)^2+(y_i)^2}} = 0$$ $$\cos{\alpha}+\cos{\beta}+\cos{\gamma}=0$$ However, that is not really helpful neither for construction nor finding the solution analytically. Some rare situations may be solved using Brianchon's theorem. That is, if we add symmetrical reflections of $x_1$ , $x_2$ , $x_3$ on the opposite sides of $l$ and the six points are vertices of a hexagon circumscribing a conic section, then the intersection of the main diagonals minimizes the sum of distances. However it is far from general solution.","['optimization', 'geometry', 'plane-geometry', 'euclidean-geometry']"
3354530,"can I ""index"" a function?","I have a function with two variables $f(x, y) = \frac{(x-y)^2}{y*(1-y)},$ where $x, y\in \Bbb R$ , $0<x<1$ , $0<y<1$ . And I have a bunch of data points $D =((x_1, y_1),(x_2, y_2),...(x_n, y_n), )$ . My goal is to find $(x^*, y^*) = \arg\max_{(x_i,y_i)\in D}f(x_i,y_i)$ Since my dataset is really big, I'm wondering whether there is any smart way to find the maximum without evaluating every data point. What can I do except for run a convex hull on my data $D$ , and only select from the boundary points?","['functions', 'discrete-mathematics']"
3354562,How to prove that a point defined by trigonometric functions involving 4 parameters is inside a certain tetrahedron?,"This question has emerged in connection with an attempt to solve this recent question (see Remark below): How can be proven (see result of simulation on 10000 points on graphics below) that a point with coordinates $$\begin{cases}
x&=&\cos(\gamma-\delta)-\cos(\beta-\alpha)\\
y&=&\cos(\beta-\delta)-\cos(\alpha-\gamma)\\
z&=&\cos(\beta-\gamma)-\cos(\alpha-\delta)
\end{cases}$$ where $\alpha,\beta,\gamma,\delta$ are arbitrary angles in $(0,2 \pi)$ is necessarily inside the regular tetrahedron defined by inequations : $$\begin{cases}
\ \ x+y+z&>&-2\\
\ \ x-y-z&>&-2\\
-x+y-z&>&-2\\
-x-y+z&>&-2
\end{cases} \ \ ? \tag{1}$$ otherwise said, tetrahedron with vertices $$\begin{pmatrix}
2\\
2\\
2
\end{pmatrix}, \begin{pmatrix}
\ \ 2\\
-2\\
-2
\end{pmatrix},\begin{pmatrix}
-2\\
\ \ 2\\
-2
\end{pmatrix},\begin{pmatrix}
-2\\
-2\\
\ \ 2
\end{pmatrix}.$$ My attempts : I have used different trigonometric formulas without any success. I have also tried to use tetrahedral coordinates, with failure as well. Remark : If the first formula in (1) is established, the upsaid question is solved by setting $$A=(\cos(\alpha),\sin(\alpha)), B=(\cos(\beta),\sin(\beta)), C=(\cos(\gamma),\sin(\gamma)), D=(\cos(\delta),\sin(\delta)),$$ and replacing the inequality to be established ( $AB^2+\cdots > -4$ ) by $$(\cos(\alpha)-\cos(\beta))^2+(\sin(\alpha)-\sin(\beta))^2+\cdots > -4$$ $$2-2 \cos(\alpha-\beta)+\cdots > -4$$","['trigonometry', 'geometric-inequalities']"
3354566,Can derivatives be defined as anti-integrals?,I see integrals defined as anti-derivatives but for some reason I haven't come across the reverse. Both seem equally implied by the fundamental theorem of calculus. This emerged as a sticking point in this question .,"['calculus', 'terminology']"
3354573,Eccentricity Vector of an Ellipse -- Geometric Derivation?,"I've been playing Kerbal Space Program again, and so I'm learning about orbital mechanics. There's a particular vector I can derive physically, but it's an intrinsically geometric object, and so I'd like to find out if there's a purely mathematical derivation of it (e g., no using conservation of energy). Consider an ellipse, with one focus at the origin, and the other on the negative x-axis. Let $\ell$ be its semi latus rectum, and $e$ be its eccentricity. Given a point P on the ellipse, let $\vec r$ be the vector from the origin to P. Draw the tangent ray T to the ellipse at P, and let $\theta$ be the angle it makes with $\vec r$ . Lastly, erect a perpendicular to T at P, with length $\ell/\sin \theta$ , and call it $\vec{q}$ . Claim: $\vec{q} - \vec{r} = re \hat{x}$ . I'm having a lot of trouble with this, primarily because I don't have a good understanding of how $\theta$ varies with P. It starts at 90Â° at the periapsis, increases up to some unknown value $M$ , goes back down to 90Â° at apoapsis, and then down to $180Â° - M$ , before finally returning to 90Â°. $M$ gets bigger as the ellipse gets more enlongated, but that's all I know about it. I can coordinate bash of course, but I'm hoping for something cleaner. There's gotta be some way to exploit the geometric properties of the ellipse. Here's the physics behind why I care about this, feel free to skip if you're not interested in motivation. From my position and velocity vectors, I'd like to determine my orbit. We know it's an ellipse, because Kepler, so there's only a few things we need to determine: size, shape, orientation. Size is easy to find from the vis-viva equation , which itself comes from conservation of energy. Most of orientation is also easy; the orbital plane is the plane containing our position and velocity vectors, and so it's just the plane normal to $\vec r \times \vec v$ . The remaining information we need to find is the eccentricity $e$ , and how our ellipse is oriented within the orbital plane. We'll encode the latter by an angle $\nu$ , the angle from  periapsis to our position (astronomers call this the ""true anomaly""). There's something in the literature called the ""eccentricity vector"", which is defined as $\vec e = \frac{\vec v \times \vec h}{\mu} - \frac{\vec r}{r}$ , where $\vec h$ is the specific angular momentum $\vec r \times \vec v$ . It allegedly has magnitude $e$ , and makes angle $-\nu$ with our position vector (i.e., this is a positive multiple of the periapsis vector). I don't really have much intuition around this vector. The first component points perpendicularly outward from the ellipse, and somehow the second term is exactly the ""correction"" you need to point along the focal axis. It's not even clear that $\vec e$ is a constant! One nice thing about it though is that it behaves cleanly when the orbit is circular; $\vec e$ will be zero, leaving $\nu$ indeterminate, which makes sense because there's no well-defined periapsis on a circle. To show that this vector is what it promises to be, first, you show it's a constant, by taking the time derivative, and using the conservation of angular momentum. Then, consider the value at periapsis. Velocity is perpendicular to position there, so $h = rv$ , and since both terms point in the right direction, the direction is correct, and the magnitude is $rv^2/\mu - 1$ , which if you plug in $r_{periapsis} = (1-e)a$ and $v_{periapsis}^2 = \frac{\mu}{a} \frac{1+e}{1-e}$ , will yield the eccentricity $e$ . But I feel like you should be able to do this geometrically, via the following interpretation. The semi latus rectum of the ellipse is $\ell = h^2/\mu$ , so the magnitude of $\frac{\vec v \times \vec h}{\mu}$ is $vh/\mu = h/(r \sin \theta) \cdot h/\mu$ , where $\theta$ is the angle between $\vec r$ and $\vec v$ . Now we've eliminated all non-geometric quantities like velocity. Scaling the vector by $r$ , to give it units of length, we arrive at the problem above. *""specific"" means ""divided by orbiter mass""","['physics', 'geometry', 'mathematical-physics', 'plane-geometry']"
3354578,"For $x$ and $k$ real numbers, determine the values of $k$ for which the graphs of $f(x)=x^2-4$ and $g(x)=2|x|+k$ do not intersect","For $x$ and $k$ real numbers, determine the values of $k$ for which the graphs of $f(x)=x^2-4$ and $g(x)=2|x|+k$ do not intersect. For $x$ and $k$ real numbers, determine the values of $k$ for which the graphs of $f(x)=x^2-4$ and $g(x)=2|x|+k$ have exactly 2 intersections. I graphed them on Desmos and found that when k is smaller than -5 they donât have any intersections and when k is greater or equal to -5 theyâll have two intersections, but I really have no idea what to begin with when it comes to handwriting.",['functions']
3354582,Compute $\sum \binom{n}{k} \binom{n+k-1}{k} (-1)^k$,How can we study a series like this : $\sum_{k=0}^{n} \binom{n}{k}\binom{n+k-1}{k} (-1)^k$ . I thought about consider $S(x) = \sum_{k=0}^{n} \binom{n}{k}\binom{n+k-1}{k} x^k$ . But the only one I've found is hypergeometric function. It's hard to analyze. Hope there are more combinatorical ideas for finding such series. Any hints? Maybe generating functions?,"['binomial-coefficients', 'combinatorics']"
3354592,Find a limit of $\ln(u_n)/2^n$,"Let $u_0 \in \mathbb{N}^{*}$ and $(u_n)$ such that $u_{n+1}=1+u_n^2$ . I've shown that the sequence $\displaystyle \left(\frac{\ln\left(u_n\right)}{2^n}\right)$ converges and I wonder if it possible to find its limit $\ell$ For example, if $u_0=1$ then $\ell \approx 0.40735$ , if $u_0=15$ then $\ell \approx  2.7103$ .",['sequences-and-series']
3354598,Polygon and its diagonals.,"So there is a N sided polygon, and some given number of lines, K that we  can use as diagonals to connect its vertices. For simplicity, if we view the polygon as a cyclic graph with N vertices, the goal is to use all K lines to connect its diagonals and minimize the number of  incoming edges (or degree) of all vertices of the graph as much as possible by distributing every K over the vertices optimally, and finally get the maximum of degrees of all the vertices as the answer to the problem.
In short, you have to try minimizing the degree of individual vertices, then the result is the maximum of the set of degrees of all vertices at that configuration. for example , if , N = 6 and K = 2 , the maximum degree of the graph after using all K lines to connect vertices( 6 in this case, one diagonal connecting two each),  is 3 . Explanation , 
lets number the vertices of the hexagon here as 0, 1, 2, ...5. Now, we have to use all the K (=3) lines here as diagonals, so we can connect, vertex 0 to vertex 2, vertex 1 to vertex 4, vertex 3 to vertex 5. We have used up all the K lines as we were asked and the maximum of all the  degrees we get is 3 ( 2  + 1) , where 2 edges are of adjacent vertices of that vertex and remaining one is the diagonal to the opposite vertex. ( there can be another configurations too like 0 to 3, 1 to 5 and 2 to 4, the output is same). In this case, every vertex will have same degree, i.e. 3, hence the max is 3. Apparently, the output is same of same hexagon and with K = 2 , because the maximum of degrees of all the vertices is still 3 . A few more examples ,
for, N = 6 and K = 8, result is 5 , for N = 5 and K = 4, result is 4 . I have been trying to establish some algebraic relation between N and K by simulating different situations but i have not been able to observe any pattern till now. Any ideas ? P.S. : For clarification, the K cannot exceed $n*(n-3)/2$ , because that is the max. number of diagonals a polygon can have. also , you can't overwrite already existing diagonals.","['graph-theory', 'linear-algebra', 'geometry', 'combinatorics']"
3354624,Center and radius of the osculating circle - The limiting position of a circle trough three points,"I am stucked on problem 1.7.2.b of Differential Geometry of Curves and Surfaces by Manfredo do Carmo. The problem is similar as this topic , but here the exercise defines the osculator circle, ie, this circle of exercise is whose we call osculator circle, and is in $\mathbb{R}^3$ . My difficult is another of that topic. (Adaptated exercise) Let $\alpha(s) \colon I \to \mathbb{R}^3$ be the parametrization by arc length of a curve. Consider a circle that passes through the three points $\alpha(0)$ , $\alpha(h_1)$ , and $\alpha(h_2)$ .  Prove that as $(h_1, h_2) \to (0, 0)$ , the limiting position of this circle has center at the line that contains $n(0)$ and radius $1/k(0)$ . We can consider the canonical form of the curve at $0$ . As some similar exercises of the book, I beginned witring $P: ax+by+cz=0$ as the plane containing $\alpha(0),\alpha(h_1),\alpha(h_2)$ (by item a) of the question we know that its limiting position is the osculating plane). The center $C$ of this circle $\Pi$ is given by the intersection of mediatrixs at $P$ , with directions $$v_1=(a,b,c)\wedge \alpha(h_1)\quad,\quad v_2=(a,b,c)\wedge\alpha(h_2)$$ Am I correct...? Well, considering $|(a,b,c)|=1$ , we obtain: $$v_1\cdot v_2=\begin{vmatrix}|(a,b,c)|^2&0\\
0&\alpha(h_1)\cdot \alpha(h_2) \end{vmatrix}=\alpha(h_1)\cdot \alpha(h_2)$$ We ask for $k_1,k_2$ s.t. $$\alpha(h_1)+v_1k_1=\alpha(h_2)+v_2k_2$$ I wrote some calculus from here but they are confused and I could not finish... Many thanks in advance.","['curves', 'frenet-frame', 'osculating-circle', 'differential-geometry']"
3354629,"Understanding what is meant by ""Integrability"" with regards to the Euler Top","I am reading Discrete Systems and Integrability by F.W. Nijhoff, J. Hietarinta, and N. Joshi. Currently, I am investigating Chapter 6 and in particular the Euler Top. The book says the following: There are several properties associated with integrability of maps,
  for example, the existence of a sifficient number of conserved
  quantities, symmetries, Lax pair and the behavior around
  singularities. The book then defines a type of Integrability A system with $2N$ -dimension phase space is called Louiville
  Integrable if there are $N$ conserved quantities The Euler Top is  given by $$
\begin{cases}
\overset{\cdot}{x}_1 = \alpha_1 x_2 x_3, \\
\overset{\cdot}{x}_2 = \alpha_2 x_{3}x_{1}, \\
\overset{\cdot}{x}_3 = \alpha_3 x_{1}x_{2}.
\end{cases}
$$ where $\alpha_{1,2,3}$ are real parameters. This is one of the most famous integrable systems of classicaly mechanics. The function $H(x) = \gamma_1 x_1^2 + \gamma_2 x_2^2 + \gamma_3 x_3^2$ is an integral (conserved quantity?) for the above system $\iff \gamma \perp \alpha$ . In particular, there are $3$ integrals of motion (conserved quantities) of the system where only two of them are independent: $$H_1 = \alpha_2 x_3^2 - \alpha_3 x_2^2, \; H_2 = \alpha_3 x_1^2 - \alpha_1 x_3^2 , \; H_3 = \alpha_1 x_2^2 - \alpha_2 x_1^2$$ So, here is what I know. The dimension of the Euler Top is $3$ because there are three independent variables $x_1, x_2, x_3$ . We have found $2$ independent conserved quantities. Since the dimension of the system is $3$ , then this doesn't fit the definition of the Louiville Integrability. So, by what definition is the Euler Top called Integrable?","['integrable-systems', 'ordinary-differential-equations', 'dynamical-systems']"
3354634,"Measure of the set $\{x\in [0,1]: \text{the decimal expansion of } x \text{ contains infinitely many 7.} \}$.","I got this question in an old qualifier exam of Real analysis. Let $A$ be the set of all real numbers in the closed interval $[0,1]$ whose decimal expansion contains infinitely many 7. Find the Lebesgue measure of the set $A.$ (Now ownward when I say a number, it is supposed to be in the unit interval.)
If I define $A_i$ to be the set of all those numbers whose decimal expansion has a $7$ at the $i$ th place. It is clear that the set $A$ in the question is limit supremum of all $A_i$ . And, I can show that $A_i$ are independent event (Lebesgue measure restricted to the unit interval is a probability measure). We next observe that $$\sum |A_i| = \infty.$$ An application of second Borel-Cantelliâs lemma therefore gives that the measure of $A$ is 1. My problem is that the notion of independence is a probabilistic notion (and not introduced in most real analysis classes). So, I am interested in a more direct (say, analytical?) approach to this problem. While writing this question this thought came to mind that I want something along the following line: Try showing that the set of all numbers whose decimal expansion has only finitely many $7$ appearing has a measure 0. In order to do so, we first obeserve that there are only countably many finite subsets of natural number. So if for a fixed finite subset $F$ of natural numbers, I can show that the set of numbers which has 7 only at those locations specified by the set $F$ has measure 0, then I will be done. Any thought on this? Would this work or not? Any alternate approach is also a welcome? Meanwhile I will try the above idea and update accordingly.","['borel-cantelli-lemmas', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
3354684,Proving inequality $|\sin(n\theta) |\le n\sin\theta$,"I was trying to prove following inequality: $$|\sin n\theta| \leq n\sin \theta \
\text{for all n=1,2,3... and } \ 
0<\theta<Ï $$ I succeeded in proving this via induction but I didn't get ""feel"" over the proof. Are there other proof for this inequality?","['alternative-proof', 'trigonometry', 'inequality']"
3354715,Different terminology for spectrum?,"The common terminology we get used to on spectrum of linear operators is point spectrum $ \sigma_p $ , continuous spectrum $ \sigma_c $ , residual spectrum $ \sigma_r $ . However, we recently notice an terminology of another type: purely point spectrum $ \sigma_{pp} $ , absolutely continuous spectrum $ \sigma_{ac} $ , singularly continuous spectrum $ \sigma_{sc} $ . The latter type is defined with respect to decomposition measure and orthogonal decomposition of spectral projections. For details, see for instance Chapter 3.3 in ""G. Teschl: Mathematical Methods in Quantum Mechanics - With
  Applications to SchrÃ¶dinger Operators (2014)"". Question: What is the relation between above two terminologies, can we give an exact correspondence for them? Edit: The relation I have found so far is $ \sigma_{pp} = \overline{\sigma_{p}} $ . One possibly useful result: Chapter IV, Proposition 1.16 in [Engel, Nagel: One-Parameter Semigroups for Linear Evolution Equations
  (2000)] However, I can not see clearly how to use this result to explain the connections between the two terminologies.","['spectral-theory', 'functional-analysis', 'terminology']"
3354759,Solve $\tan x =\sec 42^\circ +\sqrt{3}$,"For the trigonometric equation, $$\tan x =\sec 42^\circ+\sqrt{3}$$ Find the angle $x$ , where $0<x<180^\circ$ . I tried to solve for an unknown angle $x$ in a geometry problem with a trigonometric approach. I ended up with the trig equation above. Without hesitation, I reached my calculator, entering the right-hand-side and arctan-ing it for $x$ . To my surprise, the angle $x$ comes out at exactly 72 degrees. I did not expect such a neat relationship. Then, I thought I should have solved the equation analytically for the whole-degree angle without the calculator. I spent a good amount of time already and  was not able to derive it yet. Either the equation is not as innocent as it looks, or a straightforward method just eludes me.","['algebra-precalculus', 'trigonometry']"
3354913,Solution of $AX+XA=B$ through eigenvectors of $A$,I see that Matlab uses the spectral decomposition to solve the continuous Lyapunov equation $$AX+XA=B$$ The formula they use for positive definite $A$ with matrix of eigenvectors $U$ and column vector of eigenvalues $s$ is as follows $$U \left( \frac{U' BU}{s + s'} \right) U'$$ (Addition of row and column is done with NumPy broadcasting rules) Any suggestions on how to derive this?,"['matrices', 'matrix-equations', 'linear-algebra', 'eigenvalues-eigenvectors']"
3354935,"Prove that if analytic function $f$ is such $f(0) \neq 0$, it has no zeroes in a certain disk [duplicate]","This question already has an answer here : Distance of nearest zero of an analytic function (1 answer) Closed 4 years ago . Problem. Assume $\displaystyle f(z)=\sum_{n=0}^\infty a_n z^n$ is analytic in $\overline{U}=\{|z|\leqslant R\}$ and $a_0\ne 0$ ãProve: $f$ has no zeroes in the circular disk $\left \{|z|< \dfrac{|a_0|R}{|a_0|+M}\right\}$ , where $M = \max_{z\in \partial \overline U} |f(z)|$ . My attempt: I tried to use proof by contradiction, that is to assume $f$ has zeroes in the disk, but however I was not able to deduce the contradiction. So I guess probably it is not the right approach for this problem. Also I guess it may be related to the maximum modulus principle , which I failed to establish connections with. Any help is appreciated.","['complex-analysis', 'power-series', 'roots']"
3354990,Finding a function based on their limits,"I have points and limits of a function and even the shape of the function and I'm looking for the function, something that very interesting for me how could I control the curve of the function? (1) $\lim\limits_{x \to inf} f(x) = 1 $ (2) $f(\frac{1}{c}) = 1 $ (3) $0\lt x$ (4) $0\lt c \leq 1$","['limits', 'functions']"
3355050,"Prove $f(x)$ has three extremum points on $(-2\pi,2\pi)$","Prove $f(x)$ has three extremum points on $(-2\pi,2\pi)$ $$f(x) = \frac{\sin x}{x}, x \not= 0 $$ $$f(x) = 1, x = 0 $$ I calculated its derivative. $$f'(x) = \frac{x\cos x-\sin x}{x^2}, x \not= 0$$ $$f'(x) = 0, x = 0$$ I can see that $f'(x)$ has a solution for x = 0. I am not sure how to solve for $x\not= 0$ though. I have tried this: $$ \frac{x\cos x - \sin x}{x^2} = 0 $$ Multiply both sides by $x^2$ $$ x\cos x - \sin x = 0 $$ Add $\sin x$ to both sides $$ x\cos x = \sin x$$ Now divide both sides by $\cos x$ $$ x = \tan x $$ Did I do anything wrong? Can this equation be figured out without using any kind of graphing tool?","['calculus', 'trigonometry']"
3355122,Other Near Inverses in Mathematics,"As a young mathematician, one learns that the derivative and integral are kind of like inverse operations, but not quite. The two fundamental theorems of calculus require some additional assumptions to go through. Moreover, the maps $f \mapsto f'$ and $f \mapsto F$ (where $F(x) = \int_a^x f(t) dt$ for some fixed $a$ ) both fail to be injective and surjective on many function spaces. Yet, it remains a useful intuition to think of integration and differentiation as opposite, if not inverse, operations. Are there other examples of this near inverse behavior in mathematics? Is there a more general phenomenon like this that integration and differentiation are just examples of?","['calculus', 'soft-question', 'functional-analysis']"
3355137,Number of labeled Abelian groups of order n,"I calculated the number of labeled Abelian groups of order $N$ (i.e., the number of distinct, abelian group laws on a set of $N$ elements). This sequence is given by OEIS A034382 , but my solution differs at $N=16$ . Please point out mistakes or confirm my solution? Let $C_n$ be a cyclic group of order $n$ , $Aut(G)$ be an automorphism set of $G$ . Tthe number of labeled Abelian groups of order $N$ is $\displaystyle{\sum \frac{N!}{\# Aut(G)}}$ where G runs representative of isomorphic equivalence. I got $\displaystyle{
\# Aut(C_{p^n}^k)=p^{(n-1)k^2}\prod_{j=0}^{k-1} (p^{k}-p^{j})
}$ and $\displaystyle{
\# Aut(\prod_i C_{p^{n_i}}^{k_i})
=\prod_i \left( (p^{(n_i-1)k_i^2}\prod_{j=0}^{k_i-1} (p^{k_i}-p^{j})) ( \prod_{j\neq i} p^{\min(n_i,n_j)k_j} )^{k_i} \right)
}$ . From the fundamental theorem of finite abelian groups, There are 5 groups for $N=16$ : $C_{16}, C_2 \times C_8, C_4^2, C_2^2\times C_4, C_2^4$ . Therefore, the number of groups that are isomorphic to each group is: $C_{16}$ ... $\displaystyle{\frac{16!}{8}}$ $C_2 \times C_8$ ... $\displaystyle{\frac{16!}{(1\times 2)\times (2\times 4)}}$ $C_4^2$ ... $\displaystyle{\frac{16!}{16\times 3\times 2}}$ $C_2^2\times C_4$ ... $\displaystyle{\frac{16!}{((3\times 2)\times 2^2)\times (2^2\times 2)}}$ $C_2^4$ ... $\displaystyle{\frac{16!}{15\times 14\times 12\times 8}}$ Sum of them is $4250979532800$ . OEIS says $4248755596800$ .","['finite-groups', 'group-theory', 'combinatorics']"
3355161,Hard inequality :$\Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4}$,"I have a hard problem this is it : Let $a,b,c>0$ such that $a^ab^bc^c=1$ then we have : $$\Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4}$$ I try to use Jensen's ienquality applied to the function $f(x)=\frac{1}{x^2}$ but it doesn't works for all the values . if we apply Am-Gm it's like Jensen's inequality so I forget this ways . Maybe If we apply Karamata's inequality but I didn't found the right majorization . I try also to use Muirhead inequality but without success . So I'm a bit lost with that if you have a hint or a full answer I will be happy to read your work .","['exponentiation', 'inequality', 'real-analysis']"
3355180,What is the mathematical notation for mentioning variables names explicitly in a function assignment?,"I have a function with two parameters: $f(s,t)$ . I want to assign to the function $\alpha$ and $\beta$ : $f(\alpha,\beta)=5$ . But I don't want the reader to confuse $s$ and $t$ (the function's variables) from the values $\alpha$ and $\beta$ .
I want to mention explcitly which is assigned to $s$ and which is assigned to $t$ . So I would like to denote it, for example, like this: $f(s=\beta,t=\alpha)=5$ , or like this $f(s:\beta,t:\alpha)=5$ . Is there a convention for such a notation?
What is the correct notation?","['notation', 'functions']"
3355215,"For $x$ and $k$ real numbers, for what values of $k$ will the graphs of $f(x)=-2\sqrt{x+1}$ and $g(x)=\sqrt{x-2}+k$ intersect?","For $x$ and $k$ real numbers, for what values of $k$ will the graphs of $f(x)=-2\sqrt{x+1}$ and $g(x)=\sqrt{x-2}+k$ intersect? I tried to make an equation of them, but Iâm stuck with the two variables and I couldnât solve it. Much appreciation. We didnât do calculus yet..","['algebra-precalculus', 'functions']"
3355244,Number of muffins that can be eaten in three hours. 4 variables.,"Assuming that all clones have the same appetite, in one hour, $100$ clones can eat $1500$ cupcakes and $1000$ muffins. In two hours, $ 60 $ clones can eat $1200$ cupcakes and $1500$ muffins.
In three hours, $50$ clones can eat $750$ cupcakes and $x $ muffins. Find $x$ . I tried using variations here by letting the number of clones be equal to the number of cupcakes to be eaten times a variable $k$ all over the number of hours times the number of muffins to be eaten, but $k$ does not seem to be constant.","['word-problem', 'algebra-precalculus', 'puzzle']"
3355248,expected value of sum over sum of squares,"I was trying to compute the expectation of something where something of the form $$\mathbb{E}\left[\frac{\sum_i X_i}{\sum_i X_i^2}\right]$$ appears multiple times, where $X_i \sim \mathcal{N}(\mu_x, \sigma_x^2)$ is normally distributed. Now, I have this feeling that this form should be something that could be known in statistics. After all this looks very much like the quotient of empirical mean and (uncorrected) variance, but I don't seem to find anything on what this could be distributed like or what this expectation is. I am aware that this form looks very similar to the t-distribution, but since the step from variance to standard deviation is non-linear, I am not sure whether this relation would help me in any way. Is there anybody out there who could give me some pointer(s) what distribution this is or at least how I could compute the expectation of something like this?","['expected-value', 'statistics', 'probability-distributions', 'normal-distribution']"
3355273,Dynamical Systems / Differentiatial Equations Problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I need help with my Dynamical systems homework, I honestly don't even really know where to start on it. Any help would be appreciated Show that if $x(t)$ is a solution to $x'(t) = x(t)^2 - 1 ,$ then so is $z(t) = -x(t)^{-1}$ and $y(t) = -x(-t).$ What type of symmetries are these and how are they reflected in the phase plane?","['ordinary-differential-equations', 'dynamical-systems']"
3355342,Does convergence in distribution imply anything about the KL divergence?,"Given that some sequence of random variables $X_n$ converge in distribution to $X$ and assuming that the distribution of all $X_n$ are absolutely continuous with respect to a Lebesgue measure, is there anything that we can say about $KL(p(X_n)|p(X))$ ?","['statistics', 'probability-limit-theorems', 'weak-convergence']"
3355426,Vector space structure on orbit spaces of Group action on $\mathbb{C}$ by $S^{1}$,"Let $G=S^{1}$ be a group with multiplication and $V=\mathbb{C}$ be a vector space of complex  numbers. Let $G$ acts on $V$ as $(g,z)=gz$ usual multiplication of complex numbers. Then the orbit $Gz$ will be a circle centered at $0$ and having radius $|z|$ . The orbit space $V/G= \{ Gz : z \in V \}$ is the set of concentric circles centered at origin.  How can we  define a vector space structure on $ V/G $ ? First we will try to define addition : $ Gz_{1}+Gz_{2} $ We cannot define it as $ G z_{1}+Gz_{2}= G(z_{1}+z_{2}) $ . Consider the case $ Gz_{1}=Gz_{2}=S^{1} $ . Let us take $ 1 $ and $ -1 $ on $ G1=G(-1)= S^{1} $ . Their sum will be $ G0 =0  $ but if we take other two points $ 1 $ and $ i $ on same orbit their sum will go to the orbit $ G(1+i) $ . Hence this type of vector addition is not well defined. Suppose we define it as $Gz_{1}+Gz_{2}= G(z_{1}.z_{2})$ . Then multiplication of two points on two concentric circles with radius $r_{1}$ and $r_{2}$ will give a point on a concentric circle with radius $r_{1}.r_{2}$ rotated by an angle $\theta_{1}+\theta_{2}$ . This operation is well defined. It is closed, commutative, and associative. $S^{1}=G{1}$ is the additive identity. But all the orbits except $G0=0$ has inverse. We cannot find multiplicative inverse to $0$ . Is their any other way to define vector space structure on orbit space in this case ? Can we define vector space structure on orbit space $V/G$ in general (Let $G$ is a group and $V$ is a vector space ) ? Do we require any other conditions for defining vector space structure on $V/G$ ?","['group-theory', 'group-actions', 'vector-spaces']"
3355435,Numerical phenomenon. Who can explain?,"I was doing some software engineering and wanted to have a thread do something in the background to basically just waste CPU time for a certain test. While I could have done something really boring like for(i < 10000000) { j = 2 * i } , I ended up having the program start with $1$ , and then for a million steps choose a random real number $r$ in the interval $[0,R]$ (uniformly distributed) and multiply the result by $r$ at each step. When $R = 2$ , it converged to $0$ . When $R = 3$ , it exploded to infinity. So of course, the question anyone with a modicum of curiosity would ask: for what $R$ do we have the transition. And then, I tried the first number between $2$ and $3$ that we would all think of, Euler's number $e$ , and sure enough, this conjecture was right. Would love to see a proof of this. Now when I should be working, I'm instead wondering about the behavior of this script. Ironically, rather than wasting my CPUs time, I'm wasting my own time. But it's a beautiful phenomenon. I don't regret it. $\ddot\smile$","['stochastic-processes', 'probability']"
3355445,The density of simple functions in the intersection of mixed norm spaces,"Problem 1. Let $\mathcal{A}([0,1] \times [0,1])$ be the set of all measurable functions $f:[0,1] \times [0,1] \to \mathbb{C}$ that satisfy the conditions $$
\alpha_1(f)
=
\operatorname*{ess sup}_{y \in [0,1]}
\int\limits_{0}^{1}
\lvert f(x,y) \rvert \,
dx
< \infty,
$$ $$
\alpha_2(f)
=
\operatorname*{ess sup}_{x \in [0,1]}
\int\limits_{0}^{1}
\lvert f(x,y) \rvert \,
dy
<\infty.
$$ One can prove that $\mathcal{A}([0,1] \times [0,1])$ is a Banach space with the norm $$
\lVert f \rVert_{\mathcal{A}([0,1] \times [0,1])}
=
\max \bigl(\alpha_{1}(f), \alpha_{2}(f) \bigr).
$$ Question 1. Consider the set $\Phi([0,1] \times [0,1])$ of a simple functions of the form $$
\phi(x,y)
=
\sum_{i=1}^{N}
\lambda_i \, \chi_{E_i}(x,y),
$$ where $\lambda_1, \ldots \lambda_N, \in \mathbb{C}$ is a sequence of complex numbers, $E_1, \ldots, E_N$ is a sequence of disjoint measurable subsets of a set $[0,1] \times [0,1]$ , $\chi_{E_i}$ is the indicator function of the set $E_i$ .
Is $\Phi([0,1] \times [0,1])$ dense in $\mathcal{A}([0,1] \times [0,1])$ ? Question 2. Consider the set $\Psi([0,1] \times [0,1])$ of a simple functions of the form $$
\psi(x,y)
=
\sum_{i=1}^{N}
\lambda_i \, \chi_{X_i}(x) \, \chi_{Y_i}(y),
$$ where $X_1, \ldots, X_N$ and $Y_1, \ldots, Y_N$ are sequences of disjoint measurable subsets of a set $[0,1]$ .
Is $\Psi([0,1] \times [0,1])$ dense in $\mathcal{A}([0,1] \times [0,1])$ ? Problem 2. Let $\mathcal{A}(\mathbb{R} \times \mathbb{R})$ be the set of all measurable functions $f:\mathbb{R} \times \mathbb{R} \to \mathbb{C}$ that satisfy the conditions $$
\alpha_1(f) 
=
\operatorname*{ess sup}_{y \in \mathbb{R}}
\int\limits_{-\infty}^{\infty}
\lvert f(x,y) \rvert \,
dx
< \infty,
$$ $$
\alpha_2(f)
=
\operatorname*{ess sup}_{x \in \mathbb{R}}
\int\limits_{-\infty}^{\infty}
\lvert f(x,y) \rvert \,
dy
<\infty.
$$ It is not hard to show that $\mathcal{A}(\mathbb{R} \times \mathbb{R})$ is a Banach space with the norm \begin{equation}
\lVert f \rVert_{\mathcal{A}(\mathbb{R} \times \mathbb{R})}
=
\max \bigl(\alpha_{1}(f), \alpha_{2}(f) \bigr).
\end{equation} Question 3. Similar to questions 1 and 2:
are $\Phi(\mathbb{R} \times \mathbb{R})$ and $\Psi(\mathbb{R} \times \mathbb{R})$ dense in $\mathcal{A}(\mathbb{R} \times \mathbb{R})$ ? As I can figure it out, simple functions are not dense in the Banach space of functions $f:\mathbb{R} \times \mathbb{R} \to \mathbb{C}$ that satisfy the condition $\alpha_1(f) < \infty$ only.
(See A. Benedek, R. Panzone. The spaces $L^p$ with mixed norm, Duke Mathematical Journal, Volume 28, issue 3, 1961, p. 308),
but I failed to understand why.
And what about the space $\mathcal{A}(\mathbb{R} \times \mathbb{R})$ ? My attempt to answer question 1. Let $E \subset [0,1] \times [0,1]$ , and define \begin{equation*}
E^x = \{y \in [0,1]:\: (x,y) \in E \},
\end{equation*} \begin{equation*}
E^y = \{x \in [0,1]:\: (x,y) \in E \}.
\end{equation*} Since for any $\phi \in \Phi([0,1] \times [0,1])$ \begin{equation*}
\alpha_{1} (\phi)
=
\operatorname*{ess sup}_{y \in [0,1]}
\sum_{i=1}^{N}
\lambda_i \,  \mu(E_i^y),
\quad
\alpha_{2} (\phi)
=
\operatorname*{ess sup}_{x \in [0,1]}
\sum_{i=1}^{N}
\lambda_i \,  \mu(E_i^x),
\end{equation*} where $\mu$ is the measure on $[0,1]$ ,
then $\Phi([0,1] \times [0,1]) \subset \mathcal{A}([0,1] \times [0,1])$ . Suppose $f^+ \in \mathcal{A}([0,1] \times [0,1])$ , $f^+(x,y) \geqslant 0$ for any $x,y \in [0,1]$ and define $$
\phi_m(x,y) =
\begin{cases}
\frac{k}{2^m},
&\text{if} \
\frac{k}{2^m} \leqslant f^+(x,y) <  \frac{k+1}{2^m}, \
k=0,1,\ldots, 2^{m}m - 1,
\\
m,
&\text{if} \
f^+(x,y) \geqslant m.
\end{cases}
$$ Then we have $$
\forall m \in \mathbb{N}: \quad \phi_m \in \Phi([0,1] \times [0,1]),
$$ $$\forall x,y \in [0,1]: \quad \phi_1(x,y) \leqslant \phi_2(x,y) \leqslant \ldots \leqslant f^+(x,y),
$$ and \begin{equation*}
\forall x,y \in [0,1]: \quad f^+(x,y) = \lim\limits_{m \to \infty} \phi_m(x,y).
\end{equation*} From conditions $\alpha_1(f^+) < \infty$ and $\alpha_2(f^+) < \infty$ it follows that
for any fixed $y \in [0,1]$ the function $f^+(\cdot, y)$ is integrable
and for any fixed $x \in [0,1]$ the function $f^+(x, \cdot)$ is integrable too.
Hence, the dominated convergence theorem shows that the sequences of functions \begin{equation*}
g^{1}_m(y) =
\int\limits_{0}^{1}
\left(
f^+(x, y) - \phi_m(x,y)
\right)
dx,
\quad
y \in [0,1],
\end{equation*} \begin{equation*}
g^{2}_m(x) =
\int\limits_{0}^{1}
\left(
f^+(x, y) - \phi_m(x,y)
\right)
dy,
\quad x \in [0,1],
\end{equation*} converges to $0$ pointwise.
Now using Egorov's theorem we can conclude that the sequences $g^{1}_m$ and $g^{2}_m$ converges to $0$ almost uniform but not uniform. Any help would be greatly appreciated!","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3355487,"If $T$ is a bounded linear operator from $L^p(\mathbb{R})$ to $L^q(\mathbb{R})$ and $T$ is non-zero, then $p \leq q$","I'm a student in a Fourier Analysis class with a broad background in Functional Analysis. When this question was posed to me, I wasn't sure how to begin. Poking around, I looked at $L^p$ embeddings but $T$ may not be injective into $L^q$ . Trying to solve this problem naively, I considered $f$ to have $L^p$ norm 1 (the sphere) and considered $$(\int_{\mathbb{R}} |Tf(x)|^q)^{\frac{1}{q}} \leq C  $$ where $C$ is the norm of $T$ . Can I ""normalize"" T and assume $C=1$ to get rid of that $\frac{1}{q}$ exponent. How would $f$ having norm 1 help me here? Edit: Would it be possible to ""mod out"" by the kernel of $T$ and consider that as an injective linear operator from $\frac{L^p(\mathbb{R}) }{kerT}$ to $L^q(\mathbb{R})$ ? I don't think I'd be able to use any embedding results here though.","['harmonic-analysis', 'lp-spaces', 'functional-analysis']"
3355539,Continuity of $y \sin(\frac{1}{x})$,"Problem: for which values of $\alpha \in \mathbb{R}$ is the function \begin{equation*}
f: \mathbb{R}^2\to\mathbb{R}^2 \qquad \begin{pmatrix} x \\ y\end{pmatrix} \mapsto \begin{cases} y \sin(\frac{1}{x}) \quad &\text{if $x\neq 0$}\\0  \quad &\text{if $x= 0$}\end{cases}
\end{equation*} continuous at $a = \begin{pmatrix} 0 \\ \alpha\end{pmatrix}$ ? Prove your answer. Attempted solution: I suppose the answer is: $\left\{ \begin{pmatrix} 0 \\ 0\end{pmatrix} \right\}$ . I tried to reason as follows, but I am not sure if this is technically correct. $f$ is continuous at $a$ $\iff \lim_{x\to 0} f\begin{pmatrix} x \\ \alpha\end{pmatrix} = \begin{pmatrix} 0 \\ 0\end{pmatrix}$ $\iff \lim_{x\to 0} \alpha \sin\left(\frac{1}{x} \right)= \begin{pmatrix} 0 \\ 0\end{pmatrix}$ $\iff \alpha = 0$ since $\sin$ is periodic, and thus diverges. Is this correct?","['multivariable-calculus', 'real-analysis']"
3355550,Prove that sum of the $k$ numbers in the $k$th group = ${\frac{1}{2}\left(k(k^2+1)\right)}$.,"Consider an arrangement of the positive integers, grouped as shown, so that the $k$ th group has $k$ elements: $(1),(2,3),(4,5,6),(7,8,9,10), \ldots$ . The expression for the sum of the $k$ numbers in the $k$ th group turns out to be ${\frac{1}{2}\left(k(k^2+1)\right)}$ . However, how would you prove this? I am assuming that you would have to proof by induction, but I can't seem to construct it as of now.",['discrete-mathematics']
3355554,Equality of limits or counterexample [duplicate],"This question already has answers here : $f(x+1)-f(x)$ converges $\Rightarrow\frac{f(x)}x$ converges (2 answers) Closed 4 years ago . Let $f: \mathbb{R} \to \mathbb{R}$ such that $\lim\limits_{x \to \infty} \frac{f(x)}{x} = c$ where $c\neq 0$ or $\infty$ . Is it true that for all such functions $f$ , if $\lim\limits_{x \to \infty}(f(x+1)-f(x))$ exists, then it is equal to $c$ ? Note : Say if $f(x) = x+\sin(x)$ , then the condition is clearly false because the second limit does not exist. But say, we impose another condition that the second limit should exist, then is it necessary that it goes to $c$ as well?","['limits', 'calculus', 'real-analysis']"
3355624,The fitted values of a simple linear regression are linear combinations of the observed,"I have notes which say that the i th fitted value $\hat{Y}_i$ is a linear combination of the response values $$\hat{Y}_i = \sum_{j=1}h_{ij}Y_j$$ where $$h_{ij} = \frac 1 n + \frac{(x_i-\overline{x})(x_j - \overline{x})}{S_{xx}} $$ First I'm a little unclear on the exact meanings of the variables.  In a linear regression, I thought there was an infinity of fitted values along the line of best fit.  Are these just the fitted values at the observed $x_i$ values?  And are the observed responses $Y_j$ all observed responses or just those at $x_i$ ? Second, I've searched for a proof of this but I can't find one.  I have a textbook showing that the coefficients of the regression are linear combinations of response variables (which if these are normal then the coefficient is normal), but that's not this.  Can anyone either direct me to one or give the proof?","['linear-regression', 'statistics']"
3355672,Simplification of $\prod\limits_{i=1}^{n}{\sin(it)}$,I am trying to find anything about the product $$\sin(t)\sin(2t)...\sin(nt)$$ The few things I have discovered the series of this product starts from $t^n$ coefficient. the first few coefficients are integers. But I was unable to discover any details about the count and nature of those coefficients. I need an efficient way to calculate this product. Any hint is appreciated.,"['trigonometry', 'taylor-expansion', 'complex-numbers', 'products']"
3355745,Help at the last step in the application of Levy's $0-1$ law,"Let $X_n$ be a Markov chain. Use Levy's zero-one law to show that if $$P(\cup_{m=n+1}^{\infty}\{X_m\in B_m\}|X_n)\geq\delta>0 \text{ on }\{X_n\in A_n\}\text{, then}$$ $$P(\{X_n\in A_n\text{ i.o.}\} - \{X_n\in B_n\text{ i.o.}\})=0$$ My attempt: $$\text{Let }C_n = \cup_{m=n+1}^{\infty}\{X_m\in B_m\}\implies \cap C_n=\{X_n\in B_n \text{ i.o.}\}$$ $$P(\cup_{m=n+1}^{\infty}\{X_m\in B_m\}|X_n) = E(1_{C_n}|X_n)=E(1_{C_n}|\mathcal{F}_n)\rightarrow E(1_{\cap C_n}|\mathcal{F}_{\infty}) = 1_{\cap C_n}  \text{ by 0-1 law}$$ where $\mathcal{F}_n = \sigma(X_1,..,X_n)$ and the second equality holds because of Markov property. We know that on $$\{X_n\in A_n\}, E(1_{C_n}|X_n)\geq \delta>0 \text{ and thus on } \{X_n\in A_n \text{ i.o.}\}, E(1_{C_n}|X_n)\geq \delta>0 \text{ i.o.}$$ and since the limit of $E(1_{C_n}|X_n) = \cap C_n = \{X_n\in B_n \text{ i.o.}\}$ , I guess $$\{X_n\in A_n \text{ i.o.}\}\subset \{X_n\in B_n \text{ i.o.}\}$$ May I know how that leads to the answer, I am missing something small. Thanks.","['probability-theory', 'markov-chains', 'real-analysis']"
3355749,Pre-calculus method for finding the 'neat' value $\sin(\frac{\pi}{3})=\frac{\sqrt3}{2}$?,"If you have a unit circle and the Pythagorean theorem, how do you discover that $\sin(\frac{\pi}{3})=\frac{\sqrt3}{2}$ ? Finding the $1, 1, \sqrt2$ triangle seems more obvious. Do you consult a chart of previously-found Pythagorean triples and scale them to a unit hypotenuse? Do you have some reason (and, if so, what?) for wanting to know the sine whose cosine $=\frac{1}{2}$ and get lucky with a neat (as long as you don't mind surds) value? Do you use lengthy trial and error (historically, over centuries)? Or is there some other pre-calculus method than Pythagoras?","['pythagorean-triples', 'algebra-precalculus', 'math-history', 'trigonometry']"
3355751,Why is belonging not transitive?,"From Halmos's Naive Set Theory , section 1: Observe, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader. Belonging seems transitive. Can someone explain?","['elementary-set-theory', 'relations', 'terminology']"
3355771,Computing the integral of the following piece wise function,"Compute the integral of $$f(t)=\begin{cases}1, \text{ if $t$ is rational}\\0,\text{ otherwise}\end{cases}$$ on $(0,1)$ . The way I approached this problem was using the upper Darboux integrals, note that $\displaystyle{\sup_{x\in(0,1)}}\{f(t)\}=1$ and $\displaystyle{\inf_{x\in(0,1)}}\{f(t)\}=0$ , thus $$\overline{\int_{0}^1}f(t)\,dt=1\quad\text{and}\quad \underline{\int_{0}^1}f(t)\,dt=0$$ thus the integral does not exist. Is this correct
?","['integration', 'real-analysis']"
3355810,Understanding the proof of fundamental group of H-space is abelian,"I'm trying to understand this part of wikipedia page on H-space : The fundamental group of an H-space is abelian. To see this, let $X$ be an H-space with identity $e$ and let $f$ and $g$ be loops at $e$ .
  Define a map $F: [0,1]Ã[0,1] â X$ by $F(a,b) = f(a)g(b).$ Then $F(a,0)$ $= F(a,1) = f(a)e$ is homotopic to $f$ , and $F(0,b) = F(1,b) = eg(b)$ is homotopic to $g$ . It is clear how to define a homotopy from $[f][g]$ to $[g][f].$ "" $F(a,0)=F(a,1)=f(a)$ is homotopic to $f$ "" -- So $F(a,0)=F(a,1)$ is homotopic to the loop $f$ ? But they're just equal to $f$ ? What exactly is homotopic to $f$ ? Same question with "" $F(0,b)=F(1,b)=g(b)$ is homotopic to $g$ "". And what's the homotopy from $[f][g]$ to $[g][f]$ ? Can any one flesh out the details?
(If anyone could write the explicit map, that would really help. Thanks.)","['general-topology', 'algebraic-topology']"
3355823,Correct use of the Substitution Rule (or Integration by Substitution),"When using the Substitution Rule (or Integration by Substitution), Stewart ( Calculus: Early Transcendentals , 8e, 2016, p. 413) writes $$\int\sqrt{1+x^2}2x\,dx\overset{1}{=}\int\sqrt{u}\,du.$$ But strictly speaking, is $\overset{1}{=}$ correct? I thought $u$ was just a dummy variable that can be replaced by any other letter, e.g. $x$ . In which case, $$\int\sqrt{u}\,du=\int\sqrt{x}\,dx=\frac{2}{3}x^{3/2}+C,$$ which is clearly not equal to $\int\sqrt{1+x^2}2x\,dx$ . And if $\overset{1}{=}$ is wrong, then what is the correct way to use the Substitution Rule?","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
3355836,Arctangent sum $\sum_{n=1}^{\infty}\arctan\frac{1}{8n^{2}}$,"Prove the following by considering a telescoping series $$\sum_{n=1}^{\infty}\arctan\frac{1}{8n^{2}} = \frac{\pi}{4} - \arctan \left(\tanh\frac{\pi}{4}\right)$$ I recognise this question has been asked here Evaluate $\sum_{n=1}^\infty \arctan\left(\frac{1}{8n^2} \right)$ however , I would specifically like an answer using telescoping given that this question is in the telescoping chapter of Jack D'aurizio's Superior Mathematics from an Elementary Point of View . 
I have tried writing the sum as $$\sum_{n=1}^{\infty}\arctan\left(\frac{1}{4n-1}\right) - \arctan\left(\frac{1}{4n+1}\right)$$ but this doesn't telescope and I have no idea how $e$ , let alone $\tanh$ is going to appear. The RHS can also be rewritten as $\arctan(e^{-\pi/2})$ , but this has not been useful to me. Could someone provide some hints or an outline of a solution using telescoping?","['trigonometric-series', 'sequences-and-series']"
3355848,"Dummy variables in integration: Is $\int x^2\,dx=\int y^2\,dy$?","My understanding was that when writing out indefinite integrals, the variable we use (customarily $x$ ) is merely a dummy variable that can be replaced by any other letter. So for example, we may write $$\int x^2 \,dx=\int y^2 \,dy=\int z^2 \,dz=\int u^2 \,du.$$ However, I posted this other question and was told that I am mistaken. Am I? And if I am mistaken, then what precisely does the expression $\int x^2\,dx$ mean? How are we supposed to know when $\int x^2\,dx$ is not the same as $\int y^2\,dy$ ? My understanding was that Given the mapping $x\mapsto x^2$ , the expression $\int x^2\,dx$ refers to that mapping's antiderivative (or antiderivatives or set of antiderivatives). $x$ is merely a dummy variable. The choice of the letter $x$ is merely customary and we can replace $x$ with any other letter. What (if anything) is wrong with my understanding?","['integration', 'calculus', 'notation']"
3355859,Prove that a convergent real sequence always has a smallest or a largest term,"My attempt: Suppose not, i.e., suppose there exists a convergent sequence $(a_n)$ that does not have a smallest or largest term. $\implies (a_n)$ is bounded sequence. $\implies A=\{a_n:n\in\mathbb{Z}^+\}$ is a bounded subset of $\mathbb{R}$ . $\implies \sup{A},\inf{A}$ exists in $\mathbb{R}$ . Now, we need to show that at least one of $\sup{A},\inf{A}\in A$ . Equivalently, we need to show that the following case is impposible: ""Both $\sup{A}\notin A$ and $\inf{A}\notin A$ "". I don't know how to proceed or if I am working it out correctly.",['sequences-and-series']
3355901,"In an ultrametric space, is every open set closed?","I saw the following well-known fact for ultrametric spaces Every open ball is closed. So this stimulates me to think whether this is true for open set or not. By an ultramtric space, it's a metric space $(M,d)$ whose metric satisfies the following condition (stronger than triangle inequality): $$
d(x,z) \leqslant \max \{ d(x,y), d(y,z)\}, \;\; \forall \; x,y,z \in M.
$$ My attempt: After I try to prove this statement is true by contradiction argument, I realized there is always a gap. So I believe this is false now. But I can't still find a counterexample. I also try to google some key words, but things I can find out are for open balls . I don't see any discussion for my problem.","['general-topology', 'metric-spaces']"
3355953,Find the number of real solutions of the equation $x^3+1=2(2x-1)^{\frac{1}{3}}$,"Question: Find the number of real solutions of the following equation: $$x^3+1=2(2x-1)^{\frac{1}{3}}$$ My Approach: Since the question was under the topic ""Inverse Functions"", I just tried to relate this problem to the mentioned topic. Luckily, I found $\frac{x^3+1}{2}$ and $(2x-1)^{\frac{1}{3}}$ are inverses of each other, by using the general method of expressing $x$ in terms of $y$ for either L.H.S. or R.H.S. We know a function and its inverse are mirror images about the line $y=x$ . So I thought, the function and it's inverse if at all they meet or intersect, they must do so on the line $y=x$ . So, I solved $\frac{x^3+1}{2}=x$ and obtained the values of $x$ as follows: $$x=1, \frac{-1+\sqrt5}{2},\frac{-1-\sqrt5}{2}$$ So there are three values of $x$ that satisfy the obtained equation and so the answer for the above question, i.e., number of real solutions is three. And the answer is also correct according to the book. Then, I thought of functions like $y=\frac{1}{x}$ which intersect with their inverses at points other than on the line $y=x$ . So the above method I used is not rigorous even though it gave the correct answer here. Kindly give me an alternate solution for this problem and tell me whether I am allowed to use the method described above.","['alternative-proof', 'functions', 'inverse', 'inverse-function']"
3356012,Prove that $\lim\limits_{\lambda \to\infty}\frac{f(\lambda)}{\lambda/\log_e\lambda}=1$,"For any real number $\lambda>1$ , let $f(\lambda)$ denote the solution to the equation $$x(1+\log_ex)=\lambda.$$ Prove that $$\lim\limits_{\lambda \to\infty}\frac{f(\lambda)}{\lambda/\log_e\lambda}=1.$$ My Attempt Let $g(x)=x(1+\log_ex)$ . Then $g'(x)=2+\log_ex$ and $g''(x)=\frac{1}{x}$ . Now, $$g\left(\frac{1}{e^2}\right)=-\frac{1}{e^2}$$ is the minimum value of $g(x)$ . But after this, I am not able to get ahead.","['limits', 'calculus', 'real-analysis']"
3356042,Is this a correct application of Stokes' theorem?,"The problem: Compute the line integral $$\int_{\gamma} F \,\mathrm d r$$ where $F$ is a vector field $$F(x, y, z) = \left( y^2+z^4+x, 2xy-z^2+x, x^{18}y^6+z^{2016} \right)$$ and $\gamma$ is the bound of the intersection $T_1 \cap T_2$ where $$T_1 = \big\{(x, y, z) \in \mathbb{R}^3 \mid x^2+y^2\leq16 \big\}$$ and $$T_2 = \big\{(x, y, z)\in \mathbb{R}^3 \mid z=4, 0\leq y \leq 3 \big\}$$ The orientation of the curve is clockwise if looked at from the point $(0, 0, 100)$ . Since the given vector field seems highly complicated I immediately assumed that Stokes' theorem should be used. So the curve that we're integrating over is the circle $x^2+y^2\leq16$ on the height of $z=4$ where $0\leq y \leq 3$ . If looked at from the top it looks something like this: Now the surface that this encloses can be parametrized as: $$S: r(x, y)=(x, y, 4), \text{where } (x, y)\in D=\big\{(x, y)\in \mathbb{R^2}|x^2+y^2\leq16, 0 \leq y \leq 3 \big\}$$ The normal vector of this surface is $r'_x \times r'_y=(0, 0, 1)$ . Since, after applying Stokes' theorem, I'm going to be doing a scalar product of $(\nabla \times F)(r(x, y)) \cdot (r'_x \times r'_y) = (\nabla \times F)(r(x, y)) \cdot (0, 0, 1)$ , I've concluded that I don't care about the first two components of $\nabla \times F$ , therefore I've computed it as: $$\nabla \times F=(P, Q, \frac{d}{dx}(2xy-z^2+x) - \frac{d}{dy}(y^2+z^4+x)) = (P, Q, 2y+1-2y) = (P, Q, 1) = G$$ By the way, the $P$ and $Q$ are not meant to represent the original components of the vector field $F$ but rather just placeholders. And since Stokes' theorem says that the curve that encloses the surface should be oriented such that, when traversing the curve, the surface remains ""on the left side"", we'll add a minus sign, getting: $$\int_{\gamma}Fdr = - \iint_S{(\nabla \times F)dS} = -\iint_S{GdS} = -\iint_D{G(r(x, y)) \cdot (0, 0, 1)dxdy} = -\iint_D{(P(r(x, y), Q(r(x, y), 1) \cdot (0, 0, 1)}dxdy = -\iint_Ddxdy = - A(D)$$ Computing the area of the region $D$ gives some really uncomfortable results. I've done it by subtracting the area of the little 'cap' on the top (the part that gets cut off by the line $y=3$ ) from the area of the half-circle. Furthermore I've gotten the area of the cap by subtracting the area of the triangle formed by points $(0, 0), (\sqrt{7}, 3), (-\sqrt{7}, 3)$ from the area of the ""pizza-slice"" (don't know the English word for it) enclosed by the circle and lines $y=\frac{\pm \sqrt{7}}{3}$ . All that process got me to a really ugly solution: $$= 8\pi\bigg(1-\pi+2\arctan\bigg(\frac{3}{\sqrt7}\bigg)\bigg) + 3\sqrt7$$ So the final solution should be $-1$ times that. This is the first problem in which I've actually applied Stokes' theorem and you can see why I'm being skeptical about whether I've done it correctly. So, does my process seem correct? (also concerned about the orientation part aside from the very complicated result). Thanks.","['integration', 'surface-integrals', 'multivariable-calculus', 'stokes-theorem', 'line-integrals']"
3356089,Metric on finite sets of natural numbers,"I was reading this question and I had a go at proving that $d^J$ is a metric. That is, if $X,Y\subset\mathbb{N}$ are finite, define $$d^J(X,Y)=1-\frac{|X\cap Y|}{|X|+|Y|-|X\cap Y|}=1-\frac{|X\cap Y|}{|X\cup Y|}.$$ I got stuck proving the triangle inequality. If $X,Y,Z\subset\mathbb{N}$ are finite, we must show $$d(X,Y)+d(Y,Z)=2-\frac{|X\cap Y|}{|X\cup Y|}-\frac{|Y\cap Z|}{|Y\cup Z|}\geq 1-\frac{|X\cap Z|}{|X\cup Z}=d(X,Z),$$ or $$1+\frac{|X\cap Z|}{|X\cup Z|}\geq \frac{|X\cap Y||Y\cup Z|+|X\cup Y||Y\cap Z|}{|X\cup Y||Y\cup Z|}$$ or $$|X\cup Y||Y\cup Z||X\cup Z|+|X\cap Z||X\cup Y||Y\cup Z|\geq |X\cap Y||Y\cup Z||X\cup Z|+|X\cup Y||Y\cap Z||X\cup Z|,$$ which doesn't seem easy to prove. I then tried writing $|X\cup Y|=|X|+|Y|-|X\cap Y|$ etc., but this produces something even more complicated. How can I prove the triangle inequality for this metric?","['elementary-set-theory', 'metric-spaces']"
3356142,Reordering of matrix multiplication,"I want to compute the matrix multiplication $ABA$ , where $A$ and $B$ are real and orthogonal matrices. In fact, they are specifically $3\times3$ rotation matrices.  However, it is much easier if I can reverse the order of $BA$ somehow, because I can then perform the multiplication much easier. I know that matrix multiplication is not commutative, however, I am asking because both $A$ and $B$ are orthogonal matrices, and hopefully, may be there is some trick to utilize their orthogonality to reorder the product. I tried to solve this, but got stuck here: $$
ABA = A((BA)^{-1})^{-1}=A(A^{-1}B^{-1})^{-1}
$$ Is there a way to proceed from here? Edit: I also know that matrix multiplication is associative; however, I am not after associativity here. I want to multiply the $A$ matrix by $A$ (or by its inverse/transpose), then multiply the result by $B$ . Edit: To put this into context, consider the following product of rotation matrices $$
R_x(\theta)R_z(\pi)R_x(\theta)
$$ where, $R_x(\theta)$ is the rotation matrix about the $x$ -axis by $\theta$ , and $R_z(\pi)$ is the rotation matrix about the $z$ -axis by $\pi$ . This product simplifies to $R_z(\pi)$ . Is it possible to come to this conclusion without carrying out the matrix multiplication of the three rotation matrices? It looks that the $R_x(\theta)$ got cancelled somehow.","['matrices', 'orthogonal-matrices', 'linear-algebra']"
3356162,Inequality and $\ell^2$-norm,"Let $a_1\geq a_2\geq \ldots \geq a_n$ and $b_1\geq b_2 \geq \ldots \geq b_n$ be two sequences with $\sum_{i=1}^n a_i=0$ and $\sum_{i=1}^n b_i=0.$ I want to prove if there exists $1\leq j\leq n$ such that $(a_j-b_j)^2>m$ , then $$\sum_{i=1}^n (a_i-b_i)^2>2m.$$ I assumed that $\sum_{i=1}^n (a_i-b_i)^2\leq 2m$ and tried to use their property to get the result but it didn't work. Any kind of suggestion is appreciated.
 Is there any books about inequalities like these?
Thanks to everyone for the help.","['analysis', 'real-analysis', 'calculus', 'cauchy-schwarz-inequality', 'inequality']"
3356207,Show that $\underbrace{1\ldots1}_{81\text{ times}}$ is divisible by $81$ but not by $243$?,"Question How do I show that $$\underbrace{1\ldots1}_{81\text{ times}}$$ is divisible by $81$ but not by $243$ ? My Attempt Write $$\underbrace{1\ldots1}_{81\text{ times}}=\underbrace{111,111,111}_{9\text{ times}}$$ and note that $111,111,111=9 \times 12,345,679$ . This also shows that $9 \mid 111,111,111$ . Now, the sum of digits of $$\underbrace{12,345,679}_{9\text{ times}}$$ is $37 \times 9$ which is clearly divisible by $9$ , thereby showing that $9 \mid 12,345,679$ . This in turn shows that $$81 \mid \underbrace{111,111,111}_{9\text{ times}}$$ Next, note that $$111,111,111=243 \times 457247.37\overline{037}=243 \times \left(457247+\frac{370}{999}\right)$$ As a result, we have $$\underbrace{111,111,111}_{9\text{ times}}=243 \times \left(457247+\frac{370}{999}\right) \times \left(1+10^{9}+\cdots+10^{72}\right)$$ To show that $$243 \nmid \underbrace{111,111,111}_{9\text{ times}}$$ we need only show that $999 \nmid (1+10^9+\cdots+10^{72})$ . Indeed, as $10^3 \equiv 1 \pmod {999}$ , so $10^9=(10^3)^3 \equiv 1^3 \equiv 1 \pmod {999}$ and in general $10^{9a} \equiv 1 \pmod {999}$ for $a \in \mathbb{N}$ . Since $$(1+10^9+\cdots+10^{72}) \equiv \underbrace{1+\cdots+1}_{9\text{ times}} =9 \pmod {999}$$ so $999 \nmid (1+10^9+\cdots+10^{72})$ and we get the desired result. Doubt Is the above solution correct? If it is correct, is there a shorter and more elegant solution?","['number-theory', 'divisibility']"
3356258,Divisibility and number theory in terms of a and b,"Are there infinitely many pairs of $(a, b)$ of relatively prime integers $a > 1$ and $b > 1$ such that $a^b+b^a$ is divisible by $a+b$ ? I've spent almost two hours on this question to no avail. The small cases I have tried, such as $(3, 5)$ , $(3, 7)$ and $(5, 7)$ all work, so I'm conjecturing that the answer is yes. However, all methods of proof I have tried have failed. Tried modulo arguments but can't really simplify my results. Any help would be greatly appreciated.","['number-theory', 'coprime', 'divisibility']"
3356266,Reading the universal quantifier as 'any' versus 'every'?,"Let $E(x,y)$ denote $x$ eats $y.$ We want to express: Tom never eats only one thing. So: $$âx\;\bigg(E(\text{Tom},x) ââz\;\Big(E(\text{Tom},z) \,â§\, z â  x\Big)\bigg).$$ If I translate it as ""If Tom eats ANYTHING, then Tom eats something else too"", then this sentence makes sense to me. But if I translate it as ""If Tom eats EVERYTHING, then Tom eats something else too"", then this sentence differs in meaning. I'm confused by how to make the distinction between reading the universal quantifier as ""anything"" versus as ""everything"".","['quantifiers', 'predicate-logic', 'discrete-mathematics', 'logic-translation']"
3356288,"If $f:S_1\rightarrow S_2, C^{\infty}$ is invertible and $f^{-1}$ is differentiable, so $f^{-1}$ is $C^{\infty}$","I am trying to follow some notes of classes and the professor wrote. It is a result for differentiable applications between surfaces. If $f:S_1\rightarrow S_2, C^{\infty}$ is invertible and $f^{-1}$ is differentiable, so $f^{-1}$ is $C^{\infty}$ . Proof: If $g=f^{-1}$ , so $g ~\circ~f=I $ and $dg_{f(p)}\circ df_p=I$ . Then, $dg_{f(p)}=(df_p)^{-1}=Inv~\circ df_p$ . Therefore, $g$ is $C^{\infty}$ . I could follow the steps, but do not get how obtain the conclusion ""therefore..."". Many thanks.","['derivatives', 'differential-geometry']"
3356298,One type of Optional Sampling Theorem,"I know the proof of one kind Optional Sampling Theorem: Let $X=\left\{ X_{n}\right\} _{n\in\mathbb{N}}$ be a $\left\{ \mathscr{F}_{n}\right\} _{n\in\mathbb{N}}$ -submartingale with a last element $X_{\infty}$ ,(i.e. $X_{n}\rightarrow X_{\infty}$ a.e. and $\left\{ X_{n}:n=1,2,\cdots,\infty\right\}$ is $\left\{ \mathscr{F}_{n}:n=1,2,\cdots,\infty\right\}$ -submartingale). Then, for any stopping time $S,T$ with $S\le T$ (not necessarily finite) , we have $E\left(X_{T}|\mathscr{F}_{S}\right)\geq X_{S}$ a.e. What if I replace the condition of ""existence of a last element""  with ""there exists an integrable random variable $Y$ , such that $X_{n}\le E\left(Y|\mathscr{F}_{n}\right)$ a.s., for every $n\in\mathbb{N}$ ""? I think the Optional Sampling Theorem still holds but I do not know how to prove it. Can anyone give me a hint or show me the book where I can find the proof of this statement? Many thanks in advance. In fact, this problem is closely related to problem 1.3.23 in Karatzas and Steven E. Shreve, Brownian Motion and Stochastic Calculus. This is Theorem 1.3.22: and this is problem 1.3.23: I think the question I ask is a crucial part of the solution.","['stochastic-processes', 'probability-theory', 'martingales']"
3356332,Why any countable subset of $\mathbb{R}â\mathbb{R}$ is generated by a finite set under composition?,"Given a sequence of functions $\{g_k\}$ , where $g_k: \Bbb R\to\Bbb R$ for all $n\in \mathbb N$ . Prove that there exists a finite set of functions $$f_1,f_2,\ldots,f_n$$ such that any function $g_k$ can be expressed as a composition $$f_{k_1}\circ f_{k_2}\circ\cdots\circ f_{k_m}.$$","['elementary-set-theory', 'finitely-generated', 'function-and-relation-composition']"
3356335,"About subdivision in homology (demonstration in Lee Introduction to Topological Manifolds, proposition 13.19)","My question has to do with the following assetion given in J.M. Lee: Introduction to Topological Manifolds, page 364 , in the context of demonstrating there exists a chain homotopy $h$ between the subdivision operator $s$ and $\mathcal{i}_p$ , the identity in $\Delta_p$ , defined by $$h\sigma=\sigma_\#b_p*(\mathcal{i_p}-s\mathcal{i}_p-h\partial s\mathcal{i_p})$$ Observe also that if $\sigma$ is a $\mathcal{U}$ -small simplex, then $h\sigma$ is a $\mathcal{U}$ -small chain, so $h$ maps $C_p^\mathcal{U}(X)$ to $C_{p+1}^\mathcal{U}(X)$ How can we actually be sure that is the case? I searched for an answer in Glen E. Bredon: Topology and Geometry , but I found nothing. It would be interesting if anyone could give more insight to what $h$ actually does $-$ geometrically speaking $-$ , since the definition by recurrence doesn't offer any intuition at all Thanks in advance for your help.","['homological-algebra', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3356353,Cartesian product of finite sets - check my proof,"Let $A$ und $B$ be non-empty finite sets. Then the Cartesian product is finite and it holds $$ \vert A \times B\vert = \vert A \vert \cdot \vert B \vert .$$ My proof: As $A$ und $B$ are finite we have two bijections $f: A \to [m]$ and $g:B\to [n]$ , where $[m], [n]$ are the intervalls $[1, ..., m]$ and $[1, ..., n]$ in $\mathbb{N}$ , respectively. I define the following function $h: A\times B \to [m\cdot n]$ , $$h(a,b) = f(a)+(g(b)-1)m$$ The function $h$ is injective because it's simply a composition of injective functions (note that $m>0$ ). Let $y \in [m\cdot n]$ then there exists a $k \in [n]$ such that $(k-1)m < y \leq km$ . Also, $y-(k-1)m \in [m]$ . So there exist two elements $a \in A $ with $f(a) = y-(k-1)m$ and $b \in B$ with $g(b) = k$ . Pluggin them into $h$ yields: $h(a,b) = f(a)+(g(b)-1)m=y-(k-1)m+m(k-1)=y$ . As $y$ was arbitrarily chosen, every element of $[m\cdot n]$ has a preimage. Hence, $h$ is bijective so that $A \times B$ is finite and has a cardinality of $\vert A \times B\vert =mn=\vert A \vert \cdot \vert B \vert$ . Is this correct? Or would you do it differently?","['elementary-set-theory', 'proof-verification']"
3356386,Geometry - Prove a right triangle resulting from three inscribed circles,"Two half circles and a full circle fit inside a larger quarter circle as shown in the diagram. The centers of the two half circles are on the two sides of the quarter circle, respectively. Prove that the triangle formed by the centers of three smaller circles, $\triangle O_1O_2O_3$ , is a right triangle. I was able to apply the Pythagorean formula to a few triangles involving the radii of the inscribed circles and proved that the ratios of the three radii are 1:2:3. Then, the centers of the three circles form a triangle with side-length ratios 3:4:5, hence, a right triangle. On the other hand, I feel the proof may be an overkill, and evaluating the three radii explicitly may be unnecessary. There ought to be clean geometric solutions to show directly that the vertex $O_3$ is of a right angle, which I am not sure how to figure out.","['euclidean-geometry', 'analytic-geometry', 'circles', 'geometry', 'geometric-transformation']"
3356453,"If $E(|X_t-Y_t|^2)=0$, then $P\{\omega:|X_t(\omega)-Y_t(\omega)|=0,\forall t\in [0,T]\}=1$?","Let $X_t, Y_t$ be stochastic processes with almost sure continuous paths defined on some probability space. Define $$v(t):=E(|X_t-Y_t|^2)$$ for $0\leq t\leq T$ . Clearly $v(t)$ is continuous on $[0,T]$ . Now suppose $v(t)=0$ for all $t\in [0,T]$ . From this can we conclude that $$P\{\omega:|X_t(\omega)-Y_t(\omega)|=0,\forall t\in [0,T]\}=1$$ I am wondering how this can be proved. Any help is appreciated!","['stochastic-processes', 'probability-theory', 'probability', 'stochastic-calculus']"
3356461,circle with two tangent lines - how do I show that AN=MF?,"hello. given that AD=DE=EF and that AB,BF  are Tangent Line to the circle,
how do I show that AN=MF ? I thought about connecting DM , NE ,NM and to show that triangle BND is similar to ABF , using that BN=BM (because AB and BF  are Tangent Line to the circle) , but then I am stuck.
any help please?","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
3356474,Average queue length under FCFS and LCFS simultaneously,"Suppose customers join a queue with a poisson arrival rate ð. If a customer is not served within a unit of time, she abandons the queue. Customers are served by two servers: one of the servers runs the first-come-first-served (FCFS) policy and the other one the last-come-first-served (LCFS) policy. The FCFS server has a service time that is iid exponentially with mean $\lambda m$ , where $\lambda<1$ . The LCFS server has a service time that is iid exponentially with mean $\delta m$ , where $\delta<1-\lambda$ .  A customer departs the queue after being served by either of the servers. I would like to show that the average length of the queue is at least $(1-\delta)m -o(m)$ . Any input will be appreciated! P.S. A special case of this problem when $\delta=0$ has been solved here Average queue length with impatient customers while the intuition seems similar, I have not been able to adapt this approach.","['stochastic-processes', 'poisson-process', 'queueing-theory', 'probability-theory', 'probability']"
3356476,Derivative of polynomial all those roots are not simple(multiplicity > 1),"Let $\operatorname{char}\mathbb K = 0$ . Consider the polynomial $$q(x) := (x - a_1)^{n_1}...(x - a_m)^{n_m} \in \mathbb K[x]$$ and the polynomial $$Q(x) := q(x)(x - a_1)...(x - a_m) \in \mathbb K[x],$$ where $n_i \in \mathbb N \setminus\{0\}$ and $a_i \neq a_j, i \neq j$ .
Now consider $$f(x) := \frac{Q^{'}}{q} \in \mathbb K[x].$$ Is it true that all roots of $f$ are simple(of multiplicity 1)? I can show that this is true for $m \leq 3$ . I'm doing this by directly writing down $f(x)$ and calculating it's discriminant. Any help is welcome","['derivatives', 'polynomials']"
3356495,"Can an infinite series be thought of as adding up ""infinitely many"" terms?","Formally, I understand that infinite series are not defined by adding up ""infinitely many"" terms, but are instead defined as equalling their limit. As user Brian M. Scott outlined in an answer to a similar question ( Why is an infinite series not considered an infinite sum of terms? ), it is easier to define infinite series by the limit of their partial sums than by considering every single term. However, one geometric proof of the convergence of $1/2 + 1/4 +1/8+1/16+ \cdots$ has made me question why we circumvent the problem of adding infinitely many terms with the concept of the limit: Image credit: https://www.mathsisfun.com/algebra/infinite-series.html When you look at the diagram above, it seems like every single term has been included (not literally, but it is clear what the diagram represents). Furthermore, if you plotted the above shape on the Cartesian plane, the area of the shape would be $1$ . Even the point $(0.99999,0.99999)$ would be covered by a square/rectangle. When all of the terms have been plotted, it seems like the infinite series not only tends to 1, it equals $1$ . To me, this is not just because we define an infinite series to equal its limit. The limit only concerns the partial sums, whereas the diagram shows that even if we allow ourselves to add infinitely many terms, there is no immediate contradiction. Obviously, defining infinite series like this formally can create problems: infinite series do not have always have the commutative property, for example. However, is there anything conceptually wrong with thinking of infinite series as adding up infinitely many terms, even though technically this can lead to some problems?","['definition', 'intuition', 'sequences-and-series']"
3356536,expanding a factorial by simplifying with $(k-1)!$,"I'm wandering how $\frac{(k+2)!}{(k-1)!} = \frac{(k+2)(k+1)(k)(k-1)!}{(k-1)!}$ . I'm confused about how $(k-1)!$ can be inserted into the numerator here. I know that this eventually works out to $k^3+3k+2k$ , but why would $(k-1)!$ be justified here from the numerator $(k+2)!$ ? Thank you","['combinations', 'factorial', 'combinatorial-proofs', 'discrete-mathematics']"
3356544,Is $(-1)^{2.16}$ a real number?,"A lot of calculators actually agree with me saying that it is defined and the result equals 1, which makes sense to me because: $$ (-1)^{2.16} = (-1)^2 \cdot (-1)^{0.16} = (-1)^2\cdot\sqrt[100]{(-1)^{16}}\\ 
= (-1)^2 \cdot \sqrt[100]{1} = (-1)^2 \cdot 1 = 1$$ However, there are certain calculators (WolframAlpha among them) which contest this answer, and instead claim it is equal to: Graphing this as an exponential function was not possible. What's going on?","['complex-analysis', 'real-analysis']"
3356579,"Defining a functor $\mathscr F:[\mathscr A,\mathscr B]^{op}\to[\mathscr A^{op},\mathscr B^{op}]$","First attempt (disregard it and go to the second): I'm trying to prove that $[\mathscr A,\mathscr B]^{op}\cong [\mathscr A^{op},\mathscr B^{op}]$ . I started with defining a functor $\mathscr F:[\mathscr A,\mathscr B]^{op}\to[\mathscr A^{op},\mathscr B^{op}]$ as follows. On objects: $(F^{op}:\mathscr B\to \mathscr A)\mapsto (\mathscr F(F^{op}):\mathscr A^{op}\to\mathscr B^{op})$ where the functor $F(F^{op}):\mathscr A^{op}\to\mathscr B^{op}$ sends an object $A$ to $F(A)$ and a morphism $(f^{op}:A'\to A)$ to $(F^{op}(f^{op})=F(f)^{op}:F(A')\to F(A))$ . On morphisms: not sure. Given a natural transformation $\alpha=(\alpha_B:F^{op}(B)\to G^{op}(B))_{B\in\mathscr B}$ between $F$ and $G$ , we need to define a natural transformation $\mathscr F(\alpha)=(\mathscr F(\alpha)_A:\mathscr F(F^{op})(A)\to \mathscr F(G^{op})(A))_{A\in\mathscr A^{op}}=(\mathscr F(\alpha)_A:F(A)\to G(A))_{A\in\mathscr A}$ . I don't quite see how to define this in terms of $\alpha_B$ . Each $\alpha_B$ is an arrow in $\mathscr A$ , but we need $\mathscr F(\alpha)_A$ to be an arrow in $\mathscr B$ . I thought about taking $\mathscr F(\alpha)_A$ to be $F(\alpha_A)$ , but $\alpha_A$ (for $A\in \mathscr A$ ) doesn't make sense. Another attempt: Define $\mathscr F:[\mathscr A,\mathscr B]^{op}\to[\mathscr A^{op},\mathscr B^{op}]$ as follows. On objects: given a functor $F:\mathscr A\to \mathscr B$ , assign to it the functor $F^{op}:\mathscr A^{op}\to\mathscr B^{op}$ . (This $F^{op}$ is defined on objects by $F^{op}(A)=F(A)$ and on morphisms by $F^{op}(f^{op}:A'\to A)=F(f)^{op}$ .) Now take a morphism in $[\mathscr A,\mathscr B]^{op}$ . It is a natural transformation $\alpha^{op}: G\to F$ (where $F,G:\mathscr A\to\mathscr B$ are functors). The naturality of $\alpha^{op}$ says this: for any arrow $g:A\to A'$ in $\mathscr A$ , we have $$\alpha_{A'}^{op}\circ G(g)=F(g)\circ \alpha_A^{op}.$$ We need to assign to $\alpha^{op}$ a natural transformation $\mathscr F(\alpha^{op}): G^{op}\to F^{op}$ . So for each $A\in\mathscr A$ we need to define $F(\alpha^{op})_A=:\beta_A$ . The following naturality condition must hold: for any arrow $f^{op}:A'\to A$ in $\mathscr A^{op}$ , $$F(f)\circ_{op} \beta_{A'}=\beta_A\circ_{op} G(f)^{op}.$$ I was thinking about $\beta=(\alpha^{op})^{op}=\alpha$ . But $\alpha_A$ is an arrow $F(A)\to G(A)$ , whereas $\beta_A$ has to be an arrow $G(A)\to F(A)$ . And $\beta_A=\alpha_A^{op}$ doesn't work either because if you substitute $\alpha$ for $\beta$ in the naturality condition (second display), it doesn't check out.","['functions', 'category-theory']"
3356644,Periodic functions whose sum is null,"If $f_1,\ldots,f_n:\mathbb{R}\rightarrow\mathbb{R}$ are periodic functions such that $$ \lim\limits_{x\rightarrow +\infty}{(f_1(x)+\ldots+f_n(x))}=0 $$ how can I prove that $f_1+\ldots+f_n=0$ ?","['periodic-functions', 'functions', 'real-analysis']"
3356645,Counting the number of subsets with a given property,"Let $S = \{1, 2, 3, \ldots, 12\}$ be a set. How many subsets $S' \subseteq S$ satisfy the property that for any $x, y\in S'$ , $|x - y| > 2$ ? This is a problem from a math contest, but I'm not so sure how to figure it out. I tried splitting it into casework on the sizes. For example, I think $|S'| = 4$ is the maximal sized subset but there are even a lot of possible subsets for that size. For $|S'| = 1$ , there are $12$ possible subsets (the condition is vacuously true). I'm sure there's probably some really clever way to count these subsets, and I am seeking help to find it. Thanks Following lulu's approach (in comments): $a_1 = 2$ (empty set and the singleton) $a_2 = 3$ (two possible singletons and the empty set) $a_3 = 4$ (three possible singletons and the empty set) Now using the recurrence $a_n = a_{n - 1} + a_{n - 3}$ , we get $a_4 = 6$ , $a_5 = 9$ , $a_6 = 13$ , $a_7 = 19$ , $a_8 = 28$ , $a_9 = 41, a_{10} = 60, a_{11} = 88, a_{12} = 129$ . So the answer is $129$ . Is this right?","['combinations', 'combinatorics']"
3356666,How can one make a periodic Markov chain aperiodic with the smallest change in the main structure of the chain?,"Or, how can one ""deform"" a stochastic, irreducible, periodic matrix into a stochastic, irreducible, aperiodic matrix with the smallest change? If there are several possible procedures, then please mention them. If the procedure is complex, then please provide sufficient explanation or link of blogs/books etc. I want to analyze them and check which makes the smallest change in the original graph/chain/matrix.","['markov-chains', 'matrices', 'linear-algebra', 'stochastic-matrices', 'probability']"
3356703,Finding an upper bound to the order of finite subgroup of the automorphism group of rational map,"Let $\phi: \Bbb P^1 \to \Bbb P^1$ be a rational map then we define $Aut(\phi)=\{f \in PGL_2(\Bbb C): f^{-1}\phi f(z)=\phi(z)\}$ Here, in general, the definition of a rational map is: Let $\mathbb{P}^n$ and $\mathbb{P}^m$ be projective spaces. If $m$ homogeneous polynomials in $n+1$ variables of the same degree ""d"" give a partially defined map from $\mathbb{P}^n$ to $\mathbb{P}^m$ then this map is called a rational map. We can exactly define this on a projective variety as well. Now for $\mathbb{P}^1$ we can prove that: Let $\phi: \Bbb P^1 \to \Bbb P^1$ be a rational map of degree $d \geq 2$ . Then $Aut(\phi)$ is a finite subgroup of $ PGL_2(\Bbb C)$ and its order is bounded by a function of $d$ . This proof can be done by using the fact that $f\in PGL_2(\Bbb C)$ is basically a Mobius transformation and once it fixes three points it is constant and once we know this result we can manipulate the periodic points of $\phi$ to have an upper bound.
If you ask for details I can fill in the gaps. The problem arises if we go to higher dimension say $\mathbb{P}^2$ or $\mathbb{P}^n, n\geq 2$ . Then we will have at least 3 homogeneous polynomials in at least $3$ variables of the same degree. Can we extend this result and have some bounds? Any hint or result would be much appreciated. Even if you give some sequential hints I can try to fill the gaps and reach out for help in the comments.","['automorphism-group', 'complex-dynamics', 'arithmetic-dynamics', 'algebraic-geometry', 'dynamical-systems']"
3356735,Divergence of $\prod_{n=1}^{\infty} a\sin(n)$ for $a>1$ to $0$ or $\infty$,"Consider the product $$\displaystyle\prod_{n=1}^{\infty} a\sin(n).$$ When $a=1$ , clearly this product diverges to $0$ , as $|{\sin{(n)}}|\le1$ , so the value of the partial products can only decrease or stay the same (and they will only decrease if $n\in \mathbb{N}$ ). However, the divergence to 0 or infinity of this product becomes much more complex if $a>1$ . The question of divergence to $0$ is certainly not intuitive, as when $\sin(n)\approx0,$ the partial product will drop significantly, but when $|\sin(n)|>{1\over{a}}$ the partial product will grow (and this happens frequently for large enough $a$ ). What seems to happen is that, for $a\le2$ the partial sums chaotically grow, before eventually divergine to $0$ . For example, $\displaystyle\prod_{n=1}^{307} 2\sin(n)\approx1402$ , but $\displaystyle\prod_{n=1}^{5000} 2\sin(n)\approx 4.8\times10^{-13}$ . When I ask Mathematica about the infinite product when, it rather quickly claims that the product does not converge for any $a$ (I do not know whether this also means it does not diverge to 0). Numerical evidence seems to suggest divergence to $0$ otherwise for $a\le2$ . For $a>2$ , it seems to fairly quickly diverge to infinity. $a=2$ is probably the most interesting case, as approximately $50$ % of the time it should grow, since $0<|\sin(n)|<1$ and the partial product grows iff $|\sin(n)|>1/2$ . Is it known whether this product diverges to $0$ for even one example of $a$ where $1<a\le2$ ? And if $a=2$ ? If it doesn't necessarily diverge to $0$ , does it remain bounded? 
And can divergence to infinity be proven for any $a>2$ ?","['infinite-product', 'trigonometry', 'convergence-divergence', 'sequences-and-series']"
3356746,Proof using derivative information to find limit,"This is the last exercise of a quite challenging exercises paper a friend who is taking calculus has which I'm trying to help. I already helped her doing the other bunch. But this got me. I will appreciate anyone help to see my work and to tell me if is right or If I need to correct something. The exercise is: If $f'(a)=1$ for $a>0$ , find $\lim_{x \to a} \frac{f(x)-f(a)}{\sqrt{x}-\sqrt{a}}$ . What came to my mind was to rationalize the denominator. $$\lim_{x \to a} \frac{f(x)-f(a)}{\sqrt{x}-\sqrt{a}}$$ $$=\lim_{x \to a} \frac{f(x)-f(a)}{\sqrt{x}-\sqrt{a}}\cdot \frac{\sqrt{x}+\sqrt{a}}{\sqrt{x}+\sqrt{a}}$$ $$=\lim_{x \to a} \frac{(f(x)-f(a))(\sqrt{x}+\sqrt{a})}{x-a}$$ $$=\lim_{x \to a} \left(\frac{f(x)-f(a)}{x-a}\cdot (\sqrt{x}+\sqrt{a})\right)$$ $$=\lim_{x \to a} \frac{f(x)-f(a)}{x-a}\cdot \lim_{x \to a}(\sqrt{x}+\sqrt{a})$$ $$=f'(a)\cdot \lim_{x \to a}(\sqrt{x}+\sqrt{a})$$ $$=1\cdot \lim_{x \to a}(\sqrt{x}+\sqrt{a})$$ $$=\lim_{x \to a}(\sqrt{x}+\sqrt{a})$$ $$=\sqrt{a}+\sqrt{a}$$ $$=2\sqrt{a}$$","['limits', 'calculus', 'derivatives']"
3356750,Vector in 9 dimensions always has a solution?,"I was watching Professor Gilbert Strang's lecture on Linear Algebra, and in the video, he talks about when we can or cannot say that linear combinations of vectors span the entire region in that dimension. He said that if we had nine vectors, each of dimension 9,  and if the vectors were all random, then the answer is probably yes. But if it is the case that two of the nine vectors are the same, then they don't add anything new to the equation, and what we will get is probably an 8-dimensional plane. I don't understand how this is the case. Also, could someone explain what this means in terms of the number of solutions we will get?",['linear-algebra']
3356764,Rate of mixing of two regions containing particles undergoing Brownian motion,"Suppose the plane is densely populated with particles at a density of $\delta_0$ per unit area, except for in a bounded region $\Omega$ , in which there are none. Suppose the particles move according to Brownian motion, and that they may wander into $\Omega$ through a small opening $\ell$ in the boundary of $\Omega$ . See my diagram: How does the density of $\Omega$ change over time? I know that as $t\to\infty$ , the density of $\Omega$ approaches $\delta_0$ . I suspect that the rate of convergence to $\delta_0$ is determined by the length of $\ell$ , and also by the parameters of the random motion. My question is: what is an expression for the density of $\Omega$ at a given time $t\in\Bbb R$ ?","['stochastic-processes', 'ordinary-differential-equations']"
