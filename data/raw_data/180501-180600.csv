question_id,title,body,tags
3285348,$SU(3)$ acts transitively on $S^5$.,"I'm told that $SU(n)$ acts transitively on $S^{2n-1} \subset \mathbb{C}^n$ by matrix multiplication. Yet I can't find a proof of this anywhere, so I was trying to construct a proof on my own by mimicking a version of the proof that I know for showing $SO(n)$ acts transitively on $S^{n-1}$ . My proof is outlined as follows: To show transitive, it suffices to show that the point $x= \begin{pmatrix} 
1  \\
0  \\
0 
\end{pmatrix} \in S^5 \subset \mathbb{C}^3$ can be taken to any other point $p=\begin{pmatrix} 
p_1  \\
p_2  \\
p_3
\end{pmatrix} \in S^5 \subset \mathbb{C}^3. $ A complex matrix lies in $SU(3)$ if and only if its columns form an orthonormal basis for $\mathbb{C}^3$ . If $A \in SU(3)$ has the property that $Ax=p$ , then $A$ must have $p$ as its first column. Completing $p$ to a basis for $\mathbb{C}^3$ , then running the Gram-Schmidt algorithm on this basis (starting with $p$ in the first step of the algorithm) completes $p$ to an orthonormal basis for $\mathbb{C}^3$ . Then sticking these basis elements in as the columns of $A$ , with $p$ as the first column, yields a matrix in $U(n)$ that takes $x$ to $p$ . This proof in general shows that $U(n)$ acts transitively on $S^{2n-1}$ , but the matrix I constructed does not necessarily have determinant $1$ . In the completely analogous proof for showing $SO(n)$ acts on $S^n$ transitively, the matrix has determinant $\pm1$ so if the determinant is $-1$ , we need only interchange the last two columns to get a matrix with determinant $1$ . This proof breaks down here because we only know that the determinant lies on the unit circle in $\mathbb{C}$ , that is $|\det(A)|=1$ . Is there some way to produce a matrix with determinant dead equal to $1$ with this process? Otherwise, is there another simple proof of transitivity?","['linear-groups', 'group-actions', 'lie-groups', 'differential-geometry']"
3285392,When is the closure characterization of continuity ever really useful?,"A classical exercise in elementary general topology is to show that a mapping $f\colon X\to Y$ between topological spaces is continuous if and only if for any subset $A$ of $X$ we have $$ f(\overline{A})\subset \overline{f(A)}, $$ that is the image of the closure is contained in the closure of the image. However, I cannot recall seeing this characterization being used explictly even a single time. Are there any prominent applications? In which kind of situation can one benefit from it?","['continuity', 'general-topology']"
3285396,"""Distance"" between summands in Taylor Series of Cosine","The Taylor series of cosine is $$\cos(x)=\sum_{n=0}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}=\frac{x^0}{0!}-\frac{x^2}{2!}+\frac{x^4}{4!}\mp\ ...$$ If we now plot the summands (ignoring the sign and the first summand for simplicity), one gets the following plot using GeoGebra : In this picture it seems as if the distance between the graphs is the same for the different summands. Furthermore, the distance seems to be something around $\frac{\pi}{4}$ . Is this a fact? And if so, why? I'd guess that it has something to do with the fact that we can write Pi using the Leibniz-series: $$\frac{\pi}{4}=\sum_{k=0}^\infty\frac{(-1)^k}{2k+1}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}\ ...$$ However I can't quite see the connection between those two series. It seems that this has very little to do with the cosine, but rather with monomials themselves - but why do even monomials divided by the factorial of their exponent have a distance in $x$ -direction of $\frac{\pi}{4}$ ?","['monomial-function', 'pi', 'taylor-expansion', 'sequences-and-series']"
3285424,Comparing the marks of students from different universities,"The standard deviation of first year statistics exam marks is known to be $14$ . A sample of $50$ first year statistics students from University A had a mean exam mark of $75$ , while a sample
of $36$ University B students had a sample mean of $80$ . Test at the $10$ % level of significance
whether the marks for University B are significantly better than A. I want to know if a one-sided ( $\mu_{A}<\mu_{B}$ ) hypothesis test is appropriate for this question and if the critical value $z$ is $-1.28$ (very important!) and the test statistic $Z$ is $-1.634$ .","['statistics', 'hypothesis-testing']"
3285425,Reconciling measure-theoretic definition of expectation versus expectation defined in elementary probability,"Measure theoretic definition (e.g., Durrett, Brzezniak): Given a probability space $(\Omega, \mathcal{F}, P)$ and a random
  variable $X: \Omega \to \mathbb{R}$ assumed to be integrable, $$\int_\Omega |X| dP < \infty$$ then the expectation of $X$ is
  defined by, $$E[X] = \int_\Omega X dP$$ Elementary probability definition (e.g., Sheldon Ross, Tsitsiklis), The expectation of a random variable $X$ is $$E[X] = \begin{cases} \sum\limits_{x:p(x)>0} x p(x) & X \text{ is
discrete} \\ \int\limits_{-\infty}^\infty xf(x) dx & X \text{ is
continuous} \end{cases}$$ where $p$ is the probability mass function and $f$ is the probability density function. It seems to that the $dP$ symbol somehow magically turns into $p(x)$ and $f(x)dx$ . I can't see the link here since I don't understand the symbol $dP$ . Is it a function? Is it a constant? What is the $d$ doing there (that's not a part of the definition at all)? the $\int_\Omega$ symbol turns into $\sum$ or a $\int$ over the space of $x$ on a situational basis, which is again strange to me. Can someone explain how the measure theoretic definition of the expectation turns into the ones that most (non-mathematician) students study in a course on probability?","['integration', 'lebesgue-integral', 'definition', 'probability-theory', 'random-variables']"
3285558,"Find absolute extrema of $f(x)=\begin{cases}x^2-1,&x<1,\\\ln(x),&x\geq1\end{cases}$","Find absolute extrema of $$f(x)=\begin{cases}x^2-1,&x<1,\\\ln(x),&x\geq1.\end{cases}$$ I could find the critical points by setting $f'(x)=0$ : $$f'(x)=\begin{cases}2x,&x<1,\\\dfrac{1}{x},&x\geq1\end{cases}=0\implies2x=0\implies x=0,$$ and $$f''(x)=\begin{cases}2,&x<1,\\-\dfrac{1}{x^2},&x\geq1\end{cases}\implies f''(0)=2>0,$$ therefore $x=0$ is a relative minimum of $f$ , which has value $f(0)=-1$ : How can we prove that it is also an absolute minimum? I tried the following: It is not hard to see that $x^2-1\geq-1$ for $x<1$ , and $\ln(x)\geq0$ for $x\geq1$ . Combining these, $x=-1$ is the first of the upper bounds of $f$ , so $x=-1$ is an absolute minimum. Is it correct? If not, how would you prove it? Thanks!!","['calculus', 'proof-verification', 'derivatives']"
3285589,Evaluate $\int_0^{\frac{π}{2}}\tan (x)\ln (\sin x)\ln (\cos x)dx$,Evaluate: $$I=\int_0^{\frac{π}{2}}\tan (x)\ln (\sin x)\ln (\cos x)dx$$ My ideas is to use the Fourier series of log sin and log cos: $$\ln (2\sin x)=-\sum_{k=1}^{\infty}\frac{\cos (2kx)}{k}$$ $$\ln (2\cos x)=-\sum_{k=1}^{\infty}\frac{(-1)^{k}\cos (2kx)}{k}$$ But my problem is that I find difficult integrals like: $$\int\tan (x)\cos (2kx)dx$$ My another idea is: Use the substation : $y=\tan x$ then $dx=\frac{dy}{1+y^2}$ Then where $x=0 \Rightarrow y=0$ and for $x=\frac{π}{2} \Rightarrow y=\infty$ So: $$I=\frac{1}{2}\int_0^{\infty}\frac{y\ln \left(\frac{y}{\sqrt{1+y^2}}\right)\ln (1+y^2)}{1+y^2}dy$$ But now I don't know how to complete.,"['integration', 'definite-integrals', 'closed-form']"
3285611,Evaluate this $\lim\limits_{n \to \infty}\int\limits_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}dx$,"We seeking to evaluate this integral $$\lim_{n \to \infty}\int_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx$$ using $\cos a -\cos b=-2\sin[(a+b)/2]\sin[(a-b)/2]$ $$\lim_{n \to \infty}-2\int_{1/n}^{n}\sin\left(\frac{3x}{4}\right)\sin\left(\frac{x}{4}\right)\frac{\ln x}{x}\mathrm dx$$ I stuck, not sure what to do next. $v=\int \frac{\ln x}{x}=\frac{1}{2}\ln^2 x$ $u^{'}=-\sin x+\frac{1}{2}\sin(x/2)$ $$\int \left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx=\left[\cos x-\cos(x/2)\right]\frac{\ln^2 x}{2}-\frac{1}{2}\int \left[-\sin x+\frac{1}{2}\sin(x/2)\right]\ln^2 x$$ this integral it getting more complicated due to $\ln^2 x$","['integration', 'limits', 'definite-integrals']"
3285615,Minimal polynomial of $\sqrt{2+\sqrt[3]{3}}$ over $\mathbb{Q}$,"About 2 weeks ago, I tried to solve the following problem. Find the minimal polynomial of $\alpha=\sqrt{2+\sqrt[3]{3}}$ over $\mathbb{Q}$ . My attempt First, I tried to find the polynomial with rational(integer) coefficients having $\alpha$ as a root, and $f(x)=x^6-6x^4+12x^2-11$ is a polynomial such that $f(\alpha)=0$ . Unfortunately, $6$ and $12$ is not divided by $11$ , so I could not use the Eisenstein's criterion. Instead of directly showing that $f(x)$ is irreducible over $\mathbb{Q}$ , I tried to show that $[\mathbb{Q}(\alpha):\mathbb{Q}]=6$ . Since $[\mathbb{Q}(\alpha):\mathbb{Q}]=[\mathbb{Q}(\alpha):\mathbb{Q}(\alpha^2)][\mathbb{Q}(\alpha^2):\mathbb{Q}]$ and $\alpha^2=2+\sqrt[3]{3}$ , we know that $[\mathbb{Q}(\alpha^2):\mathbb{Q}]=3$ . Thus if we succeed to show that $[\mathbb{Q}(\alpha):\mathbb{Q}(\alpha^2)]=2$ , then the proof is over. However, I could not do it. I supposed that $\alpha\in\mathbb{Q}(\alpha^2)=\mathbb{Q}(\sqrt[3]{3})$ , then there are $a,b,c\in \mathbb{Q}$ such that $$
\alpha=a+b\sqrt[3]{3}+c\sqrt[3]{9}.
$$ By squaring both sides of the equation, we obtain $$
(a^2-6bc-2)+(3c^2-2ab-1)\sqrt[3]{3}+(b^2+2ca)\sqrt[3]{9}=0
$$ and $a^2-6bc-2=3c^2-2ab-1=b^2+2ca=0$ . However, I don't know how to show that this system of equations does not have a rational root and I'm stuck here. Question: Is there a (or an alternative) way to solve the problem?","['field-theory', 'minimal-polynomials', 'abstract-algebra', 'extension-field']"
3285619,What type of connection is required for the Chern-Gauss-Bonnet Theorem?,"How general a connection on $TM$ can be used in the Chern-Gauss-Bonnet theorem?  Wikipedia only states the theorem for the Levi-Civita connection, but this is probably needlessly restrictive.  (If the theorem can be proven for the LC connection of some metric then it holds for the LC connection for any metric, and so we see already that there is some family of connections for which it holds. 
 This is also asserted in Q. Yuan's answer here ). What I understand of Chern-Weil theory suggests that the theorem should hold for any connection whatsoever on $TM$ , but perhaps I am missing something.  Chern seems to only consider Levi-Civita connections in A Simple Intrinsic Proof of the Gauss-Bonnet Formula for Closed Riemannian Manifolds .","['connections', 'vector-bundles', 'riemannian-geometry', 'differential-geometry']"
3285670,How derivative of matrix leads to its transpose?,"I'm deriving backpropagation step of training neural networks using vectorized equations. Following are two forward propagation equations between last hidden layer and output layer. $$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $$ $$ A^{[2]} = \hat{y} = g(Z^{[2]})$$ Where $g(z)$ is activation function. Now in backpropagation, we calculate the change in cost function ( $J$ ) w.r.t. all the parameters. I've successfully calculated $\partial{J}/\partial{A^{[2]}}$ and $\partial{J}/\partial{Z^{[2]}}$ using chain rule. Now to calculate $\partial{J}/\partial{W^{[2]}}$ , I've formed following chain $$ \frac{\partial{J}}{\partial{W^{[2]}}} = \frac{\partial{J}}{\partial{A^{[2]}}} \frac{\partial{A^{[2]}}}{\partial{Z^{[2]}}}  \frac{\partial{Z^{[2]}}}{\partial{W^{[2]}}}$$ Now to calculate $\frac{\partial{Z^{[2]}}}{\partial{W^{[2]}}}$ , I used $ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $ equation which simply gives the derivation $ A^{[1]} $ . But in literature , it's given as $ A^{[1]^{T}} $ , which is transpose of my answer. By checking dimensions of the answer, I could verify that the answer should be $ A^{[1]^{T}} $ instead if $ A^{[1]} $ . But is there any general rule for such cases using which I could directly tell whether the derivative will be the transpose of a matrix or not without verifying dimensions? I've also checked matrix cookbook but couldn't find any related thumb rules.","['neural-networks', 'machine-learning', 'calculus', 'partial-derivative', 'derivatives']"
3285678,Condition for a quotient map to have compact image.,"Let $q:X\to X_{/\sim}$ be a quotient map for some relation $\sim$ on $X$ . If there is a compact subspace $A\subset X$ such that every element of $X$ is in relation with some element of $A$ , then $X_{/\sim}$ is compact, simply because the condition can be rewritten $q(A)=X_{/\sim}$ . I am wondering if the converse is true in the general case, that is if $X_{/\sim}$ is compact, can we find a compact subspace $A\subset X$ such that any element of $X$ is in relation with some element of $A$ ?  If not are there some assumptions that we can put on $X$ and $\sim$ to make the converse true? For example even if $X$ is a topological manifold and $\sim$ is generated by a covering space action, I am not sure the converse holds. Motivation: When $\Gamma$ is a group acting on a manifold $X$ by covering space action, it is sometimes pretty clear that there is no compact subspace $A\subset X$ such that "" $A/\Gamma=X/\Gamma$ "" (for example $X=\Bbb R^2$ and $\Gamma=\langle (x,y)\mapsto (x+1,y)\rangle$ ). In that case we want to conclude that $X/\Gamma$ is not compact, but what is the ""most general case"" in which we have such a conclusion? Edit: This question has been asked on mathoverflow here . The answer is negative in general, but positive if $X$ is second countable and locally compact, and $X_{/∼}$ is first countable.","['quotient-spaces', 'general-topology', 'compactness']"
3285701,"Help in proving, that $\int_{0}^{\infty} \frac{1}{\Gamma(x)} d x=e+\int_{0}^{\infty} \frac{e^{-x}}{\pi^{2}+(\ln x)^{2}} d x$ using real methods only","The above identity is the difference formula for the Fransén-Robinson Constant. Proving this statement gave me severe headaches those last days, since everytime I try to calculate the RHS I either miss the $+e$ term or nothing converges. In the picture you can find one of my attempts. As it seems as of now, the integral in the last line on the left does not converge. I'm completely at a loss. I would appreciate some help alot!!","['integration', 'gamma-function', 'improper-integrals', 'real-analysis']"
3285714,singular shock solutions for strictly hyperbolic system of conservation law,"Shallow water equation \begin{eqnarray}
\rho_t+q_x=0\\
q_t + \left(q^2 +\frac{\rho ^2}{2} \right)_x=0
\end{eqnarray} being a strictly hyperbolic equation does not admit delta shock solutions. Where as some strictly hyperbolic systems such as Chaplygin gas equation given by \begin{eqnarray}
\rho_t +q_x=0\\
q_t+\left( \frac{q^2-1}{\rho} \right)_x=0
\end{eqnarray} admits delta shock solution. Why is this so? What is the difference between these two equations?. Why the procedure to get solution of the Riemann problem as a combination of shock and rarefaction for strictly hyperbolic equation as explained in  ""Numeric approximations of hyperbolic systems of conservation laws"" by Godlewsky and Raviart, does not work for this equation?","['hyperbolic-equations', 'analysis', 'partial-differential-equations']"
3285779,Is this function decreasing in $x$?,"Consider $$x \mapsto \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z}$$ decreasing in $x\in [0,\infty)$ , if $c > b > 0$ ? What I tried: taking the derivative in $x$ yields: $$\frac{\left( e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}} \right)\int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z - \left( e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}} \right)\int_{x-b}^{x+b} e^{-\frac{z^2}{2}}\text d z}{\left( \int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z \right)^2} $$ where the sign is determined by the numerator, so the question is if $$\frac{e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}}}{e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}}} - \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z} \geq 0 \ ?$$","['integration', 'calculus', 'integral-inequality', 'inequality', 'error-function']"
3285804,compare $m=50^{50}$ with $n=49^{51}$,"A multiple choice question: If $m=50^{50}$ and $n=49^{51}$ , then (A) $m>n$ (B) $m<n$ (C) $m=n$ (D) The given information is not enough My attempt: Since ordinary calculators can not evaluate large numbers as $m$ and $n$ , then we can use a trick, which is taking the logarithm of both $m$ and $n$ to the same base, lets use $\ln$ (log to the base $e$ ). $50\ln(50)$ VS $51\ln(49)$ $195.60$ VS $198.48$ Hence $49^{51}$ is greater. So, B must be the correct choice. This question was asked in a national exam for high school students. However: Calculators are not allowed. Log tables are not provided. Students may not have any knowledge about logarithms and their properties. Students should have basic knowledge about exponents like $(a/b)^k=a^k/b^k$ , $a^j \times a^k = a^{(j+k)}$ , and some other basics. The average time to solve a question in this exam is 75 seconds. How can we answer this question?","['exponentiation', 'algebra-precalculus', 'inequality']"
3285806,"Do injective, yet not bijective, functions have an inverse?","The formal definition I was given was that in order for a function $f(x)$ to have an inverse, $f(x)$ is required to be bijective. Nevertheless, further on, I was introduced to the inverse of trigonometric functions, such as the inverse of $\sin(x)$ . But $\sin(x)$ is not bijective, but only injective (when restricting its domain). As you can see the topics I'm studying are probably very basic, so excuse me if my question is silly, but ultimately does a function need to be bijective in order to have an inverse? If this is the case, how can we talk about the inverse of trigonometric functions such as $\sin$ and $\cos$ ?","['functions', 'inverse', 'analysis']"
3285831,Estimates on number of topologies on a finite set,"I've seen that there is currently no known formula for the amount of distinct topologies on a finite set. However I was wondering there are some rough estimates, aside from the trivial ones on the amount. Given a finite set $X$ of cardinality $n$ we have the trivial upper bound of $2^{2^n}$ . I was wondering whether there are some stronger estimates. 
In particular, I was wondering whether one can show there are less topologies on $X$ then there are relations on $X\times X$ ? i.e: Can we say that it is larger than $2^{n^2}$ or smaller than $2^{n^2}$ ? I would also welcome any help in estimates for an infinite set $X$ .","['general-topology', 'combinatorics']"
3285838,Zeros of the $n$-th derivative,"Suppose $f \in C^{\infty}(-1, 1)$ and $\sup_{(-1, 1)}|f(x)| \leq 1$ . I need to prove that for every $n \in \mathbb{N}$ there is some $\alpha_n \in \mathbb{R}$ such that if $|f'(0)| \geq \alpha_n$ then there is at least $n-1$ solutions to $f^{(n)}(x) = 0$ on $(-1, 1)$ . Now this obviously needs some kind of induction. It is easy to prove this for $n=2$ (just pick some $A > 1$ and suppose $|f'(0)| \geq A$ . Then if $|f'(x)|$ is $\geq A$ on all the $(-1, 1)$ , then $|f(-1 + \epsilon) - f(1 - \epsilon)| > 2$ for some $\epsilon$ by the mean value theorem, which contradicts the supremum assumption, so there is points left and right to 0, where $|f'(0)|$ is less than $A$ , which gives at least two points of different signs for $f''(x)$ , and that, since $f''(x)$ is continuous, gives a zero somewhere inbetween). Now if by induction we have $n-2$ zeros for $f^{n-1}(x)$ , it is easy to prove the existence of at least $n-3$ zeros for $f^{n}(x)$ by the same mean value theorem. But how do I find 2 more by simply increasing $|f'(0)|$ ?","['derivatives', 'analysis']"
3285960,Recover an ultrafilter from a maximal ideal,"Let $\mathcal{F}(I,\mathbb{R})$ denote the algebra of functions from an infinite set $I$ to $\mathbb{R}$ . We know that if $U$ is an ultrafilter on $I$ , then $\{f: I \rightarrow \mathbb{R} \ \vert \ \exists Q \in U: \ \forall x \in Q, \ f(x) = 0\} \subseteq \mathcal{F}(I,\mathbb{R})$ is a maximal ideal. Is the converse true? More precisely, let $J \subseteq \mathcal{F}(I, \mathbb{R})$ be a maximal ideal: is the set $\{f^{-1}(0): f \in J\}$ an ultrafilter over $I$ ? If not, are there any sufficient conditions on $I$ that would make it an ultrafilter?","['elementary-set-theory', 'filters', 'commutative-algebra', 'ideals']"
3285966,"The drawn diagonals divide the $N\times N$ board into $K$ regions. For each $N$, determine the smallest and the largest possible values of $K$.","Let $N$ be a positive integer. In each of the $N^2$ unit squares of an $N\times N$ board, one of the two diagonals is drawn. The drawn diagonals divide the $N\times N$ board into $K$ regions. For each $N$ , determine the smallest and the largest possible values of $K$ . So, I manage to find the smallest value which is $2N$ and it can be realised with drawing all diagonal parallel to each other. Why it can not be smaller? We can observe this configuration as a planar graph with $2N$ horizontal and $2N$ vertical edges on the border of the table and $N^2$ diagonal edges, so we have overall $E=4N+N^2$ edges. Also this graph has $4N$ vertices on the border of the table and some, say $r$ vertices in the interior of the table, so we have $V = 4N+r$ vertices. Clearly $r\leq (N-1)^2$ . We are interested in the number of bounded faces of this graph. Since this graph is not necesary connected we have to use general Euler formula $$V-E+F = c+1$$ where $c$ is a number of a connected components. Since $c \geq 1$ the number \begin{eqnarray}
K & =   & F-1\\
  & =   & E-V+c\\ 
  & =   & (4N+N^2)-(4N+r)+c\\ 
  & =   & N^2-r+c\\ 
  &\geq & N^2-(N-1)^2+1\\  
  & =   & 2N
\end{eqnarray} Now I would like to find also the largest value using Euler's formula but I can not find easly an estimate for $c$ which would do. Some idea? Anyway, the answer is $K_{\max} = \Big[{(N+1)^2 +1\over 2}\Big]$","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'planar-graphs', 'discrete-optimization']"
3285997,Manifold with injective continuous map into $\mathbb R^k$ admits embedding into $\mathbb R^{k+1}$,"This is Problem 4-34 from John Lee's book Introduction to Topological Manifolds: Suppose $M$ is an $n$ -manifold that admits an injective continuous map
  into $\mathbb R^k$ for some $k$ . Show that $M$ admits a proper
  embedding into $\mathbb R^{k+1}$ . [Hint: use an exhaustion function] My first thought was to let $f:M\to\mathbb R^k$ be the injective continuous map and let $g:M\to\mathbb R$ be an exhaustion function, then to map $x$ to $(f(x),g(x))$ , but this isn't a homeomorphism. In the book, it is proved that every compact manifold is homeomorphic to a subset of Euclidean space. Because of this, I was thinking that, since $g^{-1}((-\infty,c])$ is compact for every $c$ , maybe I could consider for each $c$ a homeomorphism $h_c:g^{-1}((-\infty,c])\to\mathbb R^{k_c}$ . However, I wasn't able to show that this leads to a proper embedding into $\mathbb R^{k+1}$ . I guess I'm not quite sure where to start with this since I'm not very familiar with how to use exhaustion functions/why they are useful. Also, I don't know anything about immersions or smooth manifolds, so I wasn't able to understand the stuff that I found online (which talked about the Whitney immersion theorem). Thanks!","['manifolds', 'general-topology']"
3286029,Algebraic Geometry Kempf 4.1.8 4.1.9,"I am working on the following exercises: 4.1.8. Let $\sigma$ and $\tau$ be sections of a decent presheaf over an open subset $U$ . Show that the subset $\{ u \in U | \sigma_u=\tau_u\}=V$ is open. 4.1.9 What sheaves do you know for which the subset $V$ is always closed in $U$ To solve exercise 4.1.8, I check what is written on page 41: A presheaf $F$ is called decent if any section of $F$ is determined by its local behavior. Thus, $F$ is decent if, for any two sections $\sigma$ and $\tau$ over an open subset $V$ , $\sigma = \tau \iff \sigma_v=\tau_v$ for all points $v\in V \iff$ for some covering $V=\cup V_{\alpha}, \sigma|_{V_{\alpha}}=\tau|_{V_{\alpha}}$ for each $\alpha$ Back in exercise 4.1.8, I claim that $U=V$ . This is because if $F$ is decent over $U$ , according to the quoted text, then $\sigma_u=\tau_u$ for all $u\in U$ Then for exercise 4.1.9, we have that any presheaf in the discrete topology gives us that $V$ is closed. In fact, any decent presheaf over a connected component $U$ will give us that $V$ is closed. I am not sure if this reasoning is correct. I feel that there is something I am missing or misunderstanding in the definition of a decent presheaf.","['algebraic-geometry', 'sheaf-theory']"
3286034,Derivative of the antipodal map of $S^n$ in the context of smooth manifolds.,"I was reading a solution of a question here (see item $c$ on page $7$ ), but I don't understand why the antipodal map of $S^n$ has derivative equal to minus the identity on $T_pS^n$ for $p \in S^n$ . This is clear if we see $S^n \subset \mathbb{R}^{n+1}$ , then it's a simple computation of analysis in Euclidian spaces, but how to prove this in the context of smooth manifolds, where we don't have an ambient space to $S^n$ necessarily?","['derivatives', 'smooth-manifolds']"
3286036,Intuition behind the generator of a semigroup,"This is how we defined the domain $D(A)$ and the operator $A$ of a semigroup: Let $(P_t)_{t>0}$ be a semigroup of linear contractions on $L$ . Let $$D(A) := \left\{ f\in L : \operatorname*{\mathit{s}-lim}_{h\downarrow 0} \frac 1 h [P_h f-f] \text{ exists in $L$}\right\}.$$ We define an operator $A$ on $D(A)$ by $$Af := \operatorname*{\mathit{s}-lim}_{h\downarrow 0} \frac 1 h [P_h f-f], \qquad\text{for $f\in D(A)$.}$$ Then $A$ is linear but not necessarily bounded with domain $D(A)\subset L$ . The operator $A$ is called generator of $(P_t)_{t\ge 0}$ . Is there an intuitive reason/image why $A$ is the generator of $(P_t)$ ? And what is the reason?","['semigroup-of-operators', 'derivatives', 'functional-analysis']"
3286065,Riesz-Markov-Kakutani representation theorem: qualitative or abstract proofs.,"I am referring to the famous result which states: Let X be a locally compact Hausdorff space. For any positive linear functional $\psi$ on $C_c(X)$ , there is a unique regular Borel measure μ on X such that $$\psi(f)=\int_{X}fd\mu(x)\ \\ \forall f\in C_c(x)$$ The proofs I read of this result involve very tricky and seemingly articificial calculations: they are very quatitative and make a lot of use of $\epsilon-\delta$ arguments. I absolutely do not mean they are less truthful whatsoever, nonetheless I find it strange there is not a 'qualitative' proof of this result, as we have for a lot of important duality theorems. Do anyone know some enlightening proof?","['measure-theory', 'duality-theorems', 'alternative-proof', 'functional-analysis', 'soft-question']"
3286093,Conjecture: No positive integer can be written as $a^b+b^a$ in more than one way,"Today, I came up with the following problem when trying to solve this . Are there distinct integers $a,b,m,n>1$ such that the equation $$a^b+b^a=m^n+n^m$$ holds? That is, is there ever an integer that can be written as $a^b+b^a$ in more than one way? I claim that the answer is No, but I think solving this is beyond my knowledge. For a very preliminary observation, the simplest case is to consider the powers of $1,5,6,0$ , since they end in those same digits. For example, $$\begin{cases}a\equiv5\pmod{10}\\b\equiv6\pmod{10}\end{cases}\implies a^b+b^a\equiv1\pmod{10}.$$ However, this brings about an issue, since there is hardly any indication as to what values $x$ and $y$ can take other than them having opposite parity. PARI/GP code is intfun(a,b,m,n)={for(i=2,a,for(j=2,b,for(k=2,m,for(l=2,n,if(i<>k && i<>l && j<>k && j<>l && i^j+j^i-k^l-l^k==0,print(i,"" "",j,"" "",k,"" "",l))))));} No solutions have been found up to $a,b,m,n\le100$ .","['number-theory', 'conjectures', 'modular-arithmetic', 'diophantine-equations']"
3286099,Evaluate $\int_{0}^\frac{\pi}{2} \sqrt{1+\sin^2(x)}dx$,"I feel like I'm very close, so I would only like a hint. I'm only using real methods with the main thing I'm trying to connect the integral to is the Beta function. With a bunch of substitutions, I have boiled the integral down to $$\int_{0}^\frac{\pi}{2} \sqrt{1+\sin^2(x)}dx=\sqrt{2}\int_{0}^\infty \frac{\sqrt{1+x^4}}{(1+x^2)^2}dx=\sqrt{2}\int_{0}^\infty \frac{2x^2(x^4-x^2+2)}{(1+x^2)^3\sqrt{1+x^4}}dx$$ I feel like there is some substitution that could convert the integral into something in terms of the Beta function but I cannot figure it out for the life of me. For reference, $$\int_{0}^\frac{\pi}{2} \sqrt{1+\sin^2(x)}dx=\frac{1}{4\sqrt{2\pi}}\left(4\Gamma^2\left(\frac{3}{4}\right)+\Gamma^2\left(\frac{1}{4}\right)\right) $$","['integration', 'calculus']"
3286110,What does the vertex/focus/directrix of a conic correspond to in projective space?,"A conic section can be viewed as the preimage of a circle in two dimensional projective-space. Take a polynomial $p(x, y)$ in $\mathbb{R}[x, y]$ , of degree $2$ . Write $p(x, y) = ax^2 + bxy + cy^2 + d x + ey + f$ . The embedding of varieties $f: \mathbb{R}^2 \hookrightarrow \mathbb{RP}^3$ sending $(x, y)$ to $[(x, y, 1)]$ has $f^{-1} (V(Q)) = V(p)$ , where $Q (x, y, z) \in \mathbb{R}^3[x, y, z]$ is the degree $3$ polynomial which is the `homogenization' of $p$ , $ax^2 + bxy + cy^2 + dxz + eyz + fz^2$ . $Q$ is a quadratic form, so there is a symmetric linear map $A : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ such that $Q(v) = v^* Av$ for $v \in \mathbb{R}^3$ . By a general formula, we have $$ A = \left[ \begin{matrix}a & b/2 & c/2  \\ 
b/2 & c & e/2 \\ 
c/2 & e/2 & f  \end{matrix} \right]$$ We would like to switch $Q$ for $Q \circ \Phi$ where $\Phi$ is an isomorphism chosen so that $Q \circ \Phi$ has a nice form. This swaps $\Phi^* A \Phi$ for $A$ . Using only orthogonal matrices $\Phi$ , we can put $A$ in the form $$\left[ \begin{matrix}r & 0 & 0  \\ 0 & s & 0 \\ 0 & 0 & t  \end{matrix} \right] $$ Then, using certain diagonal matrices, we can make $A$ into a diagonal matrix with only $0$ , $1$ , and $-1$ in the collumns, all of which give quadratic forms whose zero-sets are simple to visualize. See Sylvester's Law of Inertia for more on this point. Note that, over $\mathbb{C}$ , we could get a diagonal matrix with only $0$ 's and $1$ 's in the diagonal. Question: What is an interpretation of the following invariants of conic sections under this point of view? 1) The vertex of the conic section. 2) The foci of the conic section. 3) The directrix of the conic section. To be clear, I am asking what the vertex of the conic section corresponds to in projective space as an invariant of a projective circle, and the same for (2) and (3).","['algebraic-geometry', 'conic-sections']"
3286120,Is the integral from $x$ to $y$ of a continuous function differentiable?,"Let $f(t)$ be integrable and continuous function on $[a,b]$ . Let ${ F(x,y) = \int_{x}^{y}{f(t)\,dt} }$ . Show that $F(x,y$ ) is differentiable on the rectangle $[a,b] \times [a,b]$ . I tried to prove that ${ \frac{{ f(x_0+ \triangle x, y_0+ \triangle y)  - f(x_0, y_0) - \frac{\partial f} {\partial x}  (x_0, y_0)\triangle x - \frac{\partial f} {\partial y}  (x_0, y_0)\triangle y}}{ \sqrt{(\triangle x)^2 + (\triangle x)^2}} \to 0}$ as $(\triangle x, \triangle y) \to (0,0)$ by the differential definition. But how do I found the partial derivatives? and even if I'm able to find them, how do I estimate the whole expression?","['multivariable-calculus', 'definite-integrals', 'derivatives']"
3286150,Understanding the difference between pre-image and inverse,"I am a little confused as to what the difference between the pre-image and the inverse of a function are and how to find each given a particular function ( I had though they were essentially the same thing but I realise now I was mistaken in that thought ). From Wikipedia the definitions given are : Inverse Let $f$ be a function whose domain is the set $X$ , and whose image (range) is the set $Y$ . Then $f$ is invertible if there exists a function $g$ with domain $Y$ and image $X$ , with the property: $f ( x ) = y \iff g ( y ) = x . $ If $f$ is invertible, the function $g$ is unique, which means that there is exactly one function $g$ satisfying this property (no more, no less). That function $g$ is then called the inverse of $f$ , and is usually denoted as $f ^{−1}$ Pre-image Let $f$ be a function from $X$ to $Y$ . The preimage or inverse image of a set $B\subseteq Y$ under $f$ is the subset of $X$ defined by $f ^{− 1 }[ B ] = \{ x \in X \mid f ( x ) \in B \}$ . So using an example I want to see if I have this about right ( please correct any mistakes I make) Example 1: Lets say $f:\Bbb R \rightarrow \Bbb R$ $f(x)=x^2$ For this example, clearly $f$ cannot be invertible (hence no inverse) as there exists no function $g$ which will satisfy $ f ( x ) = y \iff g ( y ) = x$ . (as it would only map to positive values of $\Bbb R$ (i.e. not the whole set)) The pre-image of this function I believe is related to the inverse except it does not require that we map to the whole set $\Bbb R$ , but rather just a subset of it. Therefore we can find the function $f^{-1}$ in an analogous way to to finding the inverse we just have to be more considerate about what the co-domain of this function is. So if $f(x)=x^2 \Rightarrow y=x^2  $ swap variables to get $x=y^2 \Rightarrow \sqrt{x}=y=f^{-1} $ So the pre-image is the set $f ^{− 1 }[ \Bbb R_+ ] = \{ x \in X \mid f ( x ) \in \Bbb R_+, f^{-1}=\sqrt{x} \}
$ Example 2: An example of an invertible function would be $f:\Bbb R \rightarrow \Bbb R$ $f(x)=5x$ , as a function $g(x)=x/5$ has domain $\Bbb R$ and range $\Bbb R$ and satisfies $f ( x ) = y ⇔ g ( y ) = x .$ The pre-image in this case will be equal to the inverse. Could anyone please explain to me any mistakes I'm making here ?","['definition', 'functions', 'inverse', 'inverse-function']"
3286162,Show in a group $G$ that $(a^{-1})^{-1} = a $.,"Show in a group $G$ that $(a^{-1})^{-1} = a $ . Solution: Note that by definition $(a^{-1})^{-1}$ and $a$ are both inverse elements of $a^{-1}$ . Since in a group each element has a unique inverse, we can conclude that $(a^{-1})^{-1} = a$ . I will now prove that in a group inverse elements are unique. Suppose $b,c \in G$ such that $ba = e = ab$ and $ca = e = ac$ That is to say, $b$ and $c$ are both inverse elements of $a$ where $e$ is the identity of the group $G$ . Then we have $b = be = b(ac) = (ba)c = ec = c$ Therefore $b=c$ and inverse elements are unique.","['group-theory', 'proof-verification']"
3286299,How to get the sample number?,"I have this statement: The weights of a school are distributed in a normal way $\sim N(85,
> 8)$ If a sampling is done. what should be the size of this sample, so that
  the probability of the mean of this sample is less than $87$ , is $0.977$ ? My attempt: I need a sample $K$ of the weights, of size $n$ , such that $P(\overline K<87) = 0.977$ . So $n$ need to be $> 30$ (according to central limit theorem) and thus $\overline K \sim N(85, \frac{8}{\sqrt{n}})$ . Now I standardize the normal distribution. $P(\overline K<87) = P(Z < \frac{\sqrt{n}}{4})$ , where $Z$ is a random var $\sim N(0,1)$ . So i have: $P(Z < \frac{\sqrt{n}}{4}) = 0.977$ But from here, I do not know how to calculate $n$ . Any hint is appreciated. PD: I must not use the formula or calculus, and the answer must be $64$",['statistics']
3286302,"The ""reduction to the affine case"" trick [Scheme theory]","I'm currently studying the basic theory of schemes from Q.Liu's book ""AG & Arithmetic Curves"". I have ran across several times in a type of argument where the author has to prove some fact about a general scheme $X$ , and in the proofs he begins with ""we may assume that $X$ is affine"" , or some variants of this like situations when he need to prove some fact about a general open set $U$ , and he just says ""by covering $U$ with affine opens, we may assume $U$ is affine"" . There are occasions when I understand what he is doing, but not always. I believe he is always using the same reasoning for each and everyone of these situations, so I am disappointed by the fact that I can understand what he is doing in some cases but not in all cases. I would like to understand if there is some general pattern of reasoning that the author is following in each one of the cases, which I fail to see. I will give two examples: First example Fact: Any open subscheme of a noetherian scheme is noetherian; and for any generic open of a noetherian scheme, its ring of sections is a noetherian ring. Sketch of proof by reduction to the affine case: Step1: A generic open of an affine noetherian scheme is noetherian. Step2: Given a finite covering of affines, the intersections with the generic open $U$ is a finite cover of $U$ by opens which are noetherian by step1, so we conclude.
For the second part of the statement, since we saw that $U$ must be noetherian we cover it with a finite number of noetherian affines $U_j$ , $j=1,...,n$ . Given an ideal $I \subseteq \mathcal{O}_X(U)$ , its extension $I_j=I\mathcal{O}_X(U_j)$ is finitely generated for any $j$ . Pulling back the generators of $I_j$ for each $j$ , we get a finite subset in $\mathcal{O}_X(U)$ and we consider the ideal $J$ generated by it. We have then $I_j=I\mathcal{O}_X(U_j)=J\mathcal{O}_X(U_j)$ for every $j$ . Next, passing to the stalks we get that $I\mathcal{O}_{U_j,x}=J\mathcal{O}_{U_j,x}$ for every point $x \in U$ , and we conclude $I=J$ by a Nakayama argument. Done. Second example: Fact: Let $X$ be an integral scheme with generic point $\xi$ , $U$ a generic open subscheme, and recall that we can view both $\mathcal{O}_X(U)$ and $\mathcal{O}_{X,x}$ as subrings of $\mathcal{O}_{X,\xi}$ . We have that $\cap_{x\in U}\mathcal{O}_{X,x}=\mathcal{O}_X(U)$ . In the proof, Q.Liu's just says ""by covering $U$ with affine opens, we may assume $U$ is affine"". I see that here there is a (partial) explanation of this second example. But the answer there is actually not very illuminating for the purpose of my main question above. It does seem that the reduction to the affine case is a general strategy of proof that is realized with some random trick which is different for every situation. Is it so?","['commutative-algebra', 'geometry', 'affine-schemes', 'algebraic-geometry', 'schemes']"
3286306,Is $\frac{\arccos\left((\sqrt{r}+1)/(r+1/\sqrt{r})\right)}{\pi \left|1-r^{-3/2}\right|}$ analytic at $r=1$?,"Is the function $f:(0,\infty)\rightarrow(0, 1)$ , defined below, analytic at $r=1$ ? $$f(r) := \frac{\arccos\left(\frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}}\right)}{\pi 
   \left|1-\frac{1}{r^{3/2}}\right|}\quad
\mathrm{if\ } r>0\mathrm{\ and\ } r\neq1,
$$ and $f(1) :=\frac{\sqrt{2}}{3 \pi }$ where $\arccos(x)\in [0,\pi].$ If you are interested, this function arises from retrograde motion . The following statements seem to be true: $\lim_{r\rightarrow\infty} f(r) = 1/2$ , $\lim_{r\rightarrow 0} \frac{f(r)}{r^{3/2}}=1/2$ , $f(r) = f(1/r)r^{(3/2)}$ , $g(r) = \frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}}$ is analytic at $r=1$ , $g(r) = 1-\frac{1}{4} (r-1)^2+\frac{1}{4} (r-1)^3-\frac{11}{64} (r-1)^4+ \frac{3}{32} (r-1)^5 -\frac{21}{512} (r-1)^6+\frac{7}{512}
   (r-1)^7+ O((r-1)^8),$ $1-g(r) = \frac{(r-1)(1-1/\sqrt{r})}{r+1/\sqrt{r}}\geq 0$ , $\mathrm{sgn}(x)\arccos(1-x^2) = \sqrt{2} x + \frac{x^3}{6\sqrt{2}} +\frac{3 x^5}{80 \sqrt{2}} + \frac{5 x^7}{448 \sqrt{2}}+O(x^9)$ , and $1-1/r^{3/2} = \frac{3 (r-1)}{2}-\frac{15}{8} (r-1)^2+\frac{35}{16}
   (r-1)^3-\frac{315}{128} (r-1)^4+\frac{693}{256} (r-1)^5-\frac{3003
   (r-1)^6}{1024}+\frac{6435 (r-1)^7}{2048}+O\left((r-1)^8\right).$","['mathematical-astronomy', 'real-analysis']"
3286307,Why can the normal distribution be used to describe general data?,"I understand the assumptions underpinning and the motivation behind error distributions which led to the development of Laplace's and eventually Gauss's error distributions as explained in Saul Stahl's essay . It makes sense that larger random errors are less likely than smaller ones, and that the magnitudes of errors are the same regardless of direction, i.e. $\phi(x) = \phi(-x)$ . What I'm having a hard time reconciling is the widespread use of the normal distribution to describe generic data, not just errors. Why can we model human heights or test grades with a normal distribution? I understand The Central Limit Theorem allows us to use the normal distribution to approximate certain sampling distributions, e.g. $\overline{x}$ , as normal (usually when $n \geq 30$ ) - but why? What does an error curve have to do with CLT? Do random factors that affect natural phenomena to cancel each other out leading to (approximately) normal distributions with large samples?","['statistics', 'normal-distribution']"
3286314,"In set theory, is a ""class"" a set of subsets, or is it just one subset of a set?","I'm not sure whether a single class refers to a single subset of a larger set, or whether it refers to multiple subsets of a larger set (i.e. a set of sets) Which is it? If it's the latter (a single class refers to multiple subsets of a set), how do you refer to a single subset within the class?",['elementary-set-theory']
3286338,Verifying a function is a solution of $y' = 3 y^{2/3}$,"Let $\alpha \in \mathbb{R}$ and consider $y = (x-\alpha)^3 {\bf 1}_{(\alpha, \infty)}$ . I want to verify that $y' = 3 y^{2/3}$ . Well, clearly, $$ y' = 3(x-\alpha)^2 = 3 [(x-\alpha)^3]^{2/3} = 3 y^{2/3} $$ and if $x \leq \alpha $ , then $y = 0$ and so $y' = 0$ and the solution is satisfied trivially. Im confused as to why do we need to verify the case $x=\alpha$ separetely as my notes says: It says that we check left- and right- hand derivates to see that y(x) at $x= \alpha$ satisfies the ODE. Can someone clarify this to me? My understanding is that my work on the third line covers this case. What am I missing?",['ordinary-differential-equations']
3286376,Four squares such that the difference of any two is a square?,"I. This post asks to find $4$ integers $a,b,c,d$ such that the difference between any two is a square. As mentioned by my answer , it is equivalent to finding $3$ squares such that the difference of any two is also a square. With the positive answer to that question, the OP of that post muses if we can also find $FIVE$ integers $a,b,c,d,e$ such that the difference of any two is a square. Equivalently, we are to solve System 1 , $$p^2+w^2 = x^2\\ q^2+w^2 = y^2\\ r^2+w^2 = z^2\\ s^2+x^2 = y^2\\ t^2+x^2 = z^2\\ u^2+y^2 = z^2$$ If this has a positive solution, then it involves three special hypotenuse $\color{blue}{x,y,z}$ expressible as Pythagorean triples in $1$ , $2$ , and $3$ ways, $$p^2+w^2 = \color{blue}{x^2}$$ $$q^2+w^2 = s^2+x^2 = \color{blue}{y^2}$$ $$r^2+w^2 = t^2+x^2 = u^2+y^2 = \color{blue}{z^2}$$ which seems doable. II. Alternatively, by solving the system for $p,q,r,s,t,u$ , then we can reduce the number of variables to finding just four squares $w^2< x^2<y^2<z^2$ such that the difference between any two is a square, or System 2 , $$-w^2+x^2 =\square_1\\-w^2+y^2=\square_2\\-w^2+z^2=\square_3\\-x^2+y^2=\square_4\\-x^2+z^2=\square_5\\-y^2+z^2=\square_6$$ For the special case $w = 0$ , the smallest of infinitely many solutions is, $$w,x,y,z = 0, 153, 185, 697$$ Q: More generally, can we find four squares $w^2,x^2,y^2,z^2$ of System 2 such that $w\neq0$ ? Update: Using zwim's data here , we find that for $w,x,y,z = 448, 952, 1073, 1105$ , then, $$-w^2+x^2 =840^2\\-w^2+y^2=975^2\\-w^2+z^2\neq\square_3\,\,\\-x^2+y^2=495^2\\-x^2+z^2=561^2\\-y^2+z^2=264^2$$ Almost, but not quite. But I believe a higher search range will yield something.","['number-theory', 'systems-of-equations', 'pythagorean-triples', 'diophantine-equations']"
3286415,Solve for the angle $x$ in the right triangle without trigonometry.,"Solve for the angle $x$ in the right triangle without trigonometry. I don't know how to find the angle $x$ . My try I drew the height from $P$ to $AQ$ , because the triangle $APQ$ is isosceles. Also i noticed that drawing a perpendicular from $P$ to $BC$ may be useful, because it is also a bisector of $\angle QPB$ . After that i tried some similarity between triangles, but found nothing. Any hints? PS: I got this problem from an exercise list, and found that they use the angle approximation of $37,53,90$ in the triangle $3-4-5$ sometimes. I don't know if this is the case.","['euclidean-geometry', 'triangles', 'geometry']"
3286423,How to justify taking the derivative operator inside of the integral?,"I'm reading a PDE book, and the authors use this often, for $u = u(x,t)$ , and V an arbitrary volume on in the interior of a solid: $${\frac{d}{dt} \int_V u \space dx}$$ $$= {\int_V u_t \space dx}$$ How is differentiating under the integral w.r.t. to time $t$ justified? I imagine it must be so simple that I'm simply rusty with my introductory analysis coursework and don't see it right away. My attempt: Writing out the difference quotient and using linearity of the integral, we have that: $$ \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx   $$ and assuming I can use the dominated convergence theorem, we have that: $$ \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx = \int_V \lim_{h \to 0} \frac { u(x,t+h) - u(x,t)} {h}  \space dx $$ $$ = {\int_V u_t \space dx} $$ So, there seem to be some nice conditions on $u$ that the authors are assuming. Thanks,","['integration', 'analysis', 'calculus', 'partial-differential-equations', 'derivatives']"
3286506,Why does $\int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta) \implies Pr[u(X)=0]=1$?,"I am trying to prove that a shifted expolential distribution is a complete set. i.e. $$X_1,...,X_n \sim_{iid} f_X(x;\theta)=e^{-(x-\theta)}, 0<\theta<x$$ and if $E[u(X)]=0$ then $$u(X) = 0$$ . The procedure I am taking is to first look at the expectation $$\int_{\theta}^{\infty} u(x)e^{-(x-\theta)}dx=0$$ and take the derivative on both sides with respect to $\theta$ . I see that the result will be $$\int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta)$$ but I do not know how this leads to $u(X)=0$ . The notes that I am reading mentions that $Pr[u(X)=0]=1$ instead of $u(X)=0$ so I am thinking that I am missing something. I would appreciate your input.","['statistics', 'parameter-estimation', 'probability']"
3286515,prove or disprove if $ab\equiv ac \bmod m$ then $b\equiv c \bmod m$,"prove or disprove
if $ab\equiv ac \bmod m$ then $b\equiv c \bmod m$ there is a theorem said that this equality holds when gcd(a,m)=1 so I try to find a counterexample to disprove this
for example $2.9 \equiv 2.3 \bmod 9$ because gcd(2,9)=1 then $9\equiv 3 \bmod 6$ holds but $4.6 \equiv 4.3 \bmod 2$ and gcd(4,2)=2 then $6\equiv 3 \bmod 2$ this not holds $2.6 \equiv 2.2 \bmod 2$ and gcd(2,2)=2 but $6\equiv 2 \bmod 2$ this holds although gcd (a,m) not 1? $4.6 \equiv 4.2 \bmod 2$ and gcd(4,2)=2 but $6\equiv 2 \bmod 2$ , $2k=4$ why this holds when gcd (a,m) not 1 too? is this also related to multiplicative inverse?
and one more thing I want to ask if gcd( $a,b$ ) $=1$ , then I can write this as $ax+by=1$ for some integers $x$ and $y$ . Is gcd( $x,y$ ) has to be $1$ or coprime? if yes why is that? thanks!","['gcd-and-lcm', 'modular-arithmetic', 'discrete-mathematics']"
3286548,Can we remove any prime number with this strange process?,"This is a prime-removal algorithm I made, which may appear to be quite complex so I will start with an example. @Max has since added this sequence to OEIS, number A332198 . The process goes as follows: Start with the first prime number, $$S(1)=2.$$ From $2$ , add the next prime number $3$ to get $$S(2)=2+3=5.$$ There are no non-trivial factors, so we move on. From $2+3$ , add the next prime number $5$ to get $$S(3)=2+3+5=10.$$ Since $10=2\times5$ and these two numbers appear in the sum, we remove $2$ and $5$ . We are left with $3$ . From $3$ , add the next prime number $5$ to get $$S(4)=3+5=8.$$ Now $8=2\times2\times2$ , but $2$ does not appear in the sum, so we move on. From $3+5$ , add the next prime number $7$ to get $$S(5)=3+5+7=15.$$ Since $15=3\times5$ and these two numbers appear in the sum, we remove $3$ and $5$ . We are left with $7$ . (and so on) So essentially, we keep on adding consecutive prime numbers until we reach a sum whose prime factorisation contains some of those primes. We remove those primes and start the process once again. Great, except... There is one more rule that needs to be added. If we continue doing this, we soon find ourselves in a rather strange scenario. (and so on) a continuation: From $37+47+59+\cdots+241+251+257$ , add the next prime number $263$ to get $$S(57)=37+47+59+\cdots+251+257+263=5918.$$ Now $5918=2\times11\times269$ , but neither of the three primes appear in the sum, so we move on. From $37+47+59+\cdots+251+257+263$ , add the next prime number $269$ to get $$S(58)=37+47+59+\cdots+251+257+263+269=6187.$$ Since $6187=23\times269$ and $269$ appears in the sum, we remove $269$ . We are left with $37+47+59+\cdots+251+257+263$ . This is a cycle! The sequence of $263$ and $269$ will continue forever, if we don't add another rule to this process. Therefore, I call $269$ a cyclic prime, and I propose this new rule. From $37+47+59+\cdots+251+257+263$ , add the next non-cyclic prime number $271$ to get $$S(59)=37+47+59+\cdots+251+257+263+271=6189.$$ Now $6189=2\times2063$ and these two numbers do not appear in the sum, so we move on. I do not know whether there are any dicyclic primes; that is, primes that are still cyclic after more than one iteration. Questions Will every prime number in the sum eventually be removed? If not, which prime numbers will remain in the sum forever? It appears to be true. From this Python program by @EuxhenH, we see that all primes up to $16903$ are eliminated at some point before overflow. The table linked shows how long it takes ( $N$ iterations) for the smallest prime $P$ in the sum to be removed. It appears that $P$ increases as $N$ increases despite significant fluctuation. Follow-up: What is the asymptotic growth of $N(P)$ ? For instance, does it admit a $\log$ or $\log\log$ increase? Are there infinitely many cyclic prime numbers? As of writing, the only known cyclic prime numbers are $269$ and $94793$ . Using @Max's PARI/GP program, I plotted $S(n)$ for $n\le575$ which should grow like $Cx^a\log^bx$ . Note that this sequence, by definition, is bounded above by $$\sum_{i\le n}p_i\sim\frac12n^2\log n.$$ Indeed, when $a=2$ and $b=1$ , Mathematica gives an optimal value of $C\approx0.4716$ . However, the residual plot suggests there is perhaps a better asymptotic.","['recursive-algorithms', 'number-theory', 'recreational-mathematics', 'prime-numbers']"
3286570,Trace of power of a real matrix,Suppose that $A \in M_n(\mathbb{R})$ . Prove that there is a $k \in \mathbb{Z}_{\geq 1}$ such that $Tr (A^k) \geq 0$ .,"['matrices', 'trace', 'linear-algebra']"
3286583,Solve $y=\frac{1}{3}[\sin x+[\sin x+[\sin x]]]$ & $[y+[y]]=2\cos x$,"$[x]$ represents the greatest integer function $y=\frac{1}{3}[\sin x+[\sin x+[\sin x]]]$ & $[y+[y]]=2 \cos x$ Find the number of solution My approach is as follows $\sin x \in (\pi,2\pi)$ $y=\frac{1}{3}[\sin x+[\sin x+[\sin x]]]$ $y=-1$ $[-1+[-1]]=2 \cos x$ $\cos x=-1$ which is possible at $x=\pi$ hence NO SOLUTION $\sin x \in {\frac{\pi}{2}}$ $y=\frac{1}{3}[\sin x+[\sin x+[\sin x]]]$ $y=1$ $[1+[1]]=2 \cos x$ $\cos x=1$ which is possible at $x=0$ hence NO SOLUTION Similary if $\sin x \in (0,\pi)-\frac{\pi}{2}$ $y=\frac{1}{3}[\sin x+[\sin x+[\sin x]]]$ $y=0$ $[0+[0]]=2 \cos x$ $\cos x=0$ which is possible at $x=\frac{\pi}{2}$ hence NO SOLUTION My answer is zero but the official answer is 3 ""three"". Please help me understand my mistake.",['trigonometry']
3286626,Restriction of scalars on the level of vector bundles,"If $\phi \colon R \to S$ is a ring homomorphism, one obtains the extension and restriction of scalars functors $\phi_* \colon R\mathbf{Mod} \to S\mathbf{Mod}$ and $\phi^* \colon S\mathbf{Mod} \to R\mathbf{Mod}$ between the module categories of $R$ and $S$ . If $f \colon X \to Y$ is a continuous map of compact Hausdorff spaces, then one gets an induced map $\phi = C(f) \colon C(Y) \to C(X)$ of the rings of complex-valued functions on the spaces given by precomposition by $f$ . In this case, a finitely generated projective module $\mathcal{E}$ over $C(Y)$ is mapped via extension of scalars to a finitely generated projective module $\phi_*(\mathcal{E})$ over $C(X)$ . Moreover, if $E \to Y$ is a complex vector bundle such that its continuous sections $\Gamma(E)$ is isomorphic to $\mathcal{E}$ as a $C(Y)$ -module, then the module $\phi_*(\mathcal{E})$ over $C(X)$ is isomorphic to the continuous sections of the pullback bundle $f^*(E)$ over $X$ . In this way, we get a description of the extension of scalars functor along the map $C(f)$ in terms of vector bundles. Is there a similar characterization of the restriction of scalars functor along $\phi = C(f)$ ? That is, if $F \to X$ is a vector bundle over $X$ , does the $C(Y)$ -module $\phi^*(\Gamma(F))$ correspond (at least in some nice cases) to the continuous sections of a vector bundle over $Y$ ? EDIT: If $E \to X$ is a vector bundle over $X$ , then the only sensible way I can think of to define a ""fiber"" of a pushforward bundle $f_*E$ at $y \in Y$ would be $(f_*E)_y = \bigoplus_{f(x) = y} E_x$ . However, for this to give a (constant rank) vector bundle, one needs the cardinality of the fibers $\{ x \in X: f(x) = y \}$ of $f$ to be constant and finite in $y$ , which is of course often not the case. However, if $X$ and $Y$ are topological groups and and $f$ is a surjective group homomorphism with finite kernel, then the above should actually be satisfied. Are there any results or references that mention this situation?","['vector-bundles', 'category-theory', 'differential-geometry']"
3286635,Proof of the equivalence of the two definitions of heavy-tailed distributions,"Heavy-tailed distributions have two equivalent definitions: For any $\lambda>0$ , the moment generating function of the distribution (denoted by $F$ ) blows up: $$\int_{\mathbb{R}}e^{\lambda x}\mathrm{d}F(x)=+\infty$$ or For any $\lambda>0$ , the decrease of the tail probability of $F$ is slower than exponentially increase: $$\lim_{x\to+\infty}e^{\lambda x}(1-F(x))=+\infty$$ That the second definition implies the first one is not difficult to prove, but how to give a proof to its converse, i.e. how to derive the second definition from the first one?","['integration', 'probability-distributions', 'calculus', 'probability-theory', 'probability']"
3286648,Does $H\leq G$ imply $[G_x: H_x]\leq [G: H]$?,"Let a group $G$ act on a set $S$ . Let $H$ be a subgroup of $G$ and $x\in S$ . I want to show that $[G_x: H_x]\leq [G: H]$ , where $G_x$ is the isotropy group of $x$ in $G$ . (If necessary, we assume that these indices are finite). If $G$ is a finite group, since $|G|=|G_x||Gx|$ , we have $$[G_x: H_x]=\frac{|G|}{|H|}\frac{|Hx|}{|Gx|}\leq\frac{|G|}{|H|}=[G:H].$$ And this is how I get the desired inequality. As for the case where $G$ is infinite, such as $SL_2(\mathbb Z)$ , I wrote $G=\bigcup_{i=1}^nHg_i$ where $n=[G:H]$ and wanted to show that $G_x=\bigcup_{i=1}^mH_xg_i'$ for some $g_i'$ s. But I cannot move on since I cannot connect these $g_i'$ s with the isotropy subgroup $G_x$ . This problem was raised by me from my studying in Fred Diamond & Jerry Shurman: A First Course in Modular Forms (Exercise 2.4.4(a)). I am not confident about my conjecture. Any help will be appreciated.","['group-actions', 'group-theory', 'abstract-algebra']"
3286656,Existence of global attractor in duffing equation,"How to prove the existence and identify global attractor in Duffing equation $$\ddot{x}+\epsilon \dot{x}+x^3-ax=0$$ where $\epsilon >0$ and $a>0$ ? I found a definition: A bounded closed set $A_1 \subset X$ is called a global attractor for a dynamical system $(X, S_t)$ , if $A_1$ is an invariant set the set $A_1$ uniformly attracts all trajectories starting in bounded sets, i.e. for any bounded set $B$ from $X$ $$\lim_{t\to \infty} \sup \lbrace \operatorname{dist}(S_t y, A_1): y\in B \rbrace=0$$ where $\operatorname{dist}(z,A)=\inf\lbrace\operatorname{d}(z,y): y\in A\rbrace$ where $\operatorname{d}(z,y)$ is the distance between the elements $z$ and $y$ in $X$ . I finished only ODE course and I don't know a lot about dynamical systems.","['nonlinear-system', 'ordinary-differential-equations', 'dynamical-systems']"
3286716,Different definitions of a relatively compact operator,"Let $T,K$ be unbounded operators on a Hilbert space $H$ .
I've seen the following definition of a relatively compact operator: (i) The operator $K$ is called relatively compact with respect to $T$ , if for some $z$ in the resolvent set of $T$ , $KR_T(z)$ is compact, where $R_T(z):=(T-z)^{-1}.$ I've also seen: (ii) The operator $K$ is called relatively compact with respect to $T$ , if for every sequence $(x_n)_{n \in \mathbb{N}}\subseteq H$ such that $(Tx_n)_{n \in \mathbb{N}}$ is bounded, $(Kx_n)_{n \in \mathbb{N}}$ contains a convergent subsequence. Do definitions (i) and (ii) have something to do with each other, or are they distinct? What is the intuition behind these definitions? Definition (ii) looks like a generalisation of a compact operator, but definition (i) is just weird. This question has also been posted on Math Overflow .","['compact-operators', 'operator-theory', 'definition', 'functional-analysis', 'spectral-theory']"
3286740,"Is there a bijection $\phi : [0,1]\to [0,1]$ with no finite orbits?","Is there any bijection $\phi : [0,1]\to [0,1]$ such that for every $x\in [0,1]$ and every $n\in\mathbb{N}$ we have $\phi ^n (x)\ne x$ ? I don't even know how to approach this other than the bijection can't be continuous. Thanks!","['elementary-set-theory', 'functions', 'dynamical-systems']"
3286747,What is the vector $x ∈ \mathbb{R^3}$ that achieves $max||x||_1$ subject to $||x||_2 = 1$?,"I'm trying to answer the questions ""What is the vector $x ∈ \mathbb{R^3}$ that achieves $max||x||_1$ subject to $||x||_2 = 1$ ?"" and ""What is the vector x ∈ $R^3$ that achieves $max||x||_∞$ subject to $||x||_2 = 1$ ? I think the first question is asking me to find a vector with three components that will have the maximum $||x||_1$ norm value where $\sqrt{x_1^2 + x_3^2 + x_2^2} = 1$ , so $x_1^2 + x_3^2 + x_2^2 = 1$ . I know the The L1 norm is just the sum of the absolute values of the vector's components. After trial and error I came up with $x = [\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}]$ , but also $[-\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}]$ , and $[-\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}]$ , etc. For my the second question, I think I need to find the vector in $\mathbb{R^3}$ that will give me the maximum value of the absolute value of the vector's components given $x_1^2 + x_3^2 + x_2^2 = 1$ . I came up with $[1, 0, 0]$ , $[0, 1, 0]$ , $[0, 0, 1]$ , $[-1, 0, 0]$ , $[0, -1, 0]$ , and $[0, 0, -1]$ . Am I correct? Is there a more formal way to figure this out and write my solution?",['linear-algebra']
3286767,"some relation $R$ is defined on $\mathbb{R}$ such that $xRy \iff x = 7^{k}y,$ for some $k\in \mathbb{Z}$. Prove that $R$ is an equivalence relation","some relation $R$ is defined on $\mathbb{R}$ such that $xRy \ \iff  \ x = 7^{k}y,$ for some $k\in \mathbb{Z}$ . Prove that $R$ is an equivalence relation I'm confused with proving that it is symmetric and transitive.","['equivalence-relations', 'relations', 'discrete-mathematics']"
3286789,Differentiating the $W$ lambert function to find optimum angle for maximum horizontal range of a projectile with air resistance,"Here is the equation I obtained, representing the horizontal range of a projectile in air resistance as a function of its initial release angle- $$x=\frac{V_0\cos(\theta)}{B}-\frac{V_0\cos(\theta)}{B}\,\exp\left(\frac{BV_0\sin(\theta)}{g}\right)\exp\left(W\left(\frac{BV_0\sin(\theta)\exp\left(\frac{BV_0\sin(\theta)}{-g}\right)}{-g}\right)\right), $$ where: $x=$ horizontal distance travelled by projectile $V_0=$ initial velocity of projectile $g=$ acceleration due to gravity $\theta=$ initial release angle I have assumed that the force of air resistance acting on the projectile is equal to a constant k multiplied by the velocity of the projectile. So, $F=kv.$ In my equation, $B= k/m,$ where $m$ is the mass of the projectile. Also, $W(\cdot)$ is the $W$ Lambert function. I have been trying to differentiate the expression with respect to $\theta$ in order find an expression for the value of $\theta$ that would result in maximum horizontal range, assuming all other terms in the equation are constant. How can this be done?","['classical-mechanics', 'projectile-motion', 'derivatives']"
3286825,Why does the order matter when I throw differently colored balls into bins? (with limited bin capacity),"I have $n$ bins, $n$ red balls, and $n$ green balls. I throw the balls into randomly selected bins one at a time. Once a bin has $2$ balls in it, I close the bin. It matters to me how many bins will have two red balls at the end. Does the order I throw the balls matter? The answer is yes, which I've verified through simulation and looking at the $n=2$ case, but I'm wondering whether there is a powerful conceptual explanation. This could take the form of calculating the expected number of red/red bins for any $n$ in ordered vs randomized sets of balls and noting the difference in the process, or it could take some other form.","['balls-in-bins', 'probability']"
3286832,Integral with two symmetric matrices,"Let ${\bf A} \in M_{n\times n}(\mathbb R)$ be a symmetric positive-definite matrix, ${\bf x}\in \mathbb R^n$ . I know how to obtain (for $p>n/2$ ): $$ \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^p} = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p-\frac{n}{2})}{\Gamma(p)} $$ by moving to a basis that diagonalises matrix ${\bf A}$ . I seek to calculate $$ I = \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^{p}({\bf x}^T {\bf B}{\bf x} + 1)^{q}}$$ where both ${\bf A}$ and ${\bf B}$ are symmetric positive-definite matrices. The same method won't work, as in general there's no way to simultaneously diagonalize ${\bf A}$ and ${\bf B}$ . It can only transform the integral to the form $$ I =\frac{1}{\sqrt{\det{\bf A}}}\int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf x} + 1)^{p}({\bf x}^T {\bf C}{\bf x} + 1)^{q}}$$ where $ {\bf C} = {\bf A}^{-\frac12}{\bf B}{\bf A}^{-\frac12}$ , but I don't know if anything more can be done. EDIT: After searching around a bit more, I've found formula $$ \frac{1}{a^p b^q} = \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}\int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big(a t + b(1-t)\big)^{p+q}}{\rm d}t$$ which allows to write $I$ in the form $$ I = \frac{1}{\sqrt{\det{\bf A}}}\frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} \int_{\mathbb R^n} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big({\bf x}^T \big(t{\bf 1}+(1-t){\bf C}\big){\bf x} + 1\big)^{p+q}} {\rm d}t {\rm d}{\bf x} = \\ = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\sqrt{\det\big(t{\bf 1}+(1-t){\bf C}\big)}} {\rm d}t$$ Therefore if the eigenvalues of ${\bf C}$ are $\lambda_i$ we get $$ I = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 t^{p-1}(1-t)^{q-1}\prod_{i=1}^n\big(t+(1-t)\lambda_i\big)^{-\frac12} {\rm d}t$$ The remaining integral looks like some kind of generalized hypergeometric function, but I don't know which one, and whether it has a simpler representation.","['integration', 'multivariable-calculus', 'matrices', 'symmetric-matrices']"
3286878,Show that $\sum_{cyc} \sqrt{8a+b^3}\ge 9$,"Prove that $$\sqrt{8a+b^3}+\sqrt{8b+c^3}+\sqrt{8c+a^3}\ge 9 \text{ 
 for  }  a,b,c\ge0 \text{ 
 and  } a+b+c=3.$$ By Holder $$\left(\sum _{cyc}\sqrt{8a+b^3}\right)^2\left(\sum _{cyc}\frac{1}{8a+b^3}\right)\ge 27$$ Or $$\sum _{cyc}\frac{1}{8a+b^3}\le \frac{1}{3} \text{ WLOG  } 0\le a\le b =a+u\le c=a+v\le 3$$ By full expanding it's obvious, because: $$u^2-uv+v^2\ge 0$$ $$\cdots $$ $$72u^9-80u^8v-776u^7v^2-591u^6v^3+683u^5v^4+1403u^4v^5+1569u^3v^6+1168u^2v^7+424uv^8+72v^9\ge $$ $$\ge uv(v-u)(80u^6+\cdots 80v^6)\ge 0$$ My solution is very ugly. Can i solve it by Holder but more beautiful than it does? Help me","['proof-explanation', 'multivariable-calculus', 'holder-inequality', 'inequality']"
3286892,Gradient of squared distance function,"Let $\theta: M \times M \to \mathbb{R}$ the squared distance function $\theta(x,y)=d(x,y)^{2}$ on complete Riemannian manifold $M$ . I would like to calcule the gradient of $d^{2}$ , where $d^{2}_{y}(x)=d^{2}(x,y)$ , for $x,y\in M$ such that $x \notin Cut(y)\cup \{y\}$ . The idea is to consider $\alpha(t,s)$ a variation through geodesics of a minimizing geodesic $\gamma(t)$ with $\gamma(0)=y$ and $\gamma(l)=x$ , $l=d(x,y)$ and if $L(s)$ denotes the length of the geodesic $t \to \alpha(t,s)$ , then by first variation formula : $$\frac{d L(s)}{ds}\Bigr|_{s=0}= \langle V,T\rangle^{l}_{0} - \int\limits^{l}_{0} \langle V, \nabla_{T}T\rangle dt$$ where $T=\frac{\partial \alpha}{\partial t}$ and $V=\frac{\partial \alpha}{\partial s}$ . Now , since $\alpha$ os geodesic $\nabla_{T}T=0$ , and taking $\alpha$ such that $V$ is the Jacobi Field such that $V(0)=0$ and $V(l)=v$ , where $v \in T_{x} M$ , thus : $$\frac{d L(s)}{ds}\Bigr|_{s=0}=\langle v,T(l)\rangle.$$ Now, I have two questions $(1)$ Why $T(l)=\frac{-1}{l}exp^{-1}_{x}(y)$ ? $(2)$ Why $d(d_{y}(x))(v)=\frac{d L(s)}{ds}\Bigr|_{s=0}$ ? Thanks","['calculus-of-variations', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
3286908,Gradient formula in Lee Smooth Manifolds differs from others?,"In example 13.31 (page 343) of Introduction to Smooth Manifolds, Lee uses the musical isomorphisms to calculate the gradient in polar coordinates. He obtains: $$\text{grad} f = \frac{\partial f}{\partial r} \frac{\partial}{\partial r} +
\frac{1}{r^2} \frac{\partial f}{\partial \theta} \frac{\partial}{\partial \theta}.
$$ The $1/r^2$ terms differs from every other expression for the gradient in polar coordinates I have seen. In every other version, it is a $1/r$ term. For example: How to obtain the gradient in polar coordinates and https://en.wikipedia.org/wiki/Del_in_cylindrical_and_spherical_coordinates I don't see any errors in Lee's derivation. What am I missing?","['polar-coordinates', 'vector-analysis', 'differential-geometry']"
3286926,Truncated Sieltjes r-atomic moment problem,"Given a certain $n \in \mathbb{N}$ , can i construct a discrete positive random variable $X$ that fullfill the following conditions : $$\forall k \in \{1,...,n\}, \mathbb{E}(X^k) = \mu_k$$ for some $\mu_k$ given (parameters). We suppose that there exists such a random variable (in facts, the $\mu$ 's come from a random variable that i want to estimate). For exemple, for $n=1$ , it is enough to set a dirac in $\mu_1$ . for $n=2$ , take a radom variable with 2 atoms $\mu_1 +x$ and $\mu_1 -x$ with equal probability and choose x such that : $$\mu_1^2 + x^2 = \mu_2$$ giving $x = \sqrt{\mu_2 - \mu_1^2}$ . What about $n \ge 3$ ? Edits : 1° Yes, it is possible (prooved to be possible by many diffrent papers, notably Tchakaloff's theorems). 
2° Yes, there exists some algorithm to do it, but i did not found a propper one yet.","['moment-problem', 'probability', 'random-variables']"
3286942,"Given $\triangle ABC$ with $C=60^\circ$, show that $\frac{1}{a+c}+\frac{1}{b+c}=\frac{3}{a+b+c}$","Given that $C=60^\circ$ on a triangle $ABC$ , prove the following relation: $$\frac{1}{a+c}+\frac{1}{b+c}=\frac{3}{a+b+c}$$ P.S. Maybe this info could be of help: I used the cosine rule of triangles, given that $C=60°$ .","['trigonometry', 'problem-solving']"
3286963,"Under what hypotheses do all bounded sets have ""area""?","In ""Infinitesimal Calculus"" by Henle and Kleinberg, page $52$ , the statement is made that, ""[U]nder some strange hypotheses, all bounded sets can be assigned a numerical 'area'."" I haven't been able to locate anything about this by Googling.  Can some elucidate the statement, or give me a reference? Full statement: https://books.google.com/books?id=mMPCAgAAQBAJ&pg=PA52 Incidentally, it has also been shown that under some strange hypotheses, all bounded sets can be assigned a numerical ""area."" The universe under these hypotheses is a very odd and interesting one, but we are not interested in it here, for in that universe the hyperreal numbers do not exist!","['calculus', 'measure-theory']"
3286990,Computing Chi-Square p-value with large degree of freedom (df).,"I'm using a Chi-square test on a large set of data points, so my degrees of freedom is rather large. So far, none of the online tables I've been able to find have options for $4095$ degrees of freedom. Is a chi-square test still applicable with this large of a dataset? Is there some way to find the p-value for $d.f. = 4065$ ?","['chi-squared', 'statistics', 'probability']"
3287005,Does $\mathbb{E}[X^6] < \infty$ imply $\mathbb{E}[X^4] < \infty$?,"Let $X$ be a random variable.
  Does $\mathbb{E}[X^6] < \infty$ imply $\mathbb{E}[X^4] < \infty$ ? My tries I know this isn't rigorous but I thought that when there was a counterexample, the rv in question would have density so I could write $$
\mathbb{E}[X^k]
= \int_{\mathbb{R}} x^k f(x) \ \text{d} x,
$$ where $f$ is the PDF of $X$ . 
I thought that if I had a rv with density $f(x) := x^{-4}$ this would yield $\mathbb{E}[X^4] = 1$ and $\mathbb{E}[X^6] = \infty$ but as $\int_{\mathbb{R}} x^{-4} \ \text{d} x = \infty$ , $f$ isn't a PDF, so I don't know where to continue from there. Jensens inequality gives $\mathbb{E}[X^k] \ge \mathbb{E}[X]^k$ from $k \in \mathbb{N}$ but if $\mathbb{E}[X] \in (0,1)$ we have $\mathbb{E}[X]^6 < \mathbb{E}[X]^4$ and for $\mathbb{E}[X] > 1$ we have $\mathbb{E}[X]^6 > \mathbb{E}[X]^4$ , so I don't know where to continue from there. I wanted to use that $L^q(\Omega) \subset L^p(\Omega)$ for $p \le q$ if $\Omega$ is a finite measure space but I have no reason to believe I can restrict myself to that special case.","['measure-theory', 'probability-theory']"
3287013,Positive definite Hessian on a Riemannian manifold,"Consider a Riemannian manifold $M$ and a smooth function $f : M \rightarrow \mathbb{R}$ . Assume that the Hessian of $f$ , $\text{Hess}f$ is positive definite at $a \in M$ , which is a critical point of $f$ i.e. $\text{grad}f(a) = 0$ . I wish to understand how to prove the following statement: There exists an open neighbourhood of $a$ in $M$ where $f$ has compact connected sub-level sets all containing $a$ and no other critical point of $f$ . Either a direct answer, sufficiently insightful hints or references would be welcome. I would request you to supply the definition of the Hessian used while answering the question, since I've seen various versions of the definition, one in which it's a linear map from the tangent space to itself [1], one in which it is a map from the tangent space to the cotangent space[2], one in which it is a bilinear map from the space of vector fields on $M$ to $\mathbb{R}$ [3]. This effort is aimed at understanding proof of theorem 2 in Mahony et. al.[2]. References [1] : P. Absil, R. Mahony, and R. Sepulchre, Optimization Algorithms on
Matrix Manifolds, Princeton, NJ, USA: Princeton University Press,
2008. [2] : R. Mahony, J. Trumpf, and T. Hamel, “Observers for kinematic systems with
symmetry,” IFAC Proceedings Volumes, vol. 46, no. 23, pp. 617 – 633,
27
2013, 9th IFAC Symposium on Nonlinear Control Systems. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1474667016317293 [3] : J. Lafontaine, An Introduction to Differential Manifolds, Springer International Publishing, 2015.","['riemannian-geometry', 'differential-geometry']"
3287031,Help figuring a formula for my job,"I'm a metal worker, I cut, weld, whatever. I'm trying to figure out a formula where I could take my cutting list 
And figure out the most efficient way to cut it with the materials I have. For example 
I have 2 20ft lengths of tubes
I want 5 peices cut at 1.5ft, 3 at 2ft and 1 at 2ft What I'm trying to do is calculate the most efficient cutting order to save the most material I'm using.
My education is limited so any help would be appreciated, thank you.",['calculus']
3287070,Where is my error in trying to find Pythagorean triples with matching areas?,"I'm trying to craft a formula for finding matching areas in Pythagorean triples the way I have done with matching sides and matching perimeters and matching area:perimeter ratios .  For example: $f(10,3)=(91,60,109)$ and $f(14,1)=(195,28,197)$ have the same area $2730.$ I thought I had solved the $area$ equation using the $cubic$ formula but, when I plug in $m=10$ , I do not get $3$ and, when I plug in $m=14$ , I do not get $1$ in a spreadsheet or in Wolfram Alpha. Here is my attempt at solving the area equation for $n$ in terms of $D$ (the area), and $m$ . $$A=m^2-n^2\quad B=2mn\implies D(area)=\frac{AB}{2}=\frac {2m^3 n-2mn^3}{2}=m^3 n-mn^3$$ $$\text{In standard form }\\
 mn^3+0n^2-m^3n+D=0\qquad a=m\quad b=0\quad c=-m^3\quad d=D$$ \begin{align*}
n&=\sqrt[3]{\biggl(\frac{-b^3}{27a^3 }+\frac{bc}{6a^2}-\frac{d}{2a}\biggr)+\sqrt{\biggl(\frac{-b^3}{27a^3}+\frac{bc}{6a^2}-\frac{d}{2a}\biggr)^2+\biggl(\frac{c}{3a}-\frac{b^2}{9a^2}\biggr)^3}}\\
&+\sqrt[3]{\biggl(\frac{-b^3}{27a^3 }+\frac{bc}{6a^2}-\frac{d}{2a}\biggr)-\sqrt{\biggl(\frac{-b^3}{27a^3}+\frac{bc}{6a^2}-\frac{d}{2a}\biggr)^2+\biggl(\frac{c}{3a}-\frac{b^2}{9a^2}\biggr)^3}}\\
&-\frac{b}{3a}   \end{align*} $$=\sqrt[3]{\biggl(-\frac{d}{2a}\biggr)+\sqrt{\biggl(-\frac{d}{2a}\biggr)^2+\biggl(\frac{c}{3a}\biggr)^3}}\\+\sqrt[3]{\biggl(-\frac{d}{2a}\biggr)-\sqrt{\biggl(-\frac{d}{2a}\biggr)^2+\biggl(\frac{c}{3a}\biggr)^3}}$$ $$=\sqrt[3]{\biggl(-\frac{D}{2m}\biggr)+\sqrt{\biggl(-\frac{D}{2m}\biggr)^2+\biggl(\frac{-m^3}{3m}\biggr)^3}}\\
+\sqrt[3]{\biggl(-\frac{D}{2m}\biggr)-\sqrt{\biggl(-\frac{D}{2m}\biggr)^2+\biggl(\frac{-m^3}{3m}\biggr)^3}}$$ Oddly, inputs $2730,3\rightarrow -10$ and $2730,1\rightarrow -14$ as though I had solve for $m$ instead of $n$ ... and changing the sign of $D$ changed the sign of the result. I tried individual components in a spreadsheet and found negatives under the square roots. Finally, I tried the simplest triple $f(2,1)=(3,4,5)$ and, as expected, when $D=\frac{3*4}{2}=6, m=1\rightarrow n=-2.$ I tried my hand at
De Moivre's Theorem because of the real and imaginary components under the cube roots but I got lost. Did I make a mistake in in my identification of $a,b,c,d$ or in my algebra? Or, can someone show me how to use De Moivre's Theorem to yield $n=1$ when $D=6$ and $m=2$ ? Oddly, when I solved this for a more complex formula that I came up with to generate $sets$ that are, themselves, a subset of triples where $GCD(A,B,C)$ is an odd square (a subset that includes all primitives and reduces clutter), I got it to work except for the smallest triangles of sets $7$ and above where there were square roots of negatives. My generator uses $(n,k)$ as though they were $(2m-1+n,n)$ . This formula generates a non-trivial triple for every $n,k\in\mathbb{N}$ . For the above exercise: $f(1,1)=(3,4,5)\quad f(4,3)=(91,60,109)\quad f(7,1)=(195,28,197)$ . $$A=(2n-1)^2+2(2n-1)k\\ B=2(2n-1)k+2k^2\\ C=(2n-1)^2+2(2n-1)k+2k^2$$ and the formula that usually works ( $6,1\rightarrow 1\quad 2730,4\rightarrow 3\quad 2730,7\rightarrow $ #NUM) is: $$k=\sqrt[3]{(\frac{D}{8(2n-1)}+\sqrt{\frac{D^2}{64(2n-1)^2 }-\frac{(2n-1)^6}{1728}}}\\
+\sqrt[3]{(\frac{D}{8(2n-1)}-\sqrt{\frac{D^2}{64(2n-1)^2 }-\frac{(2n-1)^6}{1728}}}-\frac{(2n-1)}{2}$$ I suppose, what I'm looking for is 1) my error, 2)how to use de Moivre's $n^{th}$ root theorem to deal with imaginaries in this context,  or 3) how to find the limits of the search (as I have done with my other formulae) when I input $n$ instead of $m$ .","['cubics', 'pythagorean-triples', 'polynomials', 'algebra-precalculus', 'complex-numbers']"
3287078,How to isolate individual contributions to group against another group?,"Is there a way to compare individual team member contributions when pitted against another team multiple times?  This is in a sport like rowing where you take N number of individuals from a set A and M number of individuals against set B. Is there a way to make estimations on-going with incomplete sets (not all unique combinations of N vs M exist) or with some interdependence where some in N or M are never changed out? In a similar vein to this question How to extract an individual's (normalized) contribution from a group? I am interested if there are some recommended approaches or literature on this subject.  The difference in my question is this limited to two groups with known team vs team data. Just to make it clear, I'm trying to compare relative times between teams and then try to see individual performance.  So for example here is some fake data between Team A and Team B with 9 members to choose 5 from with the teams results for that Event (note the times are the same for members because they are racing in the same boats). TEAM A      Event1     Event2   Event3
N1          55.5       53.3     51.2
N2          ----       53.3     ----
N3          55.5       53.3     ----
N4          55.5       -----    51.2
N5          55.5       ----     51.2
N6          ----       53.3     51.2
N7          ----       53.3     ----
N8          55.5       ----     ----
N9          ----       ----     51.2

Vs TEAM B
TEAM B      EVENT1     Event2   Event3
M1          55.0       52.9     53.2
M2          ----       52.9     ----
M3          55.0       52.9     ----
M4          55.0       ----     53.2
M5          55.0       ----     53.2
M6          ----       52.9     53.2
M7          ----       52.9     ----
M8          55.0       ----     ----
M9          ----       ----     53.2 Each event occurs in different conditions so only the same events can be directly compared of Team A vs B.  So for example Team A group at Event1 clocked 55.5 and Team B group won at 55.0. What I've Tried I've computed their relative time difference per event.  Then I've taken each individuals average time difference and compare them.  This works ok, but I don't think this is an accurate comparison and I don't know how to estimate cases where the data is limited or how to know what data still should be gathered.",['statistics']
3287120,Proving that the potential solution to a differential equation converges (or not),I'm looking at this equation: $$x''(t) - \sin(x(t))\cdot x'(t) + a ^{2}x(t) = 0$$ My question is therefore / how to prove that a solution is not convergent. Numerically we can see it oscillates but I'm not sure how to prove it. So far I've proven it stays at a bounded distance to the solution of: $$x''(t) + a^2x(t) = 0$$ (Gronwall lemma),"['convergence-divergence', 'ordinary-differential-equations']"
3287126,"Regarding the Post: ""Does $k+\aleph_0=\mathfrak{c}$ imply $k=\mathfrak{c}$ without the Axiom of Choice?""","I have been searching around for a proof (that I can understand) that the cardinality of the irrationals equals $\mathfrak{c},$ the cardinality of the reals. (This, of course, is different than showing that the cardinality of the irrationals is uncountable.) Based on the post I have cited in the title: Does $k+\aleph_0=\mathfrak{c}$ imply $k=\mathfrak{c}$ without the Axiom of Choice? would I be correct in arguing that the cardinality of the irrationals must be $\mathfrak{c}$ ; for otherwise, if $k$ were greater than $\aleph_0$ (say $I$ ) but less than $\mathfrak{c}$ , then $I + \aleph_0 = \mathfrak{c},$ which would be impossible since that would imply that the sum of two transfinite numbers less than $\mathfrak{c}$ can equal $\mathfrak{c}$ ? Also, am I correct in assuming that the theorem cited by the aforementioned post is indeed a published result that does not rely on the Axiom of Choice? Thank you.","['elementary-set-theory', 'axiom-of-choice', 'cardinals']"
3287158,derivative with respect to a diagonal matrix,"Had check some previous questions regarding the derivatives of diagonal matrices, but haven't found a form like this. If $K=Wdiag(s)W^T$ , in which $W$ is an m-by-n matrix, and $diag(s)$ represents an n-by-n diagonal matrix of which diagonal is represented by the vector $s$ . I'm interested in the derivative of the log-determinant of K, ( $\frac{\partial{ln}|K|}{\partial{s}}$ ), but I get stuck at solving this part: $\frac{\partial{K}}{\partial{s}}$","['matrices', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'derivatives']"
3287192,General formula for volume integral of scalar field?,"I am looking to derive general formulas for the electric fields generated by general charged objects given their charge densities. For linear and surface charge densities I have been able to derive the following expressions for a linear charge density $\lambda(\vec{r})$ and surface charge density $\eta(\vec{r})$ , respectively: $$\vec{E}=\frac{1}{4\pi\varepsilon_0}\int_C\frac{\lambda(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,ds$$ $$\vec{E}=\frac{1}{4\pi\varepsilon_0}\iint_S\frac{\eta(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,dS$$ I am able to compute these given that I am able to find a parameterization of the curve $C$ or the surface $S$ , using the following formulas: $$\int_C f\,ds=\int_a^b f\big(\vec{r}(t)\big)\Vert\vec{r}'(t)\Vert\,dt$$ $$\iint_S f\,dS=\int_c^d\int_a^b f\big(\vec{r}(u,v)\big)\Vert\partial_u\vec{r}\times\partial_v\vec{r}\Vert\,du\,dv$$ However, I am uncertain how to do this in the case with a volume charge density $\rho(\vec r)$ . I do not know a formula to simplify the volume integral. My guess would be maybe something like: $$\iiint_Rf\,dV=\int_\alpha^\beta\int_\gamma^\delta\int_\epsilon^\zeta f\big(\vec r(u,v,w)\big)\Vert\partial_u\vec r\times\partial_v\vec{r}\times\partial_w\vec{r}\Vert\,du\,dv\,dw$$ But I do not know a method to derive this. Is there a method/formula for computing these types of integrals given a parameterization of the region $V\subset\mathbf{R}^3$ ?","['integration', 'volume', 'multivariable-calculus', 'calculus', 'physics']"
3287216,Proving Two Ito Integral Idenities,"Let $f(t)$ be a deterministic function that is of bounded variation on all intervals $[0,s]$ for $s \in \mathbb R$ . Also assume that $f$ has compact support in $[0,\infty ]$ . Let $B_t$ be the standard Brownian Motion. Show that almost surely $$ \int_0^\infty f(t)\, dB_t=-\int_0^\infty B_t \, df(t) $$ Now suppose that $f_1$ and $f_2$ are continuous deterministic functions in $t$ . Prove that for each $t \ge 0$ almost surely we have that $$ \int_0^tf_2(s)\left[\int_0^sf_1(u) \, dB_u\right] \, ds=\int_0^t\left[\int_s^tf_2(r)\, dr\right]f_1(u)\, dB_u $$ For (1), if $f$ was continuous then I would be able to able to apply Itos formula to the function $g(x,t)=xt$ and then apply this with $g(B_t,f(t))$ but $f$ is not continuous here. However, can I think of the RHS as a Riemann-Stieltjes integral? I feel like my work isn't quite justified. Here is what I did. First I assumed that the RHS is a Riemann-Stieltjes integral and used the summation by parts formula $$\sum_{k=0}^nf_{k}(g_{k+1}-g_k)=f_{n+1}g_{n+1}-f_0g_0 -\sum_{k=1}^ng_{k+1}(f_{k+1}-f_k)$$ and the compactness of the support of $f$ to get that $$ \begin{align} \int_0^\infty B_t \, df(t)&= \lim_{n \to \infty}\int_{\frac{1}{n}}^n B_t \, df(t) 
\\
&=\lim_{n \to \infty}\lim_{\Vert \Delta_k \Vert \to 0}\sum_{j=0}^{k-1}B_{t_j}(f(t_{j+1})-f(t_j)) 
\\
&=\lim_{n \to \infty} B_nf(n)-B_{\frac{1}{n}}f(1/n)-\lim_{\Vert \Delta_k \Vert \to 0} \sum_{j=0}^{k-1}f(t_{j+1})(B_{t_{j+1}}-B_{t_j})
\\
& =\lim_{n \to \infty} B_nf(n)-B_{\frac{1}{n}}f(1/n)-\int_{\frac{1}{n}}^nf(t)\, dB_t
\\
&=-\int_0^\infty f(t) \, dB_t
\end{align}$$ Where above $\{\frac{1}{n} = t_0 < t_1 < ... < t_k = n\}$ is a partition of $[\frac{1}{n},n]$ and $\Vert \Delta_k \Vert $ denotes the $\max|t_j-t_{j-1}|$ As for (2) I am unsure on how to proceed. Any hints are appreciated.","['stochastic-integrals', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3287222,Difficulty seeing how two combinatorial identities are equivalent?,"In my textbook for discrete mathematics they state the following theorem (followed by proof): (i) $C(i + 1, k) = C(i, k - 1) + C(i, k)$ The proof of the above statement that is provided makes sense to me. However, a couple of examples later, they claim that this statement is equivalent to: (ii) $C(i, k) = C(i + 1, k + 1) - C(i, k + 1)$ I am having a hard time understanding why they are equivalent and was hoping somebody might be able to show me how (i) could become (ii).","['combinatorics', 'discrete-mathematics']"
3287231,Partial sum square root of reciprocal of primes,"I would like to know if the following reasoning makes sense. I want to bound/estimate the following sum $$
\sum_{p\leq x}\frac{1}{\sqrt{p}}
$$ Using integration by parts we have \begin{align}
\sum_{p\leq x}\frac{1}{\sqrt{p}}&=\int_2^x \frac{1}{\sqrt{t}}\,d(\pi(t))\\
&=\left[\frac{\pi(t)}{\sqrt{t}}\right]_2^x+\frac{1}{2}\int_2^x\frac{\pi(t)}{t^{3/2}}\,dt\\
&=\frac{\pi(x)}{\sqrt{x}}+\frac{1}{2}\int_2^x\frac{\pi(t)}{t^{3/2}}\,dt
\end{align} Now, using that by the PNT we have $\pi(x)\sim \dfrac{x}{\ln x}$ , we get $$
\sum_{p\leq x}\frac{1}{\sqrt{x}}\sim\frac{\sqrt{x}}{\ln x}+\frac{1}{2}\int_2^x\frac{1}{\sqrt{t}\ln t}\,dt
$$ On the other hand we have $$
\int_2^x \frac{1}{\sqrt{t}\ln t}\,dt=\operatorname{Ei}\left(\frac{\ln x}{2}\right)-\operatorname{Ei}\left(\frac{\ln 2}{2}\right)\sim\operatorname{Ei}\left(\frac{\ln x}{2}\right)\sim\frac{2\sqrt{x}}{\ln x}
$$ which I got using wolframalpha. Hence I obtain $$
\sum_{p\leq x}\frac{1}{\sqrt{p}}\sim \frac{2\sqrt{x}}{\ln x}
$$ Does it make sense? How could I prove the asymptotic for $\int_2^x\frac{1}{\sqrt{t}\ln t}\,dt$ without the need to refer to wolframalpha? Thank you!","['number-theory', 'asymptotics', 'taylor-expansion', 'prime-numbers']"
3287238,Stability equilibrium point of non linear system with nonpositive eigenvalues.,"We have the following non linear system: \begin{equation}
\dot{X}=f(X),
\end{equation} and lets suppose that $f(X_0)=0$ . We also have that $\lambda_1,\dots,\lambda_n$ are the eigenvales of $D_f(X_0)$ and there are $Re(\lambda_1),\dots,Re(\lambda_k)=0$ for some $k<n$ and $Re(\lambda_{k+1}),\dots,Re(\lambda_n)<0$ . What can we say about the stability of the system? Can we say is stable because it is not unstable?","['stability-theory', 'ordinary-differential-equations', 'dynamical-systems']"
3287256,How to calculate $\sum_{n= 0}^{\infty} \left( \frac {n+1} {n+2}+n(n+1)\ln \left(1-\frac 1{(n+1)^2}\right)\right)$,"How to calulate $$\sum_{n= 0}^{\infty} \left( \frac {n+1} {n+2}+n(n+1)\ln \left(1-\frac 1{(n+1)^2}\right)\right)?$$ My goal was to calculate the integral $\int_{0}^{\infty}\left[\frac{1}{x}-\frac{1}{e^x-1}\right]^2 dx$ . Using the three results 1) $\frac{1}{(1-e^{-x})^2}=\sum_{n=0}^{\infty}(n+1)e^{-nx}$ 2) $a_n=\int_0^1\int_0^1\frac{1}{(n+u+v)^3}uvdudv=\frac{1}{2(n+2)}+\frac{n}{2}\log\left(1-\frac{1}{(n+1)^2}\right)$ 3) $e^x-1-x=e^xx^2\int_0^1ue^{-ux}du$ . We deduce $\int_{0}^{\infty}\left[\frac{1}{x}-\frac{1}{e^x-1}\right]^2dx$ = $\int_{0}^{\infty}\frac{x^2}{(1-e^{-x})^2}\left[\int_0^1ue^{-ux}du\int_0^1ve^{-vx}dv\right]dx$ = $\int_0^1\int_0^1\left[\sum_0^{\infty}(n+1)\int_0^{\infty}x^2e^{-x(n+u+v)}dx\right]uvdudv$ = $ 2\int_0^1\int_0^1\left[\sum_0^{\infty}\frac{n+1}{(n+u+v)^3}\right]uvdudv $ = $ 2\sum_0^{\infty}(n+1) a_n  =?$ I'm interested in other ways too, but I'd like to understand how to calculate the sum of this series","['integration', 'calculus']"
3287301,Bernstein tail bound to expectation,"Let $Z$ be a nonnegative r.v. satisfying \begin{align*}
P(Z \geq t) \leq C e^{-\frac{t^2}{2(v^2+bt)}}
\end{align*} where $(v,b)$ are positive constants and $C \geq 1$ . Show that \begin{align*}
E(Z) \leq 2v(\sqrt{\pi}+\sqrt{\log C} ) + 4b(1+\log C).
\end{align*} I've tried expanding $EZ = \int_0^\infty P(Z \geq t) dt$ and bounding the exponential on different subsets of the positive real line but the end expression still seems mysterious to me.","['statistics', 'probability-theory']"
3287315,Are there references that talk about the connection between curvature and compactness?,"The Killing-Hopf theorem says that if $M$ is a complete connected Riemannian manifold of constant sectional curvature $K$ , its universal cover is one of the following: if $K > 0$ , it is the sphere; if $K = 0$ , it is the Euclidean space; if $K < 0$ , it is the hyperbolic space. Of these possibilities, only the first one implies a compact topology. Are there any texts or references that delve into the connection between compactness and curvature more deeply? Is there anything that classifies manifolds in the spirit of the Killing-Hopf theorem for more general cases?","['curvature', 'reference-request', 'compactness', 'differential-geometry']"
3287330,"Prove that if $S$ and $T$ are subspaces of $V$, then so is $S\cap T$","I am asked to prove that If $S$ and $T$ are subspaces of $V$ , then so is $S\cap T$ . I have been thinking about this for a while but remain perplexed. I tried this: Let $\{e_1, e_2, ..., e_k\}$ be a basis for $S$ and let $\{u_i, u_2, ..., u_k\}$ be a basis for $T$ . Let all $u_n=e_n$ be denoted $d_n$ . $\therefore$$\{d_1, d_2, ..., d_p\}$ is a basis for $S \cap T$ if $S\cap T$ is a linear space. Now all elements $x$ in $S$ have form $x=\sum^{k}_{i=1}c_ie_i$ for some $c_i \in \Bbb R$ and all elements $y$ in $T$ have form $y=\sum^k_{i=1}c_iu_i$ for some $c_i \in \Bbb R$ . Now $S\cap T$ contains all elements of $S$ and $T$ such that $x=y$ . $\therefore \sum^k_{i=1}c_ie_i=\sum^k_{i=1}c_iu_i$ . $\therefore e_i=u_i$ . $\therefore S\cap T$ is a subspace with basis $\{d_1, d_2, ..., d_p\}$ . I am not sure that this proof is valid, nor that it even really proves anything? Is there any merit to it? Otherwise, what would be a better approach to proving this?","['proof-writing', 'proof-verification', 'linear-algebra']"
3287336,"Correct bounds when finding PDF of $X + Y$, where $X,Y$ i.i.d.","Let $X,Y \sim \mathcal{U}([0,a])$ be independent identically continuously uniform distributed real random variables.
  Find the PDF of $Z := X + Y$ . I know that this can be accomplished via convolution:
For $x \in \mathbb{R}$ we have \begin{align*}
\int_{\mathbb{R}} f_{X}(x - y) f_{Y}(y) \ \text{dy}
& = \frac{1}{(a - 0)^2} \int_{\mathbb{R}} 1_{[0,a]}(x - y) \cdot 1_{[0,a]}(y) \ \text{dy} \\
& = \frac{1}{a^2} \int_{0}^{a} 1 \ \text{dy} \cdot 1_{[0,2a]}(x)
= \frac{1_{[0,2a]}(x)}{a}.
\end{align*} I obtained the bounds like this.
From the second integral in the first line we have $$
0 \le x - y \le a
\qquad \text{and} \qquad
0 \le y \le a.
$$ Adding both inequalities yields $y \le x + y \le 2a + y$ or $$
0 \le x \le 2 a.
$$ Therefore $x < a$ and the bounded on the dy-integral are [0, a] and furthermore we know that $x \in [0,2a]$ . Is this (result and procedure) correct and / or is there an easier way to do this? In this task I faced a similar challenge:
  Let $X,Y \sim \text{Exp}(\lambda)$ for some $\lambda > 0$ be independent and identically distributed.
  Find the density of $X + Y$ . \begin{align*}
(f_{X} \ast f_{Y})(x)
& = \int_{\mathbb{R}} f_{X}(x - y) f_{Y}(y) \ \text{dy} \\
& = \int_{\mathbb{R}} \lambda e^{-\lambda (x - y)} 1_{[0, \infty)}(x - y) \cdot \lambda e^{-\lambda y} 1_{[0, \infty)}(y) \ \text{dy} \\
& = \lambda^2 \int_{0}^{x} e^{-\lambda (x - y)} \cdot e^{-\lambda y} \ \text{dy} \cdot 1_{[0, \infty)}(x)
= \lambda^2 \int_{0}^{x} e^{-\lambda x} \ \text{dy} \cdot 1_{[0, \infty)}(x) \\
& = \lambda^2 x \cdot e^{-\lambda x} \cdot 1_{[0, \infty)}(x).
\end{align*} Here the same question as above applies.","['probability-distributions', 'probability-theory', 'density-function']"
3287364,Differential of Multiplication Map of Lie Group,"Checking some basic facts about $G$ , a Lie Group. Proposition $3.14$ provides an isomorphism $\alpha: T_e(G\times G)\to T_eG\oplus  T_eG:v\mapsto (d\pi_1(v),d\pi_2(v))$ , where $\pi_i;\ i=1,2$ are the projections to the first and second factor, respectively. Here is the exercise: I thought this would be routine, but I have a problem justifying a step in $(1)$ . Using the identification as a hint, write $dm_{(e,e)}(X,Y ) = dm_{(e,e)}(X, 0) + dm_{(e,e)}(0,Y ).$ Each term on the right hand side is the restriction of the differential $dm_{(e,e)}$ to vector spaces that are isomorphic to $T_eG$ so it is intuitively obvious that $dm_{(e,e)}(X,0)=dm'_e(X)$ where $m'(x)=m(x,e)$ and $dm_{(e,e)}(0,Y)=dm''_e(Y)$ where $m''(y)=m(e,y).$ Now the result follows because $m'$ and $m''$ are the identity on $G$ . How do I prove rigorously what I think is ""intuitively obvious""? Remark: for part $(2)$ I fiddled around with maps until I got one that has zero differential: $x\overset{\Delta}{\mapsto}(x,x)\overset{f}{\mapsto}(x,i(x))\overset{m}{\mapsto}e.$ The result now follows by the chain rule, using $(1)$ at the end.","['manifolds', 'tangent-spaces', 'smooth-manifolds', 'differential-geometry']"
3287405,"Assume $x,\,y\in G$ and both commute with $[x,\,y]$. Prove that for all $n\in\mathbb{Z}^+,\;(xy)^n=x^ny^n[y,\,x]^{\frac{n(n-1)}{2}}$.","Assume $x,\,y\in G$ and both commute with $[x,\,y]$ . Prove that for all $n\in\mathbb{Z}^+,\;(xy)^n=x^ny^n[y,\,x]^{\frac{n(n-1)}{2}}$ . $[x,\,y]=x^{-1}y^{-1}xy$ is the commutator of $x$ and $y$ . I have found that \begin{equation}
xy^{-1}xy=y^{-1}xyx\\
yx^{-1}y^{-1}x=x^{-1}y^{-1}xy\\
[x,\,y][y,\,x]=1\\
x=[y,\,x]x[x,\,y]\\
y=[y,\,x]y[x,\,y].
\end{equation} However, I can only prove the special case when $n=2$ .","['group-theory', 'abstract-algebra']"
3287502,"Is ""assignment"" a canonical term in math?","Wikipedia says : In vector calculus and physics, a vector field is an assignment of a
  vector to each point in a subset of space. 1 A vector field in the
  plane (for instance), can be visualised as: a collection of arrows
  with a given magnitude and direction, each attached to a point in the
  plane. Vector fields are often used to model, for example, the speed
  and direction of a moving fluid throughout space, or the strength and
  direction of some force, such as the magnetic or gravitational force,
  as it changes from one point to another point. Is ""assignment"" a canonical term in math? What does ""assignment"" mean here?","['linear-algebra', 'vector-spaces', 'terminology']"
3287525,Why do eigenvectors arise as the solution of PCA?,"I have very limited knowledge of linear algebra and therefore I don't have an geometrical intuition behind PCA. Why the eigen vectors (which are simply defined as vectors whose direction doesn't change after a linear transformation) are also the directions that maximize variance?
I have seen PCA definition using Lagrange multipliers but I would like to have the geometrical intuition behind it. Thanks","['matrices', 'statistics', 'principal-component-analysis', 'eigenvalues-eigenvectors']"
3287527,Upper Triangular Implies Diagonal?,"If all matrices can be made upper - triangular with respect to some basis by: Suppose V is a finite-dimensional complex vector space and T is a linear transformation.
  Then T has an upper-triangular matrix with respect to some basis of V. And any upper - triangular matrix can be made orthogonal: Suppose T is a linear transformation. If T has an upper-triangular matrix with respect to some
  basis of V, then T has an upper-triangular matrix with respect to some
  orthonormal basis of V. But it's clear that an upper-triangular and orthonormal matrix must be a diagonal matrix. This implies every matrix has a diagonal matrix which we know to false as it was stated earlier to not be true. What am I missing?","['matrices', 'abstract-algebra', 'linear-algebra']"
3287532,Is there a continuous transformation that does not preserve zero measure?,"Is there a continuous transformation that does NOT preserve zero measure? I found one of my textbooks claims that a continuous transformation in $\mathbb R^n$ preserves zero measure. But I am highly skeptical about it. In fact, I can only see the case when the transformation is actually Lipschitz, like in this post .  But in general, I am struggling in finding a counterexample. Any help?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3287543,determine solution for differential equation,"$$(1-t^2)x'-tx+t^2-1=0,\quad t\in(-1,1)$$ I have to determine the solution for this DE.
I've tried dividing the equation with $(1-t^2)$ , so it'd look like: $x'-\frac{t}{1-t^2}x=1$ I've tried to first solve it as a homogeneous DE: $\frac{1}{x}dx-\frac{t}{1-t^2}dt=0$ $\ln|x|+\ln(1-t^2)^{\frac{1}{2}}=C_1$ $|x|\cdot(1-t^2)^{\frac{1}{2}}=e^{C_1}$ $x=e^{C_1}(1-t^2)^{-\frac 1{2}}=C(1-t^2)^{-\frac 1{2}}$ But then I'm stuck. I can't figure out what to use this to solve the non-homogeneous form.
Any help?",['ordinary-differential-equations']
3287551,"Trouble with example of sequences $(f_n)_n,(g_n)_n\in\ell^1$ with $\|f_n\|_\infty\to0,\,\|f_n\|_2\to\infty$ and $\|g_n\|_2\to0,\,\|g_n\|_1\to\infty.$","$\textbf{a)}$ Show that the inclusions $$\ell^1\subseteq\ell^2\subseteq\ell^\infty,$$ are all strict. To see that the inclusions are strict, first consider $f=(1,1,1\dots)$ . Then $\|f\|_\infty=1$ so $f\in\ell^\infty$ , however $$\|f\|_2^2=\sum^{\infty}_{k=1}1=\infty\quad\text{and}\quad\|f\|_1=\sum^{\infty}_{k=1}1=\infty,$$ so that $f\notin\ell^2$ and $f\notin\ell^1$ . Now consider $\displaystyle g=\left(1,\frac{1}{2},\frac{1}{3},\dots\right)$ . Then $$\|g\|_2^2=\sum^{\infty}_{k=1}\frac{1}{k^2}=\frac{\pi^2}{6}\quad\text{and}\quad\|g\|_1=\sum^\infty_{k=1}\frac{1}{k}=\infty,$$ so that $g\in\ell^2$ but $g\notin\ell^1$ . Therefore, the inclusions are strict, that is, $\ell^1\subset\ell^2\subset\ell^\infty.\quad\quad\blacksquare$ $\textbf{b)}$ Give an example of sequences $(f_n)_n$ and $(g_n)_n$ in $\ell^1$ with $$\|f_n\|_\infty\to0,\,\|f_n\|_2\to\infty\quad\text{and}\quad\|g_n\|_2\to0,\,\|g_n\|_1\to\infty.$$ For the first example I cooked up the following sequence $$f_n(k)=\frac{1}{n^{1/4}}\text{ for all }k\leq n\quad\text{and}\quad f_n(k)=0\text{ for all }k>n,$$ Then $(f_n)_n\subset\ell^1,$ and we have that $$\|f_n\|_\infty\to0\quad\text{and}\quad\|f_n\|^2_2=\sqrt{n}\to\infty.$$ The second example is the one I am having trouble cooking up. I tried the following strange looking sequence $$g_n(k)=0\text{ for all }k<n\text{ and all }k\geq 2n\quad\text{ and }g_n(k)=\frac{1}{k}\text{ otherwise}.$$ Then $(g_n)_n\subset\ell^1$ and we have that $$\|g_n\|_2\to0\text{ and }\|g_n\|_1\to\infty,$$ since the $\|\cdot\|_1$ -norm of $g_n$ approaches the tail of the harmonic series. $\quad\quad\blacksquare$ Could anyone please check if my proof of part (b) is correct? Thank you for your time and appreciate any feedback.","['proof-verification', 'sequences-and-series', 'functional-analysis', 'real-analysis']"
3287587,Extracting diagonal of $J^TJ$ via automatic differentiation like techniques,"First of all please, let me know if this question is more suited for scicomp.stackexchange.com or or.stackexchange.com , and sorry for my English and math skills. Pretty often i do some numerical optimization stuff with Gauss-Newton and automatic differentiation. It's pretty handy because Gauss-Newton requires to solve following system: $$
J^TJ\Delta x = -J^Tr
$$ where $r(x)$ is residuals function, $J$ is jacobian of $r$ , $\Delta x$ is step. 
This system can be solved via Conjugate gradient method which requires $Av$ like products. Automatic differentiation allow me to compute $Jv$ and $J^Tv$ via forward and reverse mode of graph based automatic differentiation, so i can easily combine it with CG. Unfortunately CG also needs preconditioner for fast convergence. Obvious choice for preconditioner is diagonal of $J^TJ$ . My question is - how can i quickly compute diagonal of $J^TJ$ if i know expression graph for $r$ and have implementation of $J^Tv$ and $Jv$ for each elementary function? Without symbolic differentiation, if it possible.","['numerical-optimization', 'derivatives', 'chain-rule']"
3287624,Verifying that a branching process is a Markov chain,"I would like to verify that the following model of a branching process creates a Markov chain - a Markov chain here defined as having the property that $\mathrm{P}\left\{\xi_{k+1}=a_{k+1} | \xi_{0}, \ldots, \xi_{k}\right\}=\mathrm{P}\left\{\xi_{k+1}=a_{k+1} | \xi_{k}\right\}$ where we take $\mathrm{P}\left\{\xi_{k+1}=a_{k+1} | \xi_{k}\right\}$ to be the random variable $\sum_{j=1 }^ m \mathrm{P}\left\{\xi_{k+1}=a_{k+1} | \xi_{k}=b_j\right\}\mathrm {1}_{\{\xi_k=b_j\} } $ (assuming $\xi_k $ has range $\{b_1,...,b_m \} $ ), and where $\mathrm{P}\left\{\xi_{k+1}=a_{k+1} | \xi_{k}=b_j\right\} $ is defined as the conditional probability $
\mathrm {P }\left\{\xi_{k+1}=a_{k+1} , \xi_{k}=b_j\right\} /\mathrm {P }\{ \xi_{k}=b_j \}, \ \mathrm {P }\{ \xi_{k}=b_j \} >0.$ Let $\xi_0=1 $ and for $k \ge 0 $ let $\xi_{k+1 }  = \eta_1^{(k) }+...+\eta_{\xi_k } ^{(k) } $ where the random variables $\eta _j^{(k) } $ are  mutually independent for any $j $ and any $k \ge 0 $ . Verify that $\{\xi_k \} $ is a Markov chain.","['probability-theory', 'markov-chains']"
3287643,Strange shape of the distribution of the sum of the binomial coefficients ${n\choose r^2}$over squares,"Update : Initially the question was posted for $a = 1$ . Now it has been generalized for any real $a > 0$ What is known about the distribution of the sum of the binomial coefficients over multiple of squares? My experimental data seems to suggest that for a given positive real $a > 0$ $$
s_{n,a} = 
\sum_{1\leq \lfloor ak^2 \rfloor\leq n}{n\choose \lfloor ak^2 \rfloor}=
{n\choose \lfloor 1^2 a \rfloor} + {n\choose \lfloor 2^2 a \rfloor}  + \cdots + {n\choose \lfloor r^2 a \rfloor} \approx \frac{2^n}{\sqrt{2an}}
$$ Clearly the sum will be dominated by the term closest to the central binomial coefficient which in this case is the square nearest to $n/2$ . What I found interesting is the shape of the histogram of the distribution of the ratios of the actual sum to its asymptotic estimate i.e. $\dfrac{s_n \sqrt{2an}}{2^n}$ are similar for all $a$ and look like an acr-sine distribution as mentioned in the comments. Histogram of distribution for $a = 1$ Question 1 : Why does it have an arc-sine like distribution? Question 2 Where does the spikes occur? E.g. for $a = 1$ , the spikes occur roughly at $1 \pm 1/6$ . Related question: What is the sum of the binomial coefficients $n \choose p$ over prime numbers?","['statistics', 'summation', 'number-theory', 'computational-mathematics', 'combinatorics']"
3287652,Fast Computation of the Divisor Summatory Function $\sum_{i=1}^x\sum_{d|i}d$,"Denote by $\sigma_1(n)$ the sum of the divisors of $n$ . For example, when $n=9$ we get $\sigma_1(9) = 1+3+9=13$ . Define $D(x) = \sum_{n=1}^x\sigma_1(n)$ . Varius methods are known for computing $D(x)$ , most rely upon the crucial observation that summing the divisors themselves rather than looking at each number and its divisors specifically, we get a faster computation. That is: $$D(x) = \sum_{d=1}^x d \cdot \bigg \lfloor \frac xd \bigg \rfloor$$ Simply put, for each divisor, we multiply the divisor by the amount of times it appears. This form addresses an $\mathcal{O}(n)$ computation. But noticing the floor function is constant for large ranges, it admits a faster, $\mathcal{O}(n^{1/2})$ computation: $$D(x) = \sum_{d=1}^{ \big\lfloor \frac {x}{\sqrt x+1} \big \rfloor} d \cdot \bigg \lfloor \frac xd \bigg \rfloor + \sum_{d=1}^{\sqrt x} d \cdot \Bigg (T \bigg(\bigg \lfloor \frac nd \bigg \rfloor \bigg ) - T \bigg(\bigg \lfloor \frac n{d+1} \bigg \rfloor \bigg ) \Bigg )$$ where $T(n)= \frac {n\cdot(n+1)}2$ is the $n$ 'th triangular number. Further simplifaction can result in an algorithm of similiar time complexity, but still noticably faster: $$D(x) = \sum_{k=1}^{\lfloor \sqrt{x} \rfloor} \left( k \cdot \left \lfloor {\frac{x}{k}} \right \rfloor  + T \left( \left \lfloor{\frac{x}{k}} \right \rfloor \right) \right) - T(\lfloor \sqrt{x} \rfloor) \cdot \lfloor \sqrt{x} \rfloor$$ Using the last formula, I can compute the correct result for $D(10^{18})$ in about 3.5 minutes (using Java). However, in https://oeis.org/search?q=87%2C8299&language=english&go=Search , (in the links section) terms up to $D(10^{36})$ are known. Extrapolating how long my algorithm would take, it seems to me that even when using a faster computer and a faster programming language, this would still be unfeasible using the last formula, which is $\mathcal{O}(n^{1/2})$ . I would assume an algorithm with a better time complexity was used (my guess would be $\mathcal{O}(n^{1/3})$ ). Am I wrong or is there really an even faster algorithm based on a more sophisticated mathematical observation behind it? If there is, I would like to know about it.","['summation', 'number-theory', 'divisor-sum', 'algorithms', 'computer-science']"
3287676,Show that the interior of manifold is open.,"Let $X$ be a manifold with a boundary. That is, for any $x\in X$ there is a neighborhood about $x$ diffeomorphic to an open subset of $H^k=\{(x_1,x_2,\ldots,x_k)\in R^k: x_k \geq 0\}$ . The boundary of $X$ , denoted, $\partial X$ , is the set of points whose image is has the last coordinate as zero. The interior of $X$ , $Int(X)$ , is the compliment of $\partial X$ in $X$ . Show that $Int(X)$ is open in $X$ . Attempt: Let $x\in Int(X)$ . Then there is a chart $(U,\varphi)$ , $U$ is a neighborhood of $x$ which is mapped diffeomorphically to $\varphi(U)\subset R^k$ . We want to show that there is some open set $W$ containing $x$ , such that $W\subset Int(X)$ . Pick any $W\subset U$ , then $\varphi(W)$ is an open set in $R^k$ . Therefore $W\subset Int(X)$ .","['differential', 'smooth-manifolds', 'real-analysis', 'general-topology', 'differential-geometry']"
3287680,Inducing one point closed subset with a closed subscheme structure so that the stalk of the subscheme is a field,"Let $(X,\mathcal O_X)$ be a Noetherian scheme. Let $x\in X$ be a closed point and $Y:=\{x \}$ . Is it always possible to make $Y$ into a scheme such that $(Y, \mathcal O_Y)$ is a closed subscheme of $(X, \mathcal O_X )$ and $\mathcal O_{Y,x}$ is a field ? If this is indeed true, then can we have the same result if more generally  we started with some $x \in X$ and set $Y=\overline { \{x \} }$ ? EDIT:  Some thoughts: Let $x \in X$ and $Y:=\overline { \{x \} }$ . Since $\{x\}$ is irreducible in $X$ , hence so is its closure $Y$ .  Let $\mathcal O_Y$ be a structure sheaf on $Y$ , then since $x \in Y$ is generic for $Y$ and $Y$ is irreducible, so $\dim \mathcal O_{Y,x}=0$ . So to find a closed subscheme structure $(Y, \mathcal O_Y)$ such that $\mathcal O_{Y,x}$ is a field, we just need to ensure $\mathcal O_{Y,x}$ is an integral domain. Also note that the answer to both the questions are positive if $X$ is affine.","['quasicoherent-sheaves', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3287696,Does this equation hold in general?,"I have a conjecture, that the following holds for arbitrary integers $n$ , $k$ such that $0\leq k \leq n$ : $$\sum_{a=0}^{n} \sum_{b=\max[0,a-(n-k)]}^{\min[k,a]} \sum_{c=\max[0,a-(n-k)]}^{\min[k,a]} a (n-a)!a!{k\choose{b}}{k\choose{c}}{n-k\choose{a-b}}{n-k\choose{a-c}}(-1)^{b+c}=2^{n-1}nk!(n-k)!$$ So far, I haven't been able to find a counter-example or to prove it. 
I checked for many values of $n$ and $k$ numerically and I found that it is true for $n\leq 200$ .","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3287760,Picard Lindelöf Theorem with missing domain of $f$,"In my lecture we introduced the Picard-Lindelöf Theorem the following way. Theorem (Picard-Lindelöf) : Let $f(t,y)$ be continuous on a cylinder $$D = \{(t,y) \in \mathbb{R} \times \mathbb{R}^d | |t-t_0| \leq a, |y-u_0| \leq b\}$$ Let $f$ be bounded and satisfy a Lipschitz condition in its second argument. Then the IVP $$u' = f(t,u)\hspace{15pt}u(t_0) = u_0$$ is uniquely solvable on the interval $I = [t_0 - T, t_0 + T]$ for some $T> 0$ Now I'm looking at the following IVP: $$u'(t) = u(t)^\frac{1}{4}\hspace{5pt}\forall t \geq 0\hspace{30pt}u(0) = 0$$ I'm asked to give to different solutions to the IVP and show that $f(x) = x^\frac{1}{4}$ is Lipschitz-continuous on every interval $[\epsilon, \infty)$ with $\epsilon > 0$ , but not on $[0, \infty)$ . I do understand the exercise and was able to solve it. I should take home, that the existence of multiple solution is possible, since the Lipschitz-condition in the Picard-Lindelöf theorem is not fulfilled. Question: Let's look over the fact that $f$ is not Lipschitz continuous in $0$ . How would I apply the Picard-Lindelöf theorem for an IVP, where $f$ is only defined for $t \geq t_0$ , since I can't define a cylinder $D$ as in the theorem?","['initial-value-problems', 'lipschitz-functions', 'ordinary-differential-equations']"
3287797,"If $\operatorname{Corr}(X,Y)=1$, then $ \operatorname{Corr}(X,Z)=\operatorname{Corr}(Y,Z)$","$\DeclareMathOperator{\Corr}{Corr}$ Given that I have three r.v. $X,Y,Z$ and $\Corr(X,Y)=1$ , can I then conclude that $\Corr(X,Z)=\Corr(Y,Z)$ ? I've tested on some data and found that it was true in my tests, but I've had no luck in regards to the math.","['statistics', 'covariance', 'correlation']"
3287800,Chi squared test,"In the Chi Squared Test we build up a statistic $Q$ which converges in law to a $\chi^2$ as the number $n$ of observations goes to infinity.  So, if $n$ is ""big enough"", we choose to approximate $Q$ with the $\chi^2$ . I don't understand this approximation.  How can we deduce information on $Q$ by knowing its limiting law?  The limit does depend only on an arbitrary tail of the sequence, hence doesn't depend on $Q$ .","['statistics', 'probability-distributions', 'probability']"
