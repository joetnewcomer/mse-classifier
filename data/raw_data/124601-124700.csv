question_id,title,body,tags
1884718,"Given $X$ compact and Hausdorff, $C$ a component of $X$, and $U$ open set containing $C$, show that $\exists V$ clopen such that $C\subset V\subset U$","Question I'm struggling with the following: Let $C$ be a connected component of a compact Hausdorff space $X$ and let $U$ be an open set containing $C$. Prove that there exists a clopen set $V$ such that $C\subset V\subset U$. I think I have a solution, but what I've ""proved"" is stronger: that $C$ must be itself clopen, so $C$ itself can function as the desired $V$. Attempt Let $\{C_{\alpha}\,|\,\alpha\in A\}$ be the collection of components in $X$. Since $X$ is normal (compact and Hausdorff implies normal) and each $C_{\alpha}$ is closed, we can find a collection of disjoint open sets $\mathcal{U}:=\{U_{\alpha}\,|\,\alpha\in A\}$ such that $C_{\alpha}\subseteq U_{\alpha}$ for each $\alpha\in A$. Now $\mathcal{U}$ is an open cover of $X$, so via compactness there exists a finite subcover, $\mathcal{U}_0=\{U_{\alpha_1},U_{\alpha_2},\ldots \}\subseteq\mathcal{U}$. It follows that there are finitely many components of $X$ and hence each component is open: given $C_{\alpha_i}$, we see that
$$
X-C_{\alpha_i}=C_{\alpha_1}\cup\cdots\cup C_{\alpha_{i-1}}\cup C_{\alpha_{i+1}}\cup\cdots\cup C_{\alpha_n},
$$
which is the finite union of closed sets. Is this correct or have I made a grave mistake? If I've indeed made a mistake, could anyone point me in the right direction? Also, apologies for the frequent postings, I have a qualifying exam in a week and I've studying quite a lot lately. Edit 1: I should add that this is the second part of a two part question. The first part was showing that the components and quasi-components coincide. Edit 2: It's also worth noting that I am not quite sure whether ""$\subset$"" means a proper subset or not in the statement of the problem (this was a question taken from a old qualifying exam and I'm not quite sure who wrote it). However, if it were a proper subset, then I believe I could think of a counterexample to the statement. (I'm thinking of two disjoint closed disks in $\mathbb{R}^2$ where the $C$ is open of the disks and $U$ is one of the disks in union with some open set strictly contained in the other closed disk.)","['general-topology', 'proof-verification', 'connectedness']"
1884738,Is $\int_0^\infty \frac{dt}{e^t-xt}$ analytic continuation of $\sum_{k=1}^\infty \frac{(k-1)!}{k^k} x^{k-1}$?,"The following power series apparently converges only for $-e \leq x <e$: $$f(x)=\sum_{k=1}^\infty \frac{(k-1)!}{k^k} x^{k-1}$$ We can use it to define a real function $f(x)$, analytic in that interval. However, we can also use an integral to define this function: $$f(x)=\int_0^\infty \frac{dt}{e^t-xt}=\sum_{k=1}^\infty \frac{(k-1)!}{k^k} x^{k-1}$$ In the interval of convergence of the series these two definitions are equivalent. However, for $x<-e$ the power series diverges, but the integral converges: Can the integral serve as the analytic continuation of $f(x)$ for $x<-e$? How to justify this? Edit The integral works great for the complex plane as well. Here I used the integral representation to plot the real and imaginary parts of $f(z)$: We have problems on the real line for $x>e$. For the power series - they converge on a disk $|z|<e$ (with the boundary possibly included):","['analytic-continuation', 'definite-integrals', 'sequences-and-series']"
1884758,Estimated value of $\sum \limits_{i=1}^{50} \frac{1}{2i-1}$? [duplicate],"This question already has answers here : Finite sum of reciprocal odd integers (3 answers) Closed 7 years ago . Anyone has any idea the estimated result(no need to be accurate) of $\sum \limits_{i=1}^{50} \frac{1}{2i-1}$? I think this problem is not hard but I don't have a generalized method to solve! And I also remember this problem related to a very famous probability problem, any one has any idea? Thank you!","['probability', 'sequences-and-series']"
1884759,Why is finding all subsets of a set (power set) an exponential problem?,I know it's function of $2^n$ where $n$ is the number of elements in the original set. But what is so doubling (100% growth) about finding all subset of a set? Usually for $2^n$ function we can represent it as a binary tree but I can't work my way to understand why it's exponential in this case. It's a property but I can't link it to usual methods of understanding $2^n$.,"['combinations', 'exponential-function', 'elementary-set-theory']"
1884774,What is the equation of the orthogonal group (as a variety/manifold)?,"I have been studying some elementary Lie theory recently, so I have been thinking about matrix groups as manifolds. Most simple examples of manifolds that we learn in high school or college even are solutions to algebraic equations (for example, conic sections) i.e. algebraic varieties. This led to the thought (which is also my question): Is the orthogonal group also a manifold which is the solution to some multivariable polynomial equation? If so, what is it? Proposed answer: If $X$ denotes the $n \times n$ real-valued matrix whose $ij$th entry is the monomial/variable $x_{ij}$. Then orthogonal group is the zero set of the system of polynomial equations (in $n^2$ variables): $$X^T X - I_n = 0$$ Is this really the answer? It seems too dumb to be possible. Then again, I have never really thought of an entire collection of matrices as the set of solutions to a system of multilinear equations before. Check : this implies that the determinants of these matrices are $\pm 1$ by the multiplicative property of determinants. Check : for the case $n=1$ we have $$x^2 -1 = 0 \implies x = \pm 1$$ which is O(1) as necessary/expected. Check : for the case $n=2$ we have $$\begin{bmatrix} x_1 & x_3 \\ x_2 & x_4 \end{bmatrix}  \begin{bmatrix} x_1 & x_2 \\ x_3 & x_4 \end{bmatrix} - \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = 0 \implies \begin{array}{c}  x_1^2 + x_3^2 = 1 \\ x_1x_2 + x_3x_4 = 0 \\ x_2 x_1 + x_4 x_3 = 0 \\ x_2^2 + x_4^2 = 1 \end{array}$$ I know that O(2) has two path components, each of which is diffeomorphic to SO(2), which is just the unit circle, and the solution to this system of equations does suggest the disjoint union of two unit circles because of the equations $x_1^2 + x_3^2 =1$ and $x_2^2 + x_4^2=1$; however I am not sure how to interpret the auxiliary condition that $$x_1 x_2 = -x_3x_4$$ does this guarantee that the circles are disjoint somehow? Otherwise I am not sure if this equation is giving me the correct answer for the case $n=2$.","['matrix-equations', 'algebraic-geometry', 'proof-verification', 'orthogonal-matrices', 'lie-groups']"
1884800,Ratio of area of 2 triangles in a hexagon,I have no idea how to solve this question and it would be great if someone could help me with this.,"['area', 'geometry']"
1884818,"How many ways are there to split a dozen people into 3 teams, where one team has 2 people, and the other two teams have 5 people each?","Problem: How many ways are there to split a dozen people into 3 teams, where one team has 2 people, and the other two teams have 5 people each? Attempted Solution: $$N_\text{Teams} = {12 \choose 2}{10 \choose 5}{ 5 \choose 5} = {12 \choose 2}{10 \choose 5}$$ Arriving at this solution by first picking the team of 2 and then the two teams of 5, however the provided solution requires that my answer be divided by 2 due to ""Overcounting"" which is briefly mentioned but not explained. How does one gain an intuition for overcounting in this problem and also for counting problems in general?",['combinatorics']
1884819,Triangle Equality in a normed linear space,"The following statement is true or false: If $x, y$ are elements of a normed linear space, then $$\|x + y\| = \|x\| + \|y\| \iff x = 0\ \text{or}\ y = tx$$
for some $t ≥ 0$. What I have tried is as follows: It is clear that if $x = 0\ \text{or}\ y = tx $ then the equality will hold. But for the converse part, let
\begin{align*}
\|x+y\|& =\|x\|+\|y\|\\
\implies \|x+y\|^2 & =(\|x\|+\|y\|)^2\\
\end{align*} 
After that I stuck. Also in the following article Characterization of the norm triangle equality , I have read that in the case of a strictly convex normed linear space $V$, the
equality $\|x + y\| = \|x\| + \|y\|$ holds for nonzero vectors $x,y ∈ V$ if and only if $\frac{x}{\|x\|} = \frac{y}{\|y\|} $. Thank you for the help.","['functional-analysis', 'normed-spaces']"
1884836,How does one solve for a generalized quadratic in multiple dimensions as in $a w^Tw + b^T w + c = 0$?,"Consider: $$a w^Tw + b^T w + c = 0 $$ I would like to find a vector $w \in \mathbb{R}^D$ that satisfies the above equation. I realize that I need to choose D numbers to satisfy 1 equation, so my intuition (from linear algebra) tells me that there might be an infinite set of solutions for this (1 equation D unknowns). Thus, I thought that maybe additional constraints are needed. I am honestly not too worried about which one to choose as long as it satisfies the equation. However, if I really had to choose a criterion to choose a $w$ I would first require it to be real vector and perhaps choose $w$ closest to some other fixed (aprior chosen) $x$ (say for simplicity closest to the origin $x=0$). So the additional constraint is: $$ min_w \| w - x\|^2 $$ or even $$ \| w - x\|^2 = 0 $$ another constraint could be (normalized vector) is also good: $$  \| w \|^2 = 1 $$ I mostly care to satisfy the ""generalized quadratic"", the additional constraint is there just incase its needed (no complex number solutions). Also Ideally I'd like to implement the solution in a maths program, for example, python or matlab.","['optimization', 'algebraic-geometry', 'multivariable-calculus', 'numerical-methods', 'quadratic-forms']"
1884889,A combinatorics problem related to Bose-Einstein statistics,"Problem Given $N$ and $E$, how many solutions $(n_0,n_1,...,n_E)$ are there to:
$$\sum_{k=0}^En_k = N$$
$$\sum_{k=0}^Ek.n_k = E$$
where everything is non-negative integer ($N,E,n_k \in \mathbb{Z}_{\geq 0}$ )? Context $N$ is the number of indistinguishable particles that can occupy $Q+1$ distinguishable energy levels, with energies $E_k=0,1,2,...,Q$. In this case, $Q=E$, which is the total energy of the system. Here's an example for $N=6$ and $E=9$: Each rectangle represents a solution. There are 26 in total. The solution on the top left, for example, is $(5,0,0,0,0,0,0,0,0,1)$. Source: HyperPhysics . Attempts The equivalent problem for distinguishable particles is trivial if solved by the stars and bars method . The solution is given by: $$\binom{E+N-1}{E}$$ For $N=6$ and $E=9$ it would be: $$\binom{9+6-1}{9} = \frac{14!}{9!\;5!} = 2002$$ I don't know if this is relevant, but I noticed that $2002 = 77\times 26$, or $77$ times the number of solutions to the problem I want to solve. There might not even be a closed formula. Many simple combinatorics problems don't seem to have a simple solution. Maybe the problem can be solved by generating functions , as suggested by the answer to this related question , but I have no idea how that works.",['combinatorics']
1884933,Show $y^Ty-n^{-1}(\sum y_i)^2=y^T(I-n^{-1}1_n1_n^T)y=\sum(y_i-\bar y)^2$?,Here is the picture of my notes: How does $y^Ty-n^{-1}(\sum y_i)^2=\sum(y_i-\bar y)^2$? I can see that: $y^Ty-n^{-1}(\sum y_i)^2=y^T(I-n^{-1}1_n1_n^T)y$ I also see that $\frac{n}{n^2}(\sum y_i)^2=n\bar y^2$,"['statistics', 'linear-regression', 'least-squares']"
1884953,What are the higher dimensional analogues of left and right handedness?,"The dimensions of a set of three axes can be arranged in two ways; left or right handed.  Cartesian co-ordinates are by convention always oriented to comply with the right-hand rule.  It would seem this rule can be thought of as a cyclic transformation of order 3 which takes us from one axis to the next. What are the analogues for left- and right- handedness in higher dimensions?  Particularly in infinite dimensions? We know that three is geometrically special arising out of the parallelisability of three-sphere, and the only higher dimensional space in which this happens again is 7-dimensions so is there only an analogue in 7-dimensions or can the concept be extended to other spaces? In particular... and this is just a bit of background perhaps not material to the question.  I'm interested in a space I'm constructing to study number theory in which every axis represents a prime number and every point along that axis represents an increment in the power of that prime number, so along the first axis we have 2, 4, 8, ... and on the 2nd axis we have 3, 9, 27, ...  By this means every co-ordinate in the infinite-dimensional space represents a unique natural number given by the product of its co-ordinates.  If the points along each axis are then spaced according to their square root, Pythagorus theorem guarantees that all points in the infinite-dimensional space are well-ordered by their distance from the origin, which is equal to the square root of the log of natural number they represent. What I want to bring some understanding to, is how the process of counting in this space is described by some translation from any $x$ to $x+1$, and whether there might be some sense to be made of the rotation between axes that takes place with each increment.","['number-theory', 'dimension-theory-analysis', 'rotations', 'geometry']"
1884985,"How to give a ""quick proof"" of properties of the Hecke-operators?","In ""Analytic Number Theory"" by Kowalski, Iwaniec p. 370 the authors prove a formula for the Fourier-coefficients of the Hecke-operators. Namely: If $$T(n) f(z) = \frac{1}{n} \sum_{ad = n} a^k \sum_{0 \leq b < d } \sum_m a_f(m) e\left( m \frac{az+b}{d} \right) = \sum_{m=0}^\infty b(m)e(mz)$$
is the Hecke operator $T(n)$ applied to a weight $k$ modular form $f = \sum_{n\geq 0} c(n) e(nz)$ (let's assume, contrary to the book, that $f$ is a modular form over the full modular group), then the formula derived for $b(m)$ then is $$\sum_{d \mid (m, n)} d^{k-1} c\left( \frac{mn}{d^2}\right)$$ The authors then state that this can be used to ""give [a] quick proof"" of $$T(n) T(m) = \sum_{d \mid (m, n)} d^{k - 1} T\left( \frac{mn}{d^2} \right)$$
I've tried to do the proof using that the Fourier-coefficients determine a modular form. But I got only long expressions for the Fourier coefficients of the LHS and the RHS, and couldn't really conclude anything. So my question is: How can I prove this quickly using the fomula for the Fourier-coefficients? Thanks! Added: I've had the following idea: One can prove that the cusp-forms have a basis of normalized eigenforms. For eigenforms one can derive the desired formula by our expression of the Fourier-coefficients (which happen to be the eigenvalues of the eigenforms), which then holds for the whole space of cusp-forms. However, showing that the space of cusp-forms has a basis of normalized eigenforms is not a quick proof. I still think I'm missing something here...","['analytic-number-theory', 'number-theory', 'complex-analysis', 'automorphic-forms', 'modular-forms']"
1885011,Inconsistency in a Z-transform of an Euler equation?,"In this thesis, p. 27, the following Euler equation is given, see (4.9):
$$
u_{i+1,j}=\frac{h_t}{2h_x^2}\left[\left(\frac{2h_x^2}{h_t}+2a_1h_x^2-4a_2\right)u_{i,j}+(2a_2-a_3h_x)u_{i,j-1}+(2a_2+a_3h_x)u_{i,j+1}+2b_1h_x^2\right]
$$ Here, $i$ represents the time and $j$ represents the (1d-) space. Moreover, $h_t$ and $h_x$ are the step sizes with respect to time and space, respectively. $a_1,a_2$ and $b_1$ are constants. Then, on both sides, the z-tranformation (with respect to time) is applied, where
$$
\mathcal{Z}(u_{i+1,j})=:U_j,~\mathcal{Z}(u_{i,j})=z^{-1}U_j,~\mathcal{Z}(u_{i,j-1})=z^{-1}U_{j-1},~\mathcal{Z}(u_{i,j+1})=U_{j+1}.
$$ Due to the linked thesis, this gives (4.10)
$$
U_j=\frac{h_t}{2h_x^2}\left[\left(\frac{2h_x^2}{h_t}+2a_1h_x^2-4a_2\right)z^{-1}U_j+(2a_2-a_3h_x)z^{-1}U_{j-1}+(2a_2+a_3h_x)z^{-1}U_{j+1}+\color{blue}{2b_1h_x^2}\right]
$$ I am really wondering about the blue summand! Why isn't it
$$
\mathcal{Z}(2b_1h_x^2)=2b_1h_x^2\mathcal{Z}(1)?
$$ Is there some reason for that or is it just a mistake? I would really prefer that there is some reason for it. :-) In particular this seems relevant since the author is interested in poles and zeros $z$ of the z-transform with $\lvert z\rvert <1$ for stability reasons; but for $\lvert z\rvert <1$, we have that $\mathcal{Z}(1)$ diverges. Moreover, the determination of the zeros and poles does not work as done in the thesis in case the summand is $2b_1h_x^2\mathcal{Z}(1)$.","['z-transform', 'discrete-mathematics']"
1885023,What is the motivation behind the solution of this olympiad problem?,Is there a particular reason why solutions in arithmetic progression were sought for? Can you think of an alternate solution? Note : This is a problem from the Indian National Mathematical Olympiad 2001.,"['number-theory', 'contest-math']"
1885047,Skew lines and what's between them,"Is it always possible to find a line perpendicular to two skew lines in space?
And how can we visualise the proof geometrically? And if anyone could present the proof that it is always possible to exist a line perpendicular to both skew lines, please elaborate.",['geometry']
1885049,Can we express the value of $\sin 1^\circ$ without using the imaginary unit?,"I've been playing with sine of integer-degree angles; that is, $\sin\left(\frac{k \pi}{180}\right)$ , where $k$ is an integer. I've noticed that you can divide the angle by $2$ and get sine of smaller and smaller angle by solving a quadratic equation for the correct root. One can also divide the angle by $3$ and try to solve cubic equation. I found the expression for $\sin 3^\circ$ . While it's not very short, it consists of some square roots (possibly nested, but not necessarily). The point is: The expression for $\sin 3^\circ$ does not have to contain any mention of complex numbers. No roots of negative numbers, no $i$ 's in the expression. Of course, you can get smaller angles by dividing by $2$ , so you can get a nice expression for $\sin 1.5^\circ$ , etc. Then I've searched for sine of $1^\circ$ and $2^\circ$ , and  those expressions are somehow odd. They involve imaginary units. However, sine of real number is a real number, so in the end, those expressions shouldn't have a imaginary part. So I thought, ""Okay, I will play with these expressions, and eventually I should get something real consisting of some roots (possibly nested), but no complex numbers."" And here's the thing: No matter how hard I tried, my expression either reverted simply to "" $\sin 1^\circ$ "" (which is nice, but does not tell anything apart from fact that the expression was correct), or else it involved complex numbers. Here's my question: Is there a way to express $\sin 1^\circ$ through square/cubic roots (possibly nested) of positive real numbers, without any involvement of the imaginary unit? If yes, how? If not, why not? (In the latter case: Where is the point? What's the essence of having purely real expression vs. some kind of complex mid-steps though the answer is real? How's that connected to a $1^\circ$ vs $3^\circ$ ?) Thank you! Sorry if it's a stupid question with an obvious answer.",['trigonometry']
1885054,"If $f$ is continuous and $f(x+y)=f(x)f(y)$, then $\lim\limits_{x \rightarrow 0} \frac{f(x)-f(0)}{x}$ exists","I'm solving the functional equation $f(x+y)=f(x)f(y)$ and I know that I have a continuous function $f:[0,\infty\rangle \to \langle 0,\infty\rangle$ s.t. $f(0)=1$. In one of the steps, I want to show that the limit $$\lim_{x \rightarrow 0} \frac{f(x)-1}{x}$$ exists and is finite? I'm just looking for a hint.","['functional-equations', 'calculus', 'limits']"
1885066,Is it possible to find aproximation of conformal map from sequences of complex points?,"I want to find equation of conformal map (= Fatou function $\Psi : z \to u$ )  which: maps some region of complex plane ( attracting petal) to right half of complex plane in u coordinate $Re(u) > 0 $ transforms function $f(z)$   to unit translation $ F : u \to u+1$ unrolls invariant curvs ( orbits ) : maps  ""circles"" to straight lines Can I find equation which aproximates such map from sequences of points ( complex numbers) ? The easiest case is $f(z)= z^2 + z$ which has parabolic fixed point at origin ( z=0). 
Then $\Psi(z) = -1/z$  and $F : u \to u+1+1/(u-1)$, where $2/(u-1)$ is error term ( Adrien Douady, Does a Julia set depend continuously on the polynomial? ) Sequences lay along curves shown inside main chessboard boxe on this image The image is not perfect near boundaries of chessboard box ( there are kinks and curves seems to cross boundary ) On this image one can see the u and z planes for th case f(z)=z^2+z. Src code","['functional-equations', 'complex-dynamics', 'numerical-methods', 'conformal-geometry', 'sequences-and-series']"
1885068,Prove $\int_0^1 \frac{x-1}{(x+1)\log{x}} \text{d}x = \log{\frac{\pi}{2}}$,Prove $$\int_0^1 \frac{x-1}{(x+1)\log{x}} \text{d}x = \log{\frac{\pi}{2}}$$ Tried contouring but couldn't get anywhere with a keyhole contour. Geometric Series Expansion does not look very promising either.,"['integration', 'definite-integrals', 'contour-integration', 'closed-form']"
1885089,Find all positive integers $n$ such that $\frac{2^{n-1}+1}{n}$ is integer. Where I'm wrong?,"The problem statement is: Find all positive integers $n$ such that $\frac{2^{n-1}+1}{n}$ is integer. Source: LTE Amir Houssein Pavardi P.25 My approach: It is clear that $n=1$ is a solution, so supose that $n>1$. $2^{n-1}\equiv -1$ $mod$ $n$; $2^{2(n-1)}\equiv 1$ $mod$ $n$; Observe that $n$ is odd hence $n-1$ is even. But if $p=ord_n(2)$ then $p|2(n-1)$ but not $(n-1)$ so $p|2$ how ever it is a contradiction because $n-1$ is even. So the unique solution is $n=1$ Is this proof correct? Because I'm not sure if order properties does apply to composite modulo. And if anyone has a solution involving LTE trick please post a partial proof Thanks in advance!","['number-theory', 'diophantine-equations', 'modular-arithmetic', 'proof-verification']"
1885090,"Logarithm limit, almost Riemann sum $\lim\limits_{n\to\infty}\frac1n\sum\limits_{k=1}^n\log\frac kn=-1$","I want to show that $\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^n\log\frac{k}{n}=-1$. Now I could say that $\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^n\log\frac{k}{n}=\int_0^1\log xdx$ but I can't as $\log x$ isn't uniform continuous on the given interval. I found a way which goes over the convergence radius of a power series connected to the term in the limit, but I think one could just make the argument with the riemann sum work somehow?!","['real-analysis', 'limits', 'logarithms', 'summation', 'sequences-and-series']"
1885107,Ways to arrange a word so that no vowel is isolated between two consonants,"Consider a seven-letter word formed by mixing up the letters in the word COMBINE. How many ways can you do this if no vowel is isolated between two consonants? (eg. EBMCION and MOIENCB are acceptable, but BEMCNIO is not) The final answer is given to be $1872$. We are being asked to find the numeric value of (Total number of seven-letter words) - (Number of words such that a vowel is isolated between two consonants). The word ""COMBINE"" has $4$ consonants and $3$ vowels. To compute the number of constructed words with an isolated vowel between two consonants,  I tried the following: 1st attempt: We may generalize the case, representing consonants as $1$s and vowels as $0$s. Now, we are finding the total number of seven-digit binary strings with four $1$s and three $0$s, containing the substring $101$. 
There are $\frac{4!}{2!2!} = 6$ strings that can be formed from the remaining two $0$s and $1$s. We may insert the substring $101$ into any position in these 6 strings. We have five available positions, because there are four digits. Therefore, there are $6 \times 5=30$ seven-digit binary strings with four $1$s and three $0$s, containing the substring $101$. Since each vowel and consonant is unique in the word COMBINE, there are $30 \times 4!\times3!=4320$ seven-letter words formed from COMBINE that contain an isolated vowel between two consonants, if we account for permutations of the vowels and consonants. However, $7!-4320=720 \neq 1872$, meaning my answer is incorrect. 2nd attempt: Since there are three vowels, for there to be an isolated vowel between two consonants, either all vowels are separated, or two vowels may be grouped up together while the remaining vowel is isolated (there is no other case). Case when all vowels are isolated: First, we arrange the consonants, Then, we choose three out of five possible places to place the vowels (.C.C.C.C., where C denotes a consonant and the periods represent possible places to put a vowel). We finally account for the arrangement of the vowels.
$$ P(4,4) \times C(5,3) \times P(3,3) = 1440.$$ Case when one vowel is isolated, while the remaining two are a pair: First, we arrange the consonants. We then choose a vowel. Then, we choose a position out of three possible places to put it (C.C.C.C). Then, we choose a position out of four possible places to place the pair of the remaining vowels (.CVC.C.C. just as a possible example). Finally, we account for the arrangement of the vowels in the pair.
$$ P(4,4) \times C(3,1) \times C(3,1) \times C(4,1) \times P(2,2) = 1728.$$
$7!-(1440+1728) = 1872$, which confirms that this is the correct answer. What is wrong with my first attempt? Where did I make my error?","['combinatorics', 'discrete-mathematics']"
1885110,Life after linear algebra and multivariate calculus,"I have been following Strang's Linear Algebra course, and found it quite challenging. My goal, however, is to learn application of linear algebra and calculus in applied statistics (regression, linear mixed models, structural equation modeling, et cetera). I want to understand the math behind these techniques. Some people suggest learning abstract algebra after linear algebra. However, after trying one to two lectures by Benedict Gross (on youtube), I find that it is totally not for me - I was lost very soon about what he talked about. In addition, I am unsure if abstract algebra is very useful to applied statistics. Therefore, I am unsure what I should learn after linear algebra and calculus, and I preferably want to learn something with online videos (as self-learning stats can be quite difficult).","['statistics', 'linear-algebra', 'calculus']"
1885116,Heat equation to mixed boundary conditions,"I'm studying heat equations, I've more or less understood how to solve them when given two boundary conditions $u(0,t)=u(L,t)=0$  and when given $\frac{\partial u(0,t)}{\partial x}=\frac{\partial u(L,t)}{\partial x}=0$, for $0<x<L$. My problem starts when asked to solve for a mix of the two: $u(L,t)=\frac{\partial u(0,t)}{\partial x}=0$, The method described in the book details using separation of variables to define $u=X(x)T(t)$ to arrive at $X''-\lambda X= 0 $ Setting $\lambda=-k^2$ gives: $X(x)=Asin(kx) + Bcos(kx)$ But somehow applying the boundary conditions gives: $X(x)=\cos(\frac{(2n+1)\pi x}{2L})$ Would someone mind explaining how this is done? I get $A=0$ from the insulating boundary condition and $B \cos(kL)=0 => k=\frac {n \pi}{L}$ from the other BC. Not sure where the $(2n+1)$ is coming from...","['heat-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
1885126,How many sets can be formed by choosing one member from all subsets of a partition?,"Assume we are given disjoint finite sets $A_1,\ldots,A_n$ with $A=\bigcup_i A_i$. Is there a closed formula for the number $t_n$ of sets can we form with exactly one element chosen from each member of all non-empty subsets of $\{A_1,\ldots,A_n\}$? For example, if $n=2$ then the non-empty subsets are $\{A_1\}$, $\{A_2\}$ and $\{A_1,A_2\}$. Suppose $|A_1| = 3$ and $|A_2| =4$. The number of ways we can chose one element from the members of $\{A_1\}$ is $3$. The number of ways we can choose one element from the members of $\{A_2\}$ is $4$. Finally, the number of ways we can choose one element from each of the members of $\{A_1,A_2\}$ is $3\times 4=12$. Thus the total number of sets is $3+4+12=19$. In general $t_n$ is given by the recurrence relation:
$$
t_k = 
\begin{cases}
|A_1| & \text{if } k=1 \\
|A_k| t_{k-1} + |A_k| + t_{k-1}&\text{otherwise}
\end{cases}
$$ but I can't find a closed form. I suspect there is one that makes clever use of the multinomial theorem. On the other hand, there was no closed form provided for the simpler(?) case when the members of each $A_i$ are the same (as posed here ).","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
1885133,"$G$ is an abelian group of order $n$ with the property that $G$ has at most $d$ elements of order $d$, for any $d$ dividing $n$. Then $G$ is cyclic.","$G$ is an abelian group of order $n$ with the property that $G$ has at most $d$ elements of order $d$, for any $d$ dividing $n$. Then $G$ is cyclic. I am not getting any clue how to start. Please Help.","['abstract-algebra', 'group-theory', 'cyclic-groups']"
1885161,Reference request that explains the logic and ideas behind auxiliary constructions in synthetic geometry,"I'll be frank. I'm really, really bad at synthetic geometry problems that require auxiliary constructions before they can be solved. I just don't know which lines I should draw to reduce the problem to angle chasing or something as simple. At the moment, constructions are a really hit-or-miss thing for me. Just keep drawing lines and eventually something will click, right? Well it might work for some problems, but as they get harder, I don't think it'll work anymore. When I look at the solutions for the problems, I can understand what is going on, and how the construction makes it simpler, but what I can't understand is how or why someone would think of constructing those lines. As a result, I can't apply the logic or method of thinking elsewhere. I've tried to classify the logic with each problem I solve into a set of principles. For example, if I see a right angled triangle, I try drawing a median from the right angle vertex, to see if that simplifies the problem by introducing isoceles triangles by the converse of Thales' Theorem. But this is just proving to be a really tedious, and honestly not very reliable or effective, process. In short I'd like a reference where the author describes constructions through example problems in an almost algorithmic or systematic way. Or at least manages to help better my judgement in what constructions to make, as well as allowing me to see why those constructions would simplify things in the short run, if not the whole problem. I'm fine with any kind of reference: textbooks, online notes, blogs, or websites. I would prefer it if the reference were freely available though, like a free pdf.","['intuition', 'reference-request', 'euclidean-geometry', 'geometry']"
1885187,How can I get from one factored form to another?,"This came from trying to factor $x^6 + 1$. We know that $x^4 - x^2 + 1 =  (x^4 +2 x^2 + 1) -3x^2 = (x^2+1)^2 - 3x^2 = (x^2 - \sqrt 3 x + 1)(x^2 + \sqrt 3 x + 1)$ I expected that if I factored the $x^4 - x^2 + 1$ completely into $4$ factors and then regrouped them $2$ by $2$, I could arrive at $(x^2 - \sqrt 3 x + 1)(x^2 + \sqrt 3 x + 1)$ $$\left(x+ \sqrt {\dfrac {1 + \sqrt 3 i}{2}}\right) \left(x+ \sqrt {\dfrac {1 - \sqrt 3 i}{2}}\right) \left(x- \sqrt {\dfrac {1 + \sqrt 3 i}{2}}\right) \left(x- \sqrt {\dfrac {1 - \sqrt 3 i}{2}}\right)$$ But that didn't happen. No matter how I group them into quadratics, I always end up with a complex coefficient. How can I get $(x^2 - \sqrt 3 x + 1)(x^2 + \sqrt 3 x + 1)$ from the $4$ factors?","['algebra-precalculus', 'factoring', 'calculus']"
1885191,Assessing a distribution from multiple samples of its mean,"I face a random variable whose distribution I don't know. Someone draws a sample of k observations from a population and tells me their average. He repeats the process m times. I assume m is in order of magnitude of hundreds. If 1 < k < 20, What can I tell about the population variance? What about other lower moments? If k=1, I can trivially draw the emplirical distribution. What is the closest analoug for 1 < k < 20?","['statistics', 'probability-distributions']"
1885223,a self adjoint in complex vector space,"Let $V$ be a complex vector space, with Hermitian inner product $\langle z,w\rangle$ . Let $T : V → V$ be a linear transformation. Show that $T$ is self adjoint if and
only if $\langle Tz,z\rangle$ is real for every $z ∈ V$ . My solution is: In the left side: $T$ is self-adjoint $\Leftrightarrow$ $T=T^*$ $\Leftrightarrow$ $T=UAU^*$ where $UU^*=I$ and A is a diagonal matrix. In the right side: $\langle Tz,z\rangle$ is real for every $z ∈ V$ $\Leftrightarrow$ $z'T'\overline{z}$ is real $\Leftrightarrow$ $T=UU^*$ . So there is some discrepency between the two sides. Can you tell me which step is wrong?","['complex-analysis', 'vector-spaces']"
1885239,"Multivariable limit $\lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2}$ [duplicate]","This question already has answers here : Prove that $x\sqrt{1-x^2} \leq \sin x \leq x$ (3 answers) Closed 7 years ago . I'm learning multivariable calculus, specifically multivariable limits and continuity, and need help to understand the solution to the following problem: Let $$f(x,y) = \begin{cases} \frac{x \sin(x^2 y^2)}{x^2 + y^2}, & (x,y) \neq (0, 0) \\ 0, & (x,y) = (0, 0).  \end{cases}$$ Show that $f$ is continuous at $(0, 0)$ . So we need to show that $$\lim_{(x, y) \to (0, 0)} f(x, y) = f(0, 0)$$ that is $$\lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2} = 0.$$ Solution: (from textbook) We have that $$\left| \frac{x \sin(x^2 y^2)}{x^2 + y^2}  \right| = \frac{\left|x\right| \left|\sin(x^2 y^2)\right|}{x^2 + y^2} \leq \frac{\left|x\right|x^2 y^2}{x^2 + y^2}  \leq \frac{\left|x\right|^3 y^2}{x^2 + y^2} \leq \left|x\right|^3.$$ Question: I don't understand how the author found the upper bound for the sine function. Why does $\left|\sin(x^2y^2)\right| \leq x^2y^2$ ? When I first tried to solve this problem I used $\left|\sin(x^2y^2)\right| \leq 1$ without success. Can someone explein my error?","['multivariable-calculus', 'continuity', 'calculus', 'limits']"
1885253,How many k-digit numbers are both divisible by 3 and include the digit 3?,"At first I thought I could simply count how many (k-1)-digit numbers are divisible by 3 and multiply by k, accounting for the different possible placements of the final 3. But It seems that method includes duplicates. Any direction, such as Inclusion-Exclusion or something cleverer would be appreciated. I'd like to be able to apply it to 9 instead of 3 as well. (A general method for any digit d would actually solve the original problem eliciting this question.)","['number-theory', 'combinatorics']"
1885291,Computing $\int_0^\infty\frac{1}{(1+x^{2015})(1+x^2)}$ [duplicate],This question already has answers here : Is the integral $\int_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$ equal for all $a \neq 0$? (4 answers) Closed 4 years ago . How can I compute $$\int_0^\infty\frac{1}{(1+x^{2015})(1+x^2)}\quad?$$ My attempt: Looking at the limits of the integration I see that we should induce some $\tan^{-1}(x)$ so if we put $\infty$ we would get something like $\frac{\pi}{2}$ . But I am not sure how to proceed . Partial fractions don't yield good integral.,['integration']
1885311,$\frac{1^2+2^2+\cdots+n^2}{n}$ is a perfect square,"Prove that there exist infinitely many positive integers $n$ such that $\frac{1^2+2^2+\cdots+n^2}{n}$ is a perfect square. Obviously, $1$ is the least integer having this property. Find the next two least integers with this property. The given condition is equivalent to $(2n+2)(2n+1) = 12p^2$ where $p$ is a positive integer. Then since $\gcd(2n+2,2n+1) = 1$, we have that $2n+2 = 4k_1$ and $2n+1 = k_2$. We must have that $k_1$ is divisible by $3$ or that $k_2$ is divisible by $3$. If $k_1$ is divisible by $3$ and $k_2$ is not, then we must have that $k_1$ is divisible by $9$ and so $2n+2 = 36m$. Then we need $3mk_2$ to be a perfect square where $k_2+1 = 36m$. Thus if $3mk_2 = r^2$, we get $m = \dfrac{1}{72}\left(\sqrt{48r^2+1}+1\right)$.","['number-theory', 'square-numbers']"
1885312,Ergodicity of tent map,"The dynamical system $T:[0,1]\to [0,1]$ defined by $$T(x) = \begin{cases}
       2x, & \text{for } 0\leq x\leq \frac{1}{2}\\
        2-2x, & \text{for }  \frac{1}{2}\leq x\leq 1
        \end{cases}$$
is called the tent map. Prove that $T$ is ergodic with respect to Lebesgue measure. My work: Measure preserving map is ergodic if for every T-invariant measurable set is $m(A)=1$ or $m(A)=0$ (by definition). Let's prove that $T$ preserves Lebesgue measure. It is enough to prove for intervals $[a,b]$ that $m(T^{-1}[a,b])=m([a,b])$ holds. Since $T^{-1}[a,b]=[\frac{a}{2},\frac{b}{2}]\cup [\frac{2-b}{2},\frac{2-a}{2}]$, we have $m(T^{-1}[a,b])=m([\frac{a}{2},\frac{b}{2}]\cup [\frac{2-b}{2},\frac{2-a}{2}])=m([\frac{a}{2},\frac{b}{2}])+m([\frac{2-b}{2},\frac{2-a}{2}])=b-a=m([a,b])$. Therefore, $T$ is measure-preserving. Now, I'm stuck and don't know how to prove that map is ergodic. Any help is welcome. Thanks in advance.","['dynamical-systems', 'ergodic-theory', 'measure-theory']"
1885342,Evaluate $\sum_{n=0}^{\infty} \frac{n!}{(n^2)!}$,"I'm interested in a method of evaluating $\sum_{n=0}^{\infty} \frac{n!}{(n^2)!}$. If there was a linear equation with leading coefficient $1$ in the denominator or a quadratic  with leading coefficient $1$  and a coefficient of $0$ on $x$ in the numerator I would've used partial fraction, or a method seen in this answer by Jack Evaluate $\frac{0!}{4!}+\frac{1!}{5!}+\frac{2!}{6!}+\frac{3!}{7!}+\frac{4!}{8!}+\cdots$ . But those approaches don't seem to be working for this specific series. Is it even possible to evaluate this to get a closed form?","['sequences-and-series', 'calculus']"
1885352,"How many ways are there to choose three subsets $A, B$ and $C$ of $\{1,...,n\}$ such that $A \subseteq B \subseteq C$?","What I've been thinking is that there are $2^n$ ways of choosing $A$, then from the remaining elements of $\{1,...,n\}$ I could choose any subset (there would be $2^{n-|A|}$ ways to do that) and add that to A and form B, using this same procedure we can then construct C. The total ways to choose the three subsets would be:
$$ \sum_{k=0}^{n}\sum_{j=k}^{n} 2^{n}2^{n-k}2^{n-j}$$ I've tried this formula for the case n=1 and it does not work but I don't know where I went wrong.",['combinatorics']
1885376,Description of $\mathcal{O}_x(n)$.,"For any $n \in \mathbb{Z}$, we define the sheaf $\mathcal{O}_X(n)$ to be $S(n)^{\sim} $ where $X = \operatorname{Proj} S$ and $S$ is a graded ring. $S(n)$ is defined by shifting $n$-places to the left. For example, in the case that $S$ is a polynomial ring the constant terms, $S_0$ would now become terms of degree $n$. I have seen a heuristic explanation of $\mathcal{O}_x(n)$ as allowing functions on $X$ which have poles of degree $n$. As in the case when $S$ is a polynomial ring. We define the usual structure sheaf on $\mathcal{O}_x = S^{\sim}$ For each $U \subset X$, we define $S^{\sim}$ to be the set of of functions 
$$ s: U \to \bigsqcup_{\frak{p} \in U} S_{(p)} $$ where $S_{(p)}$ is the set of elements of degree zero in the localized ring $T^{-1}(S)$ where $T$ is the multplicative subset of all homogeneous elements of $S$ which are not contained in $\frak{p}$. The function $s$ have the following conditions: $s(p) \in S_{(p)}$ For each $\frak{p} \in U$, there exists an open subset $V \subset U$, and homogeneous elements $a,f$ in $S$ of the same degree, such that for all $\frak{q}$ $\in V$, $s(q) = \frac{a}{f}$ where $f \notin \frak{q}$. In other words, the function do not blow up locally for any $\frak{p} \in U$. So now I want to see what $\mathcal{O}_x(n) = S(n)^{\sim}$ allows our functions $s$ to have poles of degree $n$. In the original description of the structure sheaf it seems that poles were locally avoided by requiring that $f \notin \frak{q}$ for all $\frak{q}$ in near $\frak{p}$. First of all, let us look at $S(n)_{(p)}$. In $S_{(p)}$ our elements where fractions, $\frac{s}{a}$, of homogeneous elements of the same degree. Are the elements of $S(n)_{(p)}$ fractions of homogeneous elements which differ by $n$-degrees? Furthermore condition $(2)$ above avoids poles by demanding that the denominator of our fractions are not contained in any prime ideal. However, it seems like this demand would still be in place for $S(n)^{\sim}$. Why does $S(n)^{\sim}$ allow poles of degree $n$ but $S^{\sim}$ does not?",['algebraic-geometry']
1885389,Can the endpoints of the interval considered satisfy the mean value theorem?,"For example, if you have a graph $y=x$ and you want to find the values of $c$ that satisfy the mean value theorem for $x\in[1, 3]$, do the points $c=1$ and $c=3$ count as valid? I only ask because for a homework problem $y=-x^3+4x^2-3; [0, 4]$ find values of $c$ that satisfy the MVT, the answer was $0$ and $8/3$ but $0$ was not considered a valid answer because it was on the interval endpoint (to the best of my knowledge).",['derivatives']
1885405,"What is the nth term of the series? $0, 0 , 1 , 3 , 6 , 10 ....$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I am trying to find the relation between the number of nodes and the number of connections possible. So if there are $0$ nodes, that means $0$ connections possible, $1$ node still means $0$ connections possible, $2$ nodes $1$ connection possible, $3$ nodes $3$ connection and so on. How can I find the relation between $n$ nodes and the number of possible connections ?","['combinatorics', 'sequences-and-series']"
1885428,Prove $\sum_{n=1}^{\infty} \frac{n!}{(2n)!} = \frac{1}{2}e^{1/4} \sqrt{\pi} \text{erf}(\frac{1}{2})$,"I would like to prove: $$\sum_{n=1}^{\infty} \frac{n!}{(2n)!} = \frac{1}{2}e^{1/4} \sqrt{\pi} \text{erf}(\frac{1}{2})$$ What I did was consider: $$e^{-t^2}=\sum_{n=0}^{\infty} (-1)^n \frac{t^{2n}}{n!}$$ Then integrate term by term from $0$ to $x$ to get: $$\frac{\sqrt{\pi}}{2}\text{erf} (x)=\sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{n!(2n+1)}$$ Then I substituted in $x=\frac{1}{2}$ and tried some manipulations but didn't get anywhere. May someone help, thanks.","['error-function', 'sequences-and-series', 'calculus']"
1885456,Connection between the spectra of a family of matrices and a modelization of particles' scattering?,"In the excellent book ""Numerical Computing with MATLAB"" by Cleve B. Moler (SIAM 2004), [Moler is the ""father"" of Matlab], one finds, on pages 298-299, the following graphical representation (fig. 10.12 ; I have reconstructed it with minor changes ; Matlab program below) with the following explanations (personal adaptation of the text): Let $n$ be a fixed integer $>1$ . Let $t\in (0,1)$ ; let $A_t$ be the $n \times n$ matrix with entries $$A_{t,i,j}=\dfrac{1}{i-j+t}$$ Consider Fig. 1, gathering the spectra of matrices $A_t$ , for $n=11$ and $0.1 \leq t \leq 0.9$ with steps $\Delta t=0.005$ . Figure 1. Interpretation : the 11 rightmost points correspond to the spectrum of matrix $A_t$ for $t=0.1$ . There is a striking similarity with particles' scattering by a nucleus situated at the origin, with hyperbolic trajectories ; see for example this reference . My question is : How matrices $A_t$ can be connected to a model of particle scattering ? My attempts (mostly unsuccessful) : Consider $A_t$ as a Cauchy matrix ( $x_i=i$ , $y_j=j-t$ ) (notations of https://en.wikipedia.org/wiki/Cauchy_matrix )  with its different properties, in particular displacement equation. See as well the answers to this question . Connect $A_t$ with a quadratic form defined by its moment's matrix $\int_C z^{i}\bar{z}^{j}z^{t-1}dz$ , $z^{t-1}$ playing the rôle of a weight function. But for which curve $C$ ? Prove that the eigenvalues are situated on hyperbolas. Make many simulations (see figures below). Matlab program for Fig. 1 : n=11;
 [I,J]=meshgrid(1:n,1:n);
 E=[];
 for t=0.1:0.005:0.9
   A=1./(I-J+t);
   E=[E,eig(A)];
 end;
 plot(E,'.k');axis equal Addendum (November 22, 2018) : Here is a second figure that provides a ""bigger"" view, with $n=20$ and $0.005 \leq t \leq 0.995$ . The eigenvalues corresponding to $t=0.005$ are grouped into the rightmost big red blob on the right, to $t=0.995$ are blue filled dots (quasi-circle with radius $\approx 110$ ). Figure 2 : [enlarged version of Fig. 1 ; case $n=15$ ] Everything happens as if a planar wave enters at $t=0$ from the right, is slowed down by  nucleus' repulsion, then scattered as a circular wave... Fig. 3 : Two cases are gathered here, both for $t=0.995$ : $n=51$ (empty circles) and $n=100$ (stars). One can note that the eigenvalues are very close to the $(n+1)$ -th roots of unity. For this value of $t$ , the  radii  are given by experimental formula : $R_n=200(1-12.4/n+212/n^2-3110/n^3)$ . I am grateful to @AmbretteOrrisey who did very interesting remarks, in particular by giving the following polar representation for the trajectory of particles : [citation follows ; more details can be found in his/her answer]
""The polar equation of the trajectory of a particle being deflected by a point charge is $$r=\frac{2a^2}{\sqrt{b^2+4a^2}\sin\theta -b}\tag{1}$$ where $a$ is the impact parameter which is the closest approach to the nucleus were the path undeviated ; $b$ is the closest approach of a head-on ( $a=0$ ) particle with repulsion operating."" Figure 4 displays a reconstruction result that takes into account the fact that (n+1)th roots of unity give asymptotic directions. Figure 4 : Case $n=201$ . An approximate reconstruction of $21$ among the $n$ trajectories (Matlab program below ; polar equation - adapted from (1) can be seen on line 5). Please note the ""ad hoc"" coefficient $3.0592$ ... n=201;
 for k=1:20:n
    d=pi*k/(2*(n+1));c=cos(d);
    t=-d+0.01:0.01:d-0.01;
    r=3.0592*(1-c)*exp(i*(t-d))./(cos(t)-c); 
    plot(r);plot(conj(r));
 end; Slightly related : http://bdpi.usp.br/bitstream/handle/BDPI/35181/wos2012-3198.pdf I mention here a book gathering publications of Steve Moler [Milestones in Matrix Computation] ( https://global.oup.com/academic/product/milestones-in-matrix-computation-9780199206810?cc=fr&lang=en& ) and an article mentionning the analycity of the obtain curves : ( https://www.math.upenn.edu/~kazdan/504/eigenv.pdf )","['eigenvalues-eigenvectors', 'cauchy-matrices', 'matrices', 'mathematical-physics', 'linear-algebra']"
1885458,Diagram for third isomorphism theorem $\mathrm{(G/K)/(H/K)} \cong \mathrm{G/H}$,"I'm self-studying Lang's Algebra and on page 17, he gives a diagram for the third isomorphism theorem using a homomorphism f from $\mathrm{G/K}$ to $\mathrm{G/H}$ for two normal subgroups H and K of G such that $\mathrm{K\subseteq H}$. f is defined by $\mathrm{f(xK)=xH}$. Lang claims that the following commutative diagram represents this situation: $\require{AMScd}$
\begin{CD}
0@>>>H@>>>G@>>>G/H@>>>0\\
@. @VVcanV @VVcanV @VVidV\\
0@>>>H/K@>>>G/K@>>>G/H@>>>0
\end{CD} where the rows are exact and can is a canonical morphism and id is the identity morphism. My question is, why are the rows exact, why is the diagram commutative (the main thing which I don't understand), how does this represent the third isomorphism theorem? I know that the first row with $\mathrm{G}$ is exact as the morphism from $\mathrm{H}$ to $\mathrm{G}$ is the inclusion (injective) and the morphism from $\mathrm{G}$ to $\mathrm{G/H}$ is the canonical map (surjective) but I'm not sure why the second row is exact; I'm not even sure what the morphisms are!(Lang doesn't mention what they are and in the diagram, they're just arrows with no labels). The map from $\mathrm{G/K}$ to $\mathrm{G/H}$ is probably the f mentioned above (which is surjective) but the map from $\mathrm{H/K}$ to $\mathrm{G/K}$ could be the inclusion map or the map induced by a homomorphism g which maps $\mathrm{H}$ to $\mathrm{G/K}$, which are both injective (Lang mentions this in an earlier example and says that that example relates to this). About the commutativity of the diagram, I think it's because isomorphisms are invertible or maybe because isomorphic groups are the same in structure, intuitively, this could mean a map from one has the same effect as a some map from the other group... For the relation to the third isomorphism theorem, there is no mention of $\mathrm{(G/K)/(H/K)}$ in the diagram, unlike the diagram Lang gave for the first isomorphism theorem so I don't have any idea about how it comes in at all.","['abstract-algebra', 'group-theory']"
1885466,What shapes are described with $\rho = \cos{(\phi)}$ and $\rho = \cos{(2\theta)}$?,"I have started an Multivariable course, and I'm learning about spherical coordinates. My problem now is learn how to graph this kind of shapes. This is the problem: What shapes are described when...? Solution: a) $\rho = 1$ : Sphere with radius 1. b) $\phi = \frac{\pi}{3}$ : Cone with angle $\frac{\pi}{3}$. c) $\theta = \frac{\pi}{4}$ : Semi-circular cross-section with diameter along z-axis d) $\rho = \cos{(\phi)}$ : ? e) $\rho = \cos{(2\theta)}$ : ...? Are they correct? How to describe, verbally, the last two -d) and e).",['multivariable-calculus']
1885535,Change of variables problem: why not to take all possible regions to be integrated?,"I was working out this example in Jon Rogawski multivariable calculus book: My question is: why didn't he take all possible regions that satisfy the conditions:
$$1\le xy\le4,1\le y/x\le4$$ which include the region in the third quarter ?? Also, if we choose only the region in the first quadrant to be integrated, then after transformation of coordinate, there are two areas satisfy transformation: $$[1,2]\times [1,2]$$ as the example, and also:
$$[-1,-2]\times [-1,-2]$$ So why don't we include it in integration ?",['multivariable-calculus']
1885545,Inequality to prove Jensen+Minkowski,"How can I prove the following inequality?
Let $X$ be a positive random variable and $m$ an positive integer. $\mathbb{E}\left((\int_{a}^{b}X_{s}ds)^{m}\right)\leq \left(\int_{a}^{b}\mathbb{E}(X_{s}^{m})^{\frac{1}{m}}ds\right)^{m}$. I guess I shall use Jensen and Minkowski inequality, but Iam not able to do the proof.","['inequality', 'measure-theory']"
1885555,Example of $L^p$ function which does not have much Lebesgue points,"Let $0<p<1$ and $\mu$ be the $n$-dimensional Lebesgue measure. Let $f\in L^p(\mu)$ and define $P:=\{x\in \mathbb{R}^n: \lim_{r\to 0} \frac{1}{\mu(B(x,r))} \int_{B(x,r)} |f - f(x)|d\mu =0 \}$. Then is it possible that $\mu(P^c)>0$? Lebesgue differentiation theorem asserts that $\mu(P^c)=0$ for locally $L^1$ functions. However, $L^p (0<p<1) $ functions are not locally $L^1$ so I'm curious if there is an example that $\mu(P^c)>0$. Thank you in advance.","['real-analysis', 'examples-counterexamples', 'lebesgue-measure', 'measure-theory']"
1885586,How to prove the Bonnesen's Inequality?,"The Bonnesen's Inequality states that for a convex plane curve, which has length $L$ and encloses an area $A$, $$rL \ge A+ \pi r^2 \text{ for all } R_\text{in} \le r \le R_\text{out}$$ where $R_\text{in}$ is the inradius of the curve, and $R_\text{out}$ is the circumradius. I've seen some papers show that it comes from $2A + 2Lr + 2 \pi r^2 \le 4rL$, but still I don't have any idea about this.","['inequality', 'differential-geometry']"
1885589,"Construction of exponentiation of the integers, rationals, and reals?","I don't believe I ever learned the set theoretic construction of exponentiation for any number set other than than the naturals. That being said, for the sake of this question, assume all arithmetic operations are well defined over the natural numbers.  Now lets define the equivalence relation $\sim_Z$
$$\forall a,b,c,d:(a,b,c,d\in\mathbb{N})[(a,b)\sim_Z(c,d)\leftrightarrow a+d=c+b]$$
Then the equivalence class of an ordered pair of natural numbers is an integer:
$$\mathbb{Z}=\mathbb{N}^2/\sim_Z$$
I'm really uncertain as to how I should go about constructing exponentiation over this set, since the equivalence class $[(a,b)]_{\sim_Z}$ represents the integer $a-b$, so trying to put one of these equivalence classes to the power of another seems like trying to simplify a binomial to the power of a binomial which sounds like a pain (perhaps I'm lacking some knowledge of elementary number theory that would make this quite obvious). Next, knowing addition and multiplication over the integers, we define $\sim_Q$
$$\forall a,b,c,d:(a,b,c,d\in\mathbb{Z}\wedge b,d\neq0)[(a,b)\sim_R(c,d)\leftrightarrow ad=bc]$$
and the equivalence classes are the rational numbers:
$$\mathbb{Q}=\mathbb{Z}\times(\mathbb{Z}-\{0\})/\sim_Q$$
This seems like it would be much more simple but the problem is that the rationals are not closed under exponentiation. If we construct the reals as Cauchy sequences of rationals than I would expect that
$$\{a_n\}^{\{b_n\}}=\{a_n^{b_n}\}$$
(Assume the above are equivalence classes of sequences, not just sequences) but the problem is the same: $a_n^{b_n}$ is not necessarily rational. Please help, thank you","['real-numbers', 'rational-numbers', 'integers', 'elementary-set-theory']"
1885616,How do you evaluate this integral: $\int_0^\infty \frac {e^{ax}-e^{bx}}{(1+e^{ax})(1+e^{bx})}dx$? [duplicate],"This question already has answers here : How to integrate $\int_0^{\infty} \frac{e^{ax} - e^{bx}}{(1 + e^{ax})(1+ e^{bx})}dx$ where $a,b > 0$. (3 answers) Closed 7 years ago . This is from GRE Math subject test, Question #55, from https://www.ets.org/s/gre/pdf/practice_book_math.pdf If $a,b > 0$, then what is the value of
$$
\int_0^\infty \frac {e^{ax}-e^{bx}}{(1+e^{ax})(1+e^{bx})} dx
$$ I'm not sure if it's Calc II integration or Residue Theorem from complex analysis, but I have no idea where to start ...","['integration', 'definite-integrals', 'calculus', 'closed-form']"
1885630,Random multivariate in hyperannulus,"Given the hyperannulus, an annulus generalized to arbitrary dimensions with outer radius $r_1$ and inner radius $r_0$. This object would be a ring in $\mathbb{R}^2$ and a spherical shell in $\mathbb{R}^3$. What is the solution of picking uniformly distributed random multivariate points in this hyperannulus without rejection sampling?","['probability-theory', 'probability-distributions', 'statistics', 'geometry', 'random-variables']"
1885633,Distribution of a random real with i.i.d. Bernoulli(p) binary digits?,"Let $X_1, X_2, X_3, \ldots$ be an infinite sequence of i.i.d. Bernoulli($p$) random variables, and define the random real number $X = (0.X_1X_2X_3\ldots)_2$. Question(s) : What can be proved about the distribution of $X$ (for arbitrary $p$)? Are these singular distributions? What about their moments, etc.? Is it the case that $E[X]=p$? Here are some pictures from my simulations: In the above picture, the CDFs from upper left to lower right have increasing values of $p$. (In all cases, the simulations suggest that $E[X]=p$.) In both histograms, the bars show the simulated random sample of i.i.d. $X$ values, and the small black dot near the top of each bar shows the computed value of $P(X\in \text{cell})$ for each cell, based on the infinite series for the CDF given below. (The agreement is excellent.) There's evidently a fractal nature to these objects, and I'm unsure of the actual type of distribution, as to whether they are singular (rather than absolutely continuous or discrete). In any case, the slope of each CDF (other than for $p=\frac{1}{2}$) appears to be discontinuous at every ""dyadic"" point, i.e., at every $x=k\,2^{-m}$ for positive integers $k,m$. Motivation : In a posting elswhere , I showed that if $p=\frac{1}{2}$, then $X\sim \text{Uniform}[0,1]$, so I naturally wondered about this more-general case. The same form of argument (via disjoint union) as presented there gives the following infinite series: 
$$\begin{align}
P(X>x)&=P(X_1>x_1)\\
&+P(X_1=x_1\,\land\,X_2>x_2)\\
&+P(X_1=x_1\,\land\,X_2=x_2\,\land\,X_3>x_3)\\
&+\cdots\\ \\
&=p\,(1-x_1)\\
&+p\,^{x_1}\,(1-p)^{(1-x_1)}\,p\,(1-x_2)\\
&+p\,^{x_1+x_2}\,(1-p)^{(1-x_1)+(1-x_2)}\,p\,(1-x_3)\\
&+\cdots
\end{align}$$ 
for any $x=(0.x_1x_2x_3\ldots)_2\in[0,1)$, where, WLOG, we always choose the unique binary representation of $x$ that has no infinite tail of $1$s. An algorithm that uses only the first $n$ bits of $x$ to approximate the CDF of $X$, viz., $P(X\le x)$, is therefore described by the following very simple pseudocode: sum <- 0 
term <- 1 
for i in {1,...,n}:
    if x[i] == 1: 
        term <- term*p
    else: 
        sum <- sum + term*p
        term <- term*(1 - p) 
return (1 - sum)","['probability-theory', 'random', 'random-variables', 'probability-distributions']"
1885685,Important identities that can be obtained by manipulating the function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + ...$?,"Note that $B_n$ denotes the nth Bernoulli number. Also note that
$\frac{x}{2}\frac{e^x+1}{e^x-1} = \frac{x}{e^x-1} + \frac{x}{2} = 1 + \frac{|B_2|}{2!}x^2 - \frac{|B_4|}{4!}x^4+ \frac{|B_6|}{6!}x^6 - \cdots$ since $B_1 = -\frac{1}{2}$, the odd Bernoulli numbers are equal to zero, and $|B_{2n}| = (-1)^{n+1}B_{2n}$. Two of the most interesting identities that I've read about thus far in my studies have proofs which rely upon the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ in an essential way. The first is (Jacobi's?) proof of Faulhaber's formula, and the second is (Euler's?) proof that connects the Bernoulli numbers to values of the Riemann Zeta function for  positive even integers. Faulhaber's Formula:$$1^c+2^c+\cdots+n^c = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Riemann Zeta Function Identity:
$$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots = (-1)^{n+1}\frac{{(2\pi)}^{2n}B_{2n}}{2(2n)!}$$ Below I give the manipulations which give rise to these identities, you'll notice that the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ is the key piece of equipment that allows the connection with the Bernoulli numbers to be established in both cases. Given the importance of these two identities and their connection with this generating function I'm left to wonder if there are other important identities that require the manipulation of this generating function. Perhaps someone can provide other examples. Edit-08/09/16 I feel I should clarify. I realize that this function can be manipulated to produce many identities, but Faulhaber's Formula and this Zeta function identity seem somehow special, especially in light of how beautiful their manipulations are. I'm looking for identities that 1) Are considered to be rather important. 2) Can be established be means of the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$. 3) Perhaps have some special quality about their manipulations. Proof of Faulhaber's formula. This proof can be found in sigma notation here https://en.wikipedia.org/wiki/Faulhaber%27s_formula#Proof $$\{1^0+2^0+\cdots+n^0\} + \{1^1+2^1+\cdots+n^1\}\frac{x}{1!} + \{1^2+2^2+\cdots+n^2\}\frac{x^2}{2!} +\cdots$$
$$= \{{1^0}+\frac{1^1x^1}{1!} + \frac{1^2x^2}{2!} + \cdots\} + \{{2^0}+\frac{2^1x^1}{1!} + \frac{2^2x^2}{2!} + \cdots\} +\cdots+ \{{n^0}+\frac{n^1x^1}{1!} + \frac{n^2x^2}{2!} + \cdots\}$$
$$=e^x + e^{2x} + \cdots+e^{nx} = e^x\frac{1-e^{nx}}{1-e^x} = \frac{1-e^{nx}}{e^{-x}-1} = \frac{1}{x}\{\frac{-x}{e^{-x}-1}\}\{e^{nx}-1 \} = \frac{1}{x}\{\frac{B_0}{0!}-\frac{B_1}{1!}x+\frac{B_2}{2!}x^2 - \cdots \}\{{e^{nx}-1}\} $$$$= \frac{B_0}{0!}\frac{1}{x}\{{e^{nx}-1}\}-\frac{B_1}{1!}\{{e^{nx}-1}\}+\frac{B_2}{2!}x^1\{{e^{nx}-1}\} - \cdots  $$
$$\begin{align}
& =\frac{B_0}{0!}\{\frac{n^1}{1!}x^0 + \frac{n^2}{2!}x^1 + \frac{n^3}{3!}x^2 + \frac{n^4}{4!}x^3+\cdots\} \\
& -\frac{B_1}{1!}\{\frac{n^1}{1!}x^1 + \frac{n^2}{2!}x^2 + \frac{n^3}{3!}x^3+\frac{n^4}{4!}x^4+ \cdots\} \\
& +\frac{B_2}{2!}\{\frac{n^1}{1!}x^2 + \frac{n^2}{2!}x^3 + \frac{n^3}{3!}x^4 +\frac{n^4}{4!}x^5+ \cdots\}\\
&-\frac{B_3}{3!}\{\frac{n^1}{1!}x^3 + \frac{n^2}{2!}x^4 + \frac{n^3}{3!}x^5 +\frac{n^4}{4!}x^6+ \cdots\}+\cdots
\end{align}$$ 
$$=\frac{B_0}{0!}\frac{n}{1!} + \{\frac{B_0}{0!}\frac{n^2}{2!} - \frac{B_1}{1!}\frac{n}{1!}\}x + \{\frac{B_0}{0!}\frac{n^3}{3!} - \frac{B_1}{1!}\frac{n^2}{2!} + \frac{B_2}{2!}\frac{n}{1!}\}x^2 + \{\frac{B_0}{0!}\frac{n^4}{4!} - \frac{B_1}{1!}\frac{n^3}{3!} + \frac{B_2}{2!}\frac{n^2}{2!} - \frac{B_3}{3!}\frac{n}{1!}\}x^3+ \cdots$$ Equating coefficients, we see that $$\{1^c+2^c+\cdots+n^c\}\frac{1}{c!} = \frac{B_0}{0!}\frac{n^{c+1}}{(c+1)!}-\frac{B_1}{1!}\frac{n^c}{c!} + \cdots+ (-1)^c\frac{B_c}{c!}\frac{n}{1!}$$Thus
$$1^c+2^c+\cdots+n^c = \frac{c!}{0!(c+1)!}B_0n^{c+1}-\frac{c!}{1!c!}B_1n^c+\cdots+(-1)^c\frac{c!}{c!1!}B_cn = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Proof of Zeta Identity for positive even integers: I found this proof on pages 276-277 in Carr's Synopsis found here https://archive.org/details/synopsisofelemen00carrrich . I'm kind of just assuming that this proof traces back to Euler. Carr's Synopsis is rough in places but I would highly recommend reading certain pages that are intimately connected to Ramanujan's intuition. (I first heard about Carr's Synopsis in this video https://www.youtube.com/watch?v=QUnmAhXe9bg ) [ 2 It mistakenly refers to 1540 instead of 1541 lol. 
Note that Carr defines the Bernoulli numbers according to their absolute values.
Here is what Carr is saying: $$sin(x) = x\{1-{\left(\frac{x}{\pi}\right)}^{2}\}\{1-{\left(\frac{x}{2\pi}\right)}^{2}\}\{1-{\left(\frac{x}{3\pi}\right)}^{2}\}\cdots$$
Thus
$$log\{sin(x)\} = log(x) + log\{1-{\frac{x^2}{\pi^2}}\}+log\{1-{\frac{x^2}{(2\pi)^2}}\}+log\{1-{\frac{x^2}{(3\pi)^2}}\}+\cdots$$
Thus
$$\frac{cos(x)}{sin(x)} = \frac{1}{x} + \frac{-2\frac{x}{\pi^2}}{1-\frac{x^2}{\pi^2}} + \frac{-2\frac{x}{(2\pi)^2}}{1-\frac{x^2}{(2\pi)^2}}+ \frac{-2\frac{x}{(3\pi)^2}}{1-\frac{x^2}{(3\pi)^2}}+\cdots$$
Thus
$$xcot(x) = 1-2\frac{x^2}{\pi^2}\{1+\frac{x^2}{\pi^2}+\frac{x^4}{\pi^4}+\cdots\}-2\frac{x^2}{(2\pi)^2}\{1+\frac{x^2}{(2\pi)^2}+\frac{x^4}{(2\pi)^4}+\cdots\}-2\frac{x^2}{(3\pi)^2}\{1+\frac{x^2}{(3\pi)^2}+\frac{x^4}{(3\pi)^4}+\cdots\}-\cdots$$ $$= 1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots$$ Now, if $y = 2ix$
$$xcot(x) = ix\frac{2cos(x)}{2isin(x)} = ix\frac{\{cos(x)+isin(x)\} + \{cos(x)-isin(x)\}}{\{cos(x)+isin(x)\} - \{cos(x)-isin(x)\}} = ix\frac{e^{ix}+e^{-ix}}{e^{ix}-e^{-ix}} = ix\frac{e^{2ix}+1}{e^{2ix}-1} = \frac{y}{2}\frac{e^y+1}{e^y-1} = \frac{y}{e^{y}-1}+\frac{y}{2} $$$$= 1+\frac{|B_2|}{2!}y^2-\frac{|B_4|}{4!}y^4+\frac{|B_6|}{6!}y^6-\cdots $$$$= 1+\frac{|B_2|}{2!}(2ix)^2-\frac{|B_4|}{4!}(2ix)^4+\frac{|B_6|}{6!}(2ix)^6-\cdots$$$$= 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Thus by combining both expressions for $xcot(x)$ we obtain $$1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots = 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Equating coefficients, we see that $$\frac{2}{\pi^{2n}}\{\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots\} = \frac{|B_{2n}|}{(2n)!}2^{2n} $$
Therefore,
$$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots= (-1)^{n+1} \frac{{(2\pi)}^{2n} B_{2n}}{2(2n)!}$$ Edit 08/09/16 I removed my second question since it seemed to cause confusion. I think that my thoughts in this area need to be focused a bit more.","['intuition', 'complex-numbers', 'calculus', 'generating-functions', 'power-series']"
1885700,Cardinality of power set of $\mathbb N$ is equal to cardinality of $\mathbb R$ [duplicate],This question already has answers here : The set of real numbers and power set of the natural numbers (5 answers) Closed 7 years ago . How to prove that; The cardinality of power set of $\mathbb N$ is equal to cardinality of $\mathbb R$ I need a reference or proof outline. I looked up but didn't find results thanks.,['elementary-set-theory']
1885708,"True or false: If $f(x)$ is continuous on $[0, 2]$ and $f(0)=f(2)$, then there exists a number $c\in [0, 1]$ such that $f(c) = f(c + 1)$.","I was solving past exams of calculus course and I've encounter with a problem, which I couldn't figure out how to solve.The question is following; Prove that whether the given statement is true or false: Suppose that $f(x)$ is continuous on $[0, 2]$ and $f(0)=f(2)$. Then there exists a number $c\in [0, 1]$ such that $f(c) = f(c + 1)$. By just reading the question, you want to say ""use Mean Value Theorem or Rolle's theorem"" but we don't know whether $f(x)$ is differentiable on the interval, at least it is now given, so I am totally stuck.I would appreciate any help.",['calculus']
1885733,"If $A=\{n | n \in N, n\le 100\}$ Which number has the most representations as the sum of the elements of a subset of $A$?","If $A= \{n | n \in N, n\le 100\} $, which number has the most representations as a sum of the elements of a subset of $A$ and why? My guess would be $\frac{100.101}{2}$ but I am not sure how it can be proved.","['combinatorics', 'discrete-mathematics']"
1885736,Alternate Characterization of Linearity,"This question is prompted by this video on matrices and linear transformations , which I highly recommend as a pedagogical tool.  In it, the author characterizes linear transformations in the following way (I'm paraphrasing and formalizing) Define a line in a vector space to be a set of the form $L = \{u + t\,v: t \in \Bbb R\}$ for some vectors $u$ and $v$ (note: $L$ may consist of a single point) .  That is, $S$ is an affine subspace of dimension at most $1$ . A function $T: \Bbb R^n \to \Bbb R^m$ is linear if: $T(0) = 0$ For any line $L \subset \Bbb R^n$ , the image $T(L)$ is a line in $\Bbb R^m$ I like this definition because of its geometric appeal and the fact that it manages to ""put the line in linear"". Of course, the traditional definition of a linear map is one which preserves linear combinations. My Question: How should one prove that a function satisfying this definition preserves linear combinations? Can this be proven in a beginner-friendly way? I'll admit I haven't really banged my head against this one, but here are my thoughts: it is equivalent to prove that a function that satisfies only the second condition (i.e. maps lines to lines) is an affine transformation , i.e. that it preserves affine combinations .  From there, it would suffice to note that if $T$ is affine, then $x \mapsto T(x) - T(0)$ is linear. That being said, I don't see a quick way to handle that proof off the top of my head.  Moreover, if this really is the quickest way to reach a proof, it seems that proving this in linear algebra 101 is a bit too ambitious (which is not to say that this fact fails to be pedagogically useful).  I'm guessing a little real-analysis might have to come in at some point. A note for myself and future visitors: If we are also given that 𝑇 is bijective, then this is a consequence of the fundamental theorem of projective geometry .","['education', 'real-analysis', 'matrices', 'geometry', 'linear-algebra']"
1885745,Examples of short maps (Lipschitz functions with $k=1$) with exactly 2 fixed points.,"I was just reading about the Banach fixed point theorem, which states that a contraction (a function $f$ satisfying $|f(x)-f(y)|\leq k|x-y|$ for $0<k<1$) has a unique fixed point. If we have $k=1$ then $f$ is called a short map or a non-expansive map. I'm wondering how many fixed points such functions can have. Clearly if $f$ is the identity function then every point is a fixed point. If $f$ just translates points then there are no fixed points. Does there exist a function $f\colon\mathbb{R}\to\mathbb{R}$ satisfying $|f(x)-f(y)|\leq|x-y|$, which has exactly two fixed points? If not are there examples of short maps involving different metric spaces with exactly two fixed points?","['real-analysis', 'metric-spaces', 'analysis']"
1885755,Prove a series that equals to $\frac{e}{e-1}$,"Prove that 
$$
\lim_{N\to\infty}\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}=\frac{e}{e-1}
$$ I think $\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}$ should be a Riemann sum of a function but could find it. What is the trick in this question? In addition, the equation holds true when it could interchange the limits, but how to prove it?",['analysis']
1885780,Probability of at least k distinct events occuring after N trials,"Given a finite set $|S|$ of distinct events, each with a $1/|S|$ probability of occurrence, what is the probability that at least k distinct events occur after N trials? For every trial, there is guaranteed to be exactly one event that occurs. Events are repeatable. For instance, take $S = \{rain, sun, snow\}$. After $N=10$ days, what is the probability that at least $k=2$ distinct types of weather have occurred? In my specific problem, I have $|S| = 35$, so a method of approximation would be preferred.","['statistics', 'probability']"
1885802,Prove that $\sqrt{2}+\sqrt{3}$ is irrational.,"Problem: Prove that $\sqrt{2}+\sqrt{3}$ is irrational. 
The book where I encountered this problem had the following hint: We make a polynomial with integer coefficients called $f(x)$ that $f(\sqrt{2}+\sqrt{3})=0$. (Why?) Accepting this I solved the problem like this:
If $x=\sqrt{2}+\sqrt{3}$ then $x^2=5+2\sqrt{6}$ and so $(x^2-5)^2=24$ thus:
$$x^4-10x^2-1=0$$
But I want to know the reason that we should do this.","['number-theory', 'irrational-numbers']"
1885821,Change of variables between spaces with different dimensionality in using the Riemann integral,"On wikipedia ( https://en.wikipedia.org/wiki/Integration_by_substitution ) I find that,
\begin{equation}
\int_{\phi(U)} f(v)dv = \int_U f(\phi(u))|\det(D\phi(u))|du
\end{equation}
for the Riemann integral under a number of conditions.
I'm looking for a more general formulation, where $f:R^m\rightarrow R$, $M$ is a mapping $M:V \rightarrow U$, where $V\subset R^n$ is open and $U \subset R^m$ is some n dimensional surface, so that:
\begin{equation}
\int_{V} f(M(v))dv = \int_U f(u) \cdot \text{something} \cdot du
\end{equation}
Already in the same wikipedia article, conditions for doing this using Lebesgue integration is given, but I want to know when and how this can be done using the Riemann integral, because I'm writing a short text that should be easy to understand, so I don't want to make it too abstract. A literature reference would be appreciated.","['riemann-integration', 'change-of-basis', 'multivariable-calculus', 'integration', 'differential-geometry']"
1885861,Number theoretic characterisation of cosine?,"Let $f:[0, \pi/2]: \rightarrow \mathbb{R}$ be an infinitely differentiable function, with $f(0)=1, f(\pi/2)=0$, such that $f(\pi \mathbb{Q}) \cap \mathbb{Q} = \{0,1/2,1\}$. Classical results show that the cosine function satisfies these properties. Is it unique in this regard? I doubt it's the only such function, but I was unable to write down any other or come up with an argument proving the existence of others. Apologies if this turns out to be elementary, I couldn't find anything online about this.","['number-theory', 'real-analysis', 'trigonometry']"
1885878,Check that $\int_0^1B_tdt$ is $\int_0^1(1-t)dB_t$ and also $B_1-\int_0^1tdB_t$,"I am beginning at learning stochastic calculus, and am stuck in a small problem. The chapter I am reading is about Wiener integral, and it gives an example of: $$
\int^1_0{B(t,\omega)dt}
$$ Since it has already introduced the idea of Wiener integral, and we are able to calculate $\int^1_0{tdB(t,\omega)}$, the author uses integration by parts to obtain the following result: $$
\int^1_0{B(t,\omega)dt}=B(t,\omega)(t-1)|^1_0-\int^1_0{(t-1)dB(t,\omega)}
=\int^1_0{(1-t)dB(t,\omega)}
$$ which is a Gaussian random variable with mean $0$ and variance $1/3$. (The Wiener integral in the last term is the same as that of Riemann Stieltjes integral.) What I am confused is what happens if $t-1$ is replaced by $t$. My calculation:
$$
\int^1_0{B(t,\omega)dt}=B(t,\omega)t|^1_0-\int^1_0{tdB(t,\omega)}
=B(1,\omega)-\int^1_0{tdB(t,\omega)}
$$ And it turns out to be a Gaussian with variance $1$ minus another Gaussian with variance $1/3$. I am wondering it is the same as the above calculation, or there are errors in my reasoning.","['brownian-motion', 'probability-theory', 'stochastic-calculus']"
1885916,Why do these open subgroups of the étale fundamental group contain the kernel of an induced homomorphism?,"I'm trying to understand the proof of Proposition 5.5.6 in Szamuely's Galois Groups and Fundamental Groups . The proposition relates the kernel of the map $\phi_*: \pi_1 (S', \bar{s}')\to \pi_1(S,\bar{s})$ (induced on étale fundamental groups of connected schemes by a pointed morphism $\phi: (S',\bar{s}')\to (S,\bar{s})$, where $\bar{s}, \bar{s}'$ are geometric points) to coverings of the two schemes. More specifically, it states: Let $U'\subseteq \pi_1 (S', \bar{s}')$ be an open subgroup, and let $X'\to S'$ be the cover corresponding to the coset space $U'\backslash \pi_1 (S', \bar{s}')$. The subgroup $U'$ contains $\ker(\phi_*)$ if and only if there exists a finite étale cover $X\to S$ and an $S'$-morphism $X_i \to X'$, where $X_i$ is a connected component of $X\times_S S'$. I am confused about two parts in the proof, essentially coming down to the same problem: Firstly, for the ""if"" direction of the proof. We pick an $X\to S$ as in the statement, which corresponds to the coset space of an open subgroup $U$ of $\pi_1 (S,\bar{s})$. Szamuely then writes ""by choosing an appropriate geometric base point we may identify the component $X_i \subseteq X\times_S S'$ with the coset space $U''\backslash \pi_1 (S', \bar{s}')$ for some open subgroup $U''\subseteq \pi_1 (S', \bar{s}')$. Note that $U''$ must contain $\ker(\phi_*)$ by construction."" I don't understand why $U''$ must contain $\ker(\phi_*)$. Could somebody extend on this for me? Secondly, in the ""only if"" part of the proof, we use a group-theoretic lemma to obtain an open subgroup $V\subseteq \pi_1 (S,\bar{s})$ whose intersection with $\text{im}(\phi_*)$ is $\phi_* (U')$. This $V$ gives a finite étale cover $X = V\backslash \pi_1 (S,\bar{s}) \to S$, and we can pick a connected component $X_i \subseteq X\times_S S'$ to get another open subgroup $U''\subseteq \pi_1 (S', \bar{s}')$ for which $X_i$ is the coset space $X_i = U''\backslash \pi_1 (S', \bar{s}')$. Szamuely then claims that both groups $U'$ and $U''$ contain $\ker(\phi_*)$ (here $U'$ contained the kernel by assumption). But once again, I don't understand why $U''$ must contain $\ker(\phi_*)$. What is the reason for this? I think this is something I need to see once or twice before I get the hang of using it. Many thanks!","['galois-theory', 'algebraic-geometry', 'schemes', 'fundamental-groups', 'algebraic-topology']"
1885917,The number of all 3-digit numbers abc for which abc + ab+bc + ac+a+b+c = 29,"The number of all 3-digit numbers abc (in base 10) for which $\ abc+ab+bc+ac+a+b+c = 29$ is (A) 6 (B) 10 (C) 14 (D) 18 My working: $\ ab (c + 1) +b (c + 1) + a(c + 1) + c + 1 = 30$ $\ (a + 1) (b +1) (c+1) = 30 $ $\ 9>a>0$ $\ 0\le b,c<9$ The problem I am facing: I don't know how to figure out the no. of solutions for which this is true.",['number-theory']
1885945,Limit of sum of iid sequences,"Suppose I have two iid sequences of random variables $\{X_t\}_{t\ge1}$ and $\{Y_t\}_{t\ge1}$ that have full support. That is, if $A\subseteq\mathbb{R}$ has positive Lebesgue measure, then $P(X\in A) >0$ and $P(Y\in A)>0$ . Such an iid sequence hits with probability 1 any set of positive measure for infinitely many $t$ . That means that with probability one $$
\liminf_{t\rightarrow\infty} |X_t| = 0\qquad\text{and}\qquad \limsup_{t\rightarrow\infty} |X_t| = \infty .
$$ Question: Is it possible to find a dependence structure between $\{X_t\}_{t\ge1}$ and $\{Y_t\}_{t\ge1}$ such that $$
P\left(\liminf_{t\rightarrow\infty} |X_t| + |Y_t| = \infty\right) > 0,
$$ that is, with positive probability there exists no $M>0$ such that $|X_t| + |Y_t|<M$ for infinitely many $t$ . I have been trying to find a dependence structure but failed in doing so. I now believe that it might be impossible, but can't prove it. Thank you in advance!","['independence', 'probability-theory', 'limsup-and-liminf']"
1885968,0/0 in limit evaluation [duplicate],"This question already has answers here : Why does factoring eliminate a hole in the limit? (16 answers) Closed 7 years ago . When evaluating limits we often come across the form 0/0 and we cant get a limiting value. As stated in many textbooks and online resources, the method to still get a limiting value as the input approaches a specific value is to first factorize or simplify the function to cancel out the factors/variables responsible for the form 0/0. My question is doesn't it change the function altogether if we simplify it? Because if we had used the original function it would not have been defined at that particular input? Can anyone provide the intuition or concept behind this?","['functions', 'calculus', 'limits']"
1885983,"Laguerre polynomials, how to compute the integral?","The Laguerre Polynomials $L_n = L_n(x)$, $n=0,1,2$ satisfy the Laguerre differential equation, $$xL''+(1-x)L' +nL=0\quad(3)$$ Show that the Laguerre differential equation $(3)$ can be written in the form $$(x(e^{-x}\cdot L'))' +n(e^{-x}\cdot L)=0$$ I used the product rule to compute $$L'(e^{-x}) + x((-e^x)\cdot L' + L''e^{-x}) + ne^{-x}\cdot L$$ which gives you equations $(3)$ However I cannot compute the next part. For $n\neq m$, show that $$\int_{0}^\infty L_nL_me^{-x}dx=0.$$ I apologise for not formatting as I really struggle with this.",['ordinary-differential-equations']
1886038,Finding Euler decomposition of a symplectic matrix,"A symplectic matrix is a $2n\times2n$ matrix $S$ with real entries that satisfies the condition $$
S^T \Omega S = \Omega
$$
where $\Omega$ is the symplectic form, typically chosen to be $\Omega=\left(\begin{smallmatrix}0 & I_N \\ -I_N & 0\end{smallmatrix}\right)$. Sympletic matrices form the symplectic group $Sp(2n,\mathbb{R})$. Any symplectic matrix S can be decomposed as a product of three matrices as \begin{equation}
S = O\begin{pmatrix}D & 0 \\ 0 & D^{-1}\end{pmatrix}O' \quad \quad \forall S \in Sp(2n,\mathbb{R}),
\end{equation}
where $O, O'$ are orthogonal and symplectic - $\operatorname{Sp}(2n,\mathbb{R})\cap \operatorname{O}(2n)$; $D$ is positive definite and diagonal. The form of a matrix that is both symplectic and orthogonal can be given in block form as $O=\left(\begin{smallmatrix}X & Y \\ -Y & X\end{smallmatrix}\right)$, where $XX^T+YY^T=I_N$ and $XY^T-YX^T=0$. The decomposition above is known as Euler decomposition or alternatively as Bloch-Messiah decomposition. How can I find the matrices in the decomposition for a given symplectic matrix? Apparently, the decomposition is closely related to the singular value decomposition and I think the elements of the matrices $D$ and $D^{-1}$ coincide with the singular values of $S$. Also, I have the impression that the case where it can be assumed that $S$ is also symmetric is easier. Any help, tips or pointers would be much appreciated!","['matrices', 'symplectic-linear-algebra', 'matrix-decomposition', 'svd', 'linear-algebra']"
1886054,Probability that an exam will have a perfect predictor,"Here is a just-for-fun question, inspired by this answer of Noam Elkies : Suppose an exam with $q$ questions is taken by $s$ students. Each student independently has probability $1/2$ of getting each question right or wrong, a passing grade is getting the majority of the questions right. (Let's take $q$ odd, for simplicity.) What is the asymtotic probability that there will be at least one question which is gotten right precisely by the students who pass? Of course, there are a variety of ways in which $q$ and $s$ can go to $\infty$, so the question is which relative growth rates make this situation likely or unlikely.","['combinatorics', 'probability']"
1886058,Any good practice material for expected value and variance?,"I am trying to learn more about probability mass functions, density functions, expected value, and variance. Are there any online materials or quizzes (with answers and explanations) that I can use to test my understanding?","['expectation', 'reference-request', 'variance', 'statistics', 'probability']"
1886081,Partial Derivatives Tend to Zero implies Limit to Infinity Exist?,"Let $g:\mathbb{R}^2\to\mathbb{R}$ be a function such that $\frac{\partial g}{\partial x}$ and $\frac{\partial g}{\partial y}$ exist and are continuous on $\mathbb{R}^2$. Suppose that $$|\frac{\partial g}{\partial x}(x,y)|+|\frac{\partial g}{\partial y}(x,y)|\leq\frac{1}{x^2+y^2}$$ if $(x,y)\neq (0,0)$. Prove that $\lim_{(x,y)\to\infty}g(x,y)$ exists. I am not very sure how to go about proving this. I know that the first condition (partial derivatives exist and continuous) implies that $g$ is differentiable on $\mathbb{R}^2$. Then, I am thinking of using Taylor's Theorem somehow. One problem I face is I don't know what is the candidate limit of $\lim_{(x,y)\to\infty}g(x,y)$ which makes it hard to prove it using the Epsilon-Delta method. Another line of thinking could be the fundamental theorem of line integrals (as suggested by a commenter below). Again, I face problems coming up with the full proof. Thanks for any help! Updates (I am working on this question in the meantime): If we are proving using the $\epsilon-\delta$ definition, we need to show that there exists $c\in\mathbb{R}$ such that for all $\epsilon>0$ there exists $R>0$ such that whenever $\sqrt{x^2+y^2}>R$ we have $|g(x,y)-c|<\epsilon$. Finding a candidate for $c$ seems very hard to me, perhaps the way is to prove by contradiction. I am following AlexM.'s solution, which looks promising, except for some details in the inequality. I am thinking if that can be fixed using application of Mean Value Theorem for Definite Integrals.","['multivariable-calculus', 'real-analysis', 'analysis']"
1886116,How to show that $\mu$ is $\sigma$-finite,"This is part of a problem from an old prelim exam in analysis. I am studying to prepare for my own prelim. Let $\{q_n\}=\mathbb Q$, and let $f_n : \mathbb R \to [0,\infty)$ be a Borel measurable function with $\int f_n d\lambda=1$ and with support $[q_n-2^{-n-1},q_n+2^{-n-1}]$. $\lambda$ is the Lebesgue measure. We furthermore define a Borel measure $\mu$ by $\mu(A) := \int_A \sum_{n=1}^\infty f_n d\lambda$. For the first part of this problem, I have already proven that $\sum_{n=1}^\infty f_n(x) < \infty\ \mathrm{a.e}$, that $\mu <\!\!<\lambda$, and that every open subset of $\mathbb R$ has infinite $\mu$-measure. Now we are asked to show that $\mu$ is $\sigma$-finite, and a I find myself fully at a loss to see how to do this. It seems very difficult to see how this could even be true, since again, every open subset of $\mathbb R$ has infinite $\mu$-measure.","['real-analysis', 'lebesgue-integral', 'measure-theory', 'lebesgue-measure']"
1886167,Are all linear maps differentiable?,"Consider a linear map from $\Bbb R^n \rightarrow \Bbb R^m$. Question 1. How can we show that all such linear maps are differentiable without loss of generality? Question 2. Is it possible that even though a function is not continuous at a point, it can be differentiable at that point ? Say for example, consider a linear map $f: \Bbb R^2 \rightarrow \Bbb R$ $f(x,y) =
\begin{cases}
\frac{xy}{x^2 + y^2}  & (x,y) \neq (0,0) \\[2ex]
0 & (x,y) = (0,0)
\end{cases}$ is not continuous at $(0,0)$ but is differentiable at $(0,0)$.","['multivariable-calculus', 'linear-transformations', 'derivatives']"
1886176,Ramsey Theory Similar Problem,"Let $m,n\in \mathbb{N}$. We number the vertices of $K_{mn+1}$ $1,2,...,mn+1$ and color each of its edges either blue or red. Prove that at least one of the following must occur: $a.$ There exist $m+1$ vertices, the edges between them are all blue. (A complete $K_{m+1}$ blue subgraph) $b.$There exist $n+1$ vertices numbered $v_1<v_2<...<v_{n+1}$ for which $\{v_i,v_{i+1}\}$ is colored red for $i=1,...,n$ This looks like Erdos Szekeres relating to posets but I can't find the right operation. This also looks like it has something to do with Ramsey theory.","['combinatorics', 'ramsey-theory', 'extremal-combinatorics']"
1886193,Prove or disprove that $\mathbb{R}^2 / \mathfrak{S}$ is homeomorphic to $\mathbb{R}/ \mathfrak{R} \times \mathbb{R} / \mathfrak{R}$,"In $\mathbb{R}$ with the usual topology is considered the following equivalence relation
  $$x \mathfrak{R} y \; \text{if and only if}\ x,y\in \mathbb{Q}\ (\text{or}\ x=y)$$ $\mathfrak{S}$ $\mathfrak{R}$
  On the other hand, in $\mathbb{R}^2$ with the usual topology, is defined
  $$(x,y)\mathfrak{S} (x',y')\; \text{if and only if}\ (x,y),(x',y')\in \mathbb{Q}^2 \; ( \text{or}\ (x,y)=(x',y'))$$
  Are $\mathbb{R}^2 / \mathfrak{S}$ and $\mathbb{R}/ \mathfrak{R} \times \mathbb{R} / \mathfrak{R}$ homeomorphic? More generally, can $\mathbb{R}^2/\mathfrak{S}$ be homeomorphic to a product space? I've been trying to picture how the quotient spaces look like. All points in $\mathbb{Q}$ are one point in $\mathbb{R}/ \mathfrak{R}$, so if I take $x\in \mathbb{Q}$, then $$[x]=\{ y\in \mathbb{R} \mid x\mathfrak{R} y\}=\{y\in \mathbb{R} \mid y\in \mathbb{Q} \}=\mathbb{Q}$$
and if $x \not \in \mathbb{Q}$, then
$$[x]=\{x\}$$
and for the other equivalence relation $\mathfrak{S}$ it should work the same way, but I don't know how to prove or disprove whether the two quotient spaces are homeomorphic or not. Thanks in advance!","['general-topology', 'quotient-spaces']"
1886225,Prove that the equation $12x^2-y^2 = 1$ has no integer solutions,"Prove that the equation $12x^2-y^2 = 1$ has no integer solutions. I thought about taking this modulo some number to find a contradiction, but couldn't get anything. What should I do?",['number-theory']
1886229,How many strings contain the words...,"Good day My question is as follows Suppose the alphabet consists of ${A,B,C,D,E,F}$ (1) How many $4$-letter strings contain the word $“ACE”$? (2) How many $4$-letter strings don’t have $“F”$ in the first position and $“E”$ in the last position? (3) How many $5$-letter strings contain the word $“CAB”$? (4) How many $4$-letter strings either begin with $“C”$ or end with two vowels? So far, I answered that: a) $6\times 2=12$   $ACE$ would appear either at one end or the other. Thus, one letter is optional and has $2$ positions. b) $5\times 6\times 6\times 6\times 6\times 5=32400$ c) $6\times 6\times 4=144$ d) Begin with $C$ is  $6\times 6\times 6=216$; end with two vowels $6\times 6\times 6\times 2\times 2=864$. $6\times 2\times 2$ to get those not beginning with $C$ having two vowels at the end. $144$. $6\times 4\times 4$ for those begininning with $C$, not having two vowels at the end. $96$. Would this be the correct approach? Thank you!
-A",['discrete-mathematics']
1886314,"If two vector spaces $V$ and $W$ are isomorphic and $V$ is F-D then $W$ is F-D. Furthermore, $\text{dim} V = \text{dim} W$.","Is the following statement true? Conjecture . If two vector spaces $V$ and $W$ are isomorphic and $V$ is finite dimensional (F-D) then $W$ is finite dimensional. Furthermore, $\text{dim} V = \text{dim} W$. and if YES , then how it can be proved? I can prove the following which is slightly different from the conjecture Theorem . Two finite dimensional vector spaces $V$ and $W$ are isomorphic if and only if they have the same dimension. but it does not help to deduce the conjecture from it. However, I strongly feel that the conjecture should be true. I would be thankful if you provide a hint.  :) Motivation of the Question While I was reading Linear Algebra Done Right by Sheldon Axler I encountered this theorem However, I was not able to prove the red underlined part by the references made! It seems that a little point was overlooked by the author! The references are where the notations used are $\Bbb{F}$ is the field $\Bbb{R}$ or $\Bbb{C}$. $\Bbb{F}^{m,n}$ is the vector space  of $m \times n$ matrices. $\mathcal{L}(V,W)$ is the vector space of linear maps from $V$ to $W$. $\mathcal{M}$ is a linear map from $\mathcal{L}(V,W)$ to $\Bbb{F}^{m,n}$ which gives the corresponding matrix of a linear map belonging to $\mathcal{L}(V,W)$.",['linear-algebra']
1886317,"Suppose the alphabet consists of just {a,b,c,d,e}. How many 4-letter strings are there that do not have “aa” in the middle?","Suppose the alphabet consists of just {a,b,c,d,e}. 
How many 4-letter strings are there that do not have “aa” in the middle? I so far answered:
We assume that a word could have multiple same letters. Since the word is only four letters, a can not be in the second and third position. We look at this as a can be in either the second or third position. Accoringly: (all combos with no a in second or third position) + all combos with a in second but not third + a in third but not second. [C(5,1)*C(4,1)*C(4,1)*C(5,1)]  + [C(5,1)*C(1,1)*C(4,1)*C(5,1)] + [C(5,1)*C(4,1)*C(1,1)*C(5,1)]
=400+100+100
=600 Therefore, there are 600 4-letter strings of (a,b,c,d,e) that don't have aa in the middle. Would that be correct, or incorrect in this case! Thanks,
A",['discrete-mathematics']
1886327,Is there a matrix of every size with all its submatrices invertible?,"Let's call a real matrix of size $m \times n$ totally invertible if for every $k$ rows and $k$ columns that we choose, we get an invertible matrix. I am curious about the following: Is there a totally invertible matrix for all sizes $m \times n$? By taking the transpose, we can assume $m \leq n$. For $m=1$ we can take any vector in $\Bbb R^m$ without a zero entry. For $m=2$ we can take $\begin{pmatrix}
1 &2  & 3 & ... &n \\ 
2 & 3 & 4 & ... & n+1
\end{pmatrix}$. Indeed, $\det\begin{pmatrix}
a &b \\ 
a+1 & b+1
\end{pmatrix}=a-b$ is nonzero when $a \neq b$. This does not generalize, since $a_{ij}=i+j-1$ fabulously fails for all submatrices of size $\geq 3$. I also tried taking the rows to be $m$ of the cyclic shifts of the vector $(1,2, ... ,n)$. This does not work either because I found a $k=3, m=n=5$ counterexample. I do feel that the answer should be positive, however. Is it?","['matrices', 'linear-algebra', 'inverse']"
1886332,Continuous function with compact set,"Let $f$ be a continuous function from the reals $\mathbb{R}$ onto $I=[0,1]$ with usual topology. Prove that if $C$ is a subset of $I$ and the preimage of $C$ is closed in $\mathbb{R}$ then $C$ is closed in $I$. My attempt is to use normal space properties but it does not help.","['general-topology', 'real-analysis', 'continuity', 'compactness']"
1886341,Is saying a set is nowhere dense the same as saying a set has no interior?,"I have two statements ""A set X is nowhere dense"" ""A set X has no interior"" Are these both equivalent statement?",['general-topology']
1886385,A descending sequence of sets with outer measure 1,"Show that there is a sequence of sets $A_n\subseteq [0,1]$, with outer measure 1 ($\mu^*(A_n)=1$ for every $n$), so that $A_1\supseteq A_2\supseteq A_3\supseteq...$ and $\bigcap_{n=1}^{\infty}A_n=\varnothing$. From these conditions, the sets mustn't be measurable, so my direction is to use $\mathbb{R}/\mathbb{Q}$ in some way (like the standart construction of a non-measurable set), but I can't figure out how. Any ideas?","['outer-measure', 'lebesgue-measure', 'measure-theory']"
1886394,Solving $\sin{3x} = \sin{x}$: question about the $k2\pi$ part,"Why is $$\sin{3x} = \sin{x}$$ equivalent to $$3x = x + k2\pi$$
$$3x = \pi - x + k2\pi$$ and not $$3x + k2\pi = x + k2\pi$$
$$3x + k2\pi = \pi - x + k2\pi$$",['trigonometry']
1886399,"Stability of higher order ODE's, positive coefficients proof","Given some linear constant coefficient DE with: $$
(a_0D^n+a_1D^{n-1}+...+a_{n-1}D+a_n)y=f(t) \tag{1}
$$ Without loss of generality, we may assume that $a_0>0$, then:
  $$
\textit{(1) is stable} \Rightarrow a_0,...,a_n >0
$$ I've found the above statement on the MIT online course on ODE's. Since there is no analytic way of finding the roots of an n-order polynomial, I've tried proving it by rewriting the characteristic equation in Horner's scheme for some root $Re{\lambda_i}<0$ (since it's stable, all roots must have negative real part) - but without success. I also lack any intuition for why this statement should be true. Any hints on how to prove it?","['stability-in-odes', 'ordinary-differential-equations']"
1886408,$n$th derivative of $\sin(f(x))$,"This problem came up in some math that I am working out on my own, not from a textbook, so there may not be any solution. $$g(x) = \sin(f(x))$$ For any polynomial function $f(x)$, $$g'(x)=\cos(f(x))f'(x)$$ $$g''(x)=-\sin(f(x))f'(x)^2+f''(x)\cos(f(x))$$ $$g'''(x)=-\cos(f(x))f'(x)^3-3\sin(f(x))f'(x)f''(x)+\cos(f(x))f'''(x) \\ \vdots$$ As you can see, each one is much more complex than the last, and takes much longer to evaluate than the last. Is there pattern to find $g^{(n)}(x)$ without just brute-forcing it with the chain rule? EDIT: To make the question more specific to my case, $f(x)$ is a polynomial function, and I only need to find $g^{(n)}(x)$ at $x=0$. Thanks!","['derivatives', 'pattern-recognition', 'chain-rule', 'calculus']"
1886437,Integration of a cohomology class over a homology class.,"Can anyone explain to me what it is said in the following article : http://indico.ictp.it/event/a06114/material/0/0.pdf , page : $3$, by Mr. Aroldo Kaplan : The paragraph says : Stokes :
  $$ \int_M d \omega = \int_{ \partial M }\omega $$
  implies :
  $$ d \omega = 0 \ \ \Longleftrightarrow \ \ \int_{ \mathrm{boundary} } \omega = 0 \ \ \ \ \ \ \ \ \  \mathrm{and} \ \ \ \ \ \ \ \ \partial M = 0 \ \ \Longleftrightarrow \ \ \int_M \mathrm{exact} = 0 $$
  so, defining : $ H^k ( X ) = \dfrac{ \{ \omega \in \bigwedge^k \ : \ d \omega = 0 \} }{ \{ \omega \in \bigwedge^{k} \ : \ \omega = d \phi \} } = \dfrac{ \mathrm{closed} }{ \mathrm{exact} } $ , $ \ H_k ( X ) = \dfrac{ \{ M \subset X \ : \ \partial M = 0 \} }{ \{ M \subset X \ : \ M = \partial N \} } = \dfrac{ \mathrm{cycles} }{ \mathrm{boundaries} } $ the bilinear (function?):
  $$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$
  ( = flux of $ \omega $ through $ M $ ) is well defined. So, I still do not understand why:
$$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$
is well defined.
Can you explain to me that please ? Thanks in advance for your help.","['algebraic-topology', 'homology-cohomology', 'integration']"
1886448,A kind of non-Abelian shift operation,"I'd like to define (and calculate the properties of) an object like $e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)$, where $A$ is a collection of matrices written in a vector form, say $\boldsymbol{A\cdot\nabla}=A_{x}\partial_{x}+A_{y}\partial_{y}$ for instance (I think that's the simplest non-trivial example), in which case $f\left(\boldsymbol{x}\right)\equiv f\left(x,y\right)$. The difficulty is that $\left[A_{x},A_{y}\right]\neq0$. $f$ has all the properties one needs (smoothness and co ; it comes from a problem of physics), and it commutes with the $A$'s. The $A$'s do not depend on the coordinates (in the example, this property would read $\partial_{x}A_{x,y}=\partial_{y}A_{x,y}=0$) Does this object make any sense ? Does it have a name ? For instance, I'd like to show something like 
$$e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)e^{-\boldsymbol{A\cdot\nabla}}g\left(\boldsymbol{x}\right)=f\left(\boldsymbol{x}+\boldsymbol{A}\right)g\left(\boldsymbol{x}\right)$$
but this is obviously wrong by direct expansion (it is true when e.g. $A_{y}=0$ in the example above). At second order there are some commutators $\left[A_{i},A_{j}\right]$ appearing. Any help is warm welcome. (I'm not even sure the tags below are well chosen :-(","['partial-differential-equations', 'operator-theory', 'mathematical-physics', 'multivariable-calculus', 'linear-algebra']"
1886449,Differential of the flow of a vector field,"Let $M$ be a compact manifold, and let $X$ be a vector field. Then its flow, $\varphi$, generates a $1$-parameter group of diffeomorphisms $\{\varphi_t\}$. For each $t$, we have a map $\varphi_t:M\to M$. Thus, for $p\in M$, the differential $\mathrm d(\varphi_t)_p:T_pM\to T_{\varphi_t(p)}M$ is defined. Is there a general formula for $\mathrm d (\varphi_t)_p(v),v\in T_pM$? I am particularly interested in the case when $X=\mathrm{grad}\,f$, where $f$ is a smooth function and the gradient is determined using a Riemannian metric $g$ on $M$ and when $p$ is a critical point of $f$. Then $X$ vanishes at $p$ and the flow has $p$ as a fixed point, so $\mathrm d(\varphi_t)_p:T_pM\to T_pM$ is an endomorphism.","['riemannian-geometry', 'differential-geometry', 'differential-topology']"
1886474,solve $\tan{x} = \tan{3x}$,"I'm asked to solve $\tan{x} = \tan{3x}$ Here's my attempt : $$\tan{x} = \tan{3x}$$
$$\tan{x} = \tan{(x + 2x)}$$
$$\tan{x} = \frac{\tan{x} + \tan{2x}}{1-\tan{x}\tan{2x}}$$ Recall the identity:
$$\tan{2x} = \frac{2\tan{x}}{1-\tan^2{x}}$$ So then we have:
$$\tan{x} = \frac{\tan{x} + \frac{2\tan{x}}{1-\tan^2{x}}}{1-\tan{x}\frac{2\tan{x}}{1-\tan^2{x}}}$$
$$\tan{x} - \tan^2{(x)} \cdot \frac{2\tan{x}}{1-\tan^2{x}} = \tan{x} + \frac{2\tan{x}}{1-\tan^2{x}}$$
$$-\tan^2{(x)} \cdot \frac{2\tan{x}}{1-\tan^2{x}} = \frac{2\tan{x}}{1-\tan^2{x}}$$
$$-\tan^2{x} \cdot \frac{2\tan{x}}{1-\tan^2{x}} \cdot \frac{1-\tan^2{x}}{2\tan{x}} = 1$$
$$\tan^2{x} = -1$$ This does obviously not compute. Why is my way wrong and how can I go about solving it?",['trigonometry']
1886489,A matrix with positive principal minors has positive eigenvalues if it is real.,"Let $A$ be an $n\times n$ matrix, not necessarily symmetric, and suppose all principal minors of $A$ are positive. How to show that any real eigenvalue of $A$ is must be positive?","['matrices', 'linear-algebra']"
1886517,If $X=A\cup B$ show that $X=\bar{A}\cup B^{\circ}$,"I have recently encountered this problem in my topology course. If $(X,\mathcal{T})$ is a topological space, show that if $X=A\cup B$ then $X=\bar{A}\cup B^{\circ}$, where $\bar{A}$ denotes closure and $B^\circ$ denotes interior. At the outset I thought this would be quite simple to prove, as intuitively it seems quite obvious, however that has not proved the case. My thought was to just show containment in both directions. Showing that $\bar{A}\cup B^{\circ} \subseteq X$ is rather simple. The other way is providing headaches. I initially thought that contradiction would be the best attack so I assumed for some $x\in X$ that $x \notin \bar{A}\cup B^{\circ}$. Hence using the fact that in this case $A^c=B\backslash A$ and with the elementary properties of set operations I arrive at $x\in \bar{B}\backslash \bar{A} \cap \overline{A\backslash B}$. The inequality for the intersection of closures is the wrong way for me to conclude that $x\in \phi$ and so get my contradiction. I have tried tinkering for hours with different combinations of set differences and closures\interiors to no avail. I'm starting to think that contradiction isn't the best idea. I have just started my topology course so I don't know more than the definition of a topology plus bases,closures, interiors, boundaries all defined in an abstract topological sense. This question lies before the notions of a metric and limit points have been introduced so I'm fairly certain that this question can be answered using simple techniques only. My other idea is to show that $\partial A= \partial B$ but I'm not sure where to start with this idea at the moment as my brain has retired hurt. If anyone can tell me which of my ideas to pursue and a push in the right direction I'd be very appreciative.","['general-topology', 'elementary-set-theory']"
1886524,Bijection from $\textrm{GL}_n/O_n$ onto the set of symmetric matrices.,"Let $k$ be an algebraically closed field with characteristic $\neq 2$, and let $O_n$ be the group of orthogonal matrices, i.e. invertible matrices whose inverse is their transpose.  Let $\textrm{GL}_n/O_n$ be the set of left cosets of $O_n$ in $\textrm{GL}_n$.  Is there a natural bijection $f$ from $\textrm{GL}_n/O_n$ onto the set of symmetric $n$ by $n$ matrices? My first guess would be to associate with an $x \in \textrm{GL}_n$ the symmetric matrix $xx^t + x^tx$.  Then if $yx^{-1} \in O_n$, then $1 = (x^{-1})^ty^tyx^{-1}$ implies $x^tx = y^ty$, and similarly the fact that $xy^{-1} \in O_n$ implies that $xx^t = yy^t$, so $xx^t + x^tx = yy^t + y^ty$.  Thus the mapping $\overline{x} \mapsto xx^t + x^tx$ is at least well defined. I think I remember hearing somewhere that this mapping can be shown to be surjective.  But I am still at a loss to prove that $x,y$ invertible and $xx^t + x^tx = yy^t + y^ty$ implies that $xy^{-1} \in O_n$.","['matrices', 'matrix-decomposition', 'linear-algebra']"
1886546,Hartshorne Lemma II.5.3 Proof,"The question I have is from the proof of the lemma above, but it is actually a more general statement about quasi-coherent sheaves on an affine scheme. Suppose $X= \text{Spec }A$ for some ring $A$, and $\mathscr{F}$ is a quasi-coherent sheaf on $X$. Then for some open affine cover of $X$, the restriction sheaf is isomorphic to a sheaf of a module over the corresponding ring. In particular, if $\text{Spec }B$ is in the cover, then $\mathscr{F}|_{\text{Spec } B} \cong \widetilde{M}$ for a $B$-module $M$. This part is by definition. Now $\text{Spec }B$ is covered by distinguished open sets of the form $D(g)$ for $g\in A$, and for any such open set the inclusion $D(g)\subseteq \text{Spec }B$ is induced by the ring map $B\to A_g$. Thus $\mathscr{F}|_{D(g)} \cong (M\otimes_B A_g)^{\tilde{}}$. He deduces the last sentence from a previous proposition that deals with properties of the sheaves of modules. The two properties that seem important for this deduction are the following: For a ring map $A \to B$ inducing the map of spectra $f:\text{Spec }B \to \text{Spec }A$, (1) If $M$ and $N$ are $A$-modules, then $(M\otimes N)^{\tilde{}} \cong \widetilde{M} \otimes_{\mathcal{O}_{\text{Spec }A}} \widetilde{N}$. (2) For any $A$-module $M$, $f^*(\widetilde{M})\cong (M\otimes_{A} B)^{\tilde{}}$. I cannot seem to make the connection. So any help with his last statement would be great. Thanks.","['quasicoherent-sheaves', 'algebraic-geometry', 'commutative-algebra']"
1886596,Why is the derivative of $\frac{3e^{2x}}{2}$ equal to $3e^{2x}$?,$$f(x) = \frac{3e^{2x}}{2}$$ $$f'(x) = 3e^{2x}$$ Why is this true?,"['derivatives', 'calculus']"
1886619,"""Infinitely often"" interpretation of lim sup of sequence of sets","One type of interpretation of the lim sup and lim inf of a sequence of sets is as follows: $\text{lim sup}_n A_n = \{ \omega: \omega \in A_n \text{ for infinitely many } A_n \}$ $\text{lim inf}_n A_n = \{ \omega: \omega \in A_n \text{ for all but finitely many } A_n \}$ I don't understand the difference between these two statements. Since there are infinitely many $A_n$, I am tempted to conclude that the latter statement automatically implies that $\omega \in A_n$ for infinitely many $A_n$? Is the difference basically that elements lim sup are not necessarily in the lim inf, because they might occur in every single $A_n$, whereas elements of the lim inf are definitely in the lim sup?",['measure-theory']
1886621,Chow ring of weighted projective space,"Is the structure of the Chow ring of the weighted projective space $\mathbb{P}(a_0, \ldots a_n)$ known? The integral cohomology ring of $\mathbb{P}(a_0, \ldots a_n)$ is apparently described in the introduction of The equivariant cohomology ring of weighted projective spaces , eqn. (1.4). It's temping to hope that the Chow ring isomorphic to this ring, as is the case in standard projective spaces (and Grassmannians, etc.). Thanks! Edit: While I'm prepared to dive into the geometry of toric varieties, it would be nice to have this specific case answered without recourse to the general theory.","['intersection-theory', 'projective-geometry', 'algebraic-geometry']"
