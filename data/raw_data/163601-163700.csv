question_id,title,body,tags
2845535,Probability of sixth ball to be white,"A box contains 7 identical white balls and 5 identical black balls. They are to be drawn randomly one at a time without replacement until the box is empty. Find the probability that the 6th ball drawn is white, while before that exactly 3 black balls are drawn. Source : Principle and techniques in combinatorics by Chen Chuan Chong , Ch 1, question 24 My Approach Let '1' denote a white ball and '0' denote a black ball. Consider following sequence: 1 1 0 0 0 1 (i.e., first two balls drawn are white then next three balls are black and finally a white ball) The probability of this event is: $\displaystyle \frac 7{12} \cdot \frac 6{11} \cdot \frac 5{10} \cdot \frac 49 \cdot \frac 38 \cdot \frac 27 \cdot = \frac 1{132}$ Now, there are 10 binary sequences in which 6th digit is 1 and before them three digits are 0 hence, probability should be $\displaystyle \frac {10}{132}$ Correct Answer : $\displaystyle \frac{25}{132}$ Please indicate my mistake and if possible please give a complete solution. Thank You","['permutations', 'combinatorics', 'probability', 'proof-verification']"
2845580,"Proving the derivative of the multivariable function $f(x,y)=\frac{x^2y}{x^2+y^2}$ exists.","Given is the function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ with $f(0,0)=0$ and $f(x,y)=\dfrac{x^2y}{x^2+y^2}$ for $(x,y)\neq(0,0)$ . I understand the function is continuous at $(0,0)$ and its partial derivatives exist at $(0,0)$ . I need to prove the function is differentiable at $(0,0)$ . The problem is that I'm unsure of what ""derivative"" means in this context. Is it the total derivative? So far I thought of $$\frac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}=\frac{\frac{x^2y}{x^2+y^2}}{\sqrt{x^2+y^ 2}}\leq\frac{x^2y}{(x^2+y^2)y}=\frac{x^2}{x^2+y^2}\leq\frac{x^2}{x^2}=1.$$ Therefore $\lim_{(x,y)\rightarrow(0,0)}\dfrac{f(x,y)-f(0,0)}{\sqrt{x^2+y^2}}\leq1$ . Is this enough to prove the derivative exists in $(0,0)$ ?","['multivariable-calculus', 'derivatives']"
2845598,How To Calculate Binomial Distribution Of Really Small %?,"I asked this question on the Bitcoin Forum , but I think it's more appropriate for a mathematics forum. I'm making an informative video and I need a binomial distribution calculation. I want to find out how many trials are needed to get 1%, 50% and 90% likelihood 1 or more successes. The problem is that the likelihood of success is 1 out of 2^160 (number of distinct bitcoin/ethereum addresses). Normally for something like this, I would use a binomial distribution calculation in Excel using this formula: =1-BINOM.DIST(0,????,2^-160,TRUE) I would then tinker with the ???? until the entire cell result returned 1%, 50% and 90%. However, Excel can't handle numbers anywhere near this large. Does anyone know of a way I can calculate the number of trials required for these 3 percentages given the infinitesimally small chance of success? It would be great if there was an online tool I could use to support my results. Just to illustrate what I'm looking for. If this analysis was for something much simpler, such as a probability of success being 1% , then I could calculate the results to be: 229 trials needed for 90%, | 89.99% =1-BINOM.DIST(0,229,0.01,TRUE) 69 trials needed for 50%,  | 50.01% =1-BINOM.DIST(0,69,0.01,TRUE) 1 trial needed for 1%,  | 1.00% =1-BINOM.DIST(0,1,0.01,TRUE)","['statistics', 'binomial-distribution', 'probability']"
2845605,Learning Differentiable Manifolds prior to Classical Differential Geometry,"Is it possible to study Differentiable Manifolds from Lee's ""Introduction to smooth manifolds"" before studying classical differential geometry from DoCarmo's text or Kristopher Tapp's ""Differential Geometry of Curves and Surfaces"". Right now I'm hearing two sides, with one saying that Differentiable Manifolds doesn't really tie in with Classical Differential Geometry and the other opposing side states that its necessary to understand Classical Differential Geometry to supposedly help with intuition. In this case I'm wondering what side is right in terms of what course of action to take. Study Differentiable Manifolds first then go back to studying classical differential geometry or start in the other direction? I currently understand Topology, Abstract Algebra, Real Analysis, Linear Algebra.","['smooth-manifolds', 'differential-geometry']"
2845609,"Is $\sum_{m\in \mathbb Z} f(x-m)f(x-n) \in L^2(a,b)$ if $f\in L^2(\mathbb R)$?","Let $f\in L^2(\mathbb R)$ and $0<a<b< \infty,$ put $A= \{x\in \mathbb R: a<|x|<b\}.$
Fix $n\in \mathbb Z.$ Define
 $$ F_n(x)=\sum_{m\in \mathbb Z} f(x-n-m)f(x-m)$$
We note that $F_n$ is a periodic function with period 1.  And hence we cannot expect $F_n$ belongs to $L^2(\mathbb R).$ But can we expect  $F_n\in L^2(A)$? I'm also curios to know what happens of summation over $n$? Specifically, define 
$$G(x)= \sum_{n\in \mathbb Z} F_n(x).$$ Can we expect    $G\in L^2(A)$?","['real-analysis', 'examples-counterexamples', 'functional-analysis', 'lp-spaces', 'sequences-and-series']"
2845620,An inequality relating the ratio of the areas of two triangles,"The conjecture below is a modified version of this question: Prove that, given a triangle with sides $a,b,c$, there exists a triangle with sides $a+2b,b+2c,c+2a$ that has an area three times the original Conjecture: If $u$ is the area of a triangle with sides $a,b,c$, and $v$ is the area of a triangle with sides $a+2b,b+2c,c+2a$, then ${\large{\frac{v}{u}}}\ge 9$. Remarks: For the equilateral case ($a=b=c$), we get ${\large{\frac{v}{u}}}=9$. Limited data testing seems to support the truth of the conjecture. Trying to prove the claim via Heron's formula appears to be a disaster. Question: $\;$Is the conjecture true?","['inequality', 'geometry']"
2845624,"How can I tell if a matrix is diagonalizable knowing only the trace, one eigenvalue, and a result of the characteristic polynomial?","Given $A$, a $3 \times 3$ matrix, and: $\mathrm{tr}(A) = −2$, $\mathrm{rk}(A−2I)< 3$, $\chi_A(1) = −8$ ($\mathrm{tr}$: trace, $\mathrm{rk}$: rank, and $\chi_A(x)$: characteristic polynomial) How can I tell if the matrix is diagonalizable? What are the eigenvalues?
I know that given $\mathrm{rk}(A−2I)< 3$, $2$ must an eigenvalue with multiplicity $1$, at least. But I don't know what information does ""$\chi_A(1)=−8$"" provide me.","['eigenvalues-eigenvectors', 'matrix-equations', 'matrices', 'matrix-rank', 'linear-algebra']"
2845627,"Nonsmooth initial data in the conservation laws, their approximations and limits","In the book by R. LeVeque: ""Numerical methods for conservation laws"", Birkhauser, (1992), 2nd edition, in the Subsection 3.1.2 called "" Nonsmooth data "", the author talks about possibilities for finding generalized solutions of the conservation laws when we have nonsmooth initial data. On $22^{nd}$ page he says: ""One possibility is to approximate nonsmooth data $u_0(x)$ by a sequence of smooth functions $u^{\epsilon}_0(x)$ , with $\parallel  u^{\epsilon}_0(x)- u_0 (x) \parallel_{L^1} <\epsilon \: \mbox{as} \: \epsilon \rightarrow 0$ ... Unfortunately, this approach of smoothing the initial data will not work for nonlinear
problems."" My (nonlinear) problem is this: let's say we have conservation law $$\begin{cases}
	u_t+f(u)_x=0, \\[2ex] 
	u(x,0)=u_0 (x),
\end{cases}$$ where $u_0$ is nonsmooth Riemann initial data $$u_0(x)= \begin{cases}
	u_l, x<0, \\[2ex]
	u_r, x>0,
\end{cases}$$ where $u_l$ and $u_r$ are constants. We approximate the Riemann data with a smooth data that depends on some parameter $\epsilon$ , i.e. we change $u_0$ with $u^{\epsilon}_0(x)$ . In my case, the new initial data $u^{\epsilon}_0(x)$ are in the Sobolev space $H^s(\mathbb{R})$ , where $s$ is integer bigger than five. This is now a Cauchy problem with smooth data that we can solve. We find solutions of the approximate problem in some Sobolev-valued $H^s(\mathbb{R})$ space (e.g. $C([0,T];H^s(\mathbb{R}))$ , $L^p(0,T,H^s(\mathbb{R}))$ or similar).  And now we have a solution of approiximate problem that depends on parameter $\epsilon$ . In order to get back on the initial problem we should let $\epsilon\rightarrow 0$ . My question is: what do we need in order to pass to the limit $\epsilon\rightarrow 0$ (and then get a connection with a solution of the Riemann problem)? Generally I've always thought that this is possible either directly, by using some compactness lemma or with some energy estimates, but I am not sure how to do it in this case. Here on the one side we have a classical smooth solution, and  on the other side, after we let $\epsilon\rightarrow 0$ , we should get discontinuous (weak) solution possibly in some other space. So we have different type of solutions. Also if anyone know any paper/book where pde problem similar to this is studied let me know. I am interested in problems where we change initial condition to the smoooth one, than solve the ""approximation problem"" and than connected that solution with the solution of the original problem. It could be by letting $\epsilon\rightarrow 0$ directly or using some theorem.","['partial-differential-equations', 'reference-request', 'hyperbolic-equations', 'limits']"
2845668,Let $f'(x)=e^{x^2}$; compute $\lim_{x\to0}\frac{f(2)-f(x+2)}x$,"Let $f : [0,\infty] \to \mathbb{R}$ be a function such that its derivative $f'(x)=e^{x^2}$. Compute $$\lim_{x\to 0} \frac{f(2)-f(x+2)}{x}$$ The definition of the derivative is $$f'(a)= \lim_{x\to 0} \frac{f(a+x)-f(a)}{x}$$ so if $f'(a)=e^{a^2}$ then $$e^{a^2}=\lim_{x\to 0} \frac{f(a+x)-f(a)}{x}$$ if $a=2$ then $$e^{a^2}=\lim_{x\to 0} \frac{f(2+x)-f(2)}{x}$$ I'm a little bit stumped here, my naive approach would be to multiply both sides by $(-1)$ and get that the answer is $-e^{2^2}$ but I don't think I can do that.","['derivatives', 'calculus']"
2845696,Fixed point method for non-homogeneous ode and pde,"During my course in mathematical methods for physics, my professor introduced us to the ""method of fixed point"" for studying non-homogeneous systems of odes, odes and pdes. Although I think I understood how to use it, he wasn't so clear about WHERE to use it. Let me make some examples: $$\begin{align}&\left\{\begin{matrix}\dot{x}=3x-2y+f(t)\\\dot{y}=-2x+3y\end{matrix}\right.& f(t) = \left\{\begin{matrix}1\;\;\;\;0<t<1\\0\;\;\;\;t>1\end{matrix}\right.\end{align}$$$$\left\{\begin{matrix}\partial_{t}u = D\partial^{2}_{xx}u + f(x) \\ f(x) = \frac{d^2}{dx^2}e^{-ax^2}\\u(x,0)=e^{-ax^2}\end{matrix}\right.$$ $$\left\{\begin{matrix}\partial_{t}\phi = D\partial^{2}_{xx}\phi + \sin(2x)\\ 0<x<\pi\\ \phi(x,0) = \sin(x)+3\sin(8x) \\ \phi(0,t)=\phi(\pi,t)=0\end{matrix}\right.$$ Two of these problems, mainly the last two, I know for a fact that can be solved around their fixed points: $$\tilde{u}(x,t) = u(x,t) + {1\over D}e^{-ax^2}$$ and $$\tilde{\phi}(x,t) = \phi(x,t) + {1\over{4D}}sin(2x)$$ (even if the second one is pretty unnecessary). But for the first one I'm pretty confused. I know that that system of odes can be rewritten in the form $$\dot{\underline{x}} = \hat{A}\underline{x}+\underline{f}$$ which has solution $$\underline{x}(t) = \underline{x}(0)e^{\hat{A}t}+\int_{0}^{t}e^{\hat{A}(t-t')}\underline{f}(t')dt'$$ and from what I understood during classes I cannot study a non-homogeneous in the fixed point if the external force is a function of time , but in my notes I found that our professor studied the fist system with a variable $$\underline{z} = \underline{x}-\hat{A}^{-1}\underline{f}$$ which seems to me like a fixed point! Probably I'm confusing a lot but I cannot wrap my head around it. So coming to my question: is what I said in bold true or am I going crazy? Is it possible to study a problem like this one problem like this one with the fixed point method?
Thank you very much in advance to anyone that can give me some kind of explanation. Sadly this course is very interesting but the limited time available makes it very hard to explain everything in detail.","['fourier-analysis', 'systems-of-equations', 'partial-differential-equations', 'fixed-point-theorems', 'ordinary-differential-equations']"
2845703,"Total variation distance, bayesian networks and dependence","Consider the following bayesian network depicted below If $X_{1:L}$ are i.i.d. random variables where, for any $i \in [1:L]$, $X_i \in \mathcal{X}$ is uniformly distributed, we can factorize $q^{(1)}_{Z_1\dots Z_L}$ as \begin{equation}
q^{(1)}_{Z_1\dots Z_L} = \frac{1}{|\mathcal{X}|^L} \sum_{x_{1:L}} \prod_{i=1}^L p_{Z|AB}(z_i|x_i,x_{i-1}).
\end{equation} Now, consider a new bayesian network, where, for all $i \in [1:L]$, the dependency between $X_{i-1}$ and $Z_{i}$ is removed, and $X_{i-1}$ is replaced by a new r.v. $\bar{X}_{i-1}$ equally distributed and connected to $Z_{i}$. In this case, we have
\begin{align}
q^{(2)}_{Z_1\dots Z_L} & = \frac{1}{|\mathcal{X}|^{2L}} \sum_{x_{1:L}}\prod_{i=1}^L \sum_{\bar{x}_{i-1}} p_{Z|AB}(z_i|x_i,\bar{x}_{i-1})
\end{align} The total variation distance between $q^{(1)}_{Z_1\dots Z_L}$ and $q^{(2)}_{Z_1\dots Z_L}$ is
\begin{align}
& d_{TV}(q^{(2)}_{Z_1\dots Z_L},q^{(1)}_{Z_1\dots Z_L}) \\
& \quad = \sum_{z_{1:L}} \bigg| q^{(2)}_{Z_1\dots Z_L}(z_{1:L}) - q^{(1)}_{Z_1\dots Z_L}(z_{1:L}) \bigg| \\
& \quad = \sum_{z_{1:L}} \bigg| \frac{1}{|\mathcal{X}|^{2L}} \sum_{x_{1:L}}  \prod_{i=1}^L \sum_{\bar{x}_{i-1}} p_{Z|AB}(z_i|x_i,\bar{x}_{i-1}) 
- 
\frac{1}{|\mathcal{X}|^L} \sum_{x_{1:L}} \prod_{i=1}^L p_{Z|AB}(z_i|x_i,x_{i-1}) \bigg|
\end{align} If $p_{Z|AB}(z|a,b) = p_{Z|AB}(z|a,b^{\prime})$ for any $a,b,b^{\prime} \in \mathcal{X}$ (that is, $Z$ is determined only by $A$), we have
\begin{align}
& d_{TV}(q^{(2)}_{Z_1\dots Z_L},q^{(1)}_{Z_1\dots Z_L}) \\
& \quad = \sum_{z_{1:L}} \bigg| \frac{1}{|\mathcal{X}|^{2L}} \sum_{x_{1:L}}  \prod_{i=1}^L |\mathcal{X}| p_{Z|AB}(z_i|x_i,x^{\prime}) 
- 
\frac{1}{|\mathcal{X}|^L} \sum_{x_{1:L}} \prod_{i=1}^L p_{Z|AB}(z_i|x_i,x^{\prime}) \bigg| \\
& \quad = \sum_{z_{1:L}} \bigg| \frac{1}{|\mathcal{X}|^{L}} \sum_{x_{1:L}}  \prod_{i=1}^L  p_{Z|AB}(z_i|x_i,x^{\prime}) 
- 
\frac{1}{|\mathcal{X}|^L} \sum_{x_{1:L}} \prod_{i=1}^L p_{Z|AB}(z_i|x_i,x^{\prime}) \bigg| \\
& \quad = 0.
\end{align} QUESTION: Is it also true that $d_{TV}(q^{(2)}_{Z_1\dots Z_L},q^{(1)}_{Z_1\dots Z_L}) = 0$ implies $p_{Z|AB}(z|a,b) = p_{Z|AB}(z|a,b^{\prime})$ for any $a,b,b^{\prime} \in \mathcal{X}$.","['independence', 'probability-distributions', 'information-theory', 'statistics', 'bayesian-network']"
2845766,Negative eigenvalues of Sturm-Liouville problem,"I am trying to solve the following Sturm-Liouville problem on the circle $S^1$, i.e. the closed interval $[0,1]$ with periodic boundary conditions $(y(0)=y(1))$: $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=-\frac{\lambda}{p(x)}y.$$ Here $p$ is a positive function and is also the first eigenfunction (corresponding to $\lambda_0=-1$). In particular, I am would like to know how many negative eigenvalues the system has. Idea : Note that there can be only finitely many negative eigenvalues, since the eigenvalues satisfy $$-\infty <\lambda_0<\lambda_1\leq \dots \to \infty.$$ A theorem (8.3.1) from Coddington-Levinson says that the eigenfunctions $y_{2i+1}$ and $y_{2i+2}$ corresponding to $\lambda_{2i+1},\lambda_{2i+2}$ have precisely $2i+1$ zeros on $[0,1)$. Therefore, if $\lambda =0$ was an eigenvalue, I could count the zeros of the corresponding eigenfunction to determine how many negative eigenvalues there are. So I tried solving $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=0,$$ but haven't gotten anywhere. Are there any general results on the number of negative eigenvalues of Sturm-Liouville systems?","['eigenvalues-eigenvectors', 'sturm-liouville', 'dynamical-systems', 'functional-analysis', 'ordinary-differential-equations']"
2845787,Linearized Pitot system,I am trying to create a linearized model of a compressible pitot tube system with altitude $h$ as the input and velocity as the output. When I take the derivative and try and linearize around a point the derivative does not match (approximately) the delta between two points around it. I have pitot tube velocity equation: $T(h) =$ Temperature $P(h) =$ static pressure $P_o =$ stagnation pressure ------------------------------Constants------------------------------- $P_s = 101$ KPa $T_o = 290$ K $\gamma = 1.4$ $M = 0.0289644$ ----------------------- Governing Equations------------------------ $P = P_se^\frac{h*g*M}{RT}$ $ T = T_o-.0065*h$ $Mach = \sqrt{\frac{2}{(\gamma-1)}\biggr((\frac{P_o}{P(h)})^\frac{\gamma}{\gamma-1}-1\biggr)}$ $a = \sqrt{\gamma RT}$ $v = Mach*a$ My approach was to take the derivative (using chain rule) and add that to the nominal point $dv = \frac{dv}{dT}\frac{dT}{dh} + \frac{dv}{dP}\frac{dP}{dh}$ so that: $v = v_{nom} + dv$ I'm not sure about my method or my derivation using the chain rule.,"['derivatives', 'linearization']"
2845823,Where can I find this article by I. Ruzsa?,"Title says it all. Have tried googling and my college library, but no success so far. I. Ruzsa, On the cardinality of $A + A$ and $A − A$. In Combinatorics (Keszthely, 1976), Coll. Math. Soc. Bolyai 18 , Akadémiai Kaidó (1979), 933–938","['additive-combinatorics', 'combinatorics', 'sumset', 'reference-request']"
2845824,Find all Equilateral Triangles with vertices on 3 straight lines,"I have 3 straight lines of equations:
$$r:\begin{cases}x+y-z=0\\2x-y=0\end{cases}$$
$$s:\begin{cases}2x-z=1\\x-y=0\end{cases}$$
$$t:\begin{cases}x+z=0\\x+y-z=\frac{1}{2}\end{cases}$$ Find all equilater triangles with vertices on $r,s,t$. My idea was to equal the distance of the 3 straight line with generic points, but this way seems to be little complicated. It's like calculate:
$$\lVert \overline{RS}\rVert=\lVert \overline{ST}\rVert=\lVert \overline{TR}\rVert$$
with $R\in r$, $S\in s$, $T\in t$, generic points on respective straight lines. Is there a simple way to do this?","['linear-algebra', 'geometry']"
2845837,"If $X_1$ and $X_2$ are random variables, then how is the random variable $X=(X_1,X_2)$ defined?","If $X_1$ and $X_2$ are (we can suppose real-valued) random variables, then how is the random variable $X=(X_1,X_2)$ defined? Is there a name for this concept so I can look it up on i.e. Wikipedia? (I have come across this notation in a Decision Theory textbook). A specific question I have in mind: if $ϕ∈Ω$ (i.e., our sample space) and $X_1(ϕ)=1$ and $X_2(ϕ)=2$, then what is $X(ϕ)$ in this context?","['probability-theory', 'statistics', 'random-variables']"
2845846,"is there any maping from $S^1$ to $S^1$ of odd degree, which is not an odd function?","I want to prove for every continous function from $s^n$ to $s^n$ of odd degree there exists $x$ such that $f(-x)=-f(x)$ so I used this ""that the sum of two functions of odd degree must be odd"" but I don't know if this is really true or not.
I just know that sum of two odd functions is odd but I don't think every function of odd degree has to be an odd function, so I'm trying to find an example for this using a function from $s^1$ to  $s^1$.
thanks for any help","['algebraic-topology', 'even-and-odd-functions', 'functions']"
2845888,"Why can a real number be defined as a Dedekind cut, that is, as a set of rational numbers?","I don't know if my textbook is written poorly or I'm dumb. But I can't bring myself to understand the following definition. A real number is a cut , which parts the rational numbers into two classes. Let $\mathbb{R}$ be the set of cuts. A cut is a set of rational numbers $A \subset \mathbb{Q}$ with the following properties: i) $A \neq \emptyset$ and $A \neq \mathbb{Q}$ . ii) if $p \in A$ and $q < p$ then $q \in A$ . iii) if $p \in A$ , there exists some $r \in A$ so that $p < r$ (i.e. $A$ doesn't contain the ""biggest"" number). That's a literal translation from my textbook (which is written in Slovenian). All seems fine and I can get my head around all of the postulations except for one. The definition states in the beginning ""A real number is a cut..."", but then it also states ""A cut is a set of rational numbers..."" So a real number is 'a set of rational numbers'?! It's not my bad translation, I swear, I'm quite good at English. Either the textbook is written in such a convoluted manner that I can't properly understand the wording the author chose or I'm overlooking something big . Could you please clarify and explain the definition in full detail?","['real-analysis', 'real-numbers']"
2845918,Smooth curves that are submanifolds,"Let $f:I\subseteq \mathbb{R} \to \mathbb{R}^n$ be a smooth function ($I$ is an interval in $\mathbb{R}$). What are the sufficient conditions (or sufficient and necessary) under which the image of this function (say $S$ which is indeed a curve in $\mathbb{R}^n$) is a submanifold? Is there any famous theorem on this topic? I tried to find such conditions in this way: according to the definition of submanifold, we need a 1-dimensional submanifold chart for $S$ at every point $x$ of $S$. Thus, there should exist open interval $W \subseteq \mathbb{R}$ and smooth function $G: W \to \mathbb{R}^n$. We need $G(W)=U \cap S$ where $U\subseteq \mathbb{R}^n$ is an open set and $x\in S \cap U$. Finally, we need the function $G$ to have the form $G(w)=A\left( {\begin{array}{*{20}{c}}
w\\
g(w)
\end{array}} \right)$ where $A$ is non-singular and $g:W\to\mathbb{R}^{n-1}$ is a smooth function (I used the definition from ""Ordinary Differential Equations with Applications by C. Chicone"" ). I think the sufficient condition is that $f$ should be a diffeomorphism (it should be smooth and have smooth inverse) and $I$ be an open interval. Actually, in this case, the function $f$ plays the role of the function $G$ in the definition of submanifold. Thank you, in advance, for your help! Incidentally, I do not understand why we need $U$ in the definition of submanifold to be open? I am saying this because $S$ can be a closed subset of $\mathbb{R}^n$ and thus $U\cap S $ may not be open. Indeed, my question is that whay the image of $W$ under $G$ i.e., $G(W)=U \cap S$, is always open? Thanks.","['differential-topology', 'smooth-manifolds', 'manifolds', 'ordinary-differential-equations', 'differential-geometry']"
2845945,Generating borders with convex hull,"I have some polylines (streets on a map) take these three, for instance: I need to convert these polylines into polygons. The mapping library I'm using has a built in convex hull function which generates something like this: As you can see, using a convex hull to generate the area borders creates unwanted overlapping. What I need is to create a convex hull that sort of avoids adjacent lines, like this: There doesn't appear to be any built in method for doing this, so I'm going to have to get my hands dirty and write one myself. Problem is, I have no idea where to start. Any information to help get me started is appreciated. If someone could walk me thru the algorithm that would be even better. If there is a name for this algorithm please let me know so I can start searching.","['convex-hulls', 'polygons', 'geometry']"
2845992,$\lim_{p\to r} \|f\|_p=\|f\|_r$,"Let $(X,\tau,\mu)$ a measure space and $f$ positive measurable function, such that $f\not=0$ a.e.
Let $I_f=\left\{p\in [1,\infty]; \|f\|_p<\infty\right\}$ a) Show that $I_f$ is a interval. (I've already tried it.) b) Suppose $I_f\neq \emptyset.$
  Let $r\leq s<\infty$ in $I_f$ and let $p=\alpha r+(1-\alpha)s$ with $\alpha\in (0,1)$. 
Show that ${\|f\|_p}^p\leq {\|f\|_r}^{\alpha r}{ \|f\|_s}^{(1-\alpha)s}$.
(I've already tried it.) Deduce that the map $p\to \ln({\|f\|_p}^p)$ is convex on $I_f\setminus\left\{\infty\right\}$ and $p\to \|f\|_p$ is continuous on $int(I_f)$.
Hint: All convex function on interval J is continuous in $int(J)$. (I've already tried it.) x) Show that if $r$ is a extreme point of $I_f$, then $\|f\|_p \to \|f\|_r$ when $p\in I_f$, $p\to r$. c) Deduce, for all $r<p<s$ in $I_f$,then $||f||_p\leq \max(\|f\|_r,\|f\|_s).$
Conclude $L^r(X,\tau,\mu)\cap L^s(X,\tau,\mu)\subseteq \bigcap_{p\in (r,s)} L^p(X,\tau,\mu).$ How can the alternative x be demonstrated?
I have
${\|f\|_p}^p\leq {\|f\|_r}^{\alpha r}{\|f\|_s}^{(1-\alpha)s}$ then  $\lim_{p\to r} {\|f\|_p}^p\leq {\|f\|_r}^{\alpha r}{\|f\|_r}^{(1-\alpha)}$ because $\displaystyle {\lim_{p\to r} p}=r=\displaystyle \lim_{p\to r} \alpha r+(1-\alpha)s$, i.e., $s=r$
Therefore $\lim_{p\to r} {\|f\|_p}^p\leq {\|f\|_r}^{\alpha r}{\|f\|_r}^{(1-\alpha)}={\|f\|_r}^r$
Therefore $\lim_{p\to r} {\|f\|_p}^p\leq {\|f\|_r}^r$ It is correct?",['measure-theory']
2846019,Prove $2\mathbb{Z}$ and $3\mathbb{Z}$ are not isomorphic as rings.,"I want to prove the ring of even integers $2\mathbb{Z}$ is not isomorphic to $3\mathbb{Z}$. I do this by contradiction, assume $\phi$ is such an isomorphism from $2\mathbb{Z}$ to $3\mathbb{Z}$. Observe this equality: $$\phi(2)+\phi(2)=\phi(2+2)=\phi(2\cdot2)=\phi(2)\cdot\phi(2)$$ Let $x=\phi(2)$ and we have the equation $x^2-2x=0$ in $3\mathbb{Z}$. This factors into $x(x-2)=0$. $\phi(2)=0$ is not possible because any isomorphism will have $\phi(0)=0$. Thus we are left with $x-2=0$ which has no solution in $3\mathbb{Z}$. Question. Is this a valid argument? What does the $2$ symbol in the last equation denote, if not an element of $3\mathbb{Z}$?","['abstract-algebra', 'solution-verification']"
2846047,Are $1+p^3+p^6$ and $1+p^4+p^8$ coprime?,"What are the primes $p$ for which $1+p^3+p^6$ and $1+p^4+p^8$ are coprime?
I know it is true for $p=2$ and $p=3$ and not true for any $p \equiv 1 \mod 6$.  I conjecture that it true for all primes $p \equiv 5 \mod 6$. Any counterexample $> 10^8$. This is relevant to OEIS sequence A046685 .","['number-theory', 'prime-numbers', 'elementary-number-theory']"
2846107,Do other people use Strang's convention for integrals?,"Gilbert Strang's very vivid textbook Differential Equations and Linear Algebra uses a convention for integrals which seems unusual to me.  It is very handy in this context, though, and I would like to know if other people use it.  In effect he adopts the convention that an integral $\int_{a}^{b}f(s)ds$  only goes forwards from $a$ to $b$.  That is: $$\int_{a}^{b}f(s)ds = \begin{cases}
                         0, & \mbox{if } b\leq a \\
                         F(b)-F(a) & \mbox{when } a\leq b\mbox{ and } \frac{dF}{ds}=f(s).
                       \end{cases}$$ You can see this in his particular solution for the first order linear differential equation with shifted Heaviside function $H(t-T)$ as source.  That equation is
 $$y'-ay\ =\ H(t-T).$$
Here $t$ is the usual time variable, and $T$ is a constant time.  He gives the particular solution with initial value $y(0)=0$ as
$$y_p\ =\ \int_{T}^{t}e^{-as}ds \ =\ \frac{1}{a}(e^{a(t-T)} - e^{-at}).$$
The second equality is right if $t>T$, but for $t<T$ we must use Strang's convention for integrals. Do others use this convention? I can add: This book takes a very quick and practical approach a lot of the time.  It assumes without comment that (nearly) every function is real analytic, and while it points out some places where a solution goes to infinity it does not remark that this conflicts with assuming functions are limits of their Taylor series around arbitrary points.  It gives no proofs of existence or uniqueness of solutions to ode's except by implicitly suggesting these follow from integrating functions term by term in their Taylor series. But it is an extremely vivid book and I think it does well for a first course on ode's. The convention suits Strang's purposes.  You can see above it lets him write solutions more compactly.  He is never interested in moving the time variable backwards, and he usually cares more for the long run behavior of solutions than for what they do before some given time $T$. I think it would not be analysts who would use this convention (outside of Strang's textbook) but engineers or physicists.","['ordinary-differential-equations', 'notation', 'calculus']"
2846138,Gradient and Riemannian Hessian of function on an ellipse,"This is from an engineering assignment on Riemannian manifolds and I confess that this is my first experience with manifolds. Here is the problem: Let $\mathcal{M}\subset \mathbb{R}^2$ be an ellipse  of fixed major and minor axis. Lets say the embedding is $x=a\cos(\theta)$ and $y=b \sin(\theta)$. Let $f(\theta): \mathcal{M} \rightarrow \mathbb{R}$ be a smooth function on the ellipse. I wish to know what the gradient and Riemannian Hessian of $f$ will be (using local coordinates). My attempt: The Jacobian is $D=[-a \sin(\theta), b \cos(\theta)]^T$ so the inherited metric is $g_{1,1}=D^tD=a^2\sin^2(\theta)+b^2 \cos^2(\theta)$. Now the gradient should be $\Delta f= g^{1,1}\frac{d}{d\theta}=\left(a^2\sin^2(\theta)+b^2 \cos(\theta)\right)^{-1} \frac{df}{d\theta}$. Here the superscript is for inverse. For the Riemannian Hessian I think it should just be $\frac{d^2}{d\theta^2}$ but I do not know how to compute it. My intuition in this problem is that it should coincide with the result from change of coordinates: Writing the gradient like $\left(\frac{df}{dx}, \frac{df}{dy}\right)$ and then using the chain rule. Similarly for the Hessian matrix $H_{i,j=1}^2=\frac{d^2f}{dx^idx^j}$  where $x^1=x, x^2=y$. However, the answers I got do not coincide with that. If it helps solve the problem, note that eventually I am hoping to verify that the trace of the Hessian is the Laplace-Beltrami, which I have also not calculated yet but hope that I can after this is figured out.","['riemannian-geometry', 'differential-geometry']"
2846200,Sum of columns in matrix,"In a matrix with $a$ rows and $2b+1$ columns, each cell contains a nonnegative real number. Does there exist a fixed $k>0$, independent of $a$ and $b$, for which we can always choose a set of columns $C$, with $|C|=c\in [1,2b]$, so that For at least $k\cdot a$ rows, the sum of the $c$ numbers in each row is at least the sum of any other $2b-c$ (out of the remaining $2b+1-c$) numbers in the same row For at least $k\cdot a$ rows, the sum of the remaining $2b+1-c$ numbers in each row is at least the sum of any other $c-1$ (out of the original $c$) numbers in the same row? If we have only the first condition, an averaging argument using $c=b$ gives us that $k\geq 1/4$. But with the second condition also imposed, it doesn't look like the averaging argument still works.","['matrices', 'combinatorics', 'algebra-precalculus']"
2846273,Writing Laplacian in terms of tangential and normal derivatives in local coordinates,"I'm reading a text right now (in complex analysis -- hence the 2$n$ dimensions), and am struggling with a detail in a proof. Now in this proof we're doing a local argument near the boundary of a smoothly bounded domain $\Omega$, so we work in the coordinates of the lower half-space $(t_1,\ldots ,t_{2n-1},r)$ in $\mathbb{R}^{2n}\cong\mathbb{C}^{n}$ where $r<0$. Now in this the proof it's stated that, in terms of tangential and normal derivatives in these local coordinates, the Laplace operator can be written as
$$
\Delta =\frac{\partial^2}{\partial\nu^2}+\frac{1}{2g}\frac{\partial g}{\partial\nu}\frac{\partial}{\partial\nu}+\frac{1}{\sqrt{g}}\sum_{j,k=1}^{2n-1}\frac{\partial}{\partial t_k}\left(g^{jk}\sqrt{g}\frac{\partial}{\partial t_j}\right),
$$
where $\frac{\partial}{\partial \nu}=\frac{\partial}{\partial r}$ is the normal derivative, $(g_{jk})$ gives the Riemannian metric induced on the level sets of $r$ by the standard Euclidean metric, $(g^{jk})=(g_{jk})^{-1}$, and $g=\det(g_{jk})$. However, I don't immediately see this. I imagine it's a consequence of the representation
$$
\Delta= \frac{1}{\sqrt{\tilde{g}}}\sum_{j,k=1}^{2n}\frac{\partial}{\partial t_k}\left({\tilde{g}}^{jk}\sqrt{\tilde{g}}\frac{\partial}{\partial t_j}\right)
$$
(where now $\tilde{g}$ is the induced metric on the lower half-space and $t_{2n}:=r$ for simplicity). I've tried to separate all of the terms in the sum involving the normal derivative but there are these extra terms that I can't seem to get rid of; for example, when trying to pull the $\frac{\partial^2}{\partial\nu^2}$ term out of the sum I get
$$
\frac{1}{\sqrt{\tilde{g}}}\frac{\partial}{\partial\nu}\left(\tilde{g}^{2n2n}\sqrt{\tilde{g}}\frac{\partial}{\partial\nu}\right)=\tilde{g}^{2n2n}\frac{\partial^2}{\partial\nu^2}+\frac{\partial\tilde{g}^{2n2n}}{\partial\nu}\frac{\partial}{\partial\nu}+\frac{\tilde{g}^{2n2n}}{2\tilde{g}}\frac{\partial\tilde{g}}{\partial\nu}\frac{\partial}{\partial\nu}.
$$
So I suppose I believe the assertion if I can justify to myself that $\tilde{g}^{2n2n}\equiv 1$ and $\tilde{g}^{j2n}\equiv 0$, $1\leq j\leq 2n-1$, which sort of makes sense because of the way the coordinates are chosen, but I'm not fully convinced. I'll be happy to provide more context if necessary. Any help is greatly appreciated. Thanks.","['riemannian-geometry', 'differential-geometry']"
2846319,Why are solvable groups important?,"As we know from Galois theory, an irreducible polynomial is soluble in radicals  if and only if its Galois group is solvable. However, solvable groups seem to have an importance in group theory far beyond their implications for polynomial equations. For example, much effort was expended on proving the Feit–Thompson theorem , which is one of the pieces of the classification theorem, but only its corollary, that all finite simple groups of odd order are cyclic, is required for the classification, and perhaps (I do not know) this could have been proven without using the notion of solvability. Why are solvable groups such an important subset of groups that so much research has been dedicated to their properties?","['solvable-groups', 'group-theory', 'soft-question']"
2846333,Limit with terms of convergent series problem,"Prove that if a positive series $\sum_{\nu =1}^\infty a_\nu$ is convergent and the sequence $(\nu a_\nu)_{\nu =1}^\infty$ is decreasing, then $\lim_{\nu\to\infty}(\nu\log\nu)a_\nu=0$. I've been trying to prove this for days, but so far I've only managed to prove that if the limit exists, it is equal to 0. Could someone give me a hint?","['real-analysis', 'sequences-and-series']"
2846389,Why only two dihedral angles for a snub dodecahedron?,"The Wikipedia page for the snub dodecahedron provides explicit coordinates for its vertices, from which one can of course by brute force calculate that there are only two dihedral angles. Can anyone give a (hopefully insightful/more intuitive) shorter proof that there must be only two such angles? The symmetries of the object immediately imply that there can be at most three such angles: the triangle-pentagon one, the angle between two triangles each of which is adjacent to a pentagon, and the angle between a triangle not adjacent to any pentagon and one that is. But is there any ""quick"" way to see that the latter two angles must be equal?","['polyhedra', 'angle', 'geometry']"
2846395,Largest eigenvalues of matrix and its doubled symmetric part,"Is it true that the largest eigenvalue of $A+A^{\rm T}$ is always (in some sense) bigger than the largest eigenvalue of $A$? For example, does it always hold that 
$$
\quad {\rm Re \,} \lambda_{\rm max}(A) \le {\rm Re \,} \lambda_{\rm max}(A+A^{\rm T}) 
\quad ?
$$","['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'linear-algebra']"
2846402,Derivative of Modified Bessel function of second kind,"I have to differentiate the following function with respect to $x \dfrac{dF}{dx}$, where $\alpha$ is constant:
$$\\F(x)=\sqrt{4\alpha x}K_v(\sqrt{4\alpha x})$$ I am aware that chain rule of differentiation will apply but what about the Bessel function? Any clue?","['derivatives', 'partial-derivative', 'ordinary-differential-equations', 'bessel-functions']"
2846406,Upper bound on $\sum_{n=1}^\infty q^{(n^2)}$,"Let $q\in \mathbb R$ with $|q|<1$. I am looking for an upper bound of the converging series
$$
\sum_{n=1}^\infty q^{(n^2)}
$$
with respect to $q$.
Using the inequality $q^{(n^2)} \le q^{2n-1}$, one gets
$$
\sum_{n=1}^\infty q^{(n^2)} \le q^{-1} \sum_{n=1}^\infty q^{2n}  
= q\sum_{n=0}^\infty q^{2n}
= \frac{q}{1-q^2}
$$
Numerical experiments say that this estimate does not reflect the behavior of the sum for $q\to1$. They suggests an upper bound in the order of $\frac1{\sqrt{1-q^2}}$. Is it possible to prove this? Preferably with elementary arguments?","['power-series', 'real-analysis', 'sequences-and-series']"
2846407,Difference between Population Growth Models (Differential Equations),"I'm currently studying population growth models in Math class right now and is presented with different equations for different models. I think I understand that we use $dP/dt = rP$ (where $r$ is the intrinsic growth rate and $P$ is the population) when we have infinite growth. However, I'm struggling to find the difference between the models: $dP/dt = rP(1-P/k)$ --> (where $r$ is the intrinsic growth rate, $P$ is the population and $k$ is the carrying capacity) and $dP/dt = r(k-P)$ -->(where $r$ is the intrinsic growth rate, $P$ is the population and $k$ is the carrying capacity) as they both pertain to rate of growth with carrying capacity... Any explanation/clarification is greatly appreciated!","['biology', 'ordinary-differential-equations', 'mathematical-modeling', 'calculus']"
2846465,Deriving covariance of sample mean and sample variance,"$\newcommand{\cov}{\operatorname{cov}}$I want to find the covariance between the sample mean and the sample variance, I think I am along the right track but am not sure. Suppose we have i.i.d. random variables $X_1, X_2, \ldots, X_n$. Define $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \text{ and } S^2 = \frac{1}{2n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (X_i - X_j)^2 $$ I want to find their covariance $\cov(\overline{X}, S^2)$. Here is what I have done: \begin{align}
& \cov(\overline{X}, S^2) = \cov\left(\frac{1}{n}\sum_{i=1}^n X_i, \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n (X_i - X_j)^2 \right) \\
= {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,\sum_{k=1}^{n}\sum_{j=1}^{n} (X_k - X_j)^2) \\
= {} & \frac{1}{2n^2(n-1)} \sum_{i=1}^n \cov(X_i,2\sum_{k=1}^{n} (X_i - X_k)^2)
\end{align} (because $\cov(X_i, (X_a - X_b)^2)$ is zero if $i$ isn't $a$ or $b$) $$= \frac{1}{n^2(n-1)} \sum_{i=1}^{n} \sum_{k=1}^n \cov(X_i, (X_i - X_k)^2).$$ Now, I compute $\cov(X_i, (X_i - X_k)^2)$: for $ i = k, \cov(X_i, (X_i - X_k)^2) = 0$ for $ i \neq k,$ \begin{align}
& \cov(X_i, (X_i - X_k)^2) = \cov(X_i, X_i^2 - 2X_iX_k + X_k^2) = \cov(X_i, X_i^2) - 2\cov(X_i, X_iX_k) \\[8pt]
= {} & E(X^3) - E(X)E(X^2) -2E(X)E(X^2) + 2(E(X))^3 \\[8pt]
= {} & E(X^3) -3E(X)E(X^2) + 2(E(X))^3.
\end{align} When I plug this in, I do not get the answer that I want which is $E(X-E(X))^3$, can anyone please tell me where I went wrong? UPDATE: I plugged it in again and it is actually correct! Sorry for being so careless. Thanks to anyone who has read the question. Should I delete this post?","['statistics', 'probability']"
2846489,Are there diagrams that show the structure of combinatorial problems?,"According to Principles and Techniques of Combinatorics , any given counting problem can always be decomposed into sub-problems that can be solved by the addition or multiplication principles. This becomes fairly obvious as examples increase. A problem that can be divided into disjoint cases needs to add the results of those cases, and a problem that can be composed into ordered events, needs to multiply the number of ways each event can occur. For some of the more complicated problems, visualizing this structure becomes difficult. I'm imagining a kind of structure chart or tree diagram. I've paged through a couple of my combinatorics books, and I don't see anything resembling what I'm thinking of. Is there an existing method of diagramming the structure of a counting problem?",['combinatorics']
2846497,Why is the space of bounded continuous functions complete?,"The space of bounded continuous functions from some space to a complete metric space is complete. Now, if we consider the sequence $\lbrace f_n(x)=x^n \mid f_n \colon [0,1]\to [0,1] \rbrace_n^{\infty}$, then $f_n$ is continuous and bounded for all $n$, and converges to a discontinuous function. Since $\lbrace f_n \rbrace$ converges, it is Cauchy. But $\{f_n\}\subset B_C(\mathbb{R})$ (where $B_C (X)$ is the space of bounded continuous functions from $X$ to $X$). How does that comply with the fact that $B_C(\mathbb{R})$ is complete? Clearly, I'm missing something here. I'd appreciate an explanation. Thanks.","['continuity', 'general-topology', 'metric-spaces', 'banach-spaces']"
2846505,convergence of functions and their derivatives in measure leads to relations between the limits?,"I've seen a lot of related questions, but no one ever consider the convergence in measure. Suppose you have a sequence of functions $k_n$ that are $C^1$ on $[0,1]$. Suppose that: $k_n$ converge in measure to a function $k$ $k'_n$ converge in measure to a function $h$ Call $E$ the set of points of differentiability for $k$. Is it true that $k'(x) = h(x)$ almost everywhere in $E$? Notice that in the case $k\in C^1$, we can rephrase the problem as
$$
k_m\to 0,\quad k'_m\to h \implies h=0 \quad a.e.?
$$ From what I've seen, I expect the existence of a counterexample Thanks to the properties of the convergence in measure, you can consider equivalently the convergence almost everywhere.
Moreover, if $k_m'$ are equibounded, then the claim is true thanks to Lebesgue's Dominated Convergence Theorem, so a possible counterexample must have $k_m'$ not equibounded.","['derivatives', 'lebesgue-measure', 'convergence-divergence']"
2846508,Calabi Yau: complete intersection condition,"When you express a Calabi Yau manifold as a member of the configuration matrix defined as equation (2.1) of this paper https://arxiv.org/abs/1303.1832 how do you know a priori that every choice of the coefficient of the polynomials, i.e all the members of the configuration, yields a complete intersection manifold (is the intersection necessarily transversal too?). EDITED: Is the complete intersection condition just equation (2.2)?","['manifolds', 'intersection-theory', 'algebraic-geometry']"
2846559,anti-derivative not differentiable at any point,"Reading about primitives and anti-derivatives, I noticed that primitive functions of non-continuous functions are not differentiable at some point, but the set of non-differentiability is often negligible. I tried to think of a function horrible enough to  get a non-differentiable antiderivative, and I found some with non-negligible set of non-differentiability points. But I never found an antiderivative that is nowhere differentiable. 
Can you find one?","['derivatives', 'continuity', 'integration', 'lebesgue-measure']"
2846566,Prove the equation $\left(2x^2+1\right)\left(2y^2+1\right)=4z^2+1$ has no solution in the positive integers,"Prove the equation 
$$\left(2x^2+1\right)\left(2y^2+1\right)=4z^2+1$$
has no solution in the positive integers My work: 1) I have the usually problem $$\left(nx^2+1\right)\left(my^2+1\right)=(m+n)z^2+1$$ in the positive integers. Initially I use case $\gcd(m,n)=1$ 2) Let $m=n=2$. It is this case. I need to prove that  $(x^2-y^2)^2+(x^2+y^2) $ is not perfect square for any $x,y$","['number-theory', 'integers', 'diophantine-equations', 'elementary-number-theory']"
2846600,Evaluating $\lim_{n\to\infty}\sum_{k=1}^n \frac{\pi k}{2n}\int_0^1 x^{2n}\sin\frac{\pi x}{2}dx$,"Hello I am trying to compute $$\lim_{n\to\infty}\sum_{k=1}^n \frac{\pi k}{2n}\int_0^1 x^{2n}\sin\frac{\pi x}{2}dx$$ I have rewrited it as: $$\lim_{n\to\infty}\frac1n\sum_{k=1}^n \frac{k}{n}\int_0^1\frac{n\pi}{2} x^{2n}\sin\frac{\pi x}{2}dx$$ Now the first part is just a Riemann integral $\int_0^1 x \,dx=\frac{1}{2}\,$ however I dont know if I am allowed to use it in this case. Now for the second part using $\frac{\pi x}{2}=t$ we have:$$\int_0^1\frac{n\pi}{2} x^{2n}\sin\frac{\pi x}{2}dx=\left(\frac{\pi}{2}\right)^nn\int_0^\frac{\pi}{2}t^{2n}\sin t\,dt=\left(\frac{\pi}{2}\right)^n\frac{n}{2n+1}\int_0^\frac{\pi}{2}\left(t^{2n+1}\right)'\sin t\, dt$$ So integrating by parts two times gives: $$I(2n)=\frac{1}{2n+1}\left(\left(\frac{\pi}{2}\right)^{2n+1}-\frac{1}{2n+2}I(2n+2)\right)$$ or rewritten as:$$I(2n)=2n\left(\left(\frac{\pi}{2}\right)^{2n-1}-(2n-1)I(2n-2)\right)$$ But how do I use the relation? Could you give me some help with this? EDIT:Originally was:$$\lim_{n\to\infty}\sum_{k=1}^n \sin \frac{\pi k}{2n}\int_0^1 x^{2n}\sin\frac{\pi x}{2}dx$$","['summation', 'integration', 'limits']"
2846611,Cyclic Algebra.,"Suppose that $a,b,c$ are real numbers satisfying $a^2 + b^2 + c^2 = 1$ and $a^3 + b^3 + c^3 = 1$.
Find all possible value(s) of $a+b+c$. My solution :
$(a+b+c)(a^2+b^2+c^2)=a^3+b^3+c^3+a^2b+a^2c+b^2a+c^2a+b^2c+c^2b$ $\Rightarrow   a+b+c=1+a^2b+a^2c+b^2a+c^2a+b^2c+c^2b$ $\Rightarrow a+b+c=1+c(a^2+b^2)+a(b^2+c^2)+b(c^2+a^2)$ $\Rightarrow   a+b+c=1+c(1-c^2)+a(1-a^2)+b(1-b^2)$ $\Rightarrow   a+b+c=1+c-c^3+a-a^3+b-b^3$ $\Rightarrow   a+b+c=1+c+a+b-1$ $\Rightarrow   a+b+c=c+a+b$ This led me to the same expression $a+b+c$. Is there a smarter way to solve this problem?",['algebra-precalculus']
2846618,Deriving isotimic lines/surfaces of PDE solution,"I have the following PDE: $$\frac{\partial\psi}{\partial t}=i\frac{\partial^2 \psi}{\partial x^2}$$ Consider $\psi(x,t)$ the solution for a given initial condition $\psi_0(x)$. I am interested in finding the isotimic curves for $|\psi(x,t)|^2$ given $\psi_0$. To be more precise, starting from a point $P_0=(x_0,t=0)$ where the initial condition is $\psi_0(x_0)$ I want to be able to say something about the curve that passes through $P_0$ and for which it is true that $|\psi(x(t), t)| = |\psi_0(x_0)|$. My attempt so far is the following. Given a curve $\Gamma$ that satisfies my condition, then $\nabla_{x,t}|\psi(\hat x,\hat t)|^2\bot \Gamma$ for $(\hat x, \hat t)\in \Gamma$. From this observation and using the PDE I can compute the direction of the tangent to that curve for $t=0$. I'm not going into details, it's just a manipulation of the PDE to get $\partial_t |\psi(x,t=0)|^2$. This however is not enough as I wish to characterize the curve for more than $t=0$. Some issues I encountered so far: I want to be able to characterize the curve $\Gamma$ without solving the PDE (not even with a numeric scheme), otherwise my problem would be irrelevant. The gradient gives only the direction of the normal to $\Gamma$ but it sais nothing of the numeric value for the ""curvature"" of $\Gamma$. Is there a way to link those?","['differential-geometry', 'partial-differential-equations']"
2846628,statistical errors associated to Monte Carlo sampling,"I have $n$ successive observation $A_\mu  $  of a quantity $A$ and I need to understand how  the expectation values of the square of the statistical error depends from the  autocorrelation time but a single step in the demonstration is giving me trouble. the author of the book (Monte Carlo simulations in statistical physics -Binder) wrote : \begin{align*}
\langle {(\delta A)^2} \rangle 
&= \bigg\langle \bigg[\frac{1}{n} \sum_{\mu =1}^n (A_\mu - \langle A \rangle) \bigg]^2 \bigg\rangle \\
&= \frac{1}{n^2} \sum_{\mu=1}^n \langle(A_\mu -\langle A \rangle)^2 \rangle + \frac{2}{n^2}\sum_{ \mu_1= 1}^n \sum_{\mu_2 = \mu_1 +1}^n \bigg(\langle A_{\mu_1} A_{\mu_2} \rangle- \langle A\rangle \bigg)
\end{align*} changing the summation index $\mu_2 $ to  $\mu_1 +\mu $  with $ \mu = \mu_2 - \mu_1$this equation can be rewritten as : $$ \langle {(\delta A)^2} \rangle   = \frac{1}{n} \bigg[\langle A^2\rangle  - \langle A \rangle  ^2 + 2 \sum_{\mu =1}^n \bigg( 1-\frac{\mu}{n} \bigg) \bigg(\langle A_0 A_\mu \rangle - \langle A \rangle  \bigg) \bigg]        $$ how does this change in the summation index works? please help me understand this passage","['monte-carlo', 'statistics']"
2846634,How to differentiate a tensor Frobenius norm?,"I am wondering how to calculate $$\nabla_{\mathcal{T}} \|\mathcal{T}-\mathcal{C}\|_F^2$$ where $\mathcal{T}, \mathcal{C} \in \mathbb{R}^{n\times n\times n\times n}$ are $4$ th order tensors. Any help would be appreciated. Thank you.","['derivatives', 'tensors', 'matrix-calculus', 'linear-algebra']"
2846651,Show that $\mu(E) = 0)$ iff $\mu(E \cap E(n)) =0$ for all $n\in \mathbb{N}$.,"Let $\mathbb{T} = \{z \in \mathbb{C}: |z| = 1\}$. Define $\phi:[0,1] \to \mathbb{T}$ by $\phi(t) = e^{2\pi i t}$, let $m$ be the Lebesgue measure on $[0,1]$, and define $\mu$ by $\mu(E)= m(\phi^{-1}(E))$ for each Borel set $E$ of $\mathbb{T}$. Suppose $\theta$ is irrational and $E$ is a Borel set of $\mathbb{T}$. For any $n \in \mathbb{N}$, define $E(n) = \{ze^{2n\theta\pi i}: z \in E\}$. Show that $\mu(E) = 0)$ iff $\mu(E \cap E(n)) =0$ for all $n\in \mathbb{N}$. I've already shown that $\mu$ is a measure, and the forward direction of this is trivial. My intuition is that I might be able to show that if $E \neq \emptyset$, $$\mu\left(\bigcup_{n \in \mathbb{N}}E(n)\right) = 1$$ I think this has something to do with $\theta$ being irrational, and that if I fix a $z \in E$, the set $\{ze^{2k\theta \pi i}:k \in \mathbb{N}\}$ should be dense in $\mathbb{T}$. Then from there I think that I could get $$\mu(E) = \mu\left(E \cap \bigcup_{n \in \mathbb{N}}E(n)\right) \leq \mu(E \cap E(n)) = 0$$ but I don't know how to fill in the details or even if that is a good idea.","['lebesgue-measure', 'measure-theory']"
2846660,Proof of Sard's theorem.,"In proof of Sard's theorem in Guillemin as well as in Milnor we consider $C$ such that if $x \in C$ then $\text{rank} \ df_x < p$  of function $f:U \rightarrow \mathbb R^p$, $U \subset R^n$ and $C_i$ such that all the partial derivatives of order $\leq i$ are $0$. In the proof of the theorem the following appears For each $x \in C-C_1, \exists  V \ \text{open}, x\in V $ such that $f(V \cap C)   $ has measure $0$. How to prove this?","['general-topology', 'measure-theory', 'differential-topology']"
2846715,Probability of two friends sit one near the other,"Probability of two friends sitting together Hi, I have the following problem and I would like to ensure that I solved it correctly. In a class-room there are 4 rows of chairs, 3 chairs per raw. The teacher randomly allocates 12 pupils to 12 possible chairs. John and Jack are two friends. What is the probability that they will sit one near the other (i.e., the adjacent chairs of the same row)? My solution: 
1) For each row, there are 2 solutions that satisfy  John and Jack (unordered; I do not care where each one sits). There are 4 rows. So, number of satisfactory solutions is: 2 x 4 = 8 2) Total number of possibilities John and Jack might sit (unordered sampling) is: 12! / 10! * 2 ! = 66. 3) Probability is: 8 / 66 = 0.121 Am I right? Many thanks.","['combinatorics', 'probability']"
2846726,$p~|~f_{p-1}$ if $p\equiv\pm1$ (mod$10$),"(I edited) Let $\big(f_{n}\big)$ be the Fibonacci sequence which defined by $f_{n}=f_{n-1}+f_{n-2}$ with $f_{1}=1$ and $~f_{2}=1$. Now I have deduced that for all prime $p\ge 7$, either $p~|~f_{p-1}$ or $p~|~f_{p+1}$. Here's my attempt : As prime $p\ge 7$, we see on account of the Binet formula that 
\begin{align*}
f_{n}=\frac{\alpha^{n}-\beta^{n}}{\alpha-\beta}=\frac{\left(\frac{1+\sqrt{5}}{2}\right)^{n}-\left(\frac{1-\sqrt{5}}{2}\right)^{n}}{\sqrt 5}=\frac{1}{2^{n}\sqrt{5}}\bigg(\left(1+\sqrt{5}\right)^{n}-\left(1-\sqrt{5}\right)^{n}\bigg)\tag{*}\\
\end{align*}
and by the Binomail theorem that 
$$\left(1{\pm\sqrt{5}}\right)^{n}=\sum_{k=0}^{n}{{n}\choose{k}}\left(\pm\sqrt{5}\right)^{k}=1\pm{{n}\choose{1}}{\sqrt{5}}+{{n}\choose{2}}\left(\sqrt{5}\right)^{2}\pm{{n}\choose{3}}\left(\sqrt{5}\right)^{3}+\cdots+(-1)^{n}\left(\sqrt{5}\right)^{n}$$
From $(*)$, we know that if $n$ is odd then 
\begin{align*}
2^{n-1}f_{n}&=\frac{1}{2\sqrt{5}}\left(\left(1+\sqrt{5}\right)^{n}-\left(1-\sqrt{5}\right)^{n}\right)\\
&=\frac{1}{2\sqrt{5}}\left(\sum_{k=0}^{n}{{n}\choose{k}}\left(\sqrt{5}\right)^{k}-\sum_{k=0}^{n}{{n}\choose{k}}(-1)^{k}\left(\sqrt{5}\right)^{k}\right)\\
&=\frac{1}{2\sqrt{5}}\sum_{k=0}^{n}{{n}\choose{k}}\left(\left(\sqrt{5}\right)^{k}-(-1)^{k}\left(\sqrt{5}\right)^{k}\right)\\
&=\frac{1}{2\sqrt{5}}\sum_{m=0}^{\frac{n-1}{2}}{{n}\choose{2m+1}}2\left(\sqrt{5}\right)^{2m+1}\\
&=\sum_{m=0}^{\frac{n-1}{2}}{{n}\choose{2m+1}}\left(\sqrt{5}\right)^{2m}\\
&=\sum_{m=0}^{\frac{n-1}{2}}{{n}\choose{2m+1}}5^{m}
\end{align*}
Now let $n=p$ be an odd prime and since $p~|~{{p}\choose{i}}$ for all $1\le i<p$. Then from above, one has that $$2^{p-1}f_{p}\equiv 5^{\frac{p-1}{2}}~(mod~p)$$ But $gcd(p,2^{p-1})=1$ since $p$ is odd number, then we have $$f_{p}\equiv 5^{\frac{p-1}{2}}~(mod~p)$$ and hence that $$f^{2}_{p}\equiv 5^{p-1}\equiv 1~(mod~p)$$ , where the last modulo holds by Fermat's little theorem . Note that $f_{p}^{2}-f_{p-1}f_{p+1}=1$. Here's a detail:
\begin{align*}
f_{p}^{2}-f_{p-1}f_{p+1}&=\left(\frac{\alpha^{p}-\beta^{p}}{\alpha-\beta}\right)^{2}-\frac{\alpha^{p-1}-\beta^{p-1}}{\alpha-\beta}\frac{\alpha^{p+1}-\beta^{p+1}}{\alpha-\beta}\\
&=\frac{\alpha^{2p}-2\alpha^{p}\beta^{p}+\beta^{2p}}{\left(\alpha-\beta\right)^{2}}-\frac{\alpha^{p-1+p+1}-\alpha^{p-1}\beta^{p+1}-\beta^{p-1}\alpha^{p+1}+\beta^{p-1+p+1}}{\left(\alpha-\beta\right)^{2}}\\
&=\frac{\alpha^{2p}-2\alpha^{p}\beta^{p}+\beta^{2p}}{\left(\alpha-\beta\right)^{2}}-\frac{\alpha^{2p}-\left(
\alpha^{p-1}\beta^{p+1}+\alpha^{p+1}\beta^{p-1}\right)\beta^{2p}}{\left(\alpha-\beta+\right)^{2}}\\
&=\frac{1}{\left(\alpha-\beta\right)^{2}}\bigg(-2\alpha^{p}\beta^{p}+\alpha^{p-1}\beta^{p+1}+\alpha^{p+1}\beta^{p-1}\bigg)\\
&=\frac{1}{\left(\alpha-\beta\right)^{2}}\bigg(\left(\alpha\beta\right)^{p-1}\beta^{2}-2(\alpha\beta)^{p}+(\alpha\beta)^{p-1}\alpha^{2}\bigg)\\
&=\frac{1}{\left(\alpha-\beta\right)^{2}}\bigg((-1)^{p-1}\beta^{2}-2(-1)^{p}+(-1)^{p-1}\alpha^{2}\bigg)\\
&=\frac{1}{\left(\alpha-\beta\right)^{2}}\bigg(\alpha^{2}+\beta^{2}+2\bigg)\\
&=\frac{1}{\left(\sqrt{5}\right)^{2}}(3+2)\\
&=1\\
\end{align*} 
Therefore, as $f_{p}^{2}\equiv 1$ (mod p) then  we get $$0\equiv f^{2}_{p}-1\equiv f_{p-1}f_{p+1}~(mod~p)$$ Whence, $$\color{red}{either~~p~|~f_{p-1}~~or~~p~|~f_{p+1}}$$ since $$gcd(f_{p-1},f_{p+1})=f_{gcd(p-1,p+1)}=f_{2}=1$$
Note that as $p$ is odd then $p=2q+1$ for some $q\in{\bf N}$. Then, $$gcd(p-1,p+1)=gcd(2q,2q+2)=gcd\left(2q,2(q+1)\right)=2$$
,where the last equality is true since $gcd(q,q+1)=1$. Question: Is it possible to use this result (the red one) to show that if $p\equiv \pm1$ (mod$10$) then 
$p~|~f_{p-1}~?$ Any comment or advice I will be grateful. Thanks for patient reading .","['number-theory', 'fibonacci-numbers']"
2846743,What is an $R$-valued point on a scheme over $R$?,"An $R$-valued point on an arbitrary scheme $X$ is defined to be a morphism $\mathrm{Spec}(R) \to X$. (1) What if $X$ itself is a scheme over $R$? In this case, what is the difference between the set of points in $X$ and the set of $R$-valued points? For example, if $R=\mathbb C$ it seems that they agree in some sense, or not? (2) In general, is there any reasonable structure on the set of all $R$-valued points on a scheme $X$ (i.e. the set of all morphisms from $\mathrm{Spec}(R)$ to $X$)?","['schemes', 'affine-schemes', 'algebraic-geometry']"
2846834,Show that $\lim\limits_{n \to \infty}\sin n^2$ does not exist.,"Problem Show that $\lim\limits_{n \to \infty}\sin n^2$ does not exist, where $n=1,2,\cdots.$ Proof Assume that $\lim\limits_{n \to \infty}\sin n^2$ exists, then since $\cos (2n^2)=1-2\sin^2 n^2,$ $\lim\limits_{n \to \infty}\cos(2n^2)$ also exists. It's easy to know $\lim\limits_{n \to \infty}\sin n^2 \neq 0$, even though the limit exists. Hence, $$\lim\limits_{n \to \infty}\sin (2n)^2=\lim\limits_{n \to \infty}\sin n^2 \neq 0.\tag1$$
Since $$\sin(2n)^2=\sin 2(2n^2)=2\sin(2n^2)\cos(2n^2),\tag2$$
hence $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0.$ Otherwise, if $\lim\limits_{n \to \infty}\cos (2n^2) =0,$ then according to $(2)$, we may obtain $\lim\limits_{n \to \infty}\sin(2n^2) = 0,$ which contradicts $(1)$. Now, notice that $\sin (2n^2)=\dfrac{\sin(2n)^2}{2\cos(2n^2)}$, for the reason that $\lim\limits_{n \to \infty}\sin n^2 \neq 0$ and $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0$, we may claim that $\lim\limits_{n \to \infty}\sin (2n^2) $ exists but dose not equal $0$. Since $\sin (2n^2)=2\sin n^2\cos n^2$, hence $\cos n^2=\dfrac{\sin(2n^2)}{2\sin n^2}.$ But we have known that $\lim\limits_{n \to \infty}\sin (2n^2) $ and $\lim\limits_{n \to \infty}\sin n^2 $ both exist and $\lim\limits_{n \to \infty}\sin n^2 \neq 0 $. Thus, $\lim\limits_{n \to \infty} \cos n^2$ exists. Therefore, $\lim\limits_{n \to \infty} \sin (n+1)^2$ and $\lim\limits_{n \to \infty} \cos (n+1)^2$ both exist as well. Since $\sin(2n+1)=\sin[(n+1)^2-n^2]=\sin(n+1)^2\cos n^2-\cos(n+1)^2\sin n^2$，hence $\lim\limits_{ n \to \infty}\sin(2n+1)$ exists, which is absurd. As a result, the assumption at the beginning that $\lim\limits_{n \to \infty}\sin n^2$ exists is false. We are done. NOTE Among the proof above， we apply two facts that $\lim\limits_{n \to \infty} \neq 0,$ and $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist, which has not been proven. If necessary, I'm pleased to complement the proofs of them. Lemma 1 $\lim\limits_{n \to \infty}\sin(2n+1)\neq 0.$ Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)=0.$ Then $\lim\limits_{n \to \infty}|\sin (2n+1)|=0$ and $\lim\limits_{n \to \infty}|\cos (2n+1)|=1.$ Thus, \begin{align*}
 \lim\limits_{n \to \infty}|\sin (2n+1)|&=\lim\limits_{n \to \infty}|\sin (2n+3)|\\
&\geq  \lim\limits_{n \to \infty}|\cos (2n+1)\sin 2|- \lim\limits_{n \to \infty}|\sin (2n+1)\cos 2|\\
&= \lim\limits_{n \to \infty}|\cos(2n+1)\sin 2|\\
&=\sin2 \neq 0,
\end{align*}
which contradicts. Lemma 2 $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist. Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)$ exists. Take two arcs with the middle points $\dfrac{\pi}{2},\dfrac{3\pi}{2}$ respectively and the length $\dfrac{2\pi}{3}$ from the unit circle. Notice the common difference of $\{2n+1\}$ is $2$, which is smaller than $\dfrac{2\pi}{3}$. Hence, there are infinitely many terms locating on the arc with the middle point $\dfrac{\pi}{2}$ and the length $\dfrac{2\pi}{3}$. For such terms $\{2n_k+1\}$, we have $$\sin(2n_k+1) \geq \sin \left(\frac{\pi}{2}-\frac{\pi}{3}\right)=\sin\frac{\pi}{6}=\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2n_k+1)\geq \frac{1}{2}.\tag3$$ 
Similarily, there also exists another subsequence $\{2m_k+1\}$ locating on the arc with the middle point $\dfrac{3\pi}{2}$ and the length $\dfrac{2\pi}{3}$. Notice that $$\sin(2m_k+1) \leq \sin \left(\frac{3\pi}{2}+\frac{\pi}{3}\right)=\sin\frac{11\pi}{6}=-\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2m_k+1)\leq -\frac{1}{2}.\tag4$$ As a result, $(3)$ and $(4)$ contradict each other. Lemma 3 $\lim\limits_{n \to \infty} \sin n^2 \neq 0.$ Proof Assume that $\lim\limits_{n \to \infty} \sin n^2 = 0.$ Notice that $\sin(n\pi)=0$ for all $n=1,2,\cdots.$ and the continuity of $y=\sin x$. Thus, for any given $\varepsilon>0$, there exists a $N \in \mathbb{N_+}$, when $k \geq N$, we may choose a positive integer $n_k$ such that $$|k^2-n_k\pi|<\frac{\varepsilon}{2}.$$ Without loss of generality, we may assume $\{n_k\}$ is rigorously increasing. Then $$|(2k+1)-(n_{k+1}-n_k)\pi|=|(k+1)^2-n_{k+1}\pi-(k^2-n_k\pi)|<\varepsilon.$$ Thus, $$\lim_{k \to \infty}\sin(2k+1)=\lim_{k \to \infty}\sin(n_{k+1}-n_k)\pi=0,$$
which contradicts Lemma 1. PLEASE CORRECT ME IF I'M WRONG! HOPE TO SEE MORE ELEGANT SOLUTIONS!",['limits']
2846859,Stuck in derivation of PCA,"I'm currently studying principal component analysis (PCA) from this lecture notes. I understand that we are trying to find the axis on which the variance of  projection of all the data points is maximum. Now where I'm stuck is in the formulation of PCA. In the above notes, on page number 5, our objective function is given as following. $$\frac{1}{m}\sum_{i=1}^{m}(x^{(i)^{T}}u)^2 = \frac{1}{m}\sum_{i=1}^{m}u^Tx^{(i)}x^{(i)^T}u$$ How this is derived? I can't find any explanation on this step anywhere. As per my understanding shouldn't it be: $$\frac{1}{m}\sum_{i=1}^{m}(x^{(i)^{T}}u)^2 = \frac{1}{m}\sum_{i=1}^{m}x^{(i)^T}ux^{(i)^T}u$$ Also in this question , the objective function is very different then the one mentioned above in the notes. (i.e from here and here it seems to be the distance between data point $x^{(i)}$ and the axis $w$ , but in the notes and video lectures, it's mentioned that it's distance between projection of point and the origin). What am I missing here? Any help would be appreciated","['machine-learning', 'statistics', 'linear-algebra', 'principal-component-analysis']"
2846861,Why is continuity characterized with open sets?,"Why is the topological definition of continuous in terms of open sets? I think my main complaint might be that the notion of open set seems too flexible/general and considers too many things that don't seem the right notion of ""closeness"". Conceptually, people explain ""continuous"" as: Nearby points map to nearby points. But we can easily construct sets for which *all their points are not “nearby” but they are still open . A simple example in metric spaces: the union of two open balls. The sets are still open but the points in one ball vs the other are not nearby. However the topological definition is in terms of open sets so it would consider maps balls like this from $X$ to $Y$ while that doesn't seem right to me. Is there something that I am missing? I guess I find it better to have a notion that captures the idea of “balls of radius epsilon in Y” to “balls of radius delta in X” a better notion of continuous. Another issue I find with this is that I find this in conflict even with the traditional epsilon-delta definition. The way I see it is that the topological definition should be more general (and abstract) and should encompass the metric space definition as a special case. Which to me it’s not clear it does because there is this union of disjoint open sets issue , that seem get included in the topological definition but for me they shouldn’t. This point seems important. Why were open sets chosen as the correct notion? A better definition for me would be (instead of open sets) to be in terms of “balls of radius epsilon in Y” to “balls of radius delta in X” in some topological way to define this. I have of course read the descriptions of open sets in wikiepdia but that doesn't seem to really clarify things. I know that open sets are the set of points under some topology that are ""close"". i.e. we only need sets to classify what points are considered ""close"". Which seems to me the main motivation why open sets were chosen, but the fact that disjoint open balls pass the test and are considered ""close by"" particularly disturbs me for some reason. Why is this specific complaint OK to ignore? What justifies not being worried about it? Another reason I find it weird to use open sets is because for me open sets (since I am most familiar with the definition of open sets in metric spaces), are a type of set where everything is an interior point . It's a type of set that: for all points we can always find a perturbation such that the point remains in the set (thus there is a neighbourhood that contains it in E). I find this problematic since it doesn't seem the right notion of ""nearby"" (at least to me); the reasons I prefer the definition to be restricted to only single open balls or sets that have no weird gaps (continuous sets? for some definition of that). This interior point issue doesn't seem to be what continuity (or limits actually) encompass conceptually. Continuity/limits seem to be a property about getting closer and closer (at least conceptually) or approaching. Therefore, for me it would be better to define it in terms of sets that reflected this idea of closeness. Something like neighbourhoods or (open) balls like in the traditional way of defining balls $B_{\delta}(p) = { x \in X | d(x,p) < \delta}$. Since this seems to be a clear notion of ""nearby"". Why are these ideas not preferred? What is wrong with it?","['real-analysis', 'continuity', 'general-topology', 'metric-spaces', 'definition']"
2846925,"Find all functions $f:\mathbb{N}\rightarrow\mathbb{N}$ such that $\varphi(f(x+y))=\varphi(f(x))+\varphi(f(y))\quad\forall x,y\in\mathbb{N}$","I was studying about Cauchy's Functional Equation, and suddenly I had the following question in my mind:
Find all functions $f:\mathbb{N}\rightarrow\mathbb{N}$ such that $$\varphi(f(x+y))=\varphi(f(x))+\varphi(f(y))\quad\forall x,y\in\mathbb{N}$$
Here, $\varphi(n)$ is Euler's Totient Function. I first tried to find some trivial cases, and intuitively I felt that if $f(x)=c$, where $c$ is a constant, then $c$ must be a perfect power of a prime. And, in this case, the only trivial solution I found was $f(x)=2^k$ where $k$ is a positive integer. 
I really have no idea whether other solutions do exist or not.","['number-theory', 'functional-equations']"
2846932,Does $\mathbb{R P}^n \times \mathbb{R P}^n$ admit a metric with positive sectional curvature?,"My riemannian geometry professor gave us the following exercise. Let $M = \mathbb{R P}^n \times \mathbb{R P}^n$, where $\mathbb{R P}^n$ is the $n$-dimensional projective space. Show that $M$ does not admit a metric with positive sectional curvature. I was able to prove the result in the case $n$ is odd: In this case, $\mathbb{R P}^n$ is orientable and therefore $M$ is orientable. If $M$ admits a metric with positive sectional curvature then by the Synge theorem $M$ must be simply connected, which is not the case because $\pi^1(M) = \mathbb Z / 2\mathbb Z\times\mathbb Z / 2\mathbb Z$. How do I prove the result if $n$ is even?","['riemannian-geometry', 'differential-geometry']"
2846942,Solving $x \frac{\mathrm dy}{\mathrm dx}=y+1$,"Following differential equation is given:
$$x \frac{\mathrm dy}{\mathrm dx}=y+1.$$ Separating variables and integrating:
$$\int \frac{1}{y+1} \mathrm dy=\int \frac 1x \mathrm dx$$
$$\ln|y+1|=\ln|x|+c$$ In my textbook the following step is:
$$y+1=Ax \,\,\,\,\, \text{(where} \ A=e^c).$$ My question is why the modulus function can be omitted after exponentiating.","['indefinite-integrals', 'integration', 'ordinary-differential-equations', 'calculus']"
2846973,Discrete Schrödinger equation,"I am currently working on a form of the discrete Schrödinger equation, the Harper equation: $\psi_{m+1}-\psi_{m-1}+2\cos(2\pi \alpha m +\theta) \psi_{m}=E\psi_{m}$, where $ m\in \mathbb{Z},\quad \alpha,\theta\in \mathbb{R}.$ $E$ is the (Energy)eigenvalue. So, this is a second order difference equation. Now, I need to prove, that this equation can be written in a $q$-difference equation in the following form: $ i(z^{-1}+qz)\Psi(qz)-i(zq^{-1}+z^{-1})\Psi(q^{-1}z)=E\Psi(z)$, where $\Psi(z)=\sum_{n=0}^{Q-1}p_{n}z^{n}$ is a polynomial. There is a paper http://arxiv.org/abs/cond-mat/9808066v2 , where they do explain the procedure a bit. But I cannot really follow. I know, that this difference equation can be written as an eigenvalue problem. But I do not understand the further explanations/ derivation. Does anybody have ideas or knows how ich can solve the problem? Thanks a lot in advance.","['eigenvalues-eigenvectors', 'periodic-functions', 'polynomials', 'discrete-mathematics']"
2846980,Relative spectrum of a quasicoherent sheaf of algebras (affine case),"I'm trying to understand the concept of the relative spectrum of a sheaf of quasicoherent algebras. Here is the situation: We are given a scheme $X$ and a quasicoherent sheaf of $\mathscr{O}_X$-algebras (i.e. a sheaf of algebras that is quasicoherent as an $\mathscr{O}_X$-module) $\mathscr{B}$. Consider the contravariant functor $F = F_{\mathscr{B}, X}$ from schemes to sets that acts on objects by $$W \mapsto \{(\mu : W \to X, \varphi : \mathscr{B} \to \mu_* \mathscr{O}_W )\}$$
where $\mu$ is a morphism of schemes, $\varphi$ is a morphism of $\mathscr{O}_X$-algebras, and the right hand side is the set of all such pairs. The functor $F$ acts on morphisms $a : W' \to W$ by
\begin{align}
F(a) : F(W) &\to F(W')\\
(\mu, \varphi) &\mapsto (\mu' = \mu \circ a, \varphi' = a^* \varphi),
\end{align}
where $\varphi' = a^*\varphi : \mathscr{B} \to \mu_* \mathscr{O}_W \to {(\mu')}_* \mathscr{O}_{W'}$ in the composition of $\varphi$ with $\mu_* a^\#$, where $a^\# : \mathscr{O}_W \to a_* \mathscr{O}_{W'}$ (note $\mu_* a_* = (\mu \circ a)_* = (\mu')_*$) The idea is that we get an affine morphism of schemes $\beta : \underline{\mathrm{Spec}}_X\mathscr{B} \to X$ and morphism of $\mathscr{O}_X$-algebras $\phi : \mathscr{B} \to \beta_* \mathscr{O}_{\underline{\mathrm{Spec}}_X\mathscr{B}}$ such that the pair $(\underline{\mathrm{Spec}}_X\mathscr{B}, (\beta, \phi))$ represents the functor $F$. That is, $F(-) = \mathrm{Hom}_{\mathrm{Sch}_{/X}}(-, \underline{\mathrm{Spec}}_X\mathscr{B})$ and $(\beta, \phi) \in F(\underline{\mathrm{Spec}}_X\mathscr{B})$ corresponds to the identity morphism under the bijection $F(\underline{\mathrm{Spec}}_X\mathscr{B}) \cong \mathrm{Hom}(\underline{\mathrm{Spec}}_X\mathscr{B}, \underline{\mathrm{Spec}}_X\mathscr{B})$. The scheme $\underline{\mathrm{Spec}}_X\mathscr{B}$ is called the relative spectrum of $\mathscr{B}$ over $X$ . How do we get this scheme and universal maps? As usual, we start with the affine case. Suppose $X = \mathrm{Spec}(A)$. Then since $\mathscr{B}$ is quasicoherent, it must be o.t.f. $\mathscr{B} = \tilde{B}$ for some $A$-algebra $B$. This gives a structure morphism $\beta : \mathrm{Spec}(B) \to \mathrm{Spec}(A)$. The multiplication map $B \otimes_A B \to B$ taking $b \otimes b' \mapsto b b'$ induces a morphism of qcoh $\mathscr{O}_{\mathrm{Spec}B}$-algebras $\beta^* \mathscr{B} = \widetilde{B \otimes_A B} \to \tilde{B} = \mathscr{O}_{\mathrm{Spec}B}$, and hence by adjunction a map $\mathscr{B} \to \beta_* \mathscr{O}_{\mathrm{Spec}B}$. We claim that this data represents the functor $F_{\mathscr{B}, \mathrm{Spec}(A)}$. This is proved in https://stacks.math.columbia.edu/tag/01LT , but there are some things I don't understand. The idea is fairly clear, We must show that for all schemes $W$, the map
$$
\mathrm{Mor}_{\mathrm{Sch}}(W, \mathrm{Spec} B) \to \{(\mu, \varphi)\} \hspace{10pt} a \mapsto (a^* \beta = \beta \circ a, a^* \varphi)
$$
is bijective. Let's call this map $f$. In order to do this, we can define an inverse map $f^{-1}$ and show that for all morphisms $a : W \to \mathrm{Spec}B$, $f^{-1}(f(a)) = a$ and for all pairs $(\mu, \varphi)$, $f(f^{-1}((\mu, \varphi))) = (\mu, \varphi)$. What I don't understand is how to define the inverse map. Specifically, in the lemma I cited above, what is the map $f^*$ ($\mu^*$ in my notation) supposed to be? As a follow up question, suppose we have a pair $(\mu : W \to \mathrm{Spec} A, \varphi: \mu^* \mathscr{B} \to \mathscr{O}_W)$. Then by adjunction we get $\varphi : \mathscr{B} \to \mu_* \mathscr{O}_W$. Evaluating this map of $\mathscr{O}_{\mathrm{Spec}A}$-algebras on global sections, we have an $A$-algebra homomorphism $B \to \Gamma(W, \mathscr{O}_W)$. This corresponds to a morphism of schemes $W \to \mathrm{Spec}(B)$. If we define the inverse to be this map, does the proof go through? I'm having some trouble showing the maps are mutually inverse.","['sheaf-theory', 'algebraic-geometry']"
2846997,Can we decide if a dragon comes home?,"First, a quick definition: A (deterministic) Lindenmayer system (L-system) over an alphabet $\mathcal{A}$ is essentially specified by a function $f:\mathcal{A}\mapsto\mathcal{A}^*$ (where $\mathcal{A}^*$ is the set of strings over $\mathcal{A}$; that is, finite sequences of elements of $\mathcal{A}$). This extends to a function $f^*: \mathcal{A}^*\mapsto\mathcal{A}^*$ by concatenation: $f^*(a_1a_2\cdots a_n)=f(a_1)f(a_2)\cdots f(a_n)$. The L-system 'operates' by iterating $f^*$ starting from an initial word $w_0\in\mathcal{A}^*$, producing the sequence of words $w_1=f^*(w_0)$, $w_2=f^*(w_1)$, etc.  For instance, the Fibonacci word can be obtained by taking $\mathcal{A}=\{a,b\}$, defining $f$ via $f(a)=ab$, $f(b)=a$, and taking the initial word $w_0$ to be $a$; then the sequence is $w_1=ab$, $w_2=aba$, $w_3=abaab$, $w_4=abaababa$, etc; it can be shown tha $w_n$ is of length $F_{n+1}$ with $F_n$ $a$s in it and $F_{n-1}$ $b$s. The Wikipedia link there shows how to get several 'classic' iterated curves such as the dragon curve, Koch snowflake, etc. by generating words in an L-system and then mapping them to 'turtle-like' motions in the plane by interpreting each letter in the word as a command (i.e., a transformation in the plane): move forwards, turn $90^\circ$ left or right, etc. This interpretation prompts my question: suppose we're given a deterministic L-system over some (finite) alphabet $\mathcal{A}$, an initial word $w\in \mathcal{A}^*$, and a function $M: \mathcal{A}\mapsto G$ from the alphabet to elements of some automatic group $G$ (for instance, the 'square grid' group generated by translations of the plane by one unit in the $x$ direction and rotations by $90^\circ$).  Then can it be decided whether some iterate of $w$ maps via $M^*$ (here of course $M^*$ is the extension of $M$ to a function from $\mathcal{A}^*$ to $G$ via $M^*(a_1a_2\ldots a_n)=M(a_1)\circ M(a_2)\cdots\circ M(a_n)$ where $\circ$ is the group operator in $G$) to the identity in $G$?  If so, what's known about the algorithmic complexity of the problem? It's clear that the problem is semi-decidable: since $G$ is automatic it has solvable word problem (in fact, a given word in the generators can be tested for identity in time at worst quadratic in its size), and so we can simply to apply the L-system over and over to generate the sequence of words $w_0=w, w_1=L(w_0), w_2=L(w_1), \ldots$, testing each of the elements $M^*(w_0), M^*(w_1), \ldots$ of $G$ to see if it's the identity.  Solvability seems to be essentially equivalent to asking if there is some algorithmic bound $b=b(|w|)$ (presumably also depending on the size of the L-system and the group presentation) such that if none of the iterates through $w_b$ is the identity, then none after will be; for instance, there may be some sort of integer 'charge' definable on words such that an iteration of the L-system increases the charge and the charge of the identity is zero. Solving this problem would also solve a simple generalization where instead of checking for the identity, we check for membership in some finite subgroup $H$ of $G$ (for instance, in the 'square grid' example above, we could check whether the element is in the $C_4$ subgroup of only rotations): simply add a new symbol to the L-system, mapping to itself under iterates of the system; prepend it to the initial word, and then for each element $h\in H$, generate a function $M_h$ that extends $M$ by taking the new symbol to $h$; then for each $h$ just run the decision algorithm with the slightly larger L-system and the mapping $M_h$.  It may also be possible to test for membership in a finite-index (normal?) subgroup of $G$ through similar means, but I haven't quite worked out all the details there; that may actually be a 'proper' generalization of the problem.","['context-free-grammar', 'group-theory', 'automata']"
2847017,Gradient of the Rayleigh Quotient,"In the introduction part of the paper The Fast Convergence of Incremental PCA , the authors mention that the gradient of the Rayleigh quotient is equal to: $$
\triangledown G(v) = \frac{2}{\|v\|^2}(A - \frac{v^{T}Av}{v^{T}v} I_d)v
$$ when the Rayleigh quotient is: $$G(v) = \frac{v^{T}Av}{v^{T}v}$$ ($v \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$) What are the steps to derive the given value for $\triangledown G(v)$ ? EDIT: As suggested by @Alex R. in a comment, I tried to proceed using the identity for a derivative of a quotient. I don't know/remember the matrix calculus identities to proceed. Here's what I tried: Let $N = v^{T}Av$ and $D = v^{T}v$, Then, $G'(v) = \frac{N'D-ND'}{D^{2}} \tag{1}\label{eq1}$ $D^2$ can be written as $(v^{T}v)^2 = (\|v\|^{2})^2$. Using $\frac{dx^{T}Ax}{dx} = x^{T}(A + A^{T})$, $N'$ can be written as $v^{T}(A+A^{T})$. $D' = 2v^{T}$. Plugging these values to $\eqref{eq1}$ yields: $$
G'(v) = \frac{v^{T}(A+A^{T})v^{T}v - v^{T}Av(2v^{T})}{\|v\|^4}
$$ I can't figure out how to simplfy this. Any pointers to resources I should look are also appreciated.","['derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus']"
2847024,"""Invariants"" of Exotic spheres","An exotic sphere is a differentiable manifold M that is homeomorphic but not diffeomorphic to the standard Euclidean n-sphere. Naively, I thought that there is no algebraic topological invariant that distinguishing the exotic spheres from each others (?) . For example, are there any integration over the local quantities on the differentiable manifolds that can distinguish exotic spheres? (e.g. I dont think there are any characteristic classes, homotopy or co/homology groups, or co/bordism can distinguish or exotic spheres of the same dimension. Yes?) However, it looks that we can construct exotic spheres as the non-trivial elements of an abelian monoid under connected sum, which is a finite abelian group if the dimension is not 4 . The fact 1 seems to have some tension, if not contradicts, to the fact 2. Because the fact 2 of the abelian monoid / group structure seems to hint that there are some algebraic topological quantities as ""topological invariants""  that can distinguish exotic spheres. Yes or no? Moreover, there are so called Kervaire, Kervaire-Milnor invariants, Kervaire invariant problem and the Kirby–Siebenmann invariant. Are these quantities as invariants of exotic spheres topologically? In the sense that, we can obtain the topological data (global) from integration over the local quantity? (Analogous to characteristic classes?) In short, are ""invariants"" of Exotic spheres more of the quantities of (a) differential or (b) topological?","['differential-topology', 'manifolds', 'invariance', 'algebraic-topology', 'differential-geometry']"
2847035,Proof that Reuleaux triangles have constant width,"All pages I read on Reuleaux triangles simply use a visual demonstration to illustrate this, but fail to make a rigorous argument. How might a formal proof of this fact proceed?",['geometry']
2847045,Rank-1 matrix derivative,"I am trying to prove that $$\displaystyle{\frac{\partial}{\partial x} \left( x\cdot x^\top  \right) = x\otimes \mathbb{I}+\mathbb{I}\otimes x}$$ The product $x\cdot x^\top$ is a rank-$1$ matrix. Thus, we actually have a derivative of a matrix by a vector. How do we handle such case? Can you please help prove the equality?","['derivatives', 'matrix-calculus', 'jacobian']"
2847051,Characteristic classes of spheres,"Since an exotic sphere is a differentiable manifold $M$ that is homeomorphic but not diffeomorphic to the standard Euclidean $n$-sphere, we may not be able to distinguish spheres from exotic sphere through characteristic classes. However, it is still worthwhile to know the basic characteristic class data of spheres: Stiefel–Whitney class $w_i$: Spheres are orientable and non-spin, thus
$$
w_1(S^d)=0,
$$
$$
w_2(S^d)=0,
$$
More generally, what do we have for other $i$:
$$
w_i(S^d)=?
$$ Chern class $c_i$: Even-dimensional spheres have an even-real dimensional tangent bundle $TS^d$, thus we may define the Chern class 
$$c_i(TS^d)=c_i(S^d)=?$$
One may also consider the frame bundle of spheres 
$$c_i(FS^d)=?$$ Euler class:
$$
\chi(S^d)=2, \text{ if $d$ even}; 
$$ $$
\chi(S^d)=0, \text{ if $d$ odd.}  
$$ Wu class $u_i$: is related to the Stiefel–Whitney class $w_i$ through Stenrod square, so
$$
u_1(S^d)=u_2(S^d)=u_3(S^d)=0
$$
More generally, what do we have for other $i$:
$$
u_i(S^d)=?
$$ Pontryagin class $p_i$:
$$
p_i(S^d)=?
$$
We can consider all the $d=0 \pmod 4$ dimensions of spheres.
We know that $p_1(TS^4)=0$ and $p_1(FS^4)=?$ (The frame bundle of spheres). Are there other powerful/ useful Characteristic classes of spheres?","['homology-cohomology', 'differential-topology', 'characteristic-classes', 'algebraic-topology', 'differential-geometry']"
2847069,An annoying optimization problem,"At first I thought the following problem looked simple, but I've had serious problems pinning it down: Suppose that $\prod_{i=1}^k x_i$ is fixed. Then find the minimum value of
$$\sum_{i=1}^k (1 - x_i)^k$$
in the range $0 \leq x_1, \dots, x_k \leq 1$. A first conjecture is that it should be minimised when $x_1 = \dots = x_k$. This turns out to be false when the product is small, but I still suspect it's true when the product is reasonably large, say at least $1/2^k$. Does anyone see a simple or reasonable way to analyze this? You can get somewhere with Lagrange multipliers, but it doesn't seem to be enough. I also thought there might be an elementary argument I'm missing. Perhaps I should flesh this out a little more. Using Lagrange multipliers, one can show that $x_i(1-x_i)^{k-1}$ is the same for each $i$. Since the function $x(1-x)^{k-1}$ has a unique maximum (at $x = 1/k$), this shows that the $x_i$ take at most $2$ values. But I've had trouble reducing the problem any further, partly because, as I said, there are cases where the minimum is not in the expected place.","['multivariable-calculus', 'inequality', 'optimization', 'lagrange-multiplier']"
2847078,Trying to evaluate $\int_{0}^{\infty}\frac{\ln(1+x^3)}{1+x^3}\frac{dx}{1+x^3}$,"I would like to work this out: $$I=\large\int_{0}^{\infty}\frac{\ln(1+x^3)}{1+x^3}\frac{\mathrm dx}{1+x^3}$$ Making a sub: $u=x^3$, $dx=\frac{du}{3x^2}$ $$I=\frac{1}{3}\int_{0}^{\infty}\frac{\ln(1+u)}{u^{2/3}(1+u)^2}\mathrm du$$ Making a sub: $u=\tan^2(y)$, $du=\sec^2(y)dy$ $$I=\frac{2}{3}\int_{0}^{\pi/2}\frac{\ln\sec(y)}{\sec^2(y)\sqrt[3]{\tan^4(y)}}\mathrm dy$$ $$I=\frac{2}{3}\int_{0}^{\pi/2}\frac{\cot(y)\cos^2(y)\ln\sec(y)}{\sqrt[3]{\tan(y)}}\mathrm dy$$ I can't continue. Maybe there is another alternative way to simplify $I$","['integration', 'definite-integrals', 'trigonometric-integrals', 'calculus']"
2847090,About the convergence of a series,"Let $\Omega_r =\{(x,y,z) \in \mathbb{R}^3 : 0<x,y,z , x^2+y^2+z^2<r^2 , x^2+y^2-z^2<0 \}$ And  $ f_r(x,y,z)= \begin{cases}
       2z/(x^2+y^2+z^2)  & \text{if } (x,y,z) \in \Omega_r,  \\
       0 & \text{otherwise}.
     \end{cases}$ Show that the serie $\sum 2^{-n}f_n $ converges a.e. in $ \mathbb{R}^3 $ to an integrable function in $\mathbb{R}^3$ I have tried applying quotient criterion and it gives me $1/2 < 1$ so I can think that it is convergent. I have tried to find another function which is integrable and bounds my function but so for example if $ x, y$ or $z \ge1 $ then $f_n\le 1/2^{n-1} $, which is convergent and integrable and if $ z < 1 $ then $f_n< 1 /(2^{n-1}(x^2+y^2+z^2 ) $ but I don't know if that is enough. I need help.
Thank you in advance.","['multivariable-calculus', 'real-analysis']"
2847108,Sum of subspace and its orthogonal complement,"Let $V$ be a (not necessarily finite-dimensional) vector space (over a field $K$), $U$ a subspace of $V$ and $\beta\colon V \times V \to K$ a bilinear form. Define $U^\perp = \{v\in V : \beta(v,u)=0 \ \  \forall u \in U \}$ to be the (left) orthogonal complement of $U$. Which conditions do we need in order to get the identity $V = U + U^\perp$ or $V = U \oplus U^\perp$? I think one needs $U$ to be finite-dimensional and $\beta|_{U\times U}$ to be non-degenerate. Is it then possible for any given vector $v\in V$ to construct a (probably unique) vector $u\in U$ (dependent on $v$) such that $v-u\in U^\perp$, maybe via the map $v \mapsto \sum_{i=1}^m \beta(v,u_i)u_i $ for any given basis $\{u_1,\dotsc,u_m\}$ of $U$? A more general thought: The condition of $\beta|_{U\times U}$ being non-degenerate means that if $u\in U$ and $\beta(u,u')=0$ for all $u'\in U$ then $u=0$. So this is equivalent to $U\cap U^\perp = \{0\}$. If we ignore this condition, that means $U\cap U^\perp \ne \{0\}$, what do we still need for $V = U + U^\perp$?","['bilinear-form', 'orthogonality', 'linear-algebra', 'vector-spaces']"
2847163,Characteristic classes over the real projective spaces ${\mathbb{P}}_d({\mathbb{R}})$ [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question Inspired by other users --- this and others , also here : We may explore the characteristic classes of real projective spaces ${\mathbb{P}}_d({\mathbb{R}})$, in some sense, it is related to some sort of ""sphere"" when we consider the most simple case ${\mathbb{P}}_1({\mathbb{R}}) \simeq S^1$. We however like to know general $d$. It is still worthwhile to know the basic characteristic class data of them: Stiefel–Whitney class $w_i$: ${\mathbb{P}}_d({\mathbb{R}})$ may be non-orientable and non-spin.
Note 
$$
w_1({\mathbb{P}}_d({\mathbb{R}})) =d+1 \mod 2\neq 0, \quad \text{ if $d=0 \mod 2$}
$$
$$
w_2({\mathbb{P}}_d({\mathbb{R}}))=\frac{(d+1)d}{2} \mod 2,  \text{ if $d \geq  2$}
$$
Thus
$w_2({\mathbb{P}}_1({\mathbb{R}}))=0$, and for $d \geq 2$, we have $w_2({\mathbb{P}}_d({\mathbb{R}}))=0$ for $d=3$ or $0$ mod $4$. More generally, we have for other $w_i$:
$$
w({\mathbb{P}}_d({\mathbb{R}}))={(1+a)}^{d+1},
$$
where $H^*({\mathbb{P}}_d({\mathbb{R}}),\mathbb{Z}_2)=\frac{\mathbb{Z}_2[a]}{a^{d+1}}$ Euler class:
$$
e({\mathbb{P}}_d({\mathbb{R}}))=?
$$ Wu class $u_i$: is related to the Stiefel–Whitney class $w_i$ through Stenrod square, so
$$
u_i({\mathbb{P}}_d({\mathbb{R}}))=?
$$ Pontryagin class $p_i$:
$$
p_i({\mathbb{P}}_d({\mathbb{R}}))=? \in H^{4i}({\mathbb{P}}_d({\mathbb{R}}), \mathbb{Z})
$$
We can consider all the $d=0 \pmod 4$ dimensions.
e.g. $p_1(T{\mathbb{P}}_4({\mathbb{R}}))=0$ (yes?) and the frame bundle $p_1(F{\mathbb{P}}_d({\mathbb{R}}))=?$. Are there other helpful / useful Characteristic classes for real projective spaces ${\mathbb{P}}_d({\mathbb{R}})$ that are nontrivial (or trivial)? A list of such info is greatly appreciated. Edit. Chern class $c_i$ removed.","['differential-topology', 'algebraic-geometry', 'algebraic-topology', 'geometric-topology', 'differential-geometry']"
2847169,Prove the inequality $breadth(P) \leq dim(P)$,"Definition 1 : Let n be a positive integer. We say that an order P has breadth at most n if for all elements $x_0$, $x_1$, ..., $x_n$, $y_0$, $y_1$, ..., $y_n$ in P , if $x_i \leq y_j$ for all $i \neq j$ in {0, 1, ..., n }, then there exists $i \in$ {0, 1, ..., n } such that $x_i \leq y_i$. The breadth of P , in notation, breadth( P ), is the least positive integer n such that P has breadth at most n if such an n exists. Definition 2 : Define the order-dimension, dim( P ), of an order P as the smallest cardinal m such that P is a suborder on a product of m chains. Actually my problem is I cannot understand definition 2. In my interpretation, product is defined as follows: Definition 0 : Given the orders P and Q , we can form the direct product $P \times Q$, consisting of all ordered pairs ($x_1, x_2$) with $x_1 \in P$ and $x_2 \in Q$, ordered componentwise, that is, $(x_1, x_2) \leq (y_1, y_2)$ if $x_1 \leq y_1$ in P and $x_2 \leq y_2$ in Q . According to this definition of product, dimension of order is really similar to that of a vector. For example, (a, b) is 2 dimension and (a, b, c) is 3 dimension. Is my interpretation of definition 2 correct? Please give me some hints on how to connect definition 1 and definition 2.","['lattice-orders', 'order-theory', 'discrete-mathematics']"
2847185,Every norm in $\mathbb{R}$ has the form $||x||=a|x|$,"How can I prove the following? Every norm in $\mathbb{R}$ has the form $||x||=a|x|$ where $a>0$ and $|x|$ is the absolute value of $x$. Conclude that every norm in $\mathbb{R}$ comes from a inner product. First I think in prove by contradiction, but I don't know how to express that $||x||$ doesn't has the form $a|x|$, so maybe that's not the way. I'm just starting to study metric spaces, and I really don't know how to solve this. If there is some hint to guide me I'll appreciate it.","['normed-spaces', 'general-topology', 'metric-spaces']"
2847186,$ \sum _{k=1}^{2n} T(k) = \sum _{k=1}^{2n} k$,"For the Collatz function   $$T(n)=\frac {3n+1}{2} \text {if} \ n \ \text{is odd}$$
 and   $$T(n)=\frac {n}{2} \text {if} \ n \ \text{is even }$$
 I have found that $$ \sum _{k=1}^{2n} T(k) =  \sum _{k=1}^{2n} k$$ Proof:$$ \sum _{k=1}^{2n} T(k) = \{T(1)+T(3)+T(5)+...+T(2n-1)\} + \{T(2) +T(4) +...+T(2n)\} $$ $$= \frac {3(1)+1}{2} +\frac {3(3)+1}{2}+...+\frac {3(2n-1)+1}{2} +1+2+3+...+n $$ $$= \frac {3n^2 +n}{2} + \frac {n(n+1)}{2} = n(2n+1)=\sum _{k=1}^{2n} k.$$ Question:
Are there other non-trivial sets $S$ which satisfy  $$ \sum _{k\in S}T(k) =  \sum _{k\in S} k$$",['number-theory']
2847204,Limit of a state-transition matrix,"Let $\omega$ be a positive real number and let $A,B\in\mathbb{R}^{n\times n}$ be $n\times n$ real matrices. Consider the following linear time-varying dynamical system
$$
\dot{x}(t) = (A + \cos(\omega t)B)x(t),\quad x(0)=x_0\in\mathbb{R}^{n}.
$$ Let $\Phi(t,0)$ denote the state-transition matrix of the above system. My question. Is it true that
  $$
\lim_{\omega \to \infty} \Phi(t,0) = e^{At} \ \ \ ?
$$ Some remarks. If matrices $A$ and $B$ commute, this is true. Indeed, in this case it holds
$$
\Phi(t,0) = e^{\int_0^t A + \cos(\omega \tau)B\, \mathrm{d}\tau}.
$$
In the non-commuting case, I didn't manage to prove this. The main issue here is that a closed-form expression of $\Phi(t,0)$ does not exist, apparently. The only (perhaps useful) idea that I had so far is to exploit the Peano-Baker expansion of $\Phi(t,0)$. However, even with this tool, I couldn't provide an answer to my question. Thanks for your help!","['ordinary-differential-equations', 'dynamical-systems', 'analysis', 'limits']"
2847210,"How I finish this question about system of congruence $X \equiv a \pmod n, X \equiv b \pmod m$?","My question: For a,b,n,m $\in N$ , with n, m $>1$ . Show that: \begin{align}
X \equiv a \pmod n\\
X \equiv b \pmod m
\end{align} if and only if, $a \equiv b \mod (n,m)$ and if $(m,n) = 1$ , then is only solution modulo $mn$ . My solution: \begin{align}
x = nk + a\\
x= mk_1+b\\
0 = nk + a - mk_1 - b\\
b-a = nk - mk_1
\end{align} for diophantine equation exist solution if $(n,m)| b-a \to b\equiv a \mod (n,m)$ . My problem is how I can prove the back, I wrote only this: \begin{align}
a\equiv b \mod (m,n)\\
\to (m,n) | a -b \\
\to a -b = (m,n)k 
\end{align}","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
2847221,Is group cohomology killed by exponent of group?,"Let $G$ be a finite group, the exponent $e(G)$ is defined to be the lcm of order of elements in $G$. Let $M$ be a $G$ module, we know by restriction corestriction that $H^i(G,M)$ is annihilated by $|G|$, for positive $i$. Is there examples that $H^2(G,M)$ is not annihilated by $e(G)$?","['group-cohomology', 'group-theory']"
2847229,Poisson process strictly bounded by another?,"Consider two random processes $X, Y$. $Y$ is a Poisson process with rate $\lambda$. $X$ behaves as follows: whenever $X < Y$, it acts equivalent to a Poisson process with rate $\lambda$ (independent of $Y$). Whenever $X = Y$, it waits until $Y$ increments, so that $Y - X = 1$, before continuing to increment itself. (In other words, $X$ never allows itself to surpass $Y$). My question is: what is known about the distribution of $X$? Specifically, can anyone offer any bounds regarding the expected time until $X > n$, for arbitrary $n$? My simulations indicate that it's roughly Y's arrival time plus some very slowly growing ""lag"" when $n$ is very big. This lag is sort of intuitive: consider the case where you have a whole bunch of processes each bounding the one to its left; in that case you would intuitively have quite a big discrepancy between the leftmost process and the rightmost one.","['probability-distributions', 'poisson-process', 'statistics', 'probability', 'random-variables']"
2847276,Box-constrained orthogonal matrix,"Given constants $\ell, u \in \mathbb{R}^{3 \times 3}$ and the following system of constraints in $P \in \mathbb{R}^{3 \times 3}$
$$
P^T P = I_{3 \times 3},\quad \ell_{ij} \leq P_{ij} \leq u_{ij},
$$
I would like to find a matrix $P$ which satisfies this system, or determine that it is infeasible. Is there a computationally efficient way to perform this task? The solution doesn't have to be closed form, but it should be an algorithm implementable on a computer which runs quickly, exploiting the fact that it is an $9$ dimensional problem. A numerical algorithm which converges to a solution is also a very good option, but there should be a proof that indeed it converges to a solution of the problem.","['semialgebraic-geometry', 'real-algebraic-geometry', 'orthogonal-matrices', 'linear-algebra', 'non-convex-optimization']"
2847278,Find the number of consecutive zeros at the end of the following numbers $100!+200!$,"To find the tailing zeros of $100!+200!$, I first found $100!$ To be $24$ zeros and $200!$ To be $49$. When we add, the result should be $73$ but the answer is given as $24$ which I cannot understand. Help me out, thanks.","['number-theory', 'factorial', 'numerical-methods', 'elementary-number-theory']"
2847319,moduli of effective divisors of a projective manifold projective,Are the moduli of effective divisors of a complex projective 2-manifold projective. If so a reference?,"['differential-geometry', 'algebraic-geometry']"
2847327,partial derivatives and chain rule of functions defined on manifold,"For functions on $\mathbb{R}^{n}$, applying chain rule and taking partial derivatives is straight forward. I am a bit confused about how this concept can be extended to functions defined on manifolds. To better explain my question, first I'll give a, rather well known, example of a real valued function on $\mathbb{R}^2$, and then an example of a function on a mainfold, which I need to understand precisely. Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$, its derivative can be written as,
\begin{equation*}
\label{eq:1}
\begin{aligned}
  \frac{d}{dt}f(x_{1},x_{2}) &= \frac{d}{dx_{1}}f(x_{1},x_{2}) \frac{d}{dt} x_{1} + \frac{d}{dx_{2}}f(x_{1},x_{2}) \frac{d}{dt} x_{2}\\
  &= \left( d_{x_{1}}f \right) \dot{x}_{1} + \left( d_{x_{2}}f \right) \dot{x}_{2}
\end{aligned}
\end{equation*}
One way to interpret $d_{x_{1}}f$ is that it represents how $f$ changes in the direction of the coordinate axis $x_{1}$, also known as Lie derivative. Now here is the actual question I want to ask. For a function on a manifold, in my case, I have a Lie algebra valued function $f: SO(3)\times SO(3) \rightarrow \mathfrak{so(3)}$, is this precisely correct to write the derivative in the following way? Let $R,R_d\in SO(3)$,
\begin{equation*}
\label{eq:2}
\begin{aligned}
  \frac{d}{dt}f(R,R_d) &= \frac{d}{d_R}f(R,R_d) \dot R + \frac{d}{dR_{d}}f(R,R_d) \dot R_d\\
  &= \left( d_{R}f \right) \dot{R} + \left( d_{R_{d}}f \right) \dot{R}_{d}\\
&= \left( T_{R}f \right) \dot{R} + \left( T_{R_{d}}f \right) \dot{R}_{d},
\end{aligned}
\end{equation*} 
where $T_{R}f$ is the tangent space of $f$ at point $R$. This does not seem correct, or not mathematically precise because of the following two reasons, $d_R f$, strictly speaking, is not the change of $f$ in the direction of the ""coordinate"" $R$, as this function is geometric, and defined in a coordinate free way. This means the second line of the above equation does not make sense? $T_{R}f$ is the tangent space of $f$ at point $R$, and $T_{R_d}f$ is the tangent space of $f$ at point $R_d$, and we can't simply add vectors belonging to different tangent spaces. This means the third line of the equation does not make sense either? How can I write this derivative in a mathematically (geometrically) correct way? For some reason I don't want to merge $\dot R$, $\dot R_d$ with the term preceding it. In other words, I want to keep the term appearing because of chain rule separate with the rest of the terms. The reason I have introduced the notion of tangent spaces is because I think this is the right way to define derivatives of mainfolds. For example let $\mathcal{M}$, and $\mathcal{N}$, be manifolds, and let $f:\mathcal{M}\rightarrow \mathcal{N}$, the derivative, also called the push forward map, is defined as $D_f: T\mathcal{M}\rightarrow T\mathcal{N}$, where $T\mathcal{M}$, and $T\mathcal{N}$ are the tanget bundle of the manifolds $\mathcal{M}$, and $\mathcal{N}$, respectively. Even under this abstract definition of derivative, how can we defined derivative of a function on a manifold, while keeping the terms appearing because of chain rule separate?","['derivatives', 'manifolds', 'differential-geometry', 'pushforward', 'lie-groups']"
2847372,A modern Introduction to Complex Analysis,"Is the book Complex variables by  Carlos A Berenstein and Roger Gay a good book for a  second, more rigorous, course in complex analysis?
My first course, while I loved the applications to analytic number theory, felt dry and bland in its analysis as they just gave definitions.
This book seems to be one of its kind as it talks about algebraic topology, homological algebra, and sheaf theory while developing complex analysis from the definition of a holomorphic funtion (It gives a really good reason from linear algebra why we have the definition of a holomorphic function the way we do).
I can't find any reviews or really anything about this book except for one reply on the page .
Is this a good book to read or is there a similar one that covers a modern approach like it? (By the way my first course text was Complex analysis by Elias M. Stein which I read up to chapter 7).","['complex-analysis', 'reference-request']"
2847386,How can I calculate $\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!}$,How should I get the result $$\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!}=\sqrt2$$ without relying on Stirling's formula or the Central Limit Theorem? I am totally clueless. Can somebody help me?,"['real-analysis', 'limits', 'calculus', 'probability', 'analysis']"
2847397,Why is the Error surface for a 2 input neural network with 2 weights a parabolic bowl,"I am new to machine learning and AI in general and had a quick question regarding the error function surface regarding a simple neural net: 2 input neural net After reading the following wiki: https://en.wikipedia.org/wiki/Backpropagation I understand that the error function for a specific input and output of this neural net could be described as: $$E = (t-y)^2$$
where $t$ is the expected value and $y$ is the value of the output node in the neural net. However, $y$ is a function of $w_1$, and $w_2$: $$y=x_1w_1 + x_2w_2$$ This I believe must mean that the final error function should be: $$E = (t - x_1w_1 - x_2w_2)^2$$ If I graph this function for specific values of $t, x_1$ and $x_2$, I am getting a surface that is a parabolic cylinder instead of a parabolic bowl. Is there anything that I might be looking at wrong? Thanks so much.","['multivariable-calculus', 'machine-learning', 'artificial-intelligence', 'neural-networks']"
2847421,Density w.r.t. counting measure and probability mass function (discrete rv),"By way of disclosure, let me start by saying that I am not a mathematician. However, I came to peruse Kyle Siegrist's web-based text in probability theory ( http://www.randomservices.org ); and I a quite intrigued. Unfortunately, I am currently struggling to reconcile a few of the statements that I came across. Since my struggles might be due to a somewhat more fundamental lack of understanding, I trust you can forgive, if I start my question by summarizing what I believe to have understood so far. Pleasure feel free to skip to the demarcated passage and refer to the introduction only if you feel that I misunderstood some of the more fundamental aspects of probability theory. I start with a probability space $( \Omega,\mathcal{F},\mathbb{P})$ where $\Omega$ is a finite, countably infinite , or uncountably infinite sample space and $\mathcal{F}$ is a $\sigma$-algebra furnishing a domain for probability measure $$\mathbb{P}\,:\,\mathcal{F}\,\rightarrow\,[0,1].$$
Now, I consider a measurable space $(\mathbb{R},\mathcal{B})$ where $\mathbb{R}$ is the real line and $\mathcal{B}$ is the $\sigma$-algebra generated by the standard Euclidean topology, i.e. the extension of the collection of all open sets in $\mathbb{R}$ to a $\sigma$-algebra. This measurable space allows for the definition of a measurable function $$X\,:\,\Omega\,\rightarrow\,\mathbb{R}$$
such that the preimage $X^{-1}(A)\in\mathcal{F}$ for all $A\in\mathcal{B}$. I suppose that could also be written as $\sigma(X)\subseteq\mathcal{F}$. $X$ is then called a random variable. Utilizing a change of variables theorem, I proceed to define a measure $\mu$ on $(\mathbb{R},\mathcal{B})$ that corresponds to $\mathbb{P}$ such that $$\mu(A)=\mathbb{P}[X^{-1}(A)]\quad\forall\quad A\in\mathcal{B}.$$
Since $X^{-1}(A)\in\mathcal{F}$, $\mathbb{P}(X^{-1}(A))$ exists in $[0,1]$. Hence,
$$\mu\,:\,\mathcal{B}\,\rightarrow\,[0,1]$$ and is called the distribution of $X$. From there it is easy enough to show that $\mu$ is, in fact, a probability measure making $(\mathbb{R},\mathcal{B},\mu)$ a probability space. Now, I can make a statement about a random variable that corresponds to an event:
$$X(\omega)\in A\quad(A\in\mathcal{B})\quad\text{corresponds to}\quad X^{-1}(A)\in\mathcal{F}.$$Moreover, I can assign corresponding probabilities since $\mu(A)=\mathbb{P}[X^{-1}(A)].$
Finally, I recognize that intervals (on the real line) of the type $$(-\infty,a]$$ are Borel sets such that $(-\infty,a]\in\mathcal{B}$ for all $a\in\mathbb{R}$. Therefore, $\mu(-\infty,a]$ is defined. Equipped with $\mu$, I can then go on to define a distribution function
$$F\,:\,\mathbb{R}\,\rightarrow\,[0,1]$$
such that 
$$F(x)=\mu(-\infty,x].$$
It is reasonably straightforward to show that $F$ is monotonically increasing, right-continuous, and normalized to take values between $0$ and $1$. Moreover,
$$\mu(a,b]=F(b)-F(a).$$
and
$$\mu\{a\}=F(a)-F(a^{-})\quad\text{where $F(a^{-})$ is the limit from the left}.$$
Hence $F$ is continuous (from the left and from the right) iff $\mu\{a\}=0$ or all $a\in\mathbb{R}$. On the other hand, if $\mu$ has an atom at point $x\in\mathbb{R}$ such that $\mu\{x\}>0$, then $F$ is a step function. This insight naturally leads to my question. All of the above did not deal with the question of whether $X$ is a continuous or a discrete random variable. So I like to believe that everything I wrote is valid in both cases. But: I am not sure how to reconcile that fact that $\mu$ has a domain comprised of uncountable Borel sets with the derivation of the probability mass function (as the Radon-Nikodym derivative of $\mu$ w.r.t the counting measure) for a discrete random variable. In the continuous case I understood that I can argue as follows: $\Omega$ is necessarily uncountable and moreover the image of $\Omega$ under $X$ is also uncountable. However, it might be an uncountable subset of the reals such as $\mathbb{R}_+$. When I defined the distribution $\mu$ of $X$, though, I did so using as domain all Borel sets in $\mathbb{R}$ - even though X couldn't possibly take values in the negative reals. So what I did above can only be valid, if the Borel sets that are disjoint from the support of $X$ are simply assigned a measure of $0$. (Correct?) I can also characterize the feature of being a continuous random variable in terms of absolute continuity w.r.t to Lebesgue measure $\lambda$. That is:
$$\lambda(\emptyset)=0\qquad\text{and}\qquad \mu(\emptyset)=0$$
$$\lambda\{x\}=0\qquad\text{and}\qquad \mu\{x\}=0$$
where $\mu\{x\}=0$ is only true for continuous random variables. Question 1: I see how that relates to the distribution function being continuous from the right as well as from the left, but is there a straightforward way to link that to the image of $\Omega$ und $X$ being uncountable? So, if $\mu\{x\}=0$, the distribution $\mu$ is absolutely continuous w.r.t $\lambda$. By the Radon-Nikodym Theorem there exists a density (RN-Derivative) such that $$\mu(A)=\int_A\,f\,\text{d}\lambda\quad\forall\quad A\in\mathcal{B}$$ Since, in my example above, every non-singleton subset of the negative reals has zero measure w.r.t. $\mu$ but positive Lebesgue measure, it does follow that the RN-derivative $f(x)$ is simply set to zero, whenever $x\not\in X(\Omega).$ (Correct?) My struggle is mostly with the discrete case . I would argue that $\Omega$ can be countable or uncountable, but for a random variable $X$ to be discrete $X(\Omega)$ must be countable - so the random variable has countable support. Moreover, I understood that $X$ is discrete if the distribution $\mu$ is a discrete measure such that $\mu(\mathbb{R}\setminus X(\Omega))=0$ where $X(\Omega)$ is clearly countable. (Correct?) It is also true that 
$\#(\emptyset)=0\qquad\text{and}\quad\mu(\emptyset)=0.$
Therefore, the distribution ($\mu$) of $X$ is absolutely continuous w.r.t counting measure ($\#$). And, thus, by the Radon-Nikodym theorem, there exists a density such that.
$$\mu(A)=\int_A\,f\,\text{d}\#\quad\forall\quad A\in\mathcal{B}.$$ If $A$ was a subset of the support of $X$ ($A\subseteq X(\Omega)$), i.e. $A$ would only consist of values that $X$ actually might take, then $A$ would be countable, $f$ would be a simple function and 
$$\mu(A)=\int_A\,f\,\text{d}\#$$
$$\mu(A)=\sum_{x\in A}f(x)\#(\{x\}).$$
Since the count of a singleton is $\#(\{x\})=1$, it would follow that
$$\mu(A)=\sum_{x\in A}f(x)$$
and as $\mu(A)=\mathbb{P}(X\in A)$
$$\mathbb{P}(X\in A)=\sum_{x\in A}f(x)$$
such that the RN-derivative of $\mu$ w.r.t $\#$ has the interpretation of the usual probability mass function for discrete random variables. Question 2: But $\mu$ is defined over the Borel sets. So $\mathbb{P}(X\in A)\,[\,=\mu(A)\,]$ might take the form $\mathbb{P}(a< X\leq b)\,[\,=\mu(a,b]\,]$, which is a valid statement even for a discrete random variable.
So, even if I presume that $f(x)=0$ for all $x\not\in X(\Omega)$, how do I come from 
$$\mu(A)=\int_A\,f\,\text{d}\#\quad\forall\quad A\in\mathcal{B}.$$
to
$$\mu(A)=\sum_{x\in A}f(x)$$
since $A$ is not countable and I cannot use countable additivity? Thank you so very much. Best wishes, Jon","['probability-theory', 'measure-theory']"
2847461,ellipsoid of greatest volume is a sphere,"The equation of an ellipsoid is $$f(x,y,x)=(\frac xa)^2+(\frac yb)^2+(\frac zc)^2=1$$.
Given that the volume of an ellipsoid is $$V=\frac43\pi abc$$ and the constraint $$L=a+b+c$$ L some positive constant. Show that the ellipsoid with greatest volume is a sphere. I should use Lagrange multipliers for this question.
I tried doing $$\nabla f = \lambda\nabla V$$ which gave an anwser making no sense$$<\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}> =\lambda <0,0,0>$$ So then I tried $$""\nabla""V=\lambda""\nabla""L$$
where $""\nabla""$ treats a as x, b as y, c as z, and got
$$4\pi/3<bc,ac,ab>=\lambda<1,1,1>$$
so $ab=ac=bc$ gives $a=b=c$ a sphere. But why am  I allowed to use $""\nabla""$ as such","['multivariable-calculus', 'lagrange-multiplier', 'calculus']"
2847485,"After deleting the multiples of $2$ and multiples of $3$ from list of integers from $1$ to $N$, why are a fifth of the numbers still multiples of 5?","I was reading an explanation about there being infinitely many primes that started off like this: Say to the contrary there are finitely many and $p$ is the largest prime. Then let $N$ be the product of all the primes, so $N=2\times3\times5\times7\times\ldots\times p$. Of the numbers in the list $1,2,3,4,5,\ldots,N-2,N-1,N$, half of them are divisible by $2$. We cross those numbers off the list, and we have $1,3,5,7,9$ and so on. Then of those numbers in this list , a third of them are multiples of $3$. At first I thought the spacing of the numbers would make it so that not every 3 consecutive numbers in the list would have exactly 1 multiple of 3, but I reasoned that every 3 consecutive odd numbers $2n+1,2n+3,2n+5$ must have a multiple of 3 because $2n+1$ is either $\equiv0,1$ or $2\pmod3$. Okay, then from this list of only odd numbers, we delete all the multiples of $3$, which I now believe is a third of the numbers. Then the book claims that of this new list (with all multiples of $2$ and all multiples $3$ crossed out), exactly a fifth of them are multiples of $5$. Now I am stuck as to why exactly a fifth of these numbers are multiples of 5. I understand that a fifth of the numbers from the original list $1,2,3,\ldots, N$ are multiples of $5$, but it seemed to me that the uneven spacing of this list, with the multiples of $2$ and $3$ deleted, might make it so that we aren't guaranteed a multiple of $5$ every five consecutive numbers anymore. How do we know a fifth of the numbers in the new list are multiples of $5$? (The explanation goes on to do this with all the primes until $p$.)","['combinatorics', 'prime-numbers', 'elementary-number-theory']"
2847488,"Krull dimension of $\mathbb{R}[x_1, \ldots, x_n]/(F)$? [duplicate]","This question already has answers here : Is $\operatorname{height} \mathfrak{p} + \dim A / \mathfrak{p} = \dim A$ true? (4 answers) Closed 5 years ago . Let $F \in \mathbb{R}[x_1, \ldots, x_n]$ be irreducible over $\mathbb{R}$. 
I am trying to understand why the Krull dimension of the ring $\mathbb{R}[x_1, \ldots, x_n]/(F)$ is $n-1$. Any comments are appreciated! Thank you! PS In fact is it always the case that if $P$ is a prime ideal of height $k$ then 
the Krull dimension of the ring $\mathbb{R}[x_1, \ldots, x_n]/P$ is $n-k$?","['algebraic-geometry', 'abstract-algebra', 'krull-dimension', 'maximal-and-prime-ideals', 'commutative-algebra']"
2847538,Dual spaces and gradients and subgradients,"Suppose we have some function $f:{\mathbb R}^n \rightarrow \mathbb{R}$. Its gradient is defined as the vector which gives the directional derivative via $(v,\nabla f )=D_{v}f$ for any direction $v$. Could, or should, we think of $\nabla f$ as something belonging to the dual space of the domain of $f$? And if yes, what is the idea of going about this in this way? In particular are there some geometric ideas involved? I ran into this idea while learning about subgradients and generalised subgradients, which are defined as functionals on the space of the domain of $f$.","['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
2847554,Does the inclusion $A \subset B$ for closed densely defined operators $A$ and $B$ imply $A=B$?,"Let $H,G$ be a Hilbert spaces and $A: \mathcal D (A) \subset G \to H \ $ be a densely defined operator with domain $\mathcal D (A) \ $. Assume there is an operator $B: \mathcal D (B) \subset G \to H$ which is an extension of $A$. i.e. $\mathcal D (A) \subset \mathcal D (B) \ $. Assume both operators $A,B$ are closed. Can we conclude that $A=B$? So far I tried the following: Let $x \in \mathcal D (B)$. Since $\mathcal D (B)$ is dense there is a sequence $(x_n)$ in both domains such that $x_n \to x$ and since $B$ is closed we have $\lim_n Bx_n=Bx$. Since every $x_n$ is also in the domain of $A$ we conclude that $(Ax_n)$ converges. Hence $x$ is in the domain of the closure of $A$ but this the same as the domain of $A$ since $A$ is closed.","['functional-analysis', 'operator-theory', 'analysis']"
2847594,How is $\frac{1}{2n} \leq \sin (\frac{1}{n}) \leq \frac{1}{n} $,"How is $\frac{1}{2n} \leq \sin (\frac{1}{n}) \leq \frac{1}{n} $ I know that $\sin \theta \leq \theta, \theta$ very small But if take $ f(x) = \sin (\frac{1}{x}) -  \frac{1}{2x}, f'(x) = \frac{-1}{x^2} \cos (\frac{1}{x}) +  \frac{1}{2x^2} $ But if i take $x=\frac 4\pi, x \in (0, \pi/2), $ i am getting $f'(x) < 0$ which should be other way around. Am I missing something? I got this doubt while reading Does $\sum_{n=1}^\infty(-1)^n \sin \left( \frac{1}{n} \right) $ absolutely converge? If i say since $\sin (x)$ converges to $x$, i will have $\sin x$ values slightly less than $x$ and slightly more than $x$. But it depends on whether $f(x)$ is increasing/decreasing (local maxima or local minima) Pls clarify","['real-analysis', 'trigonometry', 'sequences-and-series']"
2847624,"Find rational numbers $a,b,c$ satisfying $(2^{1/3}-1)^{1/3} = a^{1/3}+b^{1/3}+c^{1/3}$.","$\textbf{Problem} $ Find rational numbers $a,b,c$ satisfying
  \begin{align*}
(2^\frac13-1)^\frac13 = a^\frac13+b^\frac13+c^\frac13
\end{align*} My Attempt: I try to $2^\frac13-1 = (a^\frac13+b^\frac13+c^\frac13)^3$ and compare with rational numbers and irrational numbers in LHS and RHS. Any help is appreciated... Thank you! Update: I found the answer. I want to find rational numbers $a,b,c$ without assumption $a=1/9,b=-2/9,c=4/9$. Thus,  I found the identity:
$$\sqrt[3]{m^3-n^3+6m^2n+3mn^2-3(m^2+mn+n^2)\sqrt[3]{mn(m+n)}}=\\ \sqrt[3]{m^2(m+n)}-\sqrt[3]{mn^2}-\sqrt[3]{(m+n)^2n}$$ 1) How to get the identity? 2) I want to know about uniqueness $(a,b,c)$ satisfying $(2^\frac13-1)^\frac13=a^\frac13+b^\frac13+c^\frac13$","['algebra-precalculus', 'rational-numbers', 'irrational-numbers', 'calculus']"
2847684,Is $\frac{1}{\frac{1}{n}\sum_{i=1}^{n}\frac{1}{X_{i}}}$ a consistent estimator for $\mu$,"Let $X_i \sim N(\mu,\sigma^2)$, I want to find out if $$\frac{1}{\frac{1}{n}\sum_{i=1}^{n}\frac{1}{X_{i}}}$$ is a consistent estimator for $\mu$, or not. It's easy to show, using $LLN$, that $\overline{X}$ is a consistent estimator for $\mu$, and I figure I'm supposed do some manipulation to turn this into that, but so far I haven't been able to do so. How do I proceed?","['law-of-large-numbers', 'probability-theory', 'statistics']"
2847693,What is this curve called?,"This curve can be easily generated with a spreadsheet. You can set cell A1 to be 100 and then B1 will be this formula =A1×(100+RAND()×10−5)÷100 And fill down about 150 or more cells. Then copy row B and paste formula result into column A. Repeat till you get bored. If you sort by size and graph, you will get a curve that looks like this. My question is, does this curve have a name and is there an equation that generates it? Edit to explain where this question comes from. If you choose a basket of stocks, like the S&P 500, and buy $1000 worth of each stock, then the values of the stocks will change and if you graph them they will form this curve.","['statistics', 'probability', 'economics']"
2847737,When $\|f_n\|_{L^2}=1$ where $f_n(x)= f(x-n)+(-1)^n f(x+n)$?,"Let $f: \mathbb R \to \mathbb C $ be a function and define $f_n(x)= f(x-n)+(-1)^n f(x+n).$ Can we expect to choose $f\in L^2(\mathbb R)$ such that 
  $\|f_n\|^2_{L^2}=1$ for all $n\in\mathbb Z$? Side thought :  We know that $L^2(\mathbb R)$ is complex Hilbert space with inner product $\langle f, g \rangle = \int f \bar{g}$ 
and using properties of inner product we have 
$$\|f_n\|_{L^2}^2= 2\|f\|^2_{L^2}+ 2 \text{Re} \langle f(x-n), (-1)^nf(x+n)\rangle $$","['functional-analysis', 'examples-counterexamples', 'hilbert-spaces', 'analysis']"
2847750,Subsets whose size is a power of $2$,A set with $n$ elements contains $2^n$ subsets. What if we restrict to subsets whose size is a power of two? Does this quantity behave differently asymptotically? I.e. what is the asymptotic behaviour of: $$f(n) = \sum_{k=0}^{\lfloor\log{n}\rfloor}\binom{n}{2^k}$$,"['combinatorics', 'asymptotics']"
2847762,Does there exist singular cubic surface containing $27$ distinct lines?,It is well-known that a smooth cubic surface $X\subset \mathbb{CP^3}$ contains $27$ distinct lines. But is the converse also true? I know this is true for general case (then the $27$ lines will become $6$ double lines and $15$ single lines). But I want to know is it true for all cases.,"['complex-geometry', 'algebraic-geometry']"
2847802,Stopping time clarification,"May I please seek someone's help to clear my understanding about stopping time. According to the Wikipedia definition: random variable $\tau:\Omega \rightarrow I$ is called stopping time if
$\{\omega \in \Omega : \tau(\omega) \leq t\} \in F_t$ where $F_t$ is the filtration up to time t. I am trying to understand this definition: 1> first I choose a time ""t"" 2> Given my choice of ""t"" I will have a sigma algebra $F_t$, which essentially is a collection of sets with closure properties. After this I am confused 3> Should I take each element $\omega$ from $\Omega$ and check if my stopping time event has happened or not. And if it has happened, then is it in the $F_t$. And the above will give what ? a collection of elements ? how will it give a real number which I can call stopping time. I understand I have confused myself big time but not clear where I am making the mistake in my interpretation. If someone can comment to make my understanding clear I would be very thankful.","['stopping-times', 'stochastic-calculus', 'measure-theory', 'filtrations']"
2847927,"What is the correct term for ""growth rate of growth rate""?","Let's say a company had these monthly sales: Jan: $10$ Feb: $15$ Mar: $22.5$ The growth rate of sales would be: Jan: - Feb: $50$% Mar: $50$% What is the correct term for the ""growth rate of the growth rate of sales""? e.g.: Jan: - Feb: - Mar: $0$% There are no results found when googling ""growth rate of growth rate"", which leads me to believe I don't know the common term for how to refer to things like this.",['statistics']
2847931,Squaring matrices and positive semidefiniteness,"Suppose that $A$ and $B$ are positive definite, symmetric, $n\times n$ real matrices such that $A-B$ is positive semidefinite. Write $A\succeq B$. When is it true that $A^2\succeq B^2$? I know given the conditions, $A^2\succeq B^2$ is not true in general. On the other hand, let $u_1\geq\ldots\geq u_n\geq 0$ and $v_1\geq\ \ldots \geq v_n\geq 0$ be the eigenvalues of $A$ and $B$. By the Min-Max Principle , we have
$$
u_i\geq v_i,\quad i=1,\ldots,n.
$$
Then, $u_1^2\geq\ldots\geq u_n^2$ and $v_1^2\geq\ldots\geq v_n^2$ are the eigenvalues of $A^2$ and $B^2$ and satisfy
$$
u_i^2\geq v_i^2,\quad i=1,\ldots,n.\tag{$*$}
$$
I used to think that ($*$) would be enough to give $A^2\succeq B^2$ but as the counter-example linked above shows, it clearly doesn't. So I'm wondering how we can strengthen ($*$) to give $A^2\succeq B^2$.","['inequality', 'examples-counterexamples', 'matrices', 'soft-question', 'linear-algebra']"
