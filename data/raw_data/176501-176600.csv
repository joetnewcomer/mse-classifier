question_id,title,body,tags
3171184,Is a linearly independent set whose span is dense a Schauder basis?,"If $X$ is a Banach space, then a Schauder basis of $X$ is a subset $B$ of $X$ such that every element of $X$ can be written uniquely as an infinite linear combination of elements of $B$ .  My question is, if $A$ is a linearly independent subset of $X$ such that the closure of the span of $A$ equals $X$ , then is $A$ necessarily a Schauder basis of $X$ ? If not, does anyone know of any counterexamples?","['banach-spaces', 'schauder-basis', 'normed-spaces', 'linear-algebra', 'functional-analysis']"
3171206,"Homomorphism $\phi: G \rightarrow H$, $\phi$ surjective, $\exists a \in H : |a| = 5$, show $\exists x \in G : |x| = 5$","Homomorphism $\phi: G \rightarrow H$ , $\phi$ surjective, $\exists a \in H : |a| = 5$ , show $\exists x \in G : |x| = 5$ , where $|G|$ is finite. I'm not sure if this proof is correct, but here's what I have so far: By the First Isomorphism Theorem: $G/\ker\phi$ isomorphic to $\phi(G)$ . We know $\phi(e_G) = e_H$ , so $e_G \in \ker\phi$ . So that means $\exists ge_G \in G/\ker\phi$ with a bijection mapping to the element of order $5$ in $H$ , so that means $|ge_G| = 5 = |g|$ for some $g \in G$ . Notice, I did not use the fact that $\phi$ is surjective, meaning there's most likely something wrong with the proof.","['group-homomorphism', 'group-theory', 'group-isomorphism']"
3171224,Problem with left limits.,"Let $F: \mathbb{R} \to \mathbb{R}$ a non-decreasing function and suppose $G: \mathbb{R} \to \mathbb{R}$ defined by $G(x) = F(x+)$ (= the right limit of $F$ in $x$ , which always exists for non-decreasing functions) is differentiable almost everywhere. Is it true that $F$ is differentiable almost everywhere? Here is the relevant fragment from the book that I'm reading: Note that $F$ and $G$ are continuous at the same points and they agree
  at each point at which they are continuous; furthermore, if $F(x_0) =
 G(x_0)$ , then $\frac{F(x)−F(x_0)}{x−x_0}$ lies between $\frac{G(x)−G(x_0)}{x−x0}$ and $\frac{G(x−)−G(x_0)}{x−x_0}$ . Hence if $G$ is
  differentiable at $x_0$ , then $F$ is differentiable at $x_0$ , and $F'(x_0) = G'(x_0)$ . The almost everywhere differentiability of F
  follows. I suspect the author uses the squeeze theorem here, but I see no reason why we should have that $$\frac{G(x-)-G(x_0)}{x-x_0} \to G'(x_0)$$","['measure-theory', 'derivatives', 'almost-everywhere', 'real-analysis']"
3171236,Showing that $ \int_0^\pi \frac{\sin{(kx)}}{\sin(x)} d x = \pi $ for odd integer $k$,"I am trying to show that $$ \int_0^\pi \frac{\sin{(kx)}}{\sin(x)}  d x = \pi $$ for odd integer $k$ . It seems like this could be done using multiple angle formulae, but I'm stuck. I can get to $$ \int_0^\pi \frac{\sin{((2N+1)x)}}{\sin(x)}  d x  $$ $$ = \sum_{\ell = 0} ^N  (-1)^\ell \frac{(2N+1)!}{(2\ell+1)!(2(N-\ell))!} \int_0 ^\pi (\cos(x))^{2(N-\ell)} (\sin(x))^{2\ell} d x, $$ using a multiple angle formula due to Viete (from the trig formulae Wikipedia page), but I don't know what to do from here. Is there a simpler way?","['integration', 'trigonometry', 'definite-integrals']"
3171244,"Show that $\text{Res}_{z = \infty}\left(f(z)\log\left(\frac{z-a}{z-b}\right)\right) = - \int_{a}^{b}f(z)\,dz$","I would like to show that $\text{Res}_{z = \infty}\left(f(z)\log\frac{z-a}{z-b} \right)= - \int_{a}^{b}f(z)\,dz$ , where $f(z)$ is an entire function and for $\log\left(\frac{z-a}{z-b}\right)$ we take any branch that is regular at $z = \infty$ . I have tried  using both definitions of the residue at infinity but have had no luck. First I tried the integral definition: $$\text{Res}_{z = \infty}\left(f(z)\log\frac{z-a}{z-b}\right) = \frac{-1}{2\pi i}\int_{C_r}f(z)\log\left(\frac{z-a}{z-b}\right) $$ But now I am unsure how to compute this integral. Breaking the logarithm up as a difference of two logs did not seem to help. I also tried to parametrize the circle of radius r and write this as a real integral. This got really messy. Next, I tried the definition which uses the residue at $0$ $$\text{Res}_{z = \infty}\left(f(z)\log\left(\frac{z-a}{z-b}\right) \right)$$ $$ = - \text{Res}_{z = 0}\left(\frac{1}{z^2}f\left(\frac{1}{z}\right)\log\left(\frac{z^{-1}-a}{z^{-1}-b}\right)\right) $$ $$ = - \lim_{z \to 0} \left(\frac{1}{z}f\left(\frac{1}{z}\right)\log\left(\frac{1-az}{1-bz} \right)\right)$$ The above limit computation corresponds to there being a simple pole at $0$ which I'm not even sure is true. And yet again, I am stuck. Any tips?","['complex-analysis', 'residue-calculus']"
3171257,Why does the space of real $N \times N$ rank-$k$ matrices form a manifold?,"In this paper , Uwe Helmke and Mark Shayman claim that it follows because the rank- $k$ matrices with signature $I_{p} - I_{q}$ are an orbit of a group action of $\mbox{GL}(N)$ , but I am not sure what fact is being referenced here.","['matrices', 'linear-algebra', 'manifolds', 'lie-groups', 'differential-geometry']"
3171258,Arity of Primitive Recursive Functions,"I'm currently working through a few books on computability and I am having a little bit of trouble with the primitive recursive scheme. When defining the primitive recursive scheme for unary functions most authors give something like the following: $\text{For function $h$ which is primitive recursive, the function $f$ defined by}$ $$f(0)=m $$ $$f(n+1)=h(n,f(n))$$ is primitive recursive where $m$ is a zero-ary constant function and where $h$ is a binary function. My question is can we view $h$ as a unary function since it seems to depend on only $n$ ? I feel like I could define $g(n)=h(n,f(n))$ where g is unary. This would then violate the requirement that the arity of $h$ be $k+1$ if the arity of $f$ is $k$ . I'm just a little confused about how the arity requirement works in the unary case. Edit: In addition to the above question, I'm curious about performing primitive recursion on zero-ary functions. For example, one of the authors I'm reading defines the zero-ary constant function $o'()=0$ . He then proceeds to define the family of zero-ary constant functions $c_i'()=i$ as follows: $$c_0'(0)=o'()$$ and $$c_{n+1}'()=S((c_{n}'())$$ where S is the successor function. It seems like this violates the primitive recursive scheme conditions as well with $c_0'(0)$ having arity 1 and $c_{n+1}'()$ having the same arity as $S((c_{n}'())$ . I'm guessing something is going on in the background, but I can't seem to see it.","['functions', 'logic', 'computability']"
3171337,"Is the problem Find all $x,y \in \mathbf{N}$ such that $\binom{x}{2} = \binom{y}{5}$ solved?","I was recently browsing and came upon this document which gives some open problems involving Diophantine Equations.
Document: http://www.math.leidenuniv.nl/~evertse/07-workshop-problems.pdf Upon searching a bit, I found that the first problem given has been solved. Is the second problem also solved? 
The problem is Find all $x,y \in \mathbf{N}$ such that $\binom{x}{2} = \binom{y}{5}$ .","['number-theory', 'combinatorial-number-theory', 'diophantine-equations']"
3171342,Injective mapping theorem proof discrepancy.,"I do not understand from where the 2 comes in the proof of letter(a) of the statement of the theorem: And this is proposition 12.2.4 I think the author has used the mean value inequality, but I do not know how, could anyone explain this for me please?","['proof-explanation', 'analysis', 'real-analysis', 'multivariable-calculus', 'calculus']"
3171384,computing probability ssrw,"Let $(S_n)_{n\geq 0}$ be a simple symmetric or asymmetric random walk with $S_0 =0$ and $S_{n}^* = \max_{m\leq n} S_m$ . Assume that $n$ and $a$ are even positive integers, $b$ is an even integer, $b \leq a \leq n$ , $2a-b \leq n$ . How can I compute $P(S_{n}^* < a, S_n = b)$ ? I'm having trouble understanding what the computation means and how I would go about it.","['probability-theory', 'probability']"
3171404,Why does $\sin(x) - \sin(y)=2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2})$?,Why does this equality hold? $\sin x - \sin y = 2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2})$ . My professor was saying that since (i) $\sin(A+B)=\sin A \cos B+ \sin B \cos A$ and (ii) $\sin(A-B) = \sin A \cos B - \sin B \cos A$ we just let $A=\frac{x+y}{2}$ and $B=\frac{x-y}{2}$ . But I tried to write this out and could not figure it out. Any help would be appreciated,"['trigonometry', 'analysis', 'real-analysis']"
3171407,"$x^2 + y^2+xy = 1$ , then find the minimum of $x^3 y + xy^3 +4$",x and y belongs to real numbers. $ x^2 + y^2+xy = 1 $ .  then find the minimum value of $x^3 y + xy^3 +4$ . I assume $ x = r \sin (w)$ and $ y = r\cos(w) $ . $  x^3 y + xy^3 +4 = L $ which give me $ \frac{2}{3} \le r^2 \le 2 $ I am stuck after that.Its my  Humble request to help me after that.,"['optimization', 'algebra-precalculus', 'maxima-minima', 'trigonometry']"
3171444,"Let $X$ be connected and locally connected. Let $f:X\to [0,1]$ be bijective continuous function, then $X$ is homeomorphic to $[0,1]$","Let $X$ be connected and locally connected. Let $f:X\to [0,1]$ be continuous and bijective function, then $X$ , $[0,1]$ are homeomorphic. My attempt Let $A\subset X$ be non-empty open set, let's see f(A) is open. we take $y\in f(A)$ , then there exist $x\in A$ such that $f(x)=y$ ( $f$ bijective).
By hypothesis $X$ is locally connected then exist $V\subset X$ open connected set such that $x\in V\subset A$ , by continuity $f(V)$ is connected and $y\in f(V)\subset f(A)\subset  [0,1]$ , thus $f(V)$ is an interval, then I think that is enough to prove that $f(V)$ is not of the form $[a,b]$ , $[a,b)$ , $(a,b]$ for $a>0$ , $b<1$ But I don't know how to prove it. thanks in advance",['general-topology']
3171460,Conditions on the domain for Sobolev embeddings,"I am reading the proof of the Sobolev embedding theorem presented in the book Sobolev Spaces by Robert A. Adams and John J. F. Fournier. I could not understand the proof for part II of the theorem. The purpose of the proof is to embed $W^{1,p}(\Omega)\hookrightarrow C^{0,\lambda}(\Omega)$ where $n<p\leq\infty$ , $0<\lambda\leq1-n/p$ , and $\Omega\subset\mathbb{R}^n$ is a connected open subset that satisfies the strong local Lipschitz condition (page 83, paragraph 4.9): In the actual proof, the authors reduced it to the following lemma: They proved the case in which $\Omega$ is a unit cube, and then proceeds thus: My Question I'm at a total loss. Where is the strong Lipschitz condition used? Why can we take such $P$ and $\delta_0,\delta_1$ ?","['sobolev-spaces', 'functional-analysis', 'analysis', 'partial-differential-equations']"
3171488,How to show that the given matrix has non-zero determinant,"Given $p,q$ to be primes where $p<q$ . Show that the following marix has non-zero determinant, \begin{bmatrix}
	1&2 & 2 & 2 &\dotso & 2\\
	2&q-p+1 & 1 & 1 &\dotso & 1\\
2& 1 & q-p+1 & 1 & \dotso  & 1\\2&1 & 1 & q-p+1 &\dotso & 1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso
	\\2&1 &1 &1 &\dotso & q-p+1
	\end{bmatrix} I am able to show that the submatrix of this matrix \begin{bmatrix}q-p+1 & 1 & 1 & 1 & \dotso  & 1\\ 1 &q-p+1& 1 &\dotso & \dotso &1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso
	\\1 &1 &1 &\dotso & \dotso &q-p+1\end{bmatrix} has determinant non-zero. How I can show that the original matrix has determinant non-zero? I tried using Laplace Expansion but not getting anything.
Please help.","['matrix-rank', 'determinant', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra']"
3171510,"Can someone explain the motivation behind 'normal neighborhood, normal coordinates' of a point in Riemannian manifold?","I am studying on my own from do Carmo's Riemannian-geometry text and in chapter 3, the author introduces 'normal neighborhood of a point in Riemannian manifold $M$ and normal neighborhood of $0 \in T_{p}M$ and then introduces normal coordinates around a point $p \in M$ . And then in subsequent topics the author frequently uses normal neighborhood, normal ball etc.in proofs. Now, what I don't understand is, why do we need these special type of coordinate chart around a point and special type of balls? It would be great if someone could refer me to some source or maybe explain here the motivation behind introducing such notions which is not clear just by reading the text. Thank you!","['riemannian-geometry', 'differential-geometry']"
3171514,"Graph on $n$ vertices, each vertex of degree 3 or less. Can we color the vertices in a way such that...","Let $G$ be a graph on $n$ vertices. Suppose each vertex has at most 3 neighbors. Prove that you can color the vertices either red or blue in such a way that each vertex is connected to at most 1 vertex of the same color. I tried the following approach: Color the vertices randomly. Then look at each vertex, and if it is connected to more than one vertex of the same color, change its color. I was hoping that there would be some monovariant. The one I considered, which is wordy to explain, did not work (I considered the total number of ""off"" items per vertex). My second approach was to construct the graph edge by edge, coloring as you go. I think it makes sense to split the vertices up into four partitions: $V_0$ , $V_1$ , $V_2$ , and $V_3$ where a vertex $v$ belongs into $V_i$ , 1 $\leq i \leq 3$ , if $\deg(v)=i$ . Then, go through each vertex of $V_{1}$ , adding each edge and coloring each pair of vertices opposite colors, etc.... Something along those lines. Can anyone think of a clever/simple way to prove this? Because, although my second approach probably works, it will require a ton of technical writing. There must be a simpler way to think about this problem.","['graph-theory', 'proof-writing', 'combinatorics', 'discrete-mathematics', 'bipartite-graphs']"
3171551,Longest path through a rectangular board,"The original problem: You have a $H\times W$ board. Your task is to color some tiles white and some black. There must be a unique beginning and end of the white trail and only one way to get from one to the other. Moving is allowed only through edges. Maximise the number of white tiles. Example $6 \times 8$ solution: Is there a formula that produces the maximum number of white tiles just from the height and width? Observations : Here are values I have found using a depth-first brute-force search (and the formulae below): 8x8 took over two hours to find. The empty spaces in the table are too complex for brute-force. Here are the differences compared to the tile one to the left (one tile narrower): Observations : Odd boards first get simple zig-zag patterns, but this breaks at $7 \times 7$ . A naive idea would be: But this has merely 31 white tiles. The optimal solution has 33: The formula for $H = 2$ is $\lceil{\frac{3}{2} W}\rceil$ , since columns of $2$ and $1$ white tiles alternate. The formula for $H = 3$ is $2 H + 1$ if $H < 6$ and $2 H + 2$ otherwise, since you just get extensions of this: Structure of $H = 4$ : For every three added tiles of width, there are an additional 8 white tiles. The differences loop after $W = 5$ as $3,3,2$ . Beyond that, backtracking gets more complicated and I cannot find the pre-loop values, or even prove there is a loop.","['graph-theory', 'discrete-mathematics']"
3171564,Derivative of Dirac delta function,"Is the relation of the Dirac delta function correct? $$
\frac{\partial}{\partial x''}\delta(x''-x')
= -\frac{\partial}{\partial x'}\delta(x'-x'').\tag{1}
$$ If it is, how to derive the above relation?","['derivatives', 'dirac-delta', 'distribution-theory']"
3171581,How do you prove that $\binom{n}{d} = \Theta(n^d)$?,"I am stuck on proving that that $\binom{n}{d} = \Theta(n^d)$ for any positive fixed integer d. I tried using the fact that if this is true, it means that for some integers c $_1$ and c $_2$ , $c_1n^d \le |\frac{n!}{(n-d)!d!}| \le c_2n^d$ for any $n \ge n_0$ where $n_0$ is an integer. I tried using the fact that since n is greater than d and d is positive, then $|\frac{n!}{(n-d)!d!}|$ = $\frac{n!}{(n-d)!d!}$ . If the above equation is true in which the binomial expansion is bounded, then it would mean that $c_2 \ge \frac{n!}{n^d(n-d)!d!}$ , and similarly, $c_1 \le \frac{n!}{n^d(n-d)!d!}$ So I have absolutely no idea how to find these constants $c_1$ and $c_2$ , let alone find what $n_0$ is. Can someone please help me with this in a way that allows me to at least understand more of what's going on here? Thank you in advance.","['number-theory', 'limits', 'binomial-coefficients', 'polynomials']"
3171592,Solve matrix equation $XAX^*=B$ for $X$ in least squares sense,"How can the following optimization problem be solved? $$\arg\min_{\mathbf{X} \in \mathcal{C}^{n \times m}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^* - \mathbf{B} \right\Vert_F$$ where $\mathbf{A} \in \mathcal{C}^{m \times m}$ and $\mathbf{B} \in \mathcal{C}^{n \times n}$ are Hermitian, with $m > n$ , generally. Matrix $\mathbf{A}$ is diagonal and invertible. Let $^*$ denote the conjugate transpose. Solution attempt Solution attempt was wrong, see comments. See the answers for a better attempt. I realize it is possible to take the derivative of the expression and equating it to zero, but I end up with the following expression I have trouble with $\require{enclose}
     \enclose{horizontalstrike}{\frac{d}{d \mathbf{X}}\mathrm{Tr}\left( \mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A}\mathbf{X}^* - 2\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{B} + \mathbf{B}^*\mathbf{B} \right) = 0}$ $\require{enclose}
     \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A} - \mathbf{B}\mathbf{X}\mathbf{A} = 0}$ Because $\require{enclose}
     \enclose{horizontalstrike}{\mathbf{A}}$ is invertible $\require{enclose}
     \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{X} - \mathbf{B}\mathbf{X} = 0}$ Which does not seem any simpler. Variant Would the following variant be harder to solve? I don't see it working with Måren W's answer below $\underset{\mathbf{X}}{\mathrm{argmin}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^T - \mathbf{B} \right\Vert_F$ with $m > n$ . Again the matrices are complex, but now $\mathbf{A}$ is a symmetric (not Hermitian), diagonal and invertible matrix and $\mathbf{B}$ is also symmetric.","['matrix-equations', 'matrices', 'least-squares', 'optimization', 'symmetry']"
3171605,Number of solutions to $x_1+x_2+\cdots+x_5=41$,"Use a generating function to count the number of integer solutions
   to $x_1+x_2+\cdots+x_5=41$ that satisfy $0\le x_i\le20$ for all $i$ , $x_i$ is even when $i$ is even, and $x_i$ is odd when $i$ is odd. Ignoring the conditions at the end, I believe that $g(x)=(1+x+x^2+\cdots+x^{20})^5$ , and the answer to the problem is the coefficient of $x^{41}$ in $g(x)$ . But, as I said, I have ignored the terms after all; I haven't the faintest idea of how to go about incorporating them. I'm (obviously) missing something, and I don't seem to be getting anywhere, no matter how many examples I go through. Any help is greatly appreciated!!","['combinatorics', 'generating-functions']"
3171665,Probability for a graph algorithm,"Let $G = (V, E)$ a graph. A 'dominant set' $W ⊆ V$ is a set of nodes, so that for each node $v \in V$ holds that either $v$ itself or a neighbor of $v$ is contained in $W$ . 
Assume that $G$ has minimum degree at least $d > 1$ , i.e. each node $v \in V$ has degree $deg(v) ≥ d$ . The algorithm consists of two rounds. In the first round we mark each node independently from the other nodes with probability $p$ . In the second round we look at each node $v \in V$ , if neither $v$ nor any of its neighbours were marked in the first lap, we mark $v$ . Let $X$ be the number of knots marked in the first round.
So $E(X) = |V|*p$ , because $X \sim Bin(|V|,p)$ , right ? Let $v ∈ V $ any (but fixed) node. If think the probability that neither v nor one of the neighbors of v was marked in the first round would be $(1-p)^{deg(v)}$ right ? But how can i finde a upper bound  which is only dependent from $d$ and $p$ (and not from $v$ ). Let $Y$ be the number of knots marked in the second round. How can i finde a upper bound for $E(Y)$ .","['graph-theory', 'probability-theory', 'probability', 'algorithms']"
3171669,An entrance exam problem relating to sequences and limits,"this is a problem from the entrance exam of the University of Tokyo and unfortunately, the official doesn't offer solutions. 1) Using mathematical induction. Assume $f_{n}(x)=c_{n} x^{a_{n}}$ holds for $n$ , and by $f_{n+1}(x)=p \int_{0}^{x}\left(f_{n}(t)\right)^{1 / q} \mathrm{d} t$ we can get $f_{n+1}(x)$ . Then comparing the coeffient and the exponent will show that it holds for $n+1$ too. 2) 3) 4) 5) 6) I've no idea. I tried to calc the derivatives of $g_n$ , but I don't know what to do next. I can get $a_{n+1}$ from the recursive formula, but the form of it is kinda complex which makes it hard to get $c_{n+1}$ . So I guess the rest questions could be done without knowing what actually $a_n$ and $c_n$ are. But I don't know how to do that. Also, $1 / p+1 / q=1$ seems a frequent condition, how is it usually used in solutions?","['limits', 'analysis', 'sequences-and-series']"
3171683,"Given $\lim_{n \rightarrow \infty} a_n a_{n+1} = L$, how to show: $\lim_{n \rightarrow \infty} a_n a_{n+3} = L$",let $\{ a_n \}$ be a sequence where for each $n \in \mathbb N$ $ a_n \neq 0 $ and where $\lim_{n \rightarrow \infty} a_n a_{n+1} = L$ with $L \neq 0$ I want to prove that $\lim_{n \rightarrow \infty} a_n a_{n+3} = L$ and that $\lim_{n \rightarrow \infty} a_n a_{n+2} \neq -1$ Any ideas? Thanks! Edit : Intuitively it's clear but I am looking for a real regorous proof..,"['limits', 'limsup-and-liminf', 'sequences-and-series']"
3171798,"$8$ billion people in the world, increase each year by $2\%$, how many people will be in $9$ months?","I have the following ODE problem : $8$ billion people in the world, increase each year by $2\%$ , how many people will be in $9$ months? What I did : The change each year is : $$x'(t)=0.02(x(t))$$ Therefore $$x(t)=Ce^{0.02t}$$ In $t_0$ we know that $x(t_0)=8$ , therefore $C=8*e^{-0.02t_0}$ So I got that $$x(t)=8*e^{(0.02(t-t_0))}$$ The number of people in $9$ months are $t-t_0=3/4$ Therefore I got : $$x(t)=8*e^{0.02(0.75)}=8.1209$$ In the text book the right answer is $8.1197$ I don't understand what's wrong with my solution. Any help will be appreciated, thanks.","['proof-verification', 'ordinary-differential-equations']"
3171807,Show that $\lim_{n}\sum_{k=n}^{2n}{1\over k} = \ln2$ using elementary methods.,"Prove that: $$
\lim_{n\to\infty}\left({1\over n} + {1\over n+1} + \cdots + {1\over 2n} \right) = \ln2
$$ I would like to show that using elementary methods, since I'm not allowed to even use derivatives, not to mention integrals. Before this one, I've been able to show that: $$
\exists\lim_{n\to\infty}\left(1 + {1\over 2} + {1\over 3} + \cdots + {1\over n} - \ln n\right) = L \tag 1
$$ Since the expression in $(1)$ under the sign of limit is bounded below and is monotonically decreasing, then by monotone convergence theorem it must converge to some number (which appeared to be named the Euler-Mascheroni constant.) Now since $(1)$ converges then it must satisfy Cauchy's Criteria. Let's define the following sequence: $$
x_n = 1 + {1\over 2} + {1\over 3} + \cdots + {1\over n} - \ln n
$$ Then $x_{2n}$ is defined as follows: $$
x_{2n} = 1 + {1\over 2} + {1\over 3} + \cdots + {1\over 2n} - \ln (2n)
$$ But both limits exist and are equal, which implies that: $$
\exists\lim_{n\to\infty}|x_{2n} - x_n| = 0 \tag2
$$ Now performing some algebraic manipulations on $(2)$ one may obtain: $$
\begin{align}
|x_{2n} - x_n| &= \left|\sum_{k=1}^{2n}{1\over k} - \sum_{k=1}^{n}{1\over k} -\ln(2n) + \ln n\right|\\
&=\left|\sum_{k=n+1}^{2n}{1\over k} - (\ln(2n) -\ln n)\right| \\
&=\left|\sum_{k=n+1}^{2n}{1\over k} - \ln 2\right|
\end{align}
$$ Since $|x_{2n} - x_n|$ is convergent to $0$ then: $$
\forall \epsilon > 0\ \exists N\in\Bbb N: \forall n\ge N \implies |x_{2n} - x_n| = \left|\sum_{k=n+1}^{2n}{1\over k} - \ln 2\right| < \epsilon
$$ Which is a standard definition of the limit, hence: $$
\lim_{n\to\infty}\sum_{k=n}^{2n}{1\over k} = \ln 2
$$ I would like to ask for verification of the proof above, and/or point to mistakes in case of any. Thank you! Note: This problem is given among other problems in the ""Limit of numerical sequences"" section. Long before the definition of the Integral is given.","['limits', 'proof-verification', 'sequences-and-series', 'real-analysis']"
3171813,"What does Rosenlicht mean by a ""point""?","I am reading Maxwell Rosenlicht's 1956 paper, ""Some Basic Theorems on Algebraic Groups"" ( American Journal of Mathematics 78(2):404-443) [JSTOR link] , and having trouble with some of the notation and language. In particular, since Rosenlicht admits arbitrary ground fields but is writing before Grothendieck's development of scheme theory, I'm confused by what he means by a point of an algebraic variety. Rosenlicht is working with an algebraic group $G$ defined as ""a union of a finite number of disjoint algebraic varieties [I presume ""variety"" means irreducible in this context] ... together with a group structure [defined by rational maps] ... a field $k$ is called a field of definition of $G$ if it is a field of definition for each component of $G$ and for all of the above rational maps..."" (p. 402). Question: In this context, what are the elements of $G$ ? In particular if I want to think of $G$ as a separated scheme of finite type over $k$ , is Rosenlicht talking about the closed points? Only the closed points with residue field $k$ ? Or the closed points of the base change to $\overline k$ ? Or what?","['definition', 'algebraic-geometry', 'algebraic-groups']"
3171839,Find a norm in the dual space,"Let $X$ be a normed space and $Y$ a linear subspace of $X$ . We define $$Y^{\perp}=\{f\in X^*: f(y)=0, \; \forall y\in Y\}$$ and $$\|f\|_Y=\sup\{|f(y)|: y\in Y, \; \|y\|=1\}.$$ Prove that $$\|f\|_Y=\inf\{\|f-g\|: g\in Y^{\perp}\}$$ First of all, given $f\in X^*\backslash Y^\perp$ , then for any $g\in Y^\perp$ we get that \begin{equation*}
\begin{split}
\|f-g\|&=\sup\{|f(x)-g(x)|: x\in X, \; \|x\|=1\}\\
&\geq\sup\{|f(y)+g(y)|: y\in Y, \; \|y\|=1\} \\
&= \sup\{|f(y)|: y\in Y, \; \|y\|=1\}\\
&=\|f\|_Y
\end{split}
\end{equation*} As $g\in Y^\perp$ was arbitrary, we find that $$\|f\|_Y\leq\inf\{\|f-g\|: g\in Y^{\perp}\}$$ How can I prove the other inequality?","['hahn-banach-theorem', 'functional-analysis']"
3171921,Is there a notion of a continuous basis of a Banach space?,"If $X$ is a Banach space, then a Hamel basis of $X$ is a subset $B$ of $X$ such that every element of $X$ can be written uniquely as a linear combination of elements of $B$ .  And a Schauder basis of $X$ is a subset $B$ of $X$ such that every element of $X$ can be written uniquely as an infinite linear combination of elements of $B$ . But my question is, is there a notion of a “continuous basis” of a Banach space?  That is, a subset $B$ of $X$ such that every element of $X$ can be written uniquely in terms of some kind of integral involving elements of $B$ . I’m not sure what the integral should look like, but one possibility is this.  We define some function $f:\mathbb{R}\rightarrow X$ , and we let $B$ be the range of $f$ . And then for any $x\in X$ , there exists a unique function $g:\mathbb{R}\rightarrow\mathbb{R}$ such that $x = \int_{-\infty}^\infty g(t)f(t)dt$ , where this is a Bochner integral .  And if that’s the case we say that $B$ is a continuous basis for $X$ .  Does any of this make sense? EDIT:  I've realized that my question is related to a whole bunch of other topics, including Fourier transforms , Rigged Hilbert Spaces , and Spectral Theory .  See this answer , this answer , this question , this question , and this question .","['banach-spaces', 'schauder-basis', 'distribution-theory', 'functional-analysis', 'bochner-spaces']"
3171942,Infinite sum of harmonic number:$\frac12+\left(1+\frac12\right)\frac1{2^2}+\left(1+\frac12+\frac13\right)\frac1{2^3}+\cdots$,I learned that I can find the value of some infinite sum. Then what is the value of this sum? $$\frac12 + \left(1+\frac12\right)\frac1{2^2}+\left(1+\frac12 +\frac13\right)\frac1{2^3}+\left(1+\frac12 +\frac13 +\frac14\right)\frac1{2^4} + \cdots $$ And I want to know How to find the value of the infinite sum of this-like form.,['sequences-and-series']
3171955,The number of elements in a set of matrices with some properties,"Given $M$ comprised of $n\times n$ matrices, which satisfies $I_n \in M$ and $0_{n} \not\in M$ If $A,B \in M$ ,  then $AB \in M$ or $-AB \in M$ If $A,B \in M$ ,  then $AB = BA $ or $AB = -BA$ If $A\in M$ and $A\ne I_n$ , then there exists $B \in M$ such that $AB=-BA$ Prove that the number of elements in $M$ in less than $2 n^2$ . Some thoughts For the condition 4 , we can say the corresponding $B\ne I_n,A$ . Because if $B=I_n$ , we get $A=0$ , a contradiction. If $B= A$ then $AB=0$ , which contradicts the condition 2. Thus we can consider $M$ as the set of such pairs $(A,B)$ . But how to move on? Any hints? Thank you in advance! Added It's easy to see for all $A \in M$ , $A^2$ and $-A^2$ commute with all the matrices in $M$ . Thus from conditions 2 and 4 we get $A^2 = I$ or $-I$ , which might help.","['matrices', 'abstract-algebra', 'linear-algebra']"
3172014,"Weak solutions to $\Delta u=f$ are in $W^{2,2}$","I  believe  the following statement is true. Let $\Omega$ be a smoothly , bounded domain in $\mathbb{R}^{n}$ . The statement: Let $u\in H^{1}(\Omega)$ so that there exists $f\in L^{2}(\Omega)  \;s.t.\int_{\Omega}\nabla u\nabla \varphi=\int_{\Omega} f\varphi, \forall \varphi\in H^{1}(\Omega)$ . Then $u\in H^{2}(\Omega)$ . I have searched in the book by Evans and Brezis but not so certain. Could anyone provide a reference for that? It can be seen in Evans's book that $u\in H^{2}_{loc}(\Omega)$ . Thanks so much.","['regularity-theory-of-pdes', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
3172016,A tricky integral involving hyperbolic functions,"Can anyone suggest a method for solving the integral below? I've tried numerous things but have had no luck yet. To be honest I'm not sure an analytical solution actually exists. $$I=\int\cosh(2x)\sqrt{[\sinh(x)]^{-2/3}+[\cosh(x)]^{-2/3}}\,\textrm{d}x.$$ Thanks. Here is another attempt I have made: Let $y=[\tanh(x)]^{2/3}$ , and rewrite $I$ such that $$I=\int\cosh(2x)[\sinh(x)]^{-1/3}\sqrt{1+[\tanh(x)]^{2/3}}\,\textrm{d}x.$$ Then $\textrm{d}x=(3/2)[\tanh(x)]^{1/3}[\cosh(x)]^{2}\,\textrm{d}y$ . Therefore \begin{align*}
I&=\frac{3}{2}\int\cosh(2x)[\sinh(x)]^{-1/3}[\tanh(x)]^{1/3}[\cosh(x)]^{2}\sqrt{1+y}\,\textrm{d}y
\\
&=\frac{3}{2}\int\cosh(2x)[\cosh(x)]^{5/3}\sqrt{1+y}\,\textrm{d}y
\\
&=\frac{3}{2}\int[\cosh(x)]^{11/3}\sqrt{1+y}\,\textrm{d}y
+\frac{3}{2}\int[\sinh(x)]^{2}[\cosh(x)]^{5/3}\sqrt{1+y}\,\textrm{d}y
\\
&=\frac{3}{2}\int[\textrm{sech}(x)]^{-11/3}\sqrt{1+y}\,\textrm{d}y
+\frac{3}{2}\int[\tanh(x)]^{2}[\textrm{sech}(x)]^{-11/3}\sqrt{1+y}\,\textrm{d}y
\\
&=\frac{3}{2}\int[\textrm{sech}(x)]^{-11/3}\sqrt{1+y}\,\textrm{d}y
+\frac{3}{2}\int[\textrm{sech}(x)]^{-11/3}y^3\sqrt{1+y}\,\textrm{d}y
\\
&=\frac{3}{2}\int\frac{(1+y)^{1/2}}{(1-y^3)^{11/6}}\,\textrm{d}y
+\frac{3}{2}\int y^{3}\frac{(1+y)^{1/2}}{(1-y^3)^{11/6}}\,\textrm{d}y
\\
&=\frac{3}{2}\int\frac{(1+y^3)(1+y)^{1/2}}{(1-y^3)^{11/6}}\,\textrm{d}y
\\
&=\frac{3}{2}\int\frac{(1-y^6)(1+y)^{1/2}}{(1-y^3)^{17/6}}\,\textrm{d}y
\end{align*}","['integration', 'calculus', 'hyperbolic-functions']"
3172035,Geometric reason why there is a unique Covering space corresponding to Sylow Subgroup,"By the Galois Correspondence (for path connected, locally simply connected spaces $X$ , say) if $\pi_1 (X)$ is a finite group, then there is a unique isomorphism class of non-based covering space with fundamental group $\cong P$ where $P \in \mathrm{Syl}_p (\pi_1 (X))$ for each prime $p$ dividing $|\pi_1 (X)|$ , since by the Sylow theorems any $p$ -Sylow subgroup is conjugate. I was wondering whether there is a geometric reason as to why this is true. What is the geometric relationship between the 'holes' in such spaces and coverings of these maximal prime power order?","['group-theory', 'sylow-theory', 'algebraic-topology']"
3172100,Proof that $ \sum_k (-1)^k \binom{n-k}{n-m} \binom{m}{k} = \binom{n-m}{m}$,Proof that $$ \sum_k (-1)^k \binom{n-k}{n-m} \binom{m}{k} = \binom{n-m}{m}$$ I tried to solved that in two approaches (in both failed): 1. Algebraically: Use negation theorem: $$ \sum_k (-1)^k \binom{n-k}{n-m} \binom{m}{k} = \sum_k \binom{n-k}{n-m} \binom{k-m-1}{k} $$ and after than I can: $$ \sum_k \binom{n-k}{n-m} \binom{k-m-1}{k} = \sum_k \binom{n-k}{n-m} \binom{k-m-1}{-m-1} $$ but stucked... Another approach was trying to proof that via combinatoric's interpretation. Because lack of result I don't have anything to show there in that way..,"['summation', 'combinatorics']"
3172102,Proving that a function has a maximizer,"I am trying to prove this: Let $X$ be a nonempty, convex, and compact subset of $\mathbb{R}^n$ and $f:X \rightarrow \mathbb{R}$ a concave function. Then $f$ has a maximizer. Now this is trivial (somewhat) if f is continuous because of Weistrass' theorem. But f isn't necessarily continuous here. I have been stuck on this for a couple of days now. I can sort of imagine why this would be true if $f:\mathbb{R} \rightarrow \mathbb{R}$ . But I haven't gotten any idea further than that yet. Any help would be great!","['optimization', 'functional-analysis']"
3172107,Reference request on Riemann-Hilbert correspondence,"I'm looking for any books, lecture notes, e.t.c for the Riemann-Hilbert correspondence in the form of the following equivalence between flat $G$ -connections, $G$ -local systems and Homomorphisms from a fundamental group to $G$ .","['complex-geometry', 'algebraic-geometry', 'reference-request', 'differential-geometry']"
3172144,Non-flat connection on trivial bundle?,From what I have read it seems like there exists non-flat connections on trivial vector/principal bundles. However I can't find any notes on it or examples. Can anyone confirm that such connections exist and perhaps provide an example?,"['principal-bundles', 'connections', 'curvature', 'vector-bundles', 'differential-geometry']"
3172171,Derivative of a function of matrix,"I am trying to derive the gradient of the function $f(X) = AXZ + XZX^TXZ$ where $A,X,Z \in R^{n \times n}$ with respect to $X$ matrix. I read a post Matrix-by-matrix derivative formula about matrix derivate, but I am not able to follow it. In my case $\frac{\partial f(X)}{\partial X)}$ would be a tensor, but If I try to use the formula given in the post I would get a matrix. How should I process to get the partial derivate?","['matrices', 'matrix-calculus', 'partial-derivative']"
3172174,Metric Tensor of Hyperboloid Model for Hyperbolic Space with Curvature $K$,"Let $\mathbb{H}^n_K$ denote the set of points of the hyperboloid model, which models the hyperbolic space with sectional curvature $K<0$ . So $$
\mathbb{H}^n_K=\left\{x\in\mathbb{R}^{n+1}
\,\middle|\, 
\langle x,x\rangle_*=\frac{1}{K}
\,\land\,x_1>0\right\}
$$ where $\langle x,y\rangle_*=-x_1y_1+\sum_{i=2}^{n+1}x_iy_i$ denotes the Lorentz inner product. Question: How do you derive the metric tensor $g$ for any sectional curvature $K$ ? My solution so far: The first coefficient of a point $\mathbf{x}$ on the manifold is a function of the remaining coefficients: $$
\mathbf{x}=(x_1,\underbrace{x_2,\ldots, x_{n+1}}_{=\mathbf{x'}})\in\mathbb{H}^n_K
    \Longleftrightarrow
    x_1=\sqrt{||\mathbf{x'}||_2^2-\frac{1}{K}}.
$$ So a point $\mathbf{x}\in\mathbb{H}^n_K$ is given as a function of $n$ parameters: $$
\mathbf{x}(\underbrace{u_1,\ldots,u_n}_{=\mathbf{u}})
=
\left(
\underbrace{\sqrt{||\mathbf{u}||_2^2-\frac{1}{K}}}_{x_1}, 
\overbrace{\underbrace{u_1}_{x_2},
\ldots,\underbrace{u_i}_{x_{(i+1)}},\ldots,
\underbrace{u_n}_{x_{n+1}}}^{=\mathbf{x'}}\right)^T
\in\mathbb{H}^n_K.
$$ Now, a set of tangent vectors $\{\mathbf{x}_1,\ldots,\mathbf{x}_n\}$ spanning the tangent space $\mathcal{T}_{\mathbf{x}}\mathbb{H}^n_K$ is given by $$
\mathbf{x}_i(u_1,\ldots,u_n)
:=
\frac{\partial \mathbf{x}}{\partial u_i}
=
\left(
\frac{u_i}{\sqrt{||\mathbf{u}||_2^2-\frac{1}{K}}},
0,\ldots,0,\underbrace{1}_{(i+1)\text{th index}},0,\ldots,0
\right)^T\in\mathbb{R}^{n+1}.
$$ Indeed, one can see that $\langle\mathbf{x},\mathbf{x}_i\rangle_{*}=-u_i+u_i=0$ as we require from a tangent vector $\mathbf{x}_i$ . Now, the coefficients of the first fundamental form for the basis $\{\mathbf{x}_1,...,\mathbf{x}_n\}$ of the tangent space are given by: $$
g_{ij}(\mathbf{x})
=
\langle\mathbf{x}_i,\mathbf{x}_j\rangle_{*}
=
\frac{-x_{i+1}x_{j+1}}{||\mathbf{x}'||_2^2-\frac{1}{K}}
+
\delta_{ij}
$$ Note that use the Lorentz inner product here, because the intrinsic hyberbolic geometry is induced by the extrinsic Lorentzian geometry. For convenience, we'll define: $$
\mathbf{g}(\mathbf{x})
=
\begin{pmatrix}
g_{11}(\mathbf{x}) & \cdots & g_{1n}(\mathbf{x})\\
\vdots & \ddots & \vdots\\
g_{n1}(\mathbf{x}) & \cdots & g_{nn}(\mathbf{x})\\
\end{pmatrix}\in\mathbb{R}^{n\times n}.
$$","['hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
3172227,"Prove that for each $\epsilon>0$, there exist a compact $K\subset E$ such that $m(E\cap K^c)<\epsilon$","Let $E\subset\mathbb R$ be Lebesgue measurable with $m(E)<\infty$ . Prove that for each $\epsilon>0$ , there exist a compact $K\subset E$ such that $m(E\cap K^c)<\epsilon.$ Attempt By contradiction, suppose that for all compact $K\subset E$ , $m(E\cap K^c)\ge\epsilon.$ Notice that $K$ is Lebesgue measurable, thus $m(E)=m(E\cap K)+m(E\cap K^c)\ge(E\cap K)+\epsilon$ But from here is not clear how to get a contradiction. Could someone help please?","['measure-theory', 'lebesgue-measure', 'compactness']"
3172359,Calculating Arc Hyperbolic CoSecant faster than using a standard power series,"I have used the standard Power-Series to calculate Arc Hyperbolic Co-Secant trig function. But as with my other posts before, it is very slow. Here is the standard Power-Series, taken from https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#Series_expansions $$ \operatorname{arcsch} x = \operatorname{arsinh} \frac1x  = x^{-1} - \left( \frac {1} {2} \right) \frac {x^{-3}} {3} + \left( \frac {1 \cdot 3} {2 \cdot 4} \right)  \frac {x^{-5}} {5}  - \left( \frac {1 \cdot 3 \cdot 5} {2 \cdot 4 \cdot 6} \right)  \frac {x^{-7}} {7} \pm\cdots \\
                      = \sum_{n=0}^\infty \left( \frac {(-1)^n(2n)!} {2^{2n}(n!)^2} \right) \frac {x^{-(2n+1)}} {2n+1} , \qquad \left| x \right| > 1 $$ I have used a different form to speed it up, which it did not. $$ \operatorname{arcsch} x = \frac {1}{x} - \left( \frac {1} {2} \right) \frac {1} {3 x^{3}}+ \left( \frac {1 \cdot 3} {2 \cdot 4} \right)  \frac{1} {5x^{5}}  - \left( \frac {1 \cdot 3 \cdot 5} {2 \cdot 4 \cdot 6} \right)  \frac {1} {7x^{7}} \pm\cdots \\ 
                      = \sum_{n=0}^\infty \left( \frac {(-1)^n(2n)!} {2^{2n}(n!)^2} \right) \frac {x^{-(2n+1)}} {2n+1} , \qquad \left| x \right| > 1 $$ But it is easier to reproduce in my calculator program. The part that is taking so long are terms $$ \left ( \frac {1} {7x^{7}} \right ) $$ and higher. The time spent dividing $$  {7x^{7}} $$ into $$ 1 $$ is huge. At this iteration, it takes about 15 seconds. The next ones just keep growing, using more time. And yes, this post is just my last post Calculating Arc Hyperbolic CoTangent faster than using a standard power series . Here, 2 persons showed me the Horner's method and a programming procedure. It was really fast, compared to what I was using before. I am hoping that there is a Horner-like form that a person, you people, can show me. I have read the Wiki page that explains the Horner's method, but that does not mean I can create the method. Thank you very much for your time.","['power-series', 'trigonometric-series', 'trigonometry', 'sequences-and-series']"
3172374,Does there exist a bijection $f$ from $\mathbb{N}$ to $\mathbb{Q}^+$ such that $\lim_{n \to \infty} \frac{f(n+1)}{f(n)}$ exists?,Does there exist a bijection $f$ from $\mathbb{N}$ to $\mathbb{Q}^+$ such that $$\lim_{n \to \infty} \frac{f(n+1)}{f(n)}$$ exists? My guess that no such $f$ exists.,"['rational-numbers', 'real-analysis']"
3172378,What is the expected value of this game for large N?,"For a given even $N$ , I have $N/2$ red cards and $N/2$ black cards. Each time I draw a black card I win a dollar, each time I draw a red card I lose a dollar.  I can stop at any time I like (and choose to do so in such a way that would maximize my expected winnings). What is the expected value of the game for large $N$ ? For a simple example, when $N=2$ - I would draw a card and if it's red, I would draw again (to get value of $0$ ), and if it's black I would stop, resulting in expected value of $0.5$ . To clarify, I know how to compute this numerically. I'm interested in the functional form for large $N$ . For what it is worth, from simulations it appears to be ${\cal O}(\sqrt{N})$ .","['expected-value', 'asymptotics', 'probability']"
3172406,What is the set of pointwise limits of polynomials?,"The set of pointwise limits of continuous functions from from $\mathbb{R}$ to $\mathbb{R}$ is the set of Baire class 1 functions .  My question is, my question is, what is the set of pointwise limits of polynomials from $\mathbb{R}$ to $\mathbb{R}$ ? The Weierstrauss approximation theorem implies that every continuous function is a pointwise limit of polynomials.  But are there also discontinuous functions which are pointwise limits of polynomials?","['weak-convergence', 'weierstrass-approximation', 'real-analysis', 'functional-analysis', 'descriptive-set-theory']"
3172422,Is it possible to calculate $\phi(2^{1092}-1)$,"Is it possible to calculate $\phi(2^{1092}-1)$ , where $\phi$ is the Euler Totient function? Or is completely infeasible even with the help of the most powerful machines?",['number-theory']
3172485,Fourier Series for $\cos^3(x)$,"Consider the familiar trigonometric identity: $\cos^3(x) = \frac{3}{4} \cos(x) + \frac{1}{4} \cos(3x)$ Show that the identity above can be interpreted as Fourier series expansion. so we know that cos is periodic between $\pi$ and $-\pi$ and $\cos$ is an even function, therefore, $\cos^3$ is even. so we need to compute $a_0$ ( the integral of $f(x)$ and it will equal $0$ ) and $a_n$ ( the integral from $\pi$ to $-\pi$ of $\cos^3(x) \cos(nx)$ ) how to compute $a_0$ thanks","['fourier-series', 'trigonometry', 'fourier-analysis', 'fourier-transform']"
3172525,Harmonic oscillator with time dependent friction term,"Suppose I have a harmonic oscillator of the following form: $\ddot x(t)=-F(t)\dot x(t)-x(t), F(t)>0$ for all $t$ . From the physical perspective, the term proportional to $\dot x(t)$ represents a friction term. Hence, if $F(t)>0$ for all $t$ , I would expect that $\lim_{t\to\infty} x(t)=0$ . But I am not sure how to proof that, and perhaps it is not even true, and my thinking is too simplistic. If someone had some thoughts, I would appreciate! : ) EDIT: I can probably also assume $F(t)$ to be $C^\infty$ .","['asymptotics', 'ordinary-differential-equations']"
3172556,Show that $4x^2+6x+3$ is a unit in $\mathbb{Z}_8[x]$,"My attempt is:
We guess for $p(x)$ an inverse polynomial $q(x)=2x+3$ , $(4x^2+6x+3)(2x+3)=8x^3+12x^2+6x+12x^2+18x+9=1\pmod{8}$ . The existence of such an inverse verifies this is a unit in $\mathbb{Z}_8[x]$ Edit:Typo","['ring-theory', 'abstract-algebra']"
3172620,Examples of sequential compact but not compact spaces that do not use ordinals.,"I think the title is self explanatory, I'm using Munkres' Second Edition text for Point Set Topology and I can't figure out if such examples are possible.","['general-topology', 'examples-counterexamples', 'compactness']"
3172626,Showing that $x^3-x^2-1$ has only one REAL root.,"So, I need to prove that $x^3-x^2-1$ = 0 has only one root. Here's what I have so far: We have $f(-2) = -13 < 0, f(2) = 3 > 0$ . Because f is a polynomial, it is continuous on all $\mathbb{R}$ and there is c s.t. $-2 < c < 2$ and $f(c) = 0$ by I.V.T. This shows the equation has at least one solution. Usually from here, people show that $f '(x)$ is strictly increasing, and thus can't have second root. But in this case $f'(x)$ = $3x^2-2x$ , and $f'(x) = 0, $ when $x = 0$ and x = $2/3$ . So, I have no idea how to prove it has ONLY one root :( Thanks in advance for your help!",['calculus']
3172677,Fair gambler's ruin problem intuition,"In a fair gambler's ruin problem, where the gambler starts with k dollars, wins \$1 with probability 1/2 and loses \$1 with probability 1/2, and stops when he/she reaches \$n or \$0. In the solution (from Dobrow's Introduction to Stochastic Processes with R), they let $p_k$ be defined as the probability of reaching \$n with \$k in one's inventory. Then they use the fact that $p_k - p_{k-1} = p_{k-1} - p_{k-2} = ... = p_1 - p_0 = p_1$ . Intuitively this means the probability of reaching \$n with \$k minus the probability of reaching \$n with \$k-1 is equivalent to the probability of reaching \$n with only \$1. Is there an intuitive reason why this is the case?","['stochastic-processes', 'intuition', 'probability']"
3172680,Finding the error in an argument,"If $z=f(x,y)$ and $y=x^2$ , then by the chain rule $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial z}{\partial x}+2x\frac{\partial z}{\partial y}$ Therefore $2x\frac{\partial z}{\partial y}=0$ and $\frac{\partial z}{\partial y}=0$ What is wrong with this argument? I have a feeling that 1.) $x$ and $y$ do not have partial derivatives because they are single-variable, and 2.) $\frac{\partial z}{\partial y}$ cannot be zero, because $y=x^2$ and therefore the derivative of any $y$ term exists. How is my reasoning? I am pretty confused by this question.","['partial-derivative', 'multivariable-calculus', 'calculus']"
3172740,$\ker(\phi)$ is a normal subgroup.,"Let $G_1$ and $G_2$ be groups and suppose $\phi: G_1\mapsto G_2$ is a homomorphism. Then $\ker (\phi)\unlhd G_1$ . Need some feedback and help proving this. I am still new to proofs, but here's my attempt. Proof. We want to show $\ker (\phi)$ is normal, therefore, we must show that for any $h\in \ker (\phi)$ and $g\in G_1$ , then $ghg^{-1}\in \ker (\phi)$ . Since $h\in \ker (\phi)$ , then $\phi(h)=1$ . Thus, \begin{align}
\phi(ghg^{-1})&=\phi(g)\phi(h)\phi(g^{-1})\\
&=\phi(g)\cdot 1\cdot\phi(g^{-1})\\
&=\phi(g)\phi(g^{-1})\\
&=\phi(g\cdot g^{-1})\\
&=\phi(1)\\
&=1.
\end{align} Hence, $ghg^{-1}\in \ker (\phi)$ , which implies $\ker (\phi)$ is a normal subgroup in $G_1$ .","['group-homomorphism', 'proof-explanation', 'abstract-algebra', 'normal-subgroups', 'group-theory']"
3172745,Can we compute the area of a quadrilateral with one right angle when we only know the lengths of any three sides?,"I took an IQ test for fun recently, but I take issue with the answer to one of the questions. Here's the question: My issue is that the explanation assumes angle DC is a right angle. Given that assumption, I can see the quadrilateral is indeed a rectangle and a right triangle and can follow their explanation. However, (from what I remember my high school geometry teacher telling me) even though an angle looks like a right angle, it shouldn't be assumed unless it is explicitly stated or you can prove it. To explain what I mean, if DC isn't a right angle and we exacerbated that difference, it would look like the following: Thus, even being given A, B, C and D it seems like the area could not be calculated. So my question is twofold: Is my criticism valid or am I just being too proud because I got a question wrong? Given my interpretation, DC is not a right angle, can this problem be solved?",['geometry']
3172841,Motivation behind point set topology [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Why should I study point-set topology? What initially interested me in topology was the pop-sci rubber sheet stuff or coffee cup-donut stuff or proving fundamental theorem of algebra using curves but now here I'm stuck between distinguishing between $T2.5$ and $T2.500001$ spaces with loads of weird counterexamples and all that. Also, I don't find any applications of point-set topology mentioned in Munkre's book either except for maybe finding weird counterexamples in real analysis but then honestly I find that to be very boring too (maybe something is wrong with my mathematical interests).","['motivation', 'general-topology', 'soft-question', 'intuition']"
3172943,Number of matrices with each row and column having exactly one 1.,"Consider a square matrix of order $n = 5$ such that $a_{ij} = 0 ~
 \forall ~ i+j = n+1; a_{ij} \in \{0,1\}$ . In each row as well as in each
  column there is only one non zero element. Then number of such
  matrices is? First we note that right diagonal has only $0$ s. Then I tried it this way: We choose a place for one among each row and mark the other places in that column  and same row as forbidden (i.e. no more one's). So for first column  we have 4 choices then 3 choices then 2 then 1 and then 2s. Thus, Number of ways = $4\times 3 \times 2\times 1\times 2 = 48$ But  its erroneous because the number of choices change if we place 1 above 0 in each attempt. What's the correct way to solve this question? Answer given is: 44","['matrices', 'combinations', 'permutations']"
3172971,Pollard's rho algorithm termination,"Pollard's rho algorithm is a simple probabilistic method for factoring a composite number $N$ . There are two parameters one can choose when starting the algorithm: the initial function values $x$ ( $0 \le x \le N-1$ ) and the polynomial (most often just its constant term $b$ , where $1 \le b \le N-3$ ). Even if one tried all of the parameter values within given ranges, could it still happened that the algorithm would not be find any factors?","['number-theory', 'factoring', 'prime-factorization', 'prime-numbers']"
3173011,Implication inside set builder notation,"[ Edited] Suppose a set S is defined by S = { x | x belongs to A --> x belongs to B } ( i. e. S is the set of all x such that  if x belongs to A, then x belongs to B) and that the sets A and B are (1) non empty (2) unequal (3) both different from the universal set . The question I'm asking is : is it possible that (1) S is identical to the null set? if so, in which case? (2) S is identitcal to the universal set? if so, in which case?",['elementary-set-theory']
3173156,Partition problem where partition are in increasing order.,"For given $n$ and $S$ , how many possible combinations are there such that: $x_1 + x_2 + .. + x_n = S $ $\forall i, x_i \leq x_{i+1}$ $\&$ $x_i \geq 1$ For example, if $n$ = 3 and $S$ = 5, there are 2 possible combination: 1 + 1 + 3 1 + 2 + 2","['recursive-algorithms', 'combinatorics', 'dynamic-programming', 'algorithms']"
3173158,"If $I_n=\int_0^{\pi/4}\tan^n x dx$, then evaluate $\lim_{n\to\infty}\sum_{k=0}^nI_kI_{k+1}+I_kI_{k+3}+I_{k+1}I_{k+2}I_{k+3}$","Let $$I_{n}=\int^{\pi/4}_{0}\tan^{n}x\;dx\quad(n=0,1,2,3,4,\ldots)$$ and $$S_{n}=\sum^{n}_{k=0}I_{k}I_{k+1}+I_{k}I_{k+3}+I_{k+1}I_{k+2}I_{k+3}$$ Then find $\displaystyle \lim_{n\rightarrow \infty}S_{n}$ . My Try: $$I_{k}+I_{k+2}=\int^{\pi/4}_{0}\tan^{k}x\cdot \sec^2 x\;dx=\frac{1}{k+1}$$ So $$I_{k}\left(I_{k+1}+I_{k+3}\right)=\frac{I_{k}}{k+2}$$ Could some help me to solve it, Thanks","['limits', 'sequences-and-series']"
3173219,How to derive the upper bound for the expected number and maximal number of marked vertices in a graph?,"Assume we have a graph $G = (V, E)$ with minimal degree $m$ . We mark with a probability (independent from the other vertices) of $p$ each vertex. After that we mark each vertex $v$ iff the vertex $v$ and their neighbors are not marked. So there is a upper bound for the expected value for the number of marked vertices given: $$|V|\left(p + e^{−p(m+1)}\right)$$ And there is also a formula given for the maximal number of marked vertices: $$|V| \frac{1+\ln(m+1)}{m+1}$$ I don't understand how to derive these.","['graph-theory', 'probabilistic-method', 'probability']"
3173234,Computation the shape operator of a graph of function,"Note. My question is based on Lee's Introduction to Riemannian Manifolds, exercise 8-1. Let $f: \mathbb{R}^n \to \mathbb{R}$ be a smooth function. Its graph $$
S := \{ (x, f(x)) : x \in \mathbb{R}^n \}
$$ is a hypersurface in $\mathbb{R}^{n+1}$ . It has a global parametrization $x \mapsto (x, f(x))$ , with the corresponding coordinates $(x^1, \dots, x^{n+1})$ on $S$ . I would like to know the shape operator in these coordinates. My attempt so far: Define $F: \mathbb{R}^{n} \times \mathbb{R} \to \mathbb{R}$ by $F(x, y) := f(x) - y$ is a defining map of $S$ , i.e. $S = F^{-1}(0)$ . The unit normal field on $S$ is given by $$
N = \frac{\text{grad }F}{\lVert \text{grad }F \rVert} \, ,
$$ where $$
\text{grad }F = \sum_{i=1}^n \frac{\partial f}{\partial x^i} \frac{\partial}{\partial x^i} - \frac{\partial}{\partial x^{n+1}} \, ,
$$ with length $$
\lVert \text{grad }F \rVert = \sqrt{\sum_{i=1}^n \left(\frac{\partial f}{\partial x^i}\right)^2 + 1} \, .
$$ Now compute the shape operator: For any vector field $\mathfrak{X}(S) \ni X = \sum_{j=1}^{n+1} X^j \frac{\partial}{\partial x^j}$ $$
\begin{align}
sX &= -\bar{\nabla}_X N \\[5pt]
   &= - \frac{1}{\lVert \text{grad }F \rVert} \left( \sum_{i=1}^n \sum_{j=1}^{n+1} X^j \left( \frac{\partial}{\partial x^j} \frac{\partial f}{\partial x^i} \right) \frac{\partial}{\partial x^i} + \sum_{j=1}^{n+1} X^j \left( \frac{\partial}{\partial x^j} 1 \right) \frac{\partial}{\partial x^{n+1}} \right) \\[5pt]
   &= - \frac{1}{\lVert \text{grad }F \rVert} \sum_{i=1}^n \sum_{j=1}^{n+1} X^j \frac{\partial^2 f}{\partial x^i \partial x^j} \frac{\partial}{\partial x^i}
\end{align}
$$ My questions: First of all, would the above computation be correct? Can I write $\sum_{i=1}^n \frac{\partial f}{\partial x^i} \frac{\partial}{\partial x^i}$ as $\text{grad }f$ ? I'm confused as the coordinate is defined on $\mathbb{R}^{n+1}$ , while $f$ is a function on $\mathbb{R}^n$ , thus $\text{grad }f \in \mathfrak{X}(\mathbb{R}^n)$ . Likewise, if in addition $Y \in \mathfrak{X}(S)$ , can I write $\langle sX, Y \rangle = -\frac{1}{\lVert \text{grad }F \rVert} \text{Hess }f(X, Y)$ ? I'm unsure as $\text{Hess }f$ is a covariant 2-tensor field in $\mathbb{R}^n$ , but $X, Y \in \mathfrak{X}(S) \subseteq \mathfrak{X}(\mathbb{R}^{n+1})$ .","['geometry', 'riemannian-geometry', 'differential-geometry']"
3173237,"What is the meaning of the term ""motivic""?","I'm looking for a simpler explanation of the idea behind motives , and a quick overview of applications. One particular question is the meaning of the term ""motivic"" (which I've seen multiple times applied to different nouns, i.e. ""motivic periods"", ""a [certain theory has a] motivic core"", etc) The Wikipedia article is unfortunately too unclear and convoluted for me to properly understand it. Introductory references will be highly appreciated.","['algebraic-geometry', 'algebraic-topology']"
3173282,Should $d/dx$ ever be used when solving for a partial derivative? Or would all instances of it be replaced with $\partial/\partial x$?,"You hold one of the variables constant and find the derivative of the other when trying to find the partial derivative, so wouldn't something like this be correct?: $\frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{d}{dx}|x|=\frac{|y|x}{|x|}$ Or would this be correct: $\frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{\partial}{\partial x}|x|=\frac{|y|x}{|x|}$ Or are either of those fine? Any help is appreciated.","['notation', 'calculus', 'partial-derivative', 'convention', 'derivatives']"
3173291,Distribution of Dot-Product of Two Independent Multivariate Gaussian Vectors,"Let $X,Y\stackrel{\text{i.i.d.}}{\sim}\mathcal{N}(0,I_d)$ , where $I_d$ is the $d$ -dimensional identity matrix. What is the distribution of $\langle X,Y\rangle=X^TY$ ? Approach 1: So far I know that for any $i\in\{1,...,d\}$ the MGF of $X_iY_i$ is: $$
M_{X_iY_i}(t)=\frac{1}{\sqrt{1-t^2}}.
\qquad\left(=\arcsin'(t)\right)
$$ The derivation of this can be found here . I've verified it - it's correct. Now, using the product-rule the MGF of $\langle X,Y\rangle$ is: $$
\begin{align*}
M_{\langle X,Y\rangle}(t)
&=\left(\frac{1}{\sqrt{1-t^2}}\right)^d
\\
&=(1-t^2)^{-\frac{d}{2}}
\\
&=(1-t^2)^{-\frac{d}{2}}
\\
&=(1-t)^{-\frac{d}{2}}(1+t)^{-\frac{d}{2}}
\end{align*}
$$ Now I don't know how to continue from here. Maybe this product might as the product of two typical MGFs. In that case we could express it as the sum of two typical random variables... I don't know how to continue from here. Approach 2: Let $\theta$ denote the angle between the two random vectors $X$ , $Y$ . Then $$
\begin{align*}
\langle X,Y\rangle
&=
\|X\|\|Y\|\cos(\theta)
\\
&=
\sqrt{\langle X,X \rangle}\sqrt{\langle Y,Y\rangle}\cos(\theta)
\end{align*}
$$ Now, $U:=\langle X,X\rangle\sim \chi^2(d)$ and $V:=\langle Y,Y\rangle\sim\chi^2(d)$ as one can easily verify. $$
\begin{align*}
&=
\sqrt{U\cdot V}\cos(\theta)
\end{align*}
$$ Now, $\cos(\theta)$ is distributed as the dot-product divided by the norms of the two random vectors $X,Y$ . So $\cos(\theta)$ is distributed as the dot-product of the corresponding random unit vectors of $X,Y$ . By rotational invariance of the dot-product, we may rotate both random unit vectors (corresponding to $X,Y$ ), such that one of the unit vectors has only the first coefficient being one. Hence, $\cos(\theta)$ is also distributed as the first-coefficient of a random unit vector. Since the angle between $X$ and $Y$ doesn't affect the outcome of $U$ and $V$ (since they're just the norms of the random vectors; so $\cos(\theta)$ is independent of $U$ and also $V$ ), we can model the randomness of $\cos(\theta)$ independently (as we've said through the first coefficient of a random unit vector). We model this random unit vector through sampling a random multivariate gaussian vector $Z\sim\mathcal{N}(0,I_d)$ that we then normalize. Then we just take its first coefficient. Now, w.l.o.g. we may also use Z:=X. So, $$
W:=
\cos(\theta)\sim\frac{X_1}{\sqrt{\sum_{i=1}^d X_i^2}}.
$$ And finally we have $$
\begin{align*}
\langle X, Y\rangle
&=\sqrt{U\cdot V}\cdot W
\\
&=\sqrt{\langle X,X\rangle}\sqrt{V}\frac{X_1}{\sqrt{\langle X,X \rangle}}
\\
&=\sqrt{V}X_1
\end{align*}
$$ Now, we have the product of the square root of a chi-squared distributed variable and a standard normal random variable. Do you know if that gives any familiar distribution? Right now I'm building the product distribution - let's see what comes out. Approach 3: It's known that the dot product of two random unit vectors in $\mathbb{R^d}$ follows a $Beta((d-1)/2,(d-1)/2)$ distribution (see this post ). So, let $$
Z\sim Beta((d-1)/2,(d-1)/2)
$$ Then we have that $$
\langle X,Y \rangle = Z*\|X\|\|Y\|
$$ Where $\|X\|,\|Y\|\stackrel{\text{i.i.d.}}{\sim}\chi^2(d)$ . So we have that the dot-product is a product of a Beta-, and two Chi^2-distributed variables. That might help us to continue from here by building the product distribution. Sidenote: In the limit $d\to\infty$ we have that it approaches the normal distribution (see here , or Feller II (1971, p. 516, Theorem 2).","['moment-generating-functions', 'probability-distributions', 'probability']"
3173309,Is the notion of circle necessary for proving some problem of angle?,As shown in the image for a plane geometric problem: Could we prove $\angle ACD=\angle ABD$ without using the notion of circle? It could seem easy if we have the notion of circle. But if we have no the notion of circle?,['geometry']
3173320,"There is no function $f:\Bbb R\to \Bbb R$ such that $f(y)\le a f(x)-b\ln{(y-x)},\forall x<y$","Let $a>1,b>0$ . Prove that there is no function $f:\Bbb R\to\Bbb R$ such that $$f(y)\le af(x)-b\ln{(y-x)},\forall x<y$$ Some of my thoughts：Let $y=x+1$ ,then we have $$f(x+1)\le af(x)$$ so we have $$f(x+n)\le af(x+n-1)\le a^2f(x+n-2)\le\cdots \le a^{n}f(x)$$","['functions', 'real-analysis']"
3173336,Find the angle between $AC$ and $IQ$,"Incenter of triangle $ABC$ is point $I$ . Points $M$ and $N$ are respectively middle points of $AB$ and $AC$ . Intersection point of line $CI$ and $NM$ is $P$ . There is such point $Q$ , so that $MN$ and $PQ$ would be perpendicular, and $BI$ with $QN$ parallel to each other. Find the angle between lines $AC$ and $IQ$ . What I did was to write down all the angles that we have - there are some useful conclusions  that got out of this (for example $PN=AN=NC$ , therefore angle $APC$ is right), also since no ratios or angles are given in the problem, I have a slight suspicion that the answer is $90$ degrees.",['geometry']
3173388,Second Order Differential Equation Assistance,"Hi Maths stack exchange!
I’m doing this question for homework, $$y′′+4y′+4y=0$$ I managed to find the auxiliary equation. $$y=(A+Bx)e^{-2x}$$ The issue is when I was looking at the solutions of what to do next, it said this should be the next line. $$y'(x)=Be^{-2x}+(-2)(A+Bx)e^{-2x}=(B-2A+Bx)e^{-2x}$$ I have no Idea how they got answer, I was wondering if anyone could shed some light on how this answer was obtained. Thanks
~Neamus","['derivatives', 'ordinary-differential-equations']"
3173422,Direct proof for differentiability of $\sin(x+y)$,"I've been triying expand the functions like this: $$\lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x+y)-\sin(x_0+y_0)-(\cos(x_0+y_0),\cos(x_0+y_0))\cdot(x-x_0,y-y_0)|}{||(x-x_0,y-y_0)||}\\
\lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x)\cos(y)+\sin(y)\cos(x)-\sin(x_0)\cos(y_0)-\sin(y_0)\cos(x_0)-\left[\cos(x_0)\cos(y_0)-\sin(x_0)\sin(y_0)\right]\left[(x-x_0)+(y-y_0)\right]|}{||(x-x_0,y-y_0)||}$$ But I don't know how to continue, \? Any hint to prove that this limit is actually zero?.","['multivariable-calculus', 'calculus', 'derivatives']"
3173454,Derivative of a function involving a characteristic function.,"Consider the function $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ defined by $$f(x,y,z) = y^2z\chi_{(0,\infty)^3}.$$ I would like to find $$\frac{\partial^3f}{\partial x\,\partial y\,\partial z}.$$ I would have thought that if $x,y,z >0$ then $$\frac{\partial^3f}{\partial x\,\partial y\,\partial z} = \frac{\partial^2}{\partial x\,\partial y}y^2 = \frac{\partial}{\partial x}2y = 0.$$ In the other case the function would be identically $0$ and hence it will have $0$ derivative. Is this correct?","['complex-analysis', 'derivatives', 'characteristic-functions', 'real-analysis']"
3173486,Understanding valutation and residue field,"I recently started studying the theory of schemes from ""The geometry of schemes"", by Eisenbud and Harris.
At the very beginning of this volume I unfortunately found the first osbtacle: pick a commutative ring with unity $R$ , and consider $f \in R$ as a function on Spec $R$ . The authors define $f(\mathfrak{p})$ as the image of $f$ via the canonical maps $$R \rightarrow R/\mathfrak{p}\rightarrow k(\mathfrak{p}) $$ with $k(\mathfrak{p})$ the residue field of $R/\mathfrak{p}$ . If I'm not wrong, $k(\mathfrak{p})=((A/\mathfrak{p})\setminus \{[0]\})^{-1}A/\mathfrak{p}$ , which should be equal to $k(\mathfrak{p}) \simeq A_{\mathfrak{p}}/\mathfrak{p}A_{\mathfrak{p}}$ (but I can't figure out how, all this localisations confuse me). The problem is that I'm getting confused with all this stuff, I mean, I always naively consider the function $15$ at $(7)$ , then $15((7))=1$ mod $7$ , or working over $k[x]$ , $$x^3-1((x^2+1))= -x-1 (\text{ mod } (x^2+1)).$$ But I don't see all this machinery behind this computation, I can do the calculation but I feel I don't undertand what $A_{\mathfrak{p}}/\mathfrak{p}A_{\mathfrak{p}}$ is and why I'm working in this exact space. Any help which clarifies the theoretic aspect of this calulations would be appreciate, I'm quite confused about this ring. Thanks in advance.","['field-theory', 'algebraic-geometry', 'commutative-algebra']"
3173512,Does every commutative idempotent semigroup have a representation as a union-closed family of sets,"Consider a finite semigroup $S$ whose semigroup operation $\times$ is commutative and whose elements are idempotent. Does there exist a finite union-closed family of finite sets $\mathcal{M}$ such that there is a bijection $\tau: S \rightarrow \mathcal{M}$ where $\forall a,b \in S, \tau(a \times b) = \tau(a) \cup \tau(b) $ ? I feel that this may not be the case necessarily since I haven't encoded any information about ""subsets,containment..."" into the axioms i've stipulated on the semi group, but from a purely algebraic standpoint it would seem that this characterizes the union very well.","['abstract-algebra', 'combinatorics', 'semigroups']"
3173523,Tiling a square with rectangles whose sides are powers of two,"Consider the set of all the rectangles with dimensions $2^a\times 2^b\,a,b\in \mathbb{Z}^{\ge 0}$ . We want to tile an $n\times n$ square with rectangles from this set (you can use a rectangle several times). What is the minimum number of rectangles we need? If $f(n)$ is the sum of digits of $n$ in base $2$ , I think we need at most $f(n)^2$ rectangles. I have an example for this number: write $n=2^{a_1}+2^{a_2}+...2^{a_{f(n)}}$ and split each side into segments with lengths $2^{a_1},2^{a_2},...,2^{a_{f(n)}}$ and consider the $f(n)^2$ rectangles obtained this way. On the other hand, you need at least $f(n)$ rectangles to tile a row (or column) so I think you need $f(n)^2$ rectangles, but I can't prove it. Any ideas?","['contest-math', 'combinatorics', 'tiling', 'discrete-mathematics']"
3173596,Positive semi-definite (psd) matrices associated with a family of partially recovering subsets of a given finite set,"This question is motivated by this one : Show a specially defined matrix is positive definite . Let us take it from the beginning. Let $S=\{s_1,s_2,\cdots s_N\}$ be a finite set. Let $E_k \ (k=1\cdots n)$ be a family of $n$ subsets of $S$ with any of the 3 cases $n<N, n=N, n>N$ . Let us consider the classical coding of a subset $S$ by  a sequence $s \in \{0,1\}^N$ with $$s(k)=1 \iff s_k \in S.$$ Let us first define matrix $C$ (like ""Common"") by its entries : $$C_{ij}=|E_i \cap E_j|$$ (where |.| means ""number of elements""). Proposition: : $C$ is psd (positive semi-definite). Proof : $C=EE^T$ where the $i$ th row of $E$ is the code of subset $E_i$ (as described above). Let us now define a kind of normalized version of matrix $C$ : $$A_{ij}=\frac{|E_i \cap E_j|}{|E_i \cup E_j|}$$ Proposition : $A$ is psd. Proof: see the nice answer by @kimchilover to the question mentionned at the beginning. Actually, $A_{ij}$ has been given a name : it is called the ""Jaccard index of ressemblence"" between sets $E_i$ and $E_j$ ( https://en.wikipedia.org/wiki/Jaccard_index ); its computation is mentionned in ( Jaccard index, matrix notation ). Matrix $A$ , like matrix $B$ , has also a certain number of properties. Let us now introduce a third matrix $B$ with entries : $$B_{ij}=\frac{1}{|E_i \cup E_j|}$$ A conjecture, based on numerical tests, is that $B$ is psd like $A$ and $B$ . See as well the interesting and dense answer by @darij grinberg. Questions : a) Has somebody valuable references about matrices $A, B$ and $C$ ? In particular about their spectrum, the fact that there is usually a very dominant eigenvalue with a certain interpretation of the dominant eigenvector, etc... ? b) Are there connections with correlation matrices ? c) Are there connections with some associated matrix graphs ? Remark: Gower's distance is somewhat related. The original article of Gower ""A general coefficient of similarity and some of its properties"" J.C. Gower, Biometrics, 27, 857-74, Dec. 1971, can be found here . Edit 1: A connection with neural networks . Edit 2: ""Jaccard index"" has another name: ""Tanimoto index"", especially in chemistry applications. Edit 3: (2023/01/31) In the case $E_k = [1,k]$ (closed integer interval), matrix $A$ is a Lehmer matrix","['positive-semidefinite', 'eigenvalues-eigenvectors', 'correlation', 'matrices', 'elementary-set-theory']"
3173608,Conditional law of total probability,"Let $F_1, \dots, F_n$ be a partition of $\Omega$ the sample space. Let $E$ be an event. 
The law of total probability tells us: \begin{align}
P(E) = \sum_{i=1}^n P(E\vert F_i) P(F_i)
\end{align} Let's take now an other arbitrary event $B$ , intuitively conditioning on this new event, we should have: \begin{align}
P(E\vert B) = \sum_{i=1}^n P(E\vert F_i, B) P(F_i \vert B)
\end{align} Here is my proof: On the left hand side we have: $$ P(E\vert B) = \frac{P(E \cap B)}{P(B)}$$ On the right hand side we have: \begin{align}
\sum_{i=1}^n P(E\vert F_i, B) P(F_i \vert B) &= \sum_{i=1}^n \frac{P(E\cap F_i \cap B) }{P(F_i \cap B)} \frac{P(F_i \cap B)}{ P(B)} \\
&= \sum_{i=1}^n \frac{P((E\cap B) \cap F_i)}{P(B)} \\
&= \frac{1}{P(B) }\sum_{i=1}^n P((E\cap B) \cap F_i)
\\ &= \frac{P(E\cap B)}{P(B)} \qquad \text{by the law of total probability}
\end{align} Is my demonstration correct?","['conditional-probability', 'self-learning', 'probability']"
3173638,Eigenvalue of rank 2 matrix,"Suppose $x$ and $y$ are two linearly independent nonzero vectors in $\mathbb{R}^n$ . Then we know that the matrix $M = xy^T + yx^T$ is a rank 2 matrix. I seem to have made the observation that the two nontrivial eigenvalues of $M$ are given by $\lambda_{1,2} = x^Ty \pm \|x\|\|y\|$ . For example if $x = \begin{bmatrix}a & b\end{bmatrix}^T$ and $x = \begin{bmatrix}c & d\end{bmatrix}^T$ , then the characteristic polynomial is given by $${{s}^{2}}-\left( 2{{x}^{T}}y \right)\cdot s-{{\left( bc-ad \right)}^{2}}$$ and application of the quadratic formula verifies that discriminant part (under the square root) is given by: $4\left( {{a}^{2}}+{{b}^{2}} \right)\left( {{c}^{2}}+{{d}^{2}} \right)$ so the observation holds. I have tried some numeric computation for $n=3,4,5$ and the formula seems to work. But I haven't  been able to prove it. Approach one: Write, \begin{align}
  \det \left( M-\lambda I \right) &=\det \left( x{{y}^{T}}+\underbrace{\left( -y{{x}^{T}}-\lambda I \right)}_{:=A} \right) \\ 
 & =-\left( 1+{{y}^{T}}{{\left( -y{{x}^{T}}-\lambda I \right)}^{-1}}x \right)\det \left( y{{x}^{T}}+\lambda I \right) \\ 
\end{align} and try to use the lemma but that didn't take me anywhere. Approach two: \begin{align}
  & Mv=x{{y}^{T}}v+y{{x}^{T}}v=\left( {{x}^{T}}y+\left\| x \right\|\left\| y \right\| \right)v \\ 
 & \Rightarrow x{{y}^{T}}v+y{{x}^{T}}v={{x}^{T}}yv+v\sqrt{{{x}^{T}}x{{y}^{T}}y} \\ 
\end{align} and try to match the left and right hand side, but I couldn't get that to work either. Can someone provide a hint or proof? Also, how would one go about deriving what the eigenvectors corresponding to the two eigenvalues look like?","['linear-algebra', 'eigenvalues-eigenvectors']"
3173643,"how many persons need to be in one room , so AT LEAST 3 people have the same Birth MONTH?","how many persons need to be in one room , so AT LEAST 3 people have the same Birth MONTH ? Another Answer i suggest : (correct me if i'm wrong) there should be 15 People at least , because if there's more people than months per year , then using Pigeonhole Principle at least two people are born in the same month.","['birthday', 'discrete-mathematics']"
3173652,The proof of general Leibniz rule: didn't quite understand the combinatorics of it.,"Few questions regarding this proof: 1) As I understand the reason for the third line is to make the order of derivative for each function equal. The bit I don't understand is why k=1 under the sum symbol? 
Why after the sum symbol $k$ changes to $k-1$ ? 2) On the fourth line highlighted n has changed from $n+1$ because we need to make the sums equal?  And if it had stayed the same how the last member highlighted in yellow would look like ( I know that it wouldn't be convenient and useful, just want to understand the reasoning). 3) Why does derivatives of k=0,n disappear on the last line?","['derivatives', 'proof-explanation', 'combinatorics']"
3173654,Pretty $p^2$-congruences involving Stirling numbers of the both kinds,"Let $p$ an odd prime number and ${n\brack {k}}$ (resp. ${n\brace k}$ ) be the Stirling numbers of first (resp. second) kind, such that: $$ \sum_{k\ge0} {{n}\brack {k}}x^k = \prod_{j=0}^{n-1}(x+j)$$ $$ \sum_{k\ge0} {{n}\brace {k}}\prod_{j=0}^{k-1}(x-j) = x^n$$ Let $m$ an integer such that $1\le m\le p$ , we have \begin{align*} {{p+m}\brack {p}}+{{p}\brace {p-m}}&\equiv 0 \pmod
 {p^2}\\ {{p+m}\brace {p}}+{{p}\brack {p-m}}&\equiv 0 \pmod {p^2}
 \end{align*} This is probably not new. Where can one find a reference? EDIT From @i707107 answer, for odd $m$ in the range $3\le m \le p-2 $ , we also have the nice \begin{align*}{{p+m}\brace {p}}&\equiv {{p}\brack {p-m}} \pmod {p^3}
 \end{align*}","['number-theory', 'elementary-number-theory', 'reference-request', 'combinatorics', 'stirling-numbers']"
3173719,Finding null space and image,"Let $T(f(x)) = g(x) = \int_{-1}^{1}f(t)(t-x)^2dt$ be a linear transformation from $V$ to $V$ where $V$ is the vector space of continuous functions on $[-1 , 1]$ . Find $\text{Nul}(T)$ and $\text{Im}(T)$ . I really have no idea about the solution . Can $x$ be considered constant in the integral ? And then how to evaluate integral ?","['integration', 'vector-spaces', 'functions', 'linear-algebra', 'linear-transformations']"
3173793,"What is the probability that $A,B,C$ are independent","My lecturer posed a question in class which I have tried to solve with no success, any help would be much appreciated. Let $\Omega = {1,2,3,...,n}$ be the sample space.
Choose 3 events randomly: A,B,C such that $A,B,C \subseteq P(\Omega)$ where $P(\Omega)$ denotes the power set. What is the probability that A,B and B,C and A,C are all independent? We are taking the uniform distribution on $\Omega$ and on $P(\Omega)$","['statistics', 'probability']"
3173817,Alternating functions and differential forms,"Can someone help me? Any sugestion will be helpful. According to Courant and John in Introduction to Calculus and Analysis, Vol. 2 chapter 5 section 5.1, if we take: \begin{equation}
L = f(x,y)dy - g(x,y)dx
\end{equation} the differential form of L will be: \begin{equation}
dL = dfdy - dgdx
\end{equation} \begin{equation}
dL = (f_xdx +f_ydy)dy - (g_xdx + g_ydy)dx
\end{equation} \begin{equation}
dL = f_xdxdy + f_ydydy - g_xdxdx - g_ydydx
\end{equation} By alternating differential forms we know that $dxdy = -dydx$ and $ dxdx = dydy = 0$ . So, the answer will be: \begin{equation}
dL = (f_x + g_y)dxdy
\end{equation} I know that alternating functions have an important role in the calculus of determinants, as well in vectorial product. But, I do not understand the relation here, I tried to reach the answer performing a determinant, and the best I did was this: \begin{equation}
\begin{vmatrix} f_xdx & dx \\ g_xdx & dy \end{vmatrix} + \begin{vmatrix} f_ydy & dx \\ g_ydy & dy \end{vmatrix}
\end{equation} But it makes no sense to me. So, why to use differential alternating form ? Why it is applicable in this situation ? This doubt has been tormenting me for a long time.","['calculus-of-variations', 'multivariable-calculus', 'linear-algebra', 'vector-analysis', 'differential-forms']"
3173849,Trouble understanding converse of Tarski's theorem,"I am working through a paper entitled ""Elimination of quantifiers in algebraic structures,"" and am having a lot of trouble understanding the first theorem. The paper is available here and attempts to prove a partial converse of one of Tarski's theorems ( here is the particular theorem they prove the converse of). Namely, they want to show that if $K$ is an infinite field, whose $\mathcal{L}$ -theory admits quantifier elimination (QE), where $\mathcal{L}=\{=,0,1,+,-,\cdot\}$ (i.e. the language of rings with identity), then $K$ is algebraically closed. They attempt to do this by contradiction. From here on out I will copy out bits of the proof and walkthrough what is clear, and what is not. We argue by contradiction. Let $K$ have an algebraic extension $K(\alpha)$ of degree $n>1$ . I get the degree has to be $n>1$ otherwise, $\alpha\in K$ such that $K(\alpha)=K$ , contrary to assumption. Let $f(y,\bar{x})=y^n+x_{n-1}y^{n-1}+...+x_0$ . Then there is a quantifier free formula $\phi(\bar{x})$ which we may take as a disjunction of Type A (this is explained in Lemma 1 in the paper), which is equivalent in $K$ to $\forall y\,(f(y,\bar{x})\neq0)$ . As I understand this, I think they are saying that $f$ is the minimal polynomial for $\alpha$ . So clearly, $y\in K$ cannot be a root of it. Otherwise, it would not be minimal. Clearly since $K\vDash\forall y\,(f(y,\bar{x}))$ Now, the paper goes on to assume that $\alpha$ is seperable over $K$ . I'm not entirely sure why? Does the proof become obvious if $\alpha$ is not seperable? After this part, I get very confused about what the authors are trying to do. Any help clarifying how I should read this proof would be appreciated. I'm especially having a lot of trouble seeing the relationships between $f(y,\bar{x})$ and the big $F$ polynomial. I think the crucial part of the proof I don't understand is these couple of sentences somewhere in the middle: Now $\{F_j(\bar{z})\}$ is algebraically independent over $K$ and $K$ is infinite. Thus, there is no $K$ -Zariski closed proper subset $X$ of $K^n$ such that $X\supset\{\langle F_0(\bar{K},...,F_{n-1}(\bar{k}))\rangle|\text{$\bar{k}\in K^n$, $k_i\neq 0$ for some $i>1$}\}$ .","['field-theory', 'model-theory', 'abstract-algebra', 'logic']"
3173866,How to prove eigenvectors of a matrix exponential are the same as those for the matrix?,"It's relatively easy to show that any eigenvector of an arbitrary $n \times n$ matrix $A$ is also an eigenvector of it's matrix exponential, $B = e^A$ . But how does one show the reverse is true: that any eigenvector of $B = e^A$ is also an eigenvector of $A$ ?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3173916,Is sum of two sequences of independent i.i.d random variables independent?,"I am looking for a proof that if I have two sequences of i.i.d random variable that are independent $$ (X)^\infty_{n=1}  , (Y)^{\infty}_{n=1}  $$ then its' sum $ (X+Y)_{k} $ is also independent from $ (X+Y)_{m} $ for $ \forall _{m,l} $ I want to use it in a proof that sum of two independent Poisson processes has independent increments and I guess this theorem is general, am I right ?","['stochastic-processes', 'probability-theory', 'poisson-process']"
3173934,A formula to find $ \lim_{s \to 1} \frac{1}{\zeta(s)} (\sum_{x_i=1}^\infty \dots \sum_{x_1 =1}^\infty)\frac{1}{ (\sum_{k=1}^i (x_k)^i)^s}$?,Question Using a conjectured formula of mine I believe the following relation to be true: $$ (\int_0^\infty e^{-x^\lambda} dx)^\lambda = \lim_{s \to 1} \frac{1}{\zeta(s)} (\sum_{x_\lambda=1}^\infty \dots\sum_{x_2 =1}^\infty \sum_{x_1 =1}^\infty)\frac{1}{ (\sum_{k=1}^\lambda  (x_k)^\lambda)^s} $$ where $\lambda$ is any positive integer $\geq 1$ and $\zeta(s)$ is the zeta function. Can someone prove/disprove(or find a counter-example) this formula? Background It's derivation using the conjecture is quite similar to: What is the limit of this Dirichlet series?,"['dirichlet-series', 'number-theory', 'sequences-and-series']"
3173980,Sequences of $0$ and $1$,"How many is sequences with length $4n$ which contain only $0$ and $1$ . $0$ occurs $2n$ times and $1$ occurs $2n$ times. Moreover number of occurs $0$ before $n$ 'th occur of $1$ can't be bigger than $n$ My attempt I thought that I can use there generating functions. So let assume that we have put our $""1""$ and now we want to put $0$ between them. $$[x^{2n}] (1+x)(1+x+x^2)\cdot...\cdot(1+x+x^2+...+x^{2n})$$ Each fraction represents how many zeros we can put before $i'th$ $1$ . $$[x^{2n}] \frac{(1-x^2)(1-x^4)...(1-x^{2n})}{(1-x)^{2n}}$$ $$[x^{2n}] (1-x^2)(1-x^4)...(1-x^{2n}) \cdot \sum_{0\le k}\binom{k+2n-1}{2n-1}x^k$$ But I don't know how I can find $[x^{2n}]$ there","['combinatorics', 'discrete-mathematics', 'generating-functions']"
3174033,Solve $\int \frac{e^{x}(2-x^2)}{(1-x)\sqrt{1-x^2}}\mathrm dx$,"Find $$\int \frac{e^{x}(2-x^2)}{(1-x)\sqrt{1-x^2}}\mathrm dx$$ Looking at the numerator, combined with the surd, you can get $$\int\frac{e^x(1+x)\left(\frac{1}{\sqrt{1-x^2}}+\sqrt{1-x^2}\right)}{1-x^2}\mathrm dx$$ Then this begins to look like the quotient rule, since the denominator is $\sqrt{1-x^2}^2$ , and in the numerator, $1/\sqrt{1-x^2}$ is almost like the derivative of the square root. However, it isn't quite that - there are extra terms. $$\color{red}{e^x\left(\sqrt{1-x^2}+\frac{x}{\sqrt{1-x^2}}\right)}+\frac{e^x}{\sqrt{1-x^2}}+e^x x\sqrt{1-x^2}$$ The red terms are accounted for by quotient rule (giving an integral of $\frac{e^x}{\sqrt{1-x^2}}$ ). But then what do we do with the remaining terms?","['integration', 'calculus']"
3174046,The Dimension of Vector Space,"Question:
Let $A$ be an $n×n$ complex matrix with $n$ distinct eigenvalues. Let $V$ be the set of all $n×n$ complex matrices $B$ that commute with $A$ . Prove that V is a vector space and find its dimension (Justify your answer). My Answer: I know how to show that V is a vector space, but I don't know how to find its dimension.
I tried showing that if v is an eigenvector corresponding to some eigenvalue, so is Bv, and got that for all B, Bv = kv for some scalar v. But I'm not sure if this helps.","['abstract-algebra', 'linear-algebra']"
3174067,Integral involving matrix exponential,"Is there any way to simplify the integral $$
I = \int_{t_1}^{t_2}e^{\Lambda t} A e^{\Lambda t}\,dt
$$ knowing that A is symmetric and Λ is a diagonal matrix?","['integration', 'matrix-exponential', 'matrices', 'symmetric-matrices']"
3174105,Evaluating limits of nested functions,"I am interested in how we can approximate limits of functions of the form: $$g(x)=f\circ f\circ f\circ f\circ f\circ ...(x)$$ If we take the function $f(x)=\sin(x)$ for example we get: $$g(x)=\sin(\sin(\sin(\sin...(x))))$$ as far as I can tell, for $x\in\Re$ the function $g(x)\to0$ How could I prove this? My only thought so far is that $|\sin(x)|\le1$ and so $|\sin(\sin(x))|\le\sin(1)<1$ Can I continue this to prove the limit?","['limits', 'calculus']"
3174112,Domain of $\sqrt{\sin(\cos x)}$,How we can find the Domain of $\sqrt {\sin(\cos x)}$ ? My attempt:- $$ \sin(\cos x)>0 \implies\cos x>0 \implies x>π/2$$ But it is right or wrong ?,"['algebra-precalculus', 'functions']"
3174166,"Topology ""generated by"" normal subgroups (Topological groups).","Let $G$ be a group and $L$ be a non-empty family of normal subgroups such that if $K_{1},K_{2} \in L$ and $K_{3}$ is a normal subgroup containing $K_{1} \cap K_{2}$ then $K_{3} \in L$ . Let $T$ be a family of unions of sets of cosets $Kg$ with $K \in L,g \in G$ . Show that $T$ is a topology in $G$ and that $G$ is a topological group with respect to this topology. Show that $L$ is the set of open normal subgroups of $G$ with respect to this topology. This is the first problem of Wilson's book ""Profinite Groups"". I'm having trouble with this question. Using YCor's comment, I was able to prove that $T$ is a topology with basis: $$B = \left\{C_{I,J} \mid C_{I,J} = \{K_{i}g_{j}\}_{i \in I, j \in J}\right\}.$$ Now, to prove that $G$ is topological group, I need to show that the maps $\varphi: G \times G \to G$ given by $\varphi(x,y) = xy$ and $\psi: G \to G$ given by $\psi(x) = x^{-1}$ are continuous. I'm still not familiar with topological groups (this is the first problem of the book), so I'm having a bit of trouble time with this question. So, I need to show that if $U$ is open in $G$ , $\varphi^{-1}(U)$ is open in $G \times G$ . But, is enough consider $U \in B$ . I worked with a simple subset of $B$ : $\{Kg\}$ for some $K \in L$ and $g \in G$ . The sets $$Kg \times \{1\}1$$ $$K1\times \{1\}g$$ $$Kg^{-n}\times \{1\}g^{n+1}$$ $$Kg^{n+1} \times \{1\}g^{-n}$$ and the ""symmetrical"" products are maps in $\{Kg\}$ and the union of this sets is in $T$ . If I'm not completely wrong, I think that the general case is similar, but I dont know how to formalize. Also, for the last part, Im having troubles to show that the set of open normal sets of $G$ is cointained in $L$ . I would like a hint for it.","['general-topology', 'topological-groups', 'group-theory']"
3174202,One-dimensional representations are trivial on proper normal subgroups whose quotient is cyclic,"In the book by Fulton and Harris they make the following claim: All one-dimensional representations of a finite group $G$ are trivial on proper normal subgroups whose quotient group is cyclic. My only idea is the following: Let $(\rho,V)$ be a representation of $G$ . Then, $\rho$ is trivial on $H$ (a normal subgroup) if and only if $\rho$ factors as $\bar{\rho}$ through the quotient $G/H$ . Since $G/H$ is cyclic it is enough to define $\bar{\rho}$ in the generator $gH$ . Let $\bar{\rho}(gH) = \rho(g)$ . However, I'm not able to prove that this is well defined. I know that somehow I have to use the fact that $\rho$ is one-dimensional, that is, its image is abelian. PS: This is used to prove that $S_5$ has only two non-trivial one-dimensional representations. However, in this specific case $A_5$ is not only a 'normal subgroup whose quotient is cyclic', it is the derived subgroup of $S_5$ . Therefore, every group homomorphism with abelian image is trivial on $A_5$ . The claim follows from the fact that a one-dimensional representation has abelian image. I don't know where the 'cyclic' hypothesis part enters.","['group-theory', 'abstract-algebra', 'representation-theory']"
3174227,Why do I get two different answers for this counting problem?,Suppose that we consider English alphabets so we have 26 letters which 5 of them are vowels. I want to find all three letters sequences that contain at least one vowel. My first approach is $26^3-21^3=8315$ which is the number of all three lettres sequences minus the number of three letter sequences which do not contain vowels. Second approach: at least one vowel means one vowel or two vowels or three vowels so the answer is $(5\cdot21^2)+(5^2\cdot21)+5^3=2855$ . Why are these two answers different?,"['combinatorics', 'discrete-mathematics']"
3174235,Find extrema of $\cos\left(\frac\pi2\cos x\right)+\cos\left(\frac\pi2\sin x\right)$ without differentiation,"Question is: find minimum and maximum of $f(x)$ : $$f(x)=\cos\left(\frac\pi2\cos x\right)+\cos\left(\frac\pi2\sin x\right)$$ without differentiation. This problem is supposed to be solved only with pre-calculus knowledge, but I have no idea how to do it. $f(x)$ decreases monotonically from $\frac{n\pi}2$ to $\frac{n\pi}2+\frac\pi4$ , and monotonically increases from $\frac{n\pi}2+\frac\pi4$ to $\frac{(n+1)\pi}2$ , but how can it be proved the monotonicity without calculus? I also tried to transform the expression into \begin{align}f(x)=&2\cos\left(\frac\pi4(\cos x+\sin x)\right)\cos\left(\frac\pi4(\cos x-\sin x)\right)\\=&2\cos\left(\frac{\sqrt2\pi}4\sin \left(x+\frac\pi4\right)\right)\cos\left(\frac{\sqrt2\pi}4\sin\left(-x+\frac\pi4\right)\right)\end{align} but found it has the same issue of proving the monotonicity.","['calculus', 'algebra-precalculus']"
3174277,How to simplify the the binomial coefficient in the binomial series?,"Use binomial series to expand the function $\frac{5}{(6+x)^3}$ as a power series. I understand the process to get the following summation: $\frac{5}{6^3}\sum_{n=0}^{\infty} {-3 \choose n} (\frac{x}{6})^n $ However, I am stuck on seeing what's going on with ${-3 \choose n}$ . From the Stewart Calculus textbook, it says that ${k \choose n} = \frac{k(k-1)(k-2)...(k-n+1)}{n!}$ . By applying that, I would get: ${-3 \choose n}=\frac {(-3)(-4)(-5)...[-(n+2)]}{n!}$ . I think the next step would be to extract out the negative, so I will get $(-1)^n$ . The summation would then be: $\frac{5}{6^3}\sum_{n=0}^{\infty} {\frac {(-1)^n(3)(4)(5)...[(n+2)]}{n!}} (\frac{x}{6})^n $ The solution to this problem is $\frac{5}{2}\sum_{n=0}^{\infty} {\frac {(-1)^n(n+1)(n+2)x^n}{6^{n+3}}}$ I am not sure what has happened to the factorial. Was it cancelled out due to the (3)(4)(5)... in the numerator? Where did (n+1) come from?","['power-series', 'multivariable-calculus']"
