question_id,title,body,tags
4744663,Law of Large Numbers: How Much More Powerful is Weak Law vs Strong Law?,"In mathematics, we often learn the following statements: Convergence in Probability implies Convergence in Distribution Strong Law of Large Numbers implies Weak Law of Large Numbers I am trying to create mathematical examples to compare the strength (i.e. implication) for both of these statements. For example - perhaps we can show simulate some random data and show that for the same data:  Strong Law of Large Numbers requires fewer samples to achieve the same results as the Weak Law of Large Numbers?  (I am not sure if this is possible or would even serve to demonstrate and compare the strength of Weak Law vs Strong Law) Maybe we can simulate some random data and show that for the same data: Convergence in Probability requires fewer samples to achieve the same results as Convergence in Distribution? (I am not sure if this is possible or would even serve to demonstrate and compare the strength of Convergence in Probability vs Convergence in Distribution) For example - here is an R simulation that I think might be able to show the Law of Large Numbers (I am not sure if Strong Law or Weak Law): The sample average becomes closer and closer to the population average as the size of the sample increases: set.seed(123)
n <- 1000
sample_means <- numeric(n)
for (i in 1:n) {
  x <- rnorm(i, mean = 0, sd = 1)
  sample_means[i] <- mean(x)
}
plot(sample_means, type = ""l"", main = ""Law of Large Numbers: Convergence of Sample Mean to Population Mean"", xlab = ""Sample Size"", ylab = ""Sample Mean"")
abline(h = 0, col = ""red"") Is it somehow possible to ""overlay"" a second example on this plot and compare an example for the other Law of Large Numbers - and thus compare the strengths of both laws? Can something similar be done for Convergence in Probability vs Convergence in Distribution? Thanks! References: Creating A Mathematical Example for Convergence in Distribution","['convergence-divergence', 'law-of-large-numbers', 'probability']"
4744665,Is the completion of a measure space the same as the Hahn-Kolmogorov extension?,"Let $(X,\mathcal{S},\mu)$ be a measure space.
I know two ways of extending it to a complete measure space: Forming the ""completion"" $(X,\hat{\mathcal{S}},\hat{\mu})$ , where $\hat{\mathcal{S}}$ is the collection of sets of the form $E\cup Z$ , where $E\in\mathcal{S}$ and $Z$ is $\mu$ -negligible, and $$\hat{\mu}(E\cup Z)\mathrel{\mathop{:}}=\mu(E).$$ Hahn-Kologorov extension: Extend $\mu$ to an outer measure $\mu^\ast$ , and restrict it to the set $\mathcal{M}(\mu^\ast)$ of $\mu^\ast$ -measurable sets, forming the complete measure space $(X,\mathcal{M}(\mu^\ast),\mu^\ast|_{\mathcal{M}(\mu^\ast)})$ . The completion is the smallest complete measure space containing $(X,\mathcal{S},\mu)$ , so I know that $$(X,\hat{\mathcal{S}},\hat{\mu})\subseteq(X,\mathcal{M}(\mu^\ast),\mu^\ast|_{\mathcal{M}(\mu^\ast)}).$$ My question is whether this inclusion can be a proper one. I know that the completion and Hahn-Kolmogorov extension of the Borel measure space both give the Lebesgue measure space.","['measure-theory', 'outer-measure']"
4744714,What do we mean by $\bigcup_{i\in I}^{} T_i$ where $T_i$ are sets?,What do we mean by $\bigcup_{i\in I}^{} T_i$ where $T_i$ are sets? Actually my question is more about what do we mean by index set $I$ ? Is it like sequence? To see more clear it wouldn't hurt to take $I$ as $\mathbb N$ so $\bigcup_{i\in \mathbb N}^{} T_i$ would be $ T_1 \cup T_2 \cup T_3 \dots$ However I couldn't imagine other cases different from $\mathbb N$ . Could you help me to figure out this?,"['elementary-set-theory', 'definition']"
4744754,"If $a,b,c\ge 0: ab+bc+ca=3,$ prove $\sqrt{\frac{a}{a+b+6}}+\sqrt{\frac{b}{c+b+6}}+\sqrt{\frac{c}{a+c+6}}\le \frac{3\sqrt{2}}{4}.$","Problem. If $a,b,c\ge 0$ and $ab+bc+ca=3,$ prove that $$\sqrt{\frac{a}{a+b+6}}+\sqrt{\frac{b}{c+b+6}}+\sqrt{\frac{c}{a+c+6}}\le \frac{3\sqrt{2}}{4}.$$ I tried to use Cauchy-Schwarz inequality but this way leads to reverse inequality. Indeed, $$\sum_{cyc}\sqrt{\frac{a}{a+b+6}}=\sum_{cyc}(\sqrt{a}\cdot\sqrt{\frac{1}{a+b+6}})\le (a+b+c)\sum_{cyc}\frac{1}{a+b+6}.$$ It implies to prove $$\frac{1}{a+b+6}+\frac{1}{c+b+6}+\frac{1}{a+c+6}\le \frac{9}{8(a+b+c)}.$$ But by Cauchy-Schwarz again, $$\frac{1}{a+b+6}+\frac{1}{c+b+6}+\frac{1}{a+c+6}\ge\frac{9}{2(a+b+c)+18} \ge \frac{9}{8(a+b+c)}.$$ I hope to see some ideas. Thank you for your interest.","['contest-math', 'multivariable-calculus', 'algebra-precalculus', 'inequality']"
4744774,"If the magnitude of a root of an irreducible polynomial can be expressed via radicals, can the root?","Let $f$ be an irreducible polynomial with coefficients in $\mathbb{Q}$ , with roots $c_1,...,c_n \in \mathbb{C}$ . I  have two related but slightly different questions: If $|c_1|$ lies in a radical extension of $\mathbb{Q}$ , does that imply that $c_1$ lies in a radical extension of $\mathbb{Q}$ ? If $|c_1|,...,|c_n|$ all lie in a radical extension of $\mathbb{Q}$ , does that imply that $c_1,...,c_n$ all lie in a radical extension of $\mathbb{Q}$ ? Obviously if the answer to the first question is yes, then so is the answer to the second question. However since I'm not sure of how to tackle either one, can someone with more expertise point me in the right direction?","['galois-theory', 'radicals', 'abstract-algebra']"
4744790,How does $\sqrt[6]{26+15\sqrt3}-\sqrt[6]{26-15\sqrt3}$ become $\sqrt2$?,"How can I simplify this? $$\sqrt[6]{26+15\sqrt3}-\sqrt[6]{26-15\sqrt3}$$ I know the answer is $\sqrt2$ , but don't know where to start. Thx.","['nested-radicals', 'algebra-precalculus']"
4744791,"Solving $\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx}$ via residues","I was trying to evaluate $\int_{0}^{\infty}{\frac{\cos(tx^n)}{x^n+a}\, dx}$ , with a semicircle in the upper half plane we have : $$\oint{\frac{e^{itz^n}}{z^n+a}\, dz}=\int_{-\infty}^{\infty}{\frac{e^{itx^n}}{x^n+a}\, dx}+\int_{\Gamma}{f(z)\, dz=2\pi i\sum{\mathrm{Res}f(z)}}$$ I think $\int_{\Gamma}\to0$ as the radius goes to infinity. Then the residue when $z^n+a=0$ : $$\begin{align}\lim_{z\to a^{1/n}e^{\frac{i\pi}{n}(2k+1)}}{(z-a^{1/n}e^{\frac{i\pi}{n}(2k+1)})}\frac{e^{itz^n}}{z^n+a}=\frac{e^{itae^{i\pi(2k+1)}}}{n(a^{1/n}e^{\frac{i\pi}{n}(2k+1)})^{n-1}}=-a^{1/n-1}n^{-1}e^{-ita}e^{\frac{i\pi}{n}(2k+1)}\end{align}$$ But after summing, it doesn't seem like this arrives at the answer given here . Specifically the $\csc(\frac{\pi}{n})$ . EDIT : Since part of the numerator of $\sum_{k=0}^{n-1}{e^{\frac{i\pi}{n}(2k+1)}}$ is $1-e^{2\pi i}=0$","['integration', 'complex-analysis', 'contour-integration', 'residue-calculus']"
4744814,Prove $A\cap (X \setminus A) =\emptyset$ and $(A\cap B)\cap C=A\cap (B\cap C)$,"Where $A$ , $B$ , $C$ are sets and $X$ is a set containing $A$ as a subset. Both of these are from Proposition $3.1.27$ on Tao's Analysis I. These are the only proofs where I feel like I might've made a mistake in my logic because I'm rather inexperienced. For the first I tried proving that every element $x$ in $A\cap (X\setminus A)$ is also an element of $\emptyset$ and vice versa. First suppose that $x\in \emptyset$ then the conclusion $x\in A\cap (X\setminus A)$ is vacuously true. Now suppose that $x \in A\cap (X\setminus A)$ . Then $x \in A $ and $x\in X\setminus A \iff x \in A \text{ and } x\in \{y \in X: y\notin A\} \iff x\in A \text{ and } (x\in X \text{ and } x\notin A) $ . Since we have $x\in A$ and $x \notin A$ , the conclusion $x \in \emptyset$ is vacuously true. For the second I tried proving it again by saying: We need to show that every element $x$ in $(A\cap B)\cap C$ is also an element of $A\cap (B\cap C)$ and vice versa; but I ended up proving an equivalance. Suppose $x \in (A\cap B)\cap C \iff (x\in A\cap b) \land (x\in C)\iff ((x\in A) \land (x\in B))\land (x \in C)\iff (x\in A)\land(x\in B)\land(x \in C) \iff (x\in A)\land((x\in B)\land(x\in C))\iff x\in A\cap(B\cap C)$ . So, both of these proofs kind of rely on the fact that $x\in A$ and $(x \in B \text{ and } x\in C)$ is the same as the statement $x\in A$ and $x \in B$ and $x\in C$ . But this feels artificial. For the second proof I had the thought that set $\cap$ symbol has the same logic indentities as ""and"" so reducing the problem to logic made the solution trivial to the extent that it doesn't feel like a proof. I'm basically proving associativity of intersection by using the associativity of logic. But since the same logic applies to both symbols this seems like a non-proof. To be clear, I'm not really interested in a new proof of these since I can do both of those in a more standard way that I'm pretty sure is correct. I want to know why these proofs do or don't work.","['elementary-set-theory', 'solution-verification', 'logic']"
4744869,Does the collection of bounded continuous functions characterize probability law?,"Let $X,Y :(\Omega,\mathcal{A},\mathbb{P}) \to \mathbb{R}$ be two r.v.'s defined on a probability space $\Omega$ , and $C_b(\mathbb{R})$ the collection of all real bounded continuous function.  I'm wondering whether the following holds: $$\mathbb{E}[ f(X) ] = \mathbb{E}[f(Y)] ,\enspace \forall f\in C_b(\mathbb{R})\implies \mathbb{P}_X = \mathbb{P}_Y .$$ In the above $\mathbb{P}_X\,(\mathbb{P}_Y)$ is defined as the law of $X\,(Y)$ , i.e., $ \mathbb{P}_X(A) = \mathbb{P}(X\in A)$ . My guess is that this is true. My attempt: Actually, for any open set $G \subseteq \mathbb{R}$ , we have the following: Define $g_n(x) = [ n\cdot d(x,G^c)] \wedge 1$ , it's clear that $ 0\leq g_n \leq 1$ and $(g_n) \uparrow \mathbb{I}_B$ (the indicator function of $B$ ). Since each $g_n \in C_b$ , $$
  \mathbb{E}[ f_n(X)] =  \mathbb{E}[f_n(Y)] \enspace \forall n\implies \mathbb{P}_X(G) =  \mathbb{P}_Y(G).
$$ In the last line I take the limit $n \to \infty$ and apply the dominated convergence theorem( notice that the sequence $g_n$ is uniformly bounded) to interchange the limit and the integral. This shows that at least the two probability measures agree on open sets. The next step is to show that for any Borel subset $A \in \mathcal{B}(\mathbb{R})$ , similar result holds as in the case of open sets. I try to show this by defining the collection $L = \{B \in \mathcal{B}(\mathbb{R}): \exists \{f_n\}_n \subseteq C_b, \, (f_n) \to \mathbb{I}_B \, (Lebesgue)a.e. , 0\leq |f_n| \leq C \text{ for some C>0}\}$ , and my goal is to show that it's actually a d-system(or a $\lambda$ -system), and along with the fact that the topology $\mathcal{T} \subseteq L$ , it then follows from the $\pi$ - $\lambda$ - theorem that $L = \mathcal{B}(\mathbb{R})$ . However, I fail to show $L$ is indeed a d-system, so I'm doubting that this argument doesn't work. I hope that someone can provide a complete argument, or if the statement is false, providing a counterexample.","['integration', 'measure-theory', 'real-analysis', 'probability-theory', 'probability']"
4744876,Summation of an Arithmetico-Geometric Progression,"$$ \sum_{r=1}^{11} r5^r = \frac{43\times5^{a}+ 5}{b}$$ Find (a+b) This question was asked in an exam. I got the answer 28. However the answer given was 15. Here is my attempt: Let S = $ \sum_{r=1}^{11} r5^r $ So, S = $5^1 + 2\cdot5^2 + 3\cdot5^3 + 4\cdot5^4... + 11\cdot5^{11}$ - (i) $5S$ = $\hspace{1.5cm} 5^2 + 2\cdot5^3 + 3\cdot5^4 + ... + 10\cdot5^{11}+ 11\cdot5^{12}$ - (ii) Subtracting (ii) from (i), $-4S$ = $5^1 + 5^2 + 5^3 + ... + 5^{11} - 11\cdot5^{12}$ $-4S$ = $\frac{5(5^{11} - 1)}{4} - 11\cdot5^{12}$ - (Applied Formula for Sum of GP) $4S$ = $11\cdot5^{12} - \frac{5(5^{11}-1)}{4}$ $S$ = $\frac{44\cdot5^{12}-5(5^{11}-1)}{16}$ $S$ = $\frac{44\cdot5^{12}-5^{12}+5}{16}$ So, $S$ = $\frac{43\cdot5^{12}+5}{16}$ Hence, a+b = 12+16 = 28. I cannot point out where am I wrong. Can anyone pointout where have I gone wrong?","['algebra-precalculus', 'arithmetic-progressions', 'geometric-series', 'sequences-and-series']"
4744885,The vector space of square-integrable complex functions and the triangle inequality squared,"In proving that the square-integrable functions $\varphi: \mathbb{R} \to \mathbb{C}$ can be made into a complex vector space, my book endeavors to prove that the space is closed under superposition. Thus, it writes $$\int_{-\infty}^{\infty} |\varphi(x) + \alpha \psi(x)|^2 \, dx \leq \int_{-\infty}^{\infty} |\varphi(x)|^2 \, dx + |\alpha^2|\int_{-\infty}^{\infty} |\psi(x)|^2 \, dx < \infty, $$ but I can't for the life of me understand what is going on in this second step. I imagined that we should have used the triangle inequality, $$|\varphi(x) + \alpha \psi(x)|^2 \leq |\varphi(x)|^2 + |\alpha|^2| \psi(x)|^2 + 2|\varphi(x)||\alpha|| \psi(x)|,$$ but of course this last term on the RHS does not appear above. What is going on? Is there some basic complex analysis that I don't know which is being used here?","['complex-analysis', 'hilbert-spaces', 'vector-spaces']"
4744903,Parseval-Plancherel type identity for probability generating function,"Assume that $f,g \in L^2(\mathbb R)$ and define the Fourier transform of $f$ by $$\hat{f}(\xi) = \int_{\mathbb R} \mathrm{e}^{-i\,x\,\xi}\, f(x)\,\mathrm{d}\xi, \quad \xi \in \mathbb R.$$ The well-known Parseval-Plancherel identity links the $2$ -norm of $f-g$ and the $2$ -norm of $\hat{f} - \hat{g}$ via $$\int_{\mathbb R} |f(x)-g(x)|^2\,\mathrm{d} x = \frac{1}{2\,\pi}\,\int_{\mathbb R} |\hat{f}(\xi)-\hat{g}(\xi)|^2\,\mathrm{d} \xi \label{1}\tag{1}.$$ Now suppose that $f = (f_0,f_1,\ldots) \in \mathcal{P}(\mathbb N)$ and $g = (g_0,g_1,\ldots) \in \mathcal{P}(\mathbb N)$ are probability distributions over $\mathbb N$ , and define for each $z \in (0,1)$ the probability generating function of $f \in \mathcal{P}(\mathbb N)$ by $$\hat{f}(z) = \sum_{n\geq 0} z^n\,f_n. $$ I am wondering if there exists a ""natural"" analog of the Parseval-Plancherel identity \eqref{1} involving the probability generating function. Unfortunately, I did not find any useful reference or related literature about this question. Edit: Motivated by the answer provided by Andrew, I am wondering if there exists a collection of functions (not necessarily a orthonormal basis) $\{\varphi_n(z)\}_{n \geq 0}$ such that $$f_n = \int_0^1 \hat{f}(z)\,\varphi_n(z)\,\mathrm{d} z \label{2}\tag{2}$$ for each $n \in \mathbb N$ . I want to view \eqref{2} as a sort of analog of the inverse Fourier transform (but now it involves the probability generating function instead).","['moment-generating-functions', 'parsevals-identity', 'probability', 'generating-functions']"
4744921,How do we know that the derivative number is actually the slope of the tangent line ( to the graph of a given function)?,"In elementary textbooks on differential calculus, the notion of derivative number at, say, $x=a$ (i.e. $f'(a)$ ) is sometimes introduced via a parallelism between the analytic process and the geometric process, saying: while the difference quotient ( $ \frac { f(a+h) - f(a)} { h} $ )  goes to a determinate limit (as $h$ goes to $0$ ), the straight line passing through $(a, f(a))$ and $(a+h, f(a+h))$ tends to a "" limiting position"", namely the tangent line to the graph of function $ f$ at point $(a, f(a))$ . Of course, this approach is intuitive, and using a graphing calculator we ""see"" that the line defined by $ y-f(a) = f'(a) ( x-a)$ is tangent to the graph of function $f$ . Here we seem to face an alternative: (1) either the tangent is defined as the line satisfying the above equation ( and in that case my question is pointless), or (2) we have a concept of tangent line that is independent from differential calculus , in such a way that the claim ""the line  passing through $(a, f(a))$ and having a slope equal to $f'(a)$ is tangent to the graph of $f$ "" has a substantial content. In short: is it possible to prove, as an informative (and not simply definitional) claim that the line defined by $ y-f(a) = f'(a) (x-a)$ is the tangent to the graph of $f$ passing through $(a, f(a)$ ? And if so, which concept of ""tangent "" can be used to this effect?","['calculus', 'derivatives', 'geometry', 'tangent-line']"
4744939,Electronic filter: simplifying expression with integrals,"I've come across a generalised notch filter in an academic paper. The values $x_{f} and y_{f} $ are time varying values $x_{f}(t)$ and $y_{f}(t)$ , The equation of the filter block is expressed as follows, where $y_{f}$ is the output of the filter. $$y_{f}(t) = \zeta_{m}\begin{bmatrix} \sin(\Omega t) & \cos(\Omega t)  \end{bmatrix} \int \begin{bmatrix} x_{f}(t)\sin(\Omega t)\\ x_{f}(t)\cos(\Omega t)  \end{bmatrix} dt $$ This equation evaluates to: $$y_{f}(t) = \zeta_{m}\left[\sin(\Omega t)\int x_{f}(t) \sin(\Omega t)dt + \cos(\Omega t)\int x_{f}(t) \cos(\Omega t)] dt \right]$$ The authors then simplify this down to: $$\frac{d^2y_{f}(t)}{dt^2} + \Omega^2y_{f} = \zeta_{m} \frac{dx_{f}(t)}{dt}$$ I just wonder how this result was obtained? How do the authors get rid of the sine and cosine terms? Was this done through Laplace transforms and convolution? Or am I missing an integration by parts simplification?","['integration', 'calculus', 'trigonometry', 'laplace-transform']"
4744992,Two different solutions to implicit ODE,Where is the gap in my Logic? Im trying to solve the following implicit first order differential equation: $$y=y'^2x+y'^2$$ so I make the substitution $$p=y' \Rightarrow p=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}p'=p^2+(2px+2p)p’$$ . Then after solving the linear differential equation we get that: $$p=1+\frac{c}{\sqrt{x+1}}=y’$$ So if we integrate $p$ then we get: $$y=x+2c\sqrt{x+1}+c'$$ where we have two parameters. But if we instead choose to plug $p$ in the original equation we get the answer: $$y=(\sqrt{x+1}+c)^2$$ Why the first metod yields infinitely many more solutions?,['ordinary-differential-equations']
4745019,Dubious proof in Halmos's book: Two similar ordinal numbers are always equal,"I'm studying ordinal numbers using Naive set theory of Halmos. I think there was a small mistake in his proof of the statement ""if two ordinal numbers are similar, then they are equal"" The proof goes as follow: To prove this, suppose that $\alpha$ and $\beta$ are ordinal numbers and that $f$ is a similarity from $\alpha$ onto $\beta$ ; we shall show that $f(\theta) = \theta$ for
each $\theta \in \alpha$ . The proof is a straightforward transfinite induction. Write $S$ = { $\theta \in \alpha: f(\theta) = \theta $ }. For each $\theta \in \alpha$ , the least element of $\alpha$ that does not belong to $s(\theta)$ is $\theta$ itself (here s $(\theta)$ is the initial segment of $\theta$ in $\alpha$ ). Since $f$ is a similarity, it follows that the least element of $\beta$ that does not belong to the image of $s(\theta)$ under $f$ is $f(\theta)$ . These assertions imply that if $s(\theta) \subset S$ , then $f(\theta)$ and $\theta$ are ordinal numbers with the same initial segments, and hence $f(\theta) = \theta$ . We have proved thus that $\theta \in S$ whenever $s(\theta) \subset S$ . The principle of transfinite induction implies that $S = \alpha$ , and from this it follows that $\alpha = \beta$ I think that there are 2 suspicious points in the proof of the author: He does not prove the base case for $\theta = 0$ .
In fact, to prove the base case, we must prove that $f(0) = 0$ , and I don't see any reason why should $f(0) = 0$ . Therefore, the transfinite induction may be broken even for the base case. There maybe a problem in the definition of the set $S$ by the author. Here he denotes $S$ = { $\theta \in \alpha: f(\theta) = \theta $ }. I think that this definition implies implicitly that the range of $f$ is the set $\alpha$ which may not be true (because $f$ maps from $\alpha$ to $\beta$ which can be 2 completely different sets). So therefore, it is not sure that the set $S$ is not empty. Could you please tell me if there is a small mistake in the proof or it's me who says stupid things ? Thank you very much for your help!","['elementary-set-theory', 'ordinals', 'solution-verification']"
4745030,Element in a coset whose order's prime divisors are those of the order of the coset,"This is exercise 3B.6 of Isaacs' ""Finite Group Theory""; more specifically, item $(a)$ . It goes: Let $N \lhd G$ and let $g \in G$ , where $G$ is a finite group. Suppose $Ng$ has order $m$ in the group $G/N$ . Prove there is an element $h \in Ng$ such that every prime divisor of $o(h)$ is also a prime divisor of $m$ . Isaacs actually gives a hint to this problem - he suggests finding a cyclic $\pi$ -group $C$ such that $NC = N\langle g \rangle$ , where $\pi$ is the set of prime divisors of $m$ . I managed to prove that this hint indeed solves the problem, but I don't know where to look for the group $C$ . My idea was to somehow ""cut away"" the redundant part of $\langle g \rangle$ , which is the intersection of $N$ and $\langle g \rangle$ . For this, I tried to quotient $\langle g \rangle$ by $N \cap \langle g \rangle$ and use the correspondence theorem, but to no avail. I also tried a subgroup of $\langle g \rangle$ directly, since $m$ divides the order of $g$ , but I also don't see how to proceed... This is in the chapter about complements, and the Schur-Zassenhaus Theorem, but $N$ is not a Hall subgroup of $G$ , so I don't even know if such a complement exists - my guess is that those results are going to be important for $(b)$ and $(c)$ , but not for $(a)$ . Any help is appreciated! Thanks in advance!","['group-theory', 'quotient-group', 'finite-groups']"
4745031,How do we show that $N(a + bω) = a^2 − ab + b^2 $?,"We are given Let $\omega $ be defined as $\left(\omega = e^{\frac{2\pi i}{3}} = \frac{-1 + i\sqrt{3}}{2} \right).$ subset $ A \subseteq \mathbb{C} $ consisting of all elements of the form $ a + b \omega$ , with $ a, b \in \mathbb{Z}$ , Define a function $N ∶ A →\mathbb R$ by $N(u) =\lVert u\rVert^2$ (the square of the
complex norm). Show that $N(a + bω) = a^2 − ab + b^2.$ I don't get how $N(u)$ isn't just equal to $a^2 + b^2 + b^2\omega$ . I got that by $(a + b\omega)(a - b\omega) = a^2 -b^2\omega^2$ where we know $\omega^2 = -1 - \omega$ thus giving us $a^2 +b^2 +b^2\omega$ I used this definition provided to me for complex norm: There is a function $N : \mathbb R\to\mathbb Z$ defined by $N(a + bi) := (a + bi)(a − bi) = a^2 + b^2 = ||a + bi||^2$ This is called a norm function.","['algebra-precalculus', 'complex-numbers']"
4745058,$\mathbb{E}\left[\frac{X}{Y}\right]$ relationship to $\frac{\mathbb{E}[X]}{\mathbb{E}[Y]}$ for random variables X and Y that are not independent,"Suppose $Y > 0$ with probability 1, $\mathbb{E}[X]$ and $\mathbb{E}\left[\frac{1}{Y}\right]$ are finite, and $\mathbb{E}[X] > 0$ . I know that if X and Y are independent, then we have $\mathbb{E}\left[\frac{X}{Y}\right] = \mathbb{E}[X]\mathbb{E}\left[\frac{1}{Y}\right]$ , and we can use Jensen's Inequality to say that $\mathbb{E}\left[\frac{X}{Y}\right] \geq \frac{\mathbb{E}[X]}{\mathbb{E}[Y]}$ . But what if X and Y are dependent? Can we still derive the same inequality?","['expected-value', 'inequality', 'probability-theory', 'probability', 'random-variables']"
4745083,"If $f_n$ is a sequence weakly convergent to $f$ in $L^1[0,1]$, is the convolution $f_n*g$ weakly convergent to $f*g$?","The question is as in the title. Let $f_n$ be weakly convergent to $f$ in $L^1[0,1]$ . That is, for any $\phi \in L^\infty[0,1]$ , we have \begin{equation}
\int_{[0,1]} f_n\phi \to \int_{[0,1]}f\phi
\end{equation} as $n \to \infty$ . Next, for any $g \in L^1[0,1]$ , the convolution $f_n *g$ is in $L^1[0,1]$ by Young's inequality. However, I cannot prove / disprove if $f_n *g$ converges to $f*g$ weakly in $L^1[0,1]$ as $n \to \infty$ . Could anyone please help me?","['lp-spaces', 'functional-analysis', 'real-analysis']"
4745128,"In $\triangle IME$, $O$ and $H$ are circumcenter and orthocenter. $\overline{IO} = \overline{IH}, ~\angle EIH = 2\cdot \angle HIO$, find $\angle HEO$.","Let $\triangle IME$ be an acute triangle and points $O$ and $H$ be the circumcenter and orthocenter, respectively, such that $\overline{IO} = \overline{IH}$ . If $\angle EIH = 2\cdot \angle HIO$ , find the value of angle $\angle HEO$ . I know $\overline{EO} = \overline{IO} = \overline{IH}$ , since $O$ is the circumcenter, but I do not know what to do with this information. I also thought about using trigonometry since I know $\sin{2\alpha} = 2\sin{\alpha}\cos{\alpha}$ , and one angle is two times the other, but I don't know the value of any of the sides or angles, so I don't see how this can be useful. Then, I thought about extending line $IH$ since there would be a right angle and then use trig, but this isn't working. Also, I'm struggling to see the connection between the given information, since I can't see any useful relation between the orthocenter and the circumcenter, and I don't know why the triangle needs to be acute (maybe it's just so that there's only one answer?). I am a high school student, and this problem is adapted from an entrance exam I am studying for. Nothing I tried so far worked, so I have no idea of what to do. Any help would be really appreciated.","['euclidean-geometry', 'triangles', 'geometry']"
4745136,Finding area of hexagon inscribed in rectangle,"Question: A regular hexagon is inscribed in a rectangle as shown in the diagram below. The shaded areas are $20$ and $23$ each. Find the area of the regular hexagon. I managed to deduce that the center of the hexagon is the same point as the center of the square using dilations. Using this, since the diagram is symmetric, if $A_r$ represents the area of the rectangle and $A_h$ represents the area of the hexagon, then $$43+\frac{1}{2}A_h=A_r$$ From here, I know that I could solve the question by labeling a bunch of variables and using brute force, but I am looking for a cleaner solution. I think such a solution involves the fact that the hexagon can be split into six congruent equilateral triangles and adding them to the shaded regions. However, I am unsure to proceed (if this is even correct).","['contest-math', 'geometry']"
4745143,Probability question about cherry picking,"Question: Amy has a bowl of 5 red cherries and 8 purple cherries. She takes out cherries one at a time until there are no red cherries left. What is the probability that there are exactly 2 cherries left in the bowl? Solution attempt: I imagine that the $13$ cherries are arranged in a random sequence $X_1\ldots X_{13}$ , and Amy picks them in that order. There are $13!$ possible orderings of this sequence. Out of these orderings, the ones for which exactly $2$ cherries are left in a bowl have the form $X_1\ldots X_{10}{\rm RPP}$ , i.e. the sequence ends with a red cherry followed by two purple cherries. There are $5$ ways to select the red cherry, ${8\choose 2}$ ways to pick the two purple cherries, and $10!$ ways to arrange the remaining $10$ cherries. The probability is then $$\frac{5\times{8\choose 2}\times 10!}{13!} = \frac{35}{429} \approx .082$$ I'm solving this on a website, and it does not accept my answer. I ran a Monte Carlo simulation as a sanity check, and it gave me values around $.1$ , so it seems I am indeed lowballing. What am I missing? Thanks!","['combinatorics', 'probability']"
4745146,An inequality regarding sums of sigmoid/logistic functions,"Let $\sigma(x)=1/(1+e^{-x})$ denote the sigmoid/logistic function and let $a,b,c,d>0$ such that $a\geq c$ and $a+b=c+d$ . Prove that $$
\sigma(2a-b)-\sigma(2c-d)\geq \sigma(2a+b)-\sigma(2c+d).
$$ Note that both sides of the inequality are non-negative. The proof is easy if the coefficient 2 was a 1, or if the function $\sigma$ was a linear function. I simulated over 1 million values and the result seems to hold, but I'm stuck on the proof. I tried using the Lipschitz condition, subadditivity, and direct computation.","['statistics', 'functions', 'logistic-regression']"
4745174,Can all primes greater than 5 be decomposed into the difference of a primorial and a prime?,"Conjecture:
For all primes $p_n≥7$ , there is at least one solution to the equation $$ p_n = p_k\#  - p_m$$ where $p_k\#$ is any primorial, and $p_m$ is any prime number. Has this been explored before? Can it been proven or disproven? I tried it numerically up to $ p_n =2322869$ Interestingly, the additive version of this, $$ p_n = p_k\#  + p_m$$ seems to have solutions for most primes but not all; the lowest of which lacks a solution is $p_n=149$ .","['number-theory', 'conjectures', 'prime-numbers']"
4745220,"For a Fano threefold $X$, is there a lower bound for $S^3$ where $S\subseteq X$ is an irreducible surface?","Let $X$ be a smooth projective variety over an algebraically closed field $k$ , and assume that $-K_X$ is ample, i.e. $X$ is Fano. If $\dim X=2$ , then we have that $C^2\geq -1$ for all curves $C\subseteq X$ , because $C^2=(K_X+C).C+(-K_X).C\geq -2+1=-1$ . Is there a similar statement if $\dim X=3$ , i.e. is there a lower bound for $S^3$ where $S\subseteq X$ is an irreducible surface? I can see how to bound expressions like $S^2.T+S.T^2$ for distinct irreducible surfaces $S,T$ , but for $S^3$ I have no idea.","['algebraic-geometry', 'intersection-theory', 'projective-schemes']"
4745223,Particularly nice bump function,"Is there a smooth ( $C^\infty$ ) function $f: \mathbb{R} \to \left[ 0, 1 \right]$ such that: $f(x) = 1$ iff $x = 0$ , $f(x) = 0$ iff $\left\lvert x \right\rvert \geq 1$ , and $0 < f(x) < 1$ otherwise; Every derivative of $f$ is simultaneously $0$ at $x$ if and only if $x = 0$ or $\left\lvert x \right\rvert \geq 1$ ; $f(x) = f(-x)$ for all $x$ ; $f’(x) > 0$ for $-1 < x < 0$ and $f’(x) < 0$ for $0 < x < 1$ ; and $1 - f(x) = f(1 - x)$ for $0 \leq x \leq 1$ ? If not, for which subsets of the above five conditions are there satisfactory functions? I know that the standard bump function $f(x) = e^{1 + 1/(x^2 - 1)}$ satisfies all but the last one of my conditions; what other conditions could I exclude, or is there an extension of one of the standard examples to meet the last criterion as well?","['smooth-functions', 'real-analysis']"
4745233,Strongly connected? Binary numbers of length n with bounded hamming weight,"I am wondering if the following is true, and if yes how to prove it: Construct a graph where the vertices are binary numbers of length $n$ and have a hamming weight (i.e. digit sum) between $a$ and $b$ : $$V = \{ a \leq i \leq b \mid i \text{ is binary number of length } n\}$$ Edges are given by appending a $0$ or $1$ from the right to the binary number, e.g. (0001) $\overset{1}{\longrightarrow}$ (0011) and (0001) $\overset{0}{\longrightarrow}$ (0010). Is this graph strongly connected, for $a < b$ ? If yes, how to prove it? I know that for $a=b$ it is not true in general. But from some examples I think that for $a<b$ it is true. Any hints are welcome.","['connectedness', 'binary', 'graph-theory', 'combinatorics', 'discrete-mathematics']"
4745248,Trace of matrix which equals to number of columns,"While studying linear algebra, I found that: Let $A$ be an $m \times n$ Matrix (with $m>n$ ) then the trace $tr(A(A^T A)^{-1} A^T)$ equals the number of columns of $A$ . Does this hold in general and if so: why? If the number of columns is 1, the trace seems to be 1, too. I think that each columns acts like some kind of adding 1, but I can't explain why.","['matrices', 'trace', 'linear-algebra']"
4745253,Winding number of antipodal function $f: S^1\to S^1$,"Take a continuous, antipodal function $f: S^1\to S^1$ where antipodal means $$f(-x)=-f(x)$$ for all $x\in S^1$ . How to prove that the winding number of $f$ is not equal to zero? This is my intuition but I failed to find a proof. By $S^1$ I mean a circle in the plane $\bf C$ around zero. By winding number I mean the winding number of the graph of $f$ in the plane around zero. It is an integer representing the total number of times that the graph of $f$ travels counterclockwise around zero. I would be interested in both an analytical proof as well as an algebraic topology proof (if this exists). Thank you in advance for your help! EDIT: I want to thank the people who gave comments and asked questions to improve the quality of the question. I tried to reflect them in this updated version of the question. Thank you also for the fixed point answer.","['fixed-point-theorems', 'analysis', 'complex-analysis', 'general-topology', 'algebraic-topology']"
4745278,two constants of integration in first order ODE,"Let's take for example the equation: $$y=1+y'^2$$ Now I see that this equation is seperable but I don't understand why using the general method for implicit ODEs can lead to the following strange outcome. First we substitute: $$y'=p(x)\, \,\,\,\,\,\,\, \text{ so} \,\,\,\,\,\,\, y=y(x,p(x))$$ Then we take the full derivative of $y$ to find $p(x)$ : $$\frac{dy}{dx}=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}\frac{dp}{dx}=p(x)$$ And so we get: $$p(x)=2y'p'\Rightarrow p'=\frac{1}{2}$$ But $p'=y''$ So we have two options: One is to integrate twice and get an expression for $y$ : $$\int p' dx=p=\frac{x}{2}+C_1\\ \int p \,dx=\int y'dx=y=\frac{x^2}{4}+xC_1+C_2 \\ \Rightarrow y(x)=\frac{x^2}{4}+xC_1+C_2$$ Second is to solve for $p(x)$ and plug it in the original ODE: $$\int p' dx=p=\frac{x}{2}+C \\ y=1+p^2=1+(\frac{x}{2}+C)^2$$ Why do the two methods give us different answers and what is the intuitive meaning behind the second constant? I just can't wrap my head around it. Or maybe there is a mistake in the first method that I don't catch. Please explain.",['ordinary-differential-equations']
4745280,"If 3 numbers of the form $\frac{1}{q^2+1}$ have a mean of $\frac{1}{2}$, will one of them always be $\frac{1}{2}$?","I recently came onto a set of numbers with an interesting pattern, but I can't prove it always holds true. I'd appreciate any help in figuring it out. Let the set $S$ $$S = \left \{ \frac{1}{q^2+1} , q \in \mathbb{Q^+} \right \}$$ And three elements of that set $a,b,c \in S$ My conjecture is that if their average is equal to $\frac{1}{2}$ , then at least one of them must be equal to $\frac{1}{2}$ . $$\frac{a+b+c}{3} = \frac{1}{2} \implies (a =\frac{1}{2}) \lor (b=\frac{1}{2}) \lor (c=\frac{1}{2})$$ Do you have any idea on how to prove, or disprove that claim? Edit to provide examples :
You can easily generate sets of numbers that verify the conjectured pattern. Let $f(q) = \frac{1}{q^2+1}$ for rational $q$ . All triplets of the form $(f(1), f(q), f(\frac{1}{q}))$ have an average of $\frac{1}{2}$ .","['average', 'algebra-precalculus', 'conjectures']"
4745282,Smooth affinoid rigid-analytic spaces over $\mathbb C_{p}$,"Are there any examples of smooth affinoid rigid-analytic spaces $X$ over $\mathbb C_{p}$ , which are not the base-change $X=Y_{\mathbb C_{p}}$ of a smooth rigid-analytic variety $Y$ over a finite extension of $\mathbb Q_{p}$ ?","['rigid-analytic-spaces', 'algebraic-geometry']"
4745285,Different approaches to classification of finite groups,"Currently, I'm approaching the problem of finite group classification, in particular I'm studying group cohomology. I know that to achieve this result one path mathematicians are actually working on is the Hölder program concerning group extensions and their classification. I was wondering if there exist other approches to this problem, in particular using subgroups lattices or subnormal subgroups lattices or representations.","['homological-algebra', 'group-extensions', 'group-cohomology', 'group-theory', 'homology-cohomology']"
4745296,Understanding limits and exact solutions,"Consider the following equation: $\left(x^{2}-1\right)y=1-x$ Setting $y{=}0$ forces the LHS equal to zero, so $x$ must be 1 for the RHS to be zero too. However, if we now rearrange the equation as, $y=\frac{1-x}{x^2 - 1}$ , and we apply L'Hopital's rule, we see that when $x{=}1$ , $y{=}{-}1/2$ . We see also, that y is never equal to zero. So which is it? This seems contradictory to me. Please try to be thorough with your answer for me to understand.",['limits']
4745299,"If $a,b,c\ge0:a+b+c=3,$ prove $\sqrt{a+b+b^2}+\sqrt{b+c+c^2}+\sqrt{c+a+a^2}\ge 3\sqrt{3}.$","Problem. Let $a,b,c\ge0:a+b+c=3.$ Prove that $$\sqrt{a+b+b^2}+\sqrt{b+c+c^2}+\sqrt{c+a+a^2}\ge 3\sqrt{3}.$$ Equality holds at $(1,1,1);(0,0,3).$ I tried to use some classical inequality but it seems hard to apply. The following is just a thought: By AM-GM $$\sum_{cyc}\sqrt{a+b+b^2}=\frac{1}{\sqrt{3}}\sum_{cyc}\sqrt{3(a+b+b^2)}\ge 2\sqrt{3}\sum_{cyc}\frac{a+b+b^2}{a+b+b^2+3},$$ which is not good enough. I think we can find suitable yields $a+b+b^2=f(a,b,c)$ and save two of equality cases. Hope to see some ideas. Thanks for your interest.","['contest-math', 'inequality', 'buffalo-way', 'a.m.-g.m.-inequality', 'algebra-precalculus']"
4745373,The longest chain of subgroups for real matrices,"I would like to know what is the longest possible chain of subgroups starting from the group of real invertible matrices.
Of course, I suppose there many possible chains, but the most valuable, let's assume, would be that  with the greatest number of subgroups. My initial proposition is as follows: Group of invertible real matrices $GL_n(R)$ Subgroup of invertible real matrices with rational determinant. Subgroup of invertible real matrices with determinant equal to $1$ or $-1$ . Subgroup of orthogonal matrices. Subgroup of rotation matrices (determinant equal to $1$ ) Subgroup of rotation matrices with rational elements. Subgroup of rotation matrices with elements only $1,0,-1$ . I hope all the listed subgroups are satisfying definition of subgroup.
So we have group $GL_n(R)$ and 6 subgroups where every listed group is a subgroup of it's predecessor . Such list I name a chain. Can we present a longer chain of subgroups? It is allowable to increase resolution of the given chain (if it is possible) or to give other longer chain, starting from $GL_n(R)$ .
Parallel branches are not allowable, only single simple chain.","['matrices', 'group-theory', 'linear-algebra']"
4745400,L'Hospital's Rule of infinity over infinity,"I am troubled for understanding the L'Hospital's Rule of $\infty/\infty$ : $$\lim_{x\rightarrow a}\frac{f(x)}{g(x)}= \lim_{x\rightarrow a}\frac{f^\prime(x)}{g^\prime(x)} \tag{1}$$ where $\lim_{x\rightarrow a}\{f(x),g(x)\} \rightarrow \infty$ . The equation (1) is well understood for me in the case of $\lim_{x\rightarrow a}\{f(x),g(x)\} \rightarrow 0$ , in which case : $$\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=\lim_{\Delta x\rightarrow 0}\frac{f(a)+f^\prime(a)\Delta x}{g(a)+g^\prime(a)\Delta x}= \lim_{x\rightarrow a}\frac{f^\prime(x)}{g^\prime(x)} \tag{2}$$ However, if $\lim_{x\rightarrow a}\{f(x),g(x)\} \rightarrow \infty$ , here is my attempt : $$\lim_{x\rightarrow a}\frac{f(x)}{g(x)}=\lim_{x\rightarrow a}\frac{1/g(x)}{1/f(x)}= \lim_{x\rightarrow a}\frac{-\frac{1}{g^2(x)}g^\prime(x) }{-\frac{1}{f^2(x)}f^\prime(x)} \tag{3}$$ I don't understand why this last result can be turned into $\lim_{x\rightarrow a}\frac{f^\prime(x)}{g^\prime(x)}$ ?","['functions', 'infinity', 'indeterminate-forms', 'limits', 'derivatives']"
4745426,How to define $\pi$ from geometry to the limit of a sequence?,"There are several definitions of $\pi$ based on the limit of some sequences or series. (Maybe) The most famous example is the solution of the Basel problem: $$\pi = \sqrt{6 \sum_{n=1}^{+\infty}\frac{1}{n^2}}.$$ Anyway, even though I've found several of these sequences, none of them seems to be derived by using a constructive geometric approach. What I mean? For instance, let's consider a unit circle and the regular polyhedra inscribed with $m>2$ sides. Let $s_m$ the side length of the $m$ -th polyhedron. Then, we can define $\pi$ as follows: $$\pi = \frac{1}{2}\lim_{m \to +\infty} m\cdot s_m.$$ I've tried to find out the expression of $s_m$ , but I need to define it by using $\pi$ itself, i.e. $$s_m = \frac{2}{\sin\left(\frac{2\pi}{m}\right)}.$$ So it seems that, in order to evaluate $\pi$ , one needs to use $\pi$ itself!!! Summing up, is there any sequence/series based on simple geometric arguments that can be used to define $\pi$ ?","['pi', 'geometry', 'sequences-and-series']"
4745427,Limit of the difference between two sequences,"Suppose that $l_1, l_2,\dots$ are natural numbers such that $$\sum_{i=1}^{+\infty}2^{-l_i}\le1 \tag{1}$$ and let $$a_n=\frac{1}{n}\sum_{i=1}^{n}l_i - \log_2n \tag{2}$$ I think $\lim_{n \to \infty} a_n$ doesn't exist but I'm not sure about that. For example for $l_i = i$ we have: $$\sum_{i=1}^{+\infty}2^{-i} = \frac{1/2}{1-1/2} = 1 \\ a_n = \frac{1}{n}\frac{n(n+1)}{2} - \log_2{n} = \frac{n+1}{2} - \log_2{n}$$ It can be shown that in this case $\lim_{n \to \infty} a_n = \infty$ . Is it possible to find a sequence of natural numbers $\{l_i\}$ such that $\lim_{n \to \infty} a_n$ exists? Attempt: We can rewrite $a_n$ as follows: $$a_n = \frac{1}{n}(\sum_{i=1}^{n}l_i - n\log_2n) = \frac{1}{n}(\sum_{i=1}^{n}l_i - \sum_{i=1}^{n}\log_2n) = \frac{1}{n}\sum_{i=1}^{n}(l_i - \log_2n)$$ It seems that condition $(1)$ imposes some constraint on the growth of $l_i - \log_2n$ for large $n$ . If we can compare this growth with $n$ , it would be possible to find the limit. Background: This question comes from information theory. A prefix-free code satisfies Kraft inequality which is $(1)$ . For a source with uniform distribution on $n$ symbols the entropy is $\log_2n$ . Also the average length of code for this source is $$\sum_{i=1}^{n}p_il_i = \frac{1}{n}\sum_{i=1}^{n}l_i$$ So the redundancy in code can be computed as $(2)$ .","['summation', 'information-theory', 'real-analysis', 'sequences-and-series', 'limits']"
4745460,Show this bounded linear operator is injective,"Let $X=C([0,1])$ with the $\sup$ norm and let $T: X \to X$ given by $$
Tf(t) = f(t) + \int_0^t f(s) \,\mathrm{d}s.
$$ I already showed that $T$ is a bounded linear operator with $\|T\|=2$ .  The problem asks me to show that $T$ is injective. What I tried was to show $\ker T = \{0\}$ , which means the equation $Tf(t)=0$ , or \begin{equation} \tag{1}
f(t) + \int_0^t f(s) \,\mathrm{d}s = 0
\end{equation} for all $t \in [0,1]$ has a unique solution $f \equiv 0$ .  Differentiating both sides with respect to $t$ gives the ODE: $$
f'(t) + f(t) = 0,
$$ which has a solution $f(t)=Ce^{-t}$ , where $C$ is a constant.  Since equation $(1)$ gives $f(0)=0$ , we must have $C=0$ , and equation $Tf(t)=0$ has a solution $f \equiv 0$ . However, I'm not sure how to show $f = 0$ is the unique solution to $(1)$ (or maybe it must be from the general solution to the ODE). I wonder if differentiation can be done in this case because $f$ is only continuous, not a $C^1$ function. Is there other way to solve this problem like assuming $Tf = Tg$ and showing $f=g$ (I was stuck on this)? Does it have something to do with the Hahn-Banach Theorem, (I don't know the space and the operator)? Thank you.",['functional-analysis']
4745544,ODE distribution function in phase space,"Suppose I have an ODE system governed by the matrix $A$ $$ \frac{d\mathbf{x}}{dt} = A \mathbf{x} $$ Let's say I choose a large number of random initial conditions and let them evolve for some time $t$ . These initial conditions will have some sort of distribution in phase space -- which I'm assuming can be described by a probability distribution function $f(\mathbf{x},t)$ --  that will evolve with time. Is there an equation that governs the time evolution of $f$ ? How would such an equation change if we add an inhomogeneous term?","['probability-distributions', 'ordinary-differential-equations', 'partial-differential-equations']"
4745560,"For $u$ harmonic and $f = u+iv$ holomorphic, show that $f(z) = \frac{1}{\pi i} \oint_{|\zeta|=r} \frac{u(\zeta)}{\zeta - z} d\zeta - \overline{f(0)}$","Here's a question from a previous complex analysis qualifying exam that I'm honestly just stumped on: Let $u$ be a harmonic function on the unit disc $D = \{z: |z|<1\}$ , which is the real part of the holomorphic function $f$ on $D$ . Show that for any $0<r<1$ we have $$ f(z) = \frac{1}{\pi i} \oint_{|\zeta|=r} \frac{u(\zeta)}{\zeta - z} d\zeta - \overline{f(0)}, \quad \quad \text{for } |z|<r. $$ I know that $u$ harmonic means $u_{xx}+u_{yy} = 0$ . The form of $f(z)$ given looks reminiscent of Cauchy's Integral Formula and we could say that since $f$ is holomorphic, we can write $$f(z) = \frac{1}{2\pi i} \oint_{|\zeta|=r} \frac{f(\zeta)}{\zeta - z} d\zeta =  \frac{1}{2\pi i} \oint_{|\zeta|=r} \left(\frac{u(\zeta)}{\zeta - z} + i \frac{v(\zeta)}{\zeta - z}\right) d\zeta$$ but where do I go from here? UPDATE:
Building on a suggestion from a comment, we also have $$\overline{f(z)} = \frac{1}{2\pi i} \oint_{|\zeta|=r} \frac{\overline{f(\zeta)}}{\zeta - z} d\zeta =  \frac{1}{2\pi i} \oint_{|\zeta|=r} \left(\frac{u(\zeta)}{\zeta - z} - i \frac{v(\zeta)}{\zeta - z}\right) d\zeta$$ and thus $$\overline{f(0)} = \frac{1}{2\pi i} \oint_{|\zeta|=r} \frac{\overline{f(\zeta)}}{\zeta } d\zeta =  \frac{1}{2\pi i} \oint_{|\zeta|=r} \left(\frac{u(\zeta)}{\zeta} - i \frac{v(\zeta)}{\zeta}\right) d\zeta.$$ I can add these two and with a bit of manipulation get $$f(z) + \overline{f(0)} =  \frac{1}{\pi i} \oint_{|\zeta|=r} \frac{u(\zeta)}{\zeta - z} d\zeta - \frac{1}{2\pi i} \oint_{|\zeta|=r} \frac{z\overline{f(\zeta)}}{\zeta(\zeta - z)} d\zeta.$$ It remains to show that the last term goes to zero... and I am once again stuck. UPDATE 2:
I added a bounty to this question in hopes of getting a full, complete, and clear worked answer to this problem. I need to make sure I can do this kind of problem correctly before my own upcoming qual.","['complex-analysis', 'cauchy-integral-formula', 'harmonic-functions']"
4745666,Reference request - Probabilistic interpretation of Dirichlet Laplacian eigenfunction,"Background: consider some uniformly elliptic differential operator $$L=\sum_{i,j=1}^{n}a_{ij}\frac{\partial^2}{\partial x_i\partial x_j}+\sum_{i=1}^{m}b_i\frac{\partial}{\partial x_i}$$ on a bounded domain $\Omega\subset \mathbb{R}^n$ . Suppose $(X_t)_{t\geq 0}$ is the diffusion with infinitesmial generator $L$ . Then, under suitable assumptions on the coefficient functions $a_{ij}$ and $b_i$ , the solution to the PDE $$\begin{cases}Lu=0&\text{ on }\Omega\\u=f &\text{ on }\partial \Omega\end{cases}$$ is given by $u(x)=\mathbb{E}_x[f(X_{\tau})]$ , where $\tau=\inf\{t\geq 0:X_t\notin \Omega\}$ . Then my question is as follows: consider the principal Dirichlet Laplacian eigenfunction $\phi_{\Omega}$ on a bounded domain $\Omega$ . In other words, $\phi_{\Omega}:\Omega\to\mathbb{R}$ is the unique (up to scalar multiple) function such that $$\begin{cases}-\Delta \phi_{\Omega}=\lambda_{\Omega}\phi_{\Omega}& \text{ on }\Omega\\ \phi_{\Omega}=0\text{ on }\partial \Omega\\ \phi_{\Omega}>0\text{ on }\Omega.\end{cases} $$ Is there some formula expressing $\phi_{\Omega}$ in terms of the expectation of some process? To the best of my knowledge, representation formulas exist only for operators of the form $L+c$ where $c\leq 0$ is a nonpositive constant, but here $\lambda_U>0$ . Of course, at the risk of stating the obvious, it is hopeless to express $\phi_{\Omega}=\mathbb{E}[f(X_{\tau})]$ , where $f:\partial\Omega\to\mathbb{R}$ is the zero function, as that would imply $\phi_{\Omega}\equiv 0$ . I would appreciate pointers to any kind of ""probabilistic formula"" for $\phi_U$ , even if not written as an expectation. (For people familiar with the topic: I am aware of the formula $p(t,x,y)=\sum e^{-\lambda_jt}\phi_j(x)\phi_j(y)$ , where $p(t,x,y)$ is the transition probability of killed Brownian motion in $\Omega$ , but this is not what I am looking for.)","['probability-theory', 'partial-differential-equations']"
4745671,Mean of Medians? Median of Means?,"This is a question I have wondered about for a long time and have never been able to find a full mathematical explanation behind this. Suppose there are 100 countries. As an experiment: We give Person A the median income of each country We give Person B the mean income of each country Now, suppose the following happens: Person A decides to take the mean of all median incomes Person B decides to take the median of all mean incomes Person C shows up out of nowhere and decides to take the median of all median incomes My Question: Using mathematics, we can we demonstrate that perhaps some of these calculations are not very ""meaningful""? For example, can we somehow show that some of these calculations lack important mathematical properties and are basically arbitrary? Thanks!","['means', 'median', 'probability']"
4745718,Coloring the Square. Burnside's lemma.,"Question from my last exam: We paint the vertices of a square with five colors: A, B, C, D, and E. Two ways of painting are considered identical if one can be transformed into the other through an isometry of the square, possibly combined with swapping the colors $A \leftrightarrow B$ or $C \leftrightarrow D$ . For example, the squares ${ }_{C A}^{A B}$ , ${ }_{B A}^{C B}$ , and ${ }_{A D}^{B A}$ are considered identical, but different from ${ }_{C A}^{B A}$ . Find the number of different ways to paint the square. Usually, when solving such problems, I had to create a table like the one below and manually count all the fixed points. Then, using Burnside's lemma, I had to calculate the average, which would be the result. For example, for coloring the midpoints of opposite edges, we have $3+8$ , because when using one color, we have $3$ options (the whole square is colored with A, C, or E), and for two colors, we have: $A, B = B, A$ (where the first letter represents the color of the first pair of vertices, and the second letter represents the color of the second pair of vertices) $C, D = D, C$ $A, C = B, C = A, D = B, D$ $C, A = C, B = D, A = D, B$ $A, E = B, E$ $E, A = E, B$ $C, E = D, E$ $E, C = E, D$ And the table looks like this: Rotation Number of Fixed Points Identity $180^{\circ}$ (2x) (midpoints of opposite edges) $3+8$ $180^{\circ}$ (2x) (opposite vertices) $90^{\circ}$ (center of the square) $180^{\circ}$ (center of the square) $270^{\circ}$ (center of the square) Apparently, this time, it is not the expected method to solve the problem, and it's not surprising because it would take quite a lot of time and cases to consider. What is another way to count these colorings?","['polya-counting-theory', 'combinatorics', 'discrete-mathematics']"
4745748,Connected components of conformal image of boundary,"Let $f : G \rightarrow \mathbb{D}$ be a biholomorphism (a holomorphic map with holomorphic inverse), and suppose $G$ is a bounded open subset of the plane. Let $C$ be a compact connected subset of the boundary $\partial G$ , and consider the set: $I =  \{y \in \partial \mathbb{D} : \exists (x_n)_{n=1}^{\infty} \subset G, d(x_n, C ) \rightarrow 0, y = \lim_{n} f(x_n) \}$ Is $I$ necessarily of finite connectivity? That is, does $I$ necessarily have at most finitely many connected components? It may be worth noting the image will most certainly NOT be connected. For example, take $G$ to be a slit disk, and $C$ a small compact connected interval below the tip of the slit. Then the image of $C$ under the map $f$ will correspond to two connected components of the boundary of the unit disk.","['connectedness', 'analysis', 'complex-analysis', 'plane-geometry', 'conformal-geometry']"
4745752,Connection laplacian and abstract index notation,"Let $(M,g)$ be a (pseudo-)Riemannian manifold. I am having some struggles to relate two different approaches, one based on the abstract index notation, and the other one based on global, coordinate-free, notation. Consider a general covariant $k$ -tensor field $T\in\Gamma^{\infty}(T^{\ast}M^{\otimes k})$ . In coordinates, I can write down the components of $T$ as $T_{\mu_{1}\dots\mu_{k}}$ . Then, the (connection) Laplacian of $T$ is the $k$ -tensor field $\square T\in\Gamma^{\infty}(T^{\ast}M^{\otimes k})$ with components $$(\square T)_{\mu_{1}\dots\mu_{k}}:=g^{\mu\nu}\nabla_{\mu}\nabla_{\nu}T_{\mu_{1}\dots\mu_{k}}$$ I won't write down the explicit expression of the right-hand side, but I think it is clear what it means. For example, if $k=1$ , then $$(\square T)_{\alpha}=g^{\mu\nu}(\partial_{\mu}\nabla_{\nu}T_{\alpha}-\Gamma^{\lambda}_{\mu\nu}\nabla_{\lambda}T_{\alpha}-\Gamma^{\lambda}_{\mu\alpha}\nabla_{\nu}T_{\lambda})$$ and substituting the usual expression $\nabla_{\mu}T_{\alpha}=\partial_{\mu}T_{\alpha}-\Gamma^{\lambda}_{\mu\alpha}T_{\lambda}$ , which gives a lengthy formula for the components of the Laplacian. On the other hand, on wikipedia for example, the connection Laplacian is defined globally as $$\square T:=\mathrm{tr}(\nabla^{2}T)$$ with the double covariant derivative $\nabla_{X,Y}^{2}T:=\nabla_{X}\nabla_{Y}T-\nabla_{\nabla_{X}Y}T$ . I am a bit confused about the term $\nabla_{\nabla_{X}Y}T$ . The first term $\nabla_{X}\nabla_{Y}T$ , using the trace, should give my coordinate expression above, i.e. $g^{\mu\nu}\nabla_{\mu}\nabla_{\nu}T_{\alpha}$ , but what about the second term? Or does it vanish? Maybe my confusion is related to how to relate the expression $\nabla_{\alpha}\nabla_{\beta}T_{\mu_{1}\dots\mu_{k}}$ to the double covariant derivative $\nabla^{2}T$ ...","['riemannian-geometry', 'tensors', 'laplacian', 'manifolds', 'differential-geometry']"
4745785,Evaluating $\int_0^{\pi/2} \frac{\sqrt{\tan x}}{\sin x(\cos x+\sin x)}\mathrm{d}x$,"I was solving the following integral: $$I=\int_0^{\pi/2} \frac{\sqrt{\tan x}}{\sin x(\cos x+\sin x)}\mathrm{d}x $$ Using the identity $\sin x=\frac{\tan x}{\sec x}$ , I simplified the integral in the following way: $$I=\int_0^{\pi/2}\frac{\sqrt{\tan x}}{\frac{\tan x}{\sec x}\left(\cos x+\frac{\tan x}{\sec x}\right)}\mathrm{d}x=\int_0^{\pi/2}\frac{\sec^2 x}{\sqrt{\tan x}(1+\tan x)}\mathrm{d}x=2\tan^{-1}(\sqrt{\tan x})\Bigg|^{\pi/2}_0=\lim_{x\to \pi/2} 2\tan^{-1}(\sqrt{\tan x})=\pi $$ However, I'd like to see a different solution, because I feel there's a simpler one maybe using the symmetry of the integrand: $$\int_0^{\pi/2} \frac{\sqrt{\tan x}}{\sin x(\cos x+\sin x)}\mathrm{d}x=2\int_0^{\pi/4} \frac{\sqrt{\tan x}}{\sin x(\cos x+\sin x)}\mathrm{d}x $$","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4745794,"Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$, $\forall n\ge 2$","Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$ , $\forall n\ge 2$ . My Proof : Call $d_n=a_{n+1}-a_n$ ; then from the given conditions we get, $$\sqrt{a_{n-1}} \leq d_n \leq \sqrt{a_n}$$ and $$\sqrt{a_{n}} \leq d_{n+1} \leq \sqrt{a_{n+1}}$$ Subtracting these, we get, $$d_{n+1}-d_n \leq \sqrt{a_{n+1}}-\sqrt{a_n} \quad \text{(we only took upper bound into account)}$$ $$ \implies a_{n+2}+ a_n \leq \sqrt{a_{n+1}}-\sqrt{a_n} \implies a_n + \sqrt{a_n} \leq \sqrt{a_{n+1}} - a_{n+2} \implies a_{n+2} \leq \sqrt{a_{n+1}} \implies a_{n+2}^2 \leq a_{n+1}$$ But also, $a_{n+1}^2 \leq a_{n+2}^2$ . Hence $$a_{n+1}^2 \leq a_{n+2}^2 \leq a_{n+1} \implies a_{n+1}=a_{n+2}=1$$ Inductively, we get $a_i= 1 \quad \forall \quad i=1,2,3, \cdots$ . but from the condition given in the problem, $1 \leq  (1-1)^2 \leq 1$ is wrong. Hence, there is no such sequence of natural numbers. My concern about my solution is that it feels very loose to me, I think I didn't use the lower bound to its full ability, so I would like to know if there is any fault in my solution.","['inequality', 'discrete-mathematics', 'sequences-and-series']"
4745809,What do different values of $y$ mean when we plot different graphs?,"When solving an equation such as $x^2 = 9$ we write $x = 3$ $or$ $x = -3$ . Now take the functions $f(x) = 2x$ and $g(x) = 4$ for all $x$ . When we plot them on the $xy$ plane we write $y=2x$ and $y=4$ . If we were to evaluate them at $x=1$ we have $y=2$ for the first one, and $y=4$ for the second one. Now I would like to ask, is the correct way to write this, $y=2$ $and$ $y=4$ , or is it, $y=2$ $or$ $y=4$ ? I am uncomfortable with saying $y=2$ and $y=4$ as this implies $2=4$ I think.","['algebra-precalculus', 'graphing-functions']"
4745845,Proof that a series converges to zero,"I am working on the following problem arising in time series analysis. Let us assume that $\sum_{h \in \mathbb{Z}} |\gamma(h)|<\infty$ . I would like to prove that \begin{equation*}
1) \; \; \; \lim_{n\to +\infty} \sum_{h >n } \gamma(h) = 0
\end{equation*} and the stronger result \begin{equation*}
2) \; \; \; \lim_{n\to +\infty} \sqrt{n}\sum_{h >n } \gamma(h) = 0
\end{equation*} I was able to prove 1) using the limit of the partial sum $S_n = \sum_{h=-\infty}^n\gamma(h)$ , and then writing $\sum_{h > n} \gamma(h) = \sum_{h \in \mathbb{Z}} \gamma(h) - \sum_{h = -\infty}^{n} \gamma(h) \
$ and taking the limit.
However, for the second question I think I need additional assumptions on the sequence $(\gamma(h))$ in order to prove it.","['covariance', 'calculus', 'absolute-convergence', 'sequences-and-series', 'time-series']"
4745864,$\int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt < f(\frac{\pi}{2}) \Rightarrow \int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})}$,"Assume $f:[0,\pi]\rightarrow (0,+\infty)$ is continuous. And $$
\int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt
\tag{1}
$$ if $$
\int_0^{\pi/2} f(t)\sin t dt < f(\frac{\pi}{2})
\tag{2}
$$ then, how to show $$
\int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})}    ~~~~~~?
\tag{3}
$$ I guess it when I read some paper. I am not sure it is right. And I don't know how to prove it. Seemly, the Holder, Cauchy, Young inequalities are useless for it. Response to mathworker21 (2023-8-25): I calculated a function liking $\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2})}+c1_{\{\frac{\pi}{2}\}}$ . For convenience, let $$
f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+
k(t-\frac{\pi}{2}+\epsilon) 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]}
$$ First, I have $$
\int_0^{\pi/2} f(t)\sin tdt =
-\delta^{-2}\cos\delta+\delta^{-2}-\sin\epsilon+\frac{\sqrt 2}{2}+k-k\cos\epsilon   \\
f(\frac{\pi}{2}) = k\epsilon
$$ So, $\int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2})$ imply $$
k< \frac{
\delta^{-2}\cos\delta  - \delta^{-2}  +\sin\epsilon  -\frac{\sqrt 2}{2}
}
{
1-\epsilon-\cos\epsilon
}
\tag{5}
$$ On the other hand, we have $$
\int_0^{\pi/2}\sqrt{f(t)}dt =
1+\frac{\pi}{4}-\epsilon+\frac{2}{3k}(k\epsilon)^{3/2}
$$ Therefore, $\int_0^{\pi/2}\sqrt{f(t)}dt=\frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ imply $$
\sqrt k = \frac{
1+\frac{\pi}{4}-\epsilon
}{
\frac{\pi}{2}\sqrt\epsilon -\frac{2}{3}\epsilon^{3/2}
}
\tag{6}
$$ However, It is not possible to satisfy both conditions (5) and (6) simultaneously. I use Wolfram Mathematica (a software) to get that when $\epsilon=10^{-10},\delta=10^{-10}$ , (5)  imply $$
k<1.207106781146902863457773*10^{10}
$$ (6) imply $$
k=1.291904506901873765116087*10^{10}
$$ Of course, I also test other numerical value. For example, $\epsilon=10^{-6},\delta=10^{-6}$ or $\epsilon=10^{-10},\delta=10^{-6}$ and so on. But (5) and (6) can't be satisfied simultaneously. Besides, I also calculated $$
f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+
k^2(t-\frac{\pi}{2}+\epsilon)^2 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]}
$$ It also is not the counterexample of statement 2. The Mathematica code: Response to mathworker21 (2023-8-22): I find that $$
f(t) = \delta^{-2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t)
$$ is not the  counterexample  of Statement $2$ : For every continuous $f : [0,\frac{\pi}{2}] \to (0,+\infty)$ satisfying $\int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2})$ , we have $\int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ . For this $f(t)$ there always be $\int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ . However, $$
\int_0^{\pi/2} f(t)\sin(t)dt 
=-\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4}
~~~~~~~
f(\frac{\pi}{2})=1
$$ for any $\delta\in (0,\frac{\pi}{4})$ ,  there is $$
-\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4} >1
$$ Response to mathworker21 (2023-8-11): Could you explain why it is equivalent to showing $$
\left(\frac{1}{\pi/2}\int_0^{\pi/2} \sqrt{f(t)}dt\right)^2 \le \int_0^{\pi/2} f(t)\sin(t)dt  
~~?
\tag{4}
$$ In fact, I also don't understand Ryszard Szwarc's comment. Besides, I calculate your example, when $\delta>0$ is sufficiently small, it is not false. For $$
f(t) = \delta^{-1/2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t)
$$ I have $$
\int_0^{\pi/2} \sqrt{f(t)}dt=
\int_0^\delta \sqrt{\frac{1}{\sqrt\delta}}dt
+
\int_{\pi/4}^{\pi/2} dt
=\delta^{3/4}+\frac{\pi}{4}
$$ Therefore, the left part of (4) is $$
L=\frac{4}{\pi^2} \delta^{3/2} +\frac{2}{\pi}\delta^{3/4}+\frac{1}{4}
$$ On the other hand, the right part of (4)  is $$
R=
\int_0^{\pi/2} f(t)\sin t dt
=
\int_0^\delta \frac{1}{\sqrt\delta} \sin t dt 
+
\int_{\pi/4}^{\pi/2} \sin t dt
=
\frac{1}{\sqrt\delta}-\frac{1}{\sqrt\delta}\cos\delta+\cos\frac{\pi}{4}
$$ And $$
\lim_{\delta\rightarrow 0^+} L =\frac{1}{4}
~~~~~~
\lim_{\delta\rightarrow 0^+} R =\cos\frac{\pi}{4}=\frac{\sqrt 2}{2}
$$ So, when $\delta>0$ is sufficiently small, there is $L\le R$ . But, I use Mathematica to get the grapha of $L-R$ , there is $L>R$ when $\delta$ near $\pi/4$ . $x$ is the $\delta$ PS(2023-8-8): From two aspects, I think it is right. First, by the rearrangement inequality, I feel it is right. But it is not general rearrangement, the maximum value of $f$ should be placed near $\frac{\pi}{2}$ in the rearrangement. But I still can't give a detailed proof up to now. On the other hand, I write a program to verify it. I approximate the integral by summation. There is not counter-example. The Python code is as follows: import math
import random

i=0
while i<10000:
    i=i+1

    #生成一个随机函数  (Generates a random function)
    f_list=[]
    for i0 in range(0,200):
        f_list.append(random.random())

    #计算左边积分  (Compute the left-hand integral of (1) )
    L=0
    for i1 in range(0,100):
        L=L+f_list[i1]*math.sin(i1*math.pi/200)*(math.pi/200)

    #计算右边积分  (Compute the right-hand integral of (1) )
    R=0
    for i2 in range(100,200):
        R=R+f_list[i2]*math.sin(i2*math.pi/200)*(math.pi/200)

    #调整两边积分的大小  (Adjust the size of the integral on both sides of (1) )
    d=L/R
    for i3 in range(100,200):
        f_list[i3]=f_list[i3]*d

    #判断是否小于f(pi/2)  (Determine if it is less than f(pi/2))
    if L>= f_list[100]:
        i=i-1
        continue   #跳过循环剩下部分  (Skip the rest of the loop)

    #计算最后一个式子中左边的积分  (Calculate the integral on the left of the (3) )
    LL=0
    for i4 in range(0,200):
        LL=LL+math.sqrt(f_list[i4])*(math.pi/200)

    #如果是反例，就打印出来  (If it's a counterexample, print it out)
    if LL >= math.pi*math.sqrt(f_list[100]):
        print(LL,math.pi*math.sqrt(f_list[100]),LL-f_list[100])

print(""End"")","['calculus', 'integral-inequality', 'inequality', 'real-analysis']"
4745878,Calculating an angle from an intersecting rectangle and circle,"I've been trying to do some trigonometry to find an equation for a bevel angle for a tool (the rectangle in the diagram below) ground on a rest using a circular grinding stone (the large arc in the diagram below).  I think I've formulated the problem with all the variables I know and I think there's enough there to fully define the angle, but I'm really struggling to figure out to link the tool thickness (t) into the equations. This is how I've formulated the problem: All of the blue dimensions are defined and known.  All of the red lines indicate some of the angles and lengths that I've got equations for already.  There are a few more that are quite straightforward from those angles (e.g. just 180°-something) but I was trying not to make the diagram too cluttered. I'm trying to derive $\theta$ in terms of the $r$ , $l$ , $t$ , $s$ and $g$ .  None of the equations I've written (and deemed useful) have included $t$ so far, which is obviously a problem! Close-up of the angle I'm interested in, in case it isn't clear (with the red lines removed for clarity): Can anyone help me relate the triangles to the chords? I can draw it in CAD and get the angle out very easily, but that feels to me like it's cheating and I'd like to understand the trigonometry involved! Edit: this is the same thing redrawn with g doubled to show the differences in alphas: Edit 2: Long-winded solution based on the first set of help that John Alexiou has given (but note that the solution offered by John in his edited answer is much more elegant).","['trigonometry', 'geometry']"
4745880,Generalisation of the Markov property to stopping times,"Let $(X_t)_{t\geq 0}$ be a stochastic process adapted to $(\mathscr{F}_t)_{t\geq 0}$ . The (weak) Markov property says $X_{t+s}|\mathscr{F}_t\sim X_{t+s}|X_t$ . The strong Markov property says that if $T$ is a stopping time, then $X_{T+s}|\mathscr{F}_T\sim X_{T+s}|X_T$ . Let's call property ( $\star$ ) the one that says that if $S$ is a stopping time, then $X_{S+t}|\mathscr{F}_t\sim X_{S+t}|X_t$ . The difference with the SMP is that in the case of property ( $\star$ ) I am conditioning with respect to a non-random sigma algebra and the stopping time appears ""only before the bar of conditional expectation"". So apparently it is a different way of generalising the weak Markov property. Broadly speaking, I would like to know whether this property ( $\star$ ) has a name and under what conditions it is implied by either the WMP or the SMP. My progress so far is the following: In general property $(\star)$ is not implied by the SMP. It's not difficult to find counterexamples in the discrete case, and the answer by Julian Newman gives a counterexample in the continuous case too. At page 58, Theorem 3, in the book Markov Processes, Brownian Motion, and Time Symmetry by Chung and Walsh, it is shown that $X_{S+t}|\mathscr{F}_t\sim X_{S+t}|X_t,S$ for Feller processes and if $S$ is $\mathscr{F}_t$ measurable. (What they do in the proof is fine but the way they state the final result in their equation $(5)$ is, in my opinion, wrong, since the RHS should depend also on $S$ and not only $X_t$ . See also the discussion here .) QUESTIONS : Is it true that property $(\star)$ holds for right-continuous measurable processes which satisfy the WMP in the case when $S$ is a stopping time with respect to $(\mathscr{F}^Y_r)_{r\geq 0}$ , where $(Y_r)_{r\geq 0}=(X_{t+r})_{r\geq 0}$ ? In this case I think I can prove it, because we have $S=f(X_{t+r_1},X_{t+r_2},...)$ and by the DCT $\mathbb{E}(g(X_{t+S})\,|\,\mathscr{F}_t)=\lim_n\mathbb{E}(g(X_{t+S_n})\,|\,\mathscr{F}_t)$ for all bounded $g$ , where $S_n=2^{-n}\lceil2^n S\rceil$ is a discretisation of $S$ , and we can use the WMP on each $X_{t+r_k}$ to conclude that $\mathbb{E}(g(X_{t+S_n})\,|\,\mathscr{F}_t)=\mathbb{E}(g(X_{t+S_n})\,|\,X_t)$ . More generally, can something be said about the case when $S$ is not $\mathscr{F}_t$ measurable nor is a stopping time with respect to $(\mathscr{F}^Y_r)_{r\geq 0}$ , but is just a generic stopping time with respect to $(\mathscr{F}^X_t)_{t\geq 0}$ ? In this case, I expect something like $X_{S+t}|\mathscr{F}_t\sim X_{S+t}|X_t,Z$ to be true, where $Z$ is some random variable that should capture the information about $S$ available at time $t$ (but which I don't know how to specify).","['conditional-probability', 'markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory']"
4745888,Recommendation on probability and statistic books.,"I'm currently starting to self study probability and statistic, a friend recommend me to use a book he has but his book does not go deep in the theorem and formula, instead it just state the equation and when to use it along with some properties but not the proof for the equation (for example: the chapter about the Poisson random variable just tells you how to use it and when, but lacks the proof of how mathematicians arrive at that complex equation ). I would like a book that is rigorous and proof-based for every problem in it ( like Tom M. Apostol's calculus books for example), im a colleague student and has good calculus and linear algebra background so an more advance than regular books is ok with me. Do you have any recommendation ?","['self-learning', 'statistics', 'book-recommendation', 'probability']"
4745896,"Integral over a product of polynomial, exponential and Bessel function","In a physics textbook I'm working through I found an interesting integral identity which I want to prove: \begin{equation}
\int_0^\infty t^{\nu +1} J_\nu(\beta t) e^{-\alpha t} \, dt = \frac{2\alpha (2\beta)^\nu \Gamma(\nu + \tfrac{3}{2})}{\sqrt{\pi} (\alpha^2 + \beta^2)^{\nu + 3/2}}
\end{equation} where $J_\nu$ is the Bessel function and $\Re(\nu)>-1, \Re(\alpha) > |\Im(\beta)|$ . In my case for the application of this identity these conditions are always fulfilled since it will be $\alpha > 0, \beta \geq 0, \nu \geq 1/2$ . One could write this integral as the Hankel transform of the function $f(t) = t^\nu e^{-\alpha t}$ such that \begin{equation}
\mathcal{H}_\nu [f(t)](\beta) = \int_0^\infty f(t) J_\nu (\beta t) t \, dt \,.
\end{equation} However this is more of an interesting fact about this integral and not helpful for the proof I assume. The result can perhaps be found in a table of Hankel transforms but I doubt it will be given alongside a proof. My idea and calculations so far are the following. Using the series definition of the Bessel function and then switching summation and integration we have \begin{align}
\int_0^\infty t^{\nu +1} J_\nu(\beta t) e^{-\alpha t} \, dt &= \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma(\nu+m+1)} \int_0^\infty t^{\nu+1} \left( \frac{\beta t}{2} \right)^{2m+\nu} e^{-\alpha t} \, dt \\
&= \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma(\nu+m+1)} \left( \frac{\beta}{2} \right)^{2m+\nu} \int_0^\infty t^{2(m+\nu)+1} e^{-\alpha t} \, dt \,.
\end{align} Using the integral identity \begin{equation}
\int_0^\infty x^n e^{-ax} \, dx = \frac{\Gamma(n+1)}{a^{n+1}}
\end{equation} we can write this as \begin{equation}
\sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma(\nu+m+1)} \left( \frac{\beta}{2} \right)^{2m+\nu} \frac{\Gamma(2(m+\nu+1))}{\alpha^{2(m+\nu+1)}} \,.
\end{equation} Now my idea was to use the duplication formula for the Gamma function \begin{equation}
\Gamma(z)\Gamma(z+\tfrac{1}{2}) = 2^{1-2z} \sqrt{\pi} \, \Gamma(2z)
\end{equation} which leads to \begin{gather}
\sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma(\nu+m+1)} \left( \frac{\beta}{2} \right)^{2m+\nu} \frac{\Gamma(\nu+m+1) \Gamma(\nu+m+\tfrac{3}{2})}{2^{1-2(m+\nu+1)} \sqrt{\pi} \, \alpha^{2(m+\nu+1)}} \\
= \frac{2^{\nu+1} \beta^\nu}{\sqrt{\pi} \alpha^{2(\nu+1)}} \sum_{m=0}^\infty \frac{(-1)^m}{m!} \left( \frac{\beta^2}{\alpha^2} \right)^m \Gamma(\nu+m+\tfrac{3}{2}) \,.
\end{gather} I hope that I made no mistake while simplifying the expression. Now I'm stuck at the evaluation of the sum. So far I see not much resemblance to the desired result. Using this approach I was able to show that \begin{equation}
\int_0^\infty t^{\nu +1} J_\nu(\beta t) e^{-\alpha t^2} \, dt = \frac{\beta^\nu}{(2\alpha)^{\nu+1}} \exp{(\tfrac{-\beta^2}{4\alpha})}
\end{equation} quite easily, so I think this way would work here too. I would be thankful for any input on how to proceed.","['integration', 'special-functions', 'integral-transforms', 'gamma-function']"
4745900,"Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$, $\forall n\ge 2$.","Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$ , $\forall n\ge 2$ . I have found a solution of this problem here .
The solution goes as follows: if exists then,
we have, $a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}}$ .
so, either, $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ which means, $a_n\ge 1+a_{n-1}+2\sqrt{a_{n-1}}..(1)$ .
Again, $a_{n-1}+\sqrt{a_{n-1}}\ge a_n...(2)$ So, $(1),(2)$ together gives a contradiction.
Or, we must have, $a_n=m^2$ for some $m$ which again leads to a contradiction.
Thus, no such sequence exists My doubts are: The person suggests from the expression $$a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}}$$ we get two possibilities that are, either $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ or $a_n=m^2$ .
How does that expression suggests these two possibilities? Why is $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ ?","['inequality', 'discrete-mathematics']"
4745912,Determine if a function is well defined or not,"I am doing some mathematics, and I am currently stuck on something. I do not understand this part at all, how can two slightly, minor different functions, but near identical—evaluate so differently. Exercises: Do not understand part: $$(a)\enspace f:R\to R, f(x)=\frac{x}{3},\enspace \it{Well\enspace Defined} $$ $$(b)\enspace g:J\to J, g(x)=\frac{x}{3},\enspace \it{Not\enspace well\enspace defined} $$ Why is $(a)$ well defined, whereas, $(b)$ is not? Interesting explanation found here , however, I cannot get any smarter. Reference: Discrete Mathematics for Computing, 3rd Edition by Peter Grossman EDIT 8/1/2023 4:50 PM Clarity. $R$ and $J$ are sets . EDIT: 8/1/2023 5:25 PM From the book: $N$ is the set of natural numbers (or positive integers): $\{1,2,3,4,...\}$ . $J$ is the set of integers: $\{...,-3,-2,-1,0,1,2,3,...\}$ . $Q$ is the set of rational numbers: $\{x:x=m/n\enspace for\enspace some\enspace integers\enspace m\enspace and\enspace n\}$ . $R$ is the set of real numbers.","['relations', 'functions', 'discrete-mathematics']"
4745977,Bartle's exercise to prove $\lambda(E)=\sum_{n=1}^\infty 2^{-n} \mu_n (E)$ is a measure,"Let $\lambda(E)=\sum_{n=1}^\infty 2^{-n} \mu_n (E)$ with $\mu_n:\mathcal{P}(X)\rightarrow \mathbb{R}$ a measure with $\mu_n(X)=1$ . Prove $\lambda$ is a measure. Well, we clearly have $\lambda(E)\geq 0$ and $\lambda(\emptyset)=0$ . It is interesting to note $\lambda$ is finite, because $\mu_n(E)\leq \mu_n(X\setminus E)+\mu_n(E)=\mu_n(X)=1$ , so: $$S_k=\sum_{n=1}^k2^{-n} \mu_n(E)\leq \sum_{n=1}^k 2^{-n}\leq 1$$ Because $S_k$ is an increasing bounded sequence it must converge to the supremum $\sup_k S_k$ . If the $E_i$ are disjoint, we have: $$\lambda(\cup_{i\in \mathbb{N}} E_i)=\sum_{n=1}^\infty 2^{-n}\mu_n(\cup_{i\in \mathbb{N}} E_i)=\sum_{n=1}^\infty 2^{-n}\sum_{i=1}^\infty \mu_n (E_i)=\sum_{n=1}^\infty \sum_{i=1}^\infty2^{-n}\mu_n(E_i)$$ We only need to change the order of summation to conclude the additive property of the measure. I am aware convergent series of positive terms (with one summing index) are commmutatively convergent because if $b_m=a_{\phi(m)}$ is a reordering ( $\phi: \mathbb{N}\rightarrow \mathbb{N}$ where $\phi$ is a bijection) of the $a_n$ , for every fixed $n$ , there is $m$ large enough, say $\max_{i=1,...,n}\{ \phi^{-1}(i)\}$ such that $\sum_{k=1}^n a_k \leq \sum_{k=1}^m b_k \leq \sup \sum b_n $ . Therefore, $ \sup \sum a_n\leq \sup \sum b_n$ . A similar inequality follows from taking $n$ large enough and therefore we may sum any way we want and $\sup \sum a_n=\sup \sum b_n$ . I suspect we may formally exchange the order of the double infinite summation using a similar argument, but I haven't been able to do so, because there are two supremum's involved and it gets rather convoluted...",['measure-theory']
4746028,Number of possible candidates in $n$ tests,"Suppose we want to choose $n$ candidates out of a sufficiently large class of people. Then we give $n$ tests to all these people, assuming that in each test, we have a complete order of their scores where any two of them are distinct. Then, we choose the $n$ candidate as follows.
We choose one test, and select the person with the highest score.
Then we choose another test, and select the person with the highest score among those who have not been chosen in the first test.
We repeat this process for all tests, and finally we choose $n$ candidates out. The question is, at most how many people MAY be selected because of how we order the tests, if the $n$ tests are fixed but unknown? An example for $n = 2$ is as follows. Suppose the order of people in the two tests are: Test 1: A > B > C > ... Test 2: A > C > B > ... Then, if we choose Test 1 first, the two candidates will be A and C; otherwise, the two candidates will be A and B. Therefore, there are 3 possible candidates, and this is the largest number we can get across all possible results of the two tests. I can only get an lower bound of $2n - 1$ , achieved when the first $n-1$ people in every test is the same. I guess this should be also an upper bound, but cannot prove it or give a counterexample.",['combinatorics']
4746073,Are any quotients of braid groups non-trivial free products?,"The braid group $ B_1 $ is trivial and the braid group $ B_2 $ is isomorphic to $ \mathbb{Z} $ . The braid group $B_3$ has the property that its central quotient (i.e., $B_3 / Z(B_3)$ ) is isomorphic to the modular group $\mathrm{PSL}(2,\mathbb{Z})$ . The modular group is known to be isomorphic to the free product of $(\mathbb{Z}/2\mathbb{Z}) \ast (\mathbb{Z}/3\mathbb{Z})$ . The Braid group $ B_4 $ surjects onto $ B_3 $ , so there is also a quotient of $ B_4 $ which is the free product $(\mathbb{Z}/2\mathbb{Z}) \ast (\mathbb{Z}/3\mathbb{Z})$ . I'm curious about the extent to which this is true for braid groups $B_n$ for $n \geq 5 $ . Question 1: For which $n \geq 5$ is it true that there exists two non-trivial groups $G$ and $H$ such that a quotient of $ B_n $ is isomorphic to $G \ast H$ ? I'm also curious about a weaker version of this statement where we allow amalgamation over some shared finite subgroup. Question 2: For $n \geq  5$ is it true that there are groups $G$ and $H$ with common finite proper subgroup $A$ such that $B_n/Z(B_n)$ is isomorphic to $G \ast_A H$ ? This question is a follow-up to Are the central quotients of braid groups non-trivial free products?","['trees', 'group-theory', 'free-groups']"
4746076,"Prove that $\log\left(1+\frac{1}{x^2}\right)$ is $L^1$ on $(0,\infty)$.","Let $f(x) = \log(1+\frac{1}{x^2})$ . I am trying to show $f \in L^1(\mathbb{R}^+)$ where $\mathbb{R}^+ = (0,\infty)$ . What I have done so far Since $f$ is strictly positive, I just have to show that $\int_{\mathbb{R}^+}f < \infty$ . I already proved that this is true on $(\varepsilon,\infty)$ for any $\varepsilon>0$ : since $\log(1+1/x^2) \leq 1/x^2$ for all $x > 0$ , we have $$
\int_{(\varepsilon,\infty)}\log\left(1+\frac{1}{x^2}\right)dx \leq \int_{(\varepsilon,\infty)}\frac{1}{x^2}dx = \frac{1}{\varepsilon},
$$ so $\int_{(\varepsilon,\infty)}f < \infty$ , where I used the Riemann integrability of $1/x^2$ . But here I run into trouble where I can't figure out how to bring the lower bound to zero, as $1/\varepsilon$ blows up as $\varepsilon \to 0$ . I attempted to use the fact that for any $\alpha >0$ , $$
\lim_{x \to \infty}\frac{\log(1+x)}{x^{\alpha}} = 0,
$$ i.e. there must exist an $M_{\alpha}>0$ such that $x>M_{\alpha}$ implies that $\log(1+x)\leq x^{\alpha}$ . Letting $z = 1/x^2$ , we see that for $x<1/\sqrt{M_{\alpha}}$ we have $$
\log\left(1+\frac{1}{x^2}\right)\leq \frac{1}{x^{2\alpha}}.
$$ Then we set $\alpha = 1/3$ or something like that, so for $x < 1/\sqrt{M_{1/3}} = \varepsilon$ we have $f(x)\leq 1/x^{2/3}$ . I'm thinking then to use some sort of Monotone Convergence Theorem argument, i.e. let $f_n(x) = f(x)\chi_{[1/n,\varepsilon]}$ , making sure to start at large enough $n$ such that $1/n < \varepsilon$ , then $$
\int_{(0,\varepsilon)}f_n(x)dx = \int_{(0,\varepsilon)}\log\left(1 + \frac{1}{x^2}\right)\chi_{[1/n,\varepsilon]}dx \leq \int_{1/n}^{\varepsilon}\frac{1}{x^{2/3}}dx = 3\left(\sqrt[3]{\varepsilon} - \frac{1}{\sqrt[3]{n}}\right),
$$ where I again used Riemann integrability, which implies Lebesgue integrability. Sending $n \to \infty$ and using monotone convergence theorem we have that $\int_{(0,\varepsilon)}f(x)dx \leq 3 \sqrt[3]{\varepsilon}< \infty$ . My main question I have shown then that $f$ is Lebesgue integrable on $(0,\varepsilon)$ for sufficiently small $\varepsilon$ , and also that it is Lebesgue integrable on $(\varepsilon,\infty)$ for any $\varepsilon>0$ . Is it possible to ""glue"" these results together to get that $f\in L^1(\mathbb{R}^+)$ ? This solution seems kind of sloppy and disconnected to me, but it's the closest I could come to a solution and I'd really appreciate any help someone could provide. Please let me know if something is unclear, thank you!","['lebesgue-integral', 'real-analysis']"
4746090,"Let $A$ be a unital $C^*$-algebra, $a\in A,\ x,y\in A_{sa}$. Does there exist a state $\phi$ on $A$ such that $\phi(xa^*ay)=\lVert a\rVert^2\phi(xy)$?","If $A$ is a commutative, unital $C^*$ -algebra, then $A=C(X)$ for some compact, $T_2$ space $X$ . Then $a=f,x=g$ and $y=h$ are continuous functions on $X$ . Then there is $x_0\in X$ such that $\lVert f\rVert=|f(x_0)|$ . Take $\phi=\hat{x_0}$ . Then $\phi(xa^*ay)=g(x_0)|f(x_0)|^2h(x_0)=\lVert f\rVert^2\phi(gh)=\lVert a\rVert^2\phi(xy)$ . I'm doubtful about the non-commutative case here. If $x,y=1$ , then the we consider the (commutative) $C^*$ -algebra generated by $a^*a$ , and then extend to whole of $A$ by Hahn-Banach. But here $x,y,a^*a$ may not commute with each other, so the $C^*$ -algebra generated by $x,y,a^*a$ may not be commutative. Therefore, I cannot apply the above argument. Can anyone help me with the non-commutative case? Thanks for your help in advance.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4746100,Derivative of integral of function over n-sphere is the flux outwards - step in proof involving partition of unity.,"Let $N:\mathbb R^{n+1} \to \mathbb R^{n+1}$ be a vector field defined by $N(x) = \frac{x}{\lVert x \rVert}$ . Let $f:\mathbb R^{n+1} \to \mathbb R$ be a $C^1$ function. Given a coordinate system $\mathbb X:U\to S_1^n$ for $U$ open in $\mathbb R^n$ , we define $\mathbb X_r (u) = r \mathbb X(u)$ . $\mathbb X_r$ is indeed a coordinate system. The next step would be to prove the following: Using partition of unity, show we can assume w.l.o.g $\operatorname{supp}f|_{S^n_r} \subset \operatorname {Im}(\mathbb X_r)$ . This is the first part in an exercise in which the following is proven: $$ \frac{d}{dr} \left(\frac{1}{r^n} \int_{S^n_r}fd\sigma \right)=\frac{1}{r^n}\int_{S^r_n}\langle\nabla f,N \rangle d\sigma$$ I usually try my best to have some sort of attempt in finding a solution before posting here, but I'm not sure where to begin. I know the n-sphere is compact so given  coordinate systems with $U_\alpha$ as their domains we get an open cover for $S^n_r$ we have a finite subcover that can induce a partition of unity. But I don't really know how to proceed. Would appreciate any help. A small note: This exercise was given in an analysis over manifolds I'm taking, but we haven't really covered anything of substance in differential geometry or more advanced fields (the most ""advanced"" integration theorem we've seen was Stokes' theorem), and so I'm not sure if the differential geometry tag is appropriate here.","['multivariable-calculus', 'differential-geometry', 'real-analysis']"
4746115,Geometric interpretation of dual Variety.,"While I was reading the book, Discriminants, Resultants, and Multidimensional Determinants by Gelfand, Kapranov & Zelevinsky, I came upon the following statement: Let $X \subseteq \mathbb{P}^n$ be a smooth projective variety such that $X$ is not contained in any hyperplane. Let $H \subseteq \mathbb{P}^n$ an hyperplane and $X^\vee$ the dual variety to $X$ . Then the following statements are equivalent: $H \in X^\vee$ . $X \cap H$ is singular (as a scheme). My attempt on $2 \Rightarrow 1$ was to go by contradiction but I could not finish the proof. On the other hand, for $1 \Rightarrow 2$ I could only prove it in the case of plane curves by the means of the Gauss map. I would like to know how to prove both implications.","['algebraic-geometry', 'projective-geometry', 'projective-space']"
4746144,How to approach the following differential equation,"I have a differential equation of the form $$
\frac{\mathrm{d}g}{\mathrm{d}x} = f(x) + \int_0^x g(y)f(x-y)\mathrm{d}y + \alpha g(x).
$$ $f$ is a monotonous decreasing function, satisfying $\int_0^\infty f(x)\mathrm{d}x = 1$ . To solve it, I thought of getting the second derivative to handle the integral but the convolution does not let me get rid of it. Laplace transform might work here, but I am not sure the integral term on the right side qualifies as convolution such that I can use Laplace transform of $f$ . Before trying to find the Laplace transform of $f$ , which would be pretty messy, I wanted to ask if this is the best way to approach such a differential equation?","['convolution', 'ordinary-differential-equations', 'integro-differential-equations']"
4746160,Uniformly pick a sphere's center...,"Let's pick the point of the center of a sphere on a 3-d axis the $x$ coordinate, $y$ coordinate, and $z$ coordinate of the sphere's center, $(x, y, z)$ are each separately from a uniform distribution $[0, 1] $ From these coordinates, we make a sphere with radius $1$ . What is the percent of the surface area (on average) that lies within the cube region with vertices at: $(0,0,1), (1,0,1), (0,1,1), (1,1,1),$ $(0,0,2), (1,0,2), (0,1,2), (1,1,2)$ So my first thought was to just assume that $x,y,z$ are each $0.5$ since that is the expected value of each (or the average) and then calculate it from there, but I know that that logic is flawed because of monte carlo simulations- although I can't figure out why. Also, I know that the absolute upper bound would be $1/6$ as the percent of the surface area that is in the cube region above it's center on average should be the same percent of the surface area that is in the cube region below and likewise the the $4$ around it as well by symmetry. It cannot equal $1/6$ because some of the non orthagonally adjacent cube regions will have some of the surface area in them.","['analytic-geometry', 'multivariable-calculus', 'calculus']"
4746197,Very hard AM - GM inequality question: $\sqrt{a + \sqrt{b + \sqrt{c}}} \ge \sqrt[n]{abc}$,"Find the sum of all positive integers $n,$ where the inequality $$\sqrt{a + \sqrt{b + \sqrt{c}}} \ge \sqrt[n]{abc}$$ holds for all nonnegative real numbers $a,$ $b,$ and $c.$ What I have tried: I have already tried squaring both sides but that got me nowhere. I also have tried the AM - GM inequality and this is what I have: $$\frac {a + b + c + (n - 3) \cdot 1} {n} \ge \sqrt [n] {abc}.$$ I'm mainly looking for hints, but answers are welcome.","['contest-math', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
4746250,"Show, using presentations, that $\Bbb Z_m\times\Bbb Z_n\cong\Bbb Z_{{\rm lcm}(m,n)}\times \Bbb Z_{\gcd(m,n)} .$","Note: This is an alternative-proof question and thus is not a duplicate. The Question: Show, using group presentations, that $$\Bbb Z_m\times\Bbb Z_n\cong\Bbb Z_{{\rm lcm}(m,n)}\times \Bbb Z_{\gcd(m,n)}.$$ Motivation: I was trying to answer this question in particular . . . $\Bbb Z_m \times \Bbb Z_n$ isomorphic to $\Bbb Z_{\operatorname{lcm}(m,n)}\times \Bbb Z_{\gcd(m,n)}$ . . . using only Tietze transformations. (Why not, eh?)  But I got stuck. Thoughts: Following the initial setup in the question, I began with By this standard result , we have $$\begin{align}
\Bbb Z_m\times \Bbb Z_n
&\cong\langle z,w\mid z^m, w^n, zw=wz\rangle\\
&\cong \langle x,y,z,w\mid x=zw, y=z^{m/d}w^{n/d}, z^m, w^n, zw=wz\rangle\\
&\cong\langle x,y,w\mid y=(xw^{-1})^{m/d}w^{n/d}, (xw^{-1})^m, w^n, (xw^{-1})w=w(xw^{-1})\rangle\\
&\cong\langle x,y,w\mid y=(xw^{-1})^{m/d}w^{n/d}, (xw^{-1})^m, w^n, xw=wx\rangle\\
&\cong\langle x,y,w\mid y=x^{m/d}w^{(n-m)/d}, x^m=w^m, w^n, xw=wx, xy=yx, yw=wy\rangle\\
&\cong\langle x,y,w\mid w^{{(n-m)/d}}=x^{-m/d}y, x^m=w^m, w^{n}, xw=wx, xy=yx, yw=wy\rangle\\
&\cong\langle x,y,w\mid w^{n-m}=x^{-m}y^d, w^{{(n-m)/d}}=x^{-m/d}y, x^m=w^m, w^{n}, xw=wx, xy=yx, yw=wy\rangle\\
&\cong\langle x,y,w\mid w^n=y^d, w^{{(n-m)/d}}=x^{-m/d}y, x^m=w^m, w^{n}, xw=wx, xy=yx, yw=wy\rangle\\
&\cong\langle x,y,w\mid w^n=y^d, w^mw^{n/d}=w^{m/d}x^{-m/d}y, x^m=w^m, w^{n}, xw=wx, xy=yx, yw=wy\rangle
\end{align}$$ That's about it.  How ugly! Surely there's a way to manipulate either this answer to the linked question (in the Motivation section) . . . Fix $u,v\in\Bbb Z$ with $un+vm=d$ (Bezout).
The map $$\Bbb Z_{\operatorname{lcm}(n,m)}\times\Bbb Z_{\gcd(n,m)} \to\Bbb Z_m\times\Bbb Z_n$$ $$ (a+\operatorname{lcm}(n,m)\Bbb Z,b+\gcd(n,m)\Bbb Z)\mapsto(ua+\tfrac mdb+m\Bbb Z,va-\tfrac ndb+n\Bbb Z)$$ is well-defined(!) and clearly a group homomorphism.
For the element on the left to be in the kernel, $ua+\tfrac mdb$ must be a multiple of $m$ and $va-\tfrac ndb$ a multiple of $n$ .
But then $$\frac nd\left(ua+\frac mdb\right)+\frac md\left(va-\frac ndb\right) 
=\frac{nu+vm}{d}a=a$$ is a multiple of $\frac{nm}d=\operatorname{lcm}(n,m)$ , i.e., we may as well assume that $a=0$ . Then $\frac mdb$ must be a multiple of $m$ , i.e., $b$ a multiple of $d$ , i.e. $b\equiv 0$ . We conclude that the kernel is trivial and our homomorphism injective. As both groups are finite of same order, the homomoprhism must be an isomorphism. . . . or this answer to a slightly different question : Write $m=dm', n=dn', d=mu+nv$ . Then $l=m'n=mn'$ . These row and columns operations prove that $\mathbb{Z}_m\oplus \mathbb{Z}_n \cong \mathbb{Z}_d\oplus \mathbb{Z}_l$ : $$
A=\pmatrix{ m & 0 \\ 0 & n}
\to \pmatrix{ m & mu \\ 0 & n}
\to \pmatrix{ m & mu+nv \\ 0 & n}
= \pmatrix{ m & d \\ 0 & n}\\
\to \pmatrix{ 0 & d \\ -m'n & n}
= \pmatrix{ 0 & d \\ -l & n}
\to \pmatrix{ 0 & d \\ -l & 0}
\to \pmatrix{ d & 0 \\ 0 & l}=B
$$ An explicit isomorphism can be written by collecting the row and columns operations into two matrices $P,Q$ so that $B=PAQ$ : $$
P = 
\pmatrix{ 1 & 0 \\ -n' & 1}
\pmatrix{ 1 & v \\ 0 & 1}
=\pmatrix{1 & v \\ -n' & 1 - v n'}
\\
Q =
\pmatrix{ 1 & u \\ 0 & 1}
\pmatrix{ 1 & 0 \\ -m' & 1}
\pmatrix{ 0 & -1 \\ 1 & 0}
= \pmatrix{u & -1 + u m' \\ 1 & m'}
$$ If $e_1, e_2$ is the canonical basis for $\mathbb Z^2$ , then the basis $f_1, f_2$ given by $F=Q^{-1}E$ is such that this diagram commutes: $$
\matrix { \mathbb Z^2 , \{ e_1, e_2\} & \to & \mathbb Z^2, \{ f_1, f_2\} \\
\downarrow & & \downarrow \\
\mathbb{Z}_m\oplus \mathbb{Z}_n & \to & \mathbb{Z}_d\oplus \mathbb{Z}_l
}
$$ This isomorphism does not use prime factorizations nor explicitly the Chinese Remainder Theorem. Please help :)","['alternative-proof', 'group-presentation', 'group-theory', 'combinatorial-group-theory']"
4746266,"If $xy+yz+zx+xyz=4,$ prove $\frac{1}{\sqrt{x+1}}+\frac{1}{\sqrt{y+1}}+\frac{1}{\sqrt{z+1}} \leq 1+\frac{2}{\sqrt{3}}.$","Problem. Let $x,y,z\ge 0: xy+yz+zx+xyz=4.$ Prove that $$\frac{1}{\sqrt{x+1}}+\frac{1}{\sqrt{y+1}}+\frac{1}{\sqrt{z+1}} \leq 1+\frac{2}{\sqrt{3}}.$$ I tried to use the substitution $x=\dfrac{2a}{b+c};y=\dfrac{2b}{a+c};z=\dfrac{2c}{b+a}.$ The original inequality becomes $$
\sqrt{\frac{a+b}{a+b+2 c}}+\sqrt{\frac{b+c}{b+c+2 a}}+\sqrt{\frac{c+a}{c+a+2 b}} \leq 1+\frac{2}{\sqrt{3}},
$$ for all $a,b,c\ge 0: a+b+c>0.$ Hope we can find some ideas. Thank you.","['multivariable-calculus', 'algebra-precalculus', 'lagrange-multiplier', 'inequality']"
4746348,"If A,B indenpendent, and P(C|A),P(C|B) >P(C), what is relation P(C|A,B) with P(C)? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question If A,B indenpendent, and $P(C|A),P(C|B) >P(C)$ , what is relation $P(C|A,B)$ with $P(C)$ ?(> or <)","['conditional-probability', 'bayesian', 'probability-theory', 'probability']"
4746368,Integration by parts - multivariate case,"As part of a larger problem, I realised I don't quite know how to do integration by parts in the multivariate case. I looked up some formulas, but I couldn't get them to apply. For example, how would you do integration by parts on: $$\int_{0}^{1}\int_{0}^{1}u_{xx} v\;dxdy$$ where $u$ and $v$ are functions of $x$ and $y$ and we are integrating across a unit square. The subscripts denote partial differentiation with respect to that variable. Along the boundary of the square we have $u=v$ . Many thanks!",['multivariable-calculus']
4746377,Number of real roots of equation,"Given $\cos(x)=ax$ , express the number of real roots as a function of $a$ . I've found that the sign of $a$ doesn't matter since $cos(x)$ is an even function, and that the number of roots increases as the value of $a$ decreases, for infinitely many roots as $a$ approaches zero. But I have no idea where to go from there. I know that $\cos(x)=x$ is unsolvable analytically, so is this question also only solvable numerically?","['roots', 'real-analysis']"
4746403,Indefinite or definite Trigonometric Integral,"$$\DeclareMathOperator{\sech}{sech} \int \frac{- (\arctan{ \frac{(a + \sqrt{a})x}{a-1}}) \sech^2(\pi x)(-2 \pi x + sinh(2 \pi x)) \tanh^2{\pi x}}{x^4}dx =???? $$ Alternatively: $$\int_{-\infty}^{\infty} \frac{- (\arctan{ \frac{(a + \sqrt{a})x}{a-1}}) \sech^2(\pi x)(-2 \pi x + sinh(2 \pi x)) \tanh^2{\pi x}}{x^4}dx =???? $$ For some $\forall a \in \mathbb R_{\gt 1}$ This integral came up in my research and I thought, ""Oh I'll just run it through Wolfram Alpha"". That did not work. But now I'm greatly confused as to why. I posted this question previously, but it was closed because individuals needed some additional information, which I've provided below. I will settle for a calculation in the range of integration from $-\infty$ to $\infty$ , but I really am asking about the anti-derivative. I was going to let this go, but now I have to know why this is so difficult to calculate in a reasonable amount of time.  Anyone know? (Naive) Motiviation: Typically (that is to say in my experience) these types of integrals of trigonometric products can be broken down using existing methods; We can usually reach some ""solved point"" with some additive and/or multiplicative constants notwithstanding. ""What is so special about this particular function?"" is another way to phrase what I'm asking. Some background on preferences and pitfalls when dealing with trigonometric integrals: Showing $\int_0^{\int_0^u{\rm sech}vdv}\sec vdv\equiv u$ and $\int_0^{\int_0^u\sec vdv}{\rm sech} vdv\equiv u$ In trig substitutions, why favor $\sin$, $\tan$, $\sec$, $\sinh$, $\cosh$, $\tanh$ over $\cos$, $\cot$, $\csc$, etc? https://en.wikipedia.org/wiki/Integral_of_secant_cubed EDIT: I changed the scalar argument of the the arctan function, so calculating the definite integral is not so obvious. Thanks to @KevinDietrich for the catch.","['integration', 'indefinite-integrals', 'definite-integrals']"
4746443,Sectional curvature independent of basis.,"Given a semi-Riemannian manifold $(M,g)$ ; for $p \in M$ , we define the sectional curvature of a non-degenerate $2$ -plane $\sigma$ with basis $\{u,v\}$ as $$K(\sigma) := K(u,v) = \frac{R(u,v,v,u)}{Q(u,v)}$$ where $R$ is the Riemann-curvature tensor and $$Q(u,v) := g(u,u)g(v,v)-g(u,v)^2.$$ We set $\tilde{u} = au+bv$ and $\tilde{v} = cu+dv$ . Could someone guide me through the computation that $$Q(\tilde{u},\tilde{v}) = (ac-bd)^2Q(u,v)?$$","['curvature', 'semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
4746476,Bound an integral depending on parameters,"Let us define $$E(t) = \int_{-T}^{t - \lambda(t)^2} \frac{ds}{|\ln(T - s)|^2(t - s)} \quad \text{and} \quad E^{\varepsilon}(t) = \int_{-T}^{t - \lambda(t)^2 - \varepsilon}\frac{ds}{|\ln(T - s)|^2(t - s)},$$ for $t \in [-T/4, T]$ , $\lambda(t)$ defined by $$\lambda(t) = \frac{T - t}{|\ln(T - t)|^2},$$ and $0 < \varepsilon < T$ taken arbitrarily small (I aim to take $\varepsilon \to 0$ eventually). I aim to prove that, $\exists C > 0$ independent of $\varepsilon$ such that $$|E^\varepsilon(t) - E^\varepsilon (T)| \le C|E(t) - E(T)|.$$ Using the answer given in this post we can show that we actually don't need that absolute values, so that I aim to prove $$E^\varepsilon(t) - E^\varepsilon (T) \le C(E(t) - E(T)) \quad \Leftrightarrow \quad  0 \le (CE(t) - E^\varepsilon(t)) - (CE(T) - E^\varepsilon(T)).$$ Now note that $$E(T) = \frac{1}{|\ln(2T)|} \quad \text{and} \quad E^\varepsilon(T) = \frac{1}{|\ln(2T)|} - \frac{1}{|\log \varepsilon|},$$ which in the end leads to $$0 \le (C - 1) \left(\int_{-T}^{t - \lambda(t)^2} \frac{1}{|\ln(T -s)|^2(t - s)}ds - \frac{1}{|\ln(2T)|}\right) + \int_{t - \lambda(t)^2 - \varepsilon}^{t - \lambda(t)^2} \frac{1}{|\log(T -s)|^2(t - s)}ds - \frac{1}{|\log(\varepsilon)|}.$$ Basically, if I denote $G^C(t) = CE(t) - E^\varepsilon(t)$ , I just want to show that $G^C(T) \le G^C(t)$ for every $t \in [-T/4, T]$ . The annoying thing is that this $E(t)$ is hard to manipulate as we cannot explicitly compute the integral. I computed $G^C(t) - G^C(T)$ using Python for $T=4.10^{-5}$ , $C = 4$ and $\varepsilon = T.10^{-5}$ (see picture below). We see that the green curve is always ""far"" above zero in the interval we consider, so that the desired inequality seems reasonable (I did it for various other epsilon and the shape remain roughly the same). However, I really struggle to prove it. I thought about computing the first derivative of $G^C(t)$ and show that it is decreasing in a certain range but the explicite form of the derivative is quite ugly so that I really struggle to prove the monotonicity. I also thought about showing that this function is concave, but this would involve computing the second derivative whose form is even worse to manipulate. Any idea to help would be greatly appreciated. Some precision: It is actually fine if I can have an inequality of the type $$|E^\varepsilon(t) - E^{\varepsilon}(T)| \le C\left(|E(t) - E(T)| + \frac{1}{|\ln(\varepsilon)|^2}\right),$$ so that I allow $|E^\varepsilon(t) - E^{\varepsilon}(T)|$ to be slightly bigger than $C|E(t) - E(T)|$ (with a difference of order $1/|\ln(\varepsilon)|^2$ ). I manage to show that $$|E^\varepsilon(t) - E^{\varepsilon}(T)| \le C\frac{1}{|\ln(\varepsilon)|^2}$$ when $t \in [T - \varepsilon, T]$ . If I could have $$|E^\varepsilon(t) - E^{\varepsilon}(T)| \le C|E(t) - E(T)| $$ in the range $[-T/4, T-\varepsilon]$ I would be delighted. None of my attempt have been successful so far.","['integration', 'real-analysis', 'calculus', 'upper-lower-bounds', 'inequality']"
4746506,"find all $n$: for all $n$ real number, you could make all of them equal in finite number of steps, each step replace $a,b$ to $a+b, a+b$","With a list of $n$ real numbers, in each step you can replace 2 arbitrary elements $a,b$ to $a+b, a+b$ . Find all $n$ such that no matter what is the initial $n$ elements on the list, you can make all of them equal in finite numbers of steps. In the original problem, the operation is $a,b$ to $\frac{a+b}2, \frac{a+b}2$ which is easier since the sum of all elements on that list is invariant. I tried to solve this problem for days with the goal of proving $n = 2^k$ but can't prove or disprove the case $n = 6$ . I'm not sure if the solutions for $n$ remain the same if we change ""real numbers"" to ""integers"". But for the real case, I think choosing the $n$ initial elements linear independent over $\mathbb{Q}$ might help. update: solution for the case of $2 \not|n$ and the case of $2|n$ and the initial list being a rational list is referenced in the comment bellow update2: I found a solution for the general case: every even number work and every odd number don't","['elementary-number-theory', 'combinatorics']"
4746529,When do function value and gradient along a curve determine the function in a neighbourhood of the curve?,"I have two $C^1$ functions $ f, g : U \subseteq \mathbb{R}^n \rightarrow \mathbb{R} $ and a curve $c : [0, 1] \rightarrow U $ such that $ f(x) = g(x)$ and $\nabla f(x) = \nabla g(x)$ for all points $x$ on the curve $c$ . Are you aware of any conditions that would imply that $ f(x) = g(x) $ for all $x$ in a small neighbourhood of $c$ ? In some sense, this would be similar to the identity theorem , but ideally without requiring that the functions are analytic and instead knowing some information about the gradient. Any references are appreciated, thank you!","['multivariable-calculus', 'calculus', 'vector-analysis']"
4746562,"How to prove that ABC is isosceles where $\angle{BAC}=20^\circ$ and $\sqrt[3]{a^3+b^3+c^3-3abc}= \min \{b,c\}$","Let $\bigtriangleup{ABC}, \angle{BAC}=20^\circ$ and $\sqrt[3]{a^3+b^3+c^3-3abc}=
\min \{b,c\}$ . How to prove that ABC is isosceles?
I try to use that $a^3\cos(B−C)+b^3\cos(C−A)+c^3\cos(A−B)=3abc$ .",['geometry']
4746570,"Mapping class group of surfaces, free products, and trees","Let $ \Sigma $ be a surface, possibly with boundary. Let $ MCG(\Sigma) $ denote the mapping class group. Is it true that $ MCG(\Sigma) $ has a quotient which is a nontrivial free product $ A \ast B $ if and only if that quotient is $
A \ast B \cong C_2 \ast C_3 \cong PSL(2,\mathbb{Z})
$ ?
Correspondingly, $ MCG(\Sigma) $ has a transitive action on a regular tree if and only if that tree is $ 3 $ -regular? Moreover, is it true that that the only surfaces for which these (equivalent) properties occur are $ MCG(\Sigma)=SL(2,\mathbb{Z}),B_3,B_4 $ corresponding to the torus, the disk with 3 punctures and the disk with 4 punctures, respectively? I think this follows from work of Culler and Vogtmann , sections ""Mapping class groups of surfaces of positive genus"" and ""Braid groups and mapping class groups of punctured spheres.""  But I'm not well-versed enough in this area to feel confident. For related answers see Are any quotients of braid groups non-trivial free products? and https://mathoverflow.net/questions/451191/action-of-braid-groups-on-regular-trees/451204#451204","['mapping-class-group', 'trees', 'geometric-topology', 'free-groups', 'group-theory']"
4746594,About the square root of a positive operator,"Let $A$ be a positive bounded linear operator on a Hilbert space $\mathscr{H}$ , that is, $\langle \psi,A\psi\rangle \ge 0$ holds for every $\psi \in \mathscr{H}$ . We write this property as $A \ge 0$ . I am reading Reed & Simon's proof of the following result. Theorem: Let $A$ be positive, bounded and linear. Then there exists a unique bounded linear operator $B$ such that $B \ge 0$ and $B^{2} = A$ . Moreover, $B$ commutes with every bounded linear operator which commutes with $A$ . Suppose $\|A\| \le 1$ . To define $B$ , one considers the power series: $$B = \sqrt{A} = \sqrt{I - (I-A)} = 1+c_{1}(I-A) + c_{2}(I-A)^{2} + \cdots, $$ which is absolutely convergent when $\|I-A\| \le 1$ . My first question is: how to show that $B^{2} = A$ ? I know that, because the series for $B$ is absolutely convergent, one can square it and write it as: $$B^{2} = \sum_{n=0}^{\infty}a_{n}(I-T)^{n}$$ with: $$a_{n} = \sum_{m=0}^{n}c_{m}c_{n-m} $$ and $c_{0} = 1$ . I also know that: $$1-z = \sum_{n=0}^{\infty}a_{n}z^{n}.$$ Question 1: Is this enough to prove that $B^{2} = I - (I-A) = A$ ? Question 2: To prove uniqueness, the authors assume $B'$ is another positive bounded linear operator such that $(B')^{2} = A$ . Then, they claim that $B - B'$ is self-adjoint. I don't see, however, why this is true. A positive bounded linear operator is only self-adjoint in the case of complex Hilbert spaces, but there is no mention about the underlying Hilbert space to be complex in their theorem. So, why is this true?","['proof-explanation', 'operator-theory', 'analysis', 'hilbert-spaces', 'functional-analysis']"
4746632,what's the fastest growing series such that it is bounded in $\ell^2$?,"How fast a sequence can grow in $\ell_1$ norm before it diverges in $\ell_2$ ? Let $\{a_n\} \in \ell_2(\mathbb{R})$ , i.e. such that $\sum_{n=1}^\infty a_n^2 < \infty$ , we want to find $\sum_{n=1}^N |a_n| = \Theta(N^{\varepsilon})$ (as $N \to \infty$ ) and $\varepsilon$ be the largest possible, what is the value of $\varepsilon$ ? Some remarks: By Hölder inequality, we have a necessary condition that definetely, $$
cN^\varepsilon \le \sum_{k=1}^N |a_n| \le N^{1/2} \left(\sum_{k=1}^N a_n^2 \right)^{1/2}
$$ that is $c^2 N^{-(1-2\varepsilon)} \le \sum_{n=1}^N a^2_n$ , because the series in $\ell^2$ must converges, we have that $\varepsilon \le 1/2$ . The sequence $a_n := (n+1)^{\varepsilon} - n^{\varepsilon}$ grows as $N^\varepsilon$ for $0 < \varepsilon < 1/4$ , and it satisfies the conditions: $$ \sum_{n=1}^\infty |a_n| = +\infty$$ and $$\sum_{n=1}^\infty a^2_n \le \varepsilon^2 \sum_{n=1}^\infty ((n+1)^{2\varepsilon-2}) \le \zeta(2-2\varepsilon) < \infty$$ where we used Taylor approximation and the fact that the $\zeta(\beta)$ converges as $\beta = 2 - 2\varepsilon > 1$ . Finally, polynomial ''growth'' is the fastest growth possible or exponential (for very low parameters) is possible? (I conjecture that a growth of $\Theta(N^\varepsilon (\ln N)^\gamma)$ is possible by just adding some harmonic sequence to an already polynomial growing sequence, i.e. $\frac{(n+1)^{\varepsilon + \gamma} - n^{\varepsilon + \gamma}}{n^\eta}$ but I'm not so sure).","['sequences-and-series', 'analysis', 'real-analysis']"
4746646,$< \infty$ versus $< +\infty$?,"In several books and articles (mainly those in Functional Analysis and Differential Geometry), when the authors wish to say a certain quantity or function is finite they write $< + \infty$ as opposed to just $ < \infty$ . I know the term “finite” could refer to both the negative and positive bounds but if the authors are already writing in mathematical notation, why insist on adding a plus sign? Is $< \infty$ somehow ambiguous?","['differential-geometry', 'functional-analysis', 'infinity', 'soft-question', 'terminology']"
4746654,Deriving the Resolvent Equation,"Given a Feller semigroup $(T_t)$ on locally compact, separable metric space $S$ , I wish to show that, for $\lambda > 0$ , the resolvent $R_{\lambda}f(x) := \int_0^{\infty}e^{-\lambda t}T_tf(x)dt$ satisfies $R_{\lambda}-R_{\mu}=(\mu-\lambda)R_{\mu}R_{\lambda}$ . This is a result from Kallenberg's Foundations of Modern Probability , Theorem 17.4. Here's what I have so far: The LHS is $(R_{\lambda} - R_{\mu})(f)(x)=\int_0^{\infty}(e^{-\lambda t}-e^{-\mu t})T_tf(x)dt$ The RHS is $(\mu-\lambda)\int \int e^{-\lambda t - \mu s}T_sf(x)T_tf(x)dsdt = (\mu-\lambda)\int \int e^{-\lambda t - \mu s}T_{s+t}f(x)dsdt $ , and I am not sure how to simplify either side to get closer to the other.","['measure-theory', 'probability-theory', 'stochastic-calculus', 'partial-differential-equations']"
4746689,Construction of a circle orthogonal to two given circles and tangential to a given line,"Two circles $c$ and $d$ that intersect at points $A$ and $B$ are given. Let $p$ be a line passing through $A$ that intersects circles $c$ and $d$ at points $P_1$ and $P_2$ . Construct a circle tangential to $p$ and orthogonal to both $c$ and $d$ . What I did was that I constructed a circle $k=k(P, |PS_1|)$ so that $P$ is the midpoint of $S_1S_2$ . Then I applied  inversion about $k$ , which maps  circles $c$ and $d$ to two parallel lines $c'$ and $d'$ , and  line $p$ to circle $p'$ that passes through point $P$ . Then I found line $l$ such that it's  perpendicular to $c'$ and $d'$ and tangential to circle $p '$ . Finally, I applied inversion about $k$ again so it would map $l$ to circle $l'$ . I thought $l'$ would be the solution, but apparently it isn't. Why doesn't this work and what should I do instead? EDIT: Now I see what a dumb mistake I made, so I'm sorry for hurting yalls eyes. I still wonder how this problem could be solved using inversion. I was thinking to apply inversion about circle centered at $A$ that would map both $c$ and $d$ to lines $c'$ and $d'$ and it would map line $p$ to itself, but I don't see how that would be helpful","['euclidean-geometry', 'inversive-geometry', 'geometric-construction', 'circles', 'geometry']"
4746720,Social golfer problem with additional requirement,"I need to write a program that sorts people into groups. To give a little context: The aim of the program is to create an equitable distribution of tasks and people for a school trip. Every day the groups are reshuffled and each group is assigned a task (cleaning, cooking, ...). So far, we have put the groups together randomly by drawing items from a bag. But every year the students complain if they are never in the same group as their friends or if they always have the task ""cleaning"". So I had the idea to develop a small program that solves this problem by dividing the students into groups where the people and tasks are evenly distributed. The following specifications are requested: We have a total of x people. The people will be divided into y groups. There are z days. On each day, the groups are mixed so that ideally, after all days, each person has been in a group with every other person at least once. The distribution should be even, i.e. everyone shares a group with every other person about the same number of times. There are as many tasks as there are groups. For each day, each group is assigned one of these tasks. After all days, each person should have had each task about the same number of times. Now about the algorithm. I think a solution to the ""Social golfer problem"" ( www.mdpi.com/2073-8994/13/1/13 ) might be a way to sort people into groups. But since there is the additional requirement of Tasks (people equally distributed among all groups), I really don't know how to approach this problem. After a bit of googling ""Kirkman's Schoolgirl Problem"" and ""The Running Dinner Problem"" were some other keywords that popped up. Maybe that can be helpful? If someone has a hint or even a full algorithm I would be very happy! Thank you!","['constraint-programming', 'combinatorics', 'discrete-mathematics', 'algorithms']"
4746748,Closed form of $\int_{-\infty}^\infty \frac{\sin (\pi x)}{(x^2 - 7x + 10)(x^2 + 1)} dx$,"Find the closed form for the integral $$\mathcal I= \int_{-\infty}^\infty \frac{\sin (\pi x)}{(x^2 - 7x + 10)(x^2 + 1)} dx$$ My attempt In order to solve this integral, what I first consider is using residue theory. First of all, notice that The denominator cancels out when $x^2 - 7x + 10 = 0 \Leftrightarrow x = \frac{7 \pm \sqrt{7^2 - 40}}{2} = \begin{cases} x_1 = 2 \\ x_2 = 5 \end{cases}$ , as well as when $x^2 + 1 = 0 \Leftrightarrow x=\pm i$ . Notice that the poles $2$ and $5$ are placed on the real axis, and the only one on the upper semiplane is $i$ . From Euler's identity we know that $e^{i \pi x} = \cos(\pi x) + i \sin(\pi x)$ . Therefore, we can rewrite the integral as $$\Im \left( \int_{-\infty}^\infty \frac{e^{\pi z}}{(z^2 - 7z + 10)(z^2 + 1)} dz \right) = \Im \left( \int_{-\infty}^\infty \frac{e^{i\pi z}}{(z-2)(z-5)(z+i)(z-i)} dz \right)$$ From residue theory, we know that $$\int_{-\infty}^\infty \frac{e^{i\pi z}}{(z-2)(z-5)(z+i)(z-i)} dz = 2\pi i Res(f(z); z=i) + \pi i Res(f(z); z=2) + \pi i Res(f(z); z=5)$$ Now were I'm not confident is calculating the residues. I can apply that, as all the poles are simple, $$Res(f(z); z=i) =  \lim_{z\to i} (z-i) f(z) = \lim_{z\to i} \frac{e^{i\pi z}}{(z-2)(z-5)(z+i)} = \frac{e^{-\pi}}{18i + 14}$$ $$Res(f(z); z=2) =  \lim_{z\to 2} (z-2) f(z) = \lim_{z\to 2} \frac{e^{i\pi z}}{(z-i)(z-5)(z+i)} = \frac{-e^{2i\pi}}{15}$$ $$Res(f(z); z=5) =  \lim_{z\to 5} (z-5) f(z) = \lim_{z\to 5} \frac{e^{i\pi z}}{(z-i)(z-2)(z+i)} = \frac{e^{5i\pi}}{78}$$ So $$\mathcal I=\Im \left(2\pi i \frac{e^{-\pi}}{18i + 14} + \pi i \frac{-e^{2i\pi}}{15} + \pi i \frac{e^{5i\pi}}{78} \right)$$ From here I have two concerns: How can I simpify the resulting expresion of the integral, if it is indeed the correct answer. In my previous post for an integral, I was said that the residues are indeed the terms of the Taylor expresion (since in this integral the poles are simple, they would be the $0^{th}$ term?), but I could not figure out how to find them that way, if someone could explain me that method. Thanks so much for the helping:)","['integration', 'complex-analysis', 'solution-verification', 'closed-form', 'residue-calculus']"
4746768,Maximum number of sizes of maximal cliques for a graph?,"I was wondering what is the maximum possible number of distinct sizes of maximal cliques in a graph with fixed order $N$ , and if anyone can share or point me to a proof or literature on this topic. To clarify, if a graph has 4 vertices, it could have maximal cliques of size 1 and 2, or 1 and 3, (or all maximal cliques of size 4, all of size 3, all of size 2, etc.) but we can't have more than 2 distinct sizes of maximal cliques. I've found a way to build graphs with $\left\lfloor\frac{N + 1}{2}\right\rfloor$ sizes of maximal cliques (start at isolated vertex, then add two more vertices as an edge, then add two more vertices linked to each other and to 1 of the vertices of the edge to form a triangle, then add two more vertices linked to each other and to 2 of the vertices of the triangle to form a maximal 4-clique, etc.) My hunch is that we can't do better than $\left\lfloor\frac{N + 1}{2}\right\rfloor$ sizes of maximal cliques, but I'm not sure how to prove this or even if it is true because there are so many possible intersections between maximal cliques going on at once with higher number of vertices, and maybe there are better ways of having the maximal cliques intersect to save space than the method I described above. Any help would be much appreciated.","['graph-theory', 'extremal-graph-theory', 'combinatorics', 'discrete-mathematics']"
4746782,Holomorphic functions of the form $f'(z) = \overline{f(\overline{z})}$.,"Question: Let $R = \{z \in \mathbb{C} \colon |\Re(z)|, |\Im(z)| < 1\}$ . Determine all functions $f\colon R \to \mathbb{C}$ that are holomorphic in $R$ and satisfies $$f'(z) = \overline{f(\overline{z})},\ \text{for all}\ z \in R.$$ My attempt: Claim that the only functions are of the form $A e^z$ where $A \in \mathbb{C}$ . Suppose $f$ is such a function, then $f$ is analytic in $R$ and it admits a unique power series representation at $z=0$ that is $f(z) = \sum_{n=0}^\infty a_n z^n$ for all $z \in R$ where $a_n \in \mathbb{C}$ for all $n =0,1,2,\dots$ . By the identity we have $\sum_{n=0}^\infty a_{n+1} (n+1) z^n = \sum_{n=0}^\infty \overline{a_n} z^n$ for all $z \in R$ . By uniqueness of the power series, $\overline{a_n} = a_{n+1} (n+1)$ for all $n =0,1,2,\dots$ , where the radius of convergence is precisely $\infty$ . However, I am not sure how to proceed from here.","['complex-analysis', 'delay-differential-equations']"
4746822,Professor Lee's Introduction to Smooth Manifolds Second Edition Lemma 10.35,"I'm stuck trying to verify the proof given in the text. There are parts of the
hypothesis and proof that have nothing to do with where I'm stuck, so
in the interests of brevity, I'll give
only the part I'm having trouble with. We are given $M$ , an immersed submanifold
with or without boundary in $\mathbb{R}^n$ , and $D$ , a smooth rank- $k$ subbundle
of $T\mathbb{R}^n|_M$ . The proof begins as follows: Let $p\in M$ be arbitrary, and let $(X_1,\dots,X_k)$ be a smooth local frame for $D$ over some neighborhood $V$ of $p$ in $M$ . Because immersed submanifolds are
locally embedded, by shrinking $V$ if necessary, we may assume that it is a
single slice in some coordinate ball or half-ball $U\subseteq\mathbb{R}^n$ .
Since $V$ is closed in $U$ , Proposition 8.11(c) shows that we can complete $(X_1,\dots,X_k)$ to a smooth local frame $(\tilde{X}_1,\dots,\tilde{X}_n)$ for $T\mathbb{R}^n$ over $U$ , ... My problem lies wholly within the above three sentences. The first sentence is
fine. The second sentence starts out fine, but I don't think there is such
a thing as a coordinate half-ball in $\mathbb{R}^n$ since $\mathbb{R}^n$ has
empty boundary. (Please correct me if I'm wrong here.) So I am going to assume that Professor Lee meant to say
""assume that it is a single slice or half-slice of some coordinate ball $U\subseteq\mathbb{R}^n$ ."" With that change (which doesn't solve my problem),
I am OK with the second sentence and with $V$ being closed in $U$ . Now Proposition 8.11(c) applies to smooth vector fields along a closed subset $V\subseteq U$ . For my question, I only need to talk about one of the $X_i$ ,
so let me just call it $X$ . I have no problem composing $X$ with smooth maps and
restricting codomains so that, I can consider $X$ to be a
smooth vector field $X\colon V\to TU$ over $V$ . Recall that $V$ is a neighborhood
in $M$ and as such has a smooth structure of manifold with (possibly empty) boundary
derived from that of $M$ . Now let me
quote the definition of a smooth vector field along a subset (I've changed the
names of the variables to match the present situation): If $U$ is a smooth manifold with or without boundary and $V\subseteq U$ is an
arbitrary subset, a vector field along $V$ is a continuous map $X\colon V\to TU$ satisfying $\pi\circ X=\mathrm{Id}_V$ . We call it a smooth vector field along $V$ if for each $q\in V$ , there is a neighborhood $W$ of $q$ in $U$ and a smooth vector field $\tilde{X}$ on $W$ that agrees
with $X$ on $W\cap V$ . My problem is that given $q\in V$ , I haven't been able to find a neighborhood $W$ of $q$ in $U$ and an associated smooth vector field $\tilde{X}$ on $W$ such
that $X|_{W\cap V}=\tilde{X}|_{W\cap V}$ . I've got $(U,\phi)$ , a smooth chart
centered at $p$ for $\mathbb{R}^n$ , with $\phi(U)$ being the open cube of side $r$ centered at $0_{\mathbb{R}^n}$ and with $\phi(V)=\phi(U)\cap(Z^m\times\{0_{\mathbb{R}^{n-m}}\})$ where $m=\mathrm{dim}\,M$ ,
and $Z=\mathbb{H}$ if $p\in\partial M$ and $Z=\mathbb{R}$ otherwise. I think I
can make things work if $p$ is not a boundary point of $M$ as follows:
Let $F\colon\mathbb{R}^n\to\mathbb{R}^n$ be defined by $$F(x_1,\dots,x_m,x_{m+1},\dots,x_n)=(x_1,\dots,x_m,0,\dots,0).$$ Then $F$ is smooth, and when applying $F$ to $\phi(U)$ , it produces $\phi(U)\cap(\mathbb{R}^m\times\{0_{\mathbb{R}^{n-m}}\})=\phi(V)$ .
Thus $\tilde{X}=X\circ\phi^{-1}\circ F\circ\phi\colon U\to TU$ is smooth
and equals $X$ on $V$ . [EDIT: As pointed out by Tob Ernack, this may be smooth, and may be an extension, but it is not
a section and therefore not a solution for the empty boundary
case. So I retract the statement that I know how to make it
work at an interior point.] But this won't work when $p\in\partial M$ since then $Z=\mathbb{H}$ and $F(\phi(U))=\phi(U)\cap(\mathbb{R}^m\times\{0_{\mathbb{R}^{n-m}}\})$ which is not equal to $\phi(V)=\phi(U)\cap(\mathbb{H}^m\times\{0_{\mathbb{R}^{n-m}}\})$ and therefore cannot be composed with $X\circ\phi^{-1}$ . So, my question is, how do I show $X$ is a smooth vector field along $V$ ,
when $p\in\partial M$ ?","['vector-fields', 'submanifold', 'smooth-manifolds', 'differential-geometry']"
4746840,Is this answer for finding the roots of a polynomial rigorous enough?,"Consider the following elementary question and answer: Q: Find all the real roots of $P(x)=x^2-x$ . A: $x^2-x=0$ $x(x-1)=0$ $x=0$ or $x-1=0$ $x=0$ or $x=1$ $x=0,1$ . End. This is a typical answer given by elementary school students and teachers. However, I wonder whether this answer is actually correct or valid in the context of proofs, logic, and rigor. To me, this answer only proves ""If $x^2-x=0$ , then $x=0$ or $x=1$ "", and doesn't show the converse ""If $x=0$ or $x=1$ , then $x^2-x=0$ "". The first one is needed to show no other real roots exist, while the second is needed to show these are indeed roots, and both are needed to show that the set {0,1} contains all the roots. Questions Do the statements in the answer have an if, then or an if and only if relationship? i.e., does the answer mean "" $x^2-x=0 \iff x=0$ or $x=1$ "" or "" $x^2-x=0 \Rightarrow x=0$ or $x=1$ ""? I know that the iff relationship is true for all the equations in the answer, but I would like to know whether it is what is meant in this case, given that the iff symbol is not written down between the equations. Would the answer be correct in the eyes of a teacher who demands 100% rigor? Or would it be necessary to plug the values back into the polynomial to show the converse? Would writing the iff symbol before each equation be needed, correct, or bad practice (in proofs)? When a question says "" $P(x)=C$ . Solve for $x$ ."", is there a formal restatement of the question in symbolic logic?","['algebra-precalculus', 'logic']"
4746885,Is the average distance of a prime from its nearest square $1/3$ the average distance from the next nearest square?,"Let $p_n$ be the $n$ -th prime. Let $l_n = p_n - \lfloor \sqrt{p_n} \rfloor^{2}$ be the gap between a prime and the nearest square before the prime and $u_n= (\lfloor \sqrt{p_n} \rfloor + 1)^{2} - p_n$ be the gap between a prime and the nearest square after the prime. Is it true that $$
\lim_{m \to \infty}  \frac{\sum_{n=1}^{m}\max(l_n, u_n)}{\sum_{n=1}^{m}\min(l_n, u_n)} \stackrel{?}{=} 3
$$ My experimental data shows that: $m=1000000$ , ratio $=2.999936$ $m= 100000000$ , ratio $= 3.000107$ $m= 1000000000$ , ratio $= 3.000083$","['elementary-number-theory', 'number-theory', 'asymptotics', 'limits', 'prime-numbers']"
4746898,A general form of a double integral,"I have difficulty evaluating the following double integral: $$\int_0^1\int_0^1\sqrt{(x-y)^2+m^2}\text{d}x\text{d}y$$ where m is a constant. I've tried to use substitution $x-y=m\tan u$ to calculate the inner part: $$\int\sec^3 u\text{d}u=\frac{m^2}{2}(\tan u\sec u+\ln|\tan u+\sec u|)+C$$ where $u=\arctan\frac{x-y}{m}$ , I also need to integrate the outer part. It seems that I made the problem even complex. I've also tried to use polar coordinates, but can I use substitution like this? $$x-y=r\cos\theta, m=r\sin\theta$$","['integration', 'multivariable-calculus', 'calculus']"
4746906,Conditional Jensen's inequality proof correctness. Queries regarding convex functions.,"Let $(Ω, \mathcal{F}, P)$ be a probability space and let $\mathcal{G} ⊂ \mathcal{F}$ be a sub- $σ$ -algebra. Conditional Jensen's inequality: Let $φ : R → R$ be a convex function, $X$ and $φ(X)$ be integrable random variables.
Prove the conditional Jensen inequality $φ(E[X|\mathcal{G}]) ≤ E[φ(X)|\mathcal{G}].$ Hint: Convexity of $φ$ implies that for $x$ , $y$ ∈ $R$ , there exists a measurable function $c : R → R$ such that $φ(x) ≥ φ(y) + c(y)(x − y).$ I have made an attempt on the this question but I am sure that it is not fully correct. My attempt: Attempt 1: Taking conditional expectation both sides and using monotonocity, we get: $E[φ(x)|\mathcal{G}] ≥ E[φ(y)|\mathcal{G}] + E[c(y)(x − y)|\mathcal{G}]$ , where $x,y$ are integrable random variables. Now, put $X$ instead of $x$ and $E[X|\mathcal{G}]$ instead of $y$ . Then, $E[φ(X)|\mathcal{G}] ≥ E[φ(E[X|\mathcal{G}])|\mathcal{G}] + E[c(E[X|\mathcal{G}])(X − E[X|\mathcal{G}])|\mathcal{G}]$ . Now, $E[φ(E[X|\mathcal{G}])|\mathcal{G}]=φ(E[X|\mathcal{G}])$ (Using the fact the random variable $φ(.)$ is $\mathcal{G}$ measurable.) Also, $c(E[X|\mathcal{G}])$ is a $\mathcal{G}$ -measurable function by the definition of the function $c$ and the fact that $E[X|\mathcal{G}]$ is $\mathcal{G}$ measurable. So, $E[c(E[X|\mathcal{G}])(X − E[X|\mathcal{G}])|\mathcal{G}]=c(E[X|\mathcal{G}])E[0]$ = $0$ . Hence, we get that: $E[φ(X)|\mathcal{G}] \geq φ(E[X|\mathcal{G}])$ . Is this solution alright? Solution 2: This is a solution which I found in notes: While it is fairly easy to see the value at any point of a convex function as the supremum of the values at that point of the tangent lines to the convex function, I am unable to see how to obtain a countable set over which the supremum is taken. Also, why do I need a countable set? Also, could you please explain why $E[sup_{i}(a_{i}X+b_{i})|\mathcal{G}] \geq a_{i}E[X|\mathcal{G}]+b_{i}$ ? Also, is it possible to prove the conditional Jensen's inequality using conditional Markov's inequality?","['stochastic-processes', 'measure-theory', 'jensen-inequality', 'conditional-expectation']"
4746911,"Show $E[AB]=E[A]E[B]$ for A,B independent, not necessarily finite with a continuous density function.","I'm taking a course in proability and we always use $E[AB]=E[A]E[B]$ without explaining. I'm a bit rusty with lebesgue integral and I think this problem will help me to get better at it. Problem: Let A,B be independtent r.v. Prove E[AB]=E[A]E[B]. I only wrote $$E[AB]=\int _\Omega X dP=^*\int _\Omega x\ d\mu _{AB}(x)$$ where $\mu_{AB}$ is the push forward messure for X, meaning $\mu _{AB} (X)=P(AB\in X)$ I'm not sure why the second equallity is correct and how do I continue. how do i split $\mu_{AB} (X)=P(AB\in X) =...=\mu_A \mu_B$",['probability-theory']
4746948,Regular Season Problem 11 from 2023 MIT Integration Bee,"$$
\int \left(\sqrt{2\log x}+ \frac{1}{\sqrt{2\log x}} \right) dx
$$ I am stuck on this problem from This years integration bee. I have tried substitution but it is not giving the correct answer which is $x\sqrt{2\log x}$ I supposed $\sqrt{2\log x}$ as $t$ and differentiated it wrt $x$ , and substituted it in the above integral. But the solution has an extra $(\frac {2\log x+1}{3})$ , i dont know how and why?","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
4746987,Why Was The Angle Assumed To Be In Degrees?,"In the following problem: $$F(x) = \int_x^{x+2} \sin t \ dt$$ we are required to find $x$ when $F(x)$ is a maximum. If we differentiate and equate to zero, we will get the result: $$\sin x = \frac{\sin 2}{\sqrt{2(1–\cos 2)}}$$ Now, I didn't exactly know whether the angles of $\sin 2$ and $\cos 2$ were degrees or radians, because the result was obtained by a substitution of a length function. But neither are radians, are a length quantity (because they're dimensionless), nor degrees are a length quantity. So I just tried evaluating it assuming it was degrees, which gave me $x = 89$ . This was exactly what I expected graphically. (Assuming $89$ is in degrees). When I then tried evaluating the result using radians, I got approximately $0.5707$ , which is again exactly what I was expecting to get, for: $$0.5707 = \frac{\pi}{2} – 1 \ .$$ Does it then not matter, whatever way I evaluate a trigonometric expression (either using radians or degrees)? Why is this so? Thank you in advance.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'trigonometry']"
4747009,Show that $A$ does not have any non-empty proper subsets if and only if $A=\{x\}$ for some object $x$.,"I've seemingly hit a wall with this exercise, in full it states: Euclid famously defined a point to be “that which has no part”. This exercise should be reminiscent of that definition. Define a proper subset of a set $A$ to be a subset $B$ of $A$ with $B \neq A$ . Let $A$ be a non-empty set. Show that $A$ does not have any non-empty proper subsets if and only if $A$ is of the form $A = \{x\}$ for some object $x$ . This exercise is of the form $P\iff Q$ . I'll define $P$ to be the statement: $A$ does not have any non-empty proper subsets, and $Q$ to be $A$ is of the form $A = \{x\}$ for some object $x$ . Just to make it a bit easier to talk about each statement. Assume that $A$ is a non-empty set. Since $A$ is non-empty, let $x\in A$ . To prove $Q\implies P$ , by contrapositive $(Q\implies P)\iff (\neg P\implies \neg Q)$ . Suppose $\neg P$ i.e., $A$ has a non-empty proper subset $A'$ . Then $A'\subseteq A$ and $A'\neq A$ . Thus $\forall x \in A' \implies x\in A$ . Also $\exists y\in A':y\notin A$ or $\exists z \in A :z\notin A'$ . If $y \in A'$ then $y\in A$ . Suppose $x=y$ then $x\in A\implies y\in A$ , a contradiction. Thus $x\neq y$ . Thus the set $A$ has at least two elements. If $z\in A$ then $x\in A$ and $z\in A$ . Suppose $x=z$ then $z\notin A' \implies x\notin A'$ but I don't know if I can continue with this. If I were to get a contradiction on that last statement then in both cases $A$ has at least two elements and thus would've shown $\neg Q$ . To prove $P\implies Q$ , I used contradiction, $P\land \neg Q$ and supposed $\exists t \in A:t\neq x$ and the set $B=\{x,t\}$ . Since it is a subset but it can't be proper subset we get $A=B$ but I can't continue. An argument with contrapositive/contradiction seems very ""natural"" here because of the ""does not have"" but I couldn't continue with it and everytime I tried starting over, I couldn't think of anything else other than contradiction. All the steps I did, especially for $Q\implies P$ seemed very natural so assuming everything is correct, there could be a way for it to work, Now for $P\implies Q$ , I wouldn't call it very ""natural"" so I have a bit less faith in that approach. Something that seemed weird about working with $Q$ was that its negations are infinite so I could make other assumptions but the more elaborate I tried to make them, the difficulty in getting a proof remained the same, which was kind of beautiful but also annoying. Assuming they are correct, could any of my arguments work? If not, what would be another way to go? Could the first sentence about Euclid hint some way of proving this that I didn't see?","['elementary-set-theory', 'solution-verification']"
4747067,"I have been integrating differential equations for many, many years. How should I justify a certain step in the process though?","Someone with plenty of experience of integrating differential equations, if faced with the simple first order equation \begin{equation*}
\frac{dy}{dx}= x \tag{1}
\end{equation*} Might, almost without thinking, write down something like \begin{equation*}
\int \left\{\frac{dy}{dx}\right\} dx  =  \int x dx \tag{2}
\end{equation*} \begin{equation*}
y(x)+d=\frac{x^2}{2}+k  \tag{3}
\end{equation*} \begin{equation*}
y(x)  = \frac{x^2}{2}+ (~k-d~)
\end{equation*} ( some thought may have gone into choosing to use ‘ $d$ ’ on the RHS of $(3)$ , rather than ‘ $c$ ’ ) They would find \begin{equation*}
y(x)  = \frac{x^2}{2}+ c~~~~~~~~~\text{where} ~~c=~k-d
\end{equation*} The solution can be completed by having some extra information about $y(x)$ , say $y(0)=a$ . From information like this, the value of ‘ $c$ ’ may be determined. My question is: How can getting from (2) to (3) , be justified?","['integration', 'ordinary-differential-equations']"
