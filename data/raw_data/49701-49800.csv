question_id,title,body,tags
506981,Equivalent definitions of manifolds,"From Lee's Introduction to Smooth Manifolds, p.3: Question Concerning the exercise; what if there is a point $x$ in our manifold $M$ such that it has a neighborhood $N$ that is homeomorphic to $(0,1)\cup(1,2)$? Then this neighborhood cannot be homeomorphic to $\Bbb{R}$ or any open ball in $\Bbb{R}$. I cannot see why the definitions are equivalent.","['general-topology', 'manifolds', 'differential-geometry']"
506987,Solve the equation $2^{3x+4} = 4 \sqrt 2$,Help?????? Before hand I had to write sqrt $2$ as a power of $2$ then express $4$ $\sqrt 2$ as a power of $2$. How to solve the above equation I am not too sure!?,['algebra-precalculus']
507019,Derangement of $n$ elements,"What are the total number of ways to arrange $N$ objects such that first $K$ are deranged? That is, how many permutations of $N$ objects do not fix the first $K$ ? I know the general formula for the derangement of $N$ objects. Is there any way the above problem be reduced to this one?","['derangements', 'permutations', 'combinatorics']"
507020,"Books for studying Dirac Operators, Atiyah-Singer Index Theorem, Heat Kernels","I am interested in learning about Dirac operators, Heat Kernels and their role in Atiyah-Singer Index Theorem. From various sources (including this very helpful question ), I have come to know of various references : (i) Spin Geometry by Lawson & Michelsohn (ii) Heat Kernels and Dirac Operators by Berline, Getzler and Vergne (iii) Dirac operators and spectral geometry by Giampiero Esposito (iv) Invariance Theory, the Heat Equation, and the Atiyah-Singer Index Theorem by Peter Gilkey and (v) The laplacian on a Riemannian Manifold by Rosenberg, 
however I am having difficulty deciding which one or two to study between these. I prefer reading books that start from basics but eventually cover the core aspects of the subject at fairly advanced level. Please advise which one or two of the above should I study such that the intersection of the selected books is minimal and the union is maximal. Any comments about the above-listed books will be very helpful. My background is : Analysis (As covered in 'Principles of Mathematical Analysis' by Walter Rudin but not much of measure theory), Algbera ( 1 year grad level course based on Serge Lang), Manifold Theory and Differential Geometry (Differential forms, de-Rham theory, Riemannian metrics, Geodesics, Connections, Curvature, Vector bundles & Characteristic classes, Principle bundles ) but very little functional analysis (definition and basic properties of Banach and Hilbert spaces, and the four famous theorems) and almost no Algebraic Topology beyond the definition of fundamental group. By the time I plan to start studying ASIT, I am expecting to have studied Representation theory of Lie groups and Lie Algebras and may be a little bit of Clifford Algebras as well.Please advise what other prerequisites do I need to study the books listed above. In particular, do I need to learn more Functional Analysis and Algebraic Topology for this purpose ? I have studied the prefaces of the books I have listed but unfortunately the information about necessary prerequisites is not mentioned in most cases. Any other good references are welcome. Note: I had earlier asked a related question here","['differential-geometry', 'operator-theory', 'riemannian-geometry', 'reference-request', 'functional-analysis']"
507022,"Integrate $\int \frac{\ln(\sin x)}{\sin^2 x}\,\mathrm dx.$","I'm having trouble evaluating the integral $$\int \frac{\ln(\sin x)}{\sin^2 x}\,\mathrm dx.$$ I tried $u$-substitution and integration by parts but they didn't work.","['calculus', 'integration']"
507048,Is this operation legal?,"Is this operation allowed? Going from this: 
$\left ( \frac{x^{2}+6}{x^{2}-4} \right )^{2}= \left ( \frac{5x}{4-x^{2}} \right )^{2}$ To this: $\left ( \frac{\left (x^{2}+6  \right )\left ( 4-x^{2} \right )}{\left (x^{2}-4  \right )5x} \right )^{2}= 1$",['algebra-precalculus']
507065,"If the gradient of a function $f(x, y)$ is $\nabla f = (2xy - y)\hat{\bf i} + (x^2 - x)\hat{\bf j}$, what is $f(x, y)$?",I have this gradient and I am suppose to find the original function it belongs to..Before this I always did it the other way; given a function find the gradient. Now working back is tougher for me because it probably involves some integration... I know that $\dfrac{df}{dx} = 2xy-y$ and $\dfrac{df}{dy} = x^2-x$. Now I am lost because I know I should integrate each equation with respect to the variable I differentiated it to but then what? I will have 2 integrated equations.,['multivariable-calculus']
507097,The determinant of a special matrix,"Recently, I encounter the problem of calculating the determinant of the following matrix $$\left(\begin{array}{cccc} \sin(\theta_1) & \sin(\theta_1 + \delta_1) & \cdots & \sin(\theta_1 + (n-1) \delta_1)\\ \sin(\theta_2) & \sin(\theta_2 + \delta_2) & \cdots & \sin(\theta_2 + (n-1) \delta_2) \\ \vdots & \vdots & & \vdots \\\sin(\theta_n) & \sin(\theta_n + \delta_n) & \cdots & \sin(\theta_n + (n-1) \delta_n) \end{array}\right).$$ Is there an easy way to get its determinant? Thanks!","['matrices', 'linear-algebra', 'determinant']"
507109,Is the cartesian product of homeomorphisms again a homeomorphism?,"If we have two homeomorphisms $f:A\to X$ and $g:B\to Y$, then is it true that $f\times g:A\times B\to X\times Y$ defined by $(f\times g)(a,b)=(f(a),g(b))$ is again a homeomorphism? I think the answer is yes; It's clearly a bijection. Intuitively it seems to be continuous but I don't know how to show it. If, however, this is not true, can you give me a counterexample?",['general-topology']
507117,$A$ is invertible if and only if $A^t$ is invertible,"I hate these ""easy"" proofs. They always slip under my radar. How do I show that a square matrix $A$ is invertible if and only if $A^t$ is invertible?",['matrices']
507133,"A conjecture: for all $n\in\mathbb{N}$, the least $k>1$ such that $\phi(k)\geqslant n$ is a prime","I came across a problem in a book that asked us to find the first number $n$ such that $\phi(n)\geqslant 1,000$ it turns out that the answer is 1009, which is a prime number. There were several questions of this type, and our professor conjectured that it will always be the next prime. However, no one has been able to come up with a proof for this conjecture. So, more formally the conjecture is: For all $n\in\mathbb{N}$ the least positive integer $k\in\mathbb{N}$, with $k>1$ such that $\phi(k)\geqslant n$ is a prime. I have worked with the Möbius Inversion function, and other minimal element contradictory  proofs, but nothing has worked so far. Does anyone have any good ideas?","['totient-function', 'number-theory']"
507142,discrete math>Recurrence relation>how find the general function of $a(n)=2a(n-1)+n^2$,"this is a question from discrete math exam i had $$a(n)=\sum_{k=0}^nk^22^{n-k}$$ i need to find a general function of this (without the summation and $k$) it looks like $a(n)=2a(n-1)+n^2$, $a(1)=1$, and $a(2)=6$. (if there is no mistakes in my calculations) now the question is how to solve this non-homogeneous recurrence relation (also it would be very nice know where i can read about solving non-homogeneous recurrence relations ) i think there is also a combinatorial proof to this .(but not for sure) thanks in advance","['recurrence-relations', 'discrete-mathematics', 'combinatorics']"
507170,If $X$ has a metric $d$ then the topology induced by $d$ is the smallest topology relative to which $d$ is continuous,"(Munkres, p. 126, Ex. 3) Prove the following: Let $X$ be a metric space with a metric $d$. Let $X'$ be a topological space that has the same underlying set as $X$; i.e., $X' =X$ but $X'$ might have a different topology on it. Suppose $d : X' \times X' \to \mathbb{R}$ is continuous. Then the topology of $X'$ is finer than that of $X$.","['general-topology', 'metric-spaces']"
507175,How to find the type of triangle when given the ratio of it's sides?,"Q.The sides of a triangle are in ratio 4 : 6 : 7, then the triangle is: (A) acute angled (B) obtuse angled (C) right angled (D) impossible It's definitely not (C) right-angled since $7^2 ≠ 6^2+4^2$ Is it possible to use trigonometry here even though the triangle is not right-angled?","['trigonometry', 'triangles']"
507191,Negative sign incorrect when finding determinant by applying Gaussian Elimination,"(I don't know how to make a matrix here, someone please correct it into a better format, thanks~) So I'm applying the Gaussian Elimination to find the determinant for this matrix: $\begin{pmatrix}
0&1&3\\
1&2&0\\
0&3&4\\
\end{pmatrix}$ So, I switched row $1$ and $2$: 
$\begin{pmatrix}
1&2&0\\
0&1&3\\
0&3&4\\
\end{pmatrix}$ Then, add the multiple of $-3$ of row $2$ to the third row: $\begin{pmatrix}
1&2&0\\
0&1&3\\
0&0&-5\\
\end{pmatrix}$ So the determinant I got is $-5$, however the answer key said it's $5$.  Some1 point out what I have done wrong?  Thank you!!","['matrices', 'linear-algebra']"
507195,What is the limit behavior of this random sum?,"Let $(X_n\mid n\in\mathbb{N})$ be an i.i.d. sequence of random variables taking values in $\mathbb{R}$. What can be said about the limit behavior of 
\begin{equation}
S_n:=\sum_{i=1}^n\frac{X_i}{i}
\end{equation}
as $n\rightarrow \infty$? In particular, under what conditions does $S_n$ converge (to zero?), and under what conditions does it not converge? Many thanks for any help!","['statistics', 'probability', 'probability-theory']"
507199,Show that if $A^{n}=I$ then $A$ is diagonalizable.,"Suppose $A$ is an $m \times m$ matrix which satisfies $A^{n}=1$ for some $n$, then why is $A$ necessarily diagonalizable. Not sure if this is helpful, but here's my thinking so far: We know that $A$ satisfies $p(x)=x^{n}-1=(x-1)(x^{n-1}+\ldots+x+1)$.  If $A=I$ it is clearly diagonalizable so we may assume that $A$ is a root of the other factor. Edit: Actually, I'm a bit confused and not even sure if we can say that much.  Since the ring of $m \times m$ matrices is not an integral domain, we can not conclude that if $A-I \not = 0$ then $(A^{n-1}+\ldots+A+I)=0$, correct?","['linear-algebra', 'diagonalization']"
507219,What is the formula for finding all binary numbers below B with the number of bits == 1 staying constant?,"Maybe better explained with an example: Binary number B = 1100, every number less than B is 1010, 1001, 0110, 0101, 0011. (With 2 bits == 1) So the total ways of expressing the number B and all numbers less than B that contains 2 bits == 1 is 6, or 3+2+1. I suspect this is a Combinations problem, but my math skills are kind of rusty, and I cannot deduce a consistent formula for different binary numbers. I have done my search on the web, but could not find anything that gave me an answer to this particular problem.",['combinatorics']
507228,What does the notation $\mathbb{P}V$ mean for a vectorspace $V$?,"In algebraic geometry, I keep seeing the notation $\mathbb{P}V$ when $V$ is given as a vectorspace. My best guess is that $\mathbb{P}V$ is to mean the projective closure of $V$. But it would be nice to know if this notation is standard, and if so, what the precise definition is so there is no confusion on my part.","['notation', 'algebraic-geometry', 'projective-schemes']"
507233,Help checking proof of reverse triangle inequality $|x| - |y| \le |x + y|$?,"Let $x, y \in \mathbb{R}$. Prove $|x| - |y| \le |x + y|$. By the the triangle inequality $|x| + |y| \ge |x + y|$, hence $$
\begin{align}
&|y| \ge |x+y| - |x| \\
&|x+y| \ge |x+y| - |y| \\
\end{align}
$$ Subtracting the first inequality from the second, we have
$$
\begin{align}
&|x+y|-|y| \ge |x+y| - |y| - (|x+y| - |x|) = |x| - |y| \\
&|x+y| \ge |x+y| - |y| \ge |x| - |y| \\
&|x+y| \ge |x|- |y| \\
\end{align}
$$ The mistake in this proof is that subtracting the inequalities isn't generally valid. For example, $101 \ge 100$ and $100 \ge 1$ but $101 - 100 = 1 \not\ge 99$. Taking someone's hint, a shorter proof is 
$$|x| = |x + y - y| = |x + y + (-y)| \le |x + y| + |-y| = |x + y| + |y| $$
by the triangle inequality. Then
$$
\begin{align}
&|x| \le |x+y| +|y| \\
&|x| - |y| \le |x + y|
\end{align}
$$","['inequality', 'absolute-value', 'algebra-precalculus', 'proof-verification']"
507242,"Closed form of $\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx$","Could you please help me to solve this integration problem?
$$\int_0^2\frac{1}{2+\sqrt{3\,e^x+3\,e^{-x}-2}}dx$$
Its approximate numeric value is $0.419197813818367...$ , but I could not find an exact symbolic expression for it.","['closed-form', 'calculus', 'integration', 'definite-integrals', 'exponential-function']"
507250,The projection on the first factor is a bijection,"I require some clarification and hints on the following Problem:
\begin{align}f: X \longrightarrow Y \end{align}
The image $p$ is defined as: \begin{align} p: G_f &\longrightarrow X  \\ (x,y) &\longmapsto x\end{align}
With $G_f=\lbrace (x,y) \in X \times Y : f(x)=y \rbrace$ Question: Is the function $p$ a bijection? The part I struggle the most with is that this is the first time that I deal with a 2-tuple, so please check if my approach is correct. In the general case, to show if a function is injective choose $x,x' \in X$ such that $f(x)=f(x')$ and show that if $f(x)=f(x')$ then $x=x'$ Here is my approach to this problem: Let $(x,y) \in G_f \subset X \times Y$ and $(x',y') \in G_f \subset X \times Y$ be two 2-tuples such that $p(x,y)=x=p(x',y')$ then $(x,y)=(x',y')$ is an ordered pair, therefore $x=x'$ and $y=y'$ Is this correct? For surjectivity, no ideas so far. Surjectivity: $ \forall y \in Y \exists x \in X : f(x)=y $",['analysis']
507262,Steps to simplify a Boolean Expression,Simplify: $(x \land y) \lor (x \land \neg y) \lor (\neg x \land y)$ I need to simplify this using the using properties going step by step. I keep ending up with $(x \land y)$ as the answer but when I map is out I get that is should be $(x \lor y)$. Any help would be appreciated I would like to know what I am doing wrong.,"['logic', 'boolean-algebra', 'discrete-mathematics']"
507263,"Prove that $C^1([a,b])$ with the $C^1$- norm is a Banach Space.","Consider the space of continuously differentiable functions, $$C^1([a,b]) = \{f:[a,b]\rightarrow \mathbb{R}\mid f \text{ differentiable with }f' \text{ continuous}\}$$ with the $C^1$ -norm $$\lVert f\rVert := \sup_{a\leq x\leq b}|f(x)|+\sup_{a\leq x\leq b}|f'(x)|.$$ Prove that $C^1([a,b])$ is a Banach Space. This was the proof we were given:
Assuming $C^1([a,b])$ is a normed linear space all we need to show is completeness. Let $(f_n)$ be a Cauchy Sequence in $C^1([a,b])$ with respect to the $C^1$ -norm. Then each $f_n,f'_n\in (C([a,b]),\|\cdot\|_{\sup})$ . We know that $C([a,b])$ is complete and thus there exists $f,g\in C([a,b])$ such that $f_n\rightarrow f$ , and $f'_n\rightarrow g$ (uniformly) with respect to $\|\cdot\|_{\sup}$ . If we let $$ F_n(x) = \int_a^x f_n(t)dt, \hspace{2mm}  F(x) = \int_a^x f(t)dt $$ then $F_n\rightarrow F$ uniformly because $$\lVert F_n-F\rVert_{\sup}\leq \sup_{a\leq x\leq b}\int_a^x|f_n(t)-f(t)|dt\leq \lVert f_n-f\rVert_{\sup}<\epsilon.$$ From the fundamental theorem of calculus: $$f_n(x)-f_n(a) = \int_a^x f'_n(t)dt $$ Since $f'_n\rightarrow g$ uniformly then $$ \int_a^xf'_n(t)dt\rightarrow \int_a^x g(t)dt $$ Since we know that $f_n\rightarrow f$ uniformly, $$f(x)-f(a) = \int_a^x g(t) dt $$ which by the fundamental theorem of calculues implies $f'=g$ . So we know have $f_n\rightarrow f$ and $f'_n\rightarrow g=f'$ which mean $f_n\rightarrow f\in C^1([a,b])$ with respect to $C^1$ -norm. So every cauchy sequence converges. Hence $C^1([a,b])$ is a Banach Space. So I understand most of the proof. Where I get confused is that how did we actually show this satisfies the $C^1$ -norm? Maybe I don't understand what this norm actually does. Thank you for any help, comments and advice!","['functional-analysis', 'real-analysis', 'banach-spaces']"
507279,Example of Left and Right Inverse Functions,I am independently studying abstract algebra and came across left and right inverses. I was hoping for an example by anyone since I am very unconvinced that $f(g(a))=a$ and the same for right inverses. I don't want to take it on faith because I will forget it if I do but my text does not have any examples.,"['discrete-mathematics', 'abstract-algebra']"
507287,Generating function solution to previous question $a_{n}=a_{\lfloor n/2\rfloor}+a_{\lfloor n/3 \rfloor}+a_{\lfloor n/6\rfloor}$,"In attempting to answer this question , I reduced it to a seemingly simple generating functions question, but after days of work was unable to construct a proof.  Since I do not have experience trying to do asymptotics with generating functions, I would like to know if a proof is salvageable from these methods. The problem introduces the sequence $a_n$ , defined by $a_0 = 1$ and $$
a_{n}=a_{\left\lfloor n/2\right\rfloor}+a_{\left\lfloor n/3 \right\rfloor}+a_{\left\lfloor n/6\right\rfloor}
$$ and asks for a proof that $$
\lim_{n\to\infty}\dfrac{a_{n}}{n}=\dfrac{12}{\log{432}}.
$$ Writing the generating function $\displaystyle A(x) = \sum_{n \ge 0} a_n x^n$ , this translates to $$
A(x) =
(1 + x)A(x^2)
+ (1 + x + x^2) A(x^3)
+ (1 + x + x^2 + \cdots + x^5)A(x^6)
- 2
$$ Even better, let $b_0 = a_0$ and $b_n = a_n - a_{n-1}$ for all $n \ge 1$ , and define the generating function $\displaystyle B(x) = \sum_{n \ge 0} b_n x^n = (1 - x)A(x)$ .  Multiplying the above by $(1-x)$ gives $$
(1 - x)A(x) = (1 - x^2)A(x^2) + (1 - x^3)A(x^3) + (1 - x^6)A(x^6) + 2x - 2
$$ i.e. $$
B(x) = B(x^2) + B(x^3) + B(x^6) + 2x - 2 \tag{1}
$$ After unsuccessfully trying to do asymptotics with the above elegant formula, I used it to find an explicit representation of $B$ , using the Delannoy Numbers : $$
B(x) = 1 + 2 \sum_{l, m \ge 0} \sum_{d \ge 0}
2^d {l \choose d}{m \choose d} x^{2^l 3^m}
$$ It follows that in fact \begin{align*}
b_n&= \begin{cases}
1 &n=0 \\
2 \sum_{d \ge 0} 2^d \binom{l}{d} \binom{m}{d} &n =2^l3^m \\
0 &\text{otherwise}
\end{cases}
\\[10pt]
a_n&=1+2\sum_{d\ge0}2^d\sum_{\begin{matrix}l,m\ge0\\2^l 3^m \le n\end{matrix}}{l \choose d}{m \choose d}
\tag{2}
\end{align*} One can do naive bounds on the sum in (2) -
replacing the condition $2^l 3^m \le n$ with $2^l 2^m \le n$ and $3^l 3^m \le n$ for upper and lower bounds, respectively.
But this isn't good enough;
it gives (after algebra and combinatorial work) approximately $$
\frac{n^{\log_3(1 + \sqrt{2}) - 1}}{2}
< \frac{a_n}{n} <
\frac{n^{\log_2(1 + \sqrt{2}) - 1}}{2}
$$ This seems to suggest trying to approximate (2) with the condition $(1 + \sqrt{2})^l (1 + \sqrt{2})^m \le n$ , but I have no idea how to justify that. At any rate, I've made too much of what feels like progress to give up on the problem, and if anyone can think of a way to use (2) to get a solution or else to use (1) and find the asymptotics directly, I'd be very thankful.","['recurrence-relations', 'sequences-and-series', 'generating-functions', 'combinatorics', 'limits']"
507297,Integrate $\int x \sqrt{2 - \sqrt{1-x^2}}dx $,"it seems that integration by parts with some relation to substitution... $$
\int x \sqrt{2-\sqrt{1-x^2}}
= \frac{2}{5} \sqrt{2-\sqrt{1-x^2}} \cdot \sqrt{1-x^2}+\frac{8}{15}\sqrt{2-\sqrt{1-x^2}}+c
$$ How can I get that?","['improper-integrals', 'calculus']"
507312,Expected shortest path in random graph,"Consider all connected graphs with $n$ verticies where each vertex connects to $k$ other verticies. We choose such a graph at random. What is the expected value of the shortest path between two random points? What is the expected value of the maximal shortest path? If an exact solution would be too complicated, I would also appreciate an approximate solution for $k<<n$.","['probability', 'random-graphs']"
507335,Product of measurable and integrable functions,"Let $(X,F,u)$ be a measure space and $f,g$ measurable functions. Show that if $f$ is integrable and $g$ is bounded and measurable, then $fg$ is integrable. $f$ is integrable (on a set $E$) means that $\int_E|f|d\mu<\infty$. And we want to show that $\int_E|fg|d\mu<\infty$. But if $|g(x)|<M$ for all $x$, then the function $|fg|$ is bounded from above by $M|f|$, so we have $$\int_E|fg|d\mu\leq\int_EM|f|=M\int_E|f|<\infty.$$ So we don't need the condition that $g$ is measurable, do we?","['lebesgue-integral', 'measure-theory']"
507339,Singular values of square orthogonal matrix?,"What are the singular values of an $n \times n$ square orthogonal matrix? 
How do we know that the set of all orthogonal matrices is convex? Is there an example?","['matrices', 'real-analysis']"
507386,How to write this in mathematical notation?,"I have the following claim: “If $x$ and $y$ are real numbers and their product is irrational, then either $x$ or $y$ must be
irrational.” I'm supposed to write this in mathematical notation. It's my first year of university and I'm not so sure how to go about this, I never did any of this in high school. Could anyone help me write this in mathematical notation ? I'm supposed to prove this later, but I have an idea how to do that, I just need help with this part. Thank you in advance.","['notation', 'discrete-mathematics', 'proof-writing', 'irrational-numbers']"
507402,Is this inequality true? (Inequality involving probability distribution and products),"Suppose $f(z)$ is a discrete probability distribution with space $S$. Suppose $g(z),h(z)>0$ for all $z \in S$. Is it true that $$\prod_{z \in S}{g(z)^{f(z)}}+\prod_{z \in S}{h(z)^{f(z)}} \leq \prod_{z \in S}{[g(z)+h(z)]^{f(z)}}?$$ My first impulse was to use Jensen's Inequality, but to no avail. This inequality is part of a much larger theorem I am trying to prove. Thanks for your help!","['probability', 'analysis']"
507413,Proof for number of weak compositions,"I'm looking for an alternative to the following (possibly standard) proof for the number of weak compositions: The number of $k$-compositions of $n+k$ corresponds to a weak one by subtracting 1 from each ""bin"". Thus we have $\binom{n+k-1}{k-1}$. While I kind of like this proof and always felt it made sense lately I've been left wanting something a little more convincing. Does anyone have a different proof of this result?",['combinatorics']
507420,How to think about the object $A\otimes_kk'$.,"Let $k'/k$ be a finite extension of fields, and let $A$ be a finitely generated commutative $k'$-algebra.  Through $k\hookrightarrow k'$ we can consider $A$ to be a finitely generated commutative $k$-algebra, and then we can consider the $k'$-algebra $A\otimes_kk'$. Are $A$ and $A\otimes_kk'$ isomorphic as $k'$-algebras?  If not, can we consider $A$ as a subalgebra of $A\otimes_kk'$ via $a\mapsto a\otimes 1$?  I believe this map has the right inverse $a\otimes c\mapsto ca$.  Are there conditions we can impose which make the two $k'$-algebras isomorphic? In general, I'm having trouble thinking about the object $A\otimes_kk'$.  For $A$, I just think of some quotient of a polynomial ring over $k'$.  Perhaps my question reduces to asking about the object $k'\otimes_kk'$?","['tensor-products', 'abstract-algebra', 'field-theory']"
507423,Clarification regarding basis for a topology,"This might be super trivial but I if possible would like some clarification on this topic. I am reading from Munkres' Topology, 2nd edition, page 78 (if interested). My question regards what a basis for a topology means. Below is what I am thinking it means; besides its definition I wonder if the following is true: [$\mathcal{B}$ is a basis for a topology $\tau$] $\Rightarrow \tau$ is the topology generated by $\mathcal{B}$, that is 
  $$\tau=\tau_\mathcal{B}=\{ U \subseteq X: \forall x\in U \exists B\in\mathcal{B}\left( x\in B \subseteq U\right)\}.$$ Essentially what I want is that when we say $\mathcal{B}$ is a basis for a topology, then the topology that $\mathcal{B}$ is a basis for (might just be definition here) is the topology $\tau_\mathcal{B}$ mentioned above. *Munkres' defines a basis for a topology as: If $X$ is a set, a basis for a topology on $X$ is a collection $\mathcal{B}$ of subsets of $X$ (called basis elements) such that
  $$(1)\, \text{For each $x \in X$, there is at least one basis element $B$ containing $x$}$$
  $$(2)\, \text{If $x$ belongs to the intersection of two basis elements $B_1$ and $B_2$, then there is a basis}$$
  $$\text{element $B_3$ containing $x$ such that $B_3 \subseteq B_1 \cap B_2$.}$$
  If $\mathcal{B}$ satisfies these two conditions, then we define the topology $\tau$ generated by $\mathcal{B}$ as follows: A subset $U$ of $X$ is said to be open in $X$ if for each $x \in U$, there is a basis element $B \in \mathcal{B}$ such that $x \in B$ and $B\subseteq U$. Later in a lemma we prove: Let $X$ be a set; let $\mathcal{B}$ be a basis for a topology $\tau$ on $X$. Then $\tau$ equals the collection of all unions of elements of $\mathcal{B}.$ In the proof of this lemma, it looks to me like they are saying that $\tau=\tau_\mathcal{B}$ as I defined above so this leads me to think that at this point, when we say $\mathcal{B}$ is a basis for a topology $\tau$ on $X$, we are meaning $\tau=\tau_\mathcal{B}$. I am still trying to get a sense of what your answers means, Berci. In an older topology book that I used in undergrad, we defined a basis of a topology as being some subset of a topology $\tau$ where each element of $\tau$ could be written as the union of members of $\mathcal{B}$, which I am sure is equivalent to how it is defined in Munkres' text, I am just trying to put some pieces together so that I have some hard statements to prove.",['general-topology']
507425,An integral involving Airy functions $\int_0^\infty\frac{x^p}{\operatorname{Ai}^2 x + \operatorname{Bi}^2 x}\mathrm dx$,"I need your help with this integral:
$$\mathcal{K}(p)=\int_0^\infty\frac{x^p}{\operatorname{Ai}^2 x + \operatorname{Bi}^2 x}\mathrm dx,$$
where $\operatorname{Ai}$, $\operatorname{Bi}$ are Airy functions :
$$\operatorname{Ai}\,x=\frac{1}{\pi}\int_0^\infty\cos\left(x\,z+\frac{z^3}{3}\right)\,\mathrm dz,$$
$$\operatorname{Bi}\,x=\frac{1}{\pi}\int_0^\infty\left(\sin\left(x\,z+\frac{z^3}{3}\right)+\exp\left(x\,z-\frac{z^3}{3}\right)\right)\,\mathrm dz.$$
I am not sure that $\mathcal{K}(p)$ has a general closed form, but I hope so, because approximate numerical calculations suggest these conjectured values:
$$\mathcal{K}(3)\stackrel?=\frac{5\,\pi^2}{32},\ \ \mathcal{K}(6)\stackrel?=\frac{565\,\pi^2}{512}.$$","['closed-form', 'special-functions', 'calculus', 'integration', 'definite-integrals']"
507441,Strengthened Dirichlet's Unit Theorem for Cyclotomic Fields,"If $p$ is an odd prime, $\xi$ is a $p$th root of unity, and $\mu_k = \frac{1-\xi^k}{1-\xi}$, then $\mu_2, \mu_3, \ldots, \mu_{\frac{p-1}{2}}$ are multiplicatively independent. I would greatly appreciate a reference for this fact. I am also interested in the analogous fact when $p$ is replaced with $p^k$. Thanks.","['algebraic-number-theory', 'number-theory']"
507446,"On the ""funny"" identity $\tfrac{1}{\sin(2\pi/7)} + \tfrac{1}{\sin(3\pi/7)} = \tfrac{1}{\sin(\pi/7)}$","This equality in the title is one answer in the MSE post Funny Identities . At first, I thought it had to do with $7$ being a Mersenne prime , but a little experimentation with Mathematica 's integer relations found, $$\frac{1}{\sin(2\pi/15)} + \frac{1}{\sin(4\pi/15)} + \frac{1}{\sin(7\pi/15)} = \frac{1}{\sin(\pi/15)}$$ $$\frac{1}{\sin(2\pi/31)} + \frac{1}{\sin(4\pi/31)} + \frac{1}{\sin(8\pi/31)} + \frac{1}{\sin(15\pi/31)} = \frac{1}{\sin(\pi/31)}$$ so the Mersenne number need not be prime. Let $M_n = 2^n-1$. How do we prove that, $$\frac{1}{\sin(M_{n-1}\pi/M_n)}+\sum_{k=1}^{n-2} \frac{1}{\sin(2^k\pi/M_n)} = \frac{1}{\sin(\pi/M_n)}$$ indeed holds true for all integer $n>2$? Edit (an hour later): I just realized that since, for example, $\sin(3\pi/7)=\sin(4\pi/7)$, then the question can be much simplified as, $$\sum_{k=1}^{n-1} \frac{1}{\sin(2^k\pi/M_n)} \overset{?}{=} \frac{1}{\sin(\pi/M_n)}$$","['trigonometry', 'algebra-precalculus']"
507448,Group generated by a conjugacy class,"Let $G$ be a group, $x \in G$, and $S = \{ x^g \mid g \in G\}$. Suppose $\langle S \rangle = G$ and that $H$ and $K$ are 
subgroups of $G$ with $S \subseteq H \cup K$. Show that $H=G$ or $K=G$. This is a problem from Kurzweil & Stellmacher's Theory of Finite Groups . 
If we let $\langle x \rangle = A$, then $G$ is the product of the distinct conjugates
of $A$ in any order, 
 $G=A_1 A_2 \cdots A_k$, because they generate $G$. Using this I've managed to show $G=HK$.
Also $k≥3$ because a group can't be the product of two proper conjugate subgroups.
The picture emerging 
is that since one of $H$, $K$ has to contain two distinct  $A_i$,
 somehow that forces 
it to be the whole group. Of course this only works if the cyclic subgroups generated by each individual generator of $G$ have finite index, which is not guaranteed in this introductory-level problem.
I suspect I'm overlooking something simple. Any help would be greatly appreciated.","['group-theory', 'abstract-algebra']"
507457,Find where the limit does not exist for the function,"Given the function: $f(x,y) = \frac{xy^4}{x^2+y^8}$, find a path where the limit does not exist at the origin. I am having problems with this because of lot of paths go to $0$ but I know the limit does not exist. I have tried things like $y^2$, $\sqrt(y)$ but I am getting nowhere. 
Thank you!","['multivariable-calculus', 'limits']"
507465,"$F = \{f\in C^1([0,1])| \hspace{2mm} \|f\|\leq M, \|f'\|\leq N\}$. Showing it is precompact and not closed.","I have an example in my book:
Let $C([0,1])$ denote the space of all continuous functions $f$ on $[0.1]$ with continuous derivative $f'$. For constants $M>0$ and $N>0$, we define the subset $F$ of $C([0,1])$ by $$F = \{f\in C^1([0,1])\mid \|f\|\leq M, \|f'\|\leq N\}.$$ Where $\|\cdot\|$ denotes the $\sup$-norm.The book claims that $F$ is precompact in $C(K)$ and it is not closed because the uniform limit of continuously differentiable functions need not be differentiable. My questions are:
1) How can I prove this set is precompact? I know I have to show it is bounded and equicontinuous. 2) Is there an example of such a function that can show this is not closed? This is not a homework exercise. I am trying to fill in the blanks that my text book has not filled in so I can have a better understanding. Thank you in advance for any help, advice and comments.","['self-learning', 'continuity', 'functional-analysis', 'analysis']"
507467,How do you factor this? $x^3 + x - 2$,How do you factor $x^3 + x - 2$? Hint: Write it as $(x^3-x^2+x^2-x+2x-2)$ to get $(x-1)(x^2+x+2)$ Note the factored form here . Thanks!,"['factoring', 'algebra-precalculus']"
507508,The fastest trajectory that a particle should follow through two different mediums,"Hello, after 3 failed attempts to solve this problem, I decided to start a bounty for this question. Please, I need a complete answer with an interpreation of the final result. Thank you in advance. Problem: A particle travels at speed $v_a$ in a medium $A$ and at speed $v_b$ in an medium $B$. The particle departs on time $t=0$ from the point $P_i$ and has to arrive in the minimal time to the point $P_f$, as in the picture. Determine the trajectory that the particle has to follow to arrive from to the point $P_f$ in the minimal time. Attempt 1: I know that the fastest way to go from a point to another point is the segment of straight line that joins the two points, but I don't know how to consider the velocities $v_a$ and $v_b$ of the problem, or if they're just distractors to confuse me and there's no need to consider them. Am I right or not? Attempt 2: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, and let $r$ be the distance from the left point of the boundary to $P_c$. We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{\sqrt{h^2+r^2}}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{\sqrt{m^2+(s-r)^2}}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$. Now, we have to consider the function: $t(r)=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$, differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{r}{v_a \sqrt{h^2+r^2}}-\frac{s-r}{v_b \sqrt{m^2+(s-r)^2}}=0$ So, $rv_b \sqrt{m^2+(s-r)^2}=(s-r)v_a \sqrt{h^2+r^2}$ and $r^2v_b^2 (m^2+(s-r)^2)=(s-r)^2v_a^2 (h^2+r^2)$ Rearranging, we get the following 4th degree polynomial in $r$: $(v_b^2-v_a^2)r^4+2s(v_a^2-v_b^2)r^3+(v_b^2m^2-v_a^2h^2+v_b^2s^2-s^2v_a^2)r^2+(2sv_a^2h^2)r-(s^2v_a^2h^2)=0.$ So, now do I have to solve this equation? Attempt 3: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, let $r$ be the distance from the left point of the boundary to $P_c$, and let $\alpha$ and $\beta$ be the angles in the following picture: We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{r \sin \alpha}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{(s-r) \sin \beta}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}= \frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b}.$ Now, we have to consider the function: $t(r)=\frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b},$ differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{\sin \alpha}{v_a}-\frac{r \sin \beta}{v_b}=0.$ So, $\frac{\sin \alpha}{v_a}=\frac{r \sin \beta}{v_b},$ and then $r=\frac{\sin \alpha}{\sin \beta}\frac{v_b}{v_a}=1,$ ?? by Snell's law.","['optimization', 'calculus']"
507529,Determining $(X \cap Y \subseteq \overline{A} \land Y \subseteq B) \implies Y \subseteq B - A$,"I'm trying to determine the truth value of $(X \cap Y \subseteq \overline{A} \land Y \subseteq B) \implies Y \subseteq B - A$ We got two premises: $X \cap Y \subseteq \overline{A}$ $Y \subseteq B$ Have some element $m \in Y$. If I prove that it is in $B - A$ then it's over. The only inference with the premises I can think of is that since $m \in Y$, it must be in $B$, so we got $m \in B$. Now I need to prove that $m \notin A$. Now I know that I should be working with the first premise. However, I'm not sure what can I infer from it. I know that $m\in Y$, but that doesn't necessarily mean $m \in \overline{A}$, since it may or not be in this intersection. How can I proceed then?","['discrete-mathematics', 'elementary-set-theory']"
507532,Coordinate independence of geometrical objects.,"I am still trying to get a good grasp on the motivations behind various concepts in Differential Geometry. But I am struggling to come to terms with how certain concepts have this added attribute of being coordinate independent? How does one identify such objects, be it a tangent space or a covariant derivative. How does one go about trying to prove that a certain geometric object is coordinate independent? How is coordinate independence a part of the ""geometry"" of a given surface or is it? P.S.: Actually is the concept of a coordinate system part of the intrinsic or extrinsic geometry? I think its the former, but sometimes embedded spaces tend to make me think twice. Edit: I would appreciate if the covariant derivative could be used as an example.","['soft-question', 'differential-geometry']"
507560,minimal polynomial of a matrix with some unknown entries,"Question is to prove that  : characteristic and minimal polynomial of $ \left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right) $ is $x^3-ax^2-bx-c$. what i have done so far is : characteristic polynomial of a matrix $A$ is given by $\det(A-xI)$ in case of $A= \left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)$ we have $\det(A-xI)=\det\left( \begin{array}{cccc}
-x & 0 & c  \\
1 & -x & b \\
0 & 1 & a-x \end{array} \right)=-(x^3-ax^2-bx-c)$ So, i have got the characteristic polynomial as $x^3-ax^2-bx-c$. Now, the problem is how do i find minimal polynomial. As $a,b,c$ are arbitrary, I can not factorize $x^3-ax^2-bx-c$ so as to see which factor gives me minimal polynomial. I am confused. please suggest me some hint. EDIT : This is just after Mr.Will Jagyy's hint : I have $A= \left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)$ then, $A^2= \left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)\left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)=\left( \begin{array}{cccc}
0 & c & ac  \\
0 & b & c+ab \\
1 & a & b+a^2 \end{array} \right)$ Now, $A^2+rA+sI=\left( \begin{array}{cccc}
0 & c & ac  \\
0 & b & c+ab \\
1 & a & b+a^2 \end{array} \right)+r\left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)+s\left( \begin{array}{cccc}
1 & 0 & 0  \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc}
s & c & *  \\
r & b+s & * \\
1& * & * \end{array} \right)$ As element of $3^{rd}$ row $1^{st}$ column is $1$ in above matrix, this can never be $0$ i.e., $A^2+rA+sI$ can never be $0$. Now, $A+rI=\left( \begin{array}{cccc}
0 & 0 & c  \\
1 & 0 & b \\
0 & 1 & a \end{array} \right)+r\left( \begin{array}{cccc}
1 & 0 & 0  \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right)=\left( \begin{array}{cccc}
r & *& * \\
\color{magenta}{1} & * & * \\
* & \color{magenta}{1} & * \end{array} \right)\neq 0$ if $r\neq 0$ Thus, $A^2+rA+sI\neq 0$ and $A+rI\neq 0$ for any $r,s$. Thus, minimal polynomial for $A$ can not be of order less than $3$. Thus, minimal polynomial for $A$ has to be $x^3-ax^2-bx-c$. I have written this just to make sure i have tried in correct way as i can not write this in a comment. I would be thankful if there is any other way to proceed further.. Thank you :)","['matrices', 'linear-algebra']"
507561,Questions about coercive functions and its implications,"Given this definition: A function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is $coercive$ if 
$$\lim_{||x||\rightarrow\infty}f(x) = \infty.$$
Explicitly, this means that for any $M>0$ there is an $R>0$ such that $||x||\geq R$ implies $f(x)\geq M$. I understand this definition, it is saying if the norm of a vector in $\mathbb{R}^n$ becomes large then the functions values lie outside of $M$. My questions are: 1) Does this mean that for $||x||<R$ implies $f(x)<M$? 2) This may seem trivial to most but I was told that $||x||\leq R$  is a compact set in $\mathbb{R}^n$. I am not seeing this because of my first question. My perspective is that if $||x||\geq R$ implies $f(x)\geq M$ that the $||x||<R$ implies $f(x)<M$. Which is not closed. 3) If $||x||\leq R$ is indeed a compact set how does it guarantee that $||x||=R$ will not have a function value $f(x)>M$? If anyone could help me clear my confusion I would greatly appreciate it! Thank you for any help and comments.","['functional-analysis', 'self-learning', 'functions', 'analysis', 'compactness']"
507577,Find $\lim_{x \to \pi/2} \frac{1 - \sin{x}}{(2x - \pi)^2}$ without using L'Hospital?,$$\lim_{x \to \pi/2} \frac{1 - \sin{x}}{(2x - \pi)^2}$$ I tried L'Hospital and it works like this in wolfram but if I don't use it what should I do?,"['calculus', 'limits']"
507587,"How do I determine cardinalities, given the cardinality of other sets?","Now I'm taking a look at cardinality exercises. My understanding is very basic: it is the number of elements in a set, and the cardinality of a power set like $P(A)$ is $2^{|A|}$ . So, I found this exercise: Given |A| = 10, |B| = 5, |A-B| = 7, determine $|A \triangle B|$ $|P(A \times B)|$ $|P(A \cup (A \times B))|$ Now, I solved them (I think), but my basic understanding of this doesn't make me very confident - can you help me see if my answers are right? First of all, drawing a Venn diagram, it seems to me that $A$ has $7$ exclusive elements, $B$ has $2$ and their intersection has $3$ . $|A \triangle B|$ Well, $A$ has $7$ exclusive elements and $B$ has $2$ , so the answer is $9$ . $|P(A \times B)|$ Hmph. I'm guessing that $A \times B$ means that I got to multiply the amount of elements in $A$ by $B$ 's cardinality. So $|A \times B| = 50$ . The power set of a set with $50$ elements should be $2^{50}$ . I'm not sure here: when doing $A \times B$ , should I have multiplied $10$ by $5$ , or should I have multiplied only their exclusive elements? ( $7$ by $2$ ). $|P(A \cup (A \times B))|$ Again, I'm guessing that $|A \times B|$ is $50$ . Now I have to know $A \cup (A \times B)$ . I'm a bit puzzled. Thinking about it, technically, the result of a set shouldn't have elements in the cartesian product with another set, so I guess that $A \cap A \times B = \emptyset$ right? Therefore, I can simply add $|A| + |A \times B|$ , which should be $60$ . Finally, the power set of a set with cardinality $60$ should be $2^{60}$ . Is my cardinality reasoning valid?","['discrete-mathematics', 'elementary-set-theory']"
507592,How prove this $pq|p^{aq}+q^{ap}+a$,"let $p,q$ are prime numbers,show that
  for any $(p,q)$,there must exist positive integer numbers $a$,such
  $$pq|p^{aq}+q^{ap}+a$$ since I consider this problem,and I found this problem is maybe from this http://www.artofproblemsolving.com/Forum/viewtopic.php?p=1704114&sid=3b8ef31515f3721e84dda4acdff4823f#p1704114 Korean Olympiad Finals (2007-4) problem is this Find all pairs $ (p, q)$ of primes such that $ p^p+q^q+1$ is divisible by $ pq$. But for my problem I can't prove it,Thank you,and I think this is nice problem.",['number-theory']
507599,Is the union of finitely many open sets in an omega-cover contained within some member of the cover?,"Let $\mathcal{U}$ be an open cover of $\mathbb{R}$ (Standard Topology) such that $\mathbb{R} \not \in \mathcal{U}$ and for any finite set $A$ there is a $U \in \mathcal{U}$ such that $A \subseteq U$. We call such an open cover an $\omega$-cover. Can we show that for any finite set $B \subset \mathcal{U}$, there is a $V \in \mathcal{U}$ such that $\cup B \subseteq V$? Ultimately I'm working on showing the following. Let $\langle \mathcal{U}_n: n \in \mathbb{N} \rangle$ be a sequence of $\omega$-covers. Can we find a sequence $\langle F_n: n \in \mathbb{N} \rangle$ with each $F_n \in \mathcal{U}_n$ such that $\cup F_n$ is an open cover of $\mathbb{R}$? My approach here was to use each $\mathcal{U}_n$ to cover $[-n,n]$, thus eventually covering all of $\mathbb{R}$. Since $[-n,n]$ is compact and $\mathcal{U}_n$ is a cover, $\mathcal{U}_n$ has a finite subcover. But that's as far as I can get unless what I conjectured above is true.",['general-topology']
507600,Abstracting Completeness,"Introduction This may seem like a weird question or even a silly one, but topology is vast and I find I make quicker progress working my way through it by trying ideas out loud within earshot of people that can cut them down rapidly if I'm wasting my time. It may seem more like a discussion, but I would prefer it took place as an actual post because I want to do more than just chat about it. There is a specific question at the end of this. Main Event Starting at the bottom (or top if you like) of the abstract analysis ladder we have a set with no structure. Then we go up (or down) to the next rung and define a topology and we have continuity of a function $f: X \to Y$, $X$ and $Y$ both topological spaces, with $U = f^{-1}(V)$ open in $X$ whenever $V$ open in $Y$. We can already talk about the continuity of addition and scalar multiplication in a vector space, just by defining a topology. With limits of sequences it involves having arbitrary open neighbourhoods of a point leaving out only finitely many points of the sequence. Then, if we want things to have distances between them, we move to another rung and define a metric, preferably one of actual use in solving an interesting problem in analysis as opposed to a pathological counterexample, and determine if our spaces of interest are metrizable and so on. The Actual Question Now we can start talking about complete metric spaces. Every where I look, completeness is discussed in the context of a metric space. For example, in Steen and Seebach's Counterexamples in Topolgy, their definition of completeness is given in terms of a metric space, Munkres likewise. Here's where I muse that we've made topological versions of continuity and convergence that can be applied just using a topology, so is there a version of completeness that can be defined purely topologically? Convergence can, so if Cauchy could then there it would be. Philosophical Epilogue For me the whole point of building the current massive edifice of abstract mathematical machinery was to illuminate everything at the most fundamental level in order to refine our understanding and move on to greater mathematical heights rather than get bogged down in the particular details of a particular version of a particular type of problem. Are we succeeding? Or is maths just becoming far too complicated? Conclusion After the discussion below, the point highlighted is that Cauchyness involves closeness and how to do that without a metric. I had originally mistakenly thought Cauchyness could find its way into a pure topological setting because I had forgotten that I was using a topological definition that was being used on vector spaces without the advantage of a metric. The advantage of subtracting is none the less there and kills the point. I realised how careful one has to be with the words one uses to discuss this, even in matters that are allowed. What does ""zero"" even mean in an arbitrary set? Originally above I used the word ""small"" when referring to convergence. It's kind of implied in the fact that $N$ may have to be really, really big depending on which open neighbourhood you choose, but we have no way of measuring ""small"" at the pure topological stage. Anyway, the answer is clear, there has to be something more than just the definition of a topology to be able to discuss completion. Even the idea of compactness doesn't get us around it in general. In spaces where the unit ball (already measuring) is not compact, we need equi-continuity to establish the compactness. I notice also that nobody wanted to get in to the philosophical question.","['general-topology', 'analysis']"
507611,Relationship between convexity and superadditivity?,"This question is a little vague, so let me give some motivation. I was trying to prove the generalized Holder's inequality for probability measures,
$$\mathbb{E}(X_1 \dots X_n) \leq \prod_{i=1}^n \|X_i\|_{p_i},$$
where $\sum_{i=1}^n (1/p_i) = 1$, the $X_i$ are random variables, and
$$\|X_i\|_{p_i} = \left(\int_\Omega |f(x)|^{p_i}dx\right)^{1/p_i}.$$
To prove this you can use the arithmetic-geometric mean,
$$\sum_{i=1}^n x_i^{p_i} \leq \sum_{i=1}^n p_i x_i,$$
where $x_i > 0$ and $\sum p_i = 1$. 
In getting through all this I started investigating Jensen's inequality for convex functions. I've known convexity was important but I've never grappled with it from an analytic perspective, so I don't really know the tricks / typical arguments. I tried to prove the AM-GM mean by generalising the proof of Holder for $(1/p) + (1/q) = 1$, which relies on Young's inequality,
$$ab \leq \frac{1}{p}a^p + \frac{1}{q}b^q,$$
whence Holder's follows from a clever choice of $a, b$. However I don't see any ways to extend the definition of convexity, which is that for any $\lambda \in [0,1]$, we require
$$f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y),$$
to something which would involve multiple $\lambda$s which sum to 1. 
Another definition of convexity is existence of a vector $V \in \mathbb{R}^n$ such that
$$f(y) \geq f(x) + V\cdot(y-x)$$
for all $y \in \mathbb{R}^n$ or
$$f(x) = \sup_{z \in \mathbb{R}} L_z(x)$$
where the $L_z$ are the linear support planes for the convex function $f$. I understand all these definitions separately and more or less together, but it feels like I should be able to say something like arithmetic-geometric mean (which uses Jensen's inequality) straight from what of these inequalities. I was thinking maybe the exponential could satisfy something like this:
$$f(x+y) \geq f(x) + f(y),$$
so by induction and with $f(x) = e^x$,
$$f(a+b+\dots+z) \leq f(a)+f(b)+ \dots + f(z).$$
This is ''superadditivity' and kind of says $f$ keeps growing faster and faster. One soon discovers this property is false, even for the exponential ( $x = 0$). Does it hold for $x >1$? What is the relationship  between
$$f(x+y) \geq f(x) + f(y)$$
and 
$$f\left(\frac{1}{2}x + \frac{1}{2}y\right) \leq \frac{1}{2}f(x) + \frac{1}{2}f(y)?$$ I'd also like to understand Jensen's inequality a little better. I've heard it kind of generalises the triangle inequality, since $\int |f| \geq |\int f|$ is like $\sum |x_i| \geq |\sum x_i|$. This makes sense, and I have some examples of this working (insightful ones are appreciated), but I don't really see what it's saying. Thanks for any help. @Eric Convex function with $f(0) = 0$ is superadditive on $[0,\infty)$, i.e. 
$$f(x+y) \geq f(x)+f(y), \quad x, y \geq 0.$$ First, note that if $f(0) = 0$ and $f$ is convex, i.e. $f(tx + (1-t)y) \leq t f(x) + (1-t)f(y)$, then:
\begin{align*}
f(tx) &= f(tx + (1-t)\cdot 0) \\
&\leq tf(x) + (1-t)f(0)\\
&= tf(x).
\end{align*} Hence, for $a,b \geq 0$, $\frac{a}{a+b}\in [0,1]$ and likewise for $\frac{b}{a+b}$. Thus, 
\begin{align*}
f(a) + f(b) &= f((a+b)\frac{a}{a+b}) + f((a+b)\frac{b}{a+b}) \\
&\leq \frac{a}{a+b}f(a+b) + \frac{b}{a+b}f(a+b) \\
&= f(a+b).
\end{align*} Looks right to me?","['convex-analysis', 'probability', 'real-analysis']"
507629,How exactly is Skorohod's Representation Theorem applied here?,"I have a question about applying the Skorohod Representation Theorem. Let $(R^{N+1}_+,\mathcal{F},Q)=:(\Omega,\mathcal{F},Q)$ be a probability space, $\{X_k:0\le k \le N\}$ be the canonical stochastic process, i.e. $X_k:\mathbb{R}^{N+1}_+\to\mathbb{R}$, $X_k((x_0,\dots,x_N))=x_k$ and $\mathcal{F}_k$ the filtration generated by $X_k$. Moreover we have a subset $F_n\subset \mathbb{R}_+$ such that $\lim_n F_n = \mathbb{R}_+$. Assume we have a continuous real-valued function $g:\mathbb{R}_+\to\mathbb{R}$ with $|g(x)|\le C(1+x^p)$ for $p>2$ and a constant $C$. Define $f_n:=g|_{F_n}$ to be the restriction of $g$ to $F_n$. The $f_n$ has the property that $\sup_{x\in F_n}\frac{|f_n(x)|}{(1+x^p)}\le n$ (maybe this is not needed). Since $g$ is continuous we have that for $x_n\ge 0$, $x\ge 0$ with $x_n\to x$ then $g(x)=\lim_nf_n(x_n)$. After all we have a tight sequence $(P_n)$ of probability measures given, hence there is a subsequence again denoted by $(P_n)$ which converges weakly to a measure $P$. Now using the Skorohod Representation Theorem we should establish the following equality: $$E_P[g(X_N)]=\lim_nE_{P_n}[f_n(X_N)]$$ where $E_p[\cdot]$ denotes the expectation w.r.t the measure $P$, and similaryly for $E_{P_n}[\cdot]$ w.r.t the measure $P_n$. How exactly is in this setting the Skorohod Repesentation Theorem applied? To interchange limit and Expectation, I think we need that $g(x)\le x^p$ and $E_P[S_N^p]<\infty$.","['probability-theory', 'measure-theory']"
507638,Intuition behind a matrix being invertible iff its determinant is non-zero,"Question I have been wondering about this question since I was in school. How can one number tell so much about the whole matrix being invertible or not? I know the proof of this statement now. But I would like to know the intuition behind this result and why this result is actually true. My Proof If $A$ is invertible, then $$ 1 = \det(I) = \det(AA^{-1}) = \det(A)\cdot\det(A^{-1})$$ whence $\det(A) \neq 0$ .
Conversely, if $\det(A) \neq 0$ , we have $$ A adj(A) = adj(A)A = \det(A)I$$ whence $A$ is invertible. $adj(A)$ is the adjugate matrix of $A$ . $$ adj(A)_{ji} = (-1)^{i+j}\det(A_{ij})$$ where $A_{ij}$ is the matrix obtained from $A$ by deleting $ith$ row and $jth$ column. Any other insightful proofs are also welcome.","['matrices', 'linear-algebra', 'intuition', 'determinant']"
507641,Show that the determinant of $A$ is equal to the product of its eigenvalues,"Show that the determinant of a matrix $A$ is equal to the product of its eigenvalues $\lambda_i$. So I'm having a tough time figuring this one out. I know that I have to work with the characteristic polynomial of the matrix $\det(A-\lambda I)$. But, when considering an $n \times n$ matrix, I do not know how to work out the proof. Should I just use the determinant formula for any $n \times n$ matrix? I'm guessing not, because that is quite complicated. Any insights would be great.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
507665,Decomposition of exponential random variable,I know that sum of independent Exponential random variables follows Gamma distribution . But Is it possible to decompose exponential random variate into independent and identically gamma random variates?,"['statistics', 'deconvolution', 'probability-distributions', 'probability']"
507671,The Galois group of a composite of Galois extensions,"Morandi's Field and Galois Theory, exercise 5.19b Let $K$ and $L$ be Galois extensions of $F$ . The restriction of function map, namely, $\sigma\mapsto(\sigma\vert_K,\sigma\vert_L)$ induces an injective group homomorphism $\varphi\colon\operatorname{Gal}(KL/F)\to\operatorname{Gal}(K/F)\times\operatorname{Gal}(L/F)$ . Show that $\varphi$ is surjective if and only if $K\cap L=F$ . It's not hard to show that $\varphi$ is a monomorphism. If it's surjective, it's not hard to show that $K\cap L=F$ as follow: Fix $\alpha\in K\cap L$ , let $\beta$ be a root of the minimal polynomial of $\alpha$ over $F$ . Since $K,L$ are normal, $\beta\in K\cap L$ . By isomorphism extension theorem, we can choose $\tau_1\in\operatorname{Gal}(K/F)$ such that $\tau_1(\alpha)=\beta$ . For surjectivity of the map, there's $\sigma$ such that $\sigma\vert_K=\tau_1$ and $\sigma_L=\mathrm{id}$ , which forces $\alpha=\beta$ , therefore $\alpha\in F$ , since $K,L$ are separable over $F$ . The converse seems hard. I cannot show that when $K,L$ are arbitrary Galois extensions. If they are both finite dimensional, the statement follows from natural irrationality: $\operatorname{Gal}(KL/L)\cong\operatorname{Gal}(K/K\cap L)$ , which implies that $[KL:L]=[K:K\cap L]=[K:F]$ , therefore $[KL:F]=[K:F][L:F]$ , and note that $\varphi$ is injective, thus surjective. Any help? Thanks!","['galois-theory', 'abstract-algebra', 'field-theory']"
507681,Problem : Permutation and Combination : In how many ways can we divide 12 students in groups of fours.,"Selecting groups of r items from n items Example : In how many ways we can select groups of two from 6
students. Concept : Let the students be A, B, C, D, E, F . A can group with
each of the remaining five as AB, AC, AD, AE, AF now
we have B, C, D, E, F left, B can group with each of
the remaining four as BC, BD, BE, BF, now we have C,
D, E, F left, C can group with each of the remaining
three as CD, CE, CF, now we have D, E, F left, D can
group with each of the remaining two as DE and DF, E
can group with the last one F , EF. Therefore we have
AB, AC, AD, AE, AF, BC, BD, BE, BF, CD, CE, CF, DE,
DF and EF, fifteen groups of twos. Alternatively
One student A can select the other one of his group in
$^5C_1$ ways . If B is another student who does not belong
in the group A, we can select the other one of his group
$^3C_1$ ways the remaining two consists of the third group.
= 5 $\times 3 \times 1$ But as per my understanding : I used : $^6C_2$ which is equal to 15. However if I am using this concept in the following question  : Problem : In how many ways can we divide 12 students in groups
of fours. A group of 4 can be chosen from 12 students is : $^4C_{12} = 495$ which is wrong answer and answer is $5775$ 
Please help on this.. Thanks..","['number-theory', 'combinatorics']"
507683,Evaluation of a series,"Some hints to start the evaluation of this series?
$$\sum_{k=0}^\infty \dfrac{2^{2k}(k
!)^2}{(2k)!(2k+1)^2}\left(\dfrac{1}3-\dfrac{1}{4^{k+1}}\right)$$",['calculus']
507692,General question about matrix calculus with specific example (with attempted answer),"I'm struggling to find the right way to approach matrix calculus problems generally . As an example of a problem that is bothering me, I would like to calculate the derivative of $||Ax||$ (Euclidean vector norm) with respect to the matrix $A$. How can I discover this via first principles? The natural thing seems to be to consider $||(A+H)x||-||Ax||$ as $||H||$ goes to zero but I don't see how to get something tangible from it. Addendum: This question is getting little attention. I am really looking for a general approach for solving these sorts of matrix calculus problems. In particular, finding the derivative with respect to a matrix of certain vector quantities. This comes up all the time in convex optimization algorithms like gradient descent and so on.\ Further: If we look at the derivative of $||Ax||^2$ with respect to $A$ we see that this expression can be written as trace$(Axx^TA^T$), so the derivative with respect to $A$ is $2xx^TU^T$. Edit : I don't know if this is the Frechet derivative per se, but I guess we can just notice that $||Ax||^p=(||Ax||^2)^{\frac{p}{2}}$, so by the power rule we get that the derivative of this is $p\cdot\frac{xx^T U^T}{||Ax||^{p/2 - 1}}$. Is this correct??","['matrices', 'multivariable-calculus']"
507696,Finding non-trivial functions $f(x)$ such that $\sum_{n=1}^{\infty}f(n)=\int_{0}^1f(x)dx$,"It is known that Atle Selberg found the following expression when he was 14 years old :
$$\sum_{n=1}^{\infty}n^{-n}=\int_{0}^1x^{-x}dx.$$ Then, here is my question. Question : Find the other non-trivial functions $f(x)$ such that
$$\sum_{n=1}^{\infty}f(n)=\int_{0}^1f(x)dx.\ \ \ \cdots(\star)$$ Motivation : I found the followings: Let $m$ be a natural number. Let us define a function $f(x)$ for $a\gt0$ as
$$f(x)=\frac1{(x+a)^m}-\frac1{(x+a+1)^m}\ \ \ (x\ge 0).$$
I'm going to prove that for any $m\in\mathbb N$ there exists only one $a\gt 0$ such that $f(x)$ satisfies $(\star)$. 1. The $m=1$ case. We get
$$\sum_{n=1}^{\infty}f(n)=\sum_{n=1}^{\infty}\left(\frac1{n+a}-\frac1{n+a+1}\right)=\frac1{a+1},$$
$$\int_{0}^1f(x)dx=[\log(x+a)-\log(x+a+1)]_0^1=-\log\left(1-\left(\frac1{a+1}\right)^2\right).$$ Hence, if there exist $0\lt u_0\lt 1$ such that $$u_0=-\log(1-{u_0}^2),$$then $f(x)$ satisfies $(\star)$ when $a=a_0={u_0}^{-1}-1$. In fact, we find that there exists only one such $u_0$ by observing 
$$G(u)=-\log(1-u^2)-u\ \ (0\lt u\lt 1).$$ 2. The $m\ge 2$ case. We get 
$$\sum_{n=1}^{\infty}f(n)=\frac1{(a+1)^m},$$
$$\int_{0}^1f(x)dx=\frac1{m-1}\left\{\frac1{a^{m-1}}-\frac2{(a+1)^{m-1}}+\frac1{(a+2)^{m-1}}\right\}.$$ Hence, let's prove that there exist $a\gt0$ such that
$$\frac1{(a+1)^m}=\frac1{m-1}\left\{\frac1{a^{m-1}}-\frac2{(a+1)^{m-1}}+\frac1{(a+2)^{m-1}}\right\}\ \ \ \ \cdots(\star\star)$$
Letting
$$h_m(a)=\frac{(a+1)^m}{a^{m-1}}+\frac{(a+1)^{m}}{(a+2)^{m-1}}-2(a+1)-(m-1),$$
then we know that 
$$(\star\star)\iff h_m(a)=0.$$ However, getting
$$\begin{align}
h_m(a) & = \frac{(a+1)^{m}}{a^{m-1}}+\frac{((a+2)-1)^m}{(a+2)^{m-1}}-2(a+1)-(m-1) \\
 & = \frac1{a^{m-1}}\sum_{k=0}^m\binom{m}{k}a^{m-k}+\frac1{(a+2)^{m-1}}\sum_{k=0}^m\binom{m}{k}(-1)^k(a+2)^{m-k}-2(a+1)-(m-1) \\ 
 & = -(m-1)+\sum_{k=2}^m\binom{m}{k}\left\{\frac1{a^{k-1}}+\frac{(-1)^k}{(a+2)^{k-1}}\right\}
\end{align}$$
tells us
$$\lim_{a\to +0}h_m(a)=+\infty, \lim_{a\to +\infty}h_m(a)=-(m-1)\lt 0,$$
$$h_m^{\prime}(a)=-\sum_{k=2}^m\binom{m}{k}(k-1)\left\{\frac1{a^k}+\frac{(-1)^k}{(a+2)^k}\right\}\lt0.$$ Here, note that 
$$\frac1{a^k}+\frac{(-1)^k}{(a+2)^k}\gt 0.$$ Hence, we know that there exists only one $a\gt 0$ such that $h_m(a)=0.$ Now the proof is completed. By the way, we know $a=\sqrt2$ for $m=2$. I've been looking for the other functions, but I cannot find any other function. Can anyone help?","['summation', 'integration']"
507725,How to find whether the line is inside the polygon or outside.,I have a polygon How can i prove whether the black color line lies outside the polygon or inside the polygon . Given the coordinates of the black line and all the vertices of the polygon.,"['linear-algebra', 'computational-geometry']"
507742,Distance/Similarity between two matrices,"I'm in the process of writing an application which identifies the closest matrix from a set of square matrices $M$ to a given square matrix $A$. The closest can be defined as the most similar. I think finding the distance between two given matrices is a fair approach since the smallest Euclidean distance is used to identify the closeness of vectors. I found that the distance between two matrices ($A,B$) could be calculated using the Frobenius distance $F$: $$F_{A,B} = \sqrt{trace((A-B)*(A-B)')} $$ where $B'$ represents the conjugate transpose of B. I have the following points I need to clarify Is the distance between matrices a fair measure of similarity? If distance is used, is Frobenius distance a fair measure for this problem? any other suggestions?","['numerical-linear-algebra', 'matrices', 'linear-algebra']"
507753,Understanding proof of Cartan's magic formula: $L_X = i_X \circ d+d \circ i_X$,"A possible proof of Cartan's magic formula $$L_X = i_X \circ d+d \circ i_X$$ is to follow the steps: Show that two derivations on $\Omega^{\bullet}(M)$ commuting with $d$ are equal iff they agree on $\Omega^0(M)$. Show that $L_X$ is a derivation on $\Omega^{\bullet}(M)$ commuting with $d$. Show that $i_X \circ d + d \circ i_X$ is a derivation on $\Omega^{\bullet}(M)$ commuting with $d$. Show that $L_X f = Xf = i_Xdf+ d i_Xf$ for all $f \in C^{\infty}(M)=\Omega^0(M)$. I followed the sketch of proof, and the only point where I am stuck is when I have to prove that $L_X$ and $d$ commute. More precisely, if I write $$L_Xd\omega =\frac{d}{dt}_{|t=0} \phi_t^* d\omega = \frac{d}{dt}_{|t=0} d\phi_t^* \omega = \lim\limits_{t \to 0} d \left( \frac{1}{t} \left( \phi_t^* \omega- \omega \right) \right),$$ is there an argument to permute $d$ and $\lim\limits_{t \to 0}$?","['differential-forms', 'differential-geometry']"
507772,if $AB\neq 0$ for any non zero matrix $B$ then $A$ is invertible,"Question is to check that : If $A$ is an $n\times n$ matrix over a field $F$ and $AB\neq 0$ for any non zero matrix $B_{n\times n}$ over $F$ then, $A$ is invertible. This does make some sense to me but i am not sure how to prove this. As $AB\neq 0$ for any $B$ , in particular, we have $A.A\neq 0$ i.e., $A^2\neq 0$ for similar reasons we see that $A^n\neq 0$ for any positive integer $n$ So, $A$ is not nilpotent... I see that this is just nilpotent... I am stuck to prove that $A$ is invertible. I do have some thoughts inbetween but nothing gives me simple way to conclude final result. please help me to see this by giving some hints (I am sure this must be very easy) Thank you","['matrices', 'linear-algebra']"
507774,When does an operator commute with another operator given by a series?,"Suppose $B$ is a bounded operator on some Hilbert space $\mathcal{H}$, given by a series of the form
$$
B = I + \sum^\infty_{k = 1} c_k(I - A)^k
$$
where $A$ is a given bounded operator on $\mathcal{H}$. If $C$ is another operator that commutes with $A$ then according to Reed and Simon I'd need the series above to be asbsolutely convergent in order to conclude that $C$ also commutes with $B$. Why do I need the absolute convergence ?","['operator-theory', 'hilbert-spaces', 'functional-analysis']"
507783,$\mathbb{RP}^3$ is homeomorphic to the solid ball with antipodal points identified,"I am reading the book Application of Path integrals by Schulman, which has a chapter on applications of homotopy theory to path integrals. In that he says we can geometrically describe $SO(3)$ by a solid (3 -dimensional) ball  with radius $\pi$,and with antipodal points identified. Each point in the ball at distance $\phi$ from the centre, represents a rotation about the axis passing through that point and origin, and angle of rotation $\phi$. Later he writes ""Projective 3 space is homeomorphic to the solid ball described earlier."" I imagine $\mathbb{RP}^3$ to be the 3-sphere with antipodal points identified. How is this homeomorphic to the solid ball described above?","['general-topology', 'lie-groups', 'algebraic-topology']"
507796,Show that $P_i$ and $\sum_i P_i$ being idempotent implies $P_i P_j=\delta_{ij}$,"Let $X$ be a finite dimensional real linear space, or more generally a finite dimensional vector space over a field of characteristic $0$ . 
Let $(P_i)_{i=1}^n$ be a finite sequence of linear mappings $P_i :X\rightarrow X$ such that $P_i^2=P_i$ for $i=1,...,n$ , $(P_1+...+P_n)^2=P_1+...+P_n$ . I wish to show that $$P_i\circ P_j=0 \textrm{ for } i \neq j.$$ I know how to prove it only for $n=2$ :
if $(P_1+P_2)^2=P_1+P_2$ then $$P_1P_2+P_2P_1=0. \tag{$\ast$}$$ By multiplying both sides this equality by $P_1$ from left and by $P_1$ by right and obtain two equalities: $P_1 P_2+P_1P_2P_1=0$ and $P_1P_2P_1+P_2P_1=0$ .
By subtracting: $$P_1P_2-P_2P_1=0. \tag{$\ast\ast$}$$ From $(\ast)$ , $({\ast}\ast)$ , we get $P_1P_2=0$ , $P_2P_1=0$ .","['projection-matrices', 'matrices', 'linear-algebra', 'operator-theory']"
507814,"If $n$ is odd and $abc=(n-a)(n-b)(n-c),$ then $LCM((n,a),(n,b),(n,c))=n.$","Prove that if $a,b,c,n\in \mathbb Z^{+},2\not\mid n$ and $$abc=(n-a)(n-b)(n-c),$$
  $x=(n,a),y=(n,b),z=(n,c),$ then $LCM(x,y,z)=n.$ If $n=35,a,b,c=5, 21, 28,$ then $x=(35,5)=5,y=(35,21)=7,z=(35,28)=7,LCM(x,y,z)=35.$ If $n=945,a,b,c=9, 756, 910,$ then $x,y,z=9, 35, 189,LCM(x,y,z)=945.$ I checked all $n<1000$ and this is always true. (On the other hand, if $2\mid n$ then $LCM(x,y,z)=n$ or $\dfrac n2.$)","['elementary-number-theory', 'number-theory']"
507817,Problem based on Prime numbers.,"Past codechef problem (Solutions are  public) Zach and Cody are playing a game. There are initially $N$ chips on a table. Zach starts the game making the first move. In each turn one has to choose a move from a set of moves. The set of moves consists of all the moves of removing $p^k$ chips from the table, where $p$ is any prime and $k$ is any non negative integer. The winner is the one to take the last chip. Can you decide who will win the game, assuming both the players follow a perfect strategy? If Zach wins, what will be the smallest possible number that he can remove in his first move? I have a very weak idea for its solution which is divisibility  by 6 after having look at many solutions but I need a conceptual explanation. Please provide that to get this concept. solution is if there are $n$ chips then if(n%6==0)
        Cody  wins
    else
        Zach wins the game and can choose minimum  n%6 chips in his first move. Again sorry for poor English.",['number-theory']
507827,Limit superior inequalities proof: $\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\ge e$,"Let $a_n$ be a positive sequence. Prove that 
$$\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\geqslant e.$$","['inequality', 'calculus', 'proof-writing', 'limsup-and-liminf']"
507843,Surjectivity of a function considering $f \circ h=Id_Y$,"\begin{align} f: \mathbb{R}^2 &\longrightarrow \mathbb{R} \\ (x,y) & \longmapsto x+y \end{align} Question: Is this function surjective? It seems clear to me that this function must be surjective, because the point $(x,y) \in \mathbb{R}^2$ maps to the entire codomain $\mathbb{R}$ under the addition given by function. But I wanted to try a different approach: Question (refined): Is this function surjective, using  $\ f\circ h=Id_\mathbb{R}$ Here are my steps. I defined $h$ as the following function:
\begin{align}h: \mathbb{R} &\longrightarrow \mathbb{R}^2 \\ x &\longmapsto (x,0) \end{align} Such that:
\begin{align} (f \circ h)(x)=x&=f(h(x)) \\ &=f(x,0)=x \end{align}
Are these steps correct, or aren't they even valid? Note : I am very new to this subject and in my homework assigment I am also allowed to use examples and counter examples, but I am always very eager to expand my knowledge to such sentences as introduced as above. Namely: if a right inverse exists, such that $f \circ h= Id_Y$, where $h$ is called the right inverse , then the function $f$ is surjective.",['analysis']
507865,Check if this proof about real numbers with an irrational product is correct.,"Can anyone confirm if my proof is correct, please? Claim:- “If $x$ and $y$ are real numbers and their product is irrational, then either $x$ or $y$ must be irrational.” Proof:- Assume that both $x$ and $y$ are rational.
Now, let $x = \dfrac pq$ and $y = \dfrac mn$ since both of them are rational.
$xy =\dfrac pq * \dfrac mn = \dfrac{pm}{qn}$
Thus, if the product $xy$ can be written as a fraction, it's not a irrational number.
Therefore if one of $x$ and $y$ is not irrational, then the product is not irrational. By the principle of proof by contraposition, If $x$ and $y$ are real numbers and their product is irrational, then either $x$ or $y$ must be irrational.","['products', 'discrete-mathematics', 'proof-verification', 'irrational-numbers']"
507868,"supremum of $\int |\int f(x)-f(y)\,dy| \,dx $.","Let $A_k$, $k\in \mathbb{N}$, be the family of $C^\infty([0,1])$ functions defined by
$$
A_k=\{||f^{(j)}||_{\infty}\le 1,\;\;0\le j \le k \}
$$
where $||\cdot||_{\infty}$ denotes the supremum norm over $[0,1]$ and $f^{(k)}$ is the k-th derivative of $f$. 
We define
$$
\sigma_k=\sup \left\{\int_0^1\left|\int_0^1f(x)-f(y)\,dy\right|\,dx ,\;\; f\in A_k\right\}
$$ What is the value of $\sigma_k$? Edit: I think $\sigma_0=1$. I believe $\sigma_k= \frac{1}{4}$ for all $k>0$ (with $f=x$ the supremum is reached). It is not homework and is not related to my work. Just curious. I hope you find interesting too. (((The problem turned out to be uninteresting.))) Thanks.","['functional-analysis', 'real-analysis']"
507879,Time taken to complete a piece of work,"A can complete a piece of work in 80 days . He worked work for 10 days , after that B completed the remaining work in 42 days .
If A and B work together how many days will it take them to complete the entire piece of work? a)30 b)25 c)40 d)none of these As per my solution the answer is d option but i still doubt my solution. Please help to find the correct solution.",['algebra-precalculus']
507881,Possible solutions for $f(e) = 5$ of polynomial $f$ given four values,"I'm preparing for an exam by solving the sample questions , here is the one I'm having difficulty with: Let $f(x)$ be a polynomial such that $f (a) = f (b) = f (c) = f (d) = 3$ , where $a, b, c, d $ are distinct integers. If $f(e) = 5$ , where $e$ is an integer then $e = \ $ ? (A) 1 (B) 3 (C) 4 (D) no real value of e is possible My gut feeling is that the polynomial must be degree $4$ or greater because it gives the same value for at least $4$ different integers. Is this correct?   If so, why? Anyway, I also think $e$ should be greater than $a,b,c,d$ because the polynomial evaluates to a greater value when $e$ is substituted than when $a,b,c,d$ are substituted. Is this correct? If so , why? Please guide me how to solve this problem. Thank you.","['algebra-precalculus', 'polynomials']"
507897,"How to prove the limit of a sequence using ""$\epsilon-N$""","I think I have a proper understanding of the general procedure, but I'm having difficulty manipulating my inequality so that I can isolate $n$ by itself. Sadly I wasn't given many examples to model my answer on. Prove that $\displaystyle\lim_{n\to\infty}\frac{n+1}{n^2+1}=0$ So I'm given $L=0$. I then look at the inequality $$\left| \frac{n+1}{n^2+1}-0\right|<\epsilon$$ but I have no idea how to isolate $n$. The best I can come up with, which may be the right idea, is to use another function $f$ such that $$\left|\frac{n+1}{n^2+1}\right|<f<\epsilon$$ and then work with that. But my idea of using $f=\lvert n+1\rvert$ seems to have me a bit stuck too.","['epsilon-delta', 'calculus', 'real-analysis', 'limits']"
507899,Proving the rationals are dense in R,"I know this is a common proof. I'm following Rudin's proof and I'm following everything except for one step. Suppose $x, y \in \Bbb R$ and $x < y$. Then there exists an $n \in \Bbb N$ such that $n(y-x) > 1$. Again by the Archimedean property, there exist $m_{1}, m_{2} \in \Bbb N$ such that $m_{1} > nx$ and $m_{2} > -nx$, i.e.
$$
-m_{2} < nx < m_{1}
$$ From here, Rudin says there must be an $m \in \Bbb Z$ with $-m_{2} \le m \le m_{1}$ and that
$$
m-1 \le nx < m
$$ I'm confused about these two steps. If $-m_{2} < nx < m_{1}$, then isn't $-m_{2} < m_{1}$? edit: to be clear, I follow everything up until the introduction of $m$.","['real-analysis', 'rational-numbers']"
507903,"About the set of all the solutions $\mathbf x=(x_1,x_2,\cdots,x_m)$ to $\sum_{j=1}^m\frac{1}{x_j}=\frac1n$","Let $m,n$ be natural numbers, and let $S_{m,n}$ be the set of all the natural number solutions $\mathbf x=(x_1,x_2,\cdots,x_m)$ to the following equation : $$\sum_{j=1}^m\frac{1}{x_j}=\frac1n.$$ Also, letting $$k_{m,n}=\max_{\mathbf x\in S_{m,n}}\left(\max_{1\le j\le m}x_j\right)$$ then, here is my question. Question : Is the following true for $m\ge 2$ ? $$k_{m,n}=k_{m-1,n}\left(k_{m-1,n}+1\right).$$ Motivation : I've been asking the following question on MSE and MO. What is the max of $n$ such that $\sum_{i=1}^n\frac{1}{a_i}=1$ where $2\le a_1\lt a_2\lt\cdots\lt a_n\le 99$? This got me interested in $S_{m,n}.$ In the following, I'm going to prove that $S_{m,n}$ is a finite set. Proof : Let $S_{m,n}^{\gt}$ be the set of all solutions $\mathbf x=(x_1,x_2,\cdots,x_m)$ to the above equation such that $x_1\ge x_2\ge \cdots\ge x_m$ . Since any solution $\mathbf x$ is one that we can get from the elements of $S_{m,n}^{\gt}$ by exchanging its coordinates, we get $$|S_{m,n}|\le m! |S_{m,n}^{\gt}|$$ where $|S|$ represents the number of the elements of a set $S$ . Hence, in the following, let's prove that $S_{m,n}^{\gt}$ is a finite set for any natural number $n$ by induction on $m$ . The $m=1$ case is obvious. Then, let's suppose that $S_{m-1,n}^{\gt}$ is a finite set. Since $$\sum_{j=1}^{m-1}\frac1{x^j}=\frac1n-\frac1{x_m}=\frac{x_m-n}{nx_m},$$ we get $$\sum_{j=1}^{m-1}=\frac{1}{(x_m-n)x_j}=\frac{1}{nx_m}\ \ \ \ \cdots(\star).$$ Now, $x_m$ satisfies the following inequality : $$n+1\le x_m\le mn\ \ \ \ \ \cdots(\star\star).$$ This is because if we deny $(\star\star)$ , then $\mathbf x$ cannot be a solution of the equation. For a $x_m$ which satisfies $(\star\star)$ , the number of solutions $\mathbf x^{\prime}=\left((x_m-n)x_1,(x_m-n)x_2,\cdots,(x_m-n)x_{m-1}\right)$ which satisfies $(\star)$ is finite by the supposition. Since the number of $x_m$ which satisfies $(\star\star)$ is finite as well, $|S_{m,n}^{\gt}|$ is finite. Now the proof is completed. In the following, I'm going to represent $k_{3,n}$ by $n$ . First of all, note that $k_{2,n}=n(n+1)$ . Next, we know that $$\left(n(n+1)(n(n+1)+1),n(n+1)+1,n+1\right)\in S_{3,n}^{\gt}.$$ Hence, by the definition of $k_{3,n}$ , we get $$k_{3,n}\ge n(n+1)(n(n+1)+1).$$ Now, for any $\mathbf x=(x_1,x_2,x_3)$ , we get $(rx_1,rx_2)\in S_{2,n(n+r)}^{\gt}$ by $(\star)$ where $r=x_3-n$ and $1\le r\le 2n$ by $(\star\star).$ Hence, by the definition of $k_{m,n}$ , we get $$rx_1\le k_{2,n(n+r)}=n(n+r)(n(n+r)+1).$$ Hence, we get $$x_1\le \frac 1r n(n+r)(n(n+r)+1).$$ However, since $$n(n+1)(n(n+1)+1)-\frac 1r n(n+r)(n(n+r)+1)=\frac{1}{r}n^2(n^2-n+1)(r-1)\ge \frac 1r n^2(n^2-2n+1)(r-1)\ge 0,$$ we get $$x_1\le n(n+1)(n(n+1)+1).$$ After observing this question, I reached the above expectation. However I can neither prove that this is true nor find any counterexample. Can anyone help? Update : I crossposted to MO .","['fractions', 'elementary-set-theory', 'number-theory']"
507909,Product of exponents of prime factorization,"Let $p(n)$ be the product of the exponents of the prime factorization of $n$. For example,
$$p(5184) = p(2^6 3^4) = 24 \;,$$
$$p(65536) = p(2^{16}) = 16 \;.$$
Define $P(n)$ as the number of iterations of $p(\;)$ to reduce $n$ to $1$. For example,
$P(5184) = 3$ because 
$$p(5184)=24, \;p(24) = p(2^3 3^1) = 3, \;p(3)=1 \;;$$
and $P(65536)=4$ because
$$p(65536) = 16, \;p(16)=p(2^4)=4, \;p(4)=p(2^2)=2, \; p(2)=1 \;.$$
Finally, define $m(k)$ to be the minimum value of $n$ such that $P(n) = k$. What is $m(k)$? For example, $m(1)=2$ and $m(2)=4$ and $m(3)=16$. Is $m(k)$ always $2$ to some power? Update . Calvin Lin showed that $m(4)$ is not a power of $2$, and is at most
$2^4 3^4$. Indeed I have verified (by search) that $m(4)=1296$.","['prime-numbers', 'prime-factorization', 'elementary-number-theory', 'number-theory']"
507913,Question about polynomial $\sum_{j=1}^n j^k$,"How could I prove that
$ 1^k + 2^k + \cdots + n^k \in \Theta(n^{k+1}) $
or, equivalently,
$$ 0 < \lim_{n\to\infty}\frac{\sum_{i=1}^n i^k}{n^{k+1}} < \infty? $$
I would appreciate a hint rather than a solution. Thanks in advance. ( I am sorry if this question is duplicate, I've searched but didn't found anything similar )","['polynomials', 'computational-complexity', 'real-analysis', 'limits']"
507920,"Place the numbers $1,2,3....,10$ in a random order on a circular table with 10 places.Prove there are three consecutive numbers with a sum of $\ge17$.","Let us place the numbers $1,2,3....,10$ in a random order
on a circular table with 10 places. The question is: prove that there are three consecutive numbers with a sum of 17 or more. I know that we need to use the ""Generalization of the pigeonhole principle"" to solve that problem, I just don't know how to use it. Any help will be appreciated!","['pigeonhole-principle', 'discrete-mathematics', 'combinatorics']"
507939,$n$th derivative of $x^3(x-2x^{1/2})^2$,I don't know how to find the n th derivative of this equation because there are negative integer so I don't how to use factorial on it. I Try(?): $\left(f(x)g(x)\right)^{(n)}=\sum_{k=0}^n\binom nk f^{(k)}(x)\cdot g^{(n-k)}(x)$ Let $f(x) = x^4\ and \ g(x) = (\sqrt(x)-2)^2 )$ $( x^4(\sqrt(x)-2)^2)^{(n)}=\sum_{k=0}^n\frac{n!}{k!(n-k)!}\frac{4!}{(4-n)!}x^{(4-n)}\cdot \frac{(\sqrt(x)-2)^2\cdot2!}{(x-n+k)!}$ Is this?,"['calculus', 'derivatives']"
507950,Can we define the $L^2$ norm for a vector field $F: \Omega \subseteq \mathbb{R}^d \to \mathbb{R}^d$?,"Let $\Omega \subseteq \mathbb{R}^d$ be open and suppose we have a measurable vector field $F : \Omega \to \mathbb{R}^d$ (we consider both the domain $\Omega$ and range $\mathbb{R}^d$ with Lebesgue measure). Is there a widely used notion of an $L^2$ norm for this vector field $F$? I know that, if the range is just $\mathbb{R}$ instead of $\mathbb{R}^d$, then of course the $L^2$ norm is simply $$\left( \int_{\Omega} |F|^2 dm \right)^{\frac{1}{2}}.$$ This question has come up for me because I'm reading over a paper that keeps using the notation $||F||_{(L^2(\Omega))^d}$ where $F$ is $C^\infty$ from $\Omega$ to $\mathbb{R}^d$. But I'm not sure what this norm is exactly. Could it just be that the that we are integrating $|F|^2$ again? Only this time, we would need $|\cdot|$ to be the Euclidean norm on $\mathbb{R}^d$ instead of the absolute value function. Explanations are greatly appreciated.","['notation', 'measure-theory']"
507965,quasicompact schemes,"I've proved that if $X$ is a quasi-compact scheme than every point has a closed point in its closure and so every closed subset of $X$ contains a closed point of $X$. Why this statement implies the following statement: if there is a property $P$ of points of a scheme that is ""open"" then to check if all points of a quasicompact scheme have $P$ it suffices to check only the closed points?","['geometry', 'algebraic-geometry']"
507967,Evaluate $ \lim_{x\rightarrow{\frac\pi2 }} (\sec(x) \tan(x))^{\cos(x)}$ without L'Hôpital's rule,"I have tried changing limit to $\lim_{x\rightarrow0}$ and use some trigonometry identity ($\sin^2(x)+\cos^2(x) = 1$ and $\sin (x+\pi/2) = \cos(x)$)  but doesn't work I have no idea on how to do this now... $$
\lim_{x\rightarrow{\frac\pi2 }} (\sec(x) \tan(x))^{\cos(x)} $$","['trigonometry', 'calculus', 'limits']"
507973,Choosing the best estimator [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Given yield measurements $X_1,X_2,X_3$ from three independent runs of an experiment with variance $\sigma^2$, which is the better of the two estimators:
$\hat\theta_{1}$= $\frac{X_1+X_2+X_3}{3}$,                  $\hat\theta_{2}$=$\frac{X_1+2X_2+X_3}{4}$ I know that in order to find the best estimator if both are unbiased, we are supposed to choose the one with the smallest variance.  I need help just starting this problem.  Thank you.","['statistics', 'probability']"
507975,Show that $n^3 > 2n+1$ for all $n \geq 2$,"I'm new here and unsure if this is the right way to format a problem, but here goes nothing. I'm currently trying to solve an inequality proof to show that $n^3 > 2n+1$ for all $n \geq 2$. I proved the first step $(P(2))$, which comes out to $8>5$, which is true. In the next step we assume that for some $k \geq 2$, $k^3 > 2k+1$.
Then we consider the quotient $\frac{f(k+1)}{f(k)} > \frac{g(k+1)}{g(k)}$. I so far have simplified it to the following: $$\begin{align*}
\frac{(k+1)^3}{k^3}   &>   \frac{2(k+1)+1}{2k+1}\\       
&= \frac{k^3+3k^2+3k+1)}{k^3}\\ &> \frac{2k+3}{2k+1}\\
&= 1 + \frac{3}{k} + \frac{3}{k^2} + \frac{1}{k^3}\\ &> \frac{2k+3}{2k+1}
\end{align*}$$ I don't know how to simplify the right side anymore (my algebra is terrible). I know that I have to simplify that inequality and multiply it by our previous assumption. I should end up with some variant of $(k+1)^3 > 2(k+1)+1$.  (This is $P(k+1)$). I just need help simplifying. Thanks!","['inequality', 'discrete-mathematics']"
508019,Translating a passage of a paper by L. Bérard Bergery,"I am currently studying the following paper on Einstein manifolds: L. Bérard Bergery, Sur de nouvelles variétés riemanniennes d'Einstein , Inst. Elie Cartan, Univ. Nancy №6, 1-60 (1983). I have doubts that my translation of the following sentence is correct. Also the quality of my copy is poor unfortunately, at two points I have to guess the wording and this is particularly difficult since I don't speak French. Here we go: La fibration naturelle $G/K \to G/H$ est donc ici le fibré en sphère d'un fibré vectoriel sur $G/H$, de groupe structural H..(letters missing) G-invariant. My translation: The natural fibration $G/K \to G/H$ is therefore the fibration into spheres of a vector bundle over $G/H$, with structure group $H$ which is $G$-invariant. In case this is difficult to judge I can provide more context. Below is an image of the page in question:","['translation-request', 'differential-geometry', 'manifolds', 'lie-groups', 'mathematical-french']"
508034,Finding integral roots of $x^2 + px + q = 0$ if $p+q=198$.,"Given the relation that $p+q=198$, the question is to find all the integral roots of the equation:
$$ x^2+px + q = 0 $$ How to proceed? I know we'll have to use Vieta's formulas, but I don't know how to. Hints?","['quadratics', 'algebra-precalculus']"
508045,"If the sum and the product of two sequences converges to zero, does that mean that each sequence converges to zero?","If the sum and the product of two sequences converges to zero, 
does that mean that each sequence converges to zero ? Thanks","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
508069,Complete convergence is equivalent to convergence a.s. under independence,"$X_1,X_2,\ldots$ is a sequence of random variables that are complete convergent to $X$ if $$\sum_{n=1}^{\infty} P(\mid X_n-X\mid >\epsilon)<\infty \space\forall\epsilon > 0$$. Show if $X_n$ are independent, then complete convergence is equivalent to convergence a.s. I showed that complete convergence implies convergence a.s. using the Borel-Cantelli lemma, but I'm not sure how to use show the converse using independence. This is what I have so far: $$X_n\rightarrow_{a.s.} X \implies X_n\rightarrow_{p} 0$$ WLOG, $X=0$. $$\forall\epsilon >0, P(\mid X_n\mid > \epsilon)\rightarrow 0$$
$$P(\limsup_{n\rightarrow\infty} \mid X_n\mid > \epsilon)=0$$ Since they are independent, then $\{\mid X\mid > \epsilon\}$ are independent, and can I use Borel-Cantelli (ii) to say $\sum P(\mid X_n\mid >\epsilon) < \infty$?","['measure-theory', 'probability']"
508075,How many regions do $n$ lines divide the plane into? [duplicate],"This question already has answers here : Show that $n$ lines separate the plane into $\frac{n^2+n+2}{2}$ regions (3 answers) Closed 9 years ago . Suppose you draw $n \ge 0$ distinct lines in the plane, one after another, none of the lines parallel to any other and no three lines intersecting at a common point. The plane will, as a result, be divided into how many different regions $L_n$? Find an expression for $L_n$ in terms of $L_{n-1}$, solve it explicitly, and indicate what is $L_{10}$. I have tried to come up with a solution but cannot. A little guidance would be very helpful.",['discrete-mathematics']
508087,"Integral $\int_0^\infty\left(x+5\,x^5\right)\operatorname{erfc}\left(x+x^5\right)\,dx$","Is it possible to find a closed form (possibly using known special functions) for this integral?
$$\int_0^\infty\left(5\,x^5+x\right)\operatorname{erfc}\left(x^5+x\right)\,dx$$
where $\operatorname{erfc}$ is the complementary error function $$\operatorname{erfc} x=\frac{2}{\sqrt{\pi}}\int_x^{\infty}e^{-z^2}dz.$$","['closed-form', 'calculus', 'integration', 'definite-integrals', 'error-function']"
508116,Examples of non-obvious isomorphisms following from the first isomorphism theorem,"I am learning the first isomorphism theorem, and I am working with some isomorphisms to practice for my upcoming test. I know some of the basic ones like: $\mathbb{R}/\mathbb{Z} \cong \mathcal{C}$, where $\mathcal{C}$ is the unit circle in the complex plane, under the isomorphism $$x+\mathbb{Z}\mapsto e^{2\pi x i}$$ $\dfrac{\mathbb Z \times \mathbb Z}{\langle (m,n)\rangle}\cong\mathbb Z$, where $m,n$ are integers. $\dfrac{\mathbb R^\star}{\{1, -1\}} \cong \mathbb R^+$. I would like to see more examples of such isomorphisms, intended both as a reference and to help me study for the test! Thank you.","['big-list', 'exceptional-isomorphisms', 'group-theory', 'abstract-algebra']"
508158,"proof that $\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9","I am following Stein and Shakarchi's book on analysis ( Real Analysis: Measure Theory, Integration, and Hilbert Spaces ) and in Proposition 3.9 on p.86 they present a proof that if $f$ is a measurable function on $\mathbb{R}^d$ then $\hat{f}(x,y)=f(x-y)$ is measurable on $\mathbb{R}^d \times \mathbb{R}^d$. The strange thing is, I can follow all the arguments in the proof but I cannot make sense of all of it, I can't string the facts together. A snipped of the proof is as follows: (from Google books) The book concludes the proof by saying that any measurable set $E$ can be written as a difference of a $G_\delta$ and a set of measure 0. Alright so if I proceed with this then because $E=\{z \in \mathbb{R}^d:f(z)<a \} $ as defined in the book is measurable, then it can be written as $A-B$ where $A$ is a $G_\delta$ set while $m(B)=0.$ Now how do I relate this to $\tilde{E}$ as defined in the proof? help very much appreciated!","['integration', 'real-analysis']"
508175,Condition for a compact set to be the support of a continuous function?,"I am studying Rudin's Real and Complex Analysis exercises and I am currently thinking about the following: Is there a characterization of the class of compact sets of $\mathbb{R}$ which are supports of continuous functions? Is this characterization valid in other topological spaces? For the first question, I have come to a seemingly necessary and sufficient condition on $K$, which may be a bit too complicated : $K$ is the support of a continuous function iff for all $x\in K$, for all neighborhood $V$ of $x$, there exists a nonempty open set $U\subset K\cap V$. The fact that it is necessary is pretty obvious, and for the sufficient aspect, I have considered the connected components of such a $K$, which are segments. I call a ''trivial segment'' a segment which is a singleton. Putting aside the trivial case where $K$ is empty, there is at least one nontrivial segment among the connected components of $K$. The following two cases can arise: If the number of nontrivial connected components is finite, there is no trivial segment and $f$ can be defined as a triangle-shape function on all the connected components and $0$ otherwise. $f$ is continuous and has support $K$. If the number of nontrivial connected components is infinite, it is countable and the nontrivial components can be ordered as $\{C_n\}_{n>0}$. On each $C_n$ $f$ is defined as a triangle-shape function of height $1/n$, and $0$ outside of $\cup_{n>0}C_n$. Again, $f$ is continuous and has support $K$. I am now considering the extension to other spaces and I can't really figure out a way to generalize the result. The necessary condition on $K$ seems to remain valid on a general topological space, but for the other part of the proof, I have used the particular nature of the connected sets of $\mathbb{R}$, which cannot be used in the general case. Is there a way to claim a more general result, maybe involving a simpler characterization?",['general-topology']
508190,"A strange ""pattern"" in the continued fraction convergents of pi?","From the simple continued fraction of $\pi$ , one gets the convergents, $$p_n = \frac{3}{1}, \frac{22}{7}, \frac{333}{106}, \frac{355}{113}, \frac{103993}{33102}, \frac{104348}{33215}, \frac{208341}{66317}, \frac{312689}{99532}, \frac{833719}{265381}, \frac{1146408}{364913}, \dots,$$ starting with $n=1$ , where the numerators and denominators are A002485 and A002486 , respectively. If you stare at it hard enough, a pattern will emerge between three consecutive convergents. Define, $$\left(a_n,\,b_n,\,c_n\right) = \left(p_{n}-3,\;\; p_{n+1}-3,\;\; p_{n+2}-3\right),$$ $$v_n=\text{Numerator}\,(a_n)\,\text{Numerator}\,(b_n).$$ Then, for even $n \ge 2$ , $$F(n) = \sqrt{\frac{a_n c_n}{a_n-c_n}-v_n}\in\mathbb{Z}\text{ (often)}.$$ For example, for $n = 2$ , $$\left(a_2,\,b_2,\,c_2\right) = \left(\frac{22}{7}-3,\; \frac{333}{106}-3,\; \frac{355}{113}-3\right),$$ $$F(2) = 1.$$ More generally, $$\begin{array}{cc}
n&F(n) \\
2&1 \\ 
4&16\\
6&4703\\
8&14093\\ 
10&51669\\
12&122126\sqrt{2}\\
14&7468474\\ 
16&\frac{18549059}{\sqrt{2}}\\
\end{array}$$ and so on. For even $n<100$ , I found half of the $F(n)$ were either integer or half-integer. (And all the non-integers were of form $N\sqrt{d}$ for some very small d .) Some questions: For $n<500$ , $n<1000$ , etc, how many $F(n)$ are integers or half-integers? More importantly, why is $F(n)$ often an integer?","['sequences-and-series', 'calculus', 'continued-fractions', 'approximation', 'pi']"
508203,Failure of Doob-Dynkin lemma in general measurable spaces,"The version of the Doob-Dynkin lemma given in my textbook is as follows: Let $f: \Omega_1 \to \Omega_2$ be a function, let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega_2$, and let $\sigma(f)$ be the $\sigma$-algebra on $\Omega_1$ generated by $f$. Denote the Borel $\sigma$-algebra on $\mathbb{R}$ by $B(\mathbb{R})$. Then, $h : (\Omega_1, \sigma(f)) \to (\mathbb{R}, B(\mathbb{R})) $ is measurable if and only if $h = g \circ f$ for some measurable function $g: (\Omega_2, \mathcal{F}) \to (\mathbb{R}, B(\mathbb{R}))  $. My textbook also states that this lemma fails if we replace $(\mathbb{R},B(\mathbb{R}))$ by some other measurable space, but does not provide an example of this failure. As such, I'm trying to come up with such an example on my own, but am not sure where to begin. Any help is appreciated!","['probability-theory', 'measure-theory', 'real-analysis']"
