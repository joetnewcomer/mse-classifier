question_id,title,body,tags
4381862,Are 4 colors necessary to properly color adjacent countries of congruent shape?,"The four color theorem proved that at most 4 colors are necessary to properly color any map (countries sharing a border have different colors). My question is the following: Does there exist a map with 4 countries of congruent shape (same shape and same size, rotations and translations allowed) such that 4 colors are necessary to color this map?","['coloring', 'geometry']"
4381916,Find all function $f:\mathbb Q^+ \rightarrow \mathbb Q^+$ that satisfy $f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$ and $f(2x+1)=\dfrac{f(x)}{2}$,Find all function $f:\mathbb Q^+ \rightarrow \mathbb Q^+$ that satisfy $$f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$$ and $$f(2x+1)=\dfrac{f(x)}{2}$$ for all $x$ in domain of $f$ My Approach: Put $x=1$ in $f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$ $\implies$ $f(1)+f(1)=1$ $\implies$ $f(1)=\dfrac{1}{2}$ Now put $x=2$ in $f(x)+f\bigg(\dfrac{1}{x}\bigg)$ we obtain $f(2)+f(\dfrac{1}{2})=1 \cdots \cdots (1)$ Now using $f(2x+1)=\dfrac{f(x)}{2}\cdots \cdots (2)$ Put $x=\dfrac{1}{2}$ in eqation $(2)$ $\implies$ $f\bigg(\dfrac{1}{2}\bigg)=2f(2)$ Using above result in Equation $(1)$ we obtain $f(2)=\dfrac{1}{3}$ In similar fashion we finally obtain $f(x)=\dfrac{1}{1+x}$ For Integers I want to know Some other method to solve this I also tried using $g(x)=f(x-1)$ like this Help in solving a simple functional equation: $3f(2x+1)=f(x) + 5x$ . But couldn't get desired result Also this question is similar to Showing that $f(x)=\frac x{x+1}$ is the unique function satisfying $f(x)+f\left(\frac1x\right)=1$ and $f(2x)=2f\big(f(x)\big)$,"['functional-equations', 'algebra-precalculus', 'functions']"
4381952,The average area of the shadow of a square,"This question comes from my attempt at solving the problem of the average area of a cube in an unconventional way. As the shadow of the cube is comprised of the shadows of 3 of its square faces, I am attempting to calculate the average area of each of those squares' shadows, multiply it by three, and arrive at the answer of $\frac{3s^2}{2}$ . That means that the average area of the shadow of a square should be $\frac{s^2}{2}$ , which makes intuitive sense, but I want to prove it mathematically, which I am having difficulty doing. Here's my work so far: Consider a square $\mathrm{S}$ with side lengths $s$ that is lifted up in both the $\mathrm{x}$ and $\mathrm{y}$ directions by $\theta_1$ and $\theta_2$ degrees respectfully. Then, for these $\theta$ values, the area of the square becomes $s^2\mathrm{cos}\theta_1\mathrm{cos}\theta_2$ , as can be determined by trigonometry (a right triangle with a hypotenuse of $s$ , angle $\theta$ , and side $x$ adjacent to the angle gives $x=s\mathrm{cos}\theta$ ). The median value of $\mathrm{cos}\theta$ on the interval $[0, \pi/2]$ is $\frac{\sqrt{2}}{2}$ (appearing at $\frac{\pi}{4}$ ). Here's the part I don't fully understand: Let $\mathrm{A(S_{(\theta_a, \theta_b)})}$ be a function of a square $\mathrm{S}$ that has been lifted by $\theta_a$ and $\theta_b$ on the $\mathrm{x}$ -axis and $\mathrm{y}$ -axis respectively. To find the average area of this square, we can do the following: $$\frac{\iint_R \mathrm{A(S_{(\theta_a, \theta_b)})}dA}{R_{area}}, \{R\in\mathbb{R}^2 \vert R = [0,\frac{\pi}{2}] \times [\frac{\pi}{2}]\} \\ = \frac{s^2\int_0^{\pi/2}\int_0^{\pi/2}\mathrm{cos}\theta_1\mathrm{cos}\theta_2d\theta_1d\theta_2}{\frac{\pi^2}{4}} \\ = \frac{4s^2}{\pi^2}$$ This does not yield the result I'm looking for, so I'm wondering where I messed in my logical deduction. Can I just take the median value of $\mathrm{cos}x$ over $[0,\frac{\pi}{2}]$ ? If so, that removes the need for the integral, but it seems to simple, and doesn't explain why the two contradict each other. Thanks in advance.","['trigonometry', 'calculus', 'geometry']"
4382004,"Why define Sobolev spaces $W^{1,p}_0$ as completion of $C^1_c$ and not of $C^1_0$?","Let $C^1_c(\Omega)$ be the once continuously differentiable functions with compact support, and $C^1_0(\Omega)$ once cont' diff' functions which approach zero on the boundary of $\Omega$ . I am wondering why books don't define $W^{1,p}_0$ as the completion of $C^1_0$ , and instead go in a roundabout way and prove that $C^1_0\subset W^{1,p}_0$ . I mean, when trying to prove that a classical solution which is in $C^2_0$ to some BVP is also a weak solution, it would be trivial if one would define $W^{1,p}$ as I suggested. Instead, the standard definition seems to be as the completion of $C^1_c$ , and then it takes some effort to prove that a classical solution is in $W^{1,p}_0$ . Is there something I'm missing that makes my suggested definition not good?","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4382034,"$Spin(6,2) = SU(2, 2, \mathbb{H})$","On Wikipedia https://en.wikipedia.org/wiki/Spin_group#Indefinite_signature , it says $Spin(6,2) = SU(2, 2, \mathbb{H})$ . But I cannot find any reference. Does anyone know one, or any other references which explains what this spin group is? Thank you in advance!","['lie-algebras', 'spin-geometry', 'reference-request', 'group-theory', 'lie-groups']"
4382051,"Associate bundles, equivariant sections and tangent elements","Consider a principal $G$ -bundle over a manifold $X$ , and consider yet another manifold $B$ endowed with a $G$ -action. Everything is assumed to be smooth. The associated bundle $$
\mathcal{B}=P\times_G B
$$ is a locally trivial bundle over $X$ . My question concerns the space of sections of this associated bundle, i.e., the infinite-dimensional manifold $\mathcal{S} =\Gamma(X,\mathcal{B})$ . Assume we have fixed such a section $\phi\in \Gamma(X,\mathcal{B})$ . I want to characterize, if possible, the tangent space $$
T_\phi \Gamma(X,\mathcal{B})
$$ of infinitesimal deformations $\dot{\phi}$ of that section, and I want to do it in such a way that $G$ -invariant geometric objects on $B$ give corresponding objects in this infinite-dimensional manifold $\Gamma(X,\mathcal{B})$ (will be detailed below). I want to analyze this tangent space under the light of the following fact: sections of the associated bundle correspond to $G$ -equivariant mappings $\tilde{\phi}:P\rightarrow B$ . Preliminary remarks A section $\phi$ has to always be vertical with respect to the base space. This means that if $\phi_t$ is a parametrized curve of sections of $\mathcal{B}$ , $\phi_0=\phi$ , then $\frac{d}{dt}|_{t=0} \phi_t$ has to send elements of the base $x\in X$ to vertical vectors in $\mathcal{B}$ attached at $\phi(x)$ . We could justify $$
T_\phi \mathcal{S} \simeq \Gamma(X,\phi^*V\mathcal{B})
$$ where $\phi^*V\mathcal{B}$ is the pullback to $X$ under $\phi$ of the vertical bundle of $\mathcal{B}$ . What I am missing I want a mechanism of lifting $G$ -invariant objects on $B$ to objects on $\mathcal{B}$ . Specifically, assume we have a $G$ -invariant 1-form $\sigma\in \Gamma(B,T^*B)$ . Is there a way in which I could build a field of 1-forms over $\mathcal{S}$ by equivariance? My end goal would to assume $B$ has a Kähler structure $\omega$ and to use this to endow $\mathcal{S}$ some formal Kählerian structure (I'd need to assume $X$ compact with a prescribed volume form) and to make sense of candidate Kähler form on $\mathcal{S}$ via
something like $$
\Omega(\dot{\Phi}_1,\dot{\Phi}_2) \sim \int_X \omega(\dot{\phi}_1,\dot{\phi}_2)
$$ by either identifying tangent elements $\dot{\Phi}$ in terms of $B$ -valued maps or by defining a fiberwise Kähler-form $\omega$ on each fiber of the associated bundle $\mathcal{B}$ .","['principal-bundles', 'gauge-theory', 'differential-geometry']"
4382077,Subsets which overlap in one element,"consider the set $\{1,...,n\}$ , we want to decompose it into sets $S_1,....,S_t$ such that $\vert S_i\vert \geq  k$ for all i and $\vert S_i \cap S_j \vert \leq 1$ for all $i\neq j$ . Is there an upper bound on $t$ ? Obviously depending on n and k. Clearly for $k=1$ we get no bound. And for $k=n$ we have $t=1$ . This problem comes from Lemma 2.1 in ""On the lattice property of the plane and some problems of Dirac, Motzkin and Erdős in combinatorial geometry"" by Jozsef Beck, 1983. There for $\sqrt{2n}<k \leq n$ it is claimed that $t < \frac{2n}{k}$ , but I dont see it. The proof seems to use that $t \leq \frac{2n}{k}$ holds, if I assume that this is correct I can follow that claim. Thanks in advance! Edit:
My idee is to show that $2n \geq \sum_{i=1}^t \vert S_i\vert \geq t \cdot k$ . The right inequality is clear, but I miss an argument for the left one.","['elementary-set-theory', 'combinatorial-geometry', 'combinatorics']"
4382085,What's the measure of the gray area in the figure below??,"In the figure the triangle $DCF$ is joined to the square $ABCD$ of side $2$ , $M$ is middle of $CD$ . So the gray area is: My progress: $S_{AMB}=\frac{4}2=2\implies S_{MCB}=S_{ADM}=1$ $\triangle CMF = \cong \triangle BMC\\ \therefore S_{CMF}=1$ $\frac{S_{MDG}}{S_{DFC}}=\frac{DG\cdot DM}{DF\cdot CF}=\frac{DG}{\sqrt2}$ $S_{DGM} = 1-S_{FGM}$ From Stweart's theorem; $FM =\sqrt5$ Using law of cosines in $\triangle ADF$ , $\angle ADG = 135^\circ\implies DF = 2\sqrt2$ ...?","['euclidean-geometry', 'area', 'geometry', 'plane-geometry']"
4382170,"Use of the phrase ""tangent vector of a curve""","Let us understand a curve as a differentiable map $f : J \to \mathbb R^n$ defined on an open interval $J \subset \mathbb R$ . The derivative $f'(t_0)$ of $f$ at $t_0 \in J$ is given as the vector $(f'_1(t_0),\ldots,f'_n(t_0)) \in \mathbb R^n$ where the $f_i : J \to \mathbb R$ are the coordinate functions of $f$ . I think for $n > 1$ it is usual to say that $f'(t_0)$ is the tangent vector to the curve $f$ at the point $t_0$ or  the velocity vector of the curve $f$ at the point $t_0$ . For $n = 1$ one can find the wording velocity vector , but I have never seen that $f'(t_0)$ is called the tangent vector of $f  :J \to \mathbb R$ at $t_0$ . The definition of the derivative $f'(t_0)$ as the limit $\lim_{t \to t_0}\dfrac{f(t)-f(t_0)}{t-t_0}$ is nevertheless motivated by the concept of tangent by saying that $f'(t_0)$ is the slope of the tangent of the graph $G(f) = \{(t,f(t)) \mid t \in J \} \subset \mathbb R^2$ at the point $(t_0,f(t_0))$ . There is also a notational relation to the case $n > 1$ : If we consider the curve $\bar f : J \to \mathbb R^2, \bar f(t) = (t,f(t))$ , then we get $f'(t_0)$ as the second coordinate of the tangent vector $\bar f'(t_0) \in \mathbb R^2$ . Finally, if we consider a smooth ( $C^\infty$ ) curve $f$ and a point $t_0 \in J$ such that $f'(t_0) \ne 0$ , then $M  = f(J)$ is locally around $p_0 = f(t_0)$ a smooth one-dimensional submanifold of $\mathbb R^n$ . It has a tangent space $T_{p_0}M$ at $p_0$ which we may regard as a one-dimensional linear subspace of $\mathbb R^n$ and all $v \in T_{p_0}M$ are called tangent vectors at $M$ at $p_0$ . This suggests that all scalar multiples of the tangent vector $f'(t_0)$ can also be regarded as tangent vectors (which appears reasonable to me). I find this notationally confusing. The word ""tangent vector"" seems to have various different interpretations, but in the most elementary case $n = 1$ it is not used. Would it be better to avoid using the name ""tangent vector"" for $f'(t_0)$ , but to use the unambiguous ""velocity vector""? Perhaps somebody can help me to clarify my disorientation.","['notation', 'multivariable-calculus', 'calculus', 'differential-geometry']"
4382192,Can a martingale always be written as the integral with regard to Brownian motion?,"Let $(M_t)_{t \in \mathbb R^+}$ be a continuous martingale with regard to a filtration $(\mathcal F_t)$ generated by a continuous stochastic process $(Y_t)$ . Is it true that there exists a Brownian motion $(B_t)$ adapted to $(\mathcal F_t)$ and a stochastic process $(X_t)$ such that $M_t=M_0+\int_0^t X_s d B_s$ ? This is nearly the martingale representation theorem: Let $(B_t)$ be a standard Brownian motion defined on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and $\left\{\mathcal{F}_{t}\right\}_{t \geq 0}$ be its natural filtration.
Then, every $\left\{\mathcal{F}_{t}\right\}$ -local martingale $M$ can be written as \begin{equation*}
M_t=M_{0}+\int_0^t \xi_s d B_s
\end{equation*} for a predictable, $B$ -integrable, process $\xi$ (statement taken from: https://almostsuremath.com/2010/05/25/the-martingale-representation-theorem/ ). In my statement, $(\mathcal F_t)$ is not necessarily generated by Brownian motion. Does it still work?","['stochastic-analysis', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4382203,Why is gradient a vector?,"I've just moved on from single variable to multivariable calculus and having trouble understanding the gradient as I'm trying to draw a comparison b/w single and multivariable derivatives. Why is a gradient a vector?
So, let's take this with an example that we have a scalar function with 2 inputs i.e $z= F(x,y) = x^2 - y^2$ .
Now, I do understand that gradient is a vector of the partial derivative of $z$ (or $F(x,y)$ ). However, why do we need to write it as a vector? If we write it as a vector we are also assigning it a direction. And to me, this direction seems not intuitive as partial derivatives ( $\partial_x z, \partial_y z$ ) tell us the change in $z$ w.r.t to $x$ or $y$ ( $2x$ & $-2y$ in the example above). So, if we plug in some values of $x$ and $y$ (let's say 2 and 3) then we end up with 2 scalar values which would tell us the change in $z$ . So, in our example, that would be 4(w.r.t x) and -6 (w.r.t $y$ ). Using this we can get a net change in $z$ . And to take this forward, these scalar values could be multiplied with any other unit vector to get the directional derivative (or a net change in $z$ ). Just not sure why gradient is a vector with direction? Also, the proof (dot product yields maximum value when 2 vectors point the same direction) for gradient being the steepest descent/ascent seems to be dependent on this fact (gradient as a vector). The source I'm using is Khan Academy's Multivariable calculus.","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
4382260,Is the function $f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}}$ a bump-function $\in C_c^\infty$? Diff. Eq.?,"Is the function $f(x)=\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}e^{-\frac{x^2}{1-x^2}}$ a bump-function $\in C_c^\infty$ ? Which autonomous differential equation it fulfill? (note it is not defined piece-wise) I was viewing this question on MSE and there was given here a pretty clever answer $q(x) = 1-\sqrt{x^2}+\sqrt{\left(1-\sqrt{x^2}\right)^2}$ for a compact-supported function that is not defined piece-wise, and I was trying to us it method to built a proper bump function in $C_c^\infty$ following the definition of Wikipedia (a compact-supported smooth function ) that is also not defined piece-wise. After playing a while with it on Wolfram-Alpha and Desmos , I believe that: $$f(x)=\textstyle{\frac{\left(1-x^2+\sqrt{\left(1-x^2\right)^2}\right)}{2}}{\Large e}{\begin{array}\displaystyle{ -\frac{x^2}{1-x^2}} \\ \\ \\ \end{array}}$$ it is actually a bump function $\in C_c^\infty$ since their derivatives will be polynomials multiplied again by $f(x)$ , which will keep their functions smooth and keeping smoothness at the boundaries of their non-zero values $\partial x = [-1,\,1]$ , but my math skills aren´t enough to prove it, so far, I was able only to check the first 6th derivatives and it looks fine when plotted, without having discontinuities (I am worried of the hidden $\text{abs}(1-x^2)$ function which derivatives becomes Dirac's Delta functions $\delta(x \pm 1)$ ), neither changing their starting and ending values from zero ( example ). Since the function $f(x)=0,\,|x|>1$ as intended, I think it is already a smooth function on these points so it stills being a bump-function $\in C_c^\infty$ (if I am right, the only possible analytic function of compact-support is the zero function, so I believe that piecewise zero sections are smooth given they are analytic), and within $|x|<1$ it still being smooth since the term $e^{-\frac{x^2}{1-x^2}}$ will dominate any polynomial, but in the points $x=\{-1,\,1\}$ I don't know if will dominate any derivative of the Dirac's delta function. Also I would like to know which autonomous differential equation $f'(x) = G(f(x))$ with $G(x)$ at least $C^1$ almost-everywhere will have $f(x)$ as solution, maybe a non linear ODE, or a Delay Differential Equation (DDE) as it was shown here for another bump-function (so far is the only example I know): there is found that a bump-function which fulfill $\varphi'(t)=2\left(\varphi(2t+1)-\varphi(2t-1)\right)$ when the solution is only non-zero inside $[-1;\,1]$ , but unfortunately there is no closed-form for this function, but its Fourier Transform is show to be $\hat{\varphi}(\omega) = \prod\limits_{k=0}^{\infty}\frac{\sin\left(\frac{\pi\omega}{2^k}\right)}{\frac{\pi\omega}{2^k}}$ .... actually my main objective is to find a finite-duration solution to a differential equation that fulfill what is said in this paper Finite Time Differential Equations ... I have the Motivation for this questions explained in this another question . So summarizing: Is $f(x) \in C_c^\infty$ ? Which autonomous differential equation fulfill $f(x)$ as solution? Otherwise, proving it cannot be described through an autonomous diff. eq. is also welcome. My attempts so far... I believe that the diff. eq. found by Wolfram-Alpha is wrong: $$f'(x)\left(1-x^2\right)^2+f(x)(4x-2x^3)=0,\qquad f(0)=1$$ But I have had a lot of struggle dealing with the signs, so first, to see if I am not making mistakes, I will list a few things I am using for which I am not sure if they are formally right. First, to show explicitly where I have risks for discontinuities, I have made transparent that $\sqrt{(1-x^2)^2} \equiv |1-x^2|$ an absolute value function, so $f(x)$ acquire the form $f(x) = e^{-\frac{-x^2}{1-x^2}}\frac{(1-x^2+|1-x^2|)}{2}$ Second, for keeping tractable the sign issues and maintain unchanged the solutions of the diff. equation, I have found I have to keep $\frac{1}{\text{sgn}(a)} \neq \text{sgn}(a)$ , even when both plotted look the same (honestly for easy-hand-calculation I was using that a lot before). When working with the function $s(x) = \log\left(1-x^2+|1-x^2|\right)$ , I will have that $s'(x) = -\frac{2x}{|1-x^2|}$ , but when integrating $-\frac{2x}{|1-x^2|}$ on Wolfram-Alpha it shows a completely different thing, and even so, when derivating the W-A result it didn´t shows to be $s'(x)$ (even the results has Dirac's delta functions involved). So hereinafter, I will be using $ \int -\frac{2x}{|1-x^2|} dx = \log\left(1-x^2+|1-x^2|\right) + \mathcal{c}$ Also, noting that $1-x^2+|1-x^2| \equiv |1-x^2|\left(1+\text{sgn}(1-x^2)\right)$ In all the manipulations I am ignoring possible problems because of dividing by $0$ at the points $x=\{-1,\,1\}$ (they will arise on the topic later). Using these things, I believe that ""a true"" diff. equation for $f(x)$ (but not-autonomous ) is: $$f'(x)\left(1-x^2\right)^2+2x(1+|1-x^2|)f(x) = 0\qquad \text{Eq. 1}$$ From here, this is equivalent to: $$\frac{f'(x)}{f(x)}=\frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2}\qquad \text{Eq. 2}$$ Since $f'(x)/f(x) = \frac{d}{dx}\left(\log(f(x))\right)$ , I tried to integrate the left-hand-side of Eq. 2 on Wolfram-Alpha but it was unable to find and antiderivative, but noting that: $$\frac{-2x(1+|1-x^2|)}{\left(1-x^2\right)^2} = \frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x|1-x^2|}{\left(1-x^2\right)^2} =\frac{-2x}{\left(1-x^2\right)^2}+\frac{-2x}{|1-x^2|}$$ integrating both fractions and using point (3) I will have that: $$\Rightarrow \log(f(x)) = \frac{-x^2}{(1-x^2)}+\log(1-x^2+|1-x^2|)+\mathcal{c} $$ From where it can be seen that by applying both side the $\exp()$ function I will recover $f(x)$ for $c=-\log(2)$ , so I believe Eq. 1 is right. As @blamethelag reccomend on the answer I tried to find a recurrence relation for Eq. 2 using the General Leibniz formula : $$\left(fg\right)^{(n)} = \sum\limits_{k=0}^{n} {n \choose k}f^{(k)}g^{(n-k)} $$ But I wasn´t able to find and useful formula (I get stuck). But using this formula jointly with the Faà di Bruno's formula for a function composed with an exponential: $$\left(e^{g}\right)^{(n)} = e^{g}B_n(g',g'',\cdots, g^{(n)})$$ where $B_n()$ is the  nth complete exponential Bell polynomial . With these, I tried to expand the nth derivative of f(x): $$\begin{array}{r c l}
\frac{d^n}{dx^n} f(x) & = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} \frac{d^k}{dx^k}\left(e^{-\frac{x^2}{1-x^2}} \right)\frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n {n \choose k} e^{-\frac{x^2}{1-x^2}}B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right) \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \underbrace{{n \choose k} B_k\left(\frac{d}{dx}\left(\frac{-x^2}{1-x^2}\right),\frac{d^2}{dx^2}\left(\frac{-x^2}{1-x^2}\right),\cdots,\frac{d^k}{dx^k}\left(\frac{-x^2}{1-x^2}\right) \right)}_{\mathbb{P}_{n,k}(x)} \frac{d^m}{dx^m}\left(1-x^2+|1-x^2| \right)\\
& = & e^{-\frac{x^2}{1-x^2}} \sum\limits_{\begin{smallmatrix}k=0 \\m=n-k\end{smallmatrix}}^n \mathbb{P}_{n,k}(x) \left( \frac{d^m}{dx^m}\left(1\right)+\frac{d^m}{dx^m}\left(-x^2\right)+\frac{d^m}{dx^m}\left(|1-x^2|\right) \right) \qquad \text{Eq.3}
\end{array}$$ Here, I believe that the term $e^{\frac{-x^2}{1-x^2}}$ is going to ""dominate"" every possible polynomial $\mathbb{P}_{n,k}(x)$ since it already done it for the classical example of a bump function $\in C_c^\infty$ given by $r(x) = \begin{cases} e^{\frac{-x^2}{1-x^2}},\,|x|\leq 1\\ 0,\,\text{otherwise}\end{cases}$ But I am worried about the other part of the equation, since $\frac{d^m}{dx^m}\left(|1-x^2|\right)$ will make appear Dirac's Delta functions and derivatives of it (the following derivatives were solved by wolfram-alpha, by hand I have differences so maybe I am doing something wrong): $$\begin{array}{l c l}
m = 0 & \rightarrow & |1-x^2| \\
m = 1 & \rightarrow & -2x\,\text{sgn}(1-x^2)\\
m = 2 & \rightarrow & -2\,\text{sgn}(1-x^2)+4\delta(x+1)+4\delta(x-1) \\
m = 3 & \rightarrow & 4\delta(x+1)+4\delta(x-1)+4\delta'(x+1)+4\delta'(x-1)\\
m = p\geq 4 & \rightarrow & 4 \left(\frac{d^{p-3}}{dx^{p-3}}+\frac{d^{p-2}}{dx^{p-2}}\right)\Big(\delta(x+1)+\delta(x-1)\Big)\\
\end{array}$$ And I don´t know if these Dirac's delta functions and its derivatives are actually helping by ""killing"" things outside $x=\{-1,\,1\}$ , or instead are  ruining the smoothness of $r(x)$ on the points $x=\{-1,\,1\}$ , discarding the hypothesis of the smoothness of $f(x)$ (and it is not seen on the plots because happens only in two zero-measure points $x=\{-1,\,1\}$ ). This is why I worried about the smoothness, since for every other points I think is granted by $r(x)$ . At least for the case $m=3$ , where the terms $\delta()$ and $\delta'()$ , Wolfram-Alpha is able to take the limits $x \to 1$ giving zero value, and it match with their right and left side limits $x \to 1^{\pm}$ in Wolfram-Alpha here and here . By expanding the terms of Eq. 3 you will have things of the form: $$e^{-x^2/(1-x^2)}\mathbb{P}(x)\delta^{(m)}(x\pm 1)$$ where since $w(x)=e^{-x^2/(1-x^2)}$ will ""win"" any polynomial on $x \to \pm 1$ , I will have that $w(x)\mathbb{P}(x)=0,\,x\to\pm 1$ , so by calling $u(x \pm 1) =  w(x \pm 1)\mathbb{P}(x\pm 1)$ and $z = x \pm 1$ , the ""problematic"" terms will look like $u(z)\delta^{(m)}(z)$ with $u(0) = 0$ . Now, for $m=3$ , the terms with issues are of the form $u(z)\delta(z)$ and $u(z)\delta'(z)$ using the properties of the Dirac's delta function shown on the Spanish version page of Wikipedia : $f(x)\delta'(x)=-f'(x)\delta(x)$ $x^n\delta(x) = 0,\,\forall n>0,\,x\in\mathbb{R}$ $h(x)\delta(x-a)=h(a)\delta(x-a)$ $h(x)\delta'(x-a)=h(a)\delta'(x-a)-h'(a)\delta(x-a)$ I believe it could be seen that the terms will vanish since it will behave as terms of the form $\{g(z)\to 0\}\cdot \delta(z)$ : this because of the property $z^n\delta(z)=0$ , and in this case $g(z)\to 0$ even faster than every possible $z^n$ , and if some terms $g(z)\delta(z)\delta(z)\cdots\delta(z)$ arises, they will also become zero since every left-side multiplication will become the term zero for the remaining terms. Unfortunately, I don´t know how to extend this for $m \geq 4$ since I don't know how to work with higher derivatives of the Dirac's delta function, which at least on Wikipedia are reviewed through Distribution Theory scope, for which I am completely ignorant. But, if the first property of Wikipedia is right, I believe that every product by the derivatives of the Dirac's delta function could be manipulated into standard  Dirac's delta functions, as example: $$\begin{array}{r c l}
u(x)\delta'(x) & = & -u'(x)\delta(x) \iff \delta'(x) = -\frac{u'(x)}{u(x)}\delta(x) \\
\Rightarrow u(x)\delta''(x) & = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\delta(x)\right)  \\
& = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)-u'(x)\delta'(x) \\
& = & u(x)\frac{d}{dx}\left(-\frac{u'(x)}{u(x)}\right)\delta(x)+u'(x)\frac{u'(x)}{u(x)}\delta(x)
\end{array}$$ So noting that $\frac{u^{(m)}(x)}{u(x)}\equiv \mathbb{P}(x)$ some polynomial on the variable $x$ , all the terms of these derivatives will be of the form $w(x)\mathbb{P(x)}\delta(x)$ for other polynomials $\mathbb{P}(x)$ (I am abusing of the notation), and the same procedure could be extended by construction to higher derivatives of $u(x)\delta^{(m)}(x)$ , so If this is right, the function $f(x)$ is keeping is smoothness on the points $x =\{-1,\,1\}$ , meaning $f(x)\in C_c^\infty$ inherited by the function $\exp\left(\frac{-x^2}{1-x^2}\right)$ . But since I cannot formally proving it better as I already explained (which is more an intuitive prove than a proper one - there are too many ""I believe""), I hope someone could confirm this through some theorem or valid method. 2nd thing added If the triangular function is defined as: $$\Lambda(x) = \frac{1}{2}\left(|1+x|+|1-x|-2|x| \right)$$ It looks like for bump functions $b(x)$ defined piecewise in $x \in [-1,\,1]$ , their domain could be extended by using $g(x) = b(x)\Lambda(x^n)$ with integer $n \geq 2$ . Above I have used $\Lambda(x^2) = 1-x^2+|1-x^2|$ . Also positive powers of $\Lambda^m(x^n), m\in\mathbb{Z}^+$ will work. 3rd added later - discussion about the definition of f(t) Due to enriching comments,answers, and explanations by chat, I have a better idea of what the issues are with the proposed function. Since I would like to focus the answers with the problem with the derivatives of the Dirac's Delta function $\delta^{(m)}()$ , in this section I do a brief review of the problem with the definition of the function on the points $x=\{-1,\,1\}$ . From what I have seen here , commonly bump functions are defines in open intervals as the example: $$q(x) = \begin{cases} e^{-\frac{x^2}{1-x^2}}, & |x|<1 \\ 0, & |x| \geq 1 \end{cases}$$ where in the points of the ""edges"" $\partial x = \{-1,\,1\}$ , since the function is matching the zero constant, to keep smoothness it also has to happen that $\lim\limits_{x \to \partial x^{\pm}} \frac{d^n}{dx^n}q(x) = 0,\,\forall n \leq 0,\,n \in \mathbb{z}$ , in other words, all its right and left side derivatives at the edges must match and been equal to $0$ . So far so good, but, seeing the Wikipedia page for Compact support it is said that for a function defined in an open interval $(-1,\,1)$ , its support it still being $[-1,\,1]$ , so I think is like being ""cheating"": the function domain have two points where is undefined by definition, so is discontinuous there (I think), instead of being defined on $[-1,\,1]$ and having the issue of being undefined because of dividing by zero on the exponent. However, about this forbidden division (which is mentioned by @blamethelag), there is an issue: following Wolfram-Alpha , the limit of $q(x)$ at the edges does not exist, and neither are equal their left and right side limits (solved because of the open interval definition I believe), but the same analysis for the function $f(x)$ shows that actually it is not only having identical right and left hand sides limits (which is the standard way of extending a function), following Wolfram-Alpha the limits at exactly the edges exists and are also zero, so If W-A is right, actually the function $f(x)$ is fulfilling the continuity definition $\lim\limits_{x \to c} f(x) = f(c)$ , so if this is right, the function $f(t)$ should be properly defined as a function. But since it must also fulfill the existence of all its derivatives to be a smooth function, here is where the derivatives of the Dirac's Delta function $\delta^{(m)}()$ could be doing a mess, and its where I am worried about. Also another discussion was an opinion received in other question here : @CalvinKhor has correctly noted that the function $f(x)$ is still defined piecewise since the absolute value function by definition is defined piecewise. There is no way to refute this, but I believe that in the spirit of the question it is still a function not defined piecewise because of the following: as @blamethelag noted, if I work with the piecewise section within $[-1,\,1]$ , the differential equation for the function $f(x)$ will be defined by: $$\frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2}$$ , which solution will be behaving very different from being zero outside the interval $[-1,\,1]$ . But differently from it, using the definition of $f(x)$ presented here I was able to found the differential equation: $$\frac{f'(x)}{f(x)}=\frac{-2x}{(1-x^2)^2}+\frac{-2x}{|1-x^2|}$$ which, If I am right , it will be describing a function that actually behaves as the zero function outside the interval $[-1,\,1]$ . Note aside If I have a function $y(x) = e^{p(x)}(1-x^2+|1-x^2|)$ , using point (3) its trivial differential equation will be given by: $$\frac{y'(x)}{y(x)} = \frac{d}{dx}p(x)-\frac{2x}{|1-x^2|}$$ So I was trying to found some $p(x)$ that fulfills: $$y(x)\left(\frac{d}{dx}p(x)-\frac{2x}{|1-x^2|}\right) = 2y(2x+1)-2y(2x-1)$$ for matching the solution with an already known result, this unsuccessfully, but maybe someone else have an idea of how to made the matching... Or maybe found some $p(x)$ so the differential equation take an autonomous form $f'(x) = G(f(x))$ with $G(x)$ at least $C^1$ almost-everywhere . English language is not native for me, so probably this have a lot of mistakes: my apologies in advance","['special-functions', 'ordinary-differential-equations', 'smooth-functions', 'solution-verification', 'finite-duration']"
4382311,Why is Leibniz Notation written this way for the second derivative? [duplicate],"This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Explaining intuitively the notation for derivatives [duplicate] (2 answers) Closed 2 years ago . Let $y = f(x)$ . $$f'' = \frac{d^2y}{dx^2}$$ The explanation for this being that $$ \Bigl(\dfrac{d}{dx}\Bigr)^2 y = \dfrac{d^2}{dx^2}\,y = \dfrac{d^2 y}{dx^2};$$ Since there are two $d$ 's in the bottom of the fraction, why is it not written $$\frac{d^2y}{d^2x^2}$$ Maybe it's because $dx$ needs to be thought of as a single thing.  But notice that $d$ is used by itself and squared in the numerator.. Does my point here make sense, is it just a convenience to avoid the extra exponent, or is there a logical reason it's written in the form it is?","['notation', 'calculus', 'derivatives', 'differential']"
4382348,Relation between $\int_0^{\frac{\pi}4}(1+\tan x)^2dx$ and $\int_0^1\frac1{(1+x)^2(1+x^2)}dx$.,"Let $$ \displaystyle I_1=\int_0^{\frac{\pi}{4}}(1+\tan x)^2dx, \>\>\>\>\>
\displaystyle I_2=\int_0^{1}\frac{1}{(1+x)^2(1+x^2)}dx$$ then find $\displaystyle\frac{I_1}{I_2}$ My method- I was able to solve $I_1$ using standard formulae and got $I_1=1+\ln2$ . Similarly, I solved for $I_2$ using partial fraction decomposition and got $I_2=0.25 (1+\ln 2)$ . Therefore, the required ratio is $4$ . Is there some other way to solve this question?","['integration', 'definite-integrals']"
4382356,Multiplying cosets is well-defined,"Let $(G, *)$ be a group and $(H, *)$ its normal subgroup. Then the factor group is defined as $$ G/H := \left\{g*H: g\in G \right\} .$$ We define the multiplication of cosets (elements of G/H) as a function $$ f: G/H \times G/H \rightarrow G/H  $$ given by $ \left(g_1 H, g_2 H \right) \mapsto (g_1 * g_2)*H $ . Usually, this is shown to be well-defined function by using the ""definition"": A map $f: A\rightarrow B$ is well-defined if for every $a, b \in A$ $a=b$ implies $f(a)=f(b)$ . However, this ""definition"" does not make sense to me: If $a=b$ , doesn't $ f(a)=f(b)$ always follow, since I can always rewrite $a$ as $b$ ? That is why I am hoping to find a more ""explicitly"" defined function which defines the same multiplication of cosets. To do this, I was trying to find a function $\tilde{f}: G/H \rightarrow  G$ which maps every $g*H \in G/H$ satisfying $\hat{g}*H=g*H $ to $\hat{g}$ . After that, I could define another function $ \overline{f}: G \rightarrow G/H $ given by $g \mapsto g*H$ . Then I redefine the function $f$ as $ (x, y) \mapsto \overline{f}(\tilde{f}(x)*\tilde{f}(y)) $ , which is clearly well-defined. Now the problem is, can such function $\tilde{f} $ be found?","['functions', 'quotient-group', 'group-theory', 'abstract-algebra']"
4382364,Estimate of probability when the sum of iid normal random variables goes below 0,"This is a homework problem from my probability theory course. Suppose that $X_i$ are iid normal random variables with mean $\mu > 0$ and variance $\sigma^2 > 0$ . Given that $$
S_n = S_0 + \sum_{k=1}^{n} X_k\quad (S_0 \geqslant 0),
$$ one is asked to prove $$
\mathbb{P}(S_n \leqslant 0) \leqslant \exp\left(-\frac{2\mu S_0}{\sigma^2}\right).
$$ Below is my attempt: Let $T = \inf\{n : S_n\leqslant 0\}$ and the task becomes to estimate $\mathbb{P}(T < \infty)$ . What then occurs to me is to construct a martingale out of $S_n$ . I know that for nonnegative iid random variables $X_i$ , $$
M_n = M_0 \cdot\prod_{i=1}^n X_i\quad (\mathbb EX_i = 1)
$$ is a martingale. Then I let $$
Y_k = \exp\left(X_k - \mu - \frac{\sigma^2}{2}\right)
$$ such that $Y_k$ is nonnegative and $\mathbb{E}Y_k = 1$ . Further, $$
M_n := e^{S_0}\cdot\prod_{k=1}^n Y_k = \frac{\exp(S_n)}{\exp\left(n\mu + \frac{n\sigma^2}{2}\right)}
$$ is a martingale and $S_n \leqslant 0 \iff M_n \leqslant \exp\left(n\mu + \frac{n\sigma^2}{2}\right) $ . And it is at this step that I got stuck. Could anyone please give a hint about how to proceed? Thanks in advance to any of your help! :)","['martingales', 'probability-theory']"
4382380,"Formal derivative, any problems for non-commutative rings?","Let $R$ be a ring and $R\left[ x \right]$ be the ring of polynomials over $R$ . If $f=a_nx^n+\cdots+a_0 \in R\left[ x \right]$ , we define the formal derivative $f^{'}=na_nx^{n-1}+\cdots+a_1$ . Let $f$ , $g$ $\in R\left[ x \right]$ and $c \in R$ . Using the definition, it can be verified that: $(f+g)^{'}=f^{'}+g^{'}$ $(cf)^{'}=cf^{'}$ $(f+g)^{'}=f^{'}+g^{'}$ However, I came across this passage on Wikipedia : There is a problem with this definition for noncommutative rings. The
formula itself is correct, but there is no standard form of a
polynomial. Therefore using this definition it is difficult to prove
that $(f(x)\cdot b)^{'}=f^{'}(x)\cdot b$ . This makes me confused. I don't know what it is talking about. I don't see any problems here. The proofs of 1,2,3 for commutative rings and noncommutative rings are all the same. For example, I can easily prove that $(f\cdot b)^{'}=f^{'}\cdot b$ : Let $f_k$ denote the k-th coefficient of the polynomial $f$ , $k=0,1,2\ldots$ . By definition we know $(f^{'})_k=(k+1)f_{k+1}$ . So $(LHS)_k=(k+1)(f\cdot b)_{k+1}=(k+1)(f_{k+1}\cdot b)=((k+1)f_{k+1})\cdot b=(f^{'})_k\cdot b=(f^{'}\cdot b)_k=(RHS)_k$ . QED. Where did I go wrong? What does the paragraph on Wikipedia mean? Any insights are much appreciated.","['ring-theory', 'abstract-algebra', 'polynomials', 'noncommutative-algebra', 'derivatives']"
4382385,A better way to find the radius of the complex roots of $(z + 1)^5 = 32z^5$?,"I was gnawing on this problem today: All the complex roots of $(z + 1)^5 = 32z^5,$ when plotted in the complex plane, lie on a circle. Find the radius of this circle. I solved this by first dividing $$
\left(\frac{z+1}{2z}\right)^5 = 1
$$ then using the roots of unity, and solving for complex $z$ . Then, I did regression to determine the solution equation to be $$
\left(x - \frac{1}{3}\right)^2 + y^2 = \left(\frac{2}{3}\right)^2
$$ Thus, the radius is $2/3$ .
Gross. That is (in my opinion) an absolutely awful way to solve this, and not the intended way. I know that there exists a better, non-numerical solution to this problem; Could you please help me find it?","['algebra-precalculus', 'circles', 'complex-numbers', 'quintics']"
4382387,$(n^1+n)/2$ sequence,"This is going to be hard to explain so I'll just give an example Let's say we have a standard arithmetic sequence that goes up by 1 each time 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 The last number (in this case, 10) is n Scenario z: Pick 2 numbers in the sequence and multiply them. The product should be equal to the rest of the numbers added together. For example, 6 and 7 (let's call 6 and 7 x and y for now) are between 1 and 10 and if multiplied together, we get 42. Add 1+2+3+4+5+8+9+10 (don't count the numbers that were multiplied together) and we also get 42. n can only be an integer, no decimals involved here. x and y has to be an integer between 0 and and n , and x cannot be equal to y The question is, is there a mathematical way for figuring out the different numbers that can represent n that satisfies scenario z? And if n does satisfy the given scenario, is there a way to figure out x and y without having to try every possible combination? $n^2+1$ numbers seems to always work (10, 17, 26, 37 etc), and I have no clue why. I know I explained this pretty badly, feel free to ask any questions regarding this.",['sequences-and-series']
4382423,Does a singular value decomposition always exist for complex matrix?,"I know for real matrix A, SVD always exists, but I am wondering for any complex matrix, will SVD still exist for any scenarios? Thanks.","['matrix-decomposition', 'svd', 'matrices', 'linear-algebra', 'singular-values']"
4382436,"$4$-choosability of $K_{10,10}$","A graph is $k$ -choosable if no matter how one assigns a list of $k$ colors to each vertex, the vertices in the graph can be coloured in a way that each vertex receives a colour from its list and any two adjacent vertices have different colours. One can prove that the complete bipartite graph $K_{10,10}$ is not $3$ -choosable by considering the scenario when the vertices on the left part receive all $\binom{5}{3} = 10$ different subsets of $\{1,2,3,4,5\}$ and same for the vertices on the right. A natural question: is $K_{10,10}$ $4$ -choosable? If yes, how can we prove it. (Some kind of greedy algorithm?) One can show that it is $5$ -choosable, as in Theorem 1.4.2 at https://yufeizhao.com/pm/probmethod_notes.pdf","['graph-theory', 'combinatorics', 'bipartite-graphs', 'coloring']"
4382451,how to prove $A+A'B+A'B'C+A'B'C'D=A+B+C+D$,"Prove the above relationship by using the Boolean definition. I tried $A+A'B=A+B$ , but end up with $A+B+A'B'(C+D)$ , how can I go next?","['boolean-algebra', 'discrete-mathematics', 'computer-science']"
4382493,Question about Anderson Localization and a specific theorem (RAGE),"I have a question with a theorem which appears in a text called “Invitation to Random Schrödinger Operators”, in unit 7. Theorem 7.7. Let $H$ be a selfadjoint operator on Hilbert Space, take $\psi \in H_{pp}$ and let $\Lambda_L$ denote a cube in $\mathbb Z^d$ centered at the origin with side length $2L+1$ . Then: (1) $$\lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \in \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = ||\psi||^2$$ and (2) $$\lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \notin \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = 0 $$ Proof: Since $e^{-itH}$ is unitary, we have for all $t$ $$||\psi||^2 =||e^{-itH} \psi||^2 = \sum_{x \in \Lambda_L } |e^{-itH} \psi(x)|^2 + \sum_{x \notin \Lambda_L} |e^{-itH}\psi(x)|^2 $$ Above we saw that the expression is valid for eigenfunctions $\psi$ . To prove it for other vectors in $H_{pp}$ we introduce the following notation: By $P_L$ we denote the projection onto $C\Lambda_L$ . Then equation (2) claims that: $$ ||P_L e^{-itH} \psi||^2 \to 0$$ uniformly in $t$ as $L \to \infty$ . If $\psi$ is a finite linear combination of eigenfunctions, say $\psi=\sum_{m=1}^M \alpha_k \psi_k$ , $H\psi_k=E_k \psi_k$ , then: (3) $$ ||P_L e^{-itH} \psi || = || \sum_{m=1}^M \alpha_k P_L e^{-itH} \psi_k || \leq \sum_{m=1}^M |\alpha_k| ||P_L e^{-itH} \psi_k || = \sum_{m=1}^M |\alpha_k| ||P_L e^{-itE_k} \psi_k|| = \sum_{m=1}^M |\alpha_k| ||P_L \psi_k||$$ From this moment, it is that I begin to have problems to understand By taking $L$ large enough, each term in the sum above can be made smaller then $(\sum_{m=1}^M |\alpha_k|)^{-1} \varepsilon$ . If now $\psi$ is an arbitrary element of $H_{pp}$ , there is a linear combination of eigenfunctions $\psi^{(M)} = \sum_{m=1}^M \alpha_k \psi_k$ such that $||\psi-\psi^{(M)}|| < \varepsilon $ $||P_L e^{-itH} \psi|| \leq ||P_L e^{-itH} \psi^{(M)} || + ||P_L e^{-itH} (\psi - \psi^{(M)}) || \leq ||P_L e^{-itH} \psi^{(M)}  || + || \psi - \psi^{(M)}||$ GUYS I can’t understand how $||P_L e^{-itH} (\psi - \psi^{(M)} ) || \leq || \psi - \psi^{(M)}||$ Thanks!","['ordinary-differential-equations', 'self-adjoint-operators', 'localization', 'partial-differential-equations', 'spectral-theory']"
4382501,"Does Nei's Standard Genetic ""Distance"" obey identity of indiscernibles?","In Nei 1972 he describes what is now called Nei's standard genetic distance . He defines at as follows: Consider two randomly mating diploid populations, $X$ and $Y$ , in which multiple alleles aggregrate at a locus. Let $x_i$ and $y_i$ be the frequencies of the $i$ th alleles in $X$ and $Y$ , respectively. The probability of identity of two randomly chosen genes is $j_X = \sum x_i^2$ in population $X$ , while it is $j_Y = \sum y_i^2$ in population $Y$ . The probability of identity of a gene from $X$ and a gene from $Y$ is $j_{XY} = \sum x_iy_i$ . [...] The normalized identity of genes between $X$ and $Y$ with respect to this locus is defined as $$I_j = \frac{j_{XY}}{\sqrt{j_Xj_Y}}.$$ [...] The normalized identity of genes between $X$ and $Y$ with respect to all loci is defined as $$I = \frac{J_{XY}}{\sqrt{J_XJ_Y}},$$ where $J_X$ , $J_Y$ and $J_{XY}$ are the arithmetic means of $j_X$ , $j_Y$ , and $j_{XY}$ , respectively, over all loci, including monomorphic loci. [...] The genetic distance between $X$ and $Y$ is then defined as $$D = - \log_e I.$$ He states in this work that his function does not have the triangle inequality. In the present measure of genetic distance, the triangle inequality, which is often assumed in numerical taxonomy (see, e.g., Levandowsky and Winter 1971), does not hold. Nei also said, "" Evolution does not occur so as to assure this property at least at the nucleotide level. "" in reference to the triangle inequality. Not having the triangle inequality disqualifies Nei's score from being a metric . I am curious if any of the other properties of a metric fail to hold. Due to the product of probabilities and norms always being non-negative, and the negative log of a number on $[0,1]$ , it seems clear that it has the non-negativity property. The symmetry property also holds because of the commutativity of scalar multiplication. It is clear that $D[X,X] = 0$ , but does the stronger identity of indiscernibles hold? While identity of indiscernibles has a broader meaning, in this context of a metric I more narrowly mean $D[X,Y] = 0 \iff X = Y$ .","['symmetric-functions', 'triangle-inequality', 'functions', 'metric-spaces']"
4382558,de Rham theorem for tempered distribution,"I am wondering if the following statement holds. If $u\in \mathscr{S}'$ and $u=\nabla p$ for some $p\in \mathscr{D}'$ , then $p$ is in $\mathscr{S}'$ . Here $\mathscr{S}'$ is the space of tempered distributions and $\mathscr{D}'$ is the space of distributions. I raised this question when I want to show the following claim: If $u\in \mathscr{S}'$ satisfies $\left< u,\Phi\right>=0$ for all $\Phi \in \mathscr{S}$ with $\mathrm{div}\, \Phi=0$ , then there exists $p\in \mathscr{S}'$ such that $u=\nabla p$ in $\mathscr{S}'$ . Another guess for my question is based on the generalized Lions' lemma: If $f\in \mathscr{D}'$ and $\nabla f \in H^{-1}$ , then $f\in L^2$ . Thank you for your time.","['harmonic-analysis', 'functional-analysis', 'distribution-theory']"
4382569,Find the length of the following parametric curve in R^3,"$$\gamma(t)=(\log t\sqrt2,\frac{1}{3t},3t+1)$$ $$t\in [1/3,3]$$ my try : the tangent to the curve is : $$\gamma'(t)=\left(\frac{\sqrt2}{t},\frac{-1}{3t^2},3\right)$$ the norm of the tangent to the curve: $$||\gamma(t)||=\left(\frac{2}{t^2}+\frac{1}{9t^4}+9\right)^\frac{1}{2}$$ expanding and simplifying : $$||\gamma(t)||=\frac{((12t^2+5)(12t^2+1))^\frac{1}{2}}{6t^2}$$ I don't know how to proceed to solve the integral from this","['curves', 'multivariable-calculus']"
4382584,Prove the discrete spectrum of $A$ equals to the set of those complex $\lambda$ such that $\lambda I-A$ is Fredholm.,"There is considerable divergence in the literature concerning the definition of the essential spectrum of a densely defined closed operator $A$ on a Banach space $X$ . I want to find a direct definition of the essential spectrum such that it is the complement of the discrete spectrum and is stable under compact perturbations. It seems to me that I have succeeded, but I still have one problem that I really what to know the proof. It seems that we always define the discrete spectrum $\sigma_{\text{d}}(A)$ of $A$ by $$\sigma_{\text{d}}(A):=\{\lambda\in\mathbb C: \lambda \text{ is an isolated eigenvalue of }\ A \text{ and has finite algebraic multiplicity}\}.\tag 1$$ Here the algebraic multiplicity of an isolated eigenvalue $\lambda$ is defined as the dimension of the range of the Riesz projection $P_\lambda$ . This is a relatively long post. I wrote almost all things that I have done relating to this topic. In the end of the post, I will write a self-contained statement of my problem. If you want to save time, you can skip the heavy body of the post. For a self-adjiont operator $A$ on a Hilbert space $X$ , we can define the essential spectrum $\sigma_{\text{ess}}(A)$ by $\sigma_{\text{ess}}(A):=\sigma(A)\setminus \sigma_{\text{d}}(A)$ .
Then we can use Weyl's criterion to prove that the essential spectrum of self-adjiont operators is invariant under symmetric compact perturbations. However, when it comes to general densely defined closed operators on Banach spaces, things become complicated. Kato defined in Chapter 4 of his book Perturbation of Linear Operators that $$\sigma_{\text{ess}}(A):=\sigma(A)\setminus \{\lambda\in \mathbb C: \lambda I-A \text{ is semi-Fredholm}\}.$$ Then Theorem 5.35 in Chapter 4 proves the stability of $\sigma_{\text{ess}}(A)$ under compact perturbations. But as Therorem 5.33 implies, the relation $\sigma_{\text{ess}}(A)=\sigma(A)\setminus \sigma_{\text{d}}(A)$ is not always right. I searched for some materials and finally find out that the definition given by F.Wolf should be satisfying. He wrote: The spectrum $\sigma(A)$ of an operator $A$ can be divided into two parts: The essential spectrum $\sigma_e(A)$ consisting of the points $\lambda$ at which $\Re(\lambda I-A)$ , the range of $\lambda I-A$ is not closed and of eigenvalues of infinite multipilicity. The second part may be called the Fredholm part of $\sigma(A)$ consists beside others of $\rho(A)$ , the resolvent set of $A$ and of isolated eigenvalues of finite multiplicity. But I also find that in another paper the author rewrote Wolf's definition of essential spectrum as the subset of $\sigma(A)$ consisting those $\lambda$ such that $\lambda I-A$ is not Fredholm. These two definitions seem different at the first glance, and I wonder why they are the same. My Problem. Let $A$ be a densely defined closed operator $A$ on a Banach space $X$ and define $\sigma_\text{d}(A)$ as in (1). Prove that $$\sigma_\text{d}(A)=\{\lambda\in\sigma(A): \lambda I-A \text{ is Fredholm}\}.$$ That is to say, $\sigma_\text{d}(A)$ consists of those complex $\lambda\in\sigma(A)$ for which $\Re(\lambda I-A)$ , the range of $\lambda I-A$ , is closed, $\text{dim}\ N(\lambda I-A)$ , the geometric multiplicity of $\lambda$ , is finite, $\text{dim}\ \left(X/\Re(\lambda I-A)\right)$ , the codimension of $\Re(\lambda I-A)$ , is finite. Any hints or useful references are welcome!","['spectral-theory', 'functional-analysis', 'unbounded-operators', 'reference-request']"
4382586,Evaluate $\int_{0}^{\infty} \frac{\ln x}{\left(x^{2}+1\right)^{n}} d x$.,"Latest Edit By the contributions of the writers, we finally get the closed form for the integral as: $$\boxed{\int_{0}^{\infty} \frac{\ln x}{(x^{2}+1)^n} d x =-\frac{\pi(2 n-3) ! !}{2^{n}(n-1) !} \sum_{j=1}^{n-1} \frac{1}{2j-1}}$$ I first evaluate $$I_1=\int_{0}^{\infty} \frac{\ln x}{x^{2}+1} d x \stackrel{x\mapsto\frac{1}{x}}{=}  -I_1 \Rightarrow I_1= 0.$$ and then start to raise up the power of the denominator $$I_n=\int_{0}^{\infty} \frac{\ln x}{(x^{2}+1)^n} d x .$$ In order to use differentiation, I introduce a more general integral $$I_n(a)=\int_{0}^{\infty} \frac{\ln x}{(x^{2}+a)^n} d x. $$ Now we can start with $I_1(a)$ . Using $I_1=0$ yields $$\displaystyle 	1_1(a)=\int_{0}^{\infty} \frac{\ln x}{x^{2}+a} d x \stackrel{x\mapsto\frac{x}{a}}{=} \frac{\pi \ln a}{4 \sqrt a} \tag*{}$$ Now we are going to deal with $I_n$ by differentiating it by $(n-1)$ times $$
\frac{d^{n-1}}{d a^{n-1}} \int_{0}^{\infty} \frac{\ln x}{x^{2}+a} d x=\frac{\pi}{4} \frac{d^{n-1}}{d a^{n-1}}\left(\frac{\ln a}{a}\right)
$$ $$
\int_{0}^{\infty} \ln x\left[\frac{\partial^{n-1}}{\partial a^{n-1}}\left(\frac{1}{x^{2}+a}\right)\right] d x=\frac{\pi}{4} \frac{d^{n-1}}{d a^{n-1}}\left(\frac{\ln a}{\sqrt a}\right)
$$ $$
\int_{0}^{\infty} \frac{\ln x}{\left(x^{2}+a\right)^{n}} d x=\frac{(-1)^{n-1} \pi}{4(n-1) !} \frac{d^{n-1}}{d a^{n-1}}\left(\frac{\ln a}{\sqrt{a}}\right)
$$ In particular, when $a=1$ , we get a formula for $$
\boxed{\int_{0}^{\infty} \frac{\ln x}{\left(x^{2}+1\right)^{n}} d x=\left.\frac{(-1)^{n-1} \pi}{4(n-1)!} \frac{d^{n-1}}{d a^{n-1}}\left(\frac{\ln a}{\sqrt{a}}\right)\right|_{a=1}}
$$ For example, $$
\int_{0}^{\infty} \frac{\ln x}{\left(x^{2}+1\right)^{5}} d x=\frac{\pi}{4 \cdot 4 !}(-22)=-\frac{11 \pi}{48}
$$ and $$
\int_{0}^{\infty} \frac{\ln x}{\left(x^{2}+1\right)^{10}} d x=\frac{-\pi}{4(9 !)}\left(\frac{71697105}{256}\right)=-\frac{1593269 \pi}{8257536}
$$ which is check by WA . MY question Though a formula for $I_n(a)$ was found, the last derivative is hard and tedious. Is there any formula for $$\frac{d^{n-1}}{d a^{n-1}}\left(\frac{\ln a}{\sqrt{a}}\right)? $$","['integration', 'improper-integrals', 'calculus', 'derivatives']"
4382588,Meromorphic function with $\oint z^n f(z) dz=0$ is holomorphic?,"Let $f$ be a meromorphic function on $\mathbb C$ with finitely many poles, located in $|z|<R/2$ for some large fixed $R$ . Furthermore, suppose that $$\oint_{|z|=R} z^nf(z) dz=0$$ for all $n\in \mathbb Z$ . What can we infer from this condition? I naively expect that $f$ is holomorphic. Is it true?",['complex-analysis']
4382686,elementary abelian 2-groups extension,$2^n$ denotes elementary abelian 2-groups of rank n. I am reading a paper which has $2^4.2^3 = 2^5.2^2$ Are these two extensions equal? Are there any facts I am not aware of? Because I checked an example: $Z_p$ extended by $Z_p$ x $Z_p$ can be nonabelian. So what if the left is elementary abelian and the right is not abelian.,['group-theory']
4382697,Metric spaces as schemes,"In Récoltes et Semailles, Grothendieck explains he invented the concept of scheme to unify algebraic varieties on algebraically closed fields, arithmetical varieties in characteristic $p$ , and also the usual metric spaces. This last part is not obvious to me: if we start with a metric space $(X,d)$ we do have a topological space $X$ , but we need a structure sheaf of rings to replace the metric distance $d$ (so that we can distinguish homeomorphic spaces with different metric structure, like a triangle and a circle). The sheaf of continuous functions $\mathcal{C}(X,\mathbb{R})$ seems a natural choice, because distances are $\mathbb{R}$ -valued, and because its stalks are local rings. Did Grothendieck have this sheaf in mind to represent metric spaces as schemes? Does this sheaf distinguish homeomorphic spaces with different metric structures?",['algebraic-geometry']
4382712,How to approach the problem of summation of Eisenstein series on shifted lattices?,"This question is an attempt to complete the issues discussed in a previous question of mine ( How did Gauss sum Eisenstein series? ), since my updated question did not recieve any attention. In my previous question, I mentioned that Gauss wrote an infinite series for the logarithm of the denominator of $\mathbb{sinlemn}(z) = \frac{M(z)}{N(z)}$ . Since $$N(z) = \prod (1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ (that is, $\mathbb{sinlemn}(z)$ has poles at Gaussian half-integers multiples of $\varpi$ ). Gauss wrote: $$\mathbb{log}N(z) =\frac{1}{12}z^4 - \frac{1}{280}z^8 +\frac{1}{4950}z^{12} - ...$$ . Since the logarithm of a infinite product equals an infinite sum of logarithms, one gets that: $$\mathbb{log}N(z) = \sum \mathbb{log}(1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ , and by the taylor series expansion of $\mathbb{log}(1-z)$ one gets that Gauss's infinite series for $\mathbb{log}N(z)$ is equivalent to the summation of a kind of ""generalized Eisenstein series"" in which the lattice is shifted by $(\frac{1}{2}+\frac{1}{2}i)\varpi$ . Since  such shifted lattice cannot be generated by some action of the modular group on the lattice of Gaussian integers (if it was, one could use the modularity of the Eisenstein series and deduce the series for $\mathbb{log}N(z)$ from that of $\mathbb{log}M(z)$ ), I wondered what tools enable to sum such series and what was Gauss's original method in this case. I tried to make a Google search about ""modular forms defined on shifted lattices"", but without success. Side remark I have no intention to ""spam"" StackExchange Mathematics with multitudes of more or less similar questions, so I have no problem to close this question and instead get an answer to my original (updated) question, if other users will vote to do so.","['complex-analysis', 'elliptic-functions', 'math-history']"
4382734,"Density of $\sin(\Bbb Z)$ in $[-1, 1]$ implies the same density for $\sin(\Bbb N)$","I have succeeded in proving that $\sin(\Bbb Z)$ is dense in $[-1, 1]$ , however I would also like to prove that this result implies that $\sin(\Bbb N)$ is also dense in $[-1,1]$ . So far, I have tried to proceed by contradiction, noticing first that $$\sin(\Bbb Z) =  \sin(\Bbb N) \cup \sin(-\Bbb N) = \sin(\Bbb N) \cup -\sin(\Bbb N);$$ then there is at least a point in $[-1,1]$ such that one of its neighbourhoods contains infinitely many points of $\sin(-\Bbb N)$ but zero points of $\sin(\Bbb N)$ . However I haven't been able to go further since I do not understand which property of sine must be used in order to complete the proof (if the proof is actually possible given only that hypothesis). Any help is highly appreciated! EDIT I would like to clarify the reasoning I have done so far: (1) I proved that any additive subgroup of $(\Bbb R, +)$ either has a positive least element or is dense in $\Bbb R$ ; (2) I proved that the set $\{ n+m\alpha: n,m \in \Bbb Z\}$ , where $\alpha$ is irrational, is dense in $\Bbb R$ by showing that it is an additive subgroup of $(\Bbb R, +)$ which doesn't have a least positive element; (3) now it is easy to prove that $\sin(\Bbb Z)$ is dense in $[-1,1]$ by using the periodicity (let $\alpha = 2\pi$ ), the surjectivity in $[-1,1]$ and the sequential continuity of the sine function. In light of recent comments, I think it would be easier to show that $ A = \{ n+m\alpha: n \in \Bbb N,m \in \Bbb Z\}$ is dense in $\Bbb R$ .","['number-theory', 'general-topology', 'trigonometry', 'real-analysis']"
4382740,An apparently simple but somehow unexpected inequality between integrals,"Let $f:[0,1]\to[0,1]$ be any Lebesgue integrable function. Then $$I_1= \left(\int_0^1f(x)\sqrt x dx\right)^2 \leq \frac{4}{3}\int_0^1f(x)x^2dx=I_2\,.\tag{$\star$}$$ At first sight, I would have expected the reverse inequality, as $x^2\leq \sqrt x$ on $[0,1]$ and I didn't think that taking the square of the LHS would have been enough to compensate. However, it is easy to see that if $f=\chi_{[0, b]}$ , for some $b\in(0, 1]$ , where $\chi_A$ is the characteristic function of the set $A$ , then $I_1=I_2 = \frac{4}{9} b^3$ . On the other hand, if we let $[a, b]\subseteq[0,1]$ , then taking $f=\chi_{[a, b]}$ we have $I_1 = \frac{4}{9}(b^{3/2}-a^{3/2})^2$ and $I_2 = \frac{4}{9}(b^{3}-a^{3})$ . It is not hard to prove (for instance using the sub-additivity of $\sqrt\cdot$ ) that $I_1\leq I_2$ . Then for $\epsilon\in(0,1]$ and $f=\epsilon\chi_{[a,b]}$ it is clear that the same result holds true, and actually $I_1/I_2$ will scale as $\epsilon$ , so it is maximised for $\epsilon=1$ . (Note that allowing $\epsilon>1$ the inequality would not be true, but we would not be in the hypothesis $f\subseteq [0,1]$ .) So, for all $f$ which is a simple function (non-zero and constant on finitely many intervals) we get that ( $\star$ ) is satisfied. Taking the limit of simple functions we can obtain any measurable function, and so conclude. [This is wrong! See edit.] Now, as the inequality looks quite simple, I would expect that there is a much simpler proof for it, for instance something making use of Hölder's or Jensen's inequality. I might have missed something trivial, but so far I didn't manage to do it, and I have the feeling that I am not really grasping what is going on. Notice that we can restate ( $\star$ ) as $E_1^2\leq E_2$ , where $$E_1 = \frac{3}{2}\int_0^1 f(x)\sqrt x dx$$ and $$E_2 = 3 \int_0^1 f(x) x^2 dx\,.$$ The reason for this transform is that then you can see $\frac{3}{2}\sqrt x$ and $3 x^2$ as probability densities on $[0,1]$ , and so rewrite everything in terms of expectations of $f$ . However, so far I couldn't find any smart trick for a few-lines proof of ( $\star$ ), without having to pass via a sequence of simple functions converging to $f$ . Edit. I've actually just realised that the proof I had provided does not work, as if $f=\sum_i f_i =  \chi_{(a_i, b_i)}$ , then we just have $$I_1 = \left(\sum_i\int_0^1 f_i(x)\sqrt xdx\right)^2 \leq 2\sum_i \left(\int_0^1 f_i(x)\sqrt xdx\right)\leq 2 I_2\,.$$ However the proof from @Sangchul Lee shows that the inequality $I_1\leq I_2$ holds.","['definite-integrals', 'real-analysis', 'integral-inequality', 'inequality', 'probability']"
4382743,Ellipse inscribed in an irregular quadrilateral,"I want to obtain the ellipse inscribed in the irregular quadrilateral (no parallel sides) defined by the four points A, B, C, D. I summarize the ideas given in the comments and answers: The is not an unique ellipse inside the given quadrilateral. For the unit square, there are infinite ellipses inscribed into it, with different eccentricities You cannot transform the unit square into a irregular quadrilateral using linear transformations, as those transform only two vectors into other two vectors. In this case we need to transform 4 vectors. As shown in this figure: Increasing the eccentricity, decreases the area. So the problem can be reduced to obtain the maximum area ellipse inscribed into the quadrilateral.","['conic-sections', 'linear-algebra', 'geometry']"
4382785,Finding and drawing equivalence class with a binary set defined on RxR,"In the question, it has been asked to find and draw the equivalence classes of the relation $∼$ on $(\Bbb R\times\Bbb R)\setminus\{(0, 0)\}$ which is defined as $$(x_1, x_2) \sim (y_1, y_2)~\text{if}~(y_1, y_2) = \alpha(x_1, x_2)\text{, for some }\alpha \neq 0$$ On solving, I have obtained the equivalence class as $$\begin{align}[(a,b)]&=\{(x,y)\in\Bbb R:(x,y)\sim(a,b)\}\\&
=\{(x,y)\in\Bbb R:(x,y)=\alpha(a,b)\}\end{align}$$ On solving this, I got $x=\alpha a$ and $y=\alpha b$ and $x-y=\alpha(a-b)$ which will correspond to the set of straight lines having slope $1$ and $y$ -intercept $-\alpha(a-b)$ excluding the straight line passing. But in the solution of this question, the equivalence class is given as the set of all straight lines passing through the origin(which is excluded). I have just begun discrete math, and I am not getting any idea of the solution. It would be really helpful if someone could help me with this.","['equivalence-relations', 'relations', 'discrete-mathematics']"
4382798,"Why is addition defined, and not implied, on quotient spaces?","Small question. In chapter 3, section E, page 96 of ""Linear Algebra Done Right"", addition in quotient vector spaces is defined this way: I understand why scalar multiplication has to be defined, because multiplying a subset of a vector space with a scalar was not defined. But why can addition of affine subsets be said to work this way, if the sum of subsets of a vector space was already defined? $v+U, w+U$ are both subsets of a vector space $V$ , so $(v+U)+(w+U)$ is the set containing all possible sums of elements of $v+U$ with elements of $w+U$ . Shouldn't the statement $(v+U)+(w+U)=(v+w)+U$ be a theorem?","['linear-algebra', 'vector-spaces', 'quotient-spaces']"
4382845,Why is this sum over graphs equal to the Möbius function of the poset of partitions?,"Let $\mathcal{G}(n)$ be the set of graphs on $n$ vertices.  Then there is the identity $$
\sum_{\substack{G\in\mathcal{G}(n)\\G\text{ connected}}} (-1)^{|G|} = (-1)^{n-1} (n-1)!,
$$ where the sum is over graphs that are connected.  This has been discussed for example in this question . I am curious about the fact that the right-hand side is also equal to the formula for the Möbius function $\mu_{\mathcal{P}(n)}(0,1)$ , where $\mathcal{P}(n)$ is the poset of partitions of an $n$ -element set, and $0$ and $1$ are the minimal and maximal partitions. I don't think this a total coincidence for the reason that there is an order-preserving mapping $f:\mathcal{G}(n)\to\mathcal{P}(n)$ which sends $G$ to the partition of $[n]$ into connected components of $G$ .  Here we make $\mathcal{G}(n)$ into a poset by saying $G\leq G'$ if the edge set of $G$ is a subset of the edge set of $G'$ .  Then $\mathcal{G}(n)$ is isomorphic to the boolean algebra on $\binom{n}{2}$ elements, with
Möbius function $\mu_{\mathcal{G}(n)}(H,G) = (-1)^{|G\setminus H|}$ . With this notation we can rewrite the identity above in the following suggestive way: $$
\sum_{H\in f^{-1}(0)} \sum_{G\in f^{-1}(1)} \mu_{\mathcal{G}(n)}(H,G)
= \mu_{\mathcal{P}(n)}(0,1).
$$ Now there is an identity due to Baclawski which states that if $f:P\to Q$ is an order-preserving mapping between posets then $$
\mu(Q) = \mu(P) + \sum_{y\in Q} \mu(f^{-1}(Q_{\leq y})) \mu(Q_{>y}).
$$ Here the Möbius number of a poset $P$ is defined by $\mu(P)=\mu_{\hat{P}}(\hat{0},\hat{1})$ , and $\hat{P} = P\cup\{\hat{0},\hat{1}\}$ is a new poset formed by adding a minimal element $\hat{0}$ and a maximal element $\hat{1}$ . I am curious whether there is some way to use this identity to prove this result for the signed sum of graphs.","['order-theory', 'combinatorics']"
4382864,Riemannian homogeneous aspherical iff flat torus,"We say that a connected manifold $ M $ is aspherical if $$
\pi_n(M) = 0
$$ for all $ n \geq 2 $ . Equip $ M $ with a metric $ g $ such that $ (M,g) $ is Riemannian homogeneous (i.e. the isometry group acts transitively). If $ M $ is a compact Riemannian homogeneous aspherical manifold must $ M $ be a flat torus? I believe the answer is yes. Here is the proof: A compact aspherical manifold (indeed any finite CW complex) has torsion free fundamental group. Since $ M $ is compact Riemannian homogeneous then by Transitive action by compact Lie group implies almost abelian fundamental group the commutator subgroup of the fundamental group must be finite. But $ \pi_1(M) $ is torsion free so any finite subgroup is trivial. Thus the commutator subgroup is trivial. In other words $ \pi_1(M) $ is abelian. Since $ M $ is compact $ \pi_1(M) $ is finitely generated. So $ \pi_1(M) $ is a finitely generated torsion free abelian group $$
\pi_1(M) \cong \mathbb{Z}^n
$$ Assuming that a compact Riemannian homogeneous $ K(\mathbb{Z}^n,1) $ must be a flat torus that completes the proof. But I'm not quite sure how to show that a compact Riemannian homogeneous $ K(\mathbb{Z}^n,1) $ must be a flat torus. What about the case where $ M $ is Riemannian homogenous aspherical but not compact? A Riemannian homogeneous manifold is an isometric product of a contractible piece with a Riemannian homogeneous compact piece. See https://mathoverflow.net/questions/410334/noncompact-riemannian-homogeneous-is-trivial-vector-bundle-over-compact-homogene So as long as the compact piece has dimension at least 2 then the above argument goes through and the compact piece is a flat torus so by homogeneity of the metric the whole thing is flat. But what about if the compact piece is only one dimensional?  I think the group $ H(3, \mathbb{R})/ \Gamma $ with its invariant metric (Nil geometry) is a counterexample where flatness is lost. Here $$H(3, \mathbb{R}) = \left\{\begin{bmatrix} 1 & x & z\\ 0 & 1 & y\\ 0 & 0 & 1\end{bmatrix} : x, y, z \in \mathbb{R}\right\}$$ is the three dimensional Heisenberg group, and $$\Gamma = \left\{\begin{bmatrix} 1 & 0 & c\\ 0 & 1 & 0\\ 0 & 0 & 1\end{bmatrix} :  c \in \mathbb{Z}\right\}$$ is a discrete central subgroup. Of course if there is no compact piece and $ M $ is contractible (topologically $ \mathbb{R}^n $ ) Riemannian homogeneous then there are a million different Riemannian homogeneous metrics that aren't flat. Take for example the hyperbolic metric of even the left invariant metric on any contractible Lie group (all simply connected non-abelian solvable Lie groups are good examples) This is mostly a proof verification question because this seems too general to be true but I think my proof checks out","['solution-verification', 'riemannian-geometry', 'differential-geometry']"
4382876,Can a Torus be a submanifold of a Sphere?,"If I describe a $2$ -torus in $4D$ , as the product of two independent circles $S^1\times S^1$ .
Can the resulting torus live on the $4D$ -embedded sphere $S^3$ ? I want to confirm points of my torus can all be placed at a constant distance from the origin.","['product-space', 'spheres', 'geometry', 'algebraic-topology']"
4382908,"For a non-negative integer $k$, $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{x^k}=l\implies \lim_{x\to \infty}\frac{f(x)}{x^{k+1}}=\frac l{k+1}$","Suppose that $f$ defined on $(a,\infty)$ is bounded on each finite interval $(a,b),a>b$ . For a non-negative integer $k$ , it is to be shown that $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{x^k}=l\implies \lim_{x\to \infty}\frac{f(x)}{x^{k+1}}=\frac l{k+1}$ Given an $\displaystyle \epsilon  >0,$ there exists an integer $\displaystyle N$ such that $$\displaystyle 
x \geq N \Longrightarrow \ \left| \frac{f( x+1) -f( x)}{x^{k}}
 -l \right| < \epsilon $$ Noting that \begin{align*}
f( x) & =\sum _{i=1}^{[ x]} f( x-[ x] +i) -f( x-[ x] +i-1) +\color{red}{f( x-[ x])}
\end{align*} it follows that \begin{align*}
f( x) & =\sum _{i=1}^{N}\overbrace{( f( x-[ x] +i) -f( x-[ x] +i-1))}^{h( i)} +\sum _{i=N+1}^{[ x]}( f( x-[ x] +i) -f( x-[ x] +i-1)) +\color{red}{( \ )}
\end{align*} It follows that \begin{align*}
|\frac{f( x)}{x^{k+1}} -\frac{l}{k+1} | & \leq \sum _{i=1}^{N} |\frac{h( i)}{x^{k+1}} -\frac{l}{[ x]( k+1)} |+\sum _{i=N+1}^{[ x]} |\frac{h( i)}{x^{k+1}} -\frac{l}{[x](k+1)} |+\color{red}{( )x^{-k-1}}\\
 & \leq {\textstyle \frac{\overbrace{M}^{\sup _{1\leq i\leq N} h( i)}}{x^{k+1}} +\frac{N|l|}{[ x]( k+1)} +\frac{\epsilon ([ x] -N)}{x} +|\frac{l}{x} -\frac{l}{[ x]( k+1)} |([ x] -N) +\color{red}{\overbrace{\color{red}{\sup _{t\in [ 0,1)}f(t)}}^{M'} x^{-k-1}}}\\
 & =\frac{M+M'}{x^{k+1}} +{\textstyle \frac{N|l|}{[ x]( k+1)} +\color{purple}{|\frac{l[ x]}{x} -\frac{l}{( k+1)} |} -N|\frac{l}{x} -\frac{l}{[ x]( k+1)} |+\frac{\epsilon ([ x] -N)}{x}} \tag 1
\end{align*} In $(1)$ , except the purple term every other term can be made arbitrarily small. How do I take care of the purple term so that I can conlude the desired result by limit definition.","['limits', 'calculus', 'real-analysis']"
4382912,Adjoint of unbounded integral operator,"Consider a Borel-measurable function $a:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{C}$ , and let $T_a$ be the linear operator on $L^2(\mathbb{R})$ with domain \begin{equation}
\mathcal{D}(T_a)=\left\{f\in L^2(\mathbb{R}):\int\left|\int a(x,y)f(y)\;\mathrm{d}y\,\right|^2\mathrm{d}x<\infty\right\}
\end{equation} acting as \begin{equation}
\left(T_af\right)(x)=\int a(x,y)f(y)\;\mathrm{d}y,
\end{equation} that is, the integral operator with kernel $a(x,y)$ . We assume $\mathcal{D}(T_a)$ to be dense in $L^2(\mathbb{R})$ . It is fairly immediate to prove that, if the kernel is assumed to be square-integrable with respect to both entries: \begin{equation}
\int\left|a(x,y)\right|^2\;\mathrm{d}x\,\mathrm{d}y<\infty,
\end{equation} then $T_a$ is a bounded operator (in fact, it is Hilbert-Schmidt) whose adjoint $T_a^*$ is equal to the integral operator associated with the function $a^*(x,y)=\overline{a(y,x)}$ , that is, $T_a^*=T_{a^*}$ . In particular, if $\overline{a(y,x)}=a(x,y)$ , then $T_a$ is self-adjoint. My question is whether, or under what conditions, the same holds if the condition above is relaxed, i.e. if the equality $T_a^*=T_{a^*}$ also holds for unbounded integral operators. In general, it is easy to prove that $T_{a^*}$ and $T_a$ have the same domain and \begin{equation}
T_{a^*}\subseteq T_a^*,
\end{equation} since, given $f,g\in\mathcal{D}(T_a)$ , \begin{equation}
\left\langle T_{a^*}g,f\right\rangle=\left\langle g,T_af\right\rangle.
\end{equation} However, we are left with proving the converse inequality $\mathcal{D}(T_a^*)\subseteq\mathcal{D}(T_a)$ . I was trying to do that by using the very definition of adjoint: given $g\in\mathcal{D}(T_a^*)$ , there must exist some $\tilde{g}\in L^2(\mathbb{R})$ such that, for every $f\in\mathcal{D}(T_a)$ , \begin{equation}
\left\langle\tilde{g},f\right\rangle=\left\langle g,T_af\right\rangle,
\end{equation} that is, writing the integrals explicitly and taking complex conjugates, for every $f\in\mathcal{D}(T_a)$ , \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0.
\end{equation} The idea is to use the arbitrariety of $f$ above to show that the quantity between square brackets must vanish. EDIT : More precisely, I thought about that: given $f\in L^2(\mathbb{R})$ , by assumption there is a sequence of functions $\{f_n\}_{n\in\mathbb{N}}\subset\mathcal{D}(T_a)$ with $f_n\to f$ ; for such a sequence, we have \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f_n(y)}\:\mathrm{d}y=0.
\end{equation} Formally, taking the limit $n\to\infty$ and using the continuity of the $L^2(\mathbb{R})$ scalar product, one would show that indeed \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0,
\end{equation} which would prove the desired property, since $f$ is arbitrary. However, to do that, the assumption $g\in\mathcal{D}(T_a^*)$ alone must imply that, indeed, the function $y\mapsto\int \overline{a(y,x)}g(x)\,\mathrm{d}x$ be square-integrable. Notice, as a final consideration, that a similar procedure is usually carried out for multiplication operators $(M_Af)(x)=A(x)f(x)$ (which, roughly speaking, may be interpreted as integral operators with a degenerate kernel $a(x,y)=A(x)\delta(x-y)$ , the latter being the Dirac distribution). In such a case, a procedure analogous to the one above allows one to prove that indeed $M_{A}^*=M_{A^*}$ .","['hilbert-spaces', 'adjoint-operators', 'functional-analysis']"
4382920,How to apply Laplace transform to integral?,"Question: For a real number x, assuming $f(t)=\int_0^\infty \frac{\sin{tx}}{\sqrt{x}}dx$ , apply Laplacetransformation to $f(t)$ and find $f(t)$ . Through Laplace transform, I can find $\int_0^\infty\frac{1}{\sqrt{x}}\int_0^\infty\sin{(tx)}e^{-st}dtdx=\int_0^\infty\frac{1}{\sqrt{x}}\frac{x}{s^2+x^2}dx$ , then I couldn't find a way to continue this. Did I made a mistakes here?","['calculus', 'laplace-transform', 'ordinary-differential-equations', 'real-analysis']"
4382951,Complexity of matrix multiplication over $\mathbb F_2$,"I've been reading about theoretical bounds on the computational complexity of $n\times n$ matrix-matrix multiplication in floating-point arithmetic, about how the complexity is known to fall between $\mathcal O(n^2)$ and $\mathcal O(n^3)$ , and how the exponent has been slowly whittled down over time by various researchers. It's currently unknown whether a $\mathcal O(n^2)$ algorithm is possible. I was wondering about what the complexity of multiplying two bit-matrices over the finite field $\mathbb F_2$ (or other finite fields - my intuition tells me the complexity will be the same), but I haven't found any references online. Are there any known ""tricks"" for multiplying matrices in $\mathbb F_2^{n\times n}$ that makes it less expensive than multiplication in larger fields or in floating point? What current bounds are known? Is there a known $\mathcal O(n^2)$ algorithm? Can anyone provide any relevant references for these sort of questions?","['finite-fields', 'reference-request', 'matrices', 'linear-algebra', 'algorithms']"
4382970,What is a non-effective holomorphic line bundle $L$ of degree 0 such that $L \otimes L$ is effective?,"I am trying to solve an exercise that asks for a non-effective holomorphic line bundle $L$ of degree 0 such that $L \otimes L$ is effective. I think I have mostly solved it but since I am a bit shaky on the details and would appreciate if someone could see if my approach is valid. We know that every line bundle has a meromorphic section thus we can find a divisor $D$ such that $L$ is nothing but the line bundle $[D]$ obtained by the map $[\cdot]: Div(X) \to Pic(X)$ . Hence in terms of divisors we want to find some $D \in Div(X)$ of degree 0 such that $h^0(X, D) = 0$ and $h^0(X, 2D) = 1$ , by an application of Riemann-Roch this is equivalent to $2D \sim 0$ and $D \nsim 0$ . If we have any hope of finding something like this we should look for it in some surface $X$ with non-zero genus. According to this top answer the Picard group of the torus has several $2$ -torsion elements that should do the trick, however, I have not been able to define the line bundle explicitly.","['algebraic-curves', 'riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry', 'line-bundles']"
4382974,Rademacher's integral with $sgn$ function,"Let $(r_n)_{n=1}^\infty$ be the sequence of Rademacher functions.
Prove that for all positive integers $n_1<n_2<\cdots<n_k$ and $p_1,\dots,p_k$ , it holds that \begin{equation*}
     \int_{0}^1r_{n_1}^{p_1}(t)\cdots r_{n_k}^{p_k}(t)dt=\left\{\begin{aligned}
     &1, \text{ if each }p_j\text{ is even},\\
     &0, \text{ otherwise.}
    \end{aligned}\right. 
\end{equation*} My attempt was to look at each $n$ th Rademacher's function: $$r_n(t)=sgn(\sin(2^n\pi t)),$$ for all $t\in[0,1]$ and \begin{equation*}
    sgn:\mathbb{R}\to\mathbb{R},\text{ }\text{ }sgn(x)=\left\{\begin{aligned}
        1&, &\text{ if }x>0,\\
        0&, &\text{ if } x=0,\\
        -1&, &\text{ if } x<0.
    \end{aligned}\right.
\end{equation*} So $$\int_0^1\displaystyle\prod_{j=1}^k[sgn(\sin(2^{n_j}\pi t))]^{p_j}dt$$ will be zero if at least for a $j_0$ , we have: $$\sin(2^{n_{j_0}}\pi t)=0.$$ It happens when $2^{n_{j_0}}\pi t=\pi l$ , for all $l\in\mathbb{Z}$ . Then $2^{n_{j_0}} t=l$ . Hence $2^{n_{j_0}}t$ is a integer number. I can't get anything done for the $p_j$ . Is there any other approach to this exercise?","['rademacher-distribution', 'functional-analysis', 'analysis', 'real-analysis']"
4382978,"Is each of $\int_0^\infty\frac{dx}{x^x},\int_0^\infty\frac{dx}{x^{x^{x^x}}},\int_0^\infty\frac{dx}{x^{x^{x^{x^{x^x}}}}},\cdots$ less than $2$?","A few years ago I asked about the inequality Prove that $\int_0^\infty\frac1{x^x}\, dx<2$ . As I came back to revisit it, I found that each of the following tetration integrals $$\int_0^\infty\frac{dx}{x^x},\int_0^\infty\frac{dx}{x^{x^{x^x}}},\int_0^\infty\frac{dx}{x^{x^{x^{x^{x^x}}}}},\cdots$$ appeared to be bounded above by $2$ . In the plot below, each index denotes half the number of tetrations. Obviously, one method of attack is to show that if $f_1(x)=x^x$ and $f_{k+1}(x)=x^{x^{f_k(x)}}$ , then $\int_0^\infty dx/f_{k+1}(x)>\int_0^\infty dx/f_k(x)$ for each $k>1$ , and $\lim_{k\to\infty}\int_0^\infty dx/f_k(x)<2$ . Comments. The first step in the method above means that the area gained in the interval $(0,1)$ is greater than the area lost in $(1,\infty)$ . It appears that the consecutive differences (plotted below as %97 ) $$\int_0^\infty\frac{dx}{f_{k+1}(x)}-\int_0^\infty\frac{dx}{f_k(x)}$$ decay on the order of $k^{-\log k}$ , and decrease monotonically in most instances as well. I have now cross-posted this problem on MathOverflow . For the second step, the limiting case turns out to be very easy to prove. We have $$\lim_{k\to\infty}\int_0^\infty\frac{dx}{f_k(x)}<\lim_{k\to\infty}\int_0^\infty\frac{dx}{g_k(x)}=-\int_0^\infty t\cdot\frac d{dt}\frac1{t^t}\,dt=\int_0^\infty\frac1{x^x}\,dx<2$$ where $g_{k+1}(x)=x^{g_k(x)}$ and $g_1(x)=x$ .","['integration', 'inequality', 'power-towers', 'special-functions']"
4383005,"Why is ""$\text{not }(f'>0)$"" not logically equivalent to $f' \le 0$","Recently, I was trying to solve a proof of the type $P \implies Q$ with $Q$ representing the statement $f'>0$ . I tried to solve this by contradiciton and rewrote it as $P \text{ and not }Q$ is false. Here I thought that $\text{ not} f'>0$ was equivalent to $f' \le 0$ but was told I was wrong in thinking that, can anyone explain to me why?","['derivatives', 'real-analysis']"
4383057,Questions regarding rings where every injective modules over them are also flat modules.,"Let $R$ be a ring such that every left injective $M$ module over $R$ is also a left flat module over $R$ . An example that springs to my mind to illustrate this type of rings are the regular rings, since it is well known that every module over a regular ring is flat, particularly all injective modules over a regular ring are flat. First question is: Are there any more examples of these type of rings, where every injective module over this particular ring is also a flat module over the same ring? Second question is if $R$ is ring such that every left injective $M$ module over $R$ is also a left flat module over $R$ and every right injective $M$ module over $R$ is also a right flat module over $R$ , then is it possible to prove that $R$ is a coherent ring? That is, where every submodule of a finitely generated module over $R$ is finitely presented. I've run out of ideas proving this last statement but maybe my affirmation is not true.","['homological-algebra', 'modules', 'ring-theory', 'abstract-algebra', 'noncommutative-algebra']"
4383069,Show that if $F_{U|W}( aw|w)=\frac{1}{2}$ then $U$ is gaussian; assume $W=U+Z$ where $Z$ is standard normal,"Let \begin{align}
W=U+Z
\end{align} where $Z$ is a standard normal and independent of random variable $U$ .  Suppose that for some given $a \in (0,1)$ \begin{align}
F_{U|W}(aw|w)=\frac{1}{2}, \forall w\in \mathbb{R}. 
\end{align} where $F_{U|W}(u|w)$ is the conditional cdf of $U$ given $W$ , that is \begin{align}
F_{U|W}(u|w)=P( U \le u| W=w)
\end{align} Question: Show that the above holds if and only if $U$ is Gaussian.  (Assume $U$ is zero mean) Forward direction: One of the directions is easy if $U$ is Gaussian, since the sum is Guassian we have that \begin{align}
F_{U|W}(t|w)=\Phi \left( \frac{t- E[U|W=w]}{\sqrt{Var(U|W=w)}}\right)=\Phi \left( \frac{t- b w}{\sqrt{b}} \right)
\end{align} where $\Phi(\cdot)$ is the cdf of standard normal and we have used that $E[U|W=w]=bw$ and $\frac{E[U^2]}{E[U^2]+1}=Var(U|W=w)=b$ . Therefore, the condition reduces to \begin{align}
\Phi \left( \frac{aw- b w}{\sqrt{b}} \right)=\frac{1}{2}, \forall w\in \mathbb{R}. 
\end{align} which is equivalent to \begin{align}
 aw- b w =0, \forall w\in \mathbb{R},
\end{align} Therefore, if we choose $U$ to be Gaussian such that $a=b=\frac{E[U^2]}{E[U^2]+1}$ , it will match the desired equation. The question now is how to show that Gaussian is the only one that satisfies this.","['conditional-probability', 'probability-theory', 'probability', 'normal-distribution']"
4383121,Is there a categorical characterization of covering maps?,"Given a topological space $X$ , some spaces $S$ will be covering spaces for $X$ and other spaces won't. Furthermore, some continuous maps $\pi: S \rightarrow X$ will be covering maps, and other maps won't be. I'm interested in whether or not these notions (covering space, covering map) can be defined inside the category $\bf{Top}$ ""using only notions of category theory"", that is talking only about objects and morphisms, and without actually looking at the points of the spaces. (We only need to characterize the covering maps inside $\bf{Top}$ , since a space is a covering space of another space iff a covering map exists between them.) The motivation for this question and the reason to think that such a characterization may at all be possible, is due to the well-known lifting properties of covering maps. For example, it is well-known that given a covering map $\pi:S\rightarrow X$ and a path $p:[0,1] \rightarrow X$ in $X$ , there exists a unique path $p':[0,1] \rightarrow S$ in $S$ such that $p=\pi \circ p'$ (the path-lifting property). There's also the homotopy-lifting property of covering maps and some other lifting theorems. Such lifting properties have a very ""categorical"" taste, as they discuss only existence and uniqueness of morphisms in $\bf{Top}$ and guarantee that a certain diagram be commutative. Overall, they 'smell' similar to other constructions in category theory that guarantee the commutativity of a certain diagram (such as limits). However, the path-lifting property itself is not (to my knowledge) unique to covering maps. This raises the question of whether or not some very general 'lifting property' exists that exactly characterizes covering maps, and such a property should (I think) be describable in the language of category theory. To summarize, my questions are: Is there some condition on a morphism in $\bf{Top}$ that can be described in the language of category theory (without quantifying over the elements of a space, as is done in the usual definition of covering maps), that exactly characterizes those morphisms in $\bf{Top}$ that are covering maps? If not, is this possible in some more restricted or similar category? (such as the category of points topological spaces, the category of locally path-connected semilocally simply-connected spaces, or the homotopy category $\bf{hTop}$ )? If such a condition does not exist, is there some category-theoretical explanation for why covering spaces have these lifting properties?","['general-topology', 'category-theory', 'algebraic-topology', 'covering-spaces']"
4383137,How does the angle of an isosceles triangle change as you increase the height,"$ABC$ and $ADB$ are isosceles triangles. Given $\beta,$ $R$ and $h$ , how can I find angle $\alpha$ ? $\beta$ is the top angle of the triangle $ABC$ , so $\angle{ACB}$ . $h$ is the change in height between $ABC$ and $ADB$ . $R$ is the length of one of the legs of the isosceles triangle $ADC$ , so $\vec{|AD|}$ and $\vec{|DC|}$ . $\alpha$ is the top angle of the triangle $ADB$ , so $\angle{ADB}$ My friend and I worked on it a bit, but we found some really complicated solution that was unusable. The problem itself seems pretty simple, so I feel like we did a wrong step at one point. Any help appreciated! Here is an image of the Isosceles Triangles better describing the problem:","['triangles', 'trigonometry', 'angle', 'geometry']"
4383141,"Proving $\ddot x = w-2x+x^2$, where $w\ge0$, conserves energy","Take $\ddot x = w-2x+x^2$ , where $w\ge0$ .
Show that the evolution of $x$ conserves a form of the energy and identify the potential function, sketching the phase portrait for $w=0$ . I am not too sure what is meant by the question. I know that to prove energy is conserved, you need to show that the derivative equals $0$ . If we take $V = w-2x+x^2$ , then $\dot V = -2\dot x+2x\dot x = 2\dot x(x-1) = 0$ , but how do we prove this equals $0$ , if this is what we need to show? I also thought it may be helpful to rewrite the equation as the system: $\dot x=y$ , $\dot y = w-2x+x^2$ , which would give $\dot V = 2y(x-1)$ . Any help would be hugely appreciated, thank you!","['ordinary-differential-equations', 'functions', 'partial-differential-equations', 'nonlinear-system', 'derivatives']"
4383197,Domain of solution of differential equation $y’=\frac{1}{(y+7)(t-3)}$,"If we are given the initial-value problem $$\frac{dy}{dt}=\frac{1}{(y+7)(t-3)}, y(0)=0$$ I want to solve said initial value problem & state the domain of the solution. I also want to observe what happens when $t$ approaches the limits of the solution’s domain. Via separation of variables, one obtains: $$\implies(y+7) \space dy = \frac{dt}{t-3}\implies\frac{1}{2}y^2+7y=\ln|t-3|+c_1$$ $$\iff y^2+14y+49=2\ln|t-3|+(2c_1+49)$$ Now call $C=2c_1+49$ , then $$(y+7)^2=2\ln|t-3|+C\implies y(t) = \pm\sqrt{2\ln|t-3|+C}-7$$ Substituting the initial condition: $$0=\pm\sqrt{2\ln|0-3|+C}-7\iff 7 = \pm\sqrt{2\ln(3)+C}$$ This shows we must choose the positive square root in order for our solution $y(t)$ to pass through the initial condition & solve the IVP. Then $$C=49-2\ln(3)$$ so that $$y(t)=\sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7$$ We know from the original differential equation that $y(t)\neq-7$ & $t\ne3$ . Thus: $$-7\neq \sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7 \iff\ln\left|\frac{t-3}{3}\right|\neq-\frac{49}{2}\iff t\neq\pm 3\exp\left(-\frac{49}{2}\right)+3$$ Does this tell us that the domain of the solution has to be $(-\infty,-3\exp\left(-\frac{49}{2}\right)+3)$ in order for $t=0$ to be on the domain of this solution (so the solution passes through the initial condition)? Would we just say $y(t)\to0$ as $t\to-3\exp\left(-\frac{49}{2}\right)+3$ ? Then finally $y(t)=\sqrt{2\ln\left(1-\frac{t}{3}\right)+49}-7$ , for $t\in(-\infty, -3\exp\left(-\frac{49}{2}\right)+3)$ .","['solution-verification', 'ordinary-differential-equations']"
4383209,Does this norm on $L^1$ have a name?,"We can define a norm, $\| \cdot \|$ on $L^1(X, \mathcal{A}, \mu)$ by $$\|f\| = \sup_{A \in \mathcal{A}} \bigg| \int_A f d\mu \bigg|$$ I found this norm referenced in a stats paper (they didnt say it was a norm) and was curious if it had a name or if it was related to some other concept like total variation or something.","['measure-theory', 'normed-spaces', 'terminology']"
4383267,Hessian proportional to metric implies the manifold is a warped product?,"This seems to be a straightforward result and it is intuitively true, but I some steps of this elude me. I will summarize it below. Let $f$ be a smooth function on a Riemannian manifold $(M, g)$ s.t. $$
\text{Hess}(f)(\nabla f, X) = \frac 12 (\mathcal L_{\nabla f} g)(\nabla f, X) = \lambda g(\nabla f, X)
$$ for any smooth vector field $X$ , where $\nabla f$ is the gradient of $f$ , characterized by $$
g(\nabla f, X) = \text d f(X).
$$ For brevity, write $Y = \nabla f$ , $b = g(Y, Y)$ , and $\theta_Y (X) = g(X, Y)$ . Note that, by the definition of the gradient, $\theta_Y(X) = \text d f(X)$ , so $\text d \theta_Y = 0$ . By Koszul's formula for the Riemannian connection: \begin{align*}
Xb = X g(Y, Y) &= 2 g(\nabla_X Y, Y) = (\mathcal L_Yg)(X, Y) + \text d \theta_Y(X, Y)
\\
&= 2 \lambda g(X, Y) = 2 \lambda g(X, \nabla f) = 2 \lambda \text d f(X),
\end{align*} in particular, $Yb = 2\lambda g(Y, Y) = 2\lambda b$ . Additionally, we can calculate \begin{align*}
(\mathcal L_Y \text d f)(X) &= Y (\text d f(X)) - \text d f([Y, X])
\\
&= Y(Xf) - Y(X f) + X(Y f) = X (\text d f(Y))
\\
&= X(g(\nabla f, Y)) = X g(Y, Y),
\end{align*} so by our previous computation, we see that $$
\mathcal L_Y \text d f = 2 \lambda \text d f.
$$ By expanding in local coordinates, it is easy to see that $Y b ^{-1} = -b ^{-2} Yb = - 2 \lambda b ^{-1} $ . Thus, applying the product rule for the Lie derivative, we arrive at \begin{align*}
\mathcal L_Y (b^{-1} \text d f \otimes \text d f) &= (Y b^{-1}) \text d f \otimes \text d f + b^{-1} (\mathcal L_Y \text d f) \otimes \text d f + b^{-1} \text d f \otimes (\mathcal L_Y \text d f)
\\
&= 2 \lambda b ^{-1} \text d f \otimes \text d f.
\end{align*} Now, by assumption, $$
\mathcal L_Y g = 2 \lambda g,
$$ and the author of the notes I linked above states that we can define a tensor $h$ such that $b(c) h$ is ... the restriction of $g$ to the level set $f = c$ . In particular, we have that $$
g|_p = \left(\frac 1b \text d f \otimes \text d f + b h \right)|_p, \ \text{when } f(p) = c.
$$ so that \begin{align*}
\mathcal L_{\nabla f} \left(\frac 1b \text d f \otimes \text d f + b h \right) &= 2 \lambda \left(\frac 1b \text d f \otimes \text d f + b h \right)
\end{align*} implies $g$ and $\frac 1b \text d f \otimes \text d f + b h$ satisfy the same differential equation with equal initial conditions, so they must be equal. What I am failing to understand is what exactly is $h$ , and how do we compute its Lie derivative?","['lie-derivative', 'riemannian-geometry', 'differential-geometry']"
4383273,How to get $\int_0^∞\frac{dx}{(x^2+1)^n}=\frac{(2n-3)!!}{(2n-2)!!}\frac{\pi}{2}\overset{?}=\frac{\sqrt{\pi}}{2}\frac{Γ(n-\frac{1}{2})}{Γ(n)}$?,"I recently solved the following integral using a recursive formula and integration by parts. $$\int_0^\infty\frac{dx}{(x^2+1)^n}=\frac{(2n-3)!!}{(2n-2)!!}\frac{\pi}{2}$$ Where $(n)!!$ represents the double factorial, not to be confused with $(n!)!$ . But when I plug this same integral into WolframAlpha I get this: $$\int_0^\infty\frac{dx}{(x^2+1)^n}=\frac{\sqrt{\pi}}{2}\frac{\Gamma(n-\frac{1}{2})}{\Gamma(n)}$$ How do I prove that these two results are equivalent?","['gamma-function', 'calculus', 'improper-integrals']"
4383330,Inequality in proof of Huisken's theorem,"Suppose $M$ is a uniformly convex hypersurface in $\mathbb{R}^{n+1}$ ( $A\geq \alpha Hg$ for $\alpha>0$ ) undergoing mean curvature flow. $A$ denotes the second fundamental form and $H$ denotes the mean curvature. The proof of Huisken's theorem uses a uniform bound on the following quantity. $$
  f_{\sigma,\epsilon} = \left(|A - \frac1nHg|^2-\epsilon H^2\right)H^{2-\sigma}
$$ for some $\sigma\in (0,1)$ and all $\epsilon>0$ . This is done by computing the evolution of $f_{\sigma, \epsilon}$ , estimating some of the terms, and then using the maximum principle. I am confused about the following inequality in p.294 of Extrinsic Geometric Flows by Andrews et. al: $$
\gamma \frac{|\nabla A|^2}{H^2}H^\sigma\geq \gamma f_{\sigma,\epsilon}\frac{|\nabla A|^2}{H^2} \tag{1}
$$ This implies that $H^\sigma\geq f_{\sigma,\epsilon}$ since $\gamma>0$ and $H>0$ . However, using $|A|^2\leq H^2$ , the best inequality I can get is \begin{align*}
\frac{f_{\sigma,\epsilon}}{H^\sigma} = \frac{|A|^2}{H^2}+1-\frac{2}{n}-\epsilon\leq 2-\frac{2}{n}-\epsilon\,.
\end{align*} In fact, using uniform convexity $A\geq \alpha Hg$ , we have $$
\frac{f_{\sigma,\epsilon}}{H^\sigma} =\frac{|A|^2}{H^2}+1-\frac{2}{n}-\epsilon\geq \alpha + 1-\frac{2}{n}-\epsilon
$$ so for large $\alpha$ we would have $f_{\sigma,\epsilon}\geq H^\sigma$ . What am I missing here? I have added an excerpt from the book below. Context : one of the terms in the evolution equation for $f_{\sigma, \epsilon}$ is $$-2H^\sigma|\nabla\frac{A}{H}|^2$$ Then, in p.294 they show that if $A\geq \alpha Hg$ for $\alpha>0$ , then $$
|\nabla\frac{A}{H}|^2\geq \gamma\frac{|\nabla A|^2}{H^2}
$$ where $\gamma$ depends on $\alpha$ and $n$ . Since $A\geq \alpha Hg$ is preserved under mean curvature flow, the above inequality holds for as long as the flow exists with the same constant $\gamma$ . This leads to the left side of (1). Edit: Huisken's original argument (Section 5 of this ) makes sense to me. He considers the slightly different function $$
  f_\sigma = (|A|^2 - H^2/n)H^{2-\sigma}\,.
$$ Computing the evolution of $f_\sigma$ , we have \begin{align}
  (\partial_t-\Delta) f_\sigma &= \frac{2(1-\sigma)}{H}\langle\nabla H,\nabla f_\sigma\rangle\\
  &-\frac{2}{H^{4-\sigma}}|H\nabla A - A\otimes\nabla H|^2\\
  &-\frac{\sigma(1-\sigma)}{H^{4-\sigma}}(|A|^2-H^2/n)|\nabla H|^2\\
  &+\sigma|A|^2f_\sigma\,.
\end{align} Using an earlier inequality (Lemma 2.3(ii)) and preservation of $A\geq \alpha Hg$ , the following is true under the flow $$
  |H\nabla A - A\otimes\nabla H|^2 \geq \frac12 \alpha^2 H^2|\nabla H|^2
$$ So the last three terms of $(\partial_t-\Delta) f_\sigma$ are bounded above By \begin{align}
&-\frac{\alpha^2 H^2}{H^{4-\sigma}}-\frac{\sigma(1-\sigma)}{H^{4-\sigma}}(|A|^2-H^2/n)|\nabla H|^2+\sigma|A|^2f_\sigma \\
&=-\frac{|\nabla H|^2}{H^{4-\sigma}}\left(\alpha^2 H^2+\sigma(1-\sigma)(|A|^2-H^2/n)\right)+\sigma|A|^2f_\sigma\\
&=-\frac{|\nabla H|^2}{H^{2-\sigma}}\left(\alpha^2+\sigma(1-\sigma)(|A|^2/H^2-1/n)\right)+\sigma|A|^2f_\sigma
\end{align} In the paper, this is simplified to $$
-\alpha^2\frac{|\nabla H|^2}{H^{2-\sigma}}+\sigma|A|^2f_\sigma
$$ which makes sense since $$
\sigma(1-\sigma)(|A|^2/H^2-1/n) \geq 0
$$ so it is a good term. Excerpt: (The book uses $II$ for the second fundamental form $A$ and $\mathring{II}$ for the trace free second fundamental form $A-Hg/n$ )","['mean-curvature-flows', 'riemannian-geometry', 'differential-geometry']"
4383354,Relation between a fixed point and being a well-order,"I've been trying to prove the following, but with no particular success: Given a linear order $\leq$ on $A$ , define $\pi:2^A\to 2^A$ by $X\mapsto\{y\in A: (\forall x < y)(x\in X) \}$ . Let $A_0$ be the least fixed point of $\pi$ . Prove that $x\in A_0$ iff $\{(a,b)\in A\times A: a\leq b < x\}$ is a well order. For example, suppose that $\{(a,b)\in A\times A: a\leq b < y\}$ is a well order. I must prove that $y\in A_0$ . Here's my thought process: what does it mean that $y\in A_0$ ? $A_0$ is a fixed point for $\pi$ iff $A_0=\{y\in A:(\forall x < y)(x\in A_0)\}$ . So the condition that $y$ lies in a fixed point $A_0$ means that $(\forall x < y)(x\in A_0)$ . But to chech this condition, I'd need to check the sub-condition $x\in A_0$ , which again means that $(\forall t < x)(t\in A_0)$ , and now the same problem arises with checking the sub-condition $t\in A_0$ ... Even setting this aside, I must use somehow that $\{(a,b)\in A\times A: a\leq b < y\}$ is a well order. So I'd think I need to guess some particular non-empty subset of $A$ that I need to consider, and then I need to take a least element in that subset. I don't really see which subset to consider, and how the ""leastness"" property would help. For the other direction, I have the same problem as described at the beginning. When trying to use the condition that $y\in A_0$ , an infinite chain of conditions arises...","['well-orders', 'fixed-points', 'fixed-point-theorems', 'order-theory', 'elementary-set-theory']"
4383385,Upper bound of stopped Brownian motion,"Let $\{B_t\}_t$ be a standard Brownian motion and $T$ be arbitrary stopping time, is it true that $\mathbb{E}[|B_1-B_{1\wedge T}|]\leq C\cdot P(T<\infty)$ for some constant $C$ ? Here $C$ should be a universal constant over all stopping time $T$ . I can obtain the upper bound $\mathbb{E}[|B_1-B_{1\wedge T}|]\leq 2\mathbb{E}[\sup_{t\in[0,1]}|B_t|]\triangleq M < \infty$ because $\sup_{t\in[0,1]}|B_t|$ is integrable. However, I am not sure how to make the upper bound involving $P(T<\infty)$ since on $\{T=\infty\}$ , we have $\mathbb{E}[|B_1-B_{1\wedge T}|]=0$ .","['conditional-expectation', 'stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
4383392,Probability you end dice rolling sequence with 1-2-3 and odd total number of rolls,"Here's a question from the AIME competition: Misha rolls a standard, fair six-sided die until she rolls 1-2-3 in that order on three consecutive rolls. The probability that she will roll the die an odd number of times is ${m\over{n}}$ where $m$ and $n$ are relatively prime positive integers. Find $m + n$ . This is what I did. Let's add up the following cases with an odd number of rolls, where in each sequence of XXX... there is no subsequence of 123: Probability of 123: $({1/6})^3$ Probability of XX123: $({1/6})^3$ Probability of XXXX123: $(1/6)^3(1 - 2(1/6)^3)$ Probability of XXXXXX123: $(1/6)^3(1 - 4(1/6)^3 + (1/6)^6)$ Probability of XXXXXXXX123: $(1/6)^3 (1 - 6(1/6)^3 + \binom{4}{2} (1/6)^6) = (1/6)^3 (1 - 6(1/6)^3 + 6(1/6)^6)$ Probability of XXXXXXXXXX123: $(1/6)^3 (1 - 8(1/6)^3 + \binom{6}{2} (1/6)^6 - 4(1/6)^9) = (1/6)^3 (1 - 8(1/6)^3 + 15(1/6)^6 - 4(1/6)^9)$ Probability of XXXXXXXXXXXX123: $(1/6)^3 (1 - 10(1/6)^3 + \binom{8}{2} (1/6)^6 - \binom{6}{3}(1/6)^9 + (1/6)^{12}) = (1/6)^3 (1 - 10(1/6)^3 + 28 (1/6)^6 - 20(1/6)^9 + (1/6)^{12})$ $\ldots$ and so forth. I notice some obvious patterns, but nonetheless I'm stuck with proceeding further. Any hints towards finding a way to add this all up would be well-appreciated. Please, no complete solutions. Edit: For the record, the correct probability after you add this all up should be ${{216}\over{431}}$ . Edit 2: There are a number of solutions to this problem here: https://artofproblemsolving.com/wiki/index.php/2018_AIME_II_Problems/Problem_13 However, all the solutions at the link are extremely clever, whereas my approach is a naive brute force. I would like some hints/suggestions on how to make my brute force approach work.","['summation', 'markov-chains', 'combinatorics', 'algebra-precalculus', 'probability']"
4383415,Drawing marbles until you run out of one color,"You have a bag with marbles and draw without replacement. The marbles have $k$ distinct colors and there are exactly $n$ marbles of each color. All marbles are equally likely to be drawn. You continue drawing until the bag runs out of a color so you have drawn all $n$ marbles of that color. What is the expected number of marbles you need to draw and what is the probability distribution to model this process? You clearly have do draw at least $n$ marbles and at most $k(n-1) +1$ by the pigeon hole principle. This looks similar to the negative hypergeometric distribution but it doesn't quite fit because I want to stop drawing the moment I run out of any color not just one particular color. Simplest example: There are $2$ red and $2$ green marbles in the bag. Without loss of generality let the first marble drawn be red. There is now a $1/3$ probability that the second marble is also red and I stop. If the second marble is green, the third marble will be either red or green and the bag will be out of that color so I stop no matter what. So with probability $2/3$ I will draw $3$ marbles. Hence the expected number of marbles drawn is $2\cdot (1/3) + 3\cdot (2/3)=2.67$ .","['probability-distributions', 'probability']"
4383544,How to show that $\sum_{k=1}^n k(n+1-k)=\binom{n+2}3$?,"While thinking about another question I found out that this equality might be useful there:
$$n\cdot 1 + (n-1)\cdot 2 + \dots + 2\cdot (n-1) + 1\cdot n = \frac{n(n+1)(n+2)}6$$
To rewrite it in a more compact way:
$$\sum_{k=1}^n k(n+1-k)=\frac{n(n+1)(n+2)}6.$$ This equality is relatively easy to prove:
$$\sum_{k=1}^n k(n+1-k)=
(n+1)\sum_{k=1}^n k - \sum_{k=1}^n k^2 =
(n+1) \frac{n(n+1)}2 - \frac{n(n+1)(2n+1)}6  = n(n+1) \left(\frac{n+1}2-\frac{2n+1}6\right) = n(n+1)\frac{3(n+1)-(2n+1)}6 = 
\frac{n(n+1)(n+2)}6.$$
(We only used the known formulas for the sum of the first $n$ squares and the sum of the first $n$ numbers .) Are there some other nice proofs of this equality? (Induction, combinatorial arguments, visual proofs, ...) EDIT: Now I found another question which asks about the same identity: Combinatorial interpretation of a sum identity: $\sum_{k=1}^n(k-1)(n-k)=\binom{n}{3}$ (I have tried to search before posting. But the answers posted here so far gave me some new ideas for good keywords to search which lead me to finding that question.) The questions are, in my opinion, not exact duplicates since the other question asks specifically about combinatorial proofs and my question does not have that restriction. But I agree that this is a very minor distinction. In any case, if you think that one of them should be closed as a duplicate, then you can vote to close. I will refrain from voting to close/reopen on this question. (If one of the two questions is voted to be a duplicate of the other one, they probably cannot be merged, since the summation variables are off by one.)",['summation']
4383566,Numbers $n$ whose representation as the product of two divisors require more digits than that of $n$,"Note : Posted to MO since it is unanswered in MSE. Let $f(x)$ be the number of digits in the decimal representation of $x$ e.g. $, f(0) = 1, f(1729) = 4$ . If $n = ab$ then we can show that $f(ab) > f(a) + f(b)$ is impossible. Hence $f(ab)$ is either less than or equal to $f(a) + f(b)$ . Loosely speaking if $f(ab) < f(a) + f(b)$ , we are using more digits than the number itself hence we are wasting digits. Definition : A number $n$ is said to be a wasteful number if for all its divisors $n = ab$ , we have $f(ab) < f(a) + f(b)$ . E.g. The divisors of $4321$ are $[1, 29, 149, 4321]$ and clearly $4321$ is wasteful. But $4324$ is not a wasteful number since $4324 = 46 \times 94$ and $f(4324) = f(46) + f(94) = 4$ . Trivially, all primes $p$ are wasteful since $f(p \times 1) < f(p) + f(1)$ . The first few wasteful numbers are: $$
2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 17, 19, 22, 23, 26, 29, 31, 33, 34, 37, 38, 39, 41, 43, 44, 46, 47, 50, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99
$$ Update : Added graph for the density of wasteful numbers below $10^9$ . There are $386000005$ wasteful numbers below $10^9$ . Question 1 : Does the natural density of wasteful numbers exist? Experimental data for $n \le 10^9$ suggests that it oscillates in a regular pattern between $0.2$ and $0.4$ . Question 2 : What are the necessary and sufficient conditions for a number to be wasteful? Update : 7 Apr 2023
It was proved in MO that Every wasteful number n must be its own lexicographically greatest
divisor","['divisibility', 'number-theory', 'elementary-number-theory', 'recreational-mathematics', 'prime-numbers']"
4383613,Geometric Construction: Restore the triangle with the only points marked are orthocentre and one vertex,"Given obtuse triangle $BAC$ with $\angle A$ obtuse. Let $H$ be the orthocentre. Let $M$ be the midpoint of $BC.$ Define $X$ such that $MB=MC=MX, D\in AH.$ Let $l$ be line $XM.$ Define $k$ as line $XB$ and $t$ as line $XC.$ Now everything is deleted except points $H,A,X$ and $l,k,t.$ Can we reconstruct $ABC$ triangle? For this problem, I defined the line perpendicular to $AH$ at $H.$ Let the line intersect $k,t$ at $F, G.$ Note that $GDF\sim CDB.$ Also it's well known that $DA,DM$ is isogonal.Therefore $DA$ is the symmedian of triangle $DCB.$ So we can use harmonic conjugates,etc. Consider $DD$ wrt $DCB$ then $DD\perp DM,$ as $M$ is centre. I also tried to use this famous geometry lemma, which states if $ABC$ a trinagle and $H$ is the orthocentre, $M$ is the midpoint of $BC$ then $M-G-H$ collinear where $G=(AH)\cap (ABC).$ We have $(T,D;B,C)=-1.$ And $H$ is orthocentre of $ATM.$ Also poles and polars maybe? Any ideas? The triangle before deleting: After deleting: The lemma diagram:","['euclidean-geometry', 'geometry', 'geometric-construction']"
4383709,Description of the difference tensor between a non-metric torsion-free connection and the Levi-Civita connection,"A known result is the fact that a connection on a manifold $M$ is not a tensor, but the difference between any two is. I am interested in knowing a little more about that tensor in particular cases, for example if we take $\nabla_{LC}$ the Levi-Civita connection (the unique metric torsion-free connection on $M$ ) and $\nabla$ any other metric connection the difference between them is known as the contorsion tensor ( https://en.wikipedia.org/wiki/Contorsion_tensor ). What can be said about such a tensor if $\nabla$ is not metric, but it is torsion-free? Or more general, what can be said about the difference tensor of any two torsion-free connections (not necessarily metric ones)? Thanks.","['connections', 'tensors', 'riemannian-geometry', 'differential-geometry']"
4383741,"Kolmogorov's 0-1 law, tail events and bounded sequence of random variables.","Let $(X_n)_{n\in\mathbb{N}}$ be a sequence of i.i.d. $\mathbb{R}$ -valued r.v.s such that for some $M \geq 0$ it follows that $P[|X_n| \leq M] = 1$ for all $n$ . Define $Y_n = \frac{1}{n}\sum_{i=1}^nX_i$ , and $L = \lim\sup_{n}Y_n$ , and $A = \{\lim_nY_n \text{ exists}\}$ . Then my reading material makes the following claim There exists some $c \in [-M, M]$ s.t. $P[L = c] = 1$ , and $P[A] \in \{0, 1\}$ The proof for the claim is as follows For any $q \in \mathbb{Q}$ define the tail event $A_q = \{L\geq q\}$ . By Kolmogorov's 0-1 law, the probability of each of these tail events is either 0 or 1. Thus, there exists some $c = \sup\{q:P[A_q] = 1\} = \inf\{q:P[A_q] = 0\}$ . Since $\mathbb{Q}$ is countable, $P[L\geq c] = P[L\leq c] = 1$ , and so $P[L = c] = 1$ . Finally $c \in [-M, M]$ , since $P[L\in [-M, M]] = 1$ . The main points I'm struggling to understand here are 1.) How do we know from the Kolmogorov's 0-1 law that there exists some constant $c$ s.t. $c = \sup\{q:P[A_q] = 1\} = \inf\{q:P[A_q] = 0\}$ , 2.) why does it follow from the countability of $\mathbb{Q}$ that $P[L\geq c] = P[L\leq c] = 1$ ?","['measure-theory', 'probability-theory']"
4383759,On a condition of the Szemerédi–Trotter theorem,"In the original paper from Szemerédi and Trotter: https://link.springer.com/article/10.1007/BF02579194 They state the theorem $1$ only if $\sqrt{n} \leq t \leq \binom{n}{2}$ . Where $t$ is the number of lines and $n$ is the number of points. In further sources this condition does not appear, for example Wikipedia: https://en.wikipedia.org/wiki/Szemer%C3%A9di%E2%80%93Trotter_theorem Also in the paper ""Crossing Numbers and Hard Erdős
Problems in Discrete Geometry"", https://www.cs.umd.edu/~gasarch/TOPICS/erdos_dist/szekely.pdf , it is gone. So is the condition not needed? Is this due to a better proof strategy, better methods? And can this also be shown by the methods Szemerédi and Trotter used?","['alternative-proof', 'proof-explanation', 'combinatorial-geometry', 'combinatorics']"
4383768,Why no simple proof by contradiction for mean value theorem?,"I'm reading the Wikipedia proof for the MVT, and it uses Rolle's theorem. In fact, many other websites that prove MVT do the same. When I first read the statement of the mean value theorem, I thought it must obviously be true because the alternative, that $f'(x) > \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b)$ was absurd (and same for $f'(x)$ strictly less than the right hand side), because the rate of increase of the function being higher than the average rate of increase at all points contradicts the very definition of the average rate of increase. By this, I mean that if the function $f$ was increasing at the average rate $f'(x) = \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b)$ then it would exactly go straight from $(a, f(a))$ to $(b, f(b))$ (this is the definition of average), and so if it's always increasing strictly faster , then surely it increases ""too much"" to be able to get down to $(b, f(b))$ in time, so to speak. Yet, I do not see sites formalizing this to give a proof by contradiction of the mean value theorem in a line or two, so I imagine this must be going wrong somewhere. Could someone tell me where?",['real-analysis']
4383770,Probability of having exactly one empty container,"What's the probability that if we distribute N letters among N drawers and presume that every single configuration is equally likely (saturation is at least 0 and at most N) then a single drawer will be found empty? ////////////////////// My progress so far: Total number of cases: $N^{N}$ Reasoning:
It's the implication of the saturation condition. ///////////////// The number of favorable configurations: $2 \cdot \binom{N}{2}\cdot (N - 2)!$ Reasoning: I can choose $2$ drawers out of $N$ drawers in $\binom{N}{2}$ ways. These two drawers serve as the empty one and the one containing two letters. The factor $2$ is to consider the relative location of the two drawers. $(N - 2)!$ is to take the permutation of the remaining drawers into account. /////////////////// I feel like something is mistaken. Question: What's the route of correctness?","['combinatorics', 'probability']"
4383792,Are jumps in the growth function of an infinite group increasing?,"Let $G$ be a group with a $S$ a finite subset of $G$ generating it, with $\{e\}\in S$ and $S=S^{-1}$ , and let $\gamma_G^S$ be the growth function of $G$ respect to $S$ , that is, $\gamma_G^S(l)$ is the number of elements of $G$ which can be expressed as a product of $\leq l$ elements of $S$ . Call $J(l)=\gamma_G^S(l)-\gamma_G^S(l-1)$ . If $G$ is infinite, is it true that $J(l+1)\geq J(l)$ for $l\geq1$ ? I came up with this question and it seems true, but I haven't found a way to prove it.","['geometric-group-theory', 'group-theory', 'cayley-graphs', 'subgroup-growth']"
4383857,How to determine power of a test of the difference of two binomial proportions?,"Consider a sample of 800 adults with wrist fracture where 400 are provided an operative treatment and 400 are provided physiotherapy only. Outcome of interest is whether the wrist is fully healed after 6 months. What power will a hypothesis test possess to detect a 10% improvement in the operative cohort, assuming 50% of the physiotherapy cohort are fully healed at 6 months? My attempt (assuming $\alpha = 0.05$ ): A 10% improvement would mean the proportion difference (test statistic) between the two therapies: $$\theta = p_2 - p_1 = \frac{240}{400} - \frac{200}{400} = 0.1$$ Thus, we have two sample distributions for $\theta$ . One is the null hypothesis where there is no difference between the therapies and the mean, $\bar{\theta_1}$ , is $0$ . The other distribution is where $\bar{\theta_2}$ , is $0.1$ . Here is an illustration: This is the point where I got stuck. Usually, in these types of questions I'm told that we are sampling from a normally distributed population with a known standard deviation so I can use Z statistic ( $1.64$ ) to determine the rejection threshold for the null hypothesis and go from there. However, here we are reliant on sampling two distributions (physio + operative) that I assume to be binomially distributed. Thus, the variance of the black and red curves (null vs alternate hypothesis) would be the sum of the variances from the two underlying distributions: $$Var(\theta) = \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}$$ How do I proceed from here? Can we assume the sample distributions are normally distributed? If so, I can use the Z statistic with the null distribution: $$1.96 = \frac{\theta - 0}{\sqrt{2\frac{p_1(1-p_1)}{n_1}}}$$ Edit: This question was inspired by the example in this video starting at 50:47 . I don't understand the reasoning behind his steps and I tried to break it down using the approach I was more familiar with.","['statistics', 'probability-distributions', 'probability', 'hypothesis-testing']"
4383877,Second derivative of convex decreasing functions larger than square of first derivative?,"Given a twice differentiable, convex, decreasing function $f\in C^2$ with $\lim_{x\to\infty} f(x)=k$ , I am trying to identify some sufficient conditions for the second derivative to be larger than the square of the first derivative above some level $\bar x$ , or $$ f''(x) > f'(x)^2 \qquad x>\bar x.$$ I see that this is true of the negative exponential for $x>0$ and of the negative power function $1/x^n$ for $x^n>n/(n+1)$ .","['convex-analysis', 'derivatives']"
4383935,Is solution of $\frac{dy}{dx}=x^2+y^2 +1$ an odd function,"I had this question in my exam which is of multiple select question (MSQ) type. I am sure about three options but confused about one. Question is ""Let $(-c,c)$ be the largest open interval in $R$ ( $c$ >0) on which the solution $y(x)$ of the differential equation $\frac{dy}{dx}=x^2+y^2 +1$ with initial condition $y(0)=0$ exists and is unique. Then which of the is/are true?"" Options were (A) $y(x)$ is an odd function on $(-c,c)$ (B) $y(x)$ is an even function on $(-c,c)$ (C) $(y(x))^2$ has a local minimum at $0$ (D) $(y(x))^2$ has a local maximum at $0$ Clearly $\frac{dy}{dx}>0$ , so $y$ is strictly increasing. Given that $y(0)=0$ , we then have $y(x)<0$ for $x<0$ and $y(x)>0$ for $x>0$ . Using this, options (B) and (D) get discarded and option (C) looks correct. But not getting any idea about option (A). Also I tried to solve this ODE but no method is working that I know. Any hint or help. Thanks.",['ordinary-differential-equations']
4383997,On the density of a particular subset of integers,"Given a positive integer $n$ in the standard form $$n=\prod_k p_k^{\alpha_k}$$ and the arithmetic function $$f(n)=\sum_k \alpha_k p_k$$ let's define the subset $F$ of positive integers $$F=\Big\{n\in N:f(n)\,|\,n,\;f(n)\lt n\Big\}=\Big\{16,27,30,60,70,72,84,105,150,\dots\Big\}$$ I ask if the density of this subset has ever been studied and, in particular, if it is possible to prove the convergence of the series $$\sum_{n\,\in\,F}\frac 1 n$$ Numerical experiments would show the convergence of such series towards a value quite close to the inverse of Euler's number $$\sum_{n\,\in\,F}\frac 1 n\sim\frac 1 e$$ Edit My script is still running, but after $5\cdot 10^5$ terms ( $n=584504910$ ) the sum of the series is $0.36652132586744884...\;(\frac 1 e = 0,36787944117144232...)$ : the growth is extremely slow. Work in progress The most recent values obtained are the following: $n=9928531324,\;\;3986000$ -th term of the series $,\;\;$ partial sum $\,=0.36776500537719703...$ $n=9931911561,\;\;3987000$ -th term of the series $,\;\;$ partial sum $\,=0.36776510608002266...$ $n=9935361024,\;\;3988000$ -th term of the series $,\;\;$ partial sum $\,=0.36776520674763440...$ $n=9938801814\,(\sim 10^{10}),\;\;3989000$ -th term of the series $,\;\;$ partial sum $\,=0.36776530738064540...$ I am cautiously optimistic about the convergence of the series.","['arithmetic-functions', 'number-theory', 'elementary-number-theory', 'sequences-and-series']"
4384022,Fiber bundle map is proper if the model fiber is compact,"This is one direction of problem 10-19 (c) from John Lee's Introduction to Smooth Manifolds. Suppose $\pi: E \to M$ is a fiber bundle with fiber $F$ . Show that $\pi$ is a proper map if $F$ is compact. Here, $M$ , $E,F$ are topological spaces with no further assumption and $\pi:E \to F$ is a surjective continuous map with the property that for each $x\in M$ , there exist a neighborhood $U$ of $x$ in $M$ and a homeomorphism $\Phi: \pi^{-1}(U)\to U\times F$ , called a local trivialization of $E$ over $U$ , such that we have $\pi_1 \circ \Phi = \pi$ , where $\pi_1$ is the projection onto $U$ . I have seen a solution to this problem Let $\pi: E \rightarrow M$ be a fiber bundle with fiber $F$. Then $\pi$ is a proper map $\iff F$ is compact. here that uses the manifold assumptions on $E$ and $M$ , namely Hausdorff and local compactness, but I cannot figure out a way to prove this in the general case without these assumptions on the topological spaces. Suppose that $F$ is compact and $A \subset M$ is compact. Then we need to show that $\pi^{-1}(A)$ is compact. Clearly we would need to use the local trivialization property, so that for each $p \in A$ , we can find a cover $U_p$ such that $\Phi: \pi^{-1}(U_p) \to U_p \times F$ is a homeomorphism and $p \in U_p$ . And cover $A$ with finitely many $U_i$ 's. But I cannot think of a way to progress from here without resorting to local compactness. How can we prove this? I would greatly appreciate some help.","['fiber-bundles', 'manifolds', 'general-topology', 'algebraic-topology', 'differential-geometry']"
4384025,How to phrase a question with many answers properly,"The question goes as follows: Find all values of $n$ such that $(1+i)^n + (1-i)^n = 0$ In my final line of work, I got $i^n = -1$ So how do I properly phrase the answer? There are multiple values. Something like this: $n =2,6,10, 14,18... \infty $ ? and $n = -2,-6...-\infty$ ?","['notation', 'algebra-precalculus']"
4384133,What does the limit $\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)}$ mean for non-polynomials,"The limit $$\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)}$$ has an important meaning for polynomials. Suppose $f(x)=(x-a)^nq(x)$ where $q(a)\neq 0$ . Then, the given limit is equivalent to $$\lim_{x \to a}\frac{\{n(x-a)^{n-1}q(x)+(x-a)^nq'(x)\}(x-a)}{(x-a)^nq(x)}=\lim_{x \to a}\frac{nq(x)+(x-a)q'(x)}{q(x)}=n+\lim_{x \to a}\frac{(x-a)q'(x)}{q(x)}=n$$ where the last limit comes from the fact that $q(a)\neq 0$ , but the numerator goes to zero. So, for polynomials, the value of the given limit would indicate the algebraic multiplicity of the polynomial of the term $(x-a)$ . We could also extend this concept to functions not quite polynomial, but ""looks"" like a polynomial, for example functions like $x-4\sqrt{x}+3$ . This function has the above limit value when $a=1$ as $$\lim_{x \to 1}\frac{(x-1)\times \{1-\frac{2}{\sqrt{x}}\}}{(\sqrt{x}-1)(\sqrt{x}-3)}=\frac{2\cdot (-1)}{1-3}=1$$ One could also extend to functions like $f(x)=x^\pi$ . The limit in this case would be, $$\lim_{x \to 0}\frac{x\times \pi x^{\pi-1}}{x^\pi}=\pi$$ So, the limit would similarly be the value of the ""degree of $(x-a)$ ""(which, if it is even a valid saying). (Note that we choose function $f(x)$ and the value $a$ for each limit. Yet since the limit has some meaning when $f(a)=0$ , I would omit the choice of $a$ whenever the root of $f(x)$ is unique.) My question is, how can I extend this concept to functions that are not in the form of $f(x^p)$ (where $f$ is a polynomial, $p$ is a real number)? Are there any meaning to this limit value when the function is a transcendental function? Are there any concepts that this value represents? (Edit) Here are some examples of the value of the limit for some functions. $f(x)=e^{-x}(x^3\sin x)$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^{-x}(-x^3\sin x+3x^2\sin x+x^3\cos x)}{e^{-x}(x^3\sin x)}=4$$ $f(x)=\frac{\sin x}{x}$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times \frac{x\cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=0$$ $f(x)=\frac{\sin x}{x}$ at $a=\pi$ $$\lim_{x \to \pi}\frac{(x-\pi)f'(x)}{f(x)}=\lim_{x \to \pi}\frac{(x-\pi)\frac{x \cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=1$$ $f(x)=e^x-1$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^x}{e^x-1}=1$$","['limits', 'calculus', 'real-analysis']"
4384143,Minimum KL divergence distribution with constant shift,"I have a little trouble calculating the optimal distribution $P$ with fixed variance value that minimizes the following KL divergence: $$KL(x||x-\epsilon),$$ where $\epsilon$ is known and $x\sim P$ . Not sure whether it would be related but if the objective is the entropy instead of the KL, the optimal solution would be Gaussian distribution by solving the Lagrangian function of the objective. I wonder whether there would be some similar solutions when the objective is KL divergence instead of the entropy?","['optimization', 'statistics', 'probability']"
4384157,Understanding the Honeybee Conjecture vs. Sphere Packing Conjecture,"I am trying to understand the differences between the Honeybee Conjecture and the Sphere Packing Conjecture (also called the Kepler Conjecture). As a quick overview: ""The honeycomb conjecture states that a regular hexagonal grid or honeycomb is the best way to divide a surface into regions of equal area with the least total perimeter."" ( https://en.wikipedia.org/wiki/Honeycomb_conjecture ) ""The Kepler conjecture, named after the 17th-century mathematician and astronomer Johannes Kepler, is a mathematical theorem about sphere packing in three-dimensional Euclidean space. It states that no arrangement of equally sized spheres filling space has a greater average density than that of the cubic close packing (face-centered cubic) and hexagonal close packing arrangements. The density of these arrangements is around 74.05%."" ( https://en.wikipedia.org/wiki/Kepler_conjecture ) Reading this, I had the following questions: 1) The Honeycomb Conjecture says that a hexagonal grid is the best way to divide regions into equal area and the least perimeter, whereas the Sphere Packing Conjecture says that closest packing arrangements can be achieved EITHER using Cubic Arrangements OR Hexagonal Arrangements. Do the results from both these conjectures contradict each other - or are both these conjectures addressing fundamentally different problems? 2) From the Wikipedia Page on the Honeycomb Conjecture, (apparently) the official mathematical formulation of the Honeycomb Conjecture can be written as follows: Why do we want the supremum of the limit of ""r"" to approach infinity? Why are we interested in the perimeter and area of the ""bounded components and the disk of radius r"" - does this represent a choice of tiling arrangement? Would the disk B(0,r) represent a choice of tiling like a hexagon? The fourth root of 12 is 1.86 : why is 1.86 relevant here? 3) Finally, for both the Honeybee Conjecture and the Sphere Packing Conjecture - are we able to ""bound"" the sample space of the solution (i.e. how many possible arrangements can exist)? For instance, suppose we want to predict the the possible outcomes that can arise from flipping a two-sided coin twice - the sample space corresponding to this problem would include 4 outcomes : (Head, Head), (Head, Tail), (Tail, Head), (Tail, Tail) If we were to consider the Sphere Packing Conjecture, we know that the sample space to this problem contains at least 3 possible arrangements: Cubic, Hexagonal and Random. But do there exist more possible arrangements? Are the number of total possible arrangements finite? If we were to consider the Honeybee Conjecture, the following video ( https://www.youtube.com/watch?v=7edkFs8Vu1E @ 4:03 ) shows us that there the sample space of this problem contains at least 5 possible arrangements: But do there exist more possible arrangements? Are the number of total possible arrangements finite? I found this Wikipedia Page ( https://en.wikipedia.org/wiki/Uniform_tiling ) that contains (beautiful) pictures about different ways to ""partition and pack"" space: Reading the Wikipedia page, the Euclidean Plane seems to have 3 types of uniform tiling arrangements : Square, Hexagonal and Triangular. But  reading the ""Uniform Tilings Using Star Polygons"" section - it seems like you can have plenty of other types of uniform tiling arrangements (e.g. Topological 3.12.12 , Topological 4.4.4.4, , Topological 6.6.6, etc.). "" The complete lists of k-uniform tilings have been enumerated up to k=6. There are 20 2-uniform tilings, 61 3-uniform tilings, 151 4-uniform tilings, 332 5-uniform tilings, and 673 6-uniform tilings."" ( https://en.wikipedia.org/wiki/List_of_k-uniform_tilings ) - Is it possible there could existing more tiling arrangements for k>6? Do there exist finite or infinite Non-Uniform Tiling Arrangements? ( https://commons.wikimedia.org/wiki/Category:Non-uniform_tilings_by_regular_polygons ) But to understand these (beautiful) pictures goes far beyond my knowledge, and I am still not sure if the sample space of the Honeycomb Conjecture contains finite or infinite packing arrangements. Can someone please help me understand this? Thank you!","['spheres', 'geometry', 'probability', 'packing-problem']"
4384168,Intuitive reason for an approximation to $x!$,"I was working today on approximating $x!$ using the integral of $\ln x$ , and using computer software to calculate limits, I found that $\dfrac{(12x)(\sqrt{2\pi x})(x^x)}{(12x-1)e^x}$ gives a very close approximation of $x!$ . Multiplying the figure by $\dfrac{288x^2}{288x^2+1}$ seems to render the approximation even closer, and there are obviously further terms one could add as well. What I'm curious about is the role of the $\sqrt{2\pi x}$ term; I knew there would be a (hopefully asymptotic) error term in the product, but $\sqrt{2\pi}$ had never crossed my mind as a possibility. Is there an intuitive reason that this term appears here?","['calculus', 'factorial', 'approximation', 'asymptotics']"
4384197,Relationship Between Hyperbolas and Hyperbolic Spaces,"I am trying to understand the difference between Hyperbolic Functions and Hyperbolic Spaces . In my very limited knowledge of mathematics, I have only come across: Hyperbolas : https://i.ytimg.com/vi/Iu-4-fizlD4/maxresdefault.jpg Hyperbolic (Trigonometric) Functions : https://en.wikipedia.org/wiki/Hyperbolic_functions Recently, I came across ""Hyperbolic Spaces"" ( https://en.wikipedia.org/wiki/Hyperbolic_space ). I tried to read more about this and I think I understand the general idea: Although I don't fully understand the math, but I think a ""hyperbola"" would resemble the shape of a ""hourglass cone"" in 3 dimensions. As the picture above shows, a triangle in Euclidean Space becomes ""distorted"" when projected onto a Hyperbolic Space. This is my naïve guess about the relationship between hyperbolic functions and hyperbolic spaces. Regarding all this, I have the following questions: 1) Although the sum of all 3 angles of the triangle is preserved when it is projected from Euclidean Space to Hyperbolic Space - is there a way to ""mathematically understand the projection""? I know that the general equation of a Hyperbola is f(x) = 1/x : if we know the equations that make a specific triangle, how can we use our knowledge of the general equation of a Hyperbola to describe the projection? 2) When searching for images of ""Hyperbolic Spaces"", the following types of images always come up: What is the relationship between the above diagrams and hyperbolic spaces? Are these pictures trying to illustrate some concept in particular (e.g. the projection of some shape from Euclidean Space to Hyperbolic Space, e.g. dodecahedral tessellation)? Is there any reason that these types of pictures are often used to illustrate the concept of Hyperbolic Spaces? Thank you!","['projection', 'euclidean-geometry', 'hyperbolic-geometry', 'geometry']"
4384226,"Rigorous statement on that two small trianges, one being the image of the other by a holomorphic function, are similar","Let $w = f(z)$ be a non-constant holomorphic function. Let $Z_0$ be a point in the complex plane, $Z_1$ are $Z_2$ are points in the neighbourhood of $Z_0$ .  Let $W_0, W_1, W_2$ be the corresponding points in the image complex plane of $w = f(z)$ . One can sort of claim that $\triangle W_0 W_1 W_2$ and $\triangle Z_0 Z_1 Z_2$ are similar as long as the neighbourhood is small. This statement is visually intuitive. I'm wondering how can one make this statement rigorous like the $\epsilon-\delta$ way of defining limit. Ideally the rigorous statement would keep the intuition.",['complex-analysis']
4384263,Finding the domain and range of $\left(1+\frac1x\right)^x$,"Find the domain and range of $\left(1+\frac1x\right)^x$ My Attempt: $1+\frac1x\gt0\implies\frac{x+1}x\gt0\implies x\in(-\infty,-1)\cup(0,\infty)$ $\lim_{x\to\pm\infty}\left(1+\frac1x\right)^x\to e$ $\lim_{x\to-1}\left(1+\frac1x\right)^x\to \infty$ $\lim_{x\to0}\left(1+\frac1x\right)^x\to 0$ So, I think the range should be $(0,e)\cup(e,\infty)$ . Is this correct? Major doubt: will $e$ be included in the range?","['calculus', 'functions', 'algebra-precalculus']"
4384319,"Let $w<x$ be integers with $x^2-w^2$ beeing a square. Which condition they must satisfy for a $y$ to exist with $y^2-x^2$, $y^2-w^2$ being squares?","Let $(w,x)$ be two integers with $x^2-w^2$ beeing a perfect square. One simple example is $(w,x)=(4,5)$ , since $5^2-4^2=9=\square_1=3^2$ . In this case we cannot find a third integer $y$ which satisfies that the differences $y^2-x^2=\square_2$ and $y^2-w^2=\square_3$ are again perfect squares. A working example would be $(w,x)=(264,520)$ or $(w,x)=(264,561)$ , since in both cases an integer $y=1105$ satisfies $y^2-x^2$ and $y^2-w^2$ beeing squares. My question: Does there exist a condition on $w,x$ that I may use to quickly check, whether such a third integer $y$ exist? It is quite sufficient (and very helpful) for me if this condition predicts/implies that for given $w,x$ there can be no such $y$ . It would be great, if we could determine this $y$ directly at the same time (although I guess it is almost impossible to obtain $y$ directly without bruteforce). What I investigated so far: I searched in various approaches to extract such a condition. For example I found in the paper entitled ""Cycles"" by R. C. Lyness a condition that produces cycles. For example I could generate: [[697.   153.   185.   104.   672.   680.  ]
 [697.   705.3  680.   187.2  153.   107.87]
 [697.   491.4  107.87 479.42 705.3  852.81]
 [697.   842.78 852.81 130.43 491.4  473.78]
 [697.   672.   473.78 822.22 842.78 185.  ]
 [697.   153.   185.   104.   672.   680.  ]] But it seems not to give me a condition that enables me to check whether for two given integers $w<x$ such a third integer $y$ exist. My code for producing Lyness Cycles is given below: def get_6tuple(tp:list[float]) -> list[float]:
    w = tp[0]
    x = tp[1]
    y = tp[2]
    return [y,w,x,sqrt(x**2-w**2),sqrt(y**2-x**2),sqrt(y**2-w**2)]

def rotate_np(tuple:np.ndarray) -> np.ndarray:
    p = tuple[0]
    v = tuple[4]
    w = tuple[5]
    return [p, np.float(p*w/v), w, np.float(tuple[2]*w/v), tuple[1], np.float(p*tuple[3]/v)]

def generate_cycle_np(triple:np.ndarray) -> np.ndarray:
    res = np.empty((6,6), dtype=np.float)
    tuple = np.array(get_6tuple(triple), dtype=np.float)
    for i in range (0,6):
        res[i]=tuple
        tuple = rotate_np(tuple)
    return res

print(np.array_str(generate_cycle_np([153, 185, 697]), precision=2, suppress_small=True))","['number-theory', 'square-numbers', 'elementary-number-theory', 'diophantine-equations']"
4384335,Creating a probability density function for a particular dataset [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I want to create a probability density function for a particular dataset. First of all, I calculate the mean and the variance of my dataset. So, I use the mean and the variance to create a probability density function, for example, Gaussian distribution. Is my thinking correct?","['data-analysis', 'statistics', 'probability-distributions', 'probability']"
4384343,"Let $ f: ( 0, \infty) \rightarrow ( 0, \infty)$ bijective with $ f^{-1}(x) \cdot f(x) =1$.","Let $ f: ( 0, \infty) \rightarrow  ( 0, \infty)$ bijective  with $ f^{-1}(x) \cdot f(x) =1$ .Prove that  there is an interval $ I $ such that $f(I) $ is not an interval. I choose two elements $ x, y \in ( 0, \infty)$ such that $ f(x) <1 $ and $ f(y) >1$ and I tried to send $ x \rightarrow f(x) $ of infinetely many times to obtain that $f(I) = ( 0, \infty)$ .How to try in another way?","['functional-equations', 'functions', 'inverse-function']"
4384424,"For a field $\theta \equiv \theta(t,x,y)$, is $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ for independent $x$,$y$,$t$?","Consider a field $\theta$ defined over its independent variables $x$ , $y$ and $t$ . $$\theta \equiv \theta(t,x,y) \tag{1}$$ Taking the derivative with respect to $t$ , $$\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}  +  \frac{\partial\theta}{\partial x} \frac{\mathrm{d}x}{\mathrm{d}t}  +   \frac{\partial\theta}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}t} \tag{2}$$ My colleague said that $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ because $x$ and $y$ are independent of $t$ . I wasn't too sure about that and gave the example that if we have a temperature field $\theta(t,x,y)$ defined over a two dimensional plane, equation (2) holds good but it doesn't mean anything till we define a trajectory through the plane parametrized by $x \equiv x(t)$ and $y \equiv y(t)$ . Then $\frac{\mathrm{d}x}{\mathrm{d}t}$ and $\frac{\mathrm{d}y}{\mathrm{d}t}$ take finite values and you can calculate an analytical expression for $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ . My colleague said my intuition was coming from particle mechanics (which he is right about) but for a field $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ . However, I still think that to say $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ is like defining a trajectory where you are standing at a fixed point ( $x$ , $y$ ) and evaluating $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ there. In this case, I would agree with him. However, if $x$ and $y$ are independent of $t$ , $\frac{\mathrm{d}x}{\mathrm{d}t}$ should be undefined and can't be zero because for a small change in $t$ , the corresponding change in $x$ could be anything since $x$ and $t$ are independent of each other. Can someone please shed some light on my issue regarding how do I make physical sense of $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ when $x$ and $y$ are independent of $t$ .","['partial-derivative', 'derivatives', 'ordinary-differential-equations']"
4384425,Optimal strategy in a coin game has unexpected symmetry?,"The game: I am going to toss a fair coin and you are trying to determine if I tossed a Head or Tail. You do this using the rule I follow when I toss my coin: I have before me $2$ $\color{red}{\text{red}}$ boxes each with a $\color{red}{\text{red}}$ marble inside them, I also have $2$ $\color{blue}{\text{blue}}$ boxes each with a $\color{blue}{\text{blue}}$ marble inside of them. Finally, there are two empty white boxes. If I toss a Head I must move a $\color{red}{\text{red}}$ marble from a $\color{red}{\text{red}}$ box into an empty white box. Similarly, if I toss a Tail I must move a $\color{blue}{\text{blue}}$ marble from a $\color{blue}{\text{blue}}$ box into an empty white box. To aid you in your guess of my toss, once I have moved a marble, you are then permitted to open and examine the contents of a single $\color{red}{\text{red}}$ , $\color{blue}{\text{blue}}$ or white box. The strategy: In this primitive instance of two boxes of each color, we are actually indifferent between what box we peek into. We have 75% probability of correctly guessing the toss. Consider now we have $R$ , $\color{red}{\text{red}}$ boxes, $B$ $\color{blue}{\text{blue}}$ boxes and $W$ white boxes. The optimal strategy is to peak into min{ $R,B,W$ }. I was curious as to why we have symmetry between the colored and white boxes and simply follow the heuristic of the minimum number of boxes, is there a nice transformation of the game that makes this symmetry more obvious? Thanks","['game-theory', 'statistics', 'probability']"
4384441,"Set of all real numbers in $[0,1]$ which have 5 infinitely often in decimal representation has lebesgue measure 1","I am self-studying introduction to ergodic theory and found this problem (application of Poincare recurrence theorem) which I couldn't do. I need to show the set, $E = \{x \in [0,1]: \text{decimal  representation of $x$ has $5$ infinitely often}\}$ , has measure 1. How can this be extended to show that almost all elements in $[0,1]$ contain the block (this sequence of numbers appear consecutively and in the same order) $123$ in decimal representation infinitely many times? What I did:
I know, by Poincare recurrence theorem, for almost all elements in $[0.5,0.6)$ have 5 infinitely often in the decimal representation. I am not sure how the Poincare recurrence theorem can be used to extend this to the whole of $[0,1]$ . Any hints would be helpful.","['measure-theory', 'ergodic-theory', 'analysis']"
4384461,Symmetric difference of symmetric difference,"Let $X, Y, Z \neq \emptyset$ . The set of elements that belong to exactly two of the sets $X,Y,Z$ was asked in the question and $(X\cup Y\cup Z) \setminus \left( \left(X\triangle Y\right) \triangle Z\right)$ was given as the answer
I worked this out using Venn diagrams and found this Here the unshaded portion gives the required value and we can see that this portion belongs to all 3 sets, which contradicts our assumption.
I have just begun discrete maths, and I have been sitting with this for a long time. Could you please say where I am going wrong?",['discrete-mathematics']
4384486,Is a partition of the empty set defined or not?,"Definition A partition $\Pi$ of a set $A$ is a set of nonempty subsets of $A$ that is disjoint and exhaustive, i.e., (a) no two different sets in $\Pi$ have any common elements, and (b) each element of $A$ is in some set in $\Pi$ This definition doesn't explicitly say $A$ is nonempty. And to me this is interpreted in two ways. ""nonempty subsets of $A$ "" implies $A$ is nonempty. When $A$ is the empty set some kind of vacuous true occurs(I can't make a precise argument but it seems). So empty set has a partition(but if so, what is a partition of $\emptyset$ ?).",['elementary-set-theory']
4384496,Average of an odd number of equally spaced points on a circle,"I want to show that the average of an odd number of equally spaced points on the unit circle is equal to 0. More precisely, let $n$ be an odd number, $\theta_1,\ldots, \theta_n\in[0,2\pi)$ and $$
re^{i\psi}:=\frac{1}{n}\sum_{i=1}^ne^{i\theta_i}.$$ We want to show that if the $\theta_i$ s are equally spaced then $r=0$ . I remark that I do not want to use Vieta's formulas but rather prove it ""by hand"". I was able to prove this formula for $r$ : $$r=\frac{1}{n}\left(n+2\sum_{i=1}^{n-1}\sum_{j=i+1}^n\cos(\theta_i-\theta_j)\right)^{1/2}
$$ (I wouldn't know if this is a well known formula or not...). If the angles are equally spaced, upon relabeling if necessary we have $\theta_i=\frac{2(i-1)\pi}{n},\:i=1,\ldots,n$ so that \begin{align*}
r^2 & = \frac{1}{n^2}\left(n+2\sum_{i=1}^{n-1}\sum_{j=i+1}^n\cos(\theta_i-\theta_j)\right) \\
	& = \frac{1}{n^2}\left(n+2\sum_{i=1}^{n-1}\sum_{j=i+1}^n\cos\left(\frac{2(i-j)\pi}{n}\right)\right)
\end{align*} Then we would like to show that $$\sum_{i=1}^{n-1}\sum_{j=i+1}^n\cos\left(\frac{2(i-j)\pi}{n}\right)=-\frac{n}{2}.$$ If we set $n=2k+1\:k\in\mathbb{N}$ we can rewrite the previous formula in terms of $k$ : $$\sum_{i=1}^{2k}\sum_{j=i+1}^{2k+1}\cos\left(\frac{2(i-j)\pi}{2k+1}\right)=-k-\frac{1}{2}.$$ Expanding this double sum gives \begin{align*}
\sum_{i=1}^{2k}\sum_{j=i+1}^{2k+1}\cos\left(\frac{2(i-j)\pi}{2k+1}\right) & = \cos\left(\frac{-2\pi}{2k+1}\right)+\cos\left(\frac{-4\pi}{2k+1}\right)+\ldots+\cos\left(\frac{-4k\pi}{2k+1}\right) \\
																		  & + \cos\left(\frac{-2\pi}{2k+1}\right)+\cos\left(\frac{-4\pi}{2k+1}\right)+\ldots+	\cos\left(\frac{-(4k-2)\pi}{2k+1}\right) \\
																		  &\vdots \\
																		  & + \cos\left(\frac{-2\pi}{2k+1}\right)+\cos\left(\frac{-4\pi}{2k+1}\right) \\																									  & +\cos\left(\frac{-2\pi}{2k+1}\right)
\end{align*} which can be rearranged as: $$\sum_{i=1}^{2k}\sum_{j=i+1}^{2k+1}\cos\left(\frac{2(i-j)\pi}{2k+1}\right) =2k\cos\left(\frac{2\pi}{2k+1}\right)+(2k-1)\cos\left(\frac{4\pi}{2k+1}\right)+\ldots+2\cos\left(\frac{(4k-2)\pi}{2k+1}\right)+\cos\left(\frac{4k\pi}{2k+1}\right).$$ Then, my question is the following: is it true that $$ 2k\cos\left(\frac{2\pi}{2k+1}\right)+(2k-1)\cos\left(\frac{4\pi}{2k+1}\right)+\ldots+2\cos\left(\frac{(4k-2)\pi}{2k+1}\right)+\cos\left(\frac{4k\pi}{2k+1}\right)=-k-\frac{1}{2}?$$ I checked it for some values of $k$ and try induction for the general case, but I couldn't get very far. Also, as I mentioned, one can prove the result using Vieta's formula but it seems that one should be able to prove it this way as well.","['trigonometry', 'circles', 'geometry', 'complex-numbers']"
4384525,"If I have two periodic functions so that $X = X(t)$ and $Y = X(t+\alpha)$, with $\alpha$ unknown is there a way to extract 𝛼 from $X-Y$?","If I have two periodic functions so that $X = X(t)$ and $Y = X(t+\alpha)$ , with $\alpha$ unknown is there a way to extract the value of $\alpha$ just from the difference of the two functions $X-Y$ ? $X$ and $Y$ are not necessarily sine or cosine, but whatever periodic function.",['functions']
4384540,"Proving that if $f$ is continuous on $\mathbb{R}$ and that the image of $f$ is a subset of $\mathbb{R}$\ $\mathbb{Q}$, then $f$ is a constant function","I was asked to prove that for a function $f$ that is continuous on $\mathbb{R}$ , whose image is a subset of $\Bbb{R}$ \ $\Bbb{Q}$ , $f$ is a constant function. My proof goes as follows: Suppose for a contradiction that f is not constant, so there exists $x_1$ $\neq$ $x_2$ so that $f(x_1)$ $\neq$ $f(x_2)$ , say, $x_1$ , $x_2$ $\in$ $\Bbb{Q}$ .
Now, f is continuous at c for every c $\in$ $\Bbb{R}$ , so $f_{[x_1,x_2]}$ :[ $x_1$ , $x_2$ ] $\to$ $\Bbb{R}$ is continuous at c for every c $\in$ [ $x_1$ , $x_2$ ]. Consider $v \in \Bbb{Q}$ and $v \in (f(x_1),f(x_2))$ , which exists by completeness of the set of real numbers.
There exists $x_0 \in (x_1,x_2)$ such that $f(x_0)=v \in \Bbb{Q}$ , by the intermediate value theorem.
A contradiction, since the image of $f$ is a subset of $\Bbb{R}$ \ $\Bbb{Q}$ . So $f$ is a constant function. $\square$ I feel there is an easier way, to prove this; nevertheless, what do you think of my proof, does it hold ?
Thank you!","['continuity', 'solution-verification', 'analysis']"
4384564,Suppose $E$ is a smooth vector bundle over $M$. Show that the projection map $\pi :E \to M$ is a surjective smooth submersion.,Suppose $E$ is a smooth vector bundle over $M$ . Show that the projection map $\pi :E \to M$ is a surjective smooth submersion. Since $E$ is a smooth vector bundle for each $p \in M$ there exists a neighborhood $U$ of $p$ such that $\varphi: \pi^{-1}(U) \to U \times \Bbb R^k$ is a diffeomorphism and that $\pi_1 \circ \varphi = \pi$ . In order for $\pi$ to be a smooth submersion the differential $d\pi_p : T_p E \to T_{\pi(p)} M$ must be surjective. Since we have the characterization $\pi_1 \circ \varphi = \pi$ we get that $$d\pi_{p} = d(\pi_1 \circ \varphi)_p = d\pi_{1_{\varphi(p)}} \circ d\varphi_{p}.$$ Now I need to show that $d\pi_{1_{\varphi(p)}}$ and $d\varphi_{p}$ surjective maps in order to conclude the proof. Since $\varphi$ is a diffeomorphism we have that $\varphi^{-1}$ is a smooth bijection. So defining $d\varphi^{-1}_{q} : T_{q}(U \times \Bbb R^k) \to T_p \pi^{-1}(U)$ we have that $$d\varphi_p \circ d\varphi^{-1}_{q} = d(\varphi \circ \varphi^{-1})_{q} = d(id)_{q}$$ so $d\varphi^{-1}_{\varphi(p)}$ is an inverse for $d\varphi_p$ and so it's bijective and in particular surjective. Similarly for $d\pi_{1_p}:T_p(U \times \Bbb R^k) \to T_{\pi_1(p)}U$ define $d\iota_{U_q} : T_qU \to T_{\iota_U(q)}(U \times \Bbb R^k)$ where $\iota_U:U \to U \times \Bbb R^k$ is the right inverse of $\pi_1$ . Then we get that $$d\pi_{1_p} \circ d\iota_{U_q} = d(\pi_1 \circ \iota_U)_q=d(id)_q$$ which by same reasoning makes $d\pi_1$ surjective. Is there some problems with the proofs for the surjectivities for these differentials? I'm not very confident about them.,"['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4384624,"If $f:\mathbb{R}\to\mathbb{R}$ is a differentiable function with $x^2f'(x)\to 0$ as $x\to\infty,$ then does $f(x)\to c\in\mathbb{R}$ as $x\to\infty$?","If $f:\mathbb{R}\to\mathbb{R}$ is a differentiable function with $xf'(x)\to 0$ as $x\to\infty,$ then it is not true that $f(x)\to
c\in\mathbb{R}$ as $x\to\infty. $ For example, take $f(x) = \log(\log(x)).$ But I cannot figure out the following: If $f:\mathbb{R}\to\mathbb{R}$ is a differentiable function with $x^2f'(x)\to 0$ as $x\to\infty,$ then is it true that $f(x)\to
c\in\mathbb{R}$ as $x\to\infty $ ? I'm not certain it is true but cannot think of a counter-example either. I have tried three different methods, but got nowhere: Integration by parts: $$\int_{a}^{\infty} x^2 f'(x) dx = \left[ x^2 f(x) \right]_{a}^{\infty} - \int_{a}^{\infty} 2x f(x) dx $$ but I don't see where to go from here. In fact, I'm pretty sure this is the wrong route. 2. $$ x^2 f'(x)\to 0 \implies \lim_{x\to \infty}\left( x^2 \lim_{h\to 0} \frac{f(x+h)-f(x)}{h} \right)$$ I'm not sure how we can manipulate this to help us. Given $\varepsilon>0,\ \exists\ \gamma\ $ s.t. $\ \vert x^2 f'(x)\vert < \varepsilon\ \forall x>\gamma, \implies \vert f'(x) \vert <\frac{\varepsilon}{\gamma^2}\ \forall x> \gamma.$ But now what?","['logarithms', 'real-analysis', 'limits', 'derivatives', 'problem-solving']"
4384632,"How do we find the exact value of $\int_{0}^{\infty} \frac{\ln ^{n}\left(1+x^{2}\right)}{1+x^{2}} d x$, where $n\in \mathbb N?$","Latest Edit By @KStarGamer’s help, I can finally find a reduction formula for $I_n$ as below: $$\boxed{I_n= 2 \ln 2 I_{n-1}+ (n-1)!\sum_{k=0}^{n-2} \frac{2^{n-k}-2}{k!}\zeta(n-k)  I_k}
$$ where $n\geq 2.$ In my post , I started to evaluate $$I_1=\int_{0}^{\infty} \frac{\ln \left(1+x^{2}\right)}{1+x^{2}} d x =\pi \ln 2, $$ then I challenge myself on $$I_2=\int_{0}^{\infty} \frac{\ln ^{2}\left(1+x^{2}\right)}{1+x^{2}}dx$$ Again, letting $x=\tan \theta$ as for $I_1$ yields $$I_2=\int_{0}^{\frac{\pi}{2}} \ln ^{2}\left(\sec ^{2} \theta\right) d \theta= 4 \int_{0}^{\frac{\pi}{2}} \ln ^{2}(\cos x) dx $$ It’s very hard to deal with $\ln^2$ and I was stuck. Suddenly a wonderful identity came to my mind. $$
2\left(a^{2}+b^{2}\right)=(a+b)^{2}+(a-b)^{2},
\\$$ by which $\displaystyle 2\left[\ln ^{2}(\sin x)+\ln ^{2}(\cos x)\right]=[\ln (\sin x)+\ln (\cos x)]^{2}+[\ln (\sin x)-\ln (\cos x)]^{2} ,\tag*{}\\ $ we have $\displaystyle 4 L=\underbrace{\int_{0}^{\frac{\pi}{2}} \ln ^{2}\left(\frac{\sin 2 x}{2}\right)}_{J} d x+\underbrace{\int_{0}^{\frac{\pi}{2}} \ln ^{2}(\tan x) d x}_{K} \tag*{}\\ $ For the first integral, using $ \int_{0}^{\frac{\pi}{2}} \ln (\sin x) d x=-\dfrac{\pi}{2} \ln 2 $ yields $ \begin{aligned}J &=\int_{0}^{\frac{\pi}{2}}[\ln (\sin 2 x)-\ln 2]^{2} d x \\&=\int_{0}^{\frac{\pi}{2}} \ln ^{2}(\sin 2 x) d x-2 \ln 2 \int_{0}^{\frac{\pi}{2}} \ln (\sin 2 x) d x +\frac{\pi \ln ^{2} 2}{2} \\& \stackrel{x\mapsto 2x}{=} \frac{1}{2} \int_{0}^{\pi} \ln ^{2}(\sin x) d x-\ln 2 \int_{0}^{\pi} \ln (\sin x) d x+\frac{\pi \ln ^{2} 2}{2} \\& \stackrel{symmetry}{=} L-\ln 2(-\pi \ln 2)+\frac{\pi \ln ^{2} 2}{2} \\&=L+\frac{3 \pi \ln ^{2} 2}{2}\end{aligned}\tag*{} \\$ For the second integral, letting $ y=\tan x $ and using my post yields $ \displaystyle K=\int_{0}^{\infty} \frac{\ln ^{2} y}{1+y^{2}} d y=\frac{\pi^{3}}{8}, \tag*{} \\$ then $ \displaystyle 4L=L+\frac{3 \pi \ln ^{2} 2}{2}+\frac{\pi^{3}}{8} \Rightarrow L=\frac{\pi^{3}}{24}+\frac{\pi \ln ^{2} 2}{2}\tag*{} $ Hence $ \displaystyle \boxed{I_2=4L= \frac{\pi^{3}}{6}+2 \pi \ln ^{2} 2} \tag*{} $ My Question : Can I go further with $I_n$ , where $n\geq 3$ ?","['integration', 'definite-integrals', 'logarithms', 'calculus', 'trigonometry']"
4384643,"How is calculated the frequency of a pair, a flush of 2 cards, a straight of 2 cards and a straight flush of 2 cards (52 cards, hand of 2, 3, 4, etc)?","Introduction Before starting, I would like to specify a few things: The deck has 52 cards. From 1 (Ace) to 10, J, Q, K, all in four different suits: spades, diamonds, club and hearts. I don't add jokers because that is far beyond my possibilities of calculations. For better precision, I won't use chance, instead I would use combinations or frequency / total possible combinations. This is based on poker, but it isn't exactly poker. I mean, for example, the frequency on a pair in 5-cards poker is $2860={13 \choose 1}{4 \choose 2}{12 \choose 3}{4 \choose 1}^3$ , this formula that can be found on Wikipedia already excludes the chance of having not two but three or four cards with the same rank (three or four of a kind) and also excludes the chance of a full house (three of a kind + pair). What I want to discover, is not the frequency of a pair, instead, is the frequency of having at least two cards of the same rank. I'm not using the rules of poker, so I don't want to remove from the frequency the combinations that are ""better"". For example, the hand Q Q Q J J, which normally would be a full house, for this calculation I want it to be included in the ""pair"" calculation, because if you have three Qs, that means that you have at least a pair of Q, same with the Js. What do I want? What I want to calculate is the frequency of drawing: At least two cards of the same rank (a pair). At least two cards of the same suit (a flush of 2). At least two consecutive cards (a straight of 2). At least two consecutive cards of the same suit (a straight of 2 flushed). All the previous frequencies I wanted to calculate them with: A 2-card hand. A 3-card hand. A 4-card hand. A 5-card hand. A 6-card hand. A 7-card hand. My Calculations Before starting to show my math I will show you how I got it. At wikipedia, the chance of a four of a kind if a 5-card deck is $642={13 \choose 1}{4 \choose 4}{12 \choose 1}{4 \choose 1}$ . I understand this as choose one of 13 ranks, then choose all four suits of that rank, for the last card choose one other rank and then one suit for that card. Then I tried to recreate the formula with different reasoning. $642={13 \choose 1}{4 \choose 4}{52-4 \choose 1}$ which I understand as choose one of 13 ranks, then choose all four suits of that rank, finally choose one card other than the four previously used . Below I will show you a table with my math. Hand 2 cards 3 cards 4 cards 5 cards 6 cards 7 cards Total combinations $1.326={52 \choose 2}$ $22.100={52 \choose 3}$ $270.725={52 \choose 4}$ $2.598.960={52 \choose 5}$ $20.358.520={52 \choose 6}$ $133.784.560={52 \choose 7}$ Pair $78={13 \choose 1}{4 \choose 2}$ $3.900={13 \choose 1}{4 \choose 2}{52-2 \choose 1}$ $95.550={13 \choose 1}{4 \choose 2}{52-2 \choose 2}$ $1.528.800={13 \choose 1}{4 \choose 2}{52-2 \choose 3}$ $17.963.400={13 \choose 1}{4 \choose 2}{52-2 \choose 4}$ $165.263.280={13 \choose 1}{4 \choose 2}{52-2 \choose 5}$ Flush of 2 $312={13 \choose 2}{4 \choose 1}$ $15.600={13 \choose 2}{4 \choose 1}{52-2 \choose 1}$ $382.200={13 \choose 2}{4 \choose 1}{52-2 \choose 2}$ $6.115.200={13 \choose 2}{4 \choose 1}{52-2 \choose 3}$ $71.856.600={13 \choose 2}{4 \choose 1}{52-2 \choose 4}$ $661.053.120={13 \choose 2}{4 \choose 1}{52-2 \choose 5}$ Straight of 2 $208={13 \choose 1}{4 \choose 1}^2$ $10.400={13 \choose 1}{4 \choose 1}^2{52-2 \choose 1}$ $254.800={13 \choose 1}{4 \choose 1}^2{52-2 \choose 2}$ $4.076.800={13 \choose 1}{4 \choose 1}^2{52-2 \choose 3}$ $47.902.400={13 \choose 1}{4 \choose 1}^2{52-2 \choose 4}$ $440.702.080={13 \choose 1}{4 \choose 1}^2{52-2 \choose 5}$ Straight of 2 flushed* $52={13 \choose 1}{4 \choose 1}$ $2.600={13 \choose 1}{4 \choose 1}{52-2 \choose 1}$ $63.700={13 \choose 1}{4 \choose 1}{52-2 \choose 2}$ $1.019.200={13 \choose 1}{4 \choose 1}{52-2 \choose 3}$ $11.975.600={13 \choose 1}{4 \choose 1}{52-2 \choose 4}$ $110.175.520={13 \choose 1}{4 \choose 1}{52-2 \choose 5}$ *At 12 cards $27.917.689.800={13 \choose 1}{4 \choose 1}{52-2 \choose 10}$ while the total number is $15.820.024.220={52 \choose 12}$ My problem is that as you can see, when you draw multiple hands, the combinations of for example a pair becomes higher than the total possible combination, a thing that is absurd and can be easily demonstrated with an example hand: 1, 2, 3, 4, 5, 6, 7, there are 7 different cards, none is a pair. Of course, a flush of 2 cards is allowed to get 100% if you draw 5 cards (it is impossible to get 5 different suits because there only exist 4 different); however, it should not reach 100% in a 4-card hand (an easy example would be drawing a spade, a club, a diamond and a heart, none is repeated). Question: What am I missing with my calculations? As my method of calculation of the extra cards after forming what I want (for example, the three cards remaining after having a pair using 5 cards) I used ${52-2 \choose 3}$ instead of what appears in Wikipedia, I made an experiment trying to get the same results of Wikipedia with my method. The frequency of three of a kind in Wikipedia is $54.912={13 \choose 1}{4 \choose 3}{12 \choose 2}{4 \choose 1}^2$ which I understand as choose one card of the 13 ranks, then choose three of the four suits for that card, finally choose 2 cards that aren't from the previous rank, these cards each can be of any suit . If I use ${13 \choose 1}{4 \choose 3}{52-4 \choose 2}=58.656$ I have to eliminate the frequency in which that two extra cards are a pair, because that would fall into Full House from normal poker (following Wikipedia), so $58.656-3.744=54.912$ which is fine. Now I go one step further. If I use ${13 \choose 1}{4 \choose 3}{52-3 \choose 2}=61.152$ I have to eliminate the frequency in which that two extra cards are a pair (because of full house), and the frequency in that the remaining card of the suit appears and transform my three of a kind into a four of a kind $61.152-3.744-624=56.784$ which isn't 54.912. I discovered that if I multiply (in the previous formula) the frequency of a full house by 1.5, or I multiply the frequency of a four of a kind by 4 I get exactly 54.912, but I'm not sure why, and that might be what makes my math surpass the 100% (when comparing the frequency/total possible combinations). Do you have any idea what I'm doing wrong? What would be considered an answer? An explanation of which mathematical expression I am understanding wrong, a correct expression to replace that part, an explanation of what means that suggested new expression and one example using any of the combinations that appear in my table above that surpass the 100% using that new suggested expression. For example: ""When you say this expression ... you are wrong because that means instead ... What you have to use is ... because that means ... For example, if we use this formula to get the frequency of a pair with a 7-cards hand (in which your math surpassed 100%), we get ... which is correct because ..."" Note: I'm not a math teacher, and I'm not studying for a math career, so please try to not be too much complicated in the explanation. I just know combinations because at university we had 3 classes of hypergeometric distributions (I'm studying business administration). This is not for the university, work or gambling, actually, it is for Dungeons & Dragons 5e.","['poker', 'combinatorics', 'card-games', 'probability']"
4384690,$f(f(x - y)) = f(x) f(y) - f(x) + f(y) - xy$,"Let $f : \mathbb{R} \to \mathbb{R}$ be a function such that $$f(f(x - y)) = f(x) f(y) - f(x) + f(y) - xy$$ for all $x,$ $y.$ Find the sum of all possible values of $f(1).$ I tried plugging in combinations of $0$ 's and $1$ 's, but it didn't really get me anywhere. I would appreciate any help!! Thanks!!","['functional-equations', 'algebra-precalculus', 'functions']"
