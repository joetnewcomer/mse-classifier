question_id,title,body,tags
2316202,"What are some good, rigorous precalculus textbooks for self-study?","Books that: has an introduction to proof, logic, and topics like sets and groups. Books can prepare you for rigorous calculus texts like Spivak and Apostol. Question 1. 
I've been looking at the books by Gelfand (Algebra, Trigonometry, Functions and Graphs, The Method of Coordinates). I'm not sure if they cover all of the precalculus curriculum, though. Question 2.  What would be some good calculus books other than Spivak and Apostol?","['self-learning', 'reference-request', 'algebra-precalculus', 'book-recommendation', 'education']"
2316231,prove that $ x-\frac{1}{6}x^3<\sin(x)<x-\frac{1}{6}x^3+\frac{1}{120}x^5 $ for some deleted neighborhood of $x=0$,"prove that : 
  there exists a deleted neighborhood of $x=0$
  such that :
  $$
x-\frac{1}{6}x^3<\sin(x)<x-\frac{1}{6}x^3+\frac{1}{120}x^5
$$ MyTry: let:$$f(x):=\sin x-x+\dfrac{1}{6}x^3$$
And : $$g(x):=\sin x-x+\dfrac{1}{6}x^3-\dfrac{1}{125}x^5$$ Now what ?","['derivatives', 'inequality', 'trigonometry', 'calculus']"
2316334,"If I have a $0.00048\%$ chance of dying every second, how to numerically calculate the chance I have of dying in a day?","Hypothetically, if I have a 0.00048% chance of dying when I blink, and I blink once a second, what chance do I have of dying in a single day? I tried $1-0.0000048^{86400}$ but no calculator I could find would support this.  How would I work this out manually?",['probability']
2316338,Weak convergence in $L^p$ space.,"Let $p\in [1,\infty [$ and $\Omega \subset \mathbb R^n$ an open. Let $u_n,\in L^p$. We say that $u_n\rightharpoonup u$ (weak convergence) if $$\lim_{n\to \infty }\int_\Omega (u_n-u)\varphi=0$$
for all $\varphi\in L^{p'}$ (the dual of $L^p$). Q1) First, what is a the intuition behind this definition ? Q2) How can I bu sure that $(u_n-u)\varphi\in L^1$ ?In other word that $\int (u_n-u)\varphi$ exist for all $n$.","['functional-analysis', 'lp-spaces']"
2316340,Example of product space isomorphic to sum of subspaces,"Here is the problem statement (from chapter $3$ of Axler's Linear Algebra Done Right ). Give an example of a vector space $V$ and subspaces $U_1,U_2$ of $V$ such that $U_1 \times U_2$ is isomorphic to $U_1 + U_2$ , but $U_1 + U_2$ is not a direct sum. Hint: the vector space $V$ must be infinite-dimensional. The only infinite-dimensional vector spaces mentioned in the book up to this point are $\mathbb{F}^{\infty}$ and $P(\mathbb{R})$ . I tried to construct an example using $\mathbb{F}^{\infty}$ by letting $U_1$ be the span of the standard bases $\{e_{2n}\}$ and $U_2$ the span of $\{e_{2n-1}\}$ . It seems that at least one of the $U_i$ must be infinite dimensional, or else the example could be constructed with $V$ finite dimensional. There is an isomorphism between $U_1 \times U_2$ and $U_1 + U_2$ , but $U_1 \cap U_2 = \{0\}$ so we have a direct sum, which is what we're trying to avoid. I'm not sure how to proceed from here.",['linear-algebra']
2316367,Is $f$ differentiable at $0$?,Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}$ is a continuous function which is differentiable on all of $\mathbb{R}_{0}$ and suppose that $\lim_{x \rightarrow 0}f'(x)$ exists and is finite. I was wondering if you could then conclude that $f$ is differentiable on $0$ ?,"['derivatives', 'real-analysis', 'continuity']"
2316368,"Given any two non zero vectors $x,y$ does there exist symmetric matrix $A$ such that $y = Ax$?","Let $x,y$ be non-zero vectors from $\mathbb{R}^n$. Is it true, that there exists symmetric matrix $A$ such that $y = Ax$. I was reasoning the following way. Having an equation $y = Ax$ for some symmetric matrix $A$ is equivalent to being able to solve a system of $n + \frac{n^2-n}{2}$ linear equations: $$\begin{cases} a_{11}x_1 + ... + a_{1n}x_n = y_1 \\ \vdots \\ a_{n1}x_1 + ... + a_{nn}x_n = y_n \\ a_{12} = a_{21} \\ \vdots \\ a_{n,n-1} = a_{n-1,n} \end{cases}$$ First $n$ equations are our constraints for identity $y = Ax$ to be true and the following $\frac{n^2-n}{2}$ for symmetricity of $A$. So, in our system we have $n^2$ variables and $\frac{n^2-n}{2} + n = \frac{n^2+n}{2}$ equations. As $n^2 \geq \frac{n^2+n}{2}$ such system is always solvable. That's why such matrix $A$ always exists. This solution was marked as wrong on exam, where did I make a mistake?","['matrices', 'linear-algebra', 'proof-verification']"
2316376,Proof for $A^{-1}=\frac{adj(A_j)}{det(A)}$,"I have a proof for the method to find the inverse by taking the adjugate of the matrix over the determinant. I believe that it is correct; however, I would like to have it checked over before I submit it for bonus marks. The proof goes as follows: Let $A\in\mathbb{M}_{n\times{n}}(\mathbb{R})$ be an $n\times{n}$ invertible matrix. Let $A^{-1}=\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}$. $AA^{-1}=A\begin{bmatrix}\vec{x_1}\cdots\vec{x_n}\end{bmatrix}=\begin{bmatrix}A\vec{x_1}\\\vdots\\A\vec{x_n}\end{bmatrix}=\begin{bmatrix}\vec{e_1}^T\\\vdots\\\vec{e_n}^T\end{bmatrix}$. $\therefore A\vec{x_i}=\vec{e_i}^T\text{for } 1\leq{i}\leq{n}$ By Cramer's Rule, $x_{ji}=\frac{det(A_j)}{det(A)}\text{for }1\leq{i,j}\leq{n}$ where $A_j$ is the matrix formed by replacing the j th column of $A$ with the solution vector; in this case, $\vec{e_i}$. By properties of determinants, we can add a multiple of a column to another column without changing the determinant. $\therefore det(A_j)\\=det(\begin{bmatrix}\vec{a_1}\cdots\vec{a_{j-1}}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}\cdots\vec{a_n}\end{bmatrix})\\=det(\begin{bmatrix}\vec{a_1}-a_{1i}\vec{e_i}\cdots\vec{a_{j-1}}-a_{j-1,i}\vec{e_i}\space\space\space\space\vec{e_i}\space\space\space\space\vec{a_{j+1}}-a_{j+1,i}\vec{e_i}\cdots\vec{a_n}-a_{ni}\vec{e_i}\end{bmatrix})$ This matrix has all 0s in the i th row except for a 1 in the j th column. Thus, if we expand $det(A_j)$ along the i th row, we get: $det(A_j)\\=0\space det(A_{i,1})\times(-1)^{i+1}+\cdots+1\space det(A_{i,j})\times(-1)^{i+j}+\cdots+0\space det(A_{i,n})\times(-1)^{i+n}\\=det(A_{i,j})\times(-1)^{i+j}$ where $A{i,j}$ is the matrix formed by removing the i th row and j th column of A. $\therefore x_{ji}=\frac{det(A_j)}{det(A)}$ $\therefore A^{-1}=\frac{adj(A_j)}{det(A)}$ QED My questions are:
- Can I use Cramer's rule in the proof?
- Is this proof correct?
- Are there any terminology issues or blatant inconsistencies/errors? Thanks.","['matrices', 'linear-algebra', 'proof-verification']"
2316413,Diagonal group action on a product manifold: when is the quotient a product manifold?,"Suppose $M$ and $N$ are manifolds, each acted on freely by a group $G$. If it helps, I'm happy to assume $M$ and $N$ are compact and $G$ is a finite group (or a compact Lie group). Consider the ""diagonal"" action of $G$ on the product manifold $M \times N$, defined by $g \cdot (m, n) := (g \cdot m, g \cdot n)$. What can be said about the topology of the quotient? In particular, when is $(M \times N)/G$ homeomorphic to a product of manifolds, and when is it homeomorphic to either of the products $M/G \times N$ and/or $M \times N/G$? I suppose the Künneth theorem gives necessary conditions for the quotient to be a product. Are there
  other necessary and/or sufficient conditions? Can more be said if $M = N$ and the action of $G$ is the same on both? If not much can be said in general, I am especially interested in spheres and lens spaces. For simplicity, let's say $M$ and $N$ are spheres and $G = \mathbb{Z}_2$ consists of the antipodal map and the identity (so then $M/G$ and $N/G$ are each a real projective space). For example, suppose $M = S^3$ and $N = S^5$. I think the Künneth theorem shows that $\mathbb{RP}^3 \times S^5$ and $S^3 \times \mathbb{RP}^5$ have different integral homology groups, so they are not homeomorphic. So in this case, $(M \times N)/G$ cannot be homeomorphic both to $M/G \times N$ and to $M \times N/G$. We could also consider spheres of the same dimension. For example, if $M = N = S^1$, then I think (from drawing a picture) that the quotient is homeomorphic to the torus $S^1 \times S^1$. (But maybe this is just luck due to the low dimension. Note $\mathbb{RP}^1 = S^1/\mathbb{Z}_2$ is just $S^1$ again.) If $M = N = S^3$, is the quotient homeomorphic to $\mathbb{RP}^3 \times S^3$? What if only one of the factors is $S^1$?","['differential-topology', 'group-actions', 'algebraic-topology', 'geometric-topology', 'general-topology']"
2316422,Boxes and papers probability,"Can you please help me with this problem? There are $2n$ boxes labeled with numbers $1, 2, \cdots, 2n$ and $2n$ papers also labeled $1, 2, \cdots, 2n$. If we put papers in boxes (one paper in each box) and add their numbers (sum of paper number and box number), what is the probability that: a) the sum of each paper/box pair is even? b) the sum of exactly two paper/box pairs is even? c) the sum of at least two paper/box pairs is even? Now, I have done the following: a) Let's say we put the $i^{th}$ paper in the $i^{th}$ box, pairs will be $(1,1),(2,2),\cdots,(2n,2n)$ where left coordinate is paper and right coordinate is box. There are $n!$ ways of permuting odd-labeled papers and $n!$ ways of permuting even-labeled papers. The total number of possible events is (2n)!, so the probability of having only even paper/box pairs equals $\frac{(n!)^2}{(2n)!}$. I have trouble finding the solutions for b) and c). If anyone could provide me with a detailed explanation, I would be very grateful. According to the solution, the answer to c) equals 1 - the solution for a). I don't see a connection between the two, so why is this?","['combinatorics', 'probability', 'discrete-mathematics']"
2316434,"$a_1+a_2+…+a_n=2008$ where all $a_i$ are positive integers. If $A_k = a_1 a_2 … a_k$ , what is the largest possible value of $A_1+A_2+…+A_n$?","The sum of a set of positive integers = $2008$. $a_1+a_2+…+a_n=2008$ where all $a_i$ are positive integers. If $A_k=a_1 a_2 … a_k$ , what is the largest possible value of $A_1+A_2+…+A_n$ ?",['combinatorics']
2316443,Why is there a square-root in the central limit theorem?,"I'm having trouble understanding the derivation of the central limit theorem. From my understanding it says $$\DeclareMathOperator{\Var}{\mathrm{Var}}
\overline{X_n} \rightarrow N\left(EX_1, \frac{\Var(X_1)}{n}\right)$$ My understanding of this is that since when $n \rightarrow \infty$ the sampling mean $\overline{X_n} = EX_1$ , the above basically ensures that the variance of the normal distribution is zero, and thus the distribution is just 100% probability at $EX_1$ . Now another form I've seen is $$\sqrt{n}\frac{(\overline{X_n} - EX_1)}{\sqrt{\Var(X_1)}} = \frac{(\overline{X_n} - EX_1)}{\sqrt{\Var(\overline{X_n})}} \rightarrow N(0,1)$$ Now I understand that subtracting $EX_1$ changes $\mu$ to $0$ in the normal distribution, but I don't understand how we got the $\sqrt{\Var(\overline{X_n})}$ . In the first example as $n \rightarrow \infty$ we're getting something completely different than $N(0, 1)$ . At the same time, in the second example we somehow magically got a square root in the denominator. Wouldn't just dividing by $\Var(X_1)$ give a variance of 1? Sorry if this is confusing, but I've found so many sources that all say something a little bit different, or in a different way, and almost none of them give any kind of explanation.","['probability-theory', 'central-limit-theorem']"
2316450,Classification of Finite Rotation Groups - Problem understanding the proof,"I'm working through a proof of the classification of finite rotation groups in the Euclidean Space (i.e. finite subgroups of $SO_3$) and am not understanding a particular step. My proof is from M.A. Armstrong's Groups and Symmetry in case anybody has it available, but the particular step I'm struggling with is also executed the same way in this paper . In case (d) (the icosahedral case - bottom of page 14 in the PDF linked here and bottom of p.108 / top of p.110 in Armstrong), it is claimed that we can find $u,v$ in the orbit of $z$ such that the following holds: $ 0 < \lVert z -u \rVert < \lVert z -v \rVert < 2. $ Now it is clear that the only point with distance $2$ from $z$ is $-z$, but what I don't understand is why all the points in $G(z) \setminus \{z,-z\}$ can't have the same distance from $z$. Can anybody clarify?","['symmetry', 'group-theory']"
2316463,Was a nascent inverse function theorem known to Newton?,"More specifically, was Newton aware that given an inverse pair of functions $f$ and $h$ such that $$f(h(x)) = x = h(f(x))$$ about the origin that, for $$(x,y)=(h(y),f(x)),$$ the derivatives satisfy $$f^{'}(x) = 1/h^{'}(y)$$ or $$dy/dx = 1/(dx/dy)$$ near the origin? Heuristically, this follows symbolically from $$dy = f^{'}(x)dx = f^{'}(x)h^{'}(y)dy, $$ or, equivalently, from the chain rule applied to the top equation. And it follows geometrically for a function whose graph lies in the first quadrant by reflection through the bisector of the first quadrant, the line $y=x$ . Clearly, the slope for any tangent line is inverted by the reflection just as displacements along the $x-$ axis and the $y-$ axis are interchanged. In fact, it follows directly from the tangent line perspective since $$ y = m \; x + b$$ and $$y = \frac{1}{m}(x-b)$$ describe an inverse pair. Surely, with Newton's mastery of geometric calculus, he was aware of these relationships. Is there evidence of this in Newton's work? Related MO-Q by Ziegler. Cross-posted from this MO-Q . Edit 6/12/17: An example of a calculation incorporating the IFT that would have been obvious to Newton and plausible for him to have performed if only as a simple check of his general formulas: It was known well before Newton that $$\frac{d\tan(x)}{dx} = 1+ \tan^2(x),$$ or, with $y = \tan(x)$ , $$\frac{dy}{dx} = 1+ y^2.$$ In terms of fluxions and fluents, this could be put in the form of Newton's implicit function $$g(x,y,\dot{x},\dot{y})=\dot{y}-(1+y^2)\dot{x}=0.$$ Then $$\frac{\dot{x}}{\dot{y}}= \frac{1}{1+y^2}=\frac{dx}{dy}, $$ and application of the binomial theorem and integration would give the series $$
\arctan(y) = x = y - \frac{y^3}3+\frac{y^5}5-\frac{y^7}7+\dots.
\tag3
$$ Newton could then have derived a series expression for $\tan(x)$ using his series reversion formula (see Ferraro ) for finding the series for the compositional inverse of a function from its power series. In fact, the same procedure is applied to finding a series for $\sin(x)$ in Ferraro on pages 76-78 following an alleged reconstruction by Horsley of Newton's derivation of the series. Edit (Apr 10, 2018): According to the Wikipedia article on the chain rule, both Newton and Leibniz were aware of the chain rule, and the inverse function theorem in its simplest form follows from application of the chain rule to $x = f(f^{-1}(x))$ . This would provide an easy check for the veracity of the chain rule that someone as fastidious as Newton would have used.","['real-analysis', 'math-history', 'reference-request', 'calculus', 'differential-geometry']"
2316487,What is the first fundamental form?,"The first fundamental form of a surface $S$ at a point $p$ is ""the quadratic form on the tangent plane $S_p$ inherited from the inner product structure of $\mathbb R^3$"". At the same time, the first fundamental form is apparently supposed to describe the surface at $p$ in some way, for instance you can compute Gaussian curvature from it. In particular, the first fundamental form should be different for different surfaces $S$. So what is the first fundamental form? It can't be the actual quadratic form $\langle x, x\rangle$ on $S_p$, that is, it can't simply be a function from $\mathbb R^3$ into $\mathbb R$, because that's the same for any surface $S$ that has the same tangent plane at $p$. It can't be the triplet of coefficients $(E, F, G)$ either, since they depend on the parameterization used, and in fact I think by using the right parameterization we can get any triplet $(E, F, G)$ we want.",['differential-geometry']
2316555,The inverse of a block-upper triangular matrix,"Is it true that $$\begin{pmatrix} A & * \\  0 &  B  \\  \end{pmatrix}^{-1} = \begin{pmatrix}  A^{-1}  & * \\  0 &  B^{-1} \\  \end{pmatrix}$$ where $A$ and $B$ are $m \times m$ and $n \times n$ invertible, and * is for  unspecified blocks ?","['matrices', 'block-matrices', 'inverse']"
2316561,How to evaluate integral $\frac{1}{2 \pi i}\int \limits_{c-i \infty}^{c+i \infty} \frac{ds}{s(1-q^{1-s})}\text{?}$,"How to evaluate the integral $$\frac{1}{2 \pi i}
\int \limits_{c-i \infty}^{c+i \infty} \frac{ds}{s(1-q^{1-s})}\text{?}$$ I tried with Perron's formula but I couldn't solve it. The result of the integral is $\frac{1}{2}$. Can someone help please?!",['complex-analysis']
2316564,The conditional expectation of an almost surely positive random variable,"I am trying to prove this claim. Short version: Let $\ X$ be an almost surely positive random variable (i.e. $\ X > 0$ a.s.) defined on the probability space $\ (\Omega, \mathcal G, P)$. Let $\mathcal F$ be a sub $\sigma$-algebra of $\mathcal G$, then 
$\ Y =  E[X|\mathcal F] > 0$ a.s. Long version: Let $\mathcal F(t), 0\le t \le T, $ be a filtration. Define $\ V(t) =  E[V(T)\ exp{(-\int_t^T R(u)du)}\ |\mathcal F(t)]$, and assume $\ V(T)$ is almost surely positive, $\ R(t)$ is an adapted process, We are asked to show that $\ V(t)$ is almost surely positive. If one defines $\ X = V(T)\ exp{(-\int_t^T R(u)du)} $, and given that $\ V(T)$ and $\ exp{(-\int_t^T R(u)du)}$ are almost surely positive random variables, we obtain the short version.","['probability-theory', 'conditional-expectation', 'probability', 'stochastic-calculus']"
2316646,Oscillator that only responds if two frequencies are present,"I'm looking for a differential equation that responds to an driving force and which only responds if the driving force is a superposition of two frequencies? As a counterpoint, a harmonic oscillator will respond most when it is driven by a force which has the same frequency as the oscillator. That is, when the force is in resonance with the oscillator. I'd like to know if there is a differential equation which involves a driving force which for which the solution has the largest response when the driving force is the sum of two frequencies, such as $sin{\omega_1t} + sin{\omega_2t}$ but not when any one of the frequencies is present without the other. So for instance the solution would not respond much to $sin{\omega_1t}$ or $sin{\omega_2t}$ by themselves. Just to be precise, let me define the response of a solution to a driving force as the long term mean of the square of the solution, $\int_T^\infty x(t)^2 dt$ where $x(t)$ is the solution of the differential equation. Also, for this definition to make sense, the driving force has to have a bounded amplitude so that if $f(t)$ is the driving force, it has to be above and below two constants for all time.","['harmonic-analysis', 'ordinary-differential-equations', 'nonlinear-system']"
2316649,Number of ways of dividing objects,"I've been struggling with this problem for a while now, but I'm not sure how to proceed : Given a set of $n$ distinct objects, we seek to find the number of ways we can distribute all of the elements belonging to this set to $3$ kinds of people. $a$ of them want to get an odd number of objects, $b$ of them want to get an even number of objects and $c$ of them don't care about the parity of the objects they receive. In how many ways can this division be performed, such that everybody is satisfied? Some of the people may not receive an object. Since this problem has no label in my textbook, I'm not sure how to place it in a category, but I'm fairly certain it's a combinatorial problem. I've been thinking by writing the solution as a sum of products of binomial coefficients, but I haven't been able to get an useful form. I guess that each term would be comprised by two factors, where the first one would mean in how many ways we can get an odd number of objects and the second one would be the number of ways we can choose an even number of  objects from the remaining ones. How would you solve this?",['combinatorics']
2316673,Derivative of power sum?,"In one part of a proof I'm reading I see the following: $$ p\frac{d}{dp}\sum_{k=0}^\infty p^k = p\frac{1}{1-p} $$ However, I'm confused how the derivative of $\frac{d}{dp}\sum_{k=0}^\infty p^k = \frac{1}{1-p}$, wouldn't it equal $\frac{1}{(1-p)^2}$? Here's a picture of the actual two lines as I've stripped them to the part I'm confused about: https://i.sstatic.net/MkqSQ.jpg","['derivatives', 'summation']"
2316674,Dominated convergence theorem for absolutely continuous function,"Let $f$ be an absolutely continuous function with $f' \in L^1$. Is it then true that $\lim_{h \downarrow 0}\int_0^t \frac{f(x+h)-f(x)}{h}dx= \int_0^t f'(s) ds$? Here $t>0$ is finite. What is problematic for me is to find an upper bound for the integrand. Obviously the right side exists, but is there a way to justify this?","['real-analysis', 'lebesgue-integral', 'measure-theory', 'analysis']"
2316693,"How do we prove $|\cos x-\cos y|\le|x-y|$ for all $x,y\in\mathbb{R}$?","My thought: Fix any point $y_0\in\mathbb{R}$, since we know $\cos' x=-\sin x$ and $|-\sin|\le1$, thus $|\cos' x|=\lim_{x\to y_0}\frac{|\cos x-\cos y_0|}{|x-y_0|}=...\le 1$. I think my issue is how to translate $\lim_{x\to y_0}\frac{|\cos x-\cos y_0|}{|x-y_0|}$ into $\frac{\cos x-\cos y}{x-y}$. Could someone give some insight?","['calculus', 'limits']"
2316723,Tensors in the context of engineering mechanics: can they be explained in an intuitive way?,"I've spent a few weeks scouring the internet for a an explanation of tensors in the context of engineering mechanics. You know, the ones every engineering student know and love (stress, strain, etc.). But I cannot find any explanations of tensors without running into abstract formalisms like ""homomorphisms"" and ""inner product spaces"". I'm not looking for an explanation of tensors using abstract algebra or infinite, generalized vector spaces. I just want some clarification on what they actually mean and are doing in the nice 3D, Euclidean space, especially in the context of mechanics. There are a few questions that have been bugging me that I'm hoping all you smart people here can answer: What's the difference between a linear transformation and a tensor? Somehow they can both be represented by a $3\times 3$ matrix, but they do different things when acting on a vector? Like the columns of a $3 \times 3$ matrix of a linear transformation tell you where the basis vectors end up, but the same columns of a tensor don't represent basis vectors at all? Furthermore, a linear transformation transforms all of space but a tensor is defined at every point in space? Does a tensor act on vectors the same way as linear transformations do? What is the difference between a tensor product, dyadic product, and outer product and why are engineering tensors like the Cauchy stress built from the tensor product of two vectors (i.e. traction vector and normal vector)? Is it true that scalars and vectors are just $0^\mathrm{th}$ order and $1^\mathrm{st}$ order tensors, respectively? How are all these things related to each other? What topics and/or subtopics of linear algebra are essential to grasp the essence of tensors in the context of physics and engineering? Are they really just objects that act on vectors to produce other vectors (or numbers) or are they something more? I have plenty more questions, but I figure the answers to these could already be enough to fill a whole textbook. Just to note, I have already searched Math.StackExchange for tensors but haven't found any explanations that make sense to me yet. Thanks!","['tensor-products', 'tensors', 'classical-mechanics', 'linear-algebra']"
2316729,How to use cross-validation to select probability threshold for logistic regression,"I have a question about how to use cross-validation to select probability threshold for logistic regression. Suppose I want to minimize the misclassification rate. Say, I use 5-fold CV, and is this procedure correct: 1.fit 5 logistic regression models using each 4-folds of the data. 2.for each probability threshold(e.g. from 0.01 to 0.99), apply the 5 models on the left 1-fold of data, get misclassification rate. Then average these 5 error rates. 3.the optimal probability threshold is the one with smallest misclassification rate. And suppose I fit a ridge logistic regression model, to select the tuning parameter $\lambda$, is it okay to first use CV to select an optimal $\lambda$(e.g. use cv.glmnet function in R package glmnet), then apply this parameter to the procedure above to find probability threshold?","['regression', 'machine-learning', 'statistics', 'data-analysis']"
2316742,Creating a series that skips every $n^\text{th}$ term?,"I want to write a function using sigma notation that could represent an arbitrary number of terms of, for example, $1+2+4+5+7+8+10+11+13\ldots$, skipping every third term. I think one would need functions like floor and mod, but I'm not certain.",['sequences-and-series']
2316777,Showing $T$ is continuous.,"Deﬁne $\|p\| := \sup_{[0,1]}
|p|$ and consider $T : \mathcal{P} → \mathcal{P}$ as
$$T(a_0 + a_1t + · · · + a_kt^k) := a_0 + a_1t +\frac{a_2t^2}{2} + · · · +\frac{a_k}{k}t^k.$$
Show $T$ is a continuous. I just know $\mathcal{P}$ isn't complete with any norm, so I cannot apply the closed graph theorem and showing that $T$ is bounded seems impossible to me. I appreciate nay help for this problem.","['banach-spaces', 'hilbert-spaces', 'functional-analysis', 'continuity', 'analysis']"
2316808,Calculate the Poisson parameter given a probability,"I was looking at this question , but it never received an answer. Is it possible to calculate $\lambda$ (the expected number) from a probability? I know that $\lambda$ is easily calculated if, say, we know that the P(X=0 crashes per month) = .05. Then $\lambda$ is $-\ln(.05)$ crashes per month. But what if we are given that P(X=5 crashes per month)=.05. How would I find the expected number of crashes a month in that case? (if it is even possible)","['statistics', 'probability', 'poisson-distribution']"
2316832,Why does my parametric trigonometric function appears to be a polynomial?,"I was fooling around with $(\cos^2(t),\sin^2(at))$ with varying values of $a$, and found that if $a=3$ then $(\cos^2(t),\sin^2(3t))$ gives the graph of $y=-16x^3+24x^2-9x+1$ on the domain $[0,1]$ The calculator won't do parametrics but it just looks like the graph from 0 to 1.
Eliminating the parameter gave me $\sin ^2\left(3\cos ^{-1}\left(\sqrt{x}\right)\right)$ Why is this true? And, how can I solve $\sin ^2\left(3\cos ^{-1}\left(\sqrt{x}\right)\right)=-16x^3+24x^2-9x+1$ to prove that the Cartesian form is equal to the polynomial?","['polynomials', 'parametric', 'trigonometry', 'functions']"
2316838,Derivative of map restricted to subset,"I was recently doing an exercise and in the exercise I was given the map $\phi: \mathbb{R}^4 \rightarrow \mathbb{R}^2$ defined by $\phi(x,y,s,t) = (x^2 + y, x^2 + y^2 + s^2 + t^2 + y)$. Let $S = \phi^{-1}(0,1)$. I wanted to show that $S$ was a submanifold of $\mathbb{R}^4$ and so I needed to show that the differential $d\phi$ was surjective at every point $(x,y,s,t) \in S$. What I did was calculate its differential as $$ d\phi = \begin{pmatrix} 2x & 1 & 0 & 0 \\ 2x & 2y + 1 & 2s & 2t \end{pmatrix} $$ Now, let's consider the map $\phi: S \rightarrow \mathbb{R}^2$. Then $\phi(x,y,s,t) = (0,1)$ on $S$. So if I naively calculate the differential here, I get the differential is the zero matrix, which obviously does not agree with my previous calculation. I kind of figure that this is because when originally calculating the differential, $(x,y,s,t)$ ranged over all of $\mathbb{R}^4$, but for the map $\phi: S \rightarrow \mathbb{R}^2$, the points $(x,y,s,t)$ are restricted to only points on $S$. However, shouldn't $d\phi = d(\phi|_S)$ because calculating the differential shouldn't depend on the path taken by the points $(x,y,s,t)$, and so even when restricting my points to $S$, I should get $d\phi = d(\phi|_S)$? I hope my question is clear. EDIT: I can see that a matrix representation of the differential is the Jacobian of $\phi|_S$ in local coordinates. Since these local coordinates obviously cannot be standard coordinates on $\mathbb{R}^4$, I can't just differentiate $\phi$ then restrict the derivative to $S$. I believe this answers my own question, but I will leave this open for further discussion in the case that the more knowledgeable posters would like to contribute.","['multivariable-calculus', 'differential-geometry', 'differential-topology']"
2316863,Find value of this Triple summation,"Find the value of $$S=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\sum_{k=0}^{\infty}\frac{1}{3^{i+j+k}}$$ where $i \ne j \ne k$ i have tried in this way: First fix $j$ and $k$, then we get $$S_1=\sum_{\substack{i=0 \\ i\neq j,k}}^{\infty}\frac{1}{3^{i+j+k}}$$ where $j \ne k$ So we get $$S_1=\left(\frac{3}{2} \times\frac{1}{3^{j+k}}\right)-\frac{1}{3^{2j+k}}-\frac{1}{3^{j+2k}}$$ So now $$S_2=\sum_{\substack{j=0 \\ j\neq k}}^{\infty}\left(\left(\frac{3}{2} \times\frac{1}{3^{j+k}}\right)-\frac{1}{3^{2j+k}}-\frac{1}{3^{j+2k}}\right)$$ So $$S_2=\left(\frac{9}{4} \times \frac{1}{3^k}\right)-\left(\frac{9}{8} \times \frac{1}{3^k}\right)-\left(\frac{3}{2}\times \frac{1}{9^k}\right)-\left(\frac{3}{2}\times \frac{1}{3^{2k}}\right)+\left(\frac{2}{3^{3k}}\right)$$ Finally $$S=\sum_{k=0}^{\infty}\left(\frac{9}{4} \times \frac{1}{3^k}\right)-\left(\frac{9}{8} \times \frac{1}{3^k}\right)-\left(\frac{3}{2}\times \frac{1}{9^k}\right)+\sum_{k=0}^{\infty}\frac{2}{3^{3k}}-\sum_{k=0}^{\infty}\frac{\frac{3}{2}}{3^{2k}}$$ we get $$S=\frac{27}{16}-\frac{27}{16}+\frac{54}{26}-\frac{27}{16}$$  hence $$S=\frac{81}{208}$$ is there any other approach like using integration etc","['algebra-precalculus', 'infinite-product', 'summation', 'sequences-and-series']"
2316919,"Vector fields, line integrals and surface integrals - Why one measures flux across the boundary and the other along?","Why is it that a line integral of a vector field takes the dot product of the vector field with the tangent? This results in us taking the component of the vector field in the direction of the tangent of the curve we are integrating over. This can give us work done as a physical interpretation. If this curve encloses an area, we can also use Green's theorem over that area instead of the line integral to get the same result. Green's theorem measures flux ALONG the boundary. Now the analogue to that in a higher dimension is the surface integral of a vector field. However here the definition of the surface integral is taking the dot product of the vector field with the normal (more specifically the cross product of the first partial derivatives), with its interpretation as flux across the surface. Here, we can use the divergence theorem also if the surface encloses a volume. Divergence theorem measures the flux ACROSS the boundary. Why is there a difference? So why is a surface integral measuring something across the boundary and the line integral something along? EDIT: We notice the curl is actually a vector field itself. Looking through some notes, if we take a vector field in two dimensions, we have components $$f = (P,Q,0)$$ Hence, curl is equal to $$curl f.k$$ which is just $$\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$$ and an integral with this as its integrand is exactly Green's theorem. The curl in this case is integrated over a flat surface on the xy plane and the normal to the curl is just the k component (standard basis of the z axis). By applying the definition of surface integrals on Stoke's theorem, we get Green's theorem. This shows that Green's theorem is just a special case of Stoke's theorem. So the exact definition of the surface integral can be used to result in Green's theorem, and Green's theorem can be obtained directly by considering circulation over an area which links to our line integral, so there is definitely a link here that I can't seem to grasp. EDIT 2: I would like to add the fact that I mention in my comments. The line integral and surface integral over scalar valued functions makes sense. One could say that the line integral uses a tangential component (the norm of our the derivative of our parametrisation), while the surface integral a normal (the norm of the cross product of the first partial derivatives), but this is only because of its geometrical interpretation. The tangent used in line integral approximates the line, while the norm of the cross product used in surface integrals approximate the surface using the area of a parallelogram - which is exactly the norm of the cross product between arbitrary vectors. So why is the link in vector valued functions less obvious? What am I missing in my thinking?","['vector-fields', 'line-integrals', 'surface-integrals', 'greens-theorem', 'multivariable-calculus']"
2316923,Inequalities preserved under weak convergence?,"We know if $(x_{n})_{n=0}^{\infty}$ and $(y_{n})_{n=0}^{\infty}$ are real-valued sequences converging to $x$ and $y$ respectively and $x_{n}\leq y_{n}$, then $x<y$. Now let $(\Omega, \Sigma,\mu)$ be a finite measure space. Suppose $x_{n}, y_{n}\in L^{2}(\Omega, \mu)$  with $x_{n}\rightarrow x$ and $y_{n}\rightarrow y$ weakly. Suppose $x_{n}\leq y_{n}$ for $\mu$ almost everywhere. Do we have $x\leq y$ for $\mu$-a.e.? Here is my attempt at a proof. Suppose by contradiction  $x\leq y$ does not hold for $\mu$-a.e. By definition, we have, $\{\omega \in \Omega\vert x(\omega)>y(\omega)\}\in \Sigma$ and $\mu(\{\omega \in \Omega\vert x(\omega)>y(\omega)\})>0$. Let $B \colon = \{\omega \in \Omega\vert x(\omega)>y(\omega)\}$. The indicator function $\mathbb{1}_{B}(\cdot)$ will be measurable; moreover, we have \begin{equation}
\int \mathbb{1}_{B}x \,d\mu >\int \mathbb{1}_{B}y\, d\mu
\end{equation} where strict inequalities are preserved according to an argument such as this . However, since $x_{n} \leq y_{n}$ holds $\mu$-a.e., we must have (*) \begin{equation}
\int \mathbb{1}_{B}x_{n}\,d\mu \leq \int \mathbb{1}_{B}y_{n}\,d\mu
\end{equation} Since $\mathbb{1}_{B}\in L^{2}(\Omega,\mu)$, $\int\mathbb{1}_{B}x_{n}\,\mathrm{d}\mu \rightarrow \int\mathbb{1}_{B}x\,\mathrm{d}\mu$ and $\int\mathbb{1}_{B}y_{n}\,\mathrm{d}\mu \rightarrow \int\mathbb{1}_{B}y\,\mathrm{d}\mu$. And thus $\int\mathbb{1}_{B}x\,\mathrm{d}\mu \leq \int\mathbb{1}_{B}y\,\mathrm{d}\mu$, yielding a contradiction. Is this reasoning correct? In particular, I am afraid of the step I take at *. I would appreciate any suggestions/ corrections.","['functional-analysis', 'measure-theory']"
2316939,Prove with sequence,"$(a_n)^{\infty}_{n=0}$ is a sequence that define by 
$$a_n=\begin{cases}\frac{n}{2} & \mathrm{if}  & n=2k\\\frac{n-1}{2} & \mathrm{if}& n=2k+1\end{cases}$$
suppose $S(n)=a_0+a_1+a_2+\cdots+a_n \\n\geq 1\\$ If $x,y \in \mathbb{N},$ prove that $$\forall x>y :xy=S(x+y)-S(x-y)$$
It seems to be an easy  problem , but I am stuck on this. I am thankful for a Hint or solution guide . Thanks in advanced","['algebra-precalculus', 'summation', 'sequences-and-series', 'calculus']"
2317020,Classification of modules over complex matrices,"Is it possible to classify all finite dimensional modules over the space of matrices over complex numbers, i.e. $M_n\left(\mathbb{C}\right)$? I have a feeling that any simple module should be isomorphic to $\Bbb{C}^n$, but I have no clue on how to proceed.  I can say that for any nonzero element $v$ of the module $A$, we have $A = M_n\left(\Bbb{C}\right)v$ by simplicity, so we would like to send an element of the form $Xv$ to $Xe_1 \in \Bbb{C}^n$, but I don't know how to find the element $v$ to make the map well-defined. Also, can we say that any module over $M_n\left(\Bbb{C}\right)$ is the direct sum of simple modules?  If so, how?","['abstract-algebra', 'modules']"
2317029,The integral for gravitational potential of a uniform planet,"The gravitational potential for a uniform planet is given by the integral $$V(X,Y,Z)= \int \frac{G\rho(x,y,z)}{\xi(x-X, y-Y, z-Z)}dxdydz$$
where: $G$ is the gravitation constant $\xi$ is distance from the center equal to  $\sqrt{X^2+Y^2+Z^2}$ $(x,y,z)$ is the center of the planet at $(0,0,0)$ $(X,Y,Z)$ are the coordinates of a point $P$ outside the planet. The gravitational potential to be calculated is the force that is felt by an object at $P$ $\space$ My question is in one of the steps our professor provides in coming up with this integral. Since the shape is a sphere, I know that the Volume of a sphere in polar coordinates is: $$V=\int r^2\sin\theta drd\theta d\phi$$ This next step is where I need the clarification. The notes say: Now we are going to make a clever change in coordinates, replacing $\theta$ with $R$. A famous triangular relation says that $$R^2=r^2 +\xi^2+2r\xi \cos\theta,$$ where $\xi$ is the distance from point P to the center of the planet. The differential keeping r fixed is $$2RdR - 2r\xi \sin\theta d\theta.$$  so, in the volume element replace $$\sin\theta d\theta \rightarrow \frac{R}{r\xi}dR$$ Hence $$\frac{r^2\sin\theta drd\theta d\phi}{R}=\frac{1}{\xi}rdrdRd\phi$$ My main concern is how he arrived to the differential above, and why he replaces $sin\theta d\theta$.  If you have any info on the triangulation, that would be helpful but not necessary.","['polar-coordinates', 'multiple-integral', 'mathematical-physics', 'multivariable-calculus', 'integration']"
2317080,"If $f : [a, b] \to X$ is a path from $x$ to $y$ show that $\exists$ $g : [c, d] \to X$ which is also a path from $x$ to $y$","If $f : [a, b] \to X$ is a path from $x$ to $y$ show that there exists $g : [c, d] \to X$ which is also a path from $x$ to $y$ Now I know that $[a, b]$ is homeomorphic to $[c, d]$, hence there must exist a homeomorphism $h : [c, d] \to [a, b]$. I'm assuming I'm going to have to use $h$ to prove the existence of $g$, but I'm not sure how I could do that. I was thinking that if $h$ was also a path (which we can't be sure if it is), then $g = f \circ h : [c, d] \to X$ would be the desired continuous path. But the problem with that is that we need $h$ to be a path such that $h(c) = a$ and $h(d) = b$. I'm not sure how to continue the proof. Any hints are greatly appreciated.","['general-topology', 'proof-writing', 'elementary-set-theory', 'proof-verification']"
2317084,"About the value of $\mathbb{P}(N(t)=10,N(t/2)=5|N(t/4)=3)$ where $(N(t))$ is a Poisson process","Let $\left \{N(t) \ | t \geq 0 \right \}$ be a poisson process with rate $\lambda > 0$ and let $t > 0$. I need to calculate the following probability
\begin{equation}
\mathbb{P}(N(t)=10,N(t/2)=5|N(t/4)=3)
\end{equation}
I came to the conclusion that this probability equals
\begin{equation}
\mathbb{P}(N(t)-N(t/2)=5)\mathbb{P}(N(t/2)-N(t/4)=2) = e^{-\lambda\frac{t}{2}}\frac{(t\lambda/2)^5}{5!}e^{-\lambda\frac{t}{4}}\frac{(t\lambda/4)^2}{2!}
\end{equation}
However in the final answer they state that the probability equals
\begin{equation}
\mathbb{P}(N(t)-N(t/2)=5)\mathbb{P}(N(t/2)-N(t/4)=2) = e^{-\lambda t}\frac{(t\lambda)^5}{5!}e^{-\lambda t}\frac{(t\lambda)^2}{2!}
\end{equation}
I assumed that because the intervals $(t/4,t/2)$ and $(t/2,t)$ are disjoint the probability of $x$ events in each interval had a $poi(\lambda(t/4))$, $poi(\lambda(t/2))$ distribution. Can someone explain to me where my reasoning is wrong? Thanks in advance!","['poisson-process', 'probability-theory']"
2317097,Non-negative operator & self-adjoint operator [duplicate],"This question already has answers here : Show that a positive operator on a complex Hilbert space is self-adjoint (3 answers) Closed 7 years ago . I am wondering how to show that: if $A$ is a non-negative operator, then $A$ is self-adjoint. Def. 1. $A$ is non-negative if $\langle Ax,x \rangle \geq 0$ for $\forall x\in H$, where $H$ is a Hilbert space. Def. 2. $A$ is self-adjoint if $A = A^*$.","['functional-analysis', 'linear-algebra']"
2317139,Proving that a group of order $99$ is abelian,"$G$ is a group. $|G|$ = 99. I'm to show that $G$ is abelian. $G$ has 2 normal Sylow-subgroups, $S_3$ and $S_{11}$. Since the orders of $S_3$ and $S_{11}$ are primes, they are both cyclic and abelian. Since the orders of $S_3$ and $S_{11}$ are co-primes, they intersect trivially and the direct product $S_3 \times S_{11}$ is abelian and cyclic. Since they intersect trivially one can find an injective homomorphism $\phi: 
S_3\times S_{11} \to G$. Here's the thing I don't get: how do I show that it's surjective?",['group-theory']
2317165,"Prob. 27, Chap. 5, in Baby Rudin: The initial-value problem $y^\prime = \phi(x,y)$, $y(a)=c$","Here is Prob. 27, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\phi$ be a real function defined on a rectangle $R$ in the plane, given by $a \leq x \leq b$, $\alpha \leq y \leq \beta$. A solution of the initial-value problem 
  $$ y^\prime = \phi(x, y), \qquad y(a) = c \qquad (\alpha \leq c \leq \beta) $$
  is, by definition, a differentiable function $f$ on $[a, b]$ such that $f(a) = c$, $\alpha \leq f(x) \leq \beta$, and 
  $$ f^\prime(x) = \phi\left( x, f(x) \right) \qquad (a \leq x \leq b). $$ 
  Prove that such a problem has at most one solution if there is a constant $A$ such that 
  $$ \left| \phi \left(x, y_2 \right) - \phi \left(x, y_1 \right) \right| \leq A \left| y_2 - y_1 \right| $$
  whenever $\left( x, y_1 \right) \in R$ and $\left( x, y_2 \right) \in R$. Hint: Apply Exercise 26 to the difference of two solutions. Note that this uniqueness theorem does not hold for the initial-value problem 
  $$ y^\prime = y^{1/2}, \qquad y(0) = 0, $$ 
  which has two solutions: $f(x) = 0$ and $f(x) = x^2/4$. Find all other solutions. Here is the link to my post here on Math SE on Prob. 26, Chap. 5, in Baby Rudin, 3rd edition: Prob. 26, Chap. 5 in Baby Rudin: If $\left| f^\prime(x) \right| \leq A \left| f(x) \right|$ on $[a, b]$, then $f = 0$ My Effort: Let $f_1$ and $f_2$ be any two solutions of the given initial-value problem, and let $g = f_1 - f_2$. Then as $f_1$ and $f_2$ are solutions to the given IVP, so $f_1$ and $f_2$ are differentiable functions on $[a, b]$ such that $f_1(a) = f_2(a) = c$; $\alpha \leq f_1(x) \leq \beta$ and $\alpha \leq f_2(x) \leq \beta$; and 
  $$ f_1^\prime(x) = \phi \left( x, f_1(x) \right) \ \mbox{ and } \ f_2^\prime(x) = \phi \left( x, f_2(x) \right) $$
  for all $x \in [a, b]$. For all $x \in [a, b]$, since  $\left( x, f_1(x) \right) \in R$ and $\left( x, f_2(x) \right) \in R$, therefore we can conclude that $$ \left| g^\prime(x) \right| = \left| f_1^\prime(x) - f_2^\prime(x) \right| = \left| \phi \left( x, f_1(x) \right) - \phi \left( x, f_2(x) \right) \right| \leq A \left| f_1(x) - f_2(x) \right| = A \left| g(x) \right| $$
  for all $x \in [a, b]$. Moreover, 
  $$ g(a) = f_1(a) - f_2(a) = c - c = 0.$$
  So, by the conclusion in Prob. 26, we have $g(x) = 0$ for all $x \in [a, b]$; that is, $f_1(x) = f_2(x)$ for all $x \in [a, b]$; that is, $f_1 = f_2$, and so the given initial-value problem has at most one solution. Is this proof correct? Now for the IVP $$ y^\prime = y^{1/2}, \qquad y(0) = 0. \tag{1}$$ 
  Let $b$ be any real number such that $b > 0$. If $f(x) = 0$ for all $x \in [0, b]$, then
  $$f^\prime(x) = 0 = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$
  Thus $f$ is a solution to the initial-value problem (1). And, if $f(x) = x^2/4$ for all $x \in [0, b]$, then we have 
  $$ f^\prime(x) = \frac{x}{2} = \left( \frac{x^2}{4} \right)^{1/2} = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$
  Thus this $f$ is also a solution to (1). Now if $y \neq \hat{0}$, the zero function, then from (1) we obtain 
  $$ \frac{1}{y^{1/2}} y^\prime = 1,$$
  and so, upon integrating both sides, we get
  $$ 2 y^{1/2} = x + c, $$
  where $c$ is an arbitrary constant of integration. 
  Therefore, $$ y(x) = { (x+c)^2 \over 4 } \tag{2}$$
  for all $x \in [0, b]$. As $y(0) = 0$, so from (2) we obtain 
  $$ c^2/4 = 0,$$
  which implies that $c = 0$. So (2) becomes
  $$ y(x) = x^2/4$$
  for all $x \in [0, b]$. Thus, for any real number $b > 0$, the initial-value problem (2) has only the following two solutions: $f_1(x) = 0$ and $f_2(x) = x^2/4$ for all $x \in [0, b]$. Is my reasoning correct? If so, then have I arrived at the correct conclusion as well?","['derivatives', 'real-analysis', 'ordinary-differential-equations', 'initial-value-problems', 'analysis']"
2317179,From $e^n$ to $e^x$,"Solve for $f: \mathbb{R}\to\mathbb{R}\ \ \ $        s.t. $$f(n)=e^n \ \ \forall n\in\mathbb{N}$$
$$f^{(y)}(x)>0 \ \forall y\in\mathbb{N^*} \ \forall x\in\mathbb R$$ Could you please prove that there exists an unique solution: $f(x)=e^x$? (Anyway, this problem is not about fractional calculus) $\mathbb N^*=\{1,2,3...\}, \ \mathbb N=\{0,1,2....\}$ How about try to construct a few functional spaces that intersect at one point? Try Sard Theorem and Pre image Theorem.","['real-analysis', 'differential-topology', 'exponential-function', 'algebra-precalculus', 'functional-analysis']"
2317193,Example of a space which is Hausdorff and locally Euclidean but not paracompact?,"On page 330 of John G. Ratcliffe's text on hyperbolic manifolds, he defines a manifold $M$ to be a locally Euclidean Hausdorff space. By locally Euclidean, he means as usual that for each $x \in M$ there is an open neighbour $U$ of $x$ which is homeomorphic to an open subset of $\mathbb{R}^n$. But he omits the usual paracompactness assumption. I would guess that $M$ being locally Euclidean and Hausdorff is not sufficient to imply that it is paracompact, given this problem from Lee's book. Does anyone know of such a counterexample?","['manifolds', 'general-topology', 'examples-counterexamples', 'separation-axioms']"
2317235,"If functions are linearly independent for one $x\in I$, are they linearly independent for all $x\in I$?","This theorem comes up when talking about ordinary differential equations. Basically, if we have a fundamental system, i.e. a basis $B=(f_1,f_2,...,f_n)$ of the vector space of solutions for a differential equation $$y'=A(x)y$$, we can check for linear independence ( if we are unsure if it really is a basis ) by checking that 
$$(f_1(x_0),f_2(x_0),...,f_n(x_0))$$
is linearly independent for some $x_0 \in I$. The theorem says that if they are linearly independent for some $x_0 \in I$, that's equivalent to them being linearly independent for all $x \in I$. The proof is omitted, because this equivalence is supposed to be trivial, says the author of the textbook. Could you explain why the implication from some to all holds true? I'd actually think there would be functions for which there is an $x\in I$ where all the functions happen to be zero, and then you can find coefficients which are non-zero so that you can say $$ c_1 * f_1(x) + ... + c_n * f_n(x) = 0$$ $c\in \mathbb{R}$, but why does this imply that they must be linearly independent for all $x$?","['ordinary-differential-equations', 'linear-algebra']"
2317263,Geometric multiplicity equal to 0,Is it possible for an eigenvalue of endomorphism to have geometric multiplicity equal to 0? I would be grateful if anyone who has an answer to this question would care to explain.,"['eigenvalues-eigenvectors', 'linear-algebra', 'linear-transformations']"
2317265,Ramanujan and his problem,Following was proposed by Ramanujan: $ \sqrt{11-2\sqrt{11+2\sqrt{11-2\sqrt{11+\cdots}}}}=1+4\sin(10^o)$ Working on this I got the radical on the left equal to $(1+2\sqrt{2})$ implying that $\sin(10^o)=1/\sqrt{2}$ How is this possible? What is wrong here?,"['proof-writing', 'nested-radicals', 'trigonometry']"
2317271,Prove that $X^2$ and $Y^2$ are independent if $X$ and $Y$ are independent - am I right?,$X$ and $Y$ are independent r.v's. I want to prove that $X^2$ and $Y^2$ are also independent. Here's my reasoning: $P(X=x)=P(X=x|Y=y) \implies P(X=x)=P(X=x|Y=y^2) \implies P(X=x^2)=P(X=x^2|Y=y^2)$,"['independence', 'probability-theory', 'probability']"
2317293,Counting the elements in a Hamming sphere using Combinatorics,"We have the following Hamming sphere $$\mathcal B_3= ((0,0,0,0),(\mathbb F_7)^4)$$ with $\mathbb F_7=\{0,1,2,3,4,5,6\}$ So we want to know all possible elements with Hamming distance $\le3$ to $(0,0,0,0)$ $$\{u\in\mathbb F^4_7:dist((0,0,0,0),u)\le3\}$$ It is obvious that we have to use combinatorics to solve this problem. First we notice that three elements in u have to be $\neq 0$ $$u=(v1,v2,v3,0) \ v_i \neq 0 $$ to get the Hamming distance of $3$. The next step would be to count all the possible combinations to choose $3$ positions of a vector with length $4$. $$ \frac {4!}{(4-1)!\cdot1!}=24$$ We know that $v_1,v_2,v_3\in\{1,2,3,4,5,6\}$ So for each $v_i$ we have $6$ possible combinations. How do I combine this with the $24$ from above?","['combinatorics', 'coding-theory', 'spheres', 'discrete-mathematics']"
2317319,Stability: orbits spiralling monotonically,"I'm working my way through Dynamics and Bifurcations [Jack K. Hale, Huseyin Kocak] and I can't understand this probably trivial implication in Example 11.1 (page 334). We have a following system of ODEs (perturbed harmonic oscillator)
\begin{aligned}
\dot{x}_1 & = x_2 + a x_1 (x_1^2 + x_2^2), \\
\dot{x}_2 & = -x_1 + a x_2 (x_1^2 + x_2^2),
\end{aligned}
$a\in\mathbb{R}$, which in polar coordinates
\begin{equation}
x_1 = r \cos \theta, \quad x_2 = -r \sin \theta,
\end{equation}
reads
\begin{aligned}
\dot{r} & = a r^3, \\
\dot{\theta} & = 1.  
\end{aligned}
The book says: ""Since $\dot{\theta}>0$, the orbits spiral monotonically in $\theta$ around the origin. Therefore, the stability type of the origin of [the original planar system] is the same as that of the equilibrium point of the radial equation $\dot{r} = a r$."" I fail to see why it matters whether $\theta$ is a monotonic function of $t$ or not. Can't we draw the same conclusion that the radial equation determines the stability of the origin when this condition does not hold? (Let's say $\dot{\theta} = \sin\theta$?)","['ordinary-differential-equations', 'dynamical-systems']"
2317362,Problematic definition of pullbacks of covector fields in Spivak's Calculus on Manifolds,"This is a follow-up question to a recent one: What is a $k$-form on $[0,1]^k$ in Spivak's Calculus on Manifolds? In Spivak's Calculus on Manifolds , he defines the integration on chains as follows: where $A$ is some subset of ${\bf R}^n$. In this definition, the pullback $c^*\omega$ is very confusing for me. Let me write down how Spivak developed the definitions. Suppose $f:{\bf R}^n\to{\bf R}^m$ is a differentiable function and $\omega$ is a $k$-form on ${\bf R}^m$. Then $f^*\omega$ is defined as a $k$-form on ${\bf R}^n$ by
$$
(f^*\omega)_p=f^*(\omega_{f(p)})
$$
which means if $v_1,\cdots,v_k\in{\bf R}^n_p$, then 
$$
(f^*\omega)_p(v_1,\cdots,v_k)=\omega_{f(p)}(df_p(v_1),\cdots, df_p(v_k)).
$$
Here I have changed Spivak's notation $(f^*\omega)(p)$ to $(f^*\omega)_p$ to make the notation ""clearer"". On the other hand, a singular $k$-cube $c$ in $A\subset{\bf R}^n$ is defined as a continuous function $c:[0,1]^k\to A.$ (In fact, Spivak only defines the $k$-cube for $k=n$). Here comes my question : The domain of $c$, namely $[0,1]^k$, is a closed subset of ${\bf R}^k$. one cannot even talk about $dc_p$ for $p$ on the boundary of $[0,1]^k$. How should I make sense of the notation $c^*\omega$ in the quoted definition of integration on chains?","['multivariable-calculus', 'smooth-manifolds', 'differential-geometry']"
2317391,Derivative of a function defined by integral,"This question popped up somewhere on the internet and I thought it was interesting. I attempted to solve it but I don't know if it is correct. Find the derivative of $$F(x)=\int_{\cos{x^3}}^{\int_{1}^{x} {1/(1+t^2)dt}} {\sin{w} dw}$$ $$\begin{align}
\implies F(x) & =-\cos{w}]_{\cos{x^3}}^{\arctan{x}-\frac{\pi}{4}}\\ 
& = -\cos{(\arctan{x}-\frac{\pi}{4})}+\cos{(\cos{x^3})}\\
\implies F'(x) & = \frac{1}{1+x^2} \sin{(\arctan{x}-\frac{\pi}{4}})+3x^2\sin{(x^3)} \sin{(\cos{(x^3)})}\\
\end{align}$$ Is it this simple? Or is there something I should know before solving this that changes the normal differentiation and integration techniques?","['derivatives', 'integration', 'calculus']"
2317392,Convergent or Divergent Sequence: $n^3\sin\left(\frac{5}{n^3}\right)$,"$$n^{3}\sin\left(\frac{5}{n^{3}}\right)$$
I need to figure the limit if it converges or state if it goes to $$ \pm\infty $$ or simply ""divergent"" if it diverges but not to infinity. Looking at the sequence I can tell that as n goes to infinity $$n^{3} \to \infty  $$
so it is divergent and $$\sin\left(\frac{5}{n^{3}}\right)$$ is convergent from using the p-series comparison test with 
$$\frac{1}{n^{3}}$$
so a divergent series multiplying a convergent series would overall make a divergent series ? Or because $$\frac{5}{n^{3}} $$ is approaching $0$ as $n$ increases and the $\sin(0) = 0$ would the whole series converges to $0$?","['real-analysis', 'sequences-and-series', 'calculus', 'divergent-series', 'convergence-divergence']"
2317400,Find an $x$ such that mutually exclusive events are to disjoint sets what independent events are to $x$,"To state a matter of fact, probability theory has its own terms corresponding to proper math objects. Just out of curiosity, as disjoint sets would be called mutually exclusive events in probability theory in a suitable setting, I wonder, set-theoretically or measure-theoretically, under what name(s) are independent events?","['terminology', 'probability', 'elementary-set-theory']"
2317413,Automorphism of a compact Riemann surface with finitely many points removed maps punctured disc to punctured disc,"I wonder if the following is true:
If we have a compact Riemann surface $X$ where finitely many points are removed, so $X'=X - \{x_1,...x_k\}$ and an automorphism $f$ of $X'$, is it true that $f$ maps a punctured disc around a point $p\in \{x_1,...,x_k\}$ to a punctured disc around some point $q\in \{x_1,...,x_k\}$? By a punctured disc around $p$ I mean an open set $U\subset X'$ such that $U\cup \{p\}$ is biholomorphic to a disc in $\mathbb{C}$ via a chart $(U\cup \{p\}, \phi)$ of $X$. I know that this is not the case if $X$ is a non compact Riemann surface.
For example consider $X=\mathbb{C}$ and $X'=\mathbb{C}-\{0\}$. Then the automorphism $f(z)=\frac{1}{z}$ maps the punctured disc $B_1(0)-\{0\}$ to $\mathbb{C}-\overline{B_1(0)}$ which is not a punctured disc.","['riemann-surfaces', 'complex-analysis']"
2317487,Grothendieck group of $\mathbb P^n$,"I am trying to prove that the Grothendieck group $K_0(\mathbb P^n)$ is generated by $\{\mathcal O, \mathcal O(1),...,\mathcal O(n)\}$. I already showed that this set generates a full sublattice of $K_0(\mathbb P^n)$.
For this I showed via induction that the rank is bounded above by $n+1$ and then I defined the function: $\phi:G_0(\mathbb P^n)\times G_0(\mathbb P^n) \rightarrow \mathbb Z$ $(\mathcal F,\mathcal G) \mapsto \chi(\mathcal F \otimes \mathcal G^{\vee})$ where $\chi$ denotes the Euler characteristic. Using this function I defined the matrix $\varPhi$ by 
$\varPhi_{i,j}:=\phi(\mathcal O(i),\mathcal O(j))$. How can I proceed from here that the $\mathcal O(i)$ actually generate the Grothendieck group? Sincerely, slinshady Edit: Maybe I should say that I only consider locally free $\mathcal O_{\mathbb P^n}$ -modules, as this is sufficient in sufficiently nice settings.","['algebraic-k-theory', 'algebraic-geometry']"
2317508,The hot hand and coin flips after a sequence of heads,"ESPN recently posted a story demonstrating that the ""hot hand"" concept is, in fact, real. Part of the justification is this example based on coin flips from a paper by Adam Sanjurjo and Joshua B. Miller : And now [Joshua] Miller brings it back to coin flips and the subtle selection bias of hit streaks. The broken-clock example, Miller says, is an extreme illustration of what's happening in a long line of coin flips. Let's say that in analyzing the results of 100 coin flips you see a string of three heads (HHH_) and ""_"" is, say, Flip 42 in the sequence -- which was selected because it was preceded by HHH. What is the _ in the sequence? Of course, you don't know which kind of sequence you are in. it could be HHHT or it could be HHHH. You think it's a 50-50 chance that Flip 42 is H (or T). But here's where it changes, and it is sneaky. In sequence HHHH, you could have selected Flip 43 because it continues a run of three heads, and you haven't excluded Flip 44 or 45, like you would have in sequence HHHT (it ends the run of heads). The excluded flips in the HHHT world mean it is more likely that Flip 42 will turn up T rather than H, given the condition that the flip was selected because it was preceded by three heads in a row. The choice has been restricted. To tie it back to the big ol' mansion, the broken clock excludes more times that are different from the one you are seeing, and the kind of sequence with HHHT excludes more flips that are different from the one you selected. I am struggling to follow this explanation and the significant of the restricted choice. What does excluding flips 44 or 45 mean? Why is flip 42 more likely to be T rather than H?",['probability']
2317512,Problem about the bones,"The player has $5$ tetrahedral dice. In how many ways he can throw exactly two $1$'s and one $3$ on them, if the bones numbered? How can I calculate this number? So far, no ideas","['combinatorics', 'discrete-mathematics']"
2317538,Why I can't find modern books on conic sections and analytical geometry?,"I want to learn analytical geometry but unfortunately I can't find any modern book on this subject. On this site I found a question Good books on conic section. But all the suggestions given is more than a century old. I tried two chapters from the two books suggested and I can't take it anymore. My problems with old books are 1) They are difficult to read due to the language used. 2) The problems given are so cumbersome and provide no insight,.I don't hate hard problems but problems in old books are like ""factor this 5 degree polynomial"". 3) Not really a problem but the formatting is really bad in them. My question is why there is no new books on this subject, new means after 1960s-70s, is there no mathematical interest in conic sections ? or just sales of these books are not enough to make profit ?","['reference-request', 'geometry']"
2317595,Why does invertibility of $2$ in $\mathbb{Z}_p$ guarantee that every element is a square?,"In Serre's arithemetic he claims at the top of page 18 that because $2$ is invertible in $\mathbb{Z}_p$ we have that every element in the multiplicative group $U_1 = 1 + p\cdot\mathbb{Z}_p$ is a square. Why is this true? The only step I can show is the invertibility of $2$ in $\mathbb{Z}_p$. Using the power series expansion
$$
\frac{1}{1 + p} = 1 - p + p^2 - p^3 + p^4 - \cdots
$$
we can see that
$$
\frac{1}{2} = \frac{p+1}{2}\cdot (1  - p + p^2 - p^3 + p^4 - \cdots) \in \mathbb{Z}_p
$$","['number-theory', 'p-adic-number-theory', 'elementary-number-theory']"
2317602,How to solve: $z''=-c/z^4$,"I would like tho solve the following nonlinear second order differential equation $\frac{d^2{z}}{d{t^{2}}}=-\frac{c}{z^4}$ Where $C$ is a constant. I have try to use this steps http://www.sosmath.com/diffeq/second/nonlineareq/nonlineareq.html with no success. Also, I've tryed to use the solver in Matlab, and they give me an error. I don't think that this ODE is impossible to solve analytically. Thank you in advance",['ordinary-differential-equations']
2317648,Expressing elements of $\mathbb{Z}/n\mathbb{Z}$ as sums of squares,"Let $n\ge 2$ an integer. Find the lowest integer $\kappa(n)$ such that every elements in $\mathbb{Z}/n\mathbb{Z}$ can be written as a sum of $\kappa(n)$ squares. This statement can be found in the ""smf 2017"", a french contest. I was just wondering if it is a well-known result or are there references about it. Thanks in advance !","['number-theory', 'reference-request', 'contest-math', 'elementary-number-theory']"
2317694,Find $\mathrm{Ker}(T)$ and $\mathrm{Im}(T)$ of the following linear transformation with bases,"Question : Let T: $\mathbb R^{3} → M_{2\times 2}(\mathbb R)$ be the linear transformation defined by $$T((a,b,c)) = \begin{bmatrix}
         a & 5a\\
         c & 3c\\
        \end{bmatrix} $$ Consider the bases $\alpha = \{(0, 1, 0),(0, 1, 1),(1, 1, 0)\}$ of $\mathbb R^{3}$ and
, 
$\beta = \{{\begin{bmatrix}\
         1 &−1\\
         0 & 0\\
        \end{bmatrix}} ,{\begin{bmatrix}
         0 & 1\\
         -1 & 0\\
        \end{bmatrix}} ,{\begin{bmatrix}
         0 & 0\\
         1 & 1\\
        \end{bmatrix}}, {\begin{bmatrix}
         0 &1\\
         0 & -1\\
        \end{bmatrix}}\}$ of $M_{2\times 2}(\mathbb R)$. Find $\mathrm{Ker}(T)$ and $\mathrm{Im}(T)$. Context : This question is a practice problem given to us by our introductory linear algebra professor for solving and understanding concepts (Note: this is not an assignment or graded homework question) Attempt : Finding $\mathrm{Ker}(T)$ : Row reducing the transformation matrix to RREF would give the matrix \begin{bmatrix}
         1 &0\\
         0 & 1\\
        \end{bmatrix} leaving no free variables thus $\mathrm{Ker}(T)=\mathrm{span}\{0\} $ Finding $\mathrm{Im}(T)$ : Since there are no free variables, we use the original columns from $T((a,b,c))$ to get our Image i.e. $\mathrm{Col}(A) =\mathrm{span}\{\begin{bmatrix}
         a \\
         c \\
        \end{bmatrix}, \begin{bmatrix}
         5a \\
         3c \\
        \end{bmatrix}\}$. I know we use this with the basis $\beta$ to obtain our image. How do we proceed from here? Do we perform $a\begin{bmatrix}
         1&-1 \\
         0&0 \\
        \end{bmatrix} + c\begin{bmatrix}
         0&0 \\
         1&1 \\
        \end{bmatrix}$ and $5a\begin{bmatrix}
         1&-1 \\
         0&0 \\
        \end{bmatrix} + 3c\begin{bmatrix}
         0&0 \\
         1&1 \\
        \end{bmatrix}$? This gives us $\mathrm{Im}(T) = \mathrm{span} \{\begin{bmatrix}
         a&-a \\
         c&c \\
        \end{bmatrix}, \begin{bmatrix}
         a&-5a \\
         3c&3c \\
        \end{bmatrix}\}$. Doubt : Is this correct or am I going about this the wrong way? I have a feeling I made a mistake because according to the R-N theorem, my rank and nullity must add up to $3$ but at this point, it adds up to $2$.","['linear-algebra', 'linear-transformations']"
2317755,Learn linear algebra in a week,"I'am finishing my undergraduate degree in computer science and despite having had to take some math classes my math ability is still pretty poor. I struggled a lot with it which I believe was due to missing some pieces of knowledge that I needed to know and not seeing the big picture. 
Now i'am studying neural networks and turns out they require quite some math(eg: the backpropagation algorithm uses the chain rule). Some people say you don't need the math but I don't think I will be able to to understand neural networks completely without it. And even if I could get away with I would probably still need the math later on. I have about a month that i can dedicate fully to this quest and I'm determined to learn linear algebra and multivariate calculus in this time frame. I will start with linear algebra (doesn't depend on calculus, right?) and I plan on learning from MIT 18.06 video lectures as well as doing the assignments. They have solutions so I should be able to easily track my progress. I also found this course from Berkeley Math 110. Linear Algebra . It doesn't have video lectures but it has more assignments with solutions so I can practice even more.
As for textbooks MIT uses ""Introduction to Linear Algebra, Fourth Edition, Gilbert Strang"" and Berkeley uses ""Linear Algebra by S.H. Friedberg,A.L. Insel and L.E. Spence,Fourth Edition"". People here seem to say good things about them. I'am starting this journey tomorrow. In the mean time I'd like to get some advice. Do you have any advice in order to make this process smoother? Are there any other resources I should know about?","['multivariable-calculus', 'neural-networks', 'linear-algebra']"
2317765,Use PIE to count the number of $6$-multisets of $[6]$ in which no digit occurs more than twice.,"This is one of a set of several problems in my book I am having difficulty not just solving, but also understanding the provided solutions. The given answer is $462 - 336 + 15 = 141.$ I'll try and see where the terms of the equality above come from. $[6] = \{1, 2, 3, 4, 5, 6\}.$ The number of all multisets of size six sourced out of $[6]$ is $\binom{6 + 6 -1}{6} = \binom{11}{6} = 462,$ so that's where the first term of $462 - 336 + 15$ come from. All the multisets of size six where each one contains a digit that occurs at least three times can be rewritten in their type form like so: $$(3, 3), (3, 2, 1), (3, 1, 1, 1), (4, 2), (4, 1, 1),  (5, 1), (6)$$ Then, $|(3, 3)| + |(3, 2, 1)| + |(3, 1, 1, 1)| + |(4, 2)| + |(4, 1, 1)| + |(5, 1)| + |(6)| \\ = (6 \cdot 5) + (6 \cdot 5 \cdot 4) + (6 \cdot \binom 53) + (6 \cdot 5) + (6 \cdot \binom 52) + (6 \cdot 5) + 6 \\ = 30 + 120 + 60 + 30 + 60 + 30 + 6 \\ = 336.$ So that's where the second term of  $462 - 336 + 15$ must come from. I am not clear on where $15$ in $462 - 336 + 15$ come from.  So, that's what I'd like to know. Thanks.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
2317776,Hermite polynomials for negative integers,"I have an equation which is similar to Hermite's differential equation $y''(x)+xy'(x) +\lambda y(x)=0$ with decaying boundary conditions at infinity $y\rightarrow 0$ as $x\rightarrow\pm\infty$. I believe the eigenvalues take the form $\lambda_n =-n$ where $n=0,1,2,...$ from the power series solution. The eigenfunction which satisfies the boundary condition is $\phi_n(x) = A_n e^{-y^2/2}H_{\lambda_n-1}\left(\frac{y}{\sqrt 2}\right)$. All of which are Hermite polynomials for negative integers. The recurrence relation $2 n H_{n-1}(x) = 2x H_n(x)-H_{n+1}(x)$ is no helpful since it gives indeterminate values. But I have seen them that these polynomial of non-negative orders are expressed in terms of error function, but I could not find any reference to this. Is there any way to derive them in terms of error function? It would be helpful if someone clarifies about the orthogonality condition for negative integers. Is it the same as Hermite's polynomials of positive integers?","['hermite-polynomials', 'error-function', 'eigenfunctions', 'ordinary-differential-equations', 'orthogonal-polynomials']"
2317778,How do find out what eigenvalue represent a state in a state space vector?,"Let's say that I have a system matrix A and to find out the eigenvalues $\lambda$ ,I do this: $$ \hbox{det}(\lambda I - A) = 0 $$ Then to find out if the system are controllable, I uses the Hautus Lemma test . This thest is mutch better that the regular $\hbox{rank}(\hbox{ctrb}(A, B)) = n\ $ test.
Anyway! Here it is: $$ \hbox{rank}([\lambda_i I - A, B]) = n$$ Let's say that I got 3 eigenvalues of A . They are $\lambda_1 = -2$ , $\lambda_2 = -10$ and $\lambda_3 = -0.5$.  Now I test if the system is controllable:
\begin{align} 
\hbox{rank}([\lambda_1 I - A, B]) &= 3\, ,\\
\hbox{rank}([\lambda_2 I - A, B]) &= 1\, ,\\
\hbox{rank}([\lambda_3 I - A, B]) &= 3
\end{align} So something went wrong here! I got 3 eigenvalues, which mean that my state vector is the length 3. That means that my rank of the system should be number 3. But this:
$$ \hbox{rank}([\lambda_2 I - A, B]) = 1$$
gives number 1 buy using $\lambda_2 = 10$. Question: Does this mean that something is wrong with my state vector at row number 2 beacuse the eigenvalue $\lambda_2 = 10$ must reprecent the state vector $x_2$ ?","['eigenvalues-eigenvectors', 'optimal-control', 'matrices', 'determinant', 'ordinary-differential-equations']"
2317800,$f'(a)=0 \Rightarrow f$ constant,"I am currently reviewing theorems I already learned in complex analysis and looking at Holomorphic function with zero derivative is constant on an open connected set I asked myself wether it is necessary that $f'(x)=0 \forall x \in G$ for $f:G\to \mathbb{C}$ and $G$ being a domain to get that $f$ is constant or wether a single point in the domain could be sufficient. If so, how to show that? How is this related to the maximum principle? When I think about real analysis, $f'(a)=0$ implies that there is an extremum in $a$ but it seems this is not the case in complex analysis, at least I have not found anything but the maximum principle regarding this issue.",['complex-analysis']
2317836,"If analytic functions $f_n$ converge uniformly and the limit is injective, then almost all $f_n$ are injective","I want to prove the following. Let $K \subset \Bbb C$ be a compact set, and let $f_n$ be analytic functions on $K$ that converge uniformly to a one-to-one analytic function $f$. Then for all large $n$, $f_n$ is one-to-one. The converse of this theorem, that if $f_n$ are one-to-one then f is one-to-one (or constant), is called Hurwitz's theorem, and I am hinted that it can be adapted to this problem. It is proved by assuming $f(a)=f(b)$, defining $g_n = f_n-f_n(a) \to g=f-f(a)$ that has a zero on $b$, while non of $g_n$ has a zero on $b$. Taking a small enough disk around $b$ so that $a$ is not in it, then by the uniform convergence $|g_n-g|<|g|$ for all large $n$, so by Rouché $g, g_n$ have the same number of zeros in this disk, which is a contradiction since $g$ has a zero and $g_n$ doesn't. I'm not sure how to change this to my case. I assume by contrary that there are $f(a_n)=f(b_n)$ for all large $n$, and so by passing twice to a subsequence we can have $a_{n_k}\to a, b_{n_k}\to b$ so $f(a)=f(b)$, but as user MikeMiller pointed in chat we might have $\lim a_{n_k}=\lim b_{n_k}$ so this does not contradict univalence of $f$. To mimic the proof of Hurwitz, I define $g_n=f_n-f(a_n)$ that vanish on $a_n, b_n$ and assume $a_{n_k} \to a$. If $a=\lim a_{n_k} \neq \lim b_{n_k}=b$ then $f(a)=f(b)$ so we also assume $a=b$. Then we want to show that $g=f-f(a)$ has two zeros in a neighborhood of $a$ by Rouche, but $g_{n_k}$ does not have two zeros in small disks around $a$ (not even one zero). The original text writes:","['complex-analysis', 'analysis']"
2317848,Solutions to $y''(x)-y(x)=0$,"When I solve the differential equation $$y''(x)-y(x)=0$$ using the auxiliary equation, I get $$y(x)=Ae^{x}+Be^{-x}.$$
Is $$y(x)=A\sinh(x)+B\cosh(x)$$ just as good of a solution? How about $$y(x)=A\cosh(x)+B\cosh(x),$$ etc.? Does it have infinitely many general solutions? Is this normal? Thanks. :)","['hyperbolic-functions', 'ordinary-differential-equations', 'calculus']"
2317869,Normal bundle of a section of a projective bundle,"Let $X=\mathbb{P}^1$, $E=\mathcal{O}\oplus\mathcal{O}(1)\oplus \mathcal{O}(2)$ and let $Y=\mathbb{P}(E)$. Denote $\pi:Y\to X$ the projection map. There is a section of $\pi$ corresponding to the surjection $E\to \mathcal{O}$. Let $C$ be the image of this section in $Y$. How can we compute the normal bundle of this curve? The case when $E$ is a rank 2 vector bundle is discussed here. Normal bundle of a section of a $\mathbb{P}^1$-bundle . However, I believe in our case, the argument cannot be directly applied. Thanks for the help!","['algebraic-geometry', 'birational-geometry']"
2317870,What is the height of the pyramid with these four points?,"I have a pyramid, whose base is formed of points $A (3,5,3) $, $B (-2,11,-5) $, $C (1,-1,4) $ . Now I need to find the height of the pyramid from the point $S (0,6,4) $ so my idea was this. I form a plain using vectors $AB $ and $AC$, then I find a vector which is orthagonal to that plain. After that I found the intersection of the orthagonal vector and the plain, let that be point $T $. Then my height is actually the length of vector $ST$. Here I get that the length is 63, but my textbook says it's 3.. just wondering if I made a mistake somewhere or if the solution is wrong. Thanks","['euclidean-geometry', 'linear-algebra', 'vectors', 'geometry']"
2317953,Why does the Mean Value Theorem require a closed interval for continuity and an open interval for differentiability?,"Why does the Mean Value Theorem assume a closed interval for continuity and an open interval for differentiability? The MVT says: Let $f$ be a continuous function on $[a,b]$  that is differentiable on $(a,b)$, then.... Is there any example where one of them isn't true so that the MVT is not true?","['continuity', 'calculus']"
2317955,Proper subgroup of normalizer of a finite group impies nilpotent.,"So I came upon this exercise: Let $G$ be finite. If every proper subgroup $H$ of $G$ has the property $H < N_G(H)$, then $G$ is nilpotent. I can prove the converse by induction on the nilpotency class of $G$, but I'm kind of stuck here.","['abstract-algebra', 'group-theory']"
2317974,Isomorphism between $\mathbb{T}$ and $\mathbb{R} \oplus \mathbb{Q}/\mathbb{Z}$.,"I'm trying to prove that the circle group $\mathbb{T}$ is isomorphic to $\mathbb{R} \oplus \mathbb{Q}/\mathbb{Z}$ with a little bit of cardinal arithmetics. First, I know that $\mathbb{T}$ can be decomposed (structure theorem of divisible groups) as a direct sum of $\mathbb{Q}^X$ for some set $X$ and its torsion subgroup (which is isomorphic to $\mathbb{Q}/\mathbb{Z}$). I'd like to say that $|X| = |\mathbb{R}|$ (thus $\mathbb{Q}^X \cong \mathbb{R}$ as $\mathbb{Q}$ vector spaces and the desired result follows), however, I don't see how can I show this. My reasoning was: $|\mathbb{R}|=|\mathbb{T}|=|\mathbb{Q}^X||\mathbb{Q}/\mathbb{Z}|$, then it must be the case that $|\mathbb{T}|=|\mathbb{Q}^X|$ and $|\mathbb{R}|=|\mathbb{Q}^X|$, and here I'm stuck because I cannot conclude that $|X|$ must be $|\mathbb{R}|$ (as counterexample, $|\mathbb{R}| = |\mathbb{Q}^{\mathbb{N}}|$). Is there a way to prove that $|X| = |\mathbb{R}|$ or should I try another reasoning not involving cardinals? Edit: forgot $\mathbb{Q}^{X}$ is a direct sum of copies of $\mathbb{Q}$ (is not the whole direct product because $X$ cannot be finite), so my ""counterexample"" is useless but still cannot see the equality.","['abstract-algebra', 'group-isomorphism', 'divisible-groups', 'cardinals', 'group-theory']"
2318045,Level set diffeomorphic to $\mathbb{S}^2$,"Consider the map $\phi: \mathbb{R}^4 \rightarrow \mathbb{R}^2$ defined by $\phi(x,y,s,t) = (x^2 + y, x^2 + y^2 + s^2 + t^2 + y)$. Show that $(0,1)$ is a regular value of $\phi$, and that the level set $\phi^{-1}(0,1)$ is diffeomorphic to $\mathbb{S}^2$. I get two equations describing the level set: (1) $x^2 + y = 0 \implies y = -x^2$ (2) $x^2 + y^2 + s^2 + t^2 + y = 1 \implies y^2 + s^2 + t^2 = 1$ So $\phi^{-1}(0,1) =  \{(x,y,s,t) \in \mathbb{R}^4: y = -x^2 \hspace{0.1cm} \mathrm{ and } \hspace{0.1cm} y^2 + s^2 + t^2 = 1\}$. I need to show that $d\phi(x,y,s,t)$ is surjective for all $(x,y,s,t) \in \phi^{-1}(0,1)$. I calculate: $d\phi(x,y,s,t) = \begin{pmatrix} 2x & 1 & 0 & 0 \\ 2x & 2y + 1 & 2s & 2t \end{pmatrix}$ It is easy to show that this matrix has rank $2$ for all $(x,y,s,t) \in \phi^{-1}(0,1)$ and so $\phi^{-1}(0,1)$ is an embedded submanifold of $\mathbb{R}^4$. Now, I need to show that this level set is diffeomorphic to the unit sphere. I can kind of see that it may be diffeomorphic to a spheroid, and I know I can show that the spheroid is diffeomorphic to the sphere. My only problem is coming up with this diffeomorphism from the level set onto the spheroid. I imagine I can define a map $\psi: \phi^{-1}(0,1) \rightarrow S$ by $\psi(x,y,s,t) = (x,s,t)$, where $S = \psi(\phi^{-1}(0,1))$. It is easy to see that this map is invertible and its inverse is given by $\psi^{-1}(x,s,t) = (x, -x^2, s, t)$ since $y = -x^2$. My only trouble is showing that these maps are both smooth. Both the domain and codomain are submanifolds of $\mathbb{R}^4$ and $\mathbb{R}^3$, respectively, so I need to express $\psi$ in appropriate local coordinates before differentiating, but coming up with these local coordinates seems like I am making things overly complicated. What would you suggest I do?","['differential-geometry', 'differential-topology']"
2318061,Can a total rational metric space be complete?,"Let's call a metric space $(M,d)$ a total rational metric space if: For every $x,y\in M$, $d(x,y)\in\mathbb{Q}$. For every $x\in M$ and every rational $q\geq0$ there exists a $y\in M$ such that $d(x,y)=q$. Can a total rational metric space be complete?","['real-analysis', 'metric-spaces']"
2318132,Bolzano Weierstrass Theorem for Nets?,"For real sequences, we know that: (1) Every bounded sequence has a convergent subsequent. (2) Every sequence has a monotone subsequence. For nets, do we have the corresponding theorems? (Extra: Can you give me reference?) Thank you.","['general-topology', 'reference-request', 'real-analysis', 'nets']"
2318135,"What are the area of a triangle with side lengths $\tan(x)$, $\cos(x)$ and $\sin(x)$?","Consider a non­degenerated right triangle with sides of length $\sin x$, $\cos x$, and $\tan x$ where $x$ is a real number.
  Compute the possible values of the area of this triangle. I was thinking of more along the lines of Heron's formula but that was very nasty indeed, rather I have attempted to find out what are the lengths of the triangle and I have done the Pythagorean theorem which was in vain. Another thing that I have done was to graph all three and to see which one was the largest in the y-value but it turned out that at times one graph was larger than the other and at other times it wasn't. And another thing that I have done was to plug and chug in values such as the number 3 into all of the trig functions and do Pythagorean theorem to see if it satisfied it but none of them didn't The reason why that I have done those steps was so that I can multiply the legs and divide by two to find the area of the triangle but in order to do that I must know what are the legs. I was wondering if there was any other way?","['algebra-precalculus', 'trigonometry', 'triangles']"
2318137,Probabilistic or Statistical Interepretation of Central Binomial Identity,"What is probabilistic or statistical interpretation of the identity given below? 
\begin{equation}\label{BIIntro1}
\sum_{k=0}^{n}\binom{2k}{k}\binom{2n-2k}{n-k}=4^{n}
\end{equation}","['combinations', 'statistics', 'probability', 'permutations', 'combinatorics']"
2318170,Dice puzzle: pips must sum to 21,"A friend recently told me this little puzzle but to me it seems like there might not be a right answer. Was wondering if there is in fact a best combination in this scenario. Dice rolling war You and a friend decide to have a dice rolling war consisting of 36 battles. Each player will roll 1 die. A battle is won by the player with the higher number of pips, and the winner receives a point. If both players have the same number of pips, neither player receives a point. However, before starting the battles, the players get to choose the number of pips to put on the 6 sides of the die, the constraint being that the number of pips on all sides of the die must summ to 21 and be in the range [1-6]. For example,
[4][4][4][4][4][1] would be valid choice.
Finally, the person with the most points at the end of 36 rounds wins. Part a) What pips should you choose to give you the best chance of winning? Part b) What is the probability that you will win?","['puzzle', 'probability', 'game-theory']"
2318236,How to define a finite topological space?,"I want to develop a simple way to define topologies on finite sets
$X=\{1,2,\dots,n\}$ for computational experiments. Does any function $c:X\to \mathcal P(X)$, such that $x\in c(x)$,
  define a closure operator on $X$? The idea is that $c$ should define a closure operator by
$$\mathrm{cl}(\{x_1,\cdots,x_m\})=\overline{\{x_1,\cdots,x_m\}}=\bigcup_{k=1}^{m} c(x_k)$$","['general-topology', 'computational-mathematics']"
2318312,Choosability in multipartite graph,"The following is problem from Chapter 5 of Diestel's Graph Theory book: For positive integers $r,s$, we denote by $K^r_s$ the graph with vertex set
  the disjoint union of sets $V_1,\ldots ,V_r$ of size $s$ and, for all $i\le i < j\le r$, all edges between $V_i$ and $V_j$. Prove that the choice number $\text{ch}(K^n_2) =n$. Here is what I can prove so far. I try to use induction on $n$. The cases $n=1,2$ are easy to handle. Assume the statement for $n$ and try to prove it for $n+1$. We can prove that $K^{n+1}_2$ is not $n-$choosable by assigning to each of the vertices the list $\{1,\ldots,n\}$ and using the Pigeon-hole principle. Thus it suffices to show $\text{ch}(K^{n+1}_2)\le n+1$. Suppose we assigned any list of $n+1$ colors to every vertex. It is clear that if some pair of vertices in one of the partite sets contain a common color in their lists then one can exclude that color from the lists of the other $2n$ vertices and apply induction, so we can assume otherwise. By the same kind of reasoning one can also see that for two vertices which in a partite set if we select a color from one's list and select a color from the other's list, then there is another vertex whose list has both of these colors. Is there anything else I can deduce about the lists?","['graph-theory', 'coloring', 'discrete-mathematics']"
2318357,"Does $\mathbb{P}(X\in A, Y\in A)=\mathbb{P}(X\in A)\mathbb{P}(Y\in A)$ for all $A$ borel sets imply $X$ and $Y$ are independent?","Is it true that if $\mathbb{P}(X\in A, Y\in A)=\mathbb{P}(X\in A)\mathbb{P}(Y\in A)$ for all $A$ borel sets then the random variables $X$ and $Y$ are independent? My intuition says satisfying this condition will be not enough to imply independence but I can't think of any simple counter-example. I would appreciate any hints.","['independence', 'probability-theory', 'random-variables', 'probability-distributions']"
2318428,"If $A$ is a non-square matrix with orthonormal columns, what is $A^+$?","If a matrix has orthonormal columns, they must be linearly independent, so $A^+ = (A^T A)^{−1} A^T$ . Also, the fact that its columns are orthonormal gives $A^T A = I$. Therefore, $$A^+ = (A^T A)^{−1} A^T = (I)^{-1}A^T = A^T$$ Thus, $A^+ = A^T$. Am I correct? Thank you.","['matrices', 'pseudoinverse', 'least-squares', 'linear-algebra']"
2318441,Parallel transport attempt solution. Should I parametrize the path like so.,"Context: Past exam for class that I sit my final for tomorrow :). We are in a $2$-dimensional surface, with coordinates $(\theta,\phi)$. Consider the metric $ds^2 = r_0^2 d\theta^2 + r_0^2 \sin^2(\theta)d\phi^2$ I have shown that the only non-zero Christoffel symbols are:
$$\Gamma_{12}^{\,\,\, 2}=\Gamma_{21}^{\,\,\, 2}=\cot(\theta),\qquad \Gamma_{22}^{\,\,\, 1}=-\sin(\theta)\cos(\theta)$$ I want to show that if a contravariant vector $a^i$ undergoes parallel transport around the circle from $\theta = \pi/4$ from $\phi=0$ to $\phi=2\pi$ and $a^1=0$, $a^2=1$ at $\phi=0$ then at $\phi=2\pi$ we have:
$$a^1 = \frac{\sqrt{2}}{2}\sin(\sqrt{2}\pi),\qquad a^2=\cos(\sqrt{2}\pi)$$ My attempt: We have that $$0=\frac{Da^\lambda}{Dt}=\frac{da^\lambda}{dt}+\Gamma_{\mu\nu}^{\,\,\, \lambda}a^\mu \frac{dx^\nu}{dt}$$
(where $x^1=\theta,x^2=\phi$ is convention, and does not refer to cartesian coordinates). So I have: $$\dot{a}^1=-\Gamma_{\mu\nu}^{\,\,\, 1}a^\mu \dot{x}^\nu=\sin(\theta)\cos(\theta)a^2 \dot{\phi}$$
$$\dot{a}^2=-\Gamma_{\mu\nu}^{\,\,\,2}a^\mu \dot{x}^\nu=-\cot(\theta)(a^1\dot{\phi}+a^2\dot{\theta})$$ Now since $\theta$ stays constant, we have $\theta=\frac{\pi}{4}$ and  $\dot{\theta}=0$, and so the above refines to: $$\dot{a}^1=-\Gamma_{\mu\nu}^{\,\,\, 1}a^\mu \dot{x}^\nu=\frac12a^2 \dot{\phi}$$
$$\dot{a}^2=-\Gamma_{\mu\nu}^{\,\,\,2}a^\mu \dot{x}^\nu=-(a^1\dot{\phi})$$ What to do with $\dot{\phi}$? Should I parametrize $\phi$? Say we take $\phi(t) = 2\pi t$ so that this moves along a circle over $t\in [0,1]$, then we have: $\dot{\phi}=2\pi$ and hence: $$\dot{a}^1=-\Gamma_{\mu\nu}^{\,\,\, 1}a^\mu \dot{x}^\nu=\frac12a^2 2\pi$$
$$\dot{a}^2=-\Gamma_{\mu\nu}^{\,\,\,2}a^\mu \dot{x}^\nu=-(a^12\pi)$$ So we have:
$$\begin{bmatrix}\dot{a}^1\\\dot{a}^2\end{bmatrix} = \begin{bmatrix}0&\pi \\-2\pi&0\end{bmatrix}\begin{bmatrix}a^1\\a^2\end{bmatrix}$$ and solve this in the classical way: Eigenvalues are $\pm \sqrt{2}\pi i$ and eigenvectors:
$$(1,\sqrt{2}i),(1,-\sqrt{2} i)$$ So we have:
$$\begin{bmatrix}a^1\\a^2\end{bmatrix} = e^{\sqrt{2}\pi i t}\begin{bmatrix}1\\\sqrt{2}i\end{bmatrix} + e^{-\sqrt{2}\pi i}\begin{bmatrix}1\\-\sqrt{2}i\end{bmatrix}$$ So $$a^1=\cos(\sqrt{2}\pi t) + i\sin(\sqrt{2}\pi t)+\cos(-\sqrt{2}\pi t) + i\sin(-\sqrt{2}\pi t)$$
$$=2\cos(\sqrt{2}\pi t)$$ Made an error apparently. Is this now the right method? Or something is still wrong?","['transport-equation', 'proof-verification', 'parametric', 'ordinary-differential-equations', 'differential-geometry']"
2318537,Find The Wronskian Of The Following ODE,"Let $y_1$ and $y_2$ be two solutions of the problem, $$\begin{align}
y''(t)+ay'(t)+by(t)=0,t\in \Bbb R\\
y(0)=0\;\;\;\;\;\;\;\;\;
\end{align}$$ where $a$ and $b$ are real constants. Let $W$ be the Wronskian of $y_1$ and $y_2$ . Then ( A ) $W(t)=0\;, \forall t\in \Bbb R$ ( B ) $W(t)=c\;, \forall t\in \Bbb R$ , for some  positive constant $c$ ( C ) $W$ is a non-constant positive function. ( D ) $\exists \;\;t_1,t_2\in \Bbb R $ such that $ W(t_1)<0<W(t_2)\;$ ============================================ My Attempt: Option ( D ) Can be eliminated straight away as the Wronskian will always be strictly positive,strictly negative or zero. I'm pretty confused with the first three options. Since $y_1$ and $y_2$ need not be $2$ independent solutions we cannot directly say the Wronskian vanishes. Hints Please!","['wronskian', 'ordinary-differential-equations']"
2318540,Probability of a survivor in chick-pecking tournament,"This is a follow up to this question : Suppose that $n$ chicks are arranged in a circle. Every chick randomly pecks either the chick to their right or the chick to their left. By the other question, the expected number of unpecked chicks is $n/4$. Instead of ending there, make a tournament out of it. Remove the pecked chicks from the circle and repeat the experiment with the remaining chicks. Iterate as long as possible. It is easy to see that the process ends with either $0$ or $1$ remaining chick. Question: Let $p(n)$ denote the probability that the process ends with $1$ chick. What can be said about $\lim_{n \rightarrow \infty}p(n)$? The following graph shows the result of Monte Carlo simulations which estimate $p(n)$ for all $n$ in the range $1$ to $1000$ (and using $1000$ tournaments for each $n$). The wave-like nature of the graph is interesting. To get a better handle on it, we need the exact probabilities. The following is based on a nice formula by @6005 in the comments to this answer to the other question: $p(0) = 0$ and $p(1) = 1$. For any $n \geq 2$ we have: $$p(n) = \begin{cases}
         \sum_{k=0}^\frac{n}{2} \left(\frac{\binom{n}{2k} + (-1)^k \binom{n/2}{k}}{2^{n-1}}\right) p(k) & \text{if $n$ is even} \\
         \sum_{k=0}^\frac{n-1}{2} \left(\frac{\binom{n}{2k}}{2^{n-1}}\right)p(k) & \text{otherwise}. \end{cases}
$$ The following shows the graph of $p(n)$ from $n=1$ to $1000$: The local maxima and minima appear approximately at powers of $2$ (sometimes shifted by $1$). The maxima are at powers of $2$ which are also powers of $4$ and the minima at the other powers of $2$ (hence of the form $2 \cdot 4^k$). This is somewhat intuitive given that the expected number of survisors in a single round is $\frac{n}{4}$. Furthermore, this expected value is also the most likely value (in the case that $n$ is a multiple of $4$). For example if you start with $128$, the first round could be expected to get you to $32$, the next round to $8$, from thence to $2$, which prompty peck each other, leaving you with $0$. This is a hueristic way of reasoning that becomes somewhat less plausible with each factor of $4$. So the question: Does $\lim_{n\rightarrow \infty} p(n)$ exist, and, if so, to what? My conjecture is that the observed oscillations get damped in the limit, and that the resulting limit is $0.5$, but I do not know how to compute such limits.",['probability']
2318585,Example of a measure on $\mathcal{P}(X)$,Is there an example of a set $X$ and a measure $\mu$ on $\mathcal{P}(X)$ (collection of all the the subsets of $X$) such that $\mu(X)=1$ and $\mu(\{x\})=0$ for all $x$ in $X$? At first I thought it's obvious that there should be such a measure but then I couldn't find any. Now I have a feeling it has something to do with being countably additive. but I don't know why. Is there any lemma or theorem that shows there can't be any measure with this property?,"['real-analysis', 'measure-theory']"
2318606,Is log the only choice for measuring information?,"When we quantify information, we use $I(x)=-\log{P(x)}$, where $P(x)$ is the probability of some event $x$. The explanation I always got, and was satisfied with up until now, is that for two independent events, to find the probability of them both we multiply, and we would intuitively want the information of each event to add together for the total information. So we have $I(x \cdot y) = I(x) + I(y)$. The class of logarithms $k \log(x)$ for some constant $k$ satisfy this identity, and we choose $k=-1$ to make information a positive measure. But I'm wondering if logarithms are more than just a sensible choice. Are they the only choice? I can't immediately think of another class of functions that satisfy that basic identity. Even in Shannon's original paper on information theory, he doesn't say it's the only choice, he justifies his choice by saying logs fit what we expect and they're easy to work with. Is there more to it?","['logarithms', 'information-theory', 'functions']"
2318641,If $AA^T=A^TA=I$ and $\det(A)=1$ then $p_A(1)=0$,"If $AA^T=A^TA=I$ and $\det(A)=1$ then $p_A(1)=0$.
Where $A\in M_3(\mathbb{R})$ My approach: We known from the first equation that $A^T=A^{-1}$ and $\lambda_1\lambda_2\lambda_3=1$. Now,since $A$ and $A^T$ have the same characteristic polynomial, they have the same eigenvalues. We also know that the eigenvalues for $A^{-1}$ are $\frac{1}{\lambda_1},\frac{1}{\lambda_2},\frac{1}{\lambda_3}$ and since $A^T=A^{-1}$ this basically means that $\frac{1}{\lambda_1}=\lambda_1,\frac{1}{\lambda_2}=\lambda_2,\frac{1}{\lambda_3}=\lambda_3$ From here there are 2 possible solutions:
$\lambda_1=\lambda_2=-1,\lambda_3=1$ or $\lambda_1=\lambda_2=\lambda_3=1$ Is my approach correct?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2318662,Why log likelihoods are additive and !log likelihoods are multiplicative,Reading accepted answer for Why we consider log likelihood instead of Likelihood in Gaussian Distribution by user jokek states that total likelihood is product of likelihoods. If apply log to likelihoods then total likelihood is sum instead of product. Why applying log function to likelihoods change computing total likelihood from product to sum ? What is the intuition behind this ?,"['logarithms', 'machine-learning', 'statistics']"
2318677,Laplace Transform and Analytic continuation,"I just started doing complex analysis and came across this notion of Analytic Continuation. Now I was thinking whether it is possible to analytically continue Laplace Transforms as well. What I mean is this: $$\mathcal{L}(\sin(at)) = \int_0^{\infty} e^{-st}\sin(at) \, dt $$
is only valid for $s>0$. BUT $$ \mathcal{L}(\sin(at)) = \frac{a}{s^2 + a^2}$$
is valid unless $s \neq ia$. So is it possible to use the above expression to find out the Laplace Transform for any other values for $s$ ? Thanks!","['complex-analysis', 'laplace-transform']"
2318697,Set of all infinite subsets of natural numbers is equipotent with the power set of natural numbers,"I am trying to prove the following statement:
Set of all infinite subsets of natural numbers is equipotent with the power set of natutal numbers. My thought is
Let the set of all infinite subsets of natural numbers be $S$.
We need to show that there is a bijection from $S$ to the power set of natural numbers. But I have no clue on how to start. Any help would be appreciated.",['elementary-set-theory']
2318741,How is a Stack the generalisation of a sheaf from a 2-category point of view?,"A stack is usually given in terms of: -A category $F$ fibered over another $C$ such that the functor $Hom(x,y), x,y \in F(\alpha), \alpha \in C$ is a sheaf -The descent data are effective. There is an equivalent definition , using the Grothendieck construction, which is a correspondence between fibered categories and pseudofunctors in $Cat$. Given this correspondence a stack becomes a (contravariant) pseudofunctor such that descent is effective. Now, with this definition in mind it seems obvious that a stack is a generalised sheaf. But here is my doubt: When defining a sheaf $T$ over a space $X$, is not unusual to see the following diagram (naively) $ T(X) \rightarrow T(U) \stackrel{\longrightarrow}{\longrightarrow} T(U \cap U)$ While, saying that descent data are effective is like asking a similar diagram, but there is the difference that here descent satisfies cocycle condition, something which is naively in $U \cap U \cap U$. Recalling that (for example) locally constant sheaves automatically satisfy the cocycle condition because of their correspondent covering spaces do. So my question is, at the light of my interpretation, the cocycle condition seems to me the only obstruction to the fact that a stack is a generalised sheaf. Am i wrong? Every sheaf trivially satisfies cocycle condition?","['algebraic-stacks', 'sheaf-theory', 'soft-question', 'algebraic-geometry']"
2318762,"Proof that, for finite $a$, $\lim_{x\to a^-} f(x) = -\infty$ implies $\lim_{x\to a^-} f'(x) = -\infty$","So this is just a conjecture. It might be true as is, but it also might need some further conditions, as a clever manipulation of $\sin(1/x)$ or something could break it (I haven't found a counterexample, I guess that too is a conjecture). Intuitively, it makes sense: if a graph goes downward in a sort of vertical asymptote, then the line tangent to the graph will get closer and closer to vertical in the negative direction (that was less than eloquent, but I hope it was clear). I don't know how to prove the implication. There is very little to work with. I have tried manipulating $\lim_{x\to a^-} f'(x)$ using the limit definition of a derivative, but that gets us nowhere.","['derivatives', 'real-analysis', 'calculus', 'limits']"
2318769,Non singular point in a variety,"I've just came across the following definition: Let $k$ be an algebraically closed field, $X\subset\mathbb{A}^n_k$ an affine variety of dimension $d$ and generators $f_1,...,f_m$. A point $x\in X$ is said to be nonsingular when the matrix $\left(\frac{\partial f_i}{\partial x_j}\right)_{1\leq i\leq m;\,1\leq j\leq n}$ has rank $n-d$. I've tried some examples, like $C=Z(y^2-x^3)\subset\mathbb{A}^2_k$ in which this definition meets the geometric intuition: $C$ has dimension $1$ and is singular at $(0, 0)$, since the rank is $0\neq 2-1$ (this makes sense, because $(0,0)$ is a cusp). But in the example $C'=Z(x^2, xy)\subset\mathbb{A}^2_k$, $C'$ is just a line (hence it has dimension $1$) and, by this definition, $(0,0)$ is singular, since the rank is $0\neq 2-1$. Isn't this weird? How do I interpret that geometrically?","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2318802,Show that $f(z) = c \sin (\pi z)$,"Let $f: \mathbb{C} \to \mathbb{C} $ be an entire function  such that  $f(0)=0, f(z+1) = -f(z)$ and $|f(z)| \leq e^{\pi |\text{Im}(z)|}$ for all $z \in \mathbb{C}$. Show that $f(z) = c \sin (\pi z)$, for some $c \in \mathbb{C}$. I'm trying to use Liouville's theorem. If $z \notin \mathbb{Z}$, then: $$\bigg|\frac{ f(z)}{(e^{i\pi z}- e^{-i\pi z})} \bigg| = 
\frac{|f(z)|}{|e^{i\pi z}- e^{-i\pi z}|} \leq
\frac{e^{\pi |\text{Im}(z)|}}{|e^{i\pi z}- e^{-i\pi z}|}  \leq
\frac{e^{\pi |\text{Im}(z)|}}{|e^{i\pi z}| - |e^{-i\pi z}|} $$ $$= \frac{e^{\pi |\text{Im}(z)|}}{e^{-\pi \text{Im}(z)} - e^{\pi \text{Im}(z)}} =  \frac{1}{e^{-\pi (\text{Im}(z) + |\text{Im}(z)|)} - e^{\pi (\text{Im}(z) - |\text{Im}(z)|)}}$$ I'm stuck here, I don't know how to prove that the last term is bounded.
And I don't know how to proceed when  $z \in \mathbb{Z}$. Could somebody help me out? New approaches are welcome.",['complex-analysis']
2318837,Absolutely Continuous function using sums.,"Below is an exercise from Iowa State's Qualifying Exams in analysis, and a solution I cooked up for it.  I showed it to a friend, and he seems very unhappy with the very end of the proof, though I see nothing wrong with it.  Would anyone mind pointing out whether there are any issues?  Any help is very appreciated! Suppose $\{E_n\}$ is a sequence of Lebesgue measurable subsets of $\mathbb{R}$ with 
$$
\sum_{n=1}^\infty \mu(E_n)<\infty
$$
Let $f(x)=\sum_{n=1}^\infty \mu(E_n\cap [0,x])$.  Prove that $f$ is absolutely continuous on $[0,\infty)$ We need to show the following:  given $\epsilon>0$, there exists some $\delta > 0$ so that 
    $$
 \sum_{k=1}^{N} |f(b_k) - f(a_k) | < \epsilon \quad \text{ whenever } \sum_{k=1}^{N} (b_k-a_k) < \delta
 $$
     and the intervals $(a_k,b_k)$ form a partition of $[0,\infty)$ into pairwise disjoint intervals. By assumption, there exists an $n_0 \geq 1$ such that $\sum_{n \geq n_0} \mu(E_n) < \epsilon/2$. Let $x,y \in [0,\infty)$ be such that $x<y$. By monotonicity of the Lebesgue measure, we have that 
     $$
 |f(y) - f(x)| = \left| \sum_{n=1}^{\infty} \mu(E_n \cap [0,y]) - \sum_{n=1}^{\infty} \mu(E_n \cap [0,x] )\right| \leq \sum_{n=1}^{\infty} |\mu(E_n \cap [0,y]) - \mu(E_n \cap [0,x])|
 $$
     For each $ n \geq 1$, $E_n \cap [0,x] \subset E_n \cap [0,y]$, each of which have finite measure, and so 
     $$
 \mu(E_n \cap [0,y]) - \mu(E_n \cap [0,x]) = \mu ((E_n \cap [0,y]) \setminus (E_n \cap [0,x])) = \mu(E_n \cap [x,y]),
 $$
     for each $ n \geq 1$; consequently, we have that 
     $$
 |f(y) - f(x)| \leq \sum_{n=1}^{\infty} \mu(E_n \cap [x,y]).
 $$
     Choose $\delta = \epsilon/2n_{0}$. By monotonicity of the Lebesgue measure, we see that 
     $$
 |f(y) - f(x)| \leq \sum_{n=1}^{\infty} \mu(E_n \cap [x,y] \leq \sum_{n=1}^{n_0-1} \mu(E_n \cap [x,y] + \sum_{n \geq n_0} \mu(E_n) < \mu(E_n \cap [x,y] + \epsilon/2. 
 $$
     Finally, given any partition $\{x_k,y_k)\}_{k=1^K}$ of $[0,\infty)$ into pairwise disjoint interval with $\sum_{k=1}^{K} (y_k-x_k )< \delta$. Then it follows that 
     $$
 | f(y_k) - f(x_k)| < \sum_{n=1}^{n_0-1} \mu(E_n \cap [x_k,y_k]) + \epsilon/2 \leq \sum_{n=1}^{n_0-1}( y_k-x_k )< \epsilon. 
 $$
     Hence,
     $$
 \sum_{k=1}^{K} |f(y_k) - f(x_k)| < K \cdot \epsilon,
 $$ completing the proof.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'absolute-continuity']"
2318847,Proving a jigsaw is possible,"This is an offshoot of this question . Suppose we have jigsaw puzzle pieces which are basically squares but where each side can be either straight, concave or convex. An example of three such pieces is shown below: It is clear that there can be $3^4=81$ different types of pieces. I was wondering, given one of each type (and with no rotations or flips allowed), whether it was possible to create a standard jigsaw puzzle, using just those $81$ pieces. By ""standard jigsaw puzzle"" I mean one of dimensions $m \times n$ where all perimeter sides are straight. A $1\times 81$ puzzle is clearly not possible as it would require all $81$ pieces to be straight on both the left and the right side. A similar argument holds for a $81\times 1$ puzzle. A $3\times 27$ puzzle would require $27$ pieces with a left straight side and $27$ pieces with a right straight edge, and while it is true that such two sets exist, they have an overlap of $9$ pieces. This is therefore not possible. As before, a similar argument holds for a $27\times 3$ puzzle. This leaves the $9\times 9$ possibility. A priori, I can see no reason this shouldn't be possible. And I think I have a proof that it is possible, which is what I would like your opinion on. Given a $9\times 9$ puzzle, we have the situation below: Each black side of a piece is fixed, but each green side of a piece represents a possible connection type. A connection type could be a ""straight - straight"", ""concave - convex"" or ""convex - concave"" type. It seems to me that if we run through every possible connection type for each piece, one of those scenarios must give a puzzle where each of the $81$ piece types is used exactly once. Am I right?","['puzzle', 'combinatorics', 'recreational-mathematics']"
2318848,How to prove this identify for Catalan zeta function?,"The Catalan Zeta Function is defined as
$$
\beta(n) = \sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)^n}.
$$
For example, we have $\beta(1) = \pi/4$, $\beta(3)=\pi^3/32$, $\beta(5)=5 \pi^5/1536$. I am trying to prove this identity
$$
\beta(2n+1) = (-1)^n \left(\frac \pi 2 \right)^{2n+1}\frac{E_{2n}}{2 (2n)!},
$$
where $E_n$ denotes Euler numbers . As I know that
$$\sec z=1+\frac{z^{2}}{2}+\frac{5}{24}z^{4}+\frac{61}{720}z^{6}+\cdots+\frac{(-%
1)^{n}E_{2n}}{(2n)!}z^{2n}+\cdots.$$
I only need to show that
$$
\sec\left(\frac{\pi x}{2}\right) = \sum_{n \ge 0} \frac{\beta(2n+1)}{\pi/4} x^{2n}.
$$
But I got stuck here. The question comes from The Computer as Crucible (see pp. 60). A clue is to use ""an appropriate product for
$cos(\pi x / 2)$"".",['trigonometry']
