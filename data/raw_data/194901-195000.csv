question_id,title,body,tags
3750309,Exploring the Graphs of a Prime Number Integer Sequence: Seeking Insights,"I am an engineering student and when I was doing some work on data visualisation I stumbled across an integer sequence after watching a video about sequences that produce interesting graphs. I submitted the sequence to the On-Line Encyclopedia of Integer Sequences A328225 . The sequence is as follows:
a(1) = 1, a(n) = a(n-1) - prime(prime(n)) - prime(n-1) if this produces a positive integer not yet in the sequence, otherwise a(n) = a(n-1) + prime(prime(n)) - prime(n-1). This sequence produces the following graph (up to n = 1000, n = 10000 and n = 6e6 respectively) Graph of sequence up to n = 1000 Graph of sequence up to n = 10000 Graph of sequence up to n = 6e6 I'm would love to get a understanding of the underlying principles that give rise to the curves observed in the graphs. Additionally, I'm intrigued by the apparent upper limit observed along the y-axis. I would greatly appreciate any insights, explanations, or conjectures regarding the origins of these curves and the factors contributing to the observed limits. Furthermore, any suggestions on how to further explore or analyse this sequence would be immensely valuable. Here is a text file up to n = 1e6 if anyone wants to have a play around with the sequence. If anyone would like any additional information/resources from me, feel free to ask.","['prime-numbers', 'sequences-and-series']"
3750325,"Meaning of $\int_\Omega\langle\phi,Du(x)\rangle$, in the definition of BV space","I was going through the definition of bounded variation functions on Wikipedia : https://en.wikipedia.org/wiki/Bounded_variation Definition . Let $\Omega$ be an open subset of $\mathbb{R}^n$ . A function $u\in L^1(\Omega)$ is said of bounded variation if there exists a finite vector Radon measure $Du\in \mathcal{M}(\Omega,\mathbb{R}^n)$ such that the following equality holds : $$\int_\Omega u(x)\text{div}\phi(x)dx=-\int_\Omega\langle\phi,Du(x)\rangle\quad\forall \phi\in C_c^1(\Omega,\mathbb{R}^n).$$ I am very confused with this definition, since I am not sure what $\int_\Omega\langle\phi,Du(x)\rangle$ is. I never saw such a notation, and I could not find any reference to this 'vector measure' and corresponding Lebesgue integral. It will be very appreciated if anyone could give me some explanation on this.","['measure-theory', 'bounded-variation']"
3750327,"What is special with the word ""limit"" that it describes ""approaches"" in calculus?","I have been going through calculus class but could not figure out what does the word ""limit"" means here. I mean why do we use this Word not others. I found that the limit word means in Calculus, ""a point or value which a sequence, function, or sum of a series can be made to approach progressively, until they are as close to it as desired"". But can anybody simplify it?","['limits', 'calculus', 'terminology']"
3750329,About the concept bound a function,"From wikipedia: In mathematics, a function f defined on some set X with real or complex values is called bounded if the set of its values is bounded. In other words, there exists a real number M such that $|f(x)| \leq M, \forall x \in X.$ I have a series of conceptual doubts about this: Does strict inequality need to be considered bounded? In wikipedia says that $\arctan(x)$ is bounded since $|\arctan(x)|<  \frac{\pi}{2}$ , but i want to confirm with mathematicians of MSE. The concept of bounded seems symmetric, i mean, what will happen to functions like $f(x) = \sin(x) + c$ , where $c-1 \leq f(x)\leq c+1$ ? in this case, i cant say that there exist an $M$ such that $|f(x)| \leq M$ , so, this function is considered bounded?","['functions', 'upper-lower-bounds']"
3750375,What does it mean to converge to a point if it is not clear what the point even is?,"What does it mean for a sequence to converge to an element if the limit might not necessarily be defined or known, or not necessarily in the universe under consideration (whatever this means)? I am not just talking about real numbers; it can be more general. The definition of a sequence $(x_n)$ converging to $x$ seems to say: for any $\epsilon > 0$ , there is $N \in \mathbb{N}$ such that $|x_n - x| < \epsilon$ for all $n > N$ . But doesn’t this assume the existence of a point $x$ under consideration? For example, when we show that $\{p \in \mathbb{Q}: p^2 < 2\}$ “approaches” a number, e.g. by considering $1.4, 1.41, 1.414, ...$ , what does that even mean, if we have not yet constructed the real numbers? What is meant by “number” in this case? Does it even make sense to say that? Although we might not ""know"" what the limiting point $x$ is for $\{p \in \mathbb{Q}: p^2 < 2\}$ , it seems to still make sense to speak of a limit. (Again, I'm assuming we do not as yet know what real numbers might be, or what completeness is, etc.). In such a case, how should we define convergence, if we cannot have a point to explicitly refer to? In general, couldn’t we converge to “something”, but it is not at all clear what that “something” should or might be? If it isn’t clear what that something should be, then how can we even speak of converging to that something? Is this just a logical/semantic/notational thing in the definition of convergence?","['limits', 'calculus', 'real-analysis']"
3750381,Sums of powers of cosines and sines shifted by $2\pi/3$,"I have stumbled across these two identities $$
\begin{split}
\cos^2(x)+\cos^2(x+2\pi/3)+\cos^2(x+4\pi/3) &= 3/2,\\
\cos^4(x)+\cos^4(x+2\pi/3)+\cos^4(x+4\pi/3) &= 9/8.
\end{split}
$$ There is also the more intricate $$
\begin{split}
\cos^2(x)\sin^2(x)+\cos^2(x+2\pi/3)\sin^2(x+2\pi/3)+\cos^2(x+4\pi/3)\sin^2(x+4\pi/3) &= 3/8,\\
\cos^4(x)\sin^4(x)+\cos^4(x+2\pi/3)\sin^4(x+2\pi/3)+\cos^4(x+4\pi/3)\sin^4(x+4\pi/3) &= 9/128,
\end{split}
$$ and of course the most elementary $$
\cos(x)+\cos(x+2\pi/3)+\cos(x+4\pi/3)=0.
$$ The last identity admits a rather intuitive interpretation in terms of unitary complex numbers centered about the origin. My questions are : Do the other identities admit similar more or less intuitive interpretations as well? Do such identities have names? Not all powers and combinations produce a constant; What is the general form of the expressions that do? Context : The first two identities came up while calculating the elastic response of a two-dimensional truss (a planar lattice of nodes connected with springs) that is invariant by rotations of order 3, in which case $x$ describes the orientation of the truss. We know that such trusses must exhibit an isotropic response and that justifies, in a rather convoluted manner, that these expressions must be constants. The other expressions I found by trial and error. I am looking for a satisfying, non-brute-force, non-too-group-theoretic, explanation.","['trigonometry', 'group-theory', 'permutations', 'geometry']"
3750509,Computing $f \circ g$ and $g \circ f$ for functions by cases,"I want to confirm my solution for the following problem from Ethan D. Bloch’s Proofs and Fundamentals . Problem: Let $f,g: \mathbb{R} \rightarrow \mathbb{R}$ be real functions of a real variable given by $$f(x) = \begin{cases} 1-2x, & \text{if} & 0 \leq x \\ |x|, & \text{if} & x < 0 \end{cases}, \quad \forall x \in \mathbb{R} \quad \text{and}$$ $$g(x) = \begin{cases} 3x, & \text{if} & 0 \leq x \\
x-1, & \text{if} & x < 0 \end{cases}, \quad \forall x \in \mathbb{R}.$$ Find $f \circ g$ and $g \circ f$ . Solution: We start with $f \circ g$ . First, we observe that $0 \leq g(x)$ , for all $x \in [0, \infty)$ . So, $(f \circ g) (x) = 1-6x$ , for all $x \in [0,\infty)$ . Next, we observe that $g(x) < 0$ for all $x \in (-\infty,0)$ . Hence, $(f \circ g)(x) = |x-1|$ , for all $x \in (-\infty,0)$ . Then, we compute $g \circ f$ . Following the same argument, we see that $0 \leq f(x)$ , for all $x \in (-\infty,\frac{1}{2}]$ . So, we have that $(g \circ f)(x) = 3-6x$ , for all $x \in (-\infty,\frac{1}{2}]$ . Next, we observe that $f(x) < 0$ , for all $x \in (\frac{1}{2},\infty)$ . Hence, we have that $(g \circ f)(x)=|x|-1$ , for all $x \in (\frac{1}{2},\infty)$ . Therefore, we have that $f \circ g, g \circ f: \mathbb{R} \rightarrow \mathbb{R}$ are defined by $$
(f \circ g)(x) = \begin{cases} 1-6x, & \text{if} & 0 \leq x \\
|x-1|, & \text{if} & x<0\end{cases}, \quad \forall x \in \mathbb{R} \quad \text{and}
$$ $$
(g \circ f)(x) = \begin{cases} 3-6x, & \text{if} & x \leq \frac{1}{2} \\
|x|-1, & \text{if} & \frac{1}{2} < x \end{cases}, \quad \forall x \in \mathbb{R}.
$$ My question: Is this enough and correct? Is there any online calculator (or the like) that compute this expressions? Thank you for your attention!","['function-and-relation-composition', 'functions', 'solution-verification', 'elementary-set-theory', 'problem-solving']"
3750528,"Given $f(x)$ is continuous on $[0,1]$ and $f(f(x))=1$ for $x\in[0,1]$. Prove that $\int_0^1 f(x)\,dx > \frac34$.","Let $f$ be a continuous function whose domain includes $[0,1]$ , such that $0 \le f(x) \le 1$ for all $x \in [0,1]$ , and such that $f(f(x)) = 1$ for all $x \in [0,1]$ . Prove that $\int_0^1 f(x)\,dx > \frac34$ . Here's all that I have, from the Mean Value Theorem, we have some $c\in[0,1]$ , and $a$ , such that $$a=f(c)=\int_0^1 f(x)dx.$$ By the Extreme Value Theorem, there exist some $m$ , $n\in[0,1]$ such that $$f(m)\ge f(x)\ge f(n).$$ I'm stuck here. Is this the right approach? Where do I go from here? I also got to know what the very fact that $f(f(x))=1$ shows that there is some $x$ such that $f(x)=1$ because the range of $f(x)$ is the domain of $f(x)$ (which I'm still trying to understand; I know what it means, I'm just trying to take it in).","['integration', 'extreme-value-theorem', 'proof-writing', 'continuity', 'calculus']"
3750577,Strictly increasing function $f: \mathbb{N} \to \mathbb{R}^+$ such that $\lim\limits_{n \to \infty} \frac{f(n)}{n} = 0$ and $f(n)+f(3n) \ge 2f(2n).$,"Does there exist a strictly increasing function $f: \mathbb{N} \to \mathbb{R}^+$ such that $\lim\limits_{n \to \infty} \frac{f(n)}{n} = 0$ and $f(n)+f(3n) \ge 2f(2n)$ for all sufficiently large $n$ ? The reason I am asking is that the existence of such a function would give me a nice counterexample to a claim I'm investigating that might be false. The actual claim itself is very broad and appears too good to be true, but this question is concise and specific, so I decided to focus on it. Trying to construct such a function using a greedy algorithm fails epically. You get something that's sort of an arithmetic series, which grows too quickly to satisfy the limit condition. Let $d=f(2)-f(1).$ Then $$f(24)-f(16) \ge f(16)-f(8) \ge f(15)-f(10) \ge f(10)-f(5) \ge f(9)-f(6) \ge f(6)-f(3) \ge f(6)-f(4) \ge f(4)-f(2) \ge f(3)-f(2) \ge d,$$ so $f(24) \ge 5d.$ Continuing this process gives you $f(3F_{n+1}) \ge nd$ and you don't miss many numbers (we dropped down from $16 \to 15, 10 \to 9, 4 \to 3$ ), so perhaps there is an explicit construction involving Fibonacci numbers.","['inequality', 'functions', 'functional-inequalities']"
3750597,Can $f(x)$ defined on an open interval have limits at the endpoints? Can $f(z)$ defined on an open disk have limits at the boundaries?,"For suppose $f$ a function from $(0,1)$ onto $(0,1)$ defined by $f(x)=x$ , then is the limit of $f$ exist as $x$ tends to $0$ or $1$ , I know there only one side limit is finitely defined, but from the other side, it is not defined. Now my question is that is the limit exist still (as vacuously)? Also for a linear complex function $f$ defined on a region $\{ z : |z|<1\}$ , does $f$ have limit value at the boundary points? Since, we can't reach the boundary points from all the directions, as it is not defined! Now what about the existence of limit, is it vacuously exist from all the othe directions(not defined in domain!). And my last question is little different from above. For suppose limit of function have existed at all points of its domain. Then is it possible to find such function that have limit value as limit point of codomain or outside of codomain? Please anyone help , any kind of help would be appreciated!","['limits', 'functions', 'real-analysis']"
3750604,Gradient of trace of a product with a matrix logarithm and Kronecker product,"I'm looking to compute the gradient of a reasonably complicated matrix function, which I can mostly reduce to the following problem. I'm not entirely sure if a closed-form analytic solution is possible. I want to find $\nabla_X f$ , where $$f(X) = \text{tr}\left[X\cdot \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right]$$ Here: $X$ is an $n\times n$ complex, full-rank, positive semi-definite matrix, $\{A_i\}$ is a set of real, $m\times n$ matrices (specifically, this sum is to compute the partial trace of $X$ ), $\mathbb{I}$ is the $n/m \times n/m$ identity matrix, and $\otimes$ is the regular Kronecker product. I don't have much experience with matrix calculus, but it seems at face value like most literature on the topic are cheat-sheet type rules about how to compute different basic derivatives but I don't have a good feel for how to tackle more difficult problems. For example, here, I have seen that $\frac{\partial \text{tr}(F(X))}{\partial X} = f(X)^\dagger $ , where $f$ is the scalar derivative of $F$ but it's not clear to me exactly what this scalar derivative means and I can't seem to find more information or build a ground-up approach. From this, my best guess is that $$\frac{\partial f}{\partial X}^\dagger = \frac{\partial}{\partial X}\left[\text{tr}(X)\right]\cdot\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\text{tr}\left(\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right)\right]$$ which simplifies to $$\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\log\left|\sum_i A_i X A_i^T\right|\right]\cdot\text{Tr}(\mathbb{I}_{n/m})$$ $$= \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + (n/m)\cdot X \cdot \left(\sum_i A_i^T P A_i\right)$$ with $P = \left(\sum_i A_i X A_i^T\right)^{-1}$ . I don't think you can just take the derivatives out of the trace like that, but I don't really know how to proceed with taking the derivative of what's inside the trace and then using the chain rule. Is anybody able to help with this? Am I on the right-track or is there a more systematic way to compute this? Is it even possible to find a closed form expression or should I be resorting to numerics? I know some of the non-commuting aspects to the problem can be tamed by the trace but I'm actually not totally sure which elements should be required to commute in this sense. Many thanks in advance.","['matrices', 'derivatives', 'matrix-calculus', 'linear-algebra']"
3750608,Convergence of $\frac{S_{n}}{\sqrt{n\log n}}$,"Helo I need to prove the following statement: Let $X_{1},X_{2},\ldots,X_{n}$ be $iid$ random variables with density $$f(x) = \frac{1}{|x|^3}I_{|x|>1}$$ Let $S_{t}=\sum_{i=1}^{n}X_{i}$ , show that $$\frac{S_{n}}{\sqrt{n\log n}}\stackrel{d}{\to}\mathcal{N}(0,1)$$ My take: I have shown that $\mathbb{E}[X]=0$ and that $\mathbb{E}\left[X^{2}\right]$ is not defined, so I cannot use the Central Limit Theorem. Which path shall I take? Thanks a lot","['central-limit-theorem', 'weak-convergence', 'expected-value', 'convergence-divergence', 'probability-theory']"
3750609,Does existence of a left or right inverse imply existence of inverses?,"Suppose $G$ is a set with a binary operation such that: (Associativity) For all $a, b, c \in G$ , $(ab)c = a(bc)$ . (Identity) There is $e \in G$ such that, for all $a \in G$ , $ae = ea = a$ . (Left inverse or right inverse) For all $a \in G$ , $ba = e$ for some $b \in G$ or (note the difference with and) $ac = e$ for some $c \in G$ . Does this imply that every element $a \in G$ has an inverse, i.e. an element that is both a left and right inverse? That is, for all $a \in G$ , is there $a’$ such that $aa’ = a’a = e$ ? In other words, is $G$ a group?","['monoid', 'group-theory', 'abstract-algebra', 'inverse']"
3750621,A Nice Problem In Additive Number Theory,"$\color{red}{\mathbf{Problem\!:}}$ $n\geq3$ is a given positive integer, and $a_1 ,a_2, a_3, \ldots ,a_n$ are all given integers that aren't multiples of $n$ and $a_1 + \cdots + a_n$ is also not a multiple of $n$ .
Prove there are at least $n$ different $(e_1 ,e_2, \ldots ,e_n ) \in \{0,1\}^n $ such that $n$ divides $e_1 a_1 +\cdots +e_n a_n$ $\color{red}{\mathbf{My Approach\!:}}$ We can solve this by induction (not on $n$ , as one can see in the answer provided by Thomas Bloom given in the link below). But I approached it in a different way using trigonometric sums. Can we proceed in this way successfully? $\color{blue}{\text{Reducing modulo $n$ we can assume that $1\leq a_j\leq n-1$}.}$ Throughout this partial approach, $i$ denotes the imaginary unit, i.e. $\color{blue}{i^2=-1}$ . Let $z=e^{\frac{2\pi i}{n}}$ . Then $\frac{1}{n}\sum_{k=0}^{n-1}z^{mk} =1$ if $n\mid m$ and equals $0$ if $n\nmid m$ . Therefore, if $N$ denotes the number of combinations $e_1a_1+e_2a_2+\cdots+e_na_n$ with $(e_1,e_2,\ldots, e_n)\in\{0,1\}^n$ such that $n\mid(e_1a_1+e_2a_2+\cdots+e_na_n)$ , then $N$ is equal to the following sum, $$\sum_{(e_1,e_2,\ldots, e_n)\in\{0,1\}^n}\left(\frac{1}{n}\sum_{j=0}^{n-1}z^{j(e_1a_1+e_2a_2+\cdots+e_na_n)}\right)$$ By swapping the order of summation we get, $$N=\frac{1}{n}\sum_{j=0}^{n-1}\prod_{k=1}^{n}(1+z^{ja_k})$$ Clearly, the problem is equivalent to the following inequality: $$\left|\sum_{j=0}^{n-1}\prod_{k=1}^{n}(1+z^{ja_k})\right|\geq n^2\tag{1}$$ Can we prove this inequality?
Any hint or help will be appreciated. Thank you! $\color{red}{\mathrm{Update:}}$ This is actually IMO shortlist $1991$ problem $13$ . No proofs are available except using induction. So if we can prove inequality $(1)$ , it will be a completely new proof! In fact, inequality $(1)$ is itself very interesting. $\color{red}{\mathrm{One\, more\, idea\, (maybe\, not\, useful):}}$ Let $\theta_{jk}=\frac{ja_k\pi}{n}$ and $A=\sum_{k=1}^{n}a_k$ , then we get, $$(1+z^{ja_k})=\left(1+\cos\left(\frac{2ja_k\pi}{n}\right)+i\sin\left(\frac{2ja_k\pi}{n}\right)\right)=2\cos(\theta_{jk})(\cos(\theta_{jk})+i\sin(\theta_{jk}))$$ Therefore, $$\left|\sum_{j=0}^{n-1}\prod_{k=1}^{n}(1+z^{ja_k})\right|=2^n\left|\sum_{j=0}^{n-1}\prod_{k=1}^{n}\cos(\theta_{jk})e^{i\theta_{jk}}\right|$$ So we get one more equivalent inequality, $$\left|\sum_{j=0}^{n-1}\prod_{k=1}^{n}\cos(\theta_{jk})e^{i\theta_{jk}}\right|=\left|\sum_{j=0}^{n-1}e^{i\frac{\pi Aj}{n}}\prod_{k=1}^{n}\cos(\theta_{jk})\right|\geq\frac{n^2}{2^n}\tag{2}$$ $\color{red}{\text{Remark:}}$ According to the hypothesis of the question, $n\nmid A$ . Therefore $e^{i\frac{\pi A}{n}}\neq\pm1$ . Also posted on Mathoverflow The only combinatorial solution using induction is available here .","['trigonometric-series', 'number-theory', 'additive-combinatorics', 'exponential-sum', 'inequality']"
3750630,How to find all planar squares with respect to the taxicab norm or the uniform norm?,"Excuse me if this is a silly question. Let $\|\cdot\|$ be any norm on $\mathbb R^2$ . We say that four points $a,b,c,d\in\mathbb R^2$ form a square with respect to the norm $\|\cdot\|$ if $\|a-b\|=\|b-c\|=\|c-d\|=\|d-a\|$ and $\|a-c\|=\|b-d\|$ . Conceptually, this means the four sides of the polygon with $a,b,c,d$ as vertices have equal lengths and so do the two diagonals. How to characterize all squares w.r.t. the $1$ -norm or the $\infty$ -norm? It isn't hard to see that all ""Euclidean squares"" (i.e. squares w.r.t. to the Euclidean norm) are squares w.r.t. $\|\cdot\|_1$ or $\|\cdot\|_\infty$ , but is the converse true?","['normed-spaces', 'geometry']"
3750681,Closed form for the recurrence,"Is there a closed-form for the following recurrence, if yes, how to find it? $$
f(a,x) = \begin{cases}
       \text{$1$,} &\quad\text{$a\le x$}\\
       \text{$1+f(2(a-x),2*x)$}
     \end{cases}
$$ You can consider the constraints, if necessary, as $1\le a,x\le 10^9$ . Motivation: I want to calculate the no. of times this operation needs to be applied while $a>x$ . To do so, I wrote a simple recursive function in code, which seems to work fast. Hence, I was wondering if a closed-form (formula) exists for this. Code: int count (int b, int x) {
    if (b <= x) return 1;
    return 1+count(2*(b-x),2*x);
}","['functions', 'recurrence-relations', 'recursion']"
3750724,How to progress in Real Analysis?,"I am currently reading Abbott's Understanding Analysis and also trying to solve the problems. However, I get stuck on some problems and wonder why is that the case because people say that Abbott's book is just introductory. I know that getting stuck for a while is a part of learning real analysis but how should one go further? Should I aim at solving all the exercises before moving on to the next section or chapter? Or should I solve as many as I can within a certain time frame and move on to next sections? (Of course I'll get back to the remaining exercises and try to solve them) I do not read the latter sections unless and until I feel like I have understood the preceding ones. I'd like to know how you progressed in the process of learning analysis. Thanks :)","['self-learning', 'advice', 'real-analysis']"
3750756,Find the value of $\frac{1}{\sin^3\alpha}-\frac{1}{\cos^3\alpha}$ given that $\sin\alpha-\cos\alpha=\frac12$,"Given that $\sin\alpha-\cos\alpha=\frac12$ . What is the value of $$\frac{1}{\sin^3\alpha}-\frac{1}{\cos^3\alpha}?$$ My work : $$\sin\alpha-\cos\alpha=\frac12$$ $$\sin\alpha\frac1{\sqrt2}-\cos\alpha\frac1{\sqrt2}=\frac1{2\sqrt2}$$ $$\sin\left(\alpha-\frac{\pi}{4}\right)=\frac1{2\sqrt2}$$ $$\alpha-\frac{\pi}{4}=\sin^{-1}\left(\frac{1}{2\sqrt{2}}\right)$$ I calculated the value of $\sin^{-1}\left(\frac{1}{2\sqrt{2}}\right)\approx 20.705^\circ$ ,
so I got $\alpha\approx 45^\circ+20.705^\circ=65.705^\circ$ I calculated $$\frac{1}{\sin^3\alpha}-\frac{1}{\cos^3\alpha}=\frac{1}{\sin^365.705^\circ}-\frac{1}{\cos^3 65.705^\circ}\approx -13.0373576$$ My question: Can I find the value of above trigonometric expression without using calculator? Please help me solve it by simpler method without solving for $\alpha$ . Thanks","['trigonometry', 'functions']"
3750771,Find the shortest path enclosed by two functions.,"Let $f, g : [a, b] \to \mathbb{R}$ be two  continuous functions such that $$f(x)<g(x)\ \ \forall\ x\in(a,b)$$ Let $P_1\ (x_1,\ y_1)$ and $P_2\ (x_2,\ y_2)$ such that $$a \le x_1,
x_2\le b\ ,\\ \ \ \ \ \ \ \ \ f(x_1) < y_1 < g(x_1)\  and\\ f(x_2) < y_2 < g(x_2)$$ Describe a general way to find the shortest (in length) continuous function $h$ that connects the two points and $$f(x)<h(x)<g(x)\ \ \forall\ x\in[x_1,x_2]$$ Notes You may also consider the case $$f(x)\le h(x)\le g(x)$$ and (optionally): $$\ \ \ \ \ \ \ \ f(x_1) \le y_1\le g(x_1)\  and\\ f(x_2) \le y_2\le g(x_2)$$ You may make any extra necessary assumptions (e.g. differentiability) provided that the problem does not become trivial. (Edit 6:) If you already know the answer telling me which topics I should look into is enough (and you probably don't need to read further). (Edit 4:) Comment Proposed solution (by Christian Blatter): ""Stretch a rubber band from P1 to P2. This band will be straight whenever it does not lie along one of the boundary curves."" If this statement is true, I would very much like to see a proof. (Edit 7:) (Pointed out by TonyK) There's not always a function that satisfies the original argument (not the things in the Notes section). There may only be an infimum for the length of $h$ . (Edit 5:) How the problem arose I was walking on the street which was formed by the arcs of two concentric circles. I was trying to find what trajectory I should follow so that walking ""a given angle"" around the circle and simultaneously ""crossing the street"" I would walk the least. If the line that connects the two points is on the street then problem is trivial. If not I should either: Follow the obvious tangent from $P_1$ to the small circle, walk as much as necessary on this circle to ""find"" the tangent from this circle to the other point etc. Or I should move in such a way so the distance between me and the center of the circles changes at some rate (possibly constant) making a spiral like path. Until now I don't know which option is the best. The original question is more general. The problem can be generalized even further of course. For example one could consider instead of the functions f, g a set of points. Also the problem can be extended in higher dimensions. To save time The post has been edited to include the useful comments made to it. You may skip reading them. However, I encourage you to look at the attempted answers and their comments.","['calculus-of-variations', 'calculus', 'geodesic', 'differential-geometry']"
3750776,"How to find the bounds of the volume integral $\int_\Omega (6xz + 2y +3z^2) \, \text{d} \tilde{x}$?","I'm studying on integrating over volumes and I don't know how to set the bounds in this exercise: Let $\Omega := \left\{ (x,y,z) \in \mathbb{R}^3 \,\big| \,\frac{x^2}{4} + y^2 + \frac{z^2}{9} <1 \right\}$ and $\tilde{x} = (x,y,z)$ . I want to evaluate the following integral \begin{align}
\int_\Omega (6xz + 2y +3z^2) \ \text{d} \tilde{x}.
\end{align} I reckon it's best to start off with the first integrand, using Fubini: \begin{align}
\int_\Omega 6xz \ \text{d} \tilde{x} = 6 \int_\Omega xz \ \text{d}x \text{d}y \text{d}z \\
6 \int_?^? z \int_?^? \int_?^? x \ \text{d}x \text{d}y \text{d}z,
\end{align} but I don't know how to define the bounds for each of the integrals. Obviously it depends on $\Omega$ , and its easy to see that $\Omega$ defines a contorted 3D-ellipsoid. My intuition is somehow using sphere coordinates, but Im just not sure about the exact procedure at all on how exactly can I set the bounds, any ideas? Thanks EDIT: Thanks to @heropup's answer, I can edit my question by the additional knowledge I have now. Mind that Im a newbie, so please point out anything that has room for improvement. First, we can transform $\Omega$ into a unit 3D-sphere, by using the transformation $(x,y,z) \mapsto (2u,v,3w)$ , which gives us \begin{align}
\tilde{\Omega} := \left\{ (2u,v,3w) \in \mathbb{R}^3 \,\Big| \,\frac{(2u)^2}{4} + v^2 + \frac{(3w)^2}{9} = u^2 + v^2 + w^2 < 1\right\}.
\end{align} The corresponding integral (using Transformation theorem) turns into \begin{align}
\int_{\tilde{\Omega}} (36uw + 2v + 27w^2) \det \frac{\partial (x,y,z)}{\partial(u,v,w)} \ \ \text{d} u \text{d} v \text{d} w,
\end{align} where \begin{align} \det \frac{\partial (x,y,z)}{\partial(u,v,w)} = \det
\begin{pmatrix}
2 & 0 & 0\\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
= 6,
\end{align} so \begin{align}
\int_{\tilde{\Omega}} (216uw + 12v + 162w^2) \ \text{d} u \text{d} v \text{d} w.
\end{align} We can now solve this by using spherical coordinates \begin{align}
u = r \, \sin(\phi) \cos(\theta) \\
v = r \, \sin (\phi) \sin(\theta) \\
w = r \, \cos(\phi)
\end{align} and the Jacobian determinate $r^2 \sin(\phi)$ to derive the following integral \begin{align}
\int_0^{2\pi} \int_0^\pi \int_0^1 \left( 216 r \sin(\phi) \cos(\theta) r \cos(\phi) + 12 r \sin(\phi) \sin(\theta) + 162 (r \cos(\phi))^2 \right) \\ r^2 \sin(\phi) \ \text{d} r \text{d} \theta \text{d} \phi
\end{align} So it breaks down to solving this integral.
Is everything correct so far? I would be grateful to know. Cheers","['integration', 'bounds-of-integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
3750800,Why does $\left|\frac{\sin(n+1)}{2^{n+1}}+...+\frac{\sin(n+p)}{2^{n+p}}\right|\leq\frac{|\sin(n+1)|}{2^{n+1}}+...+\frac{|\sin(n+p)|}{2^{n+p}}$ hold?,"I trying to understand a  proof (using Cauchy's general criterion of convergence) of why the series $\sum_{n=1}^{\infty }\frac{\sin (n)}{2^{n}}$ converges .
At the beginning, the following inequality is expressed: $$\left | \frac{\sin (n+1)}{2^{n+1}}+...+\frac{\sin (n+p)}{2^{n+p}} \right |\leq \frac{|\sin (n+1)|}{2^{n+1}}+...+
\frac{|\sin (n+p)|}{2^{n+p}}$$ where $n,p$ are natural numbers. Why does this hold? Is the triangle inequality with more than 2 terms on $\mathbb{R}$ a valid fact (from what seems to be the case here) ?","['algebra-precalculus', 'triangle-inequality', 'inequality', 'sequences-and-series']"
3750801,"Is a compact, connected subset of $\Bbb{R}^n$ whose boundary has empty interior inside it determined by its boundary?","Suppose $A_1,A_2$ are bounded, closed, connected subsets of $\Bbb{R}^n$ , such that $\partial A_i$ has empty interior inside $A_i$ (for both $i$ ). Is it true that if $\partial A_1=\partial A_2$ then $A_1=A_2$ ? This question is inspired by this one , in which instead of asking that $\partial A_i$ should have empty interior inside $A_i$ , it was just required that $\partial A_i\neq A_i$ . If any of the assumptions above are removed, I can find a counterexample. But this seems more involved to me, if there is a counterexample at all. In the title I asked this for general $n$ , but really I am interested in determining the minimal $n$ for which there is such an example (if there is such $n$ ). Clearly this is impossible for $n=1$ , but I'm not even sure about $n=2$ . EDIT . Let me explain what I mean by the sentence "" $\partial A_i$ has empty interior inside $A_i$ "". Since $A_i$ is closed, we have that $\partial A_i\subseteq A_i$ . Now, consider the subspace $A_i$ of $\Bbb{R}^n$ (with the subspace topology). $\partial A_i$ is a subset of this subspace, and one can look at its interior $\mathrm{Int}_{A_i}(\partial A_i)$ , as a subset of the space $A_i$ . I require that $\mathrm{Int}_{A_i}(\partial A_i)= \varnothing$ .",['general-topology']
3750806,Find $2f(x)\cdot f(x-8) - 3f(x+12) - 2 = 0$,"Function $f$ $\in \mathbb{R}$ is odd and has a period of $4$ . On a $[0,2]$ segment function $f$ is defined as $f(x)= 4x - 2x^2$ . Find the set of solutions for the equation: $$2f(x)\cdot f(x-8) - 3f(x+12) - 2 = 0$$ So, here's my attempt: function has a period of $4$ means that $f(x) = f(x+4) = f(x+8) =  f(x+12)$ . Also, from the fact that $f$ is odd, $f(x-8)$ just means $-f(x+8) = -f(x)$ (I might be wrong here). Now let $4x-2x^2 = z$ . We get the equation: $$2z\cdot(-z)-3z-2=0$$ $$-2z^2-3z-2=0$$ $$2z^2+3z+2 = 0 $$ $D < 0$ , so this doesn't have a solution. What am I doing wrong?","['periodic-functions', 'algebra-precalculus', 'functions', 'solution-verification']"
3750858,How to calculate $\lim _{x\to \infty }\left(\frac{x^2+3x+2}{x^2\:-x\:+\:1\:}\right)^x$,"I am trying to calculate $$\lim _{x\to \infty }\left(\frac{x^2+3x+2}{x^2\:-x\:+\:1\:}\right)^x$$ My initial thought is that it is in exponential form $\left(1+\frac{a}{f(x)}\right)^{f(x)}$ . I tried to factor the polynomials $\frac{(x+1)(x+2)}{x(x-1)+1}$ in order to bring it to that form,  but had no success. I also tried to apply the chain rule as following, but found nothing interesting either $$e^{x\ln({\left(\frac{x^2+3x+2}{x^2\:-x\:+\:1\:}\right)}}$$ Any ideas that don't involve D'Hopital's Rule?","['limits', 'calculus', 'limits-without-lhopital']"
3750889,We color each unit square of a table $10\times 10$ with one color so that ...,A table $10\times 10$ is divided in $100$ unit squares. We color each unit square with one color so that no column or row contains more than $5$ colors. How many colors can we use at most? Any idea how to solve it with more graph theoretical aproach? Wrong: I was thinking about to define a graph with $100$ vertices and two are connected if they are of the sam color and in the same line (row or column).Then each component is exactly the set of all vertices of the same color and thus we need to find a maximum number of components...,"['contest-math', 'coloring', 'graph-theory', 'combinatorics', 'discrete-mathematics']"
3750906,On interiors of Jordan curves,"Recently,while studying a problem in physics whose solution required an application of the Jordan curve theorem to the phase space in order to make it rigorous, I asked myself if its possible for two Jordan curves $E$ and $F$ on the plane to be contained in each other's interiors.I tried to prove that it's not,failed,realized there might be some counterexample,and ended up drawing complicated spirals all over the pages of my notebook.I haven't yet got it, so I ask for some help here.Properly phrased,my question is,for two Jordan curves $E$ and $F$ on the plane,it's known that $E$ is contained in the interior of $F$ .Is it possible for $F$ to be contained in the interior of $E$ ?","['plane-curves', 'general-topology']"
3750945,Given a positive integer $t$ does there always exist a natural number $k$ such that $(k!)^2$ is a factor of $(2k-t)!$?,"For all natural numbers $k$ the ratio $$
\frac{(2k)!}{(k!)^2}=\binom{2k}k
$$ is an integer. From staring at the Pascal triangle long and hard, we know that these ratios grow rather quickly as $k$ increases. It is therefore natural to think that may be some factors from the numerator can be dropped in such a way that the ratio would still be an integer. More specifically, can we, for some carefully chosen $k$ , leave out a chosen number of largest factors. In other words, given an integer $t>0$ does there exist a natural number $k$ such that $$\frac{(2k-t)!}{(k!)^2}\in\Bbb{Z}?$$ My curiosity about this comes from a question we had in May . The asker there had found the smallest $k$ that works for each of $t=1,2,\ldots,8$ . In that question it was settled that with $t=9$ the smallest $k$ that works is $k=252970$ . It is natural to think about such divisibility questions one prime factor $p$ at a time. It is well known that if we write a natural number $m$ in base $p$ , $$m=\sum_{i=0}^\ell m_ip^i$$ with the digits $m_i\in\{0,1,\ldots,p-1\}$ , then the highest power of $p$ that divides $m!$ is equal to $$
\nu_p(m!)=\frac1{p-1}\left(m-\sigma_p(m)\right),
$$ where $$\sigma_p(m)=\sum_{i=0}^\ell m_i$$ is the sum of ""digits"" of $m$ in base $p$ . Written in this way, my question asks for a given $t$ , whether there exists a $k$ such that the inequality $$
(2k-t)-\sigma_p(2k-t)\ge 2k-2\sigma_p(k)
$$ holds for all primes $p\le k$ . As we have that slack one might expect this to be possible. But I'm not sure. One obstruction comes from primes just below $k$ . If $k-(t/2)<p<k$ , then $p^2$ is a factor in the denominator, but $2p$ is too large to appear as factor in the numerator, so $p^2\nmid (2k-t)!$ . Occasionally a small prime is also problematic. It is not clear to me how to approach this. A construction may exist. The only thing this reminds me of is the elementary exercise $(k!)^{k+1}\mid (k^2)!$ , but that doesn't seem to apply here. In a comment under the answer to the linked question user metamorphy reports having confirmed this up to $t\le14$ . Edit/Note: The available evidence (see also Sil's comment under this question) suggests that, at least when looking for the smallest $k$ that works for a given $t$ , whenever a chosen $k$ works for an odd number $t$ , that same $k$ also works for $t+1$ . If the main question proves to be too difficult to crack, steps towards explaining this phenomenon are also interesting.","['number-theory', 'divisibility', 'elementary-number-theory']"
3750950,Properties of centralization in group theory,"Recall Let $C\subseteq G$ where $G$ is a group. If $x\in G$ for any $c\in C,$ $xc=cx,$ then $x$ centralizes $C$ $$C_G(C) := \{x \in G : \text{ for any } x\in C, xc=cx \}$$ Clearly, $$C_G(C)=\bigcap_{c\in C}C_G(c)$$ Prove that a) $C\subseteq C_G(C_G(C))$ b) If $C\subseteq D$ then $C_G(D)\subseteq C_G(C)$ c) $C_G(C_G(C_G(C)))=C_G(C)$ My Proof-trying: a) Let $c\in C$ . We will show $c\in C_G(C_G(C)).$ We need to show for any $x\in C_G(C),$ $xc=cx.$ Note that $x\in C_G(C)$ $\iff$ for any $c\in C$ $xc=cx$ by definition. Hence $c\in C_G(C_G(C)).$ Therefore $C\subseteq C_G(C_G(C)).$ b) Assume $C\subseteq D.$ Let $x\in C_G(D).$ Then for any $d\in D$ , we have $dx=xd.$ Since $C\subseteq D$ , then for any $c\in C$ , we have $cx=xc.$ So we are done. c)I couldn't show it. Can you check my proof, if there is a false, can you edit? Can you help for c)? Thanks...","['group-theory', 'proof-writing']"
3750972,What is the difference between average slope and Instant slope(Instantaneous Rate of Change),"I'm starting to learn calculus, and I'm getting confused about what average slope and instant slope(instantaneous rate of change)do and what they're differences are after looking at several sources on the internet. I know that average slope is $\frac{Δy}{Δx
}$ and instant slope is $\frac{dy}{dx}$ . Are these formulas correct? And if they are, what difference is there between Δy and dy?","['calculus', 'derivatives', 'slope']"
3751004,"$x^n+y^n+z^n+u^n+v^n=(x+y+z+u+v)^{n-1}, 3 \le n \le 5$","$$\left\{\begin{eqnarray*}{}
x^5+y^5+z^5+u^5+v^5&=&(x+y+z+u+v)^4\\
x^4+y^4+z^4+u^4+v^4&=&(x+y+z+u+v)^3\\
x^3+y^3+z^3+u^3+v^3&=&(x+y+z+u+v)^2\\
xyzuv&=&1
\end{eqnarray*}\right.$$ Find the number of sets of solutions $(x, y, z, u, v)\in\mathbb R^5$ that satisfy the system of equations. This is the multinomial formula $$(x_1+x_2+⋯+x_m)^n=∑_{k_1+k_2+⋯+k_m=n}{n \choose k_1,k_2,…,k_m}∏^m_{i=1}x_i^{k_i},$$ where $${n \choose k_1,k_2,…,k_m}=\frac{n!}{k_1!k_2!⋯k_m!}$$ This only helps for the expansion of the RHS of the equations, but does not help with the question. I do not know how to continue from this point.","['exponentiation', 'algebra-precalculus']"
3751058,Why is $f(t) = e^{ta}$ differentiable in a unital Banach algebra?,"Let $A$ be a unital Banach algebra. For $a\in A$ , we define $$\exp(a):= \sum_{n=0}^\infty \frac{a^n}{n!}$$ Consider the function $$f: \Bbb{R} \to A: t \mapsto \exp(ta) = \sum_{n=0}^\infty \frac{t^n a^n}{n!}$$ In the book I'm reading, it is claimed that $f'(t) = af(t)$ by differentiating term by term. How can we justify the differentiation term by term? Or what is another way to  show that $f$ is differentiable with $f'(t) = af(t)$ . Maybe some argument with functionals?","['frechet-derivative', 'banach-algebras', 'exponential-function', 'functional-analysis']"
3751080,Find dV/dA in terms of r.,"The volume of a sphere, $V$ cm³, of radius $r$ is given by the formula $V = \frac{4}{3} \pi r^3$ . The surface area of a sphere $A$ cm² of radius $r$ cm is given by the formula $A=4\pi r^2$ . Find $dV/dA$ in terms of $r$ . Here's my workings to the question: $$V= \frac{\frac{4}{3}\pi r^{3}}{4\pi r^2}A = \frac{1}{3}rA$$ So, $$\frac{dV}{dA} = \frac{1}{3}r =\frac{r}{3}.$$ I am not sure about this answer, so it would help to know if anyone got the same answer. Thank you!","['related-rates', 'calculus', 'derivatives']"
3751109,Generalized repetitions of letters with limited amount of adjacent letters,"Say I have the first $x$ letters of the alphabet, and I want to generate a sequence of length $y$ , such that there are no more than $z$ adjacent repeated letters. For example, if $x = 2$ , $y = 3$ and $z = 2$ , here are all the valid sequences: AAB ABA ABB BAA BAB BBA How many total possible sequences are there? Without the restriction of z , the question devolves to $x^y$ , but I am stuck as to how to proceed further. The case I have described earlier is also not too hard, I think it is $x^y-x$ since $z = y-1$ . Would it be better to count all cases and subtract (like I did for the $z = y-1$ case) or should I try and count upwards? To be clear, you are allowed to repeat letters as many times as you want so long as they never lie next to each other more than $z$ times. Thanks for the help!","['permutations', 'combinatorics-on-words', 'combinatorics']"
3751144,Is $\left(\begin{smallmatrix}0&0&1\\1&0&0\\ 0&1&0\end{smallmatrix}\right)$ diagonalizable over $\mathbb{Z}_2$?,"Is $A= \begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix}$ diagonalizable over $\mathbb{Z}_2$ ? I tried two approaches and got two different answers so I was hoping someone could point me to a flaw in my reasoning: First approach: The minimal polynomial for $A$ is easily found to be $m(x) =x^3-1$ which is the same as $x-1$ over $\mathbb{Z}_2$ . Since the minimal polynomial decomposes into distinct linear factors it must be that $A$ is diagonalizable over $\mathbb{Z}_2$ . Second approach: It follows from the minimal polynomial that $1$ is the only eigenvalue of $A$ . The eigenvector equation is $\begin{pmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} x \\ y\\ z \end{pmatrix} = \begin{pmatrix} z \\ x\\ y \end{pmatrix} = 1 \times \begin{pmatrix} x \\ y\\ z \end{pmatrix}  $ and the only solution is $\begin{pmatrix} 1 \\ 1\\ 1 \end{pmatrix}$ . But $\mathbb{Z}_2^3$ has dimension $3$ , so there is no basis for $\mathbb{Z}_2^3$ consisting of eigenvectors for $A$ . $A$ can't be diagonalized over $\mathbb{Z}_2$ . What went wrong? Many thanks!","['eigenvalues-eigenvectors', 'matrices', 'solution-verification', 'linear-algebra', 'diagonalization']"
3751165,"If the coefficients of a quadratic equation are odd numbers, show that it cannot have rational roots","If the coefficients of a quadratic equation $$ax^2+bx+c=0$$ are all odd numbers, show that the equation will not have rational solutions. I am also not sure if I should consider $c$ as a coefficient of $x^0$ , suppose if I take that $c$ is also odd, then $$b^2-4ac $$ will be odd. But that $-b$ (odd), in the quadratic formula will cancel out the oddness of $\sqrt{b^2-4ac}$ , in case if it is a perfect square. If it's not a perfect square then the root is irrational. If I take $c$ to be even, even then the  same argument runs but we noticed that when we take $c$ odd, we get that when discriminant is perfect square, so that means the question asks for $c$ not to be an coefficient. Final question: Is this right to take $c$ as one of the coefficients of the equation $ax^2+bx=c=0$ ?","['algebra-precalculus', 'quadratics']"
3751172,Prove that there exists a shortest distance between two skew lines,"In every geometry textbook it states that the shortest distance between two skew lines (lines that are not co-planar) is given by the unique line that runs perpendicular to both skew lines. This is fairly simple to prove given a shortest line exists (see my comments below). However, how can we prove that between any two skew lines there exists a shortest straight path? I have tried using calculus to show that for lines $L_1 = \mathbf{p}+s\mathbf{u}$ and $L_2 = \mathbf{q} + t\mathbf{v}$ the equation: $$R(s,t) = \Vert \mathbf{p}+s\mathbf{u}-(\mathbf{q}+t\mathbf{v})\Vert^2$$ Has a local minimum However I am not quite able to show that (without getting into page and pages of calculations) and what is more, even after I have shown this, it only proves a local minimum exists.","['calculus', 'vectors', 'geometry']"
3751173,Suppose that $f$ is an entire function satisfying $f(2z)=\frac{f(z)+f(z+1)}{2}$. Show that $f$ is constant. [duplicate],"This question already has answers here : $ 2g(z)=g(\frac{z}{2}) + g(\frac{z+1}{2})$ (2 answers) Closed 3 years ago . I've been working old qualifying exam problems from my university. I've been struggling with the following: Suppose $f$ is an entire function with the property that $f(2z)=\frac{f(z)+f(z+1)}{2}$ for all $z$ . Prove that $f$ must be a constant function. Here are my thoughts Write $f(z)=\sum_{n=0}^{\infty}c_nz^n$ . Then \begin{align*}
2\sum_{n=0}^{\infty}c_n2^nz^n&=\sum_{n=0}^{\infty}c_nz^n+\sum_{n=0}^{\infty}c_n(z+1)^n\\
&=\sum_{n=0}^{\infty}c_nz^n+\sum_{n=0}^{\infty}\sum_{k=0}^{n}c_n\binom{n}{k}z^k.
\end{align*} By equating the coeffients of the power series, we have $$
c_n2^{n+1}=c_n+\sum_{k=0}^{\infty}c_k\binom{k}{n},
$$ or $$
c_n2^{n+1}=2c_n+\sum_{k=n+1}^{\infty}c_k\binom{k}{n}.
$$ Thus $$
(2^{n+1}-2)c_n=\sum_{k=n+1}^{\infty}c_k\binom{k}{n}.
$$ Furthermore, \begin{align*}
(2^{n+1}-2)c_n-(2^{n+2}-2)c_{n+1}&=\sum_{k=n+1}^{\infty}c_k\binom{k}{n}-\sum_{k=n+2}^{\infty}c_k\binom{k}{n+1}\\
&=c_{n+1}(n+1)+\sum_{k=n+2}^{\infty}c_k\left(\binom{k}{n}-\binom{k}{n+1}\right)
\end{align*} However here I am stuck. I realize that these equations should contain everything needed to solve this problem, but I don't see how to use them. Are there any easier ways to approach this problem? Any help is greatly appreciated. Thank you.","['complex-analysis', 'power-series']"
3751197,Reference for a real algebraic geometry problem,"Disclaimer: I am not a mathematician by training. I encountered the following problem in my research. Assume that I have $N$ real variables $x_1, x_2, \dots, x_N$ . I am given $N$ homogeneous polynomials in the $x_i$ unknowns, each with a different degree. More specifically: $$\begin{aligned} P_1 &= \sum_i x_i - c_1\\ P_2 &= \sum_i x_i^2 - c_2\\  &\qquad\vdots \\ P_N &= \sum_i x_i^N - c_N \end{aligned}$$ where $c_1, c_2, \dots, c_N$ are given real coefficients. I need to find, if they exist, real solutions of the above equations. I am asking for references where I can learn the tools needed to attack this type of problems. Thank you.","['reference-request', 'algebraic-geometry', 'symmetric-polynomials', 'polynomials', 'real-algebraic-geometry']"
3751239,Algebra - Steps to rearrange a simple equation,"I have the following equation: $$\frac{1000−6n}{3+10n}=k$$ I know it can be rearranged into this, but I do not quite understand how: $$(5k+3)(10n+3)=5009$$ I know the first thing I can do is multiple both sides by $3 + 10n$ to get: $$1000 - 6n = 3k + 10kn$$ Then I can add $6n$ to both sides to get: $$ 1000 = 3k + 10kn + 6n$$ This is where my math knowledge now dies. I am unsure of what the next steps are. Could someone please explain in an idiot proof way of what needs to be done next to get the answer I am looking for?",['linear-algebra']
3751285,Functions of several variables,"If $f(x,y) = x^2 + xy + y^2 - 3x + 4y - 5$ . I know the domain is $\mathbb R^2$ . How to determine the image of f is my issue.",['multivariable-calculus']
3751288,"Showing that there is always an unbounded $f:X \to \mathbb{R}$ if $X$ is infinite, without choice","Consider the claim ""for any infinite set $X$ , there exists an unbounded $f:X \to \mathbb{R}$ "". If we assume the axiom of choice, this claim is trivial to prove. Indeed, given choice we know there exists an $S \subset X$ which is countably infinite, and thus we map that set to $\mathbb{N} \subset \mathbb{R}$ and map each $x \in X \setminus S$ to $0$ . Can this be proven without choice?","['elementary-set-theory', 'real-numbers', 'axiom-of-choice']"
3751293,Fourier transform with different measure,"I am interested in the following question: can we generalize Fourier theory to different $L^2(\mu)$ spaces, not just the Lebesgue measure?
For example on $\mathbb{R}$ , defining the Fourier transform of $f$ as $$ \mathcal{F}[f](\omega)=\int_\mathbb{R}f(x)e^{-i\omega x}d\mu(x). $$ Is there literature on this subject? Do any properties hold with this (in particular Plancherel’s theorem)?  I have tried to look this idea up online but found nothing. I am a beginner when it comes to Fourier theory, I apologize if it is a simple question.","['harmonic-analysis', 'measure-theory', 'fourier-analysis', 'real-analysis']"
3751311,What is an example of a non-Riemannian manifold?,"A Riemannian manifold $(M,g)$ is a manifold equipped with a metric tensor ( $g$ ) that is both symmetric and positive definite. Now if the metric tensor is symmetric but not positive definite, then it's a pseudo-Riemannian manifold. But what the case of a manifold equipped with a metric tensor $(M,g)$ such that $g$ is not symmetric(regardless of whether it is or is not positive definite)? That is, $\forall (x,y) \ such \ that (\ x \neq  y \ )  \in T_{p}(M) \ g(x,y)\neq g(y,x)$ at any point p in M.","['manifolds', 'riemannian-geometry', 'differential-geometry']"
3751394,"In a metric space, compact implies sequentially compact","I'd like to know if this demonstration is correct. Let $X$ be a metric space and $K \subseteq X$ . Show that if $K$ is compact, then $K$ is sequentially compact. $K$ is compact, therefore every open cover has a finite subcover. Consider then a sequence $\{x_n\}_{n \in \mathbb{N}} \subset K$ and suppose (to find a contradiction) that it has no covergent subsequence, i.e., no element of $K$ is accumulation point for $\{x_n\}_{n \in \mathbb{N}}$ . This means that, for every $x \in K$ exists a $\varepsilon_x$ such that $B_{\varepsilon_x}(x)\cap \{x_n, n \in \mathbb{N}\}$ is finite, where $B_r(x)$ denotes the open ball with radius $r$ centered in $x$ . Note that every set $B_{\varepsilon_x}(x)$ is open and the union over all $x \in K$ obviously covers $K$ . Now, as $K$ is compact by hypothesis, there exists a finite set $K_0 \subset K$ such that $$K = \bigcup_{x \in K_0}B_{\varepsilon_x}(x).$$ Now, observe that $$\{x_n,n \in \mathbb{N}\} = \{x_n,n \in \mathbb{N}\}\cap K = \{x_n,n \in \mathbb{N}\}\cap \left[\bigcup_{x \in K_0}B_{\varepsilon_x}(x)\right]$$ $$=\bigcup_{x \in K_0}\left[\{x_n,n\in \mathbb{N}\}\cap B_{\varepsilon_x}(x)\right].$$ But this last set is finite, as it is a finite union of finite sets. This is absurd as $\{x_n, n\in \mathbb{N}\}$ is infinite, therefore $\{x_n\}_{n \in \mathbb{N}}$ must have an accumulation point. This shows that $K$ compact implies $K$ sequentially compact.","['general-topology', 'solution-verification', 'metric-spaces', 'compactness']"
3751411,"If we pick a sequence of numbers $(a_k)$ at random, what is the expected radius of convergence of $\sum_k a_k x^k$?","Suppose we pick a sequence of positive integers independently and identically distributed from $\mathbb{N}^+$ : call it $(a_k)=(a_0,a_1,a_2,a_3,\ldots)$ . If we consider the corresponding generating function $f(x) = \sum_k a_k x^k$ , what can we say about the radius of convergence $R$ of $f$ ? The Cauchy-Hadamard theorem says $R^{-1}= \limsup_{k\to\infty} \sqrt[k]{|a_k|}$ , but I'm wondering if we can say any more from a probabilistic standpoint. Here are my thoughts (mostly low-hanging fruit) on the problem if the $a_k$ are positive integers. If $(a_k)$ is bounded, we have $R=1$ ; this follows immediately by comparison to the geometric series. I don't think ""most"" positive integer sequences are bounded; in fact, I suspect they are of measure zero in the set of all such sequences. If $a_k = O(k^r)$ for any real $r$ , $R=1$ as well. Likewise, if $a_k = O(M^k)$ , $R=M^{-1}$ ; by the integer stipulation, we must have $M\geq 1$ . If the $a_k$ are positive integers, I don't think we can do better than $R=1$ . I thought about how to generalize the problem if we allow the $a_k$ to be real numbers; I haven't thought about the complex case. Here are my thoughts, again somewhat rudimentary. If the $a_k$ are eventually zero, obviously $R=\infty$ . We can now have $a_k = O(M^k)$ for any $M>0$ (for instance, the Maclaurin series for tangent gives $R=\pi/2$ ) Analyzing convergence at the boundary is probably a lost cause Please feel free to ask for clarification or to change the tags if you think they could be improved. Update: instead of, ""pick a sequence of positive integers independently and identically distributed from $\mathbb{N}^+$ ,"" perhaps I should specify a distribution. After looking at several common models, I think a Boltzmann or logarithmic distribution might be best, but I'm not sure. I realize this is an important aspect of the problem and I'm sorry I don't have a better idea of what to ask.","['expected-value', 'generating-functions', 'power-series', 'convergence-divergence', 'probability-theory']"
3751428,When does $\| AB \| =\| A \| \| B\| $ hold?,"A matrix norm that satisfies $\| AB\| \leq  \|  A\| \|  B \| $ is also a submultiplicative norm, so, if $A,B$ are both square matrices, when does the equality hold?","['matrices', 'matrix-calculus', 'linear-algebra', 'inequality', 'matrix-equations']"
3751491,"Can we extend the monoid $(\mathcal P(A),\cup,\emptyset)$ to a group?","Natural numbers are famously a way to build up ""the rest"" of the numbers: integers as pairs of natural numbers modulo the correct equivalence relation, and similarly for rational numbers, etc. The powerset of some set $A$ has a similar structure to the natural numbers, in that $(\mathbb{N}, +, 0)$ is a monoid, and so is $(\mathcal{P}(A), \cup, \emptyset)$ , but neither has a subtraction that is always nicely behaved. However, one could imagine extending $\mathcal{P}(A)$ in an analogous way to how we obtain the integers, by defining an equivalence relation $$(X, Y) \sim (Z, W) \iff X \cup W = Z \cup Y$$ Essentially making $(X, Y)$ into $X \setminus Y$ , but without losing information in the case that $Y \subsetneq X$ , just as we can represent an integer $a - b$ as $(a,b)$ without losing information when $b > a$ (assuming we use saturating subtraction for natural numbers). At any rate, my question is, does this structure have a name, and if so, does the study of it lead anywhere interesting?","['elementary-set-theory', 'integers']"
3751497,How big can $U\subset\mathbb{C}$ be if there exists a non-constant holomorphic $f\colon U\to\mathbb{C}$ with $2f(2z)=f(z)+f(z+1)?$,"This question was inspired by this other question . Let $U\subset\mathbb{C}$ be open, and let $f\colon U\to \mathbb{C}$ be holomorphic and non-constant . Suppose $f$ also satisfies the following identity: for all $z\in U$ with $z+1,2z\in U,$ we have $$f(2z) = \frac{f(z)+f(z+1)}{2}.$$ The question linked above is basically about proving that such an $f$ cannot be entire, i.e., $U\neq\mathbb{C}.$ In fact, my solution proves a very slightly stronger result, see the following. Let $D\subset U$ be the closed disc of radius $R$ centred at $0.$ Assume, for a contradiction, that $R\geq2.$ The maximum value of $\lvert f \rvert$ on $D$ must be on the boundary, so is of the form $\lvert f(2w) \rvert$ for some $w$ with $\lvert w \rvert = R.$ Since $R\geq2,$ it follows that $w+1$ is in $D$ also. Therefore $\lvert f(2w) \rvert > \lvert f(w) \rvert$ and $\lvert f(2w) \rvert \geq \lvert f(w+1) \rvert$ . By the triangle inequality, $\lvert f(2w) \rvert \leq \frac{\lvert f(w) \rvert + \lvert f(w+1) \rvert}{2} < \lvert f(2w) \rvert,$ a contradiction. This proves that $\{z:\lvert z \rvert \leq 2\} \not\subset U.$ With regard to the above linked question, it therefore follows that $U\neq\mathbb{C}$ , but I'm curious about what else can be said about $U$ . Question. How big can $U$ be? Let's be specific, for the sake of defining the parameters of an answer. In the first place, does there exist a non-constant holomorphic $f\colon \{z:\lvert z\rvert >2, \text{Re}(z)>0\}\to\mathbb{C}$ such that $2f(2z)=f(z)+f(z+1)$ for all $z?$ Does there exist a $U$ admitting such an $f$ such that $U$ only misses out a discrete subset of $\mathbb{C}?$","['complex-analysis', 'examples-counterexamples']"
3751546,Differentiate $\left(x^6-2x^2\right) \ln\left(x\right) \sin\left(x\right)$,"Differentiate $$\left(x^6-2x^2\right)\ln\left(x\right)\sin\left(x\right)$$ with respect to $x$ My work so far $\class{steps-node}{\cssId{steps-node-1}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[\left(x^6-2x^2\right)\ln\left(x\right)\sin\left(x\right)\right]}}$ $=\class{steps-node}{\cssId{steps-node-3}{\class{steps-node}{\cssId{steps-node-2}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[x^6-2x^2\right]}}\cdot\ln\left(x\right)\sin\left(x\right)}}+\class{steps-node}{\cssId{steps-node-5}{\left(x^6-2x^2\right)\cdot\class{steps-node}{\cssId{steps-node-4}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[\ln\left(x\right)\right]}}\cdot\sin\left(x\right)}}+\class{steps-node}{\cssId{steps-node-7}{\left(x^6-2x^2\right)\ln\left(x\right)\cdot\class{steps-node}{\cssId{steps-node-6}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[\sin\left(x\right)\right]}}}}$ $=\class{steps-node}{\cssId{steps-node-8}{\left(\class{steps-node}{\cssId{steps-node-10}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[x^6\right]}}-2\cdot\class{steps-node}{\cssId{steps-node-9}{\tfrac{\mathrm{d}}{\mathrm{d}x}\left[x^2\right]}}\right)}}\ln\left(x\right)\sin\left(x\right)+\left(x^6-2x^2\right)\cdot\class{steps-node}{\cssId{steps-node-11}{\dfrac{1}{x}}}\sin\left(x\right)+\left(x^6-2x^2\right)\ln\left(x\right)\class{steps-node}{\cssId{steps-node-12}{\cos\left(x\right)}}$ $=\left(\class{steps-node}{\cssId{steps-node-13}{6}}\class{steps-node}{\cssId{steps-node-14}{x^5}}-2\cdot\class{steps-node}{\cssId{steps-node-15}{2}}\class{steps-node}{\cssId{steps-node-16}{x}}\right)\ln\left(x\right)\sin\left(x\right)+\dfrac{\left(x^6-2x^2\right)\sin\left(x\right)}{x}+\left(x^6-2x^2\right)\ln\left(x\right)\cos\left(x\right)$ $=\left(6x^5-4x\right)\ln\left(x\right)\sin\left(x\right)+\dfrac{\left(x^6-2x^2\right)\sin\left(x\right)}{x}+\left(x^6-2x^2\right)\cos\left(x\right)\ln\left(x\right)$ $=x\left(\left(\left(6x^4-4\right)\ln\left(x\right)+x^4-2\right)\sin\left(x\right)+\left(x^5-2x\right)\cos\left(x\right)\ln\left(x\right)\right)$ I've had great difficulty in solving this. Was my method correct? Also, would there be any shortcuts in solving this, or is this method the best way of getting the solution?","['calculus', 'derivatives']"
3751548,Asymptotic expansion of sequence,"Let $u_{1} \in \,]0,\pi[$ and $u_{n+1}=(1+\frac 1n)\sin(u_n),\  \forall n >0$ 1- Prove that $\displaystyle \lim_{n\to +\infty} u_n=0$ 2- Prove that $\displaystyle u_{n}=\frac{3}{\sqrt{n}}-\frac{9}{5n\sqrt{n}}+o(\frac{1}{n\sqrt{n}})$ To prove 1. It's easy to show that $u_n\in [0,\frac {\pi}2],\quad \forall n>1$ Put $l=\limsup u_n=\limsup u_{n+1}$ , by result 1 , and result 2 we have $$\displaystyle l=\limsup \big((1+\frac 1n) \sin(u_n)\big)=\limsup \sin(u_n) =\sin(\limsup u_n)=\sin(l)$$ But $\sin l=l$ where $l\in [0,\frac{\pi}2]$ implie $l=0$ . Thus $\displaystyle \lim_{n\to +\infty} u_n=0$ any help to prove 2?","['recurrence-relations', 'asymptotics', 'sequences-and-series']"
3751567,"If $G$ is a locally compact group equipped with a Haar measure, then $L^{1}(G)$ is a Banach algebra with the convolution as the product operation.","I'm currently working on a research project based on John B. Conway's ""A Course in Functional Analysis"", specifically Fourier theory on locally compact groups. In his book, Conway claims that $L^{1}(G)$ has the structure of a Banach algebra with the convolution of functions as a product. Most of the properties follow immediately from standard results in measure theory. However, I'm struggling with the proof of the inequality $\Vert f\ast g\Vert\leq\Vert f\Vert\Vert g\Vert$ . All the proofs I've seen at some point rely on a change in the order of integration, which is easily dealt with by making use of Fubini's theorem, but this only works when the space is $\sigma$ -finite, which apparently isn't true for an arbitrary choice of $G$ . It doesn't look like Conway implicitly assumes that this condition is satisfied since in later sections he does calculations involving discrete groups, which fail to satisfy $\sigma$ -finiteness. So I'm guessing there must be a way around this. I saw on a similar thread that you can use a special case of Young's inequality, but after looking into it, the proof of this inequality is also dependent on the ability to change the order of integration. On the other hand, since the product operation in Banach algebras is required to be associative, I have a similar problem when trying to prove this property for the convolution, since the standard proof of associativity of convolution also makes use of Fubini's theorem. So... what gives? Everything works out fine when the space is $\sigma$ -finite, but seems to break down when we drop this hypothesis. I'd be grateful if you could give me some advice on how to work around these. Edit: I found in the same book that by proving that the set of all finite Borel Measures over $G$ is a Banach algebra with an adequate norm and product then if follows that $L^{1}(G)$ is a Banach algebra with the convolution as the product. So, that seems to solve the problem. However, out of curiosity, can we prove this directly, without resorting to a more general object?","['measure-theory', 'convolution', 'topological-groups', 'haar-measure', 'functional-analysis']"
3751573,Let $G$ be a group with $33$ elements acting on a set with $38$ elements. Prove that the stabilizer of some element $x$ in $X$ is all of $G$.,"I'm trying to figure out this old qualifying exam question: Let $G$ be a group with 33 elements acting on a set with 38 elements. Prove that the stabilizer of some element $x \in X$ is all of $G$ . I think I'm supposed to use the orbit-stabilizer theorem to prove that the orbit of any $x\in X$ must be trivial, i.e. $orb_G(x)=\{x\}$ . This is what I know: $|G|$ and $|X|$ are relatively prime. Since $|orb_G(x)| $ divides $|G|$ we must have that $|orb_G(x)|=1, 3, 11 $ or 33. The orbit of each $x\in X$ partitions $X$ . If $|orb_G(x)|=1$ then by the orbit-stabilizer theorem: $|G|=|orb_G(x)||stab_G(x)| \implies |stab_G(x)|=33$ . I just don't see how to put this together in the right way. I wondered if $|orb_G(x)| $ necessarily needs to divide $|X|,$ but I didn't find anything to support that.","['group-actions', 'group-theory', 'abstract-algebra', 'finite-groups']"
3751626,Probability cumulative sum of dice is n,"The following is intuitive: if $p(n)$ is the probability of ""rolling $n$ as the cumulative sum of arbitrarily many fair dice"" then $p(n)\approx p(m)$ for $m$ and $n$ sufficiently large. This is proven in an answer here but I have a few questions: What exactly has been proven? In other words, how can I formulate this probability more precisely? How can we see that the above claim is true without explicitly computing the distribution? It seems like this should follow from some sort of law of large numbers or result about the limiting distribution of the sum of i.i.d. random variables, but I can't see it. Is there a way to see easily, i.e. without too much computation, that the limiting probability has to be the inverse of the expectation of one roll?","['statistics', 'dice', 'probability']"
3751640,"""Differential Operator"" Over Polynomial Space","Let's suppose we are given the differential operator $T \colon \mathcal{P}_2(\mathbb{C}) \longrightarrow \mathcal{P}_3(\mathbb{C})$ , over the space of quadratic polynomials with complex coefficients, such that $T(p(t)) := p(t) + t^2\,p'(t)$ , and we are asked to find it's kernel. Of course, setting $p(t) := a_0 + a_1\,t + a_2\,t^2$ , where $a_0,\,a_1,\,a_2 \in \mathbb{C}$ , one can easily find that $\text{Ker}(T) = \left\{0\right\}$ , the zero polynomial, with the polynomial equality. But, can we actualy solve the equation $p(t) + t^2\,p'(t) = 0$ ? I know we're going to find an exponential solution of the form $k\exp(1/t)$ , but can we take $k = 0$ (and hence $p(t) = 0$ ) to solve this problem? Thanks in advance!","['differential-operators', 'linear-transformations', 'linear-algebra', 'ordinary-differential-equations']"
3751642,Extension of a differentiable function $f$ to an open superset,"This is a question the book Munkres-Calculus on Manifolds pg.144(Exercise 3-b) If $f :S\to \mathbb R$ and $f$ is differentiable of class $C^r$ at each point $x_0$ of $S$ ,then $f$ may be extended to a $C^r$ function $h: A\to \mathbb R$ that is defined on an open set $A$ of $\mathbb R^n$ containing $S$ . My attempt, with $f:S \to \mathbb R$ is $C^r$ , for each $x \in S$ ,then for each $x \in S$ ,
I can choose $U_x$ open neighborhood of $x$ such that $\cup U_x = A$ open (arbitrary union of abiertos es abierto).The item before ""a""(pg.144.Exercise 3),it is show that if $f$ is $C^r$ then  always exists $g:U_x \to \mathbb R $ where $x \in U_x \subset \mathbb R^n$ such that $f$ is equal to $g$ when $U_x \cap S$ and $$h(x) =\left \{ \begin{matrix} \phi(x)g(x)& \mbox{if }x\mbox{ $\in U_x$ }
\\ 0 & \mbox{if }x\mbox{ $\notin \operatorname{supp} \phi$}\end{matrix}\right. 
$$ Is $C^r$ with $\phi:\mathbb R^n  \to \mathbb R $ is $C^r$ , where your support is in $U_x$ .Choosing $h:A \to \mathbb R $ with the extension of $f$ .","['calculus', 'functions', 'derivatives', 'analysis']"
3751694,$f^*(U_1 \times ... \times U_k) = \bigcap_{i=1}^k f^*_i(U_i)$,"I’m trying to prove this result and I would really appreciate if you could give some feedback in my proof. Result: Let $A,A_1,...,A_k$ be sets, for some positive integer $k$ , let $f: A \rightarrow A_1 \times ... \times A_k$ be a function and let $U_i \subseteq A_i$ for each $i \in \{1,...,k\}$ . Then $$f^*(U_1 \times ... \times U_k) = \bigcap_{i=1}^k f^*_i(U_i).$$ I will be using the map $\pi_i:A_1 \times ... \times A_k \rightarrow A_i$ defined by $\pi_i((x_1,...,x_k))=x_i$ for all $(x_1,...,x_k) \in A_1 \times ... \times A_k$ ( the projection map ). In addition, I will make use of the functions $f_i: A \rightarrow A_i$ defined by $f_i = \pi_i \circ f$ for all $i \in \{1,...,k\}$ ( the component functions of $f$ ) . Proof: In order to show that $f^*(U_1 \times ... \times U_k) = \bigcap_{i=1}^k f^*_i(U_i)$ , we have to show that each of the sets is a subset of the other. Let $a \in f^*(U_1 \times ... \times U_k)$ . We have that $f(a) \in U_1 \times ... \times U_k$ . Then $f(a)=(u_1,...u_k)$ with $u_i \in U_i$ for each $i \in \{1,...,k\}$ . By the definition of component functions, we see that $u_i = (\pi_i \circ f)(a)=f_i(a)$ for all $i \in \{1,...,k\}$ . Hence $f_i(a) \in U_i$ for all $i \in \{1,...,k\}$ . By definition, we have that $a \in f^*_i(U_i)$ for all $i \in \{1,...,k\}$ , so $a \in \bigcap_{i=1}^k f^*_i(U_i)$ . Therefore $f^*(U_1 \times ... \times U_k) \subseteq \bigcap_{i=1}^k f^*_i(U_i)$ . Now, let $b \in \bigcap_{i=1}^k f^*_i(U_i)$ . We have that $b \in f^*_i(U_i)$ for all $i \in \{1,...,k\}$ . By definition, $f_i(b) \in U_i$ for all $i \in \{1,...,k\}$ . This implies that $(f_1(b),...,f_k(b)) \in U_1 \times ... \times U_k$ , so $f(b) \in U_1 \times ... \times U_k$ . By definition, we conclude that $b \in f^*(U_1 \times ... \times U_k)$ . Therefore $\bigcap_{i=1}^k f^*_i(U_i) \subseteq f^*(U_1 \times ... \times U_k)$ . With this, we conclude that $f^*(U_1 \times ... \times U_k) = \bigcap_{i=1}^k f^*_i(U_i)$ . $\square$ Thank you very much!","['proof-writing', 'relations', 'functions', 'solution-verification', 'elementary-set-theory']"
3751726,"If the difference quotient $\frac{f(y)-f(x)}{y-x}$ has a limit along a line $(x,y)\to(c,c)$, does the ordinary derivative $f'(c)$ exist?","Given a function $f:\mathbb R\to\mathbb R$ , we define the difference quotient function $$q(x,y)=\frac{f(y)-f(x)}{y-x}$$ for all $(x,y)\in\mathbb R^2$ not on the diagonal line $x=y$ . The ordinary derivative $f'(c)$ is defined as a limit of $q$ along a horizontal ( $y=c$ ) or vertical ( $x=c$ ) line through $(c,c)$ . The symmetric derivative is a limit along a diagonal line $y-c=c-x$ . The left derivative is a limit along a horizontal ray $y=c,\,x<c$ . The right derivative is a limit along a vertical ray $x=c,\,y>c$ . The strong derivative is the limit of $q$ at $(c,c)$ , not along any particular path. If the ordinary derivative exists, then $q(x,y)\to f'(c)$ along any line through $(c,c)$ , or in any region (a ""cone"") separated from the diagonal by lines through $(c,c)$ : $$q(x,y)=\frac{y-c}{y-x}\cdot\frac{f(y)-f(c)}{y-c}+\frac{c-x}{y-x}\cdot\frac{f(c)-f(x)}{c-x}$$ $$=\frac{y-c}{y-x}\cdot q(c,y)+\frac{c-x}{y-x}\cdot q(x,c).$$ Note that the coefficients sum to $1$ , and we're given $q(c,x)-f'(c)\to0$ as $x\to c$ , so $$q(x,y)-f'(c)=\frac{y-c}{y-x}\big(q(c,y)-f'(c)\big)+\frac{c-x}{y-x}\big(q(x,c)-f'(c)\big)$$ $$\to0,$$ provided that the coefficients $\frac{y-c}{y-x}$ and $\frac{c-x}{y-x}$ are bounded. If $q$ has a limit $f^*(c)$ along a line, say $y-c=k(x-c)$ with $0\neq|k|\neq1$ , does the derivative exist? To simplify the notation, let's assume $c=f(c)=f^*(c)=0$ . We're given, as $x\to0$ , $$\frac{f(kx)-f(x)}{kx-x}=\frac{1}{k-1}\cdot\frac{f(kx)-f(x)}{x}\to0,$$ and we want to know whether $\frac{f(x)}{x}\to0$ . There are discontinuous counter-examples: Let $f(k^n)=1$ for $n\in\mathbb Z$ , and otherwise $f(x)=0$ ; then $q(x,kx)=0\to0$ , but $q(0,x)\not\to0$ . So let's assume that $f$ is continuous at $c$ , and maybe in a neighbourhood of $c$ . Since $q(x,y)=q(y,x)$ is symmetric, the limit along a line with slope $k$ is the same as with slope $1/k$ . So, without loss of generality, $0<|k|<1$ . If the limit is $0$ for two lines with slopes $k$ and $l$ , then it's also $0$ for a line with slope $k\cdot l$ : $$\lim_{x\to0}\frac{f(klx)-f(x)}{x}=\lim_{x\to0}\frac{f(klx)-f(lx)+f(lx)-f(x)}{x}$$ $$=l\cdot\lim_{lx\to0}\frac{f(klx)-f(lx)}{lx}+\lim_{x\to0}\frac{f(lx)-f(x)}{x}=0.$$ Thus, the limit is $0$ for any line with slope $k^n$ where $n\in\mathbb N$ . Now the derivative is $$f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x}$$ $$=\lim_{x\to0}\frac{f(x)-f(\lim_{n\to\infty} k^nx)}{x}$$ and we assumed that $f$ is continuous at $0$ : $$=\lim_{x\to0}\frac{f(x)-\lim_{n\to\infty}f(k^nx)}{x}$$ $$=\lim_{x\to0}\lim_{n\to\infty}\frac{f(x)-f(k^nx)}{x}$$ $$\overset?=\lim_{n\to\infty}\lim_{x\to0}\frac{f(x)-f(k^nx)}{x}$$ $$=\lim_{n\to\infty}(0)=0.$$ Is swapping the limits valid here?","['limits', 'calculus', 'derivatives', 'real-analysis']"
3751744,Increasing function $f: \mathbb{N} \to \mathbb{R}^{\ge 0}$ such that $f(3^n) = 2^n$ and $\forall n \exists c : f(n+c)+f(n-c) \ge 2f(n).$,"Can we build $f: \mathbb{N} \to \mathbb{R}^{\ge 0}$ which is increasing and satisfies the following criterions? $f(3^n) = 2^n$ for all $n.$ There exists $N$ such that for all $n \ge N,$ there exists some $c$ (which may depend on $n$ ) such that $f(n+c)+f(n-c) \ge 2f(n)$ I introduced the first criterion to make it easier to come up with the function on paper. I made a table using graph paper listing $n$ and $f(n)$ for $n = 1, \dots, 33$ and then put $1,2,4,8,\dots$ under the powers of $3.$ Now I'm having difficulties filling out this table so that the second criterion is satisfied Remark: Even if this task is possible, $N$ might be insanely large. However, trying to beg for a larger $N$ is just delaying the inevitable. In any case, I can just let the first $N$ terms be something like $\epsilon, 2\epsilon \dots, \epsilon N,$ but the real challenge begins after I've put down those terms because from then on, there is no violation of the second criterion. With that being said, you might as well try to see if a low value of $N$ works. 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33 1|?|2|?|?|?|?|?|4|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|8|?|?|?|?|?|?| Update: An arithmetic progression between the powers of two almost works. It only  fails right on the powers of $2.$ So maybe there's some way to modify this idea that works.","['functions', 'functional-inequalities', 'analysis']"
3751777,"Prove that $[X, Y] = \frac{\partial^2}{\partial s \partial t} \bigg|_{0} \left[\text{exp}(sX), \text{exp}(tY)\right]$ in a Lie group","Suppose that $G$ is a Lie group, and let $\mathfrak{g}$ be the Lie algebra of $G$ . For each $g \in G$ , we get the conjugate map $C_g: G \to G; x \mapsto g x g^{-1}$ . I know that for each $X, Y \in \mathfrak{g}$ , that $$[X,Y] = \frac{\partial^2}{\partial s \partial t}\bigg|_{0} C_{\text{exp}(sX)} \left( \text{exp}(tY)\right). \tag{1} \label{eq1}$$ The sketch of a proof is covered here on page 3. After testing some examples when $ G = \text{GL}_2(\mathbb{R})$ on Mathematica, identifing the Lie algebra of $\text{GL}_2(\mathbb{R})$ with $M_2(\mathbb{R})$ ,  it seems that the identity $$ [X, Y] = \frac{\partial^2}{\partial s \partial t} \bigg|_{0} \left[\text{exp}(sX), \text{exp}(tY)\right] \tag{2} \label{eq2}$$ may be true for each $X, Y \in \mathfrak{g}$ , where the bracket on the right hand side is the commutator in $G$ given by $[g,h] = g h g^{-1} h^{-1}$ , $g, h \in G$ . For example, both $(1)$ and $(2)$ are true when $G = \text{GL}_2(\mathbb{R})$ , $X = \begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}, Y = \begin{pmatrix}1 & 0 \\ 1 & 1  \end{pmatrix}.
$ Is $(2)$ true in general? If it is, what are some hints to prove it? It is quite similar to the first identity, which at first led me to believe that it is false, but alas it holds for some simple examples.","['lie-algebras', 'lie-groups', 'differential-geometry']"
3751785,The value of $\lim_{n \to \infty}\int_{0}^{1}nx^ne^{x^2}dx$,"The value of $\lim_{n \to \infty}\int_{0}^{1}nx^ne^{x^2}dx$ is _____ I tried by taking the odd $n$ values as in that case, the integral I suppose was easier to calculate.
So, denote $I_n=n\int_{0}^{1}e^{x^2}x^ndx$ , then we have : $I_1=\frac{e-1}{2},\\I_3=\frac{3}{2},\\I_5=5(\frac{e}{2}-1),\\I_7=7(3-e)$ Then I tried calculating (random) values using calculator as integration was getting cumbersome. $I_{31}=2.488$ , $I_{51}=2.57$ . I do not see any kind of recurrence so that I can find a general term for odd $n$ . I also tried the method given here . The idea was that since integral doesn't depend on $n$ considering $I(n)=\int_{0}^{1}e^{x^2}x^ndx$ , then $I'(n)=\int_{0}^{1}x^n\ln(x)e^{x^2}dx$ , but this also doesn't lead me to any conclusion as well. I believe I couldn't get to a proper approach to tackle this question. Can someone please help with the idea that involves in solving these type of questions?","['integration', 'limits']"
3751829,"If $f$ is continuous and $\lim_{n \rightarrow \infty }\frac{f(x_n)-f(0)}{x_n}=1$, is $f$ differentiable? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question In Calculus class I was discussing the sequential criteria for the limit of a function. Students asked me about this question.  If a limit along every sequence of rational numbers exists then that limit of function doesn't need to exist. If $f$ is continuous on $(-1,1)$ and $\lim_{n \rightarrow \infty }\frac{f(x_n)-f(0)}{x_n}=1$ for every sequence $\{x_n\}$ of rational numbers which converges to $0$ , then is $f$ differentiable at $0$ ?","['calculus', 'sequences-and-series', 'real-analysis']"
3751849,"Maximize $u_x(0)$ for a harmonic function $u:D\rightarrow [0,1]$","If $u$ is a harmonic function from $D$ to $[0,1]$ , where $D\subset\mathbb{R}^2$ is the open unit disk centered at $0$ .  What is the maximum of $u_x(0)$ ? My solution: first we assume that $U\supset\overline{D}$ is an open ball and $u:U\rightarrow[0,1]$ is harmonic on $U$ , then there exists an analytic function $f:U\rightarrow\mathbb{C}$ such that $\text{Re }f=u$ on $U$ . The Schwartz integral formula says $$f(z)=\int_0^{2\pi}u(\xi)\frac{\xi+z}{\xi-z}\frac{d\theta}{2\pi}+iK$$ if $z\in D$ , where $\xi=e^{i\theta}$ and $K\in\mathbb{R}$ is a constant. We differentiate this with respect to $z$ , $$f'(z)=\int_0^{2\pi}u(\xi)\frac{2\xi}{(\xi-z)^2}\frac{d\theta}{2\pi}.$$ Take $z=0$ , we have $$f'(0)=\int_0^{2\pi}u(\xi)\frac{2}{\xi}\frac{d\theta}{2\pi}.$$ Hence $$u_x(0)=\int_0^{2\pi}u(e^{i\theta})\cos\theta\,\frac{d\theta}{\pi}.$$ Since $u(e^{i\theta})\in[0,1]$ , the maximum of this integral is $\frac{2}{\pi}$ . If instead $u$ is a harmonic function from $D$ to $[0,1]$ , then what we have shown implies that for any $r\in(0,1),$ we have $ru_x(0)\leq\frac{2}{\pi}$ , therefore, $u_x(0)\leq\frac{2}{\pi}$ . This maximum can be attained: simply stipulate that $u(e^{i\theta})=1$ if $\theta\in[-\pi/2,\pi/2]$ and $u(e^{i\theta})=0$ if $\theta\in(\pi/2,3\pi/2)$ , then use the Poisson's formula with this boundary condition to find such $u$ . Therefore, the maximum is $\frac{2}{\pi}$ . My question: is my solution correct and is there some way to do this without complex analysis? For example if we would like to generalize to $n$ -dimensions. Thanks!","['complex-analysis', 'harmonic-functions']"
3751892,Measurability of a group action on a probability measure.,"Consider a measure space $(X,\mathcal{B})$ where $\mathcal{B}$ is the Borel $\sigma$ -algebra of some topology on $X$ . Suppose that the topological group $G$ acts continuously on $X$ and $G$ is given its Borel $\sigma$ -algebra, $\mathcal{A}$ . Lastly suppose that we have some measure on $X$ , $Q$ . My question is, under what conditions is the map $f:G \rightarrow \mathbb{R}$ defined by $f(g) = Q(g^{-1}B)$ measurable for all (fixed) $B \in \mathcal{B}$ ? I've tried to see if I could show this for open $B$ and then extend this to all sets in $\mathcal{B}$ but I haven't been able to show it for the open case. Thanks for your help.","['group-theory', 'measure-theory']"
3751904,Probability axioms does not make sense?,"Assume a unit square to be sample space (infinite points inside it being its elements). Let the points are $\{p_1, p_2, ...\}$ then, by probability axioms, $$1 = Pr(p_1 \cup p_2 \cup  \cdots ) = Pr(\{p_1\}) + Pr(\{p_2\}) + \cdots  + Pr(\{p_n\}) = \\
					= Pr(p_1) +  Pr(p_2) + \cdots  + Pr(p_n)
					= 0 + 0 + \cdots 		$$ (as Pr of individual point in space is zero) $= 0$ Where do I lack in understanding the logic of axioms?","['axioms', 'probability-theory', 'probability']"
3751951,Prove that the powerset of a finite set is finite. (correct proof or abuse of definitions?),"Let $ A $ be a finite set, and prove that $ \mathcal{P}\left(A\right) $ is also finite. Here's what I've done: Since $ A $ is finite, we can assume that $ |A|=n $ for some natural number $ n\in \mathbb{N} $ . From the assumption above, it follows that there exists a bijection $ f:\mathbb{N}^{<n}\to A $ . We'll define $ g:\mathcal{P}\left(A\right)\to\left\{ 0,1\right\} ^{\mathbb{N}^{<n}} $ by: For any $ B\in\mathcal{P}\left(A\right) $ $ g\left(B\right)\left(m\right)=\begin{cases}
0 & f\left(m\right)\notin B\\
1 & f\left(m\right)\in B
\end{cases} $ I'm sure we all agree that $ g $ is a bijection. And therefore $ |\mathcal{P}\left(A\right)|=|\{0,1\}^{\mathbb{N}^{<n}}| $ . Now, by definition, for any sets $ A,B $ such that $ |A|=\alpha,|B|=\beta $ , the cardinality of $ A^B $ defined as $ |A|^{|B|}=\alpha^{\beta} $ . In our case, by the definition, $ |\{0,1\}^{\mathbb{N}^{<n}}|=2^{n} $ , because $ |\{0,1\}|=2,|\mathbb{N}^{<n}|=n $ . Thus, we get that $ |\mathcal{P}\left(A\right)|=2^{n}\in\mathbb{N} $ . And since $ 2^{n}<\aleph_{0} $ , we get that $ \mathcal {P}(A) $ is finite. This proof is legit? Or maybe I've abused the definitions? I'm asking because this question appeared in my exam (it wasn't written that we have to prove by definition of finite set, so I proved my way). Thanks in advance.","['elementary-set-theory', 'cardinals', 'solution-verification']"
3752053,Attractors of nonlinear dynamical systems on the sphere,"This is related to my other recent question , which involved linear flows on the unit sphere. Here, we are going to consider nonlinear flows. Let $\mathbb S^{d-1}=\{x\in\mathbb R^d\ :\ x^Tx=1\}$ denote the unit sphere. Let moreover $A$ be a real $d\times d$ matrix. Following this MathOverflow question we consider the unique solution $x(t, x_0)$ to the nonlinear initial value problem $$
\dot{x}=(I-x x^T)Ax, \quad x(0)=x_0\in \mathbb S^{d-1}. $$ For $x\in \mathbb S^{d-1}$ , that is $\lvert x \rvert^2=1$ , we see that $$\tfrac{d}{dt}(x^T x)=x^TA^Tx-x^TA^Tx\lvert x\rvert^2+x^TAx - \lvert x \rvert^2 x^TAx=0,$$ so $x(t, x_0)$ remains on $\mathbb S^{d-1}$ for all $t>0$ . Now the linked MathOverflow question states, without proof, that if $A$ is negative semi-definite, then $x(t, x_0)$ converges to a stable equilibrium. Can you prove an appropriate version of this statement? Here's some of my thoughts. I can think of two versions of the statement to prove. But I cannot prove either of them. First of all, it is easy to see that the normalized eigenvectors of $A$ correspond to equilibria; precisely, if $Av=\lambda v$ and $v\in \mathbb S^{d-1}$ then $$
\left.\tfrac{d}{dt} x(t, v)\right|_{t=0}= (I-vv^T)\lambda v=0,$$ which implies that $x(t, v)=v$ for all $t\ge 0$ . This leads me to think that the ""stable equilibrium"" mentioned in the statement above is an eigenvector. The two conjectures follow. Conjecture 1 . For each $x_0\in \mathbb S^{d-1}$ there is an eigenvector $v\in\mathbb S^{d-1}$ of $A$ such that $x(t, x_0)\to v$ as $t\to \infty$ . Conjecture 2 . (stronger). Let $\lambda_j$ denote the eigenvalues of $A$ and suppose that $0>\lambda_1>\lambda_j$ for all $j>1$ , and that $\lambda_1$ is non-degenerate. Let $v\in \mathbb S^{d-1}$ be a $\lambda_1$ -eigenvector of $A$ . Then $$
x(t, x_0)\to v,\quad \text{or}\quad x(t, x_0)\to -v$$ as $t\to \infty$ , unless $v^Tx_0=0$ . (In the latter case the system never leaves the $(d-2)$ dimensional sphere $\{x\in\mathbb R^d\ :\ v^Tx=0,\ \lvert x\rvert^2=1\}$ ).","['nonlinear-analysis', 'spheres', 'ordinary-differential-equations', 'linear-algebra', 'dynamical-systems']"
3752127,Finding the roots of the unknown function,"The graph of the function $f (x)$ is given. How many different real roots does the $f (4-3x^2)=0$ equation have? Here is possible solution: $$\Bigg [{4-3x^2=-2 \\ 4-3x^2=4} \Longrightarrow x=\left\{0,\sqrt 2,- \sqrt 2\right\} $$ Then, here is different solution: $$f(x)=x^2-2x-8$$ $$f(4-3x^2)=0\Longrightarrow (4-3x^2)^2-2(4-3x^2)-8=0 \Longrightarrow 9x^4-18x^2=0 \Longrightarrow x=\left\{0,\sqrt 2,- \sqrt 2\right\} $$ Problematic point. Here, I assumed the function is quadratic. But, obviously, this function may not be quadratic. My questions: Question $-1:$ Is the first solution completely correct? Question $-2:$ Can we say that, the second solution is completely and definitely wrong? If so, is it possible to add something to this method and turn it into the right solution?","['quadratics', 'algebra-precalculus', 'functions', 'solution-verification']"
3752160,Is there any equation which looks like this graph in picture,"I was wondering if there is a function
which represent a graph in which
there is straight line with slope m and grows to $y=a$ then decreases at the same rate
once it reaches x axis it grows in negative direction with same slope and same way but with amplitude $a/n$ and so on like a dying heartbeat please help","['functions', 'graphing-functions']"
3752162,"Why ""normal subgroups occur as kernels of homomorphisms"" is a big deal?","I already knew that normal subgroups where important because they allow for quotient space to have a group structure.
But I was told that normal subgroups are also important in particular because they are the only subgroups that can occur as kernels of goup homomorphisms. Why is this property a big deal in algebra?","['normal-subgroups', 'abstract-algebra', 'intuition']"
3752196,"Prove that $|V_\alpha|=|\operatorname{P}(\alpha)|$ if and only if $\alpha=\{2,\omega+1\}$ or $\alpha=\kappa+1$, $\kappa=\beth_\kappa$","$\kappa$ is a cardinal, $V_\alpha$ belongs to the Von Neumann hierarchy $\begin{cases}  V_0=\emptyset \\ V_{\alpha+1}=P(V_\alpha) \\ V_\lambda=\underset{\gamma<\lambda}{\bigcup}V_\gamma \end{cases}$ and the Beth function is defined in this way: $\begin{cases}  \beth_0=\aleph_0 \\ \beth_{\alpha+1}=2^{\beth_\alpha} \\ \beth_\lambda=\underset{\gamma<\lambda}{\bigcup}\beth\gamma \end{cases}$ It's easy to see that $|V_0|\ne|\operatorname{P}(0)|, \; |V_1|\ne|\operatorname{P}(1)|, \; |V_2|=|\operatorname{P}(2)|$ and, for countable recursion, I prooved that $\forall n\in\omega \; |V_n|>|\operatorname{P}(n)|$ . $V_\omega$ is countable, whereas $|V_{\omega+1}|=2^{|V_\omega|}
=2^{\aleph_0}=|\operatorname{P}(\omega+1)|.$ Then, $\forall \; \omega+2<\alpha<\omega^2 \quad |V_\alpha|>2^{\aleph_0}=|\operatorname{P}(\alpha)|$ because these $\alpha$ are countable. Now, for ordinals $\alpha\geq\omega^2$ I use this fact: $|V_\alpha|=\beth_\alpha$ . Let be $\kappa$ a cardinal, $\forall\alpha+2$ such that $|\alpha|=\kappa$ , then $|V_{\alpha+2}|=\beth_{\alpha+2}=2^{\beth_{\alpha+1}}>\beth_{\alpha+1}=2^{\beth_{\alpha}}\geq2^{|\alpha|}\geq2^{\kappa}=\operatorname{P}(\alpha+2)$ . Cardinals and successor of cardinals are left. $\forall\kappa$ cardinal $|V_\kappa|=\sum_{\gamma<\kappa}{|V_\gamma|}=\max\{\sup_{\gamma<\kappa}{|V_\gamma|,\kappa}\}$ and I don't know how to show that it isn't equal to $|\operatorname{P}(\kappa)|.$ If $\kappa$ is a fixed point of Beth function, then $|V_{\kappa+1}|=|\operatorname{P}(\kappa+1)|$ , if $\kappa$ isn't a fixed point, it shouldn't be true, but I don't know how to go on.","['elementary-set-theory', 'ordinals', 'cardinals']"
3752205,Prove that every permutation matrix satisfies its characteristic polynomial.,"Let $P$ is a permutation matrix which represents the permutation $\sigma\in S_{n}$ . Let $\sigma_{1}$ , $\sigma_{2}$ ,... $\sigma_{k}$ denote the disjoint permutations in the cycle form of $\sigma$ .
Let $P_{i}$ and $c_{i}$ represents the permutation matrix corresponding to the permutations $\sigma_{i}$ and the cycle lengths of $\sigma_{i}$ respectively. Prove that P satisfies the equation (its characteristic polynomial) $$\prod_{i=1}^{k}(P^{c_{i}}-I) =0$$ I know the following facts: For disjoint permutation matrices $P_{i},P_{j}$ we have $(P_{j}-I)(P_{i}-I) = 0$ . For disjoint permutation matrices $P_{1},P_{2},\cdots,P_{k}$ we have $$\prod_{i=1}^{k}P_{i} = \sum_{i=1}^{k}P_{i}-(k-1)I.$$ If P and Q are disjoint permutation matrices so are $P^{m}$ and $Q^{n}$ , $\forall m,n\in \Bbb N$ . If P and Q are disjoint permutation matrices they commute. If P is a single-cycled permutation matrix with cycle length $k$ then $P^{k}=I$ . Combining the fact $2$ and $3$ for disjoint permutation matrices $P_{1},P_{2},\cdots,P_{k}$ we also have $$\prod_{i=1}^{k}P_{i}^{m} = \sum_{i=1}^{k}P_{i}^{m}-(k-1)I, \forall m\in \Bbb N.$$ Combining the fact $1$ and $3$ we have $(P_{j}^{n}-I)(P_{i}^{m}-I) = 0$ for any $n,m \in \Bbb N.$ MY TRY: I tried with an case  where $P$ breaks in two single-cycled disjoint permutations $Q$ and $R$ with cycle lengths $m,n$ respectively. We need to prove that $$(P^{n}-I)(P^{m}-I) = \Bigr((QR)^{n}-I\Bigl)\Bigr((QR)^{m}-I\Bigl) = 0$$ Using fact $4$ and $5$ $$\Bigr((QR)^{n}-I\Bigl)\Bigr((QR)^{m}-I\Bigl)=\Bigr(Q^{n}R^{n}-I\Bigl)\Bigr(Q^{m}R^{m}-I\Bigl) =  \Bigr(R^{n}-I\Bigl)\Bigr(Q^{m}-I\Bigl)$$ The fact $7$ as stated above states that it vanishes.
But it gets more calculative when P breaks in $3$ disjoint single cycled permutations. Also, generalisation would need more calculations. I do not know Cayley hamilton theorem. I am new to group theory. Please ask for clarifications if something is not clear. Any hint would be a great help.","['permutation-matrices', 'abstract-algebra', 'linear-algebra', 'characteristic-polynomial', 'group-theory']"
3752278,For which $\alpha>0$ does $x\le|W(-cx^2)|^{-\alpha}$,"Let $x\in(0,1)$ . I want to know for which $\alpha>0$ it's true that $$
x\le|W(-cx^2)|^{-\alpha},\label{1}\tag{$\ast$}
$$ where $W$ is the Lambert W-function and $c>0$ is some constant. In my numerical tests, the value of $c$ didn't really seem to matter, but \eqref{1} seemed to hold for very small $\alpha$ , for example $\alpha\approx 0.001$ . It seems difficult to prove analytically because of the non-elementary nature of the Lambert W function. For negative $y<0$ , it seems to be true that $W(-y)<0$ . So we can rewrite \eqref{1} as $$
x(-W(-cx^2))^{-\alpha}\le 1.
$$ We can define a function $f(x)=x(-W(-cx^2))^{-\alpha}$ . Then $f(0)=0$ , $f>0$ on $(0,1)$ and $f\in C^1$ since $W$ is differentiable on $(0,1)$ as it does not include the points $\{0,\frac{1}{e}\}$ . So the maximum of $f$ reached at $x_0$ should satisfy $$f'(x_0)=0\label{2}\tag{$\ast\ast$}$$ where $$
f'(x)=\left(-W(-cx^2)\right)^\alpha\left(1-\frac{2 c\alpha x^2 W'(-cx^2)}{W(-cx^2)} \right),
$$ So \eqref{2} is \begin{align*}
&\left(-W(-cx_0^2) \right)^\alpha-2c\alpha x_0^2\left(-W(cx_0^2) \right)^{\alpha-1}W'(-cx_0^2)=0
\\
\iff& \alpha=-c'\frac{W(-cx_0^2)}{x_0^2 W'(-cx_0^2)}
\\
\iff& x^2_0\frac{\mathrm{d}}{\mathrm{d}x_0}\log\left( W(-cx_0^2)\right)=-c''\frac{1}{\alpha},
\end{align*} But I don't see how to go from here, i.e., how to invert the function $$
\psi(x)=x^2\frac{\mathrm{d}}{\mathrm{d}x}\log\left(W(-cx^2)\right)
$$ to recover $x_0$ as $$
x_0=\psi^{-1}\left(-c''\frac{1}{\alpha}\right),
$$ and plug that back into \eqref{1}. But in Mathematica, it gives $$
c'''\psi^{-1}\left(-c''\frac{1}{\alpha}\right)=\pm c'''\left(\alpha W\left(\mp c'' i\frac{1}{\sqrt{\alpha}}\right)\right)^{-\frac{1}{2}},
$$ which isn't very helpful!","['logarithms', 'real-analysis', 'lambert-w', 'inequality', 'exponential-function']"
3752312,Proof verification: $f$ is convex iff $f'$ is monotonically increasing,"This is (the first half of) exercise 14 in Baby Rudin Let $f:(a, b) \to \mathbb{R}^1$ be differentiable. Prove that $f$ is convex iff $f'$ is monotonically increasing. ( $\Rightarrow$ ) Assume $f$ is convex in $(a, b)$ . Fix $0 < \lambda < 1$ and $a < y \le x < b$ . Notice that \begin{align}\tag{13.1}
        y \le x \implies y (1-\lambda) \le x (1-\lambda) \implies y - \lambda y \le x -\lambda x \implies \lambda x + y - \lambda y \le x
    \end{align} By the definition of convexity, we have that \begin{equation}\tag{13.2}
        f[\lambda x + y - \lambda y] \le \lambda f(x) + f(y) - \lambda f(y)
    \end{equation} and differentiating (13.2) with respect to x, we have \begin{equation}\tag{13.3}
        f^{\prime}[\lambda x + y - \lambda y] \cdot \lambda \le \cdot \lambda f^{\prime}(x) \implies f^{\prime}[\lambda x + y - \lambda y] \le f^{\prime}(x)
    \end{equation} By (13.1) and (13.3), we conclude that $f'$ is monotonically increasing. ( $\Leftarrow$ ) Suppose $f'$ is monotonically increasing. Fix $0< \lambda< 1$ and suppose $f$ is not convex. Then $\exists p, q \in (a, b)$ such that \begin{equation}\tag{13.4}
        f(\lambda p + q - \lambda q) > \lambda f(p) +f(q) - \lambda f(q) \stackrel{\textrm{w.r.t. } p}{\implies} f'(\lambda p + q- \lambda q) > f'(p)
    \end{equation} Without loss of generality, let $p \geq q$ , which implies \begin{align*}
        p (1-\lambda) > q (1-\lambda) \implies p - \lambda p > q -\lambda q \implies p > \lambda p + q - \lambda q 
    \end{align*} Since $f'$ is monotonically increasing, we get $f'(p) > f'(\lambda p + q- \lambda q)$ which contradicts (13.4). Can someone please critique my proof? Please don't bother suggesting a new proof as those can be found here and here . I am new to handling derivatives in an abstract setting, so I am not sure if it is valid to differentiate (13.3) and preserve the direction of the inequality, like I did. Is there a theorem/ lemma that supports this move?","['general-topology', 'solution-verification', 'convex-analysis', 'real-analysis']"
3752320,Inverse of $y=1/x$,Can anybody say anything about the inverse function of $y=1/x$ and plot it on a graph and then compare the graphs of the given function and it's inverse? Is $y=1/x$ invertible? If yes then do the graphs of $y=1/x$ and its inverse coincide?,"['calculus', 'functions', 'graphing-functions', 'inverse-function']"
3752331,Markov chains - first hitting times between transient states satisfy either $\mathbb{P}^i(T_j < \infty) < 1$ or $\mathbb{P}^j(T_i < \infty) < 1$,"We have a irreducible Markov chain $X=(X_n)_{n\geq0}$ on a state space $I$ with transition matrix $P$ and we denote by $T_i=\inf\{n\geq1: X_n=i\}$ the first hitting time of state $i$ . I have to prove the following: Let $i\neq j \in I$ be connected transient states. Then either $\mathbb{P}^i(T_j < \infty) < 1$ or $\mathbb{P}^j(T_i < \infty) < 1$ . First I need to show that $\mathbb{P}^i(T_j < \infty) < 1$ or $\mathbb{P}^j(T_i < \infty) < 1$ hold. And then if one of them holds, the other doesn't. I have a lack of intuition here. If $i$ is a tranient state, that means, starting in $i$ , we visit $i$ only a finite number of times ( $\mathbb{P}^i(V_i < \infty) =1$ ). I have already shown, that an equivalent definition of transience is that $\mathbb{P}^i (T_i < \infty) <1$ . If I now assume that both $\mathbb{P}^i(T_j < \infty) < 1$ and $\mathbb{P}^j(T_i < \infty) < 1$ hold, this would mean that starting in $i$ there is a possibility to never hit $j$ and that starting in $j$ , there is the possibility to never hit $i$ . I don't see, where this gives a contradiction. Maybe this is the point where it kicks in that $i$ and $j$ are connected, so there has to be a positive probability of going from $i$ to $j$ or (and?) from $j$ to $i$ . Could you please help me to concretize my intuition and give me hints for a formal proof of the statement? UPDATE: I came in contact with the person stating the result which should be proven. The ""either ... or..."" is not an exclusive or as I thought, it is just the normal inclusive or.","['conditional-probability', 'markov-process', 'markov-chains', 'probability']"
3752380,What's the point of obtuse angle trigonometry?,What is the point of $\sin$ or $\cos$ or $\tan$ of an obtuse angle? Don't we use these functions to find a missing side in a right angle triangle? So why would I use it on an obtuse angle? I have also done a unit circle.,['trigonometry']
3752436,"How I can solve this differential equation: $y'''(x)+ax\,y(x)=0$?","I have done some attempts to solve the following ODE: $$y'''(x)+ax\,y(x)=0\,,$$ with $a >0$ . I put $y(x)= e^{rx}$ , with $r$ is an arbitrary  real number, then $y'''=r^3 e^{rx}$ , by substitution in the titled equation I got: $r^3 e^x +a x e^{rx}=0$ implies $ e^{rx}(r^3+a x)=0$ implies $x=\frac{-a}{r^3}$ this yield to $y(x)= \exp(\frac{-a x}{r^3})$ , But I didn't got the same result using wolfram alpha as shown here , The result were given by Generalized hypergeometric function, Any help?","['mathematical-physics', 'hypergeometric-function', 'ordinary-differential-equations', 'real-analysis']"
3752471,Heron's Formula Intuitive Geometric Proof,"This is not the usual request for an intuitive proof, which has been asked already. Having looked at various sources, I've basically concluded that Heron's formula relies on proving $$xyz = x+y+z$$ where $x$ , $y$ and $z$ are the lengths between the meeting point of the incircle and the sides, and the vertices. If I take the diagram below, $x$ is $CX$ , and $z$ is $XA$ . $y$ would be the segment from $B$ to the incircle meeting the side, marked by a black dot, which is also $YC$ . Hence $x+y+z=s$ where $s$ is the semiperimeter.
From here it is easy to show that if the radius ( $r$ ) of the incircle is $1$ , then the area of the triangle is $x+y+z$ . The radius does not really matter, because when it comes to proving the formula, you can just proportionally reduce the size of the triangle by a factor of $1/r^2$ . So we can work with the $r=1$ case. Proving $xyz = x+y+z$ visually is not difficult by looking at this diagram: http://jwilson.coe.uga.edu/emt725/Heron/Heron2/Heron2.html When $r = 1$ , then $EY = xy$ ( $x = (s-c)$ and $y = (s-b)$ since $s = x+y+z$ as defined above) and $EY$ is also equal to $s/z$ , so it is conceptually not difficult to show that $x+y+z = xyz$ and from this that $A^2 = (x+y+z)xyz = s(s-a)(s-b)(s-c)$ . But to my mind, it would be even better if we could show that $xyz$ corresponds to the area of the triangle directly, rather than messing about with equivalences. If we take the $\Delta AYE$ triangle in the above diagram and form a rectangle, whose new vertex we call $P$ , and we extend a line from point $X$ to the side $EP$ , calling this new vertex $Q$ , then $PQXA$ is meant to be of the same area as the triangle, since $XA=z$ and $EY=xy$ . Any ways to prove that rectangle $PQXA$ is equivalent to triangle $\Delta ABC$ ? Or perhaps there's a better way to prove $xyz$ corresponds to the area of the triangle $\Delta ABC$ ? Cheers","['euclidean-geometry', 'triangles', 'geometry']"
3752494,How can we tell the relationship between two lim-sup sets?,"Let $f_n(x)$ be sequence of functions, and $\epsilon>0$ .
Denote two sets as follows $$E(\epsilon) = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| >\epsilon \}$$ $$F = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| > 1/n \}.$$ Based on the definitions above, can we conclude $E(\epsilon) \subset F$ for any $\epsilon>0$ ?",['limits']
3752505,Writing $\int_\Omega \nabla u^T M \nabla v$ in terms of $H^1$ inner product of $u$ with another function,"Let $\Omega \subset \mathbb{R}^n$ be a smooth domain, $u,v \in H^1_0(\Omega)$ with the usual inner product and let $M=M(x)$ be a $n\times n$ matrix with entries $m_{ij}\colon \Omega \to \mathbb{R}$ which are as smooth as necessary. Furthermore, $M(x)$ is positive-definite and invertible for every $x$ . Is it true that there exist $\varphi, \phi \in H^1_0(\Omega)$ such that $$\int_\Omega \nabla u^T M \nabla v = \int_\Omega \nabla u^T \nabla \varphi + \int_\Omega u\phi?$$ If so how we can relate $\varphi$ and $\phi$ with $M$ and $v$ ? Outside of when $M$ is a multiple of the identity, I don't know.","['matrices', 'sobolev-spaces', 'linear-algebra', 'functional-analysis']"
3752528,combinatorics: 5 people picking 10 seats when there must be at least one space between them,"I have this question: How many seating arrangements are there for $5$ people to sit in $10$ seats in a row, when $2$ people can't sit next to each other? My idea: If there must be at least one space between every 2 people, the spaces must be something like this: _ _ s _ s _ s _ s _ There must be $2$ open seats next to each other, so they have $5$ options of where to be, and the person who sits there, have $2$ options to choose from. The seating arrangements for $5$ people is $5!$ in a standard row, so overall: $5 \cdot 2 \cdot 5! = 1200$ . My answer is wrong, so I was wondering what is a better way to think about it.","['permutations', 'combinatorics']"
3752559,Why do sometimes care for where vectors originate from and sometimes not? and exactly how many kinds of vectors are there?,"When I did linear algebra in high-school, it wasn't of much importance where the vectors originated from and for me this is a really hard concept to grasp. It's like no matter where the two vectors are pivoted in 3-d space, their dot product is invariant. Like, we don't even define an origin when talking about vectors... it's like they're freely floating in space.  Why can we do this as in why do we not need to regard origin when we speak of vectors? Is the vector attached to some object? like does it not matter where the 'tail' is. Some more context This question arose mainly when I was learning about plotting vector fields, in that, I had to associate each point with a vector so definitely here the vectors origin is relevant but not in the previous case, why? I had also come across this problem when studying physics, see this post . The person answering the post says that the cross product gives an axial vector. So I wonder how many types of vectors are there? Does this mean that regular 'vector' that we learned of has many 'cousin-forms'? How many takes types of vectors are there? how do we distinguish between these kinds of vectors?","['soft-question', 'linear-algebra', 'vectors']"
3752566,"Bijection between the power set of $\mathbb{Z}_+$ and the set of all infinite sequences $(x_n)_{n=1}^\infty$ with $x_n\in\{0,1\}$","Proposition. There is a bijective correspondence between $\mathscr P{(\mathbb{Z_+})}$ , the power set of $\mathbb{Z_+}$ , and $X^\omega$ , the set of all infinite sequences of elements of $X$ , where $X=\{0,1\}$ . ( Source: Munkres Topology 7.3) Proof Let $f: \mathscr P{(\mathbb{Z_+})} \to X^\omega$ . Define $f(A) = (x_i)_{i\in\mathbb{Z_+}}$ , for $A \in \mathscr P{(\mathbb{Z_+})}$ , such that if $i \in A$ , $x_i = 1$ , otherwise $x_i = 0$ . We first show $f$ is an injection.  Suppose $A,A' \in \mathscr P{(\mathbb{Z_+})}$ such that $A=A'$ . It follows that $A,A' \subset \mathbb{Z_+}$ . Consider $f(A)$ and $f(A')$ .  Then $f(A) = x \in X^\omega$ and $f(A') = x^{'} \in X^\omega$ .  By definition of $X^\omega$ , $x = (x_i)_{i\in\mathbb{Z_+}}$ and $x^{'} = (x^{'}_i)_{i\in\mathbb{Z_+}}$ for $x_i$ , $x^{'}_i \in X$ . It follows from the definition of $f$ , that $\forall j\in\mathbb{Z_+}$ , if $j\in A$ , then $x_j = 1$ . Since $A = A'$ by assumption, $\forall j\in A$ , $j\in A'$ and $x^{'}_j = 1$ .  Hence $\forall j\in \mathbb{Z_+} \cap A = \mathbb{Z_+} \cap A^{'}$ , $x_j = 1 = x^{'}_j$ .  Further, $\forall j\in \mathbb{Z_+} - A = \mathbb{Z_+} - A^{'}$ , $x_j = 0 = x^{'}_j$ .  Thus $\forall j\in\mathbb{Z_+}$ , $x_j = x^{'}_j$ .  Hence, $x = x^{'}$ .  Therefore, $f$ is an injection. To show $f$ is a surjection, arbitrarily choose $x \in X^\omega$ .  Then $x = (x_i)_{\mathbb{i\in Z_+}}$ , such that each $x_i\in X = \{0,1\}$ .  Construct a set $A$ , as follows.For each $i\in \mathbb{Z_+}$ , if $x_i=1$ , let $i$ be an element of $A$ .  Whether empty or not, $A \subset \mathbb{Z_+}$ .  Thus $A \in\mathscr{P(\mathbb{Z_+})}$ .  Hence $\forall x\in X^\omega$ , $\exists A\in\mathscr{P(\mathbb{Z_+})}$ , such that $x = f(A)$ .  Therefore, $f$ is a surjection. We have show there exists a bijective correspondence between $\mathscr P{(\mathbb{Z_+})}$ and $X^\omega$ . Question I would appreciate a verification of this proof.  Also, is there a better way to notate the definition of $f$ above.","['elementary-set-theory', 'functions', 'solution-verification', 'sequences-and-series']"
3752579,A tricky integral using Fourier Transform and Dirac-functions,"I need to calculate the following integral: $$
\boxed{I= \int_{0^+}^{t} \int_0^\infty f'(t')\,  \omega^2 \cos(\omega(t'-t))\, d\omega\, dt'}
$$ where $t>0$ , $t' \in (0,t]$ and $f'(x)$ is the derivative of the function $f$ . My attempt: First I define $$
A= \int_0^\infty \omega^2\cos(\omega \, a)  \, d\omega
$$ using the fact that $\omega^2\cos(\omega \, a)$ is even in $\omega$ , $$
A= \frac{1}{2} \int_{-\infty}^{\infty} \omega^2\cos(\omega \, a) d\omega
$$ expressing the cosine as a sum of exponentials, $$
A= \frac{1}{4}\int_{-\infty}^{\infty} \omega^2 \left[ e^{-i \omega (-a)}+e^{-i \omega a}\right] d\omega
$$ The Fourier transform of a polynomial is given in: https://en.wikipedia.org/wiki/Fourier_transform#Distributions,_one-dimensional $$
\int_{-\infty}^{\infty} x^n e^{-i \nu x} dx = 2 \pi i^n \delta^{(n)}(x)
$$ where $\delta^{(n)}(x)$ is the n-th derivative of the Dirac-delta distribution. Therefore, $$
A= - \frac{\pi}{2} \left[ \delta^{(2)}(-a) + \delta^{(2)}(a)\right].
$$ In this page: https://mathworld.wolfram.com/DeltaFunction.html we can check equation (17), which states that $x^n \delta^{(n)}(x)= (-1)^n n! \delta(x)$ , which we can use to deduce that $\delta^{(2)}(x)$ is ""even"". We conclude that $$
A= \int_0^\infty \omega^2\cos(\omega \, a)  \, d\omega = - \pi \delta^{(2)}(a).
$$ Using this result in $I$ , $$
I= - \pi \int_{(0^+,t]} f'(t') \delta^{(2)}(t'-t) dt'
$$ after this, I am stuck, I am supposed to prove that $$
I= -\pi \left[ - f''(t) \delta(0) + \frac{1}{2} f'''(t)\right]
$$ but can not figure how. Thank you for reading :)","['integration', 'definite-integrals', 'fourier-transform', 'dirac-delta']"
3752581,"Difficult word problem, looks like statistics/combinatorics. Having trouble making progress","""A trader has discovered a source of 1000 unique types of stamps which she can buy in bulk and then sell to a network of 100 merchants through an intermediary. No matter what type of stamp, she pays $1 for each stamp. Each day, she can sell up to 100 bags of stamps (each bag containing one or more different stamps, with no limit on how many stamps can be in each bag), and she is paid the next day. Unfortunately, the intermediary hides the prices of the individual stamps, and only tells the trader the per-bag price. In order to consistently turn a profit at a reasonable margin, it would be helpful to know which of the 1000 types of stamps are the most valuable, so she could sell bags with only the most valuable stamps. Given 7 days to evaluate 1000 types of stamps using 100 bags/day max, how does she figure out which types of stamps are the most valuable?"" So, just need to find the most valuable stamp of the 1000 stamps. I'm having trouble doing better than brute force. i.e. 700 bags with one stamp each. I've considered a couple approaches, but each one seems to be equal to brute force. I don't have much stats/combinatorics experience. I need to figure out how to eventually learn about the price of y stamps using only x bags, where x<y. I can find the average stamp price, but since there's no assumed distribution of prices, I'm not sure how finding the average price would be useful. Finding the median price isn't possible. I could investigate the price variance, but it could only be an approximation. What seems to really throw me off is the fact that the prices aren't distributed in any particular way, which makes me concerned about the accuracy of approximations. To top it off, I'm not sure if the problem is solvable, or if an approximation is the best possible outcome. Ideas I've tried: selling bags of some number of stamps and checking the average price of the group, but again I can't seem to make use of the average price, given that there's no price to compare it too other than the cost of $1. When I think about selling bags with some number of stamps in them, the unknown variance gives me a hard time making use of the resulting prices. If anyone knows what kind of problem this is, or some statistical principle/theorem is at play, please let me know and I'll study it. I've had no luck with google, and my bachelors was in applied math with no stats experience whatsoever","['word-problem', 'statistics', 'sorting', 'combinatorics']"
3752612,"Help in simplifying combinatorial sum $\frac{n!}{(n-k)!}-{1\over(n-k)!}{\sum _{m=1}^{k-1} (-1)^{m+1} (n-m)! S(k,k-m)}$","I'm working on simplifying $$\frac{n!}{(n-k)!}-{1\over(n-k)!}{\sum _{m=1}^{k-1} (-1)^{m+1} (n-m)! S(k,k-m)}$$ where $S(n,k)$ refers to Stirling numbers of the second kind. I can show experimentally that this expression is equivalent to $$(n-k+1)^k,$$ and I'm stuck in showing this to be the case. I've tried to form an intuition about what the sum might be counting, and have also tried to write down and simplify the sum with the definition of the Stirling numbers, but neither approach has led to progress. Any help or advice on how to simplify is appreciated. Thanks in advance.","['summation', 'combinatorics', 'stirling-numbers']"
3752641,"If function is differentiable at a point, is it continuous in a neighborhood?","I was reading a proof for the multi-variable chain rule and in the proof the mean-value theorem was used. The use of the theorem requires that a function is continuous between two points. Hence the motivation for the question, if a function is differentiable at a point, is it continuous in some neighborhood? If not, then the multi-variable chain rule proof is fake?",['real-analysis']
3752671,"Given i+j+k=n, find sum of all possible ijk","Just solved this question, correctly (made a computer program to, successfully, verify the formula that I got): Let $S$ denote the set of triples $(i,j,k)$ such that $i+j+k=n$ and $i,j,k\in\mathbb{N}$ .
Evaluate $$\sum_{(i,j,k)\in S}ijk$$ My solution was as follows: I'll assume that $i,j,k\in\mathbb{N}_0$ and $(i,j,k)$ is an ordered triple (eg. $(1,2,0)\not\equiv(0,2,1)$ )
WLOG consider $i$ as it varies. Firstly as a sub-problem (for ease of understanding) fix $i=0\implies j+k=n$ . Now this sub-problem reduces to finding all unordered pairs $(j,k)$ such that $j+k=n$ , to then find $jk$ for each.
Notice that if $j+k=N$ for some $N\in\mathbb{N}$ , then $\exists \ N+1$ such pairs, which are $(0,N),(1,N-1),(2,N-2),\ldots,(N-1,1),(N,0)$ . So the sum of all our $jk$ is given by, generally as $N$ varies $$
\sum_{j=0}^{N}j(N-j)
$$ For $i=0$ we have $N=n$ , and the desired sum will be $$
0\times\sum_{j=0}^{n}j(n-j)
$$ Now the sub-problem can easily be generalised to the main problem, as all that is different is that as $i$ varies, we must vary $N$ with it. specifically, if $i=b$ then $N=n-b$ . So we will obtain a double sum. $$
\begin{align}
\therefore \sum_{(i,j,k)\in S}ijk
&=\sum_{i=0}^{n}i\sum_{j=0}^{n-i}j(n-i-j)
\\ &=\sum_{i=0}^{n}i\left((n-i)\sum_{j=0}^{n-i}j-\sum_{j=0}^{n-i}j^2\right)
\\ &= \sum_{i=0}^{n}i\left((n-i)(\frac 1 2 (n-i)(n-i+1))-\frac 1 6 (n-i)(n-i+1)(2(n-i)+1)\right)
\\ &= \sum_{i=0}^{n}i\left(\frac 1 2 (n-i)^2(n-i+1)-\frac 1 6 (n-i)(n-i+1)(2n-2i+1)\right)
\\ &= \sum_{i=0}^{n}i\left(\frac 1 6 (n-i)(n-i+1)(3(n-i)-(2n-2i+1))\right)
\\ &= \frac 1 6 \sum_{i=0}^{n}i(n-i)(n-i+1)(n-i-1)
\\ &= \frac 1 6 \sum_{i=0}^{n}i(n-i)((n-i)^2-1)
\\ &= \frac 1 6 \sum_{i=0}^{n}i((n-i)^3-(n-i))
\\ &= \frac 1 6 \sum_{i=0}^{n}i(n^3-3n^2i+3ni^2-i^3-n+i)
\\ &= \frac 1 6 \sum_{i=0}^{n}n(n+1)(n-1)i+(1-3n^2)i^2+3ni^3-i^4
\\ &= \frac 1 6 n(n+1)(n-1) \sum_{i=0}^{n}i+\frac 1 6 (1-3n^2) \sum_{i=0}^{n}i^2+\frac 1 2 n \sum_{i=0}^{n}i^3- \frac 1 6 \sum_{i=0}^{n}i^4
\\ &= \frac 1 {12} n^2(n+1)^2(n-1)+\frac 1 {36} n(n+1)(2n+1)(1-3n^2)+\frac 1 {8} n^3(n+1)^2 - \frac 1 {180} n(n+1)(2n+1)(3n^2+3n-1)
\\ &= (\frac 1 {12} n^5 + \frac 1 {12} n^4 - \frac 1 {12} n^3 - \frac 1 {12} n^2) +  (-\frac 1 {6} n^5 - \frac 1 {4} n^4 - \frac 1 {12} n^2 + \frac 1 {36} n) + (\frac 1 {8} n^5 + \frac 1 {4} n^4 + \frac 1 {8} n^3) - (\frac 1 {30} n^5 + \frac 1 {12} n^4 +\frac 1 {18} n^3 -\frac 1 {180} n)
\\ &= \frac{1}{120}n^5-\frac{1}{24}n^3+\frac{1}{30}n
\\ &= \frac{1}{120}n(n^4-5n^2+4)
\\ &= \frac{1}{120}n(n^2-1)(n^2-4)
\\ &= \frac{1}{120}n(n+1)(n-1)(n-2)(n+2)
\\ &= \frac{(n+2)!}{(n-3)!5!}
\\ &= \binom{n+2}{5}
\end{align}
$$ My question was, how come there is a binomial coefficient as an answer? Are there any cool generalisations/patterns here? It seems unexpected (to me) that a sum of ijk would come out as that, and quite nice, but I can't seem to relate it to anything binomial related.","['summation', 'combinatorics', 'combinations']"
3752680,Is there a geometric intuition for integration by parts? [duplicate],"This question already has answers here : What is integration by parts, really? (8 answers) Closed 3 years ago . Is there a geometric intuition for integration by parts? $$\int f(x)g'(x)\,dx = f(x)g(x) - \int g(x)f'(x)\,dx$$ This can, of course, be shown algebraically by product rule, but still where is geometric intuition? I have seen geometry of IBP using parametric equations but I don't get it. Newest edit: few similar questions has been asked before, but they use parametric equations to show geometry behind IBP. I am interested if there is geometric intuition which uses functions in Cartesian plane or some other, maybe more natural, explanation.","['integration', 'calculus', 'geometry', 'intuition']"
3752743,A question about Ramsey type statements,"Suppose we ask for the minimum $R(H)$ s.t every $2-$ coloring of the complete graph $K_{R(H)}$ must contain a monochromatic copy of $H$ . It is known that $R(H)$ is finite for all $H$ . What can be said about $R(H)$ as a function of $H$ . Meaning, what types of graphs yield a lower/higher Ramsey number? Intuitively it seems, for example, that if the number of edges $m = \Theta(n)$ for some class of graphs $H_n$ , then $R(H_n)\leq R(K_n)$ . This might be wildly wrong though. Sorry if this is a bit qualitative but any ideas/known results in this general direction would be of help.","['graph-theory', 'combinatorics', 'ramsey-theory', 'reference-request']"
3752748,solution of $f'(x)+f(-x)=e^{-x^2}$,"let $f$ be a function such that $f:\mathbb{R}\to \mathbb{R}$ , I want to determine all functions of class $C^1$ such that $f'(x)+f(-x)=e^{-x^2}$ for all $x\in \mathbb{R}$ , Now we have $f'(x)+f(-x)=e^{-x^2}$ this implies that $f'(x)=e^{-x^2}-f(-x)$ , since $f'$ is of class $C^1$ this means that $f$ is of class $c^2$ , That equation equivalent to : $f''(x)=-2x e^{-x^2}-f'(-x)$ implies to $f''(x)-f(x)=-2x e^{-x^2}-e^{x^2}$ , if I take now $x\mapsto \lambda\cos(x)+\mu\sin(x),\lambda,\mu\in\mathbb R$ as a solution without second term in order to get particular solution it would be complicated and given by error function which forbide me to get a general solution, I ask now if there is any simple way to solve that functional ?","['mathematical-physics', 'error-function', 'ordinary-differential-equations']"
3752770,is sin(x) / sin(x) = 1?,"I tested this in python using: import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 10*2*np.pi, 10000)
y = np.sin(x)
plt.plot(y/y)
plt.plot(y) Which produces: The blue line representing sin(x)/sin(x) appears to be y=1 However, I don't know if the values at the point where sin(x) crosses the x-axis really equals 1, 0, infinity or just undefined.","['calculus', 'algebra-precalculus', 'discrete-mathematics']"
3752857,"$\int_0^\infty\frac{x^2+3}{x^4+1}\,dx$ via contour integration?","I want to calculate $\displaystyle I=\int_0^\infty\frac{x^2+3}{x^4+1}\;dx$ using contour integration. I've obtained the correct answer of $I=\sqrt{2}\pi$ below, but I would appreciate if someone could verify my proof. Let $\displaystyle f(z) = \frac{z^2+3}{z^4+1}$ ; and let $C$ be the semicircle with boundary diameter on the real line (say from $-R$ to $R$ ) in the upper half plane. Now, $f(z)$ has four simple poles at $z = \pm i^{\frac{1}{2}}, \pm i^{\frac{3}{2}}$ . The two poles at $z =  i^{\frac{1}{2}},  i^{\frac{3}{2}}$ lie on the upper half plane, and the sum of the residues of $f$ at these poles is $-\sqrt{2}i$ . So $$\displaystyle \oint_C f(z)\,dz=\int_{-R}^{R} f(z) \, dz +\int_{\text{Arc}} f(z)\, {dz} $$ So by the Residue theorem $$-i\sqrt{2}\cdot 2i\pi=\int_{-R}^{R} f(z) \, dz +\int_{\text{Arc}} f(z)\, {dz} $$ Thus $$2\sqrt{2}\pi=\int_{-R}^{R} f(z) \, dz +\int_{\text{Arc}} f(z)\, {dz} $$ I tried to show that the arc integral goes to $0$ as follows $$\bigg|\int_{\text{Arc}} f(z)\, {dz} \bigg| \le  \int_{\text{Arc}} |f(z)|\, {dz} \le \frac{R^2-3}{R^4-1} \cdot \pi R \to 0,  ~\text{as} ~ R \to \infty $$ Because $|z|^4=\left|z^4\right| = \left|z^4+1-1\right| \le \left|z^4+1\right|+1  $ So $\displaystyle |{z^4+1}| \le |z|^4-1 = R^4-1 $ , similarly $|z|^2 = |z^2+3-3| \le |z^2+3|+3 \implies |z^2+3|\le |z|^2-3 = R^2-3 $ And the length of the arc is $\pi R$ combining altogether we get $$\displaystyle \bigg|\int_{\text{arc}}\frac{z^2+3}{z^4+1} \,d{z}\bigg|  \le \frac{(R^2-3)\cdot \pi R }{R^4-1}$$ Which goes to zero as $R \to \infty$ . So the arc integral goes to $0$ as $R \to 0$ . Therefore $\displaystyle \int_{-\infty}^{\infty} \frac{z^2+3}{z^4+1}\,{dz} = 2\sqrt{2}\pi $ so that $I = \sqrt{2}\pi$ since the integrand is even. Is this correct?","['complex-analysis', 'contour-integration', 'solution-verification', 'definite-integrals']"
3752874,Evaluate $\lim_{x\to 0} \frac{f(x^3)}{x}$,"Let $f:\mathbb{R}\to \mathbb{R}$ be a function such that $|f(x)|\leq 2|x|$ for every $x \in \mathbb{R}$ . Evaluate $\lim_{x\to 0} \frac{f(x^3)}{x}$ . According to the answer key, it is $0$ (which matches mine). I am not so sure about my solution (below) though. Rewritting $\lim_{x\to 0} \frac{f(x^3)}{x}$ yields $$\lim_{x\to 0} \frac{f(x^3)}{x} = \lim_{x\to 0} 2\frac{|x^3|}{x} = 2\lim_{x\to 0} \frac{|x^3|}{x}$$ Analyzing one-sided limits, we have: $$\lim_{x\to 0^-}  \frac{-x^3}{x}=\lim_{x\to 0^-} -x^2=0$$ and $$\lim_{x\to 0^+}  \frac{x^3}{x}=\lim_{x\to 0^+} x^2=0$$ Both one-sided limits exist and are equal, therefore $$2\lim_{x\to 0} \frac{|x^3|}{x}= 2\cdot0=0$$ . Is my solution correct?","['limits', 'calculus']"
3752888,Probability of a feature in a randomly permuted decision tree constrained that no feature is reached twice in any decision path,"I have a (binary) decision tree consisting of nodes $N=\{N_i\}$ that take on boolean propositions/features $F=\{F_k\}$ . Different decision paths can split on the same feature so $ |N| >> |F| $ Edit: and in addition the number of repeating features is fixed for a graph. So if F1 is repeated twice it has to be repeated twice for each permuted graph. This is because I have a ""starter"" graph and I want to see how much it would change if I rearrange the nodes. I want to compute probability $ P(N_i = F_k | G(N,E)) $ where $G(N,E)$ is a DAG with nodes $\{N_i\}$ and edges $\{E_{jk}\}$ (in particular it is a decision/binary tree). Assuming nodes are assigned random features with the constraint that that no decision path has the same feature twice (so the decision tree never repeats a decision boundary). This means the following permutation is not allowed (F1 is repeated in F1->F3->F1 path): F1
 /  \
F2  F3
   /  \
  F1  F4 but this permutation is allowed (no repeats): F2
 /  \
F1  F3
   /  \
  F1   F4 The graphs are relatively large (~1000-10000 nodes) and I need to compute over a few hundred different versions so I wonder if there is something better than brute force that I can use. Edit: So for the tree structure above let's define nodes as N0
 /  \
N1  N2
   /  \
  N3   N4 and have a feature set {F1,F1,F2,F3,F4} Then brute forcing with our constraint we have F1          F2          F3        F4
 N0:   [0.         0.33333333 0.33333333 0.33333333]
 N1:   [0.75       0.08333333 0.08333333 0.08333333]
 N2:   [0.25       0.25       0.25       0.25.     ]
 N3:   [0.5        0.16666667 0.16666667 0.16666667]
 N4:   [0.5        0.16666667 0.16666667 0.16666667]","['graph-theory', 'decision-trees', 'combinatorics', 'probability-theory']"
3752891,"Does the convergence to 0 in $L^2(0,T;L^2(B))$ where $|\mathbb{R}^{d}-B|<\infty$ imply the convergence in $L^2(0,T;L^2(\mathbb{R}^{d}))$?","Definition: One denotes $L^p(a,b;X)$ , $1\leq p<+\infty$ , the set of (classes
of) measurable functions $u:(a,b)\rightarrow X$ such that $$\|u \|_{L^p(a,b;X)}:= \left(\int_{a}^{b}\|u(t)\|^p_X dt\right)^{\frac{1}{p}}<\infty .$$ Let $(f_n)$ be a sequence in $L^2(0,T;L^2(\mathbb{R}^{d}))$ such that: $\|f_n\|_{L^2(0,T;L^2(\mathbb{R}^{d}))} \leq C_T$ for all $n \in \mathbb{N}$ ; (or $\|f_n(t)\|_{L^2(\mathbb{R}^{d})}\leq C_T$ for all $t \in (0,T))$ $f_n \rightarrow 0$ a.e. in $(0,T)\times \mathbb{R}^{d}$ ; $f_n \rightarrow 0$ in $L^2(0,T;L^2(K))$ for all compact $K \subset \mathbb{R}^{d}$ . $f_n \rightarrow 0$ in $L^2(0,T;L^2(B))$ where $B \subset \mathbb{R}^{d}$ and $|\mathbb{R}^{d}-B|<\infty;$ My question: Is it true that $f_n \rightarrow 0$ in $L^2(0,T;L^2(\mathbb{R}^{d}))$ or that there is a subsequence of $f_n$ that converges to $0$ in $L^2(0,T;L^2(\mathbb{R}^{d}))$ ? Or what additional hypothesis do I need for this convergence to be true? Note that $$\int_{0}^{T}\int_{\mathbb{R}^{d}} |f_n(x,t)|^2\,dxdt=\int_{0}^{T}\int_{B} |f_n(x,t)|^2\,dxdt+\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt=I_1+I_2.$$ Observe that $I_1 \rightarrow 0$ by virtue of the fourth hypothesis above. It remains to prove that $I_2 \rightarrow 0$ . From Egorov's Theorem, given $\varepsilon>0$ there exists a closed subset $F \subset (0,T)\times (\mathbb{R}^{d}-B)$ such that $|((0,T)\times (\mathbb{R}^{d}-B))-F|<\varepsilon|$ and $f_n\rightarrow 0$ uniformly in $F$ , which implies that $f_n \rightarrow 0$ in $L^2(F)$ . Thus, we obtain \begin{eqnarray}
I_2&=&\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt\\
&=& \int_{F} |f_n(x,t)|^2\,dxdt+\int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt.
\end{eqnarray} Is there any way to prove that the $\int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt$ integral converges to 0?","['lebesgue-integral', 'functional-analysis']"
3752923,Standard and second forms of Euler equation giving different results,"Consider the conical surface $z=h(1-\rho/a)$ , $\rho = \sqrt{x^2+y^2}$ generated by the revolution around the z axis of the straight line $ z = h(1-y/a)$ , $y\geq 0$ . We wish to find the geodesic curve  between the points $A$ and $B$ ( see figure below) a) Using cylindrical coordinates $(\rho,\varphi,z)$ with $\varphi = arctan (y/x)$ ,obtain the linear functionals $f(\varphi,\varphi';\rho)$ and $g(\rho,\rho';\varphi)$ that must be minimized to find the geodesic.It will be useful to define $\beta = \sqrt{1+(h/a)^2}$ . b) Using the standard form of the Euler equation for the most convenient functional ( $f$ or $g$ ), find the differential equation that relates $(\rho,\varphi)$ and its general solution. c) Verify that the equation of item b can also be obtained using the Second form of the Euler equation applied to $f$ or $g$ I found on item a, $f= \sqrt{\beta^2+\rho^2\varphi'^2}$ and on item b using the standard Euler equation: $\frac{d}{d\rho}(\frac{\partial f}{\partial \varphi'} )- \frac{\partial f}{\partial \varphi} = 0$ ,the differential equation: $\varphi' = \frac{c \beta}{\rho\sqrt{\rho^2 -c^2}}$ , where $c$ is a constant But when I try to use the second form of the Euler equation (item c): $\frac{\partial f}{\partial \varphi} - \frac{d}{d\varphi}(f - \varphi'\frac{\partial f }{\partial \varphi'}) = 0$ , which since $\frac{\partial f}{\partial \varphi}  = 0$ , is the same as $f- \varphi'\frac{\partial f }{\partial \varphi'}= c$ ,I get: $\varphi' = \frac{\beta\sqrt{\beta^2-c^2}}{c\rho}$ . What am I doing wrong ? Any hints or help will be appreciated.","['calculus-of-variations', 'euler-lagrange-equation', 'calculus', 'physics', 'differential-geometry']"
3752941,"If $X$ is $\sigma(\mathcal{G} \cup \mathcal{H})$-measurable can we write $X=f(G,H)$?","Let $(\Omega, \mathcal{F}, P)$ be a probability triplet.  Let $\mathcal{G}$ and $\mathcal{H}$ be two sub-sigma-algebras of $\mathcal{F}$ . Let $X:\Omega\rightarrow\mathbb{R}$ be a random variable such that $\sigma(X) \subseteq \sigma(\mathcal{G} \cup \mathcal{H})$ . Can we say there is a Borel-measurable function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ and two random variables $G$ and $H$ such that $X = f(G,H)$ , and $\sigma(G) \subseteq \mathcal{G}$ , $\sigma(H) \subseteq \mathcal{H}$ ?  Are there references where I can cite such a result? If it makes the problem easier, I am also interested in the case when $\mathcal{G}$ and $\mathcal{H}$ are independent and/or when one of the sub-sigma-algebras, say $\mathcal{G}$ , is generated by some random variable $R$ . Note 1: I can show it is true if there are random variables $R, S$ such that $\mathcal{G}=\sigma(R)$ and $\mathcal{H}=\sigma(S)$ . Note 2: There may be some hope by applying properties of countably generated sigma algebras, since $\sigma(X)$ is countably generated. If we define the events $B_x = \{X \leq x\}$ for each rational number $x$ , I wonder if there is a way of writing a particular event $B_x$ in terms of events in $\mathcal{G}$ and $\mathcal{H}$ . Application: I have a random variable $Z$ and I want to write for some $f$ : $$ E[Z|\sigma(\mathcal{G} \cup \mathcal{H})] \overset{?}{=} f(G,H)$$ This is similar in spirit to the known fact $E[Z|\sigma(Y)]=f(Y)$ for some $f$ .","['measure-theory', 'probability-theory']"
3752990,Simple conditions on Radon-Nikodym derivative to obtain equivalent measures,"Are there some simple conditions for the converse statement of the following statement: If $\mu$ and $\nu$ are equivalent (i.e. $\mu \ll \nu$ and $\nu \ll \mu$ ) then $$
\frac{d\mu}{d\nu}=\left(\frac{d\nu}{d\mu}\right)^{-1}
$$ Is strict positivity of the derivate one for instance?",['measure-theory']
3753011,Let $f$ be an entire function such that $|f'(z)|\leq |f(z)|$ for all $z$. Show that $f(z)=ae^{cz}$,"LEt $f$ be an entire function such that $|f'(z)|\leq |f(z)|$ for all $z$ . Show that $f(z)=ae^{cz}$ My proof: Consider $g(z)=\frac{f'(z)}{f(z)}$ This function is bounded by 1 per our inequality. It is holomorphic everywhere except at zeroes of $f(z)$ however, these must be removable singularities as $g(z)$ is bounded around them. Thus $g(z)$ is entire and bounded (or can be extended to be such). Thus by louvile theorem it is a constant funciton. Hence $f'(z)=af(z)$ for all $z$ that are not zeroes of $f$ , but it also works for zeroes as our inequality implies that $f(z)=0 \implies f'(z)=0$ . Thus we get that $f^{(n)}(z)=a^nf(z)$ Hence looking the the taylors series we find that it is of the form $\Sigma \frac{f(0)((\frac{z}{a})^n}{n!}=f(0)e^{z/a}$ and this conclued our proofs.","['complex-analysis', 'solution-verification']"
