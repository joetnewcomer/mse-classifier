question_id,title,body,tags
4409208,Divisibility and Lagrange Theorem,"I came across the following question from math olympiad: For $n\in \mathbb{Z}^+$ , prove that $$n!\mid\prod_{k=1}^n(2^n-2^{k-1}).$$ While I can solve it using elementary number theory, I notice that $\prod_{k=1}^n(2^n-2^{k-1})$ is the order of the general linear group $GL_n(\mathbb{F}_{2})$ over $\mathbb{F}_{2}$ and $n!$ is the order of the symmetric group $S_n$ . So I was wondering can we view $S_n$ as a subgroup of $GL_n(\mathbb{F}_{2})$ and invoke Lagrange theorem to solve the question, thank you so much.","['elementary-number-theory', 'group-theory', 'group-actions', 'divisibility']"
4409240,"Up to a change of coordinates, what does $\nabla f (\boldsymbol{x})/\|\nabla f (\boldsymbol{x})\|$ looks like?","First part: motivations If you don't bother with optimization, you can skip this part . While I was studying KKT points of quadratic penalty method applied to the optimization problems \begin{align} \min_{\boldsymbol{x} \in \mathbb{R}^{n}} &\  f(x) \\ \text{st} &\  h(\boldsymbol{x})=\boldsymbol{0} \\
&\  g(\boldsymbol{x}) \leq \boldsymbol{0} 
\end{align} I ended up with a question. Let us start with an increasing and unbounded sequence $\{ \rho_k \}_{k\in\mathbb{N}}$ and the quadratic penalty function $\Phi: \mathbb{R}^{n}\mapsto \mathbb{R}$ given by $\Phi(\boldsymbol{x}) = \left(\|\max\{\boldsymbol{g}(\boldsymbol{x}),0\}\|^2_{2} + \| \boldsymbol{h}(\boldsymbol{x})\|^2_{2}\right)$ , for all $\boldsymbol{x} \in \mathbb{R}^{n}.$ Let us suppose that an external penalty method generates a converging sequence of stationary points, say $\{\boldsymbol{x}^{k}\}_{k\in\mathbb{N}}$ for the extermal penalty method subproblem \begin{align} \min_{\boldsymbol{x} \in \mathbb{R}^{n}} &\  f(x)+ \rho_k \Phi (\boldsymbol{x}),
\end{align} i.e., a sequence so that $$\nabla f(\boldsymbol{x}^{k}) + \rho_k \nabla \Phi (\boldsymbol{x}^{k}) = 0.$$ Now observe that $$\dfrac{1}{\|\nabla f (\boldsymbol{x}^k) \|_{2}}\nabla f (\boldsymbol{x}^k) = - \dfrac{1}{\|\nabla \Phi (\boldsymbol{x}^{k})\|_{2}} \nabla \Phi (\boldsymbol{x}^{k}).$$ This means that, the only possible way of having
a converging sequence stationary point $\{\boldsymbol{x}^{k}\}_{k\in\mathbb{N}}$ is that $$\left\lbrace \dfrac{1}{\|\nabla \Phi (\boldsymbol{x}^{k})\|_{2}} \nabla \Phi (\boldsymbol{x}^{k})\right\rbrace_{k\in\mathbb{N}} \text{ converges.}$$ Now, in order to make the things more interesting, let $\{\boldsymbol{x}^{k}\}_{k\in\mathbb{N}}$ converge to a feasible point, say $\boldsymbol{x}^{*}$ , i.e., $\Phi(\boldsymbol{x}^{*}) = \boldsymbol{0}$ . This also implies that $\nabla \Phi (\boldsymbol{x}^{*}) = \boldsymbol{0},$ i.e., a ""singular point"". Second part: question Now comes the question: Considering the zero set of $\nabla \Phi$ , i.e., $\mathcal{Z} := \{ \boldsymbol{x} \in \mathbb{R}^{n} : \nabla \Phi(\boldsymbol{x}) = \boldsymbol{0}\}$ . How does the function $g: \mathbb{R}^{n} \setminus \mathcal{Z} \mapsto \mathbb{R}^{n}$ such that $g(x) = \nabla \Phi (\boldsymbol{x})/\|\nabla \Phi (\boldsymbol{x})\|$ behaves by a change of coordinate, or even a proper surjective map? Is this a projection onto $\mathcal{Z}$ by changing coordinates? Specifically, by finding a differentiable proper and surjective map $\theta : \mathbb{R}^{m} \mapsto \mathbb{R}^{n}$ , how is the basic form of $g \circ \theta $ ? Does Whitney stratification helps to find this basic form? What does it say? Take easy on me, I'm an applied mathematician. =) On Whitney stratification thing, If an easy, maybe extremely easy, reference can be found, it would be enlightening!","['optimization', 'differential-geometry', 'real-analysis']"
4409271,"Karatzas + Shreve 2.19: Show that $Y_t = \int_0^t f(s, X_s) \, ds; t \ge 0$ is progressively measurable","Let $X = \{X_t, \mathscr{F}_t; 0 \le t < \infty \}$ be a progressively measurable process, and let $T$ be a stopping time of $\mathscr{F}_t$ . With $f(t,x): [0, \infty) \times \mathbb{R}^d \to \mathbb{R}$ as a bounded, $\mathscr{B}([0, \infty)) \otimes \mathscr{B}(\mathbb{R}^d)$ measurable function, show that the process $Y_t = \int_0^t f(s, X_s) \, ds; t \ge 0$ is progressively measurable with respect to $\{ \mathscr{F}_t \}$ and $Y_T$ is an $\mathscr{F}_T$ measurable random variable. If I can show that $Y_t$ is adapted to $\mathscr{F}_t$ then the rest is fairly simple. Since each sample path is the integral of a function, it is continuous, therefore all sample paths of $Y_t$ are continuous. Then by earlier proposition 1.13, which says that any adapted process with right continuous sample paths is progressively measurable, we conclude that $Y_t$ is progressively measurable. Then given $Y_t$ is progressively measurable, by proposition 2.18, the stopped process $Y_{T \land t}$ is also progressively measurable. To show that $Y_T$ is $\mathscr{F}_T$ measurable, we need to show that for any Borel set $B \in \mathscr{B}(\mathbb{R}^d)$ and any $t \ge 0$ , that event $\{Y_T \in B\} \cap \{T \le t\}$ is $\mathscr{F}_t$ measurable. That event is equivalent to $\{Y_{T \land t} \in B\} \cap \{T \le t\}$ , and measurability of that follows from progressive measurability of $Y_{T \land t}$ . However, I'm stuck on how to show that the given $Y_t$ is adapted to $\mathscr{F}_t$ . Any suggestions? Thank you.","['stochastic-processes', 'probability-theory', 'filtrations']"
4409276,How to prove a lemma about $\lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !}$? [duplicate],"This question already has answers here : The proof of an elementary equality (2 answers) Closed 2 years ago . I recently came across the following  lemma while learning harmonic analysis, but don't know how to prove it by using analytical methods. Lemma: For all $t \geq 0, x>0$ , we have that \begin{equation}
\lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !}=\mathbf{1}_{[0, x)}(t)+\frac{1}{2} \mathbf{1}_{\{x\}}(t),
\end{equation} where $\mathbf{1}_{[0, x)}(t)$ is the indicator function on set $[0, x)$ . If anyone can provide the proof, I would like to thank you here in advance.","['harmonic-analysis', 'analysis', 'real-analysis']"
4409294,Symmetrization and the CLT,"Let $(X_n)$ be an independent sequence of real standard normal random variables and let $(\boldsymbol{\Sigma}_n)$ be a sequence of $n \times n$ (growing size) real positive definite matrices. Define the triangular array $(Y_{i,n})_{1\leq i\leq n}$ as follows: $$\boldsymbol{Y}_n:=\boldsymbol\Sigma_n^{1/2}\boldsymbol{X}_n$$ where $\boldsymbol{Y}_n=(Y_{1,n},\dots,Y_{n,n})^\top$ and $\boldsymbol{X}_n=(X_{1},\dots,X_{n})^\top$ . Let $\boldsymbol \Sigma_n=\boldsymbol Q_n^\top\boldsymbol \Lambda_n \boldsymbol Q_n$ be an eigendecomposition of $\boldsymbol \Sigma_n$ . Then $$\boldsymbol{Y}_n^\top \boldsymbol{Y}_n=\boldsymbol{Z}_n^\top \boldsymbol \Lambda_n \boldsymbol{Z}_n$$ where $\boldsymbol{Z}_n:=\boldsymbol Q_n \boldsymbol{X}_n$ is a standard normal vector of uncorrelated, hence independent, random variables. I want to apply the CLT for triangular arrays to the  normalized sum $$\xi_n:=\frac{1}{s_n}\sum_{i=1}^n (Y^2_{i,n}-\sigma_{ii,n})=\frac{1}{s_n}\sum_{i=1}^n \lambda_{i,n}(Z_{i,n}^2-1)$$ where $s^2_n=Var(\sum_{i=1}^n Y^2_{i,n})=Var(\sum_{i=1}^n \lambda_{i,n}Z_{i,n}^2)=2\sum_{i=1}^n \lambda^2_{i,n}$ . The Lyapounov's condition for $\delta=2$ is $$\frac{E[(Z_{i,n}^2-1)^4]}{4} \frac{\sum_{i=1}^n \lambda^4_{i,n}}{(\sum_{i=1}^n \lambda^2_{i,n})^2}=15 \frac{Tr(\boldsymbol\Sigma_n^4)}{Tr(\boldsymbol\Sigma_n^2)^2} \to 0 \quad \text{as} \quad n\to \infty $$ The specific example am considering is with the tridiagonal matrix $$\boldsymbol\Sigma_n = \begin{bmatrix}
1 & 1/2 \\
1/2 & 1 & 1/2 \\
& 1/2 & 1 & \ddots \\
& & \ddots & \ddots & \ddots \\
& & & \ddots  & 1 & 1/2 \\
& & & & 1/2 & 1 & 1/2 \\
& & & & & 1/2 & 1
\end{bmatrix}$$ whose eigenvalues are given by $\lambda_{i,n}=1+\cos(\frac{i\pi}{n+1})$ , $i=1,\dots,n$ . One checks that the Lyapounov's condition above is satisfied in this case. Simulations with $n=200$ reveal that the tails of the distribution of $\xi_n$ are not particularly well approximated by the normal distribution. On the other hand the tails of the symmetrized statistic $\xi_n-\xi'_n$ , where $\xi'_n$ is an independent copy of $\xi_n$ , seem to be much better fitted by the normal distribution. Are there any theoretical reasons for this? Thanks a lot for your help.","['statistics', 'central-limit-theorem', 'eigenvalues-eigenvectors', 'probability-theory', 'probability']"
4409332,Show that concatenation of two processes is measurable,"Let $(E_n,\mathcal E_n)$ be a measurable space; $(\Omega_n,\mathcal A_n,\operatorname P_n)$ be a probability space; $(\mathcal F^n_t)_{t\ge0}$ be a filtration on $(\Omega_n,\mathcal A_n)$ ; $\tau_n$ be an $(\mathcal F^n_t)_{t\ge0}$ -stopping time on $(\Omega_n,\mathcal A_n)$ $(X^n_t)_{t\ge0}$ be an $(E_n,\mathcal E_n)$ -valued $(\mathcal F^n_t)_{t\ge0}$ -adapted (maybe we even need to assume progressive ) process on $(\Omega_n,\mathcal A_n,\operatorname P_n)$ for $n=1,2$ with $E_1\cap E_2=\emptyset$ , \begin{align}E&:=E_1\cup E_2;\\\mathcal E&:=\sigma(\mathcal E_1\cup\mathcal E_2);\\\Omega&:=\Omega_1\times\Omega_2\end{align} and $$X_t:\Omega\to E\;,\;\;\;(\omega_1,\omega_2)\mapsto\begin{cases}X^1_t(\omega)&\text{, if }t<\tau_1(\omega_1);\\ X^2_{t-\tau_1(\omega_1)}(\omega_2)&\text{, otherwise}\end{cases}$$ and $$\mathcal F_t:=\sigma(\mathcal F^1_t\cup\mathcal F^2_t)$$ for $t\ge0$ . How can we show that $(X_t)_{t\ge0}$ is $(\mathcal F_t)_{t\ge0}$ -adapted? Let $t\ge0$ . We've clearly got $$F:=\{t<\tau_1\}\cap\Omega_2\in\mathcal F_t$$ and it is enough to show that $\left.X_t\right|_F$ and $\left.X_t\right|_{F^c}$ are $(\left.\mathcal F_t\right|_F,\mathcal E)$ and $(\left.\mathcal F_t\right|_{F^c},\mathcal E)$ -measurable, respectively. $X^1_t$ is clearly $(\mathcal F^1_t,\mathcal E)$ -measurable. So, the only tricky part is the measurability on $F^c$ . I know that if $(Y_t)_{t\ge0}$ is an $(\mathcal G_t)_{t\ge0}$ -progressive process and $\zeta$ is an $(\mathcal G_t)_{t\ge0}$ -stopping time, then $Y_\zeta$ is $\mathcal G_\zeta$ -measurable.","['stochastic-processes', 'measure-theory', 'probability-theory']"
4409333,Density of first hitting time of Brownian motion from its CDF,"Let $T_a$ be the first hitting time of level $a$ for standard Brownian motion. I am trying to show the density of $T_a$ is $f_a(t)=   \frac{a}{\sqrt{2\pi t^3}}\text{exp}(-\frac{a^2}{2t})$ I know that $\mathbb{P}[T_a \leq  t ] = 2\mathbb{P}[B_t \geq a]$ where $B_t \sim N(0,t)$ (standard brownian motion) I have reduced this to: $F_a(t) = 2 \Phi(-\frac{a}{\sqrt{t}})$ where $\Phi$ denotes the cumulative density of the standard normal RV. I did not have much luck with differentiation under the sign (aka Leibniz rule) Is there a simple transformation that is more fruitful?","['brownian-motion', 'probability-theory']"
4409354,If two Markov process have same $2$-dimensional distribution then they are equivalent,"My problem: Let $(\Omega,\mathcal{F}, \mathbb{P}, \mathcal{F}_t)$ be a filtered probability space and $(E,\mathcal{E})$ a measure space. If $X,Y:\mathbb{R}_{+}\times\Omega\to E $ are Markov processes (with respect to $\mathcal{F}_t)$ with the same 2-dimensional distributions, then they are equivalent (they have the same law). My attempt: I observed that $\mathbb{E}[\mathbb{I}_{X_s \in A}\mathbb{I}_{X_t \in B}]=\mathbb{E}[\mathbb{I}_{X_s \in A}\mathbb{E}[\mathbb{I}_{X_t \in B}|\mathcal{F}_s]]=\mathbb{E}[\mathbb{I}_{X_s \in A}p(s, t, X_s, B)]$ where $p(s, t) : E \times \mathcal{E} \to [0,1]$ is the transition kernel of $X$ . Thus by hypothesis we obtain that: $$\mathbb{E}[\mathbb{I}_{X_s \in A}p(s, t, X_s, B)]=\mathbb{E}[\mathbb{I}_{Y_s \in A}q(s, t, Y_s, B)]$$ We now have that $p(s,t)(x, B)=q(s,t)(x, B)$ for almost every $x \in E$ with respect to the measure $(X_s)_*(\mathbb{P})=(Y_s)_*(\mathbb{P})$ i.e. the push forward probability of $X_s$ onto $E$ . Why should this imply that the two processes have the same finite-dimensional distributions? I think that I should write $\mathbb{E}[\mathbb{I}_{X_{t_1} \in A_1} \dots \mathbb{I}_{X_{t_n} \in A_n}]$ as an integral involving $p$ with respect to measure with whom $p$ and $q$ are equal but I do not know how and I am not sure that this would conclude.","['conditional-probability', 'stochastic-processes', 'markov-process', 'probability-theory']"
4409387,"$(\int_{\mathbb{R}}|\int_\mathbb{R}{\mathcal{F}^{-1}}(\chi_{(t,\infty)}\hat{f})(\xi)m'(t)dt|^pd\xi)^{1/p}\leq C_p\|m'\|_{L^1}\|f\|_{L^p}$","I am reading the book Fourier Analysis by Javier Duandikoetxea and I am stuck in the proof of a lemma that it is the key part to prove the Marcinkiewicz multiplier theorem. In particular I am stuck in showing the following inequality: $$\left(\int_{\mathbb{R}}\left|\int_\mathbb{R}{\mathcal{F}^{-1}}(\chi_{(t,\infty)}\hat{f})(\xi)m'(t)dt\right|^pd\xi\right)^{1/p}\leq C_p\|m'\|_{L^1}\|f\|_{L^p}$$ where $\mathcal{F}^{−1}$ means the inverse fourier transform, $f,m$ are two arbitrary functions in the Scwartz class and $C_p>0$ is a constant depending on $p$ . I'm very confused, since the book claim it as obvious, but none of the usual tools for inequalities works here (Hölder, Minkowski, Cauchy-Schwarz, Plancherel, Parseval,...) so maybe there is some trick that solve this problem easily, but I can't see it. Any help will be thanked.","['integration', 'fourier-analysis', 'fourier-transform', 'lp-spaces', 'inequality']"
4409407,Multipole-like integral on the unit disc,"I am interested in computing the following integrals \begin{align}
I (\mathbf{x}_{1}) {} & = \!\! \int_{\mathbf{D}} \!\! \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} ,
\\
J (\mathbf{x}_{1} , \mathbf{x}_{2}) {} & = \!\! \int_{\mathbf{D}} \mathrm{d} \mathbf{x} \, \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}| \, |\mathbf{x}_{2} \!-\! \mathbf{x}|}{\sqrt{1 \!-\! |\mathbf{x}|^{2}}} ,
\end{align} where the integrals are to be performed on the unit disc, ${ \mathbf{D} \!=\! \{ \mathbf{x} \!\in\! \mathbb{R}^{2} \, | \, |\mathbf{x}| \!\leq\! 1 \} }$ ,
and the argument ${ \mathbf{x}_{1} , \mathbf{x}_{2} \!\in\! \mathbb{R^{2}} }$ can be both inside or outside the unit disc. I have tried numerous approaches to perform this integral.
For example, to compute ${ I(\mathbf{x}_{1}) }$ I tried: (i) Writing a Fourier decomposition of the form \begin{align}
|\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] \, \sqrt{1 \!+\! \eta^{2} \!-\! 2 \eta \cos (\phi)}
\nonumber
\\
{} & = \sum_{\ell} \mathrm{e}^{\ell \phi} \, [\cdots] ,
\end{align} with $\phi$ the polar angle between $\mathbf{x}_{1}$ and $\mathbf{x}$ , ${ 0 \!\leq\! \eta \!\leq\! 1 }$ given by ${ \eta \!=\! \mathrm{Min}[|\mathbf{x}_{1}|,|\mathbf{x}|]/\mathrm{Max}[|\mathbf{x}_{1}|,|\mathbf{x}|] }$ ,
and the Fourier coefficients, ${ [\cdots] }$ , involving elliptic integrals of the second kind. (ii) Using the usual Legendre expansion of ${1/|\mathbf{x}_{1}\!-\!\mathbf{x}|}$ ,
to write \begin{align}
|\mathbf{x}_{1} \!-\! \mathbf{x}| {} & = \frac{|\mathbf{x}_{1} \!-\! \mathbf{x}|^{2}}{|\mathbf{x}_{1} \!-\! \mathbf{x}|}
\nonumber
\\
{} & = |\mathbf{x}_{1} \!-\! \mathbf{x}|^{2} \, \sum_{\ell} \eta^{\ell} \, P_{\ell} (\cos (\phi)) .
\end{align} Unfortunately, these approaches did not prove much useful in evaluating explicitely the required integrals. From another argument, I know that for ${ |\mathbf{x}_{1}| \!\leq\! 1 }$ , one has \begin{equation}
I (\mathbf{x}_{1}) = \tfrac{1}{2} \pi^{2} + \tfrac{1}{4} \pi^{2} |\mathbf{x}_{1}|^{2} ,
\end{equation} which I can reproduce by computing ${ I(\mathbf{x}_{1}) }$ numerically, but cannot recover analytically. My question are therefore as follows: How can one compute explicitly the integrals ${ I(\mathbf{x}_{1}) }$ and ${ J (\mathbf{x}_{1} , \mathbf{x}_{2}) }$ for arguments both inside and outside of the unit disc? What is the appropriate ""multipole expansion"" that one should use to represent ${ |\mathbf{x}_{1} \!-\! \mathbf{x}| }$ , so as to easily compute such integrals?","['integration', 'improper-integrals', 'definite-integrals', 'closed-form']"
4409426,"Riemann integrable function on [a, b] which is not monotonic [a, b]. [duplicate]","This question already has answers here : Nowhere monotonic continuous function (5 answers) Closed 2 years ago . I need to find a Riemann integrable function, which is not monotonic on a closed interval. But I couldn't find one. I checked some continuous and discontinuous functions but still couldn't find a function satisfying the above things. Can anyone give an example for Riemann integrable function, which is not monotonic on a closed interval? Thanks in advance!","['functions', 'riemann-integration', 'monotone-functions']"
4409470,A sequence of divisors on surfaces,"Let $X$ be a smooth, complex projective algebraic surface. Let $C,D$ be two nonzero effective divisors on it. Then in literature one can find the following exact sequence : $0 \to \mathcal O_D(-C) \to \mathcal O_{C+D} \to \mathcal O_C \to 0$ . I'm a bit confused regarding whether it's a short exact sequence on $X$ as they are line bundles supported on different curves. Here my question is : in the above sequence are we taking the pushforward of these line bundles (under the inclusion map) to $X$ ?i.e. how one appropriately interprets this sequence? Moreover, can we twist the above sequence by line bundles on $X$ as follows : for example can we tensor the above sequence by $\mathcal O_X(C)$ to obtain : $0 \to \mathcal O_D \to \mathcal O_{C+D}(C) \to \mathcal O_C(C) \to 0$ .? Any clarification from anyone is appreciated",['algebraic-geometry']
4409482,Examples of spaces with only trivial vector bundles,"This was originally posted on mathoverflow , but it seems it's more appropriate to post here. Let $B$ be a paracompact space with the property that any (topological) vector bundle $E \to B$ is trivial. What are some non-trivial examples of such spaces, and are there any interesting properties that characterize them? For simple known examples we of course have contractible spaces, as well as the 3-sphere $S^3$ . This one follows from the fact that its rank $n$ vector bundles are classified by $\pi_3 (BO(n)) = \pi_2 (O(n)) = 0$ . I'm primarily interested in the case where $B$ is a closed manifold. Do we know any other such examples? There is this nice answer to a MSE question which talks about using the Whitehead tower of the appropriate classifying space to determine whether a bundle is trivial or not. This seems like a nice tool (of which I am not familiar with) to approaching this problem. As a secondary question, could I ask for some insight/references to this approach? EDIT Now that we know from the answer all the examples for closed $3$ -manifolds (integral homology spheres), I guess I can now update the question to the case of higher odd dimensions. Does there exist a higher dimensional example?","['vector-bundles', 'reference-request', 'algebraic-topology', 'differential-geometry']"
4409503,Existence of $\sin\alpha=\frac{b}{\sqrt{a^2+b^2}}; \cos\alpha=\frac{a}{\sqrt{a^2+b^2}}$,"Let $a,b\in\mathbb{R}$ such that $(a,b)\neq(0,0)$ . Let $x$ be a real number. We make a function such that $A(x)= a\cos(x)+b\sin(x)$ . I need to show the existence of a number $\alpha\in\mathbb{R}$ such that: $$\sin\alpha=\frac{b}{\sqrt{a^2+b^2}}; \cos\alpha=\frac{a}{\sqrt{a^2+b^2}}$$ I thought of using the fact that the sum of the square of the two numbers is equal to one is enough to deduce the existence but I feel like I am wrong. Do you guys have any hints?",['trigonometry']
4409513,"Computing $\int_V(x^2+y^2+z^2)dxdydz,$ where $V$ is the intersection of two spheres in $\Bbb R^3.$","Compute $$\int_V(x^2+y^2+z^2)dxdydz,$$ where $V$ is the intersection of the spheres $x^2+^2+z^2\le1$ and $x^2+y^2+z^2\le 2z.$ My attempt: $$\begin{aligned}\begin{cases}x^2+y^2+z^2&\le 1\\x^2+y^2+(z-1)^2&\le 1\end{cases}\\ \\\text{for the orthogonal projection of the intersection}\\\\1-z^2&=1-(z-1)^2\\2z-1&=0\\z&=\frac12\\\implies \boxed{x^2+y^2\le\frac14}\\\psi(r,\varphi,z)&=(r\cos\varphi,r\sin\varphi,z),\\J_\psi(r,\varphi,z)&=r\\S=\{(r,\varphi,z):0\le r\le\frac12,0\le\varphi\le2\pi,1-\sqrt{1-r^2}\le z\le\sqrt{1-r^2}\}\\\int_Vf(x,y,z)dxdydz&=\int_{\psi(S)}f(x,y,z)\\&=\int_0^{1/2}\int_0^{2\pi}\int_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}(r^2+z^2)rdzd\varphi dr\\&=2\pi\int_0^{1/2}r\left(r^2z+\frac{z^3}3\right)\Big|_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}\\&=2\pi\int_0^{1/2}r\left(2r^2\sqrt{1-r^2}-r^2+\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\right)dr\\&=2\pi\int_0^{1/2}\left(2r^3\sqrt{1-r^2}+\frac23r(1-r^2)^{2/3}-\frac43r+r\sqrt{1-r^2}\right)dr\end{aligned}$$ Side computations: $$\begin{aligned}\frac{\sqrt{1-r^2}^3-(1-\sqrt{1-r^2})^3}3&=\frac{(1-r^2)^{3/2}-1+3\sqrt{1-r^2}-3(1-r^2)+(1-r^2)^{3/2}}3\\&=\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\\\end{aligned}$$ $$\begin{aligned}\int_0^{1/2} 2r^3\sqrt{1-r^2}&=\begin{bmatrix}du=-\frac13r\sqrt{1-r^2}dr\implies  u=(1-r^2)^{3/2}\\v=-6r^2\implies dv=-12rdr \end{bmatrix}\\&=-6r^2(1-r^2)^{3/2}\Big|_0^{1/2}-\int_0^{1/2} -12r(1-r^2)^{3/2}dr\\&=-6r^2(1-r^2)^{1/2}\Big|_0^{1/2}-\frac{12}5(1-r^2)^{5/2}\Big|_0^{1/2}\\\int_0^{1/2}\frac23r(1-r^2)^{1/2}&=-\frac13\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac29(1-r^2)^{3/2}\Big|_0^{1/2}\\\int_0^{1/2}r(1-r^2)^{1/2}dr&=-\frac12\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac13(1-r^2)^{3/2}\Big|_0^{1/2}\end{aligned}$$ I stopped here because this already got too long and it wouldn't work on exam. Is there any better method?","['integration', 'multivariable-calculus']"
4409527,Finding roots of the equation $8x^3+4x^2-4x-1=0$ which are in form of cosine of angles.,"So, I start this by reducing it to a cubic with 0 coefficient of the 2nd degree term which gives me : $$\left(x + \frac{1}{6}\right)^3 - \frac{7}{12}\left(x + \frac{1}{6}\right)-\frac{7}{216}=0$$ Replacing $\left(x + \frac{1}{6}\right)$ by $y$ : $$y^3-\frac{7}{12}y -\frac{7}{216}=0$$ Then replacing $y$ by $r \cos(\alpha)$ and comparing the equation with : $$\cos^3 (\alpha) - \frac{3}{4}\cos(\alpha)-\frac{1}{4}\cos(3\alpha)=0$$ I get: $$r = \sqrt{\frac{7}{3}}, \alpha = \frac{1}{3}\cos^{-1}\left(\frac{1}{2\sqrt{7}}\right)$$ which finally give: $$\theta = \cos^{-1}\left(\frac{\sqrt{7}}{9}\cos^{-1} \left(\frac{1}{2\sqrt{7}}\right)-\frac{1}{6}\right)$$ which is approximately 1.329 on putting it in the calculator. But if I consider the common problem: $8x^3-4x^2-4x+1=0$ having roots $\cos(\pi/7), \cos(3\pi/7), \cos(5\pi/7)$ . If I replace $x$ by $-x$ in this equation, we get: $$8x^3+4x^2-4x-1=0$$ which is same the expression in the actual problem. So its roots must be $-\cos(\pi/7), -\cos(3\pi/7), -\cos(5\pi/7)$ . This clearly doesn't matches with the answer I got. Where I go wrong? Also can someone confirm while comparing coefficients in $cos(\alpha)$ variable cubic equation, we treat $\cos(3\alpha)$ as a constant. My book give a special section in which it shows this method of comparing in this way. So I guess its probably right? But I am not sure how is it right, isn't $\cos(3\alpha)$ also dependent on the main variable $\cos(\alpha)$ .","['algebra-precalculus', 'trigonometry']"
4409533,Minimizing the probability that two iid binomials RVs are equal,"Let $X$ and $Y$ be iid with distribution $\text{Bin}(n,p)$ . I would like to show $P(X=Y)$ is minimzed when $p=1/2$ . $$P(X=Y)=\sum_{i=0}^n P(X=i)P(Y=i)=(1-p)^{2 n} \, _2F_1\left(-n,-n;1;\frac{p^2}{(p-1)^2}\right)$$ ( $_2F_1$ is the Hypergeometric Function ) I'm wondering how to show that such a complicated function is minimzed at $p=1/2$ or if there is an all-around simpler approach for this problem.","['optimization', 'binomial-distribution', 'probability']"
4409559,Geometrical insights on differential equations,"Hi: I am researching about relationships between Differential Geometry and Differential Equations. I am looking for examples and references of the use of geometric concepts to solve or analyze differential equations . For example, in Differential Equations With Applications and Historical Notes the author relates the solution to the Brachistochrone with the Snell law and the behavior of light, providing a very valuable intuition of the form of the solution. I know a lot of geometric concepts are used in this field: orbits, symmetries... I am looking for especially creative or illuminating examples, especially if they had historical significancy. For example, in this question a geometric non-obvious intuition for an important theorem in statistics is provided. Another example is the counterexample to Poincaré-Bendixson Theorem counterexample in the torus, where the geometry of the torus is used to construct non-periodic yet recurrent orbits. Examples and references of relationships in the other direction are also welcome: for example, Picard–Lindelöf theorem can be used to prove every spatial curve is uniquely determined (up to rigid movement) by its curvature and torsion. Thanks in advance.","['ordinary-differential-equations', 'partial-differential-equations', 'differential-topology', 'soft-question', 'differential-geometry']"
4409567,"Does: ""an event recurs infinitely often almost surely"" imply ""the event occurs almost surely""?","I cite this textbook, chapter $6$ . Some definitions: A measure preserving system is a probability space $(X,\Sigma,\mu)$ equipped with a measurable ""dynamic"" $\varphi:X\to X$ such that $\mu\equiv\mu\circ\varphi^{-1}$ everywhere, which intuitively represents the evolution over time. The ""measure algebra"" $\Sigma(X)$ is the quotient set $\Sigma_{/\sim}$ where $A\sim B\iff\mu(A\setminus B)=0=\mu(B\setminus A)$ . Here, we define the ""induced map"" $\varphi^\ast:\Sigma(X)\to\Sigma(X)$ by $[A]\mapsto[\varphi^{-1}A]$ , which is well-defined as $\varphi$ preserves null sets. All the following (in)equalities will be in this a.e. equivalence sense and sets should be interpreted as elements of $\Sigma(X)$ unless otherwise stated. We say that a set $A\in\Sigma(X)$ is recurrent if: $$A\subseteq\bigcup_{n\ge1}\varphi^{\ast n}A$$ And infinitely recurrent if: $$A\subseteq\bigcap_{k\ge1}\bigcup_{n\ge k}\varphi^{\ast n}A$$ For null sets, their $\Sigma(X)$ equivalence class is simply that of all other null sets so recurrence is not meaningful, but for sets of positive measure (infinite) recurrence means that almost all points will (infinitely often) return to the set over time. Poincare's theorem is that every element of $\Sigma(X)$ is infinitely recurrent. I wish to emphasise re currence; studying the definition, we find that recurrence here defined means that almost every point in $A$ is mapped over time back into $A$ , but it says nothing about $A$ occurring in the first place. That is, given that we observe event $A$ in the state space, we can be almost sure we will observe the same event again, infinitely often. The text calmly discusses these results as statements also of oc currence which is not sitting well with me. The chapter begins with a quote from Nietzsche about recurrence: in this spirit they conclude the chapter with a whimsical exercise: The book “Also sprach Zarathustra” by
F. Nietzsche consists of roughly $680,000$ characters, including blanks. Suppose that
we are typing randomly on a typewriter having $90$ symbols. Show that we will almost surely type Nietzsche’s book (just as this book you are holding in your hand)
infinitely often. Show further that if we had been typing since eternity, we almost
surely already would have typed the book infinitely often. (This proves correct one
of Nietzsche’s most mysterious theories, see the quote at the beginning of this chapter.) The expected approach is to use the fact that typing can be modelled as a Bernoulli shift over the alphabet of $90$ symbols and the ""event"" of Nietzsche's book is the cylinder-set of sequences beginning with the first $\approx680,000$ characters of the book and all terms thereafter being arbitrary symbols. The Bernoulli shift system is measure-preserving (even ergodic) so we can say that all events, namely Nietzsche's book, will recur ad infinitum, where ""typing"" is modelled by shifting one character over in the sequence. However, this construction, it seems to me, is conditional on us starting with an element of the book-event (any sequence starting with the book's text). Were we to start with, to invoke the other apocryphal random typewriter experiment (infinite monkey theorem? :) ), Shakespeare's Hamlet, I think we can no longer be almost sure of also typing Thus Spake Zarathrustra. Am I right? The text seems to imply here (and elsewhere, but this was the only example I could find) that regardless of the initial state we would almost surely have typed the book and almost surely will continue to type the book, which seems dissonant with the actual definition of recurrence.","['measure-theory', 'probability-theory', 'infinity']"
4409581,Under what circumstances are the relative Kahler differentials a flat module?,"First let us recall one construction of the relative Kahler differentials. Let $k$ be a ring and $R$ a $k$ -algebra. The relative Kahler differentials $\Omega_{R/k}$ are the $R$ -module satisfying the following universal property: For any $R$ -module $M$ and $k$ -linear derivation $\delta:R\to M$ (an $R$ -module homomorphism satisfying the Leibniz rule), there exists a universal derivation $d:R\to\Omega_{R/k}$ and a unique $R$ -module homomorphism $\varphi:\Omega_{R/k}\to M$ such that $\varphi\circ d=\delta$ . You can use this universal property, along with some direct constructions and short exact sequences, to compute specific examples. Explicitly, suppose $R=k[x_{1},\ldots,x_{n}]/(f_{1},\ldots,f_{s})$ is a $k$ -algebra of finite type. Then one can show that $\Omega_{R/k}=(Rdx_{1}\oplus\cdots\oplus Rdx_{n})/(df_{1},\ldots,df_{s})$ , and $d:R\to\Omega_{R/k}$ is given by $f\mapsto df$ . Namely, the Kahler differentials are the cokernel of the Jacobian matrix $\begin{bmatrix}\frac{\partial f_{i}}{\partial x_{j}}\end{bmatrix}:R^{s}\to R^{n}$ . I'm curious as to what hypotheses are known under which $\Omega_{R/k}$ is a flat $R$ -module. For example, by our calculation above, $\Omega_{R/k}$ is free if $R$ is a polynomial $k$ -algebra, and by Proposition 3.9 of these linked notes by Akhil Mathew, $\Omega_{R/k}$ is projective of rank $\dim R$ if and only if $R$ is regular; hence, $\Omega_{R/k}$ is flat in both of these circumstances. My question: What else can be said if we only need $\Omega_{R/k}$ to be a flat $R$ -module? Even better, are there necessary and sufficient hypotheses on $R$ that guarantee flatness of $\Omega_{R/k}$ , like is the case for projectiveness?","['modules', 'algebraic-geometry', 'abstract-algebra', 'flatness', 'commutative-algebra']"
4409755,Understanding a proof involving the exponential function,"In an exercise sheet from my real analysis lectures, there is a question I have been stuck on for some time. I will attach the solution from the notes in the hope that someone can 'fill in the gaps' and help me understand. Question: Suppose that $f(x)$ satisfies $f(x+y) = f(x)f(y)$ , prove that if $f$ is differentiable then either $f(x)=0$ or $f(x)=e^{ax}$ . I have attached the proof in picture form so that you can see the spots I'm getting confused at: For clarity, I do not understand how the derivative in the first part has been calculated and what might make you try to calculate the $y$ derivative at $y=0$ . Also why do we have an $a$ in $e^{ax}$ ?","['proof-explanation', 'derivatives', 'exponential-function', 'real-analysis']"
4409779,Is the Likelihood of a Regression Model usually Convex?,"This is a question I have always had about Likelihood Functions (e.g. Regression Models): When dealing with Regression Models in the field of statistics, we are interested in fitting a model to some data (e.g. Covariates and a Response Variable) that describes the ""Expected Probability for Observing some Response Conditional on Observing some Specific Set of Covariates"": This is the classical regression framework - given some data, we want to create a regression model that allows us to predict the ""most likely"" value of the response variable given some covariates. There are different estimation methods that allow us to create this regression model based on the data we have observed. One of these methods is called ""Ordinary Least Squares"" (OLS) - this method estimates the ""Beta Regression Coefficients"" that result in the smallest squared distance between the Actual Response and the corresponding Predicted Response by the statistical model. Another method is called ""Maximum Likelihood Estimation"" (MLE), in which the ""Beta Regression Coefficients"" are estimated in such a way that the probability of observing the data is maximized, provided that we assume the data actually originated from some (multivariate) probability distribution function (e.g. a Normal Distribution). For a Simple Linear Regression Model (i.e. with only two Beta Regression Coefficients), for the Maximum Likelihood Formulation, we can write this conditional probability as follows: We soon see that the estimation of these Beta Regression Coefficients is effectively an optimization problem - we are interested in finding the values of ""b_0, b_1 and s^2"" that result in the likelihood formula being maximized (i.e. Arg_Min of the Likelihood): Normally, such Optimization Problems would have required us to use some iterative Optimization Algorithm such as Gradient Descent - however, due to the ""attractive and convenient"" theoretical properties of the Normal Distribution, the above Optimization Problem can give us exact formulas for these Beta Regression Coefficients: My Question: Do we know if the above Likelihood Equation L(b_0, b_1, s^2) that we are trying to Maximize (i.e Optimize) is Convex or Non-Convex? I know that in many regression models, the Optimization Problem being solved has exact solutions which do not require iterative Optimization Algorithms - however, I know that some other regression models (e.g. Logistic Regression) do not have exact solutions and do in fact require some iterative Optimization Algorithm (e.g. Newton-Raphson) to estimate the Beta Regression Coefficients. By doing a quick Google Images Search, it seems that perhaps some of these Likelihood Functions might be Convex and some others might be Non-Convex: But in general, do we ever study the Convexity of Likelihood Functions in Regression Models? Do we know that if some of these Likelihood Functions are usually Convex (e.g. Linear Regression) and if some of these Likelihood Functions are usually Non-Convex (e.g. Logistic Regression)? Does the existence of an exact solution to the Beta Regression Coefficients provide some sort of indication regarding the Convexity of the Likelihood Function (e.g. IF exact solution to the Likelihood Function exists THEN Convex ELSE Non-Convex)? Thanks! Reference: https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf","['statistics', 'normal-distribution', 'optimization', 'convex-analysis', 'probability']"
4409792,How to handle all cases imposed on $x$ so as to solve $\sin({2\cos^{-1}({\cot({2\tan^{-1}x})})}) = 0$?,"Given equation to solve: $$\sin({2\cos^{-1}({\cot({2\tan^{-1}x})})}) = 0$$ What I considered was as $2\arctan x = \arctan(\frac{2x}{1-x^2})$ only when $|x|\leq 1$ . So now, after simplifying that, we reach $2\arccos(...)$ , where again we have to use a condition on $x$ , so in general there are four ( $2\times 2$ ) situations where $x$ can lie and restricted, and we have to check each case and also terminate the extraneuous roots whenever possible. My book just considered a simple case where there is nothing $-\pi$ or $\pi$ m and didn't even bother to impose conditions like $|x|\leq 1$ (even though they were considering just that case only). Why is it possible to just consider that unique case and that will allow us to solve this equation even without bothering about $x$ 's nature? I have posted the solution too given so that if it's totally wrong it should be corrected. As nothing about the nature of $x$ is considered.",['trigonometry']
4409817,Propagating Uncertainties through The Eigenvalue Problem,"For a physics lab we are studying a hanging coupled-spring oscillator with three masses, $M_i$ , and three springs with spring constants $k_i$ . From Newton's laws, ignoring the gravitational force, and assuming the springs are ideal, we derived the following 3-by-3 matrix to describe the system: $$
\textbf{K}=\begin{bmatrix}
-\frac{k_1+k_2}{M_1} & \frac{k_2}{M_1} & 0\\
\frac{k_2}{M_2} & -\frac{k_2+k_3}{M_2} & \frac{k_3}{M_2}\\
0 & \frac{k_3}{M_3} & -\frac{k_2+k_3}{M_3}
\end{bmatrix}.
$$ Since the eigenvalues represent the normal mode frequencies (once appropriately multiplying them by some constants) and the eigenvectors represent the relative offset positions for the normal modes, we are interested in finding the eigenvalues and eigenvectors as well as the uncertainty of the eigenvalues and eigenvectors as each $k_i$ has some uncertainty (on a scale of $10^{-2}$ ). Is there a method that will properly evaluate the uncertainties of the eigenvalues and eigenvectors? A method we tried was to randomly select $10,000$ values from each $k_i$ $68\%$ confidence interval and average the eigenvalues and eigenvectors, however, this method failed due to both the large magnitude of the uncertainties and the sensitivity of the eigenvalue problem.","['statistics', 'eigenvalues-eigenvectors']"
4409819,Kronecker Product of Normal Matrices,"Theorem. If $A\in M_m$ and $B\in M_n$ are both normal, so is $A\otimes B$ . The converse is true if $A\otimes B\ne 0$ . Proof. Suppose that $A\in M_m$ and $B\in M_n$ are both normal. Then \begin{align*}
    (A\otimes B)^*(A\otimes B)=(A^*\otimes B^*)(A\otimes B)&=(A^*A)\otimes(B^*B) \\
&=(AA^*)\otimes(BB^*) \\
&=(A\otimes B)(A^*\otimes B^*)=(A\otimes B)(A\otimes B)^*. 
\end{align*} Therefore, $A\otimes B$ is normal. Now suppose that $A\otimes B$ is nonzero and normal. Then both $A$ and $B$ are nonzero. Then we have \begin{equation*}
    (A^*A)\otimes(B^*B)=(AA^*)\otimes(BB^*). 
\end{equation*} Consider the $i$ -th diagonal block of these two products: \begin{equation*}
    (A^*A)_{ii}(B^*B)=\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)=\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)=(AA^*)_{ii}(BB^*). 
\end{equation*} Summing over all $i$ 's, we can get \begin{equation*}
    \sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ji}|^2}\right)(B^*B)}=\sum_{i=1}^{n}{\left(\sum_{j=1}^{n}{|a_{ij}|^2}\right)(BB^*)}, 
\end{equation*} That is, $$\operatorname{tr}(A^*A)\cdot(B^*B)=\operatorname{tr}(AA^*)\cdot(BB^*).$$ Since $A$ is nonzero, $\operatorname{tr}(A^*A)=\operatorname{tr}(AA^*)\ne 0$ , which implies that $B^*B=BB^*$ and hence $A^*A=AA^*$ . $\blacksquare$ Can anyone please help to check if my proof is valid? For the converse part, I am not confident whether such method indeed works. Besides, if you have any alternative elegant proofs for this theorem, I would be extraordinarily happy to hear from you!","['matrices', 'solution-verification', 'linear-algebra', 'kronecker-product']"
4409839,pure Poisson birth process ordinary differential equations,"Consider the pure Poisson process defined by \begin{align}
P_n'(t) &= -\lambda_n P_n(t) + \lambda_{n-1}P_{n-1}, \quad n \geq 1,\\
P'_0(t) &= -\lambda_0 P_0(t).
\end{align} with $P_0(0) = 1$ . Let $\lambda_n > 0$ for all $n$ , prove that for every fixed $n \geq 1$ , the function $P_n(t)$ first increases, then decreases to $0$ . If $t_n$ is the place of the maximum, then $t_1 < t_2< t_3<\dots$ The hint suggests to use induction and differentiate these set of equations, but I'm not sure how that gives arise to the answer.","['ordinary-differential-equations', 'poisson-process', 'probability']"
4409851,Least number of numbers to guarantee two sum to $101$,"I'm struggling currently with one of my problems in my homework. The professor gave two hints to the following problem. Problem: Find the least amount of different numbers to pick from positive integers that are at most $100$ to guarantee two pairs of numbers that add up to $101$ . Hints: Show that $n$ is sufficient to guarantee Show that $n-1$ is not sufficient Although I understand the basic logic of the pigeon hole principle, I would greatly appreciate it if in the answer, the definition of the pigeon hole is expanded upon.","['pigeonhole-principle', 'discrete-mathematics']"
4409856,What is a chief series of a group $H=(\mathbb{Q}_8 \rtimes C_3 ) \times A_5$?,"The definition of Chief series is given in a link. Question: Consider the group G= $\mathbb{Q}_8 \rtimes C_3 $ (where the action is non trivial), its chief series is $$ e \triangleleft Z_2 \triangleleft \mathbb{Q}_8 \triangleleft G.$$ Is the following a Chief series for the group $H=(\mathbb{Q}_8 \rtimes C_3 ) \times A_5$ ? $$ e \triangleleft (e \times A_5) \triangleleft (Z_2 \times A_5) \triangleleft  (\mathbb{Q}_8 \times A_5)  \triangleleft H.$$ If this is a chief series for $H$ then my question is why?
If this is not then what is a correct series? Thanks! Edit: As @Derek Holt suggested, I have made changes in the question. Thank you!","['semidirect-product', 'group-theory', 'abstract-algebra', 'examples-counterexamples']"
4409883,"How to determine summability of ""nested series""","Consider some non-negative sequences $a_n, b_n, c_n,$ etc. Suppose I have a ""nested series"" (not sure proper terminology) $$\sum_{k = 1}^\infty a_k \sum_{j = 1}^k  b_j   \sum_{i = j}^k c_i$$ etc etc. How should I go about determining the summability of this series? Of course the first approach is to evaluate the inner term and work outwards...but is there a simpler way? Suppose I know the summability of $a_k, b_k, c_k$ (e.g., summable, non summable, summable or non-summable, summable, summable). Is there a fast way of determining if the overall series is summable? Any book that investigate these type of series would help :)","['summation', 'reference-request', 'real-analysis', 'sequences-and-series', 'terminology']"
4409886,Optimal Transport and Entropic Regularization,"We are working with discrete optimal transport. Let $P$ be a matrix and let $H(P) =- \sum_{i,j} P_{i,j} (\log(P_{i,j})-1)$ . Let $C$ be the cost matrix. And $\langle C,P\rangle$ the Frobenius inner product. We introduce the regularized optimal transport problem $\min_{P \in U(a,b)} \langle C,P\rangle + \epsilon H(P)$ . We want to prove that as $\epsilon \to 0$ , $P_\epsilon$ converges to an optimal solution to the original Kantrovich problem with maximal entropy. I understand the proof up to the point where it says for any subsequence of $P_\epsilon$ , we can choose a sub-subsequence of it that converges to an optimal transport plan with maximum entropy. Question 1) The part I don't get is when it says by strict convexity of $-H$ , we get $P^* = P_0^*$ . It is clear that $-H$ is strictly convex, but you still need $-H$ on a convex set. It seems we are only looking at optimal points in the Kantrovich problem, which is not a convex set. Question 2) It says that as $\epsilon \to \infty$ , $P$ gets less sparse, but I would have thought the opposite since more entropy means more uncertainties. Thank you!","['statistics', 'convex-optimization', 'optimal-transport', 'optimization', 'regularization']"
4409920,Understanding a proof on a special case of Riesz's representation theorem,"Let $(S,\mathscr{S},\mu)$ be a measure space and $L:\mathcal{L}^2(\mu)\to\mathbb{R}$ be a linear map satisfying $|Lf|\leq C\left\|f\right\|_{2},\forall f\in\mathcal{L}^2(\mu)$ for some fixed constant $C\geq0$ . Then there exists an a.e. unique element $g\in\mathcal{L}^2(\mu)$ such that $$
Lf=\int fg\mathsf{d}\mu,\forall f\in\mathcal{L}^2(\mu).
$$ I am reading a proof to this proposition from here (Proposition 5.11). The first part is basically showing that there exists $\hat{g}\in\mathcal{L}^2$ such that $\hat{g}\perp f,\forall f\in\ker L$ . This is the part that really confuses me. My questions include: Does the existence of a sequence of $f_n\in\ker{L}$ such that $\left\|f_n-h\right\|\to\inf_{f\in\ker L}\left\|f-h\right\|$ , as argued by the author at the very beginning, use the closedness of $\ker{L}$ ? Since $\ker L$ is closed and $\mathcal{L}^2$ is complete, is it true that the $\hat{f}$ as defined in the proof should be an element of $\ker L$ as well? Why is it true that $\hat{g}\perp f,\forall f\in\ker L$ ? The writer skips that part (he said ""directly follows"") but I am not able to see it. Thank you!","['measure-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4409977,equation for irregular shapes and determining their similarity,"I am a biologist interested in studying cell shape. Cells comes in all different shapes and forms. Some are circular and oval but most have a shape that could be asymmetrical or irregular. A circle and an oval have specific equations. Is it possible to create an equation of a shape like for example the fifth one in the image? I would also like to know if different cell shapes are similar, by that I mean if they could change into one another just by stretching, squeezing, twisting or bending or they are dissimilar, which mean one would need to do something else? In addition, cells change their shape through time, a circle can become an oval and back to circle as the cell moves. Is there a part of math that study changes of shape through time? Sorry that my question is not precise, I'd appreciate any help, for example, which books or branch of mathematics I should look into and try to find experts in, to find answer for such questions? Thank you!",['geometry']
4409993,"If $n>1$, the square of the odd Fibonacci number $F(2n+1)$ can be written as the sum of exactly $F(2n+1)+1$ nonzero squares.","While reading a paper by Owens (arXiv:1906.05913) about embeddings of rational homology balls in the complex projective plane, I found out the following somewhat unexpected number theory corollary (here $F(n)$ denotes the $n$ -th Fibonacci number, with $F(1)=1$ and $F(2)=1$ ). If $n>1$ , the square of the odd Fibonacci number $F(2n+1)$ can be written as the sum of exactly $F(2n+1)+1$ nonzero squares. I will put an answer with the proof. However, I am curious about whether a proof of this result can be obtained by purely numer-theoretic methods.","['algebraic-number-theory', 'number-theory', 'analytic-number-theory', 'differential-topology', '4-manifolds']"
4410002,Probability that N i.i.d. draws from a multinomial distribution have made all events appear,"Consider a multinomial distribution $\mathbb{P}$ on $S$ states $\{s_1,\dots,s_S\}$ where $S\in \mathbb{N}$ and $S\geq 2$ , with probabilities $\mathbb{P}(s_i)=:p_i$ .  Now consider $N$ i.i.d. draws $X_1,\dots,X_N$ from $\mathbb{P}$ and suppose that $N\geq S$ .  What is the probability that each state has occured? So far I have $$
\mathbb{P}\left(\forall s\in S\, (\exists X_i)\, X_i=s\right) = 1- \mathbb{P}(\exists s\in S \, s\not\in \{X_n\}_{n=1}^N)...
$$ But I'm completely stuck","['coupon-collector', 'combinatorics', 'recreational-mathematics', 'problem-solving', 'probability']"
4410013,Radon-Nikodym derivative of Lebesgue measure with respect to Hausdorff measure is Jacobian term?,"Consider the measurable space $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ with Lebesgue measure $\lambda$ . I have a submanifold of $\mathbb{R}^n$ defined as the level set of a smooth function $f:\mathbb{R}^n \to\mathbb{R}^m$ with $n > m$ $$
\mathcal{M} = \left\{x\in\mathbb{R}^n\,:\, f(x) = 0\right\}
$$ This submanifold is $n - m$ dimensional and is embedded in the ambient space $\mathbb{R}^n$ . Let $\mathcal{H}$ be the Hausdorff measure on this manifold. What is the Radon-Nikodym derivative of the Lebesgue measure with respect to the Hausdorff measure? $$
\frac{d\lambda}{d\mathcal{H}} = ?
$$ Attempted Solution Intuitively, I would guess this is the ""jacobian term"" $$
\frac{d\lambda}{d\mathcal{H}}(x) = \left|J_f(x) J_f(x)^\top\right|^{1/2}
$$ where $J_f(x)$ is the Jacobian matrix of $f$ . However I was not able to prove this. To be honest, I am not sure how generally one goes about finding a Radon-Nikodym derivative. What's the strategy one uses, given the two measures? I know that the definition is $$
\lambda(A) = \int_A \frac{d \lambda}{d \mathcal{H}} d\mathcal{H}
$$","['measure-theory', 'lebesgue-measure', 'hausdorff-measure', 'geometric-measure-theory', 'haar-measure']"
4410045,On a property similar to sequentially compact,"Suppose $(X,d)$ is a metric space. If $A$ is a subspace of $X$ and every sequence $\{x_n \}_{n\in \mathbb{N}}$ in $A$ has a subsequence $\{x_{n_{k}} \}_{k\in \mathbb{N}}$ that converges to a point $p$ in $X$ ( $p$ is not necessarily in $A$ ), can we prove that $\overline{A}$ has the same property? i.e., Every sequence $\{x_n \}_{n\in \mathbb{N}}$ in $\overline{A}$ has a subsequence $\{x_{n_{k}} \}_{k\in \mathbb{N}}$ that converges to a point $p$ in $X$ . My thought: if the result does not hold, then we can find a sequence $\{x_n \}_{n\in \mathbb{N}}$ in $\overline{A}\setminus A$ and $\{x_n \}_{n\in \mathbb{N}}$ does not have convergent subsequence in $X$ . But can we find a contradiction using this sequence? Anyway, the construction is as follows: Suppose the result does not hold. Then $\exists \{x_n \}_{n\in \mathbb{N}} \subset \overline {A} $ s.t. its any subsequence $\{x_{n_{k}} \}_{k\in \mathbb{N}}$ does not converge. We claim that there are infinite number of $m\in \mathbb{N}$ s.t. $x_m \in \overline{A}\setminus A$ . For otherwise there are only finite number of $m\in \mathbb{N}$ s.t. $x_m \in \overline{A}\setminus A$ . Then we just throw away these $x_m$ from $\{x_n\}$ , and the rest subsequence $\{x'_n \}_{n\in \mathbb{N}}$ is in $A$ , hence it has a convergent subsequence by the property of $A$ , contradiction. We thus find a sequence $\{x_m \}_{m\in \mathbb{N}}$ in $\overline{A}\setminus A$ and $\{x_m \}_{m\in \mathbb{N}}$ does not have convergent subsequence in $X$ .","['general-topology', 'metric-spaces', 'compactness', 'sequences-and-series']"
4410054,Deriving a mixed distribution from exponential and inverse gamma,"Question You are given the following: the amount of an individual loss in the year $2022$ follows an exponential distribution with mean $15000$ Between $2022$ and $2025$ , losses will be multiplied by an inflation factor $C$ . For example, if $C = 1.2$ , then losses in $2025$ will be $20\%$ more than that in $2022$ . You are uncertain of what $C$ will be, but estimate that it will be a random draw from a inverse gamma distribution with parameters $\alpha = 2.4$ and $\delta = 3.2$ . The inverse gamma distribution has pdf $f(x; \alpha, \delta) = \frac {\delta^{\alpha}} {\Gamma(\alpha)} x^{-\alpha + 1} e^{-\frac {\delta} x}$ . Estimate the probability that a loss in $2025$ will exceed $100000$ . My thoughts I know that the mixture of an exponential and an inverse gamma will result in a Pareto distribution. However, I am not sure how to account for $C$ in this case. I mean, if the exponential had a mean of $C$ and $C$ followed an inverse gamma, I would know how to compute the integral. Does the computation of the resulting distribution require me to take into account $C$ , or does it not matter? How should I go about evaluating this? Any intuitive explanations will be greatly appreciated :)","['statistics', 'probability-distributions', 'exponential-distribution', 'gamma-distribution', 'probability']"
4410070,Group of order $1320$ is not simple,"Group of order $1320 = 2^3\cdot 3\cdot 5\cdot 11$ is not simple. Proof. Suppose there is a simple group $G$ of order $1320$ . Then the number of Sylow $11$ subgroup $n_{11} = 12$ . Let $G$ act on ${\rm Syl}_{11}(G)$ by conjugation so we get an injective homomorphism $G\to S_{12}$ . Now, if $P_{11}\in{\rm Syl}_{11}(G)$ then $|N_G(P_{11})| = 110$ and $|N_{S_{12}}(P_{11})| = 110$ . I want to show the map $\varphi:G\to S_{12}$ equals $G\xrightarrow{\tilde{\varphi}} A_{12}\hookrightarrow S_{12}$ for some $\tilde{\varphi}$ so that I can get a contradiction $|N_{A_{12}}(P_{11})|= 55$ . How can I show this?","['simple-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4410092,How to calculate a limit of a simple sequence?,"I have to admit that I have forget most of my limit knowledge and I would appreaciate an advice with this. The problem is: $a_1 := 1$ $a_{n+1} := \dfrac{a_n^2}{4} + 1$ Calculate the limit for $n \rightarrow \infty$ . My thoughts: I would say the limit is $\infty$ and maybe rewrite the problem as $a_{n+1} := \dfrac{a_n^2 + 4}{4}$ ? Also, I remember the ""known limits"" - i.e. $(1 + 1/x)^x$ converges to $e$ in $\infty$ etc. etc.
But I cannot see anything useful to solve the problem above. Thank you for help!","['limits', 'analysis']"
4410093,Solving a 2nd order nonlinear ODE through substitution based linearisation,"I have the following nonlinear differential equation, $$y y''-(y')^2-yy'(\alpha y-\beta)=0,\qquad (\ast)$$ and I was wondering whether it would be possible to linearise it? Whilst browsing the web, I came across this equation: $$y y''-(y')^2+f(x)yy'+g(x)y^2=0,$$ which I link here, which may be solved through the substitution $u=y'/y$ . They are astoundingly similar and I was therefore curious as to whether my equation $(\ast)$ can also be solved through a similar process of linearisation. I have not made the breakthrough so far and any help would be appreciated.","['calculus', 'derivatives', 'ordinary-differential-equations']"
4410136,Does FOSD + log concavity of $f(x)$ and $g(x)$ imply MLRP?,"I am looking for a result on the ordering of distribution functions. The probability density functions $f(x)$ and $g(x)$ bear the Monotone Likelihood Ratio Property (MLRP) if $$ \frac{f(x)}{g(x)} $$ is increasing in $x$ . By First Order Stochastic Dominance (FOSD) of the probability distribution function $F(x)$ over $G(x)$ , it follows that $$ F(x) \leq G(x) $$ for all $x$ , with strict inequality for some $x$ . It is well known that the MLRP implies First Order Stochastic Dominance (FOSD) but not vice versa. I am interested in sufficient conditions on $f(x)$ and $g(x)$ that, together with $F(x)$ FOSD $G(x)$ imply the MLRP. It seems to me that assuming that both $f(x)$ and $g(x)$ are log-concave should be sufficient. It holds in the following example: Consider $g(x) = f(x + a)$ , where $a>0$ . I.e., $g$ is a version of $f$ shifted to the left. FOSD obviously holds. In this case, MLRP also holds because $$ \frac{\partial\frac{f(x)}{g(x)}}{\partial x} > 0 \Leftrightarrow \frac{f'(x)}{f(x)} - \frac{g'(x)}{g(x)} > 0\Leftrightarrow \frac{f'(x)}{f(x)} - \frac{f'(x+a)}{f(x+a)} >0.$$ The last condition holds by log-concavity, as $\frac{f'(x)}{f(x)}$ is decreasing in $x$ . I'm not so sure how to generalize from this to other cases where $f$ and $g$ are both log-concave but where $g$ might not be a simple left shift of $f$ . Has anyone got ideas how to show this or approach this problem?","['statistics', 'probability-distributions', 'real-analysis', 'order-statistics', 'probability']"
4410139,Prove that $\sum{\frac{1}{(k+1)^\alpha}\frac{1}{(n+1-k)^\beta}}\le K\frac{1}{(n+1)^\alpha}$,"I want to prove that $$\sum_{k=0}^{n}{\frac{1}{(k+1)^\alpha}\frac{1}{(n+1-k)^\beta}}\le K\frac{1}{(n+1)^\alpha}$$ for all $n\ge 0$ , where $1<\alpha\le\beta$ and $K$ a constant. Everything I tried to do, I always arrived at the constant depending on $n$ , which cannot happen. And I didn't want to use induction, is it possible?","['inequality', 'sequences-and-series', 'real-analysis']"
4410213,Mean of an Exponential Distribution whose rate parameter is also exponentially distributed,"Suppose I have a random variable $X$ with an exponential distribution with rate parameter $\lambda$ . Suppose also that I don’t know the value of $\lambda$ but that it will be drawn from another exponential distribution with rate parameter $K$ . I’m trying to figure out what my expected value for $X$ is in terms of $K$ . The integral as I understand it seems to be $\int \frac{Ke^{-Kx}}{x}$ Playing around it seems as though setting $K = 1$ gives $X$ a mean of the Exponential Integral function $\mathrm{Ei}(0)$ (please correct me if this is wrong), but I’m not familiar enough with this function to understand how changing $K$ affects this output In particular, setting $K = 2$ seems to yield $\int \frac{2e^{-2x}}{x} = 4\int \frac{e^{-2x}}{2x} = 4 \mathrm{Ei}(0)$ Which intuitively seems wrong as increasing the rate parameter should decrease the mean. Clearly I’m doing something very stupid here but would appreciate pointers! Thanks","['expected-value', 'calculus', 'exponential-distribution', 'exponential-function', 'random-variables']"
4410246,Proving that the derivative at a local extremum is 0,"We wish to prove that for differentiable $f:[a,b] \to \mathbb{R}$ , if $f$ has a local extremum $c$ , then $f'(c)=0$ . Proof:
Consider the function $f$ having a local maximum (otherwise consider $-f$ ). Supposing $c$ is a local extremum, we must have that $f(c)\ge f(x)$ $\forall x \in(c-\delta,c+\delta), \delta>0$ . We know then that $f(x)-f(c)\le 0$ . We also have that either $x-c <0$ or $x-c >0$ , as we are assuming $x$ and $c$ are distinct. Then, if we have $x-c>0$ then taking the limit as $x$ approaches $c$ of $\frac{f(x)-f(c)}{x-c}$ , we get that $f'(c) \le 0$ . On the other hand, if $x-c<0$ then taking the limit as $x$ approaches $c$ of $\frac{f(x)-f(c)}{x-c}$ , we get that $f'(c) \ge 0$ . For both conditions to hold, we must have $f'(c) =0$ . My lecturer gave a different proof in the notes for our module, but is this way correct?","['solution-verification', 'derivatives', 'real-analysis']"
4410358,Integral of $(1+\cos(t))^n$ from $-\pi$ to $\pi$,"Using an integration solver (Mathematica), I've gotten the following result $$\int_{-\pi }^{\pi } (1+\cos (t))^n \, {\rm d}t = 2^{1-n} \pi \binom{2 n}{n}$$ Any suggestion on how to prove this would be appreciated. I'm not sure how the binomial coefficient gets introduced, my best guess is to use some properties of the gamma function although in not sure which. I'm also familiar with the basics of contour integration but not sure if that can be applied here. Also, if anyone knows a source for this integral that would be appreciated.","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4410386,Invariants of bitopological spaces,"A bitopological space is a set $X$ with two fixed Hausdorff topologies $\tau_1, \tau_2$ . In my case I am interested in the case where $\tau_1 \subseteq \tau_2$ . Say that a bitopological space $(X, \tau_1, \tau_2)$ is compactly almost metric , when $\tau_1 \subseteq \tau_2$ , $\tau_2$ is metrisable $\tau_1, \tau_2$ have the same compact sets. Are there any non-trivial sufficient conditions so that if $(X, \tau_1, \tau_2)$ and $(Y, \sigma_1, \sigma_2)$ are compactly almost metric and $(X,\tau_2)$ is homeomorphic to $(Y,\sigma_2)$ , then $(X, \tau_1)$ is homeomorphic to $(Y, \sigma_1)$ ? Since the topic is rather obscure, an idea where to look up such things would be appreciated too.",['general-topology']
4410415,CFG for $\{0^i1^j0^k\mid i+2j=3k\}$,"Edited: I try to find a Context-Free grammer for $\{0^i1^j0^k\mid i+2j=3k\}$ as follow \begin{align*}
    S&\to 000S0| 111B00| 01B1| 001B1|\lambda\\
    B&\to 111B00| \lambda
\end{align*} But above grammer accept $0^21^40^3$ that doesn't belong to $L$ . How I can correct above grammer?","['formal-languages', 'context-free-grammar', 'discrete-mathematics']"
4410417,Warp product structure on the complex hyperbolic space,"It is known that the real hyperbolic space $\mathbb{H}^n$ has a warped product structure $g = dr^2 + \sinh^2 r\; ds^2_{n - 1} $ , where $r \in (0, \infty)$ . My question is, does the complex hyperbolic space $\mathbb{CH}^n$ also have a warped metric? Recall that a warped product metric on $(M, g) \times (N, h)$ is defined as $g + \varphi^2(x) h$ , where $\varphi$ is a positive smooth function on $M$ . This is probably well-known, so this is mainly a reference request.","['riemannian-geometry', 'reference-request', 'hyperbolic-geometry', 'homogeneous-spaces', 'differential-geometry']"
4410426,Martingale with Gaussian marginals that is not jointly Gaussian,"Let $X_0,X_1$ be random variables such that $E[X_1|X_0]=X_0$ , $X_0 \sim \mathcal N (0, \sigma_0^2)$ , and $X_1 \sim \mathcal N (0, \sigma_1^2)$ (notice that $\sigma_0^2 \leq \sigma_1^2$ by the martingale condition). I am wondering if the joint distribution of $(X_0,X_1)$ can be non-Gaussian under the above conditions (without the martingale condition, it is easy). And if so, could I see an example? A Gaussian example (and it is unique) is given by the conditional distribution $X_1|X_0 \sim \mathcal N (X_0, \sigma_1^2 -\sigma_0^2)$","['real-analysis', 'stochastic-processes', 'martingales', 'gaussian', 'probability-theory']"
4410498,Integration $\int_{-\infty}^{\infty} \frac{e^{-a^2x^2}}{1+e^{x+\eta}} dx$.,"I've encountered the following integral $\displaystyle \int_{-\infty}^{\infty} \frac{e^{-a^2x^2}}{1+e^{x+\eta}} dx$ . When $\eta=0$ , I have managed to solve it. However, when $\eta \neq 0$ I have not. @Jack D'Aurizio has answered the question here : $\displaystyle \int_{-\infty}^{+\infty}\frac{e^{-a^2 x^2}}{1+e^{x+\eta}}\,dx = \displaystyle \int_{-\infty}^{+\infty}\frac{e^{-a^2 (x-\eta)^2}}{1+e^x}\,dx = \displaystyle e^{-a^2\eta^2}\int_{0}^{\infty}e^{-a^2 x^2}\left(\frac{e^{2a^2\eta x}}{1+e^x}+\frac{e^{-2a^2\eta x}}{1+e^{-x}}\right)\,dx$ in combination with a geometric series expansion: $\frac{e^{\pm
 2a^2\eta x}}{1+e^{\pm x}}=\sum_{m\geq 0}(-1)^m
 e^{\pm(2a^2\eta+m)x},\qquad \int_{0}^{+\infty}e^{-a^2 x^2}\cosh(\mu
 x)\,dx=\frac{\sqrt{\pi}}{2a}\,e^{\frac{\mu^2}{4a^2}}$ . However, I can't understand one step. Namely, I do not understand the step  where $$ I=\frac{e^{\pm 2a^2\eta x}}{1+e^{\pm x}}=\sum_{m\geq 0}(-1)^m e^{\pm(2a^2\eta+m)x}.$$ Following the tip from  @Kurt G. I ended up with the following: $ \displaystyle \frac{e^{ 2a^2\eta x}}{1+e^{x}}=\sum_{m=0}^{\infty}(-1)^m e^{(2a^2\eta+m)x}, \text {for } e^{x} < 1$ .
Since the term (I) is part of an integral from 0 to $\infty$ why it is assumed that $e^{x} < 1$ ? $\displaystyle  \frac{e^{(-2a^2\eta \color{red}{ + 1}\color{black}{)x}}}{1+e^{x}}=\sum_{m=0}^{\infty}(-1)^m e^{-(2a^2\eta+m)x}, \text {for } e^{-x} < 1$ (because $x>0 \rightarrow -x<0 \rightarrow e^{-x}<1$ ). The red term is not given in (I). Is it skipped or I missed something? Thank you very much.","['integration', 'definite-integrals', 'complex-analysis', 'residue-calculus', 'complex-integration']"
4410526,Error propagation in compass and straightedge constructions,"I was trying to assess the impact of non-idealities on the outcome of a classical geometric construction, performed on paper with actual compass and straightedge. I was thinking of possible approaches, but at the same time I didn't want to start from scratch, expecting that someone must have investigated this topic before me. In fact I found this article. I seem to find only this article. The author hits the core, he perfectly delineates the matter, and in particular he includes images that are pretty self-explanatory: Unluckily he develops the discussion in qualitative terms and does not offer a useful framework. Near the ends he writes ""The goal might be to mount a precise analysis of all the standard constructions and compare competing constructions for accuracy. There is a literature of papers doing precisely this, and I will try to post some references later"", but apparently he could not recall what such references were. Can anybody help me on this subject, suggesting methods or texts? Thank you!","['analytic-geometry', 'error-propagation', 'geometry', 'geometric-construction']"
4410536,Series solution ODE,"I am trying to solve the ODE $y''+y=0$ using a series solution. Here is what I have done so far. Assume the solution is of the form $y(x)=\sum_{k=0}^\infty a_kx^k$ . Then the derivatives are $y'(x)=\sum_{k=1}^\infty ka_kx^{k-1}$ and $y''(x)=\sum_{k=2}^\infty k(k-1)a_{k}x^{k-2}$ I then combine these into the series $$\sum_{n=0}^\infty [(n+2)(n+1)a_{n+2}+a_n]x^n = 0$$ Would I then say that $\forall n\in \mathbb{N}$ $(n+2)(n+1)a_{n+2}+a_n=0$ and solve for $a_n$ ? My question is: If a power series equals zero, do all the terms have to necessarily be zero?","['power-series', 'calculus', 'analysis', 'ordinary-differential-equations']"
4410566,The product rule for Brownian Motion,"I am reading Martin Baxter's book on Financial Calculus and in it, the product rule for the common Brownian motion case is described. I cannot understand how the Ito's formula applies here. First, we start with $$
dX_t = \sigma_t dW_t + \mu_t dt
$$ $$
dY_t = \rho_t  dW_t + \nu_t dt
$$ The book now says that by applying Ito's lemma to $f(X_t, Y_t) = \frac{1}{2}\left( (X_t + Y_t)^2 - X_t^2 - Y_t^2 \right) = X_t Y_t$ we can see that: $$
d(X_t Y_t) = X_t dY_t + Y_t dX_t + \sigma_t \rho_t dt
$$ How is Ito's lemma applied to $f(X_t, Y_t)$ ? Ito's lemma states that if $dX_t = \sigma_t dW_t + \mu_t dt$ and $Y_t = f(X_t)$ then: $$
dY_t = \sigma_t f'(X_t) dW_t + \left( \mu_t f'(X_t) + \frac{1}{2} \sigma_t^2 f''(X_t) \right) dt
$$ but I don't yet know how to apply this to a function of 2 stochastic variables and the book hasn't mentioned anything about the application in this case. I see the result for $d(X_tY_t)$ but I don't understand how we got it by applying Ito's lemma.","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4410572,Probability kernels can be fully described by a certain measurable function,"I am working through some old lecture notes about stochastic processes. The author gives the following lemma there: Let $(S, \mathcal{S})$ be a measurable space, $(T, \mathcal{T})$ a Borel space, $\mu: S \times \mathcal{T} \to [0, 1]$ a probability kernel and $Y$ a $\mathcal{U}([0, 1])$ -distributed random variable. Then there exists a $\mathcal{S} \otimes \mathcal{B}([0, 1])$ -measurable function $f: S \times [0, 1] \to T$ such that $$
\mu(s, A) = \mathbb{P}\left[f(s, Y) \in A \right] = \int_0^1 1_{\{f(s,\cdot) \in A \}} (u) du
$$ for all $s \in S$ and $A \in \mathcal{T}$ . However, the proof does not seem right. After some research I found out that it is Lemma 3.22 in Kallenberg's Foundations of Modern Probability (2nd edition). There proof goes as follows: We may assume that $T$ is a subset of $[0, 1]$ . This is because by definiton of Borel spaces, there exists a Borel set $U \subset [0, 1]$ and a measurable bijection $h: T \to U$ such that the inverse $h^{-1}$ is $\mathcal{B}(\mathbb{R}) \cap U$ -measurable. ( I understand this. ) We can easily reduce this to the case $T = [0, 1]$ . ( I don't understand this part. ) Define $$f(s, t) := \sup \left\{ x \in [0, 1]: \mu(s, [0, x]) < t \right\}$$ and note that $$ f(s, t) \leq c \quad \Leftrightarrow \quad \mu(s, [0,c]) \geq t \qquad \text{for all}\ c \in [0, 1].$$ Then we have $$
\mu(s,A) = \mathbb{P}[\mu(s, A) \geq Y] = \mathbb{P}[f(s, Y) \leq c] \qquad \text{for all}\ c \in [0, 1].
$$ $f$ is $\mathcal{S} \otimes \mathcal{B}([0, 1])$ -measurable  because $$f^{-1}([0, c]) = \{ (s, t) \in S \times [0, 1] : f(s,t) \leq c \} = \{ (s, t) : \mu(s, [0, c]) -t \geq 0 \} $$ is meausrable as the reverse image of a jointly measurable function. Question: How is the second step  justified? Is it true that there is always a measurable bijective function between a Borel set $U \subset [0, 1]$ and the unit interval?","['stochastic-processes', 'measure-theory', 'probability-theory']"
4410589,"Simple quadratic function, but can't understand the question","I have a very very simple quadratic function: Now building a new tunnel that has a shape of parabolic curve. The tunnel is 10 m wide and at 4 m from either side, the height of the
tunnel is 6 m. Find the quadratic equation in standard form that
models the ceiling of the new tunnel. The question is very simple, but I just can't understand what does at 4 m from either side mean in this question while mentioning the width in 10m? I am not a native English speaker so it is hard for me understand what the information this question is asking. Any comments and answers will be appreicated! Edit: I see comments where people say that this question is badly word, I agree. Actually, this is a question I did on test and I just get the result today, none of the people in my class get the answer. It not me made the question.","['algebra-precalculus', 'quadratics']"
4410592,Combinatorial interpretation of polynomials,"I'm reading a proof for the following identity: $$\sum_{k}A_{k}(r,t)A_{n-k}(s,t) = A_{n}(r+s, t), \qquad \text{integer} \ n \geq 0 \tag{1}$$ where $A_{n}(x,t)$ is the $n$ th degree polynomial in $x$ that satisfies $$A_{n}(x,t) = \binom{x-nt}{n}\frac{x}{x-nt}, \qquad \text{for} \ x \neq nt. \tag{2}$$ The author starts off by assuming that $r \neq kt \neq s$ for $0 \leq k \leq n$ , since both sides of (1) are polynomials in $r, s, t$ . Question: I understand that we need $r \neq kt$ to prevent division by zero in $A_{k}(r,t) = \binom{r-kt}{k}\frac{r}{r-kt}$ , but why are we also allowed to assume $kt \neq s$ for $0 \leq k \leq n$ ? Writing $A_{n-k}(s,t)$ in terms of (2), we have $$A_{n-k}(s,t) = \binom{s-(n-k)t}{n-k}\frac{s}{s-(n-k)t}$$ If $s=kt$ , then $$
A_{n-k}(kt,t) = \binom{kt-(n-k)t}{n-k}\frac{kt}{kt-(n-k)t}
= \binom{2kt-nt}{n-k}\frac{kt}{2kt-nt}
$$ ...which doesn't result in division by $0$ if $k=0$ or if $k=n$ , so why is $s \neq kt$ needed?","['proof-explanation', 'binomial-coefficients', 'polynomials', 'discrete-mathematics']"
4410603,"Basic multivector derivatives $∂_X X = n$ and $∂_X X^2 = 2X$, etc…","Using geometric algebra, one may define the multivector derivative $∂_X$ with respect to a general multivector $X$ as $$
∂_X ≔ \sum_J 𝒆^J (𝒆_J * ∂_X)
$$ where each “component” $𝒆_J * ∂_X$ is defined by $$
(𝒆_J * ∂_X)f(X) ≔ \frac{\mathrm{d}}{\mathrm{d}\tau}f(X + \tau𝒆_J)\big|_{\tau=0}
.$$ Notations $A * B \equiv ⟨AB⟩_0$ denotes the scalar product.
For any multivector $A$ , we have $𝒆^J(𝒆_J * A) = A$ .
In literature, parentheses are often dropped with the understanding that $A*B C ≡ (A*B)C$ . We employ multi-index notation, $𝒆_J = 𝒆_{j_1}∧\cdots∧𝒆_{j_k}$ . (If $k = 0$ then $𝒆_J = 1$ ).
Reciprocal bases are reversed , $𝒆^J = 𝒆^{j_k}∧\cdots∧𝒆^{j_1}$ , so that $𝒆^I * 𝒆_J = δ^I_J$ is always satisfied. Problem I’m having a humiliating time trying to sanity-check this definition by verifying, e.g., $∂_X X = n$ , as stated in eq. (2.29) of [2] or eq. (7.8) of [3].
My computation begins as follows. $$
∂_X X
    = \sum_J 𝒆^J (𝒆_J * ∂_X)X
    = \sum_J 𝒆^J \frac{\mathrm{d}}{\mathrm{d}\tau} (X + \tau𝒆_J)\big|_{\tau=0}
    = \sum_J 𝒆^J 𝒆_J
.$$ There seems to be no room for confusion here.
But this is not $n$ . Indeed, \begin{align}
\sum_J 𝒆^J 𝒆_J &= \sum_{k=0}^n \sum_{j_1 < \cdots < j_k} \underbrace{𝒆^{j_1\cdots j_k}𝒆_{j_k\cdots j_1}}_1 = \sum_{k=0}^n \binom{n}{k} = 2^n
.\end{align} This contradicts Proof 46 of [3], which includes the step “ $\sum_{J_d} δ^J{}_J = d$ ” (a sum over multi-indices in $d$ dimensions) — which I can’t see to be true! My failure is easily generalised: in trying to show $∂_X X^2 = 2X$ , we have \begin{align}
∂_X X^2 &= 𝒆^J(𝒆_J * ∂_X)X^2 = 𝒆^J \frac{\mathrm{d}}{\mathrm{d}\tau} (X + \tau𝒆_J)^2\big|_{\tau=0}
\\ &= 𝒆^J(𝒆_J X + X 𝒆_J) = 2^n X + 𝒆^J X 𝒆_J
.\end{align} Note that it is easy to verify these results with the less general vector derivative , $\vec ∂ ≔ 𝒆^i ∂_i$ where $∂_i = 𝒆_i * ∂_X$ in the notation above.
Then, if $X = X^i𝒆_i$ is the position vector, we have $∂_i X = 𝒆_i$ and thus $
\vec ∂ X = 𝒆^i ∂_i X = 𝒆^i 𝒆_i = n
$ and $$
\vec ∂ X^2 = 𝒆^i \frac{\mathrm{d}}{\mathrm{d}\tau} (X + \tau𝒆_i)^2\big|_{\tau=0} = 𝒆^i(𝒆_i X + X 𝒆_i) = 2𝒆^i \, 𝒆_i * X = 2X
.$$ Clearly I am misinterpreting the way in which the vector derivative $\vec ∂ = 𝒆^i(𝒆_i * ∂_X)$ is ‘generalised’ to have components at all grades, $∂_X = 𝒆^J (𝒆_J * ∂_X)$ .
Could someone with fresh eyes help me out? References Lasenby and Doran, “Multivector Lagrangian Fields” – Ch. 1. Hestenes and Sobczyk, “Clifford Algebra to Geometric Calculus” – Ch. 2, §2. Hitzer, “Multivector Differential Calculus” – page 3.","['geometric-algebras', 'calculus', 'clifford-algebras']"
4410676,Is there a historical connection between $\sigma$-algebra and topology?,"Through learn about probability theory and topology, I feel that definition of $\sigma$ -field(algebra) and topology are similar. From this , I also know about they are not same each other. Furthermore, I interested about why they look like similar. Is there a historical connection between them? Is it result of convergent evolution? Or just by a chance? I think that, if there are something (historical) reason for similarity, it will be strongly connected to how they different. So it will make me naturally understand about the difference.","['general-topology', 'math-history', 'measure-theory']"
4410687,General solution of $\frac{d^2x}{dt^2}+a\frac{dx}{dt}+bx=0$ for identical roots,"Let the trial solution of a differential equation of the form $\frac{d^2x}{dt^2}+a\frac{dx}{dt}+bx=0$ ( $a,b$ are constants), be $x=e^{mt}$ . If it turns out that the two roots of $m^2+am+b=0$ are the same i.e. $m_1=m_2$ , the general solution cannot be obtained from $$x=c_1e^{m_1t}+c_2e^{m_2t}$$ by setting $m_1=m_2(=m)$ . Instead the solution is $$x=(c_1+c_2t)e^{mt}$$ Why is that? How do we justify this solution or explain this?",['ordinary-differential-equations']
4410720,Chance of meeting probability,"Two people arrive at a train station some random time between $5$ and $6$ a.m.. Arrival times are uniformly distributed. They stay for exactly five minutes and leave. What's the chance that they meet on a particular day? My given answer is $23/144$ . Equivalently, given $X, Y \sim U(0, 60)$ , we want $P(|X - Y| \leq 5)$ . I know that there's a geometric approach, but I want to know what's wrong with my following approach: I will compute $P(X \leq Y + 5)$ and adjust the answer to account for both cases. $$P(X \leq Y + 5) = \int P(X \leq Y + 5 \mid Y = k)P(Y = k) = \int P(X \leq k + 5)f_{Y}(k) \mathop{dk} $$ $$= \frac{1}{60} \int P(X \leq k + 5).$$ For $0 \leq k \leq 55$ , we have $P(X \leq k + 5) = (k + 5)/60$ . For $55 < k \leq 60$ , we have $P(X \leq k + 5) = 1$ . Thus, $$P(X \leq Y + 5) = \frac{1}{60} \left[\int_{0}^{55} \frac{k + 5}{60} \mathop{dk} + \int_{55}^{60} 1 \mathop{dk}\right]= \frac{1}{60}\left[\frac{715}{24} + 5\right] = \frac{167}{288}$$ This must be wrong because there's no clear way to go from this to $23/144$ . Can someone please point out my mistake?","['integration', 'calculus', 'probability-theory', 'probability']"
4410729,Proof of different sizes between similar infinite sets,"I wish you a fantastic day! Let's define $\mathbb N$ as the set of all $n≥0$ .
Let's define $\mathbb N^*$ as the set of all $n>0$ . My target within a larger problem is to try and prove that a function defined as $$f : \mathbb N^* \to  \mathbb N$$ can't be a bijection. This is, in my reasoning, because there are more elements in the second set. That is, $0$ is in $\mathbb N$ , but not in $\mathbb N^*$ and as such, $\mathbb N$ is bigger . $$\mathrm{card}(\mathbb N) > \mathrm{card}(\mathbb N^*)$$ Where card = number of elements However, I can't find a reasonable and satisfactory way of proving this. I have tried proving by association, that is: Let every element from the first set be paired with the element of the same index from the second set. Since the $\mathbb N^*$ is going to use all and just the elements from $\mathbb N \setminus \{0\}$ , $\mathbb N$ has one additional element. Thank you for reading this and thanks in advance!","['elementary-set-theory', 'elementary-number-theory', 'solution-verification']"
4410755,Is there a way to calculate the eigenvalues of $xx^T+yy^T$?,"As the title shows, is there a way to calculate the eigenvalues of $A\equiv \vec x\vec x^T+\vec y \vec y^T$ , where $\vec x$ and $\vec y$ are two linearly independent vectors in $\mathbb{R}^n$ (don't have to be unit vector). Here're some of my thoughts. We can see that $A$ is of rank 2 since it can be seen as a map from $\mathbb{R}^n$ to $\mathbb{R}^n$ and there are only two linearly independent vectors in its range. So we may set the eigenvectors of $A$ as $a\vec x+b\vec y$ , where $a$ and $b$ are two real numbers, then we have chances to get the eigenvalues by solving $$A(a\vec x+b\vec y) =\lambda (a\vec x+b\vec y)\tag{1}.$$ For example, if $\vec{x}=\frac{1}{\sqrt{2}}\left( \begin{array}{c}
	1\\
	1\\
\end{array} \right) , \vec{y}=\left( \begin{array}{c}
	1\\
	0\\
\end{array} \right) $ , we can solve eq(1) to get the eigenvalues $1\pm \frac{1}{\sqrt 2}$ . But eq(1) only have two equations while with 3 parameters $a,b,\lambda$ . So my question is : is there a way to calculate the eigenvalues of $xx^T+yy^T$ , such as a formula related to $\vec x$ and $\vec y$ that I don't know?","['matrices', 'matrix-analysis', 'linear-algebra', 'eigenvalues-eigenvectors']"
4410758,Computing $I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy$,"Compute $$I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy,$$ where $D=\{(x,y)\in\Bbb R^2\mid x^2+y^2\le 1, x\ge 0, y\ge 0\}.$ Source: Berman, task 3537 My attempt: I switched to polar coordinates $$\begin{cases}x=r\cos\varphi\\y=r\sin\varphi\end{cases}, 0\le\varphi\le \pi/2,0\le r\le 1$$ $$\begin{aligned}I&=\int_0^{\pi/2}\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdrd\varphi\\&=\frac\pi2\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdr\\&=\begin{bmatrix}t&=r^2\\dt&=2rdr\end{bmatrix}\\&=\frac\pi4\int_0^1\sqrt{\frac{1-t}{1+t}}dt\\&=\begin{bmatrix}s=\sqrt{\frac{1-t}{1+t}}\implies(1+t)s^2=1-t\implies s^2+ts^2=1-t\implies(1+s^2)t=1-s^2\implies t=\frac{1-s^2}{1+s^2}\\\implies dt=-\frac{4sds}{(1+s^2)^2}\end{bmatrix}\\&=-\frac\pi4\int_1^0\frac{4s}{(1+s^2)^2}ds\\&=\frac\pi2\int_0^1\frac{2sds}{{(1+s^2)}^2}\\&=\begin{bmatrix}v=1+s^2\\dv=2sds\end{bmatrix}\\&=\frac\pi2\int_1^2\frac{dv}{v^2}\\&=-\frac\pi2\frac1v\Big|_1^2\\&=\frac\pi4\end{aligned}$$ The solution in the book is $\frac{\pi(\pi-2)}8.$ What did I do wrong?","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4410779,Computing : $\displaystyle \int \frac {m + n\cos (x-x_0) }{ [a+b\cos (x-x_0)]^{3/2} } \mathrm{d}x $ with integration of $E$ and $F$,"Yesterday I tried to find an expression to predict the behaviour of Magnetic Field due a circular loop at any point in the space, in cylindrical co-ordinates. At the end, I am stuck with some integrals. Diagram: Process: Initially I found out the position vector of a fixed point $\displaystyle P(\rho_0, \phi_0 , z_0)$ : $\displaystyle \vec{r_0} = \rho_0 \hat{\rho_0} + z_0 \hat{z}$ And a general point on loop $\displaystyle O(\rho, \phi , 0)$ : $\displaystyle \vec{r} = \rho \hat{\rho} \implies \displaystyle \vec{r^{'}} = \rho_0 \hat{\rho_0} + z_0 \hat{z} - \rho \hat{\rho} $ As the angle between $\displaystyle \hat{\rho} \ \text{and} \ \hat{\rho_0}  $ is $ (\phi - \phi_0) $ we can derive: $\displaystyle  \hat{\rho} = \cos (\phi - \phi_0 ) \hat{\rho_0} + \sin (\phi - \phi_0 ) \hat{\phi_0} $ . Now, the general line element in cylindrical co-ordinates is : $\displaystyle \vec{\mathrm{d}l} = \mathrm{d} \rho \ \hat{\rho} + \rho \mathrm{d}\phi \ \hat{\phi} +  \mathrm{d} z \ \hat{z} \ $ but here, $$ \displaystyle \ \text{line element:} \ \vec{\mathrm{d}l} = \rho \mathrm{d}\phi \ \hat{\phi} $$ Biot-Savart Law: $ \displaystyle \vec{B} = \frac{\mu_0}{4\pi} \int_{c} \frac{i \vec{\mathrm{d}l} \times \vec{r^{'}}}{||\vec{r^{'}}||^3}$ Hence, first we need to find the cross product $\displaystyle \vec{\mathrm{d}l} \times \vec{r^{'}} =   \rho \mathrm{d}\phi \ \hat{\phi}  \times \vec{r^{'}} $ $$\displaystyle  \hat{\phi} \times \vec{r^{'}} = z_0 \cos  (\phi - \phi_0) \hat{\rho_0} + z_0  \sin (\phi - \phi_0 ) \hat{\phi_0} - (\rho + \rho_0 \cos  (\phi - \phi_0)) \hat{z}$$ And $ \displaystyle ||\vec{r^{'}}|| = \left[ z^{2}_{0} + \rho_0^{2} + \rho^{2} - 2\rho_0 \rho  \cos  (\phi - \phi_0) \right]^{1/2} $ Thus our Integral becomes: $$ \displaystyle  \vec{B} = \frac{\mu_0 i \rho }{4\pi} \int_{0}^{2\pi} \frac{z_0 \cos  (\phi - \phi_0) \hat{\rho_0} + z_0  \sin (\phi - \phi_0 ) \hat{\phi_0} - (\rho + \rho_0 \cos  (\phi - \phi_0)) \hat{z}}{\left[ z^{2}_{0} + \rho_0^{2} + \rho^{2} - 2\rho_0 \rho  \cos  (\phi - \phi_0) \right]^{3/2} } \mathrm{d} \phi  $$ The coefficient of $\displaystyle  \hat{\phi_0} $ is integrable and the definite integral here, results to be zero. This have to be true according to laws of electromagnetism (i.e no magnetic field parallel to the current element). Hence our Magnetic field becomes: $$ \displaystyle  \vec{B} = \frac{\mu_0 i \rho z_0  }{4\pi} \int_{0}^{2\pi} \frac{\cos  (\phi - \phi_0)}{\left[ z^{2}_{0} + \rho_0^{2} + \rho^{2} - 2\rho_0 \rho  \cos  (\phi - \phi_0) \right]^{3/2} } \mathrm{d} \phi \ \hat{\rho_0}  \\
-  \frac{\mu_0 i \rho }{4\pi} \int_{0}^{2\pi} \frac{\rho + \rho_0 \cos  (\phi - \phi_0)}{\left[ z^{2}_{0} + \rho_0^{2} + \rho^{2} - 2\rho_0 \rho  \cos  (\phi - \phi_0) \right]^{3/2} } \mathrm{d} \phi \  \hat{z} $$ EDIT: I have used the Ellipltical integrals (as suggested by @Maxim ) to simplify the expression as: $$
\displaystyle
\vec{B} = \frac{\mu_0 i \sqrt{a} }{2^{2.5} \pi \rho_0^{1.5} \sqrt{\rho} \sqrt{1-a}} (z_0-z) \left [ \frac{1}{1+a} \mathrm{E}\left(\pi, \frac{2a}{1-a} \right) -  \mathrm{F}\left( \pi,\frac{2a}{1-a} \right)\right ] \hat{\rho_0} -  \frac{\mu_0 i \sqrt{a}}{2^{2.5} \pi \rho_0^{1.5} \sqrt{\rho} \sqrt{1-a}} \left [ \left(\frac{\rho_0}{a+1} + \rho \right) \mathrm{E}\left(\pi, \frac{2a}{1-a} \right) -  \rho_0 \mathrm{F}\left( \pi,\frac{2a}{1-a} \right)\right ] \hat{z}\\
$$ Where $\displaystyle a = \frac{2\rho \rho_0}{(z_0-z)^2 + \rho^2 + \rho_0^2}$ and the position of the ring is $(\rho,\phi,z).$ $\mathrm{F}$ is elliptical integral of first kind and $\mathrm{E}$ is of second kind. Thus, my intial stage is well simplified. But the reason behind this complex calculation was that, I want to complute the Magnetic field due to a solenoid at any point in sapce. If we assume the number density of the coil to be $n$ and length $z_f - z_i,$ then in a small region of thickness $\mathrm{d}z$ , there will be $n\mathrm{d}z$ rings, each with nearly $\vec{B}$ as their magnetic field. Thus $$\displaystyle \vec{B}_{\text{solenoid}} = \int_{z_i}^{z_f} n\vec{B}\mathrm{d}z $$ Hence it is equivalent to compute : $$
\displaystyle 
\vec{B}_{\text{solenoid}} =  \frac{\mu_0 n i }{2^{2.5} \pi \rho_0^{1.5} \sqrt{\rho}} \int_{z_i}^{z_f}
\sqrt{\frac{a}{1-a}} 
\left(
 (z_0-z) \left [ \frac{1}{1+a} \mathrm{E}\left(\pi, \frac{2a}{1-a} \right) -  \mathrm{F}\left( \pi,\frac{2a}{1-a} \right)\right ] \hat{\rho_0} -   \left [ \left(\frac{\rho_0}{a+1} + \rho \right) \mathrm{E}\left(\pi, \frac{2a}{1-a} \right) -  \rho_0 \mathrm{F}\left( \pi,\frac{2a}{1-a} \right)\right ] \hat{z}
\right) \mathrm{d}z
$$ Thus I need help in solving, $\displaystyle \int_{z_i}^{z^f} \frac {(z_0-z) ( ( z-z_0)^2 +\rho^2 + \rho_0^2 ) }{\sqrt{( z-z_0)^2 + (\rho - \rho_0)^2} (( z-z_0)^2 + (\rho + \rho_0)^2)  } \mathrm{E} \left( \pi, \frac{4\rho \rho_0}{ (z-z_0)^2 + (\rho - \rho_0)^2 } \right) \mathrm{d}z $ $\displaystyle \int_{z_i}^{z^f} \frac {1 }{\sqrt{( z-z_0)^2 + (\rho - \rho_0)^2} } \mathrm{E} \left( \pi, \frac{4\rho \rho_0}{ (z-z_0)^2 + (\rho - \rho_0)^2 } \right) \mathrm{d}z $ $\displaystyle \int_{z_i}^{z^f} \frac {(z_0-z) }{\sqrt{( z-z_0)^2 + (\rho - \rho_0)^2} } \mathrm{F} \left( \pi, \frac{4\rho \rho_0}{ (z-z_0)^2 + (\rho - \rho_0)^2 } \right) \mathrm{d}z $ $\displaystyle \int_{z_i}^{z^f} \frac {1 }{\sqrt{( z-z_0)^2 + (\rho - \rho_0)^2} } \mathrm{F} \left( \pi, \frac{4\rho \rho_0}{ (z-z_0)^2 + (\rho - \rho_0)^2 } \right) \mathrm{d}z $ in order to solve for my expression Question: The above formula should give Magnetic field of solenoid at any point in space.
Do we have any closed form for the above integral ? Or it's taylor series that could simplify the expression. Progress: Edit $(30/03/2022) :$ I have got a lead, we can break $\mathrm{E}(\pi,k)$ into complete elliptical integral as $$\displaystyle \mathrm{E}(\pi,k) = \mathrm{E}(k) + \mathrm{E}\left(\sqrt{\frac{k^2}{1-k^2}}\right)$$ and using the expansion we will get: $$\displaystyle \mathrm{E}(\pi,k) = \pi - \frac{\pi}{2} \sum_{n=0}^{\infty} \frac{1}{16^n (2n-1)} {\binom{2n}{n}}^2 \left( k^{2n} + {\left[\frac{k^2}{1-k^2} \right]}^n \right)$$ This does simplify our integrand, but it is still vey complicated to sort out.","['integration', 'cylindrical-coordinates', 'definite-integrals', 'calculus', 'elliptic-integrals']"
4410793,Is plus sign the correct notation for Disjunction in boolean algebra?,"As far as I know, the disjunction is notated as $\vee$ in boolean algebra However, in some context, I saw that people usually tend to use the plus sign + to notate the disjunction like this: $$f(x, y) = x + y + xy $$ Instead of: $$f(x, y) = x \vee y \vee xy $$ Since the disjunction in boolean algebra is explicitly defined: $x \vee y = x + y - xy $ , in order to express this logical formula with the ordinary operations of arithmetic, I don't think the $\vee$ sign (logical operator) can be replaced by the plus sign (arithmetical operator) is mathematically correct.","['notation', 'boolean-algebra', 'definition', 'discrete-mathematics']"
4410824,Solving integral $\int \frac{\sqrt{x^2 + x}}{x}dx$ (problem 36 in section $6.25$ in Tom Apostol's calculus),"Integrals which involve $\sqrt{(cx + d)^2 - a^2}$ could often be simplified if we do a substitution $cx + d = a \sec t$ . If we take a concrete example, $\int \frac{\sqrt{x^2 + x}}{x}dx$ , then the substitution would be $$
x + \frac{1}{2} = \frac{1}{2}\sec t \\
dx = \frac{1}{2} \sec t \tan t dt \\
t = arcsec (2x+1)
$$ If I carry on that substitution, I get to $$
\frac{1}{2} \int \frac{\sqrt{\tan ^ 2t}}{\sec t - 1} \sec t \tan t dt
$$ As far as I understand, that is $$
\frac{1}{2} \int \frac{|\tan t|}{\sec t - 1} \sec t \tan t dt
$$ Now, there are two cases, $\tan t \ge 0$ and $\tan t < 0$ . $\tan t \ge 0$ case leads me to the solution also written in the book (and here ): $$
\frac{1}{2} \int \frac{1 + \cos t}{\cos ^ 2 t} dt = \\
\frac{1}{2} \tan t + \frac{1}{2} \log{\frac{1 + \tan \frac{t}{2}}{1 - \tan \frac{t}{2}}} + C = \\
\sqrt{x^2 + x} - \frac{1}{2}\log{|2\sqrt{x^2 + x} + 2x + 1|} + C
$$ But if I try the second case (i.e. $\tan t < 0$ ), I get to the negation of the previous case, so I wonder where did I go wrong? Should I maybe not even consider the negative case? Thanks!","['integration', 'calculus', 'trigonometry', 'trigonometric-integrals']"
4410883,"Solve : $y''+a^2y=\sin(bx) ,a,b\in \mathbb{R}$","Solve : $y''+a^2y=\sin(bx) ,a,b\in 
\mathbb{R}$ My solution : Suppose the solution form is: $$y(x)=x(A\cos(bx)+B\sin(bx))$$ $$\implies y''=-2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx))$$ Subtitue $y,\,y''$ in the original ode: $$-2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx))+a^2(x(A\cos(bx)+B\sin(bx))=\sin(bx)$$ Then, \begin{cases}
\begin{align}
-Ab^2+a^2A=0 \\
-Bb^2+a^2B=0 \\
\end{align}
\end{cases} \begin{cases}
\begin{align}
-2Bb=0 \\
-2Ab=1=0 \\
\end{align}
\end{cases} I get $A=B=0$ . Where am I getting wrong? I've tried to solve it by guessing an Ansatz .",['ordinary-differential-equations']
4410912,Fallacy in the proof of Lebesgue theorem in the script,"On the lectures today, another professor came and told us there is a minor omission/mistake in the proof of one direction of the Lebesgue theorem, namely $f:[a,b]\times[c,d]\to\Bbb R$ bounded and integrable implies $0$ measure set of discontinuities that we might discuss when our professor returns, so I've been thinking about it as an exercise. First, a definition and some results: $\underline{\boldsymbol{\text{definition}}}$ : Let $A\subset\Bbb R^2$ and $f:A\to\Bbb R$ any function. Oscillation $O(f,c)$ of the function $f$ at the point $c\in A$ is the infimum of the expression $\sup\{|f(x_1)-f(x_2)|:x_1,x_2\in U\cap A\}$ over all the open neighbourhoods $U\subset\Bbb R^2$ of the point $c$ . In other words $$O(f,c)=\inf_{U\ni c}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|.$$ $\underline{\boldsymbol{\text{ lemma }} 7.1}$ : A function $f:A\to\Bbb R$ is continuous at $c\in A$ if and only if $O(f,c)=0.$ $\underline{\boldsymbol{\text{result}}}$ If $D$ is the set of discontinuities of the function $f,$ then $D=\bigcup_{\varepsilon >0}D_\varepsilon,$ where $$D_\varepsilon=\{x\in A\mid O(f,c)\ge \varepsilon\}.$$ $\underline{\boldsymbol{\text{lemma } 7.5.}}$ Suppose $A$ is closed. For each $\varepsilon>0,\space D_\varepsilon$ is closed. Here is the proof as written in the script: Now, suppose that $f:[a,b]\times[c,d]\to\Bbb R$ is integrable and let $D$ be the set of all its discontinuities. Since $D=\bigcup_{n\in\Bbb N}D_{1/n}$ it is enough to show $D_{1/n}$ is of measure $0$ for all $n\in\Bbb N.$ For a given $\varepsilon>0,$ there is a partition $P$ of the rectangle $A=[a,b]\times[c,d]$ s. t. $U(P,f)- L(P,f)<\varepsilon.$ In particular, for the rectangles $A_{ij}$ that intersect $D_{1/n},$ it holds $\sum(M_{ij}-m_{ij})\nu(A_{ij})<\varepsilon,$ and, since $M_{ij}-m_{ij}\ge\frac1n$ for those rectangles $A_{ij},$ we see that the overall area of those rectangles is less than $n\varepsilon.$ Obviously, those rectangles cover $D_{1/n}.$ If $\varepsilon'>0$ is now arbitrary, we see that, with $\varepsilon=\frac{\varepsilon'}n,$ we've found finitely many rectangles which cover $D_{1/n}$ and whose overall area is less than $n\varepsilon=\varepsilon'$ . Hence, $D_{1/n}$ is of Lebesgue measure $0$ (in fact, of Jordan measure $0$ ). My attempt is that since $M_{ij}-m_{ij}\ge\frac1n$ for those rectangles $A_{ij},$ is somewhat wrong. $A\subset\Bbb R^2$ is a closed rectangle and the rectangles $A_{ij}$ in the partition of $A$ are closed. If some of them intersect $D_{1/n},$ the intersection might be contained in their boundaries. For example, if we take the function $f:[0,2]\times[0,2]\to\Bbb R,$ $$f(x,y)=\begin{cases}1, x<1,\\2, x\ge 1\end{cases}$$ and take any suitable partition so that the segment $[(1,0),(1,2)]$ contains boundaries of some rectangles, they might be a counterexample. I would like to ask if I'm on the right track and if we should isolate such cases?","['multivariable-calculus', 'real-analysis']"
4410917,Probability of teacher being in the last room: $\frac 45$ divided by $8$,"A student is looking for his teacher. There is a 4/5 chance that the teacher is in one of 8 rooms, and he has no specific room preferences. Student checked 7 of the rooms, but the teacher wasn't in any of them. What's the probability that he is in one of the 8 rooms? I tried dividing the P(4/5) by 8 and getting probability of teacher being in any one room of 0.1, and then subtracting 0.1*7 from 1 to get 0.3 - probability that he is in the last room. However that's not the right answer.","['discrete-mathematics', 'probability']"
4410950,Applications of Partition of Unity,"Theorem that I have to prove: Let $M$ be a smooth manifold, $f\colon M\to\mathbb{R}^n$ be a continuous map and let $S\subseteq M$ be a closed subset of $M$ such that the restriction $f_{|S}\colon S\to\mathbb R^n$ is smooth (this means that for all $p\in S$ there exists a neighbourhood $U_p$ of $p$ and a smooth map $g_p\colon U_p\to\mathbb{R}^n$ such that $g_{p|U_p\cap S}=f_{|U_p\cap S}$ ). Let $\epsilon\colon M\to (0,+\infty)$ be a continuous map. Then, there exists a smooth map $g\colon M\to\mathbb{R}^n$ such that $g(x)=f(x)\quad\forall x\in S$ $\lVert {g(x)-f(x)} \rVert<\epsilon(x)\quad\forall x\in S$ In order to prove it I have to prove the fact that: Fact 1: $\forall p\in M\quad\exists U_p$ neoighbourhood of $p$ , $g_p\colon U_p\to\mathbb R^n$ such that $g_p(x)=f(x)\qquad\forall x\in U_p\cap S\\
||{g_p(x)-f(x)}||<\epsilon(x)\qquad\forall x\in U_p$ . My task is to use the following fact $0$ that is a consequence of theorem of partition of unity. Fact 0: if $f\colon S\to\mathbb{R}^n$ is a smooth map, where $S\subseteq M$ is closed, then there exists a smooth map $F\colon M\to\mathbb{R}^n$ such that $F_{|S}=f$ .
Can anyone help me by proving FACT 1, please?","['smooth-functions', 'approximation', 'smooth-manifolds', 'differential-geometry']"
4410974,Geometrical meaning of the commutator of vectors on a manifold,"On a manifold, vectors do not describe finite displacements, unlike in euclidean geometry, but they do describe infinitesimal displacements, so we can take two vectors, $v^a$ and $w^a$ to span an infinitesimal parallelogram. I would like to understand how $ v^a w^b−w^a v^b$ is what describes the ""infinitesimal parallelogram"" spanned by $v$ and $w$ . For example, a vector is (parallel) transported from a point $P$ to a point $P'$ , along two different paths: the first $ P \rightarrow P_{1} \rightarrow P'$ consists of two infinitesimal shifts,
the second $ P \rightarrow P_{2} \rightarrow P'$ consists of the same shifts but in reverse order. How are the movements represented ?","['general-relativity', 'vectors', 'differential-geometry']"
4411046,When is the $\gcd$ not prime?,"This is an extension of a previous question ( Maximizing $\frac{\gcd(m,n)}{k}.$ ) which is interesting. Let $m=(p-1)^{p-1}+(p-1)!$ and let $n=((p-1)^{p-1}-(p-1)!)^{p-1}-1.$ For which odd primes is $\gcd(m,n)\neq p?$ For simplicity, let $d=\gcd(m,n).$ Then, we very easily see that $p\mid d.$ However, through some experimentation on Wolfram Alpha, I found that for most primes, $d=p,$ but not for all. The first few primes that satisfy the requirements are $S=\{5,31,41,43,53,71,97,\ldots\}$ (I hope I didn't miss one). This is not a known sequence according to OEIS. I am curious how exactly to find these primes. Two other things to note. First, we see that if $p\in S$ then $d=p(2p-3)$ and $2p-3$ is prime. However, not all primes $p$ such that $2p-3$ is prime are in $S$ , for example $p=13$ and $2p-3=23.$ Moreover, I also found that if $p\in S$ then it is not necessarily true that $2p-3\in S.$ Indeed, $2p-3\not \in S$ for the first few $p\in S,$ but the first example is $97$ and $191,$ which are both in $S.$ Edit: As noted by user dezdichado in the comments, we see that $S\subset \{p \in\mathbb{P}\mid 2p-3 \in \mathbb{P}\},$ where $\mathbb{P}$ is the set of prime numbers. Obviously, this is a weaker condition, but it would be interesting to see the proof of this.","['number-theory', 'gcd-and-lcm', 'elementary-number-theory', 'prime-numbers']"
4411049,Bertsekas' Penalty Approach's Proof of Necessary Conditions of the Equality Constrant Problem using Lagrange Multipliers,"I am having some trouble understanding the proof of the Necessary Conditions of the ECP using Lagrange Multipliers in Bertsekas' book Nonlinear programming (1999). The part relevant to my inquiry is the following partial statement Proposition 3.1.1: (Lagrange Multiplier Theorem - Necessary Conditions) Let $x^*$ be a local minimum of $f$ subject to $h(x) = 0$ , and assume  that the constraint gradients $\nabla h_1(x^*), \dots, \nabla h_m(x^*)$ are linearly independent. Then there exists a unique vector $\lambda^*=(\lambda_1^*, \dots, \lambda_m^*)$ called Lagrange multiplier vector , such that $$\nabla f(x^*)+\sum_{i=1}^{m}\lambda_i^*\nabla h_i(x^*)=0$$ [...] Regarding notation, we have the declaration at the start of the section NECESSARY CONDITIONS FOR EQUALITY CONSTRAINTS . We assume $f:\mathbb{R}^n →\mathbb{R}^n$ , $h_i:\mathbb{R}^n→\mathbb{R}$ for $i\in{1,\dots,m}$ are continuously differentiable functions. [...] For notational convenience, we introduce the constraint function $h:\mathbb{R}^n→\mathbb{R}^m$ , where $$h=(h_1,\dots,h_m)$$ For the record, $x^*$ is a local minimum of $f$ if it is the minimizer within a neighborhood of $x^*$ . A feasible point is a point that satisfies the constraints given by the problem. Now, I am going to write down a transcription of the proof presented in the book using the penalty approach. Penalty Approach Here we approximate the original constrainted problem by an unconstrained optimization problem that involves a penalty for violation of the constraints. In particular, for $k\in\mathbb{N}$ , we introduce the cost function $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2$$ where $x^*$ is the local minimum of the constrained problem and $\alpha$ is some positive scalar. [...] Since $x^*$ is a local minimum, we can select $\epsilon>0$ such that $f(x^*)\leq f(x)$ for all feasible $x$ in the closed sphere $$S=\{x : ‖x-x^*‖ \leq \epsilon\}$$ Let $x^k$ be an optimal solution of the problem of minimizing $F^k$ subject to $x\in S$ [An optimal solution exists because of Weierstrass' theorem [...]]. We will show that the sequence $\{x^k\}$ converges to $x^*$ . We have for all $k$ $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1}$$ and since $f(x^k)$ is bounded over $S$ , we obtain $$\lim_{k→\infty}‖h(x^k)‖ = 0$$ otherwise the left hand side of Eq. (1) would become unbounded above as $k→\infty$ . Therefore, every limit point $\bar{x}$ of $\{x^k\}$ satisfies $h(\bar{x})=0$ . Furthermore, Eq. (1) yields $f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*)$ for all $k$ , so by taking the limit as $k→\infty$ , we obtain $$f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*)$$ [...] My question is very simple regarding to 2 usages of the concept of limit: Regarding to this statement We have for all $k$ $$F^k(x)=f(x)+\frac{k}{2}‖h(x)‖^2+\frac{\alpha}{2}‖x-x^*‖^2\leq F^k(x^*)=f(x^*) \tag{1}$$ and since $f(x^k)$ is bounded over $S$ , we obtain $$\lim_{k→\infty}‖h(x^k)‖ = 0$$ I don't see why can we take the limit to be equal to $0$ . As far as I am concerned, we don't know if the sequence $\{x^k\}$ converges at all. Regarding this statement Furthermore, Eq. (1) yields $f(x^k)+\frac{\alpha}{2}‖x^k-x^*‖^2\leq f(x^*)$ for all $k$ , so by taking the limit as $k→\infty$ , we obtain $$f(\bar{x})+\frac{\alpha}{2}‖\bar{x}-x^*‖^2\leq f(x^*)$$ By the same reason as before, I can't quite justify that $\{x^k\}$ converges. I have tried modifying the proof by taking a subsequence that converges, as I do see that by being a bounded sequence, $\{x^k\}$ must have a limit point. And by using this subsequence, I see no problem at all when asserting the previous statements. However, I am probably missing something in the original proof, so I would appreciate if someone could shed some light on this. Thank you in advance.","['limits', 'proof-explanation', 'optimization', 'lagrange-multiplier']"
4411051,Why is one root extraneous when using complex number polynomial expansions of cos(4x) to find cos(π/8)?,"Using De Moivre's theorem, binomial expansion, and the Pythagorean identity, I have the following polynomial for $\cos 4\theta$ : $$\cos\left(4\theta\right) = 8\cos^4\theta -8\cos^2\theta +1 $$ I trying to find an exact value for $\cos\left(\frac{\pi}{8}\right)$ . Since $\cos\left(\frac{\pi}{2}\right)=0$ , then $\cos\left(4\cdot\frac{\pi}{8}\right)=0$ , so it must be true that $$ 8\cos^4\left(\frac{\pi}{8}\right) -8\cos^2\left(\frac{\pi}{8}\right) +1 =0$$ and by the quadratic formula I have $$ \cos \left(\frac{\pi}{8}\right) = \pm\frac{\sqrt{2\pm\sqrt{2}}}{2} $$ Since $\frac{\pi}{8}$ is in the first quadrant, we discard the negative root, so we have $$  \cos \left(\frac{\pi}{8}\right) = \frac{\sqrt{2\pm\sqrt{2}}}{2} $$ Now, $  \cos \left(\frac{\pi}{8}\right) = \frac{\sqrt{2+\sqrt{2}}}{2} $ is true, but $  \cos \left(\frac{\pi}{8}\right) = \frac{\sqrt{2-\sqrt{2}}}{2} $ is false. (I know this from the approximate numerical value of $\cos\left(\frac{\pi}{8}\right)$ .) My question: How do we know $\frac{\sqrt{2-\sqrt{2}}}{2}$ is an extraneous root?","['trigonometry', 'complex-numbers']"
4411074,Probability of drawing exactly k colors from a sample.,"I'm trying to solve exercise 3.3 from Jaynes' book on probability theory but my algorithm isn't suited for calculations and cannot provide results for big numbers like these. Here is the exercise: Suppose we know that the urn contains exactly 50 balls but we don't know how many different colors there are. Drawing out 20 of them, we find three
different colors; now what do we know about $k$ ? We know from deductive reasoning
(i.e. with certainty) that $3 ≤ k ≤ 33$ ; but can you set narrower limits $k_1 ≤ k ≤ k_2$ within which it is highly likely to be? Hint: This question goes beyond the sampling theory of this chapter because, like
most real scientific problems, the answer depends to some degree on our common
sense judgments; nevertheless, our rules of probability theory are quite capable of
dealing with it, and persons with reasonable common sense cannot differ appreciably
in their conclusions. To be honest with you I have no idea how the hint is supposed to help here so I ignored it. Here is my attempted solution: $C$ : The urn contains exactly $50$ balls $B$ : We draw $20$ balls out of the urn and we find exactly three different colors $A$ : There are balls of $k$ different colors With these statements I now want to find: \begin{equation}
P(A|BC)=\frac{P(A|C)\cdot P(B|AC)}{P(B|C)}
\end{equation} First $P(A|C)$ , there are $\Omega_k$ ways in which fifty $k$ different colored balls can be in an urn: \begin{equation}
\Omega_i=\frac{(50-1)!}{(50-k)!(k-1)!}
\end{equation} If we assume equal a priori probability for each combination then: \begin{equation}
P(A|C)=\frac{\Omega_k}{\sum_{i=1}^{50} \Omega_i}
\end{equation} Next, I calculate $P(B|AC)$ . The statement $AC$ is equivalent to $C(N_1+...N_k)$ where $N_i$ is a specific distribution of $k-$ colored balls so they add to $50$ in total. I use the following expression to calculate $P(B|AC)$ : \begin{equation}
p\left(B \mid N_{1} C+N_{2} C+\cdots+N_{n} X\right)=\frac{\sum_{i} p\left(N_{i} \mid C\right) p\left(B \mid N_{i} C\right)}{\sum_{i} p\left(N_{i} \mid C\right)}
\end{equation} Due to equal a prior probability: \begin{equation}
P(N_i)=\frac{1}{\Omega_k}
\end{equation} And $P(B|N_iC)$ is calculated with the help of the generalized hypergeometric distribution where we take into account all possibles combinations that we draw exactly three colors. The distribution I mentioned: \begin{equation}
h\left(r_{a}, r_{3}, \ldots, r_{k} \mid N_{a}, N_{3}, \ldots, N_{k}\right)=\frac{\left(\begin{array}{c}
N_{a} \\
r_{a}
\end{array}\right)\left(\begin{array}{c}
N_{3} \\
r_{3}
\end{array}\right) \cdots\left(\begin{array}{c}
N_{k} \\
r_{k}
\end{array}\right)}{\left(\begin{array}{c}
\sum N_{i} \\
\sum r_{i}
\end{array}\right)}
\end{equation} Finally, for $P(B|C)$ we can take the statement $C$ and divide it into $C_1+...+C_{50}$ where $C_i$ The statement that there are $i$ colors in the urn. Using the same rule used for $P(B|AC)$ we can calculate this too. For smaller numbers, this method works and the python code I wrote can handle it. For bigger numbers (like the ones in the original problem) my PC isn't able to handle it. I ""solved"" this problem like a robot would therefore the calculations for the statements $P(B|AC)$ are extremely time-consuming. Therefore I ask if there is a way to get a reasonable probability for statements like $P(B|AC)$ that don't involve taking all different possibilities into account. Thank you for taking the time to read this and sorry for the lengthy question.","['probability-theory', 'probability']"
4411100,$f$ is continuous on int $\operatorname{dom} f$ if and only if int epi $f \neq \varnothing$.,"Let $\mathcal{X}$ be a real Hausdorff locally convex topological vector space and let $f: \mathcal{X} \rightarrow \, {]}{-}\infty,+\infty]$ be convex. What we want to do is showing that $f$ is continuous on int $\operatorname{dom} f$ if and only if int epi $f \neq \varnothing$ , where $$\operatorname{dom} f := \{ x \in \mathcal{X} : f(x) < \infty\}, \qquad \operatorname{epi} f := \{ (x, \xi) \in \mathcal{X} \times \mathbb{R} : f(x) \leq \xi\}.$$ {The following is what I have tried}: For the necessity: If $\operatorname{int} (\operatorname{epi} f) = \varnothing$ , then \begin{equation*}
    \overline{\mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{epi} f} = \mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{int} \operatorname{epi} f = \mathcal{X} \times \mathbb{R}.
\end{equation*} Take any net $\{ (x_a, \xi_a) \}_{a \in A} \in \operatorname{epi} f \subset \mathcal{X} \times \mathbb{R}$ such that $ (x_a, \xi_a)  \rightarrow (x, \xi)$ , then the closeness of $\mathcal{X} \times \mathbb{R} = \overline{\mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{epi} f} $ give that \begin{equation*}
    (x, \xi) \in \overline{\mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{epi} f}
\end{equation*} The continuity of $f$ will give that $(x, \xi) \notin \mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{epi} f$ , then $(x, \xi)$ must lie in the boundary of $\mathcal{X} \times \mathbb{R} \, \setminus \, \operatorname{epi} f$ . But I am stuck here. And I also do not know how to deal with the sufficiency. Indeed, I do not know how to use the locally convex condition and $f$ is convex (I only know that $f$ is convex will give $\operatorname{epi} (f)$ is convex). Could anyone help me? Thanks in advance!","['general-topology', 'convex-analysis', 'functional-analysis']"
4411107,Natural manifold topology vs. product topology,"Let $M = \mathbb{R} \times \mathbb{R}$ , where the first factor is endowed with the discrete topology and the second with the usual Euclidean topology and so $M$ has the resulting product topology. Show that $M$ admits a differentiable structure such that the natural manifold topology on $M$ is the product topology, hence in particular not second countable. While trying to solve this I encountered some contradiction. I am quite sure my reasoning fails somewhere, but I am not too sure where. Here's my approach: $${}$$ ( I ) $\enspace$ The product topology is the topology such that the canonical projections are all continuous. Equivalently, the preimages of open sets under the canonical projections form a subbasis. In the above case of $\mathbb{R} \times \mathbb{R}$ this means that the open sets are of the form $$ \{ x \} \times (a,b) \quad , \qquad x \in \mathbb{R},  \; a < b \quad . \tag{$\ast$}$$ and unions thereof. (Correct?) ( II ) $\enspace$ The natural manifold topology is the topology such that for a maximal atlas $A_{max} = \{ \, (\phi_i, U_i ) \; | \; i \in I \, \}$ the set $\{ U_i \}_{i \in I}$ forms a basis. ( III ) $\enspace$ I now define an atlas as $A = \{ \, (id_{\mathbb{R}^2}, V ) \; | \; V \in \tau_{\text{euclidean}} \, \}$ , which obviously consists of all identity maps on open sets with respect to Euclidean topology. I now want to show that all $V \in \tau_{\text{euclidean}}$ are also elements of $\tau_{\text{product}}$ . ( IV ) $\enspace$ In order to show said property, I just have to express an open ball (because every open set in Euclidean topology is the union of such balls) in terms of these ""sliced intervals"" in $(\ast)$ . So for an open Ball with radius $r > 0$ and center $(m_1,m_2) \in \mathbb{R}^2$ : $$ B_r(m) \enspace = \enspace \bigcup_{x \in (m_1-r,m_1+r)} \{x\} \times (m_2 - \varepsilon_x, m_2 + \varepsilon_x) \quad , \qquad \varepsilon_x = r \cdot \cos \big[ \tfrac{\pi}{2r}(x-m_1) \big] $$ Which shows that the open sets $V \in \tau_{\text{euclidean}}$ are also $V \in \tau_{\text{product}}$ and by the convenient choice of coordinate charts being the identity, these charts $id_{\mathbb{R}^2} : V \longrightarrow V$ are homeomorphisms with respect to $\tau_{\text{product}}$ . This, however, would mean that $\tau_{\text{euclidean}} = \tau_{\text{product}}$ , contradicting that $M$ is not second-countable. $${}$$ Where am I wrong? Which concept do I misunderstand?","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
4411234,Proving a Rational Expression is Surjective,"I need to prove that the function $f:(-1,1)\rightarrow\mathbb{R}$ defined by $f(x) = \frac{x}{x^2 - 1}$ is surjective. My work. $b = \frac{a}{a^2 - 1} \iff b(a^2 -1)=a \iff ba^2 - b - a=0$ From here I did a few cases: Case 1) $b=0$ . Then $a=0$ . Using the quadratic formula: $a = \frac{1\pm \sqrt{1+4b^2}}{2b}$ . Case 2) $b \gt0$ . Then $a = \frac{1+ \sqrt{1+4b^2}}{2b}$ $\notin (-1,1)$ $\forall$ b $\in R$ . Case 3) $b \gt0$ . Then $a = \frac{1- \sqrt{1+4b^2}}{2b}$ $\in (-1,1)$ $\forall$ b $\in R$ . Using Case 1 and Case 3, f is subjective. Is this correct? I cannot use Calculus.","['functions', 'discrete-mathematics']"
4411277,"""Consistency"" vs. ""Convergence"" of Estimators : Are ALL ""MLE's"" ALWAYS Consistent?","I have heard the terms ""Consistency"" vs. ""Convergence"" being used interchangeably - for example: In Machine Learning applications, I have heard the term ""Convergence"" describe a situation where successive differences in iterations becomes smaller than some threshold. For example, when we say that a ""Machine Learning Model converged"" - this means that the value of the Loss Function at ""iteration n"" versus the value at ""iteration n+1"" is almost identical. In Statistics and Probability, I have heard a similar concept being used to describe the properties of estimators. For example, we can say that that the Maximum Likelihood Estimator of the ""sample average"" is ""Consistent"" with the ""population average"" - this means that as the number of observations in our sample get larger and larger, the value of the ""sample average"" will become closer and closer to the ""population average"". (I suppose we could say that as the number of samples become larger and larger, the value of the ""sample average converges to the population average""). Regarding this, I had the following question: When we consider the Maximum Likelihood Estimator for any Probability Distribution Function (e.g. ""Mu-hat-MLE"" from a Normal Distribution, ""Lambda-hat-MLE"" from an Exponential Distribution, ""p-hat-MLE"" from a Binomial Distribution, etc.) - do we know if ALL Maximum Likelihood Estimators (in theory) are ALWAYS ""Consistent""? For example, suppose we have some never-before-seen Probability Distribution Function, yet we somehow manage to maximize the corresponding likelihood function and obtain a maximum likelihood estimate for its parameters (e.g. ""mu"", ""lambda"",etc.) - by virtue of the fact that we have the MLE, will this MLE automatically be Consistent? Or do we still have to prove that this MLE will be Consistent? Thank you!","['statistics', 'estimation', 'maximum-likelihood', 'convergence-divergence', 'probability']"
4411328,Flajolet & Sedgewick: How to compute the variance of the number of cycles in a random permutation?,"I am reading the book Analytic Combinatorics 4ed by Sedgewick and Flajolet. On page 160 at Example III.4 the authors derive the variance of the number of cycles in a random permutation. I can follow the authors up to the part where the write $$\sigma_n^2 = \biggl(\sum_{k=1}^n \frac{1}{k} \biggr) - \biggl(\sum_{k=1}^n \frac{1}{k^2} \biggr). $$ I do not understand how they arrive at that. If I understand the above part correctly the authors state that $\mathbb{E}_n[\chi] = H_n$ , where $H_n$ is the $n$ -th Harmonic Number, and $\mathbb{E}_n[\chi(\chi-1)] = \sum_{k=1}^n \frac{1}{k^2}$ . I think that it would now suffice to compute $$\mathbb{E}_n[\chi^2] - \mathbb{E}_n[\chi]^2$$ by somehow utilising the above. However, I do not understand how to do that. Could you please help me? Could you please explain this to me?","['permutations', 'variance', 'analytic-combinatorics', 'combinatorics', 'probability']"
4411338,"On the log-cosine integral $\int\limits_0^{\pi/2}\ln(\cos(x))\,dx$","I know that $$\mathcal I = \int\limits_0^{\pi/2}\ln(\cos(x))\,dx = -\frac\pi2 \ln(2)$$ and I am aware of a few different clever ways to demonstrate this result (e.g. MSE 4065767 and MSE 1992462 ). I would like to know if there's any way to wrap up the method I outline below. Substituting $t=\tan\left(\frac x2\right)$ yields $$\mathcal I= 2\int_0^1 \frac{\ln\left(\frac{1-t^2}{1+t^2}\right)}{1+t^2} \, dt = 2 \left(\underbrace{\int_0^1\frac{\ln(1-t^2)}{1+t^2}\,dt}_{\mathcal J^-} - \underbrace{\int_0^1\frac{\ln(1+t^2)}{1+t^2} \, dt}_{\mathcal J^+} \right)$$ Exploiting the series expansion for $\frac1{1+t^2}$ , we have $$\mathcal J^- = \int_0^1 \ln(1-t^2) \sum_{n=0}^\infty (-t^2)^n = \sum_{n=0}^\infty (-1)^n \underbrace{\int_0^1 t^{2n} \ln(1-t^2) \, dt}_{{J_n}^-}$$ and similarly, $$\mathcal J^+ = \sum_{n=0}^\infty \underbrace{\int_0^1 t^{2n} \ln(1+t^2) \, dt}_{{J_n}^+}$$ I derive the following recurrences for ${J_n}^{\pm}$ : $$\begin{cases}{J_0}^- = 2\ln(2) - 2 \\ {J_n}^- = \frac{2n-1}{(2n+1)^2} - \frac1{2n+1} + \frac{2n-1}{2n+1} {J_{n-1}}^- & \text{for }n\ge1\end{cases}$$ $$\begin{cases}{J_0}^+ = \ln(2) - 2 + \frac\pi2 \\ {J_n}^+ = \frac{2n-1}{(2n+1)^2} - \frac{2\ln(2)-1}{2n+1} - \frac{2n-1}{2n+1}{J_{n-1}}^+ & \text{for }n\ge1\end{cases}$$ where ${J_0}^\pm$ are found by exploiting the series for $\ln(1\pm t)$ and several others derived from the expansion of $\frac1{1-t}$ . I managed to solve these for ${J_n}^{\pm}$ , so that $$\mathcal J^- = \frac{(\ln(2)-1)\pi}2 + \sum_{n=0}^\infty \frac{(-1)^n}{2n+1} \left(\sum_{i=0}^{n-1} \frac{2i+1}{2i+3} - n\right)$$ $$\mathcal J^+ = \frac{\pi^2-3\pi}8 + \sum_{n=0}^\infty \frac{(-1)^n}{2n+1} \sum_{i=0}^{n-1} (-1)^i \frac{2i+1}{2i+3}$$ Barring any mistakes, the remaining sum seems to be $$2 \sum_{n=0}^\infty \frac{(-1)^n}{2n+1} \left(\sum_{i=0}^{n-1} (1-(-1)^i) \frac{2i+1}{2i+3} - n\right) = \frac{3\pi}2\ln(2) - \frac{\pi^2}4 - \frac\pi4$$ but I have no idea where to go with this. Is there anything I can do to further refine $\mathcal J^- - \mathcal J^+$ ? I do see a bit of cancellation of terms when $i$ is even, which lets me simplify the inner sum to $$2\sum_{i=0}^{\left\lfloor\frac{n-1}2\right\rfloor} \frac{4i+1}{4i+3} - n$$ but that doesn't seem helpful. We do have, with $H_n$ the $n$ -th harmonic number, $$\sum_{i=0}^{n-1} \frac{2i+1}{2i+3} = \sum_{i=0}^{n-1}\left(1 - \frac2{2i+3}\right) = n - 2\left(H_{2n+1} - \frac12 H_n - 1\right)$$ though I'm not so sure I can write its alternating counterpart in $\mathcal J^+$ in a similar way. Then $$\mathcal J^- = \frac12\ln(2) - \pi - 2 \sum_{n=0}^\infty \frac{(-1)^n H_{2n+1}}{2n+1} + \sum_{n=0}^\infty \frac{(-1)^n H_n}{2n+1} = \frac{2\ln(2)-\pi}4 - G$$","['summation', 'definite-integrals', 'sequences-and-series']"
4411341,weak convergence of joint distribution and conditional distribution,"It is known that the weak convergence of joint distribution does not imply the weak convergence of conditional distribution (for example, see this post). What happens if we assume that the density functions always exist? I have a simple proof but I am not sure if it is correct. Suppose $(X_n, Y_n) \stackrel{d}{\to} (X, Y) \in \mathbb{R} \times \mathbb{R}$ . Assuming that CDFs are continuous and density functions $p_{X_n, Y_n}$ exist, we can write the conditional density function as: \begin{align}
p_{Y_n|X_n}(y|x) = \frac{p_{X_n, Y_n}(x, y)}{\int_{\mathbb{R}}p_{X_n, Y_n}(x, y') dy'}
\label{eq} \tag{1}
\end{align} Weak convergence is equivalent to the convergence of CDFs in their continuity points and the density functions (?). Now, both the denominator and numerator of eq. (\ref{eq}) converge, therefore the conditional density converges. I am not sure if the convergence of the CDFs results in the convergence of density functions. If that is not true in general, are there any set of general assumptions that can guarantee that? Thank you!","['conditional-probability', 'measure-theory', 'weak-convergence']"
4411366,Evaluating $\int_{-\infty}^\infty \frac{\ln{(x^4+x^2+1)}}{x^4+1}dx$,"I recently attempted to evaluate the following integral $$\int_{-\infty}^\infty\frac{\ln{(x^4+x^2+1)}}{x^4+1}dx$$ I started by inserting a parameter, $t$ $$F(t)=\int_{-\infty}^\infty\frac{\ln{(tx^4+x^2+t)}}{x^4+1}dx$$ Where F(0) is the following $$F(0)=\int_{-\infty}^\infty\frac{\ln{(x^2)}}{x^4+1}dx=2\int_0^\infty\frac{\ln{(x^2)}}{x^4+1}dx=4\int_0^\infty\frac{\ln x}{x^4+1}dx$$ We can evaluate this using a common integral from complex analysis and taking the derivative using Leibniz’s rule. $$\int_0^\infty\frac{x^m}{x^n+1}dx=\frac{1}{m+1}\int_0^\infty\frac{(m+1)x^m}{(x^{m+1})^\frac{n}{m+1}+1}dx=\frac{1}{m+1}\int_0^\infty\frac{du}{x^\frac{n}{m+1}+1}$$ $$=\frac{1}{m+1}\frac{\pi}{\frac{n}{m+1}\sin{\frac{\pi}{\frac{n}{m+1}}}}=\frac{\pi}{n\sin{\frac{\pi(m+1)}{n}}}=\frac{\pi}{n}\csc{\frac{\pi(m+1)}{n}}$$ $$\int_0^\infty\frac{\ln{x}}{x^n+1}dx=\frac{d}{dm}\int_0^\infty\frac{x^m}{x^n+1}dx\Big|_{m=0}$$ $$=\frac{\pi}{n}\frac{d}{dm}\csc{\frac{\pi(m+1)}{n}}\Big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi(m+1)}{n}}\cot{\frac{\pi(m+1)}{n}}\big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi}{n}}\cot{\frac{\pi}{n}}$$ Therefore $$F(0)=4\int_0^\infty\frac{\ln{x}}{x^4+1}dx=-\frac{\pi^2\sqrt 2}{4}=-\frac{\pi^2}{2\sqrt 2}$$ Now that we found F(0), we can start applying Feynman’s trick. $$F’(t)=\int_{-\infty}^{\infty}\frac{dx}{tx^4+x^2+t}$$ Using a formula I derived we can continue $$\int_{-\infty}^\infty\frac{dx}{ax^4+bx^2+c}=\frac{\pi}{\sqrt{c}\sqrt{b+2\sqrt{ac}}}$$ $$F’(t)=\frac{\pi}{\sqrt{t}\sqrt{1+2t}}$$ Integrating both sides $$F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}+C$$ Set $t=0$ $$C=F(0)=-\frac{\pi^2}{2\sqrt2}$$ Therefore $$F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}-\frac{\pi^2}{2\sqrt2}$$ $$I=\pi\sqrt2\ln{(\sqrt2+\sqrt3)}-\frac{\pi^2}{2\sqrt2}$$ WolframAlpha confirms it numerically I am not satisfied with this solution.  I am curious as to what other solutions there might be. How else can we solve this integral?","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4411370,"How to prove that for some constant $c$, $\frac{\sum_{k=1}^{n} X_k}{\sum_{k=1}^{n} X_k^2} \rightarrow c$ $a.s.$?","On a certain fair die, four faces each have exactly one dot, one face has exactly two dots, and one face has exactly six dots. This die is rolled repeatedly. For each $n \geq 1$ , let $X_n$ denote the number of dots that come up on the nth roll of this die. Prove that for some constant $c$ , $\frac{\sum_{k=1}^{n} X_k}{\sum_{k=1}^{n} X_k^2} \rightarrow c$ $a.s.$ as $n \rightarrow \infty$ ? As per the given hint, I proved that $A_i$ , $i \in I$ are events where $I$ is a nonempty finite or countable index set. Suppose $P(A_i)=1$ for each $i \in I$ . Prove that $P(\cap_{i \in I} A_i)=1$ . I am not sure how this result will be helpful in solving the above question. I started the above question by using Chebyshev's inequality, but did not reach to any conclusion. Can anyone suggest how can I solve this question?","['probability-theory', 'probability']"
4411384,Light Bulb hypothesis testing,"One claims that the life time distribution of its Everyday light bulbs
is exponential with mean 1000 hours. If you test a random sample of 4 light
bulbs and find that the average life time is 900 hours, do you have significant
evidence against the this claim? Set up an appropriate hypothesis testing
problem. Use the nominal level of 0.05 for your test. (Hint: You may view
the exponential distribution as a gamma distribution. With data collected from a sample of 4 light
bulbs, what is the power of your test if the actual mean life time is only
900 hours? My attempt: $\mu = 1000, \sigma = 1000, \bar X_4 = 900$ $H_0: \mu \geq 1000$ $H_a: \mu \lt 1000$ Since $\bar X_4 = 900, \sum_{i=1}^{4} X_i = 3600$ I then conduct the integration $$\int_{3600}^{\infty} \frac{(\frac{1}{1000})^4}{\gamma(4-1)}x^{4-1}e^{-0.001x} dx \approx 1.546$$ compare this result with $\alpha = 0.05$ , which is 2.353 after checking the t-test table. So I should not reject $H_0$ Is everything alright? I have no idea about the second part. Any hint or suggestion would be appreciated!","['statistical-inference', 'statistics', 'solution-verification', 'exponential-distribution', 'hypothesis-testing']"
4411433,Degree of face of icosahedron is $3$.,Given a polyhedral graph with $12$ vertices and $30$ edges I know that it is The Icosahedron. from Euler's formula I derived that number of faces is $20$ . I need to show that each face has a degree of $3$ . I that if each face has the same degree then we can conclude that the degree is $3$ . But I am not able to show that each face has the same degree.,"['graph-theory', 'combinatorics', 'discrete-mathematics']"
4411436,Probability that the histogram for the sum of two dice has an expected shape,"Suppose that two 6-sided dice are thrown $n$ times and that the sum after each throw is plotted on a histogram. Let $s_i$ be the frequency of the sum $i \in \{2, 3, \dots, 12 \}$ . As $n \rightarrow \infty$ , the probability that $s_i \le s_{i+1}$ for $i \in \{2, 3, \dots, 6\}$ and $s_{i} \ge s_{i+1}$ for $i \in \{7, 8, \dots, 11\}$ approaches $1$ (because the further the sum is from $7$ , the less likely it is to occur). But after only a finite number of throws, what is the probability that the above expression is true? Can this be expressed as a closed-form function of $n$ ?","['dice', 'probability']"
4411438,How is manipulation of derivatives possible?,"In parametric differentiation, finding the second derivative can be given by: $\frac{d^2y}{dx^2} = \frac{d}{dx}(\frac{dy}{dx}) = \frac{d}{dx}(\frac{dy}{dt}*\frac{dt}{dx})$ Now if I am differentiating two functions multiplied together, I would use the product rule right? $\frac{d}{dx}(\frac{dy}{dt}*\frac{dt}{dx})$ = $\frac{dt}{dx}*\frac{d}{dx}(\frac{dy}{dt})$ + $\frac{dy}{dt}* \frac{d}{dx}(\frac{dt}{dx})$ But that is a wrong manipulation, rather this is what is given in my textbook: Manipulating derivatives is hard, at some points, they act like fractions, while at others they do not. How do I know for sure that what I am doing is correct?","['calculus', 'derivatives']"
4411439,measure theory notation [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I've seen many equations for expectation using probability measures, some of that are using $\begin{align}\int f(x) dP(x).\end{align}$ And some of others are using $\begin{align} \int f(x) P(dx).\end{align}$ I think they are equivalent, right? Or what is the difference?","['measure-theory', 'probability-theory']"
4411515,Can someone explain what this theorem means?,"here is the theorem ""If a polynomial with real coefficients in one unknown has all of
its roots being real number, then the number of positive roots, counted with
their multiplicity, equals the number of variations of signs among the ordered
sequence of his coefficients"" Unforunately i am not understand what the theorems trying to say , can someone explain what this therem trying to say and give some examples?","['proof-explanation', 'functions', 'polynomials', 'education', 'algebra-precalculus']"
4411527,What is the MLE for centered Gaussian:$ S_y=\frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}$ or $S_y=\frac{1}{n}\boldsymbol{XX}^T$,"What is the Maximum Likelihood Estimator  (MLE) for the centered ( $\mu=0$ ) multivariate Gaussian? My book gives the MLE as $$
S_y=\frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}
$$ while I've seen online resources derive the MLE as $$
S_y=\frac{1}{n}\boldsymbol{XX}^T
$$ The context is to use PCA: $$\boldsymbol{X}=\boldsymbol{U\Sigma V}^T$$ and if the first definition is used, I would get the eigenvalues/eigenvectors of $\boldsymbol{U}$ while $\boldsymbol{V}$ if the second definition is used.","['statistics', 'probability-distributions', 'probability', 'maximum-likelihood']"
4411572,"Weak convergence in $L^{∞}[0,1]$ implies strong convergence in $L^{p}[0,1]$","I'm trying to solve an exercise in Rudin's Functional Analysis. It is the problem 19 in Chapter 11. The problem is: If $f_n\in L^{∞}[0,1]$ and converges to $0$ in the weak topology of $L^{∞}[0,1]$ , then $\int_0^1|f_n|^p\to 0$ for any $p\in (0,∞)$ . I have tried to use Gelfand Transform that Rudin mentioned in 11.13(f), which maps $L^{∞}[0,1]$ isometrically to $C(\Delta)$ as Banach Spaces, where $\Delta$ is the maximal ideal space. This approach seems to be hopeful because continuous functions are good, but I failed despite a few hours effort. Another trial is to solve this just for p=2, but I did not proceed any more, too. I think the conclusion of this problem is very powerful so I want to know how to solve it very much. So please tell me anything if you have an idea or some relative materials, thank you very much!","['banach-spaces', 'banach-algebras', 'analysis', 'real-analysis', 'functional-analysis']"
4411582,'Tensor Calculus' by J.L. Synge and A. Schild (1979 Dover publication) . Exercise 12 page 110.,"I'm solving all the exercises  of 'Tensor Calculus' by J.L. Synge and A. Schild (1979 Dover publication) . Till now everything went smooth, but now I'm stuck at exercise 12. page 110 of the third chapter on 'Curvature'.
The exercise is stated as Prove that the quantities $$G^{mn}+ \frac{1}{2a}\frac{\delta^2}{\delta x^x \delta x^s}\left[ a \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\right]$$ can be expressed in terms of the metric tensor and its first derivatives. First, there must  be a typo error as $\delta x^x $ in $\frac{\delta^2}{\delta x^x \delta x^s}$ makes no sense. So I guess this must be $\frac{\delta^2}{\delta x^r \delta x^s}$ . Secondly, and that is my first question, it seems odd to me that the authors use  the absolute derivative symbol  in this expression. That does not make sense to me and I suppose they mean the covariant derivative instead. So the expression (sorry, I use the notational convention of the book and not the more modern one with $\nabla$ or $;$ ) should be $G^{mn}+ \frac{1}{2a}\left[ a \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\right]_{|rs}$ . Am I right? Second question. Does someone know why the author chose this expression? Is it used somewhere in general relativity or physics in general? The pattern in the second term suggest remotely a connection with the Riemannian curvature ... Third question, does any one know the best approach to tackle the exercise? I made a lot of attempts but always get  stuck. Probably I got blind sighted and I'm turning around in the same wrong logic... For those interested, here's my best shot, which leads to ... nothing $$\blacklozenge$$ Let's define \begin{align}
K^{mn} &\equiv \frac{1}{2a}\frac{\delta^2}{\delta x^r \delta x^s}\left[ a \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\right]\\
T^{mn} &\equiv G^{mn}+ K^{mn}
\end{align} The strategy to proof this, is to separate in both terms of the expression, the parts that can be expressed in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ and the parts of higher order differentiation.
We begin with the second term. As the covariant derivatives of the metric tensor vanish, we get \begin{align}
K^{mn} &=\frac{1}{2a}\left[ a \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\right] _{|rs}\\
&= \frac{1}{2a}\left[ a_{|r} \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)+ a \underbrace{\left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)_{|r}}_{=0}\right] _{|s}\\
&= \frac{1}{2a}\left[ a_{|r} \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\right] _{|s}\\
&= \frac{1}{2a}\left[ \underbrace{a_{|rs}}_{=\partial^2_{rs} a} \left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)+a_{|r} \underbrace{\left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)_{|s}}_{=0} \right]
\end{align} Considering, \begin{align}
\partial^2_{rs} \ln a&= \partial_s \left(\frac{1}{a}\partial_r a\right)\\
&= \frac{1}{a}\partial^2_{rs}a-\frac{1}{a^2}\partial_r a\partial_s a\\
\Rightarrow\quad \frac{1}{a}\partial^2_{rs}a &= \partial^2_{rs} \ln a+ \frac{1}{a^2}\partial_r a\partial_s a
\end{align} \begin{align}
\Rightarrow \quad K^{mn} &= \frac{1}{2}\left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\partial^2_{rs}\ln a+\mathcal{E}^{mn} 
\end{align} with $\mathcal{E}^{mn}$ being a function in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ only. Note that $\mathcal{E}^{mn}$ is a symmetrical object in $m,n$ . Indeed, \begin{align}
\mathcal{E}^{mn} &= \frac{1}{2a^2}\left( a^{mn}a^{rs}-a^{mr}a^{ns} \right)\partial_r a\partial_s a\\
\Rightarrow\quad \mathcal{E}^{nm} &= \frac{1}{2a^2}\left( a^{nm}a^{rs}-a^{nr}a^{ms} \right)\partial_r a\partial_s a\\
&= \frac{1}{2a^2}\left( a^{mn}a^{sr}-a^{ns}a^{mr} \right)\partial_s a\partial_r a\\
&= \mathcal{E}^{mn}
\end{align} We now rewrite $G^{mn}$ . We have: \begin{align}
G^{mn} &= a^{nk}G^m_{. \ k}\\
G^m_{. \ k} &= R^m_{. \ k}-\frac{1}{2}\delta^m_k R\\
R^m_{. \ k} &= a^{mp} R_{pk}\\
R &= a^{pk}R_{pk}
\end{align} And by $3.205.$ \begin{align}
R_{pk} = \frac{1}{2} \partial^2_{pk} \ln a -  \partial_t \Gamma^t_{pk} + \mathcal{F}^{pk}
\end{align} With $\mathcal{F}^{pk}$ a function in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ only. Note that $\mathcal{F}^{pk}$ is a symmetrical object in $p,k$ as $R_{pk}$ is a symmetrical tensor in $p,k$ .
Hence by $(15)$ to $(19)$ : \begin{align}
G^{mn} &= a^{nk} R^m_{. \ k}-\frac{1}{2} a^{nk}\delta^m_k R \\
&=  a^{nk} a^{mp} R_{pk}-\frac{1}{2} a^{nm}a^{pk}R_{pk} \\
&=  \left( a^{nk} a^{mp} -\frac{1}{2} a^{nm}a^{pk}\right)R_{pk} \\
&=  \left( a^{nk} a^{mp} -\frac{1}{2} a^{nm}a^{pk}\right)\left(\frac{1}{2} \partial^2_{pk} \ln a -  \partial_t \Gamma^t_{pk} + \mathcal{F}^{pk}\right) \\
&=  \left( a^{nk} a^{mp} -\frac{1}{2} a^{nm}a^{pk}\right)\left(\frac{1}{2} \partial^2_{pk} \ln a -  \partial_t \Gamma^t_{pk} \right)+ \mathcal{H}^{mn}\\
&= \left[ \frac{1}{2} a^{nk}a^{mp}\partial^2_{pk} \ln a -  a^{nk}a^{mp}\partial_t \Gamma^t_{pk} -\frac{1}{4} a^{nm} a^{pk} \partial^2_{pk} \ln a +\frac{1}{2} a^{nm} a^{pk}  \partial_t \Gamma^t_{pk} \right]+ \mathcal{H}^{mn}
\end{align} with $\mathcal{H}^{mn}$ a function in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ only. Note that $\mathcal{H}^{mn}$ is a symmetrical object in $m,n$ . Indeed, \begin{align}
\mathcal{H}^{mn} &= \left( a^{nk} a^{mp} -\frac{1}{2} a^{nm}a^{pk}\right)\mathcal{F}^{pk}\\
\Rightarrow\quad \mathcal{H}^{nm} &= \left( a^{mk} a^{np} -\frac{1}{2} a^{mn}a^{pk}\right)\mathcal{F}^{pk}\\
&= \left( a^{mp} a^{nk} -\frac{1}{2} a^{mn}a^{kp}\right)\mathcal{F}^{kp}\\
&= \mathcal{H}^{mn}
\end{align} Putting $(10)$ and $(2)$ together with $\mathcal{L}^{mn}=\mathcal{E}^{mn}+ \mathcal{H}^{mn}$ we get, \begin{align}
T^{mn} &= Q^{mn}+\mathcal{L}^{mn}
\end{align} with $\mathcal{L}^{mn}$ a symmetrical object in $m,n$ and depending only on terms in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ and \begin{align}
 Q^{mn} &\equiv \left\{ \begin{array}{l}
\frac{1}{2} a^{nr}a^{ms}\partial^2_{rs} \ln a -  a^{nr}a^{ms}\partial_t \Gamma^t_{rs}\\\\ -\frac{1}{4} a^{nm} a^{rs} \partial^2_{rs} \ln a
+\frac{1}{2} a^{nm} a^{rs}  \partial_t \Gamma^t_{rs}\\\\+\frac{1}{2} a^{mn}a^{rs}\partial^2_{rs}\ln a-\frac{1}{2} a^{mr}a^{ns} \partial^2_{rs}\ln a\\\\
\end{array} \right. \\
&= \frac{1}{4} a^{mn}a^{rs}\partial^2_{rs}\ln a + \left( \frac{1}{2} a^{mn}a^{rs}- a^{ms}a^{nr}\right)\partial_t \Gamma^t_{rs}
\end{align} Note that \begin{align}
 Q^{nm} &= \frac{1}{4} a^{nm}a^{rs}\partial^2_{rs}\ln a + \left( \frac{1}{2} a^{nm}a^{rs}- a^{ns}a^{mr}\right)\partial_t \Gamma^t_{rs} \\
&= \frac{1}{4} a^{mn}a^{rs}\partial^2_{rs}\ln a + \left( \frac{1}{2} a^{mn}a^{rs}- a^{nr}a^{ms}\right)\partial_t \Gamma^t_{sr} \\
&= Q^{mn} 
\end{align} So $Q^{mn} $ is a symmetrical object in $m,n$ Given that by $(2.541.)$ , \begin{align}
\partial^2_{rs} \ln a &= \partial_{s} \partial_r \ln a \\
&= \partial_{s} \left(a^{kt}\partial_r a_{kt}\right)\\
&= \partial_{s} a^{kt}\partial_{r} a_{kt}+a^{kt}\partial^2_{rs} a_{kt} 
\end{align} $(32)$ can be written as \begin{align}
Q^{mn} &= \mathcal{P}^{mn}+ \mathcal{Z}^{mn}
\end{align} with $\mathcal{Z}^{mn} = \frac{1}{4} a^{mn}a^{rs}\partial_{s} a^{kt}\partial_{r} a_{kt}$ a symmetric object in $m,n$ and depending only on terms in $a_{ij}, a^{ij}, \partial_k a_{ij}, \partial_k a^{ij}$ and \begin{align}
\mathcal{P}^{mn} = \frac{1}{4} a^{mn}a^{rs}a^{kt}\partial^2_{rs} a_{kt} + \left( \frac{1}{2} a^{mn}a^{rs}- a^{ms}a^{nr}\right)\partial_t \Gamma^t_{rs}
\end{align} Expanding the Christoffel symbols and playing with the dummy indices, gives \begin{align}
\mathcal{P}^{mn} &= \left\{\begin{array}{l}\frac{1}{4} a^{mn}a^{rs}a^{kt}\partial^2_{rs} a_{kt} \\\\
+ \left( \frac{1}{2} a^{mn}a^{rs}- a^{ms}a^{nr}\right)\left( [rs,k]\partial_t a^{tk}+ a^{kt}\partial_t[rs,k]\right)
\end{array}\right.\\
&= \frac{1}{2} \mathcal{V}^{mn}  +\mathcal{A}^{mn}
\end{align} with \begin{align}
\mathcal{A}^{mn} &= \left( \frac{1}{2} a^{mn}a^{rs}- a^{ms}a^{nr}\right) [rs,k]\partial_t a^{tk}
\end{align} and \begin{align}
\frac{1}{2} \mathcal{V}_{mn} &= \left\{\begin{array}{l}
\frac{1}{4} a^{mn}a^{rs}a^{kt}\partial^2_{rs} a_{kt} \\\\
+ \left( \frac{1}{4} a^{mn}a^{rs} a^{kt}- \frac{1}{2} a^{ms}a^{nr} a^{kt}\right)\left( \partial^2_{tr}a_{sk}+\partial^2_{ts}a_{rk}-\partial^2_{tk}a_{rs}\right)
\end{array}\right.\\
&= \left\{\begin{array}{l}
\underline{\frac{1}{4} a^{mn}a^{rs}a^{kt}\partial^2_{rs} a_{kt} }
+ \underbrace{\frac{1}{4} a^{mn}a^{rs} a^{kt} \partial^2_{tr}a_{sk}+ \frac{1}{4} a^{mn}a^{rs} a^{kt} \partial^2_{ts}a_{rk}}_{= \frac{1}{2} a^{mn}a^{rs} a^{kt} \partial^2_{tr}a_{sk}}
- \underline{\frac{1}{4} a^{mn}a^{rs} a^{kt} \partial^2_{tk}a_{rs}}\\\\
- \frac{1}{2} a^{ms}a^{nr} a^{kt} \partial^2_{tr}a_{sk}
- \frac{1}{2} a^{ms}a^{nr} a^{kt} \partial^2_{ts}a_{rk}
+ \frac{1}{2} a^{ms}a^{nr} a^{kt} \partial^2_{tk}a_{rs}\\
\end{array}\right.
\end{align} giving \begin{align}
\mathcal{V}_{mn} &=
 a^{mn}a^{rs} a^{kt} \partial^2_{tr}a_{sk}
-  a^{ms}a^{nr} a^{kt} \partial^2_{tr}a_{sk}
-  a^{ms}a^{nr} a^{kt} \partial^2_{ts}a_{rk}
+  a^{ms}a^{nr} a^{kt} \partial^2_{tk}a_{rs}
\end{align} Playing with the dummy indices, gives \begin{align}
\mathcal{V}_{mn} &=
 \left(
   a^{mn}a^{rs} a^{kt} 
-  a^{mk}a^{nr} a^{st}
-  a^{mt}a^{ns} a^{kr} 
+  a^{ms}a^{nk} a^{rt} \right)\partial^2_{rt}a_{ks}
\end{align} Does $\mathcal{V}_{mn}$ vanish ?!
And here I'm stuck, ...","['tensors', 'vector-analysis', 'differential-geometry']"
4411587,Can the average of a set be lower than all of the averages of subsets?,Let's imagine there are marbles of different diameter and color. Can the average diameter of all the marbles be lower than all of the average diameter of marbles per color?,['statistics']
4411628,Examples where $(a+b+\cdots)^2 = (a^2+b^2+\cdots)$,"Consider the two infinite series $$
    \frac{\pi}{\sqrt{8}} = 1 + \frac{1}{3} - \frac{1}{5} - \frac{1}{7} + \frac{1}{9} + \frac{1}{11} - \cdots
$$ and $$
    \frac{\pi^2}{8} = 1 + \frac{1}{3^2} + \frac{1}{5^2} + \frac{1}{7^2} + \frac{1}{9^2} + \frac{1}{11^2} + \cdots
$$ (Notice that the first series has signs that go two-by-two rather than every-other.) Squaring the first equality also gives $\pi^2/8$ and so these two, when put together, satisfy the 'highschooler's dream' for squaring a sum: just square each term and sum, $$
    (a + b + c + \cdots)^2 = (a^2 + b^2 + c^2 + \cdots)
$$ with nothing like $2ab + 2ac + 2bc + \cdots$ needed. A trivial example of this would be $$
    (a + 0)^2 = a^2 + 2a0 + 0^2 = a^2 + 0^2
$$ but it only succeeds because one addend is zero. My questions are Are there any other simple nontrivial examples? I believe any other nontrivial example must be an infinite sum. edit: John Omielan provides the simple finite example $(1+1-\frac{1}{2})^2 = 1^2 + 1^2 + \frac{1}{2^2}$ . Is there an ""obvious"" demonstration that the above sum (other than the direct evaluation) satisfies the highschooler's dream?  Put another way, is there a simple demonstration that the infinite sum of ""cross terms"" vanishes?","['summation', 'sequences-and-series']"
4411661,"If $E_i$ is a topological space, does it hold $\mathcal B(E_1\times E_2)=\mathcal B(E_1)\otimes\mathcal B(E_2)$?","If $(E_i,\mathcal E_i)$ is a measurable space and $\mathcal G_i\subseteq2^{E_i}$ is a generator of $\mathcal E_i$ , then $$\mathcal E_1\otimes\mathcal E_2=\sigma(\mathcal G_1\times\mathcal G_2)\tag1.$$ Assume $(E_i,\tau_i)$ is a topological space and $\mathcal E_i=\mathcal B(E_i,\tau_i)=\sigma(\tau_i)$ is the Borel $\sigma$ -algebra on $(E_i,\tau_i)$ . Let $\tau$ denote the product topology on $E:=E_1\times E_2$ . Does it hold $\mathcal B(E,\tau)=\mathcal B(E_1,\tau_1)\otimes\mathcal B(E_2,\tau_2)$ ? If I remember correctly, this claim doesn't generally hold, unless $\tau_1$ and $\tau_2$ are Polish (complete and separable). However, does at least one inclusion hold in general? And does it hold $\mathcal B(E,\tilde\tau)=\mathcal B(E_1,\tau_1)\otimes\mathcal B(E_2,\tau_2)$ for some other topology $\tilde\tau$ on $E$ ?","['general-topology', 'probability-theory', 'measure-theory']"
4411671,Confusion regarding Lemma 5.10 in Lee's Riemannian Manifolds,"I am trying to learn about the exponential map $\mathrm{exp}$ and read about it in Lee's book Riemannian Manifolds: An Introduction to Curvature. The Lemma I am confused about can be found on page $76$ . It wants to show that for any $p \in M$ there exists an open neighbourhood $V$ of $0$ in $T_pM$ and $U$ of $p$ in $M$ such that $\mathrm{exp}_p:V \to U$ is a diffeomorphism. The proof goes as follows. First identify $T_pM \cong T_0(T_pM)$ and compute $\mathrm{exp}_p(V)$ for a tangent vector $V \in T_pM$ . Show that this is equal to $V$ under this identification and use the inverse function theorem. So as far as I can see it, one first has to note that in general $\mathrm{exp}_p$ is only defined on $$\{v \in T_pM \ | \ c_v \ \text{is defined on an interval containing} \ [0,1]\},$$ where $c_v$ denotes the geodesic with $\dot{c}_v(0)=v.$ However, one wants to compute the composition $$T_pM \to T_0(T_pM) \to T_pM$$ where the first map is given by the identification and the second map is given by $d(\mathrm{exp}_p)_p.$ Now, since $\mathrm{exp}_p$ is differentiable one can define the differential of that map, but why is it defined on $T_0(T_pM)$ even though $\mathrm{exp}_p$ is not defined on the entire tangent space $T_pM$ ? Furthermore, I fear that this is rather trivial, but what exactly is the identification map $T_pM \cong T_0(T_pM)$ ? And why does $$(\mathrm{exp}_p)_*V=\left.\frac{d}{dt}\right\vert_{t=0}(\mathrm{exp}_p \circ \tau)(t)$$ hold, where $(\mathrm{exp}_p)_*$ is defined as the composition of the exponential map with the identification above and $\tau(t)=tV.$ Is $V$ viewed as a derivation or a geometric tangent vector here? Thanks in advance for any help.","['geometry', 'differential-geometry']"
4411688,Finding Lyapunov function for particular system,"I am trying to find Lyapunov function for $$\begin{cases}\dot{t} = y\\\dot{y} = t^2-t\end{cases}$$ I tried common examples but, maybe I was wrong in my computations, couldn't derive anything. Could you help? Is there any approaches that cover some easy situations like this one (when variables are ""separated"")?","['lyapunov-functions', 'ordinary-differential-equations']"
4411695,Product of a derivative and a continuous function is a Darboux function,"Let $I\subset\mathbb{R}$ be an interval. Suppose that $f(x)$ has an antiderivative on $I$ and $g(x)$ is continuous on $I$ , I have to prove that $f(x)g(x)$ is a Darboux function, having the intermediate value property. ( $f(x)$ is said to have the intermediate value property on $I$ , if for every $a,b\in I$ , $a<b$ , and $r\in [f(a),f(b)]$ (or $[f(b),f(a)]$ ), there exists $c\in [a,b]$ such that $f(c) = r$ .) If $f$ is nonzero everywhere, we actually know that $fg$ has an antiderivative (see here ). The same applies if $f$ is bounded above of below (even just locally) since we can add a constant to $f$ . But in general, $fg$ may not have an antiderivative. An example can be found here . I have no idea what to do with this problem. Any help appreciated.","['calculus', 'derivatives', 'real-analysis']"
4411706,Relationship between Disintegration Theorem and Co-Area formula,"I have a feeling the Disintegration Theorem and the Co-Area formula for Lipschitz functions are actually very much related but I cannot seem to formalize it. DISINTEGRATION THEOREM: Let $(\mathsf{X}, \mathcal{X}, \mu)$ be a probability space and $(\mathsf{Y}, \mathcal{Y})$ be a measurable space. Let $\xi:\mathsf{X}\to\mathsf{Y}$ be a random variable with distribution $\nu = \mu \circ \xi^{-1}$ . Then $\nu$ -almost everywhere there exists a uniquely determined family of probability measures $\{\mu_y\}_{y\in\mathsf{Y}}$ on $(\mathsf{X}, \mathcal{X})$ such that (...skipping some results...) for any Borel measurable function $f:\mathsf{X}\to[0, \infty]$ the following is satisfied: $$
\int_{\mathsf{X}} f(x) d\mu(x) = \int_{\mathsf{Y}}\int_{\xi^{-1}(y)} f(x) d\mu_y(x) d\nu(y)
$$ CO-AREA FORMULA: Let $\xi:\mathbb{R}^m\to\mathbb{R}^n$ be Lipschitz with $m > n$ and $f:\mathbb{R}^m\to\mathbb{R}$ be Lebesgue measurable with generalized Jacobian $J_n\xi(x) = \left|D\xi(x) D\xi(x)^\top\right|^{1/2}$ . Then $$
\int_{\mathbb{R}^m} f(x) \lambda^m(dx) = \int_{\mathbb{R}^n}\int_{\xi^{-1}(y)} f(x) J_n\xi(x)^{-1}\mathcal{H}^{m-n}(dx) \lambda^n(dy)
$$ where $\lambda^n$ is the $n$ -dimensional Lebesgue measure and $\mathcal{H}^{n-m}$ is the $(n-m)$ -dimensional Hausdorff measure on $\xi^{-1}(y)$ . Attempted Solution The disintegration theorem seems to provide sufficiency conditions for this type of decomposition to happen, whereas the Co-Area formula gives an expression for the measures $\mu_y$ in a specific case. My guess is that one should take: $(\mathsf{X}, \mathcal{X}) = (\mathbb{R}^m, \mathcal{B}(\mathbb{R}^m))$ $(\mathsf{Y}, \mathcal{Y}) = (\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ $\mu = \lambda^m$ $\nu = \xi_*\lambda^m$ The only way in which I can conciliate the two formulas is if the following is somehow true $$
d\mu_y(x)d\nu(y) = J_n\xi(x)^{-1}d\mathcal{H}^{m-n}(dx)\lambda^n(dy).
$$ Not sure how to show it. Somehow I have a feeling that the Radon-Nikodym derivative of the pushed-forward Lebesgue measure and the Lebesgue measure gives a Jacobian $$
\frac{d \xi_*\lambda^m}{d\lambda_m} = J_n \xi(x).
$$ When $\xi$ is a diffeomorphism this would be coherent with the change of variables formula found in multivariable calculus where $$
\frac{d \xi_*\lambda^m}{d\lambda_m} = |D\xi(x)|
$$","['measure-theory', 'lebesgue-measure', 'measurable-functions', 'geometric-measure-theory']"
