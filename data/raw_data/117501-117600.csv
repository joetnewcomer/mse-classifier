question_id,title,body,tags
1737459,"what does linearly independent in C[0, 1] mean?","This is a question from my textbook I'm not quite sure what C[0, 1] mean, I tried to google the similar question and found that $C[0,1]$ usually denotes the collection of continuous functions $f: [0,1]\to \mathbb{R}$, but I'm still not quite sure what $f: [0,1]$ means, does $[0,1]$ means the domain of the function, can anyone give me a straightforward example","['linear-algebra', 'functions', 'vector-spaces']"
1737470,Parallel transport along a closed geodesic,"It do Carmo, in exercise 9.4, it is claimed that parallel transport along a closed geodesic in an even-dimensional orientable manifold ""leaves a vector orthogonal to the geodesic invariant."" So, let $\gamma:[0,a]\to M$ be this geodesic, with $\gamma(0)=\gamma(a)=p$, and let $V\in T_pM$ be orthogonal to $\gamma'(0)$. Let $P_t$ be the parallel transport along the geodesic. From Picard-Lindelöf, it is clear that $P_a\gamma'(0)=\gamma'(a)$. Let $V(t):=P_t V$. I believe the claim is that $V=P_aV$. It is clear that $\langle V(a),\gamma'(0)\rangle=0$, so $V(a)$ lies in the same $\gamma'(0)$-orthogonal subspace ($T_pM^\bot$) as $V$. Let $E_1,\dotsc,E_{n-1}$ be an orthonormal basis of this subspace. Extend these via parallel transport along the geodesic. Then $E_1(a),\dotsc,E_{n-1}(a):=P_aE_1,\dotsc,P_aE_{n-1}$ is also an orthonormal frame of $T_pM^\bot$. Since $P_t$ preserves orientation (I have already proved this), there is an $O\in\mathrm{SO}(n-1)$ such that $E_i(a)=\sum_{j=1}^{n-1}O_i{}^jE_j$. At this point I get stuck. I see no reason for $O$ to be the identity or what $M$ being even-dimensional has to do with anything. How do I continue?","['riemannian-geometry', 'differential-geometry']"
1737476,Posterior Predictive Distribution for a coin toss,"In this question, i can work out that the posterior is supposed to be a Beta (r+1, n-r+1) distribution. However, what I am struggling with is how to compute f(X_n+1|theta). Is this the binomial distribution with r+1 replacing r, because it's the probability of achieving r+1 heads in X_n+1 flips? Or is it simply theta, because it's the probability of seeing an (r+1)th head in 1 extra flip, having observed r heads in n flips? Obviously depending on which is the correct f, the answers vary significantly. Any help would be greatly appreciated! Thanks a lot","['bayesian', 'statistics']"
1737482,Confused about the definition of subspace,"The definition from my textbook is: A subspace of a vector space is a set of vectors that satisfies
  two requirements: If $v$ and $w$ are vectors in the subspace and $c$ is any
  scalar, then (1) $v + w$ is in the subspace. (2) $cv$ is in the subspace. And my textbook says vector space $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$
but let say $V=\begin{pmatrix}a\\b\end{pmatrix}$ and $W=\begin{pmatrix}c\\d\end{pmatrix}$, so $V$ and $W$ are all in $\mathbb{R}^2$, and clearly $V+W$ is in $\mathbb{R}^2$, $cV$ or $cW$ is in $\mathbb{R}^2$ too, so the first two requirements are met, why we say $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$? It looks like we have to add $V$ and $W$ should be in $\mathbb{R}^3$ as well, but why we don't have this in the definition?","['linear-algebra', 'vector-spaces']"
1737508,Show that orthogonal matrices are diagonalizable,"I want to prove that all orthogonal matrices are diagonalizable over $C$. I know that a matrix is orthogonal if $Q^TQ = QQ^T = I$ and $Q^T = Q^{-1}$, and that a matrix $A$ is diagonalizable if $A = PDP^{-1}$ where $D$ is a diagonal matrix. How can I start this proof?","['matrices', 'linear-algebra']"
1737516,Let $\int_{- \infty}^{\infty} f(x) dx =1$. Then show that $ \int_{- \infty}^{\infty} \frac{1}{1+ f(x)} dx = \infty.$,"Let $f : \mathbb{R} \to [ 0, \infty)$ be a measurable function. If $\int_{- \infty}^{\infty} f(x) dx =1$. Then I want show that $ \int_{- \infty}^{\infty}  \frac{1}{1+ f(x)} dx = \infty.$ Any help will be appreciated. Thank you in advance.","['integration', 'lebesgue-integral', 'measure-theory']"
1737530,Consider all possible orderings of the numbers from $1$ to $10$.,"Consider all possible orderings of the numbers from $1$ to $10$. For such
an ordering, a number is lucky if it appears in the same position as in the usual order. Assuming all orderings have the same probability, compute the expected number of lucky numbers in a random ordering. For this question, I was able to find the right answer by constructing the bijective mapping. Is there any way that this question can be done in probability theory? Thanks in advance","['probability-theory', 'probability']"
1737538,Are these two metric spaces complete?,"Let $J$ be an arbitrary non-empty set of indices, and let $\mathbb{R}^J$ denote the set of all the $J$-tuples of real numbers (i.e. the set of all the functions $x \colon J \to \mathbb{R}$, where we denote $x(\alpha)$ by $x_\alpha$, for each $\alpha \in J$). Now let the function $\tilde{\rho} \colon \mathbb{R}^J \times \mathbb{R}^J \to \mathbb{R}$ be defined as follows: 
$$ \tilde{\rho}( x, y) \colon= \sup \left\{ \ \min \left\{ \ \vert x_\alpha - y_\alpha \vert, \ 1 \ \right\} \ \colon \ \alpha \in J \ \right\} \ \mbox{ for all } \ x \colon= \left(x_\alpha \right)_{\alpha \in J}, \ y \colon= \left(y_\alpha \right)_{\alpha \in J} \in \mathbb{R}^J.$$
This function $\tilde{\rho}$ is of course a metric on $\mathbb{R}^J$, called the uniform metric. Is the metric space $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$ complete? My effort: Let $\left( x_n \right)_{n \in \mathbb{N}}$ be a Cauchy sequence in $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$, and let $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in J}$, for each $n = 1, 2, 3, \ldots$. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that 
$$ \tilde{\rho}(x_m, x_n ) < \epsilon \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m> N \ \mbox{ and } \ n> N. $$
Let's assume that $0< \epsilon < 1$. Let $\beta \in J$. Then for all $m, n \in \mathbb{N}$ such that $m > N$ and $n > N$, we have 
$$\epsilon > \tilde{\rho}(x_m, x_n) \geq  \min \left\{ \  \left\vert x_{\beta m} - x_{\beta n} \right\vert, \ 1 \ \right\} = \left\vert x_{\beta m} - x_{\beta n} \right\vert,$$
which shows that the sequence $\left(x_{\beta n} \right)_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$, and so this sequence converges to some real number $x_\beta$. Now since our $\beta \in J$ was arbitrary, let us define
$x \colon= \left( x_\alpha \right)_{\alpha \in J}$. Is this logic correct so far? If so, then how to proceed from here, especially if the index set  $J$ is infinite? Now let's take $J$ to be the set $\mathbb{N}$ of natural numbers, and let's denote $\mathbb{R}^J$ by $\mathbb{R}^\omega$ and define the function $D \colon \mathbb{R}^\omega \times \mathbb{R}^\omega \to \mathbb{R}$ by the formula
$$ D(x, y) \colon = \sup \left\{ \frac{ \min \left\{ \ \left\vert x_\alpha - y_\alpha  \right\vert, \ 1 \ \right\}}{\alpha } \ \colon \ \alpha  \in \mathbb{N} \ \right\} \ \mbox{ for all } \ x \colon= \left( x_\alpha  \right)_{\alpha \in \mathbb{N}}, \ y \colon= \left( y_\alpha \right)_{ \alpha \in \mathbb{N}} \in \mathbb{R}^\omega.$$
Then $D$ is also a metric on $\mathbb{R}^\omega$, and the topology induced by $D$ is the same as the product topology on $\mathbb{R}^\omega$. Is the metric space $\left( \ \mathbb{R}^\omega, \ D \ \right)$ complete? Let $\left(x_n \right)_{n \in \mathbb{N}}$, where $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in \mathbb{N}}$ for each $n \in \mathbb{N}$, be a Cauchy sequence in $\left( \ \mathbb{R}^\omega, \ D \ \right)$. Let $\beta$ be a fixed natural number. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that
$$ D(x_m, x_n) < \frac{ \epsilon}{ \beta} \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m > N \ \mbox{ and } \  n > N. $$
Let's assume that $0 < \epsilon < 1$. Then for all natural numbers $m$ and $n$ such that $m, n > N$, we have 
$$\left\vert x_{\beta m} - x_{\beta n} \right\vert < \epsilon,$$
from which it follows that the sequence $\left( x_{\beta n} \right)_{ n \in \mathbb{N}}$ is a Cauchy sequence of real numbers and therefore this sequence converges to some real number $x_\beta$. Since our $\beta \in \mathbb{N}$ was arbitrary, we can define an element $x \in \mathbb{R}^\omega$ as $$x \colon= \left( x_\alpha \right)_{\alpha \in \mathbb{N}}.$$ Is my logic correct so far? If so, then how to proceed from here? What if, in either of the above two cases, we replace $\mathbb{R}$ by an arbitrary complete metric space (and replace $\vert \cdot \vert$ by the corresponding distance of course)? Do we still have a metric in either case? And, if so, is (are) this (these) metric(s) complete?","['real-analysis', 'complete-spaces', 'general-topology', 'metric-spaces', 'analysis']"
1737563,What is represented by the set of complex numbers z satisfying the equation $(3+7i)z+(10-2i)\bar{z}+100=0$?,"I have an objective type question :- The set complex numbers $z$ satisfying the equation:-
$$(3+7i)z+(10-2i)\bar{z}+100=0$$
represents:- A) A straight line B) A pair of intersecting straight lines C) A point D) A pair of distinct parallel straight lines My approach:-
I have put $z=x+iy$ and solved equation to get :-
$$13x-9y+i(5x-7y)+100=0$$
i.e.
$$y=\frac{13}{9}x+\frac{100}{9}\tag{1.}$$&
$$y=\frac{5}{7}x\tag{2.}$$
So eq. $1$ & $ 2$ are two straight lines which are intersecting each other.So my answer is B) Now i want to ask that ,is my approach and answer correct? if yes ,then is there any simpler approach? and if no then how would i get the answer?","['plane-geometry', 'complex-numbers', 'geometry']"
1737591,Integration of a $k$-form over chains,"In Spivak's Calculus on Manifolds , he defines the integral of a $k$-form over a $k$-chain, and proves a version of Stokes' theorem for this situation, before moving on to discuss the integral of a differential form over a manifold.  Other books, such as Introduction to Smooth Manifolds by Lee and Analysis on Manifolds by Munkres, seem to skip this step.  They define the integral of a $k$-form over a $k$-manifold without mentioning or proving theorems about $k$-chains.  (Am I correct about that?) What is the advantage (if any) of Spivak's approach?  To me, $k$-chains appear to be an unnecessary weird definition in a subject that already has too many unfamiliar definitions.","['manifolds', 'differential-forms', 'integration', 'differential-geometry']"
1737661,Coordinate free Geometric Algebra vs. Linear Algebra,"I think I know what coordinate free means. But I never found in ANY text a good explanation of it or something like: This is the problem solved with coordinates and this is the problem solved without coordinates, etc. Since the philosophy of GA is that everything should be coordinate free, I would like to see an example of something that can be done in GA without coordinates but you have to use coordinates with usual linear algebra. To be specific, in GA you can make something like this in, let's say $\mathbb{G_3}$: $c=(bab)I$ The vector $a$ is rotated about the vector $b$, and then you take the dual plane of it. Is there an efficient way to do this without coordinates in linear algebra? This is just an example that I made up spontaneously; maybe there are better examples.","['clifford-algebras', 'geometric-algebras', 'linear-algebra']"
1737687,Doubt on the Krull topology on infinite Galois extension,"A pre-base (in fact a base)  of the topologycal group G of the infinite Galois extension $F/K$ is given by $\sigma Gal(F/F_i) $ with $F_i/K$ finite Galois subextension. 
Those opens are uniquely identified by the property $f|_{F_i} =\sigma|_{F_i}$. 
If we now consider $\sigma Gal(F/F_j) \cap \tau Gal(F/F_i) $ we can get: The null set if $\tau$ and $\sigma$ doesn't agree on $F_i\cap F_j$ $\gamma Gal(F/F_iF_j) $ otherwise where $\gamma$ is the extension to $F$ of the map on $F_iF_j$ which behave as $\sigma$ on $F_i$ and as $\tau$ on $F_j$ Now I fear there is an error probably in the second point (because my note aren't like this) but I can't find which assumption is wrong, can you help me?","['general-topology', 'proof-verification', 'galois-theory']"
1737716,Simple application of Donsker's theorem,"I am trying to do exercise 5.15 in Moerter's book ""Brownian Motion"". It seems quite easy, but I can't solve it somehow: Suppose $S(j)_j$ is a SRW on the integers, started at zero. Show that:
$$
\frac{1}{n^2}\min_j\{|S(j)|=n\}\to^d\min_t\{|B(t)|=1\}\, .
$$
Now, this looks like a standard application of Donsker's theorem (although, perhaps it is not?). Anyway, I tried hitting it with Donskers theorem, but I just keep getting nonesense:
$$
P(\min_t\{|B(t)|=1\}\le s)=P(M_s\ge 1)=P(B\in K_s)\, \
$$
where $M$ is the max of the modulus of a Brownian motion and and $K_s$ is the set of continuous functions whos maximum is bigger than one. But:
$$
P(S^*(t)\in K_s)=?
$$
The first problem is that we don't have the intervall $[0,1]$ as required in Donsker's theorem, and the second one is that I cannot related the interpolated and scaled SRW to the property above (the one with $1/n^2$)
Any help welcomed. thank you!","['stochastic-processes', 'brownian-motion', 'probability', 'stochastic-approximation']"
1737737,Is there a way to write this recurrence relation in faster-to-program manner?,"I have the following recurrence relation for some coefficients $$b_{n+2} = \frac{1}{(n+3)(n+2)P_0} \sum_{k=1}^n (n-k+2) (n-k+1) b_k b_{n-k+2}, \quad n>1$$ with $b_1$ to $b_3$ and $P_0$ being the  initial conditions of the series. The problem is that I have to calculate the series to a high value of $n$ (about 10000, or sometimes even more). And since the $n$th term depends on a sum that involves all the other terms before it, the calculation is really slow. So I'm trying to simplify it in some way to make it faster to program. (Getting rid of the sum would be already a good start for example.) The best cases scenario would be to have $b_n = F(n)$, which of course would be simpler to read and way faster to program, but I'm not sure this is possible. Is there a way to re-write the relation in a way that's faster to calculate? (Or at the very least in a simpler way?) If you prove that it isn't possible to achieve what I want I'd be also extremely grateful. That way I can stop worrying about it. Thank you.","['taylor-expansion', 'recurrence-relations', 'summation', 'sequences-and-series']"
1737774,What is the period of a fuction which satisfies the condition $f(a-x)=f(a+x)$?,"What is the period of a function which satisfies the condition $f(a-x)=f(a+x)$ where a is any positive integer? I tried substituting $x$ with $x-a$ but that does not seems to help me a lot.
I ended up getting $f(x)=f(-x+2a)$ I tried substituting other similar terms but was unable to get to a solution.","['periodic-functions', 'functions']"
1737775,Catalan's constant and $\int_{0}^{2 \pi} \int_{0}^{2 \pi} \ln(\cos^{2} \theta + \cos^{2} \phi) ~d \theta~ d \phi$,"According to my book (The Nature of computation, page 691): $$\int_{0}^{2 \pi} \int_{0}^{2 \pi} \ln(\cos^{2} \theta + \cos^{2} \phi) ~d \theta ~d \phi= 16 \pi^2  \left(\frac{C}{\pi}- \frac{\ln2}{2}\right),$$ where $C$ is Catalan's constant. I have tried to derive this expression by looking at integral representations of $C$, but I have not been able to perform the integral. Any help? Thank you.","['catalans-constant', 'integration', 'definite-integrals', 'trigonometric-integrals']"
1737789,Prove that $\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+\cdots$,"Prove that 
  $$\sin n\theta=n\sin \theta-\frac{n(n^2-1)}{3!}\sin^3\theta+\frac{n(n^2-1)(n^2-3^2)}{5!}\sin^5\theta+-\cdots$$ If I am not mistaken, this identity was either proven by Newton or known to him, so if possible I would really like to see the way he approached it, though any solution will suffice. My brief efforts involved induction on $n$ which failed since I ended up with having to manipulate $\sin( n+1)\theta=\sin( n\theta +\theta)=\sin n\theta \cos\theta+\cos n\theta\sin\theta $, which involves $\cos n\theta$. I tried the ""familiar"" method of expansion of $$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots$$
but this only made it more complicated$$\sin n\theta=n\theta-\frac{(n\theta)^3}{3!}+\frac{(n\theta)^5}{5!}-+\cdots=\\n\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)-\frac{n(n^2-1)}{3!}\Big(\theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-+\cdots\Big)^3+-\cdots$$
Another attempt would be to use the identity $$\sin n\theta=\frac{1}{2i}(e^{in\theta}-e^{-in\theta})$$
though this was obviously not known to Newton. In any case, any thougths, ideas are welcome.. EDIT A thorough answer has been provided below, but since it  involves the use of complex numbers, I deem that the search for another answer, one based solely on the mathematical knowledge up to Newton's time is open. After a bit more research, it appears that Newton came up with the formula after reading a book by Vieta, but I have been unable to gather further info on whether the formula was known to Vieta as well.","['real-analysis', 'reference-request', 'trigonometry', 'calculus', 'sequences-and-series']"
1737791,The $Hol$ operator is a continuous function?,"Let $\Omega$ be a compact space, and consider $C(\Omega)$ the space of 
the continuous functions over $\Omega$, consider also, $C^\gamma(\Omega)$ the 
space of all $\gamma$-holder continuous functions, i.e, $f\in C(\Omega)$ s.t
$$
Hol(f):=\sup_{x\neq y}\dfrac{f(x)-f(y)}{d(x,y)^{\gamma}}<\infty
$$ My problem is the following: Let $(f_n)\subset C^\gamma(\Omega)$ be a sequence
of Holder continuous functions s.t  $f_n$ converge uniformly to a constant function $f\equiv F.$ It is clearly that $Hol (f)=0$, my question: It is truth that $Hol(f_n)\to 0$?","['real-analysis', 'metric-spaces', 'holder-spaces', 'analysis']"
1737794,Embedding of Kähler manifolds into $\Bbb C^n$,Consider $\Bbb C^n$ with its standard hermitian product. This space produces many example of Kähler manifolds simply by taking a smooth affine variety $X\subseteq\Bbb C^n$ with the induced metric. Now I am wondering about the converse: Question: Suppose that $X$ is a Kähler manifold that is also a smooth affine variety over $\Bbb C$. Can we assume that $X$ has an embedding $X\subseteq\Bbb C^n$ such that the metric of $X$ is the one inherited from the hermitian product on $\Bbb C^n$? Note that I assume that $X$ is algebraic to begin with. It would false for a general Kähler manifold.,"['complex-geometry', 'kahler-manifolds', 'differential-geometry', 'algebraic-geometry']"
1737805,Determine $E\sum_0^\infty X_n1_{(T=n)}$,"$X_T = \sum_0^\infty X_n 1_{(T=n)}$ where $T$ is a stopping time and $(X_n)$ is a martingale. Show that if $T$ is bounded then $EX_T = EX_0$: $T \leq N$, and then consider $X_T = X_{T\wedge N} = \sum_0^NX_n1_{(T = n)}$ We have, $$EX_T = E\sum_0^NX_n1_{(T = n)} = \sum_0^NE(X_n1_{(T=n)})$$ Now I am having troubles using the properties of martingales, usually we have that $E(X_n | \mathcal{F}_{n-1}) = X_{n-1}$ but I don't see how to use that property here as we're not conditioning on $(\mathcal{F}_n)$ yet. This is my first introduction to martingales so please if someone could explain how I could tackle this problem in detail edit: using how you defined $X_T$ I have: $$E(X_T | \mathcal{F}_{n-1}) = E(X_0|\mathcal{F}_{n-1}) + \sum_1^NE((X_j - X_{j-1})1_{T\geq j})|\mathcal{F}_{n-1})$$ now as you said $\{T \geq j\} \in \mathcal{F_{j-1}}$  so $E(X_j - X_{j-1})1_{T\geq j}= 1_{(T\geq j)}( E(X_j|\mathcal{F}_{n-1}) - E(X_{j-1} | \mathcal{F}_{n-1}))$ but again I am stuck here edit: is it right to say that: $E(X_0|\mathcal{F}_{n-1}) = X_0$ and $$E((X_j - X_{j-1})1_{T\geq j})= 1_{(T\geq j)}( E(X_j|\mathcal{F}_{n-1}) - E(X_{j-1} | \mathcal{F}_{n-1}))= 1_{(T\geq j)}(X_{j-1} - X_{j-1})?$$ another solution: Could we not do it like this: If $T \leq N$ then $X_T = \sum_0^N X_n 1_{(T=n)}$ now if I take the expectation $$E(X_T | \mathcal{F}_0)= \sum_0^N E(X_n1_{(T=n)}|\mathcal{F}_0) = \sum_0^N 1_{(T=n)} E(X_n | \mathcal{F}_0)$$ now since $(X_n)$ is a martingale, we have $E(X_n | \mathcal{F}_0) = X_0$, which gives $E(X_T | \mathcal{F}_0) = X_0$ taking expectations again, and using the tower property we get $E(X_T) = EX_0$","['expectation', 'probability-theory', 'stopping-times', 'probability', 'martingales']"
1737814,Divisibilty of $1^{101} + 2^{101} + 3^{101}+ 4^{101}+\cdots+2016^{101}$ [duplicate],This question already has answers here : Doubt regarding divisibility of the expression: $1^{101}+2^{101} \cdot \cdot \cdot +2016^{101}$ (3 answers) Closed 8 years ago . $1^{101} + 2^{101} + 3^{101}+ 4^{101}+\cdots+2016^{101}$ is divisible by which of the following? $(A)$ $2014$ $(B)$ $2015$ $(C)$ $2016$ $(D)$ $2017$ Could someone share the approach to deal with such type of questions?,"['number-theory', 'binomial-theorem']"
1737819,Evaluating an indefinite integral using complex analysis,"Using tools from complex analysis, I have to prove that $$ \int_0^{\infty} \frac{\ln x}{(x^2 + 1)^2}\,dx = - \frac{\pi}{4}.$$ But I'm not really sure where I should start. Any help would be appreciated.","['indefinite-integrals', 'complex-analysis', 'complex-integration']"
1737828,What polytopes can be induced by a norm?,"Let $\|\cdot\|:\mathbb{R}^n\to\mathbb{R}_{\ge0}$ be a norm and define $B_{\|\cdot\|}(0,1):=\{x\in\mathbb{R}^n\mid\|x\|\le1\}$ be the unit circle. For which regular n-dimensional polytopes (relative to the Euclidean norm resp. the Euclidean product) there exists a norm such that $B_{\|\cdot\|}(0,1)$ is equal to the polytope given? For $n=3$ we can easily obtain a cube (wit the max norm) and an oktahedra (with $\|\cdot\|_1$) but can we find a norm withe the unit circle of a tetrahedra? What other forms are possible? I came up with this question purely out of curiosity, but I am quite clueless how to approach it. So any thoughts are appreciated.","['real-analysis', 'calculus', 'geometry']"
1737852,Laurent expansion of $\frac{1}{\sin z}$,"Question is a fully solved exercise in Gamelin's complex analysis. Exercise : Consider the Laurent series expansion for $\frac{1}{\sin z}$ that converges on the circle $\{|z|=4\}$. Find the coefficients $a_0,a_{-1},a_{-2},a_{-3}$. Determine the largest open set on which the series converges. Solution :  The only zeros of $\sin z$ are at the integral multiples of $\pi$. These are then the only singularities of $\frac{1}{\sin z}$, and they are all simple poles. The largest open annular set containing the cirlce and to which $\frac{1}{\sin z}$ extends analytically is then the annulus $\{\pi <|z|<2\pi\}$. The annulus is then the largest open set on which the Laurent series converges. We have expansions
 $$\sin z =z-\cdots \rm{near~~}z=0$$ 
 $$\sin z =-(z-\pi)-\cdots \rm{near~~}z=\pi$$ 
 $$\sin z =-(z+\pi)-\cdots \rm{near~~}z=-\pi$$ Just by considering inverses, long divisions, we see that $$\frac{1}{\sin z} =\frac{1}{z}+\rm{analytic}$$ 
 $$\frac{1}{\sin z}=-\frac{1}{z-\pi}+\rm{analytic}$$ 
 $$\frac{1}{\sin z} =-\frac{1}{z+\pi}+\rm{analytic}$$ We conclude that if $$f_1(z)=\frac{1}{z}-\frac{1}{z-\pi}-\frac{1}{z+\pi}$$ then $f_0(z)=1/\sin z-f_1(z)$ is analytic for $|z|>2\pi$. Thus, $$\frac{1}{\sin z}=f_0(z)+f_1(z)$$ is the Laurent decomposition of $1/\sin z$. I do not understand what ever is written in bold. I see that $f(z)$ is analytic for $|z|>\pi$.. could not figure out why $f_0(z)$ is analytic for $|z|<2\pi$.. We have $$f_3(z)=\frac{3}{\sin z}-f_1(z)=\left(\frac{1}{\sin z}-\frac{1}{z}\right)+\left(\frac{1}{\sin z}-\frac{1}{z-pi}\right)+\left(\frac{1}{\sin z}-\frac{1}{z+\pi}\right)$$ Each function in the brackets is analytic at $0,\pi,-\pi$ respectively.  does it imply sum is analytic?? That too in $|z|<2\pi$$?? $f_1(z)$ is just same as $f_3(z)$ except a constant.. so, if $f_3(z)$ is analytic then so would be $f_1(z)$.. Help me to fill the gaps..","['analyticity', 'complex-analysis']"
1737864,Can $C^\infty(\mathbb{T})$ become a Banach space?,"Let $T$ be the unit circle and $C^\infty(\mathbb{T})$ the set of functions defined on $\mathbb{T}$
which have derivatives of every order. I know that $C^\infty(\mathbb{T})$ with the metric induced by the seminorms 
$$\sup_{t\in\mathbb{R}}|f^{(l)}(e^{it})|,l\geq 0$$ is complete (but not a Banach space with the seminorms themselves i.e. its locally convex structure cannot be defined by one norm). Is there any chance that we can define some kind of norm on $C^\infty(\mathbb{T})$ in order to become a Banach space?",['functional-analysis']
1737865,All possible ways to order numbers in an array with decreasing rows and columns,"Given positive integer numbers $1,2,...,N\cdot M$. How many ways are there to order them in an $N\times M$ array given that the values decrease in each row from left to right and in each column from top to bottom? For small arrays one can just count but I don't find a general rule. Thanks for any help.",['combinatorics']
1737891,Does proper contraction on Hilbert space necessarily lead to convergence in norm to zero?,"I was asked this in functional analysis class: Let $ \mathbb{H} $ be a Hilbert space and we are given an operator T satisfying: $ || Th || < ||h|| $ for all $ h \in H $ . We are asked if the following is necessarily true: for all $ h \in \mathbb{H} $ does one have the convergence to zero $ T^nh \to 0 $ in the Hilbert space norm? How about the other direction? Intuitively it seems to me it is untrue as the Supremum of the operator norm might be one and the inequality given here seems weaker than the conclusion given, but I have no counterexample. For the other direction I got no real idea. Could someone be so kind as to please point me to the way? Thank you.","['hilbert-spaces', 'normed-spaces', 'operator-theory', 'functional-analysis', 'contraction-operator']"
1737905,The cylinder does not embed into $\Bbb C^n$,"The cylinder $\Bbb R\times S^1$ can be viewed as a complex manifold with a flat metric by viewing it has the quotient $\Bbb R\times\Bbb R/\Bbb Z$, where $\Bbb R\times\Bbb R=\Bbb C$. (In fact it makes the cylinder into a Kähler manifold.) Problem: Show that there is no isometric holomorphic embedding
  $$\varphi:\Bbb R\times S^1\hookrightarrow \Bbb C^n,$$
  for any $n$, where $\Bbb C^n$ has the standard Kähler structure. Motivation: I was trying to answer another of my questions here , and after some research I found this mathoverflow post of Peter Kronheimer claiming the above. He gives the following reason. Hint: Apply the maximum modulus principle to the derivative of $\varphi$. I tried this approach without any success. Does anybody know how it works?","['complex-geometry', 'complex-analysis', 'riemannian-geometry', 'differential-geometry']"
1737910,Mobius Transformation and Schwarz's Lemma,"My goal is to find a Mobius transformation $g$ that sends $K : |z| < R$ bijectively to itself, and also sends $0$ to $a \in R$. To my knowledge, there is a theorem that says the following: For a disc $K: |z| \leq 1$ and with $f:K \rightarrow K$ being analytic on $|z| <1$, $f$ is of the following form: $$f(z) = e^{i\alpha} \frac {z-a}{1-\overline a z}$$ My idea was that to accomplish what I need to, I have to generalize the proof of this theorem to a disc of radius $R$, rather than the unit disc. How would I be able to do this?","['complex-analysis', 'mobius-transformation']"
1737929,Explaining whether a function is injective/surjection ($f\colon\Bbb N\to P(\Bbb N)$),"Let $f\colon\Bbb N\to P(\Bbb N)$ be given by $$f(n)=\{n+1,n+2,n+3,\dots\}.$$
  $(a)$ Is $f$ an injection ? Explain. $(b)$ Is $f$ an surjection ? Explain. $(a)$ A function is injective when $f(x_1) = f(x_2)\implies x_1 = x_2$. But since this is a sequence, I'm not quite sure. It seems to be injective as each $n\in \Bbb N$ gives you a unique $f(n)$, e.g. $(n = 1, \{2, 3, 4,\dots\}, n = 2, \{3, 4, 5, \dots\})$ $(b)$ A function is surjective when $f(A)$ (its image) $= B$ (target space). The image of this clearly does not seem to equal the target space of $P(\Bbb N)$. e.g. $P(\mathbb{N}) = \{\{\}, \{1\}, \{2\}, \{3\},\dots,\{1,2,3\dots\}\}$ So, I would conclude that the function is not a surjection but is indeed an injection.","['elementary-set-theory', 'proof-explanation']"
1738012,Direct proof of Bolzano-Weierstrass using Axiom of Completeness,"The author of my intro analysis text has an exercise to give a proof of Bolzano-Weierstrass using axiom of completeness directly. Let $(a_n)$ be a bounded sequence, and define the set $$S=\{x \in \mathbb{R} : x<a_n \text{ for infinitely many terms } a_n\}.$$ Show that there exists a subsequence $a_{n_k}$ converging to $s = \sup S$. I feel I am close. I know that for any $\epsilon > 0$, there must be infinitely many $a_n$ such that $\sup S - \epsilon < a_n < \sup S + \epsilon$. (If there were only finitely many $a_n$ in that interval, then $\sup S + \frac{\epsilon}{2} \in S$, contradicting $\sup S$ as an upper bound.) However, I don't know how to pinpoint a single subsequence $(a_{n_k})$ such that all such elements with $k \geq \text{ some } N$ are in this interval for all $\epsilon$.",['real-analysis']
1738016,Number of integral solutions to a linear inequality,"I am trying to show the following identity: Let $k,n \in \mathbb{N}$. Then
$$
\text{card}\{x \in \mathbb{Z}^n: \sum_{i=1}^n |x_i| \leq k\} =\sum_{i=0}^n 2^{n-i} {n \choose i}{k \choose n-i}.
$$ My attempt: Let $A=\{x \in \mathbb{Z}^n: \sum_{i=1}^n |x_i| \leq k\}$. Let $0\leq i \leq n$. Let $x_1,\ldots,x_{n-i}\geq 1$ be such that $x_1+\ldots+x_{n-i} \leq k$. Then it is easy to see that $|A|=\sum_{i=0}^n 2^{n-i} {n \choose i} |\text{no. of positive integral solutions to } x_1+\ldots+x_{n-i} \leq k| $. However, I am getting the number of positive integral solutions to $x_1+\ldots+x_{n-i} \leq k$ as not equal to ${k \choose n-i}$. Can anyone help me?",['combinatorics']
1738045,Riesz Representation Theorem in Linear Algebra,"Let $\mathbb{V}$ be a finite dimensional inner product space and $\alpha : \mathbb{V} \rightarrow \mathbb{R}$ a linear functional. Prove that there is a unique vector $\overrightarrow v_{0} \in \mathbb{V}$ such that $\alpha(\overrightarrow v)=\langle\overrightarrow v,\overrightarrow v_{0}\rangle$ for all $\overrightarrow v \in \mathbb{V}$. My approach : I suppose that there is exists another vector $\overrightarrow w_{0} \in \mathbb{V}$ that satisfies the same property. We get $\langle\overrightarrow v,\overrightarrow v_{0}-\overrightarrow w_{0}\rangle=0$ and I need to show that $\overrightarrow v_{0}=\overrightarrow w_{0}$ somehow. Any tips on how to do that? I tried taking an orthonormal basis for $\mathbb{V}$ but that didn't help in the end.","['riesz-representation-theorem', 'linear-algebra']"
1738050,What does the series $\sum_{n=2}^\infty \frac{2}{n^3-n}$ converge to?,"I know that the series converges. My questions is to what. I tried seeing if it was a telescoping series:
$\sum_{n=2}^\infty \frac{2}{n^3-n} =  2\sum_{n=2}^\infty (\frac{1}{n^2-1}-\frac{1}{n})$ but it doesn't seem to cancel any terms. Thoughts?","['convergence-divergence', 'sequences-and-series', 'calculus']"
1738114,"prove $\max\{x,y\} = (x+y+|x - y|)/2$","Prove the following, 
$\max\{x,y\}=(x+y+|x-y|)/2$ Attempt at the proof:  first off I started by separating the expression into the following, 
$(x+y)/2 + |x-y|/2 $ and noting that both of them are an expression for the midpoint of the two, and so adding them together will give a max.  I just need help putting this into words and making a formal proof.","['algebra-precalculus', 'proof-verification']"
1738115,"Partition binary strings, subject to a restriction","I'm interested in counting the number of partitions of the space of binary strings of length $k$, subject to a condition. The condition is: strings with Hamming distance equal to $k$ can't be in the same partition element. For example, if $k = 2$, 01 and 10 can't be in the same partition element. I've been looking at the literature and I haven't been able to find a resource for this. I've also been unable to generalize this from the simple $k=2$ and $k=3$ cases that I've been able to work out by enumeration. Any pointers, suggestions or references are much appreciated. Thanks.","['combinatorics', 'combinatorics-on-words']"
1738118,Continuous Uniform Distribution Problem,"Let $ X $ be a continuous random variable on the interval $ (a,b) $. The mean of $ X $ is $ 800 $ and the variance of $ X $ is $ 120,000 $. Calculate the range of $ (a,b) $. My default approach was to proceed as follows: $$ E(X) = \frac{(a+b)}{2} = 800 .$$ $$ Var(X) = \frac{(b-a)^2}{12} = 120,000. $$ Therefore $$ \frac{Var(X)}{E(X)} = \frac{2(b-a)}{12} = 150, $$ and so $ b-a = 900 $. However, the solutions in the back of the book take an alternative approach, and attain different results. $$ b + a = 1,600. $$ Therefore $$ \frac{(1600 - 2a)^2}{12} = 120,000 ,$$ and so $$ 4a^2 -6,400a +1,120,000 = 0 .$$ Solving with the quadratic equation we get $ a = 200 $ and $ b = 1,400 $, giving a range OF 1,200 $. Can anyone see why we get diferent results?",['statistics']
1738173,How do I prove that $AB = BA$?,"Two $n × n$ matrices $A$ and $B$ are said to be simultaneously diagonalizable if there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal. Prove that if $A$ and $B$ are simultaneously diagonalizable $n × n$ matrices, then $AB = BA$ My attempt: if A and B are simultaneously diagonalizable, then there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal (given). $A = S^{−1}AS$ $B = S^{−1}BS$ $AB  = S^{−1}AS \times S^{−1}BS  = BA = S^{−1}BS \times S^{−1}AS$ Is that a sufficient proof? I think I am going wrong somewhere...","['matrices', 'linear-algebra']"
1738180,Is there a way to update the inverse of a sum of two matrices following a rescaling of one of them?,"Suppose I have two matrices $A$ and $B$ (let's assume that both $A$ and $B$ are invertible, as is their sum), and a scalar $g$. I am interested in the matrix $$M^{-1} = (A + gB)^{-1}$$ I am aware of various expressions for computing this inverse in general , but I am interested in whether, if I calculate $M^{-1}$ for some value of $g$, is there a way to quickly update $M^{-1}$ following an update to the value of $g$? I am specifically interested in whether this can be done without performing any additional inversions after updating $g$, i.e. if I can just store $A$, $B$, $A^{-1}$, $B^{-1}$ (or some factorizations of them) and the previous value of $g$ in memory, and then somehow update $M^{-1}$ as a function of these? I've just found this , which suggests a solution if $B=I$, but I fear I may be out of luck for the more general case where $B\neq I$. I would also potentially be interested in solutions which rely on sparsity of either $A$ or $B$, as I may have some cases in which this is true.","['matrices', 'linear-algebra', 'inverse']"
1738187,Attempting to show $P(|S_n| <1)$ for a martingale $(S_n)$,"Now, I am stuck on the last part of the question. I managed to find the solutions, but I don't udnerstand them completely. What I don't understand is: How they got that indicator function, and why they are using double expectation to calculate the probability. I'd appreciate it if someone coudl help out, thanks.","['real-analysis', 'probability', 'martingales', 'probability-theory']"
1738225,Is the Klein bottle homeomorphic to the union of two Mobius bands attached along boundary circle?,"Question: Determine whether the Klein bottle is homeomorphic to the union of two Mobius bands attached along their boundary circles. The Klein bottle is the quotient space
$$
K=I^2 /{\sim}, \quad (x,0)\sim(x,1), \; (0,y)\sim(1,1-y), \; \forall x,y\in I
$$
The Möbius band is the quotient space
$$
M=I^2 /{\sim}, \quad (0,y)\sim(1,1-y)
$$ What would be a good way to approach this question? I have not had any success constructing a map between spaces Edit: I remember that homeomorphism must preserve orientability. So this could be used to disprove a homeomorphism. The mobius band is non-orientable, as is the klein bottle. What I am not sure about is if we take the union of two non-orientable Mobius bands and attach their boundary circles, do we still get a non-orientable surface. I think the gluing the boundary step may switch the orientabilty. So we have an orientable surface which therefore cannot be homeomorphic to the non-orientable Klein bottle. I am unsure how to prove this in a formal way (with equations and notation etc) Would appreciate your help","['algebraic-topology', 'general-topology', 'mobius-band', 'klein-bottle']"
1738253,Interesting power series for $y'+y=\frac1x$,"I had the differential equation $y'+y=\frac1x$, which I solved for $y$ as a power series: $$y=\frac1x\sum_{n=0}^{\infty}\frac{n!}{x^n}$$ Which was a power series at $\infty$, so it doesn't really help me much. So my first question is whether or not $y$ is solvable here (as a power series if needed) where it actually converges. My second question is if it pure coincidence that the summation is very similar to the power series of $e^x$. $$e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$$ Is there some reason for their very similar forms, or just my stumbling upon these two unrelated power series?","['ordinary-differential-equations', 'power-series']"
1738313,How can we compute $\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}}$? Is it $\frac{\sqrt{17}-1}{2}$?,"How can we compute $\sqrt{4-\sqrt{4-\sqrt{4-\cdots}}}$? I can understand that if we define  $a_1=\sqrt{4}$ and $a_n=\sqrt{4-a_{n-1}}$ for $n>1$ which gives (also using with monotone convergent theorem) $$\lim a_n=\frac{\sqrt{17}-1}2.$$ But we can write it as $a_1=\sqrt{4-\sqrt{4}}$ and then $a_n=\sqrt{4-\sqrt{4-\sqrt{4-a_{n-1}}}}$. If there is a limit $L$ (I think we can use monotone convergent theorem), I solved with Wolfram that $$L=\sqrt{4-\sqrt{4-\sqrt{4-L}}}$$ and it compute $L=\frac{\sqrt{17}-1}{2}$ again. But I saw kind of continued fraction and gives different answers. (I sware, but not necessary, finding examples are not hard). Is  there a definition of this kind of nested numbers{?!}/limits?","['sequences-and-series', 'limits']"
1738334,Intuition about the second isomorphism theorem,"In group theory we have the second isomorphism theorem which can be stated as follows: Let $G$ be a group and let $S$ be a subgroup of $G$ and $N$ a normal subgroup of $G$ , then: The product $SN$ is a subgroup of $G$ . The intersection $S\cap N$ is a normal subgroup of $G$ . The quotient groups $SN/N$ and $S/(S\cap N)$ are isomorphic. I've seen this theorem some time now and I still can't grasp an intuition for it. I mean, it certainly is one important result, because as I've seen it is highlighted as one of the three isomorphism theorems. The first isomorphism theorem has a much more direct intuition though. We have groups $G$ and $H$ and a homomorphism $f:G\to H$ . If this $f$ is not injective we can quotient out what is stopping it from being injective and lift it to $G/\ker f$ as one isomorphism onto its image. Is there some nice interpretation like that for the second isormorphism theorem? How should we really understand this theorem?","['intuition', 'abstract-algebra', 'group-theory', 'group-isomorphism']"
1738364,Is this functor representable?,"Fix a group $G_0$ and $R$ a subset of $G_0$. Consider the functor $F$ from $\textbf{Grps}$ to $\textbf{Sets}$, sending every object $G$ in $\textbf{Grps}$ to $F(G)$, the subset of $\varphi \in \text{Hom}(G_0, G)$ such that $\varphi(r) = 1$ for every $r \in R$, and sending each homomorphism $f: G \to G'$ to the map $F(f) : \varphi \mapsto f \circ \varphi$. Question. Is this functor representable?","['category-theory', 'homological-algebra', 'group-theory']"
1738392,Diffeomorphism group of product manifold,"For a given differentiable manifold $M$, the diffeomorphism group $\mathrm{Diff}\left( M \right)$ of $M$ is the group of all $C^\infty$ diffeomorphisms of $M$ to itself. Consider a product manifold of the form $M \times N$. My question is: is $\mathrm{Diff}\left( M \times N\right) \cong \mathrm{Diff}\left(M\right) \times \mathrm{Diff}\left( N\right)$? My (physicist's) intuition is no, for consider $\mathbb{R}^2 \cong \mathbb{R} \times \mathbb{R}$. It seems like $\mathrm{Diff}\left(\mathbb{R}\right) \times \mathrm{Diff}\left( \mathbb{R}\right)$ on $\mathbb{R}^2$ can give smooth coordinate transformations along two 'axes', but it can't give 'twists' etc., as could $\mathrm{Diff}\left(\mathbb{R}^2\right)$. Am I right? And, if so, is anyone able to help make my intuition more precise? Thanks for any help!","['manifolds', 'coordinate-systems', 'differential-geometry']"
1738399,"Suppose S = {1,2,3,4}. How many diﬀerent subsets are there of S?","Given: $S=\{1,2,3,4,5,6,7\}$ How many subsets of $S$ are there which have more than one element? I know that there are $2^7=128$ subsets of $S$.  Now, if we take into account the empty set, then shouldn't there be $2^7-8=120$ subsets of $S$ that have more than one element?  I'm trying to find out if my thinking is correct. Thank you in advance!",['discrete-mathematics']
1738400,Solve for $x$ in the $80^\circ$-$80^\circ$-$20^\circ$ triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I've been solving this for days but I still I couldn't solve this. Can I know how to solve $x$ ?",['geometry']
1738414,What is the variance of a variable given itself?,"Given an event $X$, what is $\operatorname{var}[X\mid X]$. In addition, what would $E[\operatorname{var}(X\mid X)]$ be? I have been told that $\operatorname{var}[X\mid X] = 0$, but I don't understand why - $\operatorname{var}[X\mid X]$ should be a random variable, and $X\mid X$ has no more information than $X$, so $\operatorname{var}[X\mid X]$ should equal $\operatorname{var}[X]$, correct?","['statistics', 'variance', 'expectation', 'random-variables']"
1738416,Evaluation of integral of product of three functions,How can I evaluate $\int_0^\infty x^2e^{-kx}\cos (kx)dx$ where $k>0$ ? I tried Laplace transforms and integration by parts. Those methods did not work. Help solicited.,['integration']
1738460,Find the cardinality of the set of permutation matrices.,"Let $S$ be the set of all $3\times3$ matrices $A$ with integer entries such that $AA^t=I$, where $A^t$ is the transpose of $A$. Then $|S|=?$ $12$ $24$ $48$ $60$ I tried to find the structure of such matrices from an arbitrary matrix and using $AA^t=I$ and found that these are permutation matrices. So how to calculate the cardinality of set of permutation matrices? Isn't it $n!$? But that does not give me anything to relate with the options given. Any hint or help please.","['matrices', 'linear-algebra']"
1738466,Sigma algebra question,"Let $Σ_1$ and $Σ_2$ be two sigma-algebras on the same set $X$, such that their union $Σ_1 ∪Σ_2$ is also a sigma-algebra. Prove that for $A,B ⊆ X$ such that $A ∈ Σ_1 \setminus Σ_2$ and $B ∈ Σ_2 \setminus Σ_1$ it always holds that $A∩B \neq\emptyset$. I cant see how it isn't equal to the empty set. $A$ is in sigma $1$ but not in $2$. And $B$ is in $2$ but not in $1$ so they must be disjoint so their intersection must have nothing in common so it must be the empty set. EDIT: $A,B \in \Sigma_1 \cup \Sigma_2=\Sigma$ so $(X \setminus A),(X \setminus B) \in \Sigma$ so $(X \setminus A) \cap (X \setminus B)=\emptyset \in \Sigma$ then $(X \setminus A)^c \cap (X \setminus B)^c=A \cap B=\emptyset ^c=X \in \Sigma$. Is this correct?","['measure-theory', 'elementary-set-theory']"
1738489,"Convergence of series $\sum_{n = 1}^\infty \exp(-n^a)$, when $0 < a < 1$.","Consider the non-negative series $$\sum_{n = 1}^\infty e^{-n^a}, 0 < a < 1.$$ If $a = 0$, the series is divergent, and if $a \geq 1$, by root test, it is convergent. Root test doesn't give information if $0 < a < 1$. I am inclined to believe for $0 < a < 1$, it should also be convergent, but I can't prove it at the moment. Any hint?","['sequences-and-series', 'convergence-divergence']"
1738533,How many integers between $1$ and $1000$ use exactly three digits?,"There is a problem in which I am obtaining a different answer than my professor.  The problem is as follows: How many integers between $1$ and $1000$ use exactly three digits? The professor shows the solution as: $9 \cdot 9 \cdot 8=648$, but I have no idea where those numbers are coming from.  On the other hand, I say that, excluding $1$ to $99$ and $1000$, there are $900$ integers that use exactly three digits.  Can someone please explain which one of us is right and why? Thank you!","['integers', 'discrete-mathematics']"
1738560,Find the sum of $-1^2-2^2+3^2+4^2-5^2-6^2+\cdots$,"Find the sum of $$\sum_{k=1}^{4n}(-1)^{\frac{k(k+1)}{2}}k^2$$ By expanding the given summation, $$\sum_{k=1}^{4n}(-1)^{\frac{k(k+1)}{2}}k^2=-1^2-2^2+3^2+4^2-5^2-6^2+\cdots+(4n-1)^2+(4n)^2$$
$$=(3^2-1^2)+(4^2-2^2)+(7^2-5^2)+(8^2-6^2)+\cdots+[(4n-1)^2-(4n-3)^2]+((4n)^2-(4n-2)^2)$$
$$=2(4)+2(6)+2(12)+2(14)+2(20)+2(22)+\cdots+2(8n-4)+2(8n-2)$$
$$=2[4+6+12+14+20+22+\cdots+(8n-4)+(8n-2)]$$
How should I proceed further?","['algebra-precalculus', 'summation', 'sequences-and-series']"
1738566,Orientability of almost complex manifold,"I have troubles trying to prove  almost complex  two-dimensional manifold is orientable. Let I is complex structure on two-dimensional manifold M. Fix a basic $X_1,IX_1$ in each $T_xM$. Easy to see any two such bases diﬀer by a linear transformation with positive
determinant. To ﬁx an orientation on M we consider the family of all coordinate systems
$x_1,x_2$ of M such that in each coordinate neighborhood, the coordinate
basics $\frac{\partial}{\partial x_1},\frac{\partial}{\partial x_2}$ of $T_xM$ at x diﬀer from the chosen basis $X_1,IX_1$ by a linear
transformation of positive determinant. I think These coordinate systems determine a complete oriented atlas for M but i don't prove it. Here I am stuck. Could somebody show me how to prove it ?","['riemannian-geometry', 'differential-geometry']"
1738573,Eigenvalues of two related symmetric matrices,"What are the eigenvalues of following $n \times n$ matrices $A$ and $B$? $A=\begin{bmatrix}
0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 0 \\ 
0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & 
\vdots & \ddots & \vdots & \vdots & \vdots \\ 
1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\ 
1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\ 
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0
\end{bmatrix} $ Where, $a_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $a_{13}=a_{14}=...=a_{1n-1}=1$, $a_{31}=a_{41}=...=a_{(n-1)1}=1$ $a_{23}=a_{n(n-1)}=1$ As it is symmertic matrix all of its eigenvalues are real. $B=\begin{bmatrix}
0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 1 \\ 
0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\ 
1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & 
\vdots & \ddots & \vdots & \vdots & \vdots \\ 
1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\ 
1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\ 
1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0
\end{bmatrix} $ Where, $b_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $b_{13}=b_{14}=...=b_{1n}=1$, $b_{31}=b_{41}=...=a_{(n-1)1}=1$ $b_{23}=b_{n(n-1)}=b_{n1}=1$ As it is symmertic matrix all of its eigenvalues are real.","['matrices', 'eigenvalues-eigenvectors']"
1738577,What is the domain and range of the sum of two random variables?,"Let $(\Omega, \mathcal{F}, P)$ be  a probability space s.t. $\Omega = \{0,1\}$. Let $X_1: \Omega \rightarrow \{0,1\}$, $X_2: \Omega \rightarrow \{0,1\}$ be two random variables over $\Omega$ (i.e., we're in the context of a Bernouli trial with, say, a coin flip). In fact, we can let $X_1$ and $X_2$ be $id$ in this context. We can suppose further that they are $i.i.d.$ Question: When people write $X_1 + X_2$, what do they mean? In particular, what is the domain and range of $X_1 + X_2$? I'm assuming that its probability distribution forms a convolution of $X_1$ and $X_2$. As I see it, the options for the domain and range are as follows: $X_1 +X_2$ is a function from $\Omega = \{0, 1\}$ to $\{0, 2\}$ s.t. $(X_1 + X_2)(0) = 0 + 0 = 0$ and $X_1 \rightarrow X_2(1) = 1 + 1 = 0$. $X_1 + X_2$ is a function from $\{(0,0), (0,1), (1,0), (1,1)\}$ to $\{0, 1, 2\}$ s.t. $(X_1 + X_2)(i,k) = i + k$. Notice how in (2) the domain of $X_1 + X_2$ is no longer equal to the domain of $X_1$ and $X_2$. So, with those options laid out, is $X_1 + X_2$ (1), (2), or something else entirely?","['probability-theory', 'statistics', 'random-variables']"
1738588,How many one to one functions?,"how many one to one functions for $$p : (4, 5, . . . , 8) \to (4, 5, . . . , 8)$$ if: $p(6) = 6$. What about $p(6) \neq 6$? I thought 1 was 4! and 2 was $4*4!$, but that doesn't seem to be right.","['combinatorics', 'functions']"
1738606,"How many non-negative integer solutions of $x_1 + x_2 + x_3 + x_4 = 28$ are there with $x_{1} \leq 6, x_{2} \leq 10, x_{3} \leq 15, x_{4} \leq21$?","How many non-negative integer solutions of $x_1 + x_2 + x_3 + x_4 = 28$ are there with $x_{1} \leq 6, x_{2} \leq 10, x_{3} \leq 15, x_{4} \leq21$? Attempt: $x_{1} \geq 7$ $x_{2} \geq 11$ $x_{3} \geq 16$ $x_{4} \geq 22$ $N(U)$ = ${28 + 4 - 1 \choose 4-1}$ $N(x_1)$ = ${28-7 + 4 - 1 \choose 4-1}$ $N(x_2)$ = ${28-11 + 4 - 1 \choose 4-1}$ $N(x_3)$ = ${28-16 + 4 - 1 \choose 4-1}$ $N(x_4)$ = ${28-22 + 4 - 1 \choose 4-1}$ $N(x_1\cap x_2)$ = ${28-7-11 + 4 - 1 \choose 4-1}$ $N(x_1\cap x_3)$ = ${28-7-16 + 4 - 1 \choose 4-1}$ $N(x_1\cap x_4)$ = ${28-7-22 + 4 - 1 \choose 4-1}$ $N(x_2\cap x_3)$ = ${28-11-16 + 4 - 1 \choose 4-1}$ $N(x_2\cap x_4)$ = ${28-11-22 + 4 - 1 \choose 4-1}$ $N(x_3\cap x_4)$ = ${28-11-22 + 4 - 1 \choose 4-1}$ Answer = $N(U) - N(x_1) - N(x_2) -N(x_3) -N(x_4)+  N(x_1\cap x_2)+  N(x_1\cap x_3)+  N(x_1\cap x_4)+  N(x_2\cap x_3)+  N(x_2\cap x_4)+  N(x_3\cap x_4)$","['combinatorics', 'discrete-mathematics']"
1738631,"Are $(C[0,1],d_\infty)$ and $(C[0,1],d_1)$ homeomorphic?","Two metric spaces are said to be homeomorphic if there is a bijection f between them such that $f$ and  $f^{-1}$ are both continuous. Consider $C[0,1]$ with metrics: $d_\infty (f,g)=\max_{x\in [0,1]}|f(x)-g(x)|$ $d_1(f,g)=\int_0^1|f(x)-g(x)|dx$ We already know that the identity map $(C[0,1],d_1)→(C[0,1],d_∞)$ is not continuous ( Prove that the identity map $(C[0,1],d_1) \rightarrow (C[0,1],d_\infty)$ is not continuous ). Does this imply $(C[0,1],d_∞)$ and $(C[0,1],d_1)$ are not homeomorphic? Or could you find a bijection which is continuous in both direction? Any help is appreciated.","['general-topology', 'real-analysis', 'metric-spaces', 'functional-analysis']"
1738655,How to solve the differential equation $y'=y(1-y)$.,"Up until now, we simply rearranged and integrated both sides, so $$y'=y(1-y)$$ $$\frac{dy}{dx}=y(1-y)$$ $$\frac{dy}{y(1-y)}=dx$$ $$\int\frac{dy}{y(1-y)}=\int dx$$ With partial fraction decomposition one gets $$\int\frac{dy}{y} +\int\frac{dy}{(1-y)}=\int dx$$ $$\log|y| + C_1 + \log|1-y|+C_2=x+C_3$$ $$\log|y| + \log|1-y|=x+C_4$$ $$|y||1-y|=C_5e^x$$ How do I continue from here? Edit: It should be $$\int\frac{dy}{(1-y)}=-\log|1-y|+C$$","['ordinary-differential-equations', 'absolute-value']"
1738657,Why the intersection of infinite integers is $\{1\}$?,"Intersection of different sets mean that we will get only the elements that exist in each of them. Then why intersection of all $\mathbb{Z}^+$ numbers will yield $\{1\}$? It is clear that $1$ only exists once in the collection, and $2,3,4,\dots$ none of them match $1$, if we say we don't have duplicates. $$
\bigcap_{i=1}^\infty A_i = \{1\}.
$$
$$
\bigcap_{i=1}^\infty \{1,2,3,\ldots,i\} = \{1\}.
$$","['elementary-set-theory', 'discrete-mathematics']"
1738666,"How to calculate this integral with square roots: $\int\frac{ \sqrt{x+1} }{ \sqrt{ x-1 }} \, dx$","How would you calculate this integral: $$\int_{}\frac{ \sqrt{x+1} }{ \sqrt{ x-1 }} \, dx$$","['indefinite-integrals', 'radicals', 'integration', 'calculus']"
1738669,Exercise 16 from chapter 3 of Stein & Shakarchi's complex analysis,"Suppose $f$ and $g$ are holomorphic in a region containing the disc $|z| \le 1$. Suppose that $f$ has a simple zero at $z=0$ and vanishes nowhere else in $|z| \le 1$. Let $f_\epsilon (z) = f(z)+\epsilon g(z)$. Show that if $\epsilon$ is sufficiently small, then (a) $f_\epsilon (z)$ has a unique zero in $|z| \le 1$, and (b) if $z_\epsilon$ is this zero, the mapping $\epsilon \mapsto z_\epsilon$ is continuous. I already solved (a) by applying Rouche's theorem, but (b) is such a nuisance to me. I first tried the classical $\epsilon - \delta$ method, but it didn't work.
However, I couldn't find any other ways to prove the continuity. Since $f$ has a simple zero at $z=0$, I found that $f(z)=zh(z)$ for some $h(z)$ that is holomorphic and non-zero in the unit disc and $z_\epsilon h(z_\epsilon) = -\epsilon g(z_\epsilon)$. Am I on the right track? I don't know how to proceed from this.",['complex-analysis']
1738685,When does a partially defined map from a singular curve to projective space extend?,"This is just something train of thought I became curious about after reading a cryptic suggestion that resolution of singularities lets us transfer our ""understanding"" of smooth curves to singular curves. (Do we understand smooth curves? What is does the author - Ravi Vakil - mean? Anyway, onto the precise math question ...) Suppose that $C$ is a singular, integral affine curve, over some algebraically closed field $k$. Let $f : C \to X$ be some rational map to a projective $k$-scheme (or just $\mathbb{P}^n_k$). Let $C'$ be the normalization of $C$, and $\pi: C' \to C$ the normalizing map. Then the map $f \circ \pi$ extends to a regular map $j: C' \to X$, since $C'$ is regular. I want to know when the map $f : C \to X$ will extend. There are two natural obstructions to the map $f$ extending to a map defined on all of $C'$, which come from the necessity that $j = f \circ \pi$ if $f$ extended. Namely, for points $p,q \in C'$ which have $\pi(p) = \pi(q)$, $j(p) = j(q)$ is necessary. Also, for a point $p \in C'$, and a tangent vector $v \in T_p C'$, if $d\pi(v) = 0$, then necessarily $dj(v) = 0$. I want to know - are these the only conditions? (They come from looking at the normalization of the nodal cubic and the cuspidal cubic respectively. So maybe my intuition about normalizations is missing some crucial example?) If $j$ satisfies the above conditions, then was it a lift of some extension of $f$ from $C$ to $X$? If not, what are the correct conditions? I expect that some result along these lines is well known - I would appreciate very much a reference, or at least a hint of what to try.",['algebraic-geometry']
1738703,Existence of a certain nodal quartic curve,"I am reading this ( https://webusers.imj-prg.fr/~claire.voisin/Articlesweb/Univcodim2invent.pdf ) paper of Voisin, but I am having some trouble with the proof of Sublemma 2.8 (it might be something very simple but I don't exactly see it).
Namely, let $S$ be a $k\leq 7$ nodal quartic surface in $\mathbb{P}^3$ and let $Z$ be the set of nodes. Let $\pi:\tilde{S}\longrightarrow S$ be the blowup of $S$ along $Z$. Then one gets that the ""principalized"" linear series $|\pi^\ast I_Z(4)|$ is nef and big on $\tilde{S}$ (here $I_Z\subset O_S$ is the ideal sheaf of theset $Z$). From here, how can one deduce that there exists an irreducible quartic curve on $S$ which is nodal at $Z$? Any help is appreciated!","['deformation-theory', 'algebraic-geometry']"
1738765,"Let. $X \sim \mathcal{U}(0,1)$. Given $X = x$, let $Y \sim \mathcal{U}(0,x)$. How can I calculate $\mathbb{E}(X|Y = y)$?","Suppose that $X$ is uniformly distributed over $[0,1]$. Now choose $X = x$ and let $Y$ be uniformly distributed over $[0,x]$. Is it possible for us to calculate the ""expected value of $X$ given $Y = y$"", i.e. , $\mathbb{E}(X|Y = y)$? Intuitively, it seems that if $y = 0$, we gain no information, and so $\mathbb{E}(X|Y = 0) = \mathbb{E}(X) = \dfrac{1}{2}$, and also that if $y = 1$, it must be the case that $x = 1$, so $\mathbb{E}(X|Y = 1) = 1$. I don't know if this reasoning is correct, but if it is, then it seems to suggest that $\mathbb{E}(X|Y = y)$ should be a monotonic function of $y$ over $[0,1]$, increasing from $\dfrac{1}{2}$ to $1$. I can also try to do some calculations with probability densities. From the statement of the problem, we have $f_X = 1$ and $f_{Y|X} = \dfrac{1}{x}$. Now, the joint distribution is $f_{XY} = f_X f_{Y|X} = \dfrac{1}{x}$, and as a sanity check we can verify that indeed $\displaystyle\int_0^1 \int_0^x \dfrac{1}{x} \,\mathrm{d}y \,\mathrm{d}x = 1$. Now it seems to me that $\displaystyle f_Y = \int_y^1 f_{XY} \,\mathrm{d}y = -\ln(y)$, and so we can do the following calculation: $$f_{X|Y} = \frac{f_{Y|X} f_X}{f_Y} = -\frac{1}{x \ln y}.$$ Now, I thought that I would be able to perform the following calculation to arrive at my result: $$\mathbb{E}(X|Y = y) = \int_y^1 -\frac{1}{x \ln y} \cdot x \,\mathrm{d}x = \frac{y-1}{\ln y}.$$ Is this right? If not, where have I gone wrong?","['bayesian', 'statistics', 'probability']"
1738815,How to solve this question in more time efficient way?,"Q) if$$x\sin a=y\cos a=\frac{2z\tan a}{1-\tan^2 a}$$ then find $4z^2(x^2+y^2)$ a)$(x^2+y^2)^{3}$ b)$(x^2-y^2)^3$ c)$(x^2-y^2)^2$ d)$(x^2+y^2)^2$ Ans:c i solved this in a very long way: $$x\sin a=y\cos a=\frac{2z\tan a}{1-\tan^2 a}=z\tan 2a$$$$\implies x= \frac{z\tan 2a}{\sin a} , y=\frac{z\tan 2a}{\cos a}$$ $$x^2+y^2=\frac{4z^2\tan ^22a}{\sin ^2 2a}=\frac{4z^2}{\cos^2 2a}$$
$$\implies z^2=\cos^2 2a(x^2+y^2)/4\ldots (1)$$ now$$x\sin a= y\cos a\implies -2x^2\sin^2 a=-2y^2\cos^2a$$
adding $x^2$ both sides
$$x^2(1-2\sin^2 a)= x^2-2y^2\cos^2 a$$$$=x^2-2y^2+2y^2\sin^2a$$ $$=x^2-y^2-y^2+2y^2\sin^2 a$$$$=x^2-y^2-y^2(1-2\sin^2 a)=x^2-y^2-y^2\cos 2a$$$$=(x^2+y^2)\cos 2a= x^2-y^2\implies \cos 2a= \frac{x^2-y^2}{x^2+y^2}\ldots (2)$$ hence from (1) and (2) $$4z^2(x^2+y^2)= \left( \frac{x^2-y^2}{x^2+y^2} \right)^2 (x^2+y^2)^2= (x^2-y^2)^2$$ now you can see that i got the answer but there was a log way to find this. This question has been asked in an exam so there is noway its solution could be soo long there got to be some shorter way. So how could i solve this question is more time efficient way?","['problem-solving', 'trigonometry']"
1738827,Different number field discriminants in Sage and Magma,"I have been working with towers of number fields. My problem is that when computing the absolute discriminant of such number fields, Sage and Magma give different values. You can see this by doing in Sage di = [2,3,5,7]

K.< a > = NumberField([x^2 -p for p in di])

L = K.absolute_field('b')

L.discriminant() which gives $63456228123711897600000000$ and in Magma: Zx< x > := PolynomialRing(Integers());

K := NumberField(x^16 - 136*x^14 + 6476*x^12 - 141912*x^10 + 1513334*x^8 - 7453176*x^6 + 13950764*x^4 - 5596840*x^2 + 46225);

Discriminant( K ); which gives 5599292204088795725124575470812804054972446039778078505738080537221656920845173082195169325791965321100902635929600000000000000000000000000. What different types of discriminants are Sage and Magma computing? Is one of them wrong?","['sagemath', 'discriminant', 'magma-cas', 'number-theory', 'extension-field']"
1738846,How do i show the nth iteration term given a function and a starting point?,"My task is this; The function $f:\mathbb{R} \to \mathbb{R}$ is given by $f(x) = \lambda x + k$ where $\lambda, k \in \mathbb{R}$ and $\lambda \neq 1$. Show that when we are iterating $f$ with startingpoint $x_0$, then: $$x_n = \lambda^n \left(x_0 -\frac{k}{1 - \lambda}\right) + \frac{k}{1 - \lambda}.$$ The first thoughts that comes to my mind is to write out some iterations:
$x_1 = f(x_0) = \lambda x_0 + k,\: x_2 = f(x_1) = f(f(x_0)) = \lambda(\lambda x_0 + k) + k = \lambda^2 x_0 + \lambda k + k, \: x_3 = f(x_2) = f(f(f(x_0))) = \lambda(\lambda^2x_0 + \lambda k + k) + k = \lambda^3x_0 + \lambda^2k + \lambda k + k$. So it seems like for $n$ iterations, we get $\lambda^nx_0 + \sum\limits_{i = 1}^{n}\lambda^{i - 1} k = \lambda^n x_0 + \frac{k}{1 - \lambda}$. I think the last step is right, but now i just can't see the path that connects my thoughts to the given expression. Any help would be appriciated. Thanks in advance!","['multivariable-calculus', 'fixed-point-iteration']"
1738848,Find the number of solutions of the equation $\sin^22x-\cos^28x=\frac{1}{2}\cos10x$,"Find the number of solutions of the equation $\sin^22x-\cos^28x=\frac{1}{2}\cos10x$ lying in the interval $(0,\frac{\pi}{2})$ I found the period of $\sin^22x-\cos^28x$ as $\pi$ and the period of $\frac{1}{2}\cos10x$ is $\frac{\pi}{5}$ I do not know how to solve it further.",['trigonometry']
1738870,Differentiability of an action of the group of invertible elements of a $C^{*}$-algebra $\mathcal{A}$ on the dual of $\mathcal{A}$,"I am studying the actions of Banach-Lie groups on Banach manifolds, and I am not able to concretely evaluate the differentiability properties of a specific action. Let $\mathcal{A}$ be a unital $C^{*}$-algebra, and $\mathcal{A}^{*}$ its topological dual, which is a complex Banach space with norm: $$
\left|\left|\omega\right|\right|:=\sup\left\{|\omega(\mathbf{A})|\,;\mathbf{A}\in\mathcal{A}\colon||\mathbf{A}||=1\right\}
$$
Let us denote with $\mathcal{G}(\mathcal{A})$ the set of all invertible elements in $\mathcal{A}$.
This set is a Banach-Lie group with respect to the multiplication operation inherited from $\mathcal{A}$.
This group naturally acts on $\mathcal{A}$ as follows: $$
\mathbf{A}\mapsto c_{\mathbf{G}}(\mathbf{A}):=\mathbf{G}\,\mathbf{A}\,\mathbf{G}^{\dagger}\,.
$$
By means of this action, an action on $\mathcal{A}^{*}$ is obtained: $$
\omega\mapsto \alpha_{\mathbf{G}}(\omega)
$$
$$
\left(\alpha_{\mathbf{G}}(\omega)\right)(\mathbf{A}):=\omega\left(c_{\mathbf{G}}(\mathbf{A})\right)=\omega\left(\mathbf{G}\,\mathbf{A}\,\mathbf{G}^{\dagger}\right)\,.
$$
It is easy to see that this action is linear and preserves hermiticity. I am interested in the differentiability properties of this action.
By definition, the action is differentiable if the map: $$
\mathcal{G}(\mathcal{A})\times\mathcal{A}^{*}\rightarrow\mathcal{A}^{*}
$$ 
$$
(\mathbf{G}\,;\omega)\mapsto \alpha_{G}\omega
$$
is differentiable.
I know the abstract definition of differentiability (Frechet derivative) of a map between Banach manifolds, however, I do not know how to concretely proceed to investigate the subject.
Specifically, a map $f:V\rightarrow W$ between Banach space is differentiable at $x_{0}\in V$ if, for all $\epsilon>0$ there exist $\delta_{\epsilon}>0$ and a bounded linear map $Df_{x_{0}}:V\rightarrow W$ such that: $$
||f(x-x_{0}) - f(x_{0}) - Df_{x_{0}}(x-x_{0})||<\epsilon
$$
whenever $||x-x_{0}||<\delta_{\epsilon}$. Since $\mathcal{G}(\mathcal{A})$ is only a Banach manifold, I need a coordinates system in order to use the definition of Frechet derivative, and I do not know what kind of coordinates system I should use.
Furthermore, even if I knew it, I would need an ""educated guess"" of the explicit face of the derivative $Df_{x_{0}}$ to actually calculate the limit in the definition of the Frechet derivative. Are there other, more smart ways to proceed? Thank You in advance. EDIT With a little effort, I was able to prove that the linear map $\alpha_{\mathbf{G}}:\mathcal{A}^{*}\rightarrow\mathcal{A}^{*}$ is bounded, and thus continuous, for all $\mathbf{G}\in\mathcal{G}(\mathcal{A})$.
This means that $\alpha_{\mathbf{G}}$ is analytic for all $\mathbf{G}$ (it is a continuous 1-homogeneous polynomial), but I do not know if it is of some relevance. EDIT 2 According to Andreas Cap's answer, I tried to work out the details without using the $t$ parameter. Let $U_{\mathbf{G}}\subset\mathcal{G}(\mathcal{A})$ be a neighbourhood of $\mathbf{G}$ such that $\mathbf{G}+\mathbf{H}\in U$ for all $\mathbf{H}\in U$.
Then we can write: $$
\alpha(\mathbf{G}+\mathbf{H}\,;\omega + \tau)(\mathbf{A}) - \alpha(\mathbf{G}\,;\omega)(\mathbf{A})=\omega(\mathbf{G}\mathbf{A}\mathbf{H}^{\dagger} + \mathbf{H}\mathbf{A}\mathbf{G}^{\dagger}) + \omega(\mathbf{H}\mathbf{A}\mathbf{H}^{\dagger}) + \tau(\mathbf{H}\mathbf{A}\mathbf{H}^{\dagger}) + \tau(\mathbf{G}\mathbf{A}\mathbf{G}^{\dagger})
$$
The linear functional introduced by Andreas Cap is: $$
D\alpha_{\mathbf{G}\,;\omega}(\mathbf{H}\,;\tau)(\mathbf{A})=\omega(\mathbf{G}\mathbf{A}\mathbf{H}^{\dagger} + \mathbf{H}\mathbf{A}\mathbf{G}^{\dagger}) + \tau(\mathbf{G}\mathbf{A}\mathbf{G}^{\dagger})
$$
and thus: $$
\alpha(\mathbf{G}+\mathbf{H}\,;\omega + \tau)(\mathbf{A}) - \alpha(\mathbf{G}\,;\omega)(\mathbf{A}) - D\alpha_{\mathbf{G}\,;\omega}(\mathbf{H}\,;\tau)(\mathbf{A})= \tau(\mathbf{G}\mathbf{A}\mathbf{H}^{\dagger} + \mathbf{H}\mathbf{A}\mathbf{G}^{\dagger})+ (\omega+\tau)(\mathbf{H}\mathbf{A}\mathbf{H}^{\dagger})
$$
Let us call this functional $\gamma$.
The norm of $\gamma$ is: $$
||\gamma||=\sup_{\mathbf{A}\neq\mathbf{0}}\frac{|\gamma(\mathbf{A})|}{||\mathbf{A}||}
$$
Then: $$
|\gamma(\mathbf{A})|\leq|\tau(\mathbf{G}\mathbf{A}\mathbf{H}^{\dagger} + \mathbf{H}\mathbf{A}\mathbf{G}^{\dagger})| + |(\omega+\tau)(\mathbf{H}\mathbf{A}\mathbf{H}^{\dagger})|\leq
$$
$$
\leq2||\tau||\,||\mathbf{G}||\,||\mathbf{A}||\,||\mathbf{H}|| + ||\omega||\,||\mathbf{H}||^{2}\,||\mathbf{A}|| + ||\tau||\,||\mathbf{H}||^{2} \,||\mathbf{A}||
$$
which means: $$
||\gamma||=2||\tau||\,||\mathbf{G}||\,||\mathbf{H}|| + ||\omega||\,||\mathbf{H}||^{2} + ||\tau||\,||\mathbf{H}||^{2}
$$
The action $\alpha$ is differentiable at $(\mathbf{G}\,;\omega)$ if, for all $\epsilon>0$ there exists $\delta_{\epsilon}>0$ such that: $$
\frac{||\gamma||}{||(\mathbf{H}\,;\tau)||}<\epsilon
$$
whenever $||(\mathbf{H}\,;\tau)||<\delta_{\epsilon}$.
However, we have: $$
\frac{||\gamma||}{||(\mathbf{H}\,;\tau)||}=\frac{2||\tau||\,||\mathbf{G}||\,||\mathbf{H}|| + ||\omega||\,||\mathbf{H}||^{2} + ||\tau||\,||\mathbf{H}||^{2}}{||\mathbf{H}|| +||\tau||}
$$
and I am not sure that this is less than $\epsilon$.","['functional-analysis', 'banach-spaces', 'differential-geometry', 'lie-groups']"
1738871,"If the quadratic equation $x^2+(2-\tan\theta)x-(1+\tan\theta)=0$ has two integral roots,","If the quadratic equation $x^2+(2-\tan\theta)x-(1+\tan\theta)=0$ has two integral roots,then find the sum of all possible values of $\theta$ in interval $(0,2\pi).$ The given quadratic equation is $x^2+(2-\tan\theta)x-(1+\tan\theta)=0$. So $x=\frac{\tan\theta-2\pm\sqrt{\tan^2\theta+8}}{2}$ I am stuck here.I do not know what is the condition for two integer roots.","['trigonometry', 'quadratics']"
1738911,Why are we interested in cohomology?,"I've been studying algebraic topology for over half a year now and came across alot of different topics of it (fundamental groups, Van Kampen, singular homology, homology theory, Mayer Vietoris, universal coefficient theorem, knot theory etc.) and recently we started to study cohomolgy in our lecture. We defined cohomology, proved the universal coefficient theorem and with that we were able to prove quite alot of analogous results which we already proved for homology. My questions: Why do we want to study cohomology? Are there some advantages of computing the cohomology of a given topological space compared to computing its homology group? Are there some really suprising/fascinating results which heavily depend on cohomological properties/theorems?","['homology-cohomology', 'homological-algebra', 'soft-question', 'algebraic-topology', 'general-topology']"
1739052,"If X is independent to Y and Z, does it imply that X is independent to YZ ?","After years of mathematics, I am struggling with this simple question. If we have 3 r.v. $X,Y,Z$ and we have $X$ independent to $Y$ and to $Z$, then do we have that $X$ is also independent to $YZ$ ? At first sight, I thought that if $X$ is independent to $Y$ and $Z$, it is also independent to the sigma-algebra generated by $Y$ and to $Z$ and hence $YZ$ but the example below made me confused : https://en.wikipedia.org/wiki/Pairwise_independence If someone can make this clear...
Thank you very much in advance !","['independence', 'probability-theory', 'probability']"
1739056,Characterizing orthogonally-invariant norms on the space of matrices,"Denote by $M_n$ the space of $n \times n$ real matrices. We say a norm on $M_n$ is orthogonal invariant if: $$\|OX \|=\| XO\|=\|X \|  \, \, \forall O \in O_n,X \in M_n$$ I am trying to characterize all such norms. Let $\|\cdot \|$ be one.
Using singular values decomposition, we get: $\|X\|=\|U\Sigma V^T\|=\|\Sigma \|$ (when $U,V \in O_n, \Sigma$ is a diagonal with non-negative entries, i.e $U \Sigma V^T$ is a SVD of $X$) So, if we define $f:\mathbb{R}^n \to \mathbb{R}^{\ge 0}$ by $f(\sigma_1,\dots,\sigma_n)=\|\operatorname{diag}(\sigma_1,\dots,\sigma_n) \|$ we get that $\|\cdot\|$ is uniquely determined by $f$. Note that $f$ must be a norm which is invariant under signed permutations, i.e: $f(\pm x_{\tau(1)},\dots,\pm x_{\tau(n)})=f(x_1,\dots,x_n)$ for every $\tau \in S_n$. (The reason is that $\| \cdot\|$ is invariant under orthogonal multiplication, in particular by signed permutation matrices). Question: Is it true that any norm on $\mathbb{R}^n$ which is invariant under signed permutations induces an orhtogonal-invariant norm on $M_n$? (in the obvious way as described above). If not, can we characterize which norms are possible? Added Clarification: Let $f$ be a norm on $\mathbb{R}^n$. The possible candidate for a norm on $M_n$ induced by $f$ is: $\| X\|=\|U\Sigma V^T \|=f(\sigma_1,\dots,\sigma_n)$ where $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n) $. The non trivial part seems to be verifying the triangle inequality, since SVD does not behave in a structured way (known to me) w.r.t sums. Remarks and partial results: 1) Choosing $f$ to be the maximumn norm induces the Euclidean operator norm. (see here ). 2) Choosing $f$ to be the standard $p$-norm, one gets the $p-$ Schatten norm . 3) Here is a sufficient condition (which is not necessary) inducing a norm:
 $$\sum_{i=1}^n z_i \le \sum_{i=1}^n x_i + \sum_{i=1}^n y_i \Rightarrow f(z_1,\dots,z_n) \le f(x_1,\dots,x_n) + f(y_1,\dots,y_n)$$ Any $f$ which satisfies the above condition induces a norm. This follows from Lidskii inequality which says: $$\sum_{i=1}^n \sigma_i(A+B) \le \sum_{i=1}^n \sigma_i(A) + \sum_{i=1}^n \sigma_i(B) $$ Note this condition is not necessary: The maximum norm does not satisfy it, but induces a norm.","['matrices', 'normed-spaces', 'symmetry', 'linear-algebra']"
1739058,Is $\mathbb{R^2}$ Hausdorff? Give an example of a non-Hausdorff topology on $\mathbb{R}$,"these are two questions on Hausdorff topological spaces. The bit I am having particular difficulty with is finding an 'example of a non-Hausdorff topology on $\mathbb{R}$' A Hausdorff topological space $(X, \tau)$ is such that any distinct points $a, v \in X$ have disjoint open neighbourhoods. i.e. there are open neighbourhoods $U_a, V_b \in \tau$ such that $ a \in U_a  $ and $b \in V_b $ and $U_a \cap V_b = \emptyset$ Is $\mathbb{R^2}$ Hausdorff? I believe so. Take $a, b \in \mathbb{R^2}$. Take open neighbourhoods: $U_a=B_{r_a}(a)=\{(x, y) : |(x, y)-a|<r_a\}$ $V_b=B_{r_b}(b)=\{(x, y) : |(x, y)-b|<r_b\}$ Let $r=d(a,b)$. Take $r_a=r_b=\frac{r}{2}$ So $\mathbb{R^2}$ is Hausdorff. Is this correct? Give an example of a non-Hausdorff topology on the set of real numbers $\mathbb{R}$ is clearly Hausdorff. What is another example of a topology on $\mathbb{R}$ ? Please could you help me with this one?","['algebraic-topology', 'general-topology', 'real-numbers', 'elementary-set-theory']"
1739077,Does $f(x) = |x|$ have a local minimum?,"I would like to know whether $f(x) = |x|$ has a local minimum at $x = 0$. 
For a local minimum, the first derivative of the function has to be zero. However, in the case of $|x|$, the function is not differentiable at $x = 0$. Will the point (0,0) still be considered as a local minimum?","['derivatives', 'graphing-functions']"
1739088,"Difference between ""scalar line integral"" and ""line integral""","What is the difference between the phrases ""scalar line integral"" and ""line integral""? If the phrases are equivalent, what purpose does the adjective ""scalar"" serve in the phrase; why is it there?","['multivariable-calculus', 'integration', 'curves']"
1739149,How to solve this integration that I got from differential linear equation? [duplicate],This question already has answers here : Integral of $\frac{1}{(1+x^2)^2}$ (10 answers) Closed 3 years ago . I worked differential linear equation and at end of equation I got this integral.Can someone give me a hint to do this: $$\int \frac{1}{\left(u^2+1\right)^2}du$$,"['indefinite-integrals', 'ordinary-differential-equations']"
1739160,"How to construct a $4 \times 4$ symmetric, positive definite matrix with integer eigenvalues","As part of my master thesis I'm trying to construct (or find) some $4 \times 4$ symmetric, positive (semi-)definite matrices with integer components, and integer eigenvalues. The reason for the integer conditions is purely aesthetical, since typesetting the matrix and many analytical calculations look nicer with integer scalars. I'm aware of answers such as https://math.stackexchange.com/a/1377275/245055 , but the problem is that this does not produce a symmetric matrix. Any guidance will be greatly appreciated, as I would very much prefer not having to search for this by brute force or via code (which might potentially produce false positives due to numerical precision issues).","['eigenvalues-eigenvectors', 'matrices', 'positive-definite', 'integers', 'linear-algebra']"
1739162,Finding the Dimension of a given space $V$,"I am unsure how to solve this problem: If $\vec{v}$ is any nonzero vector in $\mathbb{R}^2$, what is the
  dimension of the space $V$ of all $2 \times 2$ matrices for which
  $\vec{v}$ is an eigenvector? What I have so far is: $$ \left[
  \begin{array}{ c c }
     a & b \\
     c & d
  \end{array} \right]
  \left[
  \begin{array}{ c }
     v_{1}  \\
     v_{2} 
  \end{array} \right] =  \left[
  \begin{array}{ c }
     \lambda v_{1} \\
     \lambda v_{2}
  \end{array} \right]  $$ And solving for this I get two equations with four unkowns. $$ av_{1} + bv_{2} = \lambda v_{1}$$ $$cv_{1} + dv_{2} = \lambda v_{2} $$ I am not sure where to go from here. At first I solved for $a$ and $c$ in terms of $b$, $v_1$ and $v_2$ and $d$, $v_1$, and $v_2$ respectively and got dim = 2, but the answer is dim = 3. Any hints why this is?","['eigenvalues-eigenvectors', 'linear-algebra', 'vector-spaces']"
1739245,Literature on transformed Gaussian matrices,"I am considering real $n$-by-$m$ matrices of the following type: $$
M=SM^\prime,\\
M^\prime_{ij} \overset{\text{iid}} \sim N(0,1).
$$ Here, $S$ is a fixed $n$-by-$n$ matrix and the entries of $M^\prime$ (same size as $M$) are just i.i.d Gaussian. It is important that the considered matrices are rectangular and not simply square. $S$ can be identity but, ideally, should be an arbitrary full-rank matrix. As far as I know, in the special case of $S=I, n=m$ this construction is called the real Ginibre ensemble . Can anyone suggest some literature/references for the more general case? I'm particularly interested in spectral properties of these matrices such as singular value/vector distributions.","['reference-request', 'probability-theory', 'random-matrices', 'statistics']"
1739251,"Find $y \in W_{2}^{1}[-1,1]$ s.t. $\forall x \in W_{2}^{1}[-1,1]$, $f(x)=\langle x, y \rangle$","Consider a Sobolev space $W_{2}^{1}[-1,1]$ with the following inner product: $\langle x, y \rangle = \int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt$. Let $f(x) = \int_{-1}^{1}e^{2t}x(t)dt$. I need to find $y \in$ the Sobolev space $W_{2}^{1}[-1,1]$ such that $\forall x \in W_{2}^{1}[-1,1]$, $f(x)=\langle x, y \rangle$. However, I don't even know how to begin. I know that I need $\int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt = \int_{-1}^{1}e^{2t}x(t)dt$, but that's about where what looks familiar to me ends... I've since been informed that what I need to do is find a weak solution of $-y^{\prime\prime} + y = e^{2t}$; however, I have no idea how one goes from the problem I have here to this, much simpler-looking, ODE. A detailed explanation as to where this comes from is needed. Also, I don't know what boundary conditions I'm supposed to use. I am not doing this for a Differential Equations course; I am doing it in a Functional Analysis course, where my exposure to differential equations in the past has been extremely limited (as in, haven't had a class in ODEs since 2003, and even then, it was very basic). Please explain this to me. Then, after I have my solution to the homogeneous form of the 2nd order linear ODE, I should be able to figure out the particular solution (I hope) using either variation of parameters or undetermined coefficients.","['functional-analysis', 'inner-products', 'ordinary-differential-equations', 'sobolev-spaces', 'analysis']"
1739284,Find expected value of coloured segment,"Suppose picking $n$ random points from $[0,1]$ interval. For each point we colour segment of least length to it's neighboring point. If length to both neighboring points is the same then we colour to the left neighbour. Find expeceted value of length coloured sements.","['multivariable-calculus', 'probability']"
1739313,Flipping 100 coins and getting all heads.,"I hold 100 fair coins in my hands. Then, I toss the 100 coins high into the air all at once. I understand that the chance of getting 100 heads when they land is 1 in 2 exp 100. Can someone help me determine, on average, how many attempts at tossing 100 coins all at once need to be made to obtain 100 heads.  (I believe this might be the ""expected"" number of tosses.) I'm asking because my intuition says that if the 100 coin toss was made once per second then the universe is not old enough to expect a toss with 100 heads as an outcome and I'd like to confirm this. (The universe is roughly 13.82 billion years or approximately 4.35 exp 17 seconds old.)  Thank you.",['probability']
1739358,Nested root integral $\int_0^1 \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}$,"The bigger goal is to find the antiderivative: $$\int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}~~~~~(*)$$ But I can settle for the definite integral in $(0,1)$. Motivation: $$\int \frac{dx}{\sqrt{x+\sqrt{x}}}=2\sqrt{x+\sqrt{x}}-\ln (1+2\sqrt{x}+2\sqrt{x+\sqrt{x}})+C$$ This integral is easy to solve by using the following substitution: $$x=u^4$$ $$\sqrt{x+\sqrt{x}}=u\sqrt{1+u^2}$$ Now consider the integral $(*)$. If we take $x=u^8$, we get the integral: $$(*)=\int \frac{8u^6du}{\sqrt{u^6+\sqrt{1+u^4}}}$$ Still seems bad, and Mathematica can't solve it (or the definite integral either). Another way I tried is by the following substitutions: $$\sqrt{x+\sqrt{x}}=\frac{y}{2}$$ $$(*)=\int \frac{y(\sqrt{1+y^2}-1)dy}{\sqrt{1+y^2} \sqrt{2+2y+y^2-2\sqrt{1+y^2}}}$$ $$y=\sinh t$$ $$(*)=\int \frac{\sinh t(\cosh t-1)dt}{\sqrt{(\cosh t-1)^2+2\sinh t}}$$ Believe it or not, Mathematica actually solves this integral, but the resulting expression is so long and complicated, it seems useless (and by long I mean three times the size of my screen). What do you think, is there a reasonable closed form solution for this integral? Or at least, the definite integral: $$\int_0^1 \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}=\int_0^{\sinh^{-1} \sqrt{8}} \frac{\sinh t(\cosh t-1)dt}{\sqrt{(\cosh t-1)^2+2\sinh t}}$$ Edit: $$\int \frac{\sinh t(\cosh t-1)dt}{\sqrt{(\cosh t-1)^2+2\sinh t}}=\sqrt{(\cosh t-1)^2+2\sinh t}-\int \frac{\cosh t~dt}{\sqrt{(\cosh t-1)^2+2\sinh t}}$$ Now let's make another substitution: $$e^t=v$$ $$\int \frac{\cosh t~dt}{\sqrt{(\cosh t-1)^2+2\sinh t}}=\int \frac{(v^2+1)~dv}{v\sqrt{v-1}\sqrt{v^3+v^2+7v-1}}$$ Now I see the connection to elliptic integrals (which Mathematica gives as part of the answer). We just probably need to factor: $$v^3+v^2+7v-1$$ The limits $x \in (0,1)$ will become $v \in (1,3+\sqrt{8})$. We can also make another change of variable, leaving only one radical and getting somewhat better behaved function (finite everywhere on the real line): $$z=\sqrt{v-1}$$ $$\int \frac{(v^2+1)~dv}{v\sqrt{v-1}\sqrt{v^3+v^2+7v-1}}=2\int \frac{(z^4+2z^2+2)~dz}{(z^2+1)\sqrt{z^6+4z^4+12z^2+8}}=$$ $$=2\int \frac{dz}{(z^2+1)\sqrt{z^6+4z^4+12z^2+8}}+2\int \frac{(z^2+1)~dz}{\sqrt{z^6+4z^4+12z^2+8}}$$","['nested-radicals', 'integration', 'definite-integrals']"
1739363,All the roots of $\lambda x+\cot(x)=0$ are real. Looking for an alternative proof,"I am looking for an alternative (possibly simpler) proof of the following fact, that has some relevance in finding the eigenfunctions for the laplacian operator . For any $\lambda\in\mathbb{R}^+$, $\lambda\geq 1$, all the solutions of 
  $$  \cot(x) + \lambda x = 0$$
  belong to $\mathbb{R}$. My proof goes as follows: we just have to prove that all the roots of $f(x)=\cot(x)+\lambda x$ are real. If we assume the opposite, then $f'(x)=\lambda-\frac{1}{\sin^2(x)}$ must have some complex root by the Gauss-Lucas theorem . That is the same as stating that for some $z\in\mathbb{C}\setminus\mathbb{R}$ we have $\sin(z)=\pm\frac{1}{\sqrt{\lambda}}$, or, by setting $z=\sigma+it$,
$$ e^{i\sigma-t}-e^{-i\sigma+t} = \pm\frac{2i}{\sqrt{\lambda}}. $$
However, if we set $w=e^{iz}$, the equation $w-\frac{1}{w}=\frac{2i}{\sqrt{\lambda}}$ is solved only by 
$$ w = \frac{i\pm\sqrt{\lambda-1}}{\sqrt{\lambda}} $$
that is a number on the unit circle. That leads to $z\in\mathbb{R}$, i.e. $t=0$. Extra question. What can we say about the distribution of the roots if $\lambda\in\mathbb{R}$ but $\lambda < 1$?",['complex-analysis']
1739420,Find recurrence relation,Find recurrence relation for strings of length n using 7 letter alphabet. Each character in the string is the same as previous one or the following one. The start is easy - first two characters in the sequence will have to be the same. If third character is not the same then we basically have $a_{n-2}$ on our hands. I have a problem conceptualizing what happens if the third character is the same as previous two. Looking at first few numbers of sequence isn't insightful for me either $a_0 = 1$ $a_1=0$ $a_2=7$ $a_3=7$ $a_4=7^2$ $a_5=2*7^2$ $a_6=7^3+7^2-7$,"['combinatorics', 'discrete-mathematics']"
1739431,conjectured general continued fraction for the quotient of gamma functions,"Given complex numbers $a=x+iy$, $b=m+in$ and a gamma function $\Gamma(z)$ with $x\gt0$ and $m\gt0$, it is conjectured that the following general continued fraction which is symmetric on $a$ and $b$ is true $$\frac{\displaystyle\Gamma\left(\frac{a+3b}{4(a+b)}\right)\Gamma\left(\frac{3a+b}{4(a+b)}\right)}{\displaystyle\Gamma\left(\frac{3a+5b}{4(a+b)}\right)\Gamma\left(\frac{5a+3b}{4(a+b)}\right)}=\cfrac{4(a+b)}{a+b+\cfrac{(2a)(2b)} {3(a+b)+\cfrac{(3a+b)(a+3b)}{5(a+b)+\cfrac{(4a+2b)(2a+4b)}{7(a+b)+\ddots}}}}\tag{1}$$ And can be further generalised to $$\frac{1}{\sqrt{c}}\tan\left(\frac{b-a}{b+a}\tan^{-1}\left(\frac{\sqrt{c}}{d}\right)\right)=\cfrac{(b-a)}{d(a+b)+\cfrac{c(2a)(2b)} {3d(a+b)+\cfrac{c(3a+b)(a+3b)}{5d(a+b)+\cfrac{c(4a+2b)(2a+4b)}{7d(a+b)+\ddots}}}}\tag{2}$$ Corollaries : (i) Specializing to $a=1/2$ and $b=2z/n+1/2$,we obtain
the continued fraction for $\tan\left(\frac{z\pi}{4z+2n}\right)$ found in this post after applying the functional equation of the gamma function (ii) and specializing further to $a=-1/2$ and $b=2z/n+3/2$,we obtain immediately the continued fraction for $\cot\left(\frac{z\pi}{4z+2n}\right)$ found here after applying the functional equation of the gamma function.This continued fraction was proved by @GEdgar. (iii) letting $2a=m-n$ and $2b=m+n$,we find $$\displaystyle\tan\left(\frac{\pi n}{4m}\right)=\cfrac{n}{m+\cfrac{m^2-n^2} {3m+\cfrac{4m^2-n^2}{5m+\cfrac{9m^2-n^2}{7m+\cfrac{16m^2-n^2}{9m+\ddots}}}}}$$ From which we obtain its hyperbolic companion $$\displaystyle\tanh\left(\frac{\pi n}{4m}\right)=\cfrac{n}{m+\cfrac{m^2+n^2} {3m+\cfrac{4m^2+n^2}{5m+\cfrac{9m^2+n^2}{7m+\cfrac{16m^2+n^2}{9m+\ddots}}}}}$$ (iv)and if $2a=-n$ and $2b=2m+n$,then it follows that $$\displaystyle\tan\left(\frac{\pi(m+n)}{4m}\right)=\frac{1+\tan\Big(\frac{\pi n}{4m}\Big)}{1-\tan\Big(\frac{\pi n}{4m}\Big)}=\cfrac{m+n}{m+\cfrac{(-n)(2m+n)} {3m+\cfrac{(m-n)(3m+n)}{5m+\cfrac{(2m-n)(4m+n)}{7m+\cfrac{(3m-n)(5m+n)}{9m+\ddots}}}}}$$ Q : Is the conjectured general continued fraction true (for all complex numbers $a$,$b$ with $x\gt0$ and $m\gt0$)?","['number-theory', 'conjectures', 'continued-fractions', 'gamma-function']"
1739433,Prove $\frac{2\sec\theta +3\tan\theta+5\sin\theta-7\cos\theta+5}{2\tan\theta +3\sec\theta+5\cos\theta+7\sin\theta+8}=\frac{1-\cos\theta}{\sin\theta}$,Prove that $\frac{2\sec\theta +3\tan\theta+5\sin\theta-7\cos\theta+5}{2\tan\theta +3\sec\theta+5\cos\theta+7\sin\theta+8}=\frac{1-\cos\theta}{\sin\theta}$ $$\frac{2\sec\theta +3\tan\theta+5\sin\theta-7\cos\theta+5}{2\tan\theta +3\sec\theta+5\cos\theta+7\sin\theta+8}=\frac{\frac{2}{\cos\theta} +\frac{3\sin\theta}{\cos\theta}+5\sin\theta-7\cos\theta+5}{\frac{2\sin\theta}{\cos\theta} +\frac{3}{\cos\theta}+5\cos\theta+7\sin\theta+8}$$ $$=\frac{2+3\sin\theta+5\sin\theta\cos\theta-7\cos^2\theta+5\cos\theta}{2\sin\theta+3+5\cos^2\theta+7\sin\theta\cos\theta+8\cos\theta}$$ I am stuck here.I tried to factorize numerator and denominator but does not succeed.,['trigonometry']
1739461,"Finding the bound of a linear functional defined on $C[-1,1]$","Define the linear functional: $$f(x)=\int_{-1}^{0}x(t)dt-\int_0^1x(t)dt$$ On the normed space $C[-1,1]$ which consists of all contiuous functions on the interval. The norm is defined as: $\|x\|= \max_{t\in[-1,1]}|x(t)|$ Now, let $x(t)\in C[-1,1]$ be arbitrary $$\left| f(x)\right|=\left| \int_{-1}^{0}x(t)dt-\int_0^1x(t)dt\right|$$ $$|f(x)|\leq \left|\int_{-1}^{0}x(t)dt\right|+\left|\int_0^1x(t)dt\right|$$ Also, $$\left|\int_{-1}^{0}x(t)dt\right|
    \leq (0-(-1))\cdot \max_{\ t\in[-1,1]}|x(t)|=\|x\|$$ In a similar way for the other integral, one can conclude: $$|f(x)|\leq 2\|x\|\implies \frac{|f(x)|}{\|x\|}\leq 2$$ Excluding the case where $x(t)$ is the zero function. Define $\|f\|$: 
$$\|f\|=\sup_{x\in C[-1,1]\setminus\{0\}} \left( \frac{|f(x)|}{\|x\|}\right) $$ So $\|f\|\leq 2 $ since $x(t)$ was arbitrarily chosen and we've established an upper bound. But we can't conclude $\|f\| = 2 $ from this since it need not be the least upper bound. It would be sufficient at this point to show $\frac{|f(x)|}{\|x\|}\geq 2$ for some specific $x(t)\in C[-1,1]$. I'm having no luck finding this $x(t)$. What's clear is that it needs to be some anti-symmetric function on the interval that attains a maximum value of $1$. Any suggestions would be appreciated.","['functional-analysis', 'operator-theory']"
1739512,How many points to prove a trigonometric identity?,"I am taking a trigonometric identity from another post, arbitrarily. $$\frac{2\sec\theta +3\tan\theta+5\sin\theta-7\cos\theta+5}{2\tan\theta +3\sec\theta+5\cos\theta+7\sin\theta+8}=\frac{1-\cos\theta}{\sin\theta}.$$ Besides the usual approach by reworking/simplifying the expressions using elementary identities, one could use a ""lazy"" approach, by evaluating both members for several $\theta$ and checking equality. This works for polynomials, if you probe them at $d+1$ points, where $d$ is the degree. Can we derive general rules about the number of equalities required to guarantee that trigonometric expressions of a certain complexity are indeed identical ?",['trigonometry']
1739520,Give a bijection,"Give a bijection between the following sets: (1) $\mathbb{Z}$ and $ \mathbb{Z}\backslash \{0\}$ (2) $\mathbb{Q}$ and $ \mathbb{Q}\backslash \{0\}$ I think that I can the problems with ""Hilbert's paradox of the Grand Hotel"". I think that the first (1) problem is pretty the same like the following:Suppose a new guest arrives and wishes to be accommodated in the hotel. We can (simultaneously) move the guest currently in room 1 to room 2, the guest currently in room 2 to room 3, and so on, moving every guest from his current room n to room n+1. After this, room 1 is empty and the new guest can be moved into that room. By repeating this procedure, it is possible to make room for any finite number of new guests. It's clear that if a new guest arrives and we proceed just like the the method above we get a bijection, but how can I write it precisely? I think that my method should work also for the second case(2), but I don't see which elements should I move.","['elementary-set-theory', 'discrete-mathematics']"
1739551,Fréchet Topology on $C^\infty(M)$,"In the Fréchet space wikipedia article , in the ""Examples"" section, it is stated that the space of smooth functions $C^\infty(M)$ on a compact smooth manifold $M$ can be made into a Fréchet space ""by using as seminorms the suprema of the norms of all partial derivatives"". However since of course $M$ need not admit a single coordinate chart, what is meant by ""partial derivatives""? I.e. what exactly is this countable family of seminorms on $C^{\infty}(M)$? I'm sorry if this is super standard (it seems like it should be!), but my preliminary Google searches were quite unsuccessful.","['functional-analysis', 'smooth-manifolds', 'differential-geometry']"
1739621,The infinite integral of $\frac{\sin x}{x}$ using complex analysis,"The problem i came across is the evaluation of $$\int_0^\infty\frac{\sin x}{x}\,dx$$ I chose the function $f(z) = \dfrac{e^{iz}}{z}$ and took a contour of $[\varepsilon , R ] + [R , R+iy] + [-R+iy , R+iy] + [-R,-R+iy]+[-R, -\varepsilon]$ . The problem is how do I continue now to find integrals on each of these segments ?",['complex-analysis']
1739644,Is the fixed point set of an action a submanifold?,"Let $M$ be a differentiable manifold, and $G$ a Lie group acting smoothly on $M$. Under which condition - if any - is the set of fixed points of the action a submanifold of $M$? My thoughts so far: if we could determine this set as the preimage of some non-critical value of a diffeomorphism, we would be done. In the easy case $M=\mathbb R^n$, one can consider the function $g\cdot x - x$, with $g\in G$ fixed and $x\in M$ varying, and then pick the preimage of zero. In any case, this gives a larger set than those of all the fixed points: an intersection would be needed, and then who knows what may be happening? I'm particularly interested in compact Lie groups, and I tried looking for counterexamples, but I can't find any. I have the feeling it shouldn't be too hard to find a set of fixed points self-intersecting in some point (I'm thinking of sth like the axes in $\mathbb R^2$), but I probably shouldn't be trusting feelings. References, hints, generic intuitions are very welcome!","['group-actions', 'differential-geometry', 'lie-groups']"
1739648,On the extension of the solution to a nonlinear ODE,"Consider the nonlinear ODE $$x' = (x^2 - e^{2t})f(t, x)$$ with $f$ continuous. Prove that for any $\tau > 0$, if $|x_0|$ is sufficiently small, the solution $x(t)$ to the ODE above can be extended to $\tau \leq t < \infty$. My general sketch: 1) If I can show that for any compact subset $\Omega \subset \mathbb{R}^n $, the solution $x(t)$ hits the boundary point of $\Omega$ then I can keep extending the solution to infinity; 2) Equivalently, I was trying to show that the integral $$\int_{x_0}^{x_0 + \alpha} \frac{1}{F(t, x)}$$ diverges,  where $F(t, x) = (x^2 - e^{2t})f(t, x)$; 3) The third way I was thinking is that I can use the fact $|F(t, x)| \leq K|x_0|$, the maximum interval of existence is $(-\infty, \infty)$. But I can't see to bound it.",['ordinary-differential-equations']
1739652,Finding the Spectrum of integral operator,"I have the following integral operator:
$$(Ku)(x)=\int_{0}^{1} k(x,y)u(y) \mathop{dy}$$
with $k(x,y)=$min $ \{ x,y \}$ for $0 \leq x,y \leq 1$.$\\$ I have already shown $K$ is a compact, self adjoint operator but now I want to find the spectrum of $K$. I do not understand how to do this. Do I need to find the eigenvalues? If so, how would I go about doing this? (I am self teaching myself functional analysis so perhaps there is an easy way to do this that I just haven't come across? $\\$ Thanks in advance.",['functional-analysis']
