question_id,title,body,tags
4224749,How to prove this inequality with this $\sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} $,"$x_i \ge 0 \quad (i=1,2,\dots,n)$ and $n \ge 2$ . Prove or disprove: $$
\sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} \geq \sum_{c y c} \sqrt{x_{1}^{2}+\frac{\left((n-1) x_{1}-x_{2}-\dots-x_{n}\right)^{2}}{4 n(n-1)}}
$$ This is a hard problem proposed by Chen Shengli, an expert at mechanical theorem-proving in inequality. His program didn't work on this, so I'd like to share it with you and see if anyone can prove or disprove it (proof by computer is welcomed here).","['multivariable-calculus', 'cauchy-schwarz-inequality', 'summation', 'inequality']"
4224768,"What are examples of the integration trick involving ""Simultaneous Integrals,"" ""Dual Integrals,"" or ""Pairs of Integrals""?","I'm trying to get more apt at computing integrals, and I'm looking for examples of the particular integral technique of ""Simultaneous Integrals,"" ""Dual Integrals,"" or ""Pairs of Integrals""? Here (in Method 2) is an example of the technique I'm looking to understand. The technique is used as one option for integrating $\sqrt{\tan(x)}$ , wherein two integrals $I$ and $J$ are introduced and the values of $I+J$ and $I-J$ are computed/used to determine the desired value. My questions: What is this technique called? I included three names in my title; are any of those correct? (the most important) What are some other examples of using this technique in integration? The more tractable / lower-level, the better: I'd like to be able to convey this to Calculus students. (of lesser importance) What is the theoretical underpinnings of this technique that I need to be aware of? I've read that this is really a vector space basis situation at its core; is this really a linear algebra technique? Down below, I collect some information: Is this question a duplicate? Technically, yes. This 8-year-old question asks the same thing, but the answers there didn't really scratch the itch that I have. What I've found independently: https://www.openbookpublishers.com/htmlreader/978-1-78374-142-7/Chapters/P26.html https://sci-hub.do/10.1080/10511971003742411 https://www.jstor.org/stable/2974819","['integration', 'calculus', 'linear-algebra']"
4224785,How to find a condition for when a multivariable limit exists,"If I have the multivariable limit $$ \lim_{(x,y) \to (0,0)} \frac{x^ay^b}{(x+y)^c} $$ How do I find a general relationship/condition for $a$ , $b$ , $c$ that results in the limit existing? I've found various specific examples of $a$ , $b$ , $c$ that make the limit exist, but I don't know what the relationship between them is.","['limits', 'multivariable-calculus', 'real-analysis']"
4224823,Can this stochastic integral exist?,"Assume you have a filtered probability space with a Brownian motion $B_t$ . Let $X_t$ be a progressively measurable process with $P(\int_0^T X_s^2 ds <\infty)=1.$ Assume that $\int_0^TX_sdB_s=0$ a.s. Can it be that $X_s$ is not equal to zero $Leb[0,T]\times P$ a.e.? attempt: If $E[\int_0^TX_s^2dt]<\infty$ this can not happen because then we have from the Itô-isometry $E[\int_0^TX_s^2dt]=E[(\int_0^TX_sdB_s)^2]=E[0]=0.$ But what if $E[\int_0^TX_s^2dt]=\infty$ ? In this case we know from the construction of the stochastic integral that $\int_0^tX_sdB_s$ is a local martingale, denote it by $Y_t$ . We can stop it to get a martingale with a sequence of stopping times, $Y_{\min(t,\tau_n)}=\int_0^{\min(t,\tau_n)}X_s dB_s$ . We then get $$E\left[\left(\int_0^{\min(t,\tau_n)}X_s dB_s\right)^2\right]=E\left[\int_0^t1_{\{s\le \tau_n\}}X_s^2ds\right].$$ We also have $$\lim\limits_{n \rightarrow \infty}E\left[\int_0^t1_{\{s\le \tau_n\}}X_s^2ds\right]=E\left[\lim\limits_{n \rightarrow \infty}\int_0^t1_{\{s\le \tau_n\}}X_s^2ds\right]=E\left[\int_0^tX_s^2ds\right]=\infty.$$ But I am not able to arrive to a contradiction because I do not know if $$\lim\limits_{n \rightarrow \infty}E\left[\left(\int_0^{\min(t,\tau_n)}X_s dB_s\right)^2\right]{}^?=^?E\left[\lim\limits_{n \rightarrow \infty}\left(\int_0^{\min(t,\tau_n)}X_s dB_s\right)^2\right].$$ I can't use the monotone or the dominated convergence theorem in the last case as far as I see. Do you see how to solve this? I also tried using Doobs inequality like this: $$E\left[\int_0^T1_{\{s\le \tau_n\}}X_s^2ds\right]=E\left[\left(\int_0^{\min(T,\tau_n)}X_s dB_s\right)^2\right]\\ \le E\left[\sup\limits_{0\le t \le T}\left(\int_0^{\min(t,\tau_n)}X_s dB_s\right)^2\right]\le 4 E\left[\left(\int_0^{\min(T,\tau_n)}X_s dB_s\right)^2\right]=4E\left[\int_0^T1_{\{s\le \tau_n\}}X_s^2ds\right].$$ But again I am not able to arrive at a contradiction, because if I let $n$ go to infinity I am not allowed to move the limit inside the fourth term as for as I see. Update: I am able to prove it with the additional assumption that $\int_0^t X_s dB_s = 0$ a.s, for every $t \in [0,T]$ . But I am specifically interested in the case where we only assume that $\int_0^T X_s dB_s = 0$ a.s.. The quesiton can there be solved if we can prove that if $\int_0^T X_s dB_s = 0$ a.s. then $\int_0^t X_s dB_s = 0$ a.s, for every $t \in [0,T]$ . Another way to prove it is if we can show that $\int_0^t X_s dB_s$ is a martingale. Because then $E[(\int_0^T X_s dB_s)^2]\ge E[(\int_0^t X_s dB_s)^2] $ , because $x^2$ is convex and we use Jenssen. The problem is that since we do not know if $E[\int_0^T X_s^2 ds]<\infty$ we do not know that it is a martingale?","['stochastic-integrals', 'stochastic-analysis', 'probability-theory', 'stochastic-calculus']"
4224827,Traffic Dynamics. How to fix initial condition deficiency?,"Traffic in a tunnel . A rather realistic model for the car speed in a very long tunnel
is the following: $$ v(\rho)=\left\{\begin{matrix}
v_m & 0 \leq \rho \leq \rho_c\\ 
 \lambda log(\frac{\rho_m}{\rho}) &  \rho_c \leq \rho \leq \rho_m
\end{matrix}\right.$$ where $ \lambda =\frac{v_m}{log(\rho_m / \rho_c)}.$ Observe that $v$ is continuous also at $$\rho_c=\rho_m e^{-v_m /\lambda},$$ which represents a critical density: if $\rho \leq \rho_c$ the drivers are free to reach the speed limit. Typical values are: $\rho_c=7car/Km$ , $v_m=90 Km/h$ , $\rho_m=110 car/Km$ , $v_m/\lambda=2.75.$ Assume the entrance is placed at $x=0$ that the cars are waiting (with maximum density) the tunnel to open to the traffic at time $t=0$ . Thus, the initial density is $$\rho=\begin{cases}
 \rho _m& \text{ if } x<0 \\ 
 0 & \text{ if } x>0 
\end{cases}$$ a. Determine density and car speed; draw their graphs. b. Determine and draw in the $x$ , $t$ plane the trajectory of a car initially at $x = x_0 < 0$ ,and compute the time it takes to enter the tunnel. I really don't know where to start, my procedure to find density is using the characteristics method, first reformulating the initial condition as $$g_\epsilon(x_0)=
\begin{cases}
 \rho _m&  x_0<0 \\ 
 \rho_m(1-\frac{x_0}{\epsilon}) & 0<x_0<\epsilon\\
0 & x_0>0
\end{cases}$$ then, using the conservation of matter equation $ \rho_t + q (\rho) _x = 0$ with $q (\rho) = v (\rho) \rho $ replace the value of $ v (\rho) $ that gives the exercise in $ q (\rho) $ , however I don't know very well how the function is defined in pieces. Then my idea is to evaluate $ g_\epsilon (x_0) $ in $ q (\rho) $ but since I have not defined the function well by parts I do not know how to replace. thanks!! This exercise is in the book named Partial Differential Equation in Action of Sandro Salsa, third edition, Page. 253 - exercise 4.6","['transport-equation', 'mathematical-modeling', 'analysis']"
4224855,Is there a name or some sort of definition for a curve traced by one object moving continuously towards another?,"This one is indeed just my fancy sprouted from a simple geometry problem. Suppose on a Cartesian plane there is an object at $(0,1)$ moving in the horizontal positive direction at one unit distance per unit time. Meanwhile another object is at $(0,0)$ moving with the same speed but constantly heading towards the first object. If the motion of the second object merits scrutiny, given that someone had looked into this problem before, what is the definition or name of the curve traced by the second object?","['calculus', 'geometry', 'terminology']"
4224870,Get all dihedral angles of a tetrahedron given three angles and three edges,"Suppose you have a general tetrahedron, you are given the three angles that define a vertex of that tetrahedron. You also know the length of the edges that converge on this same vertex. Given that, calculate all dihedral angles. From This question and its accepted answer I can get three of the six dihedral angles. I am trying to figure out the rest (the ones at the ""base""). I tried adapting the answer for that question, extending the definition of $\vec v_1$ , $\vec v_2$ , $\vec v_3$ , so that now they are not unit vectors, but their magnitude is the length of the edges. Then we could similarly define a unit normal vector for the ""base"" as: $$\vec n_{12}=\dfrac{\vec v_1 \times \vec v_2}{|\vec v_1 \times \vec v_2|}  \quad\text{and} \quad \vec n_{b}=\dfrac{\vec v_1 \times \vec v_2 + \vec v_2 \times \vec v_3 + \vec v_3 \times \vec v_1}{|\vec v_1 \times \vec v_2 + \vec v_2 \times \vec v_3 + \vec v_3 \times \vec v_1|}$$ so then, similar to that answer: $$\text{cos}(θ_{ab})=-\vec n_{12}\cdot \vec n_{b}$$ But this hasn´t taken me very far. I feel like I might be very close but my vector arithmetics is too rusty. Or it is just simply wrong. EDIT: Already solved this, thanks to your answers. However I am still trying to solve using my original approach. After some vector operations I get to this: $\text{cos}(θ_{ab})=- \dfrac{a^2b^2\text{sin}^2\phi_{ab}+b^2ac(\text{cos}\phi_{ab}\text{cos}\phi_{bc}-\text{cos}\phi_{ac})+a^2bc(\text{cos}\phi_{ac}\text{cos}\phi_{ab}-\text{cos}\phi_{bc})}{ab\text{sin}\phi_{ab}(|\vec v_1 \times \vec v_2 + \vec v_2 \times \vec v_3 + \vec v_3 \times \vec v_1|)}$ Where $a,b,c$ are the lengths of the edges along $\vec v_1,\vec v_2,\vec v_3$ (also their magnitudes) and $\phi_{ab}$ is the angle between edges $a$ and $b$ (and so on) So it is looking quite well, because it is mostly in terms of the angles and edge lengths, but still need to simplify those vectors. Any ideas?","['trigonometry', 'geometry']"
4224882,Do I need to have taken differential equations to understand the Cauchy Riemann equations?,"I have a soft question: I have just finished a two semester sequence of real analysis where I covered topics such as sequences and series, point set topology, continuity, differentiability, and integration (all in $\mathbb{R}$ ). I am taking a course in complex analysis next semester which has analysis as a prerequisite, but I saw that the Cauchy--Riemann equations are differential equations which must be satisfied for something to be holomorphic. Does one generally need ODEs and PDEs to take a standard undergraduate course in complex analysis?","['complex-analysis', 'partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
4224884,derivation of surface gradient,"Suppose the surface S is parameterized as $\textbf{x}=(x_1(u_1,u_2),x_2(u_1, u_2), x_3(u_1,u_2))$ ,  then the surface gradient $\nabla_{S}$ of a scalar function $p(x_1,x_2,x_3)$ on $S$ is defined as, $$\tag{1} \nabla_S p = \sum_{i,j=1}^{2}g^{ij}\frac{\partial p}{\partial u_i}\frac{\partial \textbf{x}}{\partial u_j}\hspace{2cm} $$ where $g^{ij}$ is the (i,j)th entry of the inverse of the matrix $G$ given by $$G_{ij}=\frac{\partial\textbf{x}}{\partial u_i}\cdot\frac{\partial\textbf{x}}{\partial u_j},i,j=1,2$$ From this definition, we have the following formula, \begin{equation}
\tag{2} \nabla_{S}p=\textbf{n}\times\nabla p\times \textbf{n}\hspace{2cm}
\end{equation} where $\nabla p$ is the standard gradient in 3D, $\textbf{n}$ is the unit normal vector on $S$ . I have no idea how the surface gradient $\nabla_S p $ $(1)$ is defined. I try to understand this  by deriving $(1)$ from equation $(2)$ , but this seems rather tedious. Can anyone  give a detailed derivation of $(1)$ ? Thanks!","['vector-analysis', 'differential-geometry']"
4224891,"Minimal polynomials of ""simple"" algebraic numbers","This should be a fairly trivial question, to which I have nevertheless found no satisfactory answer. I am interested in effective algorithmic computation of minimal polynomials of some particularly simple algebraic numbers, especially the complex $n$ -th roots of rational numbers: $$\alpha = \sqrt[n]{q}e^{i2k\pi/n}.$$ Each such $\alpha$ is a root of  the polynomial $x^n - q$ , but the trouble is that this polynomial might not be irreducible over $\mathbb{Q}$ . I know that the irreducible factors can be found in polynomial time, e.g., using the LLL algorithm. But how to determine the correct factor? This can usually be done easily by hand, but what about algorithmic computation? The only way of evaluating a polynomial at $\alpha$ that I am aware of already assumes that the minimal polynomial of $\alpha$ is known. Or should I use a completely different approach? Moreover, since $q$ can be assumed to be an integer, an algorithm for computing minimal polynomials of algebraic integers would be sufficient. This MathOverflow answer suggest that computing minimal polynomials of algebraic integers is easy, but I was unable to find out why. Any reference would be helpful here. Thank you in advance.","['algebraic-number-theory', 'minimal-polynomials', 'abstract-algebra', 'algebraic-numbers', 'computer-algebra-systems']"
4224906,Orbit-Stabilizer-Theorem and colourings of the cube,"This question is about colourings of the cube with 6 colours. It is well known that one can colour a cube in 30 non-equivalent ways using 6 colours. The way to see this is to observe that a cube can be rotated into 24 different positions which means that for one colouring there are 23 others that are equivalent to it. Then total number of colourings $6!$ divided by $24$ yields $30$ . Talking in terms of group actions the set the group acts on is the set of colourings and the group is the set of isometries of the cube.
The 24 coulourings that are rotaitonally equivalent form an orbit of a given colouring and the group of isometries acting on the cube. One can use Burnside's lemma to find the number of orbits. But I was wondering if the Orbit-Stabilizer-Theorem also applied. The theorem states that $\mid G \mid$ (= number of isometries of the cube = $48$ ) is equal to the stabilizer of a given element (=colouring) times the size of its orbit (= $24$ ). Now the problem I have is that $\frac{48}{24}$ equals two and not $30$ as it should.  Basically, using this I have calcualted that the stabilizers have size $2$ but I think that's wrong too because obviously the only isometry that leaves a given side alone is the identity. So my questions are: Is it possible to use the orbit-stabilizer-theorem to count ways to colour a cube? What's the size of a stabilzer in this example?","['group-theory', 'group-actions', 'combinatorics', 'coloring']"
4224907,"Given 1003 points on the plane, there are at least 2003 pairs which can be surrounded by closed balls containing no other points.","I came across the following problem: Let $X$ be a set of $1003$ distinct points on the plane s. t. no three of them are colinear and no four of them lie on the same circle. We say that a pair of points $a,b\in X$ is good if there exists a closed ball $B$ s. t. $B\cap X=\{a,b\}$ . Let $S$ be the set of all good pairs. Show that $2003\leq|S|\leq 3003$ . Second inequality is quite easy - we observe that $S$ induces a planar graph with each connected component having at least $3$ vertices and use inequality $e\leq 3v-6$ . I have a problem with the first inequality. I can show that $1003\leq |S|$ (just by showing that every point of $X$ is contained in at least $2$ good pairs, by considering cases when this point is in $int(conv(X))$ or $\partial(conv(X))$ , but I was unable to get anything significantly better than that. Any help will be appreciated.","['euclidean-geometry', 'combinatorial-geometry', 'combinatorics', 'geometry']"
4224910,"Finding $\lim\limits_{x\to 0}\,\left(\csc^2x - \frac{1}{x^2}\right)$","What is the limit of $\,\lim\limits_{x\to 0}\,\left(\csc^2x - \frac{1}{x^2}\right)$ ? My thought was $\lim\limits_{x\to 0}\,\left(\csc^2x - \frac{1}{x^2}\right) = \lim\limits_{x\to 0}\,\left(\frac{1}{\sin^2 x} - \frac{1}{x^2}\right) = \lim\limits_{x\to 0}\,\left(\frac{1}{x^2} - \frac{1}{x^2} \right) = 0$ . I used $\lim\limits_{x\to 0} \frac{\sin x}{x} = 1$ . But $\lim\limits_{x\to 0}\,\left(\csc^2x - \frac{1}{x^2}\right) = \lim\limits_{x\to 0}\,\frac{x^2-\sin^2x}{x^2\sin^2x}$ and if I apply L'Hospital's Rule four times at the second expression, I get $\lim\limits_{x\to 0}\,\left(\frac{8\cos 2x}{24\cos 2x - 32\sin 2x - 8x^2\cos 2x}\right) = \frac {1}{3}$ . What am I missing?","['limits', 'calculus', 'solution-verification']"
4224919,Why do we assume that this quadratic has one solution?,"I'm asking about a question about two lines which are tangents to a circle. Most of the question is quite elementary algebra, it's just one stage I can't get my head round. Picture here: The circle $C$ has equation $(x-6)^2+(y-5)^2=17$ . The lines $l_1$ and $l_2$ are each a tangent to the circle and intersect at the point $(0,12)$ . Find the equations of $l_1$ and $l_2$ giving your answers in the form $y=mx+c$ . Both lines have equation $y = mx + 12$ where $m$ represents two gradients to be found (both negative). The circle has equation $(x-6)^2 + (y-5)^2 = 17$ . Combining the knowledge $y = mx + 12$ for both lines and $(x-6)^2 + (y-5)^2 = 17$ produces the quadratic: $$(1+m^2)x^2 + (14m−12)x + 68 = 0$$ At this point I was confused about what step to take to get $m$ or $x$ . Looking at the worked solution, it says ""There is one solution so using the discriminant $b^2 − 4ac = 0$ ..."" From here it's straightforward algebra again, producing another quadratic based on the $b^2 - 4ac$ of the previous quadratic: $$(14m−12)^2 - 4 x (1+m^2) + 68 = 0$$ etc. until we have $m = -4$ or $-8/19$ . My question is I don't understand how we can tell it's right to  assume $b^2 - 4ac = 0$ and how we can see that's the right step to take in this question. Obviously this feels intuitively wrong since we know there are two solutions for m. Is the logic that m is a gradient which intersects with the circle once? But if so, how do you see that this is the right equation to decide it only has one solution? (I had assumed before looking at the worked example if I needed to do something more complicated based on the equations of the radii or the knowledge that the two tangents would be equal length from the circle to where they meet or something.) As you can tell, this question is based on fairly elementary algebra; I'm more concerned about knowing why this is the right step to take here. Many thanks for any answers.","['analytic-geometry', 'circles', 'geometry', 'tangent-line']"
4224934,"Intuitively, we expect a level set in $\mathbb{R}^3$ to be a $2$-dimensional surface. Why?","I am watching a lecture about multivariable mathematics (Math 3500 Day 20: Continuity and Preimages) by Prof. Theodore Shifrin. Let $f:\mathbb{R}^3\to\mathbb{R}$ be a function. Let $c\in\mathbb{R}$ be a real number. Suppose that $\{x\in\mathbb{R}^3 | f(x)=c\}\neq\emptyset$ . Intuitively, we expect the set $\{x\in\mathbb{R}^3 | f(x)=c\}$ to be a $2$ -dimensional surface. (1) Why do we expect this set $\{x\in\mathbb{R}^3 | f(x)=c\}$ is $2$ -dimensional? For example, if $f(x) = c$ for all $x\in\mathbb{R}^3$ , then the set $\{x\in\mathbb{R}^3 | f(x)=c\}$ is $\mathbb{R}^3$ , which is not a $2$ -dimensional surface. But we don't think this is a typical case. (2) For what $f:\mathbb{R}^3\to\mathbb{R}$ , is the set $\{x\in\mathbb{R}^3 | f(x)=c\}$ $2$ -dimensional? (3) I don't know the definition that a subset of $\mathbb{R}^3$ is $2$ -dimensional. What book should I read?","['multivariable-calculus', 'definition', 'surfaces']"
4224955,Random search for very big Collatz conjecture counter-examples,"I know that exhaustive search was done to test numbers up to 2^68. This seems like a big number but when looking at Collatz function as a Turing machine manipulating some input bit sequence, only sequences up to 68 bits were tested.
Maybe something interesting happens with numbers that have ~1000000 bits (that is in range ~2^1000000)? I couldn't find any project searching for Collatz counter-examples this way, do you know about any? Also, when checking random numbers, are there any results that prove that only some numbers can become such counter-examples? I found somewhere that it is sufficient to check numbers x that (x mod 6) = 2 but I don't know if it was rigorously proven.","['collatz-conjecture', 'number-theory', 'finite-state-machine', 'big-numbers']"
4224968,Difference between set and object in set theory,"Lately, I have started studying set theory from the ""Elements of Set Theory"" book. I'm just confused with the below sentence about what's the difference between set and object in set theory. A set is a collection of things (called its members or elements), the collection being regarded as a single object. Can I say ""a set is an object""?",['elementary-set-theory']
4224969,High-order derivatives are independent of the chart,"I am studing Ehresmann's jet bundles on manifolds and I came up with a (maybe silly) question. In order to make it easy I skip the details of the definition and go directly to the part that I don't quite understand. Given an $n$ -dimensional manifold $M$ and two local diffeomorphisms from a nbh of $0\in\mathbb{R}^n$ to a nbh of a point $x\in M$ $$
f_1: V_1\subset \mathbb{R}^n \rightarrow U_1 \subset M\\
f_2: V_2\subset \mathbb{R}^n \rightarrow U_2 \subset M\
$$ it is said that $f_1$ and $f_2$ define the same $r$ -jet at $x\in M$ if for any chart $\varphi$ around $x$ all the components of the maps $\varphi \circ f_1$ and $\varphi \circ f_2$ (which are functions from $\mathbb{R}^n$ to $\mathbb{R}$ ), have the same $k$ -th order derivatives at $0$ for any $0 \leq k \leq r$ . (In particular $f_1(0) = f_2(0) = x$ ). My question is: can we change ""for any chart"" for ""for some chart"" in this definition? In other words, does the property above depend on the choice of the chart? If $r=1$ the answer is yes, since we can interpret the condition above in terms of the differentials of the maps. Since the charts are local diffeomorphisms we can multiply and divide by their differentials and the condition for one chart implies the condition for another chart (since what we really have is that the differentials of $f_1$ and $f_2$ at $0$ coincide). Can we do something similar for general $r$ ?","['fiber-bundles', 'smooth-manifolds', 'jet-bundles', 'manifolds', 'differential-geometry']"
4224999,Is the product topology of scott topologies a scott topology?,"Defnition :
Let $(A, \leq)$ be a poset(partially ordered set) and $O \subseteq A$ . Then $O$ is a directed set if and only if for any $x, y \in O$ , there exists $z \in O$ such that $x \leq z \mathrm{\ and\ } y \leq z$ . $O$ is an upper set if and only if for any $x \in O$ , $y \in A$ , $x \leq y$ implies $y \in O$ . $O$ is inaccessible by directed joins if and only if for any directed set $S \ (\subseteq A)$ with a least upper bound $\sup S$ , $\sup S \in O$ implies $S \cap O \neq \emptyset$ . $O$ is scott open if and only if $O$ is an upper set and inaccessible by directed joins. The scott topology of A is the topology whose open sets are scott open sets. Question : Let $(A_i, \leq_i)$ be posets for $i = 1,2,\dots,n$ and $A := A_1 \times \cdots \times A_n$ a poset with the partial order defined as follows: $$ x \leq y \iff x_i \leq_i y_i\ (\mathrm{for\ all}\ i = 1,2,\dots,n) \\
\mathrm{for}\  x = (x_1, \dots, x_n),\ y = (y_1, \dots, y_n) \in A.$$ Then, does the product topology $\mathcal{O}$ of the Scott topologies of $A_i$ coincide with the Scott topology $\mathcal{O}'$ of $A$ ? (I checked an open set in $\mathcal{O}$ is an open set in $\mathcal{O}'$ , but I can't show the converse.) Thank you in advance.","['order-theory', 'general-topology', 'product-space', 'computer-science']"
4225005,Tensor product $L \otimes_K L$ has no nilpotent elements iff $I/I^2=0$,"Let $L \supset K$ be  a finite extension of fields.
The diagonal $L \otimes_K L$ we can endow
with structure of $L$ algebra via $L \to L \otimes_K L,\ l \mapsto l \otimes 1_L$ .
Especially $L \otimes_K L$ carries structure
of a $L$ -module via $L$ -multiplication
in first factor $l \cdot (a \otimes b) :=
la \otimes b$ . Let $d: L \otimes_K L \to L, a \otimes b \mapsto ab $ be the diagonal map and $I$ its kernel. I want to show that $L \otimes_K L$ is reduced (has no nilpotent elements)
iff $I = I^2$ , i.e. $I/I^2=0$ . Ideas: We can simplify the problem if we
choose an more accessible system of generators
for $L$ -module $I/I^2$ . I claim that $a \otimes 1_L -1_L \otimes a$ for $a \in L$ generate $I/I^2$ as $L$ -module. That's because $a \otimes b =
ab \otimes 1 - a \cdot (b \otimes 1 - 1 \otimes b)$ (recall $I$ carries $L$ -module structure by multiplication in first factor) and therefore for $\sum a_i \otimes b_i \in I$ we obtain by linearity $$\sum a_i \otimes b_i =
(\sum a_i b_i) \otimes 1  - \sum a_i \cdot
 (b_i \otimes 1 - 1 \otimes b_i)= \sum a_i \cdot
 (b_i \otimes 1 - 1 \otimes b_i)$$ Therefore it suffice to show that $L \otimes_K L$ is reduced iff
every $a \otimes 1 - 1 \otimes a, a \in L$ is
modulo $I^2$ a product of two other such $x \otimes 1 - 1 \otimes x, y \otimes 1 - 1 \otimes y,
x, y \in L$ . How can I do it? Note: If we realize that $I/I^2$ can be interpreted as module of Kahler differentials $\Omega_L$ and we use another more usual construction of $\Omega_L$ as certain quotient of some free $\bigoplus_iK[x_1,..., x_n] dx_i$ use some standard exact sequences between Kahler differentials then the claim
follows immediately. But I want to know if it possible to show the claim directly(!) working only with definition of $I/I^2$ instead of making a detour over quoting results known for Kahler differntials.","['field-theory', 'algebraic-geometry', 'abstract-algebra', 'chinese-remainder-theorem', 'commutative-algebra']"
4225016,Generating 3 random numbers that sum to 1 with two different methods.,"I am looking at ways to generate three random numbers that sum to $1$ . I came up with two different methods and played around with some code and they seem to have very different distributions. Method 1 Generate $3$ Uniform $[0,1]$ variables and then divide by their sum : Method 2 For this method I imagined picking 2 points on a stick of length 1, ensuring that their sum would always be 1, and then using the two points to break the stick into 3 pieces. Quick look at the methods It is clear to see in Method 1 the distribution of all three outputs will be the same, that is all three outputs are completely symmetrical. This is also true for Method 2 but takes some more thinking to see why. It should be clear by another reflection/symmetry argument that min and 1 - max have the same distribution and by a bijective reflection around max/2 it is clear to see that max - min has the same distribution as min. Breakdown and Problem I then ran a few thousand simulations of each method and we get vastly different results. Results method 1 Results method 2 The question Method 1 seems somewhat poisson like whereas method 2 feels to be more uniform. What is the correct way to generate n numbers that sum to 1? Why are these results so different? Have we constricted the ability to be random by demanding they sum to 1?","['statistics', 'probability']"
4225033,Proof about meromorphic function which its image is in $ \mathbb{C}\cup\left\{ \infty\right\} $,"Let $ D^{*}=D\left(0,1\right)\setminus\left\{ 0\right\}  $ . Assume $ f:D^{*}\to\mathbb{C}\cup\left\{ \infty\right\}  $ is meromorphic with a sequence of poles $z_n\in D^*$ which converge to $0$ . Prove that the image of $ f $ is densed in $ \mathbb{C}\cup\left\{ \infty\right\}  $ . I'd like to see a good proof for that. I'll share my attempt anyway and I'd be glad to hear what you guys think: Assume by contradiction that the image of $D^*$ under $ f $ is not densed. Then one can find an open disk $ D\left(a,r\right) $ (center $ a $ and radius $r $ ), such that $ f\left(D^{*}\right)\cap D\left(a,r\right)=\emptyset $ . Thus, the function defined by $ h\left(z\right)=\frac{1}{f\left(z\right)-a} $ is holomorphic in $D^* $ since the denominator never vanish, and  for $z_k$ , a pole of $ z$ , we have $h(z_k)=0$ Next, notice that $0 $ cannot be a deleted singularity of $ h $ since then by uniqueness theorem we'll get that $ h$ is constant zero, and also $0 $ cannot be a pole of $ h $ because zeroes of meromorphic function cannot accumulate at a pole. Thus we can conclude that $0 $ is an essential singularity of $h $ , and thus by the Casorati-Weierstrass theorem, the image of $D^* $ under $h$ is densed in $\mathbb{C} $ . Next, fix $z_0\neq a$ in $D(a,r )$ and fix $\varepsilon>0 $ small enough so that $ D\left(z_{0},\varepsilon\right)\subset D\left(a,r\right) $ and also $a\notin D(z_0,\varepsilon)$ . Then, by the open map theorem, since $ \frac{1}{z-a} $ is holomorphic in an open neighborhood of $D(z_0,\varepsilon)$ , we know that $ \left\{ \frac{1}{z-a}:z\in D\left(z_{0},\varepsilon\right)\right\}  $ is an open set, and thus there exists $z_1$ \in $D^*$ such that $ \frac{1}{f\left(z_{1}\right)-a}=h\left(z_{1}\right)\in\left\{ \frac{1}{z-a}:z\in D\left(z_{0},\varepsilon\right)\right\}  $ So we have found $z_1\in D^*$ such that $ f\left(z_{1}\right)\in D\left(z_{0},\varepsilon\right)\subset D\left(a,r\right) $ which is a contradiction. A few thoughts: This is not intuitive for me because Im used to be careful of writing things like $f(z)=\infty $ and stuff like that. 1.The proof is correct? 2.Can we say that if $z_k$ is a pole of $ f $ then $f(z_k)=\infty $ ? can we say that if $z_k $ is a pole of $ f $ then $1/f(z_k)=0$ ? 3.Im not sure what does it mean that the image is densed in $ f:D^{*}\to\mathbb{C}\cup\left\{ \infty\right\}  $ . Proving that the image is densed in $\mathbb{C} $ is enough? 4.I did not talk about the case where $a=\infty$ . Does it changes anything? 5.Do you have simpler proof? Any clarifications would be very helpful. Thanks in advance.",['complex-analysis']
4225068,"There is a unique Borel measure on $(0,\infty)$ with scaling invariance and $\mu([1,e])=1$.","Suppose that $\mu$ is a Borel measure on $(0,\infty)$ with $\mu([1,e])=1$ and for all $c>0$ and all $A$ Borel measurable, we have $\mu(cA)=\mu(A)$ . Show that there exists a unique $\mu$ with these properties. It is clear to me that this measure is just $\mu(A)=\int_A \frac{1}{x}\,\text{d}x$ . Both of the properties are easy to check. However I am confused about uniqueness. I have tried supposing that there are $\mu_1,\mu_2$ with these properties and considering the signed measure $\mu_1-\mu_2$ . I also have thought about using a Lebesgue decomposition, arguing that the singular part must be identically the zero measure, and then reducing to the case when $\mu$ is also absolutely continuous with respect to Lebesgue measure. Overall I feel like I can't really do much with the scaling invariance. The algebra is just not being nice to me today. Any hints will help!","['measure-theory', 'locally-compact-groups', 'haar-measure']"
4225072,"Since the radical of the ideal $I=(x, y^2)$ in $\mathbb{Q}[x, y]$ is $(x, y)$, then $I$ is a primary ideal that is not a power of a prime ideal.","I've been doing the exercises from Section 9.1 of Dummit and Foote and got stuck on the following problem: Show that the radical of the ideal $I=(x, y^2)$ in $\mathbb{Q}[x, y]$ is $(x, y)$ . Deduce that $I$ is a primary ideal that is not a power of a prime ideal. I've done the first part of this exercise, including proving that $I$ is a primary ideal, but I'm not sure why I can deduce from that that $I$ is not a power of a prime ideal. I'm aware that this same exercise has been posted here before, but the OP was stuck on the first part of the problem, which I think I was able to solve. My work so far can be seen below (which I would appreciate if someone could verify): Any element in $(x, y)$ is of the form $f(x, y)=xg(x, y)+yh(x, y)$ , so that $$f^2(x, y)=x^2g^2(x, y)+y^2h^2(x, y)+2xyg(x, y)h(x, y)=x[xg^2(x, y)+2yg(x, y)h(x, y)]+y^2[h^2(x, y)]$$ and, in particular, $f \in \sqrt{(x, y^2)}$ . Conversely, if $f \in \sqrt{(x, y^2)}$ , then $f^n \in (x, y^2)$ for some $n$ . If $f$ had a constant term $c \neq 0$ , then $f^n$ would have $c^n \neq 0$ as its constant term, a contradiction. Hence, $f$ has no constant term, so $f \in (x, y)$ . Thus, $\sqrt{(x, y^2)}=(x, y)$ . To show that $I$ is primary, let $fg \in (x, y^2)$ with $f \not\in (x, y^2)$ . The polynomial $g(x, y)$ then can't have constant term different than zero, because that would mean $fg$ has either a constant term different than zero (in the case $f$ also has such a term) or a term of the form $ay$ for $a \in \mathbb{Q}$ . Hence, $g \in (x, y)$ , so $g^2 \in (x, y^2)$ . With that, I'm not sure why I can conclude that $I$ isn't a power of a prime ideal. I know that the fact that $I$ is primary implies $\sqrt{(x, y^2)}=(x, y)$ is prime, but not sure how to proceed from here. I would particularly appreciate if someone could give me a hint or provide an answer that doesn't use results that come after Section 9.1 of D&F.",['abstract-algebra']
4225081,When are a group $G$ and a normal subgroup both extensions of a subgroup $H$ of $G$?,"Let $G$ be a group and $H$ be a subgroup of $G$ . Suppose $G$ is an extension of $H$ such that $G = N \rtimes H.$ Consider a normal subgroup $K$ of $G,$ which is also an extension of $H.$ Clearly, $K = (N \cap K) \rtimes H.$ I was trying to know what should be the condition on such a subgroup $K$ of $G$ so that it itself is an extension of $H.$ Also, can we say that $N$ is a direct product of $N \cap K$ with some central subgroup of $G$ ? Clearly, the first one that comes to mind is $H \subseteq K.$ Other one is $\frac{G}{N} \cong \frac{K}{N \cap K}$ and so $\left|\frac{G}{N}\right| = \left|\frac{K}{N \cap K}\right|.$ Is there some other important information about $K$ that I am missing?","['group-extensions', 'semidirect-product', 'group-theory', 'normal-subgroups']"
4225093,Solving $f(x)-f(x+\alpha)=g(x)$,"Problem: Suppose $g:\mathbb{R} \to \mathbb{R}$ has period $1$ and is $\mathcal{C}^{\infty}$ . Let $\alpha \in \mathbb{R}$ . Consider the following equation: \begin{equation}
f(x)-f(x+\alpha)=g(x)
\end{equation} I am asked to: $1$ . Let $\alpha \in \mathbb{Q}$ . Find necessary and sufficient condition on $g$ such that the equation has a solution $\mathcal{C}^{\infty}$ with period $1$ . $2$ . Let $\alpha \in \mathbb{R} \smallsetminus \mathbb{Q}$ such that exist $\gamma >0$ and $\tau >0$ with $|\alpha - \frac{p}{q}|>\gamma q^{-2-\tau}$ for all $\frac{p}{q} \in \mathbb{Q}$ , $q \geq 1$ . Find necessary and sufficient condition on $g$ such that the equation has a solution $\mathcal{C}^{\infty}$ with period $1$ $3$ . Prove that the set of $\alpha$ of the second point has to measure $0$ in $\mathbb{R}$ . Attempt: If we define $$\tau_{\alpha}f(x)=f(x-\alpha) 
 \space \text{for}\space f:\mathbb{R} \to \mathbb{R}$$ then we would study $(\mathbb{I}-\tau_{-\alpha})f=g$ and a necessary condition for the first point is that $$\sum_{j=0}^kg(x+j\alpha)=0$$ where $k$ is such that $k\alpha \in \mathbb{Z}$ .","['functions', 'functional-analysis', 'analysis']"
4225113,Sections of projective bundles over the projective plane,"Consider a rank two vector bundle $\mathcal{E}$ over $\mathbb{P}^2$ , set $X = \mathbb{P}(\mathcal{E})$ , and let $\pi:X\rightarrow\mathbb{P}^2$ be the projection. I would like to show that $X$ contains a surface $W\subset X$ such that $\pi_{|W}:W\rightarrow\mathbb{P}^2$ is generically one to one, and $W$ contains at most finitely many fibers of $\pi:X\rightarrow\mathbb{P}^2$ . Let $L\subset\mathbb{P}^2$ be a line and $\mathcal{U} := \mathbb{P}^2\setminus\{L\}\cong\mathbb{A}^2$ . Then there is an isomorphism $\phi:\mathcal{U}\times\mathbb{P}^1\rightarrow X|_{\mathcal{U}}$ . Take two general polynomials $f,g:\mathcal{U}\rightarrow K$ , where $K$ is the base field, and consider the function $s:\mathcal{U}\rightarrow \mathcal{U}\times\mathbb{P}^1$ given by $s(x) = (x,f(x),g(x))$ . Then $\phi\circ s:\mathcal{U}\rightarrow X|_{\mathcal{U}}$ is a section of $X|_{\mathcal{U}}\rightarrow\mathcal{U}$ . Set $S = (\phi\circ s)(\mathcal{U})\subset X|_{\mathcal{U}}\subset X$ . Then $S$ is irreducible. Let $W$ be the closure of $S$ in $X=\mathbb{P}(\mathcal{E})$ . Then $W$ is irreducible as well. Now, $W$ might contain fibers of $\pi$ over $L$ . However, if $W$ contains infinitely many fibers of $\pi$ over $L$ , then it contains $\pi^{-1}(L)$ and hence $\pi^{-1}(L)$ would be a component of $W$ contradicting the irreducibility of $W$ . So $W$ contains at most finitely many fibers of $\pi$ . Is this argument correct? Thank you.","['projective-geometry', 'projective-space', 'vector-bundles', 'algebraic-geometry', 'affine-geometry']"
4225115,Problem based on mean value theorem and intermediate value theorem,"Let $f:[0,8]\rightarrow R$ be a differentiable function such that $f(0)=0, f(4)=1$ and $f(8)=1$ , then there exists some $c \in (0,8)$ where $f'(c)=1/12$ We need to ascertain whether it is always true? My approach using LMVT in [0,4] we can say there exists $c_1 \in (0,4)$ such that $f'(c_1)=\frac{1}{4}$ and using LMVT in [4,8] we can say there exists $c_2 \in (4,8)$ such that $f'(c_2)=0.$ Since $\frac{1}{12} \in (0,\frac{1}{4})$ using IVT we can say the statement is true. However, in the book answer is given statement is not always true. Please verify.","['calculus', 'derivatives']"
4225143,Strong Law Without Summable Tail Probabilities,"Suppose that $X_{k}$ are independent but not necessarily identically distributed.  Let $S_{n} = \sum_{k=1}^{n}X_{k}$ .  Does there exist an example where $\frac{1}{n}S_{n}\rightarrow 0$ almost surely, but $\sum_{n=1}^{\infty}\mathbb{P}(|\frac{1}{n}S_{n}| > \varepsilon)=\infty$ for sufficiently small $\varepsilon$ ?",['probability-theory']
4225160,On connectivity of Lie subgroups,"Here we assume that all manifolds are second-countable. Let $ G $ be a Lie group and $ H $ be its Lie subgroup. That is, $ H $ is a subgroup of $ G $ equipped with a differential structure with which $ H $ is a Lie group and the inclusion map $ \iota\colon H \to G $ is an immersion. Let $ H' $ denote $ H $ as a topological subspace of $ G $ . Note that the topology of $ H $ is generally different from that of $ H' $ . I am thinking about the relationship between the following four conditions: $ H $ is connected; $ H $ is path-connected; $ H' $ is connected; $ H' $ is path-connected. Here is what I understood: 1 and 2 are equivalent since $ H $ is a manifold. 1 (or, equivalently, 2) implies 3 and 4 since a continuous image of a connected (resp. path-connected) is connected (resp. path-connected). 3 does not imply 1. To see this, let $ G = \mathbb{T}^2 = \mathbb{R}^2/\mathbb{Z}^2 $ and $$ H = \{(x, \sqrt{2} x) + \mathbb{Z}^2 \mid x \in \mathbb{R}\} \cup \{(x + 1/2, \sqrt{2} x) + \mathbb{Z}^2 \mid x \in \mathbb{R}\}. $$ Then $ H $ is a Lie subgroup of $ G $ which is isomorphic to $ \mathbb{R} \times \mathbb{Z}/2\mathbb{Z} $ , but $ H' \subseteq G $ is connected. 4 implies 3 (generality from General Topology), but the converse is not true as the above example shows. Now my question is: Does 4 imply 1? That is, if the subspace $ H' \subseteq G $ is path-connected, is the Lie group $ H $ (path-)connected? Remark. Without second-countability, there are trivial counterexamples such as $ (\mathbb{R}, \text{discrete}) \to (\mathbb{R}, \text{usual}) $ .","['smooth-manifolds', 'lie-groups', 'differential-geometry']"
4225165,Best book on general topology for functional analysis,I study functional analysis. From time to time I find that the basics of many concepts lie on the concepts of topology. I am familiar with basic concepts of topology. When I go deeper then wild things start. I am not interested in topology for the sake of topology. I need it for a deep understanding of functional analysis. Question 1: Is there any good book on topology for my purpose? Question 2: If I really want to be good in functional analysis do I need study the whole topology or I can be satisfied by the standard topology? Question 3: My question shows my lack of understanding and immaturity in mathematics.  I should not be distracted from the main curriculum. Yes or no? (My area of interest is harmonic analisys and operator theory. I am newbie).,"['harmonic-analysis', 'general-topology', 'book-recommendation']"
4225206,Evaluating the limit $\lim\limits_{n\to\infty} \frac{2^{n^{k}}}{n!}$,"Determine, with proof, the value of the limit $L = \lim\limits_{n\to\infty} \dfrac{2^{n^{k}}}{n!}$ where $k=1.1.$ I think it's infinite because $\frac{2^{n^k}}{n!}\ge (\frac{2^{n^{0.1}}}{n})^n=(2^{n^{0.1}-\log n})^n\to \infty.$","['limits', 'calculus', 'asymptotics', 'real-analysis']"
4225227,Calabi Yau threefolds,"Let be $X$ a Calabi-Yau manifold (i.e. Kahler, compact with trivial canonical bundle) with $\dim(X) = 3$ . Let $\phi : A \mapsto B$ a proper holomoprhic submersion such that $X_{t_{0}} := \phi^{-1}(t_{0}) = X$ for some $t_{0} \in B$ . I consider the following map $t \mapsto H^{3,0}(X_{t}) \subset H^{3}(X_{t}) \simeq H^{3}(X)$ which is valued in $\mathbb{P}(H^{3}(X))$ (since $H^{3,0}(X_{t})$ is of dimension $1$ by Serre duality). I would like to show near $t_{0}$ , the image is a manifold. This comes from Voisin's book  Hodge theory and complex algebraic geometry, chapter 10. I already know from this book that this map is a holomorphic immersion and that is differential at $t_{0}$ maps $T_{t_{0}}B$ in $\operatorname{Hom}(H^{3,0}(X), H^{2,1}(X))$ . I wish you a good day.","['hodge-theory', 'complex-geometry', 'algebraic-geometry', 'kahler-manifolds']"
4225240,Derivatives of a quadratic form.,I have this quadratic function $$\bar x^T Q \bar x$$ and I need to find the first and second derivative. My question is how can I do that by just using the matrix from? I've been multiplying $Q$ by $\bar x$ and then taking it back to matrix form but this is too impractical. Thanks for the help.,"['derivatives', 'calculus', 'linear-algebra']"
4225252,Maximization of velocity involving trigonometric functions,"I have a physics question but my main doubt is regarding the math part of it, namely the maximization of the velocity. I'll attach a diagram of the question here for reference. I am given the values of coefficients of friction, $\mu_s$ (static friction coefficient) = $\frac{1}{\sqrt3}$ , $\mu_k$ (kinetic friction coefficient) = $\frac{1}{\sqrt5}$ It is given that the point A describes a circle in the vertical plane at a constant speed $v_A$ , the question asks me to find the maximum speed $v_A$ such that the block B doesn't slide on the platform. Without showing, the main physics part of it, (I can provide the working if needed though), what I'm obtaining from this is $$v_A^2=\frac{4}{\sqrt3 \cos \theta +\sin \theta}$$ Now, to maximize $v_A$ , clearly I need to minimize the denominator here. Now, I know, $$-2 \leq\sqrt3 \cos \theta+\sin \theta \leq 2$$ Now, $v_A^2$ has to be positive and so $-2$ clearly cannot be the required minimum value of the denominator. However, I find that the answer to this question is that $v_A = \sqrt2$ which means that $v_A^2=2$ and that means that the value of $\sqrt3 \cos \theta + \sin \theta$ has been taken to be $2$ which is the maximum value of it. But, shouldn't that make the value of $v_A$ minimum, as the denominator becomes maximum? EDIT: According to http://ilin.asee.org/Conference2007program/Papers/Conference%20Papers/Session%202B/Liang.pdf ""The normal analytical approach to this problem has two steps. In the first step, using the x- and the y- components (or the normal and the tangential components) of the equation of motion, an equation of maximum velocity vA without sliding is found as $$(v_{A(max)})^2 =\frac{\mu_s g \rho} {(\cos \theta + \mu_s \sin \theta)}$$ where $g$ is the gravitational acceleration, $\rho$ is the radius of curvature, θ is the angle of crank OA as shown in the Fig 6, and max VA is the velocity permissible by the friction for the particular crank angle θ. In the second step, the minimum of $(v_{A(max)})^2$ and the corresponding θ are found using calculus or Excel Solver. Here the above two sentences must be written carefully to tell what is maximized and what is minimized. The same possible confusion can arise in the students.""","['optimization', 'trigonometry', 'maxima-minima', 'physics']"
4225271,A complete proof of the Lagrange multipliers theorem using the implicit function theorem,"While searching on MSE, I couldn't find a complete rigorous proof the method of Lagrange multipliers using the implicit function theorem. I tried to write a complete proof myself below, but am not sure about some details, mainly the reordering of coordinates part. Any advice is very appreciated. Theorem. Let $f:\mathbb{R}^d\to\mathbb{R}$ and $h:\mathbb{R}^d\to\mathbb{R}^n$ be continuously differentiable functions, $C\in\mathbb{R}^n$ , and $M=\{h=C\}$ . Assume that $\text{rank } h'(x)=n$ for all $x\in M$ . If $f$ attains a constrained local extremum at $a$ , subject to the constraint $h(a)=C$ , then there exists $\lambda_1,\dots,\lambda_n\in\mathbb{R}$ such that $$\nabla f(a)=\lambda_1\nabla h_1(a)+\dots+\lambda_n\nabla h_n(a), \quad \quad (1)$$ where $\nabla f(a)=[\frac{\partial f}{\partial x_1} (a),\dots,\frac{\partial f}{\partial x_d} (a)]^\top$ and $\nabla h_i(a)=[\frac{\partial h_i}{\partial x_1} (a),\dots,\frac{\partial h_i}{\partial x_d} (a)]^\top$ for $i=1,\dots,n$ . Proof. Suppose $f$ attains a contrained local extremum at $a$ . First note that $h'(a)=[\nabla h_1(a),\dots, \nabla h_n(a)]^\top$ is $n\times d$ , and so $\text{rank } h'(x)=n$ implies that $n\leq d$ and that $h'(a)$ has $n$ linearly independent columns. If $n=d$ , then $h'(a)$ is invertible, and we can write $f'(a)=\Lambda h'(a)$ with $\Lambda:=f'(a)[h'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n]$ . Transposing gives $(1)$ . Hence assume WLOG that $n<d$ . Next we claim that we can assume WLOG that the first $n$ columns of $h'(a)$ are linearly independent. Otherwise there exists a permutation $\pi:\{1,\dots,d\}\to \{1,\dots,d\} $ that reorders the columns of $h'(a)$ so that it holds. Define $f^*:\mathbb{R}^d\to\mathbb{R}$ and $h^*:\mathbb{R}^d\to\mathbb{R}^n$ by $$f^*(x_1,\dots,x_d)=f(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}), \quad h^*(x_1,\dots,x_d)=h(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)})$$ Then we have $$\frac{\partial f^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial f}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq j\leq d,$$ $$\frac{\partial h_i^*}{\partial x_j}(x_1,\dots,x_d)=\frac{\partial h_i}{\partial x_{\pi(j)}}(x_{\pi^{-1}(1)},\dots,x_{\pi^{-1}(d)}) \quad 1\leq i\leq n, \quad 1\leq j\leq d$$ and so the continuous differentiability of $f,h$ implies the continuous differentiability of $f^*,h^*$ . Let $a^*=(a_{\pi(1)},\dots,a_{\pi(n)})$ . Then $$\frac{\partial f^*}{\partial x_j}(a^*)=\frac{\partial f}{\partial x_{\pi(j)}}(a) \quad 1\leq j\leq d,$$ $$\frac{\partial h_i^*}{\partial x_j}(a^*)=\frac{\partial h_i}{\partial x_{\pi(j)}}(a)  \quad 1\leq i\leq n, \quad 1\leq j\leq d$$ It follows that the columns of $f^{*'}(a^*)$ and $h^{*'}(a^*)$ are those of $f'(a)$ and $h'(a)$ permuted according to $\pi$ . Moreover we see that $f^*$ attains a  constrained local extrumum at $a^*$ , subject to the constraint $h^*(a^*)=C$ . Since condition $(1)$ for $f,h$ at $a$ is equivalent to condition $(1)$ for $f^*,h^*$ at $a^*$ , we have reduced the problem to the case where the first $n$ columns of $h'(a)$ are linearly independent. As a last reduction, we may assume WLOG that $C=0$ , for otherwise we can replace $h$ by $h-C$ . Let $m=d-n$ and denote points in $\mathbb{R}^d=\mathbb{R}^{n+m}$ by $(x,y)$ with $x\in\mathbb{R}^n$ and $y\in\mathbb{R}^m$ . Then $h$ satisfies  the conditions of the implicit function theorem at the point $a=(a_x,a_y)$ , as found in Rudin PMA Theorem 9.27 for example. Therefore, there exists open sets $U\subset \mathbb{R}^{n+m}$ and $W\subset \mathbb R^m$ with $(a_x,a_y)\in U$ and $a_y\in W$ , and a continuously differentiable function $g:W\to\mathbb R^n$ such that $g(a_y)=a_x$ and $h(g(y),y)=0$ for all $y\in W$ . In particular we have $$\{(g(y),y):y\in W\}\subset M \quad \quad (2)$$ Define the function $F:W\to \mathbb R$ by $F(y)=f(g(y),y)$ . This functions is differentiable on $W$ , and the chain rule gives $F'(y)=f'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix}$ . Since $a_y\in W$ and $g$ is continuous at $a_y$ , it follows from $(2)$ that $F$ has an unconstrained local extremum at $a_y$ . For if there exists an $r>0$ such that $f|_{M\cap B_r(a)}$ has an unconstrained extremum at $a$ , then by choosing $\delta>0$ sufficiently small we obtain $(g(y),y)\in M\cap B_r(a)$ whenever $y\in W\cap B_{\delta}(a_y)$ . Using the fact that the derivative must vanish at a local extremum, and partitioning $f'(a)=[f_x'(a) \quad f_y'(a)]$ , we get $$0=F'(a_y)=[f_x'(a) \quad f_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=f_x'(a)g'(a_y)+ f_y'(a)  \quad \quad (3)$$ Next consider the function $H:W\to\mathbb R^{n+m}$ defined by $H(y)=h(g(y),y)$ . This functions is differentiable on $W$ , and the chain rule gives $H'(y)=h'(g(y),y)\begin{bmatrix} g'(y)\\ I_m \end{bmatrix}$ . Moreover we have $H=0$ by definition of $g$ , and so in particular $$0=H'(a_y)=[h_x'(a) \quad h_y'(a)]\begin{bmatrix} g'(a_y)\\ I_m \end{bmatrix}=h_x'(a)g'(a_y)+ h_y'(a) \quad \quad (4)$$ where we partioned $h'(a)=[h_x'(a) \quad h_y'(a)]$ . Since the first $n$ columns of $h'(a)$ are linearly independent, $h_x'(a)$ is invertible, and from $(4)$ we get $$g'(a_y)=-[h_x'(a)]^{-1}h_y'(a) \quad \quad (5)$$ Substituting $(5)$ in $(3)$ and solving for $f_y'(a) $ we obtain $$f_y'(a) =\Lambda h_y'(a) \quad \quad (6)$$ where $\Lambda:=f_x'(a)[h_x'(a)]^{-1}:=[\lambda_1,\dots,\lambda_n]$ . But we also have $$f'_x(a)=f'_x(a)[h_x'(a)]^{-1}h_x'(a)=\Lambda h_x'(a) \quad \quad (7)$$ so combining $(6)$ and $(7)$ gives $$f'(a)=[f'_x(a) \quad f'_y(a)]=\Lambda [h_x'(a)\quad  h_y'(a)]=\Lambda h'(a)$$ Transposing gives $(1)$ , as desired.","['lagrange-multiplier', 'real-analysis', 'multivariable-calculus', 'optimization', 'derivatives']"
4225287,Proving terms using induction in LC,"Expanding on the following question here and on the book on the $\lambda$ -calculus I'm reading, I'm trying to prove the correctness of the given solution in a more complicated manner. Let $(F_n')_{n \in \mathbb{N}}$ be the family of terms defined by induction on $n \in \mathbb{N}$ as follows: \begin{align}
F_0' &= m & F_{n+1}' &= c \, \underline{n+1} \, F_n'
\end{align} where $c$ and $m$ are variables, and $\underline{n}$ is the Church numeral representing $n \in \mathbb{N}$ . I'm trying to prove by induction on $n \in \mathbb{N}$ that $$F'_n[\mathsf{times}/c,\underline{1}/m] →_\beta^* \underline{n!}$$ and thus, by setting $F_n = \lambda c. \lambda m. F_n'$ , $$F_n \, \mathsf{times} \, \underline{1} →_\beta^* \underline{n!}$$ where $!$ is factorial, and $\mathsf{times}$ is a term such that $\mathsf{times} \, \underline{a} \, \underline{b} \to_\beta^* \underline{a \times b}$ for any $a, b \in \mathbb{N}$ . How can I do it? The proof should hopefully prove that the solution given in the previous question is valid and demonstrate the functionality of foldr function from haskell.","['proof-writing', 'discrete-mathematics', 'lambda-calculus', 'induction', 'computer-science']"
4225293,Integers solutions to the equation $x^4 + 3y^4 = z^2$,"I would like to find the integer solutions to the equation $x^4 + 3y^4 = z^2$ . I know this has infinitely solutions, because I noticed if $x=y$ , we have $4x^4 = z^2 \Rightarrow 2x^2 = z$ (for positive integers). So for instance $(1,1,2), (2,2, 8), ...$ are solutions. How to find all the integer solutions to this equation?","['number-theory', 'diophantine-equations']"
4225315,"You can't subtract inequalities, but can you multiply by -1 then add?","Suppose $\{x_n\}, \{y_n\}$ are Cauchy sequences in metric space $(X, d)$ . Prove that the sequence $\{a_n\}$ defined by $a_n = d(x_n, y_n)$ converges in $\mathbb{R}$ . My Question. Before I write my proof attempt, I'll ask my question: I have the idea for this proof in mind, but I'm not so good at writing proofs using slick manipulations of inequalities. I came up with a manipulation that I want to use, but I'm not sure if I'm allowed to. So, my question is not necessarily about how to prove the above statement. (It has been proved on stack exchange here and here .) Rather my question is whether or not I'm allowed to manipulate inequalities in the way I have below, starting at the line ""In symbols, we have..."" My Proof. Choose $\varepsilon >0$ . Because $\{x_n\}$ is Cauchy, there exists $N \in \mathbb{N}$ such that $d(x_n, x_m) < \varepsilon/8$ for all $n, m \geq N$ . A similar $M \in \mathbb{N}$ exists for sequence $\{y_n\}$ . Choose $\max\{N, M\}$ and, without loss of generality, assume $\max\{N, M\} = N$ . Then, for all $n \geq N$ , we have $x_n \in B_{\varepsilon/4}(x_N)$ and $y_n \in B_{\varepsilon/4}(y_N)$ . Let $$D = \{d(x,y) : x \in B_{\varepsilon/4}(x_N) \text{ and } y \in B_{\varepsilon/4}(y_N)\}.$$ Then for any $x_n, y_n$ with $n > N$ , the distance $d(x_n, y_n)$ is bounded above by $\sup D = d(x_N, y_N) + \varepsilon/2$ and is bounded below by $\inf D = d(x_N, y_N) - \varepsilon/2$ . In symbols, we have $$\begin{align} d(x_N, y_N) - \varepsilon/2 &< d(x_n, y_n) < d(x_N, y_N) + \varepsilon/2. \tag{1} \end{align}$$ Inequality (1) is still true for any other $m > N$ , so substituting such an $m$ then multiplying through by $-1$ , we obtain $$\begin{align} -d(x_N, y_N) + \varepsilon/2 &> -d(x_m, y_m) > -d(x_N, y_N) - \varepsilon/2. \tag{2} \end{align}$$ Rearranging (2) so that its inequality signs face the same direction as (1), we can add (1) and (2) to obtain $$\begin{align} -\varepsilon < d(x_n,y_n) - d(x_m, y_m) &< \varepsilon, \text{ i. e.} \\ |d(x_n, y_n) - d(x_m, y_m)| &< \varepsilon, \text{ meaning that} \\ |a_n - a_m| &< \varepsilon. \end{align}$$ Therefore, $\{a_n\}$ is Cauchy in $\mathbb{R}$ , and so $\{a_n\}$ converges. (qed)","['inequality', 'metric-spaces', 'analysis', 'real-analysis']"
4225336,In a party attended by $2015$ guests among any $5$ guests at most $6$ handshakes had been exchanged. Determine the maximal number of handshakes.,"In a party attended by $2015$ guests among any $5$ guests at most $6$ handshakes had been exchanged. Determine the maximal possible number of handshakes. Turkey EGMO TST 2015 P6 Some findings: Say we have $m$ handshakes. If we connect every 5 tuple with edges in it we get $${2015 \choose 5}\cdot 6\geq m\cdot {2013 \choose 3} \implies m\leq 1209\cdot 1007$$ If we observe $H:=\bar{G}$ instead of $G$ then among any five we have at least 4 edges. Say $H$ has conected components $C_1,...,C_k$ , then $k\leq 2$ since else we don't necessary get $4$ edges. If $k=2$ then $C_1$ and $C_2$ must be clique, since however we take 2 vertices $u,v$ in one component (and 3 vertices in other component), they must be connected in order to get 4 edges in total of five vertices. So $G$ is complete bipartite graph and we have $m\leq 1007\cdot 1008$ . If $k=1$ , then $H$ is connected and now...?","['contest-math', 'graph-theory', 'combinatorics', 'discrete-optimization', 'extremal-graph-theory']"
4225377,Are there any ellipses that can be truly constructed from circular arcs?,"I know that it is possible to construct an approximation of ellipses using 4 arcs, but are there any ellipses that can be constructed from arcs with 100 percent accuracy? Usually, the construction would be slightly off : approximation vs real Is it possible the there exists an ellipse where this is not the case? And it can be perfectly represented using arcs? I imagine this is impossible. But in my research, I never managed to find a definitive answer.","['circles', 'geometry', 'geometric-construction']"
4225394,"Show that $\int_1^x \log^kt\log^l\frac{x}{t} dt = xQ(\log x),$ where $Q(t)$ is a polynomial with leading term $l!t^k$","I need to show that $$\int_1^x  \log^kt\log^l\frac{x}{t} dt = xQ(\log x),$$ where $Q(t)$ is a polynomial with leading term $l!t^k$ . Here $k,l$ are nonnegative integers. I wrote $\int_1^x  \log^kt\log^l\frac{x}{t} dt = \int_1^x  \log^kt(\log x -\log t)^l dt = \sum_{j=0}^{l} (-1)^j {l \choose j} \log^{l-j}x \int_1^x \log^{k+j}t dt$ . However I calculated that $\int_1^x \log^{k+j}t dt = xq_j(\log x)$ where $q_j(t)$ is a polynomial of degree $k+j$ . Therefore $\log^{l-j}x \int_1^x \log^{k+j}t dt$ should be of the form $xR(\log x)$ where $R(t)$ is a polynomial of degree $k+l$ . So its seems like I am getting a polynomial of the wrong degree and I'm not sure where the $l!$ coefficient would come from either.","['integration', 'calculus', 'analysis']"
4225401,"If the positive part of submartingale is uniformly integrable, it is closable","I am kind of confused in reading the next proof of a theorem in ""Stochastic Analysis: Itô and Malliavin Calculus"" in Tandem by Matsumoto & Taniguchi; Here, a closable submartingale $\{ Z_n \}_n$ is defined as a submartingale that is bounded by an integrable random variable $Z$ in $$
Z_n \leq \textrm{E}[Z|\mathcal{F}_n].
$$ Where I am stuck :
The equation in the middle, \begin{equation}
\mathrm{E}[Y|\mathcal{F}_n] = \lim_{m \rightarrow \infty} \mathrm{E}[X_m^+ | \mathcal{F}_n].
\tag{1}
\label{eq:question}
\end{equation} I don't see why we can get the limit out of the expectation. The text says ""since $X_n^+$ converges also in $L^1$ "", but I don't think when $X_n \rightarrow X$ in $L^1$ , it always holds for a sub- $\sigma$ -algebra $\mathcal{G}$ , $$
\textrm{E}[X_n | \mathcal{G}] \rightarrow \textrm{E}[X | \mathcal{G}] \quad \textrm{a.s.}
$$ For we have a counterexample: we take $\mathcal{G} = \mathcal{F}$ (where $\mathcal{F}$ is the $\sigma$ -algebra of the whole probability space) and $\{X_n\}$ such that $X_n \rightarrow X$ in $L^1$ but not $\textrm{a.s.}$ , then $$
\textrm{E}[X_n | \mathcal{F}] = X_n \not\rightarrow X = \textrm{E}[X | \mathcal{F}]  \quad \textrm{a.s.}
$$ So how can we validate Eq.\eqref{eq:question}?","['martingales', 'uniform-integrability', 'probability-theory', 'probability']"
4225415,Trivial Topology and Basis,"I recently started studying topology (Royden and Fitzpatrick's Real Analysis - 4th edition) and I stumbled upon this exercise: ""Show that the trivial (indiscrete) topology on a set $\, X \,$ has a unique base."" Consider a topological space $(X, \mathcal{T})$ , where $\mathcal{T} = \{\, \varnothing, \,X \, \}$ . My first reaction was ""The unique base obviously is $\mathcal{B}_1 = \{X \}$ "". But the more a thought about it, the more it became apparent that $\mathcal{B}_2 = \{\, \varnothing, \,X \, \}$ could also be a base. So, now, I'm wondering if it is something definitional that I'm missing. The way the book defines a base is as follows: Definition: For a topological space $(X, \mathcal{T})$ and a point $x \in X$ , a collection of neighborhoods of $x$ , $\mathcal{B}_x$ is called a base for the topology at $x$ if for any neighborhood $A$ of $x$ , there is $B \in \mathcal{B}_x$ such that $B \subseteq A$ . A collection of open sets $\mathcal{B}$ is called a base for the topology $\mathcal{T}$ provided it contains a base for the topology at each point $x$ . Note that considering $\mathcal{B}_1$ or $\mathcal{B}_2$ , which are both collections of open sets of $\mathcal{T}$ , for any arbitrary $x \in X$ we can define a collection of neighborhoods of $x$ , $\mathcal{B}_x = \{ X \}$ . Then considering any neighborhood $A$ of $x$ (In this case the only possible neighborhood is $A = X$ , we can take $X \in \mathcal{B}_x$ and it is clear that $X \subseteq X$ . By definition this would imply that there is not a unique base for $\mathcal{T}$ , in fact both $\mathcal{B}_1$ and $\mathcal{B}_2$ are basis for this topology. If anyone can point out the error I made in my explanation, or (preferably) side with my answer, it would be greatly appreciated.",['general-topology']
4225421,Can a sphere with dimension $2$ admit locally negative curvature？,"I know that by the Cartan-Hadamard theorem the sphere cannot admit a global negative curvature, which can also be proved by the Gauss-Bonnet formula. But can we equip the sphere with a metric such that its curvature is negative at some point？","['riemannian-geometry', 'differential-geometry']"
4225427,Visualising independence of events,"Let's say we have a fair five-sided die. The sides of the die are numbered from 1 to 5. Each die roll is independent and all faces are equally likely. We roll twice. Event A = the total of two rolls is 10 Event B = at least one roll resulted in 5 I get that these are clearly dependent. The $P(B\mid A) = 1$ because if you get two rolls = 10, they had to be 5 and 5, so clearly B occurs. But how would I visualize this on a Venn diagram? Like I'm not sure how the P(B|A) = mA intersect B/P(A) and it equals 1. To be clear, I know why the answer is so intuitively/logically but not sure how it would look visually (like Venn diagram) and mathematically.  How do we get a 1?","['statistics', 'conditional-probability', 'independence', 'discrete-mathematics', 'probability']"
4225429,"Do we expect that ""unsolvable"" differential equations would have an analytical solution if we simply knew more math?","In my engineering studies and while reading the book Chaos, I see a lot of mentions of complicated differential equations without solutions. For example, the equation $$\frac{dx}{dt}+\sin(x(t))=\sin(wt)$$ does not have an analytical solution as far as I know. Is there hope that if we had more functions at our disposal (for example, more functions like sine, hyperbolic sine, etc.) we would be able to find such a solution? Or is something like this fundamentally unsolvable for some reason? If it would be possible, are mathematicians working to discover these new mathematical terms? It fascinates me that we don't have the math to cleanly describe the three-body problem, for example, and it's hard to imagine that a clean solution wouldn't exist if we simply knew more.","['ordinary-differential-equations', 'partial-differential-equations']"
4225511,Riemann vs. Darboux integrability for multiple integrals,"It is well-known that for bounded functions of one variable, Darboux integrability (via upper and lower sums) is equivalent to Riemann integrability (via Riemann sums.)
I tried to generalize this to functions of two variables, and I encountered some technical issues; consider, for example, a bounded function in two variables defined on a unit disk; the lower sum is built from the little squares inside the disk and infimum of function values, while the upper sum is built from the little squares intersecting the disk and supremum of function values; ( Notice that there are more little squares for the upper sum than for the lower sum. ) then one defines the Darboux integrability as the lower sum and upper sum being equal in some limit as the little squares become smaller; to make this sandwiching to work via monotone convergence theorem, one needs to assume that the function is nonnegative. A general bounded function is Darboux integrable if it is the difference between two nonnegative Darboux integrable functions. As for the Riemann integrability, the Riemann sum is built from the little squares inside the disk and  function values at an arbitrary point in each little square. It is straightforward to show that Darboux integrability implies Riemann integrability. What I'm having trouble with is the converse: Does Riemann integrability imply Darboux integrability? I can prove this only if the function is nonnegative; since upper and lower sums are used for nonnegative functions only, the usual technique cannot be applied for signed functions. Is there a way around it? Or, could it be that Riemann integrability does not imply Darboux integrability for functions of more than one variable?","['integration', 'multivariable-calculus', 'multiple-integral', 'riemann-sum', 'riemann-integration']"
4225525,Proof of the Laplace Expansion? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I just learned about Laplace Expansion for determinant calculation in high-school, they taught me how to calculate minors cofactors and everything but they did not include the proof in the book. I have tried watching YouTube videos and there is a proof on Wikipedia which I cannot understand anything. My teacher says she doesn't know the proof. Is the proof too hard for a high school student?","['matrices', 'determinant', 'linear-algebra', 'laplace-expansion']"
4225532,Venn diagram problem with three beverages liking,"In a group of people who like beverages tea, coffee and milk, 9 people like tea, 9 people like coffee and 15 people like milk. If 5 people like tea and milk, 7 people like coffee and milk and 3 people like all three beverages. How many people like tea and coffee but not milk? The total number of people is 20. To my understanding, there are 15 people who like milk. These 15 includes 3 that likes all the three. Since 5 likes both tea and milk, this is included with the 3 that likes all the three beverages. So, 2 likes both tea and milk only as per the Venn diagram. Similarly, 4 likes only coffee and milk. Going by this, I tried to find those that like only tea and coffee (x) which is 9-5-x under tea side (refer attached diagram). The x should be the same as 9-7-x for coffee. The equation 9-5-x=9-7-x is not solvable. I know I am wrong somewhere. Help appreciated.",['elementary-set-theory']
4225581,What does the equality $\|Ax\|_2=\|A\|_2 \|x\|_2$ mean?,"We know that the matrix norm $\|A\|_2$ is compatible with the euclidean vector norm $\|x\|_2$ , i.e. $\|Ax\|_2 \leq \|A\|_2 \|x\|_2$ for all matrices $A$ and vectors $x$ . I am trying to understand what happens in the case that the equality is obtained, so $\|Ax\|_2=\|A\|_2 \|x\|_2$ . Does there exist a (for example) geometric interpretation? Or something like, this equality does only hold for unitary matrices? I am sorry about the confusion, I will try to clarify by splitting my question: Does $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ only hold for special matrices, e.g. unitary matrices? What happens geometrically, if $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ ? What happens geometrically, if $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for a special $x$ ? What else does $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ tell you?","['matrices', 'inner-products', 'normed-spaces', 'linear-algebra']"
4225615,Calculating line Integral with polar coordinates,"I want to calculate the following line integral using polar coordinates and verify my calculation is correct. $$\oint\limits_{C}2x+y^2ds\, \\C=\{(x,y)\in \Bbb{R}^2 | x^2+3y^2=8\}$$ My conversion to polar coordinates is the following $$\gamma: [0,2\pi) \rightarrow \Bbb{R}^2 \\\gamma(t) = \begin{pmatrix}\sqrt8\cos(t)\\\ \frac{\sqrt8\sin(t)}{\sqrt3} \end{pmatrix}$$ Now it should be $$\oint\limits_{C}2x+y^2ds\, = \int\limits_{0}^{2\pi}4\sqrt2\cos(t)+ \frac{8\sin^2(t)}{3} dt = \frac{8\pi}{3}$$ Is this correct?","['multivariable-calculus', 'calculus', 'polar-coordinates']"
4225627,calcuate $\sum_{i=0}^{n} 2^{2i}$,I want to calcuate this problem: $\sum_{i=0}^{n} 2^{2i+5}$ I know that we can expand this problem like this: $\sum_{i=0}^{n} (2^{2i+5})$ $=\sum_{i=0}^{n} (2^5 \times 2^{2i})$ $=\sum_{i=0}^{n} (32 \times 2^{2i})$ $=32\sum_{i=0}^{n} (2^{2i})$ But I stopped here. is there any way to calcuate $\sum_{i=0}^{n} (2^{2i})$ ?,"['algebra-precalculus', 'geometric-progressions', 'summation']"
4225634,What does continuity of probability even mean intuitively?,"I'm reading ""All of Statistics A Concise Course in Statistical Inference"" by Larry Wassermann, and I came across a proof for continuity of probability like this : $\text{Theorem (1.8)(Continuity of Probabilities)}$ If $A_{n} \rightarrow A$ then $$ \lim_{n \rightarrow \infty} \big( \mathcal{P}(A_{n}) \big) \rightarrow \mathcal{P}(A)$$ Suppose that $A_{n}$ is monotone increasing so that $A_{1} \subset A_{2} \subset $ Then let $A = \lim_{n \rightarrow \infty} A_{n} = \bigcup_{i=1}^{\infty} A_{i}.$ Define $B_{1} = A_{1}$ ,and also define the set $B_{2} = \big\{ \omega \in \Omega: \omega \in A_{2} , \omega \notin A_{1}\big\} $ $B_{3} = \big\{ \omega \in \Omega: \, \omega \in A_{3}, \omega \notin A_{2}, \omega \notin A_{1} \big\}, ... $ It can be shown that $B_{1}$ , $B_{2}$ , … are disjoint, $A_{n} = \bigcup_{i=1}^{n}A_{i} = \bigcup_{i=1}^{n}B_{i}$ for each $n$ and $\bigcup_{i=1}^{\infty}B_{i} = \bigcup_{i=1}^{\infty} A_{i}$ $\big( \text{Exercise (1)} \big)$ From Axiom $(3)$ , one can say that $$\mathcal{P(A_{n})} = \mathcal{P} \bigg( \bigcup_{i = 1}^{n} B_{i} \bigg) = \sum_{i = 1} \mathcal{P}(B_{i}) $$ Using axiom $(3)$ again, $$\lim_{n \rightarrow \infty} \mathcal{P}(A_{n}) = \lim_{n \rightarrow \infty} \sum_{i = 1}^{n} \mathcal{P}(B_{i}) = \sum_{i = 1}^{ \infty} \mathcal{P}(B_{i}) = \mathcal{P} \bigg( \bigcup_{i=1}^{\infty} B_{i} \bigg) = \mathcal{P}(A).$$ So here I understand how the proof goes, as in various steps in the proof, but I don't get why and what are we trying to prove. Since I don't have a background in pure mathematics and measure theory and I'm having a hard time wrapping my head around this. I don't get what exactly are we trying to prove here :
let's say $A_{n}$ is an event so are we trying to prove that as the event grows to infinity(since A is monotonically increasing) the probability of that event approaches the probability of sample space, which is essentially equal to $1$ ? What does this intuitively mean, or what kind of pre-requisites do I need for understand this text? EDIT : I would also appreciate if someone can point out some other text books which would help me understand these things better as someone with little knowledge in pure math.","['continuity', 'measure-theory', 'statistics', 'probability']"
4225638,Is there a situation such that the 'magnitude' of discriminant is important?,"For example, we know x^2-x+1 = 0 has discriminant -3 , its sign is minus, so it has two different non-real roots. The sign of discriminant is also useful for inspecting 'root profile' of higher(cubic, quartic) degree polynomials. Whether it has any multiple roots or not, whether it has non-real root.. Also I heard that the sign of discriminant plays a role to judge whether the polynomial can be solved purely algebraically. My question is : is there any meaningful or useful aspect of the magnitude(=absolute value) of
discriminant ?
If not.. then personally I want to name 'sign of discriminant' to 'root indicator', and will not use the term 'discriminant'. The comments/answers so far are about quadratic equation. Hope there is something for cubic,quartic,.. or general n-th degree equation.","['algebra-precalculus', 'discriminant']"
4225652,Difference of $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ and as $f: \mathbb{C} \rightarrow \mathbb{C}$.,"I want to know the difference of differentation as $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ and $f: \mathbb{C} \rightarrow \mathbb{C}$ . What are their differences, $f$ as two real variables, or $f$ as differentiation as a complex function? This question arose when I took the youtube lectures by ""Richard E. BORCHERDS"" on complex analysis. First treatment of real analysis : In multivariable calculus, when we set $f(x,y)$ its total derivatives is written as \begin{align}
df =f_x dx + f_y dy 
\end{align} where $f_x, f_y$ are partial derivatives with respect to $x,y$ Formally, we say that a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ is differentiable at $a \in \mathbb{R}^2$ if it exists a continuous linear map $\nabla f(a) : \mathbb{R}^2 \rightarrow \mathbb{R}^2$ such that \begin{align}
\lim_{h \rightarrow 0} \frac{f(a+h) - f(a) - \nabla f(a) \cdot h}{\|h\|} =0
\end{align} so when we consider multivariable calculus, we have to see whether the multivariable function have a partial derivatives(or directional derivatives) and then see the above limit holds[In the calculus, we learn that a function having a partial derivatives but not differentiable, i.e., $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$ at $(x,y) \neq (0,0)$ but $0$ at $(x,y)=(0,0)$ . ] In the complex analysis, we treat $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ or $f: \mathbb{C} \rightarrow \mathbb{C}$ and define complex derivatives analogus to real derivatives and obtain Cauchy Riemann equation. For example $w=u+iv$ , \begin{align}
  \begin{pmatrix}
    u(x,y) \\
    v(x,y)
  \end{pmatrix} = \begin{pmatrix}
    u(x_0, y_0) \\
    v(x_0, y_0)
  \end{pmatrix} + \begin{pmatrix}
     \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \\
     \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}
  \end{pmatrix} \begin{pmatrix}
    x-x_0 \\
    y-y_0
  \end{pmatrix} + \epsilon
\end{align} and doing $w$ as \begin{align}
w=w_0 + A (z-z_0) + \epsilon, \quad A \in \mathbb{C}
\end{align} [This is Borcherds treatment of differentiation as a linear approximation. Like real case he treats $w$ as $\mathbb{C}$ and does the linear approximation on $\mathbb{C}$ ] then identifying the component of $A$ he obtain Cauchy Riemann equation. In complex cases, I feel Borcherds treat the differentiation as $x,y$ and $z$ equally, but in general case those two approaches are different am I? For example, when dealing with complex analysis, differentiable at some open region (analytic) implies $C^{\infty}$ but I know in multi-variable calculus this does may not happen. What are their differences, $f$ as two real variables, or $f$ as differentiation as a complex function?","['complex-analysis', 'multivariable-calculus', 'calculus', 'derivatives']"
4225661,A question about notation in matrix calculus: $\dfrac{\partial Ax}{\partial x}=A^T$ or $ \dfrac{\partial (Ax)^T}{\partial x}=A^T$?,"I was looking for an explanation of the derivation of quadratic forms in matrix calculus, and during my search I came across two different, seemingly contradictory, identities. On the one hand, some derivation used $\dfrac{\partial \mathbf{Ax}}{\partial\mathbf{x}}=\mathbf{A^T}$ , which implies that $\dfrac{\partial \mathbf{x}}{\partial\mathbf{x}}=\mathbf{I}$ . On the other hand, my book on matrix algebra for example states that $ \dfrac{\partial \mathbf{(Ax)^T}}{\partial\mathbf{x}}=\dfrac{\partial \mathbf{x^TA^T}}{\partial\mathbf{x}}=\mathbf{A^T}$ , which is quite the opposite of what I found elsewhere. I can follow the derivation in both cases, but I would like to understand the deeper context, and why there are different approaches? Which of the two notations is the most common?","['derivatives', 'matrix-calculus', 'linear-algebra', 'quadratic-forms']"
4225701,Inductively prove $1 + \frac12 + \frac14 + \cdots + \frac{1}{2^n} = 2 - \frac{1}{2^n}$.,Inductively prove that the formula holds for all $n\in\Bbb{N}$ : $$1+\frac{1}{2}+\frac{1}{4}+\cdots+\frac{1}{2^n}=2-\frac{1}{2^n}.$$ What I have so far: base: n = 1: $$1+\frac{1}{2}=2-\frac{1}{2}=1.5$$ inductionstep: n = k: $$1+\frac{1}{2}+\cdots+\frac{1}{2^k}=2-\frac{1}{2^k}$$ inductionhypothesis: n=k+1: $$1+\frac{1}{2}+\cdots+\frac{1}{2^k+1}=1+\frac{1}{2}+\cdots+\frac{1}{2^k}+\frac{1}{2^(k+1)}=(2-\frac{1}{2})+\frac{1}{2^k+1}$$ This is where I am stuck and not sure what to do next.,"['induction', 'discrete-mathematics']"
4225729,Corollary of Uniform boundedness principle,"Let $X$ be banach space, $Y$ be normed space, $\mathcal A \subset \mathcal B(X, Y)$ be some set of continuous linear operators $X \to Y$ . I need to prove that if $$\forall x \in X, g \in Y^* \;\; \sup_{A \in \mathcal A} |g(Ax)| < \infty,$$ then $$\sup_{A \in \mathcal A} ||A|| < \infty.$$ I have managed to use uniform boundedness principle to deduce $$\forall g \in Y^* \sup_{A \in \mathcal A} ||g \circ A|| <\infty.$$ I have no idea how to proceed.",['functional-analysis']
4225743,Prove the set of reals with the Zariski topology is not metrizable.,"I cannot use facts involving Hausdorff spaces, as this problem expects knowledge a little bit more elementary. I am mostly confused with the statement ""A topological space (X, $\tau$ ) is metrizable if there exists a metric d such that $\tau$ is the topology induced by d."" The wording here is a little confusing to me. I can vaguely intuitively see that the open balls that construct each U $\in$ $\tau$ must involve the metric d, but as for how this indicates whether or not it ""induces"" something I am lost at. The definition of a Zariski topology I have been given is: $\tau$ = {U $\subseteq$ $\mathbb{R}$ : U = $\emptyset$ or U = ( $\mathbb{R} \backslash S$ ) where S is some finite subset of $\mathbb{R}$ } I am thinking to use the method of contradiction here, but without a firm sense of the definitions, I'm having logical difficulties proceeding. Moreover, should I pick an arbitrary set S that fulfills $\tau$ 's property? Lastly, I was confused on how the Zariski topology is even a topology according to this definition. By the properties of a topology $\tau$ on $\mathbb{R}$ , we require that $\mathbb{R} \in \tau$ . But if for nonempty elements of $\tau$ we have that none contain s $\in$ S, (assuming S is non-empty. My logic here would imply that S must be empty always, so perhaps I am wrong somewhere in this interpretation), then there exists x $\in \mathbb{R}$ such that x $\notin \tau$ , demonstrating $\mathbb{R} \nsubseteq \tau$ . Clarification would be much appreciated.",['general-topology']
4225756,Can you transform the denominator of each term in an infinite series?,"Is there any way to transform the denominator in a power series?
For example, $$\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6}$$ and $$\sum_{n=1}^{\infty}\frac{1}{(4n)^2}=\frac{\pi^2}{96}.$$ My question asks if you can convert each term in an infinite zeta series from $n$ to $an+b.$ As shown above, scaling is easily done however I cannot think of any way to add to each denominator. i.e. $$ \sum_{n=1}^{\infty}\frac{1}{n^2} \to \sum_{n=1}^{\infty}\frac{1}{(an+b)^2}.$$","['power-series', 'sequences-and-series']"
4225771,Motivation of the Proof of the Hille-Yosida Theorem,"Let $X$ be a Banach space and $A$ be a linear map from a subspace of $X$ to $X$ . The Hille-Yosida theorem gives a necessary and sufficient condition for $A$ to be an infinitesimal generator of a semigroup of class $C_0$ .
The precise statement of the theorem is given in the following form. $A$ generates a semigroup of class $C_0$ , say $\left\{ S(t) \right\}_{t\geq0}$ such that $\Vert S(t) \Vert \leq Me^{\omega t}$ with $M>0$ and $\omega\in\mathbb{R}$ , if and only if $A$ is closed and $D(A)$ , the domain of $A$ , is dense in $X$ , every real $\lambda>\omega$ belongs to the resolvent set of $A$ and for such $\lambda$ and for all positive integers $n$ , $$\Vert (\lambda I - A)^{-n} \Vert \leq \frac{M}{(\lambda-\omega)^n}.$$ To simplify unnecessary complications, let us assume $\omega=0$ ; there is no loss of generality in assuming this. Moreover, let us denote the map $(\lambda I-A)^{-1}$ simply by $R_{\lambda}$ when $\lambda$ is in the resolvent set of $A$ . The only if part is easy to check. For the if part, we first construct a family of semigroups, one for each $\lambda>\omega=0$ , as follows: Let $A_{\lambda}:=-\lambda I + \lambda^2 R_{\lambda}$ and consider $$S_{\lambda}(t):=e^{tA_{\lambda}}=e^{t(-\lambda I + \lambda^2 R_{\lambda})}=e^{-\lambda t}\sum_{k=0}^{\infty} \frac{\lambda^{2k}t^k}{k!}R_{\lambda}^{k},\quad\text{for each } t\geq0.$$ It can be checked that $\left\{ S_{\lambda}(t) \right\}_{t\geq0}$ is a semigroup of class $C_0$ and satisfies the uniform bound $\Vert S_{\lambda}(t) \Vert \leq M$ , by using condition 2 from the statement of the theorem. It is then proved that for each $u\in D(A)$ and $t\geq0$ , the limit $$\lim_{\lambda\to\infty} S_{\lambda}(t)u$$ exists. Subsequently, it is proved that the limit actually exists for all $u\in X$ , and we denote this limit by $S(t)u$ . After checking subtle convergence and continuity issues, we can show that $\left\{ S(t) \right\}_{t\geq0}$ is a semigroup of class $C_0$ and that $\Vert S(t) \Vert \leq M$ holds. Finally, we show that the infinitesimal generator of this semigroup is exactly $A$ , completing the proof. My question is about how one can come up with the semigroups $\left\{ S_{\lambda}(t) \right\}_{t\geq0}$ in the above proof. What is the motivation for introducing them? I can see that if $\left\{T(t)\right\}_{t\geq0}$ is a semigroup, having $A$ as its infinitesimal generator, we must have $$T(t)u-S_{\lambda}(t)u=\int_{0}^{t} \frac{d}{ds}[S_{\lambda}(t-s)T(s)u]\,ds=\int_{0}^{t} -S_{\lambda}(t-s)T(s)(A_{\lambda}u-Au)\,ds$$ where $u\in D(A)$ and $t\geq0$ . Letting $\lambda\to\infty$ above implies that $T(t)u$ must be the limit of $S_{\lambda}(t)u$ , so we see that the semigroup $\left\{T(t)\right\}_{t\geq0}$ is actually unique and that the family of semigroups $\left\{ S_{\lambda}(t) \right\}_{t\geq0}$ , indexed by $\lambda$ , approximates the semigroup $\left\{T(t)\right\}_{t\geq0}$ as $\lambda$ approaches $\infty$ . The argument is very clear but I can't get the idea behind the construction of $\left\{ S_{\lambda}(t) \right\}_{t\geq0}$ . Can someone please explain this, with concrete examples if possible?","['ordinary-differential-equations', 'parabolic-pde', 'functional-analysis', 'semigroup-of-operators', 'spectral-theory']"
4225799,Standard matrices to test low rank decomposition,"I am working on a low rank decomposition technique that is robust to different types of noise (gaussian, salt and pepper, poisson). For starters, I simulated such low rank matrices and have successfully demonstrated that my technique works fairly well, and would like to test the method on more relevant matrices. As part of validation, I need to show how it performs on some commonly used low rank matrices. Is there such a repository of matrices? Here are some matrices that I have tried so far: Hyperspectral images (each column of the matrix is an image at a narrowband of wavelength) Eigenfaces (PCA of a covariance matrices constructed from images of faces) Videos (background subtraction) Thanks in advance for the help!","['applications', 'matrices', 'linear-algebra', 'numerical-linear-algebra', 'matrix-decomposition']"
4225800,"Does the Lie bracket relation $[\Gamma(D), \Gamma(E)] \subseteq \Gamma(E)$ imply the flow relation $d\theta_t(E) \subseteq E$?","Let $M$ be a smooth manifold and let $D, E \subseteq TM$ be two distributions on $M$ such that $$[\Gamma(D), \Gamma(E)] \subseteq \Gamma(E),$$ where $\Gamma(\cdot)$ is the space of smooth sections and $[\cdot, \cdot]$ is the Lie bracket of vector fields. Let $X \in \Gamma(D)$ and let $\theta_t$ be the flow of $X$ . Is it true that $$d\theta_t(E_p) \subseteq E_{\theta_t(p)}\tag{1}$$ for all $p \in M$ ? (In which case, it is actually an equality.) Some special cases are easy to check. For example, if $D = E$ , then $E$ is involutive, so the flow lines of $X$ are contained in the leafs of $E$ , and (1) follows. I'm not sure about the general case.","['smooth-manifolds', 'differential-geometry']"
4225824,"Differential equation $y' = (2y^2 + x)/(3y^2 + 5)$ (Apostol, section 8.28, ex. 30)","Problem This is from Apostol's Calculus book, section 8.28, exercise 30. Let $y = f(x)$ be that solution of the differential equation $$y' = \dfrac{2y^2 + x}{3y^2 + 5}$$ which satisfies the initial condition $f(0) = 0$ . (a) The differential equation shows that $f'(0) = 0$ . Discuss whether $f$ has a relative maximum or minimum or neither at 0. (b) Notice that $f'(x) \geq 0$ for each $x \geq 0$ and that $f'(x) \geq \frac{2}{3}$ for each $x \geq \frac{10}{3}$ . Exhibit two positive numbers $a$ and $b$ such that $f(x) > ax - b$ for each $x \geq \frac{10}{3}$ . (c) Show that $x/y^2 \to 0$ as $x \to +\infty$ . (d) Show that $y/x$ tends to a finite limit as $x \to +\infty$ and determine this limit. Solution attempt Please, I would like to ask for verification of my attempt below. (a) The second derivative is given by: $$y'' = \dfrac{(3y^2 + 5)(4yy' + 1) - 6yy'(2y^2 + x)}{(3y^2 + 5)^2}$$ At zero, the value is: $$f''(0) = \dfrac{5}{(5)^2} = \dfrac{1}{5}$$ Since this value is positive, we can conclude that the concavity of $f$ at 0 is up, so $f$ has a relative minimum at 0. (b) Since $f'(x) \geq \frac{2}{3}$ for each $x \geq \frac{10}{3}$ , in order to have the line $ax - b$ below $f(x)$ , we can choose $a = \frac{2}{3}$ . Also, we can choose $b$ such that $ax - b = 0$ at $x = \frac{10}{3}$ (since $f(x) \geq 0$ there). This gives: $$\frac{2}{3} \cdot \frac{10}{3} - b = 0 \implies b = \frac{20}{9}$$ So, the numbers are $a = \frac{2}{3}$ and $b = \frac{20}{9}$ . (c) In item (b), we found that: $$y > \frac{2}{3}x - \frac{20}{9}$$ for $x \geq \frac{10}{3}$ . This implies: $$\frac{3}{2} y + \frac{10}{3} > x$$ for $x \geq \frac{10}{3}$ . Dividing both sides by $y^2$ , this becomes: $$\dfrac{3}{2y} + \dfrac{10}{3y^2} > \dfrac{x}{y^2}$$ Also, we know that $x/y^2 \geq 0$ , so we have: $$\dfrac{3}{2y} + \dfrac{10}{3y^2} > x/y^2 \geq 0$$ Since we know that $y > \frac{2}{3}x - \frac{20}{9}$ , we have that $y \to +\infty$ as $x \to +\infty$ , so the left-hand side of the left-hand inequality above tends to zero as $x \to \infty$ : $$\dfrac{3}{2y} + \dfrac{10}{3y^2} \to 0\text{ as }x \to \infty$$ Therefore, by the squeeze theorem, it follows that $x/y^2 \to 0$ as $x \to +\infty$ . (d) From item (c), we know that $x/y^2 \to 0$ as $x \to +\infty$ . So: $$\begin{aligned}
    \lim_{x \to +\infty} y' &= \lim_{x \to +\infty} \dfrac{2y^2 + x}{3y^2 + 5} \\
    &= \lim_{x \to +\infty} \dfrac{2 + x/y^2}{3 + 5/y^2} \\
    &= \dfrac{2 + 0}{3 + 0} \\
    &= \dfrac{2}{3}
\end{aligned}$$ So, as $x \to +\infty$ , the function $y$ approaches a line of slope $\frac{2}{3}$ . That is, $y$ approaches a line of the form $y=\frac{2}{3}x + C$ , so $y/x \to \frac{2}{3}$ as $x \to +\infty$ . The argument in part (d) seems a bit informal; I'm not sure how to make it more rigorous.","['calculus', 'solution-verification', 'ordinary-differential-equations']"
4225848,Find the maximum and minimum of a multivariable function on a circle,"This question is a continuation from a previous question I recently asked: Stationary points of a multivariable function I now have to find the maximum and minimum values of my function on the circle: $x^2 +y^2 = 4$ I'll repeat what I have so you don't have to keep checking back to my previous post: The function: $f(x,y) = (x^2+2y^2)e^{-x^2-y^2}$ The partial derivatives: $f_x = (-2x(x^2+2y^2)+2x)e^{-x^2-y^2}$ , $f_y=(-2y(x^2+2y^2)+4y)e^{-x^2-y^2}$ From here I set up my Lagrange equations (Since I know that that $\nabla f = \lambda \nabla g$ where $g$ is the function of the given circle): (I also know that $e^{-x^2-y^2}$ simply becomes $e^{-4}$ since $x^2 +y^2 =4$ ) $$(-2x(x^2+2y^2)+2x)e^{-4} = 2\lambda x \space (1)$$ $$(-2y(x^2+2y^2)+4y)e^{-4} = 2\lambda y \space (2)$$ $$x^2+y^2 = 4 \space (3)$$ From here I wasn't entirely sure what to do but I tried dividing both sides of $(1)$ by $2x$ to get: $-e^{-4}(x^2+2y^2)+e^{-4} = \lambda$ , and similarly divide both sides of $(2)$ by $2y$ to get: $-e^{-4}(x^2+2y^2)+2e^{-4} = \lambda$ So my new Lagrange equations would be: $$-e^{-4}(x^2+2y^2)+e^{-4} = \lambda \space (4)$$ $$-e^{-4}(x^2+2y^2)+2e^{-4} = \lambda \space (5)$$ $$x^2 +y^2 = 4 \space (3)$$ I'm not even sure if what I did is correct and even so I'm not sure where I would go from here, equating $(4)$ and $(5)$ wouldn't help (At least I don't think it will) and so I'm quite confused on where to go from here or where I've made a mistake. If anyone can guide me in the right direction or show me a better method it would really help, thanks in advance","['maxima-minima', 'systems-of-equations', 'derivatives', 'stationary-point']"
4225889,Transform ODE system of first order into ODE of $n$-th order,"I know that you can always transform a higher order ODE into a system of first order ODEs. In our lecture the professor made the statement that the reverse, namely transforming a given system of first order ODEs into one ODE of a higher order, is not always possible. I was a bit confused by this statement because as far as I have understood the reverse is only possible in the very rare case where \begin{align}
y_0'&=y_1\\
y_1'&=y_2\\
&\;\vdots\\
y_{n-1}'&=y_n\\
y_n'&=f(x,y_0,y_1, \cdots, y_n).
\end{align} If the given system doesn't have this form you can't transform it into an ODE of $n$ -th order.
Is this correct? Did I miss something?","['ordinary-differential-equations', 'real-analysis']"
4225892,Proving the properties of polynomial functions without using calculus,"I have a little project of mine, where I'm basically trying to prove most
of the properties of polynomial functions without using any calculus whatsoever. I've been able to show, using the completeness of the real numbers, that if a polynomial function $f: \mathbf{R} \rightarrow \mathbf{R}$ is such that $f(a) f(b) < 0$ for some reals $a < b$ , then there exists a real number $c \in (a, b)$ such that $f(c) = 0$ . Then, I defined the concept of the derivative polynomial: Let $f: \mathbf{R} \rightarrow \mathbf{R}$ the polynomial function given by $\displaystyle f(x) = \sum_{k=0}^n a_k x^k$ for all $x \in \mathbf{R}$ . We define the derivative of $f$ as the polynomial function $f' : \mathbf{R} \rightarrow \mathbf{R}$ given by $\displaystyle f'(x) = \sum_{k=1}^n k a_k x^{k-1}$ for all $x \in \mathbf{R}$ . I am trying to show, by elementary means, that if $a < b$ and $f'(x) > 0$ for all $x \in (a, b)$ , then $f$ is increasing on $(a, b)$ , but I have no idea on how to do that.","['real-numbers', 'derivatives', 'polynomials', 'real-analysis']"
4225912,Find the nth derivative of $\cos(x^3)$,"This is a Calculus exam task: Let $f : \Bbb R \to \Bbb R$ be a function defined as $f(x) = \cos(x^3)$ . Calculate $f^{(25)}(0)$ . Searching for an answer online only yielded answers to questions such as $\cos^3(x)$ or $\cos(3x)$ , but not my example. I've tried deriving the function twice thus getting the following differential equation: $$y'' = -6x\sin(x^3)-9x^4y$$ While I do get only a single recursion by this method, I get stuck with a $(n-3)^{\text{rd}}$ derivative of $\sin(x^3)$ after applying the Leibniz formula: $$y^{(n)}=-6\sum_{k=0}^{n-2}\binom{n-2}{k}x^{(k)}\left(\sin(x^3)\right)^{(n-2-k)}-9\sum_{k=0}^{n-2}\binom{n-2}{k}(x^4)^{(k)}y^{(n-2-k)}$$ $$\Rightarrow y^{(n)}(0)=-6(n-2)\left(\sin(x^3)\right)^{(n-3)} -9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0)$$ I'm confused over the derivative operator's precedence. My question is whether it is legal math if I immediately plug $x=0$ as the sine's argument and then derive, thus only getting the following: $$y^{(n)}(0)\stackrel{?}{=}-9(n-2)(n-3)(n-4)(n-5)y^{(n-6)}(0)$$ If the above is true, then the given problem is fairly easy to finish: $$y^{(25)}(0) = (-9)^4\space\frac{23!}{19\cdot18\cdot13\cdot12\cdot7\cdot6}\space y'(0) = 0$$ However, something tells me that's not the way to go; otherwise all derivatives of anything may as well be 0, as they would be treated as constants when deriving. I did try to derive the second derivative a few more times, but the Leibniz formula then becomes a hot mess express. When I apply the formula over $y^{(4)}$ , which I found was the first one to have the sine function obscured under a $y$ variable, I get a multiple recursion by the means of $y^{(n)}=\ldots y^{(n-4)}+\ldots y^{(n-6)}+\ldots y^{(n-7)}$ Are there any other means of solving this task, perhaps by not using the Leibniz formula at all?","['calculus', 'derivatives']"
4225947,On a Geometric Proof (Ahlfors) that the Cross ratio is real if and only if four points lie on a circle or straight line,"The cross-ratio $(z_1,z_2,z_3,z_4)$ is real if and only if the four points lie on a circle or on a straight line. I know this question has been asked numerous times on MSE, but I have a specific question with respect to how Ahlfors proves this on page 79 of his book.  He says, ""This is evident by elementary geometry, for we obtain $$\text{arg}(z_1,z_2,z_3,z_4)=\text{arg}\frac{z_1-z_3}{z_1-z_4}-\text{arg}\frac{z_2-z_3}{z_2-z_4}$$ and if the points lie on a circle this difference of angles is either $0$ or $\pm\pi$ "" Could someone elaborate a bit on this for me?  I see that he's splitting up the cross-ratio using arguments, and I know that straight lines and circles are the same in $\mathbb{R}\cup\infty$ , but I am not seeing how the geometry proves both directions of the statement.  I would appreciate it very much!  Thank you.","['complex-analysis', 'geometry', 'cross-ratio']"
4225958,Central Limit Theorem for Divisor function,"A famous result by Erdős and Kac ( https://en.wikipedia.org/wiki/Erdős–Kac_theorem ) states that the number of prime factors of a ""random"" positive integer is approximately normally distributed. Namely, if $\Omega(n)$ denotes the total number of prime factors of $n$ , then for any $a$ , $$\frac{1}{x}\#\{n \leq x : \frac{\Omega(n)-\log \log n}{\sqrt{\log \log n}} \leq a\} \to \int_{-\infty}^{a}\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt, $$ as $x\to \infty$ . I was wondering if there is a similar result for $\log d(n)$ , where $d(n) = \sum_{d|n} 1$ .","['analytic-number-theory', 'number-theory', 'probability-theory']"
4225975,Sample size as a part of minimal sufficient statistic,"In Casella and Berger, we have the following experiment: assume we first select a positive integer randomly, let us call it $N \in \{1, 2, \ldots \}$ , s.t. $P(N = n) = p_n$ . Then, for this $N = n$ , we perform a classic Binomial experiment with parameter $\theta$ . The number of positives/ones in the sample is $X$ . It is asked to show that $(X, N)$ is a minimal sufficient statistic. I found the joint distribution of the statistic: $$f(x, n) = {n\choose x}\theta^x(1 - \theta)^{n - x}p_n$$ and the sample: $$f(x_1, \ldots, x_n \mid \theta) = \sum_{n = 1}^\infty p_n \theta^{\sum_{i = 1}^n x_i}(1-\theta)^{n - \sum_{i = 1}^n x_i}$$ By factorization theorem, we can show that this is a sufficient statistic. This series must be convergent (marginal), and we can assume that the above is a function of $X = \sum_{i = 1}^N X_i$ and $N$ , while taking $h(x) = 1$ . But how do I show it is a minimial statistic? I do not have the closed form, and I need to show that: $$\frac{f(x_1, \ldots, x_{n_1} \mid \theta)}{f(y_1,\ldots,y_{n_2} \mid \theta)} = C \iff (X, N_1) = (Y, N_2)$$ How can I show this? Additionally, the solution manual uses the joint of $f_{X, N}(x, n)$ to show the above, which is simple of course. But I think this is a mistake, because the theorem in the book requires to use the joint of the SAMPLE, and not the joint of the statistic. Theorem 6.2.13 page 281 Second Edition. Am I write considering this a mistake in the solution manual ^ PS. Solution manual link, http://www.ams.sunysb.edu/~zhu/ams570/Solutions-Casella-Berger.pdf exercise 6.12 (a). PPS. I think my confusion might come from defining the joint vs marginal distributions of $x_1, \ldots, x_n$ and $N = n$ . How can we define $P(x_1, \ldots, x_n \mid N = n)$ and $P(x_1, \ldots, x_n, n)$ ? If I observe a sample, then $N$ is a function of $x$ 's - I can count them. However, having a joint distribution, and then marginalising $N$ out also seems like a correct answer...","['statistical-inference', 'statistics']"
4226023,Probability $3$ integers $\le n$ are side lengths of triangle,"Here is a question from my probability textbook: Three different persons have each to name an integer not greater than $n$ . Find the chance that the integers named will be such that every two are together greater than the third. Here's what I did. After computing the cases up to $n = 7$ (which I'm not typing out here due to being too lazy), I was able to observe that we have the recursion $$p_1 = 1, \quad p_n = {{(n-1)^3 p_{n-1} + {{3n(n-1)}\over2} + 1}\over{n^3}}$$ However, I don't know how to solve it. Can anyone help me? Edit: I bountied the question. I'd like to see a complete self-contained solution solving the recurrence I give without reference to external sources such as OEIS, Wikipedia, etc.","['contest-math', 'combinatorics', 'recurrence-relations', 'probability']"
4226030,Solving $C\cos(\sqrt\lambda\theta)+D\sin(\sqrt\lambda\theta)=C\cos(\sqrt\lambda(\theta+2m\pi)) + D\sin(\sqrt\lambda (\theta + 2m\pi))$,"I want to solve $$C\cos(\sqrt\lambda \theta) + D\sin(\sqrt\lambda \theta) = C\cos(\sqrt\lambda (\theta + 2m\pi)) + D\sin(\sqrt\lambda (\theta + 2m\pi))$$ The solution must be valid for all $\theta$ in $\mathbb{R}$ and all $m$ in $\mathbb{Z}$ , but $C$ , $D$ , and $\lambda$ are to be determined and can be in $\mathbb{C}$ . The solutions I've found by guessing are $(C\ $ arbitrary $, D\ $ arbitrary $, \lambda = n^2)$ , where $n$ is any integer, and $(C = 0, D = 0, \lambda\ $ arbitrary $)$ . Is there some algebra I can do to show that these are the only solutions, or find the rest of the solutions to this equation?","['periodic-functions', 'trigonometry', 'solution-verification', 'multivalued-functions', 'algebra-precalculus']"
4226041,How do you visualize Stone-Čech compactification (construction using the unit interval)?,"I think I understand the mechanism of constructing Stone-Čech well from Wikipedia . However, I fail when trying to connect this with any concrete examples. For example, for the simplest examples, like $\mathbb{R}$ , how do we construct $\beta \mathbb{R}$ ? I really couldn´t find any explicit construction anywhere, so any example would be appreciated. I have read that ""we do not construct Stone-Čech compactification. we just define it and prove it exists"", but I still want to believe, haha.","['geometric-construction', 'topological-vector-spaces', 'compactification', 'general-topology', 'compactness']"
4226136,General solution to a certain simple recurrence relation,"This is follow up to a question answered here https://math.stackexchange.com/a/4224037/168758 Fix $\lambda \in [0, 1)$ . Define $m_{-1}(\lambda) = m_0(\lambda) = 1$ , and for $k \ge 1$ , define $m_k(\lambda) \in \mathbb R$ by $$
m_{k}(\lambda) = \frac{1}{k(1-\lambda)^2} \Big((2k-3)(1+\lambda) m_{k-1}(\lambda) - (k-3)m_{k-2}(\lambda)\Big). \quad (\star)
$$ It is easy to deduce that $m_1(\lambda)=\dfrac{1}{(1-\lambda)^2}(-(1+\lambda)+2) = \dfrac{1}{1-\lambda}$ and $m_2(\lambda) = \dfrac{1}{2(1-\lambda)^2} = \dfrac{1}{2(1-\lambda)^2}((1+\lambda)(1-\lambda)^{-1}+1)=\dfrac{1}{(1-\lambda)^3}$ . Question. What is a general formula for $m_k(\lambda)$ , perhaps in terms of well-known (special ?) functions, valid for all $k \ge 1$ ?","['recurrence-relations', 'special-functions', 'ordinary-differential-equations', 'real-analysis']"
4226137,"If $G$ is a nilpotent group and $H\leq G$ with $H[G,G]=G$ then $H=G$.","I am studying for a qualifying exam and this problem has been a white whale. Let $G$ be a nilpotent group with subgroup $H\leq G$ . If $H[G,G]=G$ , then $H=G$ . I believe I should use the fact that if $$G=G_0 \triangleright G_1 \triangleright \dots \triangleright G_k=1$$ is a central series (meaning $[G_i,G]\leq G_{i+1}$ for all $i$ ), then $$G=HG_0 \triangleright HG_1 \triangleright \dots \triangleright HG_k=H$$ is still subnormal. But I can't seem to get anywhere with this idea. Any help would be appreciated.","['derived-subgroup', 'nilpotent-groups', 'group-theory']"
4226202,"Prove that $a^a+b^b\ge a^b+b^a>1$ if $a,b> 0$. [duplicate]","This question already has answers here : Does $|x|^p$ with $0<p<1$ satisfy the triangle inequality on $\mathbb{R}$? (4 answers) Closed 2 years ago . I succeeded in proving the second part of the inequality by showing that for every real number $x,y ∈ (0, 1)$ , we have $x^y≥ \frac x{x + y − xy}.$ By Bernoulli’s inequality we have $x^{1−y}= (1 + x − 1)
^{1−y} 
   ≤ 1 + (x − 1)(1 − y) = x + y − xy,$ $\implies$ $x^y ≥ \frac x{x + y − xy}.$ If $a ≥ 1$ or $b ≥ 1$ then the given inequality clearly holds. So let $0 <a,b< 1.$ By the previous inequality we have $a^b + b^a≥ \frac a{a + b− ab}
+ \frac b{a + b− ab}= \frac {a+b}
{a + b− ab}
> \frac {a+b}{a+b}= 1.$ But, I found some difficulty on how to prove the first part. Thanks in advance","['calculus', 'real-analysis']"
4226211,Integral inequality $\int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2$. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Let $f \in H^1(0,\pi)$ , show that $\int^{\pi}_{0} f^2(x)dx \le \int^{\pi}_{0} (f')^2(x)dx + (\int^{\pi}_{0} f(x)dx)^2$ . $H^1(0,\pi) = W^{1,2}(0,\pi)$ , Sobolev space.
The question is from PDE course, but I guess it's true for all functions as long as integrals in the inequality are finite. I tried IBP, Holder inequality, but cannot get the answer.
Would appreciate any suggestions where to start. Thanks!","['integral-inequality', 'analysis', 'partial-differential-equations']"
4226243,Coordinate Independence of Differential Forms Understanding,"In A Geometric Approach to Differential Forms [Bachman, David], the author states the following: (On the geometric interpretation of 1-forms in $\mathbb{R^2}$ ) - 'We can interpret the act of multiplying by a constant geometrically. Suppose $\mathbb{w}$ is given by $adx+bdy$ . Then the value of $\mathbb{w}$ (V1) is the length of the projection of V1 onto the line, $l$ , where $\frac{<a,b>}{|<a,b>|^2}$ is a basis vector for $l$ . This interpretation has a huge advantage... it is coordinate free. Forms are objects that exist independently of our choice of coordinates.' I understand the geometric intuition, however, I fail to see why this is coordinate free. I would like to find out where I go wrong. What I see is that, as $\mathbb{w}$ relies on $dx$ and $dy$ , the differentials of coordinate functions themselves, how can it then be coordinate free? Furthermore, in $\frac{<a,b>}{|<a,b>|^2}$ , $<a.b>$ and thus its magnitude also depends on $dx$ and $dy$ so I fail to see how this is independent of coordinates - rather, the opposite as it makes an explicit reference to the differentials of coordinate functions? Would be very grateful if someone can show me at what point my logic goes faulty... Note: The author defines $dx$ and $dy$ as the coordinate function for $T_p\mathbb{R^2}$ , such that a vector in $T_p\mathbb{R^2}$ can be written $<dx,dy>$ where $<1,0>=\frac{d(x+t,y)}{dt}$ and $<0,1>=\frac{d(x,y+t)}{dt}$","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
4226247,Showing $\frac{\sinh t}{\sinh T}$ extremises the functional $S[x]=\int_0^T\big(\dot x(t)^2+x(t)^2\big)\ \mathrm dt$ with given boundary conditions,"Show that $t\mapsto\frac{\sinh t}{\sinh T}$ extremises the functional $S[x] = \int_0^T \left( \dot{x}(t)^2 + x(t)^2 \right) \ \mathrm dt$ with boundary conditions $x(0) = 0$ and $ x(T) = 1$ . I can show that $\frac{\sinh t}{\sinh T}$ is one solution to the resultant ODE, $\ddot{x} = x$ after simplifying the functional in the title, but I can't think of a way to prove that it is an extremal solution (specifically, it is supposed to be a minimum , according to the solutions manual [1]). How may I proceed? [1]This problem is 3-5 of James Hartle's Gravity: A Gentle Introduction to Einstein's General Relativity .","['functional-equations', 'ordinary-differential-equations', 'calculus-of-variations']"
4226298,Why is $\operatorname{arccot} x$ not $\arctan \frac{1}{x}$ when $x<0$?,"$\operatorname{arccsc}(x) = \arcsin \frac{1}{x}$ , and $\operatorname{arcsec}(x) = \arccos \frac{1}{x}$ . Why doesn't $\operatorname{arccot}(\cdot)$ behave the same..?","['calculus', 'trigonometry']"
4226338,Chromatic polynomial of this graph,"Here's the graph I'm working on, I want to find its chromatic polynomial, as well as how many proper colorings are possible using $4$ colors. For the first question, the polynomial I came up with is $x(x-1)^4(x-2)^2$ (because $e,f,b,c$ have similar properties, so do $a$ and $d$ ) does that look correct? I've heard about Birkhoff’s theorem but I'm not sure when can I apply that. Apart from the chromatic polynomial approach, can I proceed with the second part from combinatorics? There're $7$ vertices and $4$ colors, to find proper colorings, should that be $\binom{7}4-9$ (as there're $9$ edges in total)? Thanks!","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
4226343,Two Periodic Functions with Period 1,"I am currently studying for a graduate exam and I came across the following question: Suppose that $f$ and $g$ are real-valued functions on $\mathbb{R}$ having period 1 and having continuous first derivatives. Prove that $f'(c) = g'(c)$ for some non-negative $c \in \mathbb{R}$ My initial approach to this problem was to use the mean value theorem for $f$ and $g$ to obtain certain values where both derivatives vanish. However, this did not prove effective in proving the desired result. Any suggestions on how to approach this problem would be greatly appreciated.","['periodic-functions', 'derivatives', 'real-analysis']"
4226354,Bounds of an Expression,"if $(a,b,c)>0$ and $abc(a+b+c)=3$ Then what can you say about the bounds of $(a+b)(b+c)(c+a)$ ? Hint: think Geometrically! My Approach 1.assumed $a\geq b \geq c$ 2.Tried to think geometrically 3.used the Hadwiger-Finsler inequality and modified it to get $(a+b+c) \geq \sqrt{4A\sqrt{3}+2(a^2+b^2+c^2)}$ (where A is the area of a triangle and a,b,c are it's sides) 4. I suspect that as we're to find bounds of $\Pi (a+b)$ ,we could try to find some vaues of (a+b)'s. Could you please help me with this problem?","['inequality', 'geometric-inequalities', 'symmetric-polynomials', 'geometry']"
4226358,Number of ways to arrange stripes on a flag,"I came across this permutations and combinations problem. A Flag has 4 stripes. We can fill those 4 stripes with 7 colours: VIBGYOR. In how many ways can the colours be filled such that two of the colours on the flag are compulsorily V and B? This is how I approached the problem. Out of the 4 stripes, 2 are already filled by V and B, so only the other 2 have to be filled. Let each stripe be numbered 1, 2, 3, 4. Assuming that we can't repeat colours, I can put 5 x 4 colours in the two stripes which are empty. Now to fill the V and B. There can be 6 different cases.
V and B in 1 and 2 respectively
V and B in 2 and 1 respectively
V and B in 2 and 3 respectively
V and B in 3 and 2 respectively
V and B in 4 and 3 respectively
V and B in 3 and 4 respectively In each of these 6 cases, I can arrange the other two colours in 5 x 4 ways. Thus the total number of ways the colours can be arranged according to the question will be 5 x 4 x 6, which is 120. The answer given, for some reason, is 240. Could anyone please point out what I'm missing in my approach above and/or suggest a better one? Thanks!",['combinatorics']
4226360,Expectation value of sum of 3 poker cards (variation),"Assume you have a deck of 52 cards with the values K = 13, Q = 12, J = 11, 10 = 10,..., 2 = 2 and A = 1. Suppose you draw 3 cards $X_1, X_2$ and $X_3$ at random from the deck of 52 cards. Then you find out the minimum value from the 3 chosen cards and then remove it from the choice and then uniformly at random choose another card from the remaining deck and put it with your other 2 chosen cards. What is the Expected Value of the Final 3 chosen cards: a) The 3rd card is drawn such that the minimum valued card is put back into the deck b) The 3rd card is drawn such that the minimum valued card is NOT put back into the deck Source : This was asked to me in a quant trading firm interview, so I am guessing that instead of the brute force method of calculating the PMFs for each scenario, there must be a better method which leverages the linearity of the expectation operator. My Views For scenario (a), what I feel is that atleast for the first scenario - the answer should be 7 $\times$ 3 = 21 since even if the minimum card is resampled, it would not affect the other two chosen cards (thus both of them would have an individual expectation of 7) and then since the new card would be chosen from the remaining deck, it would have the same distribution as the minimum valued card But any of the 3 positions in the tuple could have been the minimum value, so essentially all the 3 card choices would have the same distribution, thus the same expectation (= 7) and the answer should be 7 $\times$ 3 = 21. I'm not at all sure how I would proceed with scenario (b) But what I feel is, if we can get the expected value of the minimum of 3 draws, we can subtract it from 7 and that would most probably be the expected value of the last card drawn. For this question, I would really appreciate some different viewpoints or entire solutions themselves, thank you!","['expected-value', 'conditional-probability', 'card-games', 'probability']"
4226367,"If $A$ and $B$ are solutions to $7\cos\theta+4\sin\theta+5=0$, then $\cot\frac{A}{2}+\cot\frac{B}{2}=-\frac{2}{3}$","If $A$ and $B$ are the solutions to ${\displaystyle 7\cos\theta+4\sin\theta+5=0\mbox{ where }A>0,0<B<2\pi;}$ Without finding the solutions to the trig equation, show that; $$\cot\left(\frac{A}{2}\right)+\cot\left(\frac{B}{2}\right)=-\frac{2}{3}$$ This is my effort so far: Since A and B are solutions then: \begin{align*}
7\cos A+4\sin A+5 & =0\\
7\left( 2\cos^{2}\frac{A}{2}-1\right) +4\times2\sin\frac{A}{2}\cos\frac{A}{2}+5 & =0\\
14\cos^{2}\frac{A}{2}+8\sin\frac{A}{2}\cos\frac{A}{2}-2 & =0\\
\mbox{Now divide by }\sin\frac{A}{2}\cos\frac{A}{2} & \mbox{ gives:}\\
14\cot\frac{A}{2}+8-\frac{2}{\sin\frac{A}{2}\cos\frac{A}{2}} & =0\\
\mbox{similarly for B;}\\
14\cot\frac{B}{2}+8-\frac{2}{\sin\frac{B}{2}\cos\frac{B}{2}} & =0
\end{align*} When I add these two equations I have $\cot\left(\frac{A}{2}\right)+\cot\left(\frac{B}{2}\right)$ ,
but am unsure how to progress.",['trigonometry']
4226415,Double derivative in parametric form,"Let there be two functions expressed in the form of a parametric variable, $y=f(t)$ and $x=g(t)$ and I have find the second derivative of $y$ with respect to $x$ . To do that, I have done as shown $$\frac{d^2y}{dx^2}= \frac{d}{dt}(\frac{dy}{dt})×(\frac{dt}{dx})^2$$ $$\frac{d^2y}{dx^2} = \frac{d^2y}{dt^2} \biggm/\left(\frac{dx}{dt}\right)^2$$ But I am not getting the correct answer and I don't know what is the problem with this. I want to know if I have done something wrong in the above procedure?","['parametric', 'derivatives']"
4226419,Are there infinitely many positive integer solutions to $(xz+1)(yz+1)=P(z)$?,"Let $P(z)\equiv 1($ mod $ \ z) $ be a polynomial of degree $n>3$ with integer coefficients. Are there infinitely many positive integers $x, y, z$ such that $(xz+1)(yz+1)=P(z)$ ? If $P(z) = a_nz^n+1$ , it has be proven that the Diophantine equation has infinitely many solutions in positive integers $x, y, z$ Prove that the diophantine equation $(xz+1)(yz+1)=az^{k}+1$ has infinitely many solutions in positive integers. . From experiment, it appears the assertion is true for all polynomials $P(z)\equiv 1$ (mod $ \ z)$ of degree $n>3$ . How do we go about proving this? Note if $n=3$ , it has been shown that the Diophantine equation has a finite number of solutions https://mathoverflow.net/questions/392002/is-xz1-a-proper-divisor-of-a-3z3a-2z2a-1z1-finitely-often/392018#392018","['number-theory', 'elementary-number-theory', 'diophantine-equations']"
4226433,"""Weaker"" postulates for a group?","In some learning resources I have, it has been claimed that the group axiom defining the identiy $e$ $$a \cdot e = e \cdot a = a$$ and the group axiom defining the inverse element $$a \cdot b = b \cdot a = e$$ can be replaced by the ""weaker"" statements $$a \cdot e = a$$ and $$a \cdot b = e$$ . The argument proceeds by asking the reader to consider elements $a$ , $b$ and $c$ in group $G$ such that $a \cdot b = e$ and $b \cdot c = e$ , then $$b \cdot a = (b \cdot a) \cdot e = (b \cdot a) \cdot (b \cdot c) = b \cdot (a \cdot b) \cdot c = b \cdot e \cdot c = b \cdot c = e$$ I am unhappy with this, because at the very end, the claim that $b \cdot e \cdot c = b \cdot c$ relies on the fact that $e \cdot c = c$ , but we are supposed to only be working from $c \cdot e = c$ The other weaker form is then ""proved"" with $$a = a \cdot e = a \cdot (b \cdot a) = (a \cdot b) \cdot a = e \cdot a$$ which is all very well except for the fact that it now uses $a \cdot b = b \cdot a = e$ which I feel has been proved on shaky ground. Am I right to be concerned with this argument/approach, or are my logic circuits shortcircuiting somewhere? Is the assertion that the properties/axioms/postulates for a group can be replaced with such a ""weaker"" form correct?",['group-theory']
4226461,Filters/ ultrafilters/ principal filters/ proper filters/ maximal filters - intuition and relations between them,"I am studying filters and seeing a lot of definitions at once. So I would like to ask you help me. Is my intuition correct for these guys? Suppose we have partially ordered set with some subsets: Filter (or order filter) - contains subsets ""large enough"" to contain ceratin thing - Im calling it just it . Filter is like a way to look for something within given set and tell where it can be located. Empty set is never in the filter (not large enough, useless to look for it there ).For any set in the filter, any bigger sets (containing the set) are also in the filter. Because if a set contains it , also bigger sets contain it of course. If two sets contain it =are in the filter), then also their intersection contains it - is also in the filter. Ultrafilter - the perfect locating scheme. The best way to look for it . Ultrafilter uses ALL subsets of our given set to determine whether it is hidden in that particular set, or not.
From this interpretation, compactness (see the mathematical characterization below) can be viewed as the property that ""no location scheme can end up with nothing"", or, to put it another way, ""always something will be found"".) 1 Maximal filter - couldn´t find any good definition, could you help? On Wikipedia, it appears to coincide with ultrafilter. Proper filter - is not equal to the whole set Principal filter - we take one set and all the sets that are ""above"" make the principal filter. z-filter - didn´t find anything about this, I only know that this is used for Stone-Čech compactification when the space is not discrete. Does this make sense? What else would you add to connect these definitions or put them into context? Any insights welcome.","['filters', 'order-theory', 'definition', 'general-topology', 'compactness']"
4226482,Representation of the magnetic field in 2D magnetostatics,"Consider a magnetostatics problem in $\mathbb{R}^3$ . The problem is governed by the following equations $$\begin{aligned}\text{Maxwell's equations}\quad &\begin{cases}\nabla\times H(x)=J(x)\\\nabla\cdot B(x)=0\end{cases}\\[3pt]
\text{Constitutive relation}\quad &\begin{cases}B(x)=\mu(x)H(x)\end{cases}\\[3pt]
\text{Boundary conditions}\quad &\begin{cases}\nu(x)\cdot(B(x^+)-B(x^-))=0\\\nu(x)\times (H(x^+)-H(x^-))=J_s(x)\end{cases}\end{aligned}$$ where quantities like $B(x^+)$ and $B(x^-)$ denote one-sided limits at a boundary point $x$ and $\nu(x)$ is the normal at $x$ . Consider now an index set $\mathcal{K}$ and bounded Lipschitz domains $\Omega_k\subset\mathbb{R}^2$ with $k\in\mathcal{K}$ . Let's assume that $\mu(x)=\mu_k$ and $J(x)=J_k$ for $x\in\Omega_k\times\mathbb{R}$ and $\mu(x)=0$ and $J(x)=0$ for $x\in(\mathcal{A}\triangleq\overline{\bigcup_{k\in\mathcal{K}}\Omega_k}^c)\times\mathbb{R}$ . It's not hard to see that boundedness of the domains $\Omega_k$ implies that the magnetic field has finite energy per unit length in the last dimension (which I'll refer to as the z direction from now on). Using Maxwell's equations it can be shown that the z component of the magnetic field will be zero and considering the problem setup, the magnetic flux density can be written as $B(x)=B_x(x_x,x_y)\,e_x+B_y(x_x,x_y)\,e_y$ , where $e_x$ and $e_y$ are unit vectors in x and y direction, respectively. Omitting the z component of $B$ it can now be seen that $B(x_x,x_y)\in L(\mathbb{R}^2)^2$ . Assuming sane surface currents $J_s$ (which are further assumed to have a z component only), it follows now from results about Hodge decompositions in [1] that $$B(x_x,x_y)=\nabla B_{\nabla}(x_x,x_y)+\text{Curl} B_{\times}(x_x,x_y),$$ where $\text{Curl}\triangleq[\partial_y,-\partial_x]^T$ . Furthermore, the derivatives are strictly speaking only considered in a weak sense as the above result resorts to the use of Sobolev spaces. Also, in the equation above $B$ is assumed to be the restriction to either one of the $\Omega_k$ or $\mathcal{A}$ and the entire magnetic field is given by combination of all of these restrictions. Next, note that assuming $B_{\nabla}=0$ is equivalent to writing $B=\nabla\times A$ with $A=B_{\times}\,e_z$ . My experience with electromagnetism is very limited, but I realize that the magnetic flux density is often written in exactly this form, which leads me to my actual question : Given the problem setup, is it fine to assume $B_{\nabla}=0$ , meaning that there exists a function $B_{\times}$ (or a distribution for the sake of mathematical correctness) such that Maxwell's equations and the boundary conditions can still be satisfied? [1] Dautray, 1990, Mathematical Analysis and Numerical Methods for Science and Technology: Volume 3 Spectral Theory and Applications","['electromagnetism', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'boundary-value-problem']"
4226485,Logarithm of normal distribution is constant plus Chi squared?,"I'm reading Bayesian data analysis . On page 85 it is stated that ""In the d dimensional normal distribution, the logarithm of the density function is a constant plus a $χ^2_d$ distribution divided by −2."" Can someone clarify how to derive this? For example, let's use $d = 1$ . Then the logarithm of the density function of the normal distribution is: $$
\begin{align}
ln(p(\theta)) &= ln\left({\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {\theta-\mu }{\sigma }}\right)^{2}}\right) \\
&= ln\left(\frac {1}{\sigma {\sqrt {2\pi }}}\right) -{\frac {1}{2}}\left({\frac {\theta-\mu }{\sigma }}\right)^{2} \\
&= C -{\frac {1}{2}}\left({\frac {\theta-\mu }{\sigma }}\right)^{2}
\end{align}
$$ So the above statement would imply that $\left({\frac {\theta-\mu }{\sigma }}\right)^{2}$ is a $χ^2_1$ distribution. However, when I look up that distribution I get: $$
\begin{align}
χ^2_1(\theta) &= {\frac {1}{2^{d/2}\Gamma (d/2)}}\;\theta^{d/2-1}e^{-\theta/2} \\
&= {\frac {1}{2^{1/2}\Gamma (1/2)}}\;\theta^{1/2-1}e^{-\theta/2} \\
&= \frac{1}{\sqrt{2\pi}}\;\theta^{-1/2}e^{-\theta/2}
\end{align}
$$ I do not see how these two things are the same. Especially the exponent of theta seems to behave very differently from what is in essence a quadratic formula in theta above.","['chi-squared', 'statistics', 'bayesian', 'normal-distribution']"
4226512,What's the link between de Bruijn-Newman constant and RH,"I'm not an expert in analytic number theory, but I've been curious in this last period about it. I stumbled upon de Bruijn-Newman constant during my researches on Wikipedia, where it is written that The constant is closely connected with Riemann's hypothesis concerning
the zeros of the Riemann zeta-function: since the Riemann hypothesis
is equivalent to the claim that all the zeroes of H(0, z) are real,
the Riemann hypothesis is equivalent to the conjecture that $ Λ ≤ 0. $ Where $$H(\lambda, z):=\int_{0}^{\infty} e^{\lambda u^{2}} \Phi(u) \cos (z u) d u$$ $$\Phi(u) = \sum_{n=1}^{\infty} (2\pi^2n^4e^{9u} - 3 \pi n^2 e^{5u} ) e^{-\pi n^2 e^{4u}}$$ Now, I understand that explaining exactly how the constant is connected to RH might be kinda complicated, but what's exactly $\Lambda$ ? Why RH is equivalent to $ Λ ≤ 0 $ ? Odlyzko (2000) proved that $-2.7\cdot10^{-9}<\Lambda$ and stated that The new bound provides yet more evidence that the Riemann Hypothesis, if true, is just barely true. What does he mean by ""barely true""? Thanks.","['conjectures', 'number-theory', 'riemann-hypothesis', 'analytic-number-theory', 'constants']"
4226527,Continuous function satisfying $f\left( {2{x^2} - 1} \right) = \left( {{x^3} + x} \right)f\left( x \right)$,"If $f\colon\left[ { - 1,1} \right] \to \mathbb R$ be continuous function satisfying $f\left( {2{x^2} - 1} \right) = \left( {{x^3} + x} \right)f\left( x \right)$ , then $\mathop {\lim }\limits_{x \to 0} \frac{{f\left( {\cos x} \right)}}{{\sin x}}$ is _______. My solution is as follow $x = \cos \left( {\frac{\theta }{2}} \right)$ $f\left( {2{{\cos }^2}\left( {\frac{\theta }{2}} \right) - 1} \right) = \left( {{{\cos }^3}\left( {\frac{\theta }{2}} \right) + \cos \left( {\frac{\theta }{2}} \right)} \right)f\left( {\cos \left( {\frac{\theta }{2}} \right)} \right)$ $\frac{{f\left( {\cos \theta } \right)}}{{\sin \theta }} = \frac{{\left( {{{\cos }^3}\left( {\frac{\theta }{2}} \right) + \cos \left( {\frac{\theta }{2}} \right)} \right)}}{{2\sin \left( {\frac{\theta }{2}} \right)\cos \left( {\frac{\theta }{2}} \right)}}f\left( {\cos \left( {\frac{\theta }{2}} \right)} \right)$ $\frac{{f\left( {\cos \theta } \right)}}{{\sin \theta }} = \frac{{\left( {{{\cos }^2}\left( {\frac{\theta }{2}} \right) + 1} \right)}}{{2\sin \left( {\frac{\theta }{2}} \right)}}f\left( {\cos \left( {\frac{\theta }{2}} \right)} \right)$ How do I proceed from here",['functions']
4226529,Limit of summation $\left(-1\right)^{x}\sum_{r=0}^{x}\left(-1\right)^{r}\binom{x}{r}\left(\frac{r}{x}\right)^{t(x)}$,I am trying to find what the function $t(x)$ would have to be in order to make the expression true. $$\lim_{x \to \infty} \left(-1\right)^{x}\sum_{r=0}^{x}\left(-1\right)^{r}\binom{x}{r}\left(\frac{r}{x}\right)^{t(x)}=\frac{1}{2}$$ I've tried using Stirling's approximation to see if things cancelled out to get something simpler to solve but it ended up looking worse. From there I haven't know where to go next.,"['limits', 'algebra-precalculus', 'functions', 'sequences-and-series']"
4226548,Problem in obtaining expected value,"Suppose that we have the following probability density function \begin{equation}
f(x)=\frac{\alpha x_m^{\alpha}}{x^{\alpha+1}}, \quad x>x_m
\end{equation} and its cumulative density function \begin{equation}
F(x)=1-\left(\frac{x_m}{x}\right)^{\alpha}, \quad x>x_m
\end{equation} I calculate expected value of $X$ in two ways. First way: \begin{eqnarray}
\int_{x_m}^{\infty}xf(x)dx&=&\int_{x_m}^{\infty}\frac{\alpha x_m^{\alpha}}{x^{\alpha}}dx\\
&=&\frac{\alpha x_m}{\alpha-1},\quad \alpha>1
\end{eqnarray} Second way: \begin{eqnarray}
\int_{x_m}^{\infty}(1-F(x))dx&=&\int_{x_m}^{\infty} \left(\frac{x_m}{x}\right)^{\alpha} dx\\
&=&\frac{x_m}{\alpha-1},\quad \alpha>1
\end{eqnarray} why the second way gives an incorrect answer? I do not know where I am doing wrong.","['integration', 'statistics', 'probability-distributions', 'expected-value']"
4226598,Is an infinite product of functions differentiable if it's components are?,"$$ p(\theta) = \left[\lim_{t \to0} p(t) \right] \lim_{n \to \infty}\prod_{i=0}^{n}( \frac{\cos^2 \frac{ \theta}{2^i} +1}{2})$$ I know each term in the product is differentiable but does that mean the total product will be ? I thought of doing a repeated product rule, but   I was a bit worried that maybe due to the limit, the differentiability would be effected.",['derivatives']
4226613,How many ways are there to go from $A$ to $C$ and come back from $C$ to $A$,"Three small towns, designated $A,B,C$ are interconnected by a system of two-way roads as shown in the following picture: Image of interconnected cities How many ways are there to go from $A$ to $C$ and come back from $C$ to $A$ such that the connections which are used to go from $A$ to $C$ cannot be used again. For example, if you use ( $R_1$ and $R_6$ ), then they are cancelled for going from $C$ to $A$ , you may use the roads ( $R_5$ and $R_2$ ) or use ( $R_8)$ . Second example, if you use $R_9$ go from $A$ to $C$ , then it will  be cancelled, so you can use ( $R_8$ ) or ( $R_7$ and $R_1$ ) or ( $R_5$ and $R_3$ ) etc for going from $C$ to $A$ . My solution: There are $4\cdot3+2=14$ ways to go from $A$ to $C$ , I tought that there may be $3\cdot2+1=7$ to come back. However, I am not sure. So, I want help for it.. NOTE: Unnecessary travelling is not allowed, i.e, you should always move to your target, for example if you go from A to B then you cannot shuttle here, you should move from B to C.","['permutations', 'combinatorics', 'contest-math']"
