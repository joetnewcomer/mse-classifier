question_id,title,body,tags
2546001,Evaluating $\int_0^\pi f^{-1} (x) dx$ with $f(x)=\cos x+ x$.,Let $f:\mathbb{R} \to \mathbb{R}$ defined by $f(x)=\cos x+ x$. Evaluate $$\int_0^\pi f^{-1} (x) dx.$$ I used the substitution $x=f(y)$ the integral transforms to $\int_{f^{-1} (0)}^{f^{-1} (\pi)} y f'(y) dy$ which evaluates to $\pi(f^{-1} (\pi) -1)$ I couldn't get where I went wrong. Any ideas?,"['real-analysis', 'integration', 'definite-integrals', 'calculus']"
2546060,Prove that idempotent operator $E$ is self-adjoint if and only if $EE^∗$ = $E^∗E$,"Let $V$ be a finite-dimensional inner product space, and let $E$ be an idempotent
linear operator on $V$, i.e., $E^2 = E$. Prove that E is self-adjoint if and only if $EE^* = E^*E$. Are there any simpler answers to the question that the answers provided here Normal, idempotent operator implies self-adjointness. . Both answers seem to be correct but contain logical steps that I can't comprehend e.g $(I−E)Ex=0 \Rightarrow (I−E^∗)Ex=0 $ and $v^\ast E^\ast Ev=0 \Rightarrow Ev=0$",['linear-algebra']
2546129,I can't figure out how to separate variables in this differential equation;,"\begin{align*}
xyy'^2 + (x^2 + y^2)y' + xy = 0 \\
\end{align*} I tried doing all kinds of things to separate the x and dx on one side and y and dy on the other and none of it worked. I'd post my attempts but I don't think you'd get anything from them since they don't go very far. Am I missing something? This is in the part of the textbook with basic differential equations, so it shouldn't require any specific methods, as far as I know.",['ordinary-differential-equations']
2546166,"Existence of a function behaving like $s^\beta$ everywhere ($\beta\in(0,1)$)","Does there exist a function $f:\mathbb R\to\mathbb R$ such that
$$
\lim_{s\downarrow 0} \frac{f(t+s)-f(t)}{s^\beta}=c,\quad \text{for every }t\in\mathbb R,
$$
where $\beta\in(0,1)$ and $c\in\mathbb R\backslash \{0\}$? I am happy if you answer the same question but with a non-zero continuous function $c(t)$ on the right  hand side.","['real-analysis', 'holder-spaces']"
2546201,"What is a bound of $\frac{1}{\sin x} - \frac{1}{x}$on $(0,a]$?","Is $\dfrac{1}{\sin x} - \dfrac{1}{x}$ bounded in $(0,a]$, $a<\frac\pi2$? How can I prove this? Any hints or suggestions?","['real-analysis', 'inequality', 'trigonometry', 'calculus', 'estimation']"
2546226,Finding the infinite limit using definitions,I would really need help proving the following limit using the definitions. My textbook gives me a solution but with very minimal explanation and I'm completely lost. This is the limit I have to find. $$\lim_{x\to1^+} \frac{x}{x-1}$$,"['analysis', 'limits']"
2546253,Is twice differentiability an open property?,"A function $f : \mathbb{R} \to \mathbb{R} $ which is differentiable at a point $a \in \mathbb{R}$ need not be differentiable in a neighbourhood of $a$. For example,
$$
f : \mathbb{R} \to \mathbb{R} : x \mapsto \begin{cases}
x^2 && x \in \mathbb{Q} \\
-x^2 && x \in \mathbb{R} \setminus \mathbb{Q}
\end{cases}
$$
is differentiable only in $0$. If $f$ is twice differentiable in $a$, then of course $f$ is differentiable in a neighbourhood of $a$. But is $f$ twice differentiable in a neighbourhood of $a$? I find it hard to come up with a natural counterexample, but see no reason why this statement should hold either.","['derivatives', 'calculus']"
2546264,Length of an implicit curve [duplicate],"This question already has an answer here : What's the arc length of an implicit function? (1 answer) Closed 5 years ago . The topography of an empty lake is given by a function f(x,y)=z. Each spring the lake is filled by water. The waterstand is given by a constant z=c, where c is a constant. How long is the shore of the lake? What I am actually asking is: Is there any way to compute the lenght of a regular implicit curve?","['implicit-function-theorem', 'discrete-geometry', 'implicit-function', 'differential-geometry', 'approximation']"
2546265,"$\lim_\limits{\sigma_A, \sigma_B \to \infty }e^{-\sigma_A-\sigma_B} \sum_{k=0}^{\infty} \frac{{\sigma_A}^k}{k!}\cdot\frac{{\sigma_B}^k}{k!}$","I am currently writing my Bachelor thesis in economics and for one proof idea I would need to compute this limit (if it exists). I am not sure how to approach this due to the double limit. Clearly, the limit of every summand for finite k is zero but that doesn't really help, does it? 
(EDIT: if you put the exponential term in the sum, that is.)
Adding up zeros infinitely often doesn't imply the sum is zero, right? Thanks in advance,
rm","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'calculus']"
2546273,Nonhomeomorphic subsets of the plane,"I'm trying to find two compact, nonhomeomorphic subsets of the plane, say $X$ and $Y$, such that $X \times [0,1]$ is homeomorphic to $Y \times [0,1]$. I can not think of how a homeomorphism arises when you product with the interval.","['general-topology', 'examples-counterexamples', 'compactness']"
2546294,"Related Rates - two airplanes, both rates given. Find rate of distance","The question reads ""Two aircraft are in the same airspace with Plane A 500km south of Plane B. If Plane A is traveling 600 km/h due south while Plane B is travelling 800km/h due west, determine how quickly the distance between the planes is changing."" My solution: $\frac{dA}{dt} = 600km/h$ $\frac{dB}{dt} = 800 km/h$ $A = 500km$ $B = 0km$ $d = \sqrt{A^2 + B^2}$ $\frac{dd}{dt} = \frac{A\frac{dA}{dt} + B\frac{dB}{dt}}{\sqrt{A^2+B^2}}$ Plugging in the values from above:
$\frac{dd}{dt} = \frac{0 + 500(600)}{\sqrt{500^2}}$ $\frac{dd}{dt} = 600 km/h $ Did I do this the correct way?","['derivatives', 'calculus']"
2546298,Vector bundle determined by differential structures?,"Let $E\to M$ be a vector bundle. Is the structure of vector bundle determined by the map $E\to M$ (as morphism between manifolds)? i.e. is it possible that there are two non-isomorphic vector bundles $E_1, E_2$ such that their underlying manifolds are the same?","['manifolds', 'vector-bundles', 'smooth-manifolds', 'differential-geometry']"
2546326,A Noetherian scheme $X$ is integral iff it’s connected and all stalks are integral domain.,"I am reading rising sea and try to prove exercise 5.3.C: A Non-empty Noetherian scheme $X$ is integral iff it’s connected and all stalks are integral domain. The “only if” part is trivial: an integral scheme is irreducible, hence is connected. It’s also obvious that stalks are integral domains. My question is about the “if” part, stalks being integral means they are reduced. What remains is the irreducibility of $X$ . Since $X$ is connected, the irreducible component of $X$ are closed, so if I can prove the irreducible components do not intersect with each other, I am done. If two irreducible components $Z,F$ intersect at $p$ , we can pick an open affine neighborhood $U$ of $p$ , and $U$ necessarily contains generic points of $Z$ and $F$ in $X$ . So,my question is: Did I miss anything in the argument above? If I didn’t, is it true that in any affine scheme a stalk at a point where two irreducible subsets (no one belongs to the other) intersect can’t be an integral domain? It’s easy to translate 2. into commutative algebra, but after that, I still don’t know how to solve it. Any hint will help, thanks a lot.","['algebraic-geometry', 'commutative-algebra']"
2546329,Find the limit $\lim_{x \to \frac{\pi}{2}}(\frac{\cos(5x)}{\cos(3x)})$ without using L'Hospital's rule,I'm trying to find the limit: $$\lim_{x \to \frac{\pi}{2}}\left(\frac{\cos(5x)}{\cos(3x)}\right)$$ By L'Hospital's rule it is $-\frac{5}{3}$ but I'm trying to solve it without using L'Hospital rule. What I tried: Write $\frac{\cos(5x)}{\cos(3x)}$ as $\frac{\cos(4x+x)}{\cos(4x-x)}$ and then using the formula for $\cos(A+B)$ . Write $\cos(x)$ as $\sin\left(x - \frac{\pi}{2}\right)$ . But I didn't have success with those methods (e.g. in the first one I got the same expression $\frac{\cos(5x)}{\cos(3x)}$ again ).,"['limits-without-lhopital', 'trigonometry', 'calculus', 'limits']"
2546348,"Showing that, under certain assumptions, $df_x$ has no eigenvalues on the unit circle","Let $M$ be a smooth manifold, $f : M \to M$ a $C^1$ diffeomorphism and $x$ a fixed point of $f$. Suppose that there exist a decomposition $T_x M = E^-_x \bigoplus E^+_x$ and constants $c > 0, \lambda \in (0, 1)$ such that
$$\|df^n_x(v)\| \leq c \lambda^n \|v\|, v \in E^-_x, n \in \mathbb{N}$$
$$\|df^{-n}_x(v)\| \leq c \lambda^n \|v\|, v \in E^+_x, n \in \mathbb{N}.$$
Show that $df_x$ has no eigenvalues on the unit circle. My work: Suppose there is $\mu$ with $|\mu| = 1$ and $v \neq 0$ such that $df_x v = \mu v$. Then $\|df_x v\| = \|\mu v\| = |\mu| \|v\| = \|v\|$. On the other hand, from the Chain Rule and the fact that $x$ is a fixed point we have $df^n_x(v) = (df_x)^n(v)$. But I can't obtain a contradiction considering the two inequalities. Can someone help me?","['eigenvalues-eigenvectors', 'dynamical-systems', 'differential-geometry']"
2546360,Prove that $G = \prod_{p\hspace{1mm} prime}\frac{\mathbb{Z}}{p\mathbb{Z}}$ is not isomorphic to $\frac{G}{torsion(G)} \bigoplus torsion(G)$.,"Let $G$ be the group  $ \prod_{p\hspace{1mm} prime}\frac{\mathbb{Z}}{p\mathbb{Z}}$, i.e. the cartesian product of the factor groups of $\mathbb{Z}$ modulo every prime $p$. (so essentially a group of sequences of integers modulo ascending primes). let $torsion(G)$ be the torsion group of $G$, i.e. $\{g \in G| \exists n \in \mathbb{N} s.t. g^n = id_G \}$. Then I want to show that $G$ is not isomorphic to  $\frac{G}{torsion(G)} \bigoplus torsion(G)$, where $\bigoplus$ denotes the direct sum. Now it would appear that this is a problem that calls for some of the more abstract areas of mathematics (My hunch is Zorn's lemma...). Here's what I got so far. Because distinct primes are always relatively prime, I can write  $ \prod_{p\hspace{1mm} prime}\frac{\mathbb{Z}}{p\mathbb{Z}} = \frac{\mathbb{Z}}{\prod_{p \hspace{1mm}prime}p\mathbb{Z}}$. This group doesn't have a maximum element under the ""divisible"" order relation right? A.k.a. no element $x$ that is divisible by every prime $p$. But $\frac{G}{torsion(G)}$ may have, by Zorn's lemma, perhaps? This problem is very mysterious to me...","['abstract-algebra', 'integers', 'group-theory']"
2546387,Confusion about definition of manifold with boundary,"Consider this definition: A space $M$ is a manifold with boundary if each point $x\in M$ has a neighborhood $U_x$ that is homeomorphic to $\mathbb R^n$ or to $\mathbb R^n_+=\{(x_1,\cdots,x_n)\;|\; x_n \ge 0\}$; the points which have neighborhoods homeomorphic to $\mathbb R^n_+$ form the boundary $\partial M$ of $M$. I want to understand how precise this definition is. Indeed, a point that has an open neighborhood homeomorphic to $\mathbb R^n$ can also have another open neighborhood that is homeomorphic to $\mathbb R^n_+$ and in that case is this point on the boundary $\partial M$ or in $M\setminus \partial M$  ? How to choose? Is my definition incomplete so that I have to say: ""In a manifold with boundary, a point in $M$ has an open neighborhood homeomorphic to $\mathbb R^n$ but if it doesn't have such neighborhood then it has to have an open neighborhood homeomorphic to $\mathbb R^n_+$"" Thank you for your help!","['manifolds', 'general-topology', 'manifolds-with-boundary']"
2546393,Classification of Groups of Order $p^2$,"The book I am reading contains contains the following claim: Every group of order $p^2$, where $p$ is a prime, is isomorphic to $\mathbf{Z}_{p^2}$ or $\mathbf{Z}_p \oplus \mathbf{Z}_p$. I tried to do the proof myself and ended up with the following. Is the following sketch correct? -- We can show all groups of order $p^2$ are abelian by examining the center of the group. https://proofwiki.org/wiki/Group_of_Order_Prime_Squared_is_Abelian We are left to show, that such an abelian group of order $p^2$ is of the two forms given above. Examine arbitrary subgroup $\langle a \rangle \subset G$. By lagrange's theorem it's order can be $p^2$, $p$, or $1$. If $|\langle a \rangle|=p^2$ we obtain an abelian subgroup order $p^2$ -- i.e. $\langle a \rangle$ generates G. So $\mathbf{Z}_{p^2} = \langle a \rangle = G$ (the first isomorphism established by sending $k \mapsto a^k$). If $|\langle a \rangle|=p$ we have an abelian subgroup of $G$ order of $p$. We can see $\mathbf{Z}_{p} = \langle a \rangle$ by again sending $k \mapsto a^k$.  Construct $G/\langle a \rangle$ (Can check $\langle a \rangle$ is normal in $G$). $|G/\langle a \rangle| = p$ by using coset definition and counting. By using Lagrange's theorem, one can show any group of order $p$ is cyclic and abelian. Thus we can construct the isomorphism: $\mathbf{Z}_{p} = G/\langle a \rangle$. We note: $\langle a \rangle = \mathbf{Z}_{p} = G/\langle a \rangle$. And so: $G/\langle a \rangle \oplus \langle a \rangle=\mathbf{Z}_p \oplus \mathbf{Z}_p$. ( How do I show $G/\langle a \rangle \oplus \langle a \rangle$ = $G$? I think this is still missing ) If $|\langle a \rangle|=1$ then $G/\langle a \rangle =G$. So we can refer to 1 and 2.","['finite-groups', 'group-theory', 'proof-verification']"
2546412,The sum of 8 consecutive Fibonacci numbers is not a Fibonacci number,"While studying Fibonacci numbers, I came up with this problem. Of course $F_n = F_{n-1}+F_{n-2}$. I'm sort of stuck with first realizing how to show a number actually isn't a Fibonacci number. I thought that I could somehow rewrite the sum $$F_n+\cdots+F_{n+7}$$
into some sort of rearrangement of $F_n$'s and $F_{n+1}$'s. Could anyone help me show that the sum of 8 consecutive Fibonacci numbers is not a Fibonacci number? Thanks","['combinatorics', 'fibonacci-numbers']"
2546454,"Is it true that $\sum_{k=0}^m\binom{n-k}k$ outputs the $(n+1)$th Fibonacci number, where $m=\frac{n-1}2$ for odd $n$ and $m=\frac n2$ for even $n$?","Does $$\sum_{k=0}^m\binom{n-k}k=F_{n+1}$$ where $m=\left\{\begin{matrix}
\frac{n-1}{2}, \text{for odd} \,n\\ 
\frac n2, \text{for even} \,n
\end{matrix}\right.$ hold for all positive integers $n$? Attempt :
I have not yet found a counterexample, so I will attempt to prove it.
$$\text{LHS} =\binom n0 + \binom{n-1}1+\binom{n-2}2+...+\left\{\begin{matrix}
\binom{1+(n-1)/2}{(n-1)/2}, \text{for odd} \,n\\ 
\binom{n/2}{n/2}, \text{for even} \,n
\end{matrix}\right.$$
Now using the identity that $\binom nk + \binom n{k+1}=\binom {n+1}{k+1}$, where $k$ is a positive integer, I find that $$\binom{n-1}1=\binom n1 - 1, \\ \binom {n-2}2=\binom n2-2\binom n1+3,\\ \binom {n-3}{3} =\binom n3 - 3\binom n2 + 6 \binom n1 - 10, \\ ...$$ This pattern suggests that the coefficients of $\binom{n-4}4$ will be square numbers, those of $\binom{n-5}5$ will be pentagonal numbers, etc. However, I cannot see a way to link these results to any Fibonacci identity. Edit : @Jack D'Aurizio♢ has provided a very succinct proof to this, but is there a more algebraic method to show the equality?","['fibonacci-numbers', 'binomial-coefficients', 'number-theory', 'combinatorics', 'summation']"
2546462,Proof of a Burnside theorem without character theory?,"Burnside proved, by use of character theory, that if a finite group $G$ has a conjugacy class $C$ such that $\vert C \vert$ is a prime power $> 1$, then $G$ is not simple. Let us call this statement ""Burnside's non-simplicity theorem"". From this theorem, Burnside deduced easily his ""$p^{a}q^{b}$ theorem"" : every finite group whose order has at most two distinct prime factors is solvable. It is well known that Burnside's $p^{a}q^{b}$ theorem can be proved indepedently from character theory. Can ""Burnside's non-simplicity theorem"" also be proved indepedently from character theory ?","['finite-groups', 'group-theory']"
2546506,"Any random variable, does tail of integral goes to 0?","Let $X$ be a random variable in $\mathbb{R}$ real.
(Not necessarily $\mathbf{E}|X| < \infty$, also if it is the case, the below is true).
Let $\mathbf{P}(|X| = \infty) = 0$.
Can we say the following holds?
\begin{equation}
\lim_{a \to \infty} \mathbf{E}\left[|X|\textbf{I}(|X|>a) \right]
= \lim_{a \to \infty} \int_{|x|>a} |x| dF_X(x) = \int_{|x|=\infty} |x| dF_X(x)
= 0.
\end{equation}
At first, I thought this is true because integration on the probability zero (measure zero set).
But I am not sure because of the following example. Let $X$ be a random variable in $\mathbb{N}$ with 
$\textbf{P}(X = n) = \frac{6}{\pi^2}\frac{1}{n^2}$.
Then for any $a$, we have 
\begin{equation}
\mathbf{E}[|X|\textbf{I}(|X|>a)] = \sum_{n = a}^\infty \frac{6}{\pi^2}\frac{1}{n} = \infty
\end{equation}
However how we interpret the limit? That is
\begin{equation}
\lim_{a \to \infty} \sum_{n = a}^\infty \frac{6}{\pi^2}\frac{1}{n} = \sum_{n = \infty}^\infty \frac{6}{\pi^2}\frac{1}{n} = 0 ?
\end{equation}
But I am not sure whether it makes sense taking limit on the value of $\infty$.
Then I feel like to say that the above doesn't have limit. But I am not strongly sure.
I think there is a missing part that I don't know and maybe an elementary reason. Any comments, suggestions, answers will be very appreciated.
Thank you so much in advance.","['probability-theory', 'probability-distributions']"
2546536,Series expansion of $\frac{x^n-1}{x-1}$ at $ x=1$.,"$$\lim_{x\to 1}\left(\frac{x^n-1}{x-1}\right)=n$$ I thought that at $x=1$ the expansion is: $$n+n(n-1)(x-1)+n(n-1)(n-2)(x-1)^2+...$$ but the answer is: $$n+\frac{1}{2}n(n-1)(x-1)+\frac{1}{6}n(n-1)(n-2)(x-1)^2+...$$ Can you explain the origin of $$\frac{1}{2},\frac{1}{6},\frac{1}{24}, ...?$$","['limits', 'calculus', 'algebra-precalculus', 'limits-without-lhopital', 'geometric-series']"
2546557,Density of Square Rationals,"This is a homework question I have in my analysis class. Prove that for all $x, z \in \Bbb Q$ with $0 < x < z$, there exists $y \in\Bbb Q$
such that $x < y < z$ and $y$ has a rational square root (i.e., $y = w^2$
for some $w \in Q$.) 
Here, $\Bbb R$ is the set of all real numbers, and $\Bbb Q$ is the set of all rational numbers. I tried the following:
Let $x,z \in \Bbb Q$ with $0 < x < z$. Since $x < y < z$, we can say $\sqrt x < \sqrt z$. 
Since $\Bbb Q$ is dense in $\Bbb R$, there exists $w \in\Bbb Q$ such that $\sqrt x < w < \sqrt z$. 
If we square the expression, we get $x < y^2 < z$, so $w^2 = y$. QED I was told that this isn't a proper way to prove this statement and haven't been able to figure out how to start it correctly. Any help is appreciated!","['analysis', 'proof-verification']"
2546565,What are the solutions to the following equation (which may or may not be a differential equation)?,"Let $X = \mathbb{R}^n$ and let $T\colon X\to X$. Suppose for every $x\in X$,
$$x - T(x) = \nabla\tfrac{1}{2}\|x-T(x)\|^2.$$
What are all the solutions to this equation? I am aware the (generally nonsmooth ) projections onto nonempty closed convex subsets of $X$ will satisfy this but are there other solutions out there?","['derivatives', 'real-analysis', 'partial-differential-equations', 'convex-analysis', 'ordinary-differential-equations']"
2546579,Set of continuous function is not measurable,"I would like to prove that $\mathcal{C}([0,1])\notin \mathcal{B}^{[0,1]}.$ For all $I\subset[0,1]$, denote $\mathcal{B}_E(I)$ the smallest $\sigma-$algebra of $E^[0,1]$ such that all $\pi^t,\; t\in I$ are measurable. 
Denote $D$ the set of countable sets of $[0,1].$ I know that $\mathcal{B}^{[0,1]}=\cup_{I\in D}\mathcal{B}_E(I).$ I imagine that we argue by contradiction and we can find a function such that $\pi^t$ are not measurable, but not sure I understand correctly the problem ?","['stochastic-processes', 'probability-theory']"
2546602,If a sequence diverges to infinity then so do all its subsequences. Then why does the sum for $\frac{1}{n}$ diverge but not $\frac{1}{n^2}$?,"If a sequence diverges to infinity then its subsequences diverge to infinity as well. How does the fact that the sequence of partial sums $S_n = \sum_{k=1}^{n}\frac{1}{k} \to \infty$ as $n \to \infty$, and the subsequence $S_{n^p} = \sum_{k=1}^{n}\frac{1}{k^p}$ converges when $p>1$ not violate the first proposition? Is my initial assumption wrong? Am I wrong in calling $S_{n^2}$ a subsequence? Thank you for the help.","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'calculus']"
2546624,Intuition Behind Necklace Formula,"Wikipedia and Wolfram MathWorld say that the formula for the number of distinct $k$-ary necklaces of length $n$ is: $$
N_k(n) = \frac{1}{n}\sum_{d|n} {\phi(d)k^{n/d}}
$$ What is the intuition behind this formula?","['combinatorics', 'polya-counting-theory', 'necklace-and-bracelets', 'totient-function']"
2546630,"Prove $\bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k) $","Let $V$ a $\mathbb{K}$ -vector space of finite dimension $n$ , with $\{f_1,...,f_n\}$ a set linearly independent of $V^*$ and $f\in V^*$ . Prove $\bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k).$ ( $\Leftarrow$ ) Let $f\in {\rm span}(f_1,...,fk)$ then exists $\alpha_1,...,\alpha_k$ such that $f=\alpha_1f_1+...+\alpha_kf_k$ . Let $f\in\bigcap_{i=1}^k\ker(f_i) $ then $f\in \ker(f_1),...,f\in \ker(f_k)$ . By hypothesis we have if $f\in {\rm span}(f_1,...,fk)$ then exists $\alpha_1,...,\alpha_k$ such that $f=\alpha_1f_1+...+\alpha_kf_k$ Here I'm a little stuck. Can someone help me? ( $\Rightarrow$ ) I do'nt know how to prove this part. Help me, if you can. I will be very grateful.",['linear-algebra']
2546698,Proving $\int_0^1 x^x \mathrm{d}x$,"How do I prove that $\int_0^1 x^x \mathrm{d}x$ is between $0.69$ and $1$? I think there does not exist a function such that its derivative is $x^x$. Is the proof approximate? Numerically, this holds true. But algebraically/theoretically I have no clue how where to start.","['integration', 'sequences-and-series', 'calculus', 'exponentiation']"
2546714,Statistical Analysis 1: What is the probablity that this sample proportion is less than the population proportion by 0.06 or more?,"I have not gotten this wording before in regards to this question: ""Mong Corporation makes auto batteries. The company claims that 80% of its LL70 batteries are good for 70 months or longer. Assume that this claim is true. Let $\hat{p}$ be the proportion in a sample of 100 such batteries that are good for 70 months or longer."" Now I cannot figure out whether it is asking: $p(\hat{p} < 0.06)$ $p(\hat{p} < .80 - 0.06) = p(\hat{p} < .74)$","['statistics', 'probability']"
2546743,Convolution of normal probability distribution and Gaussian filter,"I'm interested in generating a random scalar field according to a probability density function with a correlation length. This might correspond to say a spatial distribution of strength of a material or a temporal distribution of a signal. My plan for doing so is to generate normally-distributed random numbers $R$ at each sampling point $x_i$ and introduce the spatial correlation by convolving with a Gaussian filter. Specifically, I perform this in the frequency domain by multiplying the Fourier transforms element-wise and then performing the inverse transform to reconstruct the filtered variable. I'm using numpy 's multi-dimensional Fourier transforms to do the calculation ( numpy.fft.fftn , numpy.fft.ifftn ). $$
R(x_i) \sim N(\mu=0,\sigma^2=1)
$$ $$
f(x_i) = \exp \left( \frac{-x_i^2}{\ell^2/2}\right)
$$ $$
\tilde{R} = \alpha \cdot IFFT_n (FFT_n(R) \cdots FFT_n(f))
$$ Where I am not sure is about how to scale the filtered random field $\tilde{R}$ such that it is still normally distributed ($\sigma^2(\tilde{R})=1$). The mean is zero, but the variance is generally not one and nonlinearly varies with the correlation length $\ell$. What scaling factor $\alpha$  should I use, and how is this derived? I'm interested primarily in three-dimensional fields ($n=3$, $x_i \in R^3$).","['statistics', 'convolution', 'normal-distribution']"
2546749,Superposition of two waves,"Two waves on the string of a musical instrument superposition to give you a
  third wave. One wave is given by the equation $y =\sin(30t)$ and the other is given by
  $y = \sin(32t)$. The superposition of the two waves makes a sound (a hum) of a certain
  frequency, the amplitude of which varies periodically, making beats. What is the frequency
  of the hum and what is the frequency of the beats that you hear? Assume that time is given
  in seconds. I don't know how to approach this computationally, but this is the graph of the superposition function $\sin(30t)+\sin(32t)$. My understanding, if correct, is that each individual peak and trough implies finding the frequency of the beats, whereas the periodicity of the ""larger"" sinusoidal wave represents the frequency of the hum. How can I approach this computationally? To find the beat frequency, do I set the superposition function equal to $0$ and then solve? For the hum frequency, equal to $1$? Or what should I do?",['trigonometry']
2546754,Area of enclosed overlapping circles within an equilateral triangle,"Consider an equilateral triangle of length $\sqrt{6}$ as shown in the below figure. Find the area of the shaded region. My attempt : Since the side of the triangle is given, hence height = $\frac{3}{\sqrt{2}}$. Since its an equilateral triangle, so by virtue of symmetry the three circles must meet at the centroid. Hence inradius = $\frac{1}{\sqrt{2} }$ and
Circumradius = $\frac{2}{\sqrt{2} }$. Please guide me how to proceed from here. Any help will be appreciated.","['circles', 'triangles', 'geometry']"
2546778,About some details about the proof that real Lie algebra with positive Killing form is zero,"In the post about proving that real Lie algebra with positive Killing form is zero: real Lie algebra with positive Killing form is zero : Let $L$ be a real Lie algebra with positive definite Killing form. Its Killing
  form $\kappa$ defines an inner product on $L$. Hence $L$ is reductive. Thus the
  quotient $L/Z(L)$ is semisimple. So, the Killing form is negative definite of 
  $L/Z(L)$. Therefore, this Killing form is both positive definite and negative definite, it follows that $L/Z(L) = {0}$. So we get $L = Z(L)=\ker(\kappa)$. But $\kappa$ is non-degenerate since it’s positive definite. It follows that $L= {0}$. I am confused with the following gaps: Why killing form on $L/Z(L)$ is negative definite? Why the induced Killing form on $L/Z(L)$ is positive definite? And is there any relation with the fact that $\mathfrak{g}$ is real?","['linear-algebra', 'lie-algebras']"
2546795,Differentiability of fixed point,I am trying to review some past hw problems. I was never able to figure out how to do this problem. Can anyone help me out at all? Thanks.,['ordinary-differential-equations']
2546826,Is the intersection of a set of cardinals contained in the set?,"Given a set $\mathcal{C}$ of Cardinals, does $$D:=\bigcap_{C\in\mathcal{C}} C \in \mathcal{C}$$ hold?
If $\mathcal{C}$ is finite or contains a finite cardinal, the answer is positive even for ordinals instead of cardinals because of elementary ordinal properties. The infinite case however does not seem so obvious. Assuming the contrary, one could construct an infinite sequence of proper descreasing cardinals $C_0 \supsetneq C_1 \supsetneq C_2 \supsetneq \ldots \supsetneq D$ with all $C_k\in\mathcal{C}$. Intuitively I don't believe that's possible, not even for ordinals. I tried applying Zorn's lemma using $\supseteq$ for $\leq$ to give $D$ a proper structure, but using D as an upper limit with respect to this relation doesn't seem to work because the lemma seems to require $D\in\mathcal{C}$ to work, which is what I want to show. So that's a dead end for me. I'm hardly used to ordinals and cardinals and grateful for help. Background: I'm trying to do some formalization of (Multi)Graph theory in Mizar and given graphs with arbitrary vertex degree, I was wondering if it makes sense do differ between the concept of the infinum of all vertex degrees (i.e. their intersection) and the minimum (if present, smallest vertex degree).","['cardinals', 'elementary-set-theory', 'ordinals']"
2546857,Any solvable group of order sixty has a normal subgroup of order five,"I want to show that any solvable group of order sixty has a normal subgroup of order five. Recall that a solvable group is a group that has a normal series in which every factor is abelian. I think that I can prove this by looking at the Hall subgroups of the group. Let $G$ be a solvable group of order sixty. We know that $60=2^2\cdot 3\cdot 5$. A Hall divisor $d$ of $G$ is a divisor of $60$ such that $\gcd(d, 60/d)=1$, and $H\leq G$ is a Hall subgroup if the order of $H$ is a Hall divisor. In particular, I know that $5$ is a Hall divisor of $G$. Now since $5$ is a prime divisor of $G$, I know that $G$ has a $5$-Sylow subgroup $S$, and I also know that its order has to be $5$, by Lagrange's Theorem. If I can show that $S$ is the only $5$-Sylow subgroup, then I know it is normal, and I am done. I don't know if this is the case, and I don't know how the fact that $G$ is solvable helps. Any help is appreciated, thank you.","['finite-groups', 'abstract-algebra', 'normal-subgroups', 'solvable-groups', 'group-theory']"
2546871,Examples of Axiom of Choice used in introductory-level undergradute math,All of the applications of AoC I've encountered have been in upper level undergraduate or graduate math courses. Are there any basic results from courses like Calc I-III which (unbeknownst to students) rely on AoC?,"['real-analysis', 'education', 'calculus', 'axiom-of-choice']"
2546882,How to find Laurent expansion of $f(z)=\frac{1}{(z+i)z^2}$ on $0<|z-i|<1$,"I think the way is doing the decomposition as follows, but I just don't know how to deal with the quadratic term: $f(z)=\frac{1}{(z+i)z^2}=-\frac{1}{z+i}+\frac{1}{z}-\frac{i}{z^2}$ where, $-\frac{1}{z+i}=-\frac{1}{2i}\frac{1}{1+\frac{z-i}{2i}}$ and $\frac{1}{z}=\frac{1}{1+\frac{z-i}{i}}$ can be expanded easily. But how to deal with $-\frac{i}{z^2}$?","['laurent-series', 'complex-analysis']"
2546898,Implicit equation of a line in 3 Dimensions (disambiguation)?,"The implicit equation of a line in 2D is: $Ax + By +C =0$ It's analogous in 3D is an implicit plane, described by: $Ax+By+Cz+D=0$ What is then the implicit equation of a line described parametrically by: $P=P_0+t\vec v$ In 3 dimensions?","['implicit-function', 'linear-algebra', 'geometry']"
2547000,How to prove that the angle $\angle AMB$ gets its maximum when M is the intersection of y-axis and the minor axis of the ellipse?,"A and B are the intersections of x-axis and major axis of the ellipse, M is a point on the ellipse.","['euclidean-geometry', 'geometry']"
2547048,Find sum to infinite terms of the series $S=\frac{4}{5}+\frac{4.7}{5.8}+\frac{4.7.10}{5.8.11}+\cdots$,Find sum to infinity terms of the series $$S=\frac{4}{5}+\frac{4.7}{5.8}+\frac{4.7.10}{5.8.11}+\cdots$$ My Try: we have $$1+S=1+\frac{4}{5}+\frac{4.7}{5.8}+\frac{4.7.10}{5.8.11}+\cdots$$ now $$(1+x)^n=1+nx+\frac{n(n-1)x^2}{2}+\cdots $$  So comparing we get $$nx=\frac{4}{5}$$ and $$\frac{n(n-1)x^2}{2}=\frac{7}{10}$$ solving for $x$ and $n$ we get $$n=\frac{-16}{19}$$ and $$x=\frac{-19}{20}$$ Hence $$1+S=(1+x)^n=20^{\frac{16}{19}}$$ $$S=20^{\frac{16}{19}}-1$$ Is this alright?,"['binomial-coefficients', 'hypergeometric-function', 'convergence-divergence', 'summation', 'sequences-and-series']"
2547140,Proving trigonometric identity: $ \frac{\sin (y+x)}{\sin (y-x)} = \frac{\tan y + \tan x}{\tan y - \tan x} $ [duplicate],"This question already has answers here : Proving a trig identity: $\frac{\sin(A + B)}{\sin(A - B)}=\frac{\tan A + \tan B}{\tan A - \tan B}$ (5 answers) Closed 6 years ago . Prove that 
  $$ \frac{\sin (y+x)}{\sin (y-x)} = \frac{\tan y + \tan x}{\tan y - \tan x} $$ This is my working - $$\text{LHS} = \frac{\sin (y+x)}{\sin (y-x)} = \frac{\sin y \cos x + \cos y \sin x}{\sin y \cos x - \cos y \sin x} $$ I used the compound angle relations formula to do this step. However , now I'm stuck. I checked out the answer and the answer basically carried on my step by dividing it by
$ \frac{\cos x \cos y}{\cos x \cos y} $ If I'm not wrong , we cannot add in our own expression right? Because the question is asking to prove left is equal to right. Adding in our own expression to the left hand side will be not answering the question, I feel .","['algebra-precalculus', 'trigonometry']"
2547173,Computing: $\lim\limits_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx$,Let $f:\Bbb R \to \Bbb R_+$ be measurable function such that $$\int_\Bbb Rf(x)dx = c$$ Then compute $$\lim_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx$$ Where $a>0 $ is  a parameter. My feeling is that this limit should be $\int_\Bbb Rf(x)dx = c$ but i don't have any good justification so far. any help?,"['real-analysis', 'limits', 'calculus', 'integration', 'measure-theory']"
2547181,Quotient set and cardinality,"Let $R_A= \{\langle x, y\rangle\mid y-x\in A\}$ for $A\subseteq \mathbb{Z}$ Given $A$ is a finite set and $R_A$ is an equivalence relation, what $A$ consists of? For $k\in\mathbb{N}_+$ what is $A$ such that $R_A$ is an equivalence relation and, for it, $|\mathbb{Z}/R_A|=k$? My thoughts are that for $A$ a finite set, $A=\{0\}$, because given $R_A$ equivalence, $\langle x,x\rangle\in R_A$, which means $A$ has to have $0$. I also understand that if $z\in \mathbb{Z}$, $z\in A$, $A$ is not a finite set because for $k\in\mathbb{Z}$, $\langle kz, 0\rangle \in R_A$. Are my thought right? How do I describe them mathemathically? My thoughts are that if $|\mathbb{Z}/R_A|=k$, then $A$ contains the $k$ first prime numbers. I understand that for each $z\in A$ $kz\in A$, but how do I describe the quotient set and equivalence classes? For example, let’s say that $5k\in A$, does this mean that all $5k$ is one quotient set, or each $5k$ is a quotient  set of itself?","['quotient-spaces', 'cardinals', 'elementary-set-theory', 'discrete-mathematics']"
2547251,Analog of Vandermonde determinant for fitting a quadratic form?,"1D interpolation: finding a polynomial satisfying $\forall_i\ p(x_i)=y_i$ can be written as a system of linear equations, having well known Vandermonde determinant : $\det=\prod_{i<j} (x_i-x_j)$. Hence, the interpolation problem is well defined as long as the system of equations is determined ($\det\neq 0$), that is equivalent with condition of having no two repeating $x$-s. I need something analogous for quadratic form in $n$ dimensions: we would like to find symmetric matrix $A$ satisfying $\forall_k\ f(x^k)=y_k$, where $f(x)=x^T A x$, this time $x^k$ are vectors. We get a system of linear equations: 
$$ \forall_k\ \sum_i A_{ii} (x^k_i)^2 + 2\sum_{i<j} A_{ij} x_i^k x_j^k =y_k$$
for $D=n(n+1)/2$ coefficients of symmetric $A$. In analogy to interpolation problem, having values in $D$ points, we would like to find $A$. However, it requires that $\det\neq 0$ for the above set of linear equations. Is there known a compact form for this determinant? (in analogy to Vandermonde) If not, are there some known conditions ensuring it is nonzero - making fitting quadratic form well defined? These conditions need to contain e.g. that no two points are in one line $(x^k=a\cdot x^l)$. In what I need we can assume that all points lie on a sphere. Specifically, my motivation is that looking at eigenspaces of adjacency matrix, we can convert the graph isomorphism problem into a question if two sets of points differ only by rotation ( page 9-11 here . For strongly regular graphs these points are on a sphere and form a very regular polyhedron. Hence, I wanted to use an affine space of quadratic forms defining ""wobbling"" ellipsoids, such that they intersect only in our set - then we could use characteristic polynomial to test if they differ only by rotation. The crucial question is if e.g. $\{x: x^T A x=1 \textrm{ for all }A=A_0 + a\cdot A_1\}$ doesn't add too many extra points to the description. Geometrically: if ""wobbling"" ellipsoids with fixed some points, what extra fixed points would their intersection have? Here is example of 2D situation: describing two points as intersection of ellipses/hyperbolas. Intersection only adds symmetric ($-x$) points, the question is when it is true, also in higher dimensions:","['algebraic-geometry', 'interpolation', 'geometry', 'quadratic-forms', 'linear-algebra']"
2547320,Number of paths in a board with a puncture,"A picture of the board and my solution I have this $8 \times 6$ board but there is a restriction with it - we have a hole with a bottom-left corner at $(3, 2)$ and top-right at $(6, 4)$. I need to count the number of possible ways to get from the top-left corner to the bottom-right corner of the board. Since there are two ways to go around the hole, I counted them and summed them. Is my solution correct or did I miss something? Thank you in advance!","['combinatorics', 'discrete-mathematics']"
2547355,Problems with Fubinis theorem,"Let $f: (0,1] \times [0,1] \rightarrow \mathbb{R}$ , $ (x,y) \mapsto \begin{cases} 
\frac{1}{\sqrt{x^2+y^2}} \  \text{if} \ \ x^2 + y^2 < 1\\
1 \ \text{otherwise}\\
\end{cases}$ $f$ is a continous and integrable function. I want to calculate its integral over $x$ if $y=0$. $f$ is also positive so Fubinis theorem should be valid. ( or is it?) With $y=0$ you get $ g:(x,0) \mapsto \begin{cases} 
\frac{1}{x} \  \text{if} \ \ x^2  < 1\\
1 \ \text{otherwise}\\
\end{cases}$ and $x=1$ is the only case where $f=1$. $\{1\}$ is a null set so it should not change the integral If I want to calculate the integral if $y=0$ I assumed that I could calculate it with the one dimensional $\int_{0}^{1}{\frac{1}{x}dx}$. But it doesn't converge. This would be a cotradiction to Fubinis theorem. Now I'm confused and have problems applying $y=0$ for Fubinis theorem. If I want to use Fubinis theorem for this, is $\int_{(0,1] \times [0,1]}f(x,0) = \int_{[0,1)}\int_{[0,1]}g(x)dydx = \int_{(0,1]}g(x)dx$ this the right way to use it? $g(x)$ is independent from $y$ so $\int_{[0,1]}g(x)dy = g(x)$? Or is its integral just zero because if $y=0$,  $\int_{[0,1]}g(x)dy$ gets $0$ because the intervall gets zero or $dy$ with $y=0$ gets zero?","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2547357,Is a linear map (transformation) always a matrix multiplication,"I am studying linear maps. It is defined as a linear map $L$ which transforms a vector from dimension $n$ to dimension $k$ $L:\mathbb{R}^n \rightarrow \mathbb{R}^k$ This seems to me as a matrix multiplication (from $x$ to $y$): $y = Ax$ My question is, is this correct, and further, can a linear map always be written as a matrix multiplication?","['linear-algebra', 'linear-transformations']"
2547440,If $X$ is diagonal with distinct diagonal entries and $XY = YX$ then $Y$ is also diagonal matrix.,"If $X$ is diagonal with distinct diagonal entries and $XY = YX$ then $Y$ is also diagonal. I was trying to prove this. I tried as follows - Since we know that $X$ is of distinct diagonal entries which means that distinct eigenvalues implying that $X$ is diagonalizable. that is $X = P^{-1}XP$ where $P$ is the matrix whose columns are the eigenvectors of $X$. Now we are given that $XY = YX$ or $X = Y^{-1}XY$ but this is for all $Y$ I guess. Now for $Y = P$ it is clear that $Y$ is diagonalizable, but for other $Y$ how we can prove that they are diagonalizable?","['matrices', 'diagonalization', 'eigenvalues-eigenvectors', 'linear-algebra']"
2547449,Solving Lyapunov's Equation for Linear Systems,"In the ""Lyapunov Stability"" chapter of the text by Khalil, there is an example on how to solve a Lyapunov Equation. Here equation $(3.12)$ is $PA+A^TP=-Q$. How is the $3\times 3$ matrix on the left-side of the first matrix equation formed?","['ordinary-differential-equations', 'linear-algebra', 'stability-theory']"
2547460,What is an example of a continuous function that doesn't have a derivative at any point? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Please give an example of a continuous function $f:[0,1]\to\mathbb R$ which doesn't have a derivative at any point. I can't think of anything, can someone help please?","['derivatives', 'continuity', 'calculus']"
2547487,Every subspace of a separable metric space is separable. [duplicate],"This question already has answers here : Prove that a subset of a separable set is itself separable (4 answers) Closed 5 years ago . I want to know if this is an adequate proof. Proof: Let $X$ be a separable metric space and let $A$ be a subspace of $X$. Since $X$ is separable, it contains a countable dense subset $D$. So $\forall$ neighborhood $U$ of $x$ in $X$, $\exists$ a $d \in D: d\in U$. Let $x\in A\implies U\cap A$ is a neighborhood of $x$ relative to $A$. Which implies that $\exists d\in D:d\in U\cap A$. Let $D^{*}$ represent all elements of $D$ which are also elements of $U\cap A$. This implies $D^{*}$ is dense in $A\implies A$ is separable.","['general-topology', 'proof-verification']"
2547532,Is there a name for these regions produced by 4 intersecting circles? Can their areas be found without calculus?,"Begin with a square. Using the side length as a radius, construct four circles, centered at each vertex. These circles divide the square into nine regions: four of one shape, four of another shape, and one unique one in the center. Have we got a name for any of these shapes? Is is possible to calculate their areas without using calculus? (I have already calculated the areas, using calculus. The large central region has area $1+\frac{\pi}{3}-\sqrt3$, the four regions sharing its boundary each have area $\frac{\pi}{12}+\frac{\sqrt3}{2}-1$, and the four remaining regions, along the edges of the square, each have area $1-\frac{\pi}{6}-\frac{\sqrt3}{4}$, if I didn't make any mistakes.)","['terminology', 'geometry']"
2547535,Wave equation PDE with inhomogeneous boundary,"I found this question on the physics side of our network and tried in vain to solve it. It's better suited here, so I'll post it. We have a vibrating string. One side of the string is fixed, and the other side of the string vibrates by the force $F(t) = A \sin\omega t$. Wave equation in 1D: $u_{tt}=k^2u_{xx}$ Boundary conditions: $u(0, t) = 0,u(l, t) = A \sin \omega t$ $u(x, 0) = u_t(x, 0) = 0$ My attempt: The $\sin \omega t$ term was always going to worry me but I ran into problems well before that! If we use separation of variables with Ansatz: $$u(x,t)=X(x)T(t)$$ and separation constant $-m^2$, we obtain two ODEs: $$\ddot{X}+m^2F=0$$
and:
$$\ddot{T}+m^2k^2G=0$$ The second has the classic solution: $$T(t)=A\cos(mkt)+B\sin(mkt)$$ Using the boundary condition: $u(x,0)=u_t(x,0)=0$:
$$u(x,0)=0\implies T(0)=0$$
$$0=A\cos 0+B\sin 0\implies 0=A+0\implies A=0$$ So $T(t)=B\sin(mkt)$.
And for the second one:
$$u_t(x,0)=0\implies \dot{T}(0)=0$$
$$\dot{T}(0)=Bmk\cos 0=0 \implies B=0$$ Or $mk=0$. Either way this makes: $$T(t)=0$$
A commenter mused ""You need to account for resonant frequencies where nonzero solutions are possible"" but I'm far from sure how to go about that.","['ordinary-differential-equations', 'partial-differential-equations']"
2547568,"Is ""$:$"" so-called Frobenius inner product?","I see some notation like
\begin{align*}
\int \nabla \mathbf{u} : \nabla \mathbf{v} \; dx
\end{align*} Here I think the two vectors $\mathbf{u}$ and $\mathbf{v}$ should be column vectors, i.e. $\mathbf{u} = [u_1,u_2,...,u_n]^T$ and $\mathbf{v} = [v_1,v_2,...,v_n]^T$. Is that right? So, here $\nabla \mathbf{u}$ will be a Jacobian matrix, and $\nabla \mathbf{v}$ as well. Right? Then, does the operation $\nabla \mathbf{u} : \nabla \mathbf{v}$ mean the element-wise multiplication such that
\begin{align*}
\nabla \mathbf{u} : \nabla \mathbf{v} &= (\nabla \otimes \mathbf{u}) : (\nabla \otimes \mathbf{v}) \\
&= \left(
\begin{bmatrix}
\nabla_1 \\
\nabla_2 \\
\vdots \\
\nabla_n
\end{bmatrix}
\begin{bmatrix}
u_1 & u_2 & \cdots & u_n
\end{bmatrix} \right) : \left(
\begin{bmatrix}
\nabla_1 \\
\nabla_2 \\
\vdots \\
\nabla_n
\end{bmatrix} 
\begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix} \right) \\
&= 
\begin{bmatrix}
\nabla_1 u_1 \nabla_1 v_1 & \nabla_1 u_2 \nabla_1 v_2 & \cdots & \nabla_1 u_n \nabla_1 v_n \\
\nabla_2 u_1 \nabla_2 v_1 & \nabla_2 u_2 \nabla_2 v_2 & \cdots & \nabla_2 u_n \nabla_2 v_n \\
\vdots & \vdots & \vdots & \vdots \\
\nabla_n u_1 \nabla_n v_1 & \nabla_n u_2 \nabla_n v_2 & \cdots & \nabla_n u_n \nabla_n v_n
\end{bmatrix}. 
\end{align*}
Am I right? I didn't ever see ""$:$"" before, but I think it is so-called the Frobenius inner product, though its Wiki page doesn't mention this notation.","['tensor-products', 'notation', 'matrices', 'tensors', 'terminology']"
2547583,Prove that $7^{100} - 3^{100}$ is divisible by $1000$,"Prove that $7^{100} - 3^{100}$ is divisible by $1000$ Equivalently, we want to show that $$7^{100} = 3^{100} \pmod {1000}$$ I used WolframAlpha (not sure if that's the right way though) and found that $\varphi (250) = 100$. So by Euler's theorem: $$7^{100} \equiv 7^{\varphi(250)} \equiv 1 \pmod {250} \\ 3^{100} \equiv 3^{\varphi(250)} \equiv 1 \pmod {250}$$ but of course, we want $\pmod {1000}$. Is that what I'm intended to do in this exercise (how to proceed if so)? Is there a solution without the need to use WolframAlpha? Thanks!","['binomial-theorem', 'divisibility', 'number-theory', 'congruences', 'elementary-number-theory']"
2547588,Theorem still true if $X$ is not complete,"Consider the following theorem: Let $S$ be a non-empty set and let $\{0\} \neq X$ be a vector space of bounded functions on $S$, with the condition that $S$ is a Banach space when $X$ is supplied with the supremum-norm. Suppose $f : S → \mathbb{F}$ is a function such that $fg \in X$ for all $g \in X$. Then the multiplication operator
$M_f : X → X$, defined by $M_f (g) = fg$ $(g ∈ X)$, is bounded. I want to know, Is $f$ necessarily bounded? Is this theorem still true if $X$ is not complete. Any insights are much appreciated.","['functional-analysis', 'complete-spaces', 'real-analysis', 'banach-spaces']"
2547615,Let $A$ be an $m\times n$ matrix. Prove that $\operatorname{rank}(AA^T) = \operatorname{rank}(A)$.,"Let $A$ be an $m\times n$ matrix. Prove that $\operatorname{rank}(AA^T) = \operatorname{rank}(A)$ . The problem tells me to prove it with the theorem that $\operatorname{rank}(A^TA) = \operatorname{rank}(A)$ . I'm a bit lost here... $AA^T$ and $A$ don't even have the same number of columns. I'm thinking maybe to prove it by showing that $m - \operatorname{nullity}(AA^T) = n - \operatorname{nullity}(A),$ but then I'm stuck here. Let $A$ be an $m\times n$ matrix. Prove that the column space and row space of $A^TA$ are the same. The problem tells me to prove it also with the theorem $\operatorname{rank}(A^TA) = \operatorname{rank}(A)$ . But I'm really running out of ideas. Help?","['transpose', 'matrix-rank', 'linear-algebra']"
2547644,Function defined as determinant of polynomial matrix,"I have the following function:
\begin{equation*}
f_n(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_n(t)\\
p'_0(t)&\dots&p'_n(t)\\
p''_0(t)&\dots&p''_n(t)\\
\vdots&\ddots&\vdots\\
p^n_0(t)&\dots&p^n_n(t)\end{array}\right)
\end{equation*}
Where $p_i(t)$ are polynomials of degree $\leq n$ and $p^k_i(t)$ is the $k$-th derivative. I'm trying to show that $f(t) = f(0), \forall t$. Now, $f(0)$, as far as I understand, works out to be the determinant of the coefficient matrix multiplied by some constant. I decided to try this by induction, but I'm stuck on the inductive step. This is what I have so far: We prove the general case, for $n \geq 1$ via induction on $n$.
For our base case, let $n = 1$ and let $p_i(t) = a_{i1}t+a_{i0}$ for $i \in \{0,1\}$. We have
\begin{equation*}
f(t) = \det\left(\begin{array}{cc}a_{01}t+a_{00} & a_{11}t+a_{10}\\
a_{01}&a_{11}\end{array}\right)= (a_{01}t+a_{00})a_{11} - (a_{11}t+a_{10})a_{01} \\ = a_{01}a_{11} t-a_{01}
   a_{11}t-a_{01} a_{10}+a_{00}
   a_{11} = a_{00} a_{11}-a_{01} a_{10}.
\end{equation*}
Clearly, $f(t)$ does not depend on $t$, and so, $f(t) = f(0)$ for all $t$. Now, we assume that the result holds for all $i < k$. We show that this implies that the result holds for $k$ as follows.
Let $p_i(t) = a_{ik}t^k+ \dots +a_{i0}$ for $i \in \{0,k\}$. We have
\begin{equation*}
f(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_k(t)\\
\vdots&\ddots&\vdots\\
p^k_0(t)&\dots&p^k_k(t)\end{array}\right)= \sum_{i=0}^k (-1)^{i}p_i(t)\det(M_{1,i+1}) = \sum_{i=0}^k (-1)^{i}p_i(t)f_{{k-1}_{1,i+1}}(0)
\end{equation*}
where $M_{m,n}$ is the minor matrix of the $m$-th row and $n$-th column and $f_{{k-1}_{1,i+1}}$ is $\det(M_{1,i+1})$ evaluated at $t = 0$. In our case, since $p_i(t)$ has degree at most $k$, $p'_i(t)$ has degree at most $k-1$, and so, for any $i$, $M_{1,i+1}$ is a matrix of the given form for $n = k-1$. By the induction hypothesis, for all $i$, $\det(M_{1,i+1})$ is a constant that does not depend on $t$. I'm not really sure how to proceed from here, why does it follow that the resulting equation does not depend on $t$? I guess it has something to do with the fact that we're multiplying each polynomial by the determinant (times some constant) of the remaining polynomials' coefficients (except the constant coefficient, $a_{i0}$, which is lost in the first derivative). I don't see any other links here that would allow me to proceed.","['derivatives', 'polynomials', 'determinant', 'induction', 'linear-algebra']"
2547701,Deriving the density of sum of iid Uniform distributions using Laplace Transforms.,"In Resnick's Adventures in Stochastic Processes, there's this example where the author derives the density of $\sum_i X_i$ where $X_i$ are iid uniform (0,1). In the picture below, I don't understand 2 things: What they mean (and why) by «$e^{-\lambda k}\lambda^{-n}$ is the transform of $\epsilon_k * g(x)$». Here, the $*$ is the symbol for convolution. After deducing the form of the convolution between $\epsilon_k$ and $g(x)$, , the author states the form of the desired density. How does he get that? Any help would be appreciated.","['stochastic-processes', 'probability-theory', 'laplace-transform', 'probability-distributions']"
2547746,$\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6$ injective immersion,"I have to prove this one Given $f:S^2\rightarrow\mathbb{R}^6$ defined by $f(x_1,x_2,x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)$, prove that: $f$ is an immersion; $f(-x_1,-x_2,-x_3)=f(x_1,x_2,x_3)$; exists an injective immersion $\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6$ such that $\tilde{f}([x_1:x_2:x_3])=f(x_1,x_2,x_3)$, $\forall [x_1:x_2:x_3]\in\mathbb{P}^2(\mathbb{R})$. I have this idea of the solution: I compute the Jacobian matrix $J_f(x_1,x_2,x_3)=\left(
\begin{array}{ccc}
2x_1 & 0 & 0 \\
0 & 2x_2 & 0 \\
0 & 0 & 2x_3 \\
x_2 & x_1 & 0 \\
x_3 & 0 & x_1 \\
0 & x_3 & x_2
\end{array}
\right)
$ and since $T_p S^2=\{v=(a,b,c)\in\mathbb{R}^3 : (x_1,x_2,x_3)\cdot(a,b,c)=0\}$, $\forall p=(x_1,x_2,x_3)\in S^2$, the expression $J_f(v)=0$ leads to the sistem $\left\{\begin{array}{l}
2x_1a=0\\
2x_2b=0\\
2x_3c=0\\
ax_2+bx_1=0\\
ax_3+cx_1=0\\
bx_3+cx_2=0
\end{array}\right.$ that has unique solution $(a,b,c)=(0,0,0)$. So $\ker(f_*)=\{0\}$ for every $p\in S^2$, hence $f_*$ is injective and f is then an immersion. $f(-x_1,-x_2,-x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)=f(x_1,x_2,x_3)$. I consider the diagram \begin{array}{ccc}
 & f & \\
S^2 & \rightarrow & \mathbb{R}^6 \\
\pi \searrow & & \nearrow \tilde{f} \\
 & \mathbb{P}^2 (\mathbb{R})& 
\end{array} where $\pi$ is the projection of $S^2$ in $S^2/\sim\cong\mathbb{P}^2(\mathbb{R})$ and since it is a local diffeomorphism, $\pi^{-1}$ is an immersion. Hence because of (1),     $ \tilde{f}=f\circ\pi^{-1}$ is composition of immersion, then an immersion. Moreover it's injective because if we take $[x_1:x_2:x_3]\neq[y_1:y_2:y_3]$ we get $\tilde{f}([x_1:x_2:x_3])\neq\tilde{f}([y_1:y_2:y_3])$. Then we conclude that $\tilde{f}$ is an injective immersion. Is everything all right? Thanks for reading all this stuff","['spheres', 'projective-space', 'differential-geometry']"
2547766,How to show $x^{10}-2x^9+3x^8-...-10x+11=0$ has no real root?,"I tried to solve below equation $$x^{10}-2x^9+3x^8-...-10x+11=0$$ I plot the graph and see there is no real root , But I get stuck how to show analytical .Can some one help me or give an idea ? Thanks in advance. This is graph of the function $$f(x)=x^{10}-2x^9+3x^8-...-10x+11$$ https://www.desmos.com/calculator/6k1lj498ra","['algebra-precalculus', 'proof-writing', 'calculus']"
2547767,Number of apples on the tree after n days?,"Start with one apple on day one. On every following day: Every apple grows $k\cdot(c+1)$ new apples, where $c$ is the number of
  connections to that apple, and $k$ is the number of steps it takes to
  reach the starting apple from that apple (counting the apple itself as
  a step). For example, the first apple takes only one step to itself. The apples
  growing from it take two steps. The apples growing from those take
  three steps, etc. (see images below for more detail) How many apples will be on the tree after  $n$ days? I computed first $25$ terms: (sequence is not in OEIS ) 1, 2, 8, 56, 548, 6752, 99908, 1724816, 34031348, 755384672, 18630078308, 505421692976, 14958279256148, 479591526968192, 16559455408832708, 612609633148083536, 24174100149092384948, 1013551337258199761312, 44995053102770888963108, 2108457649886329729936496, 104001928043774583748777748, 5386506619791901055945028032, 292264718383139371373233669508, 16578710198212615619201747731856, 981315128726093566691094046194548 First four days look like this: How can I find a formula to calculate the number of apples at the
  $n$th day? I don't know how to solve this, so I'm counting apples on each layer individually: Idea was to find a general pattern for computed layers and then sum them all up at the end. Note, Layer $k$ has all apples that take $k$ steps to the starting apple. Let number of layer $k$ apples after $n$ days be given by $a_n(k)$. Then the number of apples on the tree at day $n$ is given by $A_n=\sum_{i=1}^n{a_n(i)}$ . Here is the computed data for first $6$ layers: a_n(1) = 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
a_n(2) = 0, 1, 3, 7, 15, 31, 63, 127, 255, 511, 1023, 2047, 4095, 8191, 16383, 32767, 65535, 131071, 262143, 524287, 1048575, 2097151, 4194303, 8388607, 16777215,... 
a_n(3) = 0, 0, 4, 24, 100, 360, 1204, 3864, 12100, 37320, 114004, 346104, 1046500, 3155880, 9500404, 28566744, 85831300, 257756040, 773792404, 2322425784, 6969374500, 20912317800, 62745342004, 188252803224, 564791964100,... 
a_n(4) = 0, 0, 0, 24, 240, 1560, 8400, 40824, 186480, 818520, 3498000, 14676024, 60780720, 249401880, 1016542800, 4123173624, 16664094960, 67171367640, 270232006800, 1085570781624, 4356217681200, 17466686971800, 69992221794000, 280345359228024, 1122510953731440,... 
a_n(5) = 0, 0, 0, 0, 192, 2880, 26880, 201600, 1334592, 8164800, 47372160, 264844800, 1441632192, 7694406720, 40467248640, 210468585600, 1085328316992, 5559954344640, 28337142664320, 143847569376000, 727922413132992, 3674461807114560, 18512042531347200, 93120150431088000, 467843515029264192,...
a_n(6) = 0, 0, 0, 0, 0, 1920, 40320, 510720, 5080320, 43827840, 344615040, 2541411840, 17896919040, 121797836160, 807731084160, 5251058991360, 33611039804160, 212519521994880, 1330716675415680, 8267671479137280, 51044504568583680, 313546251542832000, 1918022127328137600, 11693253189282297600, 71090720640004665600,... Here is the python code . Update I corrected the layer data; now I managed to find formulas for first $6$ layers: $ a_n(1)=1 $ $ a_n(2)=\frac{1}{2}(2^n-2)$ $ a_n(3)=\frac{2}{3}(3^n-3\cdot2^n+3)$ $ a_n(4)=1( 4^{n} - 4\cdot3^n + 6\cdot2^{n} - 4)$ $ a_n(5)=\frac{8}{5}(5^n - 5\cdot4^n + 10\cdot3^n  - 10\cdot 2^{n}+5)$ $ a_n(6)=\frac{8}{3} (6^n - 6\cdot5^n + 15\cdot4^n - 20\cdot3^n + 15\cdot2^n - 6   )$ I noticed the pattern here, and observed that these sequences are given by: $$ a_n(k)=\frac{\lceil 2^{k-2} \rceil}{k}  \sum_{i=0}^{k-1} (-1)^i  \binom{k}i  (k-i)^n$$ And the number of apples on the day $n$ is thus given by : $$ A(n)= \sum_{k=1}^n\sum_{i=0}^{k-1} (-1)^i  \binom{k}i  (k-i)^n\frac{\lceil 2^{k-2} \rceil}{k} $$ The fact that the formula was observable from the data is nice, but how do you now mathematically show (prove) this expression holds for all $n$? Q: How would one solve this (or similar problems) algebraically to
  arrive at this expression? Without relying on computation and pattern
  analysis? What if the growing condition per apple was slightly changed; what are the methods to solve this and similar problems?","['algebra-precalculus', 'combinatorics', 'sequences-and-series']"
2547849,Any finite group is isomorphic to a subgroup of $A_n$ for some positive integer n,"This is a classical question in the basic group theory and I try to write down the proof : Without loss of generality that we assume the order of $G$ is $n\in\mathbb{N}$ .Then Cayley theorem says that the group $G$ can be embedded in the symmetry group $S_n$ .
In other words, $G\cong\iota(G)\leq S_n\,,$ where $\iota$ is the induced embedding map from $G$ to $S_n$ . Then $S_n$ also can be embedded in $A_{n+2}$ . To this end,we define the following map $\varphi :S_n\longrightarrow A_{n+2}$ by $$\varphi : \alpha \longmapsto \begin{cases}\alpha& ,whenever\;\;\alpha\in A_n\\{}\\
\alpha(n+1\;n+2)&,whenever\;\alpha\in S_n- A_n\end{cases}$$ Then the map $\varphi$ is well-defined monomorphism. Henceforth,combine these two results to obtain an embedding mapping $\varphi\,\circ\iota :G\longrightarrow A_{n+2}$ we then have $G\cong\varphi(\iota(G))\leq A_{n+2}$ , so our conclusion follows. Is there any problem for the above proof ? Any advice will appreciate it.Thanks for considering my request and patiently reading. I take note under the line and everybody can ignore it since there is some tedious,or, any comment will appreciate it. $\rule{18cm}{2pt}$ $\varphi$ is well-defined : For each $\alpha,\beta\in S_n$ with $\alpha=\beta$ . (i): If $\alpha,\beta\in A_n$ ,then $\varphi(\alpha)=\alpha=\beta=\varphi(\beta)$ (ii): If $\alpha,\beta\in S_n -A_n$ ,then $\varphi(\alpha)=\alpha(n+1\;n+2)=\beta(n+1\;n+2)=\varphi(\beta)$ $\varphi$ is a group homomorphism : (i): If $\alpha,\beta\in A_n$ ,then $\alpha\beta\in A_n$ ,moreover, $\varphi(\alpha\beta)=\alpha\beta=\varphi(\alpha)\varphi(\beta)$ (ii): If $\alpha,\beta\in S_n - A_n$ ,then $\alpha\beta\in A_n$ ,moreover, $$\varphi(\alpha\beta)=\alpha\beta(n+1\;n+2)^2=\alpha (n+1\;n+2)\beta(n+1\;n+2)=\varphi(\alpha)\varphi(\beta)$$ (iii): If $\alpha\in S_n-A_n\, ,\beta\in A_n$ ,then $\alpha\beta\in S_n-A_n$ ,moreover, $$\varphi(\alpha\beta)=\alpha\beta(n+1\;n+2)=\alpha(n+1\;n+2)\beta=\varphi(\alpha)\varphi(\beta)$$ (iv): it is just like the case (iii) $\varphi$ is monomorphism: Clearly, $(\,e_{S_n}\,)\subseteq \ker\varphi$ Now, let $\pi\in S_n$ is not the identity one with $\varphi(\pi)=e_{A_{n+2}}\,\,\,.$ Then either $\pi\in A_n$ or $\pi\in S_n-A_n.$ If $\pi\in A_n$ then $\pi=\varphi(\pi)=e_{A_{n+2}}=e_{S_{n}}$ ,a contradiction. If $\pi\in S_n-A_n$ then $\pi(n+1\;n+2)=\varphi(\pi)=e_{A_{n+2}}\,\,$ this implies that $\pi=(n+1\;n+2)\not\in S_n,$ a contradiction. Then $\ker\varphi=(e_{S_n})$ I wrote this proof explicitly, in other words, I checked everything in this problem we needed. So I think there is a quite some  difference with the three answers.","['abstract-algebra', 'group-theory', 'proof-verification']"
2547914,Why does $\lim_{n \to \infty} \left(1+\frac{1}{n}\right)^n=e$ but $\lim_{n \to \infty} \left(1-\frac{1}{n}\right)^n=e^{-1}$?,Why does $$\lim_{n \to \infty} \left(1+\frac{1}{n}\right)^n=e$$ but $$\lim_{n \to \infty} \left(1-\frac{1}{n}\right)^n=e^{-1}$$ Shouldn't the limits be the same since $\left(1+\frac{1}{n}\right) \to 1$?,"['intuition', 'real-analysis', 'calculus', 'limits']"
2547990,Convergence of series - Which test?,"I want to check for convergence the series $\displaystyle{\sum_{n=m}^{\infty}\binom{n}{m}^{-1}}$. $$$$ I have done the following: We use the ratio test. We have that $$a_n=\binom{n}{m}^{-1}=\left (\frac{n!}{m!(n-m)!}\right )^{-1}=\frac{m!(n-m)!}{n!}$$ and $$a_{n+1}=\binom{n+1}{m}^{-1}=\left (\frac{(n+1)!}{m!(n+1-m)!}\right )^{-1}=\frac{m!(n+1-m)!}{(n+1)!}$$ Therefore we get \begin{align*}\left |\frac{a_{n+1}}{a_n}\right |&=\frac{m!(n-m+1)!}{(n+1)!}\cdot \frac{n!}{m!(n-m)!}\\ & =\frac{(n-m)!(n-m+1)}{n!(n+1)}\cdot \frac{n!}{(n-m)!}\\ & =\frac{(n-m+1)}{(n+1)}\underset{n\rightarrow \infty}{\longrightarrow }1\end{align*} So, we cannot say something by the ratio test. Which convergence test should we use instead?","['convergence-divergence', 'sequences-and-series', 'analysis']"
2548041,Finding the turning points of $f(x)=\left(x-a+\frac1{ax}\right)^a-\left(\frac1x-\frac1a+ax\right)^x$,"I've just come across this function when playing with the Desmos graphing calculator and it seems that it has turning points for many values of $a$. So I pose the following problem: Given $a \in \mathbb{R}-\{0\}$, find $x$ such that $\dfrac{dy}{dx}=0$ where $y=\left(x-a+\dfrac1{ax}\right)^a-\left(\dfrac1x-\dfrac1a+ax\right)^x$ As in most maxima/minima problems, we first (implicitly) differentiate it and set to $0$ to give $$\boxed{\small\dfrac{a(ax^2-1)}{x(ax^2-a^2x+1)}\left(\dfrac{ax^2-a^2x+1}{ax}\right)^a=\left(\ln\left(\dfrac{a^2x^2-x+a}{ax}\right)+\dfrac{a(ax^2-1)}{a^2x^2-x+a}\right)\left(\dfrac{a^2x^2-x+a}{ax}\right)^x} \tag{1}$$ I have no idea how to continue from here. I thought about taking logarithms, but it appears to me that the double $\ln$ in the term $\dfrac{a^2x^2-x+a}{ax}$ would only make the equation worse. (For the simplest case when $a=1$, the problem is easy: $x=1$ and it is a point of inflexion). Let's try setting each of the terms to $0$: Case $1$ : $\left(\frac{a^2x^2-x+a}{ax}\right)^x=0$ $ \hspace{1cm}$ This is only possible when the fraction is zero; that is, solving $a^2x^2-x+a=0$ to get $$x=\frac{1\pm\sqrt{1-4a^3}}{2a^2}$$ Case $2$ : $\left(\frac{ax^2-a^2x+1}{ax}\right)^a=0$ $ \hspace{1cm}$ This gives $$\begin{align}ax^2-a^2x+1=0&\implies a^2x^2+a=a^3x\\&\implies\left(\dfrac{a^2x^2-x+a}{ax}\right)^x=\left(\dfrac{a^3x-x}{ax}\right)^x=\left(\dfrac{a^3-1}{a}\right)^x=0\end{align}$$ $ \hspace{1cm}$ so for equality between LHS and RHS, we must have $a=1$. However, the equation $ \hspace{1cm}$ $ax^2-a^2x+1=0$ has no real solutions for such $a$; hence we reach a contradiction. Case $3$ : $\frac{a(ax^2-1)}{x(ax^2-a^2x+1)}=0$ $ \hspace{1cm}$ We have $x=\pm \dfrac1a$. Now LHS is $0$, and $$\left(\dfrac{a^2x^2-x+a}{ax}\right)^x=\left(1-\dfrac1a+a\right)^{\frac1a} \neq 0$$ $ \hspace{1cm}$ for $a \in \mathbb{R} - \{\phi\}$, where $\phi$ is the golden ratio. $ \hspace{1cm}$ Suppose that $a = \phi$. Then $x$ is forced to be $-\dfrac1a=-\dfrac2{1+\sqrt5}$, since $ax^2-a^2x+1=0$ $ \hspace{1cm}$ (undefined) when $x=\dfrac 1a$. This is impossible, since $y$ is only defined when $x>0$ for this $ \hspace{1cm}$ value of $a$! Case $4$ : $\ln\left(\frac{a^2x^2-x+a}{ax}\right)+\frac{a(ax^2-1)}{a^2x^2-x+a}=0$ $ \hspace{1cm}$ This is impossible from cases $1$ and $3$. UPDATE : I have provided a partial answer to my question, now with $x$ removed from it. Any hints on how to solve $(3)=(4)$ are welcome. Here, on MathOverflow : https://mathoverflow.net/questions/302105/on-finding-the-critical-points-of-fx-leftx-a-frac1ax-righta-left-fra","['derivatives', 'implicit-differentiation', 'stationary-point', 'functions']"
2548057,Does this converge to $\int f \phi '$?,"Let $f \in L^1(\mathbb R)$ and $\phi \in C_c^{\infty} (\mathbb R)$. I am pretty certain that the following is true: $$\lim_{h \to 0} \int_{\mathbb R} f(x) \frac{\phi(x-h) - \phi(x)}{h} dx = - \int_{\mathbb R} f(x) \phi'(x) dx$$ However, I can't prove it. I want to use dominated convergence, but I can't seem to find a dominating function. Any help?","['lebesgue-integral', 'measure-theory', 'limits']"
2548098,differential equations solve for exponents,"Consider $x' = A(t)x$, $x \in \mathbb{R}^n$ where $A$ is $2\pi$-periodic. $$A(t) = \begin{bmatrix} 1+\sin(t)&0&0\\ 0&3&4\\ 0&1&3\end{bmatrix}$$ The question is to find the Floquet exponents and it also asked to find Lyapunov exponents. I am kind of new to ordinary differential equations, and it would be very nice to show step by steps to solve this problem to firmly understand the concept.",['ordinary-differential-equations']
2548113,Prove that $f(x) = (1 + x)^{\frac{1}{x}}$ is continuous,"Prove that $f(x) = (1 + x)^{\frac{1}{x}}$ is continuous on the region $(-1, \infty) \subset \mathbb{R}.$ Attempt at a solution: We need $|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < \epsilon$ whenever $|x-y| < \delta.$ It seems like the proper course of action would be to take
$$|x-y| < \epsilon^{xy}$$
$$|x-y|^{1/xy} < \epsilon$$
$$|(1+x)-(1+y)|^{1/xy} < \epsilon$$
And then show
$$|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < |(1+x)-(1+y)|^{1/xy}$$
Which is just proving that $$|a^c - b^d| < |a-b|^{cd}$$
Without loss of generality we assume $a^c > b^d$ so we just need to show $$a^c - b^d < |a-b|^{cd}$$
It seems like the binomial theorem is in order, but I'm not quite sure how to apply it here. P.S.
We know that
$$\lim_{x\to 0} f(x) =e.$$ So just define $f(0) = \lim_{x\to 0} f(x).$","['real-analysis', 'analysis']"
2548133,Show convergence/divergence of series,"We have that $(a_k)$ and $(b_k)$ are two sequences of positive numbers. I want to show the following: If $\lim_{k\rightarrow\infty}\frac{a_k}{ b_k}= c > 0$, then both $\sum_{k=1}^{\infty}a_k$ and $\sum_{k=1}^{\infty}b_k$ converge  or both diverge. If $\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}$ for almost each $k$, then from the convergence of $\sum_{k=1}^{\infty}b_k$ we get the convergence of $\sum_{k=1}^{\infty}a_k$ and from the divergence of $\sum_{k=1}^{\infty}a_k$ we get the divergence of $\sum_{k=1}^{\infty}b_k$. $$$$ Could you give me a hint how we could show that? Do we have to apply a convergence test? We have that $$\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}\Rightarrow \frac{b_{k+1}}{ b_k}\geq \frac{a_{k+1}}{ a_k}$$ Do we apply here the ration and the direct comparison test?","['convergence-divergence', 'divergent-series', 'sequences-and-series', 'analysis']"
2548163,Tangent space as the set of all derivations,"I am trying to get a grip on the concept of derivations at a point on a manifold by working out some concrete examples. Let $M$ be a smooth manifold with or without boundary, and let $p \in M$ . A linear map $v : C^{\infty}(M) \to \mathbb{R}$ is called a derivation at $p$ if it satisfies $$v(fg) = f(p)v(g) + g(p)v(f)$$ The set of all derivations of $C^{\infty}(M)$ at $p$ is denoted by $T_pM$ and is a vector space called the tangent space to $M$ at $p$ . Now let's look at an example; from single-variable calculus. Take $M = \mathbb{R}$ and $f(x) =\sin(x)$ and $p = \pi$ . We have $f'(p) = \cos(p) = -1$ . Now let's look at the general definition above, and note that for any $a \in \mathbb{R}^n$ , the $n$ derivations $$\frac{\partial}{\partial x^i}|_{a} \ \text{ defined by } \ \frac{\partial}{\partial x^i}|_{a}f = \frac{\partial f}{\partial x^i}(a)$$ for $i \in \{1, .., n\} $ form a basis for $T_a(\mathbb{R}^n)$ and has dimension $n$ . So pick $v \in T_p(\mathbb{R}^1)$ , since $v$ is a linear-combination of basis elements in a vector space of dimension $1$ , we have $v = c \cdot \frac{\partial}{\partial x}|_{p}$ for some $c \in \mathbb{R}$ . So $v(f)  = c \cdot \frac{\partial f}{\partial x}|_{p} = c\cdot \frac{d f}{d x}|_{p} = c\cdot cos(\pi) = -c$ for some $c \in \mathbb{R}$ But we need $v(f) = -1$ , for the computation from the general definition and usual calculus to coincide. Have I done anything wrong here? Shouldn't $v(f) = -1$ ?","['smooth-manifolds', 'differential-geometry']"
2548179,Area of a Fractal Tiling of a Circle,"A circle of area 1 is partitioned into square pieces in an iterative fashion, wherein each step of the iteration the largest square possible is cut out from all non-square pieces remaining from the previous step. Because squares do not tile a circle, there will be an infinite number of iterations producing an infinite number of ever-smaller squares. As the number of iterations approaches infinity, what will the average area of the squares converge to? What will the square root of the average area$^2$ converge to? Thanks in advance!",['geometry']
2548185,Finding the interval for which $f(x) =x+2\sin(x)$ is increasing,"I have a the function $f(x)=x+2\sin(x)$ and I want to find the increasing interval. So I find the derivative when it's larger than 0. Hence $f'(x)>0$ when $2\cos(x)>-1$. So by figuring when $f'(x) = 0$ and got it to $\cos(x)=-\frac{1}{2}$ so $x=\frac{4\pi}{3}$ according to the formula the increasing interval is between
 $(-\frac{4\pi}{3}+2\pi n,\frac{4\pi}{3}+2\pi n)$ I don't really understand how that's possible. Shouldn't it during some instance decrease within the interval? Is there some program where I could visualise the increase between these points?","['monotone-functions', 'trigonometric-series', 'calculus', 'functions']"
2548192,Defining $e^{i \theta}$,"I want to define $e^{i z}$ without using power series. My approach was as follows: Start with 
$$\lim_{n \to \infty} \left(1+\frac{i}{n}\right)^{nz}$$
Then make the substitution
$$\frac{1}{m} = \frac{i}{n},\ \text{ so } n=mi.$$
And we have
$$\lim_{n \to \infty} \left(1+\frac{1}{m}\right)^{miz}$$
This almost works, but for $n \to \infty$ we have $m \to i \infty.$ So at this point we can't say
$$\left[\lim_{m \to \infty} \left(1+\frac{1}{m}\right)^{m} \right]^{iz}=e^{iz}.$$
Because we don't have $m \to \infty.$ So my next idea was to try
$$\lim_{x \to 0} \left(1+ ix\right)^{z/x}$$
Then make the substitution $y=ix.$ Then for $x \to 0$ we have $y \to 0.$ So we can say
$$\lim_{y \to 0} (1+y)^{iz/y}$$
It is tempting now to write
$$\left[\lim_{y \to 0} (1+y)^{1/y}\right]^{iz} = e^{iz}.$$
But we don't know that 
$$e = \lim_{y \to 0} (1+y)^{1/y}.$$
We do know that
$$e = \lim_{y \to 0+} (1+y)^{1/y}.$$
For $y \in \mathbb{R}.$ In other words, we know that the right limit of $(1+y)^{1/y}$ is $e$, but I wan't to be able to show that its true for all $y \in \mathbb{C}$ and for all ""paths"".","['complex-analysis', 'analysis']"
2548196,Division by $0$ and its restrictions,"Consider the following expression: $$\frac{1}{2} \div \frac{4}{x}$$ Over here, one would state the restriction as  $x \neq 0 $, as that would result in division by $0$. But if we rearrange the expression, then: $$\begin{align}
\frac12\div\frac4x &= \frac{1}{2} \times \frac{x}{4} \\
&= \frac{x}{8}
\end{align}$$ In this expression, there are no restrictions. If we substitute $x = 0$, then the answer is $\frac{0}{8} = 0$. So, how come in the first unsimplified expression, when we substitute $ x =0$, we get undefined , whereas in the simplified expression we get $0$?","['algebra-precalculus', 'rational-numbers', 'proof-explanation', 'divisibility']"
2548216,"On the asymptotics of $a_n=a_{n-1}^k+a_{n-2}^k$ where $k>1$ and $a_0=0, a_1=1$","Consider the sequence defined by $a_0=0,a_1=1, a_n=a_{n-1}^k+a_{n-2}^k$, where $k$ is a fixed integer larger than $1$. One finds $a_n\sim a_{n-1}^k$ and thus $a_n\sim \alpha_k^{k^n}$ where $\alpha_k$ is a constant which depends on $k$. It seems that $\lim\limits_{k\to\infty}\alpha_k=1$. If so, how can it be proven?","['real-analysis', 'limits', 'asymptotics', 'calculus', 'sequences-and-series']"
2548236,Antiderivative of $e^x/(1+2e^x)$,"I know the solution is $$\dfrac{\ln\left(2\mathrm{e}^x+1\right)}{2}+C$$ However, the result I got was $$\dfrac{\ln\left(\mathrm{e}^x+0.5\right)}{2}+C$$ What I did was: $$ \begin{align}
\int \frac {e^x}{1+2e^x}dx &= \int \frac {e^x}{2*(0.5+e^x)}dx \\
&= 0.5 \cdot \int \frac{e^x}{0.5 + e^x} dx \\
&= 0.5 \cdot (\ln|0.5 + e^x| + C). \end{align}$$ I know there are antiderivative calculators online that show the correct method step by step, but I can't understand what I did wrong. What's the mistake?","['integration', 'calculus']"
2548249,Exterior covariant derivative and Lie derivative in Penrose abstract index notation?,"How does one express the Lie derivative of tensors, and exterior covariant derivative for forms with values in a vector bundle in Penrose abstract index notation? I've tried looking through Penrose's negative dimensional tensors article but didn't see it written down. For say a vector-bundle valued 3-form $\omega_{bcd}^A$ where upper case indices correspond to the vector bundle and lower case indices are form indices,  would the exterior covariant derivative just be $\nabla_{[a}\omega_{bcd]}^A$, where square brackets indicate antisymmetrization?","['tensors', 'vector-bundles', 'differential-geometry']"
2548266,Are there uncountably many surjections from $\mathbb N$ to $\mathbb N$?,"Statement: There are uncountably many surjections from $\mathbb N$ to $\mathbb N$. Is the following proof valid: The real numbers are uncountable. There exists a bijection between the set of real numbers and the set of all functions from $\mathbb N$ to $\{0,1\}$. Given a function, $f$, from $\mathbb N$ to $\{0,1\}$, construct a surjection from $\mathbb N$ to $\mathbb N$, denoted $g$, as follows: We map the $n$th composite number to $n$, and the nth prime, to $f(n)$. Clearly this is a surjection since the composite numbers are countable. Clearly different functions $f$ lead to different surjections $g$. Since there are uncountably many $f$s, there are uncountably many surjections $g$.","['elementary-set-theory', 'functions']"
2548290,Find all prime numbers $p$ such that $16p+1$ is a perfect cube,What I have attempted: Suppose  $16p+1=k^3$ where $k \in Z$ then $16p=k^3-1=(k-1)(k^2+k+1)$ so we can say that $k=17$ and thus $p=17^3+17+1=4931$ which is prime. How would I find the remaining numbers?,['number-theory']
2548309,Every prime ideal of height 1 in a UFD is principal,"If $R$ is a
  UFD, then every prime ideal of height 1 in $R$ is principal. I find this is a direct consequence of Kaplansky's theorem. But the proof of Kaplansky's theorem is difficult for me.
 So I wonder how to prove the statement directly.","['abstract-algebra', 'ring-theory', 'unique-factorization-domains', 'commutative-algebra']"
2548314,What is lag in a time series?,"I am curious about what a lagging time series is.
On investopedia, I saw an article that said that:
""Autocorrelation is degree of similarity between time series and a lagged version of itself over successive intervals."" Someone please explain to me what ""lagged"" means, and why autocorrelation matters in relation to time series analysis. Does autocorrelation mean that the time series will perform like the past? Thanks! Edit: Thanks for everyone's answers, especially the 2 thumbs-up answer earlier. That was very helpful. Now I am wondering why autocorrelation even matters. Sure a function may correlate with a shifted version of itself, but who says that that function will perform like that? Is it just through correlation? Why does this matter in context of autoregressive models, and how did we develop this autocorrelation, then ARM/ARIMA kinda thing to model time series in the first place. Who developed time series?","['time-series', 'statistics']"
2548319,Computing an infinite product,"Let $a_0=5/2$ and $a_k=a^2_{k-1}-2$ for all $k\geq 1$.The question is to compute $$\prod_{k=0}^{\infty} \left(1-\frac{1}{a_k}\right)$$ I tried to calculate few terms.$a_0=5/2$, $a_1=17/4,a_2=257/16$ it seems that $a_k$ is of the form $2^{2^k}+2^{-2^{k}}$ however I am not sure about it.How to proceed without doing much guesswork.Any ideas?","['infinite-product', 'sequences-and-series']"
2548326,"In a conservative line integral, do flipping the points make the integral negative?","I know that conservative line integrals are path independent. But what do flipping the points change the value of the integral. For example any path from A to B in a certain domain will be the same, but will B to A just be the negative of it? I know that for vector fields, it's generally turns negative and for scalar fields it's the same value regardless of the starting and ending points. But for conservative vector fields, is it any different? Yeah I just realized I was stupid. You're going with the vector or against the vector so the 'work' or the value will be negative if you flip the points. Thanks for the comments people!","['multivariable-calculus', 'integration', 'partial-derivative']"
2548330,Growth Rate calculation,"It is given that in the current year the total Investment Capital is USD 13.8 trillion. 
The investment Capital contains Human capital worth of USD 3.3 trillion. 
The question is If total Innovation Capital were to grow by 5% per year in the future, which of the following would be the MINIMUM required annual growth in Human Capital that would see it represent more than half of total Innovation Capital in 10 years?
A. 10% B. 15% C. 20% D. 25% I know the answer is B. How I reached it is : $$
\frac{1 * (x)^{10}}{4*(105)^{10}} = \frac{1}{2}\\
x^{10} = 2* (105)^{10}\\
x = (1.07 \times 105)^{10}\\
x = 112.5
$$
So I concluded the growth rate is $12.5\%$ for it to be exactly twice. For more than twice it should be $15\%$. I used a calculator to get the value $2^{1/10}$. But in this question, I am not supposed to use a calculator. So what am I missing? How can one do this calculation or estimate an answer without a calculator for this question?","['algebra-precalculus', 'approximation']"
2548353,Combinatorics and Matrices,"Find the number of $4\times4$ matrices such that $|a_{ij}| = 1 \forall i,j\in[1,4]$ , and sum of every row and column is zero. I tried 'counting' the number of matrices that satisfy the above conditions, that is, elements are $1$ or $-1$ and sum of every row and column is zero. In the attempt to generate a recursion I started off with a $2\times2$ matrix, for which case the answer is $2$. (First element is 1 or -1, other elements are decided accordingly) 
However, this method becomes cumbersome and mathematically disappointing for $3x3$ and larger matrices. Could someone please explain the method, or post a solution to the problem? 
Is it possible to generalise the result to an nxn matrix?","['matrices', 'combinatorics', 'linear-algebra']"
2548364,"Surjections commuting with the function $t:\mathbb N\to\mathbb N$, $t(n) = n+1$?","Let $t: \mathbb N \rightarrow \mathbb N$ be defined as $t(n) = n+1$. How many surjections $g:\mathbb N \rightarrow \mathbb N$ commute with $t$? Is the following proof correct? We require that $t(g(n)) = g(t(n))$ for every $n\geq 1$. Since 
$$g(t(n)) = g(n+1) \text{ and }\ t(g(n)) = g(n) + 1,$$
The condition $t(g(n)) = g(t(n))$ implies $g(n+1) = g(n) + 1$. For $g$ to be a surjection, $g(1) = 1$: Otherwise, if $g(1) = m>1$, then $g(n) = m + n - 1$. Since $g$ is a surjection, there exists $k > 1$ such that $g(k) = m + k - 1 = 1$. So $m + k = 2$. This is a contradiction since $m,k > 1$, and so $m + k > 2$. Thus $g$ is not a surjection. So $g(1) = 1$. Thus $g(n) = n$ for every $n \geq 1$. So the surjection is uniquely determined by the hypothesis. So there is only one surjection commuting with $t$, namely the identity function. Is this correct?","['elementary-set-theory', 'functions', 'proof-verification']"
2548438,"Integral asked on MIT bee qualifier 2012 $\int\frac{x-1}{(x+1)\sqrt{x^3+x^2+x}}\,dx$ [duplicate]","This question already has answers here : $\int_{0}^{1}\frac{1-x}{1+x}.\frac{dx}{\sqrt{x+x^2+x^3}}$ (3 answers) Closed 6 years ago . I have been trying to solve this one but I have no clue, I put it into wolfram and the result is absurd, considering this one is taken from MIT bee qualifier 2012, this is the integral:
$$\int\frac{x-1}{(x+1)\sqrt{x^3+x^2+x}} \, dx$$","['integration', 'calculus']"
2548451,Souslin set and Projection of closed set,"(Souslin set definition)
Let $X$ be a topological space. A set $B \subseteq X$ is called Souslin set if there is a family of closed sets $\{F_s|s \in \mathbb{N}^{<\mathbb{N}} \}$ in $X$ such that $$B=\bigcup_{\sigma \in \mathbb{N^N}} \bigcap_{n=1}^{\infty} F_{\sigma\upharpoonright n}.$$ I want to prove that statement. ""If $A$ is Souslin set, then there exist a closed set $H$ in $X \times \mathbb{N^N}$ such that $A=\pi_X(H)$ where $\pi_X$ denotes the projection onto the first coordinate. "" I tried to prove it. Since $A$ is Souslin set, $A=\bigcup_{\sigma \in \mathbb{N^N}} \bigcap_{n=1}^{\infty} F_{\sigma\upharpoonright n}.$ I construct a set $H=\bigcap_{n=1}^{\infty} \bigcup_{s \upharpoonright n \in \mathbb{N}^n} F_{s \upharpoonright n} \times I(s\upharpoonright n),$ where $I(s\upharpoonright n)=\{t\in \mathbb{N^N}|t\upharpoonright n=s\upharpoonright n \}.$ I know that $\pi_X{H}=A$ but I'm not sure that $H$ is a closed set in $X \times \mathbb{N^N}.$ Is it true? How to prove that $H$ is closed in $X \times \mathbb{N^N}.$ Thank you very much.","['descriptive-set-theory', 'general-topology']"
2548457,Will a 2 dimensional random walk with random orientations almost certainly return near the origin infinitely often?,"It is well known that if you perform a random walk on a 2 dimensional lattice then you will almost certainly reach every lattice point infinitely many times.  Is the same result true if, instead of walking on a lattice, we walk in a random orientation (always using a distance of 1)? Of course, we cannot expect to land on any given point with positive probability, so we modify the question to ask: in any disk, is the probability 1 that the random walk will eventually enter?  I think this is equivalent to asking if the random walk will eventually any specific disk infinitely many times (e.g. one around the origin), because then there is a positive probability of taking any set of paths with a positive probability to get to the other disk, which is in theory not hard to construct. What I have tried: One approach is to use the result on the lattice (by converting a random walk on the plane to a random walk in the lattice) to prove this result, but I haven't made any meaningful progress in this direction. Another approach is to mimic a proof that works over the lattice.  The only proof strategy I am somewhat familiar with to prove the result over the lattice (although I am aware that there are others) is to show that the sum of the probabilities that you are at the origin after $n$ steps for each $n$ diverges, and then showing that this implies that the probability that you will return infinitely many times is 1.  But it seems neither step generalizes directly.  Something we could try is to prove that. for any point $P$ and any circle of radius $r$ containing $P$, the sum of the probabilities that the walk (re-)enters that circle on step $n$ is infinite; I think this would fix the second step to work in this case by using the argument from page 163 of https://services.math.duke.edu/~rtd/PTE/PTE4_1.pdf (the proof of theorem 4.2.2) by letting $r$ be half the radius of the original disk and always choosing a circle containing the origin.","['random-walk', 'probability']"
2548475,Linear Dependency of two functions,"How the functions $f(x) = x^2$ and $g(x) = x|x|$  are linearly independent for $-\infty \lt x \lt \infty$? My Try:- I first divided the interval in two parts $x \le 0$ and $ x \gt 0$. Then I calculated the Wronskian of the two functions separately which came out to be zero in both the cases. Then why these functions are linearly independent? Is is  so because we are not able to find any nonzero constants c and k for which $cf(x)  + kg(x) \ne 0 $. And if answer is yes, then why do they contradict with Wronskian?","['ordinary-differential-equations', 'calculus']"
2548494,Does Cauchy-Schwarz Inequality depend on positive definiteness?,"Let $V$ be a vector space over $\mathbb{R}$. Suppose we have a product $\langle \cdot,\cdot\rangle:V^2\to \mathbb{R}$ that satisfies all the inner product axioms except the second part of positive-definiteness: $$\langle x,x\rangle=0\iff x=0\tag{1}$$ So far, every proof I've seen that the Cauchy-Schwarz Inequality holds for all inner product spaces uses $(1)$. But does a proof of the Cauchy-Schwarz Inequality necessarily depend on $(1)$? Specifically, I'm looking for one of the following: A proof that the Cauchy-Schwarz Inequality holds for all ""inner product spaces"" where $(1)$ does not necessarily hold. A counterexample of a product $\langle \cdot,\cdot\rangle$ that follows symmetry, linearity in the first parameter, and $\langle u,u\rangle\ge 0$ for all $u\in V$, but where $\lvert \langle u,v\rangle\rvert\le \lvert\lvert u\rvert\rvert\ \lvert\lvert v\rvert\rvert$ does not always hold. I suspect that there is a counterexample, but it's hard for me to come up with one.","['cauchy-schwarz-inequality', 'inner-products', 'linear-algebra', 'vector-spaces']"
2548499,Do harmonic maps into spaces of negative curvature locally minimize energy?,"$\newcommand{\N}{\mathcal{N}}$
$\newcommand{\M}{\mathcal{M}}$
$\newcommand{\g}{\mathfrak{g}}$
$\newcommand{\g}{\mathfrak{h}}$
$\newcommand{\IP}[2]{\left\langle #1,#2 \right\rangle}$
$\newcommand{\Volg}{\operatorname{Vol}_\g}$
Let $\M,\N$ be oriented Riemannian manifolds, and suppose the sectional curvature
of $\N$ is negative. (Suppose also $\M$ is compact and connected). Let $\phi:\M \to \N$ be harmonic map. Is it true that $\phi$ is a local minimizer for the Dirichlet energy? i.e let $\phi_t$ be a smooth variation of $\phi$. Is it true that $E(\phi) \le E(\phi_t) $ for sufficiently small $t>0$? (The sufficiently small interval $I$ of the ""good"" $t$'s can depend on the variation of course). Let us restrict the discussion for variations $\phi_t$ whose variation fields $\left. \frac{\partial\phi_t}{\partial t}  \right|_{t=0}$ are not identically zero. I proved that if the rank of $d\phi$ is everywhere $\ge 2$, then $\phi$ is a local minimizer. (See my proof below). Also, if $\text{rank }d\phi=0$ everywhere, i.e $\phi$ is constant, then $\phi$ is also a local minimizer (it's a global minimizer...). So, the question remains: Suppose $\phi$ is a non-constant harmonic map which has at least one point of degree less than $2$. Is $\phi$ a local minimizer? Proof that $\text{rank }d\phi \ge 2 \Rightarrow$ $\phi$ is locally minimizing: We first note that
$$ \left. \frac{\partial^2}{\partial t^2}  E(\phi_{t}) \right|_{t=0}=H_{\phi}(V,V), $$ where $H_{\phi}$ is the hessian of the energy functional at $\phi$, and $V=\left. \frac{\partial\phi_t}{\partial t}  \right|_{t=0}$ is the corresponding variation field. From the second variation formula, we obtain $$
\begin{split}
H(E)_{\phi}(V,V)&= \int_{\M} \IP{J_{\phi}(V)}{V}  \Volg \\
&=\int_{\M}  -\sum_i \IP{R^{T\N}(V,d\phi(e_i))d\phi(e_i)}{V}+\IP{d_{\nabla^{\phi^*(T\N)}}V}{d_{\nabla^{\phi^*(T\N)}}V} \Volg \\
& \ge -\sum_i \int_{\M} \IP{R^{T\N}(V,d\phi(e_i))d\phi(e_i)}{V}.
\end{split}
$$ By our assumption (non-zero variation field), there exist $p \in \M$ such that $V_p \neq 0$.
If $V_p,d\phi_p(e_i(p))$ are linearly dependent, then since $V_p \neq 0$,  $ d\phi_p(e_i(p)) \in \text{span} \{V_p\}$. Since we assumed $\text{rank }(d\phi_p) \ge 2$, not all the $d\phi_p(e_i(p))$ are linearly dependent of $V_p$; Thus, there exist an $1 \le i \le d$ where they are independent, hence $\IP{R^{T\N}(V,d\phi(e_i))d\phi(e_i)}{V} < 0$ by the curvature assumption. Recall that the sectional curvature of the plane spanned by $x,y \in T_q\N$ is $$ K(x,y)=\frac{\IP{R^{T\N}(x,y)y}{x} }{|x \wedge y|^2}.$$","['reference-request', 'calculus-of-variations', 'riemannian-geometry', 'differential-geometry']"
2548513,Maximum number of circle packing into a rectangle,"I'm asked to pack the maximum number of 10m^2 circle into a 257 x 157m rectangle . After a lot of research, I found out that there are no optimal solution. So, i try to pack as many as possible (taking this website as reference): 1) First, I tried to place them in rectangular pattern : I had the width 257/d (diameter) -> I got about 72.024 --> So along the width, i can place 72 circle . I had the height 157/d (diameter) -> I got about 43.999 --> So along the height, i can place 43 circle . --> That means in this case, i can fit in 43*72= 3096 circles 2) Then I try triangular pattern , which can fit more circles, 3575 circles. However, I find my math calculation kinda inefficient, long, and not correct in any other cases. So my question is : Did I calculate it in a correct way? Are there any other more effective calculation methods? Because in later question, it asks me to find the area of the circle to so that we get the maximum profit . Giving the profit of each circle is: P(a) = 200 - 200/a (a is the area of the circle)","['packing-problem', 'calculus']"
2548584,Uniform integrability of continuous function of conditional expectations,Let $X$ be an integrable real valued random variable. Let $\sigma_n$ be a sub-sigma-algebra such that $\sigma(X) = \sigma(\cup_{n\in\mathbb{N}} \sigma_n)$. Suppose $f(X)$ is integrable where function $f:\mathbb{R}\to\mathbb{R}$ is continuous. Suppose $f(E[X \mid \sigma_n])$ is integrable for all $n$. I know that $\{E[X \mid \sigma_n]:n\in\mathbb{N}\}$ is uniformly integrable. My question is whether $\{f(E[X \mid \sigma_n]):n\in\mathbb{N}\}$ is also uniformly integrable? Thank you.,"['conditional-expectation', 'probability-theory', 'examples-counterexamples', 'uniform-integrability']"
2548590,"Picard Iteration of $Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds$","Let $(t_0, y_0) \in \mathbb{R}$, $c \in \mathbb{R}$ and define $Y_0(t) = y_0$, $$Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds.$$ I need to compute $Y_n(t)$ and $Y(t) = \lim_{n \rightarrow \infty} Y_n(t).$ I know that I need to compute $Y_n(t)$ by induction but I am getting stuck on coming up with a general forumla for $Y_n(t)$. So far I have that $$Y_1(t) = y_0 \left( 1+c \frac{t^2-t_o^2}{2} \right)$$ and $$Y_2(t) = y_0 \left( 1+c \frac{t^2-t_0^2}{2} - c \frac{(t^2-t_0^2)t_0^2}{4} + c^2 \frac{t^4-t_0^4}{8} \right).$$
I can compute the expression for $Y_3(t)$, and so on, but I'm not seeing a general forumla for $Y_n(t)$ so that I can use it to do the proof by induction.","['real-analysis', 'ordinary-differential-equations', 'sequences-and-series']"
2548619,A question on finitely generated $k$ algebra,"According to this wikipidea link https://en.m.wikipedia.org/wiki/Cohen–Macaulay_ring :
Let $R$ be a local ring which is finitely generated as a module over some regular local ring $A$ contained in $R$. Such a subring exists for any localisation $R$ at a prime ideal of a finitely generated algebra over a field by the Noether normalisation lemma. I want to know why such a subring exists. By Noether normalisation if $S$ is a finitely generated $k$ algebra then $S$ is integral over say $T=k[X_1,\cdots ,X_n]$. Now $T$ is a regular ring. But how $S_p$ will be finitely generated over some regular local ring, where $p$ is a prime ideal in $S$. Thank you.","['cohen-macaulay', 'flatness', 'algebraic-geometry', 'commutative-algebra']"
