question_id,title,body,tags
682863,"How to evaluate the double integral $\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}$?","This is an exam problem that should be solvable in less than 30 minutes: $$\int _{0}^{1}\!\int _{{x}^{2}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dy}\,{dx}$$ I have tried switching the order of integration and the the boundaries like so: $$\int _{0}^{1}\!\int _{\sqrt {y}}^{1}\!{x}^{3}\sin \left( {y}^{3}\right) {dx}\,{dy}$$ But I always end up with having to evaluate something of the form: $$\int \!\sin \left( {y}^{3}\right) {dy}$$ Which even using software looks like a difficult one to evaluate and gives an absurdly long answer. Any pointers would help, I don't necessarily need all of the steps but if you can it would be very helpful. Many thanks!",['multivariable-calculus']
682878,Square pyramid water volume,"A square pyramid is filled with water to half it's height. Then it is reversed. What is the new height of water? I found that the volume of the water is $7/8$ of the volume of the pyramid, but how do I find the new height in terms of the initial height?","['geometry', 'volume']"
682879,"Limit of a trigonometric integral: $\lim\limits_{x \to \infty} \int_{0}^{\pi} \frac{\sin (t)}{1+\cos^{2}(xt)} \, \mathrm dt$","For all $x \in \mathbb{R}$, let 
$$
{\rm f}\left(x\right)
=\int_{0}^{\pi}\frac{\sin\left(t\right)}{1 + \cos^{2}\left(xt\right)}\,{\rm d}t
$$
Compute the limit when $x\rightarrow +\infty$. My attempt : I tried the substitution $u=\sin(t)$ (and $u=\cos^2(xt)$) but it seems worse:
$$
\int _{0}^{1}\!{\frac {u}{ \left( 1+ \left( \cos \left( x\arcsin
 \left( u \right)  \right)  \right) ^{2} \right) \sqrt {1-{u}^{2}}}}{du}
$$
I tried to use a subsequence $(x_n)$ which is tends to $+\infty$ and use the dominated convergence theorem but it didn't work either. Sorry if my attempt doesn't make much sense. Thank you in advance.","['calculus', 'integration', 'real-analysis', 'limits']"
682890,Linearization of a group action: why the map is equivariant?,"I'm using Dolgachev's book on invariant theory to learn linearizations of group actions. Here is a sketch of main construction: let linear algebraic group $G$ act on a quasi-projective variety $X$, and let $L$ be a very ample line bundle that provide a linearization of the action, then $G$ acts on $W=H^0(X, L)$ by
$$
(g \cdot s)(x)=gs(g^{-1}x)
$$
and the map
$$
X \to \mathbb{P}(W^\vee)
$$
that sends $x \in X$ to the hyperplane $\{s \in W | s(x)=0\} \subset W$ is equivariant, where action of $G$ on $W^\vee$ (we think about $W^\vee$ as a space of hyperplanes) is dual to the action on $W$ that is given by $$
g \cdot H=g^{-1}H,
$$
for a hyperplane $H \in W^\vee$. I don't understand one of the steps in the proof that  this map is equivariant. The following sequences of identities is used
$$
\{s \in W | s(g \cdot x)=0\}=\{s \in W | g^{-1}s(g \cdot x)=0\}=\{s \in W | (g^{-1} \cdot s)(x)=0\}=g^{-1}\{s \in W |s(x)=0 \}=g \cdot\{s \in W | s(g \cdot x)=0\}
$$
to conclude that the map is equivariant. I don't understand why 
$$
\{s \in W | (g^{-1} \cdot s)(x)=0\}=g^{-1}\{s \in W |s(x)=0 \},
$$
I think that it should be 
$$
\{s \in W | (g^{-1} \cdot s)(x)=0\}=g\{s \in W |s(x)=0 \},
$$
but then the map is not equivariant.","['geometric-invariant-theory', 'algebraic-geometry', 'algebraic-groups', 'invariant-theory']"
682898,Meromorphic functions a constant sheaf?,"Is the sheaf of meromorphic functions on a (connected) compact Riemann surface constant? I am refering here to meromorphic functions in the sense of complex analysis and not to those of algebraic geometry, where I know this to hold. I do not imagine it to be the case, but then how does one go about proving the correspondence between divisors and invertible sheaves? The algebro-geometric proof I know (the one in Hartshorne) relies crucially on the sheaf of meromorphic functions being constant ...","['sheaf-theory', 'riemann-surfaces', 'algebraic-geometry', 'complex-geometry']"
682981,BINGO Probability: Controlling average game duration,"I wandered over here from StackOverflow and my understanding of advanced mathematics is limited, so bear with me... A standard, BINGO game card has 24 numbers arranged in a 5x5 format. The center of the card has a free space. The numbers range from 1 to 75. Each column has 1/5 of the numbers (1-15, 16-30, etc). Every round results in a single number being called. To win, a player must have 5 numbers in a row, column, or diagonal for a total of 12 possible ways to win. My understanding is that a single player will on average call bingo in 41.36 rounds. See Wizard of Odds. As the number of players increase, does the average rounds to win decrease? Would proportionally increasing the number range (e.g. 1-85 instead of 1-75) cancel the effect of the increased number of players? If so, how is it related? GOAL: Given a fixed set of players P and a desired number of rounds R, how large should the set of numbers N be? Example: For 100 players, how large should the set of numbers be for the game to last, on average, 50 rounds?","['recreational-mathematics', 'probability']"
683067,"Prove that $f(m,n)=(m+2n, m-n)$ is 1-1 and Onto. The domain and co-domain are $\mathbb R\times \mathbb R$",So I know how to prove injectivity $f(x)=f(y)\Rightarrow x=y$ and surjectivity but am not sure how to go about it in this case since there are multi variables.,['discrete-mathematics']
683084,Field not closed under complex conjugation,"What is an example of an algebraic field not closed under complex conjugation? In all subfields of $\mathbb C$ I think of, complex conjugation is a transposition. I think I understand that it is often preferable to use ""smallest"" possible fields. So if we need a field in which a polynomial of degree n has n roots (is algebraically closed), we take $\mathbb Q$ and adjoin the new roots, so every element of the new, extended field (splitting field) has form $a + b\alpha + c \beta + ...$, where $a,b,...\in \mathbb Q$ and $\alpha, \beta, ...$ are the roots (and conversely, for given roots, we can find a (unique?) minimal polynomial). Now, it is claimed here , that for $\alpha = exp(\frac{2i\pi}{3})\cdot 2^{\frac{1}{3}}$, $\overline \alpha \notin Q(\alpha)$. Why is that? Also, it is stated here that, more generally, for every $\gamma$ being one of the complex solutions to an irreducible (that is ""infactorable"" in $\mathbb Q$) polynomial of degree 3, $\overline \gamma \notin \mathbb Q(\gamma)$. Could you please explain that in simple, non-Galois terms? Also, I was advised to take a look at transcendental numbers to answer the question, but am struggling to find a reasonable connection. It is apparent I don't have but basic knowledge of linear algebra (vector spaces, transformations, affine geometry, matrices, basic polynomial theorems...) and in effort to answer the question, I soon found I need to understand the concepts of field extensions, Galois theory, or even constructible numbers, so I immersed myself in those topics, but feel rather overwhelmed right now, as I don't know which parts of the theory do I need to be able answer the question. So if you don't feel like explaining in detail, please at least point me in the right direction. THANK YOU SO MUCH FOR ANSWERING!","['extension-field', 'polynomials', 'linear-algebra', 'galois-theory', 'minimal-polynomials']"
683089,"Prove that the sequence $\{\int_0^{\pi/2} \sin(t^n)\, dt\}$ converges to 0.","Prove that the sequence 
$$
\left\{\int_0^{\pi/2} \sin(t^n)\, dt :n\in\mathbb N\right\}
$$
converges to 0.","['sequences-and-series', 'real-analysis', 'limits']"
683104,Inducing homomorphisms on localizations of rings/modules,"I'm trying to work out Exercise 2.6 in Commutative Algebra by Eisenbud, which asks to prove the Chinese Remainder Theorem for commutative rings. Exercise: Let $R$ be a commutative ring, and let $I_1,\ldots,I_d$ be pairwise comaximal ideals. Prove that $R/\left(\bigcap_{k=1}^d I_k\right) \simeq \prod_{k=1}^d R/I_k$, via the map $\varphi: R\rightarrow \prod R/I_k$ given by $r \mapsto (r+I_1,\ldots,r+I_d)$. The kernel of $\varphi$ is precisely $\bigcap_{k=1}^d I_k$, so the goal of the argument is to show that $\varphi$ is surjective. Eisenbud hints to use his Corollary 2.9, which states the following. Lemma: Let $\varphi:M\rightarrow N$ be an $R$-module homomorphism. Then $\varphi$ is surjective if and only if the induced map $\varphi_{\mathfrak{m}} : M_{\mathfrak{m}} \rightarrow N_\mathfrak{m}$ is surjective for every maximal ideal $\mathfrak{m}$ of $R$. (Specifically, the map $\varphi_\mathfrak{m}$ is given by $m/u \mapsto \varphi(m)/u$.) I have a few problems with this. The lemma is a statement about module homomorphisms, but the $\varphi$ appearing in the Chinese Remainder
Theorem is not a module homomorphism (it's a ring homomorphism). I tried to change the lemma to work for ring homomorphisms, but ran into the following problem. Even if $\varphi:R\rightarrow S$ is a ring homomorphism, how does
$\varphi_\mathfrak{m} : R_\mathfrak{m} \rightarrow S_\mathfrak{m}$
make sense? I get that you can localize $R$ at a maximal ideals, but
what does ""$S_\mathfrak{m}$"" mean when $\mathfrak{m}$ is a maximal ideal of $R$? (Note this post , which actually gives the failure of the lemma for ring homomorphisms -- except in that context we localize $S$ at a prime ideal of $S$, and then localize $R$ at the preimage of that prime ideal.) Regarding the second remark above, I was thinking that one could view $S$ as an $R$-module by $rs:=\varphi(r)s$, and then consider $S_\mathfrak{m}$ as a localization of an $R$-module by a maximal ideal. But then in order for $\varphi_\mathfrak{m}:R_\mathfrak{m}\rightarrow S_\mathfrak{m}$ to be a ring homomorphism, one has to decide how $S_\mathfrak{m}$ is a ring and then define how to induce a ring homomorphism on localization. This seems like a lot for Eisenbud to sweep under the rug when he says to use the Lemma to solve the exercise. So as far as I understand, the above lemma isn't even relevant to the exercise. Am I missing something here? Is there a clever way to indirectly apply the lemma?","['commutative-algebra', 'ring-theory', 'abstract-algebra']"
683123,"The ""maximum"" of a simple random walk","Suppose $S_n$ is a simple random walk started from $S_0=0$. Denote $M_n$ to be the maximum of the walk in the first $n$ steps, i.e. $M_n=\max_{k\leq n}S_k$. Show that $M_n$ is not a Markov chain, but that $Y_n=M_n-S_n$ is a Markov chain. I wouldn't call this an ""attempt"" at solving, but more of a plan. I know $S_n=X_1+X_2+...+X_n$ with $X_i$ iid with probability $\pm1$. I could take a few specific cases, such as $M_7$: $Pr(M_7=5|M_0=0, M_1=0, M_2=1, M_3=2, M_4=3, M_5=4, M_6=4)\overset{?}{=}Pr(M_7=4|M_6=4).$ I don't see how to show that these are not equal, and as such I don't see how to prove $Y_n$ is a Markov chain, although I suspect the argument will be similar to the one that shows $M_n$ is not Markov. Some direction would be appreciated.","['probability-theory', 'stochastic-processes', 'markov-chains']"
683145,Question about Shell method,I drew the graph and its 3 dimensional shape. I'm confused on what the Radius would be in this problem. What should I look for when finding this shape's radius?,"['calculus', 'integration']"
683190,$\chi(g)$ group character $\Rightarrow$ $\chi(g^m)$ group character,"Let $G$ be a group of order $n$ and and $\gcd(m,n)=1$. Let $\chi:G\rightarrow\mathbb{C}$ be a class function and define $\chi^m\!: g\mapsto\chi(g^m)$. How can one show that $\chi^m$ is a character iff $\chi$ is a character and that $\chi^m$ is irreducible iff $\chi$ is irreducible? It is already a class function, so it is a $\mathbb{C}$-linear combination of irreducible characters $\chi_1,\ldots,\chi_r$. Thus it suffices to show that for $\langle\chi,\chi_i\rangle=\frac{1}{n}\sum_{g\in G}\chi(g)\chi_i(g^{-1})$ we have: $\forall i\!: \langle\chi,\chi_i\rangle\in\mathbb{N}$ iff $\forall
    i\!: \langle\chi^m,\chi_i\rangle\in\mathbb{N}$; $\forall i\!: \langle\chi,\chi_i\rangle\in\{0,1\}$ iff $\forall
    i\!: \langle\chi^m,\chi_i\rangle\in\{0,1\}$. Since $\gcd(m,n)=1$, there exist $k,l$ with $km+ln=1$, so the map $G\to G,g\mapsto g^m$ is surjective ($g=g^{km+ln}=g^{km}=(g^{k})^{m}$) hence bijective (since $G$ is finite) and permutes conjugacy classes. Any suggestions?","['representation-theory', 'group-theory', 'characters']"
683226,"Prove that $A\setminus (B\setminus C) = (A\setminus B) \cup (A\setminus C^c)$ for sets $A,\ B,\ C$ in some Universal Set $U$.","I'm working on this proof for some students I am tutoring and I've gotten a little stuck.   I want to show them how to do a proof in complete, extravagant detail and get them familiar with ''element chasing'' in set proofs for an intro to discrete class. I just got stuck and maybe I'm just too tired to see it.  Here's where I am so far. Notation: $A^c$ is the complement of $A$.  $A\backslash B$ means $A \cap B^c$. Prove that $A\setminus (B\setminus C) = (A\setminus B) \cup (A\setminus C^c)$ for sets $A,\ B,\ C$ in some Universal Set $U$. Proof:
($\Leftarrow$) Let $x \in (A \backslash B) \cup (A\backslash C^c)$. This means that $x\in (A \cap B^c) \cup (A \cap C^{c^c})$. Simplifying, this means that $x \in (A \cap B^c) \cup (A \cap C)$. So $x\in (A \cap B^c)$ or $x\in (A\cap C)$. Now we have two cases. Case 1: $  x \in A \cap B^c.$ So $x \in A$ and $ x\in B^c$. Note: Now we must think outside the box a little bit.  We want to show that $x$ must be in $C$ but we don't have anything to do with $C$ in what we are able to derive from our assumptions.  We only have that $x$ is in $A$ and not in $B$.   This is where we must consider our problem.  We have the Universe $U$.  We know that $A,B,C$ are all in the universe.  So any point we chose in those sets must also be in $U$.   So think about this for a moment.  We don't know if our $x$ is in $C$ or not.  We need it to be in order to have the solution we are after.    So where is $x$ in relation  to $C$?   It's either in it or not in it.   So $x \in C$ or $x\in C^c$.  So we can examine each of these cases and we will find that if $x \in C^c$, we will get a contradiction. Case 1.a.  Suppose $x \in C$. Then $x \in A$ and $x \in B^c$ and $x \in C$. So $x \in A$ and $ x\in  (B^c \cap C)$. So $x \in A \cap (B^c \cap C)$. So $x \in A \cap (B \cap C^c)^c$. So $x \in A \cap (B\backslash C)^c$. So $x \in A \backslash (B \backslash C)$. Case 1.b.  Suppose $x \in C^c$. Then $x \in A$ and $x\in B^c$ and $x\in C^c$. So $x \in A \cap C^c$ and $x \in A\cap B^c$. Now I'm stuck.   I know that I need to develop a contradiction because $x$ cannot be in $C^c$, but I'm just not seeing it.  Any suggestions?   If I can see this one, I'll be able to see the similar method I need to develop for case 2: $x\in A \cap C$ where I need to examine whether $x \in B$ or $B^c$.","['discrete-mathematics', 'elementary-set-theory']"
683246,Variance of a MLE $\sigma^2$ estimator; how to calculate,"Let $X_1, X_2,...,X_n$ be an i.i.d. random sample from $N(0, \sigma^{2})$. a. Find the variance of $\hat{\sigma}^{2}_{MLE}$ So I found $\hat{\sigma}^{2}_{MLE}$ by taking the derivative of the log of the normal pdf function, but from there I am not sure how to proceed. $\hat{\sigma}^{2}_{MLE}$ comes out to $\frac{\sum_{i=1}^n X_i^{2}}{n}$. From there, would I do $\text{var}\left(\frac{\sum_{i=1}^n X_i^{2}}{n}\right)$ ? How do I compute this? Thanks!",['statistics']
683321,Packing spheres on the boundary of a larger sphere,"Consider the following problem, which is a variation of the sphere packing problem and is somehow related to the kissing number problem . For a dimension $n\ge 2$ and a natural $k$, let $r=r(n,k)$ be the maximal radius of $k$ spheres (in $\mathbb{R}^n$) that can be packed into the boundary ring (is that the name of it?) of radius $2r$ of the unit sphere; that is, into the set of all points whose distance from 0 is in the interval $[1-2r, 1]$. It is easier to explain this using examples: $r(2,1)$ is 1, as the maximal radius $r$ of a single sphere that should be embedded in the subset of the unit sphere containing all points whose distance from 0 lies in the interval $[1-2r,1]$ is simply 1 (in which case, the said subset is the unit sphere itself). In fact, $r(n,1)=1$ for any $n\ge 2$. $r(2,2)$ is 0.5, as one can pack 2 0.5-radius spheres inside the unit sphere. I guess $r(n,2)=0.5$ for any $n\ge 2$. $r(2,3)$ is ... well, I already don't know that. What I do know ( know might be too harsh here) is that:
$$\lim_{k->\infty}k\cdot r(2,k)=\pi$$ And that makes me guess $k\cdot r(2,k)$ is an increasing function, going from 1 to $\pi$. A more general limit can be described as follows: the limit of ratio of the sum of all surface areas of all packed spheres, and the surface area of the unit sphere. If we denote that ratio with $R(n,k)$, we obtain (I think)
$$\lim_{k\to\infty}R(2,k)=\pi$$
and
$$\lim_{k\to\infty}R(3,k)=\pi$$
but this is where it stops, as
$$\lim_{k\to\infty}R(4,k)=\frac{\pi^2}{4}$$
and I think that
$$\lim_{k\to\infty}R(9,k)=\frac{\pi^4}{840}$$ So my question actually consists of three sub-questions: Can you imagine how $r(n,k)$ (or $R(n,k)$) looks like? Is it monotone, does it always have a limit for $k\to\infty$, can you estimate that limit with respect to $n$, etc. I there any clear relation between $k\cdot r(n,k)$ and $R(n,k)$? Is $$\lim_{n,k\to\infty}R(n,k) = 0,$$ and if so, do you have any intuition about it?","['geometry', 'packing-problem']"
683345,Non-abelian finite groups with exactly $n$ normal subgroups.,"Let $\mathfrak{N}$ be the class of all non-abelian finite groups and define $\nu: \mathfrak{N} \rightarrow \mathbb{N}_{\gt 1}$ by $\nu(G)=|\{{1} \leq N \leq G: N$ normal in $G\}|$. Is the map $\nu$ surjective? In other words, for any positive integer $n \gt 1$, does there always exist a non-abelian group with exactly $n$ normal subgroups? Example: if $n$ is a (non-trivial) power of $2$, then it is guaranteed: take a direct product of $n$ copies of a non-abelian simple group.","['finite-groups', 'group-theory']"
683352,A proof of a theorem of Liouville,"I need some reference for the proof of the following theorem attributed to Liouville: Theorem. Let $f(x):\Omega\longrightarrow \mathbb R^n$ be a $C^2$ function where $\Omega$ is an open subset of $\mathbb R^n$ and assume that $$
\textrm{div}\, f=\sum_{i=1}^n\frac{\partial f_i}{\partial x_i}=0.
$$ If $\varphi$ is the flow  of the differential equation $y'=f(y)$ and we consider the homeomorphism $\pi_t:\Omega\longrightarrow\Omega$, such that 
$$
x\,\longmapsto\, \pi_t(x):=\varphi(x,t),
$$ 
then the map $\pi_t$ preserves the Lebesgue measure of every measurable subset of $\Omega$.
To be more precise, for every $\mu$-measurable subset $D\subseteq \Omega$ we have that $\mu(D)=\mu\big(\pi_t(D)\big)$. The above theorem is very famous and its simpler form applied to Hamiltonian systems is often cited in texts of mechanics. However I need a proof of the general statement. Thanks in advance","['dynamical-systems', 'multivariable-calculus', 'ordinary-differential-equations', 'reference-request', 'real-analysis']"
683356,Extended matrix function,"I have a continuous matrix-valued function $f:\mathbb{R}^d\mapsto {\cal M}_{k\times d}$, with $d<k$, such that $f(x)$ is full rank for all $x\in\mathbb{R}^k$. Can I extend this function to be a continuous function $g:\mathbb{R}^k\mapsto {\cal M}_{k\times k}$ such that $g(x)$ is full rank matrix, the first $d$ columns of $g(x)$ equals to $f(x)$, and the rest columns are orthogonal with all columns of $f(x)$. Note that since $f(x)$ is full rank then we can add $k-d$ columns such that the obtained matrix is full rank (invertible). Then, using the Gram-Schmidt procedure which is a continuous process we can get $g(x)$ if the $k-d$ vectors that we choose is also conttinuous. But, how can I choose this $k-d$ vectors. Thanks for any help","['matrices', 'continuity', 'multivariable-calculus']"
683359,Distribution of higher powers than 2 of a gaussian distribution,"If $X \sim \mathcal{N}(0,1)$, then $X^2 \sim \chi^2(1)$. What about higher powers of $X$? I know that the Gamma Distribution is a generalization of the $\chi^2$ distribution, but I don't know how the Gamma Distribution parameters relate to the square part of $\chi^2$. In particular I'm trying to calculate $X^4$, where $X \sim \mathcal{N} \left(0,\frac{1}{N} \right)$. How do you even take on such a problem? Thanks for any tips!","['normal-distribution', 'probability', 'real-analysis']"
683360,Geometric interpretation of the Riemann-Roch for curves,"Let $X$ be a smooth projective curve of genus $g\geq2$ over an algebraically closed field $k$ and denote by $K$ a canonical divisor. I have some clues about the geometrical interpretation of the Riemann-Roch Theorem for smooth algebraic curves, but also some doubts which I would like to clarify. Recall that the RR formula is
$$ h^0(X,\,D)-h^0(X,\,K-D) = d-g+1\,. $$ Assume that $X$ is not hyperelliptic, so that the canonical map is actually a canonical embedding
$$ \phi_K : X \to \mathbb{P}^{g-1} \qquad P\mapsto\{ \; s\in H^0(X,\,K) \mid s(P)=0 \; \} $$
giving a preferred realization of the curve inside a $(g-1)$-dimensional projective space. The key feature of such an embedding is that there is a bijective correspondence between hyperplanes $W\subset \mathbb{P}^{g-1}$ and effective divisors in the linear system $ |K| \cong \mathbb{P}H^0(X,\,K) $. The picture shows the canonical embedding in $\mathbb{P}^2$ of a non hyperelliptic curve of genus $3$. Let $D=\sum_{i=1}^d P_i$ be an effective divisor consisting of $d<g$ distinct points of $X$. We define
$$ \phi_K(D) := \operatorname{span}\{\phi_K(P_1), \dots, \phi_K(P_d)\}. $$ The vector space $H^0(X,\,K-D)$ can be interpreted as the space of canonical divisors containing $D$, and here comes my first question: (1) Is it correct to identify $\mathbb{P}H^0(X,\,K-D)$ with the set of hyperplanes of $\mathbb{P}^{g-1}$ passing through $\phi_K(D)$ ? If so, how can one see it formally? Let $r(D) := \dim |D|$ denote the dimension of the complete linear series associated to $D$. Further, denote by $D'=K-D$ the residual divisor of degree $d'=2g-2-d$. If (1) is correct, then it follows that $r(D)$ equals the number of hyperplanes of $\mathbb{P}^{g-1}$ passing through $\phi_K(D')$. Now, notice that the RR can be rewritten as $$ r(D)=[g-1]-[d' - r(D')] $$ so that we deduce that $r(D')$ counts the number of independent linear relations on the points of $D'$ and we can give the following geometrical interpretation of the Riemann-Roch: The integer $r(D)$ is the number of hyperplanes passing though $\phi_K(D')$, hence it equals the difference between the dimension $g-1$ of the ambient space and the dimension of the space spanned by the points of $\phi_K(D')$. Of course my second question is: (2) Do you agree with this geometrical interpretation?","['algebraic-geometry', 'algebraic-curves']"
683362,How to solve this limit of a function? ($\cos^3x$),"So I'm having trouble with the following limit: $$\lim_{x\to0}{\frac{1-\cos^3x}{x\,\sin x}}$$ Sorry to bother again, but I was never good at solving limits.
Really, I don't know what to do with that $\cos^3 x$.","['trigonometry', 'functions', 'limits']"
683372,Taylor series of the inverse of $x^4+x$,"I would like to expand the inverse function of $$g(x) := x^4+x $$ in a taylor series at the point x = 0. I calculated the first and second derivate at x = 0 with the rule of 
the derivation of an inverse function. Theoretically, this process 
could be continued for higher derivates. But I would like to have an easier method to calculate higher derivates
of an inverse function in order to calculate the taylor series. Any ideas ?","['taylor-expansion', 'inverse', 'analysis']"
683390,Prove a generator of $\mathrm{SL}_2(\mathbb{R})$,"Definitions Let $\mathrm{SL}_2(\mathbb{R}) := \left \{M := \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathbb{R}^{2 \times 2}: \det{M} = 1 \right \}$ with matrix multiplication be a group of $2 \times 2$ matrices. Let \begin{align}
  A_\lambda :&= \begin{pmatrix} \lambda & 0\\ 0 & \lambda^{-1}\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with }\lambda \in \mathbb{R} \setminus \{0\}\\
  B_t :&= \begin{pmatrix} 1 & t\\ 0 & 1\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with } t \in \mathbb{R}\\
  C :&= \begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})
\end{align} be elements of $\mathrm{SL}_2(\mathbb{R})$. Question Do $A_\lambda, B_t$ and $C$ generate $\mathrm{SL}_2(\mathbb{R})$? My try I think they do. When I could show that $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$ can be multiplied with a combination of the three matrices $A, B, C$ in such a way that the result is the neutral element of this group
$$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$
then I could generate the matrix by multiplying the inverse matrices. 
The inverse matrices can be generated because: \begin{align}
  A_\lambda^{-1} &= A_{\frac{1}{\lambda}}\\
  B_t^{-1}       &= B_{-t}\\
  C^{-1}         &= C^3
\end{align} Now the only thing left is to get from 
$$M = \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$
to
$$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$ Ok. So let's try it. Case 1: $a = 0$ As $\det(M)=1 = ad - bc = 0d-bc$ we know that $bc=-1$. Especially is $c \neq 0$. $$\begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \cdot \begin{pmatrix} a & b\\ c & d\end{pmatrix} = \begin{pmatrix} c & d\\ -a & -b\end{pmatrix}$$ Continue with Case 2. Case 2: $a \neq 0$ Normalize ($M \cdot A_{\frac{1}{a}}$): $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} \frac{1}{a} & 0\\ 0 & a\end{pmatrix} = \begin{pmatrix} 1 & ab\\ \frac{c}{a} & ad\end{pmatrix}$$ Continue with Case 3. Case 3: $a=1$ Get a $0$ with $M \cdot B_{-b}$: $$\begin{pmatrix} 1 & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} 1 & -b\\ 0 & 1\end{pmatrix} = \begin{pmatrix} 1 & 0\\ c & d-bc\end{pmatrix}$$ We know that $\det{M} = 1 = ad - bc = d - bc$. Continue with Case 4. Case 4: $a=1$, $b=0, d=1$ At this stage we have matrices that look like $$\begin{pmatrix} 1 & 0\\ c & 1\end{pmatrix}$$ This is where I'm stuck. Can you help me?","['matrices', 'linear-algebra']"
683396,"A Brownian motion $B$ that is discontinuous at an independent, uniformly distributed random variable $U(0,1)$","Suppose that $\left\{B\left(t\right): t \geq 0\right\}$ is a Brownian motion and $U$ is an independent random variable, which is uniformly distributed on $\left[0,1\right]$. Then the process $\left\{\tilde{B}\left(t\right): t \geq 0\right\}$ defined by
  $$
\tilde{B}\left(t\right) :=
\begin{cases}
B\left(t\right), & \textrm{if } t \neq U, \\0, & \textrm{if } t = U
\end{cases}
$$
  has the same finite-dimensional distributions as a Brownian motion, but is discontinuous if $B\left(U\right) \neq 0$, i.e. with probability one, and hence this process is not a Brownian motion. This is Example 1.2, p. 8, from Mörters & Peres's ""Brownian Motion"" (the version that's available on Peres's website). I don't understand why the event $\left\{B\left(U\right) \neq 0\right\}$ has probability one. I'll appreciate any help.","['probability-theory', 'stochastic-processes', 'uniform-distribution', 'brownian-motion']"
683411,Lebesgue-Measure of special subsets,"I got two closed subsets $A,B\in\mathbb{R}^{m\times n} (\mathbb{R}^{n\cdot m}\text{ as vectorspace})$ , which satisfy to following properties: $A\cup B=\mathbb{R}^{m\times n}$ $\partial A=\partial B$ $A^c = B\backslash{\partial B},B^c = A\backslash{\partial B}$ where $\partial A$ means the boundary and $A^c$ the complement of a set $A$. So both set are the complements of each other (without the boundary) and their union is the full space. Can I show that the boundary $\partial A=\partial B$ of the two sets 
 have Lebesgue measure zero? Or does it needs more info about the sets?","['measure-theory', 'lebesgue-measure']"
683420,Stability of a feedback system,"Take the following feedback system: $\dot{x} = (\theta - k_1) x - k_2  x^3$ Now my book says: For $\theta > k_1$, the equilibrium $x = 0$ is unstable. I wonder why... Furthermore my book indicates that it is easy to see that $x(t)$ will converge to one of the two new equilibria $\pm \sqrt{\frac{\theta-k_1}{k_2}}$. Again, how did they obtain this result?","['dynamical-systems', 'nonlinear-system', 'ordinary-differential-equations']"
683454,Evaluating $\int_{0}^{1}\frac{\arcsin{\sqrt{x}}}{x^4-2x^3+2x^2-x+1}\operatorname d\!x$,"Find this integral
$$\operatorname I=\int\limits_{0}^{1}\dfrac{\arcsin{\sqrt{x}}}{x^4-2x^3+2x^2-x+1}\operatorname d\!x$$ My try: let
$$f(x)=x^4-2x^3+2x^2-x+1$$
I found 
$$f(1-x)=(1-x)^4-2(1-x)^3+2(1-x)^2-x+1=x^4-2x^3+2x^2-x+1=f(x)$$
so
$$I=\int_{0}^{1}\dfrac{\arcsin{\sqrt{(1-x)}}}{x^4-2x^3+2x^2-x+1}dx$$
so
$$2I=\int_{0}^{1}\dfrac{\arcsin{\sqrt{x}}+\arcsin{\sqrt{(1-x)}}}{x^4-2x^3+2x^2-x+1}dx$$
then I can't,Thank you very much","['definite-integrals', 'calculus', 'integration', 'analysis']"
683507,Convergence of series of functions: $f_n(x)=u_n\sin(nx)$,"Let $f_n(x)=u_n\sin(nx)$  where $\displaystyle\sum f_n$  converges pointwise, and $ \displaystyle x \mapsto \sum_{n=0}^{+\infty} f_n(x)$ is continuous. Prove that $ u_n\rightarrow 0$ when n tends to $+\infty$ My 'attempt': We know that  $S_n=\displaystyle\sum_{k=1}^{n}u_k\sin(kx)$ converges to a real $a$, and 
$$
S_{n+1}-S_n=u_{n+1}sin((n+1)x) 
$$
converges to $0$.
By induction we can prove that for all $x=(\frac{\pi}2)^k$ then $u_n$ tends to $0$. I really don't know how can I continue. Thank you in advance,","['fourier-series', 'sequences-and-series', 'real-analysis', 'limits']"
683513,"Evaluating the reception of (epsilon, delta) definitions","There is much discussion both in the education community and the mathematics community concerning the challenge of (epsilon, delta) type definitions in real analysis and the student reception of it. My impression has been that the mathematical community often holds an upbeat opinion on the success of student reception of this, whereas the education community often stresses difficulties and their ""baffling"" and ""inhibitive"" effect (see below). A typical educational perspective on this was recently expressed by Paul Dawkins in the following terms: 2.3. Student difficulties with real analysis definitions. The concepts of limit and continuity have posed well-documented difficulties for students both at the calculus and analysis level of instructions (e.g. Cornu, 1991; Cottrill et al., 1996; Ferrini-Mundy & Graham, 1994; Tall & Vinner, 1981; Williams, 1991). Researchers identified difficulties stemming from a number of issues: the language of limits (Cornu, 1991; Williams, 1991), multiple quantification in the formal definition (Dubinsky, Elderman, & Gong, 1988; Dubinsky & Yiparaki, 2000; Swinyard & Lockwood, 2007), implicit dependencies among quantities in the definition (Roh & Lee, 2011a, 2011b), and persistent notions pertaining to the existence of infinitesimal quantities (Ely, 2010). Limits and continuity are often couched as formalizations of approaching and connectedness respectively. However, the standard, formal definitions display much more subtlety and complexity. That complexity often baffles students who cannot perceive the necessity for so many moving parts. Thus learning the concepts and formal definitions in real analysis are fraught both with need to acquire proficiency with conceptual tools such as quantification and to help students perceive conceptual necessity for these tools. This means students often cannot coordinate their concept image with the concept definition, inhibiting their acculturation to advanced mathematical practice, which emphasizes concept definitions. See http://dx.doi.org/10.1016/j.jmathb.2013.10.002 for the entire article (note that the online article provides links to the papers cited above). To summarize, in the field of education, researchers decidedly have not come to the conclusion that epsilon, delta definitions are either ""simple"", ""clear"", or ""common sense"". Meanwhile, mathematicians often express contrary sentiments. Two examples are given below. ...one cannot teach the concept of limit without using the epsilon-delta definition. Teaching such ideas intuitively does not make it easier for the student it makes it harder to understand. Bertrand Russell has called the rigorous definition of limit and convergence the greatest achievement of the human intellect in 2000 years! The Greeks were puzzled by paradoxes involving motion; now they all become clear, because we have complete understanding of limits and convergence. Without the proper definition, things are difficult. With the definition, they are simple and clear. (see Kleinfeld, Margaret; Calculus: Reformed or Deformed? Amer. Math. Monthly 103 (1996), no. 3, 230-232.) I always tell my calculus students that mathematics is not esoteric: It is common sense. (Even the notorious epsilon, delta definition of limit is common sense, and moreover is central to the important practical problems of approximation and estimation.) (see Bishop, Errett; Book Review: Elementary calculus. Bull. Amer. Math. Soc. 83 (1977), no. 2, 205--208.) When one compares the upbeat assessment common in the mathematics community and the somber assessments common in the education community, sometimes one wonders whether they are talking about the same thing. How does one bridge the gap between the two assessments? Are they perhaps dealing with distinct student populations? Are there perhaps education studies providing more upbeat assessments than Dawkins' article would suggest? Note 1. See also https://mathoverflow.net/questions/158145/assessing-effectiveness-of-epsilon-delta-definitions Note 2. Two approaches have been proposed to account for this difference of perception between the education community and the math community: (a) sample bias: mathematicians tend to base their appraisal of the effectiveness of these definitions in terms of the most active students in their classes, which are often the best students; (b) student/professor gap: mathematicians base their appraisal on their own scientific appreciation of these definitions as the ""right"" ones, arrived at after a considerable investment of time and removed from the original experience of actually learning those definitions.  Both of these sound plausible, but it would be instructive to have field research in support of these approaches. We recently published an article reporting the result of student polling concerning the comparative educational merits of epsilon-delta definitions and infinitesimal definitions of key concepts like continuity and convergence, with students favoring the infinitesimal definitions by large margins.","['calculus', 'education', 'math-history', 'real-analysis', 'limits']"
683516,$(A _1 \times B _1 ) \cup (A _2 \times B _2 )= \textbf{?} $,"$(A _1 \times B _1 ) \cup (A _2 \times B _2 )= \textbf{?} $
I tried as follows: 
$$
(x,y) \in [(A _1 \times B _1 ) \cup (A _2 \times B _2 )] \iff (x,y) \in (A _1 \times B _1 ) \vee (x,y) \in (A _2 \times B _2 ) \iff (x \in A _1 \wedge y \in B _1 ) \vee ( x \in A _2 \wedge y \in B _2) \iff ((x \in A _1 \wedge y \in B _1)\vee x \in A _2)\wedge ((x \in A _1 \wedge y \in B _1) \vee y \in B _2 )\iff (( x \in H _1 \vee x \in A _2)\wedge (x \in A _2 \vee y \in B _1)) ...
$$
And I don't know where this is going. Help would be much appreciated! Thanks in advance!",['elementary-set-theory']
683521,Independence of disjoint events with strictly positive probability,"I'm taking a class in Probability Theory, and I was asked this question in class today: Given disjoint events $A$ and $B$ for which $$ P(A)>0\\ P(B)>0 $$ Can $A$
  and $B$ be independent? My answer was: $A$ and $B$ are disjoint, so $P(A\cap B)=0$. $P(A)>0$ and $P(B)>0$, so $P(A)P(B)>0$. $P(A\cap B)\not =P(A)P(B)$, so $A$ and $B$ are not independent. However, I was told that I am wrong and we cannot know whether or not $A$ and $B$ are independent from the given information, but I did not receive a satisfactory explanation. Is my argument valid? If not, where do I go wrong?","['independence', 'probability']"
683529,"50/50 Joker of ""Who wants to be a Millionaire"" - A ""Monty Hall Problem"" variation?","So the Monty Hall Problem itself is widely known and understood.
Nonetheless, a friend of mine and I were wondering whether the the same strategy could affectively be applied by a participant of Who wants to be a Millionaire? when using the 50/50 Joker . Let's imagine the following scenario: 
The participant P has no clue about the correct answer $ x \in \{A,B,C,D\} $ and wants to use the 50/50 Joker (eliminating two wrong answers). But instead of immediately going for it he first ""preselects"" one of the answers in his mind. There is no need to tell Quizmaster Q about his ""imaginary preselection"". Now P tells Q that he wants to use his joker and Q lets the computer eliminate two wrong answers. (1) In case  the answer P had preselected is eliminated he has no choice but to choose between the remaining two answers, effectively leaving him with a 50% chance of success - no magic happening here. (2) But what about the other case  when the answer P had preselected survives the elimination? According to the Monty Hall Problem it seems as if changing the selection (i.e. choosing the other remaining option P had not preseleted) seems to give him a 0.75 chance of success. Nevertheless, I find it hard to believe that this actually holds true, since the so called 50/50 (!) Joker would then not be p(success) = 0.5 after all. Additionally it seems unlikely that making an ""imaginary preselection"", no one else is told about, actually increases your odds. I know this problem is not exactly the same as Monty Hall since the the quizmaster does not always eliminate answers only from the ones the participant had not ""preselected"", meaning that the preselection itself could be eliminated, too, as it happens in (1) . Still the second case seems to actually be a just variation of it. So are we right and making a preselection and then going for the other remaining option is a valid strategy that increases the participant's odds of winning? If not, please help us understand our misconception.","['monty-hall', 'probability']"
683531,Find the value of $\lim_{n\to \infty}\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)^{1/2}\ldots(2)^{1/n}$,"Find the value of $$\lim_{n\to \infty}\bigg(1+\dfrac{1}{n}\bigg)\bigg(1+\dfrac{2}{n}\bigg)^{\frac12}\ldots(2)^{\frac{1}{n}}$$ My work: $\bigg(1+\dfrac{1}{n}\bigg)=\bigg\{\bigg(1+\dfrac{1}{n}\bigg)^n\bigg\}^{\frac{1}{n}}=e^{\frac{1}{n}}$ $\bigg(1+\dfrac{2}{n}\bigg)^{\frac12}=\bigg\{\bigg(1+\dfrac{2}{n}\bigg)^{\frac{n}{2}}\bigg\}^{\frac{1}{n}}=e^{2\cdot\frac12\cdot\frac{1}{n}}=e^\frac{1}{n}$ $~~~~~~~~~~~~\vdots$ $~~~~~~~~~~~~\vdots$ $\bigg(1+\dfrac{n}{n}\bigg)^{\frac{1}{n}}=e^{\frac{1}{n}}$ So, $L=e$ But, the answer says $L=e^{\frac{\pi^2}{12}}$. I do not know where I am going wrong, is the answer a typo or I am doing wrong. Please help.","['calculus', 'real-analysis', 'limits']"
683547,Difference between a uniform stable and a uniform attractive solution,"Consider the following time-varying system: $\dot{x} = f(x,t)$ The solution of this system which starts from the point $x_0$ at time $t_0 \geq 0$ is denoted as $x(t;x_0,t_0)$ with $x(t_0;x_0,t_0)=x_0$. The solution $x(t;x_0,t_0)$ of above system is: Uniform stable, if for each $\epsilon >0$ there exists a $\delta(\epsilon)>0$ such that:
$|\tilde{x}_0-x_0|<\delta  \Rightarrow  |x(t;\tilde{x}_0,t_0)-x(t;x_0,t_0)|<\epsilon, \forall t \geq t_0$ Uniform attractive, if there exist an $r>0$ and, for each $\epsilon >0$, a $T(\epsilon)>0$ such that:
$|\tilde{x}_0-x_0|<r  \Rightarrow  |x(t;\tilde{x}_0,t_0)-x(t;x_0,t_0)|<\epsilon, \forall t \geq t_0+T$ I find it very difficult to understand the differences between these definitions.","['dynamical-systems', 'nonlinear-system', 'ordinary-differential-equations']"
683571,Automorphism of $W^*$ algebra,"Let $\mathfrak{A}$ be von Neumann algebra. It is in particular $C^*$ algebra.
Is it true that every $*$-isomorphism of $\mathfrak{A}$ is also $W^*-$isomorphism?
(Note that every $*$-isomorphism of $C^*$ algebra is $C^*-$isomorphism). I'm especially interested in application to concrete von Neumann algebra (weakly closed subalgebra of $B(\mathcal{H})$ - the algebra of bounded operators on Hilbert space $\mathcal{H}$). Is it true that any isomorphism of this algebra is automatically continuous in weak, strong and ultraweak operator topologies on $\mathcal{H}$.","['c-star-algebras', 'operator-theory', 'operator-algebras', 'von-neumann-algebras', 'functional-analysis']"
683576,"Show that,$\int_0^\pi \left|\frac{\sin nx}{x}\right|\mathrm{d}x \ge \frac{2}{\pi}\left(1+\frac12+\cdots+\frac{1}{n}\right)$","Show that,$$\int_0^\pi \bigg|\dfrac{\sin nx}{x}\bigg|\mathrm{d}x \ge \dfrac{2}{\pi}\bigg(1+\dfrac12+\cdots+\dfrac{1}{n}\bigg)$$ I could not approach the problem at all. Please help.","['definite-integrals', 'calculus', 'integration', 'real-analysis']"
683585,2014 AMC 12 B problem 25,What is the sum of all positive real solutions $x$ to the following equation? $$2\cos(2x)\left( \cos(2x) - \cos{\left(\frac{2014\pi^2}{x^2}\right)} \right) = \cos(4x) - 1 $$,"['trigonometry', 'contest-math']"
683586,Identity $\int_{-\infty}^{\infty}\frac{e^{uz}}{1+e^u} \mathrm{d}u=\frac{\pi}{\sin(\pi z)}$,"I want to prove the identity 
$$F(z)=\int_{-\infty}^{\infty}\frac{e^{uz}}{1+e^u} \mathrm{d}u=\frac{\pi}{\sin(\pi z)}$$ First of all $F(z)$ defines an analytic function for $0<z<1$. I am little bit confused here because I think the notation is not very good. $\frac{e^{uz}}{1+e^u}$ has the period $2\pi i$ and is bounded in $[0,1]\times S$ where $S=\{u\in\mathbb C: |Im z|\le\pi, |u|\ge r\}$ Ths is a punched range if you draw it. You have any idea how to compute the integral now?","['integration', 'complex-analysis']"
683590,"A game with $\delta$, $\epsilon$ and uniform continuity.","UPDATE : Bounty awarded, but it is still shady about what f) is. In Makarov's Selected Problems in Real Analysis there's this challenging problem: Describe the set of functions $f: \mathbb R \rightarrow \mathbb R$ having the following properties ( $\epsilon, \delta,x_1,x_2 \in \mathbb R$ ) : a) $\forall \epsilon \qquad\qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ b) $\forall \epsilon >0 \qquad, \exists \delta \qquad \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ c) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, (x_1-x_2) < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ d) $\forall \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ e) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|>\epsilon$ f) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \epsilon \Rightarrow |f(x_1)-f(x_2)|<\delta$ g) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |f(x_1)-f(x_2)| > \epsilon \Rightarrow |x_1-x_2|> \delta$ h) $\exists \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ i) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, x_1-x_2 < \delta \Rightarrow f(x_1)-f(x_2)<\epsilon$ Here's what everybody got so far: a) $\{ \}$ b) every functions c) constant functions d) constant functions e) $\{ \}$ f) functions that are bounded on any closed interval (not sure) g) uniform continous functions h) bounded functions i) Non-decreasing and uniformly continuous. Thanks for your suggestions.","['uniform-continuity', 'continuity', 'real-analysis', 'solution-verification']"
683601,Fixed point question with convergence,"Let $f:\mathbb{R}^n \to \mathbb{R}^n$ is $C^1$ and $1$ to $1$ and there exists a strict increasing sequence $t_{n} \in \mathbb{N}$ s.t $f^{t_{n}}(x) \to p$ for all $x$ as $n\to \infty$ (composition $t_{n}$ times) where $p$ is a fixed point, i.e $f(p)=p$. Can you prove or find a counterexample that $f^n(x) \to p$ as $n \to \infty$ for all $x$.","['cauchy-sequences', 'sequences-and-series', 'real-analysis', 'analysis']"
683603,To maximise my chance of winning one prize should I put all my entries in a single draw?,"Every week there's a prize draw. It's free to enter using the code from a soup tin lid. You can enter as many times as you like during the week until Monday's draw and then it starts all over. The prizes are experiences and the value to me is not especially relevant. Winning more than once might be worth more in value terms, but I only want to win once. So my question is; in order to maximise my chance of winning once should I; batch up my lids and enter as many of them as I can in one go, probably during the last week of the prize draws? enter lids as I go along. It feels like (1) to me, but the maths to explain it is beyond me. To be clear, I'm not actually interested in trying to gain a winning advantage. I realise that in reality the odds are small and about even either way, but as a maths puzzle I'm interested. Say for instance that I have 10 lids. Lets assume that the number of other entries are an even number (say 5 each week), and lets say there are 10 weeks and one prize available per week. Do I have more chance of winning a prize by entering one lid over 10 weeks, or 10 lids in any other week.","['puzzle', 'probability']"
683608,There are 100 people in a queue waiting to enter a hall. The hall has exactly 100 seats numbered from 1 to 100. The first person in the queue...,"There are $100$ people in a queue waiting to enter a hall. The hall has exactly $100$ seats numbered from $1$ to $100$. The first person in the queue enters the hall, chooses any seat and sits there. The $n$-th person in the queue where $n$ can be $2,\ldots,100$, enters the hall after $(n-1)$-th person is seated. He sists in seat number $n$ if he finds it vacant; otherwise he takes any unoccupied seat. Find the total number of ways in which $100$ seats can be filled up, provided the $100$-th person occupies seat number $100$. I could not realise how this chaotic behaviour will end. I think the solution lies in finding that. Please help.",['combinatorics']
683622,Open covers of a topological space $X$,"An open cover $\mathcal U$ of a topological space $X$, is called An $\omega$-cover, if every finite subset of $X$, is contained in a member of $\mathcal U$. A $\gamma$-cover if it is infinite and every $x \in X$ belongs to all but finitely many elements of $\mathcal U$. Can anyoone think of a countable $\gamma$-cover which is not an $\omega$-cover for some Hausdorff topological space $X$? Thank you!","['general-topology', 'examples-counterexamples']"
683638,What commutes with a matrix in Jordan canonical form?,"The question I would like answered is the following: Given a matrix $G$ and that $G$ commutes with another matrix $X$, that is $[G, X] = 0$, what is $X$? Or more generally, what properties of $X$ may we infer? I understand however that this question is really too vague, so here's a more specific question: If $G$ is in Jordan canonical form, does $[G, X] = 0$ imply that $X$ has the same Jordan canonical form? Or still more specific, if $G$ is diagonal with no two diagonal entries the same, does $[G, X] = 0$ imply that $X$ is diagonal? I have convinced myself that the answer to the latter question is ‘yes’, but a simple proof eludes me.","['matrix-equations', 'matrices', 'linear-algebra', 'jordan-normal-form']"
683660,Pigeon hole principle with sum of 5 integers,"Prove that from 17 different integers you can always choose 5 so the sum will be divisible by 5. I tried with positive,negative numbers. Even, odd numbers etc but can't find the solution. Any thoughts on what should the holes be ?","['pigeonhole-principle', 'discrete-mathematics']"
683662,Prove $n! \geq n^2$ for $n \geq 4$,"I am working through a discrete math course, and have come upon a question that I don't understand how the solution was obtained. The question is, prove $n! \geq n^2$ Hypothesis: $p(n): n! \geq n^2, n\geq 4$ Basic step: $p(4): (4)! \geq (4)^2$. $24 \geq 16$, so $p(4)$ is true. Prove $p(n+1): (n+1)! \geq (n+1)^2$ (left side) $n!(n+1)$   (because $n!(n+1)$ is the same as $(n+1)!$) $n^2(n+1)$  (by inductive hypothesis) $n^3+n^2 $ ...  and here is where I get stuck.  The solutions key continues as follows:
$n^3+n^2 \geq n^3 + 3n^2$ (I don't understand where the $3n^2$ comes from)
$= n^3 + 2n^2 + n \geq n^2 + 2n + 1 = (n + 1)^2$ I understand how $n^2 + 2n + 1 = (n + 1)^2$,  I even get how $n^3 + 2n^2 + n \geq n^2 + 2n + 1$ by factoring out  $n$...  but I don't understand where $n$ goes.  Shouldn't it read $n^3 + 2n^2 + n \geq n(n^2 + 2n + 1)$ ? Thanks for any help offered,","['induction', 'discrete-mathematics']"
683697,Checking irreducibility,"I have the polynomial $f(X)=X^{2n}-2X^{n}+1-p$ where $p$ is a prime number and $n\in\mathbb{N}$. I want to check whether it is irreducible or not over $\mathbb{Q}[X]$. If $2^{2}\nmid1-p$ then $f(X)$ is irreducible by Eisenstein's Criterion. However, I can't make any progress when I consider the polynomial $f(X)=X^{2n}-2X^{n}+4r, r\in\mathbb{Z}$. Any hints?","['galois-theory', 'irreducible-polynomials', 'abstract-algebra', 'polynomials']"
683711,Symmetry of bicycle-lock numbers,"Suppose you have a combination bicycle lock of this sort: with $n$ dials and $k$ numbers on each dial. Let $m(n,k)$ denote the minimum number of turns that always suffice to open the lock from any starting position, where a turn consists of rotating any number of adjacent rings by one place. For example $m(2,10)=6$ and $m(3,10)=10$. I have found an efficient algorithm to compute these numbers , which reveals a symmetry I can’t currently explain: $m(n, k+1) = m(k, n+1)$ This is such a striking symmetry that I guess it has a simple explanation. Can anyone find one? Here’s the table of values for small $n$ and $k$, exhibiting the conjectured symmetry: n\k|   2    3    4    5    6    7    8    9   10
---+---------------------------------------------
1  |   1    1    2    2    3    3    4    4    5
2  |   1    2    2    3    4    4    5    6    6
3  |   2    2    4    4    6    6    8    8   10
4  |   2    3    4    6    6    8    9   10   12
5  |   3    4    6    6    9    9   12   12   15
6  |   3    4    6    8    9   12   12   15   16
7  |   4    5    8    9   12   12   16   16   20
8  |   4    6    8   10   12   15   16   20   20
9  |   5    6   10   12   15   16   20   20   25
10 |   5    7   10   12   15   18   20   24   25","['conjectures', 'combinatorics']"
683733,Sum of $k {n \choose k}$ is $n2^{n-1}$,"Proof that $\suṃ̣_{k=1}^{n}k {n \choose k}$ for $n \in \mathbb N$ is equal to $n2^{n-1}$. As a hint I got that $k {n \choose k} = n {n-1\choose k-1} $. I tried solving this by induction but, in the inductive step I'm not arriving to the correct result.","['binomial-coefficients', 'summation', 'proof-writing', 'combinatorics']"
683801,Find 2 functions so that fg = 0,"The full question : Find 2 functions $f,g: \mathbb R \to \mathbb R$ $f,g \neq 0 $ so that $fg = 0$ The confusing part is the way it is written, as far as I can tell there is no product of anything other than $0$ which yields $0$, is there?? Perhaps they meant $f(g(x))$ in this question? Thanks in advance",['functions']
683838,Is there any good text introducing a part of Borel-hierarchy which is in need in measure theory,"Is there any good text introducing a subpart of Borel-hierarchy which is in need in measure theory, which can be done in short time? Say, 1~3 days if possible. (Assuming I'm studying about 14hours a day) To be more precise, I want to learn that ""The Sigma-algebra generated by a set $S$"" is exactly $\bigcup_{\alpha<\omega_1} \prod_{\alpha}^0$","['book-recommendation', 'measure-theory', 'elementary-set-theory', 'borel-sets', 'reference-request']"
683839,Simultaneous Orthogonal basis for $L^2(\mathbb{R}^n)$ and $H^1(\mathbb{R}^n)$,"Given a smooth bounded set $U\subset \mathbb{R}^n$, there is a simultaneous orthogonal basis for  $L^2(U)$ and $H^1_0(U)$ by the existence of eigenvectors to the Laplacian in a bounded domain, which particularly requires boundedness for compactness of the solution operator of the corresponding elliptic problem. Is it possible to construct a simultaneous orthogonal basis for $L^2(\mathbb{R}^n)$ and $H^1(\mathbb{R}^n)$ as well? I thought it might be possible to use a basis for $L^2(U)$ where U is a cube and then by translations and dilations construct an orthogonal basis for  $L^2(\mathbb{R}^n)$. I do not know if that will also be orthogonal basis for  $H^1(\mathbb{R}^n)$ or if there will be some edge effects creating trouble. I wanted to know because I was reading the existence of solutions to wave equations as given in Evans's book on Partial Differential Equations using the Galerkin Method and at one point it requires this simultaneous basis for $L^2(U)$ and $H^1_0(U)$, which is available only for bounded smooth domains $U$ and I wonder if that proof could be extended for existence in $[0,T]\times \mathbb{R}^n$. Any help would be most welcome. Possible Solution : Looking at the answer to this question , I split $\mathbb{R}^n$ into the integer lattice $U_k:=U+k$ for $k\in\mathbb{Z}^n$, and where $U$ is the unit cube. For $L^2(U_k)$, there is an orthonormal basis $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ which are also eigenvectors of the Laplacian $-\Delta$, and therefore, it also forms an orthogonal basis for $H_0^1(U_k)$. Now, we may extend each $e_n^k$ outside $U_k$ by $0$ so that it belongs to $H^1(\mathbb{R}^n)$. These $\{e_l^k; k\in\mathbb{Z}^n, l\in\mathbb{Z}\}$ form an orthonormal basis for $L^2(\mathbb{R}^n)$ and also an orthogonal basis for $H^1(\mathbb{R}^n)$. Could someone confirm if this reasoning is correct or am I missing out some issue?","['spectral-theory', 'functional-analysis', 'partial-differential-equations']"
683894,Multiplying two Gamma distributions over the same variable,"I am looking at a software library where there is a function that multiplies two Gamma distributions defined over the same random variable. So, it is basically multiplying two Gamma densities with shape and rate parameters. So, the Gamma distribution parameterised by the shape and rate parameters is given as: $$
D(x; \alpha, \beta) = \frac{\beta^{\alpha} x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}
$$ where $\alpha$ is the shape parameter and $\beta$ is the rate parameter and $\Gamma$ is the Gamma function defined as $\Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha-1} e^{-x} dx$. Do, I want to get an expression for the product of $D(x; \alpha_0, \beta_0)$ and $D(x; \alpha_1, \beta_1)$. Apparently, this should be another Gamma distribution and according to this software implementation, the shape and rate parameters for this new Gamma distribution should be: $\alpha_{new} = \alpha_0 + \alpha_1 - 1$ and $\beta_{new} = \beta_0 + \beta_1$. Now, apparently this should be simple algebra but I have been unsuccessfully been trying to formulate this product in a way that I can equate the terms with the expression for the Gamma distribution that would arrive at this result.","['probability-distributions', 'gamma-function', 'calculus', 'algebra-precalculus']"
683907,Invariant subspaces and eigenvectors,"Let $T:V \rightarrow V$ be a linear transformation where $V$ is a finite dimensional vector space. Let $W \subset V$ be such that $T(W) \subset W$. If $v_1, v_2, \ldots ,v_n \in V$ are eigenvectors corresponding to distinct eigenvalues $\lambda_1, \lambda_2, \ldots ,\lambda_n$ of $T$ and $v_1 + v_2 + \ldots + v_n \in W$, prove that each $v_i \in W$. I could prove that all the above eigenvectors are linearly independent and that atleast one $v_i$ lies in $W$ but how do I show all $v_i's \in W$? Any hint is appreciated. Note : Even though this has a homework tag, I should stress that this isn't my homework. It's been a few years since I last took a university course. I found this question while going through some (really) old papers in my desk!",['linear-algebra']
683914,Pigeon Hole. 80 numbered balls,"We have $80$ numbered balls(From $1$ to 80).Among which are $45$ blue and $35$ orange. Prove that at least two blue balls differ by $9$. For example $13$ and $22$ or $69$ and $78$. So they can differ by $1,2,\ldots,79$. Hmm...","['pigeonhole-principle', 'discrete-mathematics']"
683918,Prove that the following matrices cannot represent the linear transformation $T$ in ANY basis,"$T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined as $T(x,y,z) = (2x,z,y)$ is a linear transformation. I need to prove that the following matrices cannot represent $T$ in ANY basis: $$\begin{bmatrix} 2 & 0 & 1 \\
0 & 1 & 0\\
2 & 0 & 1
\end{bmatrix}$$ $$\begin{bmatrix} 1 & 0 & 1 \\
1 & 0 & 0 \\
0 & 2 & 1\end{bmatrix}$$ My attempt for the first matrix was to assume (negatively) existence of a basis in which the given matrix is the representation, and left multiplying it by $(x,y,z)^{T}$ to get $(2x+z,y,2x+z)^{T}$. Can I conclude that given it is not in the form $(2x,z,y)^{T}$, there is NO such basis? I want to know if this is correct and, in addition (Or alternatively), learn other ways to solve this exercise. Thank you.","['matrices', 'linear-algebra', 'transformation']"
683926,What is the significance of $\sigma$-fields in probability theory?,I'm an undergraduate level maths student and I've just done a number of exercises on elementary set theory and sigma fields. We've just started the courses and are discussing probability events as sets. I can't see the link between sigma fields and probability theory. Can anyone explain this intuitively at an undergrad level?,"['probability-theory', 'elementary-set-theory']"
683943,"Simple equation, but I don't get it","I don't know how to do this simple equation, could you help me? thanks! 6x + 4   =  4 [ 2x -5 ( x -2 ) ] 6x +4 = 4  [ 2x -5x +10 ] 6x +4 = 8x -20x +40 +6x -8x +20x = -4 +40 18x  = 36 36:18 = 2 Thank you all!",['algebra-precalculus']
683965,Category theory with multisets,"An alternative to the notion of multiset introduced in Section 2.2 of Aluffi Chapter 0 is obtained by considering sets endowed with equivalence relations; equivalent elements are taken to be multiple instances of elements 'of the same kind'. Define a notion of morphism between such enhanced sets, obtaining a category MSet containing (a 'copy' of) Set as a full subcategory. (There may be more than one reasonable way to do this! This is intentionally an open-ended exercise.) Which objects in MSet determine ordinary multisets as defined in Section 2.2 and how? Spell out what a morphism of multisets would be from this point of view. (There are several natural notions of morphisms of multisets. Try to define morphisms in MSet so that the notion you obtain for ordinary multisets captures your intuitive understanding of these objects.)",['abstract-algebra']
683997,Proof or counterexample: $L^p$-boundedness gives a.e. convergent subsequence?,"Let $\Omega\subset\mathbb{R}^{d}$ open and let $f_{n}\in L^{2}\left(\Omega\right)$ be bounded.
Then there is obviously a weakly convergent subsequence. Is there also a subsequence converging almost everywhere? Do you know of a counterexample, i.e., a weakly convergent (or bounded) sequence without any subsequence converging almost everywhere? Thanks a lot in advance! :-)","['sobolev-spaces', 'weak-convergence', 'real-analysis', 'almost-everywhere', 'functional-analysis']"
683998,generating function for binary strings that don't contain $00100$ as a substring?,"On an alphabet $\{0, 1\}$, what's the generating function for the set of strings that don't contain $00100$ as a substring?
I've tried writing the set of strings that don't contain $00100$ in terms of concatenations of other sets(here is where i get stuck) and then use the product lemma.
Any hint on how i might write it?","['discrete-mathematics', 'combinatorics']"
684001,When does product of derivatives equals derivative of products?,"In general, $\frac{d}{dx}(f(x) \cdot g(x)) \neq  \frac{d}{dx}f(x) \cdot \frac{d}{dx}g(x)$ When does this result hold true? My first try is to use product rule on left side and compare the two sides, but this hasn't helped at all. Any suggestions?",['calculus']
684011,Homology of stack points,"This is a very basic question about how definitions in homology carry over to the easiest example of stacks. Let $G$ be a finite cyclic group. Consider the classifying stack $\mathcal{B}G$. This has a (nonrepresentable) morphism to a point: $st:\mathcal{B}G\to P$. What do people mean when they say the ""fundamental class"" $[\mathcal{B}G]$ of $\mathcal{B}G$? (I don't know how homology carries over to stacks.) For example, is it true that $st_*[\mathcal{B}G]=[P]$? Or that $st^*[P]=[\mathcal{B}G]$? Are these equivalent? My guess: It seems to me that the map $st$ should be considered as ""degree"" $1/|G|$. Because of this it seems like we should have $st^*[P]=|G|[\mathcal{B}G]$. I'm not sure what this would say about $st_*[P]$. Thanks!","['algebraic-geometry', 'algebraic-stacks', 'algebraic-topology']"
684013,What is the actual meaning of runs in terms of coin tossing?,"I am not able to understand the concept of runs in terms of coin tossing. As per my question, suppose a coin is tossed x times, then we have to find the expected number of runs. But, what is a run? Can anybody give a detailed example? As per the question, the sequence of tosses HHHHTTHTTTHHHTHH has 7 runs. How? I am really not able to understand. I researched and found somewhere that a run is when the previous outcome is different from current, so for this case, how will it apply? Can somebody please explain in layman terms? Thanks",['probability']
684045,Taylor expansion of a vector field on manifold,"In my work I have a need for some kind of analogue of Taylor expansion of a vector field on Riemannian manifold $\mathcal{M}$. I came to such an expression:
$$
F(\operatorname{exp}_p(v)) = \operatorname{d}\operatorname{exp}_p \vert_{v} \Big( F(p) + \nabla_{p}{F}v + \frac{1}{2}\nabla_{p}^{2}{F}(v, v) + o(||{v}||_p^2) \Big),
$$
where $v \in B_r(0) \subset T_p\mathcal{M}$, $B_r(0)$ maps on $\mathcal{M}$ through $\operatorname{exp}_p$ diffeomorphically,  $\nabla$ is the Levi-Civita connection, $\nabla_pF(v) = \nabla_vF$ for any vector $v$ from $T_p\mathcal{M}$. I haven't seen this formula anywhere in several papers/books on Riemannian geometry I've read and it made me think I've done some mistake while deriving this formula.
Here is what I've done: consider function
$$\tilde{F}(v) = \operatorname{d exp}^{-1}_{p} \vert_{\operatorname{exp}(v)} \circ F \circ \operatorname{exp}_p(v).$$
It is a function from $T_p\mathcal{M}$ to $T_p\mathcal{M}$, so we can use standart Taylor formula for expansion around $0$:
$$\tilde{F}(v) = \tilde{F}(0) + \operatorname{d}\tilde{F} \vert_{0}(v) + \operatorname{d}^2\tilde{F} \vert_{0}(v, v) + o(||v||_p^2).$$ Now we'll show that $\operatorname{d}\tilde{F}\vert_{0} = \nabla_pF$. By the chain rule we have
$$\operatorname{d}\tilde{F}\vert_{0} = \operatorname{d}\big(\operatorname{d exp}^{-1}_{p} \vert_{\operatorname{exp_p}(0)}\big)\vert_{F \circ \operatorname{exp_p}(0)} \circ \operatorname{d}F \vert_{\operatorname{exp_p}(0)} \circ \operatorname d \operatorname{exp_p}  \vert_{0}.$$
Since $\operatorname{d} \operatorname{exp_p}  \vert_{0} = \operatorname{Id}$ and since by invere function theorem $\operatorname{d exp}_p^{-1} \vert_{\exp_p(0)} = (\operatorname{d exp}_p \vert_{0})^{-1} = \operatorname{Id}^{-1} = \operatorname{Id}$, we have
$$\operatorname{d}\tilde{F} \vert_{0} = \operatorname{d}F\vert_{exp_p(0)} = \operatorname{d}F \vert_{p}.$$
As I understand, $\operatorname{d}F \vert_p = \nabla_pF$ (this is the point of my doubts). Therefore, $\operatorname{d}^{i}F \vert_p = \nabla^{i}_pF$. So,
$$\operatorname{d exp}^{-1}_{p} \vert_{\operatorname{exp}(v)} \circ F \circ \operatorname{exp}_p(v) = \tilde{F}(0) + \nabla_pF(v) + \nabla^2_pF(v, v) + o(||v||^2_p).$$ After using inverse function theorem and equality $\tilde{F}(0)=F(p)$ we get first expression. So I have following questions: is this formula wrong and if yes, where am I wrong and can someone tell me some source where analogous result is derived? UPDATE: I've found at least one mistake in above formulas (which may be the cause of absence of curvature tensor mentioned by Yuri in comments). It is the wrong use of the chain rule. Since $\operatorname{d exp}_p^{-1} \vert_{v}$ depends on $v$, differential of $\tilde{F}$ should look like this I guess (by the analogy with $\mathbb{R}^n$ case):
$$\operatorname{d}\tilde{F}\vert_{0} = \operatorname{d}h\vert_{0} \circ F \circ \operatorname{exp_p(0)} + h(0) \circ \operatorname{d}F \vert_{\operatorname{exp_p}(0)} \circ \operatorname d \operatorname{exp_p}  \vert_{0},$$
where h  = $\operatorname{d exp}_p^{-1} \vert_{v}$. But this is only a guess, I cannot justify it using properties of $\nabla$. I know that covariant derivative obeys Leibniz's rule on tensor product, I also know that it commutes with contraction, but I can't see how it helps here. I thought it was better to open dedicated question for this: Chain rule with covariant derivative .","['riemannian-geometry', 'differential-geometry']"
684053,Is it possible to calculate this integral,"$$
\mbox{Is it possible to calculate this integral}\quad
\int{1 \over \cos^{3}\left(x\right) + \sin^{3}\left(x\right)}\,{\rm d}x\quad {\large ?}
$$ I have tried  $\dfrac{1}{\cos^3(x)+\sin^3(x)}$=$\dfrac{1}{(\cos(x)+\sin(x))(1-\cos x\sin x)}$ then I made a decomposition. But I'm still stuck.
Thank you in advance.","['calculus', 'integration']"
684078,Functional equations $f(x+y)= f(x) + f(y)$ and $f(xy)= f(x)f(y)$,"Let $f:\mathbb{R}\to \mathbb{R}$ is a function such that for all real $x$ and $y$, $f(x+y)= f(x) + f(y)$ and $f(xy)= f(x)f(y)$, then prove that $f$ must be one of the two following functions: $f:\mathbb{R}\to \mathbb{R}$ defined by $f(x)=0$ for all real $x$ OR $f:\mathbb{R}\to\mathbb{R}$ defined by $f(x)=x$ for all real $x$ I got to the point where putting the two equations together, you get $f(x+y)f(x)= f(xy) + f(x)^2$ and plugging in $f(x)=x$ checks with it. So am I going in the right direction or am I just doing some guess work? Is there a more elegant way of doing it? Thanks","['real-analysis', 'functional-equations']"
684141,Check if a point is on a plane? (Minimize the use of multiplications and divisions),"In $\mathbb R3$, given a plane $\mathcal P$ defined by three 3D points points $v_0, v_1, v_2$, I want to check if another point $p$ belongs to that plane, while avoiding the use of multiplications and divisions as much as possible . The reason is to mitigate the floating point errors incurred by the computer representation of decimal numbers. My current method is: Compute the general form of the plane equation $ax+by+cz+d=0$ Where $a,b,c$ are the components of the plane's unit normal vector
$N={(v_1-v_0)\times(v_2-v_0) \over \|(v_1-v_0)\times(v_2-v_0)\|}$ And $d=N.v_0$ Plug point $p$ into the plane equation: $res=a.p_x+b.p_y+c.p_z-d$ If the result is null, the point lies on the plane Because of floating point errors, I actually check if the result is ""almost"" null: $|res|<\epsilon$ However, in certain cases, when I plug $v_2$ into the plane equation, I find that it does not belong to the plane, even though I used $v_2$ to compute the equation in the first place. (I obtain a result bigger than my $\epsilon$.) This is due to floating point errors. See my question on Stack Overflow: https://stackoverflow.com/q/21916606/143504 So I am looking for an alternate method that would mitigate these errors.","['floating-point', 'geometry', '3d']"
684154,Does analytic closed form solution exist for this trigonometric equation?,"solve for x: $\sin(ax)=k\sin(bx)$, a,b,k and x are real numbers I am looking for a very general solution when a,b and k are completely unrelated.",['trigonometry']
684163,Continuity of the derivative,"As we all know, all the basic properties of holomorphic functions (i.e. functions which are differentiable in the complex sense) can be deduced from Cauchy's formula. Moreover, Cauchy's formula itself can be viewed as a rather simple consequence of the Green-Riemann formula, provided that the holomorphic function you have at hand is assumed to have a continuous derivative. Of course, Cauchy's formula holds without assuming continuity of the derivative, and it yields continuity of the derivative and much more since it implies power series expansion. But Cauchy's formula (or, if you prefer, Cauchy's theorem) without assuming continuity of the derivative is a rather subtle thing, and this gives a rather ""indirect"" proof of the fact that holomorphic functions are in fact $\mathcal C^1$. So my question is the following: does anybody know a direct proof of the fact that if a function $f$ defined on an open subset of $\mathbb C$ is differentiable in the complex sense, then its derivative $f'$ is continuous? I'm pretty sure I am not the first one and will not be the last one to ask this question, at least for himself (or herself). So please feel free to close it if it has indeed been asked previously on this site. Edit. Perhaps I should say a few more words about what I mean by a ""direct proof"". Anything that relies in one way or another to Cauchy's formula or Cauchy's theorem is not considered as a direct argument. A direct proof should somehow establish ""from scratch"", or ""from very basic principles"" that holomorphic implies $\mathcal C^1$.",['complex-analysis']
684167,weighted sum of two i.i.d. random variables,"Suppose we know that $X_1$ and $X_2$ are two independently and identically distributed random variables. The distribution of $X_i$ ($i=1,2$) is $P$, and we have some constraints on $P$ that $$\mathbb{E} X_1 = 0$$ (zero-mean) and $$\mathbb{E} X_1^2 = 1$$ (variance is normalized). We denote the set of all feasible distributions as $\mathcal{D}$.
Then, my question is about the function 
$$f(w_1,w_2,a): = \sup_{P\in \mathcal{D}} \Pr(w_1X_1+w_2X_2\ge a)$$
where $w_1\ge 0$, $w_2\ge 0$ and $a> 0$. Q1 : Can we get an analytic solution of $f?$ Q2 : Can we know some properties of $f$? For example, it is easy to show that $f$ is monotonically decreasing as $a$ increases when $w_1$ and $w_2$ are fixed. I want to ask if $a$ is fixed, and $w_1+w_2$ is fixed, is $f$ (quasi-)convex (or concave, monotonic...) on $w_1$? For the two questions above, I also appreciate answers for another definition of the set $\mathcal{D}:=\{ P \mid \mathbb{E} X_1 = 0, P(|X_1|>1)=0 \text{ (bounded instead of restriction on variance)}\}.$ Thank you in advance.","['statistics', 'probability']"
684191,Modelling a Water Rocket. Requires Some Validation and Help. ( WARNING : Extremely Long but Interesting Post ),"Good day people of math.stackexchange.com UPDATE: Version 2 can be found here: https://physics.stackexchange.com/questions/275284/modelling-a-water-bottle-rocket-version-2-long-post-warning . This is a pet project that I plan to use to convince my Prof that I would rather try something similar to this than to do the prescribed project. Edit : There are clearly enough views, but as of yet, not 1 reply. I believe the question is stated to late. So here it is : Am I mathematically correct?  Should the $\frac{d(v^2t)}{dt}$ be left as is, then simplified like I did in my post, or should I differentiate it, then solve further?  The answer is not very different, but it will surely affect the outcome. Task The modelling of a Water Rocket's flight profile for a set of predefined variables. Assumptions to start off with No drag force on the rocket No shear stresses inside the tank due to the flowing water No buoyancy issues No phase changes in either fluid Temperature of everything is at $25^{\circ}C$ The nozzle is just a hole in the bottom of the rocket. ( depicted as a cone for clarity ) I am using SI-units 3 decimals are acceptable for this problem Basically, I should only use the conservation of mass and momentum on this.  The reason for this is that we are only working through it now.  We will start working with Navier-Stokes equations in about 5 weeks time.  I have had this subject before, but didn't make it the first time.  I plan to incorporate Navier-Stokes for my semester project in this subject.
The subject is Transport Theory 1, a third year's Chemical Engineering subject. Concerns/Things to take into account : As the rocket starts to lift off, ( assuming that with the specs I have, it can ), water is ejected through a nozzle at the bottom of the rocket.  This causes a pressure decrease of the air in the tank and an increase in volume of the air.  This in turn causes the force applied by the air to lessen over time and as a result, the acceleration of the rocket decreases. I have design the rocket's size to ensure that once all the water is ejected, the air inside the tank will have a pressure of 1 atmosphere to ensure that no air escapes and that a back-draft is not created.  Bare in mind that this model will only be valid as long as there is water in the tank.  The rest of the flight profile is quite easy. So basically we have : $ P_{air}:V_{air} : M_{water} : \rho{_{air}} : acceleration(a) : velocity(v) : displacement(x) $ are time dependant $ M_{air} : Temperature(T) : Atmospheric P(P_{atm}) : \rho{_{water}}  $ and rocket dimensions are constant. Equations available to me : Text book used : Fundamentals of Momentum, Heat and Mass Transfer by W,W,W,R Conservation of Mass $$ ∫∫_{c.s} {\rho}(\mathbf{v \bullet n})dA +  \frac{\partial}{{\partial}t}∫∫∫_{c.v}{\rho}dV = 0 $$ Where the first term is the mass change in and out of the system, and the second term is the accumulation of mass. C.S and C.V are the control surface and volume respectively. Conservation of Momentum : $$ ∫∫_{c.s} \mathbf{v}{\rho}(\mathbf{v \bullet n})dA +  \frac{\partial}{{\partial}t}∫∫∫_{c.v}\mathbf{v}{\rho}dV = \sum F  $$ This first term is the net rate of momentum efflux from the control volume, the second term is the rate of accumulation of momentum within the control volume and the right hand side is the sum of forces acting on control volume. Schematic $V_1$ indicates the volume of pressurised air and $V_2$ the volume of water. $M_1 : n_1 $ indicates the mass and mole amount respectively of air and $M_2$ the mass of water in the tank. $M_T$ is the total mass of the rocket. $d_1/A_1 $ and $d_2/A_2$ indicate the diameter/Area of the rocket and the nozzle respectively. Further assumptions: $V_3 = V_2+V_1$ $h_3 = 5d_1$ $d_1 = 10d_2$ subscript (i,f) are used to define initial and final values whereas no subscript denotes a variable's value at a certain point in time.  e.g $P_1 + P_{1,f} - P_{1,i} $ Let's begin, shall we? $h_3 = 3$ $d_1 = 0.6  : A_1 = 0.283  $ $d_2 = 0.06  : A_1 = 0.003 :  $ Calculating the mole amount of air need to fill the empty tank. $$ V_3 = A_1h_3 = 0.848 $$ $$ n_1 = \frac{P_{atm}V_3}{RT} = 34.655 $$ The Volume that the air occupies with 1.5MPa $( P_{1,i} )$ of pressure : $$V_1 = \frac{nRT}{P_1} = 0.057  $$ The Volume that the water occupies : $$V_2 = V_3 - V_1 = 0.791 $$ The mass of all things : $M_1 = 1  $ $M_2 = 791  $ $M_3 = 108 : mass of the rocket's shell ( chosen to make a round number for total mass ) $M_T = M_1 + M_2 + M_R = 791+1+108 = 900$ kg  ; Finding air pressure as a function of water mass Because the water is ejected, it creates a pressure drop.  It seems logical to formulate $P_1$ like this, for now : $$ P_1 = \frac{nRT}{V_1} : V_1 = A_1h_1 $$ $$ P_1 = \frac{nRT}{h_1A_1} $$ but recall that $h_1 = h_3 - h_2$ and that $V_2 = A_1h_2$ also, $V_2 = M_2/{\rho}_2$ thus $$ h_2 = \frac{M_2}{A_1{\rho}} = 0.0035M_2 $$ $$ h_1 = h_3 - h_2 = 3 - 0.0035M_2$$ Plugging this into the pressure equation : $$ P_1 = \frac{nRT}{A_1(3 - 0.0035M_2)} $$ The Mass Balance $$ ∫∫_{c.s} {\rho}(\mathbf{v \bullet n})dA +  \frac{\partial}{{\partial}t}∫∫∫_{c.v}{\rho}dV = 0 $$ **The first term can be written as ** $$ ∫∫_{c.s} {\rho}(\mathbf{v \bullet n})dA = ∫∫_{A_2} {\rho}vdA = {\rho_{2}}vA_2 $$ Since there is only an efflux of mass here, there is only one term on the right hand side.  The sign is positive because the direction of the efflux and the velocity is the same, downward, cancelling each other out. **The second term ** $$\frac{\partial}{{\partial}t}∫∫∫_{c.v}{\rho}dV = \frac{d}{dt}∫^{M_2}_{M_{2,i}}dM =\frac{d}{dt}(M_2-M_{2,i}) $$ Recall that : $M_2$ is the mass of water at any given time, and $M_{2,i}$ is the mass of water at $ t = 0 $ , which is known.  Since the mass is only a function of time, the partial derivative can be written as a total derivative because all variables in $M_2$ are only dependent on time. **Plugging these two terms back into the original equation and simplifying a bit ** $$ {d}(M_2-M_{2,i}) = -{\rho_{2}}vA_2dt $$ Integrating on both sides $$ ∫{d}(M_2-M_{2,i}) = -∫^{t}_{0}{\rho_{2}}vA_2dt $$ Evaluating the left side as an indefinite integral and solving for $M_2$ produces $$ M_2 = M_{2,i} - {\rho}vA_2t $$ The Momentum Balance $$ ∫∫_{c.s} \mathbf{v}{\rho}(\mathbf{v \bullet n})dA +  \frac{\partial}{{\partial}t}∫∫∫_{c.v}\mathbf{v}{\rho}dV = \sum F  $$ **The first term ** $$ ∫∫_{c.s} \mathbf{v}{\rho}(\mathbf{v \bullet n})dA = -v{\rho}vA_2  $$ The same reasoning behind the first term of the mass balance applies here, but the $-v$ is because $v$ is in a downward direction. **The second term ** $$ \frac{\partial}{{\partial}t}∫∫∫_{c.v}\mathbf{v}{\rho}dV = \frac{d}{dt}(M_2-M_{2,i}) $$ The same calculation happened here as in the mass balance.  Substituting $M_2$ results in : $$ \frac{d}{dt}(M_2-M_{2,i}) = -{\rho}A_2\frac{d(v^2t)}{dt} $$ **The right hand side of the Momentum Balance ** $$ \sum F = P_{atm}A_2 - P_1A_1 - M_Tg $$ Where the first term is the force exerted by the atmosphere on the nozzle opening in an upward direction. The second term is the force exerted by the compressed air onto the water over $A_1$ in a downward direction.  The third term is the gravitational force in a downward direction on the total mass, $M_T$ , of the system.  Recalling that $M_T$ = M_R + M_1 + $M_2 = 109 + M_2$ . Using the above and substituting $M_T$ and $M_2$ : $$ \sum F = P_{atm}A_2 - P_1A_1 - (109 +  M_{2,i} - {\rho}vA_2t)g $$ **Now, combining the three main terms to produce the completed Momentum Balance  ** $$ -{\rho}v^2A_2 -{\rho}A_2\frac{d(v^2t)}{dt} = P_{atm}A_2 - P_1A_1 - (109 +  M_{2,i} - {\rho}vA_2t)g $$ Simplifying the Momentum Balance $$ -{\rho}A_2\frac{d(v^2t)}{dt} = (109 +  M_{2,i} - {\rho}vA_2)g + P_1A_1 -{\rho}v^2A_2t - P_{atm}A_2  $$ This gives $$ \frac{d(v^2t)}{dt} = \frac{(109 +  M_{2,i} - {\rho}vA_2)g + P_1A_1 -{\rho}v^2A_2t - P_{atm}A_2}{-{\rho}A_2} $$ Using the chain rule on the left hand side : $$ \frac{d(v^2t)}{dt} = 2vt\frac{dv}{dt} $$ Substituting this and simplifying : $$ \frac{dv}{dt} = \frac{(109 +  M_{2,i} - {\rho}vA_2t)g + P_1A_1 -{\rho}v^2A_2 - P_{atm}A_2}{-2{\rho}A_2vt} $$ **Plugging in values for all the constants and simplifying each term ** $$ \frac{dv}{dt} = \frac{(8829 - 29.43vt) + (0.283P_1) - (3v^2) - (303.9)}{-6vt} $$ On to the variable $P_1$ Recall that $$ P_1 = \frac{nRT}{A_1(3 - 0.0035M_2)} $$ and $$ M_2 = M_{2,i} - {\rho}vA_2t = 791 -3vt $$ These two formulae combine to form : $$ P_1 = \frac{303400}{3 - 0.0035(791 -3vt)} = \frac{303400}{0.232 + 0.011vt} $$ In the above equation, $ nR$ , $mol.\frac{J}{molK}$ was replaced with $ M_1R$ , $ kg.\frac{J}{kgK}$ Plugging $P_1$ into the Momentum Balance and simplifying a bit $$ \frac{dv}{dt} = \frac{3v^2 + 29.43vt -8525.1 }{6vt} - \frac{0.283P_1}{6vt} $$ Substituting $P_1$ $$ \frac{0.283P_1}{-6vt} = \frac{85862.2}{0.232 + 0.011vt}\frac{1}{6vt} $$ Simplifying again $$ \frac{85862.2}{0.232 + 0.011vt}\frac{1}{6vt} = \frac{85862.2}{1.392vt + 0.066v^2t^2} $$ The Final, Simplified Momentum Balance $$ \frac{dv}{dt} = \frac{3v^2 + 29.43vt -8525.1 }{6vt} - \frac{85862.2}{1.392vt + 0.066v^2t^2} $$ The Final, Non-Simplified Momentum Balance $$ \frac{dv}{dt} = \frac{{\rho}A_2v^2 + (M_R + M_1 + M_{2,i} - {\rho}A_2v)g - P_2A_2 }{2{\rho}A_2vt} - \frac{{\rho}A_2nRT}{{\rho}A_2h_3-M_{2,i}+{\rho}A_2vt}\frac{1}{2{\rho}A_2vt} $$ Questions and Concerns I hope that if you read this horribly long peace of work, that you found my work and explanations adequate enough. The main Reason I posted this work here Is my work correct?
I believe that this equation is not explicitly solvable for $v$ , unless someone has a way?
I plan to use MatLab Simulink to graph the function $v$ and $x$ , does it seem possible? It does to me, anyway. I Thank You Sincerely for Your Time in This Matter!","['fluid-dynamics', 'ordinary-differential-equations', 'mathematical-modeling', 'physics']"
684193,"let $A$ be an n by n matrix, show that $||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP}$","We are given $A \in M_{n}(\mathbb R)$ and the following norms: $||.||_{e}$ is the standard euclidean norm of $\mathbb R^n$. $||A||_{OP}$ is the operator norm of $A$, meaning $||A||_{OP} = sup_{||v||_e=1} ||Av||_e$ $||A||_{HS}$ is the Hilbert-Schmidt norm. Meaning $||A||_{HS} = trace(A^TA)$ Show that $||A||_{OP} \leq ||A||_{HS} \leq \sqrt{n} ||A||_{OP}$ Firstly I found out what the hilbert-schmidt norm is, and $trace(A^TA) = \sum a^2, a\in A$ How can I show it is bigger than the operator norm?","['matrices', 'normed-spaces', 'linear-algebra']"
684205,Domain of an absolute value,"How do we solve for a domain of a function, when it involves absolute values?
For example (I created the example myself, so it might be a bit weird): $$f(x) = \frac{1}{\sqrt{|2x+1| - |x-3|}}$$ Thank you!","['algebra-precalculus', 'functions']"
684215,Finding two analytic functions,"I want to find two analytic functions (the first one is analytic in the upper half plane the second one in the lower half plane) $f_+(z)$ and $f_-(z)$ which satisfy $f_+(x)-f_-(x)=\frac{1-\cos x}{x}$ and $f_{+-}(x)=\lim_{\epsilon\rightarrow 0}f_{+-}(x+-i\epsilon)$ with $x\in\mathbb R$ Whats the best way to start with these kind of problems? Integrate the RHS over an appropirate contour and see if this gives me something? EDIT: I want to solve this problem in order to have some idea how to solve the integral equation $f(x)+\frac{\alpha}{i\pi}P.V\int_{-\infty}^{\infty}\frac{f(\xi)}{\xi-x}d\xi=\frac{1-\cos x}{x}$ where $\alpha$ is a constant different that +-1 and $f(x)=f_+(x)-f_-(x), \frac{1}{i\pi}P.V\int_{-\infty}^{\infty}\frac{f(\xi)}{\xi-x}d\xi=f_+(x)+f_-(x)$",['complex-analysis']
684223,Inverse of an integral transform,"Suppose that in a certain domain of analyticity we're given a function $A(s)$ in terms of the integral :
$$A(s)=\int_{0}^{\infty}\frac{a(t)}{t(t^{2}+s^{2})}dt$$
How can we recover $a(t)$?","['complex-analysis', 'integral-transforms']"
684228,"Meaning of ""Percent increase""","When someone uses the phrase ""percent increase"" what does that mean? For example, if something took $4$ seconds before and now it takes $1$ second, would that be a $400$ % increase?","['statistics', 'percentages']"
684248,Rewrite $f(x) = 3 \sin (\pi x) + 3\sqrt{3} \cos (\pi x)$ in the form $f(x) = A \sin (Kx+D)$,"I got a question like that said ""Rewrite $f(x) = 3 \sin (\pi x) + 3\sqrt{3} \cos (\pi x)$ in the form $f(x) = A \sin (Kx+D)$"". I'm inclined to think that since the periods are the same ($2$), that the amplitudes will just add up. But, I'm not sure. I also need to know the rules for combining sinusoids with different periods. I know that when you're multiplying them, the one with the longer period acts as a sort of envelope for the one with the smaller period. But what do you do with different periods when you add them? Thanks! evamvid","['trigonometry', 'algebra-precalculus', 'graphing-functions']"
684272,Generating a uniform distribution in the volume of a box,"Suppose I have a three dimensional box, of volume $V$, and with lengths $x, y$, and $z$.  I then change the box volume by $\Delta V$, such that $(V + \Delta V) = (x + \Delta x)(y + \Delta y)(z + \Delta z)$. I would like to generate a uniform distribution of box volumes on some interval, such that $\Delta V \in \left[-\epsilon, \epsilon \right]$. How should I select $\Delta x, \Delta y$, and $\Delta z$, such that they each follow the same distribution, but generate this uniform distribution in volume?","['statistics', 'probability-distributions']"
684274,Expressing $\mathbb{R} P^3$ as a fibre bundle,"This question came up in office hours with my differential topology prof and since then I've almost settled on an answer. The question was whether we could write $\mathbb{R} P^3$ as a fiber bundle with base space $\mathbb{R} P^2$. We spent a few moments thinking about it before deciding it wasn't relevant to the conversation at hand and moved on. Since then I've done some reading and become aware of a result that says if $M, N$ are compact and connected smooth manifolds and $f:M\rightarrow N$ is a submersion, then the fibers of $f$ are all diffeomorphic to a manifold $F$, and this gives rise to a fibre bundle with total space $M$, fiber $F$ and base $N$. Following that, I wrote a function $$f: \mathbb{R}P^3 \rightarrow  \mathbb{R}P^2$$ defined on homogeneous coordinates by $$(x,y,z,w) \mapsto (x,y,z).$$ $f$ is well-defined, smooth, and unless I've made a silly mistake in calculating Jacobians it's also a submersion. Now, I'm having trouble understanding what the preimage $f^{-1}(p)$ for $p \in \mathbb{R}P^2$ is. Is it simply $\mathbb{R}$ or is it something more complicated than that?","['differential-topology', 'fiber-bundles', 'differential-geometry', 'projective-space']"
684279,Is there a convenient formula for variance of discrete random variable in terms of CDF,"For a discrete random variable X, $\Omega_X\subseteq \{0,1,2,\ldots\}$, we can write $$\mathrm{E}[X] = \sum_{x=0}^\infty (1-F(x)) $$ where $F(x)$ is the cumulative distribution function of $X$. This formula is proving convenient to me on the current problem I'm working on where  the cumulative probability of being in a ""sink"" state naturally comes out of formulating the problem in terms of a transition matrix. However, I was wondering whether there is an analogous formula for variance in terms of the CDF, or whether if I want the variance I'm going to have to change tack? I'm thinking there isn't such a formula, because variance is defined as $E[(X-\mu)^2]$ and although $(X-\mu)^2$ is positive, it isn't an integer and so a similar approach won't work.",['statistics']
684293,Find the smallest k such that $n^k > \sum_{i=0}^{n-1} i^k$,"Let $n \in \mathbb{N}$. Is it possible to find the smallest $k \in \mathbb{N}$ such that $$n^k > \sum_{i=1}^{n-1} i^k \ ?$$ It's easy to prove that such $k$ exist because: $$n^k > 1^k + 2^k + 3^k + 4^k + \dots + (n-2)^k + (n-1)^k$$
$$ \iff 1 > \left(\frac 1n\right)^k + \left(\frac 2n\right)^k + \left(\frac 3n\right)^k + \dots + \left(\frac {n-1}n\right)^k$$ And since all fractions tends to zero when $k \to \infty$, there must be a solution for $k$. I've tried using logartihms, but I wasn't able to do anythyng. Then I tried to find the value of the sum using Faulhaber formula , but the Faulhaber formula depends on the exponent $k$ and since it's not fixed it's almost useless. Any other way to solve this?","['inequality', 'number-theory', 'exponential-function', 'exponentiation', 'summation']"
684306,How do you describe a CDF in terms of another CDF?,"This is homework, but I'm more interested in understanding the problem than the solution, so answering with a different example is totally fine. The problem has multiple parts, but I'm only stuck on this. We have a CDF $F_x(x)$ for a continuous random variable, $X$. The CDF was not given. But we want to express the CDF $F_y(y)$ in terms of $F_x(x)$. We define $Y = 1 - 2X$. How do we find this? I know that $F_x(-\infty)=0$ and $F_x(\infty)=1$ for a CDF. I've been scouring the book and the internet but I can't find where this is explained. At first, I was thinking you could do $F_y(y)=1-2F_x(x)$ but then $F_x(-\infty)=1$ and $F_x(\infty)=-1$ right? Which isn't possible for a CDF. I'm not sure where to go from here.","['statistics', 'probability']"
684332,Why is there no generalization of the determinant to infinite dimensional vector spaces?,"This question is to add to my understanding why the concept of a determinant does not extend to an infinite dimensional vector space. I am already aware of a couple facts which hint why this is so: The determinant of an endomorphism of a finite dimensional vector space with dimension $n$ can be defined in a basis-free way as the composition of these canonical maps:
$$\mathrm{End}(V)\xrightarrow{\phi} \mathrm{End}(\Lambda^n V)\xrightarrow{\psi} K$$
defined by $\phi(A)=((x_1\wedge\cdots\wedge x_n)\mapsto(Ax_1\wedge\cdots\wedge Ax_n))$ and $\psi$ defined as the inverse of the map $\psi^{-1}:K\rightarrow \mathrm{End}(\Lambda^n V)$ defined by $\psi^{-1}(\lambda)=(x\mapsto \lambda x)$. This construction reveals why finite
dimension is important: $\mathrm{End}(\Lambda^n V)$ need not be 1-dimensional otherwise for
any $n$ if $V$ fails to be finite dimensional. And thus our last map fails to exist. Another reason that the determinant fails to extend to infinite dimensional spaces is that there are injective linear endomorphisms which do not have an inverse. Such maps may still have left inverses, but no right inverse. Such a pair is the right-shift and left-shift maps
$$(x_1,x_2,\ldots)\mapsto(0,x_1,x_2,\ldots)\qquad (x_1,x_2,\ldots)\mapsto(x_2,x_3,\ldots)$$
where the left-shift is the left inverse of the right-shift; however, the left-shift remains non-invertible. A 'good' generalization of determinant would assign non-zero determinant to the first and zero to the last. This results in the determinant of the inverse not being the inverse of the determinant. Another way to see that the concept does not generalize is that if a determinant for an operator exists, you might expect it to be the product of the eigenvalues. In general, a linear endomorphism from an infinite dimensional space can have infinitely many eigenvalues. The previous fact suggests that we could define the determinant for a specific subset of $\mathrm{Aut}(V)$, namely those automorphisms which fix all but a finite number of 1-dimensional subspaces of $V$. But how far could we go with this generalization? Once you show that the determinant exists for a finite dimensional vector space, you can
interpret the determinant as a nontrivial map which restricts to a group homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ and assigns $0$ to the rest of the endomorphisms. Can we show that there is no nontrivial homomorphism from $\mathrm{Aut}(V)$ to $K^\times$ for an infinite-dimensional vector space? Much like we can show that the sign homomorphism does not extend to $S_{\Bbb N}$?","['linear-algebra', 'determinant']"
684336,Connection between PCA and linear regression,"Is there a formal link between linear regression and PCA? The goal of PCA is to decompose a matrix into a linear combination of variables that contain most of the information in the matrix. Suppose for sake of argument that we're doing PCA on an input matrix rather than its covariance matrix, and the columns $X_1, X2, ..., X_n$ of the matrix are variables of interest. Then intuitively it seems that the PCA procedure is similar to a linear regression where one uses a linear combination of the variables to predict the entries in the matrix. Is this correct thinking? How can it be made mathematically precise? Imagine enumerating the (infinite) space of all linear combinations of the variables $X_1, X_2, ...,X_n$ of a matrix of data and doing linear regression on each such combination to measure how much of the rows of the matrix the combination can 'explain'. Is there an interpretation of what PCA doing in terms of this operation? I.e. how in this procedure PCA would select the 'best' linear combinations? I realize this procedure is obviously not computationally feasible, I only present it to try to make the link between PCA and linear regression. This procedure works directly with linear combinations of columns of a matrix so it does not require them to be orthogonal.","['statistics', 'principal-component-analysis', 'matrices', 'regression', 'statistical-inference']"
684340,Repetitions and Set Identity,"A set consisting only of an infinite list of a repeating constant is finite. For example: $\{1, 1, 1, \dots\} = \{1\}$ My question, which I suspect is rather stupid, is whether there is not a mathematically interesting sense in which the left set is bigger than the right one? As sets they're identical (due to extensionality?), so in a set theory with nothing but sets, the answer is obvious. I wonder, however, if there may be theories either without extensionality or with urelements that would be able to distinguish between the two (it is, of course, questionable whether the two should be distinguished).",['elementary-set-theory']
684343,Metrizability under homeomorphism?,"Is metrizability preserved under homeomorphism?  That is, suppose that you have a topological space $(X, \tau_1)$ whose topology comes from a metric $d$, and you have another topological space $(Y, \tau_2)$ with $Y$ homeomorphic to $X$.  Can you find a metric on $Y$ that is induced by the topology $\tau_2$? Let $h:Y \to X$ be a homeomorphism, where $(X,d)$ is a metric space.
Then the function $D: Y\times Y \to R$ given by
$D(a,b) = d(h(a),h(b))$
is a metric on $Y$ that induces its topology. Is that true? I have
trouble verifying it. Let $T$ be the topology on $Y$, and let $S$ be the topology on $Y$ induced by $D$.
Let $U$ be a member of $T$ and let $y$ be in $U$. Then $h(U)$ is open in $X$ and contains
$h(y)$. So there exists an open ball $B_d(h(y),e)$ contained in $h(U)$. Then
$h^{-1}(B_d(h(y),e))$ is an open neighbourhood of $y$. But where is the 
$D$-open ball about $y$ contained in $U$? Is it $B_D(y,e)$???","['general-topology', 'metric-spaces']"
684379,Why the natural log is there in MLE?,Why do we use natural log for MLE?,"['statistics', 'standard-deviation', 'statistical-inference']"
684412,Probability of arriving at office before $9$ am,"I'm having trouble answering this question:
A person leaves for work between $8:00$ A.M. and $8:30$ A.M. and takes between $40$ and $50$ minutes to get to his office. Let $X$ denote the time of departure and let $Y$ denote the time of travel. If we assume that these random variables are independent and uniformly distributed, find the probability that he arrives at the office before $9:00$ A.M. Any help would be appreciated.","['probability-distributions', 'probability']"
684420,Show that the interior of a set $S^0$ is open.,"$S^0$  is the interior of a set $S$. Let $x\in S^0$ be given. We want to find $\delta>0$ such that $(x-\delta,x+\delta)\subset S^0$. $S^0$ is the interior of $S$, then $S^0\subset S$, then $x\in S$ and $\exists \delta_1>0$ such that $(x-\delta_1,x+\delta_1)\subset S$. Can we say choose $\delta<\delta_1$  such that
$(x-\delta,x+\delta)\subset(x-\delta_1,x+\delta_1)$,
then $(x-\delta,x+\delta)\subset S^0$,
therefore the interior of $S$ is open? Does it make sense? Please guide me.","['general-topology', 'real-analysis']"
684471,complex analysis (Univalent function ),"The Distortion Theorem tells us that if $f$ is a univalent function
on $\mathbb{D}:=\{z:|z|<1\}$, then $|f'(z)|\leq 12\,|f'(0)|$ for $|z|\leq\frac12$.
By iterating this, prove that if $f:\mathbb{H}\rightarrow D$ is any
conformal transformation where $\mathbb{H}=\{z:\Im(z)>0\}$ and
$\Im(z),\Im(w)\geq y>0$, then
$$|f'(w)|\leq 144_{\vphantom{!}}^{\left(\tfrac{|z-w|}{y}\right)+1}\,|f'(z)|$$",['complex-analysis']
684476,Preimage of a point by a power map in quaternions,"Suppose we have a point $x_0\in{\bf H}$ (where by $\bf H$ I denote the ring of quaternions). What I'm curious about is what can the set of solutions of $x^2=x_0$ look like? From what I've checked, for $x_0$ a positive real there are only the two real solutions, as we can write $(a+bi+cj+dij)^2=(a^2-b^2-c^2-d^2)+2abi+2acj+2adij$, which implies $b=c=d=0$ if the result is to be a positive real. On the other hand, for real $x_0<0$, all solutions must have zero real part and the solutions form a $3$-sphere, while for $x_0=r\cdot i$ ($r$ real) there are again only two solutions. Is that right, or did I make some stupid mistake? The general question I'm interested in is, what do solutions of $x^n=x_0$ look like (geometrically) for arbitrary $n\in{\bf N},x_0\in{\bf H}$? (I'm tagging this with algebraic geometry because I'm actually asking about some specific varieties, though I'm aware that the context is rather unusual; feel free to remove the tag if you find it inappropriate.) Edit : Using the argument in Lutzl's answer, we can see that there are exactly $n$ solutions whenever $x_0$ is nonreal. Furthermore, it's rather obvious we can assume $\lvert x_0\rvert=1$ (as scaling $x_0$ by a positive real factor will only scale the set of solutions by an appropriate positive real root, and for $x_0=0$ the only solution is zero, since quaternions are a domain), so that leaves us with the case $x_0=1$ and $x_0=-1$, and I'm interested in what do solutions look like exactly, like in case of $n=2$ we know that they are two distinct points or a single $3$-sphere, respectively.","['quaternions', 'algebraic-geometry', 'polynomials']"
684500,Functor of points $h_X$ is an fpqc sheaf on $\operatorname{Spec} \Bbb{Z}$,"I want to show the following. Let $X$ be any scheme (say over the terminal object $\operatorname{Spec} \Bbb{Z}$ in $\textbf{Sch}$) and $A \to B$ a faithfully flat ring homomorphism. Then $$h_X(A) \to h_X(B) \stackrel{\longrightarrow}{_\longrightarrow} h_X( B\otimes_A B)$$ is an equalizer. I am being lazy here and denoting $\operatorname{Spec} A$ by just $A$, etc. Now the case when $X$ is affine is easy; this follows from the fact that the relevant Amitsur complex is acyclic when $A \to B$ is faithfully flat. However how can we reduce to the case that $X$ is affine? I have tried many things like assume $X$ is separated and $A \to B$ finite type to see if this works, and can come up with something. Is this result though true in general?","['sheaf-theory', 'algebraic-geometry']"
684504,Probability Questions! [duplicate],"This question already has answers here : Probability Dice Game Follow Up (3 answers) Closed 10 years ago . Alex, Bret, and Chloe repeatedly take turns tossing a fair die. Alex begins; Bret always follows Alex; Chloe always follows Bret; Alex always follows Chloe, and so on. Find the probability that Chloe will be the first one to toss a six.","['dice', 'probability']"
684519,What is the most scientific way to assign weights to historical data?,"This is a common question I usually face while processing historical data. I have year on year data of an event for the past N years.I would like to assign weights to the data of these N years so that the data corresponding to the most recent year as the highest weight and the data corresponding to the oldest year has the least. This is to give more importance to recent trend as compared to very old trends. Questions Is there any scientific way to assign weights to the years i.e. what should be the weight for the most recent year, what should be the weight for the previous year an so on? Is there any scientific justification in assigning equal weights to the preceding historical data after a certain point? Is there any scientific justification to ignore the preceding historical data after a certain point i.e. weight = 0? I think the answers will depend on the exact nature of the data under study and we might not have a general answer that will work with all data. Even then can we at least have a rule of thumb which is independent of the actual data. Any reference or pointers to literature would also be helpful.","['statistics', 'data-analysis', 'average']"
684551,Question about a remark in Serre's Local Fields,"I am reading Serre's Local Fields. In Section V.4, Serre considers a finite totally ramified extension of local fields $L/K$ with the residue field $\bar{L}=\bar{K}$ a perfect field. For $\bar{K}'$ a finite extension of $\bar{K}$, let $K'$ be the corresponding unramified extension of $K$, and let $L'=LK'$. The norm maps (for $n\geq 1$)
$$
N_n: U^{\psi(n)}_L/U^{\psi(n)+1}_L \to U^n_K/U^{n+1}_K
$$
$$
N'_n: U^{\psi(n)}_{L'}/U^{\psi(n)+1}_{L'} \to U^n_{K'}/U^{n+1}_{K'}
$$
can be identified with $N_n:\bar{K}\to \bar{K}$ and $N'_n:\bar{K}'\to \bar{K}'$, given by polynomials that are determined in Section V.3. He remarks that the coefficients of the polynomials are the same for $N_n$ and $N'_n$ (which I believe), and then he writes: In the language of algebraic geometry, that means we have for each
  $n\geq 1$ a homomorphism $v_n:G_a\to G_a$ rational over $\bar{K}$, and
  that $N_n'$ is the restriction of $v_n$ to the points of $G_a$
  rational over $\bar{K}'$. Can some one elaborate on it? I have some exposure to algebraic geometry (from Hartshorne's textbook) but I fail to recognize anything familiar here. I do not even know what the word rational means here. Thanks!","['algebraic-geometry', 'class-field-theory', 'number-theory']"
684588,The number of grid points near a circle.,"There is a circle with center $(0, 0)$ and radius $r$. Let $n$ be the number of grid points inside or on the circle that at least one of its neighboring (up, down, left, right) grid points is outside the circle. With my computer, I got some $r-n$ pairs:
$$
\begin{array}{c|lcr}
r&n\\
\hline
1&4\\
2&8\\
3&16\\
4&20\\
5&28\\
10&56\\
10^2&564\\
10^3&5656\\
10^4&56568\\
10^5&565684\\
10^6&5656852\\
10^7&56568540\\
10^8&565685424\\
10^9&5656854248\\
10^{10}&56568542492\\
\end{array}
$$ And I found that $$\lim_{r\to\infty}\frac nr\approx5.6568542\approx4\sqrt2$$ My question is: how to prove the following equation? $$\lim_{r\to\infty}\frac nr=4\sqrt2$$","['analytic-geometry', 'number-theory']"
684637,Minimum value of the function $\sqrt{(1+1/m)(1+1/n)}$,"If $m, n$ are positive real variables whose sum is a constant $k$, then what is the minimum value of $$\sqrt{\bigg(1 + \frac{1}{m}\bigg)\bigg(1 + \frac{1}{n}\bigg)}$$","['optimization', 'multivariable-calculus', 'inequality']"
684638,Finding the inverse of a matrix using a series,"I want to find the inverse of the matrix $A$ given by: $ \left( \begin{array}{cc}
1 & -\epsilon  \\
\epsilon & 1  \\
  \end{array} \right) $ where $|\epsilon|$ $< 1$ (although I do not know how to use this yet) by finding the matrix $B$ such that $A = I-B$ or $B = I-A$. $B$ is $ \left( \begin{array}{cc}
0 & \epsilon  \\
-\epsilon & 0  \\
  \end{array} \right) $ And I want to find the inverse by summing the series $I + B + B^2 + B^3 . . .$ this series doesn't converge (unless we use the fact about the absolute value given, but I don't know what to do with that here).","['vector-spaces', 'matrices', 'linear-algebra']"
