question_id,title,body,tags
2808686,"If $2 (\cos \alpha-\cos\beta)+\cos\alpha\cos\beta=1$, then $p\tan\frac{\alpha}{2}+q\tan\frac{\beta}{2}=0$ for which choices of $p$ and $q$?","Let $\alpha$ and $\beta$ be nonzero real numbers such that $2 (\cos \alpha-\cos\beta)+\cos\alpha\cos\beta=1$. Then which of the following is/are true ? a)  $\sqrt3\tan(\frac\alpha2)+\tan(\frac\beta2)=0$ b)  $\sqrt3\tan(\frac\alpha2)-\tan(\frac\beta2)=0$ c)  $\tan(\frac\alpha2)+\sqrt3\tan(\frac\beta2)=0$ d)  $\tan(\frac\alpha2)-\sqrt3\tan(\frac\beta2)=0$ I solved the equation by converting all cosines to half angles and the answers options c) and d), seems pretty straightforward from there. But, when the question was asked in an engineering entrance exam (JEE ADVANCED  2017), the question was given as BONUS that would mean either the question was not complete, was wrong or might be some other mistake in understanding the question. I do not understand what was the problem. Can someone help me point out the ambiguity or mistake in the question or give a solution that differs from mine.","['trigonometry', 'education']"
2808775,Confusion with weak Nullstellensatz,"Let $X$ be an affine algebraic variety of $k^{n}$, $k$ algebraically closed. $I(X)$ the ideal of the polynomials that vanish in $X$, prove that if $I(X)\not = (1)$, then $X$ is not empty. What is the problem with taking $f\not \in I(X)$ and saying that by definition exists $x\in X$ with $f(x)\not = 0$, in particular exists $x\in X$?","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2808818,Relationship between moment generating functions,"I am trying to work out the moment generating function of a particular probability distribution $\rho(x)$ which is an even function. This means that all the odd moments are zero, so
$$M(s)= \mathbb{E}[e^{sX}] = \sum_{n=0}^\infty \frac{s^{2n} \mu_{2n}}{(2n)!}$$
where $\mu_n$ is the nth moment. For my problem, it turns out that working out the above function is difficult and it is easier to work out
$$\tilde{M}(s) = \mathbb{E}[e^{sX^2}] = \sum_{n=0}^\infty \frac{s^{n} \mu_{2n}}{n!}$$
which can be viewed as the generating function of the even moments. It seems to me that there ought to be a simple relationship between $M$ and $\tilde{M}$. If I have $\tilde{M}$ how can I recover $M$? The reason I would like to have $M$ is because it can (in principle) be inverse Laplace transformed to recover $\rho$, whereas I am not aware of any inversion formula for $\tilde{M}$. Update: Using the fact that the Fourier transform of a Gaussian is also a Gaussian I found that
$$\tilde{M}(-s) = \frac{1}{\sqrt{4 \pi s}} \int_{-\infty}^\infty M(it) e^{-t^2/4s} dt$$
However this is not really what I want since I want to go from $\tilde{M}$ to $M$ not the other way round. Also it is too general since it doesn't rely on $M$ being even.","['real-analysis', 'probability', 'statistics', 'probability-theory']"
2808852,Small contractions as blow ups,"I am trying to learn a bit about birational morphisms: $f: X\rightarrow Y$, between (projective) normal varieties. In particular, it is well known that every such morphism is a blow-up (e.g Hartshorne, Algebraic Geometry Theorem 7.17) Suppose $f$ is a contraction, i.e., $f$ has connected fibers. The situation when the exceptional set of $f$ has codim $1$ 
(divisorial contraction) is very different from the case when the exceptional set has codimension greather than or equal to two (small contraction). In particular, $f$ can only be a small contraction if $Y$ is not $\mathbb{Q}$-factorial (e.g Kollar, Mori, Birational Geometry , Corollary 2.63). I am wondering about the converse, i.e., suppose that $Y$ is not $\mathbb{Q}$-factorial is it then true that there exists an $X$ as above and a small contraction $f:X\rightarrow Y$? My naive idea is that blowing up a weil-divisor which is not $\mathbb{Q}$- Cartier ""should"" produce a small contraction. Is this true? Questions: Is the blowing up at a Weil non $\mathbb{Q}$-Cartier divisor a small contraction? Is there (another) general recipe starting from a singular enough $Y$ and blowing up $Z\subset Y$ that will always give a small contraction? In general, in terms of $f$ as a blow up at $Z$ in $Y$ how can I tell if $f$ is small or not?","['singularity-theory', 'algebraic-geometry', 'birational-geometry']"
2808863,"Taking the derivative of $A(t)\,x(t).$","Can someone tell me how to interpret $\dfrac{d}{dt}(A(t)\,x(t))$ for some matrix $A$ and a vector $x$? I can't find this sort of derivative anywhere.","['multivariable-calculus', 'vector-analysis']"
2808876,"If a finitely-generated ideal is prime, does it follow that the generators are prime?","If a finitely-generated ideal is prime, does it follow that the generators are prime? This is clearly true if it is generated by $1$ element, but is it true for any $n \ge 2$? If not, are there prime ideals generated by elements, none of which are prime?","['abstract-algebra', 'ring-theory']"
2808886,Some functional analysis problem,"In my undergraduate functional analysis course, there is this $1$ problem that I am stuck on, and it goes: For $n = 1, 2, 3,...,$ let the functions $f_n : [0, 1] → [0, 1]$ satisfy $|f_n(x)−f_n(y)|≤|x−y|$ whenever
  $|x − y| ≥ \frac{1}{n}.$ Prove that the sequence ${f_n}$ has a uniformly convergent subsequence. How do I start this proof? Can anyone give me some leads? My Solution: If we let some function $\phi_n$ be continuous, which equates to $f_n$ at some $i/n$ for $i = 0,1,2,3,...,n.$ Denote the graph of $\phi_n$ be a line segment [(i-1)/n,i/n].Then it is not hard to see that $\phi_n \in [0,1]$ implies the $\phi_n$'s are bounded (uniformly).Using the assumption given in the problem regarding $f_n,$ we can calculate the absolute values of the slopes of $\phi_n$'s on the intervals:
$$\bigg|\frac{\phi_n\big(\frac{i}{n}\big)-\phi_n\big(\frac{i-1}{n}\big)}{1/n}\bigg|=\bigg|\frac{f_n\big(\frac{i}{n}\big)-f_n\big(\frac{i-1}{n}\big)}{1/n}\bigg|\le 1$$ If $0 \le k/n \le x\le y \le (k+1)/n \le 1$, then it immediately follows that $|\phi_n(x)-\phi_n(y)|\le |x-y|.$ If on the other hand we consider the interval, $0\le x\le k/n \le m/n \le y \le 1,$ then we have the following computations: 
$$|\phi_n(x)-\phi(y)| \le \bigg|\phi_n (x) -\phi_n\bigg(\frac{k}{n}\bigg)\bigg|+\sum^{m-1}_{p=k}\bigg|\phi_n \bigg(\frac{p}{n}\bigg) -\phi_n\bigg(\frac{p+1}{n}\bigg)\bigg|+\bigg|\phi_n \bigg(\frac{m}{n}\bigg)-\phi_n(y)\bigg|$$$$\le |x-y|$$ From here take the $\epsilon$ to be $\epsilon = \delta$, then $\{\phi_n\}$ is equicontinuous. From here we can apply the Arzela - Ascoli's theorem, some subsequence $\{\phi_{n_{j}}\}$ converges uniformly on $[0,1]$ to a limit function $\phi$. Now simply fix some $x\in [0,1]$ , then for each $j$, we let $x_j$ be a point in $[0,1]$ of the form $i/n_j$ for $i = 0,1,2,...,n_j$ such that $1/n_j\le |x-x_j|\le 2/n_j.$ Now we can conduct the final step: 
$$|\phi(x)-f_{n_j}(x)|\le |\phi(x)-\phi_{n_j}(x)|+|\phi_{n_j}(x)-\phi_{n_j}(x_j)|+|f_{n_j}(x_j)-f_{n_j}(x)|\le ||\phi-\phi_{n_j}||_{\infty}+\frac{2}{n_j}+\frac{2}{n_j}\rightarrow 0$$ Hence clearly $\{f_{n_j}\}$ converges uniformly on $[0,1]$ to $\phi.$",['functional-analysis']
2808908,computing the inversion of a matrix which is the sum of a Kronecker product and an identity matrix,"I am using Gibbs sampling to compute the posterior of $\mathbf{S}_{N\times K}$ $(N>>K)$ while I should calculate a Gaussian likelihood which its covariance matrix is given as $$\mathbf{P}_{K^2\times K^2}=\Big[\Big(\mathbf{S}\otimes\mathbf{S}\Big)^T\Big(\mathbf{S}\otimes\mathbf{S}\Big)+\gamma\mathbf{I}_{K^2}\Big]^{-1}$$ where $\otimes$ is a Kronecker product. I need to compute $\mathbf{P}$ for each entry update of $s_{nk}$ which is computationally very costly. $\mathbf{S}$ is a binary matrix and has this property $\sum_{i=1}^N \mathbf{s}_i^T\mathbf{s}_i$ where $\mathbf{s}$ is the rank one matrix.  I am wondering whether there is a rank one updates of $\mathbf{S}$ exist in order to compute efficiently the inverse of $\mathbf{P}$ when only one entry of $\mathbf{s}_i$ (the $i$ -th row of $\mathbf{S}$ ) is modified? A working example when there is no Kronecker product , if we have $$\mathbf{M}^{-1}=\mathbf{S}^T\mathbf{S}+\lambda\mathbf{I}_{k},$$ since the first term can be expressed as $\sum_{i}\mathbf{s}_i\mathbf{s}_i$ then calculating $\mathbf{S}_{-i,k}$ can be done by removing the influence of a single vector $\mathbf{s}_i$ is a rank-one update to $\mathbf{M}$ , so we have $$\mathbf{M}_{-i}=(\sum_{j\neq i} \mathbf{s}_j^T\mathbf{s}_j+\lambda\mathbf{I})^{-1},$$ then we can use Sherman–Morrison formula and we get $$\mathbf{M}_{-i}=( \mathbf{M}^{-1}-\mathbf{s}_i^T\mathbf{s}_i)^{-1}=\mathbf{M}-\frac{\mathbf{M}\mathbf{s}_i^T\mathbf{s}_i\mathbf{M}}{\mathbf{s}_i\mathbf{M}\mathbf{s}_i^T-1}$$ and we also have $$\mathbf{M}=\mathbf{M}_{-i}-\frac{\mathbf{M}_{-i}\mathbf{s}_{i}^T\mathbf{s}_{i}\mathbf{M}_{-i}}{\mathbf{s}_{i}^T\mathbf{M}_{-i}\mathbf{s}_{i}+1}$$ which leads to having $M^{-1}=\mathbf{M}_{-i}^{-1}+\mathbf{s}_i^T\mathbf{s}_i$ and $M_{-i}^{-1}=\mathbf{S}_{-i}^T\mathbf{S}_{-i}+\gamma\mathbf{I}$ . By computing the product of two vectors $\mathbf{s}_i^T\mathbf{s}_i$ at the $i$ -th row, one can basically update this matrix inversion. Is there any solution already out there for this problem? Any suggestion for computing inverse of $\mathbf{P}$ ( $\mathbf{P}_{-i}$ ) for just rank one update of $\mathbf{s}$ (deleting the $i$ -th row)?? Update : Is there any recursive method, some idea similar to this article that reduce the computation of matrix inversion here considerably?","['kronecker-product', 'sampling', 'covariance', 'inverse', 'linear-algebra']"
2808921,Domain of composite function with initially restricted domain,"$(23)$ If the function $f$ has the rule $f(x) = \sqrt{x^2-9}$ and the function $g$ has the rule $g(x) = x+5$ . $a. $ find integers $c$ and $d$ such that $f(g(x)) =\sqrt{(x+c)(x+d)}$ $b.$ state the maximal domain for which $f(g(x))  $ is defined For question $(23) \;b.$ , in the above , I am struggling with finding the domain. The answer states that $x\le8$ and $x\ge2$ . However, for $f(x)$ , you can’t have a negative square root over the real field so $x\le -3$ and $x\gt 3$ . So because x has already got this restricted domain, I had gotten the answer that $f(g(x))$ would mean $x\le$ and $x\gt3$ ... I know this is a very basic question but I just need some clarification. Thanks!","['calculus', 'functions']"
2808939,What is meant by this useage of the $\mathcal{O}$ notation in statistics,"I'm reading this statistics paper, and one of the theorems reads ( emphasis mine ) Theorem: Let $(\mathcal{X},\mathcal{A},P)$ be an arbitrary probability space, $\mathcal{F}$ a class of real-valued functions on $\mathcal{X}$ with $||f||_{\infty} \leq 1$. Let $(X_n)_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables drawn according to $P$, and $(P_n)_{n \in \mathbb{N}}$ the corresponding empirical distributions. Then there exists some constant $c > 0$ such that, for all $n\in \mathbb{N}$ with probability at least $\boldsymbol{1−\delta}$,
  $$\operatorname{sup}_{f \in \mathcal{F}} \left| P_n - Pf \right| \leq \frac{c}{\sqrt{n}} \int_0^{\infty}\sqrt{\operatorname{log} N(\mathcal{F}, \varepsilon, L_2(P_n))} d\varepsilon + \sqrt{\frac{1}{2n}\operatorname{log}\boldsymbol{\frac{1}\delta}}.$$ The paper then goes on to say that We can see that if $\int_0^{\infty}\sqrt{\operatorname{log} N(\mathcal{F}, \varepsilon, L_2(P_n))} d\varepsilon  < \infty$, then the whole expression scales as $\mathcal{O}(1/√n)$ They then show that indeed, the integral is finite. They conclude: ( emphasis mine ) Let $X$ be compact subset of $\mathbb{R}^d$ and $k(x,y) = \operatorname{exp}(−||x−y||^2/\sigma^2)$. Then the eigenvectors in Theorem 16 converge with rate $\boldsymbol{\mathcal{O}(1/\sqrt{n})}$ where Theorem 16 states [...] such that,
  $$||a_n u_n - u||_{\infty} \leq C' \operatorname{sup}_{f \in \mathcal{F}} \left| P_n - Pf \right|$$ which holds almost surely and $u_n$ are the eigenvectors mentioned in the preceeding quote. My question is what they mean by ""the eigenvectors converge with rate ${\mathcal{O}(1/\sqrt{n})}$"". In my eyes, the statement seems to imply that the eigenvectors converge almost surely with a rate of ${\mathcal{O}(1/\sqrt{n})}$, since the inequality in theorem 16 holds almost surely and nowhere do they mention that the convergence is meant to be in probabilty.
However, the theorem seems to only give us that
$$\mathbb{P}\left( \operatorname{sup}_{f \in \mathcal{F}} \left| P_n - Pf \right| > \frac{C}{\sqrt{n}} (1 + \operatorname{log}\frac{1}{\delta}) \right) \leq \delta,$$
which would give us that 
$$||a_n u_n - u||_{\infty} \in \mathcal{O}_p \left(\frac{1}{\sqrt{n}}\right),$$
i.e. the eigenvectors are stochastically bounded. Here i used the Wikipedia definition of $\mathcal{O}_p$. My guess is that one of the following explanations applies here: In this context it is customary to mean $\mathcal{O}_p$ when writing $\mathcal{O}$. Because in our context $\mathcal{X}$ is compact and $\mathcal{F}$ is a Glivenko-Cantelli class , by some magic the statement of the theorem holds almost surely and independent of $\delta$. Since, for Glivenko-Cantelli classes, convergence in probabilty imples almost surely convergence, i can see this somehow working out. However i don't understand why the rate of convergence in probabilty would translate directly to the rate of almost surely convergence. I made some other conceptual mistake. Thank you for helping!","['statistics', 'asymptotics']"
2808963,Solving $(y\cos x)dx+(\sin x +2)dy=0$,"Greetings my teacher gave me the following differential equation: $$(y\cos x)dx+(\sin x +2)dy=0$$ and told me it's an total differential equation and to solve in $y(0)=\frac{1}{2}$ so I have two possible solutions and I 
 dont know which one is correct. Let's denote $P(x,y)$ to be the coefficients of $dx$ and $Q(x,y)$ to be the coeffiecients of dy. Firstly using: $$u(x,y)=\int_{x_0}^x P(t,y_0)dt +\int_{y_0}^y Q(x,t)dt$$ gives $$u(x,y)=\int_0^x \frac{1}{2}\cos{t}dt+\int_{\frac{1}{2}}^y \sin{x}dt=\frac{1}{2}\sin{x}+y\sin{x}-\frac{1}{2}\sin{x}=y\sin{x}$$ And the second method would be to use: $$u(x,y)=\int_{x_0}^x P(t,y)dt +\int_{y_0}^y Q(x_0,t)dt$$ which gives: $$u(x,y)=\int_0^x y\cos{t}dt+\int_{\frac{1}{2}}^y (\sin{0}+2)dt=y\sin{x}+2y-1$$ Also one can notice that is in fact also a separable differential equation.
Which one is correct and what shall I use? Edit: The only problem was that I forgot to put $+2$ in the second integral for the first method..",['ordinary-differential-equations']
2808991,"If $f$ is two times differentiable on $[a,b]$ and attains maximum at $x_0\in (a,b)$, prove $f''(x_0)\leq 0$.","Let $f:[a,b] \rightarrow \mathbb{R}$ be a two times differentiable function, that attains its maximum value at some $x_0\in (a,b).$ Prove that $f''(x_0)\leq 0.$ Attempt. By defintion, we have $\displaystyle f''(x_0)=\lim_{x \rightarrow x_0}\frac{f'(x)-f'(x_0)}{x-x_0}$, where $f'(x_0)=0$ by Fermat's theorem (since $x_0$ is an interior point of diferentiability of $f$), so: 
$$\displaystyle f''(x_0)=\lim_{x \rightarrow x_0^+}\frac{f'(x)}{x-x_0}.$$
It is enough to prove that for some $\delta>0$ we have that $f'(x)\leq 0$ for $x\in (x_0,x_0+\delta).$ This is the point I am stuck. Thank you in advance for the help!","['derivatives', 'real-analysis', 'calculus', 'limits']"
2809048,Evaluate $\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}}$,One of friend gave me a question today to solve which is as follows $$\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}}$$ In spite of much efforts I couldn't solve it and so I asked him to check whether the question was correct or was it this one $$\sum_{r=1}^{\infty} \frac {r}{r^4+r^2+1}$$ I thought the question would be this one because the terms inside the root can be telescoped in absence of root. And indeed I was right. The question was as I expected the latter one. But even after that I thought about whether the first question containing the square root  could also be solved or not. For just checking out the convergence of the sequence I tried the ratio test but it wasn't quite helpful. Then I tried using the integral test and indeed $$\lim_{t\to \infty} \int_{1}^{t} \sqrt{\frac {x}{x^4+x^2+1}} dx$$ this integral converges to $1.80984$ according to Wolfy. Upon lot of efforts too I am not an able to solve the first summation (with the square root terms) .  Can someone please lend me some help over this problem.,"['limits', 'sequences-and-series', 'calculus', 'integration', 'convergence-divergence']"
2809056,Simplifying $\frac16\cos(\frac12x)\cos(\frac32x)-\frac{1}{2}\cos^2(\frac12x)+\frac12\sin^2(\frac12x)+\frac16\sin(\frac12x)\sin(\frac32x)$,I'm solving a differential equations. I got the final answer which is $$\frac{1}{6}\cos\left(\frac{1}{2}x\right)\cos\left(\frac{3}{2}x\right)-\frac{1}{2}\cos^2\left(\frac{1}{2}x\right)+\frac{1}{2}\sin^2\left(\frac{1}{2}x\right)+\frac{1}{6}\sin\left(\frac{1}{2}x\right)\sin\left(\frac{3}{2}x\right)$$ The given answer in book is $$-\frac{1}{3}\cos x$$ I'm sure they are both equal. I don't know how to simplify my answer to the given one. Hope someone can help out.,"['algebra-precalculus', 'trigonometry']"
2809057,Composition of vector bundles is only a fibre bundle,"On page 101 of Lang's ""Differential and Riemannian Manifolds"", he introduces fibre-bundles because composing vector bundles does not give vector bundles. To be more precise, given a vector bundle $p : E \rightarrow X$ and the tangent bundle $p: TE \rightarrow E$, he states that the composite $f =  p \circ \pi$ is ""only a fibre bundle over $X$, a fact which is obvious by picking trivializations in charts"". Looking at the relevant trivializations, this is not clear to me at all. Why is $f$ not a vector bundle in general?","['vector-bundles', 'differential-geometry']"
2809087,Integral $\int_{-2}^0 \frac{x}{\sqrt{e^x+(x+2)^2}}dx$ [duplicate],"This question already has an answer here : Calculate $ \int_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}} dx $ (1 answer) Closed 5 years ago . I am trying to evaluate $$\int_{-2}^0 \frac{x}{\sqrt{e^x+(x+2)^2}}dx$$ So far I had no succes using trig substitution or integration by parts, also some random substitution like $x=2t$ and moved the exponential to the numerator, but I am stuck. Could you perhaps give me an idea? (this is a college admission problem)",['integration']
2809095,Find derivatives with respect to parameter or initial condition,"Find stated derivatives with respect to parameter or initial condition: a) $x'=x+\mu(t+x^2), x(0)=1 ;$ find $\frac{\partial x}{\partial \mu}|_{\mu=0}$ , b) $x'=2t+\mu x^2, x(0)=\mu -1;$ find $\frac{\partial x}{\partial \mu}|_{\mu=0}$ , c) $x'=x+x^2 +tx^3, x(2)=x_0 ;$ find $\frac{\partial x}{\partial \mu}|_{\mu=0}$ , Please help me with at least one of them, and I will try to make the others. I really don't know what to do in here.","['ordinary-differential-equations', 'analysis']"
2809107,The (orbifold) space of symmetric complex matrix,"Let $K$ be a rank-2 symmetric complex matrix, such that the transpose
$K^T=K$ is itself. Let $V$ be a rank-2 unitary matrix, $V \in U(2)$. Consider the identification between any K and K' of any rank-2
symmetric complex matrix,
$$
K\sim K',
$$
if it satisfies
$$
V^T K V =K',
$$
for any $V \in U(2).$ Originally, we can parametrize $K$ as
$$
K=\begin{pmatrix}k1 & k\\ k& k2\end{pmatrix},
$$
with $$k1,k, k2 \in \mathbb{C}.$$ 
There are 6 real degrees of freedom in total. After the identification of the $K\sim K'$, constrained by the $V\in
U(2)$ which has 4 real degrees of freedom. So totally there should be at least 2 real degrees of freedom left for
the $d \geq 2$-dimensional space of the $K\sim K'$. (But $d \geq 2$
could be more due to the consideration of stabilizer.) question: What is the real dimension of the new space of $K$ (under the
  $K\sim K'$ and $V^T K V =K'$, for any $V \in U(2)$ condition)? How do we parametrize this new space of $K$ in terms of a rank-2
  matrix (mod out the redundancy under the $K\sim K'$ and $V^T K V =K'$,
  for any $V \in U(2)$ condition)? (p.s. This space may be a called an orbifold space(?). i.e. The (orbifold) space of symmetric complex matrix after mod out a relation identifying a unitary matrix.)","['manifolds', 'differential-geometry', 'algebraic-topology', 'group-theory', 'linear-algebra']"
2809132,The unit group as a homomorphic image in a semigroup,"In this paper , the author states in the first sentence: Among the homomorphic images of a semigroup (= a set closed with respect to an associative binary operation) there is at least one group, namely the unit group $I$. How is this meant, in what sense arises the unit group as a homomorphic image? If $I$ is the group of invertible elements, if $S - I$ is an ideal, even the Rees factor semigroup introduces a zero element in the image, hence it could not be a group. So how does $I$ arises as a homomorphic image?","['abstract-algebra', 'semigroups', 'group-theory']"
2809144,Arzela Ascoli Theorem for compactness of a set,"Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R}  \right\} $ , where $f$ is continuous function on $K.$ Its metric is given by $d(f,g)=\left \| f-g \right \|_u = \sup\left \{ \left | f(x)-g(x) \right | :x\in K\right \}$. Let $B=\left \{ f\in C([0,1]):f\in C^1 ((0,1)),f(0)=0,\left | f'(x) \right | \leq 1\right \}$. Show that $B$ is compact subset of $C([0,1])$. (My attempt) I tried to apply Arzela-Ascoli theorem. If I show $B$ is closed,pointwise bounded and equicontinuous on $[0,1]$, then $B$ is compact. I used Mean Value Theorem to show that $B$ is uniformly bounded and equicontinuous on $[0,1]$. But showing the closeness of $B$ was hard for me. Let $\left \| f_n-f \right \|_u\rightarrow 0$ for $f_{n}\in B$.
Then, $\left \| f \right \|_u\leq  \left \| f-f_n \right \|_u+\left \| f_n \right \|_u \leq 1+\epsilon $ for arbitary $\epsilon > 0$. But, I don't know how to connect with 
$\left | f'(x) \right |\leq 1$ and $f(0)=0$.","['functional-analysis', 'real-analysis', 'analysis']"
2809149,Example of discontinuous subharmonic function,"Let $U$ be an open and connected subset of $\mathbb C$. $f:U \to [-\infty, \infty)$ is said to be subharmonic if its upper semi continuous and satisfy local mean value inequality. I want example of discontinuous subharmonic function. Even standard example of $log |f|$ where $f$ is holomorphic is also continuous. Thanks.","['complex-analysis', 'potential-theory', 'partial-differential-equations']"
2809215,Algebraic Torus: Etymology,"I was wondering if anyone might be able to tell me why algebraic tori are called specifically algebraic tori ? I find it difficult to see exactly how ""an algebraic group that can be described as a direct product of finitely many multiplicative groups"" is in any sense similar to the more standard notion of a doughnut. Any and all help is appreciated.","['field-theory', 'extension-field', 'group-theory', 'algebraic-geometry']"
2809248,"AIME 1997, problem 12","Please have a look : Problem The function $f$ defined by $f(x)= \frac{ax+b}{cx+d}$ , where $a$ , $b$ , $c$ and $d$ are nonzero real numbers, has the properties $f(19)=19$ , $f(97)=97$ and $f(f(x))=x$ for all values except $\frac{-d}{c}$ . Find the unique number that is not in the range of $f$ . The solution can be found here It states, without proof, that if we have the functional equality: $$\frac{px+q}{rx+s}=x$$ then $r=q=0.$ At the first solution, line $3$ why does it have to be $q = r = 0$ ? [ I understand that the opposite is true, i.e. , if $q = r = 0 $ then the fraction reduces to $x$ when $p = s$ ]",['algebra-precalculus']
2809254,Relationship between rank of binary matrix and the NOT operator,"Let $A$ be a binary matrix. I'm looking for any information about the relationship between the rank of $A$ and the rank of NOT$(A)$, where NOT replaces all $0$s with $1$s, and vice-versa. What I know These ranks can sometimes be equal. For example, applying the NOT operator to the identity matrix returns another full rank matrix. They can sometimes not be equal. For example, the matrix 
\begin{equation*} A=
\begin{bmatrix}
1 & 0 \\
1 & 1
\end{bmatrix}
\end{equation*}
has rank $2$, but 
\begin{equation*} \text{NOT}(A)=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}
\end{equation*}
has rank $1$. My questions Are there known relationships between the two ranks?","['binary', 'matrix-rank', 'linear-algebra']"
2809270,periodic solutions with constant period,"Recently I am facing a problem which leads to the following type of ODE
$$x'' + x + f(x') = 0.$$ It is obvious that when $f$ is a constant valued function, the solutions are all periodic and have a common period $2\pi$. I want to know if there is a nonconstant smooth function $f$, such that (1) the solutions are all periodic, and (2) they share a common period $T$. I have obtained many examples which satisfy the condition (1), for example $f(x)=(\sin x)^n$ for even integer $n>0$.  But none of them satisfy (2).","['ordinary-differential-equations', 'dynamical-systems']"
2809289,"Positive Integer Multiple of Convergent, Decreasing Sequence Can Be Made Arbitrarily Small","I'm working on the following problem, and for some reason I can't seem to grasp it intuitively: Let $\{ a_n \}_{n=1}^{\infty}$ be a strictly decreasing sequence of positive numbers. Assume $\sum_{n=1}^{\infty} a_n$ converges. Prove that for every $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $n > N$, $(n-N)a_n < \epsilon$. Is it correct to say that we are proving that $a_n$ eventually becomes so small that even a large number of copies of the $n^{th}$ term is arbitrarily close to 0? My attempt: First, note that as $\sum_{n=1}^{\infty} a_n$ converges, it must hold that the sequence of partial sums converges, and thus is Cauchy. That is, if $s_n = \sum_{k=1}^{n} a_k$, then for fixed $\epsilon > 0$, $\exists N$ such that
$$ |s_n - s_m| < \epsilon \qquad \forall n, m > N.$$
Equivalently, w.l.o.g. assume that $n > m$. Then, since $a_n$ is a sequence of positive numbers, 
\begin{align*}
|s_n - s_m| &= \bigg| \sum_{k=1}^n a_k - \sum_{k=1}^m a_k \bigg| \\
&= \sum_{k=m+1}^n a_k \\
&< \epsilon \qquad \forall n > m > N \\ 
\end{align*}
Now, since $a_n$ is decreasing, 
\begin{align*}
(n-N)a_n &= \sum_{N+1}^n a_n \\
&= a_n + a_n + \dots + a_n \\
&< a_{N+1} + a_{N+2} + \dots + a_n \qquad n>m\geq N+1 \\
&= \sum_{k=N+1}^n a_k \\
&< \epsilon  \qquad \forall n > N \\
\end{align*}
Since $\epsilon > 0$ was arbitrary, $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ such that 
$$ (n-N)a_n < \epsilon \qquad \forall n > N.$$ Is this a valid proof? I feel as if I might be overcomplicating things.","['analysis', 'proof-verification']"
2809344,Interpret $\prod_{n=3}^\infty 1- \frac{1}{n\choose 2}$ as a probability,"Find the infinite product $$\prod_{n=3}^\infty 1- \dfrac{1}{n\choose 2}$$ It is quite simple to expand and cancel alternating terms to obtain $\frac13$ as the answer.
However, notice how the expression looks the product of ""not"" probabilities of some event iterated over different sample spaces. All I want is a simple probabilistic proof of this evaluation. One can do this by finding a bijection between this expression and that event. For example, I thought of the situation as having bags having $2,3,4,\dots$ balls respectively. Now we select $2$ balls from each bag one by one, and write the probability of not coming up with a specific pair of balls from each bag. This is exactly equal to the given expression. Update : Check @Ian's answer for such a bijection. Now the problem remains to connect it to 1/3.","['combinatorics', 'infinite-product', 'probability', 'combinatorial-proofs']"
2809384,Restrictions on Laplacian eigenvalues in 1 dimension,"Which finite sequences $0=\lambda_0<\lambda_1<\dots<\lambda_k$ can be obtained as the first $k+1$ eigenvalues of the Laplacian operator $\Delta_g$ on the circle $S^1$? Of course the metric $g$ is allowed to be arbitrary. Thoughts: In this case, the metric is specified by a positive function $S^1\to\mathbb{R}$, and by expanding this as a Fourier series, the Laplace equation $\Delta f=\lambda f$ becomes an infinite-dimensional matrix equation for the fourier coefficients of $f$, however, I am not sure how useful this viewpoint is. By way of motivation, it is a theorem of de Verdiere that any such sequence can be realized on any closed manifold of dimension $\ge 3$ by appropriate choice of metric .","['laplacian', 'riemannian-geometry', 'differential-geometry', 'partial-differential-equations']"
2809428,"$p|a^{p-1}-1$, can I prove that $p\nmid a$?","Trying to understand the connections between concepts. If p is prime and a is a positive integer, then if $p|a^{p-1}-1$, can I prove that $p\nmid a$?",['number-theory']
2809429,Example Sequence $\{ b_n \}$ with $\lim_{n \rightarrow \infty} nb_n = 0$ but $\sum_{n=1}^{\infty} b_n$ diverges.,"As the title states, I'm looking for an example of a strictly decreasing sequence of positive numbers with the properties that 
$$ \lim_{n \rightarrow \infty} nb_n = 0$$ 
but 
$$ \sum_{n=1}^{\infty} b_n $$ 
diverges. My efforts have been unsuccessful so far. I know that nothing of the form $$ b_n = \frac{1}{n^p} $$ works, as $\lim_{n \rightarrow \infty} nb_n = 0$ if $p>1$, also implying that the series will converge via p-test. I've also tried more creative sequences like
$$ b_n = \frac{\sin(\frac{1}{n})}{n} $$
but still no luck. More than a specific example, is there a certain strategy I should employ to find such an example? I was thinking the sequence must go to zero must faster than $n$ goes to infinity, but not fast enough for the series to converge.","['sequences-and-series', 'analysis']"
2809448,Asymptotics of $e^{-n}\sum_{k=0}^{n-1}\frac{n^k}{k!}\cdot\frac1{n-k}$,"Letting 
$$
a_n = \sqrt{2\pi n}\cdot e^{-n}\sum_{k=0}^{n-1}\frac{n^k}{k!}\cdot{\frac1{n-k}},
$$
does anyone know of a simple asymptotic equivalent for $a_n$? Numerical experimentation suggests that
$$
a_n \stackrel{?}\sim \tfrac12\log n.
$$
If we get rid of the $\frac1{n-k}$, there is a simple answer, because
$$
\lim_{n\to\infty} e^{-n}\sum_{k=0}^{n-1}\frac{n^k}{k!}=\frac12.
$$
as shown here . Throwing in the factor $\frac1{n-k}$ seems to change the asymptotics entirely. The context is that $a_n$ is result of applying Stirling's approximation to $\sum_{k=1}^n \frac{n_{(k)}}{kn^k},$ which is the expected number of ""cycles"" that a random function on a set of $n$ elements to itself has, where a cycle is a set of distinct numbers $\{x_1,x_2,\dots,x_k\}$ for which $f(x_i)=x_{i+1}$. If you replace ""random function"" with ""random bijection,"" the expected number of cycles is about $\log n$, so if my guess is correct then random functions tend to have half as many cycles as permutations.","['generating-functions', 'combinatorics', 'binomial-coefficients']"
2809474,"When a matrix, $M$ equals $M^2$, why is this property true?","While we were looking at idempotent matrices, my teacher brought up some properties about them, specifically when the square of a matrix equals itself, the matrix's null space equals the column space of the matrix minus its respective identity matrix. Is this something that should be trivial, I've been trying to convince myself this is true but I can't seem to reason this to myself.","['matrices', 'linear-algebra', 'vector-spaces']"
2809488,$\sin(k\pi/n)$ an integer multiple of $\sin(\pi/n)$,"Is there any positive integers $(n,k) \neq (6,3)$ with $n\geq 6$ and $2\leq k\leq n/2$ such that:
$$\frac{\sin(k\pi/n)}{\sin(\pi/n)} \in \mathbb{N} ?$$
For $n\leq 57$ the answer is no (by computation), but $(n,k)= (57,22)$ is very close with $\displaystyle \frac{\sin(k\pi/n)}{\sin(\pi/n)} \simeq 17.00035063$.","['trigonometry', 'calculus', 'cyclotomic-fields']"
2809505,prove that $S_{\tau \land n} \to S_{\tau}$ in $L^1$ for a random walk with $E\tau^{1/2} < \infty$,"Let $X_i$ be a sequence of iid $L^2$ RVs with $EX_i = 0$ and define the martingale $S_n = \sum_1^n X_i$. I want to show that if $\tau$ is a stopping time and $E\tau^{1/2} < \infty$, then $ES_{\tau} =0$. I have been given the hint that if $Y_n$ is an $L^2$ martingale, then $E(\sup_n | Y_n|) \leq 3E(\sqrt{A_{\infty}})$, where $A = \langle Y \rangle$ is the quadratic variation process of $Y$, i.e. $A_0 =0$, $$A_n = \sum_{k=1}^nE(Y_k^2 \mid \mathcal{F}_{k-1}) - E(Y_{k} \mid \mathcal{F}_{k-1})^2$$ and $\lim_n A_n =A_{\infty}$. My work: Following the idea in Doob's OST, clearly have that $S_{\tau \land n}$ is a martingale and therefore $E[S_{\tau \land n}] = E[S_0] = 0$. Since $\tau< \infty$ a.s, I need to show $\lim_n E[S_{\tau \land n}] = E[S_{\tau}]$ (that is, $S_{\tau \land n} \to S_{\tau}$ in $L^1$). I have tried to follow this hint, and noted that since $X_i$ are $L^2$, $S_n = \sum_{i=1}^n X_i \in L^2$. Also, 
$$\begin{aligned}
A_n &= \langle S \rangle_n =\sum_{k=1}^nE(S_k^2 \mid \mathcal{F}_{k-1}) -0\\
& = E \left(\sum_{k=1}^nX_k^2 + 2\sum_{i<j \leq n} X_iX_j \mid \mathcal{F}_{k-1} \right)\\
& = E(X_n^2) + \sum_{k=1}^{n-1}X_k^2 + 2 \sum_{i<j<n}X_iX_j \\
&= E(X_n^2)+S_{n-1}^2
\end{aligned}$$
I don't know that this has a limit. If I use the hint on the $X_i$s, I get that $$\langle X\rangle_n = E(X_n^2) \implies E(\sup_n|X_n|) \leq 3(X_{\infty}^2)$$ which I don't think is helpful. I'm not sure exactly which route I should be going down to show convergence, there are so many theorems on it with slightly different conditions. I'm so lost, any ideas would be much appreciated, thanks!","['martingales', 'probability-theory', 'convergence-divergence', 'stopping-times']"
2809539,Using summation by parts on a combination,"I am trying to expand the series $\sum_{k=1}^{n}\binom{n}{k}$ when $n$ is a integer greater then zero, by using summation by parts. I am using the following definition of summation by parts.\begin{equation} \sum_{k=1}^{n}\Delta V(k)U(k)=V(k)U(k)|_{k=1}^{n+1}-\sum_{k=1}^nV(k)\Delta U(K)\end{equation}When I was expanding the series, I let $U(k)=\binom{n}{k}$ and $\Delta V(k)=1$. My result was the following.
\begin{equation}\sum_{k=1}^n\binom{n}{k}=k\binom{n}{k}|_{k=1}^{n+1}-\sum_{k=1}^nk\binom{n}{k}\left(\frac{n-2k-1}{k+1}\right)\end{equation}
When I evaluate the first term on the right side I find that I have the factorial of negative one. For instance, when I let $n=1$ I get the following result. 
\begin{equation}\sum_{k=1}^1\binom{1}{k}=k\binom{1}{k}|_{k=1}^2-\sum_{k=1}^1k\binom{1}{k}\left(\frac{1-2k-1}{k+1}\right)\end{equation}
\begin{equation}1=2\binom{1}{2}-1-(-1)\end{equation}
\begin{equation}1=\frac{1}{(-1)!}\end{equation}
By the last statement the factorial of negative one is implied to be one, but the gamma function says that the factorial of negative one is infinity.  Did I make a mistake somewhere or is this just a case that the summation by parts doesn't hold?","['discrete-mathematics', 'combinatorics', 'summation', 'discrete-calculus', 'summation-by-parts']"
2809601,Why does the Method of Characteristics matter?,"I'm learning the method of characteristics from Evans' book on PDE (Chapter 3). From what I understand, a characteristic curve of a PDE is a curve on which the solution $u$ does not vary with respect to time. I.e, for all points $(X(t),t)$, with $X(0) = x$ that $u(X(t),t)  = u(x,0)$. But I have a couple questions. First, why do we care about characteristics? Intuitively, it seems like we can construct curves that ""cover"" all of spacetime, and then piece them together to get a global solution. But how can we apply the method to solve first order equations? And what does it mean when the curves intersect? It seems like the solution loses smoothness. Is this what the whole theory of shocks is about?","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
2809632,Proof that every bounded linear operator between hilbert spaces has an adjoint.,"As a practice exercises(not an assignment question) for one of the papers I am doing currently at university we are asked to show the following; I $T:H \rightarrow K$ is a bounded linear operator from between two Hilbert spaces. Show that there exists a unique bounded linear operator $T^*:K\rightarrow H$ such that $$
\langle Th,k\rangle = \langle h,T^*k \rangle\ \ \ \ \forall h\in H,\ \ \forall k\in K.
$$ Uniqueness is easy, for if there existed $S,P \in \mathcal{L}(K,H)$ satisfying this property then we would have $$
\langle h, Pk-Sk\rangle =0 \ \ \ \ \forall h \in H \ \ \ k\in K
$$
in particular for $Pk -Sk \in H$ we would have $$
\langle Pk-Sk, Pk-Sk\rangle = 0 \ \ \ \forall k\in K
$$
which means $Pk= Sk$ for all $k\in K$ so they are the same. Proof of existence We start by fixing $k \in K$. Now we define the linear functional $L_k : H \rightarrow \mathbb{F}$ by $$
L_k(h) := \langle Th,k \rangle, \ \ \ \  h \in H
$$
($L_k$ is linear by linearity of $T$ and linearity of inner products and bounded by Cauchy-Schwarz.) Now by the Riesz - Representation Theorem we know there exists a unique $v \in H$ such that $$
L_k(h) = \langle h,v \rangle , \ \ \ \ \  \forall h \in H.
$$ Notice that the $k \in K$ that we fixed was arbitrary so for each $k \in K$ there exists a unique $v_k \in H$ such that $$
\langle Th,k\rangle = L_k(h) = \langle h,v_k \rangle , \ \ \ \ \  \forall h \in H.
$$ Now we define the function $T^* :K \rightarrow H$ where $$
T^* (k) := v_k, \ \ \ \ k \in K.
$$ Now we claim that $T^*$ is linear and bounded. We start by showing that $T^*$ is linear. So we fix $k,g \in K$ and $\lambda \in \mathbb{F}$, the case where $k = g = 0$ is trivial so we assume $k,g \neq 0$, then we need to show that $$
T^*(\lambda k +g) = \lambda T^*k +T^*g.
$$ Or by the uniqueness of $T^*(k)$ for all $k \in H$ it is enough to show show that $$
\langle Th,\lambda k +g \rangle = \langle h, \lambda T^*k +T^*g \rangle, \ \ \ \ \forall h \in H
$$
Now for arbitrary $h \in H$ we have $$
\langle Th, \lambda k +g \rangle = \bar{\lambda}\langle Th,k \rangle + \langle Th,g \rangle = \langle h, \lambda T^*k\rangle +\langle h, T^*g \rangle = \langle h, \lambda T^*k +T^*g \rangle
$$ as needed for linearity. Now for boundedness we notice for $k \in K$, (the case where $k = 0$ is trivial so we may assume $ k \neq 0$) $$
||T^*k||^2 = \langle T^*k,T^*k \rangle = \langle k , T(T^*k) \rangle \leq ||k||\ ||T(T^*k) || \leq ||k||\ ||T||_{\text{op}}\ || T^*k ||
$$ By simple rearranging it follows that $$
||T^*k|| \leq ||T||_{\text{op}}\ ||k||.
$$ As a bonus we also notice that the from infemum characterization of the operator it follows that $$
||T^*||_{\text{op}}\leq ||T||_{\text{op}}.
$$ QED","['functional-analysis', 'hilbert-spaces', 'linear-transformations', 'adjoint-operators']"
2809652,How do you find all solutions to the matrix equation $XAX=A^T$?,"I was recently asked to solve a problem in a programming interview involving word squares , and on further reflection I realized it could be recast as a linear algebra question. Since my solution has a worst-case time complexity of $O(n!)$ if $n$ is the width of $A$, out of personal curiosity I'm trying to find a better solution, as measured by worst-case time complexity. Given an $n$ by $n$ square matrix $A$ over the reals with $n\geq 2$, is there an $n$ by $n$ permutation matrix $X$ satisfying $$XA=(XA)^T?$$ Since $(XA)^T=A^TX^T$ and $X$ is a permutation matrix, we have $X^T=X^{-1}$, hence this is equivalent to asking if there are any solutions to $$XAX=A^T$$ which are permutation matrices. Naively, I was hoping there would be a unique solution, but numeric techniques strongly indicate a wide variety of solutions, even when a satisfactory permutation exists. Unfortunately, it doesn't seem that solutions form a linear subspace. For example, if $X$ is a solution, then $(aX)A(aX)=a^2A^T$ for any real $a$, so most multiples of $X$ are not solutions for most matrices $A$. Similar problems exist for sums of solutions. With this in mind, I'm not quite sure which method one might use to enumerate solutions. If they don't form a linear subspace, we can't just enumerate a basis. Any partial solutions, theorems, pointers in the right direction, or related problems would be helpful. I'm not familiar with what seems to be a nonlinear matrix equation (if that's the right terminology). Update This seems to be a special case of something called the algebraic Riccati equation , as indicated in another question . The techniques I have found so far emphasize one of two approaches: Algorithms for finding any solution from a given starting guess. Algorithms for finding the unique stabilizing solution, should one exist. Given the control theoretic background for those approaches, they typically assume that $A^T$ is symmetric. In my case, if $A^T$ were symmetric I could simply take $X$ to be the identity and be done, and that assumption oversimplifies the problem. Additionally, those solutions do not seem to have a clean way to enumerate all solutions, inhibiting one from checking if any happen to be permutation matrices. Does anyone have more tips from a Riccati equation standpoint? I would like to add that strictly speaking one can answer the question in finite time by simply enumerating all permutation matrices of the appropriate size and checking if they satisfy $XA=(XA)^T$. For some reason I'm having trouble describing, I don't think this captures the spirit of the question. Update @GCab points out that $A$ would need a very peculiar structure for $X$ to be a permutation matrix. This is indeed the case, and $A$ would need to be symmetric on some row or column permutation. This does yield some insights. If $X$ is any permutation matrix, then $$(XAX)_{ij}=A_{ji}=(A^T)_{ij}$$ for any $i$, $j$ so that $X_{ij}=1$, and this does not hold when $X_{ij}=0$. In other words, the elements of $A$ come in pairs except for possibly the main diagonal of $XA$. Using this, one can rule out many possible permutations, since any element appearing only once in $A^T$ must give a location of a $1$ in $X$. There are many other such combinatorial tricks that can greatly improve the average runtime. One can similarly note that for a permutation $X$ to exist as a solution, there must exist a bijection $f$ between the rows and columns of $A$ so that if $r$ is a row of $A$ then the count of each element of $r$ is the same as its count in $f(r)$. Such a heuristic has the potential to easily classify a choice of $A$ as not having a satisfactory solution, but it could still take an enormous amount of time to confirm that a given $A$ does have a satisfactory $X$.","['matrix-equations', 'permutation-matrices', 'systems-of-equations', 'matrices', 'linear-algebra']"
2809714,If $p^k|n!$ then $p!^k|n!$,Is there a direct proof of the result that if $p$ is a prime and if $p^k|n!$ then $p!^k|n!$? The proof that I know is rather circuitous (take $k$ to be given by de Polignac's formula and construct a subgroup of $S_n$ of order $p!^k$). The result seems too elegant to not admit a slick combinatorial proof.,"['combinatorics', 'elementary-number-theory']"
2809731,Using Homogeneous Coordinates in Differential Equations,"Recently, I asked a question on the Mathematica Stack Exchange website regarding the use of homogeneous coordinates in differential equations. The question is about extending the interval of existence in differential equations using homogeneous coordinates. A prototypical example is the initial value problem for the tangent function.
$$\begin{cases} y'=1+y^2, \\ y(0)=0,  \end{cases}$$
with the corresponding interval of existence being $(-\pi/2,\pi/2)$. Identifying $y$ with a coordinate on the projective line, via $y=y_1/y_2$, results in the underdetermined system
$$\begin{cases} y_1'y_2-y_1y_2'=y_2^2 \left[1+ \left(\dfrac{y_1}{y_2} \right)^2 \right] ,\\ \dfrac{y_1(0)}{y_2(0)}=0. \end{cases} $$
Adding the normalization condition $y_1y_1'+y_2y_2'=0$ (which is a consequence of fixing $y_1^2+y_2^2$), and using the initial conditions $y_1(0)=0,y_2(0)=1$ results in a solvable system, with global solutions  $y_1(t)=\sin(t),y_2(t)=\cos(t)$. Thus it can be said that the interval of existence now is the entire real line! I was trying to see how far I can extend this method. In the link above, for instance, I showed that it works (at least numerically) for the $\sec$-$\tan$ system
$$\begin{cases} u'=uv \\ v'=u^2, \\ u(0)=1, \\ v(0)=0. \end{cases} $$ However, with other systems such as the first Painlevé equation
$$\begin{cases} u'=v, \\ v'=6u^2+t, \end{cases} $$
the numerical integrator appears to get stuck at the first pole. My questions: Is there a way to employ homogeneous coordinates in order to extend the interval of existence in the case of the first Painlevé equation? More generally, what about systems $ \dot{\mathbf{x}}=\mathbf{f}(t,\mathbf{x})$ with the Painlevé property (where all spontaneous singularities are poles)? If so, I'd greatly appreciate some details. I'm not asking for the code here, just the appropriate mathematical formulation. Thank you!","['numerical-methods', 'singularity', 'ordinary-differential-equations']"
2809748,How to understand a 'shifted' lognormal distribution random variable (RV) and its results,"This is an applied math question. I am doing some numerical work, in Python, using Scipy.stats.  But it is really the underlying math that matters, and I am questioning the results/implementation.  It is really the math that counts. The general problem is I am using lognormal (LN) RVs to obtain multiplicative results through iteration.  So, for example, I have a starting 'known' LN RV which is sort of like a Dirac-delta function: if $Y=e^X$, where Y is lognormal and X is thus normal.  To be clear, Y has both a mean and an SD (standard deviation) which can be calculated/observed empirically.  Underylying it is a normal distribution for X with parameters $\mu$ and $\sigma$, which can be derived from Y's mean and SD (see: https://en.wikipedia.org/wiki/Log-normal_distribution ). Since it is lognormal, I can multiply it by another LN distribution to get a new lognormal distribution.  in practice - if we call the parameters of the first distribution $mu_1$ and $sigma_1$, and those of the second $mu_2$ and $sigma_2$ , we can calculate the $X$ representation as: $$\mu - \mu_1 + \mu_2$$
$$\sigma = (\sigma_1^2 + \sigma_2^2)^{0.5}$$ assuming, of course, independence. All works well.  But Python offers an additional parameter 'offset', which shifts the lognormal left or right by the fixed amount.  Thus, if you have a wrapper around the Scipy calls that creates an object RV=Lognorm(100000, 10000, -50000) the pdf delivered does, indeed, have an SD = 10,000, but centered at 50,000 (since the 100,000 offset is offset by -50,000). What I struggle with is this. If you, in fact, ask the package fro the mean and SD for RV, it gives mean=50,000 and SD = 10,000.  Thus, it looks like it creates an RV that is not totally shifted left by 50,000 - which would possibly allow positive probability of values less than zero - but that it adjusts the mean downward by 50,000. It looks to me like a bit of a software kludge that works.  To my way of thinking, shifting to the left by 'n' units could/should preserve all central moments, but(1) will allow negative values and (2) there should not exist a proper, 2-parameter lognormal that gives the same pdf - i.e., pdf(100000, 10000) shifted left 50000 is not pdf(50000, 10000) since the pdf has, in its definition, $e^{ln(x)}$ and the shift should appear as $e^{ln(x)-s}$, where $s$ is the shift amount. Am I missing something here, or is this just a convention of Python which does not conform to the actual definition of the lognorm distribution?  Or am I wrong on the definition/understanding of a three-parameter lognormal distribution?","['statistics', 'computational-mathematics']"
2809802,Degrees of freedom of a metric up to coordinate changes (precise formulation),"Let $M$ be a smooth $n$-dimensional manifold. I have heard that a Riemannian metric on $M$, depends locally on $ n(n+1) / 2 - n = n(n-1) /2$ ""independent"" functions up to coordinate changes . I can roughly see the intuition for that: The symmetric matrix $g_{ij}
$ depends on $n(n+1) / 2$ functions, but you can ""change the coordinates"" which loses $n$ degrees of freedom. Is there a precise formulation of this statement? Given an arbitrary metric, can you  really specify $n$ out of the $ n(n+1)/2$ $g_{ij}$ functions as you wish? How can we see it's impossible specify more than $n$? I heard that Cartan proved something like this, using the language of jets. 
(Something about the dimension of the space of $k$-jets of metrics modulu diffeomorphisms). Maybe there are other references; any pointer in the right direction is welcomed. This question was supposedly answered here , but I find the answer there to be too vague. I am looking for a precise statement.","['riemannian-geometry', 'differential-topology', 'reference-request', 'jet-bundles', 'differential-geometry']"
2809817,"If there are $6$ matrices in $M_{6,6}(\Bbb C)$ such that they all satisfy $A^2=0$, does this imply that at least two of them are similar?","If there are $6$ matrices in the vector space $M_{6,6}(\Bbb C)$ such that they all satisfy $A^2=0$, does this imply that at least two of them are similar? My approach:
Observation: $A^2=0$ implies that all the eigenvalues of these matrices must equal to zero. Which naturally led me to think about the Jordan forms of these, to try and see if there are less than $6$ types of Jordan matrices corresponding to this property as that would be mean that at least two of these matrices should have the same Jordan matrix, and hence must be similar. 
I found that there are more than $6$ types of corresponding Jordan matrices, so my current answer is ""not necessarily true"", but I need a definite true or false answer here. 
Could someone give me a thought process?","['matrices', 'jordan-normal-form', 'linear-algebra']"
2809832,Isomorphism between ring of regular functions and coordinate ring of an affine variety.,I'm reading Hartshorne's Algebraic geometry book and there is a theorem which states that the ring of global functions $\mathcal{C}(Y)$ is isomorphic to the coordinate ring $A(Y)$ of an affine variety $Y$. I get it that there is natural injective homomorphism from $A(Y)$ to $\mathcal{C}(Y)$ but it is not clear to me the surjective part. Can you please help me on this? Thanks.,['algebraic-geometry']
2809855,"Is this a quadratic equation? If not, how do you solve it?","Recently I came across a sum which was of the type $$25x+10\sqrt x+1 = 0.$$ Now I could solve this using factoring, but I'm wondering if this falls under the category of quadratic equation and if it does, is there any other way to solve this without factoring? Also another sum of the same type : $$4x^2+4\sqrt x+1.$$","['algebra-precalculus', 'radicals', 'quadratics']"
2809919,Interpreting a chain map as a double complex (Weibel Exercise 1.2.8),"I started reading Weibel’s An introduction to homological algebra but have trouble with the following exercise: Exercise 1.2.8 (Mapping cone)
  Let $f \colon B \to C$ be a morphism of chain complexes.
  Form a double chain complex $D$ out of $f$ by thinking of $f$ as a chain complex in $\mathbf{Ch}$ and using the sign trick, putting $B[-1]$ in the row $q = 1$ and $C$ in the row $q = 0$.
  Thinking of $C$ and $B[-1]$ as double complexes in the obvious way, show that there exists a short exact sequence of double complexes
  $$
                        0
  \to                   C
  \to                   D
  \xrightarrow{\,\delta\,}  B[-1]
  \to                   0 \,.
$$
  The total complex of $D$ is $\operatorname{cone}(f')$, the mapping cone (see section 1.5) of the map $f'$, which differs from $f$ only by some $\pm$ signs and is isomorphic to $f$. [I have added a short summary of Weibel’s indexing and sign conventions at the end of the question.] My problem is that I don’t understand how we can interpret $f$ as a double complex in such a way that the shifted complex $B[-1]$ appears.
So far I have tried the following: We can think of $f$ as a commutative square with $B$ sitting in the row $q = 1$ and $C$ sitting in the row $q = 0$.
We can then use the sign trick to replace $f_p \colon B_p \to C_p$ by $(-1)^p f_p$, resulting in a double complex which looks like this:
$$
  \require{AMScd}
  \begin{CD}
    B_{p-1}
    @<d<<
    B_{p}
    @.
    \hspace{2em}(q=1)
    \\
    @V (-1)^{p-1} f VV
    @VV (-1)^p f V
    @.
    \\
    C_{p-1}
    @<d<<
    C_{p}
    @.
    \hspace{2em}(q = 0)
  \end{CD}
$$
(All other rows $q \neq 0, 1$ vanish.)
But then the row $q = 1$ is not $B[-1]$ but $B$. We could also use the double complex which looks like this:
$$
  \require{AMScd}
  \begin{CD}
    B_{p-1}
    @< -d <<
    B_{p}
    @.
    (q=1)
    \\
    @V f VV
    @V f VV
    @.
    \\
    C_{p-1}
    @<d<<
    C_{p}
    @.
    (q = 0)
  \end{CD}
$$
But then the row $q = 1$ is still not $B[-1]$ because we only flipped the sign of the differential, but didn’t actually shift the degrees.
Also, we didn’t use the sign trick. We could forcefully put $B[-1]$ in the row $q = 1$ and $C$ in the row $q = 0$, which results in the following diagram:
$$
  \require{AMScd}
  \begin{CD}
    B_{p-2}
    @< -d <<
    B_{p-1}
    @.
    (q=1)
    \\
    @V ? VV
    @V ? VV
    @.
    \\
    C_{p-1}
    @<d<<
    C_{p}
    @.
    (q = 0)
  \end{CD}
$$
But then I don’t understand how the vertical arrow are supposed to look like, and we didn’t use the sign trick.
This option also doesn’t seem to give the desired mapping cone (up to sign) as its total complex. (What may be related to my problem is that I don’t know why Weibel labels the morphism $D \to B[-1]$ as $\delta$.
As far as I can tell, this notational choice has not been explained, so I may be missing some subtlety here.) Any help is appreciated. Conventions : Weibel uses for a double complex $D = D_{\bullet,\bullet}$ the following indexing convention:
$$
  \require{AMScd}
  \begin{CD}
    D_{p-1,q}
    @< d^h <<
    D_{p,q}
    \\
    @V d^v VV
    @V d^v VV
    \\
    D_{p-1,q-1}
    @< d^h <<
    D_{p,q-1}
  \end{CD}
$$
He requires the above square to be anti-commutative, i.e. that $d^h d^v + d^v d^h = 0$.
He then defines the sign trick as associating to $d^v$ the chain morphisms $f_{\bullet,q} \colon D_{\bullet,q} \to D_{\bullet,q-1}$ given by $f_{p,q} = (-1)^p d^v_{p,q}$ (i.e. we flip the sign columnwise).
The shift $B[p]$ of a chain complex $B = B_{\bullet}$ is defined by $B[p]_n = B_{n+p}$ and $d^{B[p]} = (-1)^p d^B$.","['abstract-algebra', 'homological-algebra']"
2809950,Conformal quivalence,"Let's say there are two conformally equivalent domains $A,B$  in the complex plane. If A is simply connected, can we infer that B is also simply connected? Thank you in advance",['complex-analysis']
2809952,Posterior distribution based on the conjugate Gaussian-gamma prior: Exercise 2.44 from Bishop's book,"I am not sure how to tag a question such that different communities in the stackexchange can see at the same time. I have posted a question in the ""cross validated"" community. In order to avoid multiple copies of the same question in the different community, I hope it is acceptable to share the link to the question: https://stats.stackexchange.com/q/350030/204499 . I would be happy to receive your feedback and comments therein. Thank you so much in advance,","['probability-theory', 'probability-distributions', 'bayesian', 'machine-learning', 'statistics']"
2809966,Distribution of exponential $x_i$s : $\sum_{i=1}^n x_i$,"Exercise : Let $X_1, \dots, X_n$ be a random sample from the Exponential Distribution with unknown parameter $\theta$ . (i) Find a sufficient and complete statistics function $T$ , for $\theta$ . (ii) Using without proof known formulas, find the distribution of $T$ . Attempt : (i) The p.d.f. for the sample is given as : $$f(x;\theta) = \begin{cases} \theta e^{-\theta x}, & x \geq 0 \\ 0, & x<0 \end{cases}$$ Thus $f(x;\theta) = \theta e^{-\theta x}\mathbb{I}_{[0,+\infty]}(x) $ which belongs to the Exponential Family of Distributions, thus the function: $$ T = \sum_{i=1}^nx_i$$ is a sufficient and complete statistics function for $\theta$ . (ii) Question : How would one proceed with finding the distribution of $T$ now ?","['exponential-distribution', 'statistics', 'probability', 'probability-distributions']"
2809967,Blow up of $\mathbb{P}^2$ in a point and direct image sheaves,"I am trying to understand better direct image sheaves. To do so, I want to start working in a particular and easy example. Let $\pi:X\rightarrow \mathbb{P}^2$ be the blow up of $\mathbb{P}^2$ in a point, and $E\subset X$ the exceptional curve. What can we say about $\pi_*\mathcal{O}_X(E)$ or $\pi_*\mathcal{O}_X$? Are they invertible sheaves? If so, which is the integer $n$ such that they coincide with $\mathcal{O}_{\mathbb{P}^2}(n)$? More in general, we can ask the same questions about the sheaves $\pi_*\mathcal{O}_X(aL+bE)$, where $a,b\in \mathbb{Z}$ and $L$ is the pullback of a line in $\mathbb{P}^2$.","['pushforward', 'sheaf-theory', 'algebraic-geometry', 'blowup']"
2809985,p.d.f of the absolute value of a Gaussian random variable of non-zero mean,"For a complex random variable (r.v), if real and imaginary parts are i.i.d with Normal distribution, the absolute value of the r.v. follow Rayleigh distribution. However, what if the real and imaginary parts are Gaussian distributed with arbitrary non-zero mean?","['probability-theory', 'probability-distributions']"
2810002,Constructing a symmetrical $100(1-a)\%$ confidence interval for $\theta$.,"Exercise : Let $X_1, \dots, X_n$ be a random sample from the distribution function $F(x) =1 - \frac{\theta^3}{x^3}, \; x \geq \theta$ where $\theta >0$ unknown parameter. (i) Find a maximum likelihood estimator for $\theta$ (ii) Find a sufficient and complete statistics function $T$ , for $\theta$ . (iii) Check if the maximum likelihood estimator you found is unbiased for $\theta$ . (iv) Find the distribution of $Y = T/\theta$ . (v) Construct a symmetrical $100(1-a)\%$ confidence interval for $\theta$ . Question : I have successfully solved each one of the parts (i) to (iv) but I find myself stuck on (v). I am not very familiar with confidence intervals, so it could be something simple. I would really appreciate a thorough explanation or solution so as I can grasp the idea of such question parts, since this is a usual exams question.","['probability-distributions', 'maximum-likelihood', 'statistics', 'probability', 'confidence-interval']"
2810014,Arzela-Ascoli theorem exercise,"The question : Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R} 
> \right\} $ , where $f$ is continuous function on $K$. Let  $K\in
 \mathbb{R}$ be compact and let $B\subset C(K)$ be compact. Prove that
  $B$ is equicontinuousas follows: Prove that the map $F:C(K)\times K\rightarrow \mathbb{R}$ defined
  by $F(f,x)=f(x)$ is continuous. Use uniform continuity of $F$ restricted to $B\times K$ to deduce
  the result. My attempt : For $F(f,x)=f(x),F(g,x)=g(x)$, $\left | F(f,x)-F(g,x) \right |=\left | f(x)-g(x) \right |\leq \sup\left | f(x)-g(x) \right |=d(f,g)$ Hence, $F$ is continuous on $C(K)\times K
$. If I show $B$ is closed and pointwise bounded ,then $B$ is equicontinuous by Arzela-Ascoli theorem. Since $B$ and $K$are compact, cartesian product $B\times K$ is also a compact set. So, $F$ is uniformly continuous on $B\times K$ . $B$ is compact. By Heine-Borel theorem, $B$ is closed and bounded. It suffices to show that $B$ is pointwise bounded. But, I don't know this part using the uniform continuity of F on $B\times K$.","['real-analysis', 'arzela-ascoli', 'equicontinuity', 'functional-analysis', 'analysis']"
2810047,"Sufficient statistics function for $N(\theta, c\theta^2)$ and symmetrical confidence interval using $\bar{X}$","Exercise: Let $X_1, \dots, X_n$ be a random sample from the Normal Distribution $N(\theta,c\theta^2)$ where $c > 0$ is a known constant and $\theta \in \mathbb R$ an unknown parameter. i) Find a sufficient statistics function for $\theta$ . ii) Using only the statistics function $\bar{X}$ , construct a $100(1 - a)\%$ confidence interval for $\theta$ . Attempt: i) \begin{align*}p(x \mid c,\theta) &= \prod_{i=1}^n(2\pi c\theta^2)^{-1/2}\exp\big\{-(x_i-\theta)^2/(2c\theta^2)\big\}\\
&=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\sum_{i=1}^n(x_i-\theta)^2\bigg\}\\
&=(2\pi c\theta^2)^{-n/2}\exp\bigg\{-\frac{n}{2c\theta^2}\bigg(\sum_{i=1}^nx_i^2 -2\theta\sum_{i=1}^nx_i+n\theta^2\bigg)\bigg\}.
\end{align*} Thus, we can continue and figure out a sufficient statistics function by Fisher's factorization theorem. (ii) How would one proceed by finding a confidence interval for $\theta$ as asked though?","['statistics', 'confidence-interval']"
2810077,How many functions are there from [9] to [7] if every image in the codomain has 3 arguments in the domain?,"I going to apply to a maths major the coming year and in order to do that, I need to pass the entry test (just so you know my level ain't high). The question is from a long list of exercises they gave me to get ready for the test: $Let$ $[n]={1,2,3,....,n}$ How many functions are there from [9] to [7] if every image in the codomain has 3 arguments in the domain? (I translated it from hebrew, I hope I'm correct) so I thought taking the set $[9]$ and divide it into 3 triplets: for example:$${(1,2,3),(4,5,6),(7,8,9)}$$
so all possible combinations are $${9 \choose 3}$$ and then I'll try to calculate all possible permutations with $[7]$ which is: $${9 \choose 3}*\frac{7!}{(7-3)!}$$ I think I made a mistake about how I counted the combinations but I don't have any solutions to check myself (silly I know), is it correct?","['permutations', 'combinatorics', 'functions', 'combinations']"
2810150,Determining the point from which the most area in a polygon is visible,"I am wondering about the following problem: Given a polygon and the set of points $S$ inside it, what are the point(s) in $S$ from which the most area in $S$ is visible? Furthermore, what is the maximum visible area? Here, I define $q$ to be visible from $p$ if the line segment between $p$ and $q$ is contained in $S$. This intends to capture the intuitive idea of what points in a room are visible when standing somewhere in the room. For example, in the figure below, the dark blue area is visible from point P, at the center of the top left quarter. The light blue area is not. While the answer for any star-shaped domain is clear, finding the answer for arbitrary polygons seems difficult. Question: How can we find the solution to the problem for a given polygon? For example, the problem is not so easy for the polygon below...","['area', 'optimization', 'geometry']"
2810179,"Group isomorphic to all its proper quotients, and not simple [duplicate]","This question already has answers here : Sort-of-simple non-Hopfian groups (2 answers) Closed 6 years ago . Let $G$ be a group such that for any proper normal subgroup $H \subset G$, we have $G/H \cong G$ (of course this isomorphism may not be given by the projection $G \to G/H$). Does it follow that $G$ is a simple group? What can we say about such a group $G$ in general? Notice that $G/H \cong G$ doesn't imply $H = \{e\}$, for instance $z \mapsto z^2$ is a surjective morphism $\Bbb C^{\times} \to \Bbb  C^{\times}$ of kernel $\{\pm 1\}$.",['group-theory']
2810192,Show sum of stopping times also stopping time,"Maybe the question is trivial. Let $(X_{n})_{n \geq 0}$ ne a sequence of random variables and $N_{1},N_{2}$ stopping times with respect to the sequence $(X_{n})_{n \geq 0}$. Now I have Show that
$$
\min(N_{1},N_{2}),~\max(N_{1},N_{2})
$$
are also stopping times. My Problem is to Show whether $N_{1} + N_{2}$ is also a stopping time. What about $$
N := \sup\{n\geq 0|X_{n} \geq 0\}.
$$
Is $N$ also a stopping time?","['probability-theory', 'probability', 'stopping-times']"
2810229,Properties of the multiplication table of $\mathbb{Z}_m$ [duplicate],"This question already has answers here : Find all $n$ with $\!\bmod n\!:\, a\,$ invertible $\Rightarrow a^{-1} \equiv a,\,$ i.e. $a^2\equiv 1$ (4 answers) Closed 6 years ago . For which $m$ do all occurrence of 1 lie on the diagonal of the multiplication table of $\mathbb{Z}_m$? I was think i have got a prove and find all the $m$ is $1,2,3,4,6,8,12$ and $24$.
but after a while, there is a bug in my prove. suppose $m>10$ and $<10$ part is too simple
first $m$ cannot be odd, or $(2,m)=1$ and there is x such that $2x\equiv1\mod{m}$, so $x\ne2$ for $m>10$ and this will against the 1 lie on diagonal. if $(3,m)=1$, then $3^2\equiv1\pmod{m}$, then $m\mid8$ impossible too. so $3\mid m$. if $(5,m)=1$, using the same strategy, $m|24$, with $m>10$ verify $\mathbb{Z}_{12}$ and $\mathbb{Z}_{24}$ is a solution. so we just consider $5\mid m$. I also verified $7\mid m$, so $m=210\cdot Q$, $Q\in\mathbb{N}$. then, if $x\in\mathbb{N}$ and $x<\sqrt{m}$, that is $x^2<m$, so $x^2\not\equiv1\pmod{m}$.... so how to continue...",['number-theory']
2810270,The spectral sequence for hyper-derived functors,"Let $F$ be a right exact functor between two abelian categories $A$ and $B$.Suppose that $C_\bullet$  is a complex in $A$,then there is a convergent spectral sequence $$E_{p,q}^2 = ({L_p}F)({H_q}({C_ \bullet }) \Rightarrow {\mathbb{L}_{p + q}}F(C).$$ According to Weibel's book An introduction to homological algebra,p148,the proof says that we just take a Cartan-Eilenberg resolution $P_{\bullet,\bullet}$ of  $C_\bullet$ and consider the double complex $F(P_{\bullet,\bullet})$.There is a spectral sequence for a double complex $$E_{p,q}^2 = H_p^vH_q^h({C_ \bullet }) \Rightarrow {H_{p + q}}({\rm{Tot}}(C)),$$ where $H_p^v$ is the homology for vertical direction and $H_q^h$ is horizontal. My question is how do I get the first   spectral sequence from the second  spectral sequence above?I have no idea why $H_p^vH_q^h(F({P_{ \bullet , \bullet }}))$ is  $({L_p}F)({H_q}({C_ \bullet })$.A rough guess is that $H_q^h$ commutes with $F$,then by the definition of Cartan-Eilenberg resolution $H_q^h(P)$ is a projective resolution of $H_q(C)$ and we done.But it is so weird that $H_q^h$ 
can commute with $F$ .So it still  get me trouble.","['spectral-sequences', 'abstract-algebra', 'homological-algebra', 'algebraic-geometry']"
2810318,"Action of $GL(2,\mathbb{R})$ on Teichmuller space of half-translation surfaces","Let $S_g$ be a Riemann surface of genus $g\ge 2$ and $P=\{p_1,\dots,p_n\}\subset S_g$ a set of $n\ge 1$ points. Denote by $Diff_P^0(S_g)$ the set of diffeomorphisms of $S_g$ homotopic to the identity and which fix the points of $P$. For any $\alpha=(\alpha_1,\dots,\alpha_n)\in \mathbb{N}^n$ such that $\sum_{i=1}^n\alpha_i=4g-4$, and $l\in \{1,-1\}$ one can define the set $Q(\alpha,l)$ of half translation surfaces with zeroes on $P$ prescribed by $\alpha$ (this means that the half-translation surfaces have a zero of order $\alpha_i$ on $p_i$, $i=1,\dots,n$) and $l$ (if $l=1$ then the half-translation surfaces have trivial holonomy, if $l=-1$ then the holonomy is $-Id$). The Teichmuller space $TQ(\alpha,l)$ of half-translation surfaces with zeroes on $P$ prescribed by $\alpha$ and $l$ is the quotient of $Q(\alpha,l)$ by the action of $Diff_P^0(S_g)$ (which acts by pullback). I have read in many articles that there is an action of $SL(2,\mathbb{R})$ on $TQ(\alpha,l)$, which for every $A\in SL(2,\mathbb{R})$ is realized simply composing the charts of the half-translation surface with $A$. My question is: is there a similar action for $GL(2,\mathbb{R})$? Yoccoz in this article this article defines it for translation surfaces, so the answer is ""yes"" for translation surfaces. Is there a problem extending the action of $GL(2,\mathbb{R})$ on $TQ(\alpha,l)$? Maybe the holonomy can be changed? I am asking because I can not figure it out.","['differential-geometry', 'algebraic-geometry']"
2810348,Limit and definite integral question [duplicate],"This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 4 years ago . I recently came across this problem: What is the value $$\mathrm{\lim_{n\to\infty}\int_0^1 \frac{nx^{n-1}}{1+x}dx}$$ Assume that the interchange of limit and integration is possible. I have no idea how to begin solving this problem. I guess expressing the integral as an inequality might help, but I could not make any way out by that method. Any ideas?","['integration', 'definite-integrals', 'limits']"
2810356,"Prob. 20 (a) & (b), Exercises 8.9, in Apostol's CALCULUS vol. 2: If $f^\prime(x;y)=0$ for every $x$ and for every $y$, . . .","Here is Prob. 20, Exercises 8.9, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: (a)  Assume that $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x}$ in some $n$ -ball $B(\mathbf{a})$ and for every vector $\mathbf{y}$ . Use the mean-value theorem to prove that $f$ is constant on $B(\mathbf{a})$ . (b) Suppose that $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for a fixed vector $\mathbf{y}$ and for every $\mathbf{x}$ in $B(\mathbf{a})$ . What can you conclude about $f$ in this case? And, here is Theorem 8.4 (The Mean-Value Theorem For Derivatives Of Scalar Fields): Assume the derivative $f^\prime ( \mathbf{a} + t \mathbf{y}; \mathbf{y} )$ exists for each $t$ in the interval $0 \leq t \leq 1$ . Then for some real $\theta$ in the open interval $0 < \theta < 1$ we have $$ f ( \mathbf{a} + \mathbf{y} ) - f(  \mathbf{a} ) = f^\prime (  \mathbf{z};  \mathbf{y} ), \ \mbox{ where } \  \mathbf{z} =  \mathbf{a} + \theta  \mathbf{y}. $$ My Attempt: Part (a) Let $\mathbf{x}$ be any point in the $n$ -ball $B( \mathbf{a}  )$ . Let us put $$\mathbf{y} \colon=  \mathbf{x} -  \mathbf{a}$$ so that $$\mathbf{x} =  \mathbf{a} +  \mathbf{y}. $$ Then, for any real number $\theta$ in the interval $0 \leq \theta \leq 1$ , we find that the point $ \mathbf{a} + \theta \mathbf{y}  $ also lies in the $n$ -ball $B( \mathbf{a} )$ , and so we must have $$ f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0. $$ Then, for some real number $\theta$ such that $0 < \theta < 1$ , we obtain $$ f(\mathbf{x}) - f(\mathbf{a}) = f(\mathbf{a} + \mathbf{y} ) - f( \mathbf{a}) = f^\prime ( \mathbf{a} + \theta \mathbf{y}; \mathbf{y} ) = 0, $$ and so $$ f(\mathbf{x}) = f(\mathbf{a}) $$ for every point $\mathbf{x}$ in that $n$ -ball. Hence $f$ is constant on the $n$ -ball. Is this proof correct? And if so, then is it clear enough too? Part (b) Since $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for a fixed vector $\mathbf{y}$ and for every $\mathbf{x}$ in $B( \mathbf{a} )$ , therefore if $t$ is a real number such that the point $\mathbf{a} + t \mathbf{y}$ is also in $B( \mathbf{a} )$ , then we see that, for some real number $\theta \in (0, 1)$ , we have $$ f( \mathbf{a} + t \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y}; t \mathbf{y} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y};  \mathbf{y} ) = 0, $$ and so $$ f( \mathbf{a} + t \mathbf{y} ) = f( \mathbf{a} ), $$ which shows that $f$ does not change along any line through the point $\mathbf{a}$ and parallel to the vector $\mathbf{y}$ . Is each and everything of what I have done (and stated) in Part (b) correct? Or, have I made an error? P.S.: Part (b) Continued: Now let $\mathbf{x}$ be any other point in $B(\mathbf{a})$ . Then using the same argument as above but with $\mathbf{a}$ replaced by $\mathbf{x}$ we can conclude that our scalar field $f$ doesn't change throughout $B(\mathbf{a})$ along any line parallel to the vector $\mathbf{y}$ . Am I right? Is my reasoning correct?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2810381,pdf of a member of a sequence of dependent random variables,"I would very much appreciate a hint for the following problem Let $\left(X_n\right)_{n=1}^\infty$ be a sequence of random variables s.t.:
$$X_1 \sim U_{[0,1]}$$
and for all $n>1$:
$$X_n \sim U_{[0,X_{n-1}]}.$$
Give a general expression for $f_{X_n}$ the pdf of $X_n$. Thanks","['probability-theory', 'probability']"
2810419,Show that this stochastic process is a.s. strictly positive.,"Consider the one-dimensional SDE
$$
dX_t = f(X_t)dt + \sigma(X_t)dW_t,\quad X_0 = 1,
$$
where $W_t$ is a Brownian motion under the measure $\mathbb{P}$, in its natural filtration. Suppose that $f(x)-\sigma^2(x)/2x=0$ and $\sigma^2(x)\le x^2$. Show that $X_t>0$ almost surely. My initial attempt was to consider the log-process $\log(X_t)$. Using Ito's lemma one finds 
$$
d\log(X_t) = \frac{1}{X_t}\left(f(X_t) 
- \frac{\sigma^2(X_t)}{2X_t}\right)dt + 
\frac{\sigma(X_t)}{X_t}dW_t = \frac{\sigma(X_t)}{X_t}dW_t.
$$ 
Integrating now leads to 
$$
\log(X_t)=\int_0^t\frac{\sigma(X_s)}{X_s}dW_s.
$$
By the Ito isometry, the variance of the above process is bounded by $t$:
$$
\text{Var}\big[\log(X_t)\big] = \mathbb{E}\left[\int_0^t\frac{\sigma^2(X_s)}{X_s^2}ds\right] \le t.
$$
I feel like this is heading in the right direction, but I'm struggling to see what the next step is. Any suggestions would be very welcome.","['stochastic-processes', 'probability-theory', 'brownian-motion', 'stochastic-calculus', 'stochastic-differential-equations']"
2810428,Existence of surjective group homomorphism with finite kernel,"The following comes from the book Teichmüller Theory by Hubbard.  During the proof of a proposition he uses the following sort of argument to say there exists a surjective homomorphism with finite kernel, but I don't understand why this proves such a map exist. Earlier he had shown for some space $X_n^*$ that $H^1(X_n^*; \mathbb{R}) = \mathbb{R}$.  He then goes on to say that since $H^1(X_n^*; \mathbb{R}) = \mathrm{Hom}(H_1(X_n^*; \mathbb{Z}), \mathbb{R})$ and since $H_1(X_n^*; \mathbb{Z})$ is a finitely-generated Abelian group, there must be a surjective homomorphism $H_1(X_n^*; \mathbb{Z}) \to \mathbb{Z}$ with finite kernel. Why does this imply there is a surjective map with finite kernel?","['algebraic-topology', 'homology-cohomology', 'group-theory']"
2810433,Polynomial and operator in Hilbert space,"I’m non able to show point 2 of this exercise: Let $H$ be a Hilbert space and $T : H \to H $ a compact self-adjoint operator. Suppose there exists a polynomial $p: \mathbb{R} \to \mathbb{R}$ with only real zeros s.t. $p(T)=0$. Show If $dim(H) = \infty $ then $0$ is an eigenvalue of $T$ If $p(s)>0$ when $s<0$, then $\langle Tx, x \rangle \ge 0$ for all $x \in H$ I’ve proved 1 but I have no idea of how to prove 2!","['functional-analysis', 'compact-operators', 'hilbert-spaces']"
2810456,measurability of argmin,"Define for every $\omega\in\Omega$:
$$f_\omega=\operatorname{argmin}_{f\in S} L(\omega,f)$$ where $\Omega$ is a measurable space $S\subseteq \big(C(\mathbb{R}^d,\mathbb{R}^n),\|{\cdot}\|_\infty\big)$ compact $L\colon\, \Omega\times C(\mathbb{R}^d,\mathbb{R}^n)\to [0,\infty)$ $\omega \mapsto L(\omega,f)$ is measurable for every $f\in C(\mathbb{R}^d,\mathbb{R}^n)$ $C(\mathbb{R}^d,\mathbb{R}^n) \ni f \mapsto L(\omega,f)$ is continuous for every $\omega\in\Omega$ Can we choose the minimizer $f_\omega$ for every $\omega$ in a way such that the mapping $(\Omega,\mathbb{R}^d) \ni(\omega,x)\mapsto f_{\omega}(x)$ is measurable? Idea:
I think that Theorem 18.19 in Aliprantis & Border 2006 could be applicable?
( Measurability of supremum over measurable set )","['measurable-functions', 'optimization', 'functional-analysis', 'measure-theory', 'calculus-of-variations']"
2810460,Simplifying $\frac{\sin\alpha\cos^2\beta}{\sin\beta}+\frac{\sin\beta\cos^2\alpha}{\sin\alpha}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to simplify the expression and get a closed form? $$\frac{\sin\alpha\cos^2\beta}{\sin\beta}+\frac{\sin\beta\cos^2\alpha}{\sin\alpha}$$
  where $\alpha := \frac12(m+1)\theta$ and $\beta:=\frac12(m-1)\theta$; and where $m$ is an integer. I wanna get rid of the denominators or make it a constant without $m$, but don't know how.",['trigonometry']
2810493,"A student must pick 5 classes from 12 courses, if he must have at least one WH class or USH class, how many different choices does he have?","The World History course and United States history course is already part of the 12 courses he must choose from. It is required to have at least one of these classes in his new class schedule. Sorry if my question didn't make sense earlier, this question was from a practice exam I'm just recalling the question from memory. I was short on time so I just solved C(12,5) and I know its wrong because I forgot about the at least. This has been bugging me all day and I'd appreciate it if someone can explain.","['combinations', 'combinatorics']"
2810516,Derivative of product of matrix by vector,"Let $\boldsymbol{\beta} := (\beta_1, \ldots, \beta_p)^T$ and $\boldsymbol{X}$ be a matrix of dimension $n \times p$. I'd like to compute the derivative $$\nabla_{\beta} \left( X \beta \right)$$","['derivatives', 'matrices', 'matrix-calculus', 'multivariable-calculus', 'jacobian']"
2810554,Incomparability of Prime ideals in integral Ring Extensions,"I'm having an issue with the theorem on incomparability of prime ideals in integral ring extensions, which I have stated as follows: Let $ R \subseteq S$ be an integral extension of commutative rings (with identity), and suppose that $P_{1} \subseteq P_{2}$ are two prime ideals of $S$ such that $R \cap P_{1} = R \cap P_{2} $, then $P_{1} = P_{2}$ The proof of which requires the following lemma: Let $R \subseteq S$ be an integral extension of commutative rings (with identity), with $S$ an integral domain. If $I$ is a non-zero ideal of $S$, then $R \cap I \neq \{ 0 \}$. The proof proceeds as follows: Let $Q = R \cap P_{1} = R \cap P_{2}$. Then $R / Q \subseteq S / P_{1}$ is an integral extension with $P_{2} / P_{1}$ an ideal of the domain $S / P_{1}$ such that $P_{2} / P_{1} \cap R / Q = Q / Q = \{ \bar{0} \} $, and so by the lemma $P_{2} / P_{1}$ is the zero ideal of $S / P_{1}$. Hence $P_{2} \subseteq P_{1} \subseteq P_{2}$ and so $P_{1} = P_{2}$ My issue with this is this proof only seems to require the primality of $P_{1}$, since the only place that primality is required is to ensure that $S / P_{1}$ is a domain so that we can apply the lemma. Am I correct that the primality of $P_{2}$ is not needed in the statement of incomparability? And if so is there a reason it is stated as needing $P_{2}$ to be prime too? $\textbf{Edit: }$ I posted this as a comment below, but I thought it might be worthwhile to replicate this here, since it serves as my motivation for asking this question, and provides a concrete example for where this stronger / generalised form of the statement of incomparability is useful. Though, I do acknowledge that, as mentioned below, it's not hard to alter this proof to allow us to apply the typical form of incomparability. $\textbf{Claim: }$ An integral extension of a Jacobson Ring is a Jacobson Ring. $\textbf{Proof :}$ Let $R \subseteq S$ be an integral extension of rings and let $I$ be a prime ideal of $S$. Then since $R$ is Jacobson, and $(I \cap R)$ is a prime ideal of $R$, there are maximal ideals of $R$, $\{ M_{i} \}_{i=1}^{l}$ say, such that $(I \cap R) = \cap_{i=1}^{l} (M_{i})$. Then for each $i$ the chain $(I \cap R) \subseteq M_{i}$ is a chain of prime ideals of $R$ with $I$ a prime ideal of $S$ laying over $(I \cap R)$, and so by the going up theorem there are primes ideals of $S$, $\{ N_{i} \}_{i=1}^{l}$ say, each containing $I$ and each lying over its respective $M_{i}$. Then $N_{i}$ is a prime ideal lying over a maximal ideal, so is itself maximal. Then let $I' = \cap_{i=1}^{l} (N_{i})$. Then $I \subseteq I'$ are ideals of $S$ with $I$ prime and 
$$
(I' \cap R) = ( \cap_{i=1}^{l} (N_{i})) \cap R = \cap_{i=1}^{l} (N_{i} \cap R) = \cap_{i=1}^{l} (M_{i}) = (I \cap R) 
$$
and so by the more general form of incomparability, noticing crucially I have not proved primality of $I'$, we see that $I = I'$. Hence $I$ is an finite intersection of maximal ideals of $S$, and so since $I$ was arbitrary in $S$ amongst the prime ideals of $S$, we see that $S$ is Jacobson. $\textbf{ Remark: }$ My proof here also uses a more specific form of the going up theorem than the one I was provided with. The statement of the going up theorem I have states that if $ \{Q_{i}\}$ is a strictly increasing chain of prime ideals in $R$, then there is a strictly increasing chain of prime ideal in $S$, $ \{P_{i}\}$ say, such that $P_{i} \cap R = Q_{i}$. However since the proof of this is inductive, and you start by choosing a prime ideal $P_{1}$ (whose existence is guaranteed by a simpler lemma) in $S$ laying over $Q_{1}$, we see that in fact if we already have a partially completed chain in $S$ we can use going up to complete it. Which is exactly what I have done in the proof above.","['abstract-algebra', 'ring-theory', 'commutative-algebra']"
2810574,Whether $SO_k(\mathbb{Q})\subsetneq SO_k(\mathbb{Q(\sqrt{n})})$,"Is it true that for all square-free $n$, and for all $k>1$, we have $SO_k(\mathbb{Q})\subsetneq SO_k(\mathbb{Q(\sqrt{n})})$? So far I have only discovered this: if $n$ has no prime factors $\equiv -1 \;\bmod 4$, then there exist $x,y\in \mathbb{Z}^+$ such that $(\dfrac{x}{\sqrt{n}})^2+(\dfrac{y}{\sqrt{n}})^2=1$, from which a rotation is easily constructed. The other cases seem more obscure.","['number-theory', 'orthogonal-matrices', 'field-theory', 'linear-algebra']"
2810623,Leftmost digit of Fibonacci sequence,"So, on another math forum, someone posted a question about the leftmost digit of a randomly chosen Fibonacci number. It likely follows Benford's Law for the distribution of the leftmost digit. I know the rightmost digit is cyclic with a cycle of 60, so I wanted to check if there was any kind of repetition for the leftmost digit. It seems that $F(n+67)$ and $F(n)$ have the same first digit a surprisingly high percentage of the time (my calculations put it at around 97% for the first 1000 Fibonacci numbers). I tried to see if this was understood at all, or just another mystery of the Fibonacci numbers, but after a few hours of searching, I did not come across anything. You can see it in action with this graph: http://www.wolframalpha.com/input/?i=FLOOR%5BFibonacci%5Bn%2B67%5D%2F10%5E(FLOOR(Log%5BFibonacci%5Bn%2B67%5D%5D%2FLog%5B10%5D))%5D-FLOOR%5BFibonacci%5Bn%5D%2F10%5E(FLOOR(Log%5BFibonacci%5Bn%5D%5D%2FLog%5B10%5D))%5D Anyway, back to my question. Does anyone know of any research related to this?","['number-theory', 'fibonacci-numbers']"
2810637,Why are all analytic functions equivalent to their Taylor series?,"""A function is analytic if and only if its Taylor series about $x_0$ converges to the function in some neighborhood for every $x_0$ in its domain."" Clearly if its Taylor series converges to $f$ then the function is analytic, but why is the converse true? I would really appreciate any help/thoughts.","['real-analysis', 'taylor-expansion', 'analyticity', 'power-series', 'analysis']"
2810702,Trouble with proof of deviations square,"I apologize upfront for any spelling mistakes, I'm not used to writing math in english! I tried searching for this question in here already but was not sure I used the best tags while doing so. Anyway, to the question: It was taken from a brazilian textbook on Basic Statistics (Bussab & Morettin, 2013). It basically justs asks me to show that: $${\sum\limits_{i = 1}^n {\left( {{x_i} - \overline x } \right)} ^2} = \sum\limits_{i = 1}^n {{x_i}^2 - n{{\overline x }^2} = \sum\limits_{i = 1}^n {{x_i}^2 - {{{{\left( {\Sigma {x_i}} \right)}^2}} \over n}} } $$ Now, I didn't really know where to start or if there's an official recommended approach to such proofs, but I just tried to start it by opening the first term: $$\eqalign{
  & {\sum\limits_{i = 1}^n {\left( {{x_i} - \overline x } \right)} ^2} = {\left( {{x_1} - \overline x } \right)^2} + {\left( {{x_2} - \overline x } \right)^2} + ... + {\left( {{x_n} - \overline x } \right)^2}  \cr 
  & {\sum\limits_{i = 1}^n {\left( {{x_i} - \overline x } \right)} ^2} = \left( {{x_1}^2 - 2{x_1}\overline x  + {{\overline x }^2}} \right) + \left( {{x_2}^2 - 2{x_2}\overline x  + {{\overline x }^2}} \right) + ...\left( {{x_n}^2 - 2{x_n}\overline x  + {{\overline x }^2}} \right) \cr} $$ At which point I felt I was close enough to start regrouping the pieces: $$\eqalign{
  & {\sum\limits_{i = 1}^n {\left( {{x_i} - \overline x } \right)} ^2} = \left( {{x_1}^2 + {x_2}^2 + ...{x_n}^2} \right) + \left( {{{\overline x }^2} + {{\overline x }^2} + ... + {{\overline x }^2}} \right) - 2\overline x \left( {{x_1} + {x_2} + ... + {x_n}} \right)  \cr 
  & {\sum\limits_{i = 1}^n {\left( {{x_i} - \overline x } \right)} ^2} = \sum\limits_{i = 1}^n {{x_i}^2}  + n{\overline x ^2} - 2\overline x \left( {{x_1} + {x_2} + ... + {x_n}} \right) \cr} $$ And that's where I stuck. I can get the $ + n{\overline x ^2}$ to be $ - n{\overline x ^2}$, and I don't know how to ""get rid"" of the third term.","['statistics', 'proof-explanation']"
2810707,$\eta$-value of a partition and its meaning,"The $\eta$-value of an integer partition $\lambda = \big( \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k \geq 0 \big)$ is defined as \begin{equation}
\eta \big( \lambda \big) \ := \ \sum_{i=1}^k \, (i-1) \, \lambda_i 
\end{equation} It makes an appearance in a $q$-version of the hook-length formula for a special value of the Schur function $s_\lambda$, namely \begin{equation}
s_\lambda \big(1, q, q^2, \dots \big) \ = \ q^{\eta(\lambda)} \, 
\prod_{\stackrel{\scriptstyle \text{boxes}}{b \, \in \, \lambda}}
\, \Big(1 \, - \, q^{\, \text{hook}\, (b)} \Big)^{-1}
\end{equation} where $q$ is a variable, $b$ is a box in the young diagram of $\lambda$,
and $\text{hook}(b)$ is the hook-length of the box $b$. Question: Is there a natural interpretation of $\eta(\lambda)$
as class-function on the symmetric group $S_n$ where $n = |\lambda|$ 
and where $\lambda$ labels the conjugacy class in $S_n$ consisting of permutations of cycle-type $\lambda$ ? ines.","['young-tableaux', 'combinatorics', 'algebraic-combinatorics', 'representation-theory']"
2810715,Spivak's Calculus: Proofs concerning Pascal's Triangle,"Problem 3 of Chapter 2 in Spivak's Calculus poses 5 problems associated with Pascal's Triangle. The first of these asks you to prove that $\binom{n+1}{k}$ = $\binom{n}{k-1} + \binom{n}{k}$ which was fairly straightforward. The second task was to prove by induction that $\binom{n}{k}$ is always a natural number. For this, I took the recursion approach: given the demonstration of the equality above, any $\binom{n}{k}$ can be decomposed into $\binom{n-1}{k-1} + \binom{n-1}{k}$ and so on, with each value decomposing until every binomial reaches either an $n$ or $k$ value of $0$, at which point they evaluate to 1 and can be summed. $1$ is a natural number, so the sum of any number of $1$s must therefore be a natural number. This proof seems to me as though it lacks formality, but I think the logic is appropriate. The third task, however, I am simply unsure what to make of. It reads: ""Give another proof that $\binom{n}{k}$ is a natural number by showing that $\binom{n}{k}$ is the number of sets of exactly $k$ integers each chosen from $1, \cdots,n$."" The word induction is not used in the prompt, and I can see that this appears to be true across the first few rows of the triangle, but how one would rigorously prove something like this, I do not know. Additionally, having proven that this is true, is the link from that to $\binom{n}{k}$ always being a natural number just the fact that you cannot have partial or negative sets, and therefore it must be a natural number?","['combinatorics', 'binomial-coefficients', 'combinatorial-proofs', 'elementary-set-theory']"
2810746,Stuck on Induction Problem,"I have had more than a day of difficulty with this problem. I have watched many youtube videos on induction and have not been able to solve this problem particularly. I would appreciate helpful hints. Heres the question: Prove: $$\forall n\gt 1, 1*2+2*3+3*4+...+n(n+1)=\frac{(n(n+1)(n+2))}{3}$$ I have shown my work below. I would appreciate some hints and guidance. My Attempt: $$1*2+2*3+3*4+...+n(n+1)=\frac{(n(n+1)(n+2))}{3}$$
Base case: $n=1$
$$1(1+1)=\frac{(1(1+1)(1+2))}{3}$$ $$2=\frac{6}{3}=2$$ The base case works. Induction Hypothesis: $n=k$ $$1*2+2*3+3*4+...+k(k+1)=\frac{(k(k+1)(k+2))}{3}$$ Assume the Induction Hypothesis is true for $k$. Then, we show: $$\frac{(k(k+1)(k+2))}{3}+(k+1)=\frac{k^2+6k+3}{3}$$ I get pretty confused trying to apply the inductive hypothesis here because it is difficult to treat the $n(n+1)$ as the $n$ since it involves multiplication. Again. I would appreciate hints or even an explantation of this problem, since I have exercised my resources on YouTube and reading the text for well over a day now.","['induction', 'discrete-mathematics']"
2810751,Differentiate an exponential operator,"I have an operator defined as: $$ \exp \Bigg[  \lambda \ \frac{\partial}{\partial R} \Bigg] $$ I'm trying to optimize this with respect to the parameter $\lambda$. In other words, I'm trying to get an expression for $$ \frac{\partial}{\partial \lambda} \Bigg[\exp \Big[  \lambda \  \frac{\partial}{\partial R} \Big] \Bigg]$$ I read about Sneddon's formula but that doesn't seem to solve the problem. Any suggestions on how I should approach this?","['derivatives', 'operator-algebras', 'chain-rule']"
2810797,Double Hodge star without coordinates.,"The proofs I've found of the fact that $**\alpha=(-1)^{k(n-k)}\alpha$, or, equivalently, the fact that the Hodge star is an isometry, all use an orthonormal basis. Is there a basis-free proof of either of these facts directly from the definition of the Hodge star by $$\alpha\wedge*\beta=\langle\alpha,\beta\rangle\mathrm{vol}?$$","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
2810798,Using Menelaus to show: The perpendicular bisectors of the angles bisectors of a triangle meet opposite sides in collinear points,"Prove that the perpendicular bisectors of the interior angle bisectors of any triangle meet the sides opposite the angles being bisected in three collinear points. Here is what I have so far. In order to show H, G, and I are collinear, I need to show that $$\frac{AH}{HB}\cdot\frac{BI}{BC}\cdot\frac{CG}{GA}=-1$$ This is true by Menelaus's Theorem. Now, I can prove by SAS that $\Delta FJO\cong \Delta CJO$ and by ASA that $\Delta OJC\cong \Delta PJC$ . Thus, by transitivity, we have $\Delta OJC \cong \Delta PJC \cong \Delta OJF$ . Therefore, $\angle PCJ\cong\angle OFJ$ . This implies that $OF\parallel BC$ since the alternate interior angles are congruent. I know that parallel lines divide the sides of the triangle proportionally, so we get $\frac{AF}{FB}=\frac{AO}{OC}$ . Through similar procedures, I can also conclude the following: $FP \parallel AC$ which implies $\frac{BP}{PC}=\frac{BF}{FA}$ and $ME \parallel BC$ which implies $\frac{AM}{MB}=\frac{AE}{EC}$ . I also know there are three simple results from the angle bisector theorem. $$\frac{BD}{AB}=\frac{CD}{AC}$$ $$\frac{AF}{AC}=\frac{BF}{BC}$$ $$\frac{AE}{AB}=\frac{EC}{BC}$$ I also know from Ceva's Theorem that since the angle bisectors are concurrent, then $$\frac{AF}{FB}\cdot \frac{BD}{DC} \cdot \frac{CE}{EA}=1$$ Though that piece of information is not really different from the results of the angle bisector theorem. I need to be able to combine things somehow, but I'm not seeing a connection between what I know and the points H, I, and G.","['euclidean-geometry', 'triangles', 'geometry']"
2810806,Finding surface of the set in $\mathbb{R}^3$,"Given set
$$M \equiv x^2 + y^2 \leq 2z,~z \in [0, 1]$$
find it's surface. Using cylindrical coordinates I'm finding that
$$r^2 = 2z \implies z = \frac{r^2}{2}.$$ Now my transformation is of the form $x = r\cos\phi$, $y = r\sin\phi$ and $z = r^2/2$. Taking my transformation as a vector $\Psi := (x, y, z) = (r\cos\phi, r\sin\phi, r^2/2)$ norm of the normal vector is then $||\partial_r \Psi \times \partial_\phi \Psi|| = ||\mathbf{n}|| = r\sqrt{1 + r^2}$. Hence the surface of the set is
$$S = \int_M 1~dS  =  \int_{0}^{2\pi}d\phi\int_{0}^{1}dr~r\sqrt{1 + r^2} = \frac{2\pi}{3}\left(2\sqrt{2} - 1\right).$$ Is my approach correct or did I made any mistake?","['surface-integrals', 'multiple-integral', 'multivariable-calculus', 'integration', 'surfaces']"
2810811,Reconcile the chain rule with a derivative formula,"I know the chain rule is like this:  $f(g(x)) = f'(g(x))g'(x)$. However, I encountered a derivative with which I cannot reconcile the statement above. Let $F$ be a function of $x$ and $z$, and $z$ is a function of $x$. Then by the chain rule:
$$\frac{dF}{dx} = \frac{\partial{F}}{\partial{x}} + \frac{\partial{F}}{\partial{z}}\frac{\partial{z}}{\partial{x}}$$ I'm not sure how the equation is derived. The second part of the left hand side $\frac{\partial{F}}{\partial{z}}\frac{\partial{z}}{\partial{x}}$ looks similar to the chain rule. I'm not sure where $\frac{\partial{F}}{\partial{x}}$ came from.","['derivatives', 'partial-derivative', 'chain-rule']"
2810870,Can an upper triangular matrix be put in jordan form using upper triangular matrices and permutations only.,"Give an upper triangular matrix A does there always exist $P, P^{-1}$ and $U, U^{-1}$ such that $PUAU^{-1}P^{-1}$ is in Jordan canonical form and U is upper triangular and P is a permutation matrix. I believe the answer is yes as when we have distinct eigenvalues we can place them together and make diagonal block and that block can be diagonalized by an upper triangular matrix so we need only concern ourselves with the repeated eigenvalues case. when we have repeated eigenvalues we can use permutation matrices in a block say $B_j$ where $j=1,...,q$ where q is the number of distinct eigenvalues. if we put with the eigenvalues down the diagonals (which we can) then we consider $(B_j-\lambda )^i e_i$ for $i=1,2,...$  we have a collection of i generalized eigenvectors for each distinct eigenvalue. this will allow us to find n generalized eigenvectors (the distinct case is obvious) where in the $v_i$th vector will have 0's in the all components after the ith entry. I believe this is equivalent to the desired result. Field is obviously $\Bbb C$ arbitrary $n \times n $ matrix. Notice that it is not true for A not upper triangular. Edit!: I guess we could technically do this over an arbitrary field given that it is upper triangular at least over $\Bbb R $ would be ok as all the eigenvalues would already be real if it's in upper triangular form.",['linear-algebra']
2810897,System of 3 variable differential equations with 3 repeating eigen values,"The system I am trying to solve is of this form $x' =ax+5y$ $y' =ay+2z$ $z' =az$ so finding eigen values we solve $
 \begin{bmatrix}
    a-\lambda &5 &0 \\
    0 &a-\lambda &2 \\
    0 &0 &a-\lambda
  \end{bmatrix}
$ and we will get $\lambda_{1,2,3}=a$ How do I continue on from here ? Will the general solution be of the form $Y = c_1K_1e^{at}+tc_2K_2e^{at}+t^2c_1K_3e^{at}$, where $K_1=K_2=K_3$ are the same eigen vectors since same eigen values Note then $K_1=K_2=K_3 = \begin{bmatrix}
    0 \\
    0 \\
    0 
  \end{bmatrix}$ , which isn't what the answer in the book is :(",['ordinary-differential-equations']
2810908,Help me understand these topological properties of linear groups.,"I'm studying Linear groups and already bamboozled after proving that there is a bijective correspondence from $SU_2$to $S^3$ and $SU_2$ can be thought of as the set of unit vectors in the quaternion algebra! [what could be more interesting than visualizing groups? :-)] I have luckily encountered two awesome questions- $1.$ There is no way to make 2-sphere into a group. $2.$ The only spheres on which one can define CONTINUOUS GROUP LAWS are the 1-sphere and 3-sphere ! I don't know how to prove these results, I know basic algebraic topology, need help to know if these results have simple proofs and please share some articles describing more about the geometry of linear groups. Thanks in advance!","['algebraic-topology', 'group-theory', 'linear-groups']"
2810911,"Finding the value of $a+2b+3c$ where $a,b,c$ are roots of a cubic equation.","Let $a$, $b$, and $c$ be positive real numbers with $a<b<c$ such that $a+b+c=12$, $a^2+b^2+c^2=50$, and $a^3+b^3+c^3=216$. Find $a+2b+3c$. I solved it like this:
$$2(ab+bc+ca)=(a+b+c)^2-(a^2+b^2+c^2)$$$$ab+bc+ca=47$$
$$a^3+b^3+c^3-3abc=(a+b+c)(a^2+b^2+c^2-ab-bc-ca)$$$$abc=60$$
S0 $a,b,c$ are solutions of $x^3-12x^2+47x-60=0$ Placing $x=y+4$$$y^3+12y^2+48y+64-12y^2-96y-192+47y+188-60=0$$$$y^3-y=0$$
So $y=-1,0,1$ and $x=3,4,5$. Thus $a=3;b=4;c=5$ and $a+2b+3c=26$ But is there any other way of solving it without actually making a cubic equation and then solving it to get the values of $a,b,c$","['algebra-precalculus', 'cubics', 'polynomials']"
2810921,Show that $\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2}$.,"Could someone please explain a step in the following proof? Show that $\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2}$. The above limit exists if for every $\varepsilon > 0$, there exists a real number $\delta > 0$ such that if $0<|x-1|<\delta$, then $\left |\frac{1}{x+1}-\frac{1}{2}  \right |=\left | \frac{2-x-1}{2(x+1)} \right |=\left | \frac{1-x}{2(x+1)} \right |=\frac{|1-x|}{2|x+1|}=\frac{|x-1|}{2|x+1|}<\varepsilon$. I understand that $|x-1|$ can be made small, but for $|x+1|$, why does the solution say the condition ""if $|x-1| < 1$, $x+1\in (1,3)$""? Why and how is the condition $|x-1| <1$ made?","['epsilon-delta', 'proof-explanation', 'limits']"
2810945,Is real part of a complex differentiable function real differentiable?,"Let $f=u+i v$ be a complex function. Then the real part $u=u(x,y)$ is a multivariable real-valued function. It is well known that if $f$ is holomorphic at $z=x+iy$, then $u$ is $C^1$ near $(x,y) \in \mathbb{R}^2$. I was wondering if $u$ is differentiable at $(x,y)$ when $f$ is complex-differentiable at a point $z=x+iy.$ Would you give me any comment of it. Thanks in advance!","['multivariable-calculus', 'complex-analysis']"
2810982,Find $\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx$,Evaluate $\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx$ I tried using the substitution $u=\frac {\pi}{2}-x$ and maybe thought this function might be symmetrical in some way and got: $$\lim_{n\to\infty}\int_{\frac {\pi}{6}}^{\frac {\pi}{2}}\frac {\cos^nx}{\sin^nx+\cos^nx}dx$$ but it's not really helping me... any other ideas?,"['integration', 'definite-integrals', 'limits-without-lhopital', 'limits']"
2811022,"Comparing some numbers $a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}$","compare $a=2016^\sqrt{2014}, b=2015^\sqrt{2015},c=2014^\sqrt{2016}$ I took 2 functions to solve this question: $f(x)=\sqrt{x+1}\ln(x-1)-\sqrt{x}\ln x$ and $g(x)=\sqrt{x}\ln x - \sqrt{x-1}\ln(x+1).$ $f(x)$ has a local maxima at a point of abscissa $x_1\in(62,63)$ and $\lim_{x\to\infty}f(x)=0$ and $f'(x)<0$ on $[x_1,\infty).\implies f(2015)>0\implies c>b.$ $g(x)$ has a local maxima in the point of absicssa $x_2\in(45,46), \lim_{x\to\infty}g(x)=0$ and $g'(x)<0$ on $[x_2,\infty)\implies g(2015)>0\implies b > a.$ Leaving us with the answer $c > b > a.$ However, I was thinking of a way of verifying this (comparing these numbers) using only $1$ function and I was thinking maybe something like $f(x)=\sqrt{x}\ln(4030+x)...$ but it doesn't really work out... how could i compare these numbers using only one function?","['radicals', 'real-analysis', 'inequality', 'exponential-function', 'calculus']"
2811090,How to calculate the replicating portfolio of a derivative,"Suppose I have a derivative that will pay $\Phi(S_T)=S_T^2$, where $S_t$ evolves as $$dS_t=\mu S_tdt+\sigma S_tdWt$$ where $(W_t)_{t\geq0}$ is a Brownian motion I have shown that its price is $V(S_t,t)=S_t^2e^{(r+\sigma^2)(T-t)}$ Now the question asks from me to calculate the replication portfolio of this derivative and I am not really sure what to do. My idea was the following: Consider a self-financing portfolio with $\phi_t$ shares at time $t$ and $\psi_t$ bonds at time $t$: $$\Pi_t=\phi_tS_t+\psi_tB_t$$ Since it is self financing then we have that $$(S_t+dS_t)d\phi_t+(B_t+dB_t)d\psi_t=0$$ Since it replicates the payoff at time $T$ we have 
$$S_T^2=\phi_TS_T+\psi_TB_T$$ and these $2$ conditions (probably?) should be enough to find $\phi_t, \psi_t$. However, I am struggling to solve this system which makes me think I have a mistake somewhere. Thanks in advance","['derivatives', 'finance']"
2811094,Maximal ideals in a group ring when the ring is a field,"Is there any simple characterization of the maximal ideals in a group ring $R[G]$ when $R$ is a field, perhaps in terms of maximal subgroups of $G$?","['abstract-algebra', 'ring-theory', 'noncommutative-algebra', 'group-theory']"
2811107,Lenght distribution of randomly drawn d dimensional vectors,"Suppose we have a uniform normal distribution $ \mathcal{N}(\mu,\,\sigma^{2})$ where $\mu = 0$ and $\sigma = 1$. $V$ is a set of $d$ dimensional vector such that $V=[v_1,v_2,..,v_\inf]$ $v_i \in R^d$. Each $v_i$ is drawn independently from the unifirm normal distribution. I would like to know the pdf the length of these vectors. I ran a simulation with $d=100$ using 10000 samples and I got the following histogram: It looks like the mean of the length distribution at $d=100$ is around 10. How do I calculate this distribution directly without running the simulation?","['probability', 'statistical-inference', 'geometry']"
2811111,Find the number of spanning trees,"Wo consider the graph $G$ that we get by deleting any edge from the complete bipartite graph $K_{7,8}$. How many spanning trees does the complement graph $\overline{G}$ of $G$ have? I have thought the following: The graph $K_{7,8}$ has $7^{8-1} \cdot 8^{7-1}$ spanning trees. Then $G$ has $6^{7-1} \cdot 7^{6-1}$ spanning trees. Does $\overline{G}$ have $7^{8-1} \cdot 8^{7-1}-6^{7-1} \cdot 7^{6-1}$ spanning trees. But, the result that we get isn't a possible one. So is my idea wrong?","['graph-theory', 'discrete-mathematics']"
2811116,Limit of error function in complex argument,"How to calculate the limit \begin{align*}
\lim_{b\to \infty}\frac{\vert \text{erf}(\sqrt{a+\mathrm{i} b})\vert^{2}}{\sqrt{a^2+b^2}}
\end{align*} where \begin{align*}
\text{erf}(a+\mathrm{i} b)=\frac{2}{\sqrt{\pi}}\int_{0}^{a+\mathrm{i} b}e^{-t^2}~dt
\end{align*} is the error function. The Mathematica shows that as $b$ grows, $\vert \text{erf}(\sqrt{a+\mathrm{i} b})\vert^{2} $ approaches to $1$ and consequently \begin{align*}
\frac{\vert \text{erf}(\sqrt{a+\mathrm{i} b})\vert^{2}}{\sqrt{a^2+b^2}}
\end{align*} becomes very small. If I try to simplify the error function, then I find an expression in terms of $\cos$ and $\sin$ and that diverge. How to proves that the above limit converges to $0$ (as Mathematica shows).","['complex-analysis', 'error-function', 'limits']"
2811118,Number of rearrangements of letters,"We have the letters 'WINPRESENT'. I want to calculate the rearrangements of these letters that contain either the word 'WIN' or the word 'PRESENT' or both of them. I have done the following: The subword 'WIN' is contained in $8!$ rearrangements. The subword 'PRESENT' is contained in $4!$ rearrangements. We have calculated twice 'WINPRESENT' and twice 'PRESENTWIN'. So the total amount of rearrangements that contain either the word 'WIN' or the word 'PRESENT' or both of them is equal to $8!+4!-2$, or not?","['permutations', 'combinatorics', 'discrete-mathematics']"
2811124,Is this a natural transformation?,"Fix a field K and a 1-dimensional vector space $W$, and consider the functor $V\mapsto V\otimes W$ from the category of finite dimensional vector spaces to itself. In the book I am reading the author says $V$ and $V\otimes W$ are isomorphic but not naturally isomorphic. To me that means there is no natural transformation whose components are linear isomorphisms, from the identity functor to the $\otimes W$ functor. Let me fix a non-zero vector $w\in W$, and for each finite dimensional vector space $V$ consider the map $V\to V\otimes W$ given by $v\mapsto v\otimes w$. For any linear map $f\colon V_1\to V_2$ the diagram \begin{array}
& V_1 &\longrightarrow &V_1\otimes W\\
\downarrow & & \ \ \ \ \ \downarrow \\
V_2 & {\longrightarrow} & V_2\otimes W
\end{array} commutes because a vector $v_1\in V_1$ gets sent to $f(v_1)\otimes w$ in both cases, right? Isn't this the definition of a natural transformation? I know ""natural things"" have to do with morphisms being independent of choices, and my linear isomorphisms depend on a non-canonical choice of generator $w\in W$. But I can't see why, in a rigorous sense, the thing I just defined wouldn't be a natural transformation.","['category-theory', 'linear-algebra']"
2811130,Another way to find the sum $S=1-\dfrac{3}{2}+\dfrac{5}{4}-\dfrac{7}{8}+\cdots+(-1)^{n-1}\cdot\dfrac{2n-1}{2^{n-1}}$,"I want to know the sum
$$S=1-\dfrac{3}{2}+\dfrac{5}{4}-\dfrac{7}{8}+\cdots+(-1)^{n-1}\cdot\dfrac{2n-1}{2^{n-1}}.$$ This is my way.
First, we find the sum
$$1 + 3 x + 5x^2 + \cdots + (2n-1) \cdot x^{n-1}=\sum _{k=1}^n (2 k-1) x^{k-1}.$$
We have
$$\sum _{k=1}^n (2 k-1) x^{k-1} = 2\sum _{k=1}^n k\cdot x^{k-1}-\sum _{k=1}^n  x^{k-1}. $$
Note that
$$\sum _{k=1}^n k\cdot x^{k-1} = \left (\sum _{k=1}^n x^k\right)' = \left (\dfrac{x\left(x^n-1\right)}{x-1}\right)'=\dfrac{n x^{n+1}-(n+1)x^n+1}{(x-1)^2}.$$
Another way
$$\sum _{k=1}^n  x^{k-1} = \dfrac{x^n-1}{x-1}.$$
From the above results, we have
$$1 + 3 x + 5x^2 + \cdots + (2n-1) \cdot x^{n-1}= \dfrac{(2 n-1) x^{n+1}-(2 n+1)x^n+x+1}{(x-1)^2}.$$
With $x=-\dfrac{1}{2}$,we get
$$S=\dfrac{2^n + (-1)^{n+1} \cdot(6n+1)}{9 \cdot 2^{n-1}}.$$
How to find the result with another way?","['summation', 'sequences-and-series', 'calculus']"
