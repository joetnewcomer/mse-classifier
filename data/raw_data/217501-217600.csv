question_id,title,body,tags
4434581,"Find $x,y,z$ satisfying $x(y+z-x)=68-2x^2$, $y(z+x-y)=102-2y^2$, $z(x+y-z)=119-2z^2$","Solve for $x,y,z$ : $$x(y+z-x)=68-2x^2$$ $$y(z+x-y)=102-2y^2$$ $$z(x+y-z)=119-2z^2$$ After some manipulation, I obtain $$xy+xz=68-x^2$$ $$yz+xy=102-y^2$$ $$xz+yz=119-z^2$$ After combining equations, I get $$y=\frac{-51-x^2+z^2}{x-z}$$ This seems too tedious. Is there a simpler way?","['algebra-precalculus', 'systems-of-equations', 'problem-solving']"
4434599,A Counterexample to the Mean Value Theorem for Integrals (Complex Case),"The problem at hand is to prove the following ""counterexample"" to the Mean Value Theorem: Let $f:\left[0, 2\pi\right] \to \mathbb{R}$ be Riemann-integrable, $f(x) \geq 0 \ \forall \, x \in \left[0,2\pi\right]$ and $\int_{0}^{2\pi} f(x) \text{d}x > 0$ . Prove there exists a continuous function $g: \left[0, 2\pi\right] \to \mathbb{C}$ , such that for all $\xi \in \left[0, 2\pi\right]$ it holds: $$\int_{0}^{2\pi} f(x)g(x)\text{d}x \neq g(\xi)\int_{0}^{2\pi} f(x)\text{d}x$$ . Own thoughts and tries: Naturally, we notice that $g(x)\in \mathbb{C} \ \forall \, x \in \left[0,2\pi\right]$ and this already suggests an idea. For example, we know that $e^{ix} \neq 0$ for all $x \in \left[0, 2\pi\right]$ and still $\int_{0}^{2\pi} e^{ix} \text{d}x = 0$ . Therefore, it would suffice to find $g : \left[0, 2\pi\right] \to \mathbb{C}$ such that $g(\xi) \neq 0 \ \forall \, \xi \in \left[0, 2\pi\right]$ and for which $\int_{0}^{2\pi} f(x)g(x)\text{d}x = 0$ . So I imagine that multiplication by $g$ could result in $fg$ being shifted in a way such that ""half of the integrand is below zero"". However, we still need to use the fact that $f(0)=f(2\pi)$ and I haven't got much of an idea how to utilize that. Also, there is no condition on the sign of $g$ and so boundedness claims from Riemann-integrability don't look like much help either. Any enlightening hint or advice would be much appreciated. Ideally, it would guide towards the solution without revealing it completely (just to help get off the saddle point). UPDATE: Actually, knowing $f(0)=f(2\pi)$ allows us to continue $f$ on $\mathbb{R}$ by means of $2\pi$ -periodisation. Identifying the continuation of $f$ with $f$ , write $\int_{0}^{2\pi} f(x)\text{d}x = \int_{-\pi}^{\pi} f(x)\text{d}x > 0$ . Further, if we assume that g is $C_{2\pi}^{0}(\mathbb{R})$ (i.e. continuous and $2\pi$ -periodic on $\mathbb{R}$ ) and even (i.e. $g(x)=g(-x)$ for all $x\in\left[0,2\pi\right]$ ), we can rewrite the integral as (since (fg) is also $2\pi$ -periodic): $$\int_{0}^{2\pi} f(x)g(x)\text{d}x = \int_{-\pi}^{\pi} f(x)g(x)\text{d}x = \int_{-\pi}^{\pi} f(x)g(-x)\text{d}x =(f * g)(0)$$ where $(f*g)$ denotes the convolution of the functions. Following this reasoning, it suffices to find $g \in C_{2\pi}^{0}(\mathbb{R})$ - even, such that $(f*g)(0) = 0$ and $g(x) \neq 0$ for all $x \in \left[0,2\pi\right]$ . Any ideas (perhaps using the Convolution theorem)?","['complex-analysis', 'mean-value-theorem', 'fourier-analysis']"
4434719,Finding the Inverse of a Matrix using Row Operations,"Problem: Let $A$ be the following matrix. Find $A^{-1}$ . $$ \begin{bmatrix}
	-1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9 \\
\end{bmatrix} $$ Answer: \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	4 & 5 & 6 & 0 & 1 & 0 \\
	7 & 8 & 9 & 0 & 0 & 1 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13 & 18 & 4 & 1 & 0 \\
	0 & 22 & 30 & 7 & 0 & 1 \\
\end{bmatrix} \\
\end{align*} Now I multiply the second row by $-22$ and the third row by $13$ . \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 22(13) & 390 & 91 & 0 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & -6 & 3 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} Now we multiply the third row by $396$ and add it to the second row. \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -286 & 2156 & -858 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} Now we need to work on the first row. \begin{align*}
\begin{bmatrix}
	-1 & 0 & 3 & 1 & \dfrac{2(98)}{13} & \dfrac{-2(66)}{13} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 0 & 3 & 1 & \dfrac{196}{13} & -\dfrac{132}{13} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} I have reason to believe the correct answer is: $$  \begin{bmatrix}
	-\dfrac{1}{2} & 1 & -\dfrac{1}{2} \\
	1 & -5 & 3 \\
	-\dfrac{1}{2} & \dfrac{11}{3} & -\dfrac{13}{6} \\
\end{bmatrix} $$ As such I am confident I made a mistake. Where did I go wrong? Based upon the feed back I got I updated my solution. Here is an updated but still wrong solution. \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	4 & 5 & 6 & 0 & 1 & 0 \\
	7 & 8 & 9 & 0 & 0 & 1 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13 & 18 & 4 & 1 & 0 \\
	0 & 22 & 30 & 7 & 0 & 1 \\
\end{bmatrix} \\
\end{align*} Now I multiply the second row by $-22$ and the third row by $13$ . \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 22(13) & 390 & 91 & 0 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & -6 & 3 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} Now we multiply the third row by $396$ and add it to the second row. \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -286 & 2156 & -858 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} Now we need to work on the first row. \begin{align*}
\begin{bmatrix}
	-1 & 0 & 3 & -1 & -\dfrac{ 98}{26} & -6 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & -3 & 1 & \dfrac{ 49}{13} & 6 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & 0 & 1 -\frac{3}{2} & \dfrac{ 49}{13} + 3 & 6 - \frac{3(13)}{6} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & 0 & -\frac{1}{2} & 9 & - \frac{1}{2} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} Where did I go wrong?","['matrices', 'linear-algebra', 'inverse']"
4434773,How would you describe the symmetries/group actions on the $\ell_p$ circle?,"We define the unit circle as the collection of all vectors with length 1 centered at some point. (The one below specifically defines the unit circle centered at the origin) $$\mathscr{C} = \{ x\in\mathbb{R}^2 \colon \|\|x\|\| = 1  \}.$$ In this case, the norm used is the Euclidean norm $p = 2$ . But we may use the $\ell_p$ metric for $p\in[1,\infty)$ where $$\|\|x\|\|_p = \left(\|x_1\|^p + \|x_2\|^p\right)^{1/p}.$$ We use these metrics to generalize the unit circle as the $\ell_p$ unit circles: $$\mathscr{C}_p = \{ x\in\mathbb{R}^2 \colon \|\|x\|\|_p = 1 \} .$$ I have depicted some unit circles from $p\in[1,\infty)$ . $\ell_p$ Unit Circles"" /> My question is, what would the group action be on these circles? Of course, the Euclidean circle is part of $O(2)$ , but that falters when $p$ deviates from 2 (is this trivial since orthogonal groups preserve the Euclidean norm only?) We see that there are lines of symmetry about $y = 0$ , $y=x$ , $y=-x$ , $x=0$ . I'm new to group theory, but would we say that the dihedral group $Dih_4$ acts on $\mathscr{C}_p$ ? Richter provides generalized trigonometric functions for $\ell_p$ metric spaces. Specifically, $$\cos_p(\theta) = \frac{\cos(\theta)}{N_p(\theta)}$$ $$\sin_p(\theta) = \frac{\sin(\theta)}{N_p(\theta)}$$ where, $N_p(\theta) = \left(\|\cos(\theta)\|^p + \|\sin(\theta)\|^p\right)^{1/p}.$ These generalized trigonometric functions can also parametrize the curve  of $\mathscr{C}_p$ as $r_p(\theta) = (\cos_p(\theta), \sin_p(\theta))$ . Certainly if $N_p(\theta) = 1$ , then we have our Euclidean trigonometric functions. I created a contour plot ( $p$ vs. $\theta$ ) for what angles $N_p(\theta) = 1$ is valid below. $N_p(\theta) = 1$ for $p$ vs $\theta$ "" /> The plot roughly shows that when $p\neq2$ this occurs when $\theta \in \{0, \pi/2, \pi, 3\pi/2\}$ which conforms with the lines of symmetry above. How do I concretely prove that these are the only lines of symmetry and demonstrate/describe the valid group action on $\mathscr{C}_p$ ? EDIT : I know that my contour plot is also trying to demonstrate that the determinant of this matrix is equal to 1, $R=\begin{bmatrix}\cos_p(\theta)&-\sin_p(\theta)\\\cos_p(\theta)&\sin_p(\theta)\end{bmatrix}$ or that $R^T = R^{-1}$ iff $\det(R) = 1$ .","['noneuclidean-geometry', 'dihedral-groups', 'geometry', 'metric-spaces', 'group-theory']"
4434788,Is there always a mapping between probability measures on the $n$-sphere?,"If $\mu,\nu\in\mathcal{P}(\mathbb{S}^{n-1})$ for $n\in\mathbb{N}$ . Can we always find a measurable mapping $f:\mathbb{S}^{n-1}\to\mathbb{S}^{n-1}$ such that $\mu=f\#\nu$ , or equivalently $$\int_{\mathbb{S}^{n-1}} gd\mu=\int_{\mathbb{S}^{n-1}} gd(f\#\nu)=\int_{\mathbb{S}^{n-1}} g\circ fd\nu$$ where $f\#\nu$ stands for the push-forward and $g\in\mathcal{L}^1(\mathbb{S}^{n-1})$ ? If not, what are the sufficient conditions for it to be true?","['pushforward', 'measure-theory', 'probability']"
4434821,"Prove that $\int_0^1\sqrt{f(x)} dx \le \frac{\pi\sqrt 5}{8}$ where $f(x) + f((1 - \sqrt{x})^2) \le 1, \forall x \in [0; 1]$.","Consider function $f$ which is continuous on the closed interval $[0; 1]$ such that $f(x) > 0, \forall x \in [0; 1]$ and $f(x) + f\left((1 - \sqrt{x})^2\right) \le 1, \forall x \in [0; 1]$ . Prove that $\displaystyle \int_0^1\sqrt{f(x)}\, \mathrm dx \le \dfrac{\pi\sqrt 5}{8}$ . Below is an attempt of mine at solving this problem. One question, why is $\pi$ there, just whhyyy~? (Welp, restarting in 3... 2... 1...) We have that $$\begin{aligned} \left(\sqrt{f(x)} + \sqrt{f\left((1 - \sqrt{x})^2\right)}\right)^2 \le 2\left[f(x) + f\left((1 - \sqrt{x})^2\right)\right] &\le 2\\ \iff \sqrt{f(x)} + \sqrt{f\left((1 - \sqrt{x})^2\right)} &\le \sqrt 2, \forall x \in [0; 1]\\ \iff \int_0^1\sqrt{f(x)}\, \mathrm dx + \int_0^1\sqrt{f\left((1 - \sqrt{x})^2\right)}\, \mathrm dx &\le \sqrt 2\\ \iff \int_0^1\sqrt{f(x)}\, \mathrm dx + \int_1^0\dfrac{1 - \sqrt{x}}{\sqrt{x}}\sqrt{f(x)}\, \mathrm dx &\le \sqrt 2\\ \iff \int_0^1\dfrac{2\sqrt{x} - 1}{\sqrt{x}}\sqrt{f(x)}\, \mathrm dx &\le \sqrt 2\end{aligned}$$ I'll add more thoughts as time goes on, but this is all for now, thanks for reading (and more if you could help~)!","['definite-integrals', 'ordinary-differential-equations']"
4434822,Generating Function Approach giving wrong combination count while normal brute force casework approach giving correct answer,"I am solving a problem: How many words are less than four letters long and contain only the letters A, B, C, D, and E? Here, 'word' refers to any string of letters. My Solution 1 uses Generating functions gives wrong answer $(1+x+x^2+x^3)(1+x+x^2+x^3)(1+x+x^2+x^3)(1+x+x^2+x^3)(1+x+x^2+x^3)$ where $(1+x+x^2+x^3)$ is Generating function for each letter. This approach gave a wrong answer $(35+15+5+1)$ which is the sum of coefficients of $(x^3, x^2, x, 1)$ in $$x^{15} + 5 x^{14} + 15 x^{13} + 35 x^{12} + 65 x^{11} + 101 x^{10} + 135 x^9 + 155 x^8 + 155 x^7 + 135 x^6$$ $$+ 101 x^5 + 65 x^4 + 35 x^3 + 15 x^2 + 5 x + 1$$ Correct Approach using Case Work gives the correct answer Case 1: The word is one letter long. Clearly, there are $5$ of these words. Case 2: The word is two letters long. Constructing the set of these words, there are $5$ options for the first letter and $5$ options for the second letter, so there are $5^2 = 25$ of these words. Case 3: The word is three letters long. By similar logic as above, we have $5$ options for the first letter, $5$ options for the second, and $5$ options for the third. Then there are $5^3 = 125$ of these letters. Adding all our cases up, there are $5 + 25 + 125 = 155$ words that are less than four letters long and contain only the letters A, B, C, D, and E. Could Someone help me in explaining why the Generating function approach failed here?","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4434850,Does the electrostatic potential have a local maximum on the sphere?,"Let $$M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\} $$ $M$ is an open subset of $( \mathbb{S}^2)^4$ . Let $E:M \to \mathbb{R}$ be defined by $$E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|},$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . Question: Does $E$ have a point of local maximum? Clearly $E$ doesn't have a global maximum, since it's unbounded. Intuitively, given any point $p \in M$ , we can move some of the $x_i$ closer to each other, thus increasing the energy. However, when we move say $x_2$ closer to $x_1$ , we might be pushing it further away from $x_3$ or $x_4$ , so there are ""competing"" changes in the contribution of each $\| x_i - x_j \|^{-1}$ term. Is there an easy way to see that one can always choose a direction where $E$ strictly increases?","['physics', 'maxima-minima', 'multivariable-calculus', 'optimization', 'differential-geometry']"
4434856,Any geometric interpretation for the adjoint system of a linear dynamical system?,"On page 26, Section 1.3, of his book on linear dynamical systems 1 , Professor Roger Brockett asks: If $$\dot{\mathbf{x}}(t) = A(t) x(t) , \qquad \mathbf{x}(0) = \mathbf{x}_0$$ and $$\dot{\mathbf{p}}(t) = -A^T(t) \mathbf{p}(t), \qquad \mathbf{p}(0) = \mathbf{p}_0 \quad (\mbox{Adjoint System}),
$$ show that $$ \langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \quad\mbox{for all} \ t \in
\mathbf{R}
$$ My take An easy way to show that a certain function $\psi(t)$ is a constant function is to show that $\dot{\psi}(t) \equiv 0$ . Thus, we find that $$
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, =  \, \langle \dot{\mathbf{x}}(t), \mathbf{p}(t) \rangle
+ \langle \mathbf{x}(t), \dot{\mathbf{p}}(t) \rangle 
$$ That is, $$
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = \, \langle A(t) \mathbf{x}(t), \mathbf{p}(t) \rangle
+  \langle \mathbf{x}(t), - A^T(t) \mathbf{p}(t) \rangle
$$ Simplifying, we get $$
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = - \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) +  \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) \equiv 0
$$ This shows that $$
\langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \ \ \mbox{for all} \ t \in
\mathbf{R}
$$ I hope that the calculations are correct. I would like to learn more on the adjoint of a linear dynamical system. Is there any geometric interpretation for the adjoint of a linear dynamical system and the identity established in the Brockett's exercise problem? References Roger W. Brockett, Finite Dimensional Linear Systems , Wiley, 1970.","['ordinary-differential-equations', 'control-theory', 'hilbert-spaces', 'linear-control', 'geometric-interpretation']"
4434857,Is Probability Measure always tight?,We know that probability measures are tight if the metric space is separable and complete. Here tight means there exists a compact set in that metric space say $K$ such that $P(K) > 1- \epsilon$ . I want to create a probability measures which is not tight. For that we have to violates the separability or completeness condition. Suppose we violates separability. And consider the space $l_{\infty}$ with respect to supremum norm. We know that this space is not separable. But how to construct an probability measures here? If we violates completeness then also this holds. As a example $c_{00}$ space is not complete. Then how to construct probability measures there? Any kind of simple examples are appreciated. I know there is some explanation and example available in the stack exchange and those are talking about 'left limit topology' kind of things. I need an simple example and construction not that much advance that's why asked this question.,"['measure-theory', 'weak-convergence', 'separable-spaces', 'metric-spaces', 'probability-theory']"
4434967,Cantor-Schröder-Bernstein theorem for rings of sets,"If $(X,\mathcal{F})$ and $(Y,\mathcal{G})$ are measurable spaces and there are bimeasurable injections $f \colon X \to Y$ and $g \colon Y \to X$ , then $X$ and $Y$ are isomorphic. This is because the usual way to construct a bijection for the usual Cantor-Schröder-Bernstein theorem gives you a bimeasurable bijection, see this post for the construction I have in mind. The proof that this bijection is bimeasurable if $f$ and $g$ are seems to rely on the fact that $\mathcal{F}$ and $\mathcal{G}$ are $\sigma$ -algebras, as opposed to more restrictive collections of sets. Say we restrict ourselves to what you might call pseudomeasurable spaces (please tell me if there is a more common name for such a structure), where $\mathcal{F}$ and $\mathcal{G}$ are taken to be mere rings of sets (so a collection of subsets closed under binary union and set difference). Does the Cantor-Schröder-Bernstein property still hold in this case? If not, what are nice examples of spaces that mutually inject but are not isomorphic such that $\mathcal{F}$ and $\mathcal{G}$ either a: are not closed under countable unions or b: do not contain the entire set?","['measurable-sets', 'measure-theory']"
4434973,"Lévy process, characteristic function, $\frac{\psi(\xi)}{i \xi}$","Given a one dimensional Lévy process $X_t$ with characteristic exponent $\psi(\xi)$ , so that \begin{align}
\mathbf{E}[e^{i \xi X_t}]= e^{t \psi(\xi)}
\end{align} for example we can find $\psi(\xi)$ from Lévy–Khintchine representation. Now I'm looking for the object that has characteristic function $\frac{\psi(\xi)}{i \xi}$ , do you know what could it be? For example in the case of subordinators we can find it easily from the Lévy measure, but for a generale one dimensional Lévy process? I think that it easy in the case such that \begin{align}
\mathbf{E}[e^{i \xi X_t}]= e^{t \int_{-\infty}^\infty (e^{i \xi y} - 1) \nu(dy)} \quad (1)
\end{align} for which Lévy processes (1) is true? Do you in general $\frac{\psi(\xi)}{i \xi}$ of what it can be the characteristic function? Thank you!","['levy-processes', 'characteristic-functions', 'measure-theory', 'probability-theory']"
4434975,"Finding the minimum value of $\frac{p}{q-r}$, where $(-1,p)$, $(0,q)$, $(1,r)$ lie on parabola $y=ax^2+bx+c$, with certain conditions","If the vertex of the parabola $$y=ax^2+bx+c \qquad (0<2a<-b)$$ is not below the x-axis.  Let there be three points on the parabola: $A(-1,p), B(0,q)$ and $C(1,r)$ . Then what is the minimum value of $\dfrac{p}{q-r}$ ? Hint: the answer is $3$ . My thoughts: Since the axis of symmetry of the function is $x=-\dfrac{b}{2a}>1$ and $\Delta=b^2-4ac<0$ , $\dfrac{p}{q-r}=-\dfrac{a-b+c}{a+b}$ ,  I thought of solving it by the image of the function. But it seems difficult to find the answer only through the limited conditions.","['maxima-minima', 'algebra-precalculus', 'functions']"
4434981,Strong maximum principle applied to a complete asymptotically flat manifold,"Pardon me for not typing the following snapshot in MathJax. I'm sorry. This picture is an excerpt from Geometric Relativity by Dan A. Lee. I was wondering about the statement circled in red. How did the author infer from the strong maximum principle that $0<u<1$ everywhere? Indeed, we are seeking a positive function $u$ that serves our need in this proof, and since $v$ is not the zero vector, we see that $u$ cannot be $1$ identically. But how was it combined with the principle to give the fact that $u$ is bounded above by $1$ ? I need some help. Thank you so much.","['partial-differential-equations', 'riemannian-geometry', 'differential-geometry']"
4435001,"A sequence of sums: $\sum_{i=1}^n i$, $\;\sum_{j=1}^{n-1}(j\sum_{i=j+1}^n i)$, $\;\sum_{k=1}^{n-2}(k\sum_{j=k+1}^{n-1}(j\sum_{i=j+1}^n i))$, $\ldots$","I'm working with a sequence of summations, as follows: $$
S_1(n) = \sum_{i=1}^n i
$$ $$
S_2(n) = \sum_{j=1}^{n-1} \left( j \sum_{i=j+1}^n i \right)
$$ $$
S_3(n) = \sum_{k=1}^{n-2} \left( k \sum_{j=k+1}^{n-1} \left( j \sum_{i=j+1}^n i \right) \right)
$$ and the pattern continues. Obviously, as the index of the sequence increases, the more summations we have in the expression. But is there a way to express this compactly, as in with only one summation for each index, or even a closed formula? For example, suppose we want to know the expression for $S_{100}$ . By the examples I've given, that would include 100 $\sum$ symbols. Is there a shorter way to write this? I'd appreciate any help. Update (5/1/2022): I've been working on this ever since I originally posted this question. So far, I've gotten to: $$
S_p \left( n \right) = \frac{f \left( n, p \right)}{g \left( n, p \right)} \cdot \frac{\Gamma \left( n + 1 \right)}{\Gamma \left( n - p + 1 \right)}
$$ where $f \left( n, p \right)$ is some polynomial of $n$ of highest degree $p$ and $g \left( n, p \right)$ is an integer given by the following function: $$
g \left( n, p \right) = \prod_i^n i^{\sum_{j=0}^{\infty} \left \lfloor \frac{n-1}{i^j \left( i - 1 \right)} \right \rfloor}
$$ where $i$ is prime. I'm still trying to decipher the sequence of polynomials given by $f \left( n, p \right)$ . Update (5/13/2022): I'm still working on this problem. I do realize that since both $f$ and $g$ are dependent on $n$ and $p$ , one could combine them into one function, but I prefer working with integers rather than rationals. Plus, we already have a function for $g$ (although, not a pleasant one). And the coefficients of $f$ are integers. In fact, I did figure out the first ten polynomials of $f$ in terms of $n$ . There's not much of a pattern, except if we separate the sequence into subsequences where $p$ is odd or even, then the coefficient of the leading term never decreases. Also, the constant term for when $p$ is odd (except when $p = 1$ ), tends to always be zero. When $p = 1$ , the constant term is $1$ . Update (6/3/2022): As I do more research on this problem, I came across Faulhaber's formula for the sum of the same powers of consecutive integers from $1$ to $n$ . This was helpful in some ways, but not in other ways. It allowed me to convert a sum of integers to the power $p$ into a polynomial in terms of $n$ , but the issue here is that it still uses the $\sum$ symbol and includes Bernoulli numbers, which have no closed formula (to my knowledge). Hence, even if I used Faulhaber's formula, the formula is still not closed in terms of $n$ and $p$ . On the plus side, I have noticed a pattern on how to find a formula for power $p$ , but it relies on knowing the closed formula for power $p - 1$ in terms of $n$ and is very convoluted, even using Faulhaber's formula. I'm hoping I can construct something using linear algebra maybe, like with the closed formula for the Fibonacci sequence. In fact, while doing so, I derived a closed formula for what I like to call the ""product of consecutive Gaussian sums"", which is the following: $$
\prod_{j=0}^{p-1} \sum_{i=1}^{n-j} i = \frac{\left( p! \right)^2}{2^p} \binom{n}{p} \binom{n+1}{p}
$$ I'm not sure if this has been figured out before, but I thought it was an interesting consequence that came from this research. Again, any input would greatly be appreciated.","['algebra-precalculus', 'stirling-numbers', 'discrete-mathematics']"
4435031,Tensor fields defining $G$-structure are parallel,"Suppose $G \leq GL_n(\mathbb{R})$ is the stabilizer of some tensors $T^0_1, ..., T^0_k$ , let $P$ be a $G$ -structure on a manifold, i.e. a principal $G$ subbundle of the frame bundle of $M$ and let $T_1, ..., T_k$ be tensor fields that are pointwise the image of $T^0_1, ..., T^0_k$ through the frames of $P$ , as in this question.
Is it true that the tensor fields $T_1, ..., T_k$ are parallel with respect to any given connection on $P$ ? If you want add the hypothesis of the connection on $P$ being torsion-free (meaning the induced connection on the tangent bundle is) but I believe this is not needed.
Alternatively, is it true that if the holonomy of a Riemannian manifold is contained in $G$ then the tensors defining (in the sense of the linked question) the $G$ structure are parallel? By looking at Berger's classification it looks true. I guess that one way to go would be to argue that the parallel transport of a generic tensor on $M$ is characterized by the principal bundle this way: take a tensor at a point, pull it back through a frame at that point getting a tensor on $\mathbb{R}^n$ , transport the frame to the point you want and finally push forward the tensor through the new frame.
However I am not at all familiar with parallel transport on induced vector bundles to argue that this is true.","['principal-bundles', 'connections', 'tensors', 'holonomy', 'differential-geometry']"
4435064,"How can I prove that, if $x,y,z>0$ and $xyz=1$, then $2(x^2+y^2+z^2)+9\geq 5(x+y+z)$","How can I prove that, if $x,y,z>0$ and $xyz=1$ , then $$2(x^2+y^2+z^2)+9\geq 5(x+y+z)$$ I used the famous inequality $$x^2+y^2+z^2+3\geq 2(x+y+z)$$ I got $$2(x^2+y^2+z^2)+9\geq 4(x+y+z)+3\geq 5(x+y+z)$$ But, the last inequality gives $x+y+z\leq3$ which is not correct.","['contest-math', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
4435069,Why can we use fundamental vector fields for vertical vector fields to prove the form of the curvature of a connection?,"There's a standard proof that roughly goes that, to prove the equivalence for a connection form $\omega$ on a $G$ -principal bundle $P$ ( $u, v \in \Gamma(TP)$ ): \begin{eqnarray}
\Omega(u,v) &=& d\omega(\mathrm{Hor}(u), \mathrm{Hor}(v))\\
&=& d\omega(u,v) + \frac{1}{2} \left[ \omega(u), \omega(v) \right]
\end{eqnarray} we can simply do it for horizontal and vertical vector fields, as any vector field can be decomposed thusly. This proof is in Kobayashi & Nomizu, it's here , it's here , etc. But for the vertical part, the proof assumes (""without loss of generality"", according to one source) that we can pick a fundamental vector field instead of a more general vertical vector field. The first part of the equality works without that choice, since $\mathrm{Hor}(u)$ is always zero for any vertical field, but for the second part, we are meant to pick $X, Y \in \mathfrak{g}$ and then use as vertical vectors $u = X^*$ , $v = Y^*$ , the fundamental vector fields based on the element $X$ and $Y$ . Part of the proof relies then on the derivative of the connection : \begin{eqnarray}
d\omega(X^*,Y^*) &=& X^*(\omega(Y^*)) + Y^*(\omega(X^*)) - \omega([X^*, Y^*])\\
&=& X^*(Y) + Y^*(X) - [X, Y]
\end{eqnarray} The term $X^*(Y) + Y^*(X)$ is then supposed to vanish, from what I have seen , due to $X, Y$ being a constant function $P \to \mathfrak{g}$ , and therefore zero when applied to a vector field as a differential operator. I can understand why it would make sense at a point to consider the value of a vertical vector field as the same as that of a fundamental vector field, but if there are derivatives involved, then the Lie algebra element associated to that field may be different at a nearby point, and the derivative may not vanish. From here , the set of fundamental vector fields very much do not cover the entire space of vertical vector fields. So what is the justification that there is no loss of generality in using fundamental vector fields here?","['connections', 'lie-algebras', 'differential-geometry']"
4435155,Solve the following differential equation: $xy''-\cos(x)y'+\sin(x)y=2$,"Solve the following differential equation: $$xy''-\cos(x)y'+\sin(x)y=2$$ We have that $f_1(x)=x,f_2(x)=-\cos(x),f_3(x)=\sin(x)$ And since $f''_1(x)-f'_2(x)+f_3(x)=0$ so the second order differential equation is exact, hence we should solve: $$\frac{d}{dx}\left[f_{1}\left(x\right)y'+\left(f_{2}\left(x\right)-f'_{1}\left(x\right)\right)y'\right]=r\left(x\right)$$ Or equivalently $$xy'+\left(-\cos\left(x\right)-1\right)y=2\int_{ }^{ }dx$$ From which we conclude $$y'+\frac{\left(-\cos\left(x\right)-1\right)}{x}y=2+\frac{c_{1}}{x}$$ So $$y=e^{-\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}\left(\int_{ }^{ }\left(2+\frac{c_{1}}{x}\right)e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}dx+c_{2}\right)$$ But $$e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}$$ doesn't have a closed form, So what should I do?",['ordinary-differential-equations']
4435177,A Proof with no words that $\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}=2$,"Question What are the words to describe the method in the image below? (from Nelsen's Proofs without Words II ) Attempt I was thinking and could define the sequence $u_1=2; u_{n+1}=f\circ g^{−1}(u_n)$ where $f(x)=\sqrt x$ and $g(x)=x−2$ , as suggested by image, and thus, by the graph, it suggests that the succession defined in this way, is increasing and that $u_n<2$ , therefore increased, which was concluded to be convergent. Once it converges, let $l\in\mathbb R$ be its limit. As $\lim u_n=\lim u_{n+1}$ , since for the limit we are only interested in terms starting from a certain order, and that $f\circ g^{−1}$ is a continuous function, a composition of continuous functions is continuous, in the respective domains, we can conclude $\lim u_{n+1}=\lim f(g^{−1}(u_n))\iff l=\sqrt{l+2}$ and thus $l=−1$ or $l=2$ , but $−1$ does not belong to the domain of the function. What do you think ? Am I complicating? I believe there will be an easier way out, although that's the idea. Thanks in advance.","['nested-radicals', 'real-analysis', 'proof-without-words', 'cobweb-diagram', 'dynamical-systems']"
4435199,I've recently discovered DCT and I'm wondering how one would solve this limit : [duplicate],"This question already has answers here : Evaluate $\lim_{n\to \infty}n\int_2^e{(\ln x)^n}dx$ (2 answers) Closed 2 years ago . The limit: $$ \lim_{n \to \infty}  n\int_{2}^{e} [\ln(x)]^n \mathrm dx $$ The book from where I took this exercise offered these as possible results : $e$ $0$ $1$ $\ln(2)$ infinity I was able to pinpoint the solution ( $e$ ) by substituting $x=e^t$ and then building the integral from $\ln(2)\le t\le1$ , and in the end reaching to $2\le n\int_{2}^{e} [\ln(x)]^n \mathrm dx \le e$ . The only solution possible, considering the answers the book gave, was $e$ . The problem is, I still didn't solve the problem. I am still not sure how to reach the correct result so in my attempt to find a way, I discovered DCT. I tried to understand as much as I can but there are still a lot of empty gaps. In order to use DCT, I tried bringing the $n$ inside the integral and then use the substitution $t=(\ln(x))^n$ which gives us $ \lim_{n \to \infty}  \int_{(\ln(2))^n}^{1} t^\frac{1}{n} \mathrm dt $ . Because the lower bound depends on $n$ , I basically got stuck. So with all of this said, is there a way to use DCT to solve this limit?","['integration', 'limits', 'definite-integrals']"
4435214,Let $a\gt 2 $ be a root of the equation $x^3-x-8 = 0$. Compute $\sqrt[3]{6a^2-13a} + \sqrt[3]{6a^2+13a+16}$,"I came across this question in some past contest prep papers belonging to Akdeniz University. To solve it, I've tried letting the cuberoots equal some $k$ and $l$ and tried writing $k+l$ in terms of $kl$ and $k^3+l^3$ but to no avail. Any help would be appreciated.","['contest-math', 'algebra-precalculus']"
4435220,an extension of an Olympiad problem,"I'm a high school student and I'm currently working on my math extended essay. I chose an Olympiad question which is: A piece of land of a square shape with dimensions 10m x 10m is divided into 100 square parcels with dimensions 1m x 1m. Initially, 9 of the parcels are overgrown by weed. If a parcel is surrounded by at least 2 parcels with weed from its sides after some time that parcel will be overgrown by weed. Can the whole piece of land grow into weed after some time? I already have a rough idea on how to solve the original question, but my essay involves an extension to 3D, namely like this: A space of land of a cube shape with dimensions 10m x 10m x 10m is divided into 1000 cubes with dimensions 1m x 1m x 1m. What is the minimum numbers of cubes to begin with so that the whole cube would grow into weed after some time, given that if a cube is surrounded by at least two other cubes with weed it will also be overgrown by weed? After some thinking I have a hypothesis (which I have no idea whether it's correct or not). My approach start with 2D, for it is proven in the first part (somehow) that for a n x n square to grow into weed, n parcels are needed to begin with. Simplifying to a 3 x 3 x 3 cube, that means at least 3 cubes are needed for one side to be overgrown. Let the side be (x, y, z1). Now just a random cube in layer (x, y, z3) can lead to the whole cube being overgrown. I also made an expression for this: c = n+z*, where z* means n/2 rounding backwards (for example 2.5 to 2) and c is the number of cubes needed. It's a bit messy but well. The problem is, I don't know how to prove this hypothesis mathematically. The proving methods I learned like induction, direct proof etc. doesn't really apply here, which is why I'm stuck. Could you maybe point out some directions for me so that I can do some research and try proving it? Or, if it is impossible to prove, please also inform me so that I can change my plan. If it's solvable (which to me isn't right now), please don't post the solution because if so I would have to find another topic to do for my essay. I just want to know some proving methods which might be helpful in this case, thank you!","['contest-math', 'solution-verification', 'logic', 'geometry']"
4435254,Law of Maximum for Brownian motion before hitting 0,Let $W$ be a Brownian motion started at $a>0$ and let $T_{0}$ be the first time $W$ hits $0$ . The goal is to find the law of $\sup _{t \leq T_{0}} W_{t}$ . My initial attempt is as follows: Reflection principle helps establish the law of $\sup_{s\leq t} W_s$ for fixed $t$ . Then we can condition on the $T_0$ and use law of total probability. Is it a valid argument? I appreciate either hints or solution to this problem. Thank you!,"['brownian-motion', 'probability-theory']"
4435291,"Coupon collectors problem, but kicking a ball around","The coupon collectors problem asks about the distribution of the number of coupons you'd need to collect in order to complete a collection. The coupons can have equal probabilities or unequal probabilities (in general). This is covered extensively here: Coupon collector's problem: mean and variance in number of coupons to be collected to complete a set (unequal probabilities) . Now, imagine $n$ soccer players training. They stand equally spaced in a circle and pass the ball to each other. Each player is more likely to pass the ball to someone standing close to them and less likely to pass to someone diametrically opposite. This implies an $n \times n$ matrix of probabilities, $p_{i, j}$ , which is given (probability player $i$ will pass to player $j$ ). How many passes are needed before every player has kicked the ball? Let's call this number of passes $N$ . What is $E(N)$ ?","['expected-value', 'coupon-collector', 'probability']"
4435303,"Why does the symbol of the Dirac operator have an ""i"" in it?","Suppose we have a differential operator of order $n$ that maps sections of a bundle $E \rightarrow M$ to sections of a bundle $F \rightarrow M$ : $$ D(\sigma) = \sum_{I} \alpha_{I} \frac{\partial^{I}}{\partial x^{I}} +\ \text{lower order terms}$$ Where $I$ ranges over multi-indices of length $n$ . Then I read that the symbol is defined to be: $$Symb(D)(\xi) = i^{n} \sum_{I} \alpha_{I} \xi^{I}$$ Where $\xi \in T^*_xM$ . The symbol is supposed to be a map between the pullbacks of the bundles $E$ and $F$ over the bundle $T^*M$ . My question is: why is there a power of $i = \sqrt{-1}$ in the expression for the symbol? If the bundles are just real vector bundles, where would the $i$ come from? (FYI I am reading this in Morgan's book on Seiberg Witten theory)","['elliptic-operators', 'differential-geometry']"
4435376,Why does $\sin(x)$ + $\sin(x+a)$ always come out as a sine function? [duplicate],"This question already has answers here : Sum of sin waves with same frequency and different amplitudes and phase (5 answers) Closed 2 years ago . How is it possible mathematically? In short, this says, $\sin(x)$ + $\sin(x+a)$ must be equal to some function like $b$ $\sin(x+c)$ somehow, but how?",['trigonometry']
4435379,Does there exist a real function continuous everywhere and differentiable everywhere except in an uncountable subset of the reals of measure zero?,"I want to know if, given any function $f\colon\mathbb{R}\rightarrow\mathbb{R}$ that is continuous everywhere and differentiable everywhere except a on subset, say $S$ , of $\mathbb{R}$ of measure zero, is it necessarily true that $S$ is countable? I know that there are real functions that are everywhere continuous but nowhere differentiable, but I don't know the answer to the above question. Any help would be appreciated.","['continuity', 'calculus', 'derivatives', 'real-analysis']"
4435382,A non trivial example of a spread out distribution,"This question is motivated by this posting . In the classic book Meyn, S. and Tweedie, R., Markov Chains and Stochastic Stability, the authors introduce the concept of a spread out distribution: Definition:
A probability measure $\mu$ on $(\mathbb{R},\mathscr{B}(\mathbb{R}))$ is a spread out distribution if there is $n\in\mathbb{N}$ such that $\mu^{*n}$ has a nonzero absolutely continuous  (w.r.t. Lebesgue measure). Here $\mu^{*n}$ denotes the convolution of $\mu$ with itself $n$ -times. These measures come as examples of random walks where ergodicity properties can be analyze in terms of small sets. Problem: Any measure $\nu$ with non trivial absolutely continuous part is of course spread out ( $n=1$ ). What I am asking (out of curiosity and not for professional need) is for an example of a purely singular measure $\nu$ (w.r.t Lebesgue measure) that is spread out in the sense above. My first instinct was look for example,  the 1/3-Cantor measure $\mu_{1/3}$ . Yuval Peres, here show me that this measure (and other fractal-like measures) preserve singularity under self-convolutions. Thus, such $\nu$ must be either too easy to obtain for my  myopic eyes, or much more sophisticated.","['measure-theory', 'probability-theory']"
4435449,What is what in a mathematical function notation,"I've started learning math from zero. As we know a function consists of three integral parts: input, relationship and output.
When we write something like $f(x) = x + 1$ , we can clearly see and understand what is what in this notation: f is a name of function, x is its argument and so on. Misunderstandings begin when other letters are used instead of the letter f or when it's combined with other letters, like: $y(x) = x^2$ $y = f(x)$ So here I cannot understand and identify the positions of those three parts of a function. And now the questions: Is the letter f or any other letter that comes before the brackets a variable or just a function name? For example, when we write $y = f(x)$ is this "" $f$ "" a variable into which the result of the function is substituted (i.e. its output), and then assigned to the variable $y$ ? If not, what about $y(x) = x^2$ ? Is y here just the function name too? In short, by what rule can I quickly determine in a function what is an input, an output, and a relationship? Because at first glance, it seems to me that there is no single way to represent a function as a formula For example, in each programming language there is a single template by which a function is defined - you must specify in a certain sequence the return type, parameters, and then a processing algorithm","['notation', 'functions']"
4435583,Markov chain and stationary distribution,"Consider an irreducible Markov chain $X=(X_k)_k$ taking values in a finite set $F$ and with a transition matrix $(P(x,y))_{x,y \in F}.$ Let $\pi$ be a stationary distribution of $X$ and $h:F \to \mathbb{R}$ a function. Prove that there exists a function $f:F \to \mathbb{R}$ such that $$\forall x \in F,Pf(x)-f(x)=h(x)-\sum_{y \in F}h(y) \pi(y).$$ I tried to find a function verifying the above with no success. How can we define or construct a function $f$ with the above property?","['stochastic-processes', 'probability-theory', 'markov-chains']"
4435687,all functions $f:A \rightarrow Y$ can be extended to $F$ if $i$ is injective.,"Given sets $A, X, Y$ and functions $i: A \rightarrow X$ . We say that $f:A \rightarrow Y$ extends to $F:X \rightarrow Y$ if for all $a \in A$ $$F(i(a)) = f(a)$$ We want to show that all functions $f:A \rightarrow Y$ can be extended to $F$ if $i$ is injective. $i$ being injective has the consequence as all $a \in A$ are mapped to distinct $X \in X$ . So the image set of $i$ contains some distinct subset of $X$ . I believe I have to break this up into three cases namely $f$ being surjective, injective. (I don't think I have to consider the bijective case since bijection is injective + surjective) Case $1$ : when $f$ is injective . $\Rightarrow$ If $f$ is injective then it maps all $a \in A$ to distinct $y \in Y$ hence the $F$ exists. Case $2$ : When $f$ is surjective $$$$ $\Rightarrow$ When $f$ is surjective it has the property that for every $y \in Y$ there is a $a \in A$ such that $f(a) = y$ So if there exists $a_1 \neq a_2 \in A$ such that $f(a_1) = f(a_2) = y_0$ (because elsewise it's the same thing as the earlier case.)
But since $a_1,a_2$ are mapped to $x \in X$ for some $x_1,x_2$ by $i$ , $F$ can map them both to $y_0$ hence such a $F$ exists Is this proof correct? Are there different (and easier) ways to prove this?","['elementary-set-theory', 'functions']"
4435690,Example for an injective but not surjective polynomial $\mathbb{R}^n\rightarrow\mathbb{R}^n$?,"The Ax-Grothendieck theorem claims that every injective polynomial function $\mathbb{C}^n\rightarrow\mathbb{C}^n$ is surjective. A proof using Hilbert's Nullstellensatz can by found in Terry Tao's blog here . Since the latter theorem holds for algebraically closed fields, I tried to look at what happens to the former theorem, if $\mathbb{Q}$ or $\mathbb{R}$ are taken instead. The first case is simple, $\mathbb{Q}\rightarrow\mathbb{Q},x\mapsto x^3$ is injective, but not surjective (It's quite funny, that you can use Fermat's last theorem to prove $2$ doesn't have a preimage.). The second case seemed as well, every injective polynomial $\mathbb{R}\rightarrow\mathbb{R}$ has to be surjective because of different limits for $x\rightarrow\pm\infty$ and the intermediate value theorem, so there is no counterexample, but I think there should be for $\mathbb{R}^n\rightarrow\mathbb{R}^n,n>1$ . After hours, I still haven't found one though. I tried using the Binomial theorem, but examples like $P\colon(x,y)\mapsto(x^2-y^2,x-y)$ (which doesn't work as $P(1;1)=P(2;2)$ ) or $(x,y)\mapsto(x(x-y),y(x-y))$ (which doesn't work as $P(x,y)=P(-x,-y)$ ) just don't fit. Using Hilbert's Nullstellensatz and the sufficient conditions for being injective and not surjective unfortnutly isn't allowed when considering fields that aren't algebrically closed. I still have the feeling, that there is a simple and elegant counterexample I just don't see? None of the literature for the Ax-Grothendieck theorem I read featured one. Is there an injective, but not surjective polynomial $\mathbb{R}^n\rightarrow\mathbb{R}^n,n>1$ ?","['algebraic-geometry', 'polynomials', 'real-analysis']"
4435698,Why vector bundles are defined differently in algebraic geometry and topology?,"Over schemes, we can define locally free sheaves; vector bundles associated to a quasi-coherent sheaf ( Stacks Project ). These notions seem to be dual to each other. It is easy to see on $\mathrm{Spec}(k)$ for a field, a locally free sheaf is a vector space $V$ , and the vector bundle associated to the quasi-coherent sheaf $V$ is $V^*$ . Lemma 27.6.3 shows an anti-equivalence between the category of vector bundles and the category of quasi-coherent sheaves. However topologically, vector bundles are defined first and its associated locally free sheaf is the sheaf of sections. This association is covariant rather than contravariant. I wonder why algebraic geometers prefer this definition of vector bundles. Are there any conceptual or historical reasons behind this?","['algebraic-geometry', 'math-history']"
4435724,Expectation of the product of two mean independent random variables,"I have random variables $a$ and $b$ , which are such that $\mathbb{E}[a|b]=0$ . I am trying to compute $E[ab]$ . Is the following correct? $$\mathbb{E}[ab]=\mathbb{E}[\mathbb{E}[ab|b]]=\mathbb{E}[b\mathbb{E}[a|b]]=0$$ Where I have used the law of iterated expectations. Thank you.","['statistics', 'solution-verification', 'probability']"
4435730,"Proving $F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right)$ is differentiable and computing $\nabla F(x,y)$","Let $f:\Bbb R\to\Bbb R$ be a function of the class $C^1$ and let $F:\Bbb R^2\to\Bbb R^3$ be given by $$F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right).$$ Prove $F$ is differentiable and compute $\nabla F(x,y).$ My thoughts: First, $F$ is differentiable if and only if all of its components are, so it remains to show $F_3(x,y)=\int_x^yf(z^2)dz$ is differentiable. I think I should use the following results: $\underline{\boldsymbol{\text{theorem } 11.1:}}$ Let $f:A=[a,b]\times[c,d]\to\Bbb R$ be continuous s. t. $\frac{\partial f}{\partial y}$ exists and is continuous on $A.$ Suppose $F:[c,d]\to\Bbb R$ is given by $$F(y)=\int_a^bf(x,y)dx.$$ Then $F$ is differentiable on $[c,d]$ and $F'(y)=\int_a^b\frac{\partial f}{\partial y}(x,y)dx.$ And its corollary: $\underline{\boldsymbol{\text{corollary }11.2:}}$ Let $f:A=[a,b]\times[c,d]\to\Bbb R$ is continuous s.t. $\frac{\partial f}{\partial y}$ exists and is continuous on $A.$ Let $u,v:[c,d]\to[a,b]$ be of class $C^1$ and suppose $F:[c,d]\to\Bbb R$ is given by $$F(y)=\int_{u(y)}^{v(y)}f(x,y)dx.$$ Then, $F$ is differentiable on $[c,d]$ and $$F'(y)=f(v(y),y)v'(y)-f(u(y),y)u'(y)+\int_{u(y)}^{v(y)}\frac{\partial f}{\partial y}(x,y)dx.$$ Both results have been proven and I got some additional insight into the corollary here . Let $G, H:\Bbb R\to\Bbb R, G(t):=F_3(0,t), H(t):=F_3(t,0).$ We could write: $$\begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_0^yf(z^2)dz-\int_0^xf(z^2)\\&=F_3(0,y)-F(0,x)\\&=G(y)-G(x)\end{aligned}$$ and $$\begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_x^0f(z^2)+\int_0^yf(z^2)dz\\&=\int_x^0f(z^2)dz-\int_y^0f(z^2)dz\\&=F_3(x,0)-F_3(y,0)\\&=H(x)-H(y)\end{aligned}$$ Since $f\in C^1(\Bbb R),$ so is the composition $f\circ\pi^2,$ where $\pi:(a,z)\mapsto z.$ Therefore, $\frac{\partial (f\circ\pi^2)}{\partial x}$ exists and is continuous, so we can apply the above corollary to functions $G$ and $H.$ I think $$\begin{aligned}\frac{\partial F_3}{\partial y}(x,y)&=G'(y)-G'(x)=f(z^2)\cdot 0-f(z^2)\cdot 0+\int_0^y\underbrace{\frac{\partial (f\circ\pi^2)}{\partial x}}_{=0}-f(z^2)\cdot 0+f(z^2)\cdot 0-\int_0^x\underbrace{\frac{\partial(f\circ\pi^2)}{\partial x}}_{=0}\\&=0\end{aligned}$$ and analogously $$\frac{\partial F_3}{\partial x}(x,y)=H'(x)-H'(y)=0,$$ so $\operatorname{grad} F_3(x,y)=(0,0)$ and $\nabla F(x,y)=\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix}$ but I'm not so sure about my result. Question: Is there anything wrong and how can we formalize the answer?","['integration', 'multivariable-calculus', 'solution-verification', 'real-analysis']"
4435750,why is $ 2 = \frac{5}{1+\frac{8}{4+\frac{11}{7 + \frac{14}{10 + \dots}}} } $,"Why is $ 2 = \cfrac{5}{1+\cfrac{8}{4+\cfrac{11}{7 + \cfrac{14}{10 + \ddots}}} } $ where the sequences $5,8,11,14,\dots$ and $1,4,7,10,\dots$ are of the form $5 + 3 n$ and $1 + 3n$ . (This converges on both even and uneven iterates) I was surprised this is an integer. Maybe it would help to rewrite this generalized continued fraction into a ""normal"" simple continued fraction.
But I believe that would give us coefficients that generalized the double factorial to a sort of "" triple factorial "" meaning $ f(n) = n \cdot f(n-3) \cdot f(n-6) \cdot f(n-9) \cdots $ and I have almost no skills or understanding of those. Maybe some transformation formula's make this easy, but Im not seeing it.","['continued-fractions', 'real-analysis']"
4435753,Best bound for $|\sum \prod_{p\mid n}(1-\frac{1}{p^k})-\frac m{\zeta(k+1)}|$?,"Follow from the previous post , we have \begin{align*}
f_2(x):=\sum_{n\le x} \prod_{p\mid n}(1-\frac{1}{p^2})-\frac x{\zeta(3)}=O(1)\quad \text{as }x\to\infty.\tag{*}
\end{align*} When I tried to plot $f_2$ , I observed that for any $m\in \Bbb N,$ $$|f_2(m)|=\left|\sum_{n\le m}\prod_{p\mid n}(1-\frac{1}{p^2})-\frac m{\zeta(3)}\right|< c\approx0.22,$$ In particular, $0<f_2(m)<\color{red}{0.222656...}$ . Here is the plot for $m\le 10000$ : I have no idea how to estimate $c\approx0.22$ through analytical method. Is it possible to find the exact value of $c$ ? If not, could we prove the less tight case (e.g. $|f_2|<0.3$ )? Since for any $\ell\ge2$ , \begin{align*}
f_\ell(x):=\sum_{n\le x} \prod_{p\mid n}(1-\frac{1}{p^\ell})-\frac x{\zeta(\ell+1)}=O(1)\quad \text{as }x\to\infty.\tag{**}
\end{align*} One may generalize the question to finding the smallest $c_\ell$ such that $$|f_\ell(m)|=\left|\sum_{n\le m}\prod_{p\mid n}(1-\frac{1}{p^\ell})-\frac m{\zeta(\ell+1)}\right|< c_\ell,\quad \forall m\in\Bbb N.$$ Update Follow Gary's comment, if we define \begin{align*}
F_2(x):=\frac1x\sum_{n\le x} f_2(n)
\end{align*} The graph is Mathematica code: jordanTotient[n_, k_: 1] := 
DivisorSum[n, #^k*MoebiusMu[n/#] &] /; (n > 0) && IntegerQ[n];
Plot[1/x Sum[(Sum[jordanTotient[n, 2]/n^2, {n, 1, m}] - 
 m/Zeta[3]), {m, 1, Floor[x]}], {x, 0, 100}] Update 2 We look at the corresponding Dirichlet series and make use of Perron's formula to get the asymptotic. \begin{align*}
f(n)=\prod_{p\mid n}(1-\frac{1}{p^2})=\frac{J_2(n)}{n^2}\quad \implies \quad
F(s)=\sum_{n=1}^\infty\frac{f(n)}{n^s}=\sum_{n=1}^\infty\frac{J_2(n)}{n^{s+2}}=\frac{\zeta(s)}{\zeta(s+2)}.
\end{align*} Where $J_k(\cdot)$ is the Jordan totient function. Now making use of Perron's formula, \begin{align*}
\sum_{n=1}^m\frac{J_2(n)}{n^2}=\frac12\frac{J_2(m)}{m^2}+\frac1{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s\,\mathrm ds\tag{***}
\end{align*} for any $c>1$ . The integrand $\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s$ has poles at $s=0$ and $s=1$ . We shift the line of integration to the left across the pole $s=1$ . Let $b\in (0,1)$ , we have that \begin{align*}
\frac1{2\pi i}\int_{c-iT}^{c+iT}\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s\,\mathrm ds=&\underbrace{\text{Res}\left(\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s,1\right)}_{=m/\zeta(3)}+
\underbrace{\frac1{2\pi i}\int_{b-iT}^{b+iT}\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s\,\mathrm ds}_{\text{vertical integral}}\\
&+ \underbrace{\frac1{2\pi i}\int_{c-iT}^{b-iT}\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s\,\mathrm ds+\frac1{2\pi i}\int_{b+iT}^{c+iT}\frac{\zeta(s)}{\zeta(s+2)}\frac{m^s}s\,\mathrm ds}_{\text{horizontal integral}}.
\end{align*} Thus, $$f_2(m)=\sum_{n=1}^m\frac{J_2(n)}{n^2}-\frac{m}{\zeta(3)}=\frac12\frac{J_2(m)}{m^2}+\color{red}{O(?)}$$ but I do not know how to estimate these integrals to get $O(?)$ . I am not sure if the above steps are correct as I am new on this topic. Any help would be much appreciated!","['analytic-number-theory', 'number-theory', 'zeta-functions', 'asymptotics']"
4435920,Example of an ergodic transformation with some properties in the spectrum of the Koopman operator,"Let $(X,\mathcal{B},\mu)$ a probability space, for simplicity we can assume $X$ a metric space. Let $T: (X,\mathcal{B},\mu) \to (X,\mathcal{B},\mu) $ be an invertible, measure-preserving transformation. We can now define the (linear) operator $U_T : L^2 (X,\mathcal{B},\mu) \to L^2(X,\mathcal{B},\mu)$ by the following formula: $$U_T f = f\circ T \text{ for }f \in L^2(X,\mathcal{B},\mu).$$ I would like to know if there is any example of an operator $T$ satisfying the above hypotheses and $\mathbf{1}$ is a eigenvalue of $U_T$ ; the dimension of the eigenspace of $\mathbf{1}$ is $1$ ; the dimension of the generalized eigenspace of $\mathbf{1}$ is greater than $1$ . Notes: The operator $U_T$ is known as the Koopman Operator ; This question arose because I have a doubt about the definition of simple eigenvalue , since it is known that if $1$ is a simple eigenvalue of $U_T$ , then $T$ is ergodic. In my head, there is a question whether the fact of being simple is talking about eigenspace or generalized eigenspace. In the literature , I always see it being treated as the dimension of the eigenspace, so there should be an example satisfying the requirements of my question, but I haven't been able to come up with one.","['operator-theory', 'ergodic-theory', 'functional-analysis', 'spectral-theory', 'dynamical-systems']"
4435969,How do we know that a rational function has at most 1 slant asymptote?,I was wondering why it must be the case that a rational function such as $$f(x) = 3x + 2 + \frac{2}{x+6}$$ has only one slant asymptote. My textbook gave an intuitive explanation of why this might be true but it was far from rigorous. I understand why $$y = 3x + 2$$ is a slant asymptote. But isn't it possible that it could have more? Could anybody explain why it must be the case that functions such as $f$ must have at most 1 slant asymptote?,"['algebra-precalculus', 'functions']"
4436135,"Prove that if $a \mid c$ and $b \mid c$, then $ab \mid c^2$ [duplicate]","This question already has answers here : $\begin{align}\color{#c00}a&\mid b\\\color{#c00}c&\mid d\end{align}\Rightarrow\ \color{#c00}{ac}\mid bd$ $\ \ \bf\small [Divisibility\ Product\ Rule]$ (4 answers) Closed 2 years ago . I am trying to solve this problem, but I am unsure if my proof if sufficient or not. Anyways, here is what I have tried: So, by using the definition of ""divides"" I get: (i): If $a \mid c$ , then there exists an integer $k$ such that $ak=c$ . Similarly, if $b \mid c$ , then there exists an integer $n$ such that $bn=c$ . (ii): Multiplying these two expressions, you get $(ak)(bn)=cc$ or $ab(kn)=c^2$ (which satisfies $(ab)l=c^2$ , where $l$ is an integer). I also have another way of trying to prove it: (i): By the same definitions as above, you get $a=c/k$ and $b=c/n$ . (ii): By the defintion of divides, you also have $ab(l)=c^2$ . (iii): Inserting $a$ and $b$ into the equation in (ii), you get $(c^2/kn)*l=c^2$ $\rightarrow$ $c^2l/kn=c^2$ $\rightarrow$ $l/kn=1$ $\rightarrow$ $l=kn$ . (iiii): Inserting $l=kn$ into (ii), you get $c^2(kn)/kn=c^2$ $\rightarrow$ $c^2=c^2$ , thus $ab \mid c^2$ Any corrections or pointers are appreciated.","['solution-verification', 'divisibility', 'discrete-mathematics']"
4436152,"Explanation for a solution: Howard Anton, Elementary Linear Algebra","I am currently working with the 1st edition of Howard Anton's ""Elementary Linear Algebra"". I tried the following problem: Excercise Set 1.2 (p. 17), Problem 12: For which values of $a$ will the following system have no solutions? Exactly one solution? Infinitely many solutions? $$\begin{array}{rccccl} 
x &+& 2y &-& 3z &=& 4 \\ 
3x &-& y &+& 5z &=& 2 \\
4x &+& y &+& (a^2 - 14)z &=& a + 2\end{array}$$ By using Gauss-Jordan-Elimination (and Gaussian for a double check), I found the following solution set: $$\begin{align*} x &= \frac{8}{7} + \frac{-a+4}{a^2-16} \\ 
y &= \frac{10}{7} + \frac{2a-8}{a^2-16} \\
z &= \frac{a-4}{a^2-16}\end{align*}$$ The solution in the textbook says, that the system has no solution if $a=-4$ an one solution if $a\neq\pm4$ . This part I understand, since $z=\frac{a-4}{a^2-16}$ as well as others terms in the formulas for $y$ and $z$ are not defined for $a=-4$ but the formulas for $x,y,z$ will yield unambiguous values for $a\neq\pm4$ . However, the solution also says, that the system has infinitely many solutions for $a=4$ and this is the point, which I don't understand. Isn't for instance $z=\frac{a-4}{a^2-16}$ still undefined for $a=-4$ or can I simply put in $z = \frac{0}{0} = 0$ . And if I can, isn't $z=0$ still a unique value. I can find no room for different values of $x,y,z$ to satisfy the system of equations. Can somebody explain this to me? Thanks in advance!","['systems-of-equations', 'linear-algebra']"
4436158,How to simplify $\cos\left(\arctan\left(x\right)\right)$ bases on complex analysis,"First, I will introduce two identities: $$\cos\left(z\right)=\frac{e^{iz}+e^{-iz}}{2}$$ $$\arctan\left(z\right)=\frac{i}{2}\left(\ln\left(1-iz\right)-\ln\left(1+iz\right)\right)=\frac{-1}{2i}\ln\left(\frac{1-iz}{1+iz}\right)$$ Now we will try to simplify the $\cos\left(\arctan\left(x\right)\right)$ : $$\cos\left(\arctan\left(x\right)\right)=\frac{e^{i\frac{-1}{2i}\ln\left(\frac{1-ix}{1+ix}\right)}+e^{-i\frac{-1}{2i}\ln\left(\frac{1-ix}{1+ix}\right)}}{2}=\frac{\sqrt{\frac{1+ix}{1-ix}}+\sqrt{\frac{1-ix}{1+ix}}}{2}=\frac{\sqrt{\frac{\left(1+ix\right)\left(1+ix\right)}{\left(1-ix\right)\left(1+ix\right)}}+\sqrt{\frac{\left(1-ix\right)\left(1-ix\right)}{\left(1+ix\right)\left(1-ix\right)}}}{2}=\frac{\sqrt{\frac{\left(ix+1\right)^{2}}{1+x^{2}}}+\sqrt{\frac{\left(ix-1\right)^{2}}{1+x^{2}}}}{2}$$ The square root extraction operation can give two possible values, we take this into account and therefore introduce two additional variables: $$\frac{\left(-1\right)^{k_{0}}\left(ix+1\right)+\left(-1\right)^{k_{1}}\left(ix-1\right)}{2\sqrt{1+x^{2}}}$$ , where $k_0 ∈ [0,1] $ and $k_1 ∈ [0,1]$ . The research of different values of variables $k_0$ and $k_1$ will lead to a two fundamentally different version: $$[\frac{\pm1}{\sqrt{1+x^{2}}},\frac{\pm ix}{\sqrt{1+x^{2}}}]$$ Q: What intuition underlies the choice? How to mathematically justify the choice of either the first version or the second? What are the reasons for having the second option, which is very similar to the simplified version of the $\sin\left(\arctan\left(x\right)\right)$ ?","['complex-analysis', 'trigonometry', 'complex-numbers']"
4436190,Combinatorial isoperimetric inequality,"I am interested in isoperimetric equalities in lattices, more precisely what could be said for surrounding territory in the game of go. Here are some statements or heuristics: To surround territory with a certain number of stones, it is better to do squares than thin rectangles: this is a basic isoperimetric inequality/optimization problem, not particularly combinatoric. Using edges is better than being in the center: the edges essentially ""turn into friend stones"" of neighboring territories, saving lots of moves to surround territory. Using corners is even better. However there are obviously finer considerations: what is the optimal shape to make territory (the circle would be in the Euclidian plane, but here we are on a grid)? What about the use of edges/corners: the better shape there may be different (a straight line looks more efficient than half a square for instance). Are there similar questions on more general lattices maybe? Any reference is welcome.","['optimization', 'combinatorics', 'integer-lattices', 'reference-request']"
4436191,Cardinality between union of open uncountable set with finite set,"I am trying to show that $|(0,1)| = $$|(0,1) \cup $ { $2,3,4,5$ } $|$ using the Schroder-Bernstein Theorem. Therefore, I need to find injections $f:(0,1) \rightarrow (0,1) \cup $ { $2,3,4,5$ } and $g:(0,1) \cup $ { $2,3,4,5$ } $\rightarrow (0,1)$ (1) We can define $f(x)=x$ since we do not have to consider 2,3,4,5. (2) We can define $g(x)= \frac{x}{6}$ in order to direct 2,3,4,5 to a number $\in (0,1)$ Therefore, by the Schroder-Bernstein Theorem, $|(0,1)| = $$|(0,1) \cup $ { $2,3,4,5$ } $|$ Is this valid?","['elementary-set-theory', 'solution-verification', 'infinity']"
4436208,Powers of Non-i.i.d. Random Matrices,"Suppose there is some Random Matrix $A$ which exists in $\mathbb{R}^{N\times N}$ . Each element in $A$ has a different $\mu$ and $\sigma$ . However, all elements are independent of eachother. $A$ is a Ginibre ensemble in that it is non-Hermitian. Furthermore, the eigenvalues of $A$ are mostly real but some are imaginary. What are the distributions of $A^M$ where $M$ is a natural number? So far, I have done numerical experiments by using expressions for the distributions of eigenvalues (eq 2.15) and the $U$ and $V$ matrices from the means of $A$ .
I approximated the distribution of $A = U \Sigma V^T$ where all randomness is contained inside $\Sigma$ and $U$ and $V$ are fixed. Then I tried to recreate the distribution of $A$ by drawing samples from the distribution for $\Sigma$ and carrying out the matrix products. However, the distributions from recreating $A$ from sampling from the eigenvalue pdfs were not similar to the original distributions of $A$ . Any information or resources is appreciated.","['random-matrices', 'statistics', 'probability-theory', 'reference-request']"
4436219,Find the density of a certain subset of $\Bbb N$,"I am trying to determine the density of the set $$ S:=\{\,ap^r-p^s\mid p\text{ prime}, a,r,s\in\Bbb N, 1\le s\le r, 2\le a\le p\,\}.$$ These can also be described as numbers written in base $p$ as an arbitrary digit, followed by zero or more digits $p-1$ (the ""nines"" of base $p$ ), followed by one or more zeroes. The reason I ask about this is that I am trying to investigate a conjecture about a sequence I stumbled over in the OEIS, but that's a different story (in particular, $S$ is not the sequence in question). What I have so far:
I write $S=\bigcup_{r\ge1} S_r$ with $$ S_r:=\{\,ap^r-p^s\mid p\text{ prime}, a,s\in\Bbb N, 1\le s\le r, 2\le a\le p\,\}.$$ I am pretty sure that I managed to show
that $\bigcup_{r\ge3} S_r$ has $O(N^{\frac23}\ln^2N)$ elements $<N$ , which contributes zero to the density of $S$ . The numbers $\in S_1$ are of the form $ap-p = (a-1)p$ , which are precisely the numbers $n$ having a prime factor $>\sqrt n$ . Apparently, this implies that $S_1$ has density $\ln 2$ . The numbers in $S_2$ come in two flavours: $$ S_{2,1}:=\{\,ap^2-p\mid p\text{ prime}, a\in\Bbb N, 2\le a\le p\,\}$$ and $$ S_{2,2}:=\{\,ap^2-p^2\mid p\text{ prime}, a\in\Bbb N, 2\le a\le p\,\}.$$ Numbers $n$ in these sets have in common that they have a prime factor $>\sqrt[3]n$ , but as discussed in the above link, it seems that one cannot extend the argument used for $>\sqrt n$ to this case. Also, these are not all numbers with prime factor $>\sqrt[3]n$ , so a specific argument may work better. For $S_{2,2}$ , we have a map bijection $S_{2,2}\to S_2$ given by $(a-1)p^2\mapsto (a-1)p$ , i.e, ""divide by the largest prime factor"". It follows easily that this makes the density of $S_{2,2}$ zero: If we ignore the first gazillion terms, the above map increases the density by at least a gazillion, hence we must have started with at most $1/$ gazillion. Remains $S_{2,1}$ , numbers of the form $ap^2-p=(ap-1)p$ . I had hoped to make an argument similar to that for $S_{2,2}$ , but as a larger prime may hide in $ap-1$ , or it may even happen that $(ap-1)p=(bq-1)q$ , I am unsure ... Perhaps a different argument helps?","['number-theory', 'prime-numbers', 'integer-sequences']"
4436242,Are all manifolds locally flat?,"The definition of manifold I usually see is “A topological space $(X,T)$ in which for every member of the topological space $x \in (X,T)$ there exists a neighborhood $M_x$ such that is homeomorphic to an open set of $\mathbb{R}^n$ The problem with this definition is that it doesn’t seem to imply the intuitive notion that manifolds are locally flat (look flat when you zoom in). All it implies is that there are neighborhoods around points where the topology looks the same, but we know of objects that have the same topology but do not geometrically look the same (.ex a coffee cup and a donut). It looks like a better definition of a manifold would be a set where for each member of the set there exists a neighborhood where the Riemann curvature tensor is approximately 0. Does this definition work? Is it implied from the 1st definition of a manifold? If so, how? Thanks","['manifolds', 'differential-topology', 'differential-geometry']"
4436390,Is there a mistake in solving this limit?,"I want to solve this: \begin{equation}
L=\lim_{x\rightarrow 0}  \frac{\sum\limits_{m=1}^{M}a_{m}\exp\left\{\frac{bm^2}{ (m^2+2x^2)x^2}\right\}{(m^2+2x^2)^{-3/2}} }   {\sum\limits_{m=1}^{M} c_{m}\exp\left\{\frac{bm^2}{ (m^2+2x^2)x^2}\right\}{(m^2+2x^2)^{-3/2}}}
\end{equation} where \begin{equation}
\begin{aligned}
m=1,2,\cdots,M\\
a_m,b,c_m \neq 0 \quad \text{and are all constants}
\end{aligned}
\end{equation} Here is my approach: Since $x\rightarrow 0$ , I ignore the term $2x^4$ in the denominator of $\exp\left\{\frac{b_m}{ (m^2+2x^2)x^2}\right\}$ and I get: \begin{equation}
\begin{aligned}
L&=\lim_{x\rightarrow 0}  \frac{\exp\left\{\frac{b}{x^2}\right\}\sum\limits_{m=1}^{M}a_{m}{(m^2+2x^2)^{-3/2}} }   {\exp\left\{\frac{b}{x^2}\right\}\sum\limits_{m=1}^{M} c_{m}{(m^2+2x^2)^{-3/2}}}\\
&=\lim_{x\rightarrow 0}  \frac{\sum\limits_{m=1}^{M}a_{m}{(m^2+2x^2)^{-3/2}} }   {\sum\limits_{m=1}^{M} c_{m}{(m^2+2x^2)^{-3/2}}}\\
&=\frac{\sum\limits_{m=1}^{M}\frac{a_{m}}{m^3} }   {\sum\limits_{m=1}^{M} \frac{c_{m}}{m^3}}
\end{aligned}
\end{equation} My question is: Is there a mistake in my derivation? If I made mistakes in the derivation, then, what is the correct derivation? Thanks for helpful comments and answers!","['limits', 'limits-without-lhopital']"
4436394,Mathematical Olympiad question (complex variable),"I found this question in an old Mathematical Olympiad: Let $0<a<1$ be a real number, and let $f(z)$ be a complex polynomial such that $$|f(z)|\leq \frac{1}{|z-a|}$$ on the unit disk $|z|\leq 1$ . Prove that $$|f(a)| \leq \frac{1}{1-a^2}.$$ My attempt: Since $f$ is analytic in $\{z\in\mathbb{C}:|z|\leq 1\}$ , we have that $$|f(a)| \leq \max_{|z|=1}|f(z)|.$$ Because of the triangle inequality, for all $z\in \mathbb{C}$ with $|z|=1$ we have $$|z-a|\geq |z|-a = 1-a.$$ By applying the hypothesis, we get $$|f(a)| \leq \max_{|z|=1}|f(z)| \leq \max_{|z|=1}\frac{1}{|z-a|}\leq \frac{1}{1-a}.$$ Nevertheless, we know that $1-a^2>1-a$ since $a\in (0,1)$ . Therefore, we cannot get the desired result from the above inequality. What can I apply to complete the proof?","['complex-analysis', 'complex-numbers', 'contest-math']"
4436412,Why does a function has to be differentiable so many times to be considered smooth?,"I'm studying ""Smoothness"". If a function is once differentiable for all x's, shouldn't it be considered smooth? Because it does ""look smooth"" for all f(x), there's no way it will have sharp corners or cusps because it's differentiable. Then why does it have to be differentiable way more times than once (and actually has to be differentiable infinite times) to be considered smooth? Or unless this is not about ""looking smooth"" but smooth in other meaning? Any help is greatly appreciated! Edit: Sorry. Please use a bit less formal math language so I can understand. I not very good at it.","['calculus', 'smooth-functions', 'real-analysis']"
4436498,"The set $C[0,1]$ is nowhere dense in $D[0,1]$.","$D[0,1]$ be the space of real functions $x$ on $[0,1]$ that are right continuous and have left hand limit. On the other hand $C[0,1]$ is set of all continuous real functions on that interval. Considering the underlying topology is Skorohod topology. I try to show that the set $C[0,1]$ is nowhere dense in $D[0,1]$ . How to do that? One idea is, Suppose consider the ball in $D[0,1]$ i.e. $B(f,\epsilon) = \{g\in D[0,1] | d(f,g) < \epsilon\}$ , where $d$ is Skorohod metric. Now it is enough to show that $C[0,1] \cap B(f,\epsilon) = \emptyset$ . Then we can conclude that $C$ is nowhere dense. But how to show this claim?","['measure-theory', 'probability-theory', 'weak-convergence', 'skorohod-space']"
4436580,How to graph the elasticity function ( knowing the - linear-demand function and the price function )? What goes wrong in my Desmos graph?,"My goal is to visualize the graph of the elasticity function for a linear demand curve . The problem I face is that the elasticity function graph I came up with looks unfamiliar. I suppose my formula for the elasticity function contains a mistake, but I can't locate it. Here is what I've done ( and I add a Desmos image below). (1) I start with a demand function ( with price as independent variable) : $$D(x)= a-bx$$ . (2) I transform this function into a price function ( with demand as independent variable), in order to obtain the traditonal demand curve ( with demand on the X axis and price on the Y axis) : $$P(x)= - \frac 1b x +\frac ab$$ . (3) I use the calculus version of the elasticity function, namely : $$ \epsilon_{\small P}= \frac {\mathit dD(P)} {\mathit dP} \times \frac {\mathit P} {\mathit D}$$ wich ( so it seems) should yield $$ \Large\epsilon(x) = D'(x) \frac {P(x)}{Q(x)}$$ and finally ( since $D'(x)=-b$ here) $$ \Large\epsilon(x) = -b \frac {P(x)}{Q(x)}$$ . But, as I said above, the graph of my alledged elasticity function looks unfamiliar. In particular, it seems to me that the elasticity should be equal to $1$ for the X-value of the middle point on the demand curve. Desmos ( https://www.desmos.com/calculator/fp7elscgtq )  :","['graphing-functions', 'economics', 'calculus', 'derivatives', 'soft-question']"
4436587,How to deal with odd $m$ in integral $\int_{0}^{\frac{\pi}{4}}(\sin^{6}m x+\cos^{6}m x) \ln (1+\tan x) d x $,"Latest edit Thanks to @Quanto for settling down the question by proving the odd one as: $$I_{2n+1}= \frac{5\pi}{64}\ln2+\frac3{16(2n+1)}\bigg(\frac\pi4-\sum_{j=0}^{2n}\frac{(-1)^j}{2j+1} \bigg)$$ By our results for both odd and even multiples $n$ of $x$ , we can conclude that $$
\lim _{n \rightarrow \infty} \displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(nx)+\cos ^{6}(nx)\right] \ln (1+\tan x) d x =\frac{5 \pi\ln 2}{64}
$$ As asked by @Claude Leibovici for the powers other than 6, I had generalised my result to even powers below as an answer: $$
I(m,n):=\int_{0}^{\frac{\pi}{4}}\left[\cos ^{2 m}(2 nx)+\sin ^{2 m}(2 n x)\right] \ln (1+\tan x) d x= \frac{\pi \ln 2}{4} \cdot \frac{(2 m-1) ! !}{(2 m) ! !}
$$ In order to evaluate the even case $$\int_{0}^{\frac{\pi}{4}}\left[\sin^{6}(2 n x)+\cos^{6}(2 nx)\right] \ln (1+\tan x) d x $$ we first simplify $\displaystyle \begin{aligned}\sin ^{6}(2 n x)+\cos ^{6}(2 n x) =& {\left[\sin ^{2}(2 n x)+\cos ^{2}(2 n x)\right]\left[\sin ^{4}(2 n x)-\sin ^{2}(2 n x) \cos ^{2}(2 n x)\right) } \\&\left.+\cos ^{4}(2 n x)\right] \\=& 1-3 \sin ^{2}(2 n x) \cos ^{2}(2 n x) \\=& 1-\frac{3}{4} \sin ^{2}(4 n x) \\=& 1-\frac{3}{8}(1-\cos 8 n x) \\=& \frac{1}{8}(5+3 \cos (8nx))\end{aligned} \tag*{} $ To get rid of the natural logarithm, a simple substitution transforms the integral into $\begin{aligned}I &=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln (1+\tan x) d x \\& \stackrel{x\mapsto\frac{\pi}{4}-x}{=} \frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(1+\tan \left(\frac{\pi}{4}-x\right)\right) d x \\&=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(\frac{2}{1+\tan x}\right) d x \\&=\frac{1}{8} \ln 2 \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x) )d x-I \\I &=\frac{\ln 2}{16} \int_{0}^{\frac{\pi}{4}}(5+3 \cos 8  n x) d x\\&=\frac{\ln 2}{16}\left[5 x+\frac{3}{8 n} \sin (8 n x)\right]_0^{\frac{\pi}{4} }\\ &=\frac{5 \pi}{64} \ln 2\end{aligned} \tag*{} $ My Question: How can we deal with the odd one $$\displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(2 n +1)x+\cos ^{6}(2 n +1)x\right] \ln (1+\tan x) d x  ?$$ Can you help?","['integration', 'calculus', 'trigonometry', 'real-analysis']"
4436603,"Transition Functions of the Weighted Projective Space $\mathbb{P}^{2}_{\{3,2,1\}}$.","I am trying to find the transition functions of the weighted projective space $\mathbb{P}^{2}_{\{3,2,1\}}$ . Bear with, I am very new to algebraic geometry. To my understanding, $\mathbb{P}^{2}_{\{3,2,1\}}$ is defined as the quotient of $\mathbb{C}^{3}\setminus{\{0\}}$ by the equivalence relation: $$
(z_{0},z_{1},z_{2})\sim(\lambda^{3}z_{0},\lambda^{2}z_{1},\lambda^{1}z_{2})
\text{ , with }\lambda\in\mathbb{C}^{*}\text{.}
$$ As far as I understand, instead of having charts as in differential geometry, in algebraic geometry we look at ""coordinate patches"". The obvious guess (which clearly covers $\mathbb{P}_{\{3,2,1\}}$ ), is: \begin{align*}
&
U_{1}
=
\{
[1:z_{1}:z_{2}]
|
z_{1},z_{2}\in\mathbb{C}
\}
\\
&
U_{2}
=
\{
[z_{0}:1:z_{2}]
|
z_{0},z_{2}\in\mathbb{C}
\}
\\
&
U_{3}
=
\{
[z_{0}:z_{1}:1]
|
z_{0},z_{1}\in\mathbb{C}
\}
\\
\end{align*} where we have used square brackets to denote equivalence classes. Then as far as I understand, these patches are supposed to be isomorphic to (some quotient of?), affine $2$ -space, in the same way we would expect homeomorphisms with $\mathbb{R}^{2}$ for manifolds. The obvious isomorphisms are: \begin{align*}
&
\phi_{1}:U_{1}\to\mathbb{A}^{2}
\text{ , }
[1:z_{1}:z_{2}]
\mapsto
(z_{1},z_{2})
\\
&
\phi_{2}:U_{2}\to\mathbb{A}^{2}
\text{ , }
[z_{0}:1:z_{2}]
\mapsto
(z_{0},z_{2})
\\
&
\phi_{3}:U_{3}\to\mathbb{A}^{2}
\text{ , }
[z_{0}:z_{1}:1]
\mapsto
(z_{0},z_{1})
\end{align*} with inverses: \begin{align*}
&
\phi_{1}^{-1}:\mathbb{A}^{2}\to U_{1}
\text{ , }
(x,y)\mapsto[1:x:y]
\\
&
\phi_{2}^{-1}:\mathbb{A}^{2}\to U_{2}
\text{ , }
(x,y)\mapsto[x:1:y]
\\
&
\phi_{2}^{-1}:\mathbb{A}^{2}\to U_{2}
\text{ , }
(x,y)\mapsto[x:y:1]
\\
\end{align*} which are also locally given by polynomials. The transition functions should then be given by $\tau_{ij}=\phi_{j}\circ\phi_{i}^{-1}$ . This means that: $$
\tau_{12}(x,y)
=
(x^{-3/2},x^{-1/2}y)
\text{ , and }
\tau_{23}(x,y)
=
(y^{-3}x,y^{-2})
\text{.}
$$ If we now write the components as $\tau_{ij}(u_{i,1},u_{i,2})=(u_{j,1},u_{j,2})$ , this says that: \begin{align*}
&
u_{1,1}=u_{2,1}^{-2/3}
\text{ , }
u_{1,2}=u_{2,2}u_{2,1}^{-1/3}
\\
&
u_{2,2}=u_{3,2}^{-1/2}
\text{ , }
u_{2,1}=u_{3,1}u_{3,2}^{-3/2}
\end{align*} This seems fine to me, but the answer given in the paper I'm reading is: \begin{align*}
&
u_{1,1}=u_{2,1}^{-1}
\text{ , }
u_{1,2}^{3}=u_{2,2}u_{2,1}^{-2}
\\
&
u_{2,2}=u_{3,2}^{-1}
\text{ , }
u_{2,1}^{2}=u_{3,1}^{3}u_{3,2}^{-1}
\end{align*} I have gone over my workings several times and can't find any issues, so I am probably misunderstanding something fundamental. Any help would be much appreciated. EDIT: Thanks to reuns' comment, I have realised that $U_{1}$ and $U_{2}$ are not simply affine space. To see this, note that for $\zeta_{n}$ an n'th root of unity, $[1:z_{1}:z_{2}]=[1:\zeta_{3}^{2}z_{1}:\zeta_{3}z_{2}]$ . This means that for the map $\phi_{1}$ to be well defined we must take the quotient by $\mathbb{Z}_{3}$ with this action. This gives: \begin{align*}
&
U_{1}\cong\mathbb{A}^{2}/(z_{1},z_{2})\sim(\zeta_{3}^{2}z_{1},\zeta_{3}z_{2})
\\
&
U_{2}\cong\mathbb{A}^{2}/(z_{0},z_{2})\sim(\zeta_{2}z_{0},\zeta_{2}z_{2})
\\
&
U_{3}\cong\mathbb{A}^{2}
\end{align*} Since my transition functions are invariant under these actions, I am still unsure about how to obtain the form given in my reference. UPDATE: Thinking about this question again sometimes later, I have made a little progress. In particular, I have been able to identify the affine charts \begin{align*}
&
U_{1}
\cong
\mathbb{A}^{2}/(z_{1},z_{2})\sim(\zeta_{3}^{2}z_{1},\zeta_{3}z_{2})
\cong
V(b^3-ac)
\\
&
U_{2}
\cong
\mathbb{A}^{2}/(z_{0},z_{2})\sim(\zeta_{2}z_{0},\zeta_{2}z_{2})
\cong V(b^2-ac)
\\
&
U_{3}
\cong
\mathbb{A}^{2}
\cong
V(b-ac)
\end{align*} under the maps \begin{align*}
&
\phi_{1}:U_{1}\to V(b^3-ac)
\text{ , }
[1:z_{1}:z_{2}]
\mapsto
(z_{1}^3,z_{1}z_{2},z_{2}^3)
\\
&
\phi_{2}:U_{2}\to V(b^2-ac)
\text{ , }
[z_{0}:1:z_{2}]
\mapsto
(z_{0}^{2},z_{0}z_{2},z_{2}^{2})
\\
&
\phi_{3}:U_{3}\to V(b-ac)
\text{ , }
[z_{0}:z_{1}:1]
\mapsto
(z_{0},z_{0}z_{1},z_{1})
\end{align*} We then find that \begin{align*}
&
(\phi_{1}\circ\phi_{2}^{-1})(a^2,ac,c^2)
=
\left(
\frac{1}{a^{2}},\frac{c}{a},\frac{c^{3}}{a}
\right)
\\
&
(\phi_{2}\circ\phi_{3}^{-1})
(a,ac,c)
=
\left(
\frac{a^2}{c^3},\frac{a}{c^2},\frac{1}{c}
\right)
\end{align*} And so we see that indeed the transition functions are rational! However, I have still been unable to rephrase these transition functions into the form given above. In particular, I am struggling with now having three constrained variables to work with instead of two free variables.","['toric-varieties', 'algebraic-geometry', 'projective-space']"
4436619,Let $1=n_0<n_1<\ldots$ be an increasing sequence of positive integers. True/False: $\sum_{i=1}^\infty\frac{n_{i+1}-n_i}{n_{i+1}}$ diverges to $\infty$ [duplicate],"This question already has answers here : $\sum_{n=1}^\infty \frac{a_{n+1}-a_n}{a_{n+1}}$ diverges [duplicate] (2 answers) If $a_n$ is a strictly increasing unbounded sequence, does $\sum_n \frac{a_{n+1} - a_n}{a_n}$ diverge? (4 answers) Closed 2 years ago . Let $1=n_0<n_1<n_2<\ldots$ be an increasing sequence of positive integers. Is it true that $\displaystyle\sum_{i=0}^{\infty} \frac{n_{i+1}-n_i}{n_{i+1}} $ diverges to $+\infty?$ For example, if $n_1=2,n_2=3,n_3=53,n_4=54,$ then the first few terms are $\frac{1}{2}, \frac{1}{3}, \frac{50}{53}, \frac{1}{54}.$ Since $\frac{50}{53}<\displaystyle\sum_{k=4}^9 \frac{1}{k},$ this makes me think it might be possible to come up with a counter-exmaple.","['sequences-and-series', 'problem-solving', 'examples-counterexamples', 'real-analysis']"
4436738,Adjoint of a end-valued complex differential form,"Let $E\to X$ be a holomorphic Hermitian vector bundle over a complex manifold. Let $\xi\in \Omega^1(X,\operatorname{End}(E))$ be an end-valued form. We define its adjoint $\xi^*$ by the identity $$h(\xi v,w)=h(v,\xi^*w)\in \Omega^1(X).$$ for all $v,w\in E$ . (The Hermitian product is linear in $\Omega(X))$ Now we have that the 2-form $\xi\wedge \xi^*$ is anti-self-adjoint. To see that let us consider two simple 1-forms $a=\alpha\otimes f, b=\beta\otimes g\in \Omega^1(X,\operatorname{End}(E))$ , we have \begin{align*}
(a\wedge b)^* &= (\alpha\wedge \beta\otimes f\circ g)^*\\
              &= \overline{\alpha\wedge \beta}\otimes (f\circ g)^*\\
              &= -\bar{\beta}\wedge \bar{\alpha}\otimes g^*\circ f^*\\
              &= -b^*\wedge a^*.
\end{align*} So in particular $$(\xi\wedge \xi^*)^*=-\xi\wedge \xi^*.$$ So a priori we should have that $$h(\xi\wedge \xi^* v,v)\in i\mathbb{R}$$ should be pure imaginary. The problem is that again a priori we have $$h(\xi\wedge \xi^* v,v)=h(\xi^*v,\xi^*v)=||\xi^*v||^2\in \mathbb{R}.$$ What did I get wrong?","['complex-geometry', 'vector-bundles', 'complex-manifolds', 'differential-geometry']"
4436848,How do I get to this formula for the area of a triangle,"I am new here and new student to geometry.
In my geometry skript there is a task: Show that the formula for the area $F$ of a triangle with sidelengths $a,b,c $ is given by $$F^2 = - \frac{1}{16} \det \begin{pmatrix} 0 & c^2 & b^2 & 1 \\ c^2 & 0 & a^2 & 1 \\ b^2 & a^2 & 0 & 1 \\ 1& 1& 1& 0 \end{pmatrix}$$ This drives me crazy because I cant see how to get there. I do know how to get to $F= \frac12 \det \begin{pmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \end{pmatrix} $ and $ F = \frac12 \det \begin{pmatrix} 1 & 1 & 1 \\ a & b & c \end{pmatrix} $ I also thought about using Heron's formula for the square of a triangle: $ F^2 = s(s-a)(s-b)(s-c) $ where $s= \frac{a+b+c}{2} $ I did multiplied it all out and can't seem to find a pattern..
Hope this question is appropriate. Maybe here is someone who has an idea :)","['linear-algebra', 'geometry']"
4436886,Open Sets in Schwarz Reflection Principle,"The Scwarz Reflection Principle says that if $A$ is an open, connected set in the upper half plane of $\mathbb{C}$ whose boundary intersects the real axis in an interval $[a,b]$ , then we can extend a holomorphic function defined on $A$ (and continuous on $A \cup (a,b)$ ) to a holomorphic function defined on $A \cup (a,b) \cup A^*$ , where $A^*$ is the reflection of $A$ over the real axis. I'm not confused about the analysis aspect of this theorem, but there's one topological wrinkle I want to iron out. How do we know that $A \cup (a,b) \cup A^*$ is open? It's an intuitive fact, but I can't figure out how to prove it rigorously. My idea is to pick a point $x \in (a,b)$ and show that there exists $\epsilon > 0$ such that the set $\{z : |z-x| < \epsilon, \operatorname{Im}(z) > 0\}$ is contained in $A$ ; I'm not sure where to go from here.","['complex-analysis', 'general-topology', 'analytic-continuation']"
4436909,Modified Gronwall lemma for $f' \le a + b f^\alpha$,"Let $f\colon [0,\infty)\to [0,\infty)$ be a smooth function such that $f(0)=0$ and \begin{equation}
f'(t) \le a(t)+b(t) \bigl( f(t) \bigr)^\alpha
\label{eq:1}
\tag{1}
\end{equation} for all $t\ge 0$ , some $\alpha\in(0,1)$ and some functions $a,b\colon[0,\infty)\to(0,\infty)$ . Question: What (possibly sharp) bound on $f$ can be derived in terms of $a,b$ and $\alpha$ ? For my problem, we can wlog. assume that $a$ and $b$ are non-decreasing, but the answer itself might be interesting in generality. My attempt If $a\equiv 0$ , then we can reduce the problem to the linear setting and apply the standard Gronwall lemma. Namely, \eqref{eq:1} is then equivalent to $(f^{1-\alpha}(t))'\le (1-\alpha)b(t)$ , whence \begin{equation}f(t)\le \bigl( (1-\alpha)\int_0^t b(s)\,ds \bigr)^{1/(1-\alpha)}.\label{eq:2}\tag{2}\end{equation} Also, we could estimate brutally $x^\alpha\le x$ for $x\ge 1$ and $f(t)\le f(t)+1$ to get that \eqref{eq:1} implies $f'(t) \le a(t)+b(t)+b(t)f(t)$ , whence by the Gronwall lemma $$ f(t) \le \int_0^t (a(s)+b(s))e^{\int_s^t b(u)\,du}\,ds. $$ This estimate however does not take $\alpha$ into considerations and seems way off (compare e.g. with \eqref{eq:2} in case $a\equiv 0$ ).","['functional-inequalities', 'ordinary-differential-equations', 'real-analysis']"
4436917,Inequality with slowly varying functions,"Question Let $X$ be a random variable with distribution function $F$ on a probability space $(\Omega, \mathcal F, P)$ . Suppose that there exist $\alpha \in (0,2)$ and a slowly varying function $\ell(\cdot)$ such that $$
\bar F(x) := 1 - F(x) = \frac{C_1(x)}{x^\alpha} \ell(x) \quad \text{and} \quad F(-x) = \frac{C_2(x)}{x^\alpha} \ell(x) \quad \text{for $x > 0$,}
$$ where $C_1(\cdot), C_2(\cdot)$ are non-negative functions with $C_i := \lim_{x \to \infty} C_i(x)$ , and $C_1 + C_2 > 0$ . Why then does the following hold? There exist $C, \tilde C > 0$ such that $$ \sum_{n=1}^\infty P\big( |X| > a_n \big) \leq \sum_{n=1}^\infty \frac{C}{nf(n)} \leq \tilde C \int_1^\infty \frac{dt}{t f(t)}, 
$$ where we define $a_n := [n f(n) \ell(n) ]^{1/\alpha}$ for an arbitrary positive function $f$ with the properties $$ \limsup_{t \to \infty} \sup_{0 \leq t \leq x} \frac{f(t)}{f(x)} < \infty \quad \text{and} \quad \int_1^\infty \frac{dt}{tf(t)}< \infty.
$$ If not, what if we also required that $f$ be slowly varying, too? Background and Thoughts This comes from Cai's 2006 paper "" Chover-Type Laws of the Iterated Logarithm for Weighted sums of $\rho^*$ -Mixing Sequences "". Cai writes on page 5 that this is ""easily seen"" based on the representation of $F$ above. I don't see why. What follows below is my attempt so far. $$
\begin{aligned}
\sum_{n=1}^\infty P\big( |X| > a_n \big) &= \sum_{n=1}^\infty \Big[ P\big( X > a_n \big) + P\big( X < - a_n \big)  \Big] \\
&\leq \sum_{n=1}^\infty \Big[ \bar F(a_n) + F(-a_n)  \Big] \\
&= \sum_{n=1}^\infty \frac{C_1(a_n) + C_2(a_n)}{a_n^\alpha} \ell(a_n) \\
&\leq \sum_{n=1}^\infty \frac{C}{a_n^\alpha} \ell(a_n) = C \sum_{n=1}^\infty \frac{1}{nf(n)} \cdot \frac{\ell(a_n)}{\ell(n)},
\end{aligned}
$$ where the inequality on the last line holds for some $C>0$ , since $C_1(\cdot)$ and $C_2(\cdot)$ are convergent. If $\frac{\ell(a_n)}{\ell(n)} = \ell\Big( \big[ n f(n) \ell(n) \big]^{1/\alpha} \Big) \Big/ \ell(n)$ were bounded, then the first desired inequality would follow. However, it's not clear why this would have to hold. Updates Indeed, I think the author implicitly uses the fact (?) that $\left\{ \frac{\ell(a_n)}{\ell(n)} \right\}$ is a bounded sequence several times in the paper. But I still don't know why that's the case. Although not stated in the paper, maybe we need some more restrictions on $f$ , such that it is also a slowly varying function. If $f$ were slowly varying, then $u(x) := x^{1/\alpha}\cdot[f(x) \ell(x)]^{1/\alpha}$ would be regularly varying with coefficient $1/\alpha$ . And $\frac{\ell(a_n)}{\ell(n)} = \frac{\ell\big(u(n)\big)}{\ell(n)}$ . Since $u(x) \to \infty$ and is regularly varying, $\ell \circ u$ is slowly varying. Hence, $\frac{\ell(a_n)}{\ell(n)} = \frac{ \ell \big(u(n) \big)}{\ell(n)}$ is slowly varying. But, of course, slowly varying functions aren't necessarily bounded.","['slowly-varying-functions', 'probability-theory', 'asymptotics', 'real-analysis']"
4437009,Hermite polynomials for non-integer degree,"I have solved an eigenvalue problem using Mathematica and the answer is in terms of Hermite polynomials. Now, for integer degrees $H_n(z)$ , I can find a nice definition. However, in the solution to the aforementioned problem, $n$ is not an integer. For example, my Mathematica solution contains HermiteH[a,b] where a and b are both real numbers. The ""degree"" of the polynomial in this case is a . Where can I find a definition for this? I suspect I can just generalize the contour integral to use a gamma function instead of a factorial so that $$
H_a(z) = \frac{\Gamma(a+1)}{2\pi i} \oint_C \frac{e^{2tx-t^2}}{t^{a+1}} \, dt
$$ for a contour $C$ around the origin, vs. $$
H_n(z) = \frac{n!}{2\pi i} \oint_C \frac{e^{2tx-t^2}}{t^{n+1}} \, dt
$$ where $n$ is a nonnegative integer. Is this correct, and if so, what's a relatively authoritative source for the generalization? Ideally, I'd prefer a series representation (or something else a bit more explicit) if it's known.","['complex-analysis', 'physics', 'hermite-polynomials', 'mathematica']"
4437032,Tough integral $\int_0^{ \pi }\frac{x^2(\pi-x)^2}{\sin^2 x} dx =6\pi\zeta(3) $,"How to prove $$\int_0^{ \pi }\frac{x^2(\pi-x)^2}{\sin^2 x} dx =6\pi\zeta(3),  $$ and does there even exist a closed form of $$\int_0^{ \pi }\frac{x^3(\pi-x)^3}{\sin^3 x} dx \ ?  $$ (Note that the easier one $$\int_0^{ \pi }\frac{x (\pi-x) }{\sin x} dx = 7~\zeta(3) ,\text { equivalently }\  \int_{0}^{1}\frac{x - x^{2} }{ \sin(\pi x)}dx = 7\frac{\zeta (3)}{\pi^{3}},$$ has been solved here .)","['integration', 'complex-analysis', 'calculus', 'definite-integrals']"
4437038,Points stabilised by conjugate of a group,"Let $G$ be a reductive group acting on an affine variety $X$ . Let $x\in X$ such that the stabiliser of $x$ in $G$ is a subgroup $H$ which is itself a reductive group. Then, it is easy to see that any point in the orbit $Gx$ is stabilised by some conjugate of the group $H$ in $G$ . Now, as the orbit $Gx$ isn’t necessarily closed, we can consider an arbitrary point $y$ in the closure of this orbit. Is the point $y$ necessarily stabilised by some conjugate of $H$ ? To simplify things, we can assume that the orbit $Gy$ of $y$ is closed in $X$ , and then, does the question have a positive answer in this case? As a reality check, this statement is trivially true when $H=G$ or if $H$ is trivial. The question is essentially asking if being closed by a conjugate of $H$ is an algebraic condition, but I don’t know how to see it.","['group-actions', 'group-theory', 'algebraic-geometry', 'reductive-groups']"
4437079,Yet another definition of uniform integrability. Is it equivalent to the classical definition by G.A. Hunt?,"I came across this posting where a definition of uniform integrability (found in Tao, T., Introduction Measure Theory. AMS, GTM vol 126, 2011) is given as follows: Definition T:  Suppose $(X,\mathscr{B},\mu)$ is a measure space (not necessarily finite). A sequence $f_n:X \rightarrow \mathbb{C}$ of absolutely Integrable functions is said to be uniformly Integrable if the following three statements hold (Uniform bounded on $L^1$ ) One has $\sup_n\|f_n\|_{L^1(\mu)}=\sup_n\int_{X}|f_n|d\mu <+\infty$ . (No escape to vertical infinity) One has $\sup_n\int_{|f_n|\ge M}|f_n|d\mu\xrightarrow{M\rightarrow\infty} 0$ . (No escape to width infinity) One has $\sup_n\int_{|f_n|\le\delta}|f_n|d\mu\xrightarrow{\delta\rightarrow0} 0$ . This is in contrast to the common used definition of uniform integrability by Hunt (Hunt, G.A., Martingales et Precessus de Markov , Paris: Dunod, 1966 pp. 254) which I state as follows: Definition H: A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0
\tag{1}\label{one}
\end{align}$$ Definition H is widely used on Probability theory and in functional analysis, for example, it provides an extension of Pettis-Dunford's theorem for $\sigma$ -finite spaces. There are several known equivalences to Hunt's definition, some of which I will state below. Observation: For a finite measure space, condition (3) Definition T is rather superfluous and, as it can be easily seen, Definitions H and T are equivalent in this setting. Problem(s): (a): Are  Definitions H and T  equivalent for general measure spaces (or at least for infinite $\sigma$ -finite measures, or only for countable families $\mathcal{F}\subset L_1$ )? (b): If not, are there any applications (problems) where Definition H does a job that Definition T can't deliver (and vice versa). I spent some considerable amount trying to work through (a). I was unsuccessful to show either of the following statements: Prop 1: Definition H implies Definition T. Definition H does imply condition (1) and (2) in general measure spaces. Whether (3) also holds, escapes me. Prop 2: Definition T implies Definition H. Did not go far at all. If all this is well known, a reference would be appreciated. Here are some well known equivalencies of Definition H. Notation: Suppose $(X,\mathscr{B},\mu)$ is a measure space. When the the ambient space is clear from the context, we use $L_1$ as a shorthand for $L_1(X,\mathscr{B},\mu)$ , and $\|f\|_1:=\int|f|\,d\mu$ (the $L_1$ -norm). We also use the notation $L^+_1=\{f\in L_1: f\geq0\}$ . Given $g,h\in L_1$ with $g\leq h$ $\mu$ -a.s., we denote $$[g,h]=\{f\in L_1: g\leq f\leq h\}$$ For any $\mathcal{A}\subset L_1$ and $f\in L_1$ , $$d(f,\mathcal{A}):=\inf\{\|f-\phi\|_1: \phi\in\mathcal{A}\}$$ The following is from K. Bichteler, Integration: A functional approach , Birkhäuser Verlag, 1998. p.p. 57. Definition B: A family $\mathcal{F}\subset L_1$ is uniformly integrable if for any $\varepsilon>0$ , there are $g,h\in L_1$ with $g\leq h$ such that \begin{align}\sup_{f\in\mathcal{F}}d(f,[g,h])<\varepsilon\tag{2}\label{2}
\end{align} Comments: For any $x\in\mathbb{R}$ and $a<b$ , define $x^b_a:=(a\vee x)\wedge b$ . It is easy to check that if $c\leq a\leq b\leq d$ , then $$|x-x^d_c|\leq |x-x^b_a|$$ Thus, Definition B may be rewritten as: Definition B': $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\inf_{g\in L^+_1}\sup_{f\in \mathcal{F}}d(f,[-g,g])=0$$ Comments: Definition B means that  a family $\mathcal{F}\subset L_1$ is uniformly integrable if their elements are close to being dominated by an integrable function. Now, for $g\geq0$ , $|f-f^g_{-g}|=(|f|-g)_+$ ; hence, as $|x-x^a_{-a}|\leq |x-b|$ for all $|b|\leq a$ ,  Definition B  can also be rewritten as (see, for example (Klenke, A., Probability Theory, Springer 2006. pp. 134) Definition H': A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_X(|f|-g)_+\,d\mu=0 \tag{1'}\label{onep}
\end{align}$$ Comments: Observe that if $g\geq0$ , then $$|f|\mathbb{1}_{\{|f|>2g\}}\leq (|f|-g)_+\mathbb{1}_{\{|f|>2g\}}+g\mathbb{1}_{\{|f|>2g\}}\leq 2(|f|-g)_+\mathbb{1}_{|f|>2g\}}$$ Hence, if $\mathcal{F}$ is uniformly integrable in the sense of Definition H' then, $$\begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0,
\end{align}$$ that is, $\mathcal{F}$ is uniformly integrable in the sense of Definition H.  Conversely, notice that for any $g\geq0$ $$(|f|-g)_+\leq|f|\mathbb{1}_{\{|f|>g\}}.$$ Hence, if $\mathcal{F}\subset L_1$ satisfies \eqref{one} then, it is uniformly integrable in the sense of Definition H'. This establishes the equivalence of Definitions B, H and H'. Notice that for any $A\in\mathscr{B}$ and $g\geq0$ $$|f|\mathbb{1}_A\leq|f|\mathbb{1}_{\{|f|>g\}}+g\mathbb{1}_A$$ This leads to another equivalency Theorem 1: A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty$ , and for any $\varepsilon>0$ there exists $g_\varepsilon\in L^+_1$ and $\delta>0$ such that for any $A\in\mathscr{B}$ , $$\int_Ag_\varepsilon<\delta\qquad\text{implies}\quad \sup_{f\in\mathcal{F}}\int_A |f|\,d\mu<\varepsilon$$ Proof: Necessity is obvious. For sufficiency, suppose $\mathcal{F}$ has the property described in the Theorem above. For $\varepsilon>0$ choose let $a$ , $g_\varepsilon\in L^+_1$ and $\delta_\varepsilon$ as in the statement of the Theorem. For any $c>0$ $$\int_{\{|f|>cg\}}g\,d\mu \leq\frac{1}{c}\int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu\leq\frac{a}{c}$$ Thus, for $c>\frac{a}{\delta_\varepsilon}$ , we obtain that $\int_{\{|f|>cg_\varepsilon\}}g_\varepsilon\,d\mu<\delta_\varepsilon$ and so, $$\int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu<\varepsilon$$ The uniform integrability follows. The case $\mu(\Omega)<\infty$ is of interest in the Theory of Probability. In this case, the infimum in  Definition H (equivalently in Definition H') can be taken over nonnegative numbers: Theorem 2: Suppose $(X,\mathscr{F},\mu)$ is a finite measure space. A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align}
\inf_{a\geq0}\sup_{f\in\mathcal{F}}\int_{\{|f|>a\}}|f|\,d\mu=0\tag{4}\label{four}
\end{align}$$ or equivalently $$\begin{align}
\inf_{a>0}\sup_{f\in\mathcal{F}}\int_X(|f|-a)_+\,d\mu=0\tag{4'}\label{fourp}
\end{align}$$ Proof: Given $\varepsilon>0$ choose $g_\varepsilon\in L^+_1$ and $a_\varepsilon\in(0,\infty)$ such that $$\begin{align}
\sup_{f\in \mathcal{F}}\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu<\varepsilon/2, \qquad
\int_{\{g_\varepsilon>a_\varepsilon\}}g\,d\mu<\varepsilon/2\end{align}$$ Then, for any $f\in \mathcal{F}$ $$\int_{\{|f|>a_\varepsilon\}}|f|\,d\mu\leq\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu+\int_{\{g>a_\varepsilon\}}g_\varepsilon\,d\mu<\varepsilon$$ Conversely, when $\mu(X)<\infty$ , the family of nonnegative constant functions is contained in $L^+_1$ and so, \eqref{four} implies \eqref{one} Comment: Versions of this theorem are stated as a definition in many probability textbooks. For $\sigma$ -finite measures, here is another equivalency found in the literature. Theorem 3: Suppose $(X,\mathscr{B},\mu)$ is $\sigma$ -finite and let $h\in L^+_1$ with $h>0$ a.s. A family $\mathcal{F}\subset L_1$ is uniformly integrable iff (i) $a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty$ , and (ii) for any $\varepsilon>0$ there is $\delta>0$ such that, for any $A\in\mathscr{B}$ $$\int_Ah\,d\mu<\delta,\qquad\text{then}\quad \sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon$$ Comment: Notice that if $\mu(X)<\infty$ , one may take $h\equiv1$ to obtain another familiar definition of uniform integrability that appears in many probability textbooks. Proof to Theorem 3: $\sigma$ -finiteness implies the existence of functions $h\in L^+_1$ with $h>0$ a.s. If $\mathcal{F}$ is uniformly integrable then, choose $g\in\mathcal{L}^+_1$ such that $$\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu\leq\varepsilon/3$$ Then, for any $f\in\mathcal{F}$ $$\int_X|f|\,d\mu\leq\int_{\{|f|>g\}}|f|\,d\mu+\int_{\{|f|\leq  g\}}g\,d\mu<\varepsilon/3 +\|g\|_1$$ Since $\phi_n:=\mathbb{1}_{\{g>nh\}}\xrightarrow{n\rightarrow\infty}0$ a.s., dominated convergence implies that $\int\phi_n g\,d\mu\xrightarrow{n\rightarrow\infty}0$ . Choose $N\in\mathbb{N}$ large enough so that $\int \phi_n g\,d\mu<\varepsilon/3$ . For any $A\in\mathscr{B}$ $$
|f|\mathbb{1}_A\leq |f|\mathbb{1}_{\{|f|>g\}} + g\mathbb{1}_{\{g>Nh\}}+ Nh\mathbb{1}_A$$ Thus, for $\delta=\varepsilon/(3N)$ , $\int_Ag\,d\mu<\delta$ implies $\sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon$ . The converse follows as in the proof of Theorem 3.","['integration', 'measure-theory', 'real-analysis']"
4437080,Adaptive step size for Euler Method - How to create?,"I think Euler's Method is a great way to simulate ODE:s. It's not the most accurate, but it's the fastest and simplest. Euler's Method is usually used with fixed step size, where $k$ is the step size larger than $0$ and $\dot x = f(x,u)$ is our ODE function. To simulate forward Euler, just iterate this equation: $$x_{i+1} = x_i + k f(x_i, u)$$ To improve stability for Euler's method, then the step size $k$ needs to be adaptive. The smaller choice of the step $k$ , the more stability is guaranteed, but the less accurate the simulation will become. Instead, the adaptive method for the step size is often used to find the best $k$ . Question: I can I improve stability for Euler's method by implementing adaptive step size?","['numerical-methods', 'nonlinear-dynamics', 'ordinary-differential-equations', 'eulers-method']"
4437160,"Regarding the derivative of Euclidean L2 norm, Definition of differentiation in Rudin.","I am trying to understand the answer posted by hemanth in this post . I understand how he derived the derivative $f:\Bbb R^n \rightarrow \Bbb R$ defined as $f(x)=\rVert x \rVert$ , $$
Df(x) = \nabla_x\rVert x\rVert_2= \frac{x}{\lVert x \rVert}.
$$ So we have $Df:\Bbb R^n \rightarrow L(\Bbb R^n,\Bbb R)$ , but according to Rudin, $Df(x)$ must be a linear transformation from $\Bbb R^n$ to $\Bbb R$ , while I do not see $Df(x)h \in \Bbb R$ for any h $ \in \Bbb R^n$ . I am trying to derive $Df(x)$ such that: $$
\lim_{h\rightarrow0}=\frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert},$$ so $Df(x)h$ must be a real number. Could it be that $Df(x)h$ is a product of a column vector and a row vector? If so, how does $$
\begin{split}
\frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert} &= \frac{\Vert\rVert x+h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}\\ 
&\le\frac{\rVert\rVert x \rVert+\rVert h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}=\frac{\rVert\rVert h \rVert -\frac{x}{\rVert x \rVert}h \rVert}{\rVert h \rVert}\rightarrow 0\;,
\end{split}
$$ as $h \rightarrow 0$ ? Anyone can help me with this confusion? It is my first time asking the question here, so my formatting might be bad. Thank you for understanding.","['real-analysis', 'multivariable-calculus', 'linear-algebra', 'linear-transformations', 'derivatives']"
4437187,Can you find a derivative of a function $f(x)$ with respect to something that isn't $x$?,"I am not sure how to phrase this exactly, but an example of what I'm talking about is finding the derivative of $x^4$ with respect to, say, $x^2$ . I was just thinking, maybe you could use some substitution to find the answer, and make $x^2=a$ , and thus find $d/dx$ of $a^2$ with respect to $a$ , and hence $d/d(x^2)$ of $x^4$ would be $2x^2$ . Can you do this? Or is it illegal?",['derivatives']
4437194,Separating two non-transversal manifolds via deformation,"I was reading Guillemin and Pollack's book to self-study differential topology, where I encountered the following two related problems (Exercise 5 and 6 in Section 2.3): (Exercise 5)  Assume that one compact submanifold $X \subset Y$ intersects another sub-manifold $Z$ and satisfies $\dim X + \dim Z < Y$ . Show that $X$ can be pulled away from $Z$ by an arbitrarily small deformation. That is to say, for any $\epsilon>0$ there exists a deformation $X_t = i_{t}(X)$ such that $X_1 = i_1(X)$ doesn't intersect $Z$ and $|x-i_1(x)|<\epsilon \ (
\forall \ x \in X)$ (Exercise 6) The claim in Exercise 5 above can be sharpened as follows: if $Z$ is closed in $Y$ and $U$ is an open set in $X$ containing $Z \cap X$ , then the deformation $X_t = i_t(X)$ above can be chosen to be constant outside of $U$ . A related post about Exercise 5 is here: Show that $X$ can be pulled apart from Z by an arbitrary small deformation I have been stuck on these two problems for several weeks. Basically, I have tried following the hint in the book and Prof. Shifrin's answer to solve the first problem, but I'm still quite confused. It seems that we need to define some smooth map $F:X \times S \rightarrow Y$ to apply the Transversality Theorem, where $S$ is some ball of radius $\epsilon$ , right? However, how can we ensure that the whole map $F$ and its boundary restriction $\partial F$ are transversal to $Z$ ? For the second problem, I'm still having no clear clue... I would appreciate it if someone is willing to provide some hints/suggestions, as it's my first time reading this book. Thanks again in advance!","['submanifold', 'geometry', 'smooth-manifolds', 'manifolds', 'differential-topology']"
4437216,Approximate LCM (Least Common Multiple) of $n$ random $k$-digit numbers,"I choose $n$ different $k$ -digit numbers randomly. I was wondering, roughly, what one can expect their LCM (least common multiple) to be? Preferably in Big O (or Big $\Theta$ ) notation. I'm particular interested in the dependence on $k$ and $n$ . Feel free to assume $n \ll e^{O(k)}$ , or any other reasonable assumptions. As related examples, $LCM(1,\ldots,n) \in O(e^n)$ , and a randomly chosen $k$ -digit number has a probability $O\left(\frac{1}{k}\right)$ of being prime. For bonus points, I'd also love an answer for the same question, but for the GCD (greatest common divisor), instead of the LCM.","['number-theory', 'gcd-and-lcm', 'asymptotics', 'discrete-mathematics', 'prime-numbers']"
4437220,Mathematical proof of induction,"Before I get into my question, I'd like to apologize if this is a silly one. I am still new to induction so I am trying to wrap my head around it. My textbook states that the principle of mathematical induction is deduced from the Well-Ordering Principle (WOP). This I understand. However, I was thinking that if mathematical induction is based on the fact that we have a basis step - that we prove to be true and we then go on to say assume that $P(n)$ is true for some $n$ , then we prove $P(n+1)$ is true for some $n$ , and this is basically like an iteration over and over again that repeats so many times that eventually all values become true - why can't we just show that if $P(1)$ is true and $P(2)$ is true, then $P(n)$ is true based on the principal of mathematical induction? Where is the issue with this? Also, why is it that when we prove for $P(n+1)$ that we sub in $P(n)$ . I understand the logic behind it, but why does assuming that if the statement we assumed to be true, which was, $P(n)$ , makes up a part of $P(n+1)$ , then $P(n+1)$ is true. For e.g., we know that $P(n)$ is the statement $1 + 3 + \ldots + (2n-1) = n^2$ and we prove the basis step and for the inductive step, we assume that it is true for some $P(n)$ . Then we say ""show $P(n+1)$ is true"" and we do so by saying $1 + 3 + \ldots + (2n - 1)+ (2n + 1) = (n+1)^2$ because from $1$ to $(2n-1)$ that is $n^2$ and $n^2 + 2n + 1 = (n+1)^2$ . What if that $2n + 1$ was false? This would make the implication false. Do we just assume it is true? If so, why? Thank you in advance.","['algebra-precalculus', 'induction']"
4437223,Expectation of Spherically Symmetric Random Vector,"Suppose we are considering a vector valued random variable $X \in \mathbb{R}^d$ that is ""spherically symmetric"" in the sense that $$ E \frac{X}{\|X\| } = 0.$$ Suppose $a_n \in \mathbb{R}$ is a deterministic sequence tending to zero. What I want to know is:  what additional conditions (if any) are needed to imply that, for any fixed nonzero vector $u$ , $$
E \frac{X}{\|X - a_n u\| } \to 0 ? 
$$ And $$
E \frac{\|X\|^2}{\|X - a_n u\|^2} \mbox{ is bounded}?
$$ Intuitively it seems these should follow from some ""dominated convergence"" condition since as $a_n \to 0$ , $\frac{X}{\|X - a_n u\| } \to X/\|X\|$ almost surely, and $\| X/\|X\| \| =1$ . Any help is much appreciated!","['probability-theory', 'probability']"
4437226,"How to show that $(X,B)$ and $(Y,W)$ satisfy the same SDE if their joint law is equal?","Let $(X,B)$ , where $B$ is a standard BM and $X$ is process, satisfy the SDE $dX_t = b(t,X_t) + \sigma(t,X_t)dB_t$ . Suppose that for some other standard BM $W$ and process $Y$ we have that the joint law of the processes $(X,B)$ and $(Y,W)$ are equal. How do I show that the second pair also satisfies the SDE? This was claimed in a talk, but no justification was given, and I am not sure how to prove it. Thanks!","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4437334,Normal bundle of a submanifold is again a manifold,"I'm studying differential topology by myself by reading Guillemin and Pollack's book and I encountered the following problem (Exercise 12 in Chapter 2.3): Given $Z \subset Y \subset \mathbb{R}^M$ , we consider the normal bundle to $Z$ in $Y$ defined as follows: $$N(Z;Y) := \{(z,v): z \in Z, v \in T_{z}(Y) \text{ and } v \perp T_{z}(Z)\}$$ Show that the normal bundle $N(Z;Y)$ is a manifold with the same dimension as $Y$ . I have tried following the given hints to use Exercise 1.4.4 and proof of the Proposition on page 71 of the book. This gives us a parametrization $\phi: U \times \mathbb{R}^{l} \rightarrow N(Z;\mathbb{R}^{M}) = N(Z)$ , which implies that the normal bundle $N(Z)$ is a manifold. However, I have been stuck on showing that this parametrization restricts to a parametrization $U \times \mathbb{R}^k \rightarrow N(Z;Y)$ , which is suggested by the hint... I have been thinking about this problem for almost a month without any significant progress, so any help/hints would be appreciated! Thank you so much in advance! Also, a post on the same question: Normal bundle to $Z$ in $Y$ is a manifold with same dimension as $Y$ (Exercise 2.3.12 of Guillemin-Pollack) (Though unfortunately there's no answer in this post...) In addition, I have attached three images of the book below, just in case that they might be useful: The problem along with the hint: The claim proved in Exercise 1.4.4: Proof of the Prop on page 71:","['geometry', 'smooth-manifolds', 'manifolds', 'general-topology', 'differential-topology']"
4437466,What is a path in a topological space?,"A path in a topological space X is a continuous function from the closed unit interval [0, 1] into X. What happens when the topological space is something more simple, for example given $X = \{ 1, 2, 3, 4\},$ consider the topology $\tau = \{ \varnothing, \{ 2 \}, \{1, 2\}, \{2, 3\}, \{1, 2, 3\}, X \}$ of six subsets of $X$ . If I now have a path in the example topological space $(X,\tau)$ , what is the respective continuous function? Does it map from time to subsets? How can it be continuous if it jumps discretely from subset to subset? The concept is very intuitive in e.g. $\mathbb{R}^2$ , but not so clear what is meant in this elementary case.",['general-topology']
4437482,Transitivity of pointwise convergence?,"I want to know if the following statement is true or not: Here we consider functions from $\mathbb{R}$ to $\mathbb{R}$ .
We are given a function $f$ , families of functions $\mathcal{F}$ and $\mathcal{G}$ with the following properties: (1) There is a sequence of functions $\{ \varphi_k \}$ in $\mathcal{F}$ pointwise convergent to $f$ . (2) For each function $\varphi \in \mathcal{F}$ , there is a sequence of functions $\{ \psi_l \}$ in $\mathcal{G}$ that converges pointwise to $\varphi$ almost everywhere. Then we can find a sequence of functions in $\mathcal{G}$ that converges pointwise to $f$ almost everywhere. In particular, if $f$ is any measurable function, $\mathcal{F}$ is the set of all simple functions and $\mathcal{G}$ is the set of all step functions then this is true. I believe it is generally false but I couldn't come up with a counter example. Initially, my question was the same as Measurable Functions as Limits (a.e) of Step Functions , which deals with the special case I described. I read both the proof in the book by Stein-Shakarchi and the answer of the question, but they rely on something stronger: (3) For each function $\varphi \in \mathcal{F}$ and $\epsilon > 0$ , we can find $\psi \in \mathcal{G}$ which agrees with $\varphi$ except for a set $E$ with $m(E) < \epsilon$ . Edit: I'm asking this question because the book starts the proof by saying: it suffices to show that every simple functions can be approximated by step functions and concludes: we've found a sequence of step functions that converges pointwise to given simple function almost everywhere. This seems to me, is using the general statement I wrote above even though we had a stronger condition (3) in the middle of the proof.","['analysis', 'real-analysis']"
4437512,Volume computed by a double integral,"We are asked to find the volume of the following solid: $$x=0, y=0, z=0, x+y+z=2, y^2=1-z, y>0$$ We should compute $\displaystyle \iint_{T}(x+y-y^2-1)dxdy$ , but I cannot figure out which is the orthogonal $T$ , in which we can integrate. I have tried to transform the above expressions into polar coordinated, but this just made things worse, so I was trying to consider the zyx coordinates system instead. I know that the result is $\frac{49}{60}$ .
Any advice on this problem would be appreciated.","['multivariable-calculus', 'volume']"
4437544,"Let $\left\vert\frac{a_{n+1}}{a_n}\right\vert\to 1,$ define $f:(-1,1)\to\mathbb{R};\ f(x)=\sum_{n=0}^{\infty}a_n x^n.$ Is $f$ continuous?","Suppose $(a_n)_n$ is a real sequence with $\displaystyle\lim_{n\to\infty}\left\vert \frac{a_{n+1}}{a_n} \right\vert=1.\ $ Define $f:(-1,1)\to\mathbb{R};\ f(x)=\displaystyle\sum_{n=0}^{\infty}a_n x^n.$ By application of the Limit comparison test and comparing this sum to a geometric series (i.e. using the ratio test also), $f(x)$ converges for all $x\in (-1,1),\ $ and so we see that $f$ is well-defined. But can we prove that $f(x)$ is: Continuous, differentiable etc? I have tried to show that $f(x)$ is continuous i.e. showing that $\displaystyle\lim_{h\to 0}\left(f(a+h)-f(a)\right)=0$ via the following method: $$f(a+h)-f(a) = (a_0 - a_0) + a_1(a+h-a) + a_2((a+h)^2-a^2) + a_3((a+h)^3-a^3) + \ldots$$ $$ = 0 + a_1 h + a_2 h (2a+h) + a_3 h (3a^2 + 3ah + h^2) + \ldots $$ $$ = h\left[ a_1 + a_2 (2a+h) + a_3 (3a^2 + 3ah + h^2) + \ldots \right]. $$ Using also the fact that $\vert a \vert < 1,$ can it be proven that the sum inside the square bracket is bounded as $h\to 0\ ?$ It would then follow that $\displaystyle\lim_{h\to 0}\left(f(a+h)-f(a)\right)=0.$ Or is there some other method to prove continuity of $f$ ?","['power-series', 'sequences-and-series', 'real-analysis']"
4437599,Every open subspace of a separable topological space is separable,"I have encountered this well-known result when reading about separable spaces. Let $(E, \tau)$ be a separable topological space. Let $X \in \tau$ and $\tau_X$ its subspace topology. Then $(X, \tau_X)$ is separable. Could you verify if my below proof is fine? Let $D$ be a countable dense subset of $E$ . This implies $\overline{D}^{\tau} = E$ , or equivalently $$
\forall x \in E, \forall \text{ nbh } V \text{ of } x \text{ in } \tau, V\cap D \neq \emptyset.
$$ Notice that if $x \in X$ and $V$ is a nbh of $x$ in $\tau$ , then $V \cap X$ is also a nbh of $x$ in $\tau$ . This implies $$
\forall x \in X, \forall \text{ nbh } V \text{ of } x \text{ in } \tau, (V \cap X)\cap (D \cap X) \neq \emptyset.
$$ Hence $$
\forall x \in X, \forall \text{ nbh } V \text{ of } x \text{ in } \tau_X, V\cap (D \cap X) \neq \emptyset.
$$ It follows that $D\cap X$ is countable and dense in $X$ , i.e., $\overline{D}^{\tau_X} = X$ .","['general-topology', 'solution-verification', 'separable-spaces']"
4437659,"Let (X, τ ) be a topological space. Show that ${\{x\}}= \bigcap_{G \in τ }G $","Let (X, τ ) be a topological space. Suppose that for any x ∈ X one has that {x} is a closed set. Show that: It is known that $\bar{\{x\}}=\{x\}$ by theorem $${\{x\}}= \bigcap_{\{x\} \subset F}F  $$ with F closed. Then $$X-\{x\}=X- \bigcap_{\{x\} \subset F}F  $$ by DeMorgan Law $$X-\{x\}=\bigcup_{\{x\} \subset F}(X-F)  $$ The problem is that I arrive at the union and not at the intersection.","['general-topology', 'topological-vector-spaces', 'functional-analysis']"
4437682,Question on elliptic curve of Weierstrass form $y^2 = x^3+ax+b$: Any class there?,"I want to present a brief question. I'm curious whether there is any class of Weierstrass form $y^2 = x^3+ax+b$ that we can assign them as rank $0$ by some particular property. In other words, is there any local, if not global, condition known to $\{(a,b)\ \mid a\in \mathbb{Q}, b\in \mathbb{Q}\}$ that $y^2=x^3+ax+b$ being rank $0$ ? As long as I know the paper from Bhargava et al. shows the biggest result regarding the rank of elliptic curves and the related conjectures so far, but I'm not sure whether there is any indication that my curiosity could be solved. If my question was not clear please let me clarify them with further comments or edits. Thanks.","['algebraic-number-theory', 'elliptic-curves', 'number-theory', 'abstract-algebra', 'algebraic-geometry']"
4437743,False proof: Every linear operator (matrix) has an eigenvalue.,"Below is a proof that any linear operator must have an eigenvalue. The proof obviously contains a mistake because the statement is wrong. But I do not see the mistake. Please point it out if you see it. Assumption: $V$ is a finitely dimensional $K$ -linear space ( $K$ is a field with infinitely many elements, e.g. $\mathbb R$ ), $V \ne \{0\}$ , $L:V \to V$ is a linear operator. Claim: $L$ must have an eigenvalue $\lambda \in K$ , i.e. there must be $\lambda \in K$ and $0 \ne v \in V$ such that $L(v)=\lambda v$ . Proof: Assume that $L$ does not have an eigenvalue. Then for all $\lambda \in K$ it should hold: $L-\lambda I$ is bijective ( $I$ is the identity operator). So we have an infinite family of bijective linear operators $S:=(L-\lambda I)_{\lambda \in K}$ in the linear space $H$ of linear operators $V\to V$ that is finitely dimensional ( $\dim H = (\dim V)^2$ ). $S$ is a linear independent family. Indeed, let $a_1,\ldots,a_n \in K$ such that $0=\sum_{i=1}^n a_i (L-\lambda_i I)=(\sum_{i=1}^n a_i)L - (\sum_{i=1}^n a_i\lambda_i) I$ . If not all $a_i$ are $0$ , the $S \ni L - \frac{ \sum_{i=1}^n a_i\lambda_i}{\sum_{i=1}^n a_i} I=0$ . Therefore $S$ contains a zero operator. This contradicts the statement above that all elements in $S$ are bijective and $V \ne \{0\}$ . So we have found an infinite linearly independent family $S$ in a finitely dimensional linear space $H$ . That is a contradiction. This $L$ has an eigenvalue. Addition : $H$ is finitely dimensional Proof: fix a basis of $V$ say $e_1,\ldots,e_m$ . Consider linear operator $L_{i,j}$ for which hold $L(e_i)=e_j$ and $L(e_k)=0$ for $k \ne i$ . Than $H$ is panned by the finite family $(L_{i,j})_{j,i = 1,\ldots m}$","['linear-algebra', 'fake-proofs', 'eigenvalues-eigenvectors']"
4437750,Want to show that $f=0$ a.e.,"This question has been asked before but I can't understand the given solution: To show that an integral is 0 a.e. if it is 0 over every subset of measure 2/3. Let $f\in L^1[0,1]$ such that for every $E\subset[0,1]$ with $m(E)=2/3$ , $\int_E f=0$ . Show that $f=0$ a.e. I'm seeking a solution that doesn't use the Lebesgue Differentiation theorem. My idea was to write sets $$E_n=\{x\in[0,1]:|f(x)|>1/n\}$$ and show that $m(E_n)=0$ for each $n$ . But I don't know how to use the given hypothesis. How do I use that $\int_E f=0$ if $m(E)=2/3$ ?","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4437860,What are the numbers that can be uniquely represented as the sum of two squares?,"I know that primes of the form $4k+1$ can be represented uniquely as the sum of two squares (not counting order or negative numbers). But there are others like $8$ and $9$ that are not ""prime and of the form $4k+1$ "" that can also be represented uniquely. Is there a general pattern?","['number-theory', 'elementary-number-theory']"
4437917,Calculate $\lim_{N\to+\infty}\left(\sum_{n=1}^N \frac{1}{n^2}-\frac{\pi^2}{6}\right)N$,"How to calculate the limit $$\lim_{N\to+\infty}\left(\sum_{n=1}^N \frac{1}{n^2}-\frac{\pi^2}{6}\right)N?$$ By using the numerical method with Python, I guess the right answer is $-1$ but how to prove? I have no idea.",['limits']
4437926,"Is there a special name for a norm that satisfies $||x+y|| \leq \max\{||x||, ||y||\}$?","If we have a vector space $V$ with a norm $||\cdot ||$ , is there a special name for it if it satisfies $$||x+y|| \leq \max\{||x||, ||y||\} $$ for any $x,y \in V$ ? Im writing up something and I need a norm that satisfies this, but I cant recall if it has a special name. Thanks!","['normed-spaces', 'vector-spaces', 'analysis']"
4438008,Verify if the proposed equivalence $A \subset B \land A \subset C \iff A \subset (B \cup C)$ holds,"Only the right direction $(\Rightarrow)$ is true. Proof : Suppose that $A \subset B$ and $A \subset C$ and let $x \in A$ . Then clearly $x$ is also in $B$ . $A \subset B \Rightarrow x \in B \cup C$ . Since $x$ was arbitrary, this shows that $A \subset (B \cup C)$ . On the other hand, the converse is not true: suppose that $A = \{5, 6, 7\}, B = \{5, 6\}$ , and $C =\{7,8\}$ . Then $A \subset \{5, 6, 7, 8\} = B \cup C$ but it is neither true that $A \subset B$ (since $7 \in A$ but $7 \notin B$ ) nor $A \subset C$ (since $5 \in A$ but $5 \notin C$ ). Am I correct and is there another way simpler or is this the simplest or the only conventional one? Well, yeah I know this is just straightforward.","['elementary-set-theory', 'solution-verification']"
4438031,Is every bounded linear functional on a subspace of a Hilbert space given by a function?,"This question comes from the Limiting Absorption Principle (LAP). I want to obtain the most general statement possible, so I proceed as follows. Let $M$ be a topological space, $(H, \langle \cdot, \cdot \rangle)$ a Hilbert space, and $\mathcal L(D, H)$ the space of linear operators from the dense domain $D \subset H$ into $H$ (not necessarily bounded). Suppose $P : M \to \mathcal L(D, H)$ is continuous in the sense that $P(x) - P(y)$ is a bounded operator and its norm is continuous over $M \times M$ . Assume further that $P(x)$ has a bounded inverse for all $x$ in a subset $\Omega \subset M$ . This inverse may be locally bounded, but it may blow up close to the boundary $\partial \Omega$ . To remedy this, we look for a Banach subspace $Y \subset H$ with a stronger norm $||\cdot ||_Y$ than that of $H$ . By the canonical identification of $H = H ^*$ , we may regard $H$ as a subset of $Y^*$ , and, if we picked the right subspace, we will have that $\sup_{x \in \Omega} ||P(x)^{-1}||_{Y \to Y ^*} < \infty$ . Now, for any $u \in Y$ , the sequence $u_n = P(x_n)^{-1} u$ is bounded in $Y^*$ , so the Banach-Alaoglu theorem furnishes a weak* limit $u_\infty \in Y^*$ s.t. $u_n(y) \to u_\infty(y)$ for all $y \in Y$ . Since $u_\infty$ is a bounded linear functional on the subspace $Y$ , the Hahn-Banach theorem furnishes us an extension $\tilde u_\infty$ of $u_\infty$ to all of $H$ . By the Riesz Representation theorem, $\tilde u_\infty(y) = \langle g, y\rangle$ for some $g \in H$ . If we show that this limit is unique, this $g$ would be a reasonable extension of $P^{-1}$ to a point on the boundary of $\Omega$ . In the classical LAP, $M = \Bbb C$ , $H = L^2(\Bbb R)$ , $P(x)$ would be something like $-\Delta + x$ , with $\Omega = \Bbb C \setminus [0, \infty)$ , and $Y$ an appropriately weighted $L^2$ space. The problem is that in this case, the extension of the resolvent $P(x)^{-1}$ to the real line is a map into the dual of $Y$ (which is just another weighted $L^2$ space with in inverse weight). This evinces a flaw in my previous argument because according to the above paragraph, the extension of the resolvent should take values in the same Hilbert space $H = L^2(\Bbb R)$ . What am I doing wrong?","['spectral-theory', 'functional-analysis', 'mathematical-physics', 'real-analysis']"
4438043,Certain Set of Smooth Conics is Finite,"This question comes from Darmon's ''Rational Points on Curves,"" page 16. Let $S$ be a finite set of primes. Let $\mathcal O$ denote the ring of $S$ -integers of a number field $K$ . Let $\mathcal M_0(\mathcal O)$ denote the set of smooth genus $0$ curves over $\operatorname{Spec} \mathcal O$ , which we know is the set of smooth conics over $K$ with good reduction outside $S$ . Darmon says that the results of Class Field Theory can be used to show that this set is finite, of cardinality $2^{\#S + r - 1}$ , where $r$ is the number of real places of $K$ . Why is this true/where can I read more about it?","['class-field-theory', 'algebraic-geometry']"
4438056,A sort of converse of Banach-Steinhaus theorem.,"$(X, \|•\|) $ and $(Y, \|•\|') $ be two normed space. $\begin{align} {\scr{B}}{(X, Y) }&=\{T\in {\scr{L}}{(X,Y)}: T \text{ is bounded } \}\end{align}$ $\|T\|_{op}=\sup\{\|Tx\|':\|x\|\le 1 \}$ Question: $\forall (T_n) \subset {\scr{B}}{(X, Y)}$ be such that $T_n\to T $ pointwise $[$ i.e $\forall x\in x,$$ T_nx\to Tx $ in the space $(Y, \|•\|') ]$ implies $T\in {\scr{B}}{(X, Y)}$ . Does this implies $(X, \|•\|) $ is a Banach space? The converse is well known ( Banach- Steinhaus theorem) . But i think the above question can be answered negatively , I mean there is some counter examples but neither I can prove it not cite a counter example. I sincerely need help. Thanks.","['banach-spaces', 'examples-counterexamples', 'analysis', 'functional-analysis', 'pointwise-convergence']"
4438061,"Error in Spivak's ""Calculus on Manifolds"". Construction of function composition for proof of change of variables","""Calculus on Manifolds"", Proof of Change of Variables theorem.
I don't understand how letting $U=k^{-1}(V)$ leads to $h(U)\subset V$ . It would most definitely make sense if we let $U=h^{-1}(V)$","['real-analysis', 'functions', 'change-of-variable', 'general-topology', 'differential-geometry']"
4438168,Showing that $\mathbb Q$ is not a direct product of groups [duplicate],"This question already has an answer here : How can I prove that the additive group of rationals is not isomorphic to a direct product of two nontrivial groups? (1 answer) Closed 1 year ago . Problem: From Aluffi's Algebra: Chapter 0 , Chapter II 3.5. Prove that $\mathbb Q$ is not the direct product of two nontrivial groups. My attempt at a proof: Suppose $\mathbb Q\cong G\times H$ for some nontrivial groups $G$ and $H$ . Let $$\mathbb Q\ni 1\leftrightarrow(g, h)\in G\times H.$$ Choose a $(x, y)\in G\times H$ such that $x\ne e_G$ and $y\ne e_H$ . Say $$
(x, e_H)\leftrightarrow a/b\quad\text{and}\quad (e_G, y)\leftrightarrow c/d\text.
$$ Then $$
(x, e_H)^b\leftrightarrow a\leftrightarrow(g, h)^a
$$ which means that $
(x, e_H)^b = (g, h)^a
$ and hence $h^a = e_H$ . Similarly, $g^c = e_G$ . Now if $a$ and $c$ are both nonzero, then $|(g, h)| < \infty$ in $G\times H$ which will mean that $|1| < \infty$ in $\mathbb Q$ which is false. Hence $a = 0$ or $c = 0$ . This means that $$(x, e_H)\leftrightarrow 0\quad\text{or} \quad (e_G, y)\leftrightarrow 0$$ which means that $x = e_G$ or $y = e_H$ since $0\leftrightarrow (e_G, e_H)$ . This is the required contradiction. $\square$ Question: This seems like an overly complicated and artificial proof, as if there has to be a simpler and more elegant way here. Can you think of any? Note: Aluffi covers only the very basic group theory until this point, hence the use of more advanced stuff is not allowed.","['group-theory', 'rational-numbers']"
4438178,Integral Basis of $O_k$,"Let $K=Q(\sqrt 6,\sqrt{11})$ . Write $α ∈ O_K$ and its conjugates in terms of a $Q$ -basis.
And show that an integral basis of $O_K$ is given by ${1,\sqrt 6,\sqrt {11},\frac{\sqrt 6+\sqrt{66}}2 }$ , from first principles. I'm really not sure how to do this, if anyone could help I'd really appreciate it!","['number-theory', 'integers', 'algebraic-number-theory', 'integral-basis']"
4438180,Does Fermat's Last Theorem imply $\sqrt{2} \not \in \mathbb{Q}$?,"A well-known overkill proof of the irrationality of $2^{1/n}$ ( $n \geqslant 3$ an integer) using Fermat's Last Theorem goes as follows: If $2^{1/n} = a/b$ , then $2b^n = b^n + b^n = a^n$ , which contradicts FLT. (See this , and see this comment for the reason this is a circular argument when using Wiles' FLT proof) The same method of course can't be applied to prove the irrationality of $\sqrt{2}$ , since FLT doesn't say anything about the solutions of $x^2 + y^2 = z^2$ . Often this fact is stated humorously as, ""FLT is not strong enough to prove that $\sqrt{2} \not \in \mathbb{Q}$ ."" But clearly, the failure of one specific method that works for $n \geqslant 3$ does not rule out that some other argument could work in the case $n = 2$ in which the irrationality of $\sqrt{2}$ is related to a Fermat-type equation. ( For example , if we knew that there are integers $x,y,z$ such that $4x^4 + 4y^4 = z^4$ , then with $\sqrt{2} = a/b$ , we would have $a^4 x^4 / b^4 + a^4 y^4 / b^4 = z^4$ and hence \begin{align}
X^4 + Y^4 = Z^4, \quad \quad (X, Y, Z) = (ax, ay, bz) \in \mathbb{Z}^3,
\end{align} a contradiction to FLT.) Is there a proof along these lines that $\sqrt{2} \not \in \mathbb{Q}$ using Fermat's Last Theorem?","['number-theory', 'recreational-mathematics', 'elementary-number-theory']"
4438221,What is wrong with this surface? (principle directions),"The surface is specified, for simplicity, using an explicit function $f(x,y)$ : $$
f: \mathbb{R}^2 \rightarrow \mathbb{R}
$$ The shape of this surface at point $[x=0,y=0]$ can be described by figure . It shows the xy-plane, where black lines are straight lines with function value $f(black)=R$ and red lines are circles with the center at $[x=0,y=0]$ and radius equal to $R$ . I can construct this function by blending the plane function and sphere function according to the angle in polar coordinates using the following formula: $$
f(\phi,r)=f_{blend}(\phi) \cdot f_{plane}(\phi,r)+(1-f_{blend}(\phi)) \cdot f_{sphere}(\phi,r)
$$ Where, i.e.: $$
f_{blend}(\phi)=\frac{1}{2}\sin(4\phi)+\frac{1}{2}
$$ $$
f_{plane}(\phi,r)=R
$$ $$
f_{sphere}(\phi,r)=\sqrt{R^2-r^2}
$$ I am interested in principle curvatures at point $[x=0,y=0]$ and in principle directions. Since Euler's theorem says that principle directions should be perpendicular and principle curvatures should be the maximum and the minimum value of normal curvature at these points, there has to be some problem with the smoothness of this surface. I tried to find partial derivatives for this instance and express the curvature as a function of angle coordinate: $$
f(\phi,r)=(\frac{1}{2}\sin(4\phi)+\frac{1}{2}) \cdot R + (1-(\frac{1}{2}\sin(4\phi)+\frac{1}{2})) \cdot \sqrt{R^2-r^2}
$$ $$
\frac{\partial f(\phi,r)}{\partial r}=-\frac{r(\sin(4\phi)+1)}{2\sqrt{R^2-r^2}}
$$ $$
\frac{\partial^2 f(\phi,r)}{\partial r^2}=-\frac{\sin(4\phi)+1}{2R}
$$ For $r=0$ I get: $$
\frac{\partial f(\phi,r=0)}{\partial r}=0
$$ $$
\frac{\partial^2 f(\phi,r=0)}{\partial r^2}=-\frac{\sin(4\phi)+1}{2R}
$$ If I use a formula for the curvature of a curve given by explicit function $y:f(x)$ to express the normal curvature as a function of angle coordinate, I get: $$
k=\frac{\lvert y'' \rvert}{\sqrt{(1+(y')^2)^3}}
$$ $$
k(\phi)=\frac{\lvert -\sin(4\phi)+1 \rvert}{2R}
$$ This result says that the minimum and maximum values of normal curvature are not perpendicular. I know that it is wrong, and there is basically some problem with smoothness at this point, but I don't see what exactly it is.","['surfaces', 'curvature', 'differential-geometry']"
4438224,Mapping from dual numbers to real numbers,"Background I was naively playing around with some interpolation ideas once again and came across the dual numbers as a way to perform differentiation implicitly. Naturally, I thought, okay, perhaps there's some way for which we can map some dual numbers to the real numbers using a polynomial function. As it turns out in general, this is not the case, and I'm struggling to understand why. Symptoms For example, if we consider the function $f$ such that $f(0)=0$ and $f(\varepsilon)=1$ we know that if $f$ is an analytic function, then: $$
f(\varepsilon)=f(0)+f'(0)\cdot\varepsilon=1\implies f'(0)\cdot\varepsilon=1
$$ Obviously there is no solution to this. So perhaps we could employ the use of say, a Lagrange polynomial, which when simplified would give: $$
f(x) = \frac{x}{\varepsilon}
$$ Again, this makes absolutely no sense; multiply numerator and denominator by $\varepsilon$ and we have division by zero. Under the real numbers this expression might 'work' (using $\frac{a}{a}=1$ , $\frac{0}{a}=0$ for non-zero $a$ ), but in this context, not so. As a last ditch, I thought we could use the matrix representation of a dual number; so we could reframe our problem as something like taking a function $F$ such that: \begin{align*}
F\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \\
F\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}=\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \\
\end{align*} However, now the problem becomes that not all functions which would interpolate these values termwise (i.e. $A_{1,1}\to B_{1,1}$ ) are well-defined. Question Obviously up until now I've listed off a couple of things that I've tried, but I suspect they're merely symptoms of a same underlying problem with attempting to perform a mapping like this. Putting aside the feasibility of the outcome of something like this, is there any particular reason that this idea fails from the very beginning?","['interpolation', 'derivatives', 'hypercomplex-numbers']"
4438285,Estimating Lambda in a Poisson population where not all samples can be observed,"Let $(x_1, x_2, \dots , x_n)$ be a random sample from a population which follows a Poisson distribution with an unknown mean $\lambda$ . If we assume that $C$ is a known constant and we can only observe the values of the sample for which $x_i < C$ .
I want to try to estimate $\lambda$ by only using these samples. I first define two variables, $r$ and $p$ , which can be defined as: $$r = max(i: x_{(i)} < C)$$ $$p = max(i: x_{(i)} \le C-2)$$ , where $x_{(i)}$ denotes the $i$ th order statistic. I assume for convenience that $x_1, x_2, \dots, x_p,\dots, x_r$ are the observed samples so they are ordered. So if $X$ is a Poisson distributed random variable with density function $p(x)$ , mean $\lambda$ and $C$ as being any constant, then $$\lambda = \sum_{x=0}^{\infty}\space x\space p(x)$$ can be split up to $$\lambda = \sum_{x=C}^{\infty}\space x\space p(x) + \sum_{x=0}^{C-1}\space x\space p(x)$$ I can calculate the first part directly: $$\sum_{x=C}^{\infty}\space x\space p(x) = \lambda(1-F(C-2))$$ , where $F(.)$ is the CDF of the Poisson distribution. An estimation of the second part would be: $$\frac{1}{n} \sum_{i=1}^r x_i$$ and if $\bar{x_r}$ is the mean of the first $r$ observations, we can write this as: $$\frac{r}{n} \bar{x_r}$$ Now, we could estimate $F(C-2)$ as $\frac{p}{n}$ So combining the terms and working out for $\lambda$ , I get: $$\lambda = \frac{r\bar{x_r}}{p}$$ This seems to be a good estimator, but when $C$ becomes very small compared against the real mean, the estimation loses accuracy. The reason seems to be that estimating $F(C-2)$ from the observed samples isn't that accurate when $C$ gets small compared to $\lambda$ , even if I use a large sample size (>100K). So the questions I'm thinking about: Is there a more accurate way to estimate $F(C-2)$ ? Or maybe there ís something wrong with the math? In which case, please point out. Or maybe there is an easier way to estimate $\lambda$ from limited observed samples? EDIT I want to expand a bit based on the comments. We can also say that $X$ follows a truncated Poisson distribution conditional on the event that $X < C$ with a known $C$ , which is the truncation level. If I read from the definition then I can write the PMF of a C-truncated Poisson distribution as $$\frac{p(x)}{F(C-1)}$$ If I then work out the log-likelihood function for $\lambda$ , given the samples $x_1, x_2, \dots, x_p, \dots, x_r$ , I get: $$L(\lambda|x_1, x_2, \dots, x_p, \dots, x_r) = \log(\lambda)\sum_{i=1}^r x_i - r\log(\sum_{i=0}^{C-1} \frac{\lambda^i}{i!}) $$ Maximizing this function in $\lambda$ indeed gives me a good estimation for $\lambda$ , but it seems that we always need a numerical method for it. If someone can elaborate more from this perspective, this is always welcome as well.","['poisson-distribution', 'probability', 'estimation']"
4438287,"""Wolstenholme prime"" Related question. [duplicate]","This question already has answers here : Prove that, if $p$ is an odd prime number, then ${f(p)}=\binom{2p-1}{p-1}-1$ is divisible by $p^2$ (4 answers) Closed 2 years ago . $Q$ . If $p$ is an odd prime, show that $$
\left(\begin{array}{c}
2 p-1 \\
p-1
\end{array}\right) \equiv 1 \bmod p^{2}
$$ My approach Here, $$
\begin{array}{l}
\left(\begin{array}{c}
2 p-1 \\
p-1
\end{array}\right)=\frac{(2 p-1) !}{(p-1) !(2 p-1-p+1) !} \\
=\frac{(2 p-1) !}{(p-1) !(p) !}
\end{array}
$$ How it can be show that this is congruent to $1(mod $ p $²$$)$ ? And also tell me that is there any relation with wolstenholme prime?",['number-theory']
4438339,Proving that T is the Taylor Polynomial of f of degree n.,"Problem: Let $I$ be an interval, $f \in C^n(I,\mathbb R), x_0 \in I,$ and $T$ , a polynomial of degree $n$ with $$\lim_{x\to a}\frac{f(x)-T(x)}{(x-x_0)^n}=0.$$ Prove that T is the Taylor polynomial of $f$ of degree $n$ in $x_0 $ . I have come across several questions proving that if T is a Taylor polynomial then we have the above equation. However, I can't seem to figure out how to make my way towards this proof as it is basically the asking for the other direction of the Taylor polynomial theorem.  I see a pattern between how the above limit is similar to what is the derivative of $f(x_0)$ , but I need some help seeing the overall picture. I know we are done once I prove that: $$T(x) = \sum_{k=0}^{n}\frac{f^{k}(x)}{k!}(x-x_0)^k$$ Any help is appreciated.","['taylor-expansion', 'analysis']"
4438345,Quotient of the Galois group of a splitting field generated by two roots,"Let $f \in \mathbb{Q}[x]$ be irreducible of degree $p$ , where $p$ is a prime. Let $K$ be the splitting field of $f$ and suppose that there are roots $\alpha$ and $\beta$ of $f$ such that $K = \mathbb{Q}(\alpha,\beta)$ . We regard the Galois group $G = G(K/\mathbb{Q})$ as a subgroup of the symmetric group $S_p$ . It is not hard to show that $p$ divides the order of $G$ and $G$ contains a $p$ -cycle $\sigma$ . Let $H$ the group generated by $\sigma$ , one can use Sylow's third theorem to argue that $H$ is normal in $G$ . Now, I'd like to show that $G/H$ is a cyclic group of order diving $p-1$ . I'm trying to use results from cyclic extension but it is required that the base field contains the $n$ -th roots of unity, which is not true in this case. Any help would be appreciated.","['galois-theory', 'group-theory', 'abstract-algebra', 'sylow-theory']"
