question_id,title,body,tags
4455192,Calculation on Riemannian manifolds,"I am learning the variational calculation of Yang Mills functional, but I can't understand 2 steps in the following calculation: Given a variation of the connection $A$ in local coordinates: $A\to A+\delta A$ . Recall that $$F_{jk}=-(\partial_j A_k-\partial_k A_j+A_jA_k-A_kA_j)$$ So, \begin{equation}\begin{split}\delta F_{jk}=&-(\partial_j\delta A_k-\partial_k\delta A_j+\delta A_j\cdot A_k+A_j\delta A_k-\delta A_k\cdot A_j-A_k\delta A_j)=\\=&-(\partial_j\delta A_k+A_j\delta A_k-\delta A_k\cdot A_j-(\partial_k\delta A_j+A_k\delta A_j-\delta A_j A_k)=\\=&-(\nabla_j\delta A_k-\nabla_k\delta A_j)\end{split}\end{equation} Thus, let $I$ be the Yang-Mills functional of $F$ , then \begin{equation}
\begin{split}
\delta I=&\delta\int_X\left\langle F,F\right\rangle=2\int_X\left\langle\delta F,F\right\rangle=\\
=&2\int_X(\nabla_j\delta A_k-\nabla_k\delta A_j)g^{jl}g^{km}F_{ml}=\\
=&2\int_X-\delta A_k\cdot g^{km}\nabla^lF_{ml}+\delta A_j g^{jl}\nabla^mF_{ml}=\\
=&4\int_X\delta A_k\cdot g^{km}\nabla^lF_{lm}
\end{split}
\end{equation} To begin with, I don't understand why $$2\int_X\left\langle\delta F,F\right\rangle=2\int_X(\nabla_j\delta A_k-\nabla_k\delta A_j)g^{jl}g^{km}F_{ml}$$ is true: it seems to me that since $F\in \Gamma(End(E)\otimes \bigwedge^2T^*M, M)$ , a metric for $F$ should include both factors representing metric for $\bigwedge^2T^*M$ and factors representing metric for $End(E)$ . I think $g^{jl}$ and $g^{km}$ are factors for metric of $\bigwedge^2 T^*M$ , but where is the factors for the metric on $End(E)$ ? Besides this, I also don't understand why $$2\int_X(\nabla_j\delta A_k-\nabla_k\delta A_j)g^{jl}g^{km}F_{ml}=2\int_X-\delta A_k\cdot g^{km}\nabla^lF_{ml}+\delta A_j g^{jl}\nabla^mF_{ml}.$$ There is a remark on my note that says this is due to integration by parts, but I only know integration by parts for integration on $\mathbb{R}^1$ and doesn't know what version of integration by parts should I use here. Besides this confusion on applying integration by parts, I also don't seem to understand the definition of $\nabla^m F_{ml}$ . Many thanks in advance!","['complex-geometry', 'differential-geometry']"
4455194,"An interesting recurrent equality, possibly easier to solve in its differential form?","I encountered an interesting inequality that I'm not sure how to approach. Here $c$ is a positive constant. $$f(n+1) - f(n) = c f(n)\sum_{m=0}^n f(m)$$ I am not familiar with techniques to solve recursive equations, so I thought about a similar differential equation of it $$\frac{\partial f}{\partial x} = cf(x)\int_{z=0}^x f(z)dz.$$ I am quite new to the topic of differential equations though, so I might be missing some obvious techniques to solve this kind of equality. With my limited knowledge I tried applying Laplace transform or the Fourier transform to this equation to no avail, but the expression looks nice enough that I suspect there is an analytical solution to it. Any help to getting an analytical expression of this equation is much appreciated! (This is coming from a real-world problem, so for now, any ""nice"" assumption on $f$ and the initial conditions and so on can be placed)","['ordinary-differential-equations', 'recursion', 'recurrence-relations', 'real-analysis', 'inequality']"
4455201,Exemple where tower property of conditional expectation is NOT verify,"Question: Let $\Omega=\{a,b,c\}$ . Give an example for $X, F_1, F_2$ in which $E(E(X|F_1)|F_2) \neq E(E(X|F_2)|F_1)$ My answer: I am not at all sure of my answer. If you have any shorter and nicer answer i will be happy to read it. -Let define: (a) $F=B(\Omega ), \; F_1=\left \{ \Omega;\left \{ \emptyset  \right \} ;\left \{ a \right \};\left \{ b;c \right \}\right \}, \; F_2=\left \{ \Omega;\left \{ \emptyset  \right \} ;\left \{ b \right \};\left \{ a;c \right \}\right \} $ .
By def: $Z_{12}=E(X|F_1)$ is a rv $F_1$ measurable, $Z_{21}=E(X|F_2)$ is a rv $F_2$ measurable. (b) X a bijective measurable function from $(\Omega ; B(\Omega )) \rightarrow (\left \{ 1;2;3 \right \}; B(\left \{ 1;2;3 \right \})) $ -Proof: $Z_{12} \neq Z_{21} \; a.s$ By absurd, we assume that: $Z_{12} = Z_{21} \; a.s \; \Rightarrow E(Z_{12}) = E(Z_{21})$ . (i) But on the other side we have: $E(Z_{12}|F_1) = Z_{12}$ because is $F_1$ measurable. (ii) And by the absurd assumption: $E(Z_{21}|F_1) = E(Z_{21}) = E(Z_{12}) $ So we get from (i)+(ii): $Z_{12}= E(Z_{21})$ Wich is not necessarly always true. And of course $Z_{12} \neq Z_{21} \; a.s \; \Rightarrow E(Z_{12}) \neq E(Z_{21})$ -Now from what we just wrotte above: $E(E(X|F_1)|F_2)=E(Z_{12}|F_2)=E(Z_{12}) \neq E(Z_{21})=E(Z_{21}|F_1)=E(E(X|F_2)|F_1)$ -Q.E.D","['measure-theory', 'conditional-expectation', 'expected-value', 'borel-sets', 'random-variables']"
4455203,Why does Galois theory most naturally take place in the context of fields?,"At least as far as I can tell, historically Galois theory was a more computational tool than it appears now, and https://hsm.stackexchange.com/questions/8099/how-did-the-modern-understanding-of-galois-theory-come-about , How Did Galois Understand the Galois Group? and Intuition behind looking at permutations of the roots in Galois theory seem to say that the ""fundamental idea of Galois theory"" is to study permutations of the roots, which was first done via symmetric polynomials. The often presented motivation of Galois theory, the insolubility of the quintic, can also be presented without ""modern Galois theory"", as in this wonderful video on Arnold's proof: https://www.youtube.com/watch?v=BSHv9Elk1MU&ab_channel=notallwrong . This video also uses the ""fundamental idea of Galois theory"", i.e. studying permutations of the roots, but this time in the context of basic Riemann surfaces. My question is this: going back to the late 1800's and early 1900's, how would I realize that the ""correct"" setting of Galois theory is to study field extensions containing the roots, and automorphisms of fields? One idea I had was that from one of the links above : Let $A=\{a_1,...,a_n\}$ be the (distinct) roots of a polynomial $f$ with coefficients in a base field $k$ . Then a permutation $\pi$ of the set $A$ is in the Galois group of $f$ (over $k$ ) if (and only if):  for every polynomial $g$ in $R=k[x_1,...x_n]$ , $g(a_1,....,a_n)=0 \iff g(\pi(a_1),...,\pi(a_n))=0$ . and hence we are really looking for permutations ""that algebra can't see"", i.e. the heart of Galois theory is really the observation that there are some permutations of the symbols denoting roots of polynomials that algebra/polynomials are completely unable to distinguish (e.g. algebra can't distinguish between $i$ and $-i$ , only our geometric embedding into the plane can). Thus we are not able to permute all the roots willy-nilly (""they are not free"")... however, if we linearize and look at the vector space (over the base field $k$ ) containing all these roots (actually we should look at it as a $k$ -algebra because only then can we talk about multiplicative correlations between the roots), then we can try to find a basis for this vector space (""a free set"") which allows us to permute willy-nilly. In this perspective, the fact that this $k$ -algebra turns out to be a field is sort of irrelevant, I think?","['field-theory', 'galois-theory', 'abstract-algebra', 'big-picture']"
4455232,Why is $\int_{0}^{2\pi} \int_0^{2\pi} \frac{\ln(21-4(\cos x+\cos y+\cos(x+y)))}{2\ln(9/2)}\frac{dx}{2\pi} \frac{dy}{2\pi}$ almost $1$?,"Consider the function $$ f(x,y) = \frac{\ln(21-4(\cos(x)+\cos(y)+\cos(x+y)))}{2\ln(9/2)} $$ Its average value is awfully close to unity: $$ \int_{0}^{2\pi} \int_0^{2\pi} f(x,y) \frac{\mathrm dx}{2\pi} \frac{\mathrm d y}{2\pi} = 1.00095 $$ (This result is rounded .) Is this just a fluke, or is there some deeper reason to it?","['integration', 'logarithms', 'trigonometric-integrals', 'multiple-integral', 'trigonometry']"
4455238,Is there a function $f$ from reals to reals such that every non-vertical line intersects $f$ infinitely many times?,"Does there exist a function $f: \mathbb{R} \rightarrow \mathbb{R}$ such that for every non-vertical line $L$ in $\mathbb{R}^2$ , $L$ intersects the graph of $f$ infinitely many times?","['euclidean-geometry', 'functions']"
4455239,Help showing that two sets are equal.,"Let $A,B\in\mathbb{N}$ and define $$\tag{1}
U(A, B):=\{K \subseteq \mathbb{N} \mid A \subseteq K \subseteq \mathbb{N} \backslash B\} \subseteq 2^{\mathbb{N}}
$$ Define further $f: 2^{\mathbb{N}} \rightarrow \prod_{n=1}^{\infty}\{0,1\}$ by $$\tag{2}
f(A)_{n}:=\left\{\begin{array}{lll}
1 & \text { for } & n \in A \\
0 & \text { for } & n \notin A
\end{array}\right.
$$ I want to show that $$\tag{3}
\left(\mathrm{pr}_{m} \circ f\right)^{-1}(\{1\})=U(\{m\}, \emptyset)
$$ where $\text{pr}_m$ is the projection operator. It is pretty clear that $U(\{m\},\emptyset)$ consists of all subsets of $\mathbb{N}$ that contain $m$ . However $\left(p r_{m} \circ f\right)(X)=\{1\}$ is true whenever the sets in $X$ contain $m$ - this (to me) appears to be true regardless of how many sets $X$ contains, in which case $pr_m\circ f$ is not bijective. Where am I going wrong here? Edit : I am suppost to use eq. $(3)$ as well as $$\tag{4}
\left(\mathrm{pr}_{m} \circ f\right)^{-1}(\{0\})=U(\emptyset,\{m\})
$$ to show that the function $f$ is continuous.","['real-analysis', 'functions', 'elementary-set-theory', 'general-topology', 'set-theory']"
4455261,For what values of $p>0$ is $\lim_{n\rightarrow\infty}\int_0^n\frac{(1-\frac{x}{n})^ne^x}{n^p}dx=0$?,"For what values of $p>0$ is $\lim_{n\rightarrow\infty}\int_0^n\frac{(1-\frac{x}{n})^ne^x}{n^p}dx=0$ ? My thoughts: We know that $(1-\frac{x}{n})^n\leq e^x$ , so the numerator is $\leq e^{2x}$ .  So, we can play with $\frac{e^{2x}}{n^p}$ .  From here, I am not quite sure what to do.  I would really like to be able to find the supremum over $n$ , but I can't really minimize the denominator to be able to replace $n$ with something in terms of $x$ , because I only have $n^p$ down there.  On the other hand, I feel like I should be splitting up the integral from $0$ to $1$ and then from $1$ to $\infty$ based on the denominator.  Then, fix some $x$ , and just use $p$ integral properties to get that $p\in(0,1)$ , but I am not quite sure.  Any help is greatly appreciated! A quick edit: I realize that I made a big mistake above by overlooking the minus sign, so instead the integrand is bounded above by $\frac{1}{n^p}\leq 1$ as $n\rightarrow \infty$ , and so we can use DCT and then treat it like a $p$ integral. A second edit: For Sangchul Lee, I edited the integral to make the upper bound $n$ so he can expand on how he got his approximation.  Thank you!","['integration', 'approximation', 'analysis', 'real-analysis']"
4455265,The coupon collector's most collected coupon,"Suppose a coupon collector is collecting a set of $n$ coupons that he receives one-by-one uniformly at random. If the collector stops exactly when the collection is complete, we know the expected number of coupons in his collection is $n*H[n]$ . What is the expected number of copies, $M$ , of his most collected coupon? When $n = 2$ , then $M = 2$ because after the first coupon he will collect the other coupon with probability p ~ Geom(1/2). So he will collect the same coupon one additional time on average, and that coupon is certain to be his most collected coupon. I don't know the exact value for any $n$ larger than 2, but found some approximate values by simulation: n M 3 2.8415 4 3.4992 5 4.0259 6 4.4633 7 4.8377 8 5.1649 9 5.4560 EDIT 1: For small n enumerating the small possibilities converges faster than simulation, and from this approach I hypothesize that $M[3] = (15 + 6 \sqrt{5})/10$ but don't have any real argument to support that claim. EDIT 2: With some more thought, I find that M has an explicit sum formula. The probability the collection terminates with a given distribution of coupons $v = \{c_1, ..., c_{n-1}\}$ other than the final collected coupon is just the multinomial coefficient $(c_1; ...; c_{n-1})$ over $n^\text{total # of coupons in the collection}$ . This is an infinite sum over n-1 variables. Here is Mathematica code the computes the sum for terms where the collection has no more than k copies of any coupon: M[n_, k_] := Sum[Max[v]*(Multinomial @@ v)/n^Total[v], {v, Tuples[Range[1, k], n-1]}] Mathematica can't actually evaluate this though for $k \rightarrow \infty$ though.","['expected-value', 'coupon-collector', 'combinatorics', 'probability']"
4455276,Find the derivative of $\frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right)$ - no chain rule,"I'm trying to find the derivative of: $$
\frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right)
$$ As per the chain rule I have to find the derivative of $tan()$ and then $(\sqrt{x})$ which at the end is equal to $\frac{\sec ^2\left(\sqrt{x}\right)}{2\sqrt{x}}$ However, in my exercise, I don't have to use the chain rule nor L'Hopital rule. I'm looking to re-write $\tan(\sqrt{x})$ in a way to solve the exercise. I'm thinking of $$
\lim _{h\to 0}\left(\frac{\tan\left(\sqrt{x+h}\right)-\tan\left(\sqrt{x}\right)}{h}\right)
$$ but I'm kind of lost on how to solve it","['calculus', 'derivatives']"
4455359,What functions are continuous in the Wasserstein metric?,"Consider the space $L_p(\mathbb{R})$ of probability measures on $\mathbb{R}$ with $p$ th moments.  This set is a metric space under the $p$ -Wasserstein metric $$W_p(\mu,\nu)^p=\inf_{Z=(X,Y):X\sim\mu,Y\sim\nu}{\mathbb{E}[d(X,Y)^p]}$$ Now, pick your favorite bounded continuous function $f\in C_b(\mathbb{R})$ .  Then $f$ defines a function $f^*$ on $L_p$ via integration: $$f^*(\mu)=\mathbb{E}_{X\sim\mu}[f(X)]$$ (Fun exercise: prove that $f(X)$ has a first moment.)  In the course of solving this problem , I found myself asking: is $f^*$ always continuous? If $f$ is Lipschitz, then the answer is ""yes"", by the argument given at the end of my solution to that problem.  But is the claim true for non-Lipschitz functions? I am not sure. On the one hand, my proof uses the Lipschitz constant in an essential way. On the other, take $p=1$ for simplicity.  Choose $\mu_n(dx)=\frac{dx}{|nx|^3+1}$ , so that $\mu$ only has moments of order $<2$ and let $f:C(\mathbb{R})$ ; $f(x)=|x|^{\alpha}$ for $\alpha<1$ .  If there were a counterexample in the non-Lipschitz case, I would think that those choices of $f$ and $\mu_n$ should be it, for they are as badly-behaved as can be while still having enough moments.  Yet $\mu_n\overset{W_1}{\longrightarrow}\delta_0$ and $\mathbb{E}_{X\sim\mu_n}[|X|^{\alpha}]\to0$ . Perhaps the problem is that the above non-counterexample is Lipschitz away from $0$ , and I should look at functions that aren't Lipschitz on a more generic region — say, $f:C((-1,1))$ ; $f(x)=\sin{\frac{1}{1-x^2}}$ , with $\mu_n\overset{W_1}{\longrightarrow}\mathcal{U}(\{-1,1\})$ ?  But I am not sure how to impose cutoff to regularize $\mathbb{E}_{X\sim\mathcal{U}(\{-1,1\})}[f(X)]$ .","['measure-theory', 'optimal-transport', 'lipschitz-functions', 'functional-analysis', 'probability-theory']"
4455381,"Exercise related to vector fields, map degrees and Poincare-Hopf's Theorem","I got stuck with one exercise from Chapter 3.5 in Guillemin and Pollack's book, which I used to study differential topology by myself: Given a vector field $\overrightarrow{v}$ with isolated zeros in $\mathbb{R}^{k}$ and a compact $k$ -dimensional submanifold $W$ of $\mathbb{R}^k$ with boundary. If $\overrightarrow{v}$ is never zero on the boundary $\partial W$ , then we have that the sum of indices of $\overrightarrow{v}$ at its zeros inside $W$ equals the degree of the following map: $$\frac{\overrightarrow{v}}{|\overrightarrow{v}|}: \partial W\rightarrow S^{k}$$ I have tried following the hint to delete (sufficiently small) balls around the zeros and follow the standard argument used to prove Poincare-Hopf's Theorem. It turns out that the manifold $W$ becomes a manifold with ""two parts"" of boundaries after we delete the balls: the original boundary $\partial W$ and the boundaries of the omitted the balls. I guess we just need to show that the degrees of the map $\frac{\overrightarrow{v}}{|\overrightarrow{v}|}$ on these two parts of boundaries sum up to zero, right? However, is there a way to consider the manifold $W$ as a whole to prove the claim? Thanks in advance! Any hint/help would be greatly appreciated!","['vector-fields', 'geometry', 'manifolds', 'general-topology', 'differential-topology']"
4455401,$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2} \sqrt{3+x^2} }\text{d}x=\frac{\pi}{8}$ and generalizations,"Someone shared this to me: $$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{4-x^4}\sqrt{9-x^4}   }
\text{d} x
-\int_{0}^{1} \frac{1}{(3+x^2)\sqrt{1-x^4} \sqrt{2+x^2}\sqrt{4+x^2}\sqrt{5+x^2}   }\text{d}x
+ \int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2} \sqrt{2+x^2}
\sqrt{3+x^2}\sqrt{4+x^2}\sqrt{5+x^2}    }\text{d}x
=\frac{\pi\sqrt{3} }{24}$$ is correct. The integral has piqued my curiosity for a while but not worked out still. So I hope someone aids me and thank you for the great appreciation. The expression is in the form of $\pi\cdot\text{Algebraic}$ .
A relatively notable integral is $$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2} 
\sqrt{3+x^2}  }\text{d}x=\frac{\pi}{8}$$","['integration', 'contour-integration', 'definite-integrals', 'real-analysis']"
4455446,Forming a committee with three restrictions,"Suppose that in the US Supreme Court a committee of seven politicians is chosen from five republicans, ten democrats and eight independents. How many different ways can the committee be chosen if the committee must include exactly one republican, at least three democrats and at least one independent? I first considered the number of ways of forming a committee with at least three democrats and thought this was equal to $$
C(10,3)\times C(13,4)+C(10,4)\times C(13,3)+C(10,5)\times C(13,2)=165,516.
$$ However the answer in my book is $73,080$ . I am yet to consider the other restrictions and yet my answer is too large. Where am I double counting?",['combinatorics']
4455476,"“Every open ball is closed"" and ""every closed ball is open"", does one imply the other?","In a metric space $(X,d)$ , by a closed ball I mean a set of the form $\{y: d(y,x)\le r\}$ , where $r > 0$ . A common example where every open ball is a closed set and every closed ball is an open set is the ultrametric spaces: the metric spaces where $d(x,y)\le \max\{d(x,z),d(y,z)\}$ . I would like to have an example where every open ball is a closed set but not every closed ball is an open set, and an example vice versa. Definitions.  An open ball is a set of the form $B_r(a) = \{x \mid d(x,a) < r\}$ where $r > 0$ .  A closed ball is a set of the form $\overline{B}_r(a) = \{x \mid d(x,a) \le r\}$ where $r > 0$ .","['general-topology', 'metric-spaces']"
4455544,Preimage of measure 0 set under norm function has measure 0,"Let $A \subseteq [0,1]$ be a null set with respect to the Lebesgue measure. Is it true that $\{x\in \mathbb R ^k: |x| \in A\}$ is a null set in $\mathbb R^k$ , for all natural $k$ ( $|(x_1,\dots,x_k)| = \sqrt{x_1^2+\dots+x_k^2}$ )? If so, how can this be shown?","['measure-theory', 'real-analysis']"
4455557,How to find the average distance between two points in a square of length $1$ by using Average = sum of all distances / number of pair of points?,"Lets consider a line of length $l$ , let $X_1$ and $Y_1$ be two points and their distance from a common end point of the line be $x$ and $y$ . Then, the distance between $X_1$ and $Y_1$ is $$ \vert x-y \vert$$ Now I will consider a function $$ f(x,y)=\vert x-y\vert$$ If I find the volume under the graph by the integral $$ \int_0^l \int_0^l \vert x-y \vert  dxdxy = \frac {l^3}{3}$$ My interpretation of this $\frac {l^3}{3} $ is that it is the sum of all possible lengths that can be marked by the two points $X_1$ and $Y_1$ . Now, to find the number of points by which I mean the number of pairs of ( $X_1,Y_1$ ) I find the base area of the graph $$\int_0^l\int_0^l dxdy = l^2$$ I think $l^2$ is the infinite sum of the number of points. Dividing these two also does give the correct result of the average distance between two points on a line $\frac l3$ . $$ $$ My idea is to consider a line that is formed by two points on the perimeter the, the sum of all the distances that could be marked by two points will be $\frac {l^3}{3}$ so if I integrate this expression for the entire perimeter I would get all the possible distance that could be marked by two points on a square. For this I divide the perimeter into $6$ pairs, $2$ pairs of opposite sides and $4$ pairs of adjacent sides. $$ $$ Its fairly simple to arrive at the expression for their lengths. Now, I integrate $\frac {l^3}{3}$ for the perimeter $$\int_0^1\int_0^1 (x^2+y^2)^\frac 32dxdy= 0.20906$$ $$\int_0^1\int_0^1 ((x-y)^2+1)^\frac 32dxdy= 0.42438$$ Now, similarly I integrate $l^2$ over the perimter to get the number of points $$\int_0^1\int_0^1 (x^2+y^2)dxdy= \frac 23$$ $$\int_0^1\int_0^1 ((x-y)^2+1)dxdy= \frac 76$$ Now to calculate the average $$ \frac {0.20906*4+0.42438*2}{\frac 23 *4+\frac 76 *2} =0.337 $$ I have described my entire process and I do  not know where I am wrong. The correct answer is close to 0.52. I would appreciate any help on this.","['multivariable-calculus', 'calculus']"
4455567,"Can we find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{M}$ which doesn't satisfy the property $\mathcal{B}$?","$f:[0, 1]\to \Bbb{R}$ be a function. $f$ satisfy the property $\mathcal{M}$ of $f(A) $ is meagre for every $A\subset [0, 1]$ meagre. $f$ satisfy the property $\mathcal{B}$ if $f(A) $ is a set with the property of Baire for every $A\subset [0, 1]$ having the property of Baire. I don't know whether the property $\mathcal{M}, \mathcal{B}$ has any name or not. $f\in \Bbb{R}^{[0, 1]}$ with property $\mathcal{B}$ and  we know every meager set satisfy the property of Baire this implies image of every meager set is a set with the baire property and this means it is a symmetric difference of an open set and meager set. I believe we can find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{B}$ which doesn't satisfy the property $\mathcal{M}$ . Question  : Can you give me an explicit example of $f\in \Bbb{R}^{[0,
 1]}$ which map every set with Baire property to a set with Baire
property but doesn't map a meager set to a meager set? Question : Can we find $f\in \Bbb{R}^{[0, 1]}$ with the property $\mathcal{M}$ which doesn't satisfy the property $\mathcal{B}$ ? The sets with the Baire property forms a $\sigma$ -algebra generated by open sets and meagre sets. Suppose $F\in \Bbb{R}^{[0, 1]}$ satisfy the property $\mathcal{M}$ . $F(M) \subset \Bbb{R}$ is meagre for $\forall M\subset [0, 1]$ meagre. Hence $F$ maps the Cantor set $\mathcal{C}$ and all subsets of $\mathcal{C}$ to meagre set. So $F(\mathcal{C}) =\mathcal{C}$ . And suppose $F(U) $ is not open for some $U\subset [0, 1]\setminus\mathcal{C}$ open. Now again we have to map every meagre subset of $U$ to a meagre set. This is difficult and how to map rest of points. Does this type of function exists? If yes how to construct?","['measure-theory', 'real-analysis', 'functional-analysis', 'descriptive-set-theory', 'general-topology']"
4455623,Time series: ARMA characteristic polynomials have common roots,"I have a question regarding the idea that if the roots of the characteristic polynomials of a time series (say some ARMA process) lie outside the unit circle, then the series will be invertible/causal (depending on which characteristic polynomial we're talking about). In addition to such roots lying outside the unit circle, the two characteristic polynomials must share no common roots. On the problems such a violation could cause, I found the following in Brockwell and Davis (pg 86 (1991). Time series: theory and methods. SpringerVerlag
) so with respect to point (a), say I had some ARMA(2,2) process which could be factorised in terms of the backshift operator as $$(1-\alpha_1B)(1-\beta_1B)X_t=(1-\alpha_1B)(1-\alpha_2B)Z_t$$ and all the roots of the characteristic polynomials lay outside the unit circle but we had the obvious common root $(1-\alpha_1B)$ . What point (a) says, is that I can simply cancel such root to leave a causal and invertible ARMA(1,1) process? $$
(1-\beta_1B)X_t=(1-\alpha_2B)Z_t
$$","['statistics', 'estimation', 'stochastic-processes', 'causality', 'time-series']"
4455692,Is it possible that linear transform changes interval data to ratio?,"I'm going through a stats intro and got puzzled by the concept of interval and ratio data. Celsius temps is an example of interval data that can not be multiplied and divided, because there is no true 0. Yet, a simple linear transformation changes Celsius into Kelvin, where 0 is defined. Kelvin temps are divided and multiplied all over physics, so clearly a ratio data. To me it seems counterintuitive that a simple linear transformation allows for definition of a whole new mathematical operation. Is it just me and there is actually nothing strange about it, or am I missing something?","['statistics', 'linear-algebra', 'linear-transformations']"
4455693,Find all $4$ digit numbers such that sum of digits is $11$.,Find all $4$ digit numbers such that sum of digits is $11$ . \begin{cases} x_1+x_2+x_3+x_4=11\\ 1\leq x_1{\leq 9}\\ 0\leq x_2{\leq 9}\\0\leq x_3{\leq 9}\\ 0\leq x_4{\leq 9}\end{cases} . Using  stars and bars there is 13 choose 3 ways to do. So we get $286$ but answer is $279$ . I saw this same question in the forum but didn't fully understand solution. Where we are overcounting when solving in this way? And mainly what we are overcounting?,"['elementary-number-theory', 'combinatorics']"
4455694,Prove that $\sum_{n=0}^\infty \frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$,"I am asked to prove that $$\sum_{n=0}^\infty \frac{1}{(2n+1)^2}=\frac{\pi^2}{8}.$$ However, I am asked to prove it using the fact that $$\frac{\pi}{2}\tan\left(\frac{\pi}{2}z\right)=\sum_{m \text{ odd}}\left(\frac{1}{m-z}-\frac{1}{m+z}\right),$$ where $z\in \mathbb{C}$ , which is something I proved in a previous exercise. My first thought was using the fact that $$\frac{1}{m-z}-\frac{1}{m-z}=\frac{2z}{m^2-z^2}$$ and therefore $$\sum_{m \text{ odd}}\left(\frac{1}{m-z}-\frac{1}{m+z}\right)=\sum_{m \text{ odd}}\frac{2z}{m^2-z^2}=\sum_{n=0}^\infty \frac{2z}{(2n+1)^2-z^2}.$$ This last series is similar to the one I am aiming at, but I don't know how to transform it into the one that I want. Can someone help me?","['complex-analysis', 'sequences-and-series']"
4455721,Find $\sum_{k=1}^\infty\frac{1}{x_k^2-1}$ where $x_1=2$ and $x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2}$ for $n \ge 2$,"Given $x_1=2$ and $x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2}, n\geq 2$ Prove that $y_n=\sum_{k=1}^{n}\frac{1}{x_k^2-1}, n\geq 1$ converges and find its limit. To prove a convergence we can just estimate $x_n > n$ , therefore $y_n<z_n$ , where $z_n=\sum_{k=1}^{n}\frac{1}{k^2-1}$ and $z_n$ converges, then $y_n$ converges too. We can notice that $x_n^2+2x_n+5=(x_n+1)^2+4$ . So $x_{n+1}$ is one of the roots of the equation: $x_{n+1}^2-(x_n+1)x_{n+1}-1=0$ So $x_{n+1}^2-1=(x_n+1)x_{n+1}$ and therefore: $y_n=\sum_{k=1}^n \frac{1}{(x_{n-1}+1)x_{n}}$ I'm stuck here.","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4455745,Regarding a proof that $T(M\times N) \cong TM\times TN$,"I want to ask about the answer in this link: Tangent Bundle of Product Manifold How is the identification $T_{(x,y)}(M\times N)=T_xM\oplus T_yN$ used in (*) ? And in writing $T(M\times N)$ and $TM\oplus TN$ this way as sets (lines 4-6), are we also using that $T_xM=R^m$ and $T_yN=R^n$ ? Why is $\phi$ a diffeomorphism? And is the last conclusion that ${\phi}^{\sim}$ a diffeomorphis a sort of a theorem? Thanks a lot","['manifolds', 'tangent-bundle', 'differential-geometry']"
4455763,Help proving statement in Lee's Introduction to Riemannian Manifolds about smooth curves into manifolds with nonempty boundary.,"The statement appears on page 33 of the second edition of Professor Lee's Introduction to Riemannian Manifolds. It is in the section on Lengths and Distances in Riemannian manifolds, but I think the statement may be generally true even if there is no Riemannian metric given for the manifold. Here is the relevant paragraph: ""Without further qualification, a curve in $M$ always means a parameterized curve , that is, a continuous map $\gamma\colon I\to M$ , where $I\subseteq\mathbb{R}$ is some interval. ... To say that $\gamma$ is a smooth curve is to say that it is smooth as a map from the manifold (with boundary) $I$ to $M$ . If $I$ has one or two endpoints and $M$ has empty boundary, then $\gamma$ is smooth if and only if it extends to a smooth curve defined on some open interval containing $I$ . (If $\partial M\neq\varnothing$ , then smoothness of $\gamma$ has to be interpreted as meaning that each coordinate representation of $\gamma$ has a smooth extension to an open interval.)"" I believe I have a correct proof of the statement for the case when $M$ has empty boundary. It is the parenthetical statement that follows, about the case when the boundary of $M$ is nonempty, that has me stymied. I have sketched a proof of one direction of that statement, namely the ""if"" direction. It is the ""only if"" direction that I can't get a handle on. That is, I assume that $\gamma$ is smooth, $(U,\phi)$ is a smooth chart for $I$ , $(V,\psi)$ is a smooth chart for $M$ , and $\gamma(U)\subseteq V$ . I set $\hat{\gamma}=\psi\circ\gamma\circ\phi^{-1}\colon\phi(U)\to\psi(V)$ . By definition, $\hat{\gamma}$ (which is a coordinate representation of $\gamma$ ) is smooth, and I need to find an open interval $J$ which contains $\phi(U)$ and a smooth map $\Gamma\colon J\to\mathbf{R}^n$ such that $\Gamma|_{\phi(U)}=\hat{\gamma}$ . Here, I have assumed that $\dim M=n$ . I tried to work on a simpler version of this problem, so I assumed first that $\phi(U)$ was open in $\mathbb{R}$ (it could be open in the half-space instead). Then a plausible candidate for $J$ is $(\inf\phi(U),\sup\phi(U))$ . But how would I define $\Gamma$ so as to match $\hat{\gamma}$ ? I know $\phi(U)$ is a countable collection of disjoint open intervals, but I haven't been able to figure out how to use something like a partition of unity to glue the restrictions of $\hat{\gamma}$ to each interval in the collection together to make $\Gamma$ . That is mostly because I haven't come up with a reasonable  open cover for $J$ . Also complicating things is that an interval which appears in $\phi(U)$ might actually have been flipped by $\phi$ compared to the interval it came from in $I$ . For example, if $(-1,1)\subseteq I$ , $\phi$ might multiply it by $-1$ but not do that to other nearby subintervals. Can someone please offer some suggestions on how to go about proving the parenthetical statement.","['manifolds-with-boundary', 'smooth-manifolds', 'differential-geometry']"
4455813,Is there a term for abelian groups in which you can divide by natural numbers?,"Is there a specific term for an abelian group (or ring) $G$ that satisfies the following property? For every element $g \in G$ and natural number $n$ , there exists a $q \in G$ such that $\underbrace{q + q + \dots + q}_{n\text{ times}} = g$ . Loosely speaking, this property means that you can ""divide by natural numbers"", even if you can't divide arbitrary (nonzero) elements as with a field. Obviously any field extension of the rational numbers satisfies this property, but I suspect that there are probably non-field examples as well. The reason I ask is that it seems that me that the property above gives the minimum amount of algebraic structure necessary to define the arithmetic mean of elements of a set, which seems like kind of a natural thing to study.","['means', 'abstract-algebra', 'group-theory', 'abelian-groups', 'terminology']"
4455834,"How do we rigorously prove that for $n>1$, $(1+x)^{n-1}<1$ for $-1<x<0$?","Given $n>1$ and $$(1+x)^{n-1}<1$$ Intuitively I can see that for $x \in (-1,0)$ , we have $1+x<1$ , and if we raise that to any power then it will be smaller than 1. How do we prove this rigorously? For context on how I came upon this question, the following is a problem from Spivak's Calculus, Ch. 11 on ""Significance of Derivatives"". Use derivatives to prove that if $n \geq 1$ , then $$(1+x)^n > 1+nx, \text{ for } -1<x<0 \text{ and } x>0$$ (notice that equality holds for $x=0$ ) The solution in the solution manual is a bit terse Let $g(x)=(1+x)^n-(1+nx)$ . Then $g(0)=0$ , but $$g'(x)=n(1+x)^{n-1}-n\tag{1}$$ Since $n-1 \neq 0$ this means that $$\begin{align}g'(x) & < 0 \text{ for } -1<x<0, \\ & >0 \text{ for }
 x>0 \end{align}\tag{2}$$ Thus $g(x)>0$ for $-1<x<0$ and $x>0$ I've been reading this book as a self-contained exposition of mathematical concepts that build upon one another chapter by chapter. I can't recall among the theorems I've seen so far a justification for the step from $(1)$ to $(2)$ . What theorem(s) justifies making this step? In particular, we reach $$(1+x)^{n-1}<1$$","['calculus', 'derivatives', 'algebra-precalculus', 'inequality']"
4455837,Finding angle in circle to produce equal areas,"I have a circle that is divided into 4 quadrants with a vertical and a horizontal axis. The center of the circle (where the axes cross) is point b . The top of the vertical axis is point d . On the horizontal axis, to the left of b is point a . A line is drawn from a diagonally up and to the right, across the vertical axis (at point c ) and meets the top right of the circle at e . Line be and point f (the right end of the horizontal axis) have been added to help with the solution. The radius of the circle is 30 and the x position of point a is -.5 from b , taking b to be (0, 0) (that is, a is to the left of b by .5) How would we go about calculating $ \angle eab$ so that $\triangle cab$ = slanty sector dce ? With trial and error in a graphing program, I found the answer to be 88.09075925431... But how would one go about solving this in a general method?","['angle', 'circles', 'geometry', 'triangles', 'trigonometry']"
4455850,Is a planar square on the equator a locally energy minimizing configuration of electrons on $\mathbb{S}^2$?,"$\newcommand{\S}{\mathbb{S}^2}$ Let $$M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\} $$ Let $E:M \to \mathbb{R}$ be defined by $$E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|},$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . Question: Let $p:=(x_1,x_2,x_3,x_4)$ be the configuration of a planar square lying on the equator of $\S$ . Is $p$ a local minimum of $E$ ? (Of course, it is not a strict minima since one can rotate). Here is an attempt: Let $\beta_i(t)$ be a path in $\mathbb{S}^2$ , $\beta_i(0)=x_i, \dot \beta(0)=w_i \in T_{x_i}\S$ . Consider the path $$\alpha(t)=(\beta_1(t),\beta_2(t),\beta_3(t),\beta_4(t)).$$ Using $$
\begin{align}
&\frac{d}{dt}| \beta_i(t) - \beta_j(t) |^{-1}=\frac{d}{dt}(| \beta_i(t) - \beta_j(t) |^2)^{-\frac{1}{2}}\\&=| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big), \tag{1}
\end{align}
$$ we get $$
\frac{d}{dt}E(\alpha(t))=\sum_{i<j}| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big). \tag{2}
$$ In particular, denoting the length of the square's edge by $a$ , and assuming that $x_1,x_2,x_3,x_4$ are the square's vertices arranged in a cyclic order we have $$
dE_p(0,0,0,w)=\sum_{i=1}^3 | x_i - x_4 |^{-3}\langle x_i,w\rangle=a^{-3}  \langle x_1+x_3,w\rangle+(a\sqrt 2)^{-3}
\langle x_2,w \rangle=0,
$$ where we used the fact that $x_3=-x_1$ , and $x_2=-x_4$ , so $\langle x_2,w \rangle=-\langle x_4,w \rangle=0$ as $w \in T_{x_4}\S$ . Differentiating equation $(2)$ again, we get $$
\frac{d^2}{dt^2}| \beta_i - \beta_j |^{-1}=| \beta_i - \beta_j |^{-3}\bigg(3| \beta_i - \beta_j |^{-2}\big(\langle \dot \beta_i, \beta_j\rangle+\langle \beta_i, \dot \beta_j\rangle\big)^2+\langle \ddot \beta_i, \beta_j\rangle+\langle \beta_i, \ddot \beta_j\rangle+2 \langle \dot\beta_i, \dot \beta_j\rangle\bigg)\tag{2}.
$$ Now, consider first all the $i<j$ such that $j=i+1 \text{mod} 4$ , i.e. $i-j$ is an edge of the square, or equivalently $d_{ij}=|x_i-x_j|=a$ . If we choose e.g. $j=4$ , then the two neighbors are $i=1,3$ , and so combining terms $1-4,3-4$ we get $$
\langle x_1+x_3, \ddot \beta_4(0)\rangle=0,
$$ and similarly for the other two edges $1-2,2-3$ .
Thus $$
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=a^{-3}\bigg( 3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle\bigg)+A,
$$ where $A$ is the part correspondong to $i-j$ equal $1-3$ , $2-4$ (the diagonals). Consider the pair $2-4$ : $\langle x_2,\dot \beta_{4}\rangle=-\langle x_4,\dot \beta_{4}\rangle=0$ , so the first summand $3(a\sqrt 2)^{-2}...$ vanishes. Thus we are left with $$A=(a\sqrt 2)^{-3}\bigg( \langle \ddot \beta_2, \beta_4\rangle+\langle \beta_2, \ddot \beta_4\rangle+2 \langle \dot\beta_2, \dot \beta_4\rangle \bigg).
$$ Since $$
\langle \ddot \beta_2, \beta_4\rangle=-\langle \ddot \beta_2, \beta_2\rangle=|\dot \beta_2|^2,
$$ we get $$
A=(a\sqrt 2)^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
$$ Thus, up to a factor of $a^{-3}$ , we have $$
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+
$$ $$
\sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
$$ Since $a=\sqrt 2$ , we get $$
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3/2\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+
$$ $$
\sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
$$ Is the last quantity $\ge 0$ ? (If I am not mistaken in my computations so far...).","['maxima-minima', 'multivariable-calculus', 'optimization', 'symmetry', 'differential-geometry']"
4455880,fourth-order finite difference for $(a(x)u'(x))'$,"Previously I asked here about constructing a symmetric matrix for doing finite difference for $(a(x)u'(x))'$ where the (diffusion) coefficient $a(x)$ is spatially varying. The answer provided there works for getting a second order accurate method. What about getting a fourth-order accurate method? If I follow the same idea and apply the following fourth-order accurate formula for first derivative $ u'_i = \dfrac{u_{i-1} - 8u_{i-1/2} + 8u_{i+1/2} - u_{i+1}}{6\Delta x}$ (1) in succession, then I end up getting a formula for $(a u')_i'$ which involves $u_{i-2}, u_{i-3/2}, u_{i-1}, u_{i-1/2}, u_i, u_{i+1/2}, u_{i+1}, u_{i+3/2}, u_2$ . It involves the mid point values $u_{i+n/2}$ and it depends on nine neighbouring values of $u_i$ , which doesn't sound right for fourth order accurate scheme. For the special case of $a(x)=1$ it also doesn't reduce to the formula $ u'' = \dfrac{-u_{i-2} + 16u_{i-1} - 30u_i + 16u_{i+1} - u_{i+2}}{12\Delta x^2}$ .   (2) So what's wrong with applying (1) in succession? What's the correct approach to get a finite difference formula for $(au')'$ with fourth order accuracy? Edit 1: After reading this post I managed to derive a symmetric four-order centered difference scheme for $[a(x)u'(x)]'$ by applying $ u'(x) = \dfrac{u_{i-3/2} - 27u_{i-1/2} + 27u_{i+1/2} - u_{i+3/2}}{24\Delta x} + \mathcal{O}(\Delta x^4)$ twice in succession. The resulting formula for $[a(x)u'(x)]'$ is $ (au')'_i = \dfrac{1}{576\Delta x^2} \left( a_{i-\frac{3}{2}}u_{i-3}
- 27(a_{i-\frac{3}{2}} + a_{i-\frac{1}{2}})u_{i-2}
+ (27a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}}+27a_{i+\frac{1}{2}})u_{i-1}
- (a_{i-\frac{3}{2}} + 729a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_i
+ (27a_{i-\frac{1}{2}} + 729a_{i+\frac{1}{2}}+27a_{i+\frac{3}{2}})u_{i+1}
- 27(a_{i+\frac{1}{2}} + a_{i+\frac{3}{2}})u_{i+2}
+ a_{i+\frac{3}{2}}u_{i+3} \right)$ which involves the mid point values of $a(x)$ and has seven stencils. Is there a formula that involves even less computations / stencils(e.g. five stencils)?","['ordinary-differential-equations', 'finite-difference-methods', 'numerical-calculus', 'numerical-methods', 'finite-differences']"
4455895,Do continuous conservative vector fields on $\mathbb{R}^3$ form a semigroup under composition?,"Let $\mathcal{F}$ denote the set of conservative vector fields on $\mathbb{R}^3$ that are continuous.  That is $$ \mathcal{F}=\{F:\mathbb{R}^3 \rightarrow \mathbb{R}^3: F \text{ is continuous and } F=\nabla \phi \text{ for some } \phi \in C^1(\mathbb{R}^3) \}.$$ Is $\mathcal{F} $ closed under composition? I suspect it is not since no property like this is mentioned when one studies conservative vector fields (and we do love algebraic structures which appear naturally in analysis, so certainly if this were a semigroup someone would have made mention of it). Does anyone have a nice example of $F\in\mathcal{F}$ and $G\in\mathcal{F}$ , such that $F\circ G \notin\mathcal{F}$ ?","['vector-fields', 'multivariable-calculus', 'abstract-algebra', 'semigroups']"
4455905,Expectation of the largest order statistic from uniform random variables [duplicate],"This question already has answers here : Expected value of $\max\{X_1,\ldots,X_n\}$ where $X_i$ are iid uniform. [duplicate] (2 answers) Closed 2 years ago . If $X_1, ..., X_n$ are iid random variables from the Uniform[ $0,\theta$ ] distribution, where $\theta >0$ , compute the expectation of the largest order statistic denoted $X_{(n)}$ . I am looking to test whether or not this statistic is an biased or unbiased estimator for $\theta$ , however I am struggling to test the bias as I am unable to compute its expectation. My initial thoughts with this question were that $E_{\theta}(X_{(n)})=\frac{\theta}2$ since this should be the expectation of any random variable from the uniform distribution on this interval. However, I can see that clearly this won't be the case in this situation since the expectation must (intuitively) depend upon $n$ in some way. I am wondering what the problem is with my initial thoughts. Edit: I have been informed in the comments of what the correct approach is, however, I am still unclear of the problem with my reasoning that $E_{\theta}(X_{(n)})=\frac{\theta}2$ since $E_{\theta}(X_i)=\frac{\theta}2$ for all possible values of $i$ . By definition, there exists some natural number $j$ such that $X_j=X_{(n)}$ so why is it that $X_j$ doesn't follow the uniform distribution when every $X_i$ does.","['statistical-inference', 'statistics', 'uniform-distribution', 'order-statistics', 'probability']"
4455914,Permutation Groups: Find $x$ such that $x^5 = (12345)$,"I am wondering about how to solve question 35 from chapter 5 (Permutation Groups) from the 10th edition of Gallian’s Abstract Algebra. The full question is as follows: What is the smallest $n$ for which there is a solution in $S_n$ to the equation $x^5 = (12345)?$ Give an example of a solution. How many solutions are there for your $n$ ? Okay, so we know that since $|x^5| = 5$ , that $|x|=25$ . From this we can hypothesize at the disjoint representation of $x$ . Notice that in order for the least common multiple of the lengths to be 25, our only choice is to have a single 25-cycle in form $(a_1, a_2, … a_{24}, a_{25})$ . Okay, so $x^5$ is going to be the product in the form $(a_1, a_2, … a_{24}, a_{25})^5$ , so notice that this will essentially have the effect of the below $(a_1, a_2, … a_{24}, a_{25})^5$ = $(a_1, a_6, a_{11}, a_{16}, a_{21})(a_2, a_7, a_{12}, a_{17}, a_{22})(a_3, a_8, a_{13}, a_{18}, a_{23})(a_4, a_9, a_{14}, a_{19}, a_{24})(a_5, a_{10}, a_{15}, a_{20}, a_{25})$ I realized that you can pretty easily get an example $x$ such that one of these products are (1,2,3,4,5), for example (1, 6, 7, 8, 9, 2, 10, 11, 12, 13, 3, 14, 15, 16, 17, 4, 18, 19, 20, 21, 5, 22, 23, 24, 25), which turns out to be the answer given in the textbook. Furthermore, they say there are $20!$ possible answers, which probably follows pretty simply using some combinatorial logic and isn’t my big concern here. But what I don’t get is why is this the answer? It seems wrong, because, this answer for $x$ doesn’t satisfy $x^5 = (12345)$ , it just satisfies $x^5 = (12345)(a_{i_1}, …)(a_{i_6}, …)(a_{i_{11}}, …) (a_{i_{16}}, …)(a_{i_{21}}, …)$ Why is this the answer? Thank you!","['symmetric-groups', 'group-theory', 'abstract-algebra', 'permutation-cycles']"
4455915,Can mathematics distinguish left and right?,"Imagine, a mathematician from another galaxy lands on the earth. Is there a way we can explain to him what is ""counterclockwise"" without showing him a picture? Things like Green's formula, Stokes formula etc. do not work - if in the beginning we choose to use ""left handed"" $x, y, z$ -coordinate axes and for any vector field ${\bf F}$ define $\text{curl}{\bf F}$ as usual, the forms of these formulas remain the same. So is there a math theory that can make an absolute distinction between left handed and right handed coordinate systems?","['differential-topology', 'abstract-algebra', 'geometry', 'differential-geometry']"
4455916,"Why aren't $\int_0^\pi\int_{-1}^1e^rdr\,d\theta$ and $\int_0^{2\pi}\int_0^1e^rdr\,d\theta$ equal? Doesn't this violate the Change of Variables thm?","Why aren't these two integrals equal? $$\int_0^\pi \int_{-1}^{1} e^r \,dr\,d\theta \qquad\neq\qquad\int_0^{2\pi} \int_{0}^{1} e^r \,dr\,d\theta$$ Let me explain why I'm asking. This is the change of variables theorem for double integrals: Now, suppose that we have the unit disc $D \subset R^2$ and the transformation $T$ given by $x=r\cos\theta$ and $y=r\sin\theta$ . Then the rectangle in the $r\theta$ -plane $-1 \leq r \leq 1, 0 \leq \theta < \pi$ maps injectively to the unit disc under $T.$ So in theory, it seems like we should be able to integrate in polar coordinates using this region $-1 \leq r \leq 1, 0 \leq \theta < \pi$ , in addition to the ""usual"" region $0 \leq r \leq 1, 0 \leq \theta < 2\pi$ . Then why aren't the above two integrals equal, and more importantly, why does this not violate the change of variables theorem?","['integration', 'multivariable-calculus', 'change-of-variable']"
4455924,"Why does it seem more natural to think of $r$ as a function of $\theta$, rather than the other way around?","When teaching functions in polar coordinates, the nearly universal practice is to consider functions of the form $r = f(\theta)$ .  I think I have never seen any examples in which $\theta$ is expressed as a function of $r$ .  My intuition tells me that the usual setup is more ``natural'' in some sense than the latter, but I am not sure whether that is just because it's what I'm used to, or if I am responding to some intrinsic difference between the two cases.  Is there any good reason why, for instance, it seems more natural to write $r = \sqrt{\theta}$ than to write $\theta = r^2$ , despite the fact that they are (obviously) equivalent? Edited to add: Here is an example of what I mean.  Consider the function $\theta = \cos (r)$ .  It has a very cool graph: Intuitively, if we think of $r$ as the independent variable, and ask how $\theta$ depends on $r$ , the description is quite simple: as we move outwards from the origin, the angle oscillates back and forth between a minimum angle of $-1$ radian and a maximum angle of $1$ radian.  This is a perfectly reasonable function to think about, graph, and study.  And yet functions like this are (as far as I can tell) completely absent both from textbooks and from usual instruction.  I find this strange, and am wondering if anybody has any thoughts on why.","['algebra-precalculus', 'functions', 'polar-coordinates']"
4455929,"Prove that a ""set of all sets"" does not exist.","Axiom I used for the proof: The Axiom Schema of Comprehension : Let P $(x)$ be a property of $x$ . For any set $A$ , there is a set $B$ such that $x\in B$ if and only if $x\in A$ and P $(x)$ . Here is my attempt: Suppose for the sake of contradiction that the set of all sets indeed exist and we call it $V$ . Now consider the property P $(x)$ : $x\notin x$ . Then by the Comprehension Schema, there exists a set $X$ in which $x\in X$ if and only if $x\in V$ and P $(x)$ ; i.e., \begin{align*}
                x\in X\iff x\in V\text{ and }x\notin x.
            \end{align*} Since $V$ is the set of all sets and $X$ is a set, then we must have $X\in V$ . If $X\in V$ then either $X\in X$ or $X\notin X$ . If $X\in X$ then we have \begin{align*}
                X\in X\iff X\in V\text{ and }X\notin X,
            \end{align*} a contradiction. Now if $X\notin X$ then \begin{align*}
                X\notin X\iff X\notin V\text{ or }X\in X,
            \end{align*} but if $X\notin V$ , then we are done. Now if $X\in X$ , this again yield a contradiction. In either case, a contradiction. Therefore $X\notin V$ , and thus the set of all sets does not exist. is this proof correct?","['elementary-set-theory', 'solution-verification']"
4456042,"The miraculous nature of the ""matrix coefficients"" $\langle Tv,v \rangle$ (especially in the context of positive type functions)","$\newcommand{\ak}[1]{\langle #1 \rangle}$ I've noticed that for linear operators $T$ and an inner product $ \ak{\bullet, \bullet }$ , the expression $\ak{ Tv,v}$ tends to show up a lot. For instance, it shows up in the min-max theorem to tell us about all the eigenvalues of a sufficiently nice operator. Similarly the expression $\ak{Tx,y}$ (and its dual) appears in the very definition of self-adjointness (which appears by mixing together the Banach space definition of a dual operator + the Riesz representation theorem for Hilbert spaces). I know in the finite dimensional case they correspond to quadratic forms $x^\top A x$ , which appear in an absurd number of scenarios (see Why Study Quadratic Forms? ), in particular optimization (2nd order coefficient of Taylor expansion), all the nice properties of definite matrices , and in number theory (starting from the very beginning of number theory , essentially motivating the entire field of algebraic number theory, and continuing to be very well regarded in the 21st century and beyond ). Even in ""abstract harmonic analysis"",  we have such expressions popping up: for instance the fact that the "" matrix coefficients "" $\phi(x):= \ak{\pi(x)u,u}$ for some unitary representation of a locally compact Hausdorff group $G$ are EXACTLY the functions of positive type on $G$ (Prop. 3.15 in Folland's Abstract Harmonic Analysis ) plays a crucial role in (Folland's proof of) the Gelfand-Raikov theorem. In the abstract harmonic analysis case, one can (slightly) motivate this by saying that $\pi(x)$ is a complicated object, namely a unitary transformation of a Hilbert space; evaluating at some $u\in \cal H$ produces a vector $\pi(x)u$ which is less complicated, but still not as easy a complex number $\ak{\pi(x)u,v}$ . Moreover since ""comparing"" $\pi(x)u$ against all vectors doesn't lose any information, so if one can understand $\ak{\pi(x)u,v}$ for all $u,v\in \cal H$ , one can understand $\pi(x)$ (this is essentially the philosophy of the weak integral, which Folland uses to define $\pi(f) : L^1(G) \to \cal B(H)$ in $\S3.2$ ). Finally, by some sort of polarization identity, one can recover $\ak{\pi(x)u,v}$ from $\ak{\pi(x)u,u}$ , so it suffices to understand those. I can accept this motivation, but I can't accept the miraculous fact that such inner products $\ak{Tv,v}$ behave so nicely. E.g. in the above paragraph they produce positive type functions, which are very related (Prop. 3.35 in Folland) to positive definite matrices (which themselves are very related to such inner products); and as mentioned above they lead to amazing formulas for eigenvalues, nice theorems for optimization, and deep connections to algebraic number theory. Why should this vague notion of ""comparing a vector to its transformed self"" result in such a profound variety of interesting mathematics? A ""perfect answer"" could lie along the lines of someone telling a somewhat cohesive and comprehensive general story about why and when these inner products $\ak{Tv,v}$ appear, to the extent that the special case of the consideration functions $\phi_u(x):=\ak{\pi(x)u,u}$ and their (1-1) connection to positive type functions/""positive definite functions"" is no longer a miraculous leap of thought, but is instead met with ""of course that's what one'd do!"". EDIT 4/30/23: they also have a physical interpretation in quantum mechanics https://physics.stackexchange.com/questions/146005/why-is-the-measured-value-of-some-observable-a-always-an-eigenvalue-of-the-co as the ""expectation of an observable $A$ under the state $\psi$ "".","['inner-products', 'harmonic-analysis', 'big-picture', 'functional-analysis', 'positive-definite']"
4456055,Is a continuous function on compact convex set where the boundary is mapped to the set a self mapping?,"Let $K ⊂ R^n$ be convex and compact with $0$ in the interior of $K$ . Let $f ∈ C(K, R^n)$ with $f(∂K) ⊂ K$ . If this is the case, do we in fact have $f(K) \subset K$ . It is probably not the case that the image of a convex set is convex as those are hard to prove, but we at least know it is compact and connected (also path-connected). But at the same time, just being continuous is not a strong enough property to make further conclusions. The image of boundary is not necessarily a boundary unless $f$ is a diffeomorphism, but here we don't even have a homeomorphism, but just continuity.","['nonlinear-analysis', 'functional-analysis', 'analysis', 'real-analysis']"
4456063,How many points does a line intersect a sphere in an infinite-dimensional normed vector space?,"Let $(E, |\cdot|)$ be a n.v.s. We fix $r>0$ and $x,y \in B(0, r)$ such that $x\neq y$ . Here $B(0, r)$ is the open ball centered at the origin and having radius $r$ . The set of all points in the line though $x$ and $y$ is $$
\{tx+(1-t)y \mid t \in \mathbb R\}.
$$ Consider the map $f: \mathbb R \to E, t \mapsto tx+(1-t)y$ . Then $f$ and thus $|f|$ are continuous. $\lim_{t \to \infty} |f(t)| =+\infty$ . $|f(0)| = |y|<r$ . It follows that the set $S := \{t\in \mathbb R \mid |f(t)|=r\}$ is non-empty and bounded. If $(E, \langle \cdot , \cdot \rangle)$ is an inner product space, then $$
\begin{align}
|f(t)|=r &\iff |f(t)|^2=r^2 \\
&\iff |x|^2t^2 + 2\langle x,y \rangle t(1-t) + |y|^2(1-t)^2 = r^2 \\
&\iff |x-y|^2t^2+2(\langle x, y \rangle - |y|^2) t + |y|^2-r^2=0.
\end{align}
$$ We have $\Delta = (\langle x, y \rangle - |y|^2)^2- |x-y|^2(|y|^2-r^2)>0$ because $|y|<r$ and $|x-y|\neq 0$ . So $S$ has exactly $2$ elements in this case. Does $\operatorname{card} (S) =2$ if $E$ is reflexive or uniformly convex?","['convex-geometry', 'normed-spaces', 'functional-analysis', 'reflexive-space']"
4456112,"Solve: $\sec(2x) \ge\sec(x) , x\in [0,\pi]$\ {$\frac{\pi}{4}, \frac{\pi}{2}, \frac{3\pi}{4}$}","Here is the following question: Solve: $\sec(2x) \ge\sec(x) , x\in [0,\pi]$ \
{ $\frac{\pi}{4}, \frac{\pi}{2}, \frac{3\pi}{4}$ } Note: This is part b of a question where in part a , I was asked to solve: $\cos(2x)=\cos(x), x\in[0,2\pi]$ For part a , I got my answers as $x = 0,\frac{2\pi}{3},\frac{4\pi}{3},2\pi$ Part a aided me as I was able to get part b to the following form: $\cos(x)>=\cos(2x)$ However, here is where I am confused. Are the answers just $x>=0, x>=\frac{2\pi}{3}$ by copying the signs?","['algebra-precalculus', 'trigonometry', 'inequality']"
4456115,How to find a fractal with a predetermined Hausdorff dimension? [duplicate],"This question already has answers here : Given a real number $d , (1<d<2)$, is there a fractal with fractal dimension $d$? [duplicate] (1 answer) Is there a general metod to construct a fractal? (1 answer) Closed 2 years ago . For many patterns that display self-similarity, the Hausdorff dimension can be found . Sometimes the dimension is calculated and approximate - as is the case with the Feigenbaum attractor - but often its closed form in terms of known mathematical constants be obtained - for instance, the Hausdorff dimension of the Cantor set is $ \log_{3}(2)$ . I am interested in the inverse problem: suppose we consider a mathematical constant like $\pi^{-1}$ or $\gamma e$ . Can we always find and describe a fractal whose Hausdorff dimension is equal to this preset number?","['measure-theory', 'fractals']"
4456121,Are there tensor structures other than a metric which could be defined on a manifold which imply a connection through compatibility criterion?,"If we say our connection is torsion free, then the metric compatibility condition completely determines it. While this is geometrically intuitive way to do it, are there other interesting tensor fields which we can put on a manifold whom when we impose a compatibility condition with the connection that we uniquely get the connection?","['connections', 'differential-geometry']"
4456146,Line Integral. Vector field. Parametrization,"Let´s say we want to find the circumferences of the plane $C$ that make the line integral $\int_C y^2dx + x^2dy$ worth zero. My attempt : The vector field given is: $\mathbf{F}(x,y)=(y^2,x^2)$ . The first thing I try to do is find a parametrization for the path, but I am not sure whether I am looking for the parametrization of a general circumference centered at $(a,b)$ with radius $R$ : $\mathbf{r}(t)=(a+R \cdot \cos(t),b+R \cdot\sin(t))$ where $0 \leq t \le 2\pi$ $\mathbf{r'}(t)=(-R\sin(t),R\cos(t))$ Being that the case: $\mathbf{F}(\mathbf{r}(t))=\mathbf{F}(x(t),y(t))=((b+R \sin(t))^2,(a+R\cos(t))^2$ ) obtaining: $\mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t)= ((b+R \cdot \sin(t))^2,(a+R \cdot\cos(t))^2) \cdot (-R\sin(t),R\cos(t)) $ Finally, we must impose: $\int_{0}^{2\pi}\mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t) dt =0.$ However, we aim to find three parameters using only one condition. Perhaps, while calculating the integral we are left with fewer unknowns, but, despite that, the calculation part is still hard. Any suggestions are welcome.","['integration', 'multivariable-calculus']"
4456190,Durrett's Probability: Theorem 6.2.6,"I am having some difficulty understanding the concept of measure preserving , invariance , and ergodic . Here is a proof from Theorem 6.2.6 in Durrett's Probability: Theory and Examples, 5e (p.338) (available at https://services.math.duke.edu/~rtd/PTE/pte.html ), which states that If A = [a, b), then the exceptional set is $\emptyset$ . Here the $\varphi$ is a measure-preserving transformation, and $A_k = [a + 1/k, b-1/k)$ . I am trying to understand two points: Why do we need to show that $G$ is dense in $[0, 1)$ ? Why does $\varphi^m \omega_k \in A_k$ imply $\varphi^m x \in A$ if $x \in [0, 1)$ , $\omega_k \in G$ , with $|\omega_k - x| < 1/k$ ? My guess is that this has something to do with the measure-preserving property of $\varphi$ or the invariance of $A$ or $A_k$ (if they are), but I am not sure where and how exactly these concepts were used in this context. Thank you very much.","['stationary-processes', 'ergodic-theory', 'probability-theory', 'equidistribution']"
4456202,"Let $2, 1+\frac{1}{2}, 3, 1+\frac{1}{3}, 4, 1+\frac {1}{4},\dots$ be a sequence. Does $a_n$ converge/diverge? Is there a $\sup$ or $\inf$?","Let $2,1+\frac{1}{2},3,1+\frac{1}{3},4,1+\frac {1}{4},...$ be a sequence then which of the statements is true? $a_n$ coverges to a finitie limit or diverges to infinity. $\limsup \limits_{n \to \infty} (a_n) = \sup \{a_n| n \in \Bbb N\}$ $\liminf \limits_{n \to \infty} (a_n) = \inf\{a_n| n \in \Bbb N\}$ The sequence $a_n$ has at least $3$ subsequential limits. None of the above. For the first statement I could not actually find a formula, but it is obvious that if we look at the subsequence in the even indexes we get $\lim \limits_{n \to \infty}a_{2n} =1 $ and in odd $\lim \limits_{n \to \infty}a_{2n-1} = \infty $ so the limit does not exist therefore the statement is not correct. For the second statement I assumed it was bounded from above, therefore there exists an $M>1$ such that for all $n$ we get $a_n \leq M$ , let $L$ be a sub sequential limit of $a_n$ so there exists a subsequence $a_{n_k}$ of $a_n$ such that $\lim \limits_{n \to \infty}a_{{n_k}} =L$ for all $n$ we have $a_n \leq M$ $\implies$ for all $k$ we have $a_{n_k} \leq M$ so we get $L = \lim \limits_{n \to \infty}a_{n_k} \leq M$ meaning all of the sub sequential limits of $a_n$ are less or equal to $M$ because one subsequential limit is $1$ and the other is infinity so it is not bounded from above and there is not $\sup$ . I thought the third statement is correct because the sequence is bounded from below by $1$ so it is the minimum, but according to the answers in the book it is not the correct answer and I could not figure out why. For the fourth statement, as we found in the first part there are two different limits that one is even and the other is odd indexes that cover the sequence so there are only two limits so it is not true. The right answer according to the book is the fifth statement but why isn't the third statement correct? Thank you for the amazing help and tips!","['supremum-and-infimum', 'sequences-and-series', 'real-analysis']"
4456280,Fibers of the Abel Jacobi map over curves,"I am studying the Abel Jacobi map $$\mathrm{Div}_{X/k} \to \mathrm{Pic}_{X/k}$$ for projective, smooth, irreducible curve $X/k$ where $k$ is algebraically closed. Let $S = \operatorname{Spec}(k)$ , $T$ a scheme over $k$ and let $\mathcal{L}$ be a line bundle on the base change $X_T = X \times_S T$ . As the Picard functor is representable for projective, smooth, irreducible curves, this gives rise to a morphism $T \to \mathrm{Pic}_{X/k}$ .
I want to understand the scheme theoretic fiber $$\mathrm{Div}_{X/k} \times_{\mathrm{Pic}_{X/k}} T.$$ I am in particular interested in the case that $T=S$ .
I believe that in this case, it is given by the projective space $\mathbb{P}(H^0(X,\mathcal{L})^*)$ where $H^0(X,\mathcal{L})^*$ is the dual of $H^0(X,\mathcal{L})$ . I wanted to apply Proposition 8.2.7 of Bosch, Néron Models. It says that if $\mathcal{L}$ is cohomologically flat in dimension 0, then the fiber is represented by the projective $k$ -scheme $\mathbb{P}((f_*\mathcal{L})^*)$ where again, $(f_*\mathcal{L})^*$ is the dual of $f_*\mathcal{L}$ . My questions are: Is the line bundle indeed cohomologically flat in dimension 0 in our setting? If the first question is true, why does $f_*\mathcal{L}$ correspond to $H^0(X, \mathcal{L})$ ? Why isn't it sufficient to understand the fibers of the $k$ -points?","['projective-space', 'fibre-product', 'divisors-algebraic-geometry', 'algebraic-geometry', 'picard-scheme']"
4456286,Prove that $\frac{a}{\sqrt{a^2+b^2}}+\frac{b}{\sqrt{b^2+c^2}}+\frac{c}{\sqrt{c^2+d^2}}+\frac{d}{\sqrt{d^2+a^2}}\leq3$,"Prove that if $a,b,c,d$ are positive reals we have: $$\frac{a}{\sqrt{a^2+b^2}}+\frac{b}{\sqrt{b^2+c^2}}+\frac{c}{\sqrt{c^2+d^2}}+\frac{d}{\sqrt{d^2+a^2}}\leq3.$$ I think that I have found a equality case, for example, when $a=x^4,b=x^3,c=x^2,d=x$ , and as $x$ tends to $\infty$ , the LHS tends to $3$ , but this means that the inequality is very unlikely to be solved with traditional methods, such as Cauchy-Schwartz (my starting idea), so I got stuck.","['contest-math', 'algebra-precalculus', 'radicals', 'inequality']"
4456289,Area of a special triangle in a quadrilateral,"As is the case with triangles, I am sure that there is an entire ocean of not-so-well-known theorems about Euclidean quadrilaterals. In particular, I am interested in the following problem and suspect the existence of a theorem that elegantly trivializes it (the black points are the mid-points of the respective sides). What would that theorem be?","['euclidean-geometry', 'quadrilateral', 'geometry', 'plane-geometry', 'recreational-mathematics']"
4456315,Exercise 4.19 (1) of Brezis,"I am trying to solve the following exercise of Brezis' book on Functional Analysis. Let $(f_n)_{n \in \mathbb{N}}$ be a sequence in $L^p(\Omega)$ with $1 < p < \infty$ and let $f \in L^p(\Omega)$ . Assume that $f_n \rightharpoonup f$ weakly $\sigma(L^p, L^{p'})$ and $\lim_{n \to \infty} ||f_n||_p = ||f||_p$ , then $f_n \rightarrow f$ strongly in $L^p(\Omega)$ . I tried to show that $(f_n)_{n \in  \mathbb{N}}$ is a Cauchy sequence in $L^p(\Omega)$ using that $(||f_n||_p)_{n \in  \mathbb{N}}$ is Cauchy since, by hypothesis, it is convergent. However, I was not able to do it. Could anyone give me a hint to continue? Moreover, in Brezis' exercise 4.19 , it is given a counterexample for this claim when $p=1$ . Is there a counterexample for $p = \infty$ ?","['lp-spaces', 'functional-analysis', 'weak-convergence']"
4456325,Find the period of the trigonometrics functions $\sin[\cos(x)]$ and $\cos[\sin(x)]$,"So I need to find the period of these two trigonometric functions: $$\cos[\sin(x)]$$ and $$\sin[\cos(x)]$$ but algebraically and without using the graph of this functions. So to be more clear, my question is: Is there any algebraic method or any mathematical trick I can use it to find the periods of these two trigonometric functions?' even  without the proof. Like the period of $\cos(bx)$ is $2\pi/b$ directly without drawing the graph and without calculations.",['trigonometry']
4456336,Is this the correct understanding of how a geometric surface works?,"I'm reading ""Elementary Differential Geometry"" by Barrett O'Neill. Most of the book is spent looking at surfaces in $\mathbb R^3$ , but eventually he introduces the ""abstract surface"", which I understand to be a surface $M$ which doesn't (necessarily) ""live"" in $\mathbb R^3$ , but still has its points referred to by ""abstract patches"" $x(u, v)$ . I.e., a region of $M$ is covered by the image of some patch $x(u,v)$ , but we don't have an explicit form for $x$ like we did when surfaces were embedded in $\mathbb R^3$ . Later, he introduces the concept of a metric tensor $g$ , and says: surface + metric tensor = geometric surface My understanding is as follows: previously, to do calculations on a surface embedded in $\mathbb R^3$ , we might have an explicit equation for a patch $x(u,v)$ and calculate its derivatives $x_u, x_v$ , which would let us calculate things like a frame  field, unit normal field, curvature, etc. Now, because the geometric surface is an abstract surface that doesn't have some explicit form for $x(u,v)$ , we calculate those same things using the metric tensor $g$ , but we still don't explicitly have an expression for $x(u,v)$ . Now, $g$ is just the tool that tells us what the inner product of $x_u, x_v$ is as a function of $(u,v)$ . Is that correct?",['differential-geometry']
4456365,The theoretical importance of the half-angle formulas,"Unlike the laws of sines , cosines and tangents , which are very well known, the half-angle formulas seem (although they appear timidly in the mathematical literature) not to enjoy the same popularity. Thus, while there are entire chapters devoted to the law of sines, cosines, and tangents and their applications, there is not even a Wikipedia article on half-angle formulas. Right now you may be imagining this version of the half angle formulas $$\sin{\frac{\alpha}{2}}=\pm\sqrt{\frac{1-\cos{\alpha}}{2}}\qquad\qquad\cos{\frac{\alpha}{2}}=\pm\sqrt{\frac{1+\cos{\alpha}}{2}},$$ that do appear in the textbooks of the first trigonometry courses (at least with the one I studied). But actually I mean these $$\sin^2{\frac{\alpha}{2}} = \frac{(s-b)(s-c)}{bc}\qquad\qquad\cos^2{\frac{\alpha}{2}}= \frac{s(s-a)}{bc},\tag{1}\label{1}$$ where $a$ , $b$ , and $c$ are the sides of a triangle, $\alpha$ is the angle opposite side $a$ , and $s$ is the semiperimeter. I found the furthest reference to these formulas in a conversation posted online between Conway and Doyle , where Conway uses them to prove Heron's formula and later claims to have taken it from a sequel by Casey . I discovered \eqref{1} independently trying to prove the law of cosines by contradiction . When I realized that they were known, I tried to generalize them and I got this $$\sin^2{\frac{\alpha}{2}}=\frac{(s-a)(s-d)}{ad+bc}\qquad\qquad \cos^2{\frac{\alpha}{2}}=\frac{(s-b)(s-c)}{ad+bc},\tag{2}\label{2}$$ where $a$ , $b$ , $c$ and $d$ are the sides of a cyclic quadrilateral, $s$ is semiperimeter and $\angle{DAB}=\alpha$ . Before discovering the conversation between Conway and Doyle, I had been excited that I had found an original proof of Heron's formula using \eqref{1}. When I found \eqref{2}, I thought that by analogous reasoning I could prove Brahmagupta's formula . So it was. But in a geometry forum someone referred me to an ancient Greek book that contained \eqref{2}. I then sent my proof of Brahmagupta's formula to Martin Josefsson who referred me to Casey's book ""A Treatise On Plane Trigonometry"" where my proof already appeared. But I didn't give up and tried to generalize \eqref{2} getting this $$ad\sin^2{\frac{\alpha}{2}}+bc\cos^2{\frac{\gamma}{2}}=(s-a)(s-d)\qquad\qquad bc\sin^2{\frac{\gamma}{2}}+ad\cos^2{\frac{\alpha}{2}}=(s-b)(s-c),\tag{3}\label{3}$$ where $a$ , $b$ , $c$ and $d$ are the sides of a general quadrilateral, $s$ is semiperimeter, $\angle{DAB}=\alpha$ and $\angle{BCD}=\gamma$ . Surprisingly, $(3)$ also generalizes the Pythagorean identity . Take a look at GeoDom - A generalization of the Pythagorean trigonometric identity . Bretschneider's formula is known to be a generalization of Heron's and Brahmagupta's formulas. Naturally, I wondered if I could generalize Casey's proof of Brahmagupta's formula using \eqref{3} and thus derive Bretschneider's formula. And I did it. I sent my formulas in \eqref{3} and my proof of the Bretschneider's formula to Josefsson (among many other mathematicians) and he told me this: ""I like your paper, especially how you put these important formulas in a single framwork. I cannot say that I remember seeing the identities (4) and (5) anywhere else before."" Where identities (4) and (5) are the identities in \eqref{3} in this post. And then he said: ""Even though much has already been written about these formulas, the ideas for proving Bretschneider' formula and the area of a bicentric quadrilateral are novel as far as I know. I hope you get your paper published."" I decided to write an article about these formulas called ""Two Identities and their Consequences"" which was published in MATINF . In almost three years of exploring possible applications of \eqref{1}, \eqref{2}, \eqref{3}, this is what I have found: Using \eqref{1}, \eqref{2} we can also derive (you can see most of the proofs at GeoDom - Proofs and applications of two well-known formulae involving sine, cosine and the semiperimeter of a triangle ): The law of cosines The law of sines The law of tangents Stewart's theorem Compound angle formulas Mollweide's formula The product $AI\cdot{BI}\cdot{CI}$ The bisector length formula Mahavira's formulas Zelich's lemma Euler's triangle inequality Yun’s Inequality (Josefsson) Other unnamed identities and inequalities: $\tan{\frac{\alpha}{2}}\tan{\frac{\beta}{2}}+\tan{\frac{\alpha}{2}}\tan{\frac{\gamma}{2}}+\tan{\frac{\beta}{2}}\tan{\frac{\gamma}{2}}=1$ $r=4R\sin{\frac{\alpha}{2}}\sin{\frac{\beta}{2}}\sin{\frac{\gamma}{2}}$ $s=4R\cos{\frac{\alpha}{2}}\cos{\frac{\beta}{2}}\cos{\frac{\gamma}{2}}$ $\sin{\frac{\alpha}{2}}\sin{\frac{\beta}{2}}\sin{\frac{\gamma}{2}}\le\frac{1}{8}$ Triangle with $\tan{\frac{\alpha}{2}}=\frac{a}{b+c}$ (see Graubner's solution) I could go on and on and on… The formulas \eqref{1}, \eqref{2}, \eqref{3} explain the Heron–Brahmagupta–Bretschneider development better than I have seen anywhere else. This made me wonder what would happen if I analogously applied the half-angle formulas to formulas where half-angles explicitly appeared, such as Mollweide's (rather Newton's) formula or the law of tangents. This is how these two generalizations arose: A generalization of Mollweide's formula (rather Newton's) A generalization of the law of tangents When questioning Martin Josefsson about the originality of these generalizations, this is what he said: ""As far as I can recall, I have not seen any of them, at least not in modern books or papers, and even if some of them where to be found in an old text, they are at least not well known, and deserve to be wider known."" Apart from the proof of the Bretschneider's formula, I haven't found any other applications for \eqref{3}. Interestingly, half angles seem to be everywhere: from circle angle theorems to the Weierstrass Substitution technique in Integral Calculus. Even when Viète derived his formula for $\pi$ using an infinite product, he started by writing $\sin{x}=2\sin{\frac12x}\cos{\frac12x}$ . Edited question: Doesn't the fact that we can derive all the aforementioned classical theorems from \eqref{1} make \eqref{1} fundamental to Euclidean geometry? Are there other aspects to take into consideration? If so, what would these aspects be?","['euclidean-geometry', 'triangle-inequality', 'inequality', 'trigonometry', 'complex-numbers']"
4456366,"Existence Problem of pigeonhole, selecting among a set of 50 numbers.","I have, a Problem related to the box (pigeonhole) principle. Let me show you the statement: Prove that any given 50 positive integers, it is always possible to select out 4 numbers $a_{1}$ , $a_{2}$ , $a_{3}$ , $a_{4}$ from them, such that $(a_{2} - a_{1}) \cdot ( a_{4} - a_{3} )$ is divisible by 2009. To be honest, I'm kind of clueless in how to start to attack this problem, because, I'd usually start with ""getting my hands dirty"" and checking some cases to try to find a patter and the use box principle. But in this case the set of numbers is too big and to check even a single case would spend many many time, so it's obviously not the way to go. There is though, a pretty similar problem of selecting 51 arbitrarily chosen number of a set from 100 numbers but again, the solution it's based on looking the fact that any single number can be rewritten as $2^{q} \cdot m$ ... So, I believe the solution is just based on looking at a clever ""pigeonhole"" like in the above problem. Would please anyone of you give me a hint? I don't really want a complete solution, just, a hint. Thanks y'all for your time (:","['pigeonhole-principle', 'combinatorics', 'set-theory']"
4456422,Solving $ A\frac{\partial z}{\partial x} + B \frac{\partial^2 z}{ \partial x \partial y} + C \frac{\partial^3 z}{\partial x \partial^2 y} = 0 $? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Non-mathematician here trying to find a hopefully analytic solution or any constructive directions  for solving differential equations of this particular form: Take a function $z(x,y)$ , is there any solution structure for $ A\frac{\partial z}{\partial x} + B \frac{\partial^2 z}{ \partial x \partial y} + C \frac{\partial^3 z}{\partial x \partial^2 y} = 0 $ ? $A,B,C \in \Bbb{R} $ Thanks","['derivatives', 'partial-differential-equations']"
4456449,Formula for $k$th order derivative of the determinant of a matrix?,"Background Jacobi's formula tells us that $$\frac{d}{dt}\det A(t) = \operatorname{tr} \left( \operatorname{adj}(A(t)) \frac{dA(t)}{dt} \right) = (\det A(t)) \cdot \operatorname{tr} \left( A(t)^{-1} \cdot \frac{dA(t)}{dt} \right)$$ for the first derivative of a matrix $A(t)$ with respect to parameter $t$ . A previous question showed that there exists such a formula $$\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +
         \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right]$$ for the second partial derivatives. Question Is there such a formula for the $k$ th order derivative $$\frac{d^k}{dt^k}\det A(t) = ?$$ of the determinant of such a matrix? Note that the question nth derivative of determinant wrt matrix is only a special case since here I am assuming each entry is function of a parameter $t$ that I am taking the derivative with respect to.","['matrix-calculus', 'determinant', 'derivatives']"
4456466,Longest geometric progression of primes,"There are arbitrarily long arithmetic progressions of primes e.g. $5, 11, 17, 23, 29$ for a $5$ -length progression, but no (infinite) arithmetic sequence of primes with common difference $d\neq 0$ , as $d\in\mathbb{Z}$ is an obvious constraint and $(a+nd)_{n\in\mathbb{N}}$ contains $a+ad=a(1+d)$ . A natural question is then: what is the longest geometric progression of primes? If $r>1$ is an integer then you can't get a progression longer than $1$ , as $ar$ has at least three distinct factors: $1, ar, a, r$ (possibly $a=r$ ). But what about arbitrary $r\in\mathbb{R}$ ? You can get a sequence of $2$ e.g. $2, 3$ by taking first term $a=2$ and common ratio $r=1.5$ . But it doesn't seem to be possible to get more. So my question is: Prove that if $a,ar,ar^2,\dots,ar^n$ is a list of prime numbers then either $r=1$ or $n\le 1$ . (Self-answering because I'm surprised not to find this question asked before; it seems elementary but interesting.)","['elementary-number-theory', 'sequences-and-series', 'geometric-series', 'prime-numbers', 'integer-sequences']"
4456510,Elementary proof that $\text{Re}\big(\frac{z^{n+1} - n z - z + n}{(z-1)^2}\big) \ge \frac{n}2$?,"I would like an elementary proof that the real part of $$ f(z) = \frac{z^{n+1} - n z - z + n}{(z-1)^2} $$ is greater than or equal to $n/2$ for any $z \in \mathbb C$ , $|z| \le 1$ , where $n$ is a natural number.  I can prove this for $|z| = 1$ , and then deduce it is true for $|z| \le 1$ using the maximum principle.  But I am looking for a direct proof. Incidentally $$ \text{Re}(f(e^{i\theta})) = \frac n2 + \frac{\sin^2(n \theta/2)}{2\sin^2(\theta/2)} ,$$ which establishes the result if $|z| = 1$ .","['complex-analysis', 'inequality']"
4456522,How to show that $\forall x \in \mathbb{R}\ \exists! n \in \mathbb{Z}$ such that $x \leq n < x+1$?,"To show that: $$
\forall x \in \mathbb{R}\ \exists! n \in \mathbb{Z} \text{ s.t. }x\leq n < x+1
$$ I know that the said $n$ is the infimum of the set of integers greater than or equal to $x$ : $$
n = \inf{ \{m:m\geq x , m \in \mathbb{Z}\}}
$$ Now, how do I show that this $n$ is less than $x+1$ ?","['elementary-set-theory', 'real-analysis']"
4456530,Help finding mistake in proof involving the quotient map.,"Consider the plane $\mathbb{R}^2=\mathbb{R}\times\mathbb{R}$ with the product topology which has basis consisting of all open squares of the form $$\tag{1}
\left]a,b\right[ \times \left]c,d\right[ \subseteq\mathbb{R}\times\mathbb{R},\quad a<b,\ c<d.
$$ Define the quotient space (with the quotient topology) $$\tag{2}
\mathbb{R}^2/\mathbb{Z}:=\mathbb{R}^2/\mathord{\sim}
$$ where the equivalence relation is given by $$\tag{3}
((x_1,x_2)\sim(y_1,y_2))\iff(x_1-y_1\in\mathbb{Z}\text{ and } x_2-y_2=0).
$$ I want to show that $$\tag{4}
\pi^{-1}\bigg(\pi\Big(\left]a,b\right[ \times \left]c,d\right[\Big)\bigg)=\bigcup_{n\in\mathbb{Z}}\Big(\left]a+n,b+n\right[\times\left]c,d\right[\Big).
$$ where $\pi:\mathbb{R}^2\rightarrow \mathbb{R}^2/\mathbb{Z}$ is the quotient map. If we consider a element $(x_1,x_2)\in\mathbb{R}^2$ then we can see from eq. $(3)$ that the effect of the quotient map is $$\tag{5}
\pi\left(\left(x_{1}, x_{2}\right)\right)=\left\{\left(y_{1}, y_{2}\right) \mid x_{1}-y_{2} \in \mathbb{Z} \text { and } x_{2}-y_{2}=0\right\}=\left\{\left(x_{1}+n, x_{2}\right) \mid n \in \mathbb{Z}\right\}.
$$ Is the inverse of this $$
\tag{6}
\pi^{-1}(\left\{\left(x_{1}+n, x_{2}\right) \mid n \in \mathbb{Z}\right\})=\bigcup_{n\in\mathbb{Z}}\{\left(x_{1}+n, x_{2}\right)\}?
$$ So that the inverse of all elements in $\pi(\left]a,b\right[ \times \left]c,d\right[)$ becomes the right side of eq. $(4)$ . If this is correct (I assume it is not), then it appears that $\pi^{-1}$ has no effect, since $\left\{\left(x_{1}+n, x_{2}\right) \mid n \in \mathbb{Z}\right\}$ and $\bigcup_{n\in\mathbb{Z}}\{\left(x_{1}+n, x_{2}\right)\}$ both are sets with elements of the type $(x_1+n,x_2)$ with $n\in\mathbb{Z}$ . So it seems I am making a mistake, but I need help identifying and correcting it.","['equivalence-relations', 'real-analysis', 'discrete-mathematics', 'quotient-spaces', 'general-topology']"
4456531,Showing that $x^3 + y = y^3 + x$ is an equivalence relation,I am asked to prove that: $x^3 + y = y^3 + x$ is an equivalence relation. So far I have the following: Reflexive: $m^3 +m = m^3 +m$ Symmetric: $m^3 + n = n^3 + m \rightarrow n^3 + m = m^3 + n$ Then: $n^3 + m = m^3 +n$ From hypothesis Transitivity (here's where I got stuck): $m^3 + n = n^3 + m \wedge n^3 + o = o^3 + n \rightarrow m^3 + o = o^3 + m$ Then: $m^3 + o = n^3 + m - n = n^3 - n + m = o^3 + n - o - n + m = o^3 - o +m$ And I cant figure out a way to go from $o^3 - o + m$ to $o^3 + m$ ; what could I do? Am I missing something?,"['boolean-algebra', 'relations', 'discrete-mathematics', 'equivalence-relations']"
4456535,"Spivak's Calculus, Ch. 11, **68: $f(x)=\alpha x+x^2\sin{1/x}$ for $x \neq 0$, $f(0)=0$. Prove $f$ is not increasing in an interval around $0$.","Two asterisks on a problem in Spivak's Calculus signal a potentially very tricky problem. I solved the following two asterisk problem from chapter 11, ""Significance of the Derivative"". I am wondering about the correctness of the steps to solve item c and d. This will be a long question, hopefully someone has patience :). **68. Let $f(x)=\alpha x+x^2\sin{1/x}$ for $x \neq 0$ , and let $f(0)=0$ . In order to find the sign of $f'(x)$ when $\alpha \geq 1$ it is
necessary to decide if $2x\sin{1/x}-\cos{1/x}$ is $<-1$ for any
numbers $x$ close to $0$ . It is a little more convenient to consider
the function $g(y)=\frac{2\sin{y}}{y}-\cos{y}$ for $y \neq 0$ ; we want
to know if $g(y)<-1$ for large $y$ . This question is quite delicate;
the most significant part of $g(y)$ is $-\cos{y}$ , which does reach
the value $-1$ , but this happens only when $\sin{y}=0$ , and it is not
at all clear whether $g$ itself can have values $<-1$ . The obvious
approach to this problem is to find the local minimum values of $g$ .
Unfortunately, it is impossible to solve the equation $g'(y)=0$ explicitly, so more ingenuity is required. (a) Show that if $g'(y)=0$ , then $$\cos{(y)}=\sin{(y)}\frac{2-y^2}{2y}$$ and conclude that $$g(y)=\sin{(y)}\frac{2+y^2}{2y}$$ (b) Now show that if $g'(y)=0$ , then $$\sin^2{(y)}=\frac{4y^2}{4+y^4}$$ and conclude that $$|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$$ (c) Using the fact that $\frac{2+y^2}{\sqrt{4+y^4}}>1$ , show that if $\alpha=1$ , then $f$ is not increasing in an interval around $0$ . (d) Using the fact that $\lim\limits_{y \to \infty}
 \frac{2+y^2}{\sqrt{4+y^4}}=1$ , show that if $\alpha>1$ , then $f$ is
increasing in some interval around $0$ . Here is my solution a) We know that $$g(y)=\frac{2\sin{y}}{y}-\cos{(y)}, \text{ for } y \neq 0\tag{1}$$ $$g(y)=$$ $$g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y}=0$$ $$\implies \cos{(y)}=\sin{(y)}\frac{2-y^2}{2y}\tag{2}$$ Now we substitute this back into $(1)$ to obtain $$g(y)=\sin{(y)}\frac{2+y^2}{2y}$$ b) Starting from $(2)$ , we square both sides to obtain $$\sin^2{(y)}=\frac{4y^2}{4+y^4}\tag{3}$$ Then $$|g(y)|=\left | \sin{(y)}\frac{2+y^2}{2y} \right |=|\sin{(y)}| \left | \frac{2+y^2}{2y} \right |$$ And after we sub in $(3)$ we obtain $$\implies |g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$$ c) My question is about this part. The problem statement claims that $\frac{2+y^2}{\sqrt{4+y^4}}>1$ . Proof $$|2+y^2|=2+y^2= \sqrt{(2+y^2)^2}$$ $$=\sqrt{4+4y^2+y^4}>\sqrt{4+y^2}, \text{ for } y \neq 0$$ $$\implies \frac{2+y^2}{\sqrt{4+y^2}}>1$$ $$\blacksquare$$ Assume $\alpha=1$ . $$x \neq 0 \implies f(x)=x+x^2\sin{(1/x)}$$ $$f'(x)=1+2x\sin{(1/x)}-\cos{(1/x)}$$ $$=1+g(1/x)$$ Since we know that $f'>0$ for some point in any interval around $0$ (we can conclude this based on a previous problem), if we can show that $f'<0$ for some point in any interval around $0$ then we will have shown that $f$ is not increasing in any interval around $0$ . Let $y=-2\pi k, k=1,2,...$ . Then $g(-2\pi k)=-\cos{(2\pi k)}=-1$ . Recall that $g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y}$ . Therefore $$g'(-2\pi k)=-\frac{1}{\pi k}<0$$ Hence there is some $c \in (-2\pi k-\delta, -2\pi k) \land g(y)<g(-2\pi k)=-1$ Finally, this means that $$\exists c, c \in (-\frac{1}{2 \pi k}, -\frac{1}{2\pi k+\delta}) \land g(1/x)<-1$$ Hence $f'(c)<-1$ . d) $\lim\limits_{y \to \infty} \frac{2+y^2}{\sqrt{4+y^4}}=1$ means $$\forall \epsilon>0\ \exists N>0\ \forall y\ y>N\implies \left | \frac{2+y^2}{\sqrt{4+y^4}} -1\right | < \epsilon$$ $$\implies 1-\epsilon<|g(y)|<1+\epsilon$$ Assume $\alpha>1$ . $$x \neq 0 \implies f(x)=\alpha x+x^2\sin{(1/x)}$$ $$f'(x)=\alpha+2x\sin{(1/x)}-\cos{(1/x)}$$ $$=\alpha+g(1/x)>\alpha-|g(1/x)|$$ Let $\epsilon=\frac{\alpha-1}{2}$ . Then $$1-\frac{\alpha-1}{2}<|g(1/x)|<1+\frac{\alpha-1}{2}$$ $$\frac{3-\alpha}{2}<|g(1/x)|<\frac{1+\alpha}{2}$$ Therefore $$f'(x)>\alpha-\frac{1+\alpha}{2}=\frac{\alpha-1}{2}>0$$ That is $$\exists N>0\ \forall y\ y>N \implies f'(x)>0$$ $$\exists N>0\ \forall x\ x<\frac{1}{N} \implies f'(x)>0$$ Therefore, $f$ is increasing on $(0, \frac{1}{N})$ . Similarly, because $\lim\limits_{y \to -\infty} \frac{2+y^2}{\sqrt{4+y^4}}=1$ we can show with very similar steps that $$\exists N<0\ \forall x\ x>\frac{1}{N} \implies f'(x)>0$$ Also, $f'(0)=\alpha$ , so we can say that $$\exists N>0\ \forall x\ |x|<\frac{1}{N} \implies f'(x)>0$$ Therefore $f$ is increasing on $(-\frac{1}{N}, \frac{1}{N})$ .","['proof-explanation', 'calculus', 'derivatives']"
4456573,Are all entrywise nonnegative positive semidefinite matrices has a rank-1 decomposition with nonnegative vectors?,"I know that all positive semidefinite matrices has a rank-1 decomposition. (Equivalently, all quadratic nonnegative polynomial is sum of squares of linear function.) $$A = \sum_{i=1}^r x_i x_i^T = X X^T$$ . I am wondering if there is an entrywise nonnegative X satisfying $A=X X^T$ if $A$ is a entrywise nonnegative positive semidefinite matrix. The converse is obviously true.. I think an important tool is that solution set of $A=X X^T$ is closed with the operation $X \rightarrow XP$ where the $P$ is an orthonormal matrix. For example, $$a^T
\begin{pmatrix}
2 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 2 \\
\end{pmatrix}a$$ can be represented as $(a_1+a_2+a_3)^2 + (a_1 - a_3)^2$ , but with multiplying by $$\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1 \\
1 & -1 \\
\end{pmatrix},
$$ we get an 'positive representation', $\frac{1}{2}(2a_1+a_2)^2 + \frac{1}{2}(a_2 + 2a_3)^2$ .","['matrices', 'positive-matrices', 'linear-algebra', 'positive-semidefinite']"
4456575,Analytic definition implies geometric definition of trigonometric functions,"It is well known how we can arrive to the power series definition of trigonometric functions starting from their definition in terms of the unit circle. I'm trying to do the converse, i.e. start from the definition of trigonometric functions by their power series and prove that we can parametrize the unit circle with them and that an angle of $\theta$ radians subtends an arc of length $\theta$ (in the unit circle, of course). Here is what I have done so far. Define $\displaystyle \pi :=2\int \limits _{-1}^1\sqrt{1-x^2}\,dx$ the area of the unit circle, and define $$C(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n)!}x^{2n}$$ and $$S(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n+1)!}x^{2n+1}.$$ The ratio test shows that this functions have infinite convergence radius, so they are in $\mathcal{C}^\infty (\mathbb{R})$ , and it is clear that $C$ is an even function and $S$ is an odd function. Differentiating gives $\dfrac{d}{dx}C(x)=-S(x)$ and $\dfrac{d}{dx}S(x)=C(x)$ . Now we prove that they verify the addition formulae for $\sin$ and $\cos$ , i.e. that $S(x+y)=S(x)C(y)+C(x)S(y)$ and $C(x+y)=C(x)C(y)-S(x)S(y)$ for all $x,y\in \mathbb{R}$ . Fix $y\in \mathbb{R}$ and define $$F(x):=S(x+y)-S(x)C(y)-C(x)S(y).$$ Then we have that $$F'(x)=C(x+y)-C(x)C(y)+S(x)S(y).$$ Now observe that $F''(x)=-F(x)$ , hence $$2F'(x)[F(x)+F''(x)]=0$$ and therefore $$\frac{d}{dx}\left [(F(x))^2+(F'(x))^2\right ]=0$$ for all $x\in \mathbb{R}$ , hence $F(x)=0$ for all $x\in \mathbb{R}$ and $F'(x)=0$ for all $x\in \mathbb{R}$ , this proves that $S(x+y)=S(x)C(y)+C(x)S(y)$ and $C(x+y)=C(x)C(y)-S(x)S(y)$ for all $x,y\in \mathbb{R}$ . Using that $C$ is even and $S$ is odd we find that $S(x\pm y)=S(x)C(y)\pm C(x)S(y)$ and that $C(x\pm y)=C(x)C(y)\mp S(x)S(y)$ , this also gives the Pythagorean Identity $$(C(x))^2+(S(x))^2=C(x-x)=C(0)=1$$ which implies that $C(x),S(x)\in [-1,1]$ for all $x\in \mathbb{R}$ . Now I want to prove that we can parametrize the unit circle using $C$ and $S$ . The Pythagorean Identity implies that $(C(x),S(x))$ is in the unit circle for all $x\in \mathbb{R}$ , but I don't know how to prove that every point in the unit circle has the form $(C(x_0),S(x_0))$ for some $x_0\in \mathbb{R}$ . We now prove that the perimeter of the unit circle is $2\pi$ . By symmetry, it is enough to prove that the perimeter of the upper half of the unit circle is $\pi$ . This curve can be parametrized as $\alpha (t):=\left (t,\sqrt{1-t^2}\right )$ for $t\in [-1,1]$ , hence $\alpha '(t)=\left (1,-\dfrac{t}{\sqrt{1-t^2}}\right )$ , and therefore $\|\alpha '(t)\|=\dfrac{1}{\sqrt{1-t^2}}$ . To prove that the perimeter of the upper half of the unit circle is $\pi$ , we have to prove that $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}\,dt=2\int \limits _{-1}^1\sqrt{1-t^2}\,dt$$ i.e. we have to prove that $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=0.$$ But $$\frac{d}{dt}\left (-t\sqrt{1-t^2}\right )=\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}$$ and therefore $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=\left .-t\sqrt{1-t^2}\right |_{-1}^1=0$$ as wanted. Now assuming that we can parametrize the unit circle using $C$ and $S$ , then we can show with line integration that the length of the portion of the unit circle that goes from $(1,0)$ to $(C(x),S(x))$ in counterclockwise sense is exactly $x$ . In particular, this would prove that $C$ and $S$ are periodic with period $2\pi$ and that $C(x)=\cos x$ and $S(x)=\sin x$ for $x\in [0,2\pi ]$ , and therefore over all of $\mathbb{R}$ . So my question here is: How can we prove that the functions $C$ and $S$ defined as the power series of (what then are going to be) the cosine and the sine functions can be used to parametrize the unit circle?","['power-series', 'trigonometry', 'geometry', 'real-analysis']"
4456578,Solve: $\arg(z+5+i5\sqrt{3})=\frac{\pi}{6}$,"My attempt: $z=x+iy$ $\arg(x+iy+5+i5\sqrt{3})=\frac{\pi}{6}$ $\arg((x+5)+i(y+5\sqrt{3}))=\frac{\pi}{6}$ $(x+5)+i(y+5\sqrt{3}) = r\operatorname{cis}(\frac{\pi}{6})$ From here, where do I go? I need to remove the "" $r$ "" somehow.","['algebra-precalculus', 'complex-numbers']"
4456609,Showing $\int_{0}^{2022}x^{2}-\lfloor{x}\rfloor\lceil{x}\rceil dx = 674$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question It is from the 2022 MIT Integration Bee Question 3 states as follows: $$\int_{0}^{2022}x^{2}-\lfloor{x}\rfloor\lceil{x}\rceil dx$$ I know that the answer is $674$ , but I do not know the process and the steps to derive this solution. Can someone please help?",['integration']
4456633,"$A,B$ such that $A\cap B=\emptyset$ and $A\cup B=\mathbb{R}$ and $B=\{x+y : x,y\in A\}$?","If set $A,B$ satisfy $A\cap B=\emptyset,A\cup B=I$ , and $B=\{x+y : x,y\in A\}$ , can $I$ be real number set $\mathbb{R}$ ? I think the answer is yes, but I can't construct it. If $A$ is odd number set, $B$ is even number set, then $I$ is integer set $\mathbb{Z}$ . But how can the set of irrational numbers and the set of other rational numbers be put into the set $A$ ? I don't understand.","['elementary-set-theory', 'real-numbers', 'abstract-algebra']"
4456650,Extending a function that gives a value to convex functions to a measure,"I am wondering if such a result exists (or similar) and or if there is a ""simple"" proof. Let $\mathcal X$ be a bounded and closed subset of a topological vector space, let $\Sigma$ be the Borel $\sigma$ -algebra associated and let $\Gamma$ be the set of bounded convex continuous function on $\mathcal X$ , suppose that the topology on $\mathcal X$ is the weakest topology such that all functions in $\Gamma$ are continuous. Suppose we have $\mu:\Gamma\to\mathbb R$ that is such that \begin{align*}
\mu(1)&=1\\
\forall f,g\in\Gamma \text{ such that }f\leq g,~~~\mu(f)&\leq \mu(g)\\
\sum_{i\in\mathcal I}a_i\mu(f_i) &=\mu\left( f \right)
\end{align*} Whenever $\sum_{i\in\mathcal I} a_i f_i=f\in\Gamma$ for $f_i$ in $\Gamma$ , $a_i \geq 0$ and $\mathcal I$ some countable set. Here $\left(\sum_{i\in\mathcal I} a_i f_i\right) (x)=\sup\left\{ \sum_{j\in \mathcal J} a_j f_j(x) : \mathcal J\subseteq \mathcal I,|\mathcal J|<\infty\right\}$ There exists a unique probability measure $\mu:\Sigma\to\mathbb R$ such that when seen as a function from $\Gamma$ to $\mathbb R$ through Lebesgue integration, $\mu$ matches with the previous function. The reason why I think this may be true is that it is known that the Choquet ordering is a partial order on a convex closed bounded set (see Definition 2.9 and Proposition 2.10 in this ), this would give uniqueness so I think we have to argue about existence. We may need to add extra assumption, my goal here is to try to find necessary and sufficient condition for extitence of such a probability measure. Maybe one way to go would be to try to use Caratheodory's extension theorem on a pre-measure that we define on the topology of $\mathcal X$ . For instance I am thinking that it could be that for a closed set $A$ , $\mu(A)=\inf\{ \mu(f)-\mu(g) : f,g\in\Gamma ~s.t.~f-g\geq \mathbf 1_A  \}$ where $\mathbf 1_A$ denotes the indicator function of $A$ . If this is a good candidate (maybe we have to change into $\inf$ ) then we can define the measure on open sets $A$ as $\mu(A)=1-\mu(A^c)$ and then extend this pre-measure to the Borel $\sigma$ -algebra using Caratheodory's extension theorem. We then have to show that this measure integrates to the right value on $\Gamma$ and conclude using Proposition 2.10 of the previous document . An argument proposed by @gerw is that we can extend $\mu$ to be a linear functional on the closure of $\Gamma-\Gamma$ which may match with the set of measurable functions (or at least contain all $1_A$ for $A$ open or close) and then by the Riesz representation theorem there is a probability measure that represents that functional. I added the condition that if $f\leq g$ are in $\Gamma$ , then $\mu(f)\leq \mu(g)$ . this will indeed be true if $\mu$ comes from a probability measure and it is possible to build values for $\mu(f)$ for any $f\in\Gamma$ such that the other two axioms are satisfied but not this one. An example is when $\mathcal X=[0,1]$ , then all convex function are countable sums of the functions of the type $h_p:x\to |x-p|$ and adding the $1$ function, but if we fix any probability measure $\mu$ and take the mapping $\mu:\Gamma \to \mathbb R$ and define it to be the same on $h_p$ except for instance for $h_{1/2}$ then extend the values to $\Gamma$ linearly, we have $\mu(1)=1$ and $\sum_{i\in\mathcal I} a_i \mu(f_i)=\mu(f)$ but for sure there is no probability measure.","['measure-theory', 'lebesgue-integral', 'functional-analysis', 'convex-analysis', 'probability-theory']"
4456660,"Given a random variable $X$, can any random variable $Y$ be ""decomposed"" as a function of $X$ and $Z$ independent of $X$?","I want to know if given a random variable $X$ on some measure space $(\Omega, \mathscr M, P)$ , can any random variable $Y$ on the same measure space be ""decomposed"" as a function of $X$ and an r.v. $Z$ (again on $\Omega$ )  independent of $X$ , i.e. $Y= f(X,Z)$ (for I gues some measurable $f$ )? The reason I ask is because I was again thinking on my old question Conditional expectation $\mathbb E(Y \mid X \in B)$ instead of $\mathbb E(Y \,|\, X = x)$ (generalization of Shorack's PFS Notation 7.4.1) , and came across this comment in Probability, conditional on a zero probability event , which presents a scenario in which conditioning on $X=x$ makes complete sense: for $Y:= Z+X$ for some independent r.v.'s $X,Z$ uniform $(0,1$ ), we have $P(Z+X > 1.5 | X = 0.8) = P(Z+0.8>1.5) = P(Z>0.7)=0.3$ , i.e. we plug in $X=0.8$ to $Z+X$ and then evaluate the probability in a very straightforward way. Thus although it is true in general that conditioning on null sets is ill-defined ( Borel-Kolmogorov paradox ; or in the scenario of my comment in my previous question , there is no way of randomly generating an arbitrary rational number in $[0,1]$ in a ""uniform"" fashion --- cf. StackOverflow --- much like there is no way of generating an arbitrary integer in a ""uniform"" fashion), the reason why conditioning on the null events $X=x$ is fine is because if my conjecture is true, then it just boils down to plugging in an explicit value for $X$ and evaluating the probabilities/expectations as normal. In more detail, if we have $Y = f(Z,X)$ , then fixing $x_0$ in the codomain of $X$ we have $\mathbb E(Y|X=x_0) = \mathbb E(f(Z,x_0))$ (which then can specialize to statements about conditional probabilities).","['conditional-expectation', 'independence', 'probability-theory', 'probability']"
4456716,"Convex compact set in $\mathbb{R}^n$ where, given any point in it, the result of replacing two of its coordinates with their mean lies in the set.","Let $X$ be a nonempty compact convex subset of $\mathbf{R}^n$ . Suppose this subset has the following property: for every $x = (x_1, \dots, x_n) \in X$ , for every $1 \le i< j \le n$ , $$({x_1}, \ldots, {x_{i - 1}},
\frac{{x_i} + {x_j}}{2},
{x_{i + 1}},
\ldots,
{x_{j - 1}},
\frac{{x_i} + {x_j}}{2},
{x_{j + 1}},
\dots,
{x_n}
) \in X.$$ Is it true that there exists some $\lambda \in \mathbb{R}$ such that $$(\lambda, \dots, \lambda) \in X?$$ One idea is that we can use the above property to get a sequence $(x, x', x'', \dots)$ where $x'$ is obtained by replacing two coordinates in $x$ with their average, and $x''$ in the same way, . . . Then we use sequential compactness to say that limit, call it $L$ , also lies in $X$ . Could we argue that every coordinate of $L$ is equal?","['compactness', 'analysis', 'real-analysis']"
4456748,"The functionals $\|\cdot\|_{L^{p,q}}$ do not satisfy the triangle inequality.","Given a measurable function $f$ on a measure space $(X,\mu)$ and $0<p,q\leq \infty$ , define $$\|f\|_{L^{p,q}}=\left\{
\begin{array}{ll}
\displaystyle{\left(\int_{0}^\infty\left(t^{1/p}f^*(t)\right)^q\,\frac{dt}{t}\right)^{1/q}}, & \mbox{si } q<\infty,  \\
\sup_{t>0}t^{1/p}f^*(t), & \mbox{si } q=\infty.  
\end{array}
\right.$$ Consider the functions $f(t)=t\quad$ and $\quad g(t)=1-t$ defined on $[0,1]$ . My question is: How can I find $f^*$ and $g^*$ ? where \begin{align*} f^*: [0,\infty)&\longrightarrow [0,\infty)\\ t&\longmapsto f^*(t)=\inf\{s>0: d_f(s)\leq t\}\end{align*} and $$d_f(s)=\mu\left(\{x: |f(x)|>s\}\right),\ \ s>0$$ denotes the distribution function.
The Loukas Grafakos-Classical Fourier Analysis book suggests that $f^*(\alpha)=g^*(\alpha)=(1-\alpha)\mathcal{X}_{[0,1]}(\alpha)$ . Here $\mathcal{X}$ denotes the characteristic function.","['fourier-analysis', 'real-analysis', 'decreasing-rearrangements', 'lp-spaces', 'functional-analysis']"
4456752,Does this locus have a name?,"This comes from variational analysis and in particular from the definition of the tangent cone of a set. Say we have a circle centered at $(0, 1/\tau)$ with radius $1/\tau$ , $\tau > 0$ . All of these circles run through the origin. Take a point $(w, 0)$ , $w \neq 0$ on the horizontal axis and let $(x_\tau, y_\tau)$ be the point where the line segment that connects $(w, 0)$ and $(0,1/\tau)$ intersects the circle. As $\tau$ moves on $(0, \infty)$ it forms the locus shown in the figure above (blue line). The points of the locus can be written in the parametric form $$
\begin{align}
x(t) {}={}& \frac{tw}{\sqrt{w^2 + t^2}},\\
y(t) {}={}& t - \frac{t^2}{\sqrt{w^2 + t^2}},
\end{align}
$$ for $t\in (0,\infty)$ . I wonder whether this locus has a name.","['locus', 'geometry', 'terminology']"
4456769,Construction of Wiener measure(or Brownian motion),"In this definition of Wiener's measure , we define the measure of a standard Brownian motion by extending the f.d.d. distribution on set of all continuous functions. However, we also know that the set of all continuous functions is not measurable in the measurable space of all functions, with $\sigma$ -field generated by all pointwise projection maps $\pi_t(\omega)=\omega(t)$ . This fact, in some textbooks(like Durrett's PTE), asks us to constraint functions on rational points and then show that the sample paths are uniformly continuous with probability 1. So my question is that whether my following understanding is correct: if we start with all functions $\{\omega:[0,\infty)\mapsto\mathbb{R}\}$ then the measurability of $C(\mathbb{R})$ is indeed a problem, which requires a detour via rational approximations; but if we start with $C(\mathbb{R})$ then everything is fine. But in this case what is the $\sigma$ -field? so the reason for starting from all functions to construct the Brownian motion is that we want more generality, that we don't want to assume in advance that the sample paths are continuous almost surely?","['wiener-measure', 'brownian-motion', 'probability']"
4456790,Is recycling samples better than drawing fresh ones?,"At a high level, I am wondering if in a sequential process it is better to reutilize samples even if these samples have been used to make past decisions. Let me formalize my doubts in a toy example where, informally, the goal is to select a coin with the lowest bias in a sequence of coins. We do so by sequentially drawing samples from two coins at a time, maintaining a $(1-\delta)$ -confidence interval for each of them, and discarding a coin as soon as the two intervals detach. The idea is that, for the coin that we keep, we reutilize the samples we have already drawn to compare it to the next coin. Let $(\mu_{k})_{k\in\mathbb{N}}$ be a sequence of numbers in $[0,1]$ ,
representing the bias of a sequence of coins (where Heads is 1 and Tails is 0). At the beginning, denote coin $1$ by ""old"", coin 2 by
""new"", $n_{0} = 0$ , and $\delta\in(0,1)$ . The process unfolds in epochs. During each epoch $\tau\in\mathbb{N}$ We draw $n_{\tau-1}$ samples of the ""new"" coin, independently
of the past and each other. We repeatedly draw 2 samples at the time, 1 from the ""old"" coin and $1$ from the ""new"" coin (independently
of the past and each other), interrupting as soon as $n_{\tau}$ samples have been drawn, where $n_{\tau}$ is the smallest
number of samples needed to verify $$
\overline{X}^{\tau,\text{old}}+\sqrt{\frac{\log(2/\delta)}{2n_{\tau}}}\le\overline{X}^{\tau,\text{new}}-\sqrt{\frac{\log(2/\delta)}{2n_{\tau}}}\qquad\text{or}\qquad\overline{X}^{\tau,\text{new}}+\sqrt{\frac{\log(2/\delta)}{2n_{\tau}}}\le\overline{X}^{\tau,\text{old}}-\sqrt{\frac{\log(2/\delta)}{2n_{\tau}}}\;,$$ and $\overline{X}^{\tau,\text{old}}$ (resp., $\overline{X}^{\tau,\text{new}}$ ) is the
sum of all the samples of the ""old"" (resp., ""new"") coin that
have been drawn throughout epochs divided by $n_{\tau}$ (i.e, the empirical means). If the first condition in the display above is true, then eliminate the
""new"" coin, conclude the current epoch, and denote by ""new""
the next unused coin in the sequence. If the second condition in the display above is true, then eliminate
the ""old"" coin, conclude the current epoch, denote by ""old""
the ""new"" coin and by ""new"" the next unused coin in the
sequence. For any $s\in \mathbb N$ , if $s$ samples have been drawn so far throughout all epochs, the process is in some (random) epoch that we denote by $\tau(s)$ .
Moreover, we denote the (random) indices of the two active ""old"" and ""new"" coins at time a $s\in\mathbb{N}$ by $k(s,\text{old})$ and $k(s,\text{new})$ . We want to find a (sharp)
upper bound, for any $s\in\mathbb{N}$ and $\varepsilon>0$ , for the probabilities $$
\mathbb{P}\left(\left|\mu_{k(s,\text{old})}-\overline{X}^{\tau(s),\text{old}} \right|>\varepsilon\right)\qquad\text{and}\qquad\mathbb{P}\left(\left|\mu_{k(s,\text{new})}-\overline{X}^{\tau(s),\text{new}}\right|>\varepsilon\right)\;.
$$ The idea is that, if we started drawing fresh samples at the beginning of each epoch (throwing away the old ones), we would easily bound the tail probability above with Hoeffding's inequality. I am wondering if the same can be done when samples are recycled, maintaining the exponentially small tails.","['statistics', 'concentration-of-measure', 'large-deviation-theory', 'stochastic-processes', 'probability-theory']"
4456862,Find the value $\hat{\beta}$ which minimizes $\sum_{i=1}^{4}|i||i- \beta|$ for $\beta \in \mathbb{R}$,"I am looking for the value $\hat{\beta}$ which satisfies the following condition: for each $\beta \in \mathbb{R}$ , $$\sum_{i=1}^{4}|i||i - \hat{\beta}| \leq \sum_{i=1}^{4}|i||i - \beta|\text{.}$$ Simulation confirms that $\hat{\beta} = 3$ works as a solution: f <- function(beta) {
  i <- seq(1, 4, 1)
  return(sum(abs(i)*abs(i- beta)))
}
optimize(f, interval = c(-10, 10))
$minimum
[1] 3.000031 However, I'm not sure how to arrive at the answer of $3$ through theoretical machinery. I had attempted to handwave this by saying that it is sufficient, by term-by-term comparison, to compare $|i - \hat{\beta}|$ to $|i - \beta|$ , for which $\hat{\beta}$ should be the median of $\{1, 2, 3, 4\}$ , yielding an answer of $2.5$ (which is not supported by simulation).","['optimization', 'algebra-precalculus', 'statistics', 'inequality']"
4456878,Variance-stabilizing transformation on a simple linear regression,"I am currently working with variance-stabilizer method and readed something about it from my textbook. I want to understand it better so I would like to consider a case where I for instance have a simple linear regression, let's say I have a model with response $y$ , intercept and explanatory variable $x$ , that is, $$y_k=\beta_0+\beta_1x_k+\epsilon_k, \;\;\;\;\;\;\;\; k=1,...,n.$$ with the residual variance proportional to the square of $x_k$ : $V(\epsilon_k)=\sigma^2x^2_k$ . Then I would take the following as data transformation: $$\tilde{y}_k:=\frac{y_k}{x_k} \text{ and } \tilde{x_k}:=\frac{1}{x_k}, \;\;\;\;\;\;\;\; k=1,...,n.$$ Here is what I don't understand. The first thing is: Can I say something about the above is indeed a variance-stabilizing transformation? Second: Can I say something about the parameters from the models with original or transformed data related
to each other? I've found examples of variance-stabilizing transformation on wikipedia where they use integrals and so on, but I cannot se how it could be applied in my example. I hope anyone can help me to better understand the variance-stabilizing transformation.","['statistics', 'variance', 'transformation', 'regression-analysis']"
4456996,Doubt regarding finding the gradient of of a scalar field,"I am new to vector calculus. I watched few you tube videos and came to the conclusion that directional derivative is something like slope with direction and its value is scalar while gradient of a scalar is the vector which has  magnitude and direction as that of directional derivative.Is my understanding correct.Please correct me if i am wrong.It would be great if you can point me some wonderful simple resource (any text book or video resource ) on which i can better understand the concept Now i stumbled upon this exercise in one of my college text books I am uploading the photo here I hardly understand this question what does $||\nabla f(x,y,z) || $ mean ..how come k becomes $\frac{5}{3}$ ..Which formula of is used here.Any help and suggestion will be highly appreciated .Thanks and regards in advance. Now","['vector-fields', 'calculus', 'slope', 'vector-analysis', 'derivatives']"
4457045,Solve $(x^2-5x+5)^{x^2+4x-60}=1$-Missing Solution,"Problem: $$(x^2-5x+5)^{x^2+4x-60}=1$$ Attempt: Taking logs on both sides: $$(x^2+4x-60)\ln(x^2-5x+5)=0$$ Yields 4 solutions: $$[1]: x^2+4x-60=(x+10)(x-6)=0 \implies x=-10,6$$ $$[2]: x^2-5x+5=1 \implies (x-4)(x-1)=0 \implies x=4,1$$ I'm missing one more solution. What am I missing?","['algebra-precalculus', 'solution-verification', 'logarithms']"
4457048,"Projective Basis Explanation, Visualisation","I have trouble understanding the geometric idea behind a projective basis. So for example in $\mathbb{P}^2(K)$ we have $[1:0:0],[0:1:0],[0:0:1], [1:1:1]$ but why? I know that $$
[1:0:0] = k\cdot \vec{e_1}
$$ then we have four lines. Why is there $[1:1:1]$ ? Isnt this a linear combination of the other ones? A very brief explanation would be enough. I really have no clue why there is always an extra vector or ""line"".","['projective-geometry', 'geometry']"
4457101,When will the quadratic equation $z^2+z_1z+z_0=0$ have a root lie on the unit circle,"Very similar to this question , but what if the coefficients are complex? Is there a necessary and sufficient condition to guarentee that there is at least a root on unit circle for $z^2+z_1z+z_0=0$ where $z_{0,1} \in \mathbb{C}$ ?","['complex-analysis', 'quadratics', 'complex-numbers']"
4457104,"Does the existence of no minimal base $(X, \tau) $ implies it is a non discrete $T_1$ space?","$(X, \tau) $ be a topological space and suppose $\mathcal{B}$ is a basis of $X$ for the topology $\tau$ . It can be shown that if $\tau$ define a non discrete $T_1$ topology on $X$ , then $X$ can't have a minimal base for the topology $\tau$ .i.e  for any basis $\mathcal{B}$ there exists $\mathcal{B}'\subsetneq \mathcal{B}$ which is also a basis. My Question : Suppose the topological space $(X, \tau) $ such that $X$ has no minimal base for the topology $\tau$ . Does this implies $(X, \tau) $ a non discrete $T_1$ space.","['examples-counterexamples', 'separation-axioms', 'elementary-set-theory', 'general-topology', 'soft-question']"
4457150,How to reconcile two approaches to the Gaussian curvature,"Gaussian curvature can be found as the ratio of determinants of the second to the first fundamental forms (from here ): $$K =\frac{\det \mathrm {II}}{\det \mathrm I}=\det \left(\mathrm {I}^{-1}\rm {II}\right)=k_1({\rm I}^{-1} {\rm {II}})k_2(\rm {I}^{-1} \rm {II})\tag 1$$ where $k_1$ and $k_2$ are the eigenvalues of $\mathrm {II}^{-1}\rm I$ or principal curvatures. In matrix notation $$\rm I = \begin{bmatrix} E & F \\ F & G\end{bmatrix}=\begin{bmatrix}\langle r_u,r_u\rangle & \langle r_u,r_v\rangle \\ \langle r_u,r_v\rangle & \langle r_v,r_v\rangle\end{bmatrix}$$ and $$\rm II = \begin{bmatrix} e & f \\ f & g\end{bmatrix}=\begin{bmatrix}\langle N,r_{uu}\rangle & \langle N, r_{uv}\rangle \\ \langle N, r_{uv}\rangle & \langle N,r_{vv}\rangle\end{bmatrix}$$ In tensor calculus the Gaussian curvature is $$K= \det B^\alpha_\beta$$ with $B^\alpha_\beta$ being the covariant-contravariant form of the curvature tensor $B_{\alpha\beta}$ $$B_{\alpha\beta}=\vec r_\alpha \cdot \frac{\partial N}{\partial r_\beta}$$ and $$B^\alpha_\beta=\rm I^\alpha_\gamma B_{\gamma\beta}  \tag 2$$ where $I^\alpha_\gamma$ is the tensor notation for the first fundamental form. How can I see that equations 1 and 2 are the same? as explained by Pavel Grinfeld in here .","['tensors', 'differential-geometry']"
4457152,What is the correct filtration?,"Let $B\overset{\circ}{=}\left(B_{t}\right)_{t\geq0}$ denote a Brownian motion in a filtration $\mathcal{F}$ . Are $X_{t}=\frac{1}{\sqrt{a}}B_{at}$ ( $a>0$ constant) and/or $Y_{t}=tB_{\frac{1}{t}}$ Brownian motions in $\mathcal{F}$ ? According to my knowledge, the first one is called Scaling-Invaraince property and the second one is called Time-Inversion property. I know they are Brownian motions, but I don't know in which filtration. I would say they are Brownian motions in there own filtrations, so $X$ is Brownian motion in $\mathcal{F}_{X}\overset{\circ}{=}\left(\sigma\left(X_{t}\right)\right)_{t}$ and $Y$ is Brownian motion in $\mathcal{F}_{Y}\overset{\circ}{=}\left(\sigma\left(Y_{t}\right)\right)_{t}$ , but not necessary in $\mathcal{F}$ . For example if $\mathcal{F}\overset{\circ}{=}\left(\sigma\left(B_{t}\right)\right)_{t}$ , then $B$ is indeed Brownian motion in $\mathcal{F}$ , but $X$ can't be. It can't be, because if $a>1$ , then at time $t$ we should know the value $B_{at}$ , but we “can't see the future”. $B_{at}$ is not adapted to $\mathcal{F}$ and neither $\frac{1}{\sqrt{a}}B_{at}$ . The similar holds if we examine $Y$ . For example at $t=\frac{1}{2}$ we don't know (nor can calculate) the value of $\frac{1}{2}B_{2}$ , therefore it is not adapted to $\mathcal{F}$ . To be Brownian motions in $\mathcal{F}$ they should be adapted to $\mathcal{F}$ . Is this train of thoughts correct? If it is not, then where is the mistake? I understand $X$ and $Y$ are Brownian motions, but in which filtration?","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'filtrations']"
4457177,"$f,g$ convex, increasing real functions with $\frac{f(x)}{g(x)}\to 1.$ Does $\frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ?$","Let $f,g:\mathbb{R}\to\mathbb{R}$ be convex strictly increasing real functions (so we have both $f(x)\to\infty $ and $g(x)\to\infty$ as $x\to\infty),$ and suppose further that $\frac{f(x)}{g(x)}\to 1.$ Then is it true that $\frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ?$ I know that since $f,g$ are convex with domain $\mathbb{R}$ , they are continuous, which is why I left this out of the question. And if $f,g$ did not have to be convex, then $f(x) = \ln(x),\ g(x) = \ln(x)+1$ would be a counter-example, as $\frac{f^{-1}(x)}{g^{-1}(x)}\to e\neq 1.$ I'm not coming up with a counter-example, and am also not sure how to approach the question otherwise as there seems to be a lot of things to consider in order to prove it true.","['functions', 'convex-analysis', 'graphing-functions', 'real-analysis']"
4457220,Splitting the Tangent Bundle of a Vector Bundle along the Zero Section,"Good evening everyone, I have a small question: Assume we have a vector bundle $E = \bigcup\limits_{x\in M} E_x$ over a manifold $M$ . I now want to show the following well-known equation: $$TE_M \cong TM \oplus E$$ I've seen it done with short exact sequences, but I wonder if it can be done more elementary: Let $s : M \hookrightarrow E$ be the zero section (i.e. $s(x) = 0 \in E_x$ ), then we get for each $x\in M$ the direct sum $E_x = \{0\} \oplus E_x = s(x) \oplus E_x$ , so: $$E= s(M) \oplus E$$ For the tangent bundle, this means (identifying $x = s(x)$ ) $$T_{x}E = T_{x}s(M) \oplus T_{x}E \cong T_xM \oplus E_x,$$ since $s(M) \cong M$ and $T_xE = T_x E_x \cong E_x$ as it is a vector space. So, my question is: Does anyone see a mistake or something essential missing from my ""proof""?","['algebraic-topology', 'differential-geometry']"
4457245,"Compute $\lim\limits_{(x,y) \to (0,0)} \frac{x^2}{x^2+y^2} \sin\left(\frac{xy}{\sqrt{x^2 + y^2}}\right)$","Acordding to Wolfram Alpha: $$\lim\limits_{(x,y) \to (0,0)} \frac{x^2}{x^2+y^2} \sin\left(\frac{xy}{\sqrt{x^2 + y^2}}\right) \quad\text{does not exist.}$$ Using: $$\gamma(t) = (t,0)\;\; \text{and}\;\; \gamma(t) = (t,t)$$ we can easily prove that $$\lim\limits_{(x,y) \to (0,0)} \frac{x^2}{x^2+y^2} \quad\text{does not exist.}$$ however $$\lim\limits_{(x,y) \to (0,0)} \sin\left(\frac{xy}{\sqrt{x^2 + y^2}}\right) = 0$$ so there are no paths that will give a result different than $0$ . My question has two parts: How do I prove that this limit does not exist. On most of the online courses that I take, in the questions they explicitly tell you if the limit exists before you start. However, my professor in the exams just tells us to calculate the limit or prove if it does not exist. Is there a easy way to determine if the limit exists? I have expend around 30 minutes trying to prove that this limit exists by the squeeze theorem, before searching the answer on Wolfram Alpha. Edit: People in the comments said that this limit does exist. If this is the case, my question becomes how to compute it.","['limits', 'multivariable-calculus']"
4457260,Algorithm to find the angle of a direction,"I need to translate the distance between two points into an angle from 0 to 359. To do this I use the new position coordinates which is defined by a vector and subtract the original position. This generates a difference which I can then use to define the slopes absolute value (I know it can be done with out the absolute value if I change my code up). When the movement is within the NW_North section the angle will be 135 when the absolute value of the slope is 1 and as the slope converges on infinity the angle will be 180. The challenging thing is I need to break up all possible values into 45 separate angles but I have no idea what the correct algorithm is since it will need to be rounded to the nearest integer. Also I dont think the center is actually 2 but its close to it and it appears that as the distance between each red line doubles the slope gets squared but I could be wrong. public static int getNormalizedMovementAngle(Vector3d vector, double x, double z) {
    final double zDifference = vector.getZ() - z;
    final double xDifference = vector.getX() - x;
    final double slope = Math.abs(zDifference / xDifference);

    if(vector.getX() < x) {
        if(vector.getZ() == z) return 90;
        else if(vector.getZ() < z) {
            if(slope == 1) return 135;
            else if(slope > 1) {
                //Formula needs to go here
            }
        }
    }
} Anyways if anyone knows how I can use the slope to generate 45 different angles between NW and North please let met know thank you!! Edit: I can use the following to get the angle for NW_North however I noticed that its a bit off for West_NW why is that exactly? I divided the side up into 45 sections which then allowed me to decrement the slope by 0.02222 and it appears that my equation does not assign each angle an equal width section. This means multiple sections get assigned the same angle (max of 2) for the first half and then it starts skipping an angle till it finally converges on 90 degrees. return Math.toIntExact(Math.round(90 * (2 - Math.pow(2, -slope)))); Does anyone know how to get atan2 to use the domain of 0 to 360 degrees? I can also swap the x and z difference values for my atan2 equation depending on which quadrant we are in if that makes the calculations easier (for example SW_West could go from 1 to infinity while South_SW can go from 1 to 0). final int angle = Math.toIntExact(Math.round(Math.atan2(zDifference, xDifference) * (180.0 / Math.PI)));","['vectors', 'geometry', 'algorithms']"
4457269,Defining a function without using Axiom of Choice,"I have a situation where I do not know if I need the axiom of choice:
Let $\mathcal{B}(\mathbb{R})$ be the collection of Borel measurable subsets of $\mathbb{R}$ .
I have a (possibly non-Borel) subset $M \subseteq \mathbb{R}$ and a probability measure $P:\mathcal{B}(\mathbb{R})\rightarrow[0,1]$ with the property: $$ P(A)=P(B) \quad \forall A, B \in \mathcal{B}(\mathbb{R}) \mbox{ such that $M\cap A = M\cap B$}  \quad (Eq. 1)$$ So I can group all sets in $\mathcal{B}(\mathbb{R})$ into equivalence
classes where $A$ and $B$ are equivalent if $M\cap A = M \cap B$ . I want to condense $P$ to a function $g$ on equivalence classes.  Specifically, define $$V = \{M\cap A: A \in \mathcal{B}(\mathbb{R})\}$$ Define $g:V\rightarrow[0,1]$ as follows:  For each $D \in V$ , I can choose an $A \in \mathbb{B}(\mathbb{R})$ such that $M \cap A = D$ , then I can define $g(D)=P(A)$ . Formally, using the axiom of
choice , there is a choice function $c:V\rightarrow \mathcal{B}(\mathbb{R})$ such that $$c(D)\in \{A\in \mathcal{B}(\mathbb{R}):M\cap A=D\} \quad \forall D \in V$$ Then I define $g(D)=P(c(D))$ . Notice by (Eq. 1) that this leads to the
same $g$ function regardless of my choice function $c(D)$ .  In particular: $$g(M\cap A) = P(A) \quad \forall A \in \mathcal{B}(\mathbb{R}) \quad (Eq. 2)$$ Question: Do I really need to use the Axiom of Choice when defining this g function? I think that, due to (Eq. 2), I do not formally need the axiom of choice here. Perhaps I can simply define objects $(A,P[A])$ for all $A \in \mathcal{B}(\mathbb{R})$ and then simply ""say"" that I condense these objects according to equivalence classes, so that my function $g$ somehow emerges. However, it is often hard to know if I am inadvertently using the axiom of choice. Edit: I guess I could just define the set $\{(M\cap A, P[A]) : A \in \mathcal{B}(\mathbb{R})\}$ and $g$ emerges...?","['axiom-of-choice', 'measure-theory', 'set-theory']"
4457297,How can I reconcile cardinality and and subsets in Set Theory?,"I have a problem with subsets in Set Theory. a set A is a subset of set B if all the elements of A are also elements of B. {1,2} is a subset of {1,2,3} Simple enough. But, as I understand it, the word ""subset"" means a set within another set.
So, intuitively if A is in B It would mean that B is {{1,2},3} This would mean that {1,2,3} and {{1,2},3}  and {1,{2,3}} are the same set as brackets would be arbitrary ways of grouping elements in subsets within the set.
This is generally the way we understand and draw sets as circles within circles. Circles are drawn depending on how we want to group the content but they have no substance in and on themselves.
This is also backed up by the fact that two identical elements are the same element, and by that token, subset A is entirely inside the set B. But when looking at cardinalities, it seems those 3 sets are very different as {1,2,3} has a cardinality of 3 while {{1,2},3} and {1,{2,3}} have a cardinality of 2.
If they were the same set, their cardinalities would be ambiguous.
Therefore, they aren't the same set. So, I conclude that I am wrong in the way I see subsets as subsets are NOT sets within sets, they are just sets which content is also included in another set. Right? In a nutshell, it's hard to tell if a set is only its elements or if a set is the collection of the elements AND the brackets around it . It can't be more basic than that but I'm confused since both interpretations could make sense but they are incompatible. If brackets are included a subset is not a set within a set (since it would make cardinality inconsistent), and if brackets are not included how can cardinality depend on sets within sets?
It's not just a problem of semantics since I get actual inconsistencies depending on how I understand it No matter how I see it, I can't seem to reconcile subsets and cardinality. EDIT: a user completely edited the title but the new formulated question was not the question I asked (and some things I considered important in my text had been erased too). So I decided to rollback to what it was. Thank you very much though for editing the format. If needed I'm happy to edit anything myself, if I'm asked to. My username can only be associated with my own words. It seems fair.",['elementary-set-theory']
4457334,On local group rings,"Let $G$ be a finite group and $k$ a field of characteristic $p$ . Suppose that the ring $k[G]$ is local. How this implies that G is a p-group? In fact, there are different proofs as in the answers below. But, I'm confused with the hint that rised in the exercise ( page 54) given in ( library.msri.org/books/Book51/files/02iyengar.pdf). Since all maximal ideals are prime, the nilradical $nil(k[G])$ is contained in the Jacobson radical $J(k[G])$ . Since $k[G]$ is a finite dimensional $k$ -algebra, it is a left-Artinian ring. So all prime ideals are maximal and then $nil(k[G])=J(k[G])$ . However, I'm not sure how to use this hint to prove that $G$ is a $p$ -group. I would appreciate any help. Thank you in advance.","['finite-groups', 'modules', 'ring-theory', 'p-groups', 'group-theory']"
4457377,What do these double integrals represent?,"Question I have three double integrals: (A) $$\int_{0}^{1}\int_{x^2}^1 dydx$$ (B) $$\int_{0}^{1}\int_{x}^{2x} x^2 dydx$$ (C) $$\int_{0}^{1}\int_{-y}^{y} dxdy$$ These need to be matched to the appropriate statements: (a) The area of the triangle in the $xy$ -plane corresponds to (b) The area of a region in the $xy$ -plane bounded on a side by a parabola corresponds to (c) The volume under the surface $z=x^2$ above a triangle in the $xy$ plane corresponds to (d) The volume under the plane $z=1$ above a triangle in the $xy$ plane corresponds to My Attempt So far I have that (a) C (b) A (c) B (d) C However I am confused if there can be multiple answers. For example, is it possible that (a) is B and C, since both regions of integration are triangles in the $xy$ plane? These are my sketches of the regions of integration:","['integration', 'area', 'definite-integrals', 'volume', 'multivariable-calculus']"
4457436,Multiplication of two random matrices over a finite field,"Consider a matrix $\mathrm{X}$ sampled uniformly at random from the set of all rank $r$ matrices over $\mathbb{F}_q^{m \times n}$ and a matrix $\mathrm{Y}$ sampled uniformly at random from the set of all full rank matrices over $\mathbb{F}_q^{n \times n}$ . Let $\mathrm{Z} = \mathrm{X}\mathrm{Y}$ . I am trying to prove something like the statement that $\mathrm{Z}$ is uniform over the set of all rank $r$ matrices over $\mathbb{F}_q^{m \times n}$ . Assume $m > n$ and $r \leq n$ . This old stackexchange answer says that this is indeed the case, but it does not explain how to reach the result. Is this a common result in random matrix theory, and if so, can someone provide a reference or a proof?","['finite-fields', 'matrices', 'linear-algebra', 'random-matrices', 'probability']"
4457458,Is integer part of $e^n$ infinitely often even and odd?,"Let $a_n:=\lfloor e^n\rfloor$ . Are $\{n\mid 2|a_n\},\,\{n\mid 2|a_n-1\}$ both infinite sets? More generally, for any irrational number $\alpha>1$ , is each set of the form $\{n\mid p|\lfloor\alpha^n\rfloor-q\}$ infinite?","['number-theory', 'irrational-numbers', 'algebraic-number-theory']"
4457539,"$a_{m^2}=a_m^2,a_{m^2+k^2}=a_ma_k$ sequence","Sequence $\{a_n\},n\in\mathbb N_+$ with all terms positive integers satisfy $a_{m^2}=a_m^2,a_{m^2+k^2}=a_ma_k$ . Find $\{a_n\}$ . I suppose all terms of $\{a_n\}$ are $1$ . This problem makes me think of a lot of conclusions, including $n\in\mathbb{Z}_+$ can be written as the sum of two squares as long as for every prime $p\equiv3\pmod4$ there's $2\mid V_p(n)$ . $(m^2+n^2)^2=(m^2-n^2)^2+(2mn)^2$ . $(a^2+b^2)(c^2+d^2)=(ac+bd)^2+(ad-bc)^2=(ac-bd)^2+(ad+bc)^2$ . Perhaps we can let the first non- $1$ term of the sequence be $a_s$ and derive a contradiction?",['sequences-and-series']
4457544,Theorem about multiplicity set of continuous functions.,"Relevant Theorems: $(a)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on every value exactly twice. $(b)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on each value either $0$ times or $2$ times. $(c)$ Find a continuous function $f$ on $\mathbb{R}$ which takes on every value exactly $3$ times. $(d)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on every value exactly $2n$ times , for all $n \in \mathbb{N}$ . Source: Spivak's ""Calculus"" Definitions: Fix a function $f:\mathbb{R}\to\mathbb{R}$ such that $f$ obtains each value only finite (possibly $0$ ) number of times. We say $E \subset \mathbb{N}$ is ""the multiplicity set"" of $f$ if $1$ . For every $n \in E$ , there exists at least one value that $f$ takes exactly $n$ times. $2$ . If $f$ takes on a value exactly $n$ times, then $n \in E$ In other words, $n \in E$ iff $n$ is the multiplicity of some value under $f$ . A subset $E $ of $N$ is said to be ""constructable"" iff there exists a continuous function $f$ on $\mathbb{R}$ such that $E$ is the multiplicity set of $f$ . The problem: With these definitions, it seems like the theorems from Spivak are just telling that the sets $\{2\},\{0,2\},\{2n\}$ cannot be constructed, and, we can construct $\{3\}$ . After this, we can also construct sets like $\{0,1,4\},\{0,2,4\},\{0,1,2,4\},\{4,5\}$ etc. So far, I have got one theorem about the construction of some sets. For every continuous function $f$ on $\mathbb{R}$ with multiplicity set $E$ , if $E$ has maximum $2n$ then $E$ includes $0$ and some member of $\{1,..,n\}$ . This theorem tells us that sets like $\{0,3,4\}$ are impossible, but doesn't tell anything about sets with odd maximums. So far I don't have anymore theorems to decide whether a set is constructable or not. Hence, my question is: If $E$ is a finite subset of $\mathbb{N}$ , then is there a way to effectively & efficiently decide whether or not $E$ is constructable or not?","['continuity', 'real-analysis']"
4457551,Schönert & Seress Algorithm - Computing all block systems - blocks of imprimitivity,"Atkinson as well as Schönert and Seress describe methods to compute the minimal block system; in particular in Permutation Group Algorithms by Ákos Seress, we find Theorem 5.5.1 Suppose that a set S of generators for some transitive $G \leq Sym(\Omega)$ is given and $|\Omega| = n$ . Then a minimal block of imprimitivity can be computed in $\mathcal{O}(n\log^3|G|+n|S|\log|G|)$ time by a deterministic algorithm. What is hard to find, however is the time complexity of computing all non-trivial blocks of imprimitivity. I have worked out some ideas but they all seem to be incorrect, so help is very appreciated. Let $Sym(n)$ . Let $G = \langle x_1, ..., x_n\rangle \leq Sym(n)$ be transitive, so $|S|=n$ . According to Handbook of Computational Group Theory by Bettina Eick, Derek F. Holt, and Eamonn O'Brien then we need to run the algorithm mentioned above $n-1$ times, on the pairs $\{1, \alpha\}$ for $2 \leq \alpha \leq n$ which gives us something like $\mathcal{O}(n^2\log^3|G|+n^3\log|G|)$ . To make things a bit simpler, we expect $n \leq |G| \leq n!$ so let us consider $|G| = n!$ , which gives us $\mathcal{O}(n^2\log^3n!+n^3\log n!)$ . Now, since $\log n! = \log n + \log (n-1) + ... + \log 2 \leq n\log n$ , we get $\mathcal{O}(n^2(n\log n)^3+n^4\log n)$ and so $\mathcal{O}(n^5\log^3+n^4\log n)$ . And this does not even account for finding all non-trivial block systems, only the minimal ones. Then we need to ""expand"" these partitions by joining and joining until no new partition shows up which has its own complexity which would be nice to know. Is there anyone out there with an idea about the theoretical complexity of computing all these block systems?","['gap', 'computational-algebra', 'magma-cas', 'computer-algebra-systems', 'group-theory']"
4457595,Inequality on expectation of exponential martingale,"Let $X$ be a continuous local martingale with $X_0=0$ . Define the exponential local martingale $$\mathcal{E}(X)=e^{X-\frac{1}{2}[X]}.$$ For any $p,q>1$ , establish the identity $$\mathcal{E}(X)^p=\mathcal{E}(\sqrt{pq}X)^{1/q}\left(e^{\frac{\sqrt{pq}(\sqrt{pq}-1)}{q-1}X}\right)^\frac{q-1}{q}.$$ Hence prove that $$\mathbb{E}\left[\mathcal{E}(X)_T^p\right]\leq\left(\mathbb{E}\left[e^{\frac{\sqrt{pq}(\sqrt{pq}-1)}{q-1}X_T}\right]\right)^\frac{q-1}{q}$$ for any finite stopping time $T$ . I have managed to obtain the identity fairly easily. However, I am really not sure how to continue. I thought to perhaps use Cauchy-Schwarz to split the expectation into 2, but this gives the wrong exponent because of the squaring. Also, I would like to use the fact that $\mathcal{E}(\sqrt{pq}X)$ is a positive local martingale and thus a supermartingale, but as $T$ is not necessarily bounded I need to show uniform integrability to use the optional stopping theorem, which I do not see why should hold. I therefore feel that my approach is probably wrong. Any advice would be greatly appreciated! EDIT: Holder's inequality will give the required splitting of integrals. However I am still not sure why I can use the OST.","['stochastic-processes', 'local-martingales', 'martingales', 'probability-theory', 'stochastic-calculus']"
4457621,Weak convergence of positive operators in Hibert space,"Let $A_n$ be a sequence of positive operators on the Hilbert space $\mathcal{H},$ weakly convergent to an operator $A$ (necessarily positive).  Does it imply the strong convergence ? By the weak convergence we mean $$\lim_n\langle A_nx,y\rangle =\langle Ax,y\rangle,\qquad x,y\in \mathcal{H}$$ while the strong convergence
(or pointwise convergence) means $$\lim_n\|A_nx-Ax\|=0,\qquad x\in \mathcal{H}$$ If $A_n\to 0$ weakly, then $A_n\to 0$ strongly
as $$\|A_n^{1/2}x\|^2=\langle A_nx,x\rangle \to 0$$ Hence $A_n^{1/2}\to 0$ strongly, and consequently $A_n=A_n^{1/2}A_n^{1/2}\to 0$ strongly (as the norms $\|A_n^{1/2}\|$ are uniformly bounded). According to post , the condition $\lim\|A_nx\|=\|Ax\|$ and weak convergence imply the strong convergence for general class of operators. Besides, if the sequence $A_n$ is monotonic, increasing $(A_n\le A_{n+1})$ or decreasing $(A_n\ge A_{n+1})$ then $A_n$ is convergent to $A$ strongly. I was unable to come up with any example such that $A_n\to A$ weakly but not strongly and $A_n\ge 0$ . Perhaps the weak  convergence implies the strong convergence for positive operators ?","['operator-theory', 'functional-analysis']"
4457632,"Maclaurin series, find the tenth derivative","The problem is as follows: Find the Maclaurin series of $$\begin{cases} \frac{\sin(x)}{x},& x \neq 0 \\ 1,& x=0 \end{cases}$$ and then find $f^{10}(0)$ . I figured out the series, if $x\neq 0$ then it is $$\sum_{k=0}^{\infty} (-1)^k \frac{x^{2k}}{(2k+1)!}$$ but I have a question about the tenth derivative. I know how to find it in the typical Maclaurin series, but because the definition of the function says that if $x=0, f(x)=1$ . Does that mean that $f^{(10)}(0)=1$ too? If no, can I have a quick clue, what should I do?","['derivatives', 'taylor-expansion', 'sequences-and-series']"
4457676,Can reparameterization make Cramer-Rao bounds tight?,"Given a family of distributions parametrized by $\theta$ for which Cramer-Rao bounds on variance for a (biased or unbiased) estimator of $\theta$ exist, these bounds may be unattainable. In the proof for the 1D case, the looseness is introduced by applying the Cauchy-Schwarz inequality on the covariance between the score and the statistic. Is there a “onto” map $\phi\mapsto\theta$ for which CRLBs for $\phi$ are tight?","['statistics', 'parameter-estimation', 'reference-request', 'real-analysis', 'change-of-variable']"
