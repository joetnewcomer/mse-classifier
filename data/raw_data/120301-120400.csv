question_id,title,body,tags
1799049,What's my confusion with the chain rule? (Differentiating $x^x$),"When deriving $x^x$, why can't you choose $u$ to be $x$, and find $\dfrac{d(x^u)}{du} \dfrac{du}{dx} = x^x$? Or you could go the other way and find $\dfrac{d(u^x)}{du}\dfrac{du}{dx}$, giving $\ln(x)\cdot{x^x}$? Both methods seem to be equally wrong.","['chain-rule', 'calculus']"
1799097,Interchanging of order summation in proposition 1.25 [Rudin RCA],"Hello! This proposition from Rudin's RCA book.
One moment confuses me, namely how he interchanges the order of summation in that double infinite series? Can anyone give a rigorous explanation of it? EDIT: I know only that theorem from Rudin's PMA. Maybe we can apply it? I would be very grateful for comment/answer.","['real-analysis', 'measure-theory']"
1799106,Can a Submanifold Become Tangent to a Nowhere Tangent Vector Field,"$\newcommand{\R}{\mathbf R}$ Let $M=\R^2$ and $S=\{(0, t):-1<t < -1\}$ be a submanifold of $M$. Let $V$ be a vector field on $M$ which is nowhere tangent to $S$. Let $\theta$ be the flow of this vector field defined on the flow domain $D\subseteq \R\times M$. Let $t_0\in \R$ be such that the integral curve starting at the origin is defined on $[0, t_0]$. It is known that there is a neighborhood $O$ of the origin such that $\theta_{t_0}:O\to M$ maps $O$ diffeomorphically onto $\theta_{t_0}(O)$, the latter being open in $M$. Write $\mathbf p=\theta_{t_0}(\mathbf 0)$ and note that $S_{t_0}:=\theta_{t_0}(O\cap S)$ is a $1$-submanifold of $M$ passing through $\mathbf p$. Question. Is it possible that $V_{\mathbf p}$ is tangent to $S_{t_0}$?","['smooth-manifolds', 'vector-fields', 'ordinary-differential-equations', 'differential-geometry']"
1799113,Disprove this number that exists in the integers.,"$\exists n\in \mathbb{Z}, n^{2}<n$ I've started to prove the contradiction is true: $\forall n\in \mathbb{Z}, n^{2}\geq n$ But not sure how to do this, unless I need to show what (n)(n) = (m), m is always greater than or equal to n? Not sure how I would write this in proof terms.",['discrete-mathematics']
1799125,How many points in a line segment?,"My teacher said that in the circumference of circle there are infinite points. When I was learning more about circle, I came to this picture: My question is: When we unroll the circle, then the length of the circle and line segment are the same. For this reason, I think that the unrolled line segment should also have infinite points! But there are only two end points. Can anyone explain to me where am I going wrong?","['infinity', 'elementary-set-theory']"
1799153,Calculate the integral $\iint_D (y^2-x^2)^{xy} (x^2+y^2)dxdy$ on a certain region,"Let $D$ be the region that's bounded by $xy=a, xy=b, y^2-x^2=1, y=x$ in the first quadrant. Calculate the integral $\iint_D(y^2-x^2)^{xy}(x^2+y^2)dxdy$. Firstly, I was able to show that the boundary of this region is a Jordan curve and that $D$ is bounded. Therefore, I can use either Green's theorem (which I couldn't find a good use for here) or change of variables. So I tried using polar coordinates but that didn't get me anywhere. I also tried the substitution $u=y-x, v=y+x$ such that $x=-\frac{1}{2}u+\frac{1}{2}v, y=\frac{1}{2}u+\frac{1}{2}v$ such that $|J|=\frac{1}{4}$. I had some trouble here showing what the range of integration is using $u,v$, so for now we'll call it $\Omega$. Therefore: $\iint_D (y^2-x^2)^{xy}(x^2+y^2)dxdy=\iint_\Omega(uv)^{\frac{1}{4}v^2-\frac{1}{4}u^2}(\frac{1}{2}u^2+\frac{1}{2}v^2)dudv$. My problems with this particular coordinate substitution are that it's difficult to calculate the range of integration and that the integral itself (regardless of the range) isn't very friendly. I'd like to know if there's some other direction that should be pursued or if there's an easier coordinate substitution. ***Edit: I do believe that the best way to do this is to use the substitution $u=xy, v=y^2-x^2$ because then $a<u<b, 0<v<1$, but I have trouble calculating this substitution's Jacobian. This is due to the fact that it's not easy to write $x,y$ as functions of $u,v$, so if anyone can help with this direction that would be appreciated.","['multivariable-calculus', 'integration', 'calculus', 'vector-analysis']"
1799180,Examples 3.35 (a) and (b) in Baby Rudin: Limit Superior and limit inferior of a couple of sequences,"This question is related to Examples 3.35 (a) and (b) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition, p. 67. Let us consider the series 
$$ \frac 1 2 + \frac 1 3 + \frac{1}{2^2} + \frac{1}{3^2} + \frac{1}{2^3} + \frac{1}{3^3} + \frac{1}{2^4} + \frac{1}{3^4} + \cdots$$ 
for which the formula for the general term $a_n$ is given by 
$$a_n = \begin{cases} \frac{1}{2^k} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{3^k} \ \mbox{ if } \ n = 2k \end{cases} $$ 
for $k = 1, 2, 3, \ldots$, and the series 
$$\frac 1 2 + 1 + \frac 1 8 + \frac 1 4 + \frac{1}{32} + \frac{1}{16} + \cdots$$ 
for which the formula for the general term is given by 
$$b_n =  \begin{cases} \frac{1}{2^n} = \frac{1}{2^{2k-1}} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{2^{n-2}} = \frac{1}{2^{2k-2}} \ \mbox{ if } \ n = 2k \end{cases} $$ for $k = 1, 2, 3, \ldots$. Now Rudin states that 
$$\liminf_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \left( \frac 2 3 \right)^n = 0,$$
$$\liminf_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[2n]{\frac{1}{3^n}} = \frac{1}{\sqrt{3}},$$
$$\limsup_{n\to\infty} \sqrt[n]{a_n} = \sqrt[2n]{\frac{1}{2^n}} = \frac{1}{\sqrt{2}},$$
$$\limsup_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \frac 1 2 \left( \frac 3 2 \right)^n = +\infty.$$
And, Rudin also states that 
$$ \liminf_{n\to\infty} \frac{b_{n+1}}{b_n} = \frac 1 8,$$
$$ \limsup_{n\to\infty} \frac{b_{n+1}}{b_n} = 2,$$ 
$$\lim_{n\to\infty} \sqrt[n]{b_n} = \frac 1 2.$$ How to rigorously verify these statements using machinery (i.e. the definitions and theorems ) developed by Rudin up to this point? I know that $\liminf$ and $\limsup$ are the infimum and supremum, resp., of the set of all the subsequential limits (in the extended real number system) of a sequence, and there is a subsequence each converging to $\liminf$ and $\limsup$. Moreover, for each $k \in \mathbb{N}$, we have 
$$\frac{a_{2k} }{a_{2k-1}} = \frac{ \frac{1}{3^k} }{ \frac{1}{2^k} } = \left( \frac 2 3 \right)^k \to 0 \ \mbox{ as } \ k \to \infty,$$
$$\frac{ a_{2k+1} }{ a_{2k} } = \frac{ \frac{1}{2^{k+1}}}{ \frac{1}{3^k} } =\frac 1 2 \left( \frac 3 2 \right)^k   \to +\infty \ \mbox{ as } \ k \to \infty,$$
$$\sqrt[2k]{a_{2k}} = \sqrt[2k]{ \frac{1}{3^k}} = \sqrt{\frac 1 3} \to \sqrt{\frac 1 3} \ \mbox{ as } \ k \to \infty,$$ 
$$\sqrt[2k-1]{a_{2k-1}} = \sqrt[2k-1]{ \frac{1}{2^k}} = \frac{1}{2^{\frac{k}{2k-1}}} \to ? \ \mbox{ as } \ k \to \infty.$$
How to find $\lim_{k \to \infty} \sqrt[2k-1]{2^k} $ using what Rudin has established (in Theorem 3.20)? Also, $$\frac{ b_{2k} }{ b_{2k-1} } = \frac{ \frac{1}{ 2^{2k-2} } }{ \frac{1}{ 2^{2k-1} } } = 2 \to 2 \ \mbox{ as } \ k \to \infty,$$
$$\frac{ b_{2k+1} }{ b_{2k} } = \frac{ \frac{1}{ 2^{2k+1} } }{ \frac{1}{ 2^{2k-2} } } = \frac 1 8 \to \frac 1 8  \ \mbox{ as } \ k \to \infty,$$ $$\sqrt[2k]{b_{2k}} = \sqrt[2k]{\frac{1}{2^{2k-2}}} = \frac{ \sqrt[2k]{4} }{ 2 } \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$ 
$$\sqrt[2k-1]{b_{2k-1}} = \sqrt[2k-1]{\frac{1}{2^{2k-1}}} = \frac 1 2 \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$","['real-analysis', 'limsup-and-liminf', 'sequences-and-series', 'convergence-divergence', 'analysis']"
1799191,the physical significance of the Lie Algebra of SE(3),"The Lie group of $SE(3)$ can be written in the form of $4\times4$ matrix, say $$
\begin{pmatrix}
R & t\\
0 & 1
\end{pmatrix},\tag{1}
$$ and its Lie Algebra, denoted as $se(3)$ , can be composed by 6 generators $$
\begin{aligned}
G_1&=\begin{pmatrix}0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_2&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_3&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{pmatrix}\\
\\
G_4&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_5&=\begin{pmatrix}0 & 0 & 1 & 0\\
0 & 0 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_6&=\begin{pmatrix}0 & -1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix},
\end{aligned}\tag{2}
$$ so its Lie Algebra can be represented by multiples of the generators in the form of $4\times4$ matrix, $$
\begin{pmatrix}
w_x & u\\
0 & 0
\end{pmatrix}.\tag{3}
$$ We can use the coordinates of rotation axis to build $w_x$ , skew symmetric matrix, and the coordinates of translation vector to build $u$ , so by exponential map, we can get homogeneous matrix in the form of (3) which we can use in the displacement of a frame. I am confused that why we can use the coordinates of rotation axis and translation vector to build $se(3)$ by multiples of the six generators, or what is the link among rotation axis ,translation vector and  the six generators? What is the physical significance of the $se(3)$ ? I hope that you can help me by explain the physical significance of $se(2)$ or $so(2)$ first to me.
Thanks for anyone's help.","['linear-algebra', 'lie-groups']"
1799227,Lebesgue-$\sigma$-algebras $\mathfrak L^{p+q}\neq\mathfrak L^p \otimes\mathfrak L^q$,"I already know that for Borel-$\sigma$-algebras it holds that $\mathfrak B^{p+q}=\mathfrak B^p \otimes\mathfrak B^q$. Now I want to show that this is not the case for Lebesgue-$\sigma$-algebras $\mathfrak L$. So first of all, given a zero Lebesgue-null-set $N$ in $\mathbb R^p$, I have proven that $N\times B$ is a Lebesgue-null-set in $\mathbb R^{p+q}$ for arbitrary $B\subseteq \mathbb R^q$. Now, how can I show that $$\mathfrak B^p \otimes\mathfrak B^q\subsetneq \mathfrak L^p \otimes\mathfrak L^q \subsetneq \mathfrak L^{p+q} $$ My work for the first part of the proof: So if we chose $B$ where $\mu(B)<\infty$, it is obvious that we obtain $$\lambda(N\times B)=\nu(N)\mu(B)=0\times M=0$$ where $M\in\mathbb R$ is an upper bound of $\mu(B)$. Thus I considered the case $\mu(B)≥\infty$. Since $\mu$ is $\sigma$ -additive, we can find a sequence $B_n\subseteq B$ such that $\bigcup_{n=1}^\infty B_n = B$ and $\mu(B_n)<\infty$ for all $n$.
It follows that $$\lambda(N\times B)= \lambda(N\times \bigcup_{n=1}^\infty B_n )=\nu(N)\mu(\bigcup_{n=1}^\infty B_n) = \nu(N)\lim_{n\to\infty}\mu( B_n)=0\times S=0$$ where $S$ is the upper bound of $B_n$.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'algebras']"
1799236,Atomless measure space without measure preserving isomorphisms,"Question : Could somebody give an example of a nontrivial atomless measure space without measure preserving isomorphisms (except for the identity)? Background : A measure preserving isomorphism on a measure space $(X,\Sigma,\mu)$ is a bijection $\phi$ such that $$\forall A\in\Sigma:\mu(\phi^{-1}(A))=\mu(\phi(A))=\mu(A)$$ Edit: By ' except for the identity ' I obviously meant to exclude all $\phi$ such that $\mu(A\triangle\phi(A))=0$ for all $A\in\Sigma$.",['measure-theory']
1799238,What is wrong with this proof that the identity map of $S^1$ is nullhomotopic?,"I have read that the identity map of the unit circle $S^1$ is not nullhomotopic. In fact, I am very new to the subject, so I wonder what is wrong with the following reasoning (that seems to suggest the opposite): If the identity map $i : S^1 \to S^1$ is nullhomotopic, there exists a homotopy $F : S^1 \times I \to S^1$ with $F(x,0)=x$ and $F(x,1)=k$, for all $x \in S^1$. But this, in other words, just means that $x$ and $k$ can be joined by a path, for any $x \in S^1$ - which is true, since $S^1$ is path connected. After all, can't we think of a homotopy of two functions $f, g : X \to Y$ as a function that for all $x \in X$, gives us a path between $f(x)$ and $g(y)$? I wonder where is the contradiction? What I am missing?","['algebraic-topology', 'general-topology', 'homotopy-theory']"
1799261,Addition of vectors given at different points on a manifold,"Let's say I have a set of tangent vectors given at different points ($\vec{v}_i \in T_{p_i}(M)$) on a riemannian manifold with metric and compatible levi civita connection and I like to calculate e.g. the ""mean"" of all vectors of this set. So I need to add vectors in some sense. Now I have learned that without connection addition of vectors does not make sense since they are all defined in different tangent spaces. How does the concept of parallel transport fix ($\vec{v}_1,p_1$) + ($\vec{v}_2,p_2$) ? Do I understand this correctly that I define a vector addition at one point e.g. $p_1$ ($\vec{v}_1,p_1$) + ($\vec{v}_2,p_2$) as: Find the geodesic between $p_1$ and $p_2$ with tangent vector $\vec{v}_2$ at $p_2$ (solve this set of ODEs) and calculate the tangent vector $\vec{\hat{v}}_2$ of this curve at $p_1$ so that  at $p_1$ ($\vec{v}_1,p_1$) + ($\vec{v}_2,p_2$) := ($\vec{v}_1,p_1$) + ($\vec{\hat{v}}_2,p_1$) = ($\vec{v}_1+\vec{\hat{v}}_2,p_1$) Is this the correct conceptual approach or do I miss a property of the connection which makes it obvious how this should work now?",['differential-geometry']
1799286,Theorem 3.44 in Baby Rudin: Can we replace the coefficients with their absolute values?,"Here's Theorem 3.44 in the book Principles of Mathematical Analysis by Walter Rudin, third edition: Suppose the radius of convergence of $\sum c_n z^n $ is $1$, and suppose $c_0 \geq c_1 \geq c_2 \geq \cdots$, $\lim_{n\to\infty} c_n = 0$. Then $\sum c_n z^n$ converges at every point on the circle $\vert z \vert = 1$, except possibly at $z = 1$. Now I have the following couple of questions. (1) Can we replace the second condition in the above theorem by the following? Suppose $\vert c_0 \vert \geq \vert c_1 \vert \geq \vert c_2 \vert \geq \cdots$. Does the conclusion of the above theorem still hold? Let $\{c_n\}_{n\in\mathbb{N}}$ be a sequence of complex numbers; suppose the power series $\sum c_n z^n$ has radius of convergence equal to $R$, where $R$ can either be a non-negative real number or $+\infty$. Then we can we say $R$ is also the radius of convergence of the power series $\sum \vert c_n \vert z^n$. Am I right?","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'power-series', 'analysis']"
1799320,Differences between realizations and representation of a group,"I am studying an introduction to group representation theory on my relativity class' lecture notes. I've previously learned in other classes and also on the Wikipedia article that a representation $T$ of a group $G$ on a space $V$ is a group homomorphism between $G$ and the automorphism of $V$:$$T:G\rightarrow Aut(V)$$ if $V$ is a vector space then the representation will be $$T:G\rightarrow GL(V)$$ and $T$ will be the linear representation of $G$ At a certain point the author of these lecture notes says that Note that by representation of a group one actually means the set $V$ of objects $D(g)$ acts upon, where $D(G)$ is a realization of the group $G$. For example, for the Lorentz transformations, the matrices $\Lambda$ such that $\Lambda^T\eta\Lambda=\eta$ are the realization of the Lorentz transformation on the vectorial representation $V=\{v^{\mu}\}$ Instead on the Wikipedia article says: Some people use realization for the general notion and reserve the term representation for the special case of linear representations. I've also found here that what i call representation is for someone (probably also for the author of the notes) the realization . So i'm a little bit confused. Is there something wrong with my definition or i'm missing something?","['representation-theory', 'group-theory']"
1799331,Why does $\sin\phi=r\frac{d\theta}{ds}$ and $\cos\phi=\frac{dr}{ds}?$,"The relation between $p$ and $r$ where $p$ is the length of the perpendicular from the fixed point $O$ on the tangent to the curve at any point $P$ is called pedal equation of the curve. I want to find the radius of curvature of the pedal curves. Let $p=f(r)$ be the pedal equation. $\psi=\theta+\phi........(1)$ and $p=r\sin\phi..........(2)$ Differentiating $(1)$ with respect to $s$, $\frac{d\psi}{ds}=\frac{d\theta}{ds}+\frac{d\phi}{ds}.............(3)$ Differentiating $(2)$ with respect to $r$, $\frac{dp}{dr}=\sin\phi+r\cos\phi\frac{d\phi}{dr}$ $\frac{dp}{dr}=\sin\phi+r\cos\phi\frac{d\phi}{ds}\frac{ds}{dr}$ $\frac{dp}{dr}=r\frac{d\theta}{ds}+r\frac{dr}{ds}\frac{ds}{dr}\frac{d\phi}{ds}$ $\because \sin\phi=r\frac{d\theta}{ds}$ and $\cos\phi=\frac{dr}{ds}$ $\frac{dp}{dr}=r(\frac{d\theta}{ds}+\frac{d\phi}{ds})=r\frac{d\psi}{ds}$....from $(3)$ $\frac{dp}{dr}=r\frac{1}{\rho}$ We know that radius of curvature$=\rho=\frac{ds}{d\phi}$ $\therefore \rho=r\frac{dr}{dp}$ In this derivation i do not understand why does $\sin\phi=r\frac{d\theta}{ds}$ and $\cos\phi=\frac{dr}{ds}?$.Please help me.","['trigonometry', 'polar-coordinates', 'calculus']"
1799396,Does $A^2 \geq B^2 > 0$ imply $ACA \geq BCB$ for square positive definite matrices?,"Assume we have two $n \times n$ real nondegenerate matrices $ A^2 $ and $B^2$, such that
$$
A^2 \geq B^2 > 0,
$$
where ""$\geq$"" means positive semidefinite (Loewner) ordering. Does the following inequality holds for any real matrix $C$ 
$$
ACA \geq BCB \ ?
$$
If not, under which conditions on $C$ (or additional conditions on $A$ and $B$) does it holds? I would appreciate any ideas, suggestions, counterexamples.
Thanks!","['matrices', 'positive-definite', 'linear-algebra']"
1799411,May a 'ball' that has been 'cut off' still be called a 'ball'?,"Consider the metric subspace $[0,1] \subseteq \mathbb{R}$ with the metric defined in the usual sense, and the ball $B(0,1)$, defined to be the ball centred at $x=0$ with radius $1$. Now since only the right half of the ball 'exists' within our subspace, then can we still say that the ball $B(0;1)$ exists?","['general-topology', 'metric-spaces']"
1799471,Is really $f(x)=\int g(x) dx$ a function?,"I saw many of this kind of questions on some text/question books. Is there any other explanation of this, or is it really wrong as I thought? Here is a question of that kind: If $\displaystyle f(x)=\int x(x^2-a)^2 dx$ and $f(a)=7$ then $f(-a)=?$ Here what is $f(0)$ or $f(1)$? $\displaystyle f(0)=\int 0(0^2-a)^2 d0$ or $\displaystyle f(1)=\int 1(1^2-a)^2 d1$ does not make sense. For me, a right function need to be as: $\displaystyle  f_c(x)=\int_c^xt(t^2-a)^2dt$ (where $c$ is some constant, can be $0$ as usual).","['integration', 'functions', 'definition']"
1799495,identity on Pascal's triangle modulo 2,"Consider Pascal's triangle with entries modulo $2$, and let $(k,l)$ denote the $l$-th entry in the $k$-th row by $(k,l)$. Show that, for all $n \in \mathbb{N}$, each entry of the triangle with vertices $(0,0)$, $(2^n-1,0)$, $(2^n-1,2^n-1)$ is mapped via the translation $(k,l) \mapsto (2^n+k,l)$ to an equal entry (modulo $2$), i.e. the translation preserves the triangle entries modulo $2$. This explains the ""nice pattern"" of Pascal's triangle modulo $2$. Since Pascal's triangle simply shows the binomial coefficients, another way to state this is: Let $k,l \in \{0,\dots,2^n-1\}$ such that $l \leq k$, and let $n \in \mathbb{N}$. Then it holds $$\binom{k}{l} \equiv \binom{2^n+k}{l} \mod 2.$$ I am searching for an elementary proof for this (perhaps by induction, but I got stuck trying this).","['combinatorics', 'binomial-coefficients', 'modular-arithmetic']"
1799497,Functorial modifications of a topology,"Let $S$ be a set. Then there are only two ways to attach
functorialy a topology $\mathcal{T(S)}$ to it: The discrete and the trivial topology. Functorial means in this case that all maps $f \colon S \to S$ are continuous with respect to $\mathcal{T}(S)$. Now, let $X$ be a topological space with topology $\mathcal{T}(X)$. What topologies $\mathcal{T}'(X)$ can be attached to $X$ in a functorial way, i.e. such that all continuous maps $X \to X$ for $\mathcal{T}(X)$ are continuous for $\mathcal{T}'(X)$? More generally, what are the endofunctors $\mathcal{Top} \to \mathcal{Top}$ of the category of topological spaces that fix the underlying set of the objects and the morphisms (as maps of sets)? In other words, what are the endofunctors of $\mathcal{Top}$ that are functors over the forgetful functor $\mathcal{Top} \to \mathcal{Set}$? A proof for the statement in the beginning: Assume $\mathcal{T}(S)$ is not the trivial topology. Then there is a nonempty proper open subset $U \subset S$. Fix two points $x_1 \in U$, $x_2 \in S \setminus U$. For
an arbitrary set $V \subset S$, consider the ""characteristic function"" $f_V$:
$$f_V \colon S \to S$$
$$f_V(x) = \begin{cases}x_1 & x \in V \\ x_2 & x \not \in V \end{cases} $$
Because all functions $S \to S$ are continuous, $f_V$ is continuous, and $f_V^{-1}(U)=V$ is open. Because $V$ was arbitrary, every subset of $S$ is open, i.e. the topology is discrete.","['category-theory', 'general-topology']"
1799499,Why does this approximation work (and why does it fail)?,"I have a function $$f(x)=\frac{e^{-x}}{x}$$ and I am trying to find an expression for the inverse function $f^{-1}(x)$. So far I have come up with the approximation: $$\hat{f}^{-1}(x)=\left( \left( x+v \right )^e-v^e \right)^{-\frac{1}{e}}$$ where $v \approx 0.83$ This formula is a good approximation for $x>0$. I basically came up with this formula from tinkering, but I'm not exactly sure what the relation is between the e-th root of an expression and e to the power of said expression. Does it have anything to do with the limit definition of e: $$ \lim_{n \rightarrow \infty}{\left (1+\frac{1}{n} \right)^n} $$ However, this is just an approximation — an approximation that fails. Different values of $v$ offer better approximations, and I think that $v$ is related to $e$ in some way, however I'm not sure what way that is. Why does my function come so close, but also fail? Is this just a special case? Here is a diagram showing $f^{-1}(x)$ (red) and $\hat{f}^{-1}(x)$ (blue).","['algebra-precalculus', 'calculus']"
1799529,Representing every positive rational number in the form of $(a^n+b^n)/(c^n+d^n)$,"About a month ago, I got the following : For every positive rational number $r$, there exists a set of four positive integers $(a,b,c,d)$ such that 
  $$r=\frac{a^\color{red}{3}+b^\color{red}{3}}{c^\color{red}{3}+d^\color{red}{3}}.$$ For $r=p/q$ where $p,q$ are positive integers, we can take 
  $$(a,b,c,d)=(3ps^3t+9qt^4,\ 3ps^3t-9qt^4,\ 9qst^3+ps^4,\ 9qst^3-ps^4)$$
  where $s,t$ are positive integers such that $3\lt r\cdot(s/t)^3\lt 9$. For $r=2014/89$, for example, since we have $(2014/89)\cdot(2/3)^3\approx 6.7$, taking $(p,q,s,t)=(2014,89,2,3)$ gives us $$\frac{2014}{89}=\frac{209889^3+80127^3}{75478^3+11030^3}.$$ Then, I began to try to find every positive integer $n$ such that the following proposition is true : Proposition : For every positive rational number $r$, there exists a set of four positive integers $(a,b,c,d)$ such that $$r=\frac{a^\color{red}{n}+b^\color{red}{n}}{c^\color{red}{n}+d^\color{red}{n}}.$$ The followings are what I've got. Let $r=p/q$ where $p,q$ are positive integers. For $n=1$, the proposition is true. We can take $(a,b,c,d)=(p,p,q,q)$. For $n=2$, the proposition is false . For example, no such sets exist for $r=7/3$. For even $n$, the proposition is false because the proposition is false for $n=2$. However, I've been facing difficulty in the case of odd $n\ge 5$. I've tried to get a similar set of four positive integers $(a,b,c,d)$ as the set for $n=3$, but I have not been able to get any such set. So, here is my question. Question : How can we find every odd number $n\color{red}{\ge 5}$ such that the following proposition is true? Proposition : For every positive rational number $r$, there exists a set of four positive integers $(a,b,c,d)$ such that $$r=\frac{a^n+b^n}{c^n+d^n}.$$ Update : I posted this question on MO . Added : Problem N2 of IMO 1999 Shortlist asks the case $n=3$.","['number-theory', 'perfect-powers', 'rational-numbers']"
1799559,Integration theorem: enough assumptions?,"Let $f:[a,b]\to\mathbb{R}$ be a continious function. Show that if $$\int_a^b f(x)g(x)dx=0$$
  for all continious functions $g:[a,b]\to\mathbb{R}$ with $g(a)=g(b)=0$, then  $f(x)=0$ $\forall x\in[a,b]$ I have difficulties proving this question. Consider for example $g(x)=0$ $\forall x\in[a,b]$, then assumptions still hold but $f(x)$ can be anything (f.i. $f(x)=1\neq0$). Could anyone tell me if this reasoning is correct?","['integration', 'calculus']"
1799581,"Weyl group, bilinear form, and character/cocharacter pairing. Many questions!","Let $G$ be a connected linear algebraic group, $T$ a maximal torus of $G$, and $\alpha$ a weight of $T$ such that $G_{\alpha} = Z_G(S)$ is not solvable, where $S = (\textrm{Ker } \alpha)^0$.  I have been really confused about a few things concerning the Weyl group $W(G,T) = N_G(T)/Z_G(T)$, which injects into the group of automorphisms of the group of characters of $T$, as $(nZ_G(T) \cdot \chi)(t) = \chi(ntn^{-1})$. I had asked a question earlier ( Why is $s_{\alpha}$ a Euclidean reflection? ) and user yisishoujo had given me a detailed answer.  Unfortunately I was not able to understand all of his answer, although most of it was very helpful.  I feel like there is something very subtle about this topic which everyone else realizes but that I am not aware of.  So I want to break this question into several questions in the hope someone can point out to me what it is I'm missing. Notation: $X, Y$ are the groups of characters/cocharacters of $T$. $V = \mathbb{R} \otimes_{\mathbb{Z}} X$ $V ^\wedge{} = \mathbb{R} \otimes_{\mathbb{Z}} Y$. We can identify $X$ as an additive subgroup of $V$ as $\chi \mapsto 1 \otimes \chi$.  Similarly for $Y$ and $V^{\wedge}$. Relevant facts: 1 . There is a canonical pairing $\langle -, -\rangle:X \times Y \rightarrow \mathbb{Z}$, where for $\chi \in X, \gamma \in Y$, $\langle \chi, \gamma \rangle$ is the unique integer for which $\chi \circ \gamma(x) = x^{\langle \chi, \gamma \rangle}$ for all $x \in k^{\ast}$. 2 . If $\gamma \in Y$, $\gamma$ induces an abelian group homomorphism of $X$ into $\mathbb{Z}$ by $\chi \mapsto \langle \chi, \gamma \rangle$.  The assignment $\gamma \mapsto \langle -, \gamma \rangle$ is an isomorphism of abelian groups $Y \rightarrow \textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$.  We identify $Y$ with $\textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$. 3 . Identifying $Y$ as above, there is a canonical isomorphism of real vector spaces from $V^{\wedge}$ onto the dual of $V$, where to any generator $\lambda \otimes \gamma$, we associate the map $V \rightarrow \mathbb{R}$ defined on generators by $r \otimes \chi \mapsto r \lambda \langle \chi, \gamma \rangle$. 4 . Any abelian group automorphism of $X$ extends to a linear isomorphism of $V$, the same for $Y$ and $V^{\wedge}$.  The subgroup of the Weyl group, $W_{\alpha} := Z_G(S) \cap N_G(T)/Z_G(T)$, can be shown to have order exactly two.  Choosing an $n \in Z_G(S) \cap N_G(T)$ which is not in $Z_G(T)$, we obtain an automorphism $s$ of $V$ which has order exactly $2$. 5 .  The Weyl group also injects into the group of automorphisms of $Y$, as $(nZ_G(T) \cdot \gamma)(x) = n^{-1}\gamma(x)n$.  Under this action, we have $\langle \chi, \gamma \rangle = \langle w \cdot \chi, w \cdot \gamma \rangle$ for any $w$ in the Weyl group, $\chi \in X, \gamma \in Y$.  The actions of the Weyl group on $X, Y$ extend respectively to actions on $V, V^{\wedge}$.  The pairing $\langle -, - \rangle$ extends to a Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ Questions: 1 . Why does $s(\alpha) \neq \alpha$? I know it is not the case that $s(\chi) = \chi$ for all characters $\chi$, but it is not clear what happens to $\alpha$.  If I can show that $s(\alpha) \neq \alpha$, yisishoujo explained why we must have $s(\alpha) =- \alpha$.  Indeed, conjugation by $n$ (the $n$ as in Fact 4) induces an automorphism of algebraic groups $T/S$.  But $T/S$ is isomorphic to $k^{\ast}$, so there are only two such automorphisms, the identity map and the inverse map.  If $\textrm{Int } n$ induced the identity, then we would have $ntn^{-1}S = tS$ for all $t \in T$, i.e. $\alpha(ntn^{-1}) = \alpha(t)$ for all $t$, which implies $s \alpha = \alpha$, contradiction.  It follows that $ntn^{-1}S = t^{-1}S$ for all $t$, whence $s(\alpha) = - \alpha$. 2 .  Take $s, n$ as in Fact 4.  We can think of $s$ as an automorphism on $Y$, even of $V^{\wedge}$, as in Fact 5. Why does there exist a cocharacter $\gamma$ such that $s \cdot \gamma = - \gamma$? Here is my attempt at a proof.  We know $s$ extends to an automorphism of $V^{\wedge}$, and still $s^2 = 1_{V^{\wedge}}$.  Choosing a basis for $Y$, we can think of $s$ as a matrix with entries in $\mathbb{Z}$.  It follows that the only eigenvalues of $s$ as $\pm 1$.  If I could show $-1$ was an eigenvalue, then the nullspace of $s + 1_{V^{\wedge}}$ would be nontrivial.  Since solving a system of linear equations doesn't depend on the ambient field, we would find a solution with entries in $\mathbb{Z}$, i.e. a solution $\gamma \in Y$ with $s \cdot \gamma = - \gamma$.  But why must $-1$ be an eigenvalue? 3 .  Take $s, n$ as in Question 2. Supposing that Question 2 has been answered, is it possible to choose $\gamma \in Y$ such that $s \cdot \gamma = -\gamma$ as well as such that $\langle \alpha, \gamma \rangle \neq 0$? yisishoujo explains, given such a cocharacter, how to show the following: ($\ast$) If $\chi$ is a character of $T$, and $\langle \chi, \lambda \rangle = 0$, then $s \cdot \chi = \chi$. 4 .  The Weyl group acts as a group of automorphisms of $V$ (Fact 5).  By the standard averaging process, one can show there exists a symmetric, positive define bilinear form on $V$ which is invariant under the action of the Weyl group $(s \cdot v, s \cdot w) = (v,w)$ for $s$ in the Weyl group, $v, w \in V$. Is such a bilinear form unique up to a scalar? 5 .  Similar to question 4, we identified a canonical Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ as in Facts 1, 5. Is such a pairing unique up to scalar? 6 . What is the relationship between a Weyl group-invariant form on $V$ and the canonical pairing $V \times V^{\wedge}$?  Can they in any sense be 'identified?'  If so, does any possible nonuniqueness of the form on $V$ (as in Question 4) come into play? I know that GIVEN a Weyl group invariant form $(-,-)$ on $V$, there is a isomorphism of $V$ into its dual by sending $v$ to the functional $(v,-)$.  Composing that with the isomorphism identified in Fact 3, we get an isomorphism $V \rightarrow V^{\wedge}$.  This isomorphism induces an action of the Weyl group on $V^{\wedge}$ (which I do not believe is necessarily the canonical action as in Fact 5, however).  Letting $\Phi$ be the inverse map $V^{\wedge} \rightarrow V$, we get a Weyl group invariant pairing $\langle -,-\rangle_1 V \times V^{\wedge} \rightarrow \mathbb{R}$ by setting $\langle v, \eta \rangle_1 = (v, \Phi(\eta))$. And finally, the main question which all of this is intended to be put together to answer: given any Weyl group invariant bilinear form $(-,-)$ on $V$, and given $n,s$ as in Fact 4, why is it the case that $s$ is a Euclidean reflection of $V$ on $\alpha$?  In other words, why is it the case that $s(\alpha) =- \alpha$?  And if $v$ is any element of $V$ with $(\alpha, v) = 0$, why is it the case that $s \cdot v = v$? This main question which has been so difficult for me is an offhand comment in Springer, Linear Algebraic Groups , 7.17.  User yisishoujo indicated that the result $s \cdot v = v$ for all $v$ satisfying $(\alpha, v) = 0$ is a result of the statement $(\ast$) given in Fact 3, but I do not understand why.  Maybe I need to understand the relationship between the bilinear form and the pairing (Fact 6) better.  Thank you for your time.","['algebraic-groups', 'lie-algebras', 'linear-algebra', 'algebraic-geometry']"
1799613,Arithmetic genus of divisors on cubic surface,"This is a question from Hartshorne's Algebraic Geometry (Chapter V.4). It asks to show that for any divisor $D$ of degree $d$ on a smooth cubic surface $X$ in $\mathbb{P}^3$ the following inequality is satisfied:
$$
p_a(D) \leq
\left\{\begin{array}{ll}
\frac{1}{6}(d-1)(d-2) & d\equiv 1,2\mbox{ (mod $3$)}\\
\frac{1}{6}(d-1)(d-2) + \frac{2}{3} & d\equiv 0\mbox{ (mod $3$)}
\end{array}\right.
$$
where $p_a(D)$ is the arithmetic genus of $D$ defined for an arbitrary divisor $D$ by the adjunction formula ($2p_a(D) - 2 = D\cdot (D+K)$). Now, the adjunction formula has $p_a(D) = 1 + \frac{D^2 + D\cdot K}{2}$ and I can determine $D\cdot K$ as follows. Regarding $X$ as the blow up of $\mathbb{P}^2$ at 6 points, we have $D \equiv_{\text{lin}} aH - \sum b_iE_i$ for some $a_i, b_i \in \mathbb{Z}$ (where $H$ is the pullback of a line in $\mathbb{P}^2$ and the $E_i$ are the exceptional divisors). Then $K \equiv -3H + \sum E_i$ which means $D\cdot K = -3a + \sum b_i$. But $3H - \sum E_i = -K$ is the new hyperplane divisor of $X$ in $\mathbb{P}^3$ so $D\cdot K = -d$. So now we have $p_a(D) = 1 - \frac{d}{2} + \frac{D^2}{2}$. To complete this, I clearly need to compare $D^2$ and $d$. Working backwards, it's not hard to see that what I need is
$$
D^2 \leq \left\{\begin{array}{cl}\frac{d^2 - 4}{3} & d\equiv 1,2\mbox{ (mod $3$)}\\
\frac{d^2}{3} & d\equiv 0\mbox{ (mod $3$)}
\end{array}\right.
$$
but I can't see how to get this. All I can say is that $D^2 = a^2 - \sum b_i^2$ and 
$$
\begin{array}{rcl}
d^2 & = & (3a - \sum b_i)^2 = 9a^2 + (\sum b_i)^2 - 6a\sum b_i\\
& = & 9a^2 + (\sum b_i)^2 - 6a(3a-d)\\
& \leq & -9a^2 + 6\sum b_i^2 + 6ad \mbox{ (because $(\sum b_i)^2 \leq 6\sum b_i^2$ by Schwarz's inequality)}\\
& = & -6D^2 - 3a^2 + 6ad\\
& = & -6D^2 - 3(a-d)^2 - 3d^2\\
& \leq & -6D^2 - 3d^2
\end{array}
$$
but this gives me $D^2 \leq -\frac{2}{3}d^2$ which seems to be way stronger than what I want (and thus probably not true). I just can't see what I might have done wrong and what is the correct way to compare $D^2$ and $d$. Can anyone see it?",['algebraic-geometry']
1799642,"Example of two affine varieties $X,Y$ such that the image of $\phi:X \rightarrow Y$ is not locally closed","In my course Algebraic Geometry I always find it hard to come up with examples or counterexamples. For instance in the following question: Give an example of two affine varieties $X,Y$ and a morphism $\phi:X \rightarrow Y$ such that the image of $\phi$ is not locally closed in $Y$. We defined locally closed here as: A subset $Z$ of a topological space is called locally closed if $Z$ is the intersection of an open subset and a closed subset of $X$.","['general-topology', 'algebraic-geometry']"
1799706,When does a matrix admit a Jordan canonical form?,"If a matrix over the field $\mathbb R$ has as elementary divisors: $x-4$, $x^2 + 2$, does it then admit a Jordan canonical form? Am I right in thinking that a matrix has a Jordan canonical form only when its elementary divisors are all of the form $(x-a)^n$? sorry if my question is too basic. I am learning these stuff on my own and I am having some difficulty understanding them","['jordan-normal-form', 'linear-algebra']"
1799709,Entire function $f$ such that $\lim\limits_{z\rightarrow \infty}f(z)=0$ and $f(0)=1$?,"The question is this: Does there exist an entire function $f$ such that $\lim_{z\rightarrow \infty}f(z)=0$ and $f(0)=1$. I immediately would point to $f(z)=e^{-z}$. It is entire and satisfies the above. However my professor gives this answer: No. Since entire functions are bounded on bounded sets, the condition $\lim_{z\rightarrow \infty}f(z)=0$ implies that $f$ is bounded on the whole complex plane. Then by Liouville's Theorem, $f(z)=f(0)$ and $\lim_{z\rightarrow \infty}f(z)=1\neq 0.$ Who is correct? Maybe the question is wrong and it is supposed to have the limit for $\pm \infty$?","['complex-analysis', 'exponential-function']"
1799712,Let $z \in\Bbb{C}^{\times}$ such that $|z^3+\frac{1}{z^3}|\leq 2$. Prove that $|z+\frac{1}{z}|\leq 2$.,"Problem : Let $z \in\Bbb{C}^{\times}$ such that $|z^3+\frac{1}{z^3}|\leq 2$ . Prove that $|z+\frac{1}{z}|\leq 2$ . My approach : Since : $(a^3+b^3)=(a+b)^3-3ab(a+b)$ $\Rightarrow z^3+\frac{1}{z^3}=(z+\frac{1}{z})^3-3(z+\frac{1}{z})$ Now I don't know further about this problem whether or not this will help here. Please guide to solve this problem will be of great help, thanks.","['algebra-precalculus', 'cubics', 'inequality', 'complex-numbers']"
1799741,"Verifying that the weighted projective space $\Bbb{P}_{\Bbb{Q}}(1,1,2,3)$ is singular.","I while ago I attended a talk that was somewhat over my head, and the speaker mentioned in passing that the weighted projective space $\Bbb{P}:=\Bbb{P}_{\Bbb{Q}}(1,1,2,3)$ is singular. I suppose this means that there is a point $P\in\Bbb{P}$ for which
$$\dim\mathcal{O}_{\Bbb{P},P}\neq\dim_{k(P)}\mathfrak{m}_P/\mathfrak{m}_P^2,$$
where $\mathfrak{m}_P$ is the maximal ideal of $\mathcal{O}_{\Bbb{P},P}$. I believe $P$ need not be a $\Bbb{Q}$-point, but please correct me if I'm wrong. Unfortunately I have no clue how to check this claim; how would I find such a singular point? Checking this (in)equality for a point $P\in\Bbb{P}$ is very cumbersome (I have tried for a handful of points, to no avail), and it is obviously impossible to check all points $P\in\Bbb{P}$. Is there an easy way to check the (in)equality above? Or some way to narrow down the number of candidates for singular points (drastically)? Thanks in advance for any answers or pointers as to where to look for them.","['singularity', 'projective-space', 'algebraic-geometry']"
1799745,Laurent series of $\frac{e^z}{z^2+1}$,I cant  figure out the laurent series of the following function. Let $f(z)= \frac{e^z}{z^2+1} $ and $|z|\gt 1$ $$\frac{1}{z^2+1}=\sum_{n=0}^{\infty}(-1)^nz^{-2n-2}$$ and $$e^z = \sum_{n=0}^{\infty}\frac{z^{n}}{n!}$$ $$e^z*\frac{1}{z^2+1} =\sum_{n=0}^{\infty}\sum_{k=0}^{n}(-1)^nz^{-2n-2}*\frac{z^{n-k}}{(n-k)!}$$ How can I go from here ?,"['laurent-series', 'complex-analysis', 'cauchy-product', 'power-series', 'sequences-and-series']"
1799766,Relation between counting measure and Tonelli theorem,"This is from Rudin's RCA book. But I can't understand how he got Corollary. What he takes as $f_n, X$? If we consider counting measure how integral converts to sum? I can't show this after some attempts. Can anyone give a detailed answer please? I would be grateful for answer.","['real-analysis', 'measure-theory']"
1799767,63% chance of event happening over repeated attempts,"This question is of interest: If there is a $1 / x$ chance of something happening, in $x$ attempts, for
  large numbers over $50$ or so, the likelihood of it happening is about
  $63\%$. If there's a $1$ in $10\,000$ chance of getting hit by a meteor if you
  go outside, if you go outside $10\,000$ times, you have a $63\%$ chance of
  getting hit with a meteor at some point. If there's a $1$ in a million
  chance of winning the lottery and you buy a million (random) lottery
  tickets, you have a $63\%$ chance of winning. What is the main idea underlying this property?","['recreational-mathematics', 'probability']"
1799777,How to get a Presentation of a Group,"$\newcommand{\R}{\mathbf R}$ Let $G$ be the group of homeomorphisms of $\R^2$ generated by $g$ and $h$, where $g(x, y)=(x+1, y)$ and $h(x, y)=(-x, y+1)$. To show that $G\cong \langle a, b|\ b^{-1}aba\rangle$. I tried the following: Define a map $f:\langle a, b\rangle \to G$ which sends $a$ to $g$ and $b$ to $h$. Then it can be checked that $b^{-1}aba$ lies in the kernel of $f$.
So $f$ factors through $\langle a, b|\ b^{-1}aba\rangle$ to give a map $\bar f: \langle a, b|\ b^{-1}aba\rangle\to G$. What I am unable to show is that $\bar f$ is injective. Also, here we were already given a presentation which we had to show is isomorphic to $G$. If it were not given, then is there a general way to get one? Thank you.","['group-theory', 'free-groups']"
1799798,Solution of partial differential equation,"Solve the differential equation,
$$
z=\frac{\partial z}{\partial x}x + \frac{\partial z}{\partial y}y+ (\frac{\partial z}{\partial x})^2 + \frac{\partial z}{\partial x}\frac{\partial z}{\partial y}+  (\frac{\partial z}{\partial y})^2.$$ Can you please solve and explain the solution assuming that I don't have any idea.","['partial-derivative', 'ordinary-differential-equations', 'implicit-differentiation', 'partial-differential-equations']"
1799804,A counterexample to $x^n + y^n = h^2 + nf^2$ implies $x + y = h'^2 + nf'^2$ in the integers,"The Wikipedia page for Sophie Germain contains the following: In the same 1807 letter, Sophie claimed that if $x^n + y^n$ is of the form $h^2 + nf^2$, then $x + y$ is also of that form. Gauss replied with a counterexample: $15^{11} + 8^{11}$ can be written as $h^2 + 11f^2$, but $15 + 8$ cannot. WolframAlpha confirms that the equation $15^{11} + 8^{11} = h^2 + 11f^2$ has two solutions in the positive integers, $(h, f) = (935166, 841201)$ and $(h, f) = (1595826, 745391)$, and it is obvious that $15 + 8 = h^2 + 11f^2$ has no solutions in the integers. However, how would one be able to confirm that $15^{11} + 8^{11} = h^2 + 11f^2$ has solutions in the positive integers without such a computational aid? In a related question, how might Gauss have come up with this counterexample in the first place?",['number-theory']
1799840,"Integral with arithmetic-geometric mean ${\large\int}_0^1\frac{x^z}{\operatorname{agm}(1,\,x)}dx$","The arithmetic-geometric mean $^{[1]}$ $\!^{[2]}$ of positive numbers $a$ and $b$ is denoted $\operatorname{agm}(a,b)$ and defined as follows: 
$$\text{Let}\quad a_0=a,\quad b_0=b,\quad a_{n+1}=\frac{a_n+b_n}2,\quad b_{n+1}=\sqrt{a_n b_n}.$$
$$\text{Then}\quad\operatorname{agm}(a,\,b)=\lim_{n\to\infty}a_n=\lim_{n\to\infty}b_n.\tag1$$
It can be expressed $^{[3]}$ in terms of the complete elliptic integral of the $1^{\text{st}}$ kind $^{[4]}$ $\!^{[5]}$ :
$$\operatorname{agm}(a,\,b)=\frac\pi4\cdot\frac{a+b}{{\bf K}\!\left(\frac{a+b}{a-b}\right)}.\tag2$$ It appears that
$$\int_0^1\frac{x^z}{\operatorname{agm}(1,\,x)}dx\stackrel{\color{gray}?}=\frac{\Gamma\!\left(\frac z2+\frac12\right)}{2\,\Gamma\!\left(\frac z2+1\right)},\quad\forall z\in\mathbb C,\,\Re(z)>-1.\tag3$$
How can we prove it?","['calculus', 'closed-form', 'integration', 'definite-integrals', 'elliptic-integrals']"
1799854,Is the $L$ in $LU$ factorization unique?,"I was doing an $LU$ factorization problem
\begin{bmatrix}
    2       & 3 & 2  \\
    4       & 13 & 9  \\
-6 & 5 &4
\end{bmatrix}
and I was going to multiply the second row by .$5$ and subtract the result from row $1$, then do something similar to row reduce row $3$. My book tells me that you first row-reduce to get $U$, then you extract certain columns from various steps in the reduction process to get $L$. That's when I realized that if I instead multiplied row $1$ by $2$ subtracted from one times row $2$, this would be equivalent to getting $U$, but it would change the resultant $L$ matrix! What is wrong with this approach?","['matrices', 'matrix-decomposition', 'lu-decomposition', 'numerical-methods', 'linear-algebra']"
1799914,Distribution for function,"I would like a good book to study distribution or generalized functions like the ""Basic idea"" of that Wiki page . Is there anyone could give me some good book references in this domain? Thanks!","['distribution-theory', 'functions']"
1799916,Distribution of residual term in regression.,In regression analysis for classical linear regression model the residual term is independent of x and y and normally distributed and it is a random variable but i found somewhere written u~N and u~NID.I cannot understand the difference so can someone explain the meaning of NID (normally and independently distributed)?(sorry for my bad English).,"['statistics', 'normal-distribution']"
1799921,How do I show that a contraction mapping in a metric space is continuous?,"I start out by letting $V$ be an arbitrary open set in $X$.  Then 
$$
f^{-1}(V) = \{x\in X\mid f(x) \in B_\epsilon(f(a))\}.
$$ This can be re-written as:
$$
f^{-1}(V) = \{x\in X\mid d(f(a), f(x)) < \epsilon \}.
$$  I realize that contraction mappings have an $0<r<1$ such that 
$$
d(f(x_1), f(x_2)) \leq r\cdot d(x_1,x_2),\quad \forall x_1,x_2 \in X.
$$  I construct an open ball 
$$
B_{\frac{\delta}{r}}(a) = \{x\in X\mid r\cdot d(a, x) \lt \delta \}
$$ but from here I'm unsure as to how to show that $f^{-1}(V)$ is open.","['general-topology', 'real-analysis', 'metric-spaces']"
1799954,Shortest proof for showing $\mathbb{Z}[\frac{1+\sqrt{-19}}{2}]$ is a PID.,"I'm looking for an easy proof for that $\mathbb{Z}[\frac{1+\sqrt{-19}}{2}]$ is a PID. One proof I know is to show that the field norm is a Dedekind-Hasse norm, but this proof is quite dirty( that it proves case by case) and technical. Since any ring is a PID iff it admits a Dedekind-Hasse norm, I think every proof would look similar to the one I know. However, since this problem has been on qualifying exam quite several times , I'm curious if there is one which is less technical. Thank you in advance.","['abstract-algebra', 'alternative-proof', 'principal-ideal-domains']"
1799958,how to verify $\frac{\sin(x)\cos(x)}{\cos^2(x)-\sin^2(x)}=\frac{\tan(x)}{1-\tan^2(x))}$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How would I verifty the following trig identity? $$
\frac{\sin(x)\cos(x)}{\cos^2(x)-\sin^2(x)}=\frac{\tan(x)}{1-\tan^2(x)}
$$ I am not sure how to start.","['algebra-precalculus', 'trigonometry']"
1799974,closed $1$-form that is exact,"Let $\alpha=\sum_{i=1}^n f_idx_i$ be a closed $1$-form defined on all of $\mathbb{R}^n$. Verify that the function $g(\textbf{x})=\sum_{i=1}^nx_i\int_0^1f_i(t\textbf{x})dt$ satisfies $dg=\alpha$. Proof. we must show that $\frac{\partial g}{\partial x_i}=f_i$. Then \begin{align}
\frac{\partial g}{\partial x_j}(\textbf{x})&=\frac{\partial }{\partial x_j}\left(\sum_{i=1}^n x_i\int_0^1f_i(t\textbf{x})dt\right)\\
&=\sum_{i=1}^n \frac{\partial }{\partial x_j}\left( x_i\int_0^1f_i(t\textbf{x})dt\right)\\
&=\sum_{i=1}^n\left( \frac{\partial }{\partial x_j}x_i\int_0^1f_i(t\textbf{x})dt+x_i\frac{\partial }{\partial x_j}\int_0^1f_i(t\textbf{x})dt\right)\\
&=\sum_{i=1}^n\left(\frac{\partial }{\partial x_j}x_i\int_0^1f_i(t\textbf{x})dt\right)+\sum_{i=1}^nx_i\int_0^1\frac{\partial}{\partial x_j}f_i(t\textbf{x})dt\\
&=\int_0^1f_j(t\textbf{x})dt+\sum_{i=1}^nx_i\int_0^1\frac{\partial}{\partial x_j}f_i(t\textbf{x})dt
\end{align} I don't know how to continue",['differential-geometry']
1800023,"Show that the sequence $x_n=\Big[\frac32x_{n+1}\Big]$, $x_1=2$ contains an infinite set of odd numbers and an infinite set of even numbers.","I am having a hard time proving this Let $x_n$ be a sequence such that $x_{n+1}=\Big[\frac32x_n\Big]$
  for $n\gt1$, where $[x]$ denotes the nearest integer function and $x_1=2$. Show that this sequence has an infinite set of odd numbers and an infinite set of even numbers. I think the best way to approach it is by contradiction. That is, assume that the sequence contains a finite number of odds, let that be $$S=\{x_{k_1},x_{k_2},\cdots x_{k_m}\}$$ Thus, all $x_{k_{m+i}}$ terms must be even numbers. But $x_{k_{m+1}}=2l$ for $l\in\Bbb{Z}$, so $$x_{k_{m+2}}=\Big[\frac322l\Big]=\Big[3l\Big]$$ And since $[3l]$ is in $\Bbb{Z}$ for all $l\in\Bbb{Z}$ it follows that all $x_{k_j}$ will be of the form $[3t]$, $t\in \Bbb{Z}$. If we can show that there are infinite number of odds of this form, I believe that we are done. But that is not hard to show, since $\forall t$ odd, $[3t]$ is also odd. I am having strong doubts though about how rigorous this is, I fear I am missing something crucial here. Any ideas are welcome.","['recurrence-relations', 'sequences-and-series', 'proof-verification', 'elementary-number-theory']"
1800026,"If $f(\frac{x+y}{2})=\frac{f(x)+f(y)}{2}$, then find $f(2)$","Let $f(\frac{x+y}{2})=\frac{f(x)+f(y)}{2}$ and it is given that $f(0)=1$ and $f'(0)=-1$, where $f'$ denotes first derivative. Find the value of $f(2)$ Could someone tell me how to use $f'(0)=-1$ here? I am not able to use this information.","['calculus', 'functions']"
1800042,"Integrate $\int_0^\infty \frac{e^{-x/\sqrt3}-e^{-x/\sqrt2}}{x}\,\mathrm{d}x$","I can't solve the integral
$$\int_0^\infty \frac{e^{-x/\sqrt3}-e^{-x/\sqrt2}}{x}\,\mathrm{d}x$$
I tried it by using Beta and Gamma function and integration by parts. Please help me to solve it.","['calculus', 'closed-form', 'improper-integrals', 'integration', 'definite-integrals']"
1800056,How to solve $f'(x)=f'(\frac{x}{2})$,How do we solve this given $f'(0)=-1$. It does not look separable. I can integrate both sides but end up with a functional equation with is not helpful.,"['ordinary-differential-equations', 'calculus', 'functional-equations']"
1800065,"Given a symmetric positive-definite matrix $M$, find all $A$ such that $A^\top M A=M$","Given $M$ a real symmetric positive-definite matrix, I would like to characterise all matrices $A$ such that $A^\top M A=M$. Note that the question of finding $A$ solutions to $A^\top M A=M$ for all positive-definite matrices $M$ has already been answered here . Necessarily $\det(A)^2=1$. Reducing the problem to diagonal p.q. matrices $M$ is symmetric, so diagonalisable in an orthonormal basis: $M=Q^\top D Q$ with $D$ diagonal and $QQ^\top=I$. Then, \begin{align}
&A^\top Q^\top D Q A=Q^\top D Q \\
&(QA)^\top D(QA)=Q^\top D Q \\
&(QAQ^\top)^\top D (QAQ^\top)=D
\end{align} so I think the problem can be simplified in $$\text{Find the matrices $B$ such that $B^\top D B=D$}$$
  where $D=\operatorname{diag}(d_1,\dots,d_n)$, $d_i>0$. and then recover $A$ from $A=Q^\top B Q$ ($Q$ and $D$ are known since $M$ is given). The $d_i>0$ stems for $M$ positive-definite. Trying to solve this problem The $2^n$ matrices $B=\operatorname{diag}(\pm 1,\dots, \pm 1)$ are obvious solutions. What about the other ones? Writing $B^\top D B=D$ in components, the term $(i,j)$ is given by
$$\sum_k B_{k,i}d_k B_{k,j}=d_i\delta_{ij}$$
In particular, 
$$\sum_k B_{k,i}^2d_k =d_i.$$
However, I don't see any clear conclusions using these equalities. Question : How can to find/characterise the $B$ such that $B^\top D B=D$?","['matrices', 'matrix-equations', 'linear-algebra']"
1800099,Determine the composition of the functions $f(x)=4x+3$ and $g(x)=-5x^2+1$,"Answer: 
\begin{align*}
(f \circ g)(x) & = f(g(x))\\
               & = 4(-5x^2+1)+3\\
               & = -20x^2+8+3\\
               & = -20x^2+11
\end{align*} \begin{align*}         
(g \circ f)(x) & = g(f(x))\\
               & = -5(4x+3)^2+1\\
               & = -5[(4x+3)(4x+3)]+1\\
               & = -5(16x^2+24x+9)+1\\
               & = -80x^2-19x-45+1\\
               & = -80x^2-19x-44
\end{align*} Is my working out and answer correct?",['functions']
1800122,"Prob. 4 (a), Sec. 20, in Munkres' Topology, 2nd ed: Are these functions continuous in the product, uniform, and box topologies?","Here is Prob. 20 (a) in the book Topology by James R. Munkres, 2nd edition. Consider the product, uniform, and box topologies on $\mathbb{R}^\omega$. In which of these topologies are the following functions from $\mathbb{R}$ to $\mathbb{R}^\omega$ continuous? 
  $$f(t) = \left( t, 2t, 3t, \ldots \right),$$
  $$g(t) = \left( t, t, t, \ldots \right),$$
  $$h(t) = \left( t, \frac{t}{2}, \frac{t}{3}, \ldots \right)$$
  for each $t \in \mathbb{R}$. Let $U \colon= \prod_{n \in \mathbb{N}} U_n$ be a product topology basis element for $\mathbb{R}^\omega$; let $n_1, \ldots, n_k$ be the natural numbers such that $U_{n_i}$ is a proper open subset of $\mathbb{R}$ for each $i = 1, \ldots, k$; suppose $U_n = \mathbb{R}$ for all other $n \in \mathbb{N}$. Suppose $t \in f^{-1}(U)$ so that $f(t) \in U$ and hence $$\pi_{n_i}\left(f(t) \right) = n_i t \in  U_{n_i}$$ for each $i = 1, \ldots, k$. Here, for each $n \in \mathbb{N}$, the map  $\pi_n \colon \mathbb{R}^\omega \to \mathbb{R}$ is defined by $$\pi_n(x) \colon= x_n \ \mbox{ for all } \ x \colon= \left( x_1, x_2, x_3, \ldots \right) \in \mathbb{R}^\omega.$$
Since $U_{n_i}$ is open in $\mathbb{R}$ under the usual topology, there exists a positive real number $\delta_i$ such that 
$$\left( \pi_{n_i}\left( f(t) \right) - \delta_i, \pi_{n_i}\left( f(t) \right) + \delta_i \right) = \left( n_i t - \delta_i , n_i t + \delta_i \right) \subset U_{n_i}$$ for each $i = 1, \ldots, k$. 
Let $$\delta \colon= \min \left\{ \frac{\delta_1}{n_1}, \ldots, \frac{\delta_k}{n_k} \right\}. $$ This $\delta$ is a positive real number, and if $s \in (t- \delta, t+\delta)$, then $$ t - \frac{ \delta_{n_i} }{ n_i } < s < t + \frac{ \delta_{n_i} }{ n_i },$$ which implies that $$n_i t - \delta_i <  n_i s < n_i t + \delta_i,$$ which in turn implies that $$n_i s \in U_{n_i},$$ for each $i = 1, \ldots, k$, and hence $f(s) \in U$. Thus, for each $t \in f^{-1}(U)$, there is a positive real number $\delta$ such that $$t \in (t- \delta, t + \delta) \subset f^{-1}(U),$$ showing that the inverse image $f^{-1}(U)$ is open for each basis element $U$ of the product topology on $\mathbb{R}^\omega$. Now suppose that $t \in g^{-1}(U)$. Then, for each $i = 1, \ldots, k$, 
$$\pi_{n_i}\left( g(t) \right) = t \in U_{n_i}$$
and hence $$\left( t - \delta_i, t+\delta_i \right) \subset U_{n_i}$$ for some positive real number $\delta_i$. Let $$\delta \colon= \min \left\{ \delta_1, \ldots, \delta_k \right\}.$$ This $\delta$ is a positive real number, and if $s \in ( t-\delta, t+\delta )$, then $s \in \left(t-\delta_i, t + \delta_i \right)$ and hence $g(s) \in U$, showing that $g^{-1}(U)$ is open. Finaly, if $t \in h^{-1}(U)$, then, for each $i = 1, \ldots, k$, 
$$ \pi_{n_i}\left( h(t) \right) = \frac{t}{n_i} \in U_{n_i}$$
and hence $$\left( \frac{t}{n_i} - \delta_i , \frac{t}{n_i} + \delta_i \right) \subset U_{n_i}$$ for some positive real number $\delta_i$. Let's take $$\delta \colon= \min \left\{ n_1 \delta_1, \ldots, n_k \delta_k \right\}.$$ This $\delta $ is a positive real number, and if $s \in (t - \delta , t + \delta )$, then 
$$t- n_i \delta_i < s < t + n_i \delta_i$$ and hence  $$ \frac{t}{n_i} - \delta_i < \frac{s}{n_i} < \frac{t}{n_i} + \delta_i $$ or $$ \pi_{n_i}\left( h(s) \right) \in \left( \frac{t}{n_i} - \delta_i, \frac{t}{n_i} + \delta_i \right) \subset U_{n_i}$$ for each $i = 1, \ldots, k$, which implies that $s \in h^{-1}(U)$, from which it follows that $h^{-1}(U)$ is open in $\mathbb{R}$. Have I been able to get these proofs right? Now for the uniform topology. Let's take a real number $\varepsilon$ such that $0 < \varepsilon < \frac 1 2$. Let $t \in \mathbb{R}$. If $s \in \mathbb{R}$ such that $s \neq t$, then
$$\tilde{\rho}\left( f(s), f(t) \right) = \sup \left\{ \ \min \left\{ n \vert s-t\vert, 1 \right\} \ \colon \ n \in \mathbb{N} \ \right\} = 1 > \varepsilon,$$
from which it follows that $f$ is not continuous (at any point of $\mathbb{R}$). Now let us take a real number $\delta$ such that $0 < \delta \leq \varepsilon$. Then, for any points $s, t \in \mathbb{R}$ such that $\vert s-t \vert < \delta$, we have
$$\tilde{\rho}\left( g(s), g(t) \right) = \sup \left\{ \ \min \left\{ \vert s-t \vert, 1 \right\} \ \colon \  n \in \mathbb{N} \right\} = \vert s- t \vert < \varepsilon,$$ and hence it follows that $g$ is (uniformly) continuous. Now let's choose $\varepsilon$ and $\delta$ as above. Then, for any $s, t \in \mathbb{R}$ such that $\vert s-t\vert < \delta$, we have 
$$\tilde{\rho}\left( h(s), h(t) \right) = \sup \left\{ \ \min \left\{ \frac{\vert s-t \vert}{n}, 1 \right\} \ \colon \ n \in \mathbb{N} \ \right\} = \min \left\{ \vert s-t\vert, 1 \right\} = \vert s-t\vert < \varepsilon,$$ from which it follows that $h$ is (uniformly) continuous. Have I been able to come up with the right answer in each case? If so, have I been able to get these proofs right as well? For the product topology, we can also use Theorem 19.6 in Munkres. Here I have attempted to show the continuity of $f$, $g$, and $h$ directly. And, the box topology on $\mathbb{R}^\omega$ is the one havaing as a basis all sets of the form 
$$ (a_1, b_1) \times (a_2, b_2) \times (a_3, b_3) \times \cdots, $$
where $\left(a_i \right)_{i \in \mathbb{N}} \in \mathbb{R}^\omega$ and $\left(b_i \right)_{i \in \mathbb{N}} \in \mathbb{R}^\omega$ are such that $a_i < b_i$ for each $i = 1, 2, 3, \ldots$, and $(a_i, b_i)$ denotes the segment (i.e. open interval) with $a_i$ as the left endpoint and $b_i$ as the right endpoint. The functions $f$, $g$, and $h$ are not continuous when $\mathbb{R}^\omega$ is given the box topology. The inverse image under each of  $f$, $g$, and $h$ of the basis element 
$$B \colon= \left( -1, 1 \right) \times \left(-\frac{1}{2^2}, \frac{1}{2^2} \right) \times \left( -\frac{1}{3^2}, \frac{1}{3^2} \right) \times \cdots, $$ 
for example, contains the point $t = 0$ since 
$$f(0) = g(0) = h(0) = (0, 0, 0, \ldots) \in B.$$
So, in order for this inverse image to be open in $\mathbb{R}$ with the usual topology, there must be an open interval $\left(-\delta_f, \delta_f \right)$, $\left( -\delta_g, \delta_g \right)$, and $\left( -\delta_h, \delta_h \right)$, for some positive real numbers $\delta_f$, $\delta_g$, and $\delta_h$, respectively, such that 
$$ \left(-\delta_f, \delta_f \right) \subset f^{-1}(B),$$ 
$$\left( -\delta_g, \delta_g \right) \subset g^{-1}(B), $$
$$\left( -\delta_h, \delta_h \right) \subset h^{-1}(B). $$
In particular, we must have 
$$f\left( \frac{\delta_f}{2} \right) \in B,$$
$$g\left( \frac{\delta_g}{2} \right) \in B,$$
$$h\left( \frac{\delta_h}{2} \right) \in B.$$ 
So, for each $n \in \mathbb{N}$, we have 
$$\frac{n \delta_f}{2} \in \left( - \frac{1}{n^2}, \frac{1}{n^2}\right), \ \frac{\delta_g}{2} \in \left( - \frac{1}{n^2}, \frac{1}{n^2}\right), \ \frac{\delta_h}{2n} \in \left( - \frac{1}{n^2}, \frac{1}{n^2}\right), $$ 
and hence 
$$n^3 < \frac{2}{ \delta_f } \ \mbox{ for all} \  n \in \mathbb{N},$$
$$n^2 < \frac{2}{ \delta_g } \ \mbox{ for all} \  n \in \mathbb{N},$$
$$n < \frac{2}{ \delta_h } \ \mbox{ for all} \  n \in \mathbb{N},$$
each of which is impossible. Am I right? That $g$ is not continuous was also shown by Munkres himself in Example 2, Sec. 19 on page 117.","['continuity', 'general-topology', 'uniform-continuity', 'solution-verification']"
1800208,Trouble understanding definition of an attracting set,"From Wiggins' book,
""Let $\cal{M}$ be a trapping region. Then $A=\cap_{t>0}\phi(t,\cal{M})$ is called an attracting set"". Then he gives an example: $\dot{x}=x-x^3$ $\dot{y}=-y$, and claims that the closed interval $[-1,1]$ is an attracting set with some appropriate trapping region (namely an ellipse $\cal{M}$ that surrounds all three critical points of the system. But I cannot relate this to the definition of an attracting set; I mean, if $t$ is arbitrarily large, then shouldn't all points be either at $(-1,0),(0,0)$ or $(1,0)$? How is the containment $[-1,1] \subset \cap_{t>0}\phi(t,\cal{M})$ seen? I have attached a diagram that he has provided below.","['ordinary-differential-equations', 'dynamical-systems']"
1800232,Stuck trying to solve a PDE by method of characteristics,"I've been trying to solve the inhomogeneous PDE IVP $u_t+c u_x = e^{2x}$; $u(x,0)=f(x)$, but got stuck. Would appreciate some help. Here's what I did by trying to use the method of characteristics: $\frac{dt}{ds}=1$, $\frac{dx}{ds}=c$, $\frac{dz}{ds}=e^{2x}$, where $s$ is the parametrization variable for a characteristic curve lying in the surface $u(x,t)$. So I get $t(s) = s+C_1$, $x(s) = cs+C_2$, $z(s) = e^{2x}+C_3$. We can thus see that $ct-x = C_1-C_2 = D_1$, where $C_i$ and $D_i$ are some constants. But here's where I'm stuck since I have no idea what should be done next. That is, how can we now relate $ct-x$ to $z(x,t)$ to build the surface? I was thinking that, maybe, do something like this: $z-te^{2x}=C_3-C_1=D_2$, so $z(x,t) = D_2 + te^{2(ct-D_1)}$ (after substituting). But is this the solution then and what's remaining is to apply the initial condition? $z(x,t) = u(x,t)$, $u(x,0) = D_2 = f(x)$, so $u(x,t) = f(x) + te^{2(ct-D_1)}$. Unfortunately, this does not match the solution from WolframAlpha.","['ordinary-differential-equations', 'partial-differential-equations']"
1800253,$ \int_0^\infty \ \frac{(x\cdot\cos x - \sin x)^3}{x^6} \ dx$,"What is the value of $$
\int_0^\infty \ \frac{(x\cdot\cos x - \sin x)^3}{x^6} \ dx
$$ I have no idea how to start with this integral, any hint?","['integration', 'calculus', 'analysis']"
1800257,Derivation of Dirac delta function,Is there anyone could give me a hint how to find the distributional derivative of the delta function $\delta$? I don't know how to deal with the infinite point.,"['derivatives', 'real-analysis', 'limits', 'distribution-theory', 'dirac-delta']"
1800273,Moving half of the nuts,"An even number of nuts is divided into three nonempty piles. In each step, we are allowed to take half the nuts from a pile with an even number of nuts, and put them on another pile. Can we always reach a point where exactly half of the nuts belong to one pile? For example, if we start with $(3,5,6)$, we can transform as $(3,5,6)\rightarrow (3,8,3)\rightarrow (3,4,7)$, and now the last pile has half of the nuts. Note that in each step, some pile of nuts must be even, so we can keep moving. Moreover, we will never empty a pile.",['combinatorics']
1800318,How do I prove that $f_n\to f$ in $L^p$?,"Let $\{f_n\}$ be a sequence in $L^p([0,1])$ for $p\geq 1$. Suppose there exists $f\in L^p([0,1])$ satisfying $\lim_{n\to\infty} \int_0^1 f_n(x)g(x)dx = \int_0^1 f(x)g(x)dx$ for any $g\in L^2([0,1])$. Moreover, assume that $\lim_{n\to\infty} ||f_n||_p=||f||_p$. In this case, how do I prove that $f_n\rightarrow f$ in $L^p$? Even when $p=2$, the result is non-trivial since $\lim_{n\to\infty}<f_n-f,g>=0$ (weak convergence) does not simply imply the convergence in $L^2$. More seriously, if $p\neq 2$, I'm not sure what's going on here. Why for given $f\in L^p$ and $g\in L^2$, $fg\in L^1$?. Holder inequality cannot be applied here. Moreover, I'm not sure how to use the condition $\lim_{n\to\infty} ||f_n||_p =||f||_p$. If $f_n \to f$ pointwise a.e., this condition seems useful, but this is not the case. How do I prove this? Thank you in advance.",['real-analysis']
1800331,Formula relating dimension of fiber of morphism between varieties,"Let $f: X \to Y$ be a morphism of (irreducible) varieties, where the dimension of every fiber dim$f^{-1}(y)=n$ is the same. Must it follow that dim$X=$ dim$Y+n$? The reason I am asking this is that this is a theorem in some notes I am reading, but I am sure the proof presented is problematic, yet I cannot seem to patch the errors. However this does seem geometrically intuitive... Any help is appreciated!","['schemes', 'algebraic-geometry']"
1800342,Find Surface Area Via a Line Integral (Stokes' Theorem),"I am trying to use Stokes' Theorem to calculate the surface area of a parametrized surface via a line integral. The surface is the part of $z= x^2+y^2$ below the plane $z=5$. I know this can be done the usual way, without Stokes' Theorem , but there is another surface whose area I'm trying to calculate for which the usual way doesn't work well due to tough limits of integration. As stated in the question Un-Curl Operator , we can find the surface area via $\oint\vec{F}\cdot \vec{ds}$ over the curve of intersection of the paraboloid and plane, provided we can find a field $\vec{F}$ such that $\text{curl}(\vec{F})\cdot \vec{n}=1.$ This would mean that $$\text{curl}(\vec{F}) = \frac{\vec{n}}{{\Vert \vec{n} \Vert}^2},$$ so that, $$\text{curl}(\vec{F})\cdot \vec{n}=\left\Vert\frac{\vec{n}}{{\Vert \vec{n} \Vert}^2}\right\Vert\cdot\Vert \vec{n} \Vert=\frac{{\Vert \vec{n} \Vert}^2}{{\Vert \vec{n} \Vert}^2}=1,$$as required.
But finding such a vector field has proven difficult. I first tried using the method described in the question Anti-Curl Operator , but realized that this wouldn't work because $\text{curl}(\vec{F})$ has a nonzero divergence. I then turned to the method described in an answer to the Un-Curl Operator question, known as the Helmholtz decomposition. The idea is that given our vector field, $\text{curl}(\vec{F})$ in this case, we can split it into a curl-free part and a divergence-free part: $$\text{curl}(\mathbf{F})=-\nabla\Phi+\nabla\times \mathbf{A}$$ I believe ( I could be wrong though ) that it remains only to find $\mathbf{A}$, since the first term will not contribute to the line integral around a closed path. However, this is where my confusion begins. According to Wikipedia , $\mathbf{A}$ is found as follows: $$ \mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_V\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}V'
-
\frac{1}{4\pi}\oint_S\mathbf{\hat{n}}'\times\frac{\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}S'$$ I do not know how to use this formula. Specifically, I do not know what most of the symbols represent in the context of this problem. What is the difference between $\mathbf{r}$ and $\mathbf{r}'$? What is the region of integration? Also, the Wikipedia page mentions that the second term can be dropped if the field satisfies certain conditions. Does that apply here? I am also wondering if there is anything else I need to do to specify that the dot product of $\text{curl}(\mathbf{F})$ and $\mathbf{n}$ is $1$. The answer on the Un-Curl Operator question used the following notation: $$\mathbf{G}\vert_{\partial\Omega}\cdot\mathbf{n}=1,$$ where $\mathbf{G}$ = $\text{curl}(\mathbf{F})$. I'm pretty sure that $\partial\Omega$ represents the boundary of a domain $\Omega$, but what is $\Omega$ in the context of this problem? Am I missing something? How would we use the Helmholtz decomposition to find the vector field $\mathbf{A}$ in this problem?","['multivariable-calculus', 'stokes-theorem', 'line-integrals']"
1800384,Prove the fractions aren't integers,"Prove that if $p$ and $q$ are distinct primes then $\dfrac{pq-1}{(p-1)(q-1)}$ is never an integer. Is it similarly true that if $p,q,r$ are distinct primes then $\dfrac{pqr-1}{(p-1)(q-1)(r-1)}$ is also never an integer? I think using a modular arithmetic argument here would help. In other words, we must have for the first fraction $pq-1 \equiv 0 \pmod{(p-1)(q-1)}$. Then I am unsure how to proceed next since we can't really use the Chinese Remainder Theorem.",['number-theory']
1800439,Doing a magic trick with limited memory (from a problem solving course),"I got the following question in a problem solving course: There are four different objects lying on places 1, 2, 3, 4. A magician closes his eyes and someone from the audience comes. He switches pairs of objects 10 times, and each time shouts the places he switched. Then he does a secret switch and doesn't tell the magician the places. Then he switches another 10 times and shouts the places as before. The magician open his eyes, look at the objects and point on one of the objects that participated in the secret switch. The magician has a bad memory, thus he can only remember one number between 1 to 10. How does he do it? My direction is not to find the secret switch itself, but 2-3 options with a mutual object and choose that object, but I can't figure out how.","['permutations', 'contest-math', 'puzzle', 'group-theory']"
1800440,Significance of Convex Sets for I-Projection,"I have been reviewing the literature on information theoretic methods in statistics, and in particular, the method of I-projections . Given a discrete, finite alphabet $\mathcal{X}$, let $\prod$ denote a set of probability mass functions (pmfs) on $\mathcal{X}$. Suppose $Q$ is a pmf on $\mathcal{X}$ which does not belong to $\prod$. The I-projection of $Q$ on $\prod$ is defined as the element $P^{*}\in \prod$ such that \begin{eqnarray}P^{*}=\arg \min \limits_{P\in \prod} D(P||Q),\end{eqnarray} where $D(P||Q)$ denotes the Kullback-Leibler divergence (KL-divergence) between pmfs $P$ and $Q$, defined as \begin{eqnarray}D(P||Q)=\sum\limits_{x\in \mathcal{X}} P(x)\log\left(\frac{P(x)}{Q(x)}\right).\end{eqnarray} A sufficient condition for the existence of a $P^{*}$ as above requires $\prod$ to be closed. In addition, in the literature, $\prod$ is considered to be a convex set. For example, the family \begin{eqnarray}
\mathcal{P}=\lbrace P: \sum\limits_{x\in \mathcal{X}} P(x)f_{i}(x)=\alpha_{i}, ~1\leq i\leq k\rbrace,
\end{eqnarray}
where $f_{i}:\mathcal{X}\rightarrow \mathbb{R}$ are measurable functions and $\alpha_{i}\in \mathbb{R}$, called a linear family of pmfs, is typically considered in place of $\prod$ in the literature. Question: Can someone please explain the significance of using convex sets? Do things fail if we do not consider convex sets?","['information-theory', 'statistics']"
1800467,Show there are only a finite number of integers with $\dfrac{\prod_{i=1}^n a_i-1}{\prod_{i=1}^n (a_i-1)} $ an integer,"Show, for each $n$, there are only a finite number of integral $(a_i)_{i=1}^n$ such that $2\le a_i \le a_{i+1}$ and $\dfrac{\prod_{i=1}^n a_i-1}{\prod_{i=1}^n (a_i-1)} $ is an integer. My question is inspired by this one: Prove the fractions aren't integers I can show that
$a_1
\le \dfrac1{2^{1/n}-1}
$. I think there is a proof by induction
of the general case,
but I don't have one now. A comment:
This reminds me of
the Egyptian fraction
problem. Here's what I have now. $\begin{array}\\
r
&=\dfrac{\prod_{i=1}^n (a_i-1)+\prod_{i=1}^n a_i-\prod_{i=1}^n (a_i-1)-1}{\prod_{i=1}^n (a_i-1)}\\
&=1+\dfrac{\prod_{i=1}^n a_i-\prod_{i=1}^n (a_i-1)-1}{\prod_{i=1}^n (a_i-1)}\\
&=1+\dfrac{\prod_{i=1}^n (b_i+1)-\prod_{i=1}^n b_i-1}{\prod_{i=1}^n b_i}
\quad\text{where }b_i = a_i-1\\
\end{array}
$ Letting
$B = \prod_{i=1}^n b_i
$,
the fraction is,
dividing the numerator and denominator 
by $B$,
$r-1
=s
=\prod_{i=1}^n (1+1/b_i)-1-1/B
$. We have
$s 
\gt 1+\sum_{i=1}^n 1/b_i-1-1/B
=\sum_{i=1}^n 1/b_i-1/B
\gt 0
$
since
$b_i \le B$
and $n \ge 2$. Therefore,
if $s < 1$,
$r$ is not an integer. If
$c_i = 1/b_i$,
$s 
= \prod_{i=1}^n (1+c_i)-1-\prod_{i=1}^n c_i
$,
so
$\dfrac{ds}{dc_j}
=\prod_{i=1, i\ne j}^n (1+c_i)-\prod_{i=1,i\ne j}^n c_i
$,
so that
$s$ 
is an increasing function of each $c_i$
and so is a decreasing function of each $b_i$. In particular,
$s
\le (1+1/b_1)^n-1-1/b_1^n
$,
so that if
$(1+1/b_1)^n-1
\le 1$,
$s$ is not an integer.
This is
$1+1/b_1
\le 2^{1/n}
$
or
$b_1
\ge \dfrac1{2^{1/n}-1}
$.","['sequences-and-series', 'divisibility']"
1800489,Maximal (inclusion) ideal proof,"Let $X$ be a set and $I$ its ideal. $I\neq \emptyset$ is ideal if $I\subseteq \mathcal{P}(X)$, so that for all $A,B\in \mathcal{P}(X)$ following holds
$$(A\subseteq B \text{ and } B\in I)\Rightarrow A\in I$$
and
$$A,B\in I \Rightarrow A\cup B\in I.$$ I want to prove that for $X\not\in I$ and $\cup I=X$, $I$ is a greatest ideal (considering inclusion), which doesn't contain $X$, if and only if 
$$\forall A\subset X \colon A\in I \Leftrightarrow X\setminus A\not\in I.$$ Any ideas on how to prove this both ways? For $\Leftarrow$ part, I showed that if $A\not\in I$ and $X\setminus A\not\in I$, then $I$ can't be maximal. Does this make the rigorously make the argument for me?","['elementary-set-theory', 'ideals']"
1800510,Applying the chain rule to compute $\frac{d}{dx}(\cos^6 x)$,"$$\frac{d}{dx}(\cos^6x)$$ Using the chain rule $
M'(N(x)).N'(x)$, I'm deconstructing the $\cos$ function $$\begin{align*}
&M= \cos^6 \\
&N= x\end{align*}$$ End result should be $$-6\sin^5x \cdot 1$$ or $$-6\sin^5x$$ Yet my book said the end result is $$-6\sin x \cos^ 5 x$$ Why?","['derivatives', 'calculus']"
1800556,Factorization of $x^5+x$,"I need to make the decomposition in $\mathbb{R}$ of: $$x^5+x$$ Here my steps: $$x(x^4+1)$$
$$x\big[ (x^2)^2+1 \big]$$
$$x\big[(x^2+1)^2-2x^2\big]$$ how should I proceed?",['algebra-precalculus']
1800628,Differentiablity at $0$ of a function $f: \mathbb R \to \mathbb R$ which is twice differentiable in $\mathbb R \setminus \{0\}$,"Let $f: \mathbb R \to \mathbb R$ be a function , twice differentiable in $\mathbb R \setminus \{0\}$ such that $f'(x)<0<f''(x) , \forall x <0$ and $f'(x)>0>f''(x) , \forall x >0$ ; then is it true that $f$ cannot be differentiable at $0$ ? EDIT : My attempt : Suppose $f'$ exists at $0$ , then $f$ is differentiable everywhere , then as $f'(-1)<0<f'(1)$ , so by Darboux theorem , there is $c \in \mathbb R$ such that $f'(c)=0$ , but $f'(x) \ne 0 , \forall x \ne 0$ so $c=0$  , thus $f'(0)=0$ . I cannot make any further progress , Please help . Thanks in advance","['derivatives', 'real-analysis', 'continuity']"
1800658,Probability of having a Girl [duplicate],"This question already has answers here : very simple conditional probability question (6 answers) Closed 11 months ago . A and B are married. They have two kids. One of them is a girl. What is the probability that the other kid is also a girl? Someone says $\frac{1}{2}$, someone says $\frac{1}{3}$. Which is correct? Now A and B have 4 children and all of them are boys. B is pregnant. So what is the probability that A and B are gifted with a baby girl?
Is it $\frac{1}{2}$ or there will be some conditional probability?",['probability']
1800667,"$f \in C^2(\mathbb R)$ , $(f(x))^2 \le 1$ ; $(f'(x))^2+(f''(x))^2 \le 1 $ ; then is $(f(x))^2+(f'(x))^2 \le 1 $?","Let $f \in C^2(\mathbb R)$ be such that $$(f(x))^2 \le 1 ;
 (f'(x))^2+(f''(x))^2 \le 1 , \forall x \in \mathbb R$$  Then is it
  true that $(f(x))^2+(f'(x))^2 \le 1 , \forall x \in \mathbb R$ ? I haven't gotten anywhere with this problem . Please help . Thanks in advance","['derivatives', 'real-analysis', 'inequality', 'calculus', 'continuity']"
1800683,What is the differential of the quotient map?,"We can view the projective space $P(\mathbb R^n)$ as the quotient of $S^n/\sim$  where $x \sim y$ if and only if $x = -y$. The quotient map $q: S^n \to P(\mathbb R^n)$ is the map $x \mapsto [x]$ where $[x] = \{x,-x\}$. I want to calculate the Jacobian matrix differential of $q$ but I don't see what it is. To me it seems that it is both the identity $I$ and $-I$ at the same time. What is the differential of $q$ and how to caluclate it? Here is what I have so far: Since I didn't know how to do the general case I tried to do the explicit caluclation for $n=2$. Let $\varphi : \mathbb R^2 \to S^2, (x,y) \mapsto ({2x \over x^2 + y^2 + 1}, {2y \over x^2 + y^2 +1}, {x^2 +y^2 -1\over x^2 +y^2 +1})$ and $\psi^{-1}:P(\mathbb R^2) \to \mathbb R^2, [(x,y,z)] \mapsto ({y\over x}, {z\over x})$ and $F: S^2 \to P(\mathbb R^2), x \mapsto [x]$. Then $$ \psi^{-1}\circ F \circ \varphi (x,y) = ({y\over x}, {x^2 +y^2 -1 \over 2x}) $$ and the Jacobian I calculated as $$ J_{\psi^{-1}\circ F \circ \varphi} =\left ( \begin{matrix} -{y \over x^2} & {1\over x} \\ {2x^2 - 2y^2 + 2 \over 4 x^2 } & {y\over x} \end{matrix}\right )$$ Is this correct? And if so, how can I generalise to arbitrary $n$?","['differential-geometry', 'proof-verification']"
1800690,Spherical Harmonics and $L_+$ and $L_-$ operators,"I have the spherical harmonics $Y_{m}^{l}\left(\theta,\varphi\right)$ and I want to show that the operators $L^{\pm}$ act as ""creation"" and ""annihilation"" operators such that $$L^{\pm}Y_{m}^{l}=\sqrt{l\left(l+1\right)-m\left(m\pm1\right)}Y_{m\pm1}^{l}$$ I'm quite sure that if I substitute all the definitions and proceed analitically I will end up after 3 or 4 pages of calculations with the right solution, but I'm wondering if there's a smarter and algebric way to demonstrate this equation. Does anybody have at least an hint to follow? Here's the definitions I used: Spherical Harmonics $$Y_{m}^{l}\left(\theta,\varphi\right)=\left(-1\right)^{m}\sqrt{\frac{\left(2l+1\right)}{4\pi}\frac{\left(l-m\right)!}{\left(l+m\right)!}}P_{l}^{m}(\cos\theta)e^{\imath m\varphi}$$ Legendre Functions $$P_{l}^{m}(x)=\frac{1}{2^{l}l!}\left(1-x^{2}\right)^{m/2}\left(\frac{d}{dx}\right)^{l+m}\left(x^{2}-1\right)^{l},$$ Operators $$
L^{+}	=L_{1}+\imath L_{2}=e^{\imath\varphi}\left(-\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right),$$
$$
L^{-}	=L_{1}-\imath L_{2}=e^{-\imath\varphi}\left(\imath\frac{\partial}{\partial\theta}+\cot\theta\frac{\partial}{\partial\varphi}\right),
$$
$$
L_{z}	=\imath L_{3}=-\imath\frac{\partial}{\partial\varphi}.
$$ Commutation relations $$\left[L_{z},\,L^{+}\right]=L^{+},$$
$$\left[L_{z},\,L^{-}\right]=-L^{-},$$
$$\left[L^{+},L^{-}\right]=2L_{z}.$$ Laplacian or Casimir Operator $$L^{-}L^{+}	=L_{1}^{2}+L_{2}^{2}+\imath\left[L_{1},\,L_{2}\right],$$
$$ L^{+}L^{-}	=L_{1}^{2}+L_{2}^{2}-\imath\left[L_{1},\,L_{2}\right],
$$
 $$\mathbf{L}^{2}=L_{z}^{2}+L_{z}+L^{-}L^{+}.$$","['lie-groups', 'functional-analysis', 'complex-analysis', 'lie-algebras', 'analysis']"
1800710,Dimension of an abelian subvariety in the proof of the Poincaré's Reducibility Theorem,"I am trying to understand the proof of the Poincaré's Reducibility Theorem, that I'm reading from the book ""Abelian varieties"" of J.S. Milne (see Proposition 10.1) and from the book ""Abelian varieties"" of D. Mumford (page 173 of the old edition). The theorem says: Let $X$ be an abelian variety and let $Y$ be an abelian subvariety of $X$, $0\neq Y\neq X$. Then there exists an abelian subvariety $Y'\subset X$ such that the map \begin{align*} Y\times Y' &\to X\\ (y,y') &\mapsto y+y' \end{align*} is an isogeny. I have understood the first part of the proof: Let $i: Y\hookrightarrow X$ be the inclusion map and let $\mathcal{L}$ be an ample line bundle on $X$. Then we have:
  \begin{equation*}
X \xrightarrow{\phi_{\mathcal{L}}} X^\vee\xrightarrow{i^\vee}Y^\vee
\end{equation*}
  where $\phi_{\mathcal{L}}$ is an isogeny in this case.  Let $Y'$ be the connected component of $\ker(i^\vee\circ\phi_{\mathcal{L}})=\phi_{\mathcal{L}}^{-1}(\ker i^\vee)$ passing through $0$. Then $Y'$ is an abelian variety and
  \begin{equation*}
\dim Y' =\dim\ker i^\vee \ge \dim X^\vee-\dim Y^\vee=\dim X-\dim Y.
\end{equation*} Now, in the last equation, I dont' understand why the equality $\dim Y' =\dim\ker i^\vee$ holds. I guess that \begin{equation*}\dim Y'=\dim\phi_{\mathcal{L}}^{-1}(\ker i^\vee)= \dim \ker i^\vee
\end{equation*} where the last equality holds since $\phi_{\mathcal{L}}$ is an isogeny, but I don't see why $\dim Y'=\dim\phi_{\mathcal{L}}^{-1}(\ker i^\vee)$.
Can you help me?
Thank you!","['abelian-varieties', 'algebraic-geometry']"
1800769,Why do subvarieties correspond to Hodge classes?,"Let $X$ be a smooth complex projective variety and define
$$Hdg^k(X)=H^{2k}(X,\mathbb{Z})\cap H^{k,k}(X)$$
the group of integral $(k,k)$ cycles on $X$. Now it is a fact that we can 
associate to the complex subvariety of $X$ an element in $Hdg^{k}(X)$ but I don't quite get the details of the association. From what I have gathered so far I think we are working with the diagram $$
\require{AMScd}
\begin{CD}
\mathcal{Z}_p(X) @>{i}>> H_{2n-2p}(X;\mathbb Z) @>{PD}>> H^{2p}(X;\mathbb Z)\\
@VVV   @. @VV{i}V \\
H^{2n-2p}_{dR}(X)^* @>{\cong}>> H^{2p}_{dR}(X) @>{de Rham}>> H^{2p}(X;\mathbb{C})
\end{CD}$$ Where along the top row we map
$$Z\mapsto [Z]\mapsto [Z]^{PD}$$
Where $^{PD}$ denotes the Poincare dual and $[Z]$ the pushforward of the fundamental class of $Z$ over the inclusion. Along the bottom row we map
$$Z\mapsto \left[\omega\mapsto \int_Zi^*\omega\right]\mapsto\left( \alpha \text{ s.t. } \int_{X}\beta\wedge \alpha=\int_Zi^*\beta\right)\mapsto \left(\alpha^{top} \text{ s.t. }\alpha^{top}(V)=\int_V\alpha\right)$$
(we assume that $Z$ is smooth and all that to make things easier). The last map is given by the inverse of the de Rham isomorphism. Now the image of the bottom row can easily be shown to be a class of type $(p,p)$. The image of the top row is clearly the image of the integral cohomology class, by definition. Now if this diagram commutes we have a map $Z_p(X)\to Hdg^p(X)$. However, I don't see why this diagram should commute. So my question is: why does the above diagram commute? I'll add the following link . At the bottom of page $148$ the authors states without proof (I renamed the objects to be consistent with the above): The canonical morphism $H_{2n-2}(X,\mathbb{Z})\to H^{2n-2p}(X,\mathbb{C})^*$ carries the topological class $[Z]$ of an analytic subspace $Z$ of codimension $p$ in $X$ into the fundamental class $\left[\omega\mapsto \int_Zi^*\omega\right]$.
  Similarly, the morphism $H^{2p}(X,\mathbb{Z})\to H^{2p}(X,\mathbb{C})$ carries the topological class $[Z]^{PD}$ to $[\alpha^{top}]$ I am looking for a proof of this result. (this is equivalent with the diagram above commuting, but might give a bit more context)","['algebraic-topology', 'algebraic-geometry']"
1800778,Hadamard counterexample Dirichlet-problem,"Good afternoon, I try to understand the follwoing counterexample of Hadamard: The classical solution of $
\begin{cases}
\Delta u = 0 ~~\text{ in }\Omega\\
u=g \text{ auf }~~ \partial \Omega
\end{cases}
$ with $u \in C^2(\Omega) \cap C(\partial \Omega)$ not always lies in $H^1(\Omega)$ . To show this Hadamard took the function $
\begin{equation}
     u(r, \phi) = \sum\limits_{n=0}^{\infty}r^{n!}\frac{sin(n! \phi)}{n^2}
\end{equation}
$ in polar-coordinates on $\Omega =$ ""the unit-disk"". After computing the Laplacian in polar coordinates it was very easy to show that this function is indeed a classical solution (i.e. u harmonic) but now I am supposed to choose an appropriate boundary value function $g \in C(\partial \Omega)$ , such that this $u(r,\phi)$ is not in $H^1(\Omega)$ . Can someone help? By the way in our lecture we had the statement that harmonic functions have minimal seminorms, perhaps this could be useful.","['functional-analysis', 'sobolev-spaces', 'partial-differential-equations']"
1800779,Explicit unit/counit of inverse image/direct image adjunction.,"Is there a nice explicit description for the unit and counit of the inverse image/direct image adjunction $f^{-1} \dashv f_*$ between sheaves of rings (and in the version $f^* \dashv f_*$ for $\mathcal{O}_X$-modules)? It is said these come from natural maps, but I can only see they exist because I can argue we should have a hom-set adjunction, and that iso is constructed by using the universal property of the colimit of which $f^{-1}$ is a sheafification (in the sheaf case). Here I'm using the definition that given a morphism $f:(X,\mathcal{O}_X) \to (Y,\mathcal{O}_Y)$ of ringed spaces, and sheaves $\mathcal{F}$ on $X$ and $\mathcal{G}$ on $Y$ (resp. of $\mathcal{O}_X$ and $\mathcal{O}_Y$ modules), then $f^{-1}\mathcal{G}$ is the shefification of the presheaf $U \mapsto \ colim_{f(U) \subseteq V}\ \mathcal{G}(V)$ and in the case of $\mathcal{O}_X$-modules we define $f^*\mathcal{G} = f^{-1}\mathcal{G} \otimes_{f^{-1} \mathcal{O}_Y} \mathcal{O}_X$.","['category-theory', 'sheaf-theory', 'algebraic-geometry']"
1800782,Prove the n-th power of a matrix is the null matrix,"Let $A,B$ be $n\times n$ matrices with complex elements, $AB=BA$ , $\det(B)\ne0$ , having the following property: $$|\det(A+zB)|=1, \forall z \in \mathbb{C}, |z|=1.$$ Prove $A^n=0_n$ . Does this remain true if $AB \ne BA$ ? So far, no idea. Any help is appreciated. Update I think I made some progress: Let $f(z)=\det(A+zB)=a_nz^n + .. + a_0$ From $|f(z)|=1$ we have $f(z)\overline {f(z)}=1, \forall z, |z|=1$ then, after replacing $\overline z$ with $\frac 1 z$ : $(a_nz^n + .. + a_0)(\overline a_0z^n + .. + \overline a_n)=z^n \tag 1$ Because (1) is true for infinitely many $z$ then (1) must be a polynomial identity. Identifying coefficients I've got $$a_0=a_1=..a_{n-1}=0, a_n\overline a_n=1$$ So $\det(A+zB)=a_nz^n$ where $|a_n|=1$ . It follows that $det(A)=0, det(B)=a_n$ .","['matrices', 'linear-algebra', 'complex-numbers', 'determinant']"
1800806,Irreducible polynomials in $\mathbb F_3[x]$,"Finding irreducible polynomials in $\mathbb F_3[x]$ of degree less or equal to $4$ for $d=2,3$ the polynomial should not have a root case $d=2$ there are $2\cdot 3\cdot 3=18$ polynomials with degree $2$. Testing all I get; $x^2+1, x^2+x+2, x^2+2x+2, 2x^2+2, 2x^2+x+1, 2x^2+2x+1$ are irreducible when $d=3$ it gets complicated, at least by a theorem I know that there are $9$ irreducible monic polynomials ($x^{p^d}-x$ is the product of all irreducible monic polynomials in $\mathbb F_p[x]$) Is there a technique to find these ? here for example there is a complicated formula, which only determines the monic ones","['abstract-algebra', 'linear-algebra']"
1800830,"Prove that $R(+,.)$ is a division ring but I disproved it","QUESTION : Let $R=\left[\begin{matrix}\alpha & \beta \\ \bar\beta &
 \bar\alpha\end{matrix}\right]\in \mathbf{M_2(\mathbb{C})} $ where 
  $\bar\alpha,\bar\beta$ denote the conjugates of $\alpha, \beta$ 
  respectively. Prove that $R$ is a division ring but not field under 
  the usual matrix addition and multiplication. MY ATTEMPT : I am comfortable with what I have to do and what I have to prove. I have successfully proved that it is not a field as the matrix multiplication is not commutative. But instead of proving that it is a division ring, I have disproved it. We know that, A division ring , also called a skew field, is a ring in which division
  is possible. Specifically, it is a nonzero ring in which every
  nonzero element a has a multiplicative inverse, i.e., an element $x$
  with $a·x = x·a = 1$. Stated differently, a ring is a division ring if
  and only if the group of units equals the set of all nonzero elements . Now the condition in bold is what I have shown not to hold. Actually the inverse of a matrix exists iff the matrix is not singular. But $\left|\begin{matrix}\alpha & \beta \\ \bar\beta &
 \bar\alpha\end{matrix}\right|=|\alpha|^2-|\beta|^2$ which can be $0$ if $|\alpha|=|\beta|$ which holds for infinitely many $\alpha,\beta \in \mathbb{C}$. So $R(+,.)$ is not a division ring since it contains an infinite number of non-invertible matrices. Am I right or wrong? Please help.","['matrices', 'abstract-algebra', 'ring-theory', 'field-theory']"
1800841,Showing that the multivariate normal density integrates to 1,"This is NOT the same as How to show the normal density integrates to 1? . Let $\mathbf{x} \in \mathbb{R}^d$ be a multivariate normal random vector, with $\mathbb{E}[\mathbf{x}] = \boldsymbol\mu$ and positive-definite $\text{Var}[\mathbf{x}] = \boldsymbol\Sigma$. Then $$\int_{\mathbb{R}^d}\dfrac{1}{(2\pi)^{n/2}|\boldsymbol\Sigma|^{1/2}}\exp\left[-\dfrac{1}{2}(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\right]\text{ d}\mathbf{x}$$
should equal $1$. How do I show this? I am allowed to use the one-dimensional case as a fact, so I thought, perhaps I should use induction on $d$. One-dimensional case is true, great. Now suppose it's true for $k$ dimensions. At the $k+1$th dimension, the difficulty is working with the new variance-covariance matrix and the Mahalanobis distance term 
 $$(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\text{.}$$
Does anyone have any suggestions? I don't need a complete solution, but I would like a starting point.","['multivariable-calculus', 'statistics', 'probability']"
1800852,Prerequisites to prove central limit theorem,"What are the prerequisites to prove the central limit theorem? In my statistics textbook it is stated without a complete proof, so I guess I need more than calculus. However, do I need more than undergraduate real and complex analysis? In what book can I find a complete proof of the central limit theorem?","['statistics', 'central-limit-theorem']"
1800856,Is $2^{2^{\aleph_0}}$ a higher cardinality than $2^{\aleph_0}$?,"As far as I understand, $2^{\aleph_0}$ is the cardinality of the real numbers (and whether this equals $\aleph_1$ is the continuum hypothesis). But would $2^{2^{\aleph_0}}$ be of a higher cardinality than the cardinality of the real numbers?","['cardinals', 'elementary-set-theory']"
1800940,"Meromorphic, analytic, holomorphic and all that","I must have slept through something in my complex variables course, because all my life I have used the terms holomorphic, meromorphic, and analytic somewhat interchangeably.  These are all also related to regular functions. I have also thought of  ""entire"" and ""everywhere analytic"" as interchangeable terminology. What are the distinctions between these terms?  And what is the correct terminology for a function which may have poles but not essential singularities.
(For example, $$e^{-\frac{1}{z^2}}$$ is in some sense nastier at $z=0$ than $z^{-4}$)?",['complex-analysis']
1800964,Computation of $\int _{-\pi} ^\pi \frac {e^{in\theta} - e^{i(n-1)\theta}} {\mid \sin {\theta} \mid} d\theta .$,I need to compute $$\int \limits _{-\pi} ^\pi \frac {e^{in\theta} - e^{i(n-1)\theta}} {\mid \sin {\theta} \mid} d\theta .$$ Does anyone see any good strategy? Thanks.,"['definite-integrals', 'integration', 'trigonometry', 'analysis']"
1800975,Solving inhomogeneous PDEs when you can't separate variables,"$$4+U_y - U_{xy} =0, \quad U(x,0)=0, \quad U_y(0,y)=3y^2 .$$ Usually I can solve these kind of problems with separation of variables, so I tried
$$ U=XY, \quad U_y=XY',  \quad U_{xy}=X'Y' $$
$$ \Rightarrow 4+XY'-X'Y'=0$$
But, I can never separate variables X and Y at different sides of equation. And I dont know how else I can solve this.","['partial-derivative', 'ordinary-differential-equations', 'partial-differential-equations']"
1800978,Every bounded sequence of dual space contains a subsequence which is weak* convergent,"I were doing this problem in Functional Analysis of Erwin Kreyszig(part 4.9, problem 10, page 269), but got stuck in the last point to come to the conclusion. Can anyone give me some hint to move on? Thanks. Let $X$ be a separable Banach space and $M \subset X'$ a bounded set. Show that every sequence of elements of $M$ contains a subsequence which is weak* convergent to an element of $X'$. Here $X'$ is the space contains all bounded linear functionals on $X$. Let $\{T_n\} \subset X'$, then we said $\{T_n\}$ weak* converges to $T \in X'$ if $\lim_{n \rightarrow \infty}{T_n(x)} = T(x)$ for all $x \in X$. What I tried so far: Suppose we have a given sequence $\{T_n\} \subset M$. From the assumption, we've already have the sequence $\{||T_n||\}$ bounded, so from Corollary 4.9-7, we only need to find a subsequence $\{T_{n_i}\}$ of $\{T_n\}$such that the sequence $\{T_{n_i}(x)\}$ is Cauchy for every x in a total subset of $X$. Because $X$ is separable, there exists a dense countable subset of $X$, we call $S$. For each $x \in X$, because $\{||T_n||\}$ bounded, we must have $\{||T_n(x)||\}$ bounded for each $x \in S$, by Bozzano-Weirstrass, there exists convergent subsequence of this sequence, and obviously, the subsequence is Cauchy sequence. Here is the place I got stuck. If $S$ is finite, then we can repeat that process for the subsequence we just found, and finally, we have the subsequence $\{T_{n_i}\}$ of $\{T_n\}$ such that the $\{T_{n_i}(x)\}$ is Cauchy for all $x \in S$. But because $S$ is not finite, but countable, we can't do that. Is there any trick to overcome this difficulty? I really appreciate any help.","['functional-analysis', 'banach-spaces', 'weak-convergence']"
1801005,List of counter-examples to $\mathcal{L}_p(\mathbb{R})$ inclusions.,"Given $1 \leq p < q \leq \infty$, it is well-known that 
$$\ell_p \subseteq \ell_q$$ 
and that 
$\mathcal{L}_p(\mu) \supseteq \mathcal{L}_q(\mu)$
whenever $\mu$ is finite. However $\mathcal{L}_p(\mathbb{R})$  spaces with Lebesgue measure fall in no such categories. I was thus trying to find some counter-examples to the most common inclusion that one could think of. Here is my list so far and any help to check my solution and to extend it would be the most welcome : $\mathcal{L}_p(\mathbb{R}) \not\supseteq \mathcal{L}_q(\mathbb{R})$ for $1 \leq p < q < \infty$. Consider the function $f(x) := \frac{1}{x^{\frac{1}{p}}}\chi_{[1,\infty)}$. $\mathcal{L}_p(\mathbb{R}) \not\subseteq \mathcal{L}_q(\mathbb{R})$  for $1 \leq p < q < \infty$. Consider $f(x) := \sum\limits_{n=2}^\infty \left(\frac{1}{n\log^2(n)}\right)^{\frac{1}{p}}\chi_{[n,n+1)}$. $\mathcal{L}_p(\mathbb{R}) \not\subseteq \mathcal{L}_\infty(\mathbb{R})$  for $1 \leq p< \infty$.  Consider $f(x) := \frac{1}{x^{\frac{2}{p}}}\chi_{[1, \mathbb{R})} \in \mathcal{L}_p(\mathbb{R})$. $\mathcal{L}_p(\mathbb{R}) \not\supseteq \mathcal{L}_\infty(\mathbb{R})$ for $1 \leq p < \infty$. Consider $f(x) :\equiv 1$. $\mathcal{C}(\mathbb{R}) \not\subseteq \mathcal{L}_p(\mathbb{R})$ for $1 \leq p< \infty$. Consider $f(x) := x^\frac{1}{p}$. $\mathcal{C}(\mathbb{R}) \not\subseteq \mathcal{L}_\infty(\mathbb{R})$ for $1 \leq p < \infty$. Consider $f(x) := x$. Here is a function that is in $\mathcal{L}_p(\mathbb{R})$ and no other $\mathcal{L}_q(\mathbb{R})$ spaces :
$$\sum\limits_{n=1}^{\infty} \left(\frac{c_n}{x^{r_n}}\right)^{\frac{1}{p}}\chi_{[0,1]} + \sum\limits_{n=2}^\infty \left(\frac{1}{n\log^2(n)}\right)^{\frac{1}{p}}\chi_{[n,n+1)},$$
where $r_n := 1-\frac{1}{r^n}$ and $c_n := \frac{r_n}{\int_0^1 \frac{1}{x^{r_n}}}\; dx$.","['functional-analysis', 'normed-spaces', 'lp-spaces', 'examples-counterexamples']"
1801014,Proving Logical equivalence [5-26],"I have to prove a problem statement with logical equivalences but I seem to keep getting stuck. Here is the problem: $$ [(q \to p) \land \lnot p] \to (p \land q) \equiv p \lor q $$
Here is the work I have so far:
$$\equiv \lnot [(\lnot q \lor p) \land \lnot p] \lor (p \land q) $$
$$\equiv [( q \land \lnot p) \lor p] \lor (p \land q) $$
$$\equiv [( p \lor q) \land (p \lor \lnot p)] \lor (p \land q) $$
$$\equiv [( p \lor q) \land T] \lor (p \land q) $$
$$\equiv ( p \lor q) \lor (p \land q) $$ Any hints or pointing out mistakes I made so far would be great","['idempotents', 'logic', 'discrete-mathematics']"
1801039,Counting the numnber of (labelled and unlabelled) rooted trees on $n$ vertices with height $h$,"As far as I know, the number of labelled rooted trees on $n$ vertices is $n^{n-1}$. Is there a known result for counting the number of (labelled and unlabelled) rooted trees on $n$ vertices having height $h$? Note that these trees need not be binary. Also, I am looking for a closed form answer if possible, which will likely be a function of $n$ and $h$. In the worst case, I would like to know the asymptotic amount if an exact result isn't known.","['combinatorics', 'trees', 'reference-request']"
1801068,Lie bracket of $\mathfrak{so}(3)$,"I know that for $\mathfrak{so}(3)=\mathcal{L}(SO(3))$, the set of $3\times 3$ real antisymmetric matrices, we can define a basis
$$T^1=\begin{pmatrix}0&0&0\\ 0&0&-1\\ 0&1&0\end{pmatrix}\quad
T^2=\begin{pmatrix}0&0&1\\ 0&0&0\\ -1&0&0\end{pmatrix}\quad
T^3=\begin{pmatrix}0&-1&0\\ 1&0&0\\ 0&0&0\end{pmatrix}$$
such that
$$\left(T^a\right)_{bc}=-\epsilon_{abc}$$
and the Lie bracket is then
$$\left[T^a,T^b\right]=\epsilon_{abc}T^c$$
I can see that it works by explicitly putting in the matrices $T^a$ defined above, but how would one manipulate the $\epsilon$ symbols to show it formally?","['lie-algebras', 'proof-writing', 'group-theory', 'linear-algebra', 'lie-groups']"
1801106,How does one integrate $x^2 \frac{e^x}{(e^x+1)^2}$?,"How can I show this? $$ \int_{-\infty}^{\infty} x^2 \frac{e^x}{(e^x+1)^2} dx = \pi^2/3$$ I tried applying residuals, but the pole is of infinite(?) order.","['integration', 'definite-integrals', 'residue-calculus']"
1801109,Infinite differentiability of a function with a removable discontinuity,"How would I prove that $\frac x{e^x-1}$ is infinitely differentiable?
(This question came up since the No 1 answer in Maclaurin series for $\frac{x}{e^x-1}$ states that the function is infinitely differentiable, which - at least to me - isn't obvious at the first glance. Both the function and all its derivatives have a power of $e^x-1$ in the denominator which causes a (removable) discontinuity in $x=0$, but I suppose the answer can't be to look the removability of this discontinuity for all derivatives, right?)","['derivatives', 'limits', 'exponential-function', 'calculus', 'continuity']"
1801112,Simplest solution for differential equation,"Find the simplest solution: $y' + 2y = z' + 2z$ I think proper notation is not sure, y' means first derivate of y. ($\frac{dy}{dt}+ 2y = \frac{dz}{dt} + 2z$) $y(0)=1$ I got kind of confused, is $y=z=1$ a proper solution here? Or is disqualified because a constant is not reliant on time and something like $e^t$ is the simplest solution? You can choose z and y however you like.",['ordinary-differential-equations']
1801120,Can a elementary row operation change the size of a matrix?,"My question is very simple - Can an elementary row operation change the size (eg: $2\times2$ or $3\times 2$) of a matrix? I think the answer should be no, but while reading Linear Algebra by Hoffman Kunze I stumbled upon this: Definition. An $m\times n$ matrix is said to be an elementary matrix if it can be obtained from the $m\times m$ identity matrix by means of a single elementary row operation. Now, I know of 3 elementary row operations, (adding a multiple of one row to another, multiplying throughout a row by a non zero constant and interchanging two rows) but none of them can change the size of a matrix. But since this is a highly praised book I can't trust myself as much as I'd like to.","['matrices', 'linear-algebra']"
1801163,Spivak's Calculus on Manifolds - Statement of Lemma 2-10 is incorrect?,"In Spivak's Calculus on Manifolds , there is a Lemma 2-10 that is later used to prove the Inverse Function Theorem. Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that
  $| D_j f^i (x) | \leq M$ for all $x$ in the interior of $A$, then
  $$
|f(x)-f(y)| \leq n^2 M |x-y|
$$
  for all $x,y \in A$. To prove this lemma, we first write
$$
f^i(y)-f^i(x) = \sum_{j=1}^n (f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n)).
$$
Applying the mean-value theorem, we get
$$
f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n) = (y^j - x^j) \cdot D_j f^i(z_{ij})
$$
for some $z_{ij}$. Then, Spivak uses the hypothesis in the lemma to say that the right hand side has absolute value less than or equal to $M \cdot |y^j - x^j|$. But strictly speaking we cannot use this hypothesis yet, because it is possible that both $x$ and $y$ are boundary points such that $z_{ij}$ is also a boundary point. For example, $A \subset \mathbb{R}^2$ could be $[0,1]\times[0,1]$ and $x$ and $y$ could be $(0,0)$ and $(1,0)$, respectively. Then
$$
f^i(1,0) - f^i(0,0) = (f^i(1,0) - f^i(0,0)) + (f^i(1,0)-f^i(1,0))\\
\Rightarrow f^i(1,0) - f^i(0,0) = (1-0) \cdot D_1 f^i (z_{i1})
$$
where $z_{i1}$ lies on the $x$-axis and so is a boundary point of $A$. So, the lemma should be stated more accurately as follows: Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that
  $| D_j f^i (x) | \leq M$ for all $x$ in $A$, then
  $$
|f(x)-f(y)| \leq n^2 M |x-y|
$$
  for all $x,y \in A$. The same proof as earlier should work, provided $z_{ij}$ is appropriately chosen from $A \cup \text{boundary $A$}$. Note: Spivak has defined a function $f : A \subset \mathbb{R}^n \to \mathbb{R}^m$ to be differentiable, where $A$ is any subset, if $f$ can be extended to a differentiable function on some open set containing $A$. I would like to know whether my reasoning is accurate. Thank you in advance. EDIT: @rogerl has clarified my original question in the comments below. I would like to add another question though: is it necessary that $f$ be continuously differentiable? Can't it just be differentiable? To apply the Mean Value Theorem on an interval $[a,b]$ we only require that the function be continuous on $[a,b]$ and differentiable on $(a,b)$. The proof does not seem to require continuity of the derivative. The proof continues as follows:
$$
|f^i (y) - f^i (x)| \leq \sum_{j=1}^n |y^j - x^j | \cdot M \leq nM |y - x|
$$
since each $|y^j - x^j| < |y-x|$. Finally
$$
|f(y) - f(x)| \leq \sum_{i=1}^n |f^i (y) - f^i (x)| \leq n^2 M |y - x|.
$$","['multivariable-calculus', 'proof-verification']"
1801177,"If $f$ is differentiable in $(a,b)$ then $\frac{1}{f}$ is differentiable at $(a,b)$, provided $f(a,b)\neq0$","""Suppose that $f$ is a differentiable function at $(a,b)$. Prove that $\frac{1}{f}$ is differentiable in $(a,b)$, provided $f(a,b)\neq0$"" We were given the following definition of differentiability: a function is differentiable in $(a,b)$ if there exists a linear map $L$ such that $f(x,y)=f(a,b)+L_{(a,b)}\cdot(x-a,y-b)+r(x,y)$ with $\frac{r(x,y)}{|(x-a,y-b)|}\to0$. However, I still have no idea how to use this to prove the theorem. I guess you want to end up with something like $\frac{1}{f(x,y)}=\frac{1}{f(a,b)}+L_{(a,b)}\cdot(x-a,y-b)+r(x,y)$ with $\frac{r(x,y)}{|(x-a,y-b)|}\to0$, but I don't see how you would get there. Please help. Thanks in advance.","['multivariable-calculus', 'real-analysis', 'derivatives']"
1801190,"$a(n+1, k) = ka(n,k) + a(n,k-1)$","While working a combinatorial problem, I have encountered the recurrence relation $$a(n+1, k) = ka(n,k) + a(n,k-1)$$
where $a(0,0) = 1$ and $a(0,k)=0$ if $k \ne 0$. Except for the $k$ multiplier, this would be the same recursion as the binomials. Does anyone know about these numbers? Particularly if there is a non-recurring expression for them? Some facts I've noted: $$a(0,k) = 0, k>0$$$$a(n,k)=0, n < 0\text{ or }k<0\text{ or } n < k$$
Restricting to $n \ge 1, 1 \le k \le n$ $$a(n,1)= a(n,n) = 1$$ $$a(n,2) = 2^{n-1} - 1$$ $$a(n,n-1) = \frac{n(n-1)}{2}$$
and it appears that $p \mid a(p, k), 2 \le k < p$ for prime $p$, though I've not confirmed it.","['combinatorics', 'recurrence-relations']"
1801208,Area form for $M^2 \subseteq \Bbb R^4$,"We know that in general, given a orientable hypersurface $M^{n-1} \subseteq \Bbb R^n$, the volume form on $M$ is given by $$dM = \sum_{i=1}^n(-1)^{i-1}n_i\,dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n,$$where $(n_1,\cdots,n_n)$ is the unit normal to $M$. Moreover, we have $$n_i\,dM = (-1)^{i-1}dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n$$for tangent vectors. Another general case is for bi-dimensional surfaces in $\Bbb R^n$, for which in coordinates we can write using $\sqrt{EG-F^2}$. But I wanted to write something like the first expression for surfaces $M^2 \subseteq \Bbb R^4$, specifically. There is no standard choice of orthonormal basis for the normal space as far as I know. Is there a way to get around this, at least in this case? If there is, I'd guess it would adapt for submanifolds of codimension $2$, but that would be the cherry on the cake.","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
1801211,Infinite surface area,"I am reading an article (reference: http://www.jstor.org/stable/1971139?seq=1#page_scan_tab_contents ), and in the proof of the main theorem, the author states that ""it is a fact that complete, simply-connected surfaces in $\mathbb{R}^3$ of non-positive Gaussian curvature have infinite area"" without any reference. I was not able to prove this so far. How can one see this result? Is this fact known as a theorem?",['differential-geometry']
1801215,Equivalence of norm and weak topologies in finite dimensional space.,"I haven't solved a lot of problems on functional analysis, so I don't have intuition whether my idea of proof is correct or not. The weakness of weak-topology is obvious from the definition. To show another direction of the proof take an open ball $B(a,\epsilon )$ in norm topology. Since all norms are equivalent in finite dimensional spaces this could be a usual ball with standart metric. Let $y\in B(a, \epsilon)$ and $f:X\to \mathbb{R}$ s.t. $f(e)=1$ for any basis element of vector space. Then I can find $(u,v) \subset \mathbb{R}$ s.t. $y$ lies in weakly open set $f^{-1}(u,v)\subset B(a, \epsilon )$. I know that it is not a rigorous proof(if it is indeed a proof). I'm asking about correctness of idea itself.","['functional-analysis', 'weak-convergence', 'proof-verification']"
1801231,Why do we use both sets and predicates?,"For every set S we can define s as $$ \forall x:s(x) \iff x \in S$$, and for every predicate p we can define $$P:=\{x|p(x)\}$$. Operations and their properties correspond, etc. In every theorem or proof or definition, I believe, in principle, I could translate from set to predicate notation or vice versa. Would that change the meaning? It seems to me that the only differences are superficial ones: - syntax: xeP vs p(x)

- intuition: ""bag"" vs ""question"" I assume logic and predicates are more basic and necessary. Hence, why use sets besides predicates if they are essentially equivalent? Is the reason for using sets only for the sake of better intuition? Or because 1st order logic does not allow predicates on predicates, but sets allow us to circumvent that restriction? Or is it historical? Please consider in your answer that I only have highschool level math skills, in case that was not obvious from my question :)","['logic', 'elementary-set-theory']"
