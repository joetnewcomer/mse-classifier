question_id,title,body,tags
4568389,Dimension of the fixed space of stochastic matrices (reference request),"Let $A \in \mathbb{R}^{d \times d}$ be row stochastic (i.e., all entries of $A$ are $\ge 0$ , and each row sums up to $1$ . Let $G(A)$ denote the directed graph that is associated to $A$ , i.e., $G(A)$ has vertices $\{1, \dots, n\}$ , and there is an edge from $j$ to $k$ iff $A_{jk} > 0$ .
Let us call a non-empty subset $S \subseteq \{1, \dots, n\}$ a sink of $G(A)$ if, from every vertex of $G(A)$ , there is a path to at least one point in $S$ . Theorem. (a) One has $$
  \dim \operatorname{Fix}(A) 
  = 
  \min \Big\{ \lvert S \rvert : \, S \subseteq \{1, \dots, n\} \text{ is a sink of } G(A) \Big\},
$$ where $\operatorname{Fix}(A) := \ker(\operatorname{id} - A)$ denotes the fixed space of $A$ (in other words: the eigenspace of $A$ for the eigenvalue $1$ ). (b) If $S$ is a sink of $G(A)$ and $A_{jj} > 0$ for all $j \in S$ , then $A$ is aperiodic, i.e., $A$ does not have any eigenvalues in the unit circle except for $1$ . The theorem can, for instance, be proved by combining the classical Perron-Frobenius theorem with a bit of vector lattice theory (but it seems likely that one can find several different proofs). A few colleagues and myself would like to apply this result in an article. It seems very likely that this is known - so we would prefer giving a reference instead of including a proof -, but I couldn't find it in the classical textbooks on (nonnegative) matrices (more specifically, I checked in Gantmacher , Berman and Plemmons , and Minc ). Question: Do you know a reference that contains the theorem? Remark. The theorem also holds if the assumption that $A$ be row stochastic is replaced with the weaker assumption that $A$ has nonnegative entries and a fixed vector whose entries are all strictly positive. A reference for this more general result would be even better - but it can also reduced to the theorem above by a similarity transformation.","['nonnegative-matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'reference-request', 'matrices']"
4568412,Show that $\lim_{n \to \infty} \nu(T^{-n}A) = \mu(A)$,"The following is an exercise from my lecture notes on Ergodic Theory. I managed to solve part a), but I have doubts on one step in my solution of part b). Let $(X, \mathcal{F}, \mu)$ be a probability space and $T \colon X \to X$ a transformation. a) Let $\nu$ be a probability measure on $(X, \mathcal{F})$ that is equivalent to $\mu$ and let $f$ be the Radon-Nikodym derivative $f = \frac{d\mu}{d\nu}$ . Let $P_{T, \mu}$ and $P_{T, \nu}$ denote the Perron-Frobenius operators of $T$ with respect to the measures $\mu$ and $\nu$ . Profe that for any $g \in L^1(X, \mathcal{F}, \mu), $$
p_{T, \mu}(g) = \frac{P_{T, \nu}(fg)}{f}.
$$ (I was able to show this by first considering indicator functions and then extend to simple functions by linearity and using Monotone Convergence for general functions). b) Let $\nu$ be a probability measure on $(X, \mathcal{F})$ that is absolutely continuous with respect to $\mu$ . Assume that $\mu$ is $T$ -invariant and that $T$ is strongly mixing with respect to $\mu$ . Prove that $$
\lim_{n \to \infty} \nu(T^{-n}A) = \mu(A)
$$ holds for any $A \in \mathcal{F}$ . My idea for b) was to use the following fact: $T$ is strongly mixing if and only if for all $f \in L^1(X, \mathcal{F}, \mu)$ and $g \in L^\infty(X, \mathcal{F}, \mu)$ , $$
\lim_{n \to \infty} \int_X (P_T^n f) \cdot g d\mu = \int_X f d\mu \int_X g d\mu.
$$ Toward this end, i want to write $$
\nu(T^{-n}A) = \int_{T^{-n}A} \,d\nu = \int_A P_{T, \nu}^n(1) \,d \nu = \int_A P_{T^n, \nu}(1) \,d\nu = \int_X \frac{1}{f} P_{T^n, \mu}(f) \cdot \chi_A \,d\nu \\ \stackrel{*}{=} \int_X P_{T^n, \mu}(f) \cdot \chi_A \, d\mu,
$$ where in * i would like to use the fact that $\int_X h \, d\nu = \int_X h\cdot f \, d\mu$ . The problem is that I don`t know that $f$ is nonzero $\mu$ -a.e. I know it is nonzero $\nu$ -a.e., why writing the integral just before * is fine. If $\mu$ and $\nu$ were equivalent (as in a), then i would know that $f$ is in fact nonzero $\mu$ -a.e. and it should be fine. Is there a way to fix it without that assumption? Thanks! Let me know if I should provide additional definitions.","['measure-theory', 'ergodic-theory', 'lebesgue-integral', 'analysis', 'radon-nikodym']"
4568426,Limit of an integral (coming from random walks and networks) as $n\to\infty$,"I have the following integral on hand $$I_n=\int_{0}^\infty\frac1x\left[1-\left(\frac{x-1}{x+1}\right)^n\left(\frac{x-i}{x+i}\right)^{-n}\right]\ \mathrm dx$$ I am trying to find out what $\lim_{n\to\infty} I_n$ evaluates to. I have a sneaking suspicion that the limit is infinite (which comes from a heuristic argument based on the fact that a uniform random walk on $\mathbb Z^2$ is recurrent), but I am not sure if this particular integral would diverge. My heuristics is as follows. I model my lattice $\mathbb Z^2$ as a resistance network $G$ , and then look at square sublattices $G_n$ centered at the origin. Then $G=\lim_{n\to\infty} G_n$ . Let us look at a unit current flow, such that the voltage at the origin is $V$ and at the boundary is $0$ . Then the probability of a random walk starting at the origin 'escaping' to the boundary of $G_n$ before returning to the origin is (by a standard theorem in electric networks) given by $$\mathbb P_0=\frac{r(0)}{R^{(n)}_{eff}}$$ where $r(0)$ is the resistance at $0$ (here it is $1/4$ ) and $R^{(n)}_{eff}$ is the effective resistance between $0$ and the boundary of $G_n$ . Since the random walk in $\mathbb Z^2$ is recurrent, this probability must go to $0$ , and thus the effective resistance must diverge (That the effective resistance diverges is shown by the fact that $R_{eff}^{(n)}\ge \sum_{k=0}^{n-1}\frac{1}{2k+1}\to \infty$ , which is another method of showing the random walk is recurrent). If we now just try to evaluate the effective resistance between any 2 points $G$ physically, it quickly becomes a mess. Here's a nice walkthrough of how one goes about doing this. A more thorough mathematical treatment is in Probability on Trees and Networks by Russel Lyon and Yuval Peres , section 4.3. Also see this answer . The takeaway is that the effective resistance between the origin and any point $(m,n)\in\mathbb Z^2$ is given by $$R_{m,n}=\frac{1}{2\pi}\int_{x=0}^\infty\frac1x\left[1-\left(\frac{x-1}{x+1}\right)^{m+n}\left(\frac{x-i}{x+i}\right)^{m-n}\right]\ \mathrm dx$$ (see equation 14 in the walkthrough). I am looking at this integral with $m=0$ , and the constant factors stripped way to get my $I_n$ . Since the effective resistance should diverge however I look (this is probably not precise, but that is why I am calling this a heuristic), I am expecting this integral to diverge, but I am not sure to proceed. Any help is appreciated.","['integration', 'improper-integrals', 'network-flow', 'physics', 'convergence-divergence']"
4568447,"Prove that if the measure P is discrete, then its characteristic function $\varphi(\lambda)$ does not tend to zero as $\lambda \to \infty$","Prove that if the measure P is discrete, then its characteristic function $\varphi(\lambda)$ does not tend to zero as $\lambda \to \infty$ I don't know how to give a proof. But I try some discrete random variables and I find the characteristic function does not go to 0. For example, $X=1$ or $X=-1$ with probabilities $1/2$ . Then I find the characteristic function us $\cos t$ , which is periodic. Thus it cannot go to 0. However, I don't know how to prove the general statement above.","['probability-theory', 'probability']"
4568469,Functions whose distributional second derivative is finite,"Let $f\in L^1(\mathbb{R})$ (not necessarily continuous) such that $$\operatorname{sup} \left\{\int\limits f(x)\phi'(x)dx: \phi\in C_c^1(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\} < \infty.$$ In otherwords, distributional derivative of $f$ is a finite Radon measure.
Then such functions satisfy the following $$\int\limits_{\mathbb{R}}|f(x+h)-f(x)|\leq Ch  \quad \text{for all } h\geq 0.\label{1}\tag{1}$$ Similarly, can we characterise functions in some $L^p(\mathbb{R})$ which satisfy \begin{eqnarray}
\int\limits_{\mathbb{R}}|f(x+h)-2f(x)+f(x-h)|dx \leq Ch^2 \label{2}\tag{2}
\end{eqnarray} for all $h\geq 0?$ For example, suppose we assume $$\operatorname{sup} \left\{\int\limits f(x)\phi''(x)dx: \phi\in C_c^2(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\}< \infty,$$ then do we get \eqref{2}? P.S.: Functions in $C_c^1(\mathbb{R})\cap W^{1,\infty}(\mathbb{R})$ and $C_c^2(\mathbb{R})\cap W^{2,\infty}(\mathbb{R})$ satisfy \eqref{1} and \eqref{2} respectively. But I am looking for a more general class of functions(not necessarily continuous).","['measure-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4568501,Multivariate differentiable function on a compact is a Lipschitz function?,"Let $n,m\in\mathbb{N}$ , such that $E=(\mathbb{R}^n,\|\cdot\|_E)$ and $F=(\mathbb{R}^m,\|\cdot\|_F)$ with $\|\cdot\|_E$ and $\|\cdot\|_F$ two norms (we do not care which one since $n,m\in\mathbb{N}$ ). I have a little doubt about the ""lipschitzness"" of a mapping $f$ . Assuming $f:E\rightarrow F$ , differentiable over $E$ (thus continuous over $E$ ). If $C\subset E$ is compact, then $f$ is $k$ -lipschitz for a given $k>0$ over $C$ , right ? If $n=m=1$ , I know that is true, but I didn't find the same result in multivariate fashion (maybe because it is too obvious) and I did not go further on this question since it's been a while I do not have pratice math.
Since $\mathbb{R}^n$ has very good topological properties, I think that this result is true. Am I wrong ? Thanks ! Edit : Nice counter examples below ! The aforementioned statement is right only if $f$ is $C^1$ .","['multivariable-calculus', 'lipschitz-functions']"
4568533,Isometries of direct sums of Hilbert spaces,"If $H$ is a separable Hilbert space, then for any norm one vectors $x$ and $y$ we can find an surjective isometry $U$ such that $Ux=y$ . Is the same true for $H\oplus H$ , that is, the direct sum in $l_1$ sense? This space is isomorphic, but not isometric with $H$ , so I am not sure the property still holds. Of course, if we consider the direct sum $\oplus_2$ in $l_2$ sense, then the space is isometric to $H$ and the property again holds. I suspect it is not true, but I cannot come up with a pair of points for which it fails.","['hilbert-spaces', 'banach-spaces', 'isometry', 'functional-analysis']"
4568553,What's the maximum order of an element in $SL_2(\mathbb{Z} /p\mathbb{Z})$ for $p>2$ prime?,"I know the answer is $2p$ as I've checked it for $p=3,5$ and $71$ .
The characteristic polynomial of a matrix $A\in$ $SL_2(\mathbb{Z} /p\mathbb{Z})$ is $P_A(x)=x^2-tr(A)x+1$ , so if this polynomial has a solution, the matrix can be diagonalized.
I've studied all of the cases when a matrix is diagonalizable: $A$ has double eigenvalues $1$ or $-1$ of multiplicity $2$ , so it's similar to a matrix $\left(\begin{matrix} \pm 1 & 0 \\ 0 & \pm 1 \end{matrix}\right)$ , which have respectively orders $1$ or $2$ . $A$ two distinct eigenvalues $\omega$ and $\omega ^{-1}$ , with $\omega \in (\mathbb{Z}/p\mathbb{Z})^*$ , so it's similar to $\left(\begin{matrix} \omega & 0 \\ 0 & \omega ^{-1} \end{matrix}\right)$ , with order $p-1$ . $A$ has a double eigenvalue $1$ of multiplicity $1$ , so it's similar to $\left(\begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix}\right)$ , of order $p$ . $A$ has a double eigenvalue $-1$ of multiplicity $1$ , so it's similar to $\left(\begin{matrix}  -1 & 1 \\ 0 & -1 \end{matrix}\right)$ , of order $2p$ . This covers every case except when $P_A(x)$ has no solution. So, I'd just need to prove that, in that case, the order is $\le 2p$ .","['general-linear-group', 'modular-group', 'linear-algebra', 'group-theory', 'diagonalization']"
4568635,Subgroups of $S_n$ that are automorphism groups of a graph with $n$ vertices.,"Can one find all the subgroups of $S_n$ that are automorphism group of a graph with $n$ vertices? I know that every subgroup of $S_n$ can be the automorphism group for a graph with enough vertices via Cayley's graphs. But I can't find anything imposing the restriction that the graph must have $n$ vertices. I know that the result is false in general, e.g. there's no graph with 3 vertices such that $Aut(G)\cong C_3$ the $3$ -cycle. I was thinking that maybe one can find conditions on the generators of the automorphism group of a graph, so maybe one can find a familiy of subgroups of $S_n$ that have the property or something like that.","['graph-theory', 'group-theory', 'finite-groups', 'automorphism-group']"
4568645,Find an explicit solution of this equation,"Let $\alpha \in (0,1)$ . Is it possible to solve  the equation $$\cos{x}=\frac{\alpha}{\pi}\int_{0}^{\infty}\frac{\cos{(xy)}}{y^{1-\alpha}(y^{2\alpha}+1)}dy\quad (1).$$ It is very easy to compute $$\int_{0}^{\infty}\frac{1}{y^{1-\alpha}(y^{2\alpha}+1)}dy=\frac{\pi}{2\alpha}.$$ Simply change variables $y^{\alpha}\rightarrow y$ .
This shows that (1) has a solution, since the modulus of the right side is bounded by $\frac{1}{2}$ . In fact the equation has infinitely many solutions by the periodicity of the cosine function. But, I don't know how to solve it.","['integration', 'ordinary-differential-equations', 'real-analysis', 'complex-analysis', 'contour-integration']"
4568680,Can the second integral of $x^x$ be expressed in terms of the first integral and standard mathatical functions?,"Note : by elementary I also mean functions like $\operatorname{Li}(x)$ and $\operatorname{Erfi}(x)$ . Edit: This is not a duplicate. I am not asking if the integral of $x^x$ is elementary. Im asking if the second integral of $x^x$ can be expressed in terms the first + normal mathematical functions. I know the first integral of $x^x$ is not elementary. Please stop directing me to those resources. I decided one day to experiment with inventing a new special function, $\DeclareMathOperator{Ti}{Ti}\Ti_2(x)$ (Tetrational Integral), defined as $$\DeclareMathOperator{Ti}{Ti}
\Ti_2(x)=\int_0^x t^tdt.
$$ With this new function, any function of the form $f(x)^{f(x)}f'(x)$ can be integrated as $\Ti_2(f(x))$ . In addition, $e^{W(\ln(x))}$ can be trivially integrated, as it is the inverse function of $x^x$ . I have tried finding $$\int \Ti_2(x)dx.$$ But performing integration by parts requires finding $$
\int x^{x+1}dx = \int x\cdot x^xdx,
$$ which expands infinitely. By performing an integration by substitution and put $u = \ln(x)$ , (expanding $x\cdot x^x$ to $xe^{x\ln(x)}$ ). I get $\int \Ti_2(x)dx= \int e^{u^2}e^{ue^u}du,$ but I have had no progress after that. It will most likely involve $\operatorname{Erfi}(x)$ due to the presence of the $e^{u^2}$ part, I also know this can be reduced to $x^2x^{x-1}$ which I recall making some more actual progress with, but not a whole bunch. And i don't remember a tone of specifics on that right now. Is it even possible to integrate this in terms of elementary functions (plus common special functions of a single variable like $\operatorname{Li}(x)$ ) and $\Ti_2$ itself? Can someone give me a proof that it is not or is? If not are there any known functions whatsoever this can be done in terms of? (Hyper-geometric functions for example). Finally, as related question can $\Ti_2(x)$ itself (not of its integral) be expressed in terms of generalized hyper-geometric functions or other related functions?","['integration', 'calculus', 'tetration']"
4568709,Australian Math Competition Problem,"I am not really sure what to tag this question as, so please forgive me for wrongly categorising it. The question is as follows:
The first number in a list is 2. After that, each number is calculated by adding the digits of the previous number together and squaring the result. What is the 2022nd number in the list? If I have not misunderstood, the beginning of the list would be [2, 4, 16, 49, 169...]. Please assist me in solving this problem, as I have no idea how to calculated the nth number in this.","['contest-math', 'sequences-and-series']"
4568715,Solution to $2^x=x!$,"I was trying to solve the equation for $x!=2^x$ , where $x\ge0$ . I plotted it on Desmos and found two solutions for the same. Attaching image for reference. Graph plot of $2^x$ and $x!$ As per the plot, there are two solutions for the equation. But I am only able to derive the solution $x=0$ . (Through observation and guesswork). How can the second solution be derived ( $x\approx3.46$ )? I couldn't figure it out! It looks so simple, yet its quite a bummer actually(at least for me). Can someone please help regarding the same? Thanks in advance!","['functions', 'factorial', 'real-analysis']"
4568732,Determine for each possible values of x if series converges or diverges. Identify interval of convergence and the radius of convergence.,"Here is the math problem: $$\sum^\infty_{n=1}\sin\left(\frac{2n\pi+1}{4}\right)\frac{x^n}{n^5}$$ I'm trying to find the interval of convergence and radius of convergence for this problem. Usually, I'd apply either the ratio test, or root test, then pull out all the terms with an $x$ , then calculate the limit of the terms surrounding an $n$ . However, in this case, with the $\sin$ , I'm not sure how to. I'd share what I have done so far, except, I just don't know where to start. Any help would be appreciated, thanks. If it helps, we have not yet reached Maclauren or Taylor series in our maths class yet.","['calculus', 'convergence-divergence', 'trigonometry', 'sequences-and-series']"
4568752,Do tensors in the tangent space act on functions and vectors?,"I know via isomorphism we may treat the tangent space of a point on a manifold as the vector space of derivations on functions at that point. I.e. we can give the tangent space the basis of partials: $$
v=v^i \partial_i 
$$ This allows vectors to act on functions, and also on eachother via the Lie bracket. My question is, for tensor products of the tangent space, can we still act on function or vectors? For a 2-tensor $T$ : $$
T= T^{ij} \partial_i \otimes \partial_j
$$ Can $T$ be made to act on functions or vectors in an unambiguous way?","['lie-algebras', 'tangent-spaces', 'derivatives', 'tangent-bundle', 'differential-geometry']"
4568759,"If in a $\Delta ABC$, $\sqrt 3 \sin C = 2\sec A - \tan A$ and $\angle C = {\lambda ^0}$, find the value of $\lambda$","If in a $\Delta ABC$ , $\sqrt 3 \sin C = 2\sec A - \tan A$ and $\angle C = {\lambda ^0}$ , find the value of $\lambda$ My approach is as follow $\sqrt 3 \sin C = 2\sec A - \tan A \Rightarrow \sqrt 3 \sin C = \sec A + \sec A - \tan A$ $\sqrt 3 \sin C = \sec A + \left( {\frac{{{{\sec }^2}A - {{\tan }^2}A}}{{\sec A + \tan A}}} \right) \Rightarrow \sqrt 3 \sin C = \sec A + \left( {\frac{1}{{\sec A + \tan A}}} \right)$ $\sqrt 3 \sin C = \sec A + \left( {\frac{{\cos A}}{{1 + \sin A}}} \right) \Rightarrow \sqrt 3 \sin C = \frac{1}{{\cos A}} + \left( {\frac{{\cos A}}{{1 + \sin A}}} \right) \Rightarrow \sqrt 3 \sin C = \frac{{1 + \sin A + 1 - {{\sin }^2}A}}{{\cos A\left( {1 + \sin A} \right)}}$ $\sqrt 3 \sin C = \frac{{2 + \sin A - {{\sin }^2}A}}{{\cos A\left( {1 + \sin A} \right)}}$ Not able to proceed further",['trigonometry']
4568778,How to Evaluate the Integral? $\int_{0}^{1}\frac{\ln\left( \frac{x+1}{2x^2} \right)}{\sqrt{x^2+2x}}dx=\frac{\pi^2}{2}$,"I am trying to find a closed form for $$
\int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right)
{{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,}.
$$ I have done trig substitution and it results in $$
\int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right)
{{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,} =
\int_{0}^{\pi/3}\sec\left(\theta\right)
\ln\left(\frac{\sec\left(\theta\right)}
{2\left[\sec\left(\theta\right) - 1\right]^{\,2}} \right){\rm d}\theta
$$ which doesn't help. By part integration with $\displaystyle u = \ln\left(\frac{x + 1}{2x^{2}} \right)$ , $\displaystyle\,\,{\rm d}v=\frac{\displaystyle\,\,{\rm d}x}{\,\sqrt{\,{x^{2} + 2x}\,}\,}$ also makes it more complicated. I appreciate any help on this problem.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'polylogarithm']"
4568838,Illuminated fraction of Moon's surface ratio question,"On page 345 of Astronomical Algorithms by Meeus, he states that the illuminated fraction $k$ of the Moon's disk is given by the ratio $BC:AC$ (see diagram) where $BC$ is the width of the illuminated crescent and $AC$ is the Moon's diameter. He then goes on to say that this is the same ratio as the crescent's area ( $NCSB$ ) to the total disk area ( $NCSA$ ). The illuminated fraction is given by $$k=\frac{1+\cos i}{2},$$ where $i$ is the phase angle, the angle between the Sun and Earth as seen from the centre of the Moon. Now, I can understand (and derive) why $k$ is the ratio of the lengths $BC:AC$ , but I cannot see why it is also the ratio of the areas. I've looked at the (complicated, to my eyes) formula for the area of a lune (a crescent shape), given in Wikipedia and can't make the link between it and the total disk area. Thanks.","['trigonometry', 'ratio']"
4568878,"Let G be a non abelian group with a square-free order. Prove that there are two elements $a,b\in G$ such that $ab\neq ba$ and $ord(a)=ord(b)$.","Let $G$ be a non abelian finite group of order $p_1p_2p_3\cdot...\cdot p_n$ where $n\in \mathbb{N}, n\geq2$ and $p_i$ is a prime for every $i$ . Prove that there are two elements $a,b\in G$ such that $ab\neq ba$ and $ord(a)=ord(b)$ . My idea for this problem was to firstly assume the contrary, and then to look at all the sets like $M_a$ that contains all elements from $G$ of order $a$ because a element from $M_a$ commutes with any other member of $M_a$ .","['group-theory', 'abstract-algebra', 'abelian-groups', 'prime-numbers']"
4568884,What are the conditions on the points that make up a tetrahedron?,"So I have the following question to solve. For which $a\in\mathbb{R}$ are the points $P = (−1, 2, 1)$ , $Q = (−11, 0, 1)$ , $R = (a, −1, 4)$ and $S = (0, 1, a)$ vertices in a tetrahedron? Determine the area of the triangle $P QR$ and the volume of the tetrahedron $PQRS$ for such values of $a$ . (Positively oriented
ON-system assumed.) The question above seems weird since I thought that four points make up a tetrahedron if and only if the points are not coplanar. So for the question above I just need to find the values $a$ for which the four points are coplanar and for every other value of $a$ we have a tetrahedron. But this gives infinite values $a$ so I'm thinking maybe my assumption about the necessary and sufficient condition for four points to make up a tetrahedron is wrong.","['linear-algebra', 'geometry']"
4568937,Prove that if the intersection of $n+1$ distinct spheres is non-empty then it has at most $n$ elements in $\mathbb{R}^n$,"Let $C \in \mathbb{R}^n$ , $D > 0$ , and $C_1,\ldots,C_{n+1}$ some distinct points with $\|C - C_k\| = D$ . Consider the balls $\mathcal{B}(C_k,r_k)$ for some $r_k > 0$ . Can we prove that $$\left| \bigcap_{k=1}^{n+1} \partial \mathcal{B}(C_k,r_k) \right| \leq n$$ i.e the intersection has at most $n$ elements? Later edit: It might be true even for an intersection of $3$ such spheres ...","['euclidean-geometry', 'linear-algebra']"
4568962,Find example: $\lim_{h\to0}\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h}\ \ \text{exists }\ \ \not\!\!\!\implies\ f'(x_0)\ \ \text{exists}$,"Assume that $f:[a,b]\to\mathbb R$ is a continuous function. For $x_0\in (a,b)$ and $\alpha,\beta>0$ , we define the (asymetric) difference quotient $$(\Delta_{\alpha,\beta,h}f)(x_0):=\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h},\qquad h\neq 0.$$ Given $\alpha,\beta>0$ with $\alpha\neq \beta$ , I want to find an example of $f$ such that $$\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists}.$$ This is a problem that I came up with while I'm preparing my recitation class as a teaching assistant in Analysis (I) course. If $f'(x_0)$ exists, we can prove that $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)$ by Taylor's theorem: It follows from $f(x_0+\alpha h)=f(x_0)+\alpha f'(x_0)h+o(h)$ and $f(x_0-\beta h)=f(x_0)-\beta f'(x_0)h+o(h)$ that $(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)+o(1)$ as $h\to0$ , hence $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)$ . Now, a natural question appears: What about the reverse problem? If $\alpha=\beta$ , then for $f(x)=|x|$ and $x_0=0$ we have $(\Delta_{\alpha,\beta,h}f)(0)=0$ for all $h\neq0$ , but $f'(0)$ doesn't exist. However, for $\alpha\neq \beta$ , I failed to find an example. I found that if $\alpha\neq \beta$ , and if $f_-'(x_0)$ , $f_+'(x_0)$ both exist, then we must have $$\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \implies\ f'(x_0)\ \ \ \text{exists}.$$ So, for any example in which $\alpha\neq \beta$ and $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists}$ , there must be one of $f_-'(x_0)$ and $f_+'(x_0)$ that doesn't exist. (Note that $f$ should be continuous in this post.) Any help would be appreciated!","['calculus', 'derivatives', 'analysis', 'real-analysis']"
4568986,Why is abstract algebra named algebra?,"Why is abstract algebra named like the algebra we see in high school ? They have nothing in common. Also, why are group, rings and fields named algebraic structure ?","['abstract-algebra', 'soft-question', 'reference-request']"
4569120,Is this proof that $e^z$ is transcendental correct?,"Lemma .  If $p(z)$ is a non-zero polynomial and $k\geq 1$ a natural number, then there exists a polynomial $q(z)$ of the same degree such that $\frac{d}{dz}\Big(p(z)\cdot e^{kz}\Big) = q(z)\cdot e^{kz}$ .  (Indeed, \begin{align}
\frac{d}{dz}\Big(p(z)\cdot e^{kz}\Big) &= p'(z)\cdot e^{kz} + kp(z)\cdot e^{kz} \\
&= e^{kz}\Big(p'(z) + kp(z)\Big),
\end{align} and $p'(z) + kp(z)$ has the same degree as $p(z)$ .) $\quad\square$ Recall that $f(z)$ is an algebraic function iff there exists a polynomial $F(z, y)\in\mathbb{Q}[X, Y]$ such that $F(z, f(z))\equiv 0.$ Assume that $e^z$ is algebraic and let \begin{equation}
F(z, y) = \sum_{k=0}^n p_k(z)y^k = p_0(z) + \sum_{k=1}^n p_k(z)y^k,\quad p_n(z)\not\equiv 0,
\end{equation} be a polynomial of minimal degree in $y$ such that $F(z, e^z) \equiv 0$ . Now, $p_0(z)$ cannot be the zero polynomial, or else $F(z, y) = y\cdot G(z, y)$ , where $G$ is a polynomial of strictly lesser degree in $y$ than $F$ such that $G(z, e^z)\equiv 0$ (this because $e^z$ vanishes nowhere), contradicting the minimality of $deg_y(F)$ .  So call $d:= deg(p_0\big(z)\big)$ and take $\frac{d^{d+1}}{dz^{d+1}}$ of both sides of the equation $F(z, e^z) \equiv 0$ .  By linearity of derivative, the lemma, and the fact that the $(d+1)^{th}$ derivative of a degree $d$ polynomial is identically zero, this results in the equation \begin{equation}
\sum_{k=1}^n q_k(z)e^{kz} \equiv 0,
\end{equation} where $\deg(q_k) = \deg(p_k)$ for all $k\geq 1$ .  We may factor $e^z$ out of this equation to obtain \begin{equation}
e^z\cdot\sum_{k=0}^{n-1} q_{k+1}(z)e^{kz} \equiv 0,
\end{equation} and since $e^z$ vanishes nowhere this equality implies that \begin{equation}
\sum_{k=0}^{n-1} q_{k+1}(z)e^{kz} \equiv 0.
\end{equation} But this implies that $\deg_y(F)$ was not minimal after all, contradiction.  So $e^z$ must be transcendental, ce qu'il fallait démontrer . $\clubsuit$","['complex-analysis', 'transcendental-functions', 'exponential-function']"
4569153,How does changing a bilinear form change its associated orthogonal group?,"I'm trying to understand bilinear forms and how they can be 'equivalent' and how this 'equivalence' translates to their associated orthogonal groups. Fix a field $F$ . I'm comfortable translating between a quadratic form $Q$ (homogenous degree $2$ polynomial in $\ell$ variables), its bilinear form on $F^\ell$ $B_Q$ , and the symmetric $\ell \times \ell$ matrix $A_Q$ . Each of these 'representations' has a group of matrices which preserve them, for example $SO(Q,F) := \{g \in SL_(\ell,F)  |  Q(gv) = Q(v) \; \forall v \in F^\ell\}$ . Each of these representations has an associated group of matrices preserving it and I can show these groups are literally equal. On the other hand, there are operations on the vector space that seem to change the bilinear form into things more pleasant to deal with ('diagonalizing the quadratic form') and some notation ( $SO(p,q)$ ) that seems to indicate that all of these are determined by the 'signature' of the quadratic form, etc. It seems like this should be explained by changing the underlying basis for the vector space, but I can't quite see it. Since my context is reading about these orthogonal groups as Lie groups (Morris Arithmetic Groups), I'd like to know how changing the form affects the group and why. It seems like it should be conjugation by a change of basis matrix, but again, I can't really explain why or find a reference. Explicitly, I'm looking for an explanation of how/when quadratic forms can be considered 'equivalent' (for example, why does the signature characterize them?), what effect that has on the orthogonal groups, and hopefully how it translates between the various representations listed above. I know that's a lot, so a reference would also be good.","['reference-request', 'bilinear-form', 'linear-algebra', 'lie-groups', 'quadratic-forms']"
4569187,Limit as $x \to 0^-$ of $x^x$ from the negative side using complex numbers,"We know that $\lim_\limits{x \to 0^+}x^x = 1$ . We also know that the limit as $x \to 0^-$ does not exist, at least for $x \in \mathbb{R}$ . When considering complex numbers, for $z\in \mathbb{C}$ , does the following limit converge? $$\lim_\limits{z \to 0^-}z^z$$ My attempt: Separate $z$ into its real and imaginary components, making this a multivariable limit. Let $\text{Re}(z)=\sigma,$ and $\text{Im}(z) = t$ . Thus, we have $$\lim_\limits{(\sigma, t) \to 0^-}(\sigma + it)^{\sigma+it}$$ But I'm not sure where to go from here.","['limits', 'complex-numbers', 'analysis']"
4569215,Is the number satisfying $\eta=\sin(\cos(\eta))$ transcendental?,"I was graphing the function $\sin(\cos(\sin(\cos(\sin(\cos...$ when I realized it started to flatten out. This meant that this approaches a constant. Since the sine and cosine repeat, we can make a finite equation for the number that I will call $\eta$ : $$\eta=\sin(\cos(\eta))$$ Is this transcendental? I know that $\sin$ and $\cos$ are transcendental functions, but I am not sure that the composition of two transcendental functions gives a transcendental number.","['trigonometry', 'transcendental-numbers', 'transcendental-equations']"
4569225,"show that $\log(1+e^z) = \log(1+e^{-|z|}) + \max(0, z)$ and explain why the right latter is more numerically stable","I used 2 separate cases. First case is when $z < 0$ and second case $z \geq 0$ . I proved the first case but I have no clue how to deal with the second. prove that $\log(1+e^z) = \log(1+e^{-|z|}) + \max (z, 0)$ : we use separate cases. first case, when $ z < 0 $ $\log(1+e^z)$ $\operatorname{sign}(z) = -1$ because $z < 0$ therefore $z = -|z|$ $\log(1+e^z) = \log(1+e^{-|z|})$ $z < 0$ therefore $\max (z, 0) = 0$ $\log(1+e^z) = \log(1+e^{-|z|}) + \max (z, 0)$ when $ z \geq 0$ $\max (z, 0) = z$ $\log(1+e^z) = \log(1+e^{-|z|}) + z$ $\log(1+e^z) - \log(1+e^{-|z|}) = z$ $\log(\frac{1+e^z}{1+e^{-|z|}}) = z$ am I on right path for the second case? how to prove the identity?","['machine-learning', 'algebra-precalculus', 'numerical-optimization']"
4569319,Alternative proof that maximal abelian normal subgroups of a nilpotent group are self-centralizing,"I was writing lecture notes on nilpotent groups, and wanted to show the following well-known basic result. Theorem : Let $G$ be a nilpotent group and let $A$ be an abelian normal subgroup of $G$ , maximal amongst such subgroups. Then $A=C_G(A)$ . The standard proof of this is easy: certainly $C=C_G(A)$ contains $A$ , so if this is proper then consider $C/A$ . As this is a non-trivial normal subgroup of $G/A$ it intersects the centre non-trivially, so let $x\neq 1$ be in this intersection. The subgroup $\langle A,x\rangle$ is abelian, since $A$ commutes with $x$ , and normal since $\langle A,x\rangle/A$ is central, hence normal in $G/A$ . Since $A$ is maximal, this yields a contradiction. Now, the problem is that when I was writing this result, I had not yet proved that normal subgroups intersect the centre non-trivially, so I tried to come up with a proof that did not use that. In the end I gave up and rearranged the results so the other one came first. But is there a proof that doesn't essentially prove the normal subgroups result along the way? (Note that I am not assuming that $G$ is finite, or even finitely generated.)","['alternative-proof', 'nilpotent-groups', 'group-theory', 'normal-subgroups']"
4569322,Direct sum of two Hilbert spaces is a inner product.,"Today I am working with Functional Analysis regarding Hilbert Spaces from the notes. Let $H_1$ and $H_2$ be Hilbert Spaces. The direct sum $H_1\oplus H_2$ is the vector spaces $H_1\times H_2$ with the following inner product, $$\langle (x,y)|(x',y')\rangle:=\langle x|x' \rangle_{H_1}+\langle y|y' \rangle_{H_2} ,$$ where $(x,y),(x',y')\in H_1\times H_2 $ . Well I know the definition of an inner product, i.e. linearity, symmetry, and positive definiteness however I cannot see how to do it with this direct sum. Anyone who can give me a hint or maybe show one of them and then I can try to work the rest out. Thanks in advance. Notice this is NOT homework. It is just me that wants to convince myself that this is a inner product. Edit: I've shown that this indeed defines an inner product. How about norm? Is it true that the norm of $H_1\oplus H_2$ is $ \|\langle x,x'\rangle,\langle y,y'\rangle\|=(\|\langle  x,x' \rangle\|^2+\|\langle y,y' \rangle\|^2)^\frac{1}{2}$","['hilbert-spaces', 'direct-sum', 'functional-analysis', 'operator-algebras']"
4569333,Troubles on the domain of this two variables function.,"What's the domain of ? $$f(x, y) = \sqrt{\ln(x) + \ln(y)}$$ So, first of all I thought: I cannot write the argument of the root as $\ln(xy)$ since this propert only applies if $x >0$ and $y >0$ . In writing it as $\ln(xy)$ I then would be allowed to have both $x$ and $y$ negative. Then: surely $x>0$ and $y>0$ is required. But also $\ln(x) > -\ln(y)$ that is $x \geq 1/y$ .
Clearly this bonds with the condition $y > 0$ . So am I right in saying $$\Omega = \{(x, y)\in\mathbb{R}^2: x>0, y>0, x\geq 1/y\}$$ or am I missing something?","['multivariable-calculus', 'calculus', 'analysis']"
4569372,Prove for every $R>0$ there's an integer $n_0>0$ such that if $n\geq n_0$ then $f_n$ has no zeroes. Using Hurwitz Theorem.,"Let $$f_n(z)=\sum_{k=0}^n \frac{z^k}{k!}$$ and let $$f(z)=\sum_{k=0}^\infty \frac{z^k}{k!}=e^z.$$ As a polynomial $f_n$ has $n$ roots in $\Bbb{C}$ . Prove for every $R>0$ there's an integer $n_0>0$ such that if $n\geq n_0$ then $f_n$ has no zeroes in $B_R(0)$ . So I want to appeal to Hurwitz's theorem that States if $f_n \rightarrow f$ uniformly on every compact subset of $\Bbb{C}$ which the $f_n$ do converge uniformly to $e^z$ on every compact subset, then if $f$ has a zero at $z_0$ of order $m$ so do the $f_n$ . Then since $f(z)$ has no zeroes, can I conclude the $f_n$ 's have no zeroes? Or do I need to find such an $n_0$ .",['complex-analysis']
4569404,Combinatorial structure of the tesseract,"On a cube, you can define the notion of ""the next edge around this vertex, in clockwise order."" Formally, if $D$ is the space of darts (edges of the cube with one of the endpoint vertices marked as special), this ""next edge"" function is a permutation $\sigma:D\rightarrow D$ that keeps the special vertex the same. If you also let $\alpha:D\rightarrow D$ be the involution that flips which endpoint is considered special, you get a map $\alpha\circ \sigma$ whose orbits make a nice picture: Now I am trying to do the same thing on a tesseract, but I don't know how to define the ""next edge around this vertex"" function $\sigma:D\rightarrow D$ .  I believe the tesseract is orientable, so it should be possible to choose a $\sigma$ with the right combinatorial properties (even though there's no cross product in 4D). According to my math*, the resulting map $\alpha\circ \sigma$ should have eight orbits (compared to the cube's four orbits shown above). My question is this: Given an edge of the tesseract with one endpoint marked as special, how do I define ""the next edge"" with that same endpoint? More precisely, I think my question is: Question : If $D$ is the space of darts (edges of the tesseract with one of the endpoints marked as special), and $\alpha:D\rightarrow D$ is the involution that changes which endpoint is special, is there a permutation $\sigma:D\rightarrow D$ such that: The cycles of $\sigma$ are exactly the sets of darts with a common special endpoint. Intuitively, $\sigma$ acts like it ""pivots"" darts around their special endpoint. The orbits of $(\alpha \circ \sigma^{-1} \circ \alpha \circ \sigma)$ all have the same size, and there are 2*8 = 16 orbits. (I suppose this means that since there are 32 edges = 64 darts, each orbit is a cycle of 4 points.) Those are the two main constraints. A bonus constraint, sort of difficult to articulate formally, is that I would expect $\sigma$ to interact nicely with the symmetries/rotations of the tesseract. (I think I mean that for every rotation $\rho$ of the tesseract, $\rho^{-1}\sigma\rho = \sigma$ .) I am hoping for a concrete answer, perhaps even in terms of a specific tesseract such as $\langle \pm 1, \pm 1, \pm 1, \pm 1\rangle$ . I would like to understand how unique the answer is---but it is ok if the answer is not unique as long as it has the required combinatorial structure. * Note on my math: If $T$ is the Tutte polynomial of the tesseract graph, then $1+\log_2 |T(-1,-1)|$ is the number of strands in the knotwork, i.e. the desired number of orbits of this permutation $\alpha\circ\sigma$ .","['knot-theory', 'polytopes', 'combinatorics', 'orientation']"
4569426,"Does ""achieving more GH-distances than some compact space"" imply compactness?","For complete metric spaces $X,Y$ , write $X\trianglelefteq Y$ iff for every complete metric space $Z$ such that the Gromov-Hausdorff distance between $X$ and $Z$ is achieved by some embedding, the Gromov-Hausdorff distance between $Y$ and $Z$ is also achieved by some embedding. For example, this answer by James Hanson shows that the one-point space is strictly $\triangleright$ a two-point space. I'm mostly interested in the interaction between $\trianglelefteq$ and compactness. Specifically, suppose $X\trianglelefteq Y$ and $X$ is compact. Must $Y$ be compact as well? Such a $Y$ must achieve its Gromov-Hausdorff distance with every compact space , and this seems like a fairly strong requirement; however, I can't seem to conclude compactness from this alone (or even the stronger hypothesis actually being asked about here). There are several additional questions I'm interested in about $\trianglelefteq$ (e.g. what's the ordertype of $\trianglelefteq$ on the compact spaces?), but this seems like a good place to start.","['gromov-hausdorff-limit', 'general-topology', 'metric-spaces', 'compactness']"
4569446,Can $\prod\limits_{k=1}^\infty \left(1- \frac{1}{e^{ \sqrt{2} \pi k}}\right)$ be put into closed form?,Let $$ \alpha =  \prod_{k=1}^{\infty} \left(1- \frac{1}{e^{ \sqrt{2} \pi  k}}\right) $$ and $$ \beta = \prod_{k=1}^{\infty} \left(1 + \frac{1}{e^{ \sqrt{2} \pi  k}}\right) = \frac{\exp \left(\frac{\sqrt{2}\pi }{24}\right)}{\sqrt[4]{2}} $$ Then $$ \sum_{n=1}^{\infty} \text{arctanh} \left(\frac{1}{e^{ \sqrt{2} \pi  n}} \right) = -\text{arctanh} \left( \frac{\alpha - \beta}{\alpha + \beta}\right) \approx 0.0119025137323901862  $$ Is there a way to solve for the product $\alpha$ ? I figure that it's likely given that $\beta$ can be.,"['infinite-product', 'sequences-and-series']"
4569458,Symmetric random walk on a regular hexagon,"I wonder if there is any trick in this problem. The following graph is a regular hexagon with its center $C$ and one of the vertices $A$ . There are $6$ vertices and a center on the graph, and now assume we perform the symmetric random walk on it. Suppose the random walk starts from $A$ . Given now the process is at one of the vertices, then it has probability $\frac{1}{3}$ for entering into $C$ for the next step, and $\frac{1}{3}$ probability for moving to its two neighbors respectively as well. We want to compute the probability of this random process started from $A$ and finally return to $A$ , and it must not go through $C$ before its first arrival back to $A$ . Therefore, basically $A$ and $C$ are two absorbing states. Denote this discrete random walk as $\left\{ X_{t} \right\}_{t\geq 0}$ , and I want to compute: $$
P(X_{t} = A, ~ X_{s} \notin \left\{A, C \right\} \mbox{ for } \forall ~ 0 < s < t ~ | ~ X_{0} = A)
$$ The following are my thoughts: The first intuition came to me was DTMC by regarding $A$ and $C$ as two absorbing states and the process absorbed by $A$ eventually. This requires me to write down a probability transition matrix and then solve linear equations. However, it is actually an interviewing question, so I suppose there is a much easier way to do this. Also, by intuition I think by constructing a stopping time might help, but my thought just stuck at here.",['probability']
4569531,How to solve $\lim\limits_{n\to∞}\frac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n})$,"How to solve $\lim\limits_{n\to∞}\dfrac1{n^{3/2}}(\sqrt{2n+1}+\sqrt{2n+2}+\cdots+\sqrt{2n+n})$ This problem was asked by another user, but was deleted when I found the answer below. So I thought I should post it here . Please check if my use of Riemann Integral was correct, since I have very little knowledge on it.
Thanks.","['limits', 'calculus', 'riemann-sum', 'sequences-and-series']"
4569534,Problems in mathematical analysis exam,"I want to ask some problems that are asked in exam (actually the example of exam that I'm preparing in this post it is Mathematica analysis) I want some suggestions to my answer (some are obtained by reading other posts in here that I just aggregate) I want to know if there are some suggestions and improvements and corrections. Thank you. Let $(a_n)$ be a positive real sequence such that $\lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2}$ . Prove that $\lim_{n\rightarrow \infty} a_n=0$ . My first sol If $\lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2}$ we have $$\left|\dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}\right|<\varepsilon,\forall n\ge N_0, \forall \varepsilon >0.$$ Let us choose $\varepsilon =1/4$ then for $n\ge N$ we have $\dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}<\dfrac{1}{4}$ , so $a_{n+2}<\dfrac{3}{4}a_n$ for all such $n$ . Since $\lim_{m\rightarrow \infty}(3/4)^m=0$ (not sure if I could use it here withput proof), and by mathematical induction we have for all $k>N$ $$a_{2k}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N},$$ $$a_{2k+1}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N+1}.$$ Therefore, for all $n\ge 2N+M+1$ , $a_n<\varepsilon$ as $(3/4)^{m}<\varepsilon/\max(a_{2N},a_{2N+1}), \forall \varepsilon>0, \forall m\ge M$ $\blacksquare$ My second sol By the ratio test we have both $\displaystyle \sum_{k\ge 1}a_{2k+1}$ , $\displaystyle \sum_{k\ge 1}a_{2k}$ converges. If we let $b_k=a_{2k+1}$ and $c_k=a_{2k}$ , then we have $b_k,c_k\rightarrow 0$ as $k\rightarrow \infty$ . That is, $|a_{2k+1}|<\varepsilon$ and $|a_{2k}|<\varepsilon$ for all $\varepsilon$ and $n\ge N$ , which means $|a_n|<\varepsilon$ $\blacksquare$ Let $(a_n)$ be a positive real sequence such that $\sum_{n\ge 1} na_n<\infty$ . Prove that $\sum_{m\le n}a_m $ is a Cauchy sequence. I think it can be done by comparison test that $a_n\le na_n$ $\forall n\ge 1$ , thus $\sum_{m\ge 1}a_m$ converges and so $\sum_{m\le n} a_m$ is a Cauchy sequence (not sure if this is okay since every convergent sequence is Cauchy.) This is something like if we let $s_n=\sum_{m\le n}a_m$ , we have $\lim_{n\rightarrow\infty} s_n=c,\exists c$ , and use the definiton again. Let $f:\mathbb R\rightarrow \mathbb R$ is continuous (does this mean it is continuous all over $\mathbb R?$ ) such that $f(0)>0$ . Prove that $\exists \varepsilon>0$ and $\delta>0$ such that $f(x)>\varepsilon$ whenever $x$ satisfying $|x|<\delta$ . Since $f$ is continuous at $0$ , $\forall\varepsilon,\exists \delta>0$ , $|f(x)-f(0)|<\varepsilon$ whenever $|x|<\delta$ . As $f(0)>0$ we let $f(0)=c,\exists c>0$ , thus $f(x)>c-\varepsilon$ . So we choose $\varepsilon=c/2$ so that $\exists\delta'>0$ we have $f(x)>c/2$ for any $|x|<\delta$ $\blacksquare$ Let $f:\mathbb R\rightarrow\mathbb R$ is continuous at $a\in\mathbb R$ . Let $g(x)=(x-a)f(x)$ . Determine whether $g$ is differentiable at $a$ and explain why. Since $f$ is continuous at $a$ , we have that $|f(x)-f(a)|<\varepsilon$ for any $\varepsilon>0$ and some $\delta=\delta(\varepsilon)>0$ and for any $|x-a|<\delta$ . For $h\neq 0$ , we have $$\dfrac{g(a+h)-g(a)}{h}=\dfrac{hf(a+h)-0}{h}=f(a+h).$$ For any $|h|<\delta$ , we have $|f(a+h)-f(a)|<\varepsilon$ so the limit above (as $h\to 0$ ) exists and equals $f(a)$ $\blacksquare$","['contest-math', 'problem-solving', 'analysis', 'real-analysis']"
4569535,"Prove this $\sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right)$","Set the natural number $n$ . For each $S \subseteq\{1, \ldots, n\}$ define $\pi(S)$ as the product of the members of $S$ , with the agreement $\pi(\emptyset)=1$ . Prove that $$
\sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right)
$$ My working: I've worked on the right side so that the result is like this $$(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+ 1}\right)
$$ $$(n+1)!$$ $$
\sum_{S \subseteq\{1, \ldots, n\}}(n+1) !\left(\frac{1}{n+1}\right)
$$ $$
\sum_{S \subseteq\{1, \ldots, n\}}(n+1)n !\left(\frac{1}{n+1}\right)
$$ $$
\sum_{S \subseteq\{1, \ldots, n\}}n !
$$ I don't know if my method is correct or not, please help and correct it","['statistics', 'probability', 'combinatorics', 'problem-solving', 'stochastic-calculus']"
4569562,Trick of selecting fundamental matrix in nonhomogeneous differential equations to simplify calculations,"Problem: I came across the following differential equation set: $$
\begin{cases}
	\dot{x}&=3x-2y+t\\
	\dot{y}&=4x-y+t^2\\
\end{cases}
$$ My solution I followed normal practices and found the general solution to the homogeneous equation $$
\left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] '=\left[ \begin{matrix}
	3&		-2\\
	4&		-1\\
\end{matrix} \right] \cdot \left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] \Rightarrow \left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] =\left[ \begin{array}{c}
	e^t\left( c_1\cos 2t+c_2\sin 2t \right)\\
	e^t\left( \left( c_1-c_2 \right) \cos 2t+\left( c_1+c_2 \right) \sin 2t \right)\\
\end{array} \right] 
$$ I selected $(c_1, c_2)=(1,0)$ and $(0,1)$ to form a fundamental matrix $X=\left[ \begin{matrix}
	e^t\cos 2t&		e^t\sin 2t\\
	e^t\left( \cos 2t+\sin 2t \right)&		e^t\left( \sin 2t-\cos 2t \right)\\
\end{matrix} \right] $ , and plugged it into the variation of constants formula to get a particular solution $y(t)=X(t)\int_{0}^{t}\left(X^{-1}(\tau)\cdot \left[ \begin{array}{c}
	\tau\\
	\tau ^2\\
\end{array} \right]\right)\mathrm{d}\tau $ , and after a one-page long calculation and the help of Wolframalpha, I got the following solution of $y$ : Question I wonder what's wrong with my selection since the what wolfram gave me if I typed the original equation set would be like this: So I guess there must be something in my selection of the fundamental matrix that makes the calculations really complex, but how can I improve it? Also, what's your favourite way of solving this kind of nonhomogeneous linear differential equation sets?","['computational-complexity', 'linear-algebra', 'ordinary-differential-equations']"
4569565,Curious how to retrieve original vector $x$ from an $x^Tx$ operation,"I am doing some analysis on a signal $x(t)$ which is a vector of length $m$ . After performing the operation $x^Tx$ of the vector on itself, I get a matrix $Y$ of size $m\times m$ . Now, how can I retrieve the original $x(t)$ ? Should this be the elements in the diagonal of $Y$ ? I did some basic plotting and while the diagonal looks similar to original signal vector, they are not equal. Looks straightforward but I am missing something, probably need to normalize it somehow. In summary, Is it always possible to retrieve the original signal without information loss? If it is possible, how is it done?","['matrices', 'signal-processing', 'linear-algebra']"
4569572,Variable substitution with 3 variables,"I'm trying to determine the following triple integral $$
\iiint_{K}\left[\left(x - a\right)^{2} +
\left(y - b\right)^{2} + \left(z - c\right)^2\right]{\rm d}x\,{\rm d}y\,{\rm d}z,\quad
K = \left\{\left(x,y,z\right): x^{2} + y^{2} + z^{2}
\leq 1\right\}.
$$ I tried the following variable substitution $$\begin{cases} 
x = r\sin\left(\theta\right)\cos\left(\varphi\right) + a
\\[1mm]
y = r\sin\left(\theta\right)\sin\left(\varphi\right) + b
\\[1mm] 
z = r\cos\left(\theta\right) + c 
\end{cases}$$ with $$
0<r<1 \quad , \quad 0< \theta < \pi \quad , \quad
0 < \varphi < 2\pi
$$ and I'm getting $4\pi/5$ , which is wrong, the answer should be $4\pi/5 + 4\pi\left(a^{2} + b^{2} + c^{2}\right)/3$ , but I can't really find where I missed up !.","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4569624,A hypothesis: Integral of $f^n(x)$ is monotonic as $n$ reaches infinity.,"When I'm solving some analysis questions I discovered some interesting phenomenons. I can't prove it or give any counterexamples so I'm seeking help. If $f(x)$ is a continuous, monotonic, non-negative function on $[a,b]$ , then sequence $$\left\{\int_a^bf^n(x)\mathrm{d}x\right\}_{n=1}^{\infty}$$ is monotonic, or at least starting from some $N\in\mathbb{N}$ it is. If someone can prove it or give a counterexample. Thanks! ( $f^n(x)$ means $[f(x)]^n$ .)","['calculus', 'analysis', 'real-analysis']"
4569639,Is Wolfram Alpha correct about this limit of an oscillating function?,"I'm struggling with the following limit $$\lim_{x \to \infty} e^x\big(1 + \sin(x)\big).$$ First of all I checked the solution using Wolfram . To my surprise Wolfram says that this limit exists and is equal to $\infty$ . This goes against my intuition being honest. I decided to try to show that this limit does not exist using Heine definition. I found the following sequences $$x_n = n\pi, \qquad y_n = \bigg(2n + \frac{3}{2}\bigg)\pi.$$ Of course both $x_n$ and $y_n$ tend to $\infty$ whenever $n \to \infty$ . Moreover we have $\sin(x_n) = 0$ and $\sin(y_n) = -1$ for all $n \in \mathbb{N}$ . The first limit is easy, $$\lim_{n \to \infty} e^{x_n}\big(1 + \sin(x_n)\big) = \infty.$$ The problems appears when computing the next limit, which is $$\lim_{n \to \infty} e^{y_n}\big(1 + \sin(y_n)\big).$$ We clearly have an indeterminate form, i.e. $\infty \times 0$ . How can I solve my problem? Does this limit exist?","['limits', 'solution-verification', 'wolfram-alpha', 'real-analysis']"
4569661,On Bartle's 'Elements of Integration' Exercise 7.U,"I haven't found a reference solution for this problem on the net, and I'm wondering whether my proof makes sense / is correct. Also I believe that it should be possible to find a simpler proof for the implication by using Vitali's Convergence theorem, but I'm not sure how.
Any suggestions and comments are welcome. Problem statement (adapted from original to make it self-contained; from R.G. Bartle 'The Elements of Integration', p.79, exercise 7.U): Let $(X, \mathbf X, \mu)$ be a finite measure space and let $1 \leq p < \infty$ . Let $\varphi$ be continuous on $\mathbb R$ to $\mathbb R$ and satisfy the condition: $(*)$ there exists $K > 0$ such that $|\varphi(t)| \leq K |t|$ for $|t| \geq K$ .
If $(f_n)$ converges to f in $L_p$ then $(\varphi \circ f_n)$ converges in $L_p$ to $\varphi \circ f$ . Conversely, if condition $(*)$ is not satisfied, there exists a finite measure space and a sequence $(f_n)$ which converges in $L_p$ to $f$ but such that $(\varphi \circ f_n)$ does not converge in $L_p$ to $\varphi \circ f$ . Proof: $\Longrightarrow$ (1. sequence and limit functions are in $L_p$ - implication in exercise 7.T) Let $M = \max_{t \in [0, K]} |\varphi(t)|^p$ ; ( $M < \infty$ as $[0, K]$ compact and $\varphi$ continuous) $\Rightarrow$ $|\varphi \circ f|^p = |\varphi \circ f|^p (\chi_{|f|^{-1}([0, K[)} + \chi_{|f|^{-1}([K, \infty[)}) \leq M \chi_{|f|^{-1}([0, K[)} + K^p |f|^p \chi_{|f|^{-1}([K, \infty[)} \leq M + K^p |f|^p;$ $\Rightarrow$ $||\varphi \circ f||_p^p = \int |\varphi \circ f|^p \mathrm d \mu \leq \int M + K^p |f|^p \mathrm{d} \mu = M \mu(X) + K^p ||f||_p^p < \infty$ ; (by $\mu(X) < \infty$ and $f \in L_p$ ) $(\Rightarrow \varphi \circ f \in L_p)$ $(\mbox{by the same reasoning for } f_n \Rightarrow \varphi \circ f_n \in L_p, n \in \mathbb N)$ (2. every subsequence has an $L_p$ convergent subsubsequence) Let $\Lambda \subset \mathbb N$ be infinite; $\Rightarrow (f_n)_{n \in \Lambda}$ converges in $L_p$ to $f$ ; $\Rightarrow \exists \Lambda' \subset \Lambda$ infinite $ : f_n \rightarrow f \; \mu\mbox{-a.e.} \; (n \in \Lambda');$ $\forall n \in \Lambda'$ $\Rightarrow \varphi \circ f_n \rightarrow \varphi \circ f \; \mu\mbox{-a.e.};$ (by continuity of $\varphi$ ) $\Rightarrow \varphi \circ f_n \rightarrow \varphi \circ f$ almost uniformly; (by $\mu(X) < \infty$ and Egoroff's Theorem) Let $\delta > 0, E_{\delta} \subset X, \mu(E_{\delta}) < \delta : (\varphi \circ f_n |_{X \setminus E_{\delta}})$ converges uniformly to $\varphi \circ f |_{X \setminus E_{\delta}};$ on $X \setminus E_{\delta}$ $\Rightarrow \exists N \in \Lambda' : (n > N \Rightarrow |f_n - f| \leq 1)$ ; Let $g = \sum_{n = 1}^{N} |f_n - f|^p + 1  \geq |f_n - f|^p$ ; $\Rightarrow g \in L_1$ ; (by $\mu(X) < \infty$ ) $\Rightarrow |f_n|^p = |f + (f_n - f)|^p \leq (2 \max\{|f|, |f_n - f|\})^p \leq 2^p(|f|^p + |f_n - f|^p) \leq 2^p(|f|^p + g)$ ; $\Rightarrow |\varphi \circ f_n|^p \leq M + K^p |f_n|^p \leq M + K^p 2^p (|f|^p + g) \in L_1;$ (again by $\mu(X) < \infty$ ) $\Rightarrow ||\varphi \circ f_n||_p \rightarrow ||\varphi \circ f||_p$ ; (by Lebesgue Dominated convergence Theorem) $\Rightarrow ||\varphi \circ f_n||_p \rightarrow ||\varphi \circ f||_p \;$ on $X \setminus E$ where $E = \bigcap_{\delta > 0} E_{\delta}$ ; $\Rightarrow \forall \delta > 0 \; E \subset E_{\delta} \Rightarrow \mu(E) \leq \mu(E_{\delta}) < \delta \Rightarrow \mu(E) = 0;$ $\Rightarrow ||\varphi \circ f_n||_p \rightarrow ||\varphi \circ f||_p \;$ on $X$ ; ( $\Rightarrow \forall \Lambda \subset \mathbb N $ infinite $\, \exists \Lambda' \subset \Lambda : (\varphi \circ f_n)_{n \in \Lambda'}$ converges in $L_p$ to $\varphi \circ f$ ); (3. sequence converges in $L_p$ ) Suppose $\varphi \circ f_n \nrightarrow \varphi \circ f$ in $L_p$ ; $\Rightarrow \exists \epsilon >0 : \forall N \in \mathbb N \, \exists n > N : ||f_n - f||_p \geq \epsilon;$ Let $\Lambda = \{n \in \mathbb N \mid ||f_n - f||_p \geq \epsilon\};$ $\Rightarrow (\varphi \circ f_n)_{n \in \Lambda}$ is a subsequence which does not posses an $L_p$ convergent subsubsequence, contrary to what was proven before. $(\Rightarrow \varphi \circ f_n \rightarrow \varphi \circ f$ in $L_p)$ $\Longleftarrow$ (1. construction of finite measure space and limit function - converse in exercise 7.T) $\lnot (*) \Rightarrow \forall K > 0 \; \exists |t| \geq K: |\varphi(t)| > K |t|$ ; Let $t_n \in \mathbb R, t_n > n, |\varphi(t_n)| > n |t_n|, n \in \mathbb N$ ; Let $(X = \mathbb N, 2^{\mathbb N}, \mu)$ be a measure space with measure $\mu(E) = \sum_{n \in E} \frac 1 {{|t_n|}^p n^2}$ ; $\Rightarrow \mu(X) = \sum^{\infty}_{n = 1} \frac 1 {{|t_n|}^p n^2} \leq \sum^{\infty}_{n = 1} \frac 1 {n^2} < \infty$ ( $\Rightarrow$ the measure space is finite); Let $f:\mathbb N \ni n \mapsto t_n$ ; $\Rightarrow ||f||_p^p = \int |f|^p \mathrm d \mu = \sum_{n=1}^{\infty} \frac{|f(n)|^p} {{|t_n|}^p n^2} = \sum_{n=1}^{\infty} \frac 1 {n^2} < \infty;$ $\Rightarrow ||\varphi \circ f||_p^p = \int |\varphi \circ f|^p \mathrm d \mu = \sum_{n=1}^{\infty} \frac{|\varphi(f(n))|^p} {{|t_n|}^p n^2} = \sum_{n=1}^{\infty} \frac{|\varphi(t_n)|^p} {{|t_n|}^p n^2} \geq \sum_{n=1}^{\infty} \frac{1} {n^{(2-p)}} \geq \sum_{n=1}^{\infty} \frac{1} {n} = \infty;$ $(\Rightarrow f \in L_p \, \land \, \varphi \circ f \notin L_p)$ (2. construction of sequence) Let $f_n = (1 - \frac 1 n) f, \, n \in \mathbb N$ ; $\Rightarrow ||f_n||_p = ||(1-\frac 1 n)f||_p = (1-\frac 1 n)||f||_p < \infty \Rightarrow f_n \in L_p \; \forall n \in \mathbb N;$ $\Rightarrow ||f_n - f||_p = \frac 1 n ||f||_p \rightarrow 0;$ ( $f_n \rightarrow f$ in $L_p$ ) Suppose $\varphi \circ f_n \rightarrow \varphi \circ f$ in $L_p$ ; $\Rightarrow \forall n \in \mathbb N \; ||\varphi \circ f||_p \leq ||\varphi \circ f_n||_p + ||\varphi \circ f_n - \varphi \circ f||_p < \infty;$ (by Minkowski Inequality) $\varphi \circ f \notin L_p \Rightarrow$ contradiction. ( $\Rightarrow \varphi \circ f_n \nrightarrow \varphi \circ f$ in $L_p$ ) $\square$","['measure-theory', 'sequence-of-function', 'lebesgue-integral', 'lp-spaces', 'uniform-convergence']"
4569665,Complement of $\left\{x\in\mathbb{R}\mid \liminf\limits_{n\to\infty}f_n(x)\geq f(x)\right\}$,"In the context of the question: ""What is the complement of $M:=\left\{x\in\mathbb{R}\mid \liminf\limits_{n\to\infty}f_n(x)\geq f(x)\right\}$ ?"" I ran into the following problem. We see that \begin{align*}
&M=\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f_j(x)-f(x)>\frac{1}{m}\right\}\cup\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f(x)-f_j(x)<\frac{1}{m}\right\}.
\end{align*} So by law of De Morgan it follows directly \begin{align*}
&M^c=\left\{x\in\mathbb{R}\mid \liminf\limits_{n\to\infty}f_n(x)< f(x)\right\}\\
&=\left(\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f_j(x)-f(x)>\frac{1}{m}\right\}\cup\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f(x)-f_j(x)<\frac{1}{m}\right\}\right)^c\\
&=\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f_j(x)-f(x)\leq\frac{1}{m}\right\}\cap\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f(x)-f_j(x)\geq\frac{1}{m}\right\}.
\end{align*} If we consider the example $f:\mathbb{R}\to\{0\}$ and $f_n:\mathbb{R}\to\{-1,1\}$ with $f_n(x)=(-1)^n$ , then clearly $\liminf\limits_{n\to\infty}f_n(x)=-1<0$ for all $x\in\mathbb{R}$ . However, in this case $$
\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{x\in\mathbb{R}\mid f_j(x)-f(x)\leq\frac{1}{m}\right\}=\emptyset.
$$ And this makes no sense. Where is my mistake?","['elementary-set-theory', 'limsup-and-liminf', 'solution-verification', 'real-analysis']"
4569705,On the variance of powers of a random variable,"Let $(X_n)$ be a sequence of nonnegative random variables, and let $p>1$ such that ${\rm Var\,} X_n^p\to 0$ as $n\to \infty$ . Then the variance of each $X_n$ is finite, and it seems intuitively clear that it should tend to zero as well, but I guess I just can't find an easy proof of this fact. In short, the question is $$
{\rm Var\,} X_n^p\to 0 \implies {\rm Var\,} X_n\to 0\,? \quad (p>1)
$$",['probability-theory']
4569708,Unit normal of ellipse,"I am reading through a book which states that for an ellipse specified by the points that satisfy: $f(x_2,x_3)=x_2^2/a^2 + x_3^2/b^2 = 1$ the unit normal is given by $\mathbf{n}=\frac{\nabla f}{|\nabla f|}=\frac{2}{|\nabla f|} \left[\frac{x_2}{a^2}\mathbf{e_2} + \frac{x_3}{b^2}\mathbf{e_3}\right] = b^2 x_2\mathbf{e_2} + a^2x_3 \mathbf{e_3}$ where $\mathbf{e_i}$ denotes the unit vector in the $i^{\mathrm{th}}$ direction. This of course implies that $|\nabla f| = 2/a^2b^2$ , but I can't seem to derive that magnitude of $|\nabla f|$ myself. Could someone help me out?","['plane-curves', 'calculus', 'geometry']"
4569709,Infinite family of finite groups without surjections,"Is there an infinite family $\mathcal{F}$ of finite groups such that their exponent is bounded, i.e. $\exists N \geq 1 : \forall x \in G \in \mathcal{F}, x^N = 1_G$ ; there does not exist any surjective morphism between two distinct groups in $\mathcal{F}$ ? (Without the bound on the order, { $\mathbb{Z}/(p) \mid p$ prime} is an obvious solution.)","['group-theory', 'finite-groups']"
4569718,"Find the number of subsets $\{a_1,\cdots, a_k\}$ of $\{1,\cdots, p\}$ so that $a_1+a_2+\cdots + a_k\equiv 0\mod p.$","Let $p$ be an integer exceeding 1 and suppose $1\leq k\leq p-1.$ Find the number of $k$ -element subsets $\{a_1,\cdots, a_k\}$ of $\{1,\cdots, p\}$ so that $a_1+a_2+\cdots + a_k\equiv 0\mod p.$ I've only managed to solve the above problem when p is prime. If $p=2,$ then the number of 1-element subsets of $\{1,2\}$ with sum congruent to 0 mod 2 is $1.$ Henceforth assume $p>2$ . Then note that for every $1\leq k\leq p-1$ , we can find a k element subset whose sum is congruent to 0 mod p. Indeed, if k is odd and $k=2y+1$ for some $0\leq y\leq p/2 - 1$ , we can take the sum $p+(p-1)+1 + \cdots (p-y) + y$ as $p-i, i < p, p-i\neq i$ for $i < p$ , $p-y > y$ since $y < p/2,$ and $y$ is the highest second element among the pairs $(p-1,1),(p-2,2),\cdots, (p-y,y)$ while the first terms are strictly decreasing. If $k=2y$ is even, then we have the sum $(p-1 + 1) + \cdots + (p-y+y),$ which is a sum of distinct elements by similar reasoning to above as $y \leq (p-1)/2.$ Now given such a subset, say $B := \{a_1,\cdots, a_k\},$ define for $0\leq i\leq p-1$ the subset $B_i$ by $$B_i := \{f(a_1+i), \cdots, f(a_k+i)\}$$ where $f(x)$ is the unique element $y$ of $\{1,\cdots, p\}$ so that $x\equiv y\mod p.$ Then note that all the elements $a_j + i$ are distinct modulo $p$ and since $f(a_j+i)\equiv a_j+i\bmod p$ for all $a_j+i$ , the terms $f(b)$ are distinct where $b$ ranges over $B.$ Also, $\sum_{a\in B_i} a \equiv a_1+\cdots + a_k + ik \equiv ik\mod p.$ Each residue $ik\mod p$ is distinct because if $ik \equiv jk$ for some $0\leq i < j < p,$ then $p$ would divide $(i-j)k$ and hence $p$ would divide $(i-j)$ . Since $p$ is coprime to $k$ , this implies $i-j$ is divisible by $p$ , which is a contradiction since $0<|i-j|<p.$ Hence each $B_i$ is distinct. Now note that each $k$ -subset of $\{1,\cdots, p\}$ belongs to exactly one $B_i, 0\leq i\leq p-1$ for some $B$ . Indeed, let for each $B,$ the set of $B_i$ 's defined above be the cycle of B. Note that if two cycles intersected, then they would be the same cycle. Note that here it's crucial that $p$ is prime (the claims above could still work if $p$ was replaced by $\dfrac{p}{\gcd(p,k)}$ ). Suppose the cycles of $B=\{b_1,\cdots, b_k\}$ and $A=\{a_1,\cdots, a_k\}$ intersected. Then there exist $s,t$ so that $A_t = B_s$ . So $a_i + t \equiv b_{\sigma(i)} + s$ for some permutation $\sigma$ of $\{1,\cdots, k\}.$ Then $a_i\equiv b_{\sigma(i)} + s-t$ for all i. But then A equals $B_{s-t}$ ( $B_{s-t}$ may not be defined if we replaced $p$ with $\dfrac{p}{\gcd(p,k)}$ in our construction of the $B_i$ 's). Hence for all $0\leq i\leq p-1, A_i$ equals $B_{s-t + i\mod p}$ and since the cycle elements $B_i$ are distinct for all i, this shows that the map $A_i\mapsto B_{(s-t+i)\mod p}$ defines a bijection between the cycle of A and the cycle of B. Hence distinct cycles are disjoint. But in each cycle, each of the $B_i$ 's has a distinct sum modulo p as proved above, so exactly one $B_i$ has sum $0$ mod p. Thus, the required answer is $\dfrac{{p\choose k}}{p}.$ I'm not sure if generating functions could be useful when p is not prime.
Let $\xi$ be a primitive p-th root of unity (note that these are precisely the numbers of the form $e^{2\pi i k/p}$ where k is coprime to p). Consider the product $\prod_{j=1}^p (1+\xi^j).$ Note that when this product is written in the form $b_1 \xi + b_2\xi^2+\cdots + b_p \xi^p$ (which is possible since $\xi^p=1\Rightarrow \xi^{p+k} = \xi^k$ for all k), $b_p$ equals the number of subsets of $x$ with sum divisible by p.","['contest-math', 'elementary-number-theory', 'combinatorics', 'modular-arithmetic']"
4569862,Is it possible to find the distribution of this nonlinear SDE?,"Consider the nonlinear SDE for $(X_t)_{t\geq 0}$ \begin{equation}
\mathop{dX_t}=X_t\left(\mu\mathop{dt}+\sqrt{v_0+\omega\left(\phi(t)-\ln X_t\right)^2}\mathop{dW_t}\right),
\end{equation} where $\phi:t\rightarrow \mathbb{R}_{\geq 0}$ is a deterministic bijection and $\mu,v_0,\omega\in\mathbb{R}_{>0}$ are constants. Applying the variance (or Lamperti) transform \begin{align}
f(x,t)&=\int\frac{1}{x\sqrt{v_0+\omega\left(\phi(t)-\ln x\right)^2}}\mathop{dx},\\
&=-\frac{1}{\sqrt{\omega}}\tanh^{-1}\left(\frac{\left(\phi(t)-\ln x\right)\sqrt{\omega}}{\sqrt{v_0+\omega\left(\phi(t)-\ln x\right)^2}}\right),
\end{align} and Itô's Lemma should result in a linear SDE for $Y_t:=f(x,t)$ with variance $1$ : \begin{align}
\mathop{dY_t}=\frac{\partial f(X_t,t)}{\partial t}\mathop{dt}+\frac{\partial f(X_t,t)}{\partial X}\mathop{dX_t}+\frac{1}{2}\frac{\partial^2f(X_t,t)}{\partial X^2}\left(\mathop{dX_t}\right)^2.
\end{align} Is it possible to then derive the probability density function of $X_t$ ? Any help would be much appreciated. Note that, using the Fokker-Planck equation, this is also equivalent to solving the PDE \begin{equation}
p_t(x,t)=\frac{x^2p_x(x,t)(v_0+\omega(\phi(t)-\ln x)^2)}{2}+xp(x,t)\left(v_0+\omega\left((1-2\phi(t))\ln x+\ln^2 x +\phi(t)(1-\phi(t))\right)\right)-\mu xp_{x}(x,t)-\mu p(x,t),
\end{equation} with initial condition $p(x,0)=\delta(x-X_0)$ . However, this seems intractable and so I was wondering if applying an appropriate transformation would simplify the problem.","['stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4569866,Equation of motion given by $\ddot x = -\mbox{sgn}(x)$,I need to solve the following differential equation $$\ddot x = -\mbox{sgn}(x)$$ where $x \mapsto \mbox{sgn} (x)$ is the sign function. I can see that it would behave like constant acceleration but it also flips and oscillates when it crosses the origin.,['ordinary-differential-equations']
4569882,"Prove that: $x^x-y^y\ge(x-y)^x-(x-y)^y$, where $x\ge y>0.$","I want to analyze the following inequality: Let $x,y\in\Bbb R^{+}$ , then prove that: $$x^x-y^y\ge(x-y)^x-(x-y)^y$$ holds for all $x\ge y$ . I observed that, for possible simplification, dividing both side of the inequality into any of the expressions like $x^x, x^y, y^y, y^x$ doesn't work. Using the identity $x^x=e^{x\ln x}$ and $y^y=e^{y\ln y}$ doesn't help. I used the well-known inequality $e^x≥x+1$ and obtained: $$x^x=e^{x\ln x}≥x\ln x+1$$ or $$y^y=e^{y\ln y}≥y\ln y+1$$ But, this also doesn't help. Some observations: Big problem occurs when, $x=0.002, y=0.001$ . $$-0.0055≈x^x-y^y\ge(x-y)^x-(x-y)^y≈-0.0068$$ If $x=5,y=4.5$ then: $$(x-y)^x-(x-y)^y≈-0.001<0$$ If $x=5,y=3$ then: $$(x-y)^x-(x-y)^y≈24>0$$ If $x-y=1$ , then: $(x-y)^x-(x-y)^y=0$ . Thus $$x^x-(x-1)^{x-1}\ge 0$$ which is correct, since $x>1$ . If $x=y>0$ , then the equality trivially holds: $0≥0$ The possible inequality plot is as follows: I also include the reverse inequality plot: $$x^x-y^y<(x-y)^x-(x-y)^y$$ How can we analyze this inequality?  This doesn't seem possible without using calculus. Maybe the inequality is simple.  However, I could not find a good point, unfortunately.  Also, I'm having trouble understanding the first graph that Wolfram Alpha presents.","['exponentiation', 'inequality', 'calculus', 'algebra-precalculus', 'exponential-function']"
4569906,What is the probability that he escapes the island?,"A man is stranded on an island. A benevolent genie presents three boxes, 23 white marbles, and 7 black marbles and instructs the man, ""You may distribute the marbles into the boxes any way you see fit, but you must use all of the marbles. Once you finish, you will choose a box at random and then choose a marble from the box at random. If the marble is whiten, then I will help you escape from this place. Assuming the man distributes the marbles in his best interest, what is the probability that he escapes the island? Apart from the first two cases, the probability that the man draws a white marble from the box is $3/4$ $\left(\frac{1}{3}\right)(1)+\left(\frac{1}{3}\right)(1)+\left(\frac{1}{3}\right)\left(\frac{3}{4}\right)=\frac{11}{12}$ This might solve my problem but my brain still keeps bugging about the outcome. The main question arises: Why this choice is optimal? I started to think why we are analyzing the  chance of escaping than not escaping. When I tried to solve this, while splitting up the marbles into multiple boxes: again its not optimal. How can I find an optimal way of explanation to this now?","['probability-distributions', 'probability-theory', 'probability']"
4569960,Through two distinct points in $\mathbb{R}^2$ passes a unique line,"I am asked to prove the following statement: Through two distinct points $a, b \in \mathbb{R}^2$ , $a\neq b$ , there passes a unique line $l$ . If we have a line given by $L_{a,b} = \{(x_1, x_2) \in \mathbb{R}^2 : c_1x_1 + c_2x_2 = d \}$ , how would we prove this statement? Writing it as a system of equations and showing the solution is unique? There is surely a better way, isn't it? I tried something like this, but I am not very confident this will work: If $l$ contains $a$ and $b$ , then (after some rearranging) we have $c_1(a_1-b_1) + c_2(a_2-b_2) = 0$ , now suppose there is another line containing these points. Let's call it $l'$ . This should look like this: $c'_1(a_1-b_1) + c'_2(a_2-b_2) = 0$ So we see, apart from being scalable with a factor $\lambda \in \mathbb{R} $ , these are the same line. However, I am fairly unconfident this proves anything.
How would we go about this, without using the dimension of the solution space of the equations or linear independence etc.?","['linear-algebra', 'geometry']"
4570052,Why is $Z(y^2 - x^p - t)$ a regular scheme?,"Let $k_0$ be a field of characteristic $p > 0$ and $k = k_0(t)$ . Now we let $C \subset \mathbb{A}^2_k$ be the curve defined by $y^2 = x^p - t$ . We wish to show that although $C$ is regular, it is not smooth over $k$ . Denote $A = k[x,y]/(y^2 - x^p - t)$ the coordinate ring of $C$ . For smoothness, we note that the module of differentials over $k$ is given by $$\Omega = (Adx \oplus Ady)/(d(y^2 - x^p - t)) = (Adx \oplus Ady)/(2ydy) = Adx \oplus A/(y) dy$$ Now let $P \in C$ be a closed point, associated to a maximal ideal $\mathfrak{m}$ of $A.$ If $y \notin \mathfrak{m}$ , then $y$ is a unit in $A_\mathfrak{m}$ so that $A_{\mathfrak{m}}/(y) = 0$ and $$\Omega_P = \Omega_{A_\mathfrak{m}/k} = A_\mathfrak{m}dx$$ which implies that $\Omega_P \otimes k(P) = k(P)dx$ has rank $1$ . On the other hand, if $y \in \mathfrak{m}$ then $\mathfrak{m} = (y)$ since $A/(y) = k(t^{1/p})$ is a field, so that $$\Omega_P = k(P) dx \oplus k(P) dy.$$ Hence, $C$ is not smooth over $k$ since the dimension of the fibers of $\Omega_P \otimes k(P)$ change as the closed points vary. Now I want to show that $C$ is a regular scheme. It suffices to show that $A_\mathfrak{m}$ is a regular local ring for all maximal ideals $\mathfrak{m}$ of $A$ . One case is easy: if $(y) = \mathfrak{m}$ then $A_{\mathfrak{m}}$ is a DVR and hence regular. The other case is not so clear. Perhaps you can use some fundamental exact sequence for differentials to get the right inequality? I'm quite stuck on this part. Does anyone have any ideas to show that $\dim_{k(x)} \mathfrak{m}_x/\mathfrak{m}_x^2 \leq 1$ for the other closed points? Also, is my reasoning fo concluding $C$ isn't smooth over $k$ sound? Thanks! For context, this is the first exercise in Hartshorne III.10. EDIT: Perhaps the jacobian criterion works in the positive characterstic non-algebraically closed case?","['algebraic-curves', 'algebraic-geometry', 'commutative-algebra']"
4570058,"""continuity"" of symmetric difference wrt intersection","Question 71(b) of Kirillov and Gvishiani's problem book on functional analysis is to show: $$(A_1\cap A_2)\triangle (B_1\cap B_2)\subset (A_1\triangle B_1)\cap (A_2\triangle B_2).$$ Isn't this a counter-example: Let $A_1=A_2=B_1=\{0\}$ and $B_2=\{1\}$ . I am also hoping to better understand the way this result is described, as ""continuity of $\triangle$ with respect to intersection."" How does this connect to the literal definition of continuity?",['elementary-set-theory']
4570071,An attempt at showing that function is lipschitzian,"Let $X$ be normed space and let $B$ denote the closed unit ball in $X$ . Suppose we have map $T_0: B \rightarrow B$ which satisfies Lipschitz condition with some constant $k > 1$ . We extend this map to $T_1: 2B \rightarrow B$ in the following way: $$ T_1x = \begin{cases} T_0x, & \text{for } \|x\| \leq 1, \\
(2-\|x\|)T_0(Px), & \text{for } 1 < \|x\| \leq 2,
\end{cases}$$ where $Px = \frac{x}{\|x\|}$ for $\|x \| > 1$ and $Px = x$ for $\|x\| \leq 1$ ( $P$ is radial projection onto unit ball). Im reading a paper where the author claims this extended map still satisfies Lipschitz condition, this time with constant $2k + 1$ . I tried to verify this. $\textbf{Case 1}:$ If $x,y \in B$ , then $\|T_1x - T_1y\| = \|T_0x - T_0y\| \leq k\|x - y\|$ . $\textbf{Case 2}:$ Let $x \in B$ and $y \in 2B \setminus B$ . Then \begin{equation} 
\begin{split}
\|T_1x - T_1y\| & = \|T_0x - (2-\|y\|)T_0(Py)\| \\ 
&\leq \Bigl\|T_0x - T_0(Py) - T_0(Py) + \|y\|T_0(Py) \Bigr\| \\
&\leq \|T_0(Px) - T_0(Py)\| + (\|y\| - 1)\|T_0(Py)\| \\
&\leq k\|Px - Py\| + \|y\| - \|x\| \\ 
&\leq 2k\|x - y\| + \|x - y\| \\
& = (2k + 1)\|x - y\|.
\end{split}
\end{equation} Here I used the fact that in every normed space projection $P$ is Lipschitz with constant $2$ . $\textbf{Case 3}:$ Now this is where I can't finish the job. Let $x,y \in 2B \setminus B$ . I only managed to show that this map is Lipschitz with constant $8k + 1$ . \begin{equation}
\begin{split}
\|T_1x - T_1y\| & = \Bigl\|(2-\|x\|)T_0(Px) - (2-\|y\|)T_0(Py) \Bigr\| \\ 
& = \Bigl\|2T_0(Px) - 2T_0(Py) - \|x\|T_0(Px) + \|y\|T_0(Py) \Bigr\| \\
& \leq 2\|T_0(Px) - T_0(Py)\| + \Bigl\| \|x\|T_0(Px) - \|y\|T_0(Px) \Bigr\| + \Bigl \|\|y\|T_0(P_x) - \|y\|T_0(Py) \Bigr\| \\
& \leq 4k\|x - y\| + \Bigl | \|x\| - \|y\| \Bigr | \|T_0(Px)\| + \|y\|\|T_0(Px) - T_0(Py)\| \\
& \leq 4k\|x - y\| + \|x-y\| + 4k\|x-y\| \\
& = (8k + 1)\|x - y\|.
\end{split}
\end{equation} This is all I can get at the moment. Any help will be appreciated. $\textbf{Edit}:$ There is extra assumption on map $T_0$ which I didnt mention, because I thought its irrelevant in evaluating Lipschitz constant. Namely, we assume $\inf\{\|x - T_0x\|: x \in B\} > 0$ (such maps exist in infinite dimensional spaces).","['normed-spaces', 'functional-analysis', 'lipschitz-functions']"
4570077,Continuous functional calculus of multiplication operator,"Let $A$ be an unbounded operator on $L^2(\mathbb{R}^d)$ unitarily equivalent to a multiplication operator $UAU^*= B $ . One thus has $$UAU^*f(x) = B(x)f(x)$$ for a suitable subset of $L^2(\mathbb{R}^d)$ . Now, one may define functions of $A$ via the functional calculus. I am interested in continuous $f$ . My question is the following: Can one conclude that $Uf(A)U^*$ is again a multiplication operator acting like $$Uf(A)U^* g(x) = f(B(x))g(x) \text{?}$$ I think it's easy to show for polynomials. Unfortunately, I don't see how I can extend this to general (possibly unbounded over $\mathbb{R}^d$ ) continuous $f$ , since the $L^2$ space is over $\mathbb{R}^d$ and I dont know how to do an approximation result here. Is approximation the wrong approach? I saw something about uniqueness of spectral measures on polynomials ( Absolutely continuous spectrum invariant under unitary equivalence ), but I don't understand the argument. Other questions I found were only dealing with bounded $f$ . Edit: I think one possibility is the following: One can restrict $g \in L^2$ with compact support. For such $g$ the equality also holds, especially for an arbitrary continuous $f$ . Then, use the denseness of such functions to show equality in the Operatornorm. That's it. I will post a full argument tomorrow.","['spectral-theory', 'functional-analysis', 'functional-calculus']"
4570095,"Is writing the divergence as a ""dot product"" a deception?","Suppose we have the following vector field in $\mathbb{R}^3$ : $$\vec{F}(x,y,z) = F_x \hat{x}+F_y \hat{y}+F_z \hat{z}$$ where $\hat{x}$ , $\hat{y}$ , and $\hat{z}$ are unit vectors in each of the directions on a Cartesian coordinate system, and $F_x$ , $F_y$ , and $F_z$ are some scalar-valued functions of $x$ , $y$ , and $z$ . Equivalently, in spherical coordinates, $$\vec{F}(r,\theta,\phi) = F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi}$$ where $\hat{x}$ , $\hat{y}$ , and $\hat{z}$ have been transformed into $\hat{r}$ , $\hat{\theta}$ , and $\hat{\phi}$ using the relations here , and $F_r$ , $F_\theta$ , and $F_\phi$ are some combinations of $F_x$ , $F_y$ , and $F_z$ that result from this transformation. My question is about deriving the formula for the divergence of this vector field in spherical coordinates. The divergence is commonly written as $$Div(\vec{F})=\vec{\nabla} \cdot \vec{F}$$ One approach I can take to derive the formula is shown here , where I rewrite $$Div(\vec{F})=\vec{\nabla} \cdot \vec{F} = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}$$ and convert $\frac{\partial}{\partial x} = \frac{\partial r}{\partial x} \frac{\partial}{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial}{\partial \theta} + \frac{\partial \phi}{\partial x} \frac{\partial}{\partial \phi}$ , and $F_x(x,y,z)=F_x(r,\theta,\phi)$ , and so on. Then, I eventually get the correct formula: $$Div(\vec{F}) = \frac{1}{r^2} \frac{\partial (r^2 F_r)}{\partial r} + \frac{1}{r \sin(\theta)} \frac{\partial (F_\theta \sin(\theta))}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi}$$ However, I would like to know why the following does not work . I can rewrite the gradient operator as: $$\vec{\nabla} =\frac{\partial}{\partial x} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z} = \frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi}$$ Then: $$Div(\vec{F}) = \vec{\nabla} \cdot \vec{F} = (\frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi}) \cdot (F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi})$$ Finally, using the relations between the unit vectors for an orthogonal coordinate system (e.g. $\hat{r} \cdot \hat{\theta} = 0$ , $\hat{\phi} \cdot \hat{\phi} = 1$ , etc.), I end up with $$Div(\vec{F}) = \frac{\partial F_r}{\partial r} + \frac{1}{r} \frac{\partial F_\theta}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi}$$ which is not the correct formula. I would like to know where my fault is, and whether this ""dot product notation"" for divergence is only applicable when evaluated in the Cartesian system. And, if so, why?","['divergence-operator', 'coordinate-systems', 'multivariable-calculus', 'spherical-coordinates', 'vector-analysis']"
4570100,Tangent plane at a point to surface $\mathbf{F}=\mathbf{0}$,"Let $\mathbf{F}:\mathbb{R}^5\to\mathbb{R}^3, \mathbf{F}\begin{pmatrix}x_1\\x_2\\y_1\\y_2\\y_3\end{pmatrix}=\begin{bmatrix}2x_1+x_2+y_1+y_3-1\\x_1x_2^3+x_1y_1+x_2^2y_2^3-y_2y_3\\x_2y_1y_3+x_1y_1^2+y_2y_3^2\end{bmatrix}$ and $\mathbf{a}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}$ . The equation $\mathbf{F}=\mathbf{0}$ defines $\mathbf{y}=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}$ as a function of $\mathbf{x}=\begin{bmatrix}x_1\\x_2\end{bmatrix}$ near $\mathbf{a}$ since $\mathbf{F(a)}=\mathbf{0}$ , $\mathbf{F}$ is $\mathcal{C}^1$ and $\frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})=\begin{bmatrix}1 & 0 & 1\\0 & 1 & -1\\ 1 & 1 &1 \end{bmatrix}$ is nonsingular. So we have that near $\mathbf{a}$ there is a $\mathcal{C}^1$ function $\mathbf{\phi}$ such that $\mathbf{y}=\mathbf{\phi(x)}$ and $D\mathbf{\phi}\begin{pmatrix}0\\ 1\end{pmatrix}=-\left(\frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})\right)^{-1} \left(\frac{\partial\mathbf{F}}{\partial\mathbf{x}}(\mathbf{a})\right)=\begin{bmatrix}-3 & -5\\1 & 2\\1 & 4\end{bmatrix}$ . My question: is it correct to say that the tangent plane at $\mathbf{a}$ of the surface $\mathbf{F}=\mathbf{0}$ is the graph of $\mathbf{g}:\mathbb{R}^2\to\mathbb{R}^5$ $\mathbf{g}\begin{pmatrix}x_1\\x_2\end{pmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\\frac{\partial\phi}{\partial x_1}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}+x_2\begin{bmatrix}0\\1\\\frac{\partial\phi}{\partial x_2}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\-3\\1\\1\end{bmatrix}+x_2\begin{bmatrix}0\\1\\-5\\2\\4\end{bmatrix}?$","['multivariable-calculus', 'implicit-function-theorem', 'real-analysis']"
4570127,"Prove That The Directional Derivative Exists at (0,0)","Define $f:\mathbf{R}^{2}\rightarrow\mathbf{R}$ by $f(x,y) = \frac{x^{2}y}{x^{4}+y^{2}}, (x,y)\neq (0,0)$ with $f(0,0):=0$ I am attempting to show that the directional derivative, $\partial_{(a,b)}f(0,0)$ exists for all real $(a,b)\neq (0,0)$ I have managed to show that the directional derivative is $\frac{a^{2}}{b}$ , for $b\neq0$ , but I am unable to figure out how to prove it for the $b=0$ case. Here is my workings for $b\neq0$ : The directional derivative exists if the limit $\lim_{t\rightarrow0}\frac{f(t(a,b))-f(0,0)}{t}$ exists. $\partial_{(a,b)}f(0,0) = \lim_{t\rightarrow0}\frac{\frac{(at)^{2}(bt)}{(at)^{4}+(bt)^{2}}}{t}=\lim_{t\rightarrow0}\frac{a^{2}b}{t^{2}a^{4}+b^{2}}=\frac{a^2}{b}$ This clearly is not the case for $b=0$ , but, provided $a\neq0$ , the directional derivative should exist.","['multivariable-calculus', 'calculus', 'partial-derivative', 'limits', 'derivatives']"
4570136,Central limit of independent indicator functions,"Suppose $\{A_n\}_{n=1}^\infty$ is a sequence of independent events, each with probability $\mathbb{P}(A_n) = p_n$ such that $\sum_{n=1}^\infty p_n = \infty$ .
The goal here is to prove a stronger version of the second Borel-Cantelli Lemma using Lindeberg-Feller Central Limit Theorem; here is my claim which I'm trying to prove $$
\frac{1}{\sigma_n}\left(\frac{\sum_{k=1}^n \mathbf{1}_{A_k}}{\sum_{k=1}^np_k}-1\right)\overset{\mathcal{D}}{\to}N(0,1)\quad\text{as}\quad n\to\infty.
$$ where $\sigma_n^2:=\mathrm{Var}\left(\frac{\sum_{k=1}^n \mathbf{1}_{A_k}}{\sum_{k=1}^np_k}-1\right)$ . To prove this, I constructed a triangular array $\{X_{n,k}\}$ for $n\in\mathbb{N}$ and $k=1,\ldots,n$ with $$
X_{n,k}:=\frac{\mathbf{1}_{A_k}-p_k}{\sum_{k=1}^n p_k}
$$ so that $\mathbb{E}[X_{n,k}]=0$ and $X_{n,1},\ldots,X_{n,n}$ are independent within each row $n$ .
The first hypothesis of Lindeberg-Feller CLT is satisfied: $$
\sum_{k=1}^n\mathbb{E}[(X_{n,k}/\sigma_n)^2] = 1
$$ for all $n\in\mathbb{N}$ , so the limit as $n\to\infty$ of the equation above is $1>0$ .
To deduce the claim, I need to show that the other hypothesis is also satisfied: $$
\sum_{k=1}^n\mathbb{E}\left[(X_{n,k}^2/\sigma_n)^2\cdot\mathbf{1}_{|X_{n,k}|>\varepsilon}\right]\to 0\quad\text{as}\quad n\to\infty
$$ for all $\varepsilon>0$ . Here are my questions: How do I prove that the claim is true (if it is true at all)? More specifically, how do I show that the second hypothesis is satisfied? The claim only shows convergence in distribution. How do I deduce the convergence almost surely to $0$ of the term in the bracket, i.e., $$
\frac{\sum_{k=1}^n \mathbf{1}_{A_k}}{\sum_{k=1}^np_k}-1\to 0\quad\text{a.s.?}
$$","['probability-limit-theorems', 'central-limit-theorem', 'borel-cantelli-lemmas', 'law-of-large-numbers', 'probability-theory']"
4570175,A question about a measure defined using two CDF's,"I know that the sum of measures is a measure (see this question ). However, if $X_1$ and $X_2$ are random variables with CDF's $F_1$ and $F_2$ , respectively, I know that $$F = F_1 + F_2$$ is not even a CDF ( in fact, the CDF of $Z = X_1+X_2$ is given by the convolution of $F_1*F_2$ . See this question ). Moreover, given a CDF $F$ , I also know that I can define measures, for example, as $$\nu(E):= \int_E h(x) dF(x), \quad E \,\, \hbox{any borelian.}$$ In this case I have \begin{equation}\label{I}\tag{I}
d\nu(x)=h(x)dF(x)
\end{equation} So I can 'change' the measures: $$\int g(x)dF(x) = \int \frac{g(x)}{h(x)} d\nu(x)$$ So, my question is related to the following measure: \begin{equation}\label{kasjkjas}\tag{M}
\nu(E) = \sum_{i=1}^2 \int_E |x|^2 dF_i(x)
\end{equation} So I'm dying to interchange the integral and the sum: \begin{equation}\label{II}\tag{II}
\nu(E) =  \int_E |x|^2 dF(x), \quad dF(x):= \sum_{i=1}^2 dF_i(x)
\end{equation} But I don't know if this is mathematically rigorous. The impression I get is that I'm treating $F_1 + F_2$ as a CDF, which is not true. Moreover, as in (\ref{I}) $$d\nu(x)= |x|^2 dF(x) = |x|^2 [dF_1(x)+dF_2(x)]$$ So is it correct to define $dF(x)$ as in (\ref{II}) even though $F_1+F_2$ is not well defined? If this is not correct, how can we rewrite the following integral in terms of $dF_1(x)$ and $dF_2(x)$ : $$\int f(x) \frac{1}{|x|^2} d\nu(x)\,\,\,\,?$$ (If (\ref{II}) is correct, ignore this last question ) Attempt after comments According to Snoop's comments, setting $\quad dF(x):= \sum_{i=1}^2 dF_i(x)$ is mathematically incorrect. So, I define $$\nu_i (E):= \int_E |x|^2 dF_i(x), \quad i =1,2$$ It implies $$d\nu_i(x)= |x|^2 dF_i(x), \quad i =1,2$$ So substituting in (\ref{kasjkjas}), we have: $$\nu(E)= \int_E d\nu_1(x)+ \int_E d\nu_2(x)= \nu_1(E)+\nu_2(E)$$ Moreover $d\nu= d(\nu_1 + \nu_2)= d\nu_1 + d\nu_2$ . So, $$\int f(x) \frac{1}{|x|^2} d\nu(x) = \int f(x) \frac{1}{|x|^2} (d\nu_1(x) + d\nu_2(x)) = \int f(x) \frac{1}{|x|^2} d\nu_1(x) + \int f(x) \frac{1}{|x|^2}  d\nu_2(x)$$ So we conclude $$\int f(x) \frac{1}{|x|^2} d\nu(x) = \sum_{i=1}^2 \int f(x) dF_i(x) $$","['measure-theory', 'definition', 'probability-theory', 'real-analysis']"
4570178,Norm of bounded operators of direct sum,Yesterday I posted this one regarding direct sum on Hilbert spaces $H_1$ and $H_2$ - have a look! Direct sum of two Hilbert spaces is a inner product. I am studying bounded operators and I just want to know if we can say something about the norm of my direct sum. I.e. let $T_1\in B(H_1)$ and $T_2\in B(H_2)$ then $T_1\oplus T_2$ is bounded by using the definition of bounded operator but how about its norm? I cannot se how this should be possible. Any suggestion? I belive we should use operator norm but I still cannot see how that would work. Thanks in advance.,"['operator-algebras', 'operator-theory', 'direct-sum', 'hilbert-spaces', 'functional-analysis']"
4570190,"A $q$-Pochhammer integral: $\int_0^\infty\!\frac1{1+x}\frac2{2+x}\frac4{4+x}\frac8{8+x}\cdots\,dx=\log2$","Question : Show that for any $a>1$ , we have $$\int_0^\infty\!\frac{dx}{(1+x)(1+\frac xa)(1+\frac x{a^2})(1+\frac x{a^3})\cdots}\overset?=\log a.$$ WolframAlpha is able to evaluate specific instances of the above integral numerically, but not symbolically. For fixed $a>1$ , denote the integrand by $f_a(x)$ . It is not hard to check that $f_a$ is characterised by the following properties: $f_a:[0,\infty)\to\mathbb R$ is continuous; $f_a(0)=1$ ; $(1+x)f_a(x)=f_a(\frac xa)$ for all $x>0$ . We also have other identities relating the integrands for different values of $a$ , eg. $f_a(x)=f_{a^2}(x)f_{a^2}(\frac xa)$ . In fact, we can write $f_a(x)$ in terms of the q-Pochhammer symbol , namely $$f_a(x)=\frac1{\left(-x;\frac1a\right)_\infty}.$$ It is not clear to me that the desired identity is an easy consequence of above functional equations and relations. It is also not obvious how to set up the usual tools for 1D definite integrals (Feynman trick, complex contour integration, ...).","['integration', 'special-functions']"
4570195,"Let $\mu_n,\mu \in \mathcal M(X)$ such that $\mu_n \rightharpoonup \mu$ and $[\mu_n] \to [\mu]$, then $|\mu_n| \rightharpoonup |\mu|$","Let $X$ be a metric space, $\mathcal M(X)$ the space of all finite signed Borel measures on $X$ , $\mathcal M_+(X)$ the space of all finite nonnegative Borel measures on $X$ , and $\mathcal C_b(X)$ be the space of real-valued bounded continuous functions on $X$ . Then $\mathcal C_b(X)$ is a real Banach space with supremum norm $|\cdot|_\infty$ . For $\mu \in \mathcal M(X)$ , let $(\mu^+, \mu^-)$ be its Jordan decomposition and $|\mu| := \mu^+ + \mu^-$ its variation. We endow $\mathcal M(X)$ with the total variation norm $[\cdot]$ where $[\mu] := |\mu|(X)$ . Then $(\mathcal M(X), [\cdot])$ is a Banach space. For $\mu_n,\mu \in \mathcal M(X)$ , we define the weak convergence $$
\mu_n \rightharpoonup \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_b(X).
$$ I have mimicked the proof in this answer for below theorem, i.e., Theorem: Let $\mu_n,\mu \in \mathcal M(X)$ such that $\mu_n \rightharpoonup \mu$ and $[\mu_n] \to [\mu]$ , then $|\mu_n| \rightharpoonup |\mu|$ . Could you have a check on my attempt? Proof: By Portmanteau theorem for $\mathcal M_+(X)$ , it suffices to prove that $$
|\mu|(\Theta) \leq \liminf _{n \rightarrow \infty}\left|\mu_n\right|(\Theta) 
$$ for every open subset $\Theta$ of $X$ . Fix $\varepsilon>0$ and $\delta := \varepsilon/4$ . Let $(D^+, D^-)$ be the Hahn decomposition $X$ w.r.t. $\mu$ , i.e., $\mu$ is positive on $D^+$ and negative on $D^-$ . Let $A^\pm:=\Theta\cap D^\pm$ . Since $|\mu|$ is inner regular, there are closed subsets $C^\pm$ of $X$ such that $C^\pm \subset A^\pm$ and $|\mu|(A^\pm \setminus C^\pm) < \delta$ . Since $X$ is normal, there are open subsets $O^\pm$ of $X$ such that $C^\pm \subset O^\pm$ and $O^+ \cap O^- = \emptyset$ . WLOG, we assume $O^\pm \subset \Theta$ . By Urysohn's lemma, there is $f^\pm:X \to [0, 1]$ continuous such that $f^\pm (C^\pm) =1$ and $f^\pm (X \setminus O^\pm) =0$ . Notice that $f^\pm$ is supported on $O^\pm$ , so $f^+ f^- =0$ . Let $f:=f^+-f^-$ . Then $f$ is continuous and $|f| \le 1$ . Hence $|f| \in \mathcal C_b(X)$ . It follows from $O^\pm \subset \Theta$ that $|f| \le 1_\Theta$ . Let's prove that $$
\int_X f \mathrm d \mu \ge |\mu|(\Theta) - \varepsilon.
$$ We have $$
\begin{aligned}
    |\mu|(O^-\setminus C^-) &\le |\mu|((\Theta\setminus O^+)\setminus C^-) \\ 
    &= |\mu|(\Theta)-|\mu|(O^+)-|\mu|(C^-) \\ 
    &\le |\mu|(\Theta)-|\mu|(C^+)-|\mu|(C^-) \\ 
    &<|\mu|(\Theta)-|\mu|(A^+)-|\mu|(A^-)+2\delta \\
    &= 2\delta.  
\end{aligned}
$$ Then $$
\begin{aligned}
    \int_X f\mathrm d\mu&=\int_{O^+} f^+\mathrm d\mu-\int_{O^-} f^-\mathrm d\mu \\ 
    &\ge\int_{C^+} f^+\mathrm d\mu-\int_{C^-} f^-\mathrm d\mu -\int_{O^-\setminus C^-} f^-\mathrm d\mu \\ 
    &\ge \mu(C^+)-\mu(C^-) -\int_{O^-\setminus C^-} f^-\mathrm d\mu \\ 
    &\ge\mu(C^+)-\mu(C^-) -|\mu|(O^-\setminus C^-) \\ 
    &>\mu(A^+)-\delta-\mu(A^-)-\delta -2\delta \\ 
    &=|\mu|(\Theta)-4\delta=|\mu|(X)-\varepsilon.
\end{aligned}
$$ Finally, $$
\begin{align}
\int_X f \mathrm d \mu = \lim_n \int_X f \mathrm d \mu_n \le \liminf_n \int_X |f| \mathrm d |\mu_n| \le \liminf_n \int_X 1_{\Theta} \mathrm d |\mu_n| \le \liminf_n |\mu_n|(\Theta).
\end{align}
$$ This completes the proof. Update: In above proof, we basically prove the following lemma, i.e., Lemma 1: If $X$ is a metric space and $\mu_n,\mu \in \mathcal M(X)$ such that $\mu_n \rightharpoonup \mu$ , then $$
|\mu|(\Theta) \leq \liminf _{n \rightarrow \infty}\left|\mu_n\right|(\Theta) 
$$ for every open subset $\Theta$ of $X$ . Let $\mathcal C_c(X)$ be the space of real-valued continuous functions on $X$ with compact supports. For $\mu_n,\mu \in \mathcal M(X)$ , the weak $^*$ convergence is defined by $$
\mu_n \overset{*}{\rightharpoonup} \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_c (X).
$$ With stronger assumption on $X$ , we can get stronger result Lemma 2: If $X$ is a locally compact separable metric space and $\mu_n,\mu \in \mathcal M(X)$ such that $\mu_n \overset{*}{\rightharpoonup} \mu$ , then $$
|\mu|(\Theta) \leq \liminf _{n \rightarrow \infty}\left|\mu_n\right|(\Theta) 
$$ for every open subset $\Theta$ of $X$ . For the proof of Lemma 2 , we need the following results, i.e., Lemma 3 Let $X$ be a locally compact separable metric space. Then $X$ is a Radon space . Lemma 4 Let $X$ be a locally compact Hausdorff space. Let $K$ be a compact subset of $X$ and $U$ an open subset of $X$ such that $K \subset U$ . Then there is an open subset $V$ of $X$ such that $K \subset V \subset \overline V \subset U$ and $\overline V$ is compact. Lemma 3 allows us to get compact subsets $K^\pm$ instead of just closed subsets $C^\pm$ . Lemma 4 allows us to construct an $f \in \mathcal C_c(X)$ instead of just $f \in \mathcal C_b(X)$ .","['measure-theory', 'signed-measures', 'weak-convergence', 'metric-spaces', 'general-topology']"
4570202,Probability of receiving a signal,"You have a signal receiver that is able to receive two signals, 1 and 2. Both signals have mean 0 and arrive with equal frequency. Signal 1 is normally distributed with variance 4, and signal 2 is normally distributed with variance 9. You receive a signal with magnitude 2. What is the probability that the signal you received is signal 1? Do I solve this problem by finding the sd of signal 1 as $\sqrt{4}=2$ , that a signal with magnitude 2 is $\frac{2-0}{2} = 1$ standard deviation away from the mean, and the probability that that occurs is the answer? I'm also confused on how to incorporate the distribution of signal 2 in my answer.",['statistics']
4570205,Portmanteau theorem for finite signed Borel measures,"Let $X$ be a metric space, $\mathcal M(X)$ the space of all finite signed Borel measures on $X$ , $\mathcal M_+(X)$ the space of all finite nonnegative Borel measures on $X$ , and $\mathcal C_b(X)$ be the space of real-valued bounded continuous functions on $X$ . Then $\mathcal C_b(X)$ is a real Banach space with supremum norm $|\cdot|_\infty$ . For $\mu \in \mathcal M(X)$ , let $(\mu^+, \mu^-)$ be its Jordan decomposition and $|\mu| := \mu^+ + \mu^-$ its variation. We endow $\mathcal M(X)$ with the total variation norm $[\cdot]$ where $[\mu] := |\mu|(X)$ . Then $(\mathcal M(X), [\cdot])$ is a Banach space. For $\mu_n,\mu \in \mathcal M(X)$ , we define the weak convergence $$
\mu_n \rightharpoonup \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_b(X).
$$ I would like to prove below result, i.e., Theorem: Let $\mu_n,\mu \in \mathcal M(X)$ such that $\limsup_n [\mu_n] \le [\mu]$ . Then $\mu_n \rightharpoonup \mu$ IFF $\mu^+_n \rightharpoonup \mu^+$ and $\mu^-_n \rightharpoonup \mu^-$ . Could you have a check on my attempt? Proof: The reverse direction is obvious. Let's prove the other one. First, we prove that $\liminf_n [\mu_n] \ge [\mu]$ . It's well-known that $$
\mathcal M(X) \to \mathcal C_b(X)^*, \nu \mapsto \left (L_\nu :f \mapsto \int_X f \mathrm d \nu \right).
$$ is an isometrically isomorphic embedding. This implies $[\nu] = \|L_\nu\|$ for all $\nu \in \mathcal M(X)$ . Notice that $\mu_n \rightharpoonup \mu$ if and only if $L_{\mu_n} \to L_\mu$ in the weak $^*$ topology $\sigma(\mathcal C_b(X)^*, \mathcal C_b(X))$ . As such, $\|L_\mu| \le \liminf_n \|L_{\mu_n}\|$ and thus $\liminf_n [\mu_n] \ge [\mu]$ . As such, $\lim_n [\mu_n] = [\mu]$ . Lemma Let $\mu_n,\mu \in \mathcal M(X)$ such that $\mu_n \rightharpoonup \mu$ and $[\mu_n] \to [\mu]$ , then $|\mu_n| \rightharpoonup |\mu|$ . By our Lemma , $|\mu_n| \rightharpoonup |\mu|$ . So we have $\mu^+_n - \mu_n^- \rightharpoonup \mu^+ - \mu^-$ and $\mu^+_n + \mu_n^- \rightharpoonup \mu^+ + \mu^-$ . The claim then follows.","['measure-theory', 'signed-measures', 'metric-spaces', 'weak-convergence']"
4570210,How to count polyhedral rotations?,"Suppose I have a regular polytope $P$ which I'm representing as a graph $G_P$ with vertices and edges.  I can already put this data into a computer program to find a list of symmetries of $P$ ---they're just the graph isomorphisms from $G_P$ to itself. But as I understand it, half(?) of these isomorphisms will be orientation-preserving and the other half will be orientation-flipping. How can I make a list of the graph isomorphisms that are orientation-preserving?  Does the adjacency data from the graph (which is abstract and not embedded in any geometric space) already determine which isomorphisms preserve/flip orientation, or is it necessary to supply some additional structure such as an embedding of the graph, a directed traversal of the edges, or a list of the vertices of a face in clockwise order?  Given whatever additional data, what should the algorithm be? Edit: I'm wondering if I can find the orientation-preserving symmetries using algebra: I believe the square of any symmetry is a rotation. So there's a subgroup generated by the squares, and they're all rotations. Is this all the rotations (I'd generally doubt it), or how might we get the rest? Maybe there's only a few groups that satisfy the properties that (1) they contain exactly half the members of the symmetry group (2) they contain all the perfect squares.  In that case, it might be that any one of those groups could be the rotation group given an appropriate embedding --- i.e. each one is isomorphic to the rotation group. In that case I'd wonder if we need any additional constraints to guarantee that all the subgroups that satisfy these constraints are (isomorphic to) the group of rotations. Edit #2: I can prove that the sign of a group automorphism is independent of choice of embedding, but I am still searching for a constructive method to compute it from the graph without using coordinates as an intermediate. The proof that the sign is independent of the choice of embedding is as follows: Let $P$ be a regular polytope, fix $S\subset \mathbb{R}^n$ as points for $P$ in $\mathbb{R}^n$ , let $G$ be the abstract graph of $P$ and let $V$ be its vertices. A graph automorphism is any bijection $\rho:V\rightarrow V$ that preserves and reflects the adjacency relationships of the graph $G$ . A choice of coordinates for $G$ is a map $\chi: V\cong S\subset \mathbb{R}^n$ .  Let's fix one such coordinate choice as our canonical coordinates: $\chi :V\cong S$ . Every other choice of coordinates factors uniquely through $\chi$ : if $\chi^\prime$ is any other choice of coordinates, then $\chi^\prime = \sigma\circ \chi$ for some unique isometry $\sigma:\mathbb{R}^n\rightarrow \mathbb{R}^n$ . Given a choice of coordinates $\chi$ , every graph automorphism $\rho:V\rightarrow V$ corresponds to a unique isometry of Euclidean space $\sigma_\rho :\mathbb{R}^n\rightarrow \mathbb{R}^n$ , such that $\sigma_\rho \circ \chi = \chi \circ \rho$ (""the isometry after embedding is equivalent to the automorphism before embedding"") Let $\rho:V\rightarrow V$ be any graph automorphism, and let $\chi, \chi^\prime : V\cong S \subset \mathbb{R}^n$ be two embeddings of the graph. We'll show that, under the two embeddings $\chi,\chi^\prime$ , the automorphism $\rho$ induces two isometries $\sigma,\sigma^\prime$ with the same sign. This shows that the isometry induced by $\rho$ has the same sign under any choice of coordinates, which means that $\rho$ 's sign is determined independent of any embedding. By (5), we can find the unique isometries $\sigma$ and $\sigma^\prime$ such that $$\sigma\circ \chi = \chi \circ \rho$$ and $$\sigma^\prime \circ \chi^\prime = \chi^\prime \circ \rho.$$ By (4), we can factor $\chi^\prime = \tau\circ \chi$ for some unique isometry $\tau$ . Factoring $\tau$ out of the second equation above gives: $$\sigma^\prime \circ \tau \circ \chi = \tau \circ \chi \circ \rho$$ Composing $\tau$ on the left of the first equation gives: $$\tau \circ \sigma \circ \chi = \tau \circ \chi \circ \rho $$ The right hand sides of these two equations are equal; hence their left hand sides must be equal as well: $$\sigma^\prime \circ \tau \circ \chi = \tau \circ \sigma \circ \chi$$ The choice of coordinates is injective, so we can drop it, leaving just a commutative square: $\sigma^\prime \circ \tau = \tau \circ \sigma$ . Now every term in this equation is an isometry $\mathbb{R}^n\rightarrow \mathbb{R}^n$ , meaning it has a determinant whose value is $\pm 1$ . $$\det(\sigma^\prime \circ \tau) = \det(\tau \circ \sigma)$$ $$\det(\sigma^\prime)\cdot \det(\tau) = \det(\tau) \cdot \det(\sigma)$$ $$\det(\sigma^\prime) = \det(\sigma)$$ Hence the two isometries induced by the two choices of coordinates for the graph automorphism $\rho$ have the same sign, QED.","['graph-isomorphism', 'polyhedra', 'geometry', 'coxeter-groups', 'symmetry']"
4570255,Circulation and the flux in a field,"Problem: Find the circulation and flux of the field $F=x^2i+y^2j$ around and across the closed semicircular path that consists of the semicircular arch $r_1(t)=(a\cos(t))i+(a\sin(t))j$ , $0\le t\le\pi$ , followed by the line segment $r_2(t)=ti$ , $-a\le t\le a$ . Then I compute the followings: $r_1(t)=(a\cos(t))i+(a\sin(t))j\implies r_1'(t)=(-a\sin(t))i+(a\cos(t))j$ $F(r_1(t))=(a^2\cos^2t)i+(a^2\sin^2t)j$ $F(r_1(t))\cdot r_1'(t)=-a^3\sin t\cos^2 t+a^3\sin^2t\cos t$ Circulation along $r_1=\int_{-a}^aa^3(\sin^2 t\cos t-\sin t\cos^2t)dt=\frac{2}{3}a^3\sin^3a$ Flux along $r_1=\int_0^{\pi}(a\cos t)(a\cos t)-(a\sin t)(-a\sin t)dt=a^2\pi$ $r_2(t)=ti\implies r_2'(t)=i$ $F(r_2(t))=t^2i$ $F(r_2(t))\cdot r_2'(t)=t^2$ Circulation along $r_2=\int_{-a}^at^2dt=\frac{2}{3}a^3$ Flux along $r_2=\int_{-a}^a(t(0)-(0)(1))dt=0$ Hence the total circulation $=\frac{2}{3}a^3\sin^3a+\frac{2}{3}a^3=\frac{2}{3}a^3(1+\sin^3a)$ , and the total flux $=a^2\pi+0=a^2\pi$ . My questions are: Are my computations valid? (I mean the steps here, not the arithmetics) It seems that the calculation of the flux is independent of the field. Is that normal?","['multivariable-calculus', 'solution-verification']"
4570363,A lemma in the proof of push-relabel maximum flow algorithm.,"It is lemma 4.3, I have been thinking about it for 3 days. A detailed description is given below. In ""A new approach to the Maximum-Flow Problem"", 1988, by Goldberg, to prove the $\mathcal{O}(n^3)$ bound, the author breaks the whole execution into numerous ""pass"", a pass is just like a run in the BFS-style traversal of executing push/relabel on the vertices activated previously. He tries to show that there exists a $\mathcal{O}(n^2)$ upper bound for the number of passes, and in each pass, the number of non-saturating pushes must be lower than $n$ . The latter statement is trivial, however, the former one is hard for me to show. He established a measure $\Phi$ , the maximum of the label of all active vertices. And it might go up, go down or remain constant in a pass. When it goes up or remains constant, there must be an increase in the sum of labels over all vertices, which has a upper-bound $2n^2$ . So the total number of passes that make it go up or remain unchanged is $2n^2$ . Considering that $\Phi$ is equal (both be 0) after the algorithm terminates (where there are no active vertices) and before the first pass(when using simple initialization), the number of passes that decrease the $\Phi$ is smaller than the upper bound of the number of passes that increase $\Phi$ . So here another upper bound of $2n^2$ can be drawn. (I think here might be problematic, $\Phi$ can follow a pattern like $1, n^2, n^2 - 1, n^2 - 2, ...., 1, n^2$ , here the number of increasing path is at a magtitude $n^2$ lower than that of decreasing pass. Though clearly it is impossible, but proving that need other information on the pattern of the move of $\Phi$ . ) In total, the number of passes has an upper bound $4n^2$ . I could not think of why there must be one label increased (equivalently, a relabel operation triggered) if $\Phi$ stays the same. The proof above is slightly modified to make it easier to follow, but the key statement, as in original text, 'If $\Phi$ is not changed by the pass, some vertex label must increase by at least 1' is still mysterious for me. It must be true that the number of passes cannot be larger than $4n^2$ , but how to prove it. The paper is now open access, here is the link: goldberg","['graph-theory', 'network-flow', 'discrete-mathematics', 'algorithms', 'computational-complexity']"
4570390,There is a diffeomorphism $f$ of $M$ such that $f(x_i) = y_i$ and $df_{x_i}(v_i) = w_i$,"Let $x_1,...,x_k$ and $y_1,...,y_k$ be two sets of distinct points in a connected smooth manifold $M$ with $\dim M>1$ , and $v_1,...,v_k$ and $w_1,...,w_k$ be the corresponding two sets of nonzero tangent vectors at these points. Show that there is a diffeomorphism $f$ of $M$ such that $f(x_i) = y_i$ and $df_{x_i}(v_i)= w_i$ for $i =1,2,...,k$ . I think I can prove for the case $k=1$ . First suppose $x,y$ are close enough so that there is an open coordinate ball chart $(B,\varphi)$ containing $x$ and $y$ . Let $(x^i)$ a coordinate on $B$ and let $v_1 = v^i{\partial\over\partial x^i}\big|_x$ and $w_1 = w^i{\partial\over\partial x^i}\big|_y$ . Define a curve $\gamma:[0,1]\to B$ from $x$ to $y$ such that $\gamma'(0) = (v^1,...,v^n)$ and $\gamma'(1) = (w^1,...,w^n)$ in local coordinate. Then this gives a smooth vector field $X$ along $\gamma$ such that $X_x = v_1$ and $X_y =w_1$ . By the uniqueness of the flow, the corresponding flow of $X$ is $\gamma$ . Since the image of $\gamma$ is closed, we can extend $X$ onto $M$ with compact support (also denoted by $X$ ). Since $X$ has compact support, $X$ admits the global flow $\Phi$ . Then $\Phi_1:M\to M$ is a diffeomorphism such that $\Phi_1(x) =y$ and $\color{red}{d(\Phi_1)_x(v_1) =w_1}$ . The usual connectedness argument proves the statement. I'm not quite confident about my $k=1$ proof (the red part) but it should be like this (I guess). But for $k>1$ , I have no any idea. Please help. The vector field along the curve I have in mind:","['smooth-manifolds', 'differential-geometry']"
4570436,Sum of two i.i.d random variables cannot have uniform distribution,"My goal is to show that for $U \sim \mathcal{U}[-1,1]$ there are no i.i.d random variables $X,Y$ such that $U = X+Y$ . I have seen some arguments why it is impossible, but I wanted to do it with characteristic functions. So the assumptions lead me to the equality $$\varphi_X^2(t) = \frac{\sin{t}}{t}$$ Is there a simple argument why it cannot happen?","['characteristic-functions', 'probability-theory']"
4570449,"Proving that $S\mapsto\text{Hom}(\bigwedge^2S,\mathbb R)$ defines a vector bundle over a Grassmanian.","My question has to do with the most voted answer to this post . There, it is suggested that the map $$S\in G_n(\mathbb R^{2n})\mapsto\text{Hom}(\wedge^2S,\mathbb R)$$ determines a vector bundle over the Grassmanian $G_n(\mathbb R^{2n})$ . However, I don't know how to prove this rigorously. Specifically, I am having trouble with the maps one should take as trivializations. Lets recall that $G_n(\mathbb R^{2n})$ has a natural smooth manifold structure, in which the coordinate domains are of the form $$U_T=\{V\in G_n(\mathbb R^{2n})\,|\,V\cap T=\{0\}\}$$ I would like to give trivializations based on these open sets. More specifically, I would like to find maps $$\Phi_T:\pi^{-1}(U_T)\rightarrow U_T\times\mathbb R^{\bullet}$$ that are bijective and isomorphisms in its restriction to each fiber. However, for this we should have some way of parametrizing bases of vector spaces $V$ such that $V\cap T=\{0\}$ in terms of a canonical, fixed basis of $T$ . My idea is that we could complete such a basis of $T$ to a basis of $\mathbb R^{2n}$ , then apply rotations in suitable directions to parametrize all such vector spaces. This would give a decomposition of $\pi^{-1}(U_T)$ into a product of the form $U_T\times\mathbb R^\bullet$ , where some part of the $\mathbb R^\bullet$ part has to encode the information about the rotations performed. I don't know if this is the way to think about this problem, but thanks in advance for your answers.","['smooth-manifolds', 'grassmannian', 'vector-bundles', 'differential-topology', 'differential-geometry']"
4570484,Continuous extensions for an inequality,"Let $\{x_n\}_{n=1}^\infty$ be a countable subset of $\ell_1(\mathbb{N})$ . Suppose that there exists a constant $C>0$ such that for all $y\in c_{00}(\mathbb{N})$ , $$
\limsup_{n\to\infty}|\langle y,x_n\rangle|\leq C\|y\|_{\infty},\qquad(1)
$$ Now could we prove (1) holds for all $y\in c_0(\mathbb{N})$ ? For definition of $c_{00}(\mathbb{N})$ , see $c_{00}(\mathbb{N})$ . Note that $c_{00}(\mathbb{N})$ is dense in $c_0(\mathbb{N})$ . My attempt: For any $y\in c_0(\mathbb{N})$ , there exists a coutable subset $\{y_k\}_{k=1}^\infty$ in $c_{00}(\mathbb{N})$ such that $\|y_k-y\|_\infty\to0$ . Notice that for each $n$ , we have $$
|\langle y,x_n\rangle|=\lim_{k\to\infty}|\langle y_k,x_n\rangle|
$$ and hence $$
\limsup_{n\to\infty}|\langle y,x_n\rangle|=\limsup_{n\to\infty}\lim_{k\to\infty}|\langle y_k,x_n\rangle|\qquad(2)
$$ If we are allowed to swap the limit of RHS in (2), then with (1) we have $$
\lim_{k\to\infty}\limsup_{n\to\infty}|\langle y_k,x_n\rangle|\leq C\lim_{k\to\infty}\|y_k\|_\infty=C\|y\|_\infty
$$ which is the desired result. Now the concern is whether we can swap the limit in (2). Or any additional condition needed for $\{x_n\}_{n=1}^{\infty}$ to make the swap valid? Update: Thanks for the counter example from Gred! If we impose the condition that for any $1<s\leq2$ , $\{x_n\}_{n=1}^{\infty}$ is bounded in $\ell_s(\mathbb{N})$ , will the result be true? In fact, the problem is from a paper and the authour aims to show $\{x_n\}_{n=1}^{\infty}$ is bounded in $\ell_1(\mathbb{N})$ . So we can not assume $\{x_n\}_{n=1}^{\infty}$ is bounded in $\ell_1(\mathbb{N})$ .","['limits', 'functional-analysis', 'limsup-and-liminf', 'real-analysis']"
4570499,What is meant by linearity in more than one dimension?,"I am slightly unclear on some terminology that I have come across in the following question: Is $\partial _vf(0,0)$ linear in $v$ ? My understanding of linearity for a single variable function is fine, although I'm a little unclear on what it means for a function that takes multiple variables? What is the generalisation here, and how should I interpret this word in these types of problems? I would be grateful for any clarification here regarding the meaning of linearity in this context.","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'terminology']"
4570513,Interesting double integral $\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy$,"I found this integral in a FB page: $$I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy$$ I am trying to evaluate it, but it is hard. My attemps: convert by using substitution $x=r\cos^{2/3}({\theta}), y=r\sin^{2/3}({\theta})$ then it becomes: $$I=\frac{4}{3}\int_{0}^{\frac{\pi}{4}}\int_{0}^{\frac{1}{\cos^{2/3}({\theta})}}\frac{\log{(1+r^3)}-\log{(r^2\cos^{2/3}({\theta})\sin^{2/3}({\theta}))}}{(1+r^3)\cos^{1/3}({\theta})\sin^{1/3}({\theta})}dxdy$$ And it seems impossible due to the antiderivative is a mess. I also try to transform: $x^3+y^3=(x+y)^3-3xy(x+y)$ so i set $u=x+y, v=xy$ but it is still hard. Another try, because of symmetry then the integral can be rewritten as: $$I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(x^2)}}{1+x^3+y^3}dxdy$$ But it doesn't help much. I also add more variable like this: $$I=\int_{0}^{1}\int_{0}^{1}\int_{0}^{\infty}(\log{(1+x^3+y^3)}-\log{(x^2)})e^{-z(1+x^3+y^3)}dxdydz$$ And it leads to something messier.
So, I need some helps from every one, thank you for your time.",['multivariable-calculus']
4570547,Probability of sum of independent events,"Let events $A_1$ , $A_2$ , $A_3$ be mutually independent such that $P(A_j) = (0,5)^j$ for $j = 1,2,3$ . Find $P((A_1 \cup A_2)-A_3)$ . My solution: $$ P((A_1 \cup A_2)-A_3) = $$ $$P((A_1 \cup A_2)\cap A'_3) = $$ $$  P((A_1 \cap A'_3)\cup (A_2 \cap A'_3)) = $$ $$  P((A_1 \cap A'_3)) + ((A_2 \cap A'_3)) - P((A_1 \cap A'_3) \cap (A_2 \cap A'_3)) = $$ $$  P((A_1 \cap A'_3)) + ((A_2 \cap A'_3)) - P(A_1 \cap A'_3 \cap A_2 \cap A'_3) = $$ Given that events are mutually independent we have: $$  P(A_1)P(A'_3) + P(A_2)P(A'_3) - P(A_1)P(A'_3)P(A_2)P(A'_3) = $$ $$  P(A_1)(1-P(A_3)) + P(A_2)(1-P(A_3)) - P(A_1)P(A_2)(1-P(A_3))^2 = $$ $$  (1-P(A_3))(P(A_1) + P(A_2) - P(A_1)P(A_2)(1-P(A_3)) = $$ $$  (1-1/8))(1/2 + 1/4 - (1/2*1/4)(1-1/8)) = 287/512$$ Is that correct? The number seems bit off. Thank you.","['elementary-set-theory', 'probability']"
4570556,How can I prove this exercise using Hahn Banach theorem?,"Let $X$ be a normed space, $Y$ a closed subspace of $X$ and $x\in X\setminus Y$ . I need to prove that there exists $\bar f\in X^*$ s.t. $\bar f=0$ on $Y$ and $\bar f(x)=\inf \{||x-y||: y\in Y\}=:d(x,Y)$ My idea was the following. Let me define $$p=d(\cdot, Y):X\setminus Y\rightarrow \Bbb{R}; x\mapsto d(x,Y)$$ and $$f:Y\rightarrow \Bbb{R}; y\mapsto 0$$ Then clearly $p$ is sublinear and $f$ is linear such that $f\leq q$ . Now applying Hahn-Banach theorem we get that there exists $\bar f\in X^*$ such that $\bar f(x)=f(x)=0$ for all $x\in Y$ and $\bar f(x)\leq p(x)=d(x,Y)$ for all $x\in X$ . Now I thought that I would be done if I could show that $\bar f=p(x)$ for all $x\in X\setminus Y$ . But I don't see how to do this. Could someone help me? Edit Could I finish the proof as follows: Let me assume $\bar f(x)<d(x,Y)$ for all $x\in X$ then pick $x\in Y$ so $$0=\bar f(x)<0$$ which is a contradiction, hence $\bar f(x)=p(x)$ ?","['analysis', 'real-analysis', 'solution-verification', 'functional-analysis', 'hahn-banach-theorem']"
4570591,Is there a $5 \times 5$ matrix with exactly $2$ fun elements?,"An element of a matrix is called 'fun' if changing this element
changes the determinant. Is there a $5 \times 5$ matrix with exactly $2$ fun elements? My intuition is that there does not exist such a matrix. I have tried using properties of the determinant to no avail. I have also tried to relate eigenvalues and the characteristic polynomial to this problem, but I'm having trouble relating eigenvalues to specific elements of a matrix. Any help or guidance would be appreciated, thank you!","['matrices', 'linear-algebra']"
4570609,Inequality Proof by Induction involving Euler Totient function and Summation of Euler's Phi function,"I want to Prove the following result using induction: Show that P $_n$ : \begin{equation}
2\sum_{k=1}^n \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right)+2 \geq \frac{12(n-1)^2}{\pi^2}\label{eq:15}
\end{equation} for all $n,k\in\mathbb{Z}_+$ , where the product is over all prime factors of $k$ . Proof: P $_1$ case: For $n = 2$ we get, \begin{equation}
2\sum_{k=1}^2 \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right)+2 \geq \frac{12(2-1)^2}{\pi^2} \implies 4 \geq \frac{12}{\pi^2}, \hspace{2em} \text{P$_1$ is True.} 
\end{equation} P $_\alpha$ case: Suppose Eqn\eqref{eq:15} holds for some $\alpha$ s.t. \begin{equation}2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right)+2 \geq \frac{12(\alpha-1)^2}{\pi^2} ,\hspace{2em} \forall (\alpha,k) \in \mathbb{Z}_+. \label{eq:17}
\end{equation} P $_{\alpha + 1}$ case: Now consider that it holds for some $\alpha$ + 1, \begin{equation} 2\sum_{k=1}^{\alpha + 1} \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right)+2 \geq \frac{12((\alpha +1)-1)^2}{\pi^2},\hspace{2em} \label{eq:18}
\end{equation} Simplifying and expanding the LHS and RHS we get, \begin{equation} 2\left(\underbrace{\prod_{p \vert 1}\left(1-\frac{1}{p}\right) + ... + \prod_{p \vert \alpha}\left(1-\frac{1}{p}\right)}_{\text{using} \ \text{P}_\alpha} + \prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right) \right)+2 \geq \frac{12\alpha^2}{\pi^2} ,\hspace{2em} \label{eq:19}
\end{equation} Therefore we get, \begin{equation}
\left[2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right) + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) \right]+2 \geq \frac{12\alpha^2}{\pi^2} ,\hspace{2em} \label{eq:20}
\end{equation} We are assuming that Eqn\eqref{eq:17} is true so, if we replace $\Phi(\alpha)$ in the above expression with the smaller number $\frac{12(\alpha-1)^2}{\pi^2}$ , we produce a smaller result. So: \begin{equation}2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right) + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 \geq\frac{12(\alpha-1)^2}{\pi^2} + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2. \label{eq:21}
\end{equation} Now, \begin{equation}
\frac{12(\alpha-1)^2}{\pi^2} + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 = \frac{12(2-\alpha)^2}{\pi^2}. \label{eq:22}
\end{equation} Thus, \begin{equation}
2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right) + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 \geq \frac{12(2-\alpha)^2}{\pi^2} \label{eq:23}
\end{equation} Since $\alpha$ $\geq$ 1, we know that $\frac{12(2-\alpha)^2}{\pi^2}$ $\geq$ 0, so \begin{equation}
2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right) + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 \geq \frac{12(\alpha-1)^2}{\pi^2} \geq 0 \label{eq:24}
\end{equation} Or, \begin{equation}
2\Phi(\alpha) + \varphi(\alpha + 1) + 2 \geq \frac{12(\alpha-1)^2}{\pi^2} \geq 0 \label{eq:25}
\end{equation} Where, \begin{equation}
\Phi(\alpha) = \sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right)
\end{equation} and \begin{equation}
\varphi(\alpha + 1) = \prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)
\end{equation} Hence, by the principle of mathematical induction, P $_n$ is true for all integers $\alpha \geq 1$ . Q.E.D Question I know I have made a mistake in this proof and is definitely not correct but Im not sure what I've done wrong, I think the error is in this part, \begin{equation}
\frac{12(\alpha-1)^2}{\pi^2} + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 = \frac{12(2-\alpha)^2}{\pi^2}
\end{equation} \begin{equation}
2\sum_{k=1}^\alpha \left(\prod_{p \vert k}\left(1-\frac{1}{p}\right)\right) + 2\left(\prod_{p \vert \alpha + 1}\left(1-\frac{1}{p}\right)\right) + 2 \geq \frac{12(2-\alpha)^2}{\pi^2}
\end{equation} Since $\alpha\geq 1$ , we know that $\frac{12(2-\alpha)^2}{\pi^2} \geq 0$ , so, what mistake did I make and how should I fix it?","['number-theory', 'induction', 'solution-verification', 'totient-function']"
4570653,Help with this 2 variables limit,"I'm asked to calculate this limit: $$
\lim_{(x,y)\rightarrow(0,0)}\frac{\ln(1+x)+\ln(1+y)}{x+y}
$$ After calculating iterated limits and using some directions ( $y=\lambda x$ and $ y=\lambda x^2$ ) all I can deduce is that the limit is actually $1$ , but using polar coordinates I'm not able to prove it. Is there any other way to solve it?","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
4570658,Does there exist an infinite amount of prime numbers with its binary representarion being prime in base 10?,"I'll try and explain what I asked. Take the number $5$ , which is prime. Its binary representation, $101$ , is also prime. I computed a program in python that picks all numbers from a list that verifies this ( $k$ is prime and its binary representation is prime if we consider it as a decimal representation), and it keeps showing numbers up until $5000$ (which is as far as I checked). I guess we first had to prove that there exists an infinite amount of primes numbers whose digits are onle $0's$ and $1's$ , and then we would have to prove that the number of prime numbers (let's call the set of primes $A$ ) is the same as the number of prime numbers with digits $0's$ and $1's$ (let's call this set $B$ ). This second part can be easy ifthe first result is true, since $B\subset A \subset \Bbb N$ . So the second result is true. Does anyone have any idea on how to prove the first result? Or anything similar to it? Thanks in advance! Edit: Thanks to Peter and CyclotomicField for some insight! I run a program in python that showed how many primes of this type can be found from $2$ to $1001$ , $1002$ to $2001$ , $2002$ to $3001$ , ... until $1000001$ . I drew in Mathematica the list of these numbers of primes, and got this: It might not be much, but I thought it was interesting...","['number-theory', 'binary', 'prime-numbers']"
4570682,Why are the reflection groups $H_r(a; b)$ and $H_r(da; db)$ naturally isomorphic?,"I have a question about the following remark 5.6 of this article , I'm reading right now. I try to give a little pretext, with the notations used in the paper: Definition 5.2: Let $H(a; b) \in GL(n, \mathbb{C})$ be a hypergeometric group with parameters $a_1, ..., a_n, ; b_1 ..... b_n$ , and generators $h_0, h_1, h_{\infty}$ as in Definition 3.1. The subgroup $H_r(a;b)$ of $H(a;b)$ generated by the reflections $h_{\infty}^k h_1 h_{\infty}^{-k}$ for $k\in \mathbb{Z}$ is called  reflection subgroup of $H(a; b)$ . Definition 5.5: A scalar shift of the hypergeometric group $H (a; b)$ is a hypergeometric group $H (da; db)= H (da_1, ..., da_n ; db_1, ..., db_n)$ for some $d\in \mathbb{C}^*$ . Remark 5.6: If $d$ has the form $d=exp(2\pi i \delta)$ for some $\delta \in \mathbb{C}$ then a scalar shift
from $H(a; b)$ to $H(da; db)$ is the effect on the monodromy group obtained by
multiplying all solutions of the hypergeometric equation by $z^{-\delta}$ . Observe that the associated reflection groups $H_r(a; b)$ and $H_r(da; db)$ are naturally isomorphic. My question and what i have done so far: As said before, my question is about remark 5.6, or to be more precise, about the ""second"" part, regarding the natural isomorphism. I still want to at least explain my progress on the first part of 5.6 so far: First note, that the monodromy group of a hypergeometric equation $D(\alpha;\beta)u=0$ is a hypergeometric group $H(a;b)$ with parameters $a_j=exp(2 \pi i \alpha_j)$ and $b_j = exp(2 \pi i \beta_j)$ , according to proposition 3.2. Here the complex numbers $\alpha_j$ , $\beta_j$ are obtained via the local exponents of the solutions of said equation around $z=0,\infty$ . Also note, that $exp(2 \pi i \alpha_j)$ , $j=1,...,n$ are the eigenvalues of $h_{\infty}$ and $exp(-2 \pi i \beta_j)$ are the eigenvalues of $h_0$ respectively. I do understand, that if $d$ is of the form as in the remark, then the scalar shift is the effect on the monodromy group, by multiplying the solutions with $z^{-\delta}$ . This can be calculated fairily easy, by using that as in (2.9) the solutions are of the form $z^{1-\beta_i} {}_n F_{n-1}(...)$ . More precisely, by doing so we get local exponents $1-\beta_i-\delta$ in $z=0$ and $\alpha_i + \delta$ in $z=\infty$ respectively. Hence, we obtain a hypergeometric group $H(\tilde{a};\tilde{b})$ with parameters $$\tilde{a_i} = exp(2 \pi i(\alpha_i + \delta)) = exp(2 \pi i \delta)exp(2 \pi i \alpha_i) = d a_i,$$ $$\tilde{b_i} = exp(2 \pi i(\beta_i + \delta)) = exp(2 \pi i \delta)exp(2 \pi i \beta_i) = d b_i.$$ So as stated before, my remaining question now is: Why are the associated reflection subgroups naturally isomorphic in this case? * Edit: Bounty expires in one day!","['group-theory', 'abstract-algebra', 'group-isomorphism']"
4570712,Is there an identity for $\sum_{k=0}^{n-1}\tan^4\left({k\pi\over n}\right)$?,"Is there a simple relation for $\sum_{k=0}^{n-1}\tan^4\left({k\pi\over n}\right)$ like there is for $\sum_{k=0}^{n-1}\tan^2\left({k\pi\over n}\right)$ ? Looking at Jolley, Summation of Series, formula 445: $\sum_{k=0}^{n-1}\tan^2\left(\theta+{k\pi\over n}\right)=n^2\cot^2\left({n\pi\over2}+n\theta\right)+n(n-1)$ or more simply: $\sum_{k=0}^{n-1}\tan^2\left({k\pi\over n}\right)=n(n-1)$ Does $\sum_{k=0}^{n-1}\tan^4\left({k\pi\over n}\right)$ possibly equals some integer values? The numerical table seems to suggest so even for higher even powers of $\tan$ : $\left(
\begin{array}{ccccc}
n & \sum_{k=0}^{n-1}\tan^2\left({k\pi\over n}\right)&\sum_{k=0}^{n-1}\tan^4\left({k\pi\over n}\right)&\sum_{k=0}^{n-1}\tan^6\left({k\pi\over n}\right)&\sum_{k=0}^{n-1}\tan^8\left({k\pi\over n}\right)\\
 3. & 6. & 18. & 54. & 162. \\
 5. & 20. & 180. & 1700. & 16100. \\
 7. & 42. & 742. & 14154. & 271558. \\
 9. & 72. & 2088. & 66600. & 2.14049\times 10^6 \\
 11. & 110. & 4730. & 226622. & 1.09528\times 10^7 \\
 13. & 156. & 9308. & 624780. & 4.2335\times 10^7 \\
 15. & 210. & 16590. & 1.48533\times 10^6 & 1.34314\times 10^8 \\
\end{array}
\right)$ I had tried to use residue methods and I particularly like falager's method in this answer using Vieta's formula but kept getting stuck. If one could make progress with the quartic, one could possibly find for higher even powers of tan too.","['summation-method', 'summation', 'trigonometric-series', 'contour-integration', 'trigonometry']"
4570718,Sylow-$2$ Subgroups of a simple group of order 168,"I'm stuck somewhere in the following claim, I would appreciate if you could help: Claim: Let $G$ be a simple group of order $168(=2^3\cdot 3\cdot 7).$ Then all Sylow $2$ -subgroups of $G$ are dihedral. We can easily see that the number of Sylow- $7$ subgroups of $G$ must be $8,$ and each of them are cyclic. With a little effort we can also see that the number of Sylow- $3$ subgroups of such a group must be $28,$ and again each of them are cyclic. Also, counting the elements one can easily see that the number of Sylow $2$ -subgroups of $G$ is $21.$ However, I could not see how to get the desired result. Thanks in advance.","['dihedral-groups', 'simple-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4570725,Approximating $\sqrt{2}$ by Pythagorean triples? [duplicate],"This question already has answers here : Odd convergents of $\sqrt 2$ and Pythagorean triangles with consecutive legs (2 answers) Closed 1 year ago . I am trying to find sequence of Pythagorean triples $(x_{n}, y_{n}, z_{n})\in\mathbb{Z}^{3}$ such that $x_{n}/y_{n}\rightarrow 1$ . This way, both $z_{n}/x_{n}$ and $z_{n}/y_{n}$ would converge to $\sqrt{2}$ as $n\rightarrow\infty$ . I am aware there are various other ways of approximating $\sqrt{2}$ by rational numbers such as here , here , and here , but none of these give me a way to generate Pythagorean triples in the way I desire. I am wondering if there is an elegant way to do this.","['approximation', 'elementary-number-theory', 'pythagorean-triples', 'radicals', 'algebra-precalculus']"
4570796,"The Cantor set + the Cantor set = $[0,2]$","Let $C$ be the Cantor set .  Prove $ C + C = [0,2]$ .  ( Notation : If $S, R$ are sets, then $S + R$ is the set of all $s + r$ with $s \in S, r \in R$ .) Proofs of this are readily available.  This question asks to help me complete my proof.  I ask that you do not give away the entire answer, but simply help me take the next step. Definitions: Let $I$ be a closed interval $[a,b]$ .  Then $I'$ , the cantorization of $I$ , is $[a, a + \delta] \cup [b - \delta, b]$ with $\delta = \frac{b-a}{3}$ .  The interval $[a, a + \delta]$ is called the lower third , the interval $[b - \delta, b]$ is called the upper third , and $I \setminus I'$ is called the middle third . Let $S$ be a finite union of disjoint, separated, closed intervals, each of uniform (finite) width.  Then $S'$ , the cantorization of $S$ , is the union of $I'$ for all $I$ where $I$ is a closed interval, a subset of $S$ , and separated from $S \setminus I$ . Lemma: $S + S = S' + S'.$ Proof: Let $I_1$ and $I_2$ be two suitable intervals in $S$ such $I_1 = [a, a + \delta], I_2 = [b, b + \delta], a \leq b$ .  Then $$(I_1 \cup I_2) + (I_1 \cup I_2) = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\$$ and $$I_1' = [a, a + \frac{1}{3}\delta] \cup [a + \frac{2}{3}\delta, a + \delta] \\
(I_1' + I_1') = [2a, 2a + \frac{2}{3}\delta] \cup [2a + \frac{2}{3}\delta, 2a + \frac{4}{3}\delta] \cup [2a + \frac{4}{3}\delta, 2a + 2\delta] = [2a, 2a + 2\delta] \\
I_2' = [b, b + \frac{1}{3}\delta] \cup [b + \frac{2}{3}\delta, b + \delta] \\
(I_2' + I_2') = [2b, 2b + \frac{2}{3}\delta] \cup [2b + \frac{2}{3}\delta, 2b + \frac{4}{3}\delta] \cup [2b + \frac{4}{3}\delta, 2b + 2\delta] = [2b, 2b + 2\delta] \\
(I_1' + I_2') = [a + b, a + b + \frac{2}{3}\delta] \cup [a + b + \frac{2}{3}\delta, a + b + \frac{4}{3}\delta] \cup [a + b + \frac{4}{3}\delta, a + b + 2\delta] \\
(I_1' + I_2') = [a + b, a + b + 2\delta] \\
(I_1' \cup I_2') + (I_1' \cup I_2') = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\
$$ so $$
(I_1 \cup I_2) + (I_1 \cup I_2)
 = (I_1' \cup I_2') + (I_1' \cup I_2')$$ Since this holds for any $I_1, I_2$ in $S$ , $S + S = S' + S'.$ Main Proof: By induction using the lemma, for any $n \in \mathbb{n}, C_n + C_n = [0,2].$ We now need to show this for $C$ itself, and here is where I need some help. It is not enough that for any $n$ , there exists $x_n, y_n \in C_n$ such that $x_n + y_n = t$ .  We need to show that the $x$ and $y$ are in $C$ , that is, that the same $x_n, y_n$ are in all $C_n$ . One way to do this is to apply that sequences within compact sets converge to limits within the compact set.  Thus, $(x_n) \to x \in C_n, (y_n) \to y \in C_n$ .  But does that show that if for all $n$ , $x_n + y_n = t$ , then $x + y = t$ ? Another approach might be to take for each $n$ the intersection of $C_n$ and the set of ( $x, y \in [0,2]$ that sum to $t$ ).  Since both of these are compact, their intersection is compact.  But defining the set ( $x, y \in [0,2]$ that sum to $t$ ) is tricky.","['cantor-set', 'real-analysis']"
4570805,Can $a + bX + cX^2 \overset{D}{=} X + X^2$ for a continuous random variable $X$ and $c\neq 1$?,"Let $a,b,c\in\mathbb{R}$ and $X$ a continuous random variable. Can it happen that $$a + bX + cX^2 \overset{D}{=} X + X^2,$$ while $c\neq 1$ ? My intuition is that no (if the equality holds, then $c=1$ ). I figured that e.g., $a$ is uniquely given from other constants by $a = (1-b)\mathbb{E}[X] + (1-c)\mathbb{E}[X^2]$ . Maybe looking at higher moments can help, but I didn't explore it much.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4570807,Can different topological spaces have the same square?,"For topological spaces $X$ and $Y$ , is it possible that $X \times X$ and $Y \times Y$ are homeomorphic, but $X$ and $Y$ are not homeomorphic? (I poked around with finite spaces, and manifolds, and the Cantor set, without seeing any examples.) This was inspired by Existence of topological space which has no “square-root” but whose “cube” has a “square-root” . Cartesian product makes the proper class of topological spaces (up to homeomorphism) into a large abelian monoid, so here's a bonus question: what is known about the structure of this monoid? For example, the dogbone space shows that it is not cancellative. Does it have torsion in the sense that sometimes $X^n \not\cong X$ but $X^{n + 1} \cong X$ ?",['general-topology']
4570822,Euler cycle in a $m\times n$ rectangular grid.,"Let $G=(V,E)$ a graph which consists in an $m\times n$ rectangular grid as the image shows: I need to find the values of $m,n$ for which this graph has an Euler cycle (or euler circuit, don't repeat edges, and go through them all). $m$ and $n$ refers to the vertices positions as in a matrix ( $m$ for rows, $n$ for columns). I could find Euler cycles for $1\times 1$ and $1\times 2$ (and of course $2\times 1$ ) but i'm not able to find euler cycles for bigger values. I'm trying to think about the parity of $m$ and $n$ but can't conclude anything. Edit: $m$ and $n$ are edges on the grid, we sould have $m+1,n+1$ vertices as @Daniel Schepler pointed out.","['eulerian-path', 'discrete-mathematics']"
4570896,how to compute the de Rham cohomology of the punctured plane just by Calculus?,"I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\mathbb{R}^2\setminus\{0\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?","['smooth-manifolds', 'calculus', 'homology-cohomology', 'algebraic-topology', 'differential-geometry']"
4570918,Binary matrices such with distinct columns such that after the removal of any one row the columns are no longer distinct,"Let $A$ be an $m \times 2^n$ binary (i.e. with entries from $\{0,1\}$ ) matrix where $m \gg n$ , such that all columns of $A$ are distinct $m$ -bit bitwords. Suppose further that by removing any row of $A$ we obtain a $(m-1)\times 2^n$ matrix whose columns are no longer unique in the above sense. For any such matrix one clearly has the upper bound $m < 2^{n}$ . What is the best possible upper bound on $m$ in terms of $n$ ? Example for $n = 3, m = 5$ : $$\begin{bmatrix}1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 \\
0 & 1 & 1 & 1 & 0 & 0 & 1 & 1 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\ \end{bmatrix}$$ Edit: This is a very closely related question.","['matrices', 'binary', 'linear-algebra', 'combinatorics']"
4570919,Is $G*G \cong G$ where $G$ is the fundamental group of the Hawaiian earring?,"Let $X$ be the Hawaiian earring and let $x_0$ be the point $(0,0)$ . Consider the wedge sum $Y:=X\vee X=X\coprod X/x_0\sim x_0$ . Then using $\Bbb N=\{1,3,5,\dots\}\cup \{2,4,6,\dots\}$ , $X$ is homeomorphic to $Y$ . My question is: Do we have $G*G\cong G$ (isomorphic as groups), where $G=\pi_1(X,x_0)$ ? If this is not true, but this will be a counterexample of the statement that $\pi_1(X_1\vee X_2)\cong \pi_1(X_1)*\pi_1(X_2)$ for path-connected topological spaces $X_1,X_2$ . (This is true if the wedge point has a neighborhood that deformation retracts onto itself, by the van Kampen theorem). The group $G$ is quite complicated , and I have no idea whether $G*G$ is isomorphic to $G$ or not. Edit. It is known that the $H_1(X)$ , which is the abelianization of $G$ , is $(\prod_{i=1}^\infty \Bbb Z) \oplus (\prod_{i=1}^\infty \Bbb Z/\bigoplus_{i=1}^\infty \Bbb Z)$ , so one way to prove that $G*G\not\cong G$ is showing $$
\left( \prod_{i=1}^\infty \Bbb Z \right) \oplus \left( \prod_{i=1}^\infty \Bbb Z/\bigoplus_{i=1}^\infty \Bbb Z \right) \not\cong \left( \prod_{i=1}^\infty \Bbb Z \right) \oplus \left( \prod_{i=1}^\infty \Bbb Z/\bigoplus_{i=1}^\infty \Bbb Z \right) \oplus \left( \prod_{i=1}^\infty \Bbb Z \right) \oplus \left( \prod_{i=1}^\infty \Bbb Z/\bigoplus_{i=1}^\infty \Bbb Z \right).$$","['group-theory', 'fundamental-groups', 'free-product', 'algebraic-topology']"
4570936,When are compact sets NOT Borel?,"The theory of Radon measures relies a lot on the hypothesis that compact subsets of a topological space are Borel (i.e., in the $\sigma$ -algebra generated by the open sets). This is an okay assumption in Hausdorff spaces (where the bulk of the introductory theory takes place) because all compact subsets are closed and hence Borel. However, this answer remarks that there ARE topological spaces containing non-Borel compact sets. To flesh out the linked answer, let $X$ be any set containing more than one point. Then let $\tau \subseteq \mathscr{P}(X)$ be the trivial topology (e.g., $\tau = \{\varnothing, X\}$ ). Then the Borel $\sigma$ -algebra is then $\mathcal{B}_X = \tau = \{\varnothing,X\}$ . Any singleton $\{x\} \subseteq X$ is then clearly not Borel, yet it is compact since all singletons are so (in fact, all subsets in the trivial topology are trivially compact). My question is this: What conditions must be placed on the topology to ensure compact subsets are or aren't Borel? We have the obvious extremes above. However, I am aware that ""Hausdorff"" is not a necessary condition. For an example from algebraic geometry, the Zariski topology on the prime spectrum of a ring is $T_0$ yet has the peculiar property that the basic open sets are compact. To be more specific, every subset of $\text{Spec}(\mathbb{Z})$ is Borel (every nonzero point is closed, and the whole space is countable, so zero is Borel too). This gives us a few avenues of attack: (1). Are all compact subsets of a $T_0$ topological space Borel? Or, (2). Is there an example of a $T_0$ space having a non-Borel compact subset? (Note the trivial topology is $T_0$ iff it's also discrete.) (3). Same questions, but for $T_1$ spaces instead. Any insight is appreciated!","['borel-sets', 'general-topology', 'borel-measures', 'compactness']"
4570967,"What is the sum $\sum_{(m, n)\in\mathbb{Z}\\(m,n)\neq(0,0)}\frac{1}{(m+in)^\alpha}$ equal to?","I had a look at this webpage , which led me to wonder what $$\sum_{(m, n)\in\mathbb{Z}\\(m,n)\neq(0,0)}\frac{1}{(m+in)^\alpha}$$ would equal to. My thoughts are as follows: \begin{align}
\sum_{(m, n)\in\mathbb{Z}\\(m,n)\neq(0,0)}\frac{1}{(m+in)^\alpha}&=\sum_{(m, n)\in\mathbb{Z}\\(m,n)\neq(0,0)}\frac{(m-in)^\alpha}{\left(m^2+n^2\right)^\alpha} \\
&=\sum_{m\in\mathbb{Z}/\{0\}}\frac{1}{m^\alpha}+\sum_{n\in\mathbb{Z}/\{0\}}\frac{1}{(in)^\alpha}+\sum^\infty_{m=-\infty}\sum^\infty_{n=1}\frac{(m-in)^\alpha+(m+in)^\alpha+(-m+in)^\alpha+(-m-in)^\alpha}{\left(m^2+n^2\right)^\alpha} \\
&=\sum_{m=1}^\infty\frac{1}{m^\alpha}+\sum_{m=1}^\infty\frac{1}{(-m)^\alpha}+\sum_{n=1}^\infty\frac{1}{(in)^\alpha}+\sum_{n=1}^\infty\frac{1}{(-in)^\alpha}+\sum^\infty_{m=1}\sum^\infty_{n=1}\frac{\left(1+(-1)^\alpha\right)\left[(m-in)^\alpha+(m+in)^\alpha\right]}{\left(m^2+n^2\right)^\alpha} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha\right)\zeta(\alpha)+\sum^\infty_{m=1}\sum^\infty_{n=1}\frac{\left(1+(-1)^\alpha\right)\cdot2\sqrt{m^2+n^2}^\alpha\cos\left[\alpha\arctan\left(\frac nm\right)\right]}{\left(m^2+n^2\right)^\alpha} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha\right)\zeta(\alpha)+2\left(1+(-1)^\alpha\right)\sum^\infty_{m=1}\sum^\infty_{n=1}\frac{\cos\left[\alpha\arctan\left(\frac nm\right)\right]}{\left(m^2+n^2\right)^\frac\alpha2} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha\right)\zeta(\alpha)+2\left(1+(-1)^\alpha\right)\sum^\infty_{k=1}\sum_{\gcd(m,n)=1}\frac{\cos\left[\alpha\arctan\left(\frac{kn}{km}\right)\right]}{\left(k^2m^2+k^2n^2\right)^\frac\alpha2} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha\right)\zeta(\alpha)+2\left(1+(-1)^\alpha\right)\sum^\infty_{k=1}\sum_{\gcd(m,n)=1}\frac{\cos\left[\alpha\arctan\left(\frac{n}{m}\right)\right]}{\left(k^2m^2+k^2n^2\right)^\frac\alpha2} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha\right)\zeta(\alpha)+2\left(1+(-1)^\alpha\right)\sum^\infty_{k=1}\frac{1}{k^\alpha}\sum_{\gcd(m,n)=1}\frac{\cos\left[\alpha\arctan\left(\frac{n}{m}\right)\right]}{\left(m^2+n^2\right)^\frac\alpha2} \\
&=\left(1+i^\alpha+(-1)^\alpha+(-i)^\alpha+2\left(1+(-1)^\alpha\right)\sum_{\gcd(m,n)=1}\frac{\cos\left[\alpha\arctan\left(\frac{n}{m}\right)\right]}{\left(m^2+n^2\right)^\frac\alpha2}\right)\zeta(\alpha) \\
\end{align} Though I couldn't (and presumably lack the analytical ability to) simply the expression further. This seems tangentially related to the sum $$\sum_{(m, n)\in\mathbb{Z}\\(m,n)\neq(0,0)}\frac{1}{(m^2+n^2)^\frac{\alpha}{2}}$$ which is equal to $4\beta\left(\frac{\alpha}{2}\right)\zeta\left(\frac{\alpha}{2}\right)$ , though I don't know how this sum can be applied.","['riemann-zeta', 'integer-lattices', 'sequences-and-series']"
4571014,Find all relations on A that are both reflexive and antisymmetric,"This is the only question on my homework that still gives me a headache, I have to find out how many relations there are on $A = \{1,2,3,...,7\}$ that are both reflexive and antisymmetric.
So far all I've gotten is the following: For antisymmetry alone, there are $7$ distinct pairs $(a,a)$ and $21$ distinct pairs $(a,b)$ with $a < b$ .
The pairs $(a,a)$ are either contained within the relation or not, while the pairs $(a,b)$ can be contained, not be contained, or be inversed as $(b,a)$ . This results in $2^7 \times 3^{21}$ antisymmetric possible relations, ignoring the fact that the relation should also be reflexive. However, since both criteria need to be met, I need to find a way to incorporate the reflexive argument as well. This implies that if $(a,b)$ is contained, there is also a corresponding $(a,a)$ , but I don't see any way to continue from here. In retrospect, I feel as though my approach wasn't great to begin with, but I am now at a point where I have absolutely no idea what else to do, so any assistance would be greatly appreciated.","['relations', 'combinatorics', 'discrete-mathematics']"
4571030,Writing a formal proof about infimum and supremum,"Let the function $\Phi : [0,\infty) \rightarrow [0,\infty)$ be a convex bijection, $$
\|f\|_{L^{\Phi}}:=\inf\left\{\lambda>0:\int_{\mathbb{R}^n}\Phi\Big(\frac{|f(x)|}{\lambda}\Big)dx\leq 1\right\},
$$ and $$
\Vert f\Vert_{WL^{\Phi}}:=\inf\left\{\lambda>0\ :\ \sup_{t>0}\Phi(t)|\{x\in\mathbb{R}^n: \frac{|f(x)|}{\lambda}>t\}|\ \leq 1\right\}.
$$ I want to show that $$
\Vert f\Vert_{WL^{\Phi}}=\sup_{t>0}\|t\chi_{\{|f|>t\}}\|_{L^{\Phi}},
$$ where $\chi$ denotes the characteristic function. My attempt: First note that $$
\Vert f\Vert_{WL^{\Phi}}=\inf\left\{\lambda>0\ :\ \sup_{t>0}\Phi(\frac{t}{\lambda})|\{x\in\mathbb{R}^n: |f(x)|>t\}|\ \leq 1\right\}=:\inf B.
$$ If we write $\sup_{t>0}\|t\chi_{\{|f|>t\}}\|_{L^{\Phi}}$ explicitly, we get $$
\sup_{t>0}\|t\chi_{\{|f|>t\}}\|_{L^{\Phi}}=\sup_{t>0}\inf\left\{\lambda>0:\Phi\Big(\frac{t}{\lambda}\Big)\int_{\{x\in\mathbb{R}^n: |f(x)|>t\}}dx\leq 1\right\}=:\sup_{t>0}\inf A_t.
$$ If $\lambda\in B$ then $\sup_{t>0}\Phi(\frac{t}{\lambda})|\{x\in\mathbb{R}^n: |f(x)|>t\}|\ \leq 1$ . Therefore $\Phi(\frac{t}{\lambda})|\{x\in\mathbb{R}^n: |f(x)|>t\}|\ \leq 1,\, \forall t>0$ which means $\lambda \in A_t$ for all $t>0$ . Hence $B\subset A_t,\, \forall t>0$ and this imply $\inf B\geq \inf A_t,\, \forall t>0$ and consequently $\inf B\geq \sup_{t>0}\inf A_t$ .
Now let $t$ be fixed and $\lambda \in A_t$ . Then $\Phi(\frac{t}{\lambda})|\{x\in\mathbb{R}^n: |f(x)|>t\}|\ \leq 1$ . I think that i can not write $\sup_{t>0}\Phi(\frac{t}{\lambda})|\{x\in\mathbb{R}^n: |f(x)|>t\}|\ \leq 1$ since $t$ is not arbitrary? Am i right? How can i continue? I am very confused.","['calculus', 'supremum-and-infimum', 'functional-analysis', 'real-analysis']"
4571078,How to find the set X?,"I am given $2$ sets $A$ and $B$ : $A = \{1,2,5,6,7\}, B=\{0,4,6,7,9\}$ and two more sets $C = \{0,1,2,6,7,9\}$ and $M = \{0,1,2,3,4,5,6,7,8,9\}$ . I have the following set equation to be solved: $(A \cap X) \cup (B \cap X^c) = C$ My own thoughts have been to use the law of inverse : $X \cup X^c  = \emptyset$ . But I can't use the distributive law for sets stated below: $A \cup (A \cap B) = (A \cup B) \cap (A \cup C)$ When $X \subseteq M$ . So how can I find the X that satisfies $(A \cap X) \cup (B \cap X^c) = C$ ?",['discrete-mathematics']
