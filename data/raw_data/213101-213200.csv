question_id,title,body,tags
4305576,For which $f: \mathbb{Q}\rightarrow\mathbb{Q}^+$ does the sum $\sum_{q\in\mathbb{Q}} f(q)$ converge?,"BACKGROUND: This is not a homework question -- I am not even in school. I am purely interested in the question itself. I actually asked this question in my second semester of real analysis during my math major, but that was years ago. The professor found the question interesting but was unable to provide a satisfying answer. I know the sum $\sum_{q\in\mathbb{Q}\cap[0,\varepsilon)}q$ diverges to $\infty$ (since infinitely many terms are larger than $\frac{\varepsilon}{2}$ ). But $\mathbb{Q}$ is countable, and so there must be some ""sums over the rationals"" which converge. Of course we need to avoid the ambiguity of unordered infinite series with positive and negative terms (since rearrangement of terms does not preserve the sum). So I am only interested in sums of positive terms, i.e. $\sum_{q\in\mathbb{Q}}f(q)$ where $f:\mathbb{Q}\rightarrow\mathbb{Q}^+$ . Intuitively, the density of $\mathbb{Q}$ makes convergence a lot harder to achieve for an arbitrary sum. We won't have success with something like $\sum_{q\in\mathbb{Q}\backslash\{0\}}\frac{1}{q^2}$ , since there will still be too many terms that are bigger than any $\varepsilon>0$ , in this case for all $q\in \mathbb{Q}^{\neq0}\cap\big(-\frac{1}{\sqrt{\varepsilon}},\frac{1}{\sqrt{\varepsilon}}\big)$ . This seems to suggest that functions that have a simple formula for $f(q)$ , written in terms of $q$ , will rarely (almost never?) have a convergent sum. So to make my question a bit more specific and therefore easier to answer, I want to focus my question only on the following example. MY ACTUAL QUESTION: Any rational number can be uniquely described as a fraction in lowest terms. For which positive real numbers $a$ and $b$ does the following sum converge? $$\sum_{\frac{m}{n} \in \mathbb{Q}} \frac{1}{|m|^a+|n|^b}$$ Intuitively, if we use powers of $1$ , then this fraction might--like the harmonic series--not shrink to zero quickly enough for the sum to converge. To further build intuition, I will informally define the "" ${l}^a$ complexity"" of rational numbers to be $$\bigg|\frac{m}{n}\bigg|_a=\big(|m|^a+|n|^a\big)^{\frac{1}{a}}$$ To illustrate, $\big|\frac{-3}{4}\big|_2=(3^2+4^2)^\frac{1}{2}=\sqrt{25}=5$ , but $\big|\frac{-3}{4}\big|_1=(3^1+4^1)^\frac{1}{1}=7$ . To avoid ambiguity, we will choose the canonical representation of $0$ to be the fraction $\frac{0}{1}$ , so that $|0|_a=(0^a+1^a)^\frac{1}{a}=1$ . Since $0=\frac{0}{1}$ is the rational number written with the smallest possible numerator and denominator, we see that $|q|_a \geq 1$ for any $q\in\mathbb{Q}$ . In general, ""more complicated"" rational numbers will have larger $l^a$ complexity than ""simpler"" rational numbers: $\big|\frac{456}{1207}\big|_a>\big|\frac{1}{2}\big|_a$ . This leads to an even smaller and more restrictive question. A SIMPLER QUESTION THAT MIGHT HELP ME MAKE PROGRESS: For which $a>1$ does the following sum converge? $$\sum_{q\in\mathbb{Q}} \frac{1}{|q|_a^a}$$","['rational-numbers', 'functional-analysis', 'sequences-and-series']"
4305593,"prove that $\int_0^\infty \frac{\sin^2 x-x\sin x}{x^3} \, dx= \frac{1}{2} - \ln 2$","Prove that $$ \int_{0}^{\infty} \frac{\sin^2 x-x\sin x}{x^3} \, dx = \frac{1}{2} - \ln 2 .$$ Integration by parts gives \begin{align*}
&\lim_{R\to \infty} \int_{0}^{R} \frac{\sin^2 x-x\sin x}{x^3} \, dx \\
&= \lim_{R\to \infty} \biggl( \int_{0}^{R} \frac{\sin^2x}{x^3} \, dx - \int_{0}^{R} \frac{\sin x}{x^2} \, dx \biggr)\\
&= \lim_{R\to\infty} \biggl( \frac{\sin^2 x}{-2x^2}\Biggr\rvert_{0}^{R} - \int_{0}^{R} \frac{\sin (2x)}{-2x^2} \, dx - \biggl(-\frac{\sin x}{x} \Biggr\rvert_{0}^{R} + \int_{0}^{R} \frac{\cos x}{x} \,dx \biggr) \biggr) \\
&= \lim_{R\to \infty} \biggl(\frac{1}{2} + \int_{0}^{2R} \frac{\sin u}{u^2/2} \, \Bigl(\frac{1}2 \, du\Bigr) - \biggl( 1 + \int_{0}^{R} \frac{\cos x}{x} \, dx \biggr)\biggr) \\
&\hspace{22em}\text{(using the substitution $u\mapsto 2x$)}\\
&= -\frac{1}{2} + \lim_{R\to \infty} \biggl(\int_{0}^{2R} \frac{\sin u}{u^2} \, du - \int_{0}^{R} \frac{\cos x}{x} \,dx \biggr)\\
&= \frac{1}{2} + \lim_{R\to\infty} \biggl(\int_{R}^{2R} \frac{\cos x}{x} \, dx \biggr)
\end{align*} Thus it suffices to show that $\lim_{R\to\infty} \int_{R}^{2R} \frac{\cos x}{x} \, dx = \ln 2$ . The Taylor series expansion of $\cos x$ is given by $\cos x = \sum_{i=0}^{\infty} \frac{(-1)^i x^{2i}}{(2i)!}$ . (If the step below (the one involving the interchanging of an infinite sum and integral) is valid, why exactly is it valid? For instance, does it use uniform convergence?) The limit equals $$
\lim_{R\to\infty} \int_{R}^{2R} \biggl( \frac{1}{x} + \sum_{i=1}^{\infty} \frac{(-1)^i x^{2i-1}}{(2i)!} \biggr) \, dx
= \ln 2 + \lim_{R\to\infty} \sum_{i=1}^{\infty} \biggl[\frac{(-1)^ix^{2i}}{(2i)(2i)!}\biggr]_{R}^{2R} . $$ But I don't know how to show $\lim_{R\to\infty} \lim_{R\to\infty} \sum_{i=1}^{\infty} \Bigl[\frac{(-1)^ix^{2i}}{(2i)(2i)!}\Bigr]_{R}^{2R} = -2 \ln 2$ .","['integration', 'calculus', 'improper-integrals', 'real-analysis']"
4305595,"Determining all $(a,b)$ on the unit circle such that $2x+3y+1\le a(x+2)+b(y+3)$ for all $(x,y)$ in the unit disk","In the middle of another problem, I came up with the following inequality which needed to be solve for $(a, b)$ : $$2x+3y+1\le a(x+2)+b(y+3)$$ for all $(x, y)\in\mathbb{R}^2$ with $x^2+y^2\le1.$ Here the solution for $(a, b)$ must be a subset of the unit circle, and I believe that it is a singleton. Since I have no clue to solve it in this form, I tried to substitute some points of the closed unit disk and make a system of inequalities. But it seems like there should be a general way of solving these type of problems.","['calculus', 'geometric-inequalities', 'algebra-precalculus', 'inequality']"
4305649,$f(x^2)=f(x)+f(-x)$,"Is there only one function (or class of functions) for which $f(x^2)=f(x)+f(-x)$ ? I know $\ln(1-x)$ fits the identity...
I tried finding $f(xy)$ , hoping that it equals $f(x)+f(y)$ because then it would be a logarithm, to no avail. Thanks Edit, the domain of the function should be $(-1,1)$","['functions', 'exponential-function', 'logarithms']"
4305689,$x^4+x^2=\frac{11}{5}$ then what is the value of $\sqrt[3]{\frac{x+1}{x-1}}+\sqrt[3]{\frac{x-1}{x+1}}$?,$x^4+x^2=\frac{11}{5}$ then what is the value of $\sqrt[3]{\frac{x+1}{x-1}}+\sqrt[3]{\frac{x-1}{x+1}}$ ? My approaches which didn't yield anything I tried manipulating $\sqrt[3]{\frac{x+1}{x-1}}+\sqrt[3]{\frac{x-1}{x+1}}$ Observations $\frac{x+1}{x-1}=\frac{(x+1)(x^3-x^2+2x-2)}{(x-1)(x^3-x^2+2x-2)}=\frac{x^4+x^2-2}{x^4-2x^3+3x^2-4x+2}=\frac{\frac{11}{5}-2}{\frac{11}{5}-2x^3+2x^2-4x+2}$ which is not giving any assistance to find the required value. let a= $\sqrt[3]{\frac{x+1}{x-1}}+\sqrt[3]{\frac{x-1}{x+1}}$ then we observe that $a^3=2\frac{x^2+1}{x^2-1}+3a$ which again is not useful how should we proceed then to get the required ?,"['radicals', 'algebra-precalculus', 'polynomials']"
4305701,Probability of draw with random play on $N\times N$ Tic Tac Toe,"I was coding a tic tac toe game where $2$ players play tic tac toe randomly on a given $N$ board size $(N\times N)$ . $X$ starts first. If one side gets $N$ consecutive (horizontal/vertical/diagonal) of their symbols, they win. If no one won and there is no more space on the board, it is a draw. I implemented it and give it a go a couple of times. I realized when $N$ gets bigger the draw rate increase a lot even with relatively small $N$ 's. For example I couldn't get a single non-draw game with $10\times 10$ board in $20$ games. My question is, what is the probability of a draw on $N\times N$ tic tac toe board with random play in terms of $N$ ? or if it is too complicated to express, what is the general relation between $N$ and the draw rate? P.S: I don't know the answer.","['tic-tac-toe', 'probability']"
4305711,"Prove that there exists 3 numbers $a,b,c$ so that $P(a)=b, P(b)=c, P(c)=a$","The problem Given that $P(x)=x^3-3x$ . Prove that there exists $a \neq b \neq c$ such that $P(a)=b, P(b)=c, P(c)=a$ My ideas So the conditions here makes it quite clear that I intend to use the Lagrange Interpolation: third degree polynomial versus creating three equations. It is even more generous to think reversely that if we have 3 functions, then we can still add one other function of choice to re-create $P(x)=x^3-3x$ . (Even though I think that function is $P(0)=0$ But still, I can't get the original problem done. So any help is appreciated!","['lagrange-interpolation', 'algebra-precalculus', 'polynomials']"
4305724,Find the limit of $b_n = \sum_{i=1}^n \frac{1}{a_ia_{i+1}}$,"So I got this question in a textbook, it is basically: Let $a_n$ be a sequence such that \begin{cases}
a_1=3, a_2=7 \\ a_{n+2}=3a_{n+1}-a_n, \forall n \ge 1
\tag{1}
\end{cases} and $b_n$ defined dependently on the value of $a_n$ as: \begin{align}
b_n= \sum_{i=1}^n \frac{1}{a_i a_{i+1}}
\end{align} Then prove that $b_n$ converges and find $\lim_{n \to \infty} b_n$ My attempts I definitely believe that there's some way we can prove this recursively, like extracting $a_i$ and $a_{i+1}$ from the products, so we have the classic way to solve Egyptian fractions: \begin{align}
\frac1{a_ia_{i+1}} = k ( \frac1{a_i}-\frac1{a_{i+1}})
\end{align} Like that. But I haven't been able to figure it out. Is there any possible way to do that, or should I try something else? Would $b_n$ has itself a defining formula that is not dependent on $a_i$ ? Any help is appreciated!","['multivariable-calculus', 'calculus', 'algebra-precalculus', 'sequences-and-series']"
4305728,Common ways Leibniz notation is abused,"I've listed out a couple of instances where Leibniz's notation is abused, sometimes with nasty consequences. What are some other situations where Leibniz's notation is abused? Chain Rule Let $u = g(x)$ and $y = f(u)$ , then: $$(f(g(x)))' = \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}$$ Even with differential forms, it is still a bit awkward. Let $u(x) = g(x)$ and $y(x) = f(u)$ , then: $$\frac{dy}{dx} = \frac{ \frac{df}{du} du}{dx} = \frac{ \frac{df}{du} \frac{du}{dx} dx}{dx} = \frac{df}{du} \frac{du}{dx}$$ There's plenty of good answers which discuss its problems and alternatives. But just to add on to it, here's a common example of its abuse: $$\frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = \frac{dy}{dt} \frac{1}{ \frac{dx}{dt} } = \frac{y'(t)}{x'(t)}$$ This method is commonly shown in multivariable calculus courses when ""deriving"" curvature. Often called the dummy variable approach to ""deriving"" curvature. It's featured in LibreTexts and plenty of educational YouTube videos ""deriving"" curvature. An actual derivation using Lagrange's notation is significantly more enlightening. Multivariable Chain Rule for implicit functions In Leibniz's notation, we can obtain the partial derivative for the implicit function $F(x, y, z) = 0$ as: $$\frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z}$$ Which can cause the contradiction: \begin{aligned}
\frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z} \\
\frac{\partial z}{\partial x} = - \frac{\partial F}{\partial x} \frac{\partial z}{\partial F} \\
\frac{\partial z}{\partial x} = - \frac{\partial z}{\partial x}
\end{aligned} In Lagrange's notation, this is a non problem: $(z)'_x = - \frac{F'_x}{F'_z}$ . This kind of contradiction has been discussed in, other answers . The contradiction appears due to a misunderstanding/abuse of Leibniz notation which is very easy to make. Integration by substitution Integration by substitution, or u-substitution, often includes a similar process. As an example: $$\int (x+1)^2\ dx$$ Let $u=x+1$ , then $\frac{du}{dx} = 1 \implies dx = \frac{du}{1}$ $$\int (x+1)^2\ dx = \int (u)^2\ \frac{du}{1}$$ Which is a helpful mnemonic, yet the same rule can be derived quite nicely from the  chain rule, as it's simply dividing out the $g'(x)$ produced by the chain rule. Again, this has been discussed in other questions . Separation of variables This one is named after the abuse of notation. Unfortunately, it rather obscures the fact that it's simply integration by substitution: $$\frac{dy}{dx} = g(x) \cdot h(y)$$ $$dy = g(x) \cdot h(y) \cdot dx$$ $$\frac{1}{h(y)} dy = g(x) \ dx$$ $$\int \frac{1}{h(y)} dy = \int g(x) \ dx$$ This whole splitting step does almost seem mystical , there are plenty of great answers and alternatives to this method. It is unfortunate however that the name ""Separation of variable"" has stuck, which goes to show how prevalent the abuse of notation is. Green's theorem $$\int_C Pdx + Qdy = \int_C (Pdx + Qdy) \frac{dt}{dt} = \int_C \left(P\frac{dx}{dt} + Q\frac{dy}{dt}\right) dt$$ While a bit unfair, since it is simply a differential form, the reasoning offered to students can only be characterised as an abuse of notation. Personally, I feel that hinting/suggesting differential forms is unnecessary and unhelpful in most multivariable calculus course.","['notation', 'calculus', 'derivatives']"
4305733,Simplified estimate for derivatives of $f\circ g$?,"Let $g:\mathbb R\to \mathbb R$ be a smooth compactly supported function, and let $f:\mathbb R\to \mathbb R$ be a smooth function with $f(0)=0$ . Let $\|f\|_m := \|f\|_{C^m}=\sum_{0\le k\le m}\sup_{x\in\mathbb R} |f^{(k)}(x)|$ and let $[f]_m:= \|  f^{(m)} \|_{0}$ . I am able to prove the following estimates- \begin{align}
[f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}\|g\|_{0}^{i-1}[g]_{m}, \tag{$A_1$}\label{A1}\\
[f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}[g]_{1}^{(i-1) \frac{m}{m-1}}[g]_{m}^{\frac{m-i}{m-1}},\tag{$A_2$}\label{A2}
\end{align} ( $a\lesssim_m b$ means that $a\le Cb$ with a constant $C>0$ depending on $m$ .) Their proofs are just Faà di Bruno's formula followed up by either of the interpolation inequalities \begin{align}
[g]_i \lesssim \|g\|_{0}^{1-i/m} [g]_m^{i/m}, \quad\text{or}\quad [g]_i \lesssim [g]_{1}^{1-\frac{i-1}{m-1}} [g]_m^{\frac{i-1}{m-1}} . \quad(i\ge 1)
\end{align} Question I'm wondering however if its possible to get the following simpler looking bounds? \begin{align}
[f \circ g]_{m} & \overset{\color{red} ?}\lesssim_m  [f]_1[g]_{m} +\|f'\|_{m-1}\|g\|_0^{m-1}[g]_m, \tag{$B_1$}\label{B1}\\
[f \circ g]_{m} & \overset{\color{blue} ?}\lesssim_m  [f]_1[g]_m + \|f'\|_{m-1}[g]_1^m.\tag{$B_2$}\label{B2}
\end{align} These are something like only taking the $i=1$ and $i=m$ terms in the sums of ( $A_{1,2}$ ), but some seminorms are replaced with norms. I came across this in a certain Arxiv paper but I have found a number of other (minor) mistakes so my confidence in exactly this inequality is not the highest. ( $B_{1,2}$ ) are obvious in the simplest case $f(x)=x$ , and 'feel' like the above interpolation inequalities (specifically after further applying Young's inequality) $ [g]_j \lesssim \|g\|_0 + [g]_m$ , but I am unable to see how to estimate a general term of $(A_{1,2})$ , because it seems $f$ and $g$ cannot be estimated separately without decoupling them, hence getting a worse estimate, like e.g. $$ [f\circ g]_m \lesssim_m [f]_1 [g]_m + \|f'\|_{m-1}\|g\|^m_{m}.$$ I did manage to get a counterexample to a stronger estimate $[f \circ g]_{m}  \lesssim_m  [f]_1[g]_m + [f]_m[g]_1^m$ , indicating that $\|Df\|_{m-1}$ cannot be replaced with $[f]_m$ . (Just take $f(x)=x^2$ , $g(x)=\lambda g_0(x)$ with $g_0\in C^\infty_c$ , then send $\lambda\to\infty$ .)","['analysis', 'real-analysis', 'inequality', 'derivatives', 'chain-rule']"
4305781,How do we construct the cotangent space?,"I am following a graduate course in algebraic geometry and our professor introduced last week the cotangent space at a point p of a variety as the quotient $m/m^2$ where $m$ is the maximal ideal of the local ring at the point p. This is completely clear to me.
He then proceeded to explain some equivalent definitions in differential geometry, which I understood much less, given that I have just followed a quite easy introductory course. First off he said that in general we have a map $d: C(V)\rightarrow M$ , where $M$ is the cotangent space of all vector fields and $C(V)$ the smooth functions on the manifold, which sends $f\mapsto df$ . He called $df$ ""the one form df"" and gave no further explanation as to what it is, which is not clear to me. I especially don't understand how it acts on the space of all vector fields. He then went on to say that the cotangent space of all vector fields could also be defined to be generated by the elements ""of the form dr"" with r an element of the ring of continuous functions, imposing the relations given by Leibniz rule on them. In the end he said that the same construction could be made by tensoring the ring of continuous with itself and then quotient out by an appropriate ideal. Not having understood what came before, this remained mysterious. There is a lot of confusion in my mind right now and if you could clarify even a little bit of this definitions it would help me greatly.","['co-tangent-space', 'definition', 'algebraic-geometry', 'differential-geometry']"
4305796,Weak-$*$ sequential compactness of closed ball in bidual,"Let $(X,\|\cdot\|)$ be a normed vector space and let $X^*,X^{**}$ denote its continuous and second continuous dual, each endowed with the usual norm. Let $B$ denote the closed unit ball of $X^{**}$ . Since $X^{**}$ is the dual of $X^*$ , from the Banach-Alaoglu theorem, it is known that $B$ is compact in the weak- $*$ topology. I want to know if $B$ is sequentially compact, or relatively sequentially compact (still in the weak- $*$ topology). I know that the Eberlein-Šmulian theorem can be helpful when dealing with weak topologies, but here we're interested in the weak- $*$ topology. I don't mind adding the assumption that $X$ is Banach. However, I don't want to add a reflexivity assumption on $X$ . I am not well-versed in functional analysis or weak topologies. Actually, my question comes from the theory of optimization of functions.","['banach-spaces', 'weak-topology', 'functional-analysis', 'general-topology', 'compactness']"
4305806,Integral of $1 / \sqrt x$ using Limits,Actually the problem here is to find out the INTEGRAL of $\frac{1}{\sqrt x}$ using the limit definition. I am very well able to solve the question using POWER rule but that is not allowed in the question. $$b-a=nh$$ where $h$ is very small. Then $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}{\frac{h}{\sqrt x} + \frac{h}{\sqrt{x+h}}+...+\frac{h}{\sqrt{x+(n-1)h}}}$$ $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}({\frac{1}{\sqrt x} + \frac{1}{\sqrt{x+h}}+...+\frac{1}{\sqrt{x+(n-1)h}}})h$$ Every problem starts from this very point as from here I am unable to cancel out the $h$ 1 Tried to remove the sqrt from the denominator $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}({\frac{\sqrt x}{x} + \frac{\sqrt{x+h}}{x+h}+...+\frac{\sqrt{x+(n-1)h}}{x+(n-1)h}})h$$ After which it seems much more complicated 2 Taking the $h$ in the numerator to the denominator $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}{\frac{1}{\sqrt{\frac{x}{h^2}}} + \frac{1}{\sqrt{\frac{x+h}{h^2}}}+...+\frac{1}{\sqrt{\frac{x+(n-1)h}{h^2}}}}$$ After some cancelling $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}{\frac{1}{\sqrt{\frac{x}{h^2}}} + \frac{1}{\sqrt{\frac{x}{h^2}+\frac{1}{h}}}+...+\frac{1}{\sqrt{\frac{x}{h^2}+\frac{(n-1)}{h}}}}$$ Which left me in a frenzy 3 Lastly i tried to rake common first $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}{\frac{h}{\sqrt x} + \frac{h}{\sqrt{x+h}}+...+\frac{h}{\sqrt{x+(n-1)h}}}$$ Taking $\frac{h}{\sqrt x}$ common $$\int_a^b{\frac{1}{\sqrt x}} dx= \lim_{h \to 0}\frac{h}{\sqrt x}({1 + \frac{1}{\sqrt{1+\frac{h}{x}}}+...+\frac{1}{\sqrt{1+\frac{(n-1)h}{x}}}})$$ I was not able to remove $h$ from this method even,"['integration', 'limits', 'calculus', 'definite-integrals']"
4305903,How to find following limit of sequence?,"How to following limit $$\lim _{n \rightarrow \infty}\left(\frac{1}{n^{2}}+\frac{2}{n^{2}}+\cdots+\frac{(n-1)}{n^{2}}\right)\left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{n+n}\right). $$ My attempt: \begin{align*}
L_n&=\left(\frac{1}{n^{2}}+\frac{2}{n^{2}}+\cdots+\frac{(n-1)}{n^{2}}\right)\left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{n+n}\right).\\
&=\left(\frac{n-1}{2n}\right)\left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{n+n}\right).\\
\end{align*} By using camparison with $$\frac{n}{2n}\le \left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{n+n}\right)\le \frac{n}{n+1}$$ I got $$\frac{1}{4}\le \lim_{n\to \infty}L_n\le \frac{1}{2}$$ But I need exact answer. But I could not get. Its answer given is $\ln \sqrt 2$ . Thank you very much for your valuable time.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4305937,How many different ways of planting the flowers?,"Peter's neighbour, Paul likes tulips. He would like to plant 2 white, 5 red and 6 black tulips in a row in a way such that a red and a black tulip cannot be next to each other. How many different ways can he design the row? I count two cases: …w…w… bwrwb in which wrw takes 7 positions and it can start from 7 different places. rwbwr wbw takes 8 positions and can start from 6 different places. …ww… red ww black black ww red
Which is 2 ways So in Total I got $7+6+2=15$ ways.
But the answer is 33 ways.
I can’t think of any other ways of arrangement…","['combinations', 'combinatorics', 'discrete-mathematics']"
4305951,When two resolutions of coherent sheaves are homotopic,Let $\mathcal{F}$ be a coherent sheaf on a projective manifold $X$ . It is well known that one can construct a resolution of $\mathcal{F}$ by holomorphic vector bundles (locally free sheaves). Are two such resolutions homotopic? Any reference would be much appreciated.,"['complex-geometry', 'coherent-sheaves', 'reference-request', 'vector-bundles', 'algebraic-geometry']"
4305972,What exactly is a Collatz-like Problem,"What exactly is a Collatz-like problem? Let $f:\mathbb{N} \to\mathbb{N}$ . The Collatz function states that the following iterated map will eventually equal to 1: $$f(n) =
\begin{cases}
n/2,  & \text{if}\  2\mid n\\
3n+1, & \text{otherwise} \\
\end{cases}$$ I have seen many different iterated functions ( 1 , 2 , 3 )being described as Collatz-like or Collatz related once the search is for bounded or unbounded orbits. Searching for bounded or unbounded orbits is the intention of the person behind the function and doesn’t constitute a definition to me (but I could be wrong). This led me to wonder if there is a formal mathematical definition for a Collatz-like problem.","['collatz-conjecture', 'number-theory', 'dynamical-systems', 'sequences-and-series']"
4305974,The inverse of a complex function,"$f: \Bbb C \to U(0,1)$ ={ $z \in \Bbb C; |z|<1$ } $f(z)=\frac{z}{1+|z|} $ I have found it is bijective, but how can I find $f^{-1}$ ?
I wrote z = $\frac{u}{1+|u|}$ and tried to write u depending on z.","['functions', 'complex-numbers']"
4305978,"How to prove Algebra generated by$\{1,e^{ix}\}$ is dense in $C([0,\pi],\mathbb C)$?","Is it possible to approximate every function $f\in C([0,\pi],\mathbb C)$ uniformly with arbitary precision bby functions in the algebra generated by $\{1,e^{ix}\}$ ? I know it's true for $\{1,e^{ix},e^{-ix}\}$ , because for every continious function $f(x)=u(x)+v(x)i$ , $u,v$ is able to be approximated by algebra generated by $\{1,\cos x\}$ according to stone-weierstrass approximation theorem. But here we don't have $e^{-ix}$ ？","['weierstrass-approximation', 'calculus', 'trigonometry', 'uniform-convergence']"
4305979,"(Solved)Find the number of $9$ letter words using the letters P, Q, and R containing at least one P and at least two Qs.","Please help me with the last question on my discrete maths assignment because I can't get what I am doing wrong. Find the number of 9 letter words using the letters P, Q, and R containing at least one P and at least two Qs. number of $9$ letter words that use all $3$ letters $= 3^9$ $A_1 =$ number of $9$ letter words without the use of P $= 2^9$ $A_2 =$ number of $9$ letter words without the use of Q $= 2^9$ $A_3 =$ number of $9$ letter words using at least 1 Q $= 3^9 - 2^9$ $A_1 \cap A_2 = 1^9$ $A_1 \cap A_3 = 2^9 - 1$ $A_2 \cap A_3 = 0$ $A_1 \cap A_2 \cap A_3 = 0$ number of words using at least 1 P and at least 2 Qs $$= 3^9 - (2^9 + 2^9 + (3^9 - 2^9) - ( 1 + 2^9 - 1 - 0) + 0) = 0$$ I can't understand where I am making a mistake because everything seems to make sense.","['combinatorics-on-words', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4306023,"Why does repeatedly drawing circles around a point create a ""ring"" pattern?","Define a point $P_0$ at the origin. Define an ""iteration"" by putting a circle of radius one around $P_0$ and choosing a uniformly random point on the circle. Let that new point become $P_1$ . Then repeat that process with $P_1$ , say 15 times. Then repeat the steps before a large number, say 250,000 times (view this here ). Why does the image provided produce rings of color? (By rings I mean there's areas of color) The color is defined from the number of iterations to get to that point, and the darker the color the more iterations it took. Points that occur multiple times have their color averaged, and black means no points hit that location. The white circle is the first iteration. (Also, points outside of the circle do NOT stop it iterating, they are just not drawn) Also, just some curious questions: Is there a way to calculate the average distance for $n$ iterations (without simulating it)? Is there a function that can ""smooth"" out either image (doesn't have to be perfect)? What if you could choose a random radius? (250,000 simulations here , 35px is one radii)","['random-walk', 'circles', 'probability']"
4306035,Reduce to first order and solve $yy'' = 3y'^2$,"Reduce to first order and solve $yy'' = 3y'^2$ Dividing both sides by $y$ and re-arranging $y'' - 3y' = 0$ . This is clearly an homogenous equation, a solution might be $e^{3x}$ to test this: $y' = 3e^{3x}, y'' = 9e^{3x} \implies 9e^x-9e^x = 0$ Substituting $y = ue^{3x}, y' = u'e^{3x}+3ue^{3x}, y'' = u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x}$ Plugging these in $$(u''e^{3x}+3u'e^{3x}+3u'e^{3x}+9ue^{3x})-3\cdot (u'e^{3x}+3ue^{3x})=0$$ $$\implies u''e^{3x}+3u'e^{3x}=0$$ Substituting $v = u'$ $$\implies v'e^{3x}+3ve^{3x}=0$$ $$=\int\frac{dv}{v} = 3\int dx$$ However the solution to this exercise is $(c_1x+c_2)^{-\frac{1}{2}}$ , how did they get this?","['multivariable-calculus', 'ordinary-differential-equations']"
4306075,Cardinality of a subset of $S_{p-1}$.,"Let $p$ be an odd prime and $S_{p-1}$ the symmetric group of degree $p-1$ . Let's consider the subset: $$X:=\{\sigma\in S_{p-1}\mid P(1)\wedge P(2)\wedge P(3)\}$$ where: $P(1)$ : $\sigma(i)\ne i$ for every $i=1,\dots,p-1$ ; $P(2)$ : $\left|\sigma\right|\mid p-1$ ; $P(3)$ : if $\sigma=(i_{11}\dots i_{1s_1})\dots(i_{r1}\dots i_{rs_r})$ is the disjoint cycle decomposition of $\sigma$ , then $\forall j=1,\dots,r$ : $$\sum_{k=1}^{s_j}i_{jk}\equiv 0\pmod p \tag 1$$ So, in particular, every $(p-1)$ -cycle lays in $X$ . On the other hand, if $\sigma\in X$ is not a $(p-1)$ -cycle, then its cycle structure $(l_1,\dots,l_r)$ is such that $l_1+\dots+l_r=p-1$ , with every cycle's length $2\le l_j\le\frac{p-1}{2}$ and $\operatorname{lcm}(l_1,\dots,l_r)\mid p-1$ . For instance, if $p=7$ , then elements of $X$ are, e.g. : $\sigma=(123456)$ , $\sigma=(16)(25)(34)$ , $\sigma=(124)(356)$ . So, my question is: what's the cardinality of $X$ , for a given $p$ ? Edit . I had conjectured that the disjoint cycles would have all be of the same length, but $\sigma=(1,12)(2,11)(3,10)(49)(5678)$ is a counterexample for $p=13$ . Edit#2 . The connection between the set $X$ and $\Bbb Z_p^*$ , recalled by @kabenyuk's comment, was expected actually, considered the context where this question popped up, namely: Let's assume to be known that $\left|\operatorname{Aut}(C_p)\right|=p-1$ ; if we set $\operatorname{Aut}(C_p)=\{Id_{C_p},\varphi_1,\dots,\varphi_{p-2}\}$ , then it turns out that every $\varphi_i$ is of the form: \begin{alignat}{1}
\varphi_i(1)&=1 \\
\varphi_i(a^j)&=a^{\sigma_i(j)}
\end{alignat} where $\pmb{\sigma_i\in X}$ . My question here was meant as an intermediate step to finally prove that some of the $p-2$ permutations $\sigma_i\in X$ must be a $(p-1)$ -cycle, so as to have a "" $\Bbb Z_p^*$ -free"" proof of $\operatorname{Aut}(C_p)\cong C_{p-1}$ . But I see that this result seems really inseparable from the notion of $\Bbb Z_p^*$ .","['permutations', 'modular-arithmetic', 'combinatorics', 'symmetric-groups', 'group-theory']"
4306084,Find total number of bikes in the town,"In a town, there are 33 families that own either 1, 2, or 3 bikes. The number of families that own 1 bike is equal to the number of families that own 3 bikes. What is the number of the bikes in the town? I know the answer is 66. If you test cases you will see that the number is always 66. FOr example: 15 own 1b, 15 own 3b, 3 own 2b = 66 10 own 1b, 10 own 3b, 13 own 2b = 66 My question is, how do you explain this result in a technical way, taking into consideration the mean?","['statistics', 'puzzle', 'means']"
4306233,Is function $X \mapsto \mbox{Tr}\left(X X^T X X^T\right)$ convex?,"Is $\mbox{Tr}\left(X X^T X X^T\right)$ a convex function of arbitrary real matrix $X$ ? More generally, is $\mbox{Tr}\left(\left(X X^{\dagger}\right)^m\right)$ a convex function of arbitrary complex matrix $X$ for any integer $m \ge 1$ ? Any advice or suggestions would be greatly appreciated. The proof hint: Let us apply SVD to a matrix $X$ : $X$ = $U D V^{\dagger}$ . Every matrix has SVD with non-negative singular values on the main diagonal of $D$ . Next: $\left(X X^{\dagger}\right)^m$ = $U D V^{\dagger} V D U^{\dagger} U D V^{\dagger} V D U^{\dagger} \ldots U D V^{\dagger} V D U^{\dagger}$ = $U D^{2 m} U^{\dagger}$ . Note, all unitary matrices are cancelled in between, because $U^{\dagger}U = V^{\dagger}V = I$ . $\mbox{Tr} \left(\left(X X^{\dagger}\right)^m\right)$ = $\mbox{Tr} \left(U D^{2 m} U^{\dagger}\right)$ = $\mbox{Tr} \left(D^{2 m} U^{\dagger} U\right)$ = $\mbox{Tr} \left(D^{2 m}\right)$ = $\sum_i \sigma_i^{2 m}$ , where we used the cyclic property of trace operation, and $\{\sigma_i\}$ are the singular values of matrix $X$ . Let $\{x_i\}$ , $\{y_i\}$ and $\{z_i\}$ be the singular values of arbitrary matrices $X$ , $Y$ and their convex combination $Z$ = $\alpha X + (1 - \alpha) Y$ respectively. It was shown below by @PSL that for the Frobenius norm ( $m = 1$ ) the following holds true: $\alpha \sum_i x_i^2 + (1 - \alpha) \sum_i y_i^2 \ge \sum_i z_i^2$ . Considering that the function $\phi: x \rightarrow x^m, x \in R^+$ is convex, would it be possible to show that the case $m > 1$ is also satisfied: $\alpha \sum_i x_i^{2 m} + (1 - \alpha) \sum_i y_i^{2 m} \ge \sum_i z_i^{2 m}$ ? Update : by numerical simulation I found that $\sum_i x_i^{2} \ge \sum_i z_i^{2}$ does not necessarily entails $\sum_i x_i^{4} \ge \sum_i z_i^{4}$ on roughly 9% of random configurations. Seems like this line of thoughts does not work. However, the extended brute force simulation still succeeds for $m$ 2 to 5. Brute force approach to answer the questions. Here I literally check convexity on random matrices. The Python code speaks for itself: import numpy as np

tol = 10.0 * np.finfo(float).eps
count_ok, count_fail = int(0), int(0)

for m in range(2, 5 + 1):
    print(""m:"", m)
    for dim in range(2, 10 + 1):
        print(f""matrix size: {dim}x{dim}"")
        for test in range(100000):
            X = 2 * np.random.rand(dim, dim) - 1
            Y = 2 * np.random.rand(dim, dim) - 1
            XXt = X @ X.T
            YYt = Y @ Y.T
            for t in np.linspace(0.01, 0.99, 20):
                Z = X * t + Y * (1 - t)
                ZZt = Z @ Z.T
                ok = (np.trace(np.linalg.matrix_power(ZZt, m)) <=
                      np.trace(np.linalg.matrix_power(XXt, m)) * t +
                      np.trace(np.linalg.matrix_power(YYt, m)) * (1 - t) + tol)
                if ok:
                    count_ok += 1
                else:
                    count_fail += 1

print(f""succeeded: {count_ok} times"")
print(f""failed: {count_fail} times"")
print("""")
.......
succeeded: 72,000,000 times
failed: 0 times","['matrices', 'trace', 'convex-analysis']"
4306303,Chain rule substitution differentials,"I'm reading through Bamberg and Sternberg, and I'm on Chapter 5. It has the attached passage. I understand it up until it says we can substitute our $dy=15x^2dx$ into the $d(y^2)=2y\space dy$ equation. It states that $dy$ replaces $h$ as our displacement, since before it was made clear that we could write $d\alpha_y(h)=\alpha'(y)h,$ so this is merely a change in notation. However, all we did in the bottom was define a function $y(x)=5x^3+1$ whose differential $dy$ so happeend to have the same name as what we replaced $h$ with. So, how come we can just substitute this in to result in an example of the chain rule? To be clear, I mean: if the $dy$ in $d\alpha=\alpha'dy$ is just a real number and if the $dy$ in $dy=15x^2dx$ is instead a function of $x,$ how can we substitute the second instance of $dy$ into the first instance of $dy$ , just because we named them the same?","['calculus', 'derivatives', 'differential-forms', 'chain-rule']"
4306326,How to solve for the value of a discrete random variable without brute force? (Poisson Distribution),"Say I had a question like so: $X$ ~ $Po(3.1)$ Given $P(X < a) = 0.8$ (1 s.f) Solve for $a$ I can brute force it by summing the value of the probability mass function at $X=0$ , $X=1$ , $X=2$ etc. up until I reach the probability of $0.8$ If I do that, then I get: $a = 5$ $$ \sum_{k=0}^{4} [\frac{e^{-3.1} \cdot 3.1^{k}}{k!}] = 0.7981... $$ $ = 0.8$ (1 s.f) so I get $P(X < 5) = 0.8$ thus $a=5$ My question is: is there a cleaner and non-brute-force method of solving for this?","['statistics', 'poisson-distribution']"
4306356,What's the measure of the $\angle BAC$ in the triangle below?,"For reference: In the right triangle $ABC$ , right at $B$ , the corner $AF$ is drawn such that $AB = FC$ and $\angle ACB = 2 \angle BAF$ . Calculate $\angle BAC$ . My progress: $\triangle ABF: cos(\frac{C}{2}) = \frac{x}{AF}\\
AF^2 = x^2+BF^2\\
\triangle AFC: Law ~of~ cosines:\\
AF^2 = x^2+AC^2-2.x.AC.cosC\\
\triangle ABC:\\
cos C = \frac{BC}{AC} =\frac{x+BC}{AC}\\
x^2+(x+BF)^2 = AC^2\\
Th.Stewart \triangle ABC:\\
AC^2.BF+x^3=AF^2BC+BC.x.BF$ ...??","['euclidean-geometry', 'geometry', 'plane-geometry']"
4306459,The Laplace transform $\mathcal L$ is a self-adjoint operator on $L^2(\mathbb R_+)$,"Let $\mathbb R_+ = [0, \infty)$ and consider the kernel $K(x,y) = e^{-xy}$ on $\mathbb R_+ \times \mathbb R_+$ . The associated integral operator on $L^2(\mathbb R_+)$ is called the Laplace transform $\mathcal L$ , i.e. $$(\mathcal L f)(x) = \int_0^\infty K(x,y) f(y)\, dy$$ Show that the Laplace transform $\mathcal L$ is a self-adjoint operator on $L^2(\mathbb R_+)$ . This question is similar to another question that I'd asked a few days ago, but the same approach doesn't work since the kernel is not square-integrable - so Fubini's theorem may not be applicable. My work: We want to show $\langle \mathcal Lf, g\rangle = \langle f, \mathcal Lg\rangle$ for all $f,g\in L^2(\mathbb R_+)$ . We have $$\begin{align*}
\langle \mathcal Lf,g\rangle &= \int_0^\infty \mathcal Lf(x) \overline{g(x)} \, dx\\
&= \int_0^\infty \int_0^\infty e^{-xy} f(y) \overline{g(x)}\, dy\, dx
\end{align*}$$ and $$\begin{align*}
\langle f, \mathcal L g\rangle &= \int_0^\infty f(x) \overline{\mathcal Lg(x)}\, dx\\ &= \int_0^\infty \int_0^\infty e^{-xy} f(x) \overline{g(y)} \, dy\, dx\\&=  
\int_0^\infty \int_0^\infty e^{-xy} f(y) \overline{g(x)} \, dx\, dy  
\end{align*}$$ So, we must show $$\int_0^\infty \int_0^\infty e^{-xy} f(y) \overline{g(x)}\, dy\, dx =  \int_0^\infty \int_0^\infty e^{-xy} f(y) \overline{g(x)} \, dx\, dy $$ This can be done easily if Fubini's theorem is applicable. For Fubini's theorem, we require $$\int_0^\infty \int_0^\infty |e^{-xy}| |f(y)| |g(x)| \, dx\, dy < \infty$$ By applying Cauchy-Schwarz inequality twice (as in the linked post's answer), I was able to get $$\int_0^\infty \int_0^\infty |e^{-xy}| |f(y)| |g(x)| \, dx\, dy \le \|f\|_2 \|g\|_2 \left(\int_0^\infty \int_0^\infty e^{-2xy}\, dx\, dy \right)^{1/2}$$ Unfortunately, $$\int_0^\infty \int_0^\infty e^{-2xy}\, dx\, dy$$ diverges, so this bound is useless. Two possibilities arise: (i) either Fubini's theorem is applicable, and we must find a stronger bound to show $\int_0^\infty \int_0^\infty |e^{-xy}| |f(y)| |g(x)| \, dx\, dy < \infty$ , or (ii) we must show that the two integrals are equal using some technique other than Fubini's theorem. To me, (ii) seems less likely. I'd appreciate any help in completing the proof. Thank you! Note: $\mathcal L$ is a bounded bijective operator on $L^2(\mathbb R_+)$ , in fact, $\| \mathcal L\| = \sqrt\pi$ . The explicit calculations can be found in Rajendra Bhatia's Notes on Functional Analysis , Pg. $26-27$ , and also in Setterqvist E., Unitary Equivalence: A New Approach to the Laplace transform and the Hardy operator. Master’s Thesis, Department of Mathematics Lule ̊a University of Technology, 2005:329 CIV here . Further, see Post 1 , Post 2 and Post 3 .","['improper-integrals', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'fubini-tonelli-theorems']"
4306461,A few questions on Linear Algebra,"I actually posted this question a few weeks ago where I wanted my solutions to a few Linear Algebra questions checked. Now thanks to useful links provided by @GerryMyerson I can ask my questions (and verify my answers). The first three questions I just want solution verifications and suggestions for methods which are faster than the ones I used (also no answers were provided by the creator, so i'm not even sure of the correct answer). Question 1 : Show that the equation: $$x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0 $$ represents a sphere, and find its centre and radius. My Solution: $$x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0 $$ By completing the square we have that: $$((x+4)^2 -16) + ((y-3)^2 -9) + ((z+1)^2 -1) + 17 = 0 $$ Simplifying and collecting we have that: $$(x+4)^2 + (y-3)^2 + (z+1)^2 = 9 .$$ Since a sphere is an equation of the form: $$(x-x_0)^2 + (y-y_0)^2 + (z-z_0)^2 = r^2$$ where $x_0,y_0,z_0 = (-4,3,-1)$ are the coordinates of the centre of the sphere. Since $r^2 = 9$ the radius is $3$ . Our expression can be rewritten as: $$(x-(-4))^2 + (y-(3))^2 + (z-(-1))^2 = 0,$$ which is indeed the representation of a sphere (I think). Question 2: Find the area of the triangle with vertices $P(1,-2,3), Q(0,3,1)$ and $R(-1,1,0)$ . My Solution: Since $\vec{PQ} = \langle -1,5,-2 \rangle$ and $\vec{PQ} = \langle -2,3,-3 \rangle$ we take $\vec{PQ} \cdot \vec{QR}$ and have that: \begin{vmatrix}
i & j & k\\ 
-1 & 5 & -2 \\ 
-2 & 3 & -3
\end{vmatrix} Simplifying we get: $-9i + j + 7k$ therefore $\vec{PQ} \cdot \vec{QR} = \langle -9, 1, 7\rangle$ . $\Vert \langle \vec{PQ} \cdot \vec{QR} \rangle \Vert = \sqrt{(-9)^2 + (1)^2 + (7)^2} = \sqrt{131}$ . Therefore using the formula $A = \frac{1}{2}\Vert \vec{v} \cdot \vec{u} \Vert$ we get: $$A = \frac{1}{2} \cdot \sqrt{131}$$ $$A = 0.5 \cdot 11.4455$$ $$A = 5.723$$ units squared. Question 3: Find the volume of the paralelpiped spanned by the vectors $a = \langle 1,2,3 \rangle$ , $b = \langle 0,1,-1 \rangle$ and $c = \textbf{i} + \textbf{j}$ My Solution: Vector $\textbf{c}$ can be rewritten as $\langle 1,1,0 \rangle$ . Taking the $3\times 3$ matrix we get that: $$\begin{vmatrix}
1 & 2 & 3\\ 
0 & 1 & -1 \\ 
1 & 1 & 0
\end{vmatrix} = 1 \begin{vmatrix}
1 & -1 \\ 
1 & 0 \end{vmatrix} - 2 \begin{vmatrix}
0 & -1 \\ 
1 & 0 \end{vmatrix} + 3 \begin{vmatrix}
0 & 1 \\ 
1 & 1 \end{vmatrix} = \vert -4 \vert$$ Simplifying the determinant we get that the volume is $4$ units cubed. Those were the three question's that I need method/solution verification and improvements (if needed) for. I am still a beginner so if you do have any advice could you please explain it in a more fundamental way.","['matrices', 'solution-verification', 'linear-algebra', 'vectors']"
4306518,Trivial solution of differential equations,"For the following differential equation: $$y'+2xy^2=0$$ the trivial solution $y=0$ is a valid solution (as far as I know). However, the general solution for this DE is: $$y=\frac{1}{x^2+c}$$ For this general solution, there is no way to get exactly $y=0$ . How is it possible for the trivial solution to be valid but not achievable by the general solution, or did I make a mistake somewhere?",['ordinary-differential-equations']
4306603,Relation between the Hilbert-Hankel operator and Laplace transform on $L^2(\mathbb R_{>0})$,"Let $\mathbb R_{>0} = (0, \infty)$ . The Hilbert-Hankel operator $H$ is the integral kernel operator on $L^2(\mathbb R_{>0})$ defined as $$(Hf)(x) = \int_0^\infty \frac{f(y)}{x+y}\, dy$$ Prove that $H = \mathcal L^2$ , where $\mathcal L$ is (the Laplace transform) the integral kernel operator on $L^2(\mathbb R_{>0})$ defined as $$(\mathcal L f)(x) = \int_0^\infty e^{-xy} f(y)\, dy$$ Thus, conclude that $\|H\| =  \pi$ . Questions : The task is to show that $\mathcal L$ is a square root of the Hilbert-Hankel operator, i.e. $H = \mathcal L^2$ . Is this square root unique , i.e. does it make sense to define $H^{1/2}:= \mathcal L$ ? Is my proof (in particular the applicability of Fubini's theorem) correct? I worry because $\frac{1}{\sqrt x} \to \infty$ as $x\to 0$ , so I am not sure if Fubini's theorem is really applicable? Although, we only have to fix $x \in (0, \infty)$ and then evaluate the integrals - in which case, $\frac{1}{\sqrt x}$ is certainly just a finite number. So I wonder, are there any interesting results known about $\mathcal L^n$ , for $n\in \mathbb N$ ? Also, can we find the $n$ th root of the Hilbert-Hankel operator, i.e. what is $H^{1/n}$ for $n\in \mathbb N$ ? My work: If $H = \mathcal L^2$ , it is easy to see that $\|H\| = \|\mathcal L^2\| = \|\mathcal L\|^2 = \pi$ . This follows since $\|\mathcal L\| = \sqrt\pi$ and $\mathcal L$ is self-adjoint, as shown in this post. For self-adjoint operators $A$ (and more generally, for normal operators) it is true that $\|A\|^2 = \|A^2\|$ . The main effort lies in showing $H = \mathcal L^2$ .
Directly from definitions, we have $$\begin{align}
\mathcal L^2f(x) &= \int_0^\infty \int_0^\infty e^{-xy-yz} f(z)\, dz\, dy\\ &= \int_0^\infty\int_0^\infty e^{-xy-yz} f(z)\, dy\, dz\\
&= \int_0^\infty f(z) \int_0^\infty e^{-xy-yz} \, dy \, dz\\ &= \int_0^\infty \frac{f(z)}{x+z}\, dz\\ &= Hf(x)
\end{align}$$ where we have used Fubini's theorem. Fubini's theorem is applicable because $$\begin{align}
\int_0^\infty\int_0^\infty e^{-xy-yz} |f(z)| dz dy &\le \|f\|_2 \int_0^\infty \left(\int_0^\infty e^{-2xy-2yz} \, dz\right)^{1/2}\, dy\\ &= \|f\|_2 \int_0^\infty \frac{e^{-xy}}{\sqrt{2y}}\, dy \\ &= \sqrt{\frac{\pi}{2x}} \|f\|_2 < \infty 
\end{align}$$ for $0 < x < \infty$ . Thanks a lot!","['harmonic-analysis', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'fubini-tonelli-theorems']"
4306640,Existence of measurable maximum map.,"Let $Y$ be metric compact, $X$ be a continuous image of complete separable metric space. Assume we have $f: Y \times X \to \mathbb{R}$ (s.a. $f$ is continuous w.r.t. the first argument and borel w.r.t. the second one). We want to prove that there is exist measurable $F : X \to Y$ s.a. $F(x)$ is maximum of $y \to f(y,x)$ for all $x$ . Intuitively we want to show that $\sup_y f(y, x)$ is measurable. But I don't know where should we use compactness of $Y$ ? Should it be covered with finite open sets, and then ...? Any hints. UPD. as it was mentioned, we can consider the following task as measurability of $\pi_X(\{(x,y): f(y,x) > \epsilon\})$ for all $\epsilon$ .","['measure-theory', 'functional-analysis']"
4306654,"Asymptotic convergence of ODE solutions to a unique function, regardless of initial conditions","I have a non-linear differential equation of the form $$
\frac{dy}{dx}=F(x) - G(y,x)
$$ where $G$ is of the form $$
G(y,x) = y^3f_3(x)-y^2f_2(x)
$$ and $f_2,f_3>0$ for all $x$ , and $F(x)>0$ for all $x$ . Below I attach results from numerical integration for different initial conditions (the integration is backwards from $x=3$ to $x=0$ ), indicated by the colored lines.
It seems that regardless of the initial condition, the solutions seem to be asymptotic to a unique function. Is there a general statement I can use to prove this result? Namely, what conditions do $F,G$ need to satisfy in order for solutions to be asymptotic to a unique function, regardless of the initial conditions? Furthermore, it seems that a good approximation for this asymptotic behavior is given by the solution to the equation (shown as the black dashed line) $$
G(y,x)=F(x)
$$ which one gets if he sets $dy/dx=0$ in the above equation (which, of course, is not true). How can one explain this?","['numerical-methods', 'asymptotics', 'ordinary-differential-equations']"
4306680,"Converting $ \int_0^{\infty} \frac{e^{-\varepsilon s} \, (s+s^2)^{\beta}}{\log^{\gamma}((1+s)/s)} \, ds$ to a sum?","Could anyone shed some light on how to convert the following integral to a sum? $$ I=\int_0^{\infty} \frac{e^{-\varepsilon s} \, (s+s^2)^{\beta}}{\log^{\gamma}((1+s)/s)}  \, ds; \qquad\,\,\varepsilon,\beta,\gamma>0$$ With $t=\log((1+s)/s)$ , we get $$ I=I_3=e\int_0^{\infty} \frac{e^{-\frac{\varepsilon}{e^t-1}} \, e^{\beta t}}{t^{\gamma} \left(e^t-1\right)^{2\beta+2}}  \, dt=??$$ Using the generalized Bernoulli polynomials $B^\alpha_n(x)$ of order $\alpha$ defined by the generating function $$\frac{t^\alpha e^{xt}}{(e^{t}-1)^\alpha} =\sum_{n=0}^{\infty} B_n^\alpha(x) \frac{t^n}{n!}, \qquad (*)$$ Then, for $x=0$ and $\alpha=2\beta+2$ , we get $$ \frac{t^{2\beta+2}}{ \left(e^t-1\right)^{2\beta+2}} =\sum_{n=0}^{\infty} B_n^{2\beta+2}(0) \frac{t^n}{n!}. $$ Invert the sum & integral, $I$ read as \begin{align*}
I=e\int_0^{\infty} \frac{e^{-\frac{\varepsilon}{e^t-1}} e^{\beta t}}{t^{\gamma} \left(e^t-1\right)^{2\beta+2}}  \, dt= e\sum_{n=0}^{\infty} \frac{B_n^{2\beta+2}(0)}{n!} \int_0^{\infty} \frac{e^{-\frac{\varepsilon}{e^t-1}}e^{\beta t}}{t^{\gamma+2\beta-n+2}} dt. 
\end{align*} It remains to calculate $\int_0^{\infty} \frac{e^{-\frac{\varepsilon}{e^t-1}} e^{\beta t}}{t^{\gamma+2\beta-n+2}} dt=?$ . But some references they impose the condition $|t|<2\pi$ to use the formula $(*)$ . In my case $t\geq 0$ . I don't know, is this condition ( $|t|<2\pi$ ) necessary to write the formula $(*)$ ? If not, another trick for converting integral $I$ to a sum (in terms of some special functions)??","['integration', 'special-functions', 'calculus', 'closed-form', 'generating-functions']"
4306723,Estimate the number of zeros for the solution for Schrödinger equation.,"I have moved heaven and earth to solving the problem for three weeks, but there is no progress... The problem comes from ""Inverse scattering on the line"". Here I want to estimate the number of zeros for the solution for Schrödinger equation $$m''(x,k)+2ikm'(x,k)=q(x)m(x,k),\quad m(x,k)\rightarrow1\ as\ x\rightarrow\infty$$ in $Im(k)>0$ . Here $x$ is a real number (it's a 1-dim. Schrodinger equation), and $q(x)$ is a real potential which belongs to $\{q(x):\int^\infty_{-\infty}|q(x)|(1+|t|)dt<\infty\}$ I want to give an explanation for the upper bound of the number of zeros is $\int^{\infty}_x(t-x)|q(t)|dt$ . Previously, I finished that $m(x,k)=1+\sum^\infty_{k=1}g_n(x,k)$ , where $$g_n(x,k)=\int^\infty_x\int^{x_n}_x\int^{x_{n-1}}_x...\int^{x_2}_x\prod^{n}_{i=1} q(x_i)\prod^n_{i=1}\frac{e^{2ik(x_i-x_{i-1})}-1}{2ik}dx_1dx_2...dx_n$$ and $x_{-1}$ is $x$ . In this form, when $Im(k)>0$ , $m(x,k)$ is analytic. About $\int^{\infty}_x(t-x)|q(t)|dt$ , I have a estimate that $$|g_n(x,k)|\leq\frac{(\int^{\infty}_x(t-x)|q(t)|dt)^n}{n!}$$ By the way, my advisor told me use Sturm Comparison theorem, so I turn it into $$(e^{2ikx}m'(x,k))'-q(x)e^{2ikx}m(x,k)=0.$$ But I don't have any way to estimate the number of zeros for a differential equation.
Please give me some ideas.","['ordinary-differential-equations', 'partial-differential-equations']"
4306761,Are there countably many symbols (in the context of computation theory)?,"I've seen this kind of argument on countability of Turing-recognizable languages in several places: For any Turing machine $M$ consider it's encoding into a string $ \langle M \rangle$ . This encoding is in some $ \Sigma^*$ for some alphabet $\Sigma$ , say the alphabet of english language or simply $ \{0,1\}$ . Since the set of all strings in $ \Sigma^* $ is countable, so is the set of all Turing machines, and therefore all Turing-recognizable languages. I believe this argument demands the countability of the set of all symbols, since if there were uncountably many symbols, then there would be uncountably many turing machines like $M_{\sigma}$ which accepts exactly the symbol $\sigma$ . Only if symbols were countable we could ensure that the set of all of it's finite subsets are countable, also as mentioned in a comment here . So my question is, do we always take the set of tape symbols $\Gamma$ of turing machines to be a subset of some fix mother set which is countable? In my books ( Introduction to Languages and the Theory of Computation by John Martin and Introduction to the Theory of Computation by Michael Sipser) no such thing is mentioned; I would also  appreciate references.","['elementary-set-theory', 'computability', 'turing-machines', 'computer-science']"
4306784,Is this predicate-logic formula a tautology?,"$$F(x) \rightarrow \forall x F(x)$$ My teacher says that the formula is a tautology. Because when the antecedent is false, all the formula should always true, and when the antecedent is true, consequent can be infered by using $\forall +$ rule. Is this correct? I thought some problem in it, but I can't point it out. Sorry for my poor English.","['predicate-logic', 'logic', 'discrete-mathematics']"
4306806,"Distribution and moments of $\frac{X_iX_j}{\sum_{i=1}^n X_i^2}$ when $X_i$'s are i.i.d $N(0,\sigma^2)$","Suppose $X_1,X_2,\ldots,X_n$ are independent $N(0,\sigma^2)$ random variables. For $i,j\in \{1,2,\ldots,n\}$ , consider $$U=\frac{X_iX_j}{\sum_{i=1}^n X_i^2}$$ Provided $n>1$ , we know that $U$ has a Beta distribution when $i=j$ : $$U=\frac{X_i^2/\sigma^2}{\sum_{i=1}^n X_i^2/\sigma^2} \sim \text{Beta}\left(\frac12,\frac{n-1}2\right) \quad,\,i=1,2,\ldots,n$$ What can we say regarding the distribution of $U$ when $i\ne j$ ? What are the moments of $U$ in this case? For $n=2$ , if we transform $(X_1,X_2)$ to polar coordinates $(R,\Theta)$ , then $$U=\frac{X_1X_2}{X_1^2+X_2^2}=\frac{R^2\cos\Theta\sin\Theta}{R^2}=\frac{\sin(2\Theta)}{2}$$ Since $\Theta$ is uniformly distributed on $(0,2\pi)$ , it seems $\sin(2\Theta)$ has an $\text{Arcsine}(-1,1)$ distribution with pdf $$f(x)=\frac1{\pi \sqrt{1-x^2}}\mathbf 1_{(-1,1)}(x)$$ So that $U$ has pdf $$f_U(u)=2 f(2u)=\frac2{\pi\sqrt{1-4u^2}}\mathbf1_{\left(-\frac12,\frac12\right)}(u)$$ If $\boldsymbol X=(X_1,X_2,\ldots,X_n)^T$ , we can think of $U$ as the product of $i$ th and $j$ th components of the vector $\frac{\boldsymbol X}{\lVert \boldsymbol X \rVert}$ . And we know that $\frac{\boldsymbol X}{\lVert \boldsymbol X \rVert}$ is uniformly distributed on the surface of a unit sphere. I am not sure if this helps in any way.","['statistics', 'variance', 'probability-distributions', 'normal-distribution', 'expected-value']"
4306854,Deriving the thickness of a line on a grid,"Let's say we have a line going through the centers of two cells on a 2D grid, and we define a ""pixel-perfect rasterized line"" as the minimal set of 8-connected cells that best approximate the original line. Finding such a set of cells is usually done via the Bresenham's algorithm , but in some cases [ 1 , 2 , 3 ] it can be useful to have some closed-form formula e.g. $$\text{distance(point, line)} \,<\, \text{thickness}/2,$$ that tells whether a given cell's center belongs to such ""rasterized line"" or not. For this purpose, $\text{thickness} = 1$ doesn't always give correct results, but empirically I found that $$\text{thickness} = \max(|N_x|, |N_y|)$$ (where $N$ is the normalized direction of the line) seems to always work. Illustration of the correct (green) and incorrect (red) results: Unfortunately, I don't have enough mathematical background to understand why this ends up being the correct answer. How would one actually derive it?","['analytic-geometry', 'geometry']"
4306855,How to prove the following determinant identity,"Prove: $$
\begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & y & y & y & \cdots & y \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & y^{2} & 2 y^{2} & 2^{2} y^{2} & \cdots & 2^{m-1} y^{2} \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & y^{3} & 3 y^{3} & 3^{2} y^{3} & \cdots & 3^{m-1} y^{3} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & y^{n} & n y^{n} & n^{2} y^{n} & \cdots & n^{m-1} y^{n} \end{array}=(x-y)^{m^{2}}(x y)^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2}
$$ where $n=2m-1$ . A friend of mine gave me this problem. He claimed that he solved this by a very complicated method(which is too long to type here, and to be frank, I didn't get it at all). The following part is  my progress. First, we want to extract $y$ so that we can let $z=\frac{x}{y}$ , and then the determinant should be a polynomial $f(z)$ of variable $z$ . Thus we can use calculus to simplify it. This trick  usually works nicely on two-varible homogeneous determinant, but it doesn't kill this problem. Since we know the original problem is equivalent to $$
\begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & 1 & 1 & 1 & \cdots & 1 \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & 1 & 2  & 2^{2}  & \cdots & 2^{m-1}  \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & 1 & 3  & 3^{2}  & \cdots & 3^{m-1}  \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & 1 & n  & n^{2}  & \cdots & n^{m-1}  \end{array}=(x-1)^{m^{2}}x^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2}
$$ Note $f(x)=LHS$ , calculate the derivative of $f(x)$ , we have $$
x\frac{\,\mathrm{d}}{\,\mathrm{d}x}f(x)=\begin{array}{|cccccccccc|}
1 & 0 & \cdots & 0 & 0 & 1 & 0 & 0 & \cdots & 0 \\
x & x & \cdots & x & x & 1 & 1 & 1 & \cdots & 1 \\
x^{2} & 2 x^{2} & \cdots & 2^{m-2} x^{2} & 2^{m} x^{2} & 1 & 2 & 2^{2} & \cdots & 2^{m-1} \\
x^{3} & 3 x^{3} & \cdots & 3^{m-2} x^{3} & 3^{m} x^{3} & 1 & 3 & 3^{2} & \cdots & 3^{m-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x^{n} & n x^{n} & \cdots & n^{m-2} x^{n} & n^{m} x^{n} & 1 & n & n^{2} & \cdots & n^{m-1}
\end{array}
$$ which doesn't help at all. I hope you can share some thoughts of this problem.","['algebra-precalculus', 'determinant', 'linear-algebra', 'multilinear-algebra']"
4306863,Prove the given inequality related to log-sum inequality.,"Let $$ X_{1}, \cdots, X_{n}, X_{n+1}$$ be independent and identically distributed random variables such that $$ P(m \leq X_{1} \leq M) = 1, ~ \infty > M \geq m > 0$$ for given constants $m$ and $M$ . Then, I want to show the following inequality $$
E \left \{ \log \left ( \frac{c + \sum_{i=1}^{n+1} X_{i}}{c + \sum_{i=1}^{n} X_{i}}\right ) \right \}  = E \left \{ \log \left ( 1 + \frac{X_{n+1}}{c + \sum_{i=1}^{n} X_{i}}\right ) \right \} \geq \log \left ( 1 + \frac{1}{n+1} \right ), ~ {}^{\forall} n \geq 1,
$$ where $c$ is a constant satisfying $0 < c \leq E(X_{1})$ . I found that the LHS of the above inequality is related to the log-sum inequality. However, it is hard to find the literature about the lower bound of that term. I numerically check that the above inequality holds. Thanks for reading this post.","['logarithms', 'expected-value', 'inequality', 'probability-theory', 'probability']"
4306906,Determinant of a $4 \times 4$ matrix,"If $$ A = \begin{pmatrix}a&b&c&d\\-b&a&-d&c\\-c&d&a&-b\\-d&-c&b&a\end{pmatrix} $$ calculate $\det(A)$ . If you calculate $$AA^t=\begin{pmatrix}a^2+b^2+c^2+d^2&0&0&0\\0&a^2+b^2+c^2+d^2&0&0\\ 0&0&a^2+b^2+c^2+d^2&0\\0&0&0&a^2+b^2+c^2+d^2\end{pmatrix}$$ $$\det(AA^t)=(a^2+b^2+c^2+d^2)^4\Leftrightarrow (\det(A))^2=(a^2+b^2+c^2+d^2)^4$$ The answer is $\det(A)=(a^2+b^2+c^2+d^2)^2$ and not $\det(A)=-(a^2+b^2+c^2+d^2)^2$ . Short of calculating it by hand, why is it not negative?","['matrices', 'determinant', 'linear-algebra']"
4306908,Cheeger-Gromov convergence of manifolds implies Gromov-Hausdorff convergence,"I'm reading about convergence of Riemannian manifolds: A sequence of pointed complete Riemannian manifolds is said to converge in the pointed $C^{m, \alpha}$ topology $(M_i, p_i, g_i) \rightarrow (M,p,g)$ if for every $R > 0$ we can find
a domain $\Omega \supset B (p,R) \subset M$ and embeddings $F_i : \Omega → M_i$ for large i such that $F_i (\Omega) \supset B(p_i ,R)$ and $F_i^* g_i \rightarrow g$ on $\Omega$ in the $C^{m, \alpha}$ topology. Then it is stated that it is easy to see that
this type of convergence implies pointed Gromov-Hausdorff convergence. I haven't been able to figure out how to see this. Can you help me with this? I guess we need to contruct some metric on $M \sqcup M_i$ using the $C^{m, \alpha}$ -convergence of $F^*g_i$ to see this?","['gromov-hausdorff-limit', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
4306910,Programming-how to find the angle,"I want to write a function in python that has as two arguments, the sine and the cosine of an angle and it returns the angle. Which formula do we use to calculate the angle knowing the sine and the cosine? If we could just use the inverse trigonometric functions we wouldn't have to define two arguments for the function. I don't have an idea how we can calculate the angle. Can you help please?","['python', 'trigonometry', 'angle', 'computer-science']"
4306915,Finding the asymptotic behavior of a function defined implicitly,"I encountered this when trying to solve a number theory problem. I have two variables $x,y$ related by $$(\ln(x))^{y+1}=(\ln(xy))^y$$ and I want to know how big $y(x)$ is as $x\to\infty$ . Ideally I want to know that $y(x)\sim\ln(x)$ or $y(x)\sim \sqrt{x}$ or whatever it is. But if that's not possible, a tight lower bound would suffice. I've messed with this expression enough that I managed to convince myself it isn't possible to isolate either variable, except maybe using something like Lambert's W function, and even that I wasn't able to do. So the remaining alternative is to try to solve it as a problem about an implicit function. This sounds like something that is possible with standard calculus tools.","['implicit-function', 'calculus', 'asymptotics']"
4306941,"""Centralizer"" of an irreducible character?","There is a chance that my formulations of the questions is wrong (in which case I apologize), but I want to give it a try as I thought these questions would be natural to ask, and there should have been answers already. Let $G$ be a finite group, and $\mathbb{C}$ the field of complex numbers. So the group algebra $\mathbb{C}G$ can be decomposed as full matrix algebras: $\mathbb{C}G = B_1 \oplus B_2 \oplus \dots \oplus B_k$ where $k$ is the number of conjugacy classes of $G$ , and each $B_i$ is isomorphic to the full $d_i \times d_i$ matrix algebra $M_{d_i}(\mathbb{C})$ . Each $B_i$ is a subalgebra, or 2-sided ideal of $\mathbb{C}G$ . Consider the regular representation of $G$ by right multiplication. So each $B_i$ can be decomposed as direct sum of $d_i$ copies of isomorphic irreducible $\mathbb{C}G$ -modules. This decomposition is only unique up to isomorphism, but not unique. Let's say one of the decomposition is as follows: $B_i = V_{i,1} \oplus V_{i,2} \oplus \dots \oplus V_{i,d_i}$ Each of the above summands is a $d_i$ -dimensional irreducible (right) $\mathbb{C}G$ -module affording the same irreducible character $\psi$ of $G$ . Now here are the questions: As $B_i$ is also a left $\mathbb{C}G$ -module, if we fix one of the summands, say $V=V_{i,1}$ , and consider the set $N_G(V)$ of elements in $G$ which fixes $V$ from the left, i.e. $N_G(V) = \{g \in G | gV = V\}$ , I think this is a subgroup of $G$ ? For instance if $d_i = 1$ then $N_G(V) = G$ . But if $d_i > 1$ it's definitely not $G$ . If I'm correct (that $N_G(V)$ is a subgroup of $G$ ), is there any characterization of this subgroup of $G$ ? Obviously this subgroup depends on the choice of $V$ , but are they all isomorphic or conjugate in $G$ for all the irreducible (right) $\mathbb{C}G$ -submodules isomorphic to $V$ ? What is the relationship between these subgroups and $\psi$ ? Again I feel like if this reasoning were correct, there should have been discussions about this topic in the literature already (I'm not aware of any), so it's quite possible I made a mistake somewhere. I hope experts can point me to the right direction.","['group-theory', 'representation-theory', 'characters']"
4306945,"Sum of the total number of ""moves"" needed to sort the n cards for each permutation.","We have n cards numbered from 1 to n. We get pre-shuffled cards and we have to sort them in ascending order according to some rules: we look at the number of the first card in the sequence (let's denote it by x) and find the card with the number x - 1 (unless x = 1, it then finds
card number n), and then translates it to the beginning. The time it takes to complete this operation is proportional to the distance of the found card from the beginning of the sequence of cards. The time for sorting cards is the sum of all times
operations performed. We are to calculate the sum of the operations for all n! permutation. Example: for n = 3 we have 6 permutation 1 2 3   (0 moves) 1 3 2   ( 1 move ) 3 1 2 (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 5 moves) 2 1 3   ( 1 move ) 1 2 3 2 3 1   (2 moves) 1 2 3 3 1 2   (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 4 moves) 3 2 1   ( 1 move ) 2 3 1 (2 moves) 1 2 3 (summary 3 moves) getting it all together to sort 3! permutations we needs 15 moves. for n = 4 we needs 168 moves for n = 5 we needs 1700 moves for n = 6 we needs 17,220 moves for n = 7 we needs 182,406 moves for n = 8 we needs 2,055,200 moves For small n we can easly calculate it using a brute force algorithm, but how to do it for large n like 100,000 or bigger. Is it any mathematical formula to count all possibilities ? I have no idea how to go about it, do you have any usefull tips ? What area of mathematics describes this issue ?????????","['discrete-mathematics', 'combinatorics', 'card-games', 'probability']"
4307023,Diffeomorphisms of the torus $\mathbb T^n$,"While reading the first chapter of Orbifolds and Stringy Topology by Adem, Leida and Ruan, I stumbled upon the following assertion, in page 5 The group $\text{GL}_n(\mathbb Z)$ acts by matrix multiplication on $\mathbb R^n$ , taking the lattice $\mathbb Z^n$ to itself. This
then induces an action on $\mathbb T^n=(\mathbb R/\mathbb Z)^n$ . In fact, one can easily show that the
map induced by looking at the action in homology, $\Phi:\text{Aut}(\mathbb T^n)\rightarrow\text{GL}_n(\mathbb Z)$ ,
is a split surjection. What? I know that a linear isomorphism $A:\mathbb R^n\rightarrow\mathbb R^n$ induces a diffeomorphism of $\mathbb T^n$ if, and only if the matrix associated to $A$ has coefficients in $\mathbb Z$ and has determinant equal to $\pm1$ , that is, if it belongs to $\text{GL}_n(\mathbb Z)$ . There are many posts about that on this site, but I am puzzled about that action in homology thing, and that it induces an split surjection from the group $\text{Aut}(\mathbb T^n)$ onto $\text{GL}_n(\mathbb Z)$ . I can imagine diffeomorphisms of the torus that don't come from any linear isomorphism on $\mathbb R^n$ , but why should these have any structure whatsoever? Thanks in advance for your answer.","['automorphism-group', 'differential-topology', 'group-theory', 'group-actions', 'differential-geometry']"
4307045,Reversing level to xp formula,"I have written a function to convert a given ""level"" from a game into the amount of XP required to reach that level. Level $0\to 1$ costs $100$ XP, from then on each level costs $25$ more than the previous $0\to 1$ = $100$ XP $0\to 2$ = $225$ XP $0\to 3$ = $375$ XP The formula I have to convert level to XP is as follows; $$\text{XP}=(100 \times \text{level}) + \frac{25 \times (\text{level}^2- \text{level})}{2}$$ I need to calculate the reverse of this, converting XP into the user's current level. How can I convert this into a formula for the level?","['algebra-precalculus', 'algorithms']"
4307066,Can there be a $C^1$ everywhere but $C^2$ nowhere function?,"Obviously examples of continuous-everywhere and differentiable-nowhere functions abundant. Similarly there are examples of everywhere-smooth nowhere-analytic functions. And I'm well aware of the various differentiability properties of the family of functions $$f_n(x) = x^n \sin(1/x), \quad n=0,1,2,\ldots$$ Namely that $f_2(x)$ is differentiable by not $C^1$ (etc. for higher values of $n$ ). But a colleague just asked me if there are any examples of $C^1$ functions that are nowhere $C^2$ , and I came up blank. I would imagine the same kind of construction could be asked about functions that are differentiable everywhere, but $C^1$ nowhere.","['derivatives', 'analysis', 'real-analysis']"
4307131,Derivation involving taking moment of Navier Stokes equation,"The article under concern is: Germano, M. (1992). Turbulence: The filtering approach. Journal of Fluid Mechanics, 238, 325-336. doi:10.1017/S0022112092001733 On page 328, Germano considers the Navier-Stokes equation (equation 17) written in Einstein notation: $u_{i, t}+\left(u_{i} u_{k}\right)_{, k}=-p_{, i}+\sigma_{i k, k}$ and states that 'by taking a moment of (17) with $u_j$ and adding this to another moment of the same equation but with the indices interchanged', the following equation can be achieved: $\left(u_{i} u_{j}\right)_{, t}+\left(u_{i} u_{j} u_{k}\right)_{, k}=-\left[p u_{i} \delta_{j k}+p u_{j} \delta_{i k}-\nu\left(u_{i} u_{j}\right)_{, k}\right]_{,k}+2 p s_{i j}-2 v u_{i, k} u_{j, k}$ where $\sigma_{i j}=2 \nu s_{i j} ; \quad s_{i j}=\frac{1}{2}\left(u_{i, j}+u_{j, i}\right)$ . The derivation is made with the following commuting properties of the moment operator: $\left( f_{, t}\right)=(f)_{, t} ; \quad\left( f_{, k}\right)=( f)_{, k}$ After hours of attempts, I got nowhere near the final derivation. One of the significant issues I am facing is how $p$ , without the derivative (so $p_{,i}$ ), is achieved as well as where the $\delta_{jk}$ and $\delta_{ik}$ come from. Also, regarding the 'indices interchanged' part, is the author referring to equation (17) itself or after taking the moment of it with $u_j$ ? I would really appreciate if someone could shed some light on how I should go about deriving this.","['fluid-dynamics', 'statistics', 'partial-differential-equations']"
4307164,Homomorphism from $S_4$ to $S_6$,"I was asked the question There are $6$ subsets of order $2$ of $\{1,2,3,4\}$ . Any element of $S_4$ permutes these subsets. Show that the resulting homomorphism from $S_4$ to $S_6$ is injective and its image lies in $A_6$ . Now, I can understand that any element of $S_4$ permutes some of the elements of order $2$ , and that gives us a permutation of $S_6$ . But, I can't seem to have a strong grasp of how the permutation of $S_4$ is influencing the permutation of $S_6$ . So, I can't figure out why two different permutations of $S_4$ can't take us to the same permutation of $S_6$ . Please help in this. Also, I can see that since we are only considering the two element subsets, there's already a sense of parity introduced in the system. But, how can we mathematically show that all of these permutations of $S_4$ only gives us even permutations?","['permutations', 'group-homomorphism', 'finite-groups', 'symmetric-groups', 'group-theory']"
4307174,Use of moore-penrose inverse when modeling PCA,"In the derivation for principal component analysis we model our observed data points y as the result of a linear transformation restricted to being an axis change, W , applied to a set of uncorrelated variables x , where x lives in a lower dimensional space than y . Thus, we can represent our observations as y = Wx , and what we are trying to find as x = W'y , where W' is the pseudo-inverse of W . Is the reason we choose to model the mapping of y onto the pca axes with use of a pseudo-inverse due to the fact that we want to be able to use PCA on systems where y doesn't completely follow our assumptions? Where not all points in y are contained in the column space of W ?","['statistics', 'linear-algebra', 'linear-transformations']"
4307195,Probability that the sum of outcomes from a single die is odd after the number 1 is rolled,"So, I'm a programmer and yesterday I was at a job interview and I was asked the following question: A six-sided die is rolled until the number 1 is thrown.
What is the probability that the sum of all previous outcomes (including the last one) is odd? I'm reasonably OK with probability, but this question had me stumped. How can I calculate this probability without knowing how many times the die was rolled? I would appreciate any ideas on the best way to approach this kind problem. Thanks!","['conditional-probability', 'probability-theory', 'probability']"
4307221,Can this set be open in a topological vector space?,"Let $X$ be a topological vector space, where the topology (as in Rudin's Functional Analysis ) is such that, every singleton is a closed set, the vector space operations are continuous. Now consider the set $S \subset X$ with the following property. $0 \in S$ , for a nonzero $v \in X$ and all $0 < t < a \in \mathbb R$ , we have $tv \notin S$ . Can $S$ be open ? some may argue that the open ball $B_r(0)$ , an open beighborhood, intersects $tv$ and so $0$ is not an interior point of $S$ and therefore $S$ can't be open. But that is cheating, since there may be no metric or norm on the space to tell what is an open ball, I know that $0$ looks like a boundary point but how to show that for a vector topology. If my reasoning is wrong somewhere, please hint! Added: If am I allowed to take the discrete topology, which doesn't seem to be inconsistant with the vector topology above, then perhaps $S$ is open, but only if the field of scalars has this (discrete) topology too, otherwise, e.g. for $\mathbb R$ , it is not going to be the case. Correct me if I'm wrong Added: What are the further conditions on the vector topology such that $S$ is open, if we exclude the (trivial) discrete topology ?","['general-topology', 'topological-vector-spaces', 'functional-analysis']"
4307286,Polynomial of compact operator on Hilbert space.,"Apologies if this is a duplicate. And just to note, this is self-study.
Let $B$ be a compact self-adjoint operator on a Hilbert space $H$ . We consider the polynomial $p(x) = a_0+ \sum_{i=1}^n a_i x^i$ .
Consider now the polynomial of $B$ , $$
p(B) = a_0 I +\sum_{i=1}^n a_i B^i. 
$$ I am to show that $p(B)$ be a compact operator for all real values of the coefficients.
I believe that this cannot be the case unless $a_0 = 0$ . My thinking is that we know that the (positive) powers and sums of compact operators are again compact, so $\sum_{i=1}^n a_i B^i$ is compact. If we assume that $p(B)$ is compact, then $p(B)-\sum_{i=1}^n a_i B^i$ should be compact, but this is $a_0 I$ , and the identity operator is not compact (unless H is finite-dimensional), so $p(B)$ cannot be compact... but nevertheless, the lecture notes claim that this is the case, so where am I making a mistake?","['compact-operators', 'functional-analysis']"
4307297,"$x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right)$, $\lim_{n\to\infty}x_n=\dfrac{f'(0)}{2}$","Let $f(x)$ be a function and $f(0)=0$ , $f'(0)$ exists. Let $$x_n=f\left(\frac{1}{n^2}\right)+f\left(\frac{2}{n^2}\right)+\cdots+f\left(\frac{n}{n^2}\right).$$ Prove that $\lim_{n\to\infty}x_n=\dfrac{f'(0)}{2}$ . The solution is given by: Since $f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x-0}$ , we have that $$f(x)=f(0)+f'(0)x+o(x)=f'(0)x+o(x).$$ When $n\to+\infty$ , we have that $$f\left(\frac{1}{n^2}\right)=\frac{f'(0)}{n^2}+o\left(\frac{1}{n^2}\right)=
\frac{f'(0)}{n^2}+o\left(\frac{1}{n}\right),\quad
f\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{2}{n^2}\right)=\frac{2f'(0)}{n^2}+o\left(\frac{1}{n}\right)$$ $$f\left(\frac{k}{n^2}\right)=\frac{kf'(0)}{n^2}+o\left(\frac{k}{n^2}\right)=
\frac{kf'(0)}{n^2}+o\left(\frac{1}{n}\right),\dots$$ Hence $$x_n=f'(0)\cdot\frac{1+2+\cdots+n}{n^2}+n\cdot o\left(\frac{1}{n}\right)=f'(0)\cdot\frac{n+1}{2n}+o(1),$$ then $$\lim_{n\to\infty}x_n=\frac{f'(0)}{2}.$$ I have a question: in the above steps, $o\left(\frac{1}{n}\right)$ are different, can we do this $n\cdot o\left(\frac{1}{n}\right)=o(1)$ . A more accurate question is: suppose there is a two-dimensional sequence $\{x_{nm}\}_{n,m\in\mathbb{N}}$ and for a fixed $m$ , there will always be $\lim_{n\to\infty}x_{nm}=0$ , is it right that $$\lim_{n\to\infty}\frac{x_{n1}+x_{n2}+\cdots+x_{nn}}{n}=0 ?$$ The original intention of this question is that we used the conclusion $n\cdot o(1/n)=o(1)$ in the answers to the above questions. Since these $o(1/n)$ are different, set to $$\alpha_{n1},\alpha_{n2},\dots,\alpha_{nn},\quad \alpha_{nk}=o(1/n),\quad 1\leq k\leq n.$$ which is $$\lim_{n\to\infty}\dfrac{\alpha_{nk}}{\dfrac{1}{n}}=\lim_{n\to\infty}n\alpha_{nk}=0,\quad 1\leq k\leq n.$$ Is the limit of the sum of $\alpha_{n1},\alpha_{n2},\dots,\alpha_{nn}$ is also $0$ ? That is whether there is $$\lim_{n\to\infty}(\alpha_{n1}+\alpha_{n2}+\cdots+\alpha_{nn})=
\lim_{n\to\infty}\frac{n\alpha_{n1}+n\alpha_{n2}+\cdots+n\alpha_{nn}}{n}\stackrel{?}=0.$$ This is the solution: This is the $\epsilon$ - $\delta$ solution given by a teacher, here we assume $f'(0)=1$","['calculus', 'real-analysis']"
4307298,Clarification of showing that something is an involution e.g. *-algebra,"Let $\ell^1(\mathbb{Z}):=\{(x_n)_{n\in\mathbb{Z}}:x_n\in\mathbb{C},\sum_{k\in\mathbb{Z}}|x_n|<+\infty\}$ .
Given by $(x^\ast)_n:=\overline{x_{-n}}$ , we want to show that for $x \in \ell^{1}(\mathbb{Z})$ , $\ell^{1}(\mathbb{Z})$ is a $*-algebra$ claim: $(x^\ast)^\ast=x$ , $\forall x\in \ell^1(\mathbb{Z})$ . \begin{equation*}
    (x^\ast)^\ast=(\bar{x})^\ast=\bar{\bar{x}}=x.
\end{equation*} 2. Let $x,y\in \ell^1(\mathbb{Z})$ and $a,b\in\mathbb{C}$ . Claim: \begin{equation*}
    (ax+by)^\ast=\bar{a}x^\ast+\bar{b}y^\ast=\bar{a}\bar{x}+\bar{b}\bar{y}=\overline{ax}+\overline{by}=(ax)^\ast+(by)^\ast=(ax+by)^\ast.
\end{equation*} 3. $x,y\in\ell^1(\mathbb{Z}) $ claim: \begin{equation*}
    (xy)^\ast=y^\ast x^\ast
\end{equation*} But $(xy)^\ast=(\overline{xy})=(yx)$ and then what??? Furthermore, I must show that $*$ is an isometry, which can easily be done if I show that \begin{align*}
    ||x||^{2} \leq ||x^{*}x||,
\end{align*} but I am not quite sure if that inequality holds $\forall x \in \ell^{1}(\mathbb{Z}) $","['calculus', 'operator-theory', 'linear-algebra', 'functional-analysis']"
4307346,Is subset of $L^1$ functions bounded by $1 / x^2$ compact?,"I was given the following exercise Given $E \subset L^1([1, +\infty))$ as follows $$
E = \left\{ f \in L^1([1, +\infty)) \;\middle|\; \forall x \in [1, +\infty) \quad |f(x)| \leq \frac{1}{x^2} \right\}
$$ is $E$ bounded? is $E$ closed? is $E$ compact? I tried solving it with other people. First we noticed that for $E$ to be well defined in $L^1$ the condition should actually be for almost all $x \in [1, +\infty)$ otherwise it is meaningless in $L^1$ . Then I was told that $E$ was compact and that there are various (non obvious) ways to prove it: one uses the fact that given $f_n(x)$ we can use Lousin's theorem to get families of continuos functions that are then used to construct $f(x)$ over the rationals... Another one uses convolution of the $f_n$ with a smooth kernel to get a continuous function to define the limit $f(x)$ ... But after this we found that the sequence of functions given by $$
f_n(x) = \left|\frac{\sin(nx)}{x^2}\right|
$$ is in $L^1$ as $\forall n \; \int_1^\infty |\sin(nx)/x^2| \mathrm dx \leq \int_1^\infty |1/x^2| \mathrm dx = \|1 / x^2\|_{L^1} < +\infty$ and $|f_n| \leq 1 /x^2$ so satisfies the condition but we could not find what this function should converge to in $E$ , is this an actual counterexample and the statement is actually false or is there an explicit limit for this sequence? Note. We got this exercise as a kind of extension of the analogous set in $\ell^1$ given by $\{ (x_n)_n \in \ell^1 \mid |x_n| \leq 1 / n^2 \}$ .","['lebesgue-measure', 'functional-analysis', 'oscillatory-integral', 'compactness']"
4307406,Limit of a conditional probability concerning a sequence of random bit-rewrites,"Suppose a sequence of random bits $(X_1,X_2,...)$ is defined as follows, where $X_0=1$ and $p\in(0,1):$ $$X_i=\begin{cases}X_{i-1}&\text{ with probability }p,\\[1ex] 1-X_{i-1}&\text{ with probability }1-p. \end{cases}\\[2ex]$$ Thus each random bit is either a direct copy or an inverted copy of its predecessor. Question : How to prove the following? ... $$\text{For all }p\in(0,1),\ \lim_\limits{n\to\infty}P(X_1=1\mid X_n=1)=p.\tag{1}$$ Plots suggest conjecture (1), with monotonic convergence when $p\ge 1/2$ and oscillatory convergence when $p<1/2$ : For each $n$ , $\mathsf P(X_1=1\mid X_n=1)={\mathsf P(X_1=1,X_n=1)\over\mathsf P(X_n=1)}$ is a ratio of polynomials in $p$ , e.g. as below (in lowest form) for a few cases: n  P(X_1 = 1 | X_n = 1)
-- --------------------
1  1 
2  p^2/(2*p^2 - 2*p + 1)
3  (2*p^2 - 2*p + 1)/(4*p^2 - 6*p + 3)
4  (4*p^4 - 6*p^3 + 3*p^2)/(8*p^4 - 16*p^3 + 12*p^2 - 4*p + 1)
5  (8*p^4 - 16*p^3 + 12*p^2 - 4*p + 1)/(16*p^4 - 40*p^3 + 40*p^2 - 20*p + 5)
6  (16*p^6 - 40*p^5 + 40*p^4 - 20*p^3 + 5*p^2)/(32*p^6 - 96*p^5 + 120*p^4 - 80*p^3 + 30*p^2 - 6*p + 1)
7  (32*p^6 - 96*p^5 + 120*p^4 - 80*p^3 + 30*p^2 - 6*p + 1)/(64*p^6 - 224*p^5 + 336*p^4 - 280*p^3 + 140*p^2 - 42*p + 7)
8  (64*p^8 - 224*p^7 + 336*p^6 - 280*p^5 + 140*p^4 - 42*p^3 + 7*p^2)/(128*p^8 - 512*p^7 + 896*p^6 - 896*p^5 + 560*p^4 - 224*p^3 + 56*p^2 - 8*p + 1) If $n$ is not too large, these ratios are readily obtained by representing the events $(X_1=1,X_n=1)$ and $(X_n=1)$ as length- $n$ pattern strings of form $1??...?1$ and $???...?1$ , respectively. Each of these can then be converted to a list of all possible bitstrings matching the pattern, whose probabilities are simple products whose sum is to be taken. E.g., with $q=1-p$ , $$\mathsf P(X_1=1\mid X_n=3)={\mathsf P(1?1)\over \mathsf P(??1)}={\mathsf P(101)+\mathsf P(111)\over \mathsf P(001)+\mathsf P(011)+\mathsf P(101)+\mathsf P(111)}={pqq+ppp\over qpq+qqp+pqq+ppp}={2p^2 - 2p + 1\over 4p^2 - 6p + 3}.$$ We note that, apparently, when the ratios are expressed in lowest form, $$\text{numerator of $\mathsf P(X_1=1\mid X_n=1)$}=\begin{cases}\text{denominator of $\mathsf P(X_1=1\mid X_{n-1}=1)$}&\text{for odd }n>1\\[2ex] p^2\times\text{denominator of $\mathsf P(X_1=1\mid X_{n-1}=1)$}&\text{for even }n>1 \end{cases}
$$ but I don't know if this is useful. (The question seems so elementary, I feel like a proof ought to be likewise -- sorry if I'm just not seeing something obvious.)","['conditional-probability', 'polynomials', 'probability-theory', 'probability', 'random-variables']"
4307477,Ways to place $5$ Ps in a $4 × 4$ grid so that each row has at least one P?,"In how many ways can $5$ P's be placed in $16$ identical squares formed by a $4$ × $4$ grid, such that each row has at least one P? The number of ways of introducing first P will be: $16$ ( let us say the chosen cell is in first row) Next P has $12$ cells to go in. (let us put that in $2$ nd row) $3$ rd P has $8$ cells. (put that in $3$ rd row) $4$ th P has $4$ cells. (put that in $4$ th row) And because $4$ cells out of $16$ have been full now, so the final P has $12$ cells to go in. Making the total number of ways to place = $16 \cdot 12 \cdot 8 \cdot 4 \cdot 12 = 73728$ ways. Where am I going wrong?","['permutations', 'combinatorics']"
4307489,How to calculate a definite integral with a variable as an endpoint and get its derivative,$$F(x) = \int_{{3}}^{x^2} \frac{t^{{5}} + \sin(t)}{t^{{5}}} dt$$ $$Find\:F'(x)$$ I've been trying to think about how to do this question for hours and trying many different things. I would imagine there is a way to do it without taking the antiderivative as it seems a really complex task. If someone could point me in the right direction on this that would be great.,"['integration', 'calculus', 'definite-integrals', 'derivatives']"
4307566,Convex function is proper when it has at least one finite value in the relative interior of its effective domain,"Problem Statement let $f: \mathbb{E}\mapsto \bar{\mathbb{R}}$ be a convex function, show that if there exists a point $x\in \text{ri}(\text{dom}(f))$ with $f(x)$ taking a finite value, then $f$ must be proper. This is an exercise problem that is highly relevant to Rockafellar's Textbook on Variational Analysis. There was a similar exercise related to improper function, but professor tweaked it. My Take So far this is my take, I don't think it's correct, or rigorous. $\text{epi}(f)$ is convex, then $$
f\left(
\sum_{i = 1}^n \lambda_i x_i
\right) \le \sum_{i = 1}^n \lambda_i f(x_i) \quad \lambda \in \Delta_n, x_i \in \text{dom}(f)
$$ Convexity of Epigraph implies an inequality, relating to the convex combinations of points in the effective domain of the function $f(x)$ . Now consider fixing value of $a \in \text{dom}(f)$ such that $f(a)$ is finite, then ( Not very rigours here ): $$
a = \sum_{i = 1}^n \lambda_i x_i
$$ A can be represented as a convex combination of points in the affective domain of the function. Then: $$
-\infty < f(a) = f\left(
\sum_{i = 1}^n \lambda_i x_i
\right) \le \sum_{i = 1}^n \lambda_i f(x_i)
$$ Therefore, for any $x_i$ , $f(x_i)$ is not $-\infty$ , the function is proper. However, I don't understand why the original statement stated the existence of finite value inside the Relative Interior of the Affective Domain, instead of just the effective domain? I am not sure how many points I should choose to construct the convex combinations to be equal to $a$ , nor I am sure whether is possible. Appreciate it.","['variational-analysis', 'convex-optimization', 'convex-geometry', 'analysis', 'convex-analysis']"
4307604,Is it possible to guess a number from an infinite range with yes or no questions?,"(I hope this qualifies as a math question; it might just be a puzzle.) I've had a thought experiment a few days ago: Assume you have a machine that randomly picks a whole number from 1 to infinity, and you can ask that machine as many yes or no question about the number it picked as you need; can you find out what number it picked? Edit (comments): to clarify, it has to be a whole number, it cannot pick infinity itself. Examples questions could be ""is the number larger/smaller than x?"" , ""is the nth digit x?"", ""is the number prime?"" , etc. My questions are: Is it possible to find out what number it picked (at all)? Is it possible in a finite amount of time? If yes, what could be the most efficient strategy to find out? (asking for ""higher than x"" , going through every single digit one by one, or something else entirely?) My intuition tells me that it's not possible to find out what number it picked, because no matter how large I make ""x"" (to ask ""is the number larger than x?"" ), there are always infinitely many more larger numbers the machine could've picked from. So the chances of me ever finding the upper bound of that number should be basically 0, since it's infinitely more likely to have picked a larger number?","['infinity', 'puzzle', 'probability']"
4307628,"How can I convert ""norms"" using the bijection between $\mathbb{N}$ and $\mathbb{Z}^{d}$?","Suppose $\varphi$ is a sequence $\varphi = \{\varphi(x)\}_{x\in \mathbb{Z}^{d}}$ satisfying the following condition. There exists $k \in \mathbb{N}$ and $C \ge 0$ such that: $$|\varphi(x)| \le C ||x||^{k} \tag{1}\label{1}$$ for every $x \in \mathbb{Z}^{d}$ . Here, $||x|| = \sqrt{x_{1}^{2}+\cdots x_{d}^{2}}$ . Since $\mathbb{Z}^{d}$ is countable, I can actuallt treat $\varphi$ to be a sequence indexed by $\mathbb{N}$ instead of $\mathbb{Z}$ , say, $\varphi = \{\varphi_{n}\}_{n\in \mathbb{N}}$ . This is what I'm trying to prove. Claim 1: There exists $m \in \mathbb{Z}$ such that: $$\sum_{n\in \mathbb{N}}n^{2m}|\varphi_{n}|^{2} < \infty \tag{2}\label{2}.$$ Claim 2: If (\ref{2}) holds, then (\ref{1}) also hold. Of course, I have to use condition (\ref{1}) to get (\ref{2}) and vice-versa, but I don't know how exactly does the $||x||$ is afected by the bijection $T: \mathbb{N} \to \mathbb{Z}^{d}$ . In other words, when $\{\varphi(x)\}_{x\in \mathbb{Z}^{d}}$ becomes $\{\varphi_{n}\}_{n\in \mathbb{N}}$ , how is condition (\ref{1}) changed, so I can use it to prove (\ref{2})?","['real-analysis', 'functional-analysis', 'locally-convex-spaces', 'sequences-and-series', 'convergence-divergence']"
4307633,Prove certain integral equals $0$,"I've been trying to solve some problems from my Partial Differential Equations course. I got the idea to solve this one but there's one step at the end I'm not sure about. It goes like this: Given $$G(x)=\int_{x_0}^xg^*(y)dy,$$ where $g^*$ is a $2l$ -periodic
odd function, and $x_0$ is fixed. Prove $G$ is $2l$ -periodic. The question was a bit more complex in context (about wave equation) but I've simplified it so anyone can understand it (making it just a real analysis problem). So what I need to prove is that $G(x)=G(x+2l)$ for any $x\in\mathbb R$ . I've seen that $$G(x+2l)=\int_{x_0}^{x+2l}g^*(y)dy = \int_{x_0}^xg^*(y)dy + \int_x^{x+2l}g^*(y)dy = G(x) + \int_x^{x+2l}g^*(y)dy,$$ so proving $G(x+2l)=G(x)$ would be equivalent to proving that $$\int_x^{x+2l}g^*(y)dy=0, \text{ }\forall x\in\mathbb R.$$ To prove this, I obviously need to use the fact that $g^*$ is both odd and $2l$ -periodic. I've tried doing: $$\int_x^{x+2l}g^*(y)dy=\int_{-l}^{l}g^*(y+x+l)dy,$$ what would be equal to zero if $g^*(y+x+l)$ was still an odd function (because it's integral limits are $-l$ and $l$ after that change). I'm not sure if changing what's inside the function $g^*$ makes it stop being odd or if it just does not matter. Is it still odd? If it is, is it trivial or do I need to prove it? If it's not, how can I see then that the integral equals zero? Another way to prove it that came to my mind was using that the antiderivative of a periodic function is also a periodic function of the same period, but I'm not sure if it's true (I know the other way it's true, derivative of periodic is periodic, but my intuition tells me that antiderivative of periodic may not necessarily be periodic of the same period, don't know why). Any help, hint or comment will be appreciated, thanks in advance.","['integration', 'even-and-odd-functions', 'periodic-functions', 'analysis', 'real-analysis']"
4307640,Measurability of orbits by continuous actions,"Let $G$ a topological group, $(X,\mathcal{B})$ a measurable space, and $G \curvearrowright X$ a measurable action, that is, the map $G \times B \rightarrow B$ is measurable, when $G \times X$ is endowed with the product $\sigma$ -algebra of the Borel sets on $G$ and $\mathcal{B}$ . Let $K$ be a compact subset of $G$ and $B \in \mathcal{B}$ . Is it true that $KB  \in \mathcal{B}$ ? Can you provide sufficient topological conditions on $G$ , or conditions on the $\sigma$ -algebra $\mathcal{B}$ , such that the statement is true? Thoughts: When trying to produce counterexamples, like in the following attempt, the assumption of measurability of the action looks difficult to prove or disprove. Attempt 1: If $G=X=\mathbb{R}$ , if $G$ is endowed with the usual topology, if $G$ acts on $X$ by translations, if $\mathcal{B}$ is the $\sigma$ -algebra of subsets of $X$ that are either countable or of countable complement, $B := \{0\}$ and $K := [0,1]$ , then $KB \not \in \mathcal{B}$ . However, I don't think the action is measurable: if $a \in X$ , $\{(g,x) \ \vert \ g\cdot x = a\} = \{(x,y) \ \vert \ x+y = a\}$ and I don't believe that a line of slope $-1$ can be in the product $\sigma$ -algebra. Attempt 2: If $X$ is a topological space and $\mathcal{B}$ is the Borel $\sigma$ -algebra, and if both $K$ and $B$ are compact, then $KB$ is a Borel set. We can form the set $\{A \in \mathcal{B} \ \vert \ KB \in \mathcal{B}\}$ and we know that it contains all the compacts subsets of $X$ . It is closed under taking countable unions, but probably not under taking complements.","['measurable-sets', 'general-topology', 'group-actions', 'measure-theory']"
4307650,Why is $\frac{1}{1-x}\circ\frac{1}{1-x}\circ\frac{1}{1-x}=x$ (graphically)?,"Of course, I can evaluate this composition of functions algebraically which is a fairly elementary exercise. But I'm curious as to whether one can find a (simple) graphical or visual argument (likely based on symmetry, perhaps due to representing this as a composition of the functions $1/x$ and $1-x$ ). To this extent, I've had a look at the following cobweb plot: Which represents the composition. But this provides me with no geometric clarity or insight, perhaps because I'm not familiar enough with the geometric properties of hyperbolas? Anyways, if anyone has such an insight or ideas then I'd be grateful.","['group-theory', 'rational-functions', 'algebra-precalculus']"
4307652,Finding number of permutations of order $k$ in $S_n$,"I want to find the number of permutations of order $k$ in $S_n$ . I found this very helpful answer that elaborates specifically how to do it. But I'm having trouble with the step 1 of that answer. As far as I understand, order of a permutation is the least common multiple of lengths of disjoint cycles. So, in symbols, I'd want to write a permutation $\sigma$ as product of disjoint cycles of lengths $r_1, r_2, \ldots, r_m$ such that $\text{lcm}(r_1, r_2, \ldots, r_m)=k$ and $\sum r_i=n$ , so from this I conclude that each cycle length $r_i$ must be a divisor of $k$ . Okay so far so good. So now, I'd want to write down $n$ as a sum of divisors of $k$ such that the $\textrm{lcm}$ of the divisors is $k$ . Now if $n$ is small, I don't think I have a problem because there are only so many divisors or cycle structures possible. But I keep messing up when $n$ is big. I keep missing some partitions of $n$ when I write them down manually and it takes a lot of time. Question: What would be a systematic way to methodically do step 1 without missing any partitions whose $\text{lcm}$ is $k$ in my exams where I have very limited time to solve a question like this? What should be my line of thought while approaching such a question?","['permutations', 'permutation-cycles', 'discrete-mathematics', 'symmetric-groups', 'group-theory']"
4307698,What are the chances of losing a game of hangman?,"Halfway through a game of hangman (the word guessing game) I was playing, I wanted to calculate the chances of losing the game.
I have $15$ letters remaining, and $3$ spaces to fill-up (no letter repeats). I have only $2$ wrong guesses left before losing the game. What is the probability of me losing the game? What I tried: Since I have $3$ spaces but only $2$ wrong guesses, I can either make: $2$ wrong guesses. $$Probability=\frac{12}{15}\times\frac{11}{14}$$ $1$ correct guess followed by $2$ wrong guesses. $$Probability=\frac{3}{15}\times\frac{12}{14}\times\frac{11}{13}$$ $2$ correct guesses followed by $2$ wrong guesses. $$Probability=\frac{3}{15}\times\frac{2}{14}\times\frac{12}{13}\times\frac{11}{12}$$ Wrong, correct, wrong $$Probability=\frac{12}{15}\times\frac{3}{14}\times\frac{11}{13}$$ Wrong, correct, correct, wrong $$Probability=\frac{12}{15}\times\frac{3}{14}\times\frac{2}{13}\times\frac{11}{12}$$ Adding all these individual probabilities must (if I am not wrong) give me the total probability of the event ( i.e., losing)
This gave me the following answer: $Probability=\frac{440}{455}\,\text{or} ~96.7\text{%}$ Is this the correct approach to solve the problem? Am I missing something here?","['recreational-mathematics', 'probability-theory', 'probability']"
4307718,Can this property of sets be detected by the subset relation?,"Consider a set $S$ of sets, and an element $s$ of $S$ . It is very easy to say that $s$ is a maximal element of $S$ under the subset order. However, I am interested in a strictly stronger property. I want to be able to say that $s$ has at least one element that is in no other set of $S$ . This is indeed strictly stronger than just saying $s$ is a maximal element. My question is, is this even possible? That is, in the structure $(S;s,\subseteq)$ , is it possible to characterize that $s$ has that property using a first-order formula?","['elementary-set-theory', 'logic']"
4307777,Is it possible to find $\tan\theta$ by squaring both sides of $\sec\theta+\tan\theta=5$ right away?,"From $\sec\theta+\tan\theta=5$ , in order to find $\tan\theta $ we can rewrite it as $\sec\theta=5-\tan\theta$ then square, $$1+\tan^2\theta=\tan^2\theta-10\tan\theta+25\quad\Rightarrow\quad \tan\theta=\frac{24}{10}$$ But When I square the both sides of $\sec\theta+\tan\theta=5$ right away I get, $$(1+\tan^2\theta)+\tan^2\theta+2\tan\theta\sec\theta=25$$ $$\tan^2\theta+\tan\theta\sec\theta=12$$ I'm wondering is it possible to find $\tan\theta$ from above equation?","['algebra-precalculus', 'trigonometry']"
4307779,Solving a problem in Coordinate Geometry,"Suppose, $2x \cos \alpha - 3y \sin \alpha = 6$ is equation of a variable straight line. From two point $A(\sqrt{5},0$ ) and $B(-\sqrt{5},0)$ foot of the altitude on that straight line is $P$ and $Q$ respectively. Show that the product of the length of two line segment $AP$ and $BQ$ is free of $\alpha$ . This question appeared in my exam today. The way I did it is first constructed the equation of two perpendicular line to $2x \cos \alpha - 3y \sin \alpha = 6$ which goes through the points $A(\sqrt{5},0$ ) and $B(-\sqrt{5},0)$ . In this way, for $A$ and $B$ , I got the following equations respectively- $$3x \sin \alpha + 2y \cos \alpha -3 \sqrt{5}\sin \alpha=0 \qquad(1)$$ $$3x \sin \alpha + 2y \cos \alpha +3 \sqrt{5}\sin \alpha=0 \qquad(2)$$ Then I found that the line $(1)$ intersects the line $2x \cos \alpha - 3y \sin \alpha = 6$ at point $$P\left ( \frac{9\sqrt5 \sin^2 \alpha+12 \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha}, \frac{-18 \sin \alpha+6\sqrt{5} \sin \alpha \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha} \right )$$ and the line $(2)$ intersects the line $2x \cos \alpha - 3y \sin \alpha = 6$ at point $$Q\left ( \frac{-9\sqrt5 \sin^2 \alpha+12 \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha}, \frac{-18 \sin \alpha-6\sqrt{5} \sin \alpha \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha} \right )$$ Now using distance formula $$AP = \sqrt{ \left ( \frac{9\sqrt5 \sin^2 \alpha+12 \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha}- \sqrt5 \right )^2 + \left ( \frac{-18 \sin \alpha+6\sqrt{5} \sin \alpha \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha} \right )^2}$$ and $$ BQ= \sqrt{ \left ( \frac{-9\sqrt5 \sin^2 \alpha+12 \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha} +\sqrt5 \right )^2 + \left ( \frac{-18 \sin \alpha-6\sqrt{5} \sin \alpha \cos \alpha}{4 \cos^2 \alpha + 9 \sin^2 \alpha} \right )^2}$$ Now the multiplication product $AP \cdot BQ$ indeed gives a constant value of $4$ which is free of the arbitrary variable $\alpha$ as you can see here is the simplified version of product of those two quantity. But this is tedious and I do not think this is the only way to do it and an appropriate way to follow in exam with limited time. So, I am looking for an alternative, time saving proof of it. I was wondering if using parametric form would help me, but I think it would get as difficult equally. I created a visualisation for you on desmos to help my problem understand better.","['coordinate-systems', 'analytic-geometry', 'geometry']"
4307790,Gateaux derivatives is a linear continuous operator,"In Clarke's book p.61, it is said that the Gâteaux derivatives $F'(x;v)$ at $x$ in the direction $v$ of $F:X\to Y$ ( $X,Y$ being normed spaces) imply that $v\mapsto F'(x;v)$ is linear continuous. But in the Wikipedia page (paragraph dedicated to Fréchet derivatives), first they say that in general $F'(x;v)$ may fail to be linear or continuous but in the Banach settings $F'(x;v)$ is linear. Does Wikipedia miss the continuity property or the definition of the book is false in infinite dimensional settings ?","['gateaux-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
4307795,Let $\sum_{n=1}^\infty a_n<\infty$. Then $\prod_{n=1}^\infty(1+a_n x)$ is subexponential in $x$.,"In the process of trying to prove an estimate precluding blowup of a PDE, I have come across this problem. Suppose we are given an absolutely convergent sum, $\sum_{n=1}^\infty a_n<\infty$ , with $a_n\ge0$ . I claim that the function $$
F(x) = \prod_{n=1}^\infty (1+a_n x)
$$ is sub-exponential in $x$ , that is, $$
\lim_{x\to+\infty} F(x) e^{-cx}=0
$$ for all $c>0$ . For $a_n$ non-terminating, $F(x)$ is of course super-polynomial. Furthermore, the bound $1+a_n x\le e^{a_n x}$ gives us $$
F(x) = \prod_{n=1}^\infty (1+a_n x)\le e^{\sum_{n=1}^\infty a_n x},\qquad x\ge0
$$ so $F(x)$ grows at most exponentially. However, I have yet to find a better bound on $F(x)$ . Can we prove that $F(x)$ is in fact sub-exponential? As an example of such an $F$ , we consider the identity $$
\prod_{n=1}^\infty \left( 1+\frac{x}{n^2} \right) = \frac{\sinh(\pi\sqrt x)}{\pi\sqrt x}
$$ which can be derived from Euler's infinite product representation of the sine function .","['partial-differential-equations', 'sequences-and-series', 'asymptotics', 'real-analysis']"
4307875,Is this method of showing that $\frac{2n^2}{n^3+3} \rightarrow 0$ correct?,"Given $$
a_n = \frac{2n^2}{n^3+3}
$$ I want to show that $$
a_n \rightarrow 0
$$ My solution proceeds as follows. \begin{align}
\left\lvert \frac{2n^2}{n^3+3} - 0 \right\rvert &< \epsilon \\
\frac{2n^2}{n^3+3} &< \epsilon
\end{align} since \begin{align}
\frac{2n^2}{n^3+3} &< \frac{2n^2}{n^3} \\
&= \frac{2}{n}
\end{align} then \begin{align}
\frac{2}{n} < \epsilon \implies n > \frac{2}{\epsilon}
\end{align} Therefore, let $N = \frac{2}{\epsilon}$ . Fix $\epsilon > 0$ such that $\forall n > N$ , \begin{align}
\left\lvert \frac{2n^2}{n^3+3} - 0 \right\rvert < \frac{2}{n} < \frac{2}{N} = \epsilon
\end{align} I am relying on the fact that since $\lvert a_n - 0 \rvert$ is upper-bounded by $\frac{2}{n}$ , then the $N$ that I find for $\frac{2}{n}$ will be greater than the $N$ that I find for $\lvert a_n - 0 \rvert$ . Is all of this correct? If so, is there a name for this kind of method/reasoning?","['limits', 'solution-verification', 'sequences-and-series']"
4307890,"Does this sequence $\,\sqrt[n]{1+\cos2n}\,$ have a limit?","Does this $\,\lim_{n\to\infty}\sqrt[n]{1+\cos2n}\,$ exist? It took me and my tutor 2 days to find this limit. We believe that this function has no limit or if it does, we are not intellectual enough to find it. Hope someone can help us.","['irrational-numbers', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4307894,How does Wikipedia derive the formula for the Asymptotic Mean Integrated Square Error (AMISE)?,"In this Wikipedia article they show that the Mean Integrated Square Error (MISE) is given by $$\mathrm{MISE}(h)=\mathrm{E}\left[\int(\hat{f}_h(x)-f(x))^2dx\right],$$ where $f(x)$ is the real density function, $\hat{f}_h(x)$ is the estimate given by $$\hat{f}_h(x)=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-x_i}{h}\right),$$ $K$ is the kernel, $x_i$ are sample points, $n$ is the number of samples and $h$ is the bandwidth. They then show that the Asymptotic MISE (AMISE) is given by $$\mathrm{AMISE}(h)=\frac{R(K)}{nh}+\frac{1}{4}m_2(K)^2h^4R(f''),$$ where $$R(g)=\int g(x)^2dx,$$ for a function $g(x)$ , $$m_2(K)=\int x^2K(x)dx$$ and $f''(x)$ is the second derivative of $f(x)$ . My question is how is the formula for $\mathrm{AMISE}$ derived? I understand terms of of order $o\left(\frac{1}{nh} + h^4\right)$ are neglected. But other than that I cannot see how this formula is derived.","['mean-square-error', 'calculus', 'asymptotics', 'statistics']"
4307945,Proving that this is a method of moments estimator for $Var(X)$ for $X\sim Geo(p)$.,"Let $X\sim Geo(p)$ be a random variable, and $X_1,...,X_n$ a random random sample from the distribution of $X.$ Prove that $\frac{1}{2n}\sum^n_{i=1}X_i^2-\frac{1}{2}\bar X$ is an estimator in the methods of moments for $Var(X)$ . Basically what I've tried to do is try to express $Var(X)$ as a function of $E(X)=\mu_1,E(X^2)=\mu_2$ . I did these two attempts: $Var(X)=E(X^2)-[E(X)]^2=\mu_2-\mu_1^2=g_1(\mu_1,\mu_2)$ . $Var(X)=\frac{1-p}{p^2}=\frac{1-\frac{1}{\mu_1}}{\frac{1}{\mu_1^2}}=\mu_1(\mu_1-1)=g_2(\mu_1)$ . Both these functions led to two different estimators. But $g_1$ led to: $\frac{1}{n}\sum^n_{i=1}X_i^2-(\bar X)^2$ . Which somehow looks like the estimator I'm asked to prove. But I got stuck here, I would appreciate any help in how to prove this. Thanks in advance!",['statistics']
4307948,Relation between the dimensions of the fixed point and the submodule given by the augmentation ideal,"Let $k$ be a field with characteristic $p>0$ , let $G$ be a finite $p$ -group of order $n=p^m$ , and let $W$ be a (left) $k[G]$ -module such that the $\dim_k(W)=n$ . If we denote by $I$ the augmentation ideal of $k[G]$ , then we can consider $IW$ and $W^G$ , two $k$ -submodules of $W$ . My question is: Are $\dim_k(W/IW)$ and $\dim_k(W^G)$ equal? Or, if not equal, at least related? My attempts: I have found some specific example that can emerge in number theory (so with Galois extension of $p$ -adic fields) where this works, but the methods are not easily generalisable. In the general case, my first try was to look if $W^G$ and $IW$ are in direct sum, and their sum gives all $W$ , but this is not true: the trace of $x\in W$ is both in $IW$ and $W^G$ , and not necessarily zero. Maybe something cohomological? Something like $W^G=H^0(k[G],W)$ and $W/W=H_0(k[G],W)$ (we are extending the usual group cohomology in the category of (left) $k[G]$ -modules, for example taking the functors Ext and Tor). But even in the classical case, where we take $Z[G]$ instead of $k[G]$ , the result does not hold. (It holds, for example, if $W$ is induced). In general, I did not find useful sequences involving both homology and cohomology (except for the sequence with Tate groups). So I am out of ideas... Remark: if we need this, we can assume that $W=M/PM$ , where $M$ is an $R$ -module, for $R$ a dvr with residue field $R/P=k$ , and that $\dim_k(W^G)=1$ . In this case, it should be true that $\dim_k(W/IW)=1$ , but I cannot see why also in this easier case.","['algebraic-number-theory', 'number-theory', 'modules', 'abstract-algebra', 'noncommutative-algebra']"
4308007,How to prove that a surface whose normal lines intersect a given line is a portion of a surface of revolution,"I'm stuck on Exercise 3.70 from Kristopher Tapp's Differential Topology of Curves and Surfaces : Let $S$ be a connected oriented regular surface. Suppose there exists a line $L \subset \mathbb R^3$ that intersects all normal lines to $S$ (that is, for every $p \in S$ , the trace of the normal line $t \mapsto p + t N(p)$ intersects $L$ ). Prove that $S$ is a portion of a surface of revolution. I was able to show that every point $p \in S$ has a neighbourhood in $S$ whose intersection with the plane perpendicular to $L$ through $p$ is a circle with centre on $L$ (with the help of part (i) in this question: How to solve this questions about regular surfaces? ): Note that we further require $L$ not to intersect $S$ . First, suppose $L$ is the $z$ -axis and let $p = (p_1, p_2, p_3) \in S$ . Then $p + s N(p) = (0, 0, z)$ for some $s \in \mathbb R - \{0\}$ and some $z \in \mathbb R$ , so $N(p) = (-p_1 / s, -p_2 / s, (z - p_3) / s)$ . Note that $\langle(-p_1 / s, -p_2 / s, 0), N(p)\rangle = (p_1^2 + p_2^2) / s^2 \neq 0$ , so $(-p_1 / s, -p_2 / s, 0) \notin T_pS$ . Thus, either $e_1 \notin T_pS$ or $e_2 \notin T_pS$ , and so by Exercise 3.48, there is a neighbourhood $V$ of $p$ in $S$ that is the graph of a smooth function of one of the following forms: $x = f(y, z)$ , or $y = f(x, z)$ , respectively. Define the curve $\gamma: I \to V$ by $\gamma(t) = (f(t, p_3), t, p_3)$ or $\gamma(t) = (t, f(t, p_3), p_3)$ , respectively. Then $\gamma'(t) = (df_{(t, p_3)}(1, 0), 1, 0) \neq 0$ or $\gamma'(t) = (1, df_{(t, p_3)}(1, 0), 0) \neq 0$ , and $\gamma(I) \subset p + \operatorname{span}\{e_1, e_2\}$ . Conversely, if $(x, y, p_3) \in V$ , then $x = f(y, p_3)$ or $y = f(x, p_3)$ , and so $V \cap (p + \operatorname{span}\{e_1, e_2\}) \subset \gamma(I)$ . We claim $\gamma$ is a portion of the circle centred at $(0, 0, p_3)$ through $p$ . For each $t \in I$ , let $\mathfrak n(t)$ denote the projection of $N(\gamma(t))$ onto $\operatorname{span}\{e_1, e_2\}$ . Then $$\langle\mathfrak n(t), \gamma'(t)\rangle = \langle N(\gamma(t)), \gamma'(t)\rangle - \langle N(\gamma(t)), e_3\rangle \langle e_3, \gamma'(t)\rangle = 0$$ and $\gamma(t) - (0, 0, p_3) = -\frac{\lvert\gamma(t) - (0, 0, p_3)\rvert}{\lvert\mathfrak n(t)\rvert} \mathfrak n(t)$ for all $t \in I$ . Thus, $$\frac{d}{dt}\langle\gamma(t) - (0, 0, p_3), \gamma(t) - (0, 0, p_3)\rangle = -\frac{2 \lvert\gamma(t) - (0, 0, p_3)\rvert}{\lvert\mathfrak n(t)\rvert} \langle\gamma'(t), \mathfrak n(t)\rangle = 0,$$ and so $\lvert\gamma(t) - (0, 0, p_3)\rvert$ is constant. What I'm having trouble showing is that there's a single curve whose surface of revolution about the line $L$ contains $S$ , i.e. that if two points of the surface are in the same plane perpendicular to $L$ , then they're at the same distance from $L$ (the question Surfaces of revolution - Problem does not address this); I'm pretty sure I need to use the fact that $S$ is connected, and hence path connected (I'm pretty sure I can show it is in fact smoothly path connected, if needed), but I can't seem to use a path between two points to obtain the single curve I need. Given two points $p = (p_1, p_2, p_3), q = (q_1, q_2, q_3) \in S$ with $p_3 = q_3$ (still assuming $L$ is the $z$ -axis), I believe I want to show that $p_1^2 + p_2^2 = q_1^2 + q_2^2$ , or perhaps find a way to write $p_1^2 + p_2^2$ as a function of $p_3$ . I've tried using the fact that there are functions $s: S \to \mathbb R - \{0\}, z: S \to \mathbb R$ such that $p + s(p) N(p) = (0, 0, z(p))$ and manipulating this equation to do the latter, and I've tried introducing a path (or curve, if needed) $\beta: [0, 1] \to S$ from $p$ to $q$ and using the fact that $\beta'(t) \in T_{\beta(t)}S$ to do the former, but haven't found anything that I can tell is useful. Another thing I've thought to try after the discussion in the comments is to show that if there are points $p = (p_1, p_2, p_3), q = (q_1, q_2, q_3) \in S$ with $p_3 = q_3$ at different distances from the $z$ -axis, then there is a point $\beta(t)$ on the curve from $p$ to $q$ (which would need to be regular, which I think I could guarantee by taking a piecewise-regular path and rounding off the corners in a neighbourhood thereof) with $\beta'(t)$ horizontal such that $T_{\beta(t)}S$ is horizontal, but I don't really see how to show this. By the first part of the proof, I know that one basis vector in $T_{\beta(t)}S$ is the tangent vector to the horizontal circle through $\beta(t)$ , and presumably I need to somehow use the fact that $p, q$ are at different distances from the $z$ -axis to show that $\beta'(t)$ is linearly independent of that vector, but I don't see how.","['surfaces', 'differential-geometry']"
4308023,Uniformly most powerful test for $H_0: \lambda \ge \lambda_0$,"I cannot find anywhere answer to this question. Let's take exponential distribution i.e. $$f(x) = \lambda e^{-\lambda x}\mathbb{1}_{(0, +\infty)}(x)$$ I want to derive uniformly most powerful test for hypothesis: $H_0:\lambda\ge\lambda_0$ and $H_1:\lambda<\lambda_0$ . How I would start with this problem is to take two $\lambda_1 \ge\lambda_0$ and $\lambda_2 < \lambda_0$ and then start to write: $$\frac{L(x, \lambda_2)}{L(x, \lambda_1)} = \frac{\lambda_2^ne^{-\lambda_2(\sum_{i=1}^nx_i)}}{\lambda_1^ne^{-\lambda_1(\sum_{i=1}^nx_i)}}$$ Did I start to write correct likelihood proportion for my problem?","['statistics', 'probability', 'hypothesis-testing']"
4308058,Is a graph determined by its multiset of spanning trees?,"Is it known whether a (connected) graph is determined by its multiset of spanning trees? In other words, do any two non-isomorphic connected graphs have different multisets of spanning trees? By a multiset I mean the set of spanning trees (up to isomorphism) and the counts of how many times each occurs. I cannot find counterexamples or positive results on this. I see that a previous post has asked this question without considering the counts of each spanning tree.","['graph-theory', 'graph-isomorphism', 'combinatorics']"
4308077,The sum of the cube of the first n odd numbers,"Prove that The sum of the cube of the first n odd numbers = $n^2(2n^2-1)$ That is $$\sum_{i=1}^n (2i-1)^3 = n^2(2n^2-1)$$ First I tried to solve it by induction: $Basis-step:$ For $n=1$ we have $$\sum_{i=1}^n (2i-1)^3 = 1$$ and $$n^2(2n^2-1) = 1$$ $Induction-step:$ Let $n 	\in \mathbb{N},$ $$Assume  \sum_{i=1}^n (2i-1)^3 = n^2(2n^2-1)$$ $$Prove \sum_{i=1}^{n+1} (2i-1)^3 = (n+1)^2(2(n+1)^2-1)$$ I know that $$\sum_{i=1}^{n+1} (2i-1)^3 =  \sum_{i=1}^n (2i-1)^3 + (2(n+1)-1)^3 $$ But i was not able to reach a solution.
If anyone can give a small hint, that would be helpful.","['summation', 'solution-verification', 'induction', 'discrete-mathematics']"
4308088,Does a homeomorphism between metric spaces map open balls to open balls?,"A homeomorphism $h: (X,d_1) \rightarrow (Y,d_2)$ is a bijection between two metric spaces $(X,d_1)$ and $(Y,d_2)$ such that $h$ maps open balls of $X$ to the interior of open balls of $Y$ and $h^{-1}$ maps open balls of $Y$ to the interior of open balls of $X$ . It is also true that $h$ maps open sets of $X$ to open sets of $Y$ , and $h^{-1}$ maps open sets of $Y$ to open sets of $X$ . The open sets of metric spaces are either open balls or unions of open balls. Is it true that, for a homeomorphism, the image of an open ball of $X$ must be an open ball of $Y$ ? Does the answer change if both metric spaces are compact? COMMENT: The convoluted counter-scenario I'm imagining is that $h$ could map an open ball of $X$ into a union of some open balls of $Y$ that isn't itself an open ball, but is contained within some other open ball in $Y$ .","['general-topology', 'open-map', 'metric-spaces', 'compactness']"
4308105,Prove inequality using Lagrange's remainder,"My problem is the following: Prove that $\left | e^{x}+e^{-x}-2-x^{2} \right |\leq \frac{1}{6}x^{4}$ when $\left | x \right|\leq1$ . Using Maclaurin's expansion formula, I get that $$e^{x}=1+x+\frac{x^{2}}{2}+\frac{x^{3}}{6}+x^{4}\frac{e^{\Theta_1x}}{24},0\leq\Theta_1\leq1$$ $$e^{-x}=1-x+\frac{x^{2}}{2}-\frac{x^{3}}{6}+x^{4}\frac{e^{-\Theta_2x}}{24},0\leq\Theta_2\leq1$$ I can now simplify the inequality to: $$\left | x^{4}\frac{e^{\Theta_1x}}{24}+x^{4}\frac{e^{-\Theta_2x}}{24} \right |=\frac{x^{4}}{24}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq \frac{1}{6}x^{4}\Leftrightarrow\frac{x^{4}}{4}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq x^{4}$$ Aaaand here I'm pretty much stuck. What am I supposed to do here? I guess I have to find something in between my LHS and my RHS and then prove that that thing is less than my RHS. Thankful for help!","['absolute-value', 'analysis', 'calculus', 'taylor-expansion', 'inequality']"
4308109,"Inequality on the unit cube $[0, 1]^d$","I was trying to solve prove the following inequality: For all compactly supported smooth functions on the $d$ -dimensional
unit cube $u\in C^\infty([0,1]^d)$ (where if $\Omega\subset\mathbb{R}^d$ is open, we define $C^\infty(\overline{\Omega})=\{u\in C^\infty(\Omega)\mid u \text{ has
> a continuous extension to } \overline{\Omega}\}$ ), we have $$\int_{\Omega}|u|^{2} d x \leq \alpha \int_{\Omega} \operatorname{dist}(x, \partial \Omega)^{2}
\left(|u|^{2}+\|\nabla u\|^{2}
\right) d x,
$$ where $\Omega=(0,1)^d$ and if $A, B\subset\mathbb{R}^d$ , then $\operatorname{dist}(A,B)=\inf_{a\in A,b\in B}d(a,b)$ . To prove it, I thought I better study how the function $\operatorname{dist}(x,\partial\Omega)$ behaves on $\Omega$ . For that, I determined the following open subsets of $\Omega$ : for each $1\leq i\leq d$ , $$
U_i^+=\{x\in\Omega:x_i>\max\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\},\\
U_i^-=\{x\in\Omega:x_i<\min\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\}.
$$ These sets (which on $d=1$ are just $(0,1/2)$ and $(1/2,0)$ , on $d=2$ can be seen to be some triangles, and on $d=3$ some piramids) satisfy the property $$
\begin{align}
x\in U_i^+&\Rightarrow d(x,\partial\Omega)=1-x_i,\\
x\in U_i^-&\Rightarrow d(x,\partial\Omega)=x_i.
\end{align}
$$ Furthermore, if $\Omega'=\bigcup_{i=1}^d U_i^+\cup U_i^-$ , then $\Omega\setminus\Omega'$ is a null set (i.e., the $d$ -dimensional Lebesgue measure equals zero). From here, I was thinking of using some partition of unity which involved the $U_i^\pm$ 's to ""break down"" the integral in different manageable parts. But I could not find out exactly which and how to apply it. I was even trying to prove it on $d=1$ but I could not get the proof to work. Trying to get the bound taking a compactly supported function $\theta$ on $(0,1)$ with values $0\leq\theta\leq 1$ and which evaluates $1$ on $[1/n,1/2-1/n]\cup[1/2+1/n,1-1/n]$ and trying to bound $\int_0^1\theta udx$ . But I could not control the value of the derivative of $\theta$ . Any thoughts on the problem will be appreciated :)","['smooth-functions', 'functional-analysis', 'upper-lower-bounds', 'real-analysis']"
4308112,Can the Vitali Convergence Theorem hold even where the sequence of functions isn't integrable?,"$\newcommand{\d}{\,\mathrm{d}}$ I here use Royden's definitions of equiintegrability, and tightness: A family of measurable functions $\{f_n\}_{n\in\Bbb N}:X\to\Bbb R$ , where $(X,\mathcal{A},\mu)$ is a measure space, is said to be equiintegrable if: $$\forall\varepsilon\gt0,\,\exists\delta\gt0:\forall A\in\mathcal{A},\,\forall n,\,\mu(A)\lt\delta\implies\int_A|f_n|\d\mu\lt\varepsilon$$ The family is said to be tight if: $$\forall\varepsilon\gt0,\,\exists E\in\mathcal{A},\,\mu(E)\lt\infty:\forall n,\,\int_{X\setminus E}|f_n|\d\mu\lt\varepsilon$$ With that out of the way, and paraphrasing the Wikipedia article to account for my different nomenclature (I use Royden's so as to be able to follow my own notes as I type this), I find that Wikipedia formally requires the family $\{f_n\}$ to be a subset of $\mathcal{L}^p(X)$ , yet the proof I have here never requires the $f_n$ to be integrable; for the sake of rigour, I would dearly like to know if it is required that $\{f_n\}\subset\mathcal{L}^p(X)$ for $\lim_{n\to\infty}\|f-f_n\|_{\mathcal{L}^p}=0$ . Vitali's Convergence Theorem, Finite Case (one possible statement - I have seen many slight variants): Let $\{f_n:n\in\Bbb N\}\subset\mathcal{L}^p(X)$ where $(X,\mathcal{A},\mu)$ is a finite measure space. Suppose $(f_n)$ has a pointwise limit $f$ ; if $\{f_n\}$ is an equiintegrable family, then $f\in\mathcal{L}^p$ and $\lim_{n\to\infty}\int_Xf_n\d\mu=\int_Xf\d\mu$ . And ""my"" proof (a combination of Royden's less general work and what I have scrounged from other stack posts): As this proof immediately generalises if $p\neq1$ , w.l.o.g take $p=1$ . As $\mu(X)\lt\infty$ , Egorov's theorem ensures that $\forall\delta\gt0,\,\exists E\in\mathcal{A}:\mu(E)\lt\delta,\,(f_n)|_{X\setminus E}\rightrightarrows f|_{X\setminus E}$ . The $\mu$ -finiteness of $X$ also gives that $X\setminus E$ is of finite measure, hence, by uniform convergence: $$\forall\varepsilon\gt0,\,\exists\delta\gt0,\,\int_{X\setminus E}|f-f_n|\d\mu\lt\frac{\varepsilon}{3}$$ Where $E$ is as before and all $n\gt N$ are suitably large, and $\delta$ is chosen suitably small. Fatou's Lemma combined with the equiintegrability of $\{f_n\}$ shows that, if $\delta$ is suitably small: $$\int_E|f|\d\mu=\int_E\liminf_{n\to\infty}|f_n|\d\mu\le\liminf_{n\to\infty}\int_E|f_n|\d\mu\lt\frac{\varepsilon}{3}$$ Since equiintegrability requires that if $\mu(E)\lt\delta,\int_E|f_n|\d\mu$ may be made as small as desired, with the same $\delta$ for all $n$ .
Taking $\delta$ to be the smaller of the two used above, and for $n$ as large as before, and $E$ as before, then one has: $$\begin{align}\forall\varepsilon\gt0,\,\forall n\gt N:\int_X|f-f_n|\d\mu&\le\int_E|f-f_n|\d\mu+\int_{X\setminus E}|f-f_n|\d\mu\\&\le\int_E|f|\d\mu+\int_E|f_n|\d\mu+\int_{X\setminus E}|f-f_n|\d\mu\\&\lt\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon\end{align}$$ And therefore $f_n$ converges to $f$ in $\mathcal{L}^p$ norm. One deduces that $f\in\mathcal{L}^p(X)$ by a similar argument, that necessitates the integrability of $\{f_n\}$ . However, I am fairly sure that one has convergence in $\mathcal{L}^p$ norm anyway, regardless of whether or not $\{f_n\}\subset\mathcal{L}^p$ , since Egorov's theorem doesn't require integrability, and I don't believe any of my steps did either. Using the above definition of tightness, if one requires that $\{f_n\}$ is also tight then the requirement that $\mu(X)\lt\infty$ may be dropped; moreover, in this case, I believe one has that $f$ is $p$ -integrable and that $f_n\to f$ in norm regardless of whether or not $f_n$ are themselves $p$ -integrable. I will add the proof I have if this statement requires expanding. The question: The formal statements require $\{f_n\}$ to be integrable; I believe that to get the convergence-in-norm conclusion, one may drop this requirement, and if one replaces it with $\{f_n\}$ must be tight, one gets all conclusions independent of the integrability of each $f_n$ . Is this right?","['measure-theory', 'solution-verification', 'lebesgue-integral']"
4308119,"""Second"" Lie derivative?","Many of us are familiar with the standard definition of the Lie derivative of some smooth function $\varphi \in \Omega^{0}(X)$ as $$
\frac{d}{dt}(f_{t}^{*}\varphi) \big|_{t=0} =: L_{V}\varphi,
$$ where $f_{t}$ is the flow generated by the vector field $V$ . My question is given this , can we compute $$
\frac{d^{2}}{dt^{2}}(f_{t}^{*}\varphi) \big|_{t=0}
$$ in terms of $L_{V}\varphi$ ? My naive hunch is that this would simply be $L_{V}(L_{V}\varphi)$ , but I'm cautious about this and haven't found a convincing way of proving it. Is this true? Moreover, I wonder if whatever we compute this ""second derivative"" to be, should it hold not only for $\varphi \in \Omega^{0}(X)$ but for any $\omega \in \Omega^{k}(X)$ , $0 \leq k \leq \mathrm{dim}(X)$ ? That would be even nicer. Thanks in advance to anyone who thinks a bit about this and shares any ideas.","['differential-topology', 'lie-derivative', 'differential-geometry']"
4308123,Show whether $\sum_{k=1}^{\infty} \frac{3^k+k^2}{4^k +1}$ converges or diverges,"Here's the problem to be solved: Show whether $\sum_{k=1}^{\infty} \frac{3^k+k^2}{4^k +1}$ converges or diverges My attempt $\sum_{k=1}^{\infty} \frac{3^k+k^2}{4^k +1} \leq \sum_{k=1}^{\infty} \frac{3^k+k^2}{4^k} = \sum_{k=1}^{\infty} (3/4)^k + \sum_{k=1}^{\infty} \frac{k^2}{4^k}$ The first term in the last equality converges, however, I'm struggling to show whether $\sum_{k=1}^{\infty} \frac{k^2}{4^k}$ converges or diverges. My hypothesis is that it converges, since $4^k$ is growing much faster than $k^2$ , so I somehow must show that it's less than or equal to something else. I'd be glad if you could share any tips. Thanks.","['calculus', 'sequences-and-series', 'real-analysis']"
4308224,convert hyperbola in rectangular form to polar form,"i am trying to convert the rectangular equation of a conic (hyperbola) to a polar form. the rectangular equation is: $$3y^2 - 16y -x^2 + 16 = 0.$$ i substituted $r\sin\theta$ for y and $r\cos\theta$ for x, and tried to simplify, but I am stuck. i tried substituting $1-\sin^2\theta$ for the resulting $\cos^2\theta$ term, and ended up with the following expression after some manipulation: $$r^2(2\sin\theta-1)(2\sin\theta+1)-16r\sin\theta + 16 = 0.$$ the solution to the question is supposed to be: $$r = \frac{4}{1 + 2\sin\theta}.$$ I just can't figure out how to get from where I am to the expected solution.  Perhaps I made the wrong substitution? I just need to figure out the intermediate steps to get from the original rectangular equation to the final equation in polar form given above. Hope someone can help me -- thanks!","['coordinate-systems', 'trigonometry', 'conic-sections', 'polar-coordinates']"
4308250,Proving the inverse relationship between exponential function and natural logarithm,"I am working to prove that $E(x) = \displaystyle\sum_{n=0}^\infty \frac{x^n}{n!}$ and $L(x) = \displaystyle\int_1^x\frac{1}{t}dt $ are inverse functions to one another. I have already found that $(L \circ E)(x) = x$ for all $x \in \mathbb{R}$ , however I am struggling to prove that $(E \circ L)(x)=x$ for all $x > 0$ . My work for $L \circ E$ involved finding $(L \circ E)'(x) = 1$ to imply my result. Any tips how to proceed for this $(E \circ L)$ result? Thanks!","['integration', 'analysis', 'real-analysis', 'calculus', 'sequences-and-series']"
4308256,Expected number of trials until all 3 (independent?) events happened,"Suppose there are three boxes, each has $9$ white balls and $1$ colored ball: Box $1$ : $9$ white and $1$ red. Box $2$ : $9$ white and $1$ green. Box $3$ : $9$ white and $1$ blue. Now I am drawing balls from the boxes with replacement. In each trial, I draw $1$ ball from each of the three boxes. If any of them is red, I increase my red ball counter by $1$ , then I put all balls back to their respective boxes. Similarly, I increase the corresponding counter if I get a green ball or a blue ball. The trials end only if my counters for all red, green and blue balls are at least $1$ (i.e., I get balls of each color at least once over all the trials). The question is, how do I calculate the expected number of trials until I hit the end condition? I know, if I only have $1$ box, then it is a simple geometric distribution, with $p=\frac{1}{10}$ , and hence the expected number of trials will be $10$ . But now, although the three events (get a red ball, get a green ball and get a blue ball) are independent, the combined event is not. And also since it is possible to have more than one event happen in each trial (e.g., getting red from box $1$ and green from box $2$ in the same trial), I cannot consider the events to happen one after another. I ran a quick simulation and it seems like the expected number of trials is around $17.9$ . I would like to know if there is a mathematical way to justify this number.","['expected-value', 'conditional-probability', 'probability']"
4308271,Verify that $I$ and $X$ are negatively correlated.,"In certain situations, a random variable $X$ with known mean is simulated to
obtain an estimate of $P(X \leq a)$ for some constant given $a$ . The simple estimator of a simulation for a run is $I = I (X \leq a)$ . Verify that $I$ and $X$ are negatively correlated. My approach: By definition $$Corr(X,I)=\frac{Cov(X,I)}{\sqrt{Var(X)Var(Y)}}$$ Then the only chance for the correlation to be negative is when covariance is negative. $$Cov(X,I)=E[XI]-E[X]E[I]$$ I'm not sure on how to interpret the random variable $I$ and its expected value. Any suggestion would be great!!","['statistics', 'simulation', 'parameter-estimation', 'correlation', 'probability']"
4308293,Fundamental theorem of double integral,"Suppose $R$ is a ""reasonable"" region of $\mathbb{R}^2$ and $f(x,y)$ is a continuous function on $R$ . Then the double integral $\int\int_R f(x,y) dA$ exists (this is the usual limit of double sum as $\max{\Delta x_i}, \max{\Delta y_i} \to 0$ , in particular, the order of limit does not matter). Now suppose, in addition, that the region $R = \{(x,y) | a \le x \le b, g_1(x) \le y \le g_2(x) \}$ where $g_1(x), g_2(x)$ are differentiable functions when $x \in [a,b]$ . Then the double integral $\int \int_R f(x,y)dA = \int_{a}^{b} \int_{g_1(x)}^{g_2(x)} f(x,y) dydx$ . Is this theorem statement correct?","['multivariable-calculus', 'analysis', 'real-analysis']"
4308316,"Expected value of a ratio. To exist or not to exist, that is the question.","I am given the question. Suppose X and Y are iid uniform random variables on the interval (-2,2). Let $Z=\frac{Y}{X}$ .
Does the expectation of Z exist? If it exists, calculate $\mathbb{E}[Z]$ . If it does not exist, explain why. I have 2 different interpretations of this questions and I don't know which one or if any is correct. 1 way I see this is once we calculate the pdf of Z. We can use it to calculate the expected value as, $$\mathbb{E}[Z]=\int zf_{Z}(z)dz$$ But, if we look at as, $$\mathbb{E}[Z]=\mathbb{E}\left[\frac{Y}{X}\right]$$ Since X and Y are independent, $$\mathbb{E}\left[\frac{Y}{X}\right]=E[Y]\cdot\mathbb{E}\left[\frac{1}{X}\right]$$ But, the expected value of 1/X is $$\mathbb{E}\left[\frac{1}{X}\right]=\int_{-2}^2 \frac{1}{x}f_{X}(x)dx$$ This is a divergent integral and thus the expected value is not possible to calculate. I don't know which interpretation if either is correct.","['expected-value', 'statistics']"
4308370,Asymptotic behavior of $\sum_{r=0}^\infty r\left[ \left(1-\frac 1{2^{r+1}}\right)^m-\left(1-\frac 1{2^{r}}\right)^m\right]$ as $m\to+\infty$.,"What is the asymptotic behavior of the following series as $m\to+\infty$ ? $$S_m=\sum_{r=0}^\infty r\left[ \left(1-\frac 1{2^{r+1}}\right)^m-\left(1-\frac 1{2^{r}}\right)^m\right],\qquad m\in\mathbb N_+.$$ This problem comes from a friend of mine majoring in engineering science, who did some numerical simulations and told me it is likely to approach $\log_2m$ . After expanding the $(1-a)^m$ using Binomial formula and calculating the series $\sum_{r=0}^\infty rx^r$ , I convert the above series to the following finite summation: $$S_m=\sum_{k=1}^m {m\choose k}\frac{(-1)^{k-1}}{2^k-1},\qquad m\in\mathbb N_+. $$ Is there any method to see the asymptotic behavior of the above summation as $m\to+\infty$ ? I not only want to know whether it approches $\log_2m$ , but also want to know the convergence rate. So it will be better to write down its asymptotic expansion at $m\to+\infty$ . Any help will be appreciated!","['calculus', 'asymptotics', 'analysis', 'sequences-and-series']"
4308415,Prove that $\ \frac{z_1^k + z_2^k + \cdots + z_n^k}{n} = \left(\frac{z_1 + z_2 + \cdots + z_n}{n}\right)^k$,"If $z_{1},z_{2},z_{3},...,z_{n}$ be the n-vertices of a regular n-sided polygon on a complex plane, then prove that $$\ \frac{z_1^k + z_2^k + \cdots + z_n^k}{n} = \left(\frac{z_1 + z_2 + \cdots + z_n}{n}\right)^k$$ where $k\in N, k<n$ . Further, if centre of the polygon is $z_0$ then prove that $$\ \sum_{r=1}^nz^k_r=nz^k_0$$ My Attempt $z_2-z_0=(z_1-z_0)e^{\frac{i2\pi}{n}}$ $z_3-z_0=(z_1-z_0)e^{\frac{i4\pi}{n}}$ $z_4-z_0=(z_1-z_0)e^{\frac{i6\pi}{n}}$ ... But what to do about things like $z^k$","['roots-of-unity', 'algebra-precalculus', 'geometry', 'complex-numbers']"
4308427,Radon-Nikodym derivative and derivative of functions of real variable,"I have read Radon–Nikodym derivative and ""normal"" derivative but I am still quite confused. Here are the two kinds of derivative and I am looking for the analog: The derivative of a function of real variables measures the change of function value w.r.t. its input. The $\mu$ -integrable function $f=d\nu/d\mu$ is called the Radon-Nikodym derivative whenever $\nu\ll\mu$ . My idea is that the absolute continuity gives us a hint of the change of value $\nu$ w.r.t. $\mu$ . Is it correct?","['measure-theory', 'derivatives', 'radon-nikodym']"
