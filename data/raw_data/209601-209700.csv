question_id,title,body,tags
4208196,Which space do we obtain if we take a M√∂bius strip and identify its boundary circle to a point?,"I know that the boundary circle of a M√∂bius strip is actually formed by the horizontal sides of $[0 ,1] \times [0,1]$ .If we identify all the points of the 1st horizontal side to a single point and do the same for the second horizontal side we get a disc with the antipodal points identified to itself(which is $\mathbb{RP}^2$ ). However I am not sure of how to write it down rigorously. $\require{AMScd}$ \begin{CD}
    Mobius strip  @>i(x)>> [0,1] \times[0,1] @>g(x)>> D  @>f(x)>> D/\sim_1\\
     @VV\pi(x) = cl(x)V \\
    Mobius Strip / \sim  \\
\end{CD} where $\sim$ denotes identifying the boundary cirlce  of a Mobius strip and $\sim_1$ denotes identifying the opposite points of the disc. I am not sure of what $g(x)$ is ? or the fact that mobius strip will be compact or the composition of $f \circ g \circ i(x)$ will be onto? Any other method will be appreciated but I wanted to know whether the mobius strip is compact?","['non-orientable-surfaces', 'general-topology', 'mobius-band']"
4208197,Conjugacy classes of finite group G having size at most 4 implies G is solvable.,"I am stuck on the following question and am looking for help/ hints: Question: Show that if the conjugacy classes of a finite group $G$ have size at most 4, then $G$ is solvable. (I have not been able to find anything on stackexchange that is the same or a similar question). Context : This is a past paper question for an algebra preliminary exam I am studying for, not a homework question. Apologies if this is still against the rules. The sequence associated with the exam uses Dummit-Foote as the main textbook to give some idea of the level the questions are aimed at: I have not taken the sequence and did not learn solvable groups (or composition/ normal series) in undergrad, but have read the relevant parts of Dummit-Foote and Aluffi's Chapter 0 and tried some problems from there (I much prefer the style of exposition in the latter). So I'm quite new to the topic of solvable groups and not at all comfortable/ proficient with it yet. My thoughts/ attempt : I have mostly been trying to work with the derived series definition, as I didn't get very far with trying to come up with an abelian/ cyclic series for $G$ . I've showed that conjugacy classes are identified in the abelianization, but this just gives me that $|G/[G,G]| \leq$ (number of conjugacy classes of $G$ ), but the hypothesis of the question just implies that the number of conjucagy classes is at least $|G|/4$ , which is unhelpful. Abelian groups are solvable but they are their abelianization, so the only a priori bound on the number of conjugacy classes is $|G|$ , so I don't think this approach has gotten me anywhere. I am mainly stuck on firstly which characterisation of solvable groups to use, and secondly how to use the bound given in the hypothesis of the question.  I also noted that the hypothesis can be restated as: $ [ G \colon C_{G}(g) ] \leq 4 $ for any $ g \in G $ . But $C_{G}(g)$ is not a normal subgroup a priori so I can't quotient by it (it would be if its index were bounded by 2, however). Apologies for the length, this is my first post and I wanted to provide context for my level and the level the question is aimed at, as well as showing that I've thought and attempted the question (but am quite lost at the moment). I tried to follow the guidelines for asking questions as much as possible. For questions on solvable groups I am feeling a bit overwhelmed by all the different approaches one can take: derived series terminating in the identity, showing $G$ has a cyclic (or merely abelian) series, finding a solvable normal subgroup $N$ such that $G/N$ is solvable... and I always find it hard to see which approach will be most viable when starting out on a problem. At this stage I would appreciate hints etc. to point me in the right direction and get me unstuck, and not a full answer, as working out some of it on my own would be more beneficial to me. EDIT : Thanks to the helpful hint from @DerekHolt I've managed to work out the solution so will now accept answers (if none come I will type one up when I have time) so that this can be marked as answered.","['group-theory', 'abstract-algebra', 'finite-groups', 'solvable-groups']"
4208222,Fenchel Dual of indicator function,"It is well known that if I have the indicator function $$\iota_S(x)=\cases{ 0 \text{ if } x \in S \\ +\infty \text{ else} }$$ of a convex set $S$ , then this is a convex functional and its Fenchel dual is the support function $$\sigma_S(y) = \sup_{x\in S} \langle y,x \rangle .$$ One can then use this in convex optimisations problems to find the dual of the problem itself. So, if I have $$\min_{x\in C} \psi(x) \text{ s.t.} Ax=b,$$ then I can rewrite the problem as $\min_x \psi(x) + \iota_C(x) + \iota_S(Ax)$ , where $\iota_s(x) = \cases {0 \text{ if } x = b,\\ +\infty \text{ else}}$ and compute the dual of my problem, using Fenchel-Rockafeller duality as $\sup_y -\sigma_S(-y) - (\psi+\iota_c)^\star(A^\star y).$ In this simple case, it's easy to see that $-\sigma_S(-y)= \inf_{x\in S} \langle y,x \rangle = \langle y,b \rangle $ (as $x \in S \iff x = b$ ). My question then is: what happens if $S$ , instead of being $S=\{ x : x = b\}$ is a set denoting inequality constraints? For instance, $S=\{ x: x \geq \gamma\}$ ? Is there a standard/straight-forward way of characteristing the Fenchel dual? More precisely, if I have a problem $\min_{x\in C} \psi(x) \text{ s.t } Ax \geq \gamma$ what would the dual be? If our space were the space of Radon Measures and $A$ were a linear operator (say, the integral of the measure with respect to a fixed measurable function $g$ ) how would the constraint that $\int dQ g \geq \gamma$ impact the dual? Intuitively, the new $-\sigma_S(-u)$ would be $\inf_{Q\in S} \int u dQ$ . If $u=g$ then clearly this infimum can be $\gamma$ , otherwise it is $-\infty$ . But, how do I reason about all the possible measures and functionals I can plug in this infimum?
If $u = \lambda g$ with $\lambda >0$ , is it then true that $\inf_{Q\in S}\int u dQ = \inf_{\lambda>0} \lambda \gamma$ ? Is it arbitrarily small then? What about other (non-linear) transformations of $g$ ? Thank you for any suggestion/reference you might provide me!","['convex-optimization', 'measure-theory', 'duality-theorems', 'functional-analysis']"
4208225,surface measure and Gauss-Green theorem proof,"The famous Gauss-Green theorem states the follows.
Let $\Omega$ be a bounded open set of $\mathbb{R}^n$ and $\partial \Omega$ be $C^1$ boundary. Then  for $f,g \in C^1(\overline{\Omega}),$ the following holds: \begin{eqnarray}
\int\limits_{\Omega}f_{x_i}g dx =-\int\limits_{\Omega}fg_{x_i} dx + \int\limits_{\partial \Omega} fg dS \quad \text{for } i=1,2,\ldots,n.
\end{eqnarray} where $dS$ is the surface measure on $\partial \Omega$ . How to prove this result in a rigorous measure-theoretic setup?
What is the underlying $\sigma-$ algebra and measure on $\partial \Omega$ ? P.S.: Detailed answer or a reference that contains all these details is highly appreciated.","['integration', 'measure-theory', 'analysis', 'partial-differential-equations']"
4208227,"Let $K$ be an $o$-symmetric body, that covers $X$. Does it imply $\operatorname{conv}(X)\subset X+K$?","The question is inspired by the answer (and a comment) of daw at Covering of a polytope by balls with midpoints at the vertices Let $K\subset \mathbb R^n$ be an $o$ -symmetric ( $K=-K$ ) convex body. Does $X\subset K$ imply $\operatorname{conv}(X)\subset X+K$ ? We already know, that it's true, if $K$ is an euclidean ball or ellipsoid.","['polytopes', 'convex-geometry', 'geometry', 'linear-algebra', 'discrete-mathematics']"
4208234,On the 'wrong proof' of the chain rule,"I am looking through an old analysis course that I had and I was pondering a bit about the proof of chain rule (especially the notorious wrong proof that you can give). I'd be happy if someone was willing to verify my reasoning below. I end with an actual question. Let's start with the following nice result. Let $f\colon \mathbb{R}\to \mathbb{R}$ be a continuous function which is differentiable on $\mathbb{R}_0$ . Assume that $\lim_{x\to 0}f'(x)=L\in \mathbb{R}$ . Then $f$ is differentiable in $0$ . Proof: For each $h\neq 0$ , the mean value theorem yields a $c_h\in \mathbb{R}$ strictly between $h$ and $0$ such that $f'(c_h)=\frac{f(h)-f(0)}{h}$ . Letting $h\to 0$ , is it is obvious that $c_h\to 0$ as each $|c_h|<|h|$ . Hence $$\lim_{h\to 0}\frac{f(h)-f(0)}{h}=\lim_{h\to 0}f'(c_h)=L.$$ $\square$ Great, let's apply this to the following function: $$\phi\colon \mathbb{R}\to \mathbb{R}:x\mapsto \begin{cases}x^3\sin(\frac{1}{x}) & \mbox{ if }x\neq 0,\\0 & \mbox{ if } x=0.\end{cases}$$ Clearly $\phi$ is differentiable on $\mathbb{R}_0$ and $$\phi'(x)=3x^2\sin(\frac{1}{x})-x^3\cos(\frac{1}{x})\frac{1}{x^2}=3x^2\sin(\frac{1}{x})-x\cos(\frac{1}{x})$$ for all $x\neq 0$ .
It is straightforward to see that $\lim_{x\to 0}\phi'(x)=0$ and thus the above result yields that $\phi'(0)=0$ (in particular $\phi$ is differentiable on the whole of $\mathbb{R}$ ). Now at this point, recall the chain rule. Let $f,g\colon \mathbb{R}\to \mathbb{R}$ be functions. If $a\in\mathbb{R}$ such that $f'(a)$ and $g'(f(a))$ both exist, then $(g\circ f)'(a)=g'(f(a))f'(a)$ . The obvious argument to try is the following 'wrong proof': \begin{eqnarray}
\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a} &=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \frac{f(x)-f(a)}{x-a}\\
&=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \lim_{x\to a}\frac{f(x)-f(a)}{x-a}\\
&=& g'(f(a))f'(a).
\end{eqnarray} Here we used that $f$ is continuous in $a$ to see that $f(x)\to f(a)$ as $x\to a$ . $\triangle$ However, there is an obvious error in the above reasoning. If for example $f$ is a constant function $f(x)=f(a)$ for all $x\in \mathbb{R}$ , then $\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}=\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{0}$ is nonsensical! Having said that, it is also clear that the above proof does work for functions such that $\exists \delta>0:\forall x\in (a-\delta,a+\delta)\setminus \{a\}:f(x)\neq f(a)$ . In that case, $f(x)$ does not equal $f(a)$ for $x$ near $a$ (and $x\neq a$ ). So the above proof only fails for a particular type of function, the easiest of which are constant functions. However, for a constant function $f$ , one can calculate $(g\circ f')(a)$ directly and show that it's $0$ . A natural question at this point is to wonder whether there exists a nonconstant function $f$ such that $f$ is differentiable in $a$ and $f(x)=f(a)$ infinitely often for $x$ near $a$ . The answer is yes and the function $\phi$ given in the example above (with $a=0$ ) satisfies these properties. (Also, the wikipedia page of the chain rule gives the function $f(x)=x^2\sin(\frac{1}{x})$ for $x\neq 0$ and $f(0)=0$ as an example, but this function is not differentiable in $0$ . As far as I can tell, this is a worse example than just a constant function to pinpoint the failure of the 'wrong proof'. Perhaps this should be changed?) In general let $f$ be such a function (thus $\forall \delta>0:\exists x\neq a: |x-a|<\delta$ and $f(x)=f(a)$ ). If $\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a}$ exists , then we can compute this limit by choosing an appropriate sequence $x_n\to a$ . For each $n\geq 1$ , there exists an $x_n\neq a$ such that $|x_n-a|<\frac{1}{n}$ and $f(x_n)=f(a)$ . It follows that \begin{eqnarray}
\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a}&=&\lim_{n\to \infty}\frac{g\circ f(x_n)-g\circ f(a)}{x_n-a}\\
&=& \lim_{n\to \infty}\frac{g\circ f(a)-g\circ f(a)}{x-a}\\
&=& 0.
\end{eqnarray} This shows that if $f$ is a function for which the 'wrong proof' of the chain rule fails, then $(g\circ f)'(a)=0$ . Off course, I was only able to show this under the assumption that $(g\circ f)'(a)$ actually exists (which off course is true as one can actually prove the chain rule). Nonetheless, this begs the question whether there is a more direct way of showing that $(g\circ f)'(a)$ actually exists (and equals zero) if $f$ is a function for which the 'wrong proof' fails. If so, one can actually fix this 'wrong proof' by considering two cases.","['analysis', 'real-analysis', 'solution-verification', 'derivatives', 'chain-rule']"
4208318,Function Analysis,"I tried to differentiate the function and set the value at =0 in order to find x in terms of a and b. Then, I also tried to plug the value of x into in and set the value of y into 1 and 4. At the end, I was struggling since it became more tedious. So anyone can suggest a nicer way to solve this? Thanks in advance","['maxima-minima', 'calculus', 'derivatives']"
4208327,Sparse PCA vs Orthogonal Matching Pursuit,"Can't wrap my head around the difference between Sparse PCA and OMP.
Both try to find a sparse linear combination. Of course, the optimization criteria is different. In Sparse PCA we have: \begin{aligned} \max & x^{T} \Sigma x \\ \text { subject to } &\|x\|_{2}=1 \\ &\|x\|_{0} \leq k \end{aligned} In OMP we have: $$
\min _{x}\|f-D x\|_{2}^{2} \text { subject to }\|x\|_{0} \leq k
$$ Even though these are different, they resemble one another in my eyes.
I'll explain: In PCA we wish to take the projection that counts for the most variance.
If we add the ""sparsity"" constraint, than we regularize and get some projection which is sparser, but does not account for the most variance possible (without the constraint). In OMP (the algorithm procedure here ), we pretty much do the same thing, iteratively - we find the ""atom"" that gives the largest inner product - which is the most correlative. Differences I see: Different optimization problem (already said) - however the ""applicative"" view look very similar, therefore I ask this question OMP is an iterative (greedy) procedure, while in Sparse PCA, there are direct solutions? Moreover,
how about this minor modification: we assume that $f$ is ""taken out"" of $D$ (which applicatively mean that we include ùëì to our sparse vectors) - now we have a ""basis"" $D$ , where the OMP result will give us the ""best"" sparse approximation (variance-wise) of $f$ , which resembles PCA on the covariance matrix, no? Example : $D$ is our data (samples and features). In SPCA we find the projections that count for the most variance of the data (done by defining $\Sigma=DD^T$ and applying the Sparse PCA). In OMP we do something similar, we ""take"" one sample, $f$ , ""out"" of matrix $D$ , and try to approximate it with the other samples.
This ""forces"" us to ""use"" $f$ as an atom (with coef = $1.0$ ), but eventually we will get $y = f - D'x' $ with minimum variance, which translates to $Dx$ where there is coefficient $1.0$ in the row of the sample. Thanks!","['sparse-matrices', 'sparsity', 'numerical-optimization', 'linear-algebra', 'numerical-linear-algebra']"
4208361,"$\lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e,\ $ and $\lim_{x\to\infty}\left(1-\frac{1}{x}\right)^{x} = \frac{1}{e}.\ $ Is this generalisable?","Suppose $f(x)\to\infty,\ g(x)\to\infty,\ $ both as $\ x\to\infty,\ $ and $\lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\ $ Then does $\lim_{x\to\infty}\left(1-\frac{1}{f(x)}\right)^{g(x)} = \frac{1}{c}\ ?$ The proof I know/like for $\ f(x) = x\ $ and $\ g(x)=x\ $ is as follows: Suppose $\lim_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} = e.\ $ Then \begin{align} \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{x}\right)^x}=\lim_{x\to\infty}\left(\frac{x}{x-1}\right)^x=\lim_{(x+1)\to\infty}\left(\frac{x+1}{x}\right)^{x+1}\\
\\
=\lim_{(x+1)\to\infty}\left(\left(\frac{x+1}{x}\right)^x\ \cdot\ \left(\frac{x+1}{x}\right)\right)=\lim_{x\to\infty}\left(\left(1+\frac{1}{x}\right)^x\ \cdot\ \left(1+\frac{1}{x}\right)\right) = e\cdot1 = e\\
\end{align} and we are done. I tried the same method with the generalisation: Suppose $\lim_{x\to\infty}\left(1+\frac{1}{f(x)}\right)^{g(x)} = c>1.\ $ Then $$ \lim_{x\to\infty}\frac{1}{\left(1-\frac{1}{f(x)}\right)^{g(x)}}=\lim_{x\to\infty}\left(\frac{f(x)}{f(x)-1}\right)^{g(x)}=\lim_{(x+1)\to\infty}\left(\frac{f(x+1)}{f(x+1)-1}\right)^{g(x+1)}=\ ?$$ Maybe this works with some Binomial or Taylor expansion, but I doubt it? Also I'm not even sure if the result it true...","['limits', 'functions', 'taylor-expansion', 'real-analysis']"
4208448,Infinite series $\sum_{n=1}^{\infty}{\log}^2(1+\frac{1}{n})$,"Does anybody help me obtain a closed form for the following series (via a bit elementary ways)? $$\sum_{n=1}^{\infty}{\log}^2\left(1+\frac{1}{n}\right)$$ Seemingly, the answer would be $$2\zeta(2)-\sum_{n=1}^{\infty} \frac {\zeta (2n)}{n^2}.$$ There are some questions with answers (e.g. this one ) on Stack Exchange relating to this series and pointing out the answer, but they possess advanced approaches in this regard. Thanks in advance!","['riemann-zeta', 'real-analysis']"
4208453,Derivative of the exponential function (of matrix functions) by a strange integral and a function object which does not commute with its derivative,"At the bottom of this section of an article, Wikipedia claims: $$\frac{d}{dt}\exp(X(t))=\int_0^1\exp(\alpha X(t))\frac{dX(t)}{dt}\exp((1-\alpha)X(t))\,d\alpha$$ For any general $t$ -dependent object $X$ . I can't get access to the paper they referenced from. My attempts to find this are as follows: When $X(t)$ commutes with $\frac{d}{dt}X(t)$ the result of that integral is trivially what would be expected from the chain rule. When $X$ does not commute however, there is a problem! I let $\Delta$ denote the object, be it a matrix (what I'm most interested in) or something else, representing the direction derivative of $X$ . $$\begin{align}\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{d}{dt}\frac{(X(t)^n)}{n!}=\sum_{n=0}^\infty\frac{1}{n!}\frac{d}{dt}(X(t)^n)\\&=\sum_{n=0}^\infty\frac{1}{n!}\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}\cdot\frac{dX(t)}{dt}
\\\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}&=\lim_{h\to0}\frac{h(\Delta X^{n-1}+X\Delta X^{n-2}+X^2\Delta X^{n-3}+\cdots)+o(h^2)}{h}
\\&=\Delta X^{n-1}+X\Delta X^{n-2}+\cdots
\\&=\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)}
\\\therefore\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{1}{n!}\cdot\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)}\cdot\frac{dX(t)}{dt}\end{align}$$ And I have no idea whether any of this is right, nor how to make my "" $\Delta$ "" anything meaningful (I would assume it is $\frac{d(X(t))}{dt}$ , but I don't know). I don't even know if it should be in the above calculations. I more importantly have no idea how to calculate the integral formula at the top of this question or turn my workings into an integral, and I've just been showing my failed attempts thus far. I would greatly appreciate any references, hints or answers, as this seems like a very important (at the very least interesting) formula but not one that I will be able to solve with my own knowledge. Many thanks.","['matrix-exponential', 'matrices', 'matrix-calculus', 'derivatives', 'exponential-function']"
4208474,Asymptotic behaviour of transformed Schwartz functions,"Given two Schwartz functions $f, g$ , I consider the following simultaneous transform of them: $$\Lambda_{f,g}(\lambda) := \int^{\infty}_{-\infty} dx f(x) g(x) e^{i\sinh(x+\lambda)}.$$ Of course, the product of Schwartz functions is still Schwartz so I could consider just a single function here, however I keep it this was for my attempt at the problem.
What I would like to show is a Riemann-Lebesgue type property: $$\lim_{\lambda \to \pm \infty} \Lambda_{f,g}(\lambda) = 0.$$ Due to the presence of the $\sinh$ term in the exponential, the usual arguments for proving Riemann-Lebesgue don't seem to be useable here as I cannot seem to find a primitive for $e^{i\sinh(x-\lambda)}$ . My attempt to show this was to apply a hyperbolic trig identity, then split the integral into two using a delta distribution: $$\int^{\infty}_{-\infty} dx f(x) g(x) e^{i\sinh(x+\lambda)} = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty} dx dy f(x)e^{i\cosh(\lambda)\sinh(x)} g(y) e^{i\sinh(\lambda)\cosh(y)} \delta(x-y)$$ Then I apply a substitution: $$= \int^{\infty}_{-\infty} dx dy \frac{f(x)}{\cosh(x)} \cosh(x)e^{i\cosh(\lambda)\sinh(x)} \frac{g(y)}{\sinh(y)}\sinh(y) e^{i\sinh(\lambda)\cosh(y)} \delta(x-y) \\= 2\int^{\infty}_{-\infty}\int^{\infty}_{0}dx dy\frac{f(x)}{\cosh(\sinh^{-1}(x))}e^{i\cosh(\lambda)x} \frac{g(y)}{\sinh(\cosh^{-1}(y))}e^{i\sinh(\lambda)y} \delta(\cosh^{-1}(x) -\sinh^{-1}(y))$$ Now the ""altered"" test functions $\frac{f(x)}{\cosh(\sinh^{-1}(x))}$ and $\frac{g(y)}{\sinh(\cosh^{-1}(y))}$ are still Schwartz as the denominator contributions are only polynomial. From here, I was hoping to apply a Riemann-Lebesgue type argument to the integral over x to show the final integrals vanishes asymptotically in $\lambda$ , but I'm not convinced due to the presence of the delta distribution. Does this argument work? Any other possible directions of investigation to show this, other than what I have attempted, is also welcome.","['complex-analysis', 'fourier-analysis', 'analysis']"
4208480,"Does ""probability distribution"" refer to the PDF or CDF","In my work and studies, I keep coming across statements that are similar to the following: Quote from one source: A better angle, at least from the perspective of GANs, is to define similarity in
the sense of probability distribution. Two data sets are considered similar if they
are samples from the same (or approximately same) probability distribution. Thus
more specifically we have our training data set X ‚äÇRn consisting of samples from a
probability distribution Œº (with density p(x)), and we would like to find a probability
distribution ŒΩ (with density q(x)) such that ŒΩ is a good approximation of Œº. By taking
samples from the distribution ŒΩ we obtain generated objects that are ‚Äúsimilar‚Äù to the
objects in X. Quote from a different source: The graph of a continuous probability distribution is a curve. Probability is represented by area under the curve. We have already met this concept when we developed relative frequencies with histograms in Chapter 2. The relative area for a range of values was the probability of drawing at random an observation in that group. Again with the Poisson distribution in Chapter 4, the graph in Example 4.14 used boxes to represent the probability of specific values of the random variable. In this case, we were being a bit casual because the random variables of a Poisson distribution are discrete, whole numbers, and a box has width. Notice that the horizontal axis, the random variable x, purposefully did not mark the points along the axis. The probability of a specific value of a continuous random variable will be zero because the area under a point is zero. Probability is area. The curve is called the probability density function (abbreviated as pdf). We use the symbol f(x) to represent the curve. f(x) is the function that corresponds to the graph; we use the density function f(x) to draw the graph of the probability distribution. I have been trying to really understand at more than a surface level the mathematics behind generative models, but a roadblock has been trying to determine what authors mean by ""probability distribution"". I am fairly certain that ""probability distribution"" in the first quote is referring to the CDF, since the author specifically denotes the density as p(x), which I think refers to the PDF. In the second quote, the author portrays ""the curve"" is a probability distribution, and taking an integral over it results in a probability, obviously conveying it is the PDF. Maybe ""probability distribution"" in fact has no concrete definition and the reader is left to figure out what it is referring to themselves, but it would make my life a lot easier if I knew it referred to one or the other.","['measure-theory', 'statistics', 'probability-distributions', 'probability-theory', 'probability']"
4208511,Finding the number of minimizers of a sextic polynomial,"From the Indian Joint Entrance Examination ‚Äì Main : Find the number of minimizers of the following sextic univariate polynomial $$f(x) = 10x^6 - 24x^5 + 15x^4 - 40x^2 + 108$$ I know the general method for cubic polynomials is finding roots of $f'(x)$ and checking for sign of $f''(x)$ at those points, but I am not able to solve for degree $6$ .","['contest-math', 'maxima-minima', 'functions', 'polynomials']"
4208533,Computation of an integral using residue theorem.,Using residue theorem compute the following integral $$\int_{-\infty}^{\infty} \frac {e^{\frac {x} {2}}} {1 + e^x}\ dx.$$ I have tried to proceed by the method of substitution by taking $y = e^{\frac {x} {2}}.$ Then $dy = \frac {1} {2} e^{\frac {x} {2}}\ dx$ i.e. $e^{\frac {x} {2}}\ dx = 2\ dy.$ Then the limit of integration becomes $0$ to $\infty.$ So by this substitution the integral becomes $$2 \int_{0}^{\infty} \frac {dy} {1 + y^2} = 2 \lim\limits_{x \to \infty} \arctan x = \pi.$$ By residue theorem how to deal with the same integral? Any help in this regard would be much appreciated. Thanks for your time.,"['integration', 'complex-analysis', 'residue-calculus', 'improper-integrals']"
4208566,Should I tick a multiple choice question at random or not in an exam with different weight for correct and wrong answer?,"The JEE exam contains only questions with 4 answer choices. If you select right choice then you get 4 marks If you select wrong  you get -1 marks( lose) If you don't select anything you don't get or lose any mark. Given this scenario, is it a better idea to select any choice of every question in which student has no idea of all 4 choices?","['statistics', 'probability']"
4208594,Asymptotic behaviour of $I_{\alpha}(x):=\int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy$ with $0<\alpha<1$ as $\alpha\rightarrow 0$,"I am trying to obtain the asymptotic behaviour of the integral $$I_{\alpha}(x):= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy$$ explicitly as $\alpha\rightarrow 0^{+}$ .
Clearly, by the dominated convergence theorem $$I_{\alpha}(x)\rightarrow
\int_{\mathbb{R}^{n}}e^{-|x-y|^2}dy=\int_{\mathbb{R}^{n}}e^{-|y|^2}dy=c.$$ My naive attempt is  to calculate $I_{x}$ explicitly in the hope to get an asymptotic. Here is what I got:
Since $|x-y|^2=|x|^2-2 x\cdot y+|y|^2$ then $$I_{\alpha}(x)= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy=e^{-|x|^2}\int_{\mathbb{R}^{n}}\frac{e^{-2 x\cdot y+|y|^2}}{|y|^{\alpha}}dy.$$ Using spherical coordinates, we get $$I_{\alpha}(x)= e^{-|x|^2}\int_{\mathbb{S}^{n-1}}
\int_{0}^{\infty}
e^{-2 x\cdot \omega r+r^2}r^{n-1-\alpha}dr d\sigma(\omega).$$ By Fubini's theorem we have $$I_{\alpha}(x)= e^{-|x|^2}
\int_{0}^{\infty}\int_{\mathbb{S}^{n-1}}
e^{-2 x\cdot \omega r} d\sigma(\omega)
e^{-r^2}r^{n-1-\alpha}dr.$$ Using the formula (Grafakos, Classical Fourier Analysis, Appendix D) $$C\int_{\mathbb{S}^{n-1}} F(x\cdot \omega)d\sigma(\omega)= \int_{-1}^{1}F(s|x|)(\sqrt{1-s^2})^{n-3} ds$$ we have $$I_{\alpha}(x)= e^{-|x|^2}
\int_{0}^{\infty}\int_{-1}^{1}
e^{-2 s|x| r}(\sqrt{1-s^2})^{n-3} ds 
e^{-r^2}r^{n-1-\alpha}dr.$$ Applying Fubini's theorem one more time $$I_{\alpha}(x)= e^{-|x|^2}
\int_{-1}^{1}\int_{0}^{\infty}
e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr(\sqrt{1-s^2})^{n-3} ds.$$ Is there a way to calculate $$\int_{0}^{\infty}
e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr$$ Mathematica gives some kind of the hypergeometric function. It is difficult to understand how Mathematica's answer behaves in terms of $\alpha$ .","['integration', 'multivariable-calculus', 'analysis', 'real-analysis']"
4208698,Arrange letters to avoid consecutive positions,"Given 7 letters: a,a,b,b,c,d,e, how many possible ways are there for us to arrange them so that matching letters are not in consecutive positions? (e.g. aabcdbe) is not legal. I tried to solve the question by finding the total possible ways first. If there's no restriction, then there're $\frac{7!}{2!2!}$ ways. Here we don't want $aa$ and $bb$ appear in the string, so we need to subtract $2\cdot\frac{6!}{2!}$ from $\frac{7!}{2!2!}$ . Is that right? Did I miss any unwanted cases? Thanks for the help!",['combinatorics']
4208742,"Fastest way of multiplying two $n√ón$ matrices for fixed $n=1,2,3,\ldots$?","While the question of what is the asymptotically fastest matrix multiplication algorithm is still open, and tremendous improvements were made between 1968 and 1990 (Strassen, Coppersmith-Winograd), I was wondering in the opposite direction, whether it is known what the fastest matrix multiplication algorithm is for matrices of fixed size $n=1,2,3,\ldots$ More specifically, given fixed costs for basic scalar operations, is there some table out there on the web containing this data? How do these algorithms look like for, say, $n=1\ldots 20$ ? What is smallest $n$ for which the answer is unknown? (I suspect it will be rather small due to combinatorial explosion issues.)","['matrices', 'computational-complexity']"
4208785,Evaluating $\lim_{x\to\pi/4} \frac{(x - \frac{\pi}{4})\sin\left(3x - \frac{3\pi}{4}\right)}{2(1-\sin2x)}$ only by using simple algebra,"As I mentioned in the title, I'm not allowed to use L'hopital, series representation, and other advance method. By using trigonometric rules and simple algebra, how to evaluate this limit below: $$\lim_{x\to\pi/4} \frac{(x - \frac{\pi}{4})\sin\left(3x - \frac{3\pi}{4}\right)}{2(1-\sin(2x))}$$ I know the answer is $3/4$ (I'm using L'hopital). I'm curious if that problem can be solved by using simple algebra and trigonometric rules only since that problem is for high school (they haven't learned about L'hopital). Any idea? My problem is getting rid of $(x - \frac{\pi}{4})$ . Thanks in advance!","['limits', 'trigonometry', 'limits-without-lhopital']"
4208810,Showing that a density function of a given r.v. is given by another density function,"Let $X$ have a density such that $f_X (Œº + x) = f_X (Œº ‚àí x)$ , i.e., it is symmetric about $Œº$ . Let $Y = 2Œº ‚àí X$ . Show that the density of $Y$ is given by $f_X$ . Use this to determine the distribution of $Y$ when $X ‚àº N(Œº,œÉ^2)$ . Is this the correct approach? $$\begin{align}f_Y(y) &= [F_Y(y)]' \\&= [P(Y \leq y)]' \\&= [P(2Œº - X \leq y)]' \\&= [1 - F_X(2Œº - y)]'\\& = -f_X(2Œº - y)\cdot (-1) \\&= f_X(2Œº - y) \\&= f_X(Œº + (Œº - y)) \\&= f_X(Œº - (Œº - y))\\& =  f_X(y)\end{align}$$ using the fact that $f_X (Œº + x) = f_X (Œº ‚àí x)$ Also, how can I determine the distribution of $Y$ when $X ‚àº N(Œº,œÉ^2)$ ?","['probability-theory', 'probability']"
4208823,If $A \subseteq X$ and $B \subseteq Y$ then $A \coprod B \subseteq X \coprod Y$.,"I am beginning to study algebraic topology. Suppose $X$ and $Y$ are topological spaces. In the lecture notes, the topology of $X \coprod Y$ was defined in the following way:
""the open subsets are precisely those of the form $A \coprod B$ with $A \in \tau_X$ and $B \in \tau_Y$ "". This tacitly implies that if $A \subseteq X$ and $B \subseteq Y$ then $A \coprod B \subseteq X \coprod Y$ . I drew the following diagram for this situation: $\require{AMScd}$ \begin{CD}
A @>{i_A}>> A \coprod B @<{i_B}<< B\\
@V{\iota_A}VV @V{\phi}VV @V{\iota_B}VV\\
X @>{i_X}>> X \coprod Y @<{i_Y}<< Y
\end{CD} Here $i_A, \iota_A$ etc. are all the normal inclusion maps, and $\phi$ is defined in the natural way, associating $i_B(b)$ with $i_Y(\iota_B(b))$ for all $b \in B$ and $i_A(a)$ with $i_X(\iota_A(a))$ for all $a \in A$ . Clearly $\phi$ is an injection, so I can see how $A \coprod B \cong \phi(A \coprod B) \subseteq X \coprod Y$ , but I don't see how $A \coprod B \subseteq X \coprod Y$ . In other places on the internet (e.g. Definition of Disjoint Union Spaces ) the topology is defined in a slightly different way that seems not to have this issue. Does the definition in my lecture notes actually make sense? Am I just being too pedantic about pointless details?","['elementary-set-theory', 'general-topology']"
4208828,Tranformations of Curves,"A while ago I asked a question similar to this, but looking back, I think I would have to further clarify. Please excuse how I ask this question, as I am very new to the Math Stack Exchange. Suppose I have a curve, with one endpoint on the origin of the x-y axis, and the other at some other point on the x-axis. In other words, the two endpoints of the curve are on (0,0) and some (x,0) If I were to change the position of the latter endpoint (x,0), how do other points on curve f(x) change with respect to the change of (x,0)? Intuitively, if I have a string, and I stretch an endpoint to some other position, how does the original string change? Moreover, if I were to move that endpoint in a specific path (represented as a function), what would the paths of the other points on the curve be? Looking at the problem, I thought about representing the function as a series of points, to which I would draw respective lines through each point, and would then calculate the motion of the other points with respect to the change of the latter endpoint. I did this for a 'one-point system', which is merely a point connected with 2 lines, with these lines connecting towards the two endpoints listed above (I would draw a diagram but I do not know how to on this software). Doing this, I found an equation, although it was huge. I then did this for two-point systems, and I soon realized that the solutions would be much bigger than I imagined. I am sure that if I continued to work on this problem, I would arrive at somewhat of a verdict. However, the people on this Stack Exchange are much more well-trained in mathematics than I am. And so my question is this: Does a solution currently exist to this problem? If so, what is it? If not, how would I further proceed with the method I used above? How would you all solve the problem? Please note that the new, transformed curve is subject to the previous curve's arc length (the original and new curve have the same length). The question I seek to find is how the original curve changes into its new form. Again, please excuse any difficulties with how I have written this post. Hopefully, you all do not mind.","['geometry', 'ordinary-differential-equations']"
4208839,To show that the integral of a certain series converges.,"Let $a_n$ be a decreasing sequence of positive real numbers converging to $0$ such that $\sum_{n=1}^\infty a_n \log(1/a_n)<\infty$ . Further, let $b_n$ be an arbitrary sequence and define $$f_n(x):=\begin{cases} 
      \cfrac{a_n}{|x-b_n|}, &  a_n\leq |x-b_n| \\
       0, & \text{else} \\
   \end{cases}
$$ I would like to show that for every $R>0$ , $$\int_{-R}^R\sum_{n=1}^\infty f_n(x) \;dx<\infty.$$ My attempt is to interchange the integral and sum (MCT allows this since $f_n$ is non-negative and partial sums are increasing) and then evaluate the integral of $f_n$ . However, for large enough $R$ , this integral must be divergent since $b_n\in [-R,R]$ , for large enough $R$ , so I think this will not work. Any hints? Please don't answer this problem in its entirety, I would very much like to do it myself. Thanks for you time.","['measure-theory', 'lebesgue-integral', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4208855,New identities involving infinite summation of trigonometric functions?,"Lately, I'm trying to prove that $\lim \limits_{n\to\infty} \int_0^\infty \sin(xt)e^{-t}\sum_{k=0}^n \frac{t^k}{k!}dt =\frac{1}{x}$ (for $x\neq$ zeros of $\sin(x)$ ) by using mellin transform of $f(t)=\sin(xt)e^{-t}$ , to be specific, $\int_0^\infty t^{s-1}\sin(xt)e^{-t}dt = \frac{\Gamma(s)\sin(s\tan^{-1}(x))}{(x^{2}+1)^{\frac{s}{2}}}$ . After dividing $\Gamma(s)$ , summing up on both sides and changing the variable, I got the following equation( needed to be proven): $\sum_{k=1}^\infty \frac{\sin(k\tan^{-1}(x))}{(x^2+1)^{\frac{k}{2}}}=\frac{1}{x}$ . I noticed that $\tan^{-1}(x)=\cos^{-1}({(x^{2}+1)^{-\frac{1}{2}}})$ , so after substitution, this is the equivalent equation: $\sum_{k=0}^\infty \sin((k+1)x)\cos(x)^k = \csc(x)$ . I have worked a lot and I found some other equations (with the assumption that the equation above is true): $\sum_{k=0}^\infty \cos((k+2)x)\cos(x)^{k}= -1$ , $\sum_{k=0}^\infty \cos(kx)\cos(x)^{k}= 1$ By using those equations, with the formula $(\cos(x)+i\sin(x))^n=\cos(nx)+i\sin(nx)$ , I could form a geometric series which visually proves that my assumption is true. But the problem is I actually guessed the answer first, means that my whole work is just a circular reasoning without a proof. I'm wondering if this identity was discovered and proven before because I can't find them in public discussions. By the way, could you help me prove this? Any helps will be respected. Thanks in advance.","['calculus', 'trigonometry', 'real-analysis']"
4208922,"Are ""most"" subsets of reals non-Lebesgue Measurable?","I have an intuition that most subsets of reals are not Lebesgue measurable. The tricky part is defining ""most"". For that, we need a measure on the set of subsets of the powerset of reals, in other words, a measure on the powerset of the powerset of reals. Has anyone defined such a measure, and either confirmed or denied my intuition?","['measure-theory', 'reference-request']"
4208929,Find the derivative of the function $f(x)=\int_{0}^\infty e^{-y^2-(x/y)^2} dy$,"This problem confuses me, Can we use Leibniz  rule here? But in the chapter Leibniz rule was not introduced. Is it possible to find the derivative using simple concepts. The actual problem is to evaluate the integral $\int_{0}^\infty e^{-y^2-(9/y)^2} dy$ using differential equations.","['integration', 'derivatives', 'ordinary-differential-equations']"
4208931,Question about discrete math: Division by integers and GCD,"could anyone tell me if my resolution is correct? Problem : Let $a$ , $b$ , and $c$ be  integers satisfying: $a|(b+4c)$ $\qquad$ $(1)$ $a|(b-2c)$ $\qquad$ $(2)$ $a$ leaves remainder $1$ when divided by $3$ $\qquad$ $(3)$ Under these conditions can we say that $a$ divides $b$ ? My solution : Theorems and properties used: $1$ : If $a|b$ and $a|c$ then $a|(b+c)$ ; $2$ : If $a|b$ then $a|bc$ for any integer $c$ ; $3$ : If $a$ and $b$ are integers and $a = q*b + r$ where $q$ and $r$ are integers, then: $GCD(a,b)=GCD(b,r)$ ; $4$ : let $a$ , $b$ and $c$ be three integers such that $a$ divides $bc$ and $a$ and $b$ are prime to each other, then $a$ divides $c$ . Using Theorem $2$ and relation $(2)$ we can say that: $a|2(b-2c)$ $\longrightarrow$ $a|(2b-4c)$ $\qquad$ $(4)$ Using Theorem $1$ and relation $(4)$ we obtain that: $a|[(b+4c)+(2b-4c)]$ $\longrightarrow$ $a|3b$ $\qquad$ $(5)$ Due to condition $(3)$ we can write $a$ as: $|a|=3k+1$ , $\qquad$ $k\in \mathbb{Z}$ $\qquad$ $(6)$ Using property $3$ : $GCD(a,3)=GCD(3,1)=1$ $\qquad$ $(7)$ Finally, due to the result obtained in $(7)$ and property $4$ we can conclude that $a|b$ I believe it's all right, but I still don't have much confidence if the properties and theorems I've chosen can be applied here or if I've applied them correctly. If you can confirm that everything is correct, or if it is wrong, tell me where I went wrong and how to proceed, I would be grateful.","['education', 'discrete-mathematics']"
4208946,Are all sine functions odd?,"If I have a function like : $f(x) = \sin(e^{x/2} + e^{-x/2})$ or something equally complicated, do I actually need to work out if $f(-x) = -f(x)$ , or are all sine functions odd no matter what it is a function of and it is just a matter of proving this using trig identities? thanks!","['algebra-precalculus', 'functions', 'trigonometry']"
4208952,Why smooth section of vector bundle $F\to M$ is $\Gamma(TM) \times \Gamma(TM) \to \Gamma(NM)$,"Let $M\subset \tilde{M}$ be the embedded Riemann submanifold,We can construct the vector bundle $F\to M$ where each fiber is bilinear map $T_pM\times T_pM \to N_pM$ ,which is a smooth vector bundle. The question is why the smooth section of this bundle is $\Gamma(TM) \times \Gamma(TM) \to \Gamma(NM)$ where $NM$ is normal bundle of $M$ ,and $\Gamma$ means smooth section of the corresponding vector bundle. Let's make the theorem more clear,we have the following charaterization lemma: Let $B$ be a rough section of vector bundle $F$ then we can define the map $B(X,Y)(p) = B_p(X_p,Y_p)\in N_pM$ then $B$ is a smooth section of $F$ if and only if $B(X,Y)(p)$ is a
smooth section of $NM$ for each smooth vector field $X,Y$","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4209009,Solving the integral $\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)$,"I want to solve the following integral $$
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
$$ with $a\in\mathbb{R}$ and $t\in\mathbb{R}^+$ . I have tried some substitutions and the most promising are $u=\sqrt{t-s} \ \left(du=-\frac{1}{2\sqrt{t-s}} \  ds\right)$ so that the integral becomes \begin{align}
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
&=-2\int_{\sqrt{t}}^{0}du\ \sqrt{s}\operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)\\
&=\color{orange}{2\int^{\sqrt{t}}_{0}du\ \sqrt{t-u^2}\operatorname{erfc}\left(\frac{a}{\sqrt{t-u^2}}\right).}
\end{align} $v=\frac{1}{\sqrt{s}} \ \left(dv = -\frac{1}{2s^{3/2}} \ \ \ ds \right)$ which yields \begin{align}
\int_{0}^{t} ds \sqrt{\frac{s}{t-s}} \operatorname{erfc}\left(\frac{a}{\sqrt{s}}\right)
&=2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ v^{3/2} \frac{1}{v\sqrt{t-\tfrac{1}{v^2}}} \ \operatorname{erfc}\left(av\right)\\
&=\color{orange}{2\int_{\tfrac{1}{\sqrt{t}}}^{\infty} dv \ \frac{v^{3/2}}{\sqrt{v^2t-1}}  \ \operatorname{erfc}\left(av\right)}
\end{align} Furthermore, I have looked at the table of integrals [ 1 ], [ 2 ] and [ 3 ]. However, so far, no luck. Is there someone who knows a way forward?","['integration', 'definite-integrals', 'calculus', 'error-function', 'probability']"
4209055,How to directly show $S=\sum_{k=1}^{m}e^{2\pi ik^2/m}=\sqrt{m}$,"Let $S=\sum_{k=1}^{m}e^{2\pi ik^2/m}$ ,if $m$ is odd,how to directly calculate the absolute value of $S=\sqrt{m}$ .Don't use Gauss sum since here it says ""it's easily shown"" My try is as follows: $$ \begin{align}S^2&=\left(\sum_{k=1}^{m}e^{2\pi ik^2/m}\right)\left(\sum_{k=1}^{m}e^{-2\pi ik^2/m}\right)
\\&=\sum_{k=1}^{m-1}\sum_{d=1}^{m-k}2\cos\left(\frac{2\pi}{m}(2dk+k^2)\right)+m\end{align}$$ we must prove $\sum_{k=1}^{m-1}\sum_{d=1}^{m-k}2\cos(\frac{2\pi}{m}(2dk+k^2))=0$ .If m is small, I can directly calculate, but if m is large, how to do so by induction or any other solutions.",['complex-analysis']
4209074,Prove that at least one student solved all the problems using PHP,"In a physics exam, $5$ problems were given to a class of $N$ students.  Suppose every two of these problems were solved by more than $\frac{3N}{5}$ students.Prove that at least one student solved all the problems I have used PHP here. We know if at least $(k.n +1)$ objects are distributed among $n$ boxes, then one of the boxes must contain at least $(k+1)$ objects.
So according to this problem $(k.n +1)$ = $2$ |where $n$ = $\frac{3N}{5}$ .
But unable to prove at least one student solved all the problems .
I have to prove basically $(k+1)$ = $5$ . It will be helpful for me if someone helps me with this.","['permutations', 'pigeonhole-principle', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics']"
4209085,Sequence $a_1=2$ and $a_n=\frac{2}{1+a_{n-1}}$ is convergent or divergent? [duplicate],"This question already has answers here : Prove that sequence $ x_{(n+1)}= \frac {a}{1+x_n}$ is convergent to positive root of $x^2+x-a=0,$where $a >0$ and $x_1 >0$ [duplicate] (2 answers) Closed 2 years ago . Given a sequence with $a_1=2$ and $a_n=\frac{a_1}{1+a_{n-1}}$ , I have to check whether this sequence is convergent or divergent. I have reached the conclusion till now that its subsequence $a_{2n}$ is an increasing sequence and bounded above, while the subsequence $a_{2n-1}$ is a decreasing sequence and bounded below.
So, both the subsequences are convergent.
But I am not able to find $\lim_{n\to\infty} a_{2n}$ or $\lim_{n\to\infty} a_{2n-1}$ .
Can you help me to proceed further
or maybe find another way to show its convergence or divergence?","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4209104,ring of $p$-adic algebraic integers,"Let $K/\mathbb{Q}_p$ be a finite extension, and $\overline{K}$ be some fixed algebraic closure of $K$ . Let $\mathcal{O}_{\overline{K}}$ be the ring of all algebraic integers in $\overline{K}$ . I know it is a valuation ring so it is an integrally closed local domain. I would like to know what is the Krull dimension of this ring? Is the maximal ideal the only nontrivial prime ideal? In particular, I am interested in whether $\mathrm{Spec}(\overline{K})\to\mathrm{Spec}(\mathcal{O}_{\overline{K}})$ is an open immersion?","['p-adic-number-theory', 'algebraic-geometry', 'commutative-algebra']"
4209122,About Theorem 3.4 Hartshorne: detailed proof.,"I propose a detailed version of part of the proof of Theorem 3.14 from Hartshorne 's book Algebraic Geometry . The questions are inserted from time to time within the proof. Thanks for your patience. Notation Let $Y\subseteq\mathbb{P}^n$ be a projective variety. Letting $S=k[x_0,\dots, x_n]$ (equipped with the usual grading $S=\oplus_{r\ge 0} S_r$ ), there is a naturale grading on $S(Y)=S/I(Y)$ which is constructed as follows: for each integer $r\ge 0$ define $$S(Y)_r:=S_r/I(Y)_r:=\{f+I(Y)_r\;:\; f\in S_r\}$$ where $I(Y)_r=I(Y)\cap S_r.$ Theorem. Let $Y\subseteq\mathbb{P}^n$ be a projective variety with homogeneous coordinate ring $S(Y)$ . Then we have: $(a)\quad$ for any point $P\in Y$ , let $\mathcal{m}_P\subseteq S(Y)$ denote the ideal generated by the set of all homogeneous elements $f\in S(Y)$ such that $f(P)=0$ . Then $\mathcal{O}_P=S(Y)_{(\mathcal{m}_P)}$ ; $(b)\quad$ $K(Y)\cong S(Y)_{((0))}$ Proof. Let $U_i\subseteq\mathbb{P}^n$ be the open set defined by $x_i\ne 0$ and set $Y_i:= Y\cap U_i$ . We may consider $Y_i$ as an affine variety. We can construct a natural isomorphism $$\varphi_i^*\colon A(Y_i)\to S(Y)_{(x_i)}$$ as $$\varphi_i^*\bigg(f(y_1,\dots, y_{i-i},1, y_i, \dots, y_n)+I(Y_i)\bigg)=\frac{f(x_0,\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r},$$ where $f\in S_r$ . $(b)\quad$ Let $P\in Y$ and chose any $i$ so that the $i-$ th coordinate of $P$ is non-zero. Then $P\in Y\cap U_i$ . We may write $P=[a_0:\dots: a_{i-1}:1:a_i:\dots: a_n]$ . Set $P_0:=(a_0,\dots, a_{i-1}, a_{i+1},\dots, a_n)=\varphi_i(P)$ , the corresponding point in the affine space. Denote with $\mathcal{m'}_{P_0}$ the maximal ideal of $A(Y_i)$ corresponding to $P_0$ . More precisely, $$\mathcal{m'}_{P_0}=\bigg\{g+I(Y_i)\in A(Y_i)\;:\; g(P_0)=0\bigg\}.$$ It's easy to prove that: $$\boxed{\varphi_i^*(\mathcal{m'}_{P_0})=\mathcal{m}_P\cdot S(Y)_{(x_i)}}$$ Recall that the product in the right of the above equation is interpreted in the localization as follows (with the natural induced grading on the quotient ring $S(Y)=S/I(Y))$ $$\mathcal{m}_p\cdot S(Y)_{(x_i)}=\bigg\{\frac{f(x_0\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}\;:\; f+I(Y)_r\in\mathcal{m}_P\cap S(Y)_r\;\text{for same}\; r\ge 0\bigg\}.$$ Using this isomorphism, we then have the isomorphism of the localizations $$A(Y_i)_{\mathcal{m'}_{P_0}}\cong \big(S(Y)_{(x_i)}\big)_{\mathcal{m}_P\cdot S(Y)_{(x_i)}}$$ Question 1. I believe that the last isomorphism can be obtained from exercise number 4 on page 44 of Introduction to Commutative Algebra by Atiyay - MacDonald, is that so? We need to argue that $$\big(S(Y)_{(x_i)}\big)_{\mathcal{m}_P\cdot S(Y)_{(x_i)}}\cong S(Y)_{(\mathcal{m}_P)}\tag1$$ Question2. Why $(1)$ is not usual transitivity of localizations? A typical element in $\big(S(Y)_{(x_i)}\big)_{\mathcal{m}_P\cdot S(Y)_{(x_i)}}$ is of the form $$\frac{\frac{f+I(Y)_{s+r}}{x_i^{r+s}+I(Y)_{s+r}}}{\frac{g+I(Y)_r}{x_i^r+I(Y)_r}}\equiv \frac{f+I(Y)_{s+r}}{x_i^sg+I(Y)_{s+r}}$$ where $g+I(Y)_r\notin\mathcal{m}_p\cap S(Y)_r$ , and the identification takes place in the quotient field of $S(Y)$ . Now since $x_i^s+I(Y)_s\notin \mathcal{m}_p\cap S(Y)_s$ it follows that the product $x_i^sg +I(Y)_{s+r}\notin\mathcal{m}_p\cap S(Y)_{s+r}$ . So the element on the right is a degree $0$ element of the localization $$S(Y)_{\mathcal{m}_P}=\bigoplus_{n\in\mathbb{Z}}\big(S(Y)_{\mathcal{m}_P}\big)_n$$ where $$\big(S(Y)_{\mathcal{m}_P}\big)_n=\bigg\{\frac{p+I(Y)_{n+r}}{q+I(Y)_r}\;:\; q+I(Y)_r\notin \mathcal{m}_P\cap S(Y)_r\;\text{for same}\; r\ge 0\bigg\}.$$ This proves that $$A(Y_i)_{\mathcal{m'}_{P_0}}\cong (S(Y)_{\mathcal{m}_p})_0=S(Y)_{(\mathcal{m}_P)}$$ $(b)\quad$ Observe that $K(Y)\cong K(Y_i)$ . The final part is to show that $$K(Y_i)\cong S(Y)_{((0))}.$$ We have already obtained the isomorphism $\varphi_i^*\colon A(Y_i)\to S(Y)_{(x_i)}$ . Extending this isomorpgism to $\tilde{\varphi_i^{*}}$ on the quotient fields fo both the sides we get $$\tilde{\varphi_i^*}\bigg(\frac{f(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}{g(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}\bigg)=\frac{\frac{f(x_0,\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}{\frac{g(x_0,\dots, x_n)+I(Y)_s}{x_i^s+I(Y)_s}}=\frac{x_i^s f(x_0,\dots, x_n)+I(Y)_{r+s}}{x_i^rg(x_0, \dots, x_n)+I(Y)_{r+s}}$$ In the denominator of the last expression above we have $g(x_0,\dots, x_n)+I(Y)_s\ne 0$ So to make sense of this, we need $x_i^r+I(Y)_r\ne 0$ as well. Since $I(Y)$ is a homogeneous prime ideal, this is violated only if $x_i\in I(Y)$ . I have proved that $x_i\in I(Y)$ is equivalent to $Y_i=\emptyset$ , which is absurd according to our choise of $U_i$ . Finally, notice that the element in the image of $\tilde{\varphi_i^*}$ in above are elements of $S(Y)_{((0))}$ . Question 3 It remains to show that every element of $S(Y)_{((0))}$ is also an image of an element of $K(Y_i)$ . How can I show this? My Solution for question 3. A typical element in $S(Y)_{((0))}$ is of the form $$\frac{f(x_0,\dots, x_n)+I(Y)_r}{g(x_0,\dots, x_n)+I(Y)_r},$$ where $f+I(Y)_r\in S(Y)_r$ and $g+I(Y)_r\notin S(Y)_r\cap (0)$ . Let us consider the dehomogenized of $f$ and $g$ : $$f(y_1,\dots, y_{i-1}, 1, y_i,\dots, y_n)\quad\text{and}\quad g(y_1,\dots, y_{i-1}, 1, y_i,\dots, y_n),$$ where $\deg f(y_1,\dots, y_{i-1}, 1, y_i,\dots, y_n)= \deg g(y_1,\dots, y_{i-1}, 1, y_i,\dots, y_n) = r$ , results $$\begin{split}\tilde{\varphi_i^*}\bigg(\frac{f(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}{g(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}\bigg)=&\frac{\frac{f(x_0,\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}{\frac{g(x_0,\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}\\
=\frac{f(x_0,\dots, x_n)+I(Y)_{r}}{g(x_0, \dots, x_n)+I(Y)_{r}}\end{split}$$ The above happens just in case $x_i\nmid f$ and $x_i \nmid g$ . Suppose now that $x_i\mid f(x_0,\dots, x_n)$ and that $x_i\nmid g(x_0,\dots, x_n)$ We denote by $h(x_0,\dots, x_n)$ the homogenized polynomial of $f(y_1,\dots, y_{i-1},1,\,y_i,\dots, y_n)$ , then $h\in S_{\deg f}$ , then $$
\begin{split}
\tilde{\varphi_i^*}\bigg(\frac{f(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}{g(y_1,\dots, y_{i-1},1, y_i, \dots, y_n)+I(Y_i)}\bigg)=
\frac{\frac{h(x_0,\dots, x_n)+I(Y)_{\deg f}}{x_i^{\deg f}+I(Y)_{\deg f}}}{\frac{g(x_0,\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}=&\frac{x_i^{r-\deg f} h(x_0,\dots, x_n)+I(Y)_{r}}{g(x_0, \dots, x_n)+I(Y)_{r}}\\
=&\frac{f(x_0,\dots, x_n)+I(Y)_r}{g(x_0,\dots, x_n)+I(Y)_r}
\end{split}
$$","['proof-explanation', 'proof-writing', 'algebraic-geometry', 'solution-verification']"
4209141,Perpendicular lines in Isoceles triangle,"So I was reviewing my olimpiad materials for Geometry and in the notes I took in different classes I found this question: Consider Isoceles triangle $ABC$ $(AB=AC)$ and let $M$ be the midpoint of $BC$ . We draw $MH$ perpendicular to $AC$ . Now let $N$ be the midpoint of $MH$ , prove that $AN$ and $BH$ are perpendicular. Any ideas for a solution!? I tried finding a cyclic quadrilateral and drew some parallels with MH to maybe create some helpful similarities but got to no where!","['euclidean-geometry', 'geometry', 'alternative-proof', 'triangles', 'trigonometry']"
4209167,The expected distance between two points on a sphere and on a circle,"Two points are randomly selected on the circle. What is the expected
distance between them? And what will be the expected distance between
the two points on a sphere? An interesting problem, I had several ideas: we can generate a uniform distribution in an $n$ -dimensional cube described around a unit ball, remove points outside the ball from the sample, and obtain a uniform distribution of vectors in the ball. We normalize the vectors - we get on the sphere. And we use Monte Carlo.... Of course, there are a lot of iterations, and the accuracy is low, but for a rough estimate and for checking the exact calculations, it will do well. I also reasoned like this: since the task does not change when rotating, I can take one point fixed. We get the expectation of the distance from a random point of the circle to a fixed one. This is an obvious integral: $$ \frac{1}{2 \pi} \int_{-\pi}^{\pi} \sqrt{(1-\cos x)^{2}+\sin ^{2} x}\,dx=\frac{1}{\pi} \int_{-\pi}^{\pi}\left|\sin \frac{x}{2}\right|dx=\frac{2}{\pi} \int_{0}^{\pi} \sin \frac{x}{2}\,dx=\frac{4}{\pi}$$ - for the unit circle, of course. $$ \begin{aligned} &\frac{1}{4 \pi} \int_{0}^{\pi} \int_{-\pi}^{\pi} \sin \theta \sqrt{(1-\cos \theta)^{2}+\sin ^{2} \theta \cos ^{2} \varphi+\sin ^{2} \theta \sin ^{2} \varphi}\, d \varphi d \theta= \\ &=\frac{1}{2} \int_{0}^{\pi} 2 \sin \frac{\theta}{2} \sin \theta\, d \theta=\frac{1}{2} \int_{0}^{\pi} \cos \frac{\theta}{2}-\cos \frac{3 \theta}{2} d \theta=\left.\left(\sin \frac{\theta}{2}-\frac{1}{3} \sin \frac{3 \theta}{2}\right)\right|_{0} ^{\pi}=\frac{4}{3} \end{aligned}.$$ -and this is for the sphere. Parameterization of the sphere: $z=\cos(\theta)$ , $x=\sin(\theta)\cos(œÜ)$ , $y=\sin(\theta)\sin(œÜ)$ . As a fixed point we take $(0,0,1)$ . Jacobian $\sin(\theta)$ . I took a slightly non-standard parameterization relative to theta, so that later I would not mess with things like $\sin (\pi/4-\theta/2)$ . Here are my thoughts. I ask you to double-check me and, if possible, write your own version.","['calculus', 'probability-distributions', 'geometry', 'probability']"
4209178,Random walk returning probability: Back to the origin,"Considering a discrete random walk in 2D starting from $(0,0)$ with 1/4 probability of moving in each of the four directions for each step, calculate the probability of returning to $(0,0)$ after $2n$ moves. For instance, the probability for the point returing to its origin (0,0) after $2$ moves is: $4/4^2 = 0.25$ . I have got an answer as follows, but I think it is not elegant: After 2n moves, there are $4^{2n}$ outcomes. Let A be the event in which the point returns, and $n(A_{n})$ be the number of outcomes in event A. Suppose A include the point moving upwards and downwards $k$ times(so that the point will not shift in the y-axis direction), and rightwards and leftwards $(n-k)$ times. In such case, there are $\displaystyle{\frac{(2n)!}{[k!k!(n-k)!(n-k)!]}}$ outcomes. Sum up to get event A: $
\begin{eqnarray}
n(A_{n}) &=&  \displaystyle{\sum^{n}_{k=0}\frac{(2n)!}{(k!)^2[(n-k)!]^2}}= 
\displaystyle{\frac{(2n)!}{(n!)^2}}\sum_{k=0}^{n} \left[ \frac{n!}{k!(n-k)!} \right]^2 \\
&=&   \displaystyle{ \binom{2n}{n} \sum_{k=0}^{n}}\binom{n}{k}^2 = \displaystyle{\binom{2n}{n}^2} \\ \text{because} \displaystyle{ \sum_{i=0}^{n}\binom{n}{i}^2=\binom{2n}{n}}
\end{eqnarray}
$ Hence, the probability of returning is $P(A_n)=\displaystyle{\frac{ \binom{2n}{n}^2}{4^{2n}} } $ Obseving the numerator I think there might be some easy ways to figure out the answer: Since the point "" moves upwards and downwards $k$ times "", and "" leftwards and rightwards $(n-k)$ times "", if combining the number of steps moving $\color{red}{up}$ and $\color{red}{right}$ it would be $\color{red}{n}$ steps in total, and choose $\color{red}{n}$ steps from $2n$ , there would be $\color{fuchsia}{\binom{2n}{n}}$ possibilities. Considering the answer above $P(A_n)=\displaystyle{\frac{ \binom{2n}{n}^2}{4^{2n}} } $ , the product and numerator $\binom{2n}{n} \binom{2n}{n} $ must contain the event $A$ , which is ""the point moves upwards and downwards $k$ times , and leftwards and rightwards $(n-k)$ times. Let the first part $\binom{2n}{n} $ contain ""moving $n$ steps leftwards and upwards in total"", and let the second part $\binom{2n}{n} $ contain ""moving $n$ steps rightwards and upwards in total"". Together, I get the event $A$ ,
but I don't know how to proceed...","['random-walk', 'probability-theory', 'probability']"
4209215,Find the max and min values of a multivariable function on the boundary of a domain,"I have the multivariable function: $f(x,y) = x^3 + y^3 - 3xy + 2$ I want to find the maximum and minimum values of this function on the domain: $$ D=[(x,y) : x,y\ge 0, x^2 +y^2\le4] $$ I found the partial derivatives to be: $f_x=3x^2 -3y$ $f_y=3y^2-3x$ From here I set both my partial derivatives to equal $0$ and solved for $x$ and $y$ and got $(1,1)$ and $(0,0)$ , I believe these two could potentially be the max/min I'm looking for, however I know there are more possibilities. I then created a function $g$ where $g=x^2 + y^2$ ( $g = 4$ ) I know that $\nabla f= \lambda \nabla g$ so $(3x^2-3y,3y^2-3x) = \lambda(2x,2y) $ I then set up $3$ equations: $\lambda x = \frac32  (x^2) - \frac32  (y)$ $\lambda y = \frac32  (y^2) - \frac32  (x)$ $x^2 + y^2 = 4$ But I'm not sure where to go from here and how to solve these equations for $x$ and $y$ and $\lambda$ . To be honest I'm not even sure if this is the best way to go about answering this question. If anyone could show me how I would find the maximum and minimum of the function on this domain with a better method, or help me continue with mine, it would really help.","['partial-derivative', 'maxima-minima', 'multivariable-calculus', 'lagrange-multiplier']"
4209243,Hypothesis for differentiating under the integral,"I've been trying to understand when it is allowed to move a derivative inside the integral, especifically the requirements on the derivative of the function being integrated. $$\frac{d}{dx}\int_a^bf(x,y)dy=\int_a^b\frac{\partial}{\partial x}f(x,y)dy$$ In some sources I find that it just requires that $\frac{\partial}{\partial x}f(x,y)$ be continuous in some neighbourhood while in others it is also required that there exists a function $g(y)$ such that $|\frac{\partial}{\partial x}f(x,y)|<g(y)$ . Can someone please explain to me if/when the second hypothesisis not necessary?","['integration', 'proof-explanation', 'calculus', 'derivatives']"
4209245,Proof that if $X>Y$ then there exists some $\epsilon>0$ such that $x-\epsilon>y+\epsilon$,"I am trying to prove that if $x>y$ , then there exists some $\epsilon>0$ such that $x-\epsilon>y+\epsilon$ . So far I have: Suppose, for a contradiction, that no such $\epsilon$ exists. That is, $\forall \epsilon>0$ , suppose that $x-\epsilon \leq y+\epsilon$ . Beyond this, I am not at all sure how to proceed. Must one make reference to the fact that limits preserve weak inequalities? Thank you.","['analysis', 'real-analysis']"
4209251,"Link between left Poisson bracket, Hamiltonian vector field and the flow of the Hamiltonian vector field","I am going through Variations on a Theme by Kepler , Guillemin and Sternberg. It is stated that a symplectic transformation on a manifold $\phi:X \rightarrow X$ induces a transformation on function. $$f \rightsquigarrow f \circ \phi^{-1}
$$ They say: It turns out that every function $f$ on phase space generates a (local) one-parameter group, $\phi_t$ , of symplectic transformations such that $$\{f, g\} = \dfrac{d}{dt}g \circ \phi_{-t}\rvert_{t=0}
$$ My understanding is that they are referring to the flow of the Hamiltonian vector field $X^f = \{f, \cdot\}$ $$\phi_t = \exp(tX^f)
$$ I am however confused as to how everything fits together. We have for the flow $\rho$ of a complete vector field $X$ that: $$\dfrac{d\rho_t(p)}{dt} = X(\rho_t(p))
$$ and so $$X^f(g) = \{f, g\} = \dfrac{d}{dt}g \circ \phi_{-t}\rvert_{t=0}
$$ Intuitively, it seems to me that there is a minus sign error, indeed we are considering the change in $g$ as we travel against the flow, and I would expect this to be $-X^f(g)$ I am asking this because I am trying to understand the statement ""left Poisson bracket by $f$ is the infinitesimal symmetry associated to $f$ "" and how the above statements fit together EDIT: According to Wikipedia, some authors define the Hamiltonian vector field with the opposite sign. My best answer is this must be the case here. $$-\{f, \cdot\} = X^f
$$ Then indeed $$X^f(g) = -\{f, g\} = -\dfrac{d}{dt}g \circ \phi_{-t}\rvert_{t=0}
$$ This can also be checked with the fact the Lie derivative for functions is $\mathcal{L}_X f = X(f)$ along with the general definition of the Lie derivative of a k-form: $$\mathcal{L}_X \omega := \dfrac{d}{dt}(\exp(tX))^*\omega \Big \rvert_{t=0}
$$ giving $$\mathcal{L}_X f = \dfrac{d}{dt}f \circ(\phi_t)) \Big \rvert_{t=0} = X(f)
$$ In this way, for a function $f$ , $-\{f, \cdot\}$ = $X^f$ is the infinitesimal generator for the one-parameter family of symmetry transformations on the manifold $$\phi_t:X\rightarrow X$$ $$\phi_t = \exp^{tX^f}$$ while $\phi^*_{-t}$ is the induced transformation on a function $g$ Is it thus correct to say the following ? $$\dfrac{d}{dt}\exp(tX)\Big\rvert_{t=0}f = X(f) = \dfrac{d}{dt}f\circ\exp(-tX)\Big\rvert_{t=0}
$$","['vector-fields', 'symplectic-geometry', 'differential-geometry']"
4209255,Algebraic(!!) version of the Riemann existence theorem,"In Appendix B to Hartshorne's Algebraic Geometry , Hartshorne claims that one can prove that compact Riemann surfaces are algebraic in the following way.  First show there exists a nonconstant meromorphic function from $X$ to $\mathbb{P}^1$ , which realizes $X$ as a ramified covering over $\mathbb{P}^1$ .  Second, use the Riemann existence theorem to show this gives $X$ the structure of a nonsingular algebraic curve. I am asking about this second step.  Everything I've seen about the Riemann existence theorem has been about the existence of things related to Riemann surfaces, not algebraic curves a priori.  For example, one statement is that the meromorphic functions on a compact Riemann surface separate points, and another is that if we prescribe certain ramification data over some $Y$ - $d$ points, then we can find a corresponding map of Riemann surfaces $X\rightarrow Y$ .  Anyways, I don't see any statement that says that $X$ must then be an algebraic curve (i.e. a complex projective variety of dim 1), and I would appreciate it if anybody gave me a reference or a proof of this particular step. And just to be clear: I know there are other proofs that $X$ is algebraic, but I am looking for the one Hartshorne is referring to here.  And of course I see the generalized Riemann existence theorem he states right after which is much more high-powered, but I'd like the elementary argument Hartshorne refers to.","['algebraic-curves', 'complex-geometry', 'algebraic-geometry', 'riemann-surfaces']"
4209261,"What do $‚àÇùëß,‚àÇùë•$ and $‚àÇùë¶$ individually mean?","So, I came up with this (weird) warning paragraph in my calc textbook : The symbol $‚àÇz$ , unlike the differential $dz$ , has no meaning of it's own. For
example if we were to ""cancel"" partial symbols in the chain-rule formula $\frac{\partial z}{\partial u}=\frac{\partial z}{\partial x}\frac{\partial x}{\partial u}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial u}$ we would obtain: $\frac{\partial z}{\partial u}=\frac{\partial z}{\partial u}+\frac{\partial z}{\partial u}$ which is false in cases $\frac{\partial z}{\partial u}‚â†0$ . I know that for variable $y$ dependent on $x$ in such a way that $y=f(x)$ we can say: $dx=‚àÜx$ and $dy=\lim_{‚àÜx\rightarrow0}‚àÜy$ . Analogously for the variable $z$ dependent on $x$ and $y$ in such a way that $z=f(x,y)$ can we say: $dx=‚àÜx,dy=‚àÜy$ and $dz=\lim_{(‚àÜx,‚àÜy)\rightarrow(0,0)}‚àÜz$ $??$ Are $‚àÇx=dx,‚àÇy=dy$ and $‚àÇz=dz$ $??$ If they are not, what do $‚àÇz,‚àÇx$ and $‚àÇy$ individually mean $?$ And what does the differential $dz$ expresses? [I am probably sounding like a dumb econ undergrad (which I am) so mind my lack of mathematical knowledge.]","['partial-derivative', 'multivariable-calculus', 'partial-differential-equations']"
4209287,Problem with indicator in $\mathbb E[|X|\unicode{x1D7D9}_E]$,"Suppose $\mathbb E\:|X|<\infty$ . Show that for all $\epsilon>0$ , there exists a $p>0$ such that if a set $E\subseteq \Omega$ satisfies $\mathbb P(E)<p$ then $\mathbb E[|X|\unicode{x1D7D9}_{E}]<\epsilon$ . Hint: For any set E and $n>0$ , recall $\mathbb E[|X|\unicode{x1D7D9}_E]=\mathbb E[|X|\unicode{x1D7D9}_E\unicode{x1D7D9}_{\{|X|>n\}}]+\mathbb E[|X|\unicode{x1D7D9}_E\unicode{x1D7D9}_{\{|X|\leq n\}}]$ To be honest I don't understand their hint. Like why $\mathbb E[|X|\unicode{x1D7D9}_E]$ equal to $\mathbb E[|X|\unicode{x1D7D9}_E\unicode{x1D7D9}_{\{|X|>n\}}]+\mathbb E[|X|\unicode{x1D7D9}_E\unicode{x1D7D9}_{\{|X|\leq n\}}]$ . why did they bring such an indicator? And is $\mathbb E[|X|\unicode{x1D7D9}_E]$ means expectation of $|X(\omega)|$ when $\omega\in E?$ I managed to get the solution: Fix $\epsilon>0$ . Recall that since $|X(\omega)|<\infty$ for all $\omega$ , $|X|\unicode{x1D7D9}_{\{|X|>n\}}\rightarrow0\textrm{ a.s.}$ Also, $|X|\unicode{x1D7D9}_{\{|X|>n\}}\leq |X|$ which is integrable, so by Dominated Convergence Theorem(DCT) it follows that $\mathbb E[|X|\unicode{x1D7D9}_{\{|X|>n\}}]\rightarrow0$ Thus, we can choose $n$ such that $\mathbb E[|X|\unicode{x1D7D9}_{\{|X|>n\}}]<\frac{\epsilon}{2}$ . Now, consider any set $E$ . Observe that, $$
\begin{align}
\mathbb E[|X|\unicode{x1D7D9}_E] &= \mathbb E[|X|\unicode{x1D7D9}_{\{|X|>n\}}]+E[|X|\unicode{x1D7D9}_{\{|X|\leq n\}}] \\
& \color{red}{\stackrel{?}{\leq}}  \mathbb E[|X|\unicode{x1D7D9}_{\{|X|>n\}}]+n\mathbb E[\unicode{x1D7D9}_E] \\
& \color{red}{\stackrel{?}{\leq}} \frac{\epsilon}{2}+n\mathbb P(E)
\end{align}  
$$ From this, if $\mathbb P(E)< \frac{\epsilon}{2n}$ , $\mathbb E[|X|\unicode{x1D7D9}_E]<\epsilon$ , so the claim is shown for $p=\frac{\epsilon}{2n}$ , where $n$ is
specified above. From the solution, I am also some questions like Why $|X|\unicode{x1D7D9}_{\{|X|>n\}}\leq |X|?$ How they come up with those red marked inequalities? explicitly why $E[|X|\unicode{x1D7D9}_{\{|X|\leq n\}}] \leq n\mathbb E[\unicode{x1D7D9}_E] \leq n\mathbb P(E)?$ I think all of that confusion arises because of that indicator. Maybe I am not understanding its role here.","['measure-theory', 'probability-theory']"
4209318,Compute $\int_0^1\frac{\arctan(\lambda x)}{x\sqrt{1-x^2}}\mathrm dx$ and $\int_0^1\frac{\arctan(\lambda x)}{\sqrt{1-x^2}}\mathrm dx$,"A friend gave me this integration as a challenge that he had failed to solve. I tried substituting $\tan^{-1}(\lambda x)=\theta$ . I also tried integrating by parts. None worked, any advice/ hints would be greatly appreciated. Even better if someone could tell me what is a general way to approach problems like these! $$\int_{0}^{1}\frac{\tan^{-1}(\lambda x)}{x\sqrt{1-x^{2}}}dx$$ The second part of the problem asks the value of $$\int_{0}^{1}\frac{\tan^{-1}(\lambda x)}{\sqrt{1-x^{2}}}dx$$","['integration', 'calculus', 'trigonometry']"
4209331,When is the image of an entire function all of $\mathbb{C}$?,"Little Picard Theorem: If a function $f:\mathbb{C}\to\mathbb{C}$ is entire and non-constant, then the set of values that $f(z)$ assumes is either the whole complex plane or the plane minus a single point. Define "" Picard-0 functions "" to be the functions which are entire, non-constant and their image is $\mathbb{C}$ . Define "" Picard-1 functions "" to be the functions which are entire, non-constant and their image is $\mathbb{C}$ without a single point. Some elementary examples of Picard-0 functions are $\sin$ and $\cos$ . However, $\exp$ is a well known example of a Picard-1 function. Just by looking at the power series expansions at the origin, $$\sin z=\sum_{n=0}^\infty\operatorname{Im}(i^n)\frac{z^n}{n!},\quad z\in\mathbb{C},$$ $$\cos z=\sum_{n=0}^\infty\operatorname{Re}(i^n)\frac{z^n}{n!},\quad z\in\mathbb{C},$$ $$\exp z=\sum_{n=0}^\infty \frac{z^n}{n!},\quad z\in\mathbb{C},$$ it is not immediately clear whether they are Picard-0 functions or Picard-1 functions, given that they are entire and non-constant (this is just an illustrative example). (In the series, $0^0=1$ was assumed.) If $f(z)=\sum_{n=0}^\infty a_n z^n$ and $f$ is entire and non-constant, is it possible to state some explicit conditions for $a_n$ which would determine whether $f$ is a Picard-0 function or a Picard-1 function?","['complex-analysis', 'functions', 'sequences-and-series']"
4209342,Understanding and Solving Trigonometric Functions by Hand [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question I have heard of people who can do trigonometric functions by hand, without a calculator. I know that, for example, $\sin(\theta)=\text{opposite}/\text{hypotenuse}$ , but calculators can also do $\sin(64.5)$ , and I don't understand how that works. What really is a trigonometric function besides its ratio and how can you solve it by hand? I know that the ratio mentioned above can be used to create relationships between sides and angles, but what is the core definition of the function $\sin(x)$ ? I also know that if you graph $\sin(x)$ , you get the wavy sinusoid but again, what is the function definition?","['trigonometry', 'functions', 'definition']"
4209374,Family of subsets of $n$-element set s. t. union of each four subsets has cardinality at most $n-2$ has size at most $2^{n-2}$,"Let $n>1$ , $X$ be a set of cardinality $n$ and $A_1,\ldots, A_k$ be a sequence of distinct subsets of $X$ such that $|A_i\cup A_j \cup A_l \cup A_f|<n-1$ for all $1\leq i, j, l, f\leq k$ . Show that $k\leq 2^{n-2}$ . I tried many techniques to solve that problem, in particular I solved some simple cases, observed that the proof is straightforward if there exists an element which is in at least $2^{n-3}-1$ of sets $A_i$ or if there exist three of them such that their union has cardinality exactly $n-2$ , but I don't know if this is helpful when considering the general case. Any help will be appreciated.","['combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4209381,How to straighten a parabola?,"Consider the function $f(x)=a_0x^2$ for some $a_0\in \mathbb{R}^+$ . Take $x_0\in\mathbb{R}^+$ so that the arc length $L$ between $(0,0)$ and $(x_0,f(x_0))$ is fixed. Given a different arbitrary $a_1$ , how does one find the point $(x_1,y_1)$ so that the arc length is the same? Schematically, In other words, I'm looking for a function $g:\mathbb{R}^3\to\mathbb{R}$ , $g(a_0,a_1,x_0)$ , that takes an initial fixed quadratic coefficient $a_0$ and point and returns the corresponding point after ""straightening"" via the new coefficient $a_1$ , keeping the arc length with respect to $(0,0)$ . Note that the $y$ coordinates are simply given by $y_0=f(x_0)$ and $y_1=a_1x_1^2$ . Any ideas? My approach: Knowing that the arc length is given by $$
L=\int_0^{x_0}\sqrt{1+(f'(x))^2}\,dx=\int_0^{x_0}\sqrt{1+(2a_0x)^2}\,dx
$$ we can use the conservation of $L$ to write $$
\int_0^{x_0}\sqrt{1+(2a_0x)^2}\,dx=\int_0^{x_1}\sqrt{1+(2a_1x)^2}\,dx
$$ which we solve for $x_1$ . This works, but it is not very fast computationally and can only be done numerically (I think), since $$
\int_0^{x_1}\sqrt{1+(2a_1x)^2}\,dx=\frac{1}{4a_1}\left(2a_1x_1\sqrt{1+(a_1x_1)^2}+\arcsin{(2a_1x_1)}\right)
$$ Any ideas on how to do this more efficiently? Perhaps using the tangent lines of the parabola? More generally , for fixed arc lengths, I guess my question really is what are the expressions of the following red curves for fixed arc lengths: Furthermore, could this be determined for any $f$ ? Edit: Interestingly enough, I found this clip from 3Blue1Brown. The origin point isn't fixed as in my case, but I wonder how the animation was made (couldn't find the original video, only a clip, but here's the link ) For any Mathematica enthusiasts out there, a computational implementation of the straightening effect is also being discussed here , with some applications.","['analytic-geometry', 'geometry', 'calculus', 'functions', 'quadratics']"
4209419,Show $\lim\limits_{t\to\infty}\Bigg|\sum\limits_{n=0}^\infty\Theta(t-nR)\frac{(\Gamma(t-nR))^n}{n!}e^{-\Gamma(t-nR)}\Bigg|^2=\frac1{(1+\Gamma R)^2}$,"I have encountered the following problem while studying non-Markovian effects in real-time dynamics of open quantum systems. In particular, I was studying a system comprised of two qubits (qubit is a standard shorthand for two level quantum system) separated by distance (in configuration space e.g. a laboratory) $R$ from one another and coupled to a one-dimensional Bosonic reservoir hosting a pair of boson species corresponding to right and left moving photon fields with linear dispersion relation and fixed propagation speed (equal to $1$ in my units (Planck's constant is set to be $2\pi$ )). It is really one of the simplest system one can imagine which possesses the so-called delayed coherent quantum feedback, a property that dynamics of the quantum system at any time $T$ depends on the entire history of its dynamics for all times $t\in[0, T]$ in deterministic (i.e. classical, not quantum, and thus very human-being controllable) fashion. You can easily guess that without any knowledge of physics whatsoever, you have quantum information that propagates with finite velocity $v=1$ over given distance $R$ , so there is an intrinsic time delay $\tau=vR$ in the system and due to Lorentz covariance this cannot be altered by any quantum effects. The Hamiltonian operator of such a system can be written as $H=H_{0}+V$ ,  where \begin{align}H_{0}&=\sum_{n=1, 2}\frac{\Delta_{n}}{2}\sigma_{3}^{(n)}+\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dk\omega_{\mu}(k)a^{\dagger}_{\mu}(k)a_{\mu}(k),\\V&=\sum_{n=1, 2}\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dkg_{\mu, n}(k)a^{\dagger}_{\mu}(k)\sigma_{+}^{(n)}+g_{\mu, n}^{*}(k)\sigma_{-}^{(n)}a_{\mu}(k).\end{align} Here $\Delta_{n}$ is the detuning of the qubit number $n$ from some reference energy $k_{0}$ , i.e. $\Delta_{n}=\Omega_{n}-k_{0}$ , where $\Omega_{n}$ is the transition frequency of the qubit number $n$ and $k_{0}$ is the momentum around which the spectrum of the bath of boson fields was linearised (we of course expect photons of energy ( $hvk_{0}/(2\pi)=k_{0}$ ) $k_{0}$ to couple most strongly to some system with transition frequency $\min[\Omega_{1}, \ \Omega_{2}]$ ). Further, $\omega_{\mu}(k)=hvk/(2\pi)=k$ is the energy of the photon of flavour $\mu$ and momentum $k$ . Such a photon is created by operator $a^{\dagger}_{\mu}(k)$ and destroyed by $a_{\mu}(k)$ , these obey non-zero commutation $[a_{\mu}(k), a^{\dagger}_{\mu'}(k')]=\delta(k-k')\delta_{\mu, \mu'}$ . $\sigma_{j}^{(1)}=\sigma_{j}\otimes\sigma_{0}, \ \sigma_{j}^{(2)}=\sigma_{0}\otimes\sigma_{j}$ , where $\sigma_{\pm}=(\sigma_{1}+i\sigma_{2})/2$ , and $\sigma_{1,2, 3}$ are usual Pauli-matrices ( $\sigma_{0}$ is an identity on $\mathbb{C}^{2}$ ). The coupling constants are defined as $$g_{\mu, n}(k)=\sqrt{\frac{\Gamma_{n}}{2\pi}}e^{-ic_{\mu}c_{n}(k_{0}+k)R/2},$$ where $\Gamma_{n}$ is the bare decay rate of a single qubit into the continuum, and $c_{s}=(-)^{s+1}, \ s=\mu, \ n$ distinguishes the coupling to right/left (with index $\mu$ ) photons of an atom number $n=1, 2$ , located at $\pm R/2$ . As a mock up problem I consider the following. Consider the system described above Hamiltonian prepared at time $t_{0}=0$ in the following 2-parameter family of states $$|\psi(0)\rangle=(\cos\vartheta|1\rangle\otimes|0\rangle+e^{i\varphi/2}\sin\vartheta|0\rangle\otimes|1\rangle)\otimes|\Omega\rangle.$$ Here the quantum states are ordered as qubit 1, qubit 2, bath, i.e. $|a\rangle\otimes|b\rangle\otimes|c\rangle$ = qubit one is in state $a$ , qubit $2$ is in $b$ and bosons are in $c$ . Here $|\Omega\rangle$ is the ""vacuum"" state of bosons defined by $a_{\mu}(k)|\Omega\rangle=0,\forall \mu=1,2 , \ k\in\mathbb{R}$ . In physics you'll call such a setup a spontaneous emission problem. I was able to deduce with the help of diagrammatic techniques (see our recent paper https://arxiv.org/pdf/2101.07603.pdf related to the $T\rightarrow\infty$ limit of non-Markovian systems) that the exact survival probability amplitude for an initial state defined above has the form (this result is exact in all parameter regime iff $\Theta(t)$ the Heaviside step function is defined equal to $1$ at $t=0$ , it is indeed ambiguous since we use use both Hille‚ÄìYosida theorem and Sokhotski-Plemelj theorem which are good until $T=t_{0}=0$ where divergencies happen, then you have to maintain the order of limits with some care) $$P(t)=|a(t)|^{2},\quad a(t)=\oint_{C_{+\eta}}\langle{\psi(0)|G(z)|\psi(0)\rangle}e^{-izt}\frac{dz}{2\pi i}, \ t>0.$$ Here the integration contour $C_{+\eta}$ is a Bromiwich style contour suspending itself above the real axis with positive imaginary part $\eta\searrow0$ . $G(z)$ is the operator valued function known in physics as the retarded Green's function. One thus sees that all important information about dynamics is contained in the poles and branch cuts of $G(z)$ . The projection of the retarded Green's function onto the single atomic excitation subspace can be determined analytically in the exact form: $$
G^{(1)}(z)=\frac{1}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}(g_{1}(zc|10\rangle\langle{10}|+g_{2}(z)|01\rangle\langle{01}|$$ $$-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)})$$ $$=G_{e1}(z)|10\rangle\langle{10}|+G_{e2}(z)|01\rangle\langle{01}|+G_{o}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)}),
$$ where \begin{align}
G_{ej}(z)=\frac{g_{j}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}, \quad G_{o}(z)=\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}},
\end{align} and $g_{j}(z)=(z-\Delta_{j}+i\Gamma_{j})^{-1}$ are the single-quit green's functions (note that their Laplace transform is trivial $e^{-i\Delta_{j}t}e^{-\Gamma_{j}t}$ due to locality (like you've learned in high school-an exponential decay)). The complete answer is a bit involved but can be clearly expressed in terms of Fourier images of $G_{ej}(z), \ G_{o}(z)$ , these are \begin{align}
G_{e1}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{1}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
&=e^{-i\Delta_{1}t}e^{-\Gamma_{1}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m+1, m, t),\\
G_{e2}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{2}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
&=e^{-i\Delta_{2}t}e^{-\Gamma_{2}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m, m+1, t),\\
G_{o}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
=&-i\sqrt{\Gamma_{1}\Gamma_{2}}e^{ik_{0}R}\sum_{m=0}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m+1, m+1, m+1, t)
\end{align} where \begin{align}
&I(a, b, c, t)=\oint_{C_{+\eta}}\frac{dz}{2\pi i}e^{-iz(t-aR)}g_{1}^{b}(z)g_{2}^{c}(z)
=\int_{C_{+}}\frac{dz}{2\pi{i}}\frac{e^{-iz(t-aR)}}{(z-\Delta_{1}+i\Gamma_{1})^{b}(z-\Delta_{2}+i\Gamma_{2})^{c}}\\
&=\Theta(t-aR)\Bigg(\sum_{k=0}^{b-1}\frac{(-1)^{k}(c+k-1)!(-i(t-aR))^{b-k-1}}{k!(b-k-1)!(c-1)!}\frac{e^{-i\Delta_{1}(t-aR)}e^{-\Gamma_{1}(t-aR)}}{(\Delta_{2}-\Delta_{1}+i(\Gamma_{2}-\Gamma_{1}))^{c+k}}\\
&+\sum_{k=0}^{c-1}\frac{(-1)^{k}(b+k-1)!(-i(t-aR))^{c-k-1}}{k!(c-k-1)!(b-1)!}\frac{e^{-i\Delta_{2}(t-aR)}e^{-\Gamma_{2}(t-aR)}}{(\Delta_{1}-\Delta_{2}+i(\Gamma_{1}-\Gamma_{2}))^{b+k}}\Bigg).
\end{align} So far so good. These functions are looking physically correct, etc. You can see an infinite number of revival peaks in survival probability which happen on all integer multiples of delay time $vR$ , i.e. when the photon emitted by one atom reaches the other, isn't it cool you can control what atom does by just moving it around?=) Many physicists though are recently discussing the possibility of so called DARK states, the states for which $p(t)=1, \ \forall t>0$ . Their argument is based on Markov approximation though, it is the limit where $\max{\Gamma_{n}}_{n=1, \ 2}\times vR\ll1$ . The first assumption to be done is that the quits are equivalent and ""bright"" (no detuning) $\Delta_{1}=\Delta_{2}=0, \ \Gamma_{n}=\Gamma$ . The second is $\theta=\pi/4, \mod 2\pi, \varphi=4\pi, \mod 2\pi$ for bright and $\theta=\pi/4, \mod 2\pi, \varphi=2\pi, \mod 2\pi$ for dark states. My analysis shows \begin{align}
\label{eq: DE58}
p_{B}(t)=&\Bigg{|}G_{e}(t)+G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(-\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2},\\
\label{eq: DE59}
p_{D}(t)=&\Bigg{|}G_{e}(t)-G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2}.
\end{align} I.e. for the dark state we obtain $p_{D}(t)=1, \forall t>0$ at $R=0$ . For $R\neq0$ when $t\rightarrow \infty$ the sereies converges to a finite value $$p(t)\rightarrow\text{constant}.$$ By numerical exercise I'd like to claim that the $t$ infinity limit of $p_{D}(t)$ is precisely $1/(1+\Gamma R)^{2}$ . Can you help me to prove or disprove that? And if you happen to know the closed form the above series do not hesitate to share. By the long-time limit I understand you take above series with all $\Theta$ equal to $1$ (do not bother about how rigorous this is please). To be specific: if you happen to know something about series like $x^{n}(c-nx)^n/n!$ give us a clue. Another clue from numerics: at super large times $\Gamma t\gg1$ we deduce that the quantum decay of bright state follows a sub-exponential power law $1/t$ in the long time regime. Numerics show the $R/(2\pi t)$ law, see picture.","['quantum-field-theory', 'sequences-and-series', 'mathematical-physics', 'real-analysis']"
4209443,Problems with seemingly not enough information,"Two of my favorite geometry problems are as follows: Consider two concentric circles with the property that a chord of the larger circle with length 20 is tangent to the inner circle.  What is the area of the region between the circles? Consider a sphere with a circular hole drilled through its center, such that the height of the remaining ring (in the direction along the hole) is $h$ .  What is the volume of the ring? The fascinating thing about these problems is that they seem to be under-determined: In the first case, it seems that you should need to know at least one of the circles' radii; in the second case it seems that you should need to know the radius of the sphere or the radius of the hole.  It turns out that the answer is independent of these unknown quantities, however, so the questions are well posed. Another cute fact about these problems is that, supposing them to be well posed, they admit very easy computations of their answers, since we can choose the unknown parameters to be whatever we want to facilitate the computation: In the first case, choosing the radius of the inner circle to be zero, the chord is a diameter of the outer circle, and the desired area is just the area of the same circle, $\pi \times 10^2 = 100 \pi$ . In the second case, choosing the radius of the hole to be zero, the volume is just that of a full sphere of radius $h/2$ , i.e. $\frac{1}{6}\pi h^3$ . Question: What are other examples of problems which seem to be ill-posed, but are not? I once thought that these two problems were anomalous, but I've recently discovered there are other examples.  (I will post one if no one else does.)  The examples need not come from geometry.  Please post only one problem per answer, and if (as with the above problems) a computation is simplified by assuming the problem to be well posed, please explain.","['big-list', 'geometry']"
4209471,A modified median,"Assume we have real numbers $x_1 < x_2 < \dots < x_n$ .
Consider the following function which returns the average distance of a point $t$ from $x_1,\dots,x_n.$ $$
D_1(t) = \frac{1}{n}\sum_{i=1}^n|x_i - t|.
$$ It is well known that $D_1(t)$ is minimized at a median of $x_1,\dots,x_n$ . I am interested in the centrality parameter that is obtained when the mean is replaced by the median above (you may assume $n$ is odd and $n \geq 3$ if it helps). So, my question is this : Is there a simple expression for $$ \arg\min D_2(t) $$ where $$
D_2(t) = \operatorname{median}\{|x_1-t|,\dots,|x_n-t|\}?
$$ I have done some numerical experiments. I think that there is always a minimum at a point of the form $\frac{x_i + x_j}{2}$ where $i \leq j$ and $\frac{x_i+x_j}{2}\leq x_{i+1}.$ This problem is combinatorial because one has to keep track of how the order of $|x_i - t|$ 's change as $t$ changes. Update: User Joe has conjectured that if $n=2k+1$ is odd then the minimum occurs that $t = \frac{x_m + x_{m+k}}{2}$ where $m = \arg\min (x_{m+k} - x_m)$ . The plot of the objective function for $1, 2, 3, 10, 11, 12, 20$ shows that a median may not be the minimizer. The conjecture is valid here (k=3), a minimum occurs at $\frac{x_1+x_4}{2}$ and $x_4-x_1=9$ . Another example where the conjecture is valid. Here $k=4$ and the minimum occurs at $\frac{x_1+x_5}{2}=3.$","['optimization', 'statistics', 'combinatorics']"
4209496,Alternative to the Binomial PMF?,"Since hypergeometric distribution is the without replacement version of the Binomial distribution why can't we replace the combinations in the Hypergeometric PMF with combinations with replacement and expect the same result with the standard Binomial PMF ? To elaborate this is the regular combination formula counting unordered without replacement : $$C(n,r)= \frac{n!}{r!(n‚àír)!}$$ And this is the combination with replacement formula counting unordered with replacement: $$C^R(n,r)= \frac{(n+r‚àí1)!}{r!(n‚àí1)!}$$ and this is the hypergeometric distribution PMF: For $X\sim\operatorname{HGeom}(w,b,n)$ $$P(X=k)= \frac{C(w,k)C(b,n-k)}{C(w+b,n)}$$ My question is, since binomial and hypergeometric are both fixed number of trials where Binomial is counting with replacement and Hypergeometric is counting without replacement, why doesn't altering the Hypergeometric formula to be $$P(X=k)= \frac{C^R(w,k)C^R(b,n-k)}{C^R(w+b,n)}$$ give us the PMF of the Binomial Distribution ? What am I missing ?","['statistics', 'probability-distributions', 'binomial-distribution', 'combinatorics', 'probability']"
4209516,"If $\phi_\sigma$ is the pdf of $\mathcal N(0,\sigma^2)$ and $\psi$ is another pdf, does $(\phi_\sigma*\psi)/\phi_\sigma\to1$ for $\sigma\to\infty$?","Let $\phi_\sigma$ denote the probability density function of $\mathcal N(0,\sigma^2\cdot id)$ , where $id$ is the identity matrix in $\Bbb R^{n\times n}$ . If $X,Y$ are independent $\Bbb R^n$ -valued random variables with $X_\sigma\sim\mathcal N(0,\sigma^2\cdot id)$ , then $X_\sigma+Y$ has the density $$
\Phi_\sigma(x)=\int_{\Bbb R^n} \phi_\sigma(x-y) P(Y\in dy), \quad x\in\Bbb R^n.
$$ For the sake of simplicity, I'll suppose that $Y$ also has a density $\psi$ , so $\Phi_\sigma$ is simply the convolution $\phi_\sigma*\psi$ . What I am interested in: Find conditions on the law of $Y$ under which we have $$
\frac{\Phi_\sigma(x)}{\phi_\sigma(x)} \xrightarrow{\sigma\to\infty} 1, \quad x\in\Bbb R^n. \tag{1}
$$ Any reference that treats this or similar questions is just as helpful as a hint or a solution. I'd expect that the law of $Y$ needs to have sufficiently nice tails (or just moments) for this to work... What I have done so far: For any fixed $y\in\Bbb R^n$ and compact set $K\subset\Bbb R^n$ , we have \begin{align*}
P(X_\sigma+y\in K)&=\int_{K-y}\phi_\sigma(x)dx=\int_K \phi_\sigma(x-y)dx\\
&=\int_K \phi_\sigma(x) \exp((1/2\sigma^2)(2x^\top y-|y|^2)) dx.
\end{align*} For $\sigma \to \infty$ , the term $\exp((1/2\sigma^2)(2x^\top y-|y|^2))$ goes to 1 locally uniformly in $x$ and $y$ , so we can infer that $$
\frac{P(X_\sigma+y\in K)}{P(X_\sigma\in K)} \xrightarrow{\sigma\to\infty} 1
$$ locally uniformly in $y$ . Here's where my argument becomes sloppy: For sufficiently large $N\in\Bbb N$ and $\sigma>0$ we should then have \begin{align*}
P(X_\sigma+Y\in K)&\approx \int_{[-N,N]} P(X_\sigma+y\in K)\psi(y)dy \\
&\approx \int_{[-N,N]} P(X_\sigma\in K)\psi(y)dy \\
&\approx P(X_\sigma\in K),
\end{align*} but here I just naively change the order of limits with respect to $N$ and $\sigma$ . Even if we assume that this can be fixed and $$
\frac{P(X_\sigma+Y\in K)}{P(X_\sigma\in K)} \xrightarrow{\sigma\to\infty} 1
$$ actually holds, is this enough to conclude that (1) is true?","['probability-distributions', 'asymptotics', 'reference-request', 'gaussian', 'probability-theory']"
4209576,"If $H$ is a special subgroup of $G$, then is $H$ a normal subgroup of $G$?","A teacher shared with me his textbook (writing in progress) about group theory and this exercise is at the end of the first chapter. The whole problem:
A subgroup $H$ of $G$ is special if for all $x \in G\backslash H$ and $y \in G$ exists only one $a \in H$ such that $y^{-1}xy=a^{-1}xa$ . Prove that if $H$ is a special subgroup of $G$ , then $H \unlhd G$ . I tried by contradiction, suppose $ H $ is not a normal subgroup of $G$ . Then, there is $g \in G$ and $h \in H$ such that $gHg^{-1}\neq H $ . Equivalently, there is $g \in G$ and $h \in H$ such that $ghg^{-1} \in G \backslash H $ . Since $H$ is a special group, there is only one $a \in H$ such that $y^{-1}(ghg^{-1})y = a^{-1}(ghg^{-1})a$ .
What i'm trying to do is to show that there is more than one $a \in H$ that satisfies this property.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
4209580,Finding the tangent hyperplane to a function at a given point,"Given a function $f : \mathbb{R}^n \to \mathbb{R}$ and a point $p$ in the domain, how would you find the equation of the tangent hyperplane to the surface at point $p$ ? We know that if $h \to 0$ , then the Jacobian matrix $J_f(p)h = 0$ . Can we use this to find the equation of the hyperplane? Maybe something along the lines of $J_f(p) \cdot [x_i - p_i]^{\text{T}} = c$ ?","['jacobian', 'multivariable-calculus']"
4211699,Solutions to $2^a3^b+1=2^c+3^d$,"Find all $a,b,c,d$ positive integer such that: $2^a3^b+1=2^c+3^d$ My progress: One solution satisfying is $$\boxed{a=1,b=1,c=2,d=1} $$ We first take $\mod 3$ which gives $$ L.H.S\equiv 1\mod 3,~~R.H.S\equiv 2^c\mod 3$$ Hence we get $c$ even. So let $c=2k.$ We get $$2^a3^b-3^d=2^{2k}-1=(2^k-1)(2^k+1). $$ Now note that $2^k-1,2^k+1$ are relatively prime. Because, if not then let $d$ be the common divisor. Then $$d|2^k-1,~~d|2^k+1\implies d|(2^k+1)-(2^k-1)=2\implies d|2^k\implies d|2^k+1-2^k=1.$$ Now there are two cases. So using the fact that $2^k-1,2^k+1$ are relatively prime and then for odd k $3|2^k+1$ and for even $k$ $3|2^k-1$ Case 1: When $d<b$ then $$2^a3^b-3^d=3^d(2^a3^{b-d}-1)=(2^k-1)(2^k+1)$$ $K$ is odd $$\implies v_3(2^k+1)=d,~~3\nmid 2^k-1. $$ $K$ is even $$\implies v_3(2^k-1)=d,~~3\nmid 2^k+1. $$ Case 2: When $d>b$ then $$2^a3^b-3^d=3^b(2^a -3^{d-b})=(2^k-1)(2^k+1)$$ $K$ is odd $$\implies v_3(2^k+1)=b,~~3\nmid 2^k-1. $$ $K$ is even $$\implies v_3(2^k-1)=b,~~~~3\nmid 2^k+1. $$ P.S. This is my 100th post in MSE. The other solutions are there in the chat. Any elementary method?","['contest-math', 'modular-arithmetic', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
4211707,"Matrix Recurence Relation, $M_n=M_{n-1}M_{n-2}$","Let $M_0, M_1\in\mathbb R^{k\times k}$ be matrices, and define $M_n=M_{n-1}M_{n-2}$ .
Then $M_n$ is a product of $F_n$ ( $n$ th Fibonacci number) many copies of $M_0$ or $M_1$ . How do I compute the limit $M_n^{1/F_n}$ as $n\to\infty$ ? This is of course easy if $M_0$ and $M_1$ commute, but I'm not sure how to do it in a more general setting. I'm particularly interested in the spectral norm $\|M_n\|^{1/F_n}$ . Can we show, for example, that $\|M_n\|^{1/F_n} \ge \|\frac{M_0+M_1}{2}\|^{1/F_n}$ ?","['matrices', 'matrix-equations', 'linear-algebra', 'sequences-and-series']"
4211730,Lowenheim-Skolem number for satisfaction via continuous function rings,"EDIT: now asked at MO . For $\mathcal{X}$ a topological space, let $C(\mathcal{X})$ be the ring of continuous functions $\mathcal{X}\rightarrow\mathbb{R}$ . For a first-order sentence $\varphi$ in the language of rings, write $\mathcal{X}\models_{C}\varphi$ iff $C(\mathcal{X})\models\varphi$ in the usual sense. Say that $\varphi$ is C -satisfiable iff $\mathcal{X}\models_C\varphi$ for some topological space $\mathcal{X}$ . I'm generally curious about analogues of classical model-theoretic questions in this context. For example: What is the least cardinal $\kappa$ such that, if $\varphi$ is $C$ -satisfiable, then $\mathcal{X}\models_C\varphi$ for some space $\mathcal{X}$ with cardinality $<\kappa$ ? The obvious Lowenheim-Skolem argument doesn't help much here. Roughly speaking, given $\mathcal{X}\models_C\varphi$ we can form a ""small"" elementary substructure $(\mathcal{Y}, R)$ in the usual first-order sense of (an appropriate version of) the pair $(\mathcal{X},C(\mathcal{X}))$ . However, in general we might have $R\subsetneq C(\mathcal{Y})$ (this is similar to the distinction between Henkin and standard semantics for second-order logic). (Of course number of points isn't the only cardinal invariant for which we could ask a Lowenheim-Skolem flavored question, but it seems the simplest to start with. Another natural thing to do would be to restrict attention to a ""nice"" class of spaces, e.g. the compact Hausdorff spaces , but again this seems like a better first step.) One issue here is that I don't know an abstract characterization of which rings are isomorphic to some $C(\mathcal{X})$ . Obviously any such ring $R$ must be the underlying ring of a commutative associative unital $\mathbb{R}$ -algebra, but beyond that I don't see anything. I recall seeing a very snappy theorem addressing this, but I can't find it at the moment (I think it was more complicated than just ""any commutative associative unital $\mathbb{R}$ -algebra's underlying ring,"" but I'm not certain). My broader interest incidentally is in what happens if we replace $\mathbb{R}$ with some other topological structure (= first-order structure + topology such that all functions are continuous and all relations are closed, as subsets of the appropriate product space) . Given a topological structure $\mathcal{A}$ and a topological space $\mathcal{X}$ we can always turn the set of continuous maps from $\mathcal{X}$ to $\mathcal{A}$ into a structure of the same type as $\mathcal{A}$ by defining everything ""pointwise."" So for each topological structure $\mathcal{A}$ we get an analogue $\models_C^\mathcal{A}$ of the relation $\models_C$ above. Ultimately my interest is in developing a connection between ""coarse"" model-theoretic properties (e.g. variants of the Lowenheim-Skolem number) of $\models_C^\mathcal{A}$ and topological algebraic properties of $\mathcal{A}$ . While this question specifically asks about $\mathcal{A}=\mathbb{R}$ , if anyone has information about a different (""nontrivial"") $\mathcal{A}$ or class of $\mathcal{A}$ s I'd be very interested.","['model-theory', 'logic', 'functional-analysis', 'general-topology', 'set-theory']"
4211754,Is there a way to find errors in a Gaussian fit?,"If I have a set of data that $(x_i,y_i)$ could be visualized on a scatter plot, and I want to apply the least-square method to fit them using a Gaussian function: $$
G(x) = B+A\exp\left[-\left(\frac{x-\mu}{\sigma}\right)^2\right]
$$ I think this would give us an estimation of the expected value $\mu$ . However, is there a way I can find the error of those 4 parameters $A,B,\mu,\sigma$ using this method?","['statistics', 'least-squares', 'gaussian']"
4211777,"Choice of $\delta$ for ""brute force"" proof of continuity of exponential function $e^x$","I have read several answers ( example 1 , example 2 ) about continuity of $e^x$ , but most rely on Power Series definition of $e^x$ , or sequential definition of a limit, or squeeze theorem. I would like a brute-force proof that meets the following criteria: Does NOT use sequential definition of limit Does NOT use Squeeze Theorem Uses $\epsilon-\delta$ definition of continuity directly Does NOT use perturbations (e.g. $|e^{a + h} - e^a|$ ) Uses definition of limit, starting with a $0 < |x-a| < \delta$ and ending with $|e^x - e^a| < \epsilon$ Is NOT based on power series definition of $e^x$ Is based on elementary limit definition $e^x = \lim_{n \to 0} (1+n)^{\frac{x}{n}}$ I would like to use exponential bounds (which come from Bernoulli's Inequality) like this answer : \begin{align*}
y+1 \le \ & \ e^y \le \frac{1}{1-y} \\
\to \quad \quad y \le \ & \ e^y - 1 \ \le \ \frac{y}{1-y} \\
\to \quad x-a \le \ & \ e^{x-a}-1 \ \le \ \frac{x-a}{1-(x-a)}
\end{align*} except I am trying to modify that proof so it doesn't depend on Squeeze Theorem. Proof attempt : Let $\epsilon > 0$ and $a > 0$ arbitrary. Choose $\delta = \frac{\epsilon}{e^a}$ . Then \begin{align*}
& \quad 0 < |x - a| < \delta \quad \quad \quad \textrm{ (Given)}\\
&\to \quad |e^{x-a}-1| \quad < \delta \quad \quad \textrm{ (Reason unknown?)} \\
&\to \quad |e^{x-a}-1| < \frac{\epsilon}{e^a} \quad \quad \textrm{ (Substitute $\delta=\frac{\epsilon}{e^a}$)} \\
&\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by $e^a$)} \\
&\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute $e^a$ into absolute value)} \\
& \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)}
\end{align*} I know my proof is supposed to use the exponential bounds, $$x-a \le e^{x-a}-1 \le \frac{x-a}{1-(x-a)},$$ so I tried using it (probably incorrectly) in step 2. Just because $|x-a| < \delta$ doesn't mean $e^{x-a}-1$ (bigger) is also less than $\delta$ . It may be bigger than $\delta$ . So I am having trouble going from step 1 to step 2. Edit 7/29 ( $2^{nd}$ Proof Attempt) : Some comments are suggesting, based on Chappers' answer here , that I should choose $$\delta=\max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right|\right\}.$$ Making this substitution, our proof becomes \begin{align*}
& \quad 0 < |x - a| < \delta \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad\textrm{ (Given)}\\
& \quad 0 < |x - a| < \max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right| \right\} \quad \quad \quad \textrm{ (Substitution of $\delta$)}\\
&\quad \quad \quad \vdots \\
&\quad \quad \quad ? \\
&\quad \quad \quad \vdots \\
&\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by $e^a$)} \\
&\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute $e^a$ into absolute value)} \\
& \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)}
\end{align*} I am not sure how to fill in the gaps. The left hand side needs to somehow become $e^a |e^{x-a}-1|$ . The right hand side needs to become $\epsilon$ . But it seems to me that by making this choice, $\delta$ is no longer a function of $\epsilon$ . Edit 8/19 (Final proof): For those it helps, here's my final proof based off Paramanand Singh's answer: Let $\epsilon > 0$ and $a>0$ arbitrary. Choose $\delta= \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\}$ . Then \begin{align*}
& \quad \left|x - a\right| < \delta \tag{Given} \\
\to& \quad \left|x - a\right| < \frac{1}{2} \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{$\delta = \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\}$} \\
\to& \quad 2\left|x - a\right| < \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{Multiplication by 2} \\
\to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \min\left\{1,\frac{\epsilon}{e^a}\right\} \tag{$\left|\frac{h}{1-h}\right|<2|h|$ if $|h|<\frac{1}{2}$} \\
\to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \frac{\epsilon}{e^a} \tag{$\min\left\{1,\frac{\epsilon}{e^a}\right\}< \frac{\epsilon}{e^a}$} \\
\to& \quad \left|e^{x-a}-1\right| < \frac{\epsilon}{e^a} \tag{Exponential Bound Lemma} \\
\to& \quad e^a\left|e^{x-a}-1\right| < \epsilon \tag{Multiplication by $e^a$} \\
\to& \quad \left|e^a\cdot e^{x-a}-e^a\right| < \epsilon \tag{Distribution Property} \\
\to& \quad \left|e^x-e^a\right| < \epsilon \tag{$e^s\cdot e^t = e^{s+t}$} \\
\to& \quad \lim_{x \to a} e^x = e^a \tag{Definition of limit}
\end{align*} The above proof relies on the facts $e^x\cdot e^y=e^{x+y}$ and also the Exponential Bound Lemma $e^x \ge 1+x$ , which gives \begin{align*}
& \quad e^h \ge \left(1+\frac{h}{n}\right)^n \\
\to& \quad e^h \ge 1+h \\
\to& \quad e^{-h} \ge 1-h \\
\to& \quad e^h \le \frac{1}{1-h} \\
\to& \quad e^h-1 \le \frac{h}{1-h}.
\end{align*}","['epsilon-delta', 'real-analysis', 'continuity', 'calculus', 'limits']"
4211813,A metric on the Grassmannian manifold induced by the Riemannian metric,"In Wikipedia , they define the Grassmannian manifold by $$Gr(r,n)=O(n)/(O(r)\times O(n-r))$$ where $O(m)$ is the orthogonal group of $m\times m$ matrices. They say that it gives a metric on the Grassmannian by $$d(W,W^\prime)=\|P_W-P_{W^\prime}\|$$ where $P_V$ is the projection operator on a space $V$ . My questions are: In what sense does this metric $d$ rise from the formulation of the Grassmannian with the orthogonal group? Is $d$ equal to the metric induced by the Riemannian metric on $O(n)$ ? (and is this Riemannian metric given by $g_p(A,B)=tr(AB^{t})$ like $GL(n)$ ?) Why is this metric $d$ natural from the construction of the Grassmannian by the orthogonal group as was described above, and not from the alternative definition $GL(n)/H$ for $H$ the stabilizer of a $r$ -dimensional subspace? Are these constructions the same in the sense that the Riemannian metrics on both $GL(n)$ and $O(n)$ would induce the same metric $d$ described above?","['grassmannian', 'homogeneous-spaces', 'riemannian-geometry', 'differential-geometry']"
4211878,Prove cylinder local isometric to the plane,"Let $C = \{(x,y,z)\mid x^2+y^2 = 1\}\subset \Bbb{R}^3$ be the cylinder and $P = \{(x,y,z)\mid z = 0\}$ . Prove if we gives the induced metric to $C$ and $P$ from $\Bbb{R}^3$ .they are local isometry. Here local isometry means a smooth map $\varphi:P\to C$ such that $\varphi^*(g_c) = g_p$ . I have proved it clear,but seems trapped into some detail.Is there some clear proof of this exercise? I constructed as follows: $$\varphi:P\to C \\(x,y,0) \to (\cos x,\sin x ,y)$$ The hard part is to clearly show that it's local isometry. First pick local parametrization for $C$ as $X(u,v) = (\cos u,\sin u,v)$ .hence we see that :let $i_C :C\to \Bbb{R}^3$ ,then the induced metric on $C$ under the coordinate chart is $du^2 + dv^2$ .If we compute $\varphi^*(du^2 +dv^2) = d(u\circ\varphi)^2 + d(v\circ \varphi)^2$ where $u \circ \varphi (x,y,0) = x + C$ and $v \circ \varphi (x,y,0) = y + C$ ,for some constant $C$ , hence we have $\varphi^*(du^2 + dv^2) = dx^2 + dy^2$ on $P$ coinside with metric on $P$","['riemannian-geometry', 'differential-geometry']"
4211880,"Is $d$ a multiple of $4$ in the equation $(x+a)(x+b)(x+c)+d=(x+w)(x+y)(x+z)$ where $a$, $b$, $c$, $d$, $w$, $y$, and $z$ are integers?","I tried to find the value of integers $a$ , $b$ , $c$ , $d$ , $w$ , $y$ , and $z$ so that the polynomial $(x+a)(x+b)(x+c)+d$ and $(x+w)(x+y)(x+z)$ are equal. By using brute-force approach with Microsoft Excel, I find some solutions like: $(x-13)(x-12)(x-8) - 12 = (x-9)(x-10)(x-14)$ $(x-13)(x-10)(x-10) + 4 = (x-12)(x-9)(x-12)$ $(x+6)(x+7)(x+11) - 12 = (x+9)(x+10)(x+5)$ $(x-7)(x+3)(x-13) - 288 = (x-1)(x-15)(x-1)$ and so on. To my surprise, all the values of $d$ that I obtained are multiples of $4$ . Is it true that $d$ is always a multiple of $4$ ? If so, why? I tried to use Wolfram Alpha to find out why this happens but it gave the solutions $w$ , $y$ , and $z$ in terms of $a$ , $b$ , $c$ , and $d$ and the solutions are very long and complicated. Note: I have also tried to find the integers $a$ , $b$ , $c$ , $d$ , $m$ , $v$ , $w$ , $y$ , and $z$ that solve the equation $(x+a)(x+b)(x+c)(x+d)+m =(x+v)(x+w)(x+y)(x+z)$ and found that $m$ is also a multiple of $4$ . Maybe there is some generalization to what I found.","['number-theory', 'algebra-precalculus']"
4211906,Method of finding solution of $y'=y+e^x$,"( Edited after serious comments from Chongxu Ren, Ulrich, Gerry Myerson etc ; thanks to them for bringing my attention to put the question in precise form) I am trying to find the solution of differential equation $y'=y+e^x$ for $x\in\mathbb{R}$ . This can be solved by using method of integrating factor; but without referring to it, I went for looking solution step-by-step with complexity, as follows. Simple Case: If it would have been $y'=e^x$ , i.e. $Dy=e^x$ , we could have taken integration of both sides to get $y=ae^x+b$ . (The method of integrating factor brings above equation to form $D(e^{-x}y)=1$ ) General Case: The right side is not only the function of $x$ , but some terms of the (unknown) function $y$ also; an easy example is the equation in title. We then move to collect terms of $y$ on one side, and keep function of $x$ other side. We can write main equation as $$(D-\mathbf{1})y=e^x,$$ and we want to invert $D-\mathbf{1}$ to get information of unknown $y$ . [In case $Dy=f(x)$ , we expect to get $y=\int f(x)$ , provided, $f(x)$ satisfies some conditions on given domain of it.] For $(D-\mathbf{1})y=e^x$ , I went to do like: $y=\frac{1}{D-\mathbf{1}}e^x= (\mathbf{1}+D+D^2+\cdots )(-e^x)$ . But, this last expression on RHS does not make sense since $D^n(-e^x)=-e^x$ for all integers $n\ge 0$ . Question: When does such method of inverting $D-\mathbf{1}$ or a polynomial expression of $D$ actually works to give solution of given differential equation - say $(D^r + a_1D^{r-1} + \cdots + a_r\mathbf{1})y=f(x)$ ?",['ordinary-differential-equations']
4211947,What does X1 mean in the statistics?,"I'm trying to refresh my knowledge in mathematical statistics since my university days. I'm mostly using Russian resources but assume that it's applicable for others. What isn't really clear for me is the meaning of the $X_1$ . For example, in the book https://mipt.ru/diht/students/courses/mathematical_statistics.pdf we have a definition of unbiased estimator and an example of applying it to a sample mean on 11th page. I will duplicate the example here: $$ E_\theta \bar{X} = \frac{1}{n} \sum{E_\theta X_i} = E_\theta X_1 $$ The last transition isn't obvious to me. Perhaps, $X_1$ means the first element from the sample but I don't get why we chose exactly the first element and not, for instance, the 2nd. Thank you in advance!","['expected-value', 'statistics', 'probability-theory']"
4211954,"Let $f(x)$ be twice differentiable function, with $f(x) = x$ has $3$ roots","Let $f(x)$ is twice differentiable increasing  function everywhere such that $f(x) = x$ has $3$ distinct root $\alpha ,\beta$ and $\gamma$ $(\alpha  < \beta  < \gamma )$ . Let $h(x) = \underset{n\to \infty }{\mathop{\lim }}\,\underset{n\,\,\,times}{\mathop{(f(f(...(f(x))))}}\,$ . (1)    If $f‚Äô‚Äô(x) > 0$ $\forall \,x\,\in \,(-\infty ,\beta )$ , $f‚Äù(x) < 0$ $ \forall x\,\in \,(\beta ,\infty ]\,$ and $f''(\beta )=0,\,$ then find $h(x)$ (2)    If $f(x)\ge x\,\forall \,x\,\in \,(-\infty ,\alpha ]\,\cup \,[\gamma ,\infty )$ and $f(x)\le x\,\forall \,x\,\in \,[\beta ,\gamma ]\,$ then find $h(x)$ I tried with a fucntion $k(x) = f(x)-x$ , such that $k(\beta)$ is $0$ .
But how to do further.","['functions', 'derivatives']"
4211963,Intuition for homogeneous spaces?,"I would like to gain some intuition about homogeneous spaces.
According to the definition, these spaces admit a transitive group action. How I interpret what homogeneity tells us about a space (let¬¥s call it $X$ and the group action given by a group $G$ ): The maps on X coming from elements of the group preserve the structure of X Transitivity says that we can get from any element to any other element (for any elements $x$ , $y$ from the space, there is a $g$ from the group s.t. $x = gy$ .) Is this correct intuition for homogeneous spaces? And what else is ""hidden"" behind this definition? Since homogeneous spaces have interesting topological properties, I want to understand, how is homogeneity so important and what it tells about the space.","['group-theory', 'intuition', 'homogeneous-spaces', 'group-actions', 'general-topology']"
4211972,Solving the equation $3\cos x+\frac{1}{2}=\cos ^{2}x\left(1+2\cos x\left(1+4\sin ^2x\right)\right)$,"Solve the equation: $$3\cos x+\frac{1}{2}=\cos ^{2}x\left(1+2\cos x\left(1+4\sin ^2x\right)\right)$$ To solve it, I tried writing the equation in term of $\cos x$ . ( I denote $\cos x$ by $c$ ): $$3c+\frac12=c^2(1+2c(5-4c^2))$$ $$3c+\frac12=c^2+10c^3-8c^5$$ $$16c^5-20c^3-2c^2+6c+1=0 \qquad\text{Where $c\in[-1,1]$}$$ I tried $c=\pm1,\pm\frac12,0$ , but neither of them satisfied the equation. So I don't know how to find $\cos x$ .","['algebra-precalculus', 'trigonometry']"
4212017,Showing that the corresponding two spaces are homeomorphic.,"I want to show that the following map induces a homeomorphism between the two structures. $\require{AMScd}$ \begin{CD}
    \mathbb{B}^{n} @>f(x_1,\cdots , x_n) = (x_1 , \cdots x_n , \sqrt{1- \sum_{i=1}^n(x_i)^2})>>  \mathbb{S}^n @>\pi_1(x) = cl(\{x\}) >> \mathbb{S}^{n} / \ \sim_2 \\
     @VV\pi_2(x) = cl(x)V \\
      \mathbb{B}^{n} / \sim_1  \\
\end{CD} where $\sim_1$ signifies identifying the antipodal points of the boundary circle of $\mathbb{B}^n$ and $\mathbb{S}^n / \sim_2$ signifies identifying the antipodal points of $\mathbb{S}^n$ . I want to show that $\mathbb{S}^n / \sim_2$ and $\mathbb{B}^n / \sim_1$ are homeomorphic using the theorem, $\require{AMScd}$ \begin{CD}
    \mathbb{X} @>g(x)>> Y \\
     @VV p V  \\
     \  \mathbb{X} / \sim  \\
\end{CD} where $g(x)$ is an identification map and $p$ is a projection map. The part where I am stuck at is: 1)Showing that $\pi_1 \circ f (x) $ is a surjective map. 2)Is showing that $\pi_1 \circ f(x)$ surjective enough? Or is there anything else we have to show to prove that the two maps are homeomorphic? Edit $1$ :Let $(x_1, \cdots , x_{n+1}) \in  \mathbb{S}^n/\sim_2$ then, case $1:$ Assume that $x_{n+1} > 0$ then $\pi_1^{-1}((x_1,\cdots ,x_{n+1})) = (x_1, \cdots ,x_{n+1})$ where $\sum_{i=1}^{n+1}(x_i)^2 = 1$ . $f^{-1}(x_1,\cdots,x_{n+1}) = (x_1,\cdots x_n)$ s.t. $\sum_{i=1}^n(x_i)^2 < 1$ . Then $$\pi_2(x_1, \cdots x_n) = (x_1 , \cdots x_n)$$ case $2:$ Assume that $x_{n+1} = 0$ then $(x_1,\cdots x_{n+1})$ is a point on the equator of the sphere. Then, $\pi_1^ {-1} (x_1,\cdots x_{n},0)= \{[x],[x]'\} $ where $[x]= (x_1, \cdots ,x_{n+1})$ and $[x]'$ is the antipodal point of $[x]$ . Also $\sum_{i=1}^n(x_i)^2 = 1$ . I am stuck in this part.","['general-topology', 'analysis']"
4212018,Why can't the Quaternion Group definition be satisfied by a pair of $\mathbb{Z}_2$ elements?,"There is a quaternion group definition which states $$Q_8 = \langle a, b \mid a^4 = 1, a^2 = b^2, ba = a^{-1}b\rangle$$ where $1$ is the identity element. This is suppose to produce eight distinct elements, but I believe I've found a way to do it with four. Let $a = ([0], [1])$ and $b = ([1], [0])$ , where $[0], [1] \in \mathbb{Z}_2$ , and let the group operation be addition of the elements of the pairs, $$([w], [x])([y], [z]) = ([w + y], [x + z]).$$ In this case, $1 = ([0], [0]) \neq a \neq b$ , $(a^2)^2 = a^4 = 1$ , $a^2 = 1 = b^2$ , and $ba = ab = a^{-1}b$ . Every condition is satisfied. You could also think of this as two reflections in $D_4$ , a square. Two perpendicular reflections $r_1$ and $r_2$ would satisfy all the same properties. The definition of a quaternion is supposed to produce a more complicated set of elements than this, but I can't figure out why four elements don't cut it. What am I missing about generators or the definition to cause my interpretation to be so different from what was intended?","['group-presentation', 'group-theory', 'abstract-algebra', 'quaternions']"
4212071,Lefschetz hyperplane theorem for etale homotopy groups,"Lefschetz hyperplane theorem : Let $X$ be a smooth projective variety over $\mathbb{C}$ and $H$ be a hyperplane section of $X$ such that $X \setminus H$ is a smooth affine(automatically true in this situation) algebraic variety, then the map $\pi_i(H(\mathbb{C})) \rightarrow \pi_i(X(\mathbb{C}))$ induced by the inclusion $H \hookrightarrow X$ on the homotopy group is an isomorphism for $i< \dim X - 1$ and is injective for $i = \dim X$ . Now, let us assume that $X$ is defined over a field $K$ of characteristic zero and $H$ as before. We can prove that $\pi^{et}_1(H) \rightarrow \pi^{et}_1(X)$ is an isomorphism if $\dim X \geq 3$ using weak Lefschetz theorem and the comparison theorem for topological fundamental group of $X(\mathbb{C})$ and etale fundamental group of $X$ . Here we can use any embedding $K \hookrightarrow \mathbb{C}$ . It seems that that the isomorphism on etale fundamental group continues to hold in positive characteristic under certain assumptions ( $H$ is also a regular hyperplane section.). Let us now turn to etale version of Lefschetz hyperplane theorem conerning higher homotopy groups. I am new to this topic so I might be making several mistakes here. There do exist notions of higher etale homotopy groups which was first developed by Artin-Mazur. I have come to know that the first fundamental group computed in this theory does not match up with the Grothendieck's etale fundamental group but with the fundamental group defined in SGA3. There is an agreement when $X$ is a normal noetherian scheme. With this in mind, we may ask the following question : Let $X$ be a smooth projective variety over a field $K$ which can be of positive characteristic and let $H$ be moreover regular. Then what can we say about the induced map on higher etale homotopy groups? are they isomorphic in degrees up to $\dim X - 1$ ?","['algebraic-geometry', 'homotopy-theory']"
4212105,Periodicity of derivatives of function,"Can we find a function $f:\mathbb{R}\to\mathbb{R}$ , such that it's derivatives repeat after a certain period $T$ . For example, exponential function $f(x)=Ae^x$ satisfies $f(x)=f'(x)=f''(x)=\cdots$ and in this case we can call the period to be $1$ . Now, I also found a function such that $f(x) \ne f'(x) $ but $f(x)=f''(x)$ and it repeats at a period of $2$ . The example is $f(x)=Ae^x+Be^{-x}$ . However, I was unable to find a function whose period is $3$ . Although, for period $4$ , we have trigonometric functions, but I think it would be possible to find a class of functions, in which we can find such functions according to our wish by just varying $T$ . It seems we can manipulate exponential functions so that it is possible for any $T$ . Any ideas would be appreciated! I found this too: Functions that are their Own nth Derivatives for Real $n$ but I'm not sure if this is what I am looking for, because I am looking for real valued functions, and the answer given here gives functions in terms of $n$ th roots of unity.","['calculus', 'derivatives', 'exponential-function']"
4212107,Ricci tensor for warped submersion,"In the setting that we have a Riemannian submersion under the $S^1$ action for $M=\mathbb{R}_{\geq 0}\times S^{2n+1}$ with the metric $k(t) = -dt^2+\alpha^2(t)g + \beta^2(t)h$ where $g$ is the horizontal and $h$ the vertical part of the metric and $ds^{2n+1} = g+h$ , I want to calculate the Ricci tensor. For simplicity I leave out all the terms that involve the vector field $\partial_t$ . I started with a ONB $\{E_\lambda\}$ of $(S^{2n+1},ds^{2n+1})$ where $g(E_i,E_i) = h(E_{2n+1},E_{2n+1})=1$ , $i\neq2n+1$ . Now I used Koszul formula to calculate the Levi-civita connection. I got $\nabla_{E_i} E_j = \nabla^{2n+1}_{E_i}E_j$ $\nabla_{E_i} E_{2n+1} = \frac{\beta^2}{\alpha^2}\nabla^{2n+1}_{E_i}E_{2n+1}$ $\nabla_{E_{2n+1}} E_{2n+1} = \frac{\beta^2}{\alpha^2}\nabla^{2n+1}_{E_{2n+1}}E_{2n+1}$ $\nabla_{E_{2n+1}} E_{i} = \frac{\beta^2}{\alpha^2}\mathscr{H}\nabla^{2n+1}_{E_{2n+1}}E_{i} + \mathscr{V}\nabla^{2n+1}_{E_{2n+1}}E_{i}$ Here $\mathscr{H}, \mathscr{V}$ denote the projections onto horizontal/vertical subspaces. But now when I use these results to calculate the Ricci tensor, I get in trouble. For example, if I calculate $k(\nabla_i\nabla_{j}E_j - \nabla_j\nabla_i E_j - \nabla_{[E_i,E_j]}E_j,E_i) 
\\= k(\nabla_i^{2n+1}\mathscr{H}\nabla_j^{2n+1}E_j - \nabla_j^{2n+1}\mathscr{H}\nabla_i^{2n+1}E_j - \nabla^{2n+1}_{\mathscr{H}[E_i,E_j]}E_j,E_i) + \frac{\beta^2}{\alpha^2}k(\nabla_i^{2n+1}\mathscr{V}\nabla_j^{2n+1}E_j - \nabla_j^{2n+1}\mathscr{V}\nabla_i^{2n+1}E_j - \nabla^{2n+1}_{\mathscr{V}[E_i,E_j]}E_j,E_i)$ where I don't know how to simplify this.","['manifolds', 'riemannian-geometry', 'differential-geometry']"
4212112,Conditions on $f$ and $g$ if $(g\circ f)^{-1}$ exists,"Consider functions $f:A\to B$ and $g:B\to C$ $(A,B,C\subseteq R)$ such that $(g\circ f)^{-1}$ exists, then : (1) $f$ is onto and $g$ is one-one (2) $f$ is one-one and $g$ is onto (3) $f$ and $g$ are both one-one (4) $f$ and $g$ are both onto One of the above options is supposed to be correct but I think for inverse to exist we must have bijection so both (3) and (4) should be correct. I am little confused","['functions', 'function-and-relation-composition', 'inverse-function']"
4212122,Elliptic Integral along complex path,"I am interested in the following integral \begin{align}
\int \frac{dt}{t\sqrt{(t-e_1)(t-e_2)(t-e_3)}}
\end{align} where $e_1=\bar{e}_3$ and $e_2\in\mathbb{R}$ is negativ. Assume that the branch cut of the square root goes to minus infinity. Also assume that the real part of $e_1$ , and hence of $e_3$ , is greater than zero. I want to perform the integration along the straight vertical path from $e_1$ to $e_3$ and rewrite it in terms of elliptic integrals of first, second or third kind. A suitable reference is of course the book Handbook of elliptic functions by Byrd, Friedman and on page 89 they the consider this integral by integrating along the real line. For example, integrating  from $e_2$ to minus infinity the integral takes a particular easy form (see page 94, eq. 245.06 and page 216, eq. 361.60 with $f_1=0$ ). I feel like the form for integrating from $e_1$ to $e_3$ should be similar since we are still integrating between the branch points. Maybe someone can help me or post a reference for similar problems? Edit: The case I mentioned above is \begin{align}
  \int_{-\infty}^{e_2}\frac{dt}{t\sqrt{(t-e_1)(t-e_2)(t-e_3)}}=c\int_{0}^{u_1}\frac{1-cnu}{1+\alpha\, cn u}
\end{align} for some constants $c,\alpha$ depending on the $e_i$ and $cn u_1=-1$ . The right hand side can be simplified to \begin{align}
  \frac{1}{\alpha}(-2K(m)+\frac{2}{1-\alpha}\Pi(\frac{\alpha^2}{\alpha^2-1},m))
\end{align} where $K$ and $\Pi$ are the complete elliptic integrals of the first and third kind, respectively. Edit2: I kind of found the solution by myself but this will probably not work in the general case because I have some additional information. Nevertheless, maybe the edit will help someone else. The following calculation should be right modulo signs. Assume that the imaginary part of $e_1$ is greater than zero (which is no restriction since we can just exchange $e_1$ and $e_3$ ). The additional information I have is that I know the imaginary part of the integral \begin{align*}
  \int_{e_1}^{e_2}\frac{dt}{t\sqrt{(t-e_1)(t-e_2)(t-e_3)}}.
\end{align*} Denote the imaginary part of this integral by $v$ . Since $\int_{e_1}^{e_3}$ is completely imaginary and $\int_{e_1}^{e_3}+\int_{e_3}^{\infty}+\int_{\infty}^{e_1}=0$ by the residue theorem we get $\textrm{Im}\int_{e_1}^{\infty}=\frac{1}{2}\textrm{Im}\int_{e_1}^{e_3}$ . On the other hand $\int_{e_1}^{e_2}+\int_{e_2}^{e_3}+\int_{e_3}^{e_1}=2\pi i$ and hence $\int_{e_3}^{e_1}=2\pi i - 2 i v$ (as the imaginary part of $\int_{e_1}^{e_2}$ and $\int_{e_2}^{e_3}$ are the same). Therefore \begin{align*}
  \textrm{Im}\int_{e_1}^{\infty}=\pi  - v.
\end{align*} By holomorphicity $\int_{e_1}^{e_2}=\int_{e_1}^{-\infty}+\int_{-\infty}^{e_2}$ (above the branch cut). However, we also have $\textrm{Im}\int_{e_1}^{\infty}=\textrm{Im}\int_{e_1}^{-\infty}$ we and hence \begin{align*} 
  \textrm{Im}\int_{e_1}^{\infty}=v-\textrm{Im}\int_{e_2}^{-\infty}.
\end{align*} The integral $\int_{e_2}^{-\infty}$ is purely imaginary. Putting these equations together we can express the integral $\int_{e_1}^{e_3}$ in terms of some constants and the formula for $\int_{e_2}^{-\infty}$ in my first edit. Again, I did not take care of the signs in the integrations at all so be careful there. Remark: For the general case I tried to identify $\wp=x$ and then use the relations of Weierstrass elliptic functions for rhombic tori, i.e. $\wp(z)=e_2+H_2\frac{1+cn(z';m)}{1-cn(z';m)}$ (see Abramowitz/Stegun p. 649 for the notation).  We can integrate the Jacobi ellitpic functions (eq. 361.60 from Handbook of ellitpic functions ) and for the elliptic integral of the third kind I tried using the identities of the book Elliptic functions and applications, Lawden on page 76. But unfortunately I did not succed and mathematica always gave me wrong results.","['integration', 'elliptic-curves', 'elliptic-functions', 'indefinite-integrals', 'elliptic-integrals']"
4212129,Proof that f = g a.e. with Fatou's lemma,"In the book ""A user friendly introduction to Lebesgue measure and integration"" by Nelson, Exercise 24 in Ch. 2 states: Let $(f_n)$ be a sequence of functions in $\mathcal L [a,b]$ . Suppose $f \in \mathcal L [a,b]$ and $$
\lim_{n\to \infty}\int_a^b|f_n-f| = 0.
$$ If the sequence $(f_n)$ converges pointwise a.e. on $[a,b]$ to the function $g$ , show that $f=g$ a.e. on $[a,b]$ .
Suggestion: Consider the sequence $(|f-f_n|)$ and Fatou's Lemma. My try: From $\lim_{n\to\infty} f_n  =g$ a.e., we know that $\lim_{n\to\infty}  f_n -f  =  g - f$ a.e. and $\lim_{n\to\infty} | f_n -f | = | g - f|$ a.e.. Integrating, we have \begin{align}
0\leq \int_a^b|g-f|&=\int_a^b\lim_{n\to\infty}|f_n -f|=\int_a^b\liminf_{n\to\infty}|f_n -f|\\
&\leq \liminf_{n\to\infty}\int_a^b|f_n -f| = 0,
\end{align} where the second inequality follows from Fatou's lemma and the last step from the assumption that $\lim_{n\to \infty}\int_a^b|f_n-f| = 0$ . We conclude that $|g-f| = 0$ a.e.. Is this the correct direction to approach the exercise? I don't see how to go from the above result to showing that $f=g$ a.e.. I see that $\int_a^b(g-f) \leq \int_a^b|g-f|$ ...","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4212143,Definite integral of $\int e^{-\sin^2x}\cos\Bigl(6x-\frac{\sin(2x)}{2}\Bigr)dx$,"I have the integral with $\sin()$ sum expression in $\cos()$ argument: $$\int_0^{2\pi}e^{-\sin^2x}\cos\Bigl(6x-\frac{\sin(2x)}{2}\Bigr)dx.$$ Can anyone please explain an algorithm for solving it? I've tried so far: Weierstrass substitution (if I can use it with $\int \cos(f(x))dx$ ), Euler formula, integration by parts, formula of $\cos(x-y)$ and $\sin(2x)$ formula and everything not seems to work or I make actions not in the right order. Need some fresh view on the problem. Thanks!","['integration', 'trigonometric-integrals', 'definite-integrals']"
4212146,"A simpler way to minimize the mean of squares, given the mean?","Suppose I have $N$ real numbers and I already know their mean, $\bar{x}$ : $$
\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_i
$$ (but I don't know the individual values $x_1,x_2,\dots,x_N)$ . I want to find the smallest possible value of the mean of the squares, $y$ : $$
y=\frac{1}{N}\sum_{i=1}^{N}x_i^2
$$ I thought a bit about the case where $N=2$ and it seemed like the answer should be $x_1=x_2=\bar{x}$ . For example, if $\bar{x}=5$ , then $5^2+5^2=50$ , but $4^2+6^2=52$ and $3^2+7^2=58$ , and so on. I feel like this should be a very simple thing to prove for any $N$ , but I had to dig into the back of an old (engineering) textbook to recall the method of Lagrange multipliers... which (unless I made a mistake) indeed gave me the answer $x_1=x_2=x_3=\dots=x_N=\bar{x}$ . Or, using the vector notation of the textbook: $\underline{x}=\bar{x}\underline{1}$ . I wrote my calculations below, just in case this result is wrong. My question is : Is there a ""simpler"" way to prove this result (e.g. using basic high school mathematics)? Is it over-complicated to view this as a constrained optimization problem? Many thanks for any help. My Calculations: Function to minimize (subject to constraint): $$f(\underline{x})=\frac{1}{N}\underline{1}^T\underline{x}^2$$ Constraint: $$c(\underline{x})=\frac{1}{N}\underline{1}^T\underline{x}-\bar{x}=0$$ Unconstrained function to minimize: $$h(\underline{x})=f(\underline{x})+\lambda c(\underline{x})$$ Minimize to get $\lambda$ : $$\frac{\partial f}{\partial \underline{x}} + \frac{\partial}{\partial \underline{x}}\left( \lambda c(\underline{x}) \right) = \underline{0}$$ $$\frac{2}{N}\underline{x} + \frac{\lambda}{N}\underline{1} = \underline{0}$$ $$\frac{2}{N}\underline{1}^T\underline{x} + \frac{\lambda}{N}\underline{1}^T\underline{1} = 0$$ $$\lambda = -2\bar{x}$$ Substitute $\lambda$ back in to solve for $x$ : $$\frac{2}{N}\underline{x} = \frac{2\bar{x}}{N}\underline{1}$$ $$\underline{x} = \bar{x}\underline{1}$$","['optimization', 'calculus', 'lagrange-multiplier', 'statistics']"
4212158,Method of Moments estimation,"I am sorry in advance if this question seems like low effort, but I really do not know how to solve this problem. My understanding of method of moments estimation is bad. Here is the definition of method of moments estimation in my book: Let $\{X_1,X_2,...,X_n\}$ be a random sample from a population $F(x;\theta)$ . Suppose $\theta$ has $p$ components (for example, for a normal popoulation $N(\mu, \sigma^2),p=2$ ; for Poisson population with parameter $\lambda$ , $p=1$ ). Let $$\mu_k=\mu_k(\theta)=E(X^k)$$ denote the kth population moment, for k=1,2,... Therefore, $\mu_k$ depends on the unknown parapemter $\theta$ , as everything else about the distribution $F(x;\theta)$ is known. Denote the $k$ th sample moment by: $$M_k=\frac{1}{n}\sum^n_{i=1}X_i^k=\frac{X_1^k+X_2^k+...+X_n^k}{n}.$$ The MM estimator (MME) $\hat{\theta}$ of $\theta$ is the solution of the $p$ equations $$\mu_k(\hat{\theta})=M_k \text{ for } k=1,2,...,p$$ So my problem I am struggling to solve is the following: An economist decides to model the distribution of income in a  country with the probability density function: $$f_x(x;\alpha,k)=\frac{\alpha k^{\alpha}}{x^{\alpha+1}} \text{ for } x\geq k$$ and $0$ otherwise, where $k>0$ and $\alpha >2$ . Let $\{X_1,X_2,...,X_n\}$ be a random sample of size $n$ from this distribution. You may use the fact that: $$E(X)=\frac{\alpha k}{\alpha-1} \text{ and } E(X^2)=\frac{\alpha k^2}{\alpha-2}$$ Show that the method of moments estimators of $\alpha$ and $k$ are the solutions of: $$\frac{1}{\hat{\alpha}(\hat{\alpha}-2)}=\left(\frac{n-1}{n}\right)\frac{S^2}{\overline{X}^2} \text{ and } \hat{k}=\frac{(\hat{\alpha}-1)\overline{X}}{\hat{\alpha}}$$ where: $$\overline{X}=\frac{1}{n}\sum^n_{i=1}X_i\text{ and } S^2=\frac{1}{n-1}\sum^n_{i=1}(X_i-\overline{X})^2$$ My attempt to solve this: We have that $\mu_1(\alpha,k)=\frac{\alpha k}{\alpha-1}$ , $\mu_2(\alpha,k)=\frac{\alpha k^2}{\alpha - 2}$ So we have to solve the following equations: $$\mu_1(\alpha,k)=M_1 \text{ and } \mu_2(\alpha,k)=M_2$$ We have that $$\mu_1(\alpha,k)=M_1\Rightarrow \frac{\alpha k }{\alpha-1}=\overline{X}$$ Now, $$\mu_2(\alpha,k)=\frac{1}{n}\sum^n_{i=1}X^2_i\Rightarrow \frac{\alpha k^2}{\alpha-2}=\frac{1}{n}\sum^n_{i=1}X^2_i$$ So from the first equation we have that $$\frac{\alpha k}{\overline{X}}-1=\alpha-2$$ Plugging $\alpha-2$ into the first equation we get $$\frac{\alpha k^2}{\frac{\alpha k}{\overline{X}}}=\frac{1}{n}\sum^n_{i=1}X^2_i\Leftrightarrow k\overline{X}=\frac{1}{n}\sum^n_{i=1}X^2_i\Leftrightarrow \hat{k}=\frac{1}{\overline{X}}\sum^n_{i=1}X^2_i$$ Now I'd like to find $\hat{\alpha}$ . So for $k$ we plug in $\hat{k}$ (in equation $\mu_1(\alpha,k)=M_2$ ): $$\frac{\alpha k}{\alpha -1}=\overline{X}\Leftrightarrow \frac{alpha\frac{1}{n\overline{X}}\sum^n_{i=1}X^2_i}{\alpha-1}=\overline{X}\Leftrightarrow \frac{alpha \sum X^2_i}{n\overline{X}(\alpha-1)}=\overline{X}\Leftrightarrow \alpha = \frac{(\alpha-1)n\overline{X}^2}{\sum X_i^2}\text{ this is equation (1) }$$ Now we plug in $\hat{k}$ in the equation $\mu_2(\alpha,k)=M_2$ : $$\frac{\alpha k^2}{\alpha-2}=\sum X_i^2\Leftrightarrow \frac{\alpha\frac{1}{\left(n\overline{X}\right)^2}\left(\sum X^2_i\right)^2}{\alpha-2}=\sum X^2_i \Leftrightarrow \alpha = \frac{\left(n\overline{X}\right)^2(\alpha-2)}{\sum X^2_i} \text{ this is equation (2) for alpha }$$ Equating the 2 equations of $\alpha$ and solving for $\alpha$ : $$\frac{\left(n\overline{X}\right)^2(\alpha-2)}{\sum X_i^2}=\frac{(\alpha-1)n \overline{X}^2}{\sum X^2_i}$$ $$\Leftrightarrow (\left(n\overline{X}\right)^2(\alpha-2)=n\overline{X}^2(\alpha-1)\Leftrightarrow $$ $$\alpha\left(\left(n\overline{X}\right)^2-n\overline{X}^2\right)=2(n\overline{X}^2-n\overline{X}^2\Leftrightarrow \hat{\alpha}=\frac{2n-1}{n-1}$$ Well now that I have these, I don't know how they are the solutions of $$\frac{1}{\hat{\alpha}(\hat{\alpha}-2)}=\left(\frac{n-1}{n}\right)\frac{S^2}{\overline{X}^2} \text{ and } \hat{k}=\frac{(\hat{\alpha}-1)\overline{X}}{\hat{\alpha}}$$ Did I solve for the estimators correctly? How do I go on from here?","['statistics', 'probability']"
4212226,Solve for x : $\left(\sin x\right)^{\tan x}=\left(\cos x\right)^{\cot x}$,"Solve for x: $$\left(\sin x\right)^{\tan x}=\left(\cos x\right)^{\cot x}$$ My Attempt: One obvious solution is obtained by writing the given equation as $$\left(\sin x\right)^{\tan x}=\left(\sin(\frac{\pi}{2}- x)\right)^{\tan(\frac{\pi}{2}- x)}$$ Clearly, $x=\frac{\pi}{4}$ and $x=\frac{5\pi}{4}$ are possible solutions. So the general solution can be written as $x=\frac{\pi}{4}\pm n\pi$ . But how can I rule out the other solutions or there really aren't other solutions. Also,if I let $f(x)=\left(\sin x\right)^{\tan x}$ ,then given equation is equivalent to $f(x)=f(\frac{\pi}{2}-x)$ i.e. $f(\frac{\pi}{4}-x)=f(\frac{\pi}{4}+x)$ Can this lead to some conclusion","['trigonometry', 'exponential-function', 'monotone-functions']"
4212238,Is $4| \left(\prod_{i=1}^n a_i-\prod_{i=1}^n b_i\right)$ always true?,"Question: Suppose $a_i$ , and $b_i$ are all integers, $1\leq i\leq n$ , $n\geq 3$ , and the following conditions are known: $$\sum a_i=\sum b_i {\tag 1}$$ For every $k \in \mathbb{Z}$ , where $2\leq k \leq n-1$ , we have $i_1, i_2,...,i_k \in \{1,2,...,n\}$ , and $i_1, i_2, ..., i_k$ are all distinct, then it is true that: $$\sum_{symmetric} a_{i_1}\cdot a_{i_2}...\cdot a_{i_k}=\sum_{symmetric} b_{i_1} \cdot b_{i_2} \cdot...\cdot b_{i_k} {\tag 2}$$ If $(1)$ and $(2)$ are true, is it true that $\prod a_i \equiv \prod b_i \mod 4$ ?
If this is true, is this property unique to $\mod 4$ ? Motivation: I came across this post here today, and this problem is the generalized case. I have posted a solution via brute-force for the linked question, for the $n=3$ case, where it holds. But my approach provided no insight for a general solution, which would be more interesting to me. Attempt: I tried to come up with an alternative solution for the $n=3$ case itself, hoping that perhaps that would be useful for a generalization. But even this I couldn't finish. Here is my attempt: $$a+b+c=w+z+y {\tag 3}$$ and $$ab+bc+ac=wz+wy+zy {\tag 4}$$ We wish to arrive at $$abc \equiv wyz \mod 4$$ Squaring $(3)$ and using $(4)$ allows us to arrive at $a^2+b^2+c^2=w^2+z^2+y^2$ .
This means that $a^3+b^3+c^3-3abc= w^3+y^3+z^3-3wyz$ Hence, if I could show that $$\sum a^3 \equiv \sum w^3 \mod 4$$ our problem is finished. But this is where I got stuck. Edit: Although I could make no further advances on the problem, a more interesting reformulation of the problem might be note-worthy-
The conjecture is that any $n$ degree polynomial with $n$ integer roots, if after translation along the $y$ axis gives a polynomial which again has $n$ integer roots, then we must have shifted the original polynomial by $4k$ units, $k \in \mathbb{Z}$ . Is this true?","['number-theory', 'elementary-number-theory']"
4212258,On well-definedness of Shimura curve.,"Suppose that we have a quaternion algebra $D$ over a totally real number field $K$ such that $[K \colon {\Bbb Q}] = {\mathrm{odd}}$ . We assume that $D$ splits everywhere at finite places of $K$ and at one infinite place of $K$ . (Hence, $D$ is ramified at $[K \colon {\Bbb Q}] -1$ infinite places.) Suppose we have an Eichler ordre ${\cal O}_N$ of level $N$ in $D$ . ${\cal O}_N$ is defined as the intersection of two maximal ordres of $D$ and level $N$ is defined as the index of ${\cal O}_N$ in any maximal ordre which contains ${\cal O}_N$ . We shall fix one isomorphism $\phi \colon D \otimes_{\Bbb Q} {\Bbb R} \cong M_2({\Bbb R})$ . Take ${\cal O}_N^{(1)} \subset {\cal O}_N$ which consists of norm $1$ elements. Q. Is Shimura curve $C \colon = {\Bbb H}/{\cal O}_N^{(1)*}$ well-defined? That is, ${\cal O}_N$ is defined uniquely up to conjugate in $D$ ? I.e., type number of ${\cal O}_N$ is one. By the way, I have learned that $C$ is actually well-defined as follows: Shimura curve over ${\Bbb C}$ is just a quotient space $C \colon= D^* \backslash D_A^*/\widehat{{\cal O}_N}^*F_{\infty}^*C$ for a maximal compact subgroup $C$ . So it is a union of $a \widehat{{\cal O}_N} a^{-1} \cap D^* \backslash {\Bbb H}$ for  a running over a complete representative set of $D^* \backslash D_A^*/{\widehat{R}}^*D_{\infty}^*$ . That is, all adelic conjugacy classes of ${\cal O}_N$ appears once. I cannot very well understand this explanation, although I know it is correct. My biggest question is that when one speaks of uniformisation, one must once and for all choose arithmetic Fuchsian group up to conjugacy. Hence the uniquness of Eichler ordre ${\cal O}_N$ in $D$ must be ensured somehow in ordre to give the precise definition of Shimura curve for our quaternion algebra.","['algebraic-curves', 'complex-geometry', 'algebraic-geometry', 'division-algebras', 'arithmetic-geometry']"
4212322,When does $F'(c)$ exist? When does $F'(c) = f(c)$?,"If $f : [a,b] \to \mathbb{R} $ is Riemann integrable, we may define the function $F : [a,b] \to \mathbb{R}$ given by $$
F(x) = \int_a^x f(t) dt
$$ The first fundamental theorem of calculus says that if $f$ is continuous at $c \in (a,b)$ , then $F'(c) = f(c)$ . If $f$ is not continuous at $c$ , then $F'(c)$ can fail to exist, and, if it does exist, it can happen that $F'(c) \neq f(c)$ . I'm wondering if there are easily stated necessary and sufficient conditions for when: $F'(c)$ exists. $F'(c) = f(c)$ . I've given this a little thought, but all I can see is that we have the following equivalences: $F'(c)$ exists $\iff$ $\lim_{h \to 0} \frac{\int_c^{c + h} f(t) dt}{h}$ exists $F' (c) = f(c) \iff \lim_{h \to 0} \frac{\int_c^{c + h} \left[f(t) - f(c)\right] dt}{h} = 0$ These of course are necessary and sufficient conditions, but, to my knowledge, they don't have a special name. Morally, 1 says that averages of $f$ don't ""blow up"" as intervals get smaller, and 2 says that $f$ is continuous ""in the mean."" Is there a named condition equivalent to 1 or 2?","['limits', 'calculus', 'derivatives']"
4212324,Worked Exercises about Residues of Differentials,"I would first like to say that if this post is inappropriate for this StackExchange please let me know and I will remove it. A few days ago I posted a question asking about Understanding the need for Residues of Differentials and I got a couple of comments with some ideas and a suggestion for me to look at geometry first. My issue is that I can't seem to find a lot of information on Residues of Differentials, and moreover, no worked exercises on their use within complex analysis. The book that I am following, Serge Lang's Introduction to Complex Analysis at a Graduate Level, does have some exercises, however, they are not worked. I did ask the people who commented on my original post if they could share any usefull resources on this matter but both of them have yet to respond (I am not too sure if the comment system notifies people on replies?). My question: Are there any standard examples of the use of differentials of residues in complex analysis that one should know? Moreover,  does anybody have any resources/notes ,that they are willing to share, on the topic that also include worked exercises?","['complex-analysis', 'residue-calculus', 'reference-request', 'differential-geometry']"
4212328,Find all integers $n$ such that the group of units of $\mathbb{Z}/n\mathbb{Z}$ is an elementary abelian $2$-group.,"Question: Find all integers $n$ such that the group of units of $\mathbb{Z}/n\mathbb{Z}$ is an elementary abelian $2$ -group. Thoughts: I know that $u$ is a unit in $\mathbb{Z}/n\mathbb{Z}$ if and only if $n$ and $u$ are coprime.  So, in each ring, there are $\phi(n)$ units.  But I am not sure if there is a ""nice"" way to see if every non-trivial element in $U(n)$ has order $2$ .  I was thinking of breaking it down into even and odd $n$ , or considering how many units each ring has, but I wasn't able to notice anything useful.  Any help is greatly appreciated!  Thank you. The question is asked here: For what values of $n$ is $U(\mathbb{Z}_n)$ an elementary abelian 2-group? , but I wasn't able to get to a conclusion on based on the last hint.","['ring-theory', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4212364,Representations of an algebraic group $G$ versus representations of the group $G(k)$,"The following question stems from the discussion here . Let $G$ be a group scheme, $G$ acting on a vector space $V$ over a field $k$ is morphism of group valued functors $G \rightarrow GL_V$ , where $GL_V$ is the functor on commutative $k$ -algebras defined by: $$
R \mapsto \text{Aut}_{R-\text{Mod}}(V\otimes_k R)
$$ This is I believe a standard definition and is used by J.S. Milne in his book on algebraic groups . Consider the case when $G$ is actually a variety, over an algebraically closed field $k$ , say an affine or quasi-projective one. Then from a geometry perspective I can think of $G$ as being it's set of $k$ points like chapter 1 of Hartshorne and $G(k)$ is just an abstract group, but when $G$ has coordinates then $G(k)$ is defined by polynomials and the group operations are given by regular functions. Lets say $G(k)$ acts on some vector space. When is this equivalent to the scheme theoretic notion above? Example when these notions don't allign even though they might seem to is the following. Any $G$ rep in the scheme theoretic sense is an increasing union of finite dimensional reps, it's ""rational"", this is corollary 4.7, page 71, of in J.S. Milne's notes linked above. But consider the case of $(\mathbb{C},+)$ , (the $\mathbb{C}$ points of $\mathbb{G}_a$ ) acting  on $\mathbb{C}(t)$ . Where $g\cdot f(t)=f(t+g)$ . Then the $G$ -submodule spanned by $1/t$ is infinite dimensional. Meaning that this representation is not something that comes out of a $\mathbb{C}[t]$ -codmodule structure like group scheme theoretic rep of $\mathbb{G}_a$ over $\mathbb{C}$ would. So given a rep of $G(k)$ how do I know it's the same as the $k$ -points of some a scheme theoretic rep, $G\rightarrow GL_V$ I'm especially interested in the case of an affine algebraic group $G$ over an algebraically closed field $k$ acting on a variety and thus $G(k)$ acting it's field of rational functions $k(G)$ . Edit: Note that in the finite dimensional case if $G$ is quasi-projective and $k$ is algebraically closed then $G(k)$ -reps as an abstract group which are defined by polynomial equations are the same as scheme theoretic maps $G\rightarrow GL_n$ , because maps of varieties in the sense of Hartshorne chapter 1 are equivalent to schemes maps. So in particular this question is of interest when the representation is infinite dimensional. Edit: The lack of the word ""acting"" in $G(k)$ acting on it's field of rational functions $k(G)$ was corrected. I also added a definition for $GL_V$ when $V$ is infinite dimensional.","['algebraic-groups', 'reductive-groups', 'representation-theory', 'algebraic-geometry', 'group-theory']"
4212380,$\tan\frac{\pi}{9} +4\sin\frac{\pi}{9} = \sqrt 3 $ [duplicate],"This question already has answers here : Prove that $\tan\frac{\pi}{9}+ 4\sin\frac{\pi}{9}= \sqrt{3}$ . (4 answers) Closed 2 years ago . Prove that $$ \tan\frac{\pi}{9} +4\sin\frac{\pi}{9} = \sqrt 3 $$ There seem to be a lot of similar identities that are provable, for example, by using roots of unity. However, here I cannot get things to work out nicely. If $u=e^{\frac{2\pi i}{9}} $ , then $$i\left(\tan\frac{\pi}{9} +4\sin\frac{\pi}{9}\right) =\frac{u-1}{2(u+1)} +2(u^4-u^5)=\frac{-4u^6 +4u^4+u-1}{2(u+1)} $$ and so $$\left(\tan\frac{\pi}{9} +4\sin\frac{\pi}{9}\right)^2 = 3 \\ \iff (-4u^6+4u^4+u-1)^2+12(u+1)^2 =0 \\ \iff 16u^8-8u^7+8u^6+8u^5-8u^4+16u^3+13u^2-10u+13 =0$$ Unfortunately, the LHS is not of the form $k(u^8+u^7 +\dots+1)$ making the equality unobvious. How to proceed?","['trigonometry', 'roots-of-unity']"
4212397,Find all positive integers $ a $ and $ b $ such that $7^a-3(2^b)=1$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am looking for all positive integers satisfying the equation $$7^a-3(2^b)=1$$ I used the fact that $ (1,1) $ is a solution and got $$(7^a-7)=3(2^b-2)$$ I thaught about using Gauss Theorem in vain.
Thanks in advance for any idea.","['elementary-number-theory', 'discrete-mathematics', 'diophantine-equations']"
4212428,"Given finite sets $A$ and $B$, is $|A \cup B| = |A \cap B| \iff A = B$ true?","Given finite sets $A$ and $B$ , is $$
|A \cup B| = |A \cap B| \iff A = B
$$ true? Here is what I tried. For $$
|A \cup B| = |A \cap B| \implies A = B,
$$ suppose that $|A \cup B| = |A \cap B|$ , and from the inclusion-exclusion principle, \begin{align}
|A \cup B| &= |A| + |B| - |A \cap B| \\
2|A \cup B| &= |A| + |B|.
\end{align} However, I am not sure how to proceed from here.",['elementary-set-theory']
4212441,How to find a point on the perimeter of a square?,"Given the four corners $(x, y)$ of a square, the center $(x, y)$ , and a starting direction (ex: $45^\circ$ ) around a $360^\circ$ rotation ( $0^\circ/360^\circ$ at the top), how do you find out the intercept on the perimeter as the direction is incrementally stepped? What I'm basically doing is taking a circle of radius $(r)$ , determining it's area $(A)$ , converting that to length $(L)$ of the sides of the square, and then determining the corners $(x, y)$ given the starting center point and the above described direction. This allows for the square to rotate it's starting position depending on the direction. I used the equations below to determine the corners (assuming $(x, y)$ is the center of the circle): $$\left(x - \frac{L}{2} * (\sin(\text{rotation}) - \cos(\text{rotation})), y + \frac{L}{2} * (\sin(\text{rotation}) + \cos(\text{rotation}))\right)$$ $$\left(x + \frac{L}{2} * (\sin(\text{rotation}) + \cos(\text{rotation})), y + \frac{L}{2} * (\sin(\text{rotation}) - \cos(\text{rotation}))\right)$$ $$\left(x + \frac{L}{2} * (\sin(\text{rotation}) - \cos(\text{rotation})), y - \frac{L}{2} * (\sin(\text{rotation}) + \cos(\text{rotation}))\right)$$ $$\left(x - \frac{L}{2} * (\sin(\text{rotation}) + \cos(\text{rotation})), y - \frac{L}{2} * (\sin(\text{rotation}) - \cos(\text{rotation}))\right)$$","['elementary-functions', 'trigonometry', 'geometry', 'parametric']"
4212511,Golden numbers series [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Number is golden if it is divisible by the number before it. What is the max number of golden terms in a permutation of 1, 2...10,000? I think the solution is that the max number of terms is half the total number of terms in the series, but I'm not sure how to confirm/prove this.","['number-theory', 'combinatorics', 'elementary-number-theory']"
4212530,Are lower bounds for set systems with restricted intersection known in special cases?,"Consider $[n] = \{1,\ldots,n\}$ . Let $\mathcal{F}$ be a family of subsets of $[n]$ such that every set in $\mathcal{F}$ has exactly $k$ elements and for any $A,B \in \mathcal{F}$ with $A \neq B$ , $|A \cap B| \leq r$ . Denote the largest possible value of $|\mathcal{F}|$ by $m(n,k,r)$ . I'm interested in the special case when $k=2k' \leq \frac{n}{2}$ and $r = k'-1$ . My main question is as follows. What are some good lower bounds for $m(n,2k',k'-1)$ with $k' \geq 3$ ? Can it be exactly determined in any cases? Some related questions and results are mentioned below. In the case $k'=2$ , this boils down to evaluating $m(n,4,1)$ which is equivalent to the $K_4$ -packing number of $K_n$ . This was determined exactly in the paper 'Optimal packing of $K_4$ 's into a $K_n$ ' by A.E Brouwer in 1979. As noted in this question which links to this paper , the general case is equivalent to finding a lower bound on the maximum number of binary codewords of length $n$ and weight $k$ such that the Hamming distance between any two codewords is at least $2k-r$ . Adapting the notation in the paper, I want a lower bound on $A(n,2k-r,k)$ . Theorems 4 and 7 do provide some lower bounds but it seems to me that the argument only works if $2k-r$ is even. Moreover, I am interested in the case $A(n,2k-r,k) = A(n,3k'+1,2k')$ and was wondering if you can somehow obtain stronger bounds in this case. (Note that in my case, the results from the paper seem to apply only if $3k'+1$ is even, or equivalently if $k'$ is odd). This question is also related and provides a nice lower bound attributed to Frenkl, but I could not find a reference to the original statement of this theorem despite some digging around. The paper 'On a  Packing and Covering Problem' by R√∂dl provides the asymptotic behavior of $m(n,k,r+1)$ (Warning: there is some difference in notation regarding $r$ and $r+1$ ). Statements such as 'the lower bound holds for sufficiently large $n$ ' are not helpful to me unless I know what this large $n$ is. Apologies for the lengthy question! Thank you.","['coding-theory', 'combinatorics', 'extremal-combinatorics']"
4212538,When must the subgroup of a product be the product of subgroups?,"Twist on a common problem. It is well-known that the following statement is false, using $G = G_1 = G_2$ and considering the subgroup $\{ (g,g) : g \in G \} \le G_1 \times G_2$ : For any groups $G_1$ and $G_2$ , any subgroup of $G_1 \times G_2$ must be of the form $H_1 \times H_2$ for some subgroups $H_i \le G_i$ . Is there a way to characterize the pairs $(G_1,G_2)$ for which it is true?","['direct-product', 'group-theory']"
4212539,How do I simplify $4\sin(2a) - 2\sin(2a) \cdot 4\sin^2(a)$?,"$4\sin(2a) - 2\sin(2a) \cdot 4\sin^2(a)$ How do I simplify the above expression by using double-angle identities and half-angle identities? I could not find a way to solve the problem and have tried these methods but I just can't seem to find a way. I've gotten $8\sin(x)\cos(x)(1-2\sin^2x)$ which would be simplified to $8\sin(x)\cos(x)\cos(2x)$ . Apparently, the answer is $2\sin(4x)$ , but I just get $4\sin(2x)(1-2\sin^2x)$ .","['algebra-precalculus', 'trigonometry']"
4212558,"Show that $|e^{\dot{\imath}x\cdot y}-1|\leq c|y|$, for $|y|<1$ and $x\in \mathbb{R}^{n}$","In the book ""Robert Strichartz, Guide to distribution theory"" last line of page 165 in the context of proving that $$I(x):=\int_{\mathbb{R}^n}\frac{|e^{\dot{\imath}x\cdot y}-1|}{|y|^{n+2s}}dy=c|x|^{2s}$$ it says near $y=0$ use that $|e^{\dot{\imath}x\cdot y}-1|\leq c|y|$ by the mean value theorem There is no condition or restriction on $x$ mentioned in this case. So, I tried both the one-dimensional mean-value theorem and the $n$ -dimensional mean value theorem. I always get $$|e^{\dot{\imath}x\cdot y}-1|\leq c|y||x|$$ Here are the details: Apply $1$ -dimensional the mean value theorem to the smooth function $f(t)= e^{\dot{\imath}t}$ on $[0,x\cdot y]$ if $x\cdot y>0$ 0r the integral $[x\cdot y,0]$ of $x\cdot y<0$ . We get $$|f(t)-f(0)|=|e^{\dot{\imath}x\cdot y}-1|=|t|=|x\cdot y|\leq |x||y|.$$ Now, apply the $n$ -dimensional the mean value theorem to the smooth function $f(y)= e^{\dot{\imath}y\cdot x}$ on the line segment $\lambda y+(1-\lambda)0$ , $\lambda\in [0,1]$ . We have $\nabla_{y}f(y)=\dot{\imath} x e^{\dot{\imath}y\cdot x}.$ So $$|e^{\dot{\imath}y\cdot x}-1|=\lambda |y||x|.\qquad (1)$$ If one regards $ e^{\dot{\imath}y\cdot x} $ as a function of $x$ in stead, and use $n$ -dimensional the mean value theorem on the line segment $\lambda x+(1-\lambda)0$ we also get (1) by symmetry. Any ideas?","['multivariable-calculus', 'analysis', 'real-analysis']"
4212676,"For a norm function $f$, do we have a bound on $f([X, Y])$ for two vector field $X$ and $Y$?","Since $[X, Y]$ is related to the commutator of the flows generated by $X$ and $Y$ , if $X$ and $Y$ are sufficiently smooth, there should be an upper bound on $f([X, Y])$ , say a function of $f(X)$ , $f(Y)$ and Lipschitz constants of $X$ and $Y$ ? I couldn't imagine it become arbitrarily large. And is it true that at least one of the inequality $f([X, Y]) < f(X)$ , $f([X, Y]) < f(Y)$ holds? If it doesn't hold for general vector field, does it hold in some Lie algebra where the vector fields are left invariant?","['vector-fields', 'lie-algebras', 'differential-geometry']"
4212681,Convergence of $\sum_{n=1}^{\infty} (\sin (\sin n))^n$,"I'm studying this series: $$\sum_{n=1}^{\infty} (\sin (\sin n))^n$$ which apparently converges (according to the solutions at the end of the book). I haven't yet learned derivatives or integrals (or L'Hopital rule); I've only learned criteria to determine if a series with only positive terms is convergent or divergent. I have two problems with this exercises: It does not look to me that the series has only positive terms (e.g. $n=5$ yields a negative term) so none of my tests for convergence could be applied I'm unable to compute the limit of the sequence as $n \to \infty$ , which is a necessary (but not sufficient) condition for the series to converge With regards to the limit, I can definitely do $\lim_\limits{n\to\infty} \sin (\sin n)$ which is indeterminate, it is unclear to me why adding a power of $n$ makes the limit go to 0 and I have been unable to prove it. So two questions: hints to prove the limit, and confirm my theory that this is not a series of positive terms. I spent a considerable amount of time on this series (and the respective limit) and I feel that I have exausted the limits of my knowledge at this point.","['convergence-divergence', 'sequences-and-series']"
