question_id,title,body,tags
3521170,Linear Isometry on Positive Elements of a $C^{\ast}$-Algebra,"I am trying to complete problem 2.5 from Murphy's $\textit{$C^{\ast}$-Algebras and Operator Theory}$ , which states the following Let $\varphi : A \rightarrow B$ be a linear isometry between unital $C^{\ast}$ -Algebras $A$ and $B$ such that $\varphi(a^\ast) = \varphi(a)^\ast$ ( $a \in A$ ) and $\varphi(1) = 1$ . Show that $\varphi(A^+) \subseteq B^+$ . Here, the notation $A^+$ denotes the set of positive elements of $A$ . I have made some progress on this problem. Let $a \in A^+$ . Evidently, $\varphi(a)$ is hermitian, so it suffices to show that every element of $\sigma(\varphi(a))$ is a positive real number. To this end, let $\lambda \in \mathbb{C}$ . Then, $$\varphi(a) - \lambda 1 \not \in \text{Inv}(B) \text{ iff } \varphi(a - \lambda 1) \not \in \text{Inv}(B).$$ My goal is to eventually use positivity of $a$ to show that $\lambda \in \mathbb{R}_{\geq 0}$ . I also haven't used the isometry property of $\varphi$ , and I'm unsure how to relate this property to some fact about spectrum to get what I want. Could someone point me in the right direction?","['c-star-algebras', 'operator-theory', 'spectral-theory', 'functional-analysis']"
3521178,"For improper multiple integrals, can divergence be concluded if one of the inner integrals diverges?","While solving $$ \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(x^2 + y^2)}{e^{x^2+ y^2}} \, \, dx dy$$ I reached the following stage after passing to polar coordinates: $$  \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(r^2)}{e^{r^2}} \, \, rdr \, d\theta = \dfrac12\iint\limits_{(-\infty, +\infty)} \dfrac{\cos(u)}{e^{u}} \, \, du \,d\theta
$$ Now, since there is no $\theta$ variable in the integrand, if we choose to integrate first with respect to $\theta$ then our partial integral is automatically divergent. 
Thus my question is, Is the divergence of at least one of the inner integrals in a multiple
  integral enough to conclude that the whole multiple integral diverges?","['integration', 'improper-integrals', 'real-analysis', 'multivariable-calculus', 'multiple-integral']"
3521228,Does $C^1$ imply locally Lipschitz on Banach spaces?,"Let $X,Y$ be Banach spaces, $\Omega \subseteq X$ open and $f: \Omega \to Y$ continuously (Fréchet-) differentiable. Does this imply that $f$ is already locally Lipschitz? For finite dimensional spaces this is trivially true, but I am not certain if this holds for infinite dimensional Banach spaces.","['frechet-derivative', 'derivatives', 'functional-analysis', 'lipschitz-functions']"
3521236,Number of sequences with given termination conditions,"This question is a generalization of this one : Two players play a tournament of games of chess where the tournament winner is the first to achieve either $m>0$ total game wins or $0<n<m$ game wins in a row. (There are no ties.)  How many distinct possible sequences of game results exist as a function of $n$ and $m$ ? For small values, one can write out the sequences (as in the linked question). Clearly the shortest sequence of games is of length $n$ (of which there are two) and longest sequence of games is of length $2 m - 1$ , but we seek the total number of distinct sequences . First steps Assume, without loss of generality, that the winning player is $A$ and the loser is $B$ . Let $0<k<2m-1$ be the length of a sequence.  If $k<n$ there is no tournament winner; we can ignore such cases. There are, then, two ranges of interest: $n \leq k < m$ $m \leq k \leq 2m -1$ In the first case, the only way $A$ wins the tournament is to win $n$ games in a row.  Of course the last game (position $k$ ) $A$ wins, terminating the tournament.  Thus the final $n$ games $A$ must win.  That leaves $k-n$ preceding ""other"" slots.  The last one of these must be won by $B$ .  Thus there are $k - n - 1$ remaining ""unassigned"" slots which can be won in any order so long as $A$ does not win $n$ in a row. The number of ways this can be achieved is: $$\sum\limits_{j=0}^{k-n-1} {k-n-1 \choose j}$$ but again, this overcounts in certain cases because it includes cases in which $A$ wins $n$ consecutive games. In the second case there are two categories of ways $A$ wins:  by winning $m$ games or by winning $n$ in a row.  In either of these cases, the last game (slot $k$ ) $A$ must win.  If $A$ wins $m$ total games (on the $k$ th game) then $A$ has won $m-1$ games in the $2m - 2$ , and that means that $B$ has won the remaining $m-1$ games (but neither have won $n$ in a row). The total number of ways this can be done is: $$\sum\limits_{j=0}^{m-1} {2m - 2 \choose j}$$ but note that some of these cases may have $A$ win $n$ in a row.  We must subtract those cases.",['combinatorics']
3521270,"let $A,B$ matrices $n \times n$ and $A \ne B , AB=0$ and $A\ne 0 , B\ne0$ prove $\left|A\right|^2+\left|B\right|^2=0$","let $A,B$ matrices $n \times n$ such that $A \ne B $ and $AB=0$ and $A\ne 0 , B\ne0$ prove $\left|A\right|^2+\left|B\right|^2=0$ My attempt: $$AB=0 \implies \det(AB)=0 \implies \det(A)*\det(B)=0 \iff $$ $$|B|=0\qquad\text{or}\qquad |A|=0\qquad\text{or}\qquad|A|=|B|=0.$$ if $|A|=0 , |B| \ne0$ then $B^{-1}$ exist $\implies ABB^{-1}=0 \implies A=0 \implies$ contradiction if $|B|=0 , |A| \ne0$ same as before and if $|B|=0 , |A| =0$ then also $\left|A\right|^2+\left|B\right|^2=0$ My question is does this correct what I did and if not where did I did wrong ? thanks","['determinant', 'linear-algebra', 'solution-verification']"
3521276,Line integral of vector field when Stokes' theorem cannot be applied directly,"Define, on $\mathbb{R}^3 \setminus \{(0,0,z) \in \mathbb{R}^3 \ \mid \ z \in \mathbb{R} \}$ , the vector fields $$G(x,y,z) = \left(\frac{x}{x^2+y^2}, \frac{2y}{x^2+y^2}, 2 \right) $$ and $$H(x,y,z) = \left(\frac{-y}{x^2+2y^2}, \frac{x}{x^2+2y^2},3 \right). $$ How do we compute the line integrals of $G$ and $H$ over the closed curve at the intersection of the following surfaces: $$x+y+z = 2 \text{ and } z = x^2+y^2. $$ What is an easy way to solve these types of questions? We can see that $\text{curl}(G) = \text{curl}(H) = 0$ , so an idea could be to use the theorem of Stokes (although, I don't think that $H$ is conservative). However, what would be a good surface to choose? The vector fields are not defined on the entire of $\mathbb{R}^3$ , so the circle that is enclosed by the curve is not a good choice. Also, a parametrization of the curve would lead to integrals that are hard to solve.","['vector-fields', 'multivariable-calculus']"
3521377,Solving Nonlinear Wave Equation $u_{tt}=a(e^{\lambda u}u_x)_x$,"Salutations, I have been trying to solve this nonlinear wave equation that is a partial differential equation of the form (just for academical curiosity): $$u_{tt}=a(e^{\lambda u}u_x)_x$$ As I read in this document ( link ) (page 3, equation No. 7), it can be solved using this form of solution: $$
\begin{align}
u(x,t) & =\phi(x)+\psi(t)\\ & \Updownarrow \\
u_{tt} & =\psi_{tt} \\ 
u_x & =\phi_x
\end{align}$$ Then, replacing in non-linear PDE, I got this procedure: $$
\begin{split}
\psi_{tt} & = a[e^{\lambda\left(\phi+\psi\right)}\phi_x]_x \\
& =a\left(e^\left(\lambda\phi\right)e^\left(\lambda\psi\right)\phi_x\right)_x \\
&=ae^\left(\lambda\psi\right)\left(e^\left(\lambda\phi\right)\phi_x\right)_x \\
& \Updownarrow \\  
\frac{1}{e^\left(\lambda\psi\right)}\psi_{tt} &=a\left(e^\left(\lambda\phi\right)\phi_x\right)_x  =m
\end{split}$$ The previous procedure led to the following equations: $$
\begin{align}
a\left(e^\left(\lambda\phi\right)\phi_x\right)_x &=m \label{1}\tag{1}\\ \psi_{tt} &=me^\left(\lambda\psi\right)\label{2}\tag{2}
\end{align}
$$ Approaching equation \eqref{1}, I got this procedure: $$
\begin{split}
a\left(e^\left(\lambda\phi\right)\phi_x\right)_x=m \implies 
\int a\left(e^\left(\lambda\phi\right)\phi_x\right)_x \,dx & = \int \frac{m}{a} \,dx\\ 
e^\left(\lambda\phi\right)\phi_x & =\frac{mx}{a}+c_1\\ 
\int e^\left(\lambda\phi\right) \,d\phi &= \int \left(\frac{mx}{a}+c_1\right) \,dx \\ 
\frac{1}{\lambda}e^\left(\lambda\phi\right)&=\frac{m}{2a}x^2+c_1x+c_2
\end{split}
$$ Finally, $$\phi(x)=\frac{1}{\lambda}\ln\left(\frac{\lambda m}{2a}x^2+\lambda c_1x+\lambda c_2\right)$$ Next, when I approached equation \eqref{2}, I got this procedure: $$
\begin{split}
\frac{\partial^2 \psi}{\partial t^2} &= me^\left(\lambda\psi\right) \\ 
\int \frac{\partial^2 \psi}{\partial t^2} \frac{d\psi}{dt} \,dt &=m\int e^\left(\lambda\psi\right) \frac{d\psi}{dt}\,dt\\ 
\frac{1}{2}\left(\frac{d\psi}{dt}\right)^2 &= me^\left(\lambda\psi\right)+c 
\end{split}
$$ Then, $$
\int \frac{1}{\sqrt{2me^\left(\lambda\psi\right)+c_1}}\,d\psi = t+k 
$$ I´ve taken trigonometric substitutions and even I´ve used the hyperbolic inverse function and I´ve got these results for integral in left side: $$
\begin{align}
\frac{-2}{\lambda\sqrt{c_1}}\ln\left(\frac{\sqrt{c_1} + \sqrt{2me^\left(\lambda\psi\right)+c_1}}{\sqrt{2c_1-2me^\left(\lambda*\psi\right)}}\right)\label{I}\tag{I}&\\
\frac{-2}{\lambda\sqrt{2me^\left(\lambda\psi\right)+c_1}} & \label{II}\tag{II} \\ \frac{-1}{\lambda\sqrt c_1}\ln\left(\frac{\sqrt{c_1} +w}{\sqrt{c_1} -w}\right)&\text{ where }w=\sqrt{2me^\left(\lambda\psi\right)+c_1}\label{3}\tag{II}
\end{align}
$$ I require help for finding the solution to equation $(2)$ because none of possible solutions guide to exact solution shown in this paper in spanish ( link ) (page 3, section 2). I would be very thankful with any guidance or starting steps and/or explanations to find the correct procedure and exact solution. Thanks for your time and your attention.","['nonlinear-system', 'wave-equation', 'analysis', 'partial-differential-equations']"
3521406,Intuition on probability of drawing two aces given that the first draw is an ace.,"I would like to know why my reasoning is wrong on this problem.
Given that the first draw is an ace, I think there would be 3/51 chances of getting two aces after I know that the first one is an ace. It seems that the probability of this event is 1/33.
Where is my reasoning incorrect? Edit : The problem is as follows. I have a standard deck of cards and I draw $2$ cards at random. I would like to compute the probability $
P( \text{Having two aces} | \text{ Have one ace} )$ and $P( \text{Having two aces} | \text{Having one ace of spades} )$ . It seems, based on these lectures from Harvard: https://www.youtube.com/watch?v=JzDvVgNDxo8&list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&index=5 at $9:30$ mark, that $$
P( \text{Having two aces} | \text{ Have one ace} ) = 1/33
$$ while $$
P( \text{Having two aces} | \text{Having one ace of spades} ) = 1/17
$$ I do not understand why my combinatorics is incorrect, even though I think I understand his solution.","['conditional-probability', 'combinatorics', 'probability']"
3521416,"Solutions to PDE $\langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= 0$","Consider the vector field $$
\mathbf{f}(x_1,x_2)=\begin{bmatrix}f_1(x_1,x_2)\\ f_2(x_1,x_2)\end{bmatrix}=\begin{bmatrix}\frac{1}{1+x_2}+x_1\\ \frac{1}{1+x_1}+x_2\end{bmatrix}.
$$ I'm interested in finding a scalar function $\psi(x_1,x_2)$ such that: $$\tag{$\ast$}\label{ast}
\langle\nabla\psi,\nabla\psi+\mathbf{f} \rangle= \frac{\partial\psi(x_1,x_2)}{\partial x_1} \left(f_1(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_1}\right)+\frac{\partial\psi(x_1,x_2)}{\partial x_2} \left(f_2(x_1,x_2)+\frac{\partial\psi(x_1,x_2)}{\partial x_2}\right)=0
$$ My question. Does there exist a solution $\psi(x_1,x_2)$ to \eqref{ast}? If so, how to compute it? I'm aware that my question could be a trivial or naive one, but since I'm new to this kind of problems I would really appreciate any help/comment/suggestion. Thank you.","['multivariable-calculus', 'hamilton-jacobi-equation', 'numerical-methods', 'partial-differential-equations']"
3521463,Proof of $\int_0^\infty\frac{\left (1- e^{\pi\sqrt3x}\cos(\pi x )\right )e^{-2\pi x/\sqrt3}}{x(1+x^3)(1+x^3/2^3)(1+x^3/3^3)\dots}~dx=0.$,"I have the following integral in my notebook: $$\int_0^\infty\frac{\left (1- e^{\pi\sqrt3x}\cos(\pi x )\right )e^{-2\pi x/\sqrt3}}{x\prod_{j=1}^\infty (1+ x^3/j^3)}\ \mathsf dx=0.$$ Though after going through all my bookmarks, I can't find where I got it from, and I certainly do not know where to begin evaluating this integral. WolframAlpha offers no useful simplification of the integrand. Any help would be appreciated. Edit: ArXiv link to the original paper found! Here: https://arxiv.org/abs/1712.07456 .","['integration', 'indefinite-integrals', 'improper-integrals', 'infinite-product']"
3521488,Solving $x = \tan x$ [duplicate],"This question already has answers here : Solution of tanx = x? (5 answers) Express solutions of equation $ \tan x= x $ in closed form (4 answers) Closed 4 years ago . Out of curiosity, I tried to solve the eqation $$x = \tan x$$ but it was harder than I  first thought. Eventually I built an algrothim to solve this eqution using the bisection method.  But, is there any way to arrive to an exact solutions? I tried using taylor series of both $\frac{\sin(x)}{\cos(x)}$ and $\tan(x)$ , but in both cases, excpet for $x = 0$ , all the solutions were complex (I only try to find real solutions), and every time I tried to use euler's formula, I reached a dead end.",['trigonometry']
3521491,How do you rewrite a determinant of a matrix into a polynomial by induction?,$$\det\begin{bmatrix} {x} & {0} & {\cdots} & {\cdots} & {0} & {a_{1}} \\  {-1} & {x} & {0} & {\cdots} & {0} & {a_{2}} \\ {\ddots} & {\ddots} & {\ddots} & {\ddots} & {\vdots} & {\vdots} \\ {\cdots} & {0} & {-1} & {x} & {0} & {a_{n-3}} \\{\cdots} & {\cdots} & {0} & {-1} & {x} & {a_{n-2}} \\{\cdots} & {\cdots} & {\cdots} & {0} & {-1} & {a_{n-1}+x} \end{bmatrix}=a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1}$$ How can I replace the determinant on the left by induction to get $a_1+a_2x+\cdots+a_{n-1}x^{n-2}+x^{n-1}$ ? I need this to determine the determinant of the companion matrix and I don't understand how this step is done. Thanks in advance!,"['determinant', 'companion-matrices', 'matrices', 'linear-algebra', 'induction']"
3521499,Motivation of generalizing the theory of metric spaces to the theory of topological spaces,"Why does one need topological spaces if one has metric spaces? What is the motivation of the abstract theory of topological spaces? For me, the theory of metric spaces is quite natural. But I do wonder why there is the need of generalizing the whole theory... For instance, what are examples of topological spaces that are not metric spaces that really show that the theory of topological spaces is useful. There should be a strong reason, pathological examples don't suffice.","['general-topology', 'soft-question', 'metric-spaces']"
3521534,Explaining a solution to a calculus problem.,"I tried solving a calculus problem and I got the right result, but I don't understand the solution provided at the end of the exercise. Even though I got the same answer, I would like to understand what's happening in the given solution aswell. Consider the function: $$\ f(x) = \begin{cases} 
       x^2+ax+b & x\leq 0 \\
       x-1 & x>0 \\    \end{cases} \ $$ Find the antiderivatives of the function $f$ if they exist. The solution provided goes something like this: For $f$ to have antiderivatives the function $f$ must have the Darboux
  property. (...Some calculations...), therefore $f$ has the Darboux
  property if and only if $b = -1$ (I understood that now the function
  is continuous, therefore it has an anitederivative). Using the
  consequences of Lagrange's theorem on the intervals $(-\infty, 0)$ and $(0, \infty)$ any antiderivative $F : \mathbb{R} \rightarrow
 \mathbb{R}$ of $f$ has the form: $$ F(x) = \ \begin{cases} 
       \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x + c_1 & x < 0 \\
       \ c_2 & x=0 \\
       \dfrac{x^2}{2} - x + c_3 & x>0     \end{cases} \ $$ $F$ being differentiable, it is also continous, so $F(0) = c_2 = c_1 =
 c_3 $ . Therefore the antiderivatives of $f$ have the form: $$ F(x) = c + \ \begin{cases} 
       \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x & x\leq 0 \\
       \dfrac{x^2}{2} - x & x>0     \end{cases} \ $$ Again, I got the same result, but I don't understand a lot of the work done above. The first thing I didn't understand is the part where they say that $f$ has an antiderivative iff it has the Darboux property. I searched a bit online and I found that a function accepts antiderivatives only if it has the Darboux property. So I guess I have to accept that as a fact. The second (and more important thing) that I didn't understand was the part where they said that they used the consequences of Lagrange's Theorem on the intervals $(-\infty, 0)$ and $(0, \infty)$ to find that first form of the antiderivative. What theorem are they refering to? How did they use it on those intervals? Why is there a separate case for $x = 0$ with an aditional constant, $c_2$ . I only used $2$ constants, why were there needed $3$ ?  Long  story short, I just don't understand at all how they arrived at that first form of the antiderivative and how they used these ""consequences of Langrange's theorem"". I understood the second form of the antiderivative, that's what I also got, but the first form put me in the dark. I know these are all just details, but I really want to understand what was used here, why was it used and how was it used.","['integration', 'calculus', 'functions']"
3521592,Does $A/B \cong C/D$ and $B \cong D$ imply $A \cong C$?,"Say that for some group $A$ who has a normal subgroup $B$ , and for some group $C$ who has a normal subgroup $D$ , we know that $A/B$ is isomorphic to $C/D$ and that $B$ is isomorphic to $D$ . Is $A$ necessarily isomorphic to $C$ ? EDIT: What if there is a homomorphism $\sigma: A \to C$ ?","['quotient-group', 'group-theory', 'normal-subgroups', 'group-isomorphism']"
3521662,Series prove using combinational argument,Evaluation of $\displaystyle \sum^{n}_{r=1}(r^2+1)\cdot r!$ using combinational argument Although i have solved it without combinational argument $\displaystyle \sum^{n}_{r=1}\bigg[(r+1)^2-2r\bigg]r!=\sum^{n}_{r=1}(r+1)(r+1)!-r(r!)-\sum^{n}_{r=1}\bigg[(r+1-1)r!\bigg]$ $\displaystyle \sum^{n}_{r=1}\bigg[(r+1)(r+1)!-r(r!)\bigg]-\sum^{n}_{r=1}\bigg[(r+1)!-r!\bigg]$ $\displaystyle (n+1)(n+1)!-1-(n+1)!+1=n(n+1)!$ But did not know using combinational argument If anyone have an idea please explain me .Thanks,"['summation', 'combinatorics', 'combinatorial-proofs']"
3521675,Weighted average of samples of a random variable,"Let $X$ and $Y$ be two random variables which are possibly correlated. Let $X_1$ , $X_2$ ,…, $X_n$ be random samples of $X$ (which are iid), and $Y_1$ , $Y_2$ ,…, $Y_n$ be the corresponding samples of $Y$ . Let $Z$ be defined as $$Z = \frac{\sum_{i=1}^nX_iY_i}{\sum_{i=1}^nX_i}$$ which can be seen to be an $X_i$ weighted average of $Y_i$ . In general, the expectation of a ratio is not the ratio of the expectations. But here, under what conditions is $$E[Z] = \frac{E[\sum_{i=1}^nX_iY_i]}{E[\sum_{i=1}^nX_i]}?$$ Update My original thought was that if we are able to show $$E[Z] = \frac{E[\sum_{i=1}^nX_iY_i]}{E[\sum_{i=1}^nX_i]},$$ always holds, then $$\frac{E[\sum_{i=1}^nX_iY_i]}{E[\sum_{i=1}^nX_i]}=\frac{\sum_{i=1}^nE[X_iY_i]}{\sum_{i=1}^nE[X_i]}=\frac{nE[X_iY_i]}{nE[X_i]}=\frac{E[X_iY_i]}{E[X_i]}=\frac{E[XY]}{E[X]},$$ which are quantities that have extensively studied. Then, $E[Z] = E[Y]$ iff $X$ and $Y$ are uncorrelated. The motivation for this originally came from a musing about a personal finance/business problem. For example, $X_i$ could represent the quantity of apples we buy on store visit $i$ , and $Y_i$ could be the average price of an apple for trip $i$ . And $Z$ would be the average price not for a given trip, but in $n$ trips. The above question I posed would then help answer: when is $E[Z] = E[Y]$ ? Note that sometimes $X$ and $Y$ may be correlated: the more apples you buy, the cheaper it may be for that trip (negative correlation). Or they may be positive correlated (often seen in online freemium games) - you may get the first ""fuel"" for free, but then you have to pay increasing cost to procure additional ""fuel"".","['statistics', 'probability-theory']"
3521718,$\exists n$ such that $3M \geq \left|\frac {f(x+ \frac {y-x} ni )-f(x+ \frac {y-x} n (i-1)) } {\frac {y-x} n}\right|$ for each $1 \leq i \leq n$?,"Let $f:[a,b] \to \mathbb{R}$ be continous. Assume $-M \leq \underline Df(x) \leq \overline Df(x) \leq M$ on $[a,b].$ We are using the notation in Royden & Fitzpatrick : $$
\begin{split}
\overline Df(x)&:=\lim_{h \to 0^+} \sup\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\}\\
\underline Df(x)&:=\lim_{h \to 0^+} \inf\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\}
\end{split}
$$ Q: Is it true that there exists some positive integer $n$ such that, for each, $1 \leq i \leq n$ , we have $$
3M \geq \left|\frac {f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right|\;?
$$ I am not 100% sure, but I think the answer to to the highlighted question is ""yes."" (In which case, it would be a helpful lemma for me to prove something more significant.) Naively, the first thought is to take a sort of ""limit"" so we could say something about the quantity $$
\left|\frac{f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right|
$$ as $n \to \infty$ , comparing it to the upper and lower Dini derivatives which are bounded by $M$ . However, the order of quantifiers makes $i$ dependent on $n$ so I am having trouble making this rigorous. I also haven't yet taken advantage of the assumption that $f$ is continous, so perhaps continuity helps somewhow?","['limits', 'derivatives', 'continuity', 'real-analysis']"
3521851,Counting the number of particular subgroups of a finite abelian group,"I want to count subgroups $H$ isomorphic to $\mathbb{Z}/3\mathbb{Z}\times  \mathbb{Z}/3\mathbb{Z}$ in a group $G$ isomorphic to $\mathbb{Z}/3\mathbb{Z}\times\mathbb{Z}/9\mathbb{Z}\times\mathbb{Z}/81\mathbb{Z}$ My idea is that, I count the elements of order $3$ in $G$ , that are $3\cdot3\cdot3-1=26$ and they are the possible elements of the first $\mathbb{Z}/3\mathbb{Z}$ in $H$ , and then I count again the elements of order $3$ in $G$ but I remove two possible elements because the generators of $\mathbb{Z}/3\mathbb{Z}\times\{0\}$ and of the second $\{0\}\times\mathbb{Z}/3\mathbb{Z}$ have trivial intersection. So there should be $26\cdot24$ subgroups. Is this way to go right?","['abelian-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
3521853,Is $\Bbb R^2\times\Bbb S^2$ homeomorphic to $\Bbb R^4$ with a line removed?,Can anyone please provide a reference or a proof of that the $\Bbb R^2\times\Bbb S^2$ space is homeomorphic to $\Bbb R^4$ with a line $\Bbb R^1$ removed? Thank you!,"['general-topology', 'metric-spaces', 'differential-geometry']"
3521859,common left and right coset representatives for a subgroup of finite index,"Assume that $G$ is a group, and that $H$ is a (not necessarily normal) subgroup of $G$ having finite index $r=[G:H]$ . A subset $\{x_1,\cdots,x_r\}\subset{G}$ is called a left transversal of $H$ in $G$ provided that $\{x_1{H},\cdots,x_r{H}\}$ is complete set of $r$ distinct left cosets of $H$ in $G$ . Similarly, a subset $\{y_1,\cdots,y_r\}\subset{G}$ is called a right transversal of $H$ in $G$ provided that $\{{H}y_1,\cdots,{H}y_r\}$ is a complete set of $r$ distinct right cosets of $H$ in $G$ .  Prove that there exists a distinguished subset $\{x_1,\cdots,x_r\}\subset{G}$ that is simultaneously (both) a left and a right transversal of $H$ in $G$ ; i.e. such that $$G={\bigcup}^{r}_{j=1}{x_j}H = {\bigcup}^{r}_{j=1} H{x_j}.$$ Not sure how to approach this. I felt that that the conjugates of $H$ might play a role, and proved that any subgroup of the form $gHg^{-1}$ must also have index $r$ . But am not able to use this fact! This problem seems to have been discussed on math stack exchange but I cannot find a solution. As respondents observed, this is a consequence of Hall's ""marriage theorem"". Is there a direct proof of this fact which does not use the marriage theorem?",['abstract-algebra']
3521885,"Prove closed expression for $\int_0^1 \log(x) \log(1+x) \log(2+x)\,dx$","In Closed form of $\int_{0}^{1} \frac{\log(1+x)\log(2+x) \log(3+x)}{1+x}\,dx$ I have proposed an integral which I could not solve, and though there were some upvotes on the question no solution was provided. Hence I looked for simplifications which still are not trivial. Here's an example where I found a closed expression with the help of Mathematica which can be verified numerically but I'm lacking a proof. Hence my question is Prove that $$\int_0^1 \log(x)\log(x+1)\log(x+2)\,dx \\
= -6+3 \log ^3(2)-\frac{\log ^3(3)}{3}+\frac{\log ^2(2)}{2}-3 \log (3) \log (2)+6 \log (3)\\+\zeta(2) (1-2 \log (2))-\frac{13 \zeta (3)}{8}\\-\operatorname{Li}_2\left(-\frac{1}{2}\right)-6 \operatorname{Li}_2\left(-\frac{1}{2}\right) \log (2)+4 \operatorname{Li}_2\left(\frac{1}{4}\right) \log (2)\\-2 \operatorname{Li}_2\left(\frac{1}{3}\right) \log (3)+\operatorname{Li}_2\left(-\frac{1}{3}\right) \log (3)\\
-4 \operatorname{Li}_3\left(-\frac{1}{2}\right)-2 \operatorname{Li}_3\left(\frac{1}{3}\right)+\operatorname{Li}_3\left(-\frac{1}{3}\right)+2 \operatorname{Li}_3\left(\frac{1}{4}\right)\\\simeq -0.18403235664237885896 $$ Notice that the expression is composed of $\pi$ , $\log(s)$ , $\zeta(s)$ , and $\operatorname{Li}_{s}(t)$ . Remark 1: Mathematica was able to find the antiderivative but it turned out to contain complex valued summands. These cancelled out numerically but I could not prove mathematically that their contribution vanishes. Remark 2: I have not found the present class of integrals (product of logs with successively shifted arguments) in the 60 problems the of the book ""(Almost) Impossible Integrals, Sums, and Series"" by
Cornel Ioan Valean ( https://it.b-ok2.org/book/4996918/0df734 ) which is famous and frequently quoted in this forum. So this type of problem seems to be new.","['integration', 'harmonic-numbers', 'polylogarithm']"
3521920,Is there an efficient algorithm for iterating over binary strings where each of the reciprocal is enumerated exactly once?,"Problem. Say we have binary strings of length $n$ , i.e. $b_1b_2\dots b_n$ . There are $2^n$ of such strings, but in this problem if two strings are reciprocal of each other (have same digits in reverse order), we need to iterate just over one of these. For example we do not want to iterate over $1011$ if we have already gone over $1101$ . Now the question is how most efficiently can this be done? Attempts. We can iterate over all binary strings of given length, and for each string we encounter, we can evaluate the string only if it is "" $\leq$ "" than its reciprocal, where "" $\leq$ "" is any ordering on the strings. For example we can interpret the the string as a binary representation of a number and compare those. In the case above, $(1011)_2=11$ and $(1101)_2=13$ and so if we get to $1011$ , we notice $11\leq 13$ and evaluate that one, whereas at $1101$ we have $13>11$ and ignore that one. Problem is that this approach still requires to iterate over all of $2^n$ strings, I was wondering if we can somehow shortcut this and iterate, ideally, directly over the ""desired"" strings. Perhaps some clever ordering on the strings would do the job. Motivation. This came up when I wanted to iterate over polynomials with $0,1$ coefficients and check if they satisfy certain property, and it turns out that property will be the same for each of the reciprocal, so I just need to check one of those.","['binary', 'combinatorics', 'algorithms']"
3521963,How the mapping from power set of a set defined?,"So sorry for such an easy and bad question.... In Huber's paper page 462 , I can't understand how $P(A\times A)=\{0,1\}^{A \times A}$ is defined? Can some one please give an example?","['elementary-set-theory', 'general-topology']"
3522136,"A group $(G,\cdot)$ where every element except $e$ is of order $p$","Let $(G,\cdot)$ be a group such that the order of any element of $G$ which is not the identity is the positive integer $p$ . a) Prove that $p$ is a prime number. b) Prove that if any subset of $G$ which has $p^2-1$ elements has $p$ elements which commute with each other, then $G$ is abelian. I managed to solve a), but I couldn't make any progress on b). For a), assume that $p$ is not prime $\implies \exists a,b\in \mathbb{N}, a, b \ge 1$ such that $p=ab$ . Now we have that $\operatorname{ord}(x^a)=b$ , $\forall x\in G\setminus \{e\}$ , so $\operatorname{ord}(x^a)<p$ , contradiction. As a result, $p$ is a prime.","['group-theory', 'abstract-algebra', 'abelian-groups']"
3522163,Jensen's Inequality in $L^1$.,"Let be $(X,A,μ)$ a probability space and let $\textit{f}\in L^{1}(X)$ a real function such that $-\infty<a<f(x)<b<\inf$ for all $x\in X$ .
Let $\varphi:(a,b)\rightarrow \mathbf{R}$ be a convex function. Prove the so called Jensen's inequality:
􏰇 $$\varphi(\int_{X}\textit{f}d\mu)=\int_{X}\varphi(\textit{f})d\mu. $$ With the help of this inequality prove that if $h:X\rightarrow[0,\infty)$ is a measurable function, then $$\sqrt{1+\left(\int_{X}h\,d\mu\right)^2}\le\int_{X}\sqrt{1+h^2}\,d\mu\le1+\int_{X}h\,d\mu$$ -I've just proved in general the Jensen's inequality $\textit{f}:X\rightarrow \mathbb{R}$ if $f $ is $\mu$ -integrable and $\varphi$ convex and with $\operatorname {Dom}(\varphi)=\mathbb{R} but here I have different hypothesis. How could I do?
Thanks you very much!!","['inequality', 'analysis']"
3522181,Is $\Bbb R^2\times\Bbb S^2$ simply connected?,"The following answer states that the $\Bbb R^2\times\Bbb S^2$ space is simply connected: Topology of Black Holes However, the following post confirms that $\Bbb R^2\times\Bbb S^2$ is homeomorphic to $\Bbb R^4$ with a line removed: Is $\Bbb R^2\times\Bbb S^2$ homeomorphic to $\Bbb R^4$ with a line removed? Simply connected means that any path between any two points can be continuously deformed into any other path between the same points without leaving the space: Simply Connected Space In $\Bbb R^4$ , I can connect any two points by two paths, one on one side of the removed line and the other on the other side of the removed line. I do not see how one path can be smoothly transitioned into another without crossing the removed line. This is evidently impossible in $\Bbb R^3$ with a line removed, does the presence of another dimension in $\Bbb R^4$ makes it possible? Equivalently, a space is simply connected if any loop can be contracted to a point without leaving the space. Consider a loop around the removed line. If I contract this loop to a point, this point would be on the removed line and thus outside the space. Again, self evident in $\Bbb R^3$ , does the presence of an extra dimension in $\Bbb R^4$ allows contracting such a loop to a point outside the removed line without crossing it? So is $\Bbb R^4$ with a line removed simply connected? And is the homeomorphic $\Bbb R^2\times\Bbb S^2$ simply connected as well? What am I missing? Thank you!","['general-topology', 'algebraic-topology', 'differential-geometry']"
3522191,Does an UMVUE always exist?,"Let $\Psi= \{f_\theta: \theta \in \Theta\}$ be a statistical model. Define $\Upsilon= \{T: E[T]= g(\theta)\}$ - i.e., the class of unbiased estimator of $g(\theta)$ . Basically, I have two doubts: Does an UMVUE always exist? Thanks to Rao-Blackwell theorem, we can improve the ""goodness"" of an unbiased estimator using a sufficient statistic, i.e. $T\mid U$ where $T$ is our unbiased estimator and $U$ our sufficient statistic. Moreover, thanks to Lehmann–Scheffé theorem, I have that if $U$ is also complete, then $T^*= E[T\mid U]$ is UMVUE. My dilemma here is that I wrote on my notes that it is not true that an UMVUE for $g(\theta)$ always exist, but I cannot understand how it is possible. If UMVUE does not always exist, it implies that a complete statistic does not always exist or an unbiased estimator of $g(\theta)$ that is function of the complete statistic does not always exist. If this is true, could you provide me a counterexample- i.e. an example where an UMVUE does not exist? Suppose that $T$ is an efficient estimator for $g(\theta)$ - i.e. $V(T)$ = Cramér-Rao lower bound. I already know that if $T$ is efficient for $g(\theta)$ , then $a+bT$ is efficient for $a+bg(\theta)$ but for no other transformation. But is $g(T)$ always UMVUE for a $g(g(\theta)) \,\forall g$ - i.e. if $T$ is an efficient estimator of $g(\theta)$ , a transformation of $T$ is always UMVUE for a transformation of $g(\theta)$ ?","['statistical-inference', 'statistics', 'parameter-estimation']"
3522239,Exercise on life of maximal solutions of ODE,"Let $V: \mathbb{R}^n \to \mathbb{R}^n$ be a $C^1$ vector field and suppose that $\int_{1}^{\infty}\frac{dr}{B(r)}=\infty$ where $$
B(r) = \sup(|V(x)|: |x|<r ).
$$ Prove that the maximal solutions of the differential equation $\dot x = V(x)$ live on all $\mathbb{R}$ . Let $\sigma_p:(\alpha, \omega) \to \mathbb{R}^n$ be the maximal solution with $\sigma_p(0)=p$ , I want to prove that $\omega = \infty$ (I imagine that $\alpha=-\infty$ can be proved similarly). I've tried reasoning by contradiction so that $t_R = \inf(0\le t \lt \omega: |\sigma_p(t)|\ge R)$ is finite for every $R>0$ , but the best I could came up with is $B(R) \ge |V(\sigma_p(t_R))|$ and I don't know if $$
\int_{1}^{\infty} \frac{1}{|V(\sigma_p(t_R))|}dR< +\infty
$$ i.e. if it converges. I've also some tried other things like $$
|\sigma_p(t)-p|\le \int_{0}^{t}|V(\sigma_p(s))|ds
$$ and got $R\le t_R B(R)$ , but nothing again. Can you give me a hint?","['ordinary-differential-equations', 'vector-fields', 'analysis', 'mathematical-physics', 'dynamical-systems']"
3522281,How to prove monotonicity of this function?,"Let $a>0$ . How to prove that the function: $$f(x)=\frac{a x-1}{\log(a x)}\cdot\frac{\log x}{x-1},$$ is monotonic (depending on $a<1$ or $a>1$ ). I know that we can calculate the derivative and determine its sign, but this needs much of calculation. I'm wondering if we can decide the monotonicity using a simple trick.","['logarithms', 'derivatives', 'monotone-functions', 'real-analysis']"
3522319,Derive an atlas of Monge patches for a surface in 3D,"Question: If I have an atlas of non-monge charts for a surface, how I can I derive an atlas of monge charts? In differential geometry, a 'Monge patch' is a chart of the form $f(u,v) = (u, v, h(u,v))$ . [2] In simple terms, it's a parametric equation for a 3D surface patch described by only a height function. This section from Solid Shape (Koenderink, MIT Press 1990 - p. 233) describes Monge patch representation of surfaces and motivates my interest in learning how to derive them. The Monge patch representation lends itself particularly well for
  visualizations and for quick and dirty ""back of the envelope""
  calculations. You should thoroughly familiarize yourself with it. In a
  local frame you can always use the Monge patch representation for a
  small neighborhood. Very often you can cover whole surfaces with just
  a few Monge patches. An 'atlas' is a set of charts that cover a surface (a differentiable manifold) so what the author is saying is you can often produce an atlas for a surface where every chart is in Monge form. Typically in the literature on differential geometry, charts for surfaces in $\mathbb{R}^3$ are not given in Monge form. As a specific clarifying example here is a typical non-monge chart on a torus. [3] \begin{align}
x &= c + a*\cos(v)*\cos(u)\\
y &= c + a*\cos(v)*\sin(u)\\
z &= c + a*\sin(v)
\end{align} $$f(u,v) = (x,y,z)$$ How can I produce an atlas of Monge charts for the torus? That is, a list of parametric equations for surface patches that cover every part of a torus and are all in Monge form. Specifically, I'd like to understand the process of working the problem in general for simple surfaces in $\mathbb{R}^3$ , not just the list of equations for the torus. Here is a visualization of one Monge chart for a torus and its domain. The parametric question for a surface patch like this is what I'm interested in. [4] [2] http://mathworld.wolfram.com/MongePatch.html [3] http://mathworld.wolfram.com/Torus.html [4] http://faculty.cooper.edu/smyth/DifferentialGeometry/ch4/torusMonge.htm","['parametric', 'geometry', 'differential-geometry']"
3522365,Error in Gilbert Strang Differential Equations 2.1 Fundamental Solution to 2nd Order Differential Equation?,"In Strang's Differential Equations and Linear Algebra book, Section 2.1, he introduces the fundamental solution / impulse response $g(t)$ to the delta forcing function: $$m g'' + kg = \delta(t) $$ with given initial conditions $g(0) = 0$ and $g'(0) = 0$ . He then says this implies that $g'(0) = 1/m$ , since $$ m g''(0) + k g(0) = \delta(t) \rightarrow m g''(0) = \delta(t) \rightarrow m g'(0) = 1 \rightarrow g'(0) = \frac{1}{m}$$ I'm confused how this doesn't contradict the initial given condition $g'(0) = 0$ . Could someone please clarify? Edit 1: Maybe I'm misunderstanding the text. I don't find Strang easy to follow. Photo below:",['ordinary-differential-equations']
3522381,Show that $B(1-t) - B(1)$ is a Brownian motion,"I need a check on the following exercise. In particular, I'd like to be sure that points ii), iii) are okay. Let $(B_t)_{t \in [0,1]}$ be a standard Brownian motion. Prove that the process $W_t=B(1-t) - B(1)$ defined for $t \in [0,1]$ is a standard Brownian motion i) First I can see that $W(0)=B(1) - B(1)=0$ a.s. ii) I need to check the the increments are independent. To this aim, I fix a partition of $[0,1]$ , $0<t_1 < t_2 < \ldots < t_n$ and consider the increments $(W_{t_1}, W_{t_2} - W_{t_1}, \ldots, W_{t_n} - W_{t_{n-1}})$ I have that $$W_{t_{i}} - W_{t_{i-1}} = B(1 - t_i) - B(1 - t_{i-1})$$ and $$W(t_{i+1}) - W(t_i) = B(1 - t_{i+1}) - B(1-t_i)$$ I need to check they're independent: since both of them are normally distributed, to check the independence is enough to check that the covariance is $0$ . Moreover, since $B$ is a Brownian motion, I can use the fact that $Cov(B_t, B_s)= s \wedge t$ . Hence $$Cov(B(1-t_i) - B(1-t_{i+1} ), B(1-t_{i-1})  - B(1-t_i)) = \\
(1-t_i) \wedge (1-t_{i-1}) - (1-t_{i+1}) \wedge (1-t_{i-1}) - (1-t_i) \wedge (1-t_i) + (1-t_{i+1}) \wedge (1- t_i) = 0$$ Hence the increments are independent. iii) I need to show that for evert $0\leq s <t$ : $W(t) - W(s) - N(0,t-s)$ I have that $W(t) - W(s) = B(1-t) - B(1-s)$ . Now I have that $1-t< 1 -s$ , therefore I note that $$ B(1-t) - B(1-s) = -(B(1-s) - B(1-t)) $$ Now I see that the right hand side has normal law since it's an increment of the Brownian motion. Moreover, I have that it's mean is clearly $0$ , while the variance is $t-s$ Therefore $W(t) - W(s) - N(0, t-s)$ . (iv) Path continuity I have that $t \mapsto B_t(\omega)$ is continuous for $P$ -almost every $\omega$ . Hence, the function $$t \mapsto B_{1-t}(\omega) - B(1)$$ is continuous, since is obtained from $B_t$ by a traslation and adding a constant $B(1)$ . Edit To show it's a  Gaussian process, I can note the following: first I note that $(B(1-t_n), B(1-t_{n-1}), \ldots, B(1-t_1), B(1))$ is a Gaussian vector (since $B$ is a Brownian motion). Therefore, I can see $\begin{pmatrix} W(t_1) \\ \vdots \\ W(t_n) \end{pmatrix}=T((B(1-t_n), B(1-t_{n-1}), \ldots, B(1-t_1), B(1)))$ where $T$ is the linear map $T(x_1,x_2, \ldots,x_n)=(x_{n-1} - x_n,x_{n-2} - x_n, \ldots, x_1 - x_n )$ . Hence, since it's the image via a linear map of a Gaussian vector, $\begin{pmatrix} W(t_1) \\ \vdots \\ W(t_n) \end{pmatrix}$ is a Gaussian vector.","['normal-distribution', 'stochastic-processes', 'solution-verification', 'brownian-motion', 'probability-theory']"
3522417,$\ker \alpha$ is involutive if and only if $\alpha \wedge d\alpha = 0$,Let $M$ be a manifold and $\alpha \in \Omega^1(M)$ is a nowhere-vanishing one-form. I have to show that $\ker \alpha$ is involutive if and only if $\alpha \wedge d\alpha = 0$ but I'm having some trouble to prove this. I found this question - Why is $\ker\omega$ integrable iff $\omega\wedge d\omega=0$? - but I'm having a hard time proving this in the case when $M$ is a manifold of an arbitrary dimension.,"['exterior-algebra', 'differential-geometry']"
3522426,Infinite sequence of RV's on finite sample space,"I'm studying probability theory through these notes and I can't quite understand what's being said on page 10 (in relation to the Weak Law of Large Numbers): Remark 4.6 : Strictly speaking, we bent our rules here. An infinite sequence of non-constant, pairwise independent variables requires an infinite sample space. Just so I understand what is being said: Suppose $(\Omega, \mathcal{F}, \mathbb{P})$ is our probability space and $\Omega = \{\omega_1,..,\omega_n\}$ is finite. Is the above statement saying that if $\{\xi_i\}_{i\in\mathbb{N}}$ is an infinite sequence of independent RV's on $(\Omega, \mathcal{F}, \mathbb{P})$ , then they must all be constants almost surely? I can't quite see why that is the case in general.","['independence', 'probability-theory', 'random-variables']"
3522433,Torsion in abelian groups,"If $A$ , $B$ and $C$ are finite abelian groups that obey the following exact sequence $$A\rightarrow B\rightarrow C\rightarrow1$$ and $$A[m]:=\{a\in A:a^m=1\}$$ is the following inequality true or false, why? $$|B[m]|\leq |A[m]||C[m]|$$ I know that it is false when I take away the finite condition. If it is false, is there some other nice upper bound on $|B[m]|$ ? One way to start is by restricting attention to $p$ primary invariant, but I am not sure how to proceed. Maybe taking the quotient of $A[p]$ and $f(A[p])$ will help where $f:A\rightarrow B$ is the homomorphism in the exact sequence, and then induction, but I am not sure if the details follow.","['finite-groups', 'group-theory', 'exact-sequence', 'abstract-algebra', 'abelian-groups']"
3522460,derivative of inverse function problems with proof,"Theorem Let $f \colon U \to V$ be homeomorphism form open set U in normed space X, to open set V in normed set Y. Let $a\in U$ , $b=f(a)$ , $f'(x_0) \colon X \to Y$ exists and it is isomorphism. Then the derivative $(f^{-1})'(b)$ exists and $(f^{-1})'(b) = (f'(a))^{-1}.$ Proof in book Let $g=f^{-1}$ It is \begin{equation*} 
f(a+x)-f(x)=f'(a)x+o(x)
\end{equation*} We have to prove \begin{equation*} 
g(b+y)-f(b)-(f'(a))^{-1}y=o(y)
\end{equation*} or \begin{equation*}
\frac{g(y+b)-g(b)-(f'(a))^{-1}y}{||y||}  \to 0  \ { when } \  y\to0.
\end{equation*} Set $x = g(y+b)-g(b)$ so $x+a=g(y+b)$ . We then have $f(x+a)=y+b$ . From this we obtain \begin{equation}
y=f(a+x)-f(a)
\end{equation} and so $g(y+b)-g(b)=(f'(a))^{-1}*o(x)$ so it is enough to show $\frac{||x||}{||y||} $ is bounded but I don't know how.","['inverse-function', 'derivatives', 'real-analysis']"
3522471,Prove $\lim_{\varepsilon\to 0^+}\frac{1}{\varepsilon}\int\limits_{X\leqslant \varepsilon}X\mathrm{d}\mathbb{P}=0$,"Let $X$ be a random variable taking values in $[0,+\infty]$ . Prove that: $$\lim_{\varepsilon\to 0^+}\frac{1}{\varepsilon}\int\limits_{X\leqslant \varepsilon}X\mathrm{d}\mathbb{P}=0 ~~~~\mathrm{and~~~} 
\lim_{x \to +\infty}\frac{1}{x}\int\limits_{X\leqslant x}X\mathrm{d}\mathbb{P}=0.$$ Attempt. Of course we have $\displaystyle \frac{1}{\varepsilon}\int\limits_{X\leqslant \varepsilon}X\mathrm{d}\mathbb{P}\leqslant \frac{1}{\varepsilon} \varepsilon =1,$ but we don't get anything interesting by that (the same arguments holds for the other integral also). Thank you in advance.","['means', 'probability-theory', 'probability', 'random-variables']"
3522485,Is the infinite (countable or uncountable) union of disjoint closed sets closed?,"Is the infinite (countable or uncountable) union of disjoint closed sets closed? I think infinite (countable or uncountable) union of disjoint closed singleton sets should be open because if there are the union of infinitely many disjoint singletons, then we have the real numbers which is an open set. What about the case of non-singleton uncountable disjoint union? I don't known about the countable union of disjoint closed sets.","['elementary-set-theory', 'general-topology', 'examples-counterexamples', 'real-analysis']"
3522501,How to prove $\lim\limits_{n \to \infty} \frac{n}{\log_2 n!} = 0$ [duplicate],"This question already has answers here : Limit of logarithm function, $\lim\limits_{n\to\infty}\frac{n}{\log \left((n+1)!\right)}$ (3 answers) Closed 2 years ago . I believe this cannot be handled by just analytical transformations and L'Hôpital's rule (there is no derivate of n!...) How can I prove that: $\lim\limits_{n \to \infty} \frac{n}{\log_2 n!} = 0$ My only attempt so far at breaking this one was: $\lim\limits_{n \to \infty} \frac{n}{\log_2 n!} = \lim\limits_{n \to \infty} \frac{\log_2 2^n}{\log_2 n!} = \lim\limits_{n \to \infty} \log_{n!} 2^n$ It is easy to prove that $\lim\limits_{n \to \infty} \frac{2^n}{n!} = 0$ Because: $\lim\limits_{n \to \infty} \frac{2^n}{n!} = \lim\limits_{n \to \infty} \binom{2}{1}\binom{2}{2}\binom{2}{3}...\binom{2}{n} < \lim\limits_{n \to \infty} \frac{4}{n} = 0$ Because both $2^n > 0$ and $n! > 0$ But I don't know how to use it as proof for limit","['limits', 'limits-without-lhopital']"
3522590,Probability of choosing integers that sum to zero,"This is just a little problem that occurred to me. I don't know how to go about solving it and thought it would be fun to post here. Let $N \geq 3$ be a natural number, and let $n \leq N$ . What is the probability that $n$ numbers randomly selected without repetition from $\{1,...,N\}$ have the property that their signs may be chosen so that they sum to $0$ ? For example, if $N=n=3$ , then the selection $1,2,3$ has the desired property because $3 +(-2) + (-1)=0$ . In this case, $1,2,3$ is the only possible selection and so the probability of obtaining the desired property is $1$ . On the other hand, if $N=3$ and $n=2$ , then no selection has the desired property and the probability is therefore $0$ .","['number-theory', 'probability']"
3522602,Why the following description implies the limit is finite?,"Let $f:[0,T]\rightarrow \mathbb{R}$ $$V(f) = \lim_{\|\Pi\|\rightarrow 0}\sum_{j=0}^{n-1}|f(t_{j+1})-f(t_j)|$$ where $\Pi$ is a partition of $[0,T]$ and define $\|\Pi\|$ as $$\Pi=\{t_0,t_1,\cdots,t_n\}, \ \ 0=t_0<t_1<\ldots<t_n=T, \ \ \|\Pi\|=\max_i(t_{i+1}-t_i)$$ Now the solution manual says that Suppose $V(f)$ is finite. Then for any $\epsilon>0$ , there exists an $N\geq 1$ , $$\sum_{j=0}^{n-1}|f(t_{j+1})-f(t_j)|<V(f) + \epsilon$$ for all $n\geq N$ . I am confused about the description of $+\epsilon$ and $\forall n \geq N$ parts. How do they and this description imply finite of $V(f)$ ? I believe this is a trick widely used in real analysis; however, cannot still understand this.","['limits', 'sequences-and-series', 'real-analysis']"
3522607,Differentials of a Lagrangian in Fluid Mechanics,"My question relates to the paper by Foures et al. The paper presents a Lagrangian approach to finding an adjoint Poiseuille flow equation. It also defines the notion of an inner product (equation (3.10) in the paper) and proceeds to outline the method in which differentials are taken with respect to the componentwise velocity, $u_i$ . In the second term on the right-hand side of equation (A.5) below, how was the differential constructed? In particular, where do terms such as $\partial_t \delta u_i$ and $\delta u_j\partial_j U_i$ come from? I'm struggling to see how these terms were derived, so if anyone could offer an explanation (as to how the differential was computed), I'd be very grateful. EDIT: Using the Gateaux derivative on the terms $U_j\partial_j u_i$ and $u_j\partial_j\bar{u}_i = u_j\partial_j(U_i - u_i)$ gives $U_j\partial_j\delta u_i + \delta u_j \partial_j(U_i-u_i-\epsilon \delta u_i) + (u_j + \epsilon \delta u_j)\partial_j(-\delta u_i)\vert_{\epsilon=0}$ . Clearly something has gone wrong here, but I'm not sure what...","['inner-products', 'lagrange-multiplier', 'calculus', 'derivatives', 'fluid-dynamics']"
3522678,"How do we know that the bounds chosen to apply the squeeze theorem are ""tight"" enough?","I have been examining many different examples and I found no objective justification to the chosen bounds in none of them, as if the choice was an intuitive process. Is it really just that? For the lower bound, do we ""begin with"" a 0 and start looking for a bound that is the furthest possible from 0 and that still satisfies the inequality? What about the upper bound? It just seems to me that it is very easy to choose the wrong bounds given the commonly counterintuitive nature of limits for beginners. Is there a way to be sure that it is the right/not right one and not fall onto that? (In case it makes a difference, the examples I have examined were applied on sequences) Any help is welcome, thank  you.","['limits', 'sequences-and-series', 'real-analysis']"
3522681,Probability no male- female pairs share same birthday,There are 8 people in a room. There are 4 males(M) and 4 females(F).  What is the probability that there are no M-F pairs that have the same birthday ? It is OK for males to share a birthday and for females to share a birthday. Assume there are $10$ total birthdays. I give a solution below. Not sure if is correct and is there a more general way to approach it ? I break it into 5 cases-summing these cases gives the total ways M-F do not share. If divide the sum by $10^8$ would obtain desired probability. Case 1: all men have different birthdays $N_1 = 10 \cdot 9 \cdot 8 \cdot 7 \cdot (10-4)^4$ Case 2: one pair men exact + two single men $N_2 = {\sideset{_{10}}{_1} C} \cdot {\sideset{_4}{_2} C} \cdot 9 \cdot 8 \cdot (10-3)^4$ the first term chooses the single BD for the pair of men. The second term selects the 2 men in the pair. The $9\cdot 8$ are the number of ways the two single men can choose their birthdays. The final term is the number of ways the $4$ woman can select the remaining $10-3 = 7$ birthdays which do not equal the men which have used $3$ birthdays. Case 3: two pair men exact $N_3 = {\sideset{_{10}}{_2} C} \cdot {\sideset{_4}{_2} C} \cdot {\sideset{_2}{_2} C} \cdot (10-2)^4$ Case 4: one triple and one single man $N_4 = {\sideset{_{10}}{_1} C} \cdot {\sideset{_4}{_3} C} \cdot {\sideset{_1}{_1} C} \cdot {\sideset{_9}{_1} C} \cdot (10-2)^4$ Case 5: all men have same birthday $N_5 = {\sideset{_{10}}{_1} C} \cdot (10-1)^4$ The sum of Case $1$ to $5$ is the total ways for no M-F pairs. The last term  in each case is the number of permutations of the 4 woman with $(10-k)^4$ choices where $k$ is the number of unique birthdays used up for the men. I do not believe the order of the people matters: I calculate assuming all the men come first. Please comment on my approach. I have not found an understandable solution on this website.,"['combinations', 'birthday', 'probability']"
3522682,"$F$ is differentiable at $x_0$, but $S=\{ x \in \mathbb{R}^3 : F(x)=c\}$ is not smooth at $x_0$","Consider the surface defined implicitly by $x^3 - x^2y^2 + z^2 = 0$ . Does it have a well-defined tangent plane at the origin? Why or why not? The polynomial function $$F(x,y,z)=x^3 - x^2y^2 + z^2$$ is $C^1$ in its domain; in particular, it's differentiable at the origin. I thought that this was sufficient to claim that its level set at height $0$ is smooth and has a well-defined tangent plane at the origin. But it's not. Is this simply because $\nabla F(0,0,0)=0$ ? If that wasn't the case, the Implicit Function Theorem would imply that $S$ is the graph of some $C^1$ function near the origin, right? So, to be extremely methodical when answering theses questions, (1) If $\nabla F (x_0) \neq 0$ , then the level set is smooth at $x_0$ iff $F$ is $C^1$ at $x_0$ ; (2) If $\nabla F (x_0) = 0$ , we have to do algebra on the equation that defines the level set...? Or what?","['multivariable-calculus', 'smooth-functions']"
3522694,Finding a basis for a set of tuples,"Let $R^\infty$ be the subspace of U={ ${f:N→R}$ } of “infinite tuples” which consist of all $f\in U$ such that f(i) = 0 for all but finitely many values of i. Define an inner product on $R^\infty$ by (x, y) $=\sum_{i=1}^\infty xiyi$ . Let $∥x − y∥$ be the metric on $R^\infty$ . Find the basis of $R^\infty$ and prove the set of basis is closed, bounded, and noncompact. This is a finite sum for the inner product according to the above definition of the subspace, since all but finitely many terms vanish. For the first part, I defined $e_i = (0,··· ,0,1,0,··· ,)$ , where 1 appears on the ith slot and I think ei form a basis for $R^\infty.$ I am not sure how to show this set is the base though. For the second part the set would be { ${e_i : i ∈ N}$ } but I thought a set is compact if it is closed and bounded but how can it be noncompact if it is closed and bounded? Finally, what does it mean by a set of infinite tuples?","['vector-spaces', 'real-analysis', 'calculus', 'elementary-set-theory', 'general-topology']"
3522707,"How to show that ""whenever $v \ne 0$"", $m\frac{dv}{dt} = -kx$","A particle of constant mass m moves along the $x$ -axis. Its velocity $v$ and position $x$ satisfy the equation: $\frac{m}{2}(v^2 - v_0^2) = \frac{k}{2}(x_0^2 - x^2)$ where $K$ , $Y_0$ , and $X_0$ are constants. Show that whenever $v \ne 0$ , $m\frac{dv}{dt} = -kx$ . My reasoning is this: we can treat the $x$ as $S$ and it's a function of $t$ . Then the $v$ becomes a function of $x$ . If we differentiate both sides of the equation according to the implicit differentiation rules, we'll get: $$
\frac{m}{2}(2vv') = \frac{k}{2}(-2x) \\
mvv' = -kx
$$ If this is correct, the only case where $m\frac{dv}{dt} = -kx$ is when $V=1$ , not ""whenever $v \ne 0$ "". So I'm stuck here. Could, please, someone explain what is wrong here?","['calculus', 'implicit-differentiation', 'derivatives']"
3522725,Removing a penny raises mean coin value from 17 to 18. How many nickles?,"So this is a question that my sister (grade 8) got wrong on a test. There is a collection of quarters, dimes, nickels, and pennies in a jar. The mean of these coins is 17 but when you remove a penny, the mean becomes 18. How many nickels are in the jar? I tried to get the multiple of $17$ which subtracted by $1$ and divided by $16$ is $18$ . For that I got $17 \cdot 17 = 289 - 1 = 288 / 16 = 18$ which is correct. The only problem that I'm encountering is which coins are in the collection and of course how many nickels are there.",['algebra-precalculus']
3522822,What's the generating function for $\sum_{n=1}^\infty\frac{\overline{H}_n}{n^2}x^n\ ?$,"Is there closed form for $$\sum_{n=1}^\infty\frac{\overline{H}_n}{n^2}x^n\ ?$$ where $\overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k}$ is the alternating harmonic number. My approach, In this paper page $95$ Eq $(5)$ we have $$\sum_{n=1}^\infty \overline{H}_n\frac{x^n}{n}=\operatorname{Li}_2\left(\frac{1-x}{2}\right)-\operatorname{Li}_2(-x)-\ln2\ln(1-x)-\operatorname{Li}_2\left(\frac12\right)$$ Divide both sides by $x$ then integrate we get $$\sum_{n=1}^\infty\frac{\overline{H}_n}{n^2}x^n=\int\frac{\operatorname{Li}_2\left(\frac{1-x}{2}\right)}{x}\ dx-\operatorname{Li}_3(-x)+\ln2\operatorname{Li}_2(x)-\operatorname{Li}_2\left(\frac12\right)\ln x$$ and my question is how to find the remaining integral? Thanks Maybe you wonder why I have it as an indefinite integral, I meant so as I am planning to plug $x=0$ to find the constant after we find the closed form of the integral if possible. I tried Mathematica , it gave Edit With help of $Mathematica$ I was able to find \begin{align}
\sum_{n=1}^\infty\frac{\overline{H}_n}{n^2}x^n&=-\frac13\ln^3(2)+\frac12\ln^2(2)\ln(1-x)-\frac12\zeta(2)\ln(x)+\frac32\ln^2(2)\ln(x)\\
&\quad-\ln(2)\ln(x)\ln(1-x)-\frac12\ln(2)\ln^2(x)-\frac12\ln^2(2)\ln(1-x)\\
&\quad-\ln^2(2)\left(\frac{x}{1+x}\right)+\ln(2)\ln\left(\frac{x}{1+x}\right)[\ln(1-x)+\ln(x)]\\
&\quad+\ln(x)\ln(1-x)\ln(1+x)+\ln(x)\operatorname{Li}_2\left(\frac{1-x}{2}\right)+\ln\left(\frac{x}{1+x}\right)\operatorname{Li}_2(x)\\
&\quad+\ln(1+x)\operatorname{Li}_2(x)+\operatorname{Li}_2\left(\frac{x}{1+x}\right)\ln\left(\frac{2x}{1+x}\right)-\operatorname{Li}_2\left(\frac{2x}{1+x}\right)\ln\left(\frac{2x}{1+x}\right)\\
&\quad+\operatorname{Li}_2\left(\frac{1+x}{2}\right)\ln\left(\frac{x}{2}\right)-\ln\left(\frac{x}{1+x}\right)\operatorname{Li}_2\left(\frac{1+x}{2}\right)-\operatorname{Li}_3(x)-\operatorname{Li}_3\left(\frac{x}{1+x}\right)\\
&\quad+\operatorname{Li}_3\left(\frac{2x}{1+x}\right)-\operatorname{Li}_3\left(\frac{1+x}{2}\right)-\operatorname{Li}_3(-x)+\ln(2)\operatorname{Li}_2(x)+\frac{7}{8}\zeta(3)
\end{align}","['integration', 'harmonic-numbers', 'closed-form', 'generating-functions', 'sequences-and-series']"
3522839,Is the spherical growth function of a finite group always a unimodular sequence?,"Let $G$ be a finite group with symmetric generating set $S$ , and let the spherical growth function of $G$ (with respect to those generators) be $a_S(n) = |\{g \in G \mid \ell_S(g) = n \}|$ , where $\ell_s(g)$ is word length of $g$ with respect to $S$ . For a typical choice of generators (say, transposistions in $S_n$ ) one expects the sequence $a_S(n)$ to increase early on and then later decrease. But is it possible for the sequence to exhibit more complicated behavior, such as increasing, then decreasing, then increasing again (that is, for the sequence to not be unimodular)? It is known that there are infinite finitely generated groups without monotonic growth function, and this seems a natural question to ask in the finite case.","['group-theory', 'finite-groups']"
3522882,Minimally Sufficient Statistics Partition Intuition,"I am trying to understand the intuitive idea of a minimally sufficient statistic. It is my understanding that a statistic $T$ is minimally sufficient for $\theta$ for a family of populations $X\sim P_\theta$ if any other sufficient statistic $S$ of $\theta$ is of the form $T=h(S)$ where $h$ is a borel measurable function. This makes sense in a data reduction sense as this will produce the smallest $\sigma$ -algebra generated by a sufficient statistic that is a subset of the $\sigma$ -algebra generated by $X$ . Where I get confused is in the sense of a partition by a minimally sufficient statistic. It is my understanding that any Statistic creates a partition of the support of $X$ , where the minimally sufficient partition produces classes where the probability within each class is independent of $\theta$ . What is so special about the minimally sufficient partition? I know that if we use the relation $X R Y$ iff $P_\theta(X)=G(X,Y)P_\theta(Y)$ where $G(X,Y)$ is independent of $\theta$ , equivalence classes are created, but why is an equivalent criterion for minimally sufficiency be that the ratio $G(X,Y)=\frac{P_\theta(X)}{P_\theta(Y)}$ does not depend on $\theta$ $\leftrightarrow T(X)=T(Y).$ How is minimally sufficient implied by this equivalent criterion? I understand the proof but I am looking more for an intuitive understanding.","['statistical-inference', 'statistics', 'sufficient-statistics', 'data-analysis']"
3522894,Weakly convex functions in the sense of support functions,"Let $I\subset\mathbb{R}$ be an interval, $f:I\to\mathbb{R}$ a continuous function. $f$ is called convex if for any $x,y\in I$ , $t\in[0,1]$ , we have $$f((1-t)x+ty)\leq (1-t)f(x)+tf(y).$$ $f$ is called weakly convex in the distribution sense if $f''\geq0$ as a distribution. $f$ is called weakly convex in the support sense if for any $p\in I$ , $\varepsilon>0$ , there exists a neighborhood $U$ of $p$ in $I$ and a $C^2$ function $f_\varepsilon:U\to\mathbb{R}$ such that (1) $f_\varepsilon''(p)\geq-\varepsilon$ ; (2) $f\geq f_\varepsilon$ on $U$ , with equality at $p$ . How are these notions related? Of course all three are equivalent when $f$ is $C^2$ . But otherwise I'm unable to prove or disprove the implications. In particular, I need the fact convexity is equivalent to weak convexity in the sense of support functions (which I can't say for sure is correct). Edit: It is Exercise 7.5.3 in Peter Petersen's Riemannian Geometry to prove that being weakly convex in the support sense is equivalent to being convex. I especially need this result because this idea of using support functions has been used repeatedly later in the book. Thanks in advance!","['analysis', 'real-analysis']"
3522967,Closed form for the skew-harmonic sum $\sum_{n = 1}^\infty \frac{H_n \overline{H}_n}{n^2}$,"In a post found here it is mentioned that a closed form for the so-called younger brother (younger in the sense the power in the denominator is only squared, rather than cubed as in the linked question) skew-harmonic sum $$S = \sum_{n = 1}^\infty \frac{H_n \overline{H}_n}{n^2}$$ can be found, though none is given. Here $H_n = \sum_{k = 1}^n \frac{1}{k}$ is the $n$ th harmonic number while $\overline{H}_n = \sum_{k = 1}^n \frac{(-1)^{k + 1}}{k}$ is the $n$ th skew-harmonic number . I seek the closed-form expression for the sum $S$ . My thoughts on a possible alternative approach to that suggested in the link is as follows. Since $$\ln 2 - \overline{H}_n = (-1)^n \int_0^1 \frac{x^n}{1 + x} \, dx,$$ then $$H_n \overline{H}_n = \ln 2 H_n -(-1)^n H_n \int_0^1 \frac{x^n}{1 + x} \, dx.$$ Thus \begin{align}
S &= \ln 2 \sum_{n = 1}^\infty \frac{H_n}{n^2} - \int_0^1 \frac{1}{1 + x} \sum_{n = 1}^\infty \frac{(-1)^n H_n x^n}{n^2} \, dx\\
&= 2 \ln 2 \zeta (3) - \int_0^1 \frac{1}{1 + x} \sum_{n = 1}^\infty \frac{H_n (-x)^n}{n^2} \, dx,
\end{align} since $\sum_{n = 1}^\infty \frac{H_n}{n^2} = 2 \zeta (3)$ . I then thought of perhaps using the following known generating function of $$\sum_{n = 1}^\infty \frac{H_n}{n^2} x^n = \operatorname{Li}_3 (x) - \operatorname{Li}_3 (1-x) + \ln (1 - x) \operatorname{Li}_2 (1 - x) + \frac{1}{2} \ln x \ln^2 (1 - x) + \zeta (3),$$ but this leads to complex valued logs and polylogs which I would rather avoid. Continuing Continuing on using the generating function, we see that \begin{align}
S &= 2 \ln 2 \zeta (3) - \int_0^1 \frac{\operatorname{Li}_3 (-x)}{1 + x} \, dx + \int_0^1 \frac{\operatorname{Li}_3 (1 + x)}{1 + x} \, dx\\
& \qquad - \int_0^1 \frac{\ln (1 + x) \operatorname{Li}_2 (1 + x)}{1 + x} \, dx - \frac{1}{2} \int_0^1 \frac{\ln (-x) \ln^2 (1 + x)}{1 + x} \, dx - \zeta (3) \int_0^1 \frac{dx}{1 + x}.
\end{align} Surprisingly, indefinite integrals for all integrals appearing above can be readily found. Here: \begin{align}
\int_0^1 \frac{\operatorname{Li}_3(-x)}{1 + x} \, dx &= \frac{1}{2} \operatorname{Li}^2_2 (-x) + \operatorname{Li}_3 (-x) \ln (1 + x) \Big{|}_0^1 = \frac{5}{16} \zeta (4) - \frac{3}{4} \ln 2 \zeta (3)\\[2ex]
\int_0^1 \frac{\operatorname{Li}_3 (1 + x)}{1 + x} \, dx &= \operatorname{Li}_4 (1 + x) \Big{|}_0^1 = \operatorname{Li}_4 (2) - \zeta (4)\\
\int_0^1 \frac{\ln (1 + x) \operatorname{Li}_2 (1 + x)}{1 + x} \, dx &= \operatorname{Li}_3 (1 + x) \ln (1 + x) - \operatorname{Li}_4 (1 + x) \Big{|}_0^1\\
&= \operatorname{Li}_3 (2) \ln 2 - \operatorname{Li}_4 (2) + \zeta (4)\\[2ex]
\int_0^1 \frac{\ln (-x) \ln^2 (1 + x)}{1 + x} \, dx &= -2 \operatorname{Li}_2 (1 + x) - \operatorname{Li}_2 (1 + x) \ln^2 (1 + x)\\
& \qquad + 2 \operatorname{Li}_3 (1 + x) \ln (1 + x) \Big{|}_0^1\\
&= -2 \operatorname{Li}_4 (2) - \operatorname{Li}_2 (2) \ln^2 2 + 2 \operatorname{Li}_3 (2) \ln 2 + 2 \zeta (4)\\
\int_0^1 \frac{dx}{1 + x} &= \ln 2
\end{align} Thus $$S = \frac{7}{4} \ln 2 \zeta (3) - \frac{53}{16} \zeta (4) + 3 \operatorname{Li}_4 (2) - 2 \operatorname{Li}_3 (2) \ln 2 + \frac{1}{2} \operatorname{Li}_2 (2) \ln^2 2.$$ Now finding values for $\operatorname{Li}_n (2)$ when $n = 2, 3$ , and $4$ . In each case the principal value is found. $n = 2$ case Using $$\operatorname{Li}_2 (z) + \operatorname{Li}_2 (1 - z) = \zeta (2) - \ln z \ln (1 - z),$$ setting $z = 2$ gives $$\operatorname{Li}_2 (2) = \frac{3}{2} \zeta (2) - i\pi \ln 2.$$ $n = 3$ case Using $$\operatorname{Li}_3 (z) = \operatorname{Li}_3 \left (\frac{1}{z} \right ) - \frac{1}{6} \ln^3 (-z) - \zeta (2) \ln (-z),$$ setting $z = 2$ gives $$\operatorname{Li}_3 (2) = \frac{21}{24} \zeta (3) + \frac{3}{2} \zeta (2) \ln 2 - \frac{i \pi}{2} \ln^2 2.$$ $n = 4$ case Finally, from the result given here one has $$\operatorname{Li}_4(2) = 2 \zeta (4) - \operatorname{Li}_4 \left (\frac{1}{2} \right ) - \frac{i \pi}{6} \ln^3 2 + \zeta (2) \ln^2 2 - \frac{1}{24} \ln^4 2.$$ Plugging in all the pieces, we finally arrive at $$\sum_{n = 1}^\infty \frac{H_n \overline{H}_n}{n^2} = \frac{43}{16} \zeta (4) - 3 \operatorname{Li}_4 \left (\frac{1}{2} \right ) - \frac{1}{8} \ln^4 2 + \frac{3}{4} \zeta (2) \ln^2 2.$$ Magical!!","['integration', 'definite-integrals', 'euler-sums', 'harmonic-numbers', 'polylogarithm']"
3523015,"Given that $a$, $b$, $c$ are natural numbers, with $a^2+b^2=c^2$ and $ c-b=1$, prove the following.","Given that $a$ , $b$ , $c$ are natural numbers, with $a^2+b^2=c^2$ and $ c-b=1$ , prove the following $a$ is odd $b$ is divisible by 4 $a^b + b^a$ is divisible by $c$ My approach to prove the first statement is as follows: given that $a² + b² = c²$ : $$a^2 = (c^2 - b^2)$$ $$a^2 = (c + b)(c - b)$$ Given that $(c - b) = 1$ , $$a^2 = c + b = 2c - 1$$ This implies $a^2$ is odd, which implies(from some established trivial result I remember) that a is odd. For the second part, I figured out that either b or c must be odd, given that they are consecutive natural numbers. Having proved $a^2$ is odd, I suspect $c^2$ must be odd(absolutely out of intuition and vague reasoning that I'll mention in the end). I have no idea how to proceed beyond that. I'm absolutely clueless about the third part, and I feel it concerns Number Theory, something I'm not familiar with yet. My intuition: a, b, c are Pythagorean triplets such as $(3, 4, 5)$ , $(5, 12, 13)$ and $(7, 24, 25)$ ; I feel many more exist. I would like an explanation behind these patterns too. My background: I'm in the last year of high school; I can comprehend basic theoretical proofs, and have little idea about number theory. The above question is from an undergrad entrance exam, intended for high school passouts. I sincerely apologise for not using MathJax yet again; every time I try to use it I end up getting confused. I assure you I'll learn it in the time to come :)","['number-theory', 'elementary-number-theory']"
3523046,What is the meaning of $(\mathbb{Z}/ n \mathbb{Z})^{\times}$?,As the notation of Multiplicative group of integers modulo n I see: $(\mathbb{Z}/n \mathbb{Z})^{\times}$ What does this notation actually mean?,"['notation', 'abstract-algebra', 'modular-arithmetic']"
3523106,"If $I_n=\int_0 ^1{\frac{x^{n+1}}{x+3}}dx$, prove $\lim_{n \to \infty} n I_n=\frac{1}{4}$","Given $I_n=\displaystyle\int_0 ^1{\frac{x^{n+1}}{x+3}}\,dx$ for $n\in\mathbb N$ , prove that $$\lim_{n \to \infty} nI_n=\frac{1}{4}$$ Here is what the limit looks like: $$\lim_{n \to \infty} n\int_0 ^1{\frac{x^{n+1}}{x+3}}\,dx$$ I am not sure how to go about this exercise, but I tried solving the integral with no success. I integrated by parts once choosing $u = x^{n+1}$ and $v = \frac{1}{x+3}$ and this is what I got. $$I_n=\ln 4-(n+1)\int_0^1{x^n\ln(x+3)}\,dx$$ I don't know how to proceed. I was asked previously to prove that $I_{n+1}+3I_n=\frac{1}{n+2}$ , do you think this can be used somehow? I tried solving for $I_n$ but I don't see how it can help because of the $I_{n+1}$ that is left over? Help me out, please!","['limits', 'calculus', 'definite-integrals', 'real-analysis']"
3523138,Derivatives of convolution in $ \mathbb{R}^n $,"I am trying to prove that if $f \in C_c^k(\mathbb{R}^n)$ and $g$ is Lebesgue integrable on $\mathbb{R}^n$ , then the derivatives of $f * g$ equal $$D^\alpha(f * g)(x) = \int_{\mathbb{R}^n}(D^\alpha f)(x-y)g(y)dy$$ and are continuous for all multi-indexes of order up to $k$ . I know the version for $\mathbb{R}$ , however I am interested in this $n$ -dimensional generalization.","['measure-theory', 'convolution', 'analysis']"
3523158,How to find:$\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}$,"Evalute: $$\lim_{x\to
 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}$$ My attempt: I used the standard limits from the table: $$\lim_{x\to 0}\frac{\sin x}{x}=1,\;\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2},\;\;\lim_{x\to 0}\frac{\tan x}{x}=1,\;\;\lim_{x\to 0}\frac{e^x-1}{x}=1$$ $$$$ $L=\displaystyle\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}=$ $$$$ $\displaystyle\lim_{x\to 0}\left[\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{e^{1-\cos^3x}-e^{1-\cos^4x}}\cdot\left(\frac{e^{1-\cos^3x}-1}{1-\cos^3x}\cdot\frac{1-\cos^3x}{x^2}-\frac{e^{1-\cos^4x}-1}{1-\cos^4x}\cdot\frac{1-\cos^4x}{x^2}\right)\cdot\frac{x}{\arctan x}\right]$ Substitution: $$[t=\arctan x\implies x=\tan t\;\;\&\;\; x\to 0\implies t\to 0]$$ $$\lim_{x\to 0}\frac{x}{\arctan x}\iff\lim_{t\to 0}\frac{\tan t}{t}=1$$ The next step: $$1-\cos^3x=(1-\cos x)(1+\cos x+\cos^2x)$$ $$1-\cos^4x=(1-\cos x)(1+\cos x)(1+\cos^2x)$$ Now I obtained: $$L=1\cdot\left(1\cdot\frac{3}{2}-1\cdot 2\right)\cdot 1=-\frac{1}{2}$$ Is this correct?","['limits', 'calculus', 'solution-verification', 'limits-without-lhopital']"
3523205,Uniform convergence of $\sum_{n=1}^\infty x^{n-1}(1-x)^{2}$,"The given series of function is as follow $$\sum_{n=1}^\infty x^{n-1}(1-x)^{2}$$ prove that given series is uniformaly convergent on $[0,1]$ The solution i tried -The given series form an $G.P$ with  ratio $x \leq1$ i.e $$(1-x)^2+x(1-x)^2+x^2(1-x)^2+...$$ Now if i form partial sum of $n$ terms it will be $$s_n=(1-x)^2 \frac{1-x^n}{1-x}$$ $$\lim_{n\to \infty}s_n=\frac{(1-x)^2}{1-x}$$ after that we get $$s=(1-x)$$ now what can i say about convergence Because we know that if the series of partial sum is uniform convergence then series is uniform convergence ,but here $s$ is something polynomial type . Please Help","['functions', 'uniform-convergence', 'real-analysis']"
3523310,Function of sum of sines from graph,"What would be the method to express the solid curve as a sum of sines in the form $$\Psi(x)=5(\sin(2 \pi a x) + \sin(2 \pi b x))$$ By calculating $\Psi(0.1)=5$ , I used the properties of sine as an odd function and get $a=-b$ , which does not work for the rest of the graph.","['trigonometry', 'functions']"
3523399,What combination of inputs results in the largest output?,"I'm trying to solve the following problem: Production of a certain company depends on $3$ inputs $x,y,z$ in the following way: $$f(x,y,z) = 50x^{\frac{2}{5}} y^{\frac{1}{5}} z^{\frac{1}{5}}$$ Budget of the company is $24 000$ dollars and products $x, y, z$ can be bought for $80$ , $12$ or $10$ dollars per unit in that order. What combination of inputs results in biggest production? It's a Calculus test preparation problem and I have no idea how I would start. Could you help me? My idea is to use constrained extremas but I don't know how.","['multivariable-calculus', 'calculus', 'optimization']"
3523416,Showing the probability of guessing correctly goes down as more and more questions are answered correctly.,"A student answers a multiple choice examination with questions that have four possible answers each. Suppose that the probability that the student knows the answer to a question is 0.80 and the probability that the student guesses is 0.20. If the student guesses, the probability of guessing the correct answer is 0.25. The questions are independent, that is, knowing the answer on one question is not influenced by the other question. (a) If there is one question on the exam and he answered the question correctly, what is the probability he knew the answer? (b) If there are two questions on the exam and he answered both questions correctly, what is the probability he knew both answers? (c) How would you generalize the above to n questions, that is, if the student answered an infinite number of questions correctly, what is the probability he knew the answers? I know the answer to A using Bayes Theorem is $\ P(A∣C)=\frac{P(C∣A)P(A)}{P(C∣A)P(A)+P(C∣Ac)P(Ac)}$ $\ \frac{(.8)(1)}{(.8)(1)+(.25)(.20)}$ But I'm completely stuck on B and C.","['statistics', 'bayes-theorem']"
3523461,"Given that $\alpha + \beta - \gamma = \pi$, prove that $\sin^2 \alpha + \sin^2 \beta - \sin^2 \gamma = 2 \sin \alpha \sin \beta \cos \gamma$.","I am told: $$\alpha + \beta - \gamma = \pi$$ And I have to prove: $$\sin^2 \alpha + \sin^2 \beta - \sin^2 \gamma = 2 \sin \alpha \sin \beta \cos \gamma$$ What should I be looking for? I kept trying to take the sine of bots sides and use the formulas: $$\sin(a + b) = \sin a \cos b + \sin b \cos a$$ $$\sin(a-b) = \sin a \cos b - \sin b \cos a$$ but got nowhere. Then I tried using the formulas: $$\sin a + \sin b = 2 \sin \bigg ( \dfrac{a + b}{2} \bigg ) \cos\bigg ( \dfrac{a - b}{2} \bigg )$$ $$\sin a - \sin b = 2 \cos \bigg ( \dfrac{a + b}{2} \bigg ) \sin \bigg ( \dfrac{a - b}{2} \bigg )$$ But again, I got nowhere. Can you give me a hint? At least what should I be looking for? What should be my strategy? Everything that I did felt just random, while kind of hoping that everything would just magically turn into the desired result. What is the strategy for this kind of problem?",['trigonometry']
3523470,Sets and Cartesian Product,"Just a quick question. I am new to set theory and I have encountered a question I am unsure of that has to do with sets and cartesian products. Here is the question: True or False: (6,7) ∈ {{6},4} X {{7},5} My original thoughts were that this is false, because (6,7) is not the same as ({6},{7}) that would be produced, but I am unsure. Is this the right thinking?",['elementary-set-theory']
3523489,$\operatorname{SO}(n)$ is an (abstractly) maximal subgroup of $\operatorname{SL}(n)$,"Can somebody explain to me a proof that $\operatorname{SO}(n)$ is a maximal subgroup of $\operatorname{SL}(n,\mathbb{R})$ meaning, that if you add one element to $\operatorname{SO}(n)$ , you generate the whole $\operatorname{SL}(n,\mathbb{R})$ ? I am particularly interested in an inductive proof, and the induction step therein.","['group-theory', 'linear-algebra', 'geometry', 'lie-groups']"
3523495,"Prove that for any $n > 0$, if $a^n$ is even, then $a$ is even","the proof at hand is: Prove that for any $n > 0$ , if $a^n$ is even, then $a$ is even. Hint: Contradiction So I know to start the problem in a contradiction format would be: $a^n$ is even, then $a$ is odd so that $a = 2k+1$ .  Then plug in that into $a^n$ , we get $(2k+1)^n$ .  This is where I become stuck. Thanks in advance!",['discrete-mathematics']
3523556,Discrepancy in evaluating an integral $\int(\sqrt{\tan x} + \sqrt{\cot x})dx$.,"Consider $$I = \int(\sqrt{\tan x} + \sqrt{\cot x}) dx$$ If we convert everything to $\sin x$ and $\cos x$ , and try the substitution $t = \sin x - \cos x$ , we get $$I= \sqrt2 \int \frac{dt}{\sqrt{1-t^2}} = \sqrt{2} \arcsin(\sin x-\cos x) + C$$ However, if we originally substitute $ \tan x = t^2$ , and proceed as how ron gordon did here: Calculate $\int\left( \sqrt{\tan x}+\sqrt{\cot x}\right)dx$ , we get a seemingly different answer, which my textbook happens to offer: $$I=\sqrt{2} \arctan\left(\frac{\tan x-1}{\sqrt{2 \tan x}}\right)+C$$ Wolfram confirms that these two functions are indeed different. What went wrong? If we draw a right triangle with an angle $\theta$ ,with the opposite side as $\tan x-1$ and the adjacent side as $\sqrt{2 \tan x}$ , then the hypotenuse becomes $\sec x$ .Thus, $\theta=\arctan\left(\frac{\tan x-1}{\sqrt{2 \tan x}}\right) = \arcsin(\sin x - \cos x)$ , which should mean the functions are equivalent. Does it have something to do with the domain of the inverse trig functions?","['integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
3523558,Upper bounds to high dimensional gaussian vectors,"I'm trying to prove a claim given in Vershynin's Book - High-dimensional probability. The notation used is $g$ as a vector, $\sim$ represents similarity. $N(\mu, \sigma^2)$ is a normal distribution with $\mu$ mean and $\sigma^2$ variance. $t \in \mathbb{R}$ and $\mathbb{P}$ is a notation for probability. The author says: Given $g \sim N(0, I_{d \times d})$ a standard Gaussian vector in d-dimensions then \begin{equation}
\forall t \ge 0: \mathbb{P} \{ | \|g \|_2^2 - d | \ge t \} \le 2 \; \mathrm{exp}\left( - \frac{t^2}{C_1 d + C_2 t} \right)
\end{equation} for arbitrary $C_1$ and $C_2$ . Continuing the claim it extends to random matrices too. As stated as: Given a random matrix $\Gamma \in \mathbb{R}^{d \times k}$ i.i.d. $N(0,1)$ For any $v \in \mathbb{R}^k$ , and $\| v\|_2 = 1$ (normalized) \begin{equation}
\forall t \ge 0: \mathbb{P}\{ | \| \Gamma v \|_2^2 - d | \ge t\} \le 2 \; \mathrm{exp}\left( - \frac{t^2}{C_1 d + C_2 t} \right)
\end{equation} I think if I can prove the first one the second part is simple since $\Gamma v \sim N(0, I_{d \times d})$ given that a centered normal is invariant by rotation.","['concentration-of-measure', 'probability-theory', 'normal-distribution']"
3523564,Are all affine blowup algebras of finitely generated ideals obtained by base change from a polynomial ring?,"Let $R$ be a ring and let $I\subseteq R$ be an ideal of $R$ .  Then the Rees algebra of $R$ at $I$ is defined to be the graded ring $$\operatorname{Bl}_I(R) = \bigoplus_{n\geq 0} I^n$$ Given an element $a\in I$ , we can form the affine blowup algebra $R[I/a]$ to be the degree $0$ part of the graded localization of the Rees algebra at the element $a$ viewed as an element in degree $1$ , $$R[I/a]:=(\operatorname{Bl}_I(R)_{a^{(1)}})_0$$ More explicitly, the elements of this ring are represented by elements of the form $\frac{x}{a^n}$ where $x\in I^n$ , and they satisfy the usual equivalence relation $\frac{x}{a^n}\sim\frac{y}{a^m}$ if and only if there exists a natural number $k$ such that $a^k(a^m\cdot x - a^n\cdot y)=0$ . Suppose now that $I$ is a finitely generated ideal with generators $f_1,\dots,f_{n-1}$ .  We can always extend this family of generators to a family $f_1,\dots,f_{n-1},a$ , since $a \in I \Rightarrow (a)+I=I$ . Let $\mathbb{Z}[X]:= \mathbb{Z}[x_1,\dots,x_n]$ be the polynomial ring in $n$ indeterminates over the integers.  Specifying a family of $n$ elements of a ring yields a unique map $f:\mathbb{Z}[X]\to R$ such that $f(x_i)=f_i$ . Then form the affine blowup algebra $\mathbb{Z}[X][X/x_n]$ .  It is easy to see that this ring is isomorphic as a $\mathbb{Z}[X]$ -algebra to $$\mathbb{Z}\left[\frac{x_1}{x_n},\dots,\frac{x_{n-1}}{x_n},x_n\right],$$ although abstractly as a commutative ring, this ring is again a polynomial ring in $n$ indeterminates. ( Note : We could equivalently describe the $\mathbb{Z}[X]$ -algebra $\mathbb{Z}[X][X/x_n]$ by the endomorphism $\iota:\mathbb{Z}[X]\to \mathbb{Z}[X]$ sending $x_i\mapsto x_i \cdot x_n$ for $1\leq i <n$ and $x_n\mapsto x_n$ .) Then is it the case that $R[I/a]\cong \mathbb{Z}[X][\frac{X}{x_n}] \otimes_{\mathbb{Z}[X]} R$ ? It seems like it ought to be, but I'm worried, since in the non-affine case, given a map $f:Y\to X$ and a closed subscheme $Z\hookrightarrow X$ , we ordinarily need $Y\to X$ to be flat in order for the canonical map $\operatorname{Bl}_{f^{-1}(Z)}(Y)\to Y\times_X\operatorname{Bl}_Z(X)$ to be an isomorphism.","['algebraic-geometry', 'commutative-algebra']"
3523581,Extrema of function with lagrange multipliers,"Let $$f(x_1, ..., x_n) = x_1 + \frac{x_2}{x_1} + \frac{x_3}{x_2} + ...+\frac{x_n}{x_{n-1}} + \frac{1}{x_n}$$ where $x_1, ..., x_n>0$ . I thought that maybe I should cast what above as optimization problem with equality constraint, but I don't know how to do that. Moreover the constraint gives unbounded open set, so even existence of extrema is tricky.","['optimization', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
3523601,Proving an evenness property for the nth power map for a finite (not necessarily commutative) group $G$,"In the course of proving some properties about an algorithm to pick an element of prime order $p$ from a finite group $G$ such that $p ∥ \lvert G \rvert$ ( $p \mid \lvert G \rvert$ but $p^2 \not\mid \lvert G \rvert$ ), I needed the following lemma: Lemma : Let $G$ have order $m$ with prime $p ∥ m$ , and let $f(x) = x^{m/p}$ . If $y$ is a non-identity element in the
  image of $f$ , then $\lvert f^{-1}(\{ y \}) \rvert = C$ for some
  constant $C$ independent of $y$ . In other words, each
  non-identity element in the image of $f$ has an equal number of
  elements of $G$ mapping to it. This lemma becomes easy if we can assume that $G$ is commutative, since then $f(x)$ is a group homomorphism, and thus each $y$ in the image is isomorphic to a coset of $\ker f$ . I've managed to prove this myself for arbitrary finite groups, but was wondering if it was correct, and if there's a better way to prove it. Here's my proof (take as given that any non-identity element in the image of $f$ has order $p$ ): Proof :   Let $A_{m/p}$ be the set of elements of $G$ with order a factor
  of $m/p$ , and let $k$ be an integer such that $k(m/p) ≡ 1 \pmod p$ . If $y$ is a non-identity element in the
  image of $f$ , then we want to show that $$
    f^{-1}(\{y\}) = \{ xy^k \mid x ∈ A_{m/p} ∩ C_G(y) \}\text{,}
  $$ where $C_G(y)$ is the centralizer of $y$ . Let $xy^k$ be in the set on the right-hand side. $x$ commutes
  with $y$ , so it must also commute with $y^k$ , and $(xy^k)^a = x^a y^{ka}$ for any $a$ . Therefore, $$
    (xy^k)^{m/p} = x^{m/p} y^{k(m/p)} = y\text{.}
  $$ Going the other way, let $z ∈ f^{-1}(\{ y \})$ . Since $y$ has
  order $p$ , $z$ had order $pn$ where $n \mid m/p$ . $p$ and $n$ are coprime, so there exists $z_1, z_2 ∈ G$ such that $z = z_1 z_2 = z_2 z_1$ , $\operatorname{ord}(z_1) = p$ , and $\operatorname{ord}(z_2) =
  n$ . (Let $ap + bn = 1$ . Then $z_1 = z^{bn}$ and $z_2 = z^{ap}$ .) $z_1$ must be in the subgroup generated by $y$ , so $z_1 = y^k$ , and $z_2 ∈ A_{m/p} ∩ C_G(y)$ . Then let $y_1$ and $y_2$ be non-identity elements in the image
  of $f$ . If $y_1$ and $y_2$ are in the same cyclic subgroup,
  then $y_2 = y_1^a$ for some $a$ , and thus $C_G(y_1) = C_G(y_2)$ .
  Therefore, the map $$
    g(xy_1^k) = xy_2^k
  $$ is a bijection from $f^{-1}(\{ y_1 \})$ to $f^{-1}(\{ y_2 \})$ . Otherwise, if $y_1$ and $y_2$ are not in the same cyclic
  subgroup, then they're in different $p$ -Sylow subgroups of $G$ . By the second Sylow theorem, there is some $w$ such that $y_2 = wy_1'w^{-1}$ for some $y_1'$ in the cyclic subgroup
  generated by $y_1$ . Then we want to show that the map $g(x) = wxw^{-1}$ is a bijection from $f^{-1}(\{y_1'\})$ to $f^{-1}(\{y_2\})$ . Let $xy_1'^k ∈ f^{-1}(\{y_1'\})$ . Then $$
    g(xy_1'^k) = g(x)y_2^k\text{.}
  $$ Conjugation preserves order, and it also commutes with taking the
  centralizer. Therefore, $g(x) ∈ A_{m/p}$ and $g(x) ∈ C_G(y_2)$ ,
  so $g(xy_1'^k) ∈ f^{-1}(\{ y_2 \})$ , and thus $g(f^{-1}(\{ y_1' \})) ⊆ f^{-1}(\{ y_2 \})$ . The other inclusion
  holds similarly. In particular, both inverse images have the same
  size, and by the first part above $f^{-1}(\{y_1\})$ has the same size as $f^{-1}(\{y_2\})$ . $∎$ (Note that, unlike the commutative case, the exclusion of the identity is necessary. If $G = S_3$ and $p = 2$ , then each transposition has exactly one element in
  the preimage of $f(x) = x^3$ (itself), but $e$ has three
  elements in the preimage of $f$ ; $e$ itself and the two $3$ -cycles.)","['group-theory', 'finite-groups']"
3523609,What are eigenvectors all about? [duplicate],"This question already has answers here : What is the importance of eigenvalues/eigenvectors? (11 answers) Closed 4 years ago . It's awkward, but to be completely honest: I never understood what eigenvectors are all about. The only reasons I can think of why they could be interesting are: They are calculated to diagonalize a matrix (which is useful, because one can calculate faster with diagonal matrices?). If one has a linear transformation that represents a rotation, an eigenvector would be the rotation axis. Everybody says that they are used ""everywhere"". But I find all of these reasons unsatisfying. The concept of an eigenvector just seems unnatural to me. I associate this concept with a lot of unmotivated calculations. I would love to hear an honest, down-to-earth explanation of why eigenvectors are interesting. Note: I am not asking what eigenvectors are, my question is purely about why a pure mathematician (not interested in numerical calculations) should care.","['matrices', 'linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3523612,Blowup along the fundamental locus of a rational map,"Assume $f:X\dashrightarrow Y$ is a rational map between varieties, where $X$ is normal and $Y$ is complete. Then, the fundamental locus the $f$ (which means cannot extend the definition of $f$ on it), say $B$ , is of codimension at least two. Intuitively, the map $f$ gives rise to a morphism $\hat f$ defined on $Bl_B X$ in a natural way. Is this intuition correct? And if yes, how can I prove it? Thanks!","['algebraic-geometry', 'blowup', 'birational-geometry']"
3523674,Minimal polynomial of $\sqrt{3+2\sqrt{3}}$,"Let $a=\sqrt{3+2\sqrt{3}}$ . Then \begin{align*}
&a=\sqrt{3+2\sqrt{3}}\\
&\implies a^2=3+2\sqrt{3}\\
&\implies a^2-3 = 2\sqrt{3}\\
&\implies (a^2-3)^2 = 4\cdot 3=12\\
&\implies (a^2-3)^2-12=0.
\end{align*} So, $a$ a root of $$(x^2-3)^2-12=x^4-6x^2-3.$$ By Eisenstein's criterion, $x^4-6x^2-3$ is irreducible over $\mathbb Q$ . So, $a$ is algebraic of degree $4$ . However, I've seen that the degree of $\mathbb Q(\sqrt{3+2\sqrt{3}})$ over $\mathbb Q$ is $2$ . What is wrong with my attempt?","['nested-radicals', 'irreducible-polynomials', 'field-theory', 'abstract-algebra', 'extension-field']"
3523710,How to find out the probability that the tallest person in a group of people is a man?,"Assume we have a population of N men and women such that exactly $N/2$ people are men (set $M$ ) and $N/2$ people are women (set $W$ ). Assume that the standard deviations for height between both groups is the same $\sigma$ however their averages are different with $\mu_M > \mu_W$ . We pick $n$ people at random from the population such that $n/2$ people are men and $n/2$ people are women. Knowing $n$ what's the probability that the tallest person in the selected group is a man? Note: this is not homework, I came up with this question on my own.
Note 2: all distributions are normal distributions. EDIT:
My current reasoning is that if $Z = M-W$ : $E(Z) = E(M-W) = E(M) - E(W)$ And $V(Z) = V(M-W) = V(M) + V(W) = \sqrt2\sigma$ Thus the probability that one man is taller than one woman is: $P(Z>0)$ And so the probability that all women are taller than all men in the sample is: $(1 - P(Z>0))^{n/2}$ : However this result has lead me to some very odd conclusions. So I suspect I am wrong.","['statistics', 'normal-distribution', 'standard-deviation']"
3523712,How to compute the expected minimum Hamming distance with 3 strings,"If we sample $3$ binary strings of length $n$ , uniformly and independently, what is the expected minimum Hamming distance between the closest pair? Numerically, it seems to be asymptotic to $n/2$ but it would be great to know if there is an exact formula.",['probability']
3523748,Recursive sequence depending on the parameter,"For the given parameter $\mathbb R\ni t\geq 1$ , the sequence is
  defined recursively: $$a_1=t,\;\;a_{n+1}a_n=3a_n-2$$ $(a)$ Let $t=4$ .
  Prove the sequence $(a_n)$ converges and find its limit. $(b)$ Which parameters $t\geq 1$ is the sequence $(a_n)$ increasing
  for? My attempt: Bolzano-Weierstrass:A sequence converges if it is monotonous and
   bounded $$a_{n+1}a_n=3a_n-2\implies a_{n+1}=3-\frac{2}{a_n}$$ $(a)$ First few terms: $a_1=4,a_2=\frac{5}{2},a_3=\frac{11}{5}$ Assumption: the sequence is decreasing Proof by induction:
the basis (n=1) is trivial: $\frac{5}{2}<4$ Assumption: $a_n<a_{n-1},\;\forall n\in\mathbb N$ Step: $$a_n<a_{n-1}\implies\frac{1}{a_n}\geq\frac{1}{a_{n-1}}\Bigg/\cdot(-2)$$ $$\iff-\frac{2}{a_n}\leq-\frac{2}{a_{n-1}}\iff \underbrace{3-\frac{2}{a_n}}_{a_{n+1}}\leq\underbrace{3-\frac{2}{a_{n-1}}}_{a_n}$$ The limit: $$L=3-\frac{2}{L}\implies L^2-3L+2=0$$ I take into account only $2$ because the parabola is convex and $$a_n\to L^-.$$ Then I have to prove: $a_n\geq 2\;\forall n\in\mathbb N$ after the formal computing: $a_{n+1}\geq 3-\frac{2}{2}=2$ $\underset{\implies}{\text{Bolzano-Weierstrass theorem}}(a_n)\to 2$ $(b)$ Since the sequence doesn't have to be convergent, only increasing: $$a_2=3-\frac{2}{t}\geq t\implies t\in[1,2]$$ Then, it should follow inductively,analogously to $(a)$ , this time it is increasing.
Is this correct?","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3523754,Show that $\sin^220^\circ\sin40^\circ = \sin10^\circ \sin30^\circ \sin60^\circ$.,"This is the final step of one old competition problem, and in textbook it simply says ""reader can prove this"", but I found this a bit nontrivial. Show that $\sin^220^\circ\sin40^\circ = \sin10^\circ \sin30^\circ \sin60^\circ$ . I tried a lot of things and formula on this, but never succeeded.
Can someone help?","['contest-math', 'euclidean-geometry', 'geometry', 'trigonometry', 'algebra-precalculus']"
3523759,"Can any solvable finite group be obtained from abelian groups and combinations of taking subgroups, quotients, and semidirect products?","It is clear that finite solvable groups are closed under those operations, so at most the solvable groups can be produced this way. Not all solvable groups can be written as semidirect products of abelian groups (for example, the quaternion group), but the quaternion group is a quotient of a semidirect product, so I'm wondering if all groups can be obtained using these methods. I'm also curious if we can produce all solvable groups with less operations (for example, remove taking subgroups from our list), and whether if we throw in all simple groups, we can obtain all groups. Finally, I wonder if a bound can be obtained on how many times we'd need to use certain operations here (for example, is every finite solvable group a quotient of a single big iterated semidirect product?).","['group-theory', 'abstract-algebra', 'finite-groups', 'solvable-groups']"
3523779,Properties of preimage of critical values of a manifold,"My question is very vague and poorly stated. Let $f: M \to \mathbb{R}$ be a smooth function with $M$ a smooth manifold. Let $y \in \mathbb{R}$ be a critical value which isn't a global extremum. We know in general that $f^{-1}(y)$ isn't necessarily a submanifold of $M$ , but I'm wondering if there are cases where it still is, or at least is ""almost"" a manifold. To be more clear about what I'm saying I'll give an example that's easy to visualize. Say $M$ is a compact surface in $\mathbb{R}^3$ and $f$ is the height function. For a regular value $x$ , the preimage of it will look like a set of circles (if I'm not wrong). I think of it as the intersection of the surface with the plane $z=x$ For a critical value $y$ that isn't a global max or min (say it is the peak of some hill), the preimage will consist of a point as well as at least one circle. This is because $f^{-1}(y)$ not only has a critical point, but also a set of regular points which map to the circle, let's say. How do we describe this preimage, is it still a manifold? It has components of different dimensions, does this contradict the definition? Can we say that removing the critical points of the preimage leaves us with a properly defined manifold?","['differential-topology', 'differential-geometry']"
3523805,When is $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$,"While solving a different problem, I found Lambert’s proof (and later Laczkovich’s simplification of Lambert’s proof) that $\pi$ is irrational: https://en.m.wikipedia.org/wiki/Proof_that_π_is_irrational#Laczkovich.27s_proof But then I found this question on SE Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number. That asks for a proof for the fact that given a nonzero rational $x$ , show that $\tan(x)$ is irrational (ironically, in order to later show that $\pi$ is irrational). This got me thinking about a problem I’ve now been stuck on for a bit. If I want to generalize this claim a little more, then what can we say about $\tan$ with respect to $\mathbb{Q}(\sqrt{2})$ . Is it true that for $x \in \mathbb{Q}(\sqrt{2})$ we have $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$ ? If not that when can we say that $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$ ? Edit: where $x$ is nonzero of course","['field-theory', 'trigonometry', 'analysis']"
3523811,Intuition behind upper and lower hemicontinuity,I've been studying quite a bit about upper and lower hemicontinuity with reference to correspondences. However im having a hard time seeing the advantage of this definition and what it adds to the overall understanding of continuity in general. Whats an intuitive way to understand upper and lower hemicontiuity/ Why is it important?,['analysis']
3523819,"Given the distance $d$ between two random points on a segment, find the MLE of its length $L$.","I could solve this problem, but I made a logic jump that I'm not sure how to justify. We have a segment of length $L$ , and two random points with uniform distribution over that segment. That is, $p_{X_{1}X_{2}}(x_{1}, x_{2}) = \frac{1}{L^2}$ . The given is the distance between those two points, so $d = |x_{1} - x_{2}|$ . I don't know any easy way of dealing with the absolute value, so I took a leap of faith and assumed that $x_{1}$ is the farthest point from $0$ , so $d = x_{1} - x_{2}$ . Then I use the delta method: $$\begin{align}
 p_{D}(d) &= \iint p_{X_{1}X_{2}}(x_{1}, x_{2})\delta(d - x_{1} + x_{2})dx_{1}dx_{2} \\ &= \int p_{X_{1}X_{2}}(d + x_{2}, x_{2}) dx_{2} \\ &= \int_{-d} ^{L-d}\frac{1}{L^2}\mathbb{1}\{0 < x_{2} < L\}dx_{2} \\ &= \int_{0} ^{L-d}\frac{1}{L^2}dx_{2} \\ &=  \frac{1}{L} - \frac{d}{L^2}
\end{align}$$ Then: $$\frac{dp_{D}(d)}{dL} = \frac{2d}{\hat{L}^3} - \frac{1}{\hat{L}^2} = 0 \iff \hat{L} = 2d$$ According to my teacher this is right, but I feel like I ignored half of the possible cases by getting rid of the absolute value in $d$ . Why does this yield the same result as if I solved the problem with $d = |x_{1} - x_{2}|$ ? Thanks.","['statistics', 'probability-distributions', 'maximum-likelihood', 'probability', 'random-variables']"
3523834,Does $\lim_{n \to \infty}\frac{n}{n + \sum_{k=1}^{n}k}$ converge to $0$ or $1$?,"This converges. I am asking whether it converges to $0$ or $1$ because both seem to make sense. Using LH, the sum becomes $$\displaystyle\lim_{n \to \infty}\frac{n}{n + \displaystyle\sum_{k=1}^{n}k}\Rightarrow _{LH}\lim_{n \to \infty}\frac{1}{1+0}=1$$ However, I believe I am making a mistake because my idea is that since $\sum_{k=1}^{n}k$ is always a constant, I can differentiate term by term in terms of $d/dn$ to get $0$ , but $\sum_{k=1}^{n}k$ is in terms of $k$ . The case that this converges to $0$ is strong since $\sum_{k=1}^{n}k > n$ for all $n>1$ , as in the denominator grows much faster than the numerator. Nonetheless, I want a solid understanding of why I cannot differentiate the sum term by term in terms of $d/dn$ and, thus, not use LH. Furthermore, I suspect that we can just convert $\sum_{k=1}^{n}k$ to its sequence of partial sums, which would be in terms of $n$ , and then evaluate the limit from there, but I got lost trying to do this.","['limits', 'calculus', 'summation']"
3523838,Sum-free sets in finite groups,"Suppose $G$ is a group, $S \subset G$ . Let’s call $S$ sum-free iff $\forall a, b \in S$ we have $ab \notin S$ . Do there exist such $\epsilon > 0$ , such that every sufficiently large finite group $G$ has a sum-free subset of cardinality $\geq \epsilon|G|$ ? It was proved by Erdos using a very neat probabilistic approach, that for cyclic $G$ it is sufficient to take $\epsilon = \frac{1}{9}$ . However I do not know what happens here in the case when the group is non-cyclic.","['additive-combinatorics', 'finite-groups', 'combinatorics', 'sumset', 'group-theory']"
3523910,Confusion regarding tensor product of redundant k-tensors,"$\newcommand{\Vs}{V^{\!\star}}\newcommand{\I}{\mathcal{I}}\newcommand{\L}{\mathcal{L}}$ The following is an excerpt from Guillemin and Haine's book on Differential Forms. Note that $\L^k(V)$ denotes the set of $k$ -tensors over $V$ . Definition. A decomposable $k$ -tensor $\ell_1\otimes\cdots\otimes\ell_k$ , with $\ell_1,\dots,\ell_k\in\Vs$ is redundant if for some index $i$ we have $\ell_i=\ell_{i+1}$ .
  Let $\I^k(V)\subset\L^k(V)$ be the linear span of the set of redundant $k$ -tensors. (...) Proposition. If $T\in \I^r(V)$ and $T'\in\L^s(V)$ then $T\otimes T'$ and $T'\otimes T$ are in $\I^{r+s}(V)$ . Proof. We can assume that $T$ and $T'$ are decomposable, i.e., $T=\ell_1\otimes\dotsm\otimes \ell_r$ and $T'=\ell_1'\otimes\dotsm\otimes\ell_s'$ and that $T$ is redundant: $\ell_i=\ell_{i+1}$ . Then $$T\otimes T'=\ell_1\otimes\dotsm\otimes\ell_{i-1}\otimes\ell_i\otimes\ell_i\otimes\dotsm\otimes\ell_r\otimes\ell_1'\otimes\dotsm\otimes\ell_s'$$ is redundant and hence in $\I^{r+s}$ . The argument for $T'\otimes T$ is similar. $\square$ The proof of the proposition is rather clear to me, except for the first part: why can we assume that $T'$ is decomposable? Of course we can assume $T$ to be decomposable since the notion of redundancy is one defined for decomposable tensors, but why can we do so for $T'$ since (unless I'm horribly mistaken) there are plenty of tensors which are not decomposable? What am I missing here? Thanks in advance!","['tensors', 'tensor-products', 'dual-spaces', 'differential-forms', 'differential-geometry']"
3523918,Expectation of the first order statistic conditional on the value of the second one.,"Let $𝑋_{(1)},\ldots,𝑋_{(𝑛)}$ be the order statistics of a set of $𝑛$ independent uniform [0,1] random variables. Find the conditional expectation of $𝑋_{(1)}$ given that $𝑋_{(2)}=x_2~$ , i.e. $~\mathbb{E}(𝑋_{(1)}|𝑋_{(2)}=x_2)$ Find the conditional expectation of $𝑋_{(2)}$ given that $𝑋_{(1)}=x_1~$ , i.e. $~\mathbb{E}(𝑋_{(2)}|𝑋_{(1)}=x_1)$ That's what I got for now, but I'm not sure is my result correct. I just found the conditional pfds $f_{X_1|X_2}$ and $f_{X_2| X_1}$ using the join distribution $f_{X_1,X_2}$ $$f_{X_1,X_2}(x_1,x_2)=n!\frac{x_1^{1-1}}{(1-1)!}\frac{(x_2-x_1)^{2-1-1}}{(2-1-1)!}\frac{(1-x_2)^{n-2}}{(n-2)!} = n(n-1)(1-x_2)^{n-2}$$ and pdfs of order statistics $f_{X_1}$ and $f_{X_2}$ $$f_{X_1}(x_1) = \frac{n!}{(1-1)!(n-1)!}x_1^{1-1}(1-x_1)^{n-1} = n(1-x_1)^{n-1}$$ $$f_{X_2}(x_2) = \frac{n!}{(2-1)!(n-2)!}x_2^{2-1}(1-x_2)^{n-2} = n(n-1)x_2(1-x_2)^{n-2}$$ Hence, $$f_{X_1|X_2}(x_1,x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \frac{n(n-1)(1-x_2)^{n-2}}{n(n-1)x_2(1-x_2)^{n-2}} = \frac{1}{x_2}$$ $$f_{X_2|X_1}(x_1,x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \frac{n(n-1)(1-x_2)^{n-2}}{n(1-x_1)^{n-1}} = (n-1)\frac{(1-x_2)^{n-2}}{(1-x_1)^{n-1}}$$ And used them to find conditional expectations. $$\mathbb{E}(X_1|x_2) = \int\limits_0^{x_2} x_1 f_{X_1|X_2}(x_1,x_2)dx_1 = \int\limits_0^{x_2} \frac{x_1}{x_2}dx_1 = \frac{x_2}{2}$$ $$\mathbb{E}(X_2|x_1) = \int\limits_{x_1}^1 x_2 f_{X_2|X_1}(x_1,x_2)dx_2 = \int\limits_{x_1}^1 x_2 (n-1)\frac{(1-x_2)^{n-2}}{(1-x_1)^{n-1}}dx_2 = \frac{1+(n-1)x_1}{n}$$ Any help is highly appreciated.","['conditional-expectation', 'statistics', 'probability-theory', 'order-statistics']"
3523966,What theorem or concept should I use to approach this problem>,"I have 4 objects in group A and 4 objects in group B. One by one I need to add the 8 objects from the two different groups in a line. But the condition is that as the line is being created, the amount of ""A"" objects is always >= the amount of ""B"" objects. What I have 4 objects in group A and 4 objects in group B. One by one I need to add the 8 objects from the two different groups in a line. But the condition is that as the line is being created, the amount of ""A"" objects is always >= the amount of ""B"" objects. I need to count the total number of possible arrangements What theorem or concept should I use?  or concept should I use?","['discrete-mathematics', 'catalan-numbers', 'combinatorics', 'probability']"
3523979,How many solutions will the equation $\sin(\sin(\sin(\sin(\sin(x))))=\frac{x}{3}$ have?,"How many real solutions does the following equation have? $$\sin(\sin(\sin(\sin(\sin(x))))=\frac{x}{3}$$ I calculated the derivative of the LHS, but there was nothing I can find. Could you please give me some hints?","['calculus', 'trigonometry']"
3524084,Minimum eigenvalue of a graph,"Let $G$ be a graph on vertices $\{1,2,\ldots ,n\}$ . Suppose the vertex $1$ is connected to all other vertices. Let $\lambda$ be the least eigen value of the graph (i.e., its adjacency matrix). Is it true that $\lambda\geq -1$ ? Or is there any kind of lower bound of $\lambda$ for such graphs? Any comment/reference would be very helpful. Thank you.","['spectral-graph-theory', 'eigenvalues-eigenvectors', 'graph-theory', 'adjacency-matrix', 'matrices']"
3524146,Find the cardinality of the image of $\varphi.$,"Consider the group homomorphism $\varphi : SL_2 (\Bbb Z) \longrightarrow SL_2 (\Bbb Z/ 3 \Bbb Z)$ defined by $$\begin{pmatrix} a & b \\ c & d \end{pmatrix} \mapsto \begin{pmatrix} \overline {a} & \overline {b} \\ \overline {c} & \overline {d} \end{pmatrix}.$$ What is the cardinality of the image of $\varphi$ ? Since $\text {Im} (\varphi)$ is a subgroup of $SL_2 (\Bbb Z/ 3 \Bbb Z)$ so by Lagrange's theorem $\#\ \text {Im} (\varphi)\ \big |\ \#\ SL_2 (\Bbb Z/ 3 \Bbb Z) = 24.$ What I have observed is that $\#\ \text {Im} (\varphi) \geq 7.$ So $\#\ \text {Im} (\varphi) = 8,12\ \text {or}\ 24.$ I have just seen that the image contains at least $10$ elements. So the possibility for cardinality of $\text {Im} (\varphi)$ is $12$ or $24.$","['matrices', 'group-homomorphism', 'group-theory']"
3524193,Prove $\int^\infty_0 \frac{e^{-t}}{t}\left[\frac1{t^2}-\frac1{(1-e^{-t})^2}+\frac1{1-e^{-t}}-\frac1{12}\right]dt=\frac34-\zeta'(-1)+\zeta'(0)$,"How to prove $$\int^\infty_0 \frac{e^{-t}}{t}\left[\frac1{t^2}-\frac1{(1-e^{-t})^2}+\frac1{1-e^{-t}}-\frac1{12}\right]dt=\frac34-\zeta'(-1)+\zeta'(0)$$ ? This integral appeared in my answer , and according to an arXiv paper and the OP's conjecture this equality is very likely to be true. This is also supported by numerical evidence. You cannot find the integral in the arXiv paper, as the integral arises in my lengthy proof of a statement (whose proof is omitted) in the paper. Thus I think it is not very useful to provide the link. Real or complex approaches are welcomed. Thanks in advance.","['integration', 'riemann-zeta', 'zeta-functions']"
3524212,Why can't sphere be the quotient space of $\Bbb R^2/\Bbb Z^2$?,"I am going through section 2.8 of Aluffi's Algebra: Chapter 0. The exercise 8.10 asks us to find the quotient of $\Bbb R^2$ over $\Bbb Z^2$ . The answer is of course a torus. However, I am wondering if the sphere $\Bbb S^2$ also works. There should be a surjective group homomorphism from $\Bbb R^2$ to $\Bbb S^2$ by sending $(a,b)$ to $(r, 2\pi a, 2\pi b)$ for some fixed radius $r$ . The kernel of this homomorphism should be $\Bbb Z^2$ . By Isomorphism Theorem $\Bbb S^2$ would be isomorphic to the quotient space of $\Bbb R^2$ over $\Bbb Z^2$ . This shouldn't be right. Where did I do wrong? Any help is appreciated!","['group-theory', 'group-isomorphism', 'spheres', 'quotient-spaces']"
3524239,Eigenvalues and eigenspaces of almost complex structures under each other [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I started studying the book of Daniel Huybrechts, Complex Geometry An Introduction. I tried studying backwards as much as possible, but I have been stuck on the concepts of almost complex structures and complexification . I have studied several books and articles on the matter including ones by Keith Conrad , Jordan Bell , Gregory W. Moore , Steven Roman , Suetin, Kostrikin and Mainin , Gauthier I have several questions on the concepts of almost complex structures and complexification. Here are some: Let $L$ be $\mathbb C$ -vector space. Let $L_{\mathbb R}$ be its realification, and let the $(L_{\mathbb R})^{\mathbb C} = (L_{\mathbb R}^2,J)$ be the complexification of its realification with almost complex structure $J(l,m):=(-m,l)$ on $L_{\mathbb R}^2$ . For every almost complex structure $K$ on $L_{\mathbb R}$ , $K \oplus K$ is an almost complex structure on $L_{\mathbb R}^2$ . Then $K^{\mathbb C} := (K \oplus K)^J$ (see notation and definitions here , in particular the bullet below 'Definition 4') is $\mathbb C$ -linear, i.e. $K \oplus K$ and $J$ commute. Based on this question , it appears we have that for $K=i^{\sharp}$ , we have that $(K \oplus K)^J$ has the same eigenvalues as $J^{K \oplus K}$ Question 1. For any almost complex structure $K$ on $L_{\mathbb R}$ , does $(K \oplus K)^J$ always have the same eigenvalues as $J^{K \oplus K}$ ? Question 2. For any eigenvalues $(K \oplus K)^J$ and $J^{K \oplus K}$ have in common, do the corresponding eigenspaces have the same underlying sets? I think the answer to both questions is yes and that this need not be only for the case where we have an almost complex structure on $L_{\mathbb R}^2$ that is the realification of a complexification of a map on $L_{\mathbb R}$ (such map must, I think , be an almost complex structure on $L_{\mathbb R}$ ): Question 3. For any almost complex structure $H$ on $L_{\mathbb R}^2$ (not necessarily the realification of a complexification of a map on $L_{\mathbb R}$ ) such that $H$ and $J$ commute, does $H^J$ always have the same eigenvalues as $J^H$ ? Question 4. For any eigenvalues $H^J$ and $J^H$ have in common, do the corresponding eigenspaces have the same underlying sets? Additional questions: Question 5. For any almost complex structures $K$ and $M$ on $L_{\mathbb R}^2$ that commute, are the eigenvalues of $K^M$ a subset of $\{ \pm i\}$ ? Question 6. If yes to Question 5, then is it that $K^K$ has $i$ as its only eigenvalue if $L \ne 0$ and has no eigenvalues if $L=0$ ? (I assume $L=0$ iff $L_{\mathbb R} = 0$ iff $(L_{\mathbb R})^{\mathbb C} = 0$ iff $L_{\mathbb R}^2 = 0$ )","['complex-geometry', 'complex-analysis', 'abstract-algebra', 'linear-algebra', 'almost-complex']"
3524310,"If $f(x)=f(2x)$, then $f$ is differentiable","Suppose $f$ is continuous from $(0,\infty)\to\mathbb{R}$ , and $f(x)=f(2x)$ . Then, can we conclude that $f$ is differentiable and uniformly continuous? I think yes, because, it is clear that $f(x)=f(2^kx)$ for any $x$ and $k\in\mathbb{N}$ . Also, this implies that for any $\epsilon>0$ , $f(x+\epsilon)=f(x)$ , for $x=\frac{\epsilon}{2^k-1}$ . This implies that $f$ is constant. Hence, the differentiability and uniform continuity is trivial. any counterexamples to this? Am I right here? Thanks beforehand.","['functional-equations', 'calculus', 'real-analysis']"
3524362,Show that $f(z) = \text{Im }z$ is not differentiable,"I want to prove that $f(z) = \text{Im }z$ is not differentiable anywhere. I know how to prove it easily with the Cauchy-Riemann equations, however I'm also interested in proving it by just using the definition of differentiability. I know that the definition of differentiability for complex functions is almost the same as for real functions, but I'm still having problem proving it since we're only considering the imaginary part. Thanks in advance",['complex-analysis']
3524390,"How to find $\lim_{n \to \infty}\int_{0}^{1}\sin^2\left(\frac{1}{ny^2}\right)\,\mathrm{d}y$ if it exists?","For hours I have been trying to determine whether or not the following limit exists: $$\displaystyle{ \lim_{n \to \infty} }\displaystyle\int_{0}^{1}\sin^2\left(\dfrac{1}{ny^2}\right)\mathrm{d}y$$ My first attempt was to try and solve it as an indefinite integral, hoping a nice closed form would result: Starting with integration by parts gave $${\displaystyle\int_{0}^{1}}\sin^2\left(\dfrac{1}{ny^2}\right)\mathrm{d}y = y\cdot \sin^2\left( \dfrac{1}{ny^2} \right) + 2\displaystyle\int_{0}^{1} \dfrac{\sin\left(\dfrac{2}{ny^2}\right)}{ny^3}\mathrm{d}y$$ Which was not much help. Thus, I tried to see how far I could get with a series of substitutions, treating it as an indefinite integral: $$ v=\dfrac{1}{y} \implies {\displaystyle\int_{}^{}}\sin^2\left(\dfrac{1}{ny^2}\right)\mathrm{d}y =-{\displaystyle\int}\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v^2} \space \mathrm{d}v$$ Then, integrating by parts: $$ = -\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v}-{\displaystyle\int}-\dfrac{4\cos\left(\frac{v^2}{n}\right)\sin\left(\frac{v^2}{n}\right)}{n}\,\mathrm{d}v = -\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v} + \dfrac{4}{n}{\displaystyle\int}\cos\left(\dfrac{v^2}{n}\right)\sin\left(\dfrac{v^2}{n}\right)\space \mathrm{d}v $$ Which simplifies to $$-\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v} + \dfrac4n{\displaystyle\int}\dfrac{\sin\left(\frac{2v^2}{n}\right)}{2}\space\mathrm{d}v \tag{$\ast$}$$ At this point, I realised that the initial substitution $v = 1/y$ will lead to problems at zero when determining the new limits so I modified the problem like this: $$ v= \dfrac{1}{y} \implies \lim_{n \to \infty} {\displaystyle\int_{0}^{1}}\sin^2\left(\dfrac{1}{ny^2}\right)\mathrm{d}y =  \lim_{n \to \infty} \left(  \lim_{c \to 0}{\displaystyle\int_{c}^{1}}\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v^2} \space \mathrm{d}v \right) $$ I am still stuck at this point. However, referring back to ( $\ast$ ), I have a few conjectures about the convergence of the individual terms: Firstly, for a fixed $v$ $$\lim_{n \to \infty} -\dfrac{\sin^2\left(\frac{v^2}{n}\right)}{v} = 0$$ And secondly, $${\displaystyle\int}\dfrac{\sin\left(\frac{2v^2}{n}\right)}{2}\space\mathrm{d}v$$ is bounded above thus $$\lim_{n \to \infty}\dfrac4n{\displaystyle\int_{0}^{1}}\dfrac{\sin\left(\frac{2v^2}{n}\right)}{2}\space\mathrm{d}v = 0$$ Therefore the initial integral is indeed convergent. Right now I am trying to find the limit but no success yet. Any thoughts and ideas will be appreciated.","['improper-integrals', 'lebesgue-integral', 'real-analysis', 'multivariable-calculus', 'limits']"
3524419,Non-unital ring of rational numbers with even numerator and odd denominator has no maximal ideals,"I've been told that the non-unital ring $\{\frac{2n}{2m+1}: n, m \in \mathbb{Z}\} \subseteq \mathbb{Q} $ has no maximal ideals. I've been trying to crack this one to no avail. This example was presented (with no proof) to stress the importance of the requirement of having an identity element in order to ensure the existence of maximal ideals. Thanks in advance. EDIT : This assertion seems to be false.","['ring-theory', 'abstract-algebra', 'commutative-algebra']"
3524422,Is it possible to find a continous but nowhere differentiable function $f:E \to E$ such that $|f-h| < \epsilon$ on $E$?,"Let $h:E \to E$ be a continous function (where $E:=[0,1]$ ) and $\epsilon >0$ . Is it possible to find a continous but nowhere differentiable function $f:E \to E$ such that $|f-h| < \epsilon$ on $E$ ? Note: For the purposes of this question, it only makes sense to talk about differentiability on interior points of the domain. I know some people define one-sided ""derivatives"" at endpoints, but I do not. WLOG, we can take $\epsilon< \frac 1 {100}$ . I already know that $v:E \to E$ by $v(x) = \sum_{i=1} ^ \infty 4^{-i} \phi(4^ix)$ is a continous but nowhere differentiable function. (Where $\phi$ is the zig-zag function with period $4$ which agrees with $|x|$ on $[-2,2]$ .) One of my first thoughts was to take $f(x)= \frac \epsilon 2 v(x)+h(x)$ . This will work if $h$ is differentiable on $(a,b)$ and if the range stays small enough, but I don't think it works in general. [Look what happens when $h(x)=1- \frac \epsilon 2v(x).$ ] Can we construct a continous but nowhere differentiable function which stays close enough to $h$ ? (Ideally, constructed from $v$ ?)","['continuity', 'derivatives', 'real-analysis']"
