question_id,title,body,tags
2460548,Failure of Riesz Theorem for L-infinity,"Consider $L^{\infty }([0,1]).$ This prelim exercise is as in the title and the hint is to use Hahn-Banach. I would appreciate feedback. Here is my attempt: Suppose, toward a contradiction, that every $T\in \left ( L^{\infty }([0,1]) \right )^{*}$ is of the form $T(f))=\int_{0}^{1}f\cdot hdx$ for some $h\in L^{1}([0,1]).$ Observe that $C([0,1)]$ is a linear subspace of $L^{\infty }([0,1])$ so if we define for each $f\in C([0,1)]$ the functional $T:f\to f(0)$, we can extend it via Hahn-Banach to all of $L^{\infty }([0,1]).$ Then, there is $h\in L^{1}([0,1])$ such that $f(0)=\int_{0}^{1}f\cdot hdx.$ If $\int_{0}^{1}hdx=0$ we may take $f=1$ to arrive at a contradiction. Otherwise $E=\left \{ h\neq 0 \right \}\subseteq [0,1]$ satisfies $\lambda (E)> 0.$ Now consider for each integer $n$, $E_n=[0+1/n,1] \cap E$. If $\lambda (E_n)=0$ for all $n$, then continuity of the measure implies that $\lambda(E)=\lim \lambda(E_n)\setminus \lambda(E\cap\left \{ 0 \right \})=0, $ so in fact some $E_n:=E'$ must satisfy $\lambda(E')>0.$ Furthermore, there is an open set $U$ disjoint from $\left \{ 0 \right \}$ and a compact set $K$ such that $K\subseteq E'\subseteq U$ and $\lambda (E'\setminus K)<\lambda (E')/2$ and $\lambda (U\setminus E')<\lambda (E')/2.$ To finish, choose a continuous function such that $f(K)=\left \{ 1 \right \}$ and $f(U^c)=\left \{ 0\right \}$  Then $f(0)=0$ but $\int_{0}^{1}f\cdot hdx\neq 0$ and again, we arrive at a contradiction.","['functional-analysis', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
2460563,Can every finite product of cosines be rewritten as a finite sum of cosines?,"I used TrigReduce on products of cosines in Mathematica. It led to me to develop the formula $\prod_{i=1}^k\cos\left(\beta_i\right)=\sum_{j=1}^{2^{k-1}}\frac{\cos\left(P_j\right)}{2^{k-1}}, k \geq1$ where $P_j$ is a permutation of the signed sums of $\beta_i$'s, i.e.
$\beta_1+\beta_2+\beta_3+\beta_4+\beta_6$ and $\beta_1-\beta_2+\beta_3-\beta_4+\beta_6$ are two of the 32 permutations with $k=6 $ ($\beta_1$ always has a positive coefficient). How would I prove this? Is this formula out there in literature? I tried proving it by using the chain rule and then reverse because integration is linear over sums, and derivatives make sine functions more ""summy"". Then was going to use a recursive formula to prove by induction. But that line of reasoning doesn't explain how the $\beta$'s get in the same cosinusoidal term. Also, nothing in my undergrad used this and I couldn't find anything on google. And Mathematica doesn't give the general formula when I use k instead of a fixed number. ( I also get something similar with sinusoidal products).",['trigonometry']
2460570,Set theory identity,"Prove that $(A\triangle B)\triangle (A\cap B) = A\cup B$ . I tried and mostly went in circles. The idea is to use that 
$(A\triangle B)=(A\setminus B)\cup (B\setminus A)$ and also 
$A\setminus B = \bar A\cap B$ where $\bar A$ is absolute complement of A together with the De Morgan's laws . These are the few lines: $(A\triangle B)\triangle (A\cap B)$ = $[(\bar A\cap B)\cup(\bar B\cap A)]\triangle(A\cap B)$ = ... I would post my solution so far but I suck at LaTeX so it would take me too long.",['elementary-set-theory']
2460571,How to derive the formula for $\sum_{n=0}^{\infty}\tan^{-1}(\frac{x^{2}}{n^{2}})$ [duplicate],"This question already has answers here : sum of arctangent (3 answers) Closed 6 years ago . How to derive the formula
  $$\sum_{n=1}^{\infty}\tan^{-1}\left(\frac{x^{2}}{n^{2}}\right)=\tan^{-1}\left[\frac{\tan\left(\frac{x\pi}{\sqrt{2}}\right)-\tanh\left(\frac{x\pi}{\sqrt{2}}\right)}{\tan\left(\frac{x\pi}{\sqrt{2}}\right)+\tanh\left(\frac{x\pi}{\sqrt{2}}\right)}\right]$$ Is the formula correct? How do I derive the above formula? I tried to apply the formula integral as the limit of sum formula.",['sequences-and-series']
2460625,Finding equation of circle under the given geometric conditions,"Finding the equation of the circle which touches the pair of lines
  $7x^2 - 18xy +7 y^2 = 0$ and the circle $x^2 + y^2 - 8x -8y = 0$ and
  contained in the given circle?? My attempt The centre of required circle would lie on angle bisector of the pair of lines ie $x=y$. Assuming circle to be $(x-h)^2+(y-h)^2=r^2$ Now $2(h-8)^2=r^2$ ( distance between the extreme of larger circle and center of contained circle,) I am unable to frame a second equation . One way would be to calculate the angle between pair of straight lines and use it to find a relation between $r$ and $h$. However I was looking for a better solution or suggestion ?","['analytic-geometry', 'circles', 'geometry']"
2460635,Conjecture: A linear transformation is onto if and only if its adjoint is 1-1,"Suppose $V$ and $W$ are inner product spaces and $T:V\rightarrow W$ is a linear mapping between them. Is $T$ onto if and only if $T^*$ is 1-1? This is either a basic fact or a total fantasy. I can find support for the ""if"" but not the ""only if"", and I am curious about showing that $T$ cannot be onto if $T^*$ has nontrivial kernel. A proof for the matrix case is good enough for my purposes.","['matrices', 'linear-algebra', 'operator-theory']"
2460638,Basis of factors for large degree polynomials,"In $\mathbb{Z}_2$, the polynomial $x^{2^6}+x+1$ or $x^{64}+x+1$ factors into  $x^4+x+1$, $x^{12}+x^9+x^5+x^2+1$, $x^{12}+x^9+x^5+x^4+x^2+x+1$, $x^{12}+x^9+x^8+x^5+1$, $x^{12}+x^9+x^8+x^5+x^4+x+1$, $x^{12}+x^9+x^8+x^6+x^3+x^2+1$.  Below, under $1+x+x^{64}$, you can see the degree 12 factors arranged as columns, followed by the basis (a gray bar separates factors and basis). The same is shown for $n$ from 5 to 13. In $\mathbb{Z}_2$, $x^{2^n}+x+1$ has many factors of degree $2 n$ and the number of basis elements always seems to be $n-2$. Here are pictures of the basis for $n$ from 7 to 18. Here's Mathematica code for the first image. data = Table[Module[{ polynomials, len, polyandbasis},
  polynomials = Last[Sort[SplitBy[SortBy[CoefficientList[#, x] & /@ (First /@ 
  FactorList[x^(2^power) + x + 1, Modulus -> 2]), {Length[#], Reverse[#]} &], Length[#] &]]];
  len = Length[polynomials[[1]]];
  polyandbasis = Flatten /@ Transpose[{ 3 Transpose[polynomials], Table[{0, 1, 0}, {len}], 
  3 Transpose[Select[RowReduce[polynomials, Modulus -> 2], Total[#] > 0 &]]}];
  Column[{Text[x^(2^power) + x + 1], ArrayPlot[polyandbasis, PixelConstrained -> True, 
  ImageSize -> {800, 2 len + 4}, Frame -> False]}, Alignment -> Center]], {power, 5, 13}];
Column[{Row[Take[data, 6], Spacer[30]], Row[Take[data, {7, 8}], Spacer[60]], Row[Take[data, {9}]]}] First question: Does the $\mathbb{Z}_2$ polynomial $x^{2^n}+x+1$ have a particular name? It has a lot of nice properties. I'd like to make pictures of higher order basis elements. Unfortunately, Mathematica doesn't want to Factor $x^{1048576}+x+1$, claiming it's out of bounds. Also, PolynomialGCD doesn't like high exponents. I've looked at the Cantor–Zassenhaus algorithm and other factorization methods over finite fields, but didn't readily understand them. Is there some clever way to get the basis of the $\mathbb{Z}_2$ factors of $x^{2^n}+x+1$ for $n$ from 19 to 120 in Mathematica? Is there some nice way of quickly getting some of the degree $2n$ factors.","['mathematica', 'finite-fields', 'polynomials', 'abstract-algebra', 'factoring']"
2460666,Distribution of Stopping Value,"Suppose we have $X_i \stackrel{\text{i.i.d}}{\sim} U(0,1)$ and we define a stopping time $N =\min \{n \mid X_1+\dots +X_n >1\}.$ How can one find the distribution of $X_N$ ? I have seen how to find the distribution of $N$ using the renewal function $m(t)$ (in particular, $E[N]=e$ ) but I cannot figure out how to see the distribution of $X_N$ . Through bootstrap methods, I see it is not just uniform, but I don't see why.","['stochastic-processes', 'probability-theory', 'stopping-times']"
2460700,Embedding of some function spaces,"Consider the strictly monotone continuous function $d:\mathbb{R^+}\to\mathbb{R^+}$, and denote by $\mathcal{D}$ the space of all measurable functions $f:[0,1]\to\mathbb{R}$ such that: $$\int_0^1 d(|f(x)|) dx<\infty$$ I am wondering if there is any result on the (dense and continuous) embedding of this function space in the $L_p$ spaces?","['real-analysis', 'measurable-functions', 'functional-analysis', 'lp-spaces', 'lebesgue-integral']"
2460712,Derivative of $(x-1)(x-2)(x-3) \cdots (x-10)$ at $x=6$,"I can see that by applying the product rule, all resulting expressions containing $(x-6)$ would become zero, hence leaving $(1)(x-1)(x-2)(x-3)(x-4)(x-5)(x-7)(x-8)(x-9)(x-10)$, which, at $x=6$, is $2880$. However, I'm not sure how to formally write out a concise method for finding this solution.","['derivatives', 'polynomials', 'calculus']"
2460725,What changes in parallel transport if we choose another connection (geometric intuition)?,"Since we can choose whichever connection we want, I wanted to know what changes in parallel transporting a vector if we choose two different connections for doing so(separately). A geometric explanation will be appreciated so I can visualize each parallel transport. In addition, how can we compute or ""see"" the differences in each parallel transport? Lastly, why should one choose the Levi-Civita connection apart from that it simplifies calculations(or it seems so due to the exchange symmetry in the two down indices(covariant) of the Christoffel symbols that correspond to the Levi-Civita connection--but, even if it turns out not to be the most simple connection to perform calculations with, the question still stands)? Thank you.","['manifolds', 'riemannian-geometry', 'differential-geometry', 'smooth-manifolds']"
2460789,Jump Process - Random Walk,"A 1-D random walker strarting at time $t=0$ and location $x=0$, moves to the right ($x+1$) or the left ($x-1$) according to independent random variables $R_1,R_2,\ldots$ and $L_1,L_2,\ldots$, such that the $k^{\mathrm{th}}$ jump to the right occurs at the time $\sum_{i=1}^{k} R_i$ and the $k^{\mathrm{th}}$ jump to the left occurs at the time $\sum_{i=1}^{k} L_i$. Assume $R_i$s and $L_i$s are samples of the same probability density function $f(x)$. Show that the probability that the location of the random walker remains $x\leq M$ after the first $N$ steps to the right, tends to $1-\delta$, for all $\delta>0$, as $N, M \to\infty$, as long as $M=\mathcal{O}(\sqrt{N})$. My Solution : If $f(x)=\lambda e^{-\lambda x}$, the memorylessness of exponential random variables makes this problem equivalent to a symmetric random walk, then we can find the survival probability of a random walk and use the Brownian motion limit to prove this (see Survival Probability in here ). How about the general $f(x)$? I think we make it equivalent to another Brownian motion, I don't know how to find the parameters of that Brownian motion.","['stochastic-processes', 'random-walk', 'statistics', 'probability', 'brownian-motion']"
2460845,How to show that $\sum_{n=1}^{\infty}{2^n\over (2^n-1)(2^{n+1}-1)(2^{n+2}-1)}={1\over 9}?$,"How to show that $${2\over (2-1)(2^2-1)(2^3-1)}+{2^2\over (2^2-1)(2^3-1)(2^4-1)}+{2^3\over (2^3-1)(2^4-1)(2^5-1)}+\cdots={1\over 9}?\tag1$$ We may rewrite $(1)$ as $$\sum_{n=1}^{\infty}{2^n\over (2^n-1)(2^{n+1}-1)(2^{n+2}-1)}={1\over 9}\tag2$$ $${2^n\over (2^n-1)(2^{n+1}-1)(2^{n+2}-1)}={A\over 2^n-1}+{B\over 2^{n+1}-1}+
{C\over 2^{n+2}-1}\tag3$$ $${2^n\over (2^n-1)(2^{n+1}-1)(2^{n+2}-1)}={1\over 3(2^n-1)}-{1\over 2^{n+1}-1}+
{2\over 3(2^{n+2}-1)}\tag4$$ $${1\over 3}\sum_{n=1}^{\infty}{1\over 2^n-1}-\sum_{n=1}^{\infty}{1\over 2^{n+1}-1}+{2\over 3}\sum_{n=1}^{\infty}{1\over 2^{n+2}-1}={1\over 9}\tag5$$",['sequences-and-series']
2460873,Help with understanding Dedekind Cuts definition,"I am trying to understand the definition of what a Dedekind Cut is and I get that these are subsets of the rationals that are non-empty not not the whole of the rationals. I also sort of understand what it meant by downwards closed, but I do not understand the third part of the definition that states there is no largest element. Any suggestions? A Dedekind Cut is a subset $x\subset\mathbb{Q}$ such that: $\emptyset\neq\ x\neq\mathbb{Q}$ $x$ is downwards closed, i.e. if $q\in\ x$ and $r<q$, then $r\in\ x$ $x$ has no largest element.","['real-numbers', 'elementary-set-theory', 'discrete-mathematics']"
2460924,"$f$ is bounded, holomorphic in the unit disc, and converges to 0 uniformly in a sector then $f$ is $0$","I want to make sure that my proof to the following problem is right. I came up with this proof before I checked any stuff online. However, none of those solutions use my approach, which in my opinion, is simpler. So I wonder there may be some mistakes or gaps in my proof. Show that if $f$ is holomorphic in the unit disc, is bounded, and converges
uniformly to zero in the sector $\theta < \text{arg} z < \varphi$ as $|z| \rightarrow 1$ , then $f = 0$ . I will not include every detail in the following proof, but the skeleton is there: Proof: WLOG we may assume $\theta = 0$ ; otherwise a rotation will do. $f$ now converges uniformly to $0$ on $0 < \text{arg} z < \varphi$ . Also, this means we can extend $f$ to $\partial \mathbb{D} \cap \text{sector}$ continuously by setting $f = 0$ on it. Now there must exist $N \in \mathbb{N}$ s.t. $N \varphi > 2\pi$ . Consider $f^N$ on $\mathbb{D}$ . We know that $f^N$ on $\partial \mathbb{D}$ is zero by the nature of $N$ and $\varphi$ . Hence $f^N \equiv 0$ on $\mathbb{D}$ by the max. mod. priciple, and the result follows. Thank you very much!","['complex-analysis', 'maximum-principle']"
2460949,$\sum_{i<j}x_i x_j=1$ implies $\sum_{i\ne k}x_i<\sqrt{2}$ for some index $k$?,"A friend of mine asked me the following olympiad-style problem, and I couldn't solve it. I post it here to get some hints or advice on it. Let $x_1,\ldots,x_n$ be $n$ distinct positive real numbers, where $n\geq2$. Suppose that the sum of products of 2 numbers among them, equals 1. That is; $$\sum_{i<j}x_i x_j=1.$$ Prove that there exist some index $k$ such that $\sum_{i\ne k}x_i<\sqrt{2}$.","['combinatorics', 'inequality', 'contest-math']"
2460960,"What is the significance of the ""0<"" in the definition of limits?","$∀ε>0,∃δ>0 $ s.t. $0< |x-a|<δ⇒|f(x)-L|<ε$ In the part where it says: ""$0< |x-a|<δ$"", what is the purpose of ""$0<$""? What if it was not there? How would that change the definition?","['epsilon-delta', 'limits']"
2460963,Convergence of an infinite product,"For which $a \in \mathbb{R^+}$ does the sequence $$\gamma_n=(1+a)(1+2a^2)\cdots(1+na^n)$$ converge? Give a brief explanation. My attempt : Is easy to see that $a=0 \implies \gamma_n \equiv1$; $a \ge 1 \implies$ $\gamma_n$ diverges. Convergence: if $0<a<1$, $\gamma_n$ converges $\iff$ $\sum_{k=0}^{\infty}\ln(1+ka^k)$ converges. $$\sum_{k=0}^\infty \ln(1+ka^k) \le \sum_{k=0}^\infty ka^k=\frac{a}{(1-a)^2}<+\infty$$ so $\gamma_n(a)$ converges in $[0,1) \space \blacksquare$ Is the solution correct? If yes, Is there a briefer explanation?","['infinite-product', 'sequences-and-series']"
2460977,The only closed subscheme of an affine scheme is the scheme itself?,"I was recently reading this excerpt from Mumford's Red Book. It concerns showing that every closed subscheme of an affine scheme $\text{Spec }R$ is determined by an ideal $R$. The individual steps themselves don't seem too hard, but once I realized what he was actually proving I became very confused. He defines the inclusion morphism $f: Y \longrightarrow \text{Spec }R$ where $Y$ is a closed subscheme of $\text{Spec }R$. He then proves, in the second paragraph of the excerpt I linked above, that $f$ is surjective. I originally thought this was a typo, but it seems to be the genuine conclusion of his argument. How is this possibly? How is the inclusion of a closed subscheme of an affine scheme surjective? That would mean that every closed subscheme of an affine scheme is homeomorphic to the whole affine scheme itself. That surely can't be the case, can it? It seems that either it is a mistake, or I have hugely misunderstood something. Knowing me, it's the latter. Note also for those not familiar with the older terminology, Mumford uses the term ""prescheme"" in the same way modern authors use the term ""scheme"", and reserves that term for those preschemes which are separated.","['schemes', 'affine-schemes', 'algebraic-geometry', 'proof-explanation']"
2460978,Are the reals $\mathbb{R}$ a subset of $\mathbb{R}^2$?,"Are the real numbers $\mathbb{R}$ a subset of $\mathbb{R}^2$? $$\mathbb{R} \subset {\mathbb{R^2}}$$ Where $\mathbb{R}^2$ is defined as $\mathbb{R} \times \mathbb{R}$ whose elements are of the form $(x,y)$ where the notation $(.)$ denotes a pair and not an open interval and $x,y \in \mathbb{R}$.",['elementary-set-theory']
2460981,Curvature of a curve $\vec{r}(s)=(3+s)\hat{x}+\hat{y}$,I am having problem in calculating the curvature of the following curve.$$ \vec{r}(s)=(3+s)\hat{x}+\hat{y} $$ where $s$ is the arc length parameter. I know that $$ \kappa=\left|\frac{d\vec{T}}{ds}\right|=\left|\frac{\vec{T}'(t)}{\vec{r}'(t)}\right|=\frac{| \vec{r}'(t)\times \vec{r}''(t) |}{|\vec{r}'(t)|^3} $$ But how to calculate in terms of $s$?,['differential-geometry']
2461015,Conditional independence given the complement of an event,"Suppose $A_1,\ldots A_n$ are conditionally independent given $B$. Are they conditionally independent given $B^c$ as well?","['independence', 'probability-theory', 'random-variables']"
2461026,What is the relationship between Haar Measure and the Birkhoff-Kakutani Theorem?,"Let $G$ be a topological group. We know that if $G$ is locally compact, then there exists a left-invariant measure on $G$ which assigns finite measure to compact sets (a.k.a. the Haar measure). Analogously, the Birkhoff-Kakutani theorem says that if $G$ has a countable base at the identity (and therefore everywhere), then there exists a left-invariant metric on $G$ which generates the same topology. My question is: if $G$ both has a countable base and is locally compact, do we know anything about how this metric relates to the Haar measure? For example, are there conditions under which the closed unit ball (with respect to the metric) is compact (and therefore assigned finite volume)? Or conversely, are there conditions under which we know that a set which has finite measure also has finite diameter?","['measure-theory', 'topological-groups', 'group-theory', 'geometric-group-theory']"
2461029,Why is the degree of a rational map of projective curves equal to the degree of the homogeneous polynomials?,"Let $C_1 \subseteq \mathbb{P}^m$ and $C_2 \subseteq \mathbb{P}^n$ be  projective curves, and let $\phi : C_1 \rightarrow C_2$ be a nonconstant rational map  given by $\phi = \left[ f_1, \ldots, f_n \right]$ for homogeneous polynomials $f_i \in K[X_0, X_1,\ldots , X_m]$ having the same degree $\deg(f_i) = d$. The degree of the rational map $\phi$ is defined as \begin{align*}
\deg(\phi) = [ K(C_1) : \phi^* K(C_2)],
\end{align*} where $\phi^* : K(C_2) \rightarrow K(C_1)$ is the injection of function fields given by precomposition: $\phi^* (f) = f \circ \phi$. In many cases it seems that $\deg(\phi) = d$, but I've been unable to find a proof of this or even conditions on when this will occur. Can someone give some insight into this? Even with specific examples, I can't seem to find a basis for $K(C_1)$ as a vector space over $\phi^*K(C_2)$ to even compute the degree.","['algebraic-curves', 'function-fields', 'extension-field', 'algebraic-geometry']"
2461137,A counterexample to Cantor–Schröder–Bernstein in groups.,"We need to find a counterexample to the Cantor–Schröder–Bernstein theorem for the category of groups (that is, find two monomorphisms $\varphi\colon G\to H$ and $\psi\colon H\to G$ such that $G$ and $H$ are non isomorphic). The professor suggested us to consider the groups $G = \bigoplus_{i \geq 1} \mathbb{Z}_{2^i}$ and $H = \bigoplus_{i \geq 2} \mathbb{Z}_{2^i}$ . I constructed the following monomorphisms: Both are indeed monomorphisms, but I haven’t been able to prove that $G$ and $H$ are non-isomorphic. Does anybody have an idea? May I get a hint?","['monomorphisms', 'group-theory']"
2461147,Details of why measurable functions of independent random variables independent.,I'm looking through this proof from Durrett's Probability book I don't understand why holds.  Can someone explain this to me?,"['independence', 'probability-theory', 'measurable-functions', 'measure-theory']"
2461156,"5.	Find a universe for variables x, y, and z for which the statement ∃x∃y∀z((x = z) ∨ (y = z)) is true and another universe in which it is false.","Find a universe for variables x, y, and z for which the statement ∃x∃y∀z((x = z) ∨ (y = z)) is true and another universe in which it is false. I'm not sure if I am approaching this question the right way, but the way I think of it is like this: say one universe is all real numbers, then the statement would be true. Now I have to find another universe where this statement is false, but I can't find any universe where this statement would be false. I might be thinking of this question the wrong way though.","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
2461171,Model for probability of choosing k successful marbles from an urn of a + b marbles,"I am wondering if there exists a model for the probability of choosing at least k successful marbles from an urn of a (successful) and b marbles with replacement given n draws, where if an a marble is selected, it is replaced as a b marble but not vice versa (i.e., a marbles become b marbles if selected, but b marbles do not become a marbles). I have come across beta distributions and beta-binomial distributions, but these don't seem to be exactly what I need. Any help is much appreciated.","['game-theory', 'statistics', 'probability']"
2461214,Is vector geometry useful within economics?,"I'm going to be taking a semester of math after my bachelor's in economics before I go on to do a master's, and one of the mandatory courses in that semester is linear algebra with a focus on vector geometry. This is how they describe it: The course gives an introduction to elementary linear algebra with a focus on vector geometry. Analytic geometry in two and three dimensions: vectors, bases and
coordinates, linear dependence, equations of lines and planes, inner
product, quadratic curves, calculation of distances and angles, vector
and volume product, calculation of area and volume. Is this stuff useful within economics? I'm fairly sure the other courses are useful but I'm unsure about this one, partly I guess because I don't have a clear picture of what vector geometry really is. The course is not designed specifically for economics students so that's why I'm asking.","['finance', 'economics', 'vectors', 'geometry']"
2461245,What is wrong in this proof that any derivative function must be continuous? [duplicate],"This question already has an answer here : Where is the error in my proof that all derivatives are continuous? (1 answer) Closed 6 years ago . $f$ is differentiable on $(a, b)$. (1) Let $\alpha \in (a, b)$. $f'(\alpha) = \lim_{h \to 0}\frac{f(\alpha + h) - f(\alpha)}h$. (1) $\implies f$ is differentiable between $\alpha$ and $\alpha + h$, inclusive. By the mean value theorem, $\exists c_n$ between $\alpha$, $\alpha + h$: $f'(c_n) = \frac{f(\alpha + h) - f(\alpha)}{\alpha + h - \alpha}$. So, $f'(\alpha) = \lim_{h \to 0}f'(c_n)$. As $h \to 0$, $c_n \to \alpha$. So, $f'(\alpha) = \lim_{c_n \to \alpha}f'(c_h) \implies f'$ is continuous at $\alpha \forall\alpha\in(a, b)$. $\therefore f$ is differentiable on $(a, b) \implies f'$ is continuous on $(a, b)$. But clearly the statement must not be true as for some functions like the following, the derivative exists, but is not continuous, at zero: $f(n) =
\begin{cases}
n^2 \sin(\frac{1}{n^2})  & n \in \mathbb R \\
0 & n = 0.
\end{cases}$ $f'(0) = 0$. But $\lim_{n \to 0}f'(n)$ does not exist.","['derivatives', 'continuity', 'functions', 'proof-explanation']"
2461253,A connected graph has an Euler circuit if and only if every vertex has even degree.,"I wrote this proof for simple graphs. Any feedback is appreciated. Theorem : A connected graph has an Euler circuit $\iff$ every vertex has even degree. Proof : $P \implies Q$, we want to show that if a connected graph $G$ has an Euler circuit, then all $v \in V(G)$ have even degree. An Euler circuit is a closed walk such that every edge in a connected graph $G$ is traversed exactly once. We define $w = v_0, v_1, v_2, ... v_k$, such that $v_0 = v_k$, to be that Euler circuit in $G$ of length $k$. We assume $|V(G)| > 0$. Case 1, $0 < i < k$: Since all edges in $w$ will be traversed, but no edge can be traversed more than once, we reach $v_i$ via one edge $\{v_{i-1}, v_i\}$ and have to leave via another untraversed edge $\{v_i, v_{i+1}\}$, so $deg(v_i) = 2n$, where $n$ is equal to the number of times $v_i$ appears in $w$, for $0 < i < k$. Case 2, $i \in {0, k}$: At the beginning of $w$, we traverse any edge incident to $v_0$, that is $\{v_0, v_1\}$. By definition of an Eulerian circuit, we also have to conclude the walk at the same vertex where we started, hence we reach $v_0 = v_k$ again via a different, $k$-th edge $\{v_{k-1}, v_k\}$. Every $n$ times we pass $v_0 = v_k$ before concluding the walk, Case 1 applies, hence $deg(v_0) = deg(v_k) = 2 + 2n$, which is even. $Q \implies P$, we want to show the other direction, that is if all $v \in V(G)$ have even degree, then a connected graph $G$ has an Euler circuit. Proposition $P(n)$: If $G$ is a connected graph with $n$ edges and all $v \in V(G)$ have even degree, then $G$ has an Euler circuit. We proceed by induction on $n$. Base case $P(0)$:  A connected graph without any edges has only one vertex $v$ with $deg(v) = 0$, therefore $P(0)$ is true. For purpose of induction, we assume $P(n)$ is true. Inductive step , $P(n + 1)$: Assume $G$ has $n+1$ edges. Since all $v \in V(G)$ have even degree, we know $G$ can't be a tree, so we can find a cycle $c = u_0, u_1, ..., u_i, ..., u_k$ of length $k$ in $G$. If $k = |E(G)|$, we found an Eulerian circuit that visits all edges exactly once and are done. Otherwise, remove all edges in $c$ from $G$. The resulting subgraph $G'$ might contain $s$ connected components $G'_j$ for $0 \leq j < s$. We know that all vertices in $c$ have even degree, so any vertex in $G'_{j}$ that was adjacent to a vertex in $c$ will have an even number of edges removed, therefore the parity of all $v \in G'$ remains even. the  Also, all $G'_j$ will have at most $n$ edges. Therefore we can assume $P(n)$ for all $G'_j$, that means we can find a Eulerian walk $w_j$ in each $G'_j$. Now reinsert the edges from $c$ into $G'$, reconstructing $G$. We walk all vertices in $c$, starting at any vertex $u \in c$. When we reach a vertex $u_i$ in $c$ for the first time and $u_i$ was part of a $G'_j$, we branch into $w_j$, follow that Eulerian walk $w_j$ until its end and continue to $u_{i+1}$. We do this for all $G'_j$ until we reach $u_k$. This will form an Eulerian walk that visits all edges in $G$ exactly once. I found the last part, where the cycle is re-inserted into the graph again quite hairy to describe properly, but leaving out superfluous details.","['graph-theory', 'proof-verification', 'discrete-mathematics']"
2461260,Show that $(a+2b+\frac{2}{a+1})(b+2a+\frac{2}{b+1})\geqslant16$ if $ ab\geqslant1$,"$a$, $b$ are positive real numbers. Show that $$\left(a+2b+\frac{2}{a+1}\right)\left(b+2a+\frac{2}{b+1}\right)\geqslant16$$
if
$$ ab\geqslant1$$ Should I use AM-GM inequality? If yes, where?","['inequality', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'fractions']"
2461292,"Motivation for the norm in $W^{s,p}(\Omega )$","Let $\Omega \subset \mathbb R^n$ a bounded domain with good condition. In the fractional Sobolev space, 
$$W^{s,p}(\Omega )=\left\{u\in L^p(\Omega )\;\Bigg|\; \frac{|u(x)-u(y)|}{|x-y|^{\frac{n}{p}+s}}\in L^p(\Omega \times \Omega )\right\},$$
and we give to this space the norm
$$\|u\|_{W^{1,s}(\Omega )}=\left(\|u\|_{L^p(\Omega )}^p+[u]_{W^{s,p}(\Omega )}^p\right)^{1/p},$$
where $$[u]_{W^{s,p}(\Omega )}=\left(\iint_{\Omega \times \Omega }\frac{|u(x)-u(y)|^p}{|x-y|^{n+sp}}dxdy\right)^{1/p}.$$ Question What is the motivation for $[u]_{W^{s,p}(\Omega )}$ ? Why such an expression ? And if $s=1$ (or maybe $s\to 1$), do we have that $$\lim_{s\to 1}\ [u]_{W^{s,p}(\Omega )}^p=\sum_{i=1}^n\left\|\frac{\partial u}{\partial x_i}\right\|^p_{L^p(\Omega )}\ \ ?$$
  If not, I don't understand where would come from $[u]_{W^{s,p}(\Omega )}$.","['functional-analysis', 'sobolev-spaces', 'operator-theory', 'fractional-sobolev-spaces']"
2461370,"Why do we require the (whole) domain to be simply connected in Cauchy's theorem, residue theorem, etc.?","Main Question A very common form of Cauchy's integral theorem is Let $A\subset\mathbf{C}$ be simply connected and open, $f:A\rightarrow\mathbf{C}$ analytic, $I$ a compact interval, and $\gamma:I\rightarrow A$ continuous (or whatever) with $\gamma(I)\subset A$ . Then $\int_\gamma f(z)dz=0$ . I think it would also suffice (and not make the proof any harder) to assume that $\gamma$ is homotopic in $A$ to a point without requiring that $A$ is simply connected. Am I correct? If so, why is it never (or seldom) stated this way? The same applies to the residue theorem and other related theorems of complex analysis. Related I would be less surprised if the following statement was completely obvious: Let $A\subset\mathbf{C}$ open, $I$ a compact interval, and $\gamma:I\rightarrow A$ a continuous path homotopic in $A$ to a point. Then there exists $U\subset A$ open and simply connected with $\gamma(I)\subset U$ . I suspect this is true, and I have a vague idea how to prove it. But it's not a one-liner and I don't remember seeing it in any textbook. Am I missing something?","['complex-analysis', 'general-topology']"
2461461,Why treating $\dfrac{\mathrm d y}{\mathrm dx}$ as a fraction gives correct answer?,"Solve: $$\int x(2-3x)^{11} \, dx $$ The book I am following uses a weird technique to solve this. I  am having trouble understanding why it works. Let: 
$$ u = 2-3x$$
$$ x = \frac{2-u}{3} $$
$$ \frac{du}{dx} = -3 $$ Book Technique: 
$$dx = - \frac{du}{3}$$ $$ \int \frac{2-u}{3} u^{11} - \frac{du}{3}$$
$$ -\frac{1}{9} \int 2u^{11} - u^{12} du$$ My problem with this is that dy/dx is not a fraction but the limit of one  as  such the terms dy and dx on there own are in a sense meaningless and cannot be manipulated algebraically. For this reason, I solved it with a more formal technique: $$ \frac{dy}{du} = (\frac{2-u}{3})(u^{11}) = \frac{2u^{11}}{3} - \frac{u^{12}}{3} $$ Given, the reverse chain rule (i.e u-substitution) $$ u = g(x),  y = f(u) $$
$$ \int \frac{du}{dx} \frac{dy}{du} dx = \int \frac{dy}{du} du$$ so: $$ -\frac{1}{3} \int -3 x(2-3x)^{11} dx = \int \frac{dy}{dx} du   $$
$$ -\frac{1}{9}  \int 2u^{11} - u^{12} du $$ My question is thus: The technique used by the book is at most an approximation, i.e by assuming that $dy/dx$ is fraction we are taking an assumption that will not always hold, but in this case it seemingly does. Is this correct way of looking at why the 'book technique' works?","['derivatives', 'integration']"
2461508,Solving $x'(t)=x(t-1)$ using Laplace transform!,"I'm trying to solve $$x'(t)=x(t-1)$$ where $x(t)=1$ for $x\in[-1,0]$. I first need to show that $s\bar{x}(s)-1= \frac{1}{s}(1-e^{-s})+e^{-s}\bar{x}(s)$ where $\bar{x}(s)=\int_{0}^{\infty}x(t)e^{-st}dt$ then show that: $$\bar{x}(s)=\frac{1}{s}+\frac{1}{s^2}\sum_{k=0}^\infty\frac{e^{-ks}}{s^k}$$ I've tried integrating $\bar{x}(s)$ by parts but I can't seem to get the right expression for $s\bar{x}(s)-1$. Please can someone give me a hint? Thanks.","['ordinary-differential-equations', 'delay-differential-equations', 'laplace-transform']"
2461521,How to obtain the solution of a differential equation using a convolution integral?,"I need to express the solution of this initial value problem about vibration below using Convolution Integral; $$my''+cy'+ky=f(t) \quad y(0)=0,\quad y'(0)=0$$ But don't have any idea where do i use the Convolution Integral. So how do I do it? I tried to take laplace transform of both sides.
$$
(ms^2+cs+k)Y(s)=L(f(t))\quad (assuming \quad L(y(t))=Y(s))
$$
I presumed;
$$
f(t)=\int_0^t g(t-T)h(T) \,dT \quad L(g(t))=G(s) \quad L(h(t))=H(s)
$$
So the Convolution theorem gives me the laplace transform of the right side;
$$
L(f(t))=G(s)H(s)
$$
and putting it into the equation;
$$
(ms^2+cs+k)Y(s)=G(s)H(s)
$$
$$
Y(s)=\frac{G(s)H(s)}{ms^2+cs+k}
$$
$$
y(t)=L^{-1}(\frac{G(s)H(s)}{ms^2+cs+k})
$$
I don't know if this solution is enough or correct.","['ordinary-differential-equations', 'convolution', 'laplace-transform']"
2461556,Arranging balls in boxes,"We have $n_1$ number of balls each labelled by the number $1$, $n_2$ number of balls each labelled $2$,..., $n_k$ number of balls each labelled $k$. In how many ways can we permute them in $r$ number of boxes assuming each box can hold exactly $1$ ball. Edit: Finding permutations for $n_1$ + $n_2$ + ... + $n_k$ = $r$ would be the same as finding permutations with repetition. Moreover if we have $n_1$ + $n_2$ + ... + $n_k$ < $r$ we may introduce $r$ - ($n_1$ + $n_2$ + ... + $n_k$) balls of another type and find permutations with repetition for $r$ total balls. So finding permutations for $n_1$ + $n_2$ + ... + $n_k$ > $r$ would be enough to answer this question.","['combinatorics', 'discrete-mathematics']"
2461578,Functions of the form $z^\alpha \exp(\beta z)$ are linearly independent,"Inspired by the linear independence of the solutions to linear homogeneous constant coefficient ODEs, I want to prove that the set of functions
$$S=\left\{z \mapsto z^\alpha \exp(\beta z):\alpha,\beta \in \mathbb{C} \right\} $$
is linearly independent over the complexes. Clearly some care has to be taken with defining $z^\alpha$ for $\alpha \notin \mathbb{Z}$. Here is my attempt at a proof: Let $f_k(z)=z^{\alpha_k} \exp(\beta_k z)$ for $k=1,\dots,N$ be distinct functions of $S$, for any branch of the powers,  and suppose that
$$\sum_{k=1}^N c_k f_k(z) \equiv 0. $$
Suppose w.l.o.g. that $\beta_1$ has the largest real part, then division gives
$$\sum_{k=1}^N c_k f_k(z) \exp(-\beta _1 z) \equiv 0, $$
and taking the limit as $z \to +\infty$ leaves us only with those powers $z^{\alpha_k}$, whose corresponding $\beta=\beta_1$. That is
$$\sum_{k=1}^n c_k z^{\alpha_k} \equiv 0, $$
where $f_k(z)=z^{\alpha_k} \exp(\beta_1 z)$ for $1 \leq k \leq n \leq N$. Using the linear independence of complex powers (which I won't prove here), we find $c_1=c_2=\dots=c_n=0$. The proof can then proceed by moving on to the $\beta$ with the second largest real part. Is the above correct? Any corrections and/or more elegant proofs are welcome. Thanks!","['complex-analysis', 'linear-algebra']"
2461644,Two symmetrical coins - the difference of the tails,We have two symmetric coins - A and B. We throw a coin A and a coin B 1000 times (per coin). What is the probability that the difference between the number of tails on coin A and the tails on coin B will be at least 100? Coin throws are independent.,"['statistics', 'probability']"
2461651,Pullback of Sheaf Cohomology,"In the Wikipedia page of Sheaf Cohomology, https://en.wikipedia.org/wiki/Sheaf_cohomology#cite_note-3 In the section Functoriality, suppose $X$ and $Y$ are two topological spaces and $E$ is any sheaf of abelian groups on $Y$, then there is a pullback homomorphism,
\begin{equation}
f^*:H^i(Y,E) \rightarrow H^i(X,f^*E)
\end{equation} I only know the proof of the special case in algebraic geometry. Does anyone know a proof of this fact in the more general setting? If we change to the more categorical setting, i.e. $X$ and $Y$ are two sites and the pullback homomorphism of sheaves on $Y$ is well defined, then do we still have this pullback homomorphism at cohomology level?","['category-theory', 'algebraic-geometry']"
2461652,Diagonalization of symmetric matrix,"Suppose we have a symmetric matrix $A \in \mathbb R^{n \times n}$ (for example, the matrix corresponding to a quadratic form) which we want to diagonalize. Now the usual way to do this is to find an orthonormal basis of $\mathbb R^n$ constisting of eigenvectors of $A$ (the spectral theorem always guarantees the existence of such) and the resulting matrix $Q \in O_n(\mathbb R)$ is such that $QAQ^{-1}$ is diagonal. However, sometimes the task is to diagonalize a quadratic form, but not the way I just described, but by performing simultaneous column and row transformations (we can do that due to Sylvester's inertia theorem). Now my questions:
Are these procedures completely different from each other or are they doing the same thing? Why don't I use the spectral theorem to diagonalize a quadratic form or why don't I use the $2$nd procedure to diagonalize a general symmetric matrix? I suppose this has to do with the type of transformation, i.e. orthogonal or not, but I would like to have some good explanation.","['eigenvalues-eigenvectors', 'matrices', 'orthogonality', 'linear-transformations', 'linear-algebra']"
2461687,Prove that an orbit is heteroclinic,"I was given the following ODE as part of an exercise: $$\frac{dx_1}{dt} = x_2$$
$$\frac{dx_2}{dt} = -x_2 + \frac{x_1^2}{2} - \frac{1}{2}$$ The question is to show that a heteroclinic orbit exists for this system. My attempt: By considering the linearization around the fixed points (1,0) and (-1,0), I deduced that (1,0) is a saddle point and (-1,0) is a sink. Then I immediately concluded (by Poincare-Bendixson Theorem) that a heteroclinic orbit exists. My teacher told me that I needed to do something more before invoking the Poincare-Bendixson Theorem. Any clues and ideas on what he meant? Or any other alternative ways to approach this problem?","['ordinary-differential-equations', 'dynamical-systems']"
2461758,Deductions from combined equation of two circles,"Given combined equation of two circles $$x^4+y^4+2x^2y^2-4x^3-4xy^2+2x^2+2y^2+4x-3=0$$ In order to find the distance between the centres and the number of common tangents to the circles ,
one way would be to factorize the equation as $$(x^2+y^2-1)(x^2+y^2-4x+3)=0$$ However I was hoping whether there was another way to calculate distance between centres and number of tangents using the coeffecients  of the combined equation ? In cases where such combined equations are not easily factorisable, using coeffecients would be easier . Thus for two circles having equation S1=0 and S2=0 , if we are given (S1)(S2)=0 , then is there a way we can find the above without having to factorize it to (S1)((S2)?","['circles', 'coordinate-systems', 'geometry']"
2461773,What is the Cantor Set? How do I write it mathematically?,"I would like some clarification about the Cantor Set: What are the elements in the Cantor Set? How do I write the Cantor Set in mathematical terms (i.e in a summation)? I have seen online a formula but I do not understand how they got it so would be grateful if you could explain why it is this formula too. EDIT: The formula I have seen online is:
$$\sum_{i=0}^\infty (a(i)/3^i) $$ where a(i) is either 0 or 2.","['functional-analysis', 'cantor-set', 'analysis']"
2461785,Given $\frac{(a-b)(b-c)(a-c)}{(a+b)(b+c)(c+a)}=\frac{1}{11}$. Find $\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a}.$,"Given: $\{a,b,c\}\subset \Bbb R$, $\frac{(a-b)(b-c)(a-c)}{(a+b)(b+c)(c+a)}=\frac{1}{11}\ \ (1)$. Find the value of $\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a}\ \ (2).$ This question appeared in OBM 2005, the Brazilian Math Olympiad. The answer is $\frac{17}{11}$. But my solution is, perhaps, unnecessarily complicated as involves a lot of algebra. Is there an easy way to solve? My attempt (outline): First, I developed expression (1), getting
$$abc=5a^2c+5ab^2+5bc^2-6a^2b-6b^2c-6c^2a\ \ (3)$$
Then, developed (2) to get
$$(2)=\frac{{3abc + 2a^2 b + 2ac^2 + 2b^2 c + a^2 c + ab^2 + bc^2 }}{{2abc + a^2 b + ac^2 + a^2 c +b^2 c + ab^2 + bc^2 }}\ (4)$$
Now, replace $abc$ definition from (3) into (4), and after some additional algebra, get 
$$\frac{{17a^2 c + 17ab^2 + 17bc^2 - 17a^2 b - 17ac^2 - 17b^2 c}}{{11a^2 c + 11ab^2 + 11bc^2 - 11a^2 b - 11ac^2 - 11b^2 c}}=\frac{17}{11}$$ Question: is there a simpler way to get to the result? Simpler answers/helpful hints are welcomed.","['algebra-precalculus', 'contest-math']"
2461822,Looking for the closed form of $\sum_{n=1}^{\infty}{\zeta(2n+1)\over (2n+1)2^{4n}}$,We was able to determine $(1)$ to have this closed form $$\ln(2)-\gamma=\sum_{n=1}^{\infty}{\zeta(2n+1)\over (2n+1)2^{2n}}\tag1$$ then we when on and try to evaluate $(2)$ and we only half of the closed form $$2\ln(2)-\gamma-2X=\sum_{n=1}^{\infty}{\zeta(2n+1)\over (2n+1)2^{4n}}\tag2$$ Where $$X=\sum_{n=0}^{\infty}{\eta(2n+1)\over(2n+1)2^{2n+1}}\tag3$$ where $\eta$ is the Dirichlet eta function and $\gamma$ is Euler-Masheroni constant How do we evaluate the closed form of $(3)?$,"['analytic-number-theory', 'zeta-functions', 'sequences-and-series', 'closed-form']"
2461823,From Exponential Distributions to Weibull Distribution (CDF),"Problem: 
Let $X_1$ and $X_2$ be two independent exponential random variables with the PDFs $f_{X_i}(x_i)={1\over \lambda_i} \exp(-\frac{x_i}{\lambda_i})$ (where $i=1,2$). 
Also, let $Y=\frac{(X_1)^2 X_2}{a}$. I want to find $(Y\leq x)$ i.e. $F_Y(x)=\frac{(X_1)^2 X_2}{a} \leq x$. My attempted sol (1): $$\eqalign{&=(X_1)^2  \leq \frac{a  x} {X_2}\\
&=\int_0^\infty     X_1  \leq \sqrt{\frac{a  x} {z_2}}    \quad f_{X_2}(z_2) dz_2\\
&= {1\over \lambda_2} \int_0^\infty     \left(1-\exp\big(-{\sqrt\frac{a  x} {z_2 \lambda_1^2}}\big)\right) \exp(-\frac{z_2}{\lambda_2})   \quad dz_2\\ 
&=1-{1\over \lambda_2} \int_0^\infty \exp\left(-{\sqrt\frac{c} {z_2}}-\frac{z_2}{\lambda_2}\right) dz_2\tag{1}}$$ I know that $\int_0^\infty \exp\left(-{\frac{\beta} {4z_2}}-{z_2 \gamma}\right) dz_2 = \sqrt{β\over\gamma}K_1(\sqrt{\beta\gamma})$ from Table of Integrals, Series and Products, 7th edition - equation §3.324.1]. However, the final form of above equation contains $\sqrt{}$ and therefore cannot be solved by using §3.324.1. So if you guys can comment or provide any kind of help that would be very helpful. My attempted sol (2): $$\eqalign{&=(X_1)^2  \leq \frac{a  x} {X_2}\\
&=\int_0^\infty     X_2  \leq {\frac{a  x} {z_1^2}}    \quad f_{X_1}(z_1) dz_1\\
&= {1\over \lambda_1} \int_0^\infty     \left(1-\exp\big(-{\frac{a  x} {z_1^2 \lambda_2}}\big)\right) \exp(-\frac{z_1}{\lambda_1})   \quad dz_1\\ 
&=1-{1\over \lambda_1} \int_0^\infty \exp\left(-{\frac{c} {z_1^2}}-\frac{z_1}{\lambda_1}\right) dz_1\tag{1}}$$
Once again to the best my knowledge this above equation doesn't submit to any closed form solution. So I am stuck here.... Since, X is exponential r.v with mean $\lambda$, then $X^{1\over\gamma}$ is a Weibull (γ, β) random variable. Can we solve it this way? by using the CDF or pdf of weibull during conditioning? Any kind of help will be very much appreciated.","['probability', 'random-variables', 'probability-distributions']"
2461845,How does the hodge codifferential operator act over a wedge product?,"We have that the exterior derivative acts over a wedge product in the following manner. Let $\alpha,\beta$ be $p,q$ forms respectively. Then we have that 
\begin{equation}
d(\alpha\wedge \beta) = (d\alpha)\wedge\beta+ (-1)^p \alpha\wedge(d\beta)
\end{equation} Is it then true, by the fact that the hodge codifferential $\delta$ is the adjoint operator to $d$, that it acts in the same way, considering it has hidden inside the exterior derivative itself? For example, 
\begin{equation}
\delta(\alpha \wedge\beta)=(\delta\alpha)\wedge\beta + (-1)^p\alpha \wedge (\delta\beta)
\end{equation}
Should this be obvious? Thanks! Edit: Attempt I tried to derive the relation and this is as far as I got in a coordinate basis:
Let $\alpha = \frac{1}{p!}\alpha_{a_1\ldots a_p}f^{a_1}\wedge\ldots\wedge f^{a_p}, \, \beta = \frac{1}{q!}\beta_{b_1\ldots b_q}f^{b_1}\wedge\ldots\wedge f^{b_q}$, then one can show that $$\star d \star (\alpha \wedge \beta)=\frac{(-1)^{n(p+q-1)}s}{p!q!}\nabla_b \alpha^{[b a_2\ldots a_p}\beta^{a_{p+1}\ldots a_{p+q}]}f_{a_{2}}\wedge\ldots\wedge f_{a_{p+q}}
$$
Where $s$ is the signature of the metric, $f^a \equiv dx^a$ are the coordinate $1$-forms, and $\nabla$ is the (at least Koszul, but could be Levi-Civita) connection on the manifold. Is there a way someone can see to simplify this? Thanks again! Edit: So, by staring at it, I believe you can convince yourself that we can apply Leibniz rule to the $\alpha\beta$ term, yielding $$\star d \star (\alpha \wedge \beta)=\frac{(-1)^{n(p+q-1)}s}{p!q!}
\left[ (\nabla_b \alpha^{[b a_2\ldots a_p})\beta^{a_{p+1}\ldots a_{p+q}]} + \alpha^{[b a_2\ldots a_p}(\nabla_b\beta^{a_{p+1}\ldots a_{p+q}]}) \right] f_{a_{2}}\wedge\ldots\wedge f_{a_{p+q}}
$$ Then we have that, in coordinates, the codifferential acting on a $p$ form is $\delta = (-1)^{n(p-1)+1}s \star d \star$ and in coordinates, its action on $\alpha$ would be $$\delta\alpha = \frac{1}{(p-1)!}\left[-\nabla^b \alpha_{b a_2 \ldots a_p}\right]f^{a_2}\wedge\ldots\wedge f^{a_p}
$$ I think I'm onto something...","['hodge-theory', 'semi-riemannian-geometry', 'differential-geometry']"
2461857,Find: $\int_0^\pi x^2\ln(\sin(x))dx$,"I've been working on a few log-sine integrals. So far I have found
$$\int_0^\pi \ln(\sin(x))dx=-\pi\ln(2)$$
$$\int_0^\pi x\ln(\sin(x))dx=-\frac{\pi^2\ln(2)}{2}$$
...but I am struggling with the integral
$$\int_0^\pi x^2\ln(\sin(x))dx$$
and I can't figure it out... however, I do know that the answer will contain $\zeta(3)$. Any hints?","['real-analysis', 'logarithms', 'trigonometry', 'calculus', 'definite-integrals']"
2461858,what's the real value of $\sqrt2^{\sqrt2^{\large \sqrt2^{\sqrt2^{\unicode{x22F0}}}}}$? [duplicate],"This question already has answers here : Are these solutions of $2 = x^{x^{x^{\:\cdot^{\:\cdot^{\:\cdot}}}}}$ correct? (4 answers) Closed 6 years ago . I heard this problem from my friend today, but unfortunately we could not figure out what's happening here. $$x^{x^{\large x^{x^{\unicode{x22F0}}}}}=2$$
$$x^2=2$$
$$x=\sqrt2$$ but $$x^{x^{\large  x^{x^{\unicode{x22F0}}}}}=4$$
$$x^4=4$$
$$x=\sqrt2$$ I have a feeling that this might be a duplicate, but I could not find this problem (basically I couldn't figure out what to search). So what's happening here, and what's the real value of
$$\sqrt2^{\sqrt2^{\large  \sqrt2^{\sqrt2^{\unicode{x22F0}}}}}$$ ?","['sequences-and-series', 'limits']"
2461867,Wronskian identically zero or never zero?,Consider the equation $$y''- \dfrac{y'}{x} = 0~.$$ Solution is $$y=Cx^2 + d~.$$ The Wronskian of $x^2$ and $1$ turns out to be $-2x$ which is zero at $x = 0$ and non zero elsewhere. But the Wronskian of solutions to an equation of type $$y'' + p(x) y' + q(x) y = 0$$ should be identically zero or never zero. What am I missing?,"['wronskian', 'ordinary-differential-equations', 'calculus']"
2461870,The difference between ∈ and ⊂,"I had a task where I had to figure if the argument was true or not. $A=\{ n ∈ ℤ \mid n^2 < 5 \}, \quad  B=\{ 7, 8, \{2\}, \{2, 7, 8\}, \{\{7\}\} \}$ The first one was $\{-1, 2\} ∈ A$ and the answer to this was not true
 since the set is not an integer. The second one was $\{-1, 2\} ⊂ A$ and the answer to this was true since
 -1 and 2 are elements of $A$. I just can't seem to understand the difference between ∈ and ⊂ in this task and why the first argument was not true. Also it confuses me that the answer to the first one was that not true because the set is not an integer but aren't -1 and 2 integers? There is other parts of this task that also add more confusion: {2, 7, 8} ∈ B Answer: True since {2, 7, 8} is an element of set B {2, 7, 8} ⊂ B Answer: Not true, for example 2 is not an element in set
  B","['notation', 'elementary-set-theory']"
2461904,"Is there a pair of functions solving this ""trigonometric-like"" system of ODEs?","The (ordinary) trigonometric functions are the solutions to the system of ordinary differential equations:
\begin{align}
c' &= -as, \\
s' &= ac,
\end{align}
with $c(0) = 1$ and $s(0) = 0$, for some constant $a$. Similarly, the hyperbolic functions are the solutions to the system
\begin{align}
c' &= as, \\
s' &= ac,
\end{align}
with the same initial conditions. Is there similarly some pair of functions satisfying
\begin{align}
c' &= \bar{z}s, \\
s' &= zc,
\end{align}
again with the same initial conditions, and where $z$ is complex and $\bar{z}$ is the complex conjugate of $z$? I have tried combining the trigonometric and hyperbolic functions in various ways, but cannot find a solution. If such a pair of functions does not exist, is there a method of finding e.g. a series solution?","['special-functions', 'ordinary-differential-equations']"
2461911,How would I go about solving this tricky induction problem?,"""A crowd of at least two people stands in a room and each one holds a cake. 
At the sound of a whistle, each person throws their cake at the person closest to them. If the number of people in the crowd is odd, then there is someone who does not get a cake thrown at them. Prove this. Assume that all the distances between pairs of people are distinct."" So far, I think I should use n = 3 as the base case (not sure about any others) and then do the inductive step with n + 2 from the base case (all odd numbers I believe). My only problem is, how would I prove things like a cake not being thrown at a person and how could I apply this for odd numbers. And also, if I need one, how would I state the induction hypotheses? Any help would be highly appreciated!","['algebra-precalculus', 'induction', 'discrete-mathematics']"
2461922,Frobenius morphism not isomorphism of varieties,"Let $k$ be a field of characteristic $p>0$ and $\phi:\mathbb{A}^1\to\mathbb{A}^1$ be the Frobenius morphism $\phi(x)=x^p$. According to Hartshorne exercise I.3.2, $\phi$ is not an isomorphism of varieties. Why is this? I think $\phi$ is the identity map (by Fermat's little theorem), so $\phi$ is its own inverse morphism. Thus $\phi$ is an isomorphism of varieties. Where is my error?",['algebraic-geometry']
2461934,"If $ab+bc+ca+abc=4$, then $\sqrt{ab}+\sqrt{bc}+\sqrt{ca}\leq 3\leq a+b+c$","Let $a,b,c$ be positive reals such that $ab+bc+ca+abc=4$. Then prove $\sqrt{ab}+\sqrt{bc}+\sqrt{ca}\leq 3\leq a+b+c $ So high guys im a high schooler trying to solve this inequality. I did a few things such as try to set a> b>c and then try to generalize it but i couldn't make much progress at all. I would appreciate a clear solution to help me understand the problem in depth. I did manage to generalize that $\sqrt{ab}+\sqrt{bc}+\sqrt{ca}\leq a+b+c $ By using the AM-GM inequality on each of a b and c however havent made much progress in proving that the LHS is less than 3 and the RHS is greater than 3.","['inequality', 'substitution', 'trigonometry', 'contest-math', 'muirhead-inequality']"
2461981,$\sigma$ algebra and complete lattice,"If Ω is not finite, let’s say it is countable, then is any σ-algebra defined on it a complete lattice?
Here is what I have been thinking suppose $\Omega$ is finite then the set has Greatest Element and the least element if I am able to prove that any subset has a lower bound then it would be a complete lattice. But I am not able to figure out how to do that and also when the $\Omega$ is not finite.",['measure-theory']
2462057,Uniqueness of minimum within interval,"Assuming a smooth real valued function $$
f(x): \mathbb{R} \rightarrow \mathbb{R}
$$ with $x^{*}$ being a local minimum of $f$. Further let $$
f^{(k)}(x) := \frac{d^k}{d x^k}f(x)
$$ be the first non-zero derivative of $f$ at $x^{*}$. Question : Say I can show that $f^{(k)}$ is strictly positive on an interval containing $x^{*}$, does this imply that $x^{*}$ is the only minimum in this interval? If yes, does this extend to the multidimensional case with $f(x):\mathbb{R}^n \rightarrow\mathbb{R}$?","['derivatives', 'optimization', 'calculus']"
2462079,Expectation for a sum of Poisson random variables,"Let $Z = \sum_{r=1}^{\infty}X_{r}$, where $X_{r}$ is of Poisson Distribution with intensity rate $\frac{1}{r^{2}}$ for $r \geq 1$. Assume $Z$ is of some distribution (not necessarily Poisson itself). I need to compute the expectation, $E(Z)$, and was given as a hint to use Monotone Convergence. However, I am having a hard time setting up this problem as well as figuring out where Monotone Convergence comes into play (for example, what here is monotone)? Since $X_{r}$ is of Poisson Distribution, we should have $E(Z) = E(\sum_{r=1}^{\infty}X_{r}) =\int_{-\infty}^{\infty} \sum_{r=1}^{\infty} X_{r}\frac{e^{1/r^2} \left(\frac{1}{r^2} \right)^{k}}{k!}dX_{r}$, I think. But, I do not know how to simplify the integral on the right (or even if I have written out the expectation correctly. Then, from that point, how do I apply the Monotone Convergence Theorem? Thanks ahead of time for your time and patience!","['expectation', 'probability-theory', 'probability-distributions', 'probability', 'measure-theory']"
2462107,Are there functions f and g whose compositions are not commutative,"Find an example of functions $f, g: \mathbb{N} \rightarrow \mathbb{N}$ whose composition $f \circ g=id_{\mathbb{N}}$ and at the same time $g \circ f \neq id_{\mathbb{N}}$. First thing we can see is that $f$ is injective and $g$ is surjective, but I can't seem to  figure out what to do next.","['function-and-relation-composition', 'functions']"
2462113,Prove that $\underline{\int_A}{f(x) \ dx} + \underline{\int_A}{g(x) \ dx} \le \underline{\int_A}{[f(x) + g(x)] \ dx}$,"Question: Let $f,g:A\to\mathbb{R}$ bounded in the set $A$. Prove that a) $$\underline{\int_A}{f(x) \ dx} + \underline{\int_A}{g(x) \ dx} \le
 \underline{\int_A}{[f(x) + g(x)] \ dx}\\\le \overline{\int_A}{[f(x) +
 g(x)] \ dx} \le \overline{\int_A}{f(x) \ dx} + \overline{\int_A}{g(x)
 \ dx}$$ b) Give an example where all inequalities above are explicit * Remember that $$\underline{\int_A}{f(x) \ dx} = \sup_{P} s(f,P) = \sup_{P}
 \sum_{B\in P} m_b\cdot vol B$$ $$\overline{\int_A}{f(x) \ dx} =
\inf_{P} S(f,P) = \sup_{P} \sum_{B\in P} M_b\cdot vol B$$ where $m_b = \inf \{f(x); x\in B\}$ and $M_b = \sup \{f(x); x \in B\}$ The part $ \underline{\int_A}{[f(x) + g(x)] \ dx}\le \overline{\int_A}{[f(x)+g(x)] \ dx}$ is obvious and comes from the fact that $m\cdot vol A \le s(f,P) \le S(f,P)\le M\cdot vol A$ Now for $\underline{\int_A}{f(x) \ dx} + \underline{\int_A}{g(x) \ dx} \le
 \underline{\int_A}{[f(x) + g(x)] \ dx}$, lets think: It's $$\sup s(f,P) + \sup s(g,P) \le \sup s(f+g, P) = \\ \sup \sum_{B\in P} m_B \cdot vol B + \sup \sum_{B\in P} m_B' \cdot vol B \le \sup \sum_{B\in P} m_B'' \cdot vol B$$ where $m_B = \inf \{f(x), x\in B\}$, $m_B' = \inf \{g(x), x\in B\}$, $m_B'' = \inf \{f(x)+g(x), x\in B\}$. How to proceed here? For b) , I can only find examples for the first or last inequality, not all at the same time. Somebody has an idea?","['real-analysis', 'integration', 'calculus', 'riemann-sum']"
2462136,Is the norm of an integral operator the essential supremum norm of its kernel?,"Is the following true? Let $\mu$ be a probability measure and let $k\in L_\infty(\mu \otimes \mu)$. Define the operator $T_k\colon L_1(\mu)\to L_1(\mu)$ by $$(T_kf)(t) = \int k(t,x)f(x)\,\mu({\rm d}x).$$ Then $\|T_k\|=\|k\|_{L_\infty(\mu \otimes \mu)}$.","['functional-analysis', 'banach-spaces', 'operator-theory']"
2462142,Probability that the sum of N dice is at least X [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If N fair 6 sided dice are thrown, what is the probability that the sum of the thrown dice is at least X?","['statistics', 'probability', 'dice']"
2462175,Expected return time and limits in discrete time Markov chain,"I have a discrete time Markov chain $\{X_n : n\geq 0\}$ with state space $\{1,2,3,4,5\}$ and transition matrix $P=\begin{pmatrix}0.2&0&0&0.8&0\\ 0&0.5&0&0&0.5\\ 
0.3&0.2&0.5&0&0\\0.3&0&0&0.7&0\\
0&0.4&0&0&0.6\end{pmatrix}$ I have the following questions: (a) Calculate $\lim_{n\to\infty}p_{11}^{(n)}$ (b) Calculate the mean recurrence time of state 2, given that $X_0 = 2$. The first thing that strikes me is that this chain is aperiodic but not irreducible. However, it is easy to verify that $f_{11} = 1$, thus state 1 is recurrent and aperiodic. Then the limit in part (a) is equal to $\frac{1}{\sum_{n=1}^{\infty}nf_{11}(n)}$, which is, again, easy to work out. As for part (b), I would let $T$ denote the time of first return to state 2 and calculate $\sum_{n=1}^{\infty}P(T\geqslant n)$ to give the expectation. Here is what I would like to ask: 1.) Is my above reasoning correct? 2.) Is there an easier approach to part (a)? 3.) Could you find the expectation in (b) using $\mu_i = \frac{1}{\pi_i}$? I am not sure if you can, since the chain is not irreducible and so the chain possesses infinitely many stationary distributions.","['stochastic-processes', 'markov-chains', 'statistics', 'markov-process', 'probability']"
2462288,"Is the derivative function typically ""worse"" than the original function?","For instance, the absolute value function is defined and continuous on the whole real line, but its derivative behaves like a step function with a jump-discontinuity. For some nice functions, though, such as $e^x$ or $\sin(x)$, the derivatives of course are no ""worse"" than the original function. Can I say something that is typical of the derivative?  Is it typically not as nice as the original function?","['derivatives', 'calculus']"
2462312,Any relation between the singular values of ${\bf A}$ and ${\bf I} - {\bf A}$,"Is there any relation between the singular values of ${\bf A}$ and ${\bf I} - {\bf A}$? When $\bf A$ is Hermitian, then the singular values of $\bf A$ is just it eigenvalues, and ${\bf I} - {\bf A}$ has its singular values being $1$ minus those eigenvalues. Is there any relation when $\bf A$ is not Hermitian? Thanks! For readers of this post, see also If the absolute value of every eigenvalue of a matrix is smaller than 1, is the maximum singular value smaller than 1? .","['matrices', 'linear-algebra']"
2462328,Why mean independence does not imply independence?,"Why mean independence does not imply independence? I considered $$
\mathbb{E}(X\mid Y=y) = \mathbb{E}(X) \text{ for all } y\in \mathcal{Y} $$ This implies that $$\int_\mathcal{X} x f_{X\mid Y} (x,y) \,dx = \int_{\mathcal{X}} x f_X (x) \,dx \text{ for all } y \in \mathcal{Y}$$ For the equality to hold, the left hand side cannot have any $y$ in it after we do the integration. This seems to suggest that $f_{X\mid Y} (x,y)$ has to be free of $y$. Then, if $f_{X\mid Y} (x,y)$ is free of $y$, and the equality holds, it seems that we must have $f_{X\mid Y} (x,y) = f_X (x)$. I think the last statement I made could have some problem because we probably only have $f_{X\mid Y} (x,y) = f_X (x)$ almost everywhere . But what would be an elementary counterexample?","['probability-theory', 'examples-counterexamples', 'probability', 'analysis']"
2462338,"Continuous dependence on the RHS in ODEs, and related exercises","In Birkhoff and Rota's ""Ordinary Differential Equations"", p. 177, the following theorem is formulated (I've modified it slightly): Let $\mathbf{x}(t)$ and $\mathbf{y}(t)$ satisfy the DEs
  $$\mathrm{d} \mathbf{x}/ \mathrm{d}t= \mathbf{X}(\mathbf{x},t) \quad \text{ and } \quad \mathrm{d}\mathbf{y}/\mathrm{d} t= \mathbf{Y}(\mathbf{y},t) $$
  respectively, on $a \leq t \leq b$. Further, let the functions $\mathbf{X}$ and $\mathbf{Y}$ be defined and continuous in a common domain $D \times [a,b]$, and let
  $$|\mathbf{X}(\mathbf{z},t)-\mathbf{Y}(\mathbf{z},t)|\leq \epsilon, \quad a\leq t \leq b, \quad \mathbf{z} \in D $$
  Finally let $\mathbf{X}(\mathbf{x},t)$ satisfy the Lipschitz condition
  $$|\mathbf{X}(\mathbf{x},t)-\mathbf{X}(\mathbf{y},t)| \leq L |\mathbf x-\mathbf y|, \quad \text{if} \quad (\mathbf{x},t),(\mathbf{y},t) \in D \times [a,b]. $$
  Then
  $$|\mathbf{x}(t)-\mathbf{y}(t)| \leq |\mathbf{x}(a)-\mathbf{y}(a)| \mathrm{e}^{L|t-a|}+ \frac{\epsilon}{L} \left[ \mathrm{e}^{L|t-a|}-1 \right].$$
  The function $\mathbf{Y}$ is not required to satisfy a Lipschitz condition. The proof of the above then makes use of a differential inequality, with the ""equals case"" being a Bernoulli differential equation. I'm upset at the fact that the authors haven't mentioned on what $t$-interval the result of the theorem holds. I suspect that the interval could be shorter than $[a,b]$, if the solutions leave the domain $D$. Later, in the exercises they ask to ""bound the difference on $[0,1]$ between solutions having the same initial value $y(0)=c$ for the DEs
$$y'=\mathrm{e}^y \; \text{and} \; y'=1+y+\cdots+\frac{y^n}{n!}. ""$$
When trying to solve this exercise, I've identified the first RHS, $\mathrm{e}^y$ with $\mathbf{X}$. The first equation can be solved explicitly as
$$y(x)=-\log(\mathrm{e}^{-c}-x), $$
with the interval of existence being $$(-\infty, \mathrm{e}^{-c}). $$
Hence, the exercise doesn't even make sense for $c \geq 0$. Even if I assume that $c<0$, I'm having difficulties finding a domain $D \ni y$, such that the solution will remain in it for $x \in [0,1]$ (which should be done without solving the equation, I suppose). Overall my questions are: In the statement of the theorem, what is the interval on which the conclusion holds? Can a claim of its size be made at all? How can one solve the aforementioned exercise? Thank you!","['continuity', 'ordinary-differential-equations']"
2462340,Both variables algebraic/ both transcendental?,"Suppose $X\subset \Bbb Z_{<0}$, and $a=\sum_{n\in X} 2^n$ is algebraic, then is $b=\sum_{n\in X}3^n$ also algebraic? (To clarify, what I mean is: does it hold for all such $X$?) We know that $a$ and $b$ must either both be rational or both irrational, simply by noting the fact that under any integer base, rational numbers coincide exactly with recurring decimals. But what about being algebraic/transcendental? Is there anything we can say about it? By the way I'm no algebraist (not even a smart math student) so apologies for not being able to proceed any deeper.","['real-analysis', 'real-numbers', 'transcendental-numbers']"
2462352,Mapping Cartier Divisors to Picard Group,"I have some questions about the steps in proof of Prop. 7.1.18 in Liu's ""Algebraic Geometry"" (page 257): Here we have the Cartier-Divisor $D \in H^0(X,\mathcal{K}^*_X /\mathcal{O}^* _X) = \mathcal{K}^*_X /\mathcal{O}^* _X (X) $ represented by $ \{(U_i, f_i)_i\}$ (sheafification property) and the map $\rho: D \to \mathcal{O}_X (D)$ where $\mathcal{O}_X (D) \subset \mathcal{K}_X$ is defined by $\mathcal{O}_X(D)|U_i = f^{-1}_iO_X |U_i$ (more detailed description of used symbols: see images below). My questions (refer to red tagged lines): 1.: 
If $D$ is the $Ker$ of $\rho$, why it's image under $\rho$ has the shape $f\mathcal{O}_X$ for $f \in \mathcal{O}_X(D)$? (note: $H^0(X, \mathcal{F}) = \mathcal{F}(X)$) 2.:
Why if we have an invertible subsheaf $\mathcal{L} \subset \mathcal{K}_X$ with covering $\{U_i\}_i$ such that $\mathcal{L}|U_i$ is free this section  is generated by a $f_i \in \mathcal{K}'_X(U_i)$ and futhermore $f_i \in \mathcal{K}'_X(U_i)^*$? (therefore locally invertible) Here the used definitions: Cartier Divisors: The sheaf $\mathcal{K_X}$: The $Pic(X)$ group:",['algebraic-geometry']
2462391,A Basic Inequality,"I want to show that $e^x \leq x + e^{x^2}$ for all $x \in \mathbb{R}$.  One can see that if $x \geq 1$ then $e^{x^2}$ dominates $e^x$.  A Taylor expansion argument of the derivative followed by term by term comparison handles the rest, but I'm curious if there is a more unified way to prove this inequality without resorting to cases.","['derivatives', 'inequality', 'exponential-function', 'calculus']"
2462400,A point $P$ in the plane and a $2017$-gon,"For a regular 2017-gon $A_1A_2...A_{2017}$ in the plane, show that there exists a point P in the plane such that the following is true:
$$
\sum_{i=1}^{2017}i\frac{\mathbf{PA_i}}{|\mathbf{PA_i}|^5}=\mathbf{0}.
$$ Here is my thought: I think that the number 5 has no special meanings and that if I can answer the problem $$
\sum_{i=1}^{2017}i\frac{\mathbf{PA_i}}{|\mathbf{PA_i}|^2}=\mathbf{0},
$$
then the original can be proved by the same method. I did not do it for number 1 because I think that plugging 1 there makes the vector a unit vector and in general it shouldn't be. Then I find out that the number 2017 is not special so I tried 3 for simplicity. But I find that solving the problem by brute force even if it is only a triangle is not that easy because of the i before the term. I saw this problem in The Simon Marais Mathematics Competition (held on 7th, October, 2017) but honestly I don't know what is the correct approach to this problem? Should I try to come up with a system of equations and try to show that there has to be a solution? Or should I just use induction (I don't think this is the proper way since I cannot find a direct relation between $P_{n+1}$ and $P_n$). Please can someone give me some insights?",['geometry']
2462413,Definition of fixed points?,"I encountered fixed points in various theories like in case of real analysis while studying about fixed points, suppose $f(x)$ is a real-valued function then fixed points of $f$ are given by $x$ satisfying $f(x)  = x$. While considering Dynamical systems for 'flows' where say for one-dimensional case, the governing equation is $\dot{x} = f(x)$, in that case the fixed points are given by $\dot{x} = 0$ or $f(x) = 0$. Are the two definitions equivalent in some way? If so how are they related?","['real-analysis', 'dynamical-systems', 'fixed-points', 'ordinary-differential-equations', 'definition']"
2462513,Proof of $ A \cap B = A \iff A \cup B = B$,"I tried to see if this was asked here before but I am pretty sure im the first one - I hope I'm right. So I am supposed to show""
$$ A \cap B = A \iff A \cup B = B$$ So what I get from this obviously I need to prove both the ""if"" and ""only if"" but I get stuck really early: 
Assumption (1)
$$ x \in B  $$ 
Premise (2)
$$ A \cup B = B  $$
(1)(2) and def. '= '
$$  x \in A\cup 
B $$ 
def intersection
$$ x \in A \lor x \in B $$ 
but as you can see this isn't really going anywhere since I cannot derive $A$ from there as I have $\lor$. So my guess is that I need to assume $x \in A$. But then this seems very wishy washy. I would really appreciate any pointers or hints on where to start to tackle this problem in a better way. EDIT: i did indeed switch up A and B on the equal side, but don't think that makes a difference","['logic', 'elementary-set-theory']"
2462636,Prove that there exists a $c$ such that $f(c-1/8)=f(c+1/8)$,"Suppose that the function $f$ is continuous on $[0,1]$ and $f(0)=f(1)$ , prove that there exists a number $c$ such that $$f(c-\frac{1}{8})=f(c+\frac{1}{8})$$ I am not sure how to prove its existence, but what I did was to find the possible range that $c$ is inside: $0\le c-\frac{1}{8} \le1$ and $0\le c+\frac{1}{8} \le1$ , and solving both inequalities gives: $$\frac{1}{8}\le c\le\frac{7}{8}$$ So far I have learnt the Extreme Value Theorem, Rolle's Theorem, Mean Value Theorem, Fermat's Theorem. Appreciate it if you can help me on what should I do!","['calculus', 'functions']"
2462653,"Two-variable limit of $\lim_{(x,y)\to(0,0)}\frac{\sin(x^4+y^4)}{x^2+y^2}$","$$\lim_{(x,y)\to(0,0)}\frac{\sin(x^4+y^4)}{x^2+y^2}$$ I tried to bound it with $\frac{\sin((x^2+y^2)^2)}{x^2+y^2}$ and using polar coordinates with $x = r\cos\theta$ and $y = r\sin\theta$, but neither of the approaches provided any results. I know that the limit exists and is equal to 0, so tricks with different paths won't work. Should I use the squeeze theorem, or is there another solution?","['multivariable-calculus', 'limits']"
2462763,Boolean Algebra Axioms in Joy of Sets,"In Keith Devlin's book Joy of Sets, he has outlined a problem (1.8.1) on Boolean Algebra. Besides the axioms for commutativity, associativity and distributivity, he presents the following axioms (I am showing the relevant ones for the question) (B3) $(b \land c) \lor c = c$ and $(b \lor c) \land c = c$ (B5) $(b \land -b) \lor b = b$ and $(b \lor -b) \land b = b$ He then asks the student to prove that all $(b \land -b)$ are equal and denoted by 0. My question is this : I feel B5 is wrongly stated as it can be derived immediately from B3 (substituting -b for b, and b for c). I think the correct statement would be : (B5) $(b \land -b) \lor a = a$ and $(b \lor -b) \land a = a$ I would like to know if I am correct, or if I am wrong what is wrong with my reasoning.",['elementary-set-theory']
2462791,Can the inequality $\frac{a+b}{c} + \frac{b+c}{a} + \frac{c+a}{b} \geq 6$ be proved with differentiation? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question $$\frac{a+b}{c} + \frac{b+c}{a} + \frac{c+a}{b} \geq 6,\quad \text{with}\quad a,b,c >  0$$ I could do it with letting $x=\frac{a}{b}$, $y=\frac{b}{c}$, $z=\frac{c}{a}$, but I wonder if it is solvable somehow with differentiation.","['derivatives', 'inequality', 'a.m.-g.m.-inequality', 'algebra-precalculus', 'fractions']"
2462799,Solving non-linear second order ODEs,"Originally, I intended to solve the following pde:
$$\frac{1}{r}\frac{\partial}{\partial r}\bigg(r \theta^{\beta}\frac{\partial \theta}{\partial r}\bigg) +\frac{\partial }{\partial z}\bigg(\theta^{\beta} \frac{\partial \theta}{\partial z} \bigg)=0 ;\ 0\leq r \leq r_0; \ 0 \leq z \leq l$$ with the following BCs:
$$\theta(r,0) = 1 \text{ ; } \theta(r,l) = \theta_0 \text{ (a constant)}$$
$$\frac{\partial \theta}{\partial r}\bigg\rvert_{(0,z)}=\frac{\partial \theta}{\partial r}\bigg\rvert_{(r_0,z)}=0$$
where $\beta$ is some constant. I employed variable separation method, assuming the solution to be of the form $\theta(r,z) = R(r)Z(z)$ This lead to the following ODEs:
$$R'' + \frac{\beta R'^2}{R} + \frac{R'}{r} - \lambda^2 R =0 \qquad ; \qquad Z'' + \frac{\beta Z'^2}{Z} + \lambda^2 Z =0$$ $\lambda^2$ being separation constant Now, How do I handle these non-linear ODEs to find closed-form solution(does it exists)? Any tricks/suggestion would be greatly appreciated. Edit: As suggest by @Professor Vector, we can use variable transform in the equation and BCs and solve it like this",['ordinary-differential-equations']
2462817,Simplifying sin function and its inverse,"I am familiar with the simplifying $\sin(\arcsin(x))$ which equals $x$ as long as the domain of $\sin$ is $-1$ to $1$, however I cannot seem to simplify $\sin(2\arcsin(x/5))$, if we assume that $\sin$ is restricted to the domain of $-1$ to $1$, how would you go about solving this?","['algebra-precalculus', 'trigonometry', 'inverse-function']"
2462830,Minimum of $\sum_{l=0}^{n} \frac{1}{(l!)^2((n-l)!)^2} x^{2l} (1-x)^{2(n-l)} $.,"$\forall n \in \mathbf{N}$, prove that the function $f(x)=\sum_{l=0}^{n} \frac{1}{(l!)^2((n-l)!)^2} x^{2l} (1-x)^{2(n-l)} $ attains its minimum at $x=\frac{1}{2}$. Now it suffices to prove that
\begin{equation*}
\sum_{l=0}^{n} \binom{n}{l}^2 (x+y)^{2l} (x-y)^{2(n-l)} = \sum_{l=0}^{n} \binom{2l}{l} \binom{2(n-l)}{n-l} x^{2l}y^{2(n-l)}
\end{equation*}","['combinatorics', 'calculus', 'analysis']"
2462850,What conditions are necessary or sufficient for this manipulation of differentials to be justified?,"This comes from a problem given in ""Physics"", Halliday-Resnick-Krane, Chapter 2, Problem 55. It asks to study a non-uniformly accelerated motion defined by $ a(t)=-3v(t)^2 $ and derive a numerical value for the time elapsed given the initial and final velocity (the initial velocity is $1.5$ and the final velocity is $0.75$). This becomes a differential equation, $ v'(t)=-3v(t)^2 $. Since, of course, first-year (standard) calculus doesn't provide, as far as I know, tools to solve this, after trying in vain on my own I looked for solutions on the web, and I found the following procedure in a physics forum: $$ \frac{dv}{dt} = -3v^2 \ \implies \ \frac{dv}{v^2}=-3dt \\ \implies \int_{v_0}^v\frac{dv}{v^2}=\int_0^t(-3dt) \implies \frac{1}{v_0}-\frac{1}{v}=-3t \ .$$
The numerical result stemming from this procedure is in perfect agreement with the numerical value given by the textbook itself (the textbook gives $0.2222$, my calculator gives $0. \bar 2 $). There are quite a lot of things that I don't understand here. There's the infamous multiplication by $dt$ and consequent cancellation of it in the LHS. What assumptions need to be made to justify this, if it is even possible? Apart from that, in the same passage they also divide by $v^2$. Is this only justified in this case because they know that both the initial and final velocity are greater than zero and the velocity is strictly decreasing? If not, how? In the next passage, they integrate the LHS as if $v$ was a variable, and I'm not sure if or how the variable substitution theorem and/or the chain rule can apply in this specific context, since we're coming from an expression (between the first and second passage) that either doesn't make sense or presents differential forms, which I haven't studied yet. In general , is there a theorem that somehow justifies this notational manipulations?, And in what conditions would such a theorem apply?","['physics', 'ordinary-differential-equations']"
2462852,Proving that $f(x)=x^3+x+2$ is bijective without calculus,"I want to prove that $f(x)=x^3+x+2$, $f: \mathbb R \rightarrow \mathbb R$ is bijective without calculus. My attempts at showing to prove that it' injective and surjective are written below: $1)$ Injectivity: I want to show that $\forall a,b \in \mathbb R$ $f(a)=f(b) \implies a=b$. I started like this:
$$f(a)=f(b) \implies a^3+a+2=b^3+b+2$$ 
$$\implies a(a^2+1)=b(b^2+1)$$
$$\implies \frac{a}{b}=\frac{b^2+1}{a^2+1}$$
Then I said since $\frac{b^2+1}{a^2+1}>0$ $\forall a,b \in \mathbb R$ then either $a \land b < 0$ or $a \land b > 0$. (For the case when $b=0 \land a \in \mathbb R$ it would be easy to prove that $a=b$.) From there it seemed pretty obvious that $\frac{a}{b}=\frac{b^2+1}{a^2+1} \implies a=b$ so I couldn't really draw a logical argument to show that $a=b$. $2)$ Surjectivity: I want to show that $\forall b \in \mathbb R$ $\exists a \in \mathbb R$ s.t. $f(a)=b$. I started like this: Let $b \in \mathbb R$ and set $f(a)=b$ then we have: $$a^3+a+2=b$$
$$\implies a^3+a=b-2$$
$$\implies a(a^2+1)=b-2$$ But then I couldn't find an expression for $a \in \mathbb R$ in terms of $b$. 
So I'm wondering if anyone can tell me how I can proceed with my surjectivity and injectivity proofs.","['elementary-set-theory', 'real-analysis', 'proof-writing', 'functions']"
2462860,"We have $101$ tenorist, every two cooperated in exactly one concert, but there is no concert in which all participated.","We have $101$ tenorist, every two cooperated in exactly one concert, but there is no concert in which all participated.
Prove that someone  participated in at least $11$ concerts. This is an old problem from Moscow math Olympiad. I did try to solve it but somehow I can't. Any thoughts? Say we have $T_1,T_2,...,T_{101}$ tenorists and $A_1,A_2,...A_n$ concerts. So every pair $\{T_i,T_j\}$ is ''connected'' to exactly one concert. So we have: $$ \sum {\deg(A_i)\choose 2} = \sum \deg(\{T_i,T_j\}) = {101\choose 2}\;\;\;\;\;(1)$$ We have to prove that the degree of some $T_j$ is at least $11$ . Suppose there is no such $j$ , then for every $T_j$ the degree is at most $10$ and we have: $$ \sum \deg(A_i) = \sum \deg(T_i) \leq 1010 \;\;\;\;\;(2)$$ I don't know what to do now.","['combinatorics', 'contest-math', 'extremal-combinatorics']"
2462903,Reverse Z-Value for normal distrubtion,"Wondering if I'm on the right path on this question. Let $X$ be a Normal distributed stochastic value with $\mu=4$ and $\sigma = 7$ Find a value $C$ so that $P(X>C)=0.78$ The way I've solved this question was to first change the equation like so $$1-P(X<C)=0.78$$
$$P(X<C)=0.22$$ Then find the $Z$ value that equals $0.22$ and that would be $Z(-0.77)$ That makes my new equation equal to. $$P(X<C)=Z(-0.77)$$
I then use the formula for the normal distrubtion and solve for a C that would make the equation equal to $-0.77$ $$\frac{C-4}{7}=-0.77$$
And then I get the answer $$C=-1.39$$ I would believe my execution is right but it differs from the solution my teacher has given. Granted his solution set has a couple of miscalculations because it has not been verified I have yet to find anyone complain about his answer on this one which leads me to believe that I have made an error. His answer was, by the way, $$C=9.39$$","['stochastic-processes', 'statistics', 'standard-deviation', 'normal-distribution']"
2462937,Prove that no uncountable family of subsets of $\mathbb{N}$ is well-ordered by relation of inclusion.,"Prove that no uncountable family of subsets of $\mathbb{N}$ is well-ordered by relation of inclusion. I had two ideas how to do it. First was to show that if such family is uncountable, then it is not true that all proper beginning segment (is it correct english name for that?) is of the form $O(x)=\{y:y<x\}$ for some $x$. Another idea was to assume that $(A,\subseteq)$ is such well-order. There are $a,b\in A, a\subset b$, such that between $a$ and $b$ there are uncountably many elements. Let's take subset $B$ which contains $a,b$ and all elements between them. Let's define sequence $z_n$, where $z_0=b$ and $z_{n+1}$ is element of set $\{x\in B:x<z_n$ and there are uncountably many elements between $a$ and $x\}$. By axiom of choice such sequence exists. Let's define $C=\{z_n:n\in\mathbb N\}$. Of course $C\subset A$ and $C$ does not contain minimal element. Therefore $(A,\subseteq)$ is not well-order. Is it correct?","['order-theory', 'elementary-set-theory']"
2463047,not understanding identity theorem for polynomials,"Suppose $f(x)$ is a polynomial such that $f(x) = a_nx^n +
 a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \cdots +a_1 x + a_0$ and there are
at least $n+1$ different values of $x$ for which $f(x) =0$ . (a) If $a_n\neq 0$ , then what does the Fundamental Theorem of Algebra
tell us about the number of different values of $x$ for which $f(x) =
 0$ ? (b) Why must we have $f(x) = 0$ for all values of $x$ ? It is with the answer to question (b) that I have the most difficulty. The answer to (a) and (b) is: If any of the $a_i$ are nonzero, then $\deg f \le n$ . Therefore, by
the Fundamental Theorem of Algebra, if any of the $a_i$ are nonzero,
then $f(x)$ cannot have more than $n$ roots. However, we are told that
there are at least $n+1$ values of $x$ for which $f(x) = 0$ . So, it is
impossible for any of the $a_i$ to be nonzero (since then the
Fundamental Theorem of Algebra would force $f(x)$ to have no more than $n$ roots). Therefore, all the $a_i$ must be 0, which means $f(x) = 0$ for all values of $x$ . □ I'm having difficulty understanding what is meant with any of the $a_i$ are nonzero . What does this mean? That none of them are 0? As far as I'm understanding, $f(x) = 0$ , like, literally. Then, there is the following, related bit: Suppose that $f(x)$ is a quadratic polynomial such that $f(2) = 4$ , $f(3) = 9$ , and $f(4) = 16$ . Prove that $f(x)=x^2$ . By inspection, we note that $f(x) = x^2$ satisfies $f(2) = 4$ , $f(3) =
 9$ , and $f(4) = 16$ . However, maybe there are other quadratics $f(x)$ such that $f(2) = 4$ , $f(3) = 9$ , and $f(4) = 16$ . How can we tell if $f(x)= x^2$ is the only possible one? We let $g(x) = f(x) - x^2$ and focus on $g(x)$ , because we know that
2, 3, and 4 are all roots of $g(x)$ . Therefore, the Factor Theorem
tells us that $x-2$ , $x-3$ , and $x-4$ are all factors of $g(x)$ . So,
we have $$ g(x) = (x - 2)(x - 3)(x - 4)q(x), $$ for some polynomial $q(x)$ . This tells us that either the degree of $g(x)$ is at least 3,
or $q(x)=g(x)= 0$ for all values of $x$ . However, because $g(x) = f(x)
 -x^2$ and $f(x)$ is a quadratic, we know that the degree of $g(x)$ cannot be larger than 2. Therefore, we must have $q(x) = g(x)= 0$ for
all $x$ . Finally, since $f(x) = g(x) + x^2$ , we know that $f(x) = x^2$ for all $x$ . But why is $g(x) = f(x) - x^2$ ? I mean, I know why , but why not any other polynomial? In other words, I'm not convinced; I'm still thinking that there might be some polynomial out there which satisfies the conditions. For example: $f(1) = 1$ , $f(2) = 8$ , $f(3) = 27$ , and $f(4) = 64$ . This looks like it might be $x^3$ , but actually, it is $f(x) = x^3 + (x - 1)(x - 2)(x - 3)(x - 4) \left( 16,\!000,\!000x^{427} 473.15x^{101} - \frac{\pi}{\sqrt{2}} x^{23} + x^5 + 99 \right)$ I could have gone and done: $g(x) = f(x) - x^3$ with $f(x) = x^3$ . Then, $g(x) = (x - 1)(x - 2)(x - 3)(x - 4)q(x)$ . Since the degree of $g(x)$ cannot be larger than 3, we must have that $q(x) = g(x) = 0$ . Finally, we have $f(x) = g(x) + x^3$ , and we know that $g(x) = 0$ , and so $f(x) = x^3$ . But apparently it doesn't proof anything because there's a polynomial out there that produces the same result! So what did I proof?","['algebra-precalculus', 'polynomials']"
2463070,Why are functions with vanishing normal derivative dense in smooth functions?,"Question Let $M$ be a compact Riemannian manifold with piecewise smooth boundary. Why are smooth functions with vanishing normal derivative dense in $C^\infty(M)$ in the $H^1$ norm? Here I define $C^\infty(M)$ to be those functions which have all orders of derivative continuous on $M$ and smooth in its interior. For example, $(x\mapsto \sin(\pi x))\in C^\infty([0,1])$ but $(x\mapsto \sqrt{x})\notin C^\infty([0,1])$. I have crossposted this to MathOverflow . Background This is inspired by my belief that the form domain of the Friedrichs extension of the Neumann Laplacian on $M$ is equal to $H^1(M)$. If my belief is wrong, I would certainly accept as answer a counterexample, preferably with some discussion/references. Here are the approaches I'm exploring. Given $u\in C^\infty(M)$ construct a function that agrees away from an $\epsilon$ neighborhood of $\partial M$, but has been modified to have zero normal derivative. This is described below, but runs into some trouble at corners and with smoothing. Given $u\in C^\infty(M)$, find some $\eta$ supported on an $\epsilon$ neighborhood of $\partial M$ such that $\nabla\eta|_{\partial M}$ is equal to the projection of $\nabla u|_{\partial M}$ onto the normal direction and $\eta$ and $|\nabla\eta|$ uniformly bounded in $\epsilon$. Then $u - \eta$ will be the desired approximation and uniform boundedness will imply $\|u - (u-\eta)\|_{H^1}\to 0$. I'm not sure if this is a search for an integrable harmonic vector field or if it's a constrained optimization problem. Simply show that any $u\in C^\infty(M)$ that is perpendicular to all smooth functions with vanishing normal derivative must be zero. In order to do this, I think it still runs into the same fundamental difficulty as the others, which is constructing functions with vanishing normal derivative supported on an $\epsilon$-neighborhood of the boundary. (I'm also happy to simply have a reference to follow up. A reference for domains in $\mathbb{R}^n$ should be fine, too, and just a partition of unity away from a Riemannian manifold.) Current Approach The approach I'm considering right now is an elaboration on possibility (2) from my list. The sketch is: set the boundary condition that $\nabla\eta$ on the boundary be equal to the normal component of $\nabla u$ and find an integrable harmonic vector field $E$ satisfying that boundary condition. Then cut off $E$ so it is only supported in a neighborhood of $\partial M$, and let $\eta$ be so that $E = \nabla\eta$. Intuition is that the maximum principle will control $|E|$ and $|\eta|$, so that $\eta$ will be bounded above in the $H^1$ norm by a constant times the volume of the $\epsilon$-neighborhood of $\partial M$. Another approach is inspired by zhw's comment below: approximate $\nabla u$ among $L^2$ vector fields, multiply it by a cutoff function so it is supported in the interior of $M$, and then integrate it to approximate $u$. This should work with some tweaking in $[0,1]$ but I'm not sure how well it will work in general. Older work My approach has been as follows. The intuition is to take an arbitrary smooth function, restrict it to the complement of a collar neighborhood, then extend the restriction to the collar neighborhood so that the value is constant on inward-normal geodesics. However I'm running into issues at corners. Let $\epsilon > 0$ be such that $\{p\in M\ |\ d(p,\partial M) < \epsilon\}$ is a collar neighborhood of $\partial M$. Let $e_\epsilon(p)$ for $p\in\partial M$ be the smaller of $\epsilon$ or the greatest time parameter such that the inward normal geodesic collides with no other inward normal geodesic. Let $N$ be the set of inward normal vectors on $\partial M$ whose lengths are no greater than $e_\epsilon$. The interior of the set $\operatorname{exp}(N)$ is foliated by geodesics. Edit- Note this is not necessarily true if the manifold has inward corners. Given a smooth, continuous function $u$ on $M$, define $\bar{u}$ to be the restriction of $u$ in the complement of $\operatorname{exp}(N)$ and on each geodesic leaf of the interior of $\operatorname{exp}(N)$ define $\bar{u}$ to take the value that $u$ takes on the inward limit of that leaf. Here's trouble. As $\bar{u}$ need not be smooth, I'd like to mollify it. Edit- I had a detail incorrect. A standard mollifier produces a function defined on compact subsets of the interior of $M$. So the mollifier has to be modified. One idea I'm following up on is varying the support of the mollifying function based on distance to $\partial M$. I'm skeptical, as varying the mollifying function will add another component to the gradient, but if it works I'll post as an answer.","['riemannian-geometry', 'reference-request', 'functional-analysis', 'sobolev-spaces', 'manifolds-with-boundary']"
2463099,On periodic and nonperiodic functions,"It's known that $\sin(x^2)$ is not periodic. Are there any theorems which generalise it? I mean something like ""$f(x^n)$ is not periodic if $f$ is a nonconstant periodic function and $n>1$""? The above conjecture turned out to be false. Dave L. Renfro gave a counterexample. What if we add continuity assumption? New conjecture: If $f$ is a nonconstant continuous periodic function and $n>1$ is an integer, then $f(x^n)$ is not periodic.","['periodic-functions', 'functions']"
2463111,Solving differential equation $\frac{dy}{dx} = ay - by^2$,"this is my second differential equation I have to solve and I need some help: $$\frac{dy}{dx} = ay - by^2 ; a > 0, b \ge 0$$ Okay, so this is what I have done: $dy/dx - ay = -by^2 | :y^2$ $\frac{1}{y^2}\cdot \frac{dy}{dx} - \frac{a}{y} = -b$ Substitution: $u = \frac{1}{y}$, so $\frac{du}{dx} = \frac{-1}{y^2} \cdot \frac{dy}{dx}$ and therefore $ - \frac{-du}{dx} = \frac{1}{y^2} \cdot \frac{dy}{dx}$ Insert this above: $- \frac{du}{dx} - au = -b$ $\frac{du}{dx} + au = b$. I hope this is correct. If so, I do not know how to conclude. Thank you very much for any kind of help!","['ordinary-differential-equations', 'proof-verification']"
2463116,Frechet Derivative of Evaluation function,"This is an exercise from Jack K.Hales book on ODEs Let $X=C^1([0,1],\mathbb{R}^n) \times [0,1]$ and consider
$$
w:X\longrightarrow \mathbb{R}^n
$$
where $w(f,t)=f(t)\in \mathbb{R}^n$. The point is to show that $w$ is Frechet differentiable and calculate it's derivative. Some thoughts are that if we fix $f\in C^1([0,1],\mathbb{R}^n)$ then
$$
|f(t+h)-f(t)+D_{t}(f)h|\leq r(t,h)
$$
with $\frac{ r(t,h)}{h}\longrightarrow 0$ for some function $r$ and $D_{t}(f)$ denoting the total derivative of the vector valued $f$ at $t$. Now would it be correct to define the operator
$$
D:X\longrightarrow \mathbb{R}^n
$$
with $D(f,t)=D_{t}(f)$ to be the Frechet derivative of $w$?","['functional-analysis', 'frechet-derivative', 'ordinary-differential-equations', 'derivatives']"
2463122,Evaluate $\int_{0}^{\infty} \frac{\ln x}{x^2+6x+10}dx$,Evaluate $$\int_{0}^{\infty} \frac{\ln x}{x^2+6x+10}dx$$ The given answer is $0.370429$. Is there any method to solve this? Thanks in advance.,"['definite-integrals', 'calculus']"
2463130,Logic of the implication in $ε$-$δ$ proofs,"Im confused by why epsilon delta proofs logically work. An example is Proof: 
Given $ε>0$, choose $δ = {ε\over3}$. For all $x$, if $0<|x−2|<δ$ then $|(3x−1)−5| < ε$. That last part if $0<|x−2|<δ$ then $|(3x−1)−5| < ε$ LOOKS A LOT LIKE $P\to Q$ because of the ""if then"" but yet the proof in the book solves it like as if its $Q\to P$?: $$\begin{align}|(3x−1)−5| &= |3x−6|\\ &= |3(x−2)|\\ &= 3|x−2|\\ &<3δ\\ &= 3\left({ε\over3}\right) \\ &= ε\end{align}$$ So my question is how come it looks like a $P\to Q$ proof but yet we start with $Q$ to show $P$?","['limits', 'calculus', 'epsilon-delta', 'proof-writing', 'proof-explanation']"
2463137,Error in classical RK method. The result went to infinity.,"I used classical ($4^\text{th}$ order) Runge-Kutta method to solve the ODE $$y'=5e^{5t}(y-t)^2+1,0\leq t\leq 1, \ y(0)=-1.$$ $h$ is the step size. When $h=0.2$, I got a good approximation of the solution. However, when $h=0.25$, the result went to infinity. What's the reason for this? What's restriction of the step size and convergent condition for classical RK method?","['numerical-methods', 'computational-mathematics', 'ordinary-differential-equations', 'runge-kutta-methods']"
2463183,The sum of series $\frac14+\frac{1\cdot3}{4\cdot6}+\cdots$,"The problem I have here today is the following;
$$\frac{1}{4}+\frac{1\cdot3}{4\cdot6}+\frac{1\cdot3\cdot5}{4\cdot6\cdot8}+\cdots$$ the problem is exactly phrased like this (I can't say that the $\infty$ sign is a bit unnecessary at the end), My Attempts We can generalize this sum by noticing that each time indices get greater the denominator and numerators are multiplied by $n+2$ for each $n$ either in numerator or denominator, we take $\dfrac{1}{4}$ out of the sum first, so the sum is equal to $$\dfrac{1}{4}+\sum_{k=4} \frac{(k-3)(k-1)}{k(k+3)}$$ then we open up the brackets and we get; .... Then I was a little stuck here because when I opened up these brackets and try to get the partitions of the sum one of them was logical $\displaystyle\sum\frac{3}{k(k+3)}=\sum\frac{1}{k}-\frac{1}{k+3}$. I couldn't carry it out longer. What do you suggest? Is this a problem way above elementary solutions?","['algebra-precalculus', 'summation', 'sequences-and-series']"
2463202,A question on Grassmannian,"Let $T: Gr(n,\mathbb C^{2n}) \rightarrow G(n,\mathbb C^{2n})$ be the involution defined by $W \rightarrow W^{\perp}$ with respect to a symplectic form on $\mathbb C^{2n}$. Is there a direct proof (without referring to the cohomology) of the fact that $T$ induces an involution on the set of young diagrams contained in rectangular diagram of shape $n \times n$ ? And, the involution takes a young diagram to its transpose.","['projective-geometry', 'representation-theory', 'algebraic-geometry', 'symplectic-geometry']"
2463212,find matrix representation of projection onto kernel,"How can I efficiently find a matrix $P_A$ that projects onto the kernel of another matrix $A$? That is, given $Ax=0$, is there an ""efficient"" (without using inverses) way to find $P_A$ such that $P_A x \in ker(A)$, i.e., $A(P_A y) = 0$ for $Ay \neq 0$? I know that I can do this with the orthogonal projection $Q_A := I - A^{\dagger}A$ for $A^{\dagger} = A^T(AA^T)^{-1}$ when $A$ has full row rank or $A^{\dagger} = (A^TA)^{-1}A^T$ when $A$ has full column rank, but I'm interested in other projections onto $ker(A)$ where we're not required to compute the pseudoinverse because of two reasons SVD of $A^TA$ or $AA^T$ might be too expensive or unreliable (because of size or poorly conditioning, respectively) $A$ might be rank deficient (not have full row rank or full column rank) In these cases, what's the best way to project onto the kernel of $A$? I imagine that we must rely on knowing something about the structure of $A$.",['linear-algebra']
2463255,Question about getting probability from a CDF,"Just wondering but if I had the following CDF: $$ F_X(x) = \begin{cases}
0 & x < 0 \\
\frac{x+2}{8} & 0 \leq x < 6 \\
1 & x \geq 6 \\
\end{cases} $$
If I wanted to calculate the probability $P(X=6)$ and $P(7\leq X \leq 8)$ How would I go about this ? In my textbook and in my class we have only ever talked/did examples where the probability were within the interval, say for this example,  $[0,5]$ so I am just curious on how I would do this type of question here. Thank you","['statistics', 'probability']"
2463267,Sequence of Functionals,"There are some statements in something that I'm reading that I don't quite understand: Consider $c=\sup c_n$ where $c_n$ is a non decreasing sequence of non-negative, uniformly continuous functions. Define the following functional: $$I_j[\pi] = \int c_j \, d\pi$$
  Also consider a sequence of measures $(\pi_n)$ which admits a cluster point $\pi_*$.
  Whenever $n \geq m$ we have $I_n[\pi_n] \geq I_m[\pi_n]$ (because $c_n \geq c_m$ for the same $\pi_n$). By continuity of $I_m$:
  $$\lim_{n \rightarrow \infty} I_n[\pi_n] \geq \limsup_{n \rightarrow \infty} I_m[\pi_n] \geq I_m[\pi_*]$$
  Further:
  $$\lim_{n \rightarrow \infty}I_n[\pi_n] \geq \lim_{m\rightarrow \infty}I_m[\pi_*]$$ So I'm trying to unpack the string of inequalities and the questions I have are: Why is this true: $$\lim_{n \rightarrow \infty} I_n[\pi_n] \geq \limsup_{n \rightarrow \infty} I_m[\pi_n]$$ I suspect continuity of $I_m$ is used in the second part of the inequality on the first line:
$$\limsup_{n \rightarrow \infty} I_m[\pi_n] \geq I_m[\pi_*]$$
I don't see how its used though. On the second line:
$$\lim_{n \rightarrow \infty}I_n[\pi_n] \geq \lim_{m\rightarrow \infty}I_m[\pi_*]$$
Does this follow from the first string of inequalities? In particular, we showed this statement was true for any particular $m$ on the first line. So we assert it must be true for any $m$ no matter how large?","['real-analysis', 'probability-theory']"
