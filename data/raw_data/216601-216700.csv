question_id,title,body,tags
4405473,Doubt in identifications for a vector field in the product of manifolds,"The following is the Exercise 1) a), Chapter 6 from do Carmo, Riemannian Geometry: Let $M_1$ and $M_2$ be Riemannian manifolds and consider $M_1 \times M_2$ with the product metric. Let $\nabla^1$ and $\nabla^2$ be the Riemannian connection in $M^1$ and $M^2$ , respectively. a) Show the Riemannian connection $\nabla$ of $M_1 \times M_2$ is given by $$\nabla_{X_1 + X_2}(Y_1 + Y_2) = \nabla^1_{X_1}Y_1 + \nabla^2_{X_2}Y_2,$$ where $X_1,Y_1 \in \mathcal{X}(M_1)$ and $X_2,Y_2 \in \mathcal{X}(M_2)$ . Here are my doubts: Doubt 1) I'm trying to understand what exactly means, for example, the sum $X_1 + X_2$ . Is it just a notation for $(X_1, X_2) \in \mathcal{X}(M_1) \times \mathcal{X}(M_2)$ ? Doubt 2) Given $p = (p_1,p_2) \in M_1 \times M_2$ , it's really commum to identify $$T_{p}(M_1 \times M_2) \equiv T_{p_1}M_1 \oplus T_{p_2}M_2.$$ Again, the meaning of the last direct sum is just $T_{p_1}M_1 \times T_{p_2}M_2$ ? In this case, the identification would be $$L : T_{p}(M_1 \times M_2)  \rightarrow T_{p_1}M_1 \times T_{p_2}M_2$$ defined by $L(w) = (\alpha_1'(0), \alpha_2'(0)),$ where $w \in T_{p}(M_1\times M_2)$ is given by $w = c'(0)$ , for $c(t) = (\alpha_1(t), \alpha_2(t))$ with $\alpha_1 : I \rightarrow M_1$ and $\alpha_2 : I \rightarrow M_2$ ? In this case, using the natural structure for the product $M_1\times M_2$ and the map $L$ , I conclude that a basis for $T_{p_1}M_1 \times T_{p_2}M_2$ would be: $$\{\partial_1, ...,\partial_{n}, \partial_{n+1}, ..., \partial_{n+m}\},$$ where $\partial_i = (\partial^1_i,0)$ , for $i = 1, ..., n$ and $\partial_i = (0, \partial^2_i)$ , for $i = n+1, ..., n+m$ [here $\partial^1_i$ and $\partial^2_i$ are tangent vector of the basis of $T_{p_1}M_1$ and $T_{p_2} M_2$ respectively]. Doubt 3) In item a) above, I believe the author is using the identification
""a tangent fild in $\mathcal{X} (M_1\times M_2)$ is given by $(X_1, X_2).$ In this case, what would mean $(X_1, X_2)(f),$ for $f \in C^{\infty}(M_1 \times M_2)$ ? What would be its expression on a local coordinate system of $M_1 \times M_2$ ? (I tried this using the basis I wrote in Doubt 2.) Doubt 4) What's the exactly relation between $\mathcal{X}(M_1\times M_2)$ and $\mathcal{X}(M_1) \times \mathcal{X}(M_2)$ ? What a tried for item a) : In general, given a Riemannian manifold $M$ of dimension $n$ , it's Levi-Civita connection is given by: $$\nabla_{X}Y = \sum_{k=1}^n(X(b_k) + \sum_{i,j=1}^n a_{i} b_{j} \Gamma_{ij}^k)\partial_k,$$ where, in coordenates, $X = \sum_{i=1}^n a_i \partial_i$ and $Y = \sum_{j=1}^n b_j \partial_j$ . I tried to go from this formula and use all the things I said in the Doubts. But I always get stuck when involves the functions in $C^{\infty}(M_1 \times M_2)$ . Something seems not to fit. Anyone may help me ?","['tangent-spaces', 'riemannian-geometry', 'differential-geometry']"
4405503,Finding the number of real roots of a polynomial,"I want to find the number of real roots of $f(t)=t^4-2t^2+4t+1$ . As $f(0)=1>0, f(-1)=-4 <0$ and $f(-2)=1>0$ , I can say that there are two real roots since the polynomial is continuous. For the rest of the roots (which are complex), I think Rolle's theorem may be used but I could not find a way to show it. How can I proceed?","['rolles-theorem', 'calculus', 'derivatives', 'polynomials']"
4405509,"if $\gcd(a,n) = 1$, show that $x^x\equiv a\pmod n$ always has a solution.","Recently, I became aware of this result: $\textbf{Theorem 1:}$ if $n>1$ and $a$ is relatively prime to $n$ , then there exists an integer $x$ such that $x^x \equiv a\pmod n$ . I found the proof of this theorem on this page (at the bottom of the page), but I was not able to fully understand it, could you help me? It goes as follow: $\textbf{Proof 1:}$ let us introduce an intermediate lemma. $\textbf{Lemma:}$ if $m\in\mathbb{N}^*$ is such that $\gcd(m,\varphi(n))=1$ , then $0^m,1^m,2^m,\dots,(n-1)^m$ is a permutation of $0,1,2,\dots, n-1$ (modulo $n$ ). ( $\textbf{Edit:}$ as noticed by @metamorphy, the lemma only hold when $n$ is square-free) $\textbf{Proof of the lemma:}$ Let $p$ be a prime divisor of $n$ . If $a,b$ are such that $a^m\equiv b^m\pmod{n}$ , then in particular $a^m\equiv b^m\pmod{p}$ . Because $\gcd(m,\varphi(n))=1$ , we have $\gcd(m,\varphi(p))=1$ , so there exists $\lambda,\mu\in \mathbb{Z}$ such that $\lambda m + \mu \varphi(p) = 1$ . As a result, $$a\equiv a^{\lambda m + \mu \varphi(p)}\equiv b^{\lambda m + \mu \varphi(p)}\equiv b\pmod{p}$$ Note that we can lift $a$ and $b$ to the power $\lambda m$ and $\mu \varphi(p)$ because both $a$ and $b$ are $\equiv 0$ or they are both invertible modulo $p$ (as $p$ is a prime). There, I don't understand how we conclude: Using the Chinese Remainder Theorem, we conclude that $a\equiv b\pmod{n}$ , which proves the lemma. I also don't understand the next part of Proof 1: For every $m\in \mathbb{Z}/p\mathbb{Z}$ , let $$\delta(m) :=\{m+p,m+2p,\dots, m+p(p-1), m+p^2\}\quad \pmod{\varphi(p) = p-1}$$ Consider all pairs ( which pairs? what does it precisely mean in this context? ) of equation $x^{m_1}\equiv a\pmod{p}$ where $m_1\in \delta(x)$ . Clearly, all such $x$ work ( Why does it ""clearly"" work? ) and once we have obtained all pairs $x_1,x_2,\dots,x_{w(n)}$ ( what is $w(n)$ ? ), we use Hensel's Lemma to lift these solutions to $p^{v_p(n)}$ ( How do we precisely apply Hensel Lemma here? ) and finish by Chinese Remainder Theorem. $\textbf{End of Proof 1}$ As you can see, there are many point where I couldn't catch what the author wanted to say. It's been a few days and I still don't understand. If someone could help me, I would really appreciate it. Thanks.","['modular-arithmetic', 'number-theory', 'proof-explanation', 'group-theory', 'prime-numbers']"
4405526,Finding the exact value of cot(23π/4) using Unit circle,"I need to find the exact value of $\cot (23π/4)$ . I am thinking that I can find $\tan(23π/4)$ first and then reciprocate it to get $\cot(23π/4)$ I am having trouble trying to represent this on the unit circle. Is it going to be $\tan (23π/4) = \tan(4π + (7π/4))$ . And I think that the zeroes(tan) are $\{nπ: n $ is an integer $\}$ so $6π$ would be a zero, so from the unit circle, I essentially start like this?",['trigonometry']
4405570,Why $g^{-1}$ in the definition of associated vector bundles?,"In the definition of the associated vector bundle $E$ to a principal bundle $\pi:P\rightarrow M$ , the equivalence relation is $$(p,v)\sim(pg,g^{-1}v)$$ where $p\in P$ , $v\in V$ , $g\in G$ , Lie group $G$ acts on vector space $V$ . I don't understand the reason/motivation for $g^{-1}$ . Why would this cause any problem: $$(p, v)\sim(pg, v)$$ Part of the reason for asking: Say the principal bundle is a frame bundle associated to the vector bundle. Then $g$ is just a basis transformation, which ""shouldn't"" change a tensorial object $v$ . (Meaning it changes the basis of the vector space and the coordinates of $v$ but not $v$ itself, i.e. $v = v^\mu\partial_\nu = v^{\mu'}\partial_{\mu'}$ where $g \partial_\mu = \partial_{\mu'}$ ).","['principal-bundles', 'vector-bundles', 'differential-geometry']"
4405618,Construct a function $f$ with $f'-af$ is odd (a>0),"Let $a>0$ , I am trying to construct a function $f$ such that $f'-af$ is odd. i.e \begin{align*}
f'(-x)-af(-x)=-f'(x)+af(x)
\end{align*} By direct computation, we have \begin{align*}
\frac{d}{dx}(f(x)-f(-x))+a(f(x)-f(-x))=0
\end{align*} Solving the ODE, I pick $f(x)-f(-x)=-e^{ax}$ . i.e \begin{align*}
f(x)=-e^{ax}+f(-x)
\end{align*} But I got stuck here. Any help would be appreciated.","['even-and-odd-functions', 'ordinary-differential-equations']"
4405620,Rudin's PMA 10.38 theorem,"Suppose $E$ is  a  convex open set in $\Bbb R^{n}$ , $f \in C^{1}$ , $p$ is an integer, $1\leq p  \leq n$ and $(D_{j}f)(x)=0$ ( $p<j \leq n ,x\in E$ ). Then there exists an $F \in C^{1}$ such that $$ (D_{p}F)(x)=f(x), (D_{j}F)(x)=0$$ if $p<j\leq n ,x \in E$ . Proof : Write $x=(x',x_{p},x'')$ , where $x'=(x_{1},\dots,x_{p-1})$ , $x''=(x_{p+1},\dots ,x_{n})$ . When $p=1$ , $x'$ is absent and when $p=n$ , $x''$ is absent. Let $V$ be the set of all ( $x',x_{p}$ ) such that $(x',x_{p},x'') \in E$ for some $x''$ . Being a projection of $E$ , $V$ is convex open set in $\Bbb R^{p}$ . Since $E$ is convex and $(D_{j}f)(x)=0$ for $p<j\leq n$ , $f(x)$ does not depend on $x''$ . Hence there's a function $\phi$ with domain $V$ s.t $f(x)=\phi(x',x_{p})$ , for all $x\in E$ . If $p$ =1, $V$ is a segment in $\Bbb R^{1}$ . Pick $c \in V$ and define $$F(x)=\int_{c}^{x_{1}} \phi (t) dt$$ ( mark this integral by ( $\star$ )). If $p$ >1, let $U$ be the set of all $x'\in \Bbb R^{p-1}$ such that $(x,x_{p}) \in V$ for some $x_{p}$ . Then $U$ is a convex open set in $\Bbb R^{p-1}$ and there' a function $\alpha \in C^{1}$ such that $(x,\alpha (x')) \in V$ for all $x' \in U$ . Define $$F(x)=\int_{\alpha (x')}^{x_{p}} \phi (x',t) dt$$ (mark this integral by ( $\oplus$ )). I don't understand why is it necessary for $E$ to be convex. I also don't understand how do ( $\star$ ) and ( $\oplus$ ) guarantees that $$ (D_{p}F)(x)=f(x), (D_{j}F)(x)=0$$ , if $p<j\leq n ,x \in E$ Would be satisfied. Any help would be appreciated.","['integration', 'ordinary-differential-equations', 'real-analysis', 'multivariable-calculus', 'calculus']"
4405770,Uniqueness of the evaluation isomorphism as a natural isomorphism,"It is well known that the evaluation map between $V$ , a topological vector space, and $V^{\star\star}$ , the double dual, given by $\epsilon_{v} = f\mapsto f(v)$ is a natural isomorphism between the identity functor and the functor $(-)^{\star\star}$ in the category of real finite dimensional vector spaces or in the category of reflexive Banach spaces. Someone once, perhaps mistakenly, told me that a topological vector space is reflexive it is naturally isomorphic to its double dual. The definition that I recall is that such a space is reflexive if the evaluation map is an isomorphism. I want to know if the previous definition is definitely wrong in a certain sense: Is it possible that for some sub-category of topological vector spaces, there is a natural isomorphism between the identity functor and the double dual functor that does not correspond to the evaluation map?","['functional-analysis', 'category-theory']"
4405771,"Why is 2-descent called ""descent""?","My understanding is 2-descent today means calculating $E(\mathbb Q)/2E(\mathbb Q)$ by computing the Selmer group and trying to figure out which curves of the Selmer group actually have a K-point. (See Silverman's Arithmetic of Elliptic curves Chapter X Prop 1.4 or Prop 4.9) I have heard that the name arose from Fermat's original proof by descent that there are no non-trivial (i.e. completely non-zero) solutions to the diophantine equation $x^4-y^4=z^2$ . See here . As shown in the link, this equation can be interpreted as the elliptic curve $E$ given by $y^2 = x^3 + 4x$ , the ""trivial"" points becoming the points $X = \{\mathcal O, (0,0), (2,4), (2,-4)\}$ . Then Fermat is proving there are no points besides $X$ on $E(\mathbb Q)$ . Specifically, he proves any $P \in E(\mathbb Q) \backslash X$ satisfies $P = [2]Q$ for some $Q \in E(\mathbb Q)\backslash X$ and that the height of Q will be strictly smaller than the height of P. An elliptic curve has only finitely many points of bounded height, and so repeating this process leads to a contradiction. I had thought the generation of smaller and smaller points was the ""descent"" part of Fermat's theorem. However, modern day 2-descent doesn't seem to be a generalization of that part of the proof? It doesn't involve a height, or shrinking points. It seems more analogous to the first step of the proof where Fermat figured out what X was. In the modern picture, descent seems to better describe the second step of the Mordell-Weil theorem where representatives of $E(\mathbb Q)/2 E(\mathbb Q)$ and the height function are used to find generators of $E(\mathbb Q)$ . Is this a bit of a misnomer/evolution of the word, or is there some other reason why computing $E/2E$ is being called descent?","['elliptic-curves', 'number-theory', 'curves', 'algebraic-geometry', 'terminology']"
4405821,Example of two non isomorphic universal covers.,"I know that if a connected and locally path-connected topological space $X$ admits an universal cover, than it's unique up to isomorphism (""isomorphism"" is meant in the category of (connected) covering spaces of $X$ ). Nevertheless,I didn't manage to find in the literature any example of a connected (and not locally path-connected) space which has two non-isomorphic universal covers. Could you help me? Important!: My definition of universal covering is NOT a covering map whom domain is simply connected! In fact, I'm studying the topic from E.H.Spanier, Algebraic Topology which gives a broader definition at page 79.","['general-topology', 'examples-counterexamples', 'algebraic-topology', 'covering-spaces']"
4405839,Probability question about conditional probability,"Let us suppose that we have a sample of 2 random distinct numbers $I=\{z_1,z_2\}$ that are generated from a uniform distribution with support in $[0,1]$ . Let's call $d = \max{I}$ the maximum of the randomly generated sample. I want to compute the probability that $z_1\leq r$ for some $0\leq r\leq1$ ( $r$ is not a random variable) given that $z_1 \neq d$ , i.e. $$
p(z_1 \leq r | z_1 \neq d)
$$ To easily compute this probability, we can notice that if $z_1\neq d$ , then $z_1$ is the minimum, and therefore $$
p(z_1 \leq r | z_1 \neq d) = p(\min I\leq r) = 1-p(z\geq r)^2 = 1 - (1-r)^2
$$ I have confirmed this result numerically on Mathematica that you can check with the following code checkDistribution[r_] := 
 Module[{win = 0, loss = 0, list, max, i, n = 2}, 
  For[i = 1, i <= 10000, i++,
   list = RandomSample[Range[100 n], n]/(100 n) // N;
   max = Max[list];
   If[list[[1]] != max, 
    If[list[[1]]^(n - 1) <= r, win = win + 1, loss = loss + 1];];
   ];
  Return[{r, win/(win + loss)} // N]]
points = Table[checkDistribution[r][[{1, 2}]], {r, 0, 1, 0.01}];
Show[points // ListPlot, Plot[2 r - r^2, {r, 0, 1}, PlotStyle -> Red]] that returns I want however to compute this probability without using the fact that $z_1$ is the minimum, but only using our knowledge that $d$ is the maximum.
We should then consider the distribution of the maximum of two random variables. Since $d$ is the maximum, we have $$
p(d\leq r) = r^2\\
p(d>r) = 1-r^2
$$ Now, we have two cases $d>r$ , in which case $p(z_1 \leq r | z_1 \neq d) = p(d>r) p(z_1<r) = (1-r)\times r$ $d\leq r$ , in which case $p(z_1 \leq r | z_1 \neq d) = p(d<r) p(z_1<d) = r^2 \times 1$ but the sum of these two terms does not give the answer. Where is the mistake? I want to solve this problem in the other way, because I want to generalize it to a set of 3 numbers $I=\{z_1,z_2,z_3\}$ and compute $$
p(z_1 \leq r | z_1 \neq d)\,.
$$ In this generalization, I don't know if $z_1$ is the minimum of the distribution.","['conditional-probability', 'probability-theory', 'probability']"
4405842,Find the solutions of : $(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0$,"Find the solutions of : $(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0$ I have to find infinite series form solutions, $y=\sum a_nx^{n+r}.$ I got the indicial equation : $r^2-1=0 \implies r_1=-1, r_2=1$ I got the general formula while $r=1$ is $a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)}a_k$ Then , one solution is $y_2(x)=x^1\sum a_kx^k$ when I get $a_k$ using $a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)} , k>0$ . How can I found the other solution when $r=-1$ ? Thanks !",['ordinary-differential-equations']
4405864,Measure of Compact Set with Empty Interior,"For my Integration course I've been proposed the following problem with which I have been struggling: Prove that there exists a compact set $K \subset \mathbb{R}^n$ with empty interior and measure $0 \leq \alpha < +\infty$ . My approach: My first idea was to use an appropiate real continuous non-negative function such that $f(x_0)=0, \lim_{x \to \infty} f(x) = +\infty$ and use the intermediate value theorem. Following this reasoning, let $A \subset \mathbb{R}^n$ Lebesgue measurable which will be explicitly constructed later. Define, $f:\mathbb{R}_{\geq 0} \rightarrow \mathbb{R}, f(x) = m(A \cap [-\underline{x},\underline{x}])$ where $\underline{x} = (x,\ldots,x)^t$ is a $\mathbb{R}^n$ vector. I claim that $f$ a continuous function such that $f(0)=0, \lim_{x \to \infty} f(x) = m(A)$ . Proof: Given $x_0 \in \mathbb{R}, \varepsilon > 0$ . Choose $\delta = (\frac{\varepsilon}{2})^{1/n}$ , if $x \in \mathbb{R}_{\geq 0} : |x-x_0| < \delta$ , we may suppose that $x < x_0$ (the other case is completely analogous), then $$|f(x)-f(x_0)| = |m(A \cap [-\underline{x},\underline{x}])-m(A \cap [-\underline{x_0},\underline{x_0}])| = $$ $$ = m(A \cap ([-\underline{x_0},-\underline{x}) \cup (\underline{x},\underline{x_0}])) = 2 m(A \cap (\underline{x},\underline{x_0}]) = 2 |x-x_0|^n < 2 \delta^n < \varepsilon$$ $f(0) = 0$ is trivial. Finally, in order to compute $\lim_{x \to \infty}$ , note that the function is monotonous increasing hence the limit exists (either finite or infinite limit). Let $x_n$ be a sequence such that $x_n \to +\infty$ . Therefore, there exists a monotonous increasing subsequence $\{x_{n_k}\}$ . Therefore, taking the limit to the sequence of $m(A \cap [-\underline{x_{n_k}},\underline{x_{n_k}}])$ is the measure  of increasing sets, hence is the measure of the union of the sets, that is, $\lim_{x \to \infty} f(x) = m(\bigcup_{k \in \mathbb{N}} A \cap [-\underline{x_{n_k}},\underline{x_{n_k}}]) = m(A)$ Note that, in particular, if $A$ is a closed set with empty interior, then the set $A \cap [-x,x]$ would be compact and with empty interior. Therefore, it will be enough to find a convenient set $A$ such that (i) is closed, (ii) has empty interior (iii) has infinite measure in order to complete the exercise. In doing so, let $C$ be the fat Cantor Set of measure $\frac{1}{2}$ in $\mathbb{R}$ , which is a closed set with empty interior. Now, let $C' = \bigcup_{n=0}^{\infty} (C+n)$ hence $m(C')=+\infty$ . Finally, define $A = \mathbb{R} \times \cdots \times \mathbb{R} \times C'$ . Then, $A$ is closed as it is product of closed sets, has empty interior and has infinite measure. Therefore, such a set has been found verifying (i),(ii),(iii). Proof of C' being closed: Let $\{x_n\} \subset C'$ be an arbitrary sequence that converges, $x_n \to x$ . Then, $x_n$ must be bounded, that is, there exist a finite number $\{n_1,\ldots,n_k\}$ such that $\{x_n\} \subset \bigcup_{i=1}^k (C+n_i)$ which is a closed set, hence the limit $x \in \bigcup_{i=1}^k (C+n_i) \subset C'$ , that is, $C'$ is closed as desired. Another idea for constructing A: Let $\{x_n\}$ be an enumeration of $\mathbb{Q}^n$ , let $A' = \bigcup_{n \in \mathbb{N}} B(x_n:1/2^n)$ . Then, $A'$ is open and has finite measure. Therefore, let $A=(A')^c$ . Then $A$ is closed, has infinite measure and empty interior (as it contains no rational points). I would like to know whether if my reasoning is correct and if there are any other alternative (simpler) ways to handle the problem. Another idea I also had was to construct a set as the union of several fat Cantor Sets which add up to the given $\alpha$ , which will be called $A'$ . Then, define $K=A' \times [0,1] \times \cdots \times [0,1]$ and I would like to use that $m(K)=m(A')$ though I'm not sure about this fact (we haven't studied how measures behave under cartesian product).","['integration', 'measure-theory', 'solution-verification', 'lebesgue-measure']"
4405889,A question from the martingale,"Suppose $X_1,\ X_2,\ldots$ are i.i.d. symmetric random variables on $\{\pm 1\}$ , i.e. $\mathbb{P}(X_k = +1) =1/2$ . Define $S_n = X_1+X_2+\ldots+X_n$ . Use martingale techniques to compute $\mathbb{P}(S_n\text{ hits }-3 \text{ before hitting }+7)$ ; i.e. compute $\mathbb{P}( \exists n \geq 1 \text{ such that } S_n=-3 \text{ and }-2\leq S_k\leq+6\ \ \forall k=1,...,n-1)$ . I tried to solve this by the following way: Define $T=\inf \{n : n \geq 1 \text{ and }S_n=-3\text{ or }+7\}$ , where $\inf(\emptyset) = \infty$ . Now I will show that $T$ is an extended stopping time with respect to $\{\sigma(X_1),\sigma(X_1,X_2), \ldots\}$ and that $S_1,S_2,..$ is a martingale with respect to those $\sigma$ -fields. But I have no idea how to use the martingale convergence theorem to solve this question. Can anyone help me with the solution to this question?","['martingales', 'probability-theory', 'probability']"
4405891,A function satisfying a condition is a polynomial of degree $\leq 1$,"Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function s.t $f(x)=\frac12(f(x+r)+f(x-r))$ for every $r>0, x\in\mathbb{R}$ . Prove that $f$ is a polynomial of degree $\leq 1$ . This is a question given to me in a course about complex analysis. I was able to prove this problem without using tools of complex analysis (I looked at $g(x)=f(x)-f(0)$ and showed that $g(x)=g(1)x$ ), but I'm curious if there's a way to use complex analysis to simplify the solution. My idea was to look at a function $g:\mathbb{C}\to\mathbb{C}$ satisfying $g(z)=\frac12(g(z+r)+g(z-r))$ for every $z\in\mathbb{C}$ and $r>0$ and then showing that $|g(z)|\leq A+B|z|$ for some $A,\ B\in\mathbb{R}$ and then using Liouville's theorem, but I couldn't make any progress. Any hint would be appreciated.","['analysis', 'real-analysis', 'complex-analysis', 'continuity', 'functions']"
4405901,Why does my topology textbook (Munkres) define positive integers as the intersection of all inductive subsets of the reals?,"This is how the topology textbook I'm reading (Munkres) defines integers: A subset of the real numbers is ""inductive"" if it contains 1 and $1+x$ for all $x$ in the subset. The intersection of all inductive subsets of the reals is the set of positive integers. Why take this route involving the intersection of so many sets? I could define the positive integers given reals as $1$ along with any sum of positive integers and get the same set much more easily.","['foundations', 'natural-numbers', 'real-analysis']"
4405939,Prove the given matrix is invertible,"Question. Let $A$ be an $n\times n$ matrix such that each row and each column of $A$ contains all of the entries $1,2,2^2,...,2^{n-1}$ in some order. Prove that $A$ is invertible. An easy fact is that $(2^n-1,e)$ is an eigenpair of $A$ where $e\in\mathbb{R}^n$ is a vector of all-one entries. However, this is not sufficient for proving $A$ is invertible. One possible route is to prove $0$ is not an eigenvalue of $A$ . I am also thinking about Gershgorin discs. The deleted absolute row sum on the $k$ -th row is $2^n-1-a_{kk}$ . Thus the Gershgorin disc is $$D_k:=\{z\in\mathbb{C}\mid|z-a_{kk}|\le2^n-1-a_{kk}\}.$$ If $0\notin D_k$ , then $$a_{kk}=|0-a_{kk}|>2^n-1-a_{kk}\implies a_{kk}>2^{n-1}-\frac12.$$ In other words, only if $a_{kk}=2^{n-1}$ will $0\notin D_k$ happen. For the structure of $A$ , I am thinking about a sudoku-like array, so it is not guaranteed that the diagonal entries of $A$ are all $2^{n-1}$ . For instance, $$\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 \\ 4 & 1 & 2 \\ 2 & 4 & 1 \end{bmatrix},\quad\begin{bmatrix} 1 & 2 & 4 & 8 \\ 2 & 4 & 8 & 1 \\ 4 & 8 & 1 & 2 \\ 8 &1 &2 & 4 \end{bmatrix}.$$ In particular, permutation similarity does not change the diagonal entries. Currently I am stuck at this point. Any clever ideas or suggestions are highly welcomed. Update. Thanks for all the comments and solutions suggested below! Jimmy’s hint is almost detailed and close to the arguments for proving the Gershgorin disc theorem. I am also conjecturing if there is no dominating terms among a given sequence (here $2^{n-1}$ is dominating as the sum of all other terms is $2^{n-1}-1$ ), then such matrix $A$ might be singular. Micheal’s smart comment exploits the special structure here. Let me write down the details for future readers. By taking all the entries modulo 2, we get a permutation matrix, whose determinant is $\pm 1$ . Thus the determinant of $A$ is odd and cannot be zero. Update 2. My conjecture is true. The following matrix is singular : $$\begin{bmatrix} 1 & 2 & 3 & 4 \\ 2 & 1 & 4 & 3 \\ 3 & 4 & 1 & 2 \\ 4 & 3 & 2 & 1 \end{bmatrix}.$$ I am beyond happy that I could communicate with your guys about this question!","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4405946,Prerequisites for Bourbaki spectral theory (French),"I am considering reading Bourbaki's text Théories spectrales (Springer, French, 2nd Ed.) and was wondering what the prerequisites are. Currently I am familiar with their texts on the Theory of Sets, General Topology (Chaps 1-4 and 5-10), Topological Vector Spaces, Algebra I (Chaps 1-3), and Integration (Chaps 1-6 and 7-9). Do I also need familiarity with their text on Commutative Algebra ? Thanks.","['spectral-theory', 'functional-analysis']"
4406963,Random Walk 'Snake Problem',"Introduction A snake is traveling around the infinite Cartesian grid. It starts at (0,0) and takes the first step to the left to (-1,0). It continues spiralling counterclockwise, so it visits (-1,-1) next. Then, as the snake arrives at (0,-1), it cannot continue to (0,0) since this place has already been visitted. In such situation the snake chooses a random direction and continues its journey. Question How many steps will the snake take on average before biting its own body? Simulation I wrote some python code to check the answer numerically and is seems to be about 56.261616. Is there a way to estimate the answer mathematically? The histogram after 10mln runs: An example journey: import numpy as np
import matplotlib.pyplot as plt
import random as rd
start=[0,0]
right=[1,0]
up=[0,1]
left=[-1,0]
down=[0,-1]

directions={0:right, 1:up, 2:left, 3:down}
facing=0
hist_data=[]
for jj in range(10000):
    pos=start
    visits=[start]
    step_num=0
    while True:
        new_pos=[sum(x) for x in zip(pos,directions[facing])]
        if new_pos not in visits:
            pos=new_pos
            visits.append(pos)
            step_num+=1
            facing=(facing+1)%4
        else: 
            facing=(facing+rd.randint(1,3))%4
            new_pos_right=[sum(x) for x in zip(pos,directions[0])]
            new_pos_up=[sum(x) for x in zip(pos,directions[1])]
            new_pos_left=[sum(x) for x in zip(pos,directions[2])]
            new_pos_down=[sum(x) for x in zip(pos,directions[3])]
            if (new_pos_right in visits) and (new_pos_up in visits) and (new_pos_left in visits) and (new_pos_down in visits):
                hist_data.append(step_num)
                break

plt.figure()
for ii in range(len(visits)-1):
    plt.plot([visits[ii+1][0],visits[ii][0]],[visits[ii+1][1],visits[ii][1]],'b-')

plt.plot(0,0,'g.',markersize=25)
plt.plot(visits[-1][0],visits[-1][1],'r.',markersize=25)
plt.title('Snake got stuck after '+str(step_num)+' steps')

plt.figure()
w=1
plt.hist(hist_data,bins=np.arange(min(hist_data), max(hist_data) + w, w))
print(np.average(hist_data))
plt.show()","['statistics', 'random-walk', 'recreational-mathematics']"
4406984,"Is it easier to pass a test with more questions and more mistakes allowed, or less questions but less mistakes allowed?","$Q$ is a set of yes or no questions. I know the answer to $q_{know}$ of these questions ( $0\leq q_{know}\leq|Q|)$ , but I have to guess for the remaining ones, with a $0.5$ probability of guessing correctly. A test $T$ is generated by randomly selecting $n$ of these questions ( $|T|=n$ ). To pass the test, I can make $k$ mistakes at the most, with $k>0$ . What is the probability of passing the test, as a function of these parameters? Assuming $n/k%$ is constant (e.g., for every ten questions in the test, 1 mistake is allowed) is it better to take the test with more or less questions? This question came up during a discussion and I thought that the number of questions doesn't matter because the probability is always the same, but I'm starting to think that it might be more complicated than this. Should it be a product of binomial distributions?","['probability-distributions', 'binomial-distribution', 'probability']"
4407038,How is it justified in euclidean geometry to take an arbitrary point?,"Is it justified using Euclides' axioms to take an arbitrary point? I mean, it is done for example in this proposition , but how it is justified inside the axiomatic? I mean, the only points we can get are those which result from the intersection of lines and circles, so how can we get that point?","['euclidean-geometry', 'geometry']"
4407059,"Can any unitary matrix be written as a product of ""2D"" unitary matrices?","Given a $n\times n$ unitary matrix $\mathbf{U}$ , can this be rewritten as a product $$
\mathbf{U}=\prod_{i=1}^n\mathbf{U}_i
$$ where $\mathbf{U}_i$ are unitary matrices themselves, but they only really `affect' two dimensions? For $n=3$ , they would have this shape (in analogy to the rotation matrices): $$
\mathbf{U}_1=
\pmatrix{
a_{11}&a_{12}&0\\
a_{21}&a_{22}&0\\
0&0&1},
\quad
\mathbf{U}_2=
\pmatrix{
b_{11}&0&b_{12}\\
0&1&0\\
b_{21}&0&b_{22}},
\mathbf{U}_3=
\pmatrix{
1&0&0\\
0&c_{11}&c_{12}\\
0&c_{21}&c_{22}}
$$ With the 2x2 unitary matrices $\mathbf{A}$ , $\mathbf{B}$ , and $\mathbf{C}$ . (Maybe, the ones have to be replaced with a phase factor $e^{i\varphi}$ to have enough flexibility?) The point is, that I have a property which is preserved under multiplication with such a ""2D"" unitary matrix. And I suspect that it generalizes to any unitary matrix (ideally even a unitary transformation in $L^2$ ), but I am not sure.","['matrices', 'unitary-matrices', 'matrix-decomposition']"
4407098,Is there a name for this set of fractions that combine to make other fractions?,"This is a little clunky so bear with me. I've been looking to buy a dowelling jig which, if you don't know, is a woodworking tool that lets you drill dowel holes into a piece of wood. Some jigs will drill a hole right into the middle of the edge of wood. But some come with spacers that let you offset that hole. For example, the dowelmax has spacers that are these thicknesses (in inches): 1/8, 3/8, 6/8, 13/8. If you need an offset that isn't one of these numbers, then you can combine them. For example, if I want an offset that is 1/2, I can add the 1/8 to the 3/8. But I can also use subtraction: if I need an offset that is 5/8, I can put the 6/8 on one side and the 1/8 on the other. This is like 6/8-1/8=5/8. Pretty clever. I worked out all the combinations I could do in a spreadsheet and these are the results: +--------+------------------------+
| needed |         combo          |
+--------+------------------------+
| 1/8    | 1/8                    |
| 2/8    | 3/8 - 1/8              |
| 3/8    | 3/8                    |
| 4/8    | 1/8 + 3/8              |
| 5/8    | 6/8 - 1/8              |
| 6/8    | 6/8                    |
| 7/8    | 6/8 + 1/8              |
| 8/8    | 13/8 + 1/8 - 6/8       |
| 9/8    | 6/8 + 3/8              |
| 10/8   | 1/8 + 3/8 + 6/8        |
| 11/8   | 13/8 - 3/8 + 1/8       |
| 12/8   | 13/8 - 1/8             |
| 13/8   | 13/8                   |
| 14/8   | 13/8 + 1/8             |
| 15/8   | 13/8 + 3/8 - 1/8       |
| 16/8   | 13/8 + 3/8             |
| 17/8   | 13/8 + 3/8 + 1/8       |
| 18/8   | 13/8 + 6/8 - 1/8       |
| 19/8   | 13/8 + 6/8             |
| 20/8   | 13/8 + 6/8 + 1/8       |
| 21/8   | 13/8 + 6/8 + 3/8 - 1/8 |
| 22/8   | 13/8 + 6/8 + 3/8       |
| 23/8   | 13/8 + 6/8 + 3/8 + 1/8 |
+--------+------------------------+ My main question is: Is there a name for this sort of thing? Also, Are there other choices of numbers other than 1, 3, 6, 13, that would do this? The only thing I thought of that was perhaps related was denominations of coins. For that, in the UK there's 1, 2, 5, 10, 20, 50, 100, 200. The US uses 1, 5, 10, 25, 50, 100. These can be combined but you'd need large multiples of each since you can't subtract. (I suppose the goal is to have few spacers, simple combinations, and a full span of combinations from 1/8 to the sum of all spacers. Is 1, 3, 6, 13 the best way?) I did some scribbling to find other combinations, and I found that if you had just 1/8, 3/8, and 9/8, you can already make combinations from 1/8 to 13/8.","['arithmetic', 'combinatorics', 'terminology']"
4407107,How to tackle the integral $\int_{0}^{1} \sqrt{-1+\sqrt{\frac{4}{x}-3}} d x$?,"$ \text {Let } y=\sqrt{-1+\sqrt{\frac{4}{x}-3}}\textrm{ then ,}$ $ \displaystyle \begin{aligned}I&=16 \int_{0}^{\infty} \frac{y^{2}\left(y^{2}+1\right) d y}{\left(y^{4}+2 y^{2}+4\right)^{2}}\\&=4\left[3 \underbrace{\int_{0}^{\infty} \frac{y^{2}\left(y^{2}+2\right)}{\left(y^{4}+2 y^{2}+4\right)^{2}} d y}_{J}+\underbrace{\int_{0}^{\infty} \frac{y^{2}\left(y^{2}-2\right)}{\left(y^{4}+2 y^{2}+4\right)^{2}}}_{K} d y\right] \end{aligned}\tag*{} $ Now let’s play a little trick on the integral $ J$ . $\displaystyle \begin{aligned}J &=\int_{0}^{\infty} \frac{1+\frac{2}{y^{2}}}{\left(y^{2}+\frac{4}{y^{2}}+2\right)^{2}} d y \\&=\int_{0}^{\infty} \frac{d\left(y-\frac{2}{y}\right)}{\left[\left(y-\frac{2}{y}\right)^{2}+6\right]^{2}} \\&=\int_{-\infty}^{\infty} \frac{d u}{\left(u^{2}+6\right)^{2}}\\ &\stackrel{u=\sqrt6 \tan \theta}{=}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \frac{\sqrt{6} \sec ^{2} \theta d \theta}{\left(6 \sec ^{2} \theta\right)^{2}}\\&=\frac{\pi}{12 \sqrt{6}} \end{aligned} \tag*{} $ For the integral $ K$ , we first split the interval into two. $ \displaystyle \begin{aligned}K &=\int_{0}^{\infty} \frac{1-\frac{2}{y^{2}}}{\left(y^{2}+\frac{4}{y^{2}}+2\right)^{2}} d y \\&=\int_{0}^{1} \frac{1-\frac{1}{y^{2}}}{\left(y^{2}+\frac{4}{y^{2}}+2\right)^{2}} d y+\int_{1}^{\infty} \frac{1-\frac{2}{y^{2}}}{\left(y^{2}+\frac{4}{y}+2\right)^{2}} d y \\&=\int_{0}^{1} \frac{d\left(y+\frac{2}{y}\right)}{\left[\left(y+\frac{2}{y}\right)^{2}-2\right]^{2}}+\int_{3}^{\infty} \frac{d\left(y+\frac{2}{y}\right)}{\left[\left(y+\frac{2}{y}\right)^{2}-2\right]^{2}} d y \\&=\int_{\infty}^{3} \frac{d u}{\left(u^{2}-2\right)^{2}}+\int_{3}^{\infty} \frac{d v}{\left(v^{2}-2\right)^{2}} \\&=0 \end{aligned} \tag*{} $ Now we can conclude that $\displaystyle \boxed{I=4\left(3 \cdot \frac{\pi}{12 \sqrt{6}}\right)=\frac{\pi}{\sqrt{6}}}\tag*{} $ My Question Is there any other substitution or method to tackle the integral?","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4407138,Infinite product of ideals,"If $R\in \mathbf{CRing}$ , and $(I_n)$ is a finite family of ideals, we can define the product of this family to be the set of finite sums of products $x_1 \cdots x_n$ where each $x_j \in I_j$ . This notion obviously makes no sense for infinite families since we don't have a notion of infinite product. So it seems natural to try generalizing it for topological (commutative) rings. Now if $R\in \mathbf{CRing(Top)}$ and $(I_n)_{n\in \mathbb N}$ , we could define the product of that family as the set of finite sums of limits of form $\prod_{n\in \mathbb N} x_n$ where each $x_n \in I_n$ (ignoring non-convergent ones obviously). It is easy to see this is an ideal. Has this notion been used? What are some references that use this? (It can also probably be more generalized using directed sets but the writter and probably many readers are not as experienced with those as with limits of sequences.)","['topological-rings', 'reference-request', 'abstract-algebra', 'ideals', 'general-topology']"
4407190,Equivalent definitions of a measurable set in a manifold,"There seem to be at least two ways of defining measurability in a smooth $n$ -manifold $M$ , and I want to know if these definitions are equivalent. (1) For the first one, we say $A\subseteq M$ is measurable if for each coordinate chart $(U,\varphi)$ , $\varphi(A\cap U)$ is a Lebesgue measurable set in $\mathbb{R}^n$ . The second one is a little complicated and goes as follows. (2) $A\subseteq M$ is said to be measurable if it can be written as a countable union of sets $A_i$ with each $A_i$ contained in some coordinate chart $(U_i,\varphi_i)$ and thus mapped onto a Lebesgue measurable set $\varphi_i(A_i)$ . Now I'd like to determine whether or not (1) $\Leftrightarrow$ (2). To prove the first definition implies the second one, I want to exploit the notion of compactness, because every open cover of a compact set contains a finite subcover. That might be helpful if we want to break $A$ into countably many pieces. However, I don't have too much information about $A$ . Maybe it is not compact at all. Then what should I do? Thank you.","['measure-theory', 'differential-geometry', 'real-analysis']"
4407192,Functional equation in $\displaystyle \mathbb{Z} \to \mathbb{Z}$: $2014f(f(x)) + 2013f(x) = x$,"Solve in $\mathbb{Z} \to \mathbb{Z}$ the following functional equation: $$2014f(f(x)) + 2013f(x) = x$$ Modulo $2014$ , we have that: $$f(x) \equiv -x \pmod  {2014}$$ Modulo $2013$ we have that: $$f(f(x))  \equiv x \pmod {2013}$$ By easy computations, we may prove that: $$f(x) \equiv -x \ \pmod {2013}$$ So, $f \equiv -\text{Id}$ on $\{1, 2, \cdots, 2013 \cdot 2014\}$ . However, I am unable to extend on $\mathbb{Z}$ . (Another easy observation is that $f$ is injective)","['elementary-number-theory', 'algebra-precalculus', 'functional-equations', 'modular-arithmetic']"
4407232,Finding second derivative of $x^{x^x}$,"If $f(x) = x^{x^x} $ , then find $f''(1)$ . My attempt: Now, $\begin{align}y =x^{x^x} \\&\implies ln(y) = x^x \cdot ln(x)
 \\&\implies ln(ln(y))= x\cdot ln(x)+ln(ln(x))\\&\implies\frac{y'}{y\cdot ln(y)}
=ln(x) +1+\frac{1}{x\cdot ln(x)}\\&\implies y'=(x^{x^x})\cdot (x^xln(x))\cdot [ln(x) +1+\frac{1}{xln(x)}]
\end{align}$ I find it really tedious to continue in this way to reach desired answer. I have already checked finding second derivative of $x^x$ . So, I wonder if I have to continue in the similar manner to get to answer or there is any better method to calculate this?","['calculus', 'derivatives', 'algebra-precalculus']"
4407245,A simple reasoning in the geometric problem,"I was asked an interesting question by a friend: You are given half a circle and a line $d$ of length $6$ is given. You
build a perpendicular to the base line (the diameter of the
half-circle) and complete a rectangle, such, that the top side is
tangent to the circle (see an animation below). What is the area $A$ of
the resultant rectangle. So, I have a very quick question, if I have all the data needed to calculate the area $A$ ? And, since I've been asked by another human, I assume that the answer is yes . So I conclude, that the while the area is a function $f(r, d)$ , where $r$ is the radius of a circle, it should be that $r = r(d)$ , and in fact it follows that $A = f(d)$ . Then I pick a convenient choice of a circle and calculate the area in my head. Basically, setting $d=2r$ is one such particularly easy choice, and $$A = \frac{d^2}{2}.$$ However, I have cheated, and now I need to establish somehow, that changing the circle, have no impact on the area. (I am able to check another choice of setting an angle to $45$ degrees between a diameter and $d$ ). Hence, I need to make calculations (using pen and paper). On the image below, I have added $x$ to be the length of the line to the left of the perpendicular. Then by triangle similarities we have the following: $$ \frac{d}{2r-x} = \frac{2r}{d}$$ or, more compactly, $\frac{d^2}{2} = r\cdot (2r-x)$ . Which is accidentally the area. I, personally, do not like to make heavy calculation in problems, where some simple reasoning should be applied. Therefore, I am looking for a simpler, just-in-head, solution, to why the area of the rectangle on the animation below, is independent of the circle radius?","['recreational-mathematics', 'geometry']"
4407251,marginal probability function for X,"I know that $f(x,\theta) = f(x|\theta)f(\theta)$ . I then think we can combine the terms from the α and β, with $x$ and $1-x$ but not sure how to proceed.
I would need to find the marginal before I can determine $E(X)$ . EDITS: Showing what I have done before: (a) Marginal of X $$f(x,\theta) = f(x|\theta)f(\theta)$$ Since $\pi(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}$ then I did $$f(x,\theta) \propto \theta^x (1-\theta)^{1-x} \theta^{\alpha-1} (1-\theta)^{\beta-1}$$ $$\propto  \theta^{\alpha+x-1} (1-\theta)^{\beta-x} $$ Therefore, $$ f(x) \propto \int_\theta f(x,\theta) $$ $$ \propto \int_0^1 \theta^{\alpha+x-1} (1-\theta)^{\beta-x} $$ $$ \propto \frac {\theta^{\alpha+x} (-1) (1-\theta)^{\beta-x+1}}{\beta-x+1}$$ $$ \propto \frac {1}{\beta-x+1} ,  \  \ \  \ x = 0, 1$$ (b) $E(X)$ by iterated expectation $$E(X) = E[E(X|\theta)]$$ $$  = \int_\theta E(X|\theta) f(\theta) d(\theta)$$ $$  = \int_0^1 \sum_{x=0}^1 x f(x|\theta) f(\theta) d(\theta)$$ $$  = \int_0^1 f(x,\theta)  d(\theta)$$ $$  \propto \int_0^1  \frac {1}{\beta-x+1}  d(\theta) \  \  \  \   \ 
  \   \text{[from (a) above]}$$ $$  \propto   \frac {1}{\beta-x+1} \big[\theta \Big|_0^1 $$ $$  \propto   \frac {1}{\beta-x+1} $$ Does this make sense at all?","['statistics', 'probability-distributions', 'probability']"
4407306,Cardinality of $\mathbb N^{\mathbb N}$ is equal to $\mathbb R$,"Claim 1: $|\mathbb R| = |(x,y)|$ for all real $x < y$ . By bijection $\arctan$ . Claim 2: There is an injection from $\mathbb N^{\mathbb N}$ to $[0,1)$ . Let $f\colon \mathbb N\to\mathbb N$ and $f_y(x)$ be the $y$ -th smallest digit of $f(x)$ where $y\in\mathbb N$ . $f\mapsto \sum_{i=0}\sum_{j=0}^{i}0.1^{(i+1)i/2+j+1}f_j(i)$ is such injection. (Diagnal construction of a decimal by writing a sequence of natural number from least significant digits line by line) Claim 3: $|[0,1)| = |\mathbb R|$ . $|\mathbb R|=|(0,0.5)|\leq|[0,1)|\leq|(0,2)|=|\mathbb R|$ . Claim 4: There is an injection from $(0,1)$ to $\{0,\ldots,9\}^{\mathbb N}$ and hence to $\mathbb N^{\mathbb N}$ . $r\mapsto (x\mapsto \text{integer part of } 10^x\times r)$ is such injection. By Claim 1 to 4 and Cantor–Schröder–Bernstein theorem, $|\mathbb N^{\mathbb N}|=|\mathbb R|$ Does this proof sketch make sense if we define $\mathbb R$ by Dedekind cut? Are those injections injective? How do I deal with multiple representations of the same real number by decimals
which I believe there are at most two?
I believe I used something like the axiom of choice in Claim 4 to pick one such representation such as $0.1=0.0999\ldots$ ? Please pick bugs in the reasoning process above and give references (links) to other
neat proofs of the equation if you have seen or known any. I believe this is a trivial question that appears in many elementary set theory textbooks.","['elementary-set-theory', 'cardinals']"
4407370,Understanding Game Theory Terminology in Probability Based Games,"I have no background and Economics and am trying to teach myself about some basic things in Game Theory. For example, I am trying to understand the following terms: Nash Equilibrium Optimal Strategy Saddle Point To illustrate these concepts, suppose we have the following game (I think the game I have created is called a ""Stackelberg Game""): There are 2 players: Player 1 and Player 2 There are 2 Coins : Coin A and Coin B Coin A has a 0.5 Probability of landing on Heads and a 0.5 Probability of landing on Tails Coin B has a 0.7 Probability of landing on Heads and a 0.3 Probability of landing on Tails If Coin A lands on Heads, a score of +1 is obtained - if Coin A lands on Tails, a score of -1 is obtained. If coin B lands on Heads, a score of -2 is obtained - if Coin A lands on Tails, a score of +3 is obtained. In this game: Player 1 selects a coin and then flips this coin and records his score Next, Player 2 selects a coin and then flips this coin and records his score The player with the highest score wins In this game, Player 2 always has an advantage. He see what coin Player 1 picked and select the more favorable coin based on the choice of Player 1. If Player 1 picked Coin B and got ""unlucky"", Player 2 automatically wins if he picks Coin A If Player 1 picked Coin B and got ""lucky"", Player 2 can only win if he also picks Coin A If Player 1 picked Coin A, regardless of Player 1's result - Player 2 should also pick Coin A if he wants to minimize his chances of loosing My Question: In this game that I created, I am trying to identify the Nash Equilibrium, Optimal Strategy and Saddle Point : I am confused between the concepts of Nash Equilibrium and Optimal Strategies. Based on the analysis I provided above, it seems like the Optimal Strategy in this game is for both players to always select Coin A - Would this be the Nash Equilibrium? I do not understand the concept of a Saddle Point in Game Theory. From Calculus and Optimization, I understand that a Saddle Point is a point on a function in which the first derivatives of the function at that point are 0 but the function does not have a maximum or a minimum of any sort at that point - in Machine Learning, Saddle Points are considered to be obstacles when trying to ""fine tune"" Machine Learning Models. However, I read a bit about Saddle Points in Game Theory, but I don't quite understand how to identify them or why they are important. Does the game I created have a Saddle Point? If so, what does the Saddle Point in this game ""mean"" (e.g. Does the Saddle Point simultaneously identify the ""best case action for Player 1 and the worst case action for Player 2"" )? If this game does not have a Saddle Point - can we ""modify this game"" (e.g. add more coins, e.g. Coin A, Coin B, Coin C, etc.) such that a Saddle Point can exist? Thanks!","['game-theory', 'optimization', 'probability']"
4407378,Is it possible to prove algebraically that these formulas for calculating a rotated point produce the same result?,"The following shows 2 formulas to calculate a point $(x',y')$ , which is $(x,y)$ rotated $\theta$ degrees. $$\begin{bmatrix}x'\\y'\end{bmatrix}=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}h\cos(\theta')\\h\sin(\theta')\end{bmatrix}$$ where $$h=\sqrt{x^2+y^2}\\\theta'=\theta+\operatorname{arctan2}(x, y)$$ where $$\operatorname{arctan2}(x, y) =
\begin{cases}
 \arctan(\frac y x) &\text{if } x > 0 \\
 \arctan(\frac y x) + \pi &\text{if } x < 0 \text{ and } y \ge 0 \\
 \arctan(\frac y x) - \pi &\text{if } x < 0 \text{ and } y < 0 \\
 +\frac{\pi}{2} &\text{if } x = 0 \text{ and } y > 0 \\
 -\frac{\pi}{2} &\text{if } x = 0 \text{ and } y < 0 \\
 0 &\text{if } x = 0 \text{ and } y = 0
\end{cases}$$ The first one is applying a 2D rotation matrix, and the second one is from a computer program I wrote some time ago, which directly handles the process of rotating a point, but computationally much slower. I was wondering why these two produce the same result. Of course, they do the same operation geometrically, but is it possible to prove algebraically that the equality always hold for any real number $x$ , $y$ , and $\theta$ ?","['trigonometry', 'geometry']"
4407391,Zarankiewicz's problem,"I was reading about Zarankiewicz's problem from the book ""Extremal combinatorics"" by Stasys Jukna. The problem sounds very interesting and I was about to understand some basics about it. Here I am attaching an excerpt from the book: Here is my question: So we have two objects: the maximal number of $1$ 's in a $0-1$ matrix of size $n\times n$ such that any $a\times a$ submatrix has at least one $0$ among its entries. Here we assume that $n\geq a\geq 2$ . For concreteness let's denote it by $m_a(n)$ . $k_a(n)=\min \mathfrak{C}$ , where $\mathfrak{C}$ is the set of all $k \in \mathbb N$ such that any bipartite graph with parts of size $n$ and more than $k$ edges contains at least one $a\times a$ clique. Question 1: 1) I guess that $\min \mathfrak{C}$ exists because $n^2-1\in \mathfrak{C}$ . It also shows very trivial estimate $k_a(n)\leq n^2-1$ . Right? Question 2: I was able to show that $m_a(n)\leq k_a(n)$ . Am I right that we cannot claim the converse inequality, i.e. $m_a(n)\geq k_a(n)$ right?","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
4407397,Divisor topology [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question In my topology course we defined the following set $\mathcal{B}=\{D_n:n\in\mathbb{N}_2\}$ , Where for each $n\in\mathbb{N}_2$ we define $D_n:=\{x\in\mathbb{N}_2:x\mid n\}$ and $\mathbb{N}_2=\{2,3,4,5,...\}$ . The topology generated by $\mathcal{B}$ is called the divisor topology on $\mathbb{N}_2$ .
Given $n\in\mathbb{N}_2$ , determine the interior and accumulation points of $\{kn:k\in\mathbb{N}\}$ . I've tried the division algorithm to find a basis element for every point in the set, but it's been useless, any help is appreciated <3","['elementary-number-theory', 'general-topology', 'analysis']"
4407424,Why probability of a diamond followed by an ace on drawing $2$ cards from a shuffled standard deck is not $1/51$?,Two cards are dealt at random from a standard deck of $52$ cards. What is the probability that the first card is a $\diamondsuit$ and the second card is an ace? I thought I got this question right but the system said it was wrong. This is how I did it. So first I know that there are $13$ diamonds in a deck of cards so the probability of getting a $\diamondsuit$ is $13/52$ . And there are $4$ aces so the probability of the second card being an ace is $4/51$ and then you multiply them together to get $1/51$ . But it is wrong can someone help me?,"['combinatorics', 'problem-solving', 'probability']"
4407426,Tools For Analyzing A Recursive Relationship,"I am working with a recursively defined sequence and don't know what tools are out there to understand the limit of the ratio of it's successive terms (which I have a hunch is finite). Here is the sequence: For $a_1=1$ , $b_1=-4$ : $$a_n=-\frac{(2n+3)}{(2n-2)}\sum_{k=1}^{n-1}a_kb_{n-k}$$ $$b_n=-\frac{(2n+2)}{(2n-1)}a_n-\frac{(n+1)}{(2n-1)}\sum_{k=1}^{n-1}b_kb_{n-k}$$ And I would like to know $\lim_{n\to\infty}\frac{a_{n+1}}{a_n}$ or equivalently $\lim_{n\to\infty}\frac{b_{n+1}}{b_n}$ .  Is there a way to get this in an explicit form? Or a way to find bounds?","['ratio', 'combinatorics', 'upper-lower-bounds', 'sequences-and-series', 'limits']"
4407429,On estimating the number of iid samples,"Suppose we have iid samples $X_1,\cdots,X_n$ , with the number of samples $n$ unknown, but I can sample from their sum $m=\sum_{i=1}^n X_i$ . Further suppose $\mathbb{E}[X_i]=\mu$ and $Var[X_t]=\sigma^2$ , with both $\mu$ and $\sigma$ known. If I want to estimate the number of samples $n$ , intuitively, one would find the nearest integer from $\frac{m}{\mu}$ (or is there any better way to estimate $n$ ?) If I want the estimate to be 95% trust-worthy, I guess there should be some requirements on the variance $\sigma^2$ and the true sample number $n$ . My attempt: Suppose $n$ is huge and according to central limit theorem, the distribution of $\frac{m}{n}$ is approximately $\mathcal{N}(\mu,\frac{\sigma^2}{n})$ . But I have a trouble handling the ""rounding function"". And probably central limit theorem is probably not proper for this circumstance, since it says what would happen for $n$ goes to infinity, but what we are trying to do here is exactly estimating $n$ . I tried to use Hoeffding's inequality, but since $n$ is stochastic here, I am not sure Hoeffding's inequality is proper for this circumstance.","['statistics', 'central-limit-theorem', 'probability']"
4407445,Obtaining a tight bound for an Expectation w.r.t a uniform random variable,"Let $x\in [0,1]^{n+1}$ , let $t > 0$ , and let $u$ be a uniform random variable over { $1,\ldots, n, n+1$ }, then I want to tightly bound $$a_t(x) = E_u \left[ \mathrm{exp}\left\lbrace t\left(\frac{1}{n}\sum_{i\neq u}x_i - x_u \right) \right\rbrace \right]$$ with respect to a fixed value of $x$ , or $$\sup_{x\in[0,1]^{n+1}} a_t(x)$$ 1st Attempt : (Hoeffding's Lemma) This attempt bounds the supremum (and hence $a_t(x)$ for any x). If we let $Y = \frac{1}{n}\sum_{i\neq u}x_i - x_u$ , then $E_u[Y] = 0$ and $ -1\leq Y \leq 1$ since $x_i\in [0,1]$ for all $i$ . Then I can use Hoeffding's Lemma to get that: $$a_t \leq e^{t^2\frac{(1 - (-1))^2}{8}} = e^{\frac{t^2}{2}}$$ I was wondering if one could obtain a tighter bound. Possibly a bound that depends on $n$ . I tried to check whether $a_t$ is a convex or concave in $x$ , so that maybe i could find the maximum analytically, but unfortunately it is neither. Update: (Numerics) I tried to find the supremum numerically and compare it to $e^{\frac{t^2}{2}}$ . Since $a_t(x)$ is neither convex or concave, I maximized it with different initial points $x_0$ and averaged the maximum. I did this for each $n\in \left\lbrace 2,4\ldots, 40\right\rbrace$ . I find it surprising that the difference initially increases rapidly but then somewhat plateaus for larger values of $n$ . I tried this for several values of $t$ and it consistently followed this pattern. Thank you! Edit 1: Fixed a typo. Edit 2: Added some numerics.","['uniform-distribution', 'expected-value', 'upper-lower-bounds', 'optimization', 'probability-theory']"
4407501,How do I calculate phi(z) to 4dp when my Normal Distribution tables are to 2dp?,"I have two Normal Distributions representing weights: X ~ N(30, 25) and Y ~ N(32, 16) Calculate the probability an item from distribution Y weighs more than X, i.e P(Y - X > 0). I did everything correct and ended-up with: B = Y - X B ~ N(2, 41) = 1 - P (B < 0) = 1 - phi((0 - 2)/root(41)) = 1 - phi(-0.3123) = 1 - (1 - 0.6217) = 0.6217 However, their answer: B = Y - X B ~ N(2, 41) P(B > 0) = 1 - P(B < 0) = 1 - 0.3774 = 0.6226 The problem is I looked-up phi(0.31) in the Normal Distribution tables, whereas they calculated phi(0.3123) exactly. How do I calculate phi(0.3123) exactly? This is an International A Level question. We're not supposed to use the phi(z) integral formula. It's either a table look-up, or calculator exercise. To calculate a cumulative probability on my calculator it requires a lower and upper bound, but here the lower bound is -infinity?","['statistics', 'normal-distribution', 'probability']"
4407569,Is there a such thing as a free group generated by a free group?,"Is there a such thing as a free group generated by a free group? Let $F(A)$ be a free group generated by the elements of a set $A$ . If we momentarily consider $F(A)$ to be simply a set of words on elements of $A$ , is there a such thing as $F(F(A))$ , i.e., the free group generated by the elements of $F(A)$ ? My guess is that this free group would still just be $F(A)$ ? Since it will be all possible words on elements of $F(A)$ , which are also words on $A$ , therefore the elements of $F(F(A))$ will simply be products of words on $A$ , but those would already be in $F(A)$ by definition of a free group, so they would be the same.","['group-theory', 'abstract-algebra', 'free-groups']"
4407589,Find the limit $\lim\limits_{s\to0^+}\sum_{n=1}^\infty\frac{\sin n}{n^s}$,"This is a math competition problem for college students in Sichuan province, China. As the title, calculate the limit $$\lim_{s\to0^+}\sum_{n=1}^\infty\frac{\sin n}{n^s}.$$ It is clear that the Dirichlet series $\sum_{n=1}^\infty\frac{\sin n}{n^s}$ is convergent for all complex number $\Re s>0$ . Here we only consider the case of real numbers. Let $$A(x)=\sum_{n\leq x}\sin n,$$ then we have that $$A(x)=\frac{\cos\frac{1}{2}-\cos([x]+\frac{1}{2})}{2\sin\frac{1}{2}},$$ here $[x]$ is the floor function. Obviously, $A(x)$ is bounded and $|A(x)|\leq\frac{1}{\sin(1/2)}$ . Using Abel's summation formula, we have that $$\sum_{n=1}^\infty\frac{\sin n}{n^s}=s\int_1^\infty\frac{A(x)}{x^{s+1}}\,dx
=\frac{\cos\frac{1}{2}}{2\sin\frac{1}{2}}-s\int_1^\infty\frac{\cos([x]+\frac{1}{2})}{x^{s+1}}\,dx.$$ The integral $\int_1^\infty\frac{\cos([x]+\frac{1}{2})}{x^{s+1}}\,dx$ or $\int_1^\infty\frac{\cos([x])}{x^{s+1}}\,dx$ is also convergent for $s>-1$ (am I right? use Dirichlet's test) My question: Is there an easy way to prove $$\lim_{s\to0^+}\int_1^\infty\frac{\cos([x])}{x^{s+1}}\,dx=
\int_1^\infty\lim_{s\to0^+}\frac{\cos([x])}{x^{s+1}}\,dx=
\int_1^\infty\frac{\cos([x])}{x}\,dx.$$ If the above conclusion is correct, we have that $$\lim_{s\to0^+}\sum_{n=1}^\infty\frac{\sin n}{n^s}=\frac{\cos\frac{1}{2}}{2\sin\frac{1}{2}}.$$ More generally, consider the Mellin tranform $g(s)=\int_1^\infty\frac{f(x)}{x^{s+1}}\,dx$ , here $f(x)$ is continuous except integers and have left and right limit at integers. If for $s=0$ , the integral $\int_1^\infty\frac{f(x)}{x}\,dx$ is convergent, do we have that $$\lim_{s\to0^+}\int_1^\infty\frac{f(x)}{x^{s+1}}\,dx\stackrel{?}=\int_1^\infty\frac{f(x)}{x}\,dx\,$$ (In Jameson's book The prime number theorem , page 124, there is a Ingham-Newman Tauberian thereom, but the conditions of the theorem there are slightly different from here.) If the condition is strengthened to for all $s\geq-1/2$ , $\int_1^\infty\frac{f(x)}{x^{s+1}}\,dx$ is convergent, is the following correct $$\lim_{s\to0^+}\int_1^\infty\frac{f(x)}{x^{s+1}}\,dx\stackrel{?}=\int_1^\infty\frac{f(x)}{x}\,dx\,$$ If this is correct, is there a simple way to prove it? (2022/3/24/21:53) If the Dirichlet integral $\int_1^\infty\frac{f(x)}{x^s}dx$ converges at $s_0$ , then it converges uniformly in $$|\arg(s-s_0)|\leq\alpha<\frac{\pi}{2}$$ for any fixed $0<\alpha<\frac{\pi}{2}$ , and thus $$\lim_{s\to s_0^+}\int_1^\infty\frac{f(x)}{x^s}dx=\int_1^\infty\frac{f(x)}{x^{s_0}}dx.$$ For the uniform convergence of Dirichlet integral, see Uniform convergence about Dirichlet integral $f(s):=\int_1^\infty\frac{a(x)}{x^s}\,dx =\lim\limits_{T\to\infty}\int_1^T\frac{a(x)}{x^s}\,dx$","['tauberian-theory', 'analysis', 'dirichlet-series', 'wieners-tauberian-theorem', 'calculus']"
4407613,"Give an example of two non-empty sets $A$ and $B$ such that {$A∪B$, $A∩B$, $A−B$, $B−A$} is the power set of some set.","Give an example of two non-empty sets $A$ and $B$ such that { $A∪B$ , $A∩B$ , $A−B$ , $B−A$ } is the power set of some set. This is problem 1.33 in Polimeni, Chartrand, and Zhang's Mathematical Proofs: A Transition to Advanced Mathematics. What is a good strategy for a problem of this kind? I understand what I am being asked to do; I am just unsure about the best way to begin.",['elementary-set-theory']
4407628,How to prove that $\sqrt{3}\pi/6=\prod_{p \equiv 1 \pmod{6}} \frac{p}{p-1}\prod_{p \equiv 5 \pmod{6}} \frac{p}{p+1}$ with $p \in \mathbb{P}$?,"I would like to prove the formula $$\frac{\sqrt{3}\pi}{6}=\left(\prod_{\substack{p \equiv 1 \pmod{6} \\ p \in \mathbb{P}}} \frac{p}{p-1}\right) \cdot \left(\prod_{\substack{p \equiv 5 \pmod{6} \\ p \in \mathbb{P}}} \frac{p}{p+1}\right)=\frac{5}{6} \cdot \frac{7}{6} \cdot \frac{11}{12} \cdot \frac{13}{12} \cdot \frac{17}{18} \cdots ,$$ it appears listed on the wikipedia article List of formulae involving π#Infinite products , I put a citation needed tag on it and left a message on the talk page of the user who added the formula but this was 5 months ago and is unlikely to get an answer as this formula is that user only edit ever so it looks more like throw away account. Numerically the formula seems to be correct so my guess is that it was taken from some paper or that it is not that hard to prove, I don't much about this stuff my only idea was try something to similar to the dirichlet beta function $\beta(s) = \prod_{p \equiv 1 \ \mathrm{mod} \ 4} \frac{1}{1 - p^{-s}} \prod_{p \equiv 3 \ \mathrm{mod} \ 4} \frac{1}{1 + p^{-s}}$ since $\beta(1)=\pi/4$ like this answer but couldn't make it work mod 6 so I got nowhere. Also looked for the formula with approach0.xyz/searchonmath.com but couldn't find it either. The code ""Product[Piecewise[{{p/(p - 1), Element[p, Primes] && Mod[p, 6] == 1}}, 1], {p, 2, 1000}] Product[Piecewise[{{p/(p + 1), Element[p, Primes] && Mod[p, 6] == 5}}, 1], {p, 2, 1000}]"" can be used on wolfram online to evaluate the series (Thanks to
@J.M.can'tdealwithit who provided me with code over the mathematica stackexchange chat) Any help would be appreciated.","['dirichlet-series', 'number-theory', 'euler-product']"
4407631,Computation of maximum likelihood estimates,"Question Solve for the maximum likelihood estimates of the distribution with parameters $\alpha$ and $\beta$ and the following pdf , where $x > 0$ : $$f(x; \alpha, \beta) = \frac {\alpha} {\sqrt {2\pi \beta}} x^{-\frac 3 2} \exp\left[-\frac {(\alpha - \beta x)^2} {2\beta x}\right].$$ Hint: You should be able to write the parameter estimates in terms of $\overline{x}$ and $\overline{1/x}$ , where $\overline{1/x} = \frac 1 n \sum^n_{i = 1} \frac 1 {x_i}$ . My working $$\begin{aligned}
l(\alpha, \beta) & = n\ln\frac {\alpha} {\sqrt{2\pi \beta}} - \frac 3 2 \sum^n_{i = 1} \ln x_i - \sum^n_{i = 1} \frac {(\alpha - \beta x_i)^2} {2\beta x_i}\\
& = n\ln \alpha - \frac 1 2 n \ln(2\pi) - \frac 1 2 n \ln \beta - \frac 3 2 \sum^n_{i = 1} \ln x_i - \sum^n_{i = 1} \frac {(\alpha - \beta x_i)^2} {2\beta x_i}\\
\implies \frac {\mathrm{d}l} {\mathrm{d}\alpha} & = \frac n {\alpha} - \sum^n_{i = 1} \frac {2(\alpha - \beta x_i)} {2\beta x_i}\\
& = n \left(\frac 1 {\alpha} + 1\right) - \sum^n_{i = 1} \frac {\alpha} {\beta x_i}\\
& = n \left(\frac 1 {\alpha} + 1\right) - \frac {n\alpha} {\beta} \overline{1/x}\\
& = n\left(\frac 1 {\alpha} + 1 - \frac {\alpha} {\beta} \overline{1/x}\right)\\
\frac {\mathrm{d}l} {\mathrm{d}\beta} & = -\frac n {2\beta} - \sum^n_{i = 1} \frac {2(\alpha - \beta x_i)(-x_i)(2\beta x_i) - (\alpha - \beta x_i)^2(2x_i)} {4\beta^2 x^2_i}\\
& = -\frac n {2\beta} + \sum^n_{i = 1} \frac {(\alpha - \beta x_i)(2\beta x_i) + (\alpha - \beta x_i)^2} {2\beta^2 x_i}\\
& = -\frac n {2\beta} + \sum^n_{i = 1} \frac {(\alpha - \beta x_i)(\alpha + \beta x_i)} {2\beta^2 x_i}\\
& = -\frac n {2\beta} + \sum^n_{i = 1} \frac {(\alpha^2 - \beta^2 x^2_i)} {2\beta^2 x_i}\\
& = -\frac n {2\beta} + \frac {n\alpha^2} {2\beta^2} \overline{1/x} - \frac n 2 \overline{x}\\
& = \frac n 2 \left(\frac {\alpha^2} {\beta^2} \overline{1/x} - \frac 1 {\beta} - \overline{x}\right)
\end{aligned}$$ When $$\begin{aligned}
\frac {\mathrm{d}l} {\mathrm{d}\alpha} & = 0,\\
\frac {\beta(\alpha + 1)} {\alpha^2} & = \overline{1/x}.
\end{aligned}$$ When $$\begin{aligned}
\frac {\mathrm{d}l} {\mathrm{d}\beta} & = 0,\\
\alpha^2 \overline{1/x} - \beta & = \beta^2 \overline{x}.
\end{aligned}$$ Assuming my score equations are correct and I have not gone wrong anywhere, I am now stuck here. In particular, I am unable to see how I can further manipulate the two equations above to return just $\alpha$ and $\beta$ . Any intuitive explanations will be greatly appreciated :)","['partial-derivative', 'optimization', 'statistics', 'maximum-likelihood']"
4407632,Can we prove $\lim _{x\to0}\frac{\sin x}x=1$ with a functional definition for $\sin(x)$?,"We all know the geometric proofs for $$\lim _{x\to0}\frac{\sin(x)}x=1$$ can we find one based on purely functional definition? For example, let's take Apostol's definition: $\sin (x)$ and $\cos (x)$ are defined for all $x \in \mathbb R$ . $\cos (0) = \sin (\frac{\pi}2) = 1$ and $\cos (\pi) = -1$ . $\cos (y-x) = \cos y \cos x + \sin x \sin y$ for all $(x,y) \in \mathbb R ^2$ . and there is a fourth one used to get the limit: $0 < \cos (x) < \frac {\sin (x)}x <1$ for $0 < x< \frac{\pi}2$ . Do we always need this fourth one? Is that limit really unobtainable without geometry? It seems to me we can't reach all values of sine and cosine with just the three properties, as we can only define $\aleph _0$ values for $\cos (x)$ , but could we change the fourth one for "" $\cos(x)$ is continuous"" or something similar and derive the fundamental limit? Notice that in the use of the Taylor expansion we're already assuming the limit to be one as we are using the derivative of $\sin(x)$ .","['limits', 'trigonometry', 'functions', 'continuity']"
4407704,What kind of Taylor expansion is this?,"Let $b \colon  \mathbb{R}^n \to \mathbb{R}^n$ be $\mathcal{C}^{1,1}(\mathbb{R})$ , i.e. its derivative $b_x$ is Lipschitz. Let $0 \leq \lambda \leq 1$ and define $x^\lambda=\lambda x_0+(1-\lambda)x_1$ for $x_0,x_1 \in \mathbb{R}^n$ . In [Yong, Jiongmin, and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations] p.188 (4.22) I find the following equality \begin{aligned}
&\left|\lambda b\left( x_{1}\right)+(1-\lambda) b\left(x_{0}\right)-b\left( x^{\lambda}\right)\right| \\
&=\Big | \lambda \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta(1-\lambda)\left(x_{1}-x_{0}\right)\right) d \theta(1-\lambda)\left(x_{1}-x_{0}\right) \\
&\quad+(1-\lambda) \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta \lambda\left(x_{0}-x_{1}\right)\right) d \theta \lambda\left(x_{0}-x_{1}\right) \Big |
\end{aligned} Where does it come from?","['derivatives', 'taylor-expansion', 'analysis', 'real-analysis']"
4407803,Identity with number of surjective functions,"Let $S(n,k)$ denote the number of surjective functions from $\{1,2\cdots,n\}$ onto $\{1,2,\cdots,k\}$ . I  have verified for $n = 2, 4, 6, 8, 10$ that $$-(1/2)S(n,1) + (1/4)S(n,2) - (1/8)S(n,3) + \cdots + (-1/2)^n S(n,n) = 0$$ Is this true for all positive even n? Is it a known identity? Any nice proof?","['combinatorics', 'generating-functions']"
4407814,How can I use the Cauchy-Schwarz inequality in this function of random variables?,I have the function $\rho_{\lambda}:RV(\Omega)\rightarrow \mathbb{R}$ defined on the space $RV(\Omega)$ supported over some scenario set $\Omega$ : $\rho_\gamma(X)=\frac{1}{\gamma} \log (\mathbb{E}[e^{-\gamma X}])$ where $\gamma>0$ . Now in my book they claim that the Cauchy-Schwarz inequality shows that $\rho_\gamma(2X)\geqslant2\rho_\gamma(X)$ for every random variable $X$ and every positive $\gamma$ . I am having trouble seeing why this is the case though. Anyone have any ideas?,"['expected-value', 'cauchy-schwarz-inequality', 'probability', 'random-variables']"
4407839,Determine the value of $\mathbb{E}[X]\mathbb{E}[1/X]$ with $X$ a random variable such that $0<a\leq X\leq b$ [duplicate],"This question already has an answer here : Kantorovich inequality and Cauchy-Schwarz inequality (1 answer) Closed 2 years ago . Let b>a>0, determine the set $$\{\mathbb{E}[X]\mathbb{E}[1/X]\colon X \ \text{is a random variable and } X(\omega)\in[a,b],\ \forall \omega\in\Omega  \}$$ It is clear that we have $$\mathbb{E}[X]\mathbb{E}[1/X]\geq \big(\mathbb{E}[\sqrt{X}\sqrt{1/X}]\big)^2=1$$ by the Cauchy-Schwarz
inequality, but I have no idea about how to determine
the upper bound of the product, any help would be appreciated
.",['probability']
4407879,Hormander's identity,"Let $\alpha=(\alpha_{1},\dots,\alpha_{n})$ a multi-index. We define $$P^{(\alpha)}(\xi)=\dfrac{\partial^{|\alpha|}P}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}(\xi)$$ for $\xi=(\xi_{1},\dots,\xi_{n})$ and $$D^{\alpha}=(-i)^{\alpha}\dfrac{\partial^{|\alpha|}}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}.$$ Now, let $u,v :\Omega\subset \mathbb{R}^{n}\to \mathbb{C},$ where $\Omega$ is domain, that is, open and connected.
Then, by Hormander's Formula $$P(D)(uv)=\sum_{|\alpha|\leq m}\dfrac{D^{\alpha}u}{\alpha !} P^{\alpha}(D)v.$$ We will consider P(D) a linear differential operator with constant coefficients and $u=x_{k}$ and $v=\varphi \in C_{0}^{\infty},$ this formula would be $$P(D)(x_{k}\varphi)=x_{k}P(D)\varphi+P^{k}(D)\varphi.$$ But I couldn't get to that. In this case, just have a sense in $\alpha=(0,\dots,0)$ and $\alpha=(0,\dots,1,0,\dots,0)$ kth coord. In $\alpha=(0,\dots,0),$ $\dfrac{D^{\alpha}x_{k}}{\alpha!}P(D)\varphi = x_{k}P(D)\varphi$ . Its ok. Now is the problem. In $\alpha=(0,\dots,1,0,\dots,0),$ $D^{\alpha}x_k= -i.$ We can write P like be $\displaystyle P(\xi)=\sum_{|\beta|\leq m}a_\beta\xi^{\beta}=\sum_{|\beta|\leq m}a_\beta\xi_{1}^{\beta_{1}}\dots\beta\xi_{n}^{\beta_{n}}$ Then $P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k}\xi_{1}^{\beta_{1}}\dots\beta_{k}^{\alpha_{k}-1}\dots\xi_{n}^{\beta_{n}}$ Now i guess we need to change $\xi_{j}$ by $-i\dfrac{\partial}{\partial \xi_{k}}$ then obtain $$P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k} (-i\dfrac{\partial}{\partial \xi_{1}})^{\beta_{1}})\dots (-i\dfrac{\partial}{\partial \xi_{k}})^{\beta_{k}-1})\dots (-i\dfrac{\partial}{\partial \xi_{n}})^{\beta_{n}})$$ And now, what can I do? Can anybody help me, please?","['analysis', 'partial-differential-equations']"
4407902,Permutation Matrix Measure,"I’m trying to make a neutal network with dynamic architecture, meaning that the weight matrices will be added/removed dynamically when they are simple permutation matrices and thus don’t actually do any computation. However, I’ve run into an issue, I need a differentiable way to determine ”how close” the weight matrices are to being a permutation matrix. This measure will then be added to the loss function as a regularization term. I know how to find the closest permutation matrix by making a cost matrix as one of the answers from here: Nearest signed permutation matrix to a given matrix $A$ However, if I use the norm of the cost matrix as a measure of ”permutivity” the simple 0 matrix will always give a lower measure (loss) than an actuall permutation matrix. Whenever I try to find or come up with a measure I always run into this same problem, there’s always some other matrix which gives a lower score than a permutation matrix. Do any of you guys know of a ”permutation matrix measure” pr do you have any brilliant ideas on how to make one? Keep in mind that it has to be differentiable to be usefull,so no max functions or anything like that. Update: I have found one measure that does what I need, but it’s too computationally heavy to use. I’ll write it here anyway, since it gives insight to what I’m looking for and might spawn new ideas: $ \text{”permutivity”} = \prod_{i = 1}^{n} \sum_{j = 1}^{m}\sum_{k = 1}^{m} (a_{j,k}-{p_{i}}_{j,k})^2 $ Where $p_i$ is the ”i:th” permutation matrix of size MxM. So basically the product of the L2 distance of all the different permutation matrices. As you can imagine, this is not practical for several reasons, first of all, the number of permutation matrices are m! and for neural networks m is usually fairly large. Secondly, the product would explode causing numerical overflows when trying to calculate it. The second issue can be remedied using some sort of ”cap” function around the sums, but I can’t see any simple way to fix the first problem :(","['matrices', 'optimization', 'linear-algebra', 'permutation-matrices']"
4408018,How do you evaluate: $\int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx$,"I want to find the value of $\displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx$ At first, I solved this elementary integral: $\displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}} \ \mathrm dx$ Using the same method, I couldn't find my asked integral. Are there any ways to connect them? Any help would be appreciated.","['integration', 'definite-integrals', 'real-analysis', 'complex-analysis', 'calculus']"
4408050,Reference needed: Symbol sequence for pseudodifferential operators,"In Higsons's book Analytic K-Homology there is a section (subsection (b) in 2.8 ""Geometric Examples of Extensions"", starting from page 46) which discusses the following exact sequence called pseudodifferential operator extension (sometimes referred to as symbol sequence ): $0 \longrightarrow \mathcal{K}(L^2(M)) \longrightarrow \Psi^{0}_{phg}(M) \xrightarrow{\sigma_0} C_0(S^*M) \longrightarrow 0$ where $M$ is a smooth compact manifold without boundary, $S^*M$ the unit cosphere bundle, $\mathcal{K}(L^2(M))$ denotes the compact operators on $L^2(M)$ , $\Psi^{0}_{phg}(M)$ are the order $0$ classical pseudodifferential operators and $\sigma_0$ is the principal symbol map. I've seen this statement quite often but always without proof. Could someone give me a reference for this? Also, it is known that (by Rellich) every negative order pseudodifferential operator extends to a compact operator $L^2(M) \to L^2(M)$ .
In particular, $\Psi^{-1}_{phg}(M) \subseteq \mathcal{K}(L^2(M))$ . How do we prove that $\overline{\Psi^{-1}_{phg}(M)} = \mathcal{K}(L^2(M))$ where on the left side we have the norm closure?","['singular-integrals', 'analysis', 'smooth-manifolds', 'pseudo-differential-operators', 'functional-analysis']"
4408060,Finding the number of the group homomorphisms $G\to S_4$ ($|G|=6$) by group actions.,"A homomorphism from a group $G$ of order $6$ to $S_4$ is equivalent to an action of $G$ on the set $X=\{1,2,3,4\}$ . By the orbit-stabilizer theorem, every orbit must have size either $1$ , or $2$ , or $3$ (because $4\nmid 6$ ). Therefore, the only allowed orbit equations are: $\space\space\space 4 = 1+1+1+1$ $\space\space\space4 = 2+1+1$ $\space\space\space4 = 2+2$ $\space\space\space4 = 3+1$ Am I right if I consider as upper bound of the number in the title the number of ways the orbit equations 1 to 4 can actually be realized? I mean: 1 can be realized in the only way: $$\{\{1\},\{2\},\{3\},\{4\}\}$$ 2 in the following $6$ ways: \begin{alignat}{1}
&\{\{1,2\},\{3\},\{4\}\} \\
&\{\{1,3\},\{2\},\{4\}\} \\
&\{\{1,4\},\{2\},\{3\}\} \\
&\{\{2,3\},\{1\},\{4\}\} \\
&\{\{2,4\},\{1\},\{3\}\} \\
&\{\{3,4\},\{1\},\{2\}\} \\
\end{alignat} 3 in the following $6$ ways: \begin{alignat}{1}
&\{\{1,2\},\{3,4\}\} \\
&\{\{1,3\},\{2,4\}\} \\
&\{\{1,4\},\{2,3\}\} \\
&\{\{2,3\},\{1,4\}\} \\
&\{\{2,4\},\{1,3\}\} \\
&\{\{3,4\},\{1,2\}\} \\
\end{alignat} and 4 in the following $4$ ways: \begin{alignat}{1}
&\{\{2,3,4\},\{1\}\} \\
&\{\{1,3,4\},\{2\}\} \\
&\{\{1,2,4\},\{3\}\} \\
&\{\{1,2,3\},\{4\}\} \\
\end{alignat} So, I'd say that there are at most $1+6+6+4=17$ homomorphisms as in the title (the trivial one included), but according to this answer (where $G=C_2\times C_3$ ) this conclusion is wrong. Does it mean that distinct actions can have one same orbit setup?","['group-homomorphism', 'finite-groups', 'symmetric-groups', 'group-theory', 'group-actions']"
4408074,Are submatrices of an arbitrary complex unitary matrix diagonalizable?,"Consider a complex unitary matrix $U \in \mathbb{C}^{N\times N}$ and pick two diagonal and two off-diagonal elements from its $m$ th and $n$ th rows to construct a $2 \times 2$ submatrix: \begin{equation}
M=
\begin{bmatrix}
U_{mm} & U_{mn}\\
U_{nm} & U_{nn}
\end{bmatrix}
=
\begin{bmatrix}
U_{mm} & U_{mn}\\
(U_{mn})^* & U_{nn}
\end{bmatrix}
\end{equation} (Here I consider the Hermitian case $U_{nm} = (U_{mn})^*$ .) Is $M$ always diagonalizable? Update : Conjecture: $M$ should be diagonalizable.
Because $U$ is unitary, the eigenvectors of $U$ should span a linear space of dimension $N$ .
If there exists any $M$ not diagonalizable, the eigenvectors of that $M$ does not span a dimension $2$ space. Brute force proof:
find the eigenvectors of this sample $2\times 2$ submatrix.
Its eigenvectors are also shown in the figure. ( $a_{1,2}, b_{1,2} \in \mathbb{R}$ , and $c \in \mathbb{C}$ ) For the two eigenvectors to be identical, the following two equations need to be satisfied: \begin{equation}
(a_1-b_1)^2 - (a_2-b_2)^2 + 4|c|^2 = 0 \text{ and } (a_1-b_1)(a_2-b_2) = 0.
\end{equation} Clearly, both equations can be simultaneously satisfied.
When they are both satisfied, $M$ does not have two independent eigenvectors and therefore is not diagonalizable. This result looks counter-intuitive. Moreover, I started to think about this question when studying this historic paper .
This paper is about a recursive way of building any $N\times N$ unitary matrix with optical experiments.
Its equation 2 shows that by applying in total $N-1$ different $2\times 2$ rotational matrices to a $(N-1)\times (N-1)$ unitary matrix, one can construct a $N\times N$ unitary matrix.
Performing this recursively, one can use $N(N-1)/2$ rotational matrices to construct a $N\times N$ unitary matrix starting from an identity matrix. However, it seems that we just showed that not all unitary matrices $U$ can be constructed in this way...","['matrices', 'unitary-matrices', 'diagonalization', 'linear-algebra']"
4408125,Proof of a weighted norm inequality.,"I am reading the book Fourier Analysis by Javier Duandikoetxea and I am stuck in the proof of a lemma that it is the key part to prove the Hörmander's multiplier theorem. This is the lemma: Let $m\in L_a^2(\mathbb{R}^n),a>\frac{n}{2}$ and $\lambda>0$ . We define the operator: \begin{align*}
		T_{\lambda m}:L^2(\mathbb{R}^n)&\rightarrow L^2(\mathbb{R}^n) \\
		f&\mapsto T_{\lambda m}(f)		
	\end{align*} where $\widehat{T_{\lambda m}(f)}(\xi)=m(\lambda\xi)\hat{f}(\xi)$ . Then $\forall~u:\mathbb{R}^n\to\mathbb{C}$ : $$\int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi\leq C\int_{\mathbb{R}^n}|f(\xi)|^2\mathcal{M}u(\xi)d\xi$$ where $C>0$ is a constant which is independent from $u$ and $\lambda$ and $\mathcal{M}$ is the Hardy-Littlewood maximal operator which is defined as: $$\mathcal{M}(f)(x)=\sup_{x\in Q}\frac{1}{|Q|}\int_{Q}|f(\xi)|d\xi$$ where $Q$ is any cube containing $x$ , and: $$L_a^2(\mathbb{R}^n)=\lbrace g:L^2(\mathbb{R}^n)\to L^2(\mathbb{R}^n)\mid (1+|\xi|^2)^{a/2}\hat{g}(\xi)\in L^2(\mathbb{R}^n)\rbrace$$ Proof:
If $K =\hat{m}$ in then by our hypothesis, $(1 + |x|^2 )^{a/2}K(x) = R(x)\in L^2(\mathbb{R}^n)$ ,
and the kernel of $T_{\lambda m}$ is $\lambda^{-n}K(\lambda^{-1}x)$ . Hence: $$\int_{\mathbb{R}^n}|T_{\lambda m}(f)(\xi)|^2u(\xi)d\xi=\int_{\mathbb{R}^n}\left|\int_{\mathbb{R}^n}\frac{\lambda^{-n} R(\lambda^{-1}(\xi-x))}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}f(x)dx\right|^2u(\xi)d\xi\leq$$ $$\leq||m||_{L_a^2}^2\int_{\mathbb{R}^n}\int_{\mathbb{R}^n}\frac{\lambda^{-n}|f(x)|^2}{(1+\left|\lambda^{-1}(\xi-x)\right|)^{a/2}}dx u(\xi)d\xi\leq C_a ||m||_{L_a^2}^2\int_{\mathbb{R}^n}|f(x)|^2Mu(x)dx $$ Honestly I am quite lost with the proof, since it skips many  steps and it is hard for me to read it. It seems obvious that the first inequality should involve Cauchy-Scwarz and that the second inequality should involve Fubinni, but I am not able to write all the steps, especially the first equality has zero sense for me, since I can't get any of these symbols in my calculations. Sorry if I can't give many thoughts, but I don't have anything better. One of the things that annoys me is that I can't see what is the point of knowing what the kernel of $T_\lambda$ is. Any hints or help will be thanked.","['integration', 'fourier-analysis', 'fourier-transform', 'lp-spaces', 'inequality']"
4408136,Conditional variance of Y given X when y is a continuous function of x,"Technically the problem term would be $E[Y^2|X]$ For simplicity, let us for a minute assume that $y = a + bx$ Is it mathematically correct to write: $E[Y^2|X] = E[(a+bx)^2]$ , and if so why? And how would this apply to the direct formula of variance? Can we formulate $$V[Y|X] = E[(Y - E[Y|X])^2 \ | \ X] = E[((a+bx)-E[Y|X])^2 ]$$ Also does anyone have a link where I can read more about this?","['expected-value', 'statistics', 'variance', 'conditional-expectation']"
4408143,Sum of Two functions with IVP,"I am looking for a different   example  from one below   of two functions that satisfy intermediate value property ( IVP ) but their sum  not. Here is my work
the Let $f(x)=g(x)=\sin (\frac{1}{x})$ for $x\neq 0$ and $f(0)=0$ , $g(0)=1$ . Then,  both of $f$ and $g$ are Darboux functions but $f-g$ is not. $f$ and $g$ satisfy IVP ,. To see $f-g$ does not satisfy IVP Notice that $(f-g)(x)=0$ for all $x\neq 0$ and $(f-g)(0)=-1$ . Now, $(f-g)(0)<-\frac{1}{2}<(f-g)(1)$ but there is no $x\in(0,1)$ such that $(f-g)(x)=-\frac{1}{2}.$ I know this is right but  I would like to see another example.","['calculus', 'functions', 'real-analysis']"
4408149,"Rearranging series and ""placid"" permutations","This question came out of a conversation with my students about Riemann's rearrangement theorem , and the general problem of which permutations are ""safe"" w/r/t summing infinite series. Let $S_\infty$ be the group of permutations of $\mathbb{N}$ . For a sequence $\mathscr{A}=(a_i)_{i\in\mathbb{N}}$ , say that a permutation $p\in S_\infty$ is $\mathscr{A}$ -placid iff for every $q\in S_\infty$ and every pair of integers $z_0, z_1$ we have $$\sum_{i\in\mathbb{N}}a_{q(i)}\simeq \sum_{i\in\mathbb{N}}a_{p^{z_0}\circ q\circ p^{z_1}(i)}$$ where "" $s\simeq t$ "" means ""either $s$ and $t$ are each undefined, or they are defined and equal."" Basically, $p$ is $\mathscr{A}$ -placid if $p$ is never interesting from the point of view of rearranging the terms in $\mathscr{A}$ . For example, the permutation swapping $2i$ and $2i+1$ for each $i$ is $\mathscr{A}$ -placid for every $\mathscr{A}$ . I'm curious whether placidity actually depends on the sequence in question (restricting attention to sequences whose corresponding series converge conditionally, to avoid triviality) . For example, are the following equivalent? $p$ is placid with respect to the alternating harmonic sequence $((-1)^{i+1}{1\over i})_{i\in\mathbb{N}}$ . $p$ is $\mathscr{A}$ -placid for every conditionally convergent sequence $\mathscr{A}$ . I suspect the answer is negative, but I don't immediately see how to prove it. EDIT: as far as I can tell, the only ""obviously placid"" permutations are those in which there is a finite bound on the distance an element of $\mathbb{N}$ is moved. Merely having finite orbits isn't enough: for example, for any conditionally convergent sequence $\mathscr{S}$ there is a permutation of $\mathbb{N}$ of order $2$ which applied to $\mathscr{S}$ results in a series with limit $+\infty$ . This was pointed out to me by a colleague after I brashly claimed otherwise! EDIT THE SECOND: Now asked at MO .","['analysis', 'real-analysis', 'conditional-convergence', 'sequences-and-series', 'convergence-divergence']"
4408158,"Baby Rudin, Ex. 24 Chapt. 5, possible imprecision?","In Rudin's ""Principle of Mathematical Analysis"", the following exercise is given in the chapter abut differentiation: The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$ . Fix some $\alpha > 1$ , and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$ Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$ . Try to explain, on the basis of properties of $f$ and $g$ , why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$ , draw the zig-zags suggested in Exercise 22.) Do the same when $0 < \alpha < 1$ . where it is referring to the following exercise: Suppose $f$ is a real function on $(-\infty, \infty)$ . Call $x$ a fixed point of $f$ if $f(x)=x$ . (c) If there is a constant $A < 1$ such that $\left| f^\prime(t) \right| \leq A$ for all real $t$ , prove that a fixed point $x$ of $f$ exists, and that $x = \lim x_n$ , where $x_1$ is an arbitrary real number and $$ x_{n+1} = f \left( x_n \right) $$ for $n = 1, 2, 3, \ldots$ . (d) Show that the process described in (c) can be visualized by the zig-zag path $$ \left( x_1, x_2 \right) \rightarrow \left( x_2, x_2 \right) \rightarrow \left( x_2, x_3 \right) \rightarrow \left( x_3, x_3 \right) \rightarrow \left( x_3, x_4 \right) \rightarrow \cdots.$$ (Credit to @Saaqib Mahmood for having written these exercises before). Now, to my actual problem with how the exercise is stated: in the first line Rudin suggests that the result of Ex. 22 Point (c) can be extended to functions $f:]0, +\infty[ \rightarrow ]0, +\infty[$ which, I think, isn't always true. For instance consider the following counter example: $f(x) = \frac{1-e^{-x}}{2}, x > 0$ . It is easy to see that it satisfies the hypothesis of Ex. 22 Point (c), however, any sequence defined by recursion through it, starting with a positive real, approaches $0$ (which is in fact its only fixed point on $\Bbb R$ ) as $n \rightarrow +\infty$ . I would like to know other opinions or thoughts on why Rudin has put it like that.","['lipschitz-functions', 'real-analysis', 'algorithms', 'general-topology', 'derivatives']"
4408162,"Baby Rudin, Exr. 24 Ch. 5, explanation","In Rudin's Principles of Mathematical Analysis (3e), the following exercise is given in the chapter about differentiation: Exr.24 (p.118): The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$ . Fix some $\alpha > 1$ , and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$ Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$ . Try to explain, on the basis of properties of $f$ and $g$ , why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$ , draw the zig-zags suggested in Exercise 22.)
Do the same when $0 < \alpha < 1$ . The exercise referred to above reads: Exr.22 (p.117): Suppose $f$ is a real function on $(-\infty, \infty)$ . Call $x$ a fixed point of $f$ if $f(x)=x$ . (c) If there is a constant $A < 1$ such that $\left| f^\prime(t) \right| \leq A$ for all real $t$ , prove that a fixed point $x$ of $f$ exists, and that $x = \lim x_n$ , where $x_1$ is an arbitrary real number and $$ x_{n+1} = f \left( x_n \right) $$ for $n = 1, 2, 3, \ldots$ . (d) Show that the process described in (c) can be visualized by the zig-zag path $$ \left( x_1, x_2 \right) \rightarrow \left( x_2, x_2 \right) \rightarrow \left( x_2, x_3 \right) \rightarrow \left( x_3, x_3 \right) \rightarrow \left( x_3, x_4 \right) \rightarrow \cdots.$$ (Credit to @Saaqib Mahmood for having written these exercises before). I know that this question was asked before (see Prob. 24, Chap. 5 in Baby Rudin: For $\alpha>1$, let $f(x) = (x+\alpha/x)/2$, $g(x) = (\alpha+x)/(1+x)$ have $\sqrt{\alpha}$ as their only fixed point ), however it didn't receive a satisfying explanation: I do not understand what is Rudin asking in Exr. 24. I tried to compare the derivatives of the functions, using also the Mean Value Theorem in order to link them to the approximation we make computing $\sqrt{\alpha}$ by means of $x_n$ , however, I only obtain a gross estimate of this error, which is too coarse to actually give any information on the rate of convergence (it is more precise to directly deal with the two functions). I also tried to compare the "" zig-zag "" (as Rudin calls it) of the two functions and I have only noticed that $f$ creates a staircase whereas $g$ creates a spiral, but I can not draw any other conclusion from that. Any help is highly appreciated as always!","['real-analysis', 'algorithms', 'derivatives', 'cobweb-diagram', 'dynamical-systems']"
4408165,Lambda Calculus Xor,"How do you prove $\mathsf{xor} \, \mathsf{True}\, \mathsf{True}$ is false in lambda calculus using call-by-value reduction. This is the approach I tried but it is not working: $$\mathsf{xor} \equiv \lambda xy.x(y F T) y$$ $$\mathsf{xor} \, T T \equiv (\lambda xy.x(y F T)y) T 
T$$ $$\mathsf{xor}\, T T \equiv (\lambda xy.x(y (\lambda x y.y) (\lambda x y.x))y) T 
T$$ Then by beta reduction of leftmost inner $$\mathsf{xor}\, T T \equiv (\lambda xy.x(y (\lambda y.y) )y) T 
T$$ But I feel there is a mistake somewhere as the steps above did not lead me to the expected answer.","['lambda-calculus', 'discrete-mathematics', 'computer-science']"
4408192,"Is the solution to $\theta''+0.021\,\text{sgn}(\theta')\sqrt{|\theta'|}+0.02\sin(\theta)=0,\,\theta_0=\pi/2,\,\theta'_0=0$ of finite duration?","Is the solution to $\ddot{\theta}+0.021\,\text{sgn}(\dot{\theta})\sqrt{|\dot{\theta}|}+0.02\sin(\theta)=0,\,\,\theta(0)=\frac{\pi}{2},\,\dot{\theta}(0) = 0 \quad\text{(Eq. 1)}$ of finite duration? I would like to know if the solution is of finite duration, meaning this that the solution reaches zero in finite time and stays there forever after (instead of being ""vanishing at infinity""): this means I need to prove if there exists a finite extinction time such after it passes the solution becomes zero and stays there $$\exists\,\, T<\infty \,/ \,\theta(t)=0 \,\,\forall t\geq T $$ Looking for a mathematical prove of that (theoretical or numerical, but such as confirms without any doubt if is truly a solution of finite duration). To be explicit about what I am asking for, here is an example: lets think in the equation $\dot{y}=-\sqrt{y},\,y(0)=1$ which has as solution $y(t)=\frac{1}{4}\left(t-2\right)^2$ , here the solution is not of finite duration, since after reaching zero at $t=2$ , it will start to rising again. But the equation $\dot{x} = -\text{sgn}(x)\sqrt{|x|},\,x(0)=1$ will have as solution $x(t) = \frac{1}{4}\left(1-\frac{t}{2}+\left|1-\frac{t}{2}\right|\right)^2$ which indeed is of finite duration reaching zero at $t=2$ and staying there forever (see plot here , and demo here ). I want to know if the solution $\theta(t)$ behaves at the end like the solution $x(t)$ , reaching zero at some ending time and staying there forever. I made this question later on a physics forum here , but in the actual question I am only focusing it on the mathematical side of it: please don´t close it as duplicate, or move the other one to the math forum instead my attempts so far I am trying to understand the papers by Vardia T. Haimo: Finite Time Differential Equations and Finite Time Controllers but they use tools I don´t have enough background to fully understand (like Liapunov Theory), so I change the classical nonlinear pendulum equation introducing a non-Lipschitz component similar to used on the papers which solutions numerically looks like having finite duration, to have something familiar to work with it. PS: Obviously, if you give the exact solution $\theta(t)$ it will be awesome! Added later Following some comments I have tried to research about Liapunov theory on the book ""Nonlinear Dynamics"" by H. K. Khalil, and I believe I am understanding the general idea, but is unreal to believe I will master the topic at least before the bounty ends (I am certainly, not a genius). But in this line, I found the paper Finite-Time Stability of Continuous Autonomous Systems by Sanjay P. Bhat and Dennis S. Bernstein where they used some Liapunov based tools to determine if a function has a Finite-settling-time behavior, but is too advanced to me. Hope someone could answer the question, to have a guide to focus on the specific topic of solution of finite-duration. another attempt Following this answer to other question I did, I believe the following: Since Eq. 1 have the zero function as a trivial solution Since the components $\theta$ , $\dot{\theta}$ , and $\ddot{\theta}$ are all jointly present in Eq . 1. So the Eq. 1 stands the existence of some time $t^*$ such $\theta(t^*)=\dot{\theta}(t^*)=\ddot{\theta}(t^*)=0$ , don't meaning here that this/these time $t^*$ exists on the nontrivial solutions, but at least the equation could stand its existence. To prove that the equation stands a finite-duration solution is equivalent to prove if there exist some ""ending time"" $T$ where $\theta(T)=\dot{\theta}(T)=0$ , because of the following: If I can choose a finite duration solution $y(t) = \theta(t)H(T-t)$ with $H(t)$ the Heaviside unitary step function: $$H(t) = \begin{cases} 1,\quad t > 0 \\ 0,\quad t \leq 0 \end{cases}$$ I will have that: $$\begin{array}{r c l}
\dot{y}(t) & = & \frac{d}{dt}\left(\theta(t)H(T-t) \right)\\
& = & \dot{\theta}(t)H(T-t) + \theta(t)\delta(T-t) \\
& = & \dot{\theta}(t)H(T-t) + \underbrace{\theta(T)\delta(T-t)}_{=\,0\,\text{if}\,\theta(T)\,=\,0} \\
& = & \dot{\theta}(t)H(T-t)
\end{array}$$ And equivalently: $$\begin{array}{r c l}
\ddot{y}(t) & = & \frac{d}{dt}\left(\dot{\theta}(t)H(T-t) \right)\\
& = & \ddot{\theta}(t)H(T-t) + \dot{\theta}(t)\delta(T-t) \\
& = & \ddot{\theta}(t)H(T-t) + \underbrace{\dot{\theta}(T)\delta(T-t)}_{=\,0\,\text{if}\,\dot{\theta}(T)\,=\,0} \\
& = & \ddot{\theta}(t)H(T-t)
\end{array}$$ So, for the second order system with zero as trivial solution, if there exists a ending time $T$ such as $\theta(T)=\dot{\theta}(T)=0$ the finite duration solution could be made by any solution after time $T$ ""stiched"" with the trivial zero function solution after time $T$ (this because $y(t) \equiv \theta(t),\, t<T$ ). I don´t know if is easier or harder to prove that exists a point $T$ that fulfills $\theta(T)=\dot{\theta}(T)=0$ than using Liapunov theory tools, but is an idea I have tried, unsuccessfully so far, but maybe it gives an idea of how to solve the problem to anybody else. Other attempts Solving Eq. 1 for $\ddot{\theta}(t) = 0$ leads to a $\tan^{-1}$ which never achieves exactly zero. Here is the attempt done by other person in the question of the physics forum. Interestingly are found a time $T \approx 90$ which is similar to the plot ending. Solving Eq. 1 for $\dot{\theta}(t) = 0$ will lead to the classic equation of the nonlinear pendulum without friction, which solutions are Jacobi Elliptic functions, so since are periodic, there are going to be infinite times where it lands on zero (I have a related question here ). Solving Eq. 1 for $\theta(t) = 0$ leads to $\dot{\theta}(t) = \frac{a^2}{4}(T-t)^2H(T-t)$ as is explained here , which looks promising since it has only one ending time $T$ , but with the actual initial conditions it shows to be centered at $T=0$ (at least under my approach), so it don´t work either (since from the plot the only point with $\ddot{\theta}=\dot{\theta}=0$ is at the end, I was hopeful about its validity). Maybe here I am making some mistake, so If someone could try it too and can fix it it will solve the question, since founding $T$ such as $\theta(T)=\dot{\theta}(T)=0$ will answer the problem (if I am not mistaken). PS (2) : About the ""arbitrarity"" of the selection of the non-lipschitz component,  the traditional pendulum model friction as proportional to the rate of change as is explained on Wikipedia as the Newtonian Law of Viscosity , but in the same section is explained that are other models, like the Power Law Model which has components actually similar with which I am using here. I hope that knowing how to model finite duration solution will lead to adapts these classic models to show solutions that indeed behaves as having an ending time $T < \infty$ , which I believe is the case on everyday phenomena, as a clear example, the Euler's Disk toy that show its ending time when the sound stops. Last update I have just found the following papers under the term sublinear damping that shows example in physics of equations really similar to the proposed example: ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""A conservation law with spatially localized sublinear damping"" - Christophe Besse & Sylvain Ervedoza ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef Particularly, the first one, since graphically the solutions shows having a decaying envelope, I think could be proving than when the solutions enters the small-angle approximation regimen $\sin(x) \approx x$ it will be indeed being a solution with a finite extinction time. Among the three papers I thought a formal prove to the main question could be done, but I wasn't able to find it by myself.","['ordinary-differential-equations', 'real-analysis', 'singular-solution', 'finite-duration', 'dynamical-systems']"
4408222,Triangle-geometry problem,"Here is the question: $\cos(A-B) = \frac{7}{8}$ , $\cos(C) = ?$ By the Law of Cosines, I get: $AB^2 = 41-40\cos(C)$ I also tried to expand $\cos(A-B)$ by the compound angle formula, getting: $\cos(A)\cos(B) + \sin(A)\sin(B)$ Which by the Law of Sines becomes: $\cos(A)\cos(B) + \frac{5}{4} \sin(B)^2 = \frac{7}{8}$ That's where I have been able to get so far. One thing though that has been bothering me is whether $AB =3$ . I am tempted to go down that way because of the Pythagorean triple $3^2 + 4^2 = 5^2$ . However, they have not specified that $\angle{A} = \frac{\pi}{2}$ , so I am worried about wrongly assuming it. Any assistance would be greatly appreciated.","['trigonometry', 'algebra-precalculus', 'geometry']"
4408271,Relationship between Dirichlet's Approximation Theorem and Convergents,"For any real number $r$ , the convergents to the continued fraction expansion of $r$ satisfy Dirichlet's approximation inequality of $|r - \frac{p}{q}| < \frac{1}{q^2}$ . Does this go the other way? That is, if given some rational number $p/q$ , we have that $|r - \frac{p}{q}| < \frac{1}{q^2}$ , does that necessarily mean that it is a convergent of the continued fraction expansion of $r$ ? It does seem, from this page , that we have a similar result that the convergents, and only the convergents, minimize $|r - \frac{p}{q}| \cdot q$ among all smaller rationals, so we would need to show that whenever this happens, we also have $|r - \frac{p}{q}| \cdot q < \frac{1}{q}$ . EDIT: Legendre has also proven that if $|r - \frac{p}{q}| < \frac{1}{2q^2}$ then $p/q$ is a convergent of $r$ . So the main question is what happens if for whatever $p/q$ we have that $\frac{1}{2q} < |r - \frac{p}{q}| < \frac{1}{q}$ : can we determine that $p/q$ is a convergent, or a semiconvergent, or anything at all?","['number-theory', 'continued-fractions', 'approximation', 'diophantine-approximation']"
4408278,"Prove that if a small circle is 'inside' a larger circle but makes contact with at least one point, then the two circles only have one point in common","I am having some difficulties answering this question: For some fixed $x_0,x_1,y_0,$ and $y_1$ in $\mathbb R$ , where the ordered pairs $(x_0,y_0) \neq (x_1,y_1)$ ,prove that the following sets have an intersection of precisely one element: $S_1=\left\{(x,y) \in \mathbb R \times \mathbb R: \sqrt{(x-x_1)^2+(y-y_1)^2}=\sqrt{(x_1-x_0)^2+(y_1-y_0)^2} \right\}$ $S_2=\left\{(x,y) \in \mathbb R \times \mathbb R: \sqrt{\left(x-(2x_1-x_0)\right)^2+\left(y-(2y_1-y_0)\right)^2}=\sqrt{\left(2\cdot(x_1-x_0)\right)^2+\left(2\cdot(y_1-y_0)\right)^2}\right\}$ These sets describe the following picture, where $(x_2,y_2)$ is simply $(2x_1-x_0,2y_1-y_0)$ : Picture of Small Circle Inside a Large Circle Making 1 Point of Contact The question at hand is equivalently framed as finding the solution to the set of equations: \begin{align} &(1) \quad \sqrt{(x-x_1)^2+(y-y_1)^2}=\sqrt{(x_1-x_0)^2+(y_1-y_0)^2} \\ &(2)\quad\sqrt{\left(x-(2x_1-x_0)\right)^2+\left(y-(2y_1-y_0)\right)^2}=\sqrt{\left(2\cdot(x_1-x_0)\right)^2+\left(2\cdot(y_1-y_0)\right)^2} \end{align} If $(x,y)$ satisfies $(1)$ and $(2)$ , then we must have $(1')$ and $(2')$ : \begin{align} &(1')\quad(x-x_1)^2+(y-y_1)^2=(x_1-x_0)^2+(y_1-y_0)^2 \\ &(2')\quad\left(x-(2x_1-x_0)\right)^2+\left(y-(2y_1-y_0)\right)^2=\left(2\cdot(x_1-x_0)\right)^2+\left(2\cdot(y_1-y_0)\right)^2 \end{align} After expanding $(2')$ , I was able to find that some portion of the terms on the left side of the equality had the form $(x-x_1)^2+(y-y_1)^2$ , so I proceeded to substitute in the right hand side of equation $(1')$ . This effectively substitutes a constant in for the quadratic terms $x^2$ and $y^2$ , while keeping the $x$ and $y$ terms, which will allow us to implicitly solve for one of the variables. After some additional simplification, I produced the equation: $$(3) \quad (x-x_1)(x_0-x_1)+(y-y_1)(y_0-y_1)=(x_0-x_1)^2+(y_0-y_1)^2$$ Some additional algebra and factoring leads to the equation that is giving me problems: $$(4) \quad (x_0-x_1)(x-x_0)+(y_0-y_1)(y-y_0)=0$$ It is easy to see from $(4)$ that $(x_0,y_0)$ is a valid solution (which makes sense from the construction of the sets). However, I am having difficulties showing that $(x_0,y_0)$ is the only valid solution. When we initially assumed that $(x_0,y_0) \neq (x_1,y_1)$ , we equivalently have that $x_0 \neq x_1 \text{ OR } y_0\neq y_1$ . Consider the case when $x_0=x_1$ and $y_0 \neq y_1$ . From $(4)$ , we see that, although we must have $y=y_0$ , $x$ can equal any number $\in \mathbb R$ . This is a problem if we want to show that only one ordered pair satisfies both equations. A similar complication arises if we assume that $x_0\neq x_1$ and $y_0 = y_1$ . As far as I can tell, all of the algebraic manipulations I made are perfectly reversible. Therefore, if $(x,y)$ satisfies $(4)$ , it should also satisfy $(2)$ . However, that is clearly not the case. i.e. it is definitely FALSE that $\forall x \in \mathbb R : (x,y_0) \in S_2$ . Where have I gone wrong in the argument?","['systems-of-equations', 'proof-writing', 'quadratics', 'geometry']"
4408281,"why do we need a sequence of random variable, isn't one function sufficient?","In sampling, we have so many situations involving a sequence of random variables, what I am confusing is why do we need a sequence of random variables to describe the process? It feels like each function is only used once. Suppose $$X_i:\Omega\to\mathbb{R}\quad,i\in\mathbb{N}$$ $X_1,X_2,X_3...$ basically just the $\mathbb{R}$ -valued image, each image has its corresponding function. why don't we only use a single random variable to describe those images, this also seems to be sufficient to describe the process, if not, what is the problem?","['sampling', 'probability-theory', 'probability', 'random-variables']"
4408288,Inverse of sets in product space,"I'm currently studying product topology(along with box topology). A problem that often comes up is to decide whether a given function is a homeomorphism or not. Bijectiveness usually isn't the hard part, continuity is. I solve these kind of problems by letting some basis element $\prod_{\alpha \in J}U_\alpha$ , and seeing whether $f^{-1}\left(\prod_{\alpha \in J}U_\alpha\right)$ is open or not. A usual problem that I encounter is whether the following statement holds: Let $f:A\to \prod_{\alpha \in J}X_\alpha$ be a bijective function, and define $f(a)=(f_\alpha(a))_{\alpha\in J}$ where $f_\alpha:A \to X_\alpha$ for each $\alpha$ . (This is the same setting as Theorem 19.6 in Munkres) For $\prod_{\alpha \in J}X_\alpha$ equipped with product/box topology, do we always have $$f^{-1}\left(\prod_{\alpha \in J}U_\alpha \right)=\prod_{\alpha \in J}f_{\alpha}^{-1}(U_\alpha)$$ where $U_\alpha$ is a subset(usually open, though) of $X_\alpha$ ? I think the statement is true, and the reasoning is $$x \in f^{-1}\left(\prod_{\alpha \in J}U_\alpha \right) \iff f(x) \in \prod_{\alpha \in J}U_\alpha \iff f_\alpha(x) \in U_\alpha \quad \text{for}\quad\forall \alpha\in J$$ $$\iff x \in f_{\alpha}^{-1}(U_\alpha) \forall \alpha \in J \iff x\in \prod_{\alpha \in J}f_{\alpha}^{-1}(U_\alpha)$$ So, in my opinion the statement holds whenever whether $\prod_{\alpha \in J}X_\alpha$ is equipped with either box/product topology, whether $U_\alpha$ 's are open or not, whether the index set $J$ is uncountable or not. Is this true?","['elementary-set-theory', 'general-topology']"
4408355,How do I compute the expectation value of the following random variable?,"I have the following problem: We have given $X_1,...,X_n$ i.i.d. random variables. And we define $F_X$ to be the distribution function of these random variable. Let $$Y=\operatorname{max}\{X_1,...,X_n\}$$ In the lecture we have just shown that $$F_Y(x)=F_X(x)^n$$ . Now I need to assume that the $X_i$ 's are Bernoulli random variables, so $$\Bbb{P}(X_i=0)=p,~~~\Bbb{P}(X_i=1)=1-p$$ I need to compute $\Bbb{E}(Y)$ . I thought about the following: $$\Bbb{E}(Y)=0\cdot \Bbb{P}(Y=0)+1\cdot \Bbb{P}(Y=1)=\Bbb{P}(Y=1)=\Bbb{P}(\operatorname{max}\{X_1,...X_n\}=1)\leq\Bbb{P}(X_1=1,...X_n=1)\stackrel{i.i.d}{=}\Bbb{P}(X_1=1)...\Bbb{P}(X_n=1)=(1-p)^n$$ But somehow I don't think this works because of this inequality. Is there maybe someone who can give me a hint how to try it? Thanks for your help","['expected-value', 'probability-theory', 'probability', 'stochastic-calculus']"
4408367,Different definitions of Hausdorff measure (balls vs cubes vs arbitrary open sets),"The $d$ -dimensional Hausdorff (outer) measure of a set $A ⊆ \mathbb{R}^n$ is usually defined to be $$
	\mathcal{H}^d(A) = \lim_{δ \to 0} \inf \left\{ \sum_i \operatorname{diam}(U_i)^d ~\middle|~  A ⊆ \bigcup_{i} U_i, \operatorname{diam} U_i < δ\right\}
$$ However the class of sets to which the $U_i$ are supposed to belong varies: Some authors require them to be balls while some allow all open sets. Cubes seems like another natural choice. Evidently the different definitions agree up to (dimensional) constants. They also agree for $d = n$ because then all three definitions of $\mathcal{H}^d$ coincide with the $n$ -dimensional Lebesgue measure. Do the different definitions coincide in general?","['measure-theory', 'hausdorff-measure', 'geometric-measure-theory']"
4408406,Prove that $B \setminus (B \setminus A) = A$,"Prove that $B \setminus (B \setminus A) = A$ My conclusion is that $B \setminus (B \setminus A) \neq A$ , OR, to be precise, $B \setminus (B \setminus A) = A \cap B$ . If I'm wrong, please help me prove it. If I'm correct, how should I show that the question asked cannot be proved?","['elementary-set-theory', 'discrete-mathematics']"
4408491,Solutions of Triangles inequalities - Duplicate,"A,B,C are angles of a triangle we are supposed to prove that $sin(\frac{A}{2})sin(\frac{B}{2})sin(\frac{C}{2})$ $\leq$ $\frac{1}{8}$ . I used trigonometric ratios of half angles which would give $\frac{(s-a)(s-b)(s-c)}{abc}$ . How can I proceed after this step? Any help would be appreciated ( I need the solution where angles are made in terms of sides and then the inequality is simplified)","['trigonometry', 'triangles', 'triangle-inequality']"
4408522,A sequence of measurable functions doesn't converge with the following properties,"Let $\{f_n\}$ be a sequence of measurable functions defined on $[0,1]$ with the following properties: $f_n(x)\in[0,1]$ for any $x\in[0,1]$ and $n\geq 1$ . $\lim_{n\to\infty} \int_{[0,1]} f_n = 0$ . $\{f_n(x)\}$ doesn't converge for any $x\in[0,1]$ . My thought was to construct a sequence of functions that are zero almost everywhere to satisfy 2). For example, the Dirichlet function on $[0,1]$ and doesn't depend on $n$ . But this sequence of functions seems not to satisfy 3) as it would converge to $1$ for any $x\in\mathbb{Q}$ . How do I construct such a sequence of functions? Any hints?","['measure-theory', 'real-analysis']"
4408567,Find the power of the matrix.,"Let $A = \left( {\begin{array}{*{20}{c}}
0&1&1\\
1&0&1\\
1&1&0
\end{array}} \right)$ .
I want to find $A^k,$ where $k \in N$ . So far I calculated $A^2, A^3, A^4,...$ but I can not see the general formula for $A^k$ . Here are $A^2, A^3, A^4, A^5$ . Not sure if this leads to anything but I found the general formula for $B^k$ , where $B = \left( {\begin{array}{*{20}{c}}
1&1&1\\
1&1&1\\
1&1&1
\end{array}} \right)$ . ${B^k} = \left( {\begin{array}{*{20}{c}}
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}\\
{{3^{k - 1}}}&{{3^{k - 1}}}&{{3^{k - 1}}}
\end{array}} \right)$ Thanks in advance.","['matrices', 'linear-algebra']"
4408570,The proof of an elementary equality,"For all $t\geq0$ , $x>0$ , we have the following important equality: \begin{equation}
\lim_{\lambda\rightarrow\infty}e^{-\lambda t}\sum_{k\leq\lambda x}\frac{(\lambda t)^k}{k!}=\chi_{[0,x)}(t)+\frac{1}{2}\chi_{\{x\}}(t),
\end{equation} where $\chi$ is the characteristic function. Actually, I can prove this equality with the help of the Poisson distribution, but I prefer to prove this equality by a basic analysis method. I've struggled with this problem for a few days, still have no idea. I'll appreciate it for any hints! Thanks for any help!","['harmonic-analysis', 'analysis', 'probability', 'real-analysis']"
4408572,How can we determine easily that the following transition matrix is irreducible?,"Let $n\in\Bbb N$ be arbitrary, and define the matrix $A=(a_{ij})_{i,j=0}^n$ via: $$a_{ij}=\begin{cases}0&|i-j|\neq1\\\frac{i}{n}&j=i-1\\\frac{n-i}{n}&j=i+1\end{cases}$$ Which represents the transition matrix of the Markov shift system where $n$ balls numbered $1\to n$ are distributed across two urns, and at every time step an integer value $1\le k\le n$ is chosen equiprobably, and the ball numbered $k$ is moved from the urn it's in to the other, and the state is the number of balls in the first urn. The text I was following claimed without proof that $A$ is always irreducible, but I don't know how one shows that. Computing $\sum_{k=1}^nA^k$ in general is not easy, nor is trying to reason about the general graph represented by $A$ . Indeed, I might try to show it is not able to be permuted into a block-triangular matrix, but I believe it is defacto block-triangular - for $n=4$ : $$A=\begin{bmatrix}0&1&0&0&0\\1/4&0&3/4&0&0\\0&1/4&0&3/4&0\\0&0&1/4&0&3/4\\0&0&0&1&0\end{bmatrix}$$ Is there not a block triangle formed by the four zeroes in the bottom left-hand corner? I feel like I am overlooking something trivial. Many thanks for any clarification.","['matrices', 'linear-algebra']"
4408581,Law of large numbers with incomplete observation,"I am currently reading the book Introduction to Reinforcement Learning by R. S. Sutton and A. G. Barto.
The authors often reason with the LLN. In particular, at one point there is an expression like this (beginning of Section 2.2 - Action-value Methods) $$
	\frac{\sum_{i = 1}^{t-1}R_i \mathbb{1}_{\{A_i = a\}}}{\sum_{i=1}^{t-1} \mathbb{1}_{\{A_i = a\}}},
	$$ where $R_i$ are the rewards and $A_i$ are the actions taken at time $i$ . If I understand correctly, they claim that by the LLN, this expression converges to the mean of $R_i$ as long as Action $a$ is chosen infinitely often. Intuitively this of course makes sense, but I am not convinced.
I am familiar with the LLN like this:
Take $X_1,X_2,\dots$ iid, where $\mathbb{E}[|X_1|]$ exists. Then $$
	\lim_{n \to \infty} \frac1n \sum_{i=1}^{n} X_i = \mathbb{E}[X_1] \hspace{20pt}
	$$ almost surely. I tried to recreate the situation from the book like this:
We have two sequences $X_1,X_2,\dots$ iid and $C_1,C_2,\dots$ iid (if necessary, let the two sequences be independent), where $\mathbb{E}[|X_1|]$ and $\mathbb{E}[|C_1|]$ exist. Let $\mathbb{P}[C_i = \pm 1] = 1/2$ . Then intuitively the expression $$
	\frac{\sum_{i = 1}^{n}X_i \mathbb{1}_{\{C_i = 1\}}}{\sum_{i=1}^{n} \mathbb{1}_{\{C_i = 1\}}}
	$$ should indeed converge almost surely to $\mathbb{E}[X_1]$ , since the probability of $\{C_i = 1\}$ only finitly many times is zero (i.e. you observe $X_i$ infinitely often almost surely). If this is correct, how can you argue this rigorously?","['statistics', 'law-of-large-numbers', 'probability']"
4408582,"Finding the limit $\displaystyle\lim_{(x,y) \to (0,0)}\frac{\log(x^2 y^2)}{\log(x+y)}$","Let $\Omega := \left\{ (x,y) \in \mathbb{R}^2 \mid x+y>0, x,y \neq 0 \right\}$ and let function $f: \Omega \to \mathbb{R}$ be defined by $$f(x,y) := \frac{\log\left(x^2 y^2\right)}{\log(x+y)}$$ Find the limit for $(x,y) \to (0,0)$ . I need some help finding this limit. I tried to substitute $y=mx$ , and the candidate is 4 (probably wrong.). As I understand it, I should create a chain of inequalities, arriving at something like $$ \left| f(x,y) - l \right| \leq \left|h(x,y)\right| $$ where $h(x,y)$ is a function that goes to zero. However, I have no clue how to do this here. EDIT: obviously, the $l$ above is the candidate limit.","['limits', 'multivariable-calculus']"
4408614,"$\sqrt{f}$ has bounded tangential derivatives if $f\in C^{1,1}$?","This question comes from the paper written by Guan, Trudinger, and Wang , which can be stated precisely as follows. Let $n\geq2$ be an integer and $\Omega\subset\mathbf R^n$ be a bounded
domain with smooth boundary. Assume $\Omega$ is uniformly convex if necessary. Given $f\in C^{1,1}(\bar\Omega)$ such that $$f>0\quad\hbox{in $\Omega$.}$$ Does there exist a constant $C>0,$ which is at most dependent on $n,~\Omega,$ and $\|f\|_{C^{1,1}(\bar\Omega)},$ such that \begin{equation}|\nabla
 f(x)\cdot\tau(x)|^2\leq Cf(x)\quad\hbox{for all $x\in\partial\Omega$},\label{1}\tag{1}\end{equation} where $\tau(x)$ is a unit tangent vector of $\partial\Omega$ at $x.$ In other words, does the tangential derivative $\partial_{\tau}\sqrt f$ have a uniform upper bound? Actually, authors used in above reference that an incorrect inequality as \begin{equation*}|\nabla f|^2\leq Cf\quad\hbox{in $\Omega.$}\end{equation*} Near the boundary, it is clear that the distance function $d(x)=\mathrm{dist}(x,\partial\Omega)$ is a counterexample. It seems that all consequences still hold if \eqref{1} is true. If $f(x_0)=0$ for some boundary point $x_0,$ then obviously we can take above $C=1$ at $x_0.$ The difficulty is to control the tangential derivative of $\sqrt f$ if $f(x)>0$ but it is very small.","['partial-differential-equations', 'analysis', 'real-analysis']"
4408643,Integrate $\int_0^\infty \frac x{ \sec x\cosh x \>+\>1}dx$,"I am interested in whether it is possible to evaluate the integral $$\int_0^\infty \frac x{ \sec x\cosh x +1}dx$$ For reference, the analogous integral below is manageable $$\int_0^\infty \frac 1{ \sec x\cosh x +1}dx= -\pi \sum_{k=1}^\infty (-1)^k \text{csch}\>k\pi
$$ which can be evaluated with the residues in the upper-half plane given the symmetry. But a similar approach for the integral in question is not applicable due to the odd integrand. I would like to know of any other  possibilities.","['integration', 'contour-integration', 'trigonometric-integrals', 'sequences-and-series']"
4408644,The palindrome counting function,"Let $b \geq 2$ be a positive integer and consider the function $f_b : \mathbb{N}^+ \to \mathbb{N}^+$ given by $$f_b (n) = |\{ k \in \mathbb{N}^+ : k \leq n \mbox{ and } k \mbox{ is palindromic in base } b\}|.$$ A simple counting argument shows that: $$f_b (b^k) = \begin{cases} (1+b) b^{\frac{k-1}{2}} - 2& \mbox{if } k \mbox{ is odd } \\ 2 \cdot b^{\frac{k}{2}} - 2 & \mbox{if } k \mbox{ is even} \end{cases}$$ Using the monotonicity of $f_b$ and the previous fact, together with the inequality $b^{\lfloor \log_b n \rfloor} \leq n \leq b^{1 + \lfloor \log_b n \rfloor}$ we get the following $$\frac{2\sqrt{b}}{b} \sqrt{n} - 2\leq 2 b^{\frac{\lfloor \log_b n \rfloor}{2}} - 2 \leq f_b(n) \leq (1+b)b^{\frac{\lfloor \log_b n \rfloor}{2}} \leq (1+b) \sqrt{n} - 2$$ From this it follows that $$ \frac{2\sqrt{b}}{b} - 2\frac{\sqrt{n}}{n} \leq \frac{f_b(n)}{\sqrt{n}} \leq (1+b) - 2\frac{\sqrt{n}}{n}$$ Consider the function $g_b: \mathbb{N}^+ \to \mathbb{N}^+$ given by $$g_b (n) = \frac{f_b(n)}{\sqrt{n}} \hspace{3mm} \forall n \in \mathbb{N}^+$$ The previous facts imply that $g_b$ is a bounded positive function, and that $$\frac{2\sqrt{b}}{b} \leq \liminf_{n \to \infty} g_b (n) \leq \limsup_{n \to \infty} g_b(n) \leq 1 + b$$ Computational evidence suggests that $$\liminf_{n \to \infty} g_b(n) = 2 \hspace{3mm} \mbox{ and } \hspace{3mm} \limsup_{n \to \infty} g_b(n) = \frac{(1+b)\sqrt{b}}{b}$$ How can i prove it? I think i need tighter bounds, and therefore a better and non trivial way to count palindromes.","['number-theory', 'real-analysis', 'palindrome', 'combinatorics', 'upper-lower-bounds']"
4408654,Tangents to a function and its inverse,"I came across the following problem: Find all $x$ values such that the tangent to the function $f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3}$ where $x \ge 0$ at that $x$ value is perpendicular to the tangent of its inverse function $f^{-1}(x)$ at that $x$ value. I know that a formula for the  derivative of the inverse function is $(f^{-1})^{'}(x) = \frac{1}{f'(f^{-1}(x))}$ and we want this expression for $(f^{-1})^{'}(x)$ to be the negative reciprocal of $f'(x)$ for the tangent of the function to be perpendicular to the tangent of its inverse at $x$ . This gives the equation $f'(x) = -f'(f^{-1}(x))$ . However, it seems the only way to solve this equation for $x$ is to find $f^{-1}(x)$ explicitly and I can't seem to find the inverse of $f$ because it is so complicated. I tried the standard swapping $y$ and $x$ and solving for $y$ trick. I even tried Wolfram Alpha but it didn't seem to find an answer in terms of elementary functions. Is there a way to solve this problem without explicitly finding the inverse or, if not, how would one go about finding the inverse of $f(x) = \frac{1}{x^2+1} + (1-2x)^{1/3}$ where $x \ge 0$ ?","['calculus', 'inverse-function', 'derivatives']"
4408687,semi perfect number - number of divisors,"The definition:
we define number as semi perfect , if the number equals to the sum of exactly k of its divisors. the question:
prove that for every n (n>0 | n belong to N)
n is semi perfect order 3 if and only if n is divided by 6. my attempt: <- n is divided by 6
because of that, its divisors are $$\frac{n}{6} ,\frac{n}{2} ,\frac{n}{3} $$ we will sum those 3 divisors - $$\frac{n}{6} + \frac{n}{2} + \frac{n}{3} = n$$ and we finished the proof for one side -> this is the side I have a bit more problem with
I know n uphold the definition above
but Im having trouble proving that n is divided by 6 with no remainder. I thought maybe trying with contradiction ( assuming it doesnt divided by 6)
but still stuck with this question would love to please have some help with it","['number-theory', 'perfect-numbers', 'elementary-number-theory']"
4408731,Solving $6^{50-x} = 2^{50}$,"I'm being asked to solve an equation for $x$ , giving the answer in the form below: $$a \log_{b} c$$ One of them is: $$6^{50-x} = 2^{50}$$ So I started by taking the $\log_6$ of both sides to give: $$\log_{6}6^{50-x}=\log_{6}2^{50} \tag1$$ Leading to: $$\begin{align}
50-x &=\log_{6}2^{50} \tag2 \\
50-x &=50\log_{6}2 \tag3 \\
-x &=-50+50\log_{6}2 \tag4
\end{align}$$ or finally: $$x=50-50\log_{6}2 \tag5$$ I've been given the answer of: $$x=50\log_{6}3 \tag6$$ How do I get from my answer to the correct form, or have I made a mistake along the way? Thanks,","['algebra-precalculus', 'logarithms']"
4408806,Why does the complex phase plot look exactly like the root locus diagram?,"BRIEF BACKGROUND Basically here's an interesting discovery I made (probably not an original discovery) when playing around with MATLAB the other day. I was basically just trying to make a good visualization of complex valued plots and trying to implement an HSV (hue, saturation, value) model of coloring a complex function. But my first step was just to plot the phase and magnitude separately with standard MATLAB colorings and this is when I made the interesting discovery that the phase plot of a complex valued function looks exactly like the root locus diagram of that function. This obviously cannot be a coincidence as when I change the poles and zeros the root locus diagram looks exactly the same. For those who are not familiar with root locus diagrams, they are useful tool in control systems to know where the poles and zeros of a system go when you change the feedback gain. If your open loop gain transfer function $L(s)$ is described by a polynomial and has certain poles and zeros when it is connected in a feedback system with a gain $K$ the poles and zeros of the total system move as $K$ is increased or decreased. Feedback System MATLAB CODE clearvars
clc
clf
format long

%% Complex Plotting Tools
% Setup our grid of complex values to cover the complex plane
z_real = linspace(-5, 5, 1000);
z_imag = linspace(-5, 5, 1000);
[RealZ, ImagZ] = meshgrid(z_real, z_imag);
z = RealZ + ImagZ * 1j;

% Set up the coefficients of our complex polynomial
% Define a function based off the numerator/denominator coefficients

zrs = [-1]';
pls = [-2+1j, -2-1j, -3]';
[num_poly, den_poly] = zp2tf(zrs, pls, 1);
f = @(x)(polyval(num_poly, x) ./ polyval(den_poly, x));

% Print the roots of the polynomials
disp(roots(num_poly));
disp(roots(den_poly));

% Plot the complex magnitude in these plots
figure(1)
hold on
grid on
surf(RealZ, ImagZ, log(abs(f(z))), 'EdgeColor', 'none');
colormap jet
colorbar

% Plot the complex phase in these plots
figure(2)
hold on
grid on
surf(RealZ, ImagZ, rad2deg(angle(f(z))), 'EdgeColor', 'none');
colormap jet
colorbar

%% Root Locus Plot
% Generate a pole/zero plot of the same system
% Generate a root locus plot of the same system

F = tf(num_poly, den_poly);

figure(3)
hold on
grid on
pzplot(F);

figure(4)
hold on
grid on
rlocus(F); PLOT DIAGRAMS Magnitude Plot of Complex Function Phase of a Complex Function Pole Zero Diagram Root Locus Diagram of Complex Function QUESTION So my question is this: Why does the phase plot of the complex valued function look exactly like its corresponding root locus diagram? I'm going to work on this myself to see if I can figure it out but figured I'd toss this question out to the larger community.","['complex-analysis', 'control-theory']"
4408809,Ring structure of the representation ring of a finite group,"I'm currently taking a course in representation theory, and I'm not entirely sure I understand the structure of the representation ring of a finite group. For a finite group $G$ with irreducible representations $(\rho_{i}, V_{i})_{i=1}^{n}$ over a field $F$ , we denote $$R_{F}[G] = \Big \{\sum_{i = 1}^{n}n_{i}V_{i} : n_{i} \in \mathbb{Z} \Big \}$$ to be the representation ring of the group $G$ with addition defined as direct addition of coefficients and multiplication defined by tensor product? I'm not entirely sure I understand the ring structure here. I understand that since we take coefficients in $\mathbb{Z}$ , we clearly have an abelian group by directing taking sums of coefficients of $V_{i}$ , but I'm not entirely sure I understand the multiplicative aspect of thing ring. From what I understand, it has to do with tensor products, but how is the tensor product actually taken? Do we distribute the tensor over all elements and then have to reduce the tensored representations back down to recover a ring element of this form? Would the multiplicative identity of this ring then just be the trivial representation with coefficient 1? This ring seems very odd to me, although it does seem to come up for instance in Brauer theorem. This question is somewhat similar to this one , but reading through the question and answer here doesn't really answer the question to me.","['ring-theory', 'abstract-algebra', 'representation-theory']"
4408871,Are there corner cases when using Hilbert basis and bounded operators?,"It seems that with a Hilbert basis and a bounded operator, you can do all the calculations while ignoring convergence issues, as if you were in a finite dimensional case. For example if $a_{ij}$ are the coefficients of operator $A$ in the basis, then $\langle f(x),e_1\rangle=\sum_{i=0}^\infty x_ia_{1i}$ like the formula for a component of a vector image by a linear application in the finite dimensional case. Or for example the product of two bounded operators would be (never seen it but I guess it works) : $c_{ij}=\sum\sum a_{ik}b_{kj}$ My question : are there still ""traps"" where treating these sums like finite sums in linear algebra, won't work ?",['functional-analysis']
4408893,Retrieve the random variable from its conditional expectations,"I came across a problem that looks easy but turns out to be extremely hard. The problem goes as follows: $X,Y$ are two independent random variables with support on interval $[0,1]$ and $\mathrm{E}[X]=\mathrm{E}[Y]=\mu \in (0,1)$ . Construct a random variable $Z=f(X,Y)$ as a function of $X,Y$ with the following two properties: $Z$ has support [0,1]. $\mathrm{E}[f(X,Y)|X] = X$ and $E[f(X,Y)|Y]=Y$ . Remark 1. The first example I have is $Z=XY/\mu$ which satisfies 2 but not 1 because $Z$ could take on value $Z=1/\mu>1$ . The second example is $Z=X+Y-\mu$ which satisfies 2 but not 1 again. Remark 2. In fact, my best strategy now is to consider a class of random variables $$
Z= \alpha \frac{XY}{\mu} + (1-\alpha) (X+Y-\mu) + \beta\mathrm{E}[(g(X)-\mathrm{E}g(X))(h(Y)-\mathrm{E}h(Y))]
$$ for any functions $g,h$ and scalars $\alpha,\beta$ . $Z$ clearly satisfies 2. But it is very hard to restrict the support of $Z$ to interval $[0,1]$ . I am stuck and I am looking forward to a fresh set of ideas from the community.","['expected-value', 'conditional-expectation', 'probability']"
4408921,Additional Criteria for function to have one minima,"I have a function $f(x_1,x_2,\ldots,x_n)$ that I want to find the region of the local minima for through some algorithm (ie coordinate descent). However I want to make sure it is the only minima when $x_j>0$ by proving this function only has one minima. However, this function is really complex (and I want to be able to generalize it to higher dimensions) so I was hoping to learn about some additional tests to see if I could apply them. Here is some additional information I know: $f$ is $C^\infty$ on the domain in question Define $g(x) = f(k_1, k_2, \ldots, x, \ldots, k_n)$ in which all variables except one are fixed. It is known that $g$ has only one minima As any variable grows arbitrarily large, $f$ grows arbitrarily large The epigraph of $f$ is likely not convex. To provide some details (without making it too much), but for any fixed values of $k_j$ , we can say that $g(x)$ is always of the form $$
g(x) = \log(xp_1+p_2) - \frac{p_3}{p_4^x} + p_5
$$ where the values $p$ are parameters based on which values you fix.","['real-analysis', 'multivariable-calculus', 'functions', 'optimization', 'algebra-precalculus']"
4408998,Cannot become ring because distribution law does not hold,"Commutative ring with unit is defined as $(R,+,\times)$ , where $(R,+)$ is abelian group and $(R,\times)$ is commutative multiplicative monoid with $1$ and $+$ and $\times$ satisfies distributive law. Could you give me an example $(R,+,\times)$ cannnot be a ring because $+$ and $\times$ does not satisfy distributive law although $(R，+)$ is abelian group and $(R,\times)$ is commutative multiplicative monoid with $1$ .","['monoid', 'ring-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4409028,Finding the equation of the Parabola,"The parabola $y = x^2 + bx + c$ has the following properties: The point on the parabola closest to $(12,3)$ is the intersection with the $y$ axis of the parabola. The parabola passes through $(-5,0).$ How can I find $(b, c)$ ? Here is my attempt: The point $(0, c)$ is the intersection with the $y$ axis of the parabola. The distance from $(12, 3)$ is $\sqrt{144 + (c-3)^2}$ , and we have the equation $-5b +25+c =0$ . But if we don't know vertex of the parabola how do we find $b$ & $c$ ?","['conic-sections', 'algebra-precalculus', 'quadratics']"
4409059,Asking About Best Upper Bound And lowerBound,"Find the best lower and upper bounds for $$\left(\cos A-\sin A\right)\left(\cos B-\sin B\right)\left(\cos C-\sin C\right),$$ $1)~~$ overall acute-angled $\Delta ABC.$ first of all we know that $\cos(A)-\sin(A) = -\sqrt{2}\sin(A - \frac{\pi}{4})$ $\Pi { -\sqrt{2}\sin(A - \frac{\pi}{4})}=-2\sqrt{2}*\Pi{(\sin(A-\frac{\pi}{4}))}$ $-\frac{1}{2}*\sqrt{2}<\sin(A-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ $-\frac{1}{2}*\sqrt{2}<\sin(B-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ $-\frac{1}{2}*\sqrt{2}<\sin(C-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ Then Lagrange? or there is any other method you all could suggest Edit: For Everyone who wanna post or finish using every possible method just write it down here","['multivariable-calculus', 'calculus', 'lagrange-multiplier', 'inequality']"
4409119,Total separatedness and separation axioms,"Recall that a nonempty topological space $X$ is said to be totally separated iff, for every distinct points $x,y \in X$ , there is a separation $U,V$ of $X$ such that $x \in U$ and $y \in V$ . It can be readily seen that every such space is completely Hausdorff. As such, I tried to find stronger conditions, while associating with separation axioms. A stronger condition involving regularity: Definition 1. A nonempty topological space $X$ is said to be regularly separated iff, it is $T_1$ and for every closed subset $A \subset X$ and a point $x \in X \setminus A$ , there is a separation $U,V$ of $X$ such that $x \in U$ and $A \subset V$ . Obviously, this implies total separatedness. The implication is strict. Deleted Tychonoff Corkscrew (Counterexamples In Topology by Steen & Seebach, Part II, Section 91.) is one counterexample. Note that this implies also complete regularity. An even stronger condition involving normality: Definition 2. A nonempty topological space $X$ is said to be normally separated iff, it is $T_1$ and for every disjoint closed subsets $A,B \subset X$ , there is a separation $U,V$ of $X$ such that $A \subset U$ and $B \subset V$ . Obviously, this implies regular separatedness. But is this implication strict? I have a strong intuition that Sorgenfrey Plane is one counterexample. Since Sorgenfrey Plane is totally separated, completely regular, but not normal, it would suffice to prove that Sorgenfrey Plane is regularly separated. How?","['sorgenfrey-line', 'general-topology', 'examples-counterexamples', 'separation-axioms']"
4409141,"System of polynomial equations induced by $f(x,y,z) = xyz - x^2 - y^2 - z^2$ where $f_x = f_y $ $= f_z = 0$","For the function $$f(x,y,z) := xyz - x^2 - y^2 - z^2$$ I was looking for the points where $f_x = f_y = f_z = 0$ , where these denote the partial derivatives with respect to the subscripted variable. This leads to the system of 3 equations: $$ \begin{aligned} xy-2z &= 0 \\ xz-2y &= 0 \\ yz-2x &= 0 \end{aligned} $$ It is clear that $x=0$ , $y=0$ , $z=0$ is one solution to this system. However, I was wondering how we find the full set of solutions for a polynomial system like this. Using an online calculator I have found that $(2,2,2)$ , $(-2,-2,2)$ , $(2,-2,-2)$ , $(-2,2,-2)$ are all solutions, although I wanted to see if I could see how to find these myself.","['systems-of-equations', 'computational-mathematics', 'functions', 'polynomials', 'partial-derivative']"
4409147,Rewrite the integral in the order dy dz dx,"Rewrite the integral $$\int_0^3 \int_0^{9-y^2} \int_\frac{y}3^1 f(x,y,z) dx dz dy $$ as an interated integral in the order dy dz dx. I have trouble visualizing if my answer is the correct iterated integral or not. I got $$\int_0^1 \int_0^{9-9x^2} \int_0^{3x} f(x,y,z) dy dz dx $$ but i am unsure if this is correct or perhaps there is some part of the domain not captured in my answer.","['integration', 'definite-integrals', 'integral-domain', 'multivariable-calculus', 'multiple-integral']"
4409154,"Square summability of sequences of the form $\int_0^1 f(x)x^n\,dx$","For a function $f\in L^2(0,1)$ let $$a_n=\int_0^1 f(x)x^n\,dx.$$ Is the sequence $\{a_n\}_{n=0}^\infty$ square summable for any $f$ ? I have tried to prove or disprove that by testing  specific examples, but with no success. I have impression that the question might be associated with the boundedness of the Hilbert matrix $H=\{(n+m+1^{-1}\}_{n,m=0}^\infty $ on $\ell^2$ space, i.e. $$\|Ha\|_{\ell^2}\le \pi \|a\|_{\ell^2},\qquad a=\{a_n\}_{n=0}^\infty$$","['linear-algebra', 'functional-analysis']"
4409155,How to find formulas for spherical coordinates in terms of Cartesian coordinates using Maple?,"We can go from spherical coordinates $(r, \theta, \phi)$ to cartesian coordinates $(x,y,z)$ using the equations: $x=r sin(\theta) cos(\phi)$ $y = r sin(\theta) sin(\phi)$ $z = r cos(\theta)$ Consider the task of finding formulas for $r$ , $\theta$ , and $\phi$ in terms of $x,y$ , and $z$ . The three equations above are a system that can be solved for $r$ , $\theta$ , and $\phi$ . This is a relatively easy task with pen and paper. $r = \sqrt{x^2+y^2+z^2}$ $\theta = cos^{-1}\frac{z}{r}=cos^{-1}\frac{z}{\sqrt{x^2+y^2+z^2}}$ $\phi = tan^{-1} \frac{y}{x}$ How can we get Maple to obtain the same result? If we try: solve({r*sin(t)*cos(p) = x, r*sin(t)*sin(p) = y, r*cos(t) = z}, [r, t, p]) We obtain some very long and relatively illegible expression. If my reasoning is correct about the simplicity of solving the original problem, why is it apparently not as straightforward in Maple? Furthermore, is there a way in Maple to obtain a relatively nice-looking solution?","['multivariable-calculus', 'maple']"
4409171,Bundle over $\mathbb CP^\infty$,"Good time of day. I have the following question Let $\eta$ -tautological bundle over $\mathbb CP^\infty$ . I don't understand why there is no such complex vector bundle $\xi$ over $\mathbb CP^\infty$ that bundle $\eta \bigoplus  \xi$ is trivial. I'm not sure about my attempt. I try to use Pontryagin classes for solving this task. If these classes are non-vanishing then this bundle will be non-trivial. We know that $2p(E \bigoplus F)=2p(E)\smile p(F)$ and it's famous fact that if $\theta$ is oriented real bundle of rank $2k$ then $p_k(\theta)=e(\theta)^2$ , where $e(\theta)^2$ is the square of Euler class. I don't know how to continue and compute this Thank you for your help","['complex-geometry', 'vector-bundles', 'algebraic-topology', 'differential-geometry']"
4409186,Proving an alternated Brownian Motion is a Martingale,"Let $B_t$ denote standard Brownian motion. Show that the stochastic process $M_t = B_t^2 - t$ is a martingale Ive shown the finiteness and adaptability. I am left to show that $\mathbb{E}[M_t  | \mathcal{F}_s] = B_s $ Here is my attempt but it is very wordy and feels unrigourous: Given $B_s$ we know $B_t$ is normally distributed with mean $B_s$ and variance $t-s$ Hence $\mathbb{E}[B_t^2 | \mathcal{F}_s] = \mathbb{E}[B_t | \mathcal{F}_s]^2 + V[B_t | \mathcal{F}_s] = B_s^2 + (t-s) $ . Therefore $\mathbb{E}[M_t | \mathcal{F}_s] = \mathbb{E}[B_t^2-t | \mathcal{F}_s] = B_s^2-s = M_s$ as desired. Could someone show a way to make this more formal? Is there an easy trick I'm missing :)
Thanks","['stochastic-processes', 'brownian-motion', 'probability-theory']"
