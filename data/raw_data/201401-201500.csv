question_id,title,body,tags
3958948,Path Independence of Line Integrals,"The Cauchy-Goursat theorem states that if a function $f$ is analytic on, and in the interior of, a contour $C$ then: $$\int_{C} f(z) \hspace{1mm} dz = 0.$$ From here, a lot of lecture notes deduce the following. Cor. Suppose $f$ is analytic on some simply connected region $A$ . Then for any two points $a, b \in A$ we have that $$\int_{C} f(z) \hspace{1mm} dz$$ is independent of the contour $C$ in $A$ joining $a$ and $b$ . However, the 'proof' often only shows $$\int_{C_1} f(z) \hspace{1mm} dz = \int_{C_2} f(z) \hspace{1mm} dz.$$ when $C_1$ and $C_2$ are non-intersecting (because then $C := C_1\cup C_2$ is a closed contour). What is the best way to prove independence for all $C_1$ and $C_2$ ? I can 'see' that one can just pick a third contour $C_3$ which doesn't intersect either $C_1$ or $C_2$ , and the proof follows from that. But how can I make the existence of $C_3$ rigorous without 'advanced machinery'?","['complex-analysis', 'contour-integration']"
3959044,"If $X$, $Y$, and $\frac{X+Y}{2}$ are nonnegative and identically distributed, is $X=Y$ almost surely?","Suppose $X$ and $Y$ are nonnegative random variables such that the distributions of $X$ , $Y$ , and $\frac{X+Y}{2}$ are identical. Is it true that $X=Y$ almost surely? $X$ and $Y$ are not necessarily assumed to be integrable. It is crucial that they are bounded on one side, since otherwise we may sample $X$ and $Y$ independently from the Cauchy distribution. $X$ and $Y$ cannot be independent because one-sided $\alpha$ -stable distributions do not exist for $\alpha \geq 1$ . (Proved by Esseen and Ramachandran in the 1960s.) My gut feeling is that if $X$ and $Y$ cannot be independent then they cannot not be distinct at all, but this relies on how, when they are square-integrable, $Y$ can be written as a linear combination of $X$ and an independent copy of it. The proof itself is easy when we assume square-integrability: $$ \mathbb{E}\left[ (X-Y)^2 \right] = \mathbb{E}\left[X^2\right] + \mathbb{E}\left[Y^2\right] - \mathbb{E}\left[(X+Y)^2\right] = 0. $$ I do not know how to proceed without this assumption. I don't think a truncation argument would work since $(X \wedge n) + (Y \wedge n) \neq (X+Y) \wedge (2n)$ . Perhaps there is a simple counterexample I have overlooked. For context, I have been reading a paper in which appears a continuous, strictly increasing process $(X_t)_{t\geq 0}$ with $X_0=0$ the following two properties: $X_s/s \stackrel{d}{=} X_t/t$ and $X_{s+t} - X_s \stackrel{d}{=} X_t$ for all $s,t>0$ . The author claims that $\lim_{t\to 0} (X_t/t)$ exists almost surely because $X_t/t = \lim_{s\to \infty} (X_s/s)$ almost surely for all $t>0$ . (The limit as $t\to\infty$ was claimed to exist almost surely by Birkhoff's pointwise ergodic theorem, although it was not assumed in the paper that $X_1$ is integrable.) I did not quite see how this is true, though I realized that the proof will hold even without the ergodic theorem if my question is answered to be true.","['stochastic-processes', 'probability-theory', 'random-variables']"
3959068,When can the local orientation of the sphere bundle piece together?,"In Bott's ""differential forms in algebraic topology"", it is said that the orientation of a sphere bundle is equivalent to say there is an open cover $\{ U_\alpha\} $ on base space $M$ (we assume $M $ is a smooth manifold and the sphere bundle $E$ is $n$ -dimensional) and generators $\{ [\sigma_\alpha]\in H^n(E|_{U_\alpha})\} $ such that $\ [\sigma_\alpha]=[\sigma_\beta]$ in $\ H^n(E|_{U_\alpha \cap U_\beta})$ and each generates the cohomology of the fiber when restricted on each fiber. Then the caveat in the book said the cohomology class $\ [\sigma_\alpha]$ agree on the overlap doesn't mean they piece together to get a global cohomology class on $M$ . In fact, the context in the book then investigates the condition when it can piece to get a global cohomology class on $M$ : the Euler class $\ e(E) \in H^{n+1}(M)$ vanishes. But according to the Mayer-Vietoris sequence on the sphere bundle: $$\ 0\to \Omega^*(E|_{U_{\alpha\cup\beta}}\space) \to  \Omega^*(E|_{U_{\alpha}}\space)\bigoplus\Omega^*(E|_{U_{\beta}}\space) \to\Omega^*(E|_{U_{\alpha\cap\beta}}\space)\to0$$ so there is a long exact sequence of cohomology: $$...\to H^n(E|_{U_{\alpha\cup\beta}}\space) \to H^n(E|_{U_{\alpha}}\space)\bigoplus H^n(E|_{U_{\beta}}\space) \to H^n(E|_{U_{\alpha\cap\beta}}\space)\to H^{n+1}(E|_{U_{\alpha\cup\beta}}\space)\to...$$ We then put $\ [\sigma_\alpha]$ and $\ [\sigma_\beta]$ in $$\ H^n(E|_{U_{\alpha}}\space)\bigoplus H^n(E|_{U_{\beta}}\space) $$ so if cohomology class $\ [\sigma_\alpha]$ and $\ [\sigma_\beta]$ agree on the overlap, it means it lies in the kernal of the next map, so there must exist at least one cohomology class $\ [\sigma_{\alpha\cup\beta}]$ when restricted on $\ U_\alpha$ and $\ U_\beta $ is $\ [\sigma_\alpha]$ and $\ [\sigma_\beta]$ respectively. So we piece together $\ [\sigma_\alpha]$ and $\ [\sigma_\beta]$ with no condition. In fact, if the manifold M is compact, we can repeat this procedure to get a global cohomology class on E. It means that we can always piece together them on a compact manifold. But the Euler class of a sphere bundle on a compact manifold is not always zero. So my question is what's wrong in it? Besides, I found a similar argument in Spanier's Algebraic Topology (page 262), it said that the similar orientation class can always be pieced together. So what's the difference behind these two arguments? I'm really confusing. Thank you!","['de-rham-cohomology', 'vector-bundles', 'homology-cohomology', 'algebraic-topology', 'differential-geometry']"
3959113,"What is the difference between ""expectation"", ""variance"" for statistics versus probability textbooks?","It seems that there are two ideas of expectation, variance, etc. going on in our world. In any probability textbook: I have a random variable $X$ , which is a function from the sample space to the real line. Ok, now I define the expectation operator, which is a function that maps this random variable to a real number, and this function looks like, $$\mathbb{E}[X] = \sum\limits_{i = 1}^n x_i p(x_i)$$ where $p$ is the probability mass function, $p: x_i \mapsto [0,1], \sum_{i = 1}^n p(x_i) = 1$ and $x_i \in \text{range}(X)$ . The variance is, $$\mathbb{E}[(X - \mathbb{E}[X])^2]$$ The definition is similar for a continuous RV. However, in statistics, data science, finance, bioinformatics (and I guess everyday language when talking to your mother) I have a multi-set of data $D = \{x_i\}_{i = 1}^n$ (weight of onions, height of school children). The mean of this dataset is $$\dfrac{1}{n}\sum\limits_{i= 1}^n x_i$$ The variance of this dataset (according to "" science buddy "" and "" mathisfun dot com "" and government of Canada ) is, $$\dfrac{1}{n}\sum\limits_{i= 1}^n(x_i - \sum\limits_{j= 1}^n \dfrac{1}{n} x_j)^2$$ I mean, I can already see what's going on here (one is assuming uniform distribution), however, I want an authoritative explanation on the following: Is the distinction real? Meaning, is there a universe where expectation/mean/variance... are defined for functions/random variables and another universe where expectation/mean/variance... are defined for raw data? Or are they essentially the same thing (with hidden/implicit assumption) Why is it no probabilistic assumption is made when talking about mean or variance when it comes to dealing with data in statistics or data science (or other areas of real life)? Is there some consistent language for distinguishing these two seemingly different mean and variance terminologies? For example, if my cashier asks me about the ""mean weight"" of two items, do I ask him/her for the probabilistic distribution of the random variable whose realization are the weights of these two items ( def 1 ), or do I just add up the value and divide  ( def 2 )? How do I know which mean the person is talking about?/","['statistics', 'soft-question', 'probability-theory', 'probability', 'terminology']"
3959122,Find $\int_0^\infty \int_0^t e^{-t}\frac{\sin(\tau)}{\tau}d\tau dt$ using laplace transform.,"Find $\int_0^\infty \int_0^t e^{-t}\frac{\sin(\tau)}{\tau}d\tau dt$ using laplace transform My attempt:
I know that $\int_0^t f(\tau)d\tau=\mathscr{L}^{-1}(\frac{F(s)}{s}).$ Consider the integral $\int_0^\infty \int_0^t e^{-t}\frac{\sin(\tau)}{\tau}d\tau dt=\int_0^\infty  e^{-t} \int_0^t\frac{\sin(\tau)}{\tau}d\tau dt.$ Now let $I=\int_0^t\frac{\sin(\tau)}{\tau}d\tau=\mathscr{L}^{-1}(\frac{F(s)}{s}).$ Here $F(s)=\int_s^\infty \frac{1}{1+u^2}du= 1-tan^{-1}(s) (\because \mathscr{L}(\frac{f(t)}{t})=\int_s^\infty  \mathscr{L}(f(t))(u) du.$ Hence $I=\int_0^t\frac{\sin(\tau)}{\tau}d\tau=\mathscr{L}^{-1}(\frac{1-tan^{-1}(s)}{s})=\mathscr{L}^{-1}(\frac{1}{s}-\frac{tan^{-1}(s)}{s})=1-\mathscr{L}^{-1}(\frac{tan^{1}(s)}{s}).$ $\mathscr{L}^{-1}(\frac{tan^{1}(s)}{s})=\int_0^ t \mathscr{L}^{-1}(tan^{1}(s))(u)du.$ I don't know how to find $\mathscr{L}^{-1}(tan^{1}(s)).$ Is there any shorter way to evaluate the above iterated integral using laplace transform?","['integration', 'laplace-transform']"
3959134,A family of sets whose outer and inner measures are equal is a $\sigma$-algebra,"(Problem)
Let $(\Omega, \mathcal{F}, P)$ : a probability space, and $\mathcal{A}$ : algebra which satisfies $ \sigma (\mathcal{A})=\mathcal{F}$ . And we difine $\tilde{P}$ as $$
\tilde{P}(A) = \inf \left\{ \sum_{i=1}^{\infty} P(A_i) \mid A \subset \cup _{i=1}^{\infty}A_i , A_i \in \mathcal{A} \right\}.
$$ We consider a class of sets $\mathcal{G}$ s.t. $$
\mathcal{G} = \{A\in\mathcal{F} \mid \tilde{P}(A) = P(A), \ \tilde{P}(A^{\text{c}}) = P(A^{\text{c}}) \}.
$$ Prove $\mathcal{G} = \mathcal{F}$ . (Question)
I managed to show $\mathcal{A} \subset \mathcal{G}$ , so I next tried to show $\mathcal{G}$ is a $\sigma$ -algebra, where I am stuck. I don't see why
a countable union of elements in $\mathcal{G}$ is also an element in $\mathcal{G}$ . How can I solve this? I looked up in some textbooks and found this might have a relationship to completion of a probability space. Thank you in advance.","['measure-theory', 'outer-measure', 'probability-theory', 'probability']"
3959182,Differents ways to evaluate the sum $\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}$,"Evaluate $$\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}$$ My approach: Let $$x=\sqrt{12+ \sqrt{12+\sqrt{12+\cdots}}}$$ so, we have that $$\sqrt{12+\sqrt{12+\sqrt{12+\sqrt{12+\cdots}}}}\iff \sqrt{12+x}=x \implies 12+x=x^{2} \iff (x+3)(x-4)=0$$ So, the answer is $\boxed{4}$ . Is correct my solution?
Can you show other ways for to solve this problem?
Can you suggest me any  textbooks with similar problems?
Thank you so much!","['nested-radicals', 'limits', 'calculus']"
3959192,Energy distance between random variable and random sample,"If $X$ and $Y$ are independent RVs we can expect $\mathbb{E}|X-Y|$ to be a measure of how ""far apart"" the variables are, but it doesn't work like a metric in the sense that its minimum is positive, attained when $Y=X'$ is an iid RV. But we can actually offset the expectation $\mathbb{E}|X-Y|$ accordingly - that is, if we pick $X'$ and $Y'$ to be iid with $X$ and $Y$ then $$ d(X,Y)^2=2\mathbb{E}|X-Y|-\mathbb{E}|X-X'|-\mathbb{E}|Y-Y'| $$ which apparently equals $$ =2\int_{\mathbb{R}} \big(F_X(t)-F_Y(t)\big)^2\,\mathrm{d}t $$ and $d(X,Y)$ is called the energy distance which measures how close $X$ and $Y$ are in distribution. Given a random variable $X$ , we can define an energy function $E(x_1,\cdots,x_n)$ to be (half the squared) energy distance between $X$ and the discrete uniform distribution on the set $\{x_1,\cdots,x_n\}$ . Then  given a random sample, i.e. $n$ RVs $X_1,\cdots,X_n$ which are iid with $X$ , we can define the energy variable $E=E(X_1,\cdots,X_n)$ , which is itself an RV. Question . Do we know anything about how $E$ is distributed? Can we calculate it explicitly, or any nice statistics (like mean, median, etc.) for particular RVs $X$ ? What about good estimates? Presumably as $n\to\infty$ we have $E\to0$ in some sense, can we describe how fast and in what sense $E$ converges to $0$ (in terms of $X$ 's distribution)? If $U$ is the uniform distribution on $[0,1]$ I calculated $$ E(u_1,\cdots,u_n) = \frac{1}{12n^2}+\frac{1}{n}\sum_{k=1}^n\left(u_k-\frac{2k-1}{2n}\right)^2 $$ where $0<u_1<\cdots<u_n<1$ . This tells us $E_{\mathrm{min}}=\frac{1}{12n^2}$ is attained when $u_k=\frac{2k-1}{2n}$ , and possibly $E_{\mathrm{max}}$ is attained when $u_k=1$ , though it seems too difficult to find $F_E(e)$ for the median, although the mean looks calculable, albeit with a pretty large formula. Edit . Intuitively, a point-set being in a narrower range should make $E$ larger, so we should expect $E_\mathrm{min}$ to not occur at a boundary point, so we can find it by solving $\nabla E=0$ , which occurs at the vector $\vec\alpha$ for which $F_X(\alpha_k)=\frac{2k-1}{2n}$ for $k=1,\cdots,n$ . Then we may integrate $\nabla E$ from $\vec\alpha$ to $\vec x$ to obtain a generalized energy formula: $$ E-E_{\mathrm{min}}=\frac{2}{n}\sum_{k=1}^n \int_{\alpha_k}^{x_k}(x_k-u)f_X(u)\,\mathrm{d}u. $$ Plugging order statistics $X_{(k)}$ into this doesn't seem very amenable but it's a nice formula.","['statistics', 'probability-theory']"
3959198,What is the number of relations on a set with n elements that are not antisymmetric?,"My attempt $A=\{1,2\}$ $$AÃ—A={(1,1),(1,2),(2,1),(2,2)}$$ The total antisymmetric relations on $AÃ—A$ is $12$ And remaining 4 are not antisymmetric.
Is it mean if on a particular set if some relations are antisymmetric then the remaining would be not antisymmetric?
A set with 3 elements have 216 antisymmetric relations.is it mean the remaining 296 are not antisymmetric?
Antisymmetric relations can also be find from $$2^n \times 3 ^{n(n-2)\over 2}$$","['relations', 'discrete-mathematics']"
3959214,Find missing angle in triangle,"In this triangle, we are given that $AB=AD=CE$ , $BE=CD$ and angle $ECD=2*AED=2a$ . We are asked to find $AED=a$ . I have seen several problems similar to this but can't follow any of the solution methods.
The only thing which is obvious to me is that since $AD+CD = BE+EC$ , the triangle is isosceles. Therefore its base angles (the ones I have drawn in green) are equal.
Also triangle ABD is isosceles, therefore its base angles are equal. I don't see any other relation to use. (FYI this is not homework or anything - just practice and self-motivation).
Thank you very much. EDIT: If this is of any help, through Geogebra I get that $a=10^{\circ}$ , therefore $ECD=20^{\circ}$ and we have a 80-80-20 triangle.",['geometry']
3959243,Darboux theorem for the symplectisation of a contact manifold,"I'm wondering if there is some ""nice"" version of Darboux's theorem that can be applied in the case of a symplectisation of a contact manifold $(X,\alpha)$ with the canonical symplectic form. i.e. $(\Bbb R \times X, d(e^t\alpha))$ , where $t$ is the $\Bbb R$ -parameter. More specifically, I'd like to know if we can assume that the first 2 coordinates in a Darboux neighbourhood are given by the Reeb vector field and the canonical Liouville vector field $\partial_t$ . I tried looking around but wasn't really able to find anything","['symplectic-geometry', 'contact-topology', 'differential-geometry']"
3959244,Conservative force: path-independent integration implies force equal to gradient,"Following the wikipedia page of a Conservative force , I want to prove that given a force such that for any closed path $C$ we have $$
\oint_C \mathbf{F}\cdot \mathbf{r}=0
$$ there is a potential $U(\mathbf{r})$ such that $$
\mathbf{F}=-\nabla U(\mathbf{r}).
$$ Proof. The webpage start by defining $$
U(\mathbf{r}):=-\int_{c}\mathbf{F}\cdot d \mathbf{r}
$$ and conclude that the claim follows from the fundamental theorem of calculus. I want to deepen the last step because the derivation is far from obvious for me. First I adapt the definition of potential by choosing some zero potential point $\mathbf{x}_0$ : $$
U(\mathbf{r}):=-\int_{\mathbf{x}_0}^\mathbf{x}\mathbf{F}\cdot d \mathbf{r}
$$ then I introduce a parametrisation of the path $$
\mathbf{s}(u),\quad u\in[0,1]
$$ Thus we get $$
-\nabla \left( -\int_{c}\mathbf{F}\cdot d \mathbf{r} \right)
=\nabla\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du
$$ For the $x$ component, intuitively but with a lot of errors, we must have something like this $$
\frac{d}{dx}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du
=\frac{du}{dx}\frac{d}{du}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du\\
=\frac{du}{dx}\left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)_{\mathbf{x}_0}^\mathbf{x}=\cdots=F_x
$$ How does the derivation looks like using formal mathematics, but remaining in calculus? EDIT My solution going back to the definition of derivative:","['integration', 'multivariable-calculus', 'calculus', 'vector-analysis']"
3959362,"If $f: [0, 1] \to \mathbb R$ is defined by $ð‘“(ð‘¥) = ð‘¥ ^2$, check $f$ is bijective or not?how?","If $f:  [0, 1] \to \mathbb R$ is  defined by $ð‘“(ð‘¥) = ð‘¥ ^2$ ,   check $f$ is bijective or not?how?
I don't think it is onto because 6 doesn't have any pre image but 6 belongs to R",['discrete-mathematics']
3959374,Permutations and number of lists,"How many permutations $a_1, a_2, a_3, a_4$ of $1, 2, 3, 4$ satisfy the condition that, for $k = 1, 2, 3$ , the list $\{a_1, \dots, a_k\}$ has a number greater than $k$ ? I am not sure if I understand what the question is asking. Probably not, since my answer is wrong. What I found is: For $k = 1$ , $\{2\},\{3\},\{4\}$ for $k = 2$ , $\{1,3\}, \{1,4\}, \{2,3\}, \{2,4\}, \{3,4\}$ for $k = 3$ , $\{1,2,4\}, \{1,3,4\}, \{2,3,4\}$ That is $11$ , but the answer is $13$ .","['permutations', 'combinatorics']"
3959407,A limit $\lim_{x \to 0} \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2} = \frac12$ and continuity of Kolmogorov distribution.,"Problem: prove that $$\sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2} \to \frac12, x\to 0,$$ not using theta-function. Motivation: Kolmogorovâ€“Smirnov statistic is well known. It's limit distribution has a continious distribution function $F(x)$ such that $F(x) = 0$ for $x \le 0$ and $$F(x) = 1 - 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2}  = \frac{\sqrt{2\pi}}{x} \sum_{k=1}^{\infty} e^{-\frac{(2k-1)^2 \pi^2}{8x^2}}$$ otherwise. The equivalence of two expressions $F_1(x) = 1 - 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2}$ and $F_2(x) = \frac{\sqrt{2\pi}}{x} \sum_{k=1}^{\infty} e^{-\frac{(2k-1)^2 \pi^2}{8x^2}}$ follows easily from transformation formula for theta-functions. Using $F_2(x)$ it's easy to prove that $F(x)$ is continuous for all $x$ . Using $F_1(x)$ it's easy to prove that $F(x)$ is continuous for all $x \ne 0$ . My question was the next one: is there any simple way to prove that $F(x) $ is continuous at $x=0$ , using the definition with $F_1(x)$ ? As $F_1(x)$ is even then the statement "" $F_1(x) \to 0, x \to 0+$ "" is equivalent to the statement "" $F_1(x) \to 0, x \to 0$ "". So, my question is: is there any simple way (without theta-functions) to prove that $$\sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2} \to \frac12, x\to 0?$$ What do I know? I know that the statement $\sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 x^2} \to \frac12, x\to 0$ follows from properties of theta-functions, but the question is about simple methods. I thought about the next idea: if $x=0$ , then we have divergent series $S = 1 - 1 + 1 - 1 + 1 - \ldots = 1 - S$ . It's a wrong way, but in some sense it's natural to write $S= 1 -S$ and hence $S = \frac{1}2$ . Unfortunatelly, $S$ doesn't exist.","['calculus', 'statistics', 'real-analysis']"
3959471,Visualising the Minkowski sum of two triangles,"Consider the figure given below. I am told that the Minkowski sum of the two triangles $A,B \subset \mathbb{R}^2$ is the regular hexagon, as shown right next to them. The Minkowski sum is defined as $$A+ B = \{a+b: a\in A, b\in B\}$$ Note that $A$ and $B$ are such that $A$ is the reflection of $B$ about the origin (and vice versa). So, knowing that the sum is a regular hexagon , I can somewhat make sense of it, but I'm not really able to see why it is a regular hexagon in the first place! Is there an intuitive way of looking at this? Finding $A+ B = \{a+b: a\in A, b\in B\}$ manually using every $a\in A, b\in B$ is obviously impossible, so there must be some special $a_1,a_2,...,a_p \in A$ and $b_1, b_2,...,b_q \in B$ we should use to construct the figure (extreme cases of some sort). Which ones are these? As an addendum, I am wondering if there is some general intuition (at least in $\mathbb{R}$ , $\mathbb{R}^2$ , and $\mathbb{R}^3$ ) I can carry with me, when finding the Minkowski sum of two figures (sets)? P.S. I am not looking for algorithms to find Minkowski sums.","['convex-geometry', 'geometry']"
3959506,Verifying the hyperbolic law of cosines and law of sines..,"I am trying to understand hyperbolic triangles. These two laws are supposedly universal in the hyperbolic plane. However, whenever I've tried to verify them using applets which let me construct hyperbolic triangles either in the PoincarÃ© disk model, or the PoincarÃ© half-plane model, they never seem to be valid. The construction below serves as an example: $\sin(72.60867Â°) / \sinh(1.07)$ $=$ $0.74195\dots $ whereas $\sin(32.02868Â°) / \sinh(0.84)$ $=$ $0.56280\dots $ Similarly, you will find an inequality when you try to verify the cosine law. Have I fundamentally misunderstood something? Why do these laws never seem to hold up on any triangle?","['trigonometry', 'hyperbolic-geometry', 'triangles', 'hyperbolic-functions']"
3959508,"For which $p$, does there exist an anti-derivative?","For positive integer $p$ , consider the holomorphic function $f(z)=\frac{\sin (z)}{z^p}$ , for $z\in \mathbb{C}\setminus \{0\}$ . For which  values of $p$ , does there exist a holomorphic function $g:\mathbb{C}\setminus \{0\}\to \mathbb{C}$ such that $g'(z)=f(z)$ for $z\in 
 \mathbb{C}\setminus \{0\}?$ $(1)$ All even integers . $(2)$ All odd integers. $(3)$ All multiples of $3$ . $(4)$ All multiples of $4$ . My thinking :- I am aware of the fact ""Every analytic function on a simply connected domain has a primitive (anti-derivative)"" Here , the domain is not simply connected and I am not sure how else to proceed. Please give a hint. Thank you.","['integration', 'cauchy-integral-formula', 'complex-analysis', 'derivatives', 'analytic-functions']"
3959538,Asymptotic behaviour of quantity related to an ODE,"Consider the differential equation $$u''(t) + u(t) + u^3(t) = f(t) u(t) + g(t),\,\, u(0) = u_0, \,u'(0) = u_1$$ where $u :\mathbb{R}^+ \to \mathbb{R}$ is a bounded solution  and $f, g: \mathbb{R}^+ \to \mathbb{R}$ are given continuous functions such that $$\lim_{t\to \infty} f(t) = \lim_{t\to \infty} g(t) = 0.$$ Define the quantity $E(t) := \frac 12 (u(t))^2 +\frac 12 (u'(t))^2 + \frac 14 (u(t))^4.$ I want to study the asymptotic of $E$ when $t \to \infty.$ First, I considered many examples for $f, g$ using mathematica (See the attached images where I plotted $t\mapsto E(t)$ ). I'm interested in the case where $\liminf_{t\to \infty}E(t) >0$ or roughly $\lim_{t\to \infty} E(t) >0.$ I wonder if this is the general case or it depends on the initial values $u_0, u_1$ and $f,g.$ In all the examples that I tested numerically, I found that $\lim_{t\to \infty} E(t) >0.$ My attempt started by differentiating $E$ with respect to $t.$ It seems that in the case where $u'$ is bounded, we have $$\lim_{t\to \infty} E'(t) =0,$$ since $E' = (f u + g) u'.$ But this does not imply necessarily that $\lim_{t\to \infty}E(t) >0.$ The limit might be zero. Thank you for any hint.","['asymptotics', 'ordinary-differential-equations']"
3959559,5 digit numbers from 1 3 3 0 0,"How many 5 digit number positive integers can be made from the following numbers: $1,3,3,0,0$ ? First I calculate all possible combinations $$\frac{5!}{2!2!}=30$$ Then I need to remove all 5 digit numbers startting with $0$ and $00$ . They are not 5 digit numbers positive integers. Starting with $00$ $$\frac{3!}{2!}=3$$ Starting with $0$ $$\frac{4!}{2!}=12$$ $12-3=9$ , else I would remove numbers starting with $00$ double. $30-3-9=18$ Is $18$ the correct answer, or where did i thought wrong?","['permutations', 'combinatorics', 'discrete-mathematics']"
3959566,Cauchy distribution won't converge to normal distribution,"Let $X_n$ be i.i.d. Cauchy sequence, $S_n:=\frac{X_1+\dots+X_n}{d_n}$ , where $\{d_n\}$ is a sequence in $\mathbb{R}^+$ . Prove that: no matter how we choose $\{d_n\}$ from $\mathbb{R}^+$ , $S_n$ will not converge to any normal r.v. $N(a, \sigma^2)$ in distribution. If we choose $d_n=\sqrt{n}$ , the convergence of $\frac{S_n}{\sqrt{n}}$ is actually the necessary condition for $X_n$ to be square-integrable. Since for Cauchy distribution $E[X_1^2]=\infty$ , $S_n$ won't converge to a normal r.v. But how to prove the situation when $\{d_n\}$ is picked randomly from $\mathbb{R}^+$ ? Thanks in advance.","['convergence-divergence', 'probability-theory', 'probability', 'normal-distribution']"
3959577,"For which $2\times 3$ matrices $A,B$ does $\operatorname{rref}(A) + \operatorname{rref}(B) = \operatorname{rref}(A+B)$ hold?","Let $A$ and $B$ be $2\times 3$ matrices. For which $A$ and $B$ does the following equation hold? $$\operatorname{rref}(A)+ \operatorname{rref}(B) = \operatorname{rref}(A+B)$$ ( $\operatorname{rref}$ is the operation that makes a matrix in reduced row echelon form .) This problem is from Strang's Introduction to Linear Algebra book, and he says about this problem in the book that, it is silly. I don't know why he said that but I couldn't figure this out for a while. I'd appreciate your help.
Thanks from now.","['matrices', 'linear-algebra']"
3959580,$\lim_{n\to\infty} \sqrt[n(n+1)]{ \prod_{r=0}^n {n\choose r}} $ [duplicate],"This question already has answers here : Limit involving binomial coefficients: $\lim_{n\to\infty}\left(\binom{n}{0}\binom{n}{1}\dots\binom{n}{n}\right)^{\frac{1}{n(n+1)}}$ (4 answers) Closed 3 years ago . I have been trying to figure out the following limit: $$\lim_{n\to\infty} \sqrt[n(n+1)]{\prod_{r=0}^n {n\choose r}}=\mathscr L$$ Itâ€™s quite natural to take the natural log of both sides: $$\ln \mathscr L = \lim_{n\to\infty} \frac{\sum_{r=0}^n \ln {n\choose r}}{n(n+1)} \\ =\lim_{n\to\infty} \frac{(n+1)\ln(n!) - 2\sum_{r=0}^n \ln(r!)}{n(n+1)}\\  =\lim_{n\to\infty} \frac{(n+1)\ln(n!) - 2\ln(\color{green}{n! \times (n-1)! \times (n-2)! \dots \times 2!\times 1!})}{n(n+1)}$$ I believe the green term is called the superfactorial of $n$ , but I barely know anything about it. The last thing I tried was to write $$n! \times (n-1)! \times (n-2)! \dots 2!\times 1! =n^1 \times (n-1)^2 \times (n-2)^3 \dots \times 2^{n-1} \times 1^n = \prod_{r=1}^n r^{n+1-r}$$ but that doesnâ€™t help. Any ideas on how to evaluate this limit?","['limits', 'calculus', 'binomial-coefficients', 'factorial']"
3959604,Subsets - Ranking / Unranking,"I am looking at the following codes: It is lexicographic order related to ranking and unranking. Here is also an example: There is also the Gray code: with the repective examples: I haven't really understood the ranking and the unranking. So we have a set and we want to find all the subsets  either with the lexicographic oder or with Gray code and then we search for a specific subset and the position is the rank? $$$$ EDIT: The subset $T \subseteq S$ can de described by the vector $x(T)$ of length $n$ where at each position there is $1$ if the respective element is contained, $0$ else. As for Gray Code : It is defined recursively as follows. We start with $G^1=[0,1]$ . in each of the next levels we take the previous one with a $0$ at the beginning and we add the previous one reversely with $1$ at the beginning, e.g. $G^2=[00,01,11,10]$ , etc. For the algorithms ranking and unranking we consider the relation between Gray Code and the binary representation. The bits $b_i$ of Gray Code is $1$ if at the binary representation the bits $b_i$ and $b_{i+1}$ are different. For the ranking we take the bits from the most important to the less important and where the respective element that we check contains in the subset we reverse the bit b (initially $0$ ). At each bit that is one we augment also the rank. For the unranking we take the binary representation of rank and we add to the subset the elements for those that the consecutive bits were different.","['gray-code', 'combinatorics', 'computer-science']"
3959623,"Solve equation $\cos (a)\cos(b)\cos(a+b) = -\frac18$ over $0 < a,b < \frac\pi2$","I have tried this question as follows: $$4[2(\cos(a)\cos(b))\cos(a+b)] = -1$$ Then converted the $2\cos(a)cos(b)$ in $\cos(a+b) + \cos(a-b)$ . And so on but this got messy and big. Another approach that I tried was that as $a$ and $b$ lie in the first quadrant then their cosine is positive this implies $\cos(a+b)$ is negative. And without losing generality we can say each term be equal to 1/2 which implies $a = b = Ï€/3$ . I googled it but didn't find any solution. Wolfram alpha after converting into the same thing as I have converted used complex numbers which was neat, but my question is:  can we solve this with a short method (using trigonometry)?",['trigonometry']
3959685,a Poset Over a Set of Partial Orders,"I've been scratching my head at this for quite a while, the question is as follows: (pardon my weird wording, I'm not studying math in English so I might get some proof wording conventions wrong) let $X$ be a set, and $\mathcal{R}$ be the set of all non-strict partial orders over set $X$ . let $\trianglelefteq,\leqslant \in \mathcal{R} $ We'll define Partial order $\sqsubseteq$ over $\mathcal{R} $ as follows: $\leqslant \sqsubseteq \trianglelefteq \ \Leftrightarrow \forall x,y\in X\ \left( x\leqslant y\ \rightarrow x\trianglelefteq y\right)$ is $(\mathcal{R},\sqsubseteq)$ a poset? The first thing that baffled me is that if I want to disprove this, I have to give a counterexample, but to do that, I need to give a partial order over $X$ , only I don't know what $X$ is. On the other hand, to prove that this is a poset, I've tried showing that it's antisymmetric to no avail, and got stuck on that front.
for example: $let\,\, R_1,R_2\in\mathcal{R}\,    such\  that\,     R_{1} \sqsubseteq R_{2}$ By the assumption, for all $x,y$ the condition is satisfied, in particular for $\hat{x} ,\hat{y} \in X\ \left(\hat{x} R_{1}\hat{y} \ \rightarrow \hat{x} R_{2}\hat{y}\right)$ then in order for $R_{2} \sqsubseteq R_{1}\,$ to also occur, the condition must also apply for all $x,y\,$ , in particular $\hat{x} R_{2}\hat{y} \ \rightarrow \hat{x} R_{1}\hat{y}$ But this is where I got stuck, I can't seem to get to a conclusion from this point. Much appreciated","['order-theory', 'discrete-mathematics']"
3959690,Do you know this function?,I'm looking for a real-valued function with the following properties: $\lim \limits_{x \to 0} f(x) = 1$ $\lim \limits_{x \to 0^+} f'(x) = -\infty$ $\lim \limits_{x \to \infty} f(x) = 0$ and $0<f(x)\leq 1$ always $1-\sqrt{x}$ fits the first two criteria and may just need the right factor. I would also appreciate ideas on how to approach problems like this generally. Thanks in advance.,"['limits', 'functions', 'real-analysis']"
3959724,How to integrate $\int_0^1\frac{dx}{1+x+x^2+\cdots+x^n}$,"I am interested in finding a solution to the integral $$I_n=\int_0^1\frac{dx}{\sum_{k=0}^nx^k}$$ Since the denominator is a geometric series with $a=1$ and $r=x$ and it is within the radius of convergence, we should be able to say $$\sum_{k=0}^nx^k=\frac{1-x^{n+1}}{1-x}=\frac{x^{n+1}-1}{x-1}$$ and $$I_n=\int_0^1\frac{x-1}{x^{n+1}-1}dx$$ It makes sense to me that, for all values of $n$ , $I_n$ is convergent since the bottom of the function is always above zero and the integral exists for $n\to\infty$ however I cannot seem to find a nice closed form for this. One thought I did have was using: $$\sum\ln(x_i)=\ln\left(\prod x_i\right)$$ but I cannot seem to make it work. Does anyone have any hints for this type of problem as I would like to try and complete it myself. Thanks :)","['integration', 'definite-integrals', 'closed-form']"
3959729,Show that $R_{w}$ is bounded/compact using no arbitrage or non-redundancy arguments,"Consider a financial market with $d+1$ assets in a one-period model. The $0-$ th asset is considered the risk-free asset, the others are risky. The vector $\overline{\pi} \in \mathbb R^{d+1}$ denotes the initial price vector at $t=0$ , and the random vector $\overline{S} \in \mathbb R^{d+1}$ denotes prices at $t=1$ . Further we have $\pi^{0}=1,\; S^{0}=1+r$ , as the $0-$ th asset is risk-free. Here a strategy $\overline{\xi}\in \mathbb R^{d+1}$ is called an arbitrage opportunity if $\overline{\xi}â‹…\overline{\pi}â‰¤0$ but $\overline{\xi}â‹…\overline{S}\geq 0$ and $\mathbb P(\overline{\xi}â‹…\overline{S}> 0)>0$ . Further the market model is called non-redundant if $\overline{\xi}\cdot \overline{S}=0 \; \;\mathbb P\text{-a.s.}\implies \overline{\xi}=0$ . By the Law of One price we know that in an arbitrage-free model, for any $\overline{\rho},\; \overline{\xi}$ such that $\overline{\xi}\cdot\overline{S}=\overline{\rho}\cdot\overline{S}$ a.s. it must follow that $\overline{\xi}\cdot\overline{\pi}=\overline{\rho}\cdot\overline{\pi}$ . Now as in https://quant.stackexchange.com/q/17839/42184 we need to show that for $w>0$ , $$\mathcal{R}_{w}:=\{\overline{\xi}\in \mathbb R^{d+1}:\overline{\xi}\cdot \overline{\pi}=w,\;\overline{\xi}\cdot\overline{S}\geq 0\; \text{a.s.}\}\;\; \text{ is compact} $$ As we are in finite-dimensions, I assume that we need to use $\text{compact}\iff \text{closed and bounded}$ . Closedness is no problem at all, using convergence arguments. However, I am struggling to show boundedness. As I have not used the no arbitrage arguments or the non-redundancy condition, I assume that I will need to use them in my argument. Any ideas?","['real-analysis', 'convergence-divergence', 'probability-theory', 'probability', 'compactness']"
3959747,Identification of ends of Bruhat-Tits Tree,"I am trying to understand why a canonical identification exists between the ""ends"" or ""rays"" of the Bruhat-Tits tree defined on $\mathbb{Z}_p$ -lattices (with metric invariant under the action of $PGL(2,\mathbb{Q}_p)$ ) and $\mathbb{P}^1(\mathbb{Q}_p)=\mathbb{Q}_p\cup\{\infty\}$ . I have consulted several online references on the Bruhat-Tits tree, all of which more or less avoid constructing the actual identification. Intuitively, I see that since any vertex in the tree has $p+1$ neighbors, a path that travels outwards without backtracking should correspond to a $p$ -adic number - for instance, if $p=2$ , you could label a path and end up with some binary string such as $0101010011...$ , which can be converted into a $2$ -adic by using the 0s and 1s as coefficients of a formal Laurent series. I am unsure, however, of what the end that corresponds to $\infty$ should look like, as well as how the exact identification between a path and a $p$ -adic would work - given a binary string, for instance, how would we know which power of $p$ would be the starting point? I am also unsure of what the implications of there being $p+1$ choices for the first outgoing edge are. Edit: After thinking further on the matter, I've realized that the $p+1$ -th edge (i.e. the ""extra"" one at your starting point) can be used to denote elements of $\mathbb{Q}_p\setminus\mathbb{Z}_p$ , by letting ""extra edge followed by $n$ zeroes"" denote multiplication of the subsequent binary sequence by $p^{-n-1}$ . So any path that does not travel through the extra edge initially simply belongs to $\mathbb{Z}_p$ , and I can identify $\infty$ with the path that goes through the extra edge and is followed by infinite 0s (since this one seems to be degenerate/lack interpretation via the above). Feedback on this identification and whether it is correct/can be made more precise would be appreciated.","['bruhat-tits-theory', 'algebraic-number-theory', 'group-theory', 'p-adic-number-theory']"
3959757,"Visual intuition for the definition of ""asymptotically equivalent""","I'm trying to intuitively grasp the following definition: The real-valued functions $f$ and $g$ are asymptotically equivalent as $x \to \infty$ if $$\lim_{x \to \infty} \dfrac{f(x)}{g(x)}=1.$$ We write this as $f \sim g$ . My question is: how do we visually interpret this in terms of the graphs of $f$ and $g$ ? Does this mean that the graphs of $f$ and $g$ get closer to each other as $x$ gets larger and larger? My only intuition for this comes from the following example: we know that $\sin x \sim x$ as $x \to 0$ (since $\lim_{x \to 0} (\sin x)/x = 1$ ). And as we can see below, the graphs of $\sin x$ (the green line) and $x$ (the black line) get closer and closer as $x$ goes to $0$ . But this intuition does not seem to hold for functions asymptotically equivalent at $\infty$ . I graphed $x^2 + x$ (black line) and $x^2$ (green line) and their graphs do not appear to be getting closer at all! In fact, it looks like there's a ""gap"" between the two graphs. This leads me to believe that I'm not interpreting ""asymptotically equivalent"" in the right way. I've come across the idea that $f \sim g$ means that $f$ and $g$ have the ""same rate of growth"", but that feels very unintuitive for me. Is there are a way to see that in the graphs? Any guidance would be greatly appreciated! Thanks.","['asymptotics', 'analysis', 'real-analysis']"
3959768,How to show that $f'(x)<2f(x)$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question This is question B4 in 1999 Putnam Exam. I would appreciate if somebody could help me with this. Let $f(x),f'(x),f''(x),f'''(x)>0$ , $f'''(x)$ is a continuous function and $f'''(x)<f(x)$ on $\mathbb{R}$. Then show that
  $$f'(x)<2f(x),~ \forall x\in \mathbb{R}.$$","['inequality', 'derivatives', 'real-analysis']"
3959775,Bound for order of a group depending on number of elements of maximal order,"In a paper On the Number of Elements of maximal order in a Group it is proven that an arbitrary group $G$ with a finite number of elements of maximal order has bounded size. Namely: $$|G|\leq\frac{mk^2}{\varphi(m)},$$ where $m$ is the maximal order and $k$ the number of elements that have order $m$ . I wanted to characterize all groups $G$ , where the limit is sharp, i.e. $|G|=\frac{mk^2}{\varphi(m)}$ . Using GAP I found all groups with this property up to order 1023 and was able to state a conjecture. It is easy to see in the paper, that a group has this property only if all elements of maximal order are conjugated. So we need this as as a requirement. I wanted to prove the following conjecture, but missing some tiny part. Maybe someone knows a way, I would be really happy. Conjecture. Let $G$ be a group with $k<\infty$ elements of maximal order $m$ , in which all elements of maximal order are conjugated. Then the following are equivalent. $i)$ $|G|=\frac{mk^2}{\varphi(m)}$ $ii)$ $k=\varphi(m)$ $iii)$ $G$ has a unique subgroup of order $m$ $iv)$ $C_m \cong C_G(x)=C_G(y)\trianglelefteq G$ for all $x,y\in G$ with maximal order Proof. $i) \implies ii)$ This is the part, I could not prove : I only could prove, that all elements of order $m$ commute: Let $C_G(x)$ be the stabilizer of an element of maximal order. Orbit-Stabilizer-Theorem tells us, that $|C_G(x)|=\frac{mk}{\varphi(m)}$ . Assume there exists an element of order $m$ , not contained in $C_G(x)$ . $\langle x \rangle$ operates via left-multiplication on $C_G(x)$ . $C_G(x)$ is partitioned into $\frac{|C_G(x)|}{m}$ orbits. According to Lemma 3 of  the paper linked above, in each orbit exist at least $\varphi(m)$ elements of order $m$ , i.e. in $C_G(x)$ exist at least $\varphi(m)\frac{|C_G(x)|}{m}$ elements of order $m$ . Our assumption tells us $\varphi(m)\frac{|C_G(x)|}{m} < k$ , which leads to the contradiction $|C_G(x)| < \frac{mk}{\varphi(m)}$ . It follows that all elements of order $m$ commute. This is where I can't proceed further. Maybe someone has an idea? $ii) \iff iii)$ If $k=\varphi(m)$ , an element of order $m$ generates a cyclic subgroup which contains $\varphi(m)$ elements of order $m$ , that all generate this subgroup. So there can't be other elements of order $m$ in different subgroups. Otherwise, if there is only one cyclic subgroup of order $m$ , then it contains $\varphi(m)$ elements of order $m$ , no additional elements of order $m$ can exist, as they would generate a second cyclic subgroup of order $m$ . $iii) \implies iv)$ Let $Z$ be the unique subgroup of order $m$ and $X=\{x_1,\dots,x_k\}$ the set of elements of order $m$ . As all $x\in X$ generate $Z$ , $Z$ must be contained in all centralizers of elements in $X$ . Note that $G$ operates on itself via conjugation. Orbit-Stabilizer-Theorem tells us for $x \in X$ : $$|G|=|^Gx||G_x|=k|G_x|=\frac{mk^2}{\varphi(m)}=mk$$ This follows as all elements of order $m$ are conjugated and $k=\varphi(m)$ holds. It follows, that $|G_x|=m$ , which leads to $G_x=Z\cong C_m$ for all $x \in X$ . For the normal subgroup part, note that $\phi(x_i)=x_j$ for an inner automorphism $\phi$ and $i,j\in \{1,\dots k\}$ . Let $y \in Z$ arbitrary, then $y=x_1^\alpha$ for $\alpha \in \mathbb{N}$ . Let $\phi$ be an arbitrary inner automorphism. It follows that there is a $i \in \{1,\dots k\}$ with $$\phi(y)=\phi(x_1^\alpha)=\phi(x_1)^\alpha=x_i^\alpha \in Z$$ It follows that $Z$ is invariant under inner automorphisms, i.e. normal. $iv) \implies i)$ Orbit-Stabilizer-Theorem tells us that $|G|=|^Gx||G_x|=mk$ . As all stabilizers of elements of order $m$ are equal to the same cyclic group of order $m$ , it follows, that there exist only one cyclic group of order $m$ , it follows $k=\varphi(m)$ and $|G|=mk=\frac{mk^2}{\varphi(m)}$ . Thanks to anyone, who read till here ;) Another property, which my GAP-study suggests to be equivalent is : $v)$ $G'$ is cyclic This proof has low priority, as I first want to have my circle-implications. I guess I can show, that $G'$ is contained in the unique cyclic group $Z$ of order $m$ , by proving, that $G/Z$ is abelian. I did not succeed yet, though.","['abelian-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
3959820,"Complex-number method to minimize equilateral-triangle area inside right triangle of side lengths $2\sqrt3$, $5$, and $\sqrt{37}$","The area of the smallest equilateral triangle with one vertex on each of the sides of the right triangle with side lengths $2\sqrt3$ , $5$ , and $\sqrt{37}$ , as shown, is $\tfrac{m\sqrt{p}}{n}$ , where $m$ , $n$ , and $p$ are positive integers, $m$ and $n$ are relatively prime, and $p$ is not divisible by the square of any prime. Find $m+n+p$ . I am trying to solve this problem just with complex numbers. I let the vertices of the large triangle to be $a,b,c$ for $b=0,a=2\sqrt{3}i,c=5.$ I let the vertices of the equilateral triangle to be $z_1,z_2,z_3$ for $z_1$ along $AB$ , $z_2$ along $AC$ , and $z_3$ along $BC$ . In the first place, it is well-known that $z_1^2+z_2^2+z_3^2=z_1z_2+z_2z_3+z_3z_1.$ It is futile to consider the equations for $A,Z_1,B$ and $B,Z_3,C$ to be collinear because we set those as the imaginary and real axes. With the equation for $A,Z_2,C$ to be collinear, we have $$\frac{z_2-a}{z_2-c}=\frac{\overline{z_2}-\overline{a}}{\overline{z_2}-\overline{c}}=\frac{\overline{z_2}+a}{\overline{z_2}-c},$$ with that step from $\overline{a}=-a$ and $\overline{c}=c,$ as these are along the imaginary and real axes respectively. With some manipulation, we get to $$a(z_2+\overline{z_2})+c(z_2-\overline{z_2})-2ac=0$$ $$\Longleftrightarrow a\Re(z_2)+c\Im(z_2)i=10\sqrt{3}$$ $$\Longleftrightarrow 2\sqrt{3}x_2+5y_2=10\sqrt{3},$$ for $\Re(z)=x,\Im(z)=y.$ But I'm not quite sure what to do now, because I can't think of any synthetic simplifications.","['contest-math', 'euclidean-geometry', 'geometry', 'complex-numbers']"
3959843,"Evaluate the integral $\int_0^{\pi/2}\frac{\tan x}{\tan(\frac{x}{2})} \, dx$","Evaluate the integral. $$\int_0^{\frac{\pi}{2}}\frac{\tan{x}}{\tan(\frac{x}{2})}\,dx$$ I tried to solve it with $u = \tan{x/2}$ , but i got divergent part of the solution. How can I integrate it, such that, when boundaries are plugged in, the result will be able to be calculated.","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'indefinite-integrals']"
3959846,Proving Beltrami-Enneper theorem,"Preamble I am trying to prove the Beltrami-Enneper theorem, which states that, given an assymptotic curve $\alpha$ in a surface $S$ that has non-zero curvature everywhere obeys that: $|\tau| = \sqrt{-K}$ Where $\tau$ is the torsion and $K$ is the Gaussian curvature. The conventions in the notation are $N$ is the normal to the surface $T_\alpha, N_\alpha, B_\alpha$ are thr tangent normal and binormal to the curve. $k_1, k_2$ are the principal curvatures of the surface at the evaluated point. $k$ is used to denote curvature. So far I have this: My Work The curve $\alpha$ is an assymptotic curve, which means that at $P \in \alpha$ the normal curvature is 0, so: $\langle N, kN_\alpha \rangle  = 0$ $N$ is a unit length vector orthogonal to $T_\alpha$ and $N_\alpha$ the tangent and normal vectors to the curve, so: $N = T_\alpha \times N_\alpha = B_\alpha \iff N' = \tau N_\alpha$ . So $N'\cdot N' = \tau^2$ . By Euler's theorem: $k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ Cheat This is where I am getting stuck, I took a sneak peak at the solution and it states that: $|N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ Where $\theta$ is the angle between the tangent to the curve and $e_1$ the first principal direction. I don't know how that above relationship was derived, I am looking for the proof that indeed: $|N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ As I failed to derive it on my own. Solution (with doubt) I managed to finish the proof by leveraging something I saw in the proof for Euler's theorem: By Euler's theorem: $k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ . But more importantly: $dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta)$ So $N'\cdot N' = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ By the fact the curve is assymptotic we get $N\cdot N_\alpha = 0 = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ So $\cos^2(\theta) = -k_2\sin^2(\theta) / k_1$ and $\sin^2(\theta) = -k_1\cos^2(\theta) / k_2$ . And thus: $N'\cdot N' = k_1^2\frac{-k_2\sin^2(\theta)}{k_1} + k_2^2\frac{k_1\cos^2(\theta)}{k_2} = k_1k_2\sin^2(\theta) + -k_1k_2\cos^2(\theta) = -k_1k_2$ So $\tau^2 = -k_1k_2$ I am still however not satisfied with my work, because I have no idea why this holds: $$dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta)$$","['multivariable-calculus', 'calculus', 'geometry', 'differential-geometry']"
3959912,Presentation of the symmetric group of a cartesian product of finite sets,"Let $X$ and $Y$ be finite sets. For any set $A$ , I will write $\mathfrak{S}(A)$ for its symmetric group, ie the group of bijections $A\to A$ . Then $\mathfrak{S}(X\times Y)$ is generated by the subgroups $\mathfrak{S}(X)^Y$ (the permutations which perserve each $X\times \{y\}$ ) and $\mathfrak{S}(Y)^X$ (the permutations which perserve each $\{x\}\times Y$ ). Is there a reference for finding the relations between the elements of those subgroups so that we get a presentation of $\mathfrak{S}(X\times Y)$ ? So, in other terms, a nice set of generators of the kernel of $\mathfrak{S}(X)^Y\ast \mathfrak{S}(Y)^X\to \mathfrak{S}(X\times Y)$ (where $G\ast H$ is the free product of $G$ and $H$ )? I would already be completely happy with the case $|Y|=2$ , so that the big group is $\mathfrak{S}(X\coprod X)$ , and the subgroups are the Young subgroup $\mathfrak{S}(X)\times \mathfrak{S}(X)$ and the group $(\mathbb{Z}/2\mathbb{Z})^X$ acting by exchanging corresponding elements in the two copies of $X$ . The presentation could for instance be in terms of the standard transpositions $(i,\, i+1)$ in each $\mathfrak{S}(X)$ (if we order $X$ ) and the standard basis elements of $(\mathbb{Z}/2\mathbb{Z})^X$ (so the transpositions exchanging two copies of one element). It seems to me that this should be a somewhat classical consideration in combinatorics or representation theory, but it is not that easy, and I have not been able to find any reference so far. Edit: It occurred to me that perhaps a nice way to formulate the issue is the following: given a connected simple graph with set of vertices $A$ , the transpositions corresponding to each edge generate the symmetric group $\mathfrak{S}(A)$ . How to describe the relations between those generators in terms of the geometry of the graph? This is very wide, but the question above corresponds to the graph being an $n\times m$ grid, with an edge between any adjacent vertices in the grid, vertically and horizontally.","['symmetric-groups', 'group-presentation', 'group-theory', 'combinatorics']"
3959935,Weak formulation of $\Delta^{2}u=f$ with boundary conditions $u=\Delta u =0$.,"Let $U\subset\mathbb{R}^{n}$ be an open subset (with sufficiently smooth boundary) and consider the boundary value problem $$\begin{cases}\Delta^{2}u=f \ \text{on $U$} \\ u=\Delta u=0 \ \text{on $\partial U$} \end{cases}.$$ To find a unique weak solution, I want to apply the Riesz representation theorem or Lax-Milgram theorem to an appropriate Sobolev space. My problem is finding this appropriate Sobolev space, that is, I'm trying to find the weak formulation of the above problem. I multiplied the above equation with a sufficiently smooth test function $v$ and found that \begin{align*}
\int_{U}\Delta^{2}u \cdot v&=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v-\int_{\partial U}\Delta u\cdot\frac{\partial v}{\partial n}+\int_{U}\Delta u\cdot\Delta v\\
&=\int_{\partial U}\frac{\partial\Delta u}{\partial n}\cdot v+\int_{U}\Delta u\cdot\Delta v.
\end{align*} I have seen that $H^{2}(U)\cap H_{0}^{1}(U)$ is the Sobolev space we are looking for, but how do I deduce that? Is there a general approach? Of course we need $v\in H^{2}(U)$ in order to apply $\Delta$ to it. Furthermore, it looks like we are forcing the integral on the boundary to be zero by requiring $v=0$ on $\partial U$ , i.e. $v\in H_{0}^{1}(U)$ . Also, in order to apply the above mentioned theorems, one needs $u$ and $v$ to lie in the same Sobolev space. But if $u\in H^{2}(U)\cap H_{0}^{1}(U)$ is a weak solution, then we do not have $\Delta u=0$ on $\partial U$ .","['multivariable-calculus', 'sobolev-spaces', 'partial-differential-equations', 'boundary-value-problem', 'weak-derivatives']"
3959939,Local obstruction to having a complete extension,"This question arose while I was reading Helgason's book on symmetric spaces. In chapter IV section 5, one can read the following: Let $M$ be a Riemannian manifold, $p$ a point in $M$ . In general it is impossible to find any neighborhood $N$ of $p$ which can be extended to a complete Riemannian manifold $\tilde{M}$ . To me this is very surprising. This means that there are local ""obstructions"" to the existence of a complete extension, so the completeness of a manifold $M$ impacts its local geometry. My first thought was that locally, say in a chart $B(0,\varepsilon)\subset \mathbf{R}^n\to  M$ , the metric doesn't vary to much from its value at $p$ . So maybe we can extend it to a metric on $\mathbf{R}^n$ using partitions of unity and we should be able to ask that the metric is the standard metric outside a ball $B(0,\varepsilon+r)$ . If this is possible I guess it would give a complete metric on $\mathbf{R}^n$ (here I'm probably wrong). But apparently this is not going to work. Hence my question is: What are some examples of manifolds $M$ and point $p$ of $M$ such that no neighborhood of $p$ can extend to a complete manifold ? From Helgason's book I know that locally symmetric spaces have these neighborhoods that extend to a complete manifold. Any kind of help will be greatly appreciated. EDIT : In fact the argument of partition of unity works in the $C^\infty$ category. However it doesn't work if everything is assumed to be real-analytic, which seems to be the what Helgason does implicitly. So I am still interested in a detailed example in the case where everything is real analytic.","['complete-spaces', 'geodesic', 'riemannian-geometry', 'symmetric-spaces', 'differential-geometry']"
3959975,"Do closed, connected subsets of manifolds always admit open neighborhoods to which they are homotopy equivalent?","Let $M$ be a topological manifold and $C$ a closed, connected subset of $M$ . Can we always find an open neighborhood $U$ of $C$ such that the inclusion $C \to U$ is a homotopy equivalence? My thought was to take $U$ as a union of small neighborhoods around each point in $C$ and show that $U$ deformation retracts onto $C$ . However, I don't know how to make this rigorous. If we're working with smooth manifolds and $C$ a submanifold, we can just take a disk bundle of the normal bundle. But in general, $C$ may not have such a tubular neighborhood.","['general-topology', 'algebraic-topology']"
3959981,Number of homeomorphism types of separable closed subspaces of $\beta \mathbb N$.,"When thinking about subalgebras of $\ell_\infty$ , the algebra of bounded, scalar-valued, I came across the following question related to counting subalgebras with weak*-separable dual ball obtainable by restricting to closed subsets of the spectrum of $\ell_\infty$ . Let $\beta \mathbb N$ be the ÄŒechâ€“Stone compactification of the discrete space of natural numbers. How many pairwise non-homeomorphic closed and separable subspaces does $\beta \mathbb N$ have? Such subspaces cannot be second-countable unless finite, but there are many separable closed subspaces (take the closure of any countable subset) and there is no obvious way to classify them up to homeomorphism.","['general-topology', 'compactification', 'compactness']"
3960020,How to find the generators for $y^2 = x^3 + 5x$,"This is a problem (3.9b on pg. 113) from ""Rational Points on Elliptic Curves 2nd Edition"" by Silverman and Tate. Let $C : y^2 = x^3 + 5x$ and $\overline{C} : y^2 = x^3 -20x $ . Let $\Gamma = C(\mathbb{Q})$ and $\overline{\Gamma} = \overline{C}(\mathbb{Q})$ . In the textbook, they define the homomorphisms $\alpha : \Gamma\to \mathbb{Q}^*/{\mathbb{Q}^*}^2$ and $\overline{\alpha}: \overline{\Gamma}\to \mathbb{Q}^*/{\mathbb{Q}^*}^2$ by: $$ \alpha((x,y)) = x \mod{{\mathbb{Q}^*}^2} $$ $$ \alpha((0,0)) = 5 \mod{{\mathbb{Q}^*}^2} $$ $$ \overline{\alpha}((x,y)) = x \mod{{\mathbb{Q}^*}^2} $$ $$ \overline{\alpha}((0,0)) = -20 \mod{{\mathbb{Q}^*}^2} $$ The methodology presented in the book tells us that the rank $r$ of $C$ is equal to: $$ 2^r = \frac{\left|\alpha(\Gamma)\right|\left|\overline{\alpha}(\overline{\Gamma})\right|}{4} $$ I have found that: $$ \alpha(\Gamma) = \{ 1,5 \} \subset \mathbb{Q}^*/{\mathbb{Q}^*}^2 $$ $$ \overline{\alpha}(\overline{\Gamma}) = \{ \pm 1, \pm 5 \} \subset \mathbb{Q}^*/{\mathbb{Q}^*}^2 $$ Therefore, the rank of $C$ is $1$ . Now, for the second part of the problem it's asking us to find generators for $C(\mathbb{Q})/2C(\mathbb{Q})$ . $$ C(\mathbb{Q}) = \mathbb{Z}\times (\text{torsion subgroup}) $$ $$ C(\mathbb{Q})/2C(\mathbb{Q}) = \mathbb{Z}/2\mathbb{Z}\times (\text{half of torsion subgroup}) $$ The generators for ""half of torsion subgroup"" are easy to find via Nagell-Lutz. I was wondering how I can find a generator for $\mathbb{Z}/2\mathbb{Z}$ in order to finish the question.","['number-theory', 'mordell-curves', 'elliptic-curves']"
3960037,Prove lower bound of binomial distribution near mean,"Let $0<p<1 / 2$ be fixed independently of $n,$ and let $X_{1}, \ldots, X_{n}$ be iid copies of a Bernoulli random variable that equals 1 with probability $p,$ thus $\mu_{i}=p$ and $\sigma_{i}^{2}=p(1-p),$ and so $\mu=n p$ and $\sigma^{2}=n p(1-p) .$ Using Stirling's formula  show that $$
\mathbf{P}\left(\left|S_{n}-\mu\right| \geq \lambda \sigma\right) \geq c \exp \left(-C \lambda^{2}\right)
$$ for some absolute constants $C, c>0$ and all $\lambda \leq c \sigma .$ What happens when $\lambda$ is much larger than $\sigma ?$ Attempt: By substituting in Stirling formula, I have the following result, which in itself is a lower bound for binomial tail: $\begin{aligned} P(\operatorname{Bin}(n, p)=&k)=\left(\begin{array}{l}n \\ k\end{array}\right) p^{k}(1-p)^{k} \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)+k \log \left(\frac{p}{1-p} \cdot \frac{n-k}{k}\right)+n \log \left(\frac{(1-p) n}{n-k}\right)\right) \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-n\left(D_{K L}(k / n \| p)\right)\right) \end{aligned}$ Let $k=n p+\lambda \sqrt{n(1-p) p}$ where $\lambda=o\left(n^{\frac{1}{6}}\right) .$ Then, we can continue our approximation $$
\begin{aligned}
=\sqrt{\frac{n}{2 \pi k(n-k)}} & \exp \left(o(1)+k \log \left(\frac{n-\lambda \sqrt{\frac{n p}{1-p}}}{n+\lambda \sqrt{\frac{n(1-p)}{p}}}\right)+n \log \left(\frac{n}{n-\lambda \sqrt{\frac{n p}{1-p}}}\right)\right) \\
&=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp (o(1)\\
&+(n p+\lambda \sqrt{n(1-p) p})\left(-\frac{\lambda}{\sqrt{n}}\left(\sqrt{\frac{p}{1-p}}+\sqrt{\frac{1-p}{p}}\right)+\frac{1}{2} \frac{\lambda^{2}}{n}\left(\frac{1}{p}-\frac{1}{1-p}\right)\right.\\
&\left.\left.+O\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)+n\left(-\lambda \sqrt{\frac{p}{1-p}} \frac{1}{\sqrt{n}}-\frac{\lambda^{2}}{2} \frac{p}{1-p} \frac{1}{n}+o\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)\right) \\
&=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-\frac{1}{2} \lambda^{2}\right)
\end{aligned}
$$ using Taylor's expansion. Now I'm stuck. I'm trying to figure out how to get rid of $\sqrt{\frac{n}{2 \pi k(n-k)}}\sim O(1/\sqrt{n})$ in the front by summing up other terms from $k$ to $n$ but I don't know how to do that. Plus, I assumed $\lambda=o(n^{1/6})$ which is not part of the question. I saw most posts on this topic on this site. Don't think there is an answer to this particular form.","['statistics', 'combinatorics', 'binomial-distribution', 'probability']"
3960140,"Proof of a new identity for the finite sum $\sum _{k=0}^{n-1} (-1)^k \sec \left(\frac{\pi\, k}{n}+\frac{\pi }{2 \ n}\right)$","Problem statement: Prove the following identity \begin{align}
\frac{\pi}{2\,\mathscr{n}} \sum _{k=0}^{\mathscr{n}-1} (-1)^k \sec \left(\frac{\pi\, k} {\mathscr{n}}\,+\,\tag{1}\label{eq:1}
\frac{\pi }{2\,\mathscr{n}}\right)+\frac{\pi}{2} \sec \left(\frac{\pi\, \mathscr{n}}{2}\right)\, 
=
\end{align} \begin{align*}
=\, \frac{\left(1-\frac{1}{\mathscr{n}}\right)}{(2 \pi )^{\mathscr{n}-1}}\,\, \mathcal{G_\texttt{1}(\mathscr{n})} \, + \,
 \frac{2}{(2 \pi )^{\mathscr{n}-1}}\,\,\mathcal{G_\texttt{2}(\mathscr{n})} 
\end{align*} \begin{align*}
=\, \frac{\left(1-\frac{1}{\mathscr{n}}\right)}{(2 \pi )^{\mathscr{n}-1}}\,\, %
G_{\mathscr{n} + 2,\mathscr{n} + 2}^{\mathscr{n} + 2,\mathscr{n} + 1}\left( 1\left\vert 
\begin{array}{c}
\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}},0,1\\ 
0,0,\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}}%
\end{array}%
\right. \right)\,+ 
\end{align*} \begin{align*}
 +\, \frac{2}{(2 \pi )^{\mathscr{n}-1}}\,\,%
G_{\mathscr{n} + 2,\mathscr{n} + 2}^{\mathscr{n} + 2,\mathscr{n} + 1}\left( 1\left\vert 
\begin{array}{c}
\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}},0,1\\ 
0,0,\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 2}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n}{\mathscr{n}}%
\end{array}%
\right. \right)
\end{align*} by serial representations of the two Meijer G-functions on the right hand side. The finite sum on the left hand side converges for any even number $\mathscr{n}$ . The identity is a special case of a more general solution of the integral Vladimir Reshetnikov and can be derived starting from stocha - Summary (1. - 3.) - and solve the integral  with the help of the Plancherel theorem, similar to Sasha .
The more challenge is to prove the identity by a serial representation of the right hand side of \eqref{eq:1}. As a result new series representations of each Meijer G function follow, respectively. Introduction: For a more general discussion the representation of the Meijer G-functions through their generalizations - the Fox H - Functions - Mathai are needed: \begin{align*}
\mathcal{H_\texttt{1}(\mathscr{n})}\, = \mathcal{G_\texttt{1}(\mathscr{n})}\, =  H_{3,3}^{3,2}\left( \tag{2}\label{eq:2}1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3,B_3\right)
\end{array}\right. \right) 
\,\text{and}\qquad 
\end{align*} \begin{align*}
\mathcal{H_\texttt{2}(\mathscr{n})}\, = \mathcal{G_\texttt{2}(\mathscr{n})}\, =  H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3 + 1,B_3\right)
\end{array}\right. \right)\text{,}
\end{align*} where we used the following coefficients and indices: \begin{align*}
\left(a_1,A_1\right) = \left(\frac{1-\mathscr{n}}{2},\mathscr{n}\right),\, \left(a_n,A_n\right) = \left(a_2,A_2\right) = \left(0,1\right)
\end{align*} \begin{align*}
\left(a_p,A_p\right) = \left(a_{n+1},A_{n+1}\right) = \left(a_ 3,A_3\right) = \left(1,1\right)
\end{align*} \begin{align*}
\left(b_1,B_1\right) = \left(0,1\right),\, \left(b_2,B_2\right) = \left(0,1\right)
\end{align*} \begin{align*}
\left(b_q,B_q\right) = \left(b_m,B_m\right) = \left(b_3,B_3\right) = \left(a_1,A_1\right)  = \left(\frac{1-\mathscr{n}}{2},\mathscr{n}\right)
\end{align*} \begin{align*}
m = 3,\, n = 2,\, p = 3,\, q = 3
\end{align*} Motivation: The identity is a special case of a more general solution of the integral Vladimir Reshetnikov \begin{align}
\int_{0}^\infty\operatorname{sech}(x)\operatorname{sech}(a\, x)\ dx\,=\,\left( 1-\frac{1}{\mathscr{n}}\right)\,\mathcal{H_\texttt{1}(\mathscr{a})}\,+\, \frac{2}{\mathscr{n}}\,\mathcal{H_\texttt{2}(\mathscr{a})}\tag{3}\label{eq:3}\\
\end{align} \begin{align*}
=\left( 1-\frac{1}{\mathscr{n}}\right)\,H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3,B_3\right)
\end{array}\right. \right)\,+
\end{align*} \begin{align*}
+\,\frac{2}{\mathscr{n}}\,
H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3 + 1,B_3\right)
\end{array}\right. \right)
\end{align*} Besides of the identity \eqref{eq:1}, which is subject of this post, other interesting unknown identities arise from the discussion in stocha and especially the interesting answer Somos . An asymptotic expansion of $\mathcal{H_\texttt{1}(\mathscr{n})}$ and $\mathcal{H_\texttt{2}(\mathscr{n})}$ may also give more insight in the approximations of Claude Leibovici . Moreover the study of the Mellin transformation of $\mathcal{H_\texttt{1}(\mathscr{n})}$ and $\mathcal{H_\texttt{2}(\mathscr{n})}$ is a valuable
replenishment to the paper ""Evaluation of a class of definite integrals"" by M.L. Glasser (Univ. Beograd. Publ. Elektrotehn. Fak. Ser. Mat. Fiz.76,498-No.541(1975), 49-50.) Since the argument of the Fox H-functions are equal to 1, we can use Mathai and write $\mathcal{H_\texttt{1}(\mathscr{n})}$ and $\mathcal{H_\texttt{2}(\mathscr{n})}$ in different forms: \begin{align*}
\mathcal{H_\texttt{1}(\mathscr{n})} = H_{3,3}^{3,2}\left( 1\left\vert \tag{4}\label{eq:4}
\begin{array}{c}
\left(a_p,A_p\right) 
\\ 
\left(b_q,B_q\right)
\end{array}\right. \right)
= H_{3,3}^{2,3}\left( 1\left\vert 
\begin{array}{c}
\left(1-b_q,B_q\right) 
\\ 
\left(1-a_p,A_p\right)
\end{array}\right. \right)
= \frac{1}{k} \,H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_p,\frac{A_p}{k}\right) 
\\ 
\left(b_q,\frac{B_q}{k}\right)
\end{array}\right. \right)\,\text{and}\qquad 
\end{align*} \begin{align*}
\mathcal{H_\texttt{2}(\mathscr{n})} = H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_p,A_p\right) 
\\ 
\left(b_q + 1,B_q\right)
\end{array}\right. \right)
= H_{3,3}^{2,3}\left( 1\left\vert 
\begin{array}{c}
\left(1-b_q + 1,B_q\right) 
\\ 
\left(1-a_p,A_p\right)
\end{array}\right. \right)
\end{align*} \begin{align*}
= \frac{1}{k} \,H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_p,\frac{A_p}{k}\right) 
\\ 
\left(b_q + 1,\frac{B_q}{k}\right)
\end{array}\right. \right)
\end{align*} where $k > 0$ Attempt at a solution: For the prove we followed two ways: The first way is to express the Fox H - Functions $\mathcal{H_\texttt{1}(\mathscr{n})}$ and $\mathcal{H_\texttt{2}(\mathscr{n})}$ in serial representation Mathai , take advantage of the fact, that the poles of $\Gamma \left(1-a_j+s A_j\right)$ at least for $\mathcal{H_\texttt{1}(\mathscr{n})}$ are simple and receive an infinite sum: \begin{align}
\mathcal{H_\texttt{1}(\mathscr{n})} = (-1)^{\frac{\mathscr{n}}{2}} \,\pi \,\log\left(2\right) + \frac{1}{\mathscr{n}}\,\sum _{\nu =0}^{\infty } (-1)^{\nu } \,\frac{\Gamma\left(\frac{\mathscr{n} - 2\,\nu -1}{2\,\mathscr{n} }\right) \Gamma\left(1 + \nu\right) \Gamma\left(\frac{1 + \mathscr{n} + 2\,\nu }{2\, \mathscr{n} }\right)^2}{\nu ! \,\Gamma\left(\frac{1 + 3 \,\mathscr{n} + 2 \,\nu }{2\, \mathscr{n} }\right)}  \tag{5}\label{eq:5}
 \end{align} \begin{align*}
= (-1)^{\frac{\mathscr{n}}{2}} \,\pi \,\log\left(2\right) + 2 \pi \, \sum _{\nu =0}^{\infty } \frac{(-1)^{\nu }\, \sec\left(\pi\,\frac{1+2\,\nu }{2\, \mathscr{n} }\right)}{1 + \mathscr{n} + 2 \,\nu }
 \end{align*} It turned out, that the equivalent sum for $\mathcal{H_\texttt{2}(\mathscr{n})}$ in contrast do not converge. Therefore we use the series representations of the Meijer G function Mathematical Functions Site , which do converge: \begin{align}
\mathcal{G_\texttt{1}(\mathscr{n})} = \sum _{k=1}^{\mathscr{n}+1} \frac{\left(\left(\prod _{j=1,j\neq k}^{\mathscr{n}+1}  \Gamma \left(a_k-a_j\right)\right) \prod _{j=1}^{\mathscr{n}+2} \Gamma\left(1+b_j-a_k\right)\right)}{\Gamma\left(1+a_{\mathscr{n}+2}-a_k\right)}\,_{\mathscr{n}+2}F_{\mathscr{n}+1}(\left\{1+b_{1}-a_k,\ldots ,1+b_{\mathscr{n}+2}-a_k\right\},  \tag{6}\label{eq:6}
\end{align} \begin{align*}
\left\{1+a_1-a_k,\ldots ,1+a_{k-1}-a_k,1+a_{k+1}-a_k,\ldots,1+a_{\mathscr{n}+2}-a_k\right\},-1)\\
\end{align*} Note that $b_{1} = b_{2}=0$ , $a_{\mathscr{n}+1} = 0$ , $a_{\mathscr{n}+2} = 1$ , $a_j = b_{j+2},\,j = 1,\ldots,n$ and \begin{align}
\frac{{\Gamma\left(1-a_{\mathscr{n}+1}\right)}^2}{\Gamma\left(2-a_{\mathscr{n}+1}\right)} = \frac{\Gamma\left(2-a_{\mathscr{n}+1}\right)}{{\left(a_{\mathscr{n}+1}-1\right)}^2}\tag{7}\label{eq:7}\\
\end{align} \begin{align}
\mathcal{G_\texttt{1}(\mathscr{n})} =  \sum _{k=1}^{\mathscr{n}+1}  \left(\left(\prod _{j=1,j\neq k}^{\mathscr{n}+1}  \Gamma \left(a_k-a_j\right)\right) \prod _{j=1}^{\mathscr{n}} \Gamma\left(1+a_j-a_k\right)\right)\frac{\Gamma\left(2-a_{k}\right)}{{\left(a_{k}-1\right)}^2} \tag{8}\label{eq:8}\\
\end{align} \begin{align*}
 \,_{\mathscr{n}+2}F_{\mathscr{n}+1}(\left\{1-a_k,1-a_k,1+a_{1}-a_k,\ldots ,1+a_{\mathscr{n}}-a_k\right\},
\end{align*} \begin{align*}
 \left\{1+a_1-a_k,\ldots ,1+a_{k-1}-a_k,1+a_{k+1}-a_k,\ldots,1+a_{\mathscr{n}+2}-a_k\right\},-1)\\
\end{align*} \begin{align}
\mathcal{G_\texttt{1}(\mathscr{n})} = \sum _{k=1}^{\mathscr{n}+1}  \left(\left(\prod _{j=1,j\neq k}^{\mathscr{n}+1}  \Gamma \left(a_k-a_j\right)\right) \prod _{j=1}^{\mathscr{n}} \Gamma\left(1+a_j-a_k\right)\right)\frac{\Gamma\left(2-a_{k}\right)}{{\left(a_{k}-1\right)}^2} \tag{9}\label{eq:9}
\end{align} \begin{align*}
 \,_{3}F_{2}(\left\{1,1-a_k,1-a_k\right\},\left\{1-a_k,2-a_k\right\},-1)\\
\end{align*} \begin{align}
\mathcal{G_\texttt{1}(\mathscr{n})} = \frac{1}{2}\,\sum _{k=1}^{\mathscr{n}+1}  \left(\left(\prod _{j=1,j\neq k}^{\mathscr{n}+1}  \Gamma \left(a_k-a_j\right)\right) \prod _{j=1}^{\mathscr{n}} \Gamma\left(1+a_j-a_k\right)\right)\frac{\Gamma\left(2-a_{k}\right)}{\left(a_{k}-1\right)}\,\left(\psi\left(\frac{1}{2}-\frac{a_k}{2}\right)-\psi\left(1-\frac{a_k}{2}\right)\right) \tag{10}\label{eq:10}\\
\end{align} Note that $a_{\mathscr{n}+1} = 0$ . For further simplifications we have to express the coefficients $a_k$ and $a_j$ as function of $\mathscr{n}$ \begin{align}
a_j\left(\mathscr{n}\right) = \frac{1-\mathscr{n}}{2 \mathscr{n}}+\frac{j-1}{\mathscr{n}}\quad a_k\left(\mathscr{n}\right) = \frac{1-\mathscr{n}}{2 \mathscr{n}}+\frac{k-1}{\mathscr{n}} \tag{11}\label{eq:11}
\end{align} The second product in \eqref{eq:6} is done with the duplication formula Wikipedia \begin{align}
\prod _{j=1}^\mathscr{n} \Gamma\left(1+a_j-a_k\right)=\prod_{j=0}^{\mathscr{n}-1} \Gamma\left(1 + \frac{k-1}{\mathscr{n}} + \frac{j}{\mathscr{n}}\right)=(2 \pi )^{\frac{\mathscr{n}-1}{2}}\mathscr{n}^{\frac{1}{2}-(\mathscr{n}-(k-1))} \,\Gamma\left(\mathscr{n}-(k-1)\right) \tag{12}\label{eq:12}
\end{align} The simplification of the first product in \eqref{eq:6} seems to be very difficult, since the sign in the gamma function for some coefficients is negative.  The same derivations have to be done similar for $\mathcal{G_\texttt{2}(\mathscr{n})}$ . We find a definite sum over $\mathscr{n}$ as expected, but make no progress at this point at all. EDIT: In the meantime, progress is done with the sum of the two Meijer - G- Functions \eqref{eq:1}: \begin{align}
\frac{2}{(2 \pi )^{\mathscr{n}-1}}\,\tag{13}\label{eq:13}
G_{\mathscr{n} + 1,\mathscr{n} + 1}^{\mathscr{n} + 1,\mathscr{n} + 1}\left( 1\left\vert 
\begin{array}{c}
0,\frac{ \frac{1-\mathscr{n}} {2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}}\\ 
0,\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}}%
\end{array}%
\right. \right) =
\end{align} \begin{align*}
=\, \frac{\left(1-\frac{1}{\mathscr{n}}\right)}{(2 \pi )^{\mathscr{n}-1}}\,\, %
G_{\mathscr{n} + 2,\mathscr{n} + 2}^{\mathscr{n} + 2,\mathscr{n} + 1}\left( 1\left\vert 
\begin{array}{c}
\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}},0,1\\ 
0,0,\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}}%
\end{array}%
\right. \right)\,+ 
\end{align*} \begin{align*}
 +\, \frac{2}{(2 \pi )^{\mathscr{n}-1}}\,\,%
G_{\mathscr{n} + 2,\mathscr{n} + 2}^{\mathscr{n} + 2,\mathscr{n} + 1}\left( 1\left\vert 
\begin{array}{c}
\frac{\frac{1-\mathscr{n}}{2}}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n - 1}{\mathscr{n}},0,1\\ 
0,0,\frac{\frac{1-\mathscr{n}}{2} + 1}{\mathscr{n}},\frac{\frac{1-\mathscr{n}}{2} + 2}{\mathscr{n}},...,\frac{\frac{1-\mathscr{n}}{2} + n}{\mathscr{n}}%
\end{array}%
\right. \right)
\end{align*} This identity can be generalized for the Fox H-Function, too: \begin{align*}
2\,H_{2,2}^{2,2}\left( 1\left\vert 
\begin{array}{c}
\left(c_1,C_1\right) ,\left(c_2,C_2\right)
\\ 
\left(d_1,D_1\right) ,\left(d_2,D_2\right) 
\end{array}\right. \right)
\end{align*} \begin{align*}
=\left( 1-\frac{1}{\mathscr{n}}\right)\,H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3,B_3\right)
\end{array}\right. \right)\,+
\end{align*} \begin{align*}
+\,\frac{2}{\mathscr{n}}\,
H_{3,3}^{3,2}\left( 1\left\vert 
\begin{array}{c}
\left(a_1,A_1\right) ,\left(a_2,A_2\right) ,\left(a_3,A_3\right)
\\ 
\left(b_1,B_1\right) ,\left(b_2,B_2\right) ,\left(b_3 + 1,B_3\right)
\end{array}\right. \right)
\end{align*} where we used the following coefficients: \begin{align*}
\left(c_1,C_1\right) =\left(d_1,D_1\right) = \left(0,1\right),\,
\left(c_2,C_2\right) =\left(d_2,D_2\right) = \left(\frac{1-\mathscr{n}}{2},\mathscr{n}\right)
\end{align*} With the new relationship \eqref{eq:13}, considering the appropriate series representation, we only have to show that: \begin{align}
\frac{\sum _ {k=1}^{\mathscr{n}+1} \left(\prod _{j=1,j\neq k}^{\mathscr{n}+1}\Gamma\left(c_k-c_j \right)\right) \prod_{j=1}^{\mathscr{n}+1}\Gamma\left(1+c_j-c_k \right)}{(2\pi )^{\mathscr{n}-1}} =
\end{align} \begin{align}
\frac{\pi}{2\,\mathscr{n}} \sum _{k=0}^{\mathscr{n}-1} (-1)^k \sec \left(\frac{\pi\, k} {\mathscr{n}}\,+\,
\frac{\pi }{2\,\mathscr{n}}\right)+\frac{\pi}{2} \sec \left(\frac{\pi\, \mathscr{n}}{2}\right)\, 
\end{align} We may express again the coefficients $c_k$ and $c_j$ as function of $\mathscr{n}$ \begin{align}
c_1=0, \quad c_{j/k}=\frac{\left(\frac{1-\mathscr{n}}{2}+\left(j/k-2\right)\right)}{\mathscr{n}} \quad j,k\geq 2
\end{align} Question: How can we proof the new identity by a series representation of the two Meijer G-functions? What are the appropriate series representation of each of the Meijer G-functions and how can the finite sum over the secant - function be splitted? Is there a closed - form expression for the product $\prod_{j=1,j\neq k}^{\mathscr{n}+1}\Gamma\left(a_k-a_j\right)$ ? Final Question - Final question remains after progress The proof is done, if we show that: \begin{align}
\prod _{j=0,j\neq k}^{\mathscr{n}-1} \Gamma\left(\frac{k}{\mathscr{n}}-\frac{j}{\mathscr{n}}\right)=\frac{(-1)^{1+k} \,\mathscr{n}^{\mathscr{n}-k-\frac{3}{2}} \,\,(2 \pi )^{\frac{\mathscr{n}-1}{2}}}{\Gamma\left(\mathscr{n}-k\right)}
\end{align}","['integration', 'sequences-and-series']"
3960147,Huffman Tree and Three Ambiguous facts,"I see three facts on Huffman Algorithms: Two of them is True: Fact A: The average length of an optimal code is always at most $\lceil \log n \rceil$ , where $n$ is the number of codewords. Fact B: The average codeword length in Huffman's algorithm is $\Omega(\log n)$ . and one Fact that say this is False : Fact C: The average codeword length in Huffman's algorithm is $O(\log n)$ . Really I couldn't get the point, what is the difference among these
three facts. Is there any example or simple intuitive idea about these
three facts? (I means what is the role of ""optimal"" words in first
fact, or what is difference among these three facts in very short sentence)","['information-theory', 'discrete-mathematics', 'algorithms', 'coding-theory', 'computer-science']"
3960156,Function composition for proving polynomials are continuous,"I am attempting to prove that polynomials $\mathbb{R} \to \mathbb{R}$ are continuous in the topological sense. Let $f$ be such a polynomial. The answer here suggests showing that linear functions are continuous (easy) and that the composition of continuous functions is continuous (easy). However - and perhaps it's because I've been looking at this stuff too long - I fail to see why $$\begin{align}
p_0(x) &= a_n \\
p_1(x) &= xp_0(x) + a_{n-1} \\
p_2(x) &= xp_1(x) + a_{n-2} \\
&\vdots \\
p_n(x) &= f(x)
\end{align}$$ is, per the answer in the question, ""just composing a linear function with a continuous function."" From my perspective, $p_0$ is a constant function, so it's continuous. Let's suppose I have a function $g: \mathbb{R} \to \mathbb{R}$ given by $g(y) = cy + d$ for some $c, d \in \mathbb{R}$ fixed. I have shown that $g$ is continuous. If I assume $c = x$ and $d = a_{n-1}$ , then I get $g(p_0(x)) = xp_0(x) + a_{n-1}$ . However, while setting $d = a_{n-1}$ seems like fair game (they are both constants), $x$ is definitely not a constant, so I don't feel comfortable setting $c = x$ . Am I missing something here?","['continuity', 'general-topology', 'solution-verification']"
3960167,Problems to understand the definition of the level of significance $\alpha$,"I'm having problems to understand the definition of the level of significance $\alpha$ . I thought I knew what $\alpha$ is but I realized I don't. When I stated to study statistics by myself I read this introductory book and everything was fine, the definition is very clear. He says on page 290: Youâ€™re probably wondering, how small does a p-value have to be for us
to achieve statistical significance? If we agree that a p-value of $0.0001$ is clearly statistically significant and a p-value of $0.50$ is not, there must be some point between $0.0001$ and $0.50$ where we cross
the threshold between statistical significance and random chance. That
point, measuring when something becomes rare enough to be called
â€œunusual,â€ might vary a lot from person to person. We should agree in
advance on a reasonable cutoff point. Statisticians call this cutoff
point the significance level of a test and usually denote it with the
Greek letter $\alpha$ (alpha) . For example, if $\alpha = 0.05$ we say we are doing
a $5\%$ test and will call the results statistically significant if the
p-value for the sample is smaller than $0.05$ . Often, short hand
notation such as $P < 0.05$ is used to indicate that the p-value is less
than $0.05$ , which means the results are significant at a $5\%$ level. Now, I'm studying about statistical inference, a more advanced subject, and I realized there are some concepts that don't exactly have the same definition as I studied before. The level of significance is an example. I'm reading this book and on page 352 he introduces the Neyman-Pearson lemma as a method to find the UMP test . Example: On the basis of a random sample of size $1$ from the p.d.f. $f(x;
 \theta)=\theta x^{\theta-1},\ 0 < x < 1\ (\theta > 1)$ : For $\theta_1>\theta_0$ , the cutoff point is calculated by: ... $C=(1âˆ’\alpha)^{\frac{1}{\theta_0}}$ For $\theta_1<\theta_0$ , we have: ... $C = \alpha^{\frac{1}{\theta_0}}$ So in this second book, the cutoff point is not necessarily $\alpha$ , I'm confused. MY ATTEMPT TO UNDERSTAND WITH THE HELP OF THE ANSWERS The alpha is predetermined, but it doesn't mean I can't have a smaller rejection region. Then I end up having a smaller rejection region using NP lemma with the same level of significance alpha. Some introductory books let the cutoff point to be $\alpha$ by standard (why?), that's the reason of my confusion, I can shrink the rejection region keeping the value of $\alpha$ . Can someone say if I'm right?","['statistical-inference', 'statistics', 'hypothesis-testing']"
3960180,Locally convex implies convex?,"There is a question about convex set. Let $\Omega\subset\mathbb R^n$ be an open, connected set. If for any $x\in\overline{\Omega}$ , there is a neighborhood $U_x$ such that $\Omega\cap U_x$ is convex, then $\Omega$ is convex. Intuitively, if $\Omega$ is not convex, there must be a point on $\partial\Omega$ such that $\Omega$ is not convex locally. But I have no idea how to write it clearly. Thanks to your help.","['general-topology', 'convex-analysis']"
3960189,Calculating expected bonus- what's wrong with my solution?,"An auto insurance company is implementing a new bonus system. In each month, if a policyholder does not have an accident, he or she will receive a $5 cashback bonus from the insurer. Among the 1000 policyholders, 400 are classified as low-risk drivers and 600 are classified as high-risk drivers. In each month, the probability of zero accidents for high-risk drivers is 0.8 and that for low-risk drivers is 0.9. Calculate the expected bonus payment from the insurer to the 1000 policyholders in one year. The question is what's wrong with my (following) solution? Probability of zero accidents $= (0.8\times 0.6) + (0.90 \times 0.4) = 0.84$ Then, the expected payment is given by $$\sum_{x=1}^{12} (0.84^x) \times 1000 \times 5x$$","['statistics', 'solution-verification', 'probability']"
3960199,Proof that set is a submanifold,"$m$ and $k$ are natural numbers satisfying $1\leq k\leq m$ , $M_{k,m}$ is the set of the entire $k\times m$ matrix, and its components are real numbers,and the subset of $M_{k,m}$ is defined as follows $$V_{k,m}= \{A\in M_{k,m} | AA^T = I_k\}$$ $I_k$ is the k-dimensional identity matrix, $A^T$ is the transposed matrix, take $M_{k,m}$ as $km$ -dimesional Euclidean space $\mathbb{R}^{k\times m}$ , $V_{k, m}$ is a subset of $\mathbb{R}^{k\times m}$ Problem: Prove that $V_{2, m}$ is a $(2m-3)$ -dimensional $C^\infty$ submanifold of $\mathbb{R}^{2m}$ My knowledge of manifolds is not very good. I think need find out the Jacobian and prove that it is a regular value, but I donâ€™t have any ideas and I donâ€™t know how to start,can anyone show me how to do this?","['submanifold', 'smooth-manifolds', 'differential-geometry']"
3960212,Complex polynomials and lines of constant argument,"Here are three domain plots of a complex polynomial of degree $5$ . The left picture is very zoomed out, and the right picture is more zoomed into the zeroes. (Pictures are taken from Elias Wegert's book "" Visual Complex Functions "".) The color indicates the argument of the function; the modulus is not featured. Say we focus our attention on one color, like yellow. Then we can see that the yellow lines coming in from infinity seem to ""end"" at the zeroes of the polynomial. My confusion is: I cannot justify why this should be the case in general. Why is it true that every yellow line (that is, a line of constant argument) coming in from infinity should terminate at a zero of the polynomial? It makes sense to me that around a zero of order $n$ , the polynomial should look like $z^n$ -- that is, there should be $n$ yellow lines emanating from that point. What is not clear to me, though, is why every yellow line coming in from infinity should terminate at a zero in particular. Why can't it terminate at any other point? Any suggestions / hints would be greatly appreciated. Thanks!","['complex-analysis', 'analysis']"
3960216,Why is integration so much harder than differentiation?,"If a function is a combination of other functions whose derivatives are known via composition, addition, etc., the derivative can be calculated using the chain rule and the like. But even the product of integrals can't be expressed in general in terms of the integral of the products, and forget about composition! Why is this?","['integration', 'calculus']"
3960244,"$ \int_{-\infty}^{+\infty}\sin(x^2)\cos2\alpha x \, \mathrm{d}x $","How to compute $$
\int_{-\infty}^{+\infty}\sin(x^2)\cos2\alpha x\,\mathrm{d}x
$$ The integrating by parts does not work. Write $$
I(\alpha)=\int_{-\infty}^{+\infty}\sin(x^2)\cos2\alpha x\,\mathrm{d}x,
$$ then $$
I'(\alpha)=-\int_{-\infty}^{+\infty}2x\sin(x^2)\sin2\alpha x\,\mathrm{d}x.
$$ I do not kwow how to go on. Appreciate any help!","['improper-integrals', 'definite-integrals', 'analysis', 'calculus', 'substitution']"
3960255,What is the maximum number of consecutive sides whose lengths you can choose without uniquely determining the polygon?,"For every $n$ -sided equiangular polygon, define $f(n)$ to be the maximum number of consecutive sides whose lengths you can choose without uniquely determining the polygon. Find the last two digits of: $$\sum_{n=3}^{2019} f(n)$$ Can someone verify this ? I think $(n-3)$ works, and I posted it in AOPS, but the guy is saying that it is wrong. Here's the solution: Answer : $36$ , $f(n)=n-3 $ clearly for any $n , n-2$ doesn't satisfy because then equiangular property fixes the polygon. For $n=3, 4$ , clearly $f(n)=n-3$ works .
For $n\ge 5$ , consider the following construction , which i took for $n=7$ ,( but one can understand what's the construction) . It's just extending $2$ sides and making a parallel side . So $n-3$ consecutives doesn't fix. So $n-3$ works . And answer is $\frac{2016\cdot 2017}{2}=2033136 \equiv 36 \pmod {100}$ Here's the other guy's solution, which many people are saying it to be true :
Equilateral triangle which is regular, square but we can have a rectangle, pentagon where all it's sides are equal and so on. Then I see a pattern. This was the crucial observation. In Euclidean Geometry, if we have an equiangular polygon with odd sides then it's all the sides will be equal, i.e it will be a regular polygon. So, we have the regular polygons with side lengths $3,5,\cdots$ and so on. Here, in this case we can choose only one of it's side length and all the side lengths will be automatically chosen since it is an equiangular and hence regular polygon. Therefore, $f(n)=\boxed{1}$ , in the case when we have an equiangular polygon with odd sides.
Now, we see what happens when we choose an equiangular polygon with odd sides, so we can choose half of the sides of the polygon(these needs to be consecutive sides). Therefore $f(n)=\frac{n}{2}$ in the case where $n=\text{even}$ . $\text{Sum}=(1+2)+(1+3)+(1+4)+\cdots+(1+1009)+1=510553 $ . Therefore Last two digits of the sum $\boxed{53}$ . Is there a flaw in the solution , I wrote ?","['contest-math', 'euclidean-geometry', 'solution-verification', 'geometry']"
3960323,Relationship between Partial Harmonic Sum and Logarithm.,"Let $H_n$ denote the partial harmonic sum $$H_n = \sum_{j=1}^n \frac{1}{j} \ \ .$$ I saw in some lecture notes that $H_n \approx \ln(n)$ without any explanation given. I tried to understand this approximation using Taylor series. Recall that the Taylor expansion of $\ln(x)$ for $x>0$ is $$\ln(x) = 2 \sum_{j=1}^{\infty} \frac{ \bigg[ \displaystyle\frac{(x-1)}{(x+1)} \bigg]^{(2j-1)} }{2j-1} \ \ .$$ I tried to use this to obtain the connection, however I was unable to see the relationship between $H_n$ and $\ln(n)$ . I performed some crude numerical calculations and I'm not sure whether the approximation is correct because $H_n$ does not appear to be close to $\ln(n)$ for the values of $n$ that I have tried. Can someone please help me understand this approximation or confirm my suspicions that there is an error in the notes. If there is an error in the notes, does a relationship exist between the partial harmonic sum and the logarithm however?","['logarithms', 'real-analysis', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
3960359,What is the greatest number of points in the plane such that the distance between any two of them is an odd integer?,"Suppose the origin is one of the points, we can take $(3,0)$ as the second point. If $(x,y)$ is some other point in the set I think we can use the fact that $x^2+y^2$ , for $x$ and $y$ odd, is never an integer. There is also a lot of lines that the points certainly cannot belong to. But I could not go beyond.",['geometry']
3960365,calculate $\mathbb{E}[X]$ . what is wrong in my attempt?,"friend A choose 3 cards from 10 cards packet, then he return the cards to the pack. friend B choose 3 cards from the same pack in an independent way from friend A. Let $X$ be the number of cards that didnt choose by any of the friends, calculate $\mathbb{E}[X]$ First I can see that $4\leq X \leq 7$ $${p(X=4)}
~=~ \frac{\binom{10}{3}\binom{7}{3}}{\binom{10}{3}\binom{10}{3}}.$$ $${p(X=5)}
~=~ \frac{\binom{10}{3}\binom{7}{2}\binom{3}{1}}{\binom{10}{3}\binom{10}{3}}.$$ $${p(X=6)}
~=~ \frac{\binom{10}{3}\binom{7}{1}\binom{3}{2}}{\binom{10}{3}\binom{10}{3}}.$$ $${p(X=7)}
~=~ \frac{\binom{10}{3}\binom{3}{3}}{\binom{10}{3}\binom{10}{3}}.$$ but I dont get the right answer when I calculate it according to $$
\mathbb{E}(X)=\sum_{t \in R_{X}} t \cdot \mathbb{P}(X=t)
$$","['expected-value', 'probability']"
3960382,Computing a Killing vector field from flow,"I am given the following manifold $N=\{(x,y)\in \mathbb{R}^2, y>0\}$ with metric: $$ds^2=\frac{dx^2+dy^2}{y^2}$$ There is a suggestion to take $z=x+iy$ and consider the transformations: $$z\to z+c\,, \quad z\to cz\,,\quad z \to \frac{z}{cz+1}\,,\quad c\in \mathbb{R}$$ I have to find three independent Killing vector fields. I think the idea is to define a flow from those transformations and then compute the Killing vectors associated to that flow. Is there any systematic way to do this?",['differential-geometry']
3960417,Evaluare $I(y)=\int_0^1\frac{1}{y+\cos(x)}dx$. Hence determine $J(y)=\int_0^1\frac{1}{(y+\cos(x))^2}dx$,"Consider the following integral: $$I(y)=\int_0^1\frac{1}{y+\cos(x)}dx $$ with Weierstrasse substitution I showed that $$ I(y)=\frac{2}{\sqrt{y^2-1}}\arctan\left(\sqrt{\frac{{y-1}}{y+1}}\right). $$ It is this next part that I am not sure about: Hence detertemine $$J(y)=\int_0^1\frac{1}{(y+\cos(x))^2}.$$ My attempt: Since $I(y)\pm J(y)$ yields no simplication, the most obvious approach would be to rewrite $I(y)$ as $$I(y)=\int_0^1\frac{(y+\cos(x))}{(y+\cos(x))^2}=yJ(y)+\underbrace{\int_0^1\frac{cos(x)}{(y+\cos(x))^2}dx}_{K(y)}$$ Applyling Weierstrasse substitituion for $K(y)$ : $$K(y)=2\int_0^1\frac{(1-x^2)}{((y-1)x^2+(y+1))^2}dx=-2\int_0^1\frac{\frac{1}{y-1}((y-1)x^2+(y+1))-\frac{y+1}{y-1}-1}{((y-1)x^2+(y+1))^2}$$ $$=-2\left(\underbrace{\frac{1}{y-1}\int_0^1\frac{1}{(y-1)x^2+(y+1)}dx}_{\text{elementary integral}\to \arctan()}-\underbrace{\frac{2}{y-1}\int_0^1\frac{1}{((y-1)x^2+(y+1))^2}dx}_{\text{evaluated with } 
 \tan(u)=\sqrt{\frac{y-1}{y+1}}x}\right)$$ so what follows is quite elmenatary. However it seems to me that $K(y)$ is by no means simpler than $J(y)$ , (i.e. the same approach can be used for $J(y)$ but without having to use the result for $I(y)$ ) so this is clearly not the point of the question?","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'substitution']"
3960450,"How to show that $\frac{a + b}{\gcd(a,b)^2}$ is a Fibonacci number when $ \frac{a+1}{b}+\frac{b+1}{a}$ is an integer?","Let $a, b$ be positive integers such that the number $ \dfrac{a + 1}{b} + \dfrac{b + 1}{a}$ is also integer. Then, show that $\dfrac{a + b}{\gcd{(a, b)^{2}}}$ is a Fibonacci number. I prove that : $ \displaystyle{\forall a, b \in \mathbb{N}} $ such that $ \displaystyle{ k = \frac{a + 1}{b} + \frac{b + 1}{a} \in \mathbb{N} \Rightarrow k = 3 }$ or $ \displaystyle{ k = 4 } $ . Also, we have that (I use the method Vieta Jumping) : $ \displaystyle{ \mathbb{S} = \Big \{ \big( a, b \big) \in \mathbb{N} \times \mathbb{N} : \frac{a + 1}{b} + \frac{b + 1}{a} = k \in \mathbb{N} \Big \} = \Big \{ \big( a, b \big) \in \mathbb{N} \times \mathbb{N} : \frac{a + 1}{b} + \frac{b + 1}{a} = 3 \text{  } or \text{  } \frac{a + 1}{b} + \frac{b + 1}{a} = 4 \Big \} } $ Fibonacci sequence : $\displaystyle{f_{0} = 0, f_{1} = 1}$ and $\displaystyle{f_{n+2} = f_{n+1} + f_{n}, \forall n \in \mathbb{N} \cup \{ 0 \} = \{ 0, 1, 2, 3, . . . \}}$ $ \displaystyle{ k = 3 } $ : If $ \displaystyle{ \big( a, b \big) \in \mathbb{S} } $ with $ \displaystyle{ \frac{a + 1}{b} + \frac{b + 1}{a} = 3 } $ , then $ \displaystyle{ \big( 3 \cdot a - 1 - b, a \big) \in \mathbb{S} } $ so we have $ \displaystyle{ \big( 2, 2 \big), \big( 3 = f_{3} + 1, 2 \big), \big( 6 = f_{5} + 1, 3 \big), \big( 14 = f_{7} + 1, 6 \big), \big( 35 = f_{9} + 1, 14 \big), . . . \in \mathbb{S} } $ (I checked some couples and I found that the request is valid) but I can not porve that $ \displaystyle{ \frac{a + b}{\gcd{\big(a, b \big)^{2}}} } $ is a Fibonacci number. $ \displaystyle{ k = 4 } $ : If $ \displaystyle{ \big( a, b \big) \in \mathbb{S} } $ with $ \displaystyle{ \frac{a + 1}{b} + \frac{b + 1}{a} = 4 } $ , then $ \displaystyle{ \big( 4 \cdot a - 1 - b, a \big) \in \mathbb{S} } $ so we have $ \displaystyle{ \big( 1, 1 \big), \big( 2, 1 \big), \big( 6, 2 \big), \big( 21, 6 \big), \big( 77, 21 \big), . . . \in \mathbb{S} } $ (I checked some couples and I found that the request is valid) but also I can not porve that $ \displaystyle{ \frac{a + b}{\gcd{\big(a, b \big)^{2}}} } $ is a Fibonacci number. I need some help, because I can not solve the problem. So my questions are : if $ \displaystyle{ \frac{a + 1}{b} + \frac{b + 1}{a} = 3 } $ where $ \displaystyle{ a = f_{2 \cdot n - 1} + 1, b = f_{2 \cdot n + 1} + 1 \in \mathbb{N}, \forall n \geqslant 2, \forall n \in \mathbb{N} } $ , how I can show that $ \displaystyle{ \frac{a + b}{\gcd{\big( a, b \big)^{2}}} } $ is a Fibonacci number ? and if $ \displaystyle{ \frac{a + 1}{b} + \frac{b + 1}{a} = 4} $ where $ \displaystyle{ a, b \in \mathbb{N} } $ , how I can show that $ \displaystyle{ \frac{a + b}{\gcd{\big( a, b \big)^{2}}} } $ is a Fibonacci number ?","['number-theory', 'fibonacci-numbers']"
3960498,Non-empty intersection.,"Let $A_1,A_2,...,A_{2019}$ be sets s.t. $|A_i|$ =45 for each $i=\overline{1,2019}$ and $|A_i\cap A_j|=1$ for every $i\neq j$ . Is it true that $$\Big|\bigcap _{i=1}^{2019} A_i\Big|=1?$$ What I thought of doing is proving by induction by i, the base is obvious. However I'm stuck on the next step.","['combinatorics', 'discrete-mathematics']"
3960536,Does weak convergence implies convergence in measure when $\sup_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $f_n,f\colon [0,1]\rightarrow \mathbb{R}$ be integrable functions satisfying $\sup\limits_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty$ , where $p>1$ . If, for all measurable set $A\in \mathcal{B}[0,1]$ , $$\int\limits_{A}f_n d\mu\rightarrow\int\limits_{A}fd\mu, $$ is it true that $\{f_n\}$ converges to $f$ in measure?","['measure-theory', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
3960570,"Injection between posets $(\Phi([-1,1]^d)\setminus\{\emptyset\},\subseteq)$ and $(\{-, 0,+\}^d,\preceq)$, where $-\preceq 0$ and $+\preceq 0$.","Let $(\{-, 0,+\}, \preceq)$ be the poset defined by the relations $-\preceq 0$ and $+\preceq 0$ and let $\left(\{-, 0,+\}^d, \preceq\right)$ be its $d$ -fold product. Show that: $$
(\Phi([-1,1]^d)\setminus\{\emptyset\},\subseteq)\cong(\{-, 0,+\}^d,\preceq),
$$ where $\Phi([-1,1]^d)$ is the face lattice of the $d$ -dimensional cube. I have an idea of what the isomorphism would be, but I cannot yet prove that itâ€™s injective. First, a remark is that, since the product of posets is a binary operation, the elements of $(\{-, 0,+\}^d, \preceq)$ technically have a lot of extra parentheses, which could be omitted without losing any important information. So, for example, instead of $((0,+),-)$ , weâ€™d simply have $(0,+,-)$ . Let $F \in \Phi([-1,1]^d)$ denote a face of the cube, and in particular let $v$ denotes a vertex. Then the mapping Iâ€™m thinking of is defined as: \begin{align}
&\phi:(\Phi([-1,1]^d)\setminus\{\emptyset\},\subseteq)\to(\{-, 0,+\}^d,\preceq) \\
&\phi(F) = (\text{sgn}(\sum_{v_i \in F}v_{i1}),â€¦,\text{sgn}(\sum_{v_i \in F}v_{id}))
\end{align} i.e. we look at the vertices contained in $F$ , summing up their entries component-wise, and then taking their signs. Now I want to show that this map is injective. Given two faces $F_1$ and $F_2$ such that $\phi(F_1) = \phi(F_2)$ , itâ€™s sufficient to show that a vertex $v_i = (v_{i1},â€¦,v_{id})$ in $F_1$ is also in $F_2$ (and vice versa). Consider an entry $j$ in $\phi(F_1) = \phi(F_2) \in \{-, 0,+\}^d$ . If $F_1$ and $F_2$ contain just one vertex, then this entry determines exactly the sum at position $j$ of the vertices in $F_1$ and $F_2$ , respectively (namely, the â€œsumâ€ of just one summand, resulting in value $-1$ or $1$ ). If $F_1$ and $F_2$ both contain $2^m$ vertices, then we can continue in an inductive fashion to show that the summands that result in the $j$ entry of $\phi(F_1)$ and $\phi(F_2)$ match each other one by one: in summing the $m$ entries in the $j$ -th position of vertices in $F_1$ and those in $F_2$ , assuming that the partial sums up to a certain position $n < m$ match, then the $j$ entry of $\phi(F_1)$ and $\phi(F_2)$ determines what the value of the next summands must be. Of course, this only works if $F_1$ and $F_2$ have the same number of vertices. I got stuck here since I canâ€™t show that given $\phi(F_1) = \phi(F_2)$ , it cannot be the case that $F_1$ contains $m$ vertices, and $F_2$ contains $mâ€™$ vertices, where $m \neq mâ€™$ . Edit: Iâ€™ve found another map, which might be easier to check for the properties of an isomorphism. Itâ€™s not yet entirely rigorous, but Iâ€™m just putting it here for comments. For $F \in \Phi([-1,1]^d), \ \phi(F) = (u_1,â€¦u_d) := u \in (\{-, 0,+\}^d, \preceq)$ , where $u$ satisfies the following: $\cdot$ $u$ has |dim( $F$ )| $0$ â€™s, at positions corresponding to the axes of $\mathbb{R}^d$ that $F$ is â€œparallelâ€ to. $\cdot$ The other positions in $u$ correspond to the remaining axes in $\mathbb{R}^d$ , have values either $1$ or $-1$ , depending whether $F$ lies in the positive or negative side of those axes, respectively.","['order-theory', 'solution-verification', 'combinatorics', 'lattice-orders']"
3960623,How to calculate the correlation between the number of heads of 100 toss of coin and the number of heads of the first 10 toss of those 100 tosses?,"It is a bit cumbersome to explain: Toss a coin is a Bernoulli distribution, with the probability of seeing a head is p if we toss this coin 100 times, we should expect $X_{1}$ times of head. Within that 100 toss (this is important, we are NOT tossing another 10 times), we should see $X_{2}$ heads from the first 10 toss. How to calculate $corr(X_{1}, X_{2})$ ? The only thing I can think of is, $X_{1} > X_{2}$ , practially, we are doing two sets: toss a coin 10 times, we see $X_{2}$ heads independently toss a coin 90 times, we see $X_{3}$ heads we want to calculate $corr(X_{2}, X_{2} + X_{3})$","['correlation', 'statistics', 'probability']"
3960633,How to solve the equation $â€Žx^2-2y=z^2$?,"Consider the following equation $$â€Žx^2-2y=z^2,$$ according some theorems in my research, I found that the only integer solution of the equation such that $xy\neq 0$ is $$(x,y,z)=(2,2,0).$$ Now my question is: how to solve the equation (or what is the way or method)? Anyone can help me. Thanks in advance(
I tried to rewrite the equation as follows \begin{align}
2y=x^2-z^2 â€Ž\Rightarrow 2y=(x-z)(x+z)â€Ž\Rightarrow y=\frac{(x-z)(x+z)}{2}
\end{align} but it did not work).","['algebra-precalculus', 'polynomials']"
3960638,"Compactness of $K(S)$, if $K$ is an infinite-dimensional compact operator in Hilbert space","$H$ is infinite-dimensional Hilbert space. $K: H \rightarrow H$ is infinite-dimensional compact operator. Let $S$ be a unit sphere in H. The task is to proof that $K(S)$ couldn't be compact. I know that: Really we need to proof that $K[S]$ is not closed. If B is unit ball, $[B]$ - closure, then $K([B])$ is also closed(for compact operator in Hilbert space). If $\mathrm{Ker} B$ is empty, i.e. $0 \notin K(S)$ then there is a sequence in $K(S)$ converging to $0$ , so $K(S)$ isn't compact. But I don't understant how to proof noncompactness of $K(S)$ even if one of the basis vectors is mapped to $0$ . On the other side we can try to proof that if $K(S)$ is compact, then $\mathrm{Im}(K)=K(H)$ shuold have finite dimension. It is easy if $K(B)$ contains some ball(like a subspace in $K(H)$ ), but it's far from being true.","['hilbert-spaces', 'compact-operators', 'functional-analysis']"
3960675,$\lim_{x \to 0}{\frac{\sinh(x)-\sin(x)}{x(\cosh(x)-\cos(x))}}$,"As stated in the title. My attempt, begin with L'Hopital: $$L=\lim_{x \to 0}{\frac{\sinh(x)-\sin(x)}{x(\cosh(x)-\cos(x))}}=\lim_{x \to 0}{\frac{\cosh(x)-\cos(x)}{(\cosh(x)-\cos(x))+x(\sinh(x)+\sin(x))}}$$ Dividing by the numerator $$\lim_{x \to 0}\frac{1}{1+\left(\frac{x\left(\sinh(x)+\sin(x)\right)}{\cosh(x)-\cos(x)}\right)}=\frac{1}{1+\lim_{x \to 0}{\left(\frac{x\left(\sinh(x)+\sin(x)\right)}{\cosh(x)-\cos(x)}\right)}}$$ L'Hopital again $$\frac{1}{1+\lim_{x \to 0}{\left(\frac{x(\cosh(x)+\cos(x))+(\sinh(x)+\sin(x))}{\sinh(x)+\sin(x)}\right)}}$$ Diving through $$\frac{1}{1+\lim_{x \to 0}{\left(1+\frac{\cosh(x)+\cos(x)}{\left(\frac{\sinh(x)}{x}+\frac{\sin(x)}{x} \right)}\right)}}=\frac{1}{1+(1+\frac{1+1}{1+1})}=\frac{1}{3}$$ Is this correct, and is there a more elegant way of doing it?","['limits', 'calculus', 'derivatives', 'trigonometry']"
3960712,How to deduce that $\arctan\left(\frac{a}{i}\right)=-i\cdot \text{ arctanh}(a)$,I was doing some calculations in wolframalpha and I found the following equality: $$\arctan\left(\frac{a}{i}\right)=-i\cdot \text{ arctanh}(a)$$ This is the first time I've seen this equality. How is this deduced? Is this just a specific case for $a/i$ or is it true also that: $$\arctan\left(\frac{a}{b}\right)=-b\cdot \text{ arctanh}(a)?$$,"['trigonometry', 'complex-numbers', 'hyperbolic-functions']"
3960714,Determinant of $n\times n$ Matrix Linear Algebra,"So, I have a matrix $$
  A = \begin{pmatrix} 
  0 & 1 & 1 & ... & 1 \\
  1 & 0 & x & ... & x \\
  1 & x & 0 & ... & x \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & x & x & ... & 0
  \end{pmatrix}
$$ I need to evaluate it's determinant. At first I calculated $\det A$ for $n=2,3,4$ . And I got a pattern $\det A=(-1)^{n-1}(n-1)x^{n-2}$ for every $n\ge2$ . But I need to solve it differently. I added all rows to the first, then multiplied every row (except 1) by $(1+x(n-2))$ and subtracted first row multiplied by $x$ . This is what I got: $$
  \begin{pmatrix} 
  n-1 & 1+x(n-2) & 1+x(n-2) & ... & 1+x(n-2) \\
  1-x & -x(1+x(n-2)) & 0 & ... & 0 \\
  1-x & 0 & -x(1+x(n-2)) & ... & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1-x & 0 & 0 & ... & -x(1+x(n-2))
  \end{pmatrix}
$$ Now I can multiply diagonal elements, but I don't know what can I do with the rest of it. Any hints will be helful.","['matrices', 'determinant', 'linear-algebra']"
3960718,Proving two sigma algebras coincide,"Let $(S, \mathcal{S})$ be a measurable space and $\Omega= \prod_{i=0}^{\infty} S$ , let $X_i:\Omega \to S$ be the ""coordinates"" and $\theta_n$ the shift operators on $\Omega$ : $w=(x_0,x_1,...) \mapsto (x_n,x_{n+1},...)$ . Finally let $\mathcal{A}=\sigma(X_n,  n\geq 0)$ and $ \mathcal{G}_n=\sigma(X_n,X_{n+1},...)$ . How would you prove that $\mathcal{G}_n=\theta_n^{-1}(\mathcal{A})$ ? I am puzzled because it seems to me that the antiimage of an element of $\mathcal{A}$ could very well be in $\mathcal{A}$ and not in $\sigma(X_n,X_{n+1},...)$ . This claim is in the context of canonical Markov Chains, Ionescu-Tulcea theorem.",['probability-theory']
3960727,Is $ \arctan(f(x))\leq f(x)$ whenever $|f(x)|<1\ \ \forall x\in\mathbb{R} $?,"Is the following always true? $$
\arctan(f(x))\leq f(x),\ \ \text{where}\ \ |f(x)|<1\ \ \forall x\in\mathbb{R}
$$ For example, is it correct to say that $$
\arctan\left(\frac{\sin x}{\sqrt{x+1}}\right)\le\frac{\sin x}{\sqrt{x+1}}\ \ \forall x\in\mathbb{R},
$$ since $\arctan t=t-\frac{t^3}{3}+\frac{t^5}{5}-\dots$ , where each term is not greater than the one before? I'm confused because in the example above $\arctan$ 's argument can be negative (i.e., $\sin x$ can be negative).","['calculus', 'functions', 'taylor-expansion']"
3960729,How to prove that following algorithm is correct?,"Suppose I have some $n$ numbers which are powers of $2$ , say $a_1,a_2,a_3.....a_n$ which are not necessarily all distinct. I have option to give them any sign. I have to find if I can make their sum after that equal to $num$ . I have following algorithm which I am sure will work, by lot of arguments and verification, but i am not able to prove it: Sort the array and star. Traverse from highest element to lowest element. Let initialize another variable $temp = 0$ If $temp>num$ then subtract $a_i$ else add $a_i$ . If in end $temp=num$ then it's possible to assign signs such that we can make $num$ out the array else not possible. See an example in comment section. How can I prove it?","['recreational-mathematics', 'discrete-mathematics', 'algorithms']"
3960743,Prove that $\frac{d}{dx}e^x=e^x$,"I would like to know how to prove that $$\frac{d}{dx}e^x=e^x$$ An argument I've seen used is that this is the defnition of the number $e$ ; we can say that for some number, let's call it $a$ , the derivative of $a^x$ is $a^x$ and we've just called this number $e$ . But a problem I have with this approach is that we are assuming that there exists a number $a$ that satisfies this property; how do we know it exists in the first place? I've also seen an approach used that I think is very circular in its reasoning; they use the Maclaurin expansion of $e^x$ to prove by first principles that $\frac{d}{dx}e^x=e^x$ . But the derivation of this series makes use of the very fact that $\frac{d}{dx}e^x=e^x$ . Any other proof will be very appreciated; an explanation of the $2$ above proofs would also be very useful. Thank you for your help.","['proof-explanation', 'alternative-proof', 'calculus', 'derivatives', 'exponential-function']"
3960788,A simple finite field extension only has finitely many intermediate fields,"Let $L/K$ be a finite and simple field extension, so $L = K(\alpha)$ and $[L:K] = n$ . Show that $L/K$ only has finitely many intermediate fields $M$ . I'd like to know if my following proof is correct or how I could improve it: Suppose $L = K(\alpha)$ . For every intermediate field $M$ let $f_M \in M[X]$ be the minimal polynomial of $\alpha$ over $M$ . Then for every $M, M'$ we have $M \subseteq M' \iff f_M \in M'[X]$ . Now I wanna show that every $f_M$ uniquely defines the corresponding intermediate field $M$ : Let $M'$ be another intermediate field s.t $f_M \in M'[X]$ . Then by the above we have $M \subseteq M'$ and by the definition of $f_M$ and $f_{M'}$ it follows that $\deg(f_{M'}) \leq \deg(f_M)$ . Since every root of $f_{M'}$ in $M'$ is also a root of $f_M$ (I'm not sure about this one so in case this isn't necessarily true some clarification would be great!), we get $f_{M'} \mid f_M$ . However, since both polynomials are irreducible and monic, we have $f_M = f_{M'}$ which gives us $M = M'$ . Similarly we have that $f_M  \mid f_K$ in $L[X]$ for every $M$ . Now, since $L[X]$ is a UFD, we get: $$f_K = \prod_{K \subseteq M \subseteq L}f_M.$$ Since this factorization is unique and all factors are monic and irreducible there are no units in the product so we get: $$\deg(f_K) = \sum_{K \subseteq M \subseteq L}\deg(f_M) = n$$ which means there are only finitely many intermediate fields.","['field-theory', 'minimal-polynomials', 'abstract-algebra', 'extension-field']"
3960837,Why doesn't Ramanujan summation converge for the geometric series?,"I wanted to verify that $$ \frac{1}{2} + \frac{1}{4} + \frac{1}{8} +\cdots  = 1 $$ Using Ramanujan summation. Recall that the Ramanujan summation formula is that $$ f(1) + f(2) + f(3) + \cdots = - \frac{f(0)}{2} + i \int_0^{\infty} \frac{f(is)-f(-is)}{e^{2\pi s} - 1} ds $$ Here was have $f(n) = 2^{-n}$ So the result should show that: $$ \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots  =   -\frac{1}{2}+ i \int_{0}^{\infty} \frac{2^{-is}-2^{is}}{e^{2\pi s} - 1} ds$$ I evaluated the RHS in Wolfram Alpha and surprisingly! I do not get an answer of $1$ . See: here Instead I get a result that is very close to $-0.44$ with some change. Why might this be so? Is it because of internal floating point error on Wolfram's end, or does Ramanujan summation just not sum some convergent series? (And if Ramanujan summation doesn't sum some series: what is the significance of their implied R-sum values?)","['divergent-series', 'summation', 'sequences-and-series']"
3960843,"Prove that the function $ \zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1 $ is infinitely differentiable in $(1, \infty) . n=1$","Prove that if $a>1$ and $k \geq 1,$ then a) $\sum_{n=2}^{\infty} \frac{(\log n)^{k}}{n^{a}}<\infty$ b)  Prove that the function $$
\zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1
$$ is infinitely differentiable in $(1, \infty) . n=1$ Hint: Use part (a) to prove part (b). You can use part (a) to prove (b), even if you do not know how to prove (a). My try: a)  As $\frac{(\log n)^{k}}{n^{a}}$ in monotonically decreasing because $\ln n$ grows slowly than $n^{a}$ . Is there a rigorous way to prove it? I tried in this way that $\frac{d}{dx}\left(\frac{(\log x)^{k}}{x^{a}}\right)=\frac{\frac{k}{x}(\log x)^{k-1}-a x^{a-1}}{x^{2 a}}=\frac{k(\log x)^{k-1}-a x^{a}}{x^{2 a+1}}$ and in this way inductively I was trying to see that $f(x)<f(y)$ whenn $x>y$ for some large $y$ . But not convinced fully. Thus we can use Cauchy's Condensation test, considering $f(n)=\frac{(\log n)^{k}}{n^{a}}$ . $\begin{aligned} 2^{n} f\left(2^{n}\right)=& \frac{2^{n}\left(\log 2^{n}\right)^{k}}{\left(2^{n}\right)^{a}}=\frac{\sin n^{k}(\log 2)^{k}}{2^{n(a-1)}} \\ &=\frac{n^{k} c}{2^{d n}} ; \text { where }\left(\log {2}\right)^{k}=c \text { finite constant }, d=a-1>0 \text { finite constant .}\end{aligned}$ Now, $\sum f(n)$ converges iff $\sum 2^{n} f\left(2^{n}\right)$ converges
iff $\sum \frac{n^{k}}{2^{d n}}$ converges.
By Ratio test the last series converges and hence the result. For part b) I can see that $h(x)=\frac{1}{n^{x}}=n^{-x}$ then $h'(x)=-n^{-x}\log n$ and $h''(x)=n^{-x}(\log n)^2$ and so on. Then I can see that the series with term by term differentiation of $\zeta(x)$ is convergent. How to conclude my answer from there?","['analysis', 'real-analysis', 'sequences-and-series', 'power-series', 'derivatives']"
3960894,Application of Induction in the analyze of the convergence a sequence defined recursive.,"Let $\left\{a_{n}\right\}$ be defined recursively by $$
a_{n+1}=\frac{1}{4-3 a_{n}}, \quad  n \geq 1
$$ Determine for which $a_{1}$ the sequence converges and in case of convergence find its limit. My approach: Note that $$a_{n +1}=\frac{1}{4-3a_{n}}, \quad n\geq 1$$ so, firstly  I would like to find $a_{n}$ . Now, I was trying to find a  pattern but I can't find this \begin{eqnarray*}
n=1 &\implies & a_{2}=\frac{1}{4-3a_{1}}=\frac{(3^{2-1}-1)-(3^{2-1}-3)a_{1}}{(2^{2})-(3^{2}-6)a_{1}}\\
n=2 & \implies & a_{3}=\frac{1}{4-3a_{2}}=\frac{1}{4-3 \left( \frac{1}{4-3a_{1}}\right)}=\frac{4-3a_{1}}{4(4-3a_{1})-3}\\
\vdots &\implies & \vdots \\
\end{eqnarray*} If I know $a_{n}=a_{n}(a_{1})$ , so I can analyze the denominator for the conclude when $a_{n}$ is not defined. How can find $a_{n}$ ? Also I know this problem was answered here . But I think, we can find an elementary solution using induction on $n$ .","['calculus', 'real-analysis']"
3960895,"How to calculate $\lim_{n \to \infty} \int_{[n,n+1)} f(x) dx$ if $f\in L^1(\mathbb{R})$","Let $f \in L^1(\mathbb{R})$ . I need to calculate the following integral: $$\lim_{n \to \infty} \int_{[n,n+1)} f(x) dx$$ but I don't really how to begin with... I would be really thankful if someone could give me a hint. I think that $$\lim_{n \to \infty} \int_{[n,n+1)} f(x) dx$$ should be equal to $$ \int_{\mathbb{R}} f(x) dx$$ but I don't get to a solution... I know it's value is finite because $f$ is an integrable function. Is it possible to say anything else about this integral?","['measure-theory', 'lebesgue-integral']"
3960899,Hasse-Minkowski for cubic forms,"We know that an analogue of the Hasse-Minkowski theorem does not hold for all cubic forms, e.g. because Selmer's cubic: $$
3x^3 + 4y^3 + 5z^3 = 0
$$ has solutions over $\mathbb{R}$ and $\mathbb{Q}_p$ for all $p$ , but no solutions over $\mathbb{Q}$ . My questions are: Can we find a (non-trivial) class of cubic forms where an analogue of the Hasse-Minkowski theorem does hold? Is there any intuition for why the local-global principle holds for quadratic but fails for cubic forms? Are there higher degree forms where the local-global principle holds again? Are questions like these addressed anywhere in the literature? Many thanks.","['number-theory', 'p-adic-number-theory', 'elementary-number-theory', 'diophantine-equations', 'abstract-algebra']"
3960925,Primes in solutions to Pell-type equations,"What is known about primes in solutions to Pell-type equations? In particular, consider the negative Pell equation $x^2 - 5 y^2 = -1$ .
As far as I've been able to check
(in the first $4000$ solutions) the only positive-integer solution with $y$ prime  is $x=38$ , $y=17$ ,
but I don't see any obvious reason why this should be the case.","['number-theory', 'pell-type-equations', 'elementary-number-theory']"
3960943,Inertia degree of primes in p-adic extensions,"I'm reading through some number theory and ran across a theorem where the proofs referenced were incomprehensible to me, and I was hoping there might be a simpler proof than slogging through another $\approx$ 100 pages of background. The statement is that if you have a finite extension $K$ of the $p$ -adic rationals $\mathbb{Q}_p$ , and $\pi$ is a uniformizer of $\mathcal{O}_K$ , then $$[\mathcal{O}_K/\pi\mathcal{O}_K : \mathbb{Z}_p/p\mathbb{Z}_p] = v_p(\vert N(\pi)\vert)$$ In the non- $p$ -adic case, where $K'$ is some extension of $\mathbb{Q}$ and where $\pi \vert p$ , showing that $[\mathcal{O}_{K'}/\pi\mathcal{O}_{K'}:\mathbb{Z}/p\mathbb{Z}] = v_p(\vert N(\pi)\vert)$ is more straightforward to me: Since $\mathcal{O}_{K'}$ is a $\mathbb{Z}$ -module, I can imagine it as $\mathbb{Z}^n$ , and then the quotient by $\pi$ is like a lattice, with $\pi$ as matrix giving a basis of that lattice, and the volume of the fundamental parallelepiped is the determinant of that matrix, which is $N(\pi)$ . The number of integer points in the fundamental parallelepiped will be equal to its volume, showing that $\vert \mathcal{O}_{K'}/\pi\mathcal{O}_{K'}\vert = \vert N(\pi)\vert$ . Since $\vert\mathbb{Z}/p\mathbb{Z}\vert =p$ , that gives the result. But this doesn't seem to extend to the $p$ -adic case. $\mathbb{Z}_p^n$ doesn't look like a lattice to me, and even if it were, I'm not sure how to relate the volume to the number of points in it. Maybe I could argue that $\mathbb{Z}$ is a subring of $\mathbb{Z}_p$ , and that $\mathbb{Z}_p/p\mathbb{Z}_p$ can take representatives of each coset from $\mathbb{Z}$ , so I can treat $\mathcal{O}_K/\pi\mathcal{O}_K$ as a $\mathbb{Z}$ -module. But then I can't seem to treat $\pi$ as a linear operator without causing problems with the determinant. Or maybe, since $\mathbb{Z}_p/p\mathbb{Z}_p \cong \mathbb{Z}/p\mathbb{Z}$ , I could show that $\mathcal{O}_K/\pi\mathcal{O}_K\cong \mathcal{O}_{K'}/\pi\mathcal{O}_{K'}$ . Certainly I can say that every linear operator over $\mathbb{Z}_p/p\mathbb{Z}_p$ is a linear operator over $\mathbb{Z}/p\mathbb{Z}$ , but I'm not sure that every element of $\mathcal{O}_K$ will form a linear operator that is equal to one from $\mathcal{O}_{K'}$ . In fact this is almost certainly false, so the quotient by $\pi$ must be crucial, but I don't know how to incorporate this fact. Is there a relatively simple proof of this, or any way to complete my attempts?","['number-theory', 'p-adic-number-theory', 'algebraic-number-theory', 'ramification']"
3960980,Function that forms an Arithmetic Sequence,"I've been working on this problem for a while: Let $n > 2$ be an integer. Let $f$ be a function from the set of all integers to itself with the following property: If the integers $a_1, a_2, a_3, \dots, a_n$ form an arithmetic progression, then the numbers $f(a_1), f(a_2), \dots f(a_n)$ form an arithmetic progression in some order (that's possibly constant). Determine all values $n$ such that the only functions $f$ with the above property are the functions in this form: $f(x)=bx+c$ , where $b$ and $c$ are integers. My work is most likely trivial, but I wrote $a_1, a_2, \dots a_n$ as $a_1, a_1+x, a_1+2x, \dots, a_1+(n-1)x$ . Then, applying $f(x)=bx+c$ to each term in the sequence, I got that the values of the sequence were $$ba_1+c, b(a_1+x)+d, \dots, b(a_1+(n-1)x)+d.$$ However, I'm unable to make more progress. Thanks for any form of help!","['functional-equations', 'functions', 'sequences-and-series']"
3961003,"To what ring is $\mathbb{Z}[X,Y,Z]/(X-Y, X^3-Z)$ isomorphic?","The problem: Let $(\mathbb{Z}[x,y,z],+,\cdot)$ be the ring of polynomials with coefficients in $\mathbb{Z}$ in the variables $x$ , $y$ and $z$ and the obvious operations $+$ and $\cdot$ . Let $(x-y, x^3-z)$ be the ideal generated by $x-y$ and $x^3-z$ . Find the ring to which $\mathbb{Z}[x,y,z]/(x-y,x^3-z)$ is isomorphic. A similar question has been asked before: Which ring is $R[X,Y,Z,T]/(X-Y^2,T-Y^4,T^3-Z)$ isomorphic to? . Using the same approach, we set $x=y$ and $x^3=z$ and substitute for $y$ and $z$ . Then we find $\mathbb{Z}[x,x,x^3]$ which is just $\mathbb{Z}[x]$ . I'm trying to find out why this approach would work, i.e., formalise it in a proper proof. My idea is to use the first isomorphism theorem, but I get stuck near the end. I would very much appreciate if someone could help fill in this gap, check my proof for mistakes or provide additional (intuitive) information as to why the above method works. My proof: Consider the map $\varphi : \mathbb{Z}[x,y,z] \to \mathbb{Z}[x] : p(x,y,z) \mapsto p(x,x,x^3)$ . This is a homomorphism, since it obeys the relations $\varphi(p) + \varphi(q) = \varphi(p+q)$ and $\varphi(p) \cdot \varphi(q) = \varphi(p \cdot q)$ for all $p,q \in \mathbb{Z}[x,y,z]$ (verifying these is quite trivial). Now take any $q \in \mathbb{Z}[x]$ and define a $p \in \mathbb{Z}[x,y,z]$ such that $p(x,y,z) = q(x)$ . Since $\varphi(p(x,y,z)) = p(x,x,x^3) = q(x)$ , $\varphi$ is surjective and it follows that $\mathbb{Z}[x] \subseteq \varphi(\mathbb{Z}[x,y,z])$ . It is obvious that $\varphi(\mathbb{Z}[x,y,z]) \subseteq \mathbb{Z}[x]$ and, therefore, we must have $\varphi(\mathbb{Z}[x,y,z]) = \mathbb{Z}[x]$ . Lastly, we must show that $\ker(\varphi) = (x-y, x^3-z)$ . For brevity's sake I will denote $I := (x-y,x^3-z).$ Let $p \in I$ . Then there must exist $q_1, q_2 \in \mathbb{Z}[x,y,z]$ such that $p = q_1(x-y) + q_2(x^3-z)$ . It follows that: \begin{align*}\varphi(p(x,y,z)) &= \varphi(q_1(x,y,z)(x-y) + q_2(x,y,z)(x^3-z))\\ &= q_1(x,x,x^3)(x-x) + q_2(x,x,x^3)(x^3-x^3) = 0.\end{align*} Thus $p \in \ker(\varphi)$ and $I \subseteq \ker(\varphi)$ , since $p$ was chosen arbitrarily in $I$ . Now, take a $q \in \ker(\varphi)$ . Then $\varphi(q(x,y,z)) = q(x,x,x^3) = 0$ . Somehow we must get that $q \in I$ so that we can conclude $\ker(\varphi) \subseteq I$ and thus $I = \ker(\varphi)$ , but I seem to get stuck on every path. Proof by contradiction or by contraposition also seems fruitless. What am I missing? With all of the above, we may apply the first isomorphism theorem to conclude that: $$\mathbb{Z}[x,y,z]/I = \mathbb{Z}[x,y,z]/\ker(\varphi) \cong \varphi(\mathbb{Z}[x,y,z]) = \mathbb{Z}[x]$$","['ring-isomorphism', 'abstract-algebra', 'polynomial-rings']"
3961031,Why $\lim_{r\to 1^-} \int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta=-\infty$?,"I am studying Greene and Krantz' ""Function theory in one complex variable"". In section $13.4$ , they want to prove that, given $f\in H^p, p\in (0,+\infty),$ if the radial limit function $\tilde f$ is zero on a set of positive measure then $f\equiv 0$ .
Their proof is quite short: if $f\not\equiv 0$ , we can assume wlog $f(0)\neq 0$ and then by Jensen's formula (given $r<1:\forall \vartheta f(re^{i\vartheta})\neq 0$ ) $$-\infty<\ln(|f(0)|)\le\int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta/2\pi $$ I have a problem with the following passage: As $r\to 1^-$ through such $^1$ values, the right hand side of this expression tends to $$\frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde{f}(e^{i\vartheta})|)d\vartheta=-\infty$$ I am unsure about one thing: Why is the limit equal to $\frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde f|)$ ? I tried using dominated convergence and Fatou's lemma, to no avail. Thanks for the help, and Happy Holidays! Note: for completeness, the proof then concludes by noting that we reached a contradiction and thus $f$ cannot be $\not\equiv 0$ . $^1$ : meaning $r$ such that $f$ is not zero on $|z|=r$ .","['complex-analysis', 'measure-theory', 'convergence-divergence']"
3961041,Is the weak topology on $\mathbb{R}^{\infty}$ the same as the box topology?,"Let $\mathbb{R}^{\infty}=\bigcup\limits_{n=1}^{\infty}\mathbb{R}^{n}$ be the subset of $\mathbb{R}^{\omega}$ consisting of all sequences which are nonzero for only finitely many terms.  Give $\mathbb{R}^{\infty}$ the weak topology, that is, $A\subset \mathbb{R}^{\infty}$ is open iff $A\cap \mathbb{R}^{n}$ is open in $\mathbb{R}^{n}$ for each $n$ .  Is $\mathbb{R}^{\infty}$ a subspace of $\mathbb{R}^{\omega}$ with the box topology? The reason why I think this is true is because it looks like the box topology is the weak topology with respect to the natural inclusions $\mathbb{R}^n\to\mathbb{R}^{\omega}$ [Edit: I have doubts about this particular claim.]","['general-topology', 'box-topology']"
3961045,Differential equation with the Substitution method,"I'm trying to solve this equation below : $$ y' = \sin\left(\frac{y}{x}\right) + \frac{y}{x}  \tag 1 $$ The first step was to to substitute $\frac{y}{x}$ with $u$ => $ u = \frac{y}{x}$ => $y' = u'x + u \tag 2$ Then with $(1) = (2)$ ,  I got at the end $$\dfrac{\ln\left(\cos\left(y\right)+1\right)-\ln\left(1-\cos\left(y\right)\right)}{2} = \ln|x| + C$$ And after more simplification : $$\frac{\cos(y)}{1-\cos(y)} = 2Cx   $$ How can I get the general function from this equation ?","['ordinary-differential-equations', 'integro-differential-equations']"
3961057,formula for relating number of successes to number of tries,"Imagine we have a jar of marbles, where there are 10 different colors of marbles in the jar.
Let N be the number of marbles drawn in a sample and
K be the number of distinct colors in the sample.
Let C be the total number of colors in the jar (which is 10 in this example). We know that:
K = F(C,N) + chance. Intuitively, the more marbles we draw at a time, the more colors we are likely to obtain, up to the maximum. This strikes me as a binomial. I am looking for a formula to express in more detail how
K = F(C,N).
An obvious feature is that C is the maximum value. So I figure this is a model where K is an increasing proportion of C as N increases. I already know how to simulate the data, but wish to provide an expression of K depending on C and N. Any suggestions will be appreciated. Simulated data for the situation I intend will be as follows. I assume sampling with replacement. For this example, we have 10 colors in the jar (C=10). Note that we start by drawing one marble from the jar, which gives us a sample where N=1 and K=1. The number of colors found in a sample depends on the size of the sample. In this case it is seq(1:20). In this particular simulation, we have an equal representation for each color. The snip of R code I use to simulate the data is: ###snip of modified code for stack exchange exchange T <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10) for (i in 1:N){ for (s in 1:n.sims) { plot.sample <- sample(T, size=N[i]) K[s,i] <- length(unique(plot.sample)) } } Note that the line is just a loess plot at this point and the proper function would level off at C, the maximum number of colors. If I use the suggested formula
K=N*C/(N+C-1)
I get the figure on the right, which has the right shape, but finds colors very slowly. If I estimate the function with a scaling coefficient, I get the following graph. Excellent fit: explained variance = 97%, though the visual suggests there could be a better function?",['probability']
3961079,Least concave majorant of a non-decreasing function,"Consider a non-decreasing discrete function $f$ defined over $\{0,1,\ldots,n\}$ .
Let $g$ be the least concave majorant of $f$ . Assume that for some $i\in \{1,\ldots,n-1\}$ , $g(i)\neq f(i)$ . How to prove that $g$ is linear over $\{i-1, i ,i+1Â \}$ , i.e., $g(i)-g(i-1) = g(i+1)-g(i)$ ? I tried the following: Suppose that $g(i)-g(i-1) \neq g(i+1)-g(i)$ , then we must have that $g(i)$ is above the line that interpolates $g(i-1)$ and $g(i+1)$ by concavity of $g$ . Then, I think that one might be able to find a contradiction with the fact that $g$ is the least concave majorant but cannot formalize it.","['convex-analysis', 'analysis', 'real-analysis']"
3961090,When can we interchange the order of limits of the expected value of products of random variables?,"Suppose that the sequence of random variables $X_n$ converges to a limiting random variable $X$ , and let $Y$ be an arbitrary random variable. Is there any specific mode of convergence of the sequence $X_n$ that guarantees that $$\lim_{n \to \infty} E[X_n Y] = E[X Y]?
$$ I'm pretty sure that convergence in distribution is not good enough, but I'm not sure about (e.g.) convergence in probability or convergence a.s. What about if there are multiple sequences (i.e. $E[X_n Y_n]\stackrel{?}{\to} E[X Y]$ ), or if $X$ or $Y$ are constant or otherwise constrained? (I know that if $Y$ is constant, then it suffices for $X_n$ to converge in mean.)","['expected-value', 'limits', 'convergence-divergence', 'probability-theory', 'random-variables']"
3961097,Need help understanding formula for probability that something good happens before something bad happens.,"My book says this. Consider a sequence of independent trials, each of which can
be classified as good, bad, or neutral, which happen (on any given trial) with
probabilities $p, q,$ and $1 âˆ’ p âˆ’ q$ , respectively. (We do not necessarily have $q = 1 âˆ’ p$ here, although that case is allowed.) Then the probability that
something good happens before something bad happens is $p/(p + q)$ . My question is, how did they derive this formula $p/(p+q)$ for probability that something good happens before something bad happens? can someone show me a derivation/explain to me this?",['probability']
3961100,What is the probability of a randomly chosen integer being even?,"There are a number of sets in math that have infinite cardinality but still have a superset. For example, the set of even integers has infinite cardinality, but it is a subset of all integers. The cardinality of the set of integers is infinity, but it has a superset, real numbers. These are just a few examples. So my question is if a random number was chosen from one of these supersets, what is the probability that it would be a part of one of its infinite subsets? There is an infinity of possibilities that are inside the subset, but also an infinity that is not.","['infinity', 'probability']"
3961122,$12$ points are arranged around a circle. How many ways can you create two pairs of distinct triangles that overlap?,"So this question was actually in three parts including a part that asked for non-overlapping triangles which I correctly calculated to be $2772$ . Take this away from the total amount of triangles $\frac{12C3\cdot9C3}{2}=9240$ gives the correct answer 6468. However , just for practice how would you calculate this from scratch without using the non-overlapping triangles? I think my attempt is very close but I'm not getting the answer: Choose the first triangle first by pinning down and disregarding any two distinct points from the circle effectively dividing it into two segments (major and minor). In this case to make it systematic, choose them 4 points apart (that is the minimum because 3 points apart will not work as they will not overlap) Then select one point in the minor segment to complete the triangle $**2C1**$ To make the other triangle, select another point in the minor segment to ensure overlapping $**1C1**$ To complete the other triangle, select two more points from the major segment $**8C2**$ Now repeat this process by gradually increasing the gap between the pinned-down points so you have $$\frac{(2C1\cdot1C1\cdot8C2+3C1\cdot2C1\cdot7C2+4C1\cdot3C1\cdot6C2+5C1\cdot4C1\cdot5C2+6C1\cdot5C1\cdot4C1+7C1\cdot6C1\cdot3C2+8C1\cdot7C1\cdot2C2)}{2}\cdot12 = 5442$$ The answer is 6468 Edit: the triangles are selected from distinct points (no repetition between the two triangles)",['combinatorics']
3961162,Help with understanding Duffing's oscillator,"I'm working through some notes regarding analysis of some weakly perturbed oscillators. Among these is Duffing's oscillator which is governed by $$ \frac{d^2y}{dt^2}+y+\epsilon y^3=0, \; \; 0<\epsilon << 1 $$ Now to leading we have \begin{align*}
   \frac{d^2y_0}{dt^2}+y_0=0
 \end{align*} with general solution $$ y_0 = a\cos t + b \sin t$$ which can be written w.l.o.g as $ y_0 = c \cos(t+\phi), \; \; c>0$ . So far so good. But now to understand the nature of the perturbation the notes make use of the Poincare-Lindstedt method, which relies on the observation that the period of the perturbed oscillator is $$2\pi(1+o(1))$$ as $\epsilon \rightarrow 0$ . Here is my first question: how exactly was this observation made? Next we define $Y(\tau; \epsilon) = y(t; \epsilon)$ where $\tau = t/\chi(\epsilon)$ and $\chi(\epsilon)$ is such that $Y$ is $2\pi$ periodic in $\tau$ . Then, expanding $Y$ and $\chi$ in powers of $\epsilon$ we have $$Y(\tau;\epsilon) \sim Y_0(\tau;\epsilon)+\epsilon Y_1(\tau;\epsilon)+...$$ and $$\chi(\epsilon) \sim 1+\epsilon\chi_1 + \epsilon^2\chi_2+...$$ Here the $Y_i$ are $2\pi$ periodic in $\tau$ . To leading order the solution has the same form, $Y_0 = a\cos(\tau + \phi)$ . Going up to first order in $\epsilon$ , we have \begin{align*}
\frac{d^2Y_1}{d\tau^2} + Y_1 &= -2\chi_1Y_0-Y_0^3 \\ 
&= -2\chi_1a\cos(\tau+\phi) -\frac{a^3}{4}\left \{3\cos(\tau+\phi)+\cos\left(3(\tau+\phi)\right)\right\},
\end{align*} where we have substituted for $Y_0$ and used a Fourier expansion. Then here is where I really have trouble. At this point then notes say that in order for $Y_1$ to be $2\pi$ perioduc in $\tau$ we must eliminate $\cos(\tau+\phi)$ terms, which gives us an algebraic expression for $\chi_1$ . But why must we eliminate such terms in order for $Y_1$ to be $2\pi$ periodic? I don't understand this.","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
3961208,Notation for direct sum of direct sums,"Suppose you have a direct sum of two terms $A$ and $B$ : $$ A \oplus B.$$ Now suppose that in fact $B = \bigoplus_i B_i$ . Then I guess we could write the above sum as $$ A \oplus \bigoplus_i B_i.$$ However, this looks terrible -- much worse than, for instance, $$ A \bigoplus_i B_i. $$ Is there a precedent to writing the latter, or some other notation better than the former? Cheers. P.S.: what about when B is a direct product instead of a direct sum?","['notation', 'abstract-algebra', 'index-notation']"
3961211,"If $f$ is continuous at $c$ and $f â€²(c) = 0$, then there exists an $h > 0$ such that $f$ is differentiable in the interval $(c â€“ h, c + h)$.","My book states the following: if $f$ is continuous at $c$ and $f â€²(c) = 0$ , then there exists an $h > 0$ such that $f$ is differentiable in the interval $(c â€“ h, c + h)$ . But I don't understand this. It is not as if $f'$ is given to be continuous, rather $f$ is given continuous and differentiable at $x=c$ . So how can we possibly comment about the existence of $f'$ in the neighborhood of $c$ too? Note that f is defined on an open interval I and c belongs to I.","['maxima-minima', 'calculus', 'derivatives']"
3961215,"Surface integral on $S=\{(x,y,z)|x^2+y^2+z^2=1,x+y+z\leq 1\}$","Let $S=\{(x,y,z)|x^2+y^2+z^2=1,x+y+z\leq 1\}$ , $F(x,y,z)=(x,0,-x)$ and $n(x,y,z)$ be the unit normal vector of $S$ such that $n(0,0,-1)=(0,0,-1)$ .
I want to evaluate $\displaystyle \iint_{S}F(x,y,z)\cdot n(x,y,z)dS$ . My Attempt Let $f(x,y,z)=x^2+y^2+z^2-1$ . Then $n$ can be calculated by $n=\frac{\nabla f}{|\nabla f|}=(x,y,z)$ . This satisfies the condition stated in the problem. Therefore we have $\displaystyle \iint_{S}F(x,y,z)\cdot n(x,y,z)dS=\iint_{S}(x^2-zx)dS$ . Now we need to calculate this surface integral, but I'm encountering issues. According to this website , I have two options. One option is to find an orthogonal projection of $S$ . The other option is to find a parameterization of $S$ . However, I couldn't do either of them. Is there a simple expression for them? Any help is appreciated.","['surface-integrals', 'multivariable-calculus', 'multiple-integral']"
3961257,Convolution of tight measures is tight,"Let $E$ be a Banach space (think of $E=\mathbb R^d$ , if it's easier to understand the following for you), $(\mu_n)_{n\in\mathbb N}$ and $(\nu_n)_{n\in\mathbb N}$ be tight sequences of finite nonnegative measures on $\mathcal B(E)$ . I would like to conclude that the sequence $(\mu_n\ast\nu_n)_{n\in\mathbb N}$ of convolutions is tight as well. Let $\varepsilon>0$ . By the tightness assumption, there is a compact $K\subseteq E$ with $$\max\left(\sup_{n\in\mathbb N}\mu_n(K^c),\sup_{n\in\mathbb N}\nu_n(K^c)\right)<\varepsilon\tag1.$$ Now, $K+K$ is clearly compact and \begin{equation}\begin{split}(\mu_n\ast\nu_n)(K+K)&=(\mu_n\otimes\nu_n)(\{(x,y)\in E^2:x+y\in K+K\})\\&\ge(\mu_n\otimes\nu_n)(K\times K)=\mu_n(K)\nu_n(K)\end{split}\tag2\end{equation} for all $n\in\mathbb N$ . If I assume that $\mu_n,\nu_n$ are probability measures for all $n\in\mathbb N$ , then $(2)$ yields $$(\mu_n\ast\nu_n)(K+K)\ge(1-\varepsilon)^2\tag3$$ and hence we obtain the claim. But how can we show the claim in general?","['probability-theory', 'functional-analysis', 'measure-theory']"
3961266,Existence of specific sections of vector bundles over a manifold,"I am trying to do the following exercise from Hirsch, one could say that it's 3 exercises but they are all related so I believe it's best to treat them together: Let $\xi=(E,M,p)$ be an $n$ -plane bundle over a connected $k$ -manifold $M$ . a) If $k<n$ then $\xi$ has a non-vanishing section. Here I belive the idea is just to use the transversality theorem, since we can approximate the zero section map $s$ by mapping the transversal to the zero section $h_k$ and for $h_k$ close enough we will get that $p\circ h_k$ is a diffeomorphism. So we can consider $h_k\circ (p\circ h_k)^{-1}$ , and this will be a section, transversal to the zero section. But then by a dimension argument we have to have that their intersection is empty. b) If $k=n$ and $x\in M$ then $\xi$ has a section vanishing only at $x$ . Here let's divide this into two cases. First suppose $M$ has a non-vanishing section $s$ , then we can consider a function $f:M\rightarrow \mathbb{R}$ such that $f^{-1}(0)=x$ , I believe this is always possible, and just consider the section $fs$ . Now from the next question $c)$ and what we just did, we can assume that $\partial M=\emptyset $ and that $M$ is compact. Now if I remove a point from $M$ I get a non-compact manifold with a non-vanishing section, to be proved, and then we can just do an analogous argument to what we just did. c) If $k=n$ , and $\partial M\neq \emptyset $ or $M$ is non-compact, then $\xi$ has a non-vanishing section. Now let's first assume that $\partial M\neq \emptyset $ and that $M$ is compact . My idea is to try and do something similiar to what was done when we proved that a compact connected manifold with boundary has a non-vanishing vector field. So let's take the double $M'$ of $M$ . This will have a section $s$ with a finite number of zeros, by an analogous argument to what we did in $a)$ , which we denote by $F$ . Here I assume I can create a vector bundle over $M'$ by just taking the fibers of the vector bundle over $M$ . I believe this is possible, but would appreciate some input. We will call this $\xi'$ . Since $M'$ is connected there is a diffeomorphism $\phi :M'\rightarrow M'$ that takes $F$ into $M-M'$ . Then we can consider the map $s\circ \phi^{-1}|M :M\rightarrow \xi'$ that has no zeros. Now I would like to have a vector bundle map that goes from $\xi'\rightarrow \xi $ and covers the identity when we restrict to $M$ . But I'm not sure if this is possible since we want this map to not create zeros from the result we get from $s\circ \phi^{-1}$ , and I'm not sure how to create a map without using some partitions of unity. Now for the non-compact case I am kinda lost, have really no ideas on what to do. I thought about using the trivializing charts and then gluing everything together with partitions of unity, but when we glue things together, how can we do it in a way that doesn't create zeros ? I don't think I can have this control. So I am out of ideas and would appreciate some input. Don't really see where I can use the non-compactness hypothesis. Thanks in advance.","['manifolds', 'differential-topology', 'vector-bundles', 'differential-geometry']"
3961279,"Equiconsistency of euclidean, hyperbolic, and elliptic geometry","Pretty much every text about non-euclidean geometries talks about the various models by Beltrami, Riemann, PoincarÃ©, Klein, and others which demonstrate that if euclidean geometry is consistent, then hyperbolic and elliptic geometry also are consistent.  So far, so good.  I think I understand these arguments. However, there are many sources which claim that it was also proved (by Klein?) that hyperbolic geometry and euclidean geometry are equiconsistent .  My understanding is that this means that if hyperbolic geometry is consistent, so is euclidean - which would place both on an equal footing. My questions now are: Is this also true for elliptic geometry?  Does the consistency of euclidean geometry follow from consistency of elliptic geometry? Where can accessible proofs for these equiconsistency results be found?  (I mean proofs that not only show the well-known direction described in my first paragraph.)","['noneuclidean-geometry', 'geometry']"
3961302,Find $r$ from three moduli,"Determine $r$ in $$x\equiv  15 \pmod{77}\\
x\equiv 16 \pmod{78}\\
x\equiv r \pmod{14}$$ With lists in Excel, I have discovered that $x = 5944$ . $$5944\equiv  15 \pmod{77}\\
5944\equiv 16 \pmod{78}\\
5944\equiv r \pmod{14}\implies r=8$$ This is not an elegant solution. How can I take on the task in a more mathematical way?","['modular-arithmetic', 'discrete-mathematics']"
