question_id,title,body,tags
2717172,Will Fermat's last theorem hold true on the Complex Plane? [duplicate],"This question already has answers here : Solutions in complex number field, instead of $\mathbb{N}$, to Fermat's Last Theorem (2 answers) Closed 6 years ago . Fermat's Last Theorem In number theory, Fermat's Last Theorem (sometimes called Fermat's conjecture, especially in older texts) states that no three positive integers a, b, and c can satisfy the equation $a^n + b^n = c^n$ for any integer value of $n$ greater than two. Now the question is will Fermat's last theorem hold true if we extend the question to the complex plane. Ie when $a$, $b$ or $c$ can be complex numbers. Why or Why not and is there any prove to it?","['number-theory', 'complex-numbers']"
2717176,"Proving that $\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}$","Let $f,g\in C^1([a,b])$ with $a<b$ then prove that $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}$$ It smells  like there is some mean value theorem going around. But I tried it as follows: Indeed, it springs from mean value theorem that There exists  $c_1,c_2\in (a,b)$ such that $$\frac{f(b)-f(a)}{b-a} = f'(c_1)~~~ and 
~~~~\frac{g(b)-g(a)}{b-a} = g'(c_2)$$ Then I have  $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2= f'(c_1)+(g'(c_2))^2\le \max_{t\in [a,b]}\{f'(t)\}+\max_{t\in [a,b]}\{(g'(t)^2)\}$$ Which is however not the required inequality. Can anyone help? how can I improve this ?","['real-analysis', 'inequality', 'functions', 'calculus', 'analysis']"
2717182,How to prove that $\mathbb{P} = \operatorname{Spec}( k[q] ) \cup \operatorname{Spec}( k[q^{-1}] )$?,"I am reading the paper on crystal bases . Let $k$ be a field. On page $11$, it is said that $\mathbb{P} = \operatorname{Spec}( k[q] ) \cup \operatorname{Spec}( k[q^{-1}] )$. How to prove this? Thank you very much.","['algebraic-geometry', 'commutative-algebra']"
2717185,Solve for $\beta$. (Series),"I am proving the least squares estimates of the regression coefficients and I've come across these 2 equations. $$\sum_{i=1}^{n}y_i=\alpha n+\beta \sum_{i=1}^{n}x_i$$ $$\sum_{i=1}^{n}y_ix_i=\alpha \sum_{i=1}^{n}x_i+\beta \sum_{i=1}^{n}x_i^2$$ I am supposed to solve what is $\beta$. The answer given is $$\beta=\frac{n(\sum_{i=1}^{n}x_iy_i)-(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)}{n(\sum_{i=1}^{n}x_i^2)-(\sum_{i=1}^{n}x_i)^2}$$ I've tried many times to work it out by substitution method. But failed. 
It's tedious. Hope someone can help me out. Thanks in advance.","['algebra-precalculus', 'statistics', 'linear-regression', 'sequences-and-series']"
2717192,Four Pillars of Functional Analysis,"I have come across to a statement in many Functional Analysis books saying that ""Hahn Banach theorem, Uniform Boundedness Principle, Open mapping theorem and Closed graph theorem are the four pillars of Functional Analysis"" I don't exactly know why they are so important, maybe these are used in many parts of Functional Analysis further.
can anyone help me, please?
thanks and regards in advance.",['functional-analysis']
2717244,Is there a rapider or more elegant way to evaluate $\int_0^{+\infty} \frac{\cos(\pi x)\ \text{d}x}{e^{2\pi \sqrt{x}}-1}$?,"$$\int_0^{+\infty} \frac{\cos(\pi x)\ \text{d}x}{e^{2\pi \sqrt{x}}-1}$$ First attempt $x\to t^2$ Geometric series by writing the denominator as $e^{2\pi t}(1 - e^{-2\pi t})$ $\cos(\pi t^2) = \Re e^{i\pi t^2}$ This leads me to $$2\sum_{k = 0}^{+\infty} \int_0^{+\infty} t e^{i\pi t^2}e^{-\alpha t}\ \text{d}t$$ Where $\alpha = 2\pi (k+1)$. Now I thought about writing it again as $$-2\sum_{k = 0}^{+\infty}\frac{d}{d\alpha} \int_0^{+\infty} e^{i\pi t^2}e^{-\alpha t}\ \text{d}t$$ The last integral can be evaluated with the use of the Imaginary Error Function, hence a Special Function method. Yet it doesn't seem me the best way. Second Attempt Basically like the previous one with the difference that $\cos( \cdot )$ stays as it; $\pi t^2 \to z$; And this brings $$-\frac{1}{\sqrt{\pi}}\frac{d}{d\alpha} \sum_{k = 0}^{+\infty}\int_0^{+\infty} \frac{\cos(z)}{z} e^{-\alpha \sqrt{\frac{z}{\pi}}}\ \text{d}z$$ But in both cases what I am thinking are just numerical methods. Or at least I could give a try with the stationary phase but... meh. I don't know if I can use residues for this, actually. Even if taking a look at the initial integral, there is this additional way: $$\frac{1}{e^{2\pi t} -1} = \frac{1}{(e^{\pi t}+1)(e^{\pi t}-1)}$$ Which for example has a pole at $t = +i$... But using residues I would obtain $$\pi \cos(\pi)$$ Where as the correct numerical result (which I checked with Mathematica) is $$\color{blue}{0.0732233(...)}$$ And it seems there is not a closed form for this. Any hint/help?","['residue-calculus', 'calculus', 'integration', 'definite-integrals', 'special-functions']"
2717269,Product rule for quaternions,"The following exercise is from Naive Lie Theory by Stillwell, and is designed (I assume) to illustrate how non-commutativity of quaternions affects the product rule. The definition of derivative for any function $c(t)$ of a real variable $t$ is 
$$c'(t) = \lim_{\Delta t \to 0} \frac{c(t + \Delta t) - c(t)}{\Delta t}.$$ (1) By imitating the usual proof of the product rule, show that if $c(t) = a(t)b(t)$ then $$c'(t) = a'(t)b(t) + a(t)b'(t).$$
(Do not assume the product rule is  commutative.) (2) Show also that if $c(t) = a(t)^{-1}$, and $a(0) = 1$, then $c'(0)= -a'(0),$ again without assuming that the product is commutative. (3) Show, however, that if $c(t)=a(t)^2$ then $c'(t)$ is not equal to $2a(t)a'(t)$ for a certain quaternionic-valued function $a(t)$. Parts (1) and (2) are straightforward, but I just cannot get the certain quaternionic-valued function for part (3). Does anyone know what it is? For the function $a(t)$ I've tried: $it, jt, (i+j)t, ijt, e^jt$ and a whole bunch of others without success. If anyone knows it, i'd be grateful, thanks.","['derivatives', 'quaternions', 'lie-groups']"
2717277,How to solve $\frac{1}{1000.1998}+\frac{1}{1001.1997}+\cdots+\frac{1}{1998.1000}$,"The question is: If $$A=\frac{1}{1.2}+\frac{1}{2.3}+\frac{1}{3.4}+\cdots+\frac{1}{1997.1998}$$ and 
$$B=\frac{1}{1000.1998}+\frac{1}{1001.1997}+\cdots+\frac{1}{1998.1000}$$
then what is the value of $\frac{A}{B}$? I could figure out that $A=\frac{1997}{1998}$, but I have no idea how to proceed with $B$. Could somebody help? Thanks for any help :-)",['sequences-and-series']
2717294,Question on coefficients of polynomials [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $$P_0(x)=x^3+313x^2-77x-8$$ For integers $n\ge1$, define $$P_n(x)=P_{n-1}(x-n)$$. Then what is the coefficient of $x$ in $P_{20}(x)$ ? I have absolutely no idea how to proceed. Could somebody help? Thanks for any help :-)","['algebra-precalculus', 'polynomials']"
2717295,Prove $\sum\limits_{n=2}^\infty \frac {1} {n\ln(n)\ln\ln{n}}$ is divergent. [duplicate],"This question already has answers here : Another simple series convergence question: $\sum\limits_{n=3}^\infty \frac1{n (\ln n)\ln(\ln n)}$ (4 answers) Closed 4 years ago . Evaluate if the following series is convergent or divergent: $\sum\limits_{n=2}^\infty \frac {1} {n\ln(n)\ln\ln{n}}$. I cannot properly understand the notation that the book employs here $\ln\ln(x)$, but I guess it is referring to $\ln(\ln(x))$. Assuming $\ln \ln(x)=\ln(\ln(x))$, I used the Weierstrass's or comparasion test to evaluate the series: $$\sum\limits_{n=2}^\infty \frac {1} {n\ln(n)\ln\ln{n}}<\sum\limits_{n=2}^\infty \frac {1} {n\ln(n)\ln(n)}=\sum\limits_{n=2}^\infty \frac {1} {n\ln^2{n}}<\sum_\limits{n=2}^{\infty}\frac{1}{n^2},$$ proving the series converge.
However the solution point out the series diverge. I have already proved the convergence $\sum_\limits{n=2}^{\infty}\frac{1}{n^2}$. Question: What am I doing wrong? How can I prove the series diverge? Is $\ln\ln(x)=\ln(\ln(x))$ meant by the author of the book?","['real-analysis', 'logarithms', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2717321,"Prob. 14, Sec. 3.4, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: The supremum of a bounded sequence not in the range","Here is Prob. 14, Sec. 3.4, in the book Introduction to Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let $\left( x_n \right)$ be a bounded sequence and let $s \colon= \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. Show that if $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$, then there is a subsequence of $\left( x_n \right)$ that converges to $s$. My Attempt: As $s-1 < s$, so there exists a natural number $n_1$ such that 
  $$ s-1 < x_{n_1} \leq s. $$
  But $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_1} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have 
  $$ s-1 < x_{n_1} < s. \tag{1} $$ As $s-1/2 < s$, so there exists a natural number $n_2$ such that 
  $$ s - \frac{1}{2} < x_{n_2} \leq s.  $$
  But again $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_2} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have 
  $$ s- \frac{1}{2} < x_{n_2} < s. \tag{2} $$ If there were no $n_2 > n_1$ for which  (2) can hold, then we would obtain 
  $$ x_n \leq s- \frac{1}{2} \ \mbox{ for every natural number } n > n_1. $$
  So, for every natural number $n$, we would obtain $$ x_n \leq \max \left\{ \ s- \frac{1}{2}, x_1, \ldots, x_{n_1} \right\} < s. $$
  And this would then imply that 
  $$ \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\} < s. $$ So there does exist a natural number $n_2 > n_1$ for which (2) can hold. Now suppose that $n_1, n_2, \ldots, n_k$ have been found such that $n_1 < n_2 < \cdots < n_k$ and such that 
  $$ s - \frac{1}{j} < x_{n_j} < s \ \mbox{ for all } j = 1, \ldots, k. $$
  Then just as we proceeded from $n_1$ to $n_2$, we can find a natural number $n_{k+1} > n_k$ such that 
  $$ s - \frac{1}{k+1} < x_{n_{k+1} } < s. $$ In this way we obtain a subsequence $\left( x_{n_k} \right)$ of $\left( x_n \right)$ that satisfies 
  $$ s - \frac{1}{k} < x_{n_k} < s $$
  for all $k \in \mathbb{N}$ and hence converges to $s$. Is this proof correct in each and every one of its steps? If not, then where is it incorrect? Is the presentation clear enough too?","['real-analysis', 'limits', 'sequences-and-series', 'proof-verification', 'convergence-divergence']"
2717331,Limit of a certain expression,"Let $f$ be a twice differentiable function on $(0,1)$ s.t. $\lim\limits_{x\to0+} f(x)=0$ and estimates
$$
|f^{(k)}(x)|\le Cx^{-k},\quad  k=0,1,2;\ x\in(0,1),
$$
hold. Is it true that $$\lim_{x\to0+} xf'(x)=0?$$ Or, for function $g(x)=xf(x)$, it is equivalent to $\lim_{x\to0+} g'(x)=0$ since $g'(x)=f(x)+xf'(x)$. Remark. If to drop the condition on derivatives for $k=2$, i.e. $|f''(x)|\le Cx^{-2}$, the statement is false as the following example shows: Let $f(x)=x \cos \frac1x$, then$$
x f'(x)=\sin \frac{1}{x}+x \cos \frac{1}{x}
$$
does not tend to $0$. In this case,$$
f''(x)=-\frac1{x^{3}}\cos \frac{1}{x}.
$$","['real-analysis', 'limits']"
2717340,"Proof that for any set $A$,$\ \ $ $\mathcal{P}\left(A\right)\nsubseteq A$","I was asked, as an exercise in a course on elementary set theory, to prove that  for any set $A$,$$\mathcal{P}\left(A\right)\nsubseteq A$$ This is the very beginning of the course, so cardinality arguments are out of the question. The only apparent contradiction I can draw from the above statement (Using only the axioms and not ""Common sense"") is that it would imply $A\in A$, but reading some about this it seems this alone is not enough to deduce a contradiction without the axiom of regularity, which was not presented yet. Hints would be greatly appreciated.",['elementary-set-theory']
2717343,Hodge theory for manifolds with boundary,"I'm looking for a good reference for Hodge theory on a compact Riemannian manifold with boundary . I read Section $5.9$ in Taylor's Partial Differential Equations I , which is actually pretty useful in the sense that it has the important theorems (I think), as well as proofs thereof. However, Taylor leaves many gaps for the readers to fill on their own, and I find some of the arguments in this specific section hard to comprehend. It seems reasonable to believe that there are other texts (either articles or textbooks) which present this theory, but for some reason, I can't find any. Has anyone of you, dear StackExchange community members, ever bumped into such a text?","['reference-request', 'riemannian-geometry', 'differential-geometry', 'partial-differential-equations']"
2717358,infinitude of primes that are $11 \bmod 12$,"Suppose there are finitely many primes $\{p_1, \ldots, p_k\}$ which are $11 \pmod {12}$ and consider $p = (p_1 \cdots p_k)^2 + 10$. Then $p_i \nmid p$ for any $i \leq k$, and $p \equiv 1 + 10 \equiv 11 \bmod 12$. Then $p$ is either a prime itself, contradiction, or $p$ has all prime divisors of the form $12n + 1, 12n + 5, 12n + 7, 12n + 11$. Not all prime factor are of the forms $12n + 1, 5, 7$ because then $p \equiv 1, 5, 7 \mod 12$. So it must have a prime factor $12n + 11$, but $p_i \nmid p$ and so we have a contradiction. Is this proof valid?","['proof-verification', 'number-theory', 'congruences', 'prime-numbers', 'elementary-number-theory']"
2717363,An $n×n$ matrix $A$ is formed with each element $(a_{ij})$ randomly selected,"Let $p$ be a prime number.Given a positive integer $n$, an $n×n$ matrix $A$ is formed with each element $(a_{ij})$ randomly selected, with equal probability, from $[0, 1, ..., p − 1]$. Let $q_n$ be probability that $\text{det}A \equiv 1 (\ mod \ p)$.Find $\lim_{n \rightarrow \infty} q_n$ I have the idea of looking each entry of the matrix in terms of modulo $p$.
I was working with small cases for $p$ like $5$.
But it left me clueless.","['number-theory', 'probability', 'linear-algebra']"
2717425,Is the space of exact forms modulo exact forms whose anti-derivatives vanish on the boundary finite dimensional?,"Let $M$ be a smooth, connected, compact manifold with boundary (of dimension $d \ge 2$). Let $E$ be the space of exact $k$-forms on $M$. Define $$E_0=\{ \eta \in E \, | \, \eta=d\theta, \theta|_{\partial M}=0\}.$$ Is it true that $ E/E_0$ infinite dimensional? The answer is positive for $k=1$: $$ E/E_0=\{ df   \, | \, f \in C^{\infty}(M) \, \}\, / \, \{ dg \, | \,  g \in C^{\infty}(M) \, \, \text{and} \,g|_{\partial M}=0\}.$$ Let $f_i$ be an infinite set of functions on $\partial M$, such that $\{1,f_1,f_2\dots\}$ are linearly independent in $C^{\infty}(\partial M) $. Let $F_i$ be smooth extensions of $f_i$ to all of $M$. Then $[dF_i]$ are linearly independent in $E/E_0$. Indeed, suppose that $a^i[dF_i]=0$. Then $0=[a^idF_i]=[d(a^iF_i)]$, so $d(a^iF_i)=dg$ for some $g \in C^{\infty}(M)$ vanishing on $\partial M$. Since $M$ is connected, this implies $a^iF_i-g$ is some constant $c$, so in particular $a^if_i=c$, which implies $a_i=0$. What about $k>1$?","['differential-forms', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2717445,Property of a continuous function in the neighborhood of a point,"Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function. Let $a \in \mathbb{R}$ and define the following sets:
  \begin{align*}
M= \{x>a \mid f(t)>f(a), \forall t \in (a,x] \}\\
N=\{x>a \mid f(t)=f(a), \forall t \in (a,x] \}\\
P=\{x>a \mid f(t)<f(a), \forall t \in (a,x] \}
\end{align*}
  Prove that at least one of these sets is not empty. I tried to prove this by contradiction. If they were all empty, then, for all $n \in \mathbb{N}_{\geq 1}$ there would be some numbers $a_n,b_n,c_n \in (a,a+\frac{1}{n})$ such that $f(a_n) \leq f(a), \: f(b_n) \neq f(a)$ and $f(c_n) \geq f(a)$. But these sequences that are formed seem independent, and I couldn't connect them such that I reach a contradiction. The fact that at least one of these sets is not empty is pretty obvious on a graph, but I didn't manage to prove it rigorously. I think I'm missing something obvious. Edit : Thank you for your counterexamples! As mentioned in one of the comments, I added the differentiability of the function.","['continuity', 'real-analysis']"
2717456,A Simple Calculus view on Fermat's Last Theorem,"My question (more of a hypothesis really) is basically this: If a function $f(x)$ is defined such that $f'(x)$ is not constant and never the same for any 2 values of $x$. Then there do not exist positive integers $a,b,c$ and $a\le b\le c$ such that, $$\int_0^{a} f(x)dx = \int_b^{c} f(x)dx\tag1$$ Taking $f(x)=x^n$ then for $n \gt 1$, $(1)$ becomes Fermat's Last Theorem, while for $n=1$, $(1)$ becomes $a^2 + b^2 = c^2$ Maybe this has something to do with the ""curviness"" (I am 16 so please forgive me for non-technical language) of the graphs of $x^n$ since for $n=1$ the graph is linear and for $n>1$ the graph is curvy. My school teachers are not talking to me about this because ""it is out of syllabus"", so I thought maybe this community could help.","['number-theory', 'real-analysis', 'conjectures', 'calculus']"
2717534,Definition of compactness in $\mathbb{R}$,"Let $A\subseteq K\subseteq\mathbb{R}$ where $K$ is compact. Since $K$ is compact, then let $\bigcup_{i\in I} U_{i}$ be a cover of $K$ by open subsets of $\mathbb{R}$ $\implies\exists\space r\in\mathbb{N}$ such that $U_{1}\bigcup U_{2}\bigcup$...$\bigcup U_{r}=C$ covers $K$ as well. But $A\subseteq K$, so $C$ covers $A$ as well. Since the initial cover was chosen arbitrarily, then by definition, $A$ must also be compact. I know there is something wrong in my reasoning because, in $\mathbb{R}$, I can pick the set $(0,1)\subseteq[-1,2]$ with $(0,1)$ not compact since it is not closed and $[-1,2]$ compact since it is closed and bounded. Can anyone tell me where I'm erring?","['general-topology', 'real-analysis', 'compactness', 'elementary-set-theory']"
2717572,Orthogonality relation seems to result in an indefinite case,"I'm trying to solve the following integral: $\int_0^1 (sin(n\pi x)sin(m\pi x))dx$ for $m\neq n$ and $m=n$. knowing that: $sin(n\pi x)sin(m\pi x)=\frac{1}{2}cos((n-m)\pi x) - \frac{1}{2}cos((n+m)\pi x)$ and integrating I obtain the following result: $\int_0^1(\frac{1}{2}cos((n-m)\pi x) - \frac{1}{2}cos((n+m)\pi x)) = \frac{sin((n-m)\pi x)}{2\pi(n-m)}-\frac{sin((n+m)\pi x)}{2\pi(n+m)}$ I can see how this will be zero for $m\neq n$. Both $m$ and $n$ are positive integers. So for all combinations of $m$ and $n$ and for $x=0,1$, $sin((n-m)\pi x)$ and $sin((n+m)\pi x)$ will be $0$. But for the case of $m=n$ the integral should be $1$ (I think), but won't $m=n$ result in an indefinite case because you're dividing by $0$? What am I missing?","['orthogonality', 'definite-integrals', 'trigonometry']"
2717614,Why does the determinant come in for Artin L-Functions?,"Let $L/K$ be a Galois extension of number fields.  For $\mathfrak p$ a prime of $K$, unramified in $L$, the Frobenius elements $\sigma_{\mathfrak P}$ for $\mathfrak P \mid \mathfrak p$ are conjugate, so if $\chi$ is a class function on $G = \operatorname{Gal}(L/K)$, $\chi(\sigma_{\mathfrak p}) := \chi(\sigma_{\mathfrak P})$ is well defined. Assume that $L/K$ is abelian, and $\chi$ is a character of $\operatorname{Id}(c)/P_c \mathfrak N(c)$ (which is isomorphic to $G$ via the Artin map).  The Weber L-function is defined for $\operatorname{Re}(s) > 1$ by $$L(s,\chi) = \prod\limits_{\mathfrak p} L_{\mathfrak p}(s,\chi) = \prod\limits_{\mathfrak p} (1 - \chi(\mathfrak p) N(\mathfrak p)^{-s})^{-1}$$ Identify $\chi$ as a character of $G$ via the Artin map.  When $\mathfrak p$ is unramified, it identifies with $\sigma_{\mathfrak p}$.  Now instead of a character of $G$, take a finite dimensional representation $\rho: G \rightarrow \operatorname{GL}_n(\mathbb C)$ of $G$ with character $\chi$.  Consider the formal logarithm of the local factor: $$\log L_{\mathfrak p}(s,\chi) = \log (1 - \chi(\sigma_{\mathfrak p}) N(\mathfrak p)^{-s})^{-1} = \sum\limits_{k=1}^{\infty} \frac{\chi(\sigma_{\mathfrak p}^k)}{kN(\mathfrak p)^{sk}}$$ The notes I'm reading say ""This exponentiates to $L_{\mathfrak p}(s,\chi) = \operatorname{Det}(I_n - N(\mathfrak p)^{-s} \rho(\sigma_{\mathfrak p}))^{-1}$.""  I don't understand how this is done.  Where does the determinant arise from the trace $\chi(g) := \operatorname{tr}(\rho(g))$ and the exponential map?","['number-theory', 'algebraic-number-theory', 'representation-theory', 'l-functions']"
2717751,Dimension of the local sections of a scheme is bounded by the dimension of the scheme,"I have been thinking in the following problem Let $X$ be a scheme. Is it true that $\dim \mathcal{O}_X(U) \leq \dim X$ for every open set $U\subset X$ ? I think it's true but I can't prove it. Here are my ideas in particular cases: If $U$ is an affine open subset then $\dim \mathcal{O}_X(U)=\dim
   U\leq \dim X$ . If $X$ is an integral $k$ -scheme of finite type and $\mathcal{O}_X(U)$ is a finitely generated $k$ -algebra ( this is not true in general ) we can take $V$ an open affine subset contained in $U$ . Then we have $k\subseteq \mathcal{O}_X(U)\subseteq
   \mathcal{O}_X(V)$ so $$\dim \mathcal{O}_X(U)= \text{tr.deg}_k\ \text{Frac}(\mathcal{O}_X(U))\leq \text{tr.deg}_k\ \text{Frac}(\mathcal{O}_X(V))=\dim V \leq \dim X$$ It would be great if someone can comment about the general case or give a proof in other cases. Edit 04/03/2018: If $X=\text{Spec}(A)$ is affine we have the inequality above for any $U$ . This follows directly from the fact that $$\mathcal{O}_X(U)=S^{-1}A \text{ where } S=A\setminus \bigcup_{\mathfrak{p}\in U} \mathfrak{p}$$ So $\dim \mathcal{O}_X(U) =\dim \text{Spec}(S^{-1}A)\leq \dim X$ because $\text{Spec}(S^{-1}A)$ is homeomorphic to a subset of $X$ . Edit: 06/02/2020: I saw this result in mathoverflow and it made me remember this problem. Using it we have that for any integral $k$ -scheme $X$ of finite type and any open set $U\subseteq X$ we have $k\subseteq \mathcal{O}_X(U)\subseteq K(X)$ , and hence $$\dim U \leq \mathrm{tr.deg}_k(\mathrm{Frac}\,\mathcal{O}_X(U))\leq \mathrm{tr.deg}_k\, K(X)=X$$ so the result is true for any open set in an integral variety.","['schemes', 'dimension-theory-analysis', 'algebraic-geometry']"
2717823,What does the notation $xf(x)$ and $f_{xxx}$ or $f_{xxy}$ mean?,"Hi I'm wondering about notations I encountered when studying differential equations and oscillation. $V''' = \frac{3\pi}{4b}(f_{xxx} + f_{xyy} +g_{xxy}+g_{yyy})+\frac{3\pi}{4b^2}[f_{xy}(f_{xx}+f_{yy})+g_{xy}(g_{xx}+g_{yy})+f_{xx}g_{xx}-f_{yy}g_{yy}]$ This is a Calculation of Stability of the limit Cycle in Hopf bifurcation, and I don't know what $f_{xxx}$ etc., means. A differential equation is given by $\frac{dx}{dt}=xf(x,y)$ What does the $xf$ stand for? Thanks
Josi","['ordinary-differential-equations', 'notation']"
2717842,Local connectedness is preserved under retractions,"I want to show that if $X$ is a locally connected topological space, $A\subseteq X$ is a subspace and $f:X \rightarrow A$ is continuous such that $f|_{A} = Id_{A}$, then $A$ must be locally connected as well. My progress so far: Take $U\subseteq A$ $A-$open and $x\in U$. Since $f^{-1} (U)$ is $X-$open and $x\in f^{-1} (U)$, there exists a connected, $X-$open subset $V\subseteq X$ such that $x\in V\subseteq f^{-1}(U)$. Now we have $x\in V\cap A \subseteq f^{-1}(U)\cap A = U$ My guess is that $V\cap A$ should be connected, but I am unsure if this is correct. Any help would be appreciated!","['continuity', 'general-topology', 'retraction', 'locally-connected']"
2717851,"Why is it that if a ring is simple, its center must be a field?","I have a statement that says that if $R$ is a simple ring, then $Z(R)$ must be a field. Since $R$ is a ring we get most of the field axioms (associativity and commutativity of addition, multiplication$\text{--}$since it's the center$\text{--}$, presence of the identities) immediately without need for explanation. But I don't see how we can get the invertibility of multiplication.","['abstract-algebra', 'ring-theory', 'field-theory']"
2717855,conditionally convergence of $\{\cos(n)/\sqrt{n}\}$,"The problem is basically the title: prove that $\sum_{n=1}^\infty \frac{\cos(n)}{\sqrt{n}}$ is conditionally convergent. I proved the convergence of the sum by Dirichlet's test, but i couldn't prove that $\sum_{n=1}^\infty \frac{|\cos(n)|}{\sqrt{n}}=\infty$. Do you have any hint?","['real-analysis', 'sequences-and-series']"
2717968,Proof By Induction: $\sum_{i=1}^n \frac{1}{\sqrt i} > \sqrt n$ [duplicate],"This question already has answers here : Proving $\sum\limits_{k=1}^{n}{\frac{1}{\sqrt{k}}\ge\sqrt{n}}$ with induction (6 answers) Closed 6 years ago . Here's what I have so far: $$\sum_{i=1}^n \frac{1}{\sqrt i} > \sqrt n$$ where $n\ge2$. Test for base case $$\frac{1}{\sqrt 1} + \frac{1}{\sqrt 2} > \sqrt 2$$ Simplify $$ 1 + .707... > 1.414... $$
$$ 1.707... > 1.414... $$ This statement is true. Thus we can assume $\sum_{i=1}^n \frac{1}{\sqrt i} > \sqrt n$ is true for $\forall n$ Now prove $n + 1$ Expand 
$$\sum_{i=1}^n \frac{1}{\sqrt i} = \frac{1}{\sqrt{1}} + \frac{1}{\sqrt{2}} + ... + \frac{1}{\sqrt{n}} \gt \sqrt n $$ Add the next n in the series, and add it to both sides of the inequality. $$\frac{1}{\sqrt{1}} + \frac{1}{\sqrt{2}} + ... + \frac{1}{\sqrt{n}} + \frac{1}{\sqrt{n+1}} \gt \sqrt n + \frac{1}{\sqrt{n + 1}} $$ Rewrite
$$\frac{1}{\sqrt{1}} + \frac{1}{\sqrt{2}} + ... + \frac{1}{\sqrt{n}} + \frac{1}{\sqrt{n+1}} = \sum_{i=1}^{n + 1} \frac{1}{\sqrt i}$$ LHS is complete now we have to try to make RHS = $\sqrt {n + 1}$ $$\sqrt n + \frac{1}{\sqrt{n + 1}} = \frac{\sqrt{n}\sqrt{n+1} + 1}{\sqrt{n+1}}$$ Given that n > 0 $$\frac{\sqrt{n}\sqrt{n+1} + 1}{\sqrt{n+1}} = \frac{\sqrt{n(n+1)} + 1} {\sqrt{n+1}}$$ Now since this is my first induction involving inequalities or summation, I was hoping you guys could show me what the next step would be, since I'm really not sure. Thanks in advance!","['induction', 'inequality', 'summation', 'discrete-mathematics']"
2718005,Coordinate ring of Grassmannian is UFD,"Hy guys. I'm trying to understand this proof, taken from this blog . The last part of the proof is not clear to me. More precisely,  I don't understand why the subgroup of elements fixing the factors is closed and has finite  index. NOTE : I need understanding this proof for trying to show the same proposition for any alg. closed field.","['algebraic-geometry', 'proof-verification', 'commutative-algebra']"
2718011,Proof of the Pythagorean Theorem without using the concept of area?,"Most of the proofs of Pythagorean Theorem that I see all seem to involve the concept of area, which to me does not seem ""trivial"" to prove. Others show proof for a particular triangle but it does not seem clear to me if it works for all right triangles or just specific variants. Is there a proof that is purely algebraic based on algebraic triangle constraints? Or one that does not rely on area at least and works for any arbitrary right triangle?","['trigonometry', 'triangles', 'geometry']"
2718013,Finding subgroups of E6 (and other Lie groups) using the Dynkin diagram,"According to the wikipedia article on $E6$ , the group has subgroups isomorphic to $SU(3) \times SU(3) \times SU(3)$, $SU(6) \times SU(2)$, and $SO(10) \times U(1)$, and it is claimed these can be read off from its Dynkin diagram.  I can see that if we delete some edges from the Dynkin diagram of $E6$, we can find the Dynkin diagrams of these three groups, however, the presence of an edge indicates the corresponding generators do not commute.  So while this argument seems valid for the case of $SO(10) \times U(1)$, in the other cases I do not see why the various non-abelian factors commute.  How is one able to see these subgroups using the Dynkin diagram?","['group-theory', 'lie-groups']"
2718033,sheaf of principal parts and sheaf of jets,"Let $(M,\mathcal{O}_M)$ be a complex manifold. Let $\Delta\colon M\to M\times M$ be the diagonal map and 
$\mathcal{I}$ the kernel of $\Delta^{-1}\mathcal{O}_{M\times M}\to \mathcal{O}_M$.
In this note , the author shows that the sheaf of order $n$ principal parts $$ 
\mathcal{P}^{(n)}:=\Delta^{-1}\mathcal{O}_{M\times M}/\mathcal{I}^{n+1} 
$$ 
  are isomorphic to  the sheaf of $n$-jets $$
\mathcal{J}et^{(n)}:=(\mathcal{O}_M\otimes_{\mathbb{C}}\mathcal{O}_M)/\mathcal{J}^{n+1},
$$ 
  where $\mathcal{J}$ is the kernel of the multiplication $\mathcal{O}_M\otimes_{\mathbb{C}}\mathcal{O}_M\to\mathcal{O}_M$ 
  and both of them are locally free $\mathcal{O}_M$-modules. (OK, it requires ""if $M$ is smooth"", but I think $M$ is smooth being viewed as a complex analytic space.) Question 1: shouldn't $\mathcal{P}^{(n)}$, instead of $\mathcal{J}et^{(n)}$, be called the sheaf of jets as the usual notion of Jet bundle is constructed from $\mathcal{P}^{(n)}$? Then, it follows that 
$$
\mathcal{I}/\mathcal{I}^2\cong\mathcal{J}/\mathcal{J}^2.
$$
Here the first corresponds to the cotangent bundle hence is the sheaf of holomorphic differentials $\Omega_M^1$, while the later, by a result in commutative algebra seeing for example in Stack Project , is isomorphic to the sheaf of Kahler differentials $\Omega_{\mathcal{O}_M/\mathbb{C}}^1$. However, from this MO question , Kahler differentials are very different from holomorphic differentials . We only have a surjective homomorphism
$$
\Omega_{\mathcal{O}_M/\mathbb{C}}^1\longrightarrow\Omega_M^1
$$
which is NOT an isomorphism (consider $de^z-e^zdz$ for example). 
In particular, the $\Omega_M^1$ is locally free of rank $\dim(M)$ while $\Omega_{\mathcal{O}_M/\mathbb{C}}^1$ cannot be since the share the same dual module - the module of derivations. Question 2: What's wrong in here? (My guess: I misunderstand the above statements of isomorphic 
as the product is taken in a different category than the category of complex manifolds.) Even ignore the isomorphism issue, the statement that $\mathcal{J}et^{(n)}$ is locally free is still strange for me since $\Omega_{\mathcal{O}_M/\mathbb{C}}^1$ is not locally free as mentioned before. Question 3: What's wrong in here?","['complex-geometry', 'differential-forms', 'algebraic-geometry', 'jet-bundles']"
2718084,Improve upon algorithm for finding a specific perfect square in a range of two very large integers $(10^{28})$,"Assume you are given a set of digits that represent every other digit in a perfect square, how can you find the root of a number that would have the same pattern - guaranteed to exist and to be unique. For example, $8*7*6$, if you replace the asterisks with all $0$’s and all $9$’s you would have $89796$ and $80706$. If you take the square of each, you get $299$ and $284$ (rounding to an integer). You can then iterate through each integer from $284$, to $299$, square them, until you find a number that matches the same pattern. Turns out the answer is $286^2$ which equals $81796$. We also know with the units digit being a $6$, that the first two digits are limited to a set of numbers, 96 being one of them. So just by knowing the last digit is $6$, one partial solution could be $8*796$, but could also be $8*744$. Regardless, there are very few options and the result set is narrowed. Just by applying $0-9$ to $8*796$, iterate through them, you would stumble across 81796 with a square root of $286$. If the problem was restricted to perfect squares with relatively smaller numbers, the solution would be simple - just find the max/min roots $(284-299)$, square them and find the perfect squares that satisfy the pattern of $8*7*6$, for example. Another example is $1*2*3*4$ where the square root is $1312^2 = 1721344$. However, using this algorithm, my program takes forever to run with very large integers. Seems like there would be a way of building the square by applying perfect square properties, or applying another algorithm that reduces the possibilities. EDITED: 
The larger examples are: $23209192748299230494155021081$ which is a perfect square and its root is: $152345635803259$. The numbers provided are: $229978920915201$, which, using the example above: $2*2*9*9*7*8*9*2*0*9*1*5*2*0*1$ with the asterisks being a placeholder for a digit. Another example: $232091909800969$ is the perfect square, and the square root is: $15234563$. The numbers provided are:$22999099$ meaning: $2*2*9*9*9*0*9*9$ with the asterisks being placeholders. The algorithm I detailed above (finding the min/max roots, squaring, and comparing) works fine for the perfect square: $232091909800969$, but not on the perfect square: $23209192748299230494155021081$ -- takes too long. Hope this makes sense. Edited:
The following is the code I used. It is Python 3.6.3 running on a MacPro 10.13.3, 2.7 GHz 12-Core Intel Xeon E5, 64GB RAM: # !/bin/python3

import sys, math, time


def removeOdds(num):
    result = """"
    num = str(num)
    for i in range(0, len(num), 2):
        result = result + num[i]
    return int(result)


def findRoot(n, num):
    def perfect_squares():
        if num[-1] == 0:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 0) and removeOdds(x ** 2) == num1)
        if num[-1] == 1:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 1 or x % 10 == 9) and removeOdds(x ** 2) == num1)
        if num[-1] == 4:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 2 or x % 10 == 8) and removeOdds(x ** 2) == num1)
        if num[-1] == 5:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 5) and removeOdds(x ** 2) == num1)
        if num[-1] == 6:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 4 or x % 10 == 6) and removeOdds(x ** 2) == num1)
        if num[-1] == 9:
            return (x for x in range(lowSq, highSq + 1) if (x % 10 == 3 or x % 10 == 7) and removeOdds(x ** 2) == num1)

    low = int('0'.join(str(x) for x in num))
    high = int('9'.join(str(x) for x in num))
    num1 = int(''.join(str(x) for x in num))
    lowSq = int(math.ceil(math.sqrt(low)))
    highSq = int(math.floor(math.sqrt(high)))
    x = perfect_squares()
    y = x.__next__()
    return y


def main(n, num):
    start = time.clock()
    root = findRoot(n, num)
    print(root)
    print (""execution time: "", time.clock() - start)


if __name__ == '__main__':
    n  = 3
    num = [8,7,6]
    #n = 4
    #num = [1, 2, 3, 4]
    # n = 8 takes approx 0.89 sec to run, expected result: 15234563
    #n = 8  # int(input().strip())
    #num = [2, 2, 9, 9, 9, 0, 9, 9]  # list(map(int, input().strip().split(' ')))
    # n = 10 takes approx 120 sec to run, expected result: 1234567890
    # n = 10
    # num = [1, 2, 1, 7, 7, 0, 9, 5, 1, 0]
    # n = 15 I've never been patient enough to see if it will complete
    #n = 15
    #num = [2, 2, 9, 9, 7, 8, 9, 2, 0, 9, 1, 5, 2, 0, 1]

    main(n, num)",['number-theory']
2718113,"How to show that the divergence can be written as the trace of total covariant derivative, ${\rm div}={\rm tr}\circ\nabla$?","Let $(M,g)$ be a Riemannian manifold. From John Lee's Riemannian Manifolds , the divergence operator $\text{div}:\Gamma(TM)\to C^{\infty}(M)$ is defined to be the map that sends $X$ to the unique function $\text{div} X$ such that
  $$(\text{div} X) \,dV_g = d(X\lrcorner \,dV_g),$$
  where $dV_g$ is the volume form and $\lrcorner$ denoted the interior multiplication. The above seems pretty complicated to me. Recently, I have found a much more accessible definition, i.e.
$$
\text{div} = \text{tr} \circ \nabla
$$
where $\nabla$ denotes the total covariant derivative. How do we show that these definitions are equivalent? It seems to me that the books I found only use either one of these and do not mention the other.","['riemannian-geometry', 'differential-geometry', 'partial-differential-equations']"
2718150,Diagonalization: Can you spot a trick to avoid tedious computation?,"I am studying for my graduate qualifying exam and unfortunately for me I have spent the last two years studying commutative algebra and algebraic geometry, and the qualifying exam is entirely 'fundamental / core' material - tricky multivariable calculus and linear algebra questions, eek! Here is the question from an old exam I am working on. Please note, how to solve the problem is not my specific question. After I introduce the problem, I will ask my specific questions about the problem below. No calculators. Let $$M = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 4 & 2 \\ 0 & -2 & -1\\ \end{bmatrix}$$ Find the determinant of $M$ , Find the eigenvalues and associated eigenvectors of $M$ , Calculate $$M^{2013} \cdot \begin{bmatrix}1\\1\\1\\\end{bmatrix}.$$ My issue is that with computational problems on an exam where calculators aren't allowed I always expect that either: there will be a trick to sidestep nasty calculations by hand, the problem will be contrived in such a way that the computation goes very easily. This seems to be the case in part 1 and part 2 of the problem since: The determinant of $M$ can very easily be found by cofactor across row 1 to get $\mathrm{det}(M) = 2(-4+4) = 0$ , or by inspection we see column 2 is quite visibly a scalar multiple of column 3 so that $\mathrm{det}(M) = 0$ . Since $\mathrm{det}(M) = 0$ we know $0$ is an eigenvalue. Noting the dependence relation between column 2 and column 3 allows us to easily read off an eigenvector for $\lambda = 0$ . Further, manually computing $\mathrm{det}(M - \lambda I)$ is again computationally easy because of the 0's across row 1. We get $p_{M}(t) = \lambda(2-\lambda)(\lambda - 3)$ . Solving for the corresponding eigenvectors is also fairly fast. Now - part 3 starts off fine. Considering part 2 it is practically implied from context clues that we are intended to diagonalize this matrix $M$ , as the only thing needed at this point is the inverse of the matrix of eigenvectors. The computation is when I go into a whirlwind because it does not flow as easily as the previous computations. In part 2 we had a degree three polynomial we wanted roots of, and of course it split into linear factors. Now I am inverting a 3x3 matrix by hand and getting all entries as ratios? On the exam, this will panic me. Time is definitely an issue and I need to learn how to not waste it. I immediately start re-studying the problem trying to see if there is some way around computing a 3 x 3 inverse by hand. One other approach I took, since I am just studying right now and not worried about time, was trying to express the vector $(1,1,1)^T$ as a linear combination of eigenvectors, say $$(1,1,1)^T = a_1v_1 + a_2v_2 + a_3v_3$$ with suitably chosen eigenvectors $v_1, v_2, v_3$ , since then $$M^{2013}(a_1 v_1 + a_2 v_2 + a_3 v_3) = a_2 \lambda_2^{2013}v_2 + a_3 \lambda_3^{2013} v_3.$$ Finding the linear combinations of eigenvectors seems to be no more or less easy than inverting the matrix of eigenvectors. Although I took a graduate abstract linear algebra course, I also worked in a tutoring center for years where I tutored problems like this without advanced methods - thus when I see questions like this, the method that immediately comes to mind is the classic one - diagonalize. Does anyone see any tricks to avoid nasty computation by hand in the problem above? More generally (I am sure lots of other users have taken graduate qual exams, and might have feedback here) does anyone have exam advice, perhaps a systematic way to decide if I should simply commit to doing the computation, and try to do it carefully yet as fast as possible, or halt myself in my tracks and say ""they wouldn't expect me to do this computation by hand, I should study the problem and see if there is a way around this."" Thank you. Edit: I suppose I may slightly be misusing this site since I know how to solve my problem, and my question is more geared towards exam skills? Part of my question even borders psychology... This is a bit of a philosophical conundrum whether my question is appropriate for the site. But, my exam is tomorrow so I will risk it! If it gets closed, so be it :)","['matrices', 'diagonalization', 'linear-algebra']"
2718158,Cross product in $\mathbb R^n$ (from Spivak's book),"Spivak defines cross product in this way: $\quad$ We conclude this section with a construction which we will restrict to $\mathbf{R}^n$. If $v_1,\ldots,v_{n-1}\in\mathbf{R}^n$ and $\varphi$ is defined by $$\varphi(w)=\det\pmatrix{v_1 \\ \vdots \\ v_{n-1} \\ w},$$ then $\varphi\in\Lambda^1(\mathbf{R}^n)$; therefore there is a unique $z\in\mathbf{R}^n$ such that $$\langle w,z\rangle=\varphi(w)=\det\pmatrix{v_1 \\ \vdots \\ v_{n-1} \\ w}$$ This $z$ is denoted $v_1\times\cdots\times v_{n-1}$ and called the cross product of $v_1,\ldots,v_{n-1}$. Why such a $z$ exists and why is it unique? When solving problems involving this notion, how do I find this $z$ explicitly (if it's possible)? Also, what's the meaning of this cross product? Many sources say that the usual cross product in $\mathbb R^3$ can't be generalized to higher dimensions.","['real-analysis', 'calculus', 'manifolds', 'differential-geometry', 'linear-algebra']"
2718177,Use the chain rule to find the formula for $g^\prime (f(c))$,"Let $I_1,I_2$ be intervals. Let $f:I_1\rightarrow I_2$ be a bijective function, and  $g:I_2\rightarrow I_1$ be the inverse. Suppose that both $f$ is differentiable at $c\in I_1$ and $f^\prime (c) \not = 0$ and $g$ is differentiable at $f(c)$. Use the chain rule to find a formula for $g^\prime (f(c))$ (in terms of $f^\prime (c)).$ Attempt:  $g \circ f = g(f(x))=h(x)$. Then, $h(x)$ is differentiable at $c$. $h^\prime (c)= g^\prime (f(c))f^\prime (c) \rightarrow g^\prime (f(c))= \frac {h^\prime (c)}{f^\prime (c)}$. I have to express $h^\prime (c)$ in terms of $f^\prime (c)$, but don't know how to do it. Could you give some hint for this? Thank you in advance!","['derivatives', 'real-analysis']"
2718190,Dual of $L^p$ space avoiding reflexivity and Radon-Nikodym Theorem,"Let $(X,\mathscr{F},\mu)$ be a measure space, with $\mu(X)<+\infty$, let $p\in [1,+\infty[$ and $q$ its Hölder-conjugate (that is, $1/p+1/q=1$). If $T\in (L^p(X,d\mu))^*$ is a continuous functional such that for every nonnegative $f\in L^p$ one have $T(f)\geq 0$, we wish to show that there exists a function $g\geq 0$ in $L^q$ such that 
$$
T(f) = \int_X fg \; d\mu, \quad \forall f\in L^p.
$$
The procedure is as follows: Show that $\lambda(A)=T(\chi_A)$ for every $A\in\mathscr{F}$ defines a finite positive measure on $X$. Let $\nu=\mu+\lambda$. Prove that $\mu(A)=0$ iff $\nu(A)=0$ and that $\mu(A)=0\Rightarrow \lambda(A)=0$. Let $S:L^2(X,d\nu)\to\mathbb{R}$ be defined by
$$
S(f) = \int_X f \; d\lambda, \quad \forall f\in L^2(X, d\nu).
$$
Show that $S$ is a well-defined linear continuouos functional and that
$$
|S(f)|\leq \|f\|_{L^2(X,d\nu)}\sqrt{\lambda(X)}.
$$ Using the Riesz's representation theorem for Hilbert spaces show that there exists a function $h\in L^2(X,d\nu)$ such that
$$
\int_X f \; d\lambda = \int_X fh \; d\nu, \quad \forall f\in L^2(X,d\nu).
$$ By considering $f=\chi_{h<0}$ and $f=\chi_{h\geq 1}$ show that $0\leq h<1$ $\nu$-a.e. and so we can replace $h$ by a function $\nu$-a.e. equal to $h$ taking values in the interval $[0,1[$. Show that for every function $f\geq 0$ in $L^p(X,d\mu)$
$$
T(f) = \int_X fh \; d\mu + \int_X fh \; d\lambda \quad \text{and} \quad T(f(1-h)) = \int_X fh \; d\mu.
$$ Show that for every positive integer $k$ the function $\min(f/(1-h),k)$ belongs to $L^p(X,d\mu)$ and that
$$
T(\min(f/(1-h),k)) = \int_X \min(f/(1-h),k) \; d\mu.
$$ Take $g=h/(1-h)$ and conclude by making $k\to +\infty$. I'm stuck in the 7th step of this proof, so if someone can help me with a hint or a proof for this part, I'll be very gratefull. I don't need already known proofs as Brezis (using reflexivity) or Folland (using Radon-Nikodym Theorem) present.","['functional-analysis', 'riesz-representation-theorem', 'lp-spaces']"
2718267,Jordan Block of Kronecker Product,"Let $A$ be a $(p\times p$)-Jordan block of generalized eigenvalue $\lambda$. Let $B$ be a $(q\times q$)-Jordan block of generalized eigenvalue $\mu$. Then what is the Jordan canonical form for $A\otimes B$, where $\otimes$ is the Kronecker product? I found a reference here without a proof (Horn, Roger A., and Charles R. Johnson. Matrix analysis. Cambridge university press, 1990.): I will be appreciated if anyone can give me a proof of part (a).","['tensor-products', 'modules', 'linear-algebra', 'jordan-normal-form']"
2718285,Outer measure induced by a Jump function,"This is from exercise 4.4 of Elstrodt's measure theory textbook. By a jump function $F:\mathbb{R}\rightarrow\mathbb{R}$ we mean a function which can be written in the form $$F(x)= \begin{cases}\phantom{-}\sum_{y\in A\cap(0,x]}p(y)&\text{if $x\geq0$}\\-\sum_{y\in A\cap(x,0]}p(y)&\text{if $x<0$}\end{cases}$$ where $A$ is some subset of $\mathbb{R}$ and $p:A\rightarrow(0,\infty)$ is a function satisfying the 'local summability' condition; that is, we require that $\sum_{y\in A\cap B}p(y)<\infty$ for every bounded subset $B$ of $\mathbb{R}$. (This makes $A$ necessarily a countable subset.) We now consider the outer measure induced by $F$. Writing this by $\eta_F$, we have, by definition, $$\eta_F(S)=\inf\left\{\sum_{i=1}^{\infty}(F(b_i)-F(a_i)):S\subset\bigcup_{i=1}^{\infty}(a_i,b_i]\right\}$$ where $S$ is any subset of $\mathbb{R}$ and $a_i,b_i\in\mathbb{R}$. With this outer measure, the Caratheodory construction now gives the $\sigma$-algebra of $\eta_F$-measurable subsets. The problem is to show that EVERY subset of $\mathbb{R}$ is $\eta_F$-measurable. I've added the original German text below for clarity. What I have tried: The problem is equivalent to showing that $\eta_F(S\cup T)=\eta_F(S)+\eta_F(T)$ for every disjoint subsets $S,T$ of $\mathbb{R}$. I figured that maybe $\eta_F(S)=\sum_{y\in A\cap S}p(y)$ holds for every $S$, and tried to prove this. I've succeeded in showing $\eta_F(S)\geq\sum_{y\in A\cap S}p(y)$, but got stuck on the other inequality. Right now I'm not quite sure whether this is the right idea... Any advice is welcome!","['outer-measure', 'real-analysis', 'real-numbers', 'measure-theory']"
2718299,Evaluating trigonometric limit $\csc^2(2x) - \frac{1}{4x^2}$,"$$\lim_{x\rightarrow 0} \left[ \csc^2(2x) - \frac{1}{4x^2} \right]$$ I've tried to use l'Hôpital's rules but still can't find the answer. Here's my approach: $$
\begin{aligned}
&\lim_{x\rightarrow 0} \left[ \csc^2(2x) - \frac{1}{4x^2} \right] \\
=& \lim_{x\rightarrow 0} \left[ \frac{1}{\sin^2(2x)} - \frac{1}{4x^2} \right] \\
=& \lim_{x\rightarrow 0} \left[ \frac{4x^2-\sin^2(2x)}{4x^2(\sin^2(2x))} \right] \\
=& \lim_{x\rightarrow 0} \left[ \frac{8x-4\sin(2x)\cos(2x)}{8x\sin^2(2x)+16x^2\sin(2x)\cos(2x)} \right] \\
=& \lim_{x\rightarrow 0} \left[ \frac{8x-2\sin(4x)}{8x\sin^2(2x)+8x^2\sin(4x)} \right] \\
=& \lim_{x\rightarrow 0} \left[ \frac{4x-\sin(4x)}{4x\sin^2(2x)+4x^2\sin(4x)} \right] \\
\end{aligned}
$$ Am I using the correct way? How to solve it correctly? P. S. I tried to use calculator and it outputs one third $1/3$.","['trigonometry', 'limits']"
2718303,Recursively counting a parking function,"Definition: Parking Function A parking function of order $n$ is a function $f:\{1,2,\ldots,n\}\rightarrow\{1,2,\ldots,n\}$ such that $$|\{x:f(x)\le i\}|\ge i\qquad \textrm{for $1\le i\le n$}$$ It is often described using the following scenario: $n$ cars $\{1,2,\ldots,n\}$ enter a one way street (or parking lot) with $n$ parking spaces. Each car enters the street one at a time in numerical order . They each proceed along the spaces $\{1,2,\ldots , n\}$ in order so that each successive car $x$ either fills its preferred space $f(x)$ or the next available space after $f(x)$. If all cars manage to find a space before exiting the parking lot at the far end then $f$ is called a ""parking function"". If any car cannot find a space after its preferred space then $f$ is not a parking function. My task is to provide a combinatoric argument for the recursive formula that counts the number of parking functions on $[n+1]$. The formula is given by $$P(n+1)=\sum_{i=0}^{n} \binom{n}{i} (i+1) P(i)P(n-i)$$ I know that explicit formula for counting the number of parking function is given by $$P(n) = (n+1)^{n-1}$$ Providing a explanation for the recursion seems to be relate to the concept of dividing the ""parking spots"" into halves. With the first half having $i$ spots and the second half having $n-i$ spots, but I can't fully relate.","['combinatorics', 'graph-theory']"
2718330,Find the largest value and the smallest value of expression: $P=a+2b+2c.$,"Let real numbers $ a, b, c$ satisfy $\left\{ \begin{align}
  & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\ 
 & 2a+b-2c\ge 12 \\ 
\end{align} \right.$ . Find the largest value and the smallest value of expression: $$P=a+2b+2c.$$ I solved the problem as below:
$$\left\{ \begin{align}
  & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\ 
 & 2a+b-2c\ge 12 \\ 
\end{align} \right.\Rightarrow {{a}^{2}}+{{b}^{2}}+{{c}^{2}}-2a-b+2c-13\le 0 
(1)$$ 
$$P=a+2b+2c\Leftrightarrow a+2b+2c-P=0 (2) $$ 
 $(1)$ corresponding sphere with center $I\left( 1;\frac{1}{2};-1 \right)$ and radius $$R=\frac{\sqrt{61}}{2}$$
$(2)$ corresponds to the plane $$\left( \alpha  \right):a+2b+2c-P=0$$
The problem of finding conditions of plane and sphere with common intersection. This happens when: $$d\left( I,\left( \alpha  \right) \right)=R\Leftrightarrow \frac{\left| -P \right|}{3}\le \frac{\sqrt{61}}{2}\Leftrightarrow -\frac{\sqrt{61}}{2}\le P\le \frac{\sqrt{61}}{2}$$
So $$\min P=-\frac{\sqrt{61}}{2},\max P=\frac{\sqrt{61}}{2}$$
Please comment and give better solution.","['multivariable-calculus', 'inequality', 'optimization']"
2718332,Why isn't the axiom of extensionality considered a definition of equality?,"This question was essentially asked and responded to here , but I don't feel it was really answered: Is the axiom of extensionality merely called an ""axiom"" by convention, or is there a clear distinction between axioms and definitions that forbids it from being considered a definition of set equality? The answer in the linked post seems to address what would happen if the axiom were rejected altogether, but doesn't discuss whether there is good reason to consider it an axiom rather than a definition. If axioms and definitions are just two different words for the same thing, then that's fine—but I get the feeling I'm missing something. Do we simply call a definition an axiom if it defines a particularly foundational concept (like equality)?","['terminology', 'axioms', 'logic', 'elementary-set-theory']"
2718345,"Calculate a quartic monic polynomial given that $P(1)=10,P(2)=20$ and $P(3)=30$.","Let $$P(x)=x^4+ax^3+bx^2+cx+d$$ where $a,b,c$ and $d$ are constants. If $P(1)=10,P(2)=20$ and $P(3)=30$, then what is the value of $$\frac{P(12)+P(-8)}{10}$$ I tried substituting $x=1,2,3$ in $P(x)$ to get three linear equations in $a,b,c$ and $d$, but it ended up being quite difficult. 
$$a+b+c+d=9$$
$$8a+4b+2c+d=4$$
$$27a+9b+3c+d=-51$$ Is there a simpler method to this question?","['algebra-precalculus', 'polynomials']"
2718380,Under what condition does the weak limit of function a function?,"Let $B$ be the set of bounded functions $[0, 1] \rightarrow \mathbb R$. 
a sequence $f_n$ of $B$ converges weakly to a distribution $\mu$ if 
$$\int_0^1 g(t) f_n(t) \rm{d}t  \rightarrow \int_0^1 g d\mu \quad \text{ for all continuous functions } g.$$ Under what conditions is $\mu$ absolutely continuous w.r.t the Lebesgues measure? (i.e. $\exists f \in B \text{ s.t } d\mu  = f dt$?) One such a condition is that the $f_n$ be uniformly bounded. Are there others?","['weak-convergence', 'lebesgue-measure', 'measure-theory']"
2718440,Exercise on properties of integration,"Suppose $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Suppose $m \leq f' \leq M$ on $(a, b)$ and denote $\mu = \dfrac{f(b) - f(a)}{b-a}$. Prove that  $$\left|\int_{a}^{b} f(x) \,\mathrm{d}x  - \frac{f(a) + f(b)}{2}(b-a)\right| \leq \frac{(M-\mu)(\mu - m)}{2(M-m)}(b-a)^2.$$ The book suggested to compare $f(x)$ with piecewise linear functions, but I do not really know how to go about it. I tried using MVT since $\mu$ was in that form, but I do not know how to proceed.","['real-analysis', 'integration']"
2718471,Integral part of sum of huge powers,"Question: What is the integral part of the following expression?$$(a+\sqrt{b})^{2n}+(a-\sqrt{b})^{2n}$$ The question has specific values of $a=2,b=5$ and $2n=2016$. I was able to simplify (or complexify) it to:$$\sum_{i=0}^n\binom{2n}{2i}a^{2i}b^{n-i}.$$ I think I need to use $$\binom{2n}{2i}=\binom{2n-1}{2i-1}+\binom{2n-1}{2i},$$ but because the powers of $a$ and $\sqrt{b}$ dont change I cant make a closed form. Please help.",['algebra-precalculus']
2718480,"How to prove that: $\forall a \in \Bbb R,$ $\|f+a\|_p\ge \frac12\|f\|_p$ when $\int_\Omega f(x)dx= 0$","Let $\Omega $ be any subset of $\Bbb R^d$ and $f\in L^p(\Omega)$ such that $$\int_\Omega f(x)dx= 0$$ Then, shows that for each real number  $a\in \Bbb R,$ we have $$\|f+a\|_{L^p(\Omega )}\ge \frac12\|f\|_{L^p(\Omega )}$$ This looks obvious but but spent already a lot of time on it. I noticed that it sufficed to prove the inequality  for $a>0 $. the case  $p=2$ is been trivial using quadratic. Any Hint or idea?","['real-analysis', 'inequality', 'functional-analysis', 'lp-spaces', 'measure-theory']"
2718482,Can we find Convolution Sum Boundaries without graphical plot (Analytic Method)?,"I was trying to convolve two discrete sequences $x[n] = (-3)^n u(-n-1)$ with $h(n)=(\frac{1}{2})^n u[n+2] $ and was wondering if the work can be completed without having to plot the graph of the functions to find the limits of  my summation formula (Completely analytic solution). I reached a place in the summation where I got 
$$\sum _{i=-\infty} ^{i=+\infty} (-3)^i u(-i-1) \bigg( \frac{1}{2}\bigg)^{n-i} u(n-i+2) $$ can I know the limits from here? I thought about $u(-i-1)>0$ then $i<-1$  and $u(n-i+2)>0$ then $i<n+2$ but is this of any use?","['summation', 'convolution', 'discrete-mathematics']"
2718512,"Prove that any graph $G$ has two sided subgraph $H$, where $|E(H)| \ge {|E(G)| \over2}$","Theorem: Any graph $G$ has two sided subgraph $H$, where $|E(H)| \ge {|E(G)| \over2}$ Proof: Taking two any sets $U$ and $W$ , so that $U \cup W = V(G)$. It is clear that any graph has a subgroup ,where vertices of subgraph is equal to vertices of graph. By ejecting edged in $U$ , where any vertex is connected to other, and doing same for $W$, we get subgraph $H$. Now showing that for any $u \in V(G)$, $d_H(v) \ge {d_G(v) \over 2}$. Suppose the opposite is true, $\exists u \in V(G)$, so that $d_H(v) < {d_G(v) \over2}$. Suppose $ u \in U$. $d_H(v) < {d_G(v) \over2}$ => $u$ vertex has more neighbor vertices in $U$ , than in $W$. I stuck here, don't have any idea , what is written in last line, any helps will be appreciated.","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2718522,Non-Numerical proof of $e<\pi$,"This is a ""coffee-time-style"" problem ( to have a taste of this style, you may like to browse the book https://www.amazon.com/Art-Mathematics-Coffee-Time-Memphis/dp/0521693950 ) interpreted from an anonymous problem once on the interactive whiteboard at my department, namely how to prove $e<\pi$ without much numerical computation like Taylor expansion or so. I once tried to use some ""intrinsic connection"" between $e$ and $\pi$ like $\sqrt{\pi}=\int_{-\infty}^{+\infty}e^{-x^2}\mathrm{d}x$ ( you can even find it in this movie http://www.imdb.com/title/tt4481414/ for testing children) and one possible way of reducing the problem is in the next paragraph. However it seems to be not that easy, any suggestion or new ideas? A stronger version of this question is : can we construct an explicit function $f(x)$ on $\mathbb{R}$ so that $f(x)\leq e^{-x^2}$ for all $x\in\mathbb{R}$ with $f(x)< e^{-x^2}$ on an open interval, and that $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$ ? We know from standard measure theory that there are $\beth_2$ such kind of Lebesgue-integrable functions, but this is the thing: how simple and explicit can what we're looking for be? Examples of very simple and explicit functions include but are not limited to piecewise elementary functions ( https://en.wikipedia.org/wiki/Elementary_function ). Unfortunately a function $f(x)$ defined piecewisely by  $$f(x)|_{(-1,1)}=e^{-|x|^r}\ \text{where}\ r\in\mathbb{Q}\cap(-\infty,2)\ \text{or}\ \mathbb{Q}\cap (-\infty,2]\ \text{respectively}$$  and $$f(x)|_{(-\infty,-1]\cup[1,\infty)}=e^{-|x|^s}\ \text{where}\ s\in\mathbb{Q}\cap [2,\infty)\ \text{or}\ \mathbb{Q}\cap(2,\infty)$$  would NOT satisfy $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$, if the values of the Gamma  function at rational points are linearly (or even algebraically) independent with $\sqrt{e}$ ( https://en.wikipedia.org/wiki/Particular_values_of_the_gamma_function ). The question is then how to move on from this first failure to search other explicit functions. I am aware that it is probably hard to ask such a question as solid as ""can we prove that CH is independent from ZFC""; after all, one can argue that any numerical inequality essentially also comes from some intrinsic inequality and hence not numerical at all. However one might try to ask in a relatively sloppy way: is there something that is at least seemingly simpler or less numerical, if not completely non-numerical ?","['inequality', 'calculus', 'eulers-number-e', 'pi', 'geometry']"
2718529,Convergence of graphs vs pointwise convergence,"Suppose that $X$ and $Y$ are compact metric spaces. Let $C(X,Y)$ be the set of continuous maps $f:X\to Y$. For $f\in C(X,Y)$, the graph of $f$ is closed in $X\times Y$. Suppose that $\{f^n\}$ is a sequence in $C(X,Y)$ and suppose that the corresponding sequence of graphs converges to a closed set in $X\times Y$ in the Hausdorff metric topology. Does that imply that there exists a map $f:X\to Y$ that is the pointwise limit of some subsequence of the sequence of functions $\{f^n\}$?","['functional-analysis', 'general-topology']"
2718532,"Extrema of $(\tan{x})^{\sin{2x}}$ on $(0,\frac{\pi}{2})$ interval","Function $(\tan{x})^{\sin{2x}}$ has its minimun for $x=u\in(0,\frac{\pi}{2})$ and maximum for $x=v\in(0,\frac{\pi}{2})$. Find $u+v$ Having calculated the derivative: $$\frac{d}{dx}(\tan{x})^{\sin{2x}} = (\tan{x})^{\sin{2x}}\cdot(2\cos{2x\cdot\ln{\tan{x}}+2})$$  I find it quite difficult to find roots of it (as if there was some other special way/technique to approach this problem) Can You give me any hint? I will appreciate everything.","['derivatives', 'real-analysis', 'trigonometry', 'calculus']"
2718545,How does one define a dimension of $V(F) = \{F = 0\}$ over the reals?,"I have questions about how dimension is defined for an affine varieties restricted to $\mathbb{R}^n$. Let $F(\mathbf{x}) \in \mathbb{R}[x_1, ..., x_n]$. How does one define dimension for $V_{\mathbb{R}}(F) = V(F) \cap \mathbb{R}^n= \{ \mathbf{x} \in \mathbb{R}^n: F(\mathbf{x}) = 0  \}$? One way to define this is to use the subspace topology (as a subspace of $\mathbb{A}_{\mathbb{C}}^n$) and define it 
as the length of a maximal chain of irreducible closed sets (here closed sets are algebraic sets in $\mathbb{C}^n$ intersected with $\mathbb{R}^n$). Another way I think is as the length of a maximal chain of irreducible algebraic sets (zero sets of real polynomials) in $\mathbb{R}^n$. They both seem like a natural definition to me... Are these two possible definitions always give the same quantity if we are considering an algebraic set (in $\mathbb{R}^n$) defined by real polynomials?
Thank you very much.",['algebraic-geometry']
2718551,Find all integer solutions of: $\;\frac{1}{m}+\frac{1}{n}-\frac{1}{mn^2}=\frac{3}{4}$,"I found the following problem from the 10th Iranian Mathematical Olympiad in Crux Magazine . Find all integer solutions of $$\frac{1}{m}+\frac{1}{n}-\frac{1}{mn^2}=\frac{3}{4}$$ Initially it looked like a typical quadratic problem, however I hit a dead end each time I solve it. My methodology is as follows, 
$$\frac{n^2+mn-1}{mn^2}=\frac{3}{4}$$
$$\implies 4n^2+4mn-4 = 3mn^2$$
$$\implies (4-3m)n^2+4m \cdot n-4=0$$
I used the quadratic formula , and got, 
$$n = \frac{-4m \pm \sqrt{(4m)^2-4\cdot(4-3m)\cdot(-4)}}{2(4-3m)}$$
I do the usual algebraic manipulations and drop at, 
$$n = \frac{-2m \pm 2\sqrt{m^2-3m+4}}{4-3m}$$ I am unsure how I go ahead from this. Some help would be much appreciated. Cheers!","['number-theory', 'contest-math', 'quadratics']"
2718576,Find the characteristic function for all n.,"Let $(Y_n)_{n\in \mathbb{N}}$ be independent and identically distributed random variables. Let $A_n=\sum_{p=1}^nY_p\quad \forall n\in \mathbb{N}$. Find $\varphi_{S_n/n}$ for all $n\in \mathbb{N}$ expressed in terms of $\varphi_{Y_1}$. I've tried the following; We have $Y_p=Y_1$ for all $p\in \mathbb{N}$ since the variables are iid. Thus we get
$\varphi_{S_n/n}=\varphi_{n\cdot Y_1 /n}=\varphi_{Y_1}$ for all $n\in \mathbb{N}$ This seems very simple, and I feel like i missed something?","['characteristic-functions', 'probability-theory']"
2718641,Question regarding quadratic residue,"I am new to this and confused myself, but will try my best to explain the problem clearly. An integer $a$ is a quadratic residue with respect to prime p if $a \equiv x^2 \mod{p}$ for some integer $x$. Here are my questions: Does $p$ need to be prime? I ask because definition from wolfram doesn't requires it to be so. And from Euler's criterion: If $a^\frac{p-1}{2} \equiv 1 \mod{p}$, it means $a$ is a quadratic
residue module $p$. If $a^\frac{p-1}{2} \equiv -1 \mod{p}$, it means $a$ is a not a quadratic residue module $p$. So if $p$ is not a prime then $a^{\frac{p-1}{2}}$ won't even be an integer. And so according to me it should be odd prime. Should $0\lt x \lt p$ be true? What if we have $a\equiv x^2\mod{p}$, and $x \gt p$? The thing is I tried finding such $x$ by pen, but always found that there is always a $y \lt p$ such that $a\equiv y^2\mod{p}$. Is this always true? And how can we prove it?","['number-theory', 'quadratic-residues', 'modular-arithmetic', 'discrete-mathematics']"
2718734,Solving integral using complex analysis,"I am looking to solve
$$I=\int_{-\infty}^{\infty}\frac{\cos{x}}{x^2+1}dx$$
To do so i defined a function of complex variable as follows:
$$f(z)=\frac{\cos{z}}{z^2+1}$$ Then i make a closed countour $C$ by uniting a semicircle (denoted $\gamma_R$) above the origin and a line connecting points $-R$ and $R$ on the real axis.
$$\oint_Cf(z)dz=\int_{-R}^{R}f(x)dx+\int_{\gamma_R}f(z)dz$$
The only pole within the contour is at $z=i$. By the residue theorem 
$$\oint_Cf(z)dz=2\pi i\cdot \operatorname{Res}[f]_{z=i}$$
I use the definition of complex cosine:
$$\cos{z}=\frac{e^{iz}+e^{-iz}}{2}$$
$$\operatorname{Res}[f]_{z=i}=\lim_{z\to i}\bigg\{(z-i)\frac{e^{iz}+e^{-iz}}{2(z-i)(z+i)}\bigg\}=\frac{1/e+e}{4i}$$
My calculation then becomes:
$$I=\pi\frac{1/e+e}{2}-\int_{\gamma_R}f(z)dz$$
Now:
$$\int_{\gamma_R}f(z)dz=\int_{-R}^{R}f(Re^{i\phi})dRe^{i\phi}=\int_{-R}^{R}\frac{\cos{(Re^{i\phi})}}{R^2e^{2i\phi}+1}d\phi$$
as $R\to\infty$ this integral goes to zero (Jordan's Lemma??). I can conclude $$I=\pi\frac{e+1/e}{2}$$
But the answer should be $$\frac{\pi}{e}$$
I can't seem to find the mistake. I am pretty new to complex analysis, would anyone give me hint, what did i do wrong please?","['complex-analysis', 'definite-integrals', 'contour-integration']"
2718766,Corollary 5 in Royden-Fitzpatrick's Real Analysis: Convergence in Measure,"Corollary 5: Let $\{f_n\}$ be a sequence of nonnegative integrable functions on $E$ . Then $$\lim_{n \to \infty} \int_E f_n = 0 ~~~~~~(5)$$ if and only if $$f_n \to 0 \mbox{ in measure on } E \mbox{ and } \{f_n\} \mbox{ is uniformly integrable and tight over } E ~~~~~(6)$$ Here's the part of the proof giving me trouble: First assume (5). Corollary 2 tells us that $\{f_n\}$ is uniformly integrable and tight over $E$ ... Here is corollary 2: Let $\{h_n\}$ be a sequence of nonnegative integrable functions on $E$ . Suppose that $h_n(x) \to 0$ for all most all $x \in E$ . Then $$\int_E h_n \to 0 \mbox{ iff } \{h_n\} \mbox{ uniformly integrable and tight over } E$$ How can we invoke corollary 2 if $f_n$ isn't assumed to converge pointwise a.e. to $0$ on $E$ ? EDIT: Okay. I encountered some more difficulties in the second half of the proof: To prove the converse, we argue by contradiction. Assume that (6) holds but (5) fails to hold. Then there is some $\epsilon_0 > 0$ and a subsequence $\{f_{n_k}\}$ for which $$\int_E f_{n_k} \ge \epsilon_0 \mbox{ for all } k$$ However, by theorem 4, a subsequence $\{f_{n_k}\}$ converges to $f \equiv 0$ pointwise almost everywhere on $E$ and this subsequence is uniformly integrable and tight so that, by the Vitali Convergence Theorem, we arrive at a contradiction to the existence of the above $\epsilon_0$ . This completes the proof. First, why does $\int_E f_{n_k} \ge \epsilon_0$ hold for all $k$ ? If I've properly negated the definition of the convergence of a sequence, we should actually have that there exists an $\epsilon > 0$ such that for every $N \in \Bbb{N}$ there exists an $k \ge N$ such that $\int_E f_{n_k} \ge \epsilon_0$ . Second, assuming that (5) fails certainly does give us a subsequence such that blah blah holds; and theorem 4 gives us a subsequence such that blah blah holds. But what reason is there for thinking that these subsequences are the same?","['real-analysis', 'measure-theory', 'proof-explanation']"
2718776,Matrix invertiblity and its Inverse,"I'd like to prove that the Matrix $L:={ M }^{ T }M$ is invertible and determine its inverse (in dependence of $A$ and $B$). $M:=\begin{pmatrix}A & B \\ 0_{q\times p}& I_q\end{pmatrix}$ and $A\in K^{p\times p},B\in K^{p\times q}$. 
Further is given that the matrix $A$ has rank $p$. I tried to form $L=\begin{pmatrix}{ A }^T A& A^T B\\ B^T A &{ B^T B+I }_{ q }\end{pmatrix}$ into the Unity Blockmatrix using elementary row operations.
Since A has a full rank it must be invertible and because A is a $p\times p$ matrix, its transpose must be invertible too, with this knowledge its fairly easy to get ${ a }_{ 21 }=0$ and  I'm stuck in getting ${ a }_{ 12 }=0$ since I know nothing about the invertibility of $B$.
Are there any other ways to solve this problem?","['matrices', 'linear-algebra', 'inverse']"
2718784,Convergence of an improper integral $\int_3^\infty \frac{\sin(x)}{x+2\cos(x)}dx$,I tried to check whether the following integral converges: $$\int_3^\infty \dfrac{\sin(x)}{x+2\cos(x)}dx$$ Dirichlet criterion doesn't work here since the function $$\dfrac{1}{x+2\cos(x)}$$ is not monotone.,"['improper-integrals', 'integration']"
2718785,Why do expressions of the form $\sum\limits^\infty_{n=1} \frac{n^k}{3^n}$ sum 'nicely'?,"Why is it that sums like $\displaystyle\sum\limits^\infty_{n=1} \frac{n^k}{3^n}$ and $\displaystyle\sum\limits^\infty_{n=1} \frac{n^k}{2^n}$ where $k$ is a non-negative integer, sum to 'nice' numbers like 0.75, 1.5, 2, 4.125, 6, 15, 26 and 150 while sums like $\displaystyle\sum\limits^\infty_{n=1} \frac{n^k}{4^n}$ and $\displaystyle\sum\limits^\infty_{n=1} \frac{n^k}{7^n}$ don't? More generally, how can one tell what a sum of the form $\displaystyle\sum\limits^\infty_{n=1} \frac{n^k}{m^n}$ sums to?","['generating-functions', 'summation', 'sequences-and-series', 'elementary-number-theory']"
2718787,Supplementary Linear Algebra textbook,"We are using Elementary Linear Algebra by Howard Anton in the class and I’m not happy with it. At times there is many pages of writing, yet, there is very little information contained. I really like vector spaces but to find appreciation of them I have to scavenge online sources. What would be a good supplement, maybe substitution? I don’t have knowledge beyond Calculus 3, and I really want to learn how to understand and write proofs. At the same time, I’d like to be able to follow at least half of the time what the author is trying to convey.","['reference-request', 'book-recommendation', 'linear-algebra']"
2718800,Remainder term in Taylor series for the complex exponential map,"The statement is the following: Let $P_n(z) = 1 + z + \frac{z^2}{2!} + ... + \frac{z^n}{n!}$ be the $n$ first terms of the Taylor series for $e^z$ centered at the origin. Then, prove that $\forall n \in \mathbb{N}$, and $\forall z \in \mathbb{C} : Re(z)< 0$, this inequality holds: $|e^z - P_n(z)|<|z|^{n+1}$ I am trying to use the integral formula for the remainder: https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor 's_theorem_in_complex_analysis And then the fact that in the half plane with $Re(z) < 0$, the exponential map has modulus less than one. But this bound is maybe too coarse, because I cannot reach the conclusion. Any help will be much appreciated.","['complex-analysis', 'holomorphic-functions', 'taylor-expansion', 'exponential-function']"
2718803,Injective restriction of $f(x) = \frac{ax-b}{cx-d}$ [duplicate],"This question already has answers here : $f(x) = \frac {ax+b}{cx+d}$ , bijection $f: \Bbb R \to\Bbb R$? (4 answers) Closed 6 years ago . For $a,b,c,d \in \mathbb{R}$ and $ad-bc \neq 0$ I have the function $f : \mathbb{R}\setminus{\{d/c\}} \to \mathbb{R},  f(x) = \frac{ax-b}{cx-d}$. How can I find the largest possible restriction of $f$ such that $f$ is injective on it and the image of this restriction? Thank you for your help. Edit: If I understand correctly, the linked question is concerned with finding a condition for injectivity on the parameters a,b,c,d (the condition they find is $bc-ad \neq 0$). With mine the values are already given. Also I know that (as a continuous function) it's injective iff it is strictly monotone, which is the case when $f'(x) = \frac{bc-ad}{(cx-d)^2}$ is positive or negative. But I don't understand how this helps me, since the denominator is always positive and the numerator a fixed value for given bc and ad? If this is even the right approach for finding a restriction.","['real-analysis', 'functions']"
2718831,Prove that a matrix $M$ is diagonalizable if $M^3 = M$,"Let $M$ be a real square matrix such that $M^3 = M$. Prove that $M$ is diagonalizable. Proof: We have a $M$, a $2\times2$ matrix, $M$ is diagonalizable if $M$ has 2 linearly independent eigenvectors ${v_1,v_2}$, if so, then: $D = P^{-1}MP$
We give a concrete example: Let $$ M = \begin{bmatrix}1&3\\2&2\end{bmatrix} $$ then the eigenvalues $x_1  =-1$ and $x_2 = 4$ with eigenvectors $(3,-2)$ and $(1,1)$, respectively. Hence, $$ P = \begin{bmatrix}3&1\\-2&1\end{bmatrix}, D = \begin{bmatrix}-1&0\\0&4\end{bmatrix} $$ This is my proof but I am not sure about the given condition in the statement $M^3 = M$, how can I use it?","['matrices', 'diagonalization', 'linear-algebra']"
2718852,Nim game special case [duplicate],"This question already has an answer here : Winning strategy in game of cutting rectangle (1 answer) Closed 6 years ago . Locked in a room with your worst enemy. On a table in the centre of the room is a bar of chocolate, divided into 23x45 squares in the usual way. One square of the chocolate is painted with a bright green paint that contains a deadly nerve poison. You and your enemy take it in turns to break off one or more squares from the remaining chocolate (along a straight line) and eat them. Whoever is left with the green square must eat it and die in agony. You may look at the bar of chocolate and then decide whether to go first or second. Describe your strategy in the following 3 cases: a) The poisonous square is in the corner b) The poisonous square is the middle square c) The poisonous square is adjacent to the middle (consider more cases if needed). a) In the first case, I will play first and cut the chocolate along a line so that I leave my enemy with a square piece 23x23 having the deadly piece in one of its four corners. This is obviously a losing position for him. b) In the second case, I don't see any winning strategy for me, if I play first. Apparently, I can't leave my enemy with a square piece of chocolate! c) Well, I would start with eating the part that contains the middle square and then leaving my enemy with a rectangular piece that contains the poisonous square in the middle of its first 1x45 column. Any ideas on how to continue?","['combinatorics', 'combinatorial-game-theory']"
2718857,Applying the ratio test and uniform convergence,"I have been trying to apply the ratio test onto $\dfrac{z^n}{1+z^n}$. After the usual initial steps. I need to show that $$\lim_{n \to \infty} \left|\dfrac{z(1+z^n)}{1+z^{n+1}}\right|<1$$ I am unsure of how to make further progress, and so what is the trick from here? Once this is shown, we can therefore say that the series
 $$\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$$
converges absolutely. Before I can deduce $\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$ is holomorphic on $D=\{z \in \mathbb{C} \mid |z| <1\},$ would I have to show uniform convergence? If so, what is the simplest way?","['uniform-convergence', 'limits', 'calculus', 'summation', 'convergence-divergence']"
2718864,Symmetry of second derivative - Sufficiency of twice-differentiability,"Symmetry of second derivative states that for $u=u(x,y)$ if $u_x,u_y$ exists and $u_{xy},u_{yx}$ exists and continuous then $u_{xy}=u_{yx}$. I proved that statement using the mean value theorem. While I was looking in Wikipedia there is a section called ""Sufficiency of twice-differentiability"", if $u(x,y):E^{\text{open set}}\subset \Bbb R^2\to \Bbb R$ and $u_x,u_y,u_{yx}$ exists everywhere and $u_{yx}$ is continuous at a point in $E$ then $u_{xy}$ exists at that point and equal to $u_{yx}$. My question is, while I proved Symmetry of second derivative I had to assume the continuity of $u_{xy}$ and $u_{yx}$, so how can I prove that this is true without even assuming the existence of one of the second derivative? I'm sitting on this for a long time and I couldn't think on any starting point and would love help with this.","['multivariable-calculus', 'partial-derivative']"
2718890,The positive net of a weak* convergent net is weak* convergent.,"Suppose $X$ is a unital $C^*$-algebra, $i:X\to X^{**}$ is the natural isometric inclusion as Banach space. Denote $S_X=\{x\in X:\|x\|=1\}$, $S_{X,+}=\{x\in X:\|x\|=1,x\ge 0\}$, so does $S_{X^{**},+}$. I need to prove that $i(S_{X,+})$ is weak* dense in $S_{X^{**},+}$. The following is my idea: Suppose $\pi\in S_{X^{**},+}$, by Goldstein theorem, there exists a net $(x_\lambda)\subset S_{X}$ such that $i(x_\lambda)\to \pi$ weak*. Can we prove that $i(|x_\lambda|)\to \pi$ weak*? If $X=\ell^1\Gamma$, actually, the above idea works, but I don't know whether it is correct generally.","['c-star-algebras', 'banach-spaces', 'functional-analysis', 'weak-convergence', 'amenability']"
2718920,Level Sets of Symmetric Functions,"If $f(x,y)$ is symmetric (i.e., $f(x,y)=f(y,x)$) and $C^1$ such that its first derivatives $f_x$ and $f_y$ are always strictly positive, is it necessarily the case that there is a $C^1$ function $g(z)$ with strictly positive derivative such that $g(x)+g(y)$ has the same level curves as $f(x,y)$? It seems to me that this ought to be true, but I am unable to prove it or to find a counterexample.  Some sort of smoothness assumption seems to be necessary, as the examples $f(x,y) = \max(x,y)$ and $f(x,y) = x+y+\max(x,y)$ show.","['real-analysis', 'plane-geometry', 'linear-algebra', 'functions']"
2718943,Prove this Hard geomtry with $HI=HP$,"Let $ABC$ be a triangle with incenter $I$ , and suppose that $AI$ , $BI$ , and $CI$ intersect $BC$ , $CA$ , and $AB$ at $D$ , $E$ , and $F$ , respectively. Let the circumcircles of $BDF$ and $CDE$ intersect at $D$ and $P$ , and let $H$ be the orthocenter of $DEF$ . Prove that $HI=HP$ I have been thinking about this geometry test question for a long time, but I haven't made it yet. This question was given to me by a friend of mine. According to him, it was the last question of a contest test","['contest-math', 'geometry']"
2718968,An integral inequality (Rudin $L^p$-spaces 3.12),"I've been learning about $L^p$-spaces and came across this problem in Rudin's Real and Complex Analysis: Suppose $\mu(\Omega)=1$ and $h:\Omega \to [0,\infty]$ is measurable. If $A= \int_\Omega {h d\mu}$, prove that:
  $$ \sqrt{1+A^2} \leq \int_\Omega \sqrt{1+h^2}d\mu \leq 1+A $$ I've already shown that $\sqrt{1+A^2} \leq \int_\Omega \sqrt{1+h^2}d\mu$ using Jensen's inequality applied to the convex function $\varphi(x)=\sqrt{1+x^2}$. I can't seem to show the latter inequality.
Any help or hints would be appreciated. Thank you in advance!","['functional-analysis', 'lp-spaces', 'measure-theory']"
2718981,How do you extend a basis?,"I recently posted a question about this; now that I've acquired some new info, I have some follow up questions about extending a basis (I'm not too sure if this is actually the name for it, so my apologies if it isn't.) Let's say I have a set $\{(1,1,1)\}$ and wanted to add vectors such that the set is a basis of $\mathbb R^3$ . Tt's rather obvious that I could easily find a couple of vectors that would make the set linearly dependent, but I'm looking for a method that I could depend on. I've seen a couple of ways of doing this, neither of which I understand fully. The first is using dot products. Let's call the vectors I want to create $a,b \in \mathbb R$ . I want an $a$ and a $b$ such that $a \cdot (1,1,1) = 0$ and that $b \cdot (1,1,1) = 0$ . If I've understood things correctly, this is because if the dot product of two vectors is $0$ , then they are orthogonal. If $a$ and $b$ are both orthogonal to $(1,1,1)$ , then $\{(1,1,1), a, b)\}$ is linearly independent, and therefore a basis on $\mathbb R$$^3$ . Is that all true? If so, then I suppose I understand this solution conceptually, but I'm not sure how to go about it otherwise. The second method I've found is by creating a matrix $A$ out of my vectors, create a basis of the nullspace $N(A)$ , and then go from there. Honestly, I don't understand much of anything here. I know how to create a matrix from my 3 vectors (2 of which are unknown). I also understand that the nullspace of A is the set of vectors such that, if multiplied with $A$ , the product is the zero vector (if this is also wrong, please correct me.) I also understand that if my matrix is built of linearly independent vectors, the nullspace is simply the zero vector. However, I don't understand what it means to create a basis of the nullspace, and thus don't know how to carry on. The last thing that bothers me is, I don't see how these two methods are any more powerful/helpful than proving that having any linear combination of the vectors equaling zero implies that the scalars multiplying the vectors are also zero. Any help is greatly appreciated, thank you.","['linear-algebra', 'vector-spaces']"
2719008,Prove $\lim_{m\to\infty}\sum_{k=1}^m\frac{2^{-k}}{k} = \log 2$,"While working on a harder double sum, I (erroneously) reduced it to the sum below, which I recognized numerically to rapidly converge to $\log 2$. Prove $$\lim_{m\to\infty}\sum_{k=1}^m\frac{2^{-k}}{k} = \log 2$$ The cute observation is that if you replace the $2^{-k}$ with $(-1)^{-k}$ you get the same result (only now the convergence becomes conditional).",['sequences-and-series']
2719021,Two skew symmetric matrices of same rank are congruent.,"Let $A$ and $B$ be two skew symmetric matrices of order $n$ over a finite field with characteristic not equal $2.$ Also let $\mathrm{Rank}(A)=\mathrm{Rank}(B)$ (which is even, we know). Then I want to show that there is an invertible matrix $P$ such that $P^tAP=B$ . I need some idea to prove this. Thanks","['matrices', 'abstract-algebra', 'skew-symmetric-matrices', 'linear-transformations', 'linear-algebra']"
2719039,Prove that $x\mapsto e^x$ is continuous at $x_0 = 1$ ($\delta-\varepsilon$ proof),"Prove that the function $$f(x)=e^x:=\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n$$ is continuous at $x_0=1$ using the delta epsilon definition of continuous, which is: $$\forall \varepsilon >0 \exists \delta>0 (\forall x\in D:|x-x_0|<\delta) |f(x)-f(x_0)|<\varepsilon$$ Since, in this particular context, the limit inside $e^x$ was proved to be convergent for all $x$ using the Bernoulli inequality and monotone convergence theorem, I'm struggling to see how I can apply a delta epsilon proof here.  In my limited experience, I've applied it to nothing more than simple algebraic functions, but this is pretty significantly different.  How do I start?","['real-analysis', 'limits', 'exponential-function', 'continuity', 'epsilon-delta']"
2719061,Evaluate $\sum_{r=0}^{100} {(-1)^{r} {100 \choose r} {r^{50}}}$,"The problem is to the evaluate the following sum: $$\sum_{r=0}^{100} {(-1)^{r} {100 \choose r} {r^{50}}}$$ I don't see how I can get started on this, since the function attached to the combination is polynomial, not an exponential. I also tried to look for a combinatorial approach, using the inclusion exclusion principle. I guess it can be interpreted as the arrangement of 100 distinct items in 50 places, however the answer doesn't seem to be correct. Would love some help on this one!","['combinatorics', 'binomial-theorem', 'binomial-coefficients', 'discrete-mathematics']"
2719065,Spectrum of Self-adjoint Diagonalizable Operators,"Let $L$ be a self-adjoint operator on a Hilbert space $H$. Moreover, assume that $L$ is diagonalizable, that is, $H$ admits an orthonormal basis consisting of eigenvectors of $L$. Is it possible that the spectrum of $L$ contains elements which are not eigenvalues of $L$?","['functional-analysis', 'spectral-theory', 'operator-algebras']"
2719088,$L_2$ norm of the generalized inverse of the empirical cumulative distribution function,"Can anyone give me some insight on how to solve this problem? Any help would be greatly appreciated ! Let $(X_n)_{n \geq 1}$ an i.i.d. sequence of real valued r.v.'s with c.d.f. $F$ which is stricly increasing and has two derivatives everywhere. Let $F_n(x) = \frac{1}{n}\sum_{i=1}^n {\bf 1}_{X_i \leq x}$ be the empirical c.d.f., $F_n^{(-1)}(u) = \inf\{x \in \mathbb{R} | F_n(x) \geq u\}$ be the generalized inverse of the empirical cumulative distribution function and $F^{-1}$ be the inverse of $F$. Note that $F_n^{(-1)}(u) = X_{(k_n)}$ where $k_n = \lceil nu \rceil$. Is this true that under the above hypothesis, 
\begin{equation}
\mathbb{E}\big[\big(F_n^{(-1)}(u) - F^{-1}(u)\big)^2\big]
\end{equation}
is uniformly bounded? Thank you","['normed-spaces', 'probability-theory', 'asymptotics', 'statistics']"
2719092,"Proving That $f: \mathbb{Z} \to \mathbb{Z}$, where $f(x) = x + 10$, Is Surjective","I am trying to prove that $f: \mathbb{Z} \to \mathbb{Z}$, where $f(x) = x + 10$, is surjective. I have included my reasoning and would appreciate it if people could check whether it is correct. My Proof Let $b \in \mathbb{Z}$. We want to show that, for any $b \in \mathbb{Z}$, there exists some $x \in \mathbb{Z}$ such that $b = f(x) = x + 10$. $\therefore x = b - 10$ We constructed an integer $x$. $\therefore f(x) = f(b - 10)$ $= (b - 10) + 10$ (By the hypothesis.) $= b$ $= x + 10 \ \ \ Q.E.D. \ \ \ $ We have now demonstrated that $b = f(x) = x + 10$. I would greatly appreciate it if people could please take the time to review my proof and the reasoning I have included in it.","['functions', 'proof-verification']"
2719108,Solving $dy/dt = k(A-y)$ (phrased as rate of change probelm),"The Problem The rate at which temperature decreases in a house is proportional to the difference between the current house-temperature and the temperature outside. It's -10 degrees outside, at the start it's 20 degrees inside the house and after an hour it's 17 degrees. After how long is it minus degrees inside? My solution (attempt) We know that $$dy/dt = k(y+10) \\ y(0) = 20 \\ y(1) = 17$$ We do a variable substitution $u(t) = y(t) + 10$ and we now have $$ du/dt = ku \\ u(0) = 30 \\ u(1) = 27$$ A solution to the equation is $u(t) = e^{kt} + A$ $$u(0) = 30 \rightarrow e^{0k}+A =  1+A = 30 \rightarrow A = 29$$
$$u(1) = 27 \rightarrow e^k + 29 = 27 \rightarrow k = ln(-2) \rightarrow Undefined$$ And that's where I'm stuck. Am I supposed to re-write the difference $y+10$ to something else?",['ordinary-differential-equations']
2719157,Connectedness and path connectedness of a set whose intersection with lines is open,"Let $A \subseteq \mathbb{R^2}$ a not open set with the property: Its intersection with every line L is open in L with the induced Euclidean topology. If the set is connected is it necessarily path connected ? Am not sure if is it true , i tried with the standard technique : Let $\alpha \in A$ and define $\Pi = \{\omega \in A : \text{there is a path from }\alpha \text{ to } \omega\}$ and tried to show that $\Pi$ is clopen by using the property of $A$ but i couldn't prove anything , it only works when $A$ is open. Any help would be nice , thanks !","['general-topology', 'path-connected', 'connectedness']"
2719159,Can two different prime knots have a Dowker-Thistlethwaite code in common?,"I was thinking about knot invariants and whether we could define an equivalence class on the set of all Dowker-Thistlethwaite codes for a knot, and whether said equivalence classes, combined with some indicator of chirality, would be a complete knot invariant for the prime knots, when it occurred to me that I have never seen a proof that a DT code generates a unique prime knot (up to chirality). I asked Professor Google but couldn't seem to turn anything up. Tait, of course, posed the ménage problem after following a similar line of logic, trying to put an upper bound on the number of knots of each crossing number, but I haven't seen an explicit proof from Tait or Dowker or Thistlethwaite or anyone else that their notation is uniquely decipherable for the prime knots up to chirality. Is the algorithm for deciphering a knot from a DT code technically a constructive proof of such uniqueness, or is there ambiguity in the algorithm such that, for some DT code somewhere, 2 different prime knots could be constructed from it?","['knot-theory', 'general-topology', 'knot-invariants']"
2719164,Show that $\sum_{k=1}^{\infty}\frac{2^{-k}}{k}=\sum_{k=1}^{\infty}\frac{(-1)^{k-1}}{k}$ without evaluating either sum,"Show that
$$
\sum_{k=1}^{\infty}\frac{2^{-k}}{k}=\sum_{k=1}^{\infty}\frac{(-1)^{k-1}}{k}
$$
without evaluating either sum. This is inspired by Prove $\lim_{m\to\infty}\sum_{k=1}^m\frac{2^{-k}}{k} = \log 2$ . This, of course,
follows from
$\ln(1+x)
=\sum_{k=1}^{\infty}\frac{(-1)^{k-1}x^{k}}{k}
$
for
$-1 \lt x \le 1$.
But I am not allowing that! What I am looking for
is a way to
manipulate one of the series
to convert it to
the other.","['summation', 'sequences-and-series']"
2719259,Prove symmetries of eigenvalues of symplectic matrices,"$M \in \mathrm{GL}(2m,\mathbb{R})$ is said to be symplectic if $$ M^T JM=J,$$ $$J =\begin{bmatrix} 0 & -I_m \\ I_m & 0\end{bmatrix}. $$ Suppose $M \in \mathrm{GL}(2m,\mathbb{R})$ is symplectic and has $\det(M)=1$, let $\chi_M(x)$ be the characteristic polynomial of M, prove that (a) If $\lambda \in \mathbb{C}, \lambda \neq 0$ is a root of $\chi_M$ with multiplicity $d$, then $\frac{1}{\lambda}$ is a root of $\chi_M$ with the same multiplicity $d$. (b) $1,-1$ have even multiplicity in $\chi_M$. It's difficult to argue part (b) rigorously, and I don't think that it is obvious from (a). The given hint is that $\chi_M = \chi_{M^{-1}}$, and therefore for $z \in \mathbb{C}, z \neq 0$ we have $$\chi_M(z) = z^{2m}\chi_M(\frac{1}{z}).$$","['matrices', 'eigenvalues-eigenvectors', 'symplectic-linear-algebra', 'linear-algebra']"
2719283,Is there a good example of a subgroup of an infinitely generated abelian group that is not isomorphic to a quotient of that group?,"Whilst I understand the classification of the finitely generated abelian groups, this had me wondering whether there is a subgroup $H$ of a general (necessarily infinitely generated) abelian group $G$ such that $H$ is not isomorphic to any quotient $G/N$ of $G$.","['abelian-groups', 'abstract-algebra', 'infinite-groups', 'group-theory']"
2719298,Why do we have $ \sin(x) = x \prod_{n=1}^{\infty} ( 1 - \frac{4}{3} \sin^2 (\frac{x}{3^n})) $,Show that $$ \sin(x) = x \prod_{n=1}^{\infty} ( 1 - \frac{4}{3} \sin^2 (\frac{x}{3^n})) $$ I know $ 1 - \sin^2x = (1 - \sin x) (1 + \sin x) $ but I'm not sure how to apply it. Or maybe we need $ 2 \sin^2 (x) = 1 - \cos(2x)$?,"['infinite-product', 'trigonometry', 'calculus']"
2719305,Computing fundamental vector field,"Let us fix $k_{1},\cdots,k_{n}\in \mathbb{Z}$, $T^n=\big\{(z_{1},\cdots,z_{n})\mid |z_{k}|=1,k\in \{1,\cdots,k\}\big\}$ and $w=\frac{i}{2}\sum dz_{i}\wedge d\overline{z_{i}}=\sum dx_{i}\wedge dy_{i}= \sum r_{i}dr_{i}\wedge d\theta_{i}$ the symplectic form on $\mathbb{C}^n$. I am trying to prove that $\varphi\colon T^n\longrightarrow \text{Diff}(\mathbb{C}^n)$ where $$\varphi_{(t_{1},\cdots,t_{n})}(z_{1},\cdots,z_{n})=(t_{1}^{k_{1}}z_{1},\cdots,t_{n}^{k_{n}}z_{n})$$ is a hamiltonian action with moment map given by $\mu \colon \mathbb{C}^n\longrightarrow (t^n)^*\cong\mathbb{R}^n$, $$\mu(z_{1},\cdots,z_{n})=-\frac{1}{2}(k_{1}|z_{1}|^2,\cdots, k_{n}|z_{n}|^2).$$ I have already proven the equivariance, so I need to show that for all $X\in t^n\cong \mathbb{R}^n$, $$d\mu^x=\iota_{X^\#}\omega.$$ Suppose that $X=(a_{1},\cdots,a_{n})\in \mathbb{R}^n$. Then, $\mu^{X}((z_{1},\cdots,z_{n}))=-\frac{a_{1}k_{1}}{2}|z_{1}|^2-\cdots-\frac{a_{n}k_{n}}{2}|z_{n}|^2$. Then, $$d\mu^X= -\frac{a_{1}k_{1}}{2}\overline{z_{1}}\frac{\partial}{\partial z_{1}}-\cdots-\frac{a_{n}k_{n}}{2}\overline{z_{n}}\frac{\partial}{\partial z_{n}}-\frac{a_{1}k_{1}}{2}z_{1}\frac{\partial}{\partial \overline{z_{1}}}-\frac{a_{n}k_{n}}{2}z_{n}\frac{\partial}{\partial \overline{z_{n}}}.$$ Now, I have problems when computing $X_{(z_{1},\cdots,z_{n})}^{\#}$ for $(z_{1},\cdots,z_{n})\in \mathbb{C}^{n}.$ I should take a curve in $T^n$ that crosses the identity at $0$ and such that the derivative at $0$ is $(a_{1},\cdots,a_{n})$. I have taken $(e^{a_{1}ti},\cdots,e^{a_{n}ti})$. Then, I have to compute the derivative at 0 of $\varphi_{(e^{a_{1}ti},\cdots,e^{a_{n}ti})}(z_{1},\cdots,z_{n})$ which is $(k_{1}a_{1}z_{1}i,\cdots,k_{n}a_{n}z_{n}i$). I do not know how to continue... I think that the problem is that I do not understand how is that written is terms of the basis of $T_{(z_{1},\cdots,z_{n})}\mathbb{C}^n\cong \mathbb{R}^{2n}$. Can anyone help me, please? $\mathbf{EDIT}$: I believe that it is not correct to take the curve $(e^{a_{1}ti},\cdots,e^{a_{n}ti})$ because its derivative at $0$ is $i(a_{1},\cdots,a_{n})$.","['symplectic-geometry', 'differential-geometry']"
2719317,$Sp(2n)$ is embedded in $GL(2n)$ and has dimension $2n^2+n$,"Let $Sp(2n):=\{A\in\mathbb{R}^{2n\times 2n}\mid A^tA_0A=A_0\}$ be the group of symplectomorphisms from $(\mathbb{R}^{2n},\omega_0)$ to itself, where: \begin{gather}
A_0
:=
\begin{bmatrix}{}
0 & I\\
-I & 0\\
\end{bmatrix}\in\mathbb{R}^{2n\times 2n}
\end{gather} represents the standard symplectic form $\omega_0$ with respect to the canonical basis of $\mathbb{R}^{2n}$. Prove that $Sp(2n)$ is an embedded submanifold of $GL(2n)$ and has dimension $2n^2+n$. I know the essential idea is to look at the map:
\begin{align*}
f:GL(2n)&\to \text{Sympl}(2n)\\
A & \mapsto A^tA_0A
\end{align*} where $\text{Sympl}(2n):=\{A\in\mathbb{R}^{2n\times 2n}\mid A=-A^t\text{ and }\det A\neq 0\}$, which is the submanifold of symplectic forms and has dimension $\frac{(2n)^2-2n}{2}$. Considering that $f$ is a submersion, we have $Sp(2n)=f^{-1}(A_0)$ is an embedded submanifold with dimension $\dim (GL(n))-\dim (\text{Sympl}(2n))=(2n)^2-\left(\frac{(2n)^2-2n}{2}\right)=2n^2+n$ by the Regular Value Theorem. My question is really basic: how do I prove $f$ is a submersion? I've tried to calculate $df_A(M)$ by taking the curve $\alpha(t)=A+tM$, so that:
\begin{align*}
df_A(M)&=(f\circ \alpha)'(0)\\
&=(A^tA_0A+tA^tA_0M+tM^tA_0A+t^2M^tA_0M)'(0)\\
&=A^tA_0M+M^tA_0A
\end{align*}
but I don't know how to prove this is surjective, and it seems complicated. Is there some trick to it? Or is there a better way?","['symplectic-geometry', 'submanifold', 'smooth-manifolds', 'differential-geometry']"
2719322,Showing Orthogonal Projection Matrix Multiplied by Full-Rank Matrices is Positive-Definite,"Here is some useful information pertaining to my question: Let $X \in\mathbb{R}^{n \times m}$ and $Z \in\mathbb{R}^{n \times p}$ be full-rank matrices. Define $B = I_n - X(X^{T}X)^{-1}X^{T}$. Assume that the columns of $X$ are linearly independent from the columns of $Z$. I am trying to show that $Z^{T}BZ$ is positive definite. My first plan of attack was to (1) show that $Z^{T}BZ$ is an orthogonal projection matrix, (2) prove that $I$ is the only positive definite orthogonal projection matrix, and (3) prove that $Z^{T}BZ=I$. This fell through because I felt that there was not enough information about $Z$ to prove (1). I am pretty sure that I will ultimately need to show either (i) $z^{T}Z^{T}BZz>0$ $\forall$ $z\in \mathbb{R}^p$ OR (ii) all the eigenvalues of $Z^{T}BZ$ are positive. I think my biggest problem stems from not knowing how to deal with $Z^{T}$ and $Z$... clearly they are important, as it seems that we can only prove that $B$ is positive semi definite. However, I am unsure how this full-rank matrix can ""transform"" $B$ from psd to pd. Any assistance would be greatly appreciated! If it helps, I have already shown that B is an orthogonal projection matrix and that B is psd. :)","['projection-matrices', 'matrices', 'positive-definite', 'matrix-rank', 'linear-algebra']"
2719328,Showing a collection of sets is pairwise disjoint,"$\bf Homework \; Problem. $ Suppose we have $A_1 \subseteq A_2 \subseteq A_3 \subseteq ... $.
   Also, let us make $A_0 = \varnothing, B_1 = A_1, B_2 = A_2 \cap
 A_1^c, ... B_n = A_n \cap A_{n-1}^c$. Show that $(B_n)$ is
   pairwise disjoint and also show $A_n = \bigcup_{k=1}^n B_k $. Attempt: by definition, if I can show $B_i \cap B_j = \varnothing$ for $i \neq j$, then Im done. So, we have $$ B_i \cap B_j = A_i \cap A^c_{i-1} \cap A_j \cap A^c_{j-1} $$ If $i > j$, then $A_j \subseteq A_i $ and this implies $A_j \cap A_i = A_j $. So far we have $$ B_i \cap B_j = A_j \cap A^c_{i-1}  \cap A^c_{j-1} $$ Now, we know $A_{j-1} \subseteq A_j $ and $A_{i-1} \subseteq A_i $. by simple reasoning, we must have that $A_{j-1} \subseteq A_{i-1}$ and so $A_{i-1}^c \subseteq A_{j-1}^c$ which means that  $A^c_{i-1}  \cap A^c_{j-1} = A^c_{i-1} $. Thus, we now have that $$ B_i \cap B_j = A_j \cap A_{i-1}^c $$ Now, since $i > j$, it must be the case that $j=i-1$ in which case we obtain the desired result, otherwise $i-1 > j$ and so $A_j \subseteq A_{i-1}$ in which case it is evident that $A_j \cap A_{i-1}^c = \varnothing$ and pairwise disjointness of the sequence of sets follows. Next, we can show induction to show the equality. For $n=1$ we have $A_1 = B_1$. Assume the result is true for some $n$, then $$ \bigcup_{k=1}^{n+1} B_k = \bigcup_{k=1}^n B_k \cup B_{n+1} = A_n \cup B_{n+1} $$ And since $B_{n+1} = A_{n+1} \cap A_n^c $, then $$ \bigcup_{k=1}^{n+1} B_k =A_n \cup B_{n+1} = A_n \cup (A_{n+1} \cap A_n^c) = (A_n \cup A_{n+1} ) \cap(A_n \cup A_n^c) $$ Since $A_n \cup A_n^c$ is the whole then we only have $A_n \cup A_{n+1}$ which becomes just $A_{n+1}$ and the result follows. My question is: Is this correct? am I overcomplicating this problem or this is correct way to do it?",['elementary-set-theory']
2719342,Prove $\operatorname{height}(P) \cdot \operatorname{width}(P) \geq |A|$ in partially ordered set?,"Let $P = (A, \preceq)$ be a poset. Problem: Use Mirsky’s theorem to show $$\operatorname{height}(P) \cdot \operatorname{width}(P) \geq |A|.$$ Please Note: Mirsky's Theorem is $\text{maximum size of a chain cover} = \text{minimum size of an anti chain}$ My approach: If we partition $A$ into $k$ anti-chains, $A = A_1 \cup A_2 \cup A_3 \cup \cdots\cup A_k$. Then, there exists $$|A_i| \geq \frac{|A|}{\operatorname{height}(P)}$$ I'm not sure this is correct and how do we come up with the width and height formula above?","['discrete-geometry', 'combinatorics', 'order-theory', 'discrete-mathematics']"
2719363,"Prove the existence of graph, which doesn't contain three vertices with same degree.","Prove that for any $n \ge 3$, with $n \in \mathbb{N}$, there is a $G$ graph with $n$ vertices, which doesn't contain three vertices with same degree. I started with induction by $n$. For $n=3$, we can take vertices $\{a,b,c\}$, and edges $e_1 = ab, e_2 = ac$ Now assuming that it is valid for $n$, we must show it for $n+1$. 
I tried to seperate two cases. Case 1: if there is a vertex with some degree, then there is also another one (graph contains two vertices with same degree), and adding a new vertex, will just add biggest degree + 1 edges Case 2: When there is at least one vertex, with unique degree, and new vertex will be connected with edges, as much it has vertex with unique degree. But it seems wrong. Any tips or helps are welcome.","['graph-theory', 'discrete-mathematics']"
2719425,What is known about sequences $a_{n}$ such that $\sum_{n=1}^{N}(-1)^{a_{n}}n = 0$?,"The question I am really asking is, how many sequences $a_{n}$ satisfy the inequality for a given $N$. Clearly the number is even due to the symmetry of the problem. Also there is no solutions for $N\equiv 1 \mod 4$ or $N\equiv 2 \mod 4$ since the number of the first $N$ natural numbers is odd in these cases (you cannot halve the amount into a positive and negative set). So for example: $N=3\Longrightarrow$ $$1 + 2 - 3=0 \quad\quad -1 - 2 + 3=0$$
$N=4\Longrightarrow$ $$+1 - 2 - 3 +4=0 \quad\quad -1 + 2 + 3 -4=0$$
$N=5\Longrightarrow$ $$\text{No solutions.}$$
$N=6\Longrightarrow$ $$\text{No solutions.}$$
$N=7\Longrightarrow$ $$\geq 4 \,\,\text{cases, don't want to write them out.}$$",['sequences-and-series']
2719438,"Application of the Inverse function theorem for $f:M_n(\mathbb{R})\rightarrow M_n(\mathbb{R})$, $f(A)=A^2$.","Let $f: M_n(\mathbb{R}) \rightarrow M_n(\mathbb{R})$ defined as $f(A) = A^2$. 1) Show that $f$ is $\mathcal{C}^1$ and calculate its differential. 2) Show that there exists $g$ a $\mathcal{C}^1$ function defined on a neighbourhood of the identity matrix such that $\forall A \in V$, we have $g(A)^2 = A$ I managed to show that beginning. For a matrix $A \in M_n(\mathbb{R})$ we have $f(A) = A^2$ thus if we defined $A = (a_{i,j})_{1\leq i,j \leq n}$ we then have $f(A) = \sum^{n}_{k1}a_{i,k}a_{k,j}$. Thus it's a polynomial, thus it's $\mathcal{C}^1$ (and even $\mathcal{C}^{\infty}$). Now, we have $f(A+H) = A^2 + AH + HA + H^2$, thus I deduce that $df(A)H = AH + HA$. Now, for the second question: let $H \in M_n(\mathbb{R})$ then $df(I)= 2I$ thus the differential is an isomorphism, we can use the inverse function theorem. We thus have two neighbourhoods $V \subset M_n(\mathbb{R})$ and $W\subset M_n(\mathbb{R})$ such that $f: V \rightarrow W$ is a diffeomorphism of class $\mathcal{C}^1$. Now, we can put $g = f^{-1}$ that is $\mathcal{C}^1$, and $g(A)^2 = (f^{-1})^2(A)$, and here I struggle to show that $(f^{-1})^2(A) = A$. Can someone help me out?","['derivatives', 'differential', 'inverse-function', 'inverse-function-theorem']"
2719443,Integral curves of height function's Hamiltonian vector field,"Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$. Fixed $z \in \Bbb R^3$, we define $h_z\colon \Bbb S^2 \to \Bbb R$ by $h_z(p) \doteq \langle p,z\rangle$. I computed the Hamiltonian vector field of $h_z$ as $X_{h_z}(p) = z \times p$. I would like to find the integral curves of $X_{h_z}$. This means solving the system $\alpha'(t) = z \times \alpha(t)$. If $z = (a,b,c)$ and $$Z = \begin{pmatrix} 0 & -c & b \\ c & 0 & -a \\ -b & a & 0\end{pmatrix},$$we want to solve $\alpha'(t) = Z\alpha(t)$. Let's put the initial condition $\alpha(0) = p_0$ to ensure uniqueness of solution. Then $$\alpha(t) = \exp(tZ)p_0,$$and we use Rodrigues' rotation formula to get $$\alpha(t) = \left({\rm Id} + \frac{\sin \|tz\|}{\|tz\|}Z + \frac{1-\cos\|tz\|}{\|tz\|^2}Z^2\right)p_0.$$I'm having trouble visualizing this solution even in simple cases. We can assume that $p_0 = (0,0,1)$ since rotations are symplectomorphisms. If for example $z = (0,0,1)$, then $\alpha$ degenerates to a point. If $z = (0,1,0)$, then $\alpha(t)$ has the form $(\ast,0,\ast)$ and so is a pre-geodesic. What happens in the general case? My sign convention is that if $(M,\omega)$ is a symplectic manifold and $f \in \mathcal{C}^\infty(M)$, then $X_f$ satisfies $\iota_{X_f}\omega = {\rm d}f$.","['symplectic-geometry', 'differential-geometry']"
2719563,Spivak: Calculus on Manifolds Norms (Problem 1.1),"I found it difficult to prove this problem due to the following fact. I want to derive it myself but am having trouble laying out the proof. First, I want to show the following holds: Show that $$(\sum_{i=1}^n |x_i|)^2= \sum_{i=1}^n x_{i}^2 + 2\sum_{i\neq j}|x_i||x_j|$$ I would also like a little more emphasis on the notation being used for the second sum. Thanks!","['multivariable-calculus', 'real-analysis']"
2719566,Find all rational points on $x^2 + y^2 = 17$,"So yes, this was asked before but I'm stuck on a specific step. So to solve this, I tried to find the the intersection between the line $y=m(x-1)+4$ (we have $(1, 4)$ as a rational point on the circle) and $x^2 + y^2 = 17$. So simply substitution, $x^2  + (m(x-1)+ 4)^2 = 17 \implies m^2(x-1)^2 + 8m(x-1) + x^2 - 1 = 0$. Now I want to find the roots to get all the rational points but I'm not sure how one would get the roots of this equation. Stuck on the algebra. Would appreciate help.","['algebra-precalculus', 'elementary-number-theory', 'geometry']"
