question_id,title,body,tags
4767055,Where exactly does the integral definition of the gradient come from?,"In the book ""Essential mathematical methods for physicists"" from Weber and Arfken, they define the integral form of the gradient,divergence and curl, althougth they give sections before an explanation of the upper parts of the divergence and curl definition, I don´t understand where does the upper part of the gradient definition come from and also I don´t understand why the integral of $d\tau$ and its limit appears in all the three definitions.","['field-theory', 'vector-fields', 'derivatives']"
4767120,Extension of Wahba's problem: finding two unknown rotations surrounding a known rotation,"Wahba's Problem seeks to find the $3\times3$ rotation matrix which minimises: $$ J(\boldsymbol{\mathrm{R}}) = \frac{1}{2} \sum_{k}|| \boldsymbol{{w}}_k - \boldsymbol{\mathrm{R}}\boldsymbol{v}_k||^2 $$ I have a similar but potentially more difficult problem, where my transformation between two vectors is given by $$ \boldsymbol{w}_k \approx \boldsymbol{\mathrm{R}}_1\, \boldsymbol{\mathrm{R}}_{2,k}\, \boldsymbol{\mathrm{R}}_3\, \boldsymbol{v}_k $$ where $\boldsymbol{\mathrm{R}}_{2,k}$ is a known Euler rotation about two axes, and $k=1, 2, \cdots, N$ . I wish to determine the best fit rotations $\boldsymbol{\mathrm{R}}_1$ , and $\boldsymbol{\mathrm{R}}_3$ which describe the two sets of vectors. With the exception of using a large set of data in a nonlinear optimisation, is there any way to estimate these two rotation matrices? Any help is appreciated - Thank you!","['geometry', 'matrices', 'linear-algebra', 'linear-transformations', 'rotations']"
4767122,Power series solution of differential equation $x^2y'=y$,"Can we solve this DE using power series method?
a power series solution of $$x^2y'=y$$ I know it can be solved by variable separation $$y=ce^{-1/x}$$ but when i try to solve it by the method of power series it leads to $$y = c_1x$$ this was asked before but no satisfying asnwer was given Power series solution of $x^2y'=y$ .","['power-series', 'calculus', 'ordinary-differential-equations']"
4767149,Proof of $e\leq3v-6$ for planar graphs without Euler's formula,"For a short talk for an audience not familiar with graph theory I want to give an informal proof that $e\leq3v-6$ holds in all planar graphs with $v>2$ and I don't want to use Euler's formula.  My plan is as follows: I'll informally introduce what a (simple, undirected) graph is. I'll then introduce planar graphs as ""can be drawn without edge crossings"". Finally, I'll introduce maximal planar as ""loses planarity if an edge is added"". Then I'll justify why maximal planar graphs consist of ""triangles"" - of cycle graphs $C_3$ that enclose faces of $G$ .  I think it's OK to do that visually. Now the main part of my proof.  We'll start with a given a drawing of a maximal planar graph $G$ without edge crossings - the ""original"".  We will use the following algorithm to copy this drawing which makes sure that the current (partial) copy is always a subgraph of $G$ consisting of triangles of the original which are enclosed by a closed cycle of edges. Start with one of the inner triangles of $G$ . As long as we're not finished, one of the edges of our construction is an outer edge of the current copy but an inner edge of the original.  It is thus part of a triangle $T$ we haven't copied yet. If we find such a triangle $T$ where the third vertex is missing, we add this vertex to our copy.  We will then also need to add the two edges connecting this vertex with the existing edge (which will now become an inner edge). If we can't find a triangle as described in the last step, we have an outer edge $E$ which needs to be connected to a vertex $V$ which is already a part of the current copy. $V$ must obviously be an outer vertex of the current copy.  We can also find $E$ and $V$ in such a way that $V$ is directly connected to one of the endpoints of $E$ .   (Otherwise, adding the missing edges between $E$ and $V$ would create a cycle graph with more than 3 vertices.)  That means we only have to add one edge. At the start of the algorithm, we have 3 outer vertices (and 3 outer edges).  At the end, we must again have 3 outer vertices (and 3 outer edges) as the result is maximal planar.  Each step of type 3 adds an outer vertex, each step of type 4 converts an outer vertex to an inner vertex.  (Alternatively, each type 3 step increases the number of outer edges by one, each type 4 step decreases it by one.)  We will thus need $n$ type 3 steps and the same amount $n$ of type 4 steps.  These add $3n$ edges and $n$ vertices to our initial triangle and so we end up with $e=3n+3$ edges and $v=n+3$ vertices, proving $e=3v-6$ . $e\leq3v-6$ for planar graphs is now a simple corollary. Are there any glaring holes in this proof that I have missed? Edit: I've added a picture to demonstrate the algorithm.  The starting triangle is blue.  Triangles added by step 3 are green, triangles added by step 4 are orange.","['graph-theory', 'solution-verification', 'discrete-mathematics', 'planar-graphs']"
4767179,On the Definition of Smooth Map in Differential Topology,"Let $X$ be a nonempty subset of $\mathbb{R}^n$ and $f:X \rightarrow \mathbb{R}^m$ . In differential topology (see e.g. Milnor, Topology from the Differentiable Viewpoint or Guillemin and Pollack, Differential Topology), we call $f$ smooth if for any $x \in X$ there is an open neighborhood $U$ of $x$ in $\mathbb{R}^n$ and a smooth (in the usual sense, that is with continuous partial derivatives of all orders) map $F:U \rightarrow \mathbb{R}^m$ such that $F$ and $f$ coincide on $U \cap X$ . This definition has clearly a local nature and it is for sure the right definition to give in differential topology.
My curiosity is its relation with its global counterpart. Call $f:X \rightarrow \mathbb{R}^m$ globally smooth if there is an open subset of $\mathbb{R}^n$ containing $X$ and a smooth function $F:U \rightarrow \mathbb{R}^m$ such that $F|_X = f$ . The two definitions are very different in nature, but are they actually different? In other terms, is there some example of a smooth function which is not globally smooth? Thank you very much for your attention. NOTE . The fact that the definition of smooth function has a local nature is clear. We can make it even more clear by stating the following simple Proposition . Let $X$ be a nonempty subset of $\mathbb{R}^n$ and $f:X \rightarrow \mathbb{R}^m$ . $f$ is smooth if and only if for any $x \in X$ there is a neighborhood $N$ of $x$ in $X$ such that $f|_N$ is smooth. Proof . If $f$ is smooth, then for any nonempty subset $S$ of $X$ , $f|_S$ is smooth. Conversely, assume that for any $x \in X$ there is a neighborhood $N$ of $x$ in $X$ such that $f|_N$ is smooth. Let $x \in X$ and let $N$ be such a neighborhood. Then there exists an open neighborhood $U$ of $x$ in $\mathbb{R}^n$ and a smooth function $F:U \rightarrow \mathbb{R}^m$ such that $F$ and $f$ coincide on $U \cap N$ . If $V$ is an open neighborhood of $x$ in $X$ such that $V \subset N$ , there is an open subset $W$ of $\mathbb{R}^n$ such that $V= W \cap X$ . But then $U \cap W$ is an open neighborhood of $x$ in $\mathbb{R}^n$ , $F|_{U \cap W}$ is a smooth map, and $F|_{U \cap W}$ and $f$ coincide on $ U \cap W \cap X = U \cap V \subset U \cap N$ .","['differential-topology', 'differential-geometry', 'analysis', 'real-analysis']"
4767186,Number of triangles a point lies in on a plane,"On a plane , there are $2n+1$ points where no three points are co-linear. Show that for any point $P$ which is one of the points, the number of triangles the interior of which $P$ lies in is always even. My attempt : an easy observation is that for any fours points, the number of triangles $P$ lies in is either 0 or 2 & Conclusion holds for $n=2$ . But I am not sure this is sufficient to give the argument for all $n$ in general.","['discrete-geometry', 'combinatorics', 'parity', 'discrete-mathematics']"
4767317,Number of bitstrings where any subpattern repeats at most $d$ times,"The following problem has come up in the context of unitary equivalence of sets of matrices. However, here I will omit the context and state it as a standalone combinatorial problem. Consider bitstrings, i.e., words made of 0's and 1's.
The rule is: No sub-pattern can repeat more than $d$ times and the boundary conditions are periodic.
Some examples of invalid bitstrings for $d=2$ : 000 (repetition), 0100 (repetition across boundary), 0101101101 (repetition of sub-pattern 101) Here are examples of valid string:
101,
010011011 Update : To clarify: a repetition only counts if the repetitions are adjacent to each other. For example, 010011011 is valid even though 01 appears multiple times, but never three times in a row. The questions are: Are there infinitely many valid strings or not? If not, can one give an upper bound to the number of valid strings? Our first impression is that one can get very long bit strings.
However, we have neither been able to show that one can generate infinitely many, nor that there are only finitely many.","['decidability', 'combinatorics', 'tiling', 'discrete-mathematics']"
4767336,Heptagon is divided into pentagons and hexagons. Prove that there are at least $27$ pentagons in this division.,"A heptagon is divided into convex pentagons and convex hexagons in such a way that each vertex of the heptagon is the vertex of at least three polygons of the division. Prove that there are at least $27$ pentagons in this division. My solution: We know that both pentagons and hexagons are convex, so we can use Eulers equation: $v + f = e + 2$ $2e \geq 3v$ , because we have $2$ vertices for every edge and we have at least 3 edges meeting in every vertice - we know that it's true for the vertices of heptagon, but it must be also true for the inner vertices since otherwise those would be just straight lines $f = f_5 + f_6 + 1$ , where: $f_5$ is the number of pentagons and $f_6$ is the number of hexagons, as it was said in the comments, we need to add one more face which is the heptagon. $5f_5 + 6f_6 = 2e$ , as @Lexponent shown in his answer. So we get: $$v + f = e + 2 \iff 2v + 2f = 2e + 4$$ $$2v + 2f_5 + 2f_6 + 2 = 5f_5 + 6f_6 + 4$$ $$2v = 3f_5 + 4f_6 + 2 \iff 3v = \frac{9}{2}f_5 + 6f_6 + 3$$ $$2e \geq \frac{9}{2}f_5 + 6f_6 + 3$$ $$5f_5 + 6f_6 \geq \frac{9}{2}f_5 + 6f_6 + 3 \iff 10f_5 + 12f_6 \geq 9f_5 + 12f_6 + 6 \iff f_5 \geq 6$$ And I got an inequality on $f_5$ , but it's not good enought.","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
4767344,Choice of deformed contour in steepest descent method,"I'm trying to learn about the method of steepest descent (from https://people.math.osu.edu/tanveer.1/m805/supplement.steepest.pdf ). An example there is $$\int_{-\infty}^{+\infty}e^{i\lambda(t+t^3/3)}dt$$ where $\lambda\rightarrow \infty$ . We have two saddle points, $t_0=\pm i$ at which the first derivative of $p=-i(t+t^3/3)$ is zero. Now, I'm trying to deform the contour to pass from either or both these points. Due to arguments that can be found in the link, the two deformations are (see p.7): $C_1$ : connects $\infty e^{5i\pi/6}$ to $\infty e^{i\pi/6}$ , passing through $+i$ . At the latter, it's parallel with the real axis. $C_2$ : connects $\infty e^{-5i\pi/6}$ to $\infty e^{-i\pi/6}$ , passing through $-i$ . At the latter, it's parallel with the real axis. The deformed contours cannot cross the real axis, so we can't deform either in order to get to both points with one contour. What I don't understand is how we choose one over the other. The author chooses $C_1$ and simply states: ""It is to be noted that even though $Re (p)$ is smaller at the other saddle $t = − i$ , there is no contribution from this since the steepest descent path equivalent to the original contour integral does not pass through $t = − i$ "". So, not only I do not understand why $C_2$ is equivalent to the original contour while $C_1$ is not, I don't get why the author even studied the $C_2$ case. Any help will be appreciated. EDIT : Since physicists tend to use a lot of saddle point approximations, maybe the question should transition to physics stackexchange?","['integration', 'complex-analysis', 'approximation', 'asymptotics']"
4767351,"Given the definition of $a_0$ and $a_n$, what is $\lim_{x\to\infty}\frac{a_n}n$?","So I was looking through Youtube to see if there were, yet again, any math equations that I thought that I might like to solve when I came across this video by Michael Penn that was apparently ""Erdős but simpler"" [ $1$ ] with this as the question: Given $$\begin{cases}a_0=&1\\a_n=&a_\left\lfloor\frac n2\right\rfloor+a_\left\lfloor
\frac n3\right\rfloor\end{cases}$$ $$\lim_{n\to\infty}\dfrac{a_n}n=\,?$$ which I thought that I might be able to solve. Here is my attempt at doing so: $$\begin{align}a_1&=a_0+a_0=1+1=2\\a_2&=a_1+a_0=2+1=3\\a_3&=a_1+a_1=2+2=4\\a_4&=a_2+a_1=3+2=5\\a_5&=a_2+a_1=3+2=5\\a_6&=a_3+a_2=4+3=7\\a_7&=a_3+a_2=4+3=7\\a_8&=a_4+a_2=5+3=8\\a_9&=a_4+a_3=5+4=9\\a_{10}&=a_5+a_3=5+4=9\\a_{11}&=a_5+a_3=5+4=9\\a_{12}&=a_6+a_4=7+5=12\end{align}$$ Now I'll let $g_n=\dfrac{a_n}n$ and see what happens when we calculate $g_n$ for each of these terms that I have already calculated: $$\begin{align}g_1&=2\\g_2&=1.5\\g_3&=\dfrac43\\g_4&=1.25\\g_5&=1\\g_6&=\dfrac76\\g_7&=1\\g_8&=1\end{align}$$ Now here's where I'm stuck. I tried plugging the points that  I got into Desmos [ $2$ ] to get an equation of some sort so I could approximate an equation that I then could use to get an answer that I could use to determine the limit of $\dfrac{a_n}n$ , but it doesn't work. Here is a table of the strategies that I used and why they don't work (explanations that are too long can be found the ""Notes"" section of this question): What I have tried What I get Why it doesn't work Logarithmic regression: $y_1\sim a+b\ln(cx_1+d)$ $R^2=0.4439$ with $a=9.18093$ , $b=-2.05547$ , $c=1.3042$ , $d=42.1349$ [ $3$ ] Sinusoidal regression (with $\sin$ ): $y_1\sim a+b\sin(cx_1+d)$ $R^2=0.5272$ with $a=1.16481$ , $b=0.287905$ , $c=0.389035$ , $d=1.0123$ With $\cos$ , as we add more terms, $R^2\to0$ Sinusoidal regression (with $\cos$ ): $y_1\sim a+b\cos(cx_1+d)$ $R^2=0.5272$ with $a=1.16481$ , $b=0.287905$ , $c=0.389035$ , $d=-0.5588498$ See above Exponential regression: $y_1\sim ab^{x_1}+c$ $R^2=0.4447$ with $a=1.787828$ , $b=0.963084$ , $c=-0.288955$ Again, as more terms are added, $R^2\to0$ Now, if that's the case, what can we do? I actually tried just plotting terms of $a_n$ , but that didn't really seem to work. So, at this point, I decided to just give up. My question Is there any way that I might be able to solve this problem in any way? [ $4$ ] Notes [ $1$ ] The original question (since Michael Penn had modified it) was: Given the exact same system of equations defining the recursive formula (as noted in the beginning), show that $\lim_{n\to\infty}\frac{a_n}n=\frac{12}{\ln(432)}$ ."" I had to go into the video to find this, but I wouldn't say it's cheating, since all I'm doing here is getting the original question for reference. [ $2$ ] [ $3$ ] The reason logarithmic regression seems to not be accurate here because the terms are constantly expanding, while this just eventually drops to $0$ [ $4$ ] As stated in Note $1$ , I'm supposed to get $\frac{12}{\ln(432)}$ , however I guess my question is more ""How would I achieve said answer if I didn't know that the solution was said answer ?"" now that I think about it.","['limits', 'recursion']"
4767355,Asymptotic property for coupon collector's problem,"In the classic coupon collector's problem, we let $T_N$ be the time it takes for the collector to collect all of the coupons in the collection. It is easy to see that, in the case where each of the coupons is obtained with probability $\frac1N$ at each step, $T_N$ is a sum of independent geometric random variables which yields $\mathbb E[T_N]=NH_N$ and in turn $\lim_{N\rightarrow\infty}\frac{\mathbb E[T_N]}{N\log N}=1$ . Knowing this, how can one show that : $$\lim_{N\rightarrow\infty}\mathbb P(T_N-N\log N\le Nx)=e^{-e^{-x}},\quad x\in\mathbb R$$ ? I tried multiple different approaches, but nothing has worked thus far. Markov's inequality is a bit useless (as an attempt to exhibit an expectation), and attempting to write $T_N=\sum_{k=0}^{N-1}(T_{k+1}-T_k)$ to get a sum of independent random variables whose laws we know doesn't seem to yield any interesting result either. I feel like it should be possible to express the probability in the left-hand side as $\sum_{k=0}^{N-1}\frac{(-1)^ke^{-kx}}{k!}$ which would give the result by letting $N\rightarrow\infty$ but I don't know how to prove this (nor if it's true for that matter). Any help would be greatly appreciated","['probability-distributions', 'combinatorics', 'probability-theory', 'probability']"
4767370,Orthonormal basis of $L^2(X \times Y)$,"Let $(X, \mathscr{S}, \mu)$ and $(Y, \mathscr{T}, \lambda)$ be two $\sigma$ -finite measure spaces. Let $\{ \phi_i\}_{\in I}$ , $\{\psi_j\}_{j \in J}$ be two orthonormal bases for $L^2(X,\mu)$ and $L^2(Y, \lambda)$ respectively. For $i \in I$ , $j \in J$ , let $$\theta_{ij}(x,y):=\phi_i(x)\psi_j(y), \,\,\,\, (x\in X, y \in Y).$$ Is it true that $\{\theta_{ij}\}_{i \in I, j \in J}$ is an orthonormal basis for $L^2(X \times Y, \mu \times \lambda)$ ? Answer is yes whenever either $L^2(X)$ or $L^2(Y)$ is separable. See for instance the following. Orthonormal basis for product $L^2$ space The question is precisely for the case when both $L^2(X)$ and $L^2(Y)$ are not separable i.e. both $I$ and $J$ are uncountable.","['measure-theory', 'functional-analysis', 'real-analysis']"
4767374,Evaluating $\cot\fracπ{24}$,"Let $\theta=\fracπ{24}$ $2\theta=\fracπ{12}$ $\tan2\theta=\tan\fracπ{12}$ $\frac{2\tan\theta}{1-\tan^2\theta}=2-√3$ $2\tan\theta=(2-√3)-(2-√3)\tan^2\theta$ $(2-√3)\tan^2\theta+2\tan\theta-(2-√3)=0$ $\tan\theta=\frac{-2±\sqrt{4+4(4-3)}}{2(2-√3)}$ $\tan\theta=\frac{-1±√2}{2-√3}$ Therefore, $\cot\theta=\frac{2-√3}{√2-1}$ Rationalising, $\cot\fracπ{24}=(2-√3)(√2+1)=√8-√6+√4-√3$ But as per this link , $\cot\fracπ{24}=√2+√3+√4+√6$ What's my mistake here?","['trigonometry', 'solution-verification']"
4767390,Let $T\in M_m(\mathbb{C})$ with $r(T)=1$. Is it true that $\lim \frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}=1$?,"Here the norm $\lVert\cdot\rVert$ denotes the operator norm. If $T$ is a Jordan matrix with spectral radius $r(T)=1$ , then I am able to prove that $\frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}\to1$ . For general $T$ , $T=PJP^{-1}$ where $J$ is the Jordan cannonical form for $T$ . Then $\lVert T^n\rVert\le \lVert P\rVert\lVert P^{-1}\rVert\lVert J^n\rVert$ and also $\lVert J^n\rVert\le \lVert P\rVert\lVert P^{-1}\rVert\lVert T^n\rVert$ . Then $$ \frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}\le (\lVert P\rVert\lVert P^{-1}\rVert)^2  \frac{\lVert J^{n+1}\rVert}{\lVert J^n\rVert}\implies\limsup \frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}\le (\lVert P\rVert\lVert P^{-1}\rVert)^2$$ Also, $$\left|\frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}-1\right|\le\frac{|\lVert T^{n+1}\rVert-\lVert T^n\rVert|}{\lVert T^n\rVert}\le \frac{\lVert T^{n+1}-T^n\rVert}{\lVert T^n\rVert}\le \lVert T-I\rVert$$ Can we say anything about the convergence of the sequence $\frac{\lVert T^{n+1}\rVert}{\lVert T^n\rVert}$ ? It may not be $1$ , but does it converge at least? Can anyone help me with any idea? Thanks for your help in advance.","['analysis', 'matrices', 'linear-algebra', 'functional-analysis', 'sequences-and-series']"
4767394,When does an algebraic group have $PSL_2$ as a quotient?,"The context for the question is due to a comment in Milne’s Introduction to Shimura Varieties. I have limited background in algebraic groups (i.e. just enough to know what it is when it is used). Besides the point, how does one determine if an algebraic group have $PSL_2$ as a quotient?  Say, $U(2,1)$ for example. Certainly dimension is a nice enough way to check so anything dimension $<3$ cannot have it as a quotient. Is there anything else one can do? Allegedly, this is doable via Dynkin diagrams. See ""Edit II"" and an answer describing this story is perfectly acceptable as well. Edit: I realize it isn’t $PSL_2$ that I care about, but $PGL_2$ . In either case, there’s many ways to generalize the above question so let me keep it to $PSL_2$ . For the $PGL_2$ case, one can use the fact $PGL_2$ is noncompact so anything that is compact, such as $U(2,1)$ , cannot have it as a quotient. Edit II: According to @jackson, there should be a component of the Dynkin diagram that is of type $A_1$ and this should follow from the classification of split connected reductive groups.","['algebraic-groups', 'algebraic-geometry', 'dynkin-diagrams', 'lie-groups']"
4767399,Regular Graph Question: Each face bounded by n edges,"I have been struggling with this question for a while now, and I'm not sure how to proceed: A simple-connected, planar graph has $v$ vertices, $e$ edges and $f$ faces. Each face is bounded by $n$ edges, and each vertex has degree $d$ . Use $f+v-e=2$ to show that $f=\frac{4d}{2d+2n-dn}$ .","['graph-theory', 'discrete-mathematics']"
4767433,Proving that $\lim_{ n \to \infty} \left\{ \frac{1}{2^{m n}} \sum_{r=1}^{2^n-1}(-1)^r r^m\right\}=-\frac{1}{2}$ independently of the value of $m$.,"It seems that, independently of the value of $m$ , we have $$
\lim_{ n \to \infty}  \left\{ \frac{1}{2^{m n}} \sum_{r=1}^{2^n-1}(-1)^r r^m\right\}=-\frac{1}{2}
$$ I've tested it numerically but I have no idea how to prove it. Can anyone do it? Or give some hints? Thanks.","['limits', 'summation', 'bernoulli-polynomials', 'sequences-and-series']"
4767454,"$P(X > Y) = ?$ when $X,Y \stackrel{iid}{\sim} \text{Bin} \left(n, \frac{1}{2} \right)$","If $X,Y \stackrel{iid}{\sim} \text{Bin} \left(n, \frac{1}{2} \right)$ , how do I calculate $P(X^2 > Y^2)$ or $P(X^2 = Y^2)$ ? Attempt: $\begin{aligned}
P(X^2 > Y^2) &= P((X-Y)(X+Y) > 0) = P((X-Y) > 0) \cdot P((X+Y) > 0) + P((X-Y) < 0) \cdot P((X-Y) < 0) \\
&= [ \cdots ]
\end{aligned}$ How do I calculate $P(X > Y)$ ? I think if I know this, I'll be able to do the rest of it. [This is not a homework problem. I was doing a longer question and this part is bothering me for sometime.]","['statistics', 'probability-distributions', 'binomial-distribution', 'probability']"
4767467,Standard notation for an unevaluated expression,"It's standard to denote an unevaluated function $f:X\rightarrow Y$ simply by "" $f$ "" (that is, the function itself, rather than its value at a given point). But say we have an expression such as $z e^{az}$ . Is there a standard way to denote the implied function (i.e. without variables), rather than simply the function evaluated at $z$ ? I suppose one option would be $(z\mapsto z e^{az})$ , but this is mildly unwieldy. For example if I wanted to express the Laplace transform of the function implied by $ze^{az}$ , it wouldn't be quite right to write $\mathcal{L}(ze^{az})$ because represents a number "" $ze^{az}$ "" rather than the function itself. Thanks in advance!","['notation', 'functions']"
4767490,"Evaluate $\,\lim\limits_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}$","How can I evaluate this limit. $$\lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}$$ This is what I did: $x\in \big(\frac{-1}{2},0\big)\;$ ; $\;\;\lfloor x\rfloor =-1 $ $$\lim_{x \to 0^{-}}\sin(\lfloor x\rfloor)=\sin(-1)<0 \;\;\text{ and }\;\lim_{x \to 0^{-}} x =0^{-}$$ then $$\lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}=+\infty$$ What you think? Any other method ?","['alternative-proof', 'limits', 'trigonometry']"
4767506,fully customizable periodic function?,"I am looking for a bell-shaped periodic function f(x) with parameters a and b,  with following characteristics:
( not sure if such function already exists or one can formulate one ) : oscillating between zero and a constant non-negative number A. Width of bell can be modified through parameter b. customizable period of  c. Preferably easy to calculate its integral Obviously such function should look like a spike with lower b numbers and conversely turn into square-like with larger b.
I tried playing with Gaussian and normal distribution, it satisfies the first two requirements but fails to drop to zero at periodicals of x = c. something like the picture below any suggestions highly appreciated !","['periodic-functions', 'functions', 'exponential-function', 'normal-distribution']"
4767513,Limit of continuous functions is Riemann integrable,"Here is an analysis problem I'm stuck on: Let $f\in C^0([0,1])$ with $f(0)=0$ and $f$ increasing and convex. Define: $$ f_n(x) = n\big[f(x)-f(x-\tfrac{1}{n})\big] $$ Show: $f(1-\tfrac{1}{n})\le\int_0^1f_n(x)\ dx\le f(1)$ There is a $g$ with $f_n(x)\rightarrow g(x)$ almost everywhere $\int_0^1 g(x)\ dx = f(1)$ I've been able to do part 1, and I believe I can do part 2. The idea is that convexity implies $f_n(x)\ge f_m(x)$ when $n\ge m$ [I call this pointwise monotonicity below], and I use that to show $\{f_n\}$ is a Cauchy sequence in $L^1([0,1])$ . There is thus a $g\in L^1([0,1])$ with $f_n\rightarrow g$ almost everywhere, and the pointwise monotonicity implies this is pointwise convergence too. Part 3 is straightforward, except for one wrinkle: I can't seem to show $g$ is Riemann integrable. It seems like the following should be generally true, but I cannot prove it: If $\{f_n\}$ is a pointwise monotonic sequence of continuous functions, converging to $g\in L^1([0,1])$ , then $g$ is continuous almost everywhere. Is this true? Or do I need to use more about this particular sequence of my original problem?","['pointwise-convergence', 'riemann-integration', 'functional-analysis', 'real-analysis']"
4767525,How can we recognize if $\sin^{-1}(x)$ is supposed to be $\arcsin(x)$ or if it is supposed to signify $\frac{1}{\sin(x)}$?,"I had a student ask me a question today involving the integral of $\sin^{-1}(x)$ . He mistakenly assumed that he could rewrite it as the integral of $\csc(x)$ , when instead the question was referring to the inverse sin function. I did not have a good explanation as to why the notation is so similar other than to say that $\csc(x)$ or $\frac{1}{\sin(x)}$ would be specified. How can we recognize if $\sin^{-1}(x)$ is supposed to be $\arcsin(x)$ or if it is supposed to signify $\frac{1}{\sin(x)}$ ? I also wondered if issues like this explains why some curricula use $\arcsin(x)$ instead of the power notation, as well as a part of the reason $\sec(x)$ , $\csc(x)$ , and $\cot(x)$ exist notationally.","['calculus', 'trigonometry']"
4767542,Proving $\sum_{k=1}^{n-1} \cos\left(\frac{2\pi k}{n}\right)=-1$,"I am currently trying to prove $$\sum_{k=1}^{n-1} \cos\left(\frac{2\pi k}{n}\right)=-1$$ My current attempt goes as follows $$\sum_{k=1}^{n-1} \cos\left(\frac{2\pi k}{n}\right)=\Re\left(\sum_{k=1}^{n-1} e^\frac{2\pi k i}{n}\right)$$ Here is where some of my confusion is, I'm not sure which geometric series to use, I have tried $$\sum_{k=1}^{n-1} z^{k} = \frac{z-z^n}{1-z}$$ but I have no clue if this is correct or where the $-1$ would come from.
Can someone please show me the correct direction?","['complex-analysis', 'sequences-and-series']"
4767543,Girsanov from stochastic starting points.,"Consider a canonical Wiener space $(\Omega,\mathscr{F},\mathbb{P})$ . Consider two SDEs: $$dX_t = \alpha(X_t,t)dt+dW_t, \ \ \text{Law}(X_0)=\mu$$ and $$dY_t = \beta(Y_t,t)dt+dW_t, \ \ \text{Law}(Y_0)=\nu$$ Suppose both SDES have solutions (say unique strong solutions). Is there a way to use Girsanov to compare the laws of these two SDEs? I am interested in both the path measures and the time marginals. This cannot be done in general, since if $\mu \perp \nu$ , (say, $X_0=x,Y_0=y$ with $y\neq x$ ) the two path measures must be singular. If $\mu=\nu$ , the usual Girsanov change of measure does the job: if $\mathscr{E}(M)$ is the exponential martingale operator, we can use Girsanov to define a probability measure $\mathbb{Q}$ with density wrt to $\mathbb{P}$ such that $$\frac{d\mathbb{Q}}{d\mathbb{P}}=\mathscr{E}\left( \int_0^T -\alpha_y(Y_t)+\beta_t(Y_t)dt\right)=\frac{\text{Law}(Y_{[0,T]})}{\text{Law}(X_{[0,T]})}$$ I would like to know if something similar can be done if $\mu \neq \nu$ , but under the assumption that $\nu \sim \mu$ .","['stochastic-analysis', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4767550,A Number-theoretic Generalization of the Union-closed Sets Conjecture,"Denote $\mathbb{N}^*=\{1,2,3,...\}, \dot k = \{k,2k,3k,...\}, \mathbb{P}=\{2,3,5,7,11,..\}$ and write $M\leq \mathbb{N}^*$ to denote that $M$ is lcm-closed, i.e. $a,b\in M\Rightarrow \text{lcm}(a,b)\in M$ . Then, an equivalent number-theoretic phrasing of Frankl's conjecture is the following $$\bbox[#FFB797,2pt,border: 2px solid red]{\array{\text{If $\{1\}\neq M\leq\mathbb{N}^*$ is finite and contains only square-free numbers, then}\\ \exists p\in \mathbb{P}:|M\cap \dot p|\geq \frac{1}{2}|M|}}$$ The equivalence can be proven constructing from $M$ the union-closed family $\mathcal{F}_M:=\{\{p\in\mathbb{P}:p\ \vert\ m\}:m\in M\}$ of the universe set $\mathbb{P}$ and constructing from a finite union-closed set $\mathcal{F}$ of the universe set $\mathbb{P}$ the set of square-free numbers $M_\mathcal{F}:=\{\prod_{p\in X}p:X\in \mathcal{F}\}$ . With this new formulation, I thought of generalizing it so that we no longer need the square-free condition. These are the two possible generalizations that I've come up with. If $\emptyset\neq M\leq\mathbb{N}^*$ is finite , then $$\array{\textbf{Conjecture 1. }\ \exists p\in \mathbb{P}:|M\cap \dot p|\geq \cfrac{\nu_p(M)}{1+\nu_p(M)}|M|}\\ \textbf{Conjecture 2. }\ \exists p\in \mathbb{P}:\sum_{m\in M}\nu_p(m)\geq \frac{1}{2}\nu_p(M)|M|$$ where $\nu_p(m)$ is the $p$ -adic valuation of $m$ and I denote $\nu_p(M)=\nu_p(\max M)=\max\limits_{m\in M}\nu_p(m)$ . Note as well that, now, we don't need to exclude the case $M=\{1\}$ anymore as $\nu_p(\{1\})=0$ . I expect that at least one of them has a counterexample. However, I am quite sloppy at finding them. So then my quesion is Q: Do the previous generalizations have clear counterexamples? I will now explain the motivation to why I chose those precise coefficients on the right hand side of both conjectural inequalities to show that they aren't arbitrary. Note that on the original Frankl conjecture, we find that for the finite power set $2^U$ the inequality $$|\{X\in 2^U:\alpha\in X\}|\geq 2^{|U|-1}=\frac{1}{2}|2^U|$$ is, in fact, an equality for every $\alpha\in U$ . So, in a sense, one expects that the maximal case is as bad as it gets. Now, it isn't hard to see that the equivalent lcm-closed set to the union-closed set $2^{\{p_1,...,p_k\}}$ where $\{p_1,...,p_k\}$ are distinct primes is the set $\text{Div}(P)$ of divisors of $P=\prod_{j=1}^k p_j$ where $P$ is a square-free number. So, if we consider any $M=\text{Div}(m_0)$ for some $m_0\in\mathbb{N}^*$ (not necessarily square-free), one easily sees that $$\Rightarrow |M\cap\dot p|=d(m_0)- d(m_0p^{-\nu_p(m_0)})=(1+\nu_p(m_0))d(m_0p^{-\nu_p(m_0)})-d(m_0p^{-\nu_p(m_0)})$$ $$=\nu_p(m_0) d(m_0p^{-\nu_p(m_0)})=\frac{\nu_p(m_0)}{1+\nu_p(m_0)}d(m_0)= \cfrac{\nu_p(M)}{1+\nu_p(M)}|M|$$ $$\Rightarrow \sum_{m\in M}\nu_p(m)=\sum_{d\vert m_0}\nu_p(d)=\sum_{j=0}^{\nu_p(m_0)}\sum_{d\vert m_0\\ \nu_p(d)=j}\nu_p(d)=\sum_{j=0}^{\nu_p(m_0)}\sum_{d\vert m_0\\ \nu_p(d)=j}j=\left(\sum_{j=0}^{\nu_p(m_0)}j\right)d(m_0p^{-\nu_p(m_0)})$$ $$\frac{1}{2}\nu_p(m_0)(1+\nu_p(m_0))d(m_0p^{-\nu_p(m_0)})=\frac{1}{2}\nu_p(m_0)d(m_0)=\frac{1}{2}\nu_p(M)|M|$$ So, if these are as bad as it gets, the previous conjectures follow. This also means that both inequalities (if true) are also sharp. Note as well that both conjectures can be rephrased as generalization of Frankl's conjecture with multisets (or, equivalently, tuples of natural numbers) avoiding number-theoretic terminology. Specifically, if $\emptyset\neq E\subseteq \mathbb{N}^n$ is a finite max-closed set, i.e. $(x_j),(y_j)\in E\Rightarrow (\max\{x_j,y_j\})\in E$ , then one can rephrase the previous conjectures as $$\array{\textbf{Conjecture 1. }\ \exists j\in n:|\{x\in E: x_j = 0\}|\leq \frac{1}{1+\max\limits_{x\in E} x_j}|E|}\\ \textbf{Conjecture 2. }\ \exists j\in n:\sum_{x\in E}x_j\geq \frac{1}{2}\left(\max\limits_{x\in E} x_j\right)|E|$$","['conjectures', 'number-theory', 'elementary-number-theory', 'combinatorics', 'open-problem']"
4767559,"Probability distributions for sorted observations of a random variable, such as the nth-highest roll from k dice.","This seems to be a bit of an unorthodox question since I haven't been able to find any questions quite like it on this site. The closest I can find are those pertaining specifically to the scenarios of rolling dice and ignoring some number of either only the lowest or only the highest. So, I'll first describe a more general case of the problem for anyone else who might need to know how the distribution of ordered observations of a random variable works. Note on notation: As per Wikipedia , I'm using $\left[ a .. b \right]$ to mean the set of integers between $a$ and $b$ —that is, $\left[ a .. b \right] = \left[ a,b \right] \cap \mathbb{N}$ General Case Take $k$ observations, $a_1$ through $a_k$ , of some random variable $A$ with distribution $\mathcal{A}$ . Put them in a list $b$ , ordered from smallest to largest, so that $b_n$ is the $n$ th-smallest value of $\{ a_i \mid i \in [1 .. k]\}$ . What is the probability distribution of $b_n$ ? I will refer to this distribution as $\Omega(\mathcal{A}, k, n)$ . Example Let's say $k$ = 5 and $A$ is a standard uniform random variable . That is, $A \sim \mathcal{U}(0,1)$ . We obtain five observations of $\mathcal{U}(0,1)$ : $a = ( .376, .531, .826, .896, .166 )$ and then we sort them: $b = ( .166, .376, .531, .826, .896 )$ Each $b_n$ is an observation of $\Omega(\mathcal{U}(0,1), 5, n)$ . For example, 0.531 is an observation of $\Omega(\mathcal{U}(0,1), 5, 3)$ . Specific Case The particular reason I came across this problem is for tabletop role-playing games, which typically use dice for determining random outcomes. A common method for modifying the shape of probability distributions without changing the minimum and maximum possible values is to have players perform one or more extra rolls and then discard the highest or lowest results before summing the remaining values. For example, roll a 20-sided dice twice and discard the highest, or roll a 6-sided dice four times and discard the lowest. When designing new game mechanics that use this type of random selection, it would be good to know what the actual probability distributions are. To determine the probability distribution of a sum of certain rolls in a sorted set, you need to know the probability distribution of each die after sorting. So, my specific case is a subset of the general case for a discrete uniform distribution with domain $[1..s]$ , where s is the number of sides on the die. I will call this distribution $D(s)$ . It has the following probability mass function and cumulative distribution function: $p(x; s) = \left\{\begin{array}{ll}\frac{1}{s} & \quad x \in [1 .. s] \\ 0 & \quad \text{otherwise} \end{array}\right.$ $F(x; s) = \left\{\begin{array}{ll} 0 & \quad x \lt 1 \\ \frac{\lfloor x \rfloor}{s} & \quad 1 \leq x \leq s \\ 1 & \quad x \gt s \end{array}\right.$ What I'm looking for is a way to get the probability distribution for the $n$ th-smallest value from rolling an $s$ -sided die $k$ times. Formally, I want to find the probability mass function $p(x; s, k, n)$ and/or the cumulative distribution function $F(x; s, k, n)$ of $\Omega(D(s), k, n)$ . Example Let's say $k$ = 4 and $A$ is a random variable that represents the outcome of rolling a 20-sided die. That is, $A \sim D(20)$ . We obtain four observations of $D(20)$ : $a = ( 19, 10, 10, 13)$ and then we sort them: $b = ( 10, 10, 13, 19 )$ Each $b_n$ is an observation of $\Omega(D(20), 4, n)$ . For example, the second 10 is an observation of $\Omega(D(20), 4, 2)$ . What I am personally looking for is just a solution to the dice problem. However, out of curiosity as to whether it is even possible, I do wonder if it is possible to come up with a more general composite function that handles the general case for any random variable. I'm sure there are use cases for needing to know the distribution of things like the middle half of observations from a normal distribution, or the tenth-highest outcome out of a hundred from a geometric distribution.","['statistics', 'probability-distributions', 'dice', 'random-variables']"
4767659,"Prove that, for every natural number $n$, $x^n +n ≥ nx + 1$ for all $x>0$","Prove that, for every natural number $n$ , $x^n +n ≥ nx + 1$ for all $x>0$ This is the first part of the question, and I am not too sure how to prove it. My steps to prove it were to find a maximum-minimum case for $x$ and $n$ (ie. when $x → ∞$ and $n → ∞$ , but I was not able to find a solution from there. Any help for this is appreciated. P.S. I also need to prove that this inequality holds for $n= 3/2$ , but I think that this can be done if I gain an understanding first of the above proof, so no spoilers please for proving the $n=3/2$ part $:)$","['algebra-precalculus', 'inequality']"
4767716,Inversion formula for characteristic functions whose argument is complex with non-zero real part,"In this paper they use an inversion formula for characteristic functions whose argument is complex with non-zero real part. Namely, given a random variable $X$ taking values in $(0,\infty)$ , they consider the characteristic function in the following way \begin{align*}
f(\phi)=\mathbb{E}\left[e^{\phi X}\right]=\mathbb{E}\left[e^{\phi_R X+ i\phi_I X}\right],
\end{align*} where $\phi=\phi_R+i\phi_I\in\mathbb{C}$ and $\phi_R,\phi_I\in\mathbb{R}$ . Note that $f$ may not be well-defined, as opposed to the standard characteristic function that is always well-defined. Then, in Equation (A8) of the paper they obtain the density function $p$ of the random variable $X$ using the following inversion theorem \begin{align*}
p(x)=\frac{1}{\pi} \int_0^\infty\Re\left[e^{-\phi x}f(\phi)\right]d\phi_I.
\end{align*} I would like to know if there is some reference for this result or if it can be deduced from the existing inversion theorems for standard characteristic functions. For instance, the previous inversion formula looks like a generalization of the Gil-Pelaez inversion formula, that is, if we consider the characteristic function $\varphi(t)=\mathbb{E}\left[e^{itX}\right]$ where now $t\in\mathbb{R}$ , then, \begin{align*}
p(x)=\frac{1}{\pi} \int_0^\infty\Re\left[e^{-it x}\varphi(t)\right]dt.
\end{align*} So far, I have been able to prove that \begin{align*}
\frac{1}{\pi} \int_0^\infty\Re\left[e^{-\phi x}f(\phi)\right]d\phi_I=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-\phi_Rx-i\phi_Ix}\mathbb{E}\left[e^{\phi_RX+i\phi_IX}\right]d\phi_I
\end{align*} Note that if $\phi_R=0$ in the previous expression we recover the usual inversion formula given by \begin{align*}
p(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-i\phi_Ix}\mathbb{E}\left[e^{i\phi_IX}\right]d\phi_I
\end{align*} Any help on how to prove this inversion theorem or any reference of the inversion theorem used in the paper would be really appreciated.","['fourier-transform', 'characteristic-functions', 'measure-theory', 'probability-theory']"
4767724,Poincaré–Miranda \ Intermediate Value theorem for mappings from $\mathbb{R}^n$ to $\mathbb{R}^m$,"currently, I am looking for the existence of zeros of a vector valued function $\vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m$ . For my specific problem, $n=2$ and $m=6$ (The dimensions could change as I make changes to function $f$ ). I know that I can expect a continuity of the function, but not continuous derivability due to the underlying physics. I saw that there is a theorem called Poincaré–Miranda or Intermediate Value Theorem, which basically tells me that If I have $\vec{f}(\vec{x}_1) \leq \vec{0} \leq \vec{f}(\vec{x}_2)$ that there is a $\vec{f}(\vec{x_3}) = \vec{0}$ . From the related Wikpedia entry it seems that this only holds for projections to the same dimensional space $\mathbb{R}^n \rightarrow \mathbb{R}^n$ . Question 1 Is there an extension of Poincaré–Miranda Theorem to functions $\vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m$ ? Question 2 What if I artificially make my projection fulfill the requirements for the Poincaré–Miranda Theorem? E.g.:
I have a function $\vec{f}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x}))$ . I know that both $f_1$ and $f_2$ are smooth. Then $f_3(\vec{x}) = f_1(\vec{x})+f_2(\vec{x})$ is also smooth. Also for $f_1(\vec{x_a}) \geq 0$ and $f_2(\vec{x_a}) \geq 0$ it follows that $f_3(\vec{x_a}) \geq 0$ and vice versa for $f_1(\vec{x_b}) \leq 0$ and $f_2(\vec{x_b}) \leq 0$ . Hence, the theorem should be applicable to the new function $\vec{f_{new}}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x}), f_3(\vec{x}))$ . Am I missing something obvious here? Thanks :)","['continuity', 'multivariable-calculus', 'roots']"
4767749,Marginal Posterior Distribution for Multinomial/Dirichlet Variables,"Suppose that some data $(y_{1},\ldots,y_{J})$ are distributed multinomially with parameters $(\theta_{1},\ldots,\theta_{J})$ and that $\theta = (\theta_{1},\ldots,\theta_{J})$ has Dirichlet prior distribution: \begin{equation*}
p(y_{1},\ldots,y_{J}|\theta_{1},\ldots,\theta_{J}) \propto \prod_{i=1}^{J}{\theta_{i}^{y_{i}}}
\end{equation*} and \begin{equation*}
p(\theta_{1},\ldots,\theta_{j}) \propto \prod_{i=1}^{J}{\theta_{i}^{\alpha_{i}-1}}
\end{equation*} such that the $\theta_{i}$ sum to unity. I want to determine the distribution of $\theta_{1},\theta_{2}|y_{1},\ldots,y_{J}$ . It is clear that the posterior distribution of $\theta$ given $y$ also has a Dirichlet distribution \begin{equation*}
p(\theta_{1},\ldots,\theta_{J}|y_{1},\ldots,y_{J}) \propto \prod_{i=1}^{J}{\theta_{i}^{\alpha_{i}+y_{i}-1}}.
\end{equation*} The answer, I think, is that \begin{equation*}
p(\theta_{1},\theta_{2}|y_{1},\ldots,y_{J}) \propto \theta_{1}^{y_{1}+\alpha_{1}-1}\theta_{2}^{y_{2}+\alpha_{2}-1}\left(1-\theta_{1}-\theta_{2}\right)^{-1+\sum_{i=3}^{J}{(y_{i}+\alpha_{i})}}.
\end{equation*} I'm not clear, though, on how to obtain this result. I know that under the assumptions of the problem, \begin{equation*}
p(\theta_{1},\theta_{2},1-\theta_{1}-\theta_{2})\propto \theta_{1}^{\alpha_{1}-1}\theta_{2}^{\alpha_{2}-1}(1-\theta_{1}-\theta_{2})^{1-\alpha_{1}-\alpha_{2}},
\end{equation*} but I haven't found how to incorporate this into my thinking. My instinct is to say that \begin{equation*}
p(\theta_{1},\theta_{2}|y_{1},\ldots,y_{J}) = \int{p(\theta_{1},\ldots,\theta_{J}|y_{1},\ldots,y_{J})\,\mathrm{d}\theta_{3}\cdots\mathrm{d}\theta_{J}},
\end{equation*} but I'm not sure what the limits of integration should be. The $\theta_{i}$ are non-negative by assumption and sum to $1$ , so the bounds need to be subsets of $[0,1]$ in every case, but it's not clear to me how to restrict them (inductively?).","['multinomial-distribution', 'statistics', 'bayesian', 'problem-solving']"
4767765,Has every infinite simple group a faithful irreducible representation?,"Has every infinite simple group a faithful irreducible representation? This question solves the finite case. However, the proof requires a non-trivial linear representation of a finite group. I want to know if the conclusion is true for an infinite simple group. If it is not, can you give me a counterexample?","['group-theory', 'simple-groups', 'representation-theory', 'infinite-groups']"
4767772,Domain over which $f(\alpha) = \int_0^\infty \frac{x^\alpha}{1 + x^2} \ \mathrm{d}x$ is convergent [duplicate],"This question already has answers here : Does the integral converge $\int_{-\infty}^{+\infty}\frac{\exp(ibx)}{1+e^x}\,dx$ for $b>0$? (4 answers) Closed 10 months ago . For $-1 < \alpha < 1$ real, the below integral is well-known and an elementary exercise in complex analysis: \begin{align*}
f(\alpha) \overset{\text{def}}{=} \displaystyle \int_0^\infty \frac{x^\alpha}{1 + x^2} \ \mathrm{d}x = \frac{\pi}{2} \sec \left(\frac{\pi}{2} \alpha\right )
\end{align*} However, $f$ can be extended as a complex function. In particular, over the region $D = \{\alpha \in \mathbb{C} : -1 < \operatorname{Re}(\alpha) < 1\}$ the above still holds, and I am reasonably sure $f$ blows up outside $\overline{D}$ . I was interested in the behaviour of $f$ over $\overline{D} \setminus D$ (i.e. when $\operatorname{Re}(\alpha) = \pm 1$ ). It is easily seen that if $\alpha = \pm 1$ then $f$ blows up, but e.g. Mathematica claims the above formula holds if $\alpha = 1 + i$ . However, Mathematica also won't give an output if I substitute $x = \tan\theta$ into the integral for $\alpha = 1 + i$ , and claims it is not convergent in a slightly older version, so I am hesitant in accepting its answer. I would like a solution that carefully analyses and determines the convergence of the integral over $\overline{D}\setminus D$ .","['integration', 'complex-analysis', 'contour-integration', 'definite-integrals']"
4767825,Zeta functions of hyperelliptic curves,"I have been wondering recently about the geometric information encoded in the zeta function of a (smooth, projective) variety over a finite field - or in its étale cohomology (i.e. l-adic cohomology) as a Galois module, I hope it's not a blunder to say they are equivalent data, if Tate's conjecture holds. For curves, one can read the genus off of it, but what about other invariants, such as the existence of certain maps to projective space? A precise question could be if we can tell whether a curve is hyperelliptic by only looking at its zeta function. I have a hunch this is not the case. If I'm not mistaken, the interesting part of the étale cohomology of a curve is essentially the Tate module of its Jacobian, and the hyperelliptic involution acts on the Jacobian as the inverse, it does not provide an interesting automorphism. Furthermore, if étale cohomology supposedly mimics the usual singular cohomology then I shouldn't expect it to single out hyperelliptic curves - this is a very weak argument, I admit, maybe hyperelliptic curves have peculiar Galois actions. I would expect a counterexample to exist, but a heuristic argument for why it should or shouldn't be true would be great as well.","['algebraic-curves', 'zeta-functions', 'finite-fields', 'algebraic-geometry']"
4767837,Obstruction of semidirect product from being a direct product?,"Let $G$ and $H$ be (nonabelian) groups, and $\varphi: G\to \text{Aut}(H)$ be a homomorphism. This defines the semidirect product $H\rtimes_{\varphi} G$ . I am wondering, what is the best way to characterise how 'far away' $H\rtimes_{\varphi} G$ is from being a direct product $H\times G$ ? My attempt: The product in $H\rtimes_{\varphi} G$ is $$(h_1,g_1)(h_2,g_2)=(h_1\varphi_{g_1}(h_2),g_1g_2)$$ for $h_i\in H$ , $g_i\in G$ . Then the semidirect product is the direct product if $\varphi_{g_1}(h_1)=h_1$ for all $g_1\in G$ and all $h_1\in H$ . This is true in turn if $\text{ker}(\varphi)=G$ . Since $\text{ker}(\varphi)\triangleleft G$ is a normal subgroup of $G$ , we could view the quotient $G/\text{ker}(\varphi)$ as an 'obstruction group' of $H\rtimes_{\varphi} G$ being isomorphic to $H\times G$ . Question: Is there a better way to characterise the deviation from $\rtimes_\varphi$ being $\times$ (such as group cohomology), or can one make a similar statement without knowing the specific form of $\varphi$ ?","['group-theory', 'semidirect-product']"
4767905,Is every sequence $\{x_n\}_{n=1}^\infty$ such that $\lim_{n\to\infty}x_n=0$ a member of $\ell^p$ for some $p>0$?,"For every sequence of real numbers $(x_n)_{n\in N}$ converging to zero, does there exist a positive number p such that the sum $\sum_{n=1}^\infty |x_n|^p$ converges. I'm not 100% sure if this is correct, but I think that the answer is yes, there exists a positive number such that the sum converges because the sequence is the absolute value and converges to 0, so it wouldn't matter how $p$ would change as long as it is positive the sum will converge. Is that correct? Also I am struggling to show this thought with equations and calculations, so  if anybody has any ideas for that I would be very grateful.","['convergence-divergence', 'functional-analysis', 'analysis', 'sequences-and-series']"
4767911,What is the expectation of switch activation?,"Consider $n$ switches, which activate at rate $a_i$ , $1\leq i\leq n$ . The time it takes for a specific switch to activate, $T_i$ , satisfies $T_i\sim \text{Exp}(a_i)$ . In particular, $E[T_i]=1/a_i$ . Now, imagine there are $m$ activating factors (available at any time for the $n$ switches) so that each switch only activates once it is bound to exactly $1$ activating factor (say, at a rate $b$ ). Once a switch is activated, the activating factor is recycled at rate $r$ . For example, if $b$ is high and $r$ is low, we expect early activation of $m$ switches (if $m<n$ ), followed by a period of no activation. What distribution does $T_i$ now follows? Is it possible to work out its expectation? Edit: Following a comment, I would just add that both binding and recycling times are exponentially distributed with the given rates, and the switch an activating factor binds to is chosen uniformly at random among those not yet activated.","['expected-value', 'statistics', 'exponential-distribution', 'random-variables']"
4767917,Representation of Real Number as infinte Product $\prod(1\pm2^{-k})$,"In some paper describing a variant of CORDIC, I found basically the claim that every real number $x$ in some interval $[u,v]$ with $u\approx0.2887$ and $v\approx2.384$ , can be represented as: $$
x = \prod_{k=1}^\infty (1\pm2^{-k}) \tag1
$$ As I found, the algorithm isn't working properly, and the cause could be that the paper is wrong, which I am trying to understand. (And maybe also to fix. For my purpose it's sufficient when $v/u\geqslant2$ .) To show that a representation like $(1)$ does not exist for a full interval containing $1$ , I startet like this: For some sequence $a=\{a_k\}_{k=1}^\infty, a_k\in\{-1,+1\}$ , let $$
f(a,n) := \prod_{k=n}^\infty (1+a_k 2^{-k}) \tag2
$$ We can introduce a complete order on all possible sequences by means of: Let $a > b$ if there is some index $m$ such that $a_m>b_m$ , and $a_k=b_k$ for all $k < m$ . Similar for $a < b$ . Let $a=b$ if $a_k=b_k$ for all $k$ . Then what I want to show is that $$
a > b \quad\implies\quad f(a,n) > f(b,n) \qquad\text{for all }n \leqslant m\tag3
$$ where $m$ is the smallest index such that $a_m\neq b_m$ . For example, take the sequence $p=\{+1,-1,-1,-1,-1,\dots\}$ , then $$
f(p,1) > f(-p,1)
$$ but there are no sequences between $-p$ and $p$ , so the image of $f(\cdot,1)$ lacks the interval $(f(-p,1),f(p,1)) \tag{3a}$ . To show $(3)$ , let $a>b$ be two sequences, and let $m$ be the smallest index such that $a_m\neq b_m$ .  As the sequences coincide for all indexes smaller than $m$ , it's enought to show that $$
f(a,m) = (1+2^{-m})\prod_{m+1}^\infty (1+a_k2^{-k})
 \ > \ (1-2^{-m})\prod_{m+1}^\infty (1+b_k2^{-k}) = f(b,m) \tag 4
$$ which follows from $$
(1+2^{-m})\prod_{m+1}^\infty (1-2^{-k})
\ > \ (1-2^{-m})\prod_{m+1}^\infty (1+2^{-k})  \tag 5
$$ due to $a_k\geqslant-1$ and $b_k\leqslant1$ . Rearranging $(5)$ : $$
\frac{1+2^{-m}}{1-2^{-m}}
 \ >\  \prod_{m+1}^\infty \frac{1+2^{-k}}{1-2^{-k}} \tag 6
$$ Numerical evidence proposes that $(6)$ actually holds, bit I am stuck here. Questions: How to show inequality $(3)\,$ ? Provided $(3)$ holds, how to show that there are always values $x$ near $1$ which cannot be represended by means of products of form $(1)\,$ ? Maybe it's even simpler, and $(3)$ is not needed at all. Interactive Desmos plot of $(x(a), f(a))$ Edit Below is a graphic that shows a plot of $f(a,1)$ for all $2^{15}$ sequences $a$ , where the expansion was stopped after $a_{15}$ . The $x$ -coordinates for the plot are $x(a)$ , where $$\begin{align}
x: \{\pm1\}^{\Bbb N} &\to [0,1] \\
a &\mapsto \sum_{k=1}^\infty (1+a_k) \cdot 2^{-k-1}
\end{align}$$ i.e. interpret $a$ as a real numer in binary representation with bits $$
0.d_1 d_2 d_3\cdots
$$ where the bits are $d_k = (1+a_k)/2\in\{0,1\}$ . Notice that $a>b ~\Rightarrow~ x(a)\geqslant x(b).$ Notice that $x(p)=x(-p)=0.5$ with $p$ from $(3\mathrm{a})$ , but the graph below has a jump at $0.5$ .  There are less pronounced jumps at 0.75 and 0.25 as well; presumable the graph is a self-similar and discontinuous. The jumps mean that the image of $f$ has holes. original graphic","['real-numbers', 'numerical-methods', 'real-analysis']"
4767919,On point line incidences,"Given a set of $n$ points $P_1, \ldots, P_n$ and $n$ lines $L_1, \ldots, L_n$ in $\mathbb{F}^2$ , consider the set $I=\{(p_i, L_j)|L_j\text{ contains }p_i\}$ of point line incidences. The famous Szemeredi Trotter theorem says that if $\mathbb{F} = \mathbb{R}$ then $|I| = O(n^{4/3})$ . I have read somewhere that an upper bound of $O(n^{3/2})$ is very easy to prove and works for any field. However i cannot seem to figure out how to prove this. How does one prove this? The best I got after a bit of thought is the trivial $|I| \leq \binom{n}{2}\dfrac{n}{n - 1}$ .","['discrete-geometry', 'combinatorics', 'discrete-mathematics']"
4767924,Definitions of the total variation distance,"Introduction. I found the following definitions of the total variation distance $d_{TV}$ between two probability distributions (also called probability measures) $P$ and $Q$ on $\mathcal{A}$ (please note that I tried to use a consistent notation in the following definitions!): \begin{align}
&{\color{blue}{\textbf{""Definition 2.4"" on page 84, in Tsybakov (2009)}}} \\&\hspace{10ex} d_{TV}(P,Q)
= \sup_{A \in \mathcal{A}} \left| P(A)-Q(A) \right| = \sup_{A \in \mathcal{A}} \left| \int_{A} (p-q)d\nu \,\right| \\ \\
&{\color{blue}{\textbf{""2.1 Definition"" on page 5, in Strasser (1985)}}} \\&\hspace{10ex}d_{TV}(P,Q)
= \left\Vert P-Q\right\Vert = \sup \{\left| P(A)-Q(A) \right| : {A \in \mathcal{A}} \} \\ \\
&{\color{blue}{\textbf{""4.1. Total Variation Distance"" on page 47, in Levin&Peres (2017)}}} \\&\hspace{10ex}d_{TV}(P,Q)
= \left\Vert P-Q\right\Vert = \max_{A \subseteq \mathcal{A}} \left| P(A)-Q(A) \right| \\ \\
&{\color{blue}{\textbf{On page 22, in Villani (2008)}}}\\&\hspace{10ex}d_{TV}(P,Q)
= \left\Vert P-Q\right\Vert = 2 \inf \left\{ \mathbb{E} [\mathcal{1}_{X \neq Y}]; \,\text{law}(X)=P, \text{law}(Y)=Q \right\}
\\
\end{align} Question. Since I got confusions on the variety of definitions of the total variation distance, could you please show/prove/derive one, or more (or all!), of the following equalities? Or suggest some references proving those equalities? \begin{align}
&{\color{red}{\textbf{First Equality:}}} \qquad
&&\sup_{A \in \mathcal{A}} \left| P(A)-Q(A) \right|
\stackrel{\bf{{\color{red}?}}}{=}
\sup_{A \in \mathcal{A}} \left| \int_{A} (p-q)d\nu \,\right| 
\\ \\
&{\color{red}{\textbf{Second Equality:}}} \qquad
&&\left\Vert P-Q\right\Vert 
\stackrel{\bf{{\color{red}?}}}{=}
\sup \{\left| P(A)-Q(A) \right| : {A \in \mathcal{A}} \}
\\ \\
&{\color{red}{\textbf{Third Equality:}}} \qquad
&&\left\Vert P-Q\right\Vert 
\stackrel{\bf{{\color{red}?}}}{=}
\max_{A \subseteq \mathcal{A}} \left| P(A)-Q(A) \right|
\\ \\
&{\color{red}{\textbf{Fourth Equality:}}} \qquad
&&\left\Vert P-Q\right\Vert 
\stackrel{\bf{{\color{red}?}}}{=}
2 \inf \left\{ \mathbb{E} [\mathcal{1}_{X \neq Y}]; \,\text{law}(X)=P, \text{law}(Y)=Q \right\}
\\
\end{align} References. Tsybakov (2009) Strasser (1985) Levin&Peres (2017) Villani (2008)","['measure-theory', 'probability-theory', 'probability-distributions', 'total-variation', 'probability']"
4767948,"Given a point P and a circle, how to check if segment (0,0) to P intersects the circle?","Suppose I am at $(0,0)$ . Given a point $P$ , I want to know if it is in the blue region or in the red. In other words, I want to know if a segment from $(0,0)$ to $P$ will intersect a given circle. I don't need to know where it intersects. I want to answer if this segment intersects the circle in the quickest way possible (less operations), for a computer program execution. I'm wondering if there is a quicker way of checking this other than taking the distance from point to line and comparing with the radius.","['analytic-geometry', 'geometry']"
4768066,Functional equation $f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right)$,"I have not any idea, how to attack the equation $$f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right)$$ with  unknown $f:\mathbb{R} _{+}\rightarrow \mathbb{R}$ . Allowing (for a while) that $y$ can be equal to $0$ , we get $$\begin{aligned}f\left( \dfrac{x}{2}\right) =f\left( x\right) \\ f\left( \dfrac{x}{2^{n}}\right) =f\left( x\right) \\ f\left( 2x\right) =f\left( x\right) \\ f\left( 2^{n}x\right) =f\left( x\right) \end{aligned}$$ This suggests that f can be constant.
Otherwise, as $$\begin{aligned}HM\left( x,y\right) \cdot AM\left( x,y\right) =GM^{2}\left( x,y\right)= xy,\end{aligned} $$ one can guess that (up to a constant) $f$ can be a logarithmic function.
So, $f(x) =A\ln x+B$ .
How to perform a rigorous proof of these ( maybe without guessing ?)
How to prove, that there's not other solutions ?
Thanks in advance for your help...",['analysis']
4768089,Is there an isomorphism between the set of all bijective functions and the induced set-valued functions?,"Given any function $f: X \to Y$ (and consider in general the set of functions with this domain and codomain), there is an induced function $f': \mathcal{P}(X) \to \mathcal{P}(Y)$ such that $f': A \mapsto f(A)$ etc. My question is about what structure is preserved between the two. Do we have to restrict the set of functions to those which are bijective in order to achieve isomorphism (and what does the isomorphism preserve? Inverses, corresponding to the preimage? What else? Is this an important relationship? For example, do I always have $(g \circ f)' = g' \circ f'$ ? Do I need to restrict to bijective functions for this and/or anything else?",['functions']
4768100,What is the maximum number of ways this set of numbers can multiple to 1?,"Fix $n\geq 2$ and a bitstring $b \in \{0,1\}^n$ , and consider the product of real numbers $$
a_1^{(b_1)}\cdot a_2^{(b_2)}\cdot \cdots \cdot a_n^{(b_n)}
$$ where $a_i^{(0)}\in (0,1]$ and $a_i^{(1)} \in (1,2]$ for $i=1,...,n$ . Of all $2^n$ possible products corresponding to different choices of $b \in \{0, 1\}^n$ with $b = b_1\cdots b_n$ , what is the maximum number of products such that $$
a_1^{(b_1)}\cdot a_2^{(b_2)}\cdot \cdots \cdot a_n^{(b_n)} = 1?
$$","['combinatorics', 'discrete-mathematics']"
4768129,Is the topology generated by a union of an increasing chain of metrizable topologies metrizable?,"Let $S = \{\mathcal{T}_i \subset \mathcal{P}(X) : i \in I\}$ be metrizable topologies on $X$ , where $I$ is a linearly ordered set, and $\mathcal{T}_i \subset \mathcal{T}_j$ whenever $i \leq j$ ; i.e. $\mathcal{T}_i$ are increasing. Is the topology generated by $\bigcup S$ metrizable?","['general-topology', 'metric-spaces']"
4768250,Encountered $\displaystyle{\sum_{k=1}^\infty\frac{(-1)^k\sin(kt)}{k}}$ while solving another integral,"I am trying to evaluate the following, $$\zeta=\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}\ dx$$ but I encountered the $\color{orange}{\text{series}}$ , $$\color{orange}{\sum_{k=1}^\infty\frac{(-1)^k\sin(kt)}{k},\quad t\in(0,2\pi)}$$ which I do not know how to compute. By a little search I found that, $$\sum_{k=1}^\infty\frac{\sin(kt)}{k}=\frac{\pi-t}{2},\quad t\in(0,2\pi)$$ which would be useful if I was evaluating, $$\sum_{k=1}^\infty\frac{1}{k}\int^\infty_{\color{red}{2\pi k}}\frac{\sin(x)}{x}\ dx$$ but that is not the case. Any ideas on $\zeta$ or the series is welcome. My attempt is below. Let $x=u+\pi k$ , $$\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}\ dx=\sum_{k=1}^\infty\frac{1}{k}\int_{0}^\infty\frac{\sin(u+\pi k)}{u+\pi k}\ du$$ note that for $k\in\mathbb{N}$ , $\sin(u+\pi k)=\sin(u)\cos(\pi k)+\sin(\pi k)\cos(u)=(-1)^k\sin(u)$ , $$\sum_{k=1}^\infty\frac{(-1)^k}{k}\int_{0}^\infty\frac{\sin(u)}{u+\pi k}\ du=\sum_{k=1}^\infty\frac{(-1)^k}{k}\int_0^\infty\frac{\sin(ks)}{s+\pi }\ ds$$ after $s=u/k$ . Then we can exploit the periodicity of the integrand as follows, $$\int_0^\infty\frac{\sin(ks)}{s+\pi}\ ds=\sum_{n=0}^\infty\int_{2\pi n}^{2\pi(n+1)}\frac{\sin(ks)}{s+\pi}\ ds=\sum_{n=1}^\infty\int_0^{2\pi}\frac{\sin(kt)}{t+(2n-1)\pi}\ dt$$ where we made the substitution $t=s-2\pi n$ and re-indexed the series. After interchanging the series' and summing over $k$ pops up, $$\sum_{n=1}^\infty\int_0^{2\pi}\color{orange}{\sum_{k=1}^\infty\frac{(-1)^k\sin(kt)}{k\color{black}{(t+(2n-1)\pi)}}}\ dt$$ Edit: Looking at the answers I am receiving, evaluating $\zeta$ may not be as straightforward as I initially thought; the $\color{\orange}{\text{series}}$ seems to complicate convergence a bit. I am going to add calculations for the analogous series, so maybe someone can help me find a solution to $\zeta$ . Let $u=x-2\pi k$ then $t=u/k$ (we could combine these substutions into one), $$\sum_{k=1}^\infty\frac{1}{k}\int^\infty_{\color{red}{2\pi k}}\frac{\sin(x)}{x}\ dx=\sum_{k=1}^\infty\frac{1}{k}\int_0^\infty\frac{\sin(u)}{u+2\pi k}\ du=\sum_{k=1}^\infty\frac{1}{k}\int_0^\infty\frac{\sin(kt)}{t+2\pi}\ dt$$ working similarly as above, $$\sum_{k=1}^\infty\frac{1}{k}\int_0^\infty\frac{\sin(kt)}{t+2\pi}\ dt=\sum_{k=1}^\infty\frac{1}{k}\sum_{n=0}^\infty\int_{2\pi n}^{2\pi(n+1)}\frac{\sin(kt)}{t+2\pi}\ dt=\sum_{k=1}^\infty\frac{1}{k}\sum_{n=1}^\infty\int_0^{2\pi}\frac{\sin(ks)}{s+2\pi n}\ ds$$ after $s=t-2\pi n$ and re-indexing. Switching the order of summation, $$\sum_{n=1}^\infty\sum_{k=1}^\infty\int_0^{2\pi}\frac{\sin(ks)}{k(s+2\pi n)}\ ds=\frac{1}{2}\sum_{n=1}^\infty\int_0^{2\pi}\frac{\pi-s}{s+2\pi n}\ ds$$ where I used the nicer result, $$\sum_{k=1}^\infty\frac{\sin(ks)}{k}=\frac{\pi-s}{2},\quad s\in(0,2\pi)$$ the integral is now easy considering the two numerators. Simple integration yields, $$\frac{\pi}{2}\sum_{n=1}^\infty\left((1+2n)\log\left(1+\frac{1}{n}\right)-2\right)$$ distributing the $1/2$ factor into the sum and rewriting as a product (we take limits), $$\lim_{N\to\infty}\pi\log\frac{1}{e^N}\prod_{n=1}^N\left(\frac{n+1}{n}\right)^{(n+1/2)}$$ the product telescopes, $$\lim_{N\to\infty}\pi\log\frac{\sqrt{N+1}}{e^N}\frac{(N+1)^N}{N!}$$ by Taylor's and Stirling's formula, $$\frac{\sqrt{N+1}}{e^N}\frac{(N+1)^N}{N!}\sim\frac{e}{\sqrt{2\pi}}\left(1-\frac{1}{12N}+\frac{25}{288N^2}-\cdots\right)$$ as $N\to+\infty$ , hence, $$\sum_{k=1}^\infty\frac{1}{k}\int^\infty_{\color{red}{2\pi k}}\frac{\sin(x)}{x}\ dx=\pi \log\frac{e}{\sqrt{2\pi}}.$$","['integration', 'solution-verification', 'definite-integrals', 'sequences-and-series']"
4768288,Is there a function whose autoconvolution is its square? $g^2(x) = g*g (x)$,"I am looking for a function over the real line, $g$ , with $g*g = g^2$ (or a proof that such a function doesn't exist on some space like $L_1 \cap L_2$ or $L_1 \cap L_\infty$ ). This relation can't hold in a non-trivial way over any finite space, by that I mean that if $f$ is a probability density function, then $fg^2 = fg * fg$ implies by integration that $E_f[g^2] = E_f[g]^2$ , so $g$ would have to have $0$ variance. Furthermore, this relation can't hold on the positive real line since $e^{-x}$ is a probability distribution with $e^{-x}(g*g) = (e^{-x}g)* (e^{-x}g)$ , which would again mean that $g$ has $0$ variance, forcing it to be zero. Alternatively, one could hope for a power series for $g$ and use the fact that $h_n \equiv x^{n-1}/(n-1)!$ has $h_n * h_m = h_{n+m}$ over the positive real line to derive that the power series of $g^2$ would have to be zero. None of the tricks in this paragraph seem to work over the entire real line, however. Instead, Fourier transforming $g^2 = g*g$ reveals the same equation in Fourier space: $\hat{g}^2 = \hat{g^2} = \hat{g} * \hat{g}$ . If we assume $g$ and $g^2$ have finite moments, this lets us relate them by differentiating $\hat{g}^2$ repeatedly to reveal the binomial-type formula $E_{g^2}[x^n] = \Sigma_{k = 0}^n {n \choose k}E_g[x^k]E_g[x^{n-k}]$ , where $E_g[x]$ means $\int_{-\infty}^\infty dx g x$ . This is quite the strange condition to me, but it doesn't seem to lead to any obvious contradictions. It says that the cumulants of $g^2$ are twice that of $g$ . If we expand $g = \sum_{n=0} a_n \psi_n$ in terms of Hermite-Gauss functions $\psi_n(x) \equiv H_n(x)e^{-x^2/2}$ and use the convolution theorem, the fact that $F[\psi_n] = (-i)^n \psi_n$ , and orthonormality of the $\psi_n$ , we can derive the fact that the 4 mod 4 families of modes $\{\psi_n:n = k \hspace{.2cm}\text{mod} 4\}$ in $g^2$ result from the products over modes in $g$ whose index sums to $k \hspace{.1cm} \text{mod} 4$ : $\langle \psi_k , g^2\rangle = \sum_{n+m = k\text{mod 4}} a_n a_m <\psi_n \psi_m , \psi_k> $ for any $k$ . This is opposed to the usual situation where $n+m = k+2$ modes are also included in the sum. This says that the index of $g$ 's modes don't mix mod 4 after squaring. This condition is difficult to work with because there is not a nice way of dealing with $<\psi_n \psi_m, \psi_k>$ . If there was an extra $e^{x^2/2}$ inside that inner product, it would let us use the tripple product formula for $H_n(x)$ which has a nice form where only finitely many terms pop out (unlike the seemingly all-to-all coupling of the $\psi$ product). A solution of $g^2 = g*g$ would lead to solutions of $g^2 = \lambda g*g$ for any $\lambda$ by using $g(x/\lambda)*g(x/\lambda) (\lambda x) = \lambda g*g(x)$ . Furthermore, using $(e^{\lambda x}g)^2 = e^{2 \lambda x} g*g = e^{\lambda x} (e^{\lambda x}g)*(e^{\lambda x})$ would give us a solution to $g^2 = e^{\lambda x} g*g$ for any $\lambda$ . Substituting $g(x) \rightarrow g(x+\delta)$ gives us solutions to translated equations $g^2(x) = g*g (x+\delta)$ . Unfortunately, none of these transformations have made the solution to the problem obvious. The problem becomes trivial if we are allowed to rescale like $g(x)^2 = g*g(\sqrt{2} x) \sqrt{2}$ , as its just a Gaussian! Herein lies the reason why I suspect that there is no solution to $g^2 = g*g$ . The left-hand side tightens $g$ , whereas the right-hand side spreads $g$ out. Interestingly enough, there are solutions to $g = g*g$ (sinc) as well as $g^{1/2} = g*g$ (again Gaussian), but the homogeneous version of the problem seems more elusive. I have read some papers about solving convolutional equations, but they as far as I have seen have either been over a finite space or have not had the non-linearity present here. I have also tried taking a fractional Fourier transform with angle $-\pi/4$ to convert $g^2 = g*g$ into $g_{-\pi/4}*_{-\pi/4}g_{-\pi/4} = g_{-\pi/4}*_{\pi/4} g_{-\pi/4}$ , which leads to another two dimensional integral which is easy to write down but not solve. I thought of doing a fractional Fourier transform about a very small angle in order to make the convolution $g*g$ less delocalized as well as the product $g*g$ smoother. This didn't really lead to anything helpful. I tried thinking up of a scheme to find better and better approximations for $g$ , as I would be happy even with an implicit answer or a numerical method to plot $g$ . The obvious candidate $g = \sqrt{g*g}$ has the problem of there being cycles (Gaussians) as well as convolutions being difficult to compute over the whole real line. If one thinks of $g^2-g*g$ as a Lagrangian and tries to do a path integral of $e^{-|g^2-g*g|}$ over configurations of $g$ with some given $||g||$ , then I think this corresponds to a badly non-local field theory. Yikes! As a final note, the problem can be restated as finding a function where the act of squaring commutes with taking a Fourier transform: $\hat{g}^2 = \hat{g^2}$ . This would be solved (sufficient but not necessary) by a function who is its own Fourier transform $g = \hat{g}$ , and whose square is its own Fourier transform $g^2 = \hat{g^2}$ . But again we run into the problem where there is no good basis in which to to do both the product and the Fourier transform. Polynomials multiply well but Fourier transform/convolve poorly, vice versa for Hermite Gaussians (etc etc with seemingly every choice of basis). Any help approaching this problem would be greatly appreciated, as I am seriously losing my mind over it! Edit: There is also a simple asymptotic argument which shows that if g is upper and lower bounded by ~ $x^{-n}$ at infinity for some n then it can't solve $g^2 = g*g$ . The reason is that the limit of $x^n g^2$ is $0$ , but the limit of $x^n g*g$ would be $\int_{-\infty}^\infty dx g(x)$ , so this would have to be zero (forcing the integral of $g^2$ to be zero as well). However such an argument fails if $g$ decays faster than any polynomial, or if it repeatedly crosses $0$ as $x$ goes off to infinity. Another edit: I believe that the answer is yes and unique (but trivial) for g taken to be a discrete vector with elements over $Z$ . This is because Fourier transforming the problem moves back to a compact space where the variance argument shows that the only solution is constant, meaning that the only solution for such a $g$ is $g_i = \delta_i$ .","['nonlinear-analysis', 'fourier-analysis', 'convolution', 'fourier-transform', 'real-analysis']"
4768294,For what schemes $X$ are Cartier divisors the same thing as invertible subsheaves of $\mathcal{K}_X$?,"Let $X$ be a scheme, and let $\mathcal{K}$ be the sheaf of total quotient rings of $X$ . Is the data of a Cartier divisor on $X$ equivalent to the data of an invertible subsheaf of $\mathcal{K}$ for all schemes $X$ or do we need additional assumptions? In more detail: In most (if not all) textbooks, a Cartier divisor is defined to be a global section of the sheaf $\mathcal{K}^*/O^*$ . Explicitly, it is given by the following data: an open cover $\{U_i\}$ of the scheme $X$ , and for each $i$ , an $f_i \in \Gamma(U_i, \mathcal{K}^*)$ , such that for each $i, j$ , $f_i/f_j \in \Gamma(U_i \cap U_j, O^*)$ . Two such data $\{(U_i, f_i)\}$ , $\{(V_j, g_j)\}$ define the same Cartier divisor if there is a common refinement $\{W_k\}$ such that $\displaystyle\frac{(f_i)|_{W_k}}{(g_j)|_{W_k}} \in \Gamma(W_k, O^*)$ . Given a Cartier divisor $D$ , one may construct an invertible sheaf $L(D)$ which is a subsheaf of $\mathcal{K}$ locally generated by $\displaystyle\frac{1}{f_i}$ on $U_i$ . All of this is stated in most textbooks, but the converse is usually not. It seems to me that this process can be reversed, i.e. given an invertible subsheaf $L \subseteq \mathcal{K}_X$ , we choose an open cover $\{U_i\}$ of $X$ such that $L|_{U_i}$ is trivial, which allows us to choose a trivializing section $g_i \in \Gamma(U_i, L) \subseteq \Gamma(U_i, \mathcal{K})$ . I believe the fact that $g_i$ generates $L|_{U_i} \cong O_{U_i}$ implies that $g_i$ is not a “zero-divisor”, therefore $g_i \in \Gamma(U_i, \mathcal{K}^*)$ . The intuition is that $\mathcal{K}^*$ models the total quotient ring in which an element is either a zero-divisor or a unit. But I am hesitant because I did not add any additional assumption on the scheme $X$ (e.g. integral, locally noetherian, etc) and after seeing a paper on misconceptions about $\mathcal{K}_X$ , I feel less confident about my reasoning. In any case, if the above reasoning stands, we have $g_i \in \Gamma(U_i, \mathcal{K}^*)$ , so we obtained a Cartier divisor $\{(U_i, g_i^{-1})\}$ (the condition $g_i/g_j \in \Gamma(U_i \cap U_j, O^*)$ is easy). The above constructions seem to be inverses of each other. That explains the title of the question: is a Cartier divisor the same thing as an invertible subsheaf of $\mathcal{K}_X$ on any scheme $X$ ? I hope someone can either confirm it or deny it. Along this line of thought, it is usually stated (e.g. Hartshorne Corollary 6.14) that on any scheme $X$ , the map $\text{CaCl} X \to \text{Pic} X$ from  the Cartier divisor class group to the Picard group is injective, but not surjective in general. I am wondering if the reason why it is not surjective is that one can’t always embed an invertible sheaf $L$ in $\mathcal{K}_X$ ? When $X$ is integral, as in Hartshorne Proposition 6.15, $\mathcal{K}_X$ is the constant sheaf of the function field of $X$ and $L \otimes \mathcal{K}_X \cong \mathcal{K}_X$ , so $L \hookrightarrow L \otimes \mathcal{K}_X \cong \mathcal{K}_X$ can be realized as a subsheaf of $\mathcal{K}_X$ . Further remarks Regarding when an invertible sheaf $L$ is associated to a Cartier divisor $D$ , I also find the following discussion useful https://mathoverflow.net/questions/53567/why-is-line-bundle-appropriate-rational-section-not-a-standard-kind-of-diviso It seems to me that given an invertible sheaf $L$ on a scheme $X$ ,
the following are equivalent: (a) there exists a Cartier divisor $D$ such that $L \cong \mathcal O(D)$ ; (b) $L$ admits an invertible rational section $s$ (appropriately defined); (c) $L \otimes_{\mathcal O_X} \mathcal K_X \cong \mathcal K_X$ . (a) $\Rightarrow$ (b): assume wlog $L = \mathcal O(D) \subseteq \mathcal K$ ,
then $1$ is an invertible rational section. (b) $\Rightarrow$ (a): take $D= \text{div} (s)$ . (c) $\Rightarrow$ (a): for any invertible sheaf $L$ , we have an injection $L \hookrightarrow L \otimes_{\mathcal O_X} \mathcal K_X$ because $L$ is locally free of rank $1$ . If $L \otimes_{\mathcal O_X} \mathcal K_X \cong \mathcal K_X$ ,
then we get an embedding $L \hookrightarrow \mathcal K_X$ ,
therefore $L$ is isomorphic to an invertible subsheaf of $\mathcal K_X$ ,
which, by what we have settled, is $\mathcal O(D)$ for some Cartier divisor $D$ . (a) $\Rightarrow$ (c): assume $L$ is an invertible subsheaf of $\mathcal K_X$ ,
then the natural map $L \otimes_{\mathcal O_X} \mathcal K_X \to \mathcal K_X$ is an isomorphism.","['divisors-algebraic-geometry', 'algebraic-geometry']"
4768344,Law of Cosines: positive or negative last term?,"The law of cosines is usually stated something like $$
C^2 = A^2 + B^2 - 2 AB \cos \theta
$$ Where $\theta$ is the angle opposite C in our triangle. However, if we have three vectors such that $$
\vec{C} = \vec{A} + \vec{B}
$$ with $\theta$ being the angle between $ \vec{A}$ and $ \vec{B}$ , we can easily take the dot product of $ \vec{C}\cdot\vec{C}$ to find that $$
C^2 = A^2 + B^2 + 2 AB \cos \theta.
$$ Why are these so similar except for the different sign at the end? I can’t figure out why this wouldn’t give the exact same answer.","['trigonometry', 'geometry']"
4768415,Irregular pentagon with maximum number of parallel and perpendicular lines,"We have 5 dots connected by lines like in the image. We have 5 points, so we have a pentagon. For a regular pentagon, we can connect all the vertices and there are 10 lines, with 5 groups of parallel lines. None of these lines are perpendicular to other. In the irregular pentagon (polygon) of the figure, there are 10 lines and each one of these is either parallel or perpendicular to another one. Is this figure unique? I.e. are there other polygons where we have 10 lines having parallel and/or perpendicular? At least one right relation. In a square with a central point, there also would be 10 parallel/perpendicular relationships, but that is not a convex polygon (a concave polygon change shape if we connect vertices). Any easy way of demonstrating using analytical geometry. I can demonstrate for a rectangular grid by using Pythagorans several times, but not for a general case. Can you find another 5 side polygon with this property? Proof with another pentagon, slightly different and only 9 relationships, using a rectangle and demonstrating it must be a 1:sqrt(2) rectangle by iterative use of Pythagoras theorem.",['geometry']
4768421,Is this a proof that ALL generalized inverses of a hermitian matrix is also hermitian?,"I know that the inverse of a nonsingular hermitian matrix is also hermitian (symmetric if matrix is real). I also know that a singular hermitian matrix has a hermitian generalized inverse. But are ALL generalized inverses of a hermitian matrix also hermitian? I could not find an answer anywhere, and the following proof suggests so. Please help me find any mistakes if it is not so. Definition 1 : A matrix $A^- \in \mathbb{C}^{n \times m}$ is a generalized inverse of a matrix $A \in \mathbb{C}^{m \times n}$ if $A A^- A = A$ . Lemma 2 : Every hermitian matrix $A \in \mathbb{C}^{n \times n}$ can be decomposed into a product of a matrix and its conjugate transpose: $A = X^\dagger X$ , where $X \in \mathbb{C}^{r \times n}$ ( proof ). Now, I will take this $X$ and construct a perpendicular (orthogonal) projection matrix onto its column space $C(X)$ . Definition 2 : $M \in \mathbb{C}^{n \times n}$ is a perpendicular projection matrix onto $X$ if and only if: $v \in C(X) \Rightarrow Mv = v$ $w \perp C(X) \Rightarrow Mw = 0$ Lemma 3 : $M$ is a perpendicular projection matrix onto $C(M) = C(X)$ if and only if: $MM = M$ $M^\dagger = M$ The proof for lemma 3 is taken from Plane Answers to Complex Questions Proposition B.32 and Theorem B.33 and slightly modified to account for complex matrices. Lemma 4 : Perpendicular projection matrices are unique (Proposition B.34 from Plane Answers to Complex Questions ). Lemma 5 : If $G$ is a generalized inverse for $X^\dagger X$ , then $X G X^\dagger X = X$ (Proposition B.43 from Plane Answers to Complex Questions ). Lemma 6 : The (unique) perpendicular projection matrix onto $C(X)$ is $X (X^\dagger X)^- X^\dagger$ . Proof: Show that this matrix holds properties in definition 2: For $v \in C(X)$ , $v = Xb$ for some $b \in \mathbb{C}^n$ . So, using lemma 5: $$X (X^\dagger X)^- X^\dagger v = X (X^\dagger X)^- X^\dagger Xb = Xb = v$$ For $w \perp C(X)$ , $X^\dagger w = 0$ . So: $$X (X^\dagger X)^- X^\dagger w = 0$$ Since lemma 4 says that perpendicular projection operators are unique, this is the one and only perpendicular projection operator onto $C(X)$ . Since lemma 3 tells us that a perpendicular projection matrix is hermitian, the matrix from lemma 6 must be hermitian: $$X (X^\dagger X)^- X^\dagger = (X (X^\dagger X)^- X^\dagger )^\dagger = X ((X^\dagger X)^-)^\dagger X^\dagger $$ Since $X^\dagger X = A$ , we have: $$A^- = (A^-)^\dagger $$ This works for any arbitrary hermitian matrix $A$ . Does this prove that any generalized inverse of a non-zero hermitian matrix is hermitian?","['statistics', 'matrices', 'solution-verification', 'linear-algebra', 'inverse']"
4768427,An Hamiltonian diffeomorphism is also a Poisson diffeomorphism,"Let $(M,\{-,-\})$ be a Poisson manifold. An Hamiltonian isotopy is a smooth family of diffeomorphisms $\{\varphi^t:M\to M\}_{t\in [0,1]}$ such that $\varphi^0=\text{id}_M$ there exists a smooth family of functions $\{h_t:M\to \mathbb{R}\}_{t\in [0,1]}$ such that $$\frac{d\varphi^t(x)}{dt}=X_{h_t}|_{\varphi^t(x)}$$ where $X_{h_t}$ is the Hamilton vector field induced by $h_t$ (i.e. $X_{h_t}(g):=\{h_t,g\}$ for any $g\in C^\infty(M)$ ). A diffeomorphism $\varphi:M\to M$ is a Hamiltonian diffeomorphism iff there exists an Hamiltonian isotopy $\{\varphi^t\}$ such that $\varphi^1=\varphi$ . I want to prove that an Hamiltonian diffeomorphism is also a Poisson diffeomorphism i.e. $$\{f\circ \varphi,g\circ \varphi\}=\{f,g\}\circ \varphi$$ for any $f,g\in C^\infty(M)$ . My attempt Let's prove that $$\{f\circ \varphi^t,g\circ \varphi^t\}-\{f,g\}\circ \varphi^t=0$$ for any $t$ . This trivially holds for $t=0$ , so I just need to prove that the derivative of the expression above (with respect to $t$ ) is $0$ : $$\frac{d}{dt}|_{t=t_0}\{f\circ \varphi^t,g\circ \varphi^t\}-\{f,g\}\circ \varphi^t=$$ $$=\left\{f_\ast\left(\frac{d}{dt}|_{t=t_0}\varphi^t\right),g\circ \varphi^{t_0}\right\}+\left\{f\circ \varphi^{t_0},g_\ast\left(\frac{d}{dt}|_{t=t_0}\varphi^t\right)\right\}-\{f,g\}_\ast \left(\frac{d}{dt}|_{t=t_0}\varphi^t\right)=$$ $$=\left\{f_\ast\left(X_{h_{t_0}}|_{\varphi^{t_0}(\cdot)}\right),g\circ \varphi^{t_0}\right\}+\left\{f\circ \varphi^{t_0},g_\ast\left(X_{h_{t_0}}|_{\varphi^{t_0}(\cdot)}\right)\right\}-\{f,g\}_\ast \left(X_{h_{t_0}}|_{\varphi^{t_0}(\cdot)}\right)=$$ $$=\{\{h_{t_0},f\}\circ \varphi^{t_0},g\circ \varphi^{t_0}\}+\{f\circ \varphi^{t_0},\{h_{t_0},g\}\circ \varphi^{t_0}\}-\{h_{t_0},\{f,g\}\}\circ \varphi^{t_0}$$ And now I feel stuck.","['vector-fields', 'symplectic-geometry', 'poisson-geometry', 'differential-geometry']"
4768500,Is there an algorithm to check whether given subgroup contained inside the Frattini subgroup?,"I am new to algorithmic group theory. I have the following question: Let $G$ be a group. The Frattini subgroup of $G$ is the intersection of all maximal subgroup of $G$ , denoted by $\Phi(G)$ . It is also equal to the set of all non-generating elements of $G$ . Question: Given a solvable group $G$ (by its multiplication table), and $H \leq G$ , is there an algorithm to check if $H\leq \Phi(G)$ ? Note: Suppose $K$ is $p$ -group, then $\Phi(K)= K^{p}[K,K]$ . Thus, given $H \leq K$ we can check if $H \leq \Phi(K)$ ? Hence we can solve the question for Nilpotent groups as well. But, is there an algorithm that find Frattini subgroup for solvable groups or even super-solvable groups? or Can we direct answer if $H \leq \Phi(G)$ without finding $\Phi(G)$ ? There are practical algorithm that finds the Frattini subgroup. I am interested in some theoretical algorithm that runs. Of course the trivial algorithm can finds subgroup lattice and find the Frattini subgroup explicitly. But is there an algorithm that runs in $|G|^c$ time for some constant $c$ .","['frattini-subgroup', 'cayley-table', 'algorithms', 'group-theory', 'computational-complexity']"
4768547,Exclusive Or vs Inclusive Or,"I have a doubt on constructing English statements from propositions. I went through three exercises and every time I faced the same situation I can not solve the proposition properly Let p and q be the propositions p : I bought a lottery ticket this week. q : I won the million dollar jackpot. I have to find the English translation ¬p ∨ (p ∧ q) My answer is: I did not buy a lottery ticket this week or I bought the lottery ticket this week and I won the million dollar jackpot. I found on the web that they used either, "" Either I didn't buy a lottery ticket this week, or I did and I won the million dollar jackpot."" I did not understand what is the need for using exclusive or instead of inclusive or? Then I tried another exercise Let p and q be the propositions “Swimming at the New Jersey shore is allowed” and “Sharks have been spotted near the shore,” respectively. compound proposition is ¬p ∧ (p∨ ¬q) My answer is : Swimming at the New Jersey shore is not allowed and swimming at the New Jersey shore is allowed or sharks have not been spotted near the shore. I found on the web: Swimming at the New Jersey shore is not allowed and either swimming at the New Jersey shore is allowed or sharks have not been spotted near the shore. Here again, people used exclusive or instead of inclusive or . Then I went though the third example, Let p and q be the propositions “The election is decided” and “The votes have been counted,” respectively. The compound proposition is ¬q ∨ (¬p ∧ q) Now, I used exclusive or after seeing so many examples. My answer is: Either the votes have not been counted or the election is not decided and the votes have been counted. This time I found on the Web two variations The votes have not been counted, or they have been counted but the election is not(yet) decided. The votes have not been counted, or the votes have been counted but the election is not decided. My confusion is when to use exclusive or and when to use inclusive or? Thank you.","['propositional-calculus', 'logic', 'discrete-mathematics']"
4768620,Calculating the functional derivative of an implicit functional,"We want to calculate the functional derivative of the following wrt $\rho$ : $$F=\int X dx$$ where $X$ is implicitly defined as: $$X=\frac{1}{1+\overline \rho(x) \int \rho(x) X dx}$$ and $\overline \rho$ is the result of the following convolution: $$\overline \rho(x) = \int \rho(x^\prime)H(\sigma^2 - (x-x^\prime)^2)dx^\prime$$ where $H$ is the Heaviside step function: $$\begin{cases}
 1,& x\geq 0\\
 0,& x < 0\end{cases}$$ How can I calculate the functional derivative $\frac{\delta F}{\delta \rho}$ analytically? Note that I can solve this numerically by solving iteratively for X and then computing the functional derivative numerically. But I'm looking for the analytical expression.","['functional-equations', 'calculus-of-variations', 'implicit-differentiation', 'functional-analysis', 'derivatives']"
4768650,Confusion about Theorem 11 Chapter 2 of Hoffman and Kunze,"This question has been asked before but I'm afraid I didn't follow the answer (as I mention in my (1a) way below, I'm not even sure the answer is complete), and neither OP nor the answerer are active. I thus ask again, with some extra details around my confusion. Note that HK = Hoffman and Kunze. The theorem statement: Theorem 11. Let $m$ and $n$ be positive integers and let $\mathbb{F}$ be a field.
Suppose $W$ is a subspace of $\mathbb{F}^n$ and $\dim(W)\leq m$ . Then there is precisely one $m\times n$ row-reduced echelon matrix over $\mathbb{F}$ which has $W$ as its row space. Before giving the proof, I'll emphasize that I understand that this is an existence and uniqueness claim. That is, given some subspace $W$ of $\mathbb{F}^n$ (1) there exists a (2) unique $m \times n$ matrix in reduced-row echelon form (RREF). (Indeed, as we'll see by the construction, the matrices with different $m$ are trivially related by adding or removing zero rows). Now for the proof, which comes in what feels to me like a convoluted order. I label the various ""stanzas"" with letters so as to refer to them later. Proof. Existence: Proof omitted as I follow this. I will call the matrix  with the requisite properties proved constructively to exist in this step as $R'$ [see 1a below]. Uniqueness: (a) Now let $R$ be any row-reduced echelon matrix which has $W$ as its row
space. Let $\rho_1,\dots,\rho_r$ be the non-zero row vectors of $R$ and suppose that
the leading non-zero entry of $\rho_i$ occurs in column $k_i$ , $i=1,\dots,r$ . The
vectors $\rho_1,\dots,\rho_r$ form a basis for $W$ ( Theorem 10 ). In the proof of Theorem 10 , we
observed that if $\beta=(b_1, \dots , b_n)$ is in $W$ , then the unique expression of $\beta$ as a linear combination of $\rho_1,\dots,\rho_r$ is $$\beta=\sum_{i=1}^{r}b_{k_i}\rho_i.$$ Thus any vector $\beta$ is determined if one knows the coordinates $b_{k_i}$ , $i=1,\dots,r$ . (b) Suppose $\beta\in W$ and $\beta\ne0$ . We claim the first non-zero coordinate
of $\beta$ occurs in one of the columns $k_s$ . Since $$\beta=\sum_{i=1}^{r}b_{k_i}\rho_i$$ and $\beta\ne0$ , we can write $$\beta=\sum_{i=s}^{r}b_{k_i}\rho_i\;,\;b_{k_s}\ne0.$$ From the fact that $R$ is a row-reduced echelon matrix one has $R_{ij}=0$ if $i>s$ and $j\leq k_s$ . Thus $$\beta=(0,\dots,0,b_{k_s},\dots,b_n)\;,\;b_{k_s}\ne0$$ and the first non-zero coordinate
of $\beta$ occurs in one of the columns $k_s$ . (c) It is now clear that $R$ is uniquely determined by $W$ . The description
of $R$ in terms of $W$ is as follows. We consider all vectors $\beta = (b_1, \dots , b_n)$ in $W$ . If $\beta\ne0$ , then the first non-zero coordinate of $\beta$ must occur in some
column $t$ : $$\beta=(0,\dots,0,b_t,\dots,b_n)\;,\;b_t\ne0.$$ Let $k_1, \dots , k_r$ be those positive integers $t$ such that there is some $\beta\ne0$ in $W$ , the first non-zero coordinate of which occurs in column $t$ . Arrange $k_1, \dots , k_r$ in the order $k_1 < k_2 < \dots < k_r$ . For each of the positive
integers $k_s$ there will be one and only one vector $\rho_s$ in $W$ such that the $k_s$ th coordinate of $\rho_s$ is 1 and the $k_i$ th coordinate of $\rho_s$ is 0 for $i\ne s$ . Then $R$ is the $m\times n$ matrix which has row vectors $\rho_1, \dots , \rho_r, 0, \dots , 0$ . Now it's clear that stanza (c) is where the actual uniqueness claim is proved, and that stanzas (a) and (b) are lemmas used in (c). But I can't quite follow how. In particular, (1a) ""For each of the positive
integers $k_s$ there will be one and only one vector $\rho_s$ in $W$ such that the $k_s$ th coordinate of $\rho_s$ is 1 and the $k_i$ th coordinate of $\rho_s$ is 0."" All we have by construction of the $k_i$ is that there is at least one vector in $W$ with a nonzero entry in $k_s$ , but that doesn't say that it's zero in the other columns $k_i$ . So where does the ""there will be one"" come from? The answer linked seems to suggest that the existence is guaranteed by the existence of $R'$ with the required properties. But why should the columns which are nonzero in the $R'$ be related a priori to the $k_i$ which we have constructed here? (1b) Further, why should this be unique? I think (but am not sure, so can someone confirm?) that this is guaranteed by stanza (a), in that fixing the $b_{k_i}$ (in particular, we've chosen to fix $b_{k_i} = \delta_{is}$ ) determines the expansion coefficients of the vector in terms of the basis given by the rows of whatever $R$ (which has row space $W$ ) we are considering in this uniqueness proof, and basis expansion are unique. (2) Accepting that (1a,b) have been solved, I still cannot see why ""Then $R$ is the $m\times n$ matrix which has row vectors $\rho_1, \dots , \rho_r, 0, \dots , 0$ ."" Given that we haven't used it, clearly this must follow from stanza (b). But I can't see how it does the trick for us?","['matrices', 'linear-algebra', 'vector-spaces']"
4768668,Finding roots of a $3$-dimensional sinusoidal function,"I have a system of three sinusoidal equations in three dimensions that I'm trying to find the root of. I have gone down a wormhole with Newton-Raphson, Brent-Decker, and Bisection methods of numerical solving. This has just made me more confused. I also have had trouble finding examples of these methods in multiple dimensions. Here are the equations; $k_n$ are all known constants and I want the root in the range $\alpha,\beta,\gamma \in [-90,90],$ $$
\begin{align}
0=\;&k_1\,\sin\left(\alpha \right)\,\sin\left(\beta \right)\,\sin\left(\gamma \right) \\\\
+& k_2\,\sin\left(\alpha \right)\,\sin\left(\beta \right)\,\cos\left(\gamma \right) \\\\
+& k_3\,\sin\left(\alpha \right)\,\sin\left(\gamma \right) \\\\
+& k_4\,\sin\left(\alpha \right)\,\cos\left(\gamma \right) \\\\
+& k_5\,\cos\left(\alpha \right)\,\sin\left(\beta \right)\,\sin\left(\gamma \right) \\\\ 
+& k_6\,\cos\left(\alpha \right)\,\sin\left(\beta \right)\,\cos\left(\gamma \right) \\\\
+& k_7\,\cos\left(\alpha \right)\,\sin\left(\gamma \right) \\\\
+& k_1\,\cos\left(\alpha \right)\,\cos\left(\gamma \right) \\\\
\\\\
0=\;&k_8\,\sin\left(\alpha \right)\,\sin\left(\gamma \right) \\\\
+& k_9\,\sin\left(\alpha \right)\,\cos\left(\gamma \right) \\\\
+& k_{10}\,\cos\left(\alpha \right)\,\sin\left(\beta \right)\,\sin\left(\gamma \right) \\\\
+& k_8\,\cos\left(\alpha \right)\,\sin\left(\beta \right)\,\cos\left(\gamma \right) \\\\
+& k_{11}\,\cos\left(\beta \right)\,\sin\left(\gamma \right) \\\\
+& k_7\,\cos\left(\beta \right)\,\cos\left(\gamma \right) \\\\
\\\\
0=\;&k_9\,\sin\left(\alpha \right)\,\sin\left(\beta \right)\,\sin\left(\gamma \right) \\\\
+& k_{12}\,\sin\left(\alpha \right)\,\sin\left(\beta \right)\,\cos\left(\gamma \right) \\\\ 
+& k_8\,\cos\left(\alpha \right)\,\sin\left(\gamma \right) \\\\
+& k_9\,\cos\left(\alpha \right)\,\cos\left(\gamma \right) \\\\
+& k_4\,\cos\left(\beta \right)\,\sin\left(\gamma \right) \\\\
+& k_{13}\,\cos\left(\beta \right)\,\cos\left(\gamma \right)
\end{align}
$$ My questions are: What's the best way to solve for the three unknowns? Are there good resources on using the methods on multiple dimension? And, is there a good cpp library I should use?","['trigonometry', 'systems-of-equations', 'roots', 'numerical-methods']"
4768699,Suggestions to start Statistical Manifolds,"I am a computer science PhD student, and I need statistical manifolds theory for my work. I am currently reading Differential Geometry of Curves and Surface by Kristopher Tapp and Carmo. I plan to study Lee's smooth Manifold next. (My supervisor's recommendations.) Unfortunately I do not have the topology background to the depth I would like to have to; I studied Topology without Tears in my second year (not completely) but I do not remember all of what I read either. But I am keeping it alongside. Can anyone suggest a self-readable set of books to get to Stat. Manifold? I might end up needing quite a bit of Discrete Differential Geometry as well. Other suggestions about alternate study plans are also welcome.","['stochastic-geometry', 'statistics', 'book-recommendation', 'general-topology', 'differential-geometry']"
4768795,Directional derivative of proximal mapping of a convex function,"Let $f:\mathbb{R}^n\rightarrow\overline{\mathbb{R}}$ be a proper closed convex function that is locally Lipschitz continuous on its domain $D(f)$ . Define the proximal mapping of $f$ to be $$\textbf{prox}_{\lambda f}(x)=\arg\min_u\left\{f(u)+\frac{1}{2\lambda}\|u-x\|^2\right\}$$ In this context, it is not hard to see $\textbf{prox}_{\lambda f}(x)$ is a Lipschitz continuous function w.r.t. variable $x$ for any parameter $\lambda>0$ . I am wondering under what assumption on $f$ , will the directional derivative of proximal mapping, i.e., $$\textbf{prox}_{\lambda f}'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{prox}_{\lambda f}(x+td)-\textbf{prox}_{\lambda f}(x)}{t}$$ exist? From the existing literature (Proposition 5.3.5, p. 141), I know if $f:=I_C$ is an indicator function of a closed convex set $C$ (hence proximal mapping reduces to projection mapping), then the directional derivative exists. To be more precise,
for any $x\in C$ and $d\in\mathbb{R}^n$ , $$\textbf{proj}_C'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{proj}_C(x+td)-\textbf{proj}_C(x)}{t} = \textbf{proj}_{T_C(x)}(d)$$ where $T_C(x)$ is the tangent cone of $C$ at $x$ . I am wondering if this is only special for projection operator or can be slightly generalized to proximal operator of convex functions with some nice properties or structures.","['non-smooth-analysis', 'convex-optimization', 'proximal-operators', 'derivatives', 'convex-analysis']"
4768955,"If the entries of a matrix are sines of distinct prime numbers, could the determinant equal $0$?","I recently learned that if the entries of a matrix are sines of distinct integers, the determinant could equal $0$ . For example, $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0$ And I learned that if the entries of a matrix are distinct prime numbers, the determinant could equal $0$ . For example, $\det\begin{bmatrix}3&5&7\\13&17&19\\23&29&31\end{bmatrix}=0$ This leads to my question: If the entries of a matrix are sines of distinct prime numbers , could the determinant equal $0$ ? I have tried to apply the ideas contained in the answers and comments in the links above, to prove that the answer to my question is yes, without success.","['determinant', 'matrices', 'linear-algebra', 'trigonometry', 'prime-numbers']"
4768979,Interpolation inequality - what does it mean?,"Suppose $f_{k} \to f$ strongly in $L^{2}(\mathbb{R}^{n})$ . Let $2 \le 2q < \eta$ where $\eta = \infty$ if $n \le 2$ and $\eta = 2n/(n-2)$ if $n \ge 3$ . During a proof, my professor wrote that $\|f_{k}-f\|_{L^{2q}(\mathbb{R}^{n})} \to 0$ as $k \to \infty$ by interpolation inequality. I searched the term interpolation inequality on the internet and found a lot of different results. What does it mean in the present context? Edit: Let me add more details about my problem. I am trying to prove the following result: Let $g \in L^{p}(\mathbb{R}^{n})$ with $p > \max\{n/2,1\}$ and suppose $f_{n} \rightharpoonup f$ on $H^{1}(\mathbb{R}^{n})$ , meaning that $\langle f_{n}, h\rangle_{L^{2}}+\langle \nabla f_{n},\nabla h\rangle_{L^{2}} \to \langle f, h\rangle_{L^{2}}+\langle \nabla f, \nabla h\rangle_{L^{2}}$ for every $h \in H^{1}(\mathbb{R}^{n})$ . Then: $$\lim \int_{\mathbb{R}^{n}}g(x)|f_{n}(x)|^{2}dx = \int_{\mathbb{R}^{n}} g(x)|f(x)|^{2}dx$$ The proof goes like this. Let $q$ be such that $1/p+1/q = 1$ . Then, by Hölder and Minkowski inequalities: $$\int_{\mathbb{R}^{n}}|g(x)|||f_{n}(x)|^{2}-|f(x)|^{2}|dx \le \int_{\mathbb{R}^{n}}|g(x)||f_{n}(x)-|f(x)|(|f_{n}(x)|+|f(x)|)dx \le \|f_{n}-f\|_{L^{2q}}\||f_{n}|+|f|\|_{L^{2q}}\|g\|_{L^{p}} \le \|f_{n}-f\|_{L^{2q}}(\|f_{n}\|_{L^{2q}}+\|f\|_{L^{2q}})\|g\|_{L^{p}} \le K\|f_{n}-f\|_{L^{2q}}$$ and the latter must go to zero by some interpolation inequality.","['functional-analysis', 'analysis', 'distribution-theory']"
4769033,Estimate for $\sin(x)/x$ and all its derivatives,"Consider the function $f(x) := \frac{\sin x}{x}, f(0) := 1$ . It is easy to see that $$\lvert f(x) \rvert \leq \frac{2}{1+\lvert x \rvert}. $$ I am trying to prove that more generally, for each $k\in \mathbb N$ we have $$\lvert f^{(k)}(x) \rvert \leq \frac{C}{1+\lvert x \rvert}, $$ where $C$ may or may not depend on $k$ , but not on $x$ . From plotting the first few derivatives of $f$ , this seems obvious. For large values of $\lvert x \rvert$ I was able to prove it, but the case where $x$ is near $0$ seems to be more subtle. For example, $$f'(x) = \frac{x\cos x - \sin (x)}{x^2},$$ and the enumerator behaves like $x^3$ near $0$ , so cancels out the singularity. I appreciate any approach for proving the inequality for general $k$ !","['estimation', 'asymptotics', 'analysis', 'real-analysis', 'inequality']"
4769039,"Finding the angle EDB in triangle ABC, where E is the intersection of the angle bisector of C with side AB and D is a point on BC","This was a question I encountered while looking at some weekly math questions my school had hung in front of the department last week: I was unable to solve it, and now that some time has passed, I'd like to take a shot at it again. The problem goes as follows: For a triangle ABC, E $\in$ [AB], and D $\in$ [BC]. [EC] is the angle bisector of ACB. 𝑚(ADC) = 40° and 𝑚(𝐴EC) = 20°. Find the value of 𝑚(EDB). And in case anyone's curious, m(ADC) would mean the measure (in degrees) of angle ADC. So, I first tried using a diagram for this (the untouched form of which I've attached below) and set ECA (and since EC is the angle bisector, simultaneously the value of ECB) as equal to $\alpha$ . From there, I simply tried to express every angle in terms of $\alpha$ so that I could end up with some kind of equation, but that didn't work out either. I also tried to see if I could find any similar triangles within the diagram (specifically, EDF and ACF) but since I couldn't be sure whether or not ED and AC were parallel, that didn't work out either. So, right now, I'm sort of stumped. I don't really want a clear cut answer here: just some push in the right direction, or if there's some property here that I need to use, an explanation of said property. Trigonometry's always been one of my weaker subjects, so it seems likely there's something about angle bisectors I don't remember- but I'm not quite sure either. Thanks in advance! Here's the diagram: (if it doesn't work, do inform me- I'll try to fix it when I can) https://i.imgur.com/7cwFG7D.jpg","['euclidean-geometry', 'triangles', 'angle', 'geometry']"
4769076,Riesz representation theorem for functionals acting on Hölder $C^\alpha$ functions,"Assume you have a linear functional $F:C^\alpha(\mathbb R^n) \mapsto\mathbb R$ such that $$
|F(f)| \leq \vert f \vert_{C^\alpha(\mathbb R^n)} 
$$ but only depending on the Holder seminorm, that is, $$
|f|_{C^\alpha} = \sup_{x,y}|x-y|^{-\alpha} |f(x)- f(y)|.
$$ Can I assert that $F(f) = \int_{\mathbb R^n} g \cdot f \, dm$ for some $g \in H^p(\mathbb R^n)$ with $\Vert g \Vert_{H^p}\lesssim 1$ where $H^p$ is the real Hardy space?
Is there any reference for Riesz representation theorems on Holder-Hardy spaces? Edit: My idea why something like this might be true, is that we have $$
\int_{\mathbb R^n} f g \, dm \lesssim \vert f \vert_{C^\alpha} \Vert g \Vert_{H^p}
$$ where $\Vert g \Vert_{H^p}$ is the atomic $H^p$ norm. Note that the dual of $H^p$ is the homogeneous space of Holder functions of order $n(1/p − 1)$ .","['harmonic-analysis', 'hardy-spaces', 'duality-theorems', 'functional-analysis']"
4769101,When two infinite direct products $\prod_I\Bbb{Z}$ and $ \prod_J\Bbb{Z}$ are isomorphic?,"It is known that two free $\Bbb{Z}$ -modules $\bigoplus_{I}\Bbb{Z}$ and $\bigoplus_{J}\Bbb{Z}$ are isomorphic if and only if $|I|=|J|$ . Moreover, it is true for any two free module over a commutative ring $R$ . Now, a natural question in this context is ""if we replace the direct sum $\bigoplus$ with the direct product $\prod$ is the ruling still true?"" Thus, I have the following quation on infinite direct product of $\Bbb{Z}$ as $\Bbb{Z}$ -module. Question . Suppose that $I$ and $J$ are two (non-measurable) infinite cardinals, such that $|\prod_I\Bbb{Z}|=|\prod_J\Bbb{Z}|$ . Can we conclude that $\prod_I\Bbb{Z}\cong \prod_J\Bbb{Z}$ as $\Bbb{Z}$ -modules?","['module-isomorphism', 'group-theory', 'ring-theory', 'abelian-groups']"
4769164,Is there a metric on the plane for which (unit) circles are triangles?,"If we use the Euclidean metric on $\mathbb R^2$ , (unit) circles are circles; if we use the Manhattan distance or $\ell^1$ distance, (unit) circles are squares. We can also get polygons with an even number of sides: for instance, circles are hexagons if we define the distance between two points to be the shortest path between them that moves only along the directions of the sides of some fixed triangle. Is there a metric on $\mathbb R^2$ (that gives the usual topology) for which circles, or at least unit circles, are triangles? (This question is inspired by though unrelated to this question .)","['geometry', 'metric-spaces']"
4769187,"Prove $\lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5}$ using the $(\epsilon, \delta)$-definition.","I want to see if my proof is correct and if my choice of $\delta$ makes sense. Prove that $\lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5}$ . Let $\delta = \min(1, \frac{15}{2} \epsilon)$ and suppose $0 < |x-1| < \delta$ . First simplifying $|\frac{x-1}{2x^2+x-3} - \frac{1}{5}| = \frac{2}{5(2x+3)} |x-1|$ . Since $|x-1| < 1$ we have $\frac{1}{2x+3} < \frac{1}{3} $ . So $\frac{2}{5(2x+3)} |x-1| < \frac{2}{15} |x-1| < \frac{2}{15} \delta \le \frac{2}{15} \frac{15}{2} \epsilon = \epsilon$ . For further details $|x-1| < 1 \iff -1<x-1<1 \iff 0<x<2 \iff 0<2x<4 \iff 3<2x+3<7 \implies \frac{1}{2x+3} < \frac{1}{3}$","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
4769204,How to calculate the angle between these rectangles? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question I am trying to make a seating area similar to the following shape with three rectangles of equal length ( $l$ ) and width ( $w$ ), where the green rectangle makes an angle ( $\theta$ ) with the blue rectangle. While, the red rectangle makes an angle ( $\phi$ ) with the blue rectangle. I know that the length of the small arc $c_1=\alpha\, c_2$ , where $c_2$ is the length of the bigger arc and $\alpha>0$ . How does the angle ( $\theta$ ) change as I change the value of $\alpha$ ?","['trigonometry', 'geometry']"
4769237,Connection between z-scores and a non-standard normal distribution,"If you have a non-standard normal distribution $N(0,\sigma^2)$ , and you're interested the relative likelihood that $x=x$ , is there a way to use the z score $\frac{x'}{\sigma}$ with the PDF of the standard normal distribution in the same way you could if you were interested in $\Pr(x < x')$ . Of course, I am not interested in explicitly solving $\Pr(x=x')$ , but I am interested in the relative likelihood of $x'$ as you vary $\sigma^2$ to answer a question like ""if I have two non-standard normal distributions with different variance and I observe $x'$ , which distribution is more likely for that to have come from?""","['statistics', 'probability']"
4769240,Probability that randomly picking $4$ out of $90$ numbers yields an ascending sequence?,"Probability: In a box there are $90$ slips, numbered from $1$ to $90$ .  Alex picks up four slips at random, one after the other, without replacement.  Find the probability that the numbers on the slips, in the order he picks up, are in ascending order. My attempt: Sample space = picking up 4 slips one-by-one without replacement should be as good as picking 4 slips at a time which can be done in ${90 \choose 4}$ ways Now each of these equally likely ${90 \choose 4}$ outcomes will provide us 1 favourable outcome as well in which all four slips would be in ascending order, but then this would be simply $\frac{{90 \choose 4}*1 }{90 \choose 4}$ , however this isn't correct .  Please help me by stating the flaw in my argument and helping me with the correct thought process. Any approach without the involvement of combinatorics would also be appreciated.","['combinatorics', 'probability']"
4769349,Projections of zonohedra,"I noticed that any generic orthogonal projection of a cube has exactly two crossings. This led me to wonder about generalizations. A friend suggested I look at zonohedra. A zonohedron is the Minkowski sum of several line segments, called generators . This includes the cube, even-sided prisms, the truncated octahedron, and more. After playing around with some examples, we came across the following conjecture: Conjecture: Any (generic) orthogonal projection of a zonohedron with $n$ generators has $2(n-2)$ crossings. (By ""generic"" I mean to exclude the measure-zero set of projections for which two line segments coincide, resulting in a continuum of intersections.) Is this conjecture true? I have no idea how to prove it; visualizing rotating complicated three-dimensional objects makes my head hurt. It's entirely possible that we just missed a counterexample. We may also conjecture the converse: Conjecture: If all generic orthogonal projections of a polyhedron have the same number of crossings, it is a zonohedron. (Note that, combined with our first conjecture, this means that in this case the number of crossings must be even.)","['solid-geometry', 'projection', 'geometry']"
4769414,Find a multivariate polynomial over finite field with given zeros (or number of zeros),"I am trying to find polynomials $f \in \mathbb{F}_{q} [x_1, x_2, \dots, x_m]/(x_i^p-x_i)$ such that $f-1=0$ has precisely a given number of roots. For example, $f(x, y)$ in $\mathbb{F}_5[x, y]/((x_1^5-x_1)(x_2^5-x_2))$ with exactly $2$ roots. Is it always possible? Are there any upper and/or lower bounds that can tell when it is impossible? P.S. I only know of the bound by Schwartz-Zippel Lemma.","['finite-fields', 'algebraic-geometry', 'roots', 'multivariate-polynomial']"
4769417,Help with starting a proof regarding empirical distribution function of a uniform distribution,"Suppose $U_1,...,U_n$ is a simple sampling from a uniform distribution $U(0,1)$ and $G_n(u)$ is an empirical distribution function. Prove that $$
\begin{gathered}
n \int_0^1\left(G_n(s)-s\right)^2 d s=\frac{1}{12 n}+\sum_{k=1}^n\left(U_{(k)}-\frac{2 k-1}{2 n}\right)^2, \\
\int_0^1\left(G_n(s)-s\right)^2 d s \leq \frac{1}{3}
\end{gathered}
$$ Look, I don't even know where to start. This is a recent task, but I don't remember any integral-related theorems or properties, I am not very comfortable with the Law of Large Numbers (even if applicable here). I am, of course, not looking for a complete solution, but any hints at which direction to even look.","['integration', 'statistics', 'law-of-large-numbers', 'uniform-distribution']"
4769419,Representation of topological K-theory via Brown representability,"We know that topological K-theory is a generalized cohomology theory, and reduced K-theory is a reduced cohomology theory. Thus, both are representable with a sequence of pointed homotopy functors, via Brown's Representability Theorem.
In particular, i found the following statement; Let $X$ be a compact, Hausdorff topological space. Then, $$\tilde{K}(S^nX)=[X,U]_*$$ where $\tilde{K}$ is the reduced K-theory functor, $U$ is the direct limit of the unitary groups, and $S^nX$ denotes the (reduced) suspension of $X$ . I have a couple of questions: Why does not the functor on the right depend from $n$ ? I doubt that suspending a space as many times as I want I still get the same thing. Is this fact proved anywhere? I can't seem to find anything on the matter. This proposition is instrumental in getting from the K-theoretic form of Bott's Periodicity Theorem to the topological/homotopical one, and I need it for a seminar I'm giving. Thanks to everybody who will put some time and effort into answering me. EDIT: In D.Husemoller's ""Fibre Bundles"", p.34, I found this: There is an isomorphism of functors between $Vect_k(-)$ and $[-,G_k(\mathbb{C}^\infty)]$ ; here, $Vect_k(-)$ is the isomorphism classes of (in my case, complex) vector bundles, and $G_k(\mathbb{C}^\infty)$ is the Grassmann manifold of $k$ -subspaces of the direct limit $\mathbb{C}^n\subseteq \mathbb{C}^{n+1}$ . Is there some way I can get to what I want from here, maybe using the group-Grassmann-Stiefel fibre bundle, $$U\rightarrow G_k(\mathbb{C}^\infty)\rightarrow V_k(\mathbb{C}^\infty)$$ or some variation thereof?","['k-theory', 'homotopy-theory', 'topological-k-theory', 'general-topology', 'algebraic-topology']"
4769424,Girsanov Transformation for discrete Random walks,"currently I am trying to solve the following exercise/idea: Setup : Take $\Omega = \{ \omega \, \colon \mathbb{N}_0 \to \mathbb Z \mid \omega(0) = 0 \}$ the state space of all $\mathbb Z$ -valued random walks starting at $0$ together with a filtration of $(\Omega,\mathcal{F})$ given by $\mathcal{F}_n = \sigma(X_1,\dots,X_n)$ . Next consider the canocial projection given by $X_n(\omega) = \omega(n)$ and suppose that we are given a family of probability measures $P_p$ with $0 < p < 1$ such that the increments $\xi_n = X_n - X_{n-1}$ are i.i.d. $p$ -Rademacher distributed, i.e. $$
P_p(\xi_n = 1) = p \quad \text{ and } P_p(\xi_n = -1) = 1-p.
$$ In other words we restrict our case to the case of simple random walks on $\mathbb Z$ . Question : Now I want to find an explicit formula for the Radon-Nikodym derivative given by $$
M_n = \frac{P_{p}\vert_{\mathcal{F}_n}}{P_{1/2}\vert_{\mathcal{F}_n}}
$$ in terms of $n$ and $X_n$ . Using this I want to show that $M_n$ is a $P_{1/2}$ martingale with respect to $\mathcal{F}_{\mathbb{N}_0}$ . My attempt : Basically since we already know that $P_p$ only assigns positive mass to random walks that are simple it is enough to restrict $\omega \in \Omega$ where the $\xi_k \in \{-1,1\}$ (all the other walks have $0$ -probability). Let us start with the easy case: Easy case : Suppose that $X$ is a $1/2$ -Rademacher on $(\Omega,\mathcal{A},P_{1/2})$ and that $Y$ is $p$ -Rademacher for $0<p<1$ , which Radon-Nikodym derivative can we apply such that we change the law of $X$ to the law of $Y$ . Well if one takes $f \colon \{-1,1\} \to [0,\infty)$ given by $f(\omega) = 2p \, \delta_{\omega = 1} + 2(1-p) \, \delta_{\omega = -1}$ where $\delta$ denotes the dirac mass. Then observe that $$
P_p(X = 1) = \int_{\{1\}} f(\omega) dP_{1/2}(\omega) = p,
$$ and $P_p(X = -1)=1-p$ respectively, thus by definition we know that $f$ is the right Radon-Nikodym derivative. Simple Random Walks : Following the same logic and using that the increments are i.i.d. I believe that the Radon-Nikodym derivative should be given by $$
M_n = \prod^{n}_{k=1} 2p \delta_{X_k = 1} + 2(1-p) \delta_{X_k=-1}.
$$ Now using the same trick as above we should have $$
E_{1/2}[M_n \vert \mathcal{F}_{n-1}] = M_{n-1} E_{1/2}[\frac{dP_{p}}{dP_{1/2}}] = M_{n-1} E_p[1] = M_{n-1},
$$ and this finishes the proof however I am not 100% sure if this is correct.","['stochastic-processes', 'probability-theory', 'probability']"
4769432,Prove a space is contractible,"I encountered this problem while reading the proof of Lemma 1.2.4.17 in Jacob Lurie's Higher Algebra Recall that a topological simplex $|\Delta^n|$ can be identified with $\{0\leq x_1\leq\cdots\leq x_n\leq1\}\subset[0,1]^n$ Let $|X|\subset|\Delta^m|\times|\Delta^n|$ be the following space: $$|X|=\big\{(0\leq x_1\leq\cdots\leq x_m\leq1,0\leq y_1\leq\cdots\leq y_n\leq1):\{x_1,\cdots,x_m\}\subset\{0,y_1,\cdots,y_n,1\}\big\}$$ Equivalent description: $|X|=\bigcup_\alpha im(|\alpha|,id)$ , where $[n]=\{0<1<\cdots<n\}$ finite partially ordered set, $\alpha:[n]\to[m]$ runs through all order preserving maps, $(|\alpha|,id):|\Delta^n|\to|\Delta^m|\times|\Delta^n|$ the induced map. We need to prove that $|X|$ is a contractible space. Lurie claimed that it suffices to prove that each fiber of the projection $X\to|\Delta^m|$ is contractible, which was showed in HA Lemma 1.2.4.16. However, as can be seen in the case for small $m,n$ , the projection $X\to|\Delta^m|$ is never fibration. It seems not sufficient to just prove that every fiber is contractible. $\require{AMScd}$ The case $m=n=1$ : first coordinate for $\Delta^m$ , second coordinate for $\Delta^n$ : \begin{CD}&&(0,0)@>>>(0,1)\\&@VVV\\(1,0)@>>>(1,1)\end{CD} The case $m=1,n=2$ is also easy to draw, but I can't type it here use the poorly behaved “AMScd”. Both the above two graph can be firstly contracted to the diagonal segment $(0,0)\to(1,1)$ , and then contracted to a point. The case $m=n=2$ is already quite complicated, and I didn't find a good way to deal with it, let alone more general case. Update: (1) Let $X=\bigcup_\alpha im(\alpha,id)$ the underlying simplicial set, where $(\alpha,id):\Delta^n\to\Delta^m\times\Delta^n$ is defined as above. It suffices to show that $X\subset\Delta^m\times\Delta^n$ is anodyne. Perhaps there's a combinatorial proof? (2) We should assume in addition that $m\leq n$ For the case $m=2,n=1$ , $|X|$ is a 1-dim complex, and there is a nontrivial loop in $|X|$ , $(0,0)\to(1,1)$ , $(0,0)\to(1,2)$ , $(0,1)\to(1,1)$ , $(0,1)\to(1,2)$ \","['general-topology', 'higher-category-theory', 'algebraic-topology', 'simplicial-stuff']"
4769468,Group structure of $\mathrm{O}_n(R)$ over a **ring** $R$,"I have been looking at the orthogonal matrix group $$
\mathrm{O}_n(\mathbb{R}) := \{ M \in \mathbb{R}^{n \times n} : M^T M = I_n \}
$$ This group for $n \geq 2$ is infinite because e.g. $\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \in \mathrm{O}_2(\mathbb{R})$ can be embedded into higher dimensions. Since there are infinitely many Pythagorean triples $(x, y, z)$ , we also know $\frac{1}{z}\begin{pmatrix} x & -y \\ y & x \end{pmatrix} \in \mathrm{O}_2(\mathbb{Q})$ , so $\mathrm{O}_n(\mathbb{Q})$ is infinite. On the other hand, I know that the group over integers is finite: each column/row must be of norm $1$ , so they must contain a single nonzero element from $\pm 1$ . In fact, $\mathrm{O}_n(\mathbb{Z}) \cong S_n \wr \{-1, 1\}$ . My question is the generalisation of this $\mathrm{O}_n(\mathbb{Q})$ vs $\mathrm{O}_n(\mathbb{Z})$ comparison: For a number field $K = \mathbb{Q}(X)$ with ring of integers $\mathcal{O}_K$ , is the matrix group $\mathrm{O}_n(\mathcal{O}_K)$ finite? In either case, can we describe the group structure or even provide generators, probably depending on $K$ itself? I looked at $\mathrm{O}_2(\mathcal{O}_K)$ , which has the condition $$
\begin{pmatrix} a & b \\ c & d \end{pmatrix} \in \mathrm{O}_2(\mathcal{O}_K) \iff a^2 + c^2 = b^2 + d^2 = 1 \land ab + cd = 0
$$ However, I can't seem to get anything beyond a few special cases. This also doesn't seem to give any insight for $n \geq 2$ . Looking at the ""opposite"" direction, we can show that $\mathrm{O}_n(\mathcal{O}_K) \cong S_n \times \mathcal{O}_K^{\times}$ doesn't hold in general: Fix an element $a \in \mathcal{O}_K$ and consider $L = K(\sqrt{1 - a^2})$ . Then, $\begin{pmatrix} a & -\sqrt{1 - a^2} \\ \sqrt{1 - a^2} & a \end{pmatrix} \in \mathrm{O}_2(L)$ . (This problem came from implementing .is_finite() in Sage) Update: I found a slightly nontrivial result, which is that if $\sqrt{-k} \in R$ for any positive non-square integer $k$ , then $\mathrm{O}_n(\mathcal{O}_K)$ is infinite. The reason is that $$
\begin{pmatrix} a & b \\ -b & a \end{pmatrix} \in \mathrm{O}_n(\mathcal{O}_K) \iff a^2 + b^2 = 1
$$ And the Pell's equation $x^2 - ky^2 = 1$ has infinitely many solution, so $x^2 + (\sqrt{-k}y)^2 = 1$ ! In particular, for $m \geq 3$ and $K = \mathbb{Q}(\zeta_{2^m})$ , we have $\frac{1}{\sqrt{2}}\left(1 + i\right) \in \mathcal{O}_K \implies \sqrt{-2} \in \mathcal{O}_K$ .","['linear-algebra', 'commutative-algebra']"
4769484,Arbitrary decimal value of $A(n)=\left(\frac{11}{10}\right)^n$,"For $n\in\mathbb{Z}$ consider the number $$A(n)=\left(1+x\right)^n\bigg{|}_{x=\frac{1}{10}}=\sum_{k=0}^\infty\binom{n}{k}10^{-k}$$ which we have expanded by the Taylor series. It is found that $$a_1=\binom{n}{1},\,a_2=\binom{n}{2},\,\dots ,\,a_k=\binom{n}{k},\dots$$ which I thought gives the explicit formula for the $k$ th digit below the decimal point of any number with the form $$A(n)=\left(\frac{11}{10}\right)^n$$ but take for example $A(5)=(11/10)^5=1.\color{red}{6}{1}051$ and say we want to find the $1$ st digit below the decimal, highlighted red. We see $a_1={}_5C_{1}=5$ which isn't $6$ as $a_2={}_5C_2=10$ contributes $1$ to the next digit. My question is, is it possible to find the $k$ th decimal value of $A(n)$ by means of coefficient extraction from some generating function? Edit 1: To clarify, I am not looking for a generating function, but some explicit formula for the $k$ th digit below the decimal point of $A(n)$ . Now, using generating functions to find the formula is of course fine.","['generating-functions', 'binomial-coefficients', 'decimal-expansion', 'sequences-and-series']"
4769555,Chern and Pontryagin classes of manifold constructed as compactification of vector bundle,"I'm trying to compute the total Chern class of the tangent bundle of a manifold constructed as a ""compactification"" or ""projective completion"", but I'm not sure I quite have all the details under control. The manifold $M_{8}$ is eight-dimensional and complex, and constructed as a two-sphere bundle over a six-dimensional (Kahler) base space $B_{6}$ . I want to think of this bundle as the total space of the anti-canonical line bundle $\mathcal{L}$ of $B_{6}$ , together with a point added at infinity in each $\mathbb{C}$ fibre to make them into spheres. In practice, from here , I believe this means I can think of $M_{8}$ as the total space of $\mathbb{P}(\mathcal{O}\oplus\mathcal{L})$ , where $\mathbb{P}$ indicates the projectivisation and $\mathcal{O}$ is the trivial line bundle over $B_{6}$ . I then want to compute the total Chern class of $M_{8}$ in terms of the data of $B_{6}$ . As a start, following this , I have that, taking $V=\mathcal{O}\oplus\mathcal{L}$ and $\pi\colon\mathbb{P} V\to B_{6}$ , $$
c(T_{\mathcal{M}_{8}})=c(\pi^{*}(\mathcal{O}\oplus\mathcal{L})\otimes\gamma^{*})c(\pi^{*}T_{B_{6}}),
$$ where $\gamma$ is the tautological line bundle over $\mathbb{P} V=\mathcal{M}_{8}$ . Since $\mathcal{L}$ is the anti-canonical bundle of $B_{6}$ , I also know that $c_{1}(\mathcal{L})=c_{1}(T_{B_{6}})$ . The Whitney sum formula, etc., then gives \begin{align*}
c(T_{\mathcal{M}_{8}}) & =c(\pi^{*}\mathcal{O}\otimes\gamma^{*})c(\pi^{*}\mathcal{L}\otimes\gamma^{*})c(\pi^{*}T_{B_{6}})\\
 & =\left(1+\pi^{*}c_{1}(\mathcal{O})+c_{1}(\gamma^{*})\right)\left(1+\pi^{*}c_{1}(T_{B_{6}})+c_{1}(\gamma^{*})\right)\pi^{*}c(T_{B_{6}}),
\end{align*} which I can expand out to read off the Chern classes. At this point, my geometry runs out. I have two questions: Naively, I'd say that $\gamma^{*}=\pi^{*}\mathcal{O}(1)$ and would then need to know $\mathcal{L}$ in terms of $\mathcal{O}(n)$ in order to make progress. Does that sound correct, or is that too quick? From this , it seems that it might be more complicated... In fact, the end goal is to compute the Pontryagin classes for $\mathcal{M}_{8}$ . In particular, is it obvious that $p_{1}^{2}$ and $p_{2}$ are always trivial, since $\mathcal{M}_{8}$ is constructed as a bundle? (Since everything reduces to classes $B_{6}$ , there are no non-trivial eight-form classes constructed from the Chern classes?) Thank you in advance, and sorry if I've said something imprecise. I think this is fairly standard material, but I haven't come across it before. Edit: I think the point is to use the tautological exact sequence $$
0 \to \gamma \to \pi^* V \to T_{\mathbb{P}(V)/B_6}\otimes \gamma \to 0
$$ and then the observation that the Chern classes of $V$ satisfy (Bott & Tu, page 270) $$
x^n + \pi^* c_1(V) x^{n-1} + \dots + \pi^* c_n(V) =0
$$ where $x = c_1(\gamma^*)$ and $V$ is rank $n$ . For $V=\mathcal{O} \oplus \mathcal{L}$ , we then have $$
x^2 + \pi^* c_1(\mathcal{L})x=0
$$ so that $x = c_1(\gamma^*) = - \pi^* c_1(\mathcal{L})$ . (In particular, $\gamma^* \neq \pi^*\mathcal{O}(1)$ , but instead picks up a dependence on $\mathcal{L}$ .) With this in hand, one can use the above formula for $c(T_{\mathcal{M}_{8}})$ to compute the Chern classes.","['complex-geometry', 'fiber-bundles', 'algebraic-geometry', 'characteristic-classes']"
4769583,Is 𝐴 ∩ 𝒫(𝐴) = ∅ true for A = { 1 } ? Or is 𝐴 ∩ 𝒫(𝐴) = { 1 } ? (Set Theory),"Is 𝐴 ∩ 𝒫(𝐴) = ∅ true for A = { 1 } ? Or is 𝐴 ∩ 𝒫(𝐴) = { 1 } ? A = {1} P(A) = {0, {1}} Is 𝐴 ∩ 𝒫(𝐴) = ∅ or is 𝐴 ∩ 𝒫(𝐴) = {1}? My train of thought is that 𝐴 ∩ 𝒫(𝐴) = ∅ should be true as A contains the integer 1, while P(A) contains the set {1} which is different from the integer? I've looked at the existing answers on the internet but I keep getting mixed responses. An additional question is, is 𝐴 ∩ 𝒫(𝐴) always ∅ for all kinds of sets A?",['elementary-set-theory']
4769603,Bounding $\max e^{x_i}$ by $\sum e^{x_i}$ vs $e^{\sum (x_i)_+}$ when $x_i$ is gaussian,"I am trying to derive an upper bound on the expectation of the maximum of $n$ independent zero mean gaussian random variables: $$\mathbb E \left [ \max_i^n w_i\right ]$$ My thought is to use the moment generating function to form this bound. Proceding: $$= \mathbb E \left [ \frac{1}{\lambda} \log \exp \lambda \max w_i \right ]$$ where $\lambda \in \mathbb R$ . Now, applying Jensen's inequality: $$\leq \frac{1}{\lambda} \log \mathbb E \left [ \exp \lambda \max w_i \right ]$$ Since the max is difficult to handle directly, I consider a union bound, but I have two ways I can apply this union bound and it's not clear to me which one is the better choice. The first option (and what I originally tried): $$\stackrel{(A)}{\leq} \frac{1}{\lambda} \log \mathbb E \left [ \exp \lambda \sum (w_i)_+ \right ]$$ where $(\,\cdot\,)_+ := \max\{0, \cdot\,\}$ . Now I can apply independence and evaluate the expression by computing the moment generating function for $(w_i)_+$ : $$= \frac{n}{\lambda} \log \left ( \frac{1}{2} + e^{\sigma^2\lambda^2/2}\Phi(\sigma \lambda)\right )$$ where $\Phi$ is the normal CDF. The next step would be to select a value for $\lambda$ which makes this expression small. This is challenging for me, so if anyone has any tips that would be very helpful. I don't know how to proceed, but the leading $n$ term suggests that this bound is not good. The core of this is inequality $(A)$ , which I thought should be good when $w_i$ are typically small. But on second look $\sum (w_i)_+ \approx \sum |w_i| / 2 = O_p(\sqrt n)$ by the CLT which is certainly not small when $n$ is large. The second option is to apply the union bound outside the exponential: $$\stackrel{(B)}{\leq}\frac{1}{n} \log n \mathbb E [\exp\{\lambda w_i\}]$$ from where we can compute the moment generating function of $w_i$ and optimize $\lambda$ : $$=\sqrt{2\sigma^2 \log n}$$ This is the typical bound that is given. Interestingly, independence is not used and the same bound applies to sub-gaussian random variables in general, so I would expect this bound to be worse than the one obtainable from my first attempt. My questions How can I bound $\frac{n}{\lambda} \log \left ( \frac{1}{2} + e^{\sigma^2\lambda^2/2}\Phi(\sigma \lambda)\right )$ ? Is there some way to use the knowledge that the $w_i$ are independent and gaussian to obtain a better bound than $\sqrt{2\sigma^2 \log n}$ ?","['moment-generating-functions', 'statistics', 'probability', 'inequality']"
4769612,$A \subseteq B \cap C$ if and only if $A \subseteq B$ and $A \subseteq C$.,"Is this the right way to write the proof for the question? $A \subseteq B \cap C \implies A \subseteq B$ and $A \subseteq C$ . Let $x \in A$ . Since $A \subseteq B \cap C$ , $x \in B \cap C$ So, $x \in B$ and $x \in C$ Hence, proving the statement $A \subseteq B$ and $A \subseteq C \implies A \subseteq B \cap C$ Let $x \in A$ . Since $A \subseteq B$ , $x \in B$ Also since $A \subseteq C$ , $x \in C$ Hence, proving the statement.","['elementary-set-theory', 'solution-verification', 'discrete-mathematics']"
4769702,"Can a sufficiently large spanning set represent every vector, using fewer than n vectors?","Let $V$ be a $n$ -dimensional vector space and suppose we have a set $A$ of vectors which span $V$ . Let $q$ be the smallest number such that for all $x \in V$ , we can always write $x$ as a linear combination of $q$ vectors in $A$ . What is the relationship between $|A|$ and $q$ ? For example, here are the two examples: By basic linear algebra, we can have $|A| = n$ when $q = n$ . Let $\mathbb{F} = \mathbb{Z}/p\mathbb{Z}$ be a finite field and $V = \mathbb{F}^n$ . For any $q$ , I can construct $A$ with size $|A| = p^{n-q} + q - 1$ . Here it is: $$ A = \{(1, 0, \dots, 0, x_{q+1}, \dots, x_n) \mid x_{q+1}, \dots, x_n \in \mathbb{F}\} \cup \{e_2, \dots, e_q\},$$ where $e_2, \dots, e_q$ are standard basis vectors. This works because for any target $y = (y_1, \dots, y_n)$ , we have $$ y = y_1(1, 0, \dots, 0, y_{q+1}y_1^{-1}, \dots, y_ny_1^{-1}) + y_2e_2 + \dots + y_qe_q.$$ I have no clue if a smaller set $A$ exists. (edit: may not work when $y_1 = 0$ , thank you @jackson for pointing it out) An easier question might be: Can the set $A$ be finite if we desire $q < n$ , when $|\mathbb{F}| = \infty$ ? A possibly harder question that I am also interested in is: What if we have $\mathbb{Z}$ instead of a field?","['abstract-algebra', 'linear-algebra']"
4769719,Functional equation $f\left( x^{2}\right) =2f\left( x\right)$and the Cauchy problem $f\left( xy\right) =f\left( x\right) +f\left( y\right)$,"Is it possible to show that if a continuous function $f:\mathbb{R} _{+}\rightarrow \mathbb{R}$ verifies the
equation $$f\left( x^{2}\right) =2f\left( x\right)$$ then $f$ also satisfies the Cauchy equation $$f\left( xy\right) =f\left( x\right) +f\left( y\right)$$ WITHOUT SOLVING THEM?
( Otherwise, given that $f\left( x\right) =k\cdot \log _{a}x$ , the question becomes trivial....) Thanks for some suggestions...",['analysis']
4769785,"Are there any efficient methods, shortcuts, theorems, or hacks to determine whether the graph is chordal for cases where it's sparse?","I'm working with a graph that has 20 vertices or nodes. This graph is relatively sparse, with an average of only 2 edges per node (or less). Are there any efficient methods, shortcuts, theorems, or hacks to determine whether the graph is chordal for cases where it's sparse (e.g., when the number of edges is much less than the number of node)? More specifically, my problem is the following: I have an algorthim that starts at an empty graph G0 (see above picture). The rules is this: For every non zero entries in the mask matrix m0,  you have to check if the graph is chordal at these entries. e.g. entries (a,b) is zero and so. you have to check if adding an undirected edge between node a and b will be chordal. Thus, you have to check if the matrix m0(a,b) is chordal. You also have to do the same for the other non zero entries(e.g. check if m0(a,c), m0(a,d) etc are chordal). 2.Every zero entries in the mask matrix (e.g m0) are the possible ways (e.g. actions) you can add ONE undirected edges to G0.  At state G0, you have to choose only one of these possible actions to traverse to the next graph e.g. G1.
For example, m0[a,b]=0. This zero entry at [a,b]represents adding undirected edge between node a and node b in G0. Similarly, m0(a,c) represents adding undirected edge between node a and c in G0 etc.
My problem is for each graph Gi,  I have to check whether m0 at every non zero entries is chordal. This gets computationally expensive when the mask matrix is large is a 2500x2500 matrix, and I have to do this for many matrices. So i am trying to find a hack or theorem so that i don't have to check if a graph is chordal at every non zero entries of a mask matrix.","['graph-theory', 'discrete-mathematics']"
4769795,Finding the limit of a radical function without conjugates,"I am a first year differential calculus student and I was solving a problem where I had to find $$\lim_{x\to \infty}(\sqrt{x^2-5x+1}-x)$$ I have been solving these types of problems by factoring out the highest degree x, like this: \begin{align}
\lim_{x\to \infty} 
\sqrt{x^2\biggl(1-{\frac5x}+{\frac1{x^2} \biggr)}}-x 
&= \lim_{x\to \infty} x 
\biggl(\sqrt{(1-{\frac5x}+{\frac1{x^2}}}-1\biggr) \\ 
&= (\infty) 
\biggl(\sqrt{(1-{\frac5{\infty}}+{\frac1{\infty^2}}}-1\biggr) \\ 
&= (\infty) \bigl(\sqrt{1} -1) \\ 
&= 0 
\end{align} But desmos shows that the right answer is found by multiplying the numerator and denominator by the conjugate to get $$\lim_{x\to \infty} \frac{(-5x +1)}{(\sqrt{x^2-5x+1}+x)} = -2.5$$ My question is why do we need to multiply by the conjugate here to get the right answer, whereas just factoring works for similar limits like $$\lim_{x\to \infty} \frac{\sqrt{x^2+7x}}{11-2x} $$ If the original expression is algebraically the same after multiplying the numerator and denominator by the conjugate, how could I have solved the limit without involving conjugates? Thanks in advance.","['limits', 'calculus']"
4769804,"Verify that the function $f(x) \geq g(x)$, where $f(x) = \cos (x)$ and $g(x) = 1 - \frac{x^2}{2}$ for $x \in (-\pi/2, \pi/2)$.","I am verifying whether the function $f(x) \geq g(x)$ , where $f(x) = \cos (x)$ and $g(x) = 1 - \frac{x^2}{2}$ for $x \in (-\pi/2, \pi/2)$ . Also I am checking whether $f(x) - g(x)$ changes sign in $(-\pi/2, \pi/2) $ or not? My attempt: Since $-1 \leq \cos x \leq 1$ . Now integrating this inequality two times between 0 to $x$ proves desired result for all $x$ . Thus $f(x) - g(x)$ will never changes sign as its positive function. Am I on the right path? I asked this question because I want to explore alternative approaches for solving it if  my current method is correct. Thank you so much for your assistance","['functions', 'real-analysis']"
4769807,"When can $L$ sets of the form $\{a,b,a+b\}$ partition $\{1,2,\dots, 3L\}$?","Now also posted to MathOverflow . Consider a set of the form $\{a,b,a+b\}$ where $a$ and $b$ are positive integers with $b > a$ . I will refer to such a set as a triplet . Consider now the problem of constructing $L$ disjoint triplets with as small a maximum element across them as we can manage. I will refer to a collection of $L$ disjoint triplets having the smallest maximum element across them as optimal . At best, the triplets will partition $\{1,2,\dots,3L\}$ giving a lower bound of $3L$ on the maximum element. Here are some examples: For $L = 1$ , we have $\{1,2,3\}$ achieving the lower bound of $3 = 3L$ . For $L=2$ , we have $\{1,3,4\}$ , $\{2,5,7\}$ achieving $7 = 3L+1$ . It appears that this is optimal for $L=2$ . For $L=3$ , we have $\{1,4,5\}$ , $\{2,6,8\}$ , $\{3,7,10\}$ achieving $10 = 3L+1$ . For $L=4$ , we have $\{1,8,9\}$ , $\{2,10,12\}$ , $\{3,4,7\}$ , $\{5,6,11\}$ achieving the lower bound of $12 = 3L$ . By random computer search, I was able to verify the existence of constructions with maximum element $3L+1$ for $L = 5,6,7$ . I am broadly interested in anything that would help with understanding this problem. Can I always achieve $3L$ or $3L + 1$ ? If not, is there a good upper bound on the maximum element across $L$ disjoint triplets? Ideally, I would hope that there is an explicit optimal construction hiding in the examples I've listed but I can't quite see the pattern.","['set-partition', 'extremal-combinatorics', 'combinatorics', 'discrete-optimization', 'difference-sets']"
4769816,Distributing marbles into buckets for maximal colour sharing,"i've got a problem that feels very much like it's NP-hard but I would love some help proving it primarily. Secondary to that, if an optimal polynomal time algorithm can be proposed that is even better, although even good ""greedy"" approximations are fine. Given $M$ marbles, $B$ buckets and each bucket having capacity $K$ , where $M = BK$ , find some distribution of the marbles that maximizes the number of colours that are shared. colour $A$ and colour $B$ are considered to be shared if they appear together in the same bucket. The distribution of colours is known for a given problem, and can be sorted and added in any order you desire. We can define a distribution as follows: $m_1 + m_2 + m_3 + \dots + m_n = M = BK$ where $n$ is the the number of colours. We also have $0 < m_k \le M = BK$ for some $k$ th colour Example: BucketId Colours $0$ $A, C$ $1$ $A, C$ $2$ $B, D$ $3$ $B, D$ $M = 8, B = 4, K = 2
\\m_1 = 2, m_2 = 2, m_3 = 2, m_4 = 2$ Clearly, this is non-optimal since we may swap $C$ in bucket 1, with $D$ in bucket 2 and achieve a better outcome (more colours shared). Any insight into this problem would be infinitely appreciated, Thanks :)","['graph-theory', 'optimization', 'combinatorics', 'np-complete']"
4769912,Another characterization of compactness,"Compactness is a central, but complex notion in analysis.
I'm finding more concise definition of compact spaces. The one I easily found is the definition by 'finite intersection property', but it doesn't quite make the situation simpler. As it is seen in nlab , there is (seemingly) very concise definition of compact space, by compact object in category theory. Directly unwinding the definition, it looks the most complicated: $X$ is compact if and only if the functor $$\mathrm{hom}(X,-)$$ preserves filtered colimits in the category of open sets $\mathrm{Op}(X)$ . If we look closer, since the category $\mathrm{Op}(X)$ is merely a poset, we can translate it as: For any filtered family of open sets $\left\{ X_i\right \}$ , $X\subseteq \bigcup X_i$ if and only if $X \subseteq X_i$ for some $i$ . Which is in turn, a definition of compact object in a poset (if your familiar with it). Again, the notion of 'filteredness' is quite involved. So my aim is to lessen the condition. The definition of 'compact object' simply means that there are no nets of proper open subsets of $X$ that unions to $X$ . I want to make the net merely to be a chain. Here is my reasoning: If a space $X$ has a chain of proper open subsets that unions to $X$ , it's clearly not compact. Conversely, if $X$ is not compact, we have an open cover $\left\{U_i\right\} $ of $X$ not admitting a finite open subcover. (With Axiom of Choice) we can well-order $\left\{U_i\right\}$ and we'll get a sequence of open subsets $\left\{U_\alpha\right\}_{\alpha<\beta}$ . First, just naively construct the increasing sequence $V_\alpha=\bigcup_{\gamma<\alpha} U_\gamma$ , which unions to $X$ . Suppose the sequence stabilizes from the $\beta_0$ th step ( $\beta_0$ can be chosen to be the least one). Then $\beta_0$ should be infinite. If $\beta_0$ is limit, $\left\{V_\alpha\right\}_{\alpha<\beta_0}$ is the desired sequence. Otherwise, $\beta_0=\beta_0'+1$ , and $V_{\beta_0'}$ should be $X$ as $V_\alpha$ is increasing. We now move $U_{\beta_0'}$ to the very first of the sequence $\left\{U_\alpha\right\}$ and still we get a well-ordered sequence. Now, retrying all the process, we get a $\beta_1$ , which is the smallest ordinal $\beta$ among $\bigcup_{\alpha<\beta} U_\alpha=X$ . $\beta_1$ is strictly smaller than $\beta_0$ , since $\bigcup_{\alpha<\beta_0'} U_\alpha = X$ (with the new sequence $\{U_\alpha\}$ . Repeating this process, we can do an infinite descent of ordinals, so it should end with a limit ordinal, getting the desired chain. Is my reasoning OK? and can I find such a definition in a Literature?","['general-topology', 'category-theory']"
4769920,Constant on path-connected components implies constant,"Suppose $X \subset \mathbb{R}^n$ is connected. Let $f:X\longrightarrow \mathbb{R}^n$ be a continuous function. Prove that if $f$ is constant on the path-connected components of $X$ , then $f$ is constant on $X$ . I was able to prove this in the case where $X$ has a finite number of path-connected components. In fact, we can proceed by induction. Starting with a path-connected component, its closure must intersect the closure of another path-connected component; by continuity the constant on these must agree. I have no idea how to handle the general case, any hint is appreciated. The case where $f$ is uniformly continuous (or Holder) is of interest too. In that case, we can reduce the problem to a compact $X$ , if that is of help. I wonder if the cardinality of path-connected components of a (compact) connected subset of $\mathbb{R}^n$ must be countable. If that was the case, maybe an induction could be useful. Thanks!","['metric-spaces', 'analysis', 'real-analysis', 'continuity', 'general-topology']"
4770047,A little bit nasty double integral,"I have difficulty calculating the following double integral: $$\int_0^1\int_0^1\sqrt{x^2-2mxy+y^2+h^2}\text{d}x\text{d}y$$ where $m\in[-1,1],h\in(0,\infty)$ are constants. What I've tried with the help of others: \begin{align*}
I&=2\int_0^{\frac{\pi}{4}}\int_0^{\sec\theta}r\cdot\sqrt{(1-2m\sin\theta\cos\theta)r^2+h^2}\text{d}r\text{d}\theta
\end{align*} let $a=1-2m\sin\theta\cos\theta$ , then the inner part looks like this： $$\int r\cdot\sqrt{ar^2+h^2}\text{d}r$$ let $u=ar^2+h^2$ , then the inner part should be: $$\frac1{2a}\int\sqrt{u}\text{d}u=\frac1{3a}u^{\frac32}=\frac1{3a}(ar^2+h^2)^{\frac32}$$ Hence, \begin{align*}
\int_0^{\sec\theta}r\sqrt{ar^2+h^2}\text{d}r
&=\frac1{3a}[(a\sec^2\theta+h^2)^\frac32-h^3 ]\\
&=\frac{(\tan^2\theta-2m\tan\theta+h^2+1)^\frac32-h^3}{3-6m\sin\theta\cos\theta}
\end{align*} So, $$\frac23\int_0^\frac{\pi}{4}\frac{(\tan^2\theta-2m\tan\theta+h^2+1)^\frac32-h^3}{1-2m\sin\theta\cos\theta}\text{d}\theta$$ But I have no idea how to deal with: $$\int_0^{\frac{\pi}{4}}\frac{(\tan^2\theta-2m\tan\theta+h^2+1)^{\frac32}}{1-2m\sin\theta\cos\theta}\text{d}\theta$$ Update: Thanks to @sreysus, we change it to $$\int_0^{\frac{\pi}{4}}\frac{\sqrt{1-2m\sin\theta\cos\theta+h^2}}{\cos^3\theta}\text{d}\theta+\int_0^{\frac{\pi}{4}}\frac{h^2\sqrt{1-2m\sin\theta\cos\theta+h^2}}{\cos\theta-2m\sin\theta\cos\theta}\text{d}\theta$$ What I've tried: \begin{align*}
\int_0^{\frac{\pi}{4}}\frac{\sqrt{1-2m\sin\theta\cos\theta+h^2}}{\cos^3\theta}\text{d}\theta
&=\int_0^{\frac{\pi}{4}}\frac{\sqrt{1-2m\sin\theta\cos\theta+h^2}}{\cos\theta}\text{d}\tan\theta\\
&=\int_0^{\frac{\pi}{4}}\sqrt{(1+h^2)\sec^2\theta-2m\tan\theta}\text{d}\tan\theta\\
&=\int_0^1\sqrt{(1+h^2)(1+u^2)-2mu}\text{d}u
\end{align*} Though it's complicated, I've worked this part out (too complicated to put it here) So I got stuck how to evaluate this part: $$\int_0^{\frac{\pi}{4}}\frac{h^2\sqrt{1-2m\sin\theta\cos\theta+h^2}}{\cos\theta-2m\sin\theta\cos^2\theta}\text{d}\theta$$ where $m\in[-1,1],h\in(0,+\infty)$ are constants. Update: I'm now wondering how to evaluate integral in the form of $$\int\frac{\sqrt{x^2+p}}{x^2+q}\text{d}x$$ Thanks everyone, I've solved it.","['integration', 'calculus', 'trigonometric-integrals']"
4770068,Does pairwise phase incoherence satisfy the triangle inequality?,"Let $S$ be the unit circle in the complex plane, $$ S = \{z \in \mathbb{C} : |z| = 1\}. $$ For values $z_1^{(1)},z_1^{(2)},z_2^{(1)},z_2^{(2)},\ldots,z_k^{(1)},z_k^{(2)} \in S$ , letting \begin{align*}
 A &:= \left|\sum_{j=1}^k z_j^{(1)}z_j^{(2)}\right| \\
 B_1 &:= \left|\sum_{j=1}^k z_j^{(1)}\right| \\
 B_2 &:= \left|\sum_{j=1}^k z_j^{(2)}\right|,
 \end{align*} do we necessarily have that \begin{equation} \hspace{30mm} A \geq B_1 + B_2 - k \, ? \hspace{30mm} (1) \end{equation} If not, do we necessarily at least have that \begin{equation} \hspace{30mm} A^2 \geq B_1^2 + B_2^2 - k^2 \, ? \hspace{26mm} (2) \end{equation} Note that (1) does indeed imply (2): the triangle inequality gives that $B_1$ and $B_2$ are each at most $k$ , and hence $$ [\text{RHS of (1)}]^2 - \text{RHS of (2)} \ = \ 2(k-B_1)(k-B_2) \ \geq \ 0. $$ Context: For multivariate data where each coordinate is an angle, it is common to measure the coherence (resp. squared coherence ) between two of the coordinates across the data set, by taking the magnitude (resp. squared magnitude) of the mean of the complex exponential of the difference between the two coordinates. The question is then essentially whether one minus this value defines a metric on the set of coordinates (with Eq. (1) being for coherence and Eq. (2) being for squared coherence). This is a highly relevant question both for phase-coherence of stationary stochastic processes as defined in https://en.wikipedia.org/wiki/Coherence_(signal_processing) , and for various notions of time-localised phase-coherence such as in Appendix B.1 of https://www.sciencedirect.com/science/article/pii/S1063520320300750 .","['statistics', 'time-series', 'inequality', 'signal-processing', 'complex-numbers']"
4770077,How do I find the value of the summation: $\dfrac{1}{1^2}+\dfrac{1}{7^2}+\dfrac{1}{9^2}+\dfrac{1}{15^2}+\dfrac{1}{17^2}+\cdots$,"I used a flawed approach to generalizing Basel-like problems, Please refer: https://math.stackexchange.com/q/4769685 I assumed $\cos=0$ (for which the roots are $\dfrac{\pi}{2},\dfrac{-\pi}{2},\dfrac{3\pi}{2},\cdots$ $a_0$ becomes 1) for deriving the Basel problem in the cited answer, if instead I used $\cos=\dfrac{1}{\sqrt{2}}$ (for which the roots are $\dfrac{\pi}{4},\dfrac{9\pi}{4},\dfrac{-7\pi}{4},\cdots$ $a_0$ becomes $1-\dfrac{1}{\sqrt{2}}$ ) the resulting summation would be: $\dfrac{1}{1^2}+\dfrac{1}{7^2}+\dfrac{1}{9^2}+\dfrac{1}{15^2}+\dfrac{1}{17^2}+\cdots$ Which results in the value $$\dfrac{\pi^2\cdot(\sqrt{2}(\sqrt{2}+1))}{32}=\dfrac{1}{1^2}+\dfrac{1}{7^2}+\dfrac{1}{9^2}+\dfrac{1}{15^2}+\dfrac{1}{17^2}+\cdots$$ These results can be seemingly generalized as (Where $X_e=\dfrac{\pi}{n}$ ): $$\dfrac{X_e^2}{2}(\csc^2(X_e)(1+\cos(X_e)))=\dfrac{\pi^2}{2\cdot n^2}(\csc^2(\dfrac{\pi}{n})(1+\cos(\dfrac{\pi}{n})))=\dfrac{1}{1^2}+\dfrac{1}{(2n-1)^2}+\dfrac{1}{(2n+1)^2}+\dfrac{1}{(1-4n)^2}+\dfrac{1}{(1+4n)^2}+\cdots$$ (For n>1) However I acknowledge that the method I used is fundamentally flawed. Is there an alternate derivation for finding the value of the summation and by extension the observed generalization? If yes, has this generalization been found before or is it wrong?","['trigonometry', 'sequences-and-series']"
4770113,Prove that the expression is identically zero.,"I would like a proof that the following expression \begin{align}
\sum_{i=1}^N\sum_{j=1\\
j\ne i}^N\frac{A_iA_j(A_i+A_j)}{(A_i-A_j)^3}\prod_{k=1\\
k\ne i\\
k\ne j}^N\frac{A_i A_k}{(A_i-A_k)^2}
\end{align} is zero for all $N\in \mathbb Z_{+}$ . I have checked that it is zero for $N$ up to 7. I obtained this expression at the next-to-leading order while expanding the Nekrasov partition function of an $\textsf{N}=2$ theory in the $\Omega$ -background. (Now crossposted at MathOverflow )","['combinatorics', 'closed-form', 'mathematical-physics']"
4770118,A person's quickest path between any two points on perimeter of elliptical lake never involves both swimming and running. Find maximum eccentricity.,"I made up this question. A police officer's job is to patrol the perimeter of an elliptical lake. They have a constant (unknown) swimming speed and a constant (unknown) running speed. Their quickest path between any two points on the perimeter never involves both swimming and running. What is the maximum value of $e$ , the eccentricity of the ellipse? Proof that $e$ has an upper bound If $e\approx 1$ then the lake looks almost like a straight river. Consider a point on one side, and another point on the other side further ""downstream"". Clearly, the quickest path between the points could involve swimming and running. We can prove that if $e=0$ (circle) then the quickest path never involves swimming and running. Suppose $e=0$ and the quickest path between two points involves swimming and running, i.e. the path includes a chord and arc that share an endpoint. Let $\alpha=$ angle subtended by the chord ( $0<\alpha<\pi$ ), and $\theta=$ angle subtended by the chord-arc pair. Assuming the radius of the circle is $1$ , the time for this path is $T=\dfrac{2\sin (\alpha/2)}{v_{\text{swim}}}+\dfrac{\theta-\alpha}{v_{\text{run}}}$ $\dfrac{d^2 T}{d\alpha^2}=-\dfrac{\sin (\alpha/2)}{2v_{\text{swim}}}<0$ So the minimum value of $T$ occurs at one of the endpoints, i.e. $\alpha=0$ or $\alpha=\theta$ , which implies that the path involves either only running or only swimming, contradiction. Therefore if $e=0$ then the quickest path never involves swimming and running. So $e$ must have an upper bound.","['arc-length', 'conic-sections', 'geometry', 'calculus', 'optimization']"
4770211,"implement a fair 0-1 random variable using a biased coin with head p, while minimizing the expected number of coin tosses to $1/H(p)$","I recently encountered an intriguing question during a mathematical interview: How can one construct a fair binary random variable that outputs either $0$ or $1$ using a biased coin with a head-up probability of $ p $ , while minimizing the expected number of coin tosses? I am aware of the conventional method for simulating a fair coin from a biased one. This method involves the following steps: Toss the biased coin twice. If the outcomes are both heads (HH) or both tails (TT), disregard the results and start anew. If the outcomes are heads-tails (HT), output $0$ . If the outcomes are tails-heads (TH), output $1$ . With this approach, the expected number of coin tosses $ n $ can be calculated as: \begin{align*}
    n &= 2 + (p^2 + (1 - p)^2) n \\
    &\Rightarrow n = \frac{2}{2p(1 - p)}
\end{align*} However, the interviewer raised a more challenging aspect: devise and implement a strategy such that the expected number of coin tosses approximates the optimal $ \frac{1}{H(p)} $ , where $ H(p) = -p \log_2(p) - (1 - p) \log_2(1 - p) $ represents the entropy? I suspect that this is a systematically solvable problem. Are there any published works that delve into this issue?","['information-theory', 'stochastic-processes', 'discrete-mathematics', 'optimization', 'probability']"
4770257,Find angle in a triangle given angle bisector and altitude without trigonometry.,"In the triangle $ABC$ , $BM$ is altitude and $E$ is in that segment such that $CE$ is angle bisector. Also, the angle $EAM = 30º$ and the angle $MCB = 20º$ . Find the value of $ABM$ . My problem with this exercise is that it has to be done without trigonometry. I did it with trigonometry and I got that the answer is $40º$ (if i am not mistaken). Any hints are appreciated.","['triangles', 'angle', 'geometry']"
4770289,"Intuition for why $\int_0^\infty\frac{1-(1+x^2)^{-a/2}}{x^2}\,dx$ should equal $\int_0^\infty \frac{1-|\cos x|^a}{x^2} \, dx$?","Let $a>0$ . One can calculate the following integral identities: $$\int_0^\infty \frac{1-\frac{1}{(1+x^2)^{a/2}}}{x^2} \, dx = \sqrt{\pi}\frac{\Gamma(\frac{a}{2}+\frac12)}{\Gamma(\frac{a}{2})}$$ $$\int_0^\infty \frac{1-|\cos x|^a}{x^2} \, dx = \sqrt{\pi}\frac{\Gamma(\frac{a}{2}+\frac12)}{\Gamma(\frac{a}{2})}$$ Miraculously, these two integrals give the same result. Does anyone have any high-level intuitive explanation for why this happens?","['integration', 'definite-integrals']"
4770453,What is the operation between a bivector and a vector that outputs another vector?,"I'll start with a physical example. Let's say we have the angular velocity (in 3D euclidean space with orthonormal basis spanning it), which has the bivector representation $$\Omega = \omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy}$$ for unit bivectors $\mathbf e_{yz}, \mathbf e_{xz}, \mathbf e_{xy}$ , and the skew symmetric matrix representation $$\Omega =
\begin{bmatrix}
0 & -\omega_z & \omega_y \\
\omega_z & 0 & -\omega_x \\
-\omega_y & \omega_x & 0 
\end{bmatrix}$$ The hodge dual of this bivector is the axial vector $$\star\Omega = \boldsymbol\omega = \omega_x\mathbf e_x  + \omega _y \mathbf e_{y} + \omega_z \mathbf e_{z}$$ When we calculate the velocity of a particle with a position vector $\mathbf r = r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z}$ and angular velocity $\Omega$ , then the velocity of the particle is said to be given by $$\mathbf v = \Omega \mathbf r \iff \mathbf v = \boldsymbol\omega\times \mathbf r$$ What I don't understand is what the operation between $\Omega$ and $\mathbf r$ is. I understand that the hodge dual of a wedge product is a cross product, which gives us the relationship that $$\text{vector}\wedge\text{vector} = \text{bivector} \iff \text{vector}\times \text{vector}=\text{axial vector}$$ However, there is also the relationship that $$\text{vector}\times \text{axial vector} = \text{vector}$$ which seems to be the relationship here. Now, if we replace the axial vector with the actual bivector (as seen in the angular velocity example above), what is the operation between the vector and the bivector??? $$ \text{bivector } ??? \text{ vector} = \text{vector}$$ Obviously, when we write the bivector as a skew symmetric matrix, we're just performing regular matrix multiplication. However, if we're writing it as just the sum of unit bivectors, what is this operation?? $$\langle\omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy}\rangle \text{ ??? } \langle r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z} \rangle$$ I initially thought it might be a wedge product, but quickly realized that it would just be wedging a bivector and a vector which returns a trivector, not a regular vector that I want. I can't think of any other operation that would return a vector between a bivector and a vector. Any help? Thanks!","['matrices', 'cross-product', 'vectors', 'exterior-algebra']"
