question_id,title,body,tags
4460432,Least Prime in Arithmetic Progression Conjecture,"Let $q>2$ be a prime, and let $p(a,q)$ denote the least prime in arithmetic progression $$a+nq,$$ where $n$ runs through the positive integers. It is conjectured that $p(1,q)\le q(q+1)+1$ . Research suggests that the above statement is related to Linnik's theorem, albeit a special case. How would one go about determining if this conjecture is valid/interesting or not and where would one start to prove this true or false and would proving something like this in any way help to decrease the bound of $L$ in Linnik's Theorem? Background: The Pocklington Criterion can be used to prove the primality of an integer $N$ , given a partial factorization of $N-1$ , specifically given that $N-1$ has a prime factor $q>\sqrt{N}-1$ . For a given odd prime $q$ , the Pocklington Criterion can be used to test exactly $\frac{q+1}{2}$ integers for primality, $1+nq$ , where $n=2, 4, ..., q+1$ . For the special case $q=2$ , $n=1,2,3$ can be tested for primality, which is $3$ , $5$ and $7$ . As an example, take $q=7$ , the following $4$ integers can be tested: $1+2(7)=15$ $1+4(7)=29$ $1+6(7)=43$ $1+8(7)=57$ From this list it is easy to see that $29$ and $43$ are primes and the Pocklington Criterion will confirm this fact. Odd $n$ terms are omitted as they are all divisible by $2$ . What we can now do is use these two primes and generate other primes. What I am interested in, and where the question comes from, is whether this process continues ad infinitum or not. Can we generate an infinite number of primes given a starting prime $q$ , or stated differently, can the Pocklington Criterion be used to generate at least one prime given a starting prime $q$ . The other part of the question is whether this is in any way useful. We already know, according to Dirichlet's Theorem on Arithmetic Progressions, that there are an infinite number of primes of the form $1+nq$ . Linnik's Theorem has to do with what the smallest prime in an Arithmetic Progression is. Research Update: There are a couple of theorems and conjectures about what the smallest prime in an Arithmetic Progression is, and these are mostly based on Linnik's Theorem. It is conjectured that $p(a,d)<d^2$ . Heath-Brown, Roger (1992). ""Zero-free regions for Dirichlet L-functions, and the least prime in an arithmetic progression"". This seems to be the smallest conjectured bound. If the above conjecture is true, then the conjecture that $p(1,q)\le q(q+1)+1$ is also true, and not necessarily useful.
So, the conjecture in the question is not unique. What seems to be unique is the very specific bound $p(1,q)\le q(q+1)+1$ . The other part that seems to be unique is the thinking that led to the conjecture. Research Update: Referencing the answer below, $q_1q_2+1=r$ , where $r$ is the smallest value such that $r=1+tq_1=1+sq_2$ , with $q_2>q_1$ . This is only true if $gcd(q_1,q_2)=1$ . So $q_1(q_1+1)+1$ is just the largest value of the arithmetic progression $1+tq_1$ that is less than the smallest possible value of $r$ , which we get when $q_2-q_1=2$ . So the bound in the question does not necessarily have anything to do with the smallest prime in arithmetic progression, but it is an interesting boundary nonetheless.","['number-theory', 'arithmetic-progressions', 'prime-numbers']"
4460439,"Prove that for every sufficiently large $n$, we can write the positive integer $n$ in the form $n = a_1+ a_2+...+a_{2018}$ with $: (a_i , a_j ) = 1 $","Prove that for every sufficiently large $n$ , we can write the positive integer $n$ in the form $n = a_1+ a_2+...+a_{2018}$ In there $: (a_i , a_j ) = 1 $ for all $i<j$ and $a_i>2018$ for all $i$ Instead of trying with $2018 ,$ I tried with $ 4 , 5 ,6 .$ And as a result, I get the prediction that: for every large enough $ n,$ we can construct $n$ as the sum of $2017$ primes and some number. (I'm pretty sure this prediction is correct since I tested over $100$ numbers.) But I have not proven my prediction yet. I hope to get help from everyone. Thanks very much !","['number-theory', 'combinatorics', 'elementary-number-theory', 'prime-numbers']"
4460590,"Mistake in ""addition rule"" in probability theory. [duplicate]","This question already has answers here : What is the probability that randomly selected $2$ pets from this vet are cat or sick? Why doesnt addition rule for probability work? (2 answers) Closed 2 years ago . Theorem : Let $E_1$ and $E_2$ be events in sample space S. Then , $P(E_1 \cup E_2) =P(E_1)+P(E_2)-P(E_1 \cap E_2)$ Question: What is the probability that a positive integer selected at random from the set of positive integers not exceeding $100$ is divisible by either $2$ or $5$ ? First way : Select one element from the set which satisfies ""divisible by either $2$ or $5$ "". There are $60$ elemens which divisible by either $2$ or $5$ in that set ,so $C(60,1)/(100,1)=3/5$ Second way: Use the theorem such that $P(E_1 \cup E_2) =P(E_1)+P(E_2)-P(E_1 \cap E_2)= (50/100)+(20/100)-(10/100)=60/100$ where $E_1$ is the elements divisible by $2$ , $E_2$ is the elements divisible by $5$ . The both ways work correctly.No problem ! However, when we want to select two or more elements , the second way is incorrect.Let solve it for selecting two elements: First way : Select one element from the set which satisfies ""divisible by either $2$ or $5$ "". There are $60$ elemens in that set ,so $C(60,2)/(100,2)=1770/C(100,2)$ Second way: Use the theorem such that $P(E_1 \cup E_2) =P(E_1)+P(E_2)-P(E_1 \cap E_2)= [C(50,2)/C(100,2)]+[C(20,2)/C(100,2)]-[C(10,2)/C(100,2)] =1370/ C(100,2)$ When we select only one element , the theorem gives correct answer , but when we select more than once ,the theorem  misses some situations. Why does it happen ,i.e why the second case is missing some elements? If the theorem may miss some cases , is it unreliable in some cases. If so , which cases are unreliable ? Should we use the theorem only the cases where only one element is selected ? Thanks in advance !!","['discrete-mathematics', 'combinatorics', 'probability-theory', 'probability']"
4460612,How does one define a well-ordering for the set of Zermelo ordinals?,"For the von Neumann ordinals, $\in$ is the standard well-ordering. I would like to know the standard way to define a well-ordering $<$ for the set of Zermelo ordinals. In particular, I would like to know, for any two distinct Zermelo ordinals $u$ and $v$ , how one determines, formally 1 , whether $u < v$ or $v < u$ . I assume that this definition of $<$ for Zermelo ordinals will depend only on Zermelo's axioms, with the possible addition of the Axiom of Foundation.  At any rate, I am interested only in such a definition.  (In particular, I would like to avoid any definition that requires the existence of the set $\omega$ of finite von Neumann ordinals. 2 ) In case the definition of Zermelo ordinals is not sufficiently standardized, below I give the one I know. If $w$ is any set, let $\rho(w)$ be the formula that asserts $$\varnothing \in w \;\; \wedge \;\; \forall y\in w (\{y\} \in w).$$ Zermelo's original Axiom of Infinity postulates the existence of a set $w$ such that $\rho(w)$ . Let set $z$ be the smallest set $w$ such that $\rho(w)$ holds.  Formally, $$z := \bigcap \,\{w : \rho(w)\}.$$ A ""Zermelo ordinal"" is an element of the set $z$ . 1 I use the word ""formally"" here to rule out the method of ""counting the number of curly braces."" :) 2 On p. 42 of his The Foundations of Mathematics Kenneth Kunen contrasts Zermelo's original Axiom of Infinity (Axiom des Unendlichen: $\exists w [\varnothing \in w \ \wedge \ \forall y\in w (\{y\} \in w)]$ ), which justifies the existence of the set of Zermelo ordinals, with the now more standard Axiom of Infinity ( $\exists w [\varnothing \in w \ \wedge \ \forall y\in w (y \cup \{y\} \in w)]$ ), which justifies the existence of the set $\omega$ of finite von Neumman ordinals.  Kunen writes (my emphasis added): ...the two Axioms of Infinity are equivalent, although one needs to use [the Axiom of] Replacement ( which Zermelo did not have ) to prove the equivalence..."". As an exercise, I would like to prove, using the Axiom of Replacement, that the existence of the set $z$ of Zermelo ordinals implies the existence of the set $\omega$ of finite von Neumman ordinals, but all my attempts stumble for not having a proper definition of a well-ordering for $z$ .",['elementary-set-theory']
4460646,Why do different calculators disagree on $\cos(452175521116192774 )$?,"I want to calculate cosine of 452175521116192774 radians (it is around $4.52\cdot10^{17}$ ) Here is what different calculators say: Wolframalpha Desmos Geogebra Python 3.9 (standard math module) Python 3.9 (mpmath library) Obviously there is only one solution. There could be errors in floating point precision for these calculators, but this stumbles me. My calculator (TI-30XIS) says domain error (which is weird because cosine of, for example, a billion works just fine). How can I get the cosine of very large integers?","['trigonometry', 'machine-precision', 'calculator']"
4460718,Why are $\cosh$ and $\cos$ related by the imaginary unit?,"I'm interested, why are $\cosh$ and $\cos$ related by the imaginary unit like $$\cos (x)=\cosh (ix)?$$ Is there any visual proof? How can be circle related to hyperbola by complex unit?","['trigonometry', 'conic-sections', 'hyperbolic-geometry', 'complex-numbers']"
4460741,Higher order expansion of hypersurface about a point (beyond second fundamental form/extrinsic curvature),"Consider a smooth, compact $(d-1)$ -dimensional hypersurface $S$ without boundary embedded in $\mathbb{R}^d$ . The surface $S$ can be described as the graph of a function $f(x_1,x_2,\cdots,x_{d-1})$ . Using the tangent plane to $S$ at some point $\mathbf{x}$ , endowed with an orthonormal basis $\{{\bf e}_1,\cdots,{\bf e}_{d-1}\}$ , one can Taylor expand the function $f$ around ${\bf x} = 0$ as \begin{aligned}\label{graph}
f({\bf x })  =  \frac{1}{2} K_{ij} x^{i} x^{j} + \frac{1}{3!} A_{ijk}x^{i} x^{j} x^k +  \frac{1}{4!} B_{ijkl}x^{i} x^{j} x^k x^l+ O \left(| {\bf x} |^5 \right).
\end{aligned} The quadratic term is the extrinsic curvature of $S$ as embedded in $\mathbb{R}^d$ , also called the second fundamental form. However, the higher order terms in the expansion are less known and I was wondering what the tensors $A$ and $B$ correspond to? I would guess they depend also on the extrinsic curvature (and its derivatives), but what is their exact form? I cannot find anything beyond second order. Disclaimer: physicist notations","['submanifold', 'differential-geometry']"
4460778,Importance of Fixed-point theorems [duplicate],"This question already has an answer here : What is the role of fixed point theorems in modern mathematics? (1 answer) Closed 2 years ago . I have a more general question on the importance of fixed-point theorems. In mathematics youre being introduced to so many fixed-point theorems but i still could not figure out why they are so important. Why would be a simply looking statement as $f(x)=x$ be so important. I would appreciate any answer. Thanks in advance. On wikipedia it says nothing about the importance, contextualisation of theorems in mathematics is sometimes not given.","['fixed-point-theorems', 'analysis', 'calculus', 'discrete-mathematics', 'general-topology']"
4460802,Which f satisfies E[f(X) X] = E[f(X)] E[X]?,"Is there a function of a random variable, uncorrelated with the random variable itself? Besides the constant function. Consider a non-negative discrete random variable $X$ , with known probability mass function $p_X(x;\theta)$ . I am looking for a function $f$ that satisfies the following equation: $$ \mathbb{E}_\theta[f(X) X] = \mathbb{E}_\theta[f(X)]\mathbb{E}_\theta[X] \tag{1}$$ It is immediate that a constant function (with probability $1$ ) satisfies the previous condition.
That is, $f(X) = c \quad \forall X \in \mathcal{X}$ , with $\mathbb{P}[X \in \mathcal{X}] = 1$ , satisfies $(1)$ . What restrictions does $(1)$ impose on $f$ ? Can I safely assume that $f(X)$ cannot depend on $X$ ? Note 1: $(1)$ must be valid for any possible value of $\theta$ . Note 2: $p_X(x;\theta )$ is a member of the one-parameter exponential family. Note 3: $X$ is a complete minimal sufficient statistics of $\theta$ .","['statistics', 'expected-value', 'probability-theory', 'probability', 'random-variables']"
4460841,Does a universal set really violate Cantor's Theorem over $\mathbf{ZFC}$?,"It's well known that a universal set, $U$ , in ZFC, leads to a number of problems; such as Russell's Paradox when combined with specification . Another popular issue, as cited by Wikipedia is that Because this power set is a set of sets, it would necessarily be a subset of the set of all sets, provided that both exist. However, this conflicts with Cantor's theorem that the power set of any set (whether infinite or not) always has strictly higher cardinality than the set itself. This makes immediate sense at first glance. However, this feels more like a proof that the cardinality of a universal set simply behaves unexpectedly. In particular, there is one subtlety of Cantor's Theorem which has raised a question for me: The proof that $\text{card}(X)<\text{card}({\mathcal{P}(X))}$ is a result relying on the assumption that some map $f:X\to\mathcal{P}(X)$ exists at all. Thus my question is, can such a map really exist for a universal set? If not, then its cardinality cannot be compared with its power set, thus cannot be said to violate Cantor's Theorem. My work so far: Of course, I realized this concern of mine can be easily disproven by an example construction of any function $f:U\to \mathcal{P}(U)$ . Actually, since $U$ is its own powerset by virtue of containing all of its subsets, we just have to construct some $f:U\to U$ . I assume this could be done in theory using replacement , but precisely how is beyond my grasp; hence any insight would be greatly appreciated.",['elementary-set-theory']
4460844,Maximize complicated $U$ with respect to $\theta_0$ to find $S_0$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I would like to maximize the function $U$ with respect to $\theta_0$ in order to find $S_0$ . That's, I want to take derivative the function $U$ with respect to $\theta_0$ in order to find $S_0$ . Which program should I use in order to calculate this derivative ? because it is a bit complicated. Is it possible to use Matlab for that? Please help me writing Matlab code for this? Thank you! (Note: Sorry for not writing the function U directly. But, I hope it is readable.)","['computational-mathematics', 'self-learning', 'calculus', 'derivatives']"
4460922,"If for the first $\|n\|$ primes $p_i, \left(\frac{p_i}n\right)=+1$, then $n$ is a square","Can we prove or disprove (perhaps under some standard hypothesis): $$\text{If for the first }\|n\|\text{ primes }p_i, \left(\dfrac{p_i} n\right)=+1,\text{ then }n\text{ is a square.}$$ where $\|n\|$ is the number of bits in $n$ , $\left(\dfrac p n\right)$ is the Kronecker symbol , and $p_i$ is the $i^\text{th}$ prime, starting with $p_1=2$ . By contraposition, the proposition can also be stated as: $$\text{If }n\in\mathbb N\text{ is not a square, then }\exists\,i\in\mathbb N^*\text{ with }2^{i-1}\le n\text{ and }\left(\dfrac{p_i}n\right)\ne+1$$ I don't know a counterexample for the stronger proposition with $2^i\le n$ . The proposition holds For $n=0$ , $n=1$ , and more generally any $n$ that is a square. For even $n>0$ , since for these $\left(\frac 2 n\right)=0\ne+1$ . Thus we can restrict to odd $n$ and use the Jacobi symbol rather than the Kronecker symbol. If $n\bmod8\in\{3,5\}$ , since for these $\left(\frac 2 n\right)=-1\ne+1$ . We can thus focus on $n\equiv\pm1\pmod8$ , and turn to use of the Legendre symbol : for $n\equiv+1\pmod8$ and odd $p_i$ , we can change $\left(\dfrac{p_i} n\right)=+1$ to $\left(\dfrac n{p_i}\right)=+1$ for $n\equiv-1\pmod8$ and odd $p_i$ , we can change $\left(\dfrac{p_i} n\right)=+1$ to $\left(\dfrac {-n}{p_i}\right)=+1$ . A closely related problem is studied by Richard F. Lukes, C. D. Patterson and Hugh C. Williams, Some results on pseudosquares , Math. Comp. 65 (1996) , 361-372. The smallest $n$ for a given $i$ are the positive pseudosquares for $n\equiv+1\pmod8$ , negative pseudosquares for $n\equiv-1\pmod8$ . See A002189 and A045535 . Initial motivation: In some primality test algorithms we need the smallest $p_i$ with $\left(\dfrac{p_i}n\right)\ne+1$ . The proposition tells that when finding none for $i\in\left[1,\|n\|\,\right]$ , we can stop and declare there's no such $p_i$ since $n$ must be a square. If it was true, the proposition would avoid making an explicit square test. Remark (not quite an argument): for a fixed small prime $p$ and a large random $n$ , there is  probability $\dfrac{p+1}{2p}>\dfrac 1 2$ that $\left(\dfrac p n\right)\ne+1$ . The probabilities for small $p_i$ are only vanishingly correlated. If we look at how many such tests we need, here are ◦ smallest non-square odd $n$ record-holders for high $\pi(p)$ ◦ the smallest $p$ with $\left(\frac p n\right)\ne+1$ ◦ the number $\|n\|$ of bits of $n$ ◦ the index $\pi(p)$ per the prime counting function ◦ the margin $\Delta=\|n\|-\pi(p)$ there is for the proposition ◦ the kind of pseudosquare $n$ : $-$ for $n\equiv-1\pmod8$ , $+$ for $n\equiv+1\pmod8$ ◦ the factorization of $n$ when composite $$\begin{array}{|r|r|r|r|c|c|l|}
\hline
n&p&\!\|n\|\!&\pi(p)\!&\Delta&\text{kind}&\text{factors of }n\\
\hline
2&2&2&1&1&&\text{Prime}\\
7&3&3&2&1&-&\text{Prime}\\
23&5&5&3&2&-&\text{Prime}\\
71&7&7&4&3&-&\text{Prime}\\
311&11&9&5&4&-&\text{Prime}\\
479&13&9&6&3&-&\text{Prime}\\
1559&17&11&7&4&-&\text{Prime}\\
5711&19&13&8&5&-&\text{Prime}\\
10559&23&14&9&5&-&\text{Prime}\\
18191&29&15&10&5&-&\text{Prime}\\
31391&31&15&11&4&-&\text{Prime}\\
118271&37&17&12&5&-&101\cdot1171\\
366791&43&19&14&5&-&\text{Prime}\\
2155919&59&22&17&5&-&59\cdot36541\\
6077111&67&23&19&4&-&1039\cdot5849\\
98538359&71&27&20&7&-&79\cdot1247321\\
120293879&73&27&21&6&-&\text{Prime}\\
131486759&83&27&23&4&-&\text{Prime}\\
508095719&89&29&24&5&-&367\cdot547\cdot2531\\
2570169839&113&32&30&2&-&439\cdot5854601\\
196265095009&131&38&32&6&+&\text{Prime}\\
513928659191&137&39&33&6&-&\text{Prime}\\
844276851239&139&40&34&6&-&794239\cdot1063001\\
1043702750999&149&40&35&5&-&389\cdot5689\cdot471619\\
4306732833311&151&42&36&6&-&\text{Prime}\\
8402847753431&157&43&37&6&-&\text{Prime}\\
47375970146951&163&46&38&8&-&151717\cdot312265403\\
52717232543951&167&46&39&7&-&223\cdot1747\cdot6863\cdot19717\\
100535431791791&173&47&40&7&-&9873817\cdot10182023\\
178936222537081&181&48&42&6&+&\text{Prime}\\
493092541684679&193&49&44&5&-&4723\cdot104402401373\\
1088144332169831&223&50&48&2&-&293\cdot464941\cdot7987687\\
11641399247947921&227&54&49&5&+&\text{Prime}\\
88163809868323439&229&57&50&7&-&96757\cdot911187923027\\
196640248121928601&233&58&51&7&+&\text{Prime}\\
423414931359807911&239&59&52&7&-&241\cdot1756908428878871\\
695681268077667119&241&60&53&7&-&3413\cdot203832777051763\\
1116971853972029831&257&60&55&5&-&1721\cdot869521\cdot746416591\\
3546374752298322551&271&62&58&4&-&\text{Prime}\\
10198100582046287689&277&64&59&5&+&277\cdot1091\cdot1151\cdot29318344777\\
\hline
\end{array}$$ Values are extracted from Richard F. Lukes, A Very Fast Electronic Number Sieve , thesis presented to the University of Manitoba (1995), tables 6.22 (negative pseudosquares A045535 ) and 6.24 (positive pseudosquares A002189 ), and in the process of being independently verified.
The terms to $\pi(p)=50$ also are in Nathan D. Bronson and Duncan A. Buell, Congruential Sieves on FPGA Computers , in Proceedings of Symposia in Applied Mathematics, Volume 48, 1994 , 547-551.","['number-theory', 'riemann-hypothesis', 'analytic-number-theory', 'quadratic-residues', 'prime-numbers']"
4460952,Laurent expansion of Meijer's G function,"I am considering the following equation (a Generalized hyper-geometric equation): $$\left(D-\beta_1\right)\left(D-\beta_2\right)f(x)-x\left(D+1-\alpha_1\right)\left(D+1-\alpha_2\right)f(x)=0$$ where in this case I specifically take $\alpha_1=\frac{1}{4}$ , $\alpha_2=\frac{5}{4}$ , $\beta_1=\frac{3}{4}$ , $\beta_2=-\frac{1}{4}$ .
In fact this equation has the following two solutions: $$f_1(x)=x^{3/4}\,{}_2F_1\left(\frac{3}{2},\frac{1}{2};2;x\right)$$ The second solution is quite unfamiliar for me, which is denoted by MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] in Mathematica. It has the following integral representation: $$f_2(x)=\frac{1}{2\pi i}\int_L\frac{\Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right)}{\Gamma\left(s+\frac{1}{4}\right)\Gamma\left(\frac{5}{4}+s\right)}x^{-s}ds$$ with $L$ a proper contour. It can be seen that $\Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right)$ has a simpole pole at $s=\frac{1}{4}$ , and has double poles at $s=-\frac{3}{4}$ , $s=-\frac{7}{4}$ , $s=-\frac{11}{4}$ ,...I sum up the residue to get a series expansion: $$f_2(x)=\frac{2}{\pi}x^{-1/4}+\frac{x^{3/4}}{\pi^2}\sum_{n=0}^{\infty}\frac{\Gamma\left(\frac{3}{2}+n\right)\Gamma\left(\frac{1}{2}+n\right)}{n!\Gamma\left(n+2\right)}\left(-\psi\left(-n-\frac{1}{2}\right)-\psi\left(\frac{1}{2}-n\right)-\ln x\right)x^{n}$$ where I use $\psi(z)=\frac{\Gamma^{\prime}(z)}{\Gamma(z)}$ The problem is, does this series expansion converge on the whole complex plane? Can I use this expansion to determine the monodromy around both $0$ and $\infty$ ? Thanks to the hint from @Mariusz Iwaniuk, I know that by using the following command in Mathematica: MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] // FunctionExpand , the output may indicate that $f_2(x)$ vanishes outside the unit disk $0<|x|<1$ . However, another question comes out: since the equation is invariant under $x\to \frac{1}{x}$ , I can construct the third solution as $f_3(x)=f_2(1/x)$ , which is nonzero only near the $\infty$ . Then we just have three independent solutions at hand. What's the problem with my reasoning?","['monodromy', 'laurent-series', 'ordinary-differential-equations', 'hypergeometric-function']"
4461012,Find a sequence of functions that converge weakly to 1/2,"I'm trying solve the following problem: Find an example of a sequence of functions $f_n:[0,1]\to[0,1]$ such that $$(f_n)_\#\mathscr{L} = \mathscr L$$ but $f_n\rightharpoonup 1/2$ (weak convergence). Note that $(f_n)_\#(\mathscr L)$ denotes the push-forward image measure, i.e. $(f_n)_\#\mathscr L = \mathscr L(f_n^{-1})$ and $\mathscr L$ is the Lebesgue measure (restricted to $[0,1]$ ). Unfortunately, the problem doesn't have any more details other than this. My professor helped me figure out to consider the sequence $$
f_n(x) = \sum_{j=0}^{2^{n-1}-1}2^{n-1}\left(x - \frac{j}{2^{n-1}}\right)\chi_{[j/2^{n-1},(j+1)/2^{n-1})} \qquad \text{and}\qquad f_n(1) = 1.
$$ and I've proved the equivalence of the pushforward of the Lebesgue measure and the Lebesgue measure, but I'm having a lot of trouble proving the weak convergence to 1/2. My professor just said to prove that if $f\in C([0,1])$ , then I need to show that $$\left< f_n,f\right> \to \left<1/2,f\right>$$ where $\left<\cdot,\cdot\right>$ is the $L^2$ inner product. Some immediate concerns I had were why would we assume that this is the weak convergence we want to use and also why do we assume $f$ is continuous. It seems the problem statement is missing details about the setting we are in. My other issue is that I'm just stuck on how to prove this limit. My work so far consists of $$\begin{align*}
        \left|\left< \frac{1}{2}-f_n,f\right> \right| &= \left|\int_0^1 \left(\frac{1}{2}-f_n(x)\right) f(x)dx\right|\\
        &= \left|\int_0^1f(x) \left(\sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}-2^{n-1}\left( x- \frac{j}{2^{n-1}}\right)\right)\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)}dx\right|\\
       &=   \left|\int_0^1f(x) \sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}- 2^{n-1}x + j\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)} dx\right|\\
       &= \left| \sum_{j=0}^{2^{n-1}-1} \int_{\frac{j}{2^{n-1}}}^{\frac{j+1}{2^{n-1}}}f(x) \left(\frac{1}{2}- 2^{n-1}x + j\right) dx\right|\\
       &= \left|\sum_{j=0}^{2^{n-1}-1} \int_0^{\frac{1}{2^{n-1}}} f\left(\frac{j+1}{2^{n-1}}-x\right) \left(2^{n-1}x-\frac{1}{2}\right) dx\right| \tag{by substitution}
    \end{align*}$$ I've tried bringing the absolute values in and pulling out the sup of $f$ , but it feels like all the things I've tried don't actually make the total quantity small, which makes me doubt that my original sequence was correct or perhaps this norm isn't correct, but if that's the case, I'm not sure what to do. If anyone has any suggestions, I'll greatly appreciate it.","['weak-convergence', 'functional-analysis', 'analysis']"
4461014,Estimate Poisson parameter,"1. Background: Given a parameter $k\in\mathbb{R}^+$ . I have a bunch of positive data pairs $\{(a_i,b_i)|a_i\in\mathbb{R}^+, b_i\in\mathbb{R}^+\}_{i=1}^N$ . For each pair $(a_i,b_i)$ , an observation value $y_i$ is generated by: $$
y_i=\frac{1}{k}(A_i-B_i),~~~~A_i\sim \text{Pois}(ka_i),~~~~B_i\sim \text{Pois}(kb_i),
$$ where $A_i$ and $B_i$ are independent. In other words, each $y_i$ is generated by the following three steps: $(1.1)$ Get $a_i$ and randomly pick a value for $A_i$ from $\text{Pois}(ka_i)$ . $(1.2)$ Get $b_i$ and randomly pick a value for $B_i$ from $\text{Pois}(kb_i)$ . $(1.3)$ Perform $y_i=\frac{1}{k}(A_i-B_i)$ . 2. My Question: Now I have a bunch of data points and observation values $\{(a_i,b_i,y_i)\}_{i=1}^N$ . How can I estimate the unknown latent parameter $k\in\mathbb{R}^+$ (globally unified)? 3. My Efforts: $(3.1)$ I may know that the expected value and variance of each $y_i$ can be calculated by: \begin{align}
E(y_i)&=\frac{1}{k}[E(A_i)-E(B_i)]=\frac{1}{k}(ka_i-kb_i)=a_i-b_i,\\
D(y_i)&=\frac{1}{k^2}[D(A_i)+D(B_i)]=\frac{1}{k^2}(ka_i+kb_i)=\frac{1}{k}(a_i+b_i).
\end{align} $(3.2)$ There are actually $N$ independent nested distributions. For the $i$ -th observation, I can not calculate the variance (since there is only one observation $y_i$ ) to perform maximum likelihood estimation (MLE) or moment estimation (ME) with using $D(y_i)=\frac{1}{k}(a_i+b_i)$ . $(3.3)$ I am confused by how to find a best $k$ that can fit the data points and observations well, by starting from some convincing ideas like MLE or ME? $(3.4)$ Since there may be actually not only one distribution, but exist $N$ compounded distributions, and each compounded distribution correspond to only one data point and observation $(a_i,b_i,y_i)$ , I have no idea about finding the best $k$ . 4. Update (1): I start from the idea of maximum likelihood estimation (MLE). First, let $Y_i$ be a random variable corresponding to $y_i$ , then I have: \begin{align}
P(Y_i=s)&=\sum_{n=0}^\infty P(A_i=n+ks)P(B_i=n)\\
&=\sum_{n=0}^\infty \frac{(ka_i)^{n+ks}e^{-ka_i}}{(n+ks)!}\times \frac{(kb_i)^{n}e^{-kb_i}}{n!}\\
&=(ka_i)^{ks}e^{-k(a_i+b_i)}\sum_{n=0}^\infty \frac{(k^2a_ib_i)^n}{n!(n+ks)!}.
\end{align} From Wolfram , I have: \begin{align}
P(Y_i=s)&=(ka_i)^{ks}e^{-k(a_i+b_i)}\sum_{n=0}^\infty \frac{(k^2a_ib_i)^n}{n!(n+ks)!}\\
&=(ka_i)^{ks}e^{-k(a_i+b_i)} (k^2a_ib_i)^{-\frac{ks}{2}}I_{ks}(2k\sqrt{a_ib_i})\\
&=(\frac{a_i}{b_i})^{\frac{ks}{2}}e^{-k(a_i+b_i)}I_{ks}(2k\sqrt{a_ib_i}),
\end{align} where $I_{ks}(\cdot)$ is the modified Bessel function of the first kind. Using the idea of MLE, I want to solve the following problem: \begin{align}
\hat{k}&=\underset{k}{\arg\max}~\prod_{i=1}^N{P(Y_i=y_i|k)}\\
&=\underset{k}{\arg\max}~\sum_{i=1}^N{lnP(Y_i=y_i|k)}\\
&=\underset{k}{\arg\max}~\sum_{i=1}^N{ln\left[(\frac{a_i}{b_i})^{\frac{ky_i}{2}}e^{-k(a_i+b_i)}I_{ky_i}(2k\sqrt{a_ib_i})\right]}\\
&=\underset{k}{\arg\max}~\sum_{i=1}^N{\frac{ky_i}{2}ln(\frac{a_i}{b_i})-k(a_i+b_i)+ln\left[I_{ky_i}(2k\sqrt{a_ib_i})\right]}\\
&=\underset{k}{\arg\max}~\sum_{i=1}^N{k\left[\frac{y_i}{2}ln(\frac{a_i}{b_i})-(a_i+b_i)\right]+ln\left[I_{ky_i}(2k\sqrt{a_ib_i})\right]}.
\end{align} Due to the existence of $ln\left[I_{ky_i}(2k\sqrt{a_ib_i})\right]$ , I am stucked and think that it is too hard for me to get the derivative and the optimal $k$ . The derivative of the above infinite series is also quite complicated: Link . After a long struggle, I am still failed to get the answer. I am not sure if the above steps are all correct. Could you please figure it out for me? 5. Update (2): Following the suggestion from @BinxuWang王彬旭, I write the following Python program for simulation: import numpy as np
from scipy.stats import skellam

data_len = 80
max_value = 1.0
A = np.random.rand(data_len,) * max_value 
B = np.random.rand(data_len,) * max_value 
k_gt = 100.0

Y_observed = (np.random.poisson(k_gt * A) - np.random.poisson(k_gt * B)) / k_gt

for k in range(9):
    k = 10 ** (k - 4)
    Y_k = Y_observed * k
    mu1 = A * k
    mu2 = B * k
    L = np.sum(np.log(skellam._pmf(Y_k, mu1, mu2)))  # L = sum(log(...))
    print(k, L) The output is: 0.0001 -0.031569807095909126
0.001 -0.2549938771522239
0.01 -1.9421743846032573
0.1 -13.274040574073975
1 -66.80138745123028
10 -162.06597061912214
100 -278.81300414250535
1000 nan
10000 nan The true value of $k$ is $100$ . However, I see that the optimal $k$ given by this program will always be zero, since the likelihood evaluation value may monotonically decrease in $(0,+\infty)$ . And even when $k=100$ , I can not see that it peaks the maximal value. Similar phenomenons are observed with various combinations of data_len , max_value and k_gt . Is my implementation wrong, or I missed something? 6. Update (3): It seems that the following approach inspired by a comment from Zhihu works. Let $Z_i=\frac{Y_i-(a_i-b_i)}{\sqrt{a_i+b_i}}$ , then we have $E(Z_i)=0$ and $D(Z_i)=\frac{1}{k}$ . Although different $Z_i$ s might not obey the same latent population distribution, I write the following program for test: import numpy as np

data_len = 1000000
max_value = 1e3

for i in range(10):
    k_gt = np.random.rand(1,) * 1e4

    A = np.random.rand(data_len,) * max_value 
    B = np.random.rand(data_len,) * max_value 

    Y_observed = (np.random.poisson(k_gt * A) - np.random.poisson(k_gt * B)) / k_gt

    z = (Y_observed - (A - B)) / ((A + B) ** 0.5)
    k_hat = 1 / z.var()
    #print('E(z) =', z.mean())
    #print('D(z) =', z.var())
    print('True k =', k_gt)
    print('Estimated k = ', k_hat)
    print('===') The output is: True k = [2298.1854775]
Estimated k =  2300.5718461385372
===
True k = [7922.65659395]
Estimated k =  7917.124293172064
===
True k = [263.85265814]
Estimated k =  263.6420801776157
===
True k = [6645.19567678]
Estimated k =  6648.825624909179
===
True k = [9224.97551002]
Estimated k =  9221.90494158757
===
True k = [5343.45415427]
Estimated k =  5343.7859362020945
===
True k = [3711.94735707]
Estimated k =  3722.572332730343
===
True k = [5075.17847329]
Estimated k =  5071.739140612038
===
True k = [4805.75795587]
Estimated k =  4806.967854078318
===
True k = [1378.56419652]
Estimated k =  1379.657342920305
=== I think this method may be sensible since it gives estimation $\hat{k}$ quite close to the latent true $k$ . And when the sample number data_len increases, the estimation becomes more accurate.","['statistics', 'poisson-distribution', 'maximum-likelihood', 'probability', 'random-variables']"
4461015,The Lie algebra of vector fields preserving a tensor,"Let $\alpha$ be a tensor field on a manifold $M$ , compact and without boundary. Denote by $\mathrm{Diff}(M,\alpha)$ the group of diffeomorphisms of $M$ preserving $\alpha$ , i.e. $f:M\to M$ such that $f^*\alpha=\alpha$ . I am interested in the Lie algebra of this group; the tangent space at the identity is given by a subset of vector fields, those satisfying $\mathcal{L}_X\alpha=0$ . I am trying to prove that this set of vector fields is a Lie subalgebra of the vector fields on $M$ ; in other words, I want to show that if $X$ and $Y$ preserve $\alpha$ then also $\mathcal{L}_{[X,Y]}\alpha=0$ . One possible way to do it is to express the flow of $[X,Y]$ using the flows of $X$ and $Y$ , as $$\mathcal{L}_{[X,Y]}\alpha=\partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha.$$ It might be difficult to express $\Phi^{[X,Y]}_t$ as a combination of $\Phi^X_t$ and $\Phi^Y_t$ , but in fact we just have to do it to first order in $t$ , as is explained in this question , for example. Writing $$\Phi_t^{[X,Y]}=\Phi^X_{\sqrt{t}}\circ\Phi^Y_{\sqrt{t}}\circ\Phi^X_{-\sqrt{t}}\circ\Phi^Y_{-\sqrt{t}}+O(t^2)$$ it seems to follow easily that $\partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha=0$ , since each of the flows preserves $\alpha$ . So, a first question is: is this reasoning correct? I am however interested in proving the identity directly, through a computation in local coordinates. This is motivated by the fact that, in the context of symplectic geometry, it is possible to directly compute in coordinates for example that Hamiltonian vector fields form a Lie algebra, even showing identites like $[X^\omega_f,X^\omega_g]=X^\omega_{\{f,g\}}$ (up to a sign depending on your conventions). So, let's say that $\alpha$ is a closed $2$ -form, and $X$ and $Y$ are vector fields such that $\alpha_{ij}X^i\mathrm{d}x^j$ and $\alpha_{ij}Y^i\mathrm{d}x^j$ are closed; how do I compute that also $$\alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j$$ is closed? As $\alpha_{ij}\partial_kX^i$ is symmetric in $j$ and $k$ (and the same holds for $Y$ ), we have $$\alpha_{ij}X^k\partial_k Y^i=\partial_j(X^k Y^i)\alpha_{ik}+\alpha_{ij}Y^k\partial_kX^i$$ so that $$\alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j=\partial_j(X^k Y^i)\alpha_{ik}\mathrm{d}x^j.$$ At this point however I am unable to check that the differential of this expression vanishes. We get then at my second question : can you proceed from here to show that $\mathrm{d}\left([X,Y]\lrcorner\alpha\right)=0$ ?","['symplectic-geometry', 'vector-fields', 'solution-verification', 'lie-groups', 'differential-geometry']"
4461085,"smooth maps between submanifolds, the image of tangent space under differential is contained in a tangent space","I want to show : This is from ""Mathematical analysis"" by Andrew Browder. This is not a manifolds text so we have only defined submanifolds on $R^n$ using the local immersion definition. I know there are at least 4 more equivalent definitions. The definition of tangent space is defined by smooth curves. Also smooth mapping on a set is defined to be a smooth extension $F$ to an open set such that $F$ and $f$ agrees on $M$ and a previous theorem showed that the differential (jacobian matrix) applied to tangent spaces are well defined. So by definition, an element of $h\in T_p(M)$ is a smooth curve $\gamma$ that maps to $M$ such that $\gamma(0)=p$ and $\gamma'(0)=h$ . So I want to show $dF_p(h)=h' \in T_{f(p)}(N)$ . Now this is where I have been stuck for a while, because we want a curve $\gamma_N$ that maps to $N$ such that $\gamma_N(0)=f(p)$ and $\gamma_N'(0)=h'$ , but I can't think of any good reason why such a curve should exist.","['submanifold', 'smooth-manifolds', 'analysis', 'manifolds', 'differential-geometry']"
4461125,Why do perfect squares in Pythagorean spirals generate triple spiral pattern,"I was generating artistic depictions of mathematical concepts by programming for fun, I made some pictures of Pythagorean Spiral, and I noticed this pattern if I only show perfect squares: Pythagorean Spiral depicting perfect squares up to 256: Perfect squares up to 512: Perfect squares up to 1024: Perfect squares up to 2048: I used the following function to calculate the spiral: def pythogorean_spiral(iterations, mode='rim', num_colors=12, condition=None):
    assert mode in ('rim', 'radial', 'triangles')
    if condition:
        assert condition in filters
        filtered = condition(iterations)
    step = 1530/num_colors
    color_values = ['#'+spectrum_position(round(step*i), 1) for i in range(num_colors)]
    colors = ['#ff0000']
    collection = []
    if not condition or 0 in filtered:
        if mode == 'rim':
            collection.append([(1, 0), (1, 1)])
            
        if mode == 'radial':
            collection.append([(0, 0), (1, 0)])
        
        elif mode == 'triangles':
            collection.append([(0, 0), (1, 0), (1, 1)])
    
    cur_x, cur_y = 1, 1
    for i in range(1, iterations):
        radius = (cur_x ** 2 + cur_y ** 2) ** .5
        new_radius = radius + 1
        angle = atan2(cur_x, cur_y)
        new_x, new_y = new_radius*cos(angle), new_radius*sin(angle)
        new_x, new_y = rotate((new_x, new_y), 90, (cur_x, cur_y))
        color = color_values[i % num_colors]
        colors.append(color)
        if not condition or i in filtered:
            if mode  == 'rim':
                collection.append([(cur_x, cur_y), (new_x, new_y)])
                
            if mode == 'radial':
                collection.append([(0, 0), (cur_x, cur_y)])
            
            elif mode == 'triangles':
                collection.append([(0, 0), (cur_x, cur_y), (new_x, new_y)])
        
        cur_x, cur_y = new_x, new_y
    
    result = {'colors': colors}
    if mode  == 'rim':
        result['rim'] = collection
    
    if mode == 'radial':
        result['radial'] = collection
    
    elif mode == 'triangles':
        result['triangles'] = collection
    
    return result Full script Why am I seeing the triple spiral pattern? Here is the data depicted in the image, in Python dictionary format, the key is the number, and the value is its position. In [176]: find_squares(2048)
Out[176]:
{0: (1, 0),
 1: (1, 1),
 4: (-1.6308097207961918, 1.5298560894922917),
 9: (-1.497112501918127, -2.7854360801498306),
 16: (4.074022642113989, -0.6343023817884941),
 25: (-1.1072688801027475, 4.977344234343855),
 36: (-4.98458532826389, -3.4862457035120658),
 49: (5.996732853004965, -3.7468913901647802),
 64: (1.2030383665443862, 7.971994649309683),
 81: (-8.74302530802115, -2.3578609932100782),
 100: (6.317606591629188, -7.815871477538745),
 121: (5.025225140018225, 9.83601099491705),
 144: (-12.02537661686118, 0.6247537295991847),
 169: (4.7131951151331055, -12.156717970187723),
 196: (9.852134360350037, 9.996771906251038),
 225: (-14.09884120422741, 5.217535500404312),
 256: (1.119993543058584, -15.992048476149222),
 289: (14.986552417661628, 8.08722737609624),
 324: (-14.34564731859593, 10.917985299974568),
 361: (-4.240153608700732, -18.547805729374634),
 400: (19.62024563990485, 4.005741008826664),
 441: (-12.349124780077803, 17.014673583882512),
 484: (-10.871585332769126, -19.152248754449722),
 529: (22.929824939835186, -2.055025116273555),
 576: (-7.958753928923706, 22.66402956005053),
 625: (-18.049465629543615, -17.326765147826205),
 676: (24.17841787504091, -9.612705605598585),
 729: (-1.3269058233345064, 26.985909674050397),
 784: (-24.89305639538484, -12.858294727381011),
 841: (22.812021983982696, -17.93353431430349),
 900: (7.087529794682105, 29.16790910246232),
 961: (-30.460426219812412, -5.844863942588621),
 1024: (18.5398249709077, -26.101243074767932),
 1089: (16.550145885300477, 28.56733573813433),
 1156: (-33.85505726799531, 3.2916709101562596),
 1225: (11.389197702942742, -33.11021255871532),
 1296: (26.114611762933436, 24.799738959740765),
 1369: (-34.33332875841694, 13.828323700524258),
 1444: (1.7284388687767098, -37.97383966728841),
 1521: (34.713383531937104, 17.80396033933651),
 1600: (-31.401435769992876, 24.798182021733684),
 1681: (-9.746528349261542, -39.83723365316765),
 1764: (41.26708260884942, 7.875778879224264),
 1849: (-24.891018197164836, 35.07758847338917),
 1936: (-22.06172637665742, -38.08254494229001),
 2025: (44.80188956600656, -4.334823100820575)} I don't feel like explaining things, so here is the Wikipedia article . Here is the full picture of the spiral with 256 triangles: Picture of triangles in the spiral corresponding to perfect squares up to 256: I made another image which is more descriptive, and the data it displays does indeed corroborate the accepted answer's theory:","['number-theory', 'visualization', 'geometry', 'elementary-number-theory']"
4461195,Deriving Green's theorem,"The reasoning leading to Green's theorem in my course makes a step I don't understand, with no justification. We have a function $P:R\to \mathbb{R}$ that has a partial derivative with respect to $y$ over $R$ . We're computing $\displaystyle \iint \limits _R\dfrac{\partial P}{\partial y}(x,y)\,dA$ . Say $R$ is regular with respect to the $x$ -axis. We can easily compute that it's equal to $\displaystyle \int \limits _a^bP(x,f_2(x))\,dx-\int \limits _a^bP(x,f_1(x))\,dx$ for $a\leq x\leq b$ .
These are line integrals along the curves $y=f_1(x)$ and $y=f_2(x)$ . We'll call them respectively $C_1$ and $C_2$ . So our original double integral becomes $\displaystyle \int \limits _{C_2}P(x,y)\,dx-\int \limits _{C_1}P(x,y)\,dx$ . The next step is the one I don't understand: my course states that this is equal to $\displaystyle -\oint \limits _{C^+}P(x,y)\,dx$ . Why?","['multivariable-calculus', 'calculus', 'line-integrals', 'greens-theorem']"
4461205,"If Z is regularly varying with index $\alpha$, then $zP(Z>z)$ is decreasing [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question i want to show: if $Z\geq0$ is regularly varying with index $\alpha$ , that the function $zP(Z>z)$ is decreasing. Def: A randomvariable $Z$ is regularly varying with index $\alpha>1$ if for all $t>0$ $\lim_{x\longrightarrow\infty}\frac{P(Z>tx)}{P(Z>x)}=t^{-\alpha}$ I tried to use the definition of an increasing function, but got stuck because i dont know where i can use the assumption, that $Z$ is reg.varying.","['statistics', 'probability-distributions', 'probability']"
4461244,Converge almost surely to zero with arbitrary $a_{nk}$,"Let $X_1,X_2,\dots,$ be i.i.d. random variables with $\mathbb{E}[X_1]=0,\operatorname{Var}(X_1)<\infty$ . Assume that $\sum_{k=1}^{n}a_{nk}^2=\frac{1}{n}$ , let $S_n=\sum_{k=1}^{n}a_{nk}X_k$ , prove that $$S_n\rightarrow0~~a.s.$$ It seems that this looks like the triangle array of some law of large numbers, we wish to use Borel-Cantelli to deal with this problem, but it seems hard since all $a_{nk}$ are arbitrary. How should we deal with the $a_{nk}$ s here in this problem?","['law-of-large-numbers', 'probability-limit-theorems', 'probability-theory', 'probability']"
4461272,Calculating contour integral with keyhole contour [duplicate],"This question already has answers here : calculate: $\int_0^\infty \frac{\log x \, dx}{(x+a)(x+b)}$ using contour integration (2 answers) Closed 2 years ago . Evaluate the integral $\int_0^{\infty}\frac{\log(x)}{(x+a)(x+b)}dx$ with $a,b>0, a\neq b$ by using the keyhole contour. My attempt-
I drew the keyhole contour (obviously it's supposed to say $-a,-b$ on the left side of the $x$ axis): And we have the following as we let $\varepsilon\to 0$ and $R\to\infty$ : $$\int_{\Gamma}f(z)dz=2\pi i(Res(f,-a)+Res(f,-b))$$ $$\int_{\gamma_{R}}\frac{\log(z)}{(z+a)(z+b)}dz\to 0$$ $$\int_{\gamma_{\varepsilon}}\frac{\log(z)}{(z+a)(z+b)}dz\to 0$$ $$\int_{\gamma_{1}}\frac{\log(z)}{(z+a)(z+b)}dz\underset{\underset{\varepsilon\to0}{R\to\infty}}{\to}\int_{0}^{\infty}\frac{\log(x)}{(x+a)(x+b)}dx$$ The only thing I'm having trouble calculating is: $$\int_{\gamma_{2}}\frac{\log(z)}{(z+a)(z+b)}dz$$ as $\varepsilon\to 0$ and $R\to\infty$ . I assume it's: $$\int_{\gamma_{2}}\frac{\log(z)}{(z+a)(z+b)}dz=-\int_{0}^{\infty}\frac{\log(x)+2\pi i}{(x+a)(x+b)}dz$$ and if I denote $I=\int_{0}^{\infty}\frac{\log(x)}{(x+a)(x+b)}dx$ then $$\int_{\gamma_{2}}\frac{\log(z)}{(z+a)(z+b)}dx=I+2\pi i\frac{\ln(a)-\ln(b)}{a-b}$$ and when summing everything $I$ cancels. What am I doing wrong here?","['integration', 'complex-analysis', 'contour-integration', 'improper-integrals']"
4461326,Are there theories about inverse (or at least adjoint) functor Top -> Diff of the forgetful functor Diff -> Top?,"Several general sources ( https://en.wikipedia.org/wiki/Category_of_topological_spaces , https://en.wikibooks.org/wiki/Topology/Manifolds/Categories_of_Manifolds ) mention the forgetful functor from category of manifolds M (or differential manifolds Diff) to the category of topological spaces Top: Diff -> Top. My question is about the inverse (or even adjoin?) functor Top -> Diff? Does math say something about it, is there theories about it? Some algorithms, that allow to list such manifolds. Just some hints what is happening about this functor? https://golem.ph.utexas.edu/category/2008/05/convenient_categories_of_smoot.html is one dicussion that mentions the problems of defining categories of manifolds and scarcely mentions the inverse functor in which I am interested. I have neural network applications in mind. From the one hand there is topological approaches to model theories (e.g. sheaves for HOL by Awodey), so, the mapping theory <-> topological spaces has been taken care for. From the other hand all the empirical data of the functioning of the neural network are point clouds. Also the first efforts to build analytical theory of neural networks work with manifolds, with functions. So - if we could connect manifolds (concrete instance) with topological spaces (concrete instance) then we could proceed all the way from empirical data about functioning of neural networks up to the theories and mappings betwen theories. While the resulting structure can be hopelessly formidable, the algebraic topology methods (computing homological and cohomological groups for neural manifolds and topological spaces) can tame this structure and give some general results. So, my question is about this missing link.","['neural-networks', 'category-theory', 'machine-learning', 'manifolds', 'general-topology']"
4461327,"How to show $\bigcap_{n=1}^{\infty}[0,1+1/n) = [0,1]$?","To show that they are equal, I need to show $\bigcap_{n=1}^{\infty}[0,1+1/n) \subset [0,1]$ and $[0,1] \subset \bigcap_{n=1}^{\infty}[0,1+1/n)$ My attempt is: let $x \in [0,1] \Rightarrow 0 \leq x \leq 1$ , since $1 < 1+1/n, \ \forall n \geq1 \Rightarrow x \in \bigcap_{n=1}^{\infty}[0,1+1/n) \Rightarrow [0,1] \subset \bigcap_{n=1}^{\infty}[0,1+1/n)$ However, I don't know how to show $\bigcap_{n=1}^{\infty}[0,1+1/n) \subset [0,1]$ . It seems obvious since $\lim_{n \to \infty}  1+1/n = 1$ , but I am having trouble to proving that. Any help or hint would be appreciated","['elementary-set-theory', 'real-analysis']"
4461361,Prove that $\frac{\sin4\alpha}{1+\cos4\alpha}\cdot\frac{\cos2\alpha}{1+\cos2\alpha}=\cot\left(\frac{3\pi}{2}-\alpha\right)$,"Prove that $$\dfrac{\sin4\alpha}{1+\cos4\alpha}\cdot\dfrac{\cos2\alpha}{1+\cos2\alpha}=\cot\left(\dfrac{3\pi}{2}-\alpha\right)$$ The RHS is equal to $\tan\alpha,$ so we are to show $$\dfrac{\sin4\alpha}{1+\cos4\alpha}\cdot\dfrac{\cos2\alpha}{1+\cos2\alpha}=\tan\alpha$$ My try for simplifying the LHS: $$\dfrac{2\sin2\alpha\cos2\alpha}{1+\cos^22\alpha-\sin^22\alpha}\cdot\dfrac{\cos^2\alpha-\sin^2\alpha}{1+\cos^2\alpha-\sin^2\alpha}=\\=\dfrac{4\sin\alpha\cos\alpha(\cos^2\alpha-\sin^2\alpha)}{1+(\cos^2\alpha-\sin^2\alpha)^2-(2\sin\alpha\cos\alpha)^2}\cdot\dfrac{\cos^2\alpha-\sin^2\alpha}{1+\cos^2\alpha-\sin^2\alpha}\\=\dfrac{4\sin\alpha\cos\alpha(\cos\alpha-\sin\alpha)(\cos\alpha+\sin\alpha)}{1+\sin^4\alpha-2\cos^2\alpha\sin^2\alpha+\cos^4\alpha-4\sin^2\alpha\cos^2\alpha}\cdot\dfrac{\cos^2\alpha-\sin^2\alpha}{2\cos^2\alpha}$$",['trigonometry']
4461383,Show that law of sum of sums converges weakly to law integral of brownian motion,"Let $(X_k)_{k=1}^\infty$ be a sequence of independent and identically distributed random variables with zero mean and unit variance. Define, for $n\geq1$ , $$S_n=\sum_{k=1}^nX_k.$$ Prove that the law  of $$\frac{1}{n^{3/2}}\sum_{k=1}^nS_k$$ converges weakly to that of the random variable $$\int_0^1B_tdt$$ where $B=(B_t:t\geq0)$ is a standard Brownian motion on $\mathbb{R}$ . Hint: The sums $\sum_{k=1}^nS_k/n^{3/2}$ approximate a certain continuous operation on the space $C[0,1]$ . I really don't know where to start with this question. It seems like Skorokhod embedding and also Donsker's invariance principle could perhaps be useful, but I don't see where to start with this. I also don't see what operation these sums could be approximating. Any advice would be greatly appreciated, thanks very much!","['random-walk', 'weak-convergence', 'martingales', 'brownian-motion', 'probability-theory']"
4461409,"Unclear of Meaning of Notation $\cap _{n=1}^\infty (0,1/n)=\varnothing$","I am a bit unclear about the notation involved in the following question of my textbook. I am concerned with understanding what is asked by the question - please do not give the answer. $\textbf{1.4.3.}$ Prove: $\bigcap \limits _{n=1}^\infty \left (0,\dfrac{1}{n}\right )=\varnothing$ . (From Stephen Abbot's ""Understanding Analysis""). What exactly is meant by $\left (0,\dfrac{1}{n}\right )$ ? Is this simply the tuple containing $0$ and $\dfrac{1}{n}$ ? If this is the case then I think my proof would simply entail proving $\dfrac{1}{n_1}\neq \dfrac{1}{n_2}$ for $n_1\neq n_2$ , which seems like a trivial result (hence why I ask my question - this feels too simple to be what the question is really asking).","['elementary-set-theory', 'real-numbers', 'real-analysis']"
4461509,Multinomial theorem with multivariate terms?,"Let $S=\{a,b,c,d,...\}$ . Let $P_n=(abc+abd+acd+...+ab+ac+ad+...a+b+c+d...)^n$ . In addition, there's the condition that for all variables, $x^n=x$ (maybe it'll be easier without this?).  Is there something like the multinomial theorem to expand out $P_n$ ? Edit: Here's what I have so far: For any $n$ , the unique terms in the expanded product is the set of terms $a^{k_1}b^{k_2}c^{k_3}...$ , such that $k_1,k_2,k_3,... \le n$ (you can have at most $n$ multiplicands in the product), and that $n \le k_1+k_2+k_3+... \le 3n$ (at minimum, there are $n$ length-1 multiplicands in the product, and at most, there are $n$ length-3 multiplicands in the product).
For any such term $a^{k_1}b^{k_2}c^{k_3}...$ , to find the number of terms that make it up, there must be $k_1$ terms that have $a$ , $k_2$ terms that includes $b$ , $k_3$ that includes c, and so on. Edit 2: I realize that this can be done recursively --- for a given term, remove one of the original terms ( $abc, abd, ...$ ), and find the composition of that smaller term,","['multivariable-calculus', 'multinomial-theorem']"
4461529,Convergence of a sequence defined by an integral,"Let me state the problem first. It is one of my calculus exercises. $$a_n = \int_0^{\pi/2} \frac{\sin^2x}{1+\sin^2 nx}dx$$ Show that $a_n$ converges and find its limit. The only theorem I know about the convergence of the sequence is monotone convergence theorem, but I cannot find out whether $a_n$ is monotone. The sequences defined by integral ususally gives the recurrence relation by IBP, but in this approach, effective choice of $u$ and $v'$ seems to be unclear. Thanks in advance for all solutions, ideas, and hints.","['integration', 'calculus', 'definite-integrals', 'sequences-and-series']"
4461543,How to prove combinatorial identities related to infinite series?,"This is an identity for the sum of combinatorial numbers. What techniques are used to prove it? \begin{align*}
\dfrac{1}{4^n}\sum _{i=0}^n \frac{1}{2 i+k}\binom{2 i}{i} \binom{2 n-2 i}{n-i}=\dfrac{(k-2)\text{!!}}{(k-1)\text{!!}} \cdot\frac{(2n+k-1)\text{!!}}{(2n+k)\text{!!}}
\end{align*} I met this identity when expanding the series, but how to prove it? \begin{align*}
S&=\dfrac{1}{k}\sum _{n=0}^{\infty}\frac{\prod _{j=1}^n (2 j+k-1)}{\prod _{j=1}^{n} (2j+k)}x^{2n+k}\\
&=\dfrac{(k-2)\text{!!}}{(k-1)\text{!!}}\sum _{n=0}^{\infty} \frac{(2n+k-1)\text{!!}}{(2n+k)\text{!!}}x^{2n+k}\\
&=\sum _{n=0}^{\infty} \frac{x^{2 n+k}}{4^n}\sum _{i=0}^n \frac{1}{2 i+k}\binom{2 i}{i} \binom{2 n-2 i}{n-i}\\
&=\left\{\sum _{m=0}^{\infty}\frac{(2m-1)\text{!!}}{\left[(2m)\text{!!}\right](2m+k)}
 x^{2m+k}\right\}\left[\sum _{m=0}^{\infty}\frac{(2m-1)\text{!!}}{(2m)\text{!!}}
 x^{2m}\right]\\
&=\left(\int_0^x \frac{t^{k-1}}{\sqrt{1-t^2}} \,\mathrm{d}t\right)\left(\dfrac{1}{\sqrt{1-x^2}}\right)\\
\end{align*}","['summation', 'binomial-coefficients', 'combinatorics']"
4461553,"Find $X,Y\subseteq \Bbb{R}$ such that there are $f\colon X\to Y$ and $g\colon Y\to X$, both bijective and continuous but $X\not\cong Y$.","I came across the following problem: Let $\Bbb{R}$ be the euclidean space. Find $X,Y\subseteq \Bbb{R}$ such that there are maps $f\colon X\to Y$ and $g\colon Y\to X$ , both bijective and continuous but $X\not\cong Y$ ( $X$ not homeomorphic to $Y$ ). The structure for a proof that I'm thinking is fairly obvios: Find the sets, find the maps, find a topological invariant that one holds that the other doesn't, QED. Easier said than done. I tried the sets $\Bbb{R}$ , $[0,1]$ but can't really find a bijective, continuous map (of course, the points $a,b$ are the ones that are causing trouble since I allready know how to do these for the set $(a,b)$ , of course this set doen's work since $\Bbb{R}\cong (a,b)$ ). I'm not being able to make much progress in this one. Any hints on how to find the sets/maps are apreciated.","['continuity', 'general-topology', 'open-map', 'real-analysis']"
4461581,Is the convexity radius continuous?,"Suppose $(M, g)$ is a Riemannian manifold. For each $x \in M$ define the $\textbf{convexity radius of $M$ at
  $x$}$ , denoted by $\text{conv}(x)$ to be the supremum of all $\epsilon > 0$ s.t. there is a geodesically
convex geodesic ball $\mathcal B_{\epsilon}(x)$ . By geodesically convex we mean each pair of points in $\mathcal B_\epsilon(x)$ can be connected by a unique minimizing geodesic staying within the ball. How to prove that $\text{conv}(x)$ is continuous? First, I wanted to prove upper-semicontinuity: Suppose $\text{conv}(x_n) \ge c$ for a sequence $x_n \to x$ . Pick $y, z \in B_c(x)$ and let $d = \max\{\text{d}(y, x)), \text{d}(z, x)\}$ . Find $n$ large enough so that $\text{d}(x_n, x) < \frac{1}{2}(c' - d)$ for some $c' \in (d, c)$ . Then \begin{align*}
  \text{d}(y, x_n) &\le \text{d}(y, x) + \text{d}(x, x_n) < d + \frac{1}{2}(c' - d) = \frac{1}{2}(c' + d) < c',
\end{align*} and similarly for $z$ . It follows that $y, z \in \mathcal B_{c'}(x_n)$ and by hypothesis we can find a unique
geodesic $\gamma \in \mathcal B_{c'}(x_n)$ connecting $y$ and $z$ which satisfies \begin{align*}
\text{d}(\gamma(t)), x) &\le \text{d}(\gamma(t), x_n) + \text{d}(x_n, x) < c' + \frac{1}{2}(c' - d).
\end{align*} If we chose $c'$ close enough to $d$ so that $\frac{1}{2}(c' - d) < c - c'$ , we would that $\gamma$ never
leaves $B_c(x)$ , so all points in this ball can be connected by a minimizing geodesic. If $\alpha : [0, T]$ were
another such geodesic, then we could just let $d = \max_{t \in [0, T]} \text{d}(\gamma(t), x)$ and repeat the
argument above to show that $\alpha$ is contained in some geodesically convex $\mathcal B_{c'}(x_n)$ , and this
would in turn force $\alpha = \gamma$ and establish the uniqueness necessary for $B_c(x)$ to be a geodesically
convex ball. Here I run into problems because part of the definition is that $B_c(x)$ must be a geodesic ball, so $\exp_x$ must be a diffeomorphism onto it. I know $\exp_x$ is bijective, but this does not necessarily mean its inverse is smooth. Is there a way to apply the global rank theorem to complete this part? Also, for lower semicontinuity, all I managed to show is that every ball centered sufficiently close to $x$ can have two of its points connected by a unique minimizing geodesic (not necessarily within the ball itself). How does one prove the lower semicontinuity of $\text{conv}(x)$ ?","['geodesic', 'convex-analysis', 'riemannian-geometry', 'differential-geometry']"
4461587,Does the question that every group of order 77 must have an element of order 7 and an element of order 11 have an elementary proof,"Question 21 of Chapter 10 of Abstract Algebra by Dan Saracino is as follows: Prove that every group of order 77 must have an element of order 7 and an element of order 11. The more general case of this question is to prove a group of order $pq$ must have an element of order $p$ and an element of order $q$ , where $p$ and $q$ are distinct primes. I have seen this question on this site with so many solutions. However, none of them works for me because, up to Chapter 10, Sylow Theorem, Cauchy Theorem, homomorphism, quotient group, and even normal subgroups have not been introduced yet. Hence, there should be an elementary proof for Question 10.21 of Dan Saracino's book, but I do not know whether this elementary proof can also work for the general case or not. I have a proof for the general case, but I know there is a problem with this proof that I will mention after the proof: My proof Suppose $G$ is our group such that $|G|=pq \ ;\ p>q$ . There are two possibilities: $G$ is cyclic. $G$ is not cyclic. In the first case, we can consider $G=\langle x\rangle $ , so $\langle x^p\rangle $ and $\langle x^q\rangle $ are subgroups of order $q$ and $p$ , respectively. In the second case, we have 3 possibilities: All elements have order $p$ . All elements have order $q$ . Some elements have order $p$ some others have order $q$ . Now, we are showing that the first and second possibilities in this case can not exist, and so only the third one is acceptable. If all elements are of order $p$ : There is an element $x$ with order $p$ . $G= \{e,x,x^2,\cdots,x^{p-1}, \cdots \}$ . Now, we only know p elements of this group. What about the other elements? So, we name another element $y$ such that $y \not\in \langle x\rangle $ . According to the assumption, the order of $y$ should be $p$ . So $G=\{e,x,x^2,\cdots,x^{p-1},y,y^2,\cdots,y^{p-1} \cdots \}$ . You should know that $\langle x\rangle \cap \langle y\rangle = \emptyset$ . We know that the group should contain the product of every two elements, and since $x^iy^j \ne x^ry^s$ for all $i \ne r $ or $ j \ne s$ , $G$ should contain at least $p^2$ elements.  This is a contradiction because $p^2 > pq$ . If all elements are of order $q$ : With a same argument, we can say $G=\{e,x,x^2,\cdots,x^{q-1},y,y^2,\cdots,y^{q-1} \cdots \}$ . Now, $G$ should contain $q^2$ elements which are the product of elements of $\langle x\rangle $ and $\langle y\rangle $ . These $q^2$ elements form a subgroup in $G$ . Indeed the subgroup is denoted by $\langle x,y\rangle $ (i.e. elements which are generated by $x$ and $y$ ). $|\langle x,y\rangle|=q^2$ , but, according to the lagrange theorem, since $q^2$ does not devide $pq$ , this is impossible to have such a subgroup, and we reach a contradiction. Hence, $G$ should contain both subgroups of order $p$ and $q$ . The problem with this proof is that, unless I know elements of $\langle x\rangle $ can commute with elements of $\langle y\rangle $ , I cannot say $|\langle x,y\rangle|=q^2$ . Now, can anyone say how I should deal with this problem? If not, can anyone give me an elementary proof for the general case without using Sylow Theorem, Cauchy Theorem, homomorphism, quotient group, and normal subgroups? If not, can anyone give me an elementary proof for the case when $|G|=77$ without using Sylow Theorem, Cauchy Theorem, homomorphism, quotient group, and normal subgroups?","['alternative-proof', 'group-theory', 'abstract-algebra', 'finite-groups']"
4461668,"General inverse for a symmetric matrix of the form $\mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right)$","I am looking for a general inverse formula for a symmetric matrix of the form $$ \mathbf A_n = \left(a_{\min(i,j)}b_{\max(i,j)}\right) = \pmatrix{ a_1b_1 & a_1b_2 & a_1b_3 & \cdots & a_1b_n \\
a_1b_2 & a_2b_2 & a_2b_3 & \cdots & a_2b_n \\
\vdots &    & \ddots  &        & \vdots \\
a_1b_n & a_2b_n &   &        & a_nb_n} $$ where $a_i,b_i $ are real numbers.  Empirically I can see that $ \mathbf A_n^{-1} $ is tridiagonal so I suspect it has previously been studied.  For example we have $$ \mathbf A_5 =
\left(
\begin{array}{ccccc}
 \frac{a_2}{a_1 a_2 b_1-a_1^2 b_2} & \frac{1}{a_1 b_2-a_2 b_1} & 0 & 0 & 0 \\
 \frac{1}{a_1 b_2-a_2 b_1} & \frac{a_1 b_3-a_3 b_1}{\left(a_2 b_1-a_1 b_2\right) \left(a_2 b_3-a_3 b_2\right)} & \frac{1}{a_2 b_3-a_3 b_2} & 0 & 0 \\
 0 & \frac{1}{a_2 b_3-a_3 b_2} & \frac{a_2 b_4-a_4 b_2}{\left(a_3 b_2-a_2 b_3\right) \left(a_3 b_4-a_4 b_3\right)} & \frac{1}{a_3 b_4-a_4 b_3} & 0 \\
 0 & 0 & \frac{1}{a_3 b_4-a_4 b_3} & \frac{a_3 b_5-a_5 b_3}{\left(a_4 b_3-a_3 b_4\right) \left(a_4 b_5-a_5 b_4\right)} & \frac{1}{a_4 b_5-a_5 b_4} \\
 0 & 0 & 0 & \frac{1}{a_4 b_5-a_5 b_4} & \frac{b_4}{a_5 b_4 b_5-a_4 b_5^2} \\
\end{array}
\right)
$$ which seems to be based on a bunch of 2x2 determinants... Any help appreciated, thank you! p.","['matrices', 'determinant', 'inverse']"
4461694,Formula to inscribe nested polygons tilted by $x^\circ$?,"I created these images way back in 2019 before I learned programming: Now I am trying to programmatically generate images like them using Python. Basically, I want to make something like this: In short, you start off with a regular polygon with n sides, then you create new regular polygon of the same number of sides such that the vertices of the new polygon are on the sides of the original polygon, and each of the sides of the new polygon and the corresponding side in the original polygon form an angle of x degrees. Then you create a new regular polygon with n sides whose vertices are on the second polygon and the pairs of corresponding sides also form an angle of x degrees. And then you recursively do this until a certain number of iterations is reached. I want to know given the number of sides of the polygon, and the angle of rotation, how to calculate the corresponding distance the vertices need to be displaced along its sides. For example, I have calculated the cases of squares and equilateral triangles: For the square, let its side be s, the angle be a, it is easy to see the two legs of the right triangle formed by the rotated side and the original sides add together to s. let x be the distance needed, let y be s - x. It is easy to see y = x / tan(a) x+y=s
y=x/tan(a)
x+x/tan(a)=s
x = s*tan(a)/(tan(a)+1) So the vertices need to be displaced along the sides by s*tan(a)/(tan(a)+1) For the equilateral triangle it is harder to calculate the distance needed, but if you draw a line from one of the rotated vertices perpendicular to its corresponding side in the original triangle, the right triangle formed by the operation will always contain a 30° angle. Let x be the amount of displacement needed, and let y be s - x, x can be calculated with the following: x + y = s
y = x/2 + x*cos(30°)/tan(a)
x + x/2 + x*cos(30°)/tan(a) = s
3x/2 + x*cos(30°)/tan(a) = s
x*(3/2+cos(30°)/tan(a))=s
x=s/(3/2+cos(30°)/tan(a)) Unfortunately my knowledge of geometry stops here, I can't calculate the amount of displacement of the vertices along the sides needed for regular polygons with more sides. I know how to calculate the coordinates of vertices of regular polygon of given number of sides, I just don't know the amount of displacement corresponding to angles. Can anyone generalize the calculations I used? Fixed logical reasoning errors in the question body, I was busy with other things so I didn't notice the error until now. I did it! Code","['geometry', 'rotations']"
4461719,Is the following relation concerning $2D$ integrals true?,"Fix $d,t>0$ and let $a \in L^2[-d,0]$ and $Y \in L^2[-d,t]$ . Is it true via some change of variable the following relation? $$\int_{(-t) \vee (-d)}^0 \int_{-d}^s a(\xi)Y(\xi-s)d\xi d s+\int_0^t\int_{-d}^{0} I_{[-(t-s),0]}(\xi)a(\xi)Y(s) d \xi ds=\int_0^t\int_{-d}^{0} a(\xi)Y(s+\xi) d \xi ds$$","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4461738,Showing that positive central difference limit implies function is strictly increasing,"Suppose $f:(a,b)\rightarrow \mathbb{R}$ is continuous, and that $$\lim_{h \rightarrow 0} \frac{f(x+h)-f(x-h)}{2h}$$ Exists for all $x\in(a,b)$ and is strictly greater than zero. How can I show that $f$ must be strictly increasing? I am comfortable with the usual MVT proof that a positive derivative implies a function is increasing, however, in this case, I cannot assume that the function is differentiable - so can't use the MVT in its standard form at least. I've had two ideas: Go right back to foundations and show that the MVT still holds in this case - by showing that Rolle's theorem still holds (does it?). Use the definition of a limit directly with an $\epsilon - \delta$ argument, showing that for all $|h|<\delta$ , $\frac{f(x+h)-f(x-h)}{2h}>0$ so somehow the function must be increasing. Any thoughts?","['limits', 'derivatives', 'mean-value-theorem', 'real-analysis']"
4461740,"How to solve $A_xB_y+A_yB_x=0$, $A,B \in \mathbb{C}[x,y]$?","Let $A,B \in \mathbb{C}[x,y]$ be arbitrary polynomials of two variables with coefficients in $\mathbb{C}$ . Assume that $A_xB_y+A_yB_x=0$ ,
where $\cdot _x$ is the partial derivative with respect to $x$ and $\cdot_y$ is the partial derivative with respect to $y$ . Question 1: Is it possible to find a general solution to such equation? I really apologize if this is an easy question; I am not familiar with the theory of solving such equations. An example: $A=x+y, B=x-y$ .
Here $A$ is symmetric and $B$ is skew-symmetric
w.r.t. the involution $(x,y) \mapsto (x,-y)$ , but general $A$ symmetric and $B$ skew-symmetric do not work, for example $A=x^2+y^2, B=x-y$ . Question 2: Replace $A_xB_y+A_yB_x=0$ by $A_xB_y-A_yB_x=0$ .
What is the answer in this case? Question 3: Let $h \in \mathbb{C}[x]$ , $h$ is any polynomial in one variable $x$ .
What is the solotion to $(h-A_x)B_y+A_yB_x=0$ ? to $(h-A_x)B_y-A_yB_x=0$ ? Edit: If an answer for a general $h$ is too complicated, I do not mind to assume that $h=1$ , so Question 3 becomes:
What is the solotion to $(1-A_x)B_y+A_yB_x=0$ ? to $(1-A_x)B_y-A_yB_x=0$ ? Now let us consider $A_xB_y-A_yB_x=-B_x$ , namely, $A_xB_y=(A_y-1)B_x$ (in qeustion 3, I have asked about $A_xB_y-A_yB_x=B_y$ , but these are analog cases). A possible solution is: $A=(x+y)^3+y, B=(x+y)^2$ . More generally, if I am not wrong, $A=f(C)+y, B=g(C)$ ,
where $C \in \mathbb{C}[x,y]$ , $f,g \in \mathbb{C}[T]$ . Question 4: Is it true that $A=f(C)+y, B=g(C)$ is a general solution to $A_xB_y=(A_y-1)B_x$ ?
If not, is it possible to describe all additional solutions? See also this question. Thank you very much!","['jacobian', 'polynomials', 'partial-derivative', 'derivatives', 'commutative-algebra']"
4461750,On a decomposition of pairs in the transitive closure $T$ of an arbitrary relation $R$,"Let $S$ be some set, and let $R$ be a relation with $\operatorname{dom}(R) = \operatorname{ran}(R) = S$ .  Hence $R \subseteq S \times S$ .  Let $T$ be the transitive closure of $R$ , defined as the smallest transitive relation that contains $R$ .  Or, symbolically: $$T = \bigcap\, \{U:  R \subseteq U\subseteq S\times S \; \wedge \; U\,\mathrm{is\;transitive} \}.$$ (Since $S \times S$ itself is a transitive relation, the argument of the $\bigcap$ operator above is nonempty.) I recently came across the following assertion, which I think is correct, but have a hard time proving. Claim: If $x, y \in S$ and $xTy$ , then either $xRy$ or there exists a $z \in S$ such that $xTz$ and $zRy$ . I think that this claim is true, because I visualize the process of creating the transitive closure of a relation as one of ""recursively adding the pair $(a, c)$ whenever the pairs $(a, b)$ and $(b, c)$ are available, until no more pairs can be added.""  Therefore, if (1) the transitive closure $T$ of $R$ includes the pair $(a, c)$ , and (2) it is not the case that $a R c$ , then there must exist some $b \in S$ such that $aTb$ and $bTc$ .  This is not too far from the stronger claim that the $b$ is such that not only $bTc$ , but that $bRc$ . But this is all hand-waving, based on my intuitive vision of some imaginary ""process"" whereby one ""generates"" the transitive closure of a relation. The problem of proving the claim above is made all the more difficult by the fact that I came across it in the course of solving a problem on Zermelo ordinals in a textbook of set theory, and this problem occurred before the autor had gotten around describing even an order relation for Zermelo ordinals, let alone a well-ordering for them, or any notion of induction or recursion based on them 1 . In fact, for the textbook's problem, all one could assume were Zermelo's original Axiom of Infinity (i.e. $\exists w [\varnothing \in w \ \wedge \ \forall x\in w (\{x\} \in w)]$ ) together with the remaining currently standard ZF axioms 2 . Without recursion, induction, well-ordering, etc. I don't know even where to begin proving the claim above. Of course, I understand that $(R \subseteq T) \leftrightarrow ((x, y) \in R \to (x, y) \in T) \leftrightarrow (xRy \to xTy)$ .  Therefore, I see that if $xTy$ , one possibility is that $xRy$ .  My problem is to show that $$(xTy \wedge \lnot xRy)\to \exists z \in S(xTz \wedge zRy)$$ EDIT: A different way to phrase the same problem uses the notion of composition of relations.  The composition $G \circ F$ of relations $G$ and $F$ is defined as $$G \circ F := \{(x, y) \in \operatorname{dom}(F) \times \operatorname{ran}(G): \exists z[(x, z) \in F \wedge (z, y) \in G]\}.$$ This notation allows the following characterization of transitivity: a relation $U$ is transitive iff $U\circ U \subseteq U$ . This notation also can also be used to give an equivalent formulation of the claim above as an assertion about set inclusion: $$ T \subseteq R \cup (R \circ T).$$ It is not hard to show the opposite inclusion: $$R \subseteq T \to R\circ T \subseteq T \circ T = T \to R \cup (R \circ T) \subseteq T,$$ ...so proving the desired conclusion amounts to proving that $$ T = R \cup (R \circ T).$$ 1 In fact, the claim about transitive closures that this post is about arose in the context of proving that the transitive closure of the $\in$ relation could serve as a well-ordering for the set $z$ of Zermelo ordinals.  In contrast to what is the case with the now-standard von Neumman ordinals, where for any two distinct ones of them, $m$ and $n$ , either $m \in n$ or $n \in m$ , with Zermelo ordinals, the $\in$ relation holds only between such an ordinal $n$ and its successor $\{n\}$ .  Hence, $\in\!\!|_z\subseteq z \times z$ is not even an partial order for $z$ (since it is not transitive), let alone a well-ordering for it. 2 Extensionality, Foundation, Comprehension/Separation, Pairing, Union, Replacement, and Power Set.","['elementary-set-theory', 'order-theory', 'well-orders', 'relations']"
4461809,Negative moment of the number of arrivals in a renewal process,"Suppose $\{X_1, \ldots, X_n,\ldots\}$ are i.i.d. non-negative random variables with $\mathbb{E}[X_1]= \mu <\infty$ , and $N(t):= \sup\{k: \sum_{i=1}^k X_i < t\}$ . Notice that $X_i$ may not have higher order moments. The classical renewal theorem tells us $$
\frac{\mathbb{E}[N(t)]}{t}\rightarrow \frac{1}{\mu}
$$ as $t\to \infty$ . I am interested in whether: $$
\mathbb{E}\left[\frac{t}{\max\{N(t),1\}}\right]\to \mu.
$$ as $t\to \infty$ ? Or at least is $$
\limsup_{t\to \infty}
   \mathbb{E}\left[\frac{t}{\max\{N(t),1\}}\right]
\leq C\mu
$$ for some constant $C$ ? (From Jensen's inequality one can easily see $
\liminf_{t\rightarrow\infty } \mathbb{E}[\frac{t}{N(t)}]\geq \mu.)
$ Thanks!","['stochastic-processes', 'renewal-processes', 'probability-theory', 'probability']"
4461947,Elementary proof of $\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty$,"This question is closely related to this question , but I am not happy with the answers there for several reasons which I will explain in a second. The limit $\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty$ , where $n$ is a natural number, is easy to see by expanding the left side with the help of the exponential series. Indeed, we have $$
n(\sqrt[n]{n}-1)=\ln(n)+\frac{1}{2}\cdot\frac{1}{n}\cdot\ln(n)^2+\frac{1}{6}\cdot\frac{1}{n^2}\cdot\ln(n)^3+\cdots\geq\ln(n)\,.
$$ Since $\ln(n)$ grows arbitrary large with $n$ large, the limit is proven. I found this limit as an exercise in Analysis 1 by K. Königsberger, 5.8 Exercises, 3(b). I am using an old printing and the numbering might have changed, but it is in the very beginning of the book in a chapter about sequences. At that stage of the book the exponential series as well as logarithms have not yet been introduced and very few means are available. For educational purposes, I am looking for a really elementary proof which uses a different bound from below which in turn goes to infinity. The book suggests that such a proof must exist but I cannot find one. Can you please help me to find such proof? What is available at this stage is the Bernoulli inequality and the expansion of $(1+x)^n$ for a natural number $n$ and arbitrary $x$ , plus some very basic limits like $\sqrt[n]{n}\to 1$ , etc. which all can be done elementary. Thank you for your time and help!","['limits-without-lhopital', 'analysis', 'real-analysis', 'sequences-and-series', 'limits']"
4462015,Winning strategy for an asymmetric game,"Two players, $\text{A}$ and $\text{B}$ , are playing an asymmetrical game. There are $n$ points on the game board. Each turn player $\text{A}$ targets a pair of points and player $\text{B}$ says whether those two points are connected or unconnected. $\text{A}$ can target each pair only once and the game ends when all pairs have been targeted. Player $\text{B}$ wins iff a point is connected with all other points on the very last turn, while player $\text{A}$ wins if any point is connected with all other points on any turn but the very last one OR if no point is connected to all other points after the last turn. For what values of $n$ does either player have a winning strategy? Clarifications on the question: $\text{B}$ gets to decide if a pair of points targeted by $\text{A}$ are $\text{connected}$ or $\text{unconnected}$ . The graph of the points isn't important, as any pair of points on the board may be targeted. My attempt: For $n = 2,3$ , $\text{B}$ has trivial winning strategies of answering $(\text{connected})$ and $(\text{connected, unconnected, connected})$ , respectively. For $n > 3$ , $\text{A}$ can naïvely approach the game by asking one-by-one if all the points from $2$ to $n$ are connected to $1$ , then if all the points from $3$ to $n$ are connected to $2$ , and so on. $\text{B}$ 's winning strategy is: answer $(\text{connected})$ for all pairs $\text{(i, i + 1)}$ to $\text{(i, n - 1)}$ for point $\text{i}$ , but answer $\text{(unconnected)}$ for the pair $\text{(i, n)}$ . At the last turn, $\text{A}$ targets $\text{(n - 1, n)}$ . $\text{B}$ can answer $\text{(connected)}$ and win, as point $\text{n - 1}$ will then be connected to all other points on the game board. Clearly, $\text{A}$ 's naïve approach isn't a winning strategy. But this does suggest that $\text{B}$ loses any chance of winning mid-game if all the points are $\text{unconnected}$ to at least one other point, because then no point can be connected to all the other points at the last turn. Hence, $\text{B}$ should try to answer $\text{connected}$ for all pairs $\text{(k, i)}$ for at least one point $\text{k}$ , so that $\text{k}$ may be $\text{connected}$ to another point in the last turn. However, I can't think of general strategies $\text{B}$ might try that would achieve this.","['number-theory', 'puzzle', 'combinatorial-game-theory']"
4462022,How many numbers between 4000 and 9999 have sum of digits equal to ten (why is exponential generating functions not correct)?,"How many numbers in between 4000 and 9999 have sum of digits equal to ten? My attempt My thinking was to use exponential generating functions because 4510 is not equal to 5401 so the order should matter, which would imply (from what I have learned) that exponential generating functions is the correct one. So I set it up as $(\frac{x^4}{4!}+\frac{x^5}{5!} \dots \frac{x^9}{9!})*(\frac{1}{0!} + \frac{x}{1!} \dots \frac{x^9}{9!})^3$ But based on How many numbers between $100$ and $900$ have sum of their digits equal to $15$? and my professor as well, one should use ordinary generating functions, i.e. $(x^4 + x^5 \dots x^9)*(1 + x \dots x^9)^3$ Why is it not correct to use exponential generating functions for this problem, and why is it correct to use ordinary generating functions? In this case, what would be question that the exponential generating function answers vs. what would be the question that the ordinary generating function answers? I question in a similar vein is also Kind of basic combinatorical problems and (exponential) generating functions","['combinatorics', 'discrete-mathematics', 'generating-functions']"
4462033,"Can I prove that $f$ convex $\implies$ $f$ continuous using a proof by all possible cases of discontinuity, showing contradiction in each one?","Given the following definitions of a convex function from Spivak's Calculus: Definition 1: A function $f$ is convex on an interval, if for all $a$ and $b$ in the interval, the line segment joining $(a,f(a))$ and $(b,f(b))$ lies above the graph of $f$ . Definition 2: A function $f$ is convex on an interval if for all $a, x$ , and $b$ in the interval with $a<x<b$ we have $\frac{f(x)-f(a)}{x-a}<\frac{f(b)-f(a)}{b-a}$ . First question: Does Definition 2 mean that $f$ has to be defined everywhere in an interval to be convex in that interval? Second question: Is it possible to prove that such a function is always continuous? If so, how? Here is my attempt at a proof. I'm going to outline the proofs, without actually proving using $\epsilon$ and $\delta$ proofs as would be required to be rigorous. If the general idea of each case below is sound, then I think the corresponding rigorous proofs are relatively simple. Let $f$ be convex. Assume $f$ is discontinuous at some point $a$ , ie $\lim\limits_{x \to a} f(x) \neq f(a)$ . My proof strategy is to go through each of the possible ways that $f$ could be discontinuous at $a$ and show that they violate convexity of $f$ . Removable discontinuities aren't possible if $f$ must be defined at every point in an interval to be convex. Jump discontinuities aren't possible for a convex function, due to the following two situations: In both cases above, the light red line has a slope that is larger than the slope of the blue line, and this violates convexity. Essential discontinuities aren't possible because the slope between any point $x_1<a$ and a point $a+h$ can be made arbitrarily high. Just choose an $h_1>0$ and an $h_2<0$ , let the slope between $x_1$ and $a+h_1$ be some value. Then we can always find an $a+h_2$ with a larger slope, violating convexity as in the picture below. Finally, consider the case of a point discontinuity Whether $f(a)>\lim\limits_{x \to a} f(x)$ or $f(a)<\lim\limits_{x \to a} f(x)$ , convexity is violated. In all possible cases, $f$ turns out not to be convex, a contradiction. Therefore, by proof by contradiction, we conclude that $f$ must be continuous at every point. Is this approach (proof by possible cases of discontinuity, showing contradiction in each one) correct?","['continuity', 'calculus', 'solution-verification', 'derivatives']"
4462051,Summability Condition for Linear Process,"I have been studying linear processes in time series of the following form: $X_t= \sum_{j=0}^{\infty}c_j\epsilon_{t-j}$ In this case, the following implication (about the covariance function) holds: $ \sum_{j=1}^{\infty} j|c_i| < \infty \Rightarrow \sum_{j=0}^{\infty} j|\gamma_i|   < \infty $ However, I have some trouble understanding the proof which runs us follows: $ \sum_{j=0}^{\infty} j^p|\gamma_i| \leq \sigma^2 \sum_{i=0}^{\infty} |c_i|\sum_{j=0}^{\infty} j^p|c_{i+j}| \leq  \sigma^2 \sum_{i=0}^{\infty} |c_i|\sum_{j=0}^{\infty} (i+j)^p |c_{i+j}| \leq \sigma^2 \sum_{i=0}^{\infty} |c_i|\sum_{j=0}^{\infty} j^p |c_{j}| < \infty$ I do not understand the following part (the last inequality): $ \sigma^2 \sum_{i=0}^{\infty} |c_i|\sum_{j=0}^{\infty} (i+j)^p |c_{i+j}| \leq \sigma^2 \sum_{i=0}^{\infty} |c_i|\sum_{j=0}^{\infty} j^p |c_{j}| < \infty$ Can anyone please help me out and explain why it holds?","['time-series', 'statistics', 'variance']"
4462081,Help solve $(1+\cos(a)+i \sin(a))^n$,"I actually already have the solution to the following expression, yet it takes a long time for me to decipher the first operation provided in the answer. I understand all of the following except how to convert $\left(1+e^{i\theta \ }\right)^n=\left(e^{\frac{i\theta }{2}}\left(e^{\frac{-i\theta }{2}}+e^{\frac{i\theta }{2}}\right)\right)^n$ I am not sure if I wrote the expression correctly, I am new to this website. Thank You!",['complex-analysis']
4462084,$\int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1}$,"I want to show that Let $\alpha$ be a positive number. Then $\int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1}$ for $0\leq a<b$ I understand that it can be directly proved using fundamental theorems of calculs.
Instead, I should use the fact that $(\alpha+1)s^{\alpha}<\frac{t^{\alpha+1}-s^{\alpha+1}}{t-s}<(\alpha+1)t^{\alpha}$ for $0\leq a<b$ .
I think I can prove the below fact by using the mean value theorem, however I can't see how the two facts are related.
Any suggestions please?","['integration', 'analysis', 'real-analysis', 'calculus', 'riemann-integration']"
4462132,Generalised circles in the context of Moebius transformations,I‘m doing complex analysis at the moment and while speaking about Möbius transformations we introduced the notion of a generalized circle. It seems to be a really important construct while discussing Möbius transformations since this transformations have good properties w.r.t. generalized circles. But somehow I don‘t know how to think about this generalized circles since our Prof. told us they are not circles as in the usual way. Could  maybe someone take some time and explain me how to think about this and also maybe how I get an intuition? Thanks for your help.,"['complex-analysis', 'functional-analysis', 'analysis', 'linear-fractional-transformation']"
4462190,Find the ratio of areas in the $ABP$ and $PMCQ$ regions.,"For reference: In the figure, $(BM):(AQ)=(MC):(QC)$ . calculate the ratio
of areas in the $ABP$ and $PMCQ$ regions. My progress $\frac{S_{ABP}}{S_{PMCQ}}=?\\
\frac{x}{y}=\frac{m}{n}\\
\\
\frac{S_{ABM}}{S_{AMC}}=\frac{x}{y}\\
\frac{S_{ABQ}}{S_{BQC}}=\frac{m}{n}\\
\frac{S_{ABM}}{S_{AMC}}=\frac{S_{ABQ}}{S_{BQC}}$ I couldn't develop from here.","['euclidean-geometry', 'geometry', 'plane-geometry']"
4462191,How to make this proof (?) (with AC) of $|X\times X| = |X|$ for infinite $X$ rigorous.,"First of all, I do not want to make the statement that the following proof has any interest on its own: it's probably horrendous, in the sense that it's completely overkill, if a proof at all. It's only that I think it may be interesting for a better understanding of FOL and Set Theory to make it rigorous. The idea is the following: First we prove that PA proofs that $f(x,y) := \frac{1}{2}(x+y)(x+y+1) + x$ is a bijection, in the sense that it proofs $\forall n(!\exists x,y(\frac{1}{2}(x+y)(x+y+1) + x = n))$ . Then we argue by the upward Löwenheim–Skolem theorem that, since PA has a countable model, it has a model of every infinite cardinal, and thus, it must be that $|X \times X| = |X|$ for all infinite $X$ . I'm pretty sure there are many things to make precise. For example, I'm pretty sure one has to appeal to the soundness theorem at some point. These would be the things I would like to understand better. Anyway I hope this isn't too nonsensical.","['elementary-set-theory', 'logic', 'first-order-logic']"
4462194,Every polynomial sequence satisfies a linear recurrence?,"I noticed an interesting pattern, which is that the sequence $a_n = n^2$ satisfies the recurrence relation $1a_{n-3} - 3 a_{n-2} + 3 a_{n-1} - 1a_n = 0.$ This is an alternating weighted sum of 4 consecutive terms in the sequence, with the weights taken from Pascal's triangle [1,3,3,1]. I noticed that the pattern seems to extend to other polynomials, such as $a_n=n$ (with $1a_{n-2} - 2 a_{n-1} + 1a_n = 0$ ) and $a_n = n^3$ (with $1a_{n-4} - 4a_{n-3} + 6a_{n-2}-4a_{n-1}+1a_n = 0)$ . I am looking for a (hopefully elegant) proof that this pascal-weighted alternating sum recurrence holds for any polynomial sequence $a_n = n^p$ , with an explanation for why the pascal weights in particular appear.  I know several facts that are potentially useful, but I haven't seen how to put them together. The integer points of every degree $d$ polynomial have constant order- $d$ successive differences, for the same reason that in a continuous setting, the $d$ th derivative is constant. This means that some fixed number of previous terms in the sequence convey enough data about the various-order differences to construct the next term in the sequence. The terms from Pascal's triangle represent the coefficients of the binomial expansion of $(x+y)^n$ . Presumably this alternating sum causes a bunch of nice cancellations, but I don't yet see how. The terms from Pascal's triangle are also recursive, in that each row is generated as the sum of the two neighboring terms in the previous row. In short, why does it seem that: $$f(x)=\sum_{k=0}^{p+1} {p+1\choose k} (-1)^{k} (x-k)^p \stackrel{?}{\equiv} 0. $$","['algebra-precalculus', 'linear-algebra', 'recurrence-relations']"
4462234,Let a point P move on a straight line according to the score shown on a fair dice that we throw by the following rules. P starts from the origin O.,"If the score is 6, the P returns to the origin O.
If the score is 1,2, or 3, then P moves 1 in a positive direction.
If the score is 4 or 5, then P moves 1 in a negative direction. When we throw the dice four times, the probablity that the point P is at the origin O is (??) At first glance, this doesn't look too hard since all it is is the P(being at origin), but when I tried to take the (cases where P is at the origin)/(the total amount of cases) I stumbled at how to calculate the total amount of cases. As for what I could figure out:
there are only 3 possible outcomes
a= anything, p = positive number, n = negative number 6 = rolling a 6 aaa6
a6pn
ppnn taking the probablity of all of these, I got $5^3/6^4$ , %5/6^3 $, and $ 1/6^2$. Taking all of that over the total cases seems fairly hard as it is, but how do I even begin to get the total amount of cases? I thought about multiplying 6 four times since we are rolling the dice 4 times, but that doesn't make since, since the total number of cases is just the total number of  permuations of 6. So i tried taking the n!/(n-1)! of this equation, and that didn't work either. Maybe I messed up my math somewhere, IDK. the answer is apparently: 7/18 All help is appreciated!","['statistics', 'probability-theory', 'probability']"
4462253,"Spivak Calculus, Ch 11, Problem 11a): Proof that if $f$ weakly convex, then [$f$ convex $\iff$ graph of $f$ contains no straight line segments].","Call a function $f$ weakly convex on an interval if for $a<b<c$ in this interval we have $$\frac{f(x)-f(a)}{x-a} \leq \frac{f(b)-f(a)}{b-a}$$ (a) Show that a weakly convex function is convex if and only if its
graph contains no straight line segments. (Sometimes a weakly convex
function is simply called ""convex"", while convex functions in our
sense are called ""strictly convex""). The proof I came up with differs from the solution manual, and I would like to verify if it is correct. Proof By assumption, $f$ is weakly convex. Assume $f$ is also (strictly) convex. Assume the graph of $f$ contains a straight line segment. Let $a<b<c$ be points on the line segment portion of $f$ 's graph. Then, $$\frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a}$$ But this contradicts our assumption that $f$ is strictly convex, which implies that $$\frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a}$$ Therefore, $f$ convex $\implies$ graph of $f$ contains no straight line segments. Now assume, for proof by contradiction, that the graph of $f$ contains no straight line segments. For any $a<b<c$ , we know that $\frac{f(b)-f(a)}{b-a} \leq \frac{f(c)-f(a)}{c-a}$ Assume that for some $a,b,$ and $c$ such that $a<b<c$ we have $\frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a}=\alpha$ . Let $x_2 \in (b,c)$ . Then by weak convexity $$\frac{f(x_2)-f(b)}{x_2-b}\leq \alpha\tag{1}$$ Let $x_1 \in (a,b)$ . Assume for proof by contradiction that $$\frac{f(x_1)-f(a)}{x_1-a}<\alpha\tag{2}$$ Then, $\alpha<\frac{f(b)-f(x_1)}{b-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \leq \frac{f(x_2)-f(b)}{x_2-b}$ . But this contradicts $(1)$ . Therefore, by proof by contradiction, $$\forall x, x \in (a,b) \implies \frac{f(x_1)-f(a)}{x_1-a}=\alpha$$ But then the graph of $f$ contains a straight line segment between $a$ and $b$ , which contradicts our initial assumption of no straight lines in the graph of $f$ . Therefore, by proof by contradiction, $$\frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a}$$ That is, $f$ is convex. Therefore, graph of $f$ contains no straight line segments $\implies f$ convex.","['calculus', 'solution-verification', 'derivatives']"
4462260,"Probability that $1,2$ are in the same set on randomly halving $\{1,...,10\}$?","Textbook problem: The integers from $1$ to $10$ inclusive are partitioned into two sets of five elements each. What is the probability that $1$ and $2$ are in the same set? My solution: $2/9$ . The total number of partitions would be $10 \choose 5$ . If $1$ and $2$ are in the same set, then there are $8 \choose 3$ ways left to create the set that contains them since two of the numbers have already been chosen.  Hence, the probability is ${8 \choose 3} / {10 \choose 5}$ which is $2/9$ . Textbook answer: $4/9$ . Question: What am I missing in my reasoning? Should I be doubling the number of ways to create a set with $1$ and $2$ in it for some reason? Textbook: The Art Of Problem Solving (Vol. 1) by Rusczyk, Chapter 26.","['solution-verification', 'combinatorics', 'probability']"
4462299,"Possible Textbook Error: Shouldn't $\bigcap_{q \in \mathbb{Q}^{\ge 0}} (-q,q) = \varnothing$?","Example $A.14$ (from page $391$ of Paolo Aluffi's Algebra: Notes from the Underground ) claims the following: Example $A.14$ : For $q$ a nonnegative rational number, let $(-q,q)$ denote the open interval in the real line with endpoints $−q$ and $q$ (as in calculus!). You can view this as a set $S_q$ determined by the chosen $q \in \mathbb{Q}^{\ge 0}$ . (Here $\mathbb{Q}^{\ge 0}$ denotes the set of nonnegative rational numbers.) Then you may verify that $$
\bigcup_{q \in \mathbb{Q}^{\ge 0}} (-q,q) = \mathbb{R} \qquad
\bigcap_{q \in \mathbb{Q}^{\ge 0}} (-q,q) = \{0\}
$$ When $(-q,q)$ denotes the open interval in the real line... the smallest possible interval would then be $(-0,0)$ for which there is no element... right?",['elementary-set-theory']
4462339,Find a $\mathbb F_p$ such that all polynomials doesn't has a root in it.,"Let $f_1,\cdots,f_r$ be a set of monic irreducible polynomials of degree $2$ in $\Bbb Z[x]$ . Can we find a prime number $p$ such that no $f_i$ has a root in $\Bbb F_p$ ? I have thought about the density, and I know that: for any $i$ , the set $p$ such that $f_i$ splits in $\Bbb F_p$ has density $\frac{1}{2}$ . But the density is too large, so I don't know how to get the result. I'm OK with algebraic number theory and basic analytic number theory, hence the methods using these two are welcome.","['analytic-number-theory', 'number-theory', 'algebraic-number-theory']"
4462369,How to simplify the calculations of reflecting a ray about an ellipse,"I wrote the script that made these images several days ago, the segments each depict a ray of light, as the light hits the boundary of the ellipse, it is reflected by the ellipse according to the laws of reflection, and the reflected ray of light is again reflected by the ellipse, and the reflection of the reflection is again reflected by the ellipse... The light keeps bouncing back and forth, over and over again, until the light has been reflected certain number of times. Step by step on how I made these images. First, you make an ellipse. $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ The above equation describes the points on the ellipse, it is also the boundary of the ellipse. (Assume the ellipse is horizontal). Construct a right triangle from one semi-minor axis and one semi-major axis, let the acute angle adjacent to the semi-major axis be $\alpha$ , then the relationship between a, b and $\alpha$ can be written as: $b = a \cdot tan(\alpha)$ Rewrite the equation of the ellipse: $\frac{x^2}{a^2} + \frac{y^2}{a^2 \cdot tan(\alpha)^2} = 1$ Pick a random point inside the ellipse, for all points inside by the ellipse, simply use this equation: $\frac{x^2}{a^2} + \frac{y^2}{b^2} <= 1$ Then the random point is given as: $(a \cdot cos(\alpha) \cdot m, b \cdot sin(\alpha) \cdot n)$ Where $\alpha$ is in range $[0, 2\pi]$ , and m, n are in range $[0, 1]$ . Then pick a random angle $\beta$ , to construct a line passes the chosen point at angle $\beta$ with the x-axis. I use the slope-intercept form
of line equation ( $y = k \cdot x + c$ ): Let the chosen point be $(x_0, y_0)$ , then the equation of the incident ray is: $y = tan(\beta) \cdot x + y_0 - x_0 \cdot tan(\beta)$ But if $\beta$ is a multiple of $\frac{\pi} {2}$ things get complicated, because tan(0) = 0 and $tan(\frac{\pi} {2})$ is undefined. So I use $y = y_0$ if the line is parallel to x-axis and $x = x_0$ if the line is perpendicular to x-axis. Now to calculate the intersections between the incident ray and ellipse. If: $c^2 < a^2 \cdot k^2 + b^2$ Then there can be two intersections $(x_0, y_0)$ and $(x_1, y_1)$ . $$\begin{align}
n_1 &= a^2 \cdot k^2 + b^2 \\
n_2 &= 2 \cdot a^2 \cdot k \cdot c \\
n_3 &= a^2 \cdot (c^2 - b^2) \\
n_4 &= \sqrt{(n_2^2 - 4 \cdot n_1 \cdot n_3)} \\
x_0 &= \frac{(-n_2 + n_4)} {(2 \cdot n_1)} \\
x_1 &= \frac{(-n_2 - n_4)} {(2 \cdot n_1)} \\
y_0 &= k \cdot x_0 + c \\
y_1 &= k \cdot x_1 + c
\end{align}$$ For lines like $x = x_0$ and $y = y_0$ however: First set: If $abs(x_0) <= a$ $y_0 = \sqrt{b^2 - x_0^2 \cdot \frac{b^2} {a^2}}$ Intersections are $(x_0, +y_0)$ and $(x_0, -y_0)$ . Second set: If $abs(y_0) <= b$ $x_0 = \sqrt{a^2 - y_0^2 \cdot \frac{a^2} {b^2}}$ Intersections are $(+x_0, y_0)$ and $(-x_0, y_0)$ Then choose the intersection to do further calculations, for $\beta$ in range $[0, \frac{\pi} {2}]$ and $[\frac{3 \pi} {2}, 2 \pi]$ I choose the right intersection, else I choose the left one. Then I calculate the tangent of the ellipse at the intersection. For a point $(x_0, y_0)$ on the ellipse given by a, b, the slope k of the line tangent to the ellipse at that point must satisfy: $k = \frac{-x_0 \cdot b^2} {a^2 \cdot y_0}$ Then for almost all lines, the equation of the tangential line is: $y = k \cdot x + \frac{b^2} {y_0}$ But if arctan(k) is a multiple of $\frac{\pi} {2}$ the above relationship breaks down and I instead fall back to constant form. Then I calculate the normal (line perpendicular to that tangent passing through that intersection): $y = \frac{-x} {k} + y_0 - \frac{-x_0} {k}$ Where $(x_0, y_0)$ is the intersection, and k is the slope of the normal. But again the above relationship breaks down if the line is special. I won't show how I deal with exceptions here, I already have shown too many equations, you can see all the calculations in the code. Then I calculate the signed angle formed by the incident ray and the normal: Let $k_1$ be the slope of the incident ray, let $k_2$ be the slope of the normal: $$\begin{align}
\alpha_1 &= atan(k_1) \\
\alpha_2 &= atan(k_2) \\
\alpha_\delta &= \alpha_2 - \alpha_1 \\
\alpha_\delta &= (\alpha_\delta + \pi) \bmod 2 \pi - \pi
\end{align}$$ Again, the above doesn't work if either of these lines are special, other calculations are required. Then I calculate the reflected ray, simply by rotate the normal line about the intersection by $\alpha_\delta$ (assuming the previous calculations succeeded): $$\begin{align}
\alpha_3 &= \alpha_2 + \alpha_\delta \\
y &= tan(\alpha_3) \cdot x + y_0 - tan(\alpha_3) \cdot x_0
\end{align}$$ Then I calculated the intersections between the reflected ray and the ellipse, there will be two intersections, this time the intersection needed is the other intersection from the current one. Then all above calculations are repeated recursively, until a certain number of iteration is reached. How to simplify all calculations involved, and calculate the reflected ray in as few steps as possible, including all the edge cases? I want it that way: given the coordinate of a point and an angle, calculate the intersection of the ray with the ellipse, and then calculate the tangent of the ellipse at that intersection, then calculate the reflected ray, all of these in as few steps as possible, using one set of equations without exceptions. Preferably the number of equations involved should be less than or equal to six.",['geometry']
4462381,"Find $\lim\limits_{(x,y)\to(0,0)}\frac{x^6+y^6}{x^3+y^3}$ using the $\epsilon-\delta$ definition?","My textbook asks the question $$f(x,y) = \frac{x^6+y^6}{x^3+y^3}$$ Does $f(x,y)$ have a limit as $(x,y) \rightarrow (0,0)$ ? I used polar coordinates instead of solving explicitly in $\mathbb R^2 $ , and it went as the following: $$ x = r \cos \theta, \qquad y = r\sin\theta $$ Hence, $$\lim_{(x,y) \to (0,0)} \frac{x^6+y^6}{x^3+y^3} = \lim_{r \to 0}\frac{{r^6\cos^6\theta + r^6\sin^6\theta}}{r^3\cos^3\theta + r^3\sin^3\theta}$$ This simplifies to, $$ \lim_{r \to 0} \frac{r^3({\cos^6\theta + \sin^6\theta})}{\cos^3\theta + \sin^3\theta}$$ Now from the above, we find that as r→ $0$ the limit is $0$ . So now i have got $0$ as a possible limit.I have tried to procceed further using $\epsilon-\delta$ definition but I cannot get anywhere. Any thoughts on how to prove that $0$ is in fact the limit?","['limits', 'multivariable-calculus', 'polar-coordinates', 'epsilon-delta']"
4462392,Problem related to composition of a function: $f^s(m)=tm$,"Consider a pair of positive integers $(s,t)$ .Does there exists a self map $f$ of $\mathbb{N}$ such that $f^s(n)=tn$ , $\forall n\in \mathbb{N}$ ?Here $f^s$ means composition of $f$ s times. $\textbf{My Solution:}$ Let 'p' be a prime co-prime with t and 'a' be a natural number satisfying exactly one of the following three conditions i) a is not a multiple of t nor a power of p ii) $a=p^{bs}$ ,where $b\in \mathbb{N}_0$ , $b\not\equiv0$ (mod t) iii) $a=p^{bs+k}$ ,where $b\equiv0$ (mod t), $0\leq k\leq s-1$ and $b\in \mathbb{N}_0$ We partition the natural numbers into three types and define a function $f$ as i) $at^n ,n\in\mathbb{N}_0$ and $f({at^n})=p^{as-1}t^n$ ii) $p^mt^n,n\in\mathbb{N}_0,m\in\mathbb{N}$ such that $m\not \equiv 0,1$ (mod s) and $m\not\equiv 0$ (mod t) and $f(p^mt^n)=p^{m-1}t^n$ iii) $p^mt^n,n\in\mathbb{N}_0,m\in\mathbb{N}$ such that $m\equiv 0$ (mod s) and $m\not\equiv 0$ (mod t) and $f(p^mt^n)=(\frac{m-1}{s}+1)t^{n+1}$ My question is can anyone find other functions satisfying the same property and even better is it possible to find all the functions that satisfy this property. Also I shall be grateful if anyone can find any flaws in my function","['number-theory', 'functions']"
4462408,Equivalent definition of essential supremum,"Let $(X,\mathcal{A},\mu)$ be a measure space and let $f\colon X\to [-\infty,+\infty]$ be a measurable function. Denote with $\mathcal{N}_\mu$ the collection of $\mu$ -null sets. On same text the definition of essential supremum is $$\operatorname{esssup}f:=\inf\left\{\sup_{x\in X\setminus N} f(x)\;\middle|\; N\in\mathcal{N}_\mu \right\}\tag 1$$ In other texts it is: $$\operatorname{esssup}f:=\inf\left\{a\ge 0\;\middle|\;\mu\left(\{x\in X\;\middle|\; f(x)>a\}\right)=0         \right\}\tag2$$ Question Are $(1)$ and $(2)$ equivalent? Why?","['measure-theory', 'supremum-and-infimum', 'real-analysis']"
4462414,"Let $(X_m)$ be a sequence of i.i.d. rv's and $n$ a Poisson distributed rv. How is $Y = (X_1, \ldots, X_n)$ indeed a rv?","Let $(X_m)$ be a sequence of i.i.d. random variables taking values in the interval $[0, 1]$ . Let $n$ be Poisson distributed with parameter $\lambda>0$ . Intuitively, $Y = (X_1, \ldots, X_n)$ is a random variable taking values in the space $[0, 1]^{< \omega}$ of all finite $[0, 1]$ -valued sequences. To be specific, $$
[0, 1]^{< \omega} := \bigcup_{i = 1}^\infty [0, 1]^i.
$$ However, I don't know which $\sigma$ -algebra $[0, 1]^{< \omega}$ is endowed with, and how $Y$ is indeed a random variable. Could you elaborate on my confusion?","['measure-theory', 'probability-theory', 'random-variables']"
4462529,"Notation for tensors: $\partial_i,\rm dx^i$ vs. $e_i, e^i$ vs. $Z_i, Z^i$","This is not a question about raising and lowering indices as suggested. This is proof that I still don't understand the topic well, but here it goes: Pavel Grinfeld in his youtube videos uses $\bf Z_i$ for the covariant basis vectors (or on the blackboard $\vec Z_i$ ), which he describes as the derivative of the position vector (assumes Euclidean straight lines) along coordinate systems. So he also uses notation such as $\Gamma^k_{ij}\bf Z^k=\frac{\partial\bf Z_i}{\partial Z^i}$ to note the partial derivative of the covariant basis with respect to each of the coordinates, and introduce the Christoffel symbols. I am sure this is perfect, and to his credit, he does mention ""a lot of Z's here"" at some point. But I would like to know what he has in mind to use these ""Z"" all over, and how to reconcile this with what I see elsewhere on this topic, i.e. $\bf e_i, e^i$ or $\partial_i,\rm dx^i.$ I see that the $\bf e$ 's may be too close to the Cartesian system for his ""coordinate-free"" thinking, perhaps, but I have no clue if this is the motivation. Likewise, perhaps the $\partial$ and differential $\rm d$ are to physics-centered, but again, no clue if this is the motivation. So what are the different notations for different occasions or disciplines to denote covariant and contravariant basis vectors in tensor algebra and calculus? NB: I am not asking about the orthogonality of vectors and covectors (and the upper/lower index position). I am asking about where and why a letter (Z or e) is expected, and where and why, in other spots, the calculus notation is used.","['notation', 'tensors', 'differential-geometry']"
4462564,Maximal Probability inequality,"In lecture notes to a lecture I'm currently attending my teacher claims that the following holds true: Let $X,Y_1,\dots,Y_n$ be random variables on the same probability space. Then for any $x > 0$ the following holds true $$\mathbb{P}\left(\max_{1\leq j \leq n}\vert X - Y_j\vert > x\right) \leq \mathbb{P}\left(\vert X\vert > x/2\right) + \mathbb{P}\left(\max_{1\leq j \leq n}\vert Y_j\vert > x/2\right).$$ I tried to verify this, but this seems not correct. What am I missing? Note that we assume neither independence nor equality in distribution. Do we maybe need such an assumption? In my attempt to verify this, my approach was to reduce the problem to the case $n=1$ . But even this didn't work to.","['measure-theory', 'maxima-minima', 'inequality', 'probability-theory', 'probability']"
4462620,A problematic integral,"I want to evaluate the integral $$I=\int_0^\infty\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx$$ This is a part of one of the solutions of my previous posts here . The user who posted this hasn't visited the site from over a month, so I thought it would be better to ask it as a post instead of a comment on the original solution. The OP suggests to using a parameter. So I tried doing that. $$I(a)=\int_0^\infty\frac{\ln x\ln(1+ax)}{x(x^2+1)}\,dx$$ Differentiating $$\begin{align}I'(a)&=\int_0^\infty\frac{\ln x}{(1+ax)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(x^2+1)(ax+1)}\,dx+\int_1^\infty\frac{\ln x}{(ax+1)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(ax+1)(x^2+1)}-\frac{x\ln x}{(x+a)(x^2+1)}\,dx\\&=\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}-\frac{2ax\ln x}{x^2+1}+\frac{a\ln x}{x+a}\,dx\\I'(a)&=\frac{\pi^2a}{24(a^2+1)}+\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}+\frac{a\ln x}{x+a}\,dx\end{align}$$ Now integrating from $0$ to $1$ , $$\begin{align}I&=\frac{\pi^2}{48}\ln 2+\int_0^1\ln x\int_0^1\frac{a^2}{(a^2+1)(ax+1)}+\frac a{(x+a)(a^2+1)}\,da\,dx\\&=\frac{\pi^2}{48}\ln2+\int_0^1\frac{\ln x}{x^2+1}\int_0^1\frac{2ax}{a^2+1}+\frac1{ax+1}-\frac x{a+x}\,da\,dx\\I&=\frac{\pi^2}{48}\ln2+\ln2\int_0^1\frac{x\ln x}{x^2+1}\,dx+\int_0^1\frac{\ln x\ln(1+x)}{x(x^2+1)}\,dx-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx+\int_0^1\frac{x\ln^2x}{x^2+1}\,dx\end{align}$$ Now solving the first and last integral and using the already proved relation $$\int_0^1\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx=-\frac34\zeta(3)-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx$$ we get $$I=-2\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx-\frac9{16}\zeta(3)$$ This doesn't help as this is what I was solving for in the first place.
Is there a better way to solve for $I$ ? Or maybe there is some possible manipulation in my solution that can potentially give a closed form. Any help would be welcomed. Please also check the original post for further clarification. Probably the final edit: I've found that if we represent my last two integrals after differentiation using dilogarithm and use the identity $$\text{Li}_2(z)+\text{Li}_2(z^{-1})=-\frac{\pi^2}6-\frac{\ln^2(-z)}2$$ we can easily solve it. No need to do manipulations using double integrals.","['integration', 'multivariable-calculus', 'calculus']"
4462663,Proof of a martingale not being uniformly integrable,"Fix $1/2<p<1$ . Let $X_1,X_2,\dots$ be independent identically distributed random variables with $\mathbb{P}(X_1=1)=p$ and $\mathbb{P}(X_1=-1)=1-p$ . Let $S_0=0$ and $S_n=X_1+\dots+X_n$ and $\mathcal{F}_n=\sigma(X_1,\dots,X_n)$ for $n\in\mathbb{N}$ . Define $$\varphi(x)=\left(\frac{1-p}{p}\right)^x.$$ I have obtained that $(\varphi(S_n))_{n\geq0}$ is a martingale. Additionally, if for $x\in\mathbb{Z}$ we define $T_x=\inf\{n\geq0:S_n=x\}$ , I have obtained that for integers $a<0<b$ we have $$\mathbb{P}(T_a<\infty)=\left(\frac{1-p}{p}\right)^{-a},\hspace{1cm}\mathbb{P}(T_b<\infty)=1.$$ Is $(\varphi(S_n))_{n\geq0}$ uniformly integrable? My thoughts: I think that naturally I expect for $(\varphi(S_n))$ to not be uniformly integrable, as we can see that $S$ will eventually almost surely hit any arbitrarily large integer. However, I am struggling to show this rigorously as $\varphi(S)$ is $L^1$ -bounded and this is the only necessary condition I know of for uniform integrability and I don't see how to use the definition of uniform integrability here. Any advice would be greatly appreciated, thanks!","['stochastic-processes', 'uniform-integrability', 'probability-theory', 'martingales']"
4462695,Procedure to check whether two hemispheres intersect,"Suppose I have two hemispheres with centers $c_1$ and $c_2$ whose orientations are defined by vectors normal to their bases, $v_1$ and $v_2$ . Also suppose these hemispheres have the same radius $r$ . I just want to know whether these two hemispheres intersect - I'm not necessarily interested in the intersection volume. I believe I can solve this problem when $v_1$ and $v_2$ are coplanar (so basically, solving whether two semicircles intersect) but the general 3D case is stumping me a bit. I am not a mathematician, but I am a scientist, and I am trying to implement this computationally.","['computational-mathematics', 'euclidean-geometry', 'spheres', 'geometry']"
4462704,Is there an corresponding concept of prime ideals in groups?,"Let $G$ be a group and $R$ a commutative ring. $G$ is simple if it has no non-trivial proper subgroups. It can be shown that $R$ is a field if it has no non-trivial proper ideals. Furthermore, if $N \unlhd G$ is maximal, $G/N$ is simple and if $I \unlhd R$ is  maximal, $R/I$ is a field, in both cases by the appropriate correspondence theorem. This seems to suggest a sort of correspondence between fields and simple groups. We also have the result that if $I \unlhd R$ is prime, $R/I$ is an integral domain. Question : Can prime ideals or integral domains in rings also be identified with any particular structures in groups? Since ideals are generally considered to be the corresponding concept in rings for normal subgroups, this might suggest that prime ideals correspond to normal subgroups with a further condition, but one weaker than maximality. I am aware there is a similar question already on MSE, but it did not ask about prime ideals.","['ring-theory', 'group-theory', 'abstract-algebra', 'maximal-and-prime-ideals']"
4462717,"In general is $\frac{d\,\mu_1}{d\,\mu_2}\circ T = \frac{d\,T\mu_1}{d\,T\mu_2}$?","Given an ergodic and non-singular dynamic system (definition provided here ) $(X, \mathcal{B}, \mu_1, T)$ where $(X, \mathcal{B}, \mu_1)$ is a measure space and $T$ is a fixed transformation, we then will have $\mu$ equivalent to $T\mu_1$ where $T\mu_1$ is defined by $T\mu_1(A)=\mu_1[T^{-1}(A)]$ for each $A\in \mathcal{B}$ . Now suppose $\mu_2$ is another measure that is equivalent to $\mu_1$ and makes $(X, \mathcal{B}, \mu_2, T)$ ergodic and non-singular. Hence, we will have $T\mu_2\sim\mu_2\sim\mu_1\sim T\mu_1$ . Do we have: $$
\frac{d\,\mu_1}{d\,\mu_2}\circ T = \frac{d\,T\mu_1}{d\,T\mu_2}
$$ $\mu_1$ -almost everywhere? Here the "" $\mu_1$ -almost everywhere"" can be replaced by "" $\mu_2$ -almost everywhere"" or any one of those four measures. By definition of $T\mu_2$ , for each $A\in\mathcal{B}$ , we have: $$
\int_X\chi_A(x)d\,T\mu_2 = \int_X\chi_A(Tx)d\,\mu_2
$$ I tried to show the following equation holds for each $A\in\mathcal{B}$ but cannot: $$
\int_A\frac{d\,\mu_1}{d\,\mu_2}(x)d\,T\mu_2 = \int_A\frac{d\,\mu_1}{d\,\mu_2}\circ T(x)d\,\mu_2 = \int_A\frac{d\,T\mu_1}{d\,T\mu_2}(x)d\,\mu_2
$$ Any hints or counter-examples will be appreciated","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
4462720,Definitions of Milnor number,"Let $f:\mathbb{C}^n\to\mathbb{C}$ be a function with an isolated singularity at $0$ , by which I mean $f'(0)=0$ and there is some $\epsilon > 0$ such that $f'(p)\not = 0$ for all $p$ nonzero with $|p|<\epsilon$ . The $\textbf{Milnor number}$ of $f$ at $0$ can be defined in two ways: i) The dimension, as a vector space over $\mathbb{C}$ , of the 'local algebra' $\mathbb{C}[z_1,...,z_n]/\langle\frac{\partial{f}}{\partial z_1}, ..., \frac{\partial{f}}{\partial z_n}\rangle$ (the algebra of functions modulo the ideal generated by the partial derivatives of $f$ ). ii) The degree of the map from $\mathbb{S}_\epsilon^{2n+1}\to \mathbb{S}_\epsilon^{2n+1}$ defined by $z\mapsto \epsilon \frac{\left(\frac{\partial{f}}{\partial z_1}, ..., \frac{\partial{f}}{\partial z_n}\right)}{\|\left(\frac{\partial{f}}{\partial z_1}, ..., \frac{\partial{f}}{\partial z_n}\right)\|}$ (the 'normalized gradient' of $f$ ). Why are these two definitions equivalent? I am aware of how to get from (ii) to (iii) = 'the middle Betti number of the Milnor fiber', so I would also be happy with an answer relating this to (i). Any hint or reference would be appreciated.","['algebraic-geometry', 'singularity-theory']"
4462731,Find the multiplicity of eigenvalue $2a$,"Let $a,b$ be real numbers with $0<2a<b$ . Let $K_n$ denote the commutation matrix of order $n$ , and let $A$ be the $n^2\times n^2$ matrix defined by $$A:=a[vec(I_n)vec(I_n)'+K_n-2diag(K_n)+I_{n^2}]+bdiag(K_n)$$ where $vec$ denotes the vectorization operator , and $diag(C)$ denotes the diagonal matrix having the same diagonal as $C$ . Finally let $M$ be an $n\times n$ idempotent matrix with rank $r<n$ , and let $B$ be the matrix defined by $$B:=[M\otimes M]A[M \otimes M]$$ where $\otimes$ denotes the kronecker product . I am trying to find the multiplicity of eigenvalue $2a$ as a function of $M$ . Also I would like to show that $2a$ is the smallest nonzero eigenvalue of $B$ . After some work I found that the eigenvalues of $A$ are as follows : $b+na  \, \text{   with multiplicity } 1 \, \text{and eigenvector} \sum_{i=1}^n (e_i\otimes e_i),$ $b  \text{   with multiplicty } n-1 \, \text{and eigenvectors} \, \{(e_1\otimes e_1)-(e_i\otimes e_i), i=2,\dots, n\},$ $2a  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)+(e_j\otimes e_i), i\neq j\},$ $0  \, \text{   with multiplicty } n(n-1)/2 \, \text{and eigenvectors} \, \{(e_i\otimes e_j)-(e_j\otimes e_i), i \neq j\},$ where $e_i$ denotes the $n\times 1$ vector with $1$ in position $i$ and zeros elsewhere.  I believe the multiplicity of $2a$ corresponds to the dimension of the $Col(M\otimes M) \cap Col(R)$ , where $R$ denotes the $n^2\times n(n-1)/2$ matrix having columns $(e_i\otimes e_j)+(e_j\otimes e_i)$ for $i\neq j$ , and $Col$ denotes the column space. Any ideas how to proceed? Thanks a lot for your help.","['symmetric-matrices', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'kronecker-product']"
4462750,Are substitutions relating multiple integration variables in iterated integrals always valid?,"A standard technique of mine when evaluating iterated integrals, such as double integrals, is to invoke a substitution that relates the integration variables of both integrals. To demonstrate and clarify what I mean, I shall present three examples for double integrals: Example 1: Gaussian integral, $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ $$I=\Gamma \left(\frac{1}{2}\right)^2 = \int_{0}^{\infty} \int_{0}^{\infty} \frac{1}{\sqrt{x t}}e^{-(t+x)}\,dx\,dt$$ We now enforce the substituion $t=sx \implies dt=x\,ds$ : $$\implies I = \int_{0}^{\infty}\int_{0}^{\infty}\frac{1}{\sqrt{s}}e^{-x (s+1)}\,dx\,ds=\int_{0}^{\infty}\frac{1}{\sqrt{s}(s+1)} \,ds=\pi$$ $$\implies \Gamma\left(\frac{1}{2}\right)=\int_{-\infty}^{\infty}e^{-x^2}\,dx=\sqrt{\pi}$$ This is a much more elementary approach than the common Jacobian polar coordinates method of evaluating the Gaussian integral and is how I originally discovered the solution for myself a few years ago prior to learning about the Jacobian. Example 2: In a previous answer of mine , I determined the following result: $$J=\frac{1}{2\pi}\int_{-\infty}^{\infty} \exp\left(\frac{x^2}{2}-\frac{y^2}{2}\right)K_0\left(\sqrt{(x-y)^2+\left(\frac{y^2}{2}-\frac{x^2}{2}\right)^2}\right)\,dy = \frac{e}{2} \sqrt{\pi}\operatorname{erfc}(1)$$ and is independent of $x$ . Here $K_0$ is the modified Bessel function of the second kind of order $0$ . A crucial part of my method of evaluation was to invoke a particular substitution on the integral: $$J=\frac{1}{2\pi}\int_0^\infty\int_0^\infty \frac{\cosh\left(xv\left(\frac{v^2}{4t}+1\right)\right)}{t}\exp\left(-t-\frac{1+2t+x^2}{4t}v^2-\frac{v^4}{16t}\right)\,dv\,dt$$ We now enforce the substitution $t = s v^2 \implies dt = v^2 \,ds$ : $$\begin{align}J&=\frac{1}{2\pi} \int_{0}^{\infty} \int_{0}^{\infty}\frac{\cosh\left(x v \left(\frac{1}{4s}+1\right)\right)}{s} \exp\left(-sv^2-\frac{1+2sv^2+x^2}{4s}-\frac{v^2}{16s}\right) \, dv \, ds\\&=\frac{1}{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}\frac{\cosh\left(xv\left(\frac{1}{4s}+1\right)\right)}{s}\exp\left(-\frac{1+x^2}{4s}\right)\exp\left(-v^2 \frac{(1+4s)^2}{16s}\right)\,dv\,ds\end{align}$$ This allows the result to fall out readily, since: $$\int_{0}^{\infty}\cosh(a v)\exp\left(-v^2 b^2\right) \, dv=\frac{\sqrt{\pi}}{2b}\exp\left(\frac{a^2}{4b^2}\right)$$ $$\implies J = \frac{1}{2\pi} \int_{0}^{\infty} \frac{\exp \left(-\frac{1+x^2}{4s}\right)}{s} \cdot \frac{\sqrt{\pi}}{2\left(\frac{1+4s}{4\sqrt{s}}\right)}\exp \left(\frac{4s x^2 \left(\frac{1}{4s}+1\right)^2}{(1+4s)^2}\right)\, ds$$ This results in the integral being independent as we wanted since the $x^2$ terms cancel. $$\implies J = \frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{1}{4s}\right)}{\sqrt{s} (1+4s)} \, ds\stackrel{s\,\mapsto\frac{1}{s}}{=}\frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \frac{\exp\left(-\frac{s}{4}\right)}{\sqrt{s} (4+s)} \, ds=\frac{e}{2} \sqrt{\pi} \, \operatorname{erfc} (1)$$ Example 3: Derivation of the relationship between the Beta function and Gamma function. $\Gamma(x)\Gamma(y)=\Gamma(x+y)B(x,y)$ : $$\Gamma(x)\Gamma(y)=\int_{0}^{\infty}\int_{0}^{\infty} e^{-u-v}u^{x-1}v^{y-1}\,du\,dy$$ We now enforce the substitution $u=zt$ and $v=z(1-t)$ : $$\begin{align}\Gamma(x)\Gamma(y)&=\int_{0}^{\infty}\int_{0}^{1}e^{-z}(zt)^{x-1}(z(1-t))^{y-1} z\,dt\,dz\\ &=\int_{0}^{\infty}e^{-z}z^{x+y-1}\,dz \cdot \int_{0}^{1}t^{x-1}(1-t)^{y-1}\\&=\Gamma(x+y)B(x,y)\end{align}$$ This is interestingly the method mentioned in the derivation of this property on the Wikipedia page of the Beta function . In this question about this derivation, the comments and answer invoke the Jacobian, suggesting that such substitutions can be made rigorous using the Jacobian, however, I would appreciate seeing exactly how that works as I only know about the Jacobian based on my own self-teaching. So to summarise is it always valid to perform substitutions on iterated integrals, such as double or triple integrals in this way, whereby one or more integration variables are written in terms of an integration variable of a different integral in the iteration. I have never come across any issue in using this technique in the past, but have never found rigorous proof in any literature that states that such substitutions are valid; it surprises me that such a simple solution to the Gaussian integral is not often used.","['integration', 'real-analysis', 'complex-analysis', 'jacobian', 'calculus']"
4462787,Finite index subgroup isomorphic to $\Bbb{Z}$ inside the free product $\Bbb{Z}_{2}*\Bbb{Z}_{2}$,"Let $\Bbb{Z_{2}*Z_{2}}$ be the free product then I want to prove that there is a finite indexed subgroup isomorphic to $\Bbb{Z}$ . My attempt : We denote the free product  by $\langle a\rangle*\langle b\rangle$ for more clarity such that $a^{2}=e$ and $b^{2}=e$ . Then the subgroup $H=\langle ab\rangle $ is a subgroup isomorphic to $\Bbb{Z}$ . Now as for the index I think it has index $2$ as the cosets are $\{H,bH\}$ . Am I correct in thinking this?","['combinatorial-group-theory', 'solution-verification', 'free-product', 'group-theory', 'algebraic-topology']"
4462844,Moments of occupation time of Wiener process,"For a Brownian motion $B(t)$ defined on $[0,1]$ , the occupation measure for a Borel set $A$ is defined as $$ \mu(A)= \int_0^1 1\{B(t) \in A \}dt$$ $\mu(A)$ denotes the amount of time $B$ spends in $A$ . Theorem 3.26 of Morters and Peres shows that $\mu(A)$ is a continuous random variable. Is it known what the moments are of $\mu(A)$ ? Even if $A$ is an interval? A literature search hasn’t turned up anything for me yet. Any help much appreciated.","['stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4462850,Weird relation between bitwise 'AND' and 'OR' Operations.,"I was trying to plot some boolean operations data using Matplotlib Python. I noticed some weird behavior of 'and'(&) and 'or'(|) operations on the same data pair.
Suppose: A=50(0011 0010)
B=60(0011 1100)

A&B = 48(0011 0000)
A|B = 62(0011 1110) Notice how both of them have the same distance(2) from the bigger and lower number. In other words- have the same distance(7) from the arithmetic average of A and B(55). Another example: A=96(0110 0000)
B=82(0101 0010)

A&B = 64(0100 0000)
A|B = 114(0111 0010) It is the same, A&B is 18 lower than B, and A|B is 18 higher than A. both are at the same distance from the mean of A and B(89) which is 25. Mathematically: $$
 |(A.B - \mu(A,B))| =  |(A+B - \mu(A,B))|
$$ It is almost true for all positive numbers(some weird problem arises if the lower bound gets under 0)
My First question is, why do they behave this way? Is this connected to De Morgan's theorem? I tried to see if there is a relation between the distance that those operations have from the actual number(A and B) and the distance of A and B. I found this- variable A = 10000, 10000, 10000, 10000,.....,10000 variable B = 10000, 10001, 10002, 10003,.....,15000 so on x-axis, I put B-A(0,1,2,3,4,....,5000). On the y-axis, I put (A|B - B) (which is equal to (A - A&B))
The result was like this: Matplotlib data of 5000 distances Now my second question is, is it possible to get a function for this relation
Something like- $$
f(y) = x^2 + sin(x)....
$$ If Anybody still reading, the plot for the distance of A&B and A|B from the arithmetic mean of A and B looks like this: Distance from the mean of A and B plot Same plot as before if A and B started from 20000, 20000. and plotted 10000 points instead of 5000. 10000 data points with A=20000, B starts from 20000 Thank you for your time:)","['boolean-algebra', 'graphing-functions', 'logic', 'functions', 'binary-operations']"
4462901,Is the Haar measure of the boundary of an open subset of the $k$-torus zero?,"I have just recently started reading the basics of topological groups and Haar measures, and have become really curious about when the boundary of a non-empty open subset $U$ of a compact abelian group $G$ , has zero Haar measure. Here and in the following, by ""Haar measure"", I will mean the normalized Haar measure of a compact abelian group. After some searching, I found these posts ( When does the boundary have measure zero? and Conditions that ensure that the boundary of an open set has measure zero ) which give very general sufficient conditions for subsets of $\mathbb R^n$ to have boundaries of Lebesgue measure zero, however I couldn't find any analogues for Haar measures. I would like to restrict $G$ to the $n$ -torus $\mathbb T^n$ , so here's my Semi-general question: Consider the $n$ -torus $\mathbb T^n = \mathbb R^n/\mathbb Z^n$ which is a topological group, and a closed subgroup $H$ (which can be assumed to be connected if necessary). Consider the Haar measure $\mu_H$ on $H$ and an open set $U \subset \mathbb T^n$ . What minimal additional conditions on $U$ might we need (if any) for the $H$ -Haar measure of the boundary $\partial(U \cap H)$ of the open subset $U \cap H \subset H$ to be zero, that is to have $\mu_H(\partial (U \cap H)) = 0$ ? If $H$ were $\mathbb T^n$ itself, then we know that $\mu_H$ is simply the $n$ -dimensional Lebesgue measure and the question reduces to one already answered in the linked posts. If $H$ were an open subgroup, then (if I understand correctly) the Haar measure of $\mathbb T^n$ would also restrict to the normalized Haar measure of $H$ , so that $\mu_H$ would just be the restriction of the $n$ -dimensional Lebesgue measure, and once again the question is answered. But of course, $\mathbb T^n$ being connected has no non-empty proper open subgroups, so the last statement doesn't really add anything in my context. So what about the general setting, namely when $H$ is a proper non-empty closed subgroup (again, assumed connected if necessary)? If nothing can be said in the above setting (even with some minimal additional conditions), the specific $H$ and $U$ I would like to know about are the following: consider any real numbers $a_1, \cdots, a_n$ and any complex numbers $c_1, \cdots, c_n$ . My $H$ is the closure of the subgroup $\{(e^{2 \pi i a_1 t}, \cdots, e^{2 \pi i a_n t}): t \in \mathbb R\} \subset \mathbb T^n$ , and $U \subset \mathbb T^n$ is the set of all $(z_1, \cdots, z_n) \in \mathbb T^n$ , for which the complex number $c_1z_1 + \cdots + c_n z_n$ has strictly positive real part. Can we say in this case that $\partial(U \cap H)$ has $H$ -Haar measure zero? Note 1 : I do not know if this will be relevant, but in the above example, $U$ can be written as the intersection of $\mathbb T^n$ with the set $V := \{(z_1, \cdots, z_n) \in \mathbb C^n: \Re \sum_{j=1}^n c_j z_j > 0\} \subset \mathbb C^n$ , so that $U \cap H = V \cap H$ . Here, $V$ is both open and convex in $\mathbb C^n$ , hence its boundary has zero Lebesgue measure. Note 2: It might seem that the role of $U$ in my semi-general question is kind of superficial since we are really working with the open subset $U \cap H$ of $H$ , but it is in line with my last example.","['measure-theory', 'topological-groups', 'haar-measure', 'manifolds', 'locally-compact-groups']"
4462904,Probability that sum of random variables is positive does not approach $1$ in the limit,"Say we have independent random variables $X_i$ , with: $$
X_i = 
\begin{cases}
1,~\text{with probability}~\frac{1}{2}+ 2^{-i},\\
-1,~\text{with probability}~\frac{1}{2}-2^{-i}.
\end{cases}
$$ I'm interested in showing that $\lim_{n\rightarrow \infty}\mathbb{P}[X_1+\dots+X_n > 0]\neq 1$ . I've seen an argument for this using the Central Limit Theorem ( here , pp. 2-3). I was wondering: is there perhaps a simpler (more elegant?) argument---perhaps using some more elementary methods, or a well-chosen concentration inequality---that could work well on an audience with less than exhaustive knowledge of statistics? I tried explicitly spelling out the probability that a majority of the $X_i$ 's have value $1$ , with the hope of bounding the sum with some inequalities, but with limited success. So am looking for another angle.","['probability', 'random-variables']"
4462950,Why is the triangle inequality equivalent to $a^4+b^4+c^4\leq 2(a^2b^2+b^2c^2+c^2a^2)$?,"Consider the existential problem of a triangle with side lengths $a,b,c\geq0$ . Such a triangle exists if and only if the three triangle inequalities $$a+b\geq c,\quad b+c\geq a\quad\text{and}\quad c+a\geq b\tag{0}$$ are all satisfied. Alternatively, if $\ell_1\leq\ell_2\leq\ell_3$ are the values of $a,b$ and $c$ ordered in ascending order, then the triangle exists iff $\ell_1+\ell_2\geq\ell_3$ . Interestingly, the three triangle inequalities can be recast into a single quartic polynomial inequality. Let $0,x,y\in\mathbb R^2$ be the three vertices of the triangle, with $\|x\|=a,\,\|y\|=b$ and $\|x-y\|=c$ . Then $c^2=\|x-y\|^2=\|x\|^2-2\langle x,y\rangle+\|y\|^2=a^2+b^2-2\langle x,y\rangle$ . Therefore $x^Ty=\langle x,y\rangle=\frac{1}{2}(a^2+b^2-c^2)$ and $$\pmatrix{x^T\\ y^T}\pmatrix{x&y}=\frac{1}{2}\pmatrix{2a^2&a^2+b^2-c^2\\ a^2+b^2-c^2&2b^2}.\tag{1}$$ The RHS of $(1)$ must be positive semidefinite because the LHS is a Gram matrix. Conversely, if the RHS is indeed PSD, it can be expressed as a Gram matrix. Hence we obtain $x$ and $y$ and the triangle exists. As $2a^2$ and $2b^2$ are already nonnegative, the RHS of $(1)$ is positive semidefinite if and only if $(2a^2)(2b^2)-(a^2+b^2-c^2)^2\geq0$ , by Sylvester's criterion. That is, the triangle exists if and only if $$-(a^4+b^4+c^4)+2(a^2b^2+b^2c^2+c^2a^2)\geq0.\tag{2}$$ This polynomial inequality can be derived by more elementary means. See circle-circle intersection on Wolfram MathWorld. The geometric explanation for the necessity of $(2)$ is given by Heron's formula , which states that the square root of the LHS is four times the area of the triangle. Since both $(0)$ and $(2)$ are necessary and sufficient conditions for the existence of the required triangle, the two sets of conditions must be equivalent to each other. Here are my questions. Is there any simple way to see why $(0)$ and $(2)$ are equivalent? Can we derive one from the other by some basic algebraic/arithmetic manipulations?","['inequality', 'geometric-inequalities', 'geometry']"
4462959,Neat claim induced from a converse to the Sard's Theorem,"I found the following two posts to be interesting when I'm studying Sard's Theorem by myself: On the converse of Sard's theorem https://mathoverflow.net/questions/423475/a-modified-version-of-the-converse-to-the-sards-theorem In particular, is there a way to prove the neat claim in the mathoverflow post (without using advanced tools like triangulation of manifolds)? The questions is restated below: For any manifold $X$ with $\dim X \geq 1$ , there always exists some map $f:X \rightarrow \mathbb{R}^2$ , such that the differential $df_{x}$ is nonzero for any $x \in X$ . (Here we don't require $df_{x}$ to be nonsingular for any $x \in X$ , which weakens the claim.) I think the claim should be true after trying several examples, but I haven't found a way to prove it...thanks for any help in advance!","['smooth-manifolds', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
4462978,"show that $\tan \alpha_i = -t_i$ for each $i=1,2,3$","My question pertains to the question and solution below. In particular, can someone prove why $\tan \alpha_i = -t_i$ for each $i$ ? I think it would be worth clarifying what the angle each normal makes with the axis of the parabola (the x-axis) is. I think it's the smaller of the two angles formed by the normal lines and the axis.","['contest-math', 'trigonometry', 'conic-sections', 'geometry']"
4463026,"Prove that if $f(x)$ is a polynomial of degree $n$, then $f(x+a)$ is also a polynomial of degree $n$","I am trying to prove an exercise from 'Chapter 1, Calculus Vol. I' by Apostol:
If $f(x)$ is a polynomial of degree $n$ , then show that $f(x+a)$ is also a polynomial of degree $n$ . I have outlined two proofs for this: Assume $f(x) = \sum_{k = 0}^{n}c_{k}x^{k}$ , which gives us $f(x+a) = \sum_{k = 0}^{n}c_{k}(x+a)^{k}$ . Using the binomial expansion formula, we can expand each term and show: $$f(x+a)=\sum_{k=0}^{n}\left(\sum_{i=k}^{n}{i \choose k}a^{i-k}c_i \right) x^{k}$$ Argue that $f(x+a)$ is a linear translation of $f(x)$ along the $x$ -axis and, graphically, the functions have the same form. I am looking for a more elegant proof not involving such heavy algebra or arguments involving higher-order objects such as graphs.","['functions', 'polynomials', 'real-analysis']"
4463088,$m(\xi)=(-1)^{[\xi^2]}$ is not a Fourier multiplier,"Let $$m(\xi)=(-1)^{[\xi^2]},\qquad \xi\in\mathbb R,$$ where $[a]$ means the largest integer less than or equal to $a$ . Prove that $m$ is not a Fourier multiplier on $L^p(\mathbb R)$ for some $p\neq 2$ . Note that $m:\mathbb{R}\to \mathbb{C}$ is called an $L^p(\mathbb R)$ Fourier multiplier if $$\|T_m f\|_p := \|(m\hat{f})^\vee\|_p \leq C\|f\|_p$$ for some $C>0$ , where $\hat{f}$ is the Fourier transform of $f$ , and $f^\vee$ is its inverse Fourier transform: $$\hat f(\xi)=\int_\mathbb{R} f(x)e^{-2\pi ix\xi}\,dx,\qquad f^\vee(x)=\int_\mathbb{R} f(\xi)e^{2\pi ix\xi}\,d\xi.$$ The problem comes from the following theorem: Theorem. If $m\in C(\mathbb R)$ is bouneded such that $m^2$ has bounded variation, then $m$ is an $L^p(\mathbb R)$ Fourier multiplier. I was wondering whether the condition $m\in C(\mathbb R)$ is redundant, and my teacher told me this counterexample. But I don't know how to show the counterexpamle. My try. I tried the functions $f_N$ such that $\hat{f_N}=\chi_{[0,\sqrt N]}$ . I wish to show that $$\sup_{N}\frac{\|T_m f_N\|_p}{\|f_N\|_p}=\infty.$$ It is direct to calculate that $f_N(x)=\frac{e^{2\pi i\sqrt N\ x}\ -1}{2\pi ix}$ , so $\|f_N\|_p\sim N^{\frac12-\frac1{2p}}.$ But I don't know how to estimate $\|T_m f_N\|p$ , so I stuck. Any help would be appreciated!","['harmonic-analysis', 'fourier-analysis', 'real-analysis']"
4463095,How to derive the following result from the uniformly law of large number?,"For a $n$ -dim Brownian motion $B_t=(B_t^1,\dots, B_t^n)$ , I would like to ask how to derive the following result from the uniformly law of large number: as $n\to \infty$ , the following converges to zero almost surely on $[0,T]^2$ , $$
\sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[\frac{1}{n}\sum B_t^iB_s^i]\right|\to 0.
$$ This means $$
\sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[B_t^1B_s^1]\right|\to 0
$$ and we take sup-norm. So $\frac{1}{n}\sum B_t^iB_s^i\to E[B_t^1B_s^1]$ . This question is enough to verify the condition of ULLN and I find one lecture note of this theorem: http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Apr23_Addison.pdf It seems that we need to check the Rademacher complexity is $o(1)$ ? How to check that?","['law-of-large-numbers', 'analysis', 'probability', 'real-analysis']"
4463226,Probability and genetics - Bayes' theorem,"Britney can be homozygous $HH$ or heterozygous $Hh$ with equal probability.
Hemophilia is a mostly inherited genetic disorder. A test to detect a dominant allele $h$ , responsible for the disorder, is carried out.
The test has $85\%$ reliability in heterozygous women (with $Hh$ genotype), that is, it successfully detects the presence of the allele $h$ in $85\%$ of the cases, while in homozygous women (with $HH$ genotype) it fails to detect $h$ in $1\%$ of the cases.
We want to calculate the following probabilities: $P (\text{Britney}\,Hh | \text{test was positive})$ and $P(\text{Britney}\,HH | \text{test was negative})$ I am not sure for the correct interpretation of the question, as I had to translate some terms I am not familiar with.
With the little knowledge I have on statistics, I will make an attempt: Prior probability Britney is homozygous or heterozygous $P(ΗΗ)= P(Hh) = 0.5$ $$P(E|Hh)= \text{Probability of a Positive Test Result given Britney is Heterozygous} = 0.85\\
\text{So, we have}\\
P(E|HH)= \text{Probability of a Positive Test Result given Britney is Homozygous} = 0.15$$ We want $$P(HH|E) = \text{Probability of Britney being Heterozygous given the test yields a Positive Result}$$ We also want $$P(Hh|E^c) = \text{Probability of Britney being Homozygous given the test yields a Negative Result}$$ So for a) $$P(HH|E) = {P(E|HH) P(HH) \over P(E)} = {P(E|HH) P(HH) \over P(E|HH)P(HH) + P(E|{Hh}) P({Hh})}$$ and similarly for the second. Are these correct? EDIT: Can you tell me if this is correct? "" $P(E|HH)= \text{Probability of a Positive Test Result given Britney is Homozygous} = 0.15$ "" or is it
"" $P(E|HH)= \text{Probability of a Negative Test Result given Britney is Heterozygous} = 0.15$ ""?","['statistics', 'probability']"
4463326,Book on Riemannian Manifold [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . Improve this question I am writing my bachelor's thesis which is in part about Riemannian Manifolds but not extensively and I am using the book Introduction to Smooth Manifolds by Lee. However, I find the Riemannian Manifold part requires a lot of information on covectors, tensors, etc. So I want to ask if there is any other book that introduces Riemannian manifold in an intuitive manner and self-contained in a way that does not require tensors, covectors, etc.","['book-recommendation', 'reference-request', 'riemannian-geometry', 'differential-geometry']"
4463332,Fields over which matrix is diagonalizable,"I recently came across diagonalizations of certain special matricies in my Linear Algebra lecture (real symmetric, self-adjoint, orthogonal, etc), and have a couple questions about knowing which field(s) a matrix is diagonalizable over: If a matrix with complex entries is diagonalizable, is it only diagonalizable over $\mathbb{C}$ , or are there such matricies that are also diagonalizable over $\mathbb{R}$ ? If a matrix with complex entries has only real eigenvalues (which is the case for self-adjoint matricies), is it diagonalizable only over $\mathbb{C}$ ? If a matrix with real entries is diagonalizable, is it always diagonalizable over $\mathbb{R}$ and not just $\mathbb{C}$ ?","['matrices', 'diagonalization', 'field-theory', 'linear-algebra']"
4463354,Dual definition for distributivity lattice conditions,"I am starting discrete mathematics and it came to me that distributive lattices has 2 equicalent representations for their definition: $a\wedge (b\vee c) = (a \wedge b)\vee (a\wedge c)$ $a\vee (b\wedge c) = (a \vee b)\wedge (a\vee c)$ However, I couldn't find proof for this equivalence. I am using ""Introduction to discrete structures for computer science and engineering"" by Preparata, FRANCO. If you can help me with an insight or at least some place I could research about the matter, I'd be very grateful.","['lattice-orders', 'duality-theorems', 'discrete-mathematics']"
4463408,How to compute a basis for the space of left-invariant 1-forms on a Lie group.,"I'm reading this paper by Fernandez and de Leon. I was having trouble regarding the computation of a basis of left-invariant 1-forms of the Lie group $G$ described below. Let $G$ be a matrix Lie group of dimension 6 whose elements are of the form $$A=\begin{bmatrix}
             e^t & 0 & xe^t & 0 & 0 & y_1 \\
             0 & e^{-t} & 0 & xe^{-t} & 0 & y_2 \\
             0 & 0 & e^t & 0 & 0 & z_1 \\
             0 & 0 & 0 & e^{-t} & 0 & z_2 \\
             0 & 0 & 0 & 0 & 1 & t \\
             0 & 0 & 0 & 0 & 0 & 1
    \end{bmatrix}$$ where $t,x,y_1,y_2,z_1,z_2\in \mathbb{R}$ . A global system of coordinates of $G$ are then given by $$t(A)=t, x(A)=x, y_i(A)=y_i, z_i(A)=z_i, 1\leq i\leq 2.$$ Then they say that a standard computation reveals that the following 1-forms on $G$ constitute a basis for the space of left-invariant 1-form: $$\alpha=dt, \beta=dx, \gamma_1=e^{-t}dy_1-xe^{-t}dz_1, \gamma_2=e^tdy_2-xe^tdz_2, \delta_1=e^{-t}dz_1, \delta_2=e^tdz_2.$$ Because I do not know any standard computation to get the basis for the space of left-invariant 1-forms on a Lie group, I have to ask how do they calculate? Is there really a standard procedure to obtain such a basis, or is it really just by trial methods? Anyone who knows this please help me... I really wanted to learn this. Thanks in advance","['differential-forms', 'lie-groups', 'differential-geometry']"
4463433,Permanent of a low-rank matrix is easy to calculate?,"In this paper , Aaronson and Arkhipov use the following lemma (lemma 67 in the paper): Let $V \in \mathbb{C}^{n \times n}$ be a matrix of rank $k$ . Then $\operatorname{Per}(V + I)$ is computable exactly in $n^{O(k)}$ time. Unfortunately, they refer to a ""forthcoming paper"" which I was unable to find. Is there a publication where this or a similar statement is proven?","['permanent', 'reference-request', 'linear-algebra', 'computational-complexity', 'computer-science']"
4463535,"Is it true that $\int_0^T e^{tA}\,dt = (e^{TA}-I)A^\dagger$ if $A$ is a psd matrix?","If $A$ is a psd matrix, can we say $F(A,T) :=\int_0^T e^{tA}\,dt = (e^{TA}-I)A^\dagger$ , where $A^\dagger$ is the Moore-Penrose pseudo-inverse of $A$ ? Afterall, The result holds if $A$ is positive-definite (thanks to the first part of this post https://math.stackexchange.com/a/658289/168758 ). For small $\lambda>0$ can approximate $A$ by a positive-definite matrix $B_\lambda := A+\lambda I$ , and it  argue that $$
F(A,t) \overset{?}{=} \lim_{\lambda \to 0^+}F(B_\lambda, T) = \lim_{\lambda \to 0^+}(e^{TB_\lambda}-I)B_\lambda^{-1} = (e^{TA}-I)A^\dagger,
$$ where the first step is not justified .","['ordinary-differential-equations', 'matrix-calculus', 'linear-algebra', 'limits', 'dynamical-systems']"
4463547,"Green's theorem and translated angular form on the path $\gamma:[0,3\pi/2]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t)$-strange result","I would like to compute the following integral $$\int_\gamma-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy,$$ where $\gamma:\left[0,\frac{3\pi}2\right]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t).$ Here is my answer which I would like to verify. The differential $1$ -form $\omega=-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy$ is a translated angular form centered at $(\pi,0),$ hence $d\omega=0,$ that is $\omega$ is closed. I would like to apply Green's theorem, so, for that purpose, I considered the following paths: $$\gamma_2:[0,1]\to\Bbb R^2,\gamma_2(t)=\left(\frac{3\pi}2,t\right),\\\gamma_3:[0,1]\to\Bbb R^2,\gamma_3(t)=\left(\frac{3\pi}2-\frac{3\pi}2t,\pi\right),\\\gamma_4:[0,2\pi]\to\Bbb R^2,\gamma_4(t)=(\pi+\cos t,-\sin t).$$ First, $$\int_{\gamma_2}\omega=\int_0^1\frac{\frac{3\pi}2-\pi}{\left(\frac{3\pi}2-\pi\right)^2+t^2}dt=\arctan\frac{t}{\frac{\pi}2}\Big|_0^1=\arctan\frac2\pi\\\int_{\gamma_3}\omega=\int_0^1\frac{\pi}{(t-\pi)^2+\pi^2}dt=\arctan\frac{t-\pi}\pi\Big|_0^1=\arctan\frac{1-\pi}\pi+\frac\pi4$$ and $$\int_{\gamma_4}\omega=-2\pi$$ because $\gamma_4$ is closed and negatively oriented. Now, let $D$ be the set ""inside"" the contour $\gamma+\gamma_2+\gamma_3$ and outside the circle $\gamma_4.$ By Green's theorem, $$\begin{aligned}\int_\gamma\omega+\int_{\gamma_2+\gamma_3+\gamma_4}\omega&=\int_{\partial D}\omega=\int_D d\omega=0\\\implies \int_\gamma\omega&=-\int_{\gamma_2+\gamma_3+\gamma_4}\omega=2\pi-\arctan\frac2\pi-\arctan\frac{1-\pi}\pi-\frac\pi4\\&=\frac{7\pi}4-\arctan\left(\tan\left(\arctan\frac2\pi+\arctan\frac{1-\pi}\pi\right)\right)\\&=\frac{7\pi}4-\arctan\left(\frac{\frac2\pi+\frac{1-\pi}\pi}{1-\frac2\pi\cdot\frac{1-\pi}\pi}\right)\\&=\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\\&=\arctan\left(\tan\left(\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\right)\right)\\&=\arctan\left(\frac{-1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}{1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}\right)\\&=\arctan\frac{2-5\pi}{2\pi^2-\pi-2}\end{aligned}$$ But I'm unsure as I didn't expect such a result. Did I make any mistakes?","['integration', 'greens-theorem', 'multivariable-calculus', 'solution-verification']"
4463565,How to use Stokes' theorem?,"$\int_{K}\left(x^{2}+y z\right) d x+\left(y^{2}+x z\right) d y+\left(z^{2}-x y\right) d z$ where K is a closed curve, oriented positive, consisting of an arc defined by the parametric equation $x=a\cos t, y=a\sin t, z=\frac{1}{2\pi}t$ and segment $BA,A=(a,0,0),B=(a,0,1)$ I drew this K-curve for the parameter a = 2 to see what area we would integrate over. That is, looking from above, it will be a circle of radius: $$
S=\left\{(x, y) \in \mathbb{R}^{2} \mid x^{2}+y^{2}=a^{2}\right\}
$$ In polar coordinate system $$
\begin{aligned}
&0 \leqslant r \leqslant a \\
&0 \leqslant \vartheta \leqslant 2 \pi
\end{aligned}
$$ So I use Stokes' theorem because the assumptions are satisfied (see below).
Stokes’ theorem: Let $D \subset \mathbb{R}^{2}$ be a regular area and let $\sigma: D \rightarrow S \subset \mathbb{R}^{3}$ be a regular oriented surface with the edge of $\partial S$ being a piecewise smooth curve. Suppose that the surface $S$ is oriented according to its parameterization, that is, in such a way that according to the right-handed screw rule, the circulation around the curve determines the return of the vector normal to the surface. Let $G \subset \mathbb{R}^{3}$ be an open set such that $G \supset S \cup \partial S$ . Suppose that the function $f=(P, Q, R): G \rightarrow \mathbb{R}^{3}$ is a function of the class $C^{(1)}$ in $G$ . Then: $$\int_{\partial S} P d x+Q d y+R d z=\iint_{S}\left(\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z}\right) d y d z+\left(\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}\right) d z d x+\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y$$ Auxiliary calculation: $$
\frac{\partial R}{\partial y}=-x\,\,  \frac{\partial Q}{\partial z}=x\quad \,\,\frac{\partial P}{\partial z}=y\,\, \frac{\partial R}{\partial x}=-y\,\,  \frac{\partial R}{\partial x}=z\quad \frac{\partial P}{\partial y}=z
$$ Then $$
\int_K{\underset{P}{\underbrace{\left( x^2+yz \right) }}dx+\underset{Q}{\underbrace{\left( y^2+xz \right) }}dy+\underset{R}{\underbrace{\left( z^2-xy \right) }}dz}=\iint_S{-2x\,\,dydz}+2y\,\,dxdz\,\, +\,\,0 \cdot dxdy = \ldots \star (?)
$$ Question: How to calculate this double integral ( $\star$ ) ? Did I make good use of Stokes' theorem? Is there a faster way to solve this problem? Thanks in advance.","['multivariable-calculus', 'multiple-integral', 'stokes-theorem', 'vector-analysis']"
4463597,If $\;f(x)+\frac{1}{x}=f^{-1}(\frac{1}{f(x)})\;$ and $f^{-1}(\frac{1}{f(x)})>0$. Then $f(x)$ is?,If $f'(x)>0\; \forall \;x\in \mathbb R^+\;$ where $f:\mathbb R^+\rightarrow \mathbb R\;$ and $\;f(x)+\dfrac{1}{x}=f^{-1}\bigg(\dfrac{1}{f(x)}\bigg)\;$ and $f^{-1}\bigg(\dfrac{1}{f(x)}\bigg)>0$ . Then $f(x)$ is?. My Approach: I replaced $f(x)$ by $\dfrac{1}{f(x)}\implies x\rightarrow f^{-1}\bigg(\dfrac{1}{f(x)}\bigg)$ . $\implies \; \dfrac{1}{f(x)}+\dfrac{1}{f^{-1}\bigg(\dfrac{1}{f(x)}\bigg)}=f^{-1}(f(x))$ $\implies\;$ $\dfrac{1}{f(x)}+\dfrac{x}{xf(x)+1}=x$ $\implies\;$$ \dfrac{2xf(x)+1}{f(x)(xf(x)+1)}=x\; \implies \{xf(x)\}^2-\{xf(x)\}-1=0$ $\implies\;$ $xf(x)=\dfrac{1-\sqrt{5}}{2}$ and $xf(x)=\dfrac{1+\sqrt{5}}{2}$ . But answer given is $f(x)=\dfrac{\sqrt{5}-1}{2x}$ . My Doubts: $1.$ What is wrong in my method? $2.$ Did I make any mistake by putting $f(x)\rightarrow \dfrac{1}{f(x)}?$,"['functional-equations', 'calculus', 'functions', 'algebra-precalculus']"
4463657,"Is $\mathcal{A}\vee \mathcal{B}=\{A\cup B : A\in \mathcal{A}, B\in \mathcal{B}\}$ a $\sigma$-Algebra?","Let $\mathcal{A}$ and $\mathcal{B}$ be $\sigma$ algebras over $\Omega$ . I know that $\mathcal{A}\cup \mathcal{B}$ is not a $\sigma$ -algebra, but what about $\mathcal{A}\vee \mathcal{B}=\{A\cup B : A\in \mathcal{A}, B\in \mathcal{B}\}$ ? It surely contains $\Omega$ and $\emptyset$ . Let $C\in \mathcal{A}\vee \mathcal{B}$ , i. e. we have $C=A\cup B$ and $\Omega\setminus C=\Omega\setminus (A\cup B)=A^C\cap B^C$ , why is this in $\mathcal{A}\vee \mathcal{B}$ and what about the sigma additivity?","['measure-theory', 'probability']"
4463674,How to quickly generate an equilibrium point of a strange attractor numerically?,"In many strange attractors (for example the Lorentz system , given appropriate parameter values), the point that is described by the system's equations of motion seems to approach a manifold with a dimensionality lower than that of the point as $t\to\infty$ . (For example, in the Lorentz system, the point has three dimensions as it is described by $x$ , $y$ and $z$ , but the manifold to which it eventually ends up arbitrarily close only seems to have two dimensions.) If the point is in this manifold, the system can be considered to be in a kind of equilibrium, as the point will never stray from this manifold as time progresses (similarly to how the entropy of a physical system in equilibrium will remain constant). My question is, is there some algorithm that can be used to quickly generate an equilibrium point of the system? I don't consider a stationary point an equilibrium point as it has a very low entropy (nor would I consider any other point with significantly lower entropy than the entropy of the manifold an equilibrium point). Obviously, the stationary point is also part of the manifold, but if our strategy is to just find a stationary point, we limit ourselves to a subset of the manifold, and since this subset contains much fewer points than what the entire manifold does, the entropy of the solutions that we find using this strategy becomes much smaller. One way to generate an equilibrium point is to simply simulate the trajectory of the point, using a numerical ODE solver, sufficiently far into the future. (In fact, a more rigorous way to define the set of equilibrium points may be as the limit set $\lim_{T\to\infty} \{\vec{u}(\vec{u}_0,t)\,|\,t\geq T,\, \vec{u}_0\in U \}$ , where $U$ is the set of all starting points that are well-behaved according to some appropriate criterion, or maybe it would even be enough if $U$ just was a set containing a single well-behaved starting point.) However, this feels very inefficient, and I'm looking for a much quicker way to get to equilibrium. I then asked myself: Can we device an equation that describes the manifold? That is, an equation that is true if and only if a given point lies within the manifold? If so, we could perhaps generate a point at random, outside of the manifold, and then either use use gradient decent or the nonlinear conjugate gradient method in order to force the value $(\text{LHS} - \text{RHS})^2$ to become zero by pushing the point more directly towards the manifold, and in that case this method could probably be used to reach an equilibrium point much faster. It would therefore also be nice to know whether we can describe the manifold by using an equation (rather than by using the much less practical limit set).","['limits', 'chaos-theory', 'entropy', 'ordinary-differential-equations']"
4463677,Differential Operator Invertible,"Good evening, I have a little problem with the invertibility of an operator $A : W^{1,2}(S^1,\mathbb{R}^{2n})\longrightarrow L^2(S^1,\mathbb{R^{2n}})$ defined as $$(A\zeta)(t) = \dot{\zeta}(t)-\dot{\Psi}(t)\cdot\Psi^{-1}(t)\cdot\zeta(t),$$ where $\Psi(t)\in \mathbb{R}^{2n\times 2n}$ is a path of symplectic matrices with $\Psi(0)=Id$ . The claim then is that $A$ is invertible if and only if $1$ is not an eigenvalue of $\Psi(t)$ for any $t$ . In the paper, it sounds like this is obvious, so I started to show that $\ker A = \{0\}$ : If $A\zeta = 0$ (i.e. $(A\zeta)(t) = 0$ a.e.), we have $$\dot{\Psi}(t)\cdot\underbrace{\Psi^{-1}(t)\cdot\zeta(t)}_{=:\alpha(t)} =\dot{\zeta}(t)  .$$ So setting $\alpha (t) := \Psi^{-1}(t)\cdot\zeta(t)$ , this simplifies to \begin{align}
\dot{\Psi}(t)\cdot\alpha(t) &=\frac{d}{dt}(\Psi(t)\cdot\alpha(t)) \\ \Leftrightarrow \dot{\Psi}(t)\cdot\alpha(t) &=\dot{\Psi}(t)\cdot\alpha(t)+\Psi(t)\dot{\alpha}(t) \\ \Leftrightarrow \Psi(t)\cdot\dot{\alpha}(t) &= 0
\end{align} Since $\Psi(t)$ is invertible (as a symplectic matrix), this means $\dot{\alpha}(t) = 0$ , so $\alpha(t)$ is a constant vector and so by definition of $\alpha$ we get that $$ \zeta(t)=\Psi(t)\cdot\alpha $$ is actually an element of the kernel. This seems be the place where the claim with $1$ (not) being an eigenvalue of $\Psi(t)$ should come into play, however I don't really see how. Of course, maybe my steps have an error somewhere, so I would really like to hear other people's opinions on it.","['operator-theory', 'symplectic-linear-algebra', 'functional-analysis', 'ordinary-differential-equations']"
4463695,Gradient of a function that takes in two vectors,"Say I have a function like $$f(\mathbf{x}, \mathbf{y})= \mathbf{x}^T\mathbf{y}$$ How can I compute its gradient?","['multivariable-calculus', 'derivatives']"
4463696,On the properties of sum-of-squares polynomials,"Definition 1. If a multivariate polynomial $f$ can be written as a finite sum of squared polynomials, i.e., $f(x)=\sum_{i = 1}^n g_i^2(x)$ , then $f$ is SOS. Definition 2. If an $n$ -variate polynomial $f$ is nonnegative on $\Bbb R^n$ , then $f$ is positive semidefinite (PSD). We know all SOS polynomials are PSD. However, It is not correct that all PSD polynomials are SOS. For example, the Motzkin polynomial: $$f(x,y)= 1 + x^2 y^4 + x^4 y^2 - 3 x^2 y^2 $$ Suppose $f(x_1, \dots ,x_n)$ is SOS and $\min\limits_{x \in \Bbb R^n} f =: r $ . Is $f(x)-\alpha$ SOS for all $0 \le \alpha <r$ ? If not, would you please give me a counterexample? Edit One way to give a counterexample is to consider a PSD polynomial $p(x)$ which is not SOS. Then, if $p(x) +k$ becomes SOS for $k=k^*$ , $f(x)=p(x) +k^*$ can be our counterexample. This is because $f$ is SOS while $f-k$ is not for some $k\le k^*$ . $m(x,y)= 1 + x^2 y^4 + x^4 y^2 - 3 x^2 y^2 $ is not SOS but PSD. Also, according to https://arxiv.org/abs/math/0103170 , $m(x,y)+k$ is not SOS for any real $k$ . Unfortunately, this example neither proves my first question nor gives a counterexample.","['multivariate-polynomial', 'algebraic-geometry', 'polynomials', 'real-algebraic-geometry', 'sum-of-squares-method']"
4463714,Does $ \sum_{k=1}^{n} \frac{(n-k)^k}{k!} $ have a closed-form expression in terms of $n \in \mathbb{N}$?,"Does $ \sum_{k=1}^{n} \frac{(n-k)^k}{k!} $ have a closed-form expression in terms of $n \in \mathbb{N}$ ? It seems to grow a bit faster than $e^{0.5n}$ , but there's clearly more to it, and I don't know how to look this up. I know that: $ \sum_{k=1}^{\infty} \frac{z^k}{k!} = e^z - 1$ . I'm looking for a similar closed-form expression that eliminates the $k$ . $ \sum_{k=1}^{n} \frac{n^k}{k!} = e^n \frac{\Gamma(n+1,n)}{\Gamma(n)} - 1$ , where the $\Gamma$ ratio is close to 1/2. $ \sum_{k=1}^{\infty} \frac{(n-k)^k}{k!} $ diverges. (It makes no sense for my purpose anyway, once the $n-k$ goes negative and the exponents flip its sign.) Thanks!","['convergence-divergence', 'absolute-convergence', 'sequences-and-series']"
4463715,Evaluating $\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8}$ without Hopital rule,"I want to evaluate this limit without applying Hopital rule, $$\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x^3-8}$$ After expanding the denominator I got, $$\frac{1}{12}\lim_{x\to2}\frac{\sqrt[3]{x^2-x-1}-\sqrt{x^2-3x+3}}{x-2}$$ Here  I can introduce numerator as $f(x)$ And say the limit is in the form $\lim_{x\to 2}\frac{f(x)-f(2)}{x-2}=f'(2)$ But it is very similar to Hopital rule (if not the same). I noticed that the fraction is in the form $2.\frac{\sqrt[3]A-\sqrt B}{A-B}$ or $2.\frac{M^2-N^3}{M^6-N^6}$ but not sure if this helps.","['limits', 'calculus', 'limits-without-lhopital']"
4463748,"Show that $|x| = |g^{-1}xg|$ for all $x,g \in G$. [duplicate]","This question already has answers here : Prove $|xax^{-1}| = |a|$ [duplicate] (3 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved I want to make sure the proof attempt is sound, but also do you have any tips on how to develop the mathematical maturity to know for myself when a proof seems correct? Thanks in advance. Proof: Assume that $|x| = n < \infty$ . Then we can expand $(g^{-1}xg)^n$ to see that there is some cancellation, specifically \begin{align*}
		(g^{-1}xg)^n &= \underbrace{(g^{-1}xg)(g^{-1}xg)...(g^{-1}xg)}_{n\text{ times.}} \\
		& = (g^{-1}x^ng) \\
		& = g^{-1}g \\
		&=1.
		\end{align*} To show that it the lowest such integer, assume for contradiction that there exists some $r \in \mathbb{N}$ for which $(g^{-1}xg)^r = 1$ with $r < n$ . By writing out the expansion exactly as before we can arrive at \begin{align*}
		(g^{-1}xg)^r &= 1 \\ 
		g^{-1}x^rg & = 1 \\
		x^r & = 1,
	\end{align*} which contradicts the minimality of $n$ as $|x| = n$ . To prove the case where $|x| = \infty$ , assume for contradiction that $x$ is of infinite order but that $|g^{-1}xg| = n <\infty$ . Identically to the last contradiction we can arrive at $x^n = 1$ contradicting that $|x| = \infty$ .","['group-theory', 'solution-verification']"
