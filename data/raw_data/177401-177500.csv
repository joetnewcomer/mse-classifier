question_id,title,body,tags
3200876,Evaluating the sum $\sum_{i=1}^n i^2\cdot\lfloor{\frac ni}\rfloor$,"I need to evaluate the sum $$\sum_{i=1}^n i^2\cdot\lfloor{\frac ni}\rfloor$$ After a little bit of math I found that the above sum is equal to: $$\sum_{i=1}^n i\cdot n - \sum_{i=1}^ni\cdot (n\space mod \space i)$$ The first (left) sum has a closed form, $\frac {n^2\cdot (n+1)}{2}$ , but what about the left? I need this for computational use, for very large $n$ , so performing the summation with $O(n)$ is probably the wrong approach, which is why I suspect it has a closed form, or at least another way to perform the summation with smaller complexity, perhaps $O(\sqrt n)$ . If the sum to the right has no closed form, then is there any other way to evaluate the original sum?","['number-theory', 'summation', 'computational-complexity', 'divisor-sum']"
3200917,Show that there exists a set $U$ which is both open and closed and $x \in U \subseteq V$.,"Let $X$ be a compact topological space. Suppose that for any $x, y \in X$ with $x \neq y$ , there exist open sets $U_x$ and $U_y$ containing $x$ and $y$ , respectively, such that $$ U_x \cup U_y = X\quad \text{and}\quad U_x \cap U_y = \varnothing.$$ Let $V \subseteq X$ be an open set. Let $x \in V$ . Show that there exists a set $U$ which is both open and closed and $x \in U \subseteq V$ . My Try: $\forall \;x\neq y\quad U_x \cap U_y = \varnothing$ , $X$ is Hausdorff hence a $T_1$ -space. $X$ is compact Hausdorff hence normal subsequently normal $T_1$ -space i.e. $T_4$ -space. As a consequence of $T_1$ -space, $\{x\}$ is closed in $X$ . Given $V$ is open and $x\in V$ . So $\bbox[5px,border:1px solid red]{\text{there exists open set $U$ such that  $x \in U \subseteq V.$}}$ $U^c$ is closed. By normality of $X$ there exists disjoint open sets $W_1$ and $W_2$ such that $$\{x\}\subseteq W_1\subseteq U \;\text{and}\; U^c\subseteq W_2\implies W_2^c\subseteq U\tag 1$$ $$W_2^c\subseteq U\implies X=W_2\cup W_2^c\subseteq U \cup W_2 \tag 2$$ Claim: $U\cap W_2=\varnothing$ .
For suppose $x\in U\cap W_2$ then, $$x\in U \;\text{and}\; x\in W_2\implies\bbox[5px,border:1px solid red]{{ x\not\in U^c \;\text{or}\; x\not\in W^c_2\implies x\not\in U^c\cup W^c_2}}$$ Also from $(1)$ and $(2)$ we see that $$U^c\cup W^c_2\subseteq U\cup W_2\implies x\not\in U^c\cup W^c_2\subseteq U\cup W_2$$ a contradiction since $x\in U\cap W_2$ but $x\not\in U \cup W_2$ . Hence the claim subsequently $U$ is both open and closed such that $x \in U \subseteq V$ . Is there anything incorrect or missing in my proof ? Are there any other alternative proofs? Update: I figured the two highlighted portions are the incorrect parts of the proof. Thanks to @Thomas Andrews and @Hagen von Eitzen for clarifying my doubts and for their answers.","['alternative-proof', 'general-topology', 'proof-verification', 'separation-axioms']"
3200940,Velleman - Conditional truth table justification,"Velleman's logic in sentence 3 under figure 4 is confusing me. He is using lines two and four of the truth table to infer what Q of line 1 should be. But lines two and four assume P --> Q are true while line 1 assumes P --> Q to be false. So the latter doesn't follow the former in the way Velleman is reasoning here. Furthermore it is already given in the first two columns of P and Q that they are both already F, so how he reasons that there is even a possible ""inference"" of Q being T is very strange. Help anyone. Thanks!","['propositional-calculus', 'proof-verification', 'logic', 'discrete-mathematics']"
3200943,Reduction type at p=2,"Let $E:y^2=x^3+Ax+B$ be an integral, minimal model for an elliptic curve over $\mathbb{Q}$ . The discriminant of $E$ is $\Delta = -2^4(4A^3+27B^2)$ so $E$ always has bad reduction at $p=2$ . A search on the LMFDB suggests that this reduction is always additive. Is this the case, and if so why? My current understanding is that a curve has additive reduction at $p$ if the reduced curve has a triple root. This occurs for some curves of this form, e.g $E_1: y^2=x^3+2$ , but not for all, e.g $E_2:y^2=x^3+1$ . Both examples have additive reduction at 2 however. Is there perhaps a more general definition? Many thanks for any guidance provided.","['algebraic-geometry', 'elliptic-curves']"
3201000,Elements in a sigma algebra generated by a partition [duplicate],"This question already has an answer here : Prove that smallest $\sigma$-algebra on a partition is $\left\{\bigcup_{i \in I} A_i : I \subseteq \{1,...,n\}\right\}$ (1 answer) Closed last year . Suppose you have a set $X$ . Consider the $\sigma$ -algebra $\mathcal{A}:= \sigma\{A_1,A_2,\cdots,A_n\}$ , where $\{A_1,A_2,\cdots,A_n\}$ form a partition on $X$ . Is it then true that every element $A \in \mathcal{A}$ can be written as $$A = \bigcup_{i\in I} A_i, $$ where $I$ is a subset of $\{1,2,\cdots,n\}$ ? It seems very trivial to me, but as I've learned along the years I should not always trust my instincts.",['measure-theory']
3201026,Assigning a value to how 'ordered' a vector is,"Consider a vector which goes in ascending order from (1, 2, 3 ... N). This vector is ordered 'correctly' and I would like to assign it a score of 1, which indicates a perfectly ascending ordered vector. On the other hand, a vector (N, N-1, N-2 ... 3, 2, 1) has the 'worst' possible score of 0. In between 0 and 1, the vector can be 'scrambled' and I would like a way to calculate how 'ordered' it is compared to the perfect case of (1, 2, 3 ... N). Does anyone know of a way to calculate such a score which measures the ordered-ness of a vector?",['statistics']
3201028,"A series such that $\sum {a_n}$ converges, but $\sum {a_{3n}}$ diverges.","Give an example of a convergent series $\sum {a_n}$ such that the series $\sum {a_{3n}}$ is divergent. Give an example of a divergent series $\sum {b_n}$ such that the series $\sum {b_{3n}}$ is convergent. Attempt: I am not sure if this is a valid forumla for a sequence : $ a_{3n-2} = \frac{1}{1+4(n-1)} ,a_{3n-1} = \frac{1}{3+4(n-1)}, a_{3n} = -\frac{1}{2n}$ . This series converges to $\frac{3}{2} \log(2)$ . But, $\sum {a_{3n}}$ diverges. We define $b_{3n-2}=1 , b_{3n-1}=1, b_{3n} = \frac{1}{n^2}$ . The series diverges, but $\sum{b_{3n}}$ converges to $\frac{\pi^2}{6} $ The problem is, I am not sure if the this type of ""formula"" works [unlike the sequence defined by $1/n$ or something. Is this valid to define the sequence ""term-by-term"" (here, three different types of indices)?].","['real-numbers', 'proof-verification', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
3201055,"For any finite abelian group $G$, there is an integer $m$ with $G$ isomorphic to a subgroup of $U(\mathbb{Z}_{m})$.","I want to prove if the following assertion from Rotmans Advanced Algebra page 205 is true: For any finite abelian group $G$ , there is some integer $m$ with $G$ isomorphic to a subgroup of $U(\mathbb{Z}_{m})$ , where $U(\mathbb{Z}_{m})$ are the units of Integers module m. Is this sentence true or false? The book I'm studying says this is not true, but I cannot find a proper counterexample to understand the mentioned claim. Thanks","['field-theory', 'galois-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
3201068,Interesting limit with Poisson and Chi-squared Distribution,"I am stuck with computing the following limit: \begin{align}
 \lim_{ n \to \infty} E \left[ \left(E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}} \, \Big  | \, U  \right] \right)^2 \right].
\end{align} In the above expression, $X$ given $U$ follows Poisson with parameter $U$ and where $U$ is a Chi-square of degree $n$ . Here is what I tried: Suppose we let \begin{align}
V_n =\left(E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}}  \, \Big | \, U  \right] \right)^2
\end{align} Then, using Jensen's inequality \begin{align}
V_n  \le E \left[  \frac{X}{n} + \frac{1}{2} \,  \Big | U \,  \right] =  \frac{U}{n}+\frac{1}{2}
\end{align} Moreover, we have that $E \left[  \frac{U}{n}+\frac{1}{2}   \right]=1+\frac{1}{2}$ . Therefore, by the dominated convergence theorem we have that \begin{align}
\lim_{n \to \infty}  E[  V_n ]=   E[   \lim_{n \to \infty}  V_n ]
\end{align} Therefore, assuming everything up to here is correct, to compute the limit we have to  find \begin{align}
 \lim_{n \to \infty}  V_n&=  \lim_{n \to \infty}   \left(E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}}  \, \Big | \, U  \right] \right)^2\\
&=  \left(  \lim_{n \to \infty}  E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}}  \, \Big | \, U  \right] \right)^2
\end{align} This is the place where I am stuck.  Is it simply another applications dominated convergence theorem?  If so, then I think the answer is \begin{align}
 \lim_{ n \to \infty} E \left[ \left(E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}} \, \Big  | \, U  \right] \right)^2 \right]=\frac{1}{2}.
\end{align} What I mean by another application of dominating convergence theorem is that for every $u>0$ \begin{align}
E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}} \, \Big  | \, U=u  \right] &\le  \sqrt{ E \left[   \frac{X}{n} + \frac{1}{2} \, \Big  | \, U=u  \right]}\\
&=   \sqrt{  \frac{u}{n} + \frac{1}{2} }\\
&= \sqrt{  u + \frac{1}{2}}
\end{align} Therefore, \begin{align}
E \left[  \sqrt{ \frac{X}{n} + \frac{1}{2}} \, \Big  | \, U=u  \right]=  E \left[  \lim_{n \to \infty}  \sqrt{ \frac{X}{n} + \frac{1}{2}} \, \Big  | \, U=u  \right]=  \frac{1}{2}.
\end{align} Is this a correct sequence of steps?  I feel a bit uneasy about the second application of the dominated convergence theorem.","['expected-value', 'convergence-divergence', 'conditional-expectation', 'probability']"
3201103,Example of a continuous function with discontinuous quadratic variation,"Let $f: [0,\infty)\to \mathbb{R}$ . The quadratic variation of $f$ , if it exists, is defined as the function $\langle f\rangle: [0,\infty) \to \mathbb{R}$ with $$  \langle f\rangle_t := \lim_{n\to \infty} \sum_{t_i \in \pi_n(t)} \left( f(t_{i+1}) - f(t_i) \right)^2  $$ for $t \in [0,\infty)$ where $\{\pi_n(t): n\in \mathbb{N}\}$ is a sequence of refining partitions of $[0,t]$ . I am looking for an example of a function $f$ such that $f$ is continuous and its quadratic variation $\langle f\rangle$ exists, but $\langle f \rangle$ is not continuous. Motivation: In probability lecture notes, one sometimes reads (e.g. in the context of the pathwise Ito formula) that a path $X(\omega)$ is assumed to be continuous with continuous quadratic variation. Therefore, I would like to understand why it is necessary to explicitly demand the quadratic variation to be continuous if this is desired.","['quadratic-variation', 'stochastic-calculus', 'real-analysis']"
3201171,Calculating the Kullback Lieber information Criterion (Quasi Maximum Likelihood Method),"Let the observed random variables $X_1,X_2$ be independent and normally distributed with zero mean and variance respectively $\sigma_1^2$ and $\theta \sigma_1^2$ , where $\theta \in \mathbb{R}$ . The model I am considering consist of the set of probability densities given by $$\left \{  \frac{1}{\sqrt{2 \pi X_1^2}}  \exp \left( \frac{-1}{2 } \right) \frac{1}{\sqrt{2 \pi \beta X_1^2}}  \exp \left( \frac{-X_2^2}{2 \beta X_1^2} \right)  \Bigg|  \beta \in \mathbb{R} \right \}$$ I want to minimize the Kullback-Liebler information criterion that is given by $$E\left[   \log \left(   \frac{\frac{1}{\sqrt{2 \pi  \sigma_1^2}}  \exp \left( \frac{-X_1^2}{2  \sigma_1^2}  \right)\frac{1}{\sqrt{2 \pi \theta \sigma_1^2}}  \exp \left( \frac{-X_2^2}{2  \theta \sigma_1^2}  \right)}{  \frac{1}{\sqrt{2 \pi X_1^2}}  \exp \left( \frac{-1}{2 } \right)  \frac{1}{\sqrt{2 \pi \beta X_1^2}}  \exp \left( \frac{-X_2^2}{2 \beta X_1^2} \right)}    \right)  \right]$$ with respect to $\beta$ . The resulting $\beta^*$ should be the value that minimizes the ""distance"" between the model and the true distribution. I would expect $\beta^* = \theta$ since $X_1^2$ is an unbiased estimator for $\sigma_1^2$ . The problem is quickly reduced to minimizing $$E\left[  \frac{1}{2} \log \beta  + \frac{X_2^2}{2 \beta X_1^2}   \right] = \frac{1}{2} \log \beta + \theta \sigma_1^2 E\left[\frac{1}{2 \beta \sigma_1^2 Y_1^2}\right]$$ where $Y_1 = X_1 / \sigma_1$ is distributed as a standard normal random variable (the independence has been used here). The expectation of an inverse  chi squared is derived here and is $\frac{1}{\nu -2}$ (where $\nu$ are the degrees of freedom, in our case $\nu = 1$ ), so we obtain $$ \frac{1}{2} \log \beta - \frac{\theta}{2 \beta} $$ taking the derivative wrt $\beta$ and imposing the equality with zero one obtains $$\frac{1}{\beta} + \frac{\theta}{\beta^2} = 0 \implies \beta^* = - \theta$$ Unfortunately this makes no sense. Have I made a sign mistake or a deeper mistake involving the understanding of the Kullback Liebler criterion? A good writeup on the Kullback-Liebler information criterion is available in The Quasi Maximum Likelihood Method Theory by Chung Ming Kuan.","['statistics', 'statistical-inference', 'maximum-likelihood', 'optimization', 'probability']"
3201191,Notation: $P(x)$ iff $x$ has property $P$,"In set theory (for example), people write $P(x)$ to indicate that $x$ has property $P$ . What is the meaning of this ""expression"" formally? Is $P$ a predicate (a Boolean-valued function on some set [what set?]) that returns $\top$ iff $x$ has property $P$ ? If this is the case, then shouldn't one write $P(x)=\top$ instead of $P(x)$ ?","['predicate-logic', 'first-order-logic', 'logic', 'notation', 'elementary-set-theory']"
3201225,Formula for combinations with equally spaced points?,"How do I calculate a specific value on this triangle array without doing it manually? https://i.sstatic.net/C03lM.jpg Please no sigma notation! I've already got it, what I'm really looking for is a pretty formula like nCr that doesn't make my calculator cry. These numbers are the ways you can place P objects in H slots, under the condition that the objects MUST be equally spaced from each other. The points columns increase like so: (0) + 0 + 0 + 0 + 1 + 1 + 1 + 2 + 2 + 2 + 3 + 3 + 3 + 4 + 4 + 4 + 5 + 5 + 5...       with the additions ticking up every (H-1) rows. This was the example for H = 4. Here's a diagram for H = 6 and P = 3 https://i.sstatic.net/rWooI.png","['matrices', 'combinations', 'binomial-coefficients', 'combinatorics']"
3201227,finding a tangent line to a parabola,"I am practicing for a math contest and I encountered the following problem that I don't know how to solve: For which value of $b$ is there only one intersection between the line $y = x + b$ and the parabola $y = x^2 + 5x + 3$ ? How do I solve it? Edit:
I made a mistake typing the problem. It should be $y = x^2 - 5x + 3$ instead of $y = x^2 + 5x + 3 $ the answer key says the answer is -6.","['contest-math', 'algebra-precalculus']"
3201268,ADM Formulation in General Relativity,"In the ADM(Arnowitt – Deser – Misner) formulation, we can foliate a globally hyperbolic spacetime by spacelike hyper-surface(Cauchy surface) $\Sigma_{t}$ , which parametrised by global time function $t$ . Therefore, in each point of spacelike hyper-surface, we can let $n^{a}$ to be a future -directed timelike unit vector field normal to the hyper surface $\Sigma_{t}$ , which satisfies $n^{a}n_{a} = -1 $ and $n_{a} \propto \nabla_{a}t$ ( $\nabla_{a}$ is associated with spacetime metric. $g_{ab}$ ). Therefore, the spacetime metric $g_{ab}$ induces a spatial metric $\gamma_{ab}$ which is defined as \begin{align}
\gamma_{ab} = g_{ab} + n_{a}n_{b}
\end{align} My difficulties are that why the spacetime metric induce such spatial metric $\gamma_{ab}$ and how can I understand the above equation.","['general-relativity', 'mathematical-physics', 'differential-geometry']"
3201285,Random mapping and entropy ordering,"Let $X_1,X_2$ be discrete random variables such that $H(X_1)<H(X_2)$ where $H()$ is the entropy. We know that for any random mapping $T$ which is invertible ( $T$ is a function of $\omega$ and $X$ , by invertible we mean invertible for every fixed $\omega$ ) and independent of a random variable $X$ the following is true $$H(T(X))\geq H(T(X)|T)=H(T^{-1}T(X)|T)=H(X),$$ and we have $$H(T(X_1))\geq H(X_1)\text{ and }H(T(X_2))\geq H(X_2).$$ If $X_1$ , $X_2$ , $T(X_1 )$ and $T(X_2 )$ have the same support, is it true that $$H(T(X_1))\leq H(T(X_2))? $$ The reason for assuming the same support is the following: If the cardinality of the support of $X_2$ is greater than the cardinality of the support of $X_1$ and $P(T(X_1))$ , $P(T(X_2))$ are uniform then the above does not hold. Please correct me if I made any mistakes. Thank you for your time.","['probability-theory', 'probability', 'information-theory']"
3201304,The set of all limit points is closed,"Let $E'$ be the set of all limit points of a set $E$ . Prove that $E'$ is closed. I'm studying baby Rudin, and this is a partial problem of prob 6 of chap 2. I found this problem is hard, so I want to check if my argument is valid. My argument: I will show that $E'^c$ is open. Take $x\in E'^c$ . Then it implies that $x$ is not a limit point of $E$ . (a point $p$ is a limit point of a set $A$ if every nbhd of $p$ contains a point $q\in A$ such that $p\not= q$ .) So there exists an open nbhd $U$ such that $x\in U$ and $(U\setminus \{x\})\cap E =\varnothing$ . Since $U\setminus \{x\}$ is also open, and it doesn't intersect $E$ , none of elements of $U\setminus \{x\}$ is a limit point of $E$ . i.e., $(U\setminus \{x\})\cap E'=\varnothing.$ Since $x\in E'^c$ , we have $U\cap E'=\varnothing$ , thus $U\subset E'^c$ . Hence every point $x\in E'^c$ is an interior point of $E'^c$ , it follows that $E'^c$ is open. PS: is it possible to show it directly? i.e., showing every limit point of $E'$ is a point of $E'$ ? If yes, How?","['general-topology', 'real-analysis']"
3201314,Closed form for totient related product,"Euler's totient function can be formulated involving a product of the form $\prod\left(1-\frac{1}{p}\right)$ . In particular, if every prime is included in the product, the product can be stated in a closed form $$\prod_{i=1}^n\left(1-\frac{1}{p_i}\right)=\frac{\phi(p_n\#)}{p_n\#}$$ With regard to a sieving method related to twin primes, I have found that by eliminating two numbers, a distance of $k$ on each side of each prime of the form $6k\pm 1$ , candidates for twin primes can be identified. Thus, for each such prime considered, the sieve removes a fraction $\frac{2}{p}$ numbers, leaving $\left(1-\frac{2}{p}\right)$ . Since each prime is coprime to every other prime, sieving over multiple primes would leave a fraction of numbers equal to $$\prod_{i=3}^n\left(1-\frac{2}{p_i}\right)$$ Note that since the primes being considered are of the form $6k\pm 1$ , the index runs from $3$ . I have thought about this a long time, but I can come up with no closed form for this product, and in particular no form related to the totient function. My first question  is: Can anyone provide a closed form for the product? Is a (hypothetical) closed form related in any way to the totient function? Since there is no limit to the number of primes, the product $\prod\left(1-\frac{1}{p}\right)$ , which in effect sieves primes from natural numbers, cannot exhaust every number, no matter how many primes are included in the product. The product I am interested in, $\prod\left(1-\frac{2}{p}\right)$ , sieves numbers at a greater rate. A quick calculation reveals that the product with respect to the first $200$ primes of the form $6k\pm 1$ eliminate over $95\%$ of numbers, but the product decreases ever more slowly as the number of primes included in the product increases. Other than manual calculation, I do not know how to evaluate the product further. My second question is: Does the limit of my product approach $0$ as the number of primes included in the product tends toward $\infty$ ?","['number-theory', 'twin-primes', 'totient-function', 'prime-numbers']"
3201360,Characterizing a module of Kahler differentials,"Consider the $\mathbb C$ -algebra $R=\mathbb C[x,y,z]/(z(y^2-x^3)-1)$ . How to prove that the module of Kahler differentials $\Omega_{R/\mathbb C}$ of $R$ over $\mathbb C$ is a free $R$ -module of rank 2? This $R$ -module is generated by $\{d(f):f\in R\}$ modulo the relations $$d(bb')=bd(b')-d(b)b'\\d(ab+a'b')=ad(b)+a'd(b')$$ for all $a,a'\in \mathbb C,b,b'\in R$ where $$d:R\to \Omega_{R/\mathbb C}$$ is a derivation (a group homomorphism such that $d(fg)=fd(g)+d(f)g$ for all $f,g\in R)$ .","['homological-algebra', 'modules', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3201364,Experiences using Brown's Topology and Groupoids in lieu of Munkres and Hatcher,"I recently became aware of the book Topology and Groupoids by Brown. After seeing a number of questions/answers here on Math.SE that referenced the book and had nice perspectives, I'm curious about its use as a somewhat introductory text for undergraduates. In the past, I have always used Munkres and Hatcher as go-to textbooks. Has anyone else mentored/taught students using this book, and if so: How does it compare to these standard texts? What aspects does the book do well in, and what does it lack? What benefit is there, if any, in introducing a student early on to the perspective that Topology and Groupoids offers?","['general-topology', 'soft-question', 'book-recommendation', 'algebraic-topology']"
3201367,"Why aren't the ""higher twist"" Möbius bands distinct line-bundles over $S^1$?","It is well known (using for instance sheaf cohomology ) that there exist only two possible one- $\mathbb R$ -dimensional vector bundles over $S^1$ : the trivial bundle and the Möbius bundle. But what about the Möbius bands with not one, but $n\in \mathbb N$ half-twists? It is not obvious to me why these are not distinct line bundles. I am also not entirely comfortable with sheaf cohomology arguments. This is also physically motivated; if you make a Möbius band, it's easy to see that two half-twists don't ""cancel"" to make a trivial bundle; so why do they ""cancel"" topologically?","['general-topology', 'algebraic-topology', 'sheaf-theory']"
3201380,Understanding exactness and finding a potential.$[e^x\cos(\pi y^2)+ay-1]dx+[by\; e^x\sin(\pi y^2)+x+y]dy$,"$$\underbrace{[e^x\cos(\pi y^2)+ay-1]}_{P}dx+\underbrace{[by\; e^x\sin(\pi y^2)+x+y]}_Q dy$$ is given I want to check if the given equation is exact, has potential, conservative. Question: If I want to check exactness, I always begin with taking partial derivatives of $P_y$ and $Q_x$ and check whether they are equal or not. I guess if $P$ and $Q$ have continuous partials then we have conservaive and potential by Green's Theorem but what about exactness? Without derivating $P$ and $Q$ just finding potential $\phi$ for the given equation how can I be sure about exactness? What does exactness mean realy? Geometrically or  intuitionaly ? For $b=-2\pi$ and $a=1$ $$P_y=Q_x=-2\pi e^x\sin(\pi y^2+1)$$ Is it enough for exactness? Note: I'm taking a course equivalent to vector calculus. Lecturer does not use a textbook and give us a refference book, he gives lectures in oldstyle. I want to learn the whole thing, any refference text would be appreciated.","['vector-fields', 'calculus', 'ordinary-differential-equations']"
3201392,Is this space equivalent to the James space?,"The James space $J$ is a famous counter-example in functional analysis. It is an example of a Banach space that is isometrically isomorphic to its double dual, but is not reflexive. Define $$J = \big\{ (a_n) \in c_0\, \big|\, |(a_n)|_J < \infty \big\}$$ where $c_0$ denotes the subspace of $l^{\infty}$ of sequences converging to $0$ and $$|a_n|_J^2 := \sup\left\{ \sum\nolimits_{i=1}^{k-1} | a_{p_{i+1}} - a_{p_i}|^2 \; \big| \; 1 \leq p_1 < \ldots < p_k \right\} $$ where the supremum is taken over all finite increasing subsequences of $\mathbb{N}$ . How is this different from requiring that $\sum_{n=1}^{\infty}|a_{n+1} - a_n|^2 < \infty$ ?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'real-analysis']"
3201479,How do I prove this combinatorial identity,"Show that $${2n \choose n} + 3{2n-1 \choose n} + 3^2{2n-2 \choose n} + \cdots + 3^n{n \choose n} \\ = {2n+1 \choose n+1} + 2{2n+1 \choose n+2} + 2^2{2n+1 \choose n+3} + \cdots + 2^n{2n+1 \choose 2n+1}$$ One way that I did it was to use the idea of generating functions.
For the left hand side expression, I can find 2 functions. Consider; $$f_1 (x) = \frac{1}{(1-3x)} \\ = 1 + 3^1x + 3^2x^2 + 3^3x^3 + \cdots + 3^nx^n + \cdots  \\ f_2(x) = \frac{1}{(1-x)^{n+1}} \\ = {n \choose n} + {n+1 \choose n}x + {n+2 \choose n}x^2 + \cdots + {2n-1 \choose n}x^{n-1} + {2n \choose n}x^n + \cdots + $$ Consider the coefficient of $x^n$ in the expansion of $f_1 (x) . f_2 (x)$ . Then the coefficient will be the expression on the left hand side. Now we further consider 2 functions for the right-hand side expression. Consider; $$f_3 (x) = \frac {1}{(1-2x)} \\ = 1 + 2^1x + 2^2x^2 + \cdots + 2^{n-1}x^{n-1} + 2^nx^n + \cdots \\ f_4 (x) = (1+x)^{2n+1} \\= 1 + {2n+1 \choose 1}x + \cdots + {2n+1 \choose n-1}x^{n-1} + {2n+1 \choose n}x^n +\cdots + {2n+1 \choose 0}x^{2n +1} \\ = {2n+1 \choose 2n+1} + {2n+1 \choose 2n}x + {2n+1 \choose 2n-1}x^2 + \cdots + {2n+1 \choose n+2}x^{n-1} + {2n+1 \choose n+1}x^{n} + \\
+ {2n+1 \choose n}x^{n+1} +\cdots + {2n+1 \choose 0}x^{2n +1}$$ Hence the coefficient of $x^n$ is the coefficient of $x^n$ in the expansion of $f_3(x) . f_4(x)$ This is what I managed to do so far. I'm not sure if $f_1(x) .f_2(x) = f_3(x).f_4(x)$ . If the two functions are indeed equal, then I can conclude that their coefficient of $x^n$ must be equal, which will immediately answer the question. If they are equal, how do I show that they are? If the two functions are not equal? How do I proceed to show this question? Edit: It might not be true that the product of the two functions are equal. I tried substituting $x=0.1, n=1$ . Seems like the two values are not equal. How do I proceed with this question?","['combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'generating-functions', 'induction']"
3201483,Is a $C^1$ immersion that is injective on a closed set $K$ injective on a neighborhood of $K$?,"I'm doing problem 7 of section 2.1 on ""Differential topology"" written by Hirsch which goes as \begin{array}{l}{\text { 7. A } C^{1} \text { immersion } f : M \rightarrow N \text { which is injective on a closed subset } K \subset M \text { is injective }} \\ {\text { on a neighborhood of } K . \text { In fact } f \text { has a neighborhood } \mathscr{N} \subset C_{S}^{1}(M, N) \text { and } K \text { has a }} \\ {\text { neighborhood } U \subset M \text { such that every } g \in \mathscr{N} \text { is injective on } U . \text { If } K \text { is compact } \mathcal{N}} \\ {\text { can be taken in } C_{W}^{1}(M, N) .}\end{array} I have no problem about the compact case, but I think the conclusion does not hold when $K$ is not compact for the following counterexample that I cook up. Consider the figure ""8"" $\beta :(-\pi, \pi) \rightarrow \mathbb{R}^{2}, \text { with } \beta(t)=(\sin  t, \sin 2t)$ and $n(t)$ be its normal $n(t)=(-2\cos2t,\cos t)$ . We define $f:(-\pi, \pi)\times(-1,1)\to\mathbb{R}^{2}$ $f(t,s)=\beta(t)+sn(t)$ . Then $f$ is $C^1$ immersion which is injective on a closed subset $K=(-\pi,\pi)\times\{0\}$ but $f$ cannot be injective on a neighborhood of $K$ since $f$ is not injective on $K\cup B_\delta(0)$ for any $\delta>0$ . Does this disprove the problem on the book?","['differential-topology', 'differential-geometry']"
3201578,"Find all function $f:\ \Bbb{R}\ \longrightarrow\ \Bbb{R}$ such that : $f(ax)f(by)=f(ax+by)+cxy$ where $a,b,c>0$","If $f:\ \Bbb{R}\ \longrightarrow\ \Bbb{R}$ and $a,b,c>0$ , then find all function such that : $$f(ax)f(by)=f(ax+by)+cxy,\quad \text{where } a,b,c>0 \text{ for all }  x,y\in \Bbb{R}.$$ My attempt When $x=0$ and $y=0$ , we find $f(0)=1$ or $0$ If $f(0)=1$ then take $x=0$ , we find $f(by)=f(by)$ I don't know how I complete and get answer!! Help me or hint me please. Thanks!","['real-numbers', 'functional-equations', 'real-analysis', 'calculus', 'functions']"
3201586,Doubt from Spivak's proof of inverse function theorem.,why must $y^i-f^i(x)=0$ for all $i$ ?,"['multivariable-calculus', 'inverse-function-theorem', 'real-analysis']"
3201640,Finding $\lim\limits_{x\to1^-}\Bigl(\prod\limits_{n=0}^{\infty}\Bigl(\frac{1+x^{n+1}}{1+x^n}\Bigr)^{x^n}\Bigr)$,Compute : $$L=\lim_{x\to 1^-} \left(\prod_{n=0}^{\infty} \left(\frac {1+x^{n+1}}{1+x^n}\right)^{x^n}\right)$$ My try: Writing out the first few terms I noticed that the limit can be expressed as $$L=\lim_{x\to 1^-} \frac 12\left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1} -x^n}\right)$$ $$L=\frac 12\lim_{x\to 1^-} \left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1}}\right)^{1-x}$$ Converting to exponential form I get $$L=\frac 12 \exp {\left(\lim_{x\to 1^-} (1-x)\sum_{n=1}^{\infty} \left(x^{n-1}\ln(1+x^n)\right)\right)}$$ And got stuck here. I did try to use approximation $\ln x\sim x$ hereon to get final answer as $\frac {\sqrt e}{2}$ but I think it was improper to use approximation and hence I believe that the answer I got is flawed. I also noticed that the form I obtained is quite similar to be used for a Riemann Sum but I didn't get a way to tackle the problem using that way. Can someone please help me with this problem....,"['infinite-product', 'limits', 'riemann-sum', 'real-analysis']"
3201695,Almost sure convergence of $\text{Poisson}(\frac 1n)$ to $0$,"Let $X_n$ a sequence of random variables such that $X_n\sim \text{Poisson}(\frac 1n)$ . Study the almost-sure convergence of $X_n$ . Since $X_n$ is integer-valued and $P(X_n=0) = \exp(-\frac 1n)$ it is easy to prove that $X_n$ converges to $0$ in probability. Note that $P(X_n\geq 1) = 1-\exp(-\frac 1n)\sim \frac 1n$ , hence $\sum_n P(X_n \geq 1) = \infty$ . If the $X_n$ are independent , Borel-Cantelli lemma yields $P\left(\limsup_n \left(X_n\geq 1\right)\right)=1$ , hence $X_n$ does not converge to $0$ almost surely. What can be said when the $X_n$ are not independent ? If the events $(X_n\geq 1)$ are negatively correlated, a stronger version of Borel-Cantelli (derived from Kochen-Stone lemma) still yields $P\left(\limsup_n \left(X_n\geq 1\right)\right)=1$ (see this ). If $X_n\to 0$ a.s, then $P(\liminf_n (X_n=0))=1$ but I haven't been able to get anything useful out of this.","['convergence-divergence', 'probability-theory', 'poisson-distribution']"
3201730,"What does it mean for a vector field to be ""along"" $\partial M$? I think ""along"" is a generalization of ""on"".","My book is An Introduction to Manifolds by Loring W. Tu. The following is an entire subsection (Subsection 22.5) of the section that introduces manifolds with boundary (Section 22, Manifolds with Boundary). Note: I believe that all manifolds with or without boundary referred in this subsection have unique dimensions by some convention (either it's implicit, or it's explicit a I missed it) in the section (The convention of the book is that manifolds with or without boundary can be locally diffeomorphic to different $\mathbb R^n$ 's. See here and here ). According to an errata by Ehssan Khanmohammadi , the only erratum to be made in this subsection is that $c((0,\varepsilon[) \subset M^\circ$ should be changed to $c(]0,\varepsilon[) \subset M^\circ$ . I still have several concerns about this subsection. What exactly is a vector field along $\partial M$ , and what is its domain? Choice 1: It is a mapping whose domain is $\partial M$ and not the whole of $M$ and much like how a manifold with boundary is not a manifold but rather a generalization of a manifold, is not a vector field on $\partial M$ but rather a generalization of a vector field on $\partial M$ , which is defined the same as a vector field on any manifold (without boundary) because $\partial M$ is a manifold (without boundary) as proved in Subsection 22.3 . The generalization is as follows: Let $X$ be a vector field on $\partial M$ . $X$ is a mapping whose domain is $\partial M$ and whose image is the tangent bundle $\cup_{p} T_p(\partial M)$ because to each $p \in \partial M$ , $X$ assigns $p$ to $X_p \in T_p(\partial M)$ . Now, $T_p(\partial M) \subseteq T_pM$ , so $X_p \in T_pM$ . Therefore, $X$ is a vector field along $\partial M$ . However, if we let $Y$ be a vector field along $\partial M$ , then for any $p \in \partial M$ , we might not have the tangent vector at $Y_p$ to be $Y_p \in T_p(\partial M)$ because we are allowed to have that $Y_p \in T_pM \setminus T_p(\partial M)$ because all we are required is that $Y_p \in T_pM$ . Therefore, $Y$ is not necessarily a vector field on $\partial M$ . Choice 2: It is a mapping whose domain is the whole of $M$ and is indeed a vector field on $M$ that has certain properties for its values at $p \in \partial M$ (such as $X_p \in T_pM$ for each $p \in \partial M$ ). I guess this would mean that $X|_{\partial M}$ isn't a vector field on $\partial M$ , which is contrary to some expectation that restrictions of vector fields on $N$ , manifolds with or without boundary to subsets $S \subseteq N$ that are manifolds with or without boundary are vector fields on $S$ or something. In this case, it seems that every vector field on $M$ is a vector field along $\partial M$ ...but conversely as well. Maybe it is a mapping whose domain is the whole of $M$ but is not necessarily a vector field on $M$ . Update: I think this is the expectation in this link. In the link, the definition of ""along"" is for a ""submanifold"" (immersed or embedded) of a manifold which I'm not sure has boundary. I think there's some notion of a ""submanifold"" of a manifold with boundary that makes $\partial M$ as ""submanifold"" of $M$ and then I guess for some reason restrictions of vector fields to ""submanifolds"" are vector fields on the submanifolds, which leads to the generalizing notion of ""along"" I think there could be a convention (like with directional derivative ) that a vector field along $\partial M$ has domain to be all of $M$ but simply satisfies the property for $p \in \partial M$ . See the ""global vector field"" in the link above : I think Lemma 5 in the link is Tu's Proposition 22.10. Also Lee's Problem 8-4 , asked here Choice 3: Somehow there's an equivalence of being defined on $M$ and only on $\partial M$ with some kind of extension. Choice 4: Other I think the next questions shed some light on the answer to this question. For the local expression of $X$ , a vector field along $\partial M$ is the following understanding correct? Asked here . Is this a correct understanding of the smoothness definition? Asked here . $ \ $ Despite the title of the subsection, I don't think there's a definition for outward-pointing vector fields. What is it exactly? Asked here In the proof of Proposition 22.10, is it understood that we cover $\partial M$ by restrictions of the $(U_{\alpha}, x^1_{\alpha}, ..., x^n_{\alpha})$ 's like in questions 2 and 3? Asked here Actually, based on Lee's Problem 8-4 , asked about here , I think we can interpret Proposition 22.10 without the concept of ""along"" as follows: Asked here","['vector-spaces', 'calculus', 'manifolds', 'general-topology', 'differential-geometry']"
3201850,Operator with every complex number as an eigenvalue,"The question : Let $V$ be the space of all functions $f : \mathbb Z \to \mathbb C$ . Find an operator $T : V \to V$ such that every $\lambda \in \mathbb C$ is an eigenvalue i.e. for every $\lambda \in \mathbb C$ , there is some non-zero $f$ such that $Tf = \lambda f$ . This question is surprising, in light of the fact that for a Banach algebra,  the spectrum is a compact subset. So this shows that $V$ cannot be given a Banach algebra structure. For this question, my idea was to hope that $V$ has an uncountable Hamel basis $e_i : i \in \mathbb C \setminus \{0\}$ . I would then have $T$ such that $Te_1 = 0$ , $Te_2 = e_1$ , $Te_3 = 2e_2$ ,..., along with $Te_l = le_l$ for every other $l \neq 1,2,...$ . This would work. I was not able to find such a Hamel basis : I tried using indicator functions of subsets of $\mathbb Z$ , but I need a concrete example (i.e. I am aware that a Hamel basis for $V$ exists and must be uncountable, but I need the exact form of the $e_i$ ), and I am not able to find one.","['hilbert-spaces', 'functional-analysis', 'eigenvalues-eigenvectors']"
3201868,Integral $\int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x y) \ln (x y)} d x d y$,"Evaluate $$\int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x y) \ln (x y)} d x d y$$ I couldn't get very far on this one, so I would appreciate some help =) My attempt so far (transcribed from the comments): By extending it to the Dirichlet Eta Function I evaluated this integral to be $\ln 2$ . I arrived at the identity below after differentiating a unit square integral expression for $\eta(2)$ . $$\eta(s)\Gamma(s)=\int_0^1\int_0^1 \frac{(-\ln(xy))^{s-2}}{1+xy}dxdy$$ But I can't for the love of it solve it any differently than that. I would love to find an ""elementary approach"" if possible.","['integration', 'definite-integrals']"
3201888,"If $f(x)$ Polynomial with real coefficient and $f(0)=1$, $f(2)+f(3)=125$ and $f(x)*f(2x^2)=f(2x^3+x)$ then what is the value of $f(5)$? [duplicate]","This question already has answers here : Solve $f(x)f(2x^2) = f(2x^3+x)$ (4 answers) Closed 5 years ago . If $f(x)$ Polynomial with real coefficient and $f(0)=1$ , $f(2)+f(3)=125$ and $f(x)*f(2x^2)=f(2x^3+x)$ then what is the value of $f(5)$ ? . What I tried $$f(0)=1$$ put x=1 $$f(1)*f(2)=f(3)$$ put x=2 $$f(2)*f(8)=f(18)$$ from this approach, I cant find f(5). Please Suggest a method to solve.","['calculus', 'functions', 'algebra-precalculus']"
3201901,"Finding minimum value of x such that GCD(A+x,B+x) = C where A , B ,C are given","I need to add Minimum non-negative Integer such that I can get the desired GCD(a+x,b+x) Let say A=12 & B=26 For GCD(12+x,26+x) = 1 , x should be 1 For GCD(12+x,26+x) = 2 , x should be 0 For GCD(12+x,26+x) = 7 , x should be 9 For GCD(12+x,26+x) = 14 , x should be 2","['number-theory', 'gcd-and-lcm']"
3201925,Expected number of visits to $k$ before hitting 0,"This problem is from Exercise 5.5.6 in Durrett's Probability: Theory and Examples , 5/E, Use Theorems 5.5.7 and 5.5.9 to show that for simple random walk on $\mathbb{Z}$ , if we start from $k$ the expected number of visits to $k$ before hitting $0$ is $2k.$ Theorem 5.5.7 and 5.5.9 are as follows: Theorem 5.5.7. Let $x$ be a recurrent state in a Markov chain, and $T=\inf(n\geq 1:X_n=x).$ Then $$\mu_x(y)=E_x\left(\sum_{n=0}^{T-1}1_{X_n=y}\right)=\sum_{n=0}^\infty P_x(X_n=y,T>n) $$ defines a stationary measure. Theorem 5.5.9. If the transition probability $p$ is irreducible and recurrent (i.e. all states are) then the stationary measure is unique up to constant multiplication. My attempt: Theorem 5.5.7 and 5.5.9 only apply to the case when the starting point and ending point are the same, but is there any means to extend those theorems to the case when the starting and ending points are distinct? Any hints or advices are welcome!","['stochastic-processes', 'random-walk', 'probability-theory', 'markov-chains']"
3201956,Intuition for commutant and bicommutant,"Let $T$ be an operator on a $\Bbbk$ -linear space $V$ . The notions of commutant and bicommutant of $T$ seem to be important but I don't have any conceptual intuition for them. In hopes of mending this I have a few questions. As background and motivation, below are some relevant results. Elementary Linear Algebra. Proposition. If $T$ is cyclic, then its commutant consists of the polynomial operators in $T$ . Lemma. Suppose $V\cong \bigoplus _i U_i$ is an internal direct sum decomposition. Then each $U_i$ is $T$ -invariant iff each projection $\pi_i$ commutes with $T$ . Corollary. If $S\in \mathrm C^2(T)$ then each $T$ -invariant direct sum decomposition of $V$ is also $S$ -invariant. Theorem. If $V$ is finite dimensional then its bicommutant consists precisely of the polynomial operators in $T$ . I can't summarize the proof well, but the finite dimension is used via existence of cyclic decomposition which reduces to relating the cyclic cases already attended to. Hilbert Spaces. Definition. Let $(V, \left\langle \cdot \right\rangle )$ be an inner product space and $T\in \mathrm{End}_\Bbbk(V)$ . A subspace of $V$ reduces $T$ if both it and its orthogonal complement are $T$ -invariant. By the above lemma, a subspace of a Hilbert space reduces $T$ iff $T$ commutes with its orthogonal projection. In this MSE question the OP asks for intuition for the bicommutant in the context of Hilbert space theory. Two parts of the answer are: The following observation for a Hilbert space. ""A normal operator $S$ is in the bicommutant of $T$ if and only if every reducing subspace of $T$ is also a reducing subspace of $S$ "". The bicommutant $\mathrm C^2(T)$ of $T$ is parallel to the sigma algebra $\sigma (X)$ generated by a random variable $X$ . Indeed the first paragraph of the answer draws analogy between [something involving normal operators and generated algebras] in functional analysis and the Doob-Dynkin lemma . Questions. ""Why"" is the bicommutant of an operator parallel to the sigma algebra generated by a random variable? Where does the commutant fit in, and how to think of it? The Doob-Dynkin lemma is intuitive, but I don't see the intuition behind the assertion made for a normal operator in the linked answer. Why should we expect it?","['hilbert-spaces', 'linear-algebra', 'functional-analysis']"
3202033,Stronger version of finite additivity of Legesgue measure,"Let $m_*(A)$ denote the Lebesgue outer measure, and when $A$ is measurable, let $m(A)=m_*(A)$ be the Lebesgue measure. Let $U=\{E_1,\cdots,E_N\}$ be a finite collection of pairwise disjoint Lebesgue measurable sets (of $\mathbb{R}^n$ ), and denote $E:=\bigcup_{i=1}^{N} E_i$ . We know that (finite additivity): $$ m(E)=\sum_{i=1}^{N}m(E_i)\tag{1}$$ If we add another set $A$ , which may not be measurable, is it true that $$m_*(A\cap E)=\sum_{i=1}^{N}m_*(A\cap E_i)\tag{$*$}$$ (1) is a special case of ( $*$ ) when $A=E$ . Anyone have any idea on proving  ( $*$ )? I thought $m_*(A\cap E)=m_*(A)-m_*(A\setminus E)$ may be useful but I don't know how to proceed. Note that the definition of a set $E$ is measurable is that $m_*(A)=m_*(A\setminus E)+m_*(A\cap E)$ for all $A$ .","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
3202049,Shifting quadratic residues modulo n,"I'm trying to solve a rather specific problem involving quadratic residues. 
Let $p,q$ be primes $(q > 2)$ , such that $p = 2q + 1$ , i.e. p is a safe prime. $\mathbb{Z}^{*}_{p}$ is a multiplicative group modulo $p$ , having order $p-1$ . Next, consider the set $G = \{ x^{2} \bmod p \vert x \in \mathbb{Z}^{*}_{p}  \}$ . It can be shown that $G$ , the set of quadratic residues modulo $p$ , is a subgroup of $\mathbb{Z}^{*}_{p}$ having order $q$ . Let's consider two distinct elements $m_{0},m_{1} \in G$ , where $m_{0} \neq m_{1}$ , and consider the sets $G + m_{0}$ and $G + m_{1}$ where the addition is defined modulo $p$ , i.e. $$G + m_{0} = \{ [g + m_{0}]\bmod p \vert g \in G \}$$ $$G + m_{1} = \{ [g + m_{1}]\bmod p \vert g \in G \}$$ I am trying to prove that $G + m_{0}$ and $G + m_{1}$ are distinct sets, i.e. their symmetric difference is non-empty. Furthermore, if possible, I'm trying to find $m_{0},m_{1}$ that maximizes the cardinality of this symmetric difference, given the group $G$ and looking for an efficient algorithm that does this. I'm not sure if one exists. The second problem is more of a bonus, which I won't be surprised if it's hard to solve. But the first problem (proving distinctness of the two sets) is pretty true I'm sure, except I'm not sure how to prove it.","['number-theory', 'group-theory', 'quadratic-residues']"
3202060,Direct image of a line bundle is a vector bundle,"Let $$f:X\to Y$$ be a holomorphic map between compact connected Riemann surfaces and let $$L\to X$$ be a holomorphic line bundle on $X$ . I'm having some trouble understanding the fact that the direct image $f_*\mathcal{O}(L)$ of the sheaf of sections of $L$ is isomorphic to the sheaf of sections of a holomorphic vector bundle $E$ on $Y$ whose rank is the degree $m$ of $f$ . I understand that we need to show that $f_*\mathcal{O}(L)$ is locally free, and it is easy to see that this is the case around any regular value $y\in Y$ . Indeed, in that case $f^{-1}(y)$ contains $m$ points and we can find a small neighbourhood $U$ of $y$ such that $f^{-1}(U)=\bigsqcup_{i=1}^mU_i$ where $U_i\cong U$ so we have $(f_*\mathcal{O}(L))(U) \cong \bigoplus_{i=1}^m \mathcal{O}(L)(U_i)\cong\bigoplus_{i=1}^m\mathcal{O}(U)$ for $U$ small enough. But I don't understand what happens over a branch point. In the simplest case, let's say $f:\mathbb{C}\to\mathbb{C}$ , $f(z)=z^2$ , with $L$ trivial. Then, for any small disc $\mathbb{D}_r$ of radius $r$ around $0$ we have $f^{-1}(\mathbb{D}_r)=\mathbb{D}_{\sqrt{r}}$ so $$(f_*\mathcal{O})(\mathbb{D}_r)=\mathcal{O}(\mathbb{D}_{\sqrt{r}})$$ which is not isomorphic to $\mathcal{O}(\mathbb{D}_r)\oplus\mathcal{O}(\mathbb{D}_r)$ as it should be. What is wrong with this argument?","['riemann-surfaces', 'vector-bundles', 'algebraic-geometry', 'differential-geometry']"
3202067,Finding $L_n$ so that $\lim_{n \rightarrow \infty} P(L_n<\rho<1)=\alpha$,"Let $(X_1,Y_1),(X_2,Y_2),..,(X_n,Y_n)$ be iid pairs of random variables with $E(X_1)=E(Y_1)$ , $\text{Var}(X_1)=\text{Var}(Y_1)=1$ ,and $\text{cov}(X_1,Y_1)=\rho \in(-1,1)$ .
Given $\alpha>0$ , obtain a statistic $L_n$ which is a function of $(X_1,Y_1),(X_2,Y_2),..,(X_n,Y_n)$ such that $\lim_{n \rightarrow \infty} P(L_n<\rho<1)=\alpha$ . Since the sample distribution of the correlation coefficient is quite hard  to work with I am really not being able to crack this one. Can anyone help?","['statistics', 'confidence-interval', 'probability-theory', 'bivariate-distributions']"
3202070,Condition for a submanifold of complex Euclidean space to be analytic,"I have seen, but don't fully understand, the following statement and sketch proof: Statement: A codimension $2$ submanifold $C \subset \mathbb{C}^2$ such that $C$ has positive intersection index with every complex line is complex analytic (i.e. every tangent space is complex). Sketch Proof:  Given a point $p \in C$ we can write $C$ as a graph over it's tangent space $T_pC$ in some neighbourhood of $p$ . Now if the tangent space is not complex then one can ""easily"" find a complex line which has negative intersection index with $C$ at $p$ . This is a contradiction and hence $T_pM$ is complex. My problems are as follows: 1) What exactly is meant by the tangent space not being complex here?
2) How does one find this line precisely? For the first point if we are considering $C$ as a subset in $\mathbb{C}^2$ then I guess we are talking about the tangent space as a plane in $\mathbb{C}^2$ which would necessarily be complex? For the second point I guess that if we take a non-complex plane a complex plane both in $\mathbb{C}^2$ then their orientations will be oposite and hence the intersection index will be negative? Any help will be greatly appreciated!","['complex-analysis', 'complex-geometry', 'differential-topology', 'differential-geometry']"
3202071,"Combinatorics problem, right solution?","We have $6$ lawyers, $7$ engineers and $4$ doctors. We plan on making a committee of $5$ people, and we want at least one person of each profession on board. So for the first place I choose an engineer, for the second a doctor and for the third a lawyer, leaving only $5$ laywers, $6$ engineers and $3$ doctors left. For the remaining two places, I could have $2$ more people of a single profession. This is $\binom{5}{2}+\binom{6}{2}+\binom{3}{2}$ possibilities. I could also have two people of different professions; a doctor and a laywer, $\binom{5}{1}\binom{3}{1}$ ; a doctor and an engineer, $\binom{6}{1}\binom{3}{1}$ ; or an engineer and a laywer $\binom{6}{1}\binom{5}{1}$ . This adds up to $\binom{5}{2}+\binom{6}{2}+\binom{3}{2}+\binom{5}{1}\binom{3}{1}+\binom{6}{1}\binom{3}{1}+\binom{6}{1}\binom{5}{1}=91$ possible committees. I have two questions regarding my approach to the problem. Question $a)$ is the reasoning right, am I not overcounting? $b)$ Even if it is right, is there a simpler way to do this? You can see that the sum I end up with, though relatively simple, is quite long and tedious. Thanks in advance!","['order-theory', 'combinatorics', 'discrete-mathematics']"
3202090,Prove there is only one solution to the Diophantine equation $p^n - p = q^m - q$ where $p$ and $q$ are odd primes $p\gt q$,"Consider numbers of the form $p^n - p$ where $p>2$ is a prime and $n>1 \in \mathbb{Z}$ . How many of these have a unique representation? $2184$ can be written in this form $2$ ways, $3^7-3, 13^3-13$ , can any other numbers? The OEIS sequence entry A057896 consists of numbers which can be expressed $m^k-m$ more than one way. The number $2184$ is the only one listed which has $m$ prime for both solutions and a computational bound of $10^{24}$ is given under which no other solutions have been found. The related sequence A308324 is a subsequence of A057896 which allows only prime $m$ . The question is closely related to the the Pillai Conjecture since uniqueness would imply there are no other solutions (except $3,13$ ) to the Diophantine Equation $p^n - q^m = p - q$ . In 2001 Bennett showed that for fixed p and q this equation has at most two solutions (no triple representations) and further conjectured that the general equation (allowing $p$ and $q$ to be composite) has only the 8 solutions which he presents. For only one of those solutions $(3,13)$ is $p$ and $q$ both prime. If the conjecture made by Bennett is correct this is the only solution to this equation. See his paper On Some Exponential Equations of
S. S. Pillai for more details.","['conjectures', 'prime-numbers', 'number-theory', 'elementary-number-theory', 'open-problem']"
3202102,A measurable function is an infinite sum of simple measurable functions. Why measurable?,"Let $\left(E,\mathcal{E}\right)$ a measurable space and let $f:E\to[0,+\infty]$ a measurable function. Then $$
f=\sum_{i=1}^{+\infty}a_{i}\chi_{A_{i}}
$$ with $a_{i}\geq0$ and $A_{i}$ measurable. In an attempt to solve this problem, I stumbled upon the suggestion of Adam Hughes proposed here: A measurable function equal to a countable sum of characteristic functions? The solution is convincing, but I don't understand how to prove that $A_{i}$ are measurable. Briefly recapitulate for those who have not read the other post. For all $y\in\mathbb{R_+}$ we can find a set $S_{y}\subset\mathbb{N}^{*}$ such that $$
y=\sum_{k\in S_{y}}\frac{1}{k}
$$ Now we define $$
\forall k\in\mathbb{N}^{*}\quad\quad A_{k}=\left\{ x\in E\mid k\in S_{f\left(x\right)}\right\} 
$$ Then $$ \displaystyle
f=\sum_{k\in\mathbb N^*} \frac{1}{k} \chi_{A_{k}}
$$","['measure-theory', 'probability', 'real-analysis']"
3202164,Why do real positive eigenvalues result in an unstable system? What about eigenvalues between 0 and 1? or 1?,"I'm confused why, if all eigenvalues of a linear system are real and positive, this entails an unstable system. For example, if eigenvalues are between 0 and 1, surely this means the system is gradually shrinking? And even more so, doesn't an eigenvalue of 1 mean the system is ""staying put""? Why is a negative eigenvalue instead related to stability? I'm confused because in the case of eigenvalues of a Markov matrix, it seemed an eigenvalue of 1 meant stability, and 0 < λ < 1 meant it was shrinking. But in both those cases the eigenvalue is positive.","['dynamical-systems', 'linear-algebra', 'stability-theory', 'eigenvalues-eigenvectors']"
3202185,Error in the Chudnovsky Formula,"The Chudnovsky formula $$\frac{1}{\pi} = 12 \sum^\infty_{k=0} \frac{(-1)^k (6k)! (545140134k + 13591409)}{(3k)!(k!)^3 \left(640320\right)^{3k + 3/2}}$$ is generally quoted as converging towards $\pi$ with a linear rate of convergence of 14 decimal places per iteration. However, when having looked for the exact analytic value, I can't seem to find a proof of this anywhere, or even an asymptotic error function describing it, having searched in a proof of the formula for the words 'convergence' and 'error', and only having found unrelated results. I considered the method proposed at the bottom of this page , however the method posted here only works for superlinear formulae - a category into which this shouldn't fall, unless all the sites claiming it to be linear are completely wrong. My final idea was to use a method I'd seen once using the Taylor series to prove the rate of convergence of the Newton Raphson method. However this method appears to require that the function be expressed in the form: $$x_{n+1} = g(x_n)$$ Which means I can't really see any obvious way of rewriting so as to exclude $n$ from the formula. And pretty much any variation on the word 'error' I enter into Google tends to come up with people trying to find errors in their algorithm. Any direction on how to solve this, or to a paper which includes a proof of it, would be greatly appreciated. Thank you for your time! EDIT:
Having written this, I since found the following comment , which relates to my question of finding an exact value for asymptotic convergence, but I still don't know why this would be true.
From further testing the value given in the linked comment seems to be derived from the reciprocal of the value found in the ratio test, but I still have little idea as to why this is true!","['complex-analysis', 'numerical-methods', 'pi', 'asymptotics']"
3202187,Smallest box to fit cuboid in.,You have a cuboid with dimensions of $30$ by $8$ by $4$ inside a box (cube). Every point of the cuboid touches some side of the box. What is the smallest box you can fit this cuboid in? I need the size of the box's edge,['geometry']
3202192,"In topology: Is there a special terminology for a map that is ""almost'' a homeomorphism?","By ""almost"" I mean that it can be slightly perturbed within a given fixed compact domain to obtain a homeomorphism from that domain to its image. To be more precise: Let a compact topological space $U \subset \mathbb{R}^n$ , and let a map $f:U\to M$ such that for any $\epsilon>0$ there exists homeomorphism $f_\epsilon:U\to V$ (continuous function with continuous inverse) between $U$ and $V$ for some $V\subset \mathbb{R}^n$ such that $$
\|f(x)-f_\epsilon(x)\|_2<\epsilon \text{ for all } x \in U. \tag{1}
$$ Examples: a constant map from cube $x \in [0,1]^3$ to $\mathbb{R}^3$ , $f(x): x \mapsto (0,0,0)$ is almost homeomorphism on the cube. Indeed, for any $\epsilon$ one can introduce $f_\epsilon(x): x \mapsto  \frac{1}{2\sqrt{3}} \epsilon( x_1, x_2, x_3)$ which is a homeomorphism, and (1) is satisfied. a function from $\mathbb{R}\to \mathbb{R}$ , $f(x):x^2$ is not almost homeomorphism on $[-1, 1]$ , since there is no inverse map for $x=0$ . In one dimension, I presume, a function is almost homeomorphism if and only if it is  monotonic, but I'm not familiar with the notion of monotonic in higher dimensions.","['general-topology', 'terminology', 'differential-geometry']"
3202219,Prove that $\limsup_{x\to\infty}\left(\cos x + \sin\left(\sqrt2 x\right)\right) = 2$,"Prove that $$
\limsup_{x\to\infty}\left(\cos x + \sin\left(\sqrt2 x\right)\right) = 2
$$ Pretty much always when I ask a question here I do provide some trials of mine to give some background. Unfortunately, this one is such a tough one for me that I don't even see a starting point. Here are some observations though. Let's denote the function under the limsup as $f(x)$ : $$
f(x) = \cos x + \sin\left(\sqrt2 x\right)
$$ Since sin's argument contains an irrational multiplier the function itself is not periodic, perhaps this may be used somehow. I've tried assuming that there exists $x$ such that the equality holds, namely: $$
\cos x + \sin\left(\sqrt2 x\right) =2
$$ Unfortunately, I was not able to solve it for $x$ . I've then tried to use Mathematica for a numeric solution, but NSolve didn't output anything in Reals. The problem becomes even harder since there are some constraints on the tools to be used. It is given at the end of the chapter on ""Limit of a function"". Before the definition of derivatives, so the author assumes the statement might be proven using more or less elementary methods. Also, I was thinking that it could be possible to consider $f(n),\ n\in\Bbb N$ rather than $x\in\Bbb R$ and use the fact that $\sin(n)$ and $\cos(n)$ are dense in $[-1, 1]$ . But not sure how that may help. What would the argument be to prove the statement in the problem section?","['limsup-and-liminf', 'real-analysis', 'calculus', 'limits', 'trigonometry']"
3202252,Probability that n sequences agree in a certain percentage of them,"I am trying to work out what the probability that $n$ coin toss sequences of length $k$ match in $m$ spots. The simplest example would be two sequences of a given length. If I toss a coin 100 times, twice, how many of positions in the sequence will match? My intuition tells me that if I have a fair coin (so $p = 0.5$ ) then I should expect two random sequences to on average, agree in 50% of their locations. I also think that I am correct in saying that the probability of two sequences of length $k$ being in complete agreement is $p^q (1-p)^{(k-q)}$ where $q$ is the number of heads in the sequence. However, I am unable to formalize my particular question (which is a fraction of the sequence, independent of subsequence length), nor am I able to extend this to the probability that 3 sequences of a particular length agreeing in 50% of their locations. Any help would be appreciated. Edit : To be more specific, I am having difficulty with the combinatorics involved when the probability $p \neq 0.5$ . Suppose the first sequence contains 2 heads. It can be $HHTT$ or $HTHT$ or $THTH$ etc.. it doesn't matter. Now I want to calculate the probability that another sequence matches $0, 1, 2, 3, \mathrm{or}\ 4$ of those outcomes. I have chosen $HHTT$ as the first sequence and enumerated the possible matching sequence for each case. In the case of 0 matching outcomes, the only possibility is \begin{array}{cccc}
& T & T & H & H \\
\end{array} In the case of 1 matching outcome, the possibilities are: \begin{array}{cccc}
& H & T & H & H \\
& T & H & H & H \\
& T & T & T & H \\
& T & T & H & T \\
\end{array} In the case of 2 matching outcomes, the possibilities are: \begin{array}{cccc}
& H & H & H & H \\
& H & T & T & H \\
& H & T & H & T \\
& T & H & T & H \\
& T & H & H & T \\
& T & T & T & T \\
\end{array} In the case of 3 matching outcomes, the possibilities are: \begin{array}{cccc}
& H & H & T & H \\
& H & T & T & T \\
& H & H & H & T \\
& T & H & T & T \\
\end{array} Finally, for all 4 matching outcomes, the only possibility is: \begin{array}{cccc}
& H & H & T & T \\
\end{array} Now the number of combinations for matching $m$ outcomes for a sequence of length $k$ is given by ${k\choose m}$ . The only problem is, because the probability is not $0.5$ , I need to enumerate how many of those contain $x$ number of $H$ , so I can sum up $p^q (1-p)^{(k-q)}$ , each multiplied by the appropriate prefactor. However, I am unable to identify the appropriate pattern . This is the main difficulty I am having. Any help towards figuring out a formula for the probability of two (or more!) sequences of length $k$ matching in $m$ spots, where the probability of each outcome is not 0.5 would be greatly appreciated. Thanks!","['permutations', 'combinations', 'probability']"
3202275,What is the Lemoine point useful for?,What is the Lemoine point useful for? Can someone give concrete examples /common example what math problems can be solved with usage of Lemoine point?,"['triangles', 'geometry']"
3202316,How to find $\sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2}$ using real analysis and in an elegant way?,"I have already evaluated this sum: \begin{equation*}
\sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2}=4\operatorname{Li_4}\left( \frac12\right)+\frac{13}{8}\zeta(4)+\frac72\ln2\zeta(3)-\ln^22\zeta(2)+\frac16\ln^42 
\end{equation*} using the identity $\displaystyle\frac{1}{1-x^2}\ln\left(\frac{1-x}{1+x}\right)=\sum_{n=1}^{\infty}\left(H_n-2H_{2n}\right)x^{2n-1}$ but kind of lengthy. any other approaches?","['euler-sums', 'real-analysis', 'alternative-proof', 'harmonic-numbers', 'polylogarithm']"
3202317,"Let $A= \{(0,y,z):z^2 + (y-2)^2=1 \}$ and let $B$ be the set obtained by rotation of $A$ around the $z$-axis. Determine a parametrization for $B$.","I am stuck on the following problem: Let $A= \{(0,y,z):z^2 + (y-2)^2=1 \}$ and let $B$ be the set obtained by rotation of $A$ around the $z$ -axis. Determine a parametrization for $B$ . I have a faint idea that the parametrization should be along the lines of: \begin{align}x&= 2+\cos u \\y&=2 + \cos v \\ z&=\sin v \end{align} In which $y,z$ are the polar coordinates for the circle given in $A$ in the $y,z-$ plane. As $B$ is the rotation along the $z$ -axis, I guessed $x$ should be something like $x=2 + \cos \psi$ akin to $y$ , but the answer in the book is: \begin{align}x&=(2+ \cos v) \cos u \\ y&=(2 + \cos v) \sin u \\ z&=\sin v\end{align} I checked on Wolfram Alpha and my parametrization describes a tube while the solution (correctly) describes a torus, but I don't know how to arrive at it. Is there some general procedure? The book I am reading basically said: ""Look at these (simple) examples. Now handle this"". I think that taking the vector $(2+\cos v,2+ \cos v )$ we'd need to ""regulate"" it's length by multiplying each coordinate by $\cos u, \sin u$ but I can't put it into words in a way that makes sense. Perhaps thinking about polar coordinates with $r=2+\cos v$ ?",['multivariable-calculus']
3202373,"Asymptotic curve with negative Gauss curvature, show $|\tau(P)|=\sqrt{-K(P)}$","Suppose $K(P) < 0$ where $K(P)$ is the Gauss curvature at $P$ , where $K(P) = \det|S_p|$ , the determinant of the shape operator at $P$ . If $C$ is an asymptotic curve with $\kappa(P) \neq 0$ , prove that its torsion satisfies $|\tau(P)|=\sqrt{-K(P)}$ . Hint: If we choose an orthonormal basis $\{U,V\}$ for $T_p(M)$ with $U$ tangent to $C$ , what is the matrix for $S_p$ ? The answer to this hint is that the matrix for $S_p$ will be symmetric, and furthermore the matrix representation of the first fundamental form will be a scalar multiple of the identity matrix. Well, first of all, since $C$ is an asymptotic curve, we have $\kappa N \cdot n=0$ where $N$ is the unit normal vector of the curve and $n$ is the unit normal vector of the surface. I'm having trouble seeing the connection to torsion here, or how the fact that the matrix for $S_p$ is symmetric is going to be useful. Insights greatly appreciated!!",['differential-geometry']
3202374,Schwarz lemma in Stein book,"Show that if $f\colon D(0,R)→\mathbb{C}$ is holomorphic with $|f(z)|\leq M$ for some $M>0$ then $$\bigg|
\frac{f(z)-f(0)}{M^2-\bar{f(0)}f(z)}\bigg|\leq \frac{|z|}{MR}
$$ This is an exercise of Stein's book Schwarz lemma section. But I do not have any idea how can I solve it, first at all, the Schwarz lemma needs that the origin is fixed by the function, but I do not have any value, only that it is bounded. So have I to do a conformal map first? I would be grateful if you give any idea.",['complex-analysis']
3202380,How many functions $f : A \rightarrow A$ are there such that $f \circ f(1) = 2$?,"Given $A = \{1,2,3,4,5\}$ , how many functions $f : A \rightarrow A$ are there such that $f \circ f(1) = 2$ ? The answer states $4 \times 5^3$ , which is close to my answer, but not quite: You would have $4$ choices for $f(1)$ , as $f(1)$ would not be able to equal $1$ . Then you would have $5$ choices for $2, 3, 4, 5$ . So I thought the answer would be $4 \times 5^4$ , not $4 \times 5^3$ . Also, given that I cannot answer this first part, I cant answer the next part which is asking for how many onto functions there are s.t. $f \circ f(1) = 2$ .","['functions', 'combinatorics', 'discrete-mathematics']"
3202410,Differentiability and integrability of a function composed with itself,"I am reviewing for an exam and came across a multi-part question that I am having a hard time with. We are asked to prove or disprove the following statements. If a statement is false, what additional hypothesis would make it true? (Note: $f^{\circ 2}=f\circ f$ ). Let $f:[0,1]\rightarrow [0,1]$ be a continuous function. If $f$ is differentiable, then so is $f^{\circ 2}$ . If $f^{\circ 2}$ is differentiable, then so is $f$ . Let $f:[0,1]\rightarrow [0,1]$ be a function (not necessarily continuous). If $f$ is Riemann integrable, then so is $f^{\circ 2}$ . If $f^{\circ 2}$ is Riemann integrable, then so is $f$ . I think statement 1 is as simple as saying that if $f$ is differentiable, then $f^{\circ 2}$ is the composition of differentiable functions and is thus differentiable. Any guidance would be much appreciated!","['riemann-integration', 'derivatives', 'real-analysis']"
3202466,Riemann Zeta function with the prime counting function in place of $n$,"Interested in the following function: $$ \Psi(s)=\sum_{n=2}^\infty \frac{1}{\pi(n)^s}=\sum_{n=1}^\infty \frac{\lambda_n}{n^s},  $$ where $\pi(n)$ is the prime counting function. Was thinking about: $$ \bar L(s,\chi)=\sum_{n=1}^\infty\frac{|\chi(n)|^2}{n^s}, $$ and $$ L_k(s,\chi)=\sum\limits_{n=1}^\infty \frac{|\chi(n)|^{2k}}{n^s}. $$ However, I don't think these can yield non-periodic integer sequences in the numerator because $\exists k\in\Bbb Z^+: \chi(n)=\chi(n+k)\,\forall n. $ So to achieve non periodicity I settled on modifying the denominator, specifically changing $n$ to $\pi(n)$ : $$ \Psi(s)=\sum_{n=2}^\infty \frac{1}{\pi(n)^s}=1+\frac{1}{2^s}+\frac{1}{2^s}+\frac{1}{3^s}+\frac{1}{3^s}+\frac{1}{4^s}+\frac{1}{4^s}+... $$ and combining like terms: $$ \Psi(s)=\sum_{n=1}^\infty \frac{\lambda_n}{n^s}=1+\frac{2}{2^s}+\frac{2}{3^s}+\frac{4}{4^s}+\frac{2}{5^s}+\frac{4}{6^s}+\frac{2}{7^s}+... $$ So far I've computed the non-periodic sequence, $\lambda_n$ , to $34$ terms: $\lambda_n=\{1,2,2,4,2,4,2,4,6,2,6,4,2,4,6,6,2,6,4,2,6,4,6,8,4,2,4,2,4,14,4,6,2,10\}.$ Just found that $\lambda_n$ is sequence A001223 in the oeis; Prime gaps: differences between consecutive primes. Questions: Does this sum converge for all $Re(s)>1$ ? Is there a closed form for the sums? Can $\Psi(s)$ be analytically continued? If so, how? Where does $\Psi(s)=0$ ? Is there a Euler product for $\Psi(s)?$","['number-theory', 'complex-analysis', 'analytic-number-theory', 'calculus', 'riemann-zeta']"
3202469,Probability of a supremum of a sequence of independent random variables,"I'm trying to prove the next: Suppose $\{X_n\}$ is an independent sequence of random variables. Show that $$P(\sup X_n<\infty)=1$$ if and only if $$\sum_n P(X_n>M)<\infty$$ for some $M.$ I'm having problems with this. Here is my attempt: We have that if : $$\sum_n P(X_n>M)<\infty$$ then $P(X_n>M i.o)=0$ because of Borel-Cantelli lemma. Then $$P(\limsup \{X_n\leq M\})=1=P(\liminf \{X_n\leq M\}),$$ but here I don't find how to relate this with the event $\{\sup X_n<\infty\}$ because not all $X_n$ satisfy $M$ is upper bound for them even though many infinity of them do it. For other direction I was trying a proof based on contradiction and using a Borel $0-1$ law without much success. Any kind of help is thanked in advanced.",['probability-theory']
3202477,Extensions of Chicken Nugget Theorem,"Given that $a,b,c\ $ are pairwise relatively prime, what is the largest number not expressible as a linear combination of $a,b,c\ $ ? What if $a$ and $b$ have common factor $m$ ? What if $a$ and $b$ have common factor $m$ , and $b$ and $c$ have common factor $n$ ?","['number-theory', 'algebraic-number-theory', 'linear-algebra']"
3202508,"Maximum ""stretch factor"" of linear map","Let $A \in M_n (\mathbb R)$ be diagonalizable matrix, let $\lambda_1, \dots, \lambda_n$ be its eigenvalues.
I want to know if the maximum ""stretch factor"" of $A$ is the maximum of its unsigned eigenvalues i.e. $$\sup_{\Vert X \Vert = 1} \Vert AX\Vert = \max_{1\leq i\leq n} |\lambda_i|$$ Any help would be appreciated. $\Vert \cdot \Vert$ will be the euclidean norm of $\mathbb R^n$ .","['matrices', 'linear-algebra']"
3202540,Why does $Re=S^{-1}R$?,"The problem is to show that if $e$ is an idempotent in a ring $R$ , then $Re=S^{-1}R$ where $S=\{1,e,e^2,e^3,\dots\}=\{1,e\}$ . In fact this doesn't even seem plausible to me, because $Re$ is ""smaller"" than $R$ (it's a subring of $R$ ) whereas $S^{-1}R$ is ""larger"" than $R$ (it's an extension of $R$ ).","['localization', 'idempotents', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3202541,Training a Boltzmann Machine (Non restricted),"I'm reading through Neural Network and Deep Learning by Charu C. Aggarwal , more specifically chapter 6 which treats Restricted Boltzmann Machines, section 6.3. treats Boltzmann Machines. I'll try to summarize the content of the section (because I'd like an answer formulated with a similar notation, but that's not essential) and I'll ask the specific question. A Boltzmann Machine (BM) is essentially a probabilistic model expressed as an undirected graph. If $i$ and $j$ are two nodes of such graph then $w_{ij} = w_{ji}$ is the associated weight for the arc $(i,j)$ , it is also assumed $w_{ii} = 0$ , so there's no self loop (no arc starts and ends in the same node). The states that a BM can model are both visible and hidden , i.e. if $q = m + d$ is the total number of states the $d$ are the visible ones and $m$ are the hidden ones. The state can be represented as a pair of vectors $(\vec{v},\vec{h})$ , ( $\vec{v}$ are the visible states and $\vec{h}$ are the hidden states). The pair represent the sets of all possible states $(\vec{v},\vec{h}) = \vec{s} = (s_1,\ldots,s_q)$ . An important concept in BM is the energy defined as $$
E = - \sum_i b_i s_i - \sum_{i,j: i < j} w_{ij} s_i s_j, \;\;\;(1)
$$ and such energy leads to the definition of energy gap $$
\Delta E_i = E_{s_i = 0} - E_{s_i = 1} = b_i + \sum_{j : j \neq i} w_{ij} s_j, 
$$ the actual expression follows from a simple calculation. The probability of a specific state is defined as $$
P(\vec{s}) = \frac{e^{-E(\vec{s})}}{Z}, \;\;\;\; (2)
$$ where $E(\vec{s})$ is given by $(1)$ and $$
Z = \sum_{\vec{s}} e^{-E(\vec{s})}.
$$ Also observe that thanks to the Bayes rule we get: $$
P(s_i = 1 | s_1,\ldots,s_{i-1},s_{i+1},\ldots s_{q}) = \frac{1}{1 + e^{-\Delta E_i}}
$$ and such probability can be used to apply either Gibbs Sampling or Markov Chain Monte Carlo to generate samples. Now the bit I don't understand To perform training we want to maximize the likelihood $$
log(P(\vec{s}) = - E(\vec{s}) - \log Z
$$ which is fine, but the following equation is drawn out of nowhere $$
\frac{\partial P(\vec{s})}{ \partial w_{ij}} = \left\langle s_i,s_j \right\rangle_{data} - \left\langle s_i, s_j \right\rangle_{model} \;\;\;\; (3)
$$ Quoting the book now Here $\left\langle s_i,s_j \right\rangle_{data}$ represents the averaged value of $s_i s_j$ obtained by running the generative process of section 6.1.3, when the visible states are clamped to attribute values in a training point. The averaging is done over a mini-batch of training points. Similarly $\left\langle s_i,s_j \right\rangle_{model}$ represents the averaged value $s_i s_j$ at thermal equilibrium without fixing visible states to training points and simply running the generative process of Section 6.3.1. I've tried to derive the equation by myself but it doesn't lead me to the same expression. Therefore can anyone show me how the equation is derived or point out to a very good reference? The book points to this paper , which I assume is the original one, but it makes use of the KL-Divergence but such measure is not mentioned anywhere. Can anyone show me how to derive the learning algorithm then?","['statistical-inference', 'statistics', 'proof-explanation', 'machine-learning', 'calculus']"
3202542,How to be good at coming up with counter example in Topology,"This is a more generalized question, but does anyone have a set of tips or tricks to come up with distinctive examples and counterexamples in Topology and Analysis? More specific, how can people often come up with exotic sequence or mappings between spaces? I can understand the intuition behind some of the simple fractions in the by playing with simple fractions like $\frac{1}{n}$ , but it seems bizarre to me at this moment how people just come up with maps involving complex numbers, trigonometry between exotic spaces out of nowhere","['intuition', 'general-topology', 'examples-counterexamples']"
3202548,Tyrtyshnikov's proof that polynomial roots depend continuously on the coefficients,"I'm having trouble understanding the following proof, which is taken from A Brief Introduction to Numerical Analysis by Eugene E. Tyrtyshnikov . (Note: The polynomials in the following are complex) Theorem 3.9.1 Consider a parametrized batch of polynomials $$p(x,t)=x^n+a_1(t)x^{n-1}+...+a_n(t),$$ where $a_1(t),...,a_n(t)\in C[\alpha,\beta]$ . Then there exist functions $$x_1(t),...,x_n(t)\in C[\alpha,\beta]$$ such that $$p(x_i(t),t)=0\;\;\;\textit{for}\;\;\;\alpha\le t\le\beta,\qquad i=1,...,n.$$ [The author argues why establishing the existence of one such function is sufficient. I will comment on this part later.] [The author states and proves the Arzelà–Ascoli Theorem, which will be used in the following. Note that he uses ""uniformly continuous"" to mean  equicontinuous.] Proof of Theorem 3.9.1. Build up on $[\alpha,\beta]$ a sequence of uniform grids $$\alpha=t_{0m}<t_{1m}<...<t_{mm}=\beta;\qquad t_{i+1,m}-t_{im}=\frac{\beta-\alpha}{m}.$$ Let $y_m(t)$ be a piecewise linear function with breaks at $t_{0m},t_{1m},...,t_{mm}$ . Define the values at the nodes as follows. Take a root $z_0$ of the polynomial $p(x,\alpha)$ , and, for all $m$ , set $$y_m(t_{0m})=z_{0m}\equiv z_0.$$ Further, let $z_{1m}$ be any of those roots of the polynomial $p(x,t_{1m})$ nearest to $z_{0m}$ , and, by induction, let $z_{i+1,m}$ be any of the roots of the polynomial $p(x,t_{i+1,m})$ nearest to $z_{im}$ . Set $$y_m(t_{im})=z_{im},\qquad i=1,...,m.$$ The uniform boundedness of the piecewise linear functions $y_m(t)$ is evident. I devised the following reasoning with some help: For $i=1,...,n$ , there are constants $A_i$ , such that $|a_i(t)|\le A_i$ for all $t\in[\alpha,\beta]$ , by the Extreme Value Theorem. Let $R\colon=1+\sum_{i=1}^nA_i$ . Then for any $x\in\mathbb{C}$ , such that $|x|\ge R$ , we have \begin{align}
|p(x,t)|\ge&|x|^n-\sum_{i=1}^n|a_i(t)||x|^{n-i}\\
\ge&|x|^n-|x|^{n-1}\sum_{i=1}^nA_i\\
\ge&R^{n-1}\left(|x|-\sum_{i=1}^nA_i\right)\\
\ge&R^{n-1}>0.
\end{align} Thus, the roots of $p(x,t)$ are contained in the disk of radius $R$ centered at the origin. For $t\in[0,1],z_1,z_2\in\mathbb{C}$ , $$|tz_1+(1-t)z_2|\le t|z_1|+(1-t)|z_2|\le\max\{|z_1|,|z_2|\}.$$ This shows that each $y_m(t)$ attains it's maximum absolute value at one of the nodes. The values at the nodes are roots of $p(x,t)$ , hence $R$ uniformly bounds the $y_m(t)$ . The uniform continuity emanates from the inequality $$|z_{i+1,m}-z_{im}|\le|p(z_{im},t_{i+1,m})|^{\frac{1}{n}}=...$$ Why does this hold? This is the most enigmatic part of the proof for me. I assume this part uses that $|z_{i+1,m}-z_{im}|$ was minimized in the construction, but I don't see how it comes together at all. $$...=|p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{\frac{1}{n}}\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{\frac{1}{n}},$$ where $R\ge1$ is the radius (not necessarily minimal) of a circle encompassing all the roots of all the polynomials $p(x,t)$ for $\alpha\le t\le\beta$ . The existence of such an $R$ has already been established above. The equality is obvious as $p(z_{im},t_{im})=0$ by construction. For any $z_{im}$ , we have either $|z_{im}|<1$ , in which case $|z_{im}|^k\le|z_{im}|<1\le R\le R^n$ for $k=1,...,n-1$ , or $|z_{im}|\ge1$ , in which case $|z_{im}|^k\le R^k\le R^n$ for $k=1,...,n-1$ . Then, \begin{align}
|p(z_{im},t_{i+1,m})-p(z_{im},t_{im})|^{1/n}&=\left|z_{im}^n+\sum_{j=1}^na_j(t_{i+1,m})z_{im}^{n-j}-z_{im}^n-\sum_{j=1}^na_j(t_{im})z_{im}^{n-j}\right|^{1/n}\\
&=\left|\sum_{j=1}^nz_{im}^{n-j}(a_j(t_{i+1,m})-a_j(t_{im}))\right|^{1/n}\\
&\le\left(\sum_{j=1}^n|z_{im}|^{n-j}|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\
&\le R\left(\sum_{j=1}^n|a_j(t_{i+1,m})-a_j(t_{im})|\right)^{1/n}\\
&\le R\left(\max_{\substack{\alpha\le t_1,t_2\le\beta\\|t_1-t_2|\le\frac{\beta-\alpha}{m}}}\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|\right)^{1/n}.
\end{align} The last inequality follows since $\alpha\le t_{i+1,m},t_{im}\le\beta$ and $|t_{i+1,m}-t_{im}|=\frac{\beta-\alpha}{m}$ . It remains to see why that maximum actually exists: Let $D\colon=\left\{(t_1,t_2)\in[\alpha,\beta]^2\mid|t_1-t_2|\le\frac{\beta-\alpha}{m}\right\}$ . This set is clearly bounded. Now if $((t_{1n},t_{2n}))_n$ is a sequence in $D$ such that $(t_{1n},t_{2n})\rightarrow(t_1,t_2)$ , then taking limits in $|t_{1n}-t_{2n}|\le\frac{\beta-\alpha}{m}$ yields $|t_1-t_2|\le\frac{\beta-\alpha}{m}$ , so $(t_1,t_2)\in D$ . Thus, $D$ is closed and, by Heine-Borel, compact. The function $f\colon D\rightarrow\mathbb{R}$ with $f(t_1,t_2)=\sum_{j=1}^n|a_j(t_1)-a_j(t_2)|$ is continuous since $a_j(t)$ is continuous for $j=1,...,n$ . Thus, by the Extreme Value Theorem, $f$ attains a maximum on $D$ . Even then, however, I do not see how the above inequality implies equicontinuity. Demonstrating equicontinuity would require bounding $|y_m(z_1)-y_m(z_2)|$ for all $m\in\mathbb{N}$ in terms of $|z_1-z_2|$ . Now, if $t_{im}\le z_1,z_2\le t_{i+1,m}$ , then $|y_m(z_1)-y_m(z_2)|\le|z_{im}-z_{i+1,m}|$ . However, one cannot impose such conditions on $z_1,z_2$ , especially since the nodes get arbitrarily close for arbitrarily large $m$ . So how does one use the above inequality to prove equicontinuity? Using the Arzela-Ascoli theorem we find a uniformly convergent subsequence. Take into account that the limit of a uniformly convergent sequence of continuous functions on $[\alpha,\beta]$ must be a continuous function. The only thing left to check is that the limit function $y(t)$ satisfies $p(y(t),t)=0$ for all $\alpha\le t\le\beta$ . That will do the proof. $\quad\square$ Let $(y_{m_k})_k$ be the uniformly convergent subsequence. Let $t\in[\alpha,\beta]$ be arbitrary and $t_{i_km_k}$ be the node closest to $t$ for each $k\in\mathbb{N}$ . Then, by construction, we have $|t-t_{i_km_k}|\le\frac{\beta-\alpha}{m_k}\rightarrow0$ , hence $t_{i_km_k}\rightarrow t$ . Now, since $y_{m_k}\rightarrow y$ uniformly , $y_{m_k}(t_{i_km_k})\rightarrow y(t)$ . Finally, from the continuity of the $a_i(t)$ and the standard results on sums and products of limits, we have $$0=p(z_{i_km_k},t_{i_km_k})=p(y_{m_k}(t_{i_km_k}),t_{i_km_k})\rightarrow p(y(t),t),\qquad\text{hence}\;\;p(y(t),t)=0.$$ Now the initial paragraph I alluded to earlier: To begin the proof, note that it is sufficient to establish the existence of any single continuous function $x_n(t)$ such that $p(x_n(t),t)=0$ for $\alpha\le t\le\beta$ . Should this be done, we write $$p(x,t)=(x-x_n(t))q(x,t),$$ where $q(x,t)=x^{n-1}+b_1(t)x^{n-2}+...+b_{n-1}(t)$ . On the strength of the familiar algorithm for diving polynomials, $b_1(t),...,b_{n-1}(t)\in C[\alpha,\beta]$ . So we may prove it by induction. The application of polynomial division is clear since $x_n(t)$ is a root of $p(x,t)$ for any $t\in[\alpha,\beta]$ . Comparing coefficients also yields that the coefficient of $x^{n-1}$ in $q(x,t)$ must be $1$ . $p(x,t)$ is continuous in $t$ due to the continuity of the $a_i(t)$ , so if $t_k\rightarrow t$ , then $$(x-x_n(t_k))q(x,t_k)=p(x,t_k)\rightarrow p(x,t)=(x-x_n(t))q(x,t).$$ However, we already know $(x-x_n(t_k))\rightarrow(x-x_n(t))$ by the continuity of $x_n(t)$ . Is this sufficient to conclude $q(x,t_k)\rightarrow q(x,t)$ ? (I believe it should be so, but the multiple variables and attempts to avoid an accidental division by zero make me wary.) Furthermore, how may we conclude the continuity of the individual coefficients $b_i(t)$ for $i=1,...,n$ from the continuity of $q(x,t)$ in $t$ as a whole? Can this be done by using the linear independence of the monomials or something of the sort? Finally, the statement of the Theorem only says there exist functions $x_1(t),...,x_n(t)$ which are roots of $p(x,t)$ for each $t\in[\alpha,\beta]$ . However, this would trivially be satisfied by already choosing them all to be the same function. Does the inductive step not actually yield the stronger claim that $$p(x,t)=(x-x_1(t))\cdot...\cdot(x-x_n(t))\text{ for }t\in[\alpha,\beta],$$ which entails that, for each $t\in[\alpha,\beta]$ , $x_1(t),...,x_n(t)$ are precisely all the $n$ roots of $p(x,t)$ counted with multiplicity? Any answer to the specific questions I asked in the above, as well as any correction of potential mistakes I committed in my additions or any supplementation that the argument may still need will be appreciated. Thanks in advance.","['proof-explanation', 'equicontinuity', 'polynomials', 'analysis']"
3202574,Lower bounds on sum of squared sub-gaussians,"Letting $\left\{X_{i}\right\}_{i=1}^{n}$ be an i.i.d. sequence of zero-mean sub-Gaussian variables with parameter $\sigma,$ define $Z_{n} :=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} .$ Prove that $$
\mathbb{P}\left[Z_{n} \leq \mathbb{E}\left[Z_{n}\right]-\sigma^{2} \delta\right] \leq e^{-n \delta^{2} / 16} \quad \text { for all } \delta \geq 0
$$ I know some results which says that the square of sub-gaussian variable are sub-exponential and maybe apply Chernoff bounds to the sum. But I am still not be able to prove this formally. Any hints will be very helpful.","['statistics', 'upper-lower-bounds', 'inequality']"
3202627,Integral that is continuous and looks like it converges to a geometric series,I think the key word is continous. the RHS totally looks like a sum from a geometric series but I dont see a trick when I think there is one .,"['integration', 'multivariable-calculus', 'calculus', 'improper-integrals']"
3202662,$x^\alpha$ as an example of an $\alpha$-Hölder continuous function,"I saw the following statement by user Mark Joshi in response to the question : Non-trivial exemple of Hölder continuous function. $x^\alpha$ for $x > 0$ and $0$ otherwise for $0 < \alpha < 1$ is Holder continuous of order $\alpha$ I cannot seem to prove this statement. How do I proceed to show that the function $f(x) = x^{\alpha}$ is Holder continuous of order $\alpha<1$ , i.e., $|f(x_1) - f(x_2)| \leq c |x_1-x_2|^{\alpha}$ for all $x_1, x_2 \in (0, \infty)$ , and some $c>0$ .","['continuity', 'functions', 'holder-spaces', 'real-analysis']"
3202665,A characterization of the probability density given information about a sum,"I came across the following probability theorem. Let $X_1$ and $X_2$ be independent and identically distributed with density $f(x,\theta)$ . If the random variable $Z=X_1+X_2$ is such that \begin{align}
\log(f(X_1=x_1,X_2=x_2,\theta))&=\log{(f(X_1=x_1,\theta))}+\log{(f(X_2=x_2,\theta)}\\
&=\phantom{aaaaaa}g(Z,\theta)+h(x_1,x_2)
\end{align} Then $f(x,\theta)$ can be written as $f(x,\theta)=exp(xc(\theta)+d(\theta)+S(x))$ for $c(\theta), d(\theta)$ and $S(x)$ real valued functions. The book I am following has the following sketch for the proof Define $r(x,\theta)=\log{(f(x,\theta))}-\log{(f(x,\theta_0))}$ for any fixed value of $\theta_0$ and let $q(x,\theta)=g(x,\theta)-g(x,\theta_0)$ . Then it follows that $q(x_1+x_2,\theta)=r(x_1,\theta)+r(x_2,\theta)$ . Now, it follows that \begin{equation}
r(x_1,\theta)-r(0,\theta)+r(x_2,\theta)-r(0,\theta)=r(x_1+x_2,\theta)-r(0,\theta) \quad (1)
\end{equation} The rest of the proof follows easily from the equation above and is left to the reader I have some questions regarding the proof. I don't understand how equation (1) is obtained from $q(x_1+x_2,\theta)=r(x_1,\theta)+r(x_2,\theta)$ Why the conclusion regarding the exponential follows from equation (1) above? What is the intuition behind the use of $q(x,\theta)$ and $r(x,\theta)$ in the proof?","['probability-distributions', 'probability-theory', 'probability']"
3202744,"Let $X=\{1,2,\ldots,10\}$. Define the relation $R$ on $X$ by: for all $a,b\in X$, $aRb$ if and only if $ab$ is even.","Question: Find the number of subsets $S$ of $X$ (of any size) that satisfy the following property: $\forall a \in S$ , $\exists b \in S$ so that $a R b$ . I have a discrete math final coming up and this was one of the questions on the practice final. The answer key for the test gives a different answer I really want to be sure. The answer key says the answer is $2^{10}-2^5$ however i really think it should be $2^{10}-31$ or $2^{10}-2^5 +1$ since the way the answer key solved it was it subtracted the power set of all the possible subsets of just having odd elements from the power set of the main set. This however, fails to take into account the empty set and I believe the empty set satisfies the condition vacuously as it says ""for any elements $a,b$ in the set if $aRb$ "". Now, an empty set has no elements so it should vacuously satisfy the condition. Hence there are $31$ sets not $32$ that should be subtracted from $2^{10}$ . Do you agree?","['combinatorics', 'discrete-mathematics']"
3202758,Stuck: Finding an Isomorphism for an Invertible Ring,"I'm stuck on a problem creating an isomorphism between rings. Specifically, let $\mathbb{Z}[\sqrt{7}] = R$ . Then for the invertible group $(R/3R)^\times$ , I want to find an isomorphism to another group (i.e. of the form $(R/3R)^\times \cong \mathbb{Z}/a\mathbb{Z} \times \mathbb{Z}/b\mathbb{Z}$ ). It's here that I'm stuck, as I'm pretty new to number theory (started a couple weeks ago), so any help would be appreciated. Cheers","['group-isomorphism', 'modular-arithmetic', 'number-theory', 'finite-rings', 'abstract-algebra']"
3202769,Why are these two characterizations of compact operators equivalent?,"If $T$ is a bounded linear operator on a Hilbert space $H$ , then I have heard that the following two things are true: $T$ is compact if and only if $T(C_1)$ is compact where $C_1$ is the closed unit ball. $T$ is compact if and only if for every bounded sequence $(x_n)$ , $T(x_n)$ has a convergent subsequence. My question is, why are these two characterizations of compact operators on Hilbert spaces equivalent?  Specifically, why does the the second clause of statement 2 imply the second clause of statement 1? If for every bounded sequence $(x_n)$ , $T(x_n)$ has a convergent subsequence, why does that imply $T(C_1)$ is compact?  After all, couldn't it be there be a sequence $(x_n)$ in $C_1$ such that $(T(x_n))$ has subsequences which converge to points in $H$ , but none of those subseqeunces converge to a point in $T(C_1)$ ?","['banach-spaces', 'compact-operators', 'hilbert-spaces', 'functional-analysis', 'general-topology']"
3202772,Does outward-pointing vector field mean each tangent vector at the boundary is outward-pointing?,"My book is An Introduction to Manifolds by Loring W. Tu. The following is an entire subsection (Subsection 22.5) of the section that introduces manifolds with boundary (Section 22, Manifolds with Boundary). Note : I believe that all manifolds with or without boundary referred in this subsection have unique dimensions by some convention (either it's implicit, or it's explicit a I missed it) in the section, contrary to the convention of the book (See here and here ). Questions: According to an errata by Ehssan Khanmohammadi , the only erratum to be made in this subsection is that $c((0,\varepsilon[) \subset M^\circ$ should be changed to $c(]0,\varepsilon[) \subset M^\circ$ . I still have several concerns about this subsection. What exactly is a vector field along $\partial M$ , and what is its domain? Asked here . For the local expression of $X$ , a vector field along $\partial M$ is the following understanding correct? Asked here . Is this a correct understanding of the smoothness definition? Asked here . $ \ $ Despite the title of the subsection, I don't think there's a definition for outward-pointing vector fields. What is it exactly? Choice 1: An outward-pointing vector field $X$ is a vector field on $M$ such that for each $p \in \partial M$ , $X_p$ is an outward-pointing tangent vector. In this case and if vector fields along $\partial M$ are defined with domain $M$ (see question 1), an outward-pointing vector field is not necessarily a vector field along $\partial M$ . Choice 2: An outward-pointing vector field $X$ is a vector field on $\partial M$ such that for each $p$ in the domain of $X$ , which is $\partial M$ , $X_p$ is an outward-pointing tangent vector. In this case and if vector fields along $\partial M$ are defined with domain $\partial M$ (see question 1), an outward-pointing vector field necessarily is a vector field along $\partial M$ . Choice 3: An outward-pointing vector field $X$ is a vector field along $\partial M$ such that for each $p$ in the domain of $X$ (see question 1), if $p \in \partial M$ , then $X_p$ is an outward-pointing tangent vector. In this case, an outward-pointing vector field necessarily is a vector field along $\partial M$ , by definition of outward-pointing vector field regardless of the domain defined for a vector field along $\partial M$ (see question 1). Choice 4: other In the proof of Proposition 22.10, is it understood that we cover $\partial M$ by restrictions of the $(U_{\alpha}, x^1_{\alpha}, ..., x^n_{\alpha})$ 's like in questions 2 and 3? (i'll repeat below, so this question will be self-contained.) Namely: 1. Consider each $p \in \partial M$ . 2. View $p \in M$ . 3. Obtain a $(U_p, x^1_p, ..., x^n_p)$ for each $p$ . 4. Restrict the $(U_p, x^1_p, ..., x^n_p)$ 's to $U_p \cap \partial M$ to get $(U_p \cap \partial M, x^1_p|{U_p \cap \partial M}, ..., x^n_p|_{U_p \cap \partial M})$ . 5. Since $\partial M$ is Lindelöf since all manifolds without boundary are Lindelöf, we can get a countable subcollection of the $(U_p, x^1_p, ..., x^n_p)$ 's to restrict to $U_p \cap \partial M$ . There are also other ways to cover $\partial M$ . (If $\partial M$ were compact, then we could have a finite subcollection) Hence we have that the cover is indexed "" $\alpha$ "" in some index set $A$ ( $\alpha \in A$ ) instead of specifically by $p$ in $\partial M$ . Update : I think the answer is negative. But here I assume that Tu is following the same convention as Munkres (this is the convention introduced in the section on compact spaces): For a topological space $X$ , $\{U_{\alpha}\}_{\alpha \in A}$ is a cover (an open cover) of $X$ if $X = \bigcup_{\alpha \in A} U_{\alpha}$ and $U_{\alpha}$ 's are subsets (open subsets) of $X$ . For a topological subspace $Y \subseteq X$ , we still have that $\{V_{\alpha}\}_{\alpha \in A}$ is a cover (an open cover) of $Y$ if $Y = \bigcup_{\alpha \in A} V_{\alpha}$ and $V_{\alpha}$ 's are subsets (open subsets) of $Y$ But we have that $\{W_{\alpha}\}_{\alpha \in A}$ is a cover (an open cover) of $Y$ by subsets (open subsets) of $X$ if $Y \subseteq \bigcup_{\alpha \in A} W_{\alpha}$ and $W_{\alpha}$ 's are subsets (open subsets) of $X$ . Therefore, it is correct that we do not expect $\partial M = \bigcup_{\alpha \in A} U_{\alpha}$ , but we still say $\{U_{\alpha}\}_{\alpha \in A}$ is a cover of $\partial M$ because $\partial M \subseteq \bigcup_{\alpha \in A} U_{\alpha}$ and $U_{\alpha}$ 's are subsets of $M$ Actually, based on Lee's Problem 8-4 (mentioned here ), I think we can interpret Proposition 22.10 without the concept of ""along"" as follows: On a manifold $M$ with boundary $\partial M$ , there is a smooth vector field $X$ on $M$ such that each $X_p$ is outward-pointing for $p \in \partial M$ . Is that correct? I really think there is no need for the concept of ""along"" here because from $X$ 's being a vector field on $M$ , we have $X_p \in T_pM$ whether $p \in \partial M$ or $p \in M^\circ$ . I could be wrong. Unless we somehow expect $X_p \in T_p(\partial M)$ , I think ""along"" does not seem to add anything new here. Of course the concept of ""along"" is new if we're talking about generalizing from vector fields $Y$ ""on"" $\partial M$ to vector fields $Y$ ""along"" $\partial M$ , as may have been the case in the paragraph before Proposition 22.10 (see question 1). But for Proposition 22.10 specifically, I think the concept of ""along"" can be omitted.","['vector-fields', 'smooth-manifolds', 'manifolds', 'general-topology', 'differential-geometry']"
3202812,Simplifying $\cos(2\arcsin(x))$ using only pythagorean trigonometric identity,"I know that one can simplify $\cos(2\arcsin(x))$ using $\cos(a+b)=\cos(a)\cdot\cos(b)-\sin(a)\cdot\sin(b)$ : \begin{alignat}{1}
\cos(2\arcsin(x))&=\cos^2(\arcsin(x))-\sin^2(\arcsin(x))
\\&=1-2\sin^2(\arcsin(x))
\\&=1-2x^2
\end{alignat} I tried to make this simplification using only $\sin^2(x)+\cos^2(x)=1$ : \begin{alignat}{1}
\cos^2(2\arcsin(x))&=1-\sin^2(2\arcsin(x))
\\ \left|\cos(2\arcsin(x))\right|&=\sqrt{1-\sin^2(2\arcsin(x))}
\\ &=\sqrt{1-\left(2x\sqrt{1-x^2}\right)^2}
\\ &=\sqrt{1-4x^2 |1-x^2|}
\\ &=\sqrt{1-4x^2(1-x^2)}
\\ &=\sqrt{1-4x^2+4x^4}
\\ &=\sqrt{\left(2x^2-1\right)^2}
\\ &=|2x^2-1|
\end{alignat} And then I could not figure out how to proceed. So, how to get rid of $|\cdot|$ in: $$\left|\cos(2\arcsin(x))\right|=|2x^2-1|$$ and get $1-2x^2$ ? Note: this is a part of my attempt to solve the integral $\int x^2\cdot\sqrt{1-x^2}\,\,\mathrm dx$ by trigonometric substitution.","['algebra-precalculus', 'functions', 'trigonometry', 'absolute-value']"
3202823,Wikipedia inconsistency with the index 2 subgroups of the Hyperoctahedral group.,"On the page https://en.wikipedia.org/wiki/Hyperoctahedral_group , under the subgroups section is stated: ""Viewed as a wreath product, there are two natural maps from the hyperoctahedral group to the cyclic group of order 2: one map coming from ""multiply the signs of all the elements"" (in the n copies of $\{\pm 1\}) $ $...$ The kernel of the first map is the Coxeter group $D_n$ ."" It is stated later that the kernal of the determinant map to $\{\pm 1\}$ is a distinct index 2 subgroup. However on the page https://en.wikipedia.org/wiki/Generalized_permutation_matrix#Signed_permutation_group , under Signed permutation group(isomorphic to Hyperoctahedral group) section, is stated: ""Its index 2 subgroup of matrices with determinant 1 is the Coxeter group $D_{n}$ "" Am I missing something or are these statements contradictory?","['group-theory', 'normal-subgroups']"
3202846,"Is this elementary, nilpotent-free approach to automatic differentiation strong enough for real analysis? How similar is it to Newton's system?","This is a sequel to this question: Is the theory of dual numbers strong enough to develop real analysis, and does it resemble Newton's historical method for doing calculus? The ring of ""dual numbers"" is of the form $a+b\,dx$ , where $dx$ is an ""infinitesimal"" quantity that squares to 0; they can be constructed pretty simply as the quotient of the polynomial ring $\Bbb R[dx]/dx^2$ . In general, the useful thing about the dual numbers is in automatic differentiation: for any analytic function $f$ , we have $f(x+dx) = f(x) + f'(x) dx$ These are interesting because they form a minimalist structure that is sufficient for a good portion of calculus, and have even been used to teach high school calculus . They also share some superficial similarity with Newton's approach in neglecting the square of an infinitesimal. The main issue people mentioned about them in my original thread is that they don't form a field. This can cause some deep issues, as also pointed out by Terry Tao in the above blog post. As a result, things like nonstandard analysis or smooth infinitesimal analysis can be better behaved in some circumstances. The strange thing is, however, these issues also seem to vanish if we just totally drop the notion that $dx^2 = 0$ , and just let it be another real variable with an odd name. Or, if we want, we can think of this as something like the ring of formal power series $\Bbb R[[dx]]$ , or the field of formal Laurent series $\Bbb R((dx))$ if a field is desired. Later edit: or something like the Levi-Civita field would probably be best. Either way, for any analytic function, we get an expansion of derivatives of all orders: $\displaystyle f(x+dx) = f(x) + f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ...$ which can be calculated symbolically to any desired degree, and of which the ""dual number"" version is a truncation. Both expansions are easy to see by starting with polynomials and extending to analytic functions from there. This can also be thought of as analogous to Newton's approach, where we are simply using the ""coefficient extraction"" operator to get the coefficient of the $dx$ term, ignoring the higher-order terms. It is fairly easy to define other approaches to differentiation in terms of this, since we have: $\displaystyle f(x+dx) - f(x) = f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ...$ All of these different approaches are just different ways to pluck off that $dx$ term: Quotienting the formal power series ring by $dx^2$ , canceling out the rest of the $dx^n$ terms (the dual number approach) Treating $dx$ as a real variable, rather than a formal quantity, dividing by it, and taking the limit as $dx \to 0$ , canceling out the rest of the $dx^n$ terms (Cauchy's approach) Treating $dx$ as an element in the formal Laurent series, dividing by it, and using the ""coefficient extraction"" operator on the formal power series to get the constant term (i.e. the non-standard analysis ""standard part"" approach, basically) Just noting formally that the $dx$ term is the one you want, and ignoring $dx^2$ and larger (Newton's approach?) The strange thing is, though, although we have presented this in the context of automatic differentiation, this is literally the most elementary theorem possible - we've just rediscovered Taylor's theorem with a change of variables, evaluating the function $f(x+dx)$ symbolically at some $x$ while leaving $dx$ indeterminate. We could even treat $dx$ as a simple real variable rather than a new formal quantity, with the only caveat being that the above may only hold for ""sufficiently small"" $dx$ , due to convergence issues. So in some sense we have gone in a huge circle, but gotten a better theory than that of the dual numbers, but which has no nilpotents or infinitesimals at all - just indeterminate real variables. My questions: This seems to be so basic that it's almost surprising that it works so well. Is there some drawback I'm not seeing? Thinking of this as the field of formal Laurent series $\Bbb R((dx))$ (or the Levi-Civita field) is algebraically interesting, but is there any technical difference between doing this and just treating $dx$ as a real indeterminate and getting this result as Taylor's theorem? Is this in any way similar to how Newton used ""fluxions,"" historically? Or similar to Leibniz's infinitesimal approach? Is there some benefit to going with non-standard analysis, or smooth infinitesimal analysis, etc, rather than this elementary approach? EDIT : In the original question I asked about Laurent series, although I am noting that perhaps the Levi-Civita field would have been better to ask about.","['nonstandard-analysis', 'real-analysis', 'calculus', 'abstract-algebra', 'math-history']"
3202879,Covering null sets by a finite number of intervals,"Let us say that a subset $A$ of $\mathbb R$ has property $P$ if for every $\epsilon>0$ there is a finite collection of open intervals $(a_1,b_1),(a_2,b_2),\cdots,(a_n,b_n)$ such that $A \subset \cup_i (a_i,b_i)$ and $\sum (b_i-a_i) <\epsilon$ . My question: if $A$ is a nowhere dense set with measure $0$ does it have property $P$ ? Some basics: every compact set of measure $0$ (obviously)  has property $P$ .
No dense set of measure $0$ can have property $P$ . More generally, if $A$ has property $P$ then $A$ is nowhere dense. Proof: if $(\alpha,\beta) \subset \overset {-} {A}$ take $\epsilon <\beta -\alpha$ . If $A \subset \cup_i (a_i,b_i)$ and $\sum (b_i-a_i) <\epsilon$ then $(\alpha,\beta)  \subset \cup_i [a_i,b_i]$ so $\beta -\alpha <\epsilon$ , a contradiction. 
My question is if every nowhere dense set of measure $0$ has property $P$ . My guess that the implication does not hold but I don't have a counterexample.",['measure-theory']
3202934,Prove there exist a non-zero vector $v$ such that $Av=v$.,"I need to prove that if $A^t=A^{-1}$ , $\det(A)>0$ for a matrix $A_{3\times3}$ then exist a vector $v$ non-zero such that $Av=v$ . I tried taking a general matrix A, $
\left( \begin{array}{cccc}
 a_{11} & a_{12} & a_{13} \\ 
 a_{21} & a_{22} & a_{23} \\
  a_{31}& a_{32} & a_{33} 
\end{array} \right) $ but I don't get anything, and I tried using the definition of eigenvalue $\lambda$ and eigenvector $v$ , $\{v \in V \mid Av=\lambda v\}$ , and I did not get anything important, so, any hint is very valuable, thanks!","['linear-algebra', 'eigenvalues-eigenvectors']"
3202950,Intuition of divergence and curl,"There is the well known expression for the divergence of a vector field $V$ as the limit of smaller and smaller surfaces of the flux of a surface. However it occurred to me that there is another way to describe the divergence which I could not figure out how to search to read more about or to even find out if it is correct. If $V = \sum V^i e_i$ is a vector field described in the standard basis then the derivative is $$\sum_{i,j}\partial_jV^i e^j\otimes e_i$$ The trace of this is the divergence. Now from what I understand of the trace $\sum\partial_iV^i$ it is proportional to averaging the tensor over all all unit vectors $v$ and their corresponding forms $w$ in the following sense. $$\text{div}(V) = \text{tr}(\sum_{i,j}\partial_jV^i e^j\otimes e_i) \propto \int_C (\sum_{i,j}\partial_jV^i e^j\otimes e_i)(v,w)$$ where $C$ is the unit circle and $w$ is $v$ as a form. Concretely with $v=(\cos\theta,\sin\theta)$ then this becomes $$\int_0^{2\pi} \partial_1V^1\cos^2\theta + \partial_1V^2\cos\theta\sin\theta +\partial_2V^1\cos\theta\sin\theta +\partial_2V^2\sin^2\theta d\theta$$ The terms with $\sin\theta\cos\theta$ evaluate to 0. So this gives $$c(\partial_1V^1 +\partial_2V^2)$$ for a constant $c$ . The initial integral can be interpreted as saying for each direction we look at the derivative of $V$ in that direction $v$ and then project that onto $v$ (this is the role $w$ plays). Said another way this is how much the vector field changes in direction $v$ but only the part along $v$ and only the magnitude change. And then we kind of average over all directions. This is different from the usual flux over surface description in that first off it deals with the derivative of the vector field whereas the the integral of the flux does not deal with the derivative of the vector field. Is there a description like this for the curl?","['multivariable-calculus', 'tensors']"
3202967,Rudin's PMA: problem 2.17,"I'm reading Rudin's Principle of Mathematical Analysis and trying problem 17 of chapter 2. Let $E$ be the set of all $x\in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$ . Is $E$ countable? Is $E$ dense in $[0,1]$ ? Is $E$ compact? Is $E$ perfect? My argument is the following: Is $E$ countable? No. - Use Cantor's diagonal process as in the proof of theorem 2.14 of baby Rudin: Take a countable subset of $E$ , namely $A=\{a_n:n\in\mathbb{N}\}$ and construct a sequence whose $n$ -th member is $7$ if $n$ -th digit of $a_n$ is $4$ and $4$ if $n$ -th digit of $a_n$ is $7$ . Then this new sequence differs from any other member of $A$ . Thus every countable subset of $E$ is proper, it follows that $E$ is uncountable (for otherwise $E$ would be a proper subset of $E$ ). Is $E$ dense in $[0,1]$ ? No. - There's no member of $E$ between $0.1$ and $0.2$ : Because it must start with $0.1...$ and $1$ is not allowed for a member of $E$ . Is $E$ compact? No. - Since by theorem 2.34 of baby Rudin, compact subsets of metric spaces are closed. So if $E$ is not closed, then it must not be compact. I will show that $E^c$ is not open. Take $x\in E^c$ . Then we need to find a open ball $B_r(x)=\{y\in [0,1]:|x-y|<r\}$ . But for every $r>0$ , $B_r(x)$ must contain a number at least one of whose digits contains $4$ or $7$ ; otherwise there would be a gap inside the ball (a segment, actually). So $E$ is not closed (since $E$ closed $\iff$ $E^c$ open). Is $E$ perfect? No. - Direct from the definition: a perfect set is a closed set whose members are limit points. Is my argument valid?","['general-topology', 'solution-verification', 'real-analysis']"
3202976,Find sequence limit $x_{n+1} = \frac{3}{4} x_n + \frac{1}{4} \int_0^{|x_n|} f(x) dx$,"I am doing my Analysis class revision, and here is one of the problem I saw in the past exam papers: Let $f: \mathbb{R} \to [0,1]$ be continuous and $x_0 \in (0,1)$ . Define $x_n$ via recurrence: \begin{equation}
x_{n+1} = \frac{3}{4} x_n^2 + \frac{1}{4}\int_0^{|x_n|}f(x)dx
\end{equation} Prove that $x_n$ is convergent, and find its limit. I think it is not hard to show the convergence. Since $f(x) \geq 0$ , the integral is non-negative. Hence inductively $x_n \geq 0$ for all $n$ . We could drop the absolute value. Also by induction, $x_n \in (0,1)$ , then we have $x_n^2 < x_n$ , and we could conclude that $x_n$ is decreasing, because $x_{n+1} \leq \frac{3}{4} x_n + \frac{1}{4} x_n = x_n$ . A decreasing function with a lower bound (i.e, $x_n \geq 0$ ) is convergent. However, I am not sure how to find the limit. Usually, if we have a sequence defined by recurrence, we could simply make $x_{n+1} = x_n = x$ , and solve for $x$ .   However, here we have an integral there, I am wondering if we need to do some estimations and use the Sandwich Theorem.","['limits', 'sequences-and-series', 'analysis', 'real-analysis']"
3202980,Prove for all $f:\mathbb{N}\to\mathbb{N}$ exists $g:\mathbb{N}\to\mathbb{N}$ such that $g$ is immediate successor of $f$ in ${R}$.,"We define $R$ on $\mathbb{N}^\mathbb{N}$ such that: $\forall f,g:\mathbb{N}\to \mathbb{N}$ , $fRg$ if and only if $\forall n\in \mathbb{N}, f(n)\leq g(n)$ .
  Then, $R$ is partially ordered set on $\mathbb{N}^\mathbb{N}$ . Prove or Disprove: For all $f:\mathbb{N}\to\mathbb{N}$ exists $g:\mathbb{N}\to\mathbb{N}$ such that $g$ is immediate successor of $f$ in ${R}$ . I think this is correct so I`m trying to prove it, however I stuck in my proof. Here is how I tried to prove it: Let be $f:\mathbb{N}\to\mathbb{N}$ a function and we define $g:\mathbb{N}\to\mathbb{N}$ , $g(n)=f(n)$ if n $\neq$ 0. and $g(n)=f(n)+1$ if n=0. According to the definition, an immediate successor $g \in \mathbb{N}^\mathbb{N}$ of $f \in \mathbb{N}^\mathbb{N}$ is such that $f\neq g$ and $fRg$ and if $h\in \mathbb{N}^\mathbb{N}$ satisfies $fRh$ and $hRg$ , then either $h=f$ or $h=g$ . please let me know if how I start is correct but I don ` t have any idea how to continue if that is correct.","['elementary-set-theory', 'proof-writing', 'proof-verification']"
3202998,Is there any relation between an eigenvector of $A$ and the eigenvector of $A^T$ with the same eigenvalue?,"Let $A$ be a square matrix over $\mathbb C$ , and let $A^T$ denote its transpose. It is not hard to see that $A$ and $A^T$ have the same set of eigenvalues , so given $Ax=\lambda x$ for some vector $x\in V$ and eigenvalue $\lambda\in\mathbb C$ , we know that there must always be some other $y\in V$ such that also $$A^T y=\lambda y.$$ We also know that, if $Ax=\lambda x$ and $A^T y=\mu y$ with $\lambda\neq \mu$ , then $\langle y^*,x\rangle=0$ , where $y^*$ denotes the vector whose elements are complex conjugate of those of $y$ , as follows from $$\langle y^*,Ax\rangle=\lambda \langle y^*,x\rangle=\mu\langle y^*,x\rangle.$$ The same argument, however, does not provide any information for the case $\mu=\lambda$ . Is there relation holding in general for such a case? More precisely, given $Ax=\lambda x$ and $A^T y=\lambda y$ , is there any general relation between $x$ and $y$ ?","['matrices', 'transpose', 'linear-algebra', 'eigenvalues-eigenvectors']"
3203019,$f$ is a smooth map $R^n \rightarrow R^n$ such that $f\circ{f}=f$. Proving that $f(R^n)$ is a smooth surface in $R^n$,"Given that $f$ is a smooth map $R^n \rightarrow R^n$ such that $f\circ{f}=f$ , how can I prove that $f(R^n)$ is a smooth surface in $R^n$ ? So, I need to prove that each point of $f(R^n)$ has a neighborhood diffeomorphic to an open subset of $R^k$ , $k \leq n$ . All I managed to understand is that $f$ is an identity function on a subset of $R^n$ and sends the complement of that subset to that same subset. Any hints?","['multivariable-calculus', 'differential-geometry']"
3203044,What is $\sum_{k=1}^\infty \rm{sinc}^8(k)$ using the sine cardinal function?,"Given the sine cardinal function , $$\rm{sinc}(x) = \frac{\sin x}x$$ for $x\neq0$ . We have the nice evaluations, $$\sum_{k=1}^\infty \rm{sinc}(k) = \sum_{k=1}^\infty \rm{sinc}^2(k)=-\tfrac12+\tfrac12\pi$$ $$\sum_{k=1}^\infty \rm{sinc}^3(k)=-\tfrac12+\tfrac38\pi$$ $$\sum_{k=1}^\infty \rm{sinc}^4(k)=-\tfrac12+\tfrac13\pi$$ $$\sum_{k=1}^\infty \rm{sinc}^5(k)=-\tfrac12+\tfrac{115}{384}\pi$$ $$\sum_{k=1}^\infty \rm{sinc}^6(k)=-\tfrac12+\tfrac{11}{40}\pi$$ then the not-so-nice, $$\sum_{k=1}^\infty \rm{sinc}^7(k)=-\tfrac12+\quad\\ \tfrac{1}{46080}(129423\pi-201684\pi^2+144060\pi^3-54880\pi^4+11760\pi^5-1344\pi^6+64\pi^7)$$ However, I found this can be prettified as, $$\sum_{k=1}^\infty \rm{sinc}^7(k)=-\frac12+\frac{7\cdot29^2\,\pi}{2^5\,6!}+\frac{\pi\big(\pi-\tfrac72\big)^6}{6!}$$ Questions: Why is the closed-form for $n=7$ far more complicated than $n<7$ ? (And a good lesson that ""patterns"" may be illusory.) What is $n=8$ in terms of $\pi$ ? (Maybe also for $n=9$ ?) Update: Courtesy of Oliver Oloa's comment, for $n=8$ , after some tweaking is, $$\sum_{k=1}^\infty \rm{sinc}^8(k)=-\frac12+\frac{151\pi}{630}-\frac{\pi\big(\pi-\tfrac82\big)^7}{7!}$$ but $n=9$ is more complicated. See second answer below.","['trigonometry', 'pi', 'closed-form']"
3203054,Find supremum of Type II error in Neyman-Pearson framework,"Let $X_1,\dots,X_n$ be an iid sample from an $N(\theta,1)$ distribution. We want to test $H_0:\:\theta=0$ against the alternative $H_1\:\theta \neq 0$ using the test statistic $$T_n(X_1,\dots,X_n) = \bar{X}_n$$ and the corresponding test function $$\delta(X_1,\dots,X_n) = \begin{cases} 1, & \text{if } |T_n(X_1,\dots,X_n| \geq Q, \\ 0, & \mbox{otherwise}\end{cases}$$ where $Q\geq0$ . The goal is now to find the smallest value of $Q$ for which the significance level is $\alpha = 0.05$ and then to find the supremum over the paramater space of the probability of type II error for that value of $Q$ . For the Type I error, I got $$\mathbb{P}_\theta(\delta = 1) = 2\cdot\Phi(-\sqrt{n}\cdot Q), \quad \theta \in \Theta_0$$ In order to fix this error at level $\alpha = 0.05$ , we need $2\cdot\Phi(-\sqrt{n}\cdot Q) = 0.05$ , i.e. $Q=1.96/\sqrt{n}$ , since $2\cdot\Phi(1.96)=2\cdot 0.025 = 0.05$ For the Type II error, I got $$\mathbb{P}_\theta(\delta = 0) = \Phi(\sqrt{n}\cdot (Q-\theta))-\Phi(-\sqrt{n} \cdot (Q+\theta)), \quad \theta \in \Theta_1$$ I now need to find the supremum of this error over the parameter space. Intuitively, the Type II error should increase the closer $\theta$ gets to 0, since that is the $\theta$ -value of $H_0$ . This would mean that $$\text{sup}_\theta \mathbb{P}_\theta(\delta = 0) = \Phi(\sqrt{n}\cdot Q)-\Phi(-\sqrt{n} \cdot Q), \quad \theta \in \Theta_1$$ Is this correct?","['statistical-inference', 'statistics', 'normal-distribution', 'hypothesis-testing']"
3203069,What is an example of an embedding which is not proper?,"An embedded submanifold is an immersed submanifold for which the inclusion map is a topological embedding. A properly embedded submaniold is one which is embedded and the inclusion map is proper. There are many classical examples of one-to-one immersions which are not emeddings e.g. a line of irrational slope on the 2-torus. The assiciated inclusion map in this case is obviously not proper, but I am having trouble thinking of an embedded submanifold which is not properly embedded. There are examples when the ambient manifold is not Hausdorff, for instance, but can someone think of an example when both manifolds are well behaved?","['general-topology', 'differential-topology', 'differential-geometry']"
3203107,Check my proof for equality in general triangle equality,"Consider the generalized triangle equality in $\mathbb C$ : $$|z_1+\dots+z_n|\le |z_1|+\dots+|z_n|\tag 1$$ The theorem in question is the following: Equality occurs in (1) if and only if $z_k/z_l\ge 0$ for any integers $k,l$ where $1\le k,l\le n$ for which $z_l\ne 0$ . My proof is as follows: The result is easy to verify for $n=2$ . (This part is solved in the textbook). We proceed by induction and assume the result for $n-1$ . If $z_n=0$ we are done anyway so we suppose $z_n\ne 0$ . Let $z_{i_1},\dots,z_{i_k}$ be all the nonzero complex numbers amongst $z_1,\dots,z_{n-1}$ . There must be at least one such nonzero number for otherwise the proof is trivial. Further let $z^*=z_{i_1}+\dots+z_{i_k}$ . Now suppose equality prevails in (1). Then, $$|z_{i_1}|+\dots+|z_{i_k}|+|z_n|=|z^*+z_n|\le |z^*|+|z_n|$$ following which $|z^*|\ge |z_{i_1}|+\dots+|z_{i_k}|.\tag 2$ Also by the triangle inequality we have $|z^*|\le |z_{i_1}|+\dots+|z_{i_k}|.\tag 3$ From (2),(3) we see that $$|z^*|=|z_{i_1}|+\dots+|z_{i_k}|.$$ This shows that $z_k/z_l\ge 0$ for any integers $k,l$ where $1\le k,l\le n-1$ for which $z_l\ne 0$ . Note that $z^*\ne 0$ for otherwise $z_{i_1}=\dots=z_{i_k}=0$ . Fix any $j$ , $1\le j\le k$ . By the induction hypothesis we now have $c_m=z_{i_m}/z_{i_j}>0$ where $1\le m\le k$ . Since $|z^*+z_n|=|z^*|+|z_n|$ so, by the case $n=2$ of the result we have $z_n=tz^*$ for some $t>0$ . Dividing both sides by $z_{i_j}$ we see that $$z_n/z_{i_j}=t(c_1+\dots+c_{j-1}+1+c_{j+1}+\dots+c_k)>0$$ following which $z_{i_j}/z_n>0$ . This completes one direction of the proof. Now suppose that for any $1\le p,q\le k$ we have $z_{i_p}/z_{i_q},z_{i_p}/z_n>0$ . Then $\frac{z_{i_1}+\dots+z_{i_k}}{z_n}>0$ and so both $z^*/z_n,z+n/z^*>0$ . Again, by the case $n=2$ this implies that $|z_n+z^*|=|z_n|+|z^*|$ . Hence, \begin{align*}
|z_1+\dots+z_n|&=|z_{i_1}+\dots+z_{i_k}|+|z_n|\\
&=|z_{i_1}|+\dots+|z_{i_k}|+|z_n|\mbox{ (by the induction hypothesis)}\\
&=|z_1|+\dots+|z_n|
\end{align*} which completes the proof. Is this proof correct? Can it be improved?","['complex-analysis', 'inequality', 'proof-verification', 'complex-numbers']"
3203149,Is multiplying the matrix by its conjugate-transpose and divide by Frobenius norm something special for the matrix itself,"Assuming a complex matrix $A$ with $n$ x $m$ dimension. What does mean multiplying $A$ with its conjugate transpose and divide by Frobenius norm? ( $AA^H$ / $||A||_F$ ). I'm asking that question because I noticed something about that. suppose that I have $y = hA$ , where $h$ is a matrix of dimension $n$ x $m$ too. the results of $A^Hy / ||A||_F$ has always the same sign of $y$ . for that I asked Is that something special for any complex matrix $A$ ?","['matrices', 'linear-algebra', 'complex-numbers']"
3203155,"Proving $[0,2]\big/[1,2]$ is homeomorphic to $[0,1]$","Prove $[0,2]\big/[1,2]$ is homeomorphic to $[0,1]$ . Where $A\big/B$ is the quotient set where all points of $B$ are identified as a single point (and other remain distinct). Our definition of a homeomorphism $i:X\to Y$ amounts to a bijective map between topological spaces such that the topology of $X$ is exactly $$\left\{ i^{-1}(U):U\text{ open in }Y \right\}$$ and we have no other theorems to hasten the process. My problem is mostly with formality, where a lot of structure is apparently ""lost"" when moving to talk about representatives instead, and the discussion becomes problematic. My proof follows, but it feels like I'm doing something wrong for a relatively simple quotient: Define $f:\left[0,2\right]\big/\left[1,2\right]\to\left[0,1\right]$ by $$ f\left(\left[x\right]\right)=\begin{cases}
x & x\in\left[0,1\right)\\
1 & x\in\left[1,2\right]
\end{cases}$$ to see this is well defined, if $\left[x\right]=\left[y\right]$ there are two cases, either $x,y\in\left[0,1\right)$ and then $x=y$ , or $x,y\in\left[1,2\right]$ and are both mapped to $1$ . Then f is injective, as $\left[x\right]\neq\left[y\right]$ means $x\neq y$ hence if $\left[x\right],\left[y\right]\neq\left[1\right]$ , $$f\left(\left[x\right]\right)=x\neq y=f\left(\left[y\right]\right)$$ and if without loss of generality $\left[x\right]=\left[1\right]$ , then $\left[y\right]\neq\left[1\right]$ and $$f\left(\left[y\right]\right)=y\neq1\neq f\left(\left[x\right]\right).$$ It is surjective as for any $x\in\left[0,1\right], f\left(\left[x\right]\right)=x$ . Suppose $q:\left[0,2\right]\to\left[0,2\right]\big/\left[1,2\right]$ is the quotient map, then given an open interval (subbasis element) $\left(a,b\right)\subset\left[0,1\right]$ , if $b\leq1$ then $$q^{-1}\left(f^{-1}\left(\left(a,b\right)\right)\right)=\left(a,b\right)$$ and if $b>1$ , $$q^{-1}\left(f^{-1}\left(\left(a,b\right)\right)\right)=\left(a,2\right] $$ showing that $f\circ q:\left[0,2\right]\to\left[0,1\right]$ is continuous, and therefore so is f. Similarly if $U\subset\left[0,2\right]\big/\left[1,2\right]$ is open, if $\left[1\right]\notin U$ then $$f\left(U\right)=q^{-1}\left(U\right)=\left\{ x:\left[x\right]\in U\right\}$$ thus from the continuity of $q$ , $f\left(U\right)$ is open, and if $\left[1\right]\in U$ , $$f\left(U\right)=q^{-1}\left(U\setminus\left[1\right]\right)\cup\left\{ 1\right\}$$ where $q^{-1}\left(U\setminus\left[1\right]\right)$ is open, as given any point $x$ it satisfies $x<1$ , and given an open neighborhood $U_{x}$ of $\left[x\right]$ in U, if it contains $\left[1\right]$ , then $$q^{-1}\left(U_{x}\setminus\left[1\right]\right)=q^{-1}\left(U_{x}\right)\setminus\left[1,2\right]$$ which is open, showing $U\setminus\left[1\right]$ also contains such a neighborhood. We therefore only need to find a neighborhood for $1\in\left[0,1\right]$ , but since $U$ contains a neighborhood for $\left[1\right]$ , $q^{-1}\left(U\right)$ contains a neighborhood for $\left[1,2\right]$ , which must contain an interval of the form $\left(a,2\right]$ for $a<1$ , hence $\left(a,1\right]\in f\left(U\right)$ showing that $f\left(U\right)$ is indeed open and $f$ is a homeomorphism.","['general-topology', 'proof-verification']"
3203162,Separating closed sets in Moore plane / Niemytzki plane (Topology),"I spent the last few days trying to solve this exercise with little success, so I really hope someone here might be able to assist: Denote Moore plane by $M$ , the $x$ -axis by $R$ and the upper
  half-plane by $H$ ( $M = R \cup H$ ). Let $A, B \subseteq M$ be closed
  and disjoint subsets of $M$ . Suppose $|A \cap R| < \infty$ . Prove that $A$ and $B$ can be separated by disjoint open neighborhoods. I am aware of the fact that $M$ is not $T_4$ , and in the previous exercises I proved the following facts: $R$ as a subspace of $M$ has the discrete topology. $H$ as a subspace of $M$ is homeomorphic to $H$ as a subspace of $\mathbb{R}^2$ (with the standard topology). $H$ is open in $M$ . The closure in $M$ of each element of the topology's basis is the same as its closure in the Euclidean plane. $M$ is a regular Hausdorff space ( $T_3$ ). I also showed that it suffices to prove the claim for the case where $A \cap R = \varnothing$ and $B = R$ (although other approaches might also work), but I'm having trouble showing that there exists an open neighborhood of $A$ whose closure does not intersect with $R$ . Your help will be much appreciated!","['general-topology', 'moore-plane', 'separation-axioms']"
3203182,A compactly supported continuous function on an open subset of $\mathbb R^n$ is Riemann integrable. What is the relevance of openness in the proof?,"My book is An Introduction to Manifolds by Loring W. Tu. A proposition in Subsection 23.2 ( Proposition 23.4 ) is If a continuous function $f: U \to \mathbb R$ defined on an open subset $U$ of $\mathbb R^n$ has compact support, then $f$ is Riemann integrable on $U$ . What is the relevance of openness of $U$ , specifically the proof given? One might argue it's about smooth extensions of $f$ like in Proposition 13.2 for $M = \mathbb R^n$ (which relies on openness of $U$ specifically through Exercise 13.1 although the openness of $U$ is not explicitly used based on the solution provided , but assuming I understand right, I think I know where the openness of $U$ ). But I think the $\tilde{f}$ is not the one in Proposition 13.2 but rather the one in Subsection 23.1 (see (1) below) , which does not make use of any bumps and has no claims of smoothness or continuity, though the $\tilde{f}$ in Proposition 23.4 is of course proven continuous on $\mathbb R^n$ assuming $f$ is continuous on $U$ , open in $\mathbb R^n$ in the same way that the $\tilde{f}$ in Proposition 13.2 is proven smooth on $M$ assuming $f$ is smooth on $U$ , open in $M$ . Part of why I think so is that it says in the proof that $\tilde{f}$ agrees with $f$ on $U$ , but that's not even what the $\tilde{f}$ of Proposition 13.2 does: $\tilde{f}$ agrees with $f$ on an equal or a smaller neighborhood: This is not only explicit in Proposition 13.2 but also emphasized in a remark just before Proposition 13.2 . Additionally, I think $\tilde{f}$ is meant to be an extension not of $f$ but rather of $f|_{\text{supp} f}$ . I ask about this here . Perhaps this should be resolved first. One thing I have in mind about how $U$ 's openness is relevant is some kind of rule like $f$ Riemann integrable on $U$ if $f$ is Riemann integrable on $\text{supp} f (\subseteq U)$ . Another thing I had in mind is something like $\overline \int_U f = \overline \int_U f 1_{\text{supp} f} = \overline \int_{\text{supp} f} f$ and $\underline \int_U f = \underline \int_U f 1_{\text{supp} f} = \underline \int_{\text{supp} f} f$ by the overline and underline versions of Remark 10.7(i) in another book From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave because $U$ is a manifold, because $U$ is open and because $\mathbb R^n$ is manifold...but I think this is overthinking not because this is from another book but because this uses geometry for an elementary analysis proposition. Update: I think I know it now: My mistake in Method 1 is deducing $\tilde{g}$ continuous at $x \in \text{supp} f$ simply because $g$ is continuous at $x$ . This is not necessarily true, but $\tilde{f}$ continuous at $x \in U$ from $f$ 's continuity at $x$ BECAUSE $U$ is open. However, we must also prove $\tilde{f}$ and $\tilde{g}$ are identical. Much like with $T_eG$ and $L(G)$ in Lie algebras , each hand washes the other: $\tilde{g}$ is an extension from a bounded but closed set and therefore while we can use Lebesgue, we can't deduce continuity. $\tilde{f}$ is an extension from an open set but possibly unbounded set and therefore while we can't use Lebesgue, we can deduce continuity. Therefore, the proof of Proposition 23.4 is as follows: For continuous $f: U \to \mathbb R$ with $U$ open in $\mathbb R^n$ and with compact support, there exists a extension of $f$ by zero $\tilde{f}: \mathbb R^n \to \mathbb R$ , i.e. $\tilde{f}(x) = f(x) \cdot 1_U(x) + 0 \cdot 1_{U^c}(x)$ which is a continuous extension because $f$ is continuous AND because $U$ is open (and compact support is not used here, I think). Lebesgue's theorem does not directly apply here because $U$ is not given to be bounded. For $g=f|_{\text{supp} f}$ , the restriction of $f$ to its support, we have $\tilde{g}: \mathbb R^n \to \mathbb R$ , $\tilde{g}(x) = g(x) \cdot 1_{\text{supp} f}(x) + 0 \cdot 1_{(\text{supp} f)^c}(x))$ , the extension of $g$ by zero. Underemphasized in my opinion : Observe $\tilde{g}$ is not only an extension from a bounded set but also identical to $\tilde{f}$ . In Lebesgue's theorem choose the bounded set $A = \text{supp} f$ , and the bounded function to be $g$ which we can do because $\text{supp} f$ is bounded because $f$ has compact support and say that $\tilde{g}$ is continuous, not because $\tilde{g}$ is an extension of a continuous function, namely $g$ , but because $\tilde{g}$ is identical to a continuous function, namely $\tilde{f}$ . Oh, there's something missing here: We have shown $g=f|_{\text{supp} f}$ is Riemann integrable. How exactly do we get that the original $f$ is Riemann integrable? Intuitively, I guess this has something to do with $(\text{supp} f)^c \subseteq \{f=0\}$ . At this point, I think geometry's/topology's role is done, and analysis has to take over. I guess $$\int_U f = \int_{\text{supp} f} f + \int_{U \cap (\text{supp} f)^c} f = \int_{\text{supp} f} f + 0 = \int_{\text{supp} f} f = \int_{U} f|_{\text{supp} f},$$ but that relies on $\int_U f$ being well-defined in the first place. Hence I suspect some equivalent definition of Riemann integrability or at least some property of Riemann integrability is that $f$ is Riemann integrable if $f_{\text{supp} f}$ is Riemann integrable. (1) Actually, the solution to Exercise 13.1 makes use of something called ""extended by zero"", but I don't think that's defined until Section 23 .","['integration', 'measure-theory', 'real-analysis', 'calculus', 'differential-geometry']"
3203200,Graph $G$ with Degree $\ge 2$ must contain a cycle,"Show that a finite graph with all vertices with degree $\geq 2$ has a cycle that contains a vertex which is non-adjacent to any other vertices not contained in the cycle. I know how to prove that the graph has at least a cycle: just start somewhere and follow the path. If you ever have an option to go back to a vertex you have visited, you have found a cycle. You can never get stuck as each vertex you come to has an exit. Eventually you will run out of vertices. However, how would i find this special cycle? i tried contradiction, suppose the cycle does not have any such vertex. Then all vertices in the cycle must connect to at least one other vertex not in the cycle. how would i find a contradiction?","['graph-theory', 'cyclic-groups', 'path-connected', 'discrete-mathematics']"
3203226,What type of function can I apply to both sides of an inequality in order to obtain an equivalent inequality?,"I'd like to know what are the types of functions that I can apply to both sides of an inequality in order to get an equivalent inequality. Are those functions the ones that are strictly increasing or strictly decreasing? I'm not sure about it.
Also, I'd like to get a recommendation about good sources where I can read about it.
Thanks.","['algebra-precalculus', 'functions', 'inequality']"
3203272,Understanding the formular $1/8(tr(A^4_G)-2|E|-4\sum_{v\in V} \binom{deg_G(v)}{2} )$ for the number of cycles of length 4 in G,"So here is the formular I am trying to understand: Let G = (V,E) be a graph, with an adjacency matrix $A_G$ . The number of cycles with a length of 4 in G is the result of the following calculation: $$\frac{1}{8}\left(\operatorname{tr}(A^4_G)-2|E| -4\sum_{v\in V}  \binom{\deg_G(v)}{2}\right)$$ What I do understand so far, is that the i'th entry of the trace of the adjacency matrix $A^4_G$ shows the number of walks of length 4 that start in $v_i$ and end in $v_i$ . Since these aren't necessarily cycles, I have to subtract all walks of length 4, that aren't cycles. I therefore subtract two times the number of edges in G, i.e. (-2|E|), since each edge is connected to two vertices and each $v_i$ has a walk of length 4 that ends in $v_i$ through moving back and forth on the same edge. The walks of length 4 that I still need to subtract, are probably the ones of the form $v_i->v_j->v_i->v_x->v_i$ , as they aren't cycles either. This is probably what $-4\displaystyle\sum_{v\in V}  \binom{\deg_G(v)}{2}$ is for, but I can't seem to understand the logic behind it. Why does this formular succeed to sum up the number of walks of the form $v_i->v_j->v_i->v_x->v_i$ in the given graph? Also, why do I have to divide the end result by 8?","['graph-theory', 'binomial-coefficients', 'discrete-mathematics']"
3203288,Composition of Taylor Series,"Suppose I have smooth functions $f,g,y_0$ and $y_1$ from $\mathbb{R}$ to $\mathbb{R}$ , such that $$y_1(x) = y_0(x) - \epsilon g(y_0(x))$$ Then I consider $$f(y_0(x)) = f(y_1(x) + \epsilon g(y_0(x)))$$ Is there a closed form expression for the Taylor series in the small parameter $\epsilon$ in terms of derivatives of $f$ and $g$ and only the function $y_1$ ? The first few terms are $$f(y_0) = f(y_1) + \epsilon f'(y_1) g(y_0) +  \frac{1}{2}\epsilon^2 f''(y_1)g^2(y_0) +..$$ Where we interpret $f(y_0)$ as $f|_{y_0(x)}$ and treat $x$ fixed. Then we can again replace the $y_0$ in $g(y_0)$ with $$g(y_0) = g(y_1)+ \epsilon g'(y_1)g(y_0) +...$$ giving $$= f(y_1) + \epsilon f'(y_1) [g(y_1) + \epsilon g'(y_1)g(y_0) + ... ]$$ $$+  \frac{1}{2}\epsilon^2 f''(y_1)[g(y_1) + \epsilon g'(y_1)g(y_0) + ... ]^2 +...$$ Continuing to replace the $y_0$ with $g(y_0)$ like this and grouping terms gives $$f(y_0) = f(y_1)+ \epsilon [f'g](y_1) + \epsilon^2[f'g'g + \frac{1}{2}f''g^2](y_1) + \epsilon^3[f'g'^2g + \frac{1}{2}f'g''g + \frac{1}{2}f''g'g + \frac{1}{6}f'''g^3](y_1) + O(\epsilon^4)$$ But is there some way to write this as a more compact sum like $$f(y_0) \sim f(y_1) + g(y_1)\sum_{n=1}^\infty\sum_{m=0}^n \epsilon^n \alpha(n,m)f^{(n)}g^{(n-m)}(y_1)$$ I am having trouble identifying the pattern. I know there will be some product involved as well. Edit: Thinking about it some more it may suffice to just set $f=id$ and consider $$y_0 = y_1 + \epsilon g(y_0)$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_0))$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_0)))$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_1 + ...)))$$ and somehow use the chain rule $$[f_1\circ f_2 \circ .... \circ f_n]' = \prod_{i=1}^n(f'_{i}\circ f_{i+1}\circ ...\circ f_n)$$","['real-analysis', 'calculus', 'taylor-expansion', 'sequences-and-series', 'infinite-product']"
3203313,Orthogonal bases of the vector space $\mathbb{Z}_2^4$,"Let $\mathbb{Z}_2$ be the two element field $\mathbb{Z}/2\mathbb{Z}$ . The vectors $e_0 = \langle1,1,1,1\rangle$ , $e_1=\langle1,1,0,0\rangle$ , $e_2 = \langle1,0,0,1\rangle$ , $e_3 = \langle1,0,1,0\rangle$ in the $\mathbb{Z}_2$ -vector space $\mathbb{Z}_2^4$ don't form a basis of $\mathbb{Z}_2^4$ . Compare this to $\langle1,0,0,0\rangle$ , $\langle 0,1,0,0\rangle$ , $\langle 0,0,1,0\rangle$ , $\langle 0,0,0,1\rangle$ which do form a orthonormal basis of $\mathbb{Z}_2^4$ and to $\langle 1,1,1,1\rangle$ , $\langle 1,1,-1,-1\rangle$ , $\langle 1,-1,-1,1\rangle$ , $\langle 1,-1,1,-1\rangle$ which form a orthogonal basis of the $\mathbb{R}$ -vector space $\mathbb{R}^4$ (the Hadamard-Walsh basis). What does this mean? Does it mean, that there is essentially only one basis of $\mathbb{Z}_2^4$ ? If not so: What are the orthogonal bases of $\mathbb{Z}_2^4$ ? What are the orthogonal bases of $\mathbb{Z}_2^8$ ? (Complete lists would be welcome.)","['orthogonality', 'change-of-basis', 'linear-algebra', 'orthonormal']"
3203319,Solving $3x^2 - 4x -2 = 0$ by completing the square,"I can't understand the solution from the textbook (Stroud & Booth's ""Engineering Mathematics"" on a problem that involves solving a quadratic equation by completing the square. The equation is this: $$
\begin{align}
3x^2 - 4x -2 = 0 \\
3x^2 - 4x = 2
\end{align}
$$ Now, divide both sides by three: $$x^2 - \frac{4}{3}x = \frac{2}{3}$$ Next, the authors add to both sides the square of the coefficient of $x$ , completing the square on the LHS: $$x^2 - \frac{4}{3}x + \left(\frac{2}{3}\right)^2 = \frac{2}{3} + \left(\frac{2}{3}\right)^2$$ Now, the next two steps (especially the second step) baffle me. I understand the right-hand side of the first quation (how they get the value of $\frac{10}{9}$ ), but the last step is a complete mystery to me: $$
\begin{align}
x^2 - \frac{4}{3}x + \frac{4}{9} = \frac{10}{9} \\
\left(x - \frac{2}{3}\right)^2 = \frac{10}{9}
\end{align}
$$ Can anyone please explain how they went from the first step to the second step?","['self-learning', 'algebra-precalculus', 'quadratics']"
3203453,What is the description of the (local) sections of the associated vector bundle of a principal bundle?,"I was reading the following lecture notes: Lecture on Connections (p. 1-2). I am going to paraphrase it a bit, but it said the following Let $\pi \colon P \to M$ be a principal $G$ -bundle and $V$ be a representation of $G$ . The associated vector bundle is the vector bundle is given by $E(V) := (P \times V)/G$ . From here the author argued that this construction is abstract, but that the (local) sections have concrete descriptions. They are given by the maps $s: P \to V$ for which $$
s(pg) = g \cdot s(p)
$$ for all $g \in G$ and $p \in P$ . I have two questions: How is $E(V) = (P \times V)/G$ exactly defined? Is it that $$(p, v) \sim (g p, g^{-1} \cdot v) \quad \text{or} \quad (p,v) \sim (gp, g \cdot v)?$$ According to Mathworld , it should be the former, but why do we choose to define it that way exactly? How do the maps $s: P \to V$ give information on the sections of $E(V)$ ? Shouldn't sections of $E(V)$ be of the form $$ \hat s: M \to E(V)? $$ Thanks in advance for all the help!","['principal-bundles', 'connections', 'vector-bundles', 'differential-geometry']"
3203455,Find $P(n+1)$ for a polynomial P,"Fix a nonnegative integer $n$ . Let $P(x)$ be a polynomial of degree $n$ (over the real numbers) such that for all $k\in\left\{0,1,\ldots,n\right\}$ , we have $P(k)=\dfrac{k}{k+1}$ . Find $P(n+1)$ . My attempt: I consider a new polynomial $Q(x)=(x+1)P(x)-x$ . This $Q$ is a polynomial of degree $\leq n+1$ satisfying: $Q(k)=0\quad\forall k\in\{0,1,2,\ldots,n\}$ . Therefore $Q(x)=\lambda x(x-1)\cdots(x-n)$ for a certain real $\lambda$ . Hence, $Q(-1)=\lambda \cdot (-1)^{n+1}(n+1)!$ , but also $Q(-1)=(-1+1)P(-1)+1=1$ . Comparing these, we get $\lambda \cdot (-1)^{n+1} (n+1)! = 1$ , thus $\lambda=\dfrac{(-1)^{n+1}}{(n+1)!}$ . Since $Q(n+1)=\lambda(n+1)!$ then $Q(n+1)=(-1)^{n+1} $ , and from $P(x) = \dfrac{Q(x)+x}{x+1}$ we deduce: $$P(n+1)=\dfrac{(-1)^{n+1}+n+1}{n+2}.$$ I doubted that the answer were $P(n+1)=\frac{n+1}{n+2}$ but the term $(-1)^{n+1}$ wasn’t expected. Is there any mistake here? Does someone have an alternative proof?","['alternative-proof', 'calculus', 'polynomials']"
