question_id,title,body,tags
2853333,Metrizability of a subset in the weak topology.,"Let $X$ be a Banach space (not reflexive). It is well-known that $(X,w)$, which is $X$ with its weak topology, is not metrizable if $X$ is infinite dimensional. I want to know under which condition can a subset $S\subset X$ be given a metric that is compatible with the weak topology $(S,w)$? If $X$ is reflexive then, if I recall correctly, the norm-boundedness of $S$ is enough. However, I am dealing with a non-reflexive space $X=W^{1,1}(\Omega)$ so the previous criterion is not applicable. What if I assume that $S$ is norm-compact? Would that be enough?","['functional-analysis', 'banach-spaces', 'sobolev-spaces', 'partial-differential-equations']"
2853365,When to use the continuity correction for normal approximations of binomial probabilities.,"so I'm confused as to when you actually use continuity correction. If a problem deals with a binomial distribution and we are asked to find probabilities using normal approximation (provided np>5 and n(1-p)>5), I know we use continuity correction, but do we use continuity correction even for sampling distribution of sample means for said binomial distribution? In the problem below, in part b, do we use continuity correction? In general, do we use continuity correction if we know the identity of population distribution (namely if it's binomial)? What if we didn't know the problem below was a binomial distribution, how would we know we have to use continuity correction? In the next image, do we use continuity correction for part b. Based on the central limit theorem, I know if n>30 then the sampling distribution of sample means is approximately normal, but all this time I hadn't worried about continuity correction.","['statistics', 'probability', 'central-limit-theorem']"
2853400,Powers of trigonometric functions,"Are formulas known for non-integer powers of trigonometric functions, analgolous to the power reduction identities?  Like if you had a root of sine, could you express it in as a finite summation of sine and cosine functions with different inputs?",['trigonometry']
2853402,"How to prove that if $f:\mathbb{R} \to \mathbb{R}^2$ is of class $C^1$, then $f$ is not onto.","In the book of Analysis on Manifolds by Munkres, at page 160, question 1.b, it is asked that If $f:\mathbb{R} \to \mathbb{R}^2$ is of class $C^1$ , show that $f$ does not carry $\mathbb{R}$ onto $\mathbb{R}^2$ . In fact show that $f(\mathbb{R})$ contains no open subset of $R^2$ . I have started with assuming that $f(\mathbb{R})$ contains an open set $U$ of $\mathbb{R}^2$ , and by the continuity of $f$ , I have argued that $f^{-1}(U)$ is open in $\mathbb{R}$ ; however, after this point, I am stuck, so I would appreciate any hint or help.","['general-topology', 'real-analysis', 'metric-spaces', 'derivatives']"
2853404,Elementary geometry: perimeter of a polygon?,"Let $\mathscr{C}$ be a circle of centre $O$, $\mathscr{P}:= [a_1, \ldots, a_n]$ and $\mathscr{Q}:= [b_1, \ldots, b_m]$ two convex polygons, inscribed in $\mathscr{C}$ such that: $O$ is inside $\mathscr{P}$ and $\mathscr{Q}$. The shortest side of $\mathscr{P}$ is bigger than the longest side of $\mathscr{Q}$. Then the perimeter de $\mathscr{P}$ is smaller than the perimeter de $\mathscr{Q}$. I have a proof using trigonometry and a convex (or concave) inequality.
However I would be interested in a proof at middle school level, not using the notion of function. Any idea?","['polygons', 'geometry']"
2853432,The union of two subsets of $\mathbb R^3$.,"I have the following subsets of $\mathbb R^3$:
$$A=\left\{(x,y,z): \ z_1\leq z\leq z_2\, , \ x<\bar{x}\, , \ y_1\leq y\leq y_2\right\}$$
$$B=\left\{(x,y,z): \ z_1\leq z\leq z_2\, , \ x>\bar{x}\, , \ y_3\leq y\leq y_2\right\}$$
where and $\bar x$, $y_1$, $y_2$, $y_3$, $z_1$, $z_2$ are fixed numbers and $y_1\leq y_3$. What is the union of the two sets? I thought:
$$A\cup B=\{z_1\leq z\leq z_2\, , \ x\neq\bar{x}\, , \ y_1\leq y\leq y_2\}\, .$$","['algebra-precalculus', 'elementary-set-theory']"
2853450,Methods for calculating the number of zeros of a polynomial with a specified real part?,"Given a polynomial with real coefficients is there a method (e.g. from algebra or complex analysis) to calculate the number of complex zeros with a specified real part? Background. This question is motivated by my tests related to this problem . Let $p>3$ be a prime number. Let $G_p(x)=(x+1)^p-x^p-1$, and let
$$F_p(x)=\frac{(x+1)^p-x^p-1}{px(x+1)(x^2+x+1)^{n_p}}$$
where the exponent $n_p$ is equal to $1$ (resp. $2$) when $p\equiv-1\pmod 6$ (resp. $p\equiv1\pmod 6$). The answer by Lord Shark the Unknown (loc. linked) implies that $F_p(x)$ is a monic polynomial with integer coefficients. The degree of $F_p$ is equal to $6\lfloor(p-3)/6\rfloor$. I can show that the complex zeros of $F_p(x)$ come in groups of six. Each of the form $\alpha,-\alpha-1,1/\alpha,-1/(\alpha+1),-\alpha/(\alpha+1),-(\alpha+1)/\alpha.$ That is, orbits of a familiar group (isomorphic to $S_3$) of fractional linear transformations. My conjecture. Exactly one third of the zeros of $F_p(x)$ have real part equal to $-1/2$. I tested this with Mathematica for a few of the smallest primes and it seems to hold. Also, each sextet of zeros of the above form seems to be stable under complex conjugation, and seems to contain a complex conjugate pair of numbers with real part $=-1/2$. 
Anyway, I am curious about the number of zeros $z=s+it$ of the polynomial $F_p(x)$ on the line $s=-1/2$. Summary and thoughts. Any general method or formula is welcome, but I will be extra grateful if you want to test a method on the polynomial $G_p(x)$ or $F_p(x)$ :-) My first idea was to try the following: Given a polynomial $P(x)=\prod_i(x-z_i)$ is there a way of getting $R(x):=\prod_i(x-z_i-\overline{z_i})$? If this can be done, then we get the answer by calculating the multiplicity of $-1$ as a zero of $R(x)$. May be a method for calculating the number of real zeros can be used with suitable substitution that maps the real axes to the line $s=-1/2$ (need to check on this)? Of course, if you can prove that $F_p(x)$ is irreducible it is better that you post the answer to the linked question. The previous bounty expired, but that can be fixed.","['complex-analysis', 'polynomials']"
2853489,Tricky Integral involving radicals,"I am trying to evaluate the following definite integral (for $a>0$): $$I=\int_{0}^{1}{{{\left( {{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x \right)}^{2}}dx}$$
Neither the substitution $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}$ nor $u={{\left( 1-{{x}^{a}} \right)}^{\frac{1}{a}}}-x$ are appropriate.I have also tried Feynman’s trick (differentiated with respect to a) but I didn’t get any success. Thanks in advance.","['radicals', 'integration', 'calculus']"
2853502,Prove that polynomial of degree $4$ with real roots cannot have $\pm 1$ as coefficients (IITJEE),"So I was going through  my 11th class package on Quadratic equations and I saw a question to prove that a polynomial of $4$th degree with all real roots cannot have $\pm 1$ as all its coefficients. I tried proving it using calculus, by showing that at least one consecutive maxima  and minima will lie either above or below the x axis, but couldn't solve it using that. I also tried using Descartes Rule of Signs but couldn't solve it with that too.
Any help?","['algebra-precalculus', 'polynomials', 'calculus', 'quadratics']"
2853557,Ways to find $\min[f(x)]$ where $f(x) = (x-1)(x-2)(x-3)(x-4)$ without using derivatives,"As title states I need to: Find $\min[f(x)]$ where $f(x) = (x-1)(x-2)(x-3)(x-4)$ without using derivatives Since I i'm restricted to not use the derivatives I've started to play with the function in different ways. After some experiments I've noticed the following: Let
$$
g(x) = (x-1)(x-3)
$$ and $$
h(x) = (x-2)(x-4)
$$ Then i tried to find vertexes with $x_v = -{b \over 2a}$ and calculate values of $g(x)$ and $h(x)$ in $x_v$ points and they appear to be the minimum values for $f(x)$. I've also checked this for $p(x) = (x-1)(x-2)(x-5)(x-6)$ and a lot of other similar polynomials. All of them are symmetric with respect to some $x$. Based on the above the $\min[f(x)] = -1$ and $\min[g(x)] = -4$ but I'm not sure why this worked. Could someone explain me what happened? I would 
also appreciate if anyone could tell whether there exists a general way of finding minimum for even power polynomials of the following kind: $$
\prod_{k=1}^{2n}(x-k)
$$","['algebra-precalculus', 'polynomials', 'maxima-minima']"
2853560,Symmetries of a curve on the unit sphere,"Let $\mathbb{S}^2 \subseteq \mathbb{R}^3$ be the unit sphere. Let $\gamma : [0, l] \to \mathbb{S}^2$ be a smooth embedded closed curve. Let $\vec{\nu}$ be one of the two continuous unit normal vector field on $\gamma$ (with a little abuse, I am denoting with $\gamma$ also the image of the curve). Just to be clear, $\vec{\nu}$ is tangent to the sphere. Let $f: \gamma \to \mathbb{R}$ be  a smooth, given function, with only a finite number of zeroes. Let assume also that the following property holds:  for every $K \in \mathcal{Kill}(\mathbb{S}^2)$
\begin{equation}\label{asd}
\int_\gamma f \langle \nu, K\rangle \, d\sigma = 0.
\end{equation} With $\mathcal{Kill}(\mathbb{S}^2)$ I mean the Lie algebra of Killing vector fields on the sphere. Question : what can I say about $\gamma$ and/or $f$? For instance, it should be true that if $\gamma$ is a circle and $f$ is constant, then $\int_\gamma f \langle \nu, K\rangle \, d\sigma = 0$ holds for every $K \in \mathcal{Kill}(\mathbb{S}^2)$. 
Is it true the converse? I feel that the converse might not be true, but maybe it is possible to prove some weaker kind of symmetry for $\gamma$ and/or $f$?","['curves', 'symmetry', 'riemannian-geometry', 'differential-geometry']"
2853588,Why is the Petersen graph no Cayley graph?,"On the Wikipedia page of the Petersen graph it is mentioned that it is not a Cayley graph. How it this proved? Honestly I don't even know how to start this. The only criteria I can think of is that all vertices must have the same degree. Also the degree being odd should imply that one generator of the group has order 2. But then how do I proceed? Edit: As already mentioned in the comments, the link did not answer my question.","['graph-theory', 'abstract-algebra', 'geometric-group-theory', 'cayley-graphs', 'group-theory']"
2853605,"Is there a difference between preference order and preference relation and if yes, what?",While reading about utility theory (in context of Game theory) I came across two terms: preference order and preference relation. I am not clear about the distinction between them. Reference Link: http://www.cs.cornell.edu/courses/cs5846/2017sp/2%20utility.pdf,"['functions', 'game-theory', 'elementary-set-theory', 'order-theory', 'algorithmic-game-theory']"
2853626,"Is it true that $\sum_{S \subset N \setminus \{i\}} {|S|!(n-|S|-1)!}=n!$, where $N=\{1, 2, \dots, n\}$ and $i \in N$?","Page 227 of An Introductory Course on Mathematical Game Theory by Gonzalez-Diaz, Garcia-Jurado, and Fiestras-Janeiro gives a formula for the Shapley value $\Phi$ of transferable utility games $v\in G^N$ as: $$\Phi_i(v):=\sum_{S\subset N \setminus \{i\}}\frac{|S|!(n-|S|-1)!}{n!}(v(S \cup \{i\})-v(S)).$$ It goes on to state that ""therefore, in the Shapley value, each player gets a weighted average of the contributions he makes to the different coalitions."" Saying that this is a weighted average of the values $(v(S \cup \{i\})-v(S))$ implies that $\sum_{S \subset N \setminus \{i\}} {|S|!(n-|S|-1)!}=n!$, but I'm curious as to why this is true. Can someone please give a detailed proof that $\sum_{S \subset N \setminus \{i\}} {|S|!(n-|S|-1)!}=n!$ ? Thank you.","['game-theory', 'combinatorics', 'factorial', 'summation']"
2853639,"A continuous, nowhere differentiable but invertible function?","I am aware of a few example of continuous, nowhere differentiable functions. The most famous is perhaps the Weierstrass functions $$W(t)=\sum_k^{\infty} a^k\cos\left(b^k t\right)$$ but there are other examples, like the van der Waerden functions, or the Faber functions. Most of these ""look like"" some variation of: (Weierstrass functions from Wolfram) Specifically, they are clearly not invertible. Since these functions are generally self-similar at many scales, this non-invertability would seem to hold essentially everywhere. I'm wondering if it's possible to construct such a function which is invertible . Intuitively, maybe this would be ""jittery"" in the same way as the Weierstrass function, but if it had a slope which always increased, it would be invertible. Or perhaps there is at least an example in which the function is invertible over some segment of the range.","['continuity', 'differential-topology', 'analysis']"
2853645,Random Variable vs data vs random sample,"I have a problem understanding the difference between random variable and random sample. I have read this thread , but still it is unclear. According to wiki random variable is a function that from a set of outcomes (events) to measurable values $\Omega$ -> $E$ where $E$ could be $\mathbb{R}$ . And random sample is a possible outcome. So, in a lecture script I have these sentences which are confusing me: let $D=\{x_1,x_2,x_3...x_n\}$ be a set of random variables. Now I want to create a concrete example for myself to understand what $\{x_1,x_2,...,x_n\}$ could be. Let's take the sample of two dice where we want to compute the probability of the sum of the figures of dice we have thrown. So we construct a random variable $X$ which just adds the thrown number of the dice: $$X:\Omega\to\mathbb{N}$$ $$X(\text{die}_1,\text{die}_2) = \text{die}_1 + \text{die}_2 $$ Where dice are uniformly distributed in $ \{ 1,\cdots,6 \} $ . And as far I know, if I throw two diсe, I have concrete values which are my random samples. Now my concrete question is: what could be in this specific example a set of random variables $D=\{x_1,x_2,x_3...x_n\}$ ? And what are then the concrete random samples? UPDATE: After reading the comments and replies, I want to underline, that I have a specific question and I would appreciate if someone could answer it explicitly:
In the above example please give me a concrete $D = {x_1,x_2,...,x_n}$ as a set of random variables. I want to see how a set of random variables $D = {x_1,x_2,...,x_n}$ (more than 2 random variables) would look like in this example.","['probability', 'random-variables']"
2853669,"Prove there is no $x, y \in \mathbb Z^+ \text{ satisfying } \frac{x}{y} +\frac{y+1}{x}=4$","Prove that there is no $x, y \in \mathbb Z^+$ satisfying
$$\frac{x}{y} +\frac{y+1}{x}=4$$ 
I solved it as follows but I seek better or quicker way: $\text{ Assume }x, y \in \mathbb Z^+\\  1+\frac{y+1}{y}+\frac{x}{y} +\frac{y+1}{x}=1+\frac{y+1}{y}+4 \\
\Rightarrow \left(1+\frac{x}{y}\right)\left(1+\frac{y+1}{x}\right)=6+\frac{1}{y}\\
\Rightarrow (x+y)(x+y+1)=x(6y+1)\\
\Rightarrow x\mid (x+y) \;\text{ or }\; x\mid (x+y+1)\\
\Rightarrow x\mid y \;\text{ or }\; x\mid (y+1)\\
\text{Put}\; y=nx ,n \in \mathbb Z^+\;\Rightarrow\; \frac{x}{nx} +\frac{nx+1}{x}=4 \;\Rightarrow\; \frac{1}{n} +\frac{1}{x}=4-n \;\rightarrow\;(1)\\
\text{But}\; \frac{1}{n} +\frac{1}{x} \gt 0 \;\Rightarrow\; 4-n \gt 0 \;\Rightarrow\; n \lt 4\\
\text{Also}\; \frac{1}{n},\frac{1}{x}\le 1 \;\Rightarrow\; \frac{1}{n} +\frac{1}{x} \le 2 \;\Rightarrow\; 4-n \le 2 \;\Rightarrow\; n \ge 2\\
\;\Rightarrow\; n=2 \;\text{ or }\; 3, \;\text{substituting in eq. (1), we find no integral values for } x.\\ \text{The same for the other case.}\\
$ So is there any other better or intelligent way to get this result?","['number-theory', 'vieta-jumping']"
2853673,Proof of this integration shortcut: $\int_a^b \frac{dx}{\sqrt{(x-a)(b-x)}}=\pi$,"I came across this as one of the shortcuts in my textbook without any proof. When  $b\gt a$, $$\int\limits_a^b \dfrac{dx}{\sqrt{(x-a)(b-x)}}=\pi$$ My attempt : I notice that the the denominator is $0$ at both the bounds. I thought of substituting $x=a+(b-a)t$ so that the integral becomes
$$\int\limits_0^1 \dfrac{dt}{\sqrt{t(1-t)}}$$ This doesn't look simple, but I'm wondering if the answer can be seen using symmetry/geometry ?","['integration', 'definite-integrals', 'calculus']"
2853712,Confusion about the splitting field of $x^3-7$,"I'm asked to find the degree of the splitting field of $x^3-7$ over the rationals. The roots are $\sqrt[3] 7e^{\frac{2\pi ik}{3}},\ k=0,1,2$. Explicitly, $$x_1=\sqrt[3] 7,\\ x_2=\sqrt[3] 7 \bigg(-\frac{1}{2}+i\frac{\sqrt 3}{2}\bigg),\\ x_3=\sqrt[3] 7 \bigg(-\frac{1}{2}-i\frac{\sqrt 3}{2}\bigg).$$ The splitting field is an extension that contains all roots. I see at least two essentially different possibilities of building it. First, I could adjoin $\sqrt[3] 7, i,\sqrt 3$. Such an extension will contain all roots. On the other hand, I could adjoin $\sqrt[3] 7, i\sqrt 3$. This extension also contains all the roots. I believe the two extensions have different degrees. So how should I understand which degree I should find to begin with? Which extension is the splitting field?","['galois-theory', 'splitting-field', 'abstract-algebra', 'extension-field', 'field-theory']"
2853721,Submmersions: $|f(x)|$ do not assume maximal value,"I am studying analysis and I have had a lot of uncertainties. For instance, I cannot solve this exercise: If $f:U\rightarrow\mathbb{R}^3$ has class $C^1$ and rank $3$ in all of the points of the open $U\in\mathbb{R^4}$ , show that $|f(x)|$ do not assume maximal value for $x\in U$ . (I guess this is the comand, but I'm so sorry if I did mistakes. My language and the language of the comand is Portuguese) Well. I know that $f$ is a submersion. So, it's an open map. From here can I get the required? If I know that $f$ is an open map, have I that $|f(x)|$ is an open set and so that it has not a maximum? Edit - September, 25 I was taking another look at this question I decided try a formal proof in here: Once the rank of $f$ is maximal (the dominium dimension) and the contradominium has a lower dimension, so $f$ is a submmersion. But the submmersions are opened applications. So, $A=\{f(x);x\in U\}\subset \mathbb{R}^4$ is open. Consider a value $|f(x_0)|,x_0\in U$ and we'll prove that it isn't maximal. Once $A$ is open, there is $\delta>0$ such that $B[f(x_0),\delta]\subset A$ . Taking $f(x_0) $ as a vector, we have $(1+\delta)f(x_0)\in A$ . So, there is $x_1\in U$ such that $f(x_1)=(1+\delta)f(x_0)$ . But $|f(x_1)|=(1+\delta)|f(x_0)|>|f(x_0)|$ and this completes the proof. What do you think? Thanks very much.","['matrix-rank', 'analysis', 'open-map']"
2853727,Issue with Matrix Multiplication using the Formal Definition,"I am writing a formal proof to show that if $B$ is the matrix obtained by interchanging the rows of a $2\times2$ matrix $A$, then $\det(B)=-\det(A)$.  My reasoning and proof are coming along nicely but I hit a bump in the road that highlighted to me a gap in my knowledge - that is, I guess I do not completely understand the definition of matrix multiplication.  Note I went the rigorous route here only because I wanted to prove to myself I fully understood matrix multiplication... and I don't.  My proof thus far is: Let $E$ be the elementary matrix obtained by performing a type 1 elementary row operation on $I_2$.  By Theorem 3.1 (Friedberg), $B=EA$.  Note
$$\det(A) =\det\begin{pmatrix}
    a & b \\
    c & d
    \end{pmatrix}=ad-bc$$ By the definition of matrix multiplication, \begin{align}
& B_{ij}=(EA)_{ij} \\[10pt]
= {} & \sum_{k=1}^2 E_{ik}A_{kj} \text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt]
= {} & E_{i1}A_{1j}+E_{i2}A_{2j}\text{ for } 1\le i\le2\text{, }1\le j\le2 \\[10pt]
\vdots\,\,\, \\[10pt]
= {} & \begin{pmatrix}
    c & d \\
    a & b
    \end{pmatrix}_{ij}
\end{align} If $B=EA=\begin{pmatrix}
    c & d \\
    a & b \\
    \end{pmatrix}$, then by the definition of a determinant of a $2\times2$ matrix, \begin{align}
\det(B) & =\det(EA)=bc-ad \\[10pt]
& =-(ad-bc) \\[10pt]
& =-\det(A)
\end{align} My issue is, how do I formally express the steps where I put my ""$\cdots$""?  That is, the column and row vector multiplication and their sum? Maybe I'm wrong, but I feel most sources don't fully explain all the steps of matrix multiplication and just resort to hand-waving.  The way I think about it - the column and row vectors I will be multiplying in my proof are actually just $2\times1$ and $1\times2$ matrices, respectively.  I know they result in a $2\times2$ matrix, but how?  And why?","['matrices', 'proof-writing', 'linear-algebra', 'vectors']"
2853766,"Analogies between $(\tan, \sec)$ and $(\sinh, \cosh)$","The pair of functions $(\tan, \sec)$ shares some interesting properties with the pair $(\sinh, \cosh)$. First of all, they satisfy the same quadratic equation, namely
$$\sec^2 x - \tan^2 x = 1 \qquad \cosh^2 x - \sinh^2 x = 1$$
for any $x$ in the respective domains. Moreover, $\tan$ and $\sinh$ are both odd functions, while $\sec$ and $\cosh$ are both even functions. Now, suppose that we define a binary operation $\oplus$ on some subset of real numbers such that
$$\tan (x \oplus y) = \tan x \sec y + \sec x \tan y$$
whenever $x \oplus y$ is defined. Then one can prove that
$$\sec(x \oplus y) = \sec x \sec y + \tan x \tan y$$
and these two formulas look exactly like the addition formulas for the hyperbolic functions. (For the subtraction formulas it is enough to let $x \ominus y = x \oplus (-y)$ whenever it is defined.) There's more: one can also prove that an analogue to De Moivre's formula holds, i.e.,
$$(\sec x + \tan x)^n = \sec (\mathring n x) + \tan (\mathring n x)$$
where $\mathring n x$ denotes $x \oplus x \oplus \dotsb \oplus x$ with $n$ addends. Finally, if we define an analogue of the derivative with this new operation by letting
$$\mathring D f(x) = \lim_{h \to 0} \frac {f(x \oplus h) - f(x)} h$$
then we obtain
$$\mathring D \tan x = \sec x \qquad \mathring D \sec x = \tan x$$
similarly to what happens with the hyperbolic functions. My questions are: Is there a way to make this correspondence precise so that one can give a simple unique explanation for all of these analogies (and possibly others that might hold)? How can we interpret the operation $\oplus$?","['hyperbolic-functions', 'trigonometry', 'calculus']"
2853800,Can all convex $3n$-iamonds be tiled by $3$-iamonds?,"Background A polyiamond is a plane figure constructed by joining together equilateral triangles of the same size along their edges. The number of convex polyiamonds is given by A096004 . Based on enumerating examples up to $n=8$, it looks plausible that all $A096004(3n)$ convex $3n$-iamonds can be tiled by the $3$-iamond (i.e. the triamond). Question If the conjecture is true, I'm curious to see a proof (or even some heuristics). If the conjecture isn't true, I'd be curious to see a counterexample. (It's perhaps worth noting that not all convex $2n$-iamonds can be tiled by the $2$-iamond.)","['discrete-geometry', 'combinatorics', 'polyomino']"
2853819,Evaluating a nested log integral,"Question: $$\int\limits_0^1\mathrm dx\,\frac {\log\log\frac 1x}{(1+x)^2}=\frac 12\log\frac {\pi}2-\frac {\gamma}2$$ I’ve had some practice with similar integrals, but this one eludes me for some reason. I first made the transformation $x\mapsto-\log x$ to get rid of the nested log. Therefore$$\mathfrak{I}=\int\limits_0^{\infty}\mathrm dx\,\frac {e^{-x}\log x}{(1+e^{-x})^2}$$
The inside integrand can be rewritten as an infinite series to get$$\mathfrak{I}=\sum\limits_{n\geq0}(n+1)(-1)^n\int\limits_0^{\infty}\mathrm dx\, e^{-x(n+1)}\log x$$The inside integral, I thought, could be evaluated by differentiating the gamma function to get$$\int\limits_0^{\infty}\mathrm dt\, e^{-t(n+1)}\log t=-\frac {\gamma}{n+1}-\frac {\log(n+1)}{n+1}$$
However, when I simplify everything and split the sum, neither sum converges. If we consider it as a Cesaro sum, then I know for sure that$$\sum\limits_{n\geq0}(-1)^n=\frac 12$$Which eventually does give the right answer. But I’m not sure if we’re quite allowed to do that especially because in a general sense, neither sum converges.",['integration']
2853821,Is there an intuitive reason why the natural logarithm shows up in the Prime Number Theorem? [duplicate],This question already has an answer here : Intuition for the prime number theorem (1 answer) Closed 5 years ago . I've always wondered if it has something to do with the idea that the probability that an integer is divisible by a prime $p$ is $1/p$.,"['number-theory', 'prime-numbers']"
2853822,Proof of the inequality $||a+b|^p - |a|^p| \leq \epsilon |a|^p + C_{\epsilon} |b|^p$,"In this paper , the following theorem is used implicitly (see Example (a) on page 488): Let $0<p<\infty$. For every $\epsilon > 0$, there exists some $C_{\epsilon} > 0$ such that $\forall a,b \in \mathbb{C}$:
  $$
\left||a+b|^p - |a|^p \right| \leq \epsilon |a|^p + C_{\epsilon} |b|^p
$$ Is this a well-known result? I would like to know its proof, and its name if it has one. Can someone point me towards a reference to this result which includes a proof, or give me a proof outline? I've tried using Jensen's Inequality for the case $p>1, a,b>0$, but it doesn't seem to work out.","['real-analysis', 'inequality']"
2853858,Singularity Type Of $f(z^2+z)$,"$f(z)$ has essential singularity at $z=0$, what type of singularity $f(z^2+z)$ has? $f(z)$ has essential singularity at $z=0$ so it can be written has $\sum_{n=0}^{-\infty} c_nz^n=c_0+\frac{c_{-1}}{z}+\frac{c_{-2}}{z^2}+...+$
substituting $z=z^2+z$ will leave all the power to be negative e.g  $c_0+\frac{c_{-1}}{z^2+z}+\frac{c_{-2}}{(z^2+z)^2}+...+$ So $f(z^2+z)$ has essential singularity too, moreover can we even ""get rid"" of essential singularity?","['complex-analysis', 'singularity']"
2853879,pushforward and pullback sheaf is isomorphism?,"I am reading Hartshorne's book chapter 5 (on surface) and I have a question: on page 371, proposition 2.3, it says: Let $X$ be surface, $C$ curve, $\pi:X\to C$ ruled surface. $f$ be a fiber, $\sigma$ be a section of $\pi$, $C_0=\sigma(C)$. Let $D$ be a divisor on $X$, $D.f$=n, and set $D'=D-nC_0$. Then he claims: $L(D')\cong \pi^*\pi_* L(D')$ I don't know how this comes from? Can someone helps me? Thanks in advance.",['algebraic-geometry']
2853891,A question about holomorphic structure in Atiyah Bott's paper.,"I am reading Atiyah & Bott's The Yang-Mills equations over Riemann surfaces . I meet a question about the holomorphic structure. Let $E\to M$ be a complex vector bundle over a compact Riemann surface. There is a Hermitian metric on $E$. Then 
\begin{equation}
	\mathscr A\cong\mathscr B
\end{equation}
for 
\begin{equation}
	\begin{split}
		\mathscr A=&\{\text{all smooth hermitian connections}\}\\
		\mathscr B=&\{\text{all holomorphic structures on }E\}
	\end{split}
\end{equation} The identification is described as: given a connection in $\mathscr A$
\begin{equation}
	d_A\colon\Omega^0(M;E)\to\Omega^1(M;E)
\end{equation}
then complexify it 
\begin{equation}
	d_A\colon\Omega^0_{\mathbb C}(M;E)\to\Omega^1_{\mathbb C}(M;E)
\end{equation}
and we have 
\begin{equation}
	\Omega^1_{\mathbb C}(M;E)=\Omega^{1,0}(M;E)\oplus\Omega^{0,1}(M;E)
\end{equation} Then 
\begin{equation}
	d_A=d_A'\oplus d_A''
\end{equation}
for 
\begin{equation}
	\begin{split}
		d_A'\colon & \Omega^0_{\mathbb C}(M;E)\to\Omega^{1,0}(M;E)\\
		d_A''\colon & \Omega^0_{\mathbb C}(M;E)\to\Omega^{0,1}(M;E)
	\end{split}
\end{equation}
and the operator $d_A''$ determines a holomorphic structure of $E$. Questions: The decomposition $\Omega^1_{\mathbb C}(M;E)=\Omega^{1,0}(M;E)\oplus\Omega^{0,1}(M;E)$ is due to a Hodge star operator. Is there a canonical Hodge star operator here? I know there is another decomposition using the canonical almost complex structure $J$, and I do not understand why not use this. How to recover $d_A$ from $d_A''$? I guess here we need the Hermitian metric. Thanks in advance.","['connections', 'differential-geometry']"
2853901,Darboux theorem for $2$-dimensional manifolds,"Let $M$ be a $2$ -dimensional manifold. Using the fact that every non-vanishing $\alpha\in\Omega^1(M)$ can be written as $\alpha=fdg$ locally for convenient smooth functions $f,g$ , prove Darboux's theorem for $2$ dimension: If $(M^2,\omega)$ is a symplectic manifold, there are local coordinates $(x,y)$ such that $\omega=dx\wedge dy$ . I guess the only way to use the given result is by considering a field $X$ such that $X\neq 0$ in some neighbourhood and the non-vanishing $1$ -form $i_X\omega$ . Then we would have $f,g$ with $i_X\omega=fdg$ , but I don't know how to deal with that. I tried to prove that $\omega=df\wedge dg$ , but I think that's not true.","['symplectic-geometry', 'differential-forms', 'differential-geometry']"
2853934,Existence of homomorphisms between finite fields,"Let $F$ and $E$ be the fields of order $8$ and $32$ respectively. Construct a ring homomorphism $F\to E$ or prove that one cannot exist. Any element $x$ of $F$ satisfies $x^8=x$ and any nonzero element $x\in F$ satisfies $x^7=1$. Let $f:F\to E$ be a homomorphism. Then $1=f(1)=f(x^7)=f(x)^7$ for all nonzero $x\in F$. Also $0=f(0)=f(2)=f(1+1)=f(1)+f(1)=0$ (here $2=1+1\in F$). Now on the one hand, $f(2^7)=f(2)^7=1$. On the other hand, $f(2^7)$ is $f$ applied to $2+\dots+2$ ($2^6$ terms), so $f(2^7)=f(2\cdot 2^6)=f(2)+\dots+f(2) \text{ ($2^6$ terms) }=0+\dots+0=0$. This is a contradiction. Is this reasoning correct? Are there other ways to solve this? In general, for which finite fields there exists a homomorphism between them? As I pointed out in the comments, the above argument is incorrect. How do I solve the problem then?","['finite-fields', 'ring-homomorphism', 'abstract-algebra', 'ring-theory', 'field-theory']"
2853938,Generating samples from certain regions of a Gaussian distribution,"Let $X$ be a Gaussian random variable where for simplicity, $X \sim N(0,1)$. I am interested in generating samples of $X$ in the following cases: $X \mid X \in (a,b)$ where $a < b$ or $X \mid X > b$. I am aware that I can do rejection sampling for this and especially for the latter, Wikipedia says (under rejection sampling) that exponential tilting is a very efficient manner to generate samples for $X \mid X>b$. I am wondering though: is there a better way to do this without rejection sampling? In other words, do algorithms exist wherein the above scenarios can be sampled by just a simple scaling/translation of a random variable that can be easily sampled? Thoughts/comments appreciated. Thanks!","['probability-theory', 'statistics', 'probability', 'monte-carlo', 'random-variables']"
2853941,Finding Mobius transformations that maps one set to another,"I am having a hard time understanding how we find mobius maps from circles, discs to half planes etc. I know how to find maps that take a set of points to another but not sets. I know about cross ratios, orientation principle, etc but I might be skipping some other important concept here. For example the below image is something I found My doubts in this are - How were the specific points chosen. Even though 1, i, -1 can be thought of as the boundary points of the disk in the half plane Im(z) > 0 but I don't understand the selection of points 0, 1, infinity. Why take points in the real axis? I didn't understand anything achieved or done in the last line. Why is f(0) important and how the final inference? I am afraid I might be missing some fundamental theorems. Any help is appreciated.","['complex-analysis', 'mobius-transformation']"
2853986,What is this property to be called?,"Two functions, $f$ and $g$, that satisfy the following identity: $$f(g(a_1,...,a_n), g(b_1,...,b_n),...) = g(f(a_1,b_1,....), f(a_2,b_2,...)...)$$
(notice the ""transposition"" of the arguments), do they possess a named and/or known property? Or as an alternative, where can I find out more about it? (Apologies if I asked a duplicate question, but I didn't know how to search for it)","['terminology', 'abstract-algebra', 'group-theory', 'functional-equations']"
2854021,"$\tan(a) = 3/4$ and $\tan (b) = 5/12$, what is $\cos(a+b)$","It is known that 
$$\tan(a) = \frac{3}{4}, \:\:\: \tan(b) = \frac{5}{12} $$
with $a,b < \frac{\pi}{2}$.
What is $\cos(a+b)$? Attempt : $$ \cos(a+b) = \cos(a) \cos(b) - \sin(a) \sin(b) $$
And we can write $\tan(a) = \sin(a)/\cos(a) = 0.3/0.4 $
and $ \sin(b)/\cos(b) = 0.05/0.12 $
so
$$ \cos(a+b) = (0.4)(0.12) - (0.3) (0.05)  = 33/1000$$ Is this correct? Thanks.","['angle', 'trigonometry', 'triangles']"
2854025,Variance of a sum of IID random variables.,"Let's say we have a sequence of $n$ IID random variables, $I_i$. Let's define a new random variable which is their sum: $$S = \sum_i I_i$$ To calculate the variance of $S$ we can say - $$V(S) = \sum_i V(I_i) = nV(I_1)$$ Or we can also say that $S$ has the same distribution as $nI_1$ So, we should have $V(S) = V(nI_1) = n^2V(I_1)$. This is of course, in direct contradiction to what we got above. What am I missing?","['independence', 'variance', 'probability', 'random-variables']"
2854052,"How to compute $H^{\bullet}(\mathbb{P}^n,\mathcal{O}(k))$ via Dolbeault cohomology?","In algebraic geometry, the cohomology groups $H^{\bullet}(\mathbb{P}^n,\mathcal{O}(k))$ are well-understood. As far as I know, in most textbooks these cohomology groups are computed via Cech cohomology or Koszul resolutions. My question is: in the case of $\mathbb{CP}^{n}$, could we compute $H^{\bullet}(\mathbb{CP}^n,\mathcal{O}(k))$ analytically via Dolbeault cohomology? Is there any references？","['reference-request', 'sheaf-cohomology', 'complex-geometry', 'algebraic-geometry']"
2854054,Nontrivial integer solutions of $\sum_{i=1}^3 a_i ^3=\sum_{i=1}^3 b_i ^3$ and $\sum_{i=1}^3 a_i =\sum_{i=1}^3 b_i$,"The goal is to understand a set of nontrivial solutions of cubic polynomials $$
\sum_{i=1}^3 a_i ^3=\sum_{i=1}^3 b_i ^3 
\Rightarrow a_1^3+a_2^3+a_3^3=b_1^3+b_2^3+b_3^3, \tag{1}
$$
  $$
\sum_{i=1}^3 a_i =\sum_{i=1}^3 b_i \Rightarrow a_1+a_2+a_3=b_1+b_2+b_3 , \tag{2}
$$
  for $a_i,b_i\in \mathbb{Z}$
  where we demand $(a_1,a_2,a_3)\neq (b_1,b_2,b_3)$ or any of its permutation (total $3!=6$ cases). Question : Are there any nontrivial solutions? (e.g. $a_i,b_i\in \mathbb{Z}$
where we demand $(a_1,a_2,a_3)\neq (b_1,b_2,b_3)$ and other $3!$ permutations.) How many simple but nontrivial solutions are there in the smaller values of $a,b,c,d$? (The simple form the solutions are the better.) Or can one prove or disprove that there are nontrivial solutions? (Even better, if we can get a quick algorithm to generate the solutions.) A non-trivial example or a proof of non-existence is required to be accepted as a final answer. Note add: My trials/attempts : Since it is encouraged to show one's own attempt. First, it looks like if we have only $i=1,2$ instead of summing over $i=1,2,3$, we have $\sum_{i=1}^2 a_i ^3=\sum_{i=1}^2 b_i ^3 $ and $\sum_{i=1}^3 a_i =\sum_{i=1}^3 b_i$, the nontrivial solutions seem to be impossible (?), but no rigorous proof has be shown yet . Let me share a few comments on what these two equations boil down to: If we take (2)$^3$-(1), we get
$$
(a_1+a_2)(a_1+a_3)(a_2+a_3)=(b_1+b_2)(b_1+b_3)(b_2+b_3), \tag{3}
$$
we can further use (2) and (3) to get a lot more interesting constraints. However, I haven't got any other linear relation constraint as useful as (2) yet.","['number-theory', 'polynomials', 'arithmetic', 'prime-numbers']"
2854087,"$(X,d)$ is separable if and only if $X$ does not contain an uncountable closed discrete subset","The problem is to prove that a metric space $(X,d)$ is separable if and only if it does not contain an uncountable closed discrete subset. The first thing I want to note is that being closed and discrete is equivalent to not having any limit points, so the problem can be restated as ""prove $(X,d)$ is separable if and only if every uncountable subset has a limit point in $X$"". The forward direction is simple enough. Suppose $X$ is separable and let $A$ be a countable dense subset. Assume to the contrary that there does exist an uncountable subset $B$ which has no limit points. Then for each $b\in B$, there exists $\epsilon_b>0$ such that the open ball $B_d(b,\epsilon_b)$ is disjoint from $B\setminus\{b\}$. Then the collection of open balls $\mathcal{A}=\{B_d(b,\epsilon_b/2):b\in B\}$ is an uncountable collection of pairwise disjoint open sets. Since $A$ is countable and $\mathcal{A}$ is not, $A$ cannot intersect every open ball in $\mathcal{A}$, so there must exist $b\in B$ such that $B_d(b,\epsilon_b/2)$ is disjoint from $A$ contradicting the fact that $A$ is dense in $X$. It's the other direction that's giving me trouble. Suppose that every uncountable subset of $X$ has a limit point in $X$. I need to construct a countable dense subset (or use a nonconstructive existence proof method such as Zorn's Lemma). I have tried coming up with ways of constructing such a countable dense subset, I have tried proving the contrapositive (which is also an existence proof), I have tried using Zorn's Lemma, but nothing I have tried worked. Does anyone have hints or ideas?","['general-topology', 'metric-spaces']"
2854119,Positive integer solutions to $a^3 + b^3 = c^4$,"Let $n$ and $m$ be positive integers. We know that $n^3 + m^3 = n^3 + m^3$ Multiply both sides by $(n^3 + m^3)^3$; on the LHS you distribute, and on the RHS you use power addition rule; $(n^4 + nm^3)^3 + (m^4 + mn^3)^3 = (n^3 + m^3)^4$ So we get an infinite array of solutions with $a = n^4 + nm^3$ $b = m^4 + mn^3$ $c = n^3 + m^3$ Are there any solutions to the equation (in the title) that cannot be expressed in this form?",['number-theory']
2854177,Second derivative symbol $\frac{d^2y}{dx^2}$! [duplicate],"This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Closed 5 years ago . I'm currently studying taking the second derivative of equations and I have been told the symbol used to represent second derivative is $$\frac{d^2y}{dx^2}$$ I was just wondering, why this symbol is chosen? Why is it not $$\frac{dy^2}{dx^2}$$ or $$\frac{d^2y}{d^2x}$$","['derivatives', 'calculus']"
2854189,If $\lfloor\log _71\rfloor+\lfloor\log _72\rfloor+\lfloor\log_73\rfloor+\dots+\lfloor\log_7N\rfloor=N$ then find $N$,"If $\lfloor\log _71\rfloor+\lfloor\log _72\rfloor+\lfloor\log_73\rfloor+\dots+\lfloor\log_7N\rfloor=N$ then find $N$. Honestly speaking,this time i do not have any clue to move forward.
I was thinking to apply only one logarithmic rule -: $\lfloor\log _71\rfloor+\lfloor\log _72\rfloor+\lfloor\log_73\rfloor+\dots+\lfloor\log_7N\rfloor=\lfloor\log_71×2×3×\dots×N\rfloor$ please help me out","['logarithms', 'functions']"
2854226,Gram-Schmidt over GF$(2)$,"I am reading the paper The Steganographic File System by Ross Anderson, Roger Needham, and Adi Shamir. On page 4, paragraph 2, the authors write: Finally, we use the Gram-Schmidt method to orthonormalise all the
  vectors from $i$ onwards by subtracting from the candidate $K_i$ all
  its components along later $K_j$ which the user knows by the chaining
  property of the $p_j$’s. This simply means that the authors use the Gram-Schmidt algorithm with the ground field GF$(2)$, and from the context, each of the original vectors the algorithm is applied to has norm $1$. However, in this case the algorithm produces a basis which is not necessarily orthonormal, not even orthogonal. Am I right? Is this a serious flaw?","['finite-fields', 'orthonormal', 'cryptography', 'gram-schmidt', 'linear-algebra']"
2854291,Prove for the time derivative of a vector with constant magnitude,"First of all, sorry for my bad English, I have several doubts about the geometric demonstration made by Kleppner for the derivative of a vector that has a constant magnitude, page 25, which is attached in this photo: . Is this proof rigorously validated? I understand that $\sin (x) \approx x$ for small $x$, by dividing both sides by $\Delta t$, and take the limit for the equality. I understand that equality is correct, but is the argument correct? In the book, i understand the intuitive part, but for example what would have happened if I had taken an approximation of higher order, this would not have had any contribution in the limit? How are these terms depreciated rigorously, since I think that depends on how the time angle $θ (t)$ depends, for example, you can say that the vector varies with $Δθ (t)$, very slow or very fast so that the terms of higher order have a contribution in the limit, and therefore the equality raised is not correct, I apologize in case the question is a triviality.","['derivatives', 'physics', 'mathematical-physics', 'geometry', 'vectors']"
2854305,Trying to prove $\partial^2=0$ on $k$-cells,"Consider a $k$-cell $ c : [0,1]^k \to U \subset \mathbb{R}^n , (t_1,...,t_k) \mapsto c (t_1,...,t_k) $. Then the boundary of $c$, $\partial c$ is defined as $$ \partial c := \sum_{i=1}^k (-1)^{i-1} \left( c_i^1 - c_i^0 \right) $$ where $$ c_i^1 := c(t_1,...,t_{i-1},1,t_{i+1},...,t_k)  \\ c_i^0 := c(t_1,...,t_{i-1},0,t_{i+1},...,t_k) $$ Then supposing $c = \sum_m a^m c_m$ is a $k$-chain ($a^m \in \mathbb{R}$, $c_m$ $k$-cells), it’s boundary is defined by $$ \partial c = \partial \left( \sum_m a^m c_m \right) := \sum_m a^m \partial c_m $$ I’m struggling to prove the special property of $\partial$ that $\partial^2 = 0$. Here’s what I have so far... Since $\partial$ is linear, it is sufficient to prove $\partial^2 c = 0$ for any $k$-cell $c$. Introducing another sigma sign, the definition of $\partial c$ can be made to look nicer, $$ 
\partial c := \sum_{i=1}^k (-1)^{i-1} \left( c_i^1 - c_i^0 \right) 
\\ 
=  \sum_{i=1}^k (-1)^{i-1} \sum_{\rho=0}^1 (-1)^{\rho+1} c_i^\rho   \\
= \sum_{i=1}^k \sum_{\rho=0}^1 (-1)^{i+\rho} c_i^\rho
$$ Then, I tried brute forcing some algebra and definitions... $$
\partial^2 c = \partial \sum_{i=1}^k \sum_{\rho=0}^1 (-1)^{i+\rho} c_i^\rho 
= \sum_{i=1}^k \sum_{\rho=0}^1 (-1)^{i+\rho} \partial c_i^\rho
\\ 
= \sum_{i=1}^k \sum_{\rho=0}^1 (-1)^{i+\rho} \sum_{j=1 | j\neq i}^k \sum_{\sigma=0}^1 (-1)^{j+\sigma} c_{ij}^{\rho \sigma} 
\\ 
= \sum_{i,j=1 | i\neq j }^{k} \sum_{\rho,\sigma = 0}^{1} (-1)^{i+j+\rho+\sigma} c_{ij}^{\rho\sigma} 
$$ where $$
c_{ij}^{\rho\sigma} = c(t_1, ... , t_{i-1} , \rho , t_{i+1} , ... , t_{j-1} , \sigma, t_{j+1}, ... , t_k) 
$$ This is where I’m stuck.",['differential-geometry']
2854322,"Is there a mathematical basis for the idea that this interpretation of confidence intervals is incorrect, or is it just frequentist philosophy?","Suppose the mean time it takes all workers in a particular city to get to work is estimated as $21$. A $95\%$ confident interval is calculated to be $(18.3, 23.7).$ According to this website, the following statement is incorrect: There is a $95\%$ chance that the mean time it takes all workers in this city to get to work is between $18.3$ and $23.7$ minutes. Indeed, a lot websites echo a similar sentiment. This one , for example, says: It is not quite correct to ask about the probability that the interval contains the population mean. It either does or it doesn't. The meta-concept at work seems to be the idea that population parameters cannot be random, only the data we obtain about them can be random ( related ). This doesn't sit right with me, because I tend to think of probability as being fundamentally about our certainty that the world is a certain way. Also, if I understand correctly, there's really no mathematical basis for the notion that probabilities only apply to data and not parameters; in particular, this seems to be a manifestation of the frequentist/bayesianism debate. Question. If the above comments are correct, then it would seem that the kinds of statements made on the aforementioned websites shouldn't be taken too seriously. To make a stronger claim, I'm under the impression that if an exam grader were to mark a student down for the aforementioned ""incorrect"" interpretation of confidence intervals, my impression is that this would be inappropriate (this hasn't happened to me; it's a hypothetical). In any event, based on the underlying mathematics, are these fair comments I'm making, or is there something I'm missing?","['statistical-inference', 'probability-theory', 'statistics', 'probability', 'philosophy']"
2854333,"A ""geometric''-ish infinite sum of matrices","Suppose I have full rank $n\times n$ matrix $A$ with $\rho(A) < 1$ and I want to find an expression for $$S = X +  A^\top X A + A^{2\top} X A^2 + A^{3\top} X A^3 + \dots$$ where $X$ is an $n\times n$ positive definite matrix. Thus, $$ S = X + A^\top S A$$ Following this answer to a similar problem, we can make an eigenvalue decomposition of $A$ such that $A=UDU^{-1}$ with $D = \text{diag}(\lambda_1,\dots,\lambda_n)$. Then $$ S = X + U^{-\top}DU^\top S UDU^{-1}$$ and $$ Z = U^\top S U = U^\top X U + DU^\top S UD$$ If we then define $T = U^\top XU$ then $$ Z = T + DZD = T + DTD + D^2TD^2 + D^3TD^3 + \dots$$ which implies that $(i,j)$'th entry of $Z$ is $$ Z_{ij} = \frac{t_{ij}}{1 - \lambda_i\lambda_j}$$ Having obtained $Z$ then we can find S through $$S = U^{-\top}ZU^{-1}$$ My questions are Is the above correct? Is the it a good (fast and numerically stable way) way to compute it? R code confirming the above in one case # assign matrices
A <- matrix(c(.8, .4, .1, .5), 2, 2)
X <- matrix(c( 1, .5, .5,  2), 2)

# compute with above formulas
eg  <- eigen(A)
U   <- eg$vectors
U_t <- t(U)
las <- eg$values
T.  <- crossprod(U, X %*% U)
Z   <- T. / (1 - tcrossprod(las))
S   <- solve(U_t, t(solve(U_t, t(Z))))

# naive solution
nai <- X
for(i in 1:1000)
  nai <- crossprod(A, nai %*% A) + X

nai - S
#R           [,1]      [,2]
#R [1,] -3.55e-15 -4.44e-16
#R [2,] -8.88e-16  0.00e+00",['linear-algebra']
2854365,Evaluating ${\Large\int} _{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{\sin(x)+\cos(x)}{\sin^4(x)-4}dx$,"This integral is giving me hard times, could anyone ""prompt"" a strategy about? I tried, resultless, parameterization and some change of variables. $${\Large\int} _{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{\sin(x)+\cos(x)}{\sin^4(x)-4}dx$$","['real-analysis', 'trigonometry', 'integration', 'definite-integrals', 'trigonometric-integrals']"
2854376,Understanding How To Find Zeroes Of Polynomials.,"I am currently learning how to find zeroes of polynomials. However, 4 things confuse me. The first question I have is why exactly does the rational zeroes theorem hold true? I get how to use it, but I don't get why it is true. Can someone please explain this? The second question I have is why is the Descartes rule of signs true? My third question is how do we find the imaginary roots of polynomial? It can't be through the rational zeroes theorem! But if we don't need to find imaginary roots, why else would we need things like the conjugate pairs theorem? My last question is of synthetic division in finding the zeroes of polynomials. I don't get why using synthetic division to test if a root is a zero of a polynomial is by taking it, and synthetically dividing it. What I don't get is, why don't you need to change the sign of the zero before synthetically dividing here? Let's say I suspected that the zero of a polynomial is x=-4. Don't I need to change the sign of the root to x+4=0, then synthetically divide by 4? Why is that you don't need to do this? Can you explain all of this using simple algebra, without complicated techniques? I don't understand any complicated techniques and theorems beyond the quadratic formula. Can you also show and explain your working, so it is easier for me to follow through? I am still a beginner, so that would help very much.","['algebra-precalculus', 'polynomials']"
2854384,Angles on a point inside a triangle,"Let $ABC$ be an isosceles triangle with $AB=AC$  and $∠BAC =  100$. A point $P$
inside the triangle $ABC$ satisfies that $∠CBP=35$ and $∠PCB= 30$. Find the
measure, in degrees, of angle $∠BAP$. Attached is the figure of the triangle I tried to Angle Chase but it seemed true for all values of $BAP$. I then tried using the sine law. In the triangle $PBC$, We have
$$\frac{PB \sin(35)}{\sin(30)}= PC$$ Trying it with triangles $APB$ ($x= BAP$) and the fact that they are isosceles $$\frac{PC\sin(x+70)}{\sin(100-x)}=\frac{PB\sin(175-x)}{\sin (x)}$$
Which becomes, $$\frac{\sin(35)\sin(x+70)}{\sin(30)\sin(100-x)} = \frac{\sin(175-x)}{\sin(x)}$$ Where in I don't know how to solve it. Other methods are welcome.","['contest-math', 'geometry']"
2854422,Equivalent defintions of minimal sufficient statistics,"Wikipedia claims that the statistic $S(X)$ is minimal sufficient if and only if $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$ $\iff$ $S(x) = S(y)$. It is also claimed that this is a direct consequence of Fisher's factorization theorem. However, I do not see how this is a direct consequence. As a definition of sufficient statistic, I know that the following two are equivalent: $S(x) = S(y)$ $\implies$  $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$ The conditional distribution of $X$ given $S(X)$ does not depend on $\theta$. As a definition of minimal sufficient, I use that a sufficient statistic $S(X)$ is a minimal sufficient statistic if and only if for all sufficient statistics $S'$, $S'(x) = S'(y)$ implies $S(x)=S(y)$. I can prove that if $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$ $\iff$ $S(x) = S(y)$, then $S(X)$ is minimal sufficient The direction $S(x) = S(y)$ $\implies$  $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$  is implies the fact $S$ is sufficient. Now if $S'$ is sufficient, then  $S'(x) = S'(y)$ implies $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$, which implies $S(x) = S(y)$. So  $S(X)$ is minimal sufficient. But I am struggeling with showing that if $S(X)$ is minimal sufficient, then  $f_{\theta}(x)/f_{\theta}(y) $ is independent of $\theta$ $\iff$ $S(x) = S(y)$. I don't see how the factorization theorem helps here.","['statistics', 'sufficient-statistics', 'statistical-inference']"
2854430,"Show that $g(x,y) = x^2y^2\log(x^2+y^2), 0$ is differentiable in (0,0)","I have the following function:
$$
g(x,y) = \left\{\begin{array}{ll} x^2y^2\log(x^2+y^2), & (x,y) \neq (0,0) \\
         0, & (x,y) = (0,0) 
\end{array}\right.
$$ I've already calculated the partial derivatives and showed that $g$ is partially differentiable:
$$
\lim_{h\rightarrow0} \frac{g(0+h,0) - g(0,0)}{h} = \frac{0-0}{h} \\
\lim_{h\rightarrow0} \frac{g(0,0+h) - g(0,0)}{h} = \frac{0-0}{h} = 0
$$ Now I want to show that $g$ is differentiable in $(x,y)=(0,0)$. This is my try so far:
$$
\begin{align}
&\lim_{h\rightarrow0} \frac{g(0+h) - g(0,0) - g'(0,0)h}{|h|} \\
&= \lim_{h\rightarrow0} \frac{h_1^2h_2^2 \log(h_1^2+h_2^2)}{\sqrt{h_1^2+h_2^2}} \\
\end{align}
$$ But now I don't know how to continue and show that the limit is $0$.","['derivatives', 'real-analysis', 'partial-derivative']"
2854438,How to calculate critical value in hypothesis testing?,"You wish to test the following claim ($H_a$) at a significance level of alpha=0.10. $H_o: \mu=70.4$ $H_a: \mu<70.4$ You believe the population is normally distributed and you know the standard deviation is $\sigma=7.6$, sample mean $M=69.3$ for a sample size of $n=51$. What is the critical value for this test? I cannot understand how to find/calculate a critical value for this test. I believe it must be based on $\alpha$, but apparently this is beyond my scope of intuition as 0.1, 0.9, 1-$\alpha$/2, etc. has not worked. Any help would be greatly provided in verbose yet simple terms. This is an intro to stats course for non-majors.","['statistics', 'hypothesis-testing']"
2854442,Properties of the Cartesian product of indexed sets,"Given the indexed sets $(X_i)_{i\in I}$, one usually defines their Cartesian product as:
$$\prod_{i\in I} X_i =_\text{def}\;
\left\{ f: f\in  \left(\bigcup_{i\in I}  X_i\right)^I \wedge\; \forall i \in I\; f(i)\in X_i\right\}
\label{cart1}\tag{1}$$ For $I=\{1,2\}$, it seems natural to set:
$$
X_1\times X_2 =_\text{def}\; \prod_{i\in \{1,2\}} X_i
$$ Then one expects, for $I=\{1\ldots n\}$:
$$\prod_{i=1}^{n} X_i  = \left(\prod_{i=1}^{n-1} X_i\right) \times X_n
\label{cart2}\tag{2}$$ I am unable to derive \ref{cart2}. In fact, the generic element of 
$
\prod\limits_{i=1}^{3} X_i  
$
is: $$
\{(1, f(1)), (2, f(2)), (3, f(3))\}, \text{ where $f$ is a function in }
\left(\bigcup_{i\in \{1,2,3\}}  X_i\right)^{\{1,2,3\}}\;;
$$ while the generic element of: 
$$
\left(\prod_{i=1}^{2} X_i\right) \times X_3
$$
is: $$
\{(1, f(1)), (2, f(2))\}, \text{ where $f$ is a function in }
\left( \left(\prod_{i=1}^{2} X_i\right) \cup   X_3\right)^{\{1,2\}}\;.
$$ So:
$$
\prod_{i=1}^{3} X_i  \neq \left(\prod_{i=1}^{2} X_i\right) \times X_3
\label{cart3}\tag{3}
$$ If that is correct, it extends to the Cartesian power:
$$
X^{n+1} =
X^{n} \times X
$$ If \ref{cart3} is wrong, please, tell me how to prove the equality. If \ref{cart3} is correct, can we properly define \ref{cart1} a Cartesian product, given that the  standard/expected Cartesian product properties are missing? UPDATE The point of my question is: since the infinite CP is a generalisation of the ordinary CP, for a finite index set: $$\prod_{i\in  \{1\ldots n\}} X_i = X_1\times\ldots X_n$$ In words: the finite versions of the infinite CP should work like the ordinary finite version. This might happen with a proper definition of $\times$ for the generalised  CP, and apparently not mine above. In a similar question , @StefanH, wants a CP construction which observes  the following properties: i) it is associative; ii) $A^0 \times A^n = A^n$ iii) gives a natural choice for $A_0$. Making the generalised $\times$ associative is the opposite of what I am looking for, because the standard finite CP is not associative. As for ii) and iii), there is no $A^0$ in the standard CP and so no standard  behaviour to generalise. As noted: if the generalised CP (as defined in $\ref{cart1}$) does not share the essential properties enjoyed by the standard CP, then the name 'generalised CP' might be a misnomer.",['elementary-set-theory']
2854506,Qing Liu exercise 4.1.3: Non-affine Dedekind scheme,"Exercise 4.1.3 of Qing Liu's Algebraic Geometry and Arithmetic Curves asks of us to prove, given a normal Noetherian local scheme $X$ of dimension $2$ with closed point $s$, that $X \backslash \{ s \}$ is a non-affine Dedekind scheme. However, a Dedekind scheme has dimension $1$ and Liu defines normal schemes to be irreducible, so by taking the reduced induced subscheme structure on $X$, which is integral, we see that the open subset $X \backslash \{ s \}$ must also have dimension $2$ and so cannot be Dedekind. Surely even if I relax the definition of normal scheme to include reducible schemes, the result wouldn't hold in the irreducible case anyway? Have I missed something?","['arithmetic-geometry', 'algebraic-geometry']"
2854521,Probability given in the form of $a^{-x}$. Is my answer correct?,"I'm having trouble solving this exercise about probability, as I'm new to the subject. It says: There is a bus supposed to arrive at 8:00. The probability of it to set out with $m$ minutes of delay is $p_m = 3^{-m} (m = 1; 2; 3; ... ;m \neq 0)$. Due to regulatory issues, the set out is never advanced (i.e. it never sets out before 8:00). 1) What's the probability of the bus setting out at 8:00? 2) Prove that the probability of the bus to set out at 8:10 is double the probability that of after 8:10. Here is what I think about it, but I have no answer to check with: I assume all possible values go from 8:00 to infinity, so if I add all those values the probability should be 1. Now, $3^{-m}$ looks like a series I know, and I know it converges as 1/3 is less than 1, so the sum of all terms is $1*\frac{1}{1-1/3}=\frac{3}{2}$, but as that formula is when n starts at 0, then I have to do $\frac{3}{2}-3^0$, which is 0.5. Then, I guess that 1-0.5 is the remaining probability, that is, of the bus setting out exactly at 8:00. Is what I've done correct? Because I don't think we're mixing calculus with probabability in the course. For 2), my guess is calculate $3^{-10}$, and I have to prove that $3^{-10} = 2*\sum_{m=11}^\infty 3^{-m}$. Is it correct? What I do next is calculate the sum, which I don't really know how to do, so I do $3/2 -\sum_{m=0}^{10} 3^{-m}$, put that in the calculator and I get that it's true. So, to wrap it all up... Have I done everything ok? Because I don't have the results. I suppose the second one is correct because I've proven it, but I've no idea about the first one, whether I'm meant to do that or there is another way. I hope you can help me. Thank you!",['probability']
2854525,Intuitive Explanation Of Descartes' Rule Of Signs,"Can someone please explain to me why, intuitively, does Descartes rule of signs work? I realize there is a previous answer for this, that ""Basically, at different values of $x$ different terms in the polynomial ""dominate."" So every the sign switches, there will be a change in the direction of the curve. Either This will result in crossing the $x$-axis and a root or There will have to be another change, meaning ""losing roots"" will always happen in pairs. So the roots are equal to, or less than
    by an even number, the number of sign changes."" But perhaps because my
    math understanding is not good enough, I still fail to see why this
    ensures that the Descartes rule of signs works. Why would different values of $x$ dominate in different areas? And why would this result in crossing the $x$-axis or losing roots? Can you please explain to me why the rule of signs can find the number of real zeroes for any polynomial? Please keep the explanation simple.","['algebra-precalculus', 'polynomials']"
2854541,Is it necessary to explicitly state that $\lnot (q \notin X)$ is equivalent to $q \in X$ when writing a proof?,"In a proof I am trying to write, I have shown that $q \in X$ and $(q \notin X) \lor (Y= \emptyset)$ are both true. Is it necessary to explicitly state $\lnot (q \notin X)$ is true in order to use the disjunction elimination to get $Y = \emptyset$, or can I assume that it is obvious?","['proof-writing', 'first-order-logic', 'propositional-calculus', 'elementary-set-theory']"
2854565,Dual norm of a product space,"Suppose we have two spaces $X, Y$, each is a linear space equipped with norm $\|\cdot\|_X, \|\cdot\|_Y$, then we define a norm on the product space $Z = X\times Y: \|(x,y)\| = \sqrt{a\|x\|_X^2+b\|y\|_Y^2}$, where $a,b > 0$. I am wondering how to compute the dual norm of this norm. By the definition of dual norm, we need to find for $z^* \in Z^*$, $\|z^*\|_* = \underset{\|z\|=1}{\mathrm{sup}} \;z^*(z)$, but what is $z^*(z)$ here? What are the elements in $Z^*$ like?","['functional-analysis', 'normed-spaces', 'dual-spaces']"
2854569,Realizing the Berkovich affine line as a union of Berkovich spectrums,"I am trying to understand what is the relation of the affine Berkovich space to the Berkovich space on an appropriate polynomial ring. A more exact version of the question is as follows: Let $(K,\Vert \cdot \Vert)$ be a complete valued field. The affine Berkovich space is defined as: $ A_K^{n, \text{an}}:=
  \begin{Bmatrix}
    &  \vert f+g\vert_x \leq \vert f\vert_x +\vert g\vert_x  \\
    \vert \cdot \vert _x:K[x_1,...,x_n]\rightarrow [0,\infty) \quad \Bigg \vert  & \vert f\cdot g\vert_x= \vert f\vert_x \cdot \vert g\vert_x \\
     &  \vert a\vert_x= \Vert a\Vert \; \text{for all } \; a\in K
  \end{Bmatrix}  $ i.e, non-trivial multiplicative semi-norms on $K[x_1,...,x_n]$ extending $\Vert \cdot \Vert$. Given a Banach ring $(A,\Vert \cdot \Vert)$, it's Berkovich  spectrum is: $ \mathcal{M}(A):= \begin{Bmatrix}
   \text{multiplicative bounded seminorms}  \\
   \vert \cdot \vert : A \rightarrow [0,\infty) 
  \end{Bmatrix}  $ They both have the weakest topology induced by evaluations. I have been trying to understand what is the relation between $A_K^{n, \text{an}}$ and a Berkovich spectrum of $K[x_1,...,x_n]$. They would coincide were it not for the extra assumption of boundedness. I saw somewhere that the Berkovich affine space is not a Berkovich spectrum, but instead an increasing union of Berkovich spectrums of power rings. I have not found thus far how these spectrums are defined, and would appreciate anyone explaining, or directing to a reference where it is indeed written.","['general-topology', 'geometric-functional-analysis', 'spectra', 'algebraic-geometry']"
2854574,Why the projective space $ \mathbb{P}^{1} $ is homeomorphic to $ S^{3}/S^{1} $?,"Define $ \mathbb{P}^{1} $ to be the space of lines in $ \mathbb{C}^{2} $ passing through the origin, i.e. $ \mathbb{P}^{1} = \{ \mathbb{C}^2 - \{ (0, 0) \} \} / {\mathbb{C}^{*}} $ where $ \mathbb{C}^{*} $ acts on $ \mathbb{C}^{2} $ by $ k(x, y) = (kx, ky) $ for $ k\in \mathbb{C}^{*} $. Take $ S^{3} = \{ (z_0, z_1) \in \mathbb{C}^{2} | |z_0|^{2}+|z_1|^{2} = 1 \} $. $ 
S^{1}=\{e^{i\theta}\} $ the unit circle in the complex plane acts on $ S^{3} $ by the natural action. Then how to deduce that $ \mathbb{P}^{1} \cong S^{3}/S^{1} $?","['general-topology', 'topological-groups', 'algebraic-geometry']"
2854588,Stronger Axiom for Circuit Matroids - why is it equivalent?,"The typical definition of a circuit matroid is as follows. A matroid $M$ consists of a finite set of elements $E(M)$ along with a collection of non-empty subsets $C(M)\subseteq 2^M$ (called circuits ) that satisfies the following properties: 1) If $X, Y \in C(M)$ and $X \subseteq Y$ then $X = Y$ (i.e. no circuit is properly contained in another circuit). 2) If $X, Y \in C(M)$ and $X \neq Y$, then for any $a \in X \cap Y$ there exists a circuit $Z \in C(M)$ such that $Z \subseteq (X \cup Y) - a$. I recently came across a paper that replaced the second axiom with a stronger version: 2) If $X, Y \in C(M)$ and $X \neq Y$, then for any $a \in X \cap Y$ and for any $b \in X\setminus Y$ there exists a circuit $Z \in C(M)$ such that $Z \subseteq (X \cup Y) - a$ and such that $b \in Z$. Based on the presentation in the paper, these definitions of matroid seem to be equivalent. So far, I've been unable to prove to that the first set of axioms implies the second. Any ideas?","['combinatorics', 'matroids']"
2854605,Straightening Theorem for Vector Fields,"Let $M$ be a smooth manifold and $X\in\mathfrak{X}(M)$. The straightening theorem says: If $X_p\neq 0$, there is a chart $(U,y_1,...,y_n)$ around $p$ for which $X=\frac{\partial}{\partial y_1}$. The link above gives a proof  using a differential equation argument, but I've tried an alternative proof: Take a chart $(U,\phi)$ around $p$ with $U$ small enough so that $X|_U$ is never zero. In that neighbourhood, we can take a smooth local frame $\{X_1,...,X_n\}$, with $X_1=X$. Then:
  $$X_j=\sum_{i=1}^na_{ij}\frac{\partial}{\partial \phi_i}$$
  for some $a_{ij}\in C^\infty(U)$. Since $X_1,...,X_n$ are linearly independent, the matrix $(a_{ij})_{i,j}$ is invertible in $U$. In the domain $U$, define: 
  $$\psi:=(a_{ij})_{i,j}^{-1}\circ\phi$$
  This function belongs to the maximal atlas, because for every $(V,\xi)$ with $U\cap V\neq \emptyset$, we have:
  $$\psi\circ\xi^{-1}=(a_{ij})_{i,j}^{-1}\circ(\phi\circ\xi^{-1})\in C^{\infty}$$
  $$\xi\circ\psi^{-1}=(\xi\circ\phi^{-1})\circ(a_{ij})_{i,j}\in C^{\infty}$$
  Therefore $(U,\psi)$ is a chart which in particular satisfies $X=\frac{\partial}{\partial \psi_1}$.$_\blacksquare$ I can't see any mistake in this proof, but I've discovered some problems as a consequence of what I did. Using the same idea, if we have fields $X,Y$ which are not zero and linearly independent in some neighbourhood, then we could extend them to a local frame $\{X_1=X,X_2=Y,...,X_n\}$ and construct a similar $\psi$ for which $X=\frac{\partial}{\partial \psi_1},Y=\frac{\partial}{\partial \psi_2}$, but I've read that this is not possible, at least not for arbitrary $X,Y$. What am I missing?","['smooth-manifolds', 'differential-geometry']"
2854620,Prove that second partial derivatives does not depend on the order of differentiation [duplicate],"This question already has an answer here : Symmetry of second derivative - Sufficiency of twice-differentiability (1 answer) Closed 5 years ago . I'm trying to prove that if $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$ are continuous in an open set containing $a \in \mathbb{R}^2$, then they are equal using the definition of derivative: First, I say $$ \dfrac{\partial^2f}{\partial x \partial y} = \dfrac{\partial}{\partial x} \left( \dfrac{\partial f}{\partial y} \right) = \dfrac{\partial}{\partial x} \left( \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \lim_{k \to 0} \dfrac{1}{k} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)}{h} - \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \\ \lim_{k \to 0} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right) $$ Then I do the same thing for $ \dfrac{\partial^2 f}{\partial y \partial x} $, and I get $$ \lim_{h \to 0} \left( \lim_{k \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right)$$ My question is: am I allow to say that those limits are equal because both $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$ 
are continuous on $a$, or do I need something extra?
Please let me know if the question is clear enough, or if I made some silly mistakes :) Thanks in advance!","['multivariable-calculus', 'calculus', 'derivatives']"
2854646,Assume I choose $n$ random integers such that the last digit is uniformly distributed. What is the distribution of the last digit of the sum?,"Say that I sample $n$ random integers from some random variable $X$. The distribution has the last digit of the integer uniformly distributed. I then take the samples and add them $$
Y = x_1+x_2+x_3 + ... + x_n
$$ What is the distribution of the last digit of $Y$? I want to also say uniform, but I'm not sure","['probability-distributions', 'statistics', 'probability', 'uniform-distribution', 'combinatorics']"
2854668,How to claim the subspace $AB-BA$ has dimension $n^2-1$?,"There is a problem given in LINEAR ALGEBRA by  HOFFMAN & kUNZE PAGE $107$, Show that the trace functional on squre matrices of order $n$ is characterized in the following sense:
If $W$ is the space of size $n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that  $f(AB)=f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. MY TRY: Put $C=AB-BA$. Clearly $\operatorname{Tr} C=0$. I claim that dimension of the subspace of such $C$ is $n^2-1$. So by the rank-nullity theorem $$\dim \ker f +\dim\operatorname{Im} f=n^2$$ i.e $n^2-1+\dim\operatorname{Im}=n^2$ ( as $f(C)=0$). Hence $\operatorname{rank} f=1$. Now since trace functional satisfies such $f$'s and it's scalar multiple is also a subspace of dimension $1$, $f$ needs to be scalar multiple of $\operatorname{Tr}$. MY PROBLEM: I claimed that subspace (the terrible fact is I'm not sure whether it's a subspace or not, can't prove it) of the matrices of the form $AB-BA$ has dimension with $n^2-1$. It means that there is no restriction on the entries of $AB-BA$ except the fact that their trace is $0$. How can I prove it? Any other approach or hint to solve that problem will also help me. Thanks for reading!","['linear-algebra', 'linear-transformations']"
2854675,Trigonometry and quadratics : Possible mismatch?,"There’s this problem I came across, gives me an invalid answer by using general quadratic formula. Wonder why? $2\sin^2{x} -5\cos{x} -4 =0 $ Here’s what I did: $2\sin^2{x} -5\cos{x} -4 =0 $ $2(1-\cos^2{x}) - 5 \cos{x} - 4 = 0$ $2 \cos^2{x} + 5 \cos{x} + 2 =0$ This is a quadratic function of $\cos x$, thus, $\cos{x} = (-5 +  3)/4$ or $\cos{x}= (-5 - 3)/4$ But, the answer given is $\cos{x}=-\frac{1}{2} $ and WolframAlpha says the same but doesn’t show how to do it. What did I do wrong? Update: Thank you very much, everyone. Turns out that I wrote the squareroot of 9 as squareroot of 3. My bad","['trigonometry', 'quadratics']"
2854676,"If $f$ derivable on $[a,b]$ does $\int_a^t f'(x)dx=f(t)-f(a)$ true?","Let $f:[a,b]\longrightarrow \mathbb R$ a derivable function. Is it true that for all $t\in [a,b]$ we have that $$f(t)=f(a)+\int_a^t f'(x)dx \ \ ?$$ The thing is since $f'$ is not supposed continuous, there is no reason for me for $f'$ to be Riemann integrable. So my questions are the followings one : Q1) In Riemann sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q2) If we assume $f'$ Riemann integrable, is the formula correct (in Riemann sense). If no, do you have a counter example ? Q3) In Lebesgue sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q4) If we assume $f'$ Lebesgue integrable, is the formula correct (in Lebesgue sense). If no, do you have a counter example ?","['real-analysis', 'integration', 'lebesgue-integral', 'riemann-integration']"
2854691,How do you show that $\sum_{n=0}^\infty\frac1{n!(n+2)}=1$?,"I've been trying to understand the result of this sum:
$$\sum_{n=0}^\infty\frac1{n!(n+2)}=\frac12+\frac13+\frac18+\frac1{30}+\frac1{144}+\dots=1$$
Could you show me how to obtain 1 as result?",['sequences-and-series']
2854693,Difficult integration by parts in deriving Euler-Lagrange equations,"I am doing some reading about the calculus of variations and I am finding it really difficult to see how the integrals are being manipulated. I sense it is due to an application of integration by parts (or some multivariable calculus) but I've been staring at this for some time and am not making any progress. In this situation, I should say that $F = F(x,y,y',y'')\in C^3(D)$ for some $D \subseteq \mathbb{R}^4$ and that $\eta \in C^4([a,b])$ is arbitrary, except that it satisfies $\eta(a) = \eta(b) = \eta'(a) = \eta'(b) = 0$. The book I am reading ( Differential and Integral Equations by P.J. Collins, pp 202) says Because we are treating $x,y,y',y''$ as independent variables, I can see what happens to the last two terms inside the first integral - both the $\eta$ and $\eta'$ are integrated whilst the $F_{y''}$ is treated as a constant, explaining why those two terms come up in the first box on the second line. However, I am incredibly stumped what happens after that. In particular, I am not sure how the integral on the second line arises. Is it some application of a product/chain rule-type thing? Any insight into this would help a lot. Thanks!","['multivariable-calculus', 'calculus-of-variations', 'integration', 'euler-lagrange-equation']"
2854694,How is discriminant related to real $x$?,"Question in my text book Solve for range of the function, $$y=\frac{x^2+4x-1}{3x^2+12x +20}$$ Text book says, cross multiply and express the obtained equation as a quadratic equation in $x$ So I get $ (3y-1)x^2 + (12y -4)x + (20y-1) = 0$ Now it says find discriminant $D$, so we have, $$ D = -4 ( 3y-1)( 8y+3)$$ Now it says, set $D≥0$ as $x$ is real.
Wait what? Isn't $x \in \mathbb{R}$ the domain for a quadratic function? Meaning ""$x$"" is always real? What does a discriminant got anything to do with $x$ being real, when all discriminant tells us is whether or not the ROOTS are real? Help please. To be more specific about my doubt, here's an edit. EDIT : I'm confused, setting discriminant $≥0$ would tell whether or not roots are real, meaning whether the graph of the quadratic function cuts/touches X axis. Now tell me what does this got anything to do with range? As far as I know, quadratic fucntions that don't have real roots are also continous throughout the X axis, meaning there SHOULD always be a corresponding $y-value$","['functions', 'quadratics']"
2854742,Strange form of the chain rule I keep seeing in differential geometry material?,"Since I've started reading up on differential geometry I keep coming across something that's bothering me, which is the chain rule. The standard chain rule from calculus is $$
\frac{df}{dt}(g(t)) = f'(g(t))\cdot \frac{dg}{dt}(t),
$$
which I could also write as
$$
\frac{d f}{dt}(g(t)) = \frac{\partial f}{\partial g(t)}(g(t))\cdot \frac{dg}{dt}(t). \quad \quad (1)
$$ Now when I'm reading these differential geometry texts I keep seeing the following strange approach to the chain rule (I'll stay in 1 dimension for simplicity). The function $g$ is specified explicitly and in the simplest case it could be $g(t) = tx$. Then the chain rule is always given as $$
\begin{align}
\frac{d f}{dt}(tx) = \frac{\partial f}{\partial x}(tx) \frac{d (tx)}{dt}.
\end{align}
$$ Replacing $tx$ by $g(t)$ (so we can compare it with the usual chain rule above) we have
$$
\begin{align}
\frac{d}{dt}f(g(t)) = \frac{\partial f}{\partial \color{red}{ \textbf{x}}}(g(t)) \cdot \frac{dg}{dt}(t), \quad \quad (2)
\end{align}
$$
where I have highlighted the problematic issue. Why have we $x$ in the denominator here instead of $g(t)$ as in the standard chain rule in (1) above? Some places, among many others, where I have seen this are: nLab - Hadamard lemma Second answer in this m.se tread The book Introduction to Manifolds by Loring Tu. So what is going on, is this some 'convention' in which this $x$ actually refers to $g(t) = tx$ or have I misinterpreted something and does the chain rule in (2) somehow agree with the standard chain rule (1)?","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
2854785,Blow up of one point is isomorphic to $\mathbb{P}(\mathcal{O} \oplus \mathcal{O}(1))$,"My question comes from an example in Hartshorne (Example V.2.11.4) which I'm having trouble following. It is claimed that the Blow up of a point $p \in \mathbb{P}^n$ is isomorphic to $\mathbb{P}(\mathcal{O} \oplus \mathcal{O}(1))$. The specific part which I'm having trouble justifying is: On the other hand, if $\mathcal{E} = (\mathcal{O} \oplus \mathcal{O}(1))$ on $\mathbb{P}^n$, then $\mathbb{P}(\mathcal{E})$ is defined as $\operatorname{Proj} S(\mathcal{E})$, where $S(\mathcal{E})$ is the
  symmetric algebra of $\mathcal{E}$. Now $\mathcal{E}$ is generated by the global sections $1$ of $\mathcal{O}$ and $y_1, ... ,y_{n+ 1}$ of $\mathcal{O}(1)$. Therefore $S(\mathcal{E})$ is a quotient of the polynomial algebra $\mathcal{O}[x_0,...,x_{n+ 1}]$ by the mapping $x_0 \to 1$, $x_i \to y_i$ for $i = 1,... ,n + 1.$ The kernel of this map is the ideal generated by all $x_iy_j - x_jy_i$, $i =1, ... ,n + 1$. Therefore $\mathbb{P}(\mathcal{E})$ is isomorphic to the subscheme of $\mathbb{P}^n \times \mathbb{P}^{n+ 1}$ defined by these equations, which is the same as the variety $V \subseteq \mathbb{P}^n \times \mathbb{P}^{n+ 1}$ defined above. Specifically, how does Hartshorne conclude that the kernel of this map is generated by all $x_iy_j - x_jy_i$, $i =1, ... ,n + 1$? This seems to be the crux of my issue, as I am interpreting the $x_i$'s and $y_j$'s as living in two different rings. Thanks in advance.","['birational-geometry', 'algebraic-geometry', 'blowup']"
2854827,Maximal diagonalization of a matrix by permutation matrices,"I found an interesting problem based on a project I'm working on at my job. I'd like to share and see if anyone either knows if it is well-known or if anyone has any algorithms or techniques for approaching it: Let $M$ be an $n \times n$ square matrix. It suffices to consider $M$ to have entries of $0$ and $1$ only and for it to be fairly sparse. Let's further suppose that $M$ is block-diagonal and that the blocks are $B_1, B_2, \cdots, B_k$, where each block matrix $B_i$ is a $m_i \times m_i$ square matrix. We define the ""diagonality"" of $M$ to be the smallest sum $\sum_{i = 1}^k m_i^2$ we can achieve We have to choose the ""smallest"" such value since we usually have some choice with the size of the $B_i$'s. A $2 \times 2$ identity matrix, for instance, can be viewed as having either one $2 \times 2$ block or two $1 \times 1$ blocks, giving scores of $2^2 + 2^2 = 8$ and $1^2 = 1$ respectively. And so we choose $1$ in this case. So a diagonal matrix will always have diagonality $n$ while a matrix of all $1$'s will have a diagonality of $n^2$. The nontrivial upper triangular $2 \times 2$ matrix will have diagonality $4$. Etc. My question is then this: Starting with a fixed matrix $M$, I suppose I am allowed to permute the rows and columns at whim. Equivalently, I am allowed to pre- or post-multiply by any permutation matrix. In doing this, how would I go finding the permutations of the rows and/or columns that minimize the diagonality of the resulting matrix? For instance, let's take the $3 \times 3$ matrix $$
\begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$ This starts off having diagonality $9$. I could permute row $1$ and row $2$ to get: \begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 0 & 1 \\
\end{bmatrix} And then I can permute column $1$ and column $2$ to get: \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1 \\
\end{bmatrix} And the result now has diagonality $5$ rather than $9$. Obviously, the setup here is my own, so I imagine that if this kind of  thing has been explored, it's been explored in greater generality. I'd love to know if anyone can lend me some insight into how I might go about minimizing this quantity.","['matrices', 'discrete-optimization', 'algorithms']"
2854844,Is there a natural category in which the morphisms are derivative operators?,"I'm studying general relativity. As I currently understand the theory, there's a part where we have a (differentiable) manifold $M$, and define a vector field on $M$ to be a function $v : (M \to \Re) \to (M \to \Re)$ satisfying: (linearity) $v(\alpha f + \beta g) = \alpha v(f) + \beta v(g)$ (Leibniz) $v(f \cdot g) = f \cdot v(g) + g \cdot v(f)$ we might also need to further restrict our attention to $v$ that are continuous. We can then show that the set of functions $v$ that satisfy these properties form a vector space, at which point we can generalize from vector fields to tensor fields. (Then, given a metric on $M$ we can derive a natural notion of differentiation on tensor fields, at which point we're ready to state some properties that the spacetime metric and the stress-energy tensor obey.) My question is, is there a (natural) category in which vector fields are just endomorphisms on $(M \to \Re)$? For example, condition (1) above arises automatically if we require that $v$ be an endomorphism of $(M \to \Re)$ in $\Re$-Vect; is there a well-known category such that conditions (1) and (2) arise automatically if we require $v$ to be an endomorphism of $(M \to \Re)$? Obviously I could simply define a category where the objects are vector spaces and the morphisms are linear maps that happen to satisfy the Leibniz rule [EDIT: This is wrong, as pointed out by Eric below -- given two $v$ that satisfy the property above, their composition does not in general satisfy the Leibniz property]; my question is, is this a well-known category (or, is there a simple variation on my question that allows me to see vector fields in a categorical light)? As an example of the genre of question that I'm asking, recall that a vector space over a field $K$ with vectors $V$ can be viewed either as a function $* : K \to V \to V$ satisfying a handful of axioms, or simply as a ring homomorphism between $K$ and the ring of group endomorphisms on $V$. In other words, we can either specify a vector space as a function obeying a bunch of axioms, or we can choose a morphism between the right objects in the right category (in this case, any $\phi : K \to_\text{Ring} (V \to_\text{Group} V)$) at which point the axioms come free. In the case of vector spaces, I could have defined a category in which all morphisms are vector spaces, but I probably wouldn't have noticed that I was trying to ask for a ring homomorphism between the scalar field and the ring of endomorphisms of the vector group. In my question here about a category where morphisms correspond to derivative operators, I'm hoping for an answer analogous to ""you're looking for a ring homomorphism between $K$ and $V \to_\text{Group} V$"".","['category-theory', 'linear-algebra', 'general-relativity', 'vector-spaces']"
2854861,Area of a mushroom-shaped curve,"Inspired by a discussion on this question , I discovered the following hybrid function: The curves in red are defined as $$f(x)=\exp\left((\sin x)^{(\sin x)^{\sin x}}\right)$$ and the curves in blue are defined as $$g(x)=(\sin x)^{\sin x}$$ The result looks like the head of a mushroom (with a bit of decoration :) Question: Consider just one 'mushroom head'. What is the area? We can rewrite the problem as $$\int_0^\pi\left[e^{(\sin x)^{(\sin x)^{\sin x}}}-(\sin x)^{\sin x}\right]\,dx$$ and we can see that it is symmetrical at $x=\pi/2$, since $\sin\left(\frac\pi2-x\right)=\sin\left(\frac\pi2+x\right)$, so this is equivalent to $$2\int_0^{\pi/2}\left[e^{(\sin x)^{(\sin x)^{\sin x}}}-(\sin x)^{\sin x}\right]\,dx\tag{1}$$ Wolfram Alpha calculates this definite integral to be around $3.88407$ (not equal to, as pointed out in the comments. So how should I tackle this integral? I do not anticipate a closed form, hence approximations would be fine. Update: I have approximated the functions into simpler ones, to give a value of $3.86029$.","['recreational-mathematics', 'integration', 'definite-integrals']"
2854900,Constructing perfect set without rationals by removing open neighborhood around rationals,"Let $\{r_n: n\in \Bbb N \}$ be all rationals and $\epsilon_n = \frac 1 {2^n}$. Then is 
  $$
F = \bigcap_{n=1}^\infty \left ( \Bbb R\setminus B(\epsilon_n; r_n) \right )
$$
  perfect? If not how can it be modified so that ($F$) it can be made perfect? This is last part of problem 3.4.9 from Understanding Analysis - Abbott. I am hard time proving or disproving it. Does there exists an enumeration of $\Bbb Q$ which leaves an isolated point under given conditions? My guess is if I take sub-sequence $n_k$ s.t. $r_{n_{k+1}}$ is boundary point (always on right side) of $B(r_{n_k}; 1/2^{n_k})$. Suppose I begin with rational $x$ whose index is under some enumeration is $n_1$, then I must end up at some irrational number $r$.\ Now suppose I begin with rational $y$ whose index is $m_k$ such that $r_{m_{k+1}}$ is boundary point (always on left side) of $B(r_{m_k}; 1/2^{m_k})$ and I end up with same irrational number $r$ as above. Does such $y$ exist, and disjoint sequence $(n_k), (m_k)$ exist?","['general-topology', 'real-analysis']"
2854913,Question of whether two given spaces are homeomorphic.,"Let $D^2$ be the closed disk on the plane. First we pick an arbitrary point $x\in bd(D^2)$ on $D^2$, and define $X = D^2-\{x\}$. Then define another space $Y$ by removing a homeomorphic image of the closed interval $I$ from the boundary , that is, $Y = D^2-h(I)$.(The original question defines $Y$ by removing the upper closed semi-circle of $D^2$ while I think this can be generalized.) I tried to construct the homeomorphism between two spaces but failed. The two spaces are both connected, non-compact and convex so I also failed to prove they are not homeomorphic.",['general-topology']
2854915,Find $\quad\cot^2(2^{\circ})+\cot^2(6^{\circ})+\cot^2({10}^{\circ})++\cdots +\cot^2({86}^{\circ})$,"Find
$$\cot^2(2^{\circ})+\cot^2(6^{\circ})+\cot^2({10}^{\circ})++\cdots +\cot^2({86}^{\circ})$$ $\mathbf {My Attempt}$ I tried to write the sum backward like this $$S=\sum_{n=1}^{22} \cot^2(4n-2) = \sum_{n=1}^{22} \cot^2({90}^{\circ}-4n) = \sum_{n=1}^{22} \tan^2(4n)$$
$S=990$ But still can't find any good trigonometry identity to form telescopic series or something like that. Any hint?","['summation', 'trigonometry']"
2854929,$f_n\to f$ then $\phi(x)=Ce^x$?,"Let $f(x)\in C_\infty(\mathbb{R})$ and a succession of derivatives $f^{(n)}(x)$ converges in $C_{[a,b]}$ to a function $\phi(x)$ in each finite interval. Show that $\phi(x)=Ce^x$, where $C$ is a constant. I know that $Ce^x\in C_{\infty}$ but I cannot prove  $\phi(x)=Ce^x$. I think the $\max|f_n-f|=\max|{Ce^x-Ce^x}|=0 $ for $n>N\in\mathbb{N}$. And I know that $Ce^x\in C_[a,b]$, once it is continuous on the $\mathbb{R}$. Question: 1) How do I prove $\phi(x)=Ce^x$? And not for example $\sin (x)$? 2) What is intended on this question?","['functional-analysis', 'real-analysis']"
2854965,operator concavity of a function involving trace and logarithm,"I want to see if 
$$f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$$
is operator concave with respect to a Hermitian positive definite matrices $A$?
 $\log$ is matrix logarithm,  $B$ and $C$ are arbitrary  Hermitian positive definite matrices, and $I$ is unit matrix. I checked it numerically with many randomly generated positive definite matrices, using the following condition from Bhatia's matrix analysis:
$$f\left({\frac{X+Y}{2}}\right)>\frac{f(X)}{2}+\frac{f(Y)}{2}$$
and it seems that the condition is satisfied. I want to prove it using the same condition. I am trying to show:
$$ \operatorname{trace}\left(C\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right)>\frac{\operatorname{trace}\left(C\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)}{2}, (1)$$ Update: First, I solve it for special case $C=I$ (i.e. $C$ is identity matrix). Using $\operatorname{trace}(\log x)=\log \det(x)$, left  side of (1) is,
$$\operatorname{trace}\left(\log\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right)
\\=\log\left(\det\left(I+\sqrt{\frac{X+Y}{2}}B\sqrt{\frac{X+Y}{2}}\right)\right)
\\=\log\left(\det\left(I+{\frac{X+Y}{2}}B\right)\right), (2)$$
Third line is from Sylvester's determinant identity( https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity ). Right  side of (1) becomes
$$\frac{1}{2}\operatorname{trace}\left(\left(\log(I+\sqrt{X}B\sqrt{X})+\log(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\
=\frac{1}{2}\left(\log\left(\det(I+\sqrt{X}B\sqrt{X})\right)+\log\left(\det(I+\sqrt{Y}B\sqrt{Y})\right)\right)\\
=\frac{1}{2}\left(\log\left(\det(I+XB)\right)+\log\left(\det(I+YB)\right)\right), (3)$$ From concavity of log-determinant ( Log-Determinant Concavity Proof ), I conclude (2) is greater than(3)  and thus the condition (1) is satisfied for $C=I$. I have two questions: 1) Is my conclusion correct?(I am not 100% sure) 2)How can I extend this result (if it is correct) to a general positive definite $C$? Update 2: I also tried the approach suggested in the answer to this question: Is the trace of inverse matrix convex? ...but the second derivative was too complicated. Any help is very appreciated. If you are aware of any other way to prove concavity of $f(A)= \operatorname{trace}(C\log(I+\sqrt{A}B\sqrt{A}))$ please let me know.","['matrices', 'positive-definite', 'convex-analysis', 'trace', 'linear-algebra']"
2854981,"For analytic functions, does existence of $\lim_{z \to 0} f(z)$ imply $\lim_{z \to 0} z f'(z)=0$?","I asked a similar question here but only received a response within the context of real analysis.  Since I am mainly interested in the context of complex analysis, I am posting a modified version here. Suppose we have a function $f$ that is analytic in some region $R$, and the point $z=0$ may be on the boundary $\partial R$.  It seems intuitive to me that if $\lim_{z \to 0} f(z)$ exists, then $\lim_{z \to 0} z f'(z) = 0$. Here is an argument for this that would convince a typical physicist like myself.  $f(z)$ cannot have an essential singularity at $0$, because  $\lim_{z \to 0} f(z)$ would not exist.  So as $z \to 0$, it scales like $f(z) = c + O( z^\alpha )$ for some constants $c$ and $\alpha$.  Moreover, $\alpha > 0$ since the limit exists.  Therefore, $z f'(z) = O( z^\alpha ) \to 0$. Is this true?  If so, is there a simpler or more elegant proof?  If not, what additional assumptions would be needed? Edit: The case I am most interested in is where $R$ satisfies the necessary conditions for the singularity classification theorem (see, for example, the Wikipedia page for an essential singularity).  That is, assume that for every open neighborhood $N$ of $z=0$, $R \cap N$ is non-empty. Edit: Sangchul Lee's response shows that this is not sufficient, but as I noted in my comment to his answer, intuitively, the counterexample suffers from defining the function on a very artificial domain that requires $z=0$ to be approached basically along the $x$-axis.  If the function is analytically continued to a natural domain of some kind (so that essential singularities can be seen for what they are), is the result true?","['complex-analysis', 'limits']"
2855013,Proof of product rule $(fg)^{(n)}$,"I went through induction proofs and they are nice. I'm just looking for an alternative. To give some context, kindly consider the following example. To
  expand $(x+a)^n$ we think of it as a combinatorics problem :
  $$(x+a)(x+a)\cdots (x+a)$$ To get $x$ term we need to
  choose $x$ from any one product and $a$ from the rest. Thus the $x$
  term would be $\binom{n}{1}xa^{n-1}$ To get the $x^2$ term
  we need to choose $x$ from any two products and $a$ from the rest:
  $\binom{n}{2}x^2a^{n-2}$ I'm wondering if the product rule can be seen using combinatorics $$\begin{align}
(fg)^{'} &=f'g+fg'\\
(fg)^{''}&=(f'g+fg')^{'} = f''g+2f'g'+fg''
\end{align}$$
This looks almost same as the earlier problem of expanding $(x+a)^n$. I'm pretty sure these two problems are identical, but I'm not able to make the connection. Any help ?","['derivatives', 'combinatorics', 'calculus']"
2855044,Why is the norm of a matrix larger than its eigenvalue?,"I know there are different definitions of Matrix Norm, but I want to use the definition on WolframMathWorld , and Wikipedia also gives a similar definition. The definition states as below: Given a square complex or real $n\times n$ matrix $A$ , a matrix norm $\|A\|$ is a nonnegative number associated with $A$ having the properties 1. $\|A\|>0$ when $A\neq0$ and $\|A\|=0$ iff $A=0$ , 2. $\|kA\|=|k|\|A\|$ for any scalar $k$ , 3. $\|A+B\|\leq\|A\|+\|B\|$ , for $n \times n$ matrix $B$ 4. $\|AB\|\leq\|A\|\|B\|$ . Then, as the website states, we have $\|A\|\geq|\lambda|$ , here $\lambda$ is an eigenvalue of $A$ . I don't know how to prove it, by using just these four properties.","['matrices', 'normed-spaces', 'eigenvalues-eigenvectors', 'linear-algebra']"
2855047,Prove that $\sum\limits_{i=1}^{n} a_i\geq n^2$.,"A hint can be helpful, but not a whole solution. The Problem (conjecture): Given a natural number $n \geq 1$ and a sequence of natural numbers $(a_i)_{1 \leq i \leq n}$ in which for every pair $(i,j)$ with $i \neq j,$ we have 
  $$\gcd(a_i,a_j)\nmid i-j$$
  prove that $\sum\limits_{i=1}^{n} a_i\geq n^2$. What I have done: During my research, I ran into this problem and I am not quite sure if it is true. It is clear if we put $a_i=n$ then the problem will be solved and the summation will be equal to $n^2$. 
  I tried to solve this problem. For example, I showed that $$a_i> max(i,n-i)$$ otherwise, I can put $j=i+a_i$ or $j=i-a_i$ and considering the fact $\gcd(a_i,a_j) \mid a_i,$ then we conclude that 
  $\gcd(a_i,a_j)\mid i-j,$
  hence, $a_i> max(i,n-i)$ which means that $a_i\geq \dfrac{n} {2}$. Moreover,  if $a_i\leq n$ and p are prime divisors of $a_i$ by putting $j=i-\dfrac {a_i}{p}$ for $i\geq \dfrac {n} {2}$ and $j=i+\dfrac {a_i}{p}$ for $i\leq \dfrac {n} {2}$ we could conclude that $a_i \mid a_j$. I could go further, but it is not enough to prove the conjecture. I also tried Induction and considered that the property holds for every $n\leq k$ and then try to prove the theorem for $n= k+1$ but again, there are some issues that I could not go further.","['number-theory', 'conjectures', 'discrete-mathematics']"
2855050,Order in a Biased Coin [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Situation: Consider the classic coin tossing experiment. We want to explore if the coin is biased. 
Coin 1: Coin is tossed $50$ times. We get $20$T and then $30$H, in that sequence
Coin 2: Coin is tossed $50$ times. We get $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H in that sequence. Q1 : Is the probability of Coin 1 and Coin 2 being biased is the same? My gut feel is that yes, because each event is independent so the order of the events doesn't matter at all. Q2:  I have a coin toss where observations are not independent. P(H | Previous toss is tail) = $0.6$ and P(T | Previous Head) = $0.5$  What kind of statistical test can I use to check the probability of coin being biased?",['probability']
2855057,Representation of rationals as finite continued fractions with restricted coefficients,"This question and its answer incidentally show that every non-zero rational number $q$ can be written as a finite generalised continued fraction of the form: $$  \dfrac{2^{n_0}}{1- \dfrac{2^{n_1}}{1 - \dfrac{2^{n_2}}{1- \dfrac{2^{n_3}}{\ddots\dfrac{\ddots}{1-2^{n_{r(q)}}}}}}}$$ with $n_i \in \Bbb N$ and $r(q) \ge 1$ depending on $q$ . For example: $$ \frac{14}{9} = \dfrac2{1- \dfrac2{1 -  8}} $$ and $$\frac{9}{5} = \dfrac1{1- \dfrac4{1 -  \dfrac8{1- 2}}} $$ and $$-17 = \dfrac1{1- \dfrac2{1 -  \dfrac8{1- \dfrac2{1-\dfrac4{1-\dfrac4{1-2}}}}}}$$ (How to get the powers of $2$ : Let $v_2$ be the $2$ -adic valuation , then if $v_2(q) \ge 0$ , set $n_i = v_2(f^i(q))$ ; if $v_2(q) <0$ , set $n_0=0, n_{i+1}= v_2(f^i\circ g(q))$ where $f(x)= 1-\frac{1}{x|x|_2}$ and $g(x) = 1-\frac{1}{x}$ are the functions in that question resp. answer.) Now, if we generalise from $2$ to an arbitrary natural $k > 1$ (in particular prime $k$ , but why restrict to that), I wondered if it is also true that: For every $ q \in \Bbb Q^*$ there is a finite tuple $(n_0, ..., n_{r(q)})$ such that $$  q = \dfrac{k^{n_0}}{1- \dfrac{k^{n_1}}{1 - \dfrac{k^{n_2}}{1- \dfrac{k^{n_3}}{\ddots\dfrac{\ddots}{1-k^{n_{r(q)}}}}}}}$$ or something similar/more general (what if the "" $1$ ""'s are replaced by another natural number etc). If not, which rationals can be written that way? Also, as I basically know nothing about continued fractions and made up an ad hoc approach for the other question, so: Are there standard tools for handling this kind of question from the theory of continued fractions? Edit : After playing around a bit, it seems more natural to me (albeit I've neither proven sufficiency nor necessity) that for general $k$ , one would allow a set of representatives modulo $k$ instead of the leading "" $1$ ""'s, i.e. $$  q = \dfrac{k^{n_0}}{b_1- \dfrac{k^{n_1}}{b_2 - \dfrac{k^{n_2}}{b_3- \dfrac{k^{n_3}}{\ddots\dfrac{\ddots}{b_{n_q}-k^{n_{r(q)}}}}}}}$$ with the $b_i \in \{1,2, ..., k-1\}$ . This would make a characterisation of the ones where all $b_i=1$ more subtle.","['number-theory', 'rational-numbers', 'continued-fractions']"
2855117,Is there any subgroup between $A_n$ and $A_{n+1}$,"Let $A_n$ be the alternating group of $n$ elements. Is there any subgroup $H$ of $A_{n+1}$ such that $A_n \subsetneq H \subsetneq A_{n+1}$ for $n \geq 5$ ? Actually, I'm working on the simplicity of $A_n$ for $n \geq 5$. And I find that it may help to consider this question. Thanks for your help.","['abstract-algebra', 'group-theory', 'symmetric-groups']"
2855144,Asymptotic approximation for $\epsilon^2 y'' = axy$,"I would like to find the first two terms of the asymptotic approximation for $\epsilon^2 y'' = axy$ on $0\leq x <
 \infty$ where $y(\infty) = 0$ and $a>0$. Here is my work so far, with the standard WKB method, we plug in 
$$y(x) = \exp\left[\frac{1}{\delta} \sum_{n=0}^\infty \delta^n S_n(x)\right]$$ and divide by it; our ODE simplifies to 
$$\epsilon^2\left[\frac{1}{\delta} S_0'' + S_1 + \cdots + \frac{1}{\delta^2} S_0'^2 + 2 \frac{1}{\delta} S_0'S_1' + \cdots \right] -ax = 0 \quad \quad (\star)$$
Our largest term $\frac{\epsilon^2}{\delta^2} S_0'^2 $must match with $-ax$, so we make $\delta = \epsilon$, and get 
$$S_0'^2 \sim ax$$
thus 
$$S_0 \sim \pm \sqrt{a} \frac{2}{3} x^{3/2}.$$
And by matching $O(\epsilon)$ terms in $(\star)$, we get 
$$2S_0'S_1' + S_0'' = 0$$
and we get $S_1 \sim - \frac{1}{4} \log(ax)$. So our approximation 
$$y (x)\sim C_1 (ax)^{-1/4} \exp \left[\frac{1}{\epsilon} \sqrt{a} \frac{2}{3} x^{3/2}\right] + C_2 (ax)^{-1/4} \exp \left[-\frac{1}{\epsilon} \sqrt{a} \frac{2}{3} x^{3/2}\right]$$
And from our condition $y(\infty) = 0$, we get $C_1 = 0$. But the above approximation does not make sense at $x=0$, how can we take care of this problem? 
I know boundary layer theory but often it has problems when the coefficient of $y'$ is constant zero. I also worked out the WKB method for a boundary layer problem here , I am not sure how would I apply to this case. Edit: If I add a boundary layer at $x=0$, we let $\xi = x/\delta$, then the ODE in terms of delta will become 
$$\frac{\epsilon^2}{\delta^2} (y_0'' + \epsilon y_1'' + \cdots ) = a\xi\delta(y_0 + \epsilon y_1, + \cdots)$$
which gives 
$$\frac{\epsilon^2}{\delta^2} y_0'' + \frac{\epsilon^3}{\delta^2} y_1'' + \cdots  - \delta a\xi y_0 + \cdots = 0$$
If I let $\delta = \epsilon$, I will get $y_0 = a\xi + b$, how would I match this with $y(x)$ from WKB method","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
2855146,Evaluate $\int_0^1 \frac{1}{1+x^m}dx$ for $m>0 $,"I came across the sum:
$$I(m)=\sum_{n=0}^\infty \frac{(-1)^n}{mn+1} $$
where $m>0$. It's easy to see that this sum is equal to:
$$\int_0^1 \frac{1}{1+x^m}dx$$ 
for $m>0$ So I tried my hardest to find a way of evaluating it. First I thought using Euler's Beta Function but to no avail. Then I decided to brute force a partial fraction decomposition, and I got this ugly mess (which is more than like incorrect): $$I(m)=\sum_{n=0}^\infty \frac{(-1)^n}{mn+1} = \sum_{k=0}^\infty A_k\ln|1-e^{-(i(2k+1))/m}| $$Where $A_k$ equals $$A_k= \frac{1}{\prod_{n=0}^k(e^{i(2k+1)}-e^{i\pi(2n+1)/m})\prod_{n=k}^\infty(e^{ik\pi/m}-e^{i(2n+3)}) } $$ I would check my work, but it's really tricky to check so I'm just hoping it's right. Is there a way of evaluating the integral that's less ugly than what I did?","['summation', 'integration']"
2855183,Finding a holomorphic function with real part $u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2}$.,"I am asked to find the holomorphic function $f(z)=f(x\pm iy)$ with real part
  $$u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2}$$
  and such that $f(0)=0$. I have tried to apply Cauchy-Riemann equations directly but I don't seem to be getting anywhere. I've also tried writing $u$ in terms of $z$ and $\bar{z}$ and then applying $f'=2\partial_z{u}$, but the expression of the derivative seems unmanagable. Since $z$ and $\bar{z}$ are symmetric in the expression of $u$, I also have the relation $\partial_z{u}=\overline{\partial_{\overline{z}}u}$, which I've tried to plug into $\partial_{\overline{z}}f=0$. But I dont know how to proceed from here.","['complex-analysis', 'analysis']"
2855185,Half Derivative of $\tan(x)$,"I know how to find one of many half derivatives of $\sin(x)$ and $\cos(x)$ which are, $${D^{\frac{1}{2}}}\sin(x)=\sin(x+\frac{\pi}{4})$$ $${D^{\frac{1}{2}}}\cos(x)=\cos(x+\frac{\pi}{4})$$ Using these is it possible to find a half derivative for $\tan(x)$ by letting. $$\tan(x)=\frac{\sin(x)}{\cos(x)}$$ Does the Quotient rule apply for half derivatives as well? For example, $$D^{\frac{1}{2}}\tan(x)=\frac{D^{\frac{1}{2}}[\sin(x)]\cos(x)-D^{\frac{1}{2}}[\cos(x)]\sin(x)}{\cos^2(x)}$$ As a side question, does the Product and Chain rule also apply for half derivatives, or some other form of it?","['derivatives', 'trigonometry', 'calculus']"
2855189,Sum of Squares of Binomial Coefficients Using Residue Theorem,"I ran across this interesting question recently that I have an idea for, but am unable to complete. Basically, we use the residue formula to find $$ \sum\limits_{k=0}^n {n\choose k}^2$$
We define $f$ as\begin{align*}
f(z)&= \frac{1}{z}(1+z)^n(1+\frac{1}{z})^n\\ &= \frac{1}{z}\left({n\choose 0} + {n\choose 1}z + \cdots + {n\choose n} z^n \right)\left({n\choose 0} + {n\choose 1}\frac{1}{z} + \cdots + {n\choose n} \frac{1}{z}^n \right)\\
\end{align*}
We see that the $\frac{1}{z}$ term will have the coefficient $\sum\limits_{k=0}^n {n\choose k}^2$. This would imply that $$res_{z=0} f(z) = \sum\limits_{k=0}^n{n\choose k}^2$$
Here's where things start falling apart for me. I choose to integrate $f$ over the unit disc $D$, which contains the pole $z=0$ and according to the residue theorem, I should get that $$ \int\limits_D f(z)dz = 2\pi i \sum\limits_{k=0}^n{n\choose k}^2$$ But calculating the integral, I get that \begin{align*} 
\int\limits_D f(z)dz &= \int\limits_0^{2\pi} \frac{ie^{i\theta}}{e^{i\theta}}(1+e^{i\theta})^n(1+e^{-i\theta})^nd\theta\\
&= i \int\limits_0^{2\pi} (1+e^{i\theta})^n (1+e^{-i\theta})^nd\theta
\end{align*}
However, Wolfram Alpha has that this integral goes to $0$ but we don't have that $\sum\limits_{k=0}^n {n\choose k}^2 = 0$. Would someone mind pointing out where I messed up in my proof or perhaps point me in a better direction?","['complex-analysis', 'integration', 'binomial-coefficients', 'residue-calculus']"
2855210,Proving the equality case in triangle inequality,"Background When plotted on a real number line, it may be deduced that if 
$$a,b,c \in \mathbb{R} $$
$$a < b < c$$ then 
$$\left| {a - c} \right| = \left| {a - b} \right| + \left| {b - c} \right|$$ Problem But the problem is with the proof. How can the above statement be proven true from the properties of order structure and the definition of absolute value?","['number-theory', 'complex-analysis', 'real-analysis', 'calculus']"
2855251,Multivariable Substitution Rule VS Pullback of Volume Forms,"Let $d\mathbf{x} = dx^1\wedge \cdots \wedge dx^n$ be the volume form on $U\subset\mathbb{R}^n$ and smooth $c : [a_1,b_1]\times\cdots\times [a_n,b_n] \to U$, $\mathbf{t} = (t_1,\dots,t_n) \mapsto \mathbf{x} = \mathbf{c}(\mathbf{t}) = (c^1(t_1,\dots,t_n),\dots,c^n(t_1,\dots,t_n))$. I’ve encountered the following two formulae: Multivariable Substitution Rule : 
$$d\mathbf{x} = d\mathbf{c} = \vert det\left((D\mathbf{c})_\mathbf{t}\right) \vert d\mathbf{t}$$ Pullback of Volume Forms : 
$$c^*(d\mathbf{x}) = d\mathbf{c} = det\left((D\mathbf{c})_\mathbf{t}\right) d\mathbf{t} $$ $(D\mathbf{c})_\mathbf{t}$ is the total derivative of $\mathbf{c}$ at $\mathbf{t}$. My question is, are these two formulae referring to different things? It feels like these two equations are talking about the same thing, but then why does one have an absolute value on the Jacobian determinant? I’m trying to understand this so I can prove that integrals of differential forms only flip signs under a re-parameterisation that reverses orientation.","['differential-forms', 'differential-geometry']"
2855287,Did Euclid prove the formula for the area of a triangle?,"In Proposition 6.23 of Euclid’s Elements, Euclid proves a result which in modern language says that the area of a parallelogram is equal to base times height.  Now Euclid did not have the concept of real numbers at his disposal, so how he phrased the result is, the ratio of the area of one parallelogram to the area of another parallelogram is equal to the ratio of the bases times the ratio of their heights (where multiplication of ratios is defined here ), But my question is, did Euclid ever prove that the area of a triangle is 1/2 base times height?  That is, did he prove that the ratio of the area of a triangle to the area of a parallelogram is equal to one half the ratio of their bases times the ratio of their heights? Euclid’s system is certainly capable of proving it; the result follows pretty directly from Proposition 6.23 along with Proposition 1.41 , which says that the area of a triangle is half the area of a parallelogram with the same base and height.  But did Euclid actually prove this result in the Elements ?","['math-history', 'euclidean-geometry', 'triangles', 'geometry', 'area']"
2855338,"Hour, second and minute hands","I've never found any such problem here on MathSE, so here is the problem: Given that hour, minute and second hands move continuously, how much time in the period from 0:00 to 12:00 (12 hours total) do these three hands are in the same semicircle? I've found this problem so difficult I couldnt even find approximal solutions(","['continuity', 'contest-math', 'circles', 'geometry']"
2855357,Converting a function of $\cos^2$ to a complete elliptic integral of the first kind,"I am having a hard time following Equation 2.6 of Taib, Bachok Bin. ""Boundary integral method applied to cavitation bubble dynamics."" (1985). The equation is on the middle of page 8 of the document or page 17 of the linked pdf file. $$\int_0^{2\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta=4\int_0^{\frac{\pi}2}\frac1{(1-k^2\cos^2(\alpha))^\frac12}\,\mathrm{d}\alpha\tag{1}$$ $$4\int_0^{\frac{\pi}2}\frac1{(1-k^2\cos^2(\alpha))^\frac12}\,\mathrm{d}\alpha=4\int_0^{\frac{\pi}2}\frac1{(1-k^2\sin^2(\alpha))^\frac12}\,\mathrm{d}\alpha\tag{2}$$ I understand how to arrive at the left hand side of equation $(1)$ above, however, I do not see how to convert to $\alpha$ and from $\cos^2$ to $\sin^2$ in that way. First question: What is the substitution from $\theta$ to $\alpha$? I see that the positive periodic nature of $\cos^2$ and $\sin^2$ means that I can break the integral into two parts that are equal. $$\int_0^{2\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta=$$
$$\int_0^{\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta\,+\int_\pi^{2\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta\tag{3}$$ $$\int_0^{\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta\,=\int_{n\pi}^{(n+1)\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta\tag{4}$$ $$\int_0^{2\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta=2\int_0^{\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta\tag{5}$$ \begin{align}
\alpha=\frac{\theta}{2} \\
\frac{\mathrm{d}\alpha}{\mathrm{d}\theta}=\frac{1}{2}
\end{align} $$2\int_0^{\pi}\frac1{(1-k^2\cos^2(\frac{\theta}2))^\frac12}\,\mathrm{d}\theta=$$
$$2\int_0^{\pi}2\frac1{(1-k^2\cos^2(\alpha))^\frac12}\,\mathrm{d}\alpha=4\int_0^{\frac{\pi}2}\frac1{(1-k^2\cos^2(\alpha))^\frac12}\,\mathrm{d}\alpha\tag{6}$$ Second question: How is equation $(2)$ obtained? When I convert from $\cos^2$ to $\sin^2$, I would imagine it as the following: $$\cos^2(\theta)+\sin^2(\theta)=1\tag{7}$$ $$\cos^2(\theta)=1-\sin^2(\theta)\tag{8}$$ Substituting $(8)$ into $(2)$: $$4\int_0^{\frac{\pi}2}\frac1{(1-k^2\cos^2(\alpha))^\frac12}\,\mathrm{d}\alpha=4\int_0^{\frac{\pi}2}\frac1{(1-k^2(1-\sin^2(\alpha)))^\frac12}\,\mathrm{d}\alpha\tag{9}$$","['elliptic-functions', 'trigonometry', 'integration', 'elliptic-integrals', 'trigonometric-integrals']"
2855381,Problems with Recurrence Relations as a form of Counting,"I have been having trouble trying to understand how to do the following problem Solve by unfolding: $a_0=3$, and for $n\geq1$, $a_n=5a_{n-1}+3$. Hint: This will involve the geometric sum formula. This is my work so far:
$$a_n=5a_{n-1}+3$$
$$a_n=5(5a_{n-2}+3)+3$$
$$a_n=(5(5(5a_{n-3}+3)+3)+3)$$
$$a_n=5^{n}*a_0+5^{n-1}*3+5^{n-2}*3+...+5*3+3$$
I am not sure if this is right, or how to really do this problem. Help, and hints would be much appreciated.","['recurrence-relations', 'discrete-mathematics']"
2855383,About the closedness of $\frac d{dx}$ operator,"The example section of the closed linear operator in the wikipedia page https://en.wikipedia.org/wiki/Unbounded_operator#Closed_linear_operators says that (1) ""Consider the derivative operator $A =\frac d
{dx}$
 where $X = Y = C([a, b])$ is the Banach space of all continuous functions on an interval $[a, b]$. If one takes its domain $D(A)$ to be $C^1([a, b])$, then $A$ is a closed operator, which is not bounded."" (2) ""On the other hand if $D(A) = C^\infty([a, b])$, then $A$ will no longer be closed, but it will be closable, with the closure being its extension defined on $C^1([a, b])$."" The first claim seems easy to understand. Since the limit of any sequence of continuously differentiable functions $f_n$ if converges  would be a continuously differentiable function $f$ and $\frac d {dx} f$ is a continuous function $\in C[a,b]$. However the second claim seems not to be easy to understand. Can anyone give a more rigorously proof to this two statements clearly? Thanks.","['functional-analysis', 'banach-spaces', 'differential-operators']"
