question_id,title,body,tags
1383554,Finding the expected value and variance of ${X^3}$,"For a random variable $X$, $(X^3-1)$ is uniformly distributed in the interval $[0,7]$ I need to find the expected value and variance of $\color{blue}{X^3}$ and I know that: cumulative distribution function:
$$F_{X}(x)=
\begin{cases}
0&;x < 1\\
\frac{x^3-1}{7}&;1\leq x \leq 2\\
1&;x>2\\
\end{cases}$$ probability density function: $$f_{X}(t)=
\begin{cases}
0&;t \not\in (1,2)\\
\frac{3x^2}{7}&; t \in (1,2)\\
\end{cases}$$ My attempt: $X^3-1 \sim U(0,7)$ $X^3 \sim U(1,8)$ $E(X^3)=\frac{1+8}{2}=4.5$ $\text{Var}(X^3)=\frac{(b-a)^2}{12}=\frac{(8-1)^2}{12}\approx4.083$ Is it correct?","['statistics', 'proof-verification']"
1383567,Behaviour of $\zeta(s)$ near $s=1$,"I would appreciate if somebody could run this over and see if it works out? any suggestions or pointers would be appreciated. I denote the standard eta function $\eta$ by $\zeta^{*}$. I have not used big O notation and just used general well behaved functions. I do not wish to express the full error term, but instead ,just the principal part. Behaviour of $\zeta(s)$ near $1$ From Abel's Theorem we can see that when $s=1$, $ \zeta^{*}(1) = \log(2)$. Now looking at $(1-2^{1-s})$ we can write it in terms of an exponential like so,
\begin{equation}
1-2^{1-s} = 1 - e^{(1-s)\log(2)}
\end{equation} The power series expansion of $e^{z}$ is, \begin{equation}
e^{z}= \sum_{n=0}^{\infty} \frac{z^{n}}{n!}\\
\Rightarrow  1-2^{1-s} = - e^{\log(2)(s-1)}=  - \sum_{n=0}^{\infty} \frac{((1-s)\log(2))^{n}}{n!}
\end{equation} We can ignore the term when $n=0$ due to it being zero and sum from $n=1$ instead, \begin{equation}
1-2^{1-s} = 0 - \sum_{n=1}^{\infty} \frac{(1-s)^{n}\log(2)^{n}}{n!}
\end{equation} Expanding this sum and multiplying in the negative sign we have, \begin{equation}
1-2^{1-s}= (s-1) \Bigg( \log(2) - \frac{\log(2)^{2}}{2!}(s-1) + \cdot \cdot \cdot \Bigg )
\end{equation} Factorizing  the $\log(2)$ term out,
\begin{equation*}
(s-1)\log(2)\Bigg [ 1 - \bigg( \frac{\log(2)}{2!}(s-1) + \frac{\log(2)}{3!}(s-1)^{2} - \cdot \cdot \cdot  \bigg ) \Bigg ]
\end{equation*} By the geometric series formula,  for $|s| < 1$, 
\begin{equation}
\frac{1}{\bigg[1 -  \bigg(  \frac{\log(2)}{2!}(s-1) + \cdot \cdot \cdot \bigg ) \bigg ] }= 1 + \Bigg( \frac{\log(2)}{2!}(s-1) + \cdot \cdot \cdot \Bigg ) +  \Bigg( \frac{\log(2)}{2!}(s-1) + \cdot \cdot \cdot \Bigg )^{2} +  \cdot \cdot \cdot 
\end{equation}
The terms of this geometric series decrease rapidly, so we are only  interested in keeping the first terms while letting a well-behaved and analytic function $g$ represent the remaining terms as a function in $s$. \begin{equation}
\frac{1}{\bigg[1 -  \bigg(  \frac{\log(2)}{2!}(s-1) + \cdot \cdot \cdot \bigg ) \bigg ] } = 1 + \frac{\log(2)(s-1)}{2} + (s-1)^{2}\cdot g(s).
\end{equation} We can now return to  $\frac{1}{1-2^{1-s}}$, and express  it in terms of what we have learned.
\begin{equation}
\frac{1}{1-2^{1-s}} = \frac{1}{\log(2)(s-1)} \Bigg(  1 + \frac{\log(2)(s-1)}{2} + (s-1)^{2}\cdot g(s) \Bigg ) 
= \frac{1}{\log(2)} \cdot \Bigg [ \frac{1}{s-1} + \frac{\log(2)}{2} + (s-1)g(s)\Bigg ]
\end{equation} We can now study $\zeta(s)$ when $s$ is near to $1$. 
\begin{equation}
\zeta(s) = \frac{\zeta^{*}(s)}{1-2^{1-s}} =  \frac{\zeta^{*}(s)}{\log(2)} \cdot \Bigg [ \frac{1}{s-1} + \frac{\log(2)}{2} + (s-1)g(s)\Bigg ]
= \frac{\zeta^{*}(s)}{\log(2)} \cdot \frac{1}{s-1} + \frac{\zeta^{*}(s)}{2 \log(2)} \log(2) + \frac{\zeta^{*}(s)(s-1)g(s)}{\log(2)}
\end{equation} As we know already, $\zeta^{*}(1) = \log(2)$ is analytic, so $\zeta^{*}(s)$ can be expanded as a series around $1$,
\begin{equation}
\zeta^{*}(s) = \log(2) + (s-1)a_1 + (s-1)^{2}a_2 +  \cdot \cdot \cdot
= \log(2) + (s-1) h(s)
\end{equation}
for a well behaved and analytic $h$. Near $s=1$ and by just looking at the principal terms,
\begin{equation}
\zeta(s) = \frac{\zeta^{*}(s)}{1-2^{1-s}} = \frac{ \log(2) +(s-1)h(s) }{\log(2)(s-1)}
= \frac{1}{s-1} + \frac{h(s)}{\log(2)}
\end{equation}","['riemann-zeta', 'complex-analysis', 'analytic-number-theory']"
1383595,Is it possible to develop Analysis solely from Peano's axioms,"...and a few definitions on the way? When I studied Calculus using Spivak's book It was clearly shown
that, in order to prove some fundamental theorems (intermediate value theorem being one of them),
one had to assume an additional property of numbers (namely the least upper bound property).
Other postulated properties included the field axioms and 3 more axioms (needed for the inequalities). But then I skimmed over ""Grundlagen der Analysis"" by Landau (which was in the recommended reading by the way)
and he developed the reals using Dedekind cuts assuming only Peano's axioms - the way I saw it.
I also read on Wikipedia that the set of Dedekind cuts has the least upper bound property.
So it seems possible that we can get from Peano's axioms all the way to theorems in analysis 
(or am I missing some additional assumptions?) If it is true that Peano's axioms are all we need, then I want to know how far we can take them...in general (not just real analysis).
Also is there an elegant way to prove the field axioms for real numbers from Peano's axioms?","['analysis', 'peano-axioms', 'calculus']"
1383611,Equilibrium Points Second Order Differential,Attempt: I get the system of the two first order equations (first order in $w$) by considering the different signs the first derivative takes. Problem is by equilibrium points: do I just set the first derivative to 0 in the (*) equation or do I get them from the two first-order equations? I just need someone to tell me how to obtain the equilibrium points. Thanks.,"['calculus', 'ordinary-differential-equations']"
1383662,Rearranging Pokemon Experience Formula to make Level the Subject,"As the title suggests, I am trying to rearrange some of the formulas for calculating experience based on level to be the other way around (calculating level based on experience). I am having trouble with rearranging this formula for n (n being level): $$EXP = \frac{n^3(100-n)}{50} $$ What I have done so far is: multiplied the 50 out $${EXP}\times{50} = {n^3(100-n)} $$ expanded the brackets
$${EXP} \times {50} = 100n^3 - n^4 $$ But I don't know how to continue from here to make n the subject so that I can calculate the Level based on Experience . Do I divide the 100 out, if so, would that affect $n^4$? Note: The links to the original formulas can be found here .",['calculus']
1383676,$S_6$ contains two subgroups that are isomorphic to $S_5$ but are not conjugate to each other,"This is a problem from Ph.D. Qualifying Exams. Show that the symmetric group $S_6$ contains two subgroups that are isomorphic to $S_5$ but are not conjugate to each other. Here is my method.  $S_5$ contains 6 Sylow 5-subgroup, and $S_5$ act by  conjugation on the 6 groups transitively by Sylow's 2nd Theorem, therefore induces a homomorphism $\phi: S_5 \to S_6$. Now, if $ker\phi$ is trvial, then $Im\phi$ is a subgroup of $S_6$ isomorphic to $S_5$.  Since $Im\phi$ is a transitive subgroup, it is not conjugate to the subgroup which permutes 5 letters fixing 1 letter, i.e., the subgroup $Sym\{1,2,3,4,5\}\cong S_5$. So what we need to do is to show $ker\phi$ is trvial. Taken one of 6 Sylow 5-subgroups $H$, there are 6 conjugation orbits, so  $N_G(H)$, the normalizer of $H$ has order $20=\frac{120}{6}$ by counting formula. Here I stopped. I know if I can show the intersection of the 6 normalizers contains only identity element, it will be done, but how can I proceed?","['sylow-theory', 'group-theory', 'permutations']"
1383694,Why would we a priori expect $V(I)$ to satisfy axioms to define the closed sets for a topology on $\text{Proj}(S)$?,"The topological space $\text{Proj}(S)$ has the underlying set$$\text{Proj}(S) = \{\mathfrak{p} \text{ a homogeneous prime such that }S_+ \not\subseteq \mathfrak{p}\},$$and the closed sets are the loci $V(I) = \{\mathfrak{p} \in \text{Proj}(S) : I \subseteq \mathfrak{p}\}$ for homogeneous ideals $I$ of $S$, where $S$ is an $\mathbf{N}$-graded ring. Perhaps this question is a bit silly since its such a straightforward check, but why would we a priori, intuitively and/or geometrically, expect such $V(I)$'s to satisfy the axioms to define the closed sets for a topology on $\text{Proj}(S)$? Edit: The comment by Hoot mentions schemes. Could I have an explanation that tries to avoid schemes if possible? Edit: Someone typed out a perfectly good response only to have deleted it? Why?","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
1383721,When does this equation $\cos(\alpha + \beta) = \cos(\alpha) + \cos(\beta)$ hold?,"I come across this problem in an advanced maths textbook for grade 11 in my country. And it's marked a star, which means that it's a difficult exercise, and so, no solution for this problem is given. I can solve problems asking for which conditions do $\sin(\alpha + \beta) = \sin(\alpha) + \sin(\beta)$, and $\tan(\alpha + \beta) = \tan(\alpha) + \tan(\beta)$ hold. They are pretty easy, and straight-forward. But for this problem ($\cos(\alpha + \beta) = \cos(\alpha) + \cos(\beta)$), I have tried using all kinds of formulas, from Sum of Angles , to Sum to Product , and Double Angles , but without any luck. So, I think there should be some glitch here that I haven't been able to spot it out. So I hope you guys can give me some hints, or just a little push as a start. Any help would be greatly appreciated, Thank you very much, And have a good day, :D",['trigonometry']
1383722,Diffeomorphism between Euclidean space,"How does one show that if $f:U\rightarrow V$ is a diffeomorphism between open sets $U\subset\mathbb{R}^m$ and $V\subset\mathbb{R}^n$ then $m=n$? Here is some working: For $u\in U$ let $v=f(u)\in V$.  The Jacobi matrices $J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^n$ and $J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^m$.  We have $f\circ f^{-1}=\text{id}_V$ and $f^{-1}\circ f=\text{id}_U$.  The chain rule yields $I_n=J_{\text{id}_V}(v)=J_f(u)\circ J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^n$, the identity, and $I_m=J_{\text{id}_U}(u)=J_{f^{-1}}(v)\circ J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^m$, again the identity. By definition this means that $J_f(u)$ is invertible with inverse $J_{f^{-1}}(v)$, so we must have $m=n$.","['differential-geometry', 'smooth-manifolds', 'multivariable-calculus']"
1383725,What is the difference between orthogonal and orthonormal in terms of vectors and vector space?,I am beginner to linear algebra. I want to know detailed explanation of what is the difference between these two and geometrically how these two are interpreted?,"['orthogonality', 'linear-algebra', 'matrices']"
1383727,Number of left cosets equals number of right cosets?,"So I've been working on abstract algebra out of John B. Fraleigh's 3rd edition text. In the exercises of chapter 11, I came upon a question which I cannot even begin to solve. ""Show that there are the same number of left as right cosets of a subgroup H of a group G, that is, exhibit a one-to-one map of the collection of left cosets onto the collection of right cosets. (Note that this result is obvious by counting for finite groups. Your proof must hold for any group.)"" The only idea that I had was using a map $\phi : coset_{left} \rightarrow coset_{right}$ by $aH\phi = Ha$, but this seems far too easy. What thought process is wrong here? And how is this accomplished?","['abstract-algebra', 'infinite-groups']"
1383730,Rearrangements that never change the value of a sum,"Which bijections $f:\{1,2,3,\ldots\}\to\{1,2,3,\ldots\}$ have the property that for every sequence $\{a_n\}_{n=1}^\infty$,
$$
\lim_{n\to\infty} \sum_{k=1}^n a_k = \lim_{n\to\infty} \sum_{k=1}^n a_{f(k)},
$$
where ""$=$"" is construed as meaning that if either limit exists then so does the other and in that case then they are equal? It is clear that there are uncountably many of these. Might it just be that $\{f(n)/n : n=1,2,3,\ldots\}$ is bounded away from both $0$ and $\infty$? These bijections form a group.  Can anything of interest be said about them as a group? PS: Here's another moderately wild guess (the one above appears to be wrong): Might it be just the bijections whose every orbit is finite?","['sequences-and-series', 'conditional-convergence', 'permutations']"
1383741,Is this determinant always non-negative?,"For any $(a_1,a_2,\cdots,a_n)\in\mathbb{R}^n$ , a matrix $A$ is defined by $$A_{ij}=\frac1{1+|a_i-a_j|}$$ Is $\det(A)$ always non-negative? I did some numerical test and it seems to be true, but I have no idea how to prove it. Thanks!","['determinant', 'linear-algebra', 'matrices']"
1383747,"Cauchy-Schwarz Inequality without using $\langle a x,y\rangle=a\langle x,y\rangle$","Let $V$ be a vector space and define a function $\langle .,.\rangle:V\times V\to\mathbb{C}$ such that $$\begin{align}
& \langle x,y\rangle=\overline{\langle y,x\rangle }\,\,\,\forall x,y\in V\\
& \langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle
\,\,\,\,\,\forall x,y,z\in V\\ 
& \langle x,x\rangle\ge0\,\,\,\,\,\forall x\in V\,\,\,\text{and equality holds iff}\,\,\,\ x=0\\ 
\end{align}$$ Does Cauchy-Schwarz inequality $$\color{Green}{|\langle x,y\rangle|^2\le\langle x,x\rangle\langle y,y\rangle}$$still vallied? (Without the condition $\langle a x,y\rangle=a\langle x,y\rangle,\,\,\,\forall a\in\mathbb{C},\,\,\,\forall x,y\in V.$) If the answer is ""NO"", can we prove it?","['vector-spaces', 'inner-products', 'real-analysis', 'linear-algebra', 'inequality']"
1383755,Cardinality of the Cartesian Product of Two Equinumerous Infinite Sets,Is the cardinality of the Cartesian product of two equinumerous infinite sets the same as the cardinality of any one of the sets? I couldn't find this explicitly stated in any handout or text. This certainly seems to be true from the examples I have seen: The Cartesian product of two infinitely countable sets is again infinitely countable. The Cartesian product of two sets with cardinality of continuum again has cardinality of continuum.,['elementary-set-theory']
1383760,Is there any neat way to show $\phi$ is a homomorphism?,"In Michael Artin's Algebra (chapter 2, page 50, example 2.5.13) the author illustrates a homomorphism from $S_4$ (all permutations of indices $(1,2,3,4)$) to $S_3$ (all permutations of indices $(1,2,3)$), as is shown in the picture below: I understand how the map sends an element from $S_4$ into $S_3$, but I just don't understand why this is a homomorphism. The author remarks that If $p,q$ are elements of $S_4$, the product $pq$ is the composed permutation $p\circ q$, and the action of $pq$ on the set $\{\Pi_1,\Pi_2,\Pi_3\}$ is the composition of the actions of $q$ and $p$. Therefore $\phi(pq)=\phi(p)\phi(q)$. But it isn't clear to me how $\phi(p),\phi(q)$ can be so ""composed""... It might not be hard to imagine, but how to sketch a rigorous proof? Of course there are finite cases so we can always do it case by case, but I'm wondering if there is a more elegant way to do this?","['permutations', 'group-homomorphism', 'abstract-algebra', 'group-theory', 'symmetric-groups']"
1383770,"Show that $\lim_{t\to \infty}u(x,t)=\frac{A+B}{2}$, for each $x\in\Bbb R$.","Let $u(x,t)$ be a $C^2$ bounded solution of $$u_t(x,t)-u_{xx}(x,t)=0,x\in \Bbb R, u(x,0)=f(x)$$ where $f\in C(\Bbb R)$ satisfies: $\lim_{x\to+\infty}f(x)=A,\lim_{x\to-\infty}f(x)=B$. Show that $\lim_{t\to \infty}u(x,t)=\frac{A+B}{2}$, for each $x\in\Bbb R$. My attempt: I try to use the integral representation for the solution of heat equation (here is one dimensional $n=1$) $$u(x,t)=\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$$ Then $\lim_{t\to\infty}u(x,t)=\lim_{t\to\infty}\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$. Then I got stuck. Could anyone kindly help? Thanks!","['heat-equation', 'real-analysis', 'partial-differential-equations']"
1383781,What is the implication that $\| \cdot \|_2$ and $\| \cdot \|_\infty$ are equivalent norms on $\mathbb{R^2}$,"Given $\mathbb{X}$ = $\mathbb{R^2}$, consider $\| \cdot \|_2$ and $\| \cdot \|_\infty$ We can show that $\| x \|_\infty \leq \| x \|_2 \leq \sqrt2 \| x \|_\infty$ Hence $\| \cdot \|_2$ and $\| \cdot \|_\infty$ are equivalent norms Is there some deeper implication regarding this particular relationship? Why do we care if two norms are equivalent in this sense?","['vector-spaces', 'banach-spaces', 'functional-analysis', 'normed-spaces']"
1383782,Higher-Order Differential Operators as Vector Fields,"On a $C^\infty$ manifold $M$, one can produce the tangent space $T_p M$ at a point by equivalence classes of tangents to smooth curves through the point $p$. When realised this way, the tangent vectors are local derivations of functions defined at $p$. Globally,then, vector fields on $M$ act on global functions as differential operators. Adapted to some chart $(U, x^1, \ldots, x^n)$, a vector field $V$ looks like $$
\sum_{i=1}^{n} v^i \frac{\partial}{\partial x^i}
$$ where the $v^i$ are functions on $U$. A lot of stuff comes from this, Lie theory, etc. All this is ok for me. But what about, say, acceleration vectors of curves through a point? These have the same geometrical interpretation that velocity vectors do. I can construct a curve in some local coordinates with any arbitrary acceleration that I like. The resulting space has the same dimension as the tangent space, too. Presumably one could also extend such things to global objects. With some fooling around, these can be made to correspond (locally) to expressions of the form $$
\sum_{i=1}^{n} v^i \frac{\partial^2}{\partial^2 x^i}?
$$ Going in the other direction, note that things of the above type close under the same Lie bracket as regular vector fields. Physically, acceleration fields look like forces, so one might expect such a thing to be physically meaningful -- yet I've never heard of them. Obviously this all carries on to derivative vectors of all orders. Do these sit in some enveloping algebra for vector fields? Some multivector construction? I would greatly appreciate any references, please, if you have them.","['differential-geometry', 'manifolds', 'reference-request']"
1383783,"if $\operatorname{Res}_{z_0}f = 0$, then $f$ has a primitive in some deleted neighborhood of $z_0$","Let $z_0$ be an isolated singularity of $f$. Prove that if $\operatorname{Res}_{z_0}f = 0$, then $f$ has a primitive in some deleted neighborhood of $z_0$ I know that if we assume $f$ has a primitive, then we can use Morera's theorem to prove that $\operatorname{Res}_{z_0}f = 0$. But, I don't know how to do the opposite side. Can someone please show me how can I do that ?","['analysis', 'complex-analysis']"
1383845,"Manipulating the maximum function, metric spaces.","I am trying to show that the supremum metric, $d_{\infty}$, is indeed a metric on $\mathbb R^2$. I have shown that the first two properties of a metric space hold, but am having trouble showing the third, namely that for any points $A,B,C\in \mathbb R^2$ then, $$d_\infty(A,B)\le d_\infty(A,C)+d_\infty(C,B)$$ Here is what I have done so far: $$d_\infty(A,B)=\max(|a_1-b_1|,|a_2-b_2|)$$ $$=\max(|a_1+c_1-c_1-b_1|,|a_2+c_2-c_2-b_2|)$$ $$\le\max(|a_1-c_1|+|c_1-b_1|,|a_2-c_2|+|c_2-b_2|)$$ Now, I know what it is that I should be getting, but I am not sure how to manipulate the $\max$ function to get it. Can somebody guide me through the next step to see what to do with the $\max$ function to obtain, $$=\max(|a_1-c_1|,|a_2-c_2|)+\max(|c_1-b_1|,|c_2-b_2|)$$ $$=d_\infty(A,C)+d_\infty(C,B)$$ $$\iff d_\infty(A,B)\le d_\infty(A,C)+d_\infty(C,B)$$","['functional-analysis', 'real-analysis', 'general-topology', 'metric-spaces']"
1383847,"If $A$ is an $m\times n$ matrix, $B$ is an $n\times m$ matrix and $n<m$, then $AB$ is not invertible.","The question was given in the early chapters of Linear Algebra by Hoffman & Kunze, so I am trying to give a proof with only the tools given to me so far - which are mainly row reduction and knowledge of matrix multiplication, row reduced echelon forms, row equivalence and linear independence. I attempted a proof as per the following: Consider $A$ as a collection (not sure if this would be the ideal expression) of $1 \times n$ row vectors, and $B$ as a collection of $n \times 1$ column vectors. Then we have that: $$
A=\begin{bmatrix} r_1 \\ \vdots \\ r_m \end{bmatrix},\ B=\begin{bmatrix} c_1 & \cdots & c_m \end{bmatrix}.
$$
  Thus it follows that:
  $$
AB =\begin{bmatrix} r_1\cdot c_1 & \cdots & r_1\cdot c_m \\ \vdots & ~ & \vdots \\ r_m\cdot c_1 & \cdots & r_m \cdot c_m  \end{bmatrix}
$$
  Clearly, by inspection, the rows are linearly dependent. Since the rows of $AB$ are linearly dependent, it naturally follows that the reduced row echelon form of $AB$ contains zero rows. Hence, $AB$ is not invertible. Would this be a mathematically sufficient proof?","['linear-algebra', 'matrices']"
1383852,"Finding the general formula for $a_{n+1}=2^n a_n +4$, where $a_1=1$.","Problem: Find the general formula for $a_{n+1}=2^n a_n +4$, where $a_1=1$. Find the sum of its first $2n$ terms with odd subscript. My effort: It seems to me that $a_{n+1} / 2^{(n+1)^2/2}=\dfrac{1}{\sqrt{2}}a_n/2^{n^2/2} +4/ 2^{(n+1)^2/2}$, which is $b_{n+1}=\dfrac{1}{2^{1/2}} b_{n} + \dfrac{1}{2^{(n+3)(n-1)/2}}$, where $b_n=a_n/2^{n^2/2}$. But it seems hard to deal with the last term. The first ten $a_n$ is {1, 6, 28, 228, 3652, 116868, 7479556, 957383172, 245090092036, 125486127122436} , which follows no immediate rule. Write the sequence in binary form, I find it {1, 110, 11100, 11100100, 111001000100, ...} which is generally in a 1 2*0 1 3*0 1 4*n ... pattern (apart from the first few). So I highly suspect that there is not closed form expression. But how to prove this?","['summation', 'sequences-and-series']"
1383863,"Finding all functions $f: [0, +\infty)\to \mathbb{R}$ with the properties $f^2(x)=k^2+x\cdot f(x+k)$ and $\frac{x+k}{2}\le f(x) \le 2\cdot (x+k)$","Let $f: [0, +\infty)\to \mathbb{R}$ be a function such that for one $k\in [0, +\infty)$ : $$f^2(x)=k^2+x\cdot f(x+k) \quad \forall x\in [k, +\infty) \tag 1 \label 1$$ and $$\frac{x+k}{2}\le f(x) \le 2\cdot (x+k) \quad \forall x\in [k, +\infty) \tag 2 \label 2$$ Find all such functions $f$ . I have tried the following: From \eqref{2}, with substitution $x=-k$ we get $0\le f(-k) \le 0$ , so $f(-k)=0$ . Then, from \eqref{1} with substitution $x=-k$ we get $0=k^2-k\cdot f(0)$ , so $f(0)=k$ , which is easily verifiable even when $k=0$ . I also see that $f(x)=x+k$ is a valid function. Any hint how to find ALL such functions? (I believe the above is the only function.) Edit: As Hagen Von Eitzen noted, I cannot use the substitution $x=-k$ , because both $x$ and $k$ are non-negative numbers...","['functional-equations', 'functions']"
1383883,Existence of solutions of the equation with a limit.,"Let f be continuous function on [0,1] and
$$\lim_{x→0} \frac{f(x + \frac13) + f(x + \frac23)}{x}=1$$
Prove that exist $x_{0}\in[0,1]$ which satisfies equation $f(x_{0})=0$ I suppouse that the numerator should approach $0$ which would implicate that for 
x near $0$ $f(x + \frac13)$ would be of opposite sign then $f(x + \frac23)$ or both be $0$. 
Then by intermediate value theorem we would know that there is $x_{0}\in[\frac13,\frac23]$ which fulfill  $f(x_{0})=0$ Yet, I have no idea how to prove that numerator $\rightarrow 0$","['continuity', 'limits']"
1383884,A problem in integration.,"As you know from basic trigonometry that $\sin(2x) = 2\sin(x)\cos(x)$. If you integrate both sides with respect to x, one finds $$\int \sin(2x) \ dx =  -\frac{1}{2}\cos(2x)+c$$ on the left hand side and $$2\int\sin(x)\cos(x)\  dx = \sin^2(x)+c$$ for the right hand side. They are different so what is the true integral?","['trigonometry', 'calculus', 'integration']"
1383925,Finding a Solution to a linear Voltera equation of the second type,"I want to solve the following integral equation: $$
u(t) = 
\int_t^T a(s) ds + \int_t^T b(s)u(s) ds ,
$$ with $a, b, u$ being functions from $[t,T] \rightarrow \mathbf{R} $. I transformed the integral equation into the following differential equation by using Leibniz' rule: $$
u'(t) = -a(t) - b(t)u(t).
$$ Using the explanation on MathWorld I derived the following solution to the differential equation: $$
u(t) = - \dfrac{\int_t^T \exp[-\int_t^s b(u) du ] a(s) ds + c }{ \exp[-\int_t^s b(u) du ]}
$$ Unfortunately, I think this solution is not correct. By being told the solution of a similar integral equation, I guess the solution to the integral equation above is: $$ 
u(t) = \int_t^T a(s) \exp \left( \int_t^s b(u) du \right) ds 
$$ However, I don't know how to verify my guessed solution to the integral equation and I don't know how to obtain this guess without exploiting knowledge of a similar problem. Additionally, I don't know why my solution of the differential equation does not match the guessed solution. Can somebody kindly point out some steps to solve the original integral equation and how to verify it's solution? P.S. I would also very much like to know the name for the class of integral equation the one above belongs to. I couldn't associate the integral equation above with what I've read about them in general.","['ordinary-differential-equations', 'integration']"
1383951,Statistics question on basil bush random variable,"The height, $H$ , in meters of a basil bush is a random variable with the probability density function $f_{_H}(t)=e^t,\;0\leq t\leq H_0$ such that $H_0$ is the maximal height. $\color{blue}{(1)}$ I need to find $H_0$ $\color{blue}{(2)}$ to find the average height of the basil bush $\color{blue}{(3)}$ to find the probability that the height of the basil bush is at least $0.3$ meters, exactly $0.3$ meters, at the most $0.3$ meters $\color{blue}{(4)}$ A basil bush is considered adult if its height is at least $0.3$ meters.
What is the probability that the height of the adult basil bush is bigger then $0.5$ meters? Bigger then $(0.7)$ meters? My attempt: $\color{blue}{(1)}$ $$1=\int_{-\infty}^{\infty}f_{_X}(t)dt=\int_{0}^{H_0}e^t(t)dt=e^{H_0}-1$$ $$e^{H_0}=2, \;H_0=\ln(2)\approx\boxed{0.693}$$ $\color{blue}{(2)}$ $$E(X)=\int_{-\infty}^{\infty}t\cdot f_{_X}(t)dt=\int_{0}^{\ln(2)}t\cdot e^t(t)dt$$ $\color{gray}{\text{By parts z=t dz=dt  , dg=$e^tdt$, $g=e^t$}}$ $$$$ $$te^t\bigg|_{0}^{\ln(2)}-\int_{0}^{\ln(2)} e^tdt=\ln(2)e^{\ln(2)}-2+1\approx\boxed{0.386}$$ $\Longrightarrow$ The average height is 0.386 meters $\color{blue}{(3)}$ At least $0.3:$ $P(X\leq 0.3)=\displaystyle\int_{0}^{0.3}e^tdt \approx \boxed{0.349}$ Exactly $0.3:$ $P(X=0.3)=\boxed{0}$ At the most $0.3:$ $P(X \geq 0.3)=1-0.349\approx \boxed{0.651}$ $\color{blue}{(4)}$ $P(X> 0.5)=\displaystyle\int_{0.5}^{\ln(2)}=0.351\;,P(X\geq 0.3)=0.651,\;P(0.3\times 0.5)=P(0.15)=0.161$ $P(X\geq 0.5\big|X\geq 0.3)=\boxed{\frac{0.161}{0.351}}$ $P(X>0.7)=\displaystyle\int_{0.7}^{\ln(2)}e^tdt=2-e^{0.7}\approx 0.0138$ $P(X> 0.7\big|X\geq 0.3)=\frac{}{0.351}$ Is this correct?","['probability', 'statistics', 'proof-verification']"
1383971,"""Mastermind""-esque safe opening problem.","I read this interview question for a trading job and it seems quite difficult. What is the technique to solving it? You have a safe with six digits and a light. You can input a code, if
  you have between 0 and 3 of the 6 digits correct, the light will turn
  red. If you have between 4 and 5 of the 6 digits correct, it will turn
  yellow and if you have all 6 digits correct, it will open. There is
  $10, 000 inside the safe, you can guess the code as many times as you
  want, but need to pay each time you guess, how much would you be
  willing to pay for each guess? Thanks","['probability', 'statistics', 'combinatorics']"
1384026,Milnor patching for general modules,"The Milnor patching theorem for projective modules is the following statement. Given a pullback diagram of rings 
$$
\begin{array}{}
R  & \xrightarrow{f_2} & R_2  \\
\downarrow{f_1} & & \downarrow{j_2} \\
R_1 & \xrightarrow{j_1} & R_3 
\end{array}
$$
with $j_1$ or $j_2$ surjective, and projective modules $P_1$ and $P_2$ over $R_1$ and $R_2$ respectively together with an isomorphism $h : R_3 \otimes_{R_2} P_2 \to R_3 \otimes_{R_1} P_1$, then 1) the $R$-module $P$ as the pullback of
$$
\begin{array}{}
P  & \xrightarrow{} & P_2  \\
\downarrow{} & & \downarrow{h(1 \otimes \text{id})} \\
P_1 & \xrightarrow{1 \otimes \text{id}} & R_3 \otimes_{R_1} P_1
\end{array}
$$
is projective. If $P_1$ and $P_2$ are finitely generated over $R_1$ and $R_2$ respectively, then $P$ is finitely generated. 2) There are natural isomorphisms $P \otimes_R R_1 \to P_1$ and $P \otimes_R R_2 \to P_2$. 3) All projective $R$-modules arise for appropriately chosen $P_1$, $P_2$ and $h$. This theorem gives an equivalence of categories of projective modules on $R$, and ""patching data"" of $P_1$, $P_2$ and $h$ above. Specifically, the equivalence may be described as follows. If we let $(P_1,P_2,h)$, $(P_1',P_2',h')$ be such patching data, a morphism of such triples are homorphisms $\phi_1 : P_1 \to P_1'$ and $\phi_2 : P_2 \to P_2'$ such that 
$$
\begin{array}{}
R_3 \otimes_{R_2} P_2  & \xrightarrow{1 \otimes \phi_2} & R_3 \otimes_{R_2} P_2'  \\
\downarrow{h} & & \downarrow{h'}\\
R_3 \otimes_{R_1} P_1 & \xrightarrow{1 \otimes \phi_1} & R_3 \otimes_{R_1} P_1'
\end{array}
$$
commutes. A projective module $P$ on $R$ yields a triple $(R_1 \otimes_R P, R_2 \otimes_R P, \text{id})$, where $\text{id}$ denotes the identity map $R_3 \otimes_R P \to R_3 \otimes P$. A morphism of modules $P \to P'$ induces a morphism of triples in the obvious way. My question is whether there exists a similar result for finitely generated modules (and modules in general), that is, not assuming $P_1$ and $P_2$ projective, with reasonable assumptions on $j_1$ and $j_2$. Optimally, no stronger conditions though. I would like an equivalence of categories of finitely generated $R$-modules and a certain notion of ""patching data"" in this context. For example, perhaps by allowing $h$ to be any homomorphism would yield the corresponding result for finitely generated modules over $R$?","['abstract-algebra', 'projective-module', 'modules', 'commutative-algebra']"
1384038,Over what rings is the Hefferonian determinant unique?,"Fix an $n\in\mathbb{N}$ and a field $\mathbb{K}$. A lot of texts in linear algebra like to define the determinant function on $\operatorname{M}_n\left(\mathbb{K}\right)$ as the unique function $\operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ which is alternating and multilinear on the rows of the matrix and sends the identity matrix $I_n$ to $1$. This definition generalizes verbatim to the case when $\mathbb{K}$ is a commutative ring. Jim Hefferon's Linear Algebra (version 22 Dec 2014) (Definition 2.1 in Chapter Four) uses a slightly modified version of this definition. In my notations, a Hefferonian determinant function means a function $f : \operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ having the following four properties: If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by adding a multiple of a row of $A$ to another row of $A$, then $f\left(B\right) = f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by switching two rows, then $f\left(B\right) = - f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by multiplying a row by a scalar $\lambda$, then $f\left(B\right) = \lambda f\left(A\right)$. We have $f\left(I_n\right) = 1$. Hefferon then shows that such a function $f$ is unique when $\mathbb{K}$ is a field. It is easy to show that, more generally, there is a unique Hefferonian determinant function when $\mathbb{K}$ is an integral domain (namely, the usual determinant $\det$). Out of curiosity, I am wondering how far this uniqueness statement can be generalized. It doesn't feel right to expect it to hold over an arbitrary commutative ring $\mathbb{K}$, as things like Gaussian elimination just will not work and there will not be any quotient field to salvage them. But I am not sure how to find a counterexample. Ideas? Notice that Hefferon is not the only author who defines a determinant in such a strange way. The definition of a determinant in Theorem 1.50 of Peter J. Olver's and Chehrzad Shakiban's Applied Linear Algebra (2006) is similar to Hefferon's. Instead of property 4, it requires $f$ to send any upper-triangular matrix to the product of its diagonal entries. This is stronger than Hefferon's property 4, and I am wondering if it is actually stronger or just equivalent? Apparently the popularity of these unnatural definitions is due to the opinion that genuine multilinearity is too difficult for students to grasp; I am not sure if this justifies them, but I believe that the question it posts is interesting!","['determinant', 'linear-algebra', 'algebraic-k-theory', 'commutative-algebra']"
1384073,Problem in Banach Fixed Point Theorem for a functional equation,"I was recently presented this within the context of topological spaces: I am asked to show that there exists a unique continuous function $ f\colon \left[0,\frac{1}{2}\right] \rightarrow \Bbb R $ such that for all $ x \in \left[0,\frac{1}{2}\right] $ the following equality holds:
$$ f(x) = \frac{x}{2}\sin f(x) +\sin\left( f\left(\frac{x}{2}\right)\right)+1. $$
I can do this because it is simply looking for a fixed point of the contracting operator $ T(f(x)) = \frac{x}{2}\sin(f(x))+\sin(f(\frac{x}{2}))+1. $ I can show it to be contracting for $ x \in [0,\frac{1}{2}] $ and the space of continuous functions
$ C\left[0,\frac{1}{2}\right] $ is of course complete, so Banach's fixed point theorem holds. Now the hard part: the exact same problem as before, only now the interval is $[0,1)$. How do I do it? I know that $C[0,1)$ is not complete and the operator itself is not a contraction, so Banach's fixed point theorem is out the window. Is there another solution?","['metric-spaces', 'fixed-point-theorems', 'general-topology']"
1384080,"Closed-forms of the integrals $\int_0^1 K(\sqrt{k})^2 \, dk$, $\int_0^1 E(\sqrt{k})^2 \, dk$ and $\int_0^1 K(\sqrt{k}) E(\sqrt{k}) \, dk$","Let denote $K$ and $E$ the complete elliptic integral of the first and second kind . The integrand $K(\sqrt{k})$ and $E(\sqrt{k})$ has a closed-form antiderivative in term of $K(\sqrt{k})$ and $E(\sqrt{k})$, so we know that
$$
\int_0^1 K\left(\sqrt{k}\right) \, dk = 2,
$$
and
$$
\int_0^1 E\left(\sqrt{k}\right) \, dk = \frac{4}{3}.
$$ I couldn't find closed-form antiderivatives to the integrals $\int K(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})^2 \, dk$, $\int E(\sqrt{k})K(\sqrt{k}) \, dk$, but I've conjectured, that $$\begin{align}
\int_0^1 K\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{2}\zeta(3),\\
\int_0^1 E\left(\sqrt{k}\right)^2 \, dk &\stackrel{?}{=} \frac{7}{8}\zeta(3)+\frac{3}{4},\\
\int_0^1 K\left(\sqrt{k}\right)E\left(\sqrt{k}\right) \, dk &\stackrel{?}{=} \frac{7}{4}\zeta(3)+\frac{1}{2}.
\end{align}$$ How could we prove this closed-forms? It would be nice to see some references to these integrals.","['calculus', 'closed-form', 'definite-integrals', 'elliptic-integrals', 'integration']"
1384112,Show $\int_E {(f_1 + f_2)d\mu } = \int_E {f_1 d\mu } + \int_E {f_2 d\mu } $,"In my textbook, given a measure space $(\Omega,F,\mu)$, the integration for a non-negative $F$ measurable function $f$ on $E$ is defined as $$\int_E f\ \mathsf d\mu  = \sup_{0 \le h \le f} I_E\left( h \right)$$ where $h$ is simple function on $E$, i.e. $$h = \sum_{k = 1}^N {a_k}{\chi _{{E_k}}}\left( x \right)$$ and $${I_E}\left( h \right) = \sum_{k = 1}^N {a_k}\mu \left( {{E_k}} \right).$$ Now the question is to show, using a previous result $I_E(g+h)=I_E(g)+I_E(h)$: $$\int_E {(f_1 + f_1)\ \mathsf d\mu }  = \int_E {f_1\ \mathsf d\mu }  + \int_E {f_2\ \mathsf d\mu } $$
  where $f_1$ and $f_2$ are all non-negative $F$-measurable functions. I am able to show $$\int_E \left( {{f_1} + {f_2}} \right)\ \mathsf d\mu  \ge \int_E {f_1}\ \mathsf d\mu  + \int_E {f_2}\ \mathsf d\mu .$$ Let \begin{align}\int_E {f_1}d\mu  &= \sup_{0 \le h \le {f_1}} {I_E}\left( h \right),\\ \int_E {f_2}d\mu  &= \sup_{0 \le g \le {f_2}} {I_E}\left( g \right).\end{align} Define sets \begin{align}
A &= \left\{ {{I_E}\left( {u} \right):0 \le u \le {f_1} + {f_2}} \right\},\\ B &= \left\{ {{I_E}\left( h \right) + {I_E}\left( g \right):0 \le h \le {f_1},\;0 \le g \le {f_2}} \right\}\end{align} It is obvious that $B \subseteq A$ since $$0 \le h+g\le f_1+f_2$$ for any $h,g$ and $$I_E(g+h)=I_E(g)+I_E(h).$$ Thus we have \begin{align}\sup B \le \sup A \Rightarrow \sup B &= \mathop {\sup }\limits_{0 \le h \le {f_1},0 \le g \le {f_2}} \left[ {{I_E}\left( h \right) + {I_E}\left( g \right)} \right]\\ &= \sup\limits_{0 \le h \le {f_1}} {I_E}\left( h \right) + \sup\limits_{0 \le g \le {f_2}} {I_E}\left( g \right)\\ &\le \sup\limits_{0 \le u \le {f_1} + {f_2}} {I_E}\left( {u} \right)\end{align} But I have difficulty proving the opposite inequality . If I can prove every $u$ can be written as $u=h+g$ for some $h,g$, then the prove is done. But it seems it is hard to prove ""every $u$ can be written as $u=h+g$"". There could be another way around. Thank you!","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1384164,10th derivative of a function,"I want to find $f^{(10)}(0)$ where $f(x)=\ln(2+x^2)$. I know that it can be done ""by hand"", but I believe there is a smarter way. I think I should use Taylor series and the fact that $f^{(n)}(0)=a_n*n!$ , but I'm not sure how.",['calculus']
1384169,First 10 digits after decimal point in the number $(1+\sqrt{3})^{2015}$,The question is how to find first 10 digits after decimal point in the number $(1+\sqrt{3})^{2015}$. I keep running into this kind of problems in a context of symmetric polynomials.,"['recreational-mathematics', 'irrational-numbers', 'algebra-precalculus']"
1384181,$\lfloor x^k \rfloor \equiv m \pmod{n}$ with $x$ irrational,"Let $x>1$ be an irrational number, and $n$ a positive integer. Is it true that, for each integer $m$, there exists an integer $k$ such that
$$
\lfloor x^k \rfloor \equiv m \pmod{n}?
$$","['congruences', 'number-theory', 'ceiling-and-floor-functions', 'irrational-numbers', 'elementary-number-theory']"
1384184,"Is $(a,a]=\{\emptyset\}$?","Let $a \in \mathbb{R}$, and consider the half open interval $(a,a]$. Is it correct to write this half open interval as $(a,a]=\{\emptyset \}$? Or $(a,a]=\{a \}$?","['convention', 'elementary-set-theory', 'real-analysis']"
1384200,Exercise 43 chapter 2 in Real Analysis of Folland,"I got stuck on this problem and couldn't find any clue to solve it. Can anyone give me some hint or give me some solution for it. I really appreciate! Suppose that $\mu(X) < \infty$ and $f: X \times [0,1] \rightarrow \mathbb{C}$ is a function such that $f(.,y)$ is measurable for each $y \in [0,1]$ and $f(x, .)$ is continuous for each $x \in X$. a) If $0 < \epsilon, \delta < 1$, then $E_{\epsilon, \delta} = \{x: |f(x, y) - f(x, 0)| \le \epsilon, \forall y \lt \delta \}$ is measurable b) For any $\epsilon \gt 0$, there is a set $E \subset X$ such that $\mu(E) < \epsilon$ and $f(., y) \rightarrow f(., 0)$ uniformly on $E^c$ as $y \rightarrow 0$","['continuity', 'convergence-divergence', 'real-analysis', 'measure-theory']"
1384202,Finding all the triangles $ABC$ satisfying $\sum \frac{a^{2}\cos\frac{B-C}{2}}{\sin\frac{A}{2}}=2(a^2+b^2+c^2)$,"$\triangle ABC$ has $BC=a, CA=b, AB=c$ and satisfies
$$\dfrac{a^{2}\cos\dfrac{B-C}{2}}{\sin\dfrac{A}{2}}+\dfrac{b^{2}\cos\dfrac{C-A}{2}}{\sin\dfrac{B}{2}}+\dfrac{c^{2}\cos\dfrac{A-B}{2}}{\sin\dfrac{C}{2}}=2(a^2+b^2+c^2)$$ Find all the triangles $ABC$.","['geometry', 'triangles', 'trigonometry']"
1384258,If a function has an inverse then it is bijective?,"I have some trouble finding the answer to this, can someone help me out: If I have a general function $f$ with domain $X$ and codomain $Y$, I know nothing about the function (injective, surjective). Say I have found and inverse that is, there is a function $g$ such that for $f(x) = y$, $g(y) = x$ (is this enough for an inverse). Then the function is bijective?","['inverse', 'functions']"
1384274,From Gravity Equation-of-Motion to General Solution in Polar Coordinates,"I'm having trouble getting the general solution of this differential equation. The gravitational equation of motion is, for constants $M$ and $G$ and position vector $\vec{r}$, $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r}
$$ By using 2D polar coordinates (one angle $\theta$ and one ""distance from origin"" $r$), one can calculate $\frac{d^2}{dt^2}\vec{r}$ by taking two time derivatives of $\vec{r}=r\hat{r}$. The hat notiation $\hat{r}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $r$. Similarly, $\hat{\theta}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $\theta$. By appropriate application of the chain rule of derivatives (remembering to take derivatives of the unit vectors themselves as well), one can derive that (dot means time-dirivative) $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r} = \hat{\theta}(2\dot{r}\dot{\theta}+r\ddot{\theta})+\hat{r}(\ddot{r}-r\dot{\theta}^2)
$$ Which means $$\begin{equation} -\frac{MG}{r^2}=\ddot{r}-r\dot{\theta}^2 \tag{1} \end{equation}
$$ and $$\begin{equation} 0=2\dot{r}\dot{\theta}+r\ddot{\theta} \tag{2} \end{equation}
$$
The right hand side of the last equation (2) turns out to be the time derivative of the angular-momentum-per-unit-mass $h$ (to a factor of $r$):
$$\frac{d}{d t}h=\frac{d}{d t}(r^2\dot{\theta})=2r\dot{r}\dot{\theta}+r^2\ddot{\theta}=0
$$
Showing that h is constant in time. My question is, using what has been laid out, how does one combine the two differential equations (1) and (2) to get something that looks like it can be solved? I have looked at references, but certain steps in the derivations seem to lack explanation. From what I've seen, it seems important to eliminate $t$ from the equations (1) and (2) to yield an equation with only $r$ and $\theta$. Thank you for any help, I have been tearing my hair out over this. --- Update ---
It appears the substitution $u=\frac{1}{r}$ leads to a $\ddot{r}$ in terms of $\frac{d^2 u}{d^2 \theta}$.","['polar-coordinates', 'ordinary-differential-equations', 'derivatives']"
1384287,When is $f^! \mathcal{O}_Y$ a line bundle?,"Let $f: X \to Y$ be a finite, surjective morphism of reduced, separated schemes of finite typer over some field of characteristic zero. The sheaf $\mathcal{H}om_Y(f_* \mathcal{O}_X, \mathcal{O}_Y)$ is a coherent $f_* \mathcal{O}_X$-module and thus it corresponds to a coherent $\mathcal{O}_X$-module denoted by $f^! \mathcal{O}_Y$. What are conditions on $f$, $X$ or $Y$ such that $f^! \mathcal{O}_Y$ is a line bundle on $X$? (e.g. $f$ is flat, $Y$ is smooth, etc.) I think if $X$ and $Y$ are both smooth, then this is the case. And I guess that it is not true in general (does someone have a reasonable counter example?).","['algebraic-geometry', 'commutative-algebra']"
1384317,"""Mean value like"" problem.","Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be differentiable, take $a<a'<b<b'$. Prove that there exists $c<c'$ such that $$\frac{f(b)-f(a)}{b-a}=f'(c) \quad and \quad \frac{f(b')-f(a')}{b'-a'}=f'(c').$$ My first tries were connected with mean value because we can find such $c,c'$ but we don't konw if they satisfie required relation. We know though that they are in $(a',b)$. I ask for some hints .","['calculus', 'real-analysis', 'derivatives']"
1384322,Are custom named functions acceptable notation?,"A custom name being, for example, my function name (MFN): $MFN(x) := ax + b$ As contrasted with: $\delta(x) := ax + b$ Questions: Is it permissible to name the function $MFN$ above? Or is this restricted to very well known functions, such as $sgn(x)$? Can you refer me to a source for the use of word abbreviations as names of functions? Is lower-case preferred to upper-case? What circumstances dictate upper-case function names (or letters)?","['notation', 'functions']"
1384338,Math Intuition and Natural Motivation Behind t-Student Distribution,"I am trying to understand with basic mathematical background how the $t$ -Student distribution is a ""natural"" pdf to define. A more accessible explanation than this post , or the daunting Biometrika paper by RA Fisher. Background: The central limit theorem states that if ${\textstyle X_{1},X_{2},...,X_{n}}$ are each a random sample of size ${\textstyle n},$ taken from a population with mean ${\textstyle \mu }$ and finite variance ${\textstyle \sigma ^{2}}$ and if ${\textstyle {\bar {X}}}$ is the sample mean, then the limiting form of the distribution of ${\textstyle Z=\left({\frac {{\bar {X}}_{n}-\mu }{\sigma /\surd n}}\right)}$ as ${\textstyle n\to \infty },$ is the standard normal distribution. If $X_1, \ldots, X_n$ are iid random variables $\sim N(\mu,\sigma^2)$ , $$\frac{\bar{X}\,-\,\mu}{\sigma/\sqrt{n}} \sim N(0,1)$$ This is the basis of the Z-test , $Z=\frac{\bar{X}\,-\,\mu}{\sigma/\sqrt{n}}$ [ Note that the preceding opening statement is now correct after reflecting @Ian and @Michael Hardy comments to the OP in reference to the CLT. ] If the standard deviation of the population, $\sigma$ , is unknown we can replace it by the estimation based on a sample, $S$ , but then the expression ( one-sample t-test statistic ) will follow a $t$ -distribution: $$ t=\frac{\bar{X}\,-\,\mu}{S/\sqrt{n}}\sim t_{n-1}$$ with $$s=\sqrt{\frac{\sum(X_i-\bar X)^2}{n-1}}.$$ Minimal manipulations of this equation for $T$ $$\begin{align} \frac{\bar{X}\,-\,\mu}{S/\sqrt{n}} &= \frac{\bar{X}\,-\,\mu}{\frac{\sigma}{\sqrt{n}}} \frac{1}{\frac{S}{\sigma}}\\[2ex]
&= Z\,\frac{1}{\frac{S}{\sigma}}\\[2ex] 
&= \frac{Z}{\sqrt{\frac{\color{blue}{\sum(X_i-\bar X)^2}}{(n-1)\,\color{blue}{\sigma^2}}}}\\[2ex]
&\sim\frac{Z}{\sqrt{\frac{\color{blue}{\chi_{n-1}^2}}{n-1}}}\\[2ex] &\sim t_{n-1}\small \tag 1
\end{align}$$ will introduce the chi square distribution, $(\chi^2).$ The chi square is the distribution that models $X^2$ with $X\sim N(0,1)$ : Let's say that $X \sim N(0,1)$ and that $Y=X^2$ and find the density
of $Y$ by using the $\text{cdf}$ method: $$\Pr(Y \leq y) = \Pr(X^2 \leq y)= \Pr(-\sqrt{y} \leq x \leq
> \sqrt{y}).$$ We cannot integrate in close form the density of the normal
distribution. But we can express it: $$ F_X(y) = F_X(\sqrt{y})- F_X(-\sqrt[]{y}).$$ Taking the derivative of the cdf: $$ f_X(y)= F_X'(\sqrt{y})\,\frac{1}{\sqrt{y}}+
> F_X'(\sqrt{-y})\,\frac{1}{\sqrt{y}}.$$ Since the values of the normal $\text{pdf}$ are symmetrical: $$ f_X(y)=  F_X'(\sqrt{y})\,\frac{1}{\sqrt{y}}.$$ Equating this to the $\text{pdf}$ of the normal (now the $x$ in the $\text{pdf}$ will be $\sqrt{y}$ to be plugged into the $e^{-\frac{x^2}{2}}$ part of the normal $\text{pdf}$ ); and remembering
to in include $\frac{1}{\sqrt{y}}$ at the end: $$\begin{align} f_X(y) &= 
 F_X'\left(\sqrt{y}\right)\,\frac{1}{\sqrt[]{y}}\\[2ex]
 &=\frac{1}{\sqrt{2\pi}}\,e^{-\frac{y}{2}}\,
 \frac{1}{\sqrt[]{y}}\\[2ex]
 &=\frac{1}{\sqrt{2\pi}}\,e^{-\frac{y}{2}}\, y^{\frac{1}{2}- 1}
 \end{align}$$ Comparing to the pdf of the chi square: $$ f_X(x)= \frac{1}{2^{\nu/2}\Gamma(\frac{\nu}{2})}e^{\frac{-x}{2}}x^{\frac{\nu}{2}-1}$$ and, since $\Gamma(1/2)=\sqrt{\pi}$ , for $\nu=1$ df, we have derived exactly the $\text{pdf}$ of the chi square. In the case of the $t$ -distribution the chi-square is suitable to model the sum of squared normals, i.e $\displaystyle \sum(X_i-\bar X)^2 $ in the set of Eq $(1),$ a well known property derived here typically with $n$ degrees of freedom, but why is it $n\,-\,1$ here, i.e. $\color{blue}{\chi^2_{n-1}}$ in eq. $(1)$ ? I don't know how to explain $\sigma^2$ in $\frac{\sum(X_i-\bar X)^2}{\sigma^2}$ in equation $(1)$ becoming ""absorbed"" into the $\chi_{n-1}^2$ part of the $\text{t-Student's pdf}.$ So it boils down to understanding why $$\frac 1 {\sigma^2} \left((X_1-\bar X)^2 + \cdots + (X_n - \bar X)^2 \right) \sim \chi^2_{n-1}.$$ After that the derivation of the pdf is not that daunting . PS: The accepted answer below becomes clear after reading this Wikipedia entry. Also, a plot may be useful including the spherical cloud $(X_1,X_2,X_3)$ (in red) with the orthogonal projection, $(X_1-\bar X,\ldots,X_n-\bar X),$ on the $n-1$ subspace forming a plane (in blue) at the origin:","['statistics', 'probability-distributions']"
1384360,Compact diagonal operator,"Suppose $A : H \to H$, where $H$ is a Hilbert space, is bounded. Also, $A$ is a diagonal operator with diagonal $\{a_n\}$. Show: If $A$ is compact, then $a_n \to 0$ as $n \to \infty$. Should I prove this by contrapositive?","['hilbert-spaces', 'compact-operators', 'operator-theory', 'functional-analysis']"
1384391,"Books on complex analysis (Ahlfors, Conway and Lang)","To make my question slightly different from others, I would like to know how would you rate on the complex analysis books by Ahlfors, Conway and Lang? I had a basic course on complex analysis during undergraduate (and you could imagine it's mostly about computing integrals and residues), and would like to learn more about the theory. There exist many good books, and the three books aforementioned are the ones I like the most. Of course I don't and won't have time to study all these three books in detail, so I have to pick one. The coverage of these books seem to be similar (except Conway's second volume, which should not be compared to others' single volume book). These three books contain rigorous proofs, so it's kind of hard to choose. Of course if you have read any two of them or all three of them you are very welcome to compare these books. If you ask me where am I headed to I would say I want to learn something about several complex variables. Also, if you think there is some book better than these three, you are welcome to mention it.","['education', 'book-recommendation', 'complex-analysis']"
1384426,Solve the trigonometric equation $\csc^2 \theta= 5 \cot \theta + 7$,Solve the given equation. Let k be any integer. $$\csc^2 θ = 5 \cot θ + 7$$ I just need the first step or two please. I tried converting it: $$\frac{1}{\sin^2 θ} = \frac {5\cosθ}{\sinθ} + 7$$ Then I tried a number of different ways to simplify it but it didn't work out,"['algebra-precalculus', 'trigonometry']"
1384441,Existence of a map $\phi \colon \mathbb N\cup \{0\} \rightarrow \mathbb N\cup \{0\}$ that holds the property $\phi (ab) = \phi(a)+ \phi(b)$,"Does  there  exist  a  map $\phi \colon \mathbb N\cup \{0\} \rightarrow \mathbb N\cup \{0\}$  that  holds  the  following  property?
 $$\phi (ab) = \phi(a)+ \phi(b)$$
 If  they  do  what  do  they  look  like$?$
 Here $\mathbb N$ is  the  set  of  all  natural  numbers  and  addition  and  multiplication  are   usual  for  integers.","['elementary-set-theory', 'number-theory']"
1384455,"Can a space $X$ be homeomorphic to its twofold product with itself, $X \times X$?","Let $X$ be  a  topological  space  of  infinite  cardinality. Is  it  possible  for  any  $X$  to  be  homeomorphic  to  $X\times X$  $?$ For example, $\mathbb R$  is  not  homeomorphic  to  $\mathbb R^{2}$, and $S^{1}$  is not  homeomorphic  to  $S^{1} \times S^{1}$ . What  other  topological  spaces  might we consider$?$ What  properties  of  a  space  may  ensure  or  contradict  this  possibility$?$  From  the  little  topology  I  have  learnt  yet,  I  have  not  seen  this  happening.","['product-space', 'examples-counterexamples', 'general-topology']"
1384456,Is there a change-of-variables solution for integrals from negative infinity to a constant?,"I found a fantastic and generalizable substitution technique for computing definite integrals that go to infinity from either negative infinity or a constant, regardless of the function (sorry for the external link): http://ab-initio.mit.edu/wiki/index.php/Cubature#Infinite_intervals But what's killing me is that I need the same sort of thing for a function going from negative infinity to a constant, and I can't convince myself that some obvious transformation of either of these two is correct. Doing u-substitution on any given integrand to take care of a (negative) infinite bound is simple enough, but a general transformation rule like one of the ones in the link - totally independent of the function itself - would be invaluable. Edit 1 (Might be a Wild Goose Chase) So the second formula up in that link, is: $\int_{-\infty}^{\infty}f(x)dx=\int_{-1}^1 f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt$ If $a$ represents our generic real-number constant, the above could certainly be broken up as: $\int_{-\infty}^{\infty}f(x)dx=\int_{-1}^a f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt+\int_{a}^1 f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt$ Which tempts me to jump to the conclusion that the first term on the RHS can be used as our general negative-infinity-to-constant formula: $\int_{-\infty}^{a}f(x)dx=\int_{-1}^a f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt$ In order for this to be true, however, the second RHS term would have to complement it by being a valid formula to integrate from any constant up to infinity, IE: $\int_{a}^{\infty}f(x)dx=\int_{a}^1 f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt$ ...but the first formula given at the above link is also supposed to be such a formula, solving for that exact LHS term! So the two would have to be equivalent - IE: $\int_{a}^{\infty}f(x)dx=\int_{a}^1 f(\frac{t}{1-t^2})\frac{1+t^2}{(1-t^2)^2}dt=\int_{0}^1 f(a+\frac{t}{1-t})\frac{1}{(1-t)^2}dt$ And here I'm stuck, because I can't figure out whether or not that's plausible for all convergent $f(x)$. I have a bad feeling about that whole approach because it relies on the assumption that dividing up a finite space at a given point is the same as dividing up an equivalent infinite space at the exact same point, proportionally.","['infinity', 'substitution', 'integration']"
1384478,How long before the prey can escape?,"I've (sort of) come across the following problem in my research. The actual scenario is a little abstract to explain, so I'm rephrasing the problem in terms of a predator/prey scenario. I'm tagging this as a soft question because: I'm not even sure if the problem is analytically tractable, and I'm interested to know what others think about this; I'm not familiar with this kind of mathematics (which I'm assuming has mostly to do with cellular automata and probability?), and so even hints as to which techniques might be useful would be helpful; The formulation is not set in stone, and I'm open to modifications that make the problem more tractable. A prey (red) is being guarded by 8 predators (blue) on a square lattice (left diagram). The predators' discipline slowly wanes away, and they start pacing around. If at any time the prey is not being guarded by at least one predator, it finds itself free to escape by flying away (right diagram). I am interested in the expected time for which the predators are able to keep the prey from escaping. I propose the following rules for the predators' movement: The prey remains stationary. At each time step $k$, each predator randomly chooses 1 out of its 8 neighboring squares to move into (regardless of these squares' occupancy). For each predator whose chosen square has not also been chosen by another predator, and is not the prey's square, the predator moves into its chosen square. Repeat Step 3 until no predator can make a further move. $k \leftarrow k+1$ and go to Step 1 if the prey is not free, or terminate if it is.","['probability-distributions', 'expectation', 'soft-question', 'cellular-automata', 'probability']"
1384500,"Prove that $\mathbb{A} \cap \mathbb{Q}(\sqrt{2},\sqrt{-3})$ is a PID.","While self-studying algebraic number theory, I came across the following problem: Prove that $\mathbb{A} \cap \mathbb{Q}(\sqrt{2},\sqrt{-3})$ is a PID. where $\mathbb{A} \cap \mathbb{Q}(\sqrt{2},\sqrt{-3})$ denotes the ring of algebraic integers in $\mathbb{Q}(\sqrt{2},\sqrt{-3})$.  My first intuition was to find the Minkowski bound and hope that it's below $2$, but this turns out to not be the case.  Let $R := \mathbb{A} \cap \mathbb{Q}(\sqrt{2},\sqrt{-3})$; then we know that $|\text{disc}(R)| = 16\cdot2\cdot3\cdot6$, so the Minkowski bound is $$ \frac{4!}{4^4}\left(\frac{4}{\pi}\right)^2\sqrt{16\cdot 6 \cdot 3 \cdot 2} \approx 3.64. $$ Thus, I need only show that the primes lying over $2$ and $3$ are principally generated, but I'm having trouble showing that.  I don't (in general) know how  primes split in this ring, so I tried to use the different.  We know that $\| \text{diff}(R) \| = |\text{disc}(R)| = 2^6\cdot 3^2$, so we can conclude $$(3) = (\frak{p}_3\frak{p}_3')^2. $$ I'm not sure how this helps, though.  Any help is appreciated.","['abstract-algebra', 'principal-ideal-domains', 'algebraic-number-theory', 'ring-theory']"
1384514,"$p,q,r$ primes, $\sqrt{p}+\sqrt{q}+\sqrt{r}$ is irrational.","I want to prove that for $p,q,r$ different primes, $\sqrt{p}+\sqrt{q}+\sqrt{r}$ is irrational. Is the following proof correct? If $\sqrt{p}+\sqrt{q}+\sqrt{r}$ is rational, then  $(\sqrt{p}+\sqrt{q}+\sqrt{r})^2$ is rational, thus $p+q+r+2\sqrt{pq}+2\sqrt{pr}+2\sqrt{qr}$ is rational, therefore $\sqrt{pq}+\sqrt{pr}+\sqrt{qr}$ is rational. If $\sqrt{pq}+\sqrt{pr}+\sqrt{qr}$ is rational, then $(\sqrt{pq}+\sqrt{pr}+\sqrt{qr})^2$ is rational, therefore $pq+qr+pr+\sqrt{p^2qr}+\sqrt{pq^2r}+\sqrt{pqr^2}$ is rational, therefore $p\sqrt{qr}+q\sqrt{pr}+r\sqrt{pq}$ is rational. Now suppose $p<q<r$. If $p\sqrt{qr}+q\sqrt{pr}+r\sqrt{pq}$  and $\sqrt{pq}+\sqrt{pr}+\sqrt{qr}$ are rational, then $$p\sqrt{qr}+q\sqrt{pr}+r\sqrt{pq}-p(\sqrt{pq}+\sqrt{pr}+\sqrt{qr})$$ is rational, therefore $(q-p)\sqrt{pr}+(r-p)\sqrt{pq}$ is rational. If $(q-p)\sqrt{pr}+(r-p)\sqrt{pq}$ is rational, then $((q-p)\sqrt{pr}+(r-p)\sqrt{pq})^2$ is rational, thus $(q-p)^2pr+2(q-p)(r-p)\sqrt{p^2qr}+(r-p)^2pq$ is rational, thus $\sqrt{qr}$ is rational. But $q,r$ are distinct primes, thus $qr$ can't be a square. Thus $\sqrt{qr}$ is irrational. Contradiction. Also, is there an easier proof?","['alternative-proof', 'number-theory', 'proof-verification', 'radicals', 'rationality-testing']"
1384532,Do 4 points in ${\mathbb R}^2$ in convex position define a unique elliplse that passes through those 4 points?,"So it takes 3 distinct points in the plane, that are not collinear, to define a unique circle that passes through the points. So what about ellipses? Arguing naively in terms of degrees of freedom doesn't seem to help too much, because for a circle we have 3 degrees of freedom (2-d center coordinates and 1-d radius), and yet it takes 3 2-d points (6 degrees of freedom), not 2 2-d points (4 degrees of freedom), to define a unique circle that passes through the points. I haven't studied conic sections much so I apologize if this question is trivial, but if we have 4 2-d points in convex position (i.e. each point is a vertex of the convex hull) does this define a unique ellipse that passes through the 4 2-d points? Or do we sometimes need 5 or more 2-d points in convex position?","['geometry', 'conic-sections', 'algebra-precalculus']"
1384538,Proving wedge product is associative,"Fix a real vector space $V$ of finite dimension. Let's denote by $\Lambda^p(V)$ the vector space of $p$-forms on $V$ (i.e. alternating $p$-tensors). Then we have the product $\wedge : \Lambda^p(V) \times \Lambda^q(V) \to \Lambda^{p + q}(V)$ given by $(\omega \wedge \eta)(X_1, \ldots, X_{p + q}) = \frac{1}{p! q!} \sum_{\sigma \in S_{p + q}} sgn\ \sigma \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + 1)}, \ldots, X_{\sigma(p + q)})$. How can I prove that $\wedge$ is associative? I've tried developing it from the definition but in the end I don't get things that look nice or are obviously equal. Specifically, I get: $(\omega \wedge \eta) \wedge \theta(X_1, \ldots, X_{p + q + r}) = \frac{1}{(p + q)! r! p! q!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{p + q}} sgn\ \sigma\ sgn \tau \cdot \omega(X_{\sigma \tau(1)}, \ldots, X_{\sigma \tau(p)}) \eta(X_{\sigma \tau(p + 1)}, \ldots, X_{\sigma \tau(p + q)}) \theta(X_{\sigma(p + q + 1)}, \ldots, X_{\sigma(p + q + r)})$ $\omega \wedge (\eta \wedge \theta)(X_1, \ldots, X_{p + q + r}) = \frac{1}{p! (q + r)! q! r!} \sum_{\sigma \in S_{p + q + r}} \sum_{\tau \in S_{q + r}} sgn\ \sigma\ sgn\ \tau \cdot \omega(X_{\sigma(1)}, \ldots, X_{\sigma(p)}) \eta(X_{\sigma(p + \tau(1))}, \ldots, X_{\sigma(p + \tau(q))}) \theta(X_{\sigma(p + \tau(q + 1))}, \ldots, X_{\sigma(p + \tau(q + r))})$","['determinant', 'linear-algebra', 'differential-forms', 'exterior-algebra']"
1384567,Differentiating composition of functions proof help,"Theorem: Let $X, Y, Z$ be normed spaces and $U\subset X$, $V \subset Y$ open sets. If the function $f:U \to V$ is differentiable in $x \in U$ and function $g: V \to Z$ differentiable in $f(x)\in V$, then the function $g \circ f: U \to Z$ is differentiable in $x \in U,$ and : $$(g \circ f)'(x)=g'(f(x))\circ f'(x)$$ Proof:(I will highlight from which point on is unclear to me..)
-since the functions are differentiable then:
$f(x+h)-f(x)=f'(x)h+R_1(h), \\g(y+h)-g(y)=g'(x)h+R_2(k) $ where $\frac{\|R_1(h)\|}{\|h\|}\to 0 $ when $h\to 0, \frac{\|R_2(k)\|}{\|k\|} \to 0$ when $k \to 0$. Let's give $f(x)$ the notation $y$, when we have : $$(g \circ f)(x+h)-(g  \circ f )(x)= g(f(x+h))-g(f(x))\\ g(y+f'(x)h+R_1(h))-g(y)=$$ $$g'(y)(f'(x)h)+g'(y)(R_1(h))+R_2(f'(x)h+R_1(h))= \\ (g'(y) \circ f'(x))h+ R(h) \text{ where } R(h)=g'(y)R_1(h)+R_2(f'(x))h + R_1(h).\text { Lets give the vector  } f'(x)h+R_1(h) \text{ the notation } H. \\ \|g'(y)R_1(h)\|\leq \|g'(y)\| \|R_1(h)\|, \\ \|H\| \leq \|f'(x) \|\|h\|+ \| R_1(h)\|, \text{Since }R_1(h)=o(h), R_2(H)=o(H), \text{ from (1),(2),(3) we have:}\\ g'(y)R_1(h)=o(h) \\  H\to 0 \text{ when } h \to 0 \text{ where } \frac{\|H\|}{\|h\|} \text{ is bounded (the professor puts a lot of attention on this part. )}$$ My definition of differentiability: Let $X$ and $Y$ be normed vector spaces upon the same field $\mathbb R$ or $\mathbb C$ and $U$ an open set in $X$. For a function $f:U \to Y$ it is said to be differentiable in point $x \in U$ if there exists a continuous linear map $A_x:X \to Y$ such that:
$$f(x+h)-f(x)=A_xh+R(h)$$ where $$\lim_{h \to 0}\frac{R(h)}{\|h\|}=0. \text{ or } R(h)=o(h)$$","['calculus', 'derivatives']"
1384575,Haar measure - a problem from Folland,"I was presented with this question from Folland's real analysis second edition involving Haar measures. It is problem 3 of chapter 11 page 347, which reads as follows: Let G be a locally compact group that is homeomorphic to an open subset U of
  $ \mathbb{R}^n $ in such a way that, if we identify G with U, left translation is an affine map - that is, $ xy = A_x(y) + b_x $ where $ A_x $ is a linear transformation of $ \mathbb{R}^n $ and $ b_x \in \mathbb{R}^n $. Then $ det| A_x |^{-1} dx $ is a left Haar measure on G, where dx denotes Lebesgue measure on
  $ \mathbb{R}^n $. (Similarly for right translations and right Haar measures.) I should mention we just got to Haar measures and topological groups in my class so it has not fully sunk in, and I have no idea how to do this. Thanks all helpers","['real-analysis', 'topological-groups', 'measure-theory']"
1384576,"Finding a general solution to a differential equation, using the integration factor method","Use the method of integrating factor to solve the linear ODE $$ y' + 2xy = e^{−x^2}.$$ And verify your answer I can solve the ODE as a linear equation (mulitply both sides, subsititute, reverse product rule, integrate etc.) to obtain the answer $$
y(x) = c_1 e^{-x^2} e^{-x^2}x
$$ However could someone show me how to do this question using the integrating factor method and (subsequently verifying it using that method?)","['calculus', 'multivariable-calculus', 'ordinary-differential-equations', 'integration']"
1384581,Definition of a complex fiber,"We define a real hypersurface as a subset $M\subset\Bbb C^n$ which is locally defined as the zero-locus of some $r\in\mathcal C^2(\Omega,\Bbb R)$ ($\Omega\subseteq\Bbb C^n$ open). Then let $z_0\in M$. If $\Bbb C^n$ was $\Bbb R^n$, we can easily see that such an $M$ is a $(n-1)$-dimensional variety.
I hope in complex case it works as well. Allowing us to think that real facts works also here (and I'm well aware that this could be strongly wrong, but the only source I have is hyper-cryptic, so I hope in the good heart of someone of you!), we know the definition of tangent space to $M$ at $z_0$ (or the fiber of $z_0$): in the real case it was $\ker \Delta r(z_0)$, now my book says that it is ""the space of vectors othogonal to $\partial r(z_0)$ under hermitian product"", and denotes this with $T_{z_0}^{\Bbb C}M$. I obviously deduced that
$$
T_{z_0}^{\Bbb C}M:=\{z\in\Bbb C^n\;:\;z\cdot\overline{\partial r(z_0)}=0\}
$$
where $\partial r(z_0)=(\partial_{z_1}r(z_0),\dots,\partial_{z_n}r(z_0))$. But this can't be right because of something I found after (it deals with the well definition of the signature of the Levi form of $M$, and it would be really long to write: I think the important is to say that at this point I reached a contradiction).
The way to avoid contradictions, is to define $T_{z_0}^{\Bbb C}M$ as $\{z\in\Bbb C^n\;:\;z\cdot\partial r(z_0)=0\}$. But this doesn't seem ""the orthogonal to $\partial r(z_0)$ under hermitian product""! Can someone help me? Many thanks!","['complex-geometry', 'differential-geometry', 'several-complex-variables']"
1384638,Classifying a differential equation,"How do I classify the following differential equation?  In particular, is this differential equation ""homogeneous?"" $$(x^3+3y^2)dx-2xydy=0$$ Solving it is not the problem, but I don't know how to recognize it.","['homogeneous-equation', 'calculus', 'ordinary-differential-equations']"
1384646,Bijective map from $\Bbb Z$ to $\Bbb Q$,There  exists  a  map $f: \Bbb Z\rightarrow \Bbb Q $  such  that  $f$  is A. Bijective  and  increasing B. Onto  and  decreasing C. Bijective  and  satisfies  $f(n)\ge 0$  if $n\le 0$ D. Has  uncountable images Now  option  D.  is  absurd . Option  C.  is  given  to  be  the  correct  answer.I  was  thinking  since  both  sets  are  countable  bijection  is  obvious. Now  why  cannot  be  increasing  ? I  could  map  $0$  to  $0$  and  the  negative  integers  to the  negative  rationals and  positive  integers  to  the  positive  rationals. And  if  increasing  would  be  possible  just  interchanging signs  would  give  the  decreasing  map. So  none  is  possible  but  why?,['elementary-set-theory']
1384653,Rational solutions to $a+b+c=abc=6$,"The following appeared in the problems section of the 
March 2015 issue of the American Mathematical Monthly . Show that there are infinitely many rational triples 
  $(a, b, c)$ such that $a + b + c = abc = 6$. For example, here are two solutions $(1,2,3)$
and $(25/21,54/35,49/15)$. The deadline for submitting solutions was July 31 2015,
so it is now safe to ask: is there a simple solution?
One that doesn't involve elliptic curves, for instance?","['elliptic-curves', 'number-theory', 'diophantine-equations', 'elementary-number-theory', 'symmetric-polynomials']"
1384673,How to prove the not-so-long rays are homeomorphic to the reals?,"The long ray (half of the long line ) is an interesting topological space. It is defined as the order topology on $\omega_1$ $\times [0, 1)$ with lexicographic order . Basically, it is an uncountable number of half open intervals glued together. My question is about not-so-long rays, where you take a countable ordinal $\alpha$, and make the space $\alpha \times [0,1)$. This is claimed to be homeomorphic to $[0,1)$. How does one define this homeomorphism? I see how this can be defined for $\omega$, $\omega^2$, and others, but I don't know how to do it in general. This property is used to prove, for example, that every interval on the long line is homeomorphic to an interval on the real line.","['real-numbers', 'ordinals', 'general-topology']"
1384690,"$A+B+C=2149$, Find $A$","In the following form of odd numbers If the numbers taken from the form where $A+B+C=2149$ Find $A$ any help will be appreciate it, thanks.",['algebra-precalculus']
1384733,"Prove $ \ \frac{a}{x^3 + 2x^2 - 1} + \frac{b}{x^3 + x - 2} \ = \ 0 \ $ has a solution in $ \ (-1,1) $","If $a$ and $b$ are positive numbers, prove that the equation $$\frac{a}{x^3 + 2x^2 - 1} + \frac{b}{x^3 + x - 2} = 0$$ has at least one solution in the interval $ \ (-1,1) \ $ . The question is from the exercises section of a textbook chapter on limits/continuity. I've been stumped on this one for a couple of days. I've been trying to calculate $\lim _{x \to -1}$ and $\lim _{x \to 1}$ and then show the function is continuous to show a root must lie in the interval. Factorising the denominators gives... $$\frac{a}{(x+1)(x^2+x-1)} + \frac{b}{(x-1)(x^2+x+2)} = 0$$ So of course $x = 1$ and $x = -1$ are undefined and so the limits will be one-sided. Playing around with equation I haven't been able to find an equivalent function across $x \neq -1, x \neq 1$. The only thing I have been able to show is $$\frac{a}{b} = - \frac{(x+1)(x^2+x-1)}{(x-1)(x^2+x+2)}$$ and so $$\lim _{x \to -1} \frac{a}{b} = 0, \lim _{x \to 1} \frac{b}{a} = 0$$ but I'm not sure if this is significant or I'm overthinking things. Could anyone point me in the right direction?","['continuity', 'limits']"
1384735,How can I find an ODE equation from $dy/dx$,"What is the ODE satisfied by $y=y(x)$ given that $$\frac{dy}{dx} = \frac{-x-2y}{y-2x}$$ I understand that I need to get it in some form of $\int \cdots \;dy = \int \cdots \; dx$, but am not sure how to go about it.","['calculus', 'ordinary-differential-equations']"
1384752,Correct Form of a Logical Statement,"I ran across a problem which has stumped me involving existential quantifiers.
Let U, our universe, be the set of all people. Let S(x) be the predicate ""x is a student"" and I(x) be the predicate ""x is intelligent"".
I want to write the statement ""Some students are intelligent"" in the correct logical form. I can see 2 possible ways to write it 1)  There exists an x in U such that ( S(x) AND I(x) ) 2)  There exists an x in U such that ( S(x) implies I(x) ) If I draw a Venn diagram, it seems like option 1 must be true, but from this same diagram (where the sets where S(x) is true and I(x) is true intersect), it is also true that there is an x such that if x is in the set where S(x) us true, then x is in the set where I(x) is true. This makes me wonder if these two statements are not logically equivalent, but I have a feeling they are not. Thanks,
Matt","['logic', 'discrete-mathematics']"
1384769,Proving that $L_{22}L_{22}^T=S$ is the Schur complement of a Cholesky factorization,"Let $A$ be an $(n+m) \times (n+m)$ symmetric positive definite matrix $$A=\begin{bmatrix}A_{11} & A_{12}\\ A_{12}^T & A_{22}\end{bmatrix}$$ where $A_{11}$ is an $n \times n$ matrix, $A_{12}$ is an $n \times m$ matrix, and $A_{22}$ is an $m \times m$ matrix. We can factor $A$ into $LL^T$ , as follows: $$A=\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}^T \text{,}$$ where $L_{11}$ is an $n \times n$ , $L_{21}$ is $n \times m$ , and $L_{22}$ is $m \times m$ . I want to show that $S=L_{22}L_{22}^T$ where $S=A_{22}-A_{12}^TA_{11}^{-1}A_{12}$ is the Schur complement of $A_{11}$ . I'm stuck trying to unravel how to apply this to block matrices.  I'm having a bit of a conceptual hurdle translating my definitions to this problem.","['schur-complement', 'matrices', 'matrix-decomposition', 'cholesky-decomposition', 'linear-algebra']"
1384796,"Closed-form of $\int_0^1 x^n \operatorname{li}(x^m)\,dx$","I've conjectured, that for $n\geq0$ and $m\geq1$ integers
$$
\int_0^1 x^n \operatorname{li}(x^m)\,dx \stackrel{?}{=} -\frac{1}{n+1}\ln\left(\frac{m+n+1}{m}\right),
$$
where $\operatorname{li}$ is the logarithmic integral . Although there is a known antiderivative of $x^n \operatorname{li}(x^m)$, the simplification of the expression seems not trivial. I think there are other ways to evaluate this definite integral problem. How could we prove this identity?","['calculus', 'closed-form', 'definite-integrals', 'logarithms', 'integration']"
1384814,Solve for θ $\csc 3θ = 5 \sin 3\theta$,"Solve for $θ$, find all solutions. $\csc 3θ = 5 \sin 3θ$ I get stuck when I do this: $$\frac{1}{\sin 3θ} = 5\sin3θ$$ $$\frac{1}{5} = (\sin 3θ)^2$$ How do I simplify the right side?",['trigonometry']
1384834,Unclear why the highest argument value for cosine function $\cos(x+\frac{\pi}{4})$ is $\frac{\pi}{2}$,"The problem to solve is to find the values of $x$ with which the function $\sin(x)+\cos(x)$ will compute to its highest value. I checked the textbook's answers section: it has the function transformed to $$\sqrt{2}\cos(x-\frac{\pi}{4})$$ Up to that point, it was all clear to me. But then the solution says, This function will have the highest value with $\cos(x-\frac{\pi}{4}) = 1$ , that is, with $x-\frac{\pi}{4}=\frac{\pi}{2}+k\pi$, or with $x=\frac{3\pi}{4}+k\pi$, which means that $x=\pi-\frac{3\pi}{4}+(k-1)\pi$, or, finally, $x=\frac{\pi}{4}+n\pi$, with $n$ being a natural number of any value. But why the argument $x-\frac{\pi}{4}$ should equal $\frac{\pi}{2}+k\pi$? Isn't the cosine maximal at zero? And isn't its period $2k\pi$? Shouldn't it be The highest value of $\cos(x-\frac{\pi}{4})$ is 1, that is, with $x-\frac{\pi}{4}=2k\pi$ P.S. The textbook's answer ends with The highest value of $\sin(x)+\cos(x)$ is $\sqrt{2}$. .. so it's not ""magnitude"" I guess. Here's the textbook's solution for this problem in full: (Saveliy Tumanov, Basic Algebra , 1962)","['optimization', 'algebra-precalculus', 'trigonometry']"
1384844,What exactly IS a line integral?,"As what happens in many math courses, a topic is learned without truly learning what one is doing. For me, this is line integrals. I can do them well, I just never truly learned what exactly I was doing. Can anyone give me (in layman's terms, something extremely basic) a good definition and example of what line integrals are truly evaluating and why we do it? Thank you","['calculus', 'integration']"
1384877,Evaluate integral with gaussian curvature,"I thought evaluating it in the following way: $$\begin{align}
\int_0^{2\pi}\int_0^{\pi}K(x,y)\sqrt{\det(g_{ij})} \, dy\,dx &= \int_0^{2\pi}\int_0^\pi \sqrt{\det L_{ij}}\cdot \sqrt{{\frac{\det L_{ij}}{\det{g_{ij}}}}} \, dy\,dx\\
&= \int_0^{2\pi}\int_0^\pi \sqrt{\det{L_{ij}}}\cdot \sqrt{\det (g^{ij}L_{ij})} \,dy\,dx \\
&=\int_0^{2\pi}\int_0^\pi\sqrt{\det L_{ij}}\cdot \sqrt {\det{L^i_{\space\space j}}} \, dy\, dx
\end{align}$$ but from here I'm stuck. How can I continue evaluating the integral above?","['differential-geometry', 'coordinate-systems', 'curvature', 'integration']"
1384910,Laurent Series at Infinity,"I thought that finding the Laurent series was something that was straightforward, however, I am having some difficulty of finding the Laurent series of $$f(z) = \frac{1}{z(1-z)}$$ for $z= \infty$.  Any suggestions?","['laurent-series', 'complex-analysis']"
1384934,"If $\cos(\alpha-\beta)+\cos(\beta-\gamma)+\cos(\gamma-\alpha)+1=0$,show that $\alpha-\beta$ or $\beta-\gamma$ or $\gamma-\alpha$ is multiple of $\pi$.","This question is from SL Loney. If $\cos(\alpha-\beta)+\cos(\beta-\gamma)+\cos(\gamma-\alpha)+1=0$, then show that $\alpha-\beta$ or $\beta-\gamma$ or $\gamma-\alpha$ is a multiple of $\pi$. My try: Let $\alpha-\beta=A$, $\beta-\gamma=B$, $\gamma-\alpha=C$ so that $A+B+C=0$. So we have to prove that: If $\cos A +\cos B+\cos C+1=0$, then show that $A,B$ or $C$ is a multiple of $\pi$. $$2 \cos\frac{A+B}{2}\cos\frac{A-B}{2}+2\cos^2\frac{C}{2}=0\\
2 \cos\frac{C}{2}\cos\frac{A-B}{2}+2\cos^2\frac{C}{2}=0\\
4 \cos\frac{A}{2} \cos\frac{B}{2} \cos\frac{C}{2}=0.$$
Either $\cos\frac{A}{2}=0$ or $\cos\frac{B}{2}=0$  or $\cos\frac{C}{2}$. Either $\frac{A}{2}$=odd multiple of $\frac{\pi}{2}$ or $\frac{B}{2}$=odd multiple of $\frac{\pi}{2}$ or $\frac{C}{2}$=odd multiple of $\frac{\pi}{2}$. Either $A$=odd multiple of $\pi$ or $B$=odd multiple of $\pi$ or $C$=odd multiple of $\pi$. But the answer is: either $A$=multiple of $\pi$ or $B$=multiple of $\pi$ or $C$=multiple of $\pi$ Is my approach correct? Or is there some other method to prove it.",['trigonometry']
1384950,Prove that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $A^T$.,"Prove that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $A^T$. I'm stucked here, i've approached the problem by looking at $\det(A-\lambda I)=0\iff\det(A^T-\lambda I)=0$. I tried some cases, and I can see it when the matrix is triangular since the main diagonal remains the same when $A$ is transposed, but this hasn't shown me a way to proceed. Any hints or ideas?",['linear-algebra']
1384954,Prove that $r^3\rho=2R \rho_1\rho_2 \rho_3$,"Let $ABC $ be an acute-angled triangle.
Let $D$, $E$, $F$ be the feet of the perpendiculars from $A $, $B$, $C $ on the opposite sides $BC $, $CA $, $AB $. Let $\rho,\rho_1,\rho_2 ,\rho_3$ be the radii of the circles inscribed in the triangles $DEF$, $AEF$, $BFD$, $CDE$. Prove that $r^3\rho=2R \rho_1\rho_2 \rho_3$. Here, $r $ is the radius of the incircle of triangle $ABC$, and $R$ is the circumradius of $ ABC$.","['triangles', 'trigonometry']"
1384960,"Are sequences properly denoted as $\subset$ of a set, or $\in$ a set?","Given some sequence $(x_n)$ of some subset $M \subset \mathbb{R^n}$, is it more appropriately to denote  $(x_n) \subset M$, or $ (x_n)\in M$? This stems from confusion of using ""in"" i.e. whenever people say ""suppose  $(x_n)$ is a sequence in $M$.","['sequences-and-series', 'elementary-set-theory', 'notation', 'definition']"
1384979,What rule can I use to compute $\frac{d^{107}}{dx^{107}} \sin x$?,Did I miss something in my calculus class? I don't remember anything concerning this type of problem: Compute $$\frac{d^{107}}{dx^{107}} \sin x.$$ So what is the rule here?,"['calculus', 'derivatives']"
1384994,Rotate a point on a circle with known radius and position,"Having a circle $\circ A(x_a, y_a)$ of radius $R$ and a point on the circle $B(x_b, y_b)$ , how can we rotate the point with a known angle $\alpha$ (radians or degrees, it doesn't really matter) on the circle so we will obtain a new point on the circle $C(x_c, y_c)$ , like in the image below? How to calculate the $C$ coordinates? Here the rotation angle is $90 ^ {\circ}$ . In this example, $x_b = x_a$ , $y_b = R$ , $\alpha = 90 ^ \circ$ . From the image we see that $x_c = R$ and $y_c = y_a$ . However, I want a general solution for any $A, B, R$ and $\alpha$ .","['circles', 'geometry', 'calculus']"
1384996,"If $\alpha, \beta, \gamma$ solutions for the equation","If $\alpha, \beta, \gamma$ solutions for the equation $$x^{3}+x^{2}+1=0$$ Then $$\frac{1+\alpha }{2-\alpha }+\frac{1+\beta }{2-\beta }+\frac{1+\gamma }{2-\gamma }=??$$ I know that the answer is $\frac{9}{13}$, It's just for sharing a new ideas, thanks :)",['algebra-precalculus']
1385026,Degree of a splitting field,"I came across something related to the degree of a splitting field for a polynomial over a field $K$.  Let's suppose $p \in K[x]$ with degree $n$, and $p$ has irreducible factors $f_{1}, \ldots, f_{c}$ with respective degrees $d_{1}, \ldots, d_{c}$. Ok, I know we can construct the splitting field as a tower of extensions.  BUT here is the question: According to Wikipedia, the degree of the splitting field is $\leq n!$, but why $n!$? Is there any way to achieve this bound? I would guess that the degree would be bound by $\prod_{i} d_{i}$, which could never be this big no matter the values of the $d_{i}$.  Where is the error in my thinking?","['abstract-algebra', 'galois-theory', 'splitting-field']"
1385043,"Intuitively, what is the difference between homeomorphism and diffeomorphism? Significance?","As the title suggests, intuitively,  what is the difference between homeomorphism and diffeomorphism? Many thanks in advance. What is the significance of such a difference?","['differential-topology', 'differential-geometry']"
1385060,Is there a purpose behind a function?,"As I understand it, a function is a relation between two sets of numbers where as for every input value there is only assigned one output.  Or for every $x$ there is only one $y$. What I don't understand is why distinguish a function in a first place.  I mean we could've just as easily defined it as a relation between two sets of numbers, period. Or even done the opposite and $y$ assign one $x$. Usually things in math make sense, not just memorizing a definition. I'm just trying to figure out what's so special that condition; what it might facilitate.","['math-history', 'terminology', 'algebra-precalculus', 'functions', 'intuition']"
1385102,Can three diagonals in a $2k+1$-gon intersect?,Is it possibly to find three diagonals in a regular $2k+1$-gon that intersect? More particularly can  they intersect inside the polygon? I am looking for an elementary solution. Can one be found without using trigonometry? Any solution is welcome.,"['contest-math', 'geometry']"
1385123,Is $X(T) = A \sin(\omega_0 t + \Phi)$ mean ergodic?,"This is an example of a tutorial but I think has not been solved properly. Please help me! $X(T) = A \sin(\omega_0 t + \Phi)$ $A$ and $\phi$ are independent $A$ is uniformly distributed over $(0,1)$ $\Phi$ is uniformly
  distributed over $(0, 2\pi)$ Is it mean-ergodic? It is solved as follows: $E[X(t)]=E[A]E[\sin(\omega_0t+\Phi)]=0$ $R_X(\tau)=E[X(t)X(t+\tau)]=E[A]E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ so it is stationary. Since its mean value is zero $C_X(\tau)=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ The next step is where I have problem with. I don't know whether we can substitute the fourier transform of $f$ when computing $\int_a^b f$ or are the fourier transforms computed here correct? Even I don't know the integration has been done correctly? The solution continues as follows: We can use the result from Fourier transform that time multiplication
  is frequency convolution and evaluate the integral shown above. $$[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2\quad and\quad\frac{\cos(\omega_0\tau)}{2}\Longleftrightarrow \frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$$ Substituting the Fourier transforms of the triangular and the cosine functions in the equation for vairance, we obtain $$\begin{align}
\sigma^2_{\hat\mu_X}&=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega\\
&=\frac{E[A^2]}{2\pi}[\frac{\sin(\omega_0T)}{\omega_0T}]^2
\end{align}$$
  $T\to\infty\,,\sigma^2_{\hat\mu_X}\to 0\,,for\;\omega_0\neq 0$ Thus $X(t)$ is mean-ergodic. 1- I don't know if the fourier transform calculation $[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2$ is true? 2- I don't know whether we can compute the integral of the fourier transform of a function instead of the original function? 3- Regarding that multiplication in time is equivalent to the convolution in frequency space I don't know whether substituting $[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}$ in $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ by $2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$ in $\sigma^2_{\hat\mu_X}=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega$ is done correctly?","['means', 'ergodic-theory', 'fourier-analysis', 'statistics', 'integration']"
1385124,Why does $x^2+47y^2 = z^5$ involve solvable quintics?,"This is related to the post on $x^2+ny^2=z^k$ . In response to my answer on, $$x^2+47y^2 = z^3\tag1$$ where $z$ is not of form $p^2+nq^2$, Will Jagy provided one for, $$x^2+47y^2 = z^5\tag2$$ $$ (14p^5 + 405p^4q + 3780p^3q^2 + 13410p^2q^3 + 11550pq^4 - 14647q^5)^2 + 47  ( p^5 - 270p^3q^2 - 2520p^2q^3 - 8115pq^4 - 8344q^5)^2 = (3p^2 + 28pq  + 81q^2)^5\tag3 $$ As noticed by Elaqqad, the cubic polynomials I used for $(1)$ involve the discriminant $d=-47$ and I assumed it would be same with the (irreducible) quintic polynomials used by Jagy for $(3)$. Then I wondered if they were solvable in radicals as well . (I knew Ramanujan played with a solvable quintic with $d=-47$.) It turns out they are. This Magma calculator computes the Galois group and the command is: Z := Integers(); P < x > := PolynomialRing(Z); f := 14*x^5 + 405*x^4 + 3780*x^3 + 13410*x^2 + 11550*x - 14647; G, R := GaloisGroup(f); G; Testing both polynomials, it shows the group has order 20 and hence is solvable. Q: Given $x^2+dy^2=z^k$ where $z\neq p^2+dq^2$, is it true that if $$\big(P_1(x)\big)^2+d\big(P_2(x)\big)^2=\big(P_3(x)\big)^k$$ then the equations $P_1(x) = P_2(x) = P_3(x) = 0$ are solvable in radicals? P.S. Or is this $5$th parameterization special only because the class number $h(-47) = 5$? One way to check would be to solve $x^2+47y^2 = z^\color{red}7$ analogous to $(3)$ ( Will, care to oblige ?) and see if it involves solvable septics.","['radicals', 'number-theory', 'galois-theory', 'diophantine-equations']"
1385137,Calculate 3D Vector out of two angles and vector length,"What is the easiest way to calculate vector coordinates in 3D given 2 angles vector length? Input: Angle between X and Y axis: $$\alpha \in [0, 360).$$ Angle between Y and Z axis: $$\beta\in [0, 360).$$ Scalar length. Expected output: X, Y, Z coordinates of the vector","['angle', 'vectors', '3d', 'trigonometry']"
1385147,Longest sequence of primes with no difference greater than $2n$,"Let $N\ge 1$ be a natural number. The object is to find the longest possible
sequence of prime numbers $p_1<p_2<...<p_n$ such that $p_{i+1}-p_i\leq 2N$ for $i=1,...,n-1$. In other words, there is no difference exceeding $2n$. For $N=1$, the maximum length is $4$ because one of the numbers $n,n+2,n+4$ must
be divisible by $3$, but $2,3,5,7$ is a sequence of length $4$ with the desired
property. For $N=2$, the sequence $2,3,5,7,11,13,17,19,23$ has the desired property, so
the maximum length is at least $9$. How can I get the maximum length for each given $N$ ?","['prime-numbers', 'sequences-and-series', 'number-theory']"
1385148,Under what conditions does a specified conditional distribution exist,"It is common to see conditional distributions specified in stats like: $$(X \mid \mu = t) \sim \mathcal{N} (t, 1)$$ (Of course, we can also use some other distribution here) How do you prove that such a conditional probability actually exists, in terms of a regular conditional probability ? And is there some condition on the underlying probability space? Proofs and/or references appreciated.","['probability-theory', 'measure-theory']"
1385165,Finding fomulas for hyperbolic functions,"I'm trying to find formulas for hyperbolic functions, starting with this image Knowing that the area between the origin, vertex and a point on hyperbola (enclosed by x-axis and hyperbola itself) is equal to $\frac{\alpha}{2}$, I wrote
$$
x^2 - y^2 = 1
$$
$$
x = \sqrt{1 + y^2}
$$
... and started integrating this expression for $x$. $$
\begin{align} 
\frac{\alpha}{2} 
        & = \int^{y}_0{\sqrt{1 + y^2}\,\textrm{d}y}
     \\ & = \int^{\arctan y}_0{\sqrt{1 + \tan^2\theta}\sec^2\theta\,\textrm{d}\theta}  && \text{let }y = \tan\theta \text{ and }\textrm{d}y = \sec^2\theta\,\textrm{d}\theta
     \\ & = \int^{\arctan y}_0{\sec\theta\sec^2\theta\,\textrm{d}\theta}
\end{align}
$$
Here I used integration by parts to find $\int{sec^3\theta\,\textrm{d}\theta}$
$$
\begin{align} 
\int{\sec^3\theta\,\textrm{d}\theta}
        & = \sec\theta\tan\theta - \int{\sec\theta\tan^2\theta \,\textrm{d}\theta}
     \\ & = \sec\theta\tan\theta - \int{\sec\theta(\sec^2\theta - 1) \,\textrm{d}\theta}
     \\ & = \sec\theta\tan\theta - \int{\sec^3\theta \,\textrm{d}\theta} + \int{\sec\theta \,\textrm{d}\theta}
     \\ & = \frac{1}{2}\bigg(\sec\theta\tan\theta + \int{\sec\theta \,\textrm{d}\theta}\bigg)
     \\ & = \frac{1}{2}\bigg(\sec\theta\tan\theta + \ln{\bigg|\sec\theta + \tan\theta \,\bigg|}\bigg)
\end{align}
$$ ... and by substituting $\tan\theta = y$, $\sec\theta = \sqrt{1 + y^2}$ back I got
$$
\alpha = y\sqrt{1 + y^2} + \ln{\bigg|\sqrt{1 + y^2} + y \,\bigg|}
$$ Which doesn't make much sense, because this is not what an inverse hyperbolic sine looks like. Where have I made mistake? What should I do different? Thanks.","['area', 'angle', 'hyperbolic-functions', 'trigonometry', 'integration']"
1385179,Complete Riemannian metric on ${\mathbb R}^2\setminus\{0\}$.,"It seems to me that the Riemannian metric $g_{ij}=\delta_{ij}/|x|^2$ on the punctured plane is complete, but I don't find a proof not involving explicit computations of the geodesic equation. Does anyone know one?","['differential-geometry', 'riemannian-geometry']"
1385197,explicit formula for recurrence relation $a_{n+1}=2a_n+\frac{1}{a_n}$,"For $n\in\mathbb N$,
$$a_{n+1}=2a_n+\frac{1}{a_n},\quad a_1=1.
$$
Can any one give an explicit formula for all $a_n$? If such an explicit general formula doesn't exist, please explain it. I've tried to figure out the $n$-iterated function $f^{(n)}$ where $f(x)=2x+1/x$ or even $f(\tan(t))$. But in either cases, I failed.Since the recurrence isn't linear nor homogeneous,the generating function method doesn't apply here.","['closed-form', 'sequences-and-series']"
1385227,"Number of increasing functions from $\{1,2,\dots, n\}$ to itself.","Let $f$ be a function from $X=\{1,2,3,...,n\}$ to itself. We say $f$ is increasing if $a\le b$ then $f(a)\le f(b)$. How do we find the number of increasing functions? I think if we can define $f(1)$ then we can count. But it is very difficult.","['combinations', 'discrete-mathematics', 'combinatorics']"
1385232,does simply connectedness require connectedness?,"My question consists of two parts. $1)$ suppose domain $D=\{(x,y)\in\mathbb R^2~|~xy>0\}$ is given. Now that is first quadrant and third quadrant with exclusion of $x$ and $y$ axis. We can easily see that $D$ is not connected, since there is a discontinuity at origin. But every closed curve we can construct in domain contains interior of it (or formally, they can shrunk to a point). So do we call it simply connected, or do we also need connectedness to say $D$ is simply connected ? $2)$ Now for second part, let origin also included in domain so that $D$ is connected. Let us construct closed curve which goes through origin. Now this curve also satisfies assumption given above. But it is not simple closed curve. Does this affects simply connectedness? Do we need simple closed curves for simply connectedness?","['multivariable-calculus', 'general-topology']"
1385240,Definition of Selmer-Group for Elliptic Curves,"Im facing a problem in Silvermans Book ""Arithmetic of elliptic Curves"" at the beginning of chapter X.4 concerning the exact sequences. Let $K$ be a number field with a valutaion $v$. I'm distinguishing between the algebraic closure of $K_v$, which i denote as $\overline{K_v}$ and the completion of the algebraic closure of $K$, which is defined as $L = \bigcup L_{iv}$ with the $L_i$ ranging over all finite extensions of $K$ (as done in Neukirch).
Now to obtain the short exact sequence $(\ast)_v$ used to define the Selmer-Group in Silverman's ""Arithmetic of Elliptic Curves"", page 331: $ 0 \longrightarrow E'(K_v)/\phi E(K_v) \longrightarrow H^1 (G_v, E[\phi]) \longrightarrow H^1 (G_v, E)[\phi] \longrightarrow 0 $ I have to work with $\overline{K_v}$, as I need an algebraically closed field. 
However, how can one let $G_v\subset Gal( \overline{K} | K)$ act on $\overline{K_v}$? I know $G_v \simeq Gal(L | K_v)$, but do not see, how to use that isomorphism here. On the other side, if I work with $L$, I don't know how to identify $H^1 (G_v, E)$ with the Weil-Chatelet-Group $WC(E/K_v)$, which is in bijection to $H^1 (Gal(\overline{K_v}|K_v), E)$. Im very thankful for every advice given! For those of you not having Silverman's book at your hands, it can be found here http://www.mathe2.uni-bayreuth.de/stoll/talks/short-course-descent.pdf. The section described above begins on page 331.","['elliptic-curves', 'algebraic-geometry', 'galois-cohomology', 'algebraic-number-theory']"
1385262,Largest domain where the function $e^z/(\sin z+\cos z)$ is analytic,"I have a function $f(z) = \frac{\exp{z}}{\sin z+\cos z}$ and I need to show the region where $f(z)$ is analytic. My work so far :- As the function is the sum and product of holomorphic functions, I conclude that it is holomorphic wherever it is defined. So it defined as long as $\sin z +\cos z \neq 0$. Using identities, $\sin(z)+\cos(z) = (\sin x+\cos x)\cosh{y} + i(\cos x-\sin x)\sinh y $ Now $\sin x + \cos x = 0$ for $x=(4n+1)\frac{\pi}{4}$ and $\cos x - \sin x = 0$ for $x=(4n-1)\frac{\pi}{4}$ For case 1, $\sinh y = 0$ which occurs at $y = 0$ For case 2, no such value of y. Hence solution is the $\mathbb{C} - (x=(4n+1)\frac{\pi}{4} , y= 0)$. Is this correct?","['solution-verification', 'complex-analysis', 'exponential-function']"
1385287,"Function Can Only be Solved by Simultaneous Equations, returns different/wrong answer each time it is solved?","I'm having a lot of trouble with a specific question regarding functions, but I'm not sure where to post it.. the question is: Let  $$ y = f(x) = a x^2 + bx + c $$ and have the values ($i \in \{1,2,3\}$): \begin{align} (x_i) &= (3,1,-2) \\ (y_i) &= (32, 6, -3) \end{align} what are $a$, $b$, $c$? f(x) |-> ax^2 + bx + c, and {x: 3, 1, -2} and the range is, correspondingly, {y: 32, 6, -3} then what are the values of a, b and c? So far I have done this by using simultaneous equations, but I get a different answer each time! For example, the first time I did it, I got $a = -13/10$, $b = 17/10$ and $c = 28/5$. But if you plug these numbers in to the function, $f(1)\mapsto 6$, $f(-2) \mapsto -3$ but for some reason $f(3)$ does not return $32$?
The second time I did it, I got $a = 25/2$, $b= 31/6$ and $c = -35/3$ which literally did not work at all. Help? Please? (PS: I'm not sure if I'm allowed to post a question this simple here, since everyone else seems to be doing university-level math.. If I'm not, inform me and I'll take it down.) My Work: 4$a$-2$b$+$c$=-3 -($a$+$b$+$c$=6) =3$a$-3$b$=9 9$a$+3$b$+$c$=32 -(4$a$-2$b$+$c$=-3) =4$a$+4$b$=35 =3$a$+3$b$=21 3$a$-3$b$=9 -(3$a$+3$b$=21) = -6$b$=-12 therefore, $b$ = -2 4$a$-(-2 * -2) +c = (-3) 4$a$+4+$c$=(-3) 4$a$+$c$=-7 -($a$-2+$c$=6) =3$a$=-11 -2-(11/3)+$c$=6 So.. $c$ = (35/3)? and $a$=-(11/3) and $b$ = -2? Instead of writing my work I just did the problem again from scratch and lo and behold, a completely different answer set.. and this one's wrong, too.",['functions']
