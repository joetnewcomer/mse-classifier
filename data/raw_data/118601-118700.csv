question_id,title,body,tags
1763268,"Determine whether $w$ is in the $Span\{v_1, v_2, v_3\}$","my question is how to determine whether a vector $w$ is in the $span\{v_1, v_2, v_3\}$. In this case: $w = \begin{bmatrix}
        9  \\
        6  \\
        1  \\  
        9  \\
        \end{bmatrix}
$ and 
$v_1 = \begin{bmatrix} 1\\2\\-1\\1 \end{bmatrix}$ , 
$v_2 = \begin{bmatrix} 2\\-1\\1\\0 \end{bmatrix}$ , 
$v_3 = \begin{bmatrix} 1\\2\\0\\3 \end{bmatrix}$ My understanding so far is that I must see if $w$ can be written as a linear combination of $v_1, v_2,$ and $v_3$. To do this, I row reduced the augmented matrix of the 4 vectors to the identity matrix $I_4$ $$I_4 = \left[\begin{array}{ccc|c} 1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1 \end{array}\right]$$ but from here I am confused. I'm just trying to better understand $span$, both in a problem like this and in a conceptual sense.","['matrices', 'linear-algebra']"
1763282,The image of the transpose of $A^\text{T}$ is the orthogonal complement of its kernel,Suppose $V$ is a finite dimensional vector space over $\mathbb{K}$ . Let $A$ be a linear map. I am trying to prove that $$\operatorname{Im}A^\text{T}=(\operatorname{Ker}A)^{\perp}$$ I know one direction: $\operatorname{Im}A^\text{T} \subset (\operatorname{Ker}A)^{\perp}$ but I don’t know how to show the other direction. Can anyone help me?,['linear-algebra']
1763283,Integer Triangle Radicals conjecture,"An integer sided triangle has an area $A$. Heronian triangle areas have no radical, or radical 1. Otherwise, $4 A$ will always be of the form $a\sqrt{r}$, where $r$ is the squarefree radical of the triangle. For the first 24 squarefree numbers, here are the smallest triangles with a given radical. Now consider sequence $G$. $$G = (24,8,1,0,8,8,3,0,8,24,1,0)$$ Integer Triangle Radicals conjecture: For a triangle with radical $r$, let $j = r \mod{12}$. Then value $4 A/\sqrt{r}$ is divisible by $G_j$ (with $j \in \{1,...,12\})$. Stated another way, $G_j$ is the GCD of all $4 A/\sqrt{r}$ values corresponding to a particular radical $r$. Another picture: I've got data from a few million integer triangles suggesting this conjecture is true (supporting code at Triangle Radicals ). All Heronian triangles have area divisible by 6, based on the Brahmagupta parametric form . If this conjecture is true, that hints there might be a more powerful parametric form that classifies all integer triangles. Does a proof exist? Mini-challenge: For radical 953, my scalenes are {{528, 681, 697}, {533, 680, 693}, {561, 656, 689}}. For radical 977, my scalenes are {{567, 690, 697}, {612, 657, 685}}. Can someone find triangles with radicals 953 or 977 where $4A$ is divisible by 8, but not 16? This came out of studying Wheel Graphs with Integer Edges .  I noticed all integer-edged wheel graphs with largest edge $<10$ were composed of triangles with exactly 1 or 2 types of radical. Any counterexamples to that?","['number-theory', 'recreational-mathematics', 'triangles', 'geometry']"
1763292,Show that the curve $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ form an ellipse,"If the definition of an ellipse is the set of points $(x,y)$ such that given two focus points $F_1, F_2$ the sum of the distances from $(x,y)$ to each focus point is constant, how can one show that the curve $\dfrac{x^2}{a^2} + \dfrac{y^2}{b^2} = 1, \quad 0 < b \leq a \quad$ forms an ellipse? The methods that I know of are to either derive the formula by considering the foci $(-c,0), (c,0)$ and the constant distance $2k$, or to set $(x,y) = (a\cos{v}, b\sin{v})$. Is there some other, relatively easy way to show that the points satisfying the equation form an ellipse? I am asking because in a book I am reading the author states that a ""direct calculation"" shows that the curve indeed forms an ellipse, but I do not understand what kind of calculation this might be.","['algebra-precalculus', 'conic-sections']"
1763296,On the linear combination of $\pm 1$ random variables,"Let $X_1,\dots, X_n$ be i.i.d symmetric $\pm 1$ random variables, i.e. $X_j$ takes values in $\{-1,1\}$ with
$$\mathbb{P}(X_j=1)=\mathbb{P}(X_j=-1)=\frac{1}{2}.$$
Let $a_1,\dots,a_n\in\mathbb{Z}$, define
$$X=\sum_{j=1}^n a_jX_j.$$
How can we prove that
$$\mathbb{P}(X=0)=\frac{1}{2\pi}\int_0^{2\pi}\prod_{j=1}^n\cos(a_jt)dt.\quad(*)$$ I noticed that the integrant in the right hand side of $(*)$ is the characteristic function of the random variable $X$, so I am thinking about using the inverse formula:
$$\frac{\mathbb{P}(X=a)+\mathbb{P}(X=b)}{2}+\mathbb{P}(a<X<b)=\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\varphi_X(t)dt$$
where $\varphi_X(t)$ is the characteristic function of $X$. However I didn't see the connection between the inverse formula and the result we want to prove. Any suggestions?","['probability-theory', 'fourier-transform', 'random-variables']"
1763299,The second isomorphism theorem for C*-Algebras,"in my functional analysis class right now we are studying the basics of C* Algebras and I was recently asked this question about the second isomorphism theorem for C* Algebras, but first let me cite the definition of a C* Algebra ismorphism: Abstract characterization of C* Algebras Now we are asked this: Let A be a C* Algebra and $ B \subset A $ a C* subalgebra of A and I an ideal. 
  We are also reminded of this result: We know that B+I is a C* subalgebra of A.
  We are asked the following: We are to show the following C* isomorphism: $ B/(B \cap I) \cong (B+I)/I $ I am stuck in part b as I do not really know how to show a bijective bounded (continuous) linear map which preserves multiplication and the * operation. I really need someone out there to show me how to find and prove a C*-isomorphism as desired. Also here is a related post here: sum of C*-subalgebra and ideal","['c-star-algebras', 'banach-algebras', 'functional-analysis', 'operator-algebras', 'ideals']"
1763301,"Langrange Multiplier, to find maximum volume of a cone","Question: A right-angled triangle is rotated about one of its sides that form the right angle to a cone. Given that the sum of the lengths of two sides of the triangle that form the right angle is $P$, find these lengths that would maximize the volume of the cone. My attempt: I know there must be a use of langrange multipliers but I am not too sure how to implement it.","['multivariable-calculus', 'calculus']"
1763330,Finding the kernel of an epimorphism onto $S_3$?,"Let $\Lambda$ denote the group with presentation $\langle a,b \mid 
 abab^{-1}a^{-1}b^{-1}\rangle$. We define the following epimorphism
  from $\Lambda$ onto $S_3$: $\theta: \Lambda(a,b) \rightarrow S_3$ using $a \mapsto (12)$ and $b
 \mapsto (23)$. We have $(12)(23)(23)^{-1}(12)^{-1}(23)^{-1}=e$
  thus indicating that our homomorphism factors through the quotient
  modulo the normal subgroup $H$ of $F$ generated by
  $abab^{-1}a^{-1}b^{-1}$ and, by definition, $\Lambda=F/H$. Find an
  explicit, finite presentation for the kernel, $\kappa$, of the
  epimorphism I know the kernel of $\theta$ is the set of all elements of $\Lambda$ that are mapped to $e\in S_3$ and is a normal subgroup of $\Lambda$. I'm a little new to finding kernels so I'm not sure how to proceed. What's a good way to find the kernel of this epimorphism?","['abstract-algebra', 'group-theory']"
1763334,"What is the probability Amy wins a lottery prize for correctly choosing 5, not six, numbers...","Here is the full question: What is the probability that Amy wins a lottery prize for correctly choosing 5, not six, numbers out of six integers chosen at random from the integers between 1 and 40 inclusive? So, what I've gotten so far is that the total number is C(40, 6): Out of 40, choose 6 cards. Next, I've gotten that, out of the 6 cards chosen out of 40, choose 5: C(6,5). So, should the probability be $\frac{C(6,5)}{C(40,6)}$ ?","['combinatorics', 'probability', 'discrete-mathematics']"
1763351,"Continuous function injective over a compact set, and locally injective on each point of the set","Suppose we have a function $F: \mathbb R^n \rightarrow \mathbb R^k$ continuous over some open set $U \in \mathbb R^n$, and let compact set $K \subset U$. $F$ satisfies the following properties: 1) F is injective over K 2) For every $x \in K$, there is some open set $U_x$ such that F is injective on $U_x$. Show that there exists an $\epsilon > 0$ such that F is injective over the set $\{x\in \mathbb R^n | dist(x,K) < \epsilon \}$. My attempt: I can form an open cover of $K$ by forming  $W = \cup(U_x \cap U)$, then consider the boundary of this open cover $W$. The boundary is certainly compact, so $dist(x,K)$ over it takes on a minimum (it's easy to show this value is greater than 0), we may call it $\epsilon$. But now I am stuck. I am not sure how to show that F is injective on all of $W$...","['general-topology', 'real-analysis', 'analysis']"
1763388,Show that there's a $A_n \downarrow \emptyset$ with $\lim_{n \to \infty} \mu(A_n) \neq 0$,"I need some help solving the following problem from basic measure theory, it can be found on Probability & Measure Theory, Ash, section 1.2 (for reference): Let $\mu$ be counting measure on $\Omega$, where $\Omega$ is an infinite set. Show that there is a sequence of sets $A_n \downarrow \emptyset$ with $\lim_{n\to \infty} \mu(A_n) \neq 0$. Where $A_n \downarrow \emptyset$ means that the sequence $A_1, A_2, \ldots$ is such that $A_k \supset A_{k+1}$ and $\bigcap_{k = 1}^\infty A_k = \emptyset$. My idea is to extract a countably infinite set $C \subset \Omega$, and create a sequence $A_1, A_2, \ldots$ with $A_1 = C$ and $A_{k+1} = A_k - \{a\}$ where $a \in A_k$, so that each subsequent set of the sequence ""has"" one less element than the last. Now, my intuition tells me that each element of the sequence has an infinite number of elements, thus $\lim_{n\to \infty} \mu(A_n) \neq 0$, and the infinite intersection satisfies $\bigcap_{k = 1}^\infty A_k = \emptyset$, because the set only has a countably infinite number of elements. But how do I justify this? If
$$\lim_{n \to \infty} \bigcap_{k = 1}^n A_k= \bigcap_{k = 1}^\infty A_k$$
why would $\lim_{n\to \infty} \mu(A_n) \neq 0$ be true if I'm taking a limit here as well?","['probability-theory', 'measure-theory']"
1763411,Functions : $ f(x) = {2x-1\over x^2} $,We have : $$ f(x) = {2x-1\over x^2} $$ 1- Determine $ D_f $ and solve the equation $ f(x) = 1 $ 2- Show that for every $ x $ from $\mathbb{R}^*_+ $ ; $f(x) \le 1 $ The first exercise is already done and here are my solutions : $$ D_f  = \mathbb{R} - \{0\} $$ By solving $  f(x) = 1 $ I've got $ x = 1 $ Sorry I didn't have much time to write how I did it. The second exercise is my real problem cause I didn't even understand the question to answer it. I did a few drafts : For every $ x $ from $\mathbb{R}^*_+ $  Could it mean that $ x>0 $ ?,['functions']
1763428,Integral (Tanh and Normal),"I am trying to evaluate the following: The expectation of the hyperbolic tangent of an arbitrary normal random variable. $\mathbb{E}[\mathrm{tanh}(\phi)]; \phi \sim N(\mu, \sigma^2)$ Equivalently: $\int_{-\infty}^{\infty} \mathrm{tanh}(\phi)\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(\phi-\mu)^2) d\phi$ I've resorted to Wolfram Alpha, and I can sometimes (!) get it to evaluate the integral for $\sigma^2 = 1$. It gives: $\exp(\mu) - 1$ for negative $\mu$ and $1-\exp(-\mu)$ for positive mu. I have no idea how it got this, but it seems plausible as I've done some simulations (i.e. lots of draws from the normal distribution and then the mean of the tanh of the draws) and the formula it gives for the $\sigma^2 = 1$ case seems pretty close. I cannot get it to evalute anything with a different variance. Mathematica also seems unwilling to compute the integral or the Fourier transform (which I thought might be a way forward) but this is tricker than the ones I know how to deal with. Thanks!","['hyperbolic-functions', 'fourier-transform', 'calculus', 'normal-distribution']"
1763454,Helmholtz decomposition of a vector field on surface,"Does it make sense to do Helmholtz decomposition of a vector field defined on a surface or on a manifold? I am mostly interested in the surface case. I was trying to find a reference for this and found only a handful of them mostly from electromagnetics literature. e.g. Scharstein, Robert W. ""Helmholtz decomposition of surface electric current in electromagnetic scattering problems."" System Theory, 1991. Proceedings., Twenty-Third Southeastern Symposium on. IEEE. On the related note, is there a well defined curl operator for a (tangential) vector field defined on a surface?","['mathematical-physics', 'electromagnetism', 'differential-geometry', 'vector-analysis']"
1763483,(Elegant) proof of : $x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x} \geq 1- (1-\frac{x}{1-x})^2$,"I am looking for the most concise and elegant proof of the following inequality:
$$
h(x) \geq 1- \left(1-\frac{x}{1-x}\right)^2, \qquad \forall x\in(0,1)
$$
where $h(x) = x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x}$ is the binary entropy function . Below is a graph of the two functions. Of course, an option would be to differentiate $1,2,\dots,k$ times, and study the function this way — it may very well work, but is not only computationally cumbersome, it also feels utterly inelegant. (For my purposes, I could go this way, but I'd rather not.) I am looking for a clever or neat argument involving concavity, Taylor expansion around $1/2$, or anything — an approach that would qualify as ""proof from the Book.""","['information-theory', 'real-analysis', 'inequality', 'entropy']"
1763503,Hypothesis test in Bayesian statistics,"Let $X\sim N(\theta,1)$ and 5 independent observations
  $X=(4.9,5.6,5.1,4.6,3.6)$. The prior probability that $\theta=4.01$ is
  $0.5$. The remain values of $\theta$ are given the density of
  $g(\theta)$. a)Assume $g(\theta)\sim N(4.01,1)$ test the hypothesis
  $$H_0:\theta=4.01\space vs\space H_1:\theta\neq 4.01$$ From what I learn to make a hypothesis test I need to find $$a_0=P(\theta\in\Theta_0|x)$$
such that $$a_0+a_1=1$$
and reject $H_0$ if $$a_0<a_1$$
in the cases where the null hypothesis is not a point I can make, but in this case I have a few doubts. From the notes that I take there is the theorem below Theorem: For any prior $$\pi(\theta)=\pi_0\space \text{if}\space \theta=\theta_0$$ $$\pi(\theta)=\pi_1 h(\theta)\space\text{if}\space
 \theta\neq \theta_0$$ such that $$\int_{\theta\neq
> \theta_0}g(\theta)d(\theta)=1$$ then $$a_0=f(\theta|x)\geq
 [1+\frac{1-\pi_0}{\pi_0}\frac{r(x)}{f(x|\theta_0)}]^{-1}$$ where
  $$r(x)=sup_{\theta\neq\theta_0}f(x|\theta)$$ usually
  $$r(x)=f(x|\hat{\theta})$$ In this case $\hat{\theta}=\overline{X}$ but the distribution of $$f(x|\overline{X})$$
doesn't make sense to me, in one example that I look they take $$f(\overline{x}|\hat{\theta})$$
but I don't understood the logic. I need to use the distribution of the likelihood estimator supposing that $\theta=\hat{\theta}$? If someone can give me a explanation with details on how it works I really appreciate.","['bayesian', 'statistics', 'hypothesis-testing', 'normal-distribution']"
1763506,"Question Regarding Proof of Taylor Remainder Theorem in Tu's ""An Introduction to Manifolds""","The statement: Let $f$ be a $C^{\infty}$ function on an open set $U\subseteq \mathbb{R}^n$ which is star shaped with respect to a point $p=(p^1,...,p^n) \in U$. Then there are functions $g_1(x),...,g_n(x)\in C^{\infty} $ such that $$f(x)=f(p)+\sum_{i=1}^n(x^i-p^i)g_i(x)$$, where $g_i(p)=\dfrac{\partial{f}}{\partial{x^i}}(p)$. The proof of the statement follows from the fact that since $U$ is star shaped, the line segment between $p$ and any point $x\in U$ is contained in $U$. Therefore for $0\leq t\leq 1$, $f(p+t(x-p))$ is defined. This upcoming portion is where I don't understand his application of the chain rule, and would appreciate some clarification. ""By the chain rule $$\dfrac{d}{dt}f(p+t(x-p))=\sum (x^i-p^i)\dfrac{\partial{f}}{\partial{x^i}}(p+t(x-p))$$ "" From my understanding, the left hand side of the equality would come from the chain rule when $f$ is a function of $x=(x^1,...,x^n)$, and each $x^i$ is a function of $t$. But in this instance each $x^i$ is not a function of $t$ as they are both used in the same equation to describe a line segment, so why can the chain rule be applied in this instance?","['multivariable-calculus', 'taylor-expansion', 'differential-geometry']"
1763518,Every proper maximal subgroup of a $p$-group $P$ is normal and has index $p$.,"Every proper maximal subgroup of a $p$-group $P$ is normal and has index $p$. I tried to search online by I can't get a complete proof. Take $M$ to be maximal and $Z$ to be central subgroup of order $p$. Case 1: $Z\subset M$ We prove by induction on $|P|$. For the base case, $|P|=p$. There is no maximal subgroup in $P$, hence the result holds Assume that the result holds for all $p$-group $Q$ with $|Q|<|P|$ Note that $|Z|>1$. Hence $|P/Z|<|P|$. Suppose that $M/Z$ is not a maximal subgroup of $P/Z$. Then there exists a subgroup $H$ such that $M/Z\subset H/Z \subset P/Z$. But this also implies that $M\subset H \subset P$. This means that $M$ is not maximal, a contradiction. Hence $M/Z$ is a maximal subgroup of $P/Z$. By induction hypothesis, $M/Z$ has index $p$ and $M/Z\trianglelefteq P/Z$. First, we can conclude that $M \trianglelefteq P$. Second, note that $[P/Z:M/Z]=p$. Hence $[P:M]=p$. Case 2: $Z\not\subset M$ This implies that $P=ZM$. Let $p=zm\in P$ where $z\in Z$ and $m\in M$. Clearly, $p^{-1}Mp=M$. So we see that $M \trianglelefteq P$. But for this case, I have no idea on how to prove $M$ has index $p$.","['abstract-algebra', 'group-theory']"
1763529,is it possible to choose points on the graph of $y = x^2$ to form vertices of an equilateral triangle?,"is it possible to choose points on the graph of $y = x^2$ to form vertices of an equilateral triangle $\Delta ABC$?where three ponit not $(0,0)$ .and find $(S_{\Delta ABC})_{min}$",['geometry']
1763546,Two Equilateral Triangles and the Golden Ratio: Simple Geomtric Proof,"Two equilateral triangles rest upon the same horizontal line, as shown in the linked figure below. The red triangle is tilted so that one corner touches a side of the black triangle at the midpoint of the side.  A perpendicular line is drawn from the midpoint of the right side of the red triangle through the point where the red triangle touches the black triangle, until it reaches the left side of the black triangle.  The line is divided into a yellow segment and a blue segment at the point where the two triangles touch.  What is the ratio between the yellow and blue lines?  (Hint: It should be the golden ratio ~1.681....)","['golden-ratio', 'euclidean-geometry', 'trigonometry', 'geometry']"
1763597,"Please prove the following: Given $ƒ(x) = e^x$, verify that $\lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x$.","Given $ƒ(x) = e^x$, verify that 
$$\lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x$$  and explain how this illustrates that $f'(x) = \ln e \cdot f(x) = f(x)$.","['derivatives', 'calculus']"
1763602,Show that $\lim_{h\to 0}\frac{e^h-1}{h} = \ln e = 1$ using at least two numerical examples.,Show that $$\lim_{h\to 0}\frac{e^h-1}{h} = \ln e = 1$$ using at least two numerical examples. To solve this should I find numbers for $h$ that makes those equations equal to $1$? And how should I go about finding the number? Guess and check?,"['derivatives', 'calculus']"
1763604,"Prove that if $A$, $B$, and $C$ are sets then $(A - B) \cup (A - C) = A - (B \cap C)$ [duplicate]","This question already has answers here : de morgan law $A\setminus (B \cap C) = (A\setminus B) \cup (A\setminus C) $ (3 answers) proof of $ A - \left (B \cap C \right)= \left (A - B \right) \cup \left (A - C \right)$ (4 answers) Closed 8 years ago . Prove that if $A$, $B$, and $C$ are sets then $(A - B) \cup (A - C) = A - (B \cap C)$. I have the proof for the first direction: Let $x \in (A - B) \cup (A - C)$ be given. Hence, $x \in (A - B)$ or $x \in (A - C)$. Suppose $x \in (A - B)$. Hence, $x \in A$ and $x \notin B$. Since $x \notin B$ it is logically true that $x \notin (B \cap C)$. So, $x \in A$ and $x \notin (B \cap C)$. Hence $x \in A - (B \cap C)$ by definition. A similar argument works in the case where $x \in (A - C)$. So, $(A - B) \cup (A - C) = A - (B \cap C)$. I'm confused as to how to go about proving the other direction. Would I assume that $x \in A - (B \cap C)$?","['proof-explanation', 'elementary-set-theory', 'proof-verification']"
1763627,"Adelic definition of ""canonical divisor""","For a function field over a curve $F/K$, some book define the canonical divisor as the divisor of a map $\omega:\mathscr{A}_{F}\rightarrow K$ (where $\mathscr{A}_{F}$ is the pre-adele, ie. adele but without completion at each prime) where $\omega$ is a continuous linear map that is trivial on the diagonal embedding of $F$. In Tate's thesis, canonical divisor is hidden in the non-duality nature of measure of adele. This suggests that there should be a definition of canonical divisor that use adele (instead of pre-adele like in previous case) that is more explicit (unlike the measure in the second case). But I can't find anything on this. Anyone know?","['algebraic-number-theory', 'algebraic-geometry']"
1763635,Decoupling coupled differential equations with time dependent coefficients,"Consider the following system of coupled differential equation. $$\left[ \begin{array}{c} \frac{dc_1}{dt} \\ \frac{dc_2}{dt} \end{array} \right] = \begin{bmatrix} -B & -V(t) \\ -V(t) & B \end{bmatrix} \times \left[ \begin{array}{c} c_1 \\ c_2 \end{array} \right]$$ I tried diagonalyzng the matrix using the eigenvectors of the coefficient matrix. But, since the matrix is time dependent so are its eigenvectors. So, how can one decouple this system?","['ordinary-differential-equations', 'coupling']"
1763637,1-How my profesor reach this solution? 2-How can I use eigenvalues to compute betas?... if there is any way,"this time I quite don't undertand how the profesor avoid using matrix algebra to solve this exercise. Statement: Below you can see a scatter plot of the data with the three regression
  lines corresponding to the three levels of the categorical predictor.
  Using the same two dummy variables you dened in (a), write down the
  tted regression model (read slope estimates o the graph). You do not
  have to make a statement about the residuals. Note, that the general
  regression model with interaction for this case is Graph: Questions: **
1) What is the procedure the proffesor utilized? 2) How one can use quadratic form and eigenvalues to compuete the betas wich I belibe will be my eigeinvalues for each eigenvector.
I hope I could explain my problem, thanks so much for your help.**","['descriptive-statistics', 'statistics', 'linear-algebra']"
1763666,Periodic solutions of $x'=x^2-1-\cos t$,"Consider $x'=x^2-1-\cos t$. What can be said about the existence of periodic solutions for this equation? I'm not sure if periodic solutions exist, but if they do, they must have period equal to $ 2\pi$ and $x(0)=x(2k\pi)$ for $\forall k\in\mathbb Z$. New Edit: I guess that may use the following lemma: Lemma: Consider the differential equation $x' = f (t , x)$ where $f(t,   x)$ is continuously differentiable in $t$ and $x$. Suppose that
   $f (t + T, x) = f (t , x)$
    for all t . Suppose there are constants $p$, $q$ such that
    $f (t , p) > 0, f (t , q) < 0$
    for all $t$ then there is a periodic solution $x(t )$ for this equation  with
     $p < x(0) < q$. Realy, I consider $p=2$ and $q=0$ but $f(t,q)=-1-\cos t\leq 0$ and this inequality is not strictly.","['ordinary-differential-equations', 'dynamical-systems']"
1763677,How to show that a continous function $f:\mathbb{R}^m \to \mathbb{R}$ has a maximum?,"My task is this: Suppose $f:\mathbb{R}^m \to \mathbb{R}$ is a positive, continous function such that $\lim_{\mid \textbf{x}\mid \to \infty} f(\textbf{x}) = \textbf{0}$. Show that $f$ has a maximum. I am not sure exactly how to show this since the domain of $f$ is all of $\mathbb{R}^m$. According to the extreme value theorem, we need $A\subset \mathbb{R}^m$ to be closed, bounded and that $f:A\to\mathbb{R}$ to be continous (which it is over the entire $\mathbb{R}^m$). I am thinking that if we could somehow split up the domain into closed sets and take the union, one could probably show that $f$ had a maximum, but I am not sure if that's the right approach. Any help would be more than welcome!",['multivariable-calculus']
1763678,How many integers from $43523$ to $93107$ contain at least one digit $7$,"How many integers from $43523$ to $93107$ contain the digit $7$ at least once? I know that if we had $43000$ to $93000$, we would subtract integers that do not contain digit $7$ from the total number:  $50000 - (5\times9\times 9\times 9 \times 9)$ But how to do it with the given numbers?",['combinatorics']
1763750,Number of ways of visiting N places,"A tourist wants to visit $N$ cities, each numbered from $1$ to $N$, but he wants to visit them in a weird order. A weird order is such in which no city numbered $i$ is the $i$-th to visit in his travel itinerary. He wants to visit each city exactly once. What are the number of possible weird orders for his travel?","['permutations', 'combinatorics']"
1763776,Integral of bounded function with limit zero at $\pm \infty$,"Very simple question here, I almost feel bad for asking it.. Lets say we have a function bounded between $0$ and $1$. This function is high dimensional: $0<f(X) \le1, ~~~ X \in \mathbb{R}^D$ Now, we calculate the limit for all elements of $X$ going to plus and minus infinity. We find out that they are zero. Can we say that the integral of the function over the entire domain of $X$ is finite? Can we say that if we get even non-zero limit? Finally, if the zero limit is insufficient, is there some other condition that suffices?","['potential-theory', 'calculus', 'multivariable-calculus', 'integration', 'vector-spaces']"
1763803,"If $BA = I$, prove that $AB = I$ (using determinants)","I've seen this problem around here, but I wanted to check if this particular solution is right. So, if $BA = I$, then $det(B)det(A) = 1$, meaning neither $det(B)$ or $det(A)$ are equal to $0$. Because $det(B) \neq 0$, $B$ must be invertible, which means $CB = I$ for some matrix $C$. Next, consider $CBA$. $BA = I$, so $CBA = C(I) = C$. $CB = I$, so $CBA = (I)A = A$ $CBA = C = A$. Now knowing that $C = A$, I can substitute $A$ in for $C$ to get $CB = AB = I$, which is what I wanted to prove.","['matrices', 'proof-verification', 'determinant']"
1763826,Is it possible to construct Hausdorff compact topology on every set?,"I'd like to know if it's possible to construct Hausdorff compact topology on every set. Assume the axiom of choice if needed.
Thanks for ideas.","['general-topology', 'compactness']"
1763853,"For which $s$ is defined $\frac{1}{\zeta(s)}\sum_{n=1}^\infty\frac{n}{ \left( \zeta(s) \right) ^n}$, where $\zeta(s)$ is the Riemann Zeta function?","Unless I'm making a mistake, if one of the factors $\sum_{n=1}^\infty a_n$ of a convolution product or Cauchy product converges, and the other $\sum_{n=1}^\infty b_n$ corverges absolutely then the product $$\sum_{n=1}^\infty \sum_{k=1}^{n} a_{k}  b_{n-k+1}$$
converges. For $n\geq 1$ I've computed this product, without formality , for $a_n=b_n=\frac{1}{(\zeta(s))^n}$ where $\zeta(s)$ is the Riemann zeta function and those $s$ such that $\zeta(s)\neq 0$ (for example I know that for $\Re s>1$ the Riemann Zeta funciton doesn't vanish). Then I've computed  without formality 
$$ \left( \sum_{n=1}^\infty\frac{1}{ \left( \zeta(s) \right) ^n}\right)^2= \frac{1}{\zeta(s)}\sum_{n=1}^\infty\frac{1}{ \left( \zeta(s) \right) ^n} \sum_{i=1}^{n} 1=\frac{1}{\zeta(s)}\sum_{n=1}^\infty\frac{n}{ \left( \zeta(s) \right) ^n}.$$ Question. For wich $s$ does make sense the previous product? I say for some easy region in the whole plane, if my problem is well posed you can study the more advance topic, but to me is enough for easy regions. Thanks in advance. My only computation was that I belive that for those $s\neq 0$ with $|\Re\zeta(s)|>1$ I can write 
$$ \left|  \sum_{n=1}^\infty\frac{1}{ \left( \zeta(s) \right) ^n}\right| \leq \frac{1}{1-|\zeta(s)|}-1=\frac{|\zeta(s)|}{|\zeta(s)|-1}.$$","['absolute-convergence', 'cauchy-product', 'riemann-zeta', 'convergence-divergence', 'sequences-and-series']"
1763885,My formula for sum of consecutive squares series?,"I stumbled upon a specific series, who's Sum of squares of consecutive integers equals the sum of squares of the continuation of that consecutive integers. For exmaple, this first number in the series results in: $3^2 + 4^2 = 5^2$. The second results in: $10^2 + 11^2 + 12^2 = 13^2 + 14^2$. The thrid: $21^2 + 22^2 + 23^2 + 24^2 = 25^2 + 26^2 + 27^2$. etc. The formula for the series is: By using; $1^2 + 2^2 + ... + n^2 = \frac{n(n+1)(2n+1)}{6}$ it is easy to prove that both side are the same. Have I found something new? Useful? I have a feeling that this is the only series with this specific property (consecutive sum of squares equal continuation of consecutive sum of squares), but I am unsure how to prove that. Any comments would be appreciated.","['sums-of-squares', 'proof-verification', 'summation', 'sequences-and-series', 'elementary-number-theory']"
1764000,Is there a simpler function with this shape?,"I need a function that has the shape shown below. I don't care what the function does for $x < 0$ or $x > 1$. I've experimented with a lot of different functions, configured first and second derivatives, and came up with this little monster. But I suspect there's something simpler. $$\frac{1 + \sin \left[\frac{\pi}{2}(544x + 81)^{1/4}\right]}{2}$$ The origin of this problem is that I'm trying to turn a difference metric between a pattern and a model into a sort of ""probability"" that the model and the pattern are a bad match. So it should be zero for the model that best matches the pattern, and low for very similar patterns, but rising quickly for patterns that are less similar. But as I look at it, it reminds me of a gravity well.",['functions']
1764008,IMO Shortlist 1995 G3 by inversion,"The incircle of $\triangle ABC$ is tangent to sides $BC$, $CA$, and $AB$ at points $D$, $E$, and $F$, respectively. Point $X$ is chosen inside $\triangle ABC$ so that the incircle of $\triangle XBC$ is tangent to $BC$ at $D$, to $CX$ at $Y$, and to $XB$ at $Z$. Prove that $EFZY$ is a cyclic quadrilateral by inversion . I drew the mappings of all points and lines in the original diagram under inversion, but now I am stuck. Could someone provide a hint (not too many spoilers please..)?","['contest-math', 'geometry']"
1764012,Can we detect smoothness of a norm by its behavior along paths?,"We say a norm $\| \cdot \|$ on $\mathbb{R}^n$ is smooth if it is smooth as a function $\mathbb{R}^n\setminus \{0\} \to \mathbb{R}$. (i.e, after restricting the domain). We say a norm is smooth along paths , if for any smooth path $\alpha:I \to \mathbb{R}^n \setminus \{0\}$, the composition$\|\cdot\| \circ \alpha$ is smooth. Does smoothness along paths of a norm implies that it's smooth? (Note that in general differentiability along paths is not enough to detect smoothness , but in this case we are talking about a norm, not a general function). Remarks: 1) Since any norm is Lipschitz (w.r.t the metric induced by it), it follows that the value of the derivative $\frac{d}{dt}\big|_{t=0} \|\alpha(t)\| $ depends only on $\dot \alpha(0),\alpha(0)$ and not on other properties of the specific path chosen. (See a proof at the end). Thus, for each $a \in \mathbb{R}^n\setminus\{0\}$, there is a function:
$T_a:\mathbb{R}^n \to \mathbb{R}$ defined by: $T_a(v)=\frac{d}{dt}\big|_{t=0} \|\alpha(t)\|$ where $\alpha(t)$ is any path satisfying $\alpha(0)=a,\dot\alpha(0)=v$. It is proved here that if $T_a$ is linear, then $\|\cdot  \|$ is differentiable at the point $a$. So, proving linearity of $T_a$ (for every $a$) will show at least that $\|\cdot\|$ is one-time differentiable. (Showing harder degrees of regularity, i.e twice differentiability etc sounds more challenging, but let's start with that). So far, using homogeneity and triangle inequality, I managed to prove:
$$T_a(\lambda v)=|\lambda|T_{\frac{a}{\lambda}}(v) \, \, \forall \lambda \in \mathbb{R},$$ $$T_a(v+w)\le T_a(v)+\|w\|\, \, , \, \,T_a(v+w) \le T_{\frac{a}{2}}(v)+T_{\frac{a}{2}}(w)$$ Proof of claim (1): (The derivative depends only on the velocity of the path) $$\big| \|\alpha(t)\|-\|\tilde \alpha(t)\| \big| \le \|\alpha(t)-\tilde \alpha(t) \|,$$ so assuming $\tilde \alpha(0)=\alpha(0),\dot{\tilde \alpha}(0)=\dot \alpha(0)=v \in \mathbb{R}^n$ we get: $$(*)\, \, \bigg| \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}-\frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}\bigg| \le \|\frac{\alpha(t)-\tilde \alpha(t)}{t} \| $$ Now since $\lim_{t \to 0}\frac{\alpha(t)-\tilde \alpha(t)}{t} =\lim_{t \to 0}\big( \frac{\alpha(t)- \alpha(0)}{t}-\frac{\tilde \alpha(t)-\tilde \alpha(0)}{t}\big)=v-v=0$ we get by the continuity of the norm, together with $(*)$ we obtain the required equality $$ \frac{d}{dt}\big|_{t=0} \|\alpha(t)\|  = \lim_{t \to 0} \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}=\lim_{t \to 0} \frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}=\frac{d}{dt}\big|_{t=0} \|\tilde \alpha(t)\|$$","['multivariable-calculus', 'normed-spaces', 'smooth-manifolds']"
1764013,Rearrangements that never change the value of a sum,"Which bijections $f:\{1,2,3,\ldots\}\to\{1,2,3,\ldots\}$ have the property that for every sequence $\{a_n\}_{n=1}^\infty$,
$$
\lim_{n\to\infty} \sum_{k=1}^n a_k = \lim_{n\to\infty} \sum_{k=1}^n a_{f(k)},
$$
where ""$=$"" is construed as meaning that if either limit exists then so does the other and in that case then they are equal? It is clear that there are uncountably many of these. Might it just be that $\{f(n)/n : n=1,2,3,\ldots\}$ is bounded away from both $0$ and $\infty$? These bijections form a group.  Can anything of interest be said about them as a group? PS: Here's another moderately wild guess (the one above appears to be wrong): Might it be just the bijections whose every orbit is finite?","['permutations', 'conditional-convergence', 'sequences-and-series']"
1764025,"Intuitive explanation for the connection between Lie Groups and projective spaces over $\mathbb{R}$, $\mathbb{C}$, and $\mathbb{H}$","In this post John Baez states that the classical simple Lie groups ""arise naturally as symmetry groups of projective spaces over $\mathbb{R}$, $\mathbb{C}$, and $\mathbb{H}$"". Is there some intuitive explanation for this connection? I understand that $SO(n)$ is the set of all rotations about the origin of $n$-dimensional Euclidean space $\mathbb{R}^n$ I understand that the rotation group $O(n+1)$ is the group of isometries of the $n$-sphere. Rotations are the subset with determinant $1$, whereas reflections are the subset with determinant $-1$. However I don't understand how projective spaces come into play here? Why are they important in this context?","['projective-geometry', 'projective-space', 'group-theory', 'lie-algebras', 'lie-groups']"
1764051,Sufficient Boundary Condition to a General PDE on a General Domain,"We know that for an ODE of $n^{th}$  order we need $n$ different boundary conditions. In PDEs, for example, for Laplace equation $\nabla^2 U=0$ (which is a second order PDE) we need only one B.C. (e.g Dirichlet or Neumann boundary conditions on the boundary $\partial \Omega$.) $\large {\frac{\partial U}{\partial \vec n}|_{\partial \Omega}=0\space}$
or 
$\large  {   \space U|_{\partial \Omega}=0}$ I would like to know what is a sufficient boundary condition to a more general equation like $\Large {\frac{\partial^m U}{\partial x^m}+\frac{\partial^n U}{\partial y^n}=0}$ on a boundary $\partial  \Omega$. How do we know if a given boundary condition is sufficient to solve a general PDE? What is the relation between the order of a PDE and the number and type of boundary conditions needed on $\partial \Omega$. Thanks.","['boundary-value-problem', 'partial-derivative', 'ordinary-differential-equations', 'partial-differential-equations']"
1764090,Suppose $f$ is differentiable at $a$. Evaluate $\lim\limits_{h\to0} \frac{f(a+16h) - f(a+15h)}h$,"Suppose $f$ is differentiable at $a$. Evaluate if possible
$$\lim\limits_{h\to0} \frac{f(a+16h) - f(a+15h)}h$$ $$\lim\limits_{h\to0} \frac{f(a+15h)}h - \lim\limits_{h\to0} \frac{f(a+15h)}h$$
which to me is basically just $f'(a)- f'(a) = 0$ or I found
$$\lim\limits_{h\to0} \frac{f(a) + f'(a)16h + r(16h) - f(a) - f'(a)15h - r(15h)}h$$
which equals to $\lim\limits_{h\to0} \frac{16hf'(a) - 15hf'(a)}h$ since $\lim\limits_{h\to0} r(16h)/h = 0$ which gives me $16f'(a) - 15f'(a) = f'(a)$ I end up with either $f'(a)$ or zero but I don't know which one is correct.","['derivatives', 'real-analysis', 'limits']"
1764118,Sobolev spaces on Riemannian manifold,"I know one can define the Sobolev spaces on a Riemannian manifold as completions, but is there an equivalent definition that uses weak derivatives, like in the case of open sets in $\mathbb{R}^n$ ? Thanks","['functional-analysis', 'differential-geometry', 'partial-differential-equations']"
1764134,Theorem 3.37 in Baby Rudin: $\lim\inf\frac{c_{n+1}}{c_n}\leq\lim\inf\sqrt[n]{c_n}\leq\lim\sup\sqrt[n]{c_n}\leq \lim\sup\frac{c_{n+1}}{c_n}$,"Here's Theorem 3.37 in the book Principles of Mathematical Analysis by Walter Rudin, third edition: For any sequence $\{c_n\}$ of positive numbers, $$\lim_{n\to\infty} \inf \frac{c_{n+1}}{c_n} \leq \lim_{n\to\infty} \inf \sqrt[n]{c_n},$$ $$ \lim_{n\to\infty} \sup \sqrt[n]{c_n} \leq \lim_{n\to\infty} \sup \frac{c_{n+1}}{c_n}.$$ Now Rudin has given a proof of the second inequality. Here's my proof of the first. Let $$\alpha = \lim_{n\to\infty} \inf \frac{c_{n+1}}{c_n}.$$ Then $\alpha \geq 0$. If $\alpha = 0$, then we're done since $$\lim_{n\to\infty} \sqrt[n]{c_n} \geq 0.$$ So we suppose that $\alpha > 0$ and choose a real number $\beta$ such that $0 < \beta < \alpha$. Then by the result analogous to Theorem 3.17 (b), there is an integer $N$ such that $n \geq N$ implies $$ \frac{c_{n+1}}{c_n} > \beta,$$ which in turn implies $$c_{n+1} > \beta c_n.$$
  So for each $n \geq N$, we have 
  $$c_n \geq  \left( c_N \cdot \beta^{-N} \right) \cdot \beta^n. $$
  Thus, for $n \geq N$, we have $$\sqrt[n]{c_n} \geq \beta \sqrt[n]{  c_N \cdot \beta^{-N}  }.$$ Then taking the limit inferior of both sides, we get 
  $$\lim_{n\to\infty}\inf \sqrt[n]{c_n} \geq \lim_{n\to\infty} \inf \left( \beta \sqrt[n]{ c_N \beta^N} \right) = \lim_{n\to\infty} \left( \beta \sqrt[n]{ c_N \beta^N} \right) = \beta \cdot 1 = \beta$$
  by Theorem 3.20 (b). Thus we have shown that for any (positive) real number $\beta$ such that $\beta < \alpha$, we have $$\beta \leq \lim_{n\to\infty} \inf \sqrt[n]{c_n},$$ 
  which implies that $$\alpha \leq \lim_{n\to\infty} \inf \sqrt[n]{c_n},$$ as required. Is the above proof correct? If so, then is my presentation good enough? If not, then where does the problem lie?","['real-analysis', 'limsup-and-liminf', 'proof-verification', 'sequences-and-series', 'analysis']"
1764174,Example 3.40 (b) in Baby Rudin: How to find $\lim_{n\to\infty} \sup \frac{1}{\sqrt[n]{n!}}$?,"Here is Theorem 3.39 in the book Principles of Mathematical Analysis by Walter Rudin, third edition: Given the power seires $\sum c_n z^n$, put $$\alpha = \lim_{n\to\infty}\sup\sqrt[n]{\vert c_n \vert}, \ \ \ R = \frac{1}{\alpha}.$$ (If $\alpha = 0$, $R = +\infty$; if $\alpha = +\infty$, $R = 0$.) Then $\sum c_n z^n$ converges if $\vert z \vert < R$, and diverges if $\vert z \vert > 1$. The proof makes use of the root test, which is Theorem 3.33. Now in Example 3.40 (b), Rudin states that the series $\sum {z^n \over n!}$ has $R = +\infty$, meaning $\alpha = 0$. How does this hold, especially in view of the fact that $\lim_{n\to\infty} \sqrt[n]{n} = \lim_{n\to\infty} \sqrt[n]{p} = 1$ for any $p > 0$, as has been stated and proved by Rudin in Theorem 3.20 (b) and (c)? That is, how to rigorously show (using only the machinery developed by Rudin until Theorem 3.39) that $$\lim_{n\to\infty} \sup {1 \over \sqrt[n]{n!}} = 0?$$ Can we state for the power series $\sum c_n z^n$ a result analogous to Theorem 3.39, using the ratio test (i.e. Theorem 3.34)?","['real-analysis', 'sequences-and-series', 'complex-analysis', 'power-series', 'analysis']"
1764188,What is the motivation for normed division algebras?,"The famous Hurwitz theorem states that the only normed division algebras are  $\mathbb{R}$, $\mathbb{C}$, $\mathbb{H}$ and $\mathbb{O}$. What is some good pedagogical motivation why we should think about normed division algebras at all?","['abstract-algebra', 'soft-question']"
1764208,Surface Normal from Cross Product,"Given an equation (in this case $x^2-y^2+z=0$) how would I find the surface normal using a cross product at a certain $(1,2,3)$ point? I know how do it with $grad(f)$ but I presume that isn't what they're asking me to do in this case? I then have to find the equation of the tangent plane at the same point but that would just involve finding $ax+by+cz=d$ wouldn't it? Thanks!","['multivariable-calculus', 'vectors']"
1764226,On those integers $n>1$ such that any commutative ring with identity having exactly $n$ ideals is a PIR,"Convention : All rings are commutative with unity unless stated otherwise. By ideals we will mean to include $\{0\}$ and $R$ also. Let us call an integer $n>1$ a "" principal number "" if any ring $R$ having exactly $n$ ideals is a PIR (from this answer we know that for any $n>1$ there exists a corresponding PIR, here we want all the corresponding rings be PIR). I know that any ring $R$ having $5$ or less ideals is a PIR, so certainly any $n \le 5$ is a principal number. My question is: Are there infinitely many principal numbers? Are there infinitely many positive integers which are not principal number? Can we in any way characterize the principal numbers? Please help. Thanks in advance.","['abstract-algebra', 'ring-theory', 'reference-request', 'ideals']"
1764261,Minor flaw in understanding of the proof of the derivative of exponential functions,"I understand the majority of the proof of the derivative formula for exponential functions of the form: (full proof at bottom of post) $\frac{d}{dx}a^x$ but I have a little trouble with the last step which implies that the rate of change of any exponential function if proportional to both the function itself and the derivative of the function at zero. I fail to see how $\lim_{h\to 0}{\frac{a^h-1}{h}}=f'(0)$ *the limit is supposedly the value of the derivative of $a^x$ at $0$, but to me the limit seems only to be a simplified part of the whole definition of the derivative of the function! Where is the flaw in this line of reasoning? (to clarify my question: It seems like the $lim_{h\to 0}$ defines the derivative of $f$ with respect to $x$ anywhere, not just $0$.) Proof: $f(x)=a^x$ $f'(x)=\lim_{h\to 0}{\frac{f(x+h)-f(x)}{h}}$ $f'(x)=\lim_{h\to 0}{\frac{a^{x+h} -a^x}{h}}$ $=\lim_{h\to 0}{\frac{a^x(a^h-1)}{h}}$ $=a^x \cdot \lim_{h\to 0}{\frac{a^h-1}{h}}$
, and supposedly $f'(0)=\lim_{h\to 0}{\frac{a^h-1}{h}}$, so $f'(x)=f'(0)\cdot a^x$ $\square$ I also apologize to anyone who finds this question too basic; I just can't seem to break this mental block. Any help from a deeper understanding is greatly appreciated!","['derivatives', 'exponential-function', 'calculus', 'proof-explanation']"
1764278,Number of points of accumulation of a sequence,"Can a sequence have infinitely many points of accumulation i.e. we can extract infinitely many subsequences from it s.t. they all converge to their respective point of accumulation? 
I have the feeling it would mean that the period of repetition of something could be infinitely big.","['sequences-and-series', 'calculus']"
1764282,Fundamental set of solutions to a differential equation,"Say I have a linear 2nd homogeneous ODE of the form $$y''(x)+p(x)y'(x)+q(x)=0$$ Now I know that the general solution to this will be of the form $$y(x)=c_{1}y_{1}(x)+c_{2}y_{2}(x)$$ where $\lbrace y_{1}(x),y_{2}(x)\rbrace$ form a fundamental set of solutions . My question is (and apologies if it is a stupid one), is there only one set of fundamental solutions to a given differential equation (of the form above) or are there many? Would someone be able to enlighten me on which case is true and why? Also, if the latter case is true, please give an example of two different fundamental sets of solutions that satisfy the same linear 2nd homogeneous ODE.","['ordinary-differential-equations', 'fundamental-solution']"
1764317,"Let $f: [0, 1] \to \mathbb{R}$ s.t $f(0)=f(1)=0$ then measure of $A = \{h \in [0, 1] \mid \exists x \text{ such that }f(x+h) =f(x)\} \geq 1/2$.","Let $f:[0,1]\to\mathbb R$ be a continuous function s.t. $f(0)=f(1)=0$. Let $$A = \{h \in [0, 1] \mid \exists x \text{ such that }f(x+h) =f(x)\}.$$ Show that set $A$ has Lebesgue measure $\geq 1/2$. I have no clue to prove the claim. Can anyone give me some detail hints? Thank you in advance.","['continuity', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
1764320,Game is winnable if and only if $n \neq k$,"Integers $n$ and $k$ are given, with $n \ge k \ge 2$. You play the following game against an evil wizard. The wizard has $2n$ cards; for each $i = 1, \ldots, n$, there are two cards labelled $i$. Initially, the wizard places all cards face down in a row, in unknown order. You may repeatedly make moves of the following form: you point to any $k$ of the cards. The wizard then turns those cards face up. If any two of the cards match, the game is over and you win. Otherwise, you must look away, while the wizard arbitrarily permutes the $k$ chosen cards and then turns them back face-down. Then, it is your turn again. We say this game is winnable if there exist some positive integer $m$ and some strategy that is guaranteed to win in at most $m$ moves, no matter how the wizard responds. Prove that the game is winnable if and only if $n \neq k$.","['algebra-precalculus', 'combinatorics', 'game-theory', 'discrete-mathematics']"
1764349,Is the set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^\mathbb{R}$?,"A function $f: \mathbb{R} \to \mathbb{R}$ is called periodic if there exists a positive number
$p$ such that $f(x) = f(x + p)$ for all $x \in \mathbb{R}$. Is the set of periodic
functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^\mathbb{R}$? Explain So I have seen a solution to this question and my question has more to do with what thought process was used to even think of the sort of function to show that the set of periodic functions is not a subspace?  First I do have a question of what $\mathbb{R}^{\mathbb{R}}$ would look like? I'm visualizing elements being of some sort of infinite list of the sort $(x_1, x_2, x_3,..........), x_i \in \mathbb{R}$. But to the main question.  So the function chosen was $$h(x) = sin\sqrt{2}x + cos(x)$$ where $f(x) = sin\sqrt{2}x$ and $g(x) = cos(x)$ Using these functions, the author arrived at a contradiction with regards to $\sqrt{2}$ being shown to be rational (which it is not). Working this out after being given that function was fine, but what was the motivation to use that function? Where did the idea to show something is irrational would help to disprove a set being a subspace? It almost feels like it arose from osmosis and brilliance.....","['abstract-algebra', 'linear-algebra', 'proof-explanation']"
1764368,If $(x_n)$ is a martingale difference then $(x_n\mathbf{1}_{\{ |x_n|\le a_n \}})$ is a martingale?,"Let $(x_n,\mathcal{F}_n, n\ge 1)$ be a martingale diference. Is $(x_n\mathbf{1}_{\{ |x_n|\le a_n \}},\mathcal{F}_n, n\ge 1)$ a martingale and why?? $a_n$ is a constant.","['probability-theory', 'martingales']"
1764370,If $f$ function then $f^{-1}$ function iff $f$ function injective (one-to-one).,"During the lecture we learned this phrase: ""If $f$ is a function then $f^{-1}$ is a function iff $f$ is injective (one-to-one)."" But why? 
What with onto? $f$ doesn't need to be Surjective (onto)? For example:
$$f:A \rightarrow B$$
$$A=\{1,2,3\} $$
$$B=\{1,2,3,4\} $$
$$f=\{(1,1)(2,2)(3,3)\}$$ $f$ is an injective function but $ f^{-1} $ can't be a function because for 4 we can`t find a place to go.","['elementary-set-theory', 'functions', 'inverse']"
1764376,Fundamental Period of $\tan x \cot x$,"What is the period of $\tan x \cot x?$ I was given this question today. What I did was simplify the expression , and it reduced to a constant function. So it had no fundamental period. But my teacher told me that the answer was $\frac{\pi}{2}$. How is it so?","['periodic-functions', 'functions']"
1764414,Poisson Equation: 'Dirichlet' type problem on all of $\mathbb{R}^N$,"I've seen a great deal about solving the Dirichlet problem for Poisson's equation
\begin{equation}
\Delta u = f \quad \mbox{in} \quad \Omega \subset \mathbb{R}^N\\
u = 0  \quad \mbox{on} \quad \partial\Omega
\end{equation}
when $\Omega$ is bounded, and quite a bit when $\Omega$ is unbounded but has nontrivial complement (e.g. when it's all of $\mathbb{R}^N$ except for the unit ball). My question concerns when, if at all, this is possible when $\Omega = \mathbb{R}^N$. Obviously, the boundary of $\mathbb{R}^N$ is meaningless, but it seems reasonable to consider an analogous problem in the class of functions with the 'boundary condition' $u \to 0$ as $|x| \to \infty$. However, we quickly run into problems; to understand what the problem might entail I picked $f = \exp(-x^2)$ and  $\Omega = \mathbb{R}$ as friendly looking example and found to my dismay that the general solution takes the form
\begin{equation}
u(x) = \frac{\sqrt{\pi}}{2}x\mbox{erf}(x)+\frac{1}{2}\exp(-x^2)+Ax+B
\end{equation}
where erf$(x) = \frac{2}{\sqrt{\pi}}\int_0^x\exp(-t^2)\mathrm{d}t$ is the error function and $A,B$ are constants. The $x$erf$(x)$ term assures that for any choice of $A,B$ that $u\to\infty$ as $|x| \to \infty$, at least in one of the directions: if we pick $A=\frac{\sqrt{\pi}}{2}$ the forward direction decays appropriately but not so in the backward direction, and vice versa. Of course, on a bounded domain we can always make a choice of constants such that the Dirichlet boundary condition is fulfilled, in particular on $[-L,L]$ the choice $A=0$, $B=-\frac{\sqrt{\pi}}{2}L\mbox{erf}(L)-\frac{1}{2}\exp(-L^2)$, but clearly as $L\to\infty$, $B$ also grows without bound. My question then, is how bad is the situation? Do things improve for $N\geq 2$? The greater variety of harmonic functions might mean we can find one such that summing it with the solution derived via convolution with the Newton Kernel will allow us to satisfy the decay conditions but then again, maybe not. How can we understand this? Can we find success by restricting the class of functions? In 1D we can guarantee that the integral of a function decays iff the mass of the function is zero; is there an analogy for integrating twice? Assuming the answer to either of these is positive, can we then say anything about the integrability of the resulting $u$ over $\mathbb{R}^N$? In particular, I'd like to be able to talk about quantities such as $||\Delta^{-1/2} f||_{L^2 (\mathbb{R}^N)}$, but I'm not sure this will be possible without moving into some kind of weighted spaces if at all.","['functional-analysis', 'real-analysis', 'elliptic-equations', 'partial-differential-equations']"
1764431,Solving $\arcsin\left(2x\sqrt{1-x^2}\right) = 2 \arcsin x$,"If we have $$\arcsin\left(2x\sqrt{1-x^2}\right) = 2 \arcsin x$$ Then, what will be the set of $x$ for which this equation is true? I tried to solve it by putting $x = \sin a$ or $\cos a$ but got no result. I am totally stuck on how to do it.","['trigonometry', 'functions', 'inverse']"
1764474,Finding the sum of all products of pairs of distinct primitive roots mod 83,I'm currently studying Number Theory and I've stumbled upon a question where I need to: Find the sum of all products of pairs of distinct primitive roots mod 83 . Solving attempt: I've tried to find all the primitive roots mod 83 but then I realized that there are probably many of them and the calculations are getting heavy on high powers. I guess there might be a simpler approach then just finding all the primitive roots and summing all the products of distinct primitive roots. I do know that the product of all primitive roots (mod p) is 1 mod p but I don't see how it helps me. Any help would be appreciated.,"['number-theory', 'abstract-algebra', 'elementary-number-theory']"
1764477,Problem about a compact operator $T:l^p\rightarrow l^p$,"I have to solve this problem. Let $\{\lambda_n\}$ be a sequence of real number such that $\lim_{n\rightarrow\infty}\lambda_n=0$ and consider the operator $T:l^p\rightarrow l^p$, $1\leq p\leq \infty$, defined by $$T(\{x_1,\ldots x_n,\ldots\})=\{\lambda_1 x_1,\ldots,\lambda_n x_n,\ldots\}
$$
  Prove that $T$ is compact. My attempt is the following. I have considered the operator $$
T_N:l^p\rightarrow l^p,\quad T_N(\{x_1,\ldots x_n,\ldots\})=\{\lambda_1 x_1,\ldots,\lambda_N x_N,0,0,\ldots\}.
$$
$T_N$ is clearly linear and it is also compact. In fact given any bounded sequence $\{x^{(n)}\}_{n\geq 1}$ in $l^p$, the sequence $\{T_Nx^{(n)}\}_{n\geq 1}$ is bounded in $\mathbb{R}^N\subset l^p$. Then, since every bounded sequence in $\mathbb{R}^N$ has a convergent subsequence, it follows that $T_N$ is compact. Now, I have to prove that $T$ is compact. For this I proved that $||T_N-T||\rightarrow 0$ as $N\rightarrow\infty$. In fact we have:
$$
||T_N-T||=\sup_{||x||=1}||(T_N-T)x||=\sup_{||x||=1}||T_Nx-Tx||=\sup_{||x||=1}\left(\sum_{i=N+1}^{\infty}|\lambda_i x_i|^p\right)^{\frac{1}{p}}
$$
and, given $\epsilon>0$, there exists $\bar n$ such that $|\lambda_n|<\epsilon$, for every $n>\bar n$. So if we take $N>\bar n$, we get
$$
\sup_{||x||=1}\left(\sum_{i=N+1}^{\infty}|\lambda_i x_i|^p\right)^{\frac{1}{p}}\leq\epsilon\sup_{||x||=1}\left(\sum_{i=N+1}^{\infty}|x_i|^p\right)^{\frac{1}{p}}\leq\epsilon
$$
Therefore $||T_N-T||\rightarrow 0$ as $N\rightarrow\infty$ implies that $T$ is compact. My questions are: (1) is the proof of the compactness of the operator $T_N$ correct? (2) is the conclusion correct? Thanks","['functional-analysis', 'compact-operators', 'lp-spaces']"
1764498,Finding a basis for the set of polynomials where f(1)=f(-1)=0,"I have the vector space $V$ above that belongs to $\mathbb{F}$, and $V$ is the group of all polynomials that are of degree $3$. $W= \{ p \in V | p(1)=p(-1)=0\}$ 1.) Prove that W is a subspace of $V$. A.) Ok let's prove that the space isn't empty. Let's take $p(x)= 0$, and this satisfies the condition of $p(1)=p(-1)=0$ B.) Now that the sum of two vectors in $W$ belong in $V$ Let's take two polynomials that satisfy $W$ and show that their sum belongs in $W$. This is pretty trivial since you can't go increase your degree from addition. Let's take $p_1(x)=x_1^3-x_1^2-x_1+1$ and $p_2(x)=x_2^3-x_2^2-x_2+1$. Their sum belongs in $V$. Let's check multiplication by a scalar $\lambda \in \mathbb{R} $(also pretty trivial): $p(\lambda x)=\lambda x^3-\lambda x^2-\lambda x+1 = \lambda p(x)$ Now onto the basis: My thought process is that I might need to find a basis that contains at least 4 components (since $\mathbb{R}_3[x]$ contains 4 components), which I don't think is correct. So far I have two components: B= $\{ (x^2-1),(x(x^2-1))\} $ This is where I'm stuck and can't find anymore polynomials that satisfy the requirement. TL;DR 1.) Is my thought process for proving that $W$ is a subspace of $V$ correct? 2.) How can I find a basis for $W$? Is there a simpler way using a system of equations rather than brute force trial and error?","['change-of-basis', 'linear-algebra']"
1764511,Can$A \cap (B' \cap C')$ be $(A \cap B') \cap (A \cap C')$?,"If I use the above statement, provided that it is right, in a question, would I have to prove it as well?","['boolean-algebra', 'proof-writing', 'elementary-set-theory', 'discrete-mathematics']"
1764583,For a normal operator is it true that $\|T^*T^2\| = \|T^3\|$?,For a normal operator is it always true that $\|T^*T^2\| = \|T^3\|$? See the accepted answer for the case in a Hilbert space Update: how about $\|T^*T^2\| = \|T\|^3$ in a Hilbert space,"['functional-analysis', 'normed-spaces', 'hilbert-spaces']"
1764590,"If $\{T_n\} \to T$ and $\{u_n\} \to u$, then $\{T_n(u_n)\} \to T(u)$.","Let $X$ and $Y$ be normed linear spaces. Define 
$$L(X,Y) = \{T:X \to Y \ \big |  \ T \text{ is bounded}\}.$$
Let $\{T_n\} \to T$ in $L(X,Y)$ and $\{u_n\} \to u$ in $X$, then $\{T_n(u_n)\} \to T(u)$ in $Y$. Proof: Let $T_n, T \in L(X,Y)$ such that $\{T_n\} \to T$ in $L(X,Y)$. Since $T$ and $T_n$ are bounded for each $n \in \mathbb{N}$, there exists constant $M_0$ and $M_n$ respectively for which $n \in \mathbb{N}$
$$||T(u)|| \leq M_0 ||u|| \quad\forall u \in X,$$
$$||T_n(u)|| \leq M_n ||u|| \quad \forall u \in X.$$
Define $M = \sup_n \{M_n\}$. We claim that $M$ is finite because each linear operator $T_n$ is bounded. Otherwise, $M = \infty$ and thus for some $n$, we have that $M_n = \infty$, so that $T_n$ is unbounded and therefore we obtain a contradiction. Now,
$$||T(u) - T_n(u_n)||= ||T(u) -T_n(u)+T_n(u)- T_n(u_n)||\leqslant \cdots\\ \cdots \leqslant ||T(u) -T_n(u)||+||T_n(u)- T_n(u_n)||\leqslant \cdots \\ \cdots \leqslant  ||T-T_n||.||u|| + M .||u-u_n||.$$
Using taking the limit as $n$ approach infinity and using the fact that $\{T_n\} \to T$ in $L(X,Y)$ and $\{u_n\} \to u$ in $X$, we have
$$\lim_{n \to \infty} ||T(u) - T_n(u_n)|| \leq \lim_{n \to \infty} \left( ||T-T_n||.||u|| + M .||u-u_n||\right) = 0$$
and therefore $\{T_n(u_n)\} \to T(u)$ in $Y$ as required.","['real-analysis', 'measure-theory', 'proof-verification']"
1764591,Functional equation $f\big(x+f(y)\big) = f(x)-y$ where $f : \mathbb{Z} \to \mathbb{Z}$,"I'm trying to solve the functional equation $f\big(x+f(y)\big) = f(x)-y$ where $f : \mathbb{Z} \to \mathbb{Z}$ . What I got so far is: $f$ is injective and $f(0) = 0$ .
Thanks in advance for your time.","['functions', 'functional-equations']"
1764619,How to prove derivative of logarithm with base $b$?,I learned how to derive a logarithm with any base. This is the formula: $$\frac{d}{dx}\log_bx=\frac{1}{x\ln b}$$ How can it be proved?,"['derivatives', 'logarithms', 'calculus']"
1764640,"Golden Ratio Conjecture in three simple Geogebra shapes--circle, triangle, and square.","A circle, equilateral triangle, and square of equal heights are all placed on the same horizontal line as shown below.  The circle is tangent to the triangle which is centered upon the left edge of the square.  A line is drawn from the center of the circle to the right edge of the square, passing through the center of the square.  The line is cut into two segments by the right side of the triangle, as shown. Show that the ratio of the length of the blue segment to the green segment is the golden ratio 1.618.  (Is it? It seems so!) I have been playing around in geogebra, but I was unable to get the circle tangent to the triangle as shown in the figure, which I drew in Adobe Illustrator.  Any geogebra assistance would be appreciated!  How do I move/translate a simple object in such a manner?  I am used to dragging and dropping it in Adobe Illustrator, but geogebra is much better suited to these golden ratio conjectures.  Thanks for all your help!  I have been successful with geogebra with a couple other constructions which I will share soon. :)","['golden-ratio', 'euclidean-geometry', 'trigonometry', 'geometry']"
1764641,Linearspan of Gaussians dense in Schwartz space,"as the title already says I am trying to show that the linear span ""A"" of the gaussians $e^{\frac{-|x|^2}{2}}$ and their translations/ dilations are dense in the Schwartzspace.
This is the space of rapidly decreasing smooth functions, i.e. $\mathcal{S}(\mathbb{R}^n) =\{f\in C^\infty(\mathbb{R}^n) | \forall_{\alpha, \beta \in \mathbb{N}_0^n} \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)| < \infty \}$ which is equipped the metric $$\rho(u,v)= \sum _{\alpha, \beta \in\mathbb{N}_0^n} 2^{-|\alpha|-|\beta|} \frac{\|u-v\|_{\alpha, \beta}}{1+\|u-v\|_{\alpha, \beta}},$$ 
where $\alpha$ and $\beta$ are multiindices and $\|u\|_{\alpha,\beta}= \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)|$ Thus I need to show that for every $\varphi \in \mathcal{S}(\mathbb{R}^n)$ there is a sequence $(u_k)_{k\geq 1}$ such that for all multiindices $\alpha$ and $\beta$ as above $\|\varphi-u_k\|_{\alpha, \beta}$ converges to $0$ as  $k\rightarrow \infty$. 
Does anyone have a hint/source on how to construct such a sequence? I'd really like to figure this out but as it stands I'm stuck at the beginning. My first guess was to use that $C_0^\infty(\mathbb{R}^n)$ is dense in $\mathcal{S}(\mathbb{R}^n)$, then show that the linear span of the Gaussians is dense in $C_0^\infty(\mathbb{R}^n)$ with respect to $\rho$ and afterwards apply some diagonal sequence argument. But so far I haven't succeeded in finding such an approximation. EDIT: So I have thought about the suggestion and this is what I understand so far: 
I know that the convolution for $\varphi \in\mathcal{D}(\mathbb{R}^n)$ and $T \in \mathcal{D}'(\mathbb{R}^n)$ is defined for all  distributions $\psi$ via 
$$< \varphi *T,\psi> := <T, \mathcal{R}(\varphi) * \psi> $$
where $\mathcal{R}f(x) = f(-x)$ is the reflexion operator. But this definition should also translate to a tempered distribution $\phi$ convoluted with a gaussian of the type $G=ae^{\frac{|x-x_0|^2}{2b}}$ for $a\in \mathbb{R}$,$b>0$ and $x\in\mathbb{R}^n$. 
Therefore if I choose $\phi$ in $\mathcal{S}'(\mathbb{R}^n)$such that $<\phi, G> =0 $ for all G and define $g_\epsilon = e^{\frac{-|x|^2}{2\epsilon^2}}$ as below the convolution $g_\epsilon*\phi$ is well-defined. Furthermore by the choice of $\phi$ I know 
$$ <g_\epsilon * \phi, G> = <\phi, \mathcal{R}(g_\epsilon)*G> = 0$$
as $\mathcal{R}(g_\epsilon)*G $ is again of type G. 
On the other hand $\lim_{\epsilon \rightarrow 0} \mathcal{R}(g_\epsilon)*G = G$ so also $$\lim_{\epsilon \rightarrow 0} <g_\epsilon * \phi, G> = <\phi,\mathcal{R}(g_\epsilon)*G>  =<\phi, g_\epsilon *G> =  <\phi, G> = 0$$ by the symmetry of $g_\epsilon$ and the definition of $\phi$. 
Maybe my thoughts are flawed but so far I don't know how to conclude that $\phi =0$. (This should be equivalent to the support of $\phi$ being the empty set, but I only see that $<\phi, \psi>$ vanishes for $\psi \in G$ and not for all $\psi \in \mathcal{S}(\mathbb{R}^n))$","['functional-analysis', 'approximation-theory', 'schwartz-space', 'fourier-analysis']"
1764645,Probability of number of people who know a rumor,"Suppose that among a group of $n$ people, some unknown number of people $K$ know a rumor. If someone knows the rumor, there is a probability $p$ that they will tell it to us if we ask. If they don't know the rumor they will always say they don't know it. If I go around and ask each person if they know about the rumor, and $M$ people say they do, what does that tell me about the number of people who actually know the rumor? In particular, what is the distribution $P(K=k|M=m)$ in terms of $P(K)$? Edit: I've been able to show that $P(K=k|M=m)=\frac{b(m,k,p)P(K=k)}{\sum_{j=m}^{n-1}b(m,j,p)P(K=j)}$ where $b(m,k,p)$ is the binomial density function (probability of $m$ successes out of $k$ trials with probability $p$ of success). Is it possible to take this any further?","['bayesian', 'statistics', 'probability', 'expectation']"
1764663,Compact sets of compact-open topology,"Let $X$ and $Y$ be topological spaces,$X$ not compact and $Y$ metric, denote with $C(X,Y)$ the set of continuous functions between $X$ and $Y$ and put on $C(X,Y)$ the compact-open topology. My question is: which are the compact sets of this topology? My guess is that each set of the form $\mathcal{K}:=\{f\in C(X,Y)| f(K_1)\subset K_2\}$ (where $K_1\subset X$ and $K_2\subset Y$ are compact sets) is compact, due to the fact that the sets of the form $\{f\in C(X,Y)| f(K_1)\subset U\}$ ($U\subset Y$ is compact) are a subbase for the topology and so for every open cover of $\mathcal{K}$ I can find a finite subcover. Am I right? are there other types of compact sets? Thank you","['functional-analysis', 'general-topology', 'functions']"
1764685,The three unsolved problems of antiquity,"In Sidelights on the Cardan-Tartaglia Controversy (Apr., 1938) by Martin A. Nordgaard in the National Mathematics Magazine, Vol. 12, No. 7, pp. 327-364, it is written on the first page The solution of the cubic had presented itself to the human mind  as
  an intellectual problem already in the fifth century B. C.; it became 
  a scientific need in Archimedes' calculation on floating bodies in the
  third century B. C.; it confronted the Arab astronomers in the Middle 
  Ages. And now it was solved! The first of ""the three unsolved 
  problems of antiquity"" to be solved. But which are the other two of ""the three unsolved problems of antiquity""? What people usually seem mean when they refer to ""the three unsolved problems of antiquity"" are the trisecting of the angle, the doubling of the cube (or the Delian problem) and the squaring of the circle (just use Google to verify). Evidently Nordgaard refers to these as one. The other problem could then very well be the question of the necessity of Euclid's fifth postulate. Is there any other place were this terminology is used in this manner, or is it just an imagination of Nordgaard? Are my suspicions correct? Or does he refer to different problems? Was the solution to the cubic indeed such a ""big deal"" to the ancients?","['math-history', 'geometry']"
1764699,$\omega$-limit set of a point $x \in X$,"I would like to verify whether the following definition of the $\omega$-limit set of a point $x \in X$ is correct: $$\omega(x) = \{ y \in M : \exists \text{ sequence }\{t_j\}, \text{ where } t_j \rightarrow \infty, \text { such that } \varphi_{t_j} (x) \rightarrow y \text{ as } j \rightarrow \infty \}.$$ The context is ODEs, so $M$ is the phase space.","['definition', 'ordinary-differential-equations', 'sequences-and-series', 'limits']"
1764711,A limit involving $\cot$ that seemingly shouldn't exist,"According to Wolfram Alpha , $$\lim_{x \to \infty} \frac{x - \cot x}{x} =1.$$ But does the limit even exist? Isn't $\frac{x - \cot x}{x}$ unbounded near $x= n \pi$ for all $n \in \mathbb{N}$? Assuming that the limit doesn't actually exist, what might explain why Wolfram Alpha thinks that it does exist?",['limits']
1764716,Solving $2\cos\left(2\theta\right) = \sqrt{3}$,"I have a question on this test review problem (that will help us on a test), and I have no clue what it's asking. We're learning trigonometry, (Analytic Trigonometry), like about the unit circle, inverse trig functions, etc... And I encounter this problem on test review: $$ 2\cos\left(2\theta\right) =  \sqrt{3}$$ I know the answers are: $\left\{\frac{\pi}{12}, \frac{11\pi}{12}, \frac{13\pi}{12}, \frac{23\pi}{12} \right\} $ But I want to know how to find it. So I get ready for test. Thank you. EDIT After some thinking i got some of it but not All!! So what i did was: $ 2\Theta = \cos^{-1} (\frac{\sqrt3}{2} )$ $2\Theta = \frac {\pi}{6}$ $\Theta = \frac{\pi}{12} + 2\pi n$ So now i just add 2pi but i have to remember $0 \le  \Theta \lt 2\pi$ I get: $\left\{\frac{\pi}{12}, \frac{13\pi}{12} \right\} $ But that obviously isnt the anwser.. Please help!",['trigonometry']
1764719,$\mathbb{Z} [\sqrt{2}]$ is an integral domain,"We know that $(\mathbb{Z} [\sqrt{2}],+,\cdot)$ is an integral domain. Someone can prove it easily if he says that is a subring of $(\mathbb{R} ,+,\cdot)$ . Can we find a different proof, more analytical? 
How can we show that $$\forall x,y\in \mathbb{Z} [\sqrt{2}], x\ne 0, y\ne 0\implies xy\neq 0$$","['abstract-algebra', 'ring-theory', 'field-theory']"
1764723,Drones and Integrals Project,"Hello everyone and thanks for taking the time to read this post. So in my college calculus class we had the opportunity to fly a drone and get it's flight data. I have a spreadsheet featuring two columns (one is $x$-velocity and the other is time). I have graphed a velocity versus time graph given these points and I have also fitted a 6th degree polynomial trend line. I am trying to find the total displacement and the total distance that my drone flew. So I know that a velocity-v-time graph provides several important things about the drone. The area underneath the curve is the total displacement of the drone; the slope of the graph is the drone's acceleration. In order to find the total displacement of the graph I have to use a definite integral. After getting my trend line equation, I tried to use a definite integral, but received a displacement way to high to be correct. I was wondering if anyone could help me figure out what I am doing wrong. Some things to note: I have the actual displacement which was measured with measuring tape. The actual/theoretical displacement of my drone is $58\, cm$. When I graphed the trend line, my minimum $x$-point was $9.8$ because the drone started recording data even when it hovered and our professor said it was fine if we do not include that data. My maximum $x$-point is $18.7$ because this is when the drone completed its flight path and landed. Velocity was negative at some points during this flight. The equation I am supposed to use is as follows: $$\int_9^T v(t)dt$$ I started off by making a graph of the velocity versus time and fitted a trend line for it. The equation for the trend line was: 
$$y = 1.3264x^6-116.14x^5+4179.3x^4-79079x^3+829395x^2-5000000x+10000000$$ After I integrated this equation for the time interval $[9.8, 18.7]$ I got a final answer of $-57519039.94$. This is obviously way wrong and I cannot seem to figure out what I am doing wrong. Thanks in advance for the help!","['derivatives', 'definite-integrals', 'calculus']"
1764727,General property regarding outer measure for a nested sequence of sets (measurable or not).,"Let  $\bigcap_{n=1}^\infty E_n=∅$ and if $\mu^*(E_n) <\infty$ and $E_{n+1}  \subseteq  E_n $ then $\lim\limits_{n\mapsto \infty} \mu^*(E_n) =0 $ even if each $E_n$ is a non-measurable set, where $\mu^*$ is outer measure. Proof sketch please?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1764729,Positiveness of energy of differential equation,"Let $x(t) : [0,T] \rightarrow \mathbb{R}^n$ be a solution of a differential equation
  $$
\frac{d}{dt} x(t) = f(x(t),t).
$$
  In addition we have functions $E :\mathbb{R}^n \rightarrow \mathbb{R}$ and  $h:\mathbb{R}^{n+1}\rightarrow \mathbb{R}$  such that
  $$
\frac{d}{dt}{E(x(t))} = h(x(t),t) E(x(t))
$$
  for any solution $x(t)$ of the differential equation. Can we show that if $E$ is positive at the time $0$ then it is positive at all times? i.e.
  $$
E(x(0))>0 \Longrightarrow E(x(t)) \qquad 0<t\leq T
$$
  Assume that all functions are at least one continuously differentiable. It is easy to show that it is true if $h$ does not depend on $x$ or can be written in the form $h(x(t),t) =\hat h(E(x(t)),t)$. Then $E$ satisfy following differential equation
$$
\frac{d}{dt}{E(t)} = \hat h(E(t),t) E(t)
$$
Thanks to the uniqueness of the solution, any solution $E(t)$ cannot cross the trivial solution $E(t)=0$ and therefore it has to have the same sign at all times. Application: Let's define matrix $A(t)$ via differential equation
$$
\frac{d}{dt}{A(t)} = B(A(t),t) A(t) \qquad A(0) = I
$$
where $B$ is matrix valued function with arguments $A$ and $t$. Is $A(t)$ invertible for all $t\geq 0$? We have 
$$
\frac{d}{dt}{\det{A(t)}} = Tr(B(A(t),t)) \det{A(t)} 
$$
If $B$ is just a function of $t$ and $\det{A(t)}$ then the answer is yes, in general I do not know if $A(t)$ is invertible or not.",['ordinary-differential-equations']
1764771,Derive zeta values of even integers from the Euler-Maclaurin formula.,"Euler showed: \begin{equation}
	B_{2 k} = (-1)^{k+1} \frac{2 \,  (2 \, k)!}{ (2 \, \pi)^{2 k}} 
			\zeta(2 k)
\end{equation} for $k=1,2, \cdots$. We could from here find $\zeta(2k)$ in terms of the
even Bernoulli coefficients $B_{2k}$. How can we derive the equivalent representation by using the Euler Maclaurin formula ? Thanks. Update Here is what I have done: \begin{eqnarray*}
  \zeta(s) = \sum_{n=1}^{\infty} \frac{1}{n^s} \quad , \quad  \mathrm{Re}(s) > 1.
\end{eqnarray*}
Euler used the Euler-Maclaurin series to find values of the Riemann Zeta function.
  We want to use  the 
Euler-Maclaurin formula with $f(x)=1/x^s=x^{-s}$. We know that \begin{eqnarray*}
  f^{(0)}(x) &=& \frac{1}{x^s} \\
  f^{(1)}(x) &=& -\frac{s}{x^{s+1}} \\
  f^{(2)}(x) &=& \frac{s(s+1)}{x^{s+2}} \\
  &\vdots& \\
  f^{(i)}(x) &=& (-1)^{i+1} \frac{s(s+1) \dots (s+i-2)}{x^{s+i}}  =
  (-1)^{i+1}   \frac{\Gamma(s+i)}{ \Gamma(s)} \frac{1}{ x^{s+i}}.
\end{eqnarray*} We then write using $h=1$, $a=1$, $b=\infty$ \begin{eqnarray*}
  \sum_{i=1}^{\infty} \frac{1}{n^s} = \int_1^{\infty} \frac{dx}{x^s} + \frac{1}{2}  +
  \left . \sum_{i=1}^{m} \frac{B_{2i}}{(2i)!} 
  \frac{\Gamma(s+2 i-1)}{ \Gamma(s)} \frac{1}{ x^{s+2 i-1}} \right |_1^{\infty} + R_{2m}
\end{eqnarray*}
with \begin{eqnarray*}
  R_{2m} =
  -\int_1^{\infty} \mathrm{B}_{2m} 
  \left \{ x-1 \right \} 
  \frac{\Gamma(s+2 m)}{(2m)! \, \Gamma(s)} \frac{dx}{x^{s+2m}}.
\end{eqnarray*}
That is \begin{eqnarray}
  \zeta(s) =  \frac{1}{s-1} + \frac{1}{2} -
   \sum_{i=1}^{m} \frac{B_{2i}}{(2i)!} 
  \frac{\Gamma(s+2 i-1)}{ \Gamma(s)}  + R_{2m}
  \label{zetazeta}
\end{eqnarray} This is an important equation since it establishes an analytic continuation for the 
$\zeta(s)$ function. We observe that the first fraction is an analytic function except
for $s=1$, then the sum of quotients of $\Gamma$  functions is analytic except for
isolated singularities in the negative integer arguments. Finally, since
the Bernoulli polynomial $B_{2m}\{x-1\}$ is bounded the residual is a convergent integral for $s + 2m >1$, so we can extend the 
convergence as far as $s > 1-2m$, for any positive number $m$. The question here is what ""$m$"" to choose. If I choose $m=1$, I did the computations and found something which made no sense.","['complex-analysis', 'analytic-number-theory', 'calculus']"
1764783,Prove that $l_2$ is a second countable space and is separable.,"This is actually a problem from Schaum's Outline Series: General Topology. Let $\{u_n\}$ be a sequence of real numbers such that the series $$\sum_{n=1}^\infty u_n^2<\infty.$$ That is, the series $u_1^2+u_2^2+\cdots$ converges. The class of all such sequences is denoted by $\mathbb R^\infty$. The $l_2$-metric $d$, is defined on $\mathbb R^\infty$ as $$d(u,v)=\sqrt{\sum_{n=1}^\infty |u-v|^2}.$$ $\mathbb R^\infty$ with the $l_2$-metric is called the $l_2$-space (or Hilbert space) $H$. Show that $H$ is second countable and separable. I think I'll be fine if I'll be given the countable base (for second countability) and countable dense subset (for separability) and I can work on the rest.",['general-topology']
1764812,Carmichael number square free,"Show that if $n$ is a Carmichael number , then $n$ is a square-free. I did this: Let $n= (p^t)(m)$  where $t >1$.
Then by modular property, 
$$b^p= b \mod n , \,\, b^m= b \mod n$$
Above two equations are also true in$\mod p$, $\mod p^t$.
But in$\mod m$, just $b^n=b \mod m$. And I tried to use CRT
But I couldn't. 
I think that I've chosen wrong way","['number-theory', 'elementary-number-theory']"
1764827,"Let $f$ be an analytic isomorphism on the unit disc $D$, find the area of $f(D)$","Let $f$ have power series $f(z) = \sum_{n=1}^\infty a_n z^n$ in $D$, then prove that   $\mathrm{area}\, f(D) = \sum_{n=1}^\infty n \,|a_n|^2$. Note: We define $\mathrm{area}\, S = \iint_S \mathrm{d}x\,\mathrm{d}y$. I presume the way to do this is to take the integral $$\iint_{f(D)} \mathrm{d}x\,\mathrm{d}y = \iint_D \mathbf{J}_f (x+iy) \,\mathrm{d}x\,\mathrm{d}y = \int_0^1 \int_0^{2\pi} r \mathbf{J}_f (r e^{i \theta}) \mathrm{d}\theta\,\mathrm{d}r,$$
and letting $\gamma_r : [0,2\pi]\to\Bbb{C}, \,\gamma_r (\theta) = r e^{i\theta},\, \gamma_r '(\theta) = i\gamma_r (\theta),$ then we get
$$\mathrm{area}\,f(D) = \int_0^1 \int_{\gamma_r}\frac{\lvert f'(z)\rvert^2 \bar{z}}{ir}\mathrm{d}z\,\mathrm{d}r.$$
Unfortunately, this approach seems to be a dead end. I think I'm meant to use Cauchy's Formula somewhere, but I can't see where that might be useful in this kind of question.","['complex-analysis', 'power-series']"
1764847,Example of a diffeomorphism from all of $\mathbb{R}$ to itself,"I can think of diffeomorphisms from an interval to $(a,b)\rightarrow \mathbb{R}$, scaling the tangent function, and from the punctured plane, polar coordinates, or some odd polynomial, but does anyone have a nontrivial example that doesn't break down at the origin?","['real-analysis', 'differential-topology', 'functions']"
1764876,How do you solve $x^2 = \left(\frac 12\right)^x $?,"I'm having trouble finding the steps to solve for $x$. The solutions to this equation are $x=-4$, $x=-2$, and $x=0.76666$ when solved graphically and through the solve function of a TI-nspire cx CAS. I tried to isolate $x$ by using various log and power rules, but the result was still something I did not know how to solve. $$x^2 =  \left(\frac 12\right)^x $$ $$\log x^2=\log\left(\frac12\right)^x$$ $$2\log x=x\log\left(\frac12\right)$$ $${\log x \over x}={\log \frac12 \over 2}$$ I also tried the following: $$x^2 =  \left(\frac 12\right)^x $$ $$x^2 = {1^x \over 2^x} $$ $1^n=1$ for all real $n$ $$x^2={1 \over 2^x} $$ $$x=\sqrt {1 \over 2^x} ={1 \over \sqrt {2^x}}={1 \over [2^x]^{1/2}}= {1 \over 2^{x/2}}$$ How do you do this?","['algebra-precalculus', 'logarithms', 'exponentiation']"
1764910,Paired T-Tests vs Independent,"The effectiveness of a training course is examined, and performance of each individual in a group is taken both before and after, and the differences are used in a paired T test. Would it be possible to also perform a two-independent-samples t test to investigate the mean difference if the data before and after were mixed as to no longer be paired? Or would the independent condition still not be satisfied, as the two sets of observations are not independent?","['statistics', 'statistical-inference']"
1764919,"How to solve the following system $\frac{\text{d}x}{\text{d} t} = -Ax + \frac{B}{y} - C$, $ \frac{\text{d}y}{\text{d} t} = -Dx + \frac{E}{y} - F$","Is there a way to analytically solve the following ODE system? $$
\frac{\text{d}x}{\text{d} t} =
-Ax +  B\left(\frac{1}{y} -1\right) \\
\frac{\text{d}y}{\text{d} t} =
-Cx + D\left(\frac{1}{y} -1\right) 
$$ Where $A,B,C,D>0$ and $x(0)=0,\ y(0)=y_0>0$. $B,D$ may also be treated as including a factor of epsilon, $\varepsilon\ll 1$, however my asymptotics hasn't gone anywhere so far.",['ordinary-differential-equations']
1764925,$\sqrt{x+938^2} - 938 + \sqrt{x + 140^2} - 140 = 38$ - I keep getting imaginary numbers,$$\sqrt{x+938^2} - 938 + \sqrt{x + 140^2} - 140 = 38$$ My attempt $\sqrt{x+938^2} + \sqrt{x + 140^2} = 1116$ $(\sqrt{x+938^2} + \sqrt{x + 140^2})^2 = (1116)^2$ $x+938^2  + 2*\sqrt{x+938^2}*\sqrt{x + 140^2} + x + 140^2 = 1116^2$ $2x + 2*\sqrt{x+938^2}*\sqrt{x + 140^2}  = 1116^2 - 938^2 - 140^2$ $x + \sqrt{x^2 + 2(938^2 + 140^2)x+(938*140)^2} = 1116^2 - 938^2 - 140^2$ At this point trying to solve for x inside the sqrt in the quadratic gives me an imaginary number. How is it possible to solve this?,"['algebra-precalculus', 'square-numbers', 'quadratics']"
1764926,Testing the Uniformly Most Powerful Test against the alternative,"Hi I am working on the following problem A single observation $X$ is made from one of three densities listed below with parameter space $\Theta=\{0,1,2\}$.
\begin{align*}
x=0\hspace{0.4cm}x=1\hspace{.4cm}x=2\hspace{.4cm}x=3\hspace{.4cm}x=4\\
f(x|\theta=0)\,\,\,\,\,\, 0.05\hspace{0.9cm} 0.05\hspace{0.9cm} 0.40\hspace{0.9cm} 0.50\hspace{0.9cm}0.00\\
f(x|\theta=1)\,\,\,\,\,\, 0.30\hspace{0.9cm} 0.40\hspace{0.9cm} 0.05\hspace{0.9cm} 0.20\hspace{0.9cm}0.05\\
f(x|\theta=2)\,\,\,\,\,\, 0.40\hspace{0.9cm} 0.30\hspace{0.9cm} 0.10\hspace{0.9cm} 0.10\hspace{0.9cm}0.10
\end{align*} a) Find the likelihood ratio test of size $\alpha=0.1$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$ b) Is the test in part a UMP against the alternative? Why or why not? c) Determine whether there exists a UMP test of size $\alpha=0.05$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$. I got the part (a) which is 
\begin{align*}
R=\{x\in\{0,1,4\}\}\\
\alpha=P_{\theta=0}(x\in\{0,1,4\})
\end{align*} I am stuck with (b) and (c) any help would be highly appreciated. Thanks in advance.","['statistics', 'statistical-inference']"
1764927,multiplication of finite sum (inner product space),"I am having difficulty to understand the first line of the proof of theorem 3.22 below. (taken from a linear analysis book) Why need to be different index, i.e. $m,n$ when multiplying the two sums? This is very basic, but I really need help for explanations.","['hilbert-spaces', 'calculus', 'functional-analysis', 'inner-products', 'linear-algebra']"
1764931,Prove the exterior derivative of the following (n-1) form is zero,Let $\omega(x)=\frac{1}{{\parallel x \parallel}^n}\displaystyle\sum_{i=1}^{n}(-1)^{i-1}x_{i} dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ be a differential $(n-1)$ form on $\mathbb{R}^n \setminus\{0\}$. Where $\widehat{dx_{i}}$ means that the $i$th term is absent from the product. Prove that the exterior derivative $d\omega=0$. What I have so far is $d\omega(x)=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1}dx_{i}\wedge dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1} (-1)^{i}dx_{1} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})dx_{1} \wedge \dots \wedge dx_{n}$ However I am unsure on how to proceed from here. I'm also not sure if what I have done so far is even correct. Any help would be greatly appreciated.,"['derivatives', 'exterior-algebra', 'tensors', 'differential-forms', 'differential-geometry']"
1764945,"In a finite dimensional inner product space with $T ∈ L(V)$, show that $\langle u,v\rangle = \langle T(u),T(v)\rangle$ implies $T$ is invertible.","Here is how I've tried to go about it, and I'm curious if it's true or if I'm way off base. T is invertible iff null$(T)=\{0\}$. Let $v∈V$ and suppose $T(v)=0$. If we can show that $v=0$, then $T$ is invertible. Consider $\langle v,v\rangle = \langle T(v),T(v)\rangle = \langle 0,0 \rangle = 0$, and $\langle v,v\rangle = 0$ iff $v=0$. Does this prove the statement? Or are there any other thoughts or methods? Thanks in advance!","['linear-algebra', 'inner-products']"
1764961,Chain Rule of Calculus as a Group Property?,"I read that the chain rule and inverse function theorem are expressions of the group property of successive non-singular transformations. How do you say this more formally? My guess is that we are saying something like 'the action of a derivation on the algebra of differentiable functions is invariant under the action of the group of non-singular transformations on the algebra'? How would you clean this up and perhaps illustrate with an example? The quoted passage mentions the implicit function theorem too, I don't see this as directly relating to a group of coordinate transformations (even though it is equivalent to the inverse function theorem), perhaps it's talking about another group property? Are other theorems from calculus like the mean value theorem or Lagrange multipliers talking about group properties? For example, Young's theorem seems to be about about the action of one-parameter groups on a function being commutative, though I don't know how to phrase this in a nicer way either. Also the definition of a convexity seems like it comes from the action of an affine group on a function v.s. it's coordinates, i.e. it's commutator is positive or negative? Any help? :D","['group-theory', 'calculus']"
1764997,"""Clear"" reason why open sets in weak topology is unbounded","In Lax's Functional Analysis book: The open sets in the weak topology are unions of finite intersections of sets of the form $\{x:a<l(x)<b\}$. Clearly, in an infinite-dimensional space the intersection of a finite number of sets of this form is unbounded. I don't really see the ""clearly"" part. I may be missing something. Do we have to consider $\cap\ker f_i$ like the answer here?: Why unit open ball is open in norm topology, but not open in weak topology?","['functional-analysis', 'analysis']"
1764999,Bronstein Integral 21.42,"Good morning. I came across the following integral in some field theory calculation: $\int_0^\pi dx\,\log\left(a^2+b^2-2ab\cos x\right)=2\pi\log\left(\max\lbrace a,b\rbrace\right)$ for $0<a,b\in\mathbb{R}$. The notation has been adapated to make contact with Bronstein's integral 21.42. Despite 6 pp.of calculations, I was not able to figure out a way to prove the identity (integration by parts, substitution, derivative trick, contour integration...). Does anyone have a proof? Best regards,
David","['integration', 'definite-integrals', 'calculus']"
1765075,$P(|X_1+X_2|<x)\le P(|X_1|<x)$ for every independent centered continuous $X_1$ and $X_2$?,"Let $X_1$ and $X_2$ be zero mean independent continuous random variables. Then, is it true that $P(|X_1+X_2|<x)\le P(|X_1|<x)$. The intuition is that summing independent variables increase variance, and would make them farther from zero more likely.","['probability-theory', 'random-variables']"
1765076,"Methods to find $f$, given the functions $f \circ g$ and $g$","There is one way, which is to use the fact that $ \ f(g(g^{-1}(x)))=f(x)$. But this method only works if $g$ has a right inverse. There are other heuristic methods, which is to ""guess the shape"" of $ \ f$, given the composite function. But is there a more powerful method that can be profitably used by non-calculus students? What about methods from calculus? I ask this because the Malaysian public exams ask these questions a whole lot, and students normally jump right into these so-called ""find the outside function"" questions by assuming that $g$ is invertible, and then get into a knot when it's not.","['elementary-functions', 'elementary-set-theory']"
1765092,A function $f$ satisfies the condition $f[f(x) - e^x] = e + 1$ for all $x \in \Bbb R$.,"Let $f$ be a function such that $f[f(x) - e^x] = e + 1$ for all $x \in \Bbb R$. Find $f(\ln 2)$. I've considered two cases: $f(x) = e^x + c$, where $c$ is constant. Then $f(c) = e^c + c = e + 1$, which implies that $c = 1$, thus $f(x) = e^x + 1$ and $f(\ln 2) = 3$. $f(x) = e + 1$. This clearly satisfies the conditions and thus $f(\ln 2) = e + 1$. Now I want to know how to analyse this equation further. And is it possible to find all $f$ that satisfy the equation above?","['algebra-precalculus', 'functional-equations']"
1765139,Areas under the graphs of $\frac{1}{x}$ and $\frac{1}{x^2}$ from $1$ to $\infty$,"A simple evaluation of the definite integral tells us that the area under the graph of  $[\frac{1}{x}]^2$ from $1$ to $\infty$ is finite whereas that of $\frac{1}{x}$ for the same limits is infinite. When we look at the graphs of these functions, we can see a striking similarity. I may, after some reasonable and concrete explanation in the direction, accept that both have a finite area under their graphs (as they are asymptotic to the x-axis) or an infinite area (as they never practically touch the x-axis). But accepting the fact that one is converging and the other is not seems absurd at this moment.
Any intuition is really appreciated.","['intuition', 'calculus', 'integration', 'definite-integrals', 'convergence-divergence']"
1765148,Cannot make sense of a derivative,"Short version of the question: In this presentation http://www.slideshare.net/ShangxuanZhang/xgboost (page 74-75)I cannot understand how the gradient of the L function is calculated. $$
L = y_i  \log  {{1}\over{1+e^{-\hat{y}_i}}}+(1-y_i)\log {{e^{-\hat{y}_i}}\over{1+e^{-\hat{y}_i}}}
$$ On page 75 it reports that $$
\textrm{grad} = {{1}\over{1+e^{-\hat{y}_i}}}-y_i
$$ But I have not been ale to reproduce the passages to get to that result. Please note that on page 73 is specified that the gradient is intended with respect to $ y_i $ , but I suspect that it should be calculated with respect to $ \hat{y}_i $ , because first it makes sense in the context, second in the whole presentation and in the paper that presents this method, gradients are always calculated with respect to $ \hat{y}_i $ . Either way I cannot understand how he obtain that result. Long version of the question: I am trying to use the xgboost R package for a little side project. One of the reasons that have lead me to try it out is the fact that it lets you define your own  customized objective function. Problem is that the example I reported above is pretty much the most detailed documentation that I were able to find. In the official documentation ( https://cran.r-project.org/web/packages/xgboost/xgboost.pdf , page 9)uses the same example and explains that you should define a function that returns gradient and second order gradient and just input it as a parameter of the training method. Making such a function wouldn't be a problem if only I were sure about what ""gradient and second order gradient"" means, and I am not sure since I cannot understand the only example I have, which is supposed to be quite simple, so if somone can explain me what calculations he permorms that would be much appreciated.","['derivatives', 'calculus']"
