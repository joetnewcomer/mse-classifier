question_id,title,body,tags
4491717,Use Abel-Plana formula to compute $\int_0^\infty \frac{\arctan(t)}{e^{2\pi t}-1} dt $,"I have asked a related problem here : $$I=\int_0^\infty \frac{\arctan(t)}{e^{2\pi t}-1} dt $$ But I want to use the Abel-Plana formula to calculate above integral. The Abel-Plana formula says: $$\sum_{n=0}^\infty f(n)=\int_0^\infty f(x)dx+\frac{1}{2}f(0)+i\int_0^\infty \frac{f(it)-f(-it)}{e^{2\pi t}-1}dt$$ This formula requires: $$|f(z)|\le \frac{C}{|z|^{1+\epsilon}}$$ to guarantee both $\sum_{n=0}^\infty f(n)$ and $\int_0^\infty f(x)dx$ converge. Can I use it if $\sum_{n=0}^\infty f(n)$ and $\int_0^\infty f(x)dx$ diverge, but $\lim_{N\rightarrow \infty} \left( \sum_{n=0}^N f(n) -\int_0^N f(x)dx\right) $ exists? Here is my attempt: $$I=\int_0^\infty \frac{\arctan(t)}{e^{2\pi t}-1} dt ~~~~~~\text{let}~~~f(z)=\frac{\ln(1+z)}{2}$$ $$f(it)-f(-it)=\frac{\ln(1+it)}{2}-\frac{\ln(1-it)}{2}=\mathrm{artanh}(it)=i\cdot \arctan(t)$$ plug into the Abel-Plana formula: $$\sum_{n=0}^\infty \frac{\ln(1+n)}{2}=\int_0^\infty \frac{\ln(1+x)}{2}dx+0-\int_0^\infty \frac{\arctan(t)}{e^{2\pi t}-1}dt$$ $$I=\int_0^\infty \frac{\arctan(t)}{e^{2\pi t}-1}dt=\lim_{N\rightarrow \infty} \left( \int_0^N \frac{\ln(1+x)}{2}dx-\sum_{n=0}^N \frac{\ln(1+n)}{2}  \right) $$ $$I=\frac{1}{2}\lim_{N\rightarrow \infty} \left( ~~(N+1)\ln(N+1)-N- \ln[(N+1)!] ~~\right) $$ let $n=N+1$ and apply Stirling's formula: $$I=\frac{1}{2}\lim_{n\rightarrow \infty} \left( ~~n\ln(n)-n+1- \ln\left[\sqrt{2\pi n} \left(\frac{n}{e}\right)^n \right] ~~\right) $$ expand and cancel the terms: $$I=\frac{1}{2}\lim_{n\rightarrow \infty} \left( 1-\frac{1}{2}\ln(2\pi)-\frac{1}{2}\ln(n)\right)$$ Surely this limit diverges. The correct result is without the last term $-\frac{1}{2}\ln(n)$ Update: I put the solution in the answer box below.","['integration', 'improper-integrals', 'definite-integrals']"
4491738,Are there at least 45 variants of l'Hopital's Rule and how to prove variant with $\lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty$?,"The following is a problem in Ch. 11 ""The Significance of the Derivative"" from Spivak's Calculus To complete the orgy of variations on l'Hopital's Rule, use Problem 55 to prove a few more cases of the following general
statement (there are so many possibilities that you should select just
a few, if any, that interest you): If $\lim\limits_{x\to [\ ]} f(x)= \lim\limits_{x\to [\ ]} g(x) = \{\
 \}$ and $\lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ )$ , then $\lim\limits_{x\to [\ ]}  \frac{f(x)}{g(x)}=(\ )$ . Here $[\ ]$ can be $a$ or $a^+$ or $a^-$ or $\infty$ or $-\infty$ , and $\{\ \}$ can be $0$ or $\infty$ or $-\infty$ , and $(\ )$ can be $l$ or $\infty$ or $-\infty$ . First of all, given the problem statement above, is it accurate to say that there are at least $5\cdot 3 \cdot 3=45$ variations of L'Hopital's Rule? I've seen questions on proving L'Hopital's Rule but given that there are many variations on the rule, and the fact that the proofs are not all exactly the same (I've gone through at least ten so far), I'd like to ask about one particular variation that I can't seem to find a proof for. When we let $[\ ]$ be $a$ , $a^+$ , or $a^-$ and let $\{\ \}$ be $\infty$ , for example, I can't seem to come up with a proof. To cut to the chase, I'd like to know how to prove the following If $\lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty$ and $\lim\limits_{x\to a^+} \frac{f'(x)}{g'(x)}=l$ , then $\lim\limits_{x\to a^+} \frac{f(x)}{g(x)}=l$ . The proofs of l'Hopital's Rule I have seen usually start by using the assumption that the limit $\lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ )$ exists to establish existence of $f'$ and $g'$ and $g'\neq 0$ on a certain interval $A$ that depends on what the limiting value is in the limits, ie what is chosen as the $[\ ]$ parameter. Next the Mean Value Theorem is invoked to establish either that $g(x)\neq 0$ or $g(x)-g(a)\neq 0$ on interval $A$ . Then the Cauchy-Schwarz MVT is invoked to obtain some relationship of one of the forms $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ $$\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ where $\alpha_x \in A$ . At this point the limit of both sides is taken and we end up with the conclusion of the particular variant L'Hopital's Rule we are proving. Obviously I skipped over a few steps in this description, but this is a general outline. I believe the issue I have for proving the statement I asked about above is application of MVT. If the limit of $f$ and $g$ when $x$ approaches $a$ is $\infty$ , we can't use point $a$ to apply the MVT as is done in the proofs of all the other variations of L'Hopital's Rule as far as I can tell.","['limits', 'calculus', 'derivatives']"
4491774,Mechanics of the second derivative test?,"I am trying to understand why $f'(x)=0$ and $f''(x)>0 $ together imply a local minimum, and why $f'(x)=0$ and $f''(x)<0$ together imply local maximum. I am new to calculus and don't know multivariable calculus, so please give the reasoning or proof in terms of single-variable calculus. An intuitive explanation is preferred but a rigorous proof is also fine.","['maxima-minima', 'calculus', 'derivatives']"
4491807,Expected number of distinct objects in sampling with replacement,"Given the set of numbers from 1 to n: { 1, 2, 3 .. n } We draw n numbers randomly (with uniform distribution) from this set (with replacement). What is the expected number of distinct values that we would draw? My Approach : Let $X(k)$ denote the expected number of distinct values in a sample of size $k$ . Then, $X(k) =  \frac{n - X(k-1)}{n}*(1 + X(k-1)) + \frac{X(k-1)}{n}*X(k-1)$ $X(k) =  1 + \frac{n-1}{n}*X(k-1)$ Since $X(1) = 1$ , solving the recursive relation, we get $X(k) =  1 + (\frac{n-1}{n}) + (\frac{n-1}{n})^2 + (\frac{n-1}{n})^3 + ... + (\frac{n-1}{n})^{k-1}$ $X(k) = \frac{1-(\frac{n-1}{n})^k}{\frac{1}{n}} = n*(1-(1-\frac{1}{n})^k)$ Hence, $X(n) = n*(1-(1-\frac{1}{n})^n)$ The answer is correct, but I doubt if my approach is correct or not. The idea behind the first equation is: after $k-1$ th sample, the probability of getting a new value in $k$ th sample is $ \frac{n - X(k-1)}{n} $ . Since $X(k-1)$ is not necessarily an integer, I doubt if the probability is correct or not. So my question is: is my approach correct or not? please provide some convincing explanation as to why or why not is it correct.","['expected-value', 'probability']"
4491820,Finding $n$ such that $\frac{n+1}{n}$ < $\frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$,"I have been thinking about this limit: $$\lim\limits_{n \rightarrow \infty}\frac{n}{\sqrt[n]{n!}} = e$$ Using a spreadsheet, I noticed that for $0 < n \le 150, \frac{n+1}{n} > \frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$ . The difference $\frac{n+1}{n} - \frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$ is strictly decreasing as $n$ increases. I wondered if there exists an integer $k$ such that if $n \ge k$ , then $\frac{n+1}{n} < \frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$ What would be a standard way to determine if $k$ exists?  And if $k$ exists, what would be a standard way to determine $k$ ? I suspect that there is a simple way to approach this question without using the gamma function . Am I right? Or does the determination of $k$ require using the gamma function?","['limits', 'inequality', 'factorial', 'radicals']"
4491826,Riemannian distance function on a product manifold,"Let $(M_1,g_1)$ and $(M_2,g_2)$ be two Riemannian manifolds. Let $d_1$ and $d_2$ be the respective induced Riemannian distance functions, i.e. metrics in the sense of metric spaces. Let $(M_1\times M_2,g_1\oplus g_2)$ be the product Riemannian manifold, and let $d$ be the induced distance function on $M_1\times M_2$ . Let $d_{\text{prod}}$ be the product metric on $M_1\times M_2$ in the metric sense, i.e. $$d_{\text{prod}}((x_1,x_2),(y_1,y_2))=\sqrt{d_1(x_1,y_1)^2+d_2(x_2,y_2)^2}.$$ Question 1: Is it always true that $$d=d_{\text{prod}}?$$ Comment: This seems to be true if either $M_1$ or $M_2$ is a manifold where any two points are connected by a unique geodesic (e.g. $\mathbb{R}$ ). I suspect that in general we only have $d\leq d_{\text{prod}}$ . But what is an illustrative example? Question 2: If not, is there anything that we can say about the relationship between $d$ and $d_{\text{prod}}$ ? (E.g. does there exist $C>0$ such that $\frac{1}{C}d\leq d_{\text{prod}}\leq Cd$ ; do they induce the same topology on $M_1\times M_2$ ?)","['riemannian-geometry', 'metric-spaces', 'manifolds', 'general-topology', 'differential-geometry']"
4491956,"Source to ""Strongly zero-dimensional and zero-dimensional is equivalent for separable metrizable spaces""?","What is the source of the claim that ""Strongly zero-dimensional and zero-dimensional is equivalent for separable metrizable spaces""? My university lecture told me that this is true, but I fail to find it anywhere in literature. Could you provide a source for this? Definitions A Hausdorff topological space $X$ is zero dimensional if for every point $x$ of $X$ and every neighborhood $U$ of $x$ in $X$ , there exists a nonempty clopen subset $V$ of $X$ such that $x \in V \subset U$ .
The clopen basis of any zero-dimensional space is a collection of clopen sets that is closed under complements and finite intersections. A Hausdorff topological space $X$ is said to be strongly zero-dimensional whenever for every closed subset $A$ of $X$ and every open subset $U$ of $X$ such that $A \subseteq U$ , there exists a clopen subset $V$ of $X$ such that $A \subseteq V \subseteq U.$","['separation-axioms', 'reference-request', 'zero-dimensionality', 'definition', 'general-topology']"
4491999,"$\{ a \mid a\not \in [G,G],\ a \in Z(G) \} \cup \{e\}$ is a group or not","Let $G$ be a group. I want to check whether the set $$\{ a \mid a\not \in [G,G],\ a \in Z(G) \} \cup \{e\}$$ is a subgroup or not, where $[G,G]$ and $Z(G)$ are the commutator subgroup and center of the group $G$ respectively. In a vector space $V$ we can find the complement subspace $U$ of any its subspace $U$ such that $V=W\oplus U$ ? Can we generalize this to my case?","['finite-groups', 'derived-subgroup', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
4492047,Help with proof (by induction) for unbiasedness of an estimator,"I am reading a paper where the following estimator is used for estimating the sample mean from a series of observations/data-points $\{r_1, r_2,....,r_n\}$ : $$ r_n = \lambda_n p + (1-\lambda_n) \hat{p_{n-1}} $$ , where $\hat{p_{n-1}}$ is defined as: $$ \hat{p_{n-1}} = \frac{\sum_{i=1}^{n-1} r_i}{n-1}$$ . For more context, it is assumed that the observations $\{r_1, r_2,....,r_n\}$ are collected sequentially, i.e., first $r_1$ is observed, followed by $r_2$ and so on. Also the subsequent $r$ values depend on the previous values. It is given that $$\mathbb{E}[r_1] = \mathbb{E}[\hat{p_{1}}] = p  $$ , where $p$ is the $\textbf{true}$ estimate of the sample mean. It is claimed in the paper that the estimator $r_n$ is an unbiased estimator of the true mean $p$ . The proof in the paper works as follows by induction: Base case $$\mathbb{E}[r_1] = \mathbb{E}[\hat{p_{1}}] = p  $$ . Then by induction: $$\mathbb{E}[\hat{p_{n}}] = \frac{(n-1) \mathbb{E}[\hat{p_{n-1}}] + \mathbb{E}[r_n] }{n} \\ 
= \frac{ \lambda_n p + (n-\lambda_n) \mathbb{E}[\hat{p_{n-1}}]   }{n} \\
= p $$ I don't understand how 3rd step is derived from the second step. $$\mathbb{E}[r_n] = \mathbb{E}[\lambda_n p + (1-\lambda_n) \hat{p_{n-1}}] \\ 
= \lambda_n p + (1-\lambda_n) \mathbb{E}[\hat{p_{n-1}}] \\
= p $$ Is this correct? From my understanding, for proof by induction, first base case should be correct, followed by assuming the statement for some value $k$ , followed by extending it for case $k+1$ . In this proof, I couldn't generalize from $r_k$ to $r_{k+1}$ (or from $\hat{p_{n}}$ to $\hat{p_{n-1}}$ ).
They use these equations for the final proof of unbiasedness as follows:","['statistics', 'conditional-expectation', 'expected-value', 'solution-verification', 'induction']"
4492054,Profit maximization with Markov Chains,"Problem : An opera singer is due to perform a long series of concerts. Having a bad temper, they are liable to pull out each night with probability $1/2$ . Once this has happened they will not sing again until the promoter convinces them of the promoter’s high regard. This the promoter does by sending flowers every day until the singer returns. Flowers costing $x$ thousand pounds, $0 ≤ x ≤ 1$ ,
bring about a reconciliation with probability $\sqrt{x}$ .The promoter stands to
make $£750$ from each successful concert. How much should they spend on flowers? I'd like to have solution verification or alternative approaches . (Problem's from exercise 1.10.4 of Markov Chains by J.R. Norris) Interpretation : I suppose the singer pulls out independent of the past given that they performed last night.  Also I suppose $x$ is ""how much they[the promoter] should spend on flowers"", which is time-homogeneous. I suppose promoter wants to maximize their expected profit. Plan : I'll use a 2 state MC to track whether the singer performs or not . Then I'll find the long-run proportion of time that singer performs : $v(x)$ . Finally $x$ will be the maximizer of $f(x) = 750v(x)-x(1-v(x))$ , I think $f(x)$ should approximate the expected profit . If the MC turns out to be irreducible , then by Theorem 1.10.2 , $v(x)$ will almost surely be the inverse of expected return time of the state that the singer performs . Theorem 1.10.2  Let $P$ be irreducible and let $\lambda$ be any distribution . If $(X_n)_{n\ge 0}$ is Markov $(\lambda,P)$ then $$
\mathbb{P}\left( \frac{V_i(n)}{n} \to \frac{1}{m_i} \text{ as } n \to \infty  \right) = 1 
$$ where $V_i(n) = \sum_{k=0}^{n-1} 1_{\{X_k = i\}}$ and $m_i$ is expected return time to state $i$ . Attempt : Let $(X_n)_{n\ge 0}$ be a Markov chain such that the initial distribution is uniform and $
X_n = 
\left\{\begin{array}{cc}
1 & \text{if singer performs } \\
0 & \text{else } 
\end{array}\right.
$ . So transition probabilities are $$
\left\{\begin{array}{cc}
p_{10} = 1/2  , &  p_{11} =  1/2 \\
p_{01} = \sqrt{x} , & p_{00} = 1 - \sqrt{x}
\end{array}\right.
$$ with $x\in (0,1] $ . $(X_n)_{n\ge 0}$ is irreducible on state space $\{0,1\}$ . The expected return time to $1$ is $m_1 = 1 + \frac{1}{2}\frac{1}{\sqrt{x}} $ , so $v(x) = 1/m_1  $ . When $x = 0 $ , $\{0\}$ becomes the absorption state , the expected profit is $\frac{1}{2} 750 \sum_{x=1}^{\infty}  x\left(\frac{1}{2}\right)^x  = \frac{1}{2} 750(2) = 750 $ . Numerically I found the maximum of $f(x) , x\in (0,1]$ is around $500$ at $x=1$ . So should I conclude that he should spend $x=0$ GBP on her ?","['markov-chains', 'solution-verification', 'discrete-mathematics', 'game-theory', 'probability']"
4492104,What is $\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx $?,"I'm looking for closed-form expressions for the integral $$I:=\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx .$$ Some related integrals that I've found include : $$J:=-\int_0^\infty \log(1-\cosh(x))\frac{x^2}{e^x}\,dx = 6 + 2\ln(2) - {2\pi^{2} \over 3} - 4\zeta(3) - 2\pi i,$$ and this one: $$K:= \int_0^{\frac{\pi}{2}}\log\Big{(}\cos(x)\Big{)}dx = - \frac{\pi}{2} \log(2) ,$$ and (p. 530) $$L:=\int_{0}^{\infty} \big{(} \ln(1-e^{-x}) \big{)}  dx = - \frac{\pi^{2}}{6} .$$ Moreover, I've encountered various integrals of the sinc function, including : $$M:= \int_{0}^{\pi/2} \ln \bigg{(} \frac{\sin(x)}{x} \bigg{)} dx = \frac{\pi}{2} \Big{(} 1 - \ln(\pi) \Big{)} .$$ However, I haven't found any closed-form expressions for $I$ yet, only an approximation: $$I \approx -5.75555891011162780816$$ and I'm not sure how to proceed with the integral.","['trigonometric-integrals', 'definite-integrals', 'analysis']"
4492138,How to prove $n^5-5n^3+4n = 5!{n+2\choose 5}$,"I understand the answer, but I am stuck on blue underline part. Please someone describe the blue underline section and how it's come from. Thank you. Source Number theory: Structure , Examples and Problems by Titu Andreescu .",['algebra-precalculus']
4492148,Is a locally free sheaf of modules free on some affine open basis?,"Let $(X,O_X)$ be a scheme, let $F$ be a locally free sheaf of $O_X$ -modules on $X$ . According to this MSE answer , it is not necessarily true that for any affine open $U \subset X$ we have $F|_U$ is (isomorphic to) a direct sum of $O_U$ 's. However, is it possible to find an affine open base $\{U_i\}_{i\in I}$ of $X$ ,
such that $F|_{U_i}$ is isomorphic to $\bigoplus_{j\in J_i}(O_{U_i})$ for some index set $J_i$ , for each $i \in I$ ? My thinking so far is, I would like to show if $F$ is ""free over"" some open set $U$ , then it is also ""free over"" any open subset of $U$ (or at least, for each $x \in U$ there is an open $V$ with $x \in V \subset U$ such that $F|_U$ is free over $V$ ). Since a localization of a free module is free, I see why the above is true when $U$ is affine, but I'm not sure what to do for the non-affine case. Could anyone explain how to approach this? Or, is my thinking incorrect here?","['algebraic-geometry', 'schemes']"
4492191,Is $\exp(\log(A))=A$ for any matrix $A$ where $\log(A)$ is defined?,"It is possible to show that: $$\exp(\log(A)) = A$$ When $A$ is diagonalizable (and the logarithm exists). Furthermore it is possible to show that when $\| A - I\| < 1$ then $\exp(\log(A))=A$ . The proof for that relies of the fact that any matrix can be approximated by a series of diagonalizable matrices, and on the fact that when $\| A - I\| < 1$ the logarithm always exists. This let's our approximation $A_n \to A$ eventually (for large enough $n$ ) lie in the ball of radius $1$ around $I$ which allows us to invoke the continuity of $\exp, \log$ and finish the theorem. My question is, is the equality $\exp(\log(A)) = A$ always true when $\log(A)$ exists? The same proof could be applied only if the $\log$ exists in some open neighborhood of $A$ , but that's not always the case. For example taking $A = 2\cdot I$ then for any $\epsilon$ we have $\log((2 + \epsilon)\cdot I)$ doesn't exist. Despite that, $\exp(\log(2\cdot I)) = 2\cdot I$ . Is there a proof that $\exp(\log(A)) = A$ when $\log(A)$ exists? Is there a counterexample? In this question the matrix logarithm is defined via the power series: $$\log(A) = \sum_{n=1}^\infty (-1)^{n+1} \frac{(A-I)^n}{n}$$","['matrices', 'matrix-exponential', 'exponential-function']"
4492207,Logarithmic Equation(s),"I have two equations. I wasn't sure how to properly proceed with them. $ x(\log_8x - 1) = 64
$ $ u(u-1) = \ln(u)$ I don't think I can set each argument to the right side like: $u = \ln(u)$ $u-1 = \ln(u)$ The top equation is easy to approximate $x = 64$ , but I'm looking for a more rigorous way. (Edit: I made a mistake on (1) it is now written correctly.",['functions']
4492237,On the convergence of $\sum_n 1 /(n\sin(2^nx))$,"Find all values of $x$ such that $\displaystyle\sum_{n=1}^\infty\frac{1}{n\sin(2^nx)}$ converges. I've been attempting to solve this problem without much success. Firstly, it is defined for all $x\in\mathbb R$ such that $\forall n\in\mathbb N$ , $2^nx\not\in\pi\mathbb Z$ . Then, if $x$ is such that $\sin(2^nx)$ converges then by comparison to $\sum\frac1n$ the series diverges. If $x\in\pi(\mathbb R\backslash\mathbb Q)$ , then $(\sin(2^nx))_{n\in\mathbb N}$ is dense in $[-1,1]$ and so it feels like the series should diverge. On the other hand, I believe that if $x\in\pi\mathbb Q$ , then $(\sin(2^nx))_{n\in\mathbb N}$ should be periodic (or not far from) and the series should converge. However, I can't prove any of this. Any help is appreciated!","['trigonometry', 'sequences-and-series', 'real-analysis']"
4492264,Measurable selection for conditional expectation,"Let $(\Omega,\mathcal{F}, P)$ be a probability space, and $\mathcal{X}\subset \mathcal{F}$ be a sub- $\sigma$ -algebra. Let $X$ be an $\mathcal{X}$ -measurable random variable and $Y$ be an $\mathcal{F}$ -measurable random variable. Then given a bounded continuous function $f:\mathbb{R}^2\to \mathbb{R}$ , is it possible to ensure  that  there exists an $\mathcal{X}$ -measurable random variable $Z$ such that $$\mathbb{E}[f(X,Y)\mid \mathcal{X}] =f(X,Z).
$$ Does the condition that $\{f(x,y)\mid y\in  \textrm{Range}(Y)\}$ is convex for all $x$ help? It is clear that if $ f(x,y)=g(x)y$ for all $x,y$ , then $Z=\mathbb{E}[Y\mid \mathcal{X}]$ satisfies the requirement. I was wondering whether it is possible to go beyond this affine case.","['measure-theory', 'convex-analysis', 'probability', 'real-analysis']"
4492306,Pinsker $\sigma$-algebra is $T$ invariant,"Let $(X,B,\nu )$ be a separable probability space and $T:X\to X$ be a measure-preserving transformation. We define $D(T)$ as the collection of all $A\in B$ for which there exists a partition $Q$ of $X$ such that $A\in Q$ , $H(Q)$ is finite and $h(T,Q)=0$ (entropy of $T$ with respect to $Q$ ). The Pinsker $\sigma$ -algebra $P(T)$ is defined as the $\sigma$ -algebra generated by $D(T)$ . I want to show that $T^{-1}(P(T))=P(T)$ . I was able to show that $T^{-1}(P(T))\subset P(T)$ , but so far all my attempts to show the converse inclusion fails. I am using as bibliography: Topics in Ergodic Theory by W. Parry Any help will be appreciated.","['measure-theory', 'entropy', 'ergodic-theory', 'probability', 'dynamical-systems']"
4492315,Is a 'column vector' actually a vector or a matrix?,"I've read this post and posted my own question , but I think I will write a more direct question related to this topic, I understand how row and column vectors can be used to represent vectors as answered in the question, but is it actually a vector, or a way of us trying to give the idea of components by putting them in a matrix? For example, Euclidian vectors have no concept of 'transpose', if the components are equal, its the same vector, yet row and column vectors are transpose of each other? A question I have is say we have: $y = \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           x_{3}
         \end{bmatrix}$ Can we have also $y = [x_{1},x_{2},x_{3}]$ ? as then we have \begin{bmatrix}
  x_{1} \\
  x_{2} \\   
  x_{3}
  \end{bmatrix} = $[x_{1},x_{2},x_{3}]$ and this gives $y=y^T$ which would be an incorrect result.","['matrices', 'soft-question', 'linear-algebra', 'vector-spaces']"
4492317,trying to prove a property of vector space,"I want to prove this property: If $V$ is a vector space, $X$ a vector in $V$ , then $0\odot X = 0$ . My proof: From axiom $1\odot X = X$ , \begin{align}
1 &\odot X = X \\
&\implies& (1+0) \odot X &= X \\
&\implies& 1 \odot X \oplus 0 \odot X &= X 
&&\text{(from axioms of the vector space)} \\
&\implies& X \oplus 0 \odot X &= X \\
&\implies& -X \oplus X \oplus0 \odot X &= X \oplus -X \\
&\implies& (-X \oplus X) \oplus 0 \odot X &= X \oplus -X 
&&\text{(associative property)} \\
&\implies& 0 \oplus 0 \odot X &= 0 \\
&\implies& 0 \odot X &= 0
\end{align} Done. Is my proof ok? Or there is another simple way?","['solution-verification', 'linear-algebra']"
4492334,"Recurrence Relation Inequality claim in ""Polynomomial Methods in Combinatorics""","While this question is in the context of combinatorics, I do not believe it plays any special role beyond producing the problem. In the proof of Theorem 10.18 in Larry Guth's book ""Polynomial Methods in Combinatorics"" the following assertions are made. (page 134 in particular) We have the recurrence relation: $$|S_{j+1}| \leq \frac{1}{100} |S_j| + \frac{1}{200} K L^{3/2 + \varepsilon}r^{-2}$$ where $L$ is a positive integer, and $K$ is a large constant. Now Guth claims that if we define $J = 1000 \log L$ and notice that $|S_1| \leq L^2$ we can conclude that $$|S_J| \leq \frac{1}{100} KL^{3/2 + \varepsilon} r^{-2}.$$ We also have the fact that the sequence of $|S_j|$ is non-increasing and that $2 < r \leq 2L^{1/2}$ . I'm not seeing how one can make this leap, but perhaps I am missing something straightforward.","['inequality', 'combinatorics', 'recurrence-relations']"
4492339,Curvature on an Associated Vector Bundle,"I am in the midst of trying to solve problem 13 from chapter 5 of Hamilton's Mathematical Gauge theory text, and have come across some terms I am not sure what to do with. I will elaborate below. Let $P\rightarrow M$ be a principal $G$ bundle, and $E=P\times_\rho V\rightarrow M$ an associated vector bundle. Fix a connection $A$ on a $P$ , the induces the following covariant derivative on $E$ , letting $\Phi$ be a local section of $E$ : $$\nabla^A_X\Phi=[s,d\phi(X_x)+\rho_*(A_s(X_x))\phi]$$ where $s$ is a local section of $P$ , $\phi$ is a map $U\subset M\rightarrow V$ , $X\in \mathfrak{X}(M)$ , and where $A_s=s^*A$ , can be thought of as a Lie algebra valued one form on the base $M$ . In a principal bundle we define curvature as: $$F=dA+\frac{1}{2}[A,A]$$ The curvature of a covariant derivative in an arbitrary vector bundle is: $$F^\nabla(X,Y)\Phi=\nabla_X\nabla_Y\Phi-\nabla_Y\nabla_X\Phi-\nabla_{[X,Y]}\Phi$$ I am trying to show that in an associated vector bundle: $$F^\nabla(X,Y)\Phi=[s,\rho_*(F_s(X,Y))\phi]$$ where $F_s=s^*F$ . So I calculated the following: $$\nabla^A_X\nabla^A_Y\Phi=[s,d\left(\rho_*(A_s(X_x)\right)(Y_x)\phi+\rho_*(A_s(X_x))d\phi(Y_x)+\rho_*(A_s(X_x))d\phi(Y_x)+\rho_*(A_s(X_x)A_s(Y_x))]$$ $$\nabla^A_X\nabla^A_Y\Phi=[s,d\left(\rho_*(A_s(Y_x)\right)(X_x)\phi+\rho_*(A_s(Y_x))d\phi(X_x)+\rho_*(A_s(Y_x))d\phi(X_x)+\rho_*(A_s(Y_x)A_s(X_x))]$$ $$\nabla^A_{[X,Y]}\Phi=[s,d\phi([X,Y]_x)+\rho_*(A_s([X,Y]_x)\phi]$$ Putting it all together I obtain that: $$F^\nabla(X,Y)\Phi=[s,d\left(\rho_*(A_s(Y_x)\right)(X_x)\phi-d\left(\rho_*(A_s(X_x)\right)(Y_x)\phi+\rho_*(A_s(X_x)A_s(Y_x)-A_s(Y_x)A_s(X_x))\phi-d\phi([X,Y]_x)-\rho_*(A_s([X,Y]_x)\phi]$$ I feel like I see all the pieces, but am unsure of how to move them around. Like the third term I'm pretty sure simplifies to $\frac{1}{2}[A_s,A_s](X_x,Y_x)$ , but I don't know what to do with the exterior derivative of $\rho_*$ , or what to do with the Lie bracket terms, especially since it seems like I have all the pieces without those terms, so I feel like they should cancel out some how but that doesn't make a ton of sense to me. Any help would be greatly appreciated. Edit: I suspect I'm messing up something up with the exterior derivative, perhaps $d(d\phi(X_x)(Y_x)$ isn't zero? Because you technically contract then apply the exterior derivative again? I am unsure...","['principal-bundles', 'curvature', 'gauge-theory', 'differential-topology', 'differential-geometry']"
4492358,Norm of geodesic velocity vector,"The definition of affine geodesic is clear: a curve with covariant derivative respective to Levi-Civita connection $\nabla$ of velocity vector $\dot{\gamma}$ respective to vector field $\dot{\gamma}$ , for arbitrary instant $t \in [0, 1]$ equal to zero. Locally, we may interchange the term ""geodesic curve"" with exponential map exp ${}_p(s v)$ . My question is: is the norm $\langle \dot{\gamma}, \dot{\gamma} \rangle$ along a geodesic constant? My best answer is ""No."". My best argument is ""Let some arbitrary positive scalar $\varepsilon$ such that following inequality $0 < \varepsilon < 1$ and a geodesic curve such that initial values are $(p, \, v)$ and the  vector norm $\langle v, \, v\rangle$ is equal scalar $\ell$ . The velocity vector $\dot{\gamma}(\varepsilon)$ at instant $\varepsilon$ . Therefore, the velocity vector norm at $\dot{\gamma}(\varepsilon)$ is equal to $\varepsilon \ell$ since we may define thee exponential map exp $_{\gamma(\varepsilon)} \dot{\gamma}(\varepsilon)$ ."" It contradicts the fact of covariant derivatives as a parallel transport operation i.e. that the vector norm is constant along the geodesic curve.",['differential-geometry']
4492369,Uniquely identify a point in 2d space in reference to known points,"Context I'm making a 2D sci-fi game, and I've convinced myself that it would be neat to have player positions described in reference to nearby stars rather than using some kind of coordinate system. Realistically, the player would not have any way to measure its distance to nearby stars, but it could measure angles between nearby stars. Problem You would need to use at least 3 nearby stars to identify the Player's location, but just any old 3 stars are not sufficient. Under certain conditions, the 3 angles between the Player and each of the 3 stars can represent 2 points. Visual showing 3 stars, the player position, and a degenerate point that has the same angles as the player Here the red circles are the 3 stars, the pink circle is the Player, and the Yellow circle is a degenerate point. The arcs show all of the points (with some error margin) that can be represented by just a single angle (ie. The green arcs show all the possible positions that have the same angle Star 2 - Player - Star 3 ). Keeping the same star positions, you can manipulate the player position to find an arangement that does not produce a degenerate point. Visual showing 3 stars and the player position, with the player position modified so that there is no degenerate point ( Here is the janky jsfiddle I made that generates these images. Re-run to randomize the star positions, click to move the player position to the mouse position. This is not precise nor efficient, but serves to provide a visual aid) Question How could I compute whether or not a given relationship produces a degenerate point? Or perhaps more relevant to my problem: Given a set of nearby stars, how could I select 3 stars such that the angles produced resolve to exactly 1 point in space?","['trigonometry', 'geometry']"
4492393,"For the set $X=\{g\in G:g^p=1\}$, show that $p$ divides $|X|$.",Let $G$ be a finite group and $p$ be a prime divisor of $|G|$ . Consider the set $X=\{g\in G:g^p=1\}$ . Show that $p$ divides $|X|$ . My attempt: Consider the action of $G$ on $X$ by conjugation. Then ${\rm Stab}_g=C_G(g)$ for all $g\in X$ . I'm stuck here.,"['group-theory', 'group-actions', 'finite-groups']"
4492420,"The matrix logarithm is well-defined - but how can we *algebraically* see that it is inverse to the exponential, as a finite polynomial?","This question is inspired by this which I saw earlier today. I started writing my answer, to share the insight that the matrix logarithm can be defined on matrices that do not have unit norm using an alternative technique. Now, Sangchul has posted a great answer explaining how it is that we know the map $X\mapsto\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n}X^n$ defines a logarithm of $1+X$ whenever the sum is convergent, whenever $\|X\|\lt1$ . By scaling, we can use this map to obtain arbitrary logarithms since there is an easy definition $\log(\lambda I)=(\log\lambda)I$ which satisfies $\exp(\log(\lambda I))=\lambda I$ trivially, and this matrix will also commute with all other matrices. I prefer an approach that sidesteps the functional calculus entirely, that I first learnt on Wikipedia over a year ago. The argument proceeds like this, paraphrased by me: For any invertible $n\times n$ complex matrix $X$ , there is a basis of the space $\Bbb C^n$ in which $X=\bigoplus_{m=1}^kJ_m$ a decomposition into Jordan blocks with some associated eigenvalues $\lambda_m$ . If we can find a matrix $Y=\bigoplus_{m=1}^nT_m$ , where $\exp(T_m)=J_m$ for all $m$ , then a simple inspection of the exponential series shows $\exp(Y)=\bigoplus_{m=1}^n\exp(T_m)=\bigoplus_{m=1}^nJ_m=X$ , so $Y$ is a logarithm of $X$ . Many will exist due to branching concerns. It remains to find a logarithm of any arbitrary Jordan block. For a block $J$ with eigenvalue $\lambda$ , we can write $J=\lambda(I+K)$ where $K$ is the matrix will all zero entries, except for entries $\lambda^{-1}$ on the first superdiagonal ( $\lambda\neq0$ by invertibility). If we suppose the formal power series argument is valid , we can say: $$\begin{align}\log(\lambda(I+K))&=\log(\lambda)I+\log(I+K)\\&=\log(\lambda)I+K-\frac{1}{2}K^2+\frac{1}{3}K^3\\&+\cdots+(-1)^j\frac{1}{j-1}K^{j-1}\end{align}$$ Since $K$ will be nilpotent of order $j$ if $j$ is the dimension of the Jordan block, the tail terms of the Mercator series vanish. Any branch of the complex logarithm is appropriate for $(\log(\lambda))I$ - due to commutativity, re-exponentiation gives $\lambda\exp(K-(1/2)K^2+\cdots)\overset{?}{=}\lambda(1+K)$ . Then to claim that this process produces a logarithm for all invertible $X$ , it suffices to demonstrate the following: For all $\lambda\in\Bbb C$ and all $n\times n$ square matrices $K$ of the form: $$K=\begin{pmatrix}0&\lambda&0&0&\cdots\\0&0&\lambda&0&\cdots\\0&0&0&\lambda&\cdots\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix}$$ We have the identity (by commutativity, the two are equivalent): $$\exp\left(\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\right)=\prod_{m=1}^{n-1}\exp\left(\frac{(-1)^{m-1}}{m}K^m\right)=I_n+K$$ I am looking for an algebraic (or similar) proof of this result. Don't get me wrong - I appreciate indirect proofs, and find them rather magical whenever they arise, but I have never studied any rigorous development of the functional calculus. Analytic functions can be extended to matrix arguments through a variety of methods - matrix Taylor series, Cayley-Hamilton, bizarre Cauchy integral representations, power series convergent in Banach space... and I accept all of these as extensions , with perhaps some convenient properties such as derivative relations carrying over from the complex/real-analytic theory. However, it seems suspicious that ""higher-order"" properties should also be preserved in this extension process. Although we can give well-defined and well-motivated analogues of $\exp$ and $\log$ to matrices, I don't see any immediate reason, a priori , to suppose that the extension reflects relations between them such as $\exp\log\equiv\mathrm{Id}$ . The main reason for my suspicion is the following observation: analysis tends to work through limiting arguments, and finite sums just won't do - they yield polynomials only. It is then rather odd that an analytic series maintains its ""special"" properties despite collapsing into a finite polynomial series - $\Bbb C$ has no nilpotent nonzero elements, but the space of matrices certainly does, and is a key ingredient in the above construction of the logarithm. So what am I looking for? I'm looking for a strong explanation for why what I'm calling ""higher-order"" properties should carry over in this extension process, especially since there are many different ways to extend analytic functions to matrix arguments: of course, any properties that can be deduced from the power series will carry over, e.g. $\exp(A+B)=\exp(A)\exp(B)$ if $A,B$ commute. However, $\exp\circ\log$ 's power series is unclear to me here, since the $\log$ is not actually a power series in this context, really, but a finite polynomial. To reiterate, it is the use of nilpotent elements that concerns me - since this challenges the algebra of $\Bbb C$ , I feel it should also challenge the analytic series which hail from $\Bbb C$ : at least, it should need some more justification. An algebraic proof (direct matrix-arithmetic proof, or maybe some clever linear algebra argument) for why this particular nilpotent logarithm should hold, would be greatly appreciated. My thoughts on this matter so far: $$T:=\sum_{m=1}^{n-1}\frac{(-1)^{m-1}}{m}K^m\\=\begin{pmatrix}0&\lambda&-\frac{1}{2}\lambda^2&\cdots&(-1)^n\frac{1}{n-1}\lambda^{n-1}\\0&0&\lambda&\cdots&(-1)^{n-1}\frac{1}{n-2}\lambda^{n-2}\\0&0&0&\cdots&(-1)^n\frac{1}{n-3}\lambda^{n-3}\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&0\end{pmatrix}$$ An observation of the way that this matrix's elements ""shift"" up a diagonal every time the matrix is squared, cubed, etc. shows that the main superdiagonal has nonzero entries once and only once in $I+T+\frac{1}{2}T^2+\cdots$ , and we can easily partially compute the exponential: $$\exp(T)=\begin{pmatrix}1&\lambda&?&?&\cdots&?\\0&1&\lambda&?&\cdots&?\\0&0&1&\lambda&\cdots&?\\0&0&0&1&\cdots&?\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&0&\cdots&1\end{pmatrix}$$ So it remains to show that the sum over all the remaining superdiagonals vanishes. Other pertinent point is that $T$ is nilpotent of order $n$ , so the exponential series terminates at $\frac{1}{(n-1)!}T^{n-1}$ . This re-explains my suspicion, since now we are claiming these two polynomials are inverse, which is false in the complex world that we began in. How can we fill in the algebraic gaps here? The matrix powers seems quite intractable to symbolically compute. N.B. Sanchul's answer in the linked post never once needs the invertibility of $X$ , as far as I can see, whereas this Wikipedia-based construction does. How do we reconcile the two?","['matrix-calculus', 'linear-algebra', 'functional-calculus', 'matrix-exponential']"
4492427,How to get the rejection region and type II error probability?,"In this paper https://arxiv.org/abs/2102.00356 , in the section 5.2
the author does the independent test on Section 5. Assume that $\pi$ is the coupling of probability measures $\mu$ and $\nu$ on $[0,1]$ . The hypothesis test for independence is that $$
H_0: \pi=\mu\times \nu \, , H_a: \pi\neq \mu\times \nu
$$ The test statistic is $W(\hat{\pi}^N)$ . In my understanding, this Corollary says that
under $H_0$ holds, we reject the $H_0$ if $$
W(\hat{\pi}^N)>C(\nu)(C_N+\frac{\sigma}{\sqrt{N}}\Phi^{-1}(1-\alpha))
$$ In the paper, the author upper bound the estimator by an asymptotic normal  random variable as in the proof in P11, that is $$
W(\hat{\pi}^N)\le C(\nu)T_N(\pi)
$$ where under $H_0$ holds, $\sqrt{N}(T_N(\pi)-C_N)/\sigma\to N(0,1)$ in Lemma A.1 (page 25). My question is that Q1: How to get this rejection region? It seems that we need to upper bound $$
P(W(\hat{\pi}^N)>C(\nu)(C_N+\frac{\sigma}{\sqrt{N}}\Phi^{-1}(1-\alpha))|H_0 true)\le \alpha
$$ for significance level $\alpha$ . Q2: In the simulation, for two data sets $X\sim Unif[0,1]$ and $Y=0.2X$ (they are dependent), why $$
P(\text{type II error})=P(\text{accept} H_0|H_0 \text{false})=P(W(\hat{\pi}^N)>>C(\nu)(C_N+\frac{\sigma}{\sqrt{N}}\Phi^{-1}(1-\alpha))?
$$ It seems this Corollary holds as $H_0$ is true. But here our data samples are not independent ( $H_0$ is wrong).","['statistics', 'probability']"
4492470,Evaluating the value of first derivative at $x=1$ for a polynomial $f$ satisfying $f(x)+f'(x)+f''(x)=x^5+64$,"Let $f(x)$ be a polynomial function such that $f(x)+f'(x)+f''(x)=x^5+64$ . Then, the value of $\lim_{x \to 1}\frac{f(x)}{x-1}$ is $\boxed{A) \; -15}$ $B) \; -60$ $C) \; 60$ $D) \; 15$ I have solved it by considering a $5$ degree general polynomial of the form $ax^5+bx^4+cx^3+dx^2+ex+f$ and then algebraically solving to obtain the required result. Is there any other method which doesn't include assumption of a function of any kind? Source: JEE Mains 2022 25th June Shift-1","['limits', 'functions']"
4492477,Coin Flip Probability Independent or Not?,"I give you a hat which has $10$ coins inside of it. $1$ out of the $10$ have two heads on it, and the rest of them are fair. You draw a coin at random from the jar and flip it $5$ times. If you flip heads $5$ times in a row, what is the probability that you get heads on your next flip? I tried to approach this question by using Bayes: Let $R$ be the event that the coin with both heads is drawn and $F$ be the event that $5$ heads are flipped in a row. Then $$\begin{align*}
P(R|F) &=  \frac{P(F|R)P(R)}{P(F)} \\ &= \frac{1\cdot 1/10}{1\cdot 1/10 + 1/2^5\cdot 9/10} \\ &= 32/41
\end{align*}$$ Thus the probability that you get heads on the next flip is $$\begin{align*}
P(H|R)P(R) + P(H|R')P(R') &= 1\cdot 32/41 + 1/2\cdot (1 - 32/41) \\ &= 73/82
\end{align*}$$ However, according to my friend, this is a trick question because the flip after the first $5$ flips is independent of the first $5$ flips, and therefore the correct probability is $$1\cdot 1/10+1/2\cdot 9/10 = 11/20$$ Is this true or not?","['statistics', 'probability']"
4492480,"How do a create a population given the mean, size and standard deviation?","How do I create a population (set of positive integers) given the size, mean and standard deviation?
Like, if N = 7, mean is 50 and the standard deviation is 20, can I generate a population that fits that criteria?",['statistics']
4492508,"Let $\phi:G\to H$ be a group homomorphism. Show that $\phi[G]$ is abelian iff $\forall x,y\in G,xyx^{-1}y^{-1}\in\ker(\phi)$.","This exercise is out of A First Course in Abstract Algebra, 7th Edition by John Fraleigh, and it is Exercise 13.50. Is my solution correct? If not, is there any way I can improve? The question is: Let $\phi:G\to H$ be a group homomorphism. Show that $\phi[G]$ is abelian if and only if for all $x,y\in G$ , we have $xyx^{-1}y^{-1}\in\ker(\phi).$ Proof: Suppose $\phi[G]$ is abelian, and let $e'$ be the identity element of $H$ . Since $\phi$ is a homomorphism, we have for all $x,y\in G$ that $\begin{align} \phi(xyx^{-1}y^{-1})&=\phi(x)\phi(y)\phi(x^{-1})\phi(y^{-1})\\
&=\phi(x)\phi(y)(\phi(x))^{-1}(\phi(y))^{-1}\\
&=\phi(x)\phi(y)(\phi(y))^{-1}(\phi(x))^{-1}\\
&=\phi(x)e'(\phi(x))^{-1}\\
&=\phi(x)(\phi(x))^{-1}\\
&=e',
\end{align}$ so $xyx^{-1}y^{-1}\in\ker(\phi)$ . Conversely, suppose $xyx^{-1}y^{-1}\in\ker(\phi)$ for all $x,y\in G$ . Then $\phi(xyx^{-1}y^{-1})=e'$ for all $x,y\in G$ . Since $\phi$ is a homomorphism, we have $\begin{align}\phi(x)\phi(y)\phi(x^{-1})\phi(y^{-1})&=e'\\
\phi(x)\phi(y)(\phi(x))^{-1}(\phi(y))^{-1}&=e'\\
\phi(x)\phi(y)=\phi(y)\phi(x).
\end{align}$ Since $\phi(x),\phi(y)\in\phi[G]$ are arbitrary, $\phi[G]$ is abelian. Therefore, $\phi[G]$ is abelian if and only if for all $x,y\in G$ , $xyx^{-1}y^{-1}\in\ker(\phi)$ . $\blacksquare$","['group-homomorphism', 'group-theory', 'abstract-algebra', 'solution-verification']"
4492519,"Do We Actually Calculate ""Inverse Laplace Transforms""?","In all my classes (engineering) in which the Inverse Laplace Transform ( https://en.wikipedia.org/wiki/Inverse_Laplace_transform ) was required, we used to have a ""lookup table"" in which the Inverse Laplace Transform was provided for many common functions. When trying to find out the Inverse Laplace Transform for some function - we would try to ""strategically"" factor the function of interest, and use this ""lookup table"" to try and recognize these factors and ""piece together"" the Inverse Laplace Transform. This of course worked for many standard functions, but I always wondered how we might be able to calculate the Inverse Laplace Transform for ""non-standard"" functions for which this ""lookup table"" did not contain the Inverse Laplace Transforms. When reading the Wikipedia page about this topic ( https://en.wikipedia.org/wiki/Inverse_Laplace_transform ), I noticed that there seem to be some mathematical formulas (e.g. Mellin's Formula, Post's Inversion Formula) that seem to provide analytical solutions (I think) for the Inverse Laplace Transform of any function - ""standard"" or ""non-standard"". This brings me to my question: How was the ""lookup table"" I was using originally created? How do I calculate the Inverse Laplace Transforms WITHOUT my ""Lookup Table""? The Wikipedia article mentions that Post's Inversion is impractical in the cases of higher-order functions since derivatives are required (I am guessing that for multivariate functions, this will require a Jacobian Matrix). As for Mellin's Formula, this integral is integrated over "" $\gamma + \sqrt{-1} T$ ""  and "" $\gamma - \sqrt{-1} T$ "" : supposedly ""if singularities of the function (i.e. derivatives of the function) are in the left half-plane"" (I don't know how someone can determine if this condition is met - I am guessing that ""singularities of the function being in the left hand plane"" means that all derivatives of the function are ""complex""), ""gamma"" becomes 0 and Mellin's Formula becomes identical to the Inverse Fourier Transform. However, I am not sure how someone would evaluate the integral required for Mellin's Formula in practice (i.e. what do the integral ranges converge to). For instance, if I wanted to use Mellin's Formula to calculate the Inverse Laplace Transform of $\sqrt t$ - what should the values of ""gamma and T"" be? I might be able to somehow figure out how to calculate the integral of "" $e^{st}F(s)$ "" - but I am not sure how to correctly define the limits over which this integral is evaluated! Is Mellin's Formula practical to use for calculating Inverse Laplace Transforms - in other words, how was the ""lookup table"" I was using originally created? How do I calculate the Inverse Laplace Transforms without my ""Lookup Table""? Thank you! Note: It would be really interesting to see if someone here could try to calculate the Inverse Laplace Transform of some function using Mellin's Formula (or some other formula) and then see if their answer matches the true Inverse Laplace Transform! Note: I am guessing that in most cases, ""Gamma"" will usually end up being 0 and ""iT"" will usually end up being infinity - is this reasonable?","['integration', 'complex-analysis', 'laplace-transform', 'complex-numbers']"
4492551,What is the geometrical difference between $\frac{dr}{dt}$ and $dr$?,"I am currently studying multivariate calculus and came across a section in my book which perplexes me. It states that a field $F(x, y) = (P(x, y), Q(x, y))$ can have the parameter $r(t) = (x(t), y(t))$ . I follow thus far. The book then continues with explaining that the formal calculation with differentials is $$
\frac{dr}{dt}=(\frac{dx}{dt},\frac{dy}{dt}) \Rightarrow dr = (dx, dy)
$$ The book mentioned it simplified by multiplying with $dt$ on each side. I follow the math but struggle with understanding what this is actually saying geometrically. what does $dr$ and $\frac{dr}{dt}$ mean geometrically? I have never fully grasped the implication of multiplying and moving around the ""delta"" of equations and when it is nonsensical. My guess would be that $dr$ is a vector and $\frac{dr}{dt}$ is the rate of change the curve has at a given point. However, the book does not mention these things so I cannot verify this. For context: The parametrisation above, in the book, is used one section down to explain the formula for curve integrals.",['multivariable-calculus']
4492557,How to calculate $\lim_{x\to 0} \ (\sec x)^x$?,"How to calculate $\lim_{x\to 0} \ (\sec x)^x$ ? My attempt: $$\lim_{x\to 0} \ (\sec x)^x =\lim_{x\to 0}\ \left(\frac{1}{\cos x}\right)^x =\left(\frac{1}{\cos 0}\right)^0 =1.$$ We literally had to just input $x=0$ in the expression, and we got the value easily. Now see what my book did. My book's attempt: Let $y=(\sec x)^x$ . So, $\ln y=x\ln (\sec x)$ . Then $$\lim_{x\to 0} \ln y=\lim_{x\to 0} x\ln (\sec x)=0.$$ So, $\lim_{x\to 0} \ln y=0$ , or $\lim_{x\to 0} y=e^0=1$ . Why did my book overcomplicate this?","['limits', 'solution-verification']"
4492566,To which degree must I rotate a parabola for it to be no longer the graph of a function?,"To which degree must I rotate a parabola for it to be no longer the graph of a function? I have no problem with narrowing the question down by only concerning the standard parabola: $$f(x)=x^2.$$ I am looking for a specific angle measure. One such measure must exist as the reflection of $f$ over the line $y=x$ is certainly no longer well-defined. I realize that preferentially I should ask the question on this site with a bit of work put into it but, alas, I have no intuition for where to start. I suppose I know immediately that it must be less than $45^\circ$ as such a rotation will cross the y-axis at $(0,0)$ and $(0,\sqrt{2})$ . Any insight on how to proceed?","['analytic-geometry', 'algebra-precalculus', 'graphing-functions', 'geometry']"
4492569,Independence of Etale Fundamental Group on Base Point,"I'm reading Milne's notes on Etale Cohomology, where on page 27, there Remark 3.3 says that (a) If $\overline{\overline{x}}$ is a second geometric point of $X$ , then there is an isomorphism, $\pi_1(X,\overline{x})\to \pi_1(X,\overline{\overline{x}})$ , well-defined up to conjugation. While this result is not unexpected, I am not quite sure if it is immediate. I'm trying to show that the two versions of $F$ we get by considering these two different points are isomorphic, but I'm unable to do so. Moreover, what does conjugation really mean in this context? To conclude my questions are: Is this remark immediate? If so, what is it that I'm missing? What does conjugation mean in this context? I'm not sure if this question has already been asked on the site previously, and I apologise if it is indeed so. Thanks.","['etale-cohomology', 'fundamental-groups', 'algebraic-geometry']"
4492575,Infinite Sum of an Inverse Trig Expression,"I am attempting to find either a closed form for the following infinite sum, or failing that, the value $p$ for which the sum converges to $2\pi$ (somewhere around $0.82$ ?). $$\sum_{i=1}^\infty \arccos \left( \frac{1 + p^i+p^{i+1}-p^{2i+1}}{1 + p^i+p^{i+1}+p^{2i+1}} \right) $$ Thank you ahead of time for your help. Later edit: I put together a post with further context and thoughts about the problem I was trying to solve for those interested. The ""object"" I was asking about looks like this: Please let me know if this is already well-trod so I can read more + reference those who have already done this work.","['trigonometric-series', 'trigonometry', 'convergence-divergence', 'summation']"
4492588,"Let $p$ be a prime number and $S\subseteq\{1,2,\cdots,p-1\}$ be a subset such that $|S|>p^{\frac{3}{4}}$...","Let $p$ be a prime number and $S\subseteq\{1,2,\cdots,p-1\}$ be a subset such that $|S|>p^{\frac{3}{4}}$ . Prove that for every
positive integer $m$ , there exist $a_{1},a_{2},b_{1},b_{2},c_{1},c_{2} \in S$ such that $$m\equiv
a_{1}a_{2}+ b_{1}b_{2}+c_{1}c_{2} \pmod p$$ Suppose there exists $m$ such that this is not true. I thought about the polynomial $$P(x,y) = \prod _{i=1}^{p-1} (x\cdot y -m-i)$$ where $x,y \in \mathbb{Z}_{p-1}^3$ . Now this polynomial which has degree $2p-2$ vanishes on $S^6$ . Now I tried to use Combinatorial Nullstellensatz but seem it is not applicable, or I didn't make the proper polynomial.","['number-theory', 'combinatorial-number-theory', 'combinatorics', 'algebraic-combinatorics']"
4492593,show that the ODE $(\sqrt x + \sqrt y)\sqrt y dx = xdy$ has no solution $y(x)$ such that $\lim_{x\to \infty}\frac{y(x)}{x}=L$ when $ L>0 \in \Bbb R$,I need to show that the ODE $(\sqrt x + \sqrt y)\sqrt y dx = xdy$ has no solution $y(x)$ such that $\lim_{x\to \infty}\frac{y(x)}{x}=L$ when $ L>0 \in \Bbb R$ I solved it by finding the general solution $y(x) = \frac{1}{4} (4 c_1^2 x + 4 c_1 x \log(x) + x \log^2(x))$ and then its easy to see that $\lim_{x\to \infty}\frac{y(x)}{x}=\infty$ and we finished. I'm asking if there is a more elegant proof to this problem instead of the straight forward solution,"['calculus', 'ordinary-differential-equations']"
4492685,How does this step rewriting the Lie derivative of a vector field work?,"I am reading some differential geometry notes and I stuck with understandig what seems to be a simple part of a proof but for me comes out of nowhere. Let $M$ be a smooth manifold and let $X,Y$ be two smooth vector fields on the manifold.
Then one can define the Lie derivative of $Y$ in direction of $X$ as $$ L_X Y = \frac{d}{dt} ((\Psi_{-t})_* Y) \bigg \vert_0.$$ Here, $\Psi_{-t}$ denotes the local flow of the vector field $X$ , that is in some neighborhood of some point one has $$ \frac{d}{dt} \Psi_t(q) = X_{\Psi_t(q)}.$$ $(\Psi_{-t})_*$ denotes the push-forward of $\Psi_{-t}$ and we have $$(\Psi_{-t})_* Y)_{\Psi_{-t}p} = {d\Psi_{-t}}_p Y_p$$ and thus $$(\Psi_{-t})_* Y)_{p} = {d\Psi_{-t}}_{\Psi_{t}(p)} Y_{\Psi_{t}(p)}.$$ Now in the proof that the definition above for the Lie derivative is the same as using $[X,Y]$ where this denotes the Lie bracket, the first step is as follows:
For a function $f \in C^{\infty}(M)$ , we have $$ {L_X Y}_p \cdot f = \frac{d}{dt}(Y_{\Psi_{t}(p)} \cdot (f \circ \Psi_{-t}) ) \bigg \vert_{t = 0}.$$ This is already the step that I do not understand: From above we have $$ {L_X Y}_p \cdot f = \frac{d}{dt}{d\Psi_{-t}}_{\Psi_{t}(p)} Y_{\Psi_{t}(p)}\bigg \vert_{t = 0} \cdot f.$$ How can we now get the function $f$ ""inside"" of the time derivative, and where does the change of the composition order come frome? If the element in brackets was some $t \to M$ then maybe it would make more sense to me, since the elements of the tangent space $T_pM$ can be represented by such curves (or rather there derivatives at point $p$ ), but here we have a curve in $T_pM$ . Could anyone please explain this step a bit to me?","['vector-fields', 'lie-derivative', 'differential-geometry']"
4492714,Prove a matrix is positive semi-definite,"Suppose we have a finite set $\Omega$ and a collection of subsets of $\Omega$ , denoted by $\{A_1,...,A_n\}$ . Define $k(X,Y)=2^{|X\cap Y|}, \forall X,Y\subset \Omega$ . Define a $n\times n$ matrix $G$ by letting $G_{i,j}=k(A_i,A_j)$ , i.e. $2^{|A_i\cap A_j|}.$ I need to show that $G$ is positive semi-definite. To this end, I fix an $x\in \mathbb R^n$ and try to show that $x^T Gx \ge 0$ . When $n=2$ , we can show that $x^T Gx$ is greater than a complete square which is nonnegative. But when $n\ge 3$ , it seems like we can no longer use trivial inequalities to get a lower bound which is complete square. I get stuck here. Thanks for any help.","['matrices', 'linear-algebra', 'positive-semidefinite']"
4492737,Finding the range of $\frac {2x^2+x-3}{x^2+4x-5}$,"I solved for the range as follows: Setting $f(x)=\frac {2x^2+x-3}{x^2+4x-5} =y$ , I rearranged it to get a quadratic in x. $$(y-2)x^{2}+ (4y-1)x +(3-5y)=0$$ Next, using $\Delta \ge 0$ , I got $$(4y-1)^2-4(y-2)(3-5y) \ge 0$$ Which boiled down to $$(6y-5)^2 \ge 0$$ And this gave me $$ y \in R$$ Next, the value of $x$ for which $y=2$ is $x=1$ , for which the function isn't defined, so that gives me $$y \in R - \{2\}$$ However, the solution is $$y \in R- \{\frac{5}{6}, 2\}$$ I understand that the original function is identical to $$g(x) =\frac{2x+3}{x+5} 
  \forall x \in R - \{1\}$$ and that $\frac{5}{6} =g(1)$ , but in the original function $f(x)$ I found $y=2$ corresponded to $x=1$ and therefore excluded it, but if $x= 1$ also corresponds to $y=\frac{5}{6}$ then wouldn't this $not$ be a function, as $x=1$ would then be associated with two different $y$ values? I've read the answers in Why D≥0 while finding the range of rational functions and Finding the range of $y =\frac{x^2+2x+4}{2x^2+4x+9}$ (and $y=\frac{\text{quadratic}}{\text{quadratic}}$ in general) but I'm still unsure of how to apply the information from those to figure out what values of $y$ need to be excluded when dealing with such questions. While I am familiar with derivatives and limits to a certain degree, we were assumed to $\underline {not}$ know calculus when we were taught this and solved such problems. I apologise if there are any issues with formatting, this is my first time using Latex.","['quadratics', 'derivatives', 'rational-functions']"
4492753,Function and graph,"A relation is a subset of a carthesian product. A function from $X$ to $Y$ is a relation with particular properties. It follows that a function is a subset of the cartesian product $X\times Y$ . The graph of f is $$ 
\{(x,y) \in  X\times Y\ |\  y=f(x)\}.
$$ Then what is the difference between the graph and the function if both are the same subset of $X\times Y$ ?","['elementary-set-theory', 'functions', 'relations']"
4492789,The interior of the set of continuously differentiable nonnegative functions,"Let $\Omega \subset \mathbb{R}^N$ be a bounded open set with smooth boundary. Consider the space $$C^1_0(\overline{\Omega}) := \{u \in C^{1}(\overline{\Omega}) : u = 0\text{ in }\partial \Omega\}$$ with the norm $$
\|u\| = \|u\|_{\infty} + \|\,|\nabla u|\,\|_{\infty}
,\quad \forall u \in C^1_0(\overline{\Omega}).$$ Also consider $P = \{u \in C^1_0(\overline{\Omega}) : u \geq 0\}$ . How can I show that $int(P) = A$ , where $$
A = \{ u \in C^1_0(\overline{\Omega}) : u(x) > 0, \forall x \in \Omega \text{ and }\langle \nabla u(x), \eta(x)\rangle < 0, \forall x \in \partial \Omega\} ?
$$ Given a point $u \in A$ , I know that it's graphic has a specific behavior in the boundary, because the cosine of the angle between the gradient and the normal vectors in the boundary is negative. I imagine that a ball around it should be a ""curved cover"" involving its graphic. However, I didn't succeed in showing that there's a very small $r > 0$ such that the functions on the open ball $B_{r}(u)$ should be nonnegative. I would appreciate any help.","['analysis', 'real-analysis', 'complex-analysis', 'partial-differential-equations', 'general-topology']"
4492815,"For a triangle, prove that $\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}=\frac{1}{r^2} $","Context : I made this question and am contemplating about  submitting it as a contest question. The contest is not a large one; its scope does not comprise even our whole class. Its just a friendly, small contest within a small group. This originally occurred to me when we were asked to prove $\tan \frac A2\tan\frac B2 +\tan \frac B2\tan\frac C2+\tan\frac C2\tan\frac A2=1$ in class. I tried a geometrical proof but failed, but I got the following for my efforts. Derivation : Given $\triangle ABC$ and its incircle $\Im$ with inradius $r$ , we have the known (and easily provable) formula $$\tan \frac A2\tan\frac B2 +\tan \frac B2\tan\frac C2+\tan\frac C2\tan\frac A2=1.\tag{1}\label{1}$$ We know, $IE=IF=IG=r$ . Now, $$\tan \frac A2=\frac{r}{AE}=\frac{r}{AF}$$ $$\tan \frac B2=\frac{r}{BE}=\frac{r}{BG}$$ $$\tan \frac C2=\frac{r}{CG}=\frac{r}{CF}$$ Putting appropriate values in the formula $(1)$ , we get $$\frac{r^2}{AE\cdot BE}+ \frac{r^2}{CG\cdot BG}+ \frac{r^2}{AF\cdot CF}=1$$ or $$\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}=\frac{1}{r^2}$$ Finally, using the identities $\Delta=rs, s=\dfrac{a+b+c}{2}, \Delta=\dfrac{abc}{4R}$ (where R is the circumradius) and the sine law we get $$\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}=\dfrac{s^2}{\Delta^2}=\dfrac{(a+b+c)^2}{4\Delta^2}=\dfrac{(a+b+c)^2\cdot 16R^2}{4a^2b^2c^2} $$ Thus, $$\sqrt{\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}}=2R\frac{a+b+c}{abc}$$ $$=2R\frac{2R(\sin A+\sin B+\sin C)}{8R^3\sin A\sin B\sin C}$$ $$=\frac{1}{2R}\left(\frac{1}{\sin B\sin C}+ \frac{1}{\sin A\sin C}+ \frac{1}{\sin B\sin A}\right)$$ so that $$2R\sqrt{\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}}=\left(\frac{1}{\sin B\sin C}+ \frac{1}{\sin A\sin C}+ \frac{1}{\sin B\sin A}\right)$$ Question: Prove that for a triangle ABC, $$2R\sqrt{\frac{1}{AE\cdot BE}+ \frac{1}{CG\cdot BG}+ \frac{1}{AF\cdot CF}}=\left(\frac{1}{\sin B\sin C}+ \frac{1}{\sin A\sin C}+ \frac{1}{\sin B\sin A}\right)$$ The result looks pretty symmetrical and nice, and also reminds one of Ceva’s theorem because of the segments taken consecutively. If you were given this question and did not know the above derivation, how would you approach it? Basically, I am looking for alternative solutions to this problem just for reference. If the question has any flaws, please tell me. Also, I would really like to know how you would rate this question for a high school level informal contest. If any of you can come up with variants of this, especially as inequalities, I would be grateful. Thanks in advance.","['contest-math', 'trigonometry', 'triangles', 'geometry']"
4492881,Find the number of solutions to the equation $\sin\left(\frac{\pi\sqrt x}4\right)+\cos\left(\frac{\pi\sqrt {2-x}}4\right)=\sqrt 2$,"The following question appeared in a JEE Mock exam, held two days ago. Question: Find the number of solutions to the equation $\sin\left(\frac{\pi\sqrt x}4\right)+\cos\left(\frac{\pi\sqrt {2-x}}4\right)=\sqrt 2$ My Attempt: I think if each term is $\frac1{\sqrt 2}$ then it would add upto $\sqrt 2$ . So, $x=1$ is a solution. How can we tell about other solutions, if any?","['contest-math', 'trigonometry', 'functions', 'roots']"
4492934,Venn diagram for basic types of operators,"This Venn diagram is an attempt to visually classify densely-defined linear operators between Banach spaces (self-adjoint operators are an exception, defined between Hilbert spaces). Operators are first divided between the bounded (B) in the left box and the unbounded (uB) in the right box. Red curve contains closable operators, green ellipse closed operators, grey circle compact operators and blue ellipse self-adjoint operators. Preliminary remarks: bounded operators are closable (Bounded Linear Transformation theorem) compact operators are bounded ( Show that a compact operator is bounded ) self-adjoint operators are closed ( Why is every selfadjoint operator closed? ) Comments for specific subdomains: cf. https://mathoverflow.net/q/47418 e.g. derivative d/dx defined on $C^\infty([a,b]) \subseteq C^0([a,b])$ e.g. https://math.stackexchange.com/a/4170649/142518 e.g. Showing derivative operator is self-adjoint e.g. Prove that $T$ is not compact e.g. Examples of self adjoint compact operators on Hilbert spaces e.g. multiplication with $i$ on finite dimensional complex normed space? e.g. any linear bounded functional with proper dense domain? e.g. multiplication with $i$ on any infinite dimensional complex Banach space? e.g. identity restricted to proper, dense domain of an infinite dimensional Banach space? Is this classification correct? Are there any better, more instructive examples?","['big-list', 'functional-analysis', 'unbounded-operators', 'compact-operators']"
4492936,Finding a bound for $\sum_{\text{cyc}}\frac{\sin B+\sin C}A$ if $\triangle ABC$ is not obtuse.,The following question appeared in a JEE mock exam held two days ago. Question: $\triangle ABC$ is not obtuse then value of $\displaystyle\sum_{\text{cyc}}\frac{\sin B+\sin C}A$ must be greater than A) $\frac6\pi$ B) $3$ C) $\frac{12}\pi$ D) $\frac1\pi$ My Attempt: I first tried with sine rule $$\frac a{\sin A}=\frac b{\sin B}=\frac c{\sin C}$$ But couldn't do anything with it. Then I used $\sin B+\sin C=2\sin\left(\frac{B+C}2\right)\cos\left(\frac{B-C}2\right)=2\cos\frac A2\cos\left(\frac{B-C}2\right)$ But couldn't finish this approach either. Then I tried using Jensen inequality but in vain. Then I thought I would assume a function and find its minimum value. But couldn't decide what function to take.,"['contest-math', 'inequality', 'calculus', 'triangles', 'trigonometry']"
4492959,Integrate $\ln^3(\sin(x))$ using Fourier series,"Show that $$\int_0^{\frac\pi2} \ln^3(\sin(x)) \, dx = -\frac\pi2 \ln^3(2) - \frac{\pi^3}8 \ln(2) - \frac{3\pi}4 \zeta(3)$$ I have seen a method for this elsewhere , but I would specifically like to reproduce this result in the same way I've computed the similar integrals of $\ln(\cos(x))\ln(\sin(x))$ and $\ln^2(\sin(x))$ , as shown here and here - which involves a ""complicated series"", as Jack puts it. In particular, I want to use the Fourier series $$f(x) = \ln(\sin(x)) = -\ln(2) - \sum_{k=1}^\infty \frac{\cos(2kx)}k$$ Expanding the integrand yields $$\begin{align*}
-f(x)^3 &= \ln^3(2) + 3 \ln^2(2) \sum_{a=1}^\infty \frac{\cos(2ax)}a \\ &\quad + 3\ln(2) \left(\sum_{a=1}^\infty \frac{\cos^2(2ax)}{a^2} + 2 \sum_{a\neq b} \frac{\cos(2ax) \cos(2bx)}{ab}\right) \\
& \quad + \left(\sum_{a=1}^\infty \frac{\cos^3(2ax)}{a^3} + 3 \sum_{a\neq b} \frac{\cos^2(2ax) \cos(2bx)}{a^2b} + 6 \sum_{a\neq b\neq c} \frac{\cos(2ax) \cos(2bx) \cos(2cx)}{abc}\right)
\end{align*}$$ In the integral, the series with $\cos(2ax)$ vanishes; by orthogonality, $\cos(2ax)\cos(2bx)$ vanishes; in combination of both of these facts, $\cos^2(2ax)\cos(2bx)$ and $\cos^3(2ax)$ also vanish. So the integral reduces to $$\int_0^{\frac\pi2} f(x)^3 \, dx = -\frac\pi2 \ln^3(2) - \frac{\pi^3}8 \ln(2) - 6 \sum_{a\neq b\neq c} \frac1{abc} \int_0^{\frac\pi2} \cos(2ax) \cos(2bx) \cos(2cx) \, dx$$ In the remaining integral, I'm fairly sure that most of the terms integrate to $0$ using the orthogonality argument, except in the case of $a+b=c$ , $$\int_0^{\frac\pi2} \cos(2ax) \cos(2bx) \cos(2(a+b)x) \, dx \\ = \int_0^{\frac\pi2} \frac{\cos(2(a-b)x)\cos(2(a+b)x) + \cos^2(2(a+b)x)}2 \, dx = \frac\pi8$$ ETA: If there are no other triples, then I should end up with $$-6 \sum_{a\neq b\neq c} \frac1{abc} \int_0^{\frac\pi2} \cos(2ax) \cos(2bx) \cos(2cx) \, dx = -\frac{3\pi}4 \sum_{a+b=c} \frac1{abc} = -\frac{3\pi}4 \zeta(3) \\ \implies \sum_{a\neq b} \frac1{ab(a+b)} = \zeta(3)$$ Now, $$\begin{align*}
\sum_{a\neq b} \frac1{ab(a+b)} &= \sum_{(a,b)\in\Bbb N^2} \frac1{ab(a+b)} - \frac12 \sum_{a=1}^\infty \frac1{a^3} \\[1ex]
&= 2 \sum_{a<b} \frac1{ab(a+b)} - \frac12 \zeta(3) \\[2ex]
\sum_{a<b} \frac1{ab(a+b)} &= \sum_{b=2}^\infty \frac1{b(b+1)} + \sum_{b=3}^\infty \frac1{2b(b+2)} + \sum_{b=4}^\infty \frac1{3b(b+3)} + \cdots \\[1ex]
&= \sum_{b=2}^\infty \left(\frac1b - \frac1{b+1}\right) + \frac14 \sum_{b=3}^\infty \left(\frac1b - \frac1{b+2}\right) + \frac19 \sum_{b=4}^\infty \left(\frac1b - \frac1{b+3}\right) + \cdots \\[1ex]
&= (H_2 - H_1) + \frac{H_4 - H_2}4 + \frac{H_6 - H_3}9 + \cdots \\[1ex]
&= \sum_{n=1}^\infty \frac{H_{2n} - H_n}{n^2} \\[1ex]
&= \sum_{n=1}^\infty \frac{H_{2n}}{n^2} - 2\zeta(3)
\end{align*}$$ where the last equality is due to (31) , and it remains to show $$\sum_{n=1}^\infty \frac{H_{2n}}{n^2} = \frac{11}4 \zeta(3)$$ Rewriting the sum as follows leads me to think there may be a hidden Cauchy product, but I have not been able to find a decomposition. $$H_{2n}-H_n = \sum_{k=1}^{2n} \frac1k - \sum_{k=1}^n \frac1k = \sum_{k=n+1}^{2n} \frac1k = \sum_{k=1}^n \frac1{n+k} \\ \implies\sum_{n=1}^\infty \frac{H_{2n}-H_n}{n^2} = \sum_{n=1}^\infty \sum_{m=1}^n \frac1{n^2(n+m)}$$","['definite-integrals', 'harmonic-numbers', 'trigonometric-integrals', 'sequences-and-series', 'riemann-zeta']"
4492960,Is the set of subrings of $\mathbb Z[X]$ countable?,"Initially, I was trying to look at the subrings of $\mathbb{Z}[X]$ . Since I have failed hard, I have tried to at least count them. So I have tried to build an injection from $\{0,1\}^\mathbb{N}$ to the set of subrings of $\mathbb{Z}[X]$ . Since $\{0,1\}^\mathbb{N}$ is uncountable, we would have the set of subrings of $\mathbb{Z}[X]$ uncountable too. By associating the sequence $(e_n)_{n\in\mathbb{N}}$ to the ring $\mathbb{Z}[1,p_1 e_1 X²,p_2 e_2 X^3,...]$ where $p_k$ is the $k$ -th prime number and $e_k\in\{0,1\}$ , it fails; for instance $\mathbb{Z}[1,2X²,3X^3,7X^5]$ is equal to $\mathbb{Z}[1,2X²,3X^3,7X^5,23X^{10}]$ because $X^{10}\in \mathbb{Z}[1,2X²,3X^3,7X^5]$ since $X^{10}=(7X^5 - (2X² * 3X^3))^2$ . I am unable to repair it, maybe it is flawed from the start. Actually, I do not even know if the set of subrings of $\mathbb{Z}[X]$ is uncountable. If possible I would like a proof or a reference to a proof see since I am curious about its countability.","['integers', 'ring-theory', 'polynomial-rings', 'abstract-algebra', 'combinatorics']"
4493005,Can we leverage a priori information about monotonicity of parameters to improve estimation?,"Suppose $(X_t)_{t \in\mathbb{N}}$ and $(Y_t)_{t \in \mathbb{N}}$ are two independent sequences of i.i.d. Bernoulli random variables, the first one of parameter $x \in [0,1]$ and the second one of parameter $y \in [x,1]$ (so, in particular $\forall t \in \mathbb{N}, \mathbb{P}[X_t = 1] =x = (1-\mathbb{P}[X_t = 0])$ and $\forall t \in \mathbb{N}, \mathbb{P}[Y_t = 1] =y = (1-\mathbb{P}[Y_t = 0])$ ). Our goal is to estimate $y-x$ using $X_1,\dots,X_{T_1},Y_1,\dots,Y_{T_2}$ , where $T_1,T_2 \in \mathbb{N}$ . My first guess was to use the estimate $$\frac{1}{T_2} \sum_{t=1}^{T_2} Y_t - \frac{1}{T_1} \sum_{t=1}^{T_1} X_t$$ because, for any $\varepsilon > 0$ , we can rely on Hoeffding's inequality \begin{equation*}
\mathbb{P}\Bigg[\bigg| \Big(\frac{1}{T_2} \sum_{t=1}^{T_2} Y_t - \frac{1}{T_1} \sum_{t=1}^{T_1} X_t\Big) -(y-x)\bigg| \ge \varepsilon\Bigg] \le 2 \exp\Bigg(-\frac{2\varepsilon^2}{\frac{1}{T_1}+\frac{1}{T_2}}\Bigg) \;,   
\end{equation*} which, for any $\delta \in (0,1)$ , can be re-read as \begin{equation*}
\mathbb{P}\Bigg[\bigg| \Big(\frac{1}{T_2} \sum_{t=1}^{T_2} Y_t - \frac{1}{T_1} \sum_{t=1}^{T_1} X_t\Big) -(y-x)\bigg| \ge \sqrt{\frac{1}{2}\Big( \frac{1}{T_1}+\frac{1}{T_2}\Big)\log\Big(\frac{2}{\delta}\Big)}\Bigg] \le \delta\;.
\end{equation*} However, due to noise, it could very well happen that $\frac{1}{T_2} \sum_{t=1}^{T_2} Y_t - \frac{1}{T_1} \sum_{t=1}^{T_1} X_t <0$ , meaning that our estimate is completely meaningless given our prior information that $y\ge x$ . I thought, since $\big(\frac{1}{T_1} \sum_{t=1}^{T_1} X_t,\frac{1}{T_2}\sum_{t=1}^{T_2} Y_t\big)$ is the solution to the unconstrained minimization problem relative to the function $$ (x',y') \mapsto \bigg(\sum_{t=1}^{T_1} (X_t-x')^2+ \sum_{t=1}^{T_2}(Y_t-y')^2\bigg) \;,$$ that a better idea could be to return the estimate $\hat{y}_{T_1,T_2}-\hat{x}_{T_1,T_2}$ , where $(\hat{x}_{T_1,T_2},\hat{y}_{T_1,T_2})$ is the solution to the corresponding constrained minimization problem, i.e., $$ (\hat{x}_{T_1,T_2},\hat{y}_{T_1,T_2}) \in \mathrm{argmin}_{(x',y') \in [0,1]^2 \\ \textrm{ s.t. }x' \le y'} \Big(\sum_{t=1}^{T_1} (X_t-x')^2+ \sum_{t=1}^{T_2}(Y_t-y')^2\Big) \;. $$ Doing this, by construction, we are guaranteed to have $\hat{y}_{T_1,T_2} - \hat{x}_{T_1,T_2} \ge 0$ . I'm wondering if an analogous to the guarantees we have derived from Hoeffding's inequality for the estimator $\frac{1}{T_2} \sum_{t=1}^{T_2} Y_t - \frac{1}{T_1} \sum_{t=1}^{T_1} X_t$ holds also for the estimator $\hat{y}_{T_1,T_2}-\hat{x}_{T_1,T_2}$ , maybe relying on other probabilistic arguments. Here the question: Do there exist constants $c_1,c_2>0$ such that for every $x,y \in [0,1]$ with $x\le y$ , for any $T_1,T_2 \in \mathbb{N}$ and for any $\delta \in (0,1)$ we have that $$\mathbb{P}\Bigg[\bigg|(\hat{y}_{T_1,T_2}-\hat{x}_{T_1,T_2}) -(y-x)\bigg| \ge \sqrt{c_1\Big( \frac{1}{T_1}+\frac{1}{T_2}\Big)\log\Big(\frac{c_2}{\delta}\Big)}\Bigg] \le \delta\;.$$ In this case, might we also guarantee that $c_1 \le \frac{1}{2}$ and $c_2 \le 2$ ?","['statistics', 'concentration-of-measure', 'parameter-estimation', 'probability-theory']"
4493068,Characterization of Riesz sequences,"Let $X \subseteq \mathbb{R}$ be a compact set.
Show that $\mathcal{E} = \{e^{int}\}_{n \in \mathbb{Z}}$ is a Riesz sequence in $L^2(X)$ if and only if $X+2\pi \mathbb{Z} = \mathbb{R}$ . We say that $\mathcal{E}$ is a Riesz sequence in $L^2(X)$ if we have the following chain of inequalities: $$A \sum_n |c_n|^2 \leq \left\|\sum_n c_n e^{int}\right\|_{L^2(X)}^2 \leq B \sum|c_n|^2,$$ where $c = (c_n)$ is a finite sequence. Suppose that $X+2\pi\mathbb{Z} = \mathbb{R}$ . Then since $X$ is bounded there is $N \in \mathbb{N}$ such that $X \subseteq [-\pi N, \pi N]$ . With this we get the the latter inequality by using the fact that $\mathcal{E}$ forms an orthogonal basis in $L^2[-\pi,\pi]$ and the periodicity of its elements. However I am unsure how to achieve the fitst inequality. One idea I had (which may be wrong) is to cover the interval $[-\pi,\pi]$ by a finite number of translates of the form $\{X+2\pi k\}_{k=1}^N$ . However, I cannot convince myself that I can do this only using a finite number of translates. If this is true, then this direction is done. Otherwise, I am unsure. As for the other direction, I have no clue on how to start. Of course one only needs to show that every real number $r \in \mathbb{R}$ can be written in the form $r = x+2\pi m$ for some $m \in \mathbb{Z}$ . Any help is appreciated, thanks!","['hilbert-spaces', 'harmonic-analysis', 'real-analysis']"
4493097,Function satisfying $F(a)-F(1)=F(1)-F(\frac{1}{a})$,"What continuous monotone increasing function has the property: $$F(a)-F(1)=F(1)-F(\frac{1}{a}) \space\space\space\space\space\space \forall a>1 $$ The context for this is that I'm looking for a continuous probability distribution on $(0,\infty)$ satisfying: $$P\left(1\le x\le a\right) = P\left(\frac{1}{a} \le x \le 1\right)$$","['statistical-inference', 'statistics', 'probability-distributions', 'monotone-functions', 'functions']"
4493131,"What is a ""physical measure""?","I see this term coming up quite often, especially in the context of mathematical physics, but I cannot find it defined anywhere. What constitutes a physical measure and what are some of its properties? EDIT: I saw this term on a few posts on the web, as well as a few papers I was reading. I will try to find these to provide some context, but in the meantime here is a SE post and a paper from arXiv that use the term: What is the probability density function on solutions to the Lorenz system? https://arxiv.org/pdf/1012.0513.pdf (this paper does define the term, but I was wondering if this is the general definition)","['measure-theory', 'ergodic-theory', 'real-analysis', 'mathematical-physics', 'dynamical-systems']"
4493135,"Combinatorics exercise about sets of forks, knives, spoons.","Problem statement: At various yard sales, a woman has acquired five forks, of which no two are alike. The
same applies to her four knives and seven spoons. In how many different ways can
three place settings be chosen if each place setting consists of exactly one fork, one
knife, and one spoon? Assume that the arrangement of the place settings on the table is
unimportant. So far I figured the following thing out: Since we have to serve 3 sets, having the sets already, we would have $3!=6$ different ways of arranging the already created sets.
What would be the next thing to calculate to count how many times ONE set could be arranged?
The part I am struggling with is that each fork (and knive, spoon respectively) is distinct from one another, so I can't just use this theorem: It would be nice if somebody would only give me a hint so I can still figure it myself (Dont just handwaive). Thank you!",['combinatorics']
4493160,Superior limit of sum of iterates of $\phi (n)$,"For reasons of personal curiosity, I was playing with the sum of iterates of $\phi (n)$ , which are listed in OEIS A053478 ; that is $a(n)=n+\phi (n)+\phi(\phi (n))\dots \ $ The sum terminates when the final term becomes $1$ . Since the sum contains $n$ explicitly, $\frac{a(n)}{n} \ge 1$ , and in fact is $1$ only at $n=1$ However, by direct observation of the first several hundred terms, I find that $\frac{a(n)}{n}<3$ . The ratio closely approaches $3$ only for some prime numbers. I know that when $n$ is prime, $a(n)>n+(n-1)\approx 2n$ . However, despite much cogitation, I cannot reach a concise understanding of why the superior limit of $a(n)$ appears to be $3n$ , and so I cannot establish that $3n$ is in fact the limit. The possibility remains open that there may exist some very large $n$ for which $\frac{a(n)}{n}>3$ . My question is: Can it be proved that the superior limit of $\frac{a(n)}{n}$ is in fact $3$ ?","['number-theory', 'totient-function']"
4493171,Find $f:\mathbb{R} \to \mathbb{R}$ which satisfies $f(x)^2+2f(x)f(y)+y^2=\big(x-f(y)\big)^2+4f(x)y$.,"Find $f:\mathbb{R} \to \mathbb{R}$ which satisfies $f(x)^2+2f(x)f(y)+y^2=\big(x-f(y)\big)^2+4f(x)y$ . Let $P(x, y): f(x)^2+2f(x)f(y)+y^2=\big(x-f(y)\big)^2+4f(x)y.$ $P(0, 0): f(0)^2-2f(0)f(0)=\big(0-f(0)\big)^2.$ $\therefore -f(0)^2=f(0)^2, f(0)=0.$ $P(x, 0): f(x)^2=x^2.$ $\therefore x^2+2f(x)f(y)+y^2=x^2-2xf(y)+y^2+4f(x)y.$ $\Rightarrow 2f(y)(f(x)+x)=4f(x)y, \ f(y)(f(x)+x)=2f(x)y.$ How can I disprove that $f(x)= \begin{cases} x & \text{if ****} \\ -x & \text{if ****}\end{cases}$ ?","['functional-equations', 'functions']"
4493194,"Let $X\sim B(n,p)$. How to calculate the expected value $E[\bar{X}]$,where $\bar{X}=|X-E[X]|$?","Let $X\sim B(n,p)$ . How to calculate the the expected value $E[\bar{X}]$ ,where $\bar{X}=|X-E[X]|$ ? Recently I want to prove that \begin{equation}
\begin{aligned}
f(n)&=\frac1nE[\bar{X}] = \frac1nE[|X-np|]=\frac1n\sum_{i=0}^{n}[C_n^i p^i (1-p)^{n-i}|i - np|]
\end{aligned}
\end{equation} is decreasing over $\mathbb{Z}^+$ , so need to know how to calculate the expected value $E[\bar{X}]$ . Of course, I would also really appreciate it if you know how to prove it. Edited on July 16, 2022 The previous $f(n)$ missed a coefficient of $\frac1n$ . After correction, I can already prove that $f(n)$ is decreasing over $\mathbb{Z}^+$ . The left side of the following figure is the image of $E[\bar{X}]$ , and the right side is the image of the current $f(n)$ .","['binomial-distribution', 'probability-distributions', 'probability-theory', 'probability']"
4493208,"If a particle travels $30$ meters every $3$ seconds, does it necessarily travel $20$ meters every $2$ seconds? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Is it possible if its motion were discontinuous? I'm trying to understand if there is a function that has this property, but I chose to say it in terms of motion because it's easier to explain.",['calculus']
4493239,Stuck on problems about differential / derivatives,"For the real number $t$ that $t \ge 6 $ , let $f(x) = \frac{1}{t} \left( \frac{1}{8}x^3 + \frac{t^2}{8}x + 2 \right)$ . Let the sum of all real numbers $k$ satisfying the following condition be $g(t)$ :
function $\{f(x)-x\}^2$ has an extreme value at $x=k$ . For the real number $p$ where $g(p) = -1$ , $$\int_{6}^{p}g'(t)(8t-t^2)dt = ?$$ My attempt: Since the function $h(x) = \{f(x)-x\}^2$ has an extreme value at $x=k$ , by differentiation, $h'(k) = 2\{ f(k)-k \}\left( f'(k)-1 \right) =0$ . So, $f(k) = k$ or $f'(k) = 1$ . $h''(k) \ne 0$ because it cannot be an inflection point. Applying it, if $f(k) = k$ , $f'(k) \ne 1$ . if $f'(k) = 1$ , $f(k) \ne k$ . And I'm stuck. It seems like I have to use the equation of $f(x)$ , but it gives me tons of equations with messy variables. A little hint would be really helpful. (+ Additional approach) : Applying step 4. to $f(x)$ , we can get $t(8-t) = k^2 + \frac{16}{k}$ ( $k\ne 2$ when $t=6$ ). By multiplying $k$ to each side, $k^3 - t(8-t)k + 16 = 0$ . This is a cubic equation with respect to $k$ , so I thought I could use $\alpha + \beta + \gamma = -\frac{b}{a}$ to get $g(t)$ . But as you can see here, it becomes $0$ since it also counts imaginary roots.","['cubics', 'derivatives', 'polynomials']"
4493310,Question about the period of specific Hamiltonian flows,"Consider the Hamiltonian of the form $H(q,p)=p^2/2 + |q|^{\beta}/\beta$ for $\beta\in (1,2)$ . In the case of $\beta=2$ , this is simply the harmonic system and we know that all contours, i.e. those of the form $\{(q,p):H(q,p)=E\}$ for fixed $E>0$ are circles and has a circumference of radius $\sqrt{q^2+p^2}=\sqrt{2H(q,p)}$ and the speed of the Hamiltonian flow is $\sqrt{J\nabla H(q,p)}=\sqrt{q^2+p^2}$ for $J=\begin{pmatrix}0 && 1 \\ -1 && 0 \end{pmatrix}$ . So the relation time=distance/speed tells us that the period length is $2\pi$ independent of the starting position. My question is what happens to the period length of the contours as $|q|$ gets large, or the energy level increases? The speed will be $\sqrt{p^2 + |q|^{2\beta - 2}}$ where $2\beta-2 \in (0,2)$ . So the distance between $q$ for each level contour will increase as $|q|$ increases. I think the period length increases as a result of this but I am not sure about this. I would greatly appreciate any insight into this.","['hamilton-equations', 'geometry', 'differential-geometry']"
4493339,If $\mu$ is atomless then its c.d.f. is continuous,"I'm trying to prove this property. Could you have a check on my attempt? Let $\mu$ be a Borel probability measure on $\mathbb R$ and $F$ its c.d.f. Then $F$ is right-continuous and non-decreasing. Theorem: If $\mu$ is atomless then $F$ is continuous. My attempt: Assume the contrary that $F$ is not continuous at $a \in \mathbb R$ . Then $F$ is not left-continuous at $a$ . Then there is a sequence $(x_n)$ such that $x_n \neq a$ , $x_n \nearrow a$ , and $F(x_n) \not\to F(a)$ . Because $F$ is non-decreasing, there is $\alpha < F(a)$ such that $F(x_n) \nearrow \alpha$ . By continuity of measure from below $$
\mu ((-\infty, a)) = \lim_n \mu ((-\infty, x_n]) = \lim_n F(x_n) =\alpha.
$$ It follows that $$
\mu (\{a\}) = \mu ((-\infty, a]) - \mu ((-\infty, a)) = F(a)- \alpha >0.
$$ Hence $a$ is an atom, which is a contradiction.","['measure-theory', 'probability-distributions', 'real-analysis']"
4493342,Proof of Multivariate Central Limit Theorem using Cramér-Wold,"Even though there are problems here have the same title to my problem but I have different question. From Jacod-Protter ''Probability Essentials'' Multivariate CLT: Let $(X_j)_{j≥1}$ be i.i.d. $,\mathbb{R}^d$ -valued r.v. Let the ( vector ) $\mu = E\{X_j\}$ , and let $Q$ denote the covariance matrix: $Q = (q_{k,l})_{1 \leq k,l \leq d}$ , where $q_{k,l} = Cov(X_j^k, X_j^l)$ , where $X_j^k$ is the $k^{th}$ component of the $\mathbb{R}^d$ -valued r.v. of $X_j$ . Then $$\lim_{n \to \infty}\frac{S_n - n\mu}{\sqrt{n}} = Z,$$ where L $(Z) = N(0,Q)$ and where converges is in distribution. I found some resources that hinted me to use the Cramer-Wold Device: Let $(X_n)_{n \geq 1}$ and $X$ be a random  vectors in $\mathbb{R}^d$ . Then $X_n \to X$ if and only if $\zeta \cdot X_n \to \zeta \cdot X$ for all $\zeta \in \mathbb{R}^d$ . Here is my attempt: Let $\zeta \in \mathbb{R}^d$ . Then $(\zeta \cdot X_n)_{n \geq 1}$ are independent and since $(X_n)_{n \geq 1}$ are identically distributed $$\varphi_{\zeta \cdot X_n}(u) = E\{e^{iu\langle \zeta, X_n \rangle}\} = E\{e^{\langle iu \zeta, X_n \rangle}\} = \varphi_{X_n}(u\zeta) = \varphi_{X_1}(u\zeta). $$ Thus $(\zeta \cdot X_n)_{n \geq 1}$ are i.i.d. Note that, $$E\{\zeta \cdot X_n\} = \zeta \cdot E\{X_n\} = \zeta \cdot E\{X_1\},$$ \begin{eqnarray}
Var\{\zeta \cdot X_n\} &=&E\{(\zeta \cdot X_n)^2\} - (E\{\zeta \cdot X_n\})^2 \\
&\vdots& \\ 
&=& \zeta \cdot \left( E\{X_nX_n^T\} - E\{X_n\}E\{E_n\}^T\right)\zeta \\
&=& \zeta \cdot Q\zeta.
\end{eqnarray} Now, using the Basic Central Limit Theorem Let $(X_n)_{n \geq 1}$ be i.i.d. with $E\{X_n\} = \mu$ and $Var\{X_n\} = \sigma^2$ for all $n$ with $0 < \sigma^2 \infty$ . Let $Y_n = \frac{\sum_{j=1}^n X_j - n\mu}{\sigma \sqrt{n}}$ . Then $Y_n$ converges in distribution to $Y$ , where L $(Y) = $ N $(0,1).$ we have $$\frac{1}{\sqrt{\zeta \cdot Q \zeta}} \left( \frac{\sum_{j=1}^n \left(\zeta \cdot X_j - \zeta \cdot  E\{X_j\}\right)}{\sqrt{n}} \right) \to Z, \text{ where } L(Z) = N(0,1)$$ \begin{eqnarray}
&\implies& \sqrt{\zeta \cdot Q\zeta} \left( \frac{\zeta}{\sqrt{\zeta \cdot Q \zeta}} \right) \left( \frac{\sum_{j=1}^n X_j - n E\{X_1\}}{\sqrt{n}} \right) \to \sqrt{\zeta \cdot Q\zeta}\ Z, \text{ where } L(Z) = N(0,1) \\
&\implies& \zeta \left(\frac{\sum_{j=1}^n X_j - n \mu}{\sqrt{n}} \right) \to \sqrt{\zeta \cdot Q\zeta} \ Z, \text{ where } L(Z) = N(0,1). \\
& \implies& \zeta \left(\frac{\sum_{j=1}^n X_j - n \mu}{\sqrt{n}} \right) \to \zeta \ Z, \text{ where } L(Z) = N(0,Q) \\
& \implies&  \left(\frac{\sum_{j=1}^n X_j - n \mu}{\sqrt{n}} \right) \to  \ Z, \text{ where } L(Z) = N(0,Q),
\end{eqnarray} as required. My question:
Is $\sqrt{\zeta \cdot Q\zeta} Z, \text{ where } L(Z) = N(0,1)$ is the same or equivalent to $ \zeta Z$ , where $L(Z) = N(0,Q)$ ? If so, can you show me why? If not, can you show me where did I go wrong? Thank you.","['central-limit-theorem', 'gaussian', 'probability-theory', 'normal-distribution']"
4493384,"$\frac{dx^5}{dx^2},$ the derivative of $x^5$ with respect to $x^2$","Let $f(x)=x^5.$ Find its derivative with respect to $x^2$ , i.e., find $\frac{dx^5}{dx^2}.$ We know that $\frac{dx^5}{dx}=5x^4.$ But what should I do when it is needed to take derivative with respect to $x^2 ,x^3$ or other weird things different from classical derivatives? My approaches: $1-)$ Say $x^2=a$ , then $x^5 =a^{5/2}$ . Hence , find $$\frac{da^{5/2}}{da}=\frac{5}{2}a^{3/2}=\frac{5}{2}x^{3}$$ $2-)$ Find $$\lim_{h\to 0}\frac{f(x^2+h)-f(x^2)}{h}=\lim_{h\to 0}\frac{(x^2+h)^5-(x^2)^5}{h}=5x^8$$ My approaches conflict. Why do they give different results? Addendum: Extra examples will be appreciated, for example, $dx^7 /dx^3,\;dx^6/dx^5.$","['calculus', 'derivatives', 'real-analysis']"
4493400,"If $\mu$ is a probability measure, then $\int_{X}\phi \ \mathrm{d}\mu\in\mathbb{C}$ lies in the closed convex hull of $\phi(X)\subset\mathbb{C}$","Let $\mu$ be a probability measure on a measurable space $X$ . Suppose that $\phi\colon X\to\mathbb{C}$ is integrable with respect to $\mu$ . How does one prove that $$\int_{X}\phi \ \mathrm{d}\mu\in\overline{\mathrm{conv}(\phi(X))}?$$ So if $\phi$ is both non-negative and simple , then the integral is of the form $$\int_{X}\phi \ \mathrm{d}\mu=\sum_{j=1}^{n}\alpha_{j}\mu(B_{j})$$ where $\alpha_{1},\ldots,\alpha_{n}\in \phi(X)$ and $\sum_{j=1}^{n}\mu(B_{j})=1$ . Hence in this case it is clear that $\int_{X}\phi \ \mathrm{d}\mu$ belongs to the convex hull of $\phi(X)$ . Since any non-negative function can be uniformly approximated by non-negative simple functions, it should follow that the integral of such a function belongs to the closure of the convex hull of $\phi(X)$ . In general we can write $\phi=\phi_{1}-\phi_{2}+i(\phi_{3}-\phi_{4})$ for non-negative $\phi_{1},\ldots,\phi_{4}$ . But I don't see how the result follows from this. Any suggestions would be greatly appreciated!","['measure-theory', 'convex-hulls', 'lebesgue-integral', 'real-analysis', 'probability-theory']"
4493439,Normal subgroup generated by one element,"Let $F$ be a free group of rank at least two and $\alpha,\beta$ be two elements in $F$ . Let $N(\alpha)$ (resp. $N(\beta)$ ) be the normal subgroup generated by $\alpha$ (resp. $\beta$ ); i.e., $N(\alpha)$ is the minimal normal subgroup of $F$ containing $\alpha$ . Question: if $N(\alpha)=N(\beta)$ , should $\alpha$ be a conjugate of $\beta$ or a conjugate of $\beta^{-1}$ ? One motivation: Two groups each defined by one generator might be isomorphic but the two generators might look ""quite different"". For example, $\langle a,b: a^2b^2\rangle$ is isomorphic to $\langle a,b:aba^{-1}b\rangle$ , as they are different representations the fundamental group of Klein bottle.  The two generators are not a pair of conjugates, but the two generators don't generate the same normal subgroups either.","['combinatorics-on-words', 'combinatorial-group-theory', 'normal-subgroups', 'free-groups', 'group-theory']"
4493465,Estimate from a differential inequality,"Let $[0,T]$ be some finite interval. Consider the differential inequality $$f'(t)\leq C t^{\alpha}f(t)^2+g(t),$$ where $g$ is positive and bounded and $\alpha >0$ , $C<0$ . Can we get a bound for $f(t)$ , $0<t\leq T$ in terms of $f(0)$ and $g$ ?
Edit: Let me just clarify what I am aiming or. If there is no $g$ in the inequality we can divide by $f(t)^2$ and integrate on both sides such that $$\frac{1}{f(t)}\geq \frac{D}{\alpha + 1}t^{\alpha+1}+\frac{1}{f(0)},$$ $D=-C$ .
Hence $$f(t)\leq \frac{D f(0)\frac{1}{t^{\alpha +1 }}+\alpha + 1}{(\alpha + 1)f(0)}.$$ Can we reach something similar when $g$ is present?
Separating the ""variables"" is not so easy as we don't know a potential for $\frac{g}{f}$ . I also tried variation of the parameters but I failed. Is there maybe some other way do this?","['inequality', 'ordinary-differential-equations', 'real-analysis']"
4493515,Finding eigenvalues and eigenvectors geometrically.,"Let $A=\left[\begin{matrix} 0 & 1\\1 & 0 \end{matrix}\right]$ represent reflection about the line $y=x$ . I can calculate eigenvalues and eigenvectors mathematically, but I have hard time getting the results geometrically. There are some similar posts available but they are not of any help. For example in this post , I don't know how they found 1 and -1 as eigenvalues?  I must be missing some points. So the question is, how to find eigenvalues and eigenvectors geometrically? Why in this case eigenvalues are 1 and -1? and how to obtain corresponding eigenvectors?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4493541,How do I know if a 4x4 unitary matrix is a tensor product of two 2x2 unitary matrices?,"Let's say I have a unitary matrix $U$ , how do I know if $U$ is the resultant of a tensor product of two other unitary matrices $U_1$ and $U_2$ (dim $\geq$ 2), such that $U_1\otimes U_2=U$ ? More specifically I am concerned with the almost trivial case of where the dimensions of $U_1$ and $U_2$ are 2x2 ( $U$ is 4x4). Do I necessarily need to find the basis where $U$ is block diagonal? Or do I need to fully diagonalize $U$ and find common factors? Are there any other strategies to verify without doing these? I tried to find some direction, but I do not know what is the right term here, is it reducibility? separability?","['matrices', 'unitary-matrices', 'tensor-products', 'kronecker-product']"
4493548,"If 8 people consisting of 4 couples are randomly arranged in a row, find the probability that no person is next to their partner.","I understand how to arrive at the correct answer using the inclusion-exclusion principle. The purpose of this post is so I can understand why the reasoning I first used is incorrect. My reasoning: there are T = 8! total ways to arrange the 8 people. Define $A_i$ to be the number of ways to arrange 8 people in a line such that exactly i of the 4 couples are seated next to each other. Since T = $\sum_{i=0}^4 A_i$ , I attempted to find $A_0$ by computing $A_1$ through $A_4$ and subtracting from T. $A_4$ : Arrange 4 couples in 4! ways, and for each couple, 2 arrangements. So $2^4$ * 4! = 384. $A_3$ : Pick 3 of the 4 couples to be seated together, each of these couples can be arranged in 2 ways, and there are 5 entities (3 couples and 2 individuals) so 5! ways to arrange the 5 entities, thus ${4 \choose 3}$ * $2^3$ * 5!. However, this has counted the ways counted in $A_4$ . So $A_3$ = ${4 \choose 3}$ * $2^3$ * 5! - $A_4$ = 3840 - 384 = 3456. $A_2$ :  Pick 2 of the 4 couples to be seated together, each of these couples can be arranged in 2 ways, and there are 6 entities (2 couples and 4 individuals) so 6! ways to arrange the 6 entities, thus ${4 \choose 2}$ * $2^2$ * 6!. But this has counted the ways counted in $A_3$ and $A_4$ , so $A_2$ = ${4 \choose 2}$ * $2^2$ * 6! - $A_3$ - $A_4$ = 17280 - 3456 - 384 = 13440. $A_1$ :  Pick 1 of the 4 couples to be seated together, the couple can be arranged in 2 ways, and there are 7 entities (1 couple and 6 individuals) so 7! ways to arrange the 7 entities, thus ${4 \choose 1}$ * $2$ * $7!$ . But this has counted the ways counted in $A_2$ and $A_3$ and $A_4$ , so $A_1$ = ${4 \choose 1}$ * $2$ * $7!$ - $A_2$ - $A_3$ - $A_4$ = 40320 - 13440 - 3456 - 384 = 23040. Now, $\sum_{i=1}^4 A_i$ = 40320 = T, implies $A_0$ = 0 which is obviously not true. Where have I gone wrong?","['statistics', 'solution-verification', 'combinatorics', 'probability']"
4493549,show that there exists a function $g(x)=\frac{\alpha x+\beta}{\gamma x+\delta}$ such that $f(g(x))=x$,"question: Let $a,b,c,d$ be given constants with the property that $ad-bc\neq0$ . If $f(x)=\frac{ax+b}{cx+d}$ , show that there exists a function $g(x)=\frac{\alpha x+\beta}{\gamma x+\delta}$ such that $f(g(x))=x$ . Also show that for these two functions it is true that $f(g(x))=g(f(x))$ . how can I get the answer $\alpha=\frac{d}{ad-bc}$ , $\beta=\frac{-b}{ad-bc}$ , $\gamma=\frac{-c}{ad-bc}$ , $\delta=\frac{a}{ad-bc}$ my approach: first calculate the value of $f(g(x))$ $$f(g(x))=\frac{(a\alpha+b\gamma)x+(a\beta+b\delta)}{(c\alpha+d\gamma)x+(c\beta+d\delta)}$$ then, $f(g(x))=x$ $$
\begin{align}
\frac{(a\alpha+b\gamma)x+(a\beta+b\delta)}{(c\alpha+d\gamma)x+(c\beta+d\delta)}&=x\\
(a\alpha+b\gamma)x+(a\beta+b\delta)&=(c\alpha+d\gamma)x^2+(c\beta+d\delta)x
\end{align}
$$ and I stuck at this step. please help!",['algebra-precalculus']
4493550,"What is the continuous version of ""diagonal matrix""?","A diagonal matrix has the nice property that its eigenvectors span the space it acts on. What if the space is infinite-dimensional? What would you call a diagonal operator? Does the space have to be a Hilbert space? Banach space? How can we think about a diagonal operator on, say, $L^2[0,1]$ ?","['operator-theory', 'functions', 'functional-analysis', 'linear-transformations']"
4493580,For which periodic sequences $(a_n)$ does the series $\sum \frac{a_n}n$ converge?,"Let $(a_n)$ be some sequence of real (or maybe even complex) numbers. For which sequences does $$S=\sum_{n=1}^\infty \frac{a_n}n$$ converge to a finite value? Let $p$ denote the period of $a_n$ , i.e. $a_n = a_{n+p}$ for all $n$ . $\large{p=1}$ The series $S$ will only converge if $a_1=0$ because otherwise, $S$ is the Harmonic series. $\large{p=2}$ The series will converge if $a_2=-a_1$ . Then the series converges to $S= a_1\ln 2$ . If $a_2=-a_1+\Delta$ then $S$ can be split into two series, one converging to $a_1\ln 2$ , and one diverging like $\Delta$ times the harmonic series. $\large{p\geqslant3}$ For this case I have only the  conjecture that $$S \text{  converges } \quad\iff\quad \sum_{n=1}^p a_n = 0$$ but I have no idea how to proceed?","['real-analysis', 'complex-analysis', 'conditional-convergence', 'sequences-and-series', 'convergence-divergence']"
4493589,Lines with irrational slope in a torus,"I'm trying understand one point about Example 15.9 in the Introduction to Manfolds by Tu (p. 167). I'll reproduce the example here. See this post , which addressed a separate question I had. Example 15.9 ( Lines with irrational slope in a torus ). Let $G$ be the torus $\mathbb{R}^2/\mathbb{Z}^2$ and $L$ a line through the origin in $\mathbb{R}^2$ . The torus can also be represented by the unit square with the opposite edges identified. The image $H$ of $L$ under the projection $\pi:\mathbb{R}^2\longrightarrow\mathbb{R}^2/\mathbb{Z}^2$ is a closed curve if and only if the line $L$ goes through another lattice point, say $(m,n)\in\mathbb{Z}^2$ . This is the case if and only if the slope of $L$ is $n/m$ , a rational number or $\infty$ ; then $H$ is the image of finitely many line segments on the unit square. It is a closed curve diffeomorphic to a circle and is a regular submanifold of $\mathbb{R}^2/\mathbb{Z}^2$ (Figure 15.1). If the slope of L is irrational, then its image H on the torus will never close up. In
this case the restriction to L of the projection map, $f := \pi\big|_L : L \rightarrow \mathbb{R}^2/\mathbb{Z}^2$ , is a one-to-one
immersion . We give H the topology and manifold structure induced from f. I don't understand why $f$ is an immersion. How would I prove this? Why is the subspace topology of H in $\mathbb{R}^2/\mathbb{Z}^2$ a strict subset of the  topology on $H$ induced from $ f : L \xrightarrow{\sim} H$ ? For the induced topology, I understand that a basic open set of H is the image under $f$ of an open set in $L$ . I also understand that basic open sets in the subspace topology will be the intersection of H with the image of an open ball in $\mathbb{R^2}$ .","['smooth-manifolds', 'differential-geometry']"
4493654,Can the equation $2^x + x = 11$ be solved without graphing?,"$2^{x}+x = 11$ Well this problem is easy to solve just by looking at its graph, and we find the answer is $x = 3$ , but I want a way of solving it rather than just looking at it to find the solution. I realized it was harder than it seemed or I am missing something obvious. Trying to use exponent and logarithmic rules all ended up with a dead end, and the graphing solution is unsatisfactory to me.","['numerical-methods', 'closed-form', 'transcendental-equations', 'algebra-precalculus', 'exponential-function']"
4493656,Left Translation Action is Ergodic with respect to Haar,"I am trying to solve exercises on ergodic group actions, from the A. Ioana's lecture notes ""Orbit Equivalence of Ergodic Group Actions"" . The following exercise (p.3, Exr.1.14) has two parts, and I am stuck on the second. Let $G$ be a compact metrizable group with a dense countable subgroup $\Gamma \le G$ . Assume $(G, \mu)$ is the standard probability space defined by the Haar measure. Show that the left translation action of $\Gamma$ on $G$ is free and ergodic. The action is free iff every non-neutral $g \in \Gamma$ satisfies $\mu(\{x \in G \mid gx = x\}) = 0$ . The action is ergodic iff any $\Gamma$ -invariant Borel subset $B \subseteq G$ satisfies $\mu(B) \in \{0,1\}$ . The fact the action is free seems fairly trivial, since the left translation has no fixed points for any non-neutral $g \in \Gamma$ . I'm having a hard time proving it is ergodic. Starting from some $\Gamma$ -invariant Borel subset $B \subseteq G$ , we can assume that $\mu(B) > 0$ . To conclude the proof, we'd then need to show that $\mu(B) = 1$ . I'm not sure which properties I'm supposed to leverage, but the density of $\Gamma$ seems key somehow. One of my attempts involved the set $\bigcup_{k \in \mathbb{N}} \Gamma^kB \subseteq B$ and attempting to use it to provide a lower bound to the measure, but I didn't get much farther with that.","['topological-dynamics', 'measure-theory', 'ergodic-theory', 'group-actions', 'dynamical-systems']"
4493658,Alternative Proofs: Closed proper subspace of Hilbert space has non-empty orthogonal complement,"Let $(H,\langle \cdot, \cdot \rangle)$ be a Hilbert space. I am asking for people to present any short/basic/fundamental proofs they know of the following theorem. Theorem 1: Let $V \subseteq H$ be a closed, proper vector subspace of $H$ . Then $V^\perp$ contains a non-zero vector. Theorem 1 follows pretty easily as a corollary of either of the following theorems. Theorem 2: A vector subspace $U$ of $H$ is dense if and only if $U^\perp$ is trivial. Notes on Theorem 2: The proof may be found here , requiring many pages of prior results. These results include $U^\perp = (\overline{U})^\perp$ and $(U^\perp)^\perp = \overline{U}$ for vector subspaces $U$ of $H$ , which in turn require the ""Hilbert projection theorem"" and the continuity of the inner product. Theorem 3: $H = U \oplus U^\perp$ for any closed vector subspace $U$ of $H$ . Notes on Theorem 3: A proof of this may be found here . I believe this proof requires less machinery than the proof of Theorem 2. In particular, only the construction of the orthogonal projection and some basic properties of the projection are required. Allow me to explain my thoughts a bit further. Theorem 2 and 3 are definitely not long proofs which make use of important, fundamental facts about orthogonality in Hilbert space. However, Theorem 2 and 3 seem to be much stronger results than Theorem 1. I have a gut feeling that Theorem 1 may be proven without having to resort to the ""machinery"" required for Theorem 2 or 3 (if you could really call it machinery). I guess another way to put it is that I believe there should be much more basic proofs of Theorem 1. However, I have not been able to make progress on such a proof myself. If I were to guess, a proof by contradiction making clever use of the continuity of the inner product might work. But, again, I've had no such success. Please feel free to drop your proof approaches below or any reasoning as to why you think Theorem 2 or 3 are the shortest / most fundamental methods. I hope this post allows for a better understanding for the true difficulty or simplicity of Theorem 1. So, although this is not strictly what I am looking for, also feel free to drop any super sleek proofs of Theorem 1 that use really heavy machinery ahaha Maybe such proofs will enlighten why Theorem 1 cannot be proven in a more fundamental manner. Thanks!","['hilbert-spaces', 'alternative-proof', 'orthogonality', 'functional-analysis']"
4493679,Equal integration of continuous functions gives equality of measures [duplicate],"This question already has an answer here : When are two measures equal? (1 answer) Closed 1 year ago . Let $X$ be a compact Hausdorff space and $C(X)$ denotes the space of all continuous functions from $X$ to $\mathbb C$ . Let $\mu$ and $\nu$ are two Borel complex measures defined on $X$ such that $\displaystyle \int f d\mu= \displaystyle \int f d\nu$ for all $f \in C(X).$ Then I want to show that $\mu=\nu.$ My approach: Let $\Delta$ be the sigma-algebra of all Borel sets of $X$ . In order to show that $\mu=\nu$ , we have to show $\mu(U)=\nu(U)$ for all $U\in \Delta$ . For that I have to show $\displaystyle \int 1_U d\mu= \displaystyle \int 1_U d\nu$ but indicator functions are not always continuous. So I have to show that, there exists a net $(f_\lambda)_\lambda$ of continuous functions in $X$ such that $~\displaystyle\lim_\lambda f_\lambda = 1_U.$ Then we are done. If my approach is on the right direction please help me to complete this or if I am in wrong direction please  help me to solve this with some different approach. Thank you for your time. Thanks.","['integration', 'measure-theory', 'borel-measures', 'real-analysis']"
4493710,Which Subsets Of $\mathbf{R}^n$ Make Sense To Define Differentiable Functions On Them?,"Just a random thought, for $A\subseteq\mathbf{R}^n$ with $0\in A$ we can define a derivative of a function $f:A\to\mathbf{R}$ at $0$ as a linear function $g:\mathbf{R}^n\to\mathbf{R}$ such that $\lim_{x\to 0}(g(x)-f(x))/\|x\|=0$ but this $g$ doesn't have to be unique.  This is the case of course when e.g. $A$ is some proper linear subspace of $\mathbf{R}^n$ , but for a less trivial example, consider $A:=\{x\in\mathbf{R}^2:|x_1|\le x_0^2\}$ and $f(x):=0$ .  Then $f$ itself is one derivative, but also $g(x):=x_1$ , as $$\frac{|g(x)-f(x)|}{\|x\|}=\frac{|x_1|}{\sqrt{x_0^2+x_1^2}}\le\frac{x_0^2}{\sqrt{x_0^2}}=x_0$$ and $\lim_{x\to 0}x_0=0$ . So, my question, what property does $A$ need to satisfy such that all functions on $A$ have at most one derivative at $0$ ?  My thought was something like there exists $\delta>0$ such that for $\epsilon>0$ there exists $P\in A^n
\subseteq\mathbf{R}^{n\times n}$ with $\|P\|<\epsilon$ and $\det(P)\ge\delta\|P\|$ for some fixed matrix norm $\|.\|$ .  In other words, $A$ contains arbitrary small bases of $\mathbf{R}^n$ whose determinant is not arbitrary small in relation to the length of the vectors in that base. Is this characterization correct and/or is there a simpler way to express this property? [EDIT] Aphelli noted that $\det(P)\in O(\|P\|^n)$ so I guess it's better to say $\det(P)\ge\delta\|P\|^n$ ?","['recreational-mathematics', 'analysis']"
4493713,Monotone converge recursive question,"Define the sequence $(x_n)$ recursively by setting $$
\begin{align} 
x_1 &= \sqrt{2} \\ x_{n+1} &= \sqrt{2 + x_n} \text{ for all $n \in 1,2,3,\ldots$} 
\end{align}
$$ (a) Show that the sequence $(x_n)$ converges. (b) Let $\lambda = \lim_{n \to \infty} x_n$ . Show $\lambda^2 - \lambda - 2 = 0$ . Hi, I was wondering if the proof to this question for part(a) is correct. We can prove this by the monotone convergence theorem. We can show that the sequence is monotone increasing and bounded above. We know that $x_1 = \sqrt{2}$ and $x_2 = \sqrt{2+\sqrt{2}}$ . Hence, $x_1 \leq x_2 \leq 2$ , which will serve as our base case for our induction step. Suppose $x_n \leq x_{n+1} \leq 2$ . $x_n \leq x_{n+1} \leq 2 \iff x_n + 2 \leq x_{n+1} + 2 \leq 4 \iff \sqrt{x_n + 2} \leq \sqrt{x_{n+1} + 2} \leq 2$ . Therefore, $x_n \leq x_{n+1} \leq 2 \implies x_{n+1} \leq x_{n+2} \leq 2$ . By induction, we know that the sequence is increasing. We also have shown that it's bounded. Therefore, by the monotone convergence theorem, $\lim x_n$ converges. Also, I am pretty sure the sequence converges to 2, but I don't know how to prove that. It would make part (b) so much easier. I was thinking of doing something with $\sup \{x_1, x_2, x_3, ...\}$","['recursion', 'analysis', 'real-analysis', 'solution-verification', 'limits']"
4493731,"Consider lists of length 4 made with symbols A,B,C,D,E,F,G. How many are there if repetition is allowed and the list has an E?","This question was from the Book of Proof by Richard H. I initially thought I can do $(1 \cdot 7^3) \cdot 4 = 1372$ , because I thought if one entry has E, then all other entries can have any symbol. I understood I was wrong when I read the solution: Now we seek the number of length- $4$ lists where repetition is allowed and the list must contain an E. Here is our strategy: By Part (a) of this exercise there are $7 \cdot 7 \cdot 7 \cdot 7 = 7^4 = 2401$ lists with repetition allowed. Obviously this is not the answer to our current question, for many of these lists contain no E. We will subtract from $2401$ the number of lists that do not contain an E. In making a list that does not contain an E, we have six choices for each list entry (because we can choose any one of the six letters A, B, C, D, F or G). Thus there are $6 \cdot 6 \cdot 6 \cdot 6 = 6^4 = 1296$ lists without an E. So the answer to our question is that there are $2401 − 1296 = 1105$ lists with repetition allowed that contain at least one E. but I am still not sure why my approach would have $267$ more lists. I guess I don't understand what $(1 \cdot 7^3) \cdot 4$ would represent.","['solution-verification', 'combinatorics', 'discrete-mathematics']"
4493806,Are these spaces of continuous functions equivalent?,"I'm wondering if $C([0,T]\times\mathbb{R}^n)$ and $C([0,T];C(\mathbb{R}^n))$ are equivalent. I know I can show $C([0,T];C(\mathbb{R}^n))\hookrightarrow C([0,T]\times\mathbb{R}^n)$ through $$|f(t,x)|\leq |f(t,x)-f(t',x)|+|f(t',x)|$$ by using uniform continuity in $t$ to $C(\mathbb{R}^n)$ . However, when going the other direction, I know for fixed $x\in\mathbb{R}^n$ , $f(t,x)$ is uniformly continuous in time, but I don't know if this uniform modulus of continuity is uniform over $x$ as it would be in $C([0,T];C(\mathbb{R}^n))$ . Edit since I can't comment: $C(\mathbb{R}^n)$ is the set of continuous, bounded functions from $\mathbb{R}^n$ to $\mathbb{R}$ .","['partial-differential-equations', 'functional-analysis', 'analysis', 'real-analysis']"
4493832,Average Energy/Light in a box,"Imagine there exists a region of completely empty space bounded by a rectangular prism, R, with length, l, width, w, and height, h. There is a star(a simple light/particle emitting object) in the center of one of the faces of the bounding rectangular prism. there is a gradient of light fall off which starts with the lights original brightness and slowly gradually decreases towards zero the farther away you get from the emitter, even outside of the bounded rectangular prism you could imagine it reaches zero at infinity. This light fall off propagates spherically from the emitter . What is the average energy/light of the entire bounding rectangular prism. Another way to visualize this is a box with density which decreases spherically from a point on the side of the box and you must evaluate the average density of the box. The function of light fall off should look something like $$L(x) = \frac{2E}{1+e^{\frac{1}{c}x}}$$ or $$L(x) = \frac{1}{\frac{1}{c}x+E}$$ where x is the distance from the emitter, E is the starting energy/density, and c is some constant which controls the rate of fall off.
while the exact function does not matter to me  I have provided two examples for you to use whichever you think will be easier to work with. Feel free to also use any functions you like which follows the same boundary conditions of starting at E at $x=0$ and ending tending towards $0$ as $x$ goes to $\infty$ . I have attempted this by trying to take rectangular cross sections of a perfect sphere/hemisphere to no avail, I also have tried to create functions which capture the x,y, and z inputs for rotation to any given ray/vector from the emitter but I am unaware and could not find how to map a function which draws out the rectangular prism in 3d space, although I figure you could do it with some form of multivariable Fourier series, but i do not feel that is the easiest method to go about generating a approximation. if it is much simpler feel free to generalize to a perfect cube rather then a rectangle but im not sure it would help much either way. Feel free to choose the center of any face on the rectangular prism to place the emitter.","['integration', 'multivariable-calculus', 'calculus', 'average', 'exponential-function']"
4493874,Is there an explicit formula for a cosine function of x-coordinate? A question on ideas in Spivak's Calculus chapter on trigonometric functions.,"Spivak's Calculus has a Chapter 15 entitled ""Trigonometric Functions"". He starts with a discussion which I found a bit convoluted. I will describe how I understood the flow of ideas and concepts and at the end I will pose two questions that I have. The idea of an angle is as simple as the union of two half-lines with a common initial point. A directed angle can be thought of as a pair of half lines with the same initial point Okay, not sure what the difference is between the two. Next, it seems Spivak tries to motivate what will ultimately be the definitions of sine and cosine with some initial ideas. Idea 1 We can specify a directed angle as a single point on the unit circle, ie a point $(x,y)$ with $x^2+y^2=1$ . We can then define sine and cosine of a directed angle as the $y$ and $x$ coordinates of the point representing the directed angle. If we specify the directed angle by a number (degrees or radians), we could define sine and cosine functions. Given the directed angle as a number, we know the point on the unit circle, given this point we know cosine and sine. In particular, if we measure the directed angle using radians, we are effectively giving each directed angle a number equal to arc length. Hence, for each arc length we get a point, and given a point we get cosine and sine. Okay, so the above is sort of an idea of what would happen if we were to define cosine and sine as simple x and y coordinates of a point on the unit circle, where the point is determined by a directed angle that is measured in radians. Idea 2 What if instead of specifying an arc length, we were to specify an area of a sector on the unit circle (by the way, of course, starting at position $(1,0)$ moving counter-clockwise). Given an arc length $x$ , the sector area is $\frac{x}{2}$ . Thus, instead of saying $\sin x$ and $\cos x$ are the coordinates of the point determined by
arc length $x$ , where $x$ is in radians we could say $\sin x$ and $\cos x$ are the coordinates of the point determined by a
sector area of $\frac{x}{2}$ , where $x$ is in radians Okay, at this point, the motivational ideas are finished and the formal definitions begin. I assume that at this point the ""definitions"" above were informal, and right now we start again from scratch, with $\cos$ and $\sin$ not yet defined. Formal Definitions First off, $\pi$ is defined as $$\pi=2\int_{-1}^1 \sqrt{1-x^2}dx$$ Then, an area function is defined. $$A(x)=\frac{x\sqrt{1-x^2}}{2}+\int_x^1 \sqrt{1-t^2}dt$$ This is just the function giving the area of a sector of the unit circle given an x-coordinate: This function has certain characteristics: it is continuous on $[-1,1]$ , is decreasing and hence one-one. So at this point $\cos x$ is defined as the unique number in $[-1,1]$ such that $$A(\cos{x})=\frac{x}{2}\tag{1}$$ and $\sin{x}$ is defined as $$\sin{x}=\sqrt{1-\cos^2{x}}$$ Then comes a theorem that shows that $\cos'{x}=-\sin{x}$ and $\sin'{x}=\cos{x}$ . I have two questions Is there a formula for $\cos{x}$ , or is it just defined implicitly as in $(1)$ ? Could we have defined $\cos$ in the following way instead of an area function we define an arc length function that given an x-coordinate would give us the arc-length (would this be done with a line integral?) find the inverse of such a function; ie a function that given an arc length gives us the x-coordinate. define $\cos x$ as the latter function define $\sin x$ as before","['integration', 'limits', 'calculus', 'derivatives']"
4493896,"Number of acute triangles using rods of lengths $1, 2, \ldots, n$","Problem. Suppose one has exactly $n \geq 3$ rods, which have lengths $1,\ldots,n$ . How many non-degenerate acute triangles can be formed using these rods? ( Clarification: You must use precisely $3$ rods in each triangle. Concatenation of rods along a side is not allowed, and reusing of any rod is not allowed like $\{1,1,1\}$ .) Almost same to Number of Triangles Using Rods of Lengths 1, 2, ..., n , but needs acute triangles. I run a quick bench on Python, from itertools import combinations
lst = []
for n in range(3,33):
    cnt=0
    for a,b,c in combinations(range(1,n+1),3):
        if a+b>c and a**2+b**2>c**2:
            cnt+=1
    lst.append(cnt)
print(lst) and I got [0, 0, 0, 1, 3, 6, 10, 15, 22, 32, 43, 56, 71, 90, 112, 137, 165, 196, 232, 271, 315, 364, 416, 473, 535, 603, 677, 756, 842, 933] but there was no correct one in OEIS.
So what's the exact formula of this?","['geometry', 'sequences-and-series']"
4493923,What is the difference between a column vector and tuple?,"We sometimes right a column vector as $(x_1,x_2,x_3)$ with parentheses (to save space and not confuse it with a row vector), we also tend to write euclidean vectors in this way, in both cases it looks like a 'tuple', we know that $\mathbb R^n$ is defined using tuples, is there an equivalence between the two? Can we write a column vector as being equal to a tuple, perhaps in the column vector case this is just an alternative notation for a column vector and in fact not a tuple? If there was an equivalence the difficulty is that both a row and column vector could in theory be written as a tuple, and as we know the matrices are not equal.","['matrices', 'notation', 'linear-algebra']"
4493961,How to prove that $AB$ is a projection if $(AB)(BA)=AB$?,"I was trying to solve the following problem: Assume $A,B\in M_n\left( \mathbb{C} \right)$ ,satisfy $$AB^2A=AB.$$ I need to proof $$\left( AB \right) ^2=AB.$$ I tried to use some equivalent substitution of matrices, but I did not succeed. I also tried to find some counterexamples of matrices, such as 2nd order matrices, but I did not succeed either. I don't know if this is a right problem or a wrong problem. I hope to solve this problem. Thanks!","['matrices', 'projection', 'linear-algebra', 'idempotents']"
4493996,Fix $x$ and $y$ in $\Bbb R$. Suppose that $x < y + e$ for all $e>0$. Prove that $x\leqslant y$.,"I just started learning proofs in analysis class and this is my first time in this page.
I learned about the Completeness Axiom and tried to solve some problems, but none of them look easy to me This is my proof: Assume $x>y$ is true, then there exists a number $n$ in natural numbers such that $n>e$ thereby making $x+n>y+e$ by the Archimedean Property $x+n>y+e$ and $x<y+e$ contradiction
Therefore, $x\leqslant y$ (I think my proof is seriously messy) If it is possible can you please tell me how can I approach proof problems?
Should I first look at the answers and memorize them all or just keep trying?
I have no idea where to start
And can you also teach me where can I learn how to write a decent coded math equation so that I can clearly deliver my questions?
Thank you so much for your help","['solution-verification', 'analysis']"
4494023,Can I use algebraic identities with matrices?,"Let $A$ and $B$ be square matrices, so that it is possible to add or multiply them with themselves and with each other. Can algebraic identities such as $(A+B)^2=A^2+2AB+B^2$ apply to them?",['matrices']
4494025,A new infinite series for the minimum of the Gamma function?,"I continue my work on the minimum of the Gamma's function and recently I found a slightly different way via an infinite series as I have the beginning : Let $0<x$ then define : $$f(x)=x!,h(x)=f'(x)$$ Then we have : $$h(k)=0$$ And : $$k\simeq \frac{1}{2}\left(\pi-e+\frac{1}{2}\right)-\frac{1}{28}\left(\pi-e\right)^{\pi e}-\frac{10}{37}\left(\pi-e\right)^{2\pi e}-\frac{10000}{3675}\left(\pi-e\right)^{3\pi e}-\frac{1000}{499}\left(\pi-e\right)^{4\pi e}$$ I would like to find a infinite series like : $$k=\frac{1}{2}\left(\pi-e+\frac{1}{2}\right)-\sum_{k=1}^{\infty}a_k\left(\pi-e\right)^{k\pi e}$$ Where $a_k>0$ I can progress numericaly not theoreticaly . Edit following Tyma Gaidash's comment : We have (if there is no mistake) 36 decimals right for the minimum value of the gamma function taking for $k$ : $$k\simeq \frac{1}{2}\left(\pi-e+\frac{1}{2}\right)-\frac{1}{28}\left(\pi-e\right)^{\pi e}-\frac{10}{37}\left(\pi-e\right)^{2\pi e}-\frac{10000}{3675}\left(\pi-e\right)^{3\pi e}-\frac{1000}{499}\left(\pi-e\right)^{4\pi e}-\frac{736}{100}\left(\pi-e\right)^{5\pi e}-\frac{5}{10}\left(\pi-e\right)^{6\pi e}$$ How to find the sequence $a_k$ ?","['gamma-function', 'maxima-minima', 'sequences-and-series', 'constants', 'convergence-divergence']"
4494053,A simple problem on regular polygon inscribed in a unit circle,"Let $A_1,A_2,...,A_n$ be the vertice of an regular polygon inscribed in a unit circle, $P$ is a point on the circle. What is the maximum and the minimum values of $\sum_{k=1}^n|PA_k|$ ? My idea : Maybe we can define $\omega=e^{\frac{2\pi i}n}$ , $A_k=\omega^k$ and $P=e^{i\theta}$ , then $$\sum_{k=1}^n|PA_k|=\sum_{k=1}^n|e^{i\theta}-e^{\frac{2k\pi i}n}|=\sum_{k=1}^ne^{i(-\arg (P-A_k))}(P-A_k)=...$$ In this way, perhaps we can write the final result in the form of trigonometric functions, so that it is convenient to find its maximum and minimum values. But I failed to expand and simplify it, can anyone help me?","['geometry', 'complex-numbers']"
4494062,Equations for geodesics of the plane in polar coordinates,"I'm studying equations for geodesics of the plane in polar coordinates from the book ""gravity by James hartle"" and I am stuck at the last step which involves partial derivative of 3 variable.
The metric of the plane in polar coordinates $r$ and $Φ$ is : $$
dS^2 = dr^2 + r^2dΦ^2
$$ A curve between two points $A$ and $B$ can be described parametrically by giving $r$ and $Φ$ as a function of a parameter $σ$ which varies between $σ=0$ at point $A$ and $σ=1$ at point $B$ distance between $A$ and $B$ is given by: $$
\int_A^B dS = \int_A^B (dr^2 + r^2 dΦ^2)^{1/2}
                  = \int_0^1 {dσ} \left[ \left(\frac{dr}{dσ}^2\right) + r^2\left(\frac{dΦ}{dσ}\right)^2\right]^{1/2}
$$ Solve the above equation by Lagrangian and we will get : $$
\frac d{dσ}\left(\frac1L\frac{dr}{dσ}\right) = \frac rL\left(\frac{dΦ}{dσ}\right)^2
\tag{1}
$$ $$
\frac d{dσ}\left(\frac1L r^2 \frac{dΦ}{dσ}\right) = 0
\tag{2}
$$ Now writer says that the value of $L$ is just $dS/dσ$ . Therefore, multiplying above equations by $dσ/dS$ , the equations for geodesics using the distance $S$ as the parameter along the curve take the simple form $$
\frac{d^2r}{ds^2} = r\left(\frac{dΦ}{dS}\right)^2
\tag{3}
$$ $$
\frac d{dS}\left(r^2 \frac{dΦ}{dS}\right) = 0
\tag{4}
$$ Can somebody please explain how do we get equations 3 and 4 from 1 and 2 ?? I understand the rest of the calculation but facing difficulty how to deal with these mix partial derivatives. Thanks","['geodesic', 'multivariable-calculus', 'polar-coordinates', 'partial-differential-equations', 'derivatives']"
4494066,proof verification for an integral inequality,"Problem: Let $f(x)$ be a continuous function on $[a,b]$ , such that $|f(x)|\leq M$ and $\int_a^b f(x)dx = 0$ . Prove that $$|\int_a^b xf(x)dx| \leq \frac{M}{4}(b-a)^2.$$ Can anyone help verify my proof? Here are my own attempts: My attempt: Write $I_a(x) = \int_a^xf(t)dt$ , $\,$ $I_b(x) = \int_b^xf(t)dt$ , $\,$ and denote by $c$ the midpoint $\frac{a+b}{2}$ of the inteval $[a,b]$ . We can see that $$\int_a^cxf(x)dx = \int_a^cxI_a^{'}(x)dx = cI_a(c) - aI_a(a) - \int_a^cI_a(x)dx = cI_a(c) - \int_a^cI_a(x)dx.$$ For the same reason $$\int_b^cxf(x)dx = cI_b(c) - \int_b^cI_b(x)dx.$$ So the left hand side of the original inequality can be written as $$|\int_a^bxf(x)dx| = |\int_a^cxf(x)dx - \int_b^cxf(x)dx|\\ = |c(I_a(c)-I_b(c))+\int_b^cI_b(x)dx-\int_a^cI_a(x)dx| \\= |\int_b^cI_b(x)dx-\int_a^cI_a(x)dx|$$ since $I_a(c)-I_b(c) = \int_a^bf(x)dx = 0$ . $\int_b^yI_b(x)dx$ can be viewed as a function $\Phi_b(y)$ of $y$ . By Taylor formula, $$\Phi_b(y) = \Phi_b(b) + \Phi_b^{'}(b)(y-b)+\frac{1}{2}\Phi_b^{''}(\theta_2)(y-b)^2 \\ = 
\frac{1}{2}f(\theta_2)(y-b)^2$$ where $\theta_2 \in (y,b)$ . Let $y = c$ we have $\int_b^cI_b(x)dx = \frac{1}{2}f(\theta_2)(\frac{b-a}{2})^2$ and $\theta_2 \in (c, b)$ . By a similar argument $\int_a^cI_a(x)dx = \frac{1}{2}f(\theta_1)(\frac{b-a}{2})^2$ , where $\theta_1 \in (a,c)$ . So $$|\int_a^bxf(x)dx| = |\int_b^cI_b(x)dx-\int_a^cI_a(x)dx| \\= \frac{1}{2}(\frac{b-a}{2})^2|f(\theta_1) - f(\theta_2)| \\ \leq \frac{1}{2}(\frac{b-a}{2})^2(|f(\theta_1)| + |f(\theta_2)|) \\ \leq \frac{1}{2}(\frac{b-a}{2})^2(2M)  = \frac{M}{4}({b-a})^2$$ because $|f(\theta_i)|\leq M$ .","['integration', 'calculus', 'integral-inequality', 'analysis']"
4494079,Calculate an indefinite integral,"Given $\lambda \in [-1,1]$ is it possible to compute explicitly the following integral? \begin{align*}
 \int \frac{1}{\sqrt{y^2(y^2 - 2) - \lambda^2(\lambda^2 - 2)}}\,dy
\end{align*} I have tried to rewrite the above primitive as \begin{align*}
 \int \frac{1}{\sqrt{(y^2 - 1)^2 - (\lambda^2 - 1)^2}}\,dy
\end{align*} and apply trigonometric substitution. But, unless that I've got some mistake, it didn't work. Can someone help me with this? I thank you in advance.","['integration', 'calculus', 'functions', 'indefinite-integrals', 'substitution']"
4494081,"If a set $A$ is a Borel set, is the union of all line segments from some point $x\in \mathbb{R}^3$ to the points in $A$ a Borel set? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Specifically, if $A\subseteq \mathbb{R}^3$ is a Borel set, is the set $C_{x,A}=\{at+x(1-t)|t\in[0,1],a\in A\}$ a Borel set if $x\in \mathbb{R}^3$ ? Is the set $C_{x,A,\infty}=\{at+x(1-t)|t\in[0,\infty),a\in A\}$ Borel?",['measure-theory']
4494093,on the size of $e^{i \theta } - 1 - i \theta$?,"For any $\theta \in \mathbb{R}$ , it is well known $$
|e^{i\theta}| = 1. 
$$ It is follows that $$
|e^{i \theta} - 1| \leq 2
$$ even though the Taylor series look like $$
e^{i \theta} - 1 = i \theta + \frac{(i \theta)^2}{2!} + ...
$$ Thus even if $\theta$ is very large, things cancel in this case.
Does this continue to the next term is my question?
In other words, does there exist a constant $C > 0$ such that $$
|e^{i \theta} - 1 - i \theta| \leq C
$$ for all $\theta \in \mathbb{R}$ . I don't think such $C$ exists but I couldn't find a definitive way to convince myself.... any suggestion appreciated","['complex-analysis', 'calculus', 'real-analysis']"
4494129,"Given $g(x) = e^{f(2x)}, f(2) = 0, f'(2) = 1$, what is $g'(1)$?","Question . Suppose $g(x) = e^{f(2x)}, f(2) = 0, f'(2) = 1.$ Find $g'(1)$ . Current progress and thoughts . I've found that $$\displaystyle g'(x)=e^{f(2x)}\cdot \frac d {dx}f(2x)$$ using the chain rule.
But I do not know how to make use of $f(2)$ or $f'(2)$ to find $g'(1)$ .
Is my derivative correct, and I just don't understand how to plug in values? Or what am I doing wrong?","['calculus', 'derivatives']"
4494219,"Prove that for all natural numbers $n \geq 2$, $n^2 \geq n + 2$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Prove that for all natural numbers $n \geq 2$ , $n^2 \geq n + 2$ . In solving this proof, I tried two methods, first working backward and then using the contradiction approach. i wish to solve this proof directly or using a contradiction however I am stuck on how to verify it for $n\geq 2$ .","['elementary-number-theory', 'proof-writing', 'discrete-mathematics']"
4494232,what is the link between exponential/logarithmic function and 1/x,"Thanks to Kalid Azad's book (betterexplained.com), I understand exponential phenomena better. $ae^{rt}$ gives the growth of ' $a$ ' after ' $t$ ' unit of times and with a ""continuous"" growth rate of ' $r$ ' (so if $rt = 1$ like in $e^1$ the formula will output the new value of ' $a$ ' after a $100\%$ of growth during $1$ unit of time ' $t$ ', it can also be thought of as a $50\%$ growth during $2$ periods of times, etc...). The $\ln(x)$ function outputs the amount of time needed to have a certain growth of the quantity ' $1$ '. e.g. $\ln(2.71\dots) = 1$ (we need $1$ unit of time to transition from $1$ to $2.71\dots$ with $100\%$ continuous growth). My definitions are not $100\%$ mathematical and precise but I can't visualize and understand $\exp(x)$ or $\ln(x)$ without them (especially their applications in engineering stuff). The $\ln(x)$ function is the antiderivative of $\frac1x$ (or $\frac1x$ is the derivative of $\ln(x)$ ), and my question is what's the link between $\frac1x$ and the time needed to have a continuous growth of a rate "" $r$ "" and during $x$ unit of times. For example, why the derivative of the $\ln(x)$ , the function returning the time to achieve $100\%$ growth during a time unit, is the inverse of the time unit $x$ . What's the intuitive explanation of $\ln(x)$ being the antiderivative of $\frac1x$ ? $$e^x = e^1, e^2, e^3, e^4,\dots$$ $$\ln(e^x) = 1, 2, 3, 4,\dots$$ $$\frac1x = \frac1{e^1}, \frac1{e^2}, \frac1{e^3}, \frac1{e^4},\dots$$","['integration', 'logarithms', 'inverse-function', 'derivatives', 'exponential-function']"
4494235,Ways to guarantee that a group presentation defines a finite group when randomly generating group presentations,"What are some ways to guarantee that a group presentation defines a finite group? Or, equivalently, is there a convenient sufficient condition for a generators-and-relations presentation of a group defining a finite group that's useful for randomly generating such groups. If there are no known convenient sufficient conditions, I am okay with a condition that gives me a finite group with high probability. I'm interested in ways of generating random finite groups. I can think of two obvious approaches: generating random instances of some other structure like a graph or a magma and looking at its automorphisms. taking a set of generators and imposing some random relations on it. I like the generators and relations presentation of groups, since the artifact that it gives me back is easy to compute with. One problem though is that it seems to hard to tell in general (maybe undecidable?) whether the defined group is finite or not. For example, $\langle a, b : a^2 = b^2 = 1 \rangle$ is not a finite group because $a \cdot b$ has infinite order. I can think of a very heavy-handed approach that might allow me to insist that the group in question is finite ... and that is using the equivalent of an axiom schema to set an upper bound (in the sense of the divisibility relation) on the order of each element. $\langle a, b : \xi^4 = 1 \;\text{and}\; a^2 = b^2 = 1 \rangle$ (where $\xi^4 = 1$ is a relation schema) defines a finite group. In order to prove this we can write an algorithm for taking any product of generators of length $\ge 8$ . Proof First, suppose the product is $^{-1}$ -free and has length $8$ , then the product contains $a \cdot a$ , contains $b \cdot b$ , or consists of the same segment repeated four times. A rewrite to a strictly shorter product is thus possible. If the product does contain $^{-1}$ , we first normalize it so that only generators are inverted, then we use the rewrites $a^{-1} = a$ and $b^{-1} = b$ . End of Proof This answer which references the Grigorchuk group , though, makes me think that just insisting that my group is finitely generated and every element has finite order isn't enough. So, I guess, to sum up the question: Is there a nice sufficient condition for group finiteness that I can impose on a generators-and-relations presentation of a group? (Keeping in mind that the intended application is randomly generating small finite groups) Are there any conditions that will help me generate a finite group with high probability? (I'm being deliberately vague about the distribution that the relations are drawn from so as not to prematurely rule out a potential result)","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
