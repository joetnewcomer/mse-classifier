question_id,title,body,tags
3413946,countability of the subset of cartesian product of integers [duplicate],"This question already has answers here : Is the set of sequences of natural numbers with trailing zeros countable? (2 answers) Closed 4 years ago . Let us consider the set $X$ of the following infinite dimensional vectors: $$
x = (x_{1}, x_{2}, \dots, x_{k}, 0, \dots,), 
$$ such that $x_{i} \in \mathbb{Z}$ and for any $x$ there exist $k < \infty$ such that $x_{j} = 0$ for all $j > k$ . The statement: $X$ is not countable. Try:
I tried to show this using the fact that the set of all subsets of all integers is uncountable",['elementary-set-theory']
3413970,Are there good lower bounds for the partial sums of the series $\sum 1/\log(n)$?,"Consider the partial sums $$S_n = \sum_{k=2}^n\frac{1}{\log(k)}.$$ Are there good lower bounds for $S_n$ as $n\to\infty$ ? I am not necessarily looking for sharp bounds (although they would be nice), just one that is fair. Of course a ""good"" lower bound would be one that go ""very"" fast to infinity.","['real-numbers', 'logarithms', 'analysis', 'real-analysis', 'sequences-and-series']"
3413973,"Let $H$ be a group, if its abelianization is $\mathbb{Z}_2$ does this mean that $H$ has torsion?","As is mentioned in the title, I have some group $H$ and I know that its abelianization is $\mathbb{Z}_2$ . Does this imply that $H$ has torsion? Edit: Since people want more context, here's some context. Basically I'm looking at the fundamental group of the Klein bottle and I want to show that it can't split as $\pi_1(K) \cong \mathbb{Z} \oplus H$ for any group $H$ . I know if I abelianize then I end up getting $\mathbb{Z} \oplus \mathbb{Z}_2$ . So if the abelianization splits over direct sums (which I'm not sure about) then if somehow I could say that $H$ must be going to $\mathbb{Z}_2$ through this abelianization map and $H$ had to have torsion I'd have a contradiction to the klein bottle being a manifold and thus having torsion-free fundamental group. I think there are a number of holes in this argument though, but I'm still interested in the particular question I asked above.","['group-theory', 'torsion-groups']"
3414016,Interesting tetrahedron problem with right dihedral angles,"A tetrahedron WYXZ, which all sides are acute triangles, has right dihedral angles at WY and XZ. Is there a way to prove that the orthocenters of all faces are on one plane? The way I tried to solve it was by connecting pairs of orthocenters on opposite sides with lines and proving that they intersect each other but I was unsuccessful.","['trigonometry', 'proof-writing', 'geometry', '3d']"
3414028,Proving a statement about determinants [duplicate],"This question already has an answer here : If $A^2=-I$, Prove that $\det{A}=1$ (1 answer) Closed 4 years ago . Given an $n \times n$ matrix $A$ with real entries such that $A^2 = -I$ , prove that $\det(A) = 1$ . This question is multi-part, but I happen to be stuck on this one. The previous parts showed: $A$ is nonsingular, $n$ is even, and $A$ has no real eigenvalues. I know that $\det(A)^2 = 1$ since $A$ has real entries and $n$ is even, but am not sure how to show that $\det(A)$ , which can be either $1$ or $-1$ , is not $-1$ . Does anyone know how to continue from here?","['determinant', 'linear-algebra']"
3414072,Studying the Continuity of a function,"Let $U \subset \mathbb{R}^N$ be an open set, let $f : U \times [a, b] \to \mathbb{R}$ be a continuous function. Consider the function $$g(x):= \int_a^b f(x,y) \,dy$$ with $x \in U$ . i) Prove that $g$ is continuous in $U$ . ii) consider the function $f : [−1, 1] \times  [−1, 1] \to \mathbb{R}$ defined by $$f (x, y) :=\begin{cases} \frac {|y|−|x|} {y^2}&\text{ if $|x|<|y|$}\\
0 &\text{ if $|x|\geq |y|$}
\end{cases}$$ Let $g(y):= \int_{-1}^1 f(x,y) \,dx$ for $y \in [-1,1]$ Study the continuity of $g$ . So I am a bit stuck on how to prove the continuity from the basics: i know how to prove that if $f$ is continuous on $[a,b]$ , then $g=\int f$ is continuous on $[a,b]$ but i assume because of the different notation and dimension here, i have to prove a different way? In addition, how would i study the continuity?","['integration', 'multivariable-calculus', 'analysis', 'real-analysis']"
3414083,On the $w^{\ast}$ Convergence of a Sequence of Measures,"Assume that $(\nu_{j})$ is a sequence of positive Radon measures supported in a compact set $K\subseteq\mathbb{R}^{n}$ and \begin{align*}
\sup_{x\in{\mathbb{R}}^{n},r>0}\dfrac{1}{r^{n-\lambda}}\int_{B(x,r)}|M_{\alpha}\nu_{j}(y)|^{p}dy\rightarrow 0,
\end{align*} where $0<\lambda,\alpha<n$ , and \begin{align*}
M_{\alpha}\nu_{j}(y)=\sup_{s>0}\dfrac{1}{s^{n-\alpha}}\nu_{j}(B(y,s)).
\end{align*} The author claims that there is a subsequence of $(\nu_{j})$ that $w^{\ast}$ converges to a measure $\nu$ which supported in $K$ . I fail to see why it must be the case. Apparently the author resorts to the Banach-Alaoglu Theorem of the sequential $w^{\ast}$ compact property regarding to the space of all positive Radon measures which supported in $K$ . Therefore we are to bound $(\nu_{j}(K))$ . But how does one bound the quantity $\nu_{j}(K)$ in terms of the integral with respect to the fractional maximal function?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
3414099,If $G$ is an inﬁnite simple group then any proper subgroup of $G$ has inﬁnite index.,"If $G$ is an inﬁnite simple group then any proper subgroup of $G$ has inﬁnite index. This question's hint is use the $n!$ -theorem but i dont understand how i use it for answer. $n!$ -theorem: Let $G$ be a group and $H$ be a subgroup of $G$ of finite index, say $|G:H|=n$ . Then there is a normal subgroup N of $G$ such that $N\subseteq H$ and $G/N$ is isomorphic to a subgroup of $S_n$ and so $|G/N|$ divides $n!$ . Indeed, ${\rm core}_G(H)$ is such a normal subgroup of $G$ .","['group-actions', 'group-theory', 'abstract-algebra']"
3414100,Prove that if $0<a<\Pr(A)$ then there exists $B\subset A$ such that $\Pr(B)=a$,"Suppose $\Pr$ is non-atomic that is $\Pr(A)$ implies the existence of $B$ with $B\subset A$ such that $0<\Pr(B)<\Pr(A) $ and $A\in\mathcal{F}$ with $\Pr(A)>0$ . (a)show that for every $\epsilon>0$ , we have $B\subset A$ such that $0<\Pr(B)<\epsilon$ . (b)Prove that if $0<a<\Pr(A)$ then there exists $B\subset A$ such that $\Pr(B)=a$ . Here is a hint:
fix $\epsilon_n \to 0$ and define inductively numbers $x_n$ and sets $G\in \mathcal{F}$ with $H_0=\emptyset$ , $H_n=\cup_{k<n} G_k$ , $x_n=\sup\{\Pr(G): G\subset A\setminus H_n, \Pr(H_n\cup G)\leq a\}$ and $G_n\subset A\setminus H_n$ such that $\Pr(H_n\cup G_n)\leq a$ and $\Pr(G_n)\geq (1-\epsilon_n)x_n$ . Let $B=\cup_{k} G_k$ .","['measure-theory', 'probability', 'real-analysis']"
3414140,commuting the covariant derivative of a function,"Let $f$ be a smooth function such on a compact kahler manifold $(M, w)$ , and the component of $w$ is denoted by $g_{ij}$ , assume there is a constant $s$ such that $sf = -g^{ij}\sqrt{-1}\partial_{j}\bar\partial_{i}f$ , we have: $s \nabla_j f   = -\nabla_j \nabla_p \nabla_{\bar p} f$ suppresing the mertic. Then how do you prove the following Bochner-Weitzenbock formula: $(\int_{U}-\nabla_j \nabla_p \nabla_{\bar p} f \nabla_\bar j f)w^n = (\int_{U} -\nabla_{\bar p} \nabla_p \nabla_{ j} f \nabla_{\bar j} f + R^{q \bar j} \nabla_{q} f \nabla_{\bar j}f )w^n$ Thoughts: the left hand side is simply $\partial_j sf = \partial_j-g^{ik}\sqrt{-1}\partial_{j}\bar\partial_{k}f = - \partial_j g^{ik} \sqrt{-1}\partial_{j}\bar\partial_{k}f -\partial_j\sqrt{-1}\partial_{j}\bar\partial_{k}f g^{ik}$ How do you proceed? Update:
My definition for the Ricci curvature is the following: Lowering the index of the curvature tensor we have $R_{i \bar j k \bar l} = -\partial_{k} \partial_{l} g_{i \bar j} + g^{p \bar q}(\partial_k g_{i \bar q})(\partial_{\bar l} g_{p \bar j})$ , then let $R_{i\bar j} = g^{k \bar l } R_{i \bar j k \bar l}$ . It is not obvious how to go from this definition.","['riemannian-geometry', 'complex-geometry', 'real-analysis', 'partial-differential-equations', 'differential-geometry']"
3414219,Why the real and imaginary parts of a complex analytic function are not independent?,"I have trouble understanding a whole array of things in complex analysis, which I have basically tracked to the statement ""real and imaginary parts of a complex analytic function are not independent."" Because of that, I don't really understand the Cauchy-Riemann equations, the fact that for an analytic function, if its real part is constant, then the whole function is constant, and other fundamental things, such as Cauchy's Integral formula, Maximum modulus principle, etc. (the last two just make zero sense to me.) The thing is, I pretty much understand the proofs, starting from the beginning, when we define differentiability of a complex function. I don't have any problems with the introduction of complex numbers as well, and different identities. But I just don't have any intuition for why things are like that, and it's very frustrating, because I always feel like I don't understand complex numbers at all, and just do some standard exercises in class, relying on proven facts that I just assume to be true as a starting point. But as soon as I go and try to understand the meaning of things we in the class work with, I just immediately stop understanding anything. Can anyone help me understand why the real and imaginary parts of a complex function are not independent?",['complex-analysis']
3414221,How does the curvature change when a surface is translated by its normal,"This is not a homework problem, but a problem from an old exam, published online, which I am trying to solve. Consider a regular parameterized surface $X:U\to\mathcal{R}^3$ with Gauss map $N:X(U) \to S^2$ and principal curvatures $\kappa_1 = 1/r_1$ and $\kappa_2 = 1/r_2$ . Let $r\in \mathcal{R}$ and define a regular parametrized surface $X^r:U\to\mathcal{R}^3$ by $$X^r(u,v) = X(u,v)+rN(u,v).$$ Show that the principal curvatures of $X^r$ are $k_1(r)=1/(r_1-r)$ and $k_2(r)=1/(r_2-r)$ , respectively. In principle what we need to do is to determine the second and first fundamental form of $X^r$ in terms of the corresponding forms of $X$ . We could then calculate the Gaussian curvature $K$ , the mean curvature $K$ , and calculate the principal curvatures $\kappa = H \pm \sqrt{H^2-K}$ . However, the calculations for this approach seem to get very messy, so I suspect it should be done differently. Any suggestions? Using $\langle X_u,N \rangle = \langle X_v,N \rangle = 0$ , one can easily see that $$X^r_u \times X^r_v = X_u \times X_v + r^2N_u \times N_v.$$ If we choose the $+$ sign in calculating the normal from the cross product of the tangent basis vectors, and using the fact that $N_u \times N_v = -N_{uv} \times N \perp X_u \times X_v$ , we get the Gauss map $$N^r = \frac{|X_u\times X_v|N+r^2N_u\times N_v}{\sqrt{|X_u\times X_v|^2 + r^4|N_u\times N_v|^2 }}.$$ From this, I see no reasonably simple way to get at the second fundamental form of $X^r$ in terms of the second (and first) fundamental form of $X$ .","['multivariable-calculus', 'surfaces', 'differential-geometry']"
3414256,Identification of the infinitely dimensional unit sphere,"I have some doubts about a statement from class: Let $\mathbb{R}^\omega$ be the countable product of $\mathbb{R}$ . Equip $\mathbb{R}^\omega$ with the scalar product $\sum_{k = 1}^\infty x_i y_i$ and the induced norm. We identify $\mathbb{R}^n$ with the space generated by the first $n$ coordinates. Let $S^\infty = \bigcup_n S^n \subset \mathbb{R}^\omega$ ( $S^n$ being the unit sphere in $\mathbb{R}^n$ ) and let $B^\infty = \{(x_k)_{k \in \mathbb{N}} \mid \sum_{k = 1}^\infty x_k^2 = 1\}$ . Show that we can identify $S^\infty$ with $B^\infty$ . What bothers me about this question is that clearly $S^\infty \subset B^\infty$ . However, I could take the sequence $(0, \frac{\sqrt{6}}{\pi} \frac{1}{1},  \frac{\sqrt{6}}{\pi}  \frac{1}{2}, \frac{\sqrt{6}}{\pi}  \frac{1}{3} , \dotsc)$ . This sequence has norm $1$ under the assumptions in the statement. But the sequence also has infinitely many non-zero elements. So how can it be in on of the $S^\infty$ ?",['general-topology']
3414265,Why is $e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x}$? [duplicate],"This question already has answers here : If $e=\lim\limits_{x\to \infty} (1+x^{-1})^x$, why doesn't $e=1$? (9 answers) Closed 4 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved The definition of the number $e$ that's used in my textbook is $e = \lim\limits_{x\to \infty} (1+\frac {1}{x})^x$ which relates to compound interest. But when trying to calculate the derivative of $e^x$ , I encountered another definition of $e$ : $$e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x}$$ Now, if it were $x$ approached $0$ from the positive direction, it would've made sense to me since $\frac {1}{x}$ would approach infinity and $1+x$ would approach $1$ with the same ""speed""(sorry I don't know the term here) as the first definition. But it's approaching $0$ from both sides. My question is, why are these two equal? EDIT The reason my question isn't a duplicate of the suggested one is that mine isn't really about the limit $\lim\limits_{x\to \infty} (1+\frac {1}{x})^x$ . It's about its alternative form. I totally understand how the former isn't equal to $1$ . Thank you so much in advance! P.S: I graphed the function $(1+x)^\frac {1}{x}$ and can ""see"" the limit exists as $x$ approaches $0$ but I would like a non-visual proof.","['limits', 'intuition']"
3414266,Is the join of two solvable subgroups solvable?,"It is an exercise in Rotman's ""introduction to the theory of groups"" to show that if $S \trianglelefteq G$ and $T \leq G$ are solvable, then so is $ST$ . I know $ST = S \vee T$ in the lattice of subgroups when $S$ is normal, so I'm curious if $S \vee T$ is solvable more generally (assuming, of course, both $S$ and $T$ are). My proof relied somewhat heavily on the normalcy of $S$ , so I'm not sure how to adapt my proof to this more general setting, and a few attempts haven't led to anything. Thanks in advance! ^_^","['group-theory', 'abstract-algebra', 'solvable-groups']"
3414280,Inverse Laplace transform from a power series with finite radius of convergence,"Suppose we are given some function $f(t)$ define on $t\in [0,\infty)$ .   Then, the Laplace of this function is given by \begin{align}
F(s)= \int_0^\infty e^{-st} f(t) dt
\end{align} where $s=\sigma+i\omega$ , and the inverst Laplace transform is given by \begin{align}
L^{-1}[F(s)](t)=\frac{1}{ 2 \pi i} \lim_{T \to \infty} \oint_{c-iT}^{c+iT} e^{ts} F(s) ds,
\end{align} where $c$ is some real constant in the region of convergnce of $F(s)$ . My question Suppose that we are given a Laplace transform in the form of a power series only: \begin{align}
F(s)=\sum_{n=0}^\infty a_n s^n
\end{align} where the radius of convergence is given by $|s|<r$ . How can we invert $F(s)$ from this the power series representation?   The issue I have here is that contour integration needs to be from $c-iT$ to $c+iT$ but the radius of convergence is finite. Can the inversion be done here in some way? Edit: Here is a concrete example. Consider \begin{align}
F(s)=\sum_{n=0}^\infty a_n s^n
\end{align} where $a_n=2^{-n-2} (i(1+i)^{n+1}+(i-1)^n) $ .  This power series has a radis of convergence $r=\sqrt{2}$ . In fact, the above power series corepsonds to a function \begin{align}
F(s)= \frac{1}{1+(1+s)^2},
\end{align} which has an inverse trasform given by \begin{align}
f(t)= e^{-t} \sin(t) u(t)
\end{align} where $u(t)$ is the step function. However, if we don't know what the actual function is, how would we use the power series representation to find the inverse.","['complex-analysis', 'power-series', 'laplace-transform']"
3414299,"Does $dx/dt=\{1$ if $x\notin \mathbb Q$, $0$ otherwise$\}$ have solutions?","Consider the following system: \begin{align*} 
\dot{x} & = f(x)\\
x(0) & = x_0,
\end{align*} where $f(x) = \begin{cases}
1 & \text{if } x \notin \Bbb Q \\
0 & \text{if } x \in \Bbb Q \end{cases}$ . Do there exist solutions? I think the following is a solution: $$ x(t) = x_0 + t, $$ because it satisfies the Lebesgue integral equation $$ x(t) = x_0 + \int_0^t f(x(\tau))\, d\mu $$ In particular, $$ \begin{aligned}
x_0 + \int_0^t f(x(\tau)) \,d\mu
& = x_0 + \int_{[0,t]\cap\Bbb Q} f(x(\tau))\, d\mu + \int_{[0,t]\setminus\Bbb Q} f(x(\tau))\, d\mu \\
& = x_0 + \int_{[0,t]\setminus\Bbb Q} d\mu \\
& = x_0 + t;
\end{aligned} $$ This is because $\Bbb Q$ has measure zero... right?","['measure-theory', 'ordinary-differential-equations']"
3414349,Uniform convergence preserves continuity Proof verification,"Proposition: Let $f_n \rightrightarrows f $ and each $f_n$ is continuous at $x_0$ , then $f$ is continuous at $x_0$ I know there are some posts that have proof of this proposition, but I didn't find any that proves it with the sequential definition of continuity, instead, they use the $\delta - \epsilon$ definition. When I saw this problem, I thought it'd be more natural to prove it with the sequential definition of continuity because we are working with sequences. However, I'm not sure if my proof is correct. So here it is. First I want to justify this inequality (triangle inequality with 2 points?) $$|a - b| = |a+c-c+d-d-b| = |(a-d) + (d-c) + (c-b)| \leq |a-d| + |d-c| + |c-b| \tag{1}$$ So, suppose that $f_n: I \to \mathbb{R} \text{ where } I \subseteq \mathbb{R}$ , let $(x_n)$ be a sequence in $I$ such that $x_n \to x_0\text{ , } x_0 \in I$ . Since each $f_n$ is continuous,  we have $f_n(x_n) \to f_n(x_0)$ . So there exists an $N_1$ such that $$ \forall q \geq N_1 \rightarrow |f_n(x_q) - f_n(x_0)| < \epsilon /3$$ Since $f_n \rightrightarrows f$ there exists $N_2$ such that $$\forall n \geq N_2 \rightarrow |f_n(x_q) - f(x_q)| < \epsilon /3 \text{ and } |f_n(x_0) - f(x_0)| < \epsilon /3 \tag{2}$$ Let $N = max(N_1, N_2) $ and in the inequiality $(1)$ , let $a = f(x_q)$ , $b = f(x_0)$ , $c = f_n(x_0)$ and $d = f_n(x_q)$ so $\forall q,n \geq N$ we have $$|f(x_q) - f(x_0)| \leq |f_n(x_q) - f(x_q)| + |f_n(x_q) - f_n(x_0)| + |f_n(x_0) - f(x_0)| < \epsilon $$ (I also used $|a-b| = |b-a|$ ) Thus $f(x_q) \to f(x_0)$ and $f$ is continuous at $x_0$ . Is it correct? I don't feel so sure about it. Also. If the sequence didn't converge uniformly, but only point-wisely, in $(2)$ , there'd be different $N_2, N_3$ that would guarantee those inequalities, so If I chose an $N$ to be equal to the max of those two, I could guarantee those inequalities without the hypothesis of uniform continuity, and I could ""prove"" that point-wise convergence preserves continuity which is false, this bothers me a lot. If the proof is wrong(I think it's wrong), how can I fix it? Thanks for taking the time to read it.","['proof-verification', 'analysis', 'calculus', 'uniform-convergence', 'sequences-and-series']"
3414426,"Spivak, Calculus on Manifolds, Problem 2-37 (b)","2-37 (a) Let $f:\mathbb{R^2}\to\mathbb{R}$ be a continuously differentiable function. Show that $f$ is $not\mbox{ 1-1}$ . $Hint:$ If, for example, $D_1f(x,y)\neq0$ for all $(x,y)$ in some open set $A$ , then consider $g:A\to\mathbb{R^2}$ defined by $g(x,y) = (f(x,y),y)$ . (b) Generalize this result to the case of a continuously differentiable function $f:\mathbb{R^n}\to\mathbb{R^m}$ with $m<n$ . I want to solve (b) analogously to (a), so here I first present my solution to (a). If $D_1f(x,y) = 0$ for any $(x,y)\in\mathbb{R^2}$ , then $f(x_1, y) = f(x_2,y)$ for any $x_1, x_2\in\mathbb{R}$ ; hence, $f$ is not $\mbox{1-1}$ . Otherwise, there is $(x,y)$ such that $D_1f(x,y)\neq 0$ . Then $g:\mathbb{R^2}\to\mathbb{R^2}$ defined as in the hint is continuously differentiable and $$\det g'(x,y) = \det\left(\begin{smallmatrix}D_1f(x,y)&D_2f(x,y)\\0&1\end{smallmatrix}\right) = D_1f(x,y)\neq0,$$ so it satisfies the conditions for the inverse function theorem. Thus, there are open $V\ni (x,y)$ and $W\ni g(x,y)$ such that $g:V\to W$ is $\mbox{1-1}$ .  For some unequal $(u,y_1),(u,y_2)\in W$ we get unequal $(x_1,y_1),(x_2,y_2)\in V$ such that $f(x_1,y_1) = f(x_2,y_2) = u$ ; hence, $f$ is not $\mbox{1-1}$ . I have three ideas on how to interpret the $D_1f$ from (a) in (b): as $D_1f^1$ , as $D_1f$ , as $\det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right)$ ; but neither of them seem to work. By assuming $D_1f^1(x)\neq 0$ for some $x$ and defining $g:\mathbb{R^n\to\mathbb{R^n}}$ by $g(x^1,...,x^n) = (f^1(x), x^2,...,x^n)$ I can prove that $f^1$ is not $\mbox{1-1}$ . Similarily, any $f^i$ is not $\mbox{1-1}$ , but it doesn't show that $f$ is not $\mbox{1-1}$ . By  assuming $D_1f(x)\neq 0$ for some $x$ and defining $g:\mathbb{R^n\to\mathbb{R^n}}$ by $g(x^1,...,x^n) = (f(x), x^{m+1},...,x^n)$ I get $$\det g'(x) = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_nf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_nf^m(x)\\O_{m\times m}&&E_{(n-m)\times (n-m)}  \end{smallmatrix}\right)  = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right);$$ but I cannot show that it's not equal to $0$ at some point $x$ to use the theorem. I want to define $g$ as in 2. but I cannot show why $f$ is not $\mbox{1-1}$ , when $$\det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right) = 0$$ for all $x\in\mathbb{R^n}$ .
The third one seems the most promising to me but I don't how to proceed. Do you have any tips?","['multivariable-calculus', 'inverse-function-theorem']"
3414428,Closed form of finite sum involving the floor function,"The sum $$
\sum_{k=1}^{p^x-1} \left\lfloor \frac{kx}{p^x} \right\rfloor
$$ has stumped me. I don't even know where I would begin. I suppose that starting with the abstraction $$
\sum_{k=1}^{f(x)-1} \left\lfloor \frac{kx}{f(x)} \right\rfloor
$$ may help, but I still don't know where to begin. I've looked at similar questions , but I still have had no luck solving this problem.","['summation', 'ceiling-and-floor-functions', 'discrete-mathematics', 'sequences-and-series']"
3414435,"If a piece wise function is not differentiable at a point, is it still possible to have a derivative of the piece wise function?","For problems 2 and 3 attached, I have found that for problem 2 that the piecewise function is not differentiable at x=3, but I was wondering if there could still be a derivative that can be written. I thought that there is no derivative, but now I am looking back at this problem and I am quite unsure. Also for problem 3, I found that the piecewise function is not continuous at t=1. Therefore, I concluded that the function is not differentiable at t=1 and therefore there is no derivative for the piecewise function. Am I wrong here?","['calculus', 'derivatives']"
3414453,Solutions for PDE $(1+u_x^2)v_{yy} -2u_xu_yv_{xy} + (1+u_y^2)v_{yy}$,"I am reading a proof on Bernstein's Theorem on Minimal Surfaces. On this proof it is claimed that if $u: \mathbb{R^2} \rightarrow \mathbb{R}$ sastisfies the minimal surface equation, ie: $$
(1+u_x^2)u_{yy} -2u_xu_yu_{xy} + (1+u_y^2)u_{xx} = 0
$$ Then both the functions $\psi_1 = \arctan(u_x)$ and $\psi_2 = \arctan(u_y)$ are solutions of the equation $$(1+u_x^2)v_{yy} -2u_xu_yv_{xy} + (1+u_y^2)v_{xx} = 0$$ Since the above equation is in someway ""symmetric"" (changing $x$ 's into $y$ 's and vice versa does not alter the equation) then I believe it is easily seen that if $\psi_1$ satisfies the equation then $\psi_2$ must also do so. What I've done so far: We have the following identities: $$
(\psi_1)_{xx} = \frac{u_{xxx}(1+u_x^2) - 2u_xu_{xx}^2}{(1+u_x^2)^2}
$$ $$
(\psi_1)_{xy} = \frac{u_{xxy}(1+u_x^2) - 2u_xu_{xy}u_{xx}}{(1+u_x^2)^2}
$$ $$
(\psi_1)_{yy} = \frac{u_{xyy}(1+u_x^2) - 2u_xu_{xy}^2}{(1+u_x^2)^2}
$$ Which when we substitute correspondingly on the $v$ 's in the equation and after using the hypothesis of the minimal surface equation we are left with $$
\frac{u_{xxx}(1+u_y^2) - 2u_xu_yu_{xxy}+u_{xyy}(1+u_x^2)+2u_{x}(u_{xx}u_{yy}-u_{xy}^2)}{(1+u_x^2)}
$$ Which I cannot then, prove is equal to $0$ .","['partial-differential-equations', 'differential-geometry']"
3414519,Does $m^{***}(A)=m^*(A)$ imply that $A$ is measurable?,"I am trying to solve an exercise from Royden's Real Analysis, 4th ed. For any set $A \subseteq \mathbb{R}$ , define $m^{***}(A)=sup\{m^*(F):F\subseteq A$ is a closed set $\}$ . How is this set function $m^{***}$ related to outer measure $m^*$ ? I am aware that this exercise Relation between $m^{***}$ with $m^*$? has been discussed before on SE, yet I do not find it satisfactory because I am not assuming per se that $A$ is measurable. Relevant definitions and theorems Keep in mind that Royden defines outer measure $m^*(A)=inf\{\sum _{i=1}^ \infty l(I_i):$ $I_1,I_2,...$ is a sequence of nonempty, bounded, open intervals whose union covers $A$ }. This is defined regardless of whether $A$ is measurable or not. It has also been proven that we have five equivalent definitions of measurability: The following are equivalent: $\forall A \subseteq \mathbb{R}[m^*(A)=m^*(A\cap E)+m^*(A \cap E^c)]$ $\forall \epsilon >0\exists$ open set $O \supseteq E$ [ $m^*(O-E)< \epsilon]$ $\exists G_δ$ set $G \supseteq E[m^*(G-E)=0]$ $\forall \epsilon >0\exists$ closed set $F \subseteq E$ [ $m^*(E-F)< \epsilon]$ $\exists F_σ$ set $F \subseteq E[m^*(E-F)=0]$ Now we tackle the actual problem... My progress so far We always have $m^{***}(A) \leq m^*(A)$ obviously. I have formulated the following conjecture: $m^{***}(A)=m^*(A)$ if and only if $A$ is measurable. I have shown that $A$ measurable implies $m^{***}(A)=m^*(A)$ . For the other direction, I have managed to show that $m^{***}(A)=m^*(A)$ implies $A$ is measurable in the special case that $m^*(A)< \infty$ . But if $m^*(A)= \infty$ , I don't know how to show it. I was able to prove the finite case by finding some $F_δ$ set $F\subseteq A$ having the same measure as $A$ , observing that $m^*(A)=m^*(A-F)+m^*(F)=m^*(A-F)+m^*(A)$ , and subtracting $m^*(A)$ from each side to obtain $m^*(A-F)=0$ (meaning that $A$ is meaurable). If $m^*(A)= \infty$ , I don't know what to since I obviously can't subtract $\infty$ from both sides.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3414567,Prove that every group of order $4$ is abelian as follows.,"Can you please check my logic and maybe suggest some other strategies for the following problem: Prove that every group of order $4$ is abelian as follows: Let $G$ be any group of order $ 4$ , i.e., $|G| = 4$ . (1) Suppose there exists $a \in G$ such that $o(a) = 4$ . Prove that $G$ is abelian. (2) Suppose that no element of $G$ has order 4. Prove that $\forall x\in G$ , $x^2 = 1$ . (3) Suppose that no element of $G$ has order 4. Prove
  that $G$ is abelian. What I got so far: (1) If there exists $a \in G$ such that $o(a) = 4$ , Case 1: $a\cdot a=b$ . Then $a\cdot a\cdot a=c$ and $a\cdot a\cdot a\cdot a=1$ . Algebra... G is abelian. Case 2: $a\cdot a=c$ ... $G$ is abelian. (2) Let $x\in G$ . If $o(x) \neq 4$ , we can clarify that an element cannot have an order greater than $4$ in a group of order $4$ and that the only element that has an order of $1$ is $1$ . Therefore the other three elements must have an order of $2$ , so $x^2=2$ for all $x \in G$ . (3) No ideas yet :(","['group-theory', 'abstract-algebra', 'abelian-groups']"
3414579,On the orbits of an action of the subgroup of rational points,"Let $k$ be a field, $G$ an affine algebraic group over $k$ and $V$ an irreducible affine variety over $k$ . Suppose that $G$ acts $k-$ morphically on $V$ and that the action is given by $(g,x) \mapsto g \cdot x$ where $g \in G, x \in V$ . In particular, the group $G(k)$ of rational $k-$ points acts on the set $V(k)$ of $k-$ rational points of $V$ . I am wondering if the following assertion holds: $$G(k) \cdot x = (G \cdot x) \cap V(k) \;\; \text{for each} \; x \in V(k).$$ That is to say, the $G(k)-$ orbit of a $k-$ rational point of $V$ consists of the $k-$ rational points in its $G-$ orbit. Remarks. (i) The inclusion $G(k) \cdot x \subseteq (G \cdot x) \cap V(k)$ should be clear. (ii) If $V(k)= \emptyset$ then the statement is vacuously true, so we may assume that $V(k) \neq \emptyset$ . (iii) The above statement holds in the following example: $G=\textbf{G}_a=\overline{k}^{\;+}$ acting by translations on $V=\mathbb{A}_{\; \overline{k}}^n$ (in this case $G(k)=k^+$ acts by translations on $V(k)=\mathbb{A}_k^n$ ). Actually, I am satisfied with a positive answer in the case where $V=\mathbb{A}^n_{\;\overline{k}}$ is the $n-$ dimensional affine space over $\overline{k}$ (the algebraic closure of $k$ ). Thank you in advance.","['group-theory', 'algebraic-geometry', 'algebraic-groups']"
3414616,Characterization of Uniform Continuity,"If $f:X \to Y$ is uniformly continuous, then there exists $\omega : (0, +\infty) \to (0, +\infty)$ with $\lim \limits_{t \to 0^+} \omega(t) = 0$ such that $d_Y(f(x), f(x')) < \omega(d_X(x, x'))$ . I am trying to come up with such an $\omega$ function. \begin{equation*}
    \omega(t) =t +  \sup \limits_{x \in X, x' \in X} \{d_Y(f(x), f(x')) : d_X(x, x') < t \}
\end{equation*} I don't know if it makes sense. There will probably be some issues with $\sup$ . Can anyone suggest improvements/corrections? Thanks.","['limits', 'uniform-continuity', 'real-analysis']"
3414649,Character table of direct group product,"Consider a finite group $G$ and assume it decomposes as $$G\cong\displaystyle\bigoplus_{k=1}^n G_k.$$ Say that I know the character table for all of $G_k$ . Can I construct the character table for $G$ ? We consider complex irreducible representations, i.e. group homomorphisms $\pi : G\to\operatorname{End}(\mathbb C^n).$ In particular, the case $n=2$ is of interest, since I have a situation like this.","['representation-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3414750,Find eigenvalues of $x^2 \frac{d^2\phi}{dx^2} + x \frac{d\phi}{dx} + \lambda \phi = 0$ with boundary conditions $\phi(1) = \phi(b) = 0$,"Since this is an equidimensional equation, determine all positive eigenvalues. \begin{align*}
  x^2 \frac{d^2\phi}{dx^2} + x \frac{d\phi}{dx} + \lambda \phi &= 0 \\
  \phi(1) &= 0 \\
  \phi(b) &= 0 \\
\end{align*} The given answer that I need to find the solution for is for $n = 1,2,\ldots$ : \begin{align*}
  \lambda_n &= \left( \frac{n \pi}{\ln b} \right)^2 \\
\end{align*} If we multiply by $1/x$ and rearrange we can get this in Sturm-Liouville form with $p(x) = x, q(x) = 0, \sigma(x) = 1/x$ : \begin{align*}
  x \frac{d^2\phi}{dx^2} + \frac{d\phi}{dx} + \frac{1}{x} \lambda \phi &= 0 \\
  \frac{d}{dx} \left( x \frac{d\phi}{dx} \right) + \lambda \frac{1}{x} \phi &= 0 \\  
\end{align*} If we look at the Rayleigh quotient we can see there are no negative eigenvalues and we can manually verify that zero is not an eigenvalue. Assume a trial solution of $\phi(x) = x^m, \phi'(x) = m x^{m-1}, \phi''(x) = m(m-1)x^{m-2}$ such that: \begin{align*}
  x^2 \frac{d^2\phi}{dx^2} + x \frac{d\phi}{dx} + \lambda \phi &= 0 \\
  m (m-1) x^m + m x^m + \lambda x^m &= 0 \\
  \left( m^2 + \lambda \right) x^m &= 0 \\
  \lambda &= -m^2 \\
\end{align*} Since we know that $\lambda > 0$ , then $m^2 < 0$ , which means that $m = i r$ where $r \in \mathbb{R}$ . However, this can't satisfy the boundary conditions: \begin{align*}
  \phi(x) &= e^{ir} \\
  \phi(1) = 0 &\neq 1^{ir} = 1 \\
\end{align*} I'm stuck on what else to try.","['sturm-liouville', 'ordinary-differential-equations']"
3414789,"Suppose $f(x) > 0, f'(x) > 0,$ and $f''(x) >0.$ Then $\lim_{x \to \infty} f(x) = \infty.$","Intuitively I see why this is true, in terms of a graph, but I can't seem to get a handle on a proof. I know that I need to show for every $M > 0$ there's a $d > 0$ such that whenever $x > d$ , $f(x) > M.$ I attempted proceeding by contradiction, but wasn't able to make much progress. I think I'm missing something.","['derivatives', 'real-analysis']"
3414800,integral of delta function of x^2,"The name says what I need to calculate. When trying to integrate I stumble upon interpretation problem $$
\int\limits_{-\infty}^{+\infty} \delta(x^2) dx = \{y=x^2\} = 2\int\limits_{0}^{+\infty} \delta(x^2) \frac{dx}{dy}dy = \int\limits_{0}^{+\infty} \delta(y)\frac{dy}{\sqrt{y}}
$$ My questions are: 1) How should I interpret the result when zero of delta falls on the limit of integration? 2) If I to ignore reasoning of (1) the answer seems to be $\infty$ . Is this true?","['integration', 'definite-integrals', 'dirac-delta']"
3414807,Finding the upper bound of a complex contour integral,"I am trying to show that $$
\left\lvert\int_\gamma \frac{\cos(z)}z \,dz\right\rvert \le 2\pi e
$$ if $\gamma$ is a path that traces the unit circle once. The book recommends that I show that $\lvert \cos(z) \rvert \le e$ if $\lvert z \rvert = 1$ to help prove this. I know that if $\lvert f(z) \rvert \le M$ for all $z \in \gamma (I)$ then $$
\left\lvert \int_\gamma f(z) \,dz \right\rvert \le M\ell (\gamma),
$$ where $\ell (\gamma)$ is the length of the path, which in this case is $2\pi$ . So I can see why I would need to show $\lvert cos(z) \rvert \le e$ if $\lvert z \rvert = 1$ to prove the inequality, but I am not sure where to go from here to show that.",['complex-analysis']
3414809,Topological nature of IEEE floating-point numbers,"If IEEE floating-point numbers had countably infinite precisions, its domain would be: $$
\{-\infty\}\cup\mathbb{R}^-\cup\{-0,+0\}\cup\mathbb{R}^+\cup\{+\infty\}\cup\{\text{NaN}\}
$$ Let's denote this $\mathbb{FP}$ and give $\mathbb{FP}^2$ the product topology. The addition is defined as: $+0 + \pm0 = +0$ $-0 + \pm0 = \pm0$ (double sign in same order) $x + (-x) = +0$ for $x \in \mathbb{R}^+$ $\pm\infty + x = \pm\infty$ for $x \in \mathbb{R}$ (double sign in same order) $\pm\infty + \pm\infty = \pm\infty$ (double sign in same order) $\pm\infty + \mp\infty = \text{NaN}$ (double sign in same order) If either operand is NaN, the result is NaN. Addition is commutative. Otherwise, the usual addition in $\mathbb{R}$ The negation is defined as $-(\pm0) = \mp0$ (double sign in same order) $-(\pm\infty) = \mp\infty$ (double sign in same order) $-\text{NaN} = \text{NaN}$ Otherwise, the usual negation in $\mathbb{R}$ Assume both addition and negation are continuous, as long as the subspaces $\mathbb{R}^+$ and $\mathbb{R}^-$ maintain the usual topology. The main question is: Are the zeros topologically distinguishable? Note: I actually want the ""prettiest"" topology that explains the behavior of IEEE floating-point numbers. This is such a subjective notion,  yet there are something I definitely don't want: Trivial topology (a separation axiom would prevent it) Discrete topology (connectedness or compactness would prevent it) I wanted the $T_1$ axiom so badly, but I couldn't separate the zeros. That's where the above question emerged. As a consequence, I want the topology to satisfy as many space properties as possible. So there is an additional question. If the answer is ""yes"", what is the maximum number of satisfiable-at-once conditions below?: Addition is quotient. (perfectness is preferable) Negation is quotient. (perfectness is preferable) $\mathbb{FP}$ is $T_1$ . (stronger separation axioms are preferable) $\mathbb{FP}$ is metrizable. (complete matrizability is preferable) $\mathbb{FP}$ is connected. (path-connectedness is preferable) $\mathbb{FP}$ is Lindelöf. (compactness is preferable) $\mathbb{FP}$ is second-countable. (first-countablility is preferable) $\mathbb{FP}$ has a countable dense subset. I might've also concerned about other operations, such as multiplication, reciprocal, absolute value function, signum function, transcendental functions, or special functions. But that really should be a story for another day...","['topological-semigroups', 'continuity', 'general-topology', 'floating-point']"
3414842,The number of subgroups of index two in $G$,"Statement : Let $G$ be a finite group. Then $ I_2(G)=[G:G^2]-1$ . where; $I_2(G)$ is the number of subgroups of index two in $G$ , $[G,G^2]$ represents the index of $G^2$ in $G$ and $G^2= \langle \{x^2: x\in G\} \rangle$ . My work : First I proved that $G^2$ is the subgroup of $G$ generated by squares of elements in $G$ and also $G^2$ is normal in $G$ . I found that $\frac{G}{G^2}$ is abelian. Then, I took for instance, $G=D_n=\langle R,S:$ $R$ is a rotation with $o(R)=n$ and $S$ is  reflection $\rangle $ so that $G^2= \langle R^2 \rangle$ . Which tells that the above statement is true for $D_n$ . My Question: I thought a lot for the proof of this statement but I am not getting any useful tool\concept to prove this. Please provide a proof of this statement.","['group-theory', 'abstract-algebra']"
3414899,Non autonomous ODE(qualitative analysis),"I have non linear non-autonomous ODE system given by $$\tilde{d}(t)=d-\frac{A \cos(t)}{\bar{A}+A \sin(t)}$$ $$\dot{x}(t)=-yx(1-x)F(y)$$ $$\dot{y}(t)=-(1-y)(3xy(1-y^4)-\tilde{d}(t))$$ where $F(y)$ is a non linear function of $y$ that is decreasing in $y$ for most of the domain. I can solve this numerically using Matlab. Doing so, I get solutions for $x(t)$ and $y(t)$ that look periodic after some intial time span. Is there any way to make some statements about how ''average'' value of $x(t)$ changes with changing $A?$ Numerical solutions seem to suggest that it increases. $\tilde{d}(t)$ averaged over a cycle increases with A. I am very new to non-autonomous systems, so I would be very happy if one could guide me to the some concepts that may help me 'prove' this statement more formally.","['nonlinear-system', 'ordinary-differential-equations']"
3414942,Have I missed some information in this exercise?,"I'm doing an exercise about functional analysis: We consider $E$ an Euclidean space such that $\operatorname{dim}(E)<+\infty$ and an application $P: E \rightarrow E$ such that a radius $\rho>0$ exists such that $\forall x \in \partial \mathcal{B}(0, \rho):\langle P(x), x\rangle \geq 0$ . Above, $\partial \mathcal{B}(0, \rho)$ refers to the sphere of radius $\rho$ . Draw an illustration in 2-D. We assume that $P$ does not vanish on $\overline{B(0, \rho)}$ and define $g$ as $$\forall x \in \overline{B(0, \rho)}: g(x)=-\frac{\rho}{\|P(x)\|} P(x)$$ Prove that $g$ is continuous. Prove that $g$ has a fixed point. Obtain a contradiction and conclude that $P$ vanishes. Explain on your picture. In question 2, we are required to prove that $g$ is continuous. I think the continuity of $g$ depends on the continuity of $P$ . I'm not sure if the fact that $P$ is called an application has some mathematical meaning in this context. Maybe I'm wrong, but I think I can not prove that $g$ is continuous with the existing information. Could you please shed me some light on this issue?","['notation', 'continuity', 'hilbert-spaces', 'functional-analysis']"
3414945,Maximal subgroups of order $p^3$ in finite simple groups,"Is there any finite simple group $G$ such that $G$ has a maximal subgroup of the form $\mathbb{Z}_{p}\ltimes\mathbb{Z}_{p^2}$ , for some prime divisor $p$ ? In case the answer is positive please guide me to find a classification of such simple groups. Otherwise please give a proof for nonexistence of such maximal subgroups.","['group-theory', 'finite-groups']"
3414980,A problem related to Relation notation.,"Consider number $2$ $and$ $3$ . There is a relation between these numbers that later one (3) is just 1 greater then first one (2). I can represent this sentence like this: $3$ R $2$ , Where R mean ""greater then"" . Now there is another notation like (3,2) ∈ R . [Here in ordered pair first element is greater then second element]. Here I having problem. In first notation R simply mean ""greater then"" but in second notation we are considering R to be a set. How  could ""greater then""  be a set? 
Pardon me if this seems stupid. But I would be grateful if you consider explaining where I am doing mistake. Thanks","['functions', 'relations']"
3414988,Integrating the Derivative of the sgn Function,"I would like to prove the following equality $\int_{\mathbb{R}^N} \partial_t |u(t,x)| \text{d}x = \int_{\mathbb{R}^N} \text{sgn}(u(t,x))\partial_t u(t,x)  \ \text{d}x $ . Where $u: (0,\infty) \times \mathbb{R}^N \rightarrow \mathbb{R}$ , is a function in $C([0,\infty); L^1(\mathbb{R}^N) \cap L^\infty(\mathbb{R}^N) ) \cap C([0,\infty); W^{2,p} (\mathbb{R}^N) ) \cap C^1((0,\infty); L^p(\mathbb{R}^N)) $ , for all $p \in [1,\infty]$ . And sgn is the sign function: $\text{sgn}(x) = \begin{cases} 1, \ x>0 \\ 0, \ x = 0 \\ -1, \ x < 0   \end{cases}$ We know that $ \partial_t |u(t,x)| = \partial_t [u(t,x)\text{sgn}(u(t,x)] = \text{sgn}(u(t,x))\partial_t u(t,x) + u(t,x) \partial_t \text{sgn}(u(t,x))$ , by the product rule. (We take derivatives here in the distribution sense.) The goal, then is to prove that $ \int_{\mathbb{R}^N} u(t,x) \partial_t \text{sgn}(u(t,x)) \text{d}x = 0$ . My question is how we can show explicitly that the above integral is $0$ . My intuition is the following: $\partial_t \text{sgn}(u(t,x)) = \partial_u \text{sgn}(u) \partial_t u $ , and we should have $\partial_u \text{sgn}(u) = 0$ almost everywhere. We only have 'jumps' for $ \text{sgn}(u) $ at $u=0$ , but since we are considering the integral of $ u(t,x) \partial_t \text{sgn}(u(t,x)) $ the extra factor of $u$ should ensure we have $ u(t,x) \partial_t \text{sgn}(u(t,x)) = 0$ , for all $t$ , and thus $ \int_{\mathbb{R}^N} u(t,x) \partial_t \text{sgn}(u(t,x)) \text{d}x = 0$ . This intuition of course isn't a formal proof. In particular, I have been told to consider the following: We know that: $\int_{\mathbb{R}} \phi(x)\partial_x \text{sgn}(x) \text{d}x = - \int_{\mathbb{R}} (\partial_x\phi(x)) \text{sgn}(x) \text{d}x$ $ = -\int_{0}^{\infty} (\partial_x\phi(x)) \text{sgn}(x) \text{d}x - \int^{0}_{-\infty} (\partial_x\phi(x)) \text{sgn}(x) \text{d}x = \phi(0) - (-\phi(0)) = 2\phi(0),$ for any differentiable function $\phi \in C(\mathbb{R}^N).$ I have been asked to show why something similar does not happen in my integral. Right away, we can see that the derivative and the integral are over different variables in my integral, but I will need a more precise answer than that... EDIT 1 It can be shown explicitly that $ \partial_t |u(t,x)| = \text{sgn}(u(t,x))\partial_t u(t,x)$ whenever $u \neq 0$ . Thus we need only show that $ \int_{A_t} u(t,x) \partial_t \text{sgn}(u(t,x)) \text{d}x = 0$ , where $A_t := \{ x \in \mathbb{R}^N \ | \ u(t,x) = 0 \} $ . EDIT 2 I have been told to consider approximating sgn with smooth functions. Let us consider a set of smooth functions $ \Psi_{\epsilon}(x) =  \displaystyle \frac{\epsilon^{-1}x}{(1+\epsilon^{-2}x^2)}  $ . Then, letting $\epsilon \in (0,1]$ , we have $ \Psi_{\epsilon}(x) \in [0,1] $ , for all $x \in \mathbb{R}$ , and we have $\lim_{\epsilon \rightarrow 0} \Psi_{\epsilon}(x) = \text{sgn}(x) $ , for all $x \in \mathbb{R}$ . It is also easy to show that $\partial_x \Psi_{\epsilon}(x) \rightarrow \partial_x \text{sgn}(x)$ , as $\epsilon \rightarrow 0$ , for all $x \neq 0$ . (i.e. almost everywhere). It seems that integrating with the derivative of $\Psi$ runs into the exact same problems as before? Does anyone know why this is helpful? EDIT 3 It can be shown, via the monotone convergence theorem, that $\int_{\mathbb{R}} \partial_x \text{sgn}(x) \text{d}x = \lim_{\epsilon \rightarrow 0} \int_{\mathbb{R}} \partial_x \Psi_{\epsilon}(x) \text{d}x = 2.$ The question then is whether we can extend this result in a useful way, perhaps $\int_{\mathbb{R}} u \partial_t u \partial_u \text{sgn}(u) \text{d}x = \lim_{\epsilon \rightarrow 0} \int_{\mathbb{R}} u \partial_t u \partial_u \Psi_{\epsilon}(u) \text{d}x$ . And if so, hopefully this integral can be found to be 0 somehow.","['integration', 'absolute-value', 'real-analysis', 'functional-analysis', 'weak-derivatives']"
3415031,Asymptotic behavior of polynomials,"The following question is taken from Introduction to Algorithms by CLRS, Chapter $3.$ Let $$p(n) = \sum_{i=0}^d a_i n^i,$$ where $a_d>0,$ be a degree- $d$ polynomial in $n$ , and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties. a. If $k\geq d,$ then $p(n)= O(n^k).$ b. If $k\leq d,$ then $p(n)= \Omega(n^k).$ c. If $k =  d,$ then $p(n)= \Theta(n^k).$ d. If $k> d,$ then $p(n)= o(n^k).$ a. If $k< d,$ then $p(n)= \omega(n^k).$ My attempt: From exercise $3.1$ - $1$ in the same chapter, we have proven that for asymptotically nonnegative function $f(n)$ and $g(n),$ $$max(f(n),g(n)) = \Theta(f(n)+g(n)).$$ So we have $$\Theta(f(n)+g(n)) = \Theta(max(f(n),g(n))).$$ It follows that $$p(n) = \Theta(p(n)) = \Theta\left( \sum_{i=0}^d a_i n^i \right) = \Theta(n^d).$$ So all parts (a)-(e) follow. Is my attempt above correct?","['asymptotics', 'polynomials', 'discrete-mathematics', 'algorithms']"
3415050,Is a sigma-algebra just a special kind of algebra?,"Is a sigma-algebra just a special kind of algebra or are the two concepts quite distinct? My understanding is that a sigma-algebra must be defined on a set.
Furthermore I read on Wikipedia that an algebra is a set with algebraic structure. This makes it seem as if the two concepts have little to do with each other. An algebra being a special kind of set (a set with algebraic structure), and, a sigma-algebra being defined on another set as a set of subsets that fullfils the 3 special requirements of a sigma-algebra.","['measure-theory', 'abstract-algebra', 'probability-theory']"
3415066,Simplify $f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x)))$ for $x\in\mathbb{R}$ and find its inflection points,"Simplify $f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x)))$ for $x\in\mathbb{R}$ and find its inflection points, local extrema, x-intercepts, y-intercept, asymptotes, etc. So the $y$ -intercept is obviously $1$ . I'm not exactly sure if my simplification below is correct, but if it is, everything else that's required should be trivial for me (finding the local extrema and inflections points is very easy, though I'd like to know if there are any shortcuts other than evaluating the derivatives and relevant limits). So I know by drawing a simple triangle that $\sin(\tan^{-1}x)) = \dfrac{x}{\sqrt{1+x^2}}$ and $\sec(\tan^{-1}x)=\sqrt{1+x^2}.$ But then, doesn't that mean that $\sec(\tan^{-1}(\sin(\tan^{-1}x)))=\sqrt{2-\dfrac{1}{1+x^2}}$ ?","['limits', 'calculus']"
3415113,Convergence of a sequence of manifolds,"Suppose $(M_k)_k$ is a sequence of connected Riemannian manifolds, and let $a_k$ be a function or a vector field defined on $M_k$ . How can one conceive the limits $M_k\to M_\infty$ , $M_\infty$ another Riemannian manifold $a_k\to a_\infty$ , $a_\infty$ a function/vector field defined on $M_k$ . For instance, imagine a sequence of hemispheres of radius $k$ . Intuitively they must converge in some sense to a flat plane. But in which sense?","['riemannian-geometry', 'differential-geometry']"
3415165,Every homomorphism $A_n\to S_n$ extends to an endomorphism of $S_n$ for $n\geq 5$,"Let $n\geq 5$ , $S_n$ the symmetric group on $n$ letters and $A_n$ the corresponding alternating group. I want to show that every homomorphism $g:A_n\to S_n$ extends to an
  endomorphism $\tilde{g}:S_n\to S_n$ compatible with the inclusion $i:A_n\to A_n$ , i.e. $\tilde{g}\circ i=g$ . Since, for $n\geq 5$ the group $A_n$ is simple, $g$ must be injective or trivial, so let us focus on the injective case. Since we need $\tilde{g}\circ i=g$ , it follows that $\tilde{g}$ must be injective too. From groupprops I know that for $n\geq 5$ the elements of $End(S_n)$ are one of these three types: automorphisms, trivial, have image of order two. Therefore, $\tilde{g}$ must be an automorphism. From the same page I know that for $n\neq 6$ we have $Aut(A_n)=Aut(S_n)=S_n$ , all of them given by conjugation. Now, since $g$ is an isomorphism onto its image, my first question raises: Are there subgroups of $S_n$ isomorphic to $A_n$ which are not equal to $A_n$ (defined as the subgroup of even permutations)? If not, then $g$ is an automorphism of $A_n$ , which is given by conjugation by an element of $S_n$ and therefore can be easily extended to all $S_n$ . For the case $n=6$ , I haven't been able to find the automorphism structure of $S_n$ and $A_n$ , I only know that $S_n< Aut(S_n)=Aut(A_n)$ . So my second question is: How can I extend $g$ when $n=6$ ?","['permutations', 'group-homomorphism', 'group-theory', 'automorphism-group']"
3415207,Polynomial bijections between $\mathbb{N}^{m}$ and $\mathbb{N}$,Is it known if a polynomial bijection from $\mathbb{N}^{m}$ to $\mathbb{N}$ must necessarily be a polynomial of degree $m$ ? Are there two polynomial bijections from $\mathbb{N}^{m}$ to $\mathbb{N}$ that are not obtainable from each other by a polynomial change of variables? I seem to remember polynomial bijections between $\mathbb{Z}^{m}$ and $\mathbb{Z}$ are not know to exist: what would be an up-to-date reference on this subject? Or am I remembering wrong?,"['number-theory', 'polynomials']"
3415212,100 row numbers to find the shortest path,"I got a question that I have 100 rows of the number, as in the picture that continuous to 100 rows. There is a sequence by starting from
the top, and then for each integer walk to the left or right value in the row beneath. That is if we start from the top, then 40 can only be followed by 95 or 55, 95 can only be followed by 72 or 86 and so on. And I need to find the shortest path from the top to the bottom(from the first row to 100 rows).
I am thinking of plotting a graph from number 1 to 5050(cause there are in total 5050 numbers.) But how can I put weight on it later on? If I calculate weights one by one that will take ages...
Is there an easier way to figure this out? This is the picture for the first nine rows: Thank you very much.","['matrices', 'trees', 'graphing-functions']"
3415232,Is the product of two unitary Householder matrices necessarily of finite order?,"Let $\mathbf A$ and $\mathbf B$ be two unitary Householder matrices. They do not necessarily commute. Is $\mathbf{AB}$ necessarily of finite order (i.e. $\exists$ $n\in \Bbb N$ s.t. $(\mathbf{AB})^n = \mathbb I$ )? If not, under what conditions on $\mathbf {A}$ and $\mathbf{B}$ would $\mathbf{AB}$ definitely be of finite order? Note : This question arose in the context of Grover's algorithm . In that case $\mathbf{A} = U_s$ and $\mathbf{B} = U_\omega$ . I want to know whether $(U_sU_\omega)^r|s\rangle$ necessarily coincides with $|s\rangle$ after a certain number of iterations $r$ .","['matrices', 'linear-algebra', 'quantum-computation']"
3415312,show that there exists $f$ s.t. $\int_E(1-f)d\mu=\int_Efd\nu$,"Let $\mu$ and $\nu$ be finite measures on a measure space $(X,\mathcal{A})$ . Show that there is a nonnegative measurable function $f$ on $X$ such that for all $E \in \mathcal{A}$ , $$\int_E(1-f)d\mu=\int_Efd\nu$$ I have no clue how to show the above: I started using below but I stuck $$\mu(E)=\int_Efd\nu + \int_Efd\mu = \int_Efd(\nu+\mu)$$ defining $\psi=\nu+\mu$ , I have to show that $\mu\ll\psi$ ?","['measure-theory', 'radon-nikodym', 'real-analysis']"
3415344,probability of rolling a 6,"Select a die from a bag containing 2 dice, one die has 6 on all faces, and the other is a fair sided die. Choosing one die at random, roll it, and get a 6. If you roll the same die, what is the probability that the next roll is also a 6? Would this be (0.5)(1) + (.5)(1/6) = 0.5833, as picking one of the two dice is equally likely and the probability of obtaining the result is what follows.",['probability']
3415371,proving that a quaternion algebra with anisotropic norm is a division algebra,"Let $H=\left(\frac{a,b}{K}\right)$ , the quaternion algebra over a field $K$ . I want to prove that $H$ is a division algebra if and only if the norm form $N(q)=\bar{q}q$ is anisotropic, i.e. $N(q)=0\implies q=0$ . The forward direction is literally trivial; naturally, if you have zero divisors, you don't have a division algebra. However, I'm a little stuck on the backward direction. I tried showing the reverse direction by contradiction, but that didn't seem to work out too well. Can I have some direction as to how to prove this?","['abstract-algebra', 'quaternions']"
3415378,How to estimate (approximate) $\sum _{k=1}^{n}{\log(k)\binom {n}{k}}$?,"I am looking for an estimation or an approximation of $\sum _{k=1}^{n}{\log(k)\binom {n}{k}}$ Any hints will be appreciated.
Thank you.","['binomial-coefficients', 'combinatorics', 'probability-theory', 'probability']"
3415453,Induction Inequality Proof,"Given a sequence $(a_{n})$ where $a_{0}=1, a_{n+1} = \sqrt{a_{n}+2}$ I'm trying to show using induction that $1 \leq a_{n} \leq 2$ , for all $n$ . (I've shown the limit is actually $2$ ). I've never actually done an inequality induction before, but an attempt is below: Base case: For $a_{0}$ we clearly have $1\leq a_{0} \leq 2$ Inductive step: Assume the result holds for $n=k$ i.e. $1\leq a_{k} \leq 2$ Then I need to show that $1 \leq a_{k+1} \leq 2$ . Because this is a sequence I'm not sure if I can say $a_{k+1} = a_{k}+a_{1}$ (I'm assuming not). I think the recursion is confusing me for some reason. Any hints on how to proceed would be helpful. Thanks.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'inequality']"
3415481,"If $\int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x)$ for all $x\geqslant 1$, prove that $f(x)\geqslant\frac{1}{2}\,(x-1).$","Let $f:[1,+\infty)\to \mathbb{R}$ be a positive, continuous function. 
  If $\int_1^xf(t)\,\mathrm{d}t\leqslant f^2(x)$ for all $x\geqslant 1$ , prove that $f(x)\geqslant\frac{1}{2}\,(x-1).$ Attempt. Usual procedures, like working on the monotonicity of the function $$g(x):=f(x)-\frac{1}{2}\,(x-1)$$ (in order to get $g$ increasing and therefore $g(x)\geqslant g(1)=f(1)>0$ for $x\geqslant 1$ ), do not work. Thanks for the help.","['calculus', 'riemann-integration', 'analysis', 'real-analysis']"
3415489,"Can we reconstruct a function $f$ by knowing its scalar product with its own shift, $\langle T_xf,f \rangle$?","Assume $f:\mathbb{R} \to \mathbb{C}$ is a function, let's say square integrable. Assume we know the values $$
a(x) = \langle T_xf,f\rangle = \int f(y-x)\overline{f(y)} \, dy,
$$ for $x \in \mathbb{R}$ , i.e. we know all the scalar products of $f$ with its own shift $T_x$ . Is there a way to construct one function $f$ such that $\langle T_xf,f\rangle = a(x)$ ? Of course, $f$ is not unique in this case.","['operator-theory', 'functional-analysis', 'analysis', 'inverse-problems']"
3415511,Use inequality to bound an expectation,"This is Exercise 3.1.1 part (a) from Amir Dembo's Note: Let $S_{n}=\sum_{k=1}^{n}X_{n}$ for $X_{k}$ mutually independent such that $v_{n}=Var(S_{n})<\infty$ . Show that if there exists $q>2$ such that $$\lim_{n\rightarrow\infty}v_{n}^{-q/2}\sum_{k=1}^{n}\mathbb{E}|X_{k}-\mathbb{E}X_{k}|^{q}=0,$$ then $$v_{n}^{-1/2}(S_{n}-\mathbb{E}S_{n})\Rightarrow\mathcal{N}(0,1).$$ I have worked out most of the proof, but I got stuck in the end. My attempt is to apply Lindeberg's Central Limit Theorem to $$\widehat{S}_{n}:=\sum_{k=1}^{n}X_{n,k}$$ where $X_{n,k}:=v_{n}^{-1/2}(X_{k}-\mathbb{E}X_{k}).$ I have managed to show that $Var(\widehat{S}_{n})=1$ and $\mathbb{E}X_{n,k}=0$ . However, I don't know how to show the Lindeberg's Condition that for each $\epsilon>0$ , we have $$g_{n}(\epsilon):=\sum_{k=1}^{n}\mathbb{E}[X_{n,k}^{2};|X_{n,k}|\geq\epsilon]\longrightarrow 0\ \text{as}\ n\longrightarrow\infty.$$ The proof in the note works in this way: \begin{align*}
\sum_{k=1}^{n}\mathbb{E}[X_{n,k}^{2};|X_{n,k}|\geq\epsilon]&=v_{n}^{-1}\sum_{k=1}^{n}\mathbb{E}[(X_{k}-\mathbb{E}X_{k})^{2};|X_{k}-\mathbb{E}X_{k}|\geq v_{n}^{-1/2}\epsilon]\\
&\leq\epsilon^{2-q}v_{n}^{-q/2}\sum_{k=1}^{n}|X_{k}-\mathbb{E}X_{k}|^{q}\longrightarrow 0.
\end{align*} I have no problem with the first equality, which is just the expansion of $X_{n,k}^{2}$ and $|X_{n,k}|$ , I have no problem with the convergence in the end since it is by hypothesis from the question. However, I don't really understand the inequality the solution claims. I tired Chebyshev (or Markov, depending on the book), since this is the only inequality concerning with the expectation with indictor function, but I don't see how I can get this inequality. Is there any other inequality I am missing? Thank you! Edit 1: Okay I figured this out following Galton's hint, as follows: By hypothesis, there exists $q>2$ such that $$\lim_{n\rightarrow\infty}v_{n}^{-q/2}\sum_{k=1}^{n}\mathbb{E}|X_{k}-\mathbb{E}X_{k}|^{q}=0.$$ Fix such $q>2$ , note that $2-q<0$ , and then we can write for each $\epsilon>0$ that \begin{align*}
\sum_{k=1}^{n}\mathbb{E}[X_{n,k}^{2};|X_{n,k}|\geq\epsilon]&=v_{n}^{-1}\sum_{k=1}^{n}\mathbb{E}[(X_{k}-\mathbb{E}X_{k})^{2};|X_{k}-\mathbb{E}X_{k}|\geq v_{n}^{\frac{1}{2}}\epsilon]\\
&=v_{n}^{-1}\sum_{k=1}^{n}\mathbb{E}[(X_{k}-\mathbb{E}X_{k})^{q}(X_{k}-\mathbb{E}X_{k})^{2-q};|X_{k}-\mathbb{E}X_{k}|\geq v_{n}^{\frac{1}{2}}\epsilon]\\
&=v_{n}^{-1}\sum_{k=1}^{n}\mathbb{E}[(X_{k}-\mathbb{E}X_{k})^{q}(X_{k}-\mathbb{E}X_{k})^{2-q};|X_{k}-\mathbb{E}X_{k}|^{2-q}\leq v_{n}^{\frac{2-q}{2}}\epsilon^{2-q}]\\
&\leq v_{n}^{-1}\sum_{k=1}^{n}\mathbb{E}[|X_{k}-\mathbb{E}X_{k}|^{q}|X_{k}-\mathbb{E}X_{k}|^{2-q};|X_{k}-\mathbb{E}X_{k}|^{2-q}\leq v_{n}^{\frac{2-q}{2}}\epsilon^{2-q}]\\
&\leq v_{n}^{-1}\cdot v_{n}^{\frac{2-q}{2}}\epsilon^{2-q}\sum_{k=1}^{n}\mathbb{E}|X_{k}-\mathbb{E}X_{k}|^{q}\\
&=\epsilon^{2-q}v_{n}^{-q/2}\sum_{k=1}^{n}\mathbb{E}|X_{k}-\mathbb{E}X_{k}|^{q}\longrightarrow 0.
\end{align*}","['expected-value', 'proof-explanation', 'central-limit-theorem', 'probability-theory']"
3415584,Example where $(V^*)^*\neq V$?,"Let $V$ be a vector space. This answer gives a nice explanation of why the ""dual dual space"" of $V$ , i.e., $(V^*)^*$ , is isomorphic to $V$ if $\dim V<\infty.$ Can someone give an example where $\dim V=\infty$ and $(V^*)^*\ncong V$ ? I'm having a hard time imagining this. Additionally, must $V$ always be at least isomorphic to a subspace of $(V^*)^*$ ? It seems that the answer must be yes, since we can always define $\xi_{v∈ V}:V^*\to\mathbb R, \omega\mapsto\xi_v(\omega):=\omega(v).$","['functional-analysis', 'dual-spaces']"
3415597,Utility of Variation of Parameters Over Reduction of Order,"Why is variation of parameters ever useful relative to reduction of order? Reduction of order can solve any linear ODE given a single, particular solution to the associated homogeneous ODE.  Variation of parameters can solve any linear ODE given the general solution to the associated homogeneous ODE.  It should always be at least as difficult, and usually more difficult, to find a general homogeneous solution than to find a particular homogeneous solution.  This sounds like a higher barrier to entry with no extra reward. Why use variation of parameters to solve second or higher order linear ODEs at all?  Can it solve any ODEs that reduction of order can't?","['reduction-of-order-ode', 'ordinary-differential-equations', 'algorithms']"
3415619,Prove that $\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M}$,"Let $f:[0,1]\to \mathbb{R}$ be a $C^2$ class function such that $f(0)=f(1)=1$ and $f(x)>1,\forall x\in (0,1)$ . Prove that $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M},$$ where $M=\max\limits_{x\in [0,1]}f(x).$ I can't solve this problem, but here are some of the things I have tried/observed: $0$ and $1$ are extreme points of $f$ , but we cannot apply Fermat's theorem since they are the endpoints of $f$ 's domain $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \int\limits_0^1 \bigg |\frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)}\bigg | dx\ge \bigg|\int\limits_0^1 \frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)} dx \bigg|$$ Then, I tried to apply Integration by Parts, but to no avail. $\exists m\in (0,1)$ such that $f(m)=M$ . From Lagrange's MVT on $[0,m]$ and $[m,1]$ , \exists $a\in (0,m), b\in (m,1)$ such that $$M=f(m)=mf'(a) \space \text{and} \space M-1=f(m)-1=(m-1)f'(b).$$ Now, we have that $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{1}{M}\int\limits_0^1 |f''(x) |dx \ge \frac{1}{M} \int\limits_a^b |f''(x)|dx \ge \frac{1}{M} \bigg|\int\limits_a^b f''(x) dx \bigg |=$$ $$=\frac{1}{M}(f'(b)-f'(a))$$ If I substitute $f'(b)$ and $f'(a)$ in terms of $m$ and $M$ , I cannot prove the required inequality. EDIT: It should be $M-1=mf'(a)$ . With this, the problem is solved if we proceed the way I did.","['inequality', 'integral-inequality', 'real-analysis']"
3415641,"Total variation distance, $L^1$ norm","Total variation distance is a measure for comparing two probability distributions (assuming that these are unit vectors in a finite space- where basis corresponds to the sample space ( $\omega$ )). I know a distance measure need to obey triangle inequality and it should satisfy that orthogonal vectors have maximum distance and the same distributions should have distance $0$ . Others should like between these two. I completely don't understand why the $L^1$ norm is chosen for measuring the distance between these vectors (prob. distributions). 
I also want to know why it is exactly defined the way it is. $TV(P_1,P_2) = \frac{1}{2}\sum_{x \in \omega} \mid {P_1(x)-P_2(x) \mid}$","['measure-theory', 'probability-distributions', 'probability-theory', 'probability']"
3415651,"Random Walk on n+1 cycle, T is time such that walk returns to the initial vertex.","Random Walk on n+1 cycle, T is time such that walk returns to the initial vertex.  Find the probability of visiting every vertex prior to time T. Where walk moves clockwise w.p. $p \in (0,1)$ and counter-clockwise w.p. $(1-p)$ . This seems to be similar to gamblers ruin or any random walk with boundaries. My approach is to consider : $S_0 = k$ where $k = 0,1,\ldots,n$ It would move to $k-1$ with probability $1-p$ so then consider the probability of hitting $k+1$ before hitting $k$ and similarly, walk moves to $k+1$ with probability $p$ then consider probability of hitting $k-1$ before $k$ . Im not sure if this is the correct approach, it seems along the correct lines but not quite.","['random-walk', 'probability']"
3415670,Laplacian of Gravitational Potential,"It is well known that the Laplacian of the gravitational potential $(V)$ is zero. One can easily show that by differentiating, as shown in the image. But at the same time one can show via Poisson's Equation that Laplacian, $V = -4\pi G\rho$ , where $\rho$ is the density at the point. Obviously if $\rho=0$ (For Example, outside the earth) this matches the result from the differentiation, but otherwise not. Where/why does this approach of differentiating fail?","['integration', 'physics', 'multivariable-calculus', 'laplacian']"
3415677,"Prove $\sum_{\gcd(p,1+i-i^2)=1} \frac{i}{1+i-i^2} = -5^{(p-3)/2} \bmod p$","Suppose $p$ as an odd prime. How to prove $\sum_{\gcd(1+i+i^2,p)=1,1\leq i< p} \frac{i}{1+i-i^2} = -5^{(p-3)/2} \bmod p$ ? Note that we take sum only for the integer $i$ s.t. $p$ and $1+i-i^2$ are coprime to each other so that inverse of $1+i-i^2$ exist. I found this formula numerically ideone . Maybe this 5 is related to the irrational part of the solution of $1+x-x^2=0\Leftrightarrow x=\frac{1\pm\sqrt5}{2}$ . Define $\phi:=\frac{1+\sqrt5}{2},\bar\phi:=\frac{1-\sqrt5}{2}$ . $$\sum \frac{i}{1+i-i^2}$$ $$=\frac{1}{\sqrt{5}} \left(\frac{i}{i-\phi} - \frac{i}{i-\bar\phi}\right)$$ $$=\frac{1}{\sqrt{5}}\sum \left(\frac{\phi}{i-\phi} - \frac{\bar\phi}{i-\bar\phi}\right)$$ My thought is stucking here. How to prove $\sum \frac{i}{1+i-i^2} = -5^{(p-3)/2} \bmod p$ ? This question is related to the another question which I post previously link .","['number-theory', 'finite-fields', 'fibonacci-numbers']"
3415684,Calculation of $2^{XXXX} + 3^{XXXX}\pmod{11}$ [duplicate],This question already has answers here : Mod of numbers with large exponents [modular order reduction] (3 answers) Modular exponentiation by hand ($a^b\bmod c$) (12 answers) Closed 4 years ago . I've a question: How do I calculate $2^{2020} + 3^{2020}\pmod{11}$ ? Is there a theorem or any trick to do it? I need to show all the steps I used to calculate the Rest but I've no clue how to even start with the calculation... Does anyone have any tips and could show me how to calculate it? Greetings!!!,"['modular-arithmetic', 'discrete-mathematics']"
3415730,"On the Idea of a ""Validity Calculus"" Analogous to Probability Calculus","The question might be judged as belonging to logic-fiction rather than to serious  logic ... Some well formed formulas are true in all possible cases/ interpretations , some are true in no possible case/ interpretation, some are contingent ( true in some cases, false in other ones). Now, amongst contingent formulas, some are true in more cases than others are. For example,  (A --> B) is true in 3 cases out of 4, while (A&B) is true in only 1 case out of 4. So, one could say that some formilas are closer to  validity than others. This suggests an analogy with the theory of probability, in which some some propositions are considered as being closer to certainty than others. Is it possible to think of rules concerning, so to say, the "" degrees"" of validity of propositional calculus formulas. Is it possible to take a theoretical advantage of the fact that all formulas are not at the same distance from validity.","['propositional-calculus', 'logic', 'probability-theory']"
3415812,Uniform Continuity of $x^2$.,"Assume all the standards, such as $f:\mathbb R→\mathbb R$ and all that other jargon. I guess it doesn't really matter, but my main question is why isn't $f(x)=x^2$ uniformly continuous. I know if we restrict the domain to some subset $D\subset \mathbb R$ , then $f$ is uniformly continuous on $D$ , but not on the entire domain of $\mathbb R$ .","['continuity', 'uniform-continuity', 'analysis', 'real-analysis']"
3415894,Find $n$ points on a circle with integer distances.,"Let $n$ be a positive integer, prove that it is possible to put $n$ points on a circle so that the distances among them are all integers. For $n \leq 3$ this is trivial. I have shown it for $n=4$ by considering a rectangle.
I don't know how to do it for larger numbers though.","['euclidean-geometry', 'number-theory', 'geometry-of-numbers', 'circles', 'plane-geometry']"
3415897,A morphism intertwining two induced representations,"Consider the space $\,{\cal{L}}^G\,$ of all continuous functions $\,G\longrightarrow{\cal{L}}\,$ mapping a Lie group $\,G\,$ into a vector space $\,{\cal{L}}\,$ . Assume that $\,G\,$ has two proper subgroups: $$
 K\,,~Q~<~G~~,
 $$ whose representations, $\,D(K)\,$ and $\,\Lambda(Q)\,$ , are acting in $\,{\cal{L}}^G\,$ . Consider two subspaces of $\,{\cal{L}}^G\,$ .  One subspace, $$
 {\mbox{Map}}_K(G,\,{\cal{L}})\,=\,\left\{\,\varphi\,\right\}~~,
 $$ comprises the vector functions $\,\varphi\,$ obeying the equivariance condition $$
 \varphi(g\, k)~=~D^{-1}(k)~\varphi(g)~,~~~k\,\in\, K~~.
 $$ In this subspace, $\,D(K)\,$ is induced to a representation of $\,G\,$ , denoted by $$
 U^{(D)}\,\equiv\,D(K)\,\uparrow\, G
 $$ and implemented with $$
 U^{(D)}_g\,\varphi(g^{\,\prime})~=~\varphi(g^{-1}\, g^{\,\prime})~~.
 $$ Another subspace, $$
 {\mbox{Map}}_Q(G,\,{\cal{L}})\,=\,\left\{\,\psi\,\right\}
 $$ will comprise the functions $\,\psi\,$ satisfying $$
 \psi(g\, q)~=~\Lambda^{-1}(q)~\psi(g)~,~~~q\,\in\, Q~~.
 $$ In this subspace, $\,\Lambda(Q)\,$ is induced to a representation of $\,G\,$ , denoted by $$
 U^{(\Lambda)}\,\equiv \,\Lambda(Q)\,\uparrow\, G
 $$ and implemented with $$
 U^{(\Lambda)}_g\,\psi(g^{\,\prime})~=~\psi(g^{- 1}\, g^{\,\prime})~~.
 $$ While both $\,U^{(D)}\,$ and $\,U^{(\Lambda)}\,$ are realised via left translations, they are different representations, as they are
 acting in subspaces defined by different subsidiary conditions. For convenience, we summarise this in the table: $$
 \varphi(g\, k)=D^{-1}(k)\,\varphi(g)~,~~k\in K  \qquad \quad \psi(g\, q)=\Lambda^{-1}(q)\,\psi(g)~,~~q\in Q
 $$ $$
 U^{(D)}\,\equiv\,D(K)\,\uparrow\, G   \qquad \qquad \qquad \qquad         U^{(\Lambda)}\,\equiv \,\Lambda(Q)\,\uparrow\, G
 $$ $$ 
 U^{(D)}_g\,\varphi(g^{\,\prime})~=~\varphi(g^{-1}\, g^{\,\prime})  \qquad \qquad \qquad \qquad   U^{(\Lambda)}_g\,\psi(g^{\,\prime})~=~\psi(g^{- 1}\, g^{\,\prime})~
 $$ Our goal is to describe the space $\,\left[\, D(K)\,\uparrow\, G\,,~\Lambda(Q)\,\uparrow\, G  \,\right]\,$ of the morphisms $\,\psi\,=\,\hat{T}\,\varphi\,$ . QUESTION: How to prove that the most general form of a morphism is $$
 \psi(g)~=~(\hat{T}\,\phi)(g)~=~\int_G t(g^{-1}\, g^{\,\prime})\,\varphi(g^{\,\prime})\,dg^{\,\prime}~~,\qquad\qquad\qquad(1)
 $$ where $\,dg\,$ is an invariant measure on $\,G\,$ . PS. As an aside, I would mention that for the equivariance conditions to be satisfied the kernel must obey $$
t(qgk) = \Lambda(q) t(g) D(k)~~,~~~q\in Q\,,~~g\in G\,,~~k\in K~~.
$$ This, however, is the next theorem; and I don't want to go there until the basic property (1) is proven.","['group-theory', 'lie-groups']"
3415925,p-groups such that all subgroups are normal must be abelian,"Let $p$ be an odd prime and let $P$ be a $p$ -group. Prove that if every subgroup of $P$ is normal then $P$ is abelian. [Use the preceding exercise and exercise 15 of section 4] The previous exercise was to prove that for an odd prime $p$ and $P$ a $p$ -group that is not cyclic then $P$ contains a normal subgroup $U$ such that $U \cong \mathbb{Z}/p \times \mathbb{Z}/p$ . Exercise 15 says that if $A$ and $B$ are two normal subgroups of a group $G$ such that $G/A$ and $G/B$ are abelian then $G/(A \cap B)$ is abelian. Clearly if $P$ is cyclic then $P$ is abelian so we may assume that it is non-cyclic and hence by the previous exercise must have a subgroup $U \cong \mathbb{Z}/p \times \mathbb{Z}/p$ . My idea is to try to get two subgroups that are disjoint to each other and such that their respective quotients are abelian as then $G/\{e\} \cong G$ would be abelian. I am unsure how to produce two groups like that, any suggestions are appreciated, thanks!","['group-theory', 'abstract-algebra', 'p-groups']"
3415955,Sequence of positive numbers,$(1)$ Suppose $\{\alpha_n\}$ and $\{\beta_n\}$ are sequence of real numbers where $\alpha_n \rightarrow \infty$ and $\alpha_n\sim\beta_n$ as $n \rightarrow \infty$ . Prove that $\alpha_n \log \alpha_n \sim\beta_n \log\beta_n $ as $n \rightarrow \infty$ . My proof : Since $\alpha_n\sim\beta_n$ then $\lim_{n \rightarrow \infty} \frac{\alpha_n}{\beta_n}=1$ . Now observe that $$\lim_{n \rightarrow \infty} \frac{\alpha_n \log \alpha_n}{\beta_n \log\beta_n} = \lim_{n \rightarrow \infty}\frac{\alpha_n}{\beta_n} \lim_{n \rightarrow \infty}\frac{\log \alpha_n}{\log \beta_n}.$$ So $$\lim_{n \rightarrow \infty}\frac{\log \alpha_n}{\log \beta_n}=\lim_{n \rightarrow \infty}\frac{1}{\alpha_n}/\frac{1}{\beta_n}=\lim_{n \rightarrow \infty}\frac{\beta_n}{\alpha_n}=1 \;\;\;(\text{as} \; n \rightarrow \infty).$$ Is my reasoning correct?,['number-theory']
3415959,Why are double derivatives always same? [duplicate],"This question already has answers here : Why can partial derivatives be exchanged? (2 answers) Closed 4 years ago . When taking double/triple derivatives of multivariable functions, we see that no matter which order we take the derivative in, as long as we take the derivative od the function with respect to the same number of variables, the same number of times, our answer is the same? Is there an intuitive explanation for this? I cannot seem to understand how.",['multivariable-calculus']
3415977,"In $C[\mathbb{R}]$, if $d(f,g) = \infty$ then $f$ and $g$ are in different connected components.","In my general topology course we were given the following problem Consider the metric space of continuous functions with the supremum metric, that is, $C[\mathbb{R}]:= \{f:\mathbb{R} \to \mathbb{R} | \ f \text{ is continuous in the usual sense}\}$ with the topology induced by the metric $d(f,g) = \sup|f(x)-g(x)|$ . Note that this metric isn't always finite. Prove that if $\ d(f,g) = \infty \ $ then $\ f$ and $\ g$ are not on the same connected component. My attempt was to show the converse: if they're both in the same connected component then they must be finitly distant. Let $C$ be a connected component and take $f,g \in C$ . Take the following $C$ -open sets $F = C \cap \left( \bigcup_{n \in \mathbb{N}} B_n(f) \right)$ and $G = C \cap \left(\bigcup_{n \in \mathbb{N}} B_n(f) \right)$ . Now (this I don't know/think its true), $C = F \ \cup \ G$ , and since $C$ is connected, $F \ \cap \ G \neq \emptyset$ . Take $h$ in the inteserction, then both $d(f,h), \ d(g,h)$ are finite and thus $d(f,g) \le d(f,h) + d(h,g) < \infty$ . Done. I'm not sure if those $F$ and $G$ work (note: if the statement is true, then $F$ and $G$ are exacly the connected components of $f\ $ and $g$ ), but I feel so close to finishing my proof. If I'm actually on the right track I would appreciate a hint on how to define/find the sets that will ""split"" $C$ conviniently. Thanks in advance.","['general-topology', 'connectedness']"
3415991,Linear dependence of entire functions.,"Given two entire function f and g , suppose exp( f ) , exp( g ) and 1 are linearly dependent in the complex field, how can we show that f , g , and 1 are also linearly dependent ?","['complex-analysis', 'functional-analysis']"
3415999,Find the inverse Laplace transform of $X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3}$ with $\Re(s)>-1$,Find the inverse Laplace transform of $$X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3} \qquad \Re(s)>-1$$ I never learned how to use this in class and so I've seen a couple youtube videos however they are too easy and not as complex as this one. I am aware that this is a step function so therefore it will involve this property: $$e^{-cs}F(s)= f(t-c)u(t-c)$$ So far I factored the denominator and separated the numerator and have: $$\frac{2}{(s+1)(s+3)} + \frac{2se^{-2s}}{(s+1)(s+3)} + e^{4s}\frac{4}{(s+1)(s+3)}$$ Anything like references to problems similar as this one would help me.,"['laplace-transform', 'ordinary-differential-equations']"
3416035,"Proving $\int f_n gd \mu \to \int f g d\mu$ for $f \in L^p, g \in L^q$ where $p$ and $q$ are Holder conjugate exponents.","I usually have a hard time with proofs about convergence, so I want to make sure if I did it right or not. The problem is: If $f_n \to f$ in $L^p$ , $g \in L^q$ , and $\frac{1}{p} + \frac{1}{q} = 1$ , then $$\int f_n g d\mu \to \int fg d\mu$$ By Hölder's inequality, for each $n$ , $$\int |f_n g| d\mu \leq ||f_n||_p ||g||_q < \infty$$ Since $f_n \to f$ in $L^p$ , that means that $f$ is integrable and, therefore, measurable. Since $g \in L^q$ , it is integrable. Therefore, by the Dominated Convergence theorem $$\lim_{n\to \infty}\int f_n g d\mu = \int\lim_{n\to \infty}(f_ng )d\mu = \int fgd\mu$$","['measure-theory', 'lp-spaces', 'convergence-divergence', 'real-analysis']"
3416056,Showing fundamental group of a particular space is trivial.,"Let $Z$ be the subspace of $\Bbb R^2$ given by $$Z=\bigg(\{0\}\times[-1,1]\bigg)\bigcup\bigg\{(x,y):0<x\leq 1,y=\sin\bigg(\frac{\pi}{x}\bigg)\bigg\}.$$ Next consider the quotient space, $$X=\frac{Z}{\{(0,0),(1,0)\}}.$$ I want to show, $X$ has trivial fundamental group. Here is my approach. Since $X$ is path connected, it is enough to show, $\pi_1(X,[(0,0)])$ is trivial. So let $f:[0,1]\to X$ be a loop based at $[(0,0)]$ . Since $[0,1]$ is compact and continuous image of a compact set is compact we have, $f([0,1])$ is a compact subset of $X$ . Hence we have a $1>\delta>0$ such that, image of $f$ is contained in $X_{\delta}$ , where $$X_{\delta}=\frac{\big(\{0\}\times[-1,1]\big)\bigcup\big\{(x,y):\delta\leq x\leq 1,y=\sin\big(\frac{\pi}{x}\big)\big\}}{\{(0,0),(1,0)\}}.$$ Now, $X_{\delta}$ is contractible space. So $f:[0,1]\to X_{\delta}$ is homotopically equivalent to constant loop. Now for the inclusion map $i:X_{\delta}\hookrightarrow X$ we have the induced inclusion map, $i_*:\pi_1(X_{\delta},[(0,0)])\hookrightarrow \pi_1(X,[(0,0)])$ . Since $\big[f:[0,1]\to X_{\delta}\big]\in \pi_1(X_{\delta},[(0,0)])$ is a trivial element, so $\big[f:[0,1]\to X\big]\in \pi_1(X,[(0,0)])$ is also a trivial element. My question is, am I right? If, not where is my fault. Another question is, is $X_{\delta}$ a contractible space? Thanks.","['general-topology', 'fundamental-groups', 'algebraic-topology']"
3416091,What is conjugate?,"The concept of conjugate seems to exist in many fields of mathermatic such as complex conjugate, group conjugate, etc. I search through many websites about what exactly is the conjugate. Most of them always claim that ""just change the sign"". Suppose there is $a+b$ , the conjugate is $a-b$ . While in the group, the conjugate of $s$ is $x*s*x^{-1}$ , where $s$ is element of group $G$ and $x$ is fixed element of group $G$ . How do we will know that the conjugate will be in what form? What is the conjugate told us or use for? I already read this What is the conjugate? . It doesn't help at all.",['abstract-algebra']
3416127,general formula for the nth derivative of $f(x)=\frac{1}{1+e^{x}}$,"consider the function : $$f(x)=\frac{1}{1+e^{x}}$$ the nth derivative of the function is given by the following formula: $$f^{(n)} (x)=\sum_{k=1}^{n+1}a_{n,k}\frac{1}{\left(1+e^{x}\right)^{k}}$$ where $$a_{n,k}=\left(-1\right)^{n}\sum_{j=0}^{k-1}\left(-1\right)^{j}{{k-1}\choose{j}}\left(j+1\right)^{n}$$ my question is that:how the formula can be derived without using induction? I have no idea about that, so any hint or full proof would be highly appreciated.","['proof-writing', 'derivatives']"
3416248,"Is $\mathbb{R}$ a subset of $\mathbb{R}^n, \ n>1$?","I am very confused right now... I think it is not, since it is like comparing apples and black holes... but I am not sure anymore... woahoah $\overset{\times \times}{\sim}$ So, is $\mathbb{R}$ a subset of $\mathbb{R}^n, \ n>1$ ?","['elementary-set-theory', 'real-numbers']"
3416264,expected operator norm of random symmetric matrices,"The following is an easy corollary from noncommutative Khintchine's inequality (see, e.g., Vershynin's high-dimensional probability book, Theorem 6.5.1). Let $A$ be an $n\times n$ symmetric random matrix whose entries on and above the diagonal are independent, mean zero random variables. Then $$
\mathbb{E}\|A\| \lesssim \sqrt{\log n}\ \mathbb{E}\max_i \| A_i\|_2
$$ where $\|A\|$ denotes the operator norm of $A$ and $A_i$ denotes the $i$ -th row of $A$ . Question : Is $\sqrt{\log n}$ necessary in the bound above? Vershynin's book claims that it is necessary (Exercise 6.5.4) but I am unable to find an example. The bound seems quite loose to me, actually, and it is already loose for diagonal matrices and Wigner matrices. I looked up the literature, and for entries that are gaussians (with different variances) the bound above is definitely loose, as it is known that when $A_{ij}\sim N(0,b_{ij}^2)$ (due to van Handel and others) we have $$
\mathbb{E}\|A\| \lesssim \max_i\sqrt{\sum_j b_{ij}^2} + (\max_{i,j} b_{ij})\sqrt{\log n}.
$$ So the hope of finding a tight example is not to have Gaussian entries, and I don't have a clue for this. Usually I think the $\sqrt{\log n}$ factor would come from the maximum of $n$ gaussians in tightness examples.","['matrices', 'random-matrices', 'expected-value', 'spectral-norm']"
3416321,An interesting geometry problem,"In a  ΔMAP, on sides MA and AP, squares are drawn. If P and D are on the same side of AM;      and M,E lie  on opposite sides of AP. D and E are the centres of the squares on MA and AP respectively. Find the angle between MP and DE. I have been trying to solve this problem since long time and I've been unable to do so. I have already found an approach using complex numbers but I want a pure geometry solution. The diagram was really complicated and visualising constructions were much harder. Would someone please help me to solve this question? Thanks for help .",['geometry']
3416322,"Odd ""How many triangles are there in the picture?"" Question From the First Stage of Israel's International Mathematical Olympiad Selection Process","So stage 1 of of Israel's International Mathematical Olympiad selection process took place on Monday, and this was one of the questions (6 or 7 out of 10). Q: (The way the shape is produced wasn't inscribed, but its a pretty easy guess):
Draw an interval from each vertex of a square to the 2 midpoints of the sides it is not on. How many triangles are there in the shape? Is there a (viable) way to solve questions similar to the one I showed?","['contest-math', 'geometry']"
3416335,Proof $f(x) = e^{-3x} + 7\cos(6x)-50x+34$ bijective and $(f^{-1})'(42)$,Let $f: \mathbb{R} \to \mathbb{R}$ with $$f(x) = e^{-3x} + 7\cos(6x)-50x+34$$ It says that this function is bijective and I can see that with WolframAlpha but how can one prove that? And when I tried calculating $(f^{-1})'(42)$ no online math tool can solve it. Why?,"['functions', 'inverse-function']"
3416355,Automorphism on subgroup lift to the whole group,"By definition, if $H$ is characteristic subgroup of $G$ , then every automorphism of $G$ restricted on $H$ is an automorphism of $H$ . From these two questions (1) (2) we know it's generally false that
if $H$ is characteristic subgroup of $G$ , given $\varphi \in \text{Aut}(H)$ , then there exists $\psi\in\text{Aut}(G)$ s.t. $\psi|_H=\varphi$ . Example from $A_3$ and $S_3$ shows even if such a $\psi$ exists, it may not be unique. My question: Suppose $H$ is subgroup of $G$ , what's the sufficient condition s.t. $\forall \varphi \in \text{Aut}(H)$ , $\exists\ \psi\in\text{Aut}(G)$ s.t. $\psi|_H=\varphi$ ? Background: This question comes from studying $\text{Aut}(S_n)$ and $\text{Aut}(A_n)$ . If $n\geqslant 4$ and $n \neq 6$ , then $\operatorname{Aut}(A_n)\cong\operatorname{Inn }(S_n)\cong\operatorname{Aut}(S_n)\cong S_n$ . The existence and uniqueness is true for $A_n$ and $S_n$ ( $n\geqslant 4$ and $n \neq 6$ ), while uniqueness is not true for $A_3$ and $S_3$ . To move this question out of the unanswered list, I put related materials in answer.","['group-theory', 'abstract-algebra', 'characteristic-subgroups']"
3416399,how do you prove that $A\times (B\setminus C) = (A\times B) \setminus (A\times C)$?,"This is what I have come up with so far, but I am rather lost at lines 3-4... $$\begin{align}
(a,b)\in A\times (B\setminus C)\iff& a\in A \land b\in (B\setminus C)\\
\iff& a\in A \land (b\in B \land \lnot(b\in C))\\
\iff& (a\in A \land b\in B) \land (a\in A \land b\notin C)\\
\iff& (a\in A \land b\in B)\setminus (a\in A \land b\in C)\\
\iff& (a,b)\in (A\times B)\setminus ((a,b)\in A\times C)\\
\iff& (a,b)\in (A\times B)\setminus (A\times C)
\end{align}$$","['elementary-set-theory', 'logic']"
3416408,What condition for $n$ such that $G_n$ is graph planar?,"Let $G_n$ be the simple graph with the set of vertices $V_n=\{1, 2, . . . , n\}$ . Two different vertices $x$ and $y$ are adjacent if $x$ is a multiple of $y$ or $y$ is a multiple of $x$ . For what $n$ is $G_n$ planar? I tried to draw graph starting from small $n$ until I cannot draw graph $G_n$ it without crossings. When that happens, moving some of the vertices so I can avoid the crossings. But I cannot find which $n$ is $G_n$ graph planar? Any hint is highly appreciated.","['graph-theory', 'discrete-mathematics']"
3416414,Maximum value of $8v_1 - 6v_2 - v_1^2 - v_2^2$ subject to $v_1^2+v_2^2\leq 1$,"Given that $g:\mathbb{R}^2 \to \mathbb{R}$ defined by $$g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2$$ find the maximum value of $g$ subject to the constraint $v_1^2+v_2^2\leq 1.$ My attempt: Note that $$g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2 = -(v_1-4)^2 - (v_2+3)^2 + 25.$$ So $g$ is a decreasing function. So, the maximum value of $g$ lies on the circumference of $v_1^2+v_2^2 = 1.$ It suffices to find the intersection between $(0,0)$ and $(4,-3)$ as $(4,-3)$ is the peak point of $g.$ The intersection point lies on both $v_1^2+v_2^2 = 1$ and $v_2 = -\frac{3}{4}v_1.$ Solving the simultaneous equation gives that $$v_1 = \frac{4}{5}, \quad v_2 = -\frac{3}{5}.$$ So, maximum value of $g$ is $$g\left(\frac{4}{5}, -\frac{3}{5}\right) = 9.$$ Is my attempt correct?","['convex-optimization', 'optimization', 'multivariable-calculus', 'qcqp']"
3416493,How to further expand $\text{grad} \left( \vec{a} \cdot\vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot\vec{b} \right )$?,"With $\vec{a}, \vec{b}: \mathbb{R}^3 \to \mathbb{R}^3$ vector fields: I want to expand $\text{grad} \left( \vec{a} \cdot \vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot \vec{b} \right )$ . So I started with: $\left [\vec{\nabla}\left (\vec{a} \cdot \vec{b} \right )  \right ]_i = \partial_i\left (a_j b_j \right ) \overset{\text{Product rule}}{=} \left ( \partial_i a_j \right ) b_j + a_j \left( \partial_i b_j \right )$ But where to go from there? In the end I'm supposed to arrive at: $\text{grad} \left( \vec{a} \cdot \vec{b} \right ) =\vec{a} \times \left(\vec{\nabla} \times \vec{b}\right) + \vec{b} \times \left(\vec{\nabla} \times \vec{a}\right) + \left(\vec{b} \cdot\vec{\nabla}\right) \vec{a} + \left(\vec{a} \cdot\vec{\nabla} \right) \vec{b}$","['notation', 'vector-fields', 'derivatives']"
3416520,A conceptual reason for why the Jacobian of a rotation by a changing angle is $1$?,"Consider $$f(x,y)=(x\cos r^2+y\sin r^2, y\cos r^2-x\sin r^2)\qquad\text{with }r=\sqrt{x^2+y^2},$$ as a map $\mathbb{R}^2 \to \mathbb{R}^2$ . Geometrically, $f(x,y)$ is obtained from the vector $(x,y)$ by rotating it by angle $r^2$ . (we rotate the more we move away from the origin). Wolframe Alpha claim that $\det (df)=1$ holds identically. Is there an elegant rigorous way of seeing this, without too much computations? Should this be ""obvious"" in retrospect? I was a bit surprised by this result... and direct computation is not so nice to do by hand (although tractable). I thought using the chain rule (treating the angle as a function of $x,y$ ) but got nowhere. Edit: I agree that roughly speaking, near a point $p$ , this function is like a standard rotation by a fixed angle $|p|^2$ . However, I do not consider this a rigorous explanation. Indeed, this vague intuition is still with us when we replace $r^2=x^2+y^2$ by $x^4+y^4$ , but then the Jacobian is non-constant. So, this property doesn't even hold for smooth radially symmetric angle function $\theta(x,y)$ . Is this just a coincidence then? Can we charavterize the angle functions (or at least radially symmetric ones) which satisfy this? (Wolframe says the Jacobian remains $1$ when we replace $r^2$ by $r$ or $r^4$ . Perhaps this remains true for any power of $r$ ?)","['differential-geometry', 'jacobian', 'multivariable-calculus', 'orthogonal-matrices', 'rotations']"
3416545,Evaluate $\int_0^{2\pi} \frac{2-\cos t}{(2\cos t-1)^2 + \sin^2 t} dt$,"\begin{align}
  \int_0^{2\pi} \frac{2-\cos t}{(2\cos t-1)^2 + \sin^2t} dt
\end{align} I am trying to evaluate above integral. The results is $2\pi$ according to Mathematica. I want to obtain this result by integrating properly Can this integral be evaluated using simple trigometric identities? Do I have to use complex analysis i.e., $\cos(\theta) = \frac{z+\frac{1}{z}}{2}$ and do residue calculus?","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
3416563,Stuck on this definite integral,"I am trying to solve this definite integral $$3\int_0^{\pi/2} \frac{\sqrt {\cos x}}{(\sqrt {\cos x}+\sqrt{\sin x})^5} \,dx$$ I take the following approach- I first remove the numerator by using the definite integral property $$\int_{a}^{b} f(x)dx=\int_{a}^{b} f(a+b-x)  dx$$ Using the above property and simplfying $$\dfrac{3}{2}\int_0^\frac{\pi}{2} \dfrac{sec^2x}{(1+\sqrt {tanx})^4} dx$$ Then I use the substitution $$\tan x=t$$ $$\Rightarrow \sec^2x dx=dt$$ Then I get a new integral $$\dfrac{3}{2}\int_0^\infty \dfrac{dt}{(1+\sqrt t)^4}$$ I am stuck now because I do not know how to evaluate limits with infinity. Please help","['calculus', 'definite-integrals', 'algebra-precalculus']"
3416591,"Define $P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2}$. Show $P_{n}$ converges uniformly on $[-1, 1]$","Let $P_{0} = 0$ and for $n = 0,1,2,...,$ define $P_{n+1}(x) = P_{n}(x)+\frac{x^{2} - P_{n}(x)^{2}}{2}$ . Show that $P_{n}$ converges uniformly on $[-1, 1]$ and find its limit. For proving that two claim we have to prove Claim 1: $0 \leq P_{n}(x) \leq P_{n + 1}(x) \leq \vert x\vert \leq 1$ for $x\in [-1, 1]$ , proved easily. Claim2: $0 \leq \vert x \vert - P_{n}(x) < \frac{2}{n + 1}$ . This is difficult for me. I get a help form web note where they first prove $0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n}$ and I understand the rest of the proof. My question is where they get intention to prove $0 \leq \vert x \vert - P_{n}(x) \leq \vert x \vert \left(1 - \frac{\vert x \vert}{2}\right)^{n}$ ?","['analysis', 'real-analysis']"
3416612,Evaluation of Integral sums,"Hey I am supposed to evaluate: $$\lim_{n \to \propto }\sum_{i=1}^{n}\frac{i}{n^{2}+i^{2}}$$ What I did: $$\lim_{n \to \propto }\sum_{i=1}^{n}\frac{i}{n^{2}+i^{2}}=\lim_{n \to \propto }\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{i}{1+\left ( \frac{i}{n} \right )^{2}}$$ But I do not know, what to do next, or how to eliminate i so I can transfer it to integral. Can anyone help me?","['limits', 'definite-integrals', 'riemann-sum']"
3416629,Dimension that maximizes surface area and volume unit $n$-sphere,"Let $S_n(R)$ and $V_n(R)$ denote the surface area and volume of an $n$ -sphere with radius $R$ respectively. It is well known that $$S_n(R) = \frac{n \pi^{\frac{n}{2}}R^{n-1}}{\Gamma\left(\frac{n}{2}+1\right)}\quad\text{and}\quad V_n(R) = \frac{\pi^{\frac{n}{2}}R^{n}}{\Gamma\left(\frac{n}{2}+1\right)}$$ where $\Gamma(z)$ is the gamma function: $$\Gamma(z) = \int_0^\infty e^{-x} x^{z-1}\, dx.$$ Question: Let $v$ and $s$ be positive integers such that $S_s(1)$ and $V_v(1)$ are the maximum. What is $v-s?$ If I am not mistaken, I saw somewhere that $s = 7$ and $v = 5$ (I think) but I couldn't find that source anymore. While trying to prove the result, I realize that I am not able to do so. Any hint is appreciated.","['calculus', 'geometry', 'recreational-mathematics']"
3416635,Find Adjoint of $L = p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x)$,"Suppose that \begin{align*}
  L &= p(x) \frac{d^2}{dx^2} + r(x) \frac{d}{dx} + q(x) \\
\end{align*} Consider \begin{align*}
  \int_a^b vL(u) \, dx \\
\end{align*} By repeated integration by parts, determine the adjoint operator $L^*$ such that \begin{align*}
  \int_a^b \left[ uL^*(v) - vL(u) \right] &= \left. H(x) \right|_a^b \\
\end{align*} What is $H(x)$ ? Under what conditions does $L=L^*$ , the self-adjoint case? Hint : Show that \begin{align*}
  L^* &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
\end{align*} The given answer that I must find is: \begin{align*}
  H(x) &= p \left( u \frac{dv}{dx} - v \frac{du}{dx} \right) + uv \left( \frac{dp}{dx} - r \right) \\
\end{align*} I'm not sure how to calculate the $L^*$ given in the hint, I obviously know integration by parts, but I don't see how that would be used here, I don't see how to calculate $H(x)$ when we have $L^*$ . Once we have $L^*$ , I can identify the self-adjoint conditions where $L = L^*$ : \begin{align*}
  p \frac{d^2}{dx^2} + r \frac{d}{dx} + q &= p \frac{d^2}{dx^2} + \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
  r \frac{d}{dx} + q &= \left( 2 \frac{dp}{dx} - r \right) \frac{d}{dx} + \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) \\
\end{align*} which would mean that \begin{align*}
  r(x) &= 2 \frac{dp}{dx} - r(x) \\
  \frac{dp}{dx} &= r(x) \\
  p(x) &= \int r(x) \, dx \\
\end{align*} and \begin{align*}
  q &= \frac{d^2p}{dx^2} - \frac{dr}{dx}(x) + q \\
  \frac{d^2p}{dx^2} &= \frac{dr}{dx}(x) \\
  \frac{dp}{dx} &= r(x) + c \\
  p(x) &= \int \left( r(x) + c \right) \, dx \\
\end{align*} Since both expressions must be true, the constant in the second expression can be eliminated. We can conclude that $L = L^*$ if and only if $p(x) = \int r(x) \, dx$ I try to calculate $u L^*(v) - v L(u)$ but this doesn't seem useful: \begin{align*}
  u L^*(v) &= u p \frac{d^2v}{dx^2} + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} + q \right) v \\
  v L(u) &= v p \frac{d^2u}{dx^2} + v r \frac{du}{dx} + v q u \\
  u L^*(v) - v L(u) &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + u \left( 2 \frac{dp}{dx} - r \right) \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - v r \frac{du}{dx} \\
  &= p \left( u \frac{d^2v}{dx^2} - v \frac{d^2u}{dx^2} \right) + 2 u \frac{dp}{dx} \frac{dv}{dx} + u \left(\frac{d^2p}{dx^2} - \frac{dr}{dx} \right) v - r \left( u \frac{dv}{dx} + v \frac{du}{dx} \right) \\
\end{align*}","['adjoint-operators', 'ordinary-differential-equations', 'partial-differential-equations']"
3416641,"A continuously differentiable function $f$ Satisfying $ \| f ( x ) - f ( y ) \| \geq \| x - y \| , \forall x , y \in \mathbb { R } ^ { n } $ is onto","Let $f: \mathbb { R^n } \rightarrow \mathbb { R } ^ { n }$ be continuously
differentiable, Satisfying $$
\| f ( x ) - f ( y ) \| \geqslant \| x - y \| , \forall x , y \in \mathbb { R } ^ { n }
$$ then how to prove that $f$ is onto. My work:
Consider $f:\mathbb R^n \rightarrow Range(f),$ Then clearly $f$ is bijective
from $\mathbb { R } ^ { n }$ onto $Range( f )$ also since $$\left\| f ^ { - 1 } ( x ) -f ^ { - l } ( y ) \right\| \leq \| x - y \|$$ $f ^ { - 1 }: \operatorname { Range } (f) \rightarrow \operatorname {\mathbb R^n }$ Continuous.
Thus $Range(f) \subseteq \mathbb { R } ^ { n }$ is
homeomorphic to $\mathbb { R } ^ { n }$ ,
is the way is correct? I am not getting idea further.","['frechet-derivative', 'derivatives', 'real-analysis']"
3416686,Why every $(n-2)\times(n-2)$-submatrix of this $(n-2)\times n$ matrix has full rank?,"Let $A$ be an $(n-2)\times n$ matrix such that for every $i=1,2,...,n-2$ the $i$ th row consists of $1, -2, 1$ at $i$ th, $(i+1)$ st, and $(i+2)$ nd column, respectively, and all other entries are $0$ . Then $n$ -term arithmetic progressions are some integer solutions of $Ax=0$ . It is said that every $(n-2)\times(n-2)$ submatrix of $A$ has full rank. I am wondering why? I have a feeling that after crossing out two columns except the first or the last one, the remaining square matrix is upper triangular with non-zero diagonal entries. But is there a way to prove it?","['matrices', 'number-theory', 'integers', 'linear-algebra']"
3416741,Class of absolutely continuous probability measures is $G_\delta$,"Consider a Polish metric space $(X,d)$ and the class of Borel probability measures on it, endowed with the usual topology of weak convergence of measures, say $(\mathcal{P},\tau_W)$ . Then, it is well known that $(\mathcal{P},\tau_W)$ is separable and completely metrizable - e.g. via the Léevy-Prohorov metric, see https://en.wikipedia.org/wiki/Lévy –Prokhorov_metric For simplicity, let $X$ be a subset of an Euclidean space and denote by $\mathcal{P}_0\subset\mathcal{P}$ the subset of probability measures which are absolutely continuous with respect to the Lebesgue measure. Clearly, $\mathcal{P}_0$ is not closed, for a sequence of absolutely continuous measures can converge to a point mass. Is $\mathcal{P}_0$ a $G_\delta$ set in $(\mathcal{P},\tau_W)$ ? ADDENDUM: Let $\nu$ be the Lebesgue measure on $X$ and define $$
\Delta=\{f \in L_1(\nu):\, \int_Xf d\nu=1,\, f \geq  0 \,a.e.\}.
$$ Then, in a 1989 paper, https://www.jstor.org/stable/pdf/4616132.pdf?refreqid=excelsior%3A4ad9120f5b9af9f4af6aa1c012f5b44a Gaudard and Hadwin argued that $\Delta$ is a closed subset of $L_1(\nu)$ (in fact, a Standard Borel subspace) and that the map $\Phi:\Delta\mapsto\mathcal{P} $ defined via $$
[\Phi(f)](D)=\int_Df d\nu, \quad \text{for all } D \text{ Borel subset of }X,
$$ is $1-\text{to}-1$ and continuous. In particular, $\Phi$ is an isomorphism of $\Delta$ into $\Phi(\Delta)=\mathcal{P}_0$ . Is this of any help? Of course, the above arguments entail that $\mathcal{P}_0$ is a Borel subset of $(\mathcal{P},\tau_W)$ , but this, per se, does not guarantee that $\mathcal{P}_0$ is also $G_\delta$ .","['measure-theory', 'lebesgue-measure', 'absolute-continuity', 'polish-spaces']"
3416775,Explanation of an integration trick for $\int \frac{a_1 \cos x + b_1 \sin x}{a\cos x + b\sin x}dx$,"I'm not sure how to formulate my question correctly. Basically it comes from solving the integral: $$
\int \frac{a_1 \cos x + b_1 \sin x}{a\cos x + b\sin x}dx\\
a^2 + b^2 \ne 0
$$ I haven't been able to solve the integral without the trick I've found after a while. This trick suggests to rewrite: $$
a_1 \cos x + b_1 \sin x = \frac{a_1a + b_1b}{a^2 + b^2}(a\cos x + b\sin x) + \frac{a_1b - ab_1}{a^2 + b^2}(b\cos x - a\sin x)\tag{1}
$$ After using this trick the integral becomes almost elementary: $$
I = \int \frac{a_1a + b_1b}{a^2 + b^2} dx + \int \frac{a_1b - ab_1}{a^2 + b^2}\frac{b\cos x - a\sin x}{a\cos x + b\sin x} dx
$$ The first part is trivial, for the second one substitute $u = a\sin x + b\cos x$ . My question is how on earth one could arrive at $(1)$ . Is that some sort of well-known expression that I just missed? Going from RHS to LHS in $(1)$ is easy, but how do I make it the other way round? Thank you!","['integration', 'algebra-precalculus', 'real-analysis']"
3416806,Invariance of the x and y axis,"My question is about the invariance of the x and y-axis of the following system of differential equations: $$\begin{pmatrix}
\dot{x}(t)\\ 
\dot{y}(t)
\end{pmatrix} = \begin{pmatrix}
3x(t)\\ 
-2y(t)+x(t)^2
\end{pmatrix}$$ Now I want to know if the x-axis or the y-axis is invariant. My idea is to look at the dynamical flow as seen here: Flow of the system . Then we see that the flow is stable for the y-axis. So therefore if we have a set $C \subseteq \mathbb{R}^2$ , we see that $y(t) \in C$ and that $y(\tau) \in C$ for all $\tau \geq t$ . So therefore the the y-axis is invariant. While the x-axis is unstable and we can't be sure $x(t)$ will stay in the set. Therefore I think the y-axis is invariant, but the x-axis is not. Is it correct to see it this way? How can I show this mathematically, any material you can recommend that would improve my understanding? Thanks for the help, much appreciated! :)","['invariance', 'ordinary-differential-equations', 'dynamical-systems']"
3416818,Virginia Tech 2019 question,"Last week I took the Virginia Tech math contest, and the following was a problem: Let S be a subset of $\mathbb{R}$ with the property that for every $s\in S$ , there exists $\epsilon > 0$ such that $(s-\epsilon, s + \epsilon) \cap S = \{s\}$ . Prove that there exists a function $f: S \rightarrow \mathbb{N}$ that is one-to-one. I eventually figured out that one must simply use the density of rationals and define a function from S to $\mathbb{Q}$ and take the composition of that function with a function from $\mathbb{Q}$ to $\mathbb{N}$ . That seems to be the standard answer. My question is whether you can prove this by way of contradiction. Can you suppose there is some function $\varphi: \mathbb{R} \rightarrow S$ that is 1-1 and get a contradiction? I know this is certainly not a preferred technique in set theory for cardinality. Is it possible to arrive at a contradiction? I certainly could not think of an obvious, explicit conclusion.","['elementary-set-theory', 'general-topology', 'contest-math']"
3416826,Coin Flip: Expected number of flips,"Two Players are playing a coin flips game. The game ends when at least one player has a Head. Whoever gets the Head first would be the winner, and the other would be the loser. The loser will continue to flip until he gets a Head. What's the expected number of flips for the winner? What about the loser?  the probability of ending a game is 3/4 since the only case of continuing is both tails, which has a probability of 1/4; so the expected number of flips for the winner would be 1/(3/4) = 4/3. I'm wondering if the expected number of flips for the loser is (4/3) + 2","['statistics', 'probability']"
