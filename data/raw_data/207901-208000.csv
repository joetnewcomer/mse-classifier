question_id,title,body,tags
4164782,"Suppose we have $n$ points in a plane, not all on the same line. Then they are determining at least $n$ different lines.","Suppose we have $n$ points in a plane, not
all on the same line. Then they are determining at least $n$ different lines. Suppose points $T_1,...,T_n$ determine $m$ lines $$\ell_i(x,y) :\;\;a_ix+b_iy+c_i=0;\;\;\;1\leq i\leq m$$ and let $L_k$ be a set of all lines $\textbf{not}$ through $T_k$ and let $$p_k(x,y) = \prod _{\ell_i\in
L_k}\ell_i(x,y)$$ Then, since no line contains all points, the
degree of $p_k\leq m-1$ . Also we have $p_k(T_k) \neq 0$ and $p_j(T_k) =0$ for all $j\ne k$ which means that all $n$ polynomials are independent over vector space $\mathbb{R}_{m-1}[x,y]$ which has a dimension ${m+1\choose 2}$ . So $$n\leq {m+1\choose 2} \implies m\geq {\sqrt{8n+1}+1\over 2}$$ which is pretty weak bound. Any idea how to improve this approach? Edit: source","['extremal-combinatorics', 'combinatorial-geometry', 'combinatorics', 'polynomials', 'algebraic-combinatorics']"
4164787,"A ""reverse"" maximal inequality","The following is an exercise from Pollard's ""A user's guide to measure theoretic probability"". I have to clarify that Pollard uses $\mathbb{P}$ to denote expectation and he omits the indicator function from sets meaning that $\{ \max_{i \leq n} |X_i| \leq \epsilon \}$ stands for $1\{ \max_{i \leq n} |X_i| \leq \epsilon \}$ . Here is my attempt to tackle this following these hints. We define the stopping time $$ \tau = \begin{cases} \inf\{ i \leq n: |X_i| > \epsilon \} \\ n \quad  \text{if} \ |X_i| \leq \epsilon \ \text{for all} \ i \leq n \end{cases}. $$ Then $\{ \max_{i \leq n} |X_i| \leq \epsilon \} \subset \{ \tau = n \} $ so that $$ V_n 1 \{ \max_{i \leq n} |X_i| \leq \epsilon \} \leq V_{\tau} 1 \{ \tau = n\}  $$ and, by our assumptions, $$ \mathbb{E} V_\tau \leq  \mathbb{E} (1+\epsilon)^2 =  (1+\epsilon)^2, $$ as $|X_\tau| \leq |X_{\tau-1} - X_{\tau}| + |X_{\tau-1}| \leq 1 + \epsilon $ and $\tau$ is finite. Combining these facts now yields the desired result. Is my reasoning correct? Thank you.","['stochastic-processes', 'solution-verification', 'probability-theory', 'martingales']"
4164820,Conjecture with the CGM of a Circle,"CGM = Continuous Geometric Mean. Heuristics and mathematics are described in: Subset as arithmetic mean of geometric means. Not really? A shortcut to the question as presented here is: $$
\operatorname{CGM}(\vec{r}) = \exp\left(-\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right)
$$ where $\,\vec{p}(t)\,$ is a circle in the plane and $\,\vec{r} = (x,y)\,$ is another point in the plane. A similar exercise for the circle as previously for a straight line segment leads to the following expression: $$
\operatorname{CGM}(\xi,\eta)=
\exp\left( \frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi}\ln\left|[\cos(t)-\xi]^2+[\sin(t)-\eta]^2\right|\,dt\right)
$$ where dimensionless $\xi=x/R$ , $\eta=y/R$ ; $(x,y)=$ coordinates in the plane, $R=$ radius of circle. When this expression is fed into MAPLE (version 8),
then surprisingly we get outcome one , independent of any $(x,y,R)$ values: > exp(int(ln((cos(t)-xi)^2+(sin(t)-eta)^2),t=0..2*Pi,'continuous')/(2*Pi)); 1 On the other hand trying to proceed by substituting polar coordinates: $$
x = r\cos(\phi) \quad ; \quad y = r\sin(\phi) \\
\exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\left[\cos(t)-\frac{r}{R}\cos(\phi)\right]^2
+\left[\sin(t)-\frac{r}{R}\sin(\phi)\right]^2\right|\,dt\right)
\quad \Longrightarrow \\ \operatorname{CGM}(\rho) =
\exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\rho^2+1-2\rho.\cos(t-\phi)\right|\,dt\right)
\quad \mbox{with} \quad \rho=r/R
$$ Because of circle symmetry we can choose $\,\phi=0\,$ without loss of generality. Care should be taken if a zero is present in the argument of the logarithm, resulting in a singularity at that place: $$
\rho^2+1-2\rho.\cos(t)=0 \quad \Longrightarrow \quad \rho = \cos(t)\pm\sqrt{\cos^2(t)-1}
\\ \Longrightarrow \quad t = 0 \quad \mbox{and} \quad \rho = 1
$$ There are two approaches to this special case. The easiest one is to ignore the Cauchy principal value and remember that
the special case represents the value zero, according to the heuristics in the referenced Q&A . The second approach is to accept the Cauchy principal value as being essential.
With our numerical method, being the trapezium rule , integration at the interval $[0,\Delta t]$ is then calculated by: $$
\frac{f_1+f_2}{2}\Delta t \quad \mbox{with} \quad f_2 = \ln|2-2\cos(\Delta t)| \approx \ln|2-2(1-\Delta t^2/2)| = 2\ln|\Delta t|
$$ On the other hand the integral at that place can be calculated more (or less) exactly: $$
\frac{f_1+f_2}{2}\Delta t \approx \int_0^{\Delta t}\ln|2-2\cos(t)|\,dt \approx
\int_0^{\Delta t}2\ln|t|\,dt = 2(\Delta t\,\ln|\Delta t| - \Delta t) \\
\frac{f_1+2\ln|\Delta t|}{2}\Delta t \approx 2(\ln|\Delta t|-1)\Delta t \quad \Longrightarrow \quad f_1 \approx 2\ln|\Delta t|-4
$$ Having programmed all this (in Delphi Pascal) we get the following output.
Mind the two bold printed values at $I(1.0)$ : the first one without and the second one with the Cauchy principal value. NUMERICALLY               CONJECTURED
I(0.0) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.1) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.2) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.3) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.4) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.5) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.6) =  9.99999999999999E-0001 =  1.00000000000000E+0000
I(0.7) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.8) =  1.00000000000000E+0000 =  1.00000000000000E+0000
I(0.9) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(1.0) =  0.00000000000000E+0000 =  0.00000000000000E+0000
I(1.0) =  9.99967575938953E-0001 =  1.00000000000000E+0000 I(1.1) =  1.21000000000000E+0000 =  1.21000000000000E+0000
I(1.2) =  1.44000000000000E+0000 =  1.44000000000000E+0000
I(1.3) =  1.69000000000000E+0000 =  1.69000000000000E+0000
I(1.4) =  1.96000000000000E+0000 =  1.96000000000000E+0000
I(1.5) =  2.24999999999999E+0000 =  2.25000000000000E+0000
I(1.6) =  2.55999999999999E+0000 =  2.56000000000000E+0000
I(1.7) =  2.89000000000002E+0000 =  2.89000000000000E+0000
I(1.8) =  3.24000000000002E+0000 =  3.24000000000000E+0000
I(1.9) =  3.61000000000001E+0000 =  3.61000000000000E+0000
I(2.0) =  4.00000000000000E+0000 =  4.00000000000000E+0000
I(2.1) =  4.41000000000001E+0000 =  4.41000000000000E+0000
I(2.2) =  4.84000000000001E+0000 =  4.84000000000000E+0000
I(2.3) =  5.29000000000000E+0000 =  5.29000000000000E+0000
I(2.4) =  5.75999999999996E+0000 =  5.76000000000000E+0000
I(2.5) =  6.24999999999998E+0000 =  6.25000000000000E+0000
I(2.6) =  6.75999999999999E+0000 =  6.76000000000001E+0000
I(2.7) =  7.29000000000010E+0000 =  7.29000000000001E+0000
I(2.8) =  7.84000000000007E+0000 =  7.84000000000001E+0000
I(2.9) =  8.41000000000004E+0000 =  8.41000000000001E+0000
I(3.0) =  9.00000000000000E+0000 =  9.00000000000001E+0000 The numerical experiments give rise to the following conjecture (contradictory on purpose): $$
\operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 0 & \mbox{for} \quad \rho = 1 \\
\rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases}
$$ Is there someone who can prove this conjecture analytically? I have seen something with complex analysis in sci.math a long time ago (2008) but
didn't quite understand the argument. CGM for a Sphere In view of further development of the theory, it seems to be advantageous to define the Continuous Geometric Mean slightly different, namely as: $$
\operatorname{CGM}(\vec{r}) = \exp\left(- -\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right)
$$ For our unit circle (with Cauchy principal value) then we have instead: $$
\operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 1/\rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases}
$$ We seek to generalize the Continuous Geometric Mean for a circle to the same for the surface of a unit sphere: $$
\operatorname{CGM}(\vec{r}) = \exp\left(-\iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA/(4\pi)\right)
$$ Expressed in spherical coordinates and specializing (without loss of generality) for $\,\vec{r} = (0,0,\rho)\,$ : $$
\vec{p}-\vec{r} = \begin{bmatrix} \sin(\theta)\cos(\phi) \\ \sin(\theta)\sin(\phi) \\ \cos(\theta)-\rho \end{bmatrix}
\quad ; \quad dA = \sin(\theta)\,d\theta\,d\phi \\
\left|\vec{p}-\vec{r}\right|^2 = \sin^2(\theta)+\left[\,\cos(\theta)-\rho\,\right]^2 = 1-2\rho\cos(\theta)+\rho^2 \\
\iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA / (4\pi)=
\frac{2\pi}{4\pi} -\!\!\!\!\!\!\!\int_0^\pi \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta
\\ \Longrightarrow \quad
\operatorname{CGM}(\rho) = \exp\left(-\frac{1}{2} -\!\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta\right)
$$ Surprisingly enough, integration is much easier in 3-D, when compared with the 2-D case. Substitution of $\,t = \cos(\theta)\,$ gives a short route to the solution: $$
-\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta =
-\!\!\!\!\!\!\int_{-1}^{+1}\ln\left|\rho^2+1-2\rho\,t\right|\,dt = \\
\frac{1}{2\rho}\left[\;u\ln|u|-u\;\right]_{u=1+\rho^2-2\rho}^{u=1+\rho^2+2\rho} \quad \Longrightarrow \\
\operatorname{CGM}(\rho) = \exp\left(-\left[(1+\rho)^2\ln\left|(1+\rho)^2\right|-(1-\rho)^2\ln\left|(1-\rho)^2\right|-4\rho\right]/(4\rho)\right)
$$ If we make a graph of the two functions - red for 2-D, black for 3-D - then there is another surprise: the graphs coincide for large values of the normed radius $\rho$ . Confirmation is found with MAPLE, series expansion for $q=1/\rho\to 0$ : > f(q) := exp(-((1+1/q)^2*ln((1+1/q)^2)-(1-1/q)^2*ln((1-1/q)^2)-4/q)/(4/q));
> series(f(q),q=0); Output: $$
({q}^{2}-{\frac {1}{3}}{q}^{4}+O \left( {q}^{6} \right) )
$$","['integration', 'complex-analysis', 'real-analysis']"
4164824,"If $\alpha,\beta$ are roots of the equation $5(x-2020)(x-2022)+7(x-2021)(x-2023)=0$, then find $[\alpha]+[\beta]$","If $\alpha,\beta$ are roots of the equation $5(x-2020)(x-2022)+7(x-2021)(x-2023)=0$ , then find $[\alpha]+[\beta]$ , where $[.]$ represents the greatest integer function. I put $x-2020=t$ and got $12t^2-38t+21=0$ and got approximate roots as $2.5$ and $0.7$ So, I got $[\alpha]+[\beta]=4042$ , which is correct. I wonder if we can find the answer without calculating the roots. Let $a,b$ be the roots of the quadratic in $t$ . So, $a+b=\frac{19}{6}=3.2$ (approx.) $a=[a]+\{a\}$ , where $\{a\}$ is the fractional part of $a$ . So, $[a]+\{a\}+[b]+\{b\}=3.2$ $3.2$ can be split as $2+1.2$ or $3+0.2$ Without calculating the roots, can we confidently split $3.2$ to get the desired answer?","['quadratics', 'radicals', 'functions', 'roots']"
4164837,Why doesn't this proof of the Basel problem violate the geometric series convergence constraints?,"In a proof of the Basel problem , the excellent YouTuber ""blackpenredpen"" relies on a manipulation which I assume is valid, but I don't know why it is valid. They split the following integral: $$\int_0^{\pi/2}\ln(2\cos x)\space dx=\int_0^{\pi/2}\ln(e^{ix}+e^{-ix})\space dx=$$ $$\int_0^{\pi/2}\ln(e^{ix}(1+e^{-2ix}))\space dx=\int_0^{\pi/2}\ln(e^{ix})\space dx + \int_0^{\pi/2}\ln(1+e^{-2ix}))\space dx$$ And in the calculation of the second part of the integral on the right hand side, they use the series expansion: $$\ln(1+x)=\sum_{n=1}^\infty(-1)^{n-1}\frac{x^n}{n}$$ to represent $\ln(1+e^{-2ix})$ and note that this is only valid for $|x|\leq1, x\neq-1$ . Although $|e^{-2ix}|=1,\forall x$ , I noticed that the integral runs up to $\frac{\pi}{2}$ , and $e^{-2i\cdot\pi/2}=-1$ , which means that the series expansion does not converge (the logarithm goes to $\ln(0)$ which is very undefined!) at the upper bound of the integral. I imagine this is valid due to integrals being limits, and perhaps we ""approach"" $\frac{\pi}{2}$ without reaching it, but I'd like a formal explanation for why this expansion is valid here - I would like to learn when we can and cannot do this sort of thing!","['integration', 'divergent-series', 'convergence-divergence']"
4164840,Existential Quantifier Distributivity in First Order Logic,"It is well-known that the following formula holds for boolean functions involving free variables: $$\exists x,P(x)\wedge Q(x)\neq (\exists x_1,P(x_1))\wedge(\exists x_2,Q(x_2)),$$ $$\exists x,P(x) \vee Q(x)=(\exists x_1,P(x_1)) \vee (\exists x_2,Q(x_2)).$$ However, I wonder whether the following holds: $$\exists x,P(x)\wedge Q(x)= (\exists x_1,P(x_1))\wedge(\exists x_2,Q(x_2))\wedge (x_1=x_2)?$$","['first-order-logic', 'logic', 'discrete-mathematics', 'quantifiers']"
4164841,inflection points of a projective curve,"Let $C \subset \mathbb{P}^2$ be a non singular  projective curve of degree $d$ . Assume base field is $\mathbb{C}$ . Does there always exist a point $x \in C$ such that intersection multiplicity of tangent line- $T_x$ - at $x$ is $d$ . This is vacuously true if $d=2$ , the case of conics. When $d=3$ , Elliptic curves also have 3-torsion points, in fact 9 of them. Is this always true for higher degrees? Edit: Given $d \geq 4$ there always exist curves without any points having intersection multiplicity greater than 3. Here's a sketch of the argument. One starts by comparing the dimension of the space of all degree $d$ curves in $\mathbb{P}^2$ (which is $\mathbb{P}^N,N = {d+2\choose 2}-1$ ) and the closed subscheme of all degree $d$ curves having the required property. The codimension of closed subscheme will atleast be 1.","['algebraic-curves', 'algebraic-geometry']"
4164891,Can we write $f(\emptyset) = \{y\in Y|\exists x\in X(y=f(x))\}$?,"Let $f:X\to Y$ . $f(X):=\{y\in Y|\exists x\in X(y=f(x))\}$ . I think we can write $f(X):=\{y\in Y|\exists x\in X(y=f(x))\}$ if and only if $X\neq\emptyset.$ Let $X\neq\emptyset$ and $\Gamma\subset X\times Y$ be a graph of $f:X\to Y$ . Let $x\in X$ . Then there is a unique element $y\in Y$ such that $(x,y)\in\Gamma$ . We write this $y$ as $f(x)$ . Let $\Gamma\subset\emptyset\times Y=\emptyset$ be the graph of $f:\emptyset\to Y$ . Then, there is no $y$ such that $(x,y)\in\Gamma$ . So, I think we cannot write $f(x)$ in this case. So, I think we cannot write $f(\emptyset) = \{y\in Y|\exists x\in X(y=f(x))\}$ . Instead, I think we can write $f(\emptyset) = \{y\in Y|\exists x\in X((x,y)\in\Gamma)\}$ . Can we write $f(\emptyset) = \{y\in Y|\exists x\in X(y=f(x))\}$ ?",['elementary-set-theory']
4164899,$L^p$ convergence and interchange of limit and integral,"The question goes as follows: ""Let $f_{n}$ be sequences in $L^{2}$ function, with domain $(a,b)$ and Lebesgue measure.
Now, there is $f$ in $L^{2}(a,b)$ such that $\lim ||f_{n} - f||_{2}$ tends to $0$ as $n$ goes to infinity.
If $a$ and $b$ are each finite and $a \leq t \leq  b$ , then show: $$\int_{a}^{t}f(x) dx = \lim_{n \to \infty} \int_{a}^{t}f_{n}(x)dx$$ My Thoughts:
My first thought was that the switching of integral and limit would involve use of DCT. However, the $L^2$ convergence does not satisfy the condition for DCT (which requires convergence in a.e.). Therefore, I thought about "" $L^{2}$ implies $L^{1}$ convergence"" and reverse triangle inequality to get: $$||f_{n}||_{1} - ||f||_{1} \leq ||f_{n}-f||_{1}$$ and that $$\lim_{n \to \infty} \int_{a}^{t}|f_{n}|d\mu = \int_{a}^{t}|f(x)| d\mu$$ However, I think the way I approached is not what the question intended.
Thank you in advance!","['measure-theory', 'real-analysis']"
4164907,Action on tangent space,"Consider the action of the cyclic group $C_p$ on the projective line over a field of characteristic $p$ via $a\cdot[x:y]=[x+ay:y]$ . I am asked to describe the tangent space at infinity, and prove that the action of $C_p$ on the tangent space is trivial. Now I know that the tangent space is one-dimensional since the projective line is a nonsingular variety, and hence the tangent space is isomorphic to the ground field. I am not sure how to handle the other part of the question. A hint suggests that $C_p$ acts linearly on the tangent space, hence the action is just multiplication by a $p$ th root of unity, i.e. $1$ in this case. I do not understand how $C_p$ is expected to act linearly on the tangent space.","['group-actions', 'algebraic-geometry']"
4164970,An entropy inequality,"Denote $H(p) = -p \log_2 p - (1-p)\log_2(1-p)$ [Shannon entropy]. It is well-known that $H$ is a concave function that increases on $(0,\frac{1}{2})$ and decreases on $(\frac{1}{2}, 1)$ . Let $\beta, \beta_1, \beta_2 \in (0,\frac{1}{2})$ be real numbers such that $ H(\beta_1) + H(\beta_2) = 2 H(\beta).$ Let $r$ be a positive real number such that all values $\beta_1 + r, \beta_2 + r,\beta + r$ are less than $\frac{1}{2}$ . How to prove the following inequality? $$ H(\beta_1 + r - 2\beta_1 r) + H(\beta_2 + r - 2\beta_2 r) \ge 2H(\beta + r - 2\beta r)$$ UPD: Why I know that this inequality holds? Because it follows from rather deep result in Kolmogorov complexity theory . But I hope there is a simpler proof of this fact:)","['information-theory', 'convex-analysis', 'real-analysis']"
4165043,"Two Squares $S_1$ and $S_2$, are inscribed in the triangle $ABC$ as $S_1$ and $ABC$ share a common vertex $C$ and $S_2$ has one of its sides on $AB$.","Let $ABC$ be a right triangle with $\angle C = 90^\circ$ . Two Squares $S_1$ and $S_2$ , are inscribed in the triangle $ABC$ such that $S_1$ and $ABC$ share a common vertex $C$ and $S_2$ has one of its sides on $AB$ . Suppose that $[S_1] = 1 + [S_2] = 441$ , where $[]$ denotes the area. Find the value of $AC + BC$ . What I Tried :- Here is a picture of my copy I am posting showing whatever I tried with this problem. Everything done is given in the picture. As you can see, after doing all these I need to somehow relate that $x_2 + y_2 = x_1 + 21$ , and then $k_2 + z_2 = y_1 + 21$ , and then solve for $AC + BC$ . but I am not able to find a good way to proceed on how to do that. Can anyone help me? Thank You.","['euclidean-geometry', 'geometry']"
4165062,Very simple question about symmetric groups: is $\{e\} \subset \ldots \subset S_{n-3} \subset S_{n-2} \subset S_{n-1} \subset S_n$?,"Is it true to conclude that for any $n \in \mathbb{N}$ ; $$\{e\} \subset \ldots \subset S_{n-3} \subset S_{n-2} \subset S_{n-1} \subset S_n$$ Intuitively the answer clearly seems yes, since any set of permutations on $n$ letters will also have a subset of all permutations on $n-1$ letters. Does this always hold true?","['permutations', 'group-theory', 'symmetric-groups']"
4165089,"Elliptic curves classification, including the case of non-perfect fields.","(1) I am currently trying to find out how to classify all elliptic curves over an arbitrary field. Several questions have accumulated while I was trying to answer this question. If you don't want to read the whole question, only read part (3) or (4). A satisfying answer to one of these would solve my problem. (2) Can elliptic curve over imperfect field be reduced to Weierstrass form? This old question has never been answered, so I'm including it here. The easy definition of the j-invariant always uses the weierstrass-form. If the curve cannot be converted into weierstrass-form, how is the j-invariant defined in the first place? Is there a general way to do it that's independent of field and form? (3) It seems like twists are the way to classify curves that happen to be isomorphic over the closure (or the seperable closure?) of a given (arbitrary) field. In this thread Mr. Silverman does a great way to explain how this works and even how to define an invariant from that method: J-invariant and isomorphism of elliptic curves over $\mathbb{Q}$ Is it possible to extend/ modify Mr. Silvermans answer, so that the classification works in arbitrary fields? (4) I have tried to find an answer in the literature. The classification from Mr. Silvermans answer above is done in both his and in Mr. Husemöllers (Chapter 7) books, but, as far as I understand it, both only classify curves for perfect fields. It seems like something similar is possible for arbitrary fields, though. In the book ""Rational Points on Varieties"" by Poonen, Chapter 4, a theorem is included for twists of quasi-projective k-varieties. This classifies curves that become isomorphic over an arbitrary Galois extension and thus also over the separable closure. This is only useful, though, if we can classify the curves over the separable closure of a given field. As far as I understand it, to classify all the different curves that are isomorphic over the separable closure of a field would require some different method because the j-invariant will only work if the separable closure is equal to the algebraic closure (case of an perfect field). How do you classify all curves over the separable closure of a non perfect field? Secondly, this book is a little bit hard. To be honest, at the moment, I don't even understand enough scheme theory do verify with certainty that elliptic curves are quasi-projective varieties in the scheme-theoretic sense. Are they? Furthermore, the book is pretty technical. If this is the way to do the classification in the case of (truly) arbitrary fields, then my last question is, wether there is a less technical and maybe less general (as I only want to do it for elliptic curves) way to do it? Do you know of any good reference for this?","['elliptic-curves', 'number-theory', 'reference-request', 'field-theory', 'algebraic-geometry']"
4165100,"Let $a = 2019^{1009} , b = 2019!$ and $c = 1010^{2019}$","Let $a = 2019^{1009} , b = 2019!$ and $c = 1010^{2019}$ . Arrange $a,b,c$ in ascending order. What I Tried :- I have solved one of the parts. We have $2019 < 1010^2$ . $\implies 2019^{1009} < ({1010^2})^{1009} = 1010^{2018} < 1010^{2019}.$ Now, I am stuck on how to compare $a,c$ with $2019!$ . I tried it for $2$ hrs, but I still did not get any idea. Can someone help me? Thank You. Note :- I might have seen this comparison of $2019!$ before but my searching did not give me results, sorry about that.","['algebra-precalculus', 'problem-solving', 'inequality']"
4165121,Baby Rudin - Union of a countable set of countable sets,"My question of more of a sanity check for reading comprehension in Rudin's text. Here is a quick overview of Rudin's proof for the theorem 2.13: Theorem 2.13. Let $A$ be a countable set, and let $B_n$ be the set of all $n$ -tuples $\left(a_1,\dots,a_n\right)$ , where $a_k\in A$ ( $k=1,\dots,n$ ), and the elements $a_1,\dots,a_n$ need not be distinct. Then $B_n$ is countable. Proof. That $B_1$ is countable is evident since $B_1=A$ .
Suppose $B_{n-1}$ is countable ( $n=2,3,4,\dots$ ). The elements of $B_n$ are of the form $$
\left(b,a\right), \qquad\left(b\in B_{n-1},a\in A\right).
$$ For every fixed $b$ , the set of pairs $\left(b,a\right)$ is equivalent to $A$ , and hence countable. Thus $B_n$ is the union of of a countable set of countable sets. I understand everything else in the proof except the bolded sentence. What is the  union expression for $B_n$ , i.e. what are the sets that we are combining in the union? Reader is given the information that all $B_k$ , $k = 1,\dots,n$ are countable, but this can't surely mean that $B_n$ is the union of the other $B_k$ sets, as $B_n$ contains all $n$ -tuples?","['elementary-set-theory', 'real-analysis']"
4165128,how is this differentiation possible?,"A particle moves in the plane xy with constant acceleration a directed along the negative y–axis. The equation of motion of the particle has the form (where p and q are constants) $$y = px –{qx}^2$$ in my book they differentiated both sides with respect to time and solved it as $$\frac {dy}{dt}=p\frac{dx}{dt}+q\cdot 2x \cdot\frac{dx}{dt}$$ they later took $\frac{dx}{dt}$ as velocity in x-axis and solved it....
my question is that we dont know the relation of x with t.so how could we do $\frac{d}{dt}{qx}^2$ and get $q\cdot 2x\cdot\frac{dx}{dt}$ after differentiating?
i have tried asking the question before and my ques got closed as i didnt know mathjax.so plz dont downvote and tell me what i can do to better edit the question.","['calculus', 'derivatives']"
4165198,SDE representation implies differentiability? (Converse of Ito's Lemma),"I'm trying to establish that a function is differentiable when the value of the function follows an SDE. In a sense, I am trying to establish a converse of Ito's Lemma. More specifically, I know two pieces of information: I know that a process $(\xi_t)_{t\ge0}$ follows an SDE $$
d \xi_t = (\delta \xi_t - X_t)dt + \omega_t dW_t, 
$$ where $(W_t)_{t\ge 0}$ is a standard Brownian motion and $(X_t)_{t \ge 0}$ is some adapted process. $(\omega_t)_{t\ge 0}$ is also an adapted process. I also know that $\xi_t = G(Y_t, V_t)$ for a continuous function $G$ . Here, $(Y_t)_{t\ge 0}$ and $(V_t)_{t\ge 0}$ are two adapted processes, with volatilities $\sigma_Y(Y_t,V_t)$ and $\sigma_V(Y_t,V_t)$ , respectively. Based on this information, can I conclude that $$
   \omega_t = G_Y(Y_t,V_t) \sigma_Y(Y_t,V_t) + G_V(Y_t,V_t) \sigma_V(Y_t,V_t)?
$$ Or, more generally, can I conclude that the function $G$ is differentiable?","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'derivatives', 'stochastic-calculus']"
4165252,Distance from intersection inequality.,"Let $X$ be a Banach space and $Y$ and $Z$ closed subspaces of $X$ . Suppose that $Y+Z$ is closed in $X$ . Prove that there exists some $M > 0$ such that $\text{dist}(x, Y\cap Z) \leq M\left(\text{dist}(x, Y) + \text{dist}(x, Z)\right)$ for all $x\in X$ . Attempt: Since $Y$ , $Z$ and $Y+Z$ are closed subspaces we know that $Y\times Z$ and $Y+Z$ are Banach spaces where $\lVert (y,z)\rVert_{Y\times Z} = \lVert y \rVert + \lVert z \rVert$ . Define $T:Y\times Z \to Y + Z$ with $T(y,z) = y+z$ . Clearly $T$ is surjective, linear and bounded operator. From the open mapping theorem we deduce that there exists some $M > 0$ such that for all $x \in Y+Z$ there exist $y \in Y$ and $z \in Z$ such that $x=y+z$ and $\lVert y \rVert + \lVert z \rVert \leq M \lVert x \rVert$ . I can't continue form there. Any hint would be appreciated. Thanks.","['banach-spaces', 'operator-theory', 'functional-analysis', 'real-analysis']"
4165265,Principal value integral by contour integration,"I'm preparing for an exam and am trying to evaluate the following practice integral: $$P\int_{-\infty}^\infty \frac{e^{-ix}}{x^n} \, dx $$ where $n$ is a positive integer. $P$ here denotes that the integral is the Principal Value kind. I can do the $n=1$ using the Indentation Lemma (or using the famous Dirichlet integral ) but for $n>1$ the Indentation Lemma (or doing it directly with the same contour) seems to fails. The contour I am trying is contour along the real line completed by a semi-circle in the lower half plane, with an semi circle indent at $z=0$ . A solution would be very much appreciated.","['complex-analysis', 'contour-integration', 'complex-integration']"
4165326,"If $|f(x)|\leq |g(x)|$, then is it true that $|\int_a^b f(x)|\leq|\int_a^b g(x)|?$","If $|f(x)|\leq |g(x)|$ then $|\int_a^b f(x)| \leq |\int_a^b g(x)|$ . I know that by Triangle inequality, $$\left|\int_a^b f(x)\right|\leq\int_a^b\left|f(x)\right|$$ and that $$\left|\int_a^b g(x)\right|\leq\int_a^b\left|g(x)\right|$$ It's true that $\int_a^b\left|f(x)\right|\leq\int_a^b\left|g(x)\right|$ , right? But is it necessarily true that $$\left|\int_a^b f(x)\right|\leq\left|\int_a^b g(x)\right|?$$","['integration', 'calculus', 'real-analysis']"
4165349,Exercise 1.24 from Joe Harris' Algebraic Geometry: First Course,"I have a question about solvability of Exercise 1.24 (p. 14) from Joe Harris' Algebraic Geometry: A First Course and correctness of following 'synthetic' construction which according to the book (or alternatively due to page 79 in this script ) should give us a rational normal curve. (Since we want to work as geometric as possible, all constructions and spaces are defined over $\mathbb{C}$ ) Harris wrote: As indicated, we can generalize this to a construction of rational normal curves
in any projective space $\mathbb{P}^d$ . Specifically, start by choosing $d$ codimension two linear spaces $ \Lambda_i \cong \mathbb{P}^{d-2} \subset \mathbb{P}^d$ .
The family $\{H_i(\lambda)\}$ of hyperplanes in $\mathbb{P}^d$ containing $\Lambda_i$ is
then parameterized by $\lambda \in \mathbb{P}^1$ ; choose such parameterizations, subject
to the condition that for each $\lambda$ the planes $H_1(\lambda), ... , H_d(\lambda)$ are independent,
i.e., intersect in a point $p(\lambda)$ . It is then the case that the locus of these points $p(\lambda)$ as $\lambda$ varies in $\mathbb{P}^1$ is a rational normal curve. Exercise 1.24. Verify the last statement So our constructed curve $C$ is given by $$ C:= \bigcup_{\lambda \in \mathbb{P}^{1}} H_1(\lambda) \cap ... \cap H_d(\lambda)  $$ On page 10 Harris gave the standard definition of a rational normal
curve : A rational normal curve is defined as the image
of the map $v_d: \mathbb{P}^1 \to \mathbb{P}^d; 
\lambda \mapsto [A_0(\lambda):...: A_d(\lambda)]$ with an arbitrary basis $A_0, ... , A_d$ of the space of
homogeneous polynomials of degree $d$ on $\mathbb{P}^1 $ . Note that this curve is projectively equivalent to the image of the Veronese map $[X_0:X_1] \mapsto [X_0^d: X_0^{d-1}X_1:..., X_1^d]$ . The exercise: Suppose we start by choosing $d$ codimension two linear
spaces $\Lambda_i \cong \mathbb{P}^{d-2} \subset \mathbb{P}^d, 
i=1,...,d$ ( here I doubt that the $\Lambda_i$ can be choosen really arbitrary!) and consider $d$ families of hyperplanes $\{H_i(\lambda) \}_{\lambda \in \mathbb{P}^1}, i=1, ..., d$ where each $ H_i(\lambda)$ contains $\Lambda_i$ parametrized by the line $\mathbb{P}^1$ such that for each $\lambda \in \mathbb{P}^1$ the planes $H_1(\lambda),..., H_d(\lambda)$ are linear independent,
i.e. intersect in a point $p(\lambda)$ . That is we have to check that the constructed curve $C$ above can be realized as
the image of such map $v_d$ , that means that the map $$ p: \mathbb{P}^1 \to \mathbb{P}^d,
\lambda \to p(\lambda)=  H_1(\lambda) \cap ... \cap H_d(\lambda) $$ has the form $\lambda \to [A_0(\lambda):...: A_d(\lambda)]$ for
appropriate basis $A_0,..., A_d$ of
the space of homogeneous polynomials of degree $d$ on $\mathbb{P}^1$ . The way I tried to solve it contains a serious problem, see below. We parametrize every linear space $\Lambda_i$ as vanishing set of two independent
linear polynomials $L_i, M_i \in \mathbb{C} \cdot Z_0 + \mathbb{C} \cdot Z_1... + 
\mathbb{C} \cdot Z_d $ . So $\Lambda_i= V(L_i, M_i)$ . Then every our pencil $\{H_i(\lambda) \}_{\lambda \in \mathbb{P}^1}$ can be parametrized as vanishing
set $$ \lambda_0 L_i + \lambda_1 M_i =0 $$ with running $\lambda=[\lambda_0: \lambda_1]$ . If we reordner the terms after
variables $Z_i$ we obtain $d$ equitions $$   \lambda_0 L_i + \lambda_1 M_i = 
r_{i,Z_0} (\lambda)Z_0+ r_{i,Z_1} (\lambda)Z_1 + ...
+ r_{i, Z_d} (\lambda)Z_d =0  $$ We can naturaly encode the coefficents of $i$ -th family as $i$ -th row
of following matrix $R \in Mat_{d+1}(\mathbb{C}[\lambda])$ ; the $(n+1)$ -row
we fill with zeroes: $$ \begin{pmatrix}
r_{1,Z_0} (\lambda) & r_{1,Z_1} (\lambda) & ... & r_{1,Z_d} (\lambda) \\
r_{2,Z_0} (\lambda) & r_{2,Z_1} (\lambda) & ... & r_{2,Z_d} (\lambda)\\
\\ ...  \\
\\ ...  \\
r_{d,Z_0} (\lambda) & r_{d,Z_1} (\lambda) & ... & r_{d,Z_d} (\lambda)\\
0 & 0 & ... & 0 \\
\end{pmatrix} $$ Since for every $\lambda$ the $H_1(\lambda), ..., H_d(\lambda)$ are independent,
this matrix $A$ has rank $d$ . What we do next looks promissing at first glance; we consider the
adjugate matrix $A$ of $R$ , that's a unique matrix $A \in Mat_{d+1}(\mathbb{C}[\lambda])$ with $$ AR=RA= det(R) \cdot I_{n+1} = 0   $$ By definition of adjugate matrix it's $d+1$ -th column $A_{\cdot, n+1}$ provides
exactly the solution we are looking for: for every $\lambda$ it is up to
multiplication by a scalar $c \neq 0$ the solution of linear equations
encoded by matrix $R$ and every entry of this column is by construction
a homogeneous polynomial of degree $d$ in $\lambda_0, \lambda_1$ , let
set $$  (A_0(\lambda), ... , A_d(\lambda)):= A_{\cdot, n+1}^T $$ and pass to it's homogenization. Problem (a really serious one): Why are the $A_i $ linearly independent,
equivalently why they form a basis of the space of homogeneous deg $d$ polynomials
in $\lambda_0, \lambda_1$ ? (see also the comment by lhl73 in this closely related question dealing with nondegeneracy of curves of this type separately) Indeed, that's equivalent to the property that the image of $\lambda \mapsto [A_0(\lambda):...: A_d(\lambda)]$ is not contained in
a proper hyperplane $H \cong \mathbb{P}^{d-1} \subset \mathbb{P}^{n-1}$ . And if we looking again at the construction, Harris nowhere imposed
additional assumptions how the $\Lambda_i$ are related to each other;
it doesn't rule out eg the bad case $\Lambda_1= \Lambda_2$ , since
one can find still two families $\{H_i(\lambda)\}, i=1,2$ of hyperplanes
containing $\Lambda_1 (=\Lambda_2)$ with $H_1(\lambda) \neq H_2(\lambda)$ for
every $\lambda$ , so the imposed condition is not violated. But eg if we choose $\Lambda_1= \Lambda_2$ by construction the complete curve $C$ would be
comtained in $\Lambda_1$ , therefore the $A_i(\lambda)$ constructed as above
will cannot be linearly independent and the curve will not be rational normal
curve as defined by Harris on page 10. Question: Does anybody have experience with this Exercise 1.24 and
knows how to solve it correctly, or if it's indeed true that the
quoted construction not always gives a rational normal curve in sense of
Harris book? (there is also nowhere Errata of this book available) Probably Harris also has forgotten to impose an additional assumption
on the spaces $\Lambda_i$ in the construction above or I'm just too stupid to solve the exercise & understand the construction. If that's so, can the construction be slightly modified in a most general way when one would obtain always a rational normal curve? E.g. does the construction give us always rational normal curve if we additionally require that all $\Lambda_i$ should be distinct? In any case I would be very thankful if anybody who has experience with this
Exercise and construction would share how it can be solved correctly.","['algebraic-curves', 'curves', 'projective-schemes', 'algebraic-geometry', 'schemes']"
4165358,What is the minimum size of a region that can be tiled by every polyomino on up to $4$ cells?,"I am interested in regions that can be tiled by all $k$ -ominoes for each $k\le n$ . If we take $n=3$ , it is obvious that the $2\times 3$ rectangle is the minimal region that can be tiled by the monomino, the domino, the $L$ -tromino, and the $1\times 3$ rectangle. If we take $n=5$ , any such region must be unbounded: consider the interaction between the $X$ pentomino and the $2\times 2$ tetromino. But at $n=4$ the question is rather interesting. There do exist solutions, like the following $96$ -celled region formed from four $2\times 12$ rectangles: However, I suspect this is not minimal; I am curious whether improvements can be made. As asked in this related question and proven here , there cannot be any simply-connected solutions. What is the smallest such region known? This page has many related problems, but does not seem to tackle this one.","['polyomino', 'geometry', 'tiling']"
4165365,"Prove that $f(t)=0$ for almost every $t\in [0,1]$","I would be glad if someone could help me to solve the following exercise. Let $f:[0,1]\to\Bbb R$ be a bounded measurable function such that $\int_0^1f(t)e^{nt}dt=0$ for every $n=0,1,2,\dots$ Prove that $f(t)=0$ for almost every $t\in [0,1]$ . I know that if $f$ is nonnegative then for $n=0$ the integral $\int_0^1f(t)dt=0$ implies that $f(t)=0$ for almost all $t\in[0,1]$ . Thanks!","['measure-theory', 'real-analysis']"
4165390,Can a function $f$ be continuous at a point $a$ even if $a$ is not a limit point of $f$?,"Suppose we adopt the following definitions of a limit and continuity: Definition of a Limit Suppose a function $\DeclareMathOperator{\epsilon}{\varepsilon}f:D\mapsto\mathbb{R}$ has a limit point at $a$ , i.e. for every $\delta>0$ there exists $x\in D$ such that $0<|x-a|<\delta$ . We say that $\lim_{x \to a}f(x)=L$ iff For every $\epsilon>0$ there exists $\delta>0$ such that, for all $x$ , if $x\in D$ and $0<|x-a|<\delta$ , then $|f(x)-L|<\epsilon$ . Definition of continuity Suppose a function $f:D\mapsto\mathbb{R}$ is defined at the point $a$ . We say that $f$ is continuous at $a$ iff For every $\epsilon>0$ there exists $\delta>0$ such that, for all $x$ , if $x\in D$ and $|x-a|<\delta$ , then $|f(x)-f(a)|<\epsilon$ . The problem I have is this: the definition of continuity is often abbreviated to $f:D\mapsto\mathbb{R}$ is continuous at $a$ iff $\lim_{x \to a}f(x)=f(a)$ . However, according to the definitions I have presented, it seems that there might be cases where $\lim_{x \to a}f(x)$ does not exist, and yet it is still the case that $f$ is continuous at $a$ . For example, consider the function $\phi=\{(0,0)\}$ . This function is trivially continuous at $0$ , but $\lim_{x \to 0}\phi(x)$ does not exist because $0$ is not a limit point of $\phi$ . So are the two definitions of continuity only equivalent if $a$ is a limit point of $f$ ? I have one further question. The definition of a limit requires that $a$ be a limit point of $f$ . Is this restriction necessary because otherwise we could argue things like $\lim_{x \to 0}\phi(x)=1$ and $\lim_{x \to 0}\phi(x)=2$ are both vacuously true?","['limits', 'calculus', 'definition', 'real-analysis']"
4165492,Why does $0.888888888889 \times 9 = 8$?,"So I'm teaching myself maths, watching alot of youtube videos about topics way beyond my head. I'm trying to unlearn the rigid way school taught maths, such as the rules and procedures to solve specific problem, which I hated. Feynman said maths is 'too abstract' and some book somewhere said  'math is about logical reasoning and pattern recognition', which I enjoy. I like to think logically and abstractly but dont have a way to show it or express it (except with coding and basic social skills). Anyway here's my question: Given $0.888888888889 \times 9 = 8$ $0.88888888889 \times 9 = 8.00000000001$ $0.8888888889 \times 9 = 8.0000000001$ ... $0.8889 \times 9 = 8.0001$ $0.889 \times 9 = 8.001$ $0.89 \times 9 = 8.01$ First question why are eleven $8$ 's the magic number for it to be a whole number again? What is the relationship between $8$ 's and the $0$ 's on the output when there are less than eleven $8$ 's? Why are they the same when less than eleven? Then why is more the eleven $8$ 's still equal $8$ ? Any sources would be great, like what field of mathematics deals with these kinds of questions, its because the calculator blah blah cant calculate that high, the history behind it showing how these things work etc. Also if this is too dumb of a question, how? (do you see things in your head that just makes sense of these symbols?) What are good math questions?  Why did you get into maths? Again I just want to learn maths, working as business rule translator (software developer) is kinda boring and I feel like I'm just coasting, if i can understand maths better then hopefully I at least have a chance to learn/work on more interesting topics like theoretical computer science (algorithms specifically), theoretical physics (how the universe works), dynamical systems (how unpredictable phenomena work), bioinformatics etc.","['algebra-precalculus', 'learning']"
4165556,What is the equivalent of generating a topology via a basis or subbasis for the closure operator definition of a topology?,"What is the equivalent to generating a topology with a basis for closure operators satisfying the Kuratowski closure operators. Let $X$ be a topological space. Let $c : 2^X \to 2^X$ be its closure operator. The set of open sets definition of a topology, as well as the set of closed sets definition, has a notion of generating a topology via a basis or subbasis . I'm wondering if there are similar construction for closure operators. Here's one I can think of off the top of my head that sends $f : 2^X \to 2^X$ to $f^* : 2^X \to 2^X $ . I think it works but I'm not sure. Then we define $f^*$ as follows: Let $g : 2^X \to 2^X$ be a closure operator on $X$ . Let's say that $g$ is compatible with $f$ if and only if: $$ f(A) \subset g(A) \;\; \text{for all $A \subset X$ where $A \neq \varnothing$} $$ $f$ is always compatible with the $g$ that sends every non-empty set to $X$ , the whole space. Therefore there is always at least one $g$ that is compatible with $f$ . We compute $f^*$ in the following way. $$ f^*(A) = \bigcap \bigg\{ g(A) : \text{$g$ is a closure operator and $g$ is compatible with $f$} \bigg\} $$ I will call each $g$ in the comprehension a component of $f^*$ . $f^*$ sends $\varnothing$ to $\varnothing$ . $f^*$ is extensive because all of its components are extensive. $f^*$ probably respects binary unions. Let $\varphi$ be the sentence $g$ is compatible with $f$ . $$ f^*(A \cup B) \\
\bigcap\{g(A \cup B) : \varphi \} \\
\bigcap \{ g(A) \cup g(B) : \varphi \} \\
\textit{This step is suspicious} \\
\big( \bigcap \{g(A) : \varphi \} \big) \;\bigcup\; \big(\bigcap \{g(B) : \varphi \} \big) $$ I am pretty sure that there can't any elements of $f^*(A \cup B)$ that are not members of $f^*(A)$ or $f^*(B)$ , but I'm not sure how to prove it. Here is my attempt to prove idempotence. It has one suspicious step. $$ f^*(f^*(A)) $$ Replace the inner $f^*$ with its definition. $$ f^*\left(\bigcap\{g(A) : \varphi\}\right) $$ Replace the outer $f^*$ with its definition. $$ \bigcap \left\{ g' \left( \bigcap \left\{ g(A): \varphi \right\} \right) : \varphi' \right\} $$ I don't trust this step. I am pretty sure the outermost closure operator distributes over the intersection by the properties of closed sets, but I am calling it out here. $$ \bigcap \left\{ g' \left( g \left( A \right) \right) : \varphi \land \varphi' \right\} $$ For a single closure operator, $g \circ g = g$ holds. Additionally, the composition of any two closure operators is a closure operator. Therefore, this construction hits all and only the closure operators. $$ \bigcap \left\{ g \left( A \right) : \varphi \right\} $$ which gives us the following as desired. $$ f^*(\varphi) $$ Here is the original definition I had for $f^*$ . It definitely does not work. $$ f^*(A) = \bigcap \bigg\{ {\cup}\big\{ f^k(Z) : k \in \mathbb{N} \big\} : A \subset Z \subset X  \bigg\} $$",['general-topology']
4165570,Intuitive proof of surface area of a sphere in relation to area of great circle using shadow.,"I'm trying to show my kids intuitive proofs of common formulas, and watched this excellent video from 3Blue1Brown: But why is a sphere's surface area four times its shadow? However, I didn't find the 'shadow' explanation intuitive (or I misunderstood it), and came up with an alternative, though I'm quite sure it's been thought before. Take a sphere and cut it and half leaving a dome, and illuminate it from above. Split the dome into equal horizontal rings, and note that each ring shades the 'ground' using some percentage of it's surface.  For the ring at the top of the dome, the shadow will be 100% of the surface of the ring, whereas for the ring at the ground the shadow will be 0% of the surface, since the ring is edge-on to the light. Since the dome is completely symmetrical we can take the 'average' shadow factor as 50%. Therefore the surface area of the dome is twice the area of the great circle, and the full sphere is 4 times. Is there a formal proof of this idea, and what is it called, or is it just a coincidence, and which part(s) of the argument are wrong?","['spheres', 'area', 'circles', 'geometry']"
4165577,Is the absolute value of the sum of six of the 16th roots of unity ever a nonzero integer?,"Let $\zeta_1, \ldots \zeta_{16}$ be the $16$ th roots of unity. For the proper subset $J \subset \{1,2,\dots,16\}$ and $|J|=6$ , can the following sum ever be satisfied for an integer not equal to zero? $$\left\lvert\sum_{j\in J} \zeta_j\right\rvert=C \in \mathbb{N}^+.$$ I think the answer is negative but have been unable to finish the following argument. Squaring both sides yields the real sum $$\sum_{j,k\in J} \zeta_j \zeta_k^* = \sum_{j,k} \cos\left(\frac{2\pi(j-k)}{16}\right) = C^2  $$ which after removing the conjugate product terms yields $$ \sum_{j\neq k} \zeta_j \zeta_k^* = C^2 - |J| . $$ I was thinking of then proceeding using the cardinality of $J$ and that $ \cos\left(\frac{2\pi}{16}\right)$ is irrational to show that no such sum is possible but keep going in circles. I suspect this question is very simple using algebraic number theory but I am still a novice there. Any suggestions on how to move forward would be appreciated, particularly using number theory.","['galois-theory', 'algebraic-number-theory', 'group-theory', 'cyclotomic-fields']"
4165584,Probability conditioned on two independent events,"Let $B$ and $C$ be independent events. We are interested in $P[A|B \cap C]$ . We know what $P[A|B]$ and $P[A|C]$ are. How do we use these two to find the answer? What additional missing pieces, if any, do we need? I arrived at $$ P[A|B \cap C] = \frac{P[A \cap B \cap C]}{P[B]P[C]}. $$ But I have not been able to make use of the given quantities in some way.","['conditional-probability', 'bayes-theorem', 'probability']"
4165595,About the inequality $x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12$,"Problem : Let $x > 0$ . Prove that $$x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12.$$ Remark 1 : The problem was posted on MSE (now closed ). Remark 2 : I have a proof (see below). My proof is not nice.
For example, we need to prove that $\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0$ for all $0 < x < 1$ for which my proof is not nice. I want to know if there are some nice proofs. Also, I want my proof reviewed for its correctness. Any comments and solutions are welcome and appreciated. My proof (sketch) : We split into cases: i) $x \ge 1$ : Clearly, $x^{x^{x^{x^{x^x}}}}\ge x^x$ .
By Bernoulli's inequality, we have $x^x = (1 + (x - 1))^x \ge 1 + (x - 1)x = x^2 - x + 1 \ge \frac12 x^2 + \frac12$ . The inequality is true. ii) $0 < x < 1$ : It suffices to prove that $$x^{x^{x^{x^x}}}\ln x \ge \ln \frac{x^2 + 1}{2}$$ or $$x^{x^{x^{x^x}}} \le \frac{\ln \frac{x^2 + 1}{2}}{\ln x}$$ or $$x^{x^{x^x}}\ln x \le \ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}$$ or $$x^{x^{x^x}}\ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}.$$ It suffices to prove that $$x^{x^{x^x}}\ge \frac{7}{12} \ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}. \tag{1}$$ First, it is easy to prove that $$x^x \ge \mathrm{e}^{-1/\mathrm{e}}
\ge \frac{1}{\ln x}\ln\frac{\ln\frac{7}{12}}{\ln x}.$$ Thus, the left inequality in (1) is true. Second, let $f(x) = x^{7/12}\ln x - \ln \frac{x^2 + 1}{2}$ . We have \begin{align*}
	f'(x) &= \frac{7}{12x^{5/12}}
	\left(\ln x + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\
	&\le \frac{7}{12x^{5/12}}
	\left(\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\
	&\le 0 \tag{2}
\end{align*} where we have used $\ln x \le \frac{3x^2 - 3}{x^2 + 4x + 1}$ for all $x$ in $(0, 1]$ .
Also, $f(1) = 0$ . Thus, $f(x) \ge 0$ for all $x$ in $(0, 1)$ .
Thus, the right inequality in (1) is true. Note : For the inequality $\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0$ for all $0 < x < 1$ ,
we let $x = y^{12}$ and it suffices to prove that $11y^{47} + \cdots + 3 \ge 0$ (a polynomial of degree $47$ , a long expression) for all $0 < y < 1$ . We are done.","['calculus', 'inequality', 'real-analysis']"
4165642,What does the symbol ⇴ mean?,I am studying Discrete Mathematics and I encountered this symbol $f:N^k$ ⇴ $N$ in a function definition about Turing machines. What does it mean?,"['functions', 'discrete-mathematics', 'turing-machines']"
4165691,How do I prove $\int_a^b |f(x)|^2 dx \leq \frac{(b-a)^2}{2} \int_a^b |f'(x)|^2 dx$. [duplicate],"This question already has answers here : Show that $\int_{a}^{b} |f|^2\le\frac{(b-a)^2}{2}\int_{a}^{b} |f'|^2$ (2 answers) Closed 3 years ago . Let $f$ be $C^1[a,b]$ continuously differentiable) with $f(a)=0$ . [This may be generalized to $f(b)=0$ or $f(a)f(b)=0$ ] I want to show $$\int_a^b |f(x)|^2\ \mathrm{d}x\ \leq\ \frac{(b-a)^2}{2} \int_a^b
 |f'(x)|^2\ \mathrm{d}x.$$ Since $f$ is differentiable, it is continuous so it is integrable.  And since $f'$ is continuous so it is integrable.  So, L.H.S and R.H.S are well defined. My first trial was the usage of integration by parts, but since I am dealing with $|\cdot |$ , the absolute value of a function, it seems it is not a good direction.",['analysis']
4165697,$\arctan(\frac{x+1}{x-1})$ to power series,"EDIT: I just realize that I should start with the derivative of $\arctan(\frac{x+1}{x-1})$ , and keep going from there. So $(\arctan(\frac{x+1}{x-1}))'=\frac{1}{1+x^2}$ , does that mean  this is the same series as $\arctan(x)$ ? -- I want to find an expression for $\arctan(\frac{x+1}{x-1})$ as a power series, with $x_0=0$ , for every $x \ne 1$ . My initial thought was to use the known $\arctan(x)=\sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{2n+1}$ , but I don't know how to keep going if I replace $x$ with $\frac{x+1}{x-1}$ . Thanks a lot!","['trigonometric-series', 'calculus', 'functions', 'sequences-and-series', 'trigonometry']"
4165713,Prove that $(f\circ g)(1)\leq (f\circ g)(2)\leq \cdots\leq (f\circ g)(n)$ for some $g$.,"Let $n\in\Bbb N$ . Prove that for every function $ f: J_n \to \Bbb R $ , there exists a bijective function $ g: J_n \to J_n $ such that $$(f\circ g)(1)\leq (f\circ g)(2)\leq \cdots\leq (f\circ g)(n).$$ Note: $J_n=\{1,2,3,\ldots,n\}.$ I try to prove that by induction. For $n=1$ , there's nothing to do. For $n=2$ , We have to show that for every function $ f: J_2 \to \Bbb R $ , there exists a function $ g: J_2 \to J_2 $ such that $$ (f \circ g) (1) \leq (f \circ g) ( 2). $$ Let's fix the function $ f: J_2 \to \Bbb R $ given by $ f (1) = r_1 $ and $ f (2) = r_2 $ , where $ r_1, r_2 \in \Bbb R $ . Without loss of generality, let's take $ r_2 \leq r_1 $ , then we can define the function $ g: J_2 \to J_2 $ as $ g (1) = 2 $ and $ g (2) = 1 $ . And we would have that $ f (g (1)) = f (2) = r_2 $ and $ f (g (2)) = f (1) = r_1 $ and clearly what we want is fulfilled. Some help for the inductive step would be appreciated, I really don't know how to do that part.","['inequality', 'functions', 'cardinals']"
4165717,Find unbiased estimator of $\theta$ when $f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }}$,"Let $f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }}$ be a probability density function of sample $(X_{1},X_{2},...,X_{n})$ where $x\geq 0$ . Find unbiased estimator for $\theta$ . Here is my solution : $E\left [ \bar{X} \right ] = E\left [ X \right ]=\frac{1}{\theta }\int_{0}^{\infty }2x^{2}e^{\frac{-x^{2}}{\theta }}dx=\frac{1}{\theta }\int_{0}^{\infty }xe^{\frac{-x^{2}}{\theta }}2xdx$ Let $$  t=\frac{x^{2}}{\theta } (t=\frac{x^{2}}{\theta }\Rightarrow \theta t=x^{2}\Rightarrow x=\sqrt{\theta t},x\geq 0) , dt=\frac{2x}{\theta }d\theta \Rightarrow \theta dt=2xdx$$ $=\frac{1}{\theta }\int_{0}^{\infty }(\theta )^{\frac{1}{2}}e^{-t}\theta dt=\frac{\theta ^{\frac{3}{2}}}{\theta }\int_{0}^{\infty }t^{\frac{1}{2}}e^{-t}dt=\sqrt{\theta }.\Gamma (\frac{3}{2})=\sqrt{\theta }.\frac{\sqrt{\pi}}{2}$ $\Rightarrow E\left ( \bar{X} \right )=\sqrt{\theta }.\frac{\sqrt{\pi}}{2}\Rightarrow E(\frac{4\bar{(X)}^{2}}{\pi})=\theta $ Hence unbiased estimator for $\theta$ is $\hat{\theta }=\frac{4\bar{(X)}^{2}}{\pi}$ Is my solution right? I am not sure the calculation. Any idea will be appreciated.","['expected-value', 'statistics', 'probability-distributions', 'parameter-estimation']"
4165753,Generalized Method to find nth power of matrix in $P^n = 5I - 8P$,"Question: Let $I$ be an identity matrix of order $2 \times 2$ and $P = \begin{bmatrix}
    2 & -1 \\
    5 & -3 \\
   \end{bmatrix}$ . Then the value of $n ∈ N$ for which $P^n = 5I - 8P$ is equal to _______. Answer: 6 Question Source: JEE Mains $18^{th}$ March Shift-2 2021 By characteristic Equation: $P^2-(\operatorname {Tr}(P))P+(\det (P))I=0$ we can find $n=6$ by hit and trial. i.e. multiplying equation by P gives $P^3=(\operatorname {Tr}(P))P^2-(\det (P))P = (\operatorname {Tr}(P))[(\operatorname {Tr}(P))P+(\det(P))I]$ And going on solving for $P^6$ . But is there a generalized method because it can't be done for high values of $n$ (eg: $100$ )?","['matrices', 'matrix-equations', 'linear-algebra', 'contest-math']"
4165754,Extreme limit of sum of Faddeeva function,"I am looking for the proof of $$
\lim_{t \to +0} \left[ \log t + 2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n} 
 - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} \right] = \log \frac{\mathrm{e}^{\gamma/2}}{\pi A}~,
$$ with $\omega_n = (2n + 1)\pi~$ and $~A>0$ . Here, $\mathrm{W}(z)$ is the Faddeeva function given by $$
\mathrm{W}(z) = \mathrm{e}^{-z^2} \mathrm{erfc}(-iz)~.
$$ The proof of this equation is troubling to me because the equation is necessary for the Eilenberger equation of superconductivity to give the value of the upper critical field at absolute zero. For example, using a function similar to the Fermi distribution function, $$
F(z) = \frac{1}{{\mathrm e}^{z} + 1} ,
$$ the infinite sum can be rewritten as the sum of the residues of a complex function. After the substitution $i\omega_{n} \to z$ and the analytic continuation that move $z$ on the real axis, the equation can be written as $$
2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n} 
 - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} = -\int_{-\infty}^{\infty} {\mathrm d}z ~ F(z) \left\{ \frac{1}{z} + i\sqrt{\pi} A t {\mathrm W}(zAt)\right\}.
$$ By using this, I was able to verify the limit value numerically. However, even in this form, it is difficult to evaluate the limit value of $t \to +0$ analytically. How can I prove the equation? I am not quite sure how to proceed. Thank you.","['limits', 'calculus']"
4165766,$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}$ to function,"I want to calculate $3-\frac{5}{2}+\frac{7}{24}-\frac{9}{720}+...$ , while using $\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}$ . Here is my attempt: Firs I proved that the radius of converges for this series is $R= \infty$ . $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n} =~_{k=n-1} ~ = \sum_{k=0}^\infty \frac{(-1)^{k+2}}{(2k)!}(2k+3)x^{2k+2} = \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}(x^{2k+3})'$$ $$= (\sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k+3})'= (x^3 \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k})'=(x^3 \cos(x))'=3x^2 \cos(x)-x^3 \sin(x)$$ So $f(x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}=3x^2 \cos(x)-x^3 \sin(x)$ . If I did everything correct, than I should find a value for $x$ which will help me to calculate the initial sum. Did I do it ok? Thanks a lot!","['calculus', 'solution-verification', 'sequences-and-series', 'power-series', 'derivatives']"
4165821,Change of variables on triangular region.,"Given an integral $$\iint\limits_\triangle f(x,y) ~dx~dy$$ switch the variables to polar coordinates, where $\triangle = \{\text{Triangle whose vertices are at } (1,0),(0,2),(5,3)\}$ Please see the edit at the bottom (Attached the hand drawn figure below). I'm taking the substitution $$x = 1 + r\cos\theta, \qquad y = r\sin\theta$$ that is a circle centred at $(1,0)$ . As shown in the figure, one can fill one part of triangle with circles (say the part $\Omega_1$ ). In $\Omega_1$ , we have $$0 \le r \le \frac{7}{\sqrt{26}}$$ (where $|AH|=\frac{7}{\sqrt{26}})$ and $$\arctan \frac 34 \le \theta \le \pi - \arctan 2$$ that is the range of angle between the sides $AB$ and $AC$ . $$\iint\limits_{\Omega_1} f(x,y)~dx~dy = \int\limits_{\arctan \frac 34}^{\pi - \arctan 2} d\theta \int\limits_{0}^{7/\sqrt{26}} f(1+r\cos\theta, r\sin\theta)|J|dr$$ Well, $\Omega_1$ is done. Now let $\Omega_2$ be the unfilled region with circles which contains the point $B$ (upper-right region) and $\Omega_3$ be the unfilled region with circles which contains the point $C$ (upper-left region). While dealing with these regions, however, $r$ and $\theta$ seems to be ""dynamic"" in the sense that their bounds are dependent to the other variable. I'm not sure how to proceed. Edit: With the following new figure, I seem to have a better approach. Then $$\iint\limits_\triangle f(x,y) ~dx~dy = \int_{\arctan(3/4)}^{\pi-\arctan(2)} d\theta \int_0^{7/(5\sin\theta - \cos\theta)} f(1+r\cos\theta, r\sin\theta)r ~dr$$ since $$BC: y = \frac{1}{5}x+2 \\
\implies r\sin\theta = \frac15(r\cos\theta + 1) + 2 \\
\implies r = \frac{7}{5\sin\theta - \cos\theta}$$ Is it correct?","['multivariable-calculus', 'solution-verification', 'multiple-integral']"
4165858,Clarification on definition of differentiability of vector-valued functions.,"As it is normally stated, the definition of differentiability of vector-valued function is as follows: $\textbf{Definition:}$ Let $U \subset \Bbb{R}^n$ be open, and let $a \in U$ . A vector-valued function $f$ is differentiable at $a$ if there is a linear map $Df(a): \Bbb{R}^n \to \Bbb{R}^n$ so that $$\lim\limits_{h \to 0} \frac{f(a+h)-f(a)-Df(a)h}{\Vert h \Vert} = 0.$$ My question is, is the vector $h$ implicitly assumed to be arbitrary such that it is analogous for the case of real-valued functions where the direction of approaching the limit point should not matter?","['multivariable-calculus', 'definition']"
4165886,calculating limit exponential function,"$$\lim_{x \rightarrow \infty} \frac{e^x+e^{-x}}{e^x-e^{-x}}$$ It was required to first try with l'hospital, show that it does not work, and then find another way. But it doesn't even fulfill the rules to apply l'Hospital? How can I even try it? what I have done: $$\lim_{x \rightarrow \infty} \frac{e^x+e^{-x}}{e^x-e^{-x}}= \lim_{x \rightarrow \infty} \frac{e^{-x}(e^{2x}+1)}{e^{-x}(e^{2x}-1)}=\lim_{x \rightarrow \infty} \frac{(e^{2x}-1)+2}{(e^{2x}-1)} = 1+\lim_{x \rightarrow \infty} \frac{2}{(e^{2x}-1)} = 1 $$ since $\lim_{x \rightarrow \infty} \frac{2}{(e^{2x}-1)} $ = 0. Could someone check whether my approach is correct? and tell me how I could nicely argue that l'Hospital will not work here?","['limits-without-lhopital', 'limits', 'exponential-function', 'real-analysis']"
4165975,A determinant of a $3\times 3$ matrix,"I have a $3\times3$ real matrix of the following form $$
A=\left(
\begin{matrix}
a & b & c \\
* & * & * \\
* & * & *
\end{matrix}
\right),
$$ where $a,b,c$ are fixed. I would like to find the other entries of the matrix in such a way that $\det A = 0$ if and only if $a=b=c=0$ and no combination of $a,b,c$ appears at the denominator of the other entries. So far I have tried with different combinations of $a,b,c$ , but nothing seems to work. In particular, I always find a determinant that vanishes e.g. when $a=0$ but the others need not be zero, or something like that. Is there a clever choice I do not see?","['matrices', 'linear-algebra']"
4166036,Exercise 2. Chapter 5. Stein,"Suppose $F(z)$ is holomorphic near $z=z_0$ and $F(z_0)=F'(z_0)=0$ , while $F''(z_0)\neq 0$ . Show that there are two curves $\Gamma_1$ and $\Gamma_2$ that pass through $z_0$ , are orthogonal at $z_0$ , and so that $F$ restricted to $\Gamma_1$ is real an has a minimum at $z_0$ , while $F$ restricted to $\Gamma_2$ is also real but has a maximum at $z_0$ . Progress: I know that $F(z)=z^2f(z)$ where $f(z)$ doesn't vanish at $z_0$ . Then in a small disk around $z_0$ the function $f$ doesn't vanish. So I can write $F(z)=g(z)^2$ where $g$ is a bijection in a small disk around $z_0$ . The hint of this exercise suggests to use this function $g$ and its inverse to get the curves $\Gamma_1$ and $\Gamma_2$ , but I don't see how to do it. Any suggestions?",['complex-analysis']
4166066,Distributional Limit $T_{g_{n}} \rightarrow \delta_{0}$,"Consider $$
\psi(x)=\left\{\begin{array}{ll}
e^{-\frac{1}{1-x^{2}}}, & |x|<1 \\
0, & |x| \geqslant 1
\end{array}\right.
$$ and $$
g_{n}(x)=\frac{\psi(n x)}{\displaystyle\int_{-\infty}^{\infty} \psi(n x) \: \mathrm{d} x}
$$ for $n \in \mathbb{N}$ . I need to prove that $T_{g_{n}} \rightarrow \delta_{0}$ in $\mathscr{D}^{\prime}$ for $n \rightarrow+\infty$ . ( $T_{g_{n}}$ denotes the regular distribution induced by $g_n$ ). I have tried to write down the action of $T_{g_n}$ on a test function $\phi(x)$ as: $$\langle T_{g_n}, \phi(x)\rangle=\int_{-\frac 1n}^{\frac 1n} \frac{e^{-\frac{1}{1-n^2x^{2}}}}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \phi(x) \: \mathrm{d}x = \frac{1}{\displaystyle\int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2y^{2}}} \: \mathrm{d} y} \int_{-\frac 1n}^{\frac 1n} e^{-\frac{1}{1-n^2x^{2}}} \phi(x) \: \mathrm{d} x $$ But I really don't know what to do from here. Any ideas?","['limits', 'distribution-theory', 'real-analysis']"
4166069,Do the properties $f(x)f(y)=f(x+y)$ and $f(x) \geq 1+x$ uniquely characterise the function $f(x)=e^x$?,"In this post about possible definitions of the exponential function, it is mentioned that $e^x$ is the unique function $f:\Bbb{R}\mapsto\Bbb{R}$ satisfying $f(x)f(y)=f(x+y)$ $f(x)\geq 1+x$ Do these properties alone really uniquely characterise the exponential function, or do we have to impose further requirements (e.g. that $f$ be continuous)?","['calculus', 'definition', 'exponential-function', 'real-analysis']"
4166161,Order of a non-linear autoregressive exogenous (NARX) model,"Suppose I have a non-linear autoregressive exogenous (NARX) model of the kind $$y(k+1)=f(y(k), y(k-1), ..., y(k-s), u(k), u(k-1),..., u(k-t)) $$ where $y$ and $u$ represent respectively the output and the input of a discrete non linear system. Is it correct to say that the order of the NARX model is $\max\{s+1, t+1\}$ ? Here the Wikipedia link on info about the NARX model: https://en.wikipedia.org/wiki/Nonlinear_autoregressive_exogenous_model Here is the paper which references the NARX model. See equation (1).","['statistics', 'regression', 'discrete-time', 'time-series', 'nonlinear-system']"
4166250,Extending homomorphism on subgroup to the whole group,"I was working on homomorphisms and related concepts and I was wondering whether there exist some good criteria involving $H$ and $G_{1}$ which guarantees that all homomorphism $\phi$ on the subgroup $H$ of $G_{1}$ into an arbitrary group $G_{2}$ can be extended to a homomorphism $\bar{\phi}$ from all of $G_{1}$ to $G_{2}$ . And just to be clear: When I speak of an extension, I mean that I require that $\bar{\phi}$ restricted to H reduces to $\phi$ . ​
My current conjecture is that this is possible for all homomorphism on $H$ if and only if there exists a normal subgroup $N$ of $G_{1}$ such that $H$ is isomorphic to $G_{1}/N$ The criteria is certainly sufficient since then $G_{1}$ is the semidirect product of H and N and by mapping all elements in N to the identity in $G_{2}$ a homomorphism is constructed.
I've not been able to produce a proof that it is necessary also. Any ideas or arguments in favour or against the conjecture? Edit: As stated in the answer below my criteria for it being a semidirect product is actually wrong. You need the canoncial projection h $\mapsto$ [h] to be an isomorphism. It then guarantees trivial intersection and G = NH","['group-homomorphism', 'group-theory', 'normal-subgroups']"
4166251,A law of large numbers for bounded processes,"Let $(\Omega,\mathcal F,P)$ be a probability space and let $([0,1],\mathcal B[0,1],\lambda$ ) be the unit interval with Lebesgue measure on the Borel subsets of $[0,1]$ . Let $f:[0,1]\times \Omega \to \mathbb R$ be $\mathcal B[0,1]\otimes \mathcal F$ measurable and bounded. I want to show the existence of a subsequence $(n_k)$ such that $$\frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty $$ for $\lambda^\infty$ -almost every sequence $(t_i)\subset [0,1]$ , where $\lambda^\infty$ denotes the product measure on the $\sigma$ -algebra $\mathcal B^\infty:=\bigotimes_{n=1}^\infty \mathcal B[0,1]$ . I tried the following: Fix $\omega\in\Omega$ . For each $j\geq1$ , let $((t_i),\omega)\mapsto f(t_j,\omega)$ denote the composition of the maps $((t_i),\omega)\mapsto (t_j,\omega)$ and $(t,\omega) \mapsto f(t,\omega)$ . Since the $j$ th projection map $(t_i)\mapsto t_j$ on $[0,1]^{\infty}:=\prod_{i=1}^{\infty} [0,1]$ is $\mathcal B^\infty$ measurable, we see that $f(t_j,\omega)$ is $\mathcal B^\infty\otimes \mathcal F$ measurable. Moreover, by construction of the product measure $\lambda^\infty$ , the projection maps are i.i.d. random variables on $([0,1]^{\infty},\mathcal B^{\infty},\lambda^{\infty})$ , and so by the SLLN we get $$\frac{1}{n}\sum_{i=1}^n f(t_i,\omega) \overset{\lambda^\infty\text{-a.s}}\to \int f(t,\omega)d\lambda \quad \text{as } n\to \infty $$ for each $\omega\in \Omega$ . For each $n\geq 1$ , define the map $$((t_i),\omega)\mapsto S^n((t_i),\omega):=\frac{1}{n}\sum_{i=1}^n f(t_i,\omega) -\int f(t,\omega)d\lambda.$$ By Fubini's theorem $\omega\to \int f(t,\omega)d\lambda$ is $\mathcal F$ -measurable, and so $S^n((t_i),\omega)$ is seen to be $\mathcal B^\infty\otimes \mathcal F$ measurable. Also $S^n((t_i),\omega)$ is bounded since $f$ is bounded. Therefore the DCT gives $$\int |S^n((t_i),\omega)| d\lambda^{\infty} \to 0  \quad \text{as } n\to \infty$$ for each $\omega\in \Omega$ . By Fubini's theorem, these integrals are $\mathcal F$ -measurable functions of $\omega$ , and they are bounded, and so another application of the DCT gives $$\int \int |S^n((t_i),\omega)| d\lambda^{\infty}dP \to 0  \quad \text{as } n\to \infty$$ We can interchange the order of integration by Fubini's theorem to obtain $$\int \int |S^n((t_i),\omega)| dPd\lambda^{\infty} \to 0  \quad \text{as } n\to \infty$$ This says that $\int |S^n((t_i),\omega)| dP \to 0 $ as $n\to \infty $ w.r.t. the $L^1([0,1]^{\infty} ,\mathcal B^{\infty},\lambda^{\infty})$ norm. We can extract a subsequence $(n_k)$ converging $\lambda^{\infty}$ -almost surely, i.e. such that $$\frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty $$ for $\lambda^\infty$ -almost every sequence $(t_i)\subset [0,1]$ . Is this reasoning correct? Thanks a lot for your help.","['measure-theory', 'lebesgue-measure', 'real-analysis', 'law-of-large-numbers', 'probability-theory']"
4166260,Ratio test and root test,"In Mathematical Analysis by Tom M. Apostol it's stated that root test is ""stronger"" than ratio test because there are series whose convergence can be decided by the former, but not by the latter. Question : I'm looking for an example of this. I prefer an example of a series of positive terms, where both limits of $a_{n+1}/a_n$ and $\sqrt[n]{a_n}$ exist. In other words, I need a sequence $(a_n)_n$ of positive terms such that $a_{n+1}/a_n\to 1$ and $\sqrt[n]{a_n}\to r\neq 1$ . Try : I have tried $\sum\dfrac{n!}{n^n}$ , but the ratio test yields $1/e<1$ . Any suggestion? Note : The example may be divergent or convergent.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4166304,"The function $ g(n)=\sum_{\substack {1\lt k\leq n \\ \gcd(k,n)=1}}d(k-1)$","In 1965 Puliyakot Keshava Menon proved that $${\displaystyle \sum _{\substack {1\leq k\leq n \\ \gcd(k,n)=1}}\gcd(k-1,n)=\varphi (n)d(n)}$$ being $\varphi(n)$ the totient function of $n$ . If we move under summation the function $d(n)$ (the function that counts the divisors of $n$ ), that is if we consider the function $${\displaystyle g(n)=\sum _{\substack {1\lt k\leq n \\ \gcd(k,n)=1}}d(k-1)}$$ and analyze its behavior up to $n=10^6$ , we find that (for $n\gt35$ ) $${\displaystyle 1 \lt \frac {g(n)} {\varphi (n)log(n)} \lt 2}$$ Is there a way to prove the above bounds? Many thanks.","['number-theory', 'arithmetic-functions', 'totient-function', 'combinatorics', 'divisor-counting-function']"
4166335,What does the notation $f^*(x)$ mean?,I'm doing an assignment for my calculus course. I don't need help with the question. But I've never come across that notation $f^*(x)\;.\;$ See the following picture : I can't find anything in my book about it. Is it just an arbitrary use of a symbol ?,"['notation', 'calculus', 'derivatives']"
4166352,"Solution of first order linear differential equation, initial value problem","My problem is: Solve $x'= $$\dfrac{1}{\sin\left(\dfrac{x(t)}{t}\right)}+\dfrac{x(t)}{t}\;,\;$ where $\;x(2) =\dfrac{2\pi}{3}\;.$ First, I did a substitution: $\;y(t)=\dfrac{x(t)}{t}$ $$ x(t)=ty(t)\implies x'(t) = y(t) + ty'(t) $$ $$  $$ $$ y(t) + ty'(t) = y(t) + \frac{1}{\sin(y(t))}$$ $$ y(2) = \frac{x(2)}{2} = \frac{\frac{2\pi}{3}}{2} = \frac{\pi}{3} $$ $$  $$ $$ y'(t) = \frac{1}{t\sin(y(t))} $$ $$ y(2)= \frac{\pi}{3} $$ $$  $$ $$G(t)=\int \frac{1}{t} dt = \ln(t) + C$$ $$H(y)= \int\sin(y) dy = -\cos(y) + C $$ $$H^{-1}(a)= -\arccos(a)  $$ $$ x(t) = ty(t) = -t\arccos(\ln(t)) $$ I think I am doing something wrong, and some things are missing, like the invervals. How can I determine the intervalls? And what am I doing wrong? I appreciate any kind of help.","['initial-value-problems', 'ordinary-differential-equations']"
4166357,Prove $A\approx A\times A$ using Zorn's Lemma,"I'm having trouble proving that if $A$ is an infinite set, then $A\approx A\times A$ using Zorn's Lemma directly. My problem is I don't know how to choose the correct set to which apply Zorn; I have tried $$X=\{f_{B}\colon B\times B\longrightarrow B\mid B\subseteq A,\ f_{B}\ {\rm injective}\}$$ since it's obvious that $A\preceq A\times A$ , and with $(X,\leq)$ (the order $f_{B}\leq f_{B'}$ if $B\subseteq B'$ and $f_{B'}$ restricted to $B\times B$ is $f_{B}$ ) I manage to get a maximal function of that kind, but then I can't ""expand it"" as usual to prove it must have domain $A\times A$ . Do you know which set $X$ should I work with?","['elementary-set-theory', 'axiom-of-choice']"
4166379,"Let $f:[0,1] \to \Bbb R$ be continuous. Show that $m(\mathcal{G})=0$, where $\mathcal{G}= \{(x,f(x)) \mid x \in [0,1]\}.$","Let $f:[0,1] \to \Bbb R$ be continuous. Show that $m(\mathcal{G})=0$ , where $\mathcal{G}= \{(x,f(x)) \mid x \in [0,1]\}.$ Since $f$ is continuous and $[0,1]$ is compact we know that $f$ is uniformly continuous on $[0,1]$ . Now that means that for all $(x,y)$ we have that $$|f(x)-f(y)| < \varepsilon : |x-y| < \delta$$ for all $\varepsilon >0$ . Apparently the idea here is to cover $\mathcal{G}$ by rectangles and use the uniform continuity. I got a hint that I can let $n \in \Bbb N$ such that $n\delta > |b-a| = |1-0| = 1$ and that there would then exist a partition $$0=x_0<x_1< \dots < x_n=1$$ such that $$|x_{i+1}-x_i| < \delta.$$ I'm bit confused where did this $n\delta > |b-a|$ come from and how does that imply $|x_{i+1}-x_i| < \delta$ ?","['measure-theory', 'real-analysis']"
4166381,Proof explanation: Every Cauchy sequence has a limit,"I have a bit of a problem with understanding the parts of the proof as stated in my book: Lemma: In a complete ordered field, every Cauchy sequence has a limit. Proof: Let $\left(a_{n}\right)$ be a Cauchy sequence in $F$ . By the
argument of lemma $9.15$ of chapter 9 (carried out in $F$ ) the
sequence is bounded. Hence so is every subset of elements in the
sequence. Define $$ b_{N}=\text { the least upper bound of
 }\left\{a_{N}, a_{N+1}, a_{N+2}, \ldots\right\} $$ This exists by
completeness. Clearly $$ b_{0} \geq b_{1} \geq b_{2} \geq \cdots $$ and the sequence $\left(b_{n}\right)$ is bounded below-say, by any
lower bound for $\left(a_{n}\right)$ . Hence we can define $c=$ the
greatest lower bound of $\left(b_{n}\right)$ . We claim that $c$ is the limit of the original sequence $\left(a_{n}\right)$ To prove this, let $\varepsilon>0$ . Suppose that
there exist only finitely many values of $n$ with $$ c-\frac{1}{2} \varepsilon<a_{n}<c+\frac{1}{2} \varepsilon $$ Then we may choose $N$ such that for all $n>N$ , $$ a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon $$ But there exists $N_{1}>N$ such that if $m, n>N_{1}$ then $\left|a_{m}-a_{n}\right|<\frac{1}{2} \varepsilon$ . Hence $$ \text {
for all } n>N_{1}, a_{n} \leq c-\frac{1}{2} \varepsilon $$ Or $$ \text
{ for all } n>N_{1}, a_{n} \geq c+\frac{1}{2} \varepsilon $$ The
latter condition implies that there exists some $m$ with $a_{n}>b_{m}$ for all $n>N_{1}$ , which contradicts the definition of $b_{m}$ . But
the former implies that we may change $b_{N_{1}}$ to $b_{N_{1}}-\frac{1}{2} \varepsilon$ , which again contradicts the
definition of $b_{N_{1}}$ . It follows that for any $M$ there exists $m>M$ such that $$ c-\frac{1}{2} \varepsilon<a_{m}<c+\frac{1}{2}
 \varepsilon $$ Since $\left(a_{n}\right)$ is Cauchy, there exists $M_{1}>M$ such that $\left|a_{n}-a_{m}\right|<\frac{1}{2} \varepsilon$ for $m, n>M_{1} .$ Hence for $n>M_{1}$ , $$
c-\varepsilon<a_{n}<c+\varepsilon $$ But this implies that $\lim a_{n}=c$ as claimed. Specifically I don't understand why the 2 bolded sentences are needed in the proof. Don't we already establish in the previous assumption ( Suppose that there exist only finitely... ) that for all $n>N,  a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon $ and from that, the paragraph afterwards ( The latter condition... ) follows? Why does the Cauchy criterion need to be invoked?","['proof-explanation', 'cauchy-sequences', 'real-analysis']"
4166403,Defining a set connotatively or denotatively.,"“It is said that a set can be defined connotatively or denotatively. Which of these terms applies to the definition by roster and which to the definition by a defining sentence?”- p. 137, Elements of Modern Math , K. O. May. The author is asking me to match the words connotative and denotative to the roster notation $\{\,\}$ or to a a set designated by a defining sentence $\{ x \mid f(x) \}$ . I say that the roster notation is connotative and a defining sentence of a set is denotative. The former lists the elements of a set, and generally what could belong in that set, while the latter defines a set, kind of, I suppose as a dictionary does. Is this correct? Any elucidating thoughts are welcome.","['foundations', 'definition', 'discrete-mathematics', 'elementary-set-theory', 'philosophy']"
4166406,Cubic addition and differentiablility,"It came to my thought that if we define $a\oplus b = (a^3 + b^3)^{\frac13}$ then $\Bbb R$ endowed with $\oplus$ and $\cdot$ the latter being the usual multiplication is a field, with usual $0$ and $1$ being the zero and the unity of this field respectively. Not really knowing what to do next with this, I decided to check how will new derivatives be different from the old ones. If I am not mistaken, it holds that $$
\lim_{h\to 0}\frac{f(x\oplus h)\ominus f(x)}{h} = \left(f'(x)\frac{f^2(x)}{x^2}\right)^{\frac13}
$$ if $x\neq 0$ , while the direct evaluation of the limit when $x = 0$ gives that indeed it is infinite unless $f(0) = 0$ . In the latter case we get the new derivative (namely, the limit) being exactly the old derivative $f'(0)$ . This looks very odd to me: why $0$ should be any special point of this field? So my questions are: Did I make a mistake somewhere? If yes, please point me to it. If not, why in this new field differentiability at $0$ is something special?","['limits', 'calculus', 'derivatives']"
4166407,$\mu^n_{s} \xrightarrow[n \uparrow \infty]{w} \mu_{s}$ implies $\mu^n_{t_k} \xrightarrow[n \uparrow \infty]{w} \mu_{t_k}$,"Suppose that $\{\mu^n_{s}\}_{n \in \mathbb{N}},\mu_s$ denote a measures with mass bounded by $1$ and that for almost every $s$ in $[0,T]$ , $\mu^n_{s} \xrightarrow[n \uparrow \infty]{w} \mu_{s}$ (weak convergence of measures). I want to show that for each $t<T$ , we have a sequence $\{t_k\}_{k \in \mathbb{N}}$ with $t_k \downarrow t$ such that $\mu^n_{t_k} \xrightarrow[n \uparrow \infty]{w} \mu_{t_k}$ for all $k \in \mathbb{N}$ . I'm not absolutely sure if this property is true, but it seems true. If I could prove it, it would make a proof that I'm trying to do a lot easier and that's why I'm asking if someone can point me in the right direction.","['measure-theory', 'convergence-divergence', 'probability-theory', 'weak-convergence']"
4166427,Which Sobolev spaces are function spaces?,"Let us consider the Sobolev space $W^{s,p}(\mathbb R^n)$ , $s \in \mathbb R$ (I am most interested in the case $n=1$ ). If $s \geq 0$ , it embeds to $L^p(\mathbb R^n)$ , so its elements may be interpreted as functions (modulo redefinitions on sets of measure zero). If $s$ is sufficiently negative then not all elements of $W^{s,p}(\mathbb R^n)$ are functions, for example the Dirac's delta may belong to $W^{s,p}(\mathbb R^n)$ . I would like to ask if there is some range of negative $s$ for which all elements in $W^{s,p}(\mathbb R^n)$ are genuine functions (e.g. from $L^1_{\mathrm{loc}}(\mathbb R^n))$ .","['measure-theory', 'analysis', 'fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis']"
4166459,Approximating the spectral density of white noise by a moving average process,"Suppose that $X_t$ is a weakly stationary process; then, its autocovariance function can be represented as: $$\gamma_X(h) = \int_{(-\pi , \pi ]} e^{i h v} dF(v)$$ The function $F$ is called the spectral distribution of $X_t$ . If $F$ admits a density with respect to the Lebesgue measure, say $f$ , we say that $f$ is the spectral density of $X_t$ .
I am interested in the following problem (taken from Brockwell and Davis, Time Series: Theory and Methods, Exercise 4.15): Suppose $x_t = w_t - 2w_{t-1}$ , where $w_t$ is a mean zero white noise sequence with variance $\sigma_w^2$ .
Given $\epsilon > 0$ , find an integer $k$ and constants $a_0, \ldots , a_k$ , with $a_0=1$ , such that if $f_y$ is the spectral density of the process \begin{align*}
    y_t = \sum_{j=0}^k a_j x_{t-j} 
\end{align*} then, \begin{align*}
    \sup_{-0.5 \leq \omega \leq 0.5} \left| f_y(\omega ) - \frac{\mathrm{Var}({y_t})}{2\pi} \right| < \epsilon
\end{align*} Motivation: The key to this problem should be that for every symmetric spectral density (e.g. the variance of $y_t$ ), there exists an invertible $MA(q)$ process $X_t$ such that the spectral density of $X_t$ is within an epsilon of the initial symmetric spectral density. However, the process in this question takes away some freedom from which MA processes I can choose from to approximate the variance of $y_t$ , which makes it non-obvious how to calculate the desired coefficients.","['time-series', 'stochastic-processes', 'statistics', 'probability-theory']"
4166508,"Given a random binary operation $\varphi$ on a finite set $G$, what is the probability that $(G,\varphi)$ is a group?","I recently learned about the 5/8 theorem, which states that if the probability that any two random elements in a group commute exceeds 5/8, then the group must be abelian.
I think its interesting seeing probability and group theory together, which led me to think of this question. This is my work so far, which has a good possibility of being flawed.
First, consider all binary operations $\varphi : G\times G \to G$ , which given $|G|=n$ we can count as $|G|^{|G\times G|^2} = n^{n^2}$ . Next, consider all binary operations that fail to have an identity $e \in G$ .
If $\varphi(g,e)\neq g$ (or $\varphi(e,g)\neq g$ ) for any single $g \in G$ , we can throw out all other possible functions given by the other elements in $G- \{g\}$ , which is given by $(n-1)^{(n-1)^2}$ operations.
There are $n^2$ elements in $|G\times G|$ that each have $(n-1)$ ways to fail, so the number of binary operations that fail to have an identity element should be given by $$n^2(n-1)(n-1)^{(n-1)^2}=n^2(n-1)^{(n-1)^2+1}$$ Hence, we can get an upper bound for how many binary operations can form groups as $$n^{n^2}-n^2(n-1)^{(n-1)^2+1}$$ For inverses, I think the same argument can be made as the above, hence an upper bound after considering inverses is given by $$n^{n^2}-2n^2(n-1)^{(n-1)^2+1}$$ I have no idea how to even get started with associativity!
Further, I have never been fantastic at counting problems, so I am sure I made a flaw in my argument thus far.
Any progress or criticisms are very much welcome.","['finite-groups', 'group-theory', 'combinatorics', 'probability']"
4166548,Use Implicit Differentiation to find $\frac{d^2y}{dx^2}$?,"Given a system of equation, \begin{align*}
x &= t^2 + 2t \\
y &= 3t^4 + 4t^3
\end{align*} I want to find $\frac{d^2 y}{dx^2}$ at $(x,y) = (8, 80)$ . Then, $\partial_x(y) = \frac{d y}{dt} \frac{dt}{dx}$ . By chain rule, \begin{align*}
\partial_x^2(y) &= \partial_x \left(\frac{d y}{dt}\right)\frac{dt}{dx} + \frac{dy}{dt} \partial_x \left(\frac{dt}{dx}\right) \\
&= \frac{d^2 y}{dt^2}\left(\frac{dt}{dx}\right)^2 + \frac{dy}{dt}\frac{d^2t}{dx^2}
\end{align*} Here, how do I find $\frac{d^2 t}{dx^2}$ ?","['calculus', 'implicit-differentiation', 'derivatives']"
4166594,Find $x$ if $\cot(x)=\csc(12^{\circ})-\sqrt{3}$,"Find $x$ in degrees if $\cot(x)=\csc(12^{\circ})-\sqrt{3}$ My attempt: $$\cot (x)=\frac{1}{\sin (12^{\circ})}-2 \sin \left(60^{\circ}\right)$$ $$\Rightarrow \cot x=\frac{1-2 \sin (12^{\circ}) \sin (60^{\circ})}{\sin \left(12^{\circ}\right)}$$ $$\Rightarrow \cot x=\frac{1-\cos 48^{\circ}+\cos 72^{\circ}}{\sin \left(12^{\circ}\right)}$$ Now let $, \theta=12^{\circ},s=\sin(\theta)$ , then we get $$\cot x=\frac{1-\cos (4 \theta)+\cos (6\theta)}{\sin (\theta)}$$ Converting to rational function in $s$ , we get $$\cot x=\frac{-32 s^{6}+40 s^{4}-10 s^{2}+1}{s}$$","['trigonometry', 'algebra-precalculus', 'geometry']"
4166622,Proving that an algorithm can generate all permutations of a matrix,"I have a matrix $$\begin{bmatrix}a & b\\\ c & d\end{bmatrix}$$ and I want to generate all 24 possible permutations of the elements of the matrix, e.g. $$\begin{bmatrix}b & a\\\ c & d\end{bmatrix}, \begin{bmatrix}a & c\\\ d & b\end{bmatrix}...$$ using a particular procedure: for each column, permute the elements of the column for each row, permute the elements of the row for each column, again permute the elements of the column In each of these cases, the permutation applied need not be the same between rows/columns. It's my gut feeling that this set of permutations could generate any permutation of a starting $M\times N$ matrix, but I'm not really sure how to go about proving it. I've shown that it works for $2\times 2$ and $3\times 3$ matrices, but I was wondering how should I approach proving this for $M\times N$ matrices? I saw these two posts, but neither of them quite answered my question, since the row/column permutations they considered were constrained: What permutations of matrix entries do row and column transpositions generate? Do cyclic permutations of rows and column entries generate all permutations? I also saw a third post that was asking a very similar question , but only did the first two steps (column permutation followed by a row permutation). Clearly in their case, they couldn't generate all permutations of a $2\times 2$ matrix since the procedure could only generate $16$ unique matrices from a starting $2\times 2$ matrix, less than the $24$ unique permutations of the starting matrix.","['permutations', 'matrices', 'linear-algebra', 'combinatorics', 'group-theory']"
4166623,How to solve $\frac{1}{2\sin50^\circ}+2\sin10^\circ$,"I want to solve the expression $\frac{1}{2\sin50^\circ}+2\sin10^\circ$ to get a much simpler and neater result. I have tried to manipulate this expression such as using sum/difference formulas, but it didn't help (and made the expression even more messy). Here is what I did: \begin{align}
\frac{1}{2\sin50^\circ}+2\sin10^\circ&=\frac{1}{2\sin(60-10)^\circ}+2\sin10^\circ \\
&= \frac{1}{2\left(\frac{\sqrt{3}}{2}\cdot\cos10^\circ-\frac{1}{2}\cdot\sin10^\circ\right)}+2\sin10^\circ \\
&= \frac{1}{\sqrt{3}\cdot\cos10^\circ-\sin10^\circ}+2\sin10^\circ \\
&= \frac{\sqrt{3}\cdot\cos10^\circ+\sin10^\circ}{\left(\sqrt{3}\cdot\cos10^\circ-\sin10^\circ\right)\left(\sqrt{3}\cdot\cos10^\circ+\sin10^\circ\right)}+2\sin10^\circ \\
&= \frac{\sqrt{3}\cdot\cos10^\circ+\sin10^\circ}{3\cos^210^\circ-\sin^210^\circ}+2\sin10^\circ
\end{align} But I don't know how to continue at this point. Multiplying in $2\sin10^\circ$ into the fraction is clearly unrealistic as it would result in trignometry of third power. Any help or hint would be appreciated. According to a calculator, the result of this expression should come to a nicely $1$ , but I just want to know how to algebraically manipulate this expression to show that it is equal to $1$ .","['algebra-precalculus', 'trigonometry']"
4166679,Find all $f : \mathbb R\to\mathbb R$ such that $f(f(x+y)) = f(x+y) + f(x)f(y) -xy$,"Find all $f : \mathbb R\to\mathbb R$ such that $$f(f(x+y)) = f(x+y) + f(x)f(y) -xy$$ for all reals $x,y.$ (Belarusian Mathematical Olympiad-1995) My answer: Consider $f(0) = c. ...(i)$ Let $x,y = 0$ at first. $$f(f(x+y)) = f(x+y) + f(x)f(y) -xy$$ $$\implies f(f(0+0)) = f(0+0) + f(0)f(0) -0 \times 0$$ $$\implies f(f(0)) = f(0) + f(0)f(0) + 0$$ $$\implies f(c) = c + c \times c + 0$$ $$\implies f(c) = c + c^2$$ $$\implies f(c) - c = c^2 ...(ii)$$ Now, let $y = -x.$ Therefore, $$f(f(x+y)) = f(x+y) + f(x)f(y) -xy$$ $$\implies f(f(x-x)) = f(x-x) + f(x)f(y) -(x\times -x)$$ $$\implies f(f(0)) = f(0) + f(x)f(-x) + x^2$$ $$\implies f(c) = c + f(x)f(-x) + x^2$$ $$\implies f(c)-c = f(x)f(-x) + x^2$$ From $..(ii),$ Therefore, $$c^2 = f(x)f(-x) + x^2$$ Now, let $x = c$ $$Therefore, c^2 = f(x)f(-x) + c^2$$ $$\implies c^2 - c^2 = f(c)f(-c)$$ $$\implies 0 = f(c)f(-c)$$ Hence, $f(c) = 0 ...(iii)$ $f(-c) = 0 ...(iv)$ Multiplying $(i) and (iii),$ $$f(0) \times f(c) = c \times 0$$ $$\implies f(0) \times f(c) = 0$$ $$\implies f(0) = 0$$ $$\implies c = 0$$ Similarly, Multiplying $(i) and (iv)$ $\implies c = 0$ Hence $c = 0.$ But $f(c) = 0,$ Therefore, $f(c) = c$ I can't go further than that can anyone help me out.","['contest-math', 'functional-equations', 'algebra-precalculus', 'functions']"
4166682,"If $b$ is a divisor of $a^2 - a + 1$, can $a$ be a divisor of $b^2 - b + 1$","I observed that for $a > 1$ , if $b$ is a divisor of $a^2 - a + 1$ then $a$ is not a divisor of $b^2 - b + 1$ . This implies that their divisors are mutually exclusive. Is this true in general? $$
b \mid a^2 - a + 1 \implies a \nmid b^2 - b + 1
$$ I have verified this for $a \le 10^9$ .","['divisibility', 'number-theory', 'elementary-number-theory', 'algebra-precalculus', 'prime-numbers']"
4166776,Should I use some laws of large numbers or should I refer to other technics?,"In a lecture (probability theory) note, there is a starred exercise in the chapter of convergence conception: Suppose $\left\{ξ_{n}\right\}$ are i.i.d. positive integer valued random variables. Let $R_{n}=|\{ξ_{1},…,ξ_{n}\}|$ denote the number of distinct elements among the first $n$ variables. Prove that $\lim  \mathbb{E} (R_{n}/n) = 0$ . I have tried to calculate $\mathbb{E} (R_{n}/n)$ directly with finite numbers of r.v.'s and a finite set of positive integers for r.v.'s to be valued in, and then take the limit. It didn't work. I also tried to employ some of the laws of large numbers, but couldn't find one suitable. I am not sure should I employ some analysis techniques such as truncation, etc. ? Or is there any other way out? There is a related question, but I believe it is a weaker version of the above one: What is the probability space/measure here? In general, what does ""almost sure"" convergence mean when a probability space is not explicitly specified?","['expected-value', 'convergence-divergence', 'probability-theory', 'probability']"
4166777,Geodesic deviation in flat space,"Suppose that $x^\mu(t,s)$ represents a family of curves.
Let $v^\mu$ represents the the tangent vector to a curve $x^\mu(t,s_0)$ with $s_0$ fixed that is $v^{\mu}=\partial x^{\mu} / \partial t$ and deviation vector is given by $\xi^{\alpha}=\partial x^{\alpha} / \partial s$ . In the usual derivation of the geodesic deviation equation it is showed that $\xi$ is lie transposed through $v$ that is $$L_v\xi=0$$ where $L$ stands for the lie derivative.
To make the discussion more clear let us suppose we are in flat spacetime with  usual Cartesian coordinates $x^0=t,x^1,x^2,x^3$ . We choose $x^\mu(t,s)=(t,st,0,0)$ and so $v^\mu=(1,s,0,0)$ and $\xi^{\alpha}=(0,t,0,0)$ we have $$L_v\xi=\left[\frac{\partial}{\partial t}+s\frac{\partial}{\partial x^{1}},t\frac{\partial}{\partial x^{1}} \right]=\frac{\partial}{\partial x^1}-\frac{\partial s}{\partial x^1}\frac{\partial}{\partial x^1}$$ So we choose the parameter $s$ for example to $2x^1$ we would have $L_v\xi \ne0$ Isn't this a contradiction?",['differential-geometry']
4166867,An $\mathcal{S}$-measurable function defined on $\mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\}$ must be constant on each $E_k$,"I have proved the following statement(s) and I would like to know if my proof is correct and/or how it could be improved, thanks. ""Suppose $X$ is a set and $E_1, E_2,\dots $ is a disjoint sequence of subsets of $X$ such that $\bigcup_{k=1}^{\infty}E_k=X$ . Let $\mathcal{S}=\{\bigcup_{k\in K}E_k:K\subset\mathbb{Z^+}\}$ . (a) Show that $\mathcal{S}$ is a $\sigma$ -algebra on $X$ . (b) Prove that a function $f:X\to\mathbb{R}$ is $\mathcal{S}$ -measurable if and only if the function is constant on $E_k$ for every $k\in\mathbb{Z^+}$ . My proof: (a) If we take $K=\emptyset$ then $K\subset\mathbb{Z^+}$ so $\bigcup_{k\in K}E_k\in\mathcal{S}$ and $\bigcup_{k\in K}E_k=\emptyset$ so $\emptyset\in\mathcal{S}$ . Let $A\in\mathcal{S}$ : then $A=\bigcup_{k\in K_A}E_k$ for some $K_A\subset\mathbb{Z^+}$ so $X\setminus A=\bigcup_{k=1}^{\infty}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}}E_k\setminus\bigcup_{k\in K_A}E_k=\bigcup_{k\in\mathbb{Z^+}\setminus K_A}E_k$ and $\mathbb{Z^+}\setminus K_A\subset\mathbb{Z^+}$ so $A\in\mathcal{S}$ . Let $A_1,A_2,\dots \in\mathcal{S}$ : then $A_1=\bigcup_{k\in K_{A_1}}E_k, A_2=\bigcup_{k\in K_{A_2}}E_k,\dots, A_n=\bigcup_{k\in K_{A_n}}E_k,\dots$ so $\bigcup_{n=1}^{\infty}A_n=\bigcup_{n=1}^{\infty}\bigcup_{k\in K_{A_n}}E_k=\bigcup_{k\in K_{A_1}}E_k\cup\bigcup_{k\in K_{A_2}}E_k\cup\dots=\bigcup_{k\in K_A}E_k$ , where $K_A=\bigcup_{n=1}^{\infty} K_{A_n}\subset\mathbb{Z^+}$ so $\bigcup_{k\in K_A}E_k\in\mathcal{S}$ hence $\bigcup_{n=1}^{\infty}A_n\in\mathcal{S}$ . (b) $\fbox{$\Rightarrow$}$ Let $f:X\to\mathbb{R}$ be an $\mathcal{S}$ -measurable function and suppose for sake of contradiction there existed some $E_{K}, K\in\mathbb{Z^+}$ such that $f$ is not constant on it, ie $f(E_K)=\{y_1,y_2\}, y_1\neq y_2$ : then since $\{y_1\}$ is a Borel measurable subset of $\mathbb{R}$ we have that $f^{-1}(\{y_1\})\in\mathcal{S}$ so $f^{-1}(\{y_1\})=\bigcup_{k\in K_{y_1}}E_k$ for some $K_{y_1}\subset \mathbb{Z^+}$ hence we have $E_K\cap\bigcup_{k\in K_{y_1}} E_k\neq\emptyset$ and since the $E_k$ s form a partition of $X$ this can only be if $K\in K_{y_1}$ ie $E_K\subseteq\bigcup_{k\in K_{y_1}} E_k$ thus $f(E_k)=\{y_1,y_2\}\subseteq f(\bigcup_{k\in K_{y_1}}E_k)=\{y_1\}$ , contradiction.
So, if $f:X\to\mathbb{R}$ is $S$ -measurable it must be constant on each $E_k, k\in\mathbb{Z^+}$ , as desired. $\fbox{$\Leftarrow$}$ Now, let $f:X\to\mathbb{R}$ be a function which is constant on each $E_k$ , ie $f(X)=\{y_1=f(E_1), y_2=f(E_2),\dots,y_n=f(E_n),\dots\}$ : then if $B$ is a Borel subset of $\mathbb{R}$ with $B\cap f(X)=\emptyset$ we have $f^{-1}(B)=\emptyset\in\mathcal{S}$ ; if $B\cap f(X)=f(X)$ we have $f^{-1}(B)=X\in\mathcal{S}$ ; if $B\cap f(X)\subsetneqq f(X), B\cap f(X)=\{y_{j_1},y_{j_2},\dots, y_{j_n},\dots \}, \{j_1,\dots,j_n,\dots\}\subset \mathbb{Z^+}$ we have $f^{-1}(B)=\bigcup_{k\in \{j_1,\dots,j_n.\dots\}}E_k\in\mathcal{S}$ . So since in all possible cases $f^{-1}(B)\in\mathcal{S}$ we can conclude that $f$ is $\mathcal{S}$ -measurable, as desired.","['measure-theory', 'solution-verification', 'real-analysis']"
4166924,"If $G$ is a group, then $a \ast x = b$ has a unique solution.","From ""A First Course In Abstract Algebra"" by John B. Fraleigh Theorem If $G$ is a group with binary operation $\ast$ , and if $a$ and $b$ are any elements of $G$ , then the linear equations $a \ast x = b$ and $y \ast a = b$ have unique solutions $x$ and $y$ in $G$ . Proof First we show the existence of at least one solution by just computing that $a' \ast b$ is a solution of $a \ast x = b$ .
Note that $$\begin{align*}
a \ast (a' \ast b) &= (a \ast a') \ast b & &\text{associative law} \\ 
&= e \ast b & &\text{definition of $a'$} \\ 
&= b & &\text{property of $e$} \\
\end{align*}$$ Thus $x = a' \ast b$ is a solution of $a \ast x = b$ . In a similar fashion, $y = b \ast a'$ is a solution of $y \ast a = b$ . To show uniqueness of $y$ , we use the standard method of assuming that we have two solutions, $y_1$ and $y_2$ , so that $y_1 \ast a = b$ and $y_2 \ast a = b$ . Then $y_1 \ast a = y_2 \ast a$ , and by Theorem 4.15, $y_1 = y_2$ . The uniqueness of $x$ follows similarly. $\blacksquare$ My question.
Why didn't Fraleigh simply solve the equation to show that a solution exists?
I mean: given the equation $a \ast x = b$ we can do as follows $$\begin{align*}a \ast x &= b \\
a' \ast (a \ast x) &= a' \ast b \\
(a' \ast a) \ast x &= a' \ast b \\
e \ast x &= a' \ast b \\
x &= a' \ast b\end{align*}$$ and this shows two things: that a solution exists and also that the solution is unique. Is it a matter of style or in this context my way is wrong?","['proof-explanation', 'group-theory', 'abstract-algebra']"
4166929,"Prove or disprove that the function $f(x)=x^{x^{x^{x}}}$ is convex on $(0,1)$","Let $0<x<1$ and $f(x)=x^{x^{x^{x}}}$ then we have : Claim : $$f''(x)\geq 0$$ My attempt as a sketch of partial proof : We introduce the function ( $0<a<1$ ): $$g(x)=x^{x^{a^{a}}}$$ Second claim : $$g''(x)\geq 0$$ We have : $g''(x)=x^{x^{a^{a}}+a^a-2}(a^{\left(2a\right)}\ln(x)+x^{a^{a}}+2a^{a}x^{a^{a}}\ln(x)-a^{a}\ln(x)+2a^{a}+a^{\left(2a\right)}x^{a^{a}}\ln^{2}(x)-1)$ We are interested by the inequality : $$(a^{\left(2a\right)}\ln(x)+x^{a^{a}}+2a^{a}x^{a^{a}}\ln(x)-a^{a}\ln(x)+2a^{a}+a^{\left(2a\right)}x^{a^{a}}\ln^{2}(x)-1)\geq 0$$ I'm stuck here . As noticed by Hans Engler we introduce the function : $$r(x)=x^{a^a}\ln(x)$$ We have : $$r''(x)=x^{a^a - 2} ((a^a - 1) a^a \ln(x) + 2 a^a - 1)$$ The conclusion is straightforward the function $\ln(g(x))$ is convex so it implies that $g(x)$ is also convex on $(0,1)$ . Now starting with the second claim and using the Jensen's inequality we have $x,y,a\in(0,1)$ : $$x^{x^{a^{a}}}+y^{y^{a^{a}}}\geq 2\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)^{a^{a}}}$$ We substitute $a=\frac{x+y}{2}$ we obtain : $$x^{x^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}+y^{y^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}\geq 2\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}$$ Now the idea is to compare the two quantities : $$x^{x^{x^{x}}}+y^{y^{y^{y}}}\geq x^{x^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}+y^{y^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}$$ We split in two the problem as : $$x^{x^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}\leq x^{x^{x^{x}}}$$ And : $$y^{y^{y^{y}}}\geq y^{y^{\left(\frac{x+y}{2}\right)^{\left(\frac{x+y}{2}\right)}}}$$ Unfortunetaly it's not sufficient to show the convexity because intervals are disjoint . A related result  : It seems that the function : $r(x)=x^x\ln(x)=v(x)u(x)$ is increasing on $I=(0.1,e^{-1})$ where $v(x)=x^x$ . For that differentiate twice and  with a general form we have : $$v''(x)u(x)\leq 0$$ $$v'(x)u'(x)\leq 0$$ $$v(x)u''(x)\leq 0$$ So the derivative is decreasing on this interval $I$ and $r'(e^{-1})>0$ We deduce that $R(x)=e^{r(x)}$ is increasing . Furthermore on $I$ the function $R(x)$ is concave and I have not a proof of it yet . We deduce that the function $R(x)^{R(x)}$ is convex on $I$ . To show it differentiate twice and use a general form like : $(n(m(x)))''=R(x)^{R(x)}$ and we have on $I$ : $$n''(m(x))(m'(x))^2\geq 0$$ And : $$m''(x)n'(m(x))\geq 0$$ Because $x^x$ on $x\in I$ is convex decreasing . Conlusion : $$x^{x^{\left(x^{x}+x\right)}}$$ is convex on $I$ The same reasoning works with $x\ln(x)$ wich is convex decreasing on $I$ . Have a look to the second derivative divided by $x^x$ In the last link all is positive on $J=(0.25,e^{-1})$ taking the function $g(x)=\ln\left(R(x)^{R(x)}\right)$ Question : How to show the first claim ?Is there a trick here ? Ps:feel free to use my ideas .","['power-towers', 'functions', 'derivatives', 'convexity-inequality']"
4166946,"A six digit number is formed randomly using digits $\{1,2,3\}$ with repetitions. Choose the correct option(s):","A six digit number is formed randomly using digits $\{1,2,3\}$ with repetitions. Choose the correct option(s): A) Probability that all digits are used at least once is $\dfrac{20}{27}$ B) Probability that digit $1$ is used odd number of times and $2$ is used even number of times is $\dfrac{1-3^{-6}}4$ C) Probability that all digits are used as well as odd digits are used odd number of times and even digit is used even number of times is $\dfrac{3^6-2^7+1}{4\cdot3^6}$ Total cases = $3^6$ A) All digits used at least once = Total cases $-$ Cases when one digit is not used Or, would it be All digits used at least once = Total cases $-$ Cases when one digit is not used $-$ Cases when two digits are not used In any case, I am not getting $\frac{20}{27}$ B) $1$ can be used $1$ or $3$ or $5$ times. $2$ can be used $0$ or $2$ or $4$ times. So, favorable cases= $^6C_1(^5C_0+^5C_2+^5C_4)+^6C_3(^3C_0+^3C_2)+^6C_5=182$ , and it matches with the answer. But the answer given in the option is of different format $\dfrac{3^6-1}{4\cdot3^6}$ . Looks like they are subtacting $1$ from the total cases and diving by $4$ to get the favorable case. Why? C) $1$ can be used $1$ or $3$ or $5$ times. $2$ can be used $2$ or $4$ times. So, favorable cases= $^6C_1(^5C_2+^5C_4)+^6C_3(^3C_2)+^6C_5=156$ But as per the option, favorable cases= $150.5$ . Also, looks like they are subtracting $2^7-1$ from total cases and then diving by $4$ , what could be the motivation for this, even if this is wrong?","['proof-explanation', 'solution-verification', 'combinatorics', 'probability']"
4166955,Password combinations help,"I've been looking through other posts here about combinations/permutations regarding possible numbers of passwords for a given set of rules, but I can't get my head around it and wondered if someone could help me. I have an example in mind for a password problem. So, how many valid passwords are there if: The password must be exactly 6 characters long. The password can only contain lowercase letters (a to z) and digits    (0 to 9). The password must contain 4 letters and must contain 2 digits. Digits can't be repeated Letters can be repeated So I thought the answer would be: P(6,2) x 10 x 9 x 26^4 P(6,2) I've calculated as: 6! / (6 - 2)! = 30 So final answer: 30 x 10 x 9 x 26^4 Is this right? Or should I be using combinations? Honestly I've been looking on here for ages and reading through different answers and inclusion/exclusion but I can't figure it out so I'd really appreciate someone walking through it. Thanks","['permutations', 'combinations', 'discrete-mathematics']"
4166968,How to prove that the biggest solution of $\dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon$ is $x(t)=(ct)^{1/(1-\varepsilon)}$?,"Let $x:[0,\infty[\to[0,\infty[$ be a differentiable function satisfying $$\dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon$$ for all $t\in[0,\infty[$ , a fixed $c\ge 0$ and a fixed $\varepsilon\in[0,1[$ . Furthermore, assume that $x(0)=0$ . I want to prove that $x(t)\le (ct)^{1/(1-\varepsilon)}$ for all $t\ge 0$ . Note: Morally speaking, there are only the solutions $x=0$ and $x(t)=(ct)^{1/(1-\varepsilon)}$ . The latter solution is obtained by separating variables, i.e. by writing $$\dot x(t)x(t)^{-\varepsilon}=\frac{c}{1-\varepsilon}$$ and then by integrating. However, that approach requires $x(t)\neq 0$ , which I can't assume in my case. Note 2: The usual uniqueness result for ODEs does not hold in this case since $x\mapsto x^\varepsilon$ is not Lipschitz-continuous around $x=0$ .","['initial-value-problems', 'ordinary-differential-equations']"
4167036,Prove that the Gaussian curvature of such a surface is constant.,"The first fundamental (FF) form of a surface is given by $$ds^2 = \frac{du^2}{(u^2+v^2+c^2)^2} + \frac{dv^2}{(u^2+v^2+c^2)^2}$$ Prove that the Gaussian curvature of such a surface is constant I was trying to find the coefficient of the second fundamental (SF) form first. From the condition we have $$s_u\cdot s_u = s_v\cdot s_v = \frac{1}{(u^2+v^2+c^2)^2}$$ For the (SF), I have to find $s_{uu}, s_{uv}$ and $s_{vv}$ but with the dot products given I'm not sure how to do it. I tried to differentiate w.r.t $u$ the dot product $s_u^2$ which gave $$2\cdot s_u \cdot s_{uu} = -\frac{8(u^2+v^2+c^2)u}{(u^2+v^2+c^2)^3}$$ However, how can I extract $s_{uu}$ out of here? Or is there any other approach?","['surfaces', 'curvature', 'differential-geometry']"
4167074,Confusion about a step of a proof of countability of algebraic numbers.,"I am trying to understand the proof of the fact that The set of all algebraic numbers is a countable set. The proof I am reading is from ""Theory of sets"" by E.Kamke.The proof is quite similar to the proofs in some other website ( Link 1 ) ( Link 2 ). It starts from defining the height $h$ of a polynomial: $$h = n + a_n + |a_{n-1}| + ... + |a_1| + |a_0|$$ Or in the second website $$h = n + \sum_{i=0}^n |a_i| $$ What I dont understand is that , why do we also need $n$ ?",['elementary-set-theory']
4167077,Generalized eigenvectors of arbitrary Hilbert spaces,"Per wikipedia: If $\lambda$ is an eigenvalue of $T$ , then the operator $T - \lambda I$ is not one-to-one, and therefore the inverse $(T - \lambda I)$ is not defined. However, the inverse statement is not true: the operator $T - \lambda I$ may not have an inverse, even if $\lambda$ is not an eigenvalue. Thus the spectrum of an operator always contains all its eigenvalues, but is not limited to them. The article goes on to give an example from $\ell^2(\mathbb{Z})$ of an operator with such a value in its spectrum (namely, the right-shift operator). I know that we can employ some machinery to ""recover"" eigenvectors corresponding to these values.  For example, for $L_2(\mathbb{R})$ , we can construct the Gelfand triple $H^s(\mathbb{R}) \subseteq L_2(\mathbb{R}) \subseteq H^{-s}(\mathbb{R})$ and recover, for instance, the ""generalized eigenvectors"" of the momentum operator $-i\frac{d}{dx}$ . In that case, it is easy to interpret the generalized eigenvectors - they are sinusoids.  But what about the earlier case of $\ell^2(\mathbb{Z})$ ?  Here, I have no real intuition, which suggests that I'm missing something fundamental in my understanding. Is there anything that can be said about what these ""generalized eigenvectors"" look like, in general?  Is there any theory characterizing what ""sort"" of operators on a general Hilbert space require this sort of additional machinery to recover their eigenbases, and why? Edit: It occurs to me upon reflection that the generalized eigenvectors of the right-shift operator on $\ell^2(\mathbb{Z})$ must be those sequences that grow or decay geometrically.  This bears a notable similarity to the above case of $L_2(\mathbb{R})$ , though again I don't feel like I quite grasp the nature of the relationship.","['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
4167094,$\mathbb{E}[M_{\tau+\theta}M_{\tau}]=\mathbb{E}[M_{\tau}^2]$,"Let $\{M_t\}_{t \geq 0}$ be a martingale with mean zero and filtration $\{\mathcal{F}_t\}_{t \geq 0}$ . I want to show that if $\tau$ and $\theta$ are stopping times bounded by $T < \infty$ then $$\mathbb{E}[M_{\tau+\theta}M_{\tau}]=\mathbb{E}[M_{\tau}^2].$$ If I had $\mathbb{E}[M_{t+s}M_{t}]$ with $s,t \in \mathbb{R^+}$ , I would know how to solve it: $$\mathbb{E}[M_{t+s}M_{t}]=\mathbb{E}[\mathbb{E}[M_{t+s}M_{t} | \mathcal{F}_t]]=\mathbb{E}[M_{t}\mathbb{E}[M_{t+s}|\mathcal{F}_t]]=\mathbb{E}[M_{t}^2].$$ But I have stopping times instead of deterministic times. I think I have to use the optional stopping theorem but I'm not sure how. I might be missing some hypothesis on the martingale (so that we can apply the optional stopping theorem) so , is there any assumption I have to make about $\{M_t\}_{t \geq 0}$ ? (for example convergence in $L^1$ )","['martingales', 'stopping-times', 'probability-theory']"
4167117,Does derived pushforward commute with exterior powers?,"Let $q : C \times X \rightarrow C$ be the projection, where $C$ is a curve and $X$ is a smooth projective variety $X$ . Consider the associated derived pushforward $q_* : D^b(X \times C) \rightarrow D^b(C)$ . Let $E$ be a sheaf on $X \times C$ . My question is: when is it true that the $k$ -th exterior power commutes with derived pushforward, i.e. when is it true that $\Lambda^k q_*(E) \cong q_*(\Lambda^k E)$ ? Thanks!","['vector-bundles', 'algebraic-geometry', 'linear-algebra', 'derived-functors']"
4167147,Collection of all compact subsets of a Hausdorff space $X$ is compact if and only if $X$ is compact.,"Let $X$ be a Hausdorff space. Let $K(X)$ be the collection of all compact subsets of $X$ . A topology on $K(X)$ is defined by a subbasis given by sets of the form $I_U=\{K\in K(X)\,|\,K\subset U\}$ and sets of the form $D_U=\{K\in K(X)\,|\,K\cap U\neq\emptyset\}$ , for every $U$ open in $X$ . Show that $K(X)$ is Hausdorff. Show that $K(X)$ is compact if and only if $X$ is compact. I may use the result that a topological space given by a subbasis is compact if and only if every open covering consisting of subbasis elements contains a finite collection that also covers the space. For 1., take two compact subsets $K,M\in K(X)$ and $K\neq M$ . Take a point $x_0\in M$ . Because $X$ is Hausdorff, we can take disjoint opens $U$ and $V$ with $K\subset U$ and $x_0\in V$ . This means that $K\in I_U$ and $M\in D_V$ . Assume $F\in I_U\cap D_V$ , then $F\subset U$ and $F\cap V\neq\emptyset$ , which contradicts $U\cap V=\emptyset$ , so $I_U\cap D_V=\emptyset$ . We conclude that $K(X)$ is Hausdorff. For 2., I honestly have no idea how to start even with the given hint All help is welcome, thank you!","['separation-axioms', 'general-topology', 'compactness']"
4167187,$\sin (\frac{\pi}{4n}) \ge \frac{\sqrt{2}}{2n}$ for all $n\in \mathbb N$,"I have found this weird inequality in a mathematical magazine. $$\sin \left(\frac{\pi}{4n}\right) \ge \frac{\sqrt{2}}{2n}, n\in \mathbb{N}$$ I tried using induction, but couldn't continue. Then, using the double angle formula, I managed to get an equivalent inequality: $\cos\left(\frac{\pi}{2n}\right) \le 1-\frac{1}{n^2}$ and tried proving it with induction, but, again, it couldn't do it. Could you help me? I am not very good with trigonometric inequalities...","['contest-math', 'trigonometry', 'inequality']"
4167204,Evaluating $\int_{0}^{\frac{\pi}{2}} x^n \csc(x) dx$,"Just from messing around on WolframAlpha, I've come across an interesting integral. For the integral $$
\operatorname{I}\left(n\right) =
\int_{0}^{\frac{\pi}{2}}x^{n}\csc\left(x\right)\,{\rm d}x\,,
$$ there are some pretty interesting answers when evaluating them at certain values. For example: \begin{align}
\operatorname{I}\left(1\right) & = 2C
\\[2mm]
\operatorname{I}\left(2\right) & =
2\pi C - {7 \over 2}\,\zeta\left(3\right)
\\[2mm]
\operatorname{I}\left(3\right) & =
\frac{1}{128}\left[192\,\pi^{2}\,C -\psi^{\left(3\right)}
\left(\frac{1}{4}\right) +\psi^{\left(3\right)}\left(\frac{3}{4}\right)\right]
\\[2mm]
\operatorname{I}\left(4\right) & =
\pi^{3}\,C -
24\pi\,{\rm i}\operatorname{Li}_{4}\left(-{\rm i}\right) + \frac{93}{2}\,\zeta(5) - \frac{7}{480}\,{\rm i}\pi^{5}
\end{align} With each step up, we introduce a new family of functions. I cannot for the life of me distinguish a pattern to try to come up with what $\operatorname{I}\left(n\right)$ might be. The only thing that I can discern, is obviously $\operatorname{I}\left(n\right)$ includes a term including $\pi^{n-1}\, C$ . Can someone give me some kind of idea as to how I might evaluate these $?$ .","['integration', 'trigonometric-integrals', 'definite-integrals']"
4167230,Why do Steen and Seebach say an ultrafilter can only have at most one cluster point?,"There's probably a really obvious answer that I've overlooked, but please can someone tell me why an ultrafilter can have at most one cluster point (according to page 10 of the book Counterexamples in topology by Steen & Seebach).  Specifically, the book says: If a point $x$ is in all the sets of a filter we call it a cluster point ; clearly an ultrafilter can have at most one cluster point. They give no additional explanation of why this is ""clear"", and I don't see why.","['elementary-set-theory', 'filters', 'general-topology']"
4167278,"$X$ rational surface, $\mathcal{F}$ nef, then $h^2(X,\mathcal{F})=0$","I'm reading this paper on anticanonical surfaces, i.e. surfaces such that $-K_X$ is effective. Throughout the paper $X$ is a smooth projective surface over an algebraically closed field. In the proof of Lemma II.2, he only assumes $X$ is rational (not yet anticanonical) and claims that the following result is elementary: (c) If $\mathcal{F}$ is a nef divisor, then $h^2(X,\mathcal{F})=0$ and $\mathcal{F}^2\geq 0$ . I know that $\mathcal{F}^2\geq 0$ is a classical result on nef divisors (it's true even if $X$ is not rational), but I can't see why $h^2(X,\mathcal{F})=0$ and why $X$ being rational should matter. The only thing I can think of is Serre duality, so that $h^2(X,\mathcal{F})=h^0(X,K_X-\mathcal{F})$ , but I don't know what to make of this.","['divisors-algebraic-geometry', 'algebraic-geometry', 'sheaf-cohomology']"
4167330,"Show that $\min(a,b,d) \leq \frac{ac-bd}{a-b+c-d} \leq \max(a,b,d)$ holds","Either of 1 or 2 holds. $a > b, b<c, c>d$ and $d<a$ $a < b, b>c, c<d$ and $d>a$ . I wish to show that $$\min(a,b,d) \leq \cfrac{ac-bd}{a-b+c-d} \leq \max(a,b,d)$$ I thought of splitting into 9 cases. However, is there any simpler way? I am open to any help to start.","['algebra-precalculus', 'discrete-mathematics']"
4167343,A tight collection of probability measures on space of probability measures,"Let $E$ be a Polish space, and let $\mathcal M_1(E)$ denote the space of probability measures on $E$ . I want to show the following: A collection $\mathcal K \subset \mathcal M_1\left(\mathcal M_1(E)\right)$ is tight if and only if, for any $\epsilon > 0$ , there exists a compact set $K \subset E$ for which $$
\tilde \mu\left(\left\{ \mu \in \mathcal M_1(E) : \mu\left(K^c\right)>\epsilon\right\}\right)< \epsilon \quad \forall \tilde \mu \in \mathcal K.
$$ I'm having trouble in both directions. My thinking for ""only if"" is to show that given any $\epsilon > 0$ , every compact subset $L \subset \mathcal M_1(E)$ (in the weak* topology) is contained in some subset of the form $$
\tilde K_\epsilon := \left\{\mu \in \mathcal M_1(E) : \mu(K^c) \leq \epsilon\right\} .
$$ Since $E$ is Polish, every probability measure is inner regular, so every $\mu \in \mathcal M_1(E)$ lies in some subset $\tilde U_{K,\epsilon} = \left\{\mu \in \mathcal M_1(E) : \mu(K^c) < \epsilon\right\}$ . If these subsets were open, then by compactness of $L$ , we could take a finite union of such sets $\tilde U_{K,\epsilon}$ and we'd be done with ""only if"". But these subsets aren't open in general (see Counterexample 1 below). For ""if"", this would be immediate if $\tilde K_\epsilon$ were compact. It's easy to show $\tilde K_\epsilon$ is closed: if $\left(\mu_n\right)\subset\tilde K_\epsilon$ and $\mu = \displaystyle \mathop{\textrm{w-lim}}_{n\to\infty} \mu_n$ , by the Portemanteau theorem, since $K^c$ is open, $$
\mu(K^c) \leq \liminf_{n \to \infty} \mu_n(K^c) \leq \epsilon,
$$ so $\mu \in \tilde K_\epsilon$ . To show compactness, by Prohorov's theorem, we need only show that $\tilde K_\epsilon$ is tight. But this also may not be true (see Counterexample 2 below). Any suggestions for other strategies I might consider? Counterexample 1: In general, $\tilde U_{K,\epsilon} \subset \mathcal M_1(E)$ need not be open in the weak* topology. Take $\mu, \mu_n \in \mathcal M_1(\mathbb R)$ to be $$
\mu_n = \frac 1 2 \left(\delta_{-1/n} + \lambda|_{\left( -\frac 1 n, 1-\frac 1 n\right]}\right), \quad \mu = \frac 1 2 \left(\delta_0 + \lambda|_{\left(0,1\right]}\right)
$$ where $\lambda$ is Lebesgue measure and $\delta_x$ are Dirac measures. In this case, $\mu([0,1/2]^c) = 1/4$ , but $\mu_n([0,1/2]^c) > 1/2$ , so using $K = [0,1/2]$ and $\epsilon = 1/2$ , $\tilde U_{K,\epsilon}$ isn't sequentially open. Counterexample 2: In general, $\tilde K_\epsilon \subset \mathcal M_1(E)$ need not be tight. Let $\beta\in (0,\epsilon/2)$ , and define $\mu_n$ by $$
\mu_n = \left(1-\frac\epsilon 2\right)\lambda|_{[0,1)} + \frac \epsilon 2 \delta_{n+1}
$$ Then $\mu_n\left([0,1]^c\right) = \frac\epsilon 2 < \epsilon$ for all $n$ , so $(\mu_n) \subset \tilde K_\epsilon$ for $K = [0,1]$ . However, for any $a > 0$ , there is an $n \in \mathbb N$ for which $\mu_n\left([-a,a]^c\right) = \epsilon/2 > \beta$ . So $\tilde K_\epsilon$ cannot be tight.","['measure-theory', 'weak-convergence', 'real-analysis', 'weak-topology', 'probability-theory']"
4167344,What angles are possible when combining two pipe fittings?,"Plumbers often combine two 45° elbows (1/8 bends) or a 45° wye and an 1/8 bend to give them more freedom through rotation before gluing PVC pipes. Often, this is necessary because the incoming pipe has a vertical angle (of 1/4"" per foot) when viewed in elevation, the outgoing pipe has its own vertical angle (again of 1/4"" per foot), and the plumber wants some horizontal angle between the two pipes (when viewed from above in plan). For a single fitting, a previously answered question shows that the space of available angles forms a cone through rotation ( New angle formed after rotating pipe ). What is the space of available vertical and horizontal angles between the two pipes? How could I visualize this? Can this be generalized to other combinations of angle fittings (1/4 bends, 1/8 bends, and 1/16 bends)?","['visualization', 'angle', 'geometry']"
4167366,Measurability in Linear Regressions,"Let $Y,X_1,X_2,\dots ,X_k$ be random vectors defined on a probability space $(\Omega,\mathcal F,P)$ and  taking values in $\mathbb R^n$ . Given $\omega\in\Omega$ , let $\mathcal M(\omega)=\text{span}\{X_1(\omega),\dots,X_k(\omega)\}$ , and define $\hat{X}(w)$ by $$\|Y(\omega)-\hat{X}(w)\|=\inf_{v\in M(\omega)} \|Y(\omega)-v\|$$ Note that $\hat{X}(w)$ is well-defined by the projection theorem. Is the map $\omega\mapsto \hat{X}(w)$ $\mathcal F$ -measurable? EDIT: Using Tomas comment we can note that $$\hat{X}=X[X'X]^{+}X'Y, $$ where $X:=[X_1,X_2,\dots ,X_k]$ is a random $n\times k$ matrix  and $[X'X]^{+}$ denotes the Moore-Penrose inverse of $X'X$ . From here we have that $A^{+}$ is a measurable function on the space of $k\times k$ matrices $A$ , and therefore we get that each component of $[X'X]^{+}$ is $\mathcal F$ -measurable. Then it follows that $\hat{X}$ is $\mathcal F$ -measurable, since measurability is preserved under matrix multiplication. Is this argument correct? Thanks a lot for your help.","['measure-theory', 'statistics', 'measurable-functions', 'convex-analysis', 'probability-theory']"
4167411,"Does the existence of an associative, injective function imply the underlying set has only one element?","Assume we have a function $f: \Sigma \times \Sigma \to \Sigma$ sucht that $f$ is associative and injective. Does this imply $|\Sigma| = 1$ ? My reasoning is the following: Let $x, y, z \in \Sigma$ . By associativity we have $f(f(x, y), z) = f(x, f(y, z))$ and by injectivity $f(x, y) = x $ as well as $f(y, z) = z$ . But since $x, y, z $ are arbitrary this implies: $\forall x, y \in \Sigma : f(x, y) = x \land f(x, y) = y$ . So $\forall x, y \in \Sigma : x = y$ which would mean $|\Sigma| = 1$ Is this reasoning correct or did I make a mistake somewhere?","['functions', 'abstract-algebra']"
4167415,Transpositions and cycle graphs,"Recently I've stucked with the following problem. Let $\Gamma$ be a cycle graph on $n$ vertices(vertices are basically numbers from $1$ to $n$ ). Consider the set of transpositions in $S_n$ which corresponds to the edges in $\Gamma$ : edge from $a$ to $b$ is associated with transposition $(a \space b)$ . Prove that the product in $S_n$ of all such a transpositions can be decomposed into product of two disjoint cycles. Well, since our graph is actually a cycle then within each transposition $(a \space b)$ in our set there also will be exactly one $(a \space c)$ and exactly one $(b \space d)$ for some $c \ne b$ , $d \ne a$ . Now we need to show that the product of all such a transpositions can be decomposed into two disjoint cycles. Any ideas?","['graph-theory', 'permutations', 'discrete-mathematics']"
4167438,"If $X$ is a supermartingale and $S$ a bounded stopping times, then so is the stopped process $X^S$.","Consider the stopped process: $X^S_t(\omega):=X_{S\wedge t}(\omega)$ . If $X$ is a supermartingale and $S$ a bounded stopping times, then the stopped process $X^S$ is also a supermartingale. My attempt: By a lemma, since $S\wedge t$ is a stopping time, we know that $X_{S\wedge t}$ is $\mathcal{F}_{S\wedge t}$ -measurable. $\forall_{m\geq n} \ S\wedge n \leq S\wedge m \Rightarrow E(X_{S\wedge m} \mid \mathcal{F}_{S\wedge n})\leq X_{S\wedge n}$ , by the Optional Stopping–Bounded Stopping Times theorem. Now, I only need to prove that $X_{S\wedge t}$ is integrable for all $t$ ... I know that $X_t$ is integrable for all $t$ , and that $X_{S\wedge t}=\sum^K_{i=0}X_{i\wedge t}1_{S=i}$ , and so, $X_{S\wedge t}$ is integrable. Is this correct?","['stochastic-processes', 'stopping-times', 'probability-theory', 'martingales']"
4167447,Is there a solvable differential equation with a nonsolvable lie group of symmetries?,"For a polynomial equation in one variable over $\mathbb{Q}$ , it is well known that the equation is solvable by radicals if and only if the equation's Galois group (which is a finite group) is solvable. The 'only if' part is important - we need it to prove that an equation of the fifth degree exists which is not solvable by radicals. For differential equations, we have the following theorem, which I quote from [1] (theorem 2.64, page 151): If an n-th order ordinary differential equation admits an n-parameter solvable group of symmetries, then the equation is solvable by quadratures - that is, the general solution to the equation can be expressed by integrals. Here by an 'n-parameter solvable group' we mean a Lie group of dimension n which is solvable. By the equation ""admitting"" the group we mean that the group, acting on $\mathbb{R}^2=X \times Y$ (where $x$ is the independent variable in the equation and $y$ is the dependent variable), transforms solutions of the equation to other solutions of the equation. Is the converse to the above theorem true? that is, If an n-th order ordinary differential equation is not solvable by quadratures, does it follow that the equation does not admit a solvable n-parameter group of symmetries? If this converse is not true, is it because there are differential equations solvable by quadratures which do not admit an n-parameter group of symmetries at all ? Or is it because there are  differential equations solvable by quadratures which admit a nonsolvable n-parameter group of symmetries? Whatever of these cases is correct, is there a concrete example of such a differential equation? [1] Olver, Peter J. , Applications of Lie groups to differential equations., Graduate Texts in Mathematics. 107. New York: Springer-Verlag. xxviii, 513 p. (1993). ZBL0785.58003 .","['ordinary-differential-equations', 'differential-algebra', 'differential-field', 'lie-groups', 'solvable-groups']"
4167471,Arc-Length of Curves,"I want to calculate the arc length of a full cycle(? by this I mean from $t=0$ to $t=2\pi$ ) of a cardioid. The parameterization of a cardioid is given by: $$
\gamma(t)=(2\cos(t)-\cos(2t),2\sin(t)-\sin(2t))
\\
|\gamma'| = \sqrt{8-8\cos(t)}
$$ And thus, the arc length of a full cycle/rotation is: $$
\int^{2\pi}_0 \sqrt{8-8\cos(t)} dt.
$$ Which is non-negative. That is why I don't think we need to take the absolute value of the function. The issue, why am I getting $0$ as the value of the integral? Furthermore, to tackle this, because of symmetry, I tried to take $\int^\pi_0 |\gamma'|dt$ instead, then multiply it by 2. However, the result of this integral is negative. What? A non-negative function everywhere has a negative integral? Edit : $$
u = 8+8cos(t) \Rightarrow \frac{du}{dt} = -8sin(t),
\\
\int^{2\pi}_0 \sqrt{8-8cos(t)}dt = \int^{2\pi}_0 \sqrt{8-8cos(t)} \frac{\sqrt{8+8cos(t)}}{\sqrt{8+8cos(t)}}dt=
\\
=\int^{a'}_{a''}\frac{|8sin(t)|}{\sqrt{u}}\cdot\frac{1}{-8sin(t)}du
$$ Now, I think I realize my mistake. I should separate this into 2, from $0$ to $\pi$ , and from $\pi$ to $2\pi$ . The final result would be $$
\int^0_{16} -\frac{1}{\sqrt{u}}du + \int^{16}_0 \frac{1}{\sqrt{u}}du
$$ Note that $a':= u = 8 + 8cos(\pi)=0, a'':= 8 + 8cos(0) = 16$ for the first integral, and $a':= u = 8 + 8cos(2\pi)=16, a'':= 8 + 8cos(pi) = 0$ So, the final answer will be 16.","['calculus', 'differential-geometry']"
4167473,Find $f \circ g$ of $f(x) = x^2$ and $g(x) = x+2$,Let $f(x) = x^2$ and $g(x) = x + 2$ . Domain is all real numbers for both. Need to find $f \circ g$ and simplify the equation as much as possible. Am I correct in understanding that the $f \circ g$ will be $f(g(x)) = x^2+2$ ? And how can I simplify it further? PS: Apologies for such a rudimentary question. I have returned to college and am taking a basic maths class EDIT: Thanks to Brian's comment realized $f \circ g$ will be $x^2 + 4x + 4$ .,"['algebra-precalculus', 'functions', 'relations']"
4167524,How to sample random variables X and Y from a joint distribution.,"I have the following joint distribution function $f(x,y)$ : $$f(x,y)=\begin{cases}
\frac{1}{30}xy+\frac{x}{y^2} & \text{ for } 1\le y\le 4,\ 1/2\le x\le 3/2\\
0 & \text{ otherwise.}
\end{cases}$$ The task is to simulate $N$ samples programatically, of $X$ and $Y$ to compute $E(X)$ , $E(Y)$ , $Var(X)$ and $Var(Y)$ . So far, I have tried the following steps: Calculate the joint CDF from the above PDF. Use some multi-variate Inverse transform sampling And then from the samples of X an Y, calculate sample mean and variance. I need to know what inverse transform I can use for this PDF and how I can go about calculating sample mean and variance from the samples of X and Y. That is to say, if my approach is not entirely wrong.","['simulation', 'probability-distributions', 'sampling', 'probability', 'random-variables']"
4167535,A formula for the $n^{th}$ derivative of a parametric equation,"Assume $x(t) \in C^{(n)}[0,\infty)$ . I am trying to find a formula for the $n^{th}$ derivative of $$Q_j(t) = e^{-x(t)}\,\frac{x(t)^j}{j!}, $$ in terms of $Q_0(t), Q_1(t), ..., Q_j(t)$ . (Or any other useful recursive formula in terms of previous derivatives, etc.) So far I have tried this: $$ Q^{'}_0(t) = -x^{'}(t)\,Q_0(t),$$ and for $j\geq1$ , $$ Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right).$$ Or we can assume $0=Q_{-1}(t)=Q_{-2}(t)=...$ and write it in a general form for $j\geq0$ , $$ Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right).$$ Therefore, for $j\geq0$ , the second derivative is \begin{equation}
\begin{split}
 Q_j^{""}(t) &= x^{""}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right) + (x^{'}(t))^2\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right). 
\end{split}
\end{equation} And for the 3rd derivative I got this, \begin{equation}
\begin{split}
Q^{(3)}_j(t) &= x^{(3)}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right)+ 3\,x^{'}(t)\,x^{""}(t)\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right) \\
& + (x^{'}(t))^3\,\left(Q_{j-3}(t)-3\,Q_{j-2}(t)+3\,Q_{j-1}(t)+ Q_{j}(t)\right).
\end{split}
\end{equation} Can we see a pattern here?","['parametric', 'derivatives', 'recursion']"
4167536,Generative model evaluation metric : Precision & Recall,"In this paper , a new metric was proposed to evaluate generative model. The equation (1) decomposes generative distribution and real distribution into two parts w.r.t their intersection of the supports. Then he demonstrated the case that the distribution on the support $S$ is equal with pictures (a)-(d) in Figure 2. My first question: In the picture (a), the distribution $P$ has two modes, and $Q$ has one mode but higher than $P$ . Why they have same distribution over the $S$ ? Then, the author discussed the case when the distribution over $S$ are different and he decomposed the distribution similarly like above. And the define a PRD function in definition 2. My second question is that after my generative model was trained, its distribution was determined. When two distribution were given, shouldn't the alpha and beta be constants? Why in the definition said ""all (alpha,beta) satisfying definition 1""?","['machine-learning', 'statistics']"
4167554,Spivak's Proof that continuous differentiability implies differentiable,"This is from Spivak's Calculus on Manifold and I have some questions about the assumptions and implications of this proof: I noticed one of the assumptions regarding the partial derivatives is that they exist not just  at the point a, but also in an open set containing a. What if the partial derivatives only exist at the point a (and is also continuous at the point a) but nowhere else? Is there a counterexample showing that f is no longer differentiable with this change in assumption? This proof does not seem to care whether we are using the standard basis vectors or not for our partials. If we choose non orthonormal basis, would this proof still be valid? So for example in $\mathbb{R}^2$ . if we choose $\{(1,1),(1,0)\}$ as our basis and coordinate axis and proved the partial derivative with this basis is continuously differentiable, does the conclusion still hold? If what I said above is correct, then is there an alternative proof to this using directional derivatives? Say we are given the standard basis (in $\mathbb{R}^2$ , but instead, we choose two linearly independent vectors and proved that the function defined in those directions are continuously differentiable at the point a, does that still allow us to conclude the function is differentiable at the point a?","['partial-derivative', 'multivariable-calculus']"
