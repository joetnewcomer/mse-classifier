question_id,title,body,tags
4217541,Understanding conditional expectation using measure-theoretical definition,"Definition: For a random variables $X\in\mathbb R^{d_1}$ and $Y\in\mathbb R^{d_2}$ , we define a conditional expectation of $X$ given $Y$ by any random variable $Z$ satisfying: there exists $g:\mathbb R^{d_2}\rightarrow\mathbb R^{d_1}$ such that $Z=g(Y)$ and $\mathbb E\left[Z\unicode{x1D7D9}_{\{Y\in A\}}\right]=\mathbb E\left[X\unicode{x1D7D9}_{\{Y\in A\}}\right]$ for all $A\subseteq \mathbb R^{d_2}$ To be honest I don't understand the definition. Like the reason for requiring $\mathbb E[X|Y]$ to be a function of $Y$ Why $\mathbb E\left[Z\unicode{x1D7D9}_{\{Y\in A\}}\right]=\mathbb E\left[X\unicode{x1D7D9}_{\{Y\in A\}}\right]$ needed for all $A\subseteq \mathbb R^{d_2}?$ Here is one example they mentioned: $\Omega=[-1,1]$ and $\mathbb P$ is uniform distribution. Define $$\begin{align}X(\omega)&=-\frac12+\unicode{x1D7D9}_{\{\omega\in[-1,-1/2]\cup[0,1/2]\}}+2\unicode{x1D7D9}_{\{\omega\in[-1/2,0]\}}\\Y(\omega)&=\unicode{x1D7D9}_{\{\omega\geq0\}}\\Z(\omega)&=1-Y(\omega)\end{align}$$ Then $\mathbb E[X|Y]=Z$ and $\mathbb P(X=Z)=0$ I didn't get how to compute conditional expectation using the above definition. Here is another definition from A First Look at Rigorous Probability Theory, by Jeffrey S. Rosenthal Definition: If $Y$ is a random variable, and if we define $v $ by $v(S)=\mathbb P(Y\in S|B)=\mathbb P(Y\in S,B)/P(B)$ , then $v=\mathcal L(Y|B)$ is a probability measure, called the conditional distribution of $Y$ given $B$ . $\mathcal L(Y\unicode{x1D7D9}_{B})=\mathbb P(B)\mathcal L(Y|B)+P(B^c)\delta_0$ , so taking expectations and re-arranging, $$\mathbb E(Y|B)=\mathbb E(Y\unicode{x1D7D9}_{B})/\mathbb P(B)$$ Here also I can't understand the role of $v$ and how it creates similar thing with above definition.","['conditional-probability', 'measure-theory', 'probability-theory', 'conditional-expectation']"
4217556,Solution of differential equation $(x-3)^2 y'' + (x-1)y' + y = 0$,"How do you solve the following differential equation? \begin{equation}
(x-3)^2 y'' + (x - 1)y' + y = 0, x \ne 3
\end{equation}",['ordinary-differential-equations']
4217585,Stiff ODEs: trouble detecting stiffness from the plot of an ODE.,"I am reading through the very good Leveque book on numerical methods for ordinary and partial differential equations. In chapter 8 he shows a simple equation that is ""stiff"" in the ODE sense of the word. $$
u' = \lambda(u - \cos{t}) - \sin{t}
$$ The solution is: $$
u(t) = e^{\lambda(t - t_0)}(\eta - cos(t_0)) + \cos{t}
$$ So the ratio between $\lambda$ and the $\sin{t}$ will define the stiffness of the equation. That all makes sense. However, when I looked at the plot for this equation I could not understand what made it stiff? I always attribute stiffness with a lot of different time scales happening or a mixture of fast and slow frequencies. But the plot seems rather smooth, so why would the trapezoid rule have so much difficulty finding the solution? That is my real question, based upon the plot below, what is making the solving stiff and causing the trapezoid method to bounce around to much--given that the curve is pretty smooth.","['numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
4217591,Prove or disprove that : $x^{\frac{\sin\left(x\right)}{x}}> \sin\left(x\right)+\frac{1}{x-1}$ for $x\geq \pi$,Hi and sorry for the inconvenience of my last question . I work again with the function : $$f(x)=x^{\frac{\sin\left(x\right)}{x}}$$ Working again with the software Desmos I found that : Claim: Let $x\geq \pi$ then we have : $$f(x)> \sin\left(x\right)+\frac{1}{x-1}$$ I cannot show it but I can prove a weaker result easily : Let $x\geq \pi$ then we have : $$f(x)>\sin(x)$$ The proof is really basic just taking the logarithm we need to show for $\sin(x)>0$ : $$\frac{\ln\left(\sin\left(x\right)\right)}{\sin\left(x\right)}<\frac{\ln\left(x\right)}{x}$$ Wich is obvious because we have : $$\frac{\ln\left(\sin\left(x\right)\right)}{\sin\left(x\right)}\leq 0<\frac{\ln\left(x\right)}{x}$$ I find this problem interesting because it evaluates some extrema of $f(x)$ wich we are unable to find explicitly . Question : How to prove or disprove the claim? Thanks for your try and your efforts in this sense .,"['exponentiation', 'trigonometry', 'inequality']"
4217638,"Is $F(x)=\int_{(-\infty,x)}f\,d\lambda$ Lipschitz continuous?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ Lebesgue integrable. Is $$F(x)=\int_{(-\infty,x)}f\,d\lambda$$ Lispchitz continuous? where $\lambda$ is the Lebesgue measure. I think $F$ is not necessarily Lipschitz continuous. I tried with the functions $\chi_{[1,\infty)}x^{-2/3}$ , $\chi_{(0,\infty)}x^{1/2}$ where $\chi_A$ is the characteristic function of $A$ , and other combinations but non of them gives me a counterexample. Any suggestion?","['measure-theory', 'lebesgue-measure', 'lipschitz-functions']"
4217666,How is linear algebra taught through abstract algebra?,"I have a numerical (so to speak) understanding of linear algebra . I have read a proof-based book on linear algebra where the determinant is defined using adjoints of a row or column..., where the Cramer's rule and the method of Gauss to found an inverse matrix is presented and that sort of stuff. However, when I took a glimpse at Lang's treatment of matrices and linear maps in chapter XIII of his algebra book , I began to notice a lot of uses of abstract algebra definitions (such as groups, homomorphism or isomorphisms - or some similar word), and to sum up, a different approach to it (at least that is what I think - note that I am just beginning my journey on higher mathematics). My question is then whether this approach and use of different tools from abstract algebra is used to correctly and fully prove the statements of linear algebra (something like what real analysis may be in this regard to calculus) or whether these abstract algebra tools are used to actually develop new and different tools. I am therefore asking what is the purpose of treating linear algebra with abstract algebra (if that makes sense). If the latter option is the case, can you give me a broad view of these tools (like names and their purposes...)?","['soft-question', 'abstract-algebra', 'linear-algebra']"
4217690,12 distinct objects are distributed into 10 distinct box such that each box is non-empty. Find probability that no box contain 3 elements,"My thinking :- $n(\text{Sample space})=10^{12}-\binom{10}{1}(10-1)^{12}+\binom{10}{2}(10-2)^{12}-...-\binom{10}{9}(10-9)^{12}$ (Found by principle of inclusion and exclusion). Now we see that one box cannot have more than $3$ objects. for example if one box has $4$ objects then we have to divide $8$ objects into $9$ boxes such that each is non-empty which is not possible. Also no two boxes can have $3$ objects as then we would have to divide $6$ objects into $8$ box such that none is empty which is also not possible. So by my logic the only possibility we have to think about is that one box has $3$ elements and the other $9$ objects are divided into 9 box such that each is non empty. And then the probability of our required event is just $1-$ the probability of the above event. If I view that I have to put 9 distinct object into 9 box such that none is empty , then I am just looking at permutation of 9 things(sort of like defining a bijective function from a set of 9 elements to another).I am concluding this from the fact that no box cannot have more than 1 element(as it would lead to 7 objects into 8 box) Now the possibilities of the above event is just:- $\binom{10}{1}\cdot9!=10!$ . But If I view this again from principle of inclusion and exclusion then I am getting the expression:- $\binom{10}{1}\cdot(\displaystyle\sum_{k=0}^{9}\binom{9}{k}(-1)^{k}(9-k)^{9})$ . Now I know that these to are equal. Can someone suggest me how to prove that this summation also equals to $9!$ ?. So anyways....the probability would just equal:- $$\displaystyle 1-\frac{10!}{(\sum_{k=0}^{10}\binom{10}{k}(-1)^{k}(10-k)^{12})}$$ Is my reasoning correct? . Did I miss some cases? . Also how to prove the summation is same as the factorial? ( Not in a intuitive way . I know that the 9 object in 9 box is 9!, I want to show the summation equals 9!).","['inclusion-exclusion', 'combinatorics', 'balls-in-bins', 'probability']"
4217699,Is there an inner product that does not look like a one-dimensional integral?,"Typical examples of inner products that I've seen are inner products between functions (eg. $\langle f, g\rangle = \int_\mathbb{R} \bar{f}(x) g(x) w(x) dx$ for a weight function $w(x)$ ) or between vectors (eg. $\langle u, v\rangle = \sum_{i} \bar{u_i} v_i$ ). These can be generalized by $\langle u, v\rangle = \int_\mathbb{R} \bar{u(x)} v(x) d\mu(x)$ for a measure $\mu(x)$ - in the first case it's just $d\mu(x) = w(x) dx$ and in the latter it's the counting measure). The Riesz Representation theorem says that any linear functional can be represented as an inner product with a fixed (first) argument, but it doesn't say that the inner product has to be a one dimensional integral. Are there spaces or inner products where the inner product is (or is better represented) as a double integral, such as $\langle u, v\rangle = \int_{\mathbb{R}^2} u(x, y) v(x, y) d\mu^2(x, y)$ ? Or alternatively, are there inner products that do not even look like any integral (of any dimension)?","['hilbert-spaces', 'measure-theory', 'inner-products']"
4217711,Distribution of this strange combination of two random variables,"We have two independent random variables $\gamma_1$ and $\gamma_2$ and we know their CDFs, given by $F_{\gamma_1}(x)$ and $F_{\gamma_2}(x)$ . We select a threshold $\gamma_T$ and then we define a random variable $Z$ as follows: If $\gamma_2 < \gamma_T$ , then $Z = \max(\gamma_1, \gamma_2)$ . If $\gamma_2 \geq \gamma_T$ , then $Z = \gamma_2$ . I am trying to calculate the CDF of $Z$ , $F_Z(x)$ . For the first case ( $\gamma_2 < \gamma_T$ ) I know that $F_Z(x) = F_{\gamma_1}(x) \cdot F_{\gamma_2}(x)$ , however I am unable to calculate the CDF for the second case ( $\gamma_2 \geq \gamma_T$ ). For the second case I have tried $F_Z(x) = F_{\gamma_1}(\gamma_T) \cdot F_{\gamma_2}(\gamma_T) + F_{\gamma_2}(x) - F_{\gamma_2}(\gamma_T)$ but comparing to simulations it doesn't seem to be the correct solution. I have also checked Example 6-17 in page 193 of the book of Papoulis (4th edition), which I think is related, but I'm not sure how to do the calculation. Any ideas?","['probability-distributions', 'probability-theory', 'probability']"
4217716,Is the sum $\sum_{k=0}^n{n \choose k}\frac{(-1)^{n-k}}{n+k+1}$ always the reciprocal of an integer $\big(\frac{(2n+1)!}{(n!)^2}\big)$?,"Denote the sum $$S_n :=  \sum_{k=0}^n{n \choose k}\frac{(-1)^{n-k}}{n+k+1}$$ This value arose in some calculations of polynomial coefficients. I'm not used to dealing with expressions of this sort. Does this arise anywhere else? The first few values are: $1, \frac{-1}{6}, \frac{1}{30}, \frac{-1}{140}, \frac{1}{630}, \frac{-1}{2772}, \frac{1}{12012}$ . My first thoughts were Bernoulli numbers and primorials, but those seem to be red herrings. It looks like they are always the reciprocal of an integer, but I don't know how to prove that. From OEIS A002457 , the absolute values of the reciprocals appear to coincide with $$a(n) = \frac{(2n+1)!}{(n!)^2}$$ If this is the case, the same page would imply that $$S_n = \int_0^1(x(1-x))^ndx$$ I would really appreciate help with: Prove each $\frac{1}{S_n}$ is an integer. Is it actually the $a(n)$ above? If not, is there a nice description? Prove that the sign of $S_n$ alternates. (This is least important)","['summation', 'binomial-coefficients', 'combinatorics', 'polynomials']"
4217737,"Are $C([0,1])$ and $C(\mathbb{R})$ elementarily equivalent as rings?","For a topological space $X$ , let $C(X)$ denote the ring of continuous functions $X\to\mathbb{R}$ , equipped with pointwise addition and multiplication. This question is related to this one of Noah Schweber's; in particular, the question arises from trying to understand how much topological data about $X$ is encoded in the first-order theory of the ring $C(X)$ . For example, $C(X)$ has a non-trivial idempotent if and only if $X$ is disconnected. A natural question to ask is when the first-order theory of $C(X)$ can also detect whether $X$ is compact, and this is the context in which the question below arises. For further elaboration and additional context, see Noah's post and the answers and comments below it. Let $[0,1]$ and $(0,1)$ be the closed and open unit intervals in $\mathbb{R}$ . Note that there is an injective ring morphism $\iota:C([0,1])\hookrightarrow C((0,1))$ that takes a map $\alpha:[0,1]\to\mathbb{R}$ to its restriction $\alpha|_{(0,1)}$ . Question: Is the map $\iota$ elementary? If the answer is no, do we nonetheless have an elementary equivalence $C([0,1])\equiv C((0,1))$ ? I suspect a negative answer, but I don't see a path for showing this. A natural first idea is to somehow try to exploit that $\operatorname{im}\alpha\subseteq\mathbb{R}$ is bounded for every $\alpha\in C([0,1])$ . So, for example, one can define a formula $$u>v\equiv \exists w\big[u-v=w^2\big]\wedge\exists w\big[(u-v)w=1\big].$$ Then, for any space $X$ and any continuous $\alpha,\beta:X \to\mathbb{R}$ , we have $C(X)\models\alpha>\beta$ if and only if $\alpha(x)>\beta(x)$ for each $x\in X$ . Indeed, $C(X)\models \exists w[\alpha-\beta=w^2]$ if and only if $\alpha(x)\geqslant\beta(x)$ for each $x\in X$ , and $C(X)\models \exists w[(\alpha-\beta)w=1]$ if and only if $\alpha(x)\neq\beta(x)$ for each $x\in X$ . With this in hand, it is fairly straightforward to come up with a first-order theory that is satisfiable in $C((0,1))$ but not in $C([0,1])$ , provided we are willing to add an additional constant symbol to our language. Indeed, let $a$ be a new constant symbol, and define a theory $T$ in the language of rings along with $a$ by taking $\neg(a<n)\in T$ for each $n\in\omega$ . Then realizing $a$ as any homeomorphism $(0,1)\to\mathbb{R}$ will make $C((0,1))$ into a model of $T$ , but no realization of $a$ can do the same for $C([0,1])$ , since any continuous image of $[0,1]$ in $\mathbb{R}$ is bounded. However, this is of course not enough to show that $C([0,1])\not\equiv C((0,1))$ as rings, and I don't see an easy way of extending the argument. If there were some way to uniformly define the subring $\mathbb{R}$ in $C([0,1])$ and $C((0,1))$ , then we would be done: if $\theta(w)$ were such a definition, then $C([0,1])$ would be a model of the sentence $\forall v\exists w[\theta(w)\wedge (w>v)]$ and $C((0,1))$ would be a model of its negation. But I'm struggling to come up with such a formula $\theta$ , and I'm not even convinced it can be done. Any insight, either on this approach or on a different one, would be much appreciated.","['model-theory', 'logic', 'ring-theory', 'abstract-algebra', 'general-topology']"
4217746,simplify radicals,"In the book $A=B$ (link) page 11 example 1.5.1 it is written By setting $\sin a = x$ and $\sin b = y$ , we see that the identity $\sin(a + b) = \sin a \cos b + \sin b \cos a$ is equivalent to $$
 \arcsin x + \arcsin y = \arcsin(x \sqrt{1-y^2} + y \sqrt{1-x^2})
$$ When $x = 0$ this is tautologous, so it suffices to prove that the derivatives of both sides with respect to $x$ are the same. This is a routinely verifiable algebraic identity. By differentiating w.r.t. $x$ I obtained: \begin{gather}\frac 1 {\sqrt{1-x^2}}+ \frac {\frac{dy}{dx}} {\sqrt{1\!-\!y^2}}
=\frac{\frac d {dx}\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)}
{\sqrt{1-\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)^2}}=\frac{\sqrt{1\!-\!y^2}\!-\!\frac {xy}{\sqrt{1\!-\!x^2}}+\left(\sqrt{1\!-\!x^2}\!-\!\frac {xy}{\sqrt{1\!-\!y^2}}\right)
\!\frac{dy}{dx}} {\sqrt{1-\left  (x\sqrt{1\!-\!y^2}+y\sqrt{1-x^2}\right)^2}}
\end{gather} where $y$ is somewhat arbitrary function of $x$ . But I cannot reduce this (especially the radical in the denominator of the RHS) to show that the equality holds. Seems to me there is no way they can be equal so do I just assume it is a mistake in the text or is there someone who can show equality? He seems to go about it by implying the squares of both sides are equal using Maple code but I can't show that either (and I don't have Maple so i can't test it that way). Petkovšek, Marko; Wilf, Herbert S.; Zeilberger, Doron , $A=B$ . With foreword by Donald E. Knuth, Wellesley, MA: A. K. Peters. xii, 212 p. (1996). ZBL0848.05002 .","['nested-radicals', 'algebra-precalculus', 'derivatives', 'radicals']"
4217750,Are $\mathfrak{Re}\varphi$ and $\mathfrak{Im}\varphi$ characteristic functions?,"Let $\varphi$ be the characteristic function of a random variable X. Are $\mathfrak{Re}\varphi$ and $\mathfrak{Im}\varphi$ characteristic functions? I'm very stuck on this question. I'm not sure what would be required to prove that something is a characteristic function. It seems like these function does not violate any of the basic properties of characteristic functions https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Properties . So I cannot say that these are not characteristic functions. However, I do not know what is required to show that these definitely are characteristic functions. So far my only thoughts have been to say $\varphi(t)=\mathbb{E}(e^{itX})=f(t)+ig(t)$ for some functions $f:\mathbb{R}\to\mathbb{R}, g:\mathbb{R}\to\mathbb{R}$ , and then the question is whether $f$ and $g$ are characteristic functions. I thought this would be a simple question, but I am quite stuck, so any help would be much appreciated!",['probability']
4217763,Vector Component Transformation Matrix in Shilov's Linear Algebra,"I am working through Georgi Shilov's Linear Algebra, and I am having trouble understanding the vector component transformation matrix definition he gives in section 5.31. I will describe this definition below. Let $e_1, e_2, \dots, e_n$ and $f_1, f_2, \dots, f_n$ be two bases in a vector field of dimension $n$ such that for some quantities $p^{(j)}_i$ we have $$f_j = p^{(j)}_1 e_1 + p^{(j)}_2 e_2 + \dots + p^{(j)}_n e_n$$ Shilov now defines the matrix of the transformation from basis $\{e\}$ to basis $\{f\}$ as $$P = 
\begin{bmatrix} 
p^{(1)}_1 & p^{(2)}_1 & \dots & p^{(n)}_1 \\
p^{(1)}_2 & p^{(2)}_2 & \dots & p^{(n)}_2 \\
\vdots & \vdots & \ddots & \vdots \\
p^{(1)}_n & p^{(2)}_n & \dots & p^{(n)}_b
\end{bmatrix}
$$ So, $P$ is the matrix with components of $f_i$ in terms of the basis ${e}$ as columns. Now, suppose we have some vector $x = \xi_1 e_1 + \xi_2 e_2 + \dots + \xi_n e_n = \eta_1 f_1 + \dots + \eta_n f_n$ . Then, the author claims that the matrix describing the transformation from the components $\xi_1, \dots, \xi_n$ to the components $\eta_1, \dots , \eta_n$ is $$S = (P^{-1})^T$$ In my understanding, the ""matrix describing the transformation from the components $\xi_1, \dots, \xi_n$ to the components $\eta_1, \dots , \eta_n$ "" means that $$S\begin{bmatrix} \xi_1 \\ \xi_2 \\ \vdots \\ \xi_n \end{bmatrix} = \begin{bmatrix} \eta_1 \\ \eta_2 \\ \vdots \\ \eta_n \end{bmatrix}$$ However, that doesn't seem to be the case. Consider an example where $e_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, e_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, e_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ and $f_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, f_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}, f_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ . Then, the matrix of the transformation from $\{e\}$ to $\{f\}$ and the respective component transformation matrix is $$P = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{bmatrix} \quad S = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}^T = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$$ However, if we try to use it to transform the vector $[5, 1, 2]_{\{e\}} = [5, -4, 1]_{\{f\}}$ we get $$\begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 4 \\ -1 \\ 2 \end{bmatrix}$$ Which is not what I expected. However, if we instead take $S = P^{-1}$ , it seems to work out: $$\begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 5 \\ -4 \\ 1 \end{bmatrix}$$ So, am I misunderstanding what the component transformation matrix is supposed to do, or did I construct $P$ incorrectly? I did my best to write the definitions as they are written in the text, however, it is very possible I just mixed up my indices.","['matrices', 'change-of-basis', 'linear-algebra', 'vector-spaces']"
4217786,"Old O level question regarding percentage error - is the wording strange, is my reading ability at fault or is it just a typo?","This is an old O level exam question (O&C) on trigonometry and reads as follows: In Jones's book of mathematical tables, which are torn, the sines of angles near $65^o$ were missing. Jones looked up $\sin 55^o$ and $\sin 75^o$ and thought that if he took their arithmetic mean he would have the right answer. Using your tables, find his percentage error. Solution gives $2.96\%$ Using first tables, then calculator for check, my initial working, to 4 s.f. was as follows: $$\sin 55^o = 0.8192$$ $$\sin 75^o = 0.9659$$ $$\sin 65^o = 0.9063$$ $$arithmetic\ mean = \frac{0.8192 + 0.9659}{2} = 0.8926$$ $$\%error = \frac{0.9063 - 0.8926}{0.9063}\times 100 = 1.52\%$$ This, as you can no doubt see, does not agree with answer in text.
The only way I can get an answer any where near that of the text is to proceed as follows: $$\%\: error\:of\: \sin55^o\:from\:\sin65^o = \frac{0.9063 - 0.8192}{0.9063}\times 100 = -9.611\%$$ $$\%\: error\:of\: \sin75^o\:from\:\sin65^o = \frac{0.9659 - 0.9063}{0.9063}\times 100 = 6.576\%$$ and $$|-9.611\% + 6.567\%| = 3.04\%$$ I should repeat answers given here have been rounded so will not be exact.
However, if this is correct, and it's not that close to the answer given, I'm not sure what I've calculated to get correct answer. Moreover, if this is correct, what did I calculate originally? As always, many thanks in advance for any advice given. I would like to add that this question is taken from an old O level book and not from a specific examination paper so a typo is possible.","['trigonometry', 'arithmetic']"
4217816,"""Uniformly"" Cauchy series of operators on $L_p$ spaces.","Let $(X_j, \mu_j)_{j=0}^\infty$ be a countable family of measure spaces and let $p \in (1,\infty)$ . Consider a sequence $(T_j)_{j=1}^\infty$ of bounded linear operators $T_j: L_p(X_0) \to L_p(X_j)$ such that $$
\sup_{\|f\|_p=1} \left( \sum_{j=1}^\infty \| T_jf\|_p^p \right)^{1/p} < \infty.
$$ Question: Let $\epsilon >0$ . Is it true that there is $N \in \Bbb{N}$ such that for all $m>n>N$ $$
\sup_{\|f\|_p=1} \left( \sum_{j=n}^m \| T_jf\|_p^p \right)^{1/p} < \epsilon?
$$ Progress: I think I've proved this for $p=2$ . Indeed, in this case we use adjoints and notice that $$\sup_{\|f\|_2=1} \left( \sum_j\| T_jf\|_2^2 \right)^{1/2}=\left\|\sum_{j}T_j^*T_j \right\|,$$ whence the desired property follows from the convergence of the series $\sum_j T_j^*T_j$ in $\mathcal{B}(L_p(X_0))$ . However, I do not quite see how to approach this for a different value of $p$ .","['banach-spaces', 'measure-theory', 'operator-theory', 'functional-analysis', 'sequences-and-series']"
4217820,Reference request : Holomorphic functions with values in Banach spaces,"I am looking for books that deal with holomorphic functions with values in Banach spaces that discuss, state and prove analogues of classical results from classical ( $ \mathbb{C}$ -valued) Complex Analysis such as holomorphic $\Leftrightarrow$ analytic, Goursat's Lemma, Cauchy's theorem, Morera's theorem etc ; I am not looking to dig deep in to the theory, but I would rather appreciate a detailed/beginner-friendly exposition of the material I described (if such a reference exists). One result I need is that uniform limits of Banach space-valued holomorphic functions is again holomorphic (or Morera's theorem in case it implies that result just like in the scalar case) so a reference that contains the proof of that result would be very welcome. Regarding my background, I have studied some basic measure theory, Functional and Complex Analysis. Also, I have just started looking into integration for functions $f:S \rightarrow X$ where $S$ is a measure space and $X$ is a Banach space (Bochner integral), but I haven't studied holomorphic functions with values in Banach spaces before.","['complex-analysis', 'banach-spaces', 'functional-analysis']"
4217831,"$\exists c \in\mathbb{R}_+^*,\forall p,r\in \mathbb{R}_+,E[|X_{p+r}-X_r||\mathcal{F}_r] \leq c$ implies the optional stopping theorem","Consider integrable submartingale $(X_r)_{r \in \mathbb{R}_+}$ relative to $(\mathcal{F}_{r})_{r \in \mathbb{R}_+}$ and such that $$\exists c \in \mathbb{R}_+^*,\forall k \in \mathbb{N},E[|X_{k+1}-X_k||\mathcal{F}_k] \leq c \ \ \ \ \ \ (P)$$ $\theta_1$ and $\theta_2$ are two $(\mathcal{F}_r)_r$ -stopping times (taking values in $\overline{\mathbb{R}}_{+}$ ) such that $\theta_1 \leq \theta_2$ and $\theta_2 \in L^1.$ If $\theta_1$ and $\theta_2$ take values in $\overline{\mathbb{N}},$ then we can prove that $X_{\theta_1},X_{\theta_2} \in L^1$ and using the optional stopping theorem that $E[X_{\theta_2}|\mathcal{F}_{\theta_1}] \geq X_{\theta_1}$ a.s. We suppose next that $(X_r)_{r \in \mathbb{R}_+}$ is right-continuous and that $\theta_1$ and $\theta_2$ take values in $\overline{\mathbb{R}}_+$ . The following pictures are taken from stochastic processes , where various conditions imply the continuous optional stopping theorem. Can we say that condition $C_3$ (picture below) is the continuous version of $(P)$ ? If so, how can we prove, under $C_3$ or $(P),$ that $X_{\theta_2} \in L^1$ ? What about the following property (following the construction of $(P)$ ): $$\exists c \in \mathbb{R}_+^*,\forall(p,r) \in (\mathbb{R}_+)^2,E[|X_{p+r}-X_r||\mathcal{F}_r] \leq c \ \ \ \ \ \ (P')$$","['stochastic-analysis', 'stochastic-processes', 'martingales', 'stopping-times', 'probability-theory']"
4217837,Existence and uniqueness of the solution to $dX_{t}=\left(2\chi_{\left\{ X_{t}>0\right\} }-1\right)\cos X_{t}dt+\cos X_{t}dW_{t}$,"Let $\chi$ denotes an indicator variable and $W$ a Wiener process. What can we say about the solution of the following SDE: $$\begin{cases}
dX_{t}=\left(2\chi_{\left\{ X_{t}>0\right\} }-1\right)\cos X_{t}dt+\cos X_{t}dW_{t}\\
X_{0}=0
\end{cases}$$ Is there any solution? Is there any kind of uniqueness in the solution if at least one solution exists? If there is uniqueness, then what kind of uniqueness is this (uniqueness in distribution, in trajectories...)? I have got this exercise in our course, but the coefficient function: $\left(2\chi_{\left\{ x>0\right\} }-1\right)\cos x$ is not even continuous, so it can't be Lipschitz continuous (here $\chi$ is the indicator function). My answer would be that we can't say anything about the solution, however I generated some paths numerically, and I didn't feel any strangeness in it.","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4217840,Why do we usually not need to find the eigenvalues of non-symmetric matrix,"I came across this line in a class note I am reading: In numerical linear algebra, we usually don't need to find the eigenvalues of a non-symmetric matrix. Can someone explain why this is the case? I understand for symmetric matrix, there are many nice properties of eigenvalues. For example the eigenvalues of a real symmetric matrix are real. SVD comes from the eigenvalues of $A^TA$ which is symmetric, etc. But why are we so confident that we usually don't need to find the eigenvalues of non-symmetric matrix? Is it purely because of the nice properties of symmetric matrix that make us tend to formulate our problems that way?","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'numerical-linear-algebra', 'numerical-methods']"
4217888,Many questions about the Chern-Weil theory and the proof of the Bott vanishing theorem,"I'm reading 'Lectures on Chern-Weil theory and Witten deformations' by Weiping Zhang. Now I have many questions: He defined $tr:\Omega^*(M,End(E))\to\Omega^*(M)$ by $\omega A\mapsto\omega tr[A]$ where $tr[A]$ as fiberwise to be a smooth function. Then he proved a lemma: Let $\nabla^E$ is a connection on $E$ , then for any $A\in\Omega^*(M,End(E))$ one has $dtr[A]=tr[[\nabla^E,A]]$ .
However, I know $tr[[\nabla^E,A]]=tr[\nabla^EA]-tr[A\nabla^E]$ . So I know what is $tr[\nabla^EA]$ , but I don't know what is $tr[A\nabla^E]$ ? I can't make it clearly and percisely. As 1, we consider $d$ a trivial connection on $\mathbb{C}^N|_M$ and $g$ is a section of $Aut(\mathbb{C}^N|_M)$ , then he said $g^{-1}dg\in\Omega^1(M,End(\mathbb{C}|_M))$ . Why??? Let $F\subset TM$ be a subbundle where $X,Y\in\Gamma(F)$ , then $[X,Y]\in\Gamma(F)$ . Take $p_{\lambda}(TM/F)$ be Pontrjagin classes of $TM/F$ . We have the Bott vanishing theorem: If $i_1+...+i_k>(\dim M-\dim F)/2$ , then $p_{i_1}(TM/F)\cdots p_{i_k}(TM/F)=0$ in $H^{4i_1+...+i_k)}_{dR}(M,\mathbb{R})$ . When we prove this theorem, we take a Riemannian metric $g^{TM}$ over $TM$ , then $TM=F\oplus(TM/F)$ as orthogonal decomposition. Take Levi-Cevita connection $\nabla^{TM}$ . Then Take $g^F,g^{TM/F}$ induced by $g^{TM}$ and let $\nabla^F=p\nabla^{TM}p,\nabla^{TM/F}=p'\nabla^{TM}p'$ where $p,p'$ are projections. Then we define the Bott connection $\nabla^{TM/F,B}$ on $TM/F$ as: for any $X\in\Gamma(TM),U\in\Gamma(TM/F)$ , if $X\in\Gamma(F)$ , then $\nabla^{TM/F,B}_XU=p'[X,U]$ ; if $X\in\Gamma(TM/F)$ , then $\nabla^{TM/F,B}_XU=\nabla^{TM/F}_XU$ . Take $R^{TM/F,B}$ are curvature of $\nabla^{TM/F,B}$ , one can easy to see that $R^{TM/F,B}(X,Y)=0$ for any $X,Y\in\Gamma(F)$ . Then he said that from this we have $R^{TM/F,B}\in\Gamma((TM/F)^*)\wedge\Omega^*(M,End(TM/F))$ where $(TM/F)^*$ is the dual bundle. But I don't know why??? Actually, this is the most important part of the proof! Moreover, the definition of $\nabla^{TM/F,B}$ is also strange because the Bott connection $\nabla^{TM/F,B}$ on $TM/F$ , why we consider $X\in\Gamma(F)$ ? So as $R^{TM/F,B}(X,Y)$ . Thank you for your help!",['differential-geometry']
4217890,Monoid cohomology,"It is well-known that the group cohomology groups of a group $G$ are isomorphic to the cohomology groups of the classifying space $|G|$ of (the nerve of the delooping) of $G$ . More generally, we have also classifying spaces for monoids , and hence we may analogously define the monoid cohomology groups of a monoid $A$ to be the cohomology groups of $|A|$ . Questions: Has this notion been studied before? How does it relate to group cohomology and group completion? More precisely, given a monoid $A$ , how are the the co/homology groups of $|A|$ and $|A^{\mathrm{grp}}|$ related?","['monoid', 'abstract-algebra', 'homotopy-theory', 'group-cohomology', 'algebraic-topology']"
4217901,"For a group $G$, $N\unlhd G$, $G/N\cong\Bbb{Z}/a\Bbb{Z}$ and $N\cong\Bbb{Z}/b\Bbb{Z}$, where $b<a$ and $(b,a)=1$, show $G$ is abelian.","I've been pounding my head on my desk attempting to figure out how to start this proof: If we have a group $G$ , a normal subgroup $N\trianglelefteq G$ , $G/N \cong \mathbb{Z}/a\mathbb{Z}$ , and $N \cong \mathbb{Z}/b\mathbb{Z}$ , where $b<a$ , and $(b,a) = 1$ , how can we show that $G$ is an abelian group? I'm stuck on how to start this proof. I know the fact that $b$ and $a$ are relatively prime will have something to do with the proof. Hints as to how to approach this are greatly appreciated (but please do not post the entire proof, or at the bare minimum please hide it in spoiler tags).","['cyclic-groups', 'quotient-group', 'normal-subgroups', 'group-theory', 'abelian-groups']"
4217903,Calculate angle $x$ in the figure,"For reference: My progress: \begin{align*} 
& AH \perp FD \\
& \triangle AFD \text{ is isosceles} 
\quad 
\therefore \measuredangle BFA  = \measuredangle FDA = x \\
& AF = FD \\ 
& \measuredangle HBA = 180-135^\circ = 45^\circ 
\quad 
\therefore \triangle HBA \text{ is isosceles}
\end{align*} I drew some auxiliary lines but it wasn't enough to reach the solution.","['euclidean-geometry', 'geometry']"
4217905,Is ultraproduct of separable Hilbert space is separable?,"Let $(H_n)_n$ be a sequence of separable Hilbert spaces and $\omega$ be a free ultrafilter. Is it true that $\Pi_{\omega}H_n$ is also separable? Note that $\Pi_{\omega}H_n$ is the quotient space $\mathcal{l}^{\infty}(\mathbb{N},H_n)/I$ , where $I=\{(x_n)_n\in\mathcal{l}^{\infty}(\mathbb{N},H_n):\ \lim_{n\rightarrow\omega}\|x_n\|=0 \}.$ If we take each $H_n=\mathbb{C}$ , Then $\mathcal{l}^{\infty}(\mathbb{N},H_n)=\mathcal{l}^{\infty}(\mathbb{N})$ , which is non separable. What about $I= \{(x_n)_n\in\mathcal{l}^{\infty}(\mathbb{N}):\ \lim_{n\rightarrow\omega}|x_n|=0 \}$ . Is it separable?","['hilbert-spaces', 'functional-analysis', 'set-theory', 'operator-algebras']"
4217913,Probability of One Geometric Random Variable when Sum of Two is given,"This problem comes directly from MIT OCW 6.041 assignment #4, question #3 .  The solution is given .  I think the solution is wrong.  The question is as follows: Suppose that X and Y are independent, identically distributed, geometric random variables with parameter p. Show that $P(X=i|X+Y=n)=\frac{1}{n-1}$ , for $i$ =1,2,..., $n$ −1. The solution given makes use of the fact that $X$ and $Y$ are independent, specifically the solution writes [Call this equation A] $P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i).$ This seems incorrect to me because if n and i are given, then $Y$ is completely dependent on those two values. $Y$ has been implicitly specified.  So the distributions are not independent.  Further, the supposed solution to the problem is that all the $X=i$ are equally likely if we specify the sum of $X$ and $Y$ , whereas prior to that sum $X$ follows a geometric distribution.  That sure seems like dependence to me.  Just using the law of multiplication the solution should have $$
P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i | X=i).
$$ $X$ is specified, one can't just throw away that dependence.  In other words, the quantity $n-i$ is not independent of $X$ , so $Y$ cannot be independent of $X$ either, given the conditioning. Here is my alternative solution.  I'm going to use Bayes' Rule: $$
P(X=i|X+Y=n)= \frac{P(X+Y=n|X=i) P(X=i)}{\sum_{j=1}^{n-1} P(X+Y=n|X=j) P(X=j)}.
$$ The key, I think, is to recognize that $P(X+Y=n|X=j) = P(Y=n-j)$ .  This is to say, that if $X$ is given to be $j$ , then the distribution of $X+Y$ is just the distribution of $Y$ .  This makes sense to me because I am adding a geometric random variable, $Y$ , to a constant $j$ , so the distribution will also be a geometric random variable.  So, the conditional probability comes out of the sum and cancels, leaving $$
P(X=i|X+Y=n)= \frac{P(X=i)}{\sum_{j=1}^{n-1} P(X=j)} = \frac{p(1-p)^{i-1}}{1-(1-p)^{n-1}},\; \mathrm{for} \; i=1,2,...,n−1.
$$ All the conditional probabilities sum to 1, they are all non-negative, they are exclusive, and they are exhaustive.  This looks like a valid probability distribution to me. Even the content of that final equation looks correct to me. $X+Y=n$ is given, and we want to know what the probability of seeing any particular $X=i$ value is.  We know that $X$ is now limited to the interval $[1,n-1]$ , because it is a geometric variable ( $X \ge 1$ ) and we know that it is added to another geometric variable to get $n$ , so $X \le n-1$ .  So all I have to do is re-weight the probabilities I had before conditioning such that the now-allowed values sum to 1 and I should be done. My question is: why is equation A used in the solution when every time I look at it, it appears wrong and I have a perfectly sensible solution that appears better in every way?  What am I missing about equation A? Edit:
I'm going to answer the opposite of my question.  The mistake I made was pulling $P(X+Y=n|X=j) = P(Y=n-j)$ out of the sum in the denominator.  The identity is correct, but that probability literally depends on $j$ and I can't pull it out of the sum.  That was silly.  Plugging in the probabilities for the geometric variables I get the same answer as the answer key. Furthermore, the identity I argue is true actually proves the identity I thought was untrue.  If I multiply both sides of my identity by $P(X=i)$ and use the multiplication rule to combine the conditional and marginal probabilities, I find $P(X=j∩X+Y=n) = P(Y=n-j)P(X=i)$ , which I spent so many hours thinking was wrong. It appears I made a mathematical error that led me to a specious answer which got me to believing a prior logical error.  What a ride.","['statistics', 'probability']"
4217921,Computing $I=\int \frac{dx}{(x+1)(3x^2+3x+1)^{1/3}}$,"I am trying to solve this integral and find its primitive, unfortunately I have not been successful. I made some variable changes, but I think it has become even more complicated.
My solution is as follows: $$I=\int \frac{dx}{(x+1)(3x^2+3x+1)^{1/3}}$$ $$u=x+\frac{1}{2}\iff u+\frac{1}{2}=x+1$$ $$du=dx$$ $$I=\int \frac{du}{(u+\frac{1}{2})(u^2+\frac{1}{12})^{1/3}}$$ $$\alpha=\frac{1}{2}$$ $$I(\alpha)=\int \frac{(u-\alpha)}{(u^2-\alpha^2)(u^2+\frac{\alpha^2}{3})^{1/3}}du$$ $$I(\alpha)=\frac{1}{3^{1/3}}\int \frac{(u-\alpha)}{(u^2-\alpha^2)(3u^2+\alpha^2)^{1/3}}du$$ I hope you can help me. Regards","['integration', 'calculus']"
4217926,What is the probability that the quadratic equation $ax^2+x+1=0$ has two real roots? [duplicate],"This question already has answers here : Probability that a quadratic polynomial with random coefficients has real roots (6 answers) Closed 2 years ago . A number $a$ is chosen at random within the interval $(-1, 1)$ . What is the probability that the quadratic equation $ax^2+x+1=0$ has two real roots? For it to have its real roots, we must guarantee that $1-4a \geq 0$ , or $a\leq \frac{1}{4}$ . It is no longer clear to me what I have to do.","['geometric-probability', 'probability']"
4217966,What would be the probability density function for a Monte Carlo sampling strategy which only samples a single value?,"Please see my edit for a more concise description of my question Ever since I started looking into the mathematics behind path-tracing algorithms, there has been one question that I haven't been able to find an answer to: How are deterministic samples weighted in an importance sampled Monte Carlo estimation? By deterministic samples, I mean values that will ALWAYS be sampled, as opposed to values which are sampled probabilistically. In importance sampling, non-uniform sampling can be achieved without introducing any bias, by dividing the value of a sample by it's probability of being chosen from a given distribution function. Deterministic samples can't be represented in a probability distribution function, since they are inherently non-probabilistic. My question is: if a specific value or range of values are always sampled, how are they weighted in order to not introduce statistical bias? This is a core mathematical principal behind path-tracing, as samples which connect vertices of a path, which is probabilistically drawn, to light sources are always sampled, in order to reduce variance. This is somehow done without introducing statistical bias, yet I cannot find any information on how this is achieved. EDIT: I have done some more research, and gotten closer to the answer, yet some question still remain. After reading this paper on Multiple Importance Sampling , I have discovered the combined sample density function : here ""n"" represents the number of sampling strategies used, "" "" the fraction of samples taken with strategy k, and "" "" being the probability distribution function of strategy k. MY REMAINING QUESTION IS: What would be the probability density function ( ) for a sampling strategy which only samples a single value? It can't be infinite, otherwise path-tracing algorithms could not handle point, or purely directional light sources, as directly sampling them would contribute an infinitely small amount towards the final estimation of radiance.","['statistics', 'monte-carlo', 'computer-science']"
4218044,Does tilted coffee form an ellipse?,"Consider a cup of coffee in a circular mug. If this mug is resting flat on a surface, the surface of the coffee forms a circle. If you tilt the cup in any direction, however, does the surface formed resemble an ellipse? If so, how do you prove it? I drew a simple 2D representation of the problem, and although it looks as though the surface formed is in fact an ellipse, I have no simple way of proving this. What happens if the cup you have is not simply cylindrical, but is two parallel circles of differing radius, with their centres both going through the same line perpendicular to themselves? Surely this does not form an ellipse, but then, what?","['conic-sections', 'geometry']"
4218045,"How can I count the 1's digits up to 999,999 using combinatorics?","The question is If you were to write out 1,2,3,4....999,999. How many times would you
have to write the number 1? 1=1, 11=2, 1,111=4...and so on. I know there are other ways to solve it, but I want to solve it using combinatorics and counting the number of ways to fill {XXX,XXX},{XX,XXX},...,{XX},{X} up with the digit 1. This formula gives the correct answer but I'm not sure why.  It seems unlikely that it would be a coincidence. $\sum_{k=0}^6 \binom6k * k \space 9^{6-k}$ $ = 600,000$ wolfram alpha I think this might be the reason this formula works. Begin by choosing k locations in XXX,XXX That's $\binom6k$ such as k=2 XX{X,X}XX Then you multiply by the number of possibilities are each X in order, $9^4$ . So, why is there a k term? Maybe this is the number of locations in {X,X}.  Maybe the number of places to start filling with 1's. Some work that seemed to be going in the correct direction. The number isn't too far off. Treating it as a counting problem. Again, counting the number of ways to fill XXX,XXX and then XX,XXX and X,XXX and etc. Add $\sum \binom6k * 9^{(6-k)}$ where k=1,...,6 $\sum \binom5k * 9^{(5-k)}$ where k=1,...5 $\sum \binom4k * 9^{(4-k)}$ $\sum \binom3k * 9^{(3-k)}$ $\sum \binom2k * 9^{(2-k)}$ $\sum \binom1k * 9^{(1-k)}$ =513240 wolfram alpha wolfram alpha","['combinatorics', 'discrete-mathematics']"
4218048,Proof of cellular approximation theorem,"I'd like to prove the cellular approximation theorem based on the proof given on Tom Dieck (p. $211$ , Theorem $8.5.4$ ). The proof can be found here: https://www.maths.ed.ac.uk/~v1ranick/papers/diecktop.pdf (p. $211$ , Theorem $8.5.4$ ). As stated in the book, we want to construct homotopies as follows : There are some facts in the proof of the inductive step that I'm unable to solve. The proof of the inductive step proceeds as follows: Without loss of generality we can assume $f(X^i) \subset Y^i$ for $i<n$ . Let $\phi_\alpha: (E_{\alpha}^n,S_{\alpha}^{n-1})\longmapsto (X,X^{n-1})$ the attaching map of an $n-$ cell (so the image of $\phi_{\alpha}$ lies in $X^{n}$ ), and $f \circ \phi_{\alpha}:(E_{\alpha}^n,S_{\alpha}^{n-1}) \longmapsto (Y,Y^{n-1})\subset  (Y^{n},Y^{n-1})$ . Since $\pi_n(Y,Y^n) = 0$ we have that $f \circ \phi_{\alpha}$ is homotopic (with homotopy costant on $S_{\alpha}^{n-1}$ ) to $g_{\alpha}: (E_{\alpha}^n,S_{\alpha}^{n-1}) \longmapsto (Y^n,Y^{n-1})$ . We can repeat this process for every $n-$ cell and so we have an homotopy $H^n$ between $f_{|_{X^{n}}}$ and a map $X^n \longmapsto Y^n$ . Since $X^n \hookrightarrow X$ is a cofibration we can extend the homotopy $H^n$ between $f$ and $\tilde{f} : X \longmapsto Y$ such that $\tilde{f}_{|_{X^{n}}} \subset Y^n$ and $H^n$ costant on $X^{n-1}$ . $\rule{18cm}{1pt}$ Edit $1$ : The points I'd like to clear that I'm unable to solve are the following two : 1 . We have an homotopy $H^n$ between $f_{|_{X^{n}}}$ and a map $X^n \longmapsto Y^n$ . I don't know how to construct the ""global"" homotopy gluing all the ""local"" homotopies, which should raise taking all the $f \circ \phi_{\alpha}$ . 2 .The homotopy $H^n$ is costant on $X^{n-1}$ . My thoughts: In particular a specific homotopy $H_\alpha$ between $f \circ \phi_{\alpha}$ and $g_\alpha$ is a map from $E_\alpha^n \times I $ to $Y^n$ which is constant on $S_{\alpha}^{n-1}$ . The problem here is that I can't compose with $\phi_{\alpha}^{-1}$ ( $\phi_{\alpha}$ is an homeomorphism only of the interior part of $E_\alpha^n$ , so the border of the disk would be ""uncover"") in order to obtain a map from a map from $X^n \times I$ to $Y^n$ which is what I want, since our final goal is to obtain a map homotopic to $f$ restricted to the $n$ -skeleton. I thought that maybe a map like $\begin{cases} H_\alpha \circ (\phi_{\alpha}^{-1}\times id) & x \in \phi_{\alpha}(\text{int}(E_{\alpha}^{n}) \\ f(x)\end{cases}$ could do the job (if continuos), but unfortunately this doesn't seem costant on $X^{n-1}$ , required by $(3)$ . As far as concerns point $2$ I don't whether the property of being constant follows immediately from the costruction of $H^n$ in point $1$ , but I suspect so. I probably have to use the fact that the ""local"" homotopies are costant on $S_{\alpha}^{n-1}$ , so it requires $1.$ Notation edit: $f: X \longmapsto Y$ is a continuos map where $X,Y$ are $CW$ complex(with skeleton $X^n$ or $X_n$ depending on the book cited) with $(\phi_{\alpha})_{\alpha}$ (or $\varphi_i$ ) the characteristic maps of the $CW$ complex. I'm denoting $E_{\alpha}^n$ (or $\mathbb{D}^n$ ) and $S_{\alpha}^{n-1}$ as the $n-$ euclidean closed disk and its boundary respectively. $\rule{18cm}{1pt}$ Edit: I think I found a proof of the fact that I'm looking for but I need to adjust the details. I've improved the previous homotopy given in Edit $1$ which was wrong. The idea should follow Lemma 7.33 $p.187$ of Kirk and Davis lectures in algebraic topology, which can be found here . The proof is the following: I don't get how $h_n$ is well defined. Of course is not defined on all $X_n$ in that way but only on $\varphi(\mathbb{D}^n)$ the image of the closed disk. So I thought that defining $h_n$ to be $\begin{cases}h_{n,i}(e) & e \in \mathbb{D}^n \\ h_{n-1}(x)\end{cases}$ could do the job but I'm not sure if this map is well defined and the continuity of this map, since the attaching maps need not to be injective on the boundary of $\mathbb{D}^n$ . Same problem costructing $h_{{n-1}_{|_{X_n}}}\simeq h_n$ where I tried (called $G$ the homotopy between $h_{n-1} \circ \varphi_i$ and $h_{n,i}$ ) $\begin{cases}G(\varphi_{i}^{-1}(x),t) & (x,t) \in \varphi_i(\text{int}(\mathbb{D}^{n})) \times I \\ h_{n-1}(x)\end{cases}$ , which of course gives the desidered homotopy and is well defined (in the quotient space I don't think on the disjoint union) but I don't whether is continuos. Which facts should I use in order to prove the continuity of such maps? Any help,proof or tip would be appreciated, thanks in advance.","['cw-complexes', 'solution-verification', 'homotopy-theory', 'general-topology', 'algebraic-topology']"
4218072,Proving $\arcsin(\sin(x))=(-1)^{k}(x-k\pi)$,"I want to write $\arcsin(\sin(x)), x\in \mathbb{R},$ as a piecewise function described by a general formula: $f(x)=\arcsin(\sin(x))=
  \begin{cases}
                                   ... ,\text{ $x\in\left[\frac{2k-1}{2}\pi,\frac{2k+1}{2}\pi\right)$} \text{ ; $k\in\mathbb{Z}^{-}$} \\
                                   x ,\quad\quad\quad\quad\text{$x\in\left[-\frac{\pi}{2},\frac{\pi}{2}\right)$} \\
  -x+\pi,\quad\text{$x\in\left[\frac{\pi}{2},\frac{3\pi}{2}\right)$} \\
x-2\pi, \quad \text{$x\in\left[\frac{3\pi}{2},\frac{5\pi}{2}\right)$}\\
... ,\text{ $x\in\left[\frac{2k-1}{2}\pi,\frac{2k+1}{2}\pi\right)$} \text{ ; $k\in\mathbb{Z}^{+}$} \\
  \end{cases}$ $f(x)=\arcsin(\sin(x))=(-1)^{k}(x-k\pi), x\in\left[\frac{2k-1}{2}\pi,\frac{2k+1}{2}\pi\right)
$ The general case I came up with seems to hold, at least when analysing the graph of $f(x)$ , however I cannot find any way of rigorously proving its validity. Is there any method I can use to prove $\arcsin(\sin(x))=(-1)^{k}(x-k\pi), x\in\left[\frac{2k-1}{2}\pi,\frac{2k+1}{2}\pi\right)$ ?","['algebra-precalculus', 'trigonometry']"
4218096,"The name of the specific tensor product ""$\boxtimes$"" of vector bundles over a Cartesian product of manifolds","Let $M, N$ be smooth (or topological, or algebraic etc.) manifolds and let $E \to M$ and $F \to N$ be two smooth vector bundles. Let $p : M \times N \to M$ and $q : M \times N \to N$ be the natural projections. Does the vector bundle $E \boxtimes F \to M \times N$ defined as $p^* E \otimes q^* F$ have a name? Berline, Getzler and Vergne, for instance, introduce it at page 74 of ""Heat Kernels and Dirac Operators"", but use it across the whole book without giving it any name, so maybe it doesn't have one.","['fiber-bundles', 'smooth-manifolds', 'vector-bundles', 'tensor-products', 'differential-geometry']"
4218136,Finding $\lim \frac{(2n^{\frac 1n}-1)^n}{n^2}$.,"I want to find limit of $a_n= \frac{(2n^{\frac 1n}-1)^n}{n^2}$ as $n\to \infty$ . $\displaystyle a_{n} =\frac{\left( 2n^{\frac{1}{n}} -1\right)^{n}}{n^{2}} =\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}}\right)^{n}$ $\displaystyle  \begin{array}{{>{\displaystyle}l}}
\log a_{n} =n\log\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}}\right) =\frac{\log\left( 1+\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}} -1\right)\right)}{\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}} -1\right)} .\left(\frac{2n}{n^{\frac{1}{n}}} -\frac{n}{n^{\frac{2}{n}}} -n\right)\\
\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\frac{\log\left( 1+\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}} -1\right)\right)}{\left(\frac{2}{n^{\frac{1}{n}}} -\frac{1}{n^{\frac{2}{n}}} -1\right)} .\left(\frac{n^{\frac{1}{n}} -1}{n^{\frac{1}{n}}}\right)^{2} .( -n)
\end{array}$ The first term on RHS has limit equal to $\displaystyle 1\ $ but the second term is giving me problem. Please help. Thanks.","['sequences-and-series', 'real-analysis']"
4218167,Extrema points of a non-differentiable function.,"Say I have a function $f\left(x\right)=\left|\frac{x^{2}-2}{x^{2}-1}\right|$ . I have to find the local minima/maxima of the function. The point is that this function is non-differentiable at $x=\pm\sqrt{2},\pm1$ . When i try to break free of the modulus, I end up getting a piece-wise function. How do I find the extrema points in this case ( I do not want to go by graph method) ? Because, sure I can find the critical points, which in this case is $x=\pm\sqrt{2},\pm1,\ 0$ . But how do I use the first derivative test to check whether these points are actually extrema or not. So basically my question is how do we go about finding local minima/maxima without any graphical method? Any hints or approaches are welcome. I don't want answer of this particular question, I just want to know the concept/method.","['calculus', 'derivatives']"
4218206,Evaluate $\int_{-1}^1\sqrt{1-x^2}dx=\pi/2$ by using complex integration,Can I evaluate $\int_{-1}^1\sqrt{1-x^2}dx=\pi/2$ by using complex integration? Using the dog bone contour it seems perfectly possible. But the function has no residue... Isn't the integral around the closed contour equal to zero? (The dog bone contour is in red below),"['integration', 'residue-calculus']"
4218223,Circuits probability problem,"In the circuit shown, each switch is closed with probability $p$ , independently of all other switches. The task is to find the probability that a flow of current is possible between $A$ and $B$ . My approach was to use the inclusion-exclusion principle as follows. Label the circuit $ABCD$ : Then the four possible routes the current can take are $ACB$ , $ADB$ , $ACDB$ and $ADCB$ . Define events $R_1 = \{\text{current can flow along } ACB \}$ , $R_2 = \{\text{current can flow along } ADB \}$ , $R_3 = \{\text{current can flow along } ACDB \}$ , $R_4 = \{\text{current can flow along } ADCB \}$ . Then the required probability is \begin{align}
\mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4) = \mathbb{P}(R_1) &+ \mathbb{P}(R_2) + \mathbb{P}(R_3) + \mathbb{P}(R_4) \\
&- \mathbb{P}(R_1 \cap R_2) - \mathbb{P}(R_1 \cap R_3) - \mathbb{P}(R_1 \cap R_4) \\
&- \mathbb{P}(R_2 \cap R_3) - \mathbb{P}(R_2 \cap R_4) - \mathbb{P}(R_3 \cap R_4) \\
&+ \mathbb{P}(R_1 \cap R_2 \cap R_3) + \mathbb{P}(R_1 \cap R_2 \cap R_4) + \mathbb{P}(R_1 \cap R_3 \cap R_4) + \mathbb{P}(R_2 \cap R_3 \cap R_4) \\
&- \mathbb{P}(R_1 \cap R_2 \cap R_3 \cap R_4)
\end{align} This works out as $\mathbb{P}(R_1 \cup R_2 \cup R_3 \cup R_4)
= (2p^2 + 2p^3) - (5p^4 + p^5) + 4p^5 - p^5
= \boxed{2p^2 + 2p^3 - 5p^4 + 2p^5}$ . However, the answer given by the textbook is $\boxed{1 - (1-p)(1-p^2)^2 - p + p[1 - (1-p)^2]^2}$ , which is equivalent, but I can't figure out what the intended method was to get the solution in this form.","['conditional-probability', 'inclusion-exclusion', 'probability']"
4218226,Chain Rule for Conditional Probability?,"So while the most basic form of the product rule for probability is $P(A \cap B) = P(A) P(B|A)$ , I heard that for any events $A, B, C,$ the following also holds: $$P(A \cap B | C) = P(A|C) P(B| A \cap C). $$ I've been trying to derive this formula and/or find the general form of this for $n$ events, but so far haven't had any success. Could someone help me see why $$P(A \cap B | C) = P(A|C) P(B| A \cap C)$$ is true (how we get from $P(A \cap B) = P(A) P(B|A)$ to this) and if there's a more general formula for this?","['conditional-probability', 'probability-theory', 'probability']"
4218256,Questions on the trig Identity $\sqrt{\frac{1-\cos x}{1+\cos x}}=\csc x-\cot x$,"The problem is: Prove the identity $$ \sqrt{\frac{1-\cos(x)}{1+\cos(x)}}=\csc(x)-\cot(x)$$ The text's solution goes as follows: $$ \sqrt{\frac{1-\cos(x)}{1+\cos(x)}}= \sqrt{\frac{(1-\cos(x))^2}{1-\cos^2(x)}}= \frac{1-\cos(x)}{\sin(x)}= \csc(x)-\cot(x).
$$ My own solution is almost the same. However, in the third equality above, I include the other solution: $$
\frac{1-\cos(x)}{-\sin(x)} = \cot(x)-\csc(x).
$$ I made the graphs of both sides of the identity and found out that each of the two solutions on the R.H.S. actually only matches with the L.H.S. for half of the domain. Thus, I conclude that: The problem's statement is just totally wrong. To make the problem sensible, I should specify the domain on which the R.H.S is well-defined(i.e. excluding the points where $\sin(x)=0$ , and is positive. So, for the solution $$\csc(x)-\cot(x)\enspace ,$$ the domain should be $ \cup_{k \in \mathbb{Z}} ( \,2k\pi,\,(2k+1)\pi)$ . Since I have to explain the problem and the solution to my younger sister, I'd like to be sure my conclusions and understanding are right. Please give me some advice. Thanks.","['algebra-precalculus', 'trigonometry']"
4218284,Discrete mathematics - ternary strings.,"Let n be a natural number, n≥3. A ternary string is a sequence of n symbols that has some of the digits 0, 1, 2. In other words, a ternary string is a n-permutation with a repetion of the set {∞⋅0,∞⋅1,∞⋅2}. I wonder how to find the number of ternary strings that have at least one 0, one 1 and one 2. I know that there is a inclusion-exclusion principle, and for that we need to know at least the number of items in the sets, such as A, B and so on.. That being said simply adding the elements in A and B together will show us the elements in the intersection, however we will need to substract the intersection of A and B in order to obtain the number of elements in the union. My confusion is if there is something different by being a ternary string and also by the domain of the sets that has those ternary strings, the mathematical approach I'm thinking of is something like: A⋃B = A + B - A ⋂ B","['combinatorics-on-words', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics', 'stirling-numbers']"
4218287,"$x_1$ , $x_2$ are roots of $x=5-x^2$. Find the equation with roots $\frac1{(x_1+1)^3}$ and $\frac1{(x_2+1)^3}$.","Suppose $x_1$ , $x_2$ are the roots of the equation $x=5-x^2$ .Then $\dfrac1{(x_1+1)^3}$ and $\dfrac1{(x_2+1)^3}$ are roots of which
equation? $1)125x^2+16x=1$ $2)125x^2=16x+1$ $3)125x^2=12x+1$ $4)125x^2+12x=1$ I solved this problem with the following approache, I've denoted the roots of the original equation by $\alpha$ and $\beta$ rather than $x_1$ , $x_2$ , $S=\alpha+\beta=-1$ and $P=\alpha\beta=-5$ . We find $S'$ and $P'$ of the new equation, $$P'=\dfrac1{[(\alpha+1)(\beta+1)]^3}=\dfrac1{(S+P+1)^3}=-\frac1{125}$$ $$S'=\dfrac{(\beta+1)^3+(\alpha+1)^3}{[(\alpha+1)(\beta+1)]^3}=-\frac1{125}\times\left((\alpha^3+\beta^3)+3(\alpha^2+\beta^2)+3(\alpha+\beta)+2\right)$$ $$=-\dfrac{(S^3-3PS)+3(S^2-2P)+3S+2}{125}=-\dfrac{-16+33-3+2}{125}=-\frac{16}{125}$$ Hence the new equation is $125x^2+16x-1=0$ . And the answer is first choice. This was a problem from a timed exam. So can you solve it with other approaches (preferably quicker one) ?","['algebra-precalculus', 'quadratics']"
4218311,$\left\lfloor\frac{x+1}2\right\rfloor=\left\lfloor\frac{2x+1}3\right\rfloor$,"I want to find the solutions in $\mathbb R$ of $\left\lfloor\frac{x+1}2\right\rfloor=\left\lfloor\frac{2x+1}3\right\rfloor$ , where $\lfloor x\rfloor$ is the unique integer such that $\lfloor x\rfloor\leq x<\lfloor x\rfloor+1$ . I solved it as follows. Let $\left\lfloor\frac{x+1}2\right\rfloor=n$ . By the definition, $n\leq\frac{x+1}2<n+1$ . Then $2n-1\leq x<2n+1$ , $4n-2\leq2x<4n+2$ , $4n-1\leq 2x+1<4n+3$ , and finally, $\frac{4n}3-\frac13\leq \frac{2x+1}3<\frac{4n}3+1$ . $(1)$ . Now I analyzise $n$ modulo $3$ . If $n=3k$ , then $(1)$ becomes $4k-\frac13\leq\frac{2x+1}3<4k+1$ . Then either $\left\lfloor\frac{2x+1}3\right\rfloor=4k-1$ , or $\frac{2x+1}3=4k$ . From the equation itself, it follows that $3k=4k-1$ , or $3k=4k$ , so $k=0$ or $k=1$ . It's easy to turn this into 2 intervals for $x$ . Similarly, I can continue for $3k+1$ and $3k+2$ . This seems to lead to a solution, but is there perhaps a better way? Thank you for any ideas/verification of my solution.","['algebra-precalculus', 'discrete-mathematics']"
4218366,Do reflexive separable Banach spaces have Schauder bases?,I know there exists an example in the literature due to Per Enflo of a separable Banach space without a Schauder basis. I am wondering if there is a reflexive counterexample? If so I would greatly appreciate a reference.,"['banach-spaces', 'schauder-basis', 'functional-analysis']"
4218376,"Schauder basis in $L^2([0,1] \times [0,1], \mathbb{R}^2)$","Consider $L^2([0,1],\mathbb{R})$ . Then, $1, \sqrt{2} cos(2 \pi j x), \sqrt{2} sin(2 \pi j x ), \quad j =1,2,...$ is a Schauder basis on $L^2([0,1], \mathbb{R})$ . I am curious, how does this generalize in the case $L^2([0,1] \times [0,1], \mathbb{R}^2)$ ? Thanks","['hilbert-spaces', 'orthonormal', 'functional-analysis', 'schauder-basis']"
4218395,Solution to the $\displaystyle \phi'' + 4\phi' + 3\phi = x \cos {3x}. $,"I came across the following differential equation while practicing $ \displaystyle \phi'' + 4\phi' + 3\phi  = x \cos {3x} .$ Now, I used VARIATION OF PARAMETERS to solve it. I found general solution the equation $ \displaystyle \phi'' + 4\phi' + 3\phi  = 0 $ to be $\displaystyle C_1 e^{-3x} + C_2 e^{-x}.$ Now we need to find two functions $\lambda \ \text{and} \ \nu $ such that $\displaystyle \lambda'e^{-3x} + \nu'e^{-x} = 0 $ and $\displaystyle -3\lambda'e^{-3x} - \nu'e^{-x} = x \cos {3x} .$ Then the general solution would be : $\displaystyle \lambda e^{-3x} + \nu e^{-x} .$ On solving I got : $$\displaystyle \lambda = -\frac {1}{2} \int x \cdot \cos{3x} \cdot e^{3x} \mathrm {d}x \\  \nu = \frac {1}{2} \int x \cdot \cos{3x} \cdot e^{x} \mathrm {d}x  $$ These integrals took me a long time to solve. I used the traditional DI method to solve for it. Finally getting the answer as : $$ \displaystyle C_1 e^{-3x} + C_2 e^{-x} + \frac {x}{30} ( 2 \sin {3x} - \cos {3x}  ) + \frac {47}{1800}\cos {3x}  +\frac {19}{200}\sin {3x} .  $$ I want my answer to get verified and am open to some good way to the integral quickly. Thank you in advance.","['integration', 'calculus', 'ordinary-differential-equations']"
4218397,"Are all open balls in $\mathcal{C}([0,1])$ uncountable?","A while ago I got confused about the cardinality of open balls in metric spaces and erroneously thought all open balls were uncountable. It turns out this is an obvious mistake -  i.e. if $(X,d)$ is a metric space and $|X|\leq\aleph_{0}$ then clearly no subset of $X$ can have cardinality of the continuum. Because all open balls in $(\mathbb{R}^{k},d_{k})$ are uncountable I think I got lazy and just assumed this to be true for all spaces. Since all open sets are unions of open balls then all open sets in $(\mathbb{R}^{k},d_{k})$ are also uncountable. See the question here; Showing all open sets in $\mathbb{R}^{n}$ are uncountable - extend to general metric space? Since I was also learning about the space $(\mathcal{C},[0,1])$ the previous question got me wondering whether all open balls of $\mathcal{C}$ , and hence all open sets of $\mathcal{C}$ , are uncountable? I think they are, and so this is my first question . I adapted the proof that worked for $(\mathbb{R}^{k},d_{k})$ , but of course encountered some new issues and the following question Is the cardinality of $\mathcal{C}[0,1]$ the same as the cardinality of $\mathbb{R}$? helped me out, in particular the above question claimed all functions in $\mathcal{C}$ are determined by values in the set $\mathbb{Q}\cap[0,1]$ (think I am happy with this part of the proof), and hence $$|\mathcal{C}|\le|\Bbb R|^{\aleph_0}=(2^{\aleph_0})^{\aleph_0}
=2^{\aleph_0\times\aleph_0}=2^{\aleph_0}=|\Bbb R|=\aleph$$ I used this upper bound for $\mathcal{C}$ in my proof, however I used a different argument since I could not easily understand where $|\mathcal{C}|\le|\Bbb R|^{\aleph_0}$ comes from (the rest I understand). Given the above difficulties I tried the following: noting for any two sets $A$ and $B$ that $|A\cap B|\leq |A\times B|=|A|\times|B|$ then $|\mathbb{Q}\times[0,1]|\leq|\mathbb{Q}|\times|[0,1]|=\aleph_{0}\aleph$ must be an upper bound for the number of functions in $\mathcal{C}$ . Since $\aleph_{0}\aleph=\aleph$ then $\mathcal{C}\leq|\mathbb{Q}\times[0,1]|\leq\aleph$ must hold. So my second question is, how is it that $\mathcal{C}$ being determined by values in the set $\mathbb{Q}\cap[0,1]$ implies $|\mathcal{C}|\le|\Bbb R|^{\aleph_0}$ (and hence that $\mathcal{C}\leq\aleph$ ), and is the above alternate method OK as a proof of this? My third question is whether the following proof is correct that all open balls of $\mathcal{C}$ are uncountable? Proof ; Choose a $f\in\mathcal{C}$ and any $\epsilon>0$ and let $B_{d_{\infty},\epsilon}(f)$ and $B'_{d_{\infty},\epsilon}(f)$ be open $d_{\infty}$ -balls about $f$ of, respectively, $\mathcal{C}$ -functions and those $\mathcal{C}$ -functions that are constant functions. The following relationships hold; \begin{align*}
B'_{d_{\infty},\epsilon}(f)\subseteq B_{d_{\infty},\epsilon}(f)\subseteq\mathcal{U}(\mathcal{C})\subseteq\mathcal{C}
\end{align*} Let $B_{d,\epsilon}(x^{*})\subset\mathbb{R}$ be the open $d$ -ball about $x^{*}:=\text{sup}\{f(t):t\in[0,1]\}$ . Note that any $x\in B_{d,\epsilon}(x^{*})$ can be mapped to a constant function $g(t):\equiv x$ for all $t\in[0,1]$ and hence $|g(t)-x^{*}|=|x-x^{*}|=d(x,x^{*})<\epsilon$ for all $t\in[0,1]$ . Thus we have \begin{align*}
d_{\infty}(g,f)&=\text{sup}\{|g(t)-f(t)|:t\in[0,1]\}\\
&=\text{sup}\{|x-f(t)|:t\in[0,1]\}\\
&\leq|x-x^{*}|\\
&=d(x,x^{*})\\
&<\epsilon,
\end{align*} which implies $g\in B'_{d_{\infty},\epsilon}(f)$ since $g\in\mathcal{C}$ . Thus $B_{d,\epsilon}(x^{*})\subseteq B'_{d_{\infty},\epsilon}(f)$ . With the above in mind, for all $x\in B_{d,\epsilon}(x^{*})$ define the bijective mapping $\phi(x)\mapsto h_{x}(t)\equiv x$ for all $t\in[0,1]$ so that $\phi:B_{d,\epsilon}(x^{*})\longrightarrow B_{d,\epsilon}(x^{*})\subseteq B'_{d_{\infty},\epsilon}(f)$ holds. We then have $\aleph=|B_{d,\epsilon}(x^{*})|\leq |B'_{d_{\infty},\epsilon}(f)|$ . Furthermore given $\delta: B'_{d_{\infty},\epsilon}(f)\longrightarrow B'_{d_{\infty},\epsilon}(f)$ , $\delta(f)\mapsto f$ is a bijective mapping and $B'_{d_{\infty},\epsilon}(f)\subseteq \mathbb{R}$ where $\mathbb{R}=\phi(\mathbb{R})$ is viewed as the set of all constant functions, then $|B'_{d_{\infty},\epsilon}(f)|\leq|\mathbb{R}|=\aleph$ . Thus we have $\aleph\leq|B'_{d_{\infty},\epsilon}(f)|\leq\aleph$ which implies $|B'_{d_{\infty},\epsilon}(f)|=\aleph$ . Now using the fact that $B'_{d_{\infty},\epsilon}(f)\subseteq B_{d_{\infty},\epsilon}(f)$ and the identity mapping $\delta$ previously defined, we have $|B'_{d_{\infty},\epsilon}(f)|\leq |B_{d_{\infty},\epsilon}(f)|$ . From the previous result that $\aleph=|B'_{d_{\infty},\epsilon}(f)|$ this then implies $\aleph\leq  |B_{d_{\infty},\epsilon}(f)|$ . Due to the density of $\mathbb{Q}$ in $\mathbb{R}$ , for any $x\in\mathbb{R}\backslash\mathbb{Q}$ we can construct a sequence of rationals $\{q_{n}\}_{n}$ such that $q_{n}\longrightarrow x$ . By the continuity of any $f\in\mathcal{C}$ we then have $\text{lim}_{n}f(q_{n})=f(\text{lim}_{n}q_{n})=f(x)$ which shows each continuous function on $[0,1]$ is determined by its values on the countable
set $\Bbb Q\cap[0,1]$ . Noting for any two sets $A$ and $B$ that $|A\cap B|\leq |A\times B|=|A|\times|B|$ then $|\mathbb{Q}\times[0,1]|\leq|\mathbb{Q}|\times|[0,1]|=\aleph_{0}\aleph$ must be an upper bound for the number of functions in $\mathcal{C}$ . Since we have $\aleph_{0}\aleph=\aleph$ then $\mathcal{C}\leq|\mathbb{Q}\times[0,1]|\leq\aleph$ must hold. Since $\delta:B_{d_{\infty},\epsilon}(f)\longrightarrow B_{d_{\infty},\epsilon}(f)\subseteq\mathcal{C}$ is a bijection then $|B_{d_{\infty},\epsilon}(f)|\leq|\mathcal{C}|=\aleph$ must be true. Thus we have $\aleph\leq|B_{d_{\infty},\epsilon}(f)|\leq\aleph$ and so we conclude $|B_{d_{\infty},\epsilon}(f)|=\aleph$ EDIT: Alternate proof as suggested in comments Choose any $f\in\mathcal{C}$ and $\epsilon>0$ . Then $B_{\epsilon}(f)$ is an open ball in $\mathcal{C}$ . Given that open balls in $\mathcal{C}$ are convex, for any $t\in[0,1]$ and any $g,h\in\mathcal{C}$ we have $tg+(1-t)h:=m_{t}\in B_{\epsilon}(f)$ . Since there is one $m_{t}$ for every $t\in[0,1]$ then $\aleph=|[0,1]|\leq |B_{\epsilon}(f)|\leq|\mathcal{C}|$ . Since $|\mathcal{C}|=\aleph$ this means $B_{\epsilon}(f)=\aleph$ must hold.","['elementary-set-theory', 'solution-verification', 'metric-spaces']"
4218415,"Is it true that $\left\{\frac{m^2}{n!}:m,n\in\mathbb{N}\right\}=\mathbb{Q}^{+}$?","Is it true that $$\left\{\frac{m^2}{n!}:m,n\in\mathbb{N}\right\}=\mathbb{Q}^{+} \tag{1}$$ My intuition comes from the fact that $m$ and $n$ -increases $m^2=p_1^{2e_1} p_2^{2e_2}...p_s^{2e_s}$ where $p_1,...,p_s$ are the first $s$ primes and $n!=q_1^{f_1} q_2^{f_2}\cdot\cdot\cdot q_r^{f_r}$ where $q_1,...,q_r$ are the first $r$ primes The following answer already shows: $$\left\{\frac{m^a}{n^b}:m,n\in\mathbb{N}\right\}=\left\{\frac{c^{\gcd(a,b)}}{d^{\gcd(a,b)}}:c,d\in\mathbb{N}\right\}$$ Since $$\left\{\frac{m^2}{n!}:m,n\in\mathbb{N}\right\}=\left\{\frac{p_1^{2e_1} p_2^{2e_2}...p_s^{2e_s}}{q_1^{f_1} q_2^{f_2}\cdot\cdot\cdot q_r^{f_r}}:e_1,...,e_r,f_1,...,f_r\in\mathbb{N}\right\}$$ and each element in $2e_{1},...,2e_r$ is relatively prime to each element in $f_{1},...,f_{r}$ We can assume that $$\left\{\frac{m^2}{n!}:m,n\in\mathbb{N}\right\}=\mathbb{Q}^{+} \tag{1}$$ holds true. If this is correct can we generalize this to: $$\left\{\frac{m^{p}}{n!}:m,n\in\mathbb{N}\right\}=\mathbb{Q}^{+}$$ Where $p\in\mathbb{Z}$ Is this also true?","['elementary-set-theory', 'number-theory', 'elementary-number-theory']"
4218441,"Determinant of a Pascal Matrix, sort of","Let $A_{n}$ be the $(n+1) \times(n+1)$ matrix with coefficients $$
a_{i j}={i+j \choose i}
$$ (binomial coefficients), where the rows and columns are indexed by the numbers from
0 to $n$ are indexed. Now I want to determine the Determinant and with the first 5 matrices i found out that it is $n+1$ if i did not make a mistake.
The Matrix looks like this: $$
\left(\begin{matrix}
{1+1 \choose 1} & {1+2 \choose 1} & {1+3 \choose 1} & \dots & {1+n+1 \choose 1} \\
{2+1 \choose 2} & {2+2 \choose 2} & {2+3 \choose 2} & \dots & {2+n+1 \choose 2} \\
{3+1 \choose 3} &{3+2 \choose 3} & {3+3 \choose 3} & \dots & {3+n+1 \choose 3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
{n+1+1 \choose n+1} & {n+1+2 \choose n+1} & {n+1+3 \choose n+1} & \dots & {n+1+n+1 \choose n+1}
\end{matrix}\right)
$$ The Problem is to bring this Matrix into an upper or lower triangle matrix. If anyone has hints or ideas that can help, please help, thanks in advance. Maybe the approach is not even good. If I make progress at all i will update this question.","['determinant', 'binomial-coefficients', 'linear-algebra']"
4218452,What does $a $< $b $<c mean?,"1.What does $a $ < $b $ <c really mean? Somebody says that it means ""3 distinct numbers"". and for example they said 1 $\le$ a $\lt$ b $\lt$ c $\le$ N means 3 numbers between 1 and N. 2.We want to say ""3 distinct numbers""; Then Why don't we use 1 $\le$ a $\ne$ b $\ne$ c $\le$ N instead of 1 $\le$ a $\lt$ b $\lt$ c $\le$ N ? 3.If we want to show 3 numbers that are not necessarily distinct, It is not possible to do that except with $\le$ . am I right? Would you please explain about all of these 3 questions? Thank you.","['notation', 'algebra-precalculus', 'inequality']"
4218486,Understanding the morphism of schemes involving nilpotents induced by: $\mathbb C[x] \to \mathbb C[x]/(x^2)$ where $x \mapsto ax \pmod {x^2}$,"From Vakil's FOAG : The following imprecise exercise will give you some sense of how to visualize maps of schemes when nilpotents are involved. Suppose $a \in \mathbb C$ . Consider the map of rings $\mathbb C[x] \to \mathbb C[x]/(x^2)$ given by $x \mapsto ax$ . Recall that $\operatorname{Spec} \mathbb C[x]/(x^2)$ may be pictured as a point with a tangent vector (§4.2). How would you picture this map if $a \ne 0$ ? How does your picture change if $a=0$ ? When $a \ne 0$ , I believe the ring map $\mathbb C[x] \to \mathbb C[x]/(x^2)$ given by $x \to ax$ induces the morphism of schemes $\operatorname{Spec} \mathbb C[x]/(x^2) \to \operatorname{Spec} \mathbb C[x]$ given by $(x)/(x^2) \mapsto (x)$ . I think this is true because if $a \ne 0$ , then $(x)/(x^2)=(ax)/(x^2)$ and the preimage of $(ax)/(x^2)$ is $(x)$ . When $a=0$ , then we also have morphism of schemes $\operatorname{Spec} \mathbb C[x]/(x^2) \to \operatorname{Spec} \mathbb C[x]$ given by $(x)/(x^2) \mapsto (x)$ because the preimage of $(x)/(x^2)$ is the set of all polynomials with zero constant term, i.e., the prime ideal $(x)$ . Is the above correct? I am still not sure how to visualize this morphism. What is this exercise trying to get across?","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'solution-verification', 'schemes']"
4218491,Central product isomorphism: $Z_4 * Q_8 \cong Z_4 * D_8$,"This problem is problem 5.1.12.b of Dummit and Foote and Ive been tearing my hair out. I managed to do 5.1.12.a and 5.1.13 which are related and were pretty simple but I can't for the life of me figure this one out. I tried to look up a solution online in the end but the solution was wrong so that didn't help either. Here is the problem: Let $Z_4=\left<x\right>$ . Let $D_8 = \left<r,s\right>$ and $Q_8=\left<i,j\right>$ be given by their usual generators and relations. Let $Z_4 * D_8$ be the central product of $Z_4$ and $D_8$ which identifies $x^2$ and $r^2$ (ie $Z_1=\left<x^2\right>, Z_2=\left<r^2\right>$ and the isomorphism is $x^2 \mapsto r^2$ ) and let $Z_4 * Q_8$ be the central product of $Z_4$ and $Q_8$ which identifies $x^2$ and $-1$ . Prove that $Z_4 * D_8 \cong Z_4 * Q_8$ . For reference of what the problem means by $Z_1$ and $Z_2$ here is the problem's definition of a central product: Let $A$ and $B$ be groups. Assume $Z(A)$ contains a subgroup $Z_1$ and $Z(B)$ contains a subgroup $Z_2$ with $Z_1\cong Z_2$ . Let this isomorphism be given by the map $x_i\mapsto y_i$ for all $x_i\in Z_1$ . A central product of $A$ and $B$ is a quotient $$\begin{equation}
(A \times B)/Z \text{ where } Z=\{(x_i,y_i^{-1}) : x_i\in Z_1\} 
\end{equation}$$ and is denoted by $A * B$ . I realize I could technically list out the elements of each set since it's only a total of 32 elements and then construct the first isomorphism that jumps out to me but that would not be interesting and very unsatisfying. Would love to hear if anyone has an elegant way to solve this.","['direct-product', 'group-isomorphism', 'quotient-group', 'abstract-algebra', 'group-theory']"
4218504,Factorial-like interpolation problem: looking for a function that satisfies $f(x) - f(x - 1) = \frac{\sin (πx)}{πx}$,"I would like a function that should satisfiy the following equality for any real number $x$ : $$ f(x) - f(x-1) = \frac{\sin (πx)}{πx} $$ But the only initial condition I have for sure is $f(0) = 1$ . This immediately yields values for all integer numbers: $$
f(x) = \left\{\begin{aligned}
0, \ \ & x=\ ...,-3,-2,-1 \\
1, \ \ & x=0,1,2\ ... 
\end{aligned}\right.
$$ But leaves no clue for what the values might be at non-integer points. Of course there are infinitely many ways to smoothly interpolate $f(x)$ on the real numbers that will satisfy the equations above, but what I'm trying to find is the most «natural» of them, like the gamma function which interpolates factorials and satisfies $f(x) / f(x-1) = x-1$ . In fact, these two problems have a lot in common, so the solution to mine might be somewhat realted. So, does anyone know / can think of a function that satisfies the aforementioned equality? As I said, the answer might be related to factorials, gamma function, binomials, etc. One of the hints may be that the following identities hold for any $x$ : $$ \frac{\sin (πx)}{πx} = \binom{x}{0}·\binom{0}{x} = \frac{1}{Γ(1-x)·Γ(1+x)} $$ Maybe there is a way to derive the desirable function via some kind of technique similar to the one used when interpolating factorials? Any tips on that would also be of great help!","['functional-equations', 'interpolation', 'recurrence-relations', 'gamma-function', 'combinatorics']"
4218513,"$f(z)$ is holomorphic on $|z| < 1$, with $|f(z)| ≤ 3$ and $f(\frac{1}{2}) = 2$. Show that $f(z) \neq 0$ when $|z| < \frac{1}{8}.$","This is an old Schwarz lemma problem from the August 2020 UMD qualifying exam for analysis, which is posted here . The precise wording from the test is: Suppose $f(z)$ is a holomorphic function on the unit disk with $|f(z)| \le 3$ for all $|z| < 1$ , and $f(1/2) = 2$ . Show that $f(z)$ has no zeros in the disk $|z| < 1/8$ . (Hint: first show $f(0) \neq 0$ ). There is another kind of standard problem that assumes $|f(0)| \ge r$ and asks to prove $|f(z)| \ge \frac{r - |z|}{1 - r|z|}$ on the disk $|z| < r$ ; see problem 5 from chapter IX.1 of Gamelin. This problem instead gives you a value for $f(a)$ , $a \neq 0$ .",['complex-analysis']
4218525,Evaluating $\sum\limits_{x=2}^\infty \frac{1}{!x}$ in exact form.,"Introduction: We know that: $$\sum_{x=0}^\infty \frac{1}{x!}=e$$ But what if we replaced $x!$ with $!x$ also called the subfactorial function also called the $x$ th derangement number? This interestingly is just a multiple of $e$ and an Incomplete Gamma function based sum. This will get us a new number as the $n=0$ and 1 terms diverge as a result of the reciprocal. I also use the Generalized Exponential Integral function and the Round function. The OEIS entry for the constant is A281682 : \begin{align*}
S
&=\sum_{x=2}^\infty \frac{1}{!x}
=\sum_{n=2}^\infty \frac{1}{\operatorname{Round}\bigl(\frac{x!}{e}\bigr)} =e\sum_{x=2}^\infty\frac{1}{Γ(x+1,-1)}= \\
&= e\sum_{n=3}^\infty \frac 1{Γ(x,-1)}=\sum_{X=0}^\infty \sum_{x=2}^\infty\frac{1}{Γ(X+1)Γ(x+1,-1)}
=-e\sum_{x=2}^\infty\frac{(-1)^x}{E_{-x}(-1)}= -e\sum_{x=-\infty}^{-2}\frac{(-1)^x}{E_x(-1)} =1.63822707…
\end{align*} Possible Abel-Plana formula Application: We can also use the Abel-Plana formula, and the alternate series version, to find an integral representation of the sum. You can also use other representations of the summand, but this integral is probably hard to work with. Note the Abel-Plana formula may not work with the constant: \begin{align*}
S
&=\sum_{x=0}^\infty\frac{1}{!(x+2)}
=\frac{1}{2} + \int_0^\infty \frac{dx}{!(x+2)} + i\int_0^\infty\frac{\frac{1}{!(2+ix)}-\frac{1}{!(2-ix)}}{e^{2\pi x}-1} \, dx \\
\implies 
-\frac{S}{e}
&=\sum_{x=0}^\infty\frac{(-1)^x}{E_{-x-2}(-1)}=-\frac{1}{2e} + \frac i2\int_0^\infty \left[\frac{1}{E_{-ix-2}(-1)}-\frac{1}{E_{ix-2}(-1)}\right] \operatorname{csch}(\pi x) \, dx
\end{align*} See this nice closed form result of $$\sum_{x=-\infty}^0 \text {Im}(!x)=-\frac{\pi}{e^2}$$ I do not think this simple looking problem has been posted so far. The sum does not need to be in closed form. You also can rewrite it in terms of a better sum. I am more looking for an evaluation or manipulation of the sum. Please correct any mistakes and give me feedback! A Mittag-Leffler Insight: Because $$\sum_{x=2}^\infty \frac{1}{!x}
=e\sum_{x= 2}^\infty\frac{1}{Γ(x+1,-1)}= e\sum_{n=3}^\infty\frac 1{Γ(x,-1)} $$ one may notice the relation to the Mittag-Leffler function : $$\text E_{a,b}(x)=\sum_{n=0}^\infty \frac{x^n}{Γ(ax+b)}$$ The only problem is if there existed a function for the incomplete gamma function analogue of the Mittag-Leffler function. Maybe one can find this function or use the already known one? Another Integral Representation: It can be shown that the following is true using @Jack Barber’s method in $$\sum_\Bbb N \text{erfc}(x)$$ Here is an integral representation using the linearity of the Floor function and the Meijer G function : $$\sum_2^\infty \frac{1}{!x}=-\int_2^\infty \lfloor x-1\rfloor \frac{d}{dx} \frac{1}{!x}dx=\int_2^\infty \frac{d}{dx} \frac{1}{!x} dx-\int_2^\infty\lfloor x\rfloor  \frac{d}{dx} \frac{1}{!x} dx=\frac1{!\infty}-\frac1{!2}-\int_2^\infty \lfloor x \rfloor \frac{d}{dx} \frac{1}{!x} =-1-\int_2^\infty \lfloor x \rfloor \frac{d}{dx} \frac{1}{!x}dx= -1-\int_2^\infty \lfloor x \rfloor\left(-\frac{\text G_{2,3}^{3,0}\big(-1\left|_{0,0,x+1}^{\ \ \ \ 1,1}\right)}{e(!x)^2}-\frac{i\pi}{!x}\right)dx =\frac1e\int_2^\infty \frac{\lfloor x\rfloor\text G_{2,3}^{3,0}\left(-1\big|_{0,0,x+1}^{\ \ \ \ 1,1}\right)}{(!x)^2} dx+i\pi\int_2^\infty\frac{\lfloor x\rfloor}{!x}dx-1$$ The Meijer G function is hard to use, but you can come up with many more integral representations using alternate forms of the floor function; there is even one in terms of elementary functions. A Manageable Series Expansion: Converting the Round function  to elementary functions , we have this form : $$e\sum_{m=0}^\infty \sum_{n=2}^\infty \frac{\left(\frac e\pi\frac{\tan^{-1}\left(\tan\left(\frac\pi en!\right) \right)}{n!}\right)^m}{n!}$$ After a bit more work, we remove $!n$ from the denominator replacing it with gamma regularized $Q(a,z)$ : $$S=\sum_{m=0}^\infty\sum_{k=0}^n(-1)^k e^{k+1}\binom mk\sum_{n=2}^\infty\frac{(!n)^k}{n!^{k+1}}=\boxed{e\sum_{m,k=0}^\infty\binom mk(-1)^k\sum_{n=2}^\infty\frac{Q^k(n+1,-1)}{n!}}$$ Shown here . Conclusion: A closed form is optional , but good alternative representations for $S$ would also work. Please do not make up any new function. Alternatively, what is $\sum\limits_{n=2}^\infty\frac{Q^k(n+1,-1)}{n!}$ ?","['factorial', 'special-functions', 'gamma-function', 'calculus', 'sequences-and-series']"
4218558,"Prove that for all $p\in M$ there's an $M$-open set $p\in U\subset M$ and $\varepsilon >0$ such that $(-\varepsilon,\varepsilon)\times U\subset D$","I'm reading the book ""Introduction to Differential Geometry"" written by J.W. Robbin and D.A. Salamon. In the page 41 (Lemma 2.4.10) there's the following assertion: Suppose that $M\subset\mathbb{R}^n$ is a smooth submanifold and $X:M\to \mathbb{R}^n$ is a smooth vector field. Define for all $p\in M$ : $$\color{red}{I_X(p)}:=\bigcup \big\{I\subset \mathbb{R} \,
 (\text{open interval}):\text{there's a integral curve }\gamma \in
 M^I\text{ of }X\text{ such that }\gamma (0)=p\big\}.$$ Then for all $p\in M$ there exists an $M$ -open neighborhood $U_p\subseteq M$ of $p$ and a constant $\varepsilon _p>0$ such that $(-\varepsilon _p,\varepsilon _p)\times U_p\subseteq D:=\bigcup _{p\in
 M}I_X(p)\times \{p\}$ . I tried to show that $U_p:=\big\{q\in M:I_X(q)=I_X(p)\big\}$ is open in $M$ because if this is true then that assertion is also true. However I failed  and I don't know how to prove that assertion. Probably this is a consequence of a known theorem about ordinary differential equations, however I didn't study about ODE yet. I say this because, given a parametrization $\psi :V\to \psi [V]\subseteq M$ of $M$ , we can define $f:V\to \mathbb{R}^n$ by $f(x):=(d\psi )_x^{-1}\circ X\circ \psi (x)$ (which is smooth) and guarantee the existence of integral curves of $X$ with the system $\begin{cases}\dot{x}=f(x)\\x(0)=p\in \psi [V]\end{cases}$ and using the Picard-Lindelöf Theorem. Thank you for your attention!","['submanifold', 'ordinary-differential-equations', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
4218564,The Diophantine equation $x^5-2y^2=1$,"I'm trying to solve the Diophantine equation $x^5-2y^2=1$ . Here's my progress so far. We can write the Diophantine equation as $$\frac{x-1}{2}\cdot(x^4+x^3+x^2+x+1)=y^2.$$ If $x\not\equiv1\pmod{5}$ , then $\gcd(\frac{x-1}{2},x^4+x^3+x^2+x+1)=1$ , so both $\frac{x-1}{2}$ and $x^4+x^3+x^2+x+1$ must be perfect squares (note: $x^4+x^3+x^2+x+1>0$ ).
In particular, $4(x^4+x^3+x^2+x+1)$ is a perfect square.
Comparison with $(2x^2+x)^2$ and $(2x^2+x+1)^2$ forces $-1\leq x\leq3$ .
This results in the solutions $(3,\pm11)$ . If $x\equiv1\pmod{5}$ , then we can write the Diophantine equation as $$\frac{x-1}{10}\cdot\frac{x^4+x^3+x^2+x+1}{5}=\left(\frac{y}{5}\right)^2,$$ where $\gcd(\frac{x-1}{10},\frac{x^4+x^3+x^2+x+1}{5})=1$ , so both $\frac{x-1}{10}$ and $\frac{x^4+x^3+x^2+x+1}{5}$ must be perfect squares.
Thus, $$x=10a^2+1,$$ $$x^4+x^3+x^2+x+1=5b^2.$$ Unfortunately, this is where I get stuck.
I can substitute the first equation into the second, giving $$10000a^8+5000a^6+1000a^4+100a^2+5=5b^2,$$ $$2000a^8+1000a^6+200a^4+20a^2+1=b^2,$$ $$2000a^8+1000a^6+200a^4+20a^2=(b-1)(b+1),$$ $$5a^2(100a^6+50a^4+10a^2+1)=\frac{b-1}{2}\cdot\frac{b+1}{2},$$ but this doesn't seem to be making progress, even with modular arithmetic considerations.","['number-theory', 'modular-arithmetic', 'diophantine-equations']"
4218637,Can you distribute the balls equally into boxes?,"You have $n$ boxes and $mn$ balls in the first box. Your goal is to distribute the balls equally into boxes so that each box contains $m$ balls. You must obey the following protocol: Step 1, move exactly $1$ ball from one box to another box. Step 2, move exactly $2$ balls from one box to another box. Step 3, move exactly $3$ balls from one box to another box. $\vdots$ Continue until you come to step $k$ , when you first find that every box contains less than $k$ balls. If this happens, you go back to step 1 and start from there again. So on and so forth. Example: If $n=3,m=2$ , a possible plan is 600-510-330-303-204-222. Question: Can you always succeed for sufficiently large $n$ ? (i.e. does there exist $N$ such that $\forall n\gt N,\forall m$ , there's a plan to equally distribute the balls?) Update: Since I strongly suspect the answer to my question is yes, let me frame the problem in a more general (and hopefully more interesting) form: The boxes are now labeled. Your goal is to transform any $m$ -ball-in- $n$ -boxes distribution into any other such distributions, for all $m,n\gt 0$ . Question: is it true that you can succeed for all but finitely many such transformations by following the aforementioned protocol?","['elementary-number-theory', 'combinatorics']"
4218662,Expressing $f(x) = \frac{x-1}{x+1}$ as a sum of an even and odd function,"I am trying to write $$f(x) = \frac{1-x}{x+1}$$ as a sum of an even and odd function. One solution, though messy, is to use the following derivation: We work backwards. Let $f = f_e + f_0$ , where $f_e$ is even and $f_0$ odd. Then $f(x) = f_e (x) + f_0 (x)$ for all $x$ in the domain of $f$ . But then $$
f(-x) = f_e (-x) + f_0 (-x) = f_e (x) - f_0 (x).
$$ Adding, we eliminate $f_0 (x)$ : $$ 
f(x) + f(-x) = 2f_e (x),
$$ so $$
f_e (x) = \frac{f(x) + f(-x)}{2}.
$$ Now, $f_0 = f(x) - f_e$ . Then: $$ 
f_0 (x) = f(x) - \frac{f(x) + f(-x)}{2} = \frac{2f(x) - f(x) - f(-x)}{2} = \frac{f(x) -f(-x)}{2}.
$$ One easily checks that $f_e$ is even and $f_0$ odd. Using this derivation, I get: \begin{align*}
\frac{1-x}{1+x} = \frac{1}{2}\left(\frac{1-x}{1+x} + \frac{1 + x}{1 - x} \right) + \frac{1}{2} \left(\frac{1-x}{1+x} - \frac{1+x}{1-x}  \right).
\end{align*} Assuming I haven't made an algebra mistake, this seems to work, but it's not elegant. I did some research into this and found another proposed solution, but I believe it to be faulty. First, I'll just present it. \begin{align*}
\frac{1-x}{1+x} & = \frac{1-x}{1+x} \cdot \frac{1-x}{1-x} \\
& = \frac{1 - 2x + x^2}{1 - x^2} \\
& = \frac{x^2 + 1}{1 - x^2} + \frac{2x}{1 - x^2}.
\end{align*} One then checks that the first function is even and the second is odd. My problem with this solution is that it is not valid for every $x$ in the domain of $f$ . By definition, the domain of $f$ is $\mathbb{R} \setminus \{-1\}$ . But this solution, in multiplying by $\frac{1-x}{1-x}$ , presupposes that $x \neq 1$ , which does not seem to me to be allowed. I'm back at square one, then, because the solution I found does not seem in any way elegant or natural, and I'm assuming there is some kind of a trick that I am missing. I tried polynomial long division, partial fractions, and so forth, and nothing, other than the above derivation, brought me any progress.","['even-and-odd-functions', 'functions', 'solution-verification']"
4218697,Every $\ell^p$ space with $p\ne 2$ has a subspace without a Schauder basis,"Rajendra Bhatia's Notes on Functional Analysis (Texts and Readings in Mathematics), Pg. $14$ states (without proof) that: Every $\ell^p$ space ( $1\le p \le \infty)$ with $p\ne 2$ has a subspace without a Schauder basis. where $\ell^p$ denotes the sequence space with the $p$ -norm. What is the proof of this fact? I have been trying for a while now, and it seems more difficult than I had imagined. Also, I would be interested to see why every subspace of $\ell^2$ has a Schauder basis. P.S. If the proof is doable with some hints, then just hints would be great too! Update: The proof of the main assertion is found in Lindenstrauss and Tzafriri's Classical Banach Spaces I and II , as mentioned by David Mitra in the comments.","['lp-spaces', 'schauder-basis', 'functional-analysis', 'real-analysis']"
4218707,What is the value of $\textstyle{{\sum\limits_{n=1}^{\infty}{\lim\limits_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$?,"First, I had put $\textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$ into wolfram alpha and got nothing. Then, I thought about inter-changing the limit and the summation which will give us $\textstyle{\displaystyle{\lim_{u\rightarrow\infty}{\sum_{n=1}^{\infty}\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}$ which did gave me a result again from wolfram alpha. Putting the sum in wolfram alpha gave me $\begin{align}&-\Gamma(s+1,0,u)\\
&=-\Gamma(s+1,0)+\Gamma(s+1,u)\\
&=\Gamma(s+1,u)-\Gamma(s+1)\end{align}$ Then taking the limit gives us $\textstyle{\displaystyle{\lim_{u\rightarrow\infty}(\Gamma(s+1,u)-\Gamma(s+1))}}$ $=-\Gamma(s+1)$ However, I am not sure that I can really just interchange the limit and summation. While searching about this I got this post on this site, which says $\begin{align}\textstyle\displaystyle{\lim_{n\rightarrow\infty}\sum_{m=1}^{\infty}f(m,n)\geq\sum_{m=1}^{\infty}\lim_{n\rightarrow\infty}f(m,n)}\end{align}$ Applying this inequality on my sum gives us $\textstyle\displaystyle{\sum_{n=1}^{\infty}\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}\leq -\Gamma(s+1)}$ However, this doesn't really give the value of the sum. So, my question is What is the value of $\textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$ ?","['limits', 'summation', 'wolfram-alpha']"
4218718,Indexing a set of objects with a symmetry or antisymmetry property,"Suppose I have a collection of objects $\{X(i, j)\}_{i,j=1}^{n}$ (which could be anything - numbers, functions, matrices, $\ldots$ ) indexed by $1 \leq i,j \leq n$ , such that the following holds: $$ X(i,j) = X(j,i).$$ Up to the above constraint, all of the objects are different. If I want to index every object, such that no equal object is indexed more than once, I can impose the condition that $i \leq j$ in my indexing. My question is, how do I generalize this to sets of objects with more than two indices. For example, if I have $\{X(i,j,k)\}$ such that $$ X(i,j,k) = X(k,j,i),$$ or $\{X(i,j,k,l)\}$ such that $$X(i,j,k,l) = X(l,k,j,i),$$ what condition on the indices can I impose so that I index each non-equal object exactly once?","['symmetry', 'relations', 'discrete-mathematics', 'index-notation']"
4218737,"proved that$f (x)$ is uniformly continuous on $[0,+\infty)$ if and only if $\lim\limits_{x\to+\infty}\frac{f(x)}{x}$ exists.","Let $f(x)$ be second-order differentiable on $[0,+\infty)$ , $f''(x)\geqslant\sin x(\forall x\in[0,+\infty))$ .Prove that $f (x)$ is uniformly continuous on $[0,+\infty)$ if and only if $\lim\limits_{x\to+\infty}\frac{f(x)}{x}$ exists.This is our monthly exam question,I try to expand it with Taylor's theorem: $$f(x+h)=f(x)+f'(x)h+\frac{f''(\xi)}{2}h^2\geqslant f(x)+f'(x)h +\frac{\sin x}{2}h^2.$$ But then I don't know how to deal with it, I also know that if $f (+\infty)$ exists, then $f (x)$ is uniformly continuous, but it doesn't seem to help the problem.","['uniform-continuity', 'real-analysis']"
4218745,What is the range of $y=\sqrt{1}x+\sqrt{2}x^2+\sqrt{3}x^3+...$?,"This is an infinite series, defined for $-1<x<1$ . On desmos I entered the first 1000 terms and the range is from $-0.3786$$\cdots$ to infinity. I am trying to find the range (specifically the lower bound) of the infinite series. I tried considering $y-xy$ , but that did not help, because I cannot express $y-xy$ in closed form.",['sequences-and-series']
4218760,Complicated Integration involving Bessel function and exponential function,"Sir,
while studying the quantum scattering for various cases, I have got the following integrals: $$\int_{t_0=-\infty}^{t-R/c}\Big[\frac{\exp{(-i\omega t_0)}}{(t-t_0)}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0$$ and similarly, $$\int_{t_0=-\infty}^{t-R/c}\Big[-\frac{\exp{(-i\omega t_0)}}{(t-t_0)^2}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0$$ where, $t$ is observation time, $t_0$ is the retarded time, $J_0$ is the Bessel function of first kind and $R$ is the distance between the observation point and the point on scatterer such that $R=\sqrt{(x-x_0)^2+(y-y_0)^2+(z-z_0)^2}$ , where, $(x,y,z)$ is the point of observation. $c$ is the speed of light and taken as constant. Further, the causality condition is followed such that $t-t_0\geq R/c$ or $t_0\leq t- R/c$ . According to the physical resctriction imposed on the problem, it is also true that in the limit $t_0\rightarrow -\infty$ , the integrand should be zero. I am trying to find out the closed form answer of the integrals (if one can be solved, the other can be solved by the same technique).I searched through  the ""Table of integrals"" by Gradshteyn but somehow, I could not find out the exact match. Would you kindly suggest me any method or any relevant texts from where I can get some help.","['integration', 'definite-integrals', 'special-functions', 'complex-analysis', 'bessel-functions']"
4218764,What kind of calculations cause the sum of survey percentages to be greater than 100%?,"I was looking at this Facts and Figures page for Mount San Antonio College and saw this note for the Age breakdown of the school: * note: Numbers do not add up to 100 because some students selected ""unknown"" or chose not to respond. This is the data: Age 24.55% 19 or less 34.77% 20 to 24 13.88% 25 to 29 6.04% 30 to 34 3.65% 35 to 39 5.78% 40 to 49 12.30% 50+ The sum of these percentages is 100.97%. I tried to scale them back down by dividing through by 1.0097 but I don't think this ""undoes"" the voodoo the original researches did. My question is, what were they doing to the data that would cause these percentages to be inflated? I could guess but I can't rationalize it and I definitely can't test it because I don't have the data. It wouldn't be hard to experiment with a fake survey (i.e. the same question, but fewer respondents with 1 non-response), but what makes an adjustment the ""correct"" adjustment? How would we know? EDIT: Ok, what it looks like this is a situation where the survey results were ""weighted"" to account for non-responses. I found a really nasty PDF writeup called Designing Surveys to Account for Endogenous Non-Response . I can't understand (can't apply) this at all. I dropped out of grad school and this writeup seems to assume I know a lot about Stats or complicated use-cases that I don't. Can somebody explain in this context? Is a model this complex even relevant? Are survey weights non-reversible without the original data (different, less-important question)? EDIT2: HOLY-POOP, THEY'RE TALKING ABOUT THIS ON NPR RIGHT NOW! (not the nitty-gritty calculations though, just the social cost)",['statistics']
4218787,Evaluating $\sum_{r=1}^{89} \frac{1}{1+\tan^3 r}$,$$\sum_{r=1}^{89} \frac{1}{1+\tan^3 r }$$ where $r$ is in degrees I tried this a lot using the $a^3+b^3$ identity but I don't seem to be getting anything fruitful :( Can someone please give me a hint? I don't think the $V_n$ method is working here. ( $T_r+T_{r-1}$ ) Is it related to the $\tan(A+B)$ identity? However I'm not getting anything through that too. I would be grateful if someone helped. Thanks!,"['algebra-precalculus', 'trigonometry', 'summation']"
4218841,show this indentity $\sum_{k=1}^{2n-1}\frac{\sin{\frac{k^2\pi}{2n}}}{\sin{\frac{k\pi}{2n}}}=n$,"let $n$ be postive integers.show that $$\sum_{k=1}^{2n-1}\dfrac{\sin{\frac{k^2\pi}{2n}}}{\sin{\frac{k\pi}{2n}}}=n$$ Try:I can show $n$ is smaller number. let $LHS=f(n)$ .when $n=1$ it is clear $$f(1)=\dfrac{\sin{\pi/2}}{\sin{\pi/2}}=1$$ when $n=2$ then $$f(2)=\dfrac{\sin{\frac{\pi}{4}}}{\sin{\frac{\pi}{4}}}+\dfrac{\sin{\frac{4\pi}{4}}}{\sin{\frac{2\pi}{4}}}+\dfrac{\sin{\frac{9\pi}{4}}}{\sin{\frac{3\pi}{4}}}=1+0+1=2$$ when $n=3$ ,then $$f(3)=\dfrac{\sin{\frac{\pi}{6}}}{\sin{\frac{\pi}{6}}}+\dfrac{\sin{\frac{4\pi}{6}}}{\sin{\frac{2\pi}{6}}}+\dfrac{\sin{\frac{9\pi}{6}}}{\sin{\frac{3\pi}{6}}}+\dfrac{\sin{\frac{16\pi}{6}}}{\sin{\frac{4\pi}{6}}}+\dfrac{\sin{\frac{25\pi}{6}}}{\sin{\frac{5\pi}{6}}}=1+1-1+1+1=3$$ $$\begin{align}f(4)&=\dfrac{\sin{\frac{\pi}{8}}}{\sin{\frac{\pi}{8}}}+\dfrac{\sin{\frac{4\pi}{8}}}{\sin{\frac{2\pi}{8}}}+\dfrac{\sin{\frac{9\pi}{8}}}{\sin{\frac{3\pi}{8}}}+\dfrac{\sin{\frac{16\pi}{8}}}{\sin{\frac{4\pi}{8}}}+\dfrac{\sin{\frac{25\pi}{8}}}{\sin{\frac{5\pi}{8}}}+\dfrac{\sin{\frac{36\pi}{8}}}{\sin{\frac{6\pi}{8}}}+\dfrac{\sin{\frac{49\pi}{8}}}{\sin{\frac{7\pi}{8}}}\\
&=1+\sqrt{2}-\tan{\frac{\pi}{8}}+0-\tan{\frac{\pi}{8}}+\sqrt{2}+1\\
&=4\end{align}$$ because use this well known $\tan{\frac{\pi}{8}}=\sqrt{2}-1$",['trigonometry']
4218862,What is the fundamental period of $\cos(\sin x) + \cos(\cos x)$?,"If we let $f(x) = \cos(\sin x) + \cos(\cos x)$ , then it is easy to show that $f(x+ \pi/2)=f(x)$ , this shows that $\pi/2$ is a period of $f$ , but the problem is that how we can show that $\pi/2$ is the fundamental period (means for no real number $T$ less than $\pi/2$ we have $f(x+T)=f(x)$ for all $x\in\mathbb{R}$ holds). There are many authors who claim that $\pi/2$ is the fundamental period of $f(x)$ , I tried to prove it but couldn't do it. Please help. Edit: Non calculus solution is preferable because this problem is usually tackled in Algebra/Trigonometry which is usually taught before Calculus. I am not sure whether it could be solved without Calculus at last.","['periodic-functions', 'trigonometry', 'functions']"
4218921,Is there a $T_{3\frac{1}{2}}$ space such that all continuous real-valued functions are bounded?,"Is there a non-compact $T_{3\frac12}$ space $X$ (with at least two points) such that all continuous functions $X\to \mathbf R$ are bounded? This is true for $T_3$ spaces as described in this answer : in this case, it is possible that all functions are simply constant. This is obviously not the case for $T_{3\frac{1}{2}}$ spaces (with at least two points). Note that this property is equivalent to the property that the natural restriction embedding $C(\beta X)\to C(X)$ is an isomorphism. As a side question, does this property have a name? If the answer is yes, then there is a further question: is it true if we replace $\mathbf R$ with an arbitrary (but fixed) $T_1$ space $Y$ (where by bounded we mean, as usual, that the range is contained in a compact subset of $Y$ ). The question is motivated by this other question about whether one can characterise compactness of a space $X$ by the first order theory of the ring $C(X)$ . A positive answer would imply that even the isomorphism type of $C(X)$ does not characterise compact spaces among $T_{3\frac12}$ spaces.","['general-topology', 'compactification']"
4218957,Probability all angles of triangle formed within semircircle less than $120^\circ$,"$3$ points $A$ , $B$ , $C$ are randomly chosen on the circumference of a circle. If $A$ , $B$ , $C$ all lie on a semicircle, then what is the probability that all of the angles of triangle $ABC$ are less than $120^\circ$ ? Okay, let's fix a semicircle and make an interval $[0, 1]$ along it. It's clear the condition that the condition that all angles are less than $120^\circ$ means that the two points of the triangle furthest apart on the semicircle are further than $2/3$ apart. So we want to calculate the percentage of the volume of the unit cube already satisfying the following: $$0 < x < y < z < 1$$ subject to the additional inequality $$z - x > {2\over3}$$ However, I'm not sure what to do from here. Any help would be well-appreciated. UPDATE: With the helpful hint of Daniel Mathias in the comments, I was able to get the answer of ${5\over9}$ . But I am wondering if there is a way to solve it in the more complicated way I proposed, where there's $3$ variables and maybe we can do some multivariable integration.","['integration', 'geometric-probability', 'multivariable-calculus', 'calculus', 'probability']"
4218965,Extreme points of the unit ball of a Banach space,"Let $X$ be a Banach space and let $B_X$ denote the closed unit ball of $X$ . Let $\mathrm{Ext}(B_X)$ denote the set of all extreme points of the unit ball $B_X$ . Whenever $X$ is finite-dimensional, it is known that $\mathrm{Ext}(B_X)\neq \emptyset.$ Are there instances, where $\mathrm{Ext}(B_X)=\emptyset$ ? Does completeness plays any role here? Is it true that we will always have $\mathrm{Ext}(B_X)\neq \emptyset$ for $X$ is Banach and $\mathrm{Ext}(B_X)$ may be empty in case of $X$ is not complete?","['banach-spaces', 'normed-spaces', 'real-analysis', 'functional-analysis', 'convex-analysis']"
4218978,"Determine the domain of the function $g(x, y, z)=\ln(16-4x^2-4y^2-z^2)$.","By the condition of the logarithm, we have: \begin{align*}
     16-4x^2-4y^2-z^2                           &> 0 \\
     4x^2+4y^2+z^2                              &<16 \\
     \frac{x^2}{4}+\frac{y^2}{4}+\frac{z^2}{16} &< 1
\end{align*} Therefore the domain of $ g $ is the interior of an origin-centered ellipsoid. I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.",['functions']
4219021,Can the formula for the cubes $a^3 + b^3 + c^3 - 3abc$ be generalized for powers other than 3?,"I recently learnt out about this formula: $$a^3 + b^3 + c^3 - 3abc = (a + b + c) (a^2 + b^2 + c^2 - ab - ac - bc)$$ Is there a way of generalizing for powers other than $3$ , i.e. $$a^n + b^n + c^n + \mathop{???} = \mathop{???} $$ where the RHS is in terms of $(a^{n-1} + b^{n-1} + c^{n-1})$ , $(a^{n-2} + b^{n-2} + c^{n-2})$ , ... , $(a^{1} + b^{1} + c^{1})$ , just like the formula for $n=3$ .",['algebra-precalculus']
4219054,$CW$ topology on $X \times Y$ coincides with the product topology,"I think I heard somewhere that for $X,Y$ (both $CW$ complex) endowing the $CW$ complex $X \times Y$ with its weak topology is equivalent to endowing $X \times Y$ with the usual product topology . Is this result true? It's certainly true when $Y=[0,1]$ , often use in proofs involving continuity of homotopies from $CW$ complex. I'd be interested in references or proofs, any help would be appreciated, thanks in andvance.","['general-topology', 'cw-complexes', 'algebraic-topology', 'reference-request']"
4219056,Does the probability of team A winning a series of seven games depend on whether team A plays until they win the four games needed or all seven?,"In the World Series of baseball, two teams (call them A and B) play a sequence of games against each other, and the first team to win four games wins the series. Let p be the probability that A wins an individual game, and assume that the games are independent. (a) What is the probability that team A wins the series? (b) Give a clear intuitive explanation of whether the answer to (a) depends on whether the teams always play 7 games (and whoever wins the majority wins the series), or the teams stop playing more games as soon as one team has won 4 games (as is actually the case in practice: once the match is decided, the two teams do not keep playing more games). I fully understand part (a); below is my solution. (a) $$P(\text{A wins}) = P(\text{A winning in 4 games}) + P(\text{A winning in 5 games}) +P(\text{A wins in 6 games}) $$ $$ + P(\text{A winning in 7 games}) = p^4 + \binom{4}{3}p^3qp + \binom{5}{3}p^3q^2p  + \binom{6}{3}p^3q^3p$$ Another solution: Let $X \sim Bin(7,p)$ and $q=1-p$ . \begin{align*}
    P(\text{A wins}) &= P(X=4) + P(X=5) + P(X=6)+P(X=7) \\
    &= \binom{7}{4}p^4q^3 + \binom{7}{5}p^5q^2+ \binom{7}{6}p^6q + p^7
\end{align*} However, I am quite confused on part (b). I know the two solutions above give the same answer, but I'm not sure why this works. Here is the solution provided by the professor, but I don't totally follow. ""Intuitively, the answer to (a) does not depend on whether the teams play all seven games no matter what. Imagine telling the players to continue playing the games even after the match has been decided, just for fun: the outcome of the match won’t be affected by this, and this also means that the probability that A wins the match won’t be affected by assuming that the teams always play 7 games!"" I know there's a question on this already, but it didn't provide an intuitive explanation.",['probability']
4219071,A question on the trace of a matrix,"Let $f$ be a smooth function on $\mathbb{R}^d$ . We write $\nabla^2f$ for the Hessian matrix of $f$ . For $x \in \mathbb{R}^d$ , we write $\{a_j(x)\}_{j=1}^d$ for the eigenvalue of $\nabla^2 f(x)$ . Then, we have \begin{align*}
\Delta f(x)=\text{Trace}[\nabla^2 f(x)]=\sum_{j=1}^d a_j(x).
\end{align*} My question Let $\{b_j\}_{j=1}^d$ be $\mathbb{R}$ -valued functions on $\mathbb{R}^d$ . In general, the following formula does not hold: \begin{align*}
\text{Trace}[B(x)\nabla^2 f(x)]=\sum_{j=1}^d b_j(x)a_j(x),\quad x \in \mathbb{R}^d.
\end{align*} Here, $B(x)$ denotes the $d$ -dimensional diagonal matrix whose $(j,j)$ -th element is $b_j(x)$ . However, is it possible to describe $\sum_{j=1}^d b_j(x)a_j(x)$ using $\nabla^2 f(x)$ and $B(x)$ ?",['matrices']
4219112,pseudoinverse of a square block diagonal matrix with projected diagonal matirces on diagonal,"Consider the following block-diagonal matrix: \begin{pmatrix}
P \Sigma P & 0\\
0 & (I-P) \Sigma (I-P)
\end{pmatrix} Where $P$ is an orthogonal projection matrix, and $\Sigma$ is a diagonal matrix. The individual diagonal elements $P \Sigma P$ and $(I-P) \Sigma (I-P)$ are generally not invertible (unless P doesn't reduce the rank), however, the entire matrix seems to be invertible (validated numerically). Is there a simple expression for its inverse? Update: @Carl_Schildkraut has shown the matrix is not invertible. Is there a simple expression for the pseudo inverse?","['orthogonality', 'matrices', 'linear-algebra', 'inverse', 'projection-matrices']"
4219124,Compact formula for the $n^\text{th}$ derivative of $\operatorname{sech}^2(x)$?,"In short, is there a clean formula for $\frac{d^n}{dx^n}\operatorname{sech}^2(x)?$ For convenience, let \begin{align}
f(x)&=\operatorname{sech}^2(x),\\\\
g(x)&=\tanh(x).
\end{align} Then \begin{align}
f'(x)&=-2\operatorname{sech}^2(x)\tanh(x)& &=af(x)g(x),\\\\
g'(x)&=\operatorname{sech}^2(x)& &=f(x),
\end{align} where $a=-2.$ We also have $g^2(x)=1-f(x).$ Hence $f^{(n)}(x)$ can always be written as a sum where each term is of the form $c_{k,p} [f(x)]^k[g(x)]^p$ where $p=0$ or $1$ . So the question becomes: is there a formula for the $c_{k,p}$ 's? I would expect some pattern to emerge from repeated uses of the power rule combined with the recursion of the above formulas, but I'm not sure how to proceed. Also, I know sometimes coefficients of special polynomials can encode information about $n^\text{th}$ derivatives (e.g., Hermite polynomials and $e^{-x^2}$ ), and I would find it acceptable to express the answer in terms of such coefficients if possible.","['derivatives', 'special-functions', 'recurrence-relations', 'hyperbolic-functions']"
4219130,What functions satisfy $D^* f = Df$?,I came across one exercise that I found pretty interesting. The problem gave us a new definition of derivate $D^*f(x) = \lim_{h \to 0} \frac{f^2(x + h) - f^2(x)}{h}$ . It asks us for what functions does $D^*f = Df$ . I know that $D^*f = 2fDf$ but I am not sure what functions satisfies $D^*f = Df$ except constant functions. Any idea for other non-trivial possibilities?,['derivatives']
4219135,How many almost-perfect permutations are there?,"A permutation $\left(a_1, a_2, a_3, \cdots , a_n\right)$ of the numbers $(1, 2, 3, \cdots , n)$ is called almost-perfect if there exists exactly one $i \in \{1, 2, 3, \cdots , n-1\}$ such that $a_i > a_{i+1}$ . What is the number of almost-perfect permutations of the numbers $(1, 2, 3, ... , 11)$ ? The problem is from a mock contest. Here is my attempt in solving the problem: I started by taking small values of $i$ and searched for a pattern. Here $i$ is a number for which $a_i>a_{i+1}$ . For $i=1$ , the first two numbers will be of the form $(k,1)$ where $k\in \{2,3,\dots,n\}$ . Otherwise, there will be more than one pair $(a_i,a_{i+1})$ for which $a_i>a_{i+1}$ . So, we have $n-1$ such permutations in this case. For $i=2$ , the first three numbers will be either of the form $(1,k,2)$ or of the form $(2,k,1)$ where $k\in \{3,4,\dots,n\}$ . So, we have $2(n-2)$ almost-perfect permutations in this case. For $i=3$ , the first four numbers will be of the form $(1,2,k,3)$ or $(1,3,k,2)$ or $(2,3,k,1)$ where $k\in\{4,5,\dots,n\}$ . So, we have $3(n-3)$ almost-perfect permutations in this case. So, I claim that there are $i(n-i)$ almost-perfect permutations for each $i$ . Thus, for $n=11$ we have a total of $1(11-1)+2(11-2)+\dots+10(11-10)=220$ almost-perfect permutations. I'm not sure whether the above solution is correct or not. But I think this is not the best way to solve the problem. So, I'm looking for a better solution.","['contest-math', 'permutations', 'combinatorics', 'elementary-set-theory', 'optimization']"
4219234,How many valid expressions for countdown numbers round?,"If you haven't seen Countdown before watch a quick numbers round . For the more detailed constraints please read . I'll paraphrase the rules below. Given a list $A$ of $n$ positive integers, how many unique sequences of calculations are possible? Each sequence is subject to: It only uses the four basic operations of addition, subtraction, multiplication and division. Not all $n$ integers have to be used to be considered a sequence. A number may not appear more times than it is provided in $A$ . Division can only be performed if there is no remainder. Only positive integers may be obtained as a result at any stage of the calculation. Associative calculations should only be counted once e.g. $(4 + 2) - 5$ is considered the same as $(2 + 4) - 5$ . My observation: For any pair of integers there are only 4 possible permutations not 8. Because addition and multiplication are associative so we only count them once leaving us with 6. One of the permutations will result in a fraction, leaving us with 5. One of the permutations will result in a negative number leaving us with 4. The last two observations hold where the numbers are different but the operations will be redundant if the numbers are identical so we still end up with 4. Following this I came up with $$
total = n!4^n + (n-1)!4^{n-2} + ... + 1!4^{0}
$$ $$
total = \sum_{i=1}^{n} i!4^{i-1}
$$ So for $n=6$ as in the game there are 769,641? This is a decent upper bound but there will still be whole branches of calculations which will be void because there could be some fraction, zero division or negative number. Any way to get a better approximation?","['permutations', 'combinatorics', 'combinatorial-proofs']"
4219249,"If $U_1,U_2,\dots$ are open subsets of $[0,1]$, then either prove or disprove the statements","Suppose $U_1,U_2,\dots$ are open subsets of $[0,1]$ . In each case, either prove the statement or disprove it. (a) If $m(\cap_{n=1}^\infty U_n)=0$ , then for some $n ≥ 1$ , we have $m(\overline{U_n})<1$ , where $m$ is Lebesgue measure and $\overline{U_n}$ is the closure of $U_n$ in the usual topology on $[0, 1]$ . (b) If $\cap_{n=1}^\infty U_n = \emptyset$ , then for some $n\geq 1$ , the set $[0,1] \setminus U_n$ contains a nonempty open interval. Thoughts . I think (a) is FALSE. i.e we can produce some open subsets $U_n$ such that $m(\cap_{n=1}^\infty U_n)=0$ and $m(\overline{U_n})=1$ For (b) I think it is TRUE. Suppose not. i.e. For each $n\geq 1$ if the closed set $[0,1]\setminus U_n$ does not contain a nonempty open interval then $m([0,1]\setminus U_n)=0$ , so $m(U_n)=m([0,1])=1$ for all $n\geq 1$ . So $\cap_{n=1}^\infty U_n\neq \emptyset$ , contradiction. I feel that I skip some details. Thanks for any comments/ideas/answers.","['measure-theory', 'real-analysis']"
4219265,Conditions for a dynamical system to be input-commutative?,"Consider an explicit discrete-time dynamical system (recurrence relation), $$
x_{t+1} = f(x_t, u_t)\ \ \ \ \forall t \in \mathbb{N}
$$ where $u$ is an exogenous sequence of ""inputs."" I am intrigued by systems for which the value of $x_t$ is invariant to permutations of the input sequence up to time $t-1$ . That is, if we write the ""solution"" to the system as, $$
x_t = g(x_0,u_0,u_1,\ldots,u_{t-1})
$$ then I define the property ""input-commutativity"" as when $g$ is commutative in the arguments $u_0,u_1,...,u_{t-1}$ . Put simply, it means that the system ends up in the same state regardless of the order that the inputs are provided in. Here are some simple examples on $\mathbb{R}$ : \begin{align*}
x_{t+1} = x_t u_t\ \  &\implies\ \ x_t = x_0\prod_{\tau=0}^{t-1}u_\tau \tag{1}\\[6pt]
x_{t+1} = \max\{x_t,u_t\}\ \  &\implies\ \ x_t = \max\{x_0,u_0,u_1,\ldots,u_{t-1}\} \tag{2}\\[6pt]
x_{t+1} = x_t + 2u_t\ \  &\implies\ \ x_t = x_0 + 2\sum_{\tau=0}^{t-1}u_\tau \tag{3}
\end{align*} The first two examples show that $f$ need not be linear and the third example shows that $f$ need not itself be commutative. Meanwhile, $x_{t+1} = 2x_t + 2u_t$ is an example of a linear and commutative $f$ that does not yield an input-commutative solution. It seems that linearity and/or commutativity of $f$ have no bearing on whether the solution will be input-commutative. A perhaps motivating example is the recursive application of Bayes rule from probability theory. If $u$ is a sequence of independent random variables then we can define the ""belief state"" for some random variable $\theta$ as $\ x_t = p(\theta|u_0,u_1,\ldots,u_{t-1})\ $ and have, $$
x_{t+1} = \frac{p(u_t|\theta) x_t}{p(u_t)}\ \  \implies\ \ x_t = x_0\prod_{\tau=0}^{t-1}\frac{p(u_\tau|\theta)}{p(u_\tau)}
$$ Though this is just a glorified version of the $x_t u_t$ example (1), it provides the nice physical interpretation that one's belief about an unknown parameter should end up the same no matter what order the evidence is provided in. (For what it's worth, the $\max(x_t,u_t)$ example (2) also has an interpretation as a ""peak detector"" device, and the additive example (3) could represent work/transactions performed on a storage of energy/money). Anyway, the only general form for a solution $g$ in terms of the dynamic $f$ is, $$
g(x_0,u_0,u_1,\ldots,u_{t-1}) = f(f(f(f(x_0,u_0),u_1),\ldots),u_{t-1})
$$ from which it is not so obvious to see what about $f$ is necessary and/or sufficient to make $g$ commutative in $u_0,u_1,\ldots,u_{t-1}$ . My questions are: Can this property of the solution $g$ be deduced by any properties of the dynamic $f$ ? Does this property have an official name? Any references for its study? Is there a formalization of this concept for continuous-time? Thanks in advance!","['dynamical-systems', 'recurrence-relations', 'sequences-and-series']"
4219268,"Let $\ x\in\mathbb{R}\setminus(\mathbb{Z}\ \cup [-1,1]).\ $ Does the set $\{\lfloor{x^k}\rfloor:k\in\mathbb{N}\}$ have roughly as many evens as odds?","I couldn't fit the precise question in the title, but I tried. What I mean precisely is: Let $\ x\in\mathbb{R}\setminus(\ \mathbb{Z}\ \cup [-1,1]\ ).\ $ Furthermore, for each $\ n\in\mathbb{N},\ $ define $\ A_n = \{\
 \lfloor{x^k}\rfloor:\ k\in\mathbb{N}\ \text{and}\ k\leq n\ \}\ $ where $\ \lfloor{\cdot}\rfloor\ $ is the floor function.
Proposition: $$ \lim_{n\to\infty} \frac{\text{amount of odd integers in}\
 A_n}{\text{amount of integers in}\ A_n} = \frac{1}{2}\quad $$ It is simply the apparently chaotic nature of the sets that makes me think this. And I know nothing of Chaos theory, so it's possible I'm out of my depth here. But I don't know if this has anything to do with Chaos theory, so I don't think I'm out of place for asking this question. Is this known to be true for some numbers $\ x\ $ and known to be false for others? Is it even known whether or not the limit always converges? Perhaps this can even be proven via a probabilistic approach? The arising sets seem to be chaotic in nature, so perhaps something to do with Chaos/Ergodic theory? Or perhaps there is a much more elementary approach? I've no idea... Edit: The responses thus far make me think that for radical numbers $\ x,\ $ the limit will $\ \neq\frac{1}{2}.\ $ And some algebraic numbers may give rise to patterns. So transcendental numbers are ""more interesting/more chaotic/random."" But I am not sure of any this, and I know little about the properties of transcendental numbers.","['real-numbers', 'number-theory', 'real-analysis', 'limits', 'probability-theory']"
4219314,nonlinear ODE's involving heaviside step functions: The bouncy ball as an example,"The dynamical equation for a ball bouncing on a plate (located at $x=0$ ) can be represented as $$ \ddot{x}(t) = -g - (k_1 x + k_2 \dot{x})H(-x), $$ where $H(x)$ is a heaviside step function and the collisions between the ball and the plate are represented with a spring-dashpot model. I would like to solve this equation for $x(0) = x_0$ and $\dot{x}(0) = v_0$ . One can solve this by calculating the velocity and time of the first collision, evaluating the Newtonian dynamics during the collision, then repeating this process iteratively through collisions. Obviously the resulting piecewise trajectory would be something like the following: This approach is somewhat unsatisfactory because I would ideally like to solve for the trajectory $x(t)$ at any arbitrary time, and this approach only yields a piecewise solution. A priori I do not know how many collisions have occurred up to a given time at which $x(t)$ is desired, so I would need to iterate an arbitrary number of times to find a solution. I wonder if there are specialized approaches to solve such ordinary differential equations analytically. The above equation is nonlinear and defined only in the sense of a distribution (since $H(x)$ is not exactly a function). Clearly the solution exhibits discontinuities. Are there any approaches I might read about to better understand such equations or solve such equations with more powerful tools than piecewise integration?","['classical-mechanics', 'nonlinear-dynamics', 'ordinary-differential-equations']"
4219323,Defining manifolds,"Firstly consider the following definitions: Definition 1: We say that $M\subseteq\mathbb{R}^n$ is a (smooth) manifold with dimension $m$ if for all $p\in M$ there's a map $\psi :V\to \mathbb{R}^n$ such that the following propositions are true: $V\subseteq \mathbb{R}^m$ is open and $\psi$ is a smooth immersion; $p\in \psi [V]$ and $\psi :V\to \psi [V]$ is a homeomorphism such that $\psi [V]=U\cap M$ in which $U\subseteq\mathbb{R}^n$ is open. Definition 2: Let $M\subseteq\mathbb{R}^m$ and $N\subseteq\mathbb{R}^n$ be two manifolds. We say that $f:M\to N$ is a smooth map in $p\in M$ if there're an open neighborhood $V\subseteq\mathbb{R}^m$ of $p$ and a smooth map $F:V\to \mathbb{R}^n$ such that $F|_{V\cap M}=f|_{V\cap M}$ . Besides, if $f:M\to N$ is a smooth map in $p$ for all $p\in M$ then we say that $f$ is a smooth map . We can see in the book ""Introduction to Differential Geometry"" (written by J.W. Robbin and D.A. Salamon) that the above definitions are compatible with the usual definitions of smooth manifolds and smooth maps. With those definitions many theorems about manifolds and its proofs became simpler (as you can see in that book I mentioned). Because of this I would like similar definitions that cover Banach spaces with finite dimensions. With that in mind I thought of the following definitions: Definition 1': Let $E$ be a $\mathbb{R}$ -Banach space with finite dimension. We say that $M\subseteq E$ is a (smooth) manifold with dimension $m$ if for all $p\in M$ there's a map $\psi :V\to E$ such that the following propositions are true: $V\subseteq\mathbb{R}^m$ and $\psi $ is a smooth immersion; $p\in \psi [V]$ and $\psi :V\to \psi [V]$ is a homeomorphism such that $\psi [V]=U\cap M$ in which $U$ is open in $E$ . Definition 2': Let $E,F$ be two $\mathbb{R}$ -Banach spaces with finite dimension. Suppose that $M\subseteq E$ and $N\subseteq F$ be manifolds. We say that $f:M\to N$ is a smooth map in $p\in M$ if there're an open neighborhood $V\subseteq E$ of $p$ and a smooth map $F:V\to F$ such that $F|_{V\cap M}=f|_{V\cap M}$ . Besides, if $f:M\to N$ is a smooth map in $p$ for all $p\in M$ then we say that $f$ is a smooth map . My question: Are the last two definitions compatible with the usual definitions of manifolds? Will there be any problem if I use the previous two definitions to study manifold contained in finite dimensional Banach spaces? By saying ""compatible"" I mean the following: in the case $M\subseteq \mathbb{R}^n$ we can prove, for example, that definition 1 is equivalent to the usual definition of manifolds (using charts). So I want to know if this is also true in the case $M$ is a subset of a finite dimensional Banach space. Thank you for your attention!","['manifolds', 'definition', 'smooth-manifolds', 'differential-geometry']"
4219340,Prove o disprove that $X$ is connected?,"Given $ r> 0 $ , let $ C_r $ be the circumference in the plane that has center at $ (0,0) $ and radius $ r $ . Let $ X $ be a subset of $ \Bbb R ^ 2 $ that has the following properties, For all $r\in\Bbb Q$ , $C_r\subseteq X$ and, for all $r\in\Bbb R\setminus \Bbb Q$ , $X\cap C_r\neq \varnothing$ . Is $X$ connected? I affirm that it is connected. And to demonstrate, I do it by contradiction and I assume that $ X $ is not connected, so there are two separate sets $ A $ and $ B $ , such that $ X = A \cup B $ . I have a minimal idea, and that is that for example, $ C_1 $ being a connected set, being contained in $ X $ , it must be completely contained in $ A $ or completely contained in $ B $ , since it is $ 1 \in \Bbb Q $ . Without loss of generality let's say that it is completely contained in $ A $ . So somehow show that all other $ C_r $ is contained in $ A $ . And then use some density argument to show that the points that are in $ X \cap C_r $ with $ r \in \Bbb R \setminus \Bbb Q $ , cannot be in $ B $ . So $ B $ has to be empty and I would have my absurdity. Any help for the exercise or how I can develop my idea. Thanks a lot.","['general-topology', 'connectedness']"
4219342,"mapping the circle $|z|=3$ into $|z-1|=1$, the point $3+3i$ into $1$ and the point $3$ into $0$.","Question: Find the bilinear transformation which carries the circle $|z|=3$ into $|z-1|=1$ , the point $3+3i$ into $1$ and the point $3$ into $0$ . My Attempt:  First, I've done problems like this before, but I ran into something at the beginning, so I want to first write my proof, then mention what I ran into and hope to have my solution verified and to see if I can get an explanation on why I had that problem and what it means.  So here goes: Let $f(z)=w$ we such a transformation.  We have that $f(3+3i)=1$ and $f(3)=0$ .  Now, we see that, using $z^*=\frac{R^2}{\bar z-\bar a}+a$ , that $3+3i$ is symmetric to $3-3i$ with respect to $|z|=1$ and that $1$ is symmetric to $\infty$ with respect to $|z-1|=1$ .  Thus, we have $f(3-3i)=\infty$ .  So, using the cross ratio, we have $(w,1,0,\infty)=(z,3+3i,3,3-3i)$ , and so we have $\frac{w-0}{w-\infty}\frac{1-\infty}{1-0}=\frac{z-3}{z-(3-3i)}\frac{3+3i-(3-3i)}{3+3i-3}$ .  Going through the calculation, we get that $w=\frac{(z-3)2i}{z-3+3i}$ . So, first, does this solution look correct?  Next, when I was trying to find that ""last point"", I was running into an issue using $f(3)=0$ .  I kept getting $3$ is symmetric to $3$ with respect to $|z|=3$ and $0$ is symmetric to $0$ with respect to $|z-1|=1$ , so I was just getting a transformation value that I already knew.  Why?  Or, should I, of course, have used the point with the nonzero imaginary part?  Any help is greatly apprecaited!  Thank you.","['complex-analysis', 'linear-transformations', 'cross-ratio']"
4219350,Why growth function little-omega $\omega(m)$ equals $\Theta(\log{m})$,"I was trying to prove which is asymptotically larger $\log(\log^*{n})$ or $\log^*(\log{n})$ , where $\log^*$ is the iterative logarithm that calculates the number of times before we reach 0. Definition : We define $\omega(g(n))$ (“little-omega of $g$ of $n$ ”) as the set $\omega(g(n))$ = { $f(n)$ : for any positive constant $c>0$ , there exists a constant $n \gt n_0$ such that $0≤cg(n)<f(n)$ for all $n ≥ n_0$ }. Definition : We define $\Theta(g(n))$ = { $f (n) $ : there exist positive constants $c_1, c_2$ , and $n_0$ such that $0 ≤c_1g(n) ≤ f(n) ≤ c_2g(n)$ for all $n ≥ n_0$ }. Problem : Why growth function little-omega $\omega(m) = \Theta(\log{m})$ please, which is not clear based on respective definitions above? I am not even sure how $\omega(m) = \Theta(\log{m})$ is connected to prove which is asymptotically larger $\log(\log^*{n})$ or $\log^*(\log{n})$ , $m$ is not defined as well, but I guess it's an integer. So, presumably, $m$ is an integer.","['elementary-number-theory', 'discrete-mathematics']"
4219414,How to decompose $\frac{1+x}{\sqrt{(1-x)}}$ into partial fractions?,"Basically homework help. The question ( Problems of Calculus in One Variable , IA Maron, number 2.3.9(b) ) is to find the derivative of the 100th order of the function $$
y = \frac{1+x}{\sqrt{(1-x)}}
$$ by 'expansion into a linear combination of simpler functions'. I can't find any help online. There is a hint at the back, which says that $y$ can be written as $2(\sqrt{1-x})^{-1} - \sqrt{1-x}$ , but how we get that, I have no idea. How am I supposed to decompose $y$ ?","['calculus', 'derivatives', 'problem-solving', 'partial-fractions']"
4219466,Catalan constant's integral representation,"Catalan constant is known to have a rich source of integral identities, here is the formula I found: $$ \int_0^\infty \frac{\sin^{-1}(\sin(x))}{x} \,dx \ =2G.$$ This can be proved by analyzing the function and using the identity: $$ \frac{2G}{\pi}-\frac{1}{2}=\ln\left(\prod_{n=1}^\infty \frac{(4n-1)^{4n-1}}{(4n-3)^{2n-1}(4n+1)^{2n}}\right).$$ Is there any ""neat"" way to prove the formula directly without using the identity? It's really hard for me to prove the identity. Update I found my own way to prove this integral indentity: Using Fourier series of triangle wave, with a little change in the coefficient, I got this equation: $$\sin^{-1}(\sin(x))=\frac{4}{\pi}\sum_{n=0}^\infty (-1)^{n}\frac{\sin((2n+1)x)}{(2n+1)^{2}}$$ Divide both sides by $x$ and integrate: $$\int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=\frac{4}{\pi}\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}\int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx$$ Using the fact that $\int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx=\int_{0}^\infty \frac{\sin(x)}{x}dx=\frac{\pi}{2}$ , we got: $$\int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=2\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}=2G$$","['integration', 'catalans-constant', 'real-analysis']"
4219508,Is this trick not true when solving an ODE?,"I have a difficult first-order differential equation to solve. Here it is $$-y'^2y^2+ay^4+by=-c$$ Where y is a function of time and a,b and c are constants. So in order to find an easier differential equation that I could solve I did the following, we know that $$1=\frac{dt}{dt}=\frac{dt}{dy}\frac{dy}{dt}=\frac{dt}{dy}y'$$ Let's thus rewrite the ODE as, $$-y'^2y^2+ay^4+by=-c\frac{dt}{dy}y'$$ Since I know that $y'$ is never 0, I divide by $y'$ on both sides, $$(-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=-c\times dt$$ So if we integrate both sides with respect to the correct variable we get, $$\int(-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=\int-c\times dt$$ $$-\frac{y'}{3}y^3+\frac{a}{5y'}y^5+\frac{b}{2y'}y^2=-c\times t+p$$ Where p is an integration constant. This puts me in a more favorable place for solving my original problem. Is the trick used in equation 3 valid? Are there any errors?","['integration', 'derivatives', 'ordinary-differential-equations']"
4219509,When is it true that $\sup g - \inf g \le 2\sup g$?,"I am currently reading the first version of the paper titled ""On the Margin Theory of Feedforward Neural Networks"" by Colin Wei, Jason D. Lee, Qiang Liu and Tengyu Ma. In Lemma C.4 of the appendix, the authors give upper bounds for the Rademacher complexity of some specific function class, and there is one step in the proof they give that I fail to understand. Page 21, the authors give the following upper bound : $$\begin{align}
\mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert - \inf_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert\right] &\le \mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i) - \inf_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right] \\
&\le 2\mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right]\end{align} $$ To provide some context here, $\epsilon_i$ are i.i.d. Rademacher random variables, $\mathbb S^d$ is the $d$ -dimensional unit sphere, $x_i$ are elements of $\mathbb R^d$ and $\phi$ is an $M$ -Lipschitz activation function. However these details don't seem very relevant here since in the first inequality, it seems that they simply used the fact that $\sup |g| - \inf |g| \le \sup g - \inf g$ . The second inequality is quite puzzling to me because it seems to use the fact that $\sup g - \inf g \le 2\sup g$ which is equivalent to $ -\inf g \le \sup g$ and is not true in general (consider for instance $g : x \mapsto -x^2$ ). I am thinking that maybe under some assumption this inequality might be true, but I can't manage to get further than this. I would be grateful for any help in proving why the second inequality holds true.","['statistics', 'real-analysis', 'machine-learning', 'inequality', 'random-variables']"
4219512,What is the difference between MVUE and UMVUE,"I was going through the minimum variance unbiased estimators and I am confused about the concept of MVUE and UMVUE. Is the unbiased estimator whose variance attaining CRLB a UMVUE or MVUE? I referred two books. One of those(A first course on parametric inference by B.K.Kale ) says that the unbiased estimator attaining CRLB is MVUE and the other(An introduction to probability and statistics by Rohatgi) says that it is UMVUE. So, are they same?","['statistical-inference', 'statistics', 'parameter-estimation']"
4219624,"Prove $\int_{\mathbb{R}^n}e^{-\max\{|x_1|,\ldots,|x_n|\}}dx=2^nn!$","Prove $\int_{\mathbb{R}^n}e^{-\max\{|x_1|,\ldots,|x_n|\}}dx=2^nn!$ My attempt:
On the region where $x_1$ has the largest absolute value $\int_{\mathbb{R}^n}e^{-\max\{|x_1|,\ldots,|x_n|\}}=2^n\int_{\mathbb{R}^n}e^{-|x_1|}dx=2^n\int_0^{\infty}\int_{\mathbb{S}^{n-1}_r}e^{-|x_1|}dSdr$ But I got stuck here.","['integration', 'calculus']"
4219642,Can you provide a symmetric presentation of this partition lattice?,"I have a question about the partition lattice on a set with four elements, shown here: Equivalently, this is the lattice of equivalence relations on that set. An observant person will notice that the lattice is nearly symmetric from left-to-right, but not quite. For example, the partition at lower left connects to the next level in a slightly different manner than the partition at lower right. Question. Is there a left-to-right symmetric presentation of this lattice? If not, how do we see this? Of course, in an abstract sense, there are many symmetries of this lattice, since any permutation of the points in the set acts on the partitions. What I want to know is whether I can present the Hasse diagram of this lattice in the plane in a way that has a vertical line symmetry. I am confused about how to think about this. I would have similar questions for larger partition lattices. The lattice of partitions on a three-element set does have a symmetric presentation.","['combinatorics', 'lattice-orders']"
4219649,How come the derivative of $e^{i\theta} $ never vanish,"Since $e^{i\theta} $ where $0\leq\theta\leq 2\pi $ is paramaterization of the unit circle, I would expect its derivative to vanish, as the tangent line slope in some point definetly should be zero. What's different here?",['complex-analysis']
4219661,Double integral separating real and imaginary parts of $\zeta (\sigma+i t)$,"Recently, I stumbled upon what I believe to be a new representation of $\zeta(\sigma+i t)^b$ by chance, and thus have no proof of it, and I am wondering if it is possible to prove. Let $\eta (s) = \zeta (s) (1-2^{1-s})$ denote the Dirichlet eta function, where $\zeta$ is the Riemann zeta function and $s = \sigma + i t$ . Motivation: To motivative the potential usefulness of this representation, notice that $\zeta$ is transformed from taking a complex argument, to a real argument, allowing safer manipulations with it, as log singularities are no longer as major of an issue. I've had Mathematica numerically verify the conjecture on a few hundred combinations of $\sigma, t$ , and $b$ , each time producing the correct values- as far as I can tell, it even seems to behave appropriately near $\zeta$ zeros, from approaching the first few zeros, above and below the real line, from all directions. $\forall \sigma, t, b \in \mathbb{R}_{\geq 0}$ : $$\eta (s)^b-1=\frac{2}{\pi}\int_{0}^{\infty}\int_{0}^{\infty}\cos(\sqrt{t}x e^{i \pi/4})\cos(x y)\left(\eta(\sigma+y^2)^b-1\right)\,dy\,dx$$ Is it possible to prove/disprove this representation? A suggestion by @DinosaurEgg was the fact that a Fourier inversion formula indicates: $$f(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \cos \left(y\left(x-t\right)\right) f(x) \, dx \, dy$$ which for even functions can be written as $$f(t) = \frac{2}{\pi} \int_{0}^{\infty}\int_{0}^{\infty} \cos \left( x y \right) \cos \left( y t\right) f(x) \, dx \, dy$$ However, to my knowledge, Fourier inversion only generally holds for functions $f : \mathbb{R} \to \mathbb{C}$ so perhaps the Paley-Wiener theorem is at play here to allow my $\eta$ representation to hold, however, I’m not sure how to apply it here.","['integration', 'improper-integrals', 'fourier-analysis', 'analysis', 'analytic-number-theory']"
4219692,Coercivity of a bilinear form on $L^2$,"This problem is from a mathematical analysis exam Define this bilinear form on $L^2([0,1])$ : $$
a(u,v)=\int_{[0,1]^2}{k(x,y) u(x) v(y) dx dy} \qquad \forall u,v \in
L^2([0,1]) $$ Can this bilinear form be coercive on $L^2$ for some choice of $k \in L^{\infty}([0,1]^2)$ ? I am quite concerned by the ""official"" solution, which is Yes, the bilinear form $ a $ is coercive as long as we make the
following assumptions regarding the Kernel function $ k $ : $ k(x,x) \in C([0,1]) $ ; $ \inf_{x \in [0,1]} k(x,x) = a_0 > 0 $ ; $ k(x,y) = 0 \;\;\; \forall \; x \ne y $ . Using these three conditions, $ \forall u \in L^2([0,1]) $ we get $$ \begin{align} a(u,u) & = \int_{[0,1]^2} k(x,y) \: u(x) \: u(y) \:
 dx \, dy \\ & = \int_{0}^{1} k(x,x) \: u(x)^2 \: dx \\ & \ge a_0 \int_{0}^{1} u(x)^2 dx = a_0 \| u \|_2^{2} \end{align} $$ My point is that in the previous example, $k$ is almost everywhere zero on $[0,1]^2$ , so, the bilinear form should be identically zero. Instead, in order to make this passages, one should take $k(x,y)=\delta_x(y)$ , which of course is not in $L^\infty$","['bilinear-form', 'lp-spaces', 'functional-analysis']"
