question_id,title,body,tags
3517376,A sum of partitions,"A friend of mine presented a problem I found interesting: Compute the following: $$\sum_{n=0}^\infty\left(\prod_{k=1}^j(1+x^k)\right)[x^{mn}]$$ where $P(x)[x^n]$ denotes the $x^n$ coefficient of $P$ . This problem is meant to be solved with the assistance of a program for large $j$ (larger than 1000) and $m\ll j$ . I'm interested in general, but if the case of $m~|~j$ is special, then that suffices for me. My first thought was to brute force compute all of the coefficients, but this takes $\mathcal O(j^3)$ time, which is too slow. My second thought was that this is essentially searching for distinct partitions . The issue is the finite upper bound, meaning that this becomes inaccurate after than $x^{j+1}$ coefficient. I'm not super familiar with partitions to know how to adjust for this without brute force like above. My third thought was to tackle this more generally with: $$\sum_{n=0}^\infty P(x)[x^{mn}]=\frac1m\sum_{n=0}^{m-1}P(e^{2\pi in/m})$$ But this almost completely ignores the form of $P$ , so I don't feel like this is how the problem should be tackled. How can I tackle this problem? Update: I just realized that when $m~|~j$ we have $$\sum_{n=0}^\infty\left(\prod_{k=1}^j(1+x^k)\right)[x^{mn}]=\frac1m\sum_{n=0}^{m-1}\left[\prod_{k=0}^{m-1}(1+e^{2\pi ink/m})\right]^{j/m}$$ which is far fewer computations. Ruby code . Still interested in alternative solutions.","['integer-partitions', 'combinatorics', 'algorithms', 'sequences-and-series']"
3517418,$\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{\sqrt[3]{kn^2}}$,"In the very beginning, I'm going to refer to an already posted question quite similar to mine: Limit $\lim_{n\to\infty} n^{-3/2}(1+\sqrt{2}+\ldots+\sqrt{n})=\lim_{n \to \infty} \frac{\sqrt{1} + \sqrt{2} + ... + \sqrt{n}}{n\sqrt{n}}$ And, to give an insight into what I've examined already: Limit of sum with cube roots . Compute: $$\lim_{n\to\infty}\frac{1+\frac1{\sqrt[3]{2}}+\ldots+\frac1{\sqrt[3]{n}}}{\sqrt[3]{n^2}}$$ My attempt: As in the references, I also thought of applying Stolz-Cesaro , and got the following: $L=\lim\limits_{n\to\infty}\frac{1+\frac1{\sqrt[3]{2}}+\ldots+\frac1{\sqrt[3]{n}}}{\sqrt[3]{n^2}}=\lim\limits_{n\to\infty}\frac{\sum\limits_{k=1}^{n+1}\frac1{\sqrt[3]{k}}-\sum\limits_{k=1}^n\frac1{\sqrt[3]{k}}}{\sqrt[3]{(n+1)^2}-\sqrt[3]{n^2}}=\lim\limits_{n\to\infty}\frac{\frac1{\sqrt[3]{n+1}}}{\sqrt[3]{(n+1)^2}-\sqrt[3]{n^2}}$ To avoid L'Hospital , I've done this: $\begin{aligned}L&=\lim_{n\to\infty}\frac1{\sqrt[3]{n+1}\left(\sqrt[3]{(n+1)^2}-\sqrt[3]{n^2}\right)}\\&=\lim_{n\to\infty}\frac{1}{n+1-\sqrt[3]{n^2(n+1)}}\\&=\frac1{\infty-\infty}\\&=\frac1{\infty}=0\end{aligned}$ I was wondering if there were another elegant method apart from Riemann sum or already used Stolz-Cesaro that I could use because this task appeared in Analysis 1 exam. Stolz-Cesaro is allowed and there was no constraint on any other methods, but I'm not familiar to Riemann sums at all. To ask explicitly(for the sake of developing new ideas): For example:
If I changed the order of the summands this way: $$\frac{1}{\sqrt[3]{n}}+\frac{1}{\sqrt[3]{n-1}}+\ldots+\frac{1}{\sqrt[3]{2}}+1$$ would that be of any use in an algebraic manipulation that would lead me on the right track?
Final question:
Is my answer: $L=0$ correct?","['limits', 'calculus', 'solution-verification']"
3517429,Evaluating the limit $ \lim_{x \to 0} \frac{x}{p} \lfloor \frac{q}{x} \rfloor$ [duplicate],This question already has answers here : what is the limit of $\displaystyle \lim_{x\to 0} \frac{x}{a}\cdot \lfloor{\frac{b}{x}\rfloor}$ (2 answers) Closed 4 years ago . $$\lim_{x \to 0} \frac{x}{p} \left\lfloor \frac{q}{x} \right\rfloor =\;\;\; ?$$ My attempt: $\frac{q}{x}-1 < \lfloor \frac{q}{x} \rfloor \le \frac {q}{x} $ . Then I consider four cases. In case 1 I took x and p both positive and then I multiplied above by $\frac{x}{p} $ and took limit and applied squeeze theorem. In other cases I did the same. My answer is $ \frac{q}{p} $ . Is the solution correct?,"['analysis', 'real-analysis', 'calculus', 'solution-verification', 'limits']"
3517487,Minimally Sufficient Statistics,"I'm trying to find the minimally sufficient statistic where $\{X_i\}_{i=1}^{n}$ are iid from the following family of populations: $$P=\{U(0,\theta): \theta>0\}$$ I looked at the ratio of the likelihood of two observations in the support of the family of populations: $$\frac{\frac{1}{\theta}^n\chi_{(x_{(n)},\infty)}(\theta)}{\frac{1}{\theta}^n\chi_{(y_{(n)},\infty)}(\theta)}=\frac{\chi_{(x_{(n)},\infty)}(\theta)}{\chi_{(x_{(n)},\infty)}(\theta)}$$ This will not be a function of $\theta$ iff $X_{(n)}=Y_{(n)}$ . One can conclude that $X_{(n)}$ is a minimally sufficient statistic. I am questioning my answer because it is my understanding that the ratio cannot include $\theta$ , I have only  shown  is  that it will not  depend on $\theta$ if $X_{(n)}=Y_{(n)}$ . Is this the same or am I  missing anything?","['statistical-inference', 'statistics', 'order-statistics', 'sufficient-statistics']"
3517492,trace 0 implies product of symmetric and skew symmetric,"If $A$ is symmetric and $B$ is skew-symmetric, then $tr(AB)=0$ . I would like to know if the converse holds true. If not, can you give me an example?","['linear-algebra', 'analysis']"
3517498,"Real, analytic exponential factorial $f(x) = x^{f(x-1)}$","I'm wondering if anyone has a reference or a method for construction a real-analytic function interpolating the exponential factorial , i.e. a function such that $f(x) = x^{f(x-1)}$ and $f(1) = 1$ , analogous to the gamma function interpolating the usual factorial. It's easy to construct a complex-valued solution that should be analytic on $\mathbb{C}\setminus\mathbb{R}$ to the clearly related functional equation $f(z) = z^{(z-1)^{(z-2)^{f(z-3)}}}$ (note that satisfying the exponential factorial equation implies this, but not the other way around). If you look at the power tower $$
f_n(z) =z^{(z-1)^{(z-2)^{(z-3)^{\cdots (z-n)}}}}
$$ the values have period 3 as $n$ approaches infinity, so it seems reasonable that there are 3 limiting analytic functions, each satisfying $f(z)=z^{(z-1)^{(z-2)^{f(z-3)}}}$ . (I haven't proven any of this formally, since I was mostly trying to find a real-valued solution and these are not real valued on $\mathbb{R}$ ) An example computation, here's a plot of the real and imaginary parts of $f_n(1.5)$ for $n$ from 0 to 100 (real part in black, imaginary part in red): $f_{3n}$ seems to converge pretty rapidly (and similarly for $f_{3n+1}$ and $f_{3n+2}$ ). But these limiting functions won't be analytic at integer values; in general it will only be $n-1$ times differentiable at $n\in\{1,2,3,\dots\}$ . Also, I don't believe they would have the property $f(x)=x^{f(x-1)}$ because of the 3-period nature of the limit. Does anyone know of a way to construct a real-analytic solution to the equation, or at least a solution that is analytic (but possibly complex-valued) on $\mathbb{R}$ ? If it's easier to solve the equation $f(x) = x^{f(x-1)}$ with a different initial value, i.e. $f(1) = c$ for some other value of $c$ , I would also be interested to see that.","['complex-analysis', 'functional-equations', 'analytic-functions', 'real-analysis']"
3517540,"Count the number of sets of $3$ integers from $\{1, 2, \ldots , 30\}$ if no two consecutive numbers can be in the same set.","Count the number of sets of $3$ integers from $\{1, 2, \ldots , 30\}$ if no two consecutive numbers can be in the same set. So far I understand that there are $^nC_k$ sets of $k$ numbers given the initial set of $n$ numbers. So the number of sets that contain no consecutive numbers $=$ $$^nC_k - [\# \text{sets w/ $2$ consecutive numbers}] - [\#\text{ sets w/ $3$ consecutive numbers}] - \ldots - [\# \text{sets w/ $n$ consecutive numbers}].$$ But I am not sure how to go about finding the sets.","['combinations', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory']"
3517678,Factoring: how to prove a polynomial can be written as product of x minus its zeros?,"For a lower degree polynomial function, we can simply expand the product of factors using distributive properties and compare that to the original polynomial. But how to prove that an arbitary degree single variable polynomail indeed equals to product of x minus all its zeros (complex numbers), in another word, validatity of factoring?","['number-theory', 'algebra-precalculus']"
3517713,How to find the expectation of a Poisson process related variable,"Q) Let $X_k\sim \operatorname{Exp}(\lambda)$ , $X_k$ are iid which represent the interarrival times of a Poisson process of mean rate $\lambda$ . Let $Y_k$ be the arrival times of the events. Let $$Z = \sum_{k=1}^N e^{-(t-Y_k)}$$ where $Y_1<Y_2<\cdots< t$ i.e. $N(t)$ events have occurred in time $t$ . Find $EZ, \operatorname{Var}(Z)$ . I have two questions. Does the $EZ$ calculation below look right and is there a simpler way to find $\operatorname{Var}(Z)$ because the expressions I have are large? $N(t)\sim \operatorname{Pois}(\lambda t)$ and $Y_k\sim \operatorname{Erlang}(k,\lambda)$ because $Y_k$ is the sum of iid exponential r.v's. $$\begin{align}
EZ &= E[E[Z\mid N(t)]] \\
&= E\left[\sum_{k=1}^{N(t)} Ee^{-(t-Y_k)}\right] \\
&= E\left[\sum_{k=1}^{N(t)}\int_0^{\infty} e^{-(t-x)}\frac{\lambda^k x^{k-1}e^{-\lambda x}}{(k-1)!}dx \right] \\
&= E\left[\sum_{k=1}^{N(t)}\frac{\lambda^k e^{-t}}{(k-1)!} \int_0^\infty e^{(1-\lambda )x}x^{k-1}dx \right] \\
\end{align}
$$ In general, $$\int_0^{\infty}e^{ax}x^n = (-a)^{-k-1}\Gamma(n+1)$$ Thus: $$\begin{align}
EZ &= E\left[\sum_{k=1}^{N(t)}\frac{\lambda^k e^{-t}}{(k-1)!}\int_0^{\infty} e^{(1-\lambda )x}x^{k-1}dx \right] \\
&= E\left[\sum_{k=1}^{N(t)}\frac{\lambda^k e^{-t}}{(k-1)!} (\lambda - 1)^{-k}\Gamma(k)\right] \\
&= E\left[\sum_{k=1}^{N(t)}\left( \frac{\lambda}{\lambda-1}\right)^k e^{-t} \right] \\
&= e^{-t} E\left[ \frac{1-\left(\frac{\lambda}{\lambda-1}\right)^{N(t)+1}}{1-\frac{\lambda}{\lambda-1}} - 1 \right] \tag{1}\\
\end{align}
$$ Note that since $N(t)\sim \operatorname{Pois}(\lambda t)$ : $$
\begin{align}
E\left[ \left(\frac{\lambda}{\lambda-1}\right)^{N(t)+1} \right] &= \frac{e^{-\lambda}}{\lambda !}\sum_{k=0}^{\infty} \left(\frac{\lambda}{\lambda-1}\right)^{k+1}\times (\lambda t)^k\\
&= \frac{1}{(\lambda-1)(\lambda-1)!}.e^{-\lambda}\sum_{k=0}^{\infty} \left(\frac{\lambda ^2 t}{\lambda-1}\right)^k \\
&= \frac{e^{-\lambda}}{(\lambda-1)(\lambda-1)!}\times \frac{1}{1-\left(\frac{\lambda ^2 t}{\lambda-1}\right)} \tag{2}
\end{align}
$$ Plugging $(2)$ in $(1)$ , we get $EZ$ .","['stochastic-processes', 'statistics', 'poisson-process', 'probability']"
3517714,Continuous in subspace topology,"Suppose $X = \lbrace 1,2,3,4,5 \rbrace$ and $\tau = \lbrace X, \emptyset , \lbrace 1 \rbrace , \lbrace 1,2 \rbrace , \lbrace 1,3,4 \rbrace , \lbrace 1,2,3,4 \rbrace , \lbrace 1,2,5 \rbrace \rbrace$ . For $\tau_{M}$ we take subspace topology on $M = \lbrace 1,3,5 \rbrace$ . 
We consider the function $f: X \rightarrow \lbrace 0,1 \rbrace$ , where $$f(n) = \left\{ \begin{array}{ll}
0 & \textrm{if $n \le 3$}\\
1 & \textrm{others }\\
\end{array} \right.
$$ Let $f \mid_{M} : M \rightarrow \lbrace 0,1 \rbrace$ be the restriction of $f$ to $(M, \tau_{M})$ . Show that these function $f$ and $f \mid_{M}$ are continuous. If not, point the counterexample. What I did? I think, that $\tau_{M} = \lbrace \lbrace 1,3,5 \rbrace , \emptyset , \lbrace 1 \rbrace  \rbrace$ . But,  have no idea, what can I do next. Please, help me :)","['continuity', 'general-topology', 'functions']"
3517726,Finding the third roots of unity by equating $(a + bi)^3$ to $1$,"I know what the third roots of unity are but I want to solve this exercise: Complex numbers can be written as $a + bi$ . Simplify $(a + bi)^3$ and
  equate the real part to $1$ and the imaginary part to $0$ to find the
  three roots of unity. So I expanded $(a + bi)^3$ to get $(a^3 - 3ab^2) + (3a^2b - b^3)i$ . I know that $a = 1$ and $b = 0$ is a solution just by looking at it. Now is there some sort of clever way to get another root of unity? I tried to do it with just algebra but it ended up being very messy. I was wondering whether there is some neat solution to this.","['algebra-precalculus', 'radicals', 'complex-numbers']"
3517748,Is the following parametrizations identifiable?,"$X$ and $Y$ are independent $N(\mu_1, \sigma^2)$ and $N(\mu_2, \sigma^2)$ , $\theta = (\mu_1, \mu_2)$ and we observe $Y − X$ . From what I understand, a model $P_\theta$ is identifiable if $\theta_1=\theta_2$ implies that $P_{\theta_1}=P_{\theta_2}$ and $\theta_1\not=\theta_2$ implies that $P_{\theta_1}\not=P_{\theta_2}$ . How can I choose ${\theta}$ to prove or disprove this? $$Y-X \sim N(\mu_2-\mu_1, 2\sigma^2)$$ If I try both $\mu_2,\mu_1$ with a shift - I think I will get the same answer? Does that mean it's Identifiable?","['statistical-inference', 'statistics']"
3517754,Step in the proof of Girsanov's theroem,"In the proof of Girsanov's theorem i see the following: Let $$M_t=e^{ia^{tr}B_t+\frac12||a||^2t}$$ where B is the standard brownian motion and $a\in \mathbb R^d$ . We then know that $$\sup_{t\le T}|M_t|$$ is bounded for all $T\ge0$ . I'm not able to understand this last step, why it is bounded? I only see that $\sup_{t\le T}|M_t|\le\infty$ since $M_t$ is a.s. continuous.","['stochastic-processes', 'brownian-motion', 'probability-theory']"
3517793,"Compact-open and Whitney $C^\infty$-topologies agree on $C^\infty(M, N)$ for compact $M$.","Let $M$ and $N$ be smooth manifolds. There are different topologies we can equip the space $C^\infty (M, N)$ of smooth mappings between them with. Two of them are the compact-open topology and the Whitney $C^k$ -topologies for $k \in \mathbb{N} \cup \{\infty\}$ (I'm using this definition based on jet bundles). I have read claims that when $M$ is compact these agree. Is this true? Could someone provide a proof or a reference? EDIT: I'm starting to seriously doubt that this is true for $k \geq 1$ .  I can show that the Whitney topology is finer than the compact-open, but for say $f: \mathbb{S}^1 \to \mathbb{R}$ allowing to change the value by any $\varepsilon$ on a compact set with nonempty interior is sufficient to make the derivative arbitrarily large.","['jet-bundles', 'manifolds', 'general-topology', 'differential-topology']"
3517838,Proving $\bigcap_{i\in\Bbb{N}}(A_i\cup B_i) = (\bigcap_{i\in\Bbb{N}}A_i) \cup (\bigcap_{i\in\Bbb{N}}B_i)$ for nonincreasing set series,"I am having some problems with the following: Prove that if $A_0 \supseteq A_1 \supseteq A_2 \supseteq ...$ and $B_0 \supseteq B_1 \supseteq B_2 \supseteq ...$ then $\bigcap_{i\in\Bbb{N}}(A_i\cup B_i) = (\bigcap_{i\in\Bbb{N}}A_i) \cup (\bigcap_{i\in\Bbb{N}}B_i)$ Although it is quite easy to show that $\bigcap_{i\in\Bbb{N}}(A_i\cup B_i) \supseteq (\bigcap_{i\in\Bbb{N}}A_i) \cup (\bigcap_{i\in\Bbb{N}}B_i)$ by unwinding the definitions, I can't find any way to prove the opposite. I tried experimenting with the fact that $\limsup_{i\to\infty} A_i=\liminf_{i\to\infty}A_i=\lim_{i\to\infty}A_i$ (following from the fact the series is nonincreasing, same for $B_i$ ), but it doesn't seem to help any bit except for adding another layer of abstraction. I thought that maybe if $\lim_{i\to\infty}(\oldstyle{A_i\cup B_i})=\lim_{i\to\infty}(\oldstyle{A_i})\cup\lim_{i\to\infty}(\oldstyle{B_i})$ for any converging $\oldstyle{(A_n)_{n\in\Bbb{N}} ,(B_n)_{n\in\Bbb{N}}}$ were true, this problem would be solvable without much hassle. However I haven't found anything about any equality like this (and I am not sure if it is true anyways).",['elementary-set-theory']
3517844,What does $\sum\limits_{i\neq j}$ mean?,"I have seen similar questions (perhaps too similar, I´m sorry if so) but none of them do too much of a good job on defining what an expression such as $$\sum_{i\neq j}^n(x_iy_j)$$ means exactly. What set of numbers am I adding up?  What does it mean to have two variables on the running variables under the sum?  Is there another notation that might clear things up for me?","['elementary-set-theory', 'summation', 'notation']"
3517887,Proving $\frac1{\sec\phi-\tan\phi} - \frac1{\sec\phi+\tan\phi} =2\tan\phi$,Prove $$\frac1{\sec\phi-\tan\phi} - \frac1{\sec\phi+\tan\phi} =2\tan\phi$$ I have managed to simplify the lefthand side of the equation to $2\sin\phi /\cos^2\phi$ (maybe incorrect) but cannot seem to finish it off.,"['fractions', 'trigonometry']"
3517890,Spectrum of the product of positive elements of a $C^\ast$-algebra.,"I am working on problem 2.a in Murphy's $\textit{$C^\ast$-Algebras and Operator Theory}$ , which asks to show that for positive elements $a, b$ of a unital $C^\ast$ -algebra $A$ , $\sigma(ab) \subset [0, \infty)$ . By the definition given in this textbook, $a \in A$ is positive if $a$ is hermitian and $\sigma(a) \subset [0, \infty)$ . It's true that, if $a$ and $b$ commute, then $ab$ is positive, as: $$ab = (a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2}),$$ from which it follows that $\sigma(ab) \subset [0, \infty)$ . Then, to solve the given problem, I invoke the following argument: $$\sigma(ab) \cup \{0\} = \sigma((a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2})) \cup \{0\} \subset [0, \infty),$$ from which it follows that $\sigma(ab) \subset [0, \infty)$ . My question is this: we have that, for arbitrary positive $a, b \in A$ , $a$ and $b$ are hermitian, from which it follows that $ab$ is hermitian. Furthermore, by the above argument, it follows that $\sigma(ab) \subset [0, \infty)$ . Does it not follow from this that $ab$ is positive for arbitrary positive $a,b$ ?","['c-star-algebras', 'operator-theory', 'spectral-theory', 'functional-analysis']"
3517897,Is there a non-mathematical definition of the geometric mean?,"I am aware that the geometric mean is often used with the lognormal distribution, because then it directly relates to the arithmetic mean with the normal distribution. But I was trying to think of an intuitive defintion of the geometric mean. For example, the median can be explained as ""the data point such that half of the data points have higher values and the other half have lower values."" Is there a similar definition for the geometric mean?","['central-tendency', 'statistics', 'means']"
3517903,mathematical induction natural number,"Tell me about this exercise, I try to solve it but it was confusing
A bank gives 20\$ and 50\$. I must use mathematical induction so that the bank will create whatever amount of money bigger or equal to 40\$, that it is multiple to 10.
Prove that for every natural number $n≥4$ there are $l,m$ Natural ,so that $10n=20l+50m$ . How i try solve it: Basic step: n=1 
induction situation: n=k so 10k=20l+50m I name this (1)relation
Basic Induction :n=k+1 so 10(k+1)=20l+50m 
                          k+1=2l+5m
                          k=2l+5m-1 i name this (2) relation
In (1) relation i replace the k from (2) so i have
10(2l+5m-1 +1)=20l+50m 
10(2l+5m)=20l+50m",['discrete-mathematics']
3517955,Differential equation satisfying $f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}$,"Find the differential equation satisfying $$f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}$$ It is solved in my reference (also in this video ) as: $$
f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}=\frac{1}{1-\cos^2\theta}=\csc^2\theta\\
f'(\theta)=-2\csc^2\theta\cot\theta\\
f'(\theta)+2f(\theta)\cot\theta=0
$$ As per my knowledge the Leibniz rule is $$
\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}g(\theta,x)dx=g(\theta,b(\theta))\frac{d}{d\theta}b(\theta)-g(\theta,a(\theta))\frac{d}{d\theta}a(\theta)
$$ iff $g(x,\theta)=g(x)$ So isn't it wrong to approach the problem as in my reference ?","['integration', 'definite-integrals', 'ordinary-differential-equations']"
3517956,Cokernel within the Category of Groups,"This question is in regards to Aluffi's Algebra: Chapter $0$ , II. $8.22$ $\textbf{8.22: }$ Let $\varphi: G \rightarrow G'$ be a group homomorphism, and let $N$ be the smallest normal subgroup containing im $\varphi$ . Prove that $G'/N$ satisfies the universal property of ${\rm coker}~\varphi$ in $\textbf{Grp}$ . My scratch work/thinking: For $G'/N$ to satisfy the universal property of ${\rm coker}~\varphi$ there must be a group homomorphism \begin{equation*}
\pi: G' \rightarrow G'/N
\end{equation*} which is initial with respect to all morphisms $\alpha: G' \rightarrow L$ such that $\alpha \circ \varphi = 0$ . Thus, it follows that im $\varphi \subseteq \ker \alpha$ . Now, this is where I get lost. I think I understand that due to the universal property of quotients with respect to group homomorphisms results in the unique induced group homomorphism $\widetilde{\alpha}: G'/N \rightarrow L$ . Does the 'initial' role come from $N$ being the smallest normal subgroup which contains ${\rm im}~\varphi$ ? Side question: Precisely what does it mean for every homomrphism $\alpha: G' \rightarrow L$ , such that $\alpha \circ \varphi$ is the trivial map, must factor through $G'/N$ ?","['group-homomorphism', 'category-theory', 'universal-property', 'abstract-algebra', 'group-theory']"
3518063,"Find the inverse of f, where $f(x)= \frac {e^x - e^{-x}} {e^x + e^{-x}} $","I have tried letting $y= f(x)^{-1}$ So, f(y) = x ${\frac {e^y - e^{-y}}{e^y + e^{-y}}= x}$ Then, multiplying the top and bottom by $e^y$ , ${\frac {e^y(e^y - e^{-y})}{e^y(e^y + e^{-y})}= x}$ ${\frac {e^{2y} -1}{e^{2y} +1}= x}$ $e^{2y} -1 = x (e^{2y}+1)$ $e^{2y} -1= e^{2y}x + x$ $e^{2y} (1-x) = x+1$ Therefore, $e^{2y} = \frac {x+1}{1-x}$ $2y=ln (\frac {x+1}{1-x})$ $y= \frac{1}{2} ( \frac {x+1}{1-x})$ $f^{-1} (x) =\frac {1}{2} ln (\frac {1+x}{1-x})$ The answer for the inverse from the book is $f^{-1} (x) =\frac {1}{2} ln (\frac {1+x}{x-1})$ So, I can’t seem to get the positive sign for x in the denominator. There’s another very similar question in Maths Stack Exchange, but the positive and negative signs are different : What is the inverse of $f(x)=\frac{e^x+e^{-x}}{e^x-e^{-x}}$? Please help me solve this. Thank you","['functions', 'inverse']"
3518143,Why is it important that every module is a quotient of a free module?,"I'm wondering why it's important that every module is a quotient of a free module. In every text I've referred to, it seems to be an important theorem, but I'm failing to see how it's actually useful. If someone could provide some motivation behind this theorem, that would be very helpful.","['abstract-algebra', 'modules']"
3518178,Is there closed form for $\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ?$,Is it possible to compute $$\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ?$$ where $\overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k}$ is the alternating harmonic number and $H_n=\int_0^1\frac{1-x^n}{1-x}\ dx$ is the harmonic number. The reason I wrote the harmonic number in integral representation instead of series representation is due to the non-integer argument $n/2$ of the harmonic number and as we know $H_n=\sum_{k=1}^n\frac1k$ works for only integer $n$ . A similar version $\displaystyle\small\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^2}$ was computed here,"['integration', 'harmonic-numbers', 'calculus', 'closed-form', 'sequences-and-series']"
3518193,Maximal closed subscheme over which a line bundle is trivial,"The following statement comes from the book ""Abelian varieties"" by Mumford, at the very beginning of chapter 10. All varieties/schemes are defined over a field $k$ . Let $X$ be a complete variety, $Y$ any scheme and $\mathcal L$ a line bundle on $X\times Y$ . Then there exists a unique closed subschemes $Y_1 \hookrightarrow Y$ such that the restriction of $\mathcal L$ to $X\times Y_1$ is isomorphic to the pullback of a line bundle $\mathcal M$ on $Y_1$ (via the projection morphism) ; and such that $Y_1$ is maximal with respect to this property. This closed subscheme $Y_1$ is called the maximal closed subscheme of $Y$ over which $\mathcal L$ is trivial. Now, this may be a silly question, but to my understanding we usually call a line bundle trivial when it is isomorphic to the structure sheaf of the scheme. With this in mind, why wouldn't we require the condition ""the restriction of $\mathcal L$ to $X\times Y_1$ is isomorphic to $\mathcal O_{X\times Y_1}$ "" instead ? It doesn't seem equivalent as the line bundle $\mathcal M$ may not be trivial. Is there a specific reason for this ?","['algebraic-geometry', 'abelian-varieties', 'sheaf-theory']"
3518224,$\epsilon$-$\delta$ definition of the limit applied to nonlinear functions (teaching),"When I teach the $\epsilon$ - $\delta$ definition of the limit, I usually start with a linear function and a table of values to intuitive show the idea where the 'guesses' for $\delta$ in terms of $\epsilon$ is taken from. For example, $\displaystyle\lim_{x \to 3} (2x-4) = 2 $ , I use a table of values for $x = 3, 3.01, 3.1 ... $ with a corresponding $f(x) = 2, 2.02, 2.2, ...$ . Here we see that if the distance from $x$ is 0.01 (from 3 to 3.01), the corresponding 'distance' from $f(x)$ is 0.02. And so $\delta = \epsilon /2$ , which nicely fits in the proof. So my question is, when we move to quadratic functions, $\displaystyle\lim_{x \to 2} x^2 = 4.$ How do I use the same illustration to intuitively explain my choice for epsilon?","['limits', 'soft-question', 'education', 'epsilon-delta']"
3518285,Infinite dimensional vector space has almost complex structure if and only if it is 'even-dimensional'?,"I started studying the book of Daniel Huybrechts, Complex Geometry An Introduction. I tried studying backwards as much as possible, but I have been stuck on the concepts of almost complex structures and complexification . I have studied several books and articles on the matter including ones by Keith Conrad , Jordan Bell , Gregory W. Moore , Steven Roman , Suetin, Kostrikin and Mainin , Gauthier I have several questions on the concepts of almost complex structures and complexification. Here is one: I understand for a finite dimensional $\mathbb R-$ vector space $V=(V,\text{Add}_V: V^2 \to V,s_V: \mathbb R \times V \to V)$ , the following are equivalent $\dim V$ even $V$ has an almost complex structure $J: V \to V$ $V$ has a complex structure $s_V^{\#}: \mathbb C \times V \to V$ that agrees with its real structure: $s_V^{\#} (r,v)=s_V(r,v)$ , for any $r \in \mathbb R$ and $v \in V$ if and only if $V \cong \mathbb R^{2n} \cong (\mathbb R^{n})^2$ for some positive integer $n$ (that turns out to be half of $\dim V$ ) if and only if $V \cong$ (maybe even $=$ ) $W^2=W \bigoplus W$ for some $\mathbb R-$ vector space $W$ . The last condition makes me think that the property 'even-dimensional' for finite-dimensional $V$ is generalised by the property ' $V \cong W^2$ for some $\mathbb R-$ vector space $W$ ' for finite or infinite dimensional $V$ . Question: For $V$ finite or infinite dimensional $\mathbb R-$ vector space, are the following equivalent? $V$ has an almost complex structure $J: V \to V$ Externally, $V \cong$ (maybe even $=$ ) $W^2=W \bigoplus W$ for some $\mathbb R-$ vector space $W$ Internally, $V=S \bigoplus U$ for some $\mathbb R-$ vector subspaces $S$ and $U$ of $V$ with $S \cong U$ (and $S \cap U = \{0_V\}$ )","['complex-geometry', 'complex-analysis', 'abstract-algebra', 'linear-algebra', 'almost-complex']"
3518299,Restriction of $\mathcal O_X(D)$ to a prime divisor occuring in $D$,"Let $X$ be a regular variety over a field $k$ , and consider $D=\sum_{i=1}^nn_iD_i$ a divisor on $X$ , with $n_i\in \mathbb Z\backslash\{0\}$ and $D_i$ a prime divisor. Consider the invertible sheaf $\mathcal L = \mathcal O_X(D)$ . I would like to understand the restriction of $\mathcal L$ to one of the prime divisors $D_i$ . More precisely, I call restriction of $\mathcal L$ to $D_i$ the pullback of $\mathcal L$ via the closed immersion $D_i \hookrightarrow X$ , and denote it by $\mathcal L_{|D_i}$ . It is still an invertible sheaf over $D_i$ ; must it be isomorphic to some $\mathcal O_{D_i}(\tilde{D})$ where $\tilde D$ is a divisor on $D_i$ ? If so, how does $\tilde D$ relate to $D$ ? As a motivation, I am trying to understand the counter example given in page 3, (2.6) remark (ii) of these notes on abelian varieties (see the cropped picture below). Namely, I try to understand why the restriction of the line bundle considered by the author to $\{0\}\times X$ and $X\times \{0\}$ is indeed trivial.","['divisors-algebraic-geometry', 'algebraic-geometry', 'abelian-varieties']"
3518338,Primitive and derivative of little $o$,"For $x\in {\rm I\!R}$ in a neighborhood of $0$ and $n>1$ , we define the function $f(x) = o(x^n)$ and one of its primitives $F(x) = \int_0^x f(t)dt$ . Is it true that: $F(x) = o(x^{n+1})$ $f'(x) = o(x^{n-1})$ How to prove it? Here are my attempts: I need to prove that: $$
\lim_{x\rightarrow 0} \frac{F(x)}{x^{n+1}} = 0
$$ I tried to develop the fraction : $$
\frac{F(x)}{x^{n+1}} = \int_0^x \frac{f(t)}{x^{n+1}}dt
$$ But I couldn't go further...
A related question seems to add a condition to get this true but I coundn't really understand the comments. I would use L'Hôpital's rule : $$
\lim_{x\rightarrow 0} \frac{f'(x)}{x^{n-1}} = \lim_{x\rightarrow 0} \frac{f(x)}{\frac{x^n}{n}} = 0 
$$ Is it correct ?","['integration', 'derivatives', 'asymptotics', 'real-analysis']"
3518474,Which set of chords through a unit circle minimizes the largest demarcated area?,"Let $A_n$ be the minimum possible largest area inside a unit circle traced by $n$ lines. A circle can be divided into $\frac{n^2+n+2}2$ regions by tracing $n$ chords, so $A_n\geq\frac{2\pi}{n^2+n+2}$ is an easy lower bound. By making the chords all evenly spaced diameters we get $A_n\leq \frac \pi {2n}$ but I get the feeling that this is very loose and $A_n$ will be closer to the lower bound for large n. For $n=1,2$ the lower bound is easily achieved, but for $n=3$ it is difficult to beat the upper bound $\frac\pi 6$ . In a symmetrical arrangement like this the area of the three large outside sections is always $\geq\pi/6$ , so if there's a better arrangement it must not have triangular symmetry. In fact I think this is the case for all $n\geq 3$ . So what can be said about $A_n$ ? I conjecture that $$\lim_{n\to\infty}n^2A_n<\infty$$ But beyond that I find it difficult to determine anything about it.","['optimization', 'circles', 'geometry', 'euclidean-geometry']"
3518495,Check if a general point is inside a given cylinder,"For a particular purpose, I want to define a cylinder in 3D space and go through a list of given 3D points and tell if the point is inside or outside the cylinder volume.
I can define the cylinder by specifying 2 points along the axis, and the radius of the cylinder. A (x1, y1, z1 )
B (x2, y2, z2 )
and radius = R right now what I'm doing is that I find the vector AB, connecting A and B by AB = A - B then calculate the shortest distance from each point to the vector AB, if the distance is less than R, the point is inside. The problem with this method is that it only works if either A or B is the origin. for example, If I try to find the points inside the cylinder connecting p1 ( 100,10,20)
p2 ( 100,-10,20) we get the points inside the cylinder ( 0,20,0) [ which is actually the cylinder formed by ( 0,0,0) and (0,20,0) ] certainly, I'm missing something, can anyone point it out? N.B: For some complicated reason, I can't use an auxiliary coordinate system or shift the origin. What I'm looking for is some pure mathematical expression ( if it exists ), which can take the particulars of the cylinder and the required point and give if it is inside or outside. similar to Empty2 's answer on this question","['euclidean-geometry', 'geometry']"
3518507,Minimum number of books to fulfill a condition,"There is a group of 100 Readers who come together every month to discuss their findings from the books they have read. They discuss in a group of two people. In order to start a discussion between two people, each individual should have read at least one book that the other person hasn't. What is the minimum number of distinct books (across all people) that are needed to achieve the condition for any two random members of the group to start a discussion?","['combinatorics', 'extremal-combinatorics']"
3518531,Third order linear differential equation and Painlevé II solution,"Consider the solution to the Painlevé II equation on $\mathbb{R}$ $$q''=2q^3+rq$$ with the boundary condition $q(r)\sim_{r\to +\infty} \mathrm{Ai}(r)$ and consider the function $f$ such that for $r\in \mathbb{R}$ , $$f(r)=\cosh\left(\frac{1}{2}\int_r^{+\infty} \mathrm{d} s \, q(s)\right)$$ We have that $f$ verifies the third order differential equation $$f'''+P(r) f' +Q(r) f =0 \qquad \qquad (\mathcal{E})$$ with $$\begin{cases}
P(r)=-\frac{9}{4}q(r)^2-r\\
Q(r)=\frac{1}{6}P'(r)+\frac{1}{6}
\end{cases}$$ My question is the following, if one is given the differential equation $(\mathcal{E})$ with the proper boundary condition, is there a standard technique / change of variable to find back the expression of the function $f$ ?","['nonlinear-analysis', 'ordinary-differential-equations']"
3518538,"Compute: $\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R$","Compute: $$\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha
 x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R$$ My attempt: $$L=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2}=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-1+1-\sqrt[n]{\cos(\beta x)}}{x^2}\\\displaystyle=\lim_{x\to 0}\frac{\left(1+(\cos(\alpha x)-1)\right)^{\frac{1}{m}}-1-\left(\left(1+(\cos(\beta x)-1)\right)^{\frac{1}{n}}-1\right)}{x^2}$$ Transformed expression: $$-\frac{(1+(\cos(\alpha x)-1))^{\frac{1}{m}}-1}{\cos(\alpha x)-1}\cdot\frac{1-\cos(\alpha x)}{\alpha^2x^2}\alpha^2+\frac{(1+(\cos(\beta x)-1))^{\frac{1}{n}}-1}{\cos(\beta x)-1}\cdot\frac{1-\cos(\beta x)}{\beta^2x^2}\beta^2$$ I applied the standard limits from the table: $$\lim_{x\to 0}\frac{(1+x)^a-1}{x}=a\;\&\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2}$$ $$L=-\frac{1}{m}\cdot\frac{1}{2}\alpha^2+\frac{1}{n}\cdot\frac{1}{2}\beta^2=\frac{m\beta^2-n\alpha^2}{2mn}$$ Is this correct?","['limits', 'calculus', 'solution-verification', 'limits-without-lhopital']"
3518592,Prove $\frac{X_n}{n} \to 0$ almost surely when $E|X_n| < \infty$,"How can we combine two following facts (1) $E|X| < \infty \iff \sum_{n=1}^{\infty}\mathbb{P}(|X| \geq n) \text{ converges}$ (2) $\forall_{\epsilon > 0} \sum_n \mathbb{P}(|X_n| > \epsilon) \text{ converges} \implies X_n \to 0 \text{ almost surely}$ to prove: If $E|X| < \infty$ and $X_n$ is sequence of random variables with same distribution as X then $\frac{X_n}{n} \to 0$ almost surely? I have been trying to wrap my mind around this, but can't find the correct proof. This is probably a one-liner solution though...","['probability-theory', 'probability']"
3518657,Can we find a simple basis for the cokernel of this derivation?,"Let $K$ be a field of characteristic zero. Let $R$ be the $K$ -algebra $K[x_0,x_1,\ldots]$ of polynomials in countably infinitely many variables.
Consider the $K$ -linear derivation $\delta:R\to R$ following the Leibniz rule $\delta(ab)=\delta(a)\cdot b+a\cdot \delta(b)$ for all $a,b\in R$ as well as the rules $\delta(x_i)=x_{i+1}$ for all $i=0,1,2,\ldots$ . Can we find a nice basis (= a basis consisting of cosets of some monomials) for the cokernel $R/\delta(R)$ ? A physicist friend asked me this question. I thought about it for a while, and made the following observation. Consider a monomial $M=\prod_{j=0}^n x_j^{a_j}$ with $a_n>0$ . Then (w.r.t. the ordering where the highest appearing index dominates) the leading term of $\delta(M)$ has the form $a_n(M/x_n)x_{n+1}$ . Observe that the exponent of $x_{n+1}$ is necessarily $1$ . This lead us to make a conjecture: Let $S$ be the set of monomials $M=\prod_{j=0}^n x_j^{a_j}, a_j\in\Bbb{N}$ for all $j$ , $n=0,1,2,\ldots$ , $a_n>1$ together with all the monomials $x_0^i, i=0,1,\ldots$ . Then the cosets $p+\delta(R)$ , $p\in S$ , form a $K$ -basis of the cokernel $R/\delta(R)$ . The intuition is, of course, that the monomials with highest index exponent $a_n=1$ can be reduced modulo $\delta(K[x_0,x_1,\ldots,x_{n-1}])$ in view of the first observation. Questions. Is this conjecture true? Or (well) known in an appropriate context? Failing that, is there another easy to describe basis for the cokernel? What buzzwords should we use when searching for more information on this theme? Asking before I commence an effort to check whether the intuition holds water, and leads to a proof. The problem feels very natural, and I suspect the answer is known already.","['abstract-algebra', 'differential-algebra']"
3518684,Converting a matrix differential to a derivative,"I would like to write down the update rule for a set of parameters in a neural network, which minimizes a loss function that I think is general enough to be instructive for others. Let $\Phi \in \mathbb{R}^{l \times m \times n}$ be a $l \times m \times n$ tensor of learnable parameters and $\mathscr{L(\Phi)}$ be a scalar loss function of those parameters to be minimized: $$\mathscr{L} = \beta\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{n}|\Phi_{i}^{\top}\Phi_{i} - \mathbb{I}_{\text{n}}|_{jk},$$ where $|\cdot|$ is element-wise absolute value, $\beta$ is some scalar constant, $\Phi_{i}$ is a $l \times n$ matrix, and $\mathbb{I}_{\text{n}}$ is the $n \times n$ identity matrix.
I would like to know the derivative of this loss with respect to an $l$ -dimensional vector: $\frac{\partial \mathscr{L}}{\partial \Phi_{ab}}$ , where $a$ and $b$ index the $m$ and $n$ dimensions of $\Phi$ , respectively. Following the chain rule described in chapter 18 from the Matrix Differential Calculus book by Magnus and Neudecker , I can use differentials to get most of the way there.
Specifically, I can modify example 18.6a to let $F(X) = |X^{\top}X|$ for some $X \in \mathbb{R}^{l \times n}$ , where again $|\cdot|$ is absolute value, not determinant.
Then, \begin{align}
\text{d}F &= \text{d}|X^{\top}X| \\
&= \frac{X^{\top}X}{|X^{\top}X|} \text{d}(X^{\top}X) \\
&= \frac{X^{\top}X}{|X^{\top}X|} (\text{d}X)^{\top}X + \frac{X^{\top}X}{|X^{\top}X|} X^{\top} \text{d}X \\
&= 2 \frac{X^{\top}X}{|X^{\top}X|} X^{\top}\text{d}X
\end{align} The book also provides an identification theorem for connecting differentials to derivatives: $$\text{d} \text{vec}F = A(X) \text{d} \text{vec}X \iff \frac{\partial\text{vec}F(X)}{\partial(\text{vec}X)^{\top}} = A(X),$$ where $\text{vec}$ is the matrix vectorization operator .
I believe I can now use the chain rule to get close to my desired derivative if I set $F=|X^{\top}X-\mathbb{I}_{\text{n}}|$ and $X=\Phi_{i}$ : \begin{align}
\frac{\partial\mathscr{L}}{\partial(\text{vec}\Phi_{i})^{\top}} &= \frac{\partial\mathscr{L}}{\partial\text{vec}F} \frac{\partial\text{vec}F}{\partial(\text{vec}\Phi_{i})^{\top}} \\
&= \frac{\partial\mathscr{L}}{\partial\text{vec}F} 2 \frac{\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}}{|\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}|} \Phi_{i}^{\top}
\end{align} I do not know how to get from this point to a partial derivative with respect to a single vector, $\Phi_{ab}$ .
I would guess that almost all of the entries from the sums in $\mathscr{L}$ will be zero for $\frac{\partial \mathscr{L}}{\partial \Phi_{ab}}$ .
I think I can use this to my advantage, which I think would mean multiplying the above derivative by $\delta_{ia}\delta_{jb}\delta_{kb}$ , but this is where I am less sure. I also used this blog post as a resource.
My question is very similar to this one , and also related to this one , this one , and this one , although I was not able to get to an answer from those posts.","['differential', 'tensors', 'matrices', 'matrix-calculus', 'derivatives']"
3518685,Inhomogeneous Cauchy-Euler equation: $x^2y''(x)+xy'(x)-y(x)=x$ for $x>0$,"I am trying to solve this DE (for $x>0$ ) by a substitution $x=e^t$ : $$x^2y''(x)+xy'(x)-y(x)=x \implies \color{blue}{e^{2t}y''(e^t)+e^ty'(e^t)}-\color{green}{y(e^t)}=e^t$$ Let $z(t)=y(x)$ and let $x=e^t \implies \color{green}{z(t)}= y(e^t)$ By the chain rule we get: $z'(t)=y'(e^t)e^t$ $\color{blue}{z''(t)}=y''(e^t)e^{2t}+y'(e^t)e^{t}$ The green and blue part can be substituted into the equation and I get: $$z''(t)-z(t)=e^t$$ The solution to the homogeneous equation $z''-z=0$ is: $$z_h(t)=c_1 e^t+c_2e^{-t}$$ By choosing a suitible ""ansatz"" $z_s(t)=Ate^t$ , I get the special solution to the inhomogeneous equation $z''-z=e^t$ : $$z_s(t)=\frac{1}{2}te^t$$ Therefore the general solution to the inhomogeneous equation is: $$z(t)=c_1 e^t+c_2e^{-t}+\frac{1}{2}te^t$$ Because I made the substitution $x=e^t$ I need to substitute $t=\ln{x}$ $$\implies y(x)=c_1e^{\ln{x}}+c_2e^{-\ln{x}}+\frac{1}{2}(\ln{x})e^{\ln{x}}\\=c_1 x+\frac{c_2}{x}+\color{red}{\frac{1}{2}x\ln{x}}$$ Here is my problem: The red part $\frac{1}{2}x\ln{x}$ doesn't solve the differential equation and I am not sure where I have gone wrong. Wolfram alpha also gives me the same (real) solution the homogeneous equation but the general solution looks like this: Where am I making a mistake?",['ordinary-differential-equations']
3518696,Cat quiz (to solve with the determinant),"Consider $100$ cats and $100$ food bowls containing
  cat food of $100$ different
  brands. Every cat
  likes an odd amount of brands. For each two
  cats, there is
  an even amount
  of brands both
  cats like. Show that one can
  distribute the $100$ food
  bowls
  to the $100$ cats such that every cat is happy. Hint :
  It's a
  determinant
  exercise
  over
  the field $\mathbb{F}_2 = 
\mathbb{Z}/2\mathbb{Z}=
\{
[0],[1]
\}$ . How can this be done using
the determinant? Thanks in advance!","['matrices', 'finite-fields', 'determinant', 'linear-algebra']"
3518743,"If $π$ is the projection of a surface $M$ onto the 2-sphere $S^2$, then $σ_{S^2}(B)=\int_{π^{-1}(B)}σ_M(dy)\frac{|⟨ν_M(y),π(y)⟩|}{|y|^2}$","Let $U\subseteq\mathbb R^2$ be open, $\phi:U\to\mathbb R^3$ be an immersion and a topological embedding of $U$ onto $M:=\phi(U)$$^1$ , $\nu_M(x)$ denote the unit normal vector of $M$ with $$\det\left({\rm D}\phi(u),\nu_M(x)\right)>0\tag1$$ for all $x\in M$ and $u=\phi^{-1}(x)$$^2$ , $\sigma_M$ denote the surface measure on $M$$^3$ and $$\pi:\mathbb R^3\setminus\{0\}\to S^2\;,\;\;\;x\mapsto\frac x{|x|}$$ denote the projection of $\mathbb R^3\setminus\{0\}$ onto the unit 2-sphere $S^2$ . Assume $\left.\pi\right|_M$ is injective. How can we show that $$\sigma_{S^2}(B)=\int_{\pi^{-1}(B)}\sigma_M({\rm d}y)\frac{\left|\langle\nu_M(y),\pi(y)\rangle\right|}{|y|^2}\tag5$$ for all $B\in\mathcal B(S^2)$ with $B\subseteq\pi(M)$ ? $^1$ which is to say that $M$ is a 2-dimensional embedded submanifold of $\mathbb R^3$ with global chart $\phi$ . $^2$ i.e. $$\nu_M\circ\phi=\frac{\partial_1\phi\times\partial_2\phi}{\left|\partial_1\phi\times\partial_2\phi\right|}\tag2.$$ $^3$ i.e. $$\sigma_M=\sqrt{g_\phi}\left.\lambda^2\right|_U\circ\phi^{-1}\tag3,$$ where $J_\phi$ is the Jacobian of $\phi$ and $$g_\phi:=\det\left(J_\phi^TJ_\phi\right)=\left|\partial_1\phi\times\partial_2\phi\right|^2\tag4.$$","['measure-theory', 'submanifold', 'surfaces', 'geometric-measure-theory', 'differential-geometry']"
3518768,Intersections of circles drawn on vertices of regular polygons,"Using only a compass, draw all possible circles on the vertices of a regular $n$ -sided polygon. (That is, in every ordered pair of vertices one is the center, and their distance is the radius.) How many intersections are there? Let $a(n)$ be the intersection count for given $n\in\mathbb N$ . First three terms $a(1),a(2),a(3)=0,2,6$ are simple. The next three terms are: Notice that the circle set (given by a $n$ -sided polygon) can be split into $n$ symmetric regions. Let $A_n$ count intersections inside one of the $n$ regions. Let $\delta_n\in\{0,1\}$ compensate for when there is one extra central intersection. This implies we can write every term as: $$a(n)=nA_n+\delta_n$$ The first $20$ terms should be (constructed in GeoGebra): $$\begin{array}{}
a(1) &= \space\space0  &= \space\space1\cdot0       \\
a(2) &= \space\space2  &= \space\space2\cdot1       \\
a(3) &= \space\space6  &= \space\space3\cdot2       \\
a(4) &= 40 &= \space\space4\cdot10      \\
a(5) &= 55 &= \space\space5\cdot11       \\
a(6) &= 145&= \space\space6\cdot24  +  1 \\
a(7) &= 238&= \space\space7\cdot34       \\
a(8) &= 584&= \space\space8\cdot73      \\
a(9) &= 612&= \space\space9\cdot68      \\
a(10) &= 1350&= 10\cdot135       \\
a(11) &= 1804&= 11\cdot164       \\
a(12) &= 2401&= 12\cdot200+1       \\
a(13) &= 3523&= 13\cdot271      \\
a(14) &= 5180&= 14\cdot370      \\
a(15) &= 6150&= 15\cdot410       \\
a(16) &= 9312&= 16\cdot582      \\
a(17) &= 11101&= 17\cdot653       \\
a(18) &= 13645&= 18\cdot758+1      \\
a(19) &= 17746&= 19\cdot934       \\
a(20) &= 22300&= 20\cdot1115 \\
\dots       
\end{array}$$ We can notice that it seems $\delta_n=1$ if and only if $n$ is a multiple of $6$ . Are there any other patterns? Is it possible to find a closed form for $a(n)$ ? My attempt: WLOG, Let $V_1,V_2,\dots,V_n$ be vertices of a regular $n$ -sided polygon with circumradius $1$ . We can take $V_i=(x_i,y_i)=(\cos(\frac{2i\pi}{n}),\sin(\frac{2i\pi}{n})),i=1,2,\dots,n$ . The $k$ -th diagonal from some vertex $V$ of the polygon will have length $2\sin(\frac{k\pi}{n})$ . At each vertex $V$ , we will have $c=1,2,\dots,\left\lfloor\frac{n}{2}\right\rfloor$ circles $^{[1]}$ with radii $r_c=2\sin(\frac{c\pi}{n})$ . $$
(i,c)\text{-Circle}\dots\space\space \left(x-\cos\frac{2i\pi}{n}\right)^2+\left(y-\sin\frac{2i\pi}{n}\right)^2=\left(2\sin\frac{c\pi}{n}\right)^2
$$ Is it possible to derive a closed form for the number of intersections from this? I believe I managed to solve a simpler problem: ""At each vertex $V$ , consider only one circle of radius $r$ ."" Then the number of such intersections $I(n,r)$ should be: $$
I(n,r)=\begin{cases}
(n-1)n, & r \gt 1\\
(n-1)n - n\left\lfloor\frac{n}{2}\right\rfloor+1, & r=1\\
(n-2)n, & \sin(\frac{\left(\frac{n}{2}-1\right)\pi}{n})\lt r\lt\sin(\frac{\pi}{2})=1\\
(n-3)n, & r = \sin(\frac{\left(\frac{n}{2}-1\right)\pi}{n})\\
\dots & \dots \\
(2k)n, & \sin(\frac{k\pi}{n}) \lt r \lt \sin(\frac{(k+1)\pi}{n})\\
(2k-1)n, & r = \sin(\frac{k\pi}{n})\\
\dots & \dots \\
4n, & \sin(\frac{2\pi}{n}) \lt r \lt \sin(\frac{3\pi}{n})\\
3n, & r = \sin(\frac{2\pi}{n})\\
2n, & \sin(\frac{\pi}{n}) \lt r \lt \sin(\frac{2\pi}{n})\\
1n, & r = \sin(\frac{\pi}{n})\\
0, & r \lt \sin(\frac{\pi}{n})
\end{cases}
$$ This solves the problem of intersections for any $r\in\mathbb R_{+}$ but for only one layer of circles. In the original problem, we have $\left\lfloor\frac{n}{2}\right\rfloor$ layers of circles with different radii on each layer. The radii of circles between layers have specific ratios (determined by $n$ ): radii are diagonals of the regular $n$ -sided polygon. My idea was to use $I(n,r_c),c=1,2,\dots,\left\lfloor\frac{n}{2}\right\rfloor$ to get to $a(n)$ . But, I get lost when trying to add and subtract all of the unique and duplicate intersections. How can we solve the original problem and find $a(n)$ ?","['combinatorics', 'geometry', 'circles', 'polygons']"
3518782,Angles do not add up to $180^\circ$,"In $\triangle ABC$ , $\alpha = 35^\circ$ , side $b = 10$ and side $c = 4$ . (i) Show that the length of side $a$ is $7.10$ (ii) Find the remaining angles $\beta$ and $\gamma$ I used the cosine rule and found the $7.10$ , but I got $53.89^\circ$ for angle $\beta$ and $18.85^\circ$ for angle $\gamma$ , which don't add to make $180^\circ$ .",['trigonometry']
3518786,What is the difference between Lipschitz and general uniform continuity?,"Showing that Lipschitz continuity is a subset of uniform continuity isn't hard. Let's assume that it is true that $|f(x_1) - f(x_2)| \leq K(x_1-x_2)$ given an arbitrary value $\epsilon > 0$ let's define $\delta = \epsilon / K$ Then trivially if $|x_1 - x_2| \leq \delta$ : $|f(x_1) - f(x_2)| \leq K\cdot|x_1 - x_2| \leq K\cdot\delta \leq K\cdot\epsilon/K \leq \epsilon$ . Which satisfies the uniform continuity property that for every $\epsilon$ there is a $\delta$ such that $|x_1-x_2| \leq \delta \implies |f(x_1) - f(x_2)| \leq \epsilon$ However, I am having a hard time trying to understand why not all uniformly continuous functions obey this property. I would appreciate a general explanation with at least one example of a non  Lipschitz continous function that is uniformly continuous. Edit: Although I admit I had not found the answer that is linked. That answer, although very good, still does not suffice. The author of the question provided multiple functions that are not Lipshitz continuous, but did not prove why they are not Lipshitz continuous (although he did prove that they are uniformly continuous). My issue is not an absence of an example, but rather a lack of intuition to understand the difference between both.","['metric-spaces', 'real-analysis']"
3518827,Sufficiency for AR(1) model,"Consider the following AR(1) model: $$X_1=\epsilon_0\,,\,X_t=\rho X_{t-1} + \epsilon_t\,,$$ where $t=2,3\ldots,n$ and $\epsilon_t \sim N(0 , \sigma^2)$ independently. It is given that $|\rho| <1$ . Let $X_1,X_2,\ldots,X_n$ be a random sample drawn from this model. Find the minimal sufficient statistic for this model. The thing is I cannot write the joint distribution of $X_i$ 's for this model explicitly as it is necessary to determine whether it belongs to the exponential family. Please help.","['time-series', 'statistical-inference', 'statistics', 'probability-distributions']"
3518837,Formally smooth fiberwise criterion over a DVR,"Question Let $f: X \to S$ be a flat morphism of schemes where $S$ is a DVR with algebraically closed residue field, special fiber is smooth, and generic fiber is formally smooth Is $f$ a formally smooth morphism? If the above is not true in general, is there are an extra condition that can be imposed that $f$ is formally smooth? Motivation I am trying to understand the proof of Proposition 5.2. in Yoshida's paper On non-abelian Lubin-Tate theory via vanishing cycles Link . To set up some notations let $K$ be a finite extension of $\mathbb{Q}_p$ and $W$ be the ring of integers of the completion of the maximal unramified extension of $K$ . Now let $Y$ be the spectrum of the representing algebra of a moduli functor defined by $\{$ Complete Noetherian Local $W$ -algebra with residue field isomorphic to that of $W$ } to $\{$ Deformations of formal $\mathcal{O}_K$ -module of height $n$ over the residue field of $W$ with Drinfeld level $\mathfrak{p}$ -structure $\}$ Then $X$ is obtained by first blowing up $Y$ at the unique closed point, pick an affine cover of the blow-up, complete along the exceptional divisor, and invert some equations, and pick a closed subscheme that eventually becomes normalization. I am being unprecise because it will take too much to describe the process. Just in case it helps, $X$ is the spectrum of $$ W_n[V_1,\ldots, V_n] \left [ \frac{1}{V_n} \right ][[X_n]]/(\omega_n - V_n X_n, u \cdot \prod_{a \in k^n \backslash \{ 0 \}} P_a - V_n^{q^n-1}) $$ where $k$ is the residue field of $K$ , $P_a$ is a power series induced from the universal deformation of the moduli functor mentioned above. I apologize for being imprecise here also. Here $W_n$ is the ring of integers of $K(\omega_n)$ where $\omega_n=\omega^{1/(q^n-1)}$ . And our $S = \mathrm{Spec}~W_n$ . I think I can find an argument if $X$ is a spectrum of a local ring, but as of right now, I feel like the above is not a local ring.","['number-theory', 'algebraic-geometry']"
3518983,Pseudocomplete spaces,"It is well-known that both locally compact Hausdorff spaces and completely metrizable spaces are Baire (i.e. satisfiy the Baire category theorem). A common generalization of both classes while still being Baire is the class of Čech-complete spaces. A finite product of Baire spaces does not have to be Baire; any product of Čech-complete spaces is Baire but not necessarily Čech-complete. In 1960 Oxtoby defined the class of pseudocomplete spaces. This is a nice property that captures the idea of the proof of the Baire category theorem. It is easy to see that every Čech-complete space is pseudocomplete, that every pseudocomplete space is Baire, and that every product of pseudocomplete spaces is pseudocomplete. In the proof of the last fact there is a small issue for me, but first the definitions: A topological space is quasiregular if every nonempty open set contains the closure of a nonempty open set. A family of nonempty open sets $\mathcal{B}$ is called a π-base (or pseudobase , but this name collides with the notion of pseudobase related to pseudocharacter ) if every nonempty open set contains a member of $\mathcal{B}$ . A topological space is pseudocomplete if it is quasiregular and there exist a sequence $\{\mathcal{B}_n: n ∈ ω\}$ of π-bases such that every sequence $B_0 ⊇ \overline{B_1} ⊇ B_1 ⊇ \overline{B_2} ⊇ B_2 ⊇ \cdots$ such that each $B_n ∈ \mathcal{B}_n$ has nonempty intersection. In the proof of the productivity of pseudocomplete spaces it is needed that even sequences $B_{n_0} ⊇ \overline{B_{n_0 + 1}} ⊇ B_{n_0 + 1} ⊇ \overline{B_{n_0 + 2}} ⊇ B_{n_0 + 2} ⊇ \cdots$ where $B_n ∈ \mathcal{B}_n$ for $n ≥ n_0$ have nonempty intersection (i.e. we may start later in the sequence of $π$ -bases). Oxtoby says that we may assume that $X$ is in every $\mathcal{B}_n$ , but I do not see why it is the case. It seems that simply adding $X$ to the π-bases may destroy the property of the sequence. On the other hand, what seems to work is to remove some members of the π-bases. Namely, we put $\mathcal{B}'_0 := \mathcal{B}_0$ and $\mathcal{B}'_{n + 1} := \{B ∈ \mathcal{B}_{n + 1}: ∃C ∈ \mathcal{B}'_n: \overline{B} ⊆ C\}$ . These will still be π-bases and for every $B_{n_0} ∈ \mathcal{B}'_{n_0}$ there is $B_{n_0 -1} ∈ \mathcal{B}'_{n_0 - 1}$ such that $B_{n_0 - 1} ⊇ \overline{B_{n_0}} ⊇ B_{n_0}$ and so on, i.e. we may extend the sequence at the beginning. This modified sequence satisfies the property we need and we may even add $X$ to these π-bases (unless $X = ∅$ ). But the argument seems to be nontrivial for a comment like “we may assume that $X ∈ \mathcal{B}_n$ ”. So I wonder, am I missing something? Updated: Let me give a concrete example of the situation. Let's write $U \prec V$ for $\overline{U} ⊆ V$ . Let $\{\mathcal{B}_n: n ∈ ω\}$ be a pseudocompleteness-witnessing sequence for $(0, 1)$ such that every member of $\mathcal{B}_n$ has diameter $< 2^{-(n + 2)}$ , let $A_n := (0, 2^{-n})$ , and let $\mathcal{B}'_n := \mathcal{B}_n ∪ \{A_n\}$ . The sequence $\mathcal{B}_0, \mathcal{B}'_1, \mathcal{B}'_2, …$ still witnesses that $(0, 1)$ is pseudocomplete since every compatible $\prec$ -decreasing sequence $\{B_n: n ∈ ω\}$ is in fact compactible with the original system $\{\mathcal{B}_n: n ∈ ω\}$ : if $B_{n + 1} = A_{n + 1}$ , then $B_n = A_n$ since $A_n$ is the only member of $B'_n$ containing $A_{n + 1}$ , but $A_0 = X ∉ \mathcal{B}_0$ . On the other hand, if we add $X$ to $\mathcal{B}_0$ , i.e. if we consider the system $\{\mathcal{B}'_n: n ∈ ω\}$ , then we have the compatible sequence $A_0 \succ A_1 \succ A_2 \succ \cdots$ with empty intersection.","['general-topology', 'baire-category']"
3519102,Asymptotic expansion of the Volterra integral equation with series ansatz,"Problem I have a problem which can be boiled down to the Volterra integral equation $$
\begin{aligned}
w(\eta) &\sim \eta \int _0^\infty K(s)F(\eta s)ds
\end{aligned}
$$ for $\alpha \in (\frac 12 , 1)$ , with kernel and non-linearity defined as $$
K(s) := \frac{1}{\Gamma(\alpha)}(1-s)^{\alpha-1} \theta(1-s) \qquad F(\eta s) := (\eta s + \eta_0)^{-1-\alpha} (c_0 + c_1w(\eta s) + c_2 w(\eta s)^2)
$$ where $\theta(s)$ is Heaviside step function is $\eta_0>0$ is a positive constant. It is assumed that for our choice of coefficients $c_0, c_1, c_2$ the function $w(\eta)\rightarrow \infty$ as $\eta \rightarrow \infty$ . The equation cannot be solved explicitly, the goal is to obtain the asymptotic expansion of $w(\eta)$ as $\eta \rightarrow \infty$ . My attempt Recall the definition of the Mellin transform $$
\mathrm M [f(s); z] = \int _0^\infty s^{z-1} f(s) ds.
$$ Once formulated as above, we can use Parseval formula for the Mellin transform and rewrite the problem as $$
w(\eta) \sim \frac{\eta}{2 \pi i} \int_{c-i \infty}^{c+i \infty} \mathrm{M}[K(s) ; 1-z] \mathrm{M}[F(\eta s) ; z] d z
$$ whereas $\Re (c)$ is in the analiticity strip of both Mellin transforms. Ansatz We now make an ansatz $$
w(\eta) = \sum_{k=1}^{-\infty} d_k \eta^{\alpha k} = d_1 \eta^\alpha + d_0 + d_{-1} \eta ^ {-\alpha} + \dots = d_1 \eta^\alpha + d_0 + O(\eta^{-\alpha})
$$ The idea is to calculate Mellin transforms in the above integral and match the coefficients to calculate $d_1$ and $d_0$ . Mellin transform of the kernel $$
M[K(s); 1-z] = \frac{\Gamma(1-z)}{\Gamma(1+\alpha-z)} = - \sum _{n=0} ^\infty \frac{(-1)^n}{\Gamma(\alpha-n) n!} \left(\frac{1}{z-(n+1)}\right).
$$ It has poles at $z = 1, 2, 3, \dots$ and hence analytical for $\Re (z) < 1$ . Mellin transform of the non-linearity If we plug in $w(\eta)$ from ansatz above into the non-linearity $F(\eta s)$ , we obtain $$
\begin{aligned}
F(\eta s) &= (\eta s + \eta _0)^{-1-\alpha}\left\{c_2 d_1^2  (\eta s)^{2\alpha} + (2 c_2 d_0d_1 + c_1 d_1)(\eta s)^{\alpha}\right\} + O((\eta s)^{-1-\alpha} ) \\
&\sim c_2 d_1^2  (\eta s)^{-1-\alpha} + (2 c_2 d_0d_1 + c_1 d_1)(\eta s)^{-1} + O((\eta s)^{-1-\alpha} )
\end{aligned}
$$ Mellin transforms of the first two terms are $$
\begin{aligned}
M\left[c_2 d_{1}^{2} (\eta s)^{-1+\alpha}; z\right] &= (c_2 d_{1}^{2}) \frac{\eta ^{-z}}{z-(1-\alpha)}\\
M\left[(2 c_2 d_0d_1 + c_1 d_1) (\eta s)^{-1}; z\right] &= (2 c_2 d_0d_1 + c_1 d_1) \frac{\eta ^{-z}}{z-1}.
\end{aligned}
$$ In order for the Mellin transforms to exist, we require $z < 1-\alpha$ . Hence $\mathrm{M}[F(\eta s) ; z]$ is analytic for $\Re(z) < 1-\alpha$ . So we see that each term in the expansion of $F(\eta s)$ leads to the pole of $\mathrm{M}[F(\eta s) ; z]$ at $z = 1-\alpha, 1, 1+\alpha, \dots$ . Shifting the contour to the right Recall that using Parseval formula we have rewritten our problem as $$
w(\eta) \sim \frac{\eta}{2 \pi i} \int_{c-i \infty}^{c+i \infty} \mathrm{M}[K(s) ; 1-z] \mathrm{M}[F(\eta s) ; z] d z.
$$ Now that we obtained analiticity strips of the Mellin transforms above, we see it is required that $\Re (c) < 1 - \alpha$ . The idea now is to shift the integration contour to the right and use Cauchy integral formula $$
f^{(n)}(a)=\frac{n !}{2 \pi i} \oint_{\gamma} \frac{f(z)}{(z-a)^{n+1}} d z
$$ to calculate the integrals around the poles. Assume at this point that we can shift the contour to the right. We plug in the Mellin transforms calculated above and shift the contour to the right so that $u \in (1, 1+\alpha)$ to obtain first two terms in the integral expansion: $$
\begin{aligned}
d_1 \eta ^\alpha + d_0 + \dots &\sim \frac{\eta}{2 \pi i} \int_{\gamma_1} \left( \frac{\Gamma(1-z)}{\Gamma(1+\alpha-z)}\right) \left(c_{2} d_{1}^{2}\right)  \frac{\eta^{-z}}{z-(1-\alpha)} \\
&+ \frac{\eta}{2 \pi i} \int_{\gamma_0} \left( \frac{\Gamma(1-z)}{\Gamma(1+\alpha-z)}\right) \left(2 c_{2} d_{0} d_{1}+c_{1} d_{1}\right) \frac{\eta^{-z}}{z-1} \\
&+ \frac{\eta}{2 \pi i} \int_{u - i\infty} ^{u + i \infty} \mathbf{M}[K(s) ; 1-z] \mathbf{M}[F(\eta s) ; z] d z
\end{aligned}
$$ such that $\gamma_1$ encloses $z=1-\alpha$ and $\gamma_0$ encloses $z=1$ . First coefficient $d_1$ The contour integral around $z = 1-\alpha$ can be calculated with Cauchy integral formula $$
\frac{\eta}{2 \pi i} \int_{\gamma_1} \left(c_{2} d_{1}^{2}\right) \left( \frac{\Gamma(1-z)}{\Gamma(1+\alpha-z)}\right) \frac{\eta^{-z}}{z-(1-\alpha)} = c_2 d_1^2 \frac{\Gamma(\alpha)}{\Gamma(2\alpha)} \eta ^\alpha
$$ Matching the first coefficient leads to $$
d_1 \eta ^\alpha = c_2 d_1^2 \frac{\Gamma(\alpha)}{\Gamma(2\alpha)} \eta ^\alpha
\qquad \text{and thus} \qquad
d_1 = \frac{\Gamma(2\alpha)}{c_2 \Gamma(\alpha)}
$$ Second coefficient $d_0$ The problem that arises now is that both kernel and non-linearity have pole at $z=1$ . Hence the integrand has the pole of order 2 and Cauchy integral formula will involve derivative of $\eta ^{-z}$ , which will yield the term $\log (\eta)$ : $$
\begin{array}{l}
\frac{\eta}{2 \pi i} \int_{\gamma_0} \frac{\Gamma(1-z)}{\Gamma(1+\alpha-z)} M[F(\eta s) ; z] d z \\
= \eta \cdot \text{Res}(\Gamma(1-z), 1) \cdot \text{Res}(M[F(\eta s); z], 1) \cdot \left. [f'(z)] \right| _{z = 1, f(z) = \frac{\eta^{-z}}{\Gamma(1+\alpha-z)}} \\
= (2c_2 d_{0} d_{1}+c_1 d_{1}) \frac{-\log (\eta) \Gamma(\alpha) + \Gamma'(\alpha)}{\Gamma(\alpha)^2}.
\end{array}
$$ Matching the coefficient $d_0$ leads to $$
d_0 = (2c_2 d_{0} d_{1}+c_1 d_{1}) \frac{\Gamma'(\alpha)}{\Gamma(\alpha)^2}.
$$ Since we know the $d_1$ , the above equation can be solved for $d_0$ . However , we are left with the residual term $- \frac{(2c_2 d_{0} d_{1}+c_1 d_{1}) \Gamma(\alpha)}{\Gamma(\alpha)^2} \log (\eta)$ of order $ \log (\eta)$ of order $ \log (\eta)$ , which cannot be matched with anything on the left-hand side, since our ansatz did not have a logarithmic term. Hence this ansatz does not work directly and probably should be modified. Remark Extending the ansatz to involve the logarithmic term $$
w(\eta) = d_1 \eta^\alpha + d_\text{log} \log (\eta) + d_0 + d_{-1} \eta ^ {-\alpha} + \dots
$$ will lead to the pole of order 3 in the integrand, and Cauchy integral formula will involve second derivative and lead to the term of order $\log ^2 (\eta)$ on the right-hand side (including $\log ^2 (\eta)$ will lead to $\log ^3 (\eta)$ on the right-hand side and so on). In conclusion, I wanted to ask whether there is a way to improve this ansatz so that all coefficients can be matched. On the other hand, maybe some alternative approach to the problem can be considered. Any help appreciated, thank you.","['integral-equations', 'cauchy-integral-formula', 'asymptotics', 'complex-analysis', 'mellin-transform']"
3519133,Clarification on proof of $\lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\to 1$,"Let $f:\mathbb{R}\to \mathbb{R}$ be differentiable at $a\in \mathbb{R}$ such that $f(a)>0$ . Evaluate: $$\lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}.$$ Attempt. A proof would go like: \begin{eqnarray} \left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}&=&\exp\left\{\ln\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\right\}=
\exp\left\{\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{n}\right\}\nonumber\\
&=&\exp\left\{\frac{1}{n^2}\,\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{\frac{1}{n}}\right\}\nonumber\\
&\to & \exp\left\{0\cdot \frac{f'(a)}{f(a)}\right\}=1,~n\to +\infty,\nonumber
\end{eqnarray} where we use the definition of derivative and the chain rule. So far, so good. My question is: one claims that since $f$ is differentible at $a,$ then $f$ is continuous at $a$ and so the limit becomes $1^0=1$ , according to the algebra of sequential limits. Is such an approach also correct? ( $1^0$ is not an indeterminate form). Thanks for the help.","['analysis', 'real-analysis', 'calculus', 'limits', 'derivatives']"
3519236,"Proving that $\mathbb E\left[\sum\limits_{k=1}^\infty Z_k\frac{\sqrt 2\sin(k\pi t)}{k \pi}\right] = 0$ (for i.i.d. $Z_k\sim N(0,1)$)","I want to prove that that $\mathbb E\left[\sum\limits_{k=1}^\infty \sqrt 2 \frac{Z_k \sin(k\pi t)}{k \pi}\right] = 0$ (for i.i.d. $Z_k\sim N(0,1)$ ). The way I was thinking to do it was to just interchange integrals and sums and get that $$\mathbb E\left[\sum\limits_{k=1}^\infty \sqrt 2 \frac{Z_k \sin(k\pi t)}{k \pi}\right] =\sum\limits_{k=1}^\infty \sqrt 2 \frac{\sin(k\pi t)}{k \pi} \mathbb E\left[Z_k\right] = 0$$ However, we need to justify why we can move the integral (i.e. expectation) and sum around. Naturally, one would turn to Fubini-Tonelli, but I do not think that the following is true: $$\sum_{k=1}^\infty \mathbb E\left[\left|\sqrt 2 \frac{Z_k\sin(k\pi t)}{k \pi}\right|\right] = \sum_{k=1}^\infty \left| \sqrt 2 \frac{\sin(k\pi t)}{k \pi} \right| \mathbb E\left[| Z_k|\right] < \infty$$ Can anyone prove this? Sidenote: the problem is from proving that $\mathbb U(t,\omega):= \sum\limits_{k=1}^\infty \sqrt 2 \frac{Z_k \sin(k\pi t)}{k \pi}$ is a Brownian bridge. I searched for the Karhunen-Loeve decomposition but it did not provide any help for calculating expectation directly.","['fubini-tonelli-theorems', 'brownian-motion', 'probability-theory', 'absolute-convergence']"
3519241,(Normal) subgroups of different orders of a group of order $20.$,"Let $G$ be a group of order $20$ in which the conjugacy classes have sizes $1$ , $4$ , $5$ , $5$ , $5$ . Then state whether true or false A) "" $G$ contains a normal subgroup of order $4$ "". The answer is supposed to be false. But I don't think so. Since one of the conjugacy classes is of size $5$ I assume some element has a centralizer of order $4$ . Since $$|cl(a)|= \frac{|G|}{|C(a)|} \ ,|cl(a)|=size \ of \ conjugacy \ class \ of \ a,\ |G|=order\ of \ group, \\|C(a)|=\ order \ of \ centralizer \ of \ a.$$ And since the centralizer is a normal subgroup isn't the group supposed to have a normal subgroup of order $4$ . B) "" $G$ contains a subgroup of order $10$ ."" This is supposed to be true. Can anyone give a reason why?","['group-theory', 'normal-subgroups']"
3519246,General formula for a sum of quadratic sequence,"I understand that the general formula for a sum of quadratic sequence is : $\displaystyle \sum_{i=1}^n {i^2} = \frac{n(n+1)(2n+1)}{6}$ However, my question is that does $i$ here has to be single term always? Can I still use the formula if I am calculating $\displaystyle \sum_{i=1}^n {(1+i)^2} $ ? For example, I was trying to calculate $\displaystyle \sum_{i=1}^3 {(2+i)^2} $ , and this is what I did $\displaystyle \frac{(2+n)((2+n)+1)(2(2+n)+1)}{6}$ $\displaystyle \frac{(2+n)(3+n)(2n+5)}{6}$ $\displaystyle \frac{(5)(6)(11)}{6} = 55$ However, the right answer is 50. I would like to know what is happening here? And why I can't use this formula directly?","['calculus', 'summation']"
3519316,Normal distribution tail probability inequality,I am trying to show that $$P(X>t)\leq \frac{1}{2}e^\frac{-t^2}{2}$$ for $t>0$ where $X$ is a standard normal random variable.  Perhaps this is simple.  I have been starting with $$ \int_{t}^{\infty} \frac{1}{\sqrt{2\pi}} e^\frac{-x^2}{2}dx \leq \int_{t}^{\infty}\frac{x}{t}\frac{1}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx = \frac{1}{t\sqrt{2\pi}}e^\frac{-t^2}{2}$$ but this is not what I want...I am looking for a much stronger inequality.   Any help in the direction of getting the $1/2$?,"['inequality', 'normal-distribution']"
3519342,Finding the leading order contribution to a certain integral.,"I am trying to compute the leading order term of the following expression in the small $\epsilon$ limit; $$
I
=
\frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)}
\int_{0}^{\infty} dt 
\frac{t^{s-1}e^{itx}}{(1-e^{i\epsilon_{1}t}) (1-e^{i\epsilon_{2} t})}
$$ First of all I tried expanding the exponentials with $\epsilon$ 's in them, leading to $$
-\frac{1}{\epsilon_{1}\epsilon_{2}}
\frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)}
\int_{0}^{\infty} 
t^{s-3}e^{itx}
dt
$$ I'm not sure how valid this is given that $\epsilon t=\mathcal{O}(1)$ in the large $t$ region, but it's all I could think of doing for now. From here I noticed that the integral looked very similar to the gamma function. I tried changing variables to convert it to something involving the gamma function, but the integration limits were giving me issues. Next I tried taking the derivative inside the integral. I found that $\left(\frac{t^{s}}{\Gamma(s)}\right)'\biggr|_{s=0}=-1$ , resulting in the following expression: $$
\frac{1}{\epsilon_{1}\epsilon_{2}}
\int_{0}^{\infty} 
t^{-3}e^{itx}
dt
$$ This looks relatively simple, but evaluating the antideriavtive gave an expression involving triginometric integrals which are divergent at zero. This makes me think that maybe one of my approximations is invalid. I am quite certain that the resulting expression should be $\frac{1}{2\epsilon_{1}\epsilon_{2}}x^{2}(\log(x)-\frac{3}{2})$ , and would really appreciate some help with proving it.","['integration', 'improper-integrals', 'asymptotics', 'complex-analysis', 'calculus']"
3519345,Joint entropy of 2 independent random variables,"Say we have two independent random variables $X$ and $Y$ . What is their joint entropy $H(X,Y)$ ? I worked this out, but I am not sure if the result I reached is correct. The definitions of entropy that I used are: $
\begin{align}
H(X) = - \sum_{x \in D(X)} P(x)\log_2P(x) \\
H(X,Y) = - \sum_{x \in D(X)} \sum_{y \in D(Y)} P(x,y)\log_2P(x,y)
\end{align}
$ I started from the definition of $H(X,Y)$ and rewrote it using the assumption that $X$ and $Y$ are independent. $
\begin{align*}
H(X,Y) = - \sum_{x \in D(X)} \sum_{y \in D(Y)} P(x)P(y)\log_2(P(x)P(y)) 
\end{align*}
$ From here I used algebra rules. However due to the double summation, I am not confident in my result. $
\begin{gather}
H(X,Y) = - \sum_{x \in D(X)} P(x) \sum_{y \in D(Y)} P(y)(\log_2P(x)+ \log_2P(y)) \tag{1}\\
       = - \sum_{x \in D(X)} P(x) \sum_{y \in D(Y)} (P(y)\log_2P(x)+ P(y)\log_2P(y)) \tag{2}\\
       = - \sum_{x \in D(X)} P(x)\log_2P(x) \sum_{y \in D(Y)} (P(y)+ P(y)\frac{\log_2P(y)}{\log_2P(x)}) \tag{3}\\
       = H(X) \sum_{y \in D(Y)} (P(y) + P(y)\frac{\log_2P(y)}{\log_2P(x)}) \tag{4}\\
       = H(X) (\sum_{y \in D(Y)} P(y) + \sum_{y \in D(Y)} P(y)\frac{\log_2P(y)}{\log_2P(x)}) \tag{5}\\
       = H(X) (\sum_{y \in D(Y)} P(y) - \frac{H(Y)}{\log_2P(x)}) \tag{6}\\
       = H(X) (1 - \frac{H(Y)}{\log_2P(x)}) \tag{7}\\
       = H(X) - \frac{H(Y)H(X)}{\log_2P(x)} \tag{8} \\
       = H(X) - H(Y)\sum_{x \in D(X)}P(x)\frac{\log_2P(x)}{\log_2P(x)} \tag{9}\\
       = H(X) - H(Y)\sum_{x \in D(X)}P(x) \tag{10} \\
       = \boxed{H(X) - H(Y)} \tag{11}
\end{gather}
$ Thanks. EDIT: New method: expand summations Starting from the rewritten expression of $H(X,Y)$ , expand the inner sum (assume there are $n$ elements in $Y$ ). $$\begin{align}& H(X,Y)
\\[2ex]&= - \sum_{x} \sum_{y} P(x)P(y)(\log P(x) + \log P(y))
\\[2ex]&= - \sum_{x} \sum_{y} (P(x)P(y)\log P(x) + P(x)P(y)\log P(y))
\\[2ex]&= - \sum_{x} P(x) (P(y_1)\log P(x) + P(y_1)\log P(y_1) + \ldots 
+ P(y_n)\log P(x) + P(y_n)\log P(y_n))
\\[2ex]&= - \sum_{x} P(x) (P(y_1)\log P(x) + \ldots + P(y_n)\log P(x) 
 + P(y_1)\log P(y_1) + \ldots + P(y_n)\log P(y_n) )
\\[2ex]&= - \sum_{x} P(x) (\log P(x)(P(y_1) + \ldots + P(y_n)) 
 + \sum_{y} P(y) \log P(y) )
\\[2ex]&= - \sum_{x} P(x) (\log P(x) - H(Y))
\\[2ex]&= - \sum_{x} (P(x)\log P(x) - P(x)H(Y))
\\[2ex]&= - (P(x_1)\log P(x_1) - P(x_1)H(Y) + \ldots + P(x_n)\log P(x_n) - P(x_n)H(Y))
\\[2ex]&= - (P(x_1)\log P(x_1) + \ldots + P(x_n)\log P(x_n) - P(x_1)H(Y) - \ldots - P(x_n)H(Y))
\\[2ex]&= - (\sum_{x}P(x)\log P(x) - H(Y)(P(x_1) + \ldots P(x_n)))
\\[2ex]&= \boxed{H(X) + H(Y)}
\end{align}$$","['statistics', 'probability-distributions', 'entropy', 'probability']"
3519548,Discrete and cocompact subgroups of isometries of $\mathbb{R}^n$,"Question 1. Let $G$ be a subgroup of isometries of $\mathbb{R}^n$ that acts discretely and cocompactly (i.e., $\mathbb{R}^n/G$ is compact). Then there exists a finite index subgroup $\mathbb{Z}^n\subset G$ . This appears as the final part of the Cheeger–Gromoll structure theorem for nonnegative Ricci curvature, Theorem 7.3.11 in Riemannian Geometry (Third Edition) by Peter Petersen. The proof there goes like this: Let $\mathbb{R}^n$ be the normal subgroup of translations. The author says that $G\cap\mathbb{R}^n$ is a finitely generated abelian group with finite index in $G$ that acts discretely and cocompactly on $\mathbb{R}^n$ . However, I don't see why this is true. In fact, I think $G\cap\mathbb{R}^n$ may well be the identity. Question 2. If moreover $G$ acts freely, then $G=\mathbb{Z}^n$ and $\mathbb{R}^n/G$ is a flat torus. Again this is content of Corollary 7.3.15 in that book, whose proof I cannot understand. So how do I prove the above facts? And by the way, are there any books on discrete isometry groups? Edit: There is something wrong about the second statement. It is required that the first Betti number of $\mathbb{R}^n/G$ should be $n$ . (Otherwise you could have things like the Klein bottle.) Then the proof in the book works.","['group-theory', 'isometry', 'riemannian-geometry', 'differential-geometry']"
3519560,Can one reconstruct a topological space by knowing all of its covering spaces?,"Let $X$ be a topological space with a universal cover $\tilde{X}$ . Let $X_{i}$ be the covering spaces of $X$ excluding $X$ and let $p_{ij}: X_{i}\to X_{j}$ be covering maps if such a map exists for $i,j$ . Is $X$ the colimit of this diagram? EDIT: One needs to add a condition to $\pi_{1}\left(X\right)$ . As a group it needs to be generated by its proper subgroups. For instance the projective plane has only one covering space which is the universal cover.","['general-topology', 'category-theory', 'algebraic-topology']"
3519602,Does $\lim_{x\to\infty}\frac{f(x)}{g(x)} = 1$ imply $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1$ for convex functions?,"Let $f,g$ be convex functions on $[0,\infty)$ such that $\lim_{x\to\infty}\frac{f(x)}{g(x)} = 1$ and $\lim_{x\to +\infty} g(x) = +\infty$ . Is it always true that $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1$ ? I can prove it when $g(x)=x$ and $g(x) = x^2$ . Edit. It is actually not so hard to show it works more generally when $g(x) = x^\alpha$ for any $\alpha \geq 1$ . The question can also be asked with series as a partial converse to the Stolz-Cesaro Theorem: Let $a$ and $b$ be increasing sequences such that and $\displaystyle\lim_{n\to\infty} \sum_{k=1}^n b_k = +\infty$ . Does $\displaystyle\lim_{n\to\infty} \dfrac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} = 1$ imply $\displaystyle\lim_{n\to\infty} \dfrac{a_n}{b_n} = 1$ ?","['divergent-series', 'real-analysis', 'sequences-and-series', 'limits', 'convex-analysis']"
3519669,"Reference-request: Closed form of $\int_0^1 (-\operatorname{W_0}(-\tfrac t{\mathrm{e}}))^x \, dt$","$\require{begingroup} \begingroup$ $\def\e{\mathrm{e}}\def\W{\operatorname{W}}\def\Wp{\operatorname{W_0}}\def\Wm{\operatorname{W_{-1}}}$ This question is the ""cousin"" of a closely related one . For $x\ge0$ \begin{align}
\int_0^1 (-\Wp(-\tfrac t{\e}))^x \, dt
&=\frac {1+\e\,x\,(\Gamma(x+2,1)-\Gamma(x+2))}{x+1}
\tag{1}\label{1}
,\\
\int_0^1 (-\Wm(-\tfrac t{\e}))^x \, dt
&=\frac{\e\,x\Gamma(x+2,1)+1}{x+1}
\tag{2}\label{2}
,\\
\int_0^1 (-\Wm(-\tfrac t{\e}))^x-(-\Wp(-\tfrac t{\e}))^x \, dt
&=\e\,x\,\Gamma(x+1) 
\tag{3}\label{3}
,
\end{align} where $\Wp,\ \Wm$ are the real branches of the Lambert $\W$ function. Equation \eqref{3} also suggests 
an elegant definition of the gamma function
in terms of 
the real branches of the Lambert $\W$ function as \begin{align}
\Gamma(x+1)
&=
\frac1{\e\,x}\int_0^1 (-\Wm(-\tfrac t{\e}))^x-(-\Wp(-\tfrac t{\e}))^x \, dt
\tag{4}\label{4}
.
\end{align} Question: Are these relations known? Any reference? $\endgroup$","['integration', 'reference-request', 'gamma-function', 'lambert-w', 'definition']"
3519716,Help understanding Reproducing Kernel Hilbert spaces?,"I am trying to wrap my head around some concepts of Reproducing Kernel Hilbert Spaces (RKHS) without having a formal background in functional analysis. Since I am trying to form an intuition about what this space is and how it does what it does, I would appreciate it if you could double-check my reasoning. A RKHS belonging to a kernel $k(x,x')$ (evaluated at $x$ , centered on $x'$ ), $x,x' \in \mathcal{X}$ contains functions of the form $$f(x)=\sum_{i=1}^{m}\alpha_ik(x,x_i)$$ where $\alpha_i \in \mathbb{R}$ are some coefficients and $m \in \mathbb{N}$ is some number to which $i$ is counting the indices. Now RKHSs have inner products. The rules for inner products state that if I have two vectors $\textbf{a}=[a_1,a_2,...,a_m]$ and $\textbf{b}=[b_1,b_2,...,b_m]$ in some $m$ -dimensional vector space, then their inner product $\langle\textbf{a},\textbf{b}\rangle$ is: $$\langle\textbf{a},\textbf{b}\rangle=\sum_{i=1}^{m}a_ib_i$$ One can see the similarity between the right-hand side of the first and second equation. So if we define $f(\cdot)=[\alpha_1,\alpha_2,...,\alpha_m]$ and $k(x,\cdot)=[k(x,x_1),k(x,x_2),...,k(x,x_m)]$ we could express $f(x)$ as an inner product in a RKHS $\mathcal{H}$ : $$f(x)=\langle f(\cdot),k(x,\cdot)\rangle_\mathcal{H}$$ That's the so-called reproducing property, if I understand correctly. Am I correct so far? If yes, I have a few questions: If the RKHS is a space, it should have a dimensionality (in our case $m$ ) and orthonormal bases. If we take a parameter space, for example, each of its dimension has a fixed interpretation (say, a 2-D space with dimensions $x=weight$ and $y=age$ allows for an inner product of two vectors, but both vectors will contain elements of the type $(weight,age)$ ). Now I can see how the expression in Equation 1 can be interpreted as an inner product, but the two vectors (albeit of equal length) contain different elements: $f(\cdot)$ is a vector of scalar coefficients, and $k(x,\cdot)$ is a vector of functions. They do not seem to share their bases in the same sense that the weight-age-example above would. Does this mean that the RKHS $\mathcal{H}$ is sort of a general-purpose space with no fixed definition as to what its dimensions represent, or is there a different interpretation I am missing? My second question relates to the dimensionality $m$ (which I adopted from the Wikipedia article). The way I understand it, this dimensionality $m$ relates to the number of elements in the set $\mathcal{X}$ . Strictly speaking, a function $f(x)$ defined according to Equation 1 could theoretically contain kernels centered on every element $x' \in \mathcal{X}$ , in which case the dimensionality of $\mathcal{H}$ would be as large as the set itself ( $m=|\mathcal{X}|$ ) and possibly infinite if the set $\mathcal{X}$ is infinitely large (e.g., $\mathcal{X}$ is the continuous real line). Wouldn't specifying $|\mathcal{X}|$ as the upper limit of the sum be more general than $m$ ? If we are only interested in $f(x)$ which are based only on a subset of $\mathcal{X}$ we could still sum over all theoretically possible dimensions and throw out the irrelevant kernels by setting their corresponding entries in $f(\cdot)$ to zero. Is this right or am I missing something?","['hilbert-spaces', 'functional-analysis', 'function-spaces', 'reproducing-kernel-hilbert-spaces']"
3519788,$\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$?,"I am currently studying the textbook Introduction to Tensor Analysis and the Calculus of Moving Surfaces , by Pavel Grinfeld. On page 3, the author begins with the following motivating example: What is the gradient of a function $F$ and a point $P$ ? You are familiar with two definitions, one geometric and one analytical. According to the geometric definition, the gradient $\nabla F$ of $F$ is the vector that points in the direction of the greatest increase of the function $F$ , and its magnitude equals the greatest rate of increase. According to the analytical definition that requires the presence of a coordinate system, the gradient of $F$ is the triplet of numbers $$\nabla F = \left( \dfrac{\partial{F}}{\partial{x}}, \dfrac{\partial{F}}{\partial{y}} \right) \tag{1.1}$$ Are the two definitions equivalent in some sense? If you believe that the connection is $$\nabla F = \dfrac{\partial{F}}{\partial{x}}\mathbf{i}  + \dfrac{\partial{F}}{\partial{y}} \mathbf{j}\tag{1.2}$$ where $\mathbf{i}$ and $\mathbf{j}$ are the coordinate basis, you are in for a surprise! Equation (1.2) can only be considered valid if it produces the same vector in all coordinate systems. You may not be familiar with the definition of a coordinate basis in general curvilinear coordinates, such as spherical coordinates. The appropriate definition will be given in Chap. 5. However, equation (1.2) yields different answers even for the two coordinate systems in Fig. 1.1. For a more specific example, consider a temperature distribution $T$ in a two-dimensional rectangular room. Refer the interior of the room to a rectangular coordinate system $x$ , $y$ where the coordinate lines are one meter apart. This coordinate system is illustrated on the left of Fig. 1.1. Express the temperature field in terms of these coordinates and construct the vector gradient $\nabla T$ according to equation (1.2). Alternatively, refer the interior of the room to another rectangular system $x'$ , $y'$ , illustrated on the right of Fig. 1.1), whose coordinate lines are two meters apart. For example, at a point where $x = 2$ , the new coordinate $x'$ equals $1$ . Therefore, the new coordinates and the old coordinates are related by the identities $$x = 2x' \ \ \ \ \ y = 2y' \tag{1.3}$$ Now repeat the construction of the gradient according to equation (1.2) in the new coordinate system: refer the temperature field to the new coordinates, resulting in the function $F'(x', y')$ , calculate the partial derivatives and evaluate the expression in equation (1.2), except with “primed” elements: $$(\nabla T)' = \dfrac{\partial{F'}}{\partial{x'}} \mathbf{i}' + \dfrac{\partial{F'}}{\partial{y'}} \mathbf{j}' \tag{1.4}$$ How does $\nabla T$ compare to $(\nabla T)'$ ? The magnitudes of the new coordinate vectors $\mathbf{i}'$ and $\mathbf{j}'$ are double those of the old coordinate vectors $\mathbf{i}$ and $\mathbf{j}$ . What happens to the partial derivatives? Do they halve (this would be good) or do they double (this would be trouble)? They double! This is because in the new coordinates, quantities change twice as fast. In evaluating the rate of change with respect to, say, $x$ , one increments $x$ by a small amount $\Delta x$ , such as $\Delta x = 10^{-3}$ , and determines how much the function $F(x, y)$ changes in response to that small change in $x$ . When one evaluates the partial derivative with respect to $x'$ in the new coordinate system, the same increment in the new variable $x'$ is, in physical terms, twice as large. It results in twice as large a change $\Delta F'$ in the function $F'(x', y')$ . Therefore, $\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$ and we conclude that partial derivatives double: $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}} \tag{1.5}$$ The first point I'm seeking clarification on is a minor problem: (1.1) is actually a doublet, right? The author refers to it as a triplet. The point that I'm uncomfortable with is the idea that $\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$ . If we're increasing the denominator by $10^{-3}$ , then why doesn't the numerator also increase by approximately the same amount, so that the result approximately cancels out to result in approximately the same value as before? I'm not satisfied that the author explains this clearly enough to make it evident. I would greatly appreciate it if people would please take the time to clarify this. EDIT The first +50 bounty has expired, so I might as well post my thoughts so far, so that it might help others formulate an answer. In trying to understand this, I had two streams of thought. The first stream of thought was focused on trying to understand (1.5), whilst the second stream of though was focused on trying to understand what I alluded to in my main post (which was the ideas behind the $\Delta F / \Delta x$ and $\Delta F' / \Delta x'$ ). It seems to me that an answer to the latter type of problem would naturally be suited to a real-analytic approach, so that's what I've been trying to do. I tried to understand the former by just using straightforward manipulations of the functions and their various implications. The main problem is that I don't have a lot of experience in real analysis, so I'm very unsure of my reasoning. I am not claiming that this post is an answer to the questions I have; rather, it is an expansion of my main post. In this expansion, I present my thoughts in trying to understand this section of the textbook, as well as some new questions that arose in the process of my reasoning. I am now going to post a second, larger bounty for this post. In addition to my original questions, I would appreciate it if people would review my reasoning here, and perhaps build upon it in their answer, and answer the questions that arose in this reasoning. The first problem I see here is that we don't know the form of the function $F(x, y)$ . It seems to me that this would have made things much easier. So we don't know whether, for instance, the function takes the form $F(x, y) = x + y$ , and, therefore, $\nabla F = (1, 1)$ , or whether the function takes some other form. I also wondered if we could use the antiderivative to recover the form of the function somehow, as is done to solve certain forms of differential equations, but I'm not sure whether that is possible/helpful in this situation. On the other hand, based on what I can infer from reasoning about this situation, it may not matter what form the function takes , or, rather, we may not need to know the precise form of the function , since we are told that the function form is such that $x = 2x', y = 2y'$ and $\dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}}$ , which might already give us enough information to understand what's going on here without needing the precise form of the function. The first part of my thoughts: So, we have that the change of coordinates $$x = 2x', \ \ \ y = 2y'$$ leads to the new function $$F'(x', y') = F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right).$$ Side-note: This assumes that $F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right)$ . This begs the question: If we have a function of the form $F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right)$ , where $x = 2x'$ and $y = 2y'$ are a change of variables, then what conditions must be satisfied for us to be able to say that $F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F'\left( x, y \right)$ ? I ask that question here , but have, as of yet, received no answers. I proceed assuming that it is valid in this case. EDIT: This property of functions seems to be referred to as "" homogeneity "" (see the aforementioned question for further details). So I proceed assuming that $F'$ is a homogeneous function . Taking the partial derivatives with respect to $x$ , we have $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'}.$$ I wonder if $\dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}}$ ? Because then we would have that $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}} \\ \therefore 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}},$$ as required. The second part of my thoughts: Using the definition of partial differentiation , I think we can proceed as follows: $$\begin{align} \dfrac{\Delta F}{\Delta x} = \dfrac{\partial{F(x, y)}}{\partial{x}} &= \lim_{\Delta x \to 10^{-3}} \dfrac{F(x + \Delta x, y) - F(x, y)}{\Delta x} \\ &= \lim_{\Delta (2x') \to 10^{-3}} \dfrac{F(2x' + \Delta (2x'), 2y') - F(2x', 2y')}{\Delta (2x')} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{2[F(x' + \Delta (x'), y') - F(x', y')]}{2\Delta (x')} \ \ \text{(Assuming that $F$ is a **homogeneous function**.)} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{F(x' + \Delta (x'), y') - F(x', y')}{\Delta (x')} \end{align}$$ I'm not sure whether I made an error here or whether it's just a dead-end, but it seems to that proceeding in this way, using the definition of partial differentiation, is a good way to gain an understanding of what the author meant. EDIT 2: My new reasoning is as follows: The denominator is $\Delta x$ . And since $x = 2x' \Rightarrow x' = \dfrac{x}{2}$ , we have that $\dfrac{\Delta F'}{\Delta x'} = \dfrac{\Delta F'}{\Delta \frac{x}{2}} = \dfrac{2\Delta F'}{ \Delta x}$ . If $\Delta x = 10^{-3}$ , then $\dfrac{2\Delta F'}{10^{-3}}$ . So, by this, we can only say that $\dfrac{\Delta F'}{\Delta x'}$ is approximately double $\dfrac{\Delta F}{\Delta x}$ if $F' = F$ , so that $\dfrac{\Delta F'}{\Delta x'} = \dfrac{2\Delta F'}{10^{-3}} = \dfrac{2\Delta F}{10^{-3}}$ .","['coordinate-systems', 'real-analysis', 'multivariable-calculus', 'vector-analysis', 'partial-derivative']"
3519888,Why can't we prove the consistency of ZFC by proving its axioms is satisfiable?,"We can prove the consistency of Peano Arithmetics by given a model of natural numbers, within this model, PA's axioms are satisfiable，thus PA is consistent. Why can't we do the same thing to ZFC","['elementary-set-theory', 'incompleteness', 'logic']"
3520013,There's something strange about $\sum \frac 1 {\sin(n)}$.,"Clearly, $$\sum_{n=1}^\infty \frac 1{\sin(n)}$$ Does not converge (rational approximations for $\pi$ and whatnot.) For fun, I plotted $$P(x)=\sum_{n=1}^x \frac 1{\sin(n)}$$ For $x$ on various intervals. At first, I saw what you might expect: Which is $P(x)$ for $x \in [0,20]$ and then $[0,300]$ . Seems a little self-similar, but whatever. Then I looked at $P(x)$ on the interval $[360,700]$ : OK, that looks suspiciously like $P(x)$ on the interval $[0,300]$ , but I'll toss out this coincidence as 'probably has to do with $\pi$ being irrational.' Here is $P(x)$ on $[700,1050]$ : And I observe similar behavior on similar intervals. Putting it all together, here is $P(x)$ on $[0,20000]$ : It's converging? Not quite. Here is $P(x)$ on $[20000,100000]$ : So again, we're seeing the function 'get closer and closer, then get farther and farther, all while alternating' from some value, just as we saw on the smaller intervals. I suspect that if my computer could handle $P(x)$ on $[100000,200000]$ , we would see the same thing (on a larger scale), though I'm not sure. So: what's going on here? How can we explain this fractal-ish behavior? Edit: I wonder if $P:\mathbb{N} \to \mathbb{R}$ is injective...","['summation', 'irrational-numbers', 'pi', 'sequences-and-series', 'fractals']"
3520053,Rings with rectangles with same perimeter but different surface,"Could someone find an example of a commutative unitary ring $(R,+,\cdot,0,1)$ , that is not an integral domain, with an element $x\neq 1,0$ such that $$\{a+b\mid a\cdot b=x\,\wedge\,a,b\neq 1\}$$ is a singleton? Or there's not such a ring because of the request hides a contradiction?","['number-theory', 'ring-theory', 'discrete-mathematics', 'integral-domain']"
3520099,Vector potential for $\nabla f \times \nabla g$,"Let $f$ and $g$ be two smooth real-valued functions on $\mathbb{R}^3$ . How can we find a vector potential for the vector field $F = \nabla f \times \nabla g$ ? In this question - Show that $\nabla\cdot (\nabla f\times \nabla h)=0$ - we have that $\text{div}(F) = 0$ , so $F$ has a vector potential, i.e. a vector field $H$ on $\mathbb{R}^3$ such that $\text{curl}(H) = F.$ But how do we specifically find it? If we let $H = (H_1, H_2, H_3)$ , then we should have that $$\displaystyle \begin{pmatrix} \frac{\partial H_3}{\partial y} - \frac{\partial H_2}{\partial z} \\ \frac{\partial H_1}{\partial z} - \frac{\partial H_3}{\partial x} \\ \frac{\partial H_2}{\partial x} - \frac{\partial H_1}{\partial y} \end{pmatrix} = \begin{pmatrix} \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial z} - \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial y} \\ \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial x} - \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial z} \\ \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial y} - \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial x} \end{pmatrix}, $$ but i don't know how to continue from here.","['vector-fields', 'multivariable-calculus', 'vector-analysis']"
3520174,Does $\log z$ have a laurent series about $0$?,I think not because $\log z$ isn't analytic on any neighborhood of 0. Is this correct? thanks,"['complex-analysis', 'laurent-series']"
3520265,"If $\displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3$ $\forall x \ge 1$ then find $f(2)$","Let $f$ be a real valued function on $[1,\infty)$ such that $f(1) = 3.$ If $\displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3$ $\forall x \ge 1$ then find $f(2).$ Here is my approach put $x =2$ to get \begin{align*}
2f(2)+ 8 &= 0\\
f(2) &= -4.
\end{align*} But if I differentiate both sides I get \begin{align*}
2f(x) &= f(x) + xf'(x) + 3x^2\\
xf'(x) - f(x) &= -3x^2.
\end{align*} Solving this gives $f(x) = -3x^2 + cx.$ Using the initial condition I get $$f(x) = -3x^2 + 6x$$ or $$f(2) = 0.$$ By different methods I am getting different values of $f(2).$ Can anyone here please tell me which one is correct and why? Thank you.","['calculus', 'integral-equations', 'ordinary-differential-equations']"
3520307,Relatively accessible introduction to Milnor-Witt $K$-theory,I'm looking for a recommendable self-contained source to learn Milnor-Witt $K$ -theory. Almost all papers that I found were research papers assuming reader's advanced knowledge on this topic. Does anybody know a script addressed to beginners on this topic? I have an algebro-topological background and I am familiar with (co)homologies and schemes at student/basic level. Thanks in advance,"['commutative-algebra', 'algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
3520347,"Combinatorics: making a 4-letter word from {a, b}","Question: Given the letters $\{a, b\}$ , how many 4 letter 'words' can one make? Define the weight of a word of the number of instances of 'ab' in it. How many instances are there of weight 0, 1, and 2? Find a generating function. Do the same for 5, 6, and n-letter words. My approach: Since you have two choices for each letter, you have $2^n$ choices, where $n$ is the length of the word. So for the first case, the 4 letter word, we have $2^4 = 16$ choices for words. I found the number of words with weights 0, 1, and 2 by brute force and got 5, 10, and 1, respectively. I'm not sure how to do that more easily, as in, not with brute force but with some sort of combinatorial logic. Then, we get the series $5 + 10x + x^2$ but this isn't an infinite series so I have no idea how to find a generating series for it. If anyone has any hints for how to solve this problem, please let me know, any help would be appreciated. Thanks!",['combinatorics']
3520366,Doing calculations with $\operatorname{Arg}z$ vs. $\arg z$,"I am confused on when to perform calculations on complex numbers using the argument or principal argument of a complex number. I know the following properties do not necessarily hold for the principal argument: $\text{Arg}z_1z_2=\text{Arg}z_1+\text{Arg}z_2$ $\text{Arg}\frac{z_1}{z_2}=\text{Arg}z_1-\text{Arg}z_2$ I was asked to show $\frac{\pi}{4}=4\tan^{-1}(\frac{1}{5})-\tan^{-1}(\frac{1}{239})$ Using $(1+i)(5-i)^4$ First I found the argument, $\arg(1+i)(5-i)^4$ which was $\arg(1+i)(5-i)^4=\{-\tan^{-1}(\frac{1}{239})+2k\pi:k \in \mathbb{Z}\}$ Then I found using the additive property of arg, $\text{arg}(1+i)=\{\frac{\pi}{4}+2k\pi:k\in \mathbb{Z}\}$ and $\text{arg}(5-i)^4=\{-4 \tan^{-1}(\frac{1}{5})+2k\pi:k \in \mathbb{Z} \} $ I then avoided the additive terms $2k\pi$ and concluded this implies $\frac{\pi}{4}=4\tan^{-1}(\frac{1}{5})-\tan^{-1}(\frac{1}{239})$ My professor said we were supposed to use the principal argument to perform the calculations. I did not because $\text{Arg}z_1z_2=\text{Arg}z_1+\text{Arg}z_2$ is not necessarily true. Because of this I am very confused and do not know when I should and should not use the principal argument in calculations.Also my professor never went over these properties of the principal argument. I noticed also the book likes to use $\text{arg}$ without taking on values that are periodic with $2\pi$ I asked the professor,""can we ever abuse notation and let $\text{arg}$ take on a single value."" He said no. This makes me more confused, if we are asked to perform a calculation like the one above where $\text{Arg}z_1z_2=\text{Arg}z_1+\text{Arg}z_2$ is not satisfied.","['complex-analysis', 'complex-numbers']"
3520367,"For finite $S$, if $f:S\rightarrow S$ is surjective, then $f$ is injective. [duplicate]","This question already has answers here : Surjectivity of $f:S\to S$ implies injectivity for finite $S$, and conversely (3 answers) Closed 4 years ago . $S$ is a finite set and $f:S\rightarrow S$ is surjective. Show $f$ is injective. Proof) Suppose $f$ is not injective. Then $\exists s_1\neq s_2$ with $f(s_1)=f(s_2)$ . Then since $S$ is finite, $|f(S)|\leq |S|-1$ .( $|\cdot|$ denotes the number of elements of a set.) Also, since $f$ is surjective, $f(S)=S$ , so $|f(S)|=|S|$ . Then $|S|\leq|S|-1$ , a contradiction. I was wondering if this is a correct argument.","['elementary-set-theory', 'functions', 'solution-verification']"
3520395,"What does hyperbolic space ""feel like""?","For example, I intuitively know what the Euclidian and Manhattan metrics feel like for a space. I don't have a good way to visualize / feel around hyperbolic space. What do distances ""feel like"" as I travel farther or closer? Are there good references for this sort of thing? I want something like Flatland for hyperbolic space!","['soft-question', 'hyperbolic-geometry', 'differential-geometry']"
3520406,How is $P(A^c \cap B^c)$ the same as $1-P(A \cup B)$?,"I don't understand how $P(A^c \cap B^c) = 1-P(A \cup B)$ is the same? If I draw $P(A^c \cap B^c)$ as a Venn diagram: If I draw $P(A \cup B)$ as a Venn diagram: So if I subtract $P(A \cup B)$ from 1, wouldn't that mean that I subtract $P(A \cup B)$ from the universe $\Omega$ , which would result int this: However, that would mean $P(A^c \cap B^c) \neq 1-P(A \cup B)$ Edit: As pointed out by multiple people. My diagram for $P(A^c \cap B^c)$ should look like the following and therefore the assumption of $P(A^c \cap B^c) = 1-P(A \cup B)$ is valid:","['elementary-set-theory', 'probability']"
3520444,A Sine Wave Where Alternate Distances Between 'Wave-center' Points Are Powers of φ,"This may be hard to visualize without my graph, see here If $\phi=\left(\frac{1+5^{1/2}}{2}\right), \alpha=\phi^{-2}, \beta=1$ , then the parametric equations, $$
(x, y)=\left(\sin(t)\cdot\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)^{-1},\; \left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)\right)
$$ produce a graph where the vertical distances between points of tangency with $x·y=±1$ on alternate sides are powers of $\phi$ . (when only positive numbers are graphed, starting at $\phi^{-1}$ and proceeding as follows: $\phi^{-1}, \phi^{0}, \phi^{1}, \phi^{2}, \phi^{3}$ ). To get some context on why the above is the case, see the update to the answer at this link: https://math.stackexchange.com/a/3515756/708680 I would like to slightly reformulate the above expression so that instead of the aforementioned distances between points of tangency being powers of $\phi$ starting at $\phi^{-1}$ (for positive numbers) and increasing by powers of $\phi$ on alternate sides, the distance between 'wave-center' points are powers of $\phi$ starting at $\phi^{-1}$ (for positive numbers) and increasing by powers of $\phi$ on alternate sides, instead. Here 'wave-center' points are defined as points on the wave whose $y$ is half of the distance between any point where the curve crosses $y$ and the closest next point of crossing to that point. See here for a graph showing the expression, points of tangency, 'wave-center' points, etc..., ['Wave-center' points are in red] ( Please note that the 'wave-center' points in my graph are not the ones I want for the new expression but are instead just to show what I mean by 'wave-center' points ): https://www.desmos.com/calculator/v7pmwr5oj9 I want the adjusted parametric equations to retain the following while being altered in the aforementioned manner; they should: 0 . Be of the form: $(x,y)=\big(f(t)^{-1}\cdot\sin(t), f(t)\big)$ . For the sake of clarity I add that, for the original equations, this $f(t)$ was in the form $$
f(t)=\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right).
$$ 1. Start at $(0, 1)$ for positive and negative numbers. 2. Have points of tangency to $x\cdot y=±1$ (as a result of 0. ). 3. Maintain a smooth, sinusoidal, 2-D spiral nature. 4. Be written in terms of $\sin(t)$ . Thanks for your help.","['golden-ratio', 'ordinary-differential-equations', 'calculus', 'wave-equation', 'trigonometry']"
3520456,"Proving that $\text{Aut}(\mathbb{P}^n)=\text{PGL}(n+1,\mathbb{C})$","Consider $\mathbb{P}^n$ as an algebraic variety over $\mathbb{C}$ . I'm trying to solve the following exercise: Prove that $\text{Aut}(\mathbb{P}^n)=\text{PGL}(n+1,\mathbb{C})$ [hint: prove that automorphisms of $\mathbb{P}^n$ send hyperplanes into hyperplanes] Let $H:=Z(L)$ be a hyperplane, where $L$ is a linear form. I've noticed that if $f\in \text{Aut}(\mathbb{P}^n)$ , then $f^{-1}(H)=Z(L\circ f)$ . Taking a representation $f=(F_0:...:F_n)$ with $F_i$ 's homogeneous of same degree $d$ , then $L\circ f$ gives us a polynomial of degree $d$ . If $H=Z(x_i)$ , and assuming the claim from the hint, then $Z(x_i\circ f)=Z(F_i)$ is a hyperplane, i.e., $d=\deg(F_i)=1$ . Since the $F_i$ 's can be rescaled by a non-zero constant, then $f\in\text{PGL}(n+1,\mathbb{C})$ . Now I can't see any intuitive reason why the claim should be true. I'm sure it must be some very particular feature of $\mathbb{P}^n$ , but I don't know what it is.","['automorphism-group', 'algebraic-geometry', 'projective-geometry']"
3520474,Set Identities in which Venn Diagram Proofs do not work,"I want to give my students some example problems for proving set equality about why you cannot take the general venn diagram proof at face value. I read this post that stated venn diagrams are not formal proofs. Assume that students are using the basic idea of two partially overlapping circles (because that is the most common version students know). In this case an identity that would fit the bill is one that does not work when either $A=B$ , $A\cap B=\varnothing$ or $A\subset B$ . Any thoughts?","['elementary-set-theory', 'big-list', 'soft-question', 'education']"
3520492,Coordinate free derivation of Euler-Lagrange equations,"I am studying Symplectic Geometry and I was wondering how one can compute Euler-Lagrange equation in a coordinate free manner. 
For instance, I know for the following Lagrangian $L(x,v)=\frac{1}{2}g_{x}(v,v)-V(x)$ the corresponding equation is $\nabla_{\dot\gamma}\dot\gamma=-\nabla V$ , but it is a bit complicated to derive it in a coordinate system.","['symplectic-geometry', 'euler-lagrange-equation', 'differential-geometry']"
3520494,I have a question about homotopy of the unit circumference with a point,"Definition: Let $\gamma_0,\gamma_1:[0,1]\to G$ two rectifiable curves and $G\subseteq\mathbb{C}$ an open connected set. We say $\gamma_0$ and $\gamma_1$ are homotopic in $G$ if there exists $\Gamma:[0,1]\times[0,1]\to G$ continuous such that: \begin{cases} 
      \Gamma(s,0)=\gamma_0(s), \Gamma(s,1)=\gamma_1(s) & 0\le s\le 1 \\
      \Gamma(0,t)=\Gamma(1,t) & 0\le t\le 1 \\
   \end{cases} The question is: Show that if we remove the condition "" $\Gamma(0,t)=\Gamma(1,t)$ "" in the above definition, then the curves $\gamma_0(s)=e^{2\pi i s}$ and $\gamma_1(s)=1$ if $0\le s\le 1$ would be homotopic in $\mathbb{C}\setminus\{0\}$ . I defined by $\Gamma(s,t) = t + (1-t)e^{2\pi is}$ . This function satisfies the definition, but I saw in a topology article that the unit circumference is not homotopic with point 1. So where is my error?","['complex-analysis', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
3520534,$2^n$th decimal place of $\sqrt{2}.$,"Someone on Art of Problem Solving claims to know how to calculate the $2^{2020}$ th decimal place of $\sqrt{2},$ and will tell us if everyone gives up. Brute force will not work, nor will a BBP style formula (for the reason that one does not exist, and the ones known so far are for base $2^k$ expansions; $10$ is not a power of $2$ ). Is there a feasible solution, or am I being trolled? Trying to search various queries relating to my question online results in nothing except spigot algorithms which spit out the digits one by one from the start. As you may know, they will not work. I am operating under the assumption that if a clever solution exists, it is not a new discovery, for such a simple question has surely been considered before. Update: The problem poster has promised to post the solution in 2021 if no one finds it before then. This is not the first time they've done this, but every time they've done this in previous years, they have delivered on the promises. 2nd Update: 2 weeks left and I've forgotten to show the source . Soon we'll see if we have been trolled or not.","['number-theory', 'radicals', 'decimal-expansion', 'sequences-and-series']"
3520549,What is the coefficient of this term in multinomial expansion?,I am trying to find the coefficient of the term $(a+d) ^{11} b^4  c^2$ in the expansion of $(a - 2b + 3c + d)^{17}$ I understand the multinomial theorem can be used and tried to solve it that way but got confused because of the $(a + d) ^{11}$ within the term. How would I go about this?,"['combinatorics', 'discrete-mathematics']"
3520574,The integral of the Dickman function,"It is well known that the integration of the Dickman function with the weight $\frac{1}{(t+2)}$ or $\frac{1}{(1+t)^2}$ gives Golomb – Dickman constant : $$\lambda=  \int_0^\infty \frac{\rho(t)}{t+2}\,dt$$ or $$\lambda=  \int_0^\infty \frac{\rho(t)}{(1+t)^2}\,dt,$$ Where $\lambda=0.624...$ - Golomb–Dickman constant, $\rho(t)$ - Dickman function. But what about the integral of the Dickman function itself $\int_0^\infty \rho(t)dt= $ ? I tried to integrate numerically with the help of WolframAlpha using its function DickmanRho(t). At the limit of accuracy, I got the value $1.7811$ , which, within the error, coincides with $ e^{\gamma} $ , where $\gamma=0.5772...$ - Euler–Mascheroni constant .This result is not unexpected considering Mertens' third theorem , but how this result сan be proved based on the definition of the Dickman function i.e. through delay differential equation with initial conditions(Definition see here )?","['integration', 'number-theory', 'ordinary-differential-equations']"
3520580,"Does $\iint_D \frac{x^2}{x^2+y^2} dx dy $ converge on $D= \left\{ (x, y) : x^2+y^2\leq ax \right\} $ ? If yes, what value does it converge to?","Powers equal to $2$ entice the polar substitution. $D= \left\{ (x, y) : x^2+y^2\leq ax \right\}$ , so $0 \leq r \leq a \cos \phi.$ For the domain to make sense, we need either $\phi \in [0, \frac{\pi}{2}] \cup [\frac{3 \pi}{2}, 2\pi)$ and $a \geq 0$ , or $\phi \in [\frac{\pi}{2}, \frac{3 \pi}{2}]$ and $a \leq 0$ . Everything below was achieved with help from the comments. $$\iint_D \frac{x^2}{x^2+y^2} dx dy = \iint_{D'} r \cos^2 \phi \ d\phi dr.$$ Case $1$ : Since $(0, 0) \in D $ (equivalently, $r=0$ for any $\phi$ ) makes the integrand indefinite, $$
\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr 
+ 
\lim_{\epsilon \rightarrow 0}
\int_{\frac{3 \pi}{2}}^{2\pi} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr
$$ $$
2\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr 
$$ $$
2\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \left( \frac{a^2 \cos^2 \phi}{2}-\frac{\epsilon^2}{2} \right) \cos^2 \phi \ d \phi 
$$ $$
\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \left( {a^2 \cos^2 \phi}-{\epsilon^2} \right) \cos^2 \phi \ d \phi 
$$ Put $0$ instead of $\epsilon$ , because it won't let me integrate: $$
a^2
\int_{0}^{\frac{\pi}{2}} { \cos^4 \phi} \ d \phi = \frac{a^2}{2} Β \left(\frac{1}{2}, \frac{5}{2} \right) = \frac{a^2}{2} \cdot \frac{\Gamma(\frac{1}{2}) \Gamma(\frac{5}{2})}{\Gamma(3)} = \frac{a^2}{2} \cdot \frac{\sqrt{\pi} \cdot 0.75 \sqrt{\pi}}{2} = \frac{3 a^2 \pi}{16}
$$ For case $2$ , the result should be same. Was this correct? Formula $Β(m, n)=2\int_0^{\pi/2}\sin^{2m-1} \theta \cos^{2n-1} \theta \ d \theta$ was used, but I'm not sure I can extract it myself, and would be grateful for reference. The answer would be, that for fixed $a$ , it's convergent. (I am not sure about the definition of convergence here, and can't look it up, since the classes were missed, and google is blocked). For Oliver Jones: let $r_i=a\frac{i}{i+1}$ , $\phi_i=\frac{\delta}{i}.$ $$\iint_{D_\delta} r \cos^2 \phi \ d\phi dr= \lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(\cos^2 \frac{\delta}{i} \right) 
\left( 
a\frac{i+1}{i+2} - a\frac{i}{i+2}
\right) 
\left( 
\frac{\delta}{i+1}-\frac{\delta}{i}
\right) =
\lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(1 \right) 
\left( 
\frac{a}{(i+1)(i+2)}
\right) 
\left( 
\frac{-\delta}{i(i+1)}
\right)
=
 \sum_{i=0}^{\infty} \left(a^2\frac{1}{i+1}\right) \left(1 \right) 
\left( 
\frac{1}{(i+1)(i+2)}
\right) 
\left( 
\frac{-\delta}{(i+1)}
\right).
$$ It converges because of the $4$ th power in the denominator. Or maybe like this: $$
\lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a x \ dx \int_{-\sqrt{x(a-x)}}^{\sqrt{x(a-x)}} \frac{1}{1+ (y/x)^2} d (y/x)$$ $$
\lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a 2 x \arctan \sqrt{x(a-x)} \ dx  $$ How to proceed here, I am not sure. $$
\lim_{\delta \rightarrow 0}\iint_{\delta^2 \leq r^2+ar \cos \theta + a^2/4 \leq a^2/4} r \cos ^2 \theta \ dr \ d\theta
$$ Let's choose $\theta \in [-\pi/2, \pi/2], a\geq 0$ . Then $
\delta^2 \leq r^2 - ar + a^2/4 \leq  r^2+ar \cos \theta + a^2/4 \leq r^2 + a^2/4 \leq a^2/4.
$ $
0 \leq r^2 - ar + a^2/4 -\delta^2 \Longrightarrow r\in (0, \frac{a-2\delta}{2}).
$ $$
\lim_{\delta \to 0} \int_{-\pi/2}^{\pi/2} \cos ^2 \theta \ d \theta \int_0^{(a-2\delta)/2} r dr = \frac{a^2}{8} В(1/2, 3/2) = \frac{a^2 \pi}{16}
$$","['integration', 'conditional-convergence', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
3520597,Is $G= \mathbb{Z}$ a group with binary operation defined as $a \cdot b \equiv a - b$?,"Is $G= \mathbb{Z}$ a group with binary operation defined as $a \cdot b \equiv a - b$ ? I am pretty sure the answer is NO but I would like some verification on my reasoning below: Associativity fails as in: Let $a,b,c \in G$ then $a \cdot (b \cdot c) = a \cdot (b-c) = a - (b-c) = a - b + c$ . But $(a \cdot b) \cdot c = (a-b) \cdot c = a - b - c \neq a \cdot (b \cdot c)$ . Identity Let $a \in G$ . Then $a \cdot 0 = a - 0 = a$ , so $0$ is the additive identity of $a$ , hence the identity element in $G$ . But $0 \cdot a = 0 - a = -a \neq a \cdot 0$ Inverse Let $a \in G \backslash \{0\}$ . Assume $G$ is a group, then $a$ has an inverse $a^{-1}$ . So $a \cdot a^{-1} = a - a^{-1} = a - (-a) \neq 0$ . I am confident about associativity, but I am not sure if my arguments about the inverse and identity are justified.","['group-theory', 'abstract-algebra', 'solution-verification']"
3520735,How many possible ways to arrange the letters of FOO_FIGHTERS such that the underscore isn't on either end?,"My thought process: There are $\frac{12!}{2!2!}$ possible ways of arranging FOO_FIGHTERS total, then I should subtract the number of ways it can be arranged with the underscore on either end. I believe the possible arrangements of FOO_FIGHTERS with the underscore on the left end would be $\frac{11!}{2!2!}$ , then I would multiply that value by two to accommodate for the underscore being on the right end. However, I'm not sure that $\frac{11!}{2!2!}$ is the correct method, because wouldn't that be disregarding the location of the underscore to begin with, or is it irrelevant anyway because it will always be in the same spot? Edit: Formatted properly with MathJax","['combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
3520787,$f$ is the complexification of a map if $f$ commutes with almost complex structure and standard conjugation. What if we had anti-commutation instead?,"I started studying the book of Daniel Huybrechts, Complex Geometry An Introduction. I tried studying backwards as much as possible, but I have been stuck on the concepts of almost complex structures and complexification . I have studied several books and articles on the matter including ones by Keith Conrad , Jordan Bell , Gregory W. Moore , Steven Roman , Suetin, Kostrikin and Mainin , Gauthier I have several questions on the concepts of almost complex structures and complexification. Here are some: Let $V$ be $\mathbb R$ -vector space, possibly infinite-dimensional. Complexification of space definition : Its complexification can be defined as $V^{\mathbb C} := (V^2,J)$ where $J$ is the almost complex structure $J: V^2 \to V^2, J(v,w):=(-w,v)$ which corresponds to the complex structure $s_{(J,V^2)}: \mathbb C \times V^2 \to V^2,$$ s_{(J,V^2)}(a+bi,(v,w))$$:=s_{V^2}(a,(v,w))+s_{V^2}(b,J(v,w))$$=a(v,w)+bJ(v,w)$ where $s_{V^2}$ is the real scalar multiplication on $V^2$ extended to $s_{(J,V^2)}$ . In particular, $i(v,w)=(-w,v)$ . Complexification of map definition : See a question I posted previously . Proposition ( Conrad , Bell ): Let $f \in End_{\mathbb C}(V^{\mathbb C})$ . We have that $f$ is the complexification of a map if and only if $f$ commutes with the standard conjugation map $\chi$ on $V^{\mathbb C}$ , $\chi(v,w):=(v,-w)$ . In symbols: If $f \circ J = J \circ f$ , then the following are equivalent: Condition 1. $f=g^{\mathbb C}$ for some $g \in End_{\mathbb R}(V)$ Condition 2. $f \circ \chi = \chi \circ f$ I think Bell would rewrite Condition 2 as $f = \chi \circ f \circ \chi$ and say $f$ 'equals its own conjugate'. Questions: Considering the half of the above proposition that says ' $f$ commutes with both $J$ and $\chi$ implies $f$ is complexification of a map', what do we get if we instead have the following? commutes with $J$ and anti-commutes with $\chi$ ( $f \circ \chi = - \chi \circ f$ ) anti-commutes with $J$ ( $f \circ J = - J \circ f$ , i.e. $f$ is $\mathbb C$ -anti-linear) and commutes with $\chi$ anti-commutes with $J$ and anti-commutes with $\chi$ Motivation : $f=J$ satisfies the case in Question 1, and $f=\chi$ satisfies the case in Question 2. Guess (for Question 2): Similar to this (the $K=-J$ part) , I kind of had the idea to define something like anti-complexification of a map: for $g \in End_{\mathbb R}(V)$ , $g^{anti-\mathbb C}$ is any $\mathbb C$ -anti-linear map such that $g^{anti-\mathbb C} \circ cpx = cpx \circ g$ , where $cpx: V \to V^{\mathbb C}$ is the complexification map, as Roman ( Chapter 1 ) calls it, or the standard embedding, as Conrad calls it. I think $g^{anti-\mathbb C}$ turns out to always exist uniquely as $g^{anti-\mathbb C}(v,w)=(g(v),-g(w))$ . Then, I think the answer for Question 2 is that $f$ is the anti-complexification of a map. We can strengthen the result to: Let $f$ be $\mathbb C$ -anti-linear on $V^{\mathbb C}$ , i.e. $f$ anti-commutes with $J$ . We have that $f$ is the anti-complexification of a map $g \in End_{\mathbb R}V$ , i.e. $f=g^{anti-\mathbb C}$ if and only if $f$ commutes with the standard conjugation map $\chi$ , i.e. $f \circ \chi = \chi \circ f$ . In the case of $f=\chi$ for Question 2, $f=\chi = g^{anti-\mathbb C}$ for $g=id_{V}$ , the identity map on $V$ , which by the way gives us $(id_{V})^{\mathbb C} = id_{V^{\mathbb C}}$","['complex-geometry', 'complex-analysis', 'abstract-algebra', 'linear-algebra', 'almost-complex']"
3520797,Card game only by talking - e.g. werewolf,"Recently I was wondering if it was possible to play card games with my math friends without having anything else than communication (no computing device, no sheets of paper), e.g. to play werewolf or mafia without cards. Formally, for n cards and n players, I want an algorithm such that: At the end, each player is assigned a number between 1 and n All assigned numbers are different No player knows the assigned numbers of other players without doing a substantial computation that one can easily not think about. All computations can be done in a reasonable time by a human brain. Nothing is secret except internal thinking I was able to devise a probabilistic algorithm that seems to work for n=3 Each player independently picks a random number between [1,3,5] Each player creates a 3-digit number whose modulo 7 is the chosen random number. Each player shares its 3-digit number. If the sum of these three numbers modulo 7 is 2 and the product modulo 7 is 1, then we have the guarantee that these numbers are 1, 3 and 5 in any order. If not, restart at 1. Add 1 to the chosen number and divide by 2. However, there is only 6 chances over 27 that this algorithm works... Could there be an algorithm that always works? For example that obtains permutation in an uniform way, such that each participant is expected to compute their own card number? And for more cards?","['uniform-distribution', 'probability', 'algorithms']"
3520807,dimension of a vector space seen as a variety,"The dimension of an affine algebraic variety $V\subset k^n$ is defined as the Krull dimension of its coordinate ring $k[V]=k[X_1,\cdots,X_n]/I(V)$ where $I(V)$ is the set of polynomials in $k[X_1,\cdots,X_n]$ vanishing on $V$ . In the case where $V$ is moreover a vector space over the field $k$ , I don't succeed in showing that the dimension of $V$ as vector space is the same as the dimension of $V$ as affine algebraic variety. In particular, what can be said about the ideal $I(V)$ in this case ?",['algebraic-geometry']
3520839,Sufficient and necessary conditions for a certain measure to be sigma finite,"If $\mu$ is a sigma finite measure on a measurable space $(X,\Omega)$ , can we give sufficient and necessary conditions for a measurable function $f \colon X \to [0,\infty]$ such that the measure $\nu \colon \Omega \to [0,\infty]$ defined by $\nu(A) = \int_{A} f d\mu$ is sigma finite?",['measure-theory']
3520893,Is there a classification of Metric Spaces?,"As in group theory, there is a concept of isomorphism between metric spaces called isometry.
Two metric spaces $X$ and $Y$ are isometric if there is a function that perserves the distance of two elements. That function is called isometry. The thing is that properties of metric spaces (completeness, compactness, connectedness, etc.) are preserved  under isometry. So, thinking about the classification of finite simple groups, I was wondering if there is any classification of Metric Spaces up to isometry, or at least a specific category of Metric Spaces (like finite simple groups in Group Theory). Also, I am interested if there is a more general topological classification of Metric Spaces up to homeomorphism (isomorphism in topology).","['general-topology', 'metric-spaces', 'analysis']"
3520909,How can I prove that $\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}\exp(-\frac{(k-1)k}{2n})=0$?,"How can I prove that $$\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}\exp\left(-\frac{(k-1)k}{2n}\right)=0?$$ I'm really trying to avoid big-O notation and complicated asymptotic-behaviour arguments. It seems that each of the terms converges to $1$ as $n$ gets large. There are $n$ such terms, so if we divide the sum by $n$ , it looks like the entire limit behaves like $n/n\to1\neq0$ ? My intuition is clearly wrong.","['summation', 'sequences-and-series', 'limits', 'convergence-divergence', 'exponential-function']"
3520932,Why is this identity about commutators of Lie derivatives true?,"I am reading the paper ""On splitting methods for Schrödinger-Poisson and cubic nonlinear Schrödinger equations"" by Lubich. On page 2147 the author claims $$[T,V](\psi) = T'(\psi) V(\psi) - V'(\psi) T(\psi) $$ where $T(\psi) = i\Delta \psi$ and $V(\psi) = -i\tilde{V}[\psi]\psi$ with $\tilde{V}([\psi])= \Delta^{-1} |\psi|^2$ where $\psi$ is a complex-valued $H^2$ -function on $\mathbb{R}^3$ (so that $\Delta\psi$ is well-defined). It is then claimed that $$
\begin{split}
 T'(\psi) V(\psi) - V'(\psi) T(\psi) = &\; i\Delta(-i\Delta^{-1}(\psi\bar{\psi}) \psi) + i\Delta^{-1}(-i\Delta \psi \bar{\psi})\psi \\
&\;+ i\Delta^{-1}(\psi  \overline{i\Delta\psi}) + i\Delta^{-1}(\psi\bar{\psi})i\Delta \psi  
\end{split}\label{1}\tag{1}
$$ So the derivative in $T'(\psi)$ is probably just some sort of formal derivative with respect to $\psi$ but what would $(i\Delta \psi)'$ be then? The first term in $(1)$ is cleary coming from $T'(\psi)V(\psi)$ but it doesn't make sense to me from the definition of $T'(\psi)$ . It would make sense as $T(V(\psi))$ , though. So is $T'(\psi)=T(\psi)$ then? The other terms are confusing as well. Naively, we would have $$(\tilde{V}[\psi]\psi)' = \tilde{V}[\psi]'\psi + \tilde{V}[\psi]$$ which, again, doesn't fit with what we have in \eqref{1}. More formal than above, $T$ and $V$ are vector fields and their and the corresponding differential equation is $$\partial_t \psi = T(\psi) + V(\psi)$$ In that sense the commutator is the Lie bracket of vector fields. Maybe someone can give me some clarification?","['lie-derivative', 'partial-differential-equations', 'ordinary-differential-equations', 'differential-geometry']"
3520957,Where am I wrong in this try to prove Stokes' theorem for a parallelogram?,"So in order to understand Stokes' theorem, I tried to prove it for a parallelogram $P$ with parameterized boundary $R(t)=(x(t),y(t))$ spanned by two vectors $\mathbf{u}$ and $\mathbf{v}$ . The parallelogram : So that for some covector field ""1-form"" $\omega$ : $$\int_{\partial P}\omega = \int_{P}d\omega$$ With the edges having $t=0,1,2,3$ I think this reduces to: $$\omega_{R(0)}(\mathbf{u})+\omega_{R(1)}(\mathbf{v})-\omega_{R(2)}(\mathbf{u})-\omega_{R(3)}(\mathbf{v})=d\omega(\mathbf{u}\wedge\mathbf{v})$$ And this is my try: $$\mathbf{u}=R(1)-R(0)=R(2)-R(3)\\ \mathbf{v}=R(2)-R(1)=R(3)-R(0)$$ Let $ω_t=ω_{R(t)}$ so: $$\omega_{0}(\mathbf{u})+\omega_{1}(\mathbf{v})-\omega_{2}(\mathbf{u})-\omega_{3}(\mathbf{v})=(ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v})$$ Let: $$ω_1-ω_0=ω_2-ω_3= \Delta_{\mathbf{u}}~ω~,~ω_2-ω_1=ω_3-ω_0= \Delta_{\mathbf{v}}~ω~,\\\Delta_{\mathbf{u}}~ω(\mathbf{v})=\Deltaω(\mathbf{u},\mathbf{v})$$ So: $$(ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v})=\\(-\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{u})+(\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{v})=\\
-\Delta_{\mathbf{u}} ~ω(\mathbf{u})-\Delta_{\mathbf{v}}~ω(\mathbf{u})+\Delta_{\mathbf{u}} ~ω(\mathbf{v})-\Delta_{\mathbf{v}}~ω(\mathbf{v})=\\- \Deltaω(\mathbf{u},{\mathbf{u}})-\Deltaω(\mathbf{v},{\mathbf{u}})+\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{v}})$$ I'm not quite sure how to continue from here but if $-\Deltaω(\mathbf{v},{\mathbf{v}})-\Deltaω(\mathbf{u},{\mathbf{u}}) =0$ , then let: $$dω(\mathbf{u},\mathbf{v})=\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{u}})$$ It's easy to proof that $d\omega$ is multi-linear and anti-symmetric so: $$d\omega(\mathbf{u},\mathbf{v})=d\omega(\mathbf{u}\wedge \mathbf{v})$$ So I know I probably made a lot of mistakes but is the core of this way to prove it right? Are there any papers who proof Stokes' Theorem in general by proving it for a parallelogram or parallelepiped spanned by vectors? Because it makes much sense and it's a beautiful approach, but what are my exact mistakes?","['linear-algebra', 'stokes-theorem', 'differential-geometry']"
3520987,"If I stop flipping when I've reached equal heads and tails, what's the chance I never stop? [duplicate]",This question already has answers here : Proving that $1$- and $2D$ simple symmetric random walks return to the origin with probability $1$ (10 answers) Closed 4 years ago . I toss a balanced coin until the number of heads I get equals the number of tails? What's the chance I never stop? I have tried considering the reverse event and a recursive reasoning but nothing conclusive. A close question has already been asked here but I don't know the Markov formalism that is used.,"['stochastic-processes', 'probability']"
3520989,Equivalent definitions of a $p$-group,"In Dummit and Foote, p.139, a $p$ -group is defined as a group of order $p^\alpha$ for $\alpha \geq 1$ . I also found online a definition of a $p$ -group as a group in which every element has a power of $p$ order. Actually, Dummit and Foote uses this second property later in the text, but I did not find a proof. I guess that those notions are involved in the proof of the equivalence: the order of a generator is the order of a cyclic group a group of prime order is cyclic But I can not connect the dots. Could someone provide a proof that both definitions of a $p$ -group are equivalent ?","['finite-groups', 'definition', 'abstract-algebra', 'p-groups', 'group-theory']"
3521010,How to solve homogeneous linear recurrence relations with constant coefficients?,"Consider a sequence $(a_n)_{n\in\mathbb N}$ defined by $k$ initial values $(a_1,\dots,a_k)$ and $$a_{n+k}=c_{k-1}a_{n+k-1}+\dots+c_0a_n$$ for all $n\in\mathbb N$ . What are some ways to get closed forms for $a_n$ ? What are some ways of rewriting $a_n$ that allows it to be computed without going through all previous values? For example, we have Binet's formula : $$F_n=\frac{\phi^n-(-\phi)^{-n}}{\sqrt5}$$ Furthermore, what about simultaneously defined linear recurrences? For example: $$\begin{cases}a_{n+1}=2a_n+b_n\\b_{n+1}=a_n+2b_n\end{cases}$$ How can these be solved? See also: Wikipedia: Recurrence relations . This is being repurposed in an effort to cut down on duplicates; see here: Coping with abstract duplicate questions List of abstract duplicates","['fibonacci-numbers', 'recurrence-relations', 'closed-form', 'sequences-and-series', 'faq']"
3521043,Prove that $(1-\sqrt{-5})\otimes(1+\sqrt{-5})\not=2\otimes 3$ in a specific tensor product.,"Background : Consider the ring $R=\mathbb{Z}[\omega]=\{a+b\omega:\omega=\sqrt{-5},a\in\mathbb{Z},b\in\mathbb{Z}\}$ , which is also a $\mathbb{Z}[\omega]$ -module. Let $I=(1-\omega,2), J=(1+\omega,3)$ be two ideals in $R$ generated by the specified elements. Question : I believe that in the tensor $R$ -module, $I\underset{R}\otimes J$ , the following statement is true: $$(1-\omega)\underset{R}{\otimes}(1+\omega)\not=2\underset{R}{\otimes}3.$$ And I am looking for a bilinear mapping defined on $I\times J$ which maps $(1-\omega)\underset{R}{\otimes}(1+\omega)$ and $2\underset{R}{\otimes}3$ to distinct values. If such a map can be found, then by the universal property of tensor product, certainly $(1-\omega)\underset{R}{\otimes}(1+\omega)\not=2\underset{R}{\otimes}3$ . Could anyone suggest such a bilinear map? Or is there a better way to determine whether the above two simple tensors are different? Update : This question was inspired by Exercise $10.4.21$ in Abstract Algebra, 3rd edition , by Dummit and Foote. A counterexample in which the map $I\otimes J\simeq IJ$ sending $i\otimes j$ to $ij$ fails to be injective can be found in Exercise $10.4.17$ in the same book.","['abstract-algebra', 'tensor-products', 'modules']"
3521066,Show that every countable well-ordered set is order isomorphic to a subset of $\mathbb{Q}$,"I'm currently going through Nik Weaver's ""Forcing for Mathematicians"" and doing the exercises. I'm looking for a hint on this one: Show that every countable well-ordred set is order isomorphic to a subset of $\mathbb{Q}$ I'm assuming that this means using the normal less than relation on $\mathbb{Q}$ . Here is what I've tried so far. Attempt 1 Let $X$ be a countable well-ordred set. I tried to take $x_1 = min(X)$ , $x_2=min(X-\{x_1\})$ , etc.... but this process may not exhaust all of $X$ (e.g. if $X=\mathbb{N}\times\mathbb{N}$ ordered by $(a_1, b_1)<(a_2,b_2)$ if either (a) $a_1<a_2$ ; or (b) $a_1=a_2$ and $b_1<b_2$ . Attempt 2 I also tried to generalize this taking $x_1,x_2,...,x_\omega,x_{\omega+1}, ...$ , etc... and mapping this to elements of $\mathbb{Q}$ but even this process may not exhaust all of $X$ . E.g. if $X$ is set of all integer polynomials of the ordinals $\omega_1$ and $\omega_2$ this process will never get to polynomials with $\omega_2$ . Attempt 3 So I wasn't able to construct a subset of $\mathbb{Q}$ like this. I am trying to take a different approach. I was thinking I can use Theorem 3.6 in Weaver's book which states: Let $V$ and $W$ be well-ordered sets. Then exactly one of the following is true: V is order isomorphic to an initial segment of $W$ , $W$ is order isomorphic to an initial segment of $V$ , or $V$ and $W$ are order isomorphic to each other. But I'm not sure if this is a good approach and, if so, how to use it. Any hints/pointers in the right direction/etc... are appreciated.",['elementary-set-theory']
3521096,How can I calculate alpha and beta in this specific case?,"It seems impossible to calculate $\alpha$ and $\beta$ . Point $A$ divides $c$ line in half, $a+b=80=d$ , $h=20$ , $c=50$ I tried to work from the right triangle to the left one like this: $$\cos\alpha=\frac{x}{a}$$ $$\tan\beta=\frac{c}{2(h+x)}$$ I really don't want to put $\cos\alpha$ in the $\tan\beta$ equation, simply because I'm looking for $\alpha$ and $\beta$ and If I put them together, What do I do? Nothing. I'm missing something with $a+b=d$ . Any tips? Show your approach to this problem.",['geometry']
3521109,"Surface integral of $\left(0, \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right)$ over an ellipse","On $\mathbb{R}^3 \setminus \{0\}$ , define the vector field $$F(x,y,z) = \left(0,  \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right), $$ where $r = \sqrt{x^2+y^2+z^2}$ . Compute the integral of $F$ over the surface $$S: x^2 + y^2 + \frac{z^2}{4} = 1, z > 0. $$ The orientation does not matter. This question seems evil. How can one compute this in an easy way? Parametrizing the surface with spherical coordinates leads to a very difficult integral. And we cannot apply the divergence theorem, since we would have to close the surface, and then the domain enclosed by it will contain the origin.","['vector-fields', 'multivariable-calculus', 'surface-integrals']"
3521148,"Consider an entire function $f$ such that $|Re f(z)|\ge |Im f(z)|, \forall z\in\mathbb{C}$ with $|z|\ge M$. Show that $f$ is constant.","Consider an entire function $f$ such that $|Re f(z)|\ge |Im f(z)|,
 \forall z\in\mathbb{C}$ with $|z|\ge M\in\mathbb{R}$ . Show that $f$ is constant. My attempt: The inequality has a geometric interpretation: For all points $z$ outside of $B(0,M)$ we can only reach points for which the real part is greater than or equal to the imaginary part. Select $w=a+bi$ with $a>M$ and $b>2a$ , then the disk $B(w,M/2)$ is not reached by $f$ , which implies that $f$ is constant. Is this a good approach? Thanks.","['complex-analysis', 'solution-verification']"
