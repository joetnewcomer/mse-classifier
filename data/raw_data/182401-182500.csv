question_id,title,body,tags
3330637,Equality like Pascal triangle,"I have noticed the following is true. 
Let's denote the equation below as (1) $$\sum_{k=1}^{d+1}(-1)^{d+1-k}\frac{1}{(d+1)(k-1)!(d+1-k)!}\prod_{i=1}^{d+1}\Big(\frac{q}{h}+(k-i)\Big)\prod_{j=k}^{d}\Big(1-jh\Big)\prod_{\ell=d-k+2}^{d}\Big(1+\ell h\Big).$$ Let's denote the equation below as (2) $$\sum_{k=1}^{d}(-1)^{d-k}\frac{1}{(d)(k-1)!(d-k)!}\prod_{i=1}^{d}\Big(\frac{q}{h}+(k-i)\Big)\prod_{j=1}^{d}\Big(1+(k-j)h\Big).$$ If I replace $q\, \text{by}\, \frac{1}{x-1}$ in eq(1) and $q\text{by}\, 1/x$ and $h\,\text{by}\, \frac{(x-1)h}{x}$ in eq (2) we have tested for few values of $d$ $(x-1)^{d}\, eq(1) = x^d \,\text{eq (2)}$ Can I have a proof of this fact? Any help or why is true? 
Notice that eq (1) has $d+1$ terms and eq(2) has $d$ terms. So its seems like pascal triangle any combinatorial viewpoint?","['algebraic-combinatorics', 'combinatorics', 'combinatorial-proofs', 'mathematical-physics']"
3330641,$\cos(x)=-\frac{24}{25}$ and $\tan(y) = \frac{9}{40}$. Calculate $\sin(x) \cos(y) + \cos(x) \sin(y)$ and $\cos(x) \cos(y) - \sin(x) \sin(y)$.,"If $\cos(x)=  -\frac{24}{25}$ and $\tan(y) = \frac{9}{40}$ for $\frac{\pi}{2} < x < \pi$ and $\pi < y <\frac{3\pi}{2}$ . What is the value of $\sin(x) \cos(y) + \cos(x) \sin(y)$ and $\cos(x) \cos(y) - \sin(x) \sin(y)$ ? Solution: If $\frac{\pi}{2} < x < \pi$ then right triangle with acute angle $x$ facing west at second quadrant, the hypotenuse must be positive and the only negative is the adjacent. $\cos(x)=  -\frac{24}{25}$ then $adjacent=-24$ and $front=7$ so $\sin(x) = \frac{7}{25}$ . If $\pi < y <\frac{3\pi}{2}$ then right triangle with acute angle $x$ facing west at third quadrant, the front and adjacent are negative. If $\tan(y) = \frac{9}{40}$ then the front is $-9$ , the adjacent is $-40$ , and the hypotenuse is $41$ . So $\sin(y) = -\frac{9}{41}$ and $\cos(y)=-\frac{40}{41}$ The right triangle that we consider is the one with the adjacent side on the $x$ -axis right..?","['algebra-precalculus', 'trigonometry']"
3330658,How to evaluate $\prod_{n=1}^{p-1} \sin(\frac{2n^2\pi}{p})$,"According to Quadratic Gauss sum I want to know what is the exact value of this product?,since I put this product on Wolfram Alpha and I got the result in this form $$\frac{a+b\sqrt{
p}}{c}$$ for some positive integers $a,b,c$ But I dont know How a,b,c are related to each other.","['trigonometry', 'quadratic-reciprocity']"
3330731,"If A is a matrix such that $A^{2}+A+2I=O$ ,then $A$ can't be skew symmetric.","If $A$ is a matrix such that $A^{2}+A+2I=O$ , then $A$ can't be skew symmetric. (True/false) When $A$ is odd order matrix then the statement is true, since $A$ is non singular. $( |A||A+I| = (-2)^{n})$ and skew symmetric matrix of odd order is singular. How to check for even case $?$","['matrices', 'abstract-algebra', 'linear-algebra', 'matrix-equations', 'skew-symmetric-matrices']"
3330742,Proof of the Structure theorem of finitely generated graded modules over graded PID's [duplicate],"This question already has answers here : Decomposition of finitely generated graded modules over PID (2 answers) Closed 2 years ago . I am looking for a proof of the structure theorem of finitely generated graded modules over graded PID's: Let $R$ be a graded PID and $M$ be a finitely generated graded module over $R$ . Then $M$ decomposes uniquely as $$M \cong \bigoplus_{i=1}^n \Sigma^{\alpha_i} R\  \oplus\  \bigoplus_{j=1}^m \Sigma^{\gamma_i} R/d_j R$$ where $d_j \in R$ are homogeneous elements so that $d_j \mid d_{j+1}$ , $\alpha_i, \gamma_j \in \mathbb{Z}$ and $\Sigma^\alpha$ denotes an $\alpha$ -shift upwards in grading. But i can't find anything. Does anybody know where i could find a proof of this theorem? I know how to proof the usual structure theorem of finitely generated modules, is there an easy way to translate this result to the graded case ? I would guess that one obtains $M \cong R^n\ \oplus\ \bigoplus_{j=1}^m R/d_j R$ by the usal structure theorem , but the isomorphism is just a usal isomorphism of modules. To get a graded isomorphism one has to eventually shift the terms on the right side in grading.","['abstract-algebra', 'graded-modules', 'modules']"
3330755,The units digit of $1!+2!+3!+4!!+5!!+\dots+k\underset{\left \lfloor \sqrt{k} \right \rfloor \text{ times}}{\underbrace{!!!\dots!}}$,"For natural numbers $n\ge m$ , let $n\underset{m \text{ times}}{\underbrace{!!!\dots!}}=n(n-m)(n-2m)(n-3m)\dots$ where all factors are natural numbers (we exclude $0$ and negative factors). Question: $\text{What is the units digit of}$ $$1!+2!+3!+4!!+5!!+\dots+k\underset{\left\lfloor \sqrt{k} \right \rfloor \text{times}}{\underbrace{!!!\dots!}}+\dots+1992\underset{44 \text{
times}}{\underbrace{!!!\dots!}}\space\space\space\space\space?$$ $\big(\left \lfloor \cdot \right\rfloor \text{denotes the floor funtion}\big)$ . My Attempt (Is wrong as  Peter Foreman commented below): Consider the first $9$ terms: $1!+2!+3!+4!!+5!!+6!!+7!!+8!!+9!!!$ $=1+2+6+8+15+48+105+384+162=731$ Each of the remaining terms includes at least on factor that ends with $0$ . Therefore, the each term ends with $0$ . Hence the units digit of the given expression is equal to the units digit of the sum of the first $9$ terms. So, $1$ is the units digit of the given expression. Peter Foreman said: "" $17!!!!=9945$ "". This showed me that my attempt is wrong. Thanks  Peter Foreman. Any help would be appreciated. THANKS.","['ceiling-and-floor-functions', 'factorial', 'modular-arithmetic', 'number-theory', 'elementary-number-theory']"
3330757,"Proving $\int_{\sqrt{\frac{3}{5}}}^1 \frac{\arctan (x)}{\sqrt{2 x^2-1} \left(3 x^2-1\right)} \, dx=\frac{3 \pi ^2}{160}$","How to prove $$\int_{\sqrt{3/5}}^1 \frac{\arctan (x)}{\sqrt{2 x^2-1} \left(3 x^2-1\right)} \, dx=\frac{3\pi^2}{160}$$ I found the integral neat enough but also tough. Is it somehow related to the Ahmed integral $?$ Any help will be appreciated. Update: Please see the link under @pisco's answer for further reference.","['integration', 'closed-form', 'definite-integrals', 'special-functions']"
3330763,Differentiability of $\log|x|$,It is known that $$\dfrac{d}{dx}\log f(x) = \dfrac{1}{f(x)} f'(x)$$ but how does it is not applicable for $\log |x|$ as $|x|$ is not differentiable?,"['derivatives', 'logarithms']"
3330767,A Baire measure with different extensions to a Radon measure,"Let $X$ be a Hausdorff space. Let $\mu : \mathcal{B}a(X) \to [0, \infty)$ be a finite Baire measure where $\mathcal{B}a(X) := \sigma(C_b(X))$ is the $\sigma$ -algebra generated by the continuous (bounded) functions $C_b(X)$ . Facts: If $\mu$ is a tight Baire measure then $\mu$ has an extension to a Radon measure, i.e. a measure $\mu : \mathcal{B}(X) \to [0, \infty)$ defined on the Borel $\sigma$ -algebra $\mathcal{B}(X)$ that is inner regular with respect to the compact sets on all Borel sets, i.e. $\mu(B) = \inf \{ \mu(K) \mid K \subseteq B,\, K \textrm{ compact} \}$ for all $B \in \mathcal{B}(X)$ [Bogachev, ""Measure Theory"", 7.3.10]. If in addition $X$ is completely regular then this Radon measure extension is unique [Bogachev, 7.3.3] (there can may be also Borel measure extensions of $\mu$ that are not Radon). Moreover, two Radon measures coincide if and only if they already coincide on the compact sets. Is there an example of a Hausdorff space $X$ (that is necessarily not completely regular) and a Baire measure $\mu$ on $X$ that has two different Radon measure extensions? These Radon measures must be different on some compact set which is not contained in the Baire $\sigma$ -algebra.",['measure-theory']
3330784,Principal open subsets of affine quasi-projective varieties,"Assume that $U$ is an affine quasi-projective variety, so $U\subseteq Y$ for some projective variety $Y$ (say everythings is defined over an algebraically closed field $K$ ). Let $D_+(f)$ be a principal basic open subset of $Y$ ( $f$ is homogeneous) and assume that $D_+(f)\subseteq U$ . Is $D_+(f)$ a principal open subset of $U$ , i.e., does there exists $g\in K[U]$ such that $D_+(f)=\operatorname{Spec} K[U]_g$ ?","['algebraic-geometry', 'projective-varieties']"
3330805,If the cardinality of $f^{-1}$ is at most $f(x)^2$ then $f$ is differentiable almost everywhere.,"I came across the following problem in a prelim question paper. The question as stated seems meaningless to me, I am adding the picture so as to avoid any error from my end. My case with the above question is that $f$ is given to be defined on $(0,1)$ so $f(x)$ makes sense only if $x\in (0,1)$ . Although, it doesn’t tell s anything but if we assume therefore that the range is contained only in $(0,1)$ . But then it would mean that a point $x$ is taken only $0$ times which is again a nonsense. I am not sure if I am missing something or the question is really wrong. If it is wrong, what can be “the nearest correct” version of it? Meaning if we take the function to be defined from $\mathbb{R}$ to $(0,\infty)$ ?","['bounded-variation', 'functional-analysis', 'real-analysis']"
3330876,Conditional expectation of Borel function of two independent variables,"I am trying to solve the following problem: Let $X$ , $Y$ be two independent random variables, and let $f$ be a Borel function. Show that $$
\mathbb{E}[f(X,Y)|Y=t]=\mathbb{E}[f(X,t)].
$$ I have no clear idea where to start. By the definition of conditional expectation, we should have that $$
\int_{\{Y=t\}}f(X,Y)dP=\int_{\{Y=t\}}\mathbb{E}[f(X,Y)|Y].
$$ On the LHS I have what I wanted, i.e. $\mathbb{E}[f(X,t)]$ . But how do I proceed from here? Also, why is independence of variables important?","['conditional-expectation', 'probability-theory']"
3330901,What does it mean by 'monotone' when referring to functions?,"To clarify, I know what it means when a function is surjective, injective, bijective, and what its inverse is. This is something my module notes covers. However, I have a question (past exam paper) asking to show that a function is monotone . I have no clue what it means, and there is not a single mention of it in the module notes (provided by the professor, and I don't have access to a recommended textbook either). Please could someone explain what monotone means?","['functions', 'monotone-functions', 'discrete-mathematics']"
3330929,How could a vanishing cycle be zero?,"I am studying the book ""Etale Cohomology and the Weil Conjectures"" by Reinhard and Kiehl. For the following, let's assume $\Lambda$ is a finite group of order prime to the characteristic of all schemes we are working with. In chapter III, paragraph 4 is about a ""local"" Picard-Lefschetz formula which statement starts as following: Let S be the spectrum of a strictly Henselian local ring $R$ , $f: X \to S$ a proper flat morphism of odd fiber dimension, let $X_s$ be its special fiber and $X_\eta$ its generic fiber. Assume $f$ is such that it is singular on a single point $a$ , then the strict henselization of $\mathcal{O}_{X,a}$ is $R$ -isomorphic to $R[X_1,\ldots X_n]/\left(\sum\limits_{\nu = 0}^mX_\nu X_{\nu+m+1} + \lambda\right)$ for some $\lambda \in \mathfrak{m}(R)$ . Then $H^\nu(X_\eta,\Lambda) \cong H^{\nu}(X_s, \Lambda)$ for all $\nu \neq n, n+1$ , There is a ""vanishing cycle"" $\delta \in H^n(X_\eta, \Lambda)\otimes \Lambda(m)$ and a ""covanishing cycle"" $\delta^* \in H^{n + 1}(X_s, \Lambda(n-m))$ ... The vanishing and covanishing cycle are then constructed as follow: First recall the normal form of smooth quadrics: they are the zero sets in $\mathbb{P}^{n+1}$ of the $n+2$ variable quadratic forms: $Q = \sum\limits_{\nu=0}^mX_\nu X_{\nu+m+1} + x_{n+1}^2$ if $n = 2m+1 $ $Q = \sum\limits_{\nu=0}^mX_\nu X_{\nu+m+1}$ if $n = 2m$ . Now for a quadric $Y$ in the case $n = 2m$ , we consider the linear subspaces: $L_1: X_0 = \cdots = X_m = 0$ $L_2: X_0 = \cdots = X_{m-1} = X_{2m+1} = 0$ Let's call $\theta_1$ and $\theta_2$ the classes in $H^n(Y, \Lambda(m))$ associated to these cycles. They are such that $H^n(Y, \Lambda(m)) = \Lambda.\theta_1 + \Lambda.\theta_2$ . Now in an odd-dimensional quadric $X$ in normal form, the hyperplane section $X_{n+1} = 0$ may be identified with $Y$ , call $X_e$ the complement $X \setminus Y$ , it is an affine quadric defined by $\sum\limits_{\nu = 0}^m X_\nu X_{\nu+m+1} + 1 $ , one has a long exact sequence \begin{align*}
H_c^{\nu}(X_e, \Lambda(-)) \rightarrow H^{\nu}(X, \Lambda(-)) \rightarrow H^{\nu}(Y, \Lambda(-)) \overset{\partial}{\rightarrow} \cdots
\end{align*} The image $\theta$ of either $\theta_1$ or $\theta_2$ by $\partial$ in $H^{n}_c(X_e, \Lambda(m))$ is then a generator of it. Then, in the setting of the Picard-Lefschetz formula, the vanishing cycle is the image of $\theta$ in $H^n(X_\eta, \Lambda)$ (the morphism is induced by the same exact triangle but with $X_{\eta}$ instead of $X$ . The vanishing cycles are uniquely determined up to sign. Now in a latter theorem, there are some cases handling the case of a null vanishing cycle. It comes as a surprise to me, because I don't really see in the construction where the cycle could become zero. $\theta_1$ and $\theta_2$ determine the cohomology of a quadric, and they can't be $zero$ , same for $\theta$ I think (otherwise, by the long exact sequence, that would mean $\theta_1$ and $\theta_2$ are images of elements of $H^{\nu}(X, \Lambda())$ , which seems impossible to me because of their definition, but I might be wrong here. The last point where $\delta$ could become zero would be if the image of $\theta$ in $H^{n}(X_\eta, \Lambda(-))$ is zero, which would mean that $\theta$ comes from a cycle in $H^{n-1}(Y_\eta, \Lambda(-))$ , which could be possible since this group is $\Lambda \oplus \Lambda$ ( $n = 2m+1$ and $Y$ is a quadric which lands in the  case $2m$ , so its $H^{n-1}$ is generated by two cycles). My question is: I have no idea of Wether this is actually correct, i.e wether I have correctly identified the conditions for $\delta$ to be null or not. Whether the case I described can actually happens (I think it could since later theorems handle the case of a null vanishing cycle). What it would actually mean for a vanishing cycle to be zero, so far I only see an algebraic condition and I have no intuition on what it should mean, or what should cause it to happen besides ""it could just happens sometimes"". I have read this MO question to help build my intuition of the Picard-Lefschetz formula, but the picture given is making me think even more that the vanishing cycle can not vanish. So I would gladly accept any answer providing me with either comprehensive references on the vanishing cycles in this case (I tried to read the relevant part of SGA7, but the situation seems similar and I can not understand why a vanishing cycle could be zero as well), or explain to me how it could happen, what it would mean and what is known about this case (besides the consequences on the monodromy being ""the singularity does not twist the monodromy action""). Or even better, provide an example in which the vanishing cycle is zero.","['etale-cohomology', 'algebraic-geometry', 'intuition']"
3330947,"Finding maxima and minima of $f(x,y)=x^4+y^4-2x^2$","Finding maxima and minima of $f(x,y)=x^4+y^4-2x^2$ I tried studying this exam problem but I need help in understanding it. I found $f_{x}=4x^3-4x \\
f_y=4y^3$ From this, I get the stationary points $A(0,0)$ , $B(1,0)$ and $C(-1,0)$ $f_{xx}=12x^2-4 \\
f_{yy}=12y^2 \\
f_{xy}=0$ Now after computing $D(x,y)$ I got $D(A)=D(B)=D(C)=0$ which is inconclusive. In the graph, it shows that the point $B$ and $C$ are local minimum. I would really appreciate some good explanation because I have an exam soon and I would like some help.","['multivariable-calculus', 'calculus', 'functions', 'maxima-minima']"
3330960,Integrals Over Complex Domains,"In Complex Analysis texts, one often sees integrals of functions $f : A \subset \mathbb{C} \to \mathbb{C}$ along contours $\gamma : [0,1] \to \mathbb{C}$ . But I've never seen discussion of integrals of functions $f : A \subset \mathbb{C} \to \mathbb{C}$ over domains $D \subset A$ in any of these texts. It seems like it should be possible to define such integrals, so why aren't they discussed in most complex analysis textbooks? Are there any major results about them like, for example, Cauchy's integral formula for contour integrals? Are there any textbooks that do delve into these kinds of integrals? Examples By a contour integral, I mean something like the following: consider the contour $\gamma : [0,1] \to \mathbb{C}$ given by $\gamma(t)  = e^{2\pi i t}$ and the function $f : \mathbb{C} - \{0\} \to \mathbb{C}$ given by $f(z) = \frac{1}{z}$ . The integral of $f$ around the contour $\gamma$ is $$
\int_\gamma f = \int_0^1 f(\gamma(t))\gamma'(t) dt = \int_0^1 \frac{1}{e^{2\pi i t}} \frac{2\pi i e^{2\pi i t}}{1} dt = \int_0^1 2\pi i dt = 2\pi i
$$ By an integral over a domain, I mean something like the following: consider the function $g : \mathbb{C} \to \mathbb{C}$ given by $g(z) = z^2$ and the domain $D = \{z \in \mathbb{C} : |z| \leq 1\}$ . The integral of $g$ over the domain $D$ is $$
\int_D g d\mu \stackrel{?}{=}
$$ where, presumably, $d\mu$ is something like the Haar measure on $\mathbb{C}$ with $\mu([0,1]\times[0,1]) = 1$ .","['integration', 'complex-analysis', 'reference-request']"
3330963,Pullback of pushforward $\sigma$-algebra,"Suppose $g:X\rightarrow Y$ is any function and $A$ is a $\sigma$ -algebra on X. Let $U$ be the push forward $\sigma$ -algebra on $Y$ . i.e., $U=\{B\subset Y|g^{-1}(B)\in A\}$ Is the pull-back $\sigma$ -algebra of U i.e., $A'=\{l\subset X|l=g^{-1}(B) ,B\in U\}$ equal to A? I don't think its true but I can't find any counter examples. I can show that $A'\subseteq A$ but I can't show that $A\subseteq A'$ . Any suggestions?",['measure-theory']
3330977,An integral for the difference of zeta functions $\zeta (s-1)-\zeta(s)$,"Starting with: $$\zeta (s)={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }{\frac {x^{s-1}}{e^{x}-1}}\,\mathrm {d} x$$ How can we prove for $s > 2$ the following conjecture: $$\zeta (s-1)-\zeta(s)={\frac {2}{\Gamma (s+1)}}\int _{0}^{\infty }{\frac {x^s e^x}{(e^{x}-1)^3}}\,\mathrm {d} x$$ Integration by parts may be a way to go with: $$u=x^s \qquad du=s x^{s-1} \\ dv=\frac {e^x dx}{(e^{x}-1)^3} \qquad v=- \frac{1}{2} \frac {1}{(e^{x}-1)^2}$$ Which gives us: $$\zeta (s-1)-\zeta(s)={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }{\frac {x^{s-1} }{(e^{x}-1)^2}}\,\mathrm {d} x$$ It's a little nicer, but I'm not sure how to prove it either. It just came to me: we should probably do another integration by parts, that's it.","['integration', 'riemann-zeta']"
3330984,"Why is the ""q-Pochhammer Symbol"" referred to as such, despite not being a q-analog of the Pochhammer symbol?","The q-Pochhammer symbol (for $k>0$ ): $(a;q)_k = \prod\limits_{j=0}^{k-1}\left(1-aq^j\right)$ The Pochhammer symbol / rising factorial: $(a)_k = \prod\limits_{j=0}^{k-1}(a+j)$ The falling factorial (sometimes identified with the Pochhammer symbol instead of the above): $a^\underline{n} = \prod\limits_{j=0}^{k-1}(a-j)$ The q-Pochhammer symbol is supposedly the q-analog of the Pochhammer symbol, meaning if we set $q=1$ , we get the Pochhammer symbol. Wolfram MathWorld makes this claim, as does Wikipedia and this very site . Yet it it clearly false. For example, $(3;1)_2 = 4$ whereas $(3)_2 = 12$ and $3^\underline{2} = 6$ . So we have an inappropriate name (""q-Pochhammer symbol"", which suggests it is a q-analog) and a clearly false statement (that it is indeed a q-analog), both apparently widespread and generally accepted. What's going on here?","['pochhammer-symbol', 'combinatorics', 'q-analogs']"
3331021,sigma algebra generated by a set example,"I wanted to check my understanding of this concept. The sigma algebra $M(\psi)$ generated by a set $\psi$ is the intersection of all sigma-algebras that contain $\psi$ . As an example if $X=\{1,2,\dots,6\}, \psi=\{\{2,4\},\{6\}\}$ then $M(\psi)$ would be the sigma algebra that is the intersection of all sigma algebras that contain $\psi$ . For a sigma algebra to contain $\psi$ it must (if I've calculated correctly) contain $\{\psi, \emptyset, \{2,4,6\},\}$ and then also the complements of these and the complements of $\{2,4\}$ and $\{6\}$ . Other than direct computation is there a faster way to calculate this?",['measure-theory']
3331063,Generalizing a function that operates on functions,"I've not studied much beyond Calculus so my notation and terminology may be rough/incorrect.
I've got a 'function' $\mathfrak F$ that uses other functions and goes like this: $$
    \mathfrak F(f,n) = D_x^nf \\
    f:\Bbb R \rightarrow \Bbb R \\
    n \in \Bbb Z
$$ Some examples: $$
    f(x) = x^2 \\
    \mathfrak F(f,2) = D_x^2x^2 = 2 \\
    \mathfrak F(f,-3)= D_x^{-3}x^2 = \frac{1}{60}x^5 + \frac{C_1}{2}x^2+C_2x+C_3
$$ My questions are: 1. What's the proper notation for this? 2. How can I write this so that $n \in \Bbb R$ or even $n \in \Bbb C$ ? 3. How can I write this so that $f$ is multi-variable? Finally, 4. Would this 'function' be differentiable along $n$ ( $n \in \Bbb R$ )?","['calculus', 'functions', 'smooth-functions', 'functional-analysis']"
3331112,"Calculating $\int \frac{e^{2x}}{1+e^x} \, dx $","Problem: Calculate $\displaystyle\int \frac{e^{2x}}{1+e^x} \, dx $ . My book says to divide in order to solve by getting $\displaystyle\int\left(e^x-\frac{e^{x}}{1+e^x}\right)\, dx $ but how am I supposed to divide? I tried long division but the exponents have variables so I don’t think you can use long division (plus my answer from long division didn’t match).","['integration', 'calculus', 'algebra-precalculus']"
3331113,Evaluate $\int_{0}^{\frac{\pi}{4}}\tan xdx $ using idea of Riemann Sum,"Evaluate $$\int_{0}^{\frac{\pi}{4}}\tan x\,dx$$ using Riemann Sum. My Attempt: $$\int_{0}^{\frac{\pi}{4}}\tan x\, dx=\frac{\pi}{4}\int_{0}^1\tan\left(\frac{\pi}{4}x\right)dx=\frac{\pi}{4}\lim_{n\to\infty}\frac{1}{n}\sum_{r=1}^{n}\tan\left(\frac{{\pi}r}{4n}\right)$$ After this I am not able to proceed","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3331114,How to extend continous function from $S^1\to S^1$ to $D^2\to D^2$ continously,"Let $f:S^1\to S^1$ is the continous map then I have to prove that there exist continous extension $\bar f$ of f such that $\bar f:D^2\to D^2 $ is continous map  where $D^2$ is closed disc in $\mathbb R^2$ I had following idea . we can map origin to origin.then suppose there is ray emaniting form origin to some point a, its image is ray emaniting form origin to f(a) and same will follows. But I could not able to write explicit form of function. Is my idea is correct? Can anyone please help me how to write explicitly map. Any Help will be appreciated","['general-topology', 'algebraic-topology', 'real-analysis']"
3331146,Calculate $\sum_{n \in A} 2^{-n}$,"Calculate $\sum_{n \in A} 2^{-n}$ where $$A = \Big\{ n \in \mathbb N : \big| \big\{ i \in \{ 2,3,5,7\} : i \mid n  \big\} \big| = 2 \Big\}.$$ Observation: $$ x \in A \rightarrow x = p_1^{a_1} p_2^{a_2}R  $$ where $$ p_1 \neq p_2 \wedge p_1, p_2 \in \left\{ 2,3,5,7\right\} \wedge  \forall y \in \left\{ 2,3,5,7\right\} y \mbox{ is not divisor of } R$$ In my opinion this sum diverges: but I am not sure how can I prove that.","['summation', 'discrete-mathematics', 'real-analysis']"
3331149,Monodromy of local system is well defined,"I want to understand, why local systems (= locally constant sheafs) with values in a vector space $V$ define a monodromy representation $\rho: \pi_1(X) \to \operatorname {GL}(V)$ . I already know, that any local system on the interval $I=[0,1]$ and on $I^2$ is constant. So let $\mathcal F$ be a local system on $X$ and $\gamma: I \to X$ a loop with base point $x_0$ . Then $\gamma^{-1} \mathcal F$ is a local system on $I$ and thus constant. Furthermore, the stalks of a inverse image sheaf are given as $$(\gamma^{-1} \mathcal F)_t = \mathcal F_{\gamma(t)}.$$ As for a constant sheaf the restriction map from global sections to stalks is an isomorphism, we get an isomorphism $$\mathcal F_{x_0} = (\gamma^{-1} \mathcal F)_0 \simeq \Gamma(\gamma^{-1} \mathcal F, I) \simeq (\gamma^{-1} \mathcal F)_1 =  \mathcal F_{x_0}. $$ Why does this isomorphism only depend on the homotpy class of $\gamma$ ? Let $H: I \times I \to X$ be a homotopy between $\gamma$ and $\gamma'$ , both loops with base point $x_0$ .
Then again, $H^{-1} \mathcal F$ is a constant sheaf, and we obtain isomorphisms $$\mathcal F_{x_0} = (H^{-1} \mathcal F)_{(0,0)} \simeq \Gamma(H^{-1} \mathcal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    F, I^2) \simeq (H^{-1} \mathcal F)_{(1,0)}= \mathcal F_{x_0}$$ and $$\mathcal F_{x_0} = (H^{-1} \mathcal F)_{(0,0)} \simeq \Gamma(H^{-1} \mathcal F, I^2) \simeq (H^{-1} \mathcal F)_{(0,1)}= \mathcal F_{x_0}.$$ I need to show that one of the isomorphisms above is the identity and the other is the one given by $\gamma$ .","['sheaf-theory', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
3331209,Comparison of two quadratic forms,Let $n \times n$ matrix $A$ be positive semidefinite and let $P$ be an orthogonal projector of some subspace of $\mathbb{R}^n$ into $\mathbb{R}^n$ .  Is the following correct? $$\left( \forall x \in \mathbb{R}^n \right) \left( x^T P A P x \leq x^T A x \right)$$,"['matrices', 'linear-algebra', 'positive-semidefinite', 'quadratic-forms']"
3331210,Equivalent definitions of foliation,"I am having trouble proving the equivalence of two different definitions of foliation. The first one is the one where you define a k-foliation on a smooth manifold $M$ with k-dimensional leaves such that every point has a local chart that sends all these submanifolds in ""horizontal subspaces"" of $\mathbb{R}^n$ . The second definition is the one where you require an atlas with transition functions like $\phi_{ij}(x,y)=(\phi^1 _{ij}(x,y), \phi^2 _{ij}(y))$ . I am not being formal because I think these are quite common definitions, and you know what I am talking about. The problem is proving that the second definition implies the first. The teacher suggests taking different topologies on $\mathbb{R}^k$ and $\mathbb{R}^{n-k}$ and then the product topology, but it is a quite short hint and I really don't understand what to do. Also, it is pointed out that the biggest problem here would be the second countability, but I can't understand the reasoning I should do. This equivalence is stated right after the two definitions are given, so there shouldn't be any need of particular results concerning foliations.","['manifolds', 'foliations', 'smooth-manifolds', 'differential-geometry']"
3331214,How can calculate $ a \mod n^2 $ be computed smartly if we know $a \mod n$?,By example: How can $ 2^{65536} \mod 7^2 $ be computed smartly? I think that there is a faster method than writing next element and find cycles. I found that $\mod 7 :$ $$ \color{red}{ 2^{0} \equiv 1\\2^{1} \equiv 2\\2^{2} \equiv 4}\\2^{3} \equiv 1 $$ and by these equations each number can be fastly computed for example: $$2^{65536} = 2^{21845\cdot 3 +1 } \equiv 2^1 \equiv 2 $$ but how can it be applied for $\mod 49$ ?,"['modular-arithmetic', 'discrete-mathematics']"
3331264,Handle a function of itself,"I have a tricky question. I want to handle a function y(x) defined in this way: $y(x)=f(y(x))g(x)$ Here, $f(y(x))$ and $g(x)$ are smooth functions: is there any way/method to express $y(x)$ as a (complicated) function of $x$ solely?
Maybe it is impossible if $f$ and $g$ are generic. Edit: If I have the numerical functions which describe $f(y)$ and $g(x)$ , is there any approximated way to express $y(x)$ ? For example with recursive formulae?
My point is not to guess what form $y(x)$ should have, but deriving it or, at least, inferring it approximately.","['numerical-methods', 'functions', 'approximation']"
3331319,"A binary operation on $G$ satisfies $a\cdot(b\cdot c)=(a\cdot b)\cdot (a\cdot c)$ and has an identity, is it a group?","A binary operation on $G$ satisfies $a\cdot(b\cdot c)=(a\cdot b)\cdot (a\cdot c)$ and has an identity, is it associative? Is it a group? I don't think it is associative but can't produce a convincing argument. Any help would be appreciated! Thanks,","['monoid', 'abstract-algebra', 'semigroups', 'binary-operations', 'group-theory']"
3331320,Property of distributions over R x R with identical marginal distributions,"Let $D_1$ and $D_2$ be probability distributions on $\mathbb{R} \times \mathbb{R}$ with identical marginal distributions (i.e. the distribution of the first component of $D_1$ is the same as the distribution of the first component of $D_2$ , and similarly for the second components). Let $e_1 = \mathbb{E}_{ D_1} [x - y]$ and $e_2 = \mathbb{E}_{ D_2} [x -y]$ . (Assume that these quantities are defined -- they could be finite or infinite.) Does $e_1$ necessarily equal $e_2$ ? (Note that this does not immediately follow from linearity of expectation, because $\mathbb{E}[x - y]$ could be defined even if $\mathbb{E}[x]$ and $\mathbb{E}[y]$ are not.)","['statistics', 'marginal-distribution', 'real-analysis', 'probability-theory', 'probability']"
3331326,Continuous choice of elements in the kernel of linear maps,"Let $\Omega\subseteq\mathbb R^l$ be open and $A:\Omega\to \mathbb R^{n\times n}$ be a matrix valued $C^k$ -function, $k\in\mathbb N_0\cup\{\infty\} $ such that for all $p\in\Omega$ $A_p$ is not injective or equivalently $\det (A_p)=0$ . Then for all $p$ we can find $x_p\in  \mathbb R^n\setminus\{0\}$ with $A_p(x_p)=0$ . The question now is: Can the $x_p$ be choosen such that $p\mapsto x_p$ is also $C^k$ ? $\textbf{Edit:}$ As Aloizio Macedo's  answer shows this is not true in general. I am still interested in the case $\Omega=\mathbb R$ though since  this is what I  originally had in mind.","['analysis', 'continuity', 'multivariable-calculus', 'linear-algebra', 'differential-geometry']"
3331345,"Integer points on $y^2=ax^2+bx+c$ ($a,b,c\in\mathbb{N}$, $\Delta>0$)","I have read about a theorem of Siegel on this Terence Tao's blogpost : Theorem (Siegel’s theorem on integer points) Let ${P \in {\bf Q}[x,y]}$ be an irreducible polynomial of two variables, such that the affine plane curve ${C := \{ (x,y): P(x,y)=0\}}$ either has genus at least one, or has at least three points on the line at infinity, or both. Then ${C}$ has only finitely many integer points ${(x,y) \in {\bf Z}^2}$ . Question : conversely, if the genus is $g\leq0$ does that imply that the number of integer points is necessarily infinite  ? For example, consider the curve $y^2=ax^2+bx+c$ with $a,b,c\in\mathbb{N}$ and the discriminant of the RHS polynomial in $x$ is $\Delta>0$ . If I understand well, it has degree $2$ , so it has genus $g=\frac{(2-1)(2-2)}{2}-s=-s$ , where $s$ is the ""number of singularities, properly counted"". So here $g\leq 0$ . Is the number of integer points then infinite for all those curves ?","['number-theory', 'algebraic-geometry']"
3331377,"How many groups of order at most $25$ are ""pleasant"" (abelian, with every non-identity element having prime order)?","A group $G$ is called pleasant if it is abelian and every non-identity element $g$ in $G$ has prime order. Up to isomorphism, how many pleasant groups are there of order at most $25?$ Options: $0, 9, 16, 25, 31$ , or infinitely many. I got $15.$ Any help would be greatly appreciated! Thanks, I got: the trivial one, cyclic groups of prime order up to $25$ (there are $9$ ) and also $C_2 \times C_2$ , $C_2 \times C_2 \times C_2$ , $C_2 \times C_2 \times C_2 \times C_2$ , $C_3 \times C_3$ and $C_5 \times C_5$ . Thank you again!","['group-theory', 'finite-groups']"
3331399,"Solve $c(2x)-2c(x)=2\alpha \int_0^{2x}e^{c(z)}\ z\ dz,$ $x \in (0,\infty)$, $\alpha \in [-1, 1]$.","I want to solve for the measurable function $c(x)$ for the following equation: $$c(2x)-2c(x)=2\alpha \int_0^{2x}e^{c(z)}\ z\ dz.$$ Where $x \in (0,\infty)$ and $\alpha \in [-1, 1].$ So I continued by assuming $c\in \mathcal{C}^2$ then: $$c'(2x)-c'(x)=4\alpha\ x\ e^{c(2x)}$$ Differentiating again: $$2c''(2x)-c''(x)=4\alpha\ \left(e^{c(2x)}+2x\ e^{c(2x)}\ c'(2x)\right)$$ Letting $w=2x$ and substituting $c'(2x)-c'(x)=4\alpha\ x\ e^{c(2x)}$ we get: $$2c''(2x)-c''(x)={c'(2x)-c'(x)\over x}+2c'(2x)(c'(2x)-c'(x))$$ $$wc''(w)-{w\over 2}c''({w\over 2})=c'({w})-c'({w\over 2})+wc'(w)^2-wc'(w)c'({w\over 2})$$ Any hints on how to solve the DDE given a boundary conditions of $c(0)=0$ and $c(\infty)=-2\alpha.$","['measure-theory', 'delay-differential-equations']"
3331412,Integral with respect to spectral measure,"Let $A:D(A)\subset H \to H$ be a self-adjoint unbounded operator on complex Hilbertspace $H$ with corresponding spectral measure $E:\mathcal{B}(\mathbb{R})\to\mathcal{L}(H)$ . I want to show  that an element $u\in H$ with $\Vert u \Vert =1$ and $E(I)u=u$ for some open interval $I$ is $ u \in D(A)$ , that is $$\int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2  < \infty.\quad (*)$$ I can think of two approaches: measure theory
 or generalisation of Riemann–Stieltjes integral. From m.t. point of view the integral in (*) means $$ \int_\mathbb{R} \lambda^2 d \mu_u $$ where $\mu_u$ is a measure on Borel sets  defined by $\mu_u(S)=\langle E(S)u,u\rangle.$ In order to calculate this integral I can use monotone convergence theorem. Since it's only matter of construction, let's assume we found a sequence $f_k:\mathbb{R}\to \mathbb R $ with $f_k \uparrow \lambda^2$ , so we may write $$ \int_\mathbb{R} \lambda^2 d \mu_u=\lim \int_\mathbb{R} f_k d \mu_u .$$ But that isn't really helpfull... If we try second approach, then (*) is defined as $$ \int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2=\lim_{\text{ partition of } \mathbb{R}  \to 0} \sum t_i^2( \Vert E(\lambda_i)u \Vert^2-\Vert E(\lambda_{i-1})u \Vert^2 ), $$ where $t_i\in (\lambda_{i-1}, \lambda_i)$ and $E(\lambda):=E((-\infty,\lambda])$ . Again it's not obvious for me that this leads anywhere. Any help, hints or suggestions are greatly appreciated! Edit 1: Let's try again. @N.S. comment suggests, it suffices to show $E(I)u \in D(A)$ , that is $$\int_\mathbb{R} \lambda^2 d \mu_{E(I)u}< \infty.$$ We note $\mu_{E(I)u}(S)=\langle E(I\cap S)u,u \rangle=\mu_u(I\cap S)=:\nu_u(S)$ for any measurable set $S$ . Coincidentally $\nu_u$ is also a measure! Question 1: Is $$ \int_\mathbb{R} \lambda^2 d \mu_{E(I)u}= \int_\mathbb{R} \lambda^2 d \nu_u=\int_{I} \lambda^2 d \mu_u \leq sup_{\lambda \in I} \{\lambda ^2\} \mu_u(I) <\infty$$ true? If so, how is second ""="" justifiable?","['measure-theory', 'self-adjoint-operators', 'functional-analysis', 'unbounded-operators', 'spectral-theory']"
3331413,Variations Calculus. How to compute some variations?,"I have the following situation: I'm trying to prove last point: the metric is invariant to re-parameterization of $f$ . So, the left member can be rewritten in the following way: $$a^2 \int^{1}_{0}{\delta((r_{1}\circ \gamma)\dot{\gamma})\delta((r_{2}\circ \gamma)\dot{\gamma})\frac{1}{r(t)}\text{d}t}+b^{2}\int^{1}_{0}{\delta(\Theta_{1} \circ \gamma)\delta(\Theta_{2} \circ \gamma)r(t)
\text{d}t}$$ and i tried $\delta((r_1 \circ \gamma)\dot{\gamma})=\delta(r_1 \circ \gamma)\dot{\gamma}+(r_1 \circ \gamma)\delta(\dot{\gamma})$ ? I am thinking is not OK. Any help it will be appreciated.","['integration', 'calculus-of-variations', 'metric-spaces', 'analysis', 'derivatives']"
3331425,Is $z=e^{\frac{1}{\log(x)}}$ a solution to an algebraic equation?,"Is $z=e^{\frac{1}{\log(x)}}$ with $x\in\Bbb Q~\cap(0,1)$ a solution to an algebraic equation? Wolfram is telling me $z$ is algebraic but I'm not sure that I believe this. I believe it would follow from Schanuel's conjecture, $$\mathbb{Q}(\ln x,\mathrm{e}^{1/\ln x})$$ has transcendence degree $2$ whenever $x$ is algebraic. Edit 9/22/2021: Transform this into $\log(z)\log(x)=1.$ We can inspect that this is of the form of a hyperbola ( if we use a change of coordinates to linearize the problem). My intuition tells me that there are two ""incompatible"" structures meshed together which need to be untangled. Let's step back and view this problem from a geometric standpoint in higher dimensional space. I will take two distinct spaces, that each have linear structures on them. The mathematical object $\zeta^2$ is simply related to $\Bbb R^2$ in the following way: $$ \exp: \Bbb R^2 \to \zeta^2$$ and this is a diffeomorphism (from a manifold theory standpoint alone). Due to the ""universality"" of the $\exp$ map, Postulate: Every linear structure on $\Bbb R^2$ can be re-constituted as a linear structure on $\zeta^2.$ For example $\exp$ is a group isomorphism and the group structure on $\Bbb R^2$ can be pushed onto an isomorphic group structure on $\zeta^2$ . Using this key fact, a field structure can be encoded in $\zeta^2$ by way of the group structure and constructing a multiplication map. I've proved that a real vector space structure can be placed on $\zeta^2.$ So we have two disjoint objects each with linear structures on them, and a map between these objects. What I believe is the main issue is that $z$ itself is a mixture of two incompatible linear structures (in dim. 1 as opposed to dim. 2) in a way that makes it nearly impossible to determine whether or not $z$ is algebraic. So the objects $\Bbb R^2$ and $\zeta^2$ can be viewed as inherently different structures with their own linear structures on them and a map between the two objects. What are we doing by inserting $\Bbb Q$ into the variable slot for $x$ ? $$z=e^{\frac{1}{\log(x)}}$$ Maybe we're forcing this inherently nonlinear structure (i.e. nonlinear analytic curve with respect to $\Bbb R^2$ ) of $\varphi(x)=e^{\frac{1}{\log(x)}}$ to obey the classical Riemannian metric $ds^2=dx^2+dy^2.$ Using this train of ideas, I have reason to believe that thinking strictly in terms of $\Bbb R^2$ and $\zeta^2$ as different objects, that $\varphi(x)$ is an algebraic curve with respect to $\zeta^2.$ Once I embed this algebraic curve into $\Bbb R^2$ with $\Bbb R^2$ 's  usual metric something is lost and I can't reconcile it. It should be clear that $\varphi(x)$ is not a polynomial in $\zeta^2$ rather its a hyperbola, so it's an algebraic curve in $\zeta^2.$ And we're inserting rational elements with respect to $\Bbb R^2$ into this algebraic curve's independent variable slot (with respect to $\zeta^2$ ).","['abstract-algebra', 'polynomials', 'rational-numbers']"
3331440,Embedding of $\mathrm{SU}(2)$ in $\mathrm{SU}(3)$,"There is an embedding of $\mathfrak{su}(2)$ in $\mathfrak{su}(3)$ given by sending the standard basis $$
\begin{pmatrix}i/2&0\\0&-i/2\end{pmatrix},\quad \begin{pmatrix}0&-1/2\\1/2&0\end{pmatrix},\quad\begin{pmatrix}0&-i/2\\-i/2&0\end{pmatrix}
$$ of $\mathfrak{su}(2)$ to $$\begin{pmatrix}i\\&0\\&&-i\end{pmatrix},\quad\frac{1}{\sqrt{2}}\begin{pmatrix}&-1\\1&&-1\\&1\end{pmatrix},\quad-\frac{i}{\sqrt{2}}\begin{pmatrix}&1\\1&&1\\&1\end{pmatrix}.
$$ in $\mathfrak{su}(3)$ . Since $\mathrm{SU}(2)$ is simply connected, this lifts to a Lie group homomorphism $$
\varphi:\mathrm{SU}(2)\longrightarrow\mathrm{SU}(3).
$$ Question. Is there an explicit formula for this map? We can express any matrix in $\mathrm{SU}(2)$ as $$
\begin{pmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{pmatrix}
$$ where $\alpha,\beta\in\mathbb{C}$ and $|\alpha|^2+|\beta|^2=1$ . Are there explicit expressions for the coordinates of $\varphi(\begin{smallmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{smallmatrix})$ in terms of $\alpha,\beta$ ? Using the exponential map, we can find special cases, such as $$
\varphi\begin{pmatrix}\alpha&0\\0&\bar{\alpha}\end{pmatrix}=\begin{pmatrix}\alpha^2\\&1\\&&\bar{\alpha}^{2}\end{pmatrix}
$$ but I am having some difficulties finding the general formula.","['lie-algebras', 'linear-algebra', 'group-theory', 'lie-groups', 'differential-geometry']"
3331450,Points in $\Bbb {R}^2$ that can be reached via steps which are $1/5$ of a unit circle.,"I was playing around with this demo of Project Euler Problem 208 which allows you to take steps which are ""left"" or ""right"" arcs of $1/5$ of a unit circle. Here's an example walk, which starts at the blue dot pointing vertically up, and which consists of steps RLLLRLLLRRLLLRLLLRRLRLLLR Question Which points in $\mathbb R^2$ can be reached in a finite number of steps, starting at the origin and pointing in the positive $y$ -direction. Is this set of points dense in $\mathbb R^2$ ? If not, what's the greatest number of points that can land in $[0,1] \times [0,1] \subset \mathbb R^2$ ?",['geometry']
3331455,"The kernel of $\mathbb{Q}[x,y]\rightarrow \mathbb{Q}(t)$ is $(x^2+y^2-1)$.","We define a ring homomorphism $F : \mathbb{Q}[x,y]\rightarrow \mathbb{Q}(t)$ by mapping $f(x,y)$ to $f(\frac{1-t^2}{1+t^2},\frac{2t}{1+t^2})$ . Show $\ker F=(x^2+y^2-1)$ . Proof) Since $(\frac{1-t^2}{1+t^2})^2+(\frac{2t}{1+t^2})^2-1=0$ , $(x^2+y^2-1)\subset \ker F$ . We note $\ker F$ is a prime ideal since it is the kernel of a ring homomorphism from a commutative ring into a field and $(x^2+y^2-1)$ is also prime as it is irreducible. If $(x^2+y^2-1)\neq \ker F$ , since $\dim \mathbb{Q}[x,y] = \dim \mathbb{Q}+2=0+2=2$ (where by dim, we mean the Krull dimensions of rings), $\:\:0\subsetneq (x^2+y^2-1) \subsetneq \ker F$ and $\ker F$ is a maximal ideal. So it now suffices to show that $\ker F$ is not maximal. Suppose it is maximal. Then in $\mathbb{Q}[x,y]/\ker F$ , $y + \ker F$ is a unit. Then there exists a polynomial $g(x,y)\in \mathbb{Q}[x,y]$ such that $yg(x,y) + \ker F = 1 + \ker F$ . So we have $yg(x,y)-1\in \ker F$ . But $\frac{2t}{1+t^2}g(\frac{1-t^2}{1+t^2},\frac{2t}{1+t^2})-1$ cannot be $0$ (if so, by clearing the denominators, $2th(t)=(1+t^2)^n$ for $n\in \mathbb{N}$ , $h(t)\in \mathbb{Q}[t]$ but the irreducible element $t$ in $\mathbb{Q}[t]$ cannot divide $(1+t^2)^n$ ), a contradiction. Could anyone read my proof and let me know any wrong point? If the proof is wrong, I hope you could give me any help. I tried more elementary way to prove the statement and failed.","['proof-verification', 'ring-theory', 'abstract-algebra', 'ideals', 'commutative-algebra']"
3331502,extracting a coefficient from formal power series multiplication,The Questions is Compute the value of $[x^n] \frac{(1+x)^n}{1-x} $ The solution is as follows: $[x^n] \frac{(1+x)^n}{1-x} $ = $[x^n](1+x)^n(1 + x + x^2 + x^3 + . . . ) $ = $\sum_{k=0}^n [x^k](1+x)^n * [x^{n-k}](1 + x + x^2 + x^3 + . . . ) $ = $\sum_{k=0}^n {n \choose k}(1)$ = $2^n $ Could someone please explain how this sum is formed?,"['combinatorics', 'discrete-mathematics']"
3331508,Relating changes in unit vectors on a surface to the surface curvature,"I was reading today, and I came upon this identity: $\frac{\partial \mathbf e_p}{\partial p} + \frac{\partial \mathbf e_q}{\partial q} = -(\nabla \cdot \mathbf n) \mathbf n$ , where $\mathbf e_p$ and $\mathbf e_q$ are unit vectors in the directions p and q of a general surface $x(p,q,t)$ and $\mathbf n$ is the unit vector normal to the surface, such that $\mathbf n = \mathbf e_p \times \mathbf e_q$ .  The unit vector $\mathbf n$ points in the direction that the surface is propagating. I'd like to figure out how to prove this mathematically, but also understand it intuitively.  I see the curvature, in the form of $\nabla \cdot \mathbf n$ , but I haven't gotten any further. Edit: Here is my idea for a possible solution, but I think it is still a bit hand-wavy, so hopefully someone will be able to sharpen up the details. First of all, I could align $\mathbf e_p$ and $\mathbf e_q$ with the principle directions of the surface.  Then, the two principle curvatures would each have a magnitude given by $||\frac{d\mathbf T}{ds}||$ , where $\mathbf T$ is the tangent vector to the surface in the principle direction.  In the p direction this magnitude would be $||\frac{\partial \mathbf e_p}{\partial p}||$ and in the q direction it would be $||\frac{\partial \mathbf e_q}{\partial q}||$ .  I know that the mean curvature is given by $H=\kappa_1 + \kappa_2$ , where 1 and 2 are the principle directions, so, with my alignment of p and q, $H=\kappa_p + \kappa_q$ .  Since $H=\nabla \cdot \mathbf n$ (up to a sign (?), and a factor that depends on convention), this suggests that I'm nearly there, but I need to get the sign and direction right. The Direction The change in a tangent vector should point in either the n or -n direction.  This is well illustrated by the answer to What is the intuition behind the unit normal vector being the derivative of the unit tangent vector? So at this point I'm happy with $\frac{\partial \mathbf e_p}{\partial p} + \frac{\partial \mathbf e_q}{\partial q} = (\nabla \cdot \mathbf n) \mathbf n$ up to a sign.  The question is, what should the sign really be? This is where I start to get somewhat confused. The Sign In the set up described here, there is a surface with the normal vector pointing in the direction of propagation.  Imagine that the surface is shaped like a hill.  Then $\nabla \cdot \mathbf n$ will be positive, because moving the normal in any coordinate direction (x,y,z), assuming that z is upwards, will make the component of the normal in that direction larger.  Likewise, if I imagine a valley, $\nabla \cdot \mathbf n$ is negative.  (So at this point I think that the choice of the direction of n determines the sign of $\nabla \cdot \mathbf n$ . Is that really true?). But if I look at $\frac{\partial \mathbf e_p}{\partial p}$ and $\frac{\partial \mathbf e_q}{\partial q}$ for the hill case, it seems like  they should point in the - n direction because the change in the tangent vector points into the hill.  This would imply that there should be a negative sign because $\nabla \cdot \mathbf n$ is positive, but I need the final result to be in the - n direction.  So: $\frac{\partial \mathbf e_p}{\partial p} + \frac{\partial \mathbf e_q}{\partial q} = -(\nabla \cdot \mathbf n) \mathbf n$ At this point, though, I'm pretty suspicious of this negative sign.  Should it really be there? A final note This argument depended on aligning the p and q directions with the principle directions.  This isn't actually necessary as Euler's theorem can be used to show that the sum of the curvatures associated with any two orthogonal directions is equal to the sum of the curvatures of the principle directions.","['curvature', 'differential-geometry']"
3331513,Is there a geometric interpretation about the euclidean distance between of 2 matrices?,"The Euclidean distance between points p and q is the length of the line segment connecting them ( $\overline{\mathbf{p}\mathbf{q}}$ ). $$\begin{aligned}d(\mathbf {p} ,\mathbf {q} )=d(\mathbf {q} ,\mathbf {p} )&={\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\cdots +(q_{n}-p_{n})^{2}}}\\[8pt]&={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\end{aligned}
$$ The Euclidean norm, or Euclidean length, or magnitude of a vector measures the length of the vector $$\left\|{\mathbf  {p}}\right\|={\sqrt  {p_{1}^{2}+p_{2}^{2}+\cdots +p_{n}^{2}}}={\sqrt  {{\mathbf  {p}}\cdot {\mathbf  {p}}}},$$ similar to above, is there a geometric interpretation about the euclidean distance between of 2 matrices? take this concrete example. X = [[0, 1],
     [2, 3]]

Y = [[1, 2],
     [3, 4]] what does this quantity represent? euclidean_distances(X , Y)

array([[1.41421356, 4.24264069],
       [1.41421356, 1.41421356]])","['matrices', 'linear-algebra', 'geometry']"
3331526,Does there always exist a random variable whose conditional distribution is a given distribution?,"Let $\{P_y|y\in \mathbb{R}^m\}$ be a family of Borel probability measures on $\mathbb{R}^n$ and $\mu$ be a Borel probability measure on $\mathbb{R}^m$ Then, does there exist random vectors $X,Y$ such that $P(X\in \cdot|Y=y)\sim P_y$ for almost every $y$ with respect to $Y_*P$ , and $Y\sim \mu$ ? Note that $$\int_{\mathbb{R}^m} P_y(A) d\mu(y)= \int_{\mathbb{R}^m} P_y(A) d(Y_*P)(y) =\int_{\mathbb{R}^m} P(X\in A|Y=y)d(Y_*P)(y)= (X_*P)(A)$$ So, we know what distribution $X$ follows. Now, to prove the existence of a pair of $(X,Y)$ , we now need to find a suitable pair such that $P(X\in \cdot|Y=y)\sim P_y$ , but how..?","['conditional-probability', 'probability-distributions', 'probability-theory']"
3331534,"Are Intrinsic Volumes defined for non-polyconvex, non-compact sets?","The intrinsic volumes (AKA Minkowski Functionals) $V_i:K\to \mathbb{R}$ of compact, convex subsets of $\mathbb{R}^d$ (denoted $K$ ) are important valuations in convex geometry. My question is simply whether these $V_i$ are defined for anything beyond polyconvex, compact sets. I have found many sources which define them for polyconvex and compact sets, but none which go beyond that. For example, can the function be meaningfully extended to open sets or to 2D surfaces with boundary embedded in $\mathbb{R}^3$ ? Any knowledge of an extension beyond polyconvex, compact sets is appreciated!","['reference-request', 'convex-geometry', 'geometry', 'differential-geometry']"
3331536,Find $ \lim _{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n}$,"What is the limit of: $$\lim_{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n} = ?$$ I did some computation with big numbers, I guess it is in the interval $\left(\frac{1}{2},1\right)$.","['limits', 'numerical-methods', 'real-analysis']"
3331546,"Is there a ""Riemann-Stieltjes integral"" for discrete 2D functions?","I was wondering if Riemann-Stieltjes integral still works when both of the functions $f$ and $g$ are discrete and two dimensional? Something like below: $$\int f(x,y) dg(x,y)$$ By using the parametric formula of the curve I would be able to write $x$ and $y$ in terms of one variable like angle $\alpha$ but I don't know how that would help me solve this integral which is actually a summation on discrete 2D signals. Any Suggestions?","['statistics', 'probability-distributions', 'discrete-mathematics', 'discrete-calculus', 'probability']"
3331575,Expected number of draws until first ace,"The following question has appeared elsewhere on the site : What is the expected number of cards that need to be turned over in a regular $52$ -card deck in order to see the first ace? The correct answer is $10.6$ . However, I got something different from the following approach of conditional expectation: Let $N$ denote the random variable for the number of cards to be turned over to see the first ace. Also, let $R$ denote the random variable for the « value » of the card in the first round, i.e. the four aces have values $1$ to $4$ respectively and the other $48$ cards admit values $5$ to $52$ respectively. Therefore, by the tower property of conditional expectation, \begin{eqnarray}
\mathbb{E}[N] & = & \sum_{i=1}^4 \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\
&  & + \sum_{i=5}^{52} \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\
& = & \sum_{i=1}^4 1 \big( \frac{1}{52} \big) + \sum_{i=5}^{52} \Big( 1 + \mathbb{E}[N] \Big) \Big( \frac{1}{52} \Big) \\
& = & \frac{4}{52} + \frac{48}{52} \Big( 1+ \mathbb{E}[N] \Big).
\end{eqnarray} This gives $$ \mathbb{E} [N] = 13. $$ I fail to see any problems with this approach of conditional expectations, yet this does not give the correct answer. Any ideas?","['expected-value', 'probability']"
3331632,Closed form for the series: $\sum_{n=1}^{\infty}x^{n^2}$,"Let $$S=\sum_{n=1}^{\infty}x^{n^2}, \quad|x|<1.$$ It is convergent and the sum is definitely less than $1/(1-x).$ Is there any closed form for $S$ ?","['theta-functions', 'closed-form', 'special-functions', 'sequences-and-series']"
3331665,Complete metric,"Suppose $M_1=(\mathbb{R}^2,g_s)$ is the plane with standard flat metric, it is a complete manifold. Now if I delete point origin, $M_2=(\mathbb{R}^2\setminus\{0\},g_s)$ is obviously not complete. However, when punctured plane is given a different metirc, it becomes complete, such as $$g=\frac{1}{|x|^2}g_s$$ I think it is geodesically complete, since when points get close to origin, the metric blows up. However, I don't know how to rigorously prove it?","['riemannian-geometry', 'differential-geometry']"
3331669,Solving $ \int \sqrt{1 + \tan(x)}\:dx$,"I know this is lazy, but I was really hoping that someone could read over my work on this integral and let me know whether I've made any errors. Here I will address the integral: \begin{equation}
    I = \int \sqrt{1 + \tan(x)}\:dx \nonumber
\end{equation} Here let $u = \tan(x)$ : \begin{equation}
    I = \int \sqrt{u + 1} \cdot \frac{1}{u^2 + 1}\:du = \int \frac{\sqrt{u + 1}}{u^2 + 1}\:du \nonumber 
\end{equation} Let $t^2 = u + 1$ : \begin{equation}
    I = \int \frac{\left|t\right|}{\left(t^2 - 1\right)^2 + 1} \cdot 2t \:dt = 2 \int \frac{t\left|t\right|}{t^4 - 2t^2 + 2}\:dt = 2 \int \frac{t\left|t\right|}{\left(t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2} \right)\left(t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2} \right)}\:dt \nonumber
\end{equation} Now: \begin{equation}
    \tan(x) + 1 \geq 0 \rightarrow u + 1 \geq 0 \rightarrow t^2 \geq 0 \nonumber
\end{equation} Which implies that $t$ can be both positive and negative. Thankfully the solution to one is merely the negative of the other. Here we proceed with the case $t > 0$ : \begin{align}
    I &= 2 \int \frac{t^2}{\left(t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2} \right)\left(t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2} \right)}\:dt \nonumber \\
    &= 2 \cdot \frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}} \int \left[\frac{t}{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}} - \frac{1}{2} \cdot \frac{t}{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\right]\:dt \nonumber \\
     &= 2 \cdot \frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}} \int \left[\frac{1}{2}\cdot \frac{2t - \sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}\sqrt{\sqrt{2} + 1}}{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}} -\frac{1}{2}\cdot  \frac{2t + \sqrt{2}\sqrt{\sqrt{2} + 1} - \sqrt{2}\sqrt{\sqrt{2} + 1}}{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\right]\:dt \nonumber \\
     &=\frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}}  \bigg[ \int \frac{2t - \sqrt{2}\sqrt{\sqrt{2} + 1} }{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt + \int \frac{ \sqrt{2}\sqrt{\sqrt{2} + 1}}{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt \nonumber \\
     &\quad-\int \frac{2t + \sqrt{2}\sqrt{\sqrt{2} + 1} }{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt + \int \frac{ \sqrt{2}\sqrt{\sqrt{2} + 1}}{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt \bigg] \nonumber \\
     &=\frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}}  \bigg[ \int \frac{2t - \sqrt{2}\sqrt{\sqrt{2} + 1} }{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt + \int \frac{ \sqrt{2}\sqrt{\sqrt{2} + 1}}{\left(t - \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}\right)^2 + \sqrt{2} - \frac{\sqrt{2} + 1}{2}}\:dt \nonumber \\
     &\quad-\int \frac{2t + \sqrt{2}\sqrt{\sqrt{2} + 1} }{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}\:dt + \int \frac{ \sqrt{2}\sqrt{\sqrt{2} + 1}}{ \left(t + \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}\right)^2 + \sqrt{2} - \frac{\sqrt{2} + 1}{2}}\:dt \bigg] \nonumber \\
     &= \frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}}  \bigg[\ln\left|t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}\right| + \sqrt{2}\sqrt{\sqrt{2} + 1} \frac{1}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\arctan\left( \frac{t - \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\right) \nonumber  \\
     &\quad+ \sqrt{2}\sqrt{\sqrt{2} + 1} \frac{1}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\arctan\left( \frac{t + \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\right)- \ln\left|t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}\right|  \bigg] + C \nonumber \\
     &= \frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}}\ln\left| \frac{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}} \right| \nonumber \\
     &\qquad+ \frac{1}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\bigg[\arctan\left( \frac{t - \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\right) + \arctan\left( \frac{t + \frac{1}{2}\sqrt{2}\sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - \frac{\sqrt{2} + 1}{2}}}\right)\bigg] + C \nonumber \\
     &= \frac{1}{\sqrt{2}\sqrt{\sqrt{2} + 1}}\ln\left| \frac{t^2 - t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}}{t^2 + t\sqrt{2}\sqrt{\sqrt{2} + 1} + \sqrt{2}} \right| \nonumber \\
     &\qquad+ \sqrt{\frac{2}{\sqrt{2} - 1}}\bigg[\arctan\left( \frac{\sqrt{2}t - \sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - 1}}\right) + \arctan\left( \frac{\sqrt{2}t + \sqrt{\sqrt{2} + 1}}{\sqrt{\sqrt{2} - 1}}\right)\bigg] + C \nonumber
\end{align} Where $C$ is the constant of integration and $t^2 = \tan(x) + 1$","['integration', 'indefinite-integrals', 'trigonometric-integrals']"
3331675,"Let $(a_n)$ be a strictly increasing sequence of positive integers such that: $a_2 = 2$, $a_{mn} = a_m a_n$ for $m, n$ relatively prime.","Let $(a_n)$ be a strictly increasing sequence of positive integers such
  that: $a_2 = 2$ and $a_{mn} = a_m a_n$ for $m, n$ relatively prime. Show that $a_n = n$ , for every positive integer $n$ . This is a result apparently due to Paul Erdős, and supposedly has a proof by induction. I tried like this, $a_{10}=a_2a_5$ . After this what we can do? [Editor's comment]
Possibly due to the apparent simplicity of the conditions it may be difficult to appreciate the subtleties of this question. If we try and construct a counterexample like $a_3=4$ , $a_4=5$ , $a_5=6$ , then the requirements dictate $a_6=8$ , $a_{10}=12$ , $a_{15}=24$ . At this point we realize that we have been speeding. For monotonicity forces $a_9\le 11$ , and therefore $a_{18}\le22<a_{15}$ , violating the requirements. It is not obvious why something similar ruins all the modifications to the sequence $a_n=n$ .
[/comment, JL]","['multiplicative-function', 'number-theory', 'elementary-number-theory', 'sequences-and-series']"
3331706,Compute $\sum_{n=1}^\infty (-1)^{n-1}\frac{H_{2n+1}}{(2n+1)^3}$ and $\sum_{n=1}^\infty (-1)^{n-1}\frac{H_{2n+1}^{(2)}}{(2n+1)^2}$,"How to prove $$S_1=\sum_{n=1}^\infty (-1)^{n-1}\frac{H_{2n+1}}{(2n+1)^3}=1+\frac{35}{128}\pi\zeta(3)+\frac{1}{48}\zeta(4)-\frac1{384}\psi^{(3)}\left(\frac14\right)$$ $$S_2=\sum_{n=1}^\infty (-1)^{n-1}\frac{H_{2n+1}^{(2)}}{(2n+1)^2}=1+\frac18G\zeta(2)-\frac{35}{64}\pi\zeta(3)-\frac{15}{16}\zeta(4)+\frac1{768}\psi^{(3)}\left(\frac14\right)$$ where $H_n=\sum_{n=1}^\infty\frac1n$ is the $n$ th harmonic number, $G$ denotes the Catalan's constant, $\zeta$ denotes the Riemann Zeta function and $\psi^{(n)}$ designates the polygamma function. These two sums were proposed by Cornel and can be found here and here . My solution to $S_1$ can be found in the first link but its long, so can we find a better way to find $S_1$ and $S_2$ ? Thanks. Note: Using the generating function of $\ \sum_{n=1}^\infty x^n\frac{H_n}{n^3}$ to evaluate $S_1$ is not allowed.","['integration', 'definite-integrals', 'harmonic-analysis', 'polygamma', 'sequences-and-series']"
3331717,"What is the difference between a response, output, hidden and latent variables in modeling?","I'm a learner of machine learning and statistics and I have some experience with both of the subjects. However, until this day it has not yet been fully revealed to me what is the fundamental difference between the four following variables: Response variable Output variable Hidden variable Latent variable To my knowledge, in data analysis our task in general is to find the functional relationship between explanatory observed data $x$ and the variable of interest $y$ . That is, in general our goal in many modeling applications is to find a function $f$ such that: $$f(x)=y,$$ using some data set $D={(x_1,y_1), (x_2,y_2), ..., (x_n, y_n)}$ . I think the ""response"" and ""output"" variables are synonyms and generally refer to the $y$ variable. But what about ""hidden"" and ""latent"" variables? What do they refer to? Are they also synonyms for $y$ or do they refer to the parameters of $f$ ? Concrete and simple examples would be both sufficient and excellent answers, thank you! UPDATE: As requested, I will also add the following into the list of variables to be explained: independent variable dependent variable confound variable","['data-analysis', 'statistics', 'mathematical-modeling']"
3331726,Is there accepted notation for specifying just the domain of a function?,"Question. Is there a notation like $$f(x \in \mathbb{R}) = x^2 + 2x + 1$$ or some variant on that, satisfying the following conditions? (a) Like the above syntax, it allows us to define a function by specifying its domain without worrying about the codomain (b) Like the above syntax, it does not force us to write $x$ more times than strictly necessary, and (c) Unlike the above syntax, it's fairly standard and won't cause too many eyebrows to be raised. The only such ""accepted notations"" I can think of are $$f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R}$$ $$\forall x \in \mathbb{R}, f(x) = x^2 + 2x + 1$$ which force us to mention $x$ an ""extra"" time, violating condition (b). Motivation 1. Promoting Readability. In structuralist mathematics codomains are fundamental, however in more 'down-to-earth' math they're often irrelevant, and cluttering the page with such details can sometimes reduce readability e.g. through misdirection. Motivation 2. Pedagogy. In my opinion, that students should encounter the concepts of ""function"" and ""domain"" at age 12 or thereabouts, while the concept of a ""codomain"" should be saved for university and the initial forays into structuralist mathematics. This means that having an alternative to the $f : X \rightarrow Y, x \mapsto E(x)$ notation often used in structural mathematics is important. Motivation 3. Laziness. Realistically people are going to leave off the $x \in \mathbb{R}$ part from expressions like $$f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R},$$ partly because it's at the end of the expression, but more fundamentally because we're repeating the $x$ unnecessarily and it starts to feel tiresome. A good notation would address this purely psychological issue.","['notation', 'functions', 'real-analysis']"
3331735,On confidence sets and linear regression,"I'm working on Exercise 3.2 from Elements of Statistical Learning . It asks to find a $95\%$ confidence interval for a linear regression prediction (ordinary least squares are used) using two different methods. In the first part we're looking for the confidence interval for a single prediction $x_0^T \hat{\beta}$ , as far as I understand it's basically $$x_0^T \hat{\beta} \pm z_{0.975} \cdot \hat{\sigma}
\sqrt{x_0^T \left(\mathbf{X}^T \mathbf{X}\right) x_0},$$ where $\mathbf{X} \in \mathbb{R}^{N \times (p + 1)}$ - design matrix, $z_{0.975} \approx 1.96$ quantile of standard normal distribution and $$\hat{\sigma} = \frac{1}{N - p - 1} \sum_{i = 1}^N \left( y_i - \hat{y}_i \right)^2$$ In the second part, we're looking for a confidence interval generated by a confidence set for the whole vector $\beta$ : $$C_\beta = \left\{ \beta: \; f(\beta) = (\beta - \hat{\beta})^T \mathbf{X}^T \mathbf{X} (\beta - \hat{\beta}) \leq \hat{\sigma} \chi_{p+1}^{2 \; (0.975)}\right\},$$ where $\chi_{p+1}^{2 \; (0.975)}$ is a $0.975$ quantile of $\chi_{p + 1}^2$ . Here I'm having troubles. First of all, how to calculate confidence interval based on $C_\beta$ ? My guess was that it's equivalent to finding the following: $$\min_{\beta \in C_\beta} x_0^T \beta \quad \text{and} \quad
\min_{\beta \in C_\beta} x_0^T \beta$$ which at the same time is equivalent to $$\begin{cases}
\alpha x_0 = \nabla f (\beta) \\
\beta \in \partial C_\beta
\end{cases}$$ for a fixed $x_0$ (here I used Lagrange multipliers method for solving an optimization problem). I've solved this and got the following confidence interval for $x_0^T \hat{\beta}$ : $$x_0^T \hat{\beta} \pm \hat{\sigma} \sqrt{\chi_{p+1}^{2 \; (0.975)} 
x_0^T \left(\mathbf{X}^T \mathbf{X}\right) x_0}$$ Is this right? My main concern is that it's just a $\chi^2$ correction of the first confidence interval, where we used estimate of $\sigma$ but not its true value, though is it indeed a confidence interval generated by $C_\beta$ ? And if it's not, could someone help with understanding what it actually is? Thanks!","['linear-regression', 'statistics', 'least-squares']"
3331751,Why do we need the covariant derivative along a curve - why are linear connections not sufficient?,"I can't figure out why we need the definition of a 'covariant derivative along a curve', i.e. I can't see why we can't use a 'linear connection' even when the vector fields are not extendible. I'm reading Lee's book on Riemannian manifolds. After he has shown that $\nabla$ depends on X and Y only around an open set, he defines the Christoffel symbols through the expression $\nabla_{E^j}E^i$ , where $E^j,E^i$ are elements of a local frame, i.e. vector fields defined only locally on an open set (and thus not necessarily extendible). Likewise, it is show that $(\nabla_{X}Y)_p$ in fact only depends on $X$ through its value at p and on Y through its values on a curve through p whose tangent at p is $X_p$ . Therefore, if $\gamma$ is a smooth curve, $(\nabla_{\dot{\gamma}}Y)_p$ should be well-defined, even if Y is only defined along $\gamma$ and isn't extendible. Where am I wrong? Thanks a lot.","['connections', 'riemannian-geometry', 'differential-geometry']"
3331753,Why are variance and expected value all we care about?,"My statistics background is almost $\varnothing$ , so I apologise if my question has already been asked, and I just didn't know the right terminology to find it here. Suppose $p_X$ is the probability density of a random variable $X$ (say, defined over $\mathbb{R}$ ). Then $$\int_{\mathbb R} p_X(x) \mathrm{d}x = 1,\quad \mathbb{E}(X) = \int_{\mathbb R} xp_X(x)\mathrm{d}x, \quad \mathrm{Var}(X) = \mathbb{E}(X^2) - \mathbb{E}(X)^2.$$ The above three conditions/definitions depend precisely on $\mathbb{E}(X^0), \mathbb{E}(X^1)$ , and $\mathbb{E}(X^2)$ . This makes me wonder why I've never encountered higher $n$ , i.e. why is $\mathbb{E}(X^n)$ not important for bigger $n$ ? (Of course the obvious answer is that they are important, but I've just never seen them because of my limited stats background. However, this feeds into a second related question below.) The above question organically formed when I was reading a proof of the law of large numbers using Fourier transforms. Assume $p_{X_i}$ has mean $0$ and finite variance $D$ for every $i$ . Let $Z_m = \frac{1}{m}(X_1 + \cdots + X_m)$ . We wish to prove that $\lim_{m\to\infty}\mathbb{E}(Z_m) = 0$ . Recalling the formula $$\hat{f}^{(n)}(\zeta) = (-2\pi i)^n (\widehat{x^nf})(\zeta),$$ we have $\hat{p}_X(0) = 1, \hat{p}_X'(0) = 0, \hat{p}_X''(0) = -4\pi^2D$ and so on. The proof goes on to say that for any choice of $\zeta$ , $$\lim_{m \to \infty} \hat{p}_{Z_m}(\zeta) = \lim_{m\to\infty} \hat{p}_X(\zeta/m)^m = \lim_{m\to\infty}\Big(1 - \frac{2\pi^2D^2 \zeta^2}{m^2}\Big)^m = 1.$$ The last step is expressing $\hat{p}_X$ as a Taylor series (expanded about 0) and ignoring higher order terms. However, without any additional information about higher order derivatives of $\widehat{p}_X$ (i.e. the values of $\mathbb{E}(X^n)$ for larger $n$ ) the assumption that the Taylor series converges everywhere is surely wrong. My dissatisfaction with this proof can again be summarised as ""why are the mean and variance enough?"". Is there an unspoken assumption that all probability distributions are analytic almost everywhere? Or do the mean and variance truly pin down the probability density function in such a way that we can ignore all higher order terms?","['probability-limit-theorems', 'fourier-analysis', 'probability-distributions', 'fourier-transform', 'probability-theory']"
3331782,How do I derive the following expression for the sum of orthogonal matrices?,"In Johansen's book 'Likelihood-based inference in cointegrated vector autoregressive models', in order to get the expression for the Granger's representation theorem he claims that: $$\beta_\bot(\alpha'_\bot \beta_\bot )^{-1} \alpha'_\bot + \alpha (\beta' \alpha)^{-1} \beta'  = I \tag{1}$$ Where: $\alpha$ is $N\times R$ , $\text{rank}(\alpha) =R$ $\beta$ is $N\times R$ , $\text{rank}(\beta) =R$ $\beta_\bot $ is $N\times (N-R)$ , $\text{rank}(\beta_\bot) =N-R$ $\alpha_\bot $ is $N\times (N-R)$ , $\text{rank}(\alpha_\bot) =N-R$ $\alpha' \alpha_\bot =0$ $\beta' \beta_\bot =0$ I am not able to prove (1). Can you help me, please?","['vector-auto-regression', 'vector-spaces', 'matrices', 'orthogonal-matrices', 'linear-algebra']"
3331787,Matrix Multiplication - Undefined Product,"I am learning linear algebra for a machine learning class and have a question about matrix multiplication. The product of two matrices is undefined whenever the rows of the first matrix (reading right to left) do not match the column of the second matrix. However, say that I would need (for some reason) to multiply a 3x3 matrix by a 2x2 one. Couldn't I just complete the operation by adding the ""missing row"" with coordinates [0,0]? I am thinking about it because if matrices represent linear transformations of space, then a 2x2 matrix represent a two dimensional transformation. However, isn't a two dimensional transformation simply a transformation where every other dimension is equal to 0? To illustrate that, let's say I want to apply a linear transformation [-1,0;0,1] to vector [3,3]. The resulting vector would be a two dimensional vector [-3,3]. Now let's say that after this transformation I want to apply another transformation to the same vector, but this time in three dimensions. To keep the example as simple as possible let's use the identity transformation for this: [1,0,0;0,1,0;0,0,1]. Doing this would require to multiply the 3x3 matrix [1,0,0;0,1,0;0,0,1] by the 2x3 one [-1,0;0,1], which is technically not possible. However, if I apply the method above (i.e. adding a third row with all 0 to the 2x3 matrix) I would still be able to compute the transformation and get the result [-1,0,0;0,1,0]. I can then multiply this to my original vector and get [-3,3,0]. The only difference I can see between [-3,3,0] and [-3,3] is that in the first one I am just ""explicitly showing"" the third dimension as having coordinate 0, whilst in the second I am keeping this implicit. What am I missing here? Regards, Federico","['matrices', 'linear-algebra', 'linear-transformations']"
3331806,"If $X$ and $Y$ are i.i.d. random variables, is $P(X < Y) = P(Y < X)$?","I am trying to solve the following probability problem: If $X$ and $Y$ are independent and identically distributed (i.i.d.) random variables, is $P(X < Y) = P(Y < X)$ ? Source: https://projects.iq.harvard.edu/files/stat110/files/strategic_practice_and_homework_5.pdf (Pg 2, Qn 2) The answer provided on Pg 7 (of the pdf) states: If $X$ and $Y$ are i.i.d., then $P(X < Y) = P(Y < X)$ by symmetry: we can
  interchange $X$ and $Y$ since both are the probability of one draw from the
  distribution being less than another, independent draw. However, my intuition tells me that since $X$ and $Y$ could be entirely different functions, the above explanation seems a little ""handwavy"" (e.g. $Y$ could take on values that are very much larger than $X$ ). So I guess I need a little algebraic proof to convince myself but I am unable to do so on my own. Could anyone please advise me on how to prove why this statement is true/false via an algebraic proof?","['probability-theory', 'random-variables']"
3331815,Matrixes of higher order like $M_{\aleph\times \aleph}$ [closed],Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 years ago . Improve this question I was wondering if thinking about matrixes of the form $M_{\aleph\times \aleph}$ and assigning properties to them like matrix multiplication makes sense and useful in some way. note: $\aleph$ is the power of the real numbers.,"['matrices', 'infinite-matrices', 'elementary-set-theory', 'linear-algebra']"
3331858,Why are power series not polynomials?,"Wikipedia says, One can view power series as being like ""polynomials of infinite
  degree,"" although power series are not polynomials. Why not?","['power-series', 'functions', 'polynomials']"
3331864,The category of Compact Hausdorff spaces is special: why? In which other contexts bijections are automatically isomorphisms of objects?,"I am writing my bachelor thesis, mainly about General Topology and Topological Vector Spaces. Moreover I know a little bit about Category Theory: categories, functors, natural transformations, representability and the Yoneda Lemma. 
A simple consideraation is the following: Any continuous function between a compact and an Hausdorff space is closed As an immediate consequence a continuous bijection between two compact Husdorff spaces is automatically a homemorphism.
This motivates two facts: Adding just an open set the topology ceases to be compact and
removing one the topology cease to be Hausdorff.  Hence the topology of a CHaus space is 'final' with respect to the property of compactness and 'initial' with respect to that of Hausdorffness A bijective
morphism in the category CHaus is automatically an isomorphism Now my questions are: First of all: are 1 and 2 'categorically' related? Secondly: I think a completely analogous result is interpreting the Banach isomorphism theorem in the category Ban of banach spaces. What's underlying? What do these category share? Can we generalize? Do we have more examples, specially in Topology/Functional Analysis? Third: can someone suggest some nice 'easy' application of category theory to general topology or functional analysis? I mainly saw algebraic and algebraic topological ones. Thanks in advance","['banach-spaces', 'general-topology', 'category-theory', 'reference-request']"
3331869,Rauch comparison theorem with other initial conditions?,"The statement of the Rauch comparison theorem (see e.g. https://en.wikipedia.org/wiki/Rauch_comparison_theorem ) involves two normal Jacobi fields $J$ and $\tilde{J}$ along two unit speed geodesics $\gamma$ and $\tilde{\gamma}$ in their respective manifolds $M$ and $\tilde{M}$ . As a hypothesis, $J(0)=\tilde{J}(0)=0$ and $\|D_tJ(0)\|=\|\tilde{D}_t\tilde{J}(0)\|$ . Also, conjugate points must be avoided in $\tilde{\gamma}$ . I've been looking for an identical statement but taking a non-zero initial condition (EDIT): $\|J(0)\|=\|\tilde{J}(0)\|=a\neq0$ . Nevertheless, I didn't find anything and I don't even know if it's possible. The only similar thing I found is on the page 149 of the Sakai's book Riemannian geometry (1996). Here, an extended result to Rauch is given but it involves an hypersurface $N$ , $N$ -Jacobi fields and focal points. I also found somewhere Rauch as a corollary of the Sturm's theorem on differential equations, but the version I'm looking for was not deduced from there (I don't know if it's possible just by taking another initial condition in the differential equation problem). To sum up, I just want the same statement as above but with a non-zero initial condition (maybe with some change on the condition of avoiding conjugate points also, I don't know). Any reference is welcome! Thank you in advance!","['differential-topology', 'riemannian-geometry', 'differential-geometry']"
3331913,How to solve the system of differential equations?,"Let $x_i=x_i(t,s),i=0,1,2$ be  functions of $t,s$ and consider the system of differential equations \begin{cases}
\frac{\partial x_0}{\partial t}=0,\\
\frac{\partial x_1}{\partial t}=x_0,\\
\frac{\partial x_2}{\partial t}=2x_1,\\
\frac{\partial x_0}{\partial s}=2x_1,\\
\frac{\partial x_1}{\partial s}=x_2,\\
\frac{\partial x_2}{\partial s}=0,
\end{cases} How to solve it?
My attemt.
First of all  I have solved the first 3 equations and get \begin{align}
 &x_{{0}}=C_1 \left( s \right) ,\\
&x_{{1}}  =C_1 \left( s \right) t+C_2(s),\\
&x_{{2}}  =C_1 \left( s
 \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s
 \right),  
\end{align} and substitute into the last 3 equation $$
\begin{cases}
\displaystyle {\frac { d}{{ d}s}}C_1 \left( s
 \right) =2\,C_1 \left( s \right) t+2\,C_2 \left( s
 \right),\\
\\
 \displaystyle t  {\frac { d}{{ d}s}}C_1 \left( s \right)+{\frac { d}{{ d}s}}C_2 \left( s \right) =C_1 \left( s \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s \right) ,\\ \\
 \displaystyle {t}^{2}\, {\frac { d}{{ d}s}}C_1
 \left( s \right)  +2\,t  {\frac { d}{{ d}s}
}C_2 \left( s \right) +{\frac { d}{{ d}s}}C_3 \left( s \right) =0,
\end{cases}
$$ or normalize it: $$
\begin{cases}
\displaystyle{\frac { d}{{ d}s}}C_1 \left( s
 \right)=2\,C_1 \left( s \right) t+2\,C_2 \left( s \right) ,\\ \\
\displaystyle{\frac { d}{{ d}s}}C_2 \left( s
 \right)=-C_1 \left( s \right) {t}^{2}+C_2 \left( s \right) ,\\ \\
\displaystyle{\frac { d}{{ d}s}}C_3 \left( s
 \right)=-2\,C_2 \left( s \right) {t}^{
2}-2\,C_2 \left( s \right) t 
\end{cases}
$$ Then I solve the system for $C_1(s),C_2(s),C_3(s)$ and get \begin{align*}
&C_1 \left( s \right) =-{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2'\,s{t}^{2}+2\,C_1'\,st+4\,C_3'\,{
t}^{2}+2\,C_2'\,t+C_1'}{4{t}^{3}}},\\
&C_2 \left( s
 \right) =\frac{1}{2}\,C_1'\,{s}^{2}+C_2'\,s+C_3', \\
&C_3
 \left( s \right) =-\,{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2\,s{t}^{2}-2\,C_1'\,st+4\,C_3'\,{t}^{2}-2\,C_2'\,t+C_1'}{4t}}.
\end{align*} Here $C_1',C_2',C_3'$ are constants. Then I substitute it  into the above expression for $x_0,x_1,x_2$ and get \begin{align*}
&x_{{0}} \left( t,s \right) ={\frac { \left( -2\,C_1'\,{s}^{2}-4\,C_2'\,s-4\,C_3' \right) {t}^{2}+ \left( 
-2\,C_1'\,s-2\,C_2' \right) t-C_1'}{4{t}^{3}}}, \\&x_{{1}
} \left( t,s \right) ={\frac { \left( -2\,C_1'\,s-2\,{C_2'} \right) t-C_1'}{4{t}^{2}}},\\ &x_{{2}} \left( t,s \right) =-
\,{\frac {C_1'}{2t}}
\end{align*} But obviously it is not solutions of the initial system! Where is my mistake? Note, the system have solution, for example $x_1^2-x_0 x_2$ is its first integral.","['systems-of-equations', 'ordinary-differential-equations', 'partial-differential-equations']"
3331917,"Proof that the sample mean is the ""best estimator"" for the population mean.","I've always heard that the sample mean $\overline{X}$ is ""the best estimator"" for the population mean $\mu$ . But is that always true regardless of the population distribution? is there any proof for that?
For example let's suppose for an unknown population, we have three samples, say $X_1$ , $X_2$ , $X_3$ . Based on what I've heard (Not necessarily true) the estimator defined as the following: $$\frac{1}{3}(X_1+X_2+X_3)$$ is always preferable to, for instance: $$\frac{1}{6}(X_1+X_3)+\frac{2}{3}X_2$$ or $$\max(X_1, X_2, X_3)$$ But in what sense is it better? and why?","['statistical-inference', 'statistics', 'parameter-estimation']"
3331923,Probability conditional on inequality,"Let $X_1,X_2,....,X_{n+1}$ independent uniform variables on the unit interval $[0,1]$ , $V:=\max(X_1,X_2,...,X_n)$ and $c \in [0,1].$ What is $\textrm{Prob}(X_{n+1}<V|V<c)$ ? Initially I thought this would be equivalent to $$\textrm{Prob}(X_{n+1}<V<c)=\int_0^cnz^ndz=\frac{n}{n+1}z^{n+1},$$ but I am concerned that equivalence may not hold.","['conditional-probability', 'probability-theory', 'probability']"
3331924,Pythagorean triple with hypotenuse a power of $2$,Is there a Pythagorean triple whose largest element is a power of 2? That is: are there solutions to $a^2 + b^2 = 2^{2k}$ in the positive integers?,"['number-theory', 'pythagorean-triples', 'perfect-powers']"
3331928,"If $P(X \geq a) =1$, then $E[X] \geq a$ proof","Suppose $X$ is a discrete random variable such that $P(X \geq a) = 1$ . Then, $E[X] \geq a$ . I feel that this proof is fairly simple, but I am having a bit of trouble with it. I think that what I want to get to is showing $(a)P(X \geq a) = (a)(1) = a$ , but I am not sure how to get from $E[X] = \sum x_ip_i$ to $E[X] \geq (a)P(X \geq a)$ . Since we don't know the range of values of $X$ , I am finding it hard to connect these two ideas. I was thinking $\sum x_ip_i \geq (a)P(X = a)$ , but I am first of all not sure that this is true (what if every value other than $a$ is negative?) and I am not sure how it relates to $(A)P(X=a)$ . It may be that I am missing something simple, but I am not sure how to connect these ideas, or if this is the correct way to do this proof.","['expected-value', 'probability-theory', 'probability']"
3331933,"Subgroup of $S_n$ generated by $(1,2,\cdots,n)$ and $(1,2,\cdots,m)$.","I'm working on the following problem: Let $G$ be the subgroup of $S_n$ generated by $(1,2,\cdots,n)$ and $(1,2,\cdots,m)$ where $1<m<n$ . Show $G$ is $S_n$ if either $m$ or $n$ is even, and otherwise, $G$ is $A_n$ . I know that $G$ is primitive and there's a related theorem: Let $G$ be a primitive subgroup of $S_n$ , if $G\neq S_n,A_n$ , then $|S_n:G|\geq[(n+1)/2]!$ .","['symmetric-groups', 'group-theory', 'abstract-algebra']"
3331958,coloring a $n\times n$ square grid in a specific way,"I recently saw a maths problem that looked fun to solve. I have a $n\times n$ square grid and I need to color the cells within the grid in such a manner that no $2\times 2$ square that is within the larger grid has 3 of its 4 cells colored(it can be 1 or 2 or sometimes 0) and when all the necessary cells are colored if you color any of the empty cells there must be a 2*2 square (within the larger grid) which has $3$ cells colored. It is a hard problem to wrap your mind around. Everything up untill the $5\times 5$ grid is not hard to solve. For example, there is only one solution for a $2\times2$ grid. No $2\times2$ square within the $3\times3$ square grid has 3 cells colored and if you were to color any of the empty cells there would be at least one $2\times2$ square which has 3 cells colored. I also have solutions for the $5\times 5$ and $6\times6$ grid. The problem is that for 6*6 there were a lot of plausible solutions so  it would take absurd amount of time to get the results by hand. The real problem is that I need to create a proof to why for example a 4*4 grids lowest number of colored cells is 7 and that is the thing I have been struggling with. I also need to understand in what manner should I color the cells to get the results I need so that I can get the results on my own as n gets larger. Maybe some of you could recommend some scientific papers that adress this kind of problem or help me figure out in what way should I color the cells. Any help would be much appreciated. Tnx :)",['combinatorics']
3331979,Messing around with the $\cos$ & $\sin$ relation with $\pi$. Apparently got wrong.,"So the $\sin$ relation with $\pi$ is; $$\pi  =\lim \limits_{n \to \infty} n\cdot\sin(\frac{180}{n}) \tag{Eq. 01}\label{1} $$ And the $\cos$ one is; $$\pi = \lim \limits_{n \to \infty} n\sqrt2\cdot\sqrt{1-\cos(\frac{180}{n})} \tag{Eq. 02}\label{2}$$ So from these two, we can get; \begin{align}
n*\sin(\frac{180}{n}) & = n\sqrt2\cdot\sqrt{1-\cos(\frac{180}{n})} \\
\sin(\frac{180}{n}) & = \sqrt2\cdot\sqrt{1-\cos(\frac{180}{n})} \\ 
\sin^2(\frac{180}{n}) & = 2(1-\cos(\frac{180}{n})) \\
\frac{\sin^2(\frac{180}{n})}{1-\cos(\frac{180}{n})} & = 2 \\ 
\frac{1-\cos^2(\frac{180}{n})}{1-\cos(\frac{180}{n})} & = 2 \\
1+\cos(\frac{180}{n}) & = 2 \\
\cos(\frac{180}{n}) & = 1 \\
\end{align} This isn't obviously true. So what's the problem here?","['limits', 'trigonometry', 'pi']"
3332081,To prove that $(n-1)!+1$ is not a power of $n$.,"If $n$ is composite, prove that $(n-1)!+1$ is not a power of $n$ . Hint: We know that if $n$ is composite and $n>4$ then $(n-1)!$ is divisible by $n$ . My Solution: Since $n=4$ is the first composite number. We have $(4-1)!+1=7$ . Clearly 4 does not divide 7.
Also we know that $(n-1)! \equiv 0$ (mod $n$ ) (for $ n>4$ and $ n$ composite). Also $1 \equiv 1$ (mod $n$ ). Adding both these equations we get : $(n-1)!+1 \equiv 1$ $\pmod n.$ Hence it is clear that $(n-1)!+1$ is not a power of $n$ . Please correct me if there is any discrepancy in proof writing or the solution. Also it is highly appreciable if someone could provide with any other solution (Using modular-arithmetic or using Wilson's Theorem). Thanks in advance.","['number-theory', 'solution-verification', 'factorial', 'elementary-number-theory']"
3332097,"You have 3 cakes. Everytime you eat one, there's 17% chance the number of cakes is reset to 3. Find average number of cakes eaten?","I did a Python simulation and the answer is 4.40. But I think there should be a theoretical approach for this problem. I remember having seen variations of it mutiple times but don't know the technical terms for it. If anyone is interested, here's my simulation: def reset_to_original(starting_number, reset_probability):
    '''
    Given <starting_number>, countdown to zero. At each turn, there's a <reset_probability> 
    that the number of remaining turns will be reset to <starting_number>. 
    Return the total number of turns.
    '''
    import random
    remaining = starting_number
    total_count = 0
    while remaining:
        remaining -= 1
        total_count += 1
        is_reset = random.random() < reset_probability
        if is_reset: remaining = starting_number
    return total_count


def main(num, starting_number, reset_probability):
    total = 0.0
    for i in range(num):
        total += reset_to_original(starting_number, reset_probability)
    print('Average: {}'.format(total / num))


main(pow(10, 6), 3, 0.17) Can someone guide me through calculating it theoretically?",['probability']
3332108,"If $a,b \in \mathbb{R}$ and $a+b\ge 0$, prove that $(a^2+b^2)^3\ge 32(a^3+b^3)(ab-a-b)$","If $a,b \in \mathbb{R}$ and $a+b\ge 0$ , prove that $(a^2+b^2)^3\ge 32(a^3+b^3)(ab-a-b)$ This question in my opinion is difficult and have tried many things. I tried to expand both sides but that will not help since I will not be able to cancel anything out. I am also struggling with other methods like AM-GM since $a,b$ are not necessarily positive. Any help would be appreciated.","['algebra-precalculus', 'symmetric-polynomials', 'a.m.-g.m.-inequality']"
3332146,Is it possible to find such an angle using only angle chasing?,"I've been trying to solve some problem and I came down to the following seemingly easy question: given two triangles ABC and ABD, and their corresponding angles, how do we find the angle $\angle ACD$ using only angle chasing, I know that it's possible to do that by using coordinate bashing or law of cosine for example, but I was wondering if it's actually possible to do so using only angle chasing, no trigonometry involved.","['angle', 'geometry']"
3332150,"Tangent to a 2-Dimensional curve, in 3 Dimensions","For line to be tangent to a given curve, they should pass through a common point and both should have same slope at that point. But how do we compare slope in 3D? E.g.=> For a circle in 2-Dimensions, we know which lines will be it's tangents. But what if I ask, which lines will be tangent to it in 3-Dimensions? Will the answer be same for both cases, or will many others lines, lying in some other planes, also be included? All I could think of is that, we need to find tangents in planes , and in every plane the curve will be set of points, except the one in which the curve is a circle. And because slope of a point is not defined, there will be no new tangents added when we go from 2D to 3D.","['tangent-line', 'geometry', 'differential-geometry']"
3332164,How should I think of $dim_x(X)=\inf\{dim(U)|$ U open neighborhood of $x\}$ where $dim(U)$ is defined by longest chain of irreducible closed sets.,This is related to Iitaka's Algebraic Geometry a.Definition on pg 22 of Sec 1.9. $X$ is a non-empty topological space. Dimension of $X$ is supremum of all non-negative integers $l$ for which $F_i$ are closed irreducibles s.t. $\emptyset\neq F_0\subset F_1\subset\dots\subset F_l\subset X$ where we require inclusion is proper here. Set $x\in X$ . Define dimension of $X$ at $x$ by $dim_x(X)=\inf\{dim (U)\vert U$ is an open neighborhood of $x\}$ $\textbf{Q:}$ How should I think of $dim_x(X)$ ? What is a good example? It looks like that $dim_x(X)$ only sees the smallest irreducible component of $X$ containing $x$ . Note that $dim_x(X)$ is defined by taking infinimum rather than supremum. What do I expect so?,"['general-topology', 'algebraic-geometry']"
3332190,"In $\triangle ABC$, if angle bisectors $AE$ and $CD$ meet at incenter $F$, and $|FE|=|FD|$, then the triangle is isosceles or $\angle B=60^\circ$","I was screwing around lately in GeoGebra and I realized something. Draw a $\triangle ABC$ , and let the bisectors for $\angle A$ and $\angle C$ meet sides $BC$ and $AB$ at points $E$ and $D$ , respectively. If the angle bisectors meet at the incenter, $F$ , and if $FD \cong FE$ , then either $\triangle ABC$ must be isosceles or $\angle B$ must be $60^\circ$ . However I was unable to prove why that is. Any help would be appreciated.",['geometry']
3332215,Automorphisms of Elliptic curve via loops in the Weierstrass fibration,"Suppose I have the Weierstrass Elliptic fibration of $\mathbb{C}$ -elliptic curves given as $\mathcal{E}=\{ y^2=x(x-1)(x-\lambda) \} \to \lambda$ . The family degenerates to nodal curves at $\lambda = 1$ and $\lambda = 0$ . I'm given automorphisms of the $E_{1/2} = $ fiber over $1/2$ , constructed in the following way: Take 2 loops based at $1/2$ , say $\ell_0$ and $\ell_1$ with the first enclosing $0$ and the second enclosing $1$ , going counter-clockwise. Following the fibers one gets a ""loop"" of $E_{\lambda}$ starting and ending at $E_{1/2}$ and so obtain two diffeomorphisms $E_{1/2} \to E_{1/2}$ My question is: how can I see explicitly what these automorphisms are, and how do the two differ, and why did it matter, if it did, on the loops passing through the singular fibers. Of course intuitively this is reminiscent of complex analysis phenomena of defining branch cuts and monodromy business, but I don't know how to formulate it precisely in terms of the family up in $\mathcal{E}$ .","['complex-geometry', 'algebraic-geometry', 'elliptic-curves']"
3332226,Picard group of degree two cover of the plane,Let $X$ be a smooth complex projective surface such that there is a degree two morphism $f:X\rightarrow \mathbb{P}^2$ . Then I know that $X$ has to be branched over a curve of degree $2n$ . I have two questions regarding $X$ . 1) what is the Picard group of $X$ ? Is it $\mathbb{Z}$ ? 2) can any such $X$ be embedded in $\mathbb{P}^3$ as a closed subscheme. If this is true then $Pic\ X$ cannot be $\mathbb{Z}$ since we would need a line bundle on $X$ with four independent sections. In general what is the minimum $m$ such that $X$ can be embedded in $\mathbb{P}^m$ ?,['algebraic-geometry']
3332240,Let $G$ be a group and $A\subseteq G$. Showing $\{g\in G: gag^{-1}\in A \text{ for all } a\in A\}$ does not have to be a subgroup of $G$.,"Let $G$ be a group and $A\subseteq G$ . Consider $H=\{g\in G: gag^{-1}\in A \text{ for all } a\in A\}$ . I want to show that that $H$ does not have to be a subgroup of $G$ . (Note that for a finite $G$ it always is a subgroup because it suffices to prove closure, which is true in this case). The material I am using gives the following counterexample, I see why it is a counterexample but I would like to understand intuitively how you can arrive at this result. Let $G$ be the set of all permutations over $\mathbb{Z}$ and define $S_f=\{n\in\mathbb{Z}:f(n)\neq n\}$ for $f\in G$ . Set $A=\{f\in G: S_f\subseteq \mathbb{N}_{>0}\}$ . Consider $g\in G$ with $g(n)=n+1$ for all $n$ . It's easy to check that $g\in H$ . To arrive at a counterexample, we now show that $g^{-1}\notin H$ . Let $a\in G$ with $a(1)=2$ , $a(2)=1$ and $a(n)=n$ for $n\neq 1,2$ , so that $a\in A$ . Then $g^{-1}ag(0)=g^{-1}a(1)=g^{-1}(2)=1$ . This shows that $0\in S_{g^{-1}ag}$ and thus $g^{-1}ag\notin A$ so that $g^{-1}\notin H$ .","['intuition', 'group-theory', 'abstract-algebra', 'examples-counterexamples']"
3332260,Is a germ equivalent to an infinite jet?,"Not all smooth functions are analytic, as it is well known, so they in general cannot be represented as a power series. If we restrict our attention to analytic functions, then a specification of the values of all derivatives of a function at a point will give us the function. My question is essentially about how much information is contained in knowing all derivatives of a smooth but not necessarily analytic function at a point. In particular, let $(E, \pi, M) $ be a fibred manifold and let $\Gamma_x(E) $ denote the space of germs of smooth sections at $x\in M$ . A germ contains information about the section in an arbitrarily small open neighborhood of $x$ . However, practically thinking, the only definite values I can associate to a germ is the value of a representative section at $x$ , and the values of its derivatives at the same point to arbitrary orders. On the other hand, points of the infinite jet space $J_x^\infty(E) $ literally consist of values of a section and derivatives up to all orders at $x$ . Nonetheless I feel that the germ space might contain more ""nonlocal"" information than the infinite jet space. So the question is, how are $J_x^\infty(E) $ and $\Gamma_x(E) $ related?","['analysis', 'real-analysis', 'jet-bundles', 'germs', 'differential-geometry']"
3332265,Almost uniform convergence implies convergence in measure,"Let $(A,\mathcal{F},\mu)$ be finite measure space and $\{f_n\}$ a sequence of finite real measurable functions so that $f_n\rightarrow f$ a.e. We say $f_n\rightarrow f$ almost uniformly if $\epsilon>0$ , there is $E\subseteq A$ such that $f_n \rightarrow f$ uniformly on $E^c$ and $\mu(E)<\epsilon$ . I want to show that $f_n\rightarrow f$ almost uniformly implies convergence in $\mu$ . For this, suppose not. Then $$\exists \eta,\epsilon>0:\forall N\in \mathbb{N}:\exists n>N:\mu(\mid f_n-f\mid\geq\epsilon)\geq \eta, $$ i.e., for infinitely many points $n\in \mathbb{N}$ . From the definition of almost uniform convergence, $\exists E:\mu(E)<\eta$ and $f_n\rightarrow f$ uniformly on $E^c$ . Contradiction. Question It seems intuitive to me. But how to deduce this contradiction precisely?
I know that if $x\in E$ , then it must satify the negation of uniform convergence which is $$\exists \epsilon>0:\forall N\in \mathbb{N}:\exists n>N:\mid f_n-f\mid\geq\epsilon.$$ Now, $x$ may not be in $\{f_n \text{ does not converge in measure to } f \}$ if $\mu(\mid f_n(x)-f(x)\mid\geq\epsilon)<\eta$ . So I conclude that $$\{f_n \text{ does not converge in measure to } f \}\subseteq E$$ implying that $\eta>\mu(E)\geq \eta$ ; a contradiction. My argument seems right but also very inefficient. How could you express this idea as clean as possible? Thanks!","['measure-theory', 'proof-writing', 'real-analysis']"
3332271,What is the derivative for the function $f(x) = \sin^2(x)\sin(x^2)?$,"I understand the process of the product rule and chain rule etc. My answer led to $[\sin^2(x)\cos(x^2)(2x)] + \sin(x^2)(2\sin x \cos x).$ According to my solutions manual this is so far correct, however the final answer reads $2\sin(x)[x\cos(x^2)+\cos(x)\sin(x^2)].$ I assume this is some algebraic manipulation with maybe trig identities however I have no idea how. I'd appreciate if someone explained how to arrive at that answer, thank you!","['calculus', 'derivatives']"
3332275,Continuity of a strange function,"Let $f: [0,1)\to\mathbb{R}$ such that $f(x)=0.a_1a_3a_5\ldots$ where $x=0.a_1a_2a_3a_4\ldots$ , i.e, $f(x)$ skips the even digits of $x$ . Prove $f$ is continuous at $0$ , and find a point where $f$ is not continuous.
Updated: If the expansion of $x$ could be finite, we adopt the finite expansion. As we can see, $f(0)=0$ and $f(x)\geq 0$ for all $x\in[0,1)$ . To prove $f$ is continuous, we want to estimate $f(x)$ less than some elementary function $g(x)$ . I tried to estimate it, but the function is so strange. Did anyone see the similar function before? Any hint would be highly appreciated.","['continuity', 'calculus', 'real-analysis']"
3332279,"If $(X_n)$ is a martingale, prove that $(X_{n\wedge N})_n$ is a martingale, where $N$ is a stoping time","Let $(\Omega ,\mathcal F,\mathbb P)$ a probability space and $(X_n)_{n\in\mathbb N}$ a martingale w.r.t. the filtration $(\mathcal F_n)$ . Let $N$ a stopping time on $\mathbb N$ . Prove that $(X_{N\wedge n})_n$ is a martingale w.r.t. $(\mathcal F_n)$ . Attempts I have to prove that $$\mathbb E[X_{N\wedge (n+1)}\mid \mathcal F_n]=X_{N\wedge n}.$$ So let $F\in \mathcal F_n$ . Then, $$\mathbb E[\mathbb E[X_{N\wedge (n+1)}\mid \mathcal F_n]\boldsymbol 1_F]=\mathbb E[\boldsymbol 1_F X_{N\wedge (n+1)}]=\sum_{p=0}^\infty\mathbb E[\boldsymbol 1_FX_{N\wedge (n+1)}\mid N=p]\mathbb P\{N=p\}.$$ Q1) Is it true that $$\mathbb E[\boldsymbol 1_FX_{N\wedge (n+1)}\mid N=p]=\mathbb E[\boldsymbol 1_FX_{p\wedge (n+1)}]\ \  ?$$ If yes, then $$\sum_{p=0}^\infty\mathbb E[\boldsymbol 1_FX_{N\wedge (n+1)}\mid N=p]\mathbb P\{N=p\}=\sum_{o=0}^\infty \mathbb E[\boldsymbol 1_FX_{p\nabla (n+1)}]\mathbb P\{N=p\}=\sum_{p=0}^n\mathbb E[\boldsymbol 1_FX_p]\mathbb P\{N=p\}+\sum_{k=n+1}^\infty \mathbb E[\boldsymbol 1_FX_{n+1}]\mathbb P\{N=p\}.$$ Since $(X_n)$ is a martingale, $$\mathbb E[\boldsymbol 1_FX_{n+1}]=\mathbb E[\boldsymbol 1_F\mathbb E[X_{n+1}\mid \mathcal F_n]]=\mathbb E[\boldsymbol 1_FX_n]$$ Q2) How can I continue ?","['stochastic-processes', 'probability-theory', 'martingales']"
3332297,Prime counting function; is it true that $\pi(n - m) \geq \pi(n) -\pi(m)$?,"Let $\pi$ be the prime counting function, and $m, n$ positive integers, $n > m > 1$ . Is it proven that $\pi(n - m) \geq \pi(n) -\pi(m)$ ?","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3332301,Convergence of measure faster than $1/n$,"Given a measure space $(X,\Sigma, \mu)$ and  integrable function $f:X \to \mathbb{R}$ , I want to show that $\lim_{n \to \infty} n\mu(\{x\in X: |f(x) \geq n\})= 0$ . It is easy to see that $n\mu(\{x\in X: |f(x) \geq n\}) \leq \int_X |f| < +\infty$ . Therefore $\lim_{n \to \infty} \mu(\{x\in X: |f(x) \geq n\})= 0.$ But I am not sure how to show this convergence to $0$ is fast enough for $n\mu(\{x\in X: |f(x) \geq n\})\to 0$ .","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3332303,Meaningful connections between game theory and differential geometry,"I'm a 3rd year undergrad in mathematics who has recently developed a burgeoning interest in differential geometry. I'm also quite interested in dynamical systems and game theory, both of which are heavily employed in my research. Are there any meaningful connections between game theory and differential geometry? (hopefully beyond formulating differential games as mechanics problems but I'd love to learn more about that too). Anything to add would be greatly appreciated.","['geometry', 'reference-request', 'differential-games', 'game-theory', 'differential-geometry']"
3332370,Counterexamples concerning the central limit theorem,"The central limit theorem states that, if $X$ is a random variable with finite variance $\sigma^2$ and expected value $\mu$ , and if $(X_n)$ is a sequence of independent random variables identically distributed like $ X $ , then \begin{equation}
Z_n = {\frac{{\overline{X}}_n-\mu}{\sqrt{\sigma^2/n}}}\ \rightsquigarrow N(0,1),
\end{equation} where ${\overline{X}}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_i$ and $\rightsquigarrow$ means convergence in distribution, which in this case is equivalent to the pointwise convergence of the cdf of $Z_n$ to the cdf of a $N(0,1)$ . Suppose moreover that $X_n$ is absolutely continuous for all $n$ ‘s. Then $Z_n$ is absolutely continuous for all $n$ ‘s. Are there examples of such sequences $(X_n)$ , such that the pdf of $Z_n$ does not converge pointwise to the pdf of a $N(0,1)$ , not even almost everywhere (w.r.t. the Lebesgue measure on $\mathbb R$ )?","['central-limit-theorem', 'probability-theory', 'probability']"
3332389,The empty set is a subset of every set,"A question in Rudin's PMA is Prove that the empty set is a subset of every set. Of course, I know the proof goes something like this: Proof: Let $S$ be any set. The proposition $$\forall x: (x \in \varnothing \implies x \in S)$$ is true because for each $x,$ the proposition $x \in \varnothing$ is false, which makes the implication true. $\Box$ My question is about the quantifier. I have conveniently left out the domain for $x$ , because I'm not really sure what it should be. My best guess is that it depends how formal we want to be. If we are informal, we would say something like ""every object in the universe"" or some weird thing like that. If we want to be a little more formal, we would say something like ""all the objects in ZFC."" (though I myself don't really know what this means, because I only know very basic set theory/logic). So my main question is: what is the domain of $x$ in the above proof? Secondly, does the domain of a quantifier have to be a set, or not? Thanks.","['elementary-set-theory', 'logic', 'analysis']"
3332410,"For the purposes of DFT, is ""the"" primitive root of unity $w_n = e^{ 2\pi i / n }$ or $w_n = e^{-2\pi i / n }$?","I'm working on the section of Strang's Linear Algebra and Its Applications 4e that discusses discrete Fourier transforms. To make the exercises easier, I wrote myself a Python script that generates the $n$ th Fourier matrix as described here: I've run into trouble regarding the definition of $w$ . While any integer value of $k$ in $e^{ 2\pi k i / n }$ will get us a complex root of unity, to fill in the matrix, we need the primitive complex root of unity. Strang defines $w_n$ as $e^{ 2\pi i / n }$ : But when I coded $w_n$ as such in my DFT matrix function, it kept giving me the complex conjugate of what I wanted, and someone on Stack Overflow pointed out that I need to use $w_n = e^{-2\pi i / n }$ instead. This is consistent with the definition given on Wikipedia: But here's what throws me off. Looking at the unit circle, it seems like the primitive root of unity should be the one corresponding to a $2\pi / n$ rotation in the counterclockwise direction. That's what Strang does in this example diagram for $w_8$ : (The omission of $i$ is just a typo, right?) If we replace that circled bit with $e^{-2\pi i / 8 }$ , we get $\bar{w}$ , an angle in the fourth quadrant, and taking increasing powers of it will move us around the unit circle in a clockwise direction. If that's correct, Why does the Fourier matrix reverse the usual convention of counterclockwise rotation? Why does my textbook seem to state one definition and use another? In general, when should I use $w_n = e^{ 2\pi i / n }$ , and when should I use $w_n = e^{ -2\pi i / n }$ ? As always, thank you. Thank you all for your detailed explanations. Indeed, Strang's convention is to work with the inverse of what's described elsewhere, hence the change in sign. However, this introduces a new issue in one of the assignment problems. Question 3.5.3 asks, If you form a 3 by 3 submatrix of the 6 by 6 matrix $F_6$ , keeping only the entries in its first, third, and fifth rows and columns, what is that submatrix? The answer text reads simply, The submatrix is $F_3$ . But using my Python function with $w_n = e^{2\pi i / n }$ , I get the following inconsistent result: F6 matrix:
[[ 1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j   ]
 [ 1. +0.j     0.5+0.866j -0.5+0.866j -1. +0.j    -0.5-0.866j  0.5-0.866j]
 [ 1. +0.j    -0.5+0.866j -0.5-0.866j  1. -0.j    -0.5+0.866j -0.5-0.866j]
 [ 1. +0.j    -1. +0.j     1. -0.j    -1. +0.j     1. -0.j    -1. +0.j   ]
 [ 1. +0.j    -0.5-0.866j -0.5+0.866j  1. -0.j    -0.5-0.866j -0.5+0.866j]
 [ 1. +0.j     0.5-0.866j -0.5-0.866j -1. +0.j    -0.5+0.866j  0.5+0.866j]]

F6 submatrix:
[[ 1. +0.j     1. +0.j     1. +0.j   ]
 [ 1. +0.j    -0.5-0.866j -0.5+0.866j]
 [ 1. +0.j    -0.5+0.866j -0.5-0.866j]]

F3 matrix (should match the above)
[[ 1. +0.j     1. +0.j     1. +0.j   ]
 [ 1. +0.j    -0.5+0.866j -0.5-0.866j]
 [ 1. +0.j    -0.5-0.866j -0.5+0.866j]] You can see the code here .","['fourier-analysis', 'fourier-transform', 'matrices', 'linear-algebra', 'complex-numbers']"
3332418,Can you do modulos with irrational numbers?,"The other day, I was tutoring a fellow student on modulos, and we came across the topic of modular fractions. As far as I have learned, there are essentially two ways to do modulos with fractions, simply use them as remainders, or actually use some modulo manipulation. As an example I gave him, if you were to do $$9.5 \equiv ? \mod{5}$$ we could just write the answer as $4.5$ , that would be the ""easy"" solution, but instead you could solve the equation $$\frac{19}{2}\equiv \mod{5}$$ $$4\equiv2n \mod{5}$$ $$n\equiv2 \mod{5}$$ , so $9.5$ would in fact be equivalent to 2 modulo 5. Now, he asked a very intriguing question, what would $\pi$ be, say, modulo 2? I had no answer. My guess would be to take successively better approximations, but would that converge to one number? Does saying ""what is $\pi$ mod 2"" even make sense? Or is there no special answer and it's just 1.1415926...","['number-theory', 'irrational-numbers', 'modular-arithmetic']"
3332422,Compact metric space and convergent subsequence,"Let $X$ be a compact metric space. Show that every sequence has a convergent subsequence. Proof (revised version): Let $(x_n)_{n\in\mathbb{N}}\subseteq X$ be a sequence.
Let $\varepsilon >0$ be arbitrary. For every $\varepsilon$ is then $\bigcup_{x\in X} B_\varepsilon (x)\supseteq X$ an open cover. Since $X$ is compact there exists a finite subcover of $n_\varepsilon$ open balls with centerpoint $y^\varepsilon_1,\dotso, y^\varepsilon_{n_\varepsilon}$ $\bigcup_{i=1}^{n_\varepsilon} B_{\varepsilon}(y^\varepsilon_i)\supseteq X$ . Hence for every $\varepsilon$ at least one $B_\varepsilon(y^\varepsilon_i)$ consists of infinite elements of $(x_n)$ and thus $(x_n)$ has a convergent subsequence. Is this proof done sufficiently?
Thanks in advance.","['general-topology', 'proof-verification', 'metric-spaces', 'compactness']"
3332462,"Evaluate $\int _0^1\int _0^1\int _0^1\int _0^1\sqrt{(z-w)^2+(x-y)^2} \, dw \, dz \, dy \, dx$","This page contains an interesting identity $$\int _0^1\int _0^1\int _0^1\int _0^1\sqrt{(z-w)^2+(x-y)^2} \, dw \, dz \, dy \, dx=\frac{1}{15} \left(\sqrt{2}+2+5 \log \left(\sqrt{2}+1\right)\right)$$ Which calculates the average distance between two random points in the unit square. I'm wondering if there's an elementary solution? Any help will be appreciated.","['integration', 'definite-integrals']"
3332466,The Ho-Lee Model (1986),"(My question) I solved the following questions. However, if you know the other solutions, please let me know those along with computation processes. Besides, $W_t$ is a S.B.M. (Thank you for your help in advance.) (Cross-link) I have posted the same question on https://quant.stackexchange.com/questions/47306/the-ho-lee-model-1986 (Original questions) Consider a short term interest rate process $(r_t)_{ t \in \mathbb{R}_+ }$ in the Ho-Lee Model with constant coefficients: \begin{eqnarray}
dr_t = \theta dt + \sigma dW_t
\end{eqnarray} and let $P(t, T)$ will denote the arbitrage price of a zero-coupon bond in this model: \begin{eqnarray}
P(t, T) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} where $t \in [0, T]$ . (1) State the bond pricing PDE satisfied by the function $F(t, x)$ defined via \begin{eqnarray}
F(t, x) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] 
\end{eqnarray} where $t \in [0, T]$ . (2) Compute the arbitrage price $F(t, r_t) =P(t, T)$ from its expression of $P(t, T)$ as a conditional expectation. (3) Check that the function $F(t, x)$ computed in Question (2) does satisfy the PDE derived in Question (1). (1) My answer One can derive the following PDE by Feynman-Kac Theorem. \begin{eqnarray}
F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\
\exp \left( - \int^t_0 r_s ds \right) F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_0 r_s ds \right) | r_t=x \right] 
\end{eqnarray} One adapts It $\hat{o}$ 's formula the above equation. Here, one has to pay attention that the R.H.S is zero because it is a constant value, namely an expectation value. Moreover, $F(T, x)=1$ by the R.H.S going to $1$ because of $t=T$ . \begin{eqnarray}
&& d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\
&& \qquad \qquad \qquad \qquad \qquad = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\
&& \qquad \qquad \qquad  \qquad \qquad \qquad  + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) dr_t \nonumber \\
&& \qquad \qquad \qquad \qquad \qquad \qquad  + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) d[r_t] 
\end{eqnarray} One substitutes $r_t$ into the above equation. \begin{eqnarray}
&& d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\
&& = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) \left(  \theta dt + \sigma dW_t \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) \sigma^2 dt \\
&& = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \right) dt \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right)   \sigma  \partial_x F(t, x)  dW_t  
\end{eqnarray} Since the coefficient of the drift term is zero due to the martingale property of S.D.E, the following equation is obtained. Here, one has to pay attention to $r_t=x$ . \begin{eqnarray}
&& -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0 \\
&& -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0
\end{eqnarray} $\square$ (2) My answer Compute S.I.E by the given S.D.E. \begin{eqnarray}
dr_t &=& \theta dt + \sigma dW_t \\
r_t &=& r_0 + \theta t + \sigma W_t
\end{eqnarray} Let $x=r_t$ : \begin{eqnarray}
&& F(t, x) \\
&&= F(t, r_t) \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_0 + \theta s + \sigma W_s  \right)  ds\right) | \mathcal{F}_t\right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( \left( r_0 + \theta t + \sigma W_t\right)+ \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\
\end{eqnarray} Here, one computes $exp$ . \begin{eqnarray}
&& \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) \\
&& \qquad =\exp \left( - r_t \int^T_t  ds \right) \cdot  \exp \left( - \theta \int^T_t  (s-t) ds \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta \frac{1}{2} (T^2- t^2) + \theta t(T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T + t) - t\right)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T - t) \right)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t)ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right)  \cdot  \exp \left( - \frac{1}{2} \theta (T-t)^2 \ \right)  \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  - \frac{1}{2} \theta (T-t)^2  \right)  \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t)  ds \right) \\
\end{eqnarray} Substitute the above result into the Expectation. \begin{eqnarray}
&& F(t, r_t) \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  - \sigma  \int^T_t  ( W_s -W_t) ds \right) | \mathcal{F}_t \right] \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} One computes the above expectation value as below. \begin{eqnarray}
&&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  W_{s-t}  ds \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0  \left(  - \sigma W_s \right)  ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} Here, It $\hat{o}$ 's formula is applied to the exponent part, and further calculation is performed. \begin{eqnarray}
d \left( - \sigma  W_s s \right) &=& - \sigma  W_s ds  -  \sigma s d W_s + \frac{1}{2} 0 d [W_s] \\
&=& - \sigma   W_s  ds  -  \sigma s d W_s \\
\int^{T-t}_0 d \left( - \sigma  W_s s \right) &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\
 - \sigma  W_{T-t}  (T-t)  &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\
 \int^{T-t}_0 \left( - \sigma  W_s \right) ds  &=& - \sigma  W_{T-t}  (T-t) + \int^{T-t}_0 \sigma s d W_s \\
 &=& - \sigma  (T-t) \int^{T-t}_0 d W_s  + \int^{T-t}_0 \sigma s d W_s \\
 &=& \sigma \int^{T-t}_0 \left( s- (T-t) \right)  d W_s \\
 &=&  \int^{T-t}_0 \left( \sigma \left( s- (T-t) \right) \right)  d W_s \\
\end{eqnarray} Substitute the above result into the expectation. \begin{eqnarray}
&&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  - \sigma W_s \right) ds \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  \sigma \left( s - (T-t)\right) \right)  d W_s  \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2} \int^{T-t}_0 \left( s - (T-t) \right)^2 ds \right) \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2}  \left[ \frac{1}{3} \left( s - (T-t) \right)^3 \right]^{T-t}_0 \right) \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} Substitute the above result into the Expectation of the $F(t, r_t)$ equation. \begin{eqnarray}
&& F(t, r_t) \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \cdot \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right) \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} $\square$ (3) My answer Let $r_t=x$ into $F(t, r_t)$ of (2). \begin{eqnarray}
F(t, x) =  \exp \left( - x (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} Then, one reaches the following equations. \begin{eqnarray}
\partial_t F(t, x) &=&  \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\
\partial_x F(t, x) &=& - (T-t)  F(t, x) \\
\partial_{xx} F(t, x) &=&  (T-t)^2  F(t, x) \\
\end{eqnarray} Therefore, one computes the P.D.E of (1). Besides, the terminal condition is $F(T, x)=1$ . \begin{eqnarray}
&& -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad = -xF(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad \qquad  + \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad \qquad - \theta (T-t)  F(t, x) + \frac{1}{2}  \sigma^2 (T-t)^2  F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad = 0
\end{eqnarray} $\square$ (Thank you for your help in advance.)","['stochastic-pde', 'ordinary-differential-equations', 'stochastic-analysis', 'stochastic-processes', 'partial-differential-equations']"
