question_id,title,body,tags
1875825,General Position,"What does it mean when the columns of a matrix are in general position? I do not know if this is relevant or not, but the matrix in question is under-determined.","['matrices', 'linear-algebra']"
1875861,Find the value $\tan^{-1}\left(\frac{1}{\sqrt2}\right) - \tan^{-1}\left(\frac{\sqrt{5 - 2{\sqrt6}}}{1+ \sqrt{6}}\right)$,"The value of $$\tan^{-1}\left(\frac{1}{\sqrt2}\right) - \tan^{-1}\left(\frac{\sqrt{5 - 2{\sqrt6}}}{1+ \sqrt{6}}\right)$$is equal to $\frac{\pi}{6}$ $\frac{\pi}{4}$ $\frac{\pi}{3}$ $\frac{\pi}{12} $ $$\tan^{-1}\left(\frac{1}{\sqrt2}\right) - \tan^{-1}\left(\frac{\sqrt{3} -\sqrt{2}}{1+ \sqrt{6}}\right)$$
$$\tan^{-1}\left(\frac{1}{\sqrt2}\right) -\tan^{-1}{\sqrt3} +  \tan^{-1} {\sqrt2} $$
$$\implies\frac{\pi}{2} -\frac{\pi}{3}=\frac{\pi}{6}$$ Another possibility is $$\tan^{-1}\left(\frac{1}{\sqrt2}\right) +\tan^{-1}{\sqrt3} -  \tan^{-1} {\sqrt2} $$
How to solve this ?",['trigonometry']
1875878,Proving that $x^4 - 10x^2 + 1$ is not irreducible over $\mathbb{Z}_p$ for any prime $p$.,"So I have seen the similar question and answers on here for $x^4 +1$, but I am having trouble extending anything there to this polynomial... I understand it is fairly trivial with Galois theory, but my class has just barely covered Field Extensions, so suffice it to say we have no Galois theory to play with. I managed to prove it for the primes such that $p \equiv 1, 7 \pmod 8$, by noting that $2$ is a square modulo those primes and thus $x^4 - 2x^2 +1 = (x^2 -1 + 2qx)(x^2 - 1 - 2qx)$ for those $\mathbb{Z}_p$... however, trying to get a similar result for $3 \pmod 8$ and $5 \pmod 8$ has been stumping me for a long time, I am having a hard time making $q^2 = -1$ and $q^2 = -2$ give me something factorable... I guess the worst part of all of this is that I don't think this solution is even particularly enlightening, in terms of abstract algebra. It's really just some number theory trickery. I don't think my course has prepared me theoretically for this problem, does anyone have an elementary approach to it?","['irreducible-polynomials', 'abstract-algebra']"
1875902,"Let $f:R\to R$ a polynomial from degree $n\in N$, such that $f(x) \ge 0$ for all $x \in R$. Explain why n is an even number. [duplicate]","This question already has answers here : Prove that If $f$ is polynomial function of even degree $n$ with always $f\geq0$ then $f+f'+f''+\cdots+f^{(n)}\geq 0$. [duplicate] (2 answers) Closed 7 years ago . Let $f:R\to R$ a polynomial from degree $n\in N$, such that $f(x) \ge 0$ for all $x \in R$. 1.) Explain why n is even. 2.) Let $g= f+f'+f''+...+f^{n}$ (where $f^{k}$ is the kth derivative). Prove that g has an absolute minimum on R. 3.) Prove that $g(x)\ge 0$ for all $x\in R$ 1.) Given the function is $\ge 0$ means that the leading coefficient is positive. Using limits, when x approaches $ \pm \infty $ for even polynomials, the limit is $\infty$. Since f is a polynomial, f is continuous on all R. When x approaches $\pm \infty$ for odd polynomials the limit is $\pm \infty$ and from the intermediate value theorem, f(x) must take on every value in-between $\pm \infty$, therefore exists c such that $f(c) \lt 0$ which is a contradiction to what we were given. 2.) Again, g is a polynomial of an even degree. How to solve this is beyond me. 3.) Again, not sure where to begin. Id appreciate any help.","['real-analysis', 'polynomials', 'calculus', 'functions']"
1875984,Compute Triple integral,"Compute $$\iiint\limits_{x^2+y^2+(z-2)^2\leq1}\frac{dx\,dy\,dz}{x^2+y^2+z^2}$$ I tried to convert it to spherical cordinates and I got $$\iiint \sin\theta \,dr\,d\theta \,d\varphi$$ However I am lost on how to find the boundaries of the new domain, can anyone help?","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
1875988,Space of all sequences converging to $0$ is separable,"Denote by $X$ the space of all real sequences convergent to $0$ and equip it with the metric $d(x,y) =\sup \left\{|x_n - y_n|: n \in\mathbb N\right\}$. Prove that $(X,d)$ is separable. The solution is Exercise $5$ here http://userwikis.fu-berlin.de/download/attachments/460783619/proofs_topologyI.pdf?api=v2 It says that the set of all rational sequences converging to $0$ is countable ""by Cantor's diagonal argument"". I don't believe this is true, for example to each real number in $(0,1)$, $x = \sum_{n \ge 1} 10^{-n} x_n$, associate the sequence $x_n/n^2$. Isn't this an injection to that set? My idea was to take the set of rational sequences which vanish after a natural number. I think that we can write this set as (where $Q$ is rational numbers): $$\cup_{k \ge 1}\left[ \cup_{1 \le n \le k} Q^n \times \prod_{j \ge k+1} \{0\}\right]$$ This set is countable and dense. Does this make sense to you?","['general-topology', 'metric-spaces', 'elementary-set-theory', 'proof-verification']"
1875995,"If $(w + 1)(w - 1) = w$, find $ { w }^{ 10 }+\frac { 1 }{ { w }^{ 10 } } $.","Recently I was asked a question by my student that completely stumped me. $$\text{If }(w + 1)(w - 1) = w\text{, find } { w }^{ 10 }+\frac { 1 }{ { w }^{ 10 } }. $$ One ""cheat"" method that I used was to solve for the exact value of $w$ from the given first equation, and then substitute it into the requested expression that we were asked to find. I got $123$ as the answer.
However, I'm quite sure there's an algebraic way to solve this. 
Anyone wants to give this a ahot?",['algebra-precalculus']
1876007,Motivation for the scaling in Tracy Widom distribution,"For past few months, I am self studying random matrices from the book  - An Introduction to Random Matrices by Anderson et.al. I was going through the result concerning the eigenvalues of the GUE at the edge of the spectrum (Theorem 3.1.4) I am trying to understand how the non intuitive scale factor N^(2/3) comes up. The proof of Lemma 3.7.2 seems to play the key role in the proof of the above theorem. Define $\displaystyle \psi_n(x)=\frac{e^{-x^2/4}H_n(x)}{\sqrt{\sqrt{2\pi}n!}}$ where $H_n(x)$ is the n-th Hermite polynomial and let $\displaystyle \Psi_n(x)=n^{1/12}\psi_n(2\sqrt{n}+\frac{x}{n^{1/6}})$. For this $\Psi_n(x)$ we have I went through the proof of lemma 3.7.2 which uses steepest descent. Why that scaling factor is used is hidden in that proof. The following are first few steps of the proof. Focus on the last paragraph of the above image. A taylor expansion of $F(s) = log(1+s) + s^2/2 - s$ starts with
  $s^3/3$ in a neighbourhood of $0$, explains the particular scaling for
  u. This line, present in the proof, seems to answer my question. However, I fail to understand how that explains the particular scaling of u . I am not so familiar with complex analysis. I would be highly obliged if anyone can explain the scaling ($n^{1/6}$) and the reasoning mentioned here.","['complex-analysis', 'probability-theory', 'random-matrices', 'proof-explanation']"
1876042,SVD and non-negative matrix factorization,"The SVD and NMF are seem to be very close, so the question: how can I obtain NMF of given matrix from its SVD decomposition? I've tried to zero-in all negative parts of SVD decomposition, but this gives bad results and iterative approach (zero-in, correct, zero-in) does not help either. Am I missing something or there is no meaningful relation between SVD and NMF?","['eigenvalues-eigenvectors', 'optimization', 'matrices', 'svd', 'linear-algebra']"
1876061,"does $f_n = |x|^{1 + 1/n} $ uniformly converge on $[-1,1]$?","Does $f_n = |x|^{1 + 1/n} $ converge uniformly to $|x| $ on $[-1,1] $ ? I have tried to calculate (for $x \in[0,1]) $ $\sup|f_n(x) - f(x)| = \sup(x-x^{1 + 1/n} ) $  i miss calculated the $x_n $ that the sup occurs at, this is the answer :
the sup occurs at $x_n = \left(\dfrac{n}{1+n}\right)^n $ 
and this implies that $sup_{x \in [0,1]}|f_n(x) -f(x) | \to 0 $when $ n \to \infty $","['sequences-and-series', 'calculus']"
1876062,Does $\lim_{x \to 0+} \left(x\lfloor \frac{a}{x} \rfloor\right)=a?$,"Does $\lim_{x \to 0+} \left(x\lfloor \frac{a}{x} \rfloor\right)=a?$ I'm going to say this statement is false, and try to use the properties of limits $$\lim_{x \to 0+} \left(x\lfloor \frac{a}{x} \rfloor\right)=\lim_{x \to 0+} x.\lim_{x \to 0+}\lfloor \frac{a}{x}\rfloor$$
=$0.\infty$ which is undefined. Or is it $\infty $ because x ends up being a little bigger than 0?","['real-analysis', 'ceiling-and-floor-functions', 'calculus', 'limits']"
1876070,What can we say about connected groups with the same Lie algebra?,Each (finite-dimensional) Lie algebra has exactly one simply connected Lie group associated to it (up to isomorphism). What can we say about all other connected groups with the same Lie algebra ? Thank you in advance,"['group-theory', 'lie-algebras', 'lie-groups']"
1876086,"What does the phrase ""except possibly at $a$ itself"" mean in the definition of a limit?","The definition of limit says that Let $f(x)$ be a function defined on some open interval that contains the number $a$, except possibly at $a$ itself. Then we say that the limit of $f(x)$ as $x$ approaches $a$ is $L$ If....{the rest of definition is left to make the question easier}. What does the phrase ""except possibly at $a$ itself"" mean?
What is the significance of defining the interval, open i.e why not closed?",['limits']
1876114,Taylor Expansion of log determinant of a matrix,"I came across this matrix expansion based on Taylor expansion, which I could not derive: let $A=(\Sigma(\theta^{'})-\Sigma(\theta))\Sigma^{-1}(\theta)$,
$$\log\det(I+A)=tr(A)-R_3$$
with
$$R_3\le c_3\sum_{i=1}^p\lambda_i^2$$
where $p$ is the dimension of $A$,$c_3$ is some constant, and $\lambda$s are the eigenvalues of $A$. Could anyone provide any hint on how to derive this equality for expansion or at least the inequality for bounding the $\log\det(I+A)$? Is this a problem on Taylor expansion of a function of single variable $\theta$? I can understand this result is reasonable, since first term is first order ($\sum_i\lambda_i$), the second term is bounded by second order terms. However, I cannot find a direct formula to derive this. Maybe relevant: in Anderson Intro to multivariate statistics, it has a Theorem A.4.8 states that: $$\det(I+xC)=1+xtr(C)+O(x^2),$$
could be useful?","['multivariable-calculus', 'linear-algebra']"
1876134,"Polynomial irreducibility test over finite field $\Bbb F_3$, $x^5-x-1\bmod 3$","I am reading a text, where it says that $X^5-X-1$ is irreducible modulo 3. I am not sure how I can know that. Could someone help? By the way, is there some practical trick to judge whether a polynomial irreducible over a finite field in general? Because when I calculate the galois group I find this information is very important, so I am very curious. Thanks!","['irreducible-polynomials', 'abstract-algebra', 'ring-theory', 'field-theory', 'primality-test']"
1876141,Finding the rank of the matrix directly from eigenvalues,"Let $e$ denote eigenvalues and
$$
e_1=0\\
e_2=2 \\
e_3=2 \\
$$ Let $B$ be a $3\times3$ matrix. This information is certainly enough to find the rank of the matrix B(according to Gilbert Strang) And the rank would be $r=2$. But how come? I know that the determinant is $0$ so rank can't be 3. But how come it's 2 but not 1? Can't it be 1? My approach of thinking is below . Is it because these three eigenvalues would correspond the two different eigenvectors. And since two different eigenvectors are in the $C(B)$ this would make B $r=2$. But how do we know that eigenvectors are linearly independent? are they always linearly independent? And do distinct eigenvalues always form the same eigenvector?","['matrices', 'eigenvalues-eigenvectors', 'matrix-rank', 'linear-algebra']"
1876148,how can I prove this $\sum_{k=1}^{n}\frac{1}{4k^{2}-2k}=\sum_{k=n+1}^{2n}\frac{1}{k}$,How can I prove the following equation?: $$s=\sum_{k=1}^{n}\frac{1}{4k^{2}-2k}=\sum_{k=n+1}^{2n}\frac{1}{k}$$ Simplifying both terms of the equation: $$\sum_{k=1}^{n}\frac{1}{4k^{2}-2k} = \sum_{k=1}^{n}\frac{1}{(2k-1)2k} =(\sum_{k=1}^{n}\frac{1}{2k-1}-\sum_{k=1}^{n}\frac{1}{2k})$$ $$\sum_{k=n+1}^{2n}\frac{1}{k}=(\sum_{k=1}^{2n}\frac{1}{k}-\sum_{k=1}^{n}\frac{1}{k})=(\sum_{k=1}^{n}\frac{1}{2k-1}+\sum_{k=1}^{n}\frac{1}{2k}-\sum_{k=1}^{n}\frac{1}{k})$$ Now we have: $$s=(\sum_{k=1}^{n}\frac{1}{2k-1}-\sum_{k=1}^{n}\frac{1}{2k})=(\sum_{k=1}^{n}\frac{1}{2k-1}\color{blue}{+}\sum_{k=1}^{n}\frac{1}{2k}\color{blue}{-\sum_{k=1}^{n}\frac{1}{k}})$$ How can I continue?,"['induction', 'discrete-mathematics']"
1876196,Non-math person needs verification re: their set notation,"Here is what I am attempting to say is: ""For every b in B, there is a set of r's such that for every v in V, there exists exactly one r"" $$\forall b \in B \{r \ | \ \forall v \in V, \exists ! r \}$$ In a similar vein, is it correct to say: $$ X = \{y = \sum j(k) \ | \ \forall c \in C, \exists ! y\}$$ Here, I am trying to say: ""There is some set X that is made up of y's (which are calculated via the summation of j(k) ) such that for every c in C, there exists only one y."" I tried the Wikipedia page for set builder notation, but all that did was confuse me even more. Another way to frame it is: I have a set of things that I value (V). For example, lets say SEX, DRUGS, and ROCK-AND-ROLL (or v1, v2, v3 if you prefer). I also have, at any given moment, a set (B) of actions (b1, b2, b3) that I can undertake. Every possible action in the set of all possible actions will yield exactly one return (r) for everything that I value. That is, every b from the set B will generate exactly one r for every v in V.","['notation', 'elementary-set-theory']"
1876206,"Minimum of f(x,y,z)","let $x,y,z> -1$,then how can we find the minimum of this function
$$f(x,y,z)=\frac{1+x^2}{1+y+z^2}+\frac{1+y^2}{1+z+x^2}+\frac{1+z^2}{1+x+y^2}$$
I think that we can use a famous inquality but  I can not find it. So thanks","['inequality', 'calculus']"
1876224,"$X$ first-countable, $A \subseteq X$, $x\in X$, then $x \in \text{Int }A\Leftrightarrow$ every sequence in $X$ converging to $x$ is eventually in $A.$","I want my proof (below) for the following proposition verified. Let $X$ be a first-countable topological space, let $A$ be a subset of $X$ and let $x\in X$. Then $x \in \operatorname{Int}A$   if and only if every sequence in X converging to $x$ is eventually in $A$. Some definitions: $\operatorname{Int}A$ is the union of all open sets contained in $A$. ""sequence is eventually in $A$"" means that all but finitely many of the terms in the sequence are contained in $A$. My attempt at the proof: Suppose $x\in \operatorname{Int}A$
  and there exists a sequence $(x_i)$
  in $X$
  which converges to $x$
  and there are infinitely many $x_i$
  in $X-A$.
 Let $U$
  be a neighbourhood of $x$
  contained in $A$.
 Then there exists a positive integer $N$
  such that $x_i\in U$
  for all $i\geq N$.
 But then $U$
  is not contained in $A$,
 this is a contradiction. Therefore $x$
  can not be in $\operatorname{Int}A$. Conversely, suppose that $x\notin \operatorname{Int}$
  and let $\mathcal{B}_x= \{ U_i\} _{i=1}^\infty$
  be a countable neighbourhood basis at $x$.
  It is clear that each $U_i$
  is not contained in $A$.
 For each $n=1,2,\ldots$,
 choose a point $x_n\in U_1\cap\cdots\cap U_n$
  and consider the sequence $(x_n)$.
 Then $x_n\rightarrow x$ and this sequence is not eventually in $A$, in fact, none of the terms in sequence are in $A$. To see that  $x_n\rightarrow x$, let $U$
  be a neighbourhood of $X$,
 then there exists some positive integer $k$
  such that $U_k$
  is contained in $U$,
 and so $x_i\in U$
  for all $i\geq k$.","['general-topology', 'sequences-and-series']"
1876249,A property of regular polygon,"I am looking for a proof of property as following: Let $M_1M_2...M_n$ be a regular polygon . Let $N_1N_2...N_n$ be its tangential polygon . Let $P$ be arbitraty point inside $M_1M_2....M_n$. Let $2n$ ray through $P$, such that the angle of neighbor rays is $\pi/n$. The rays meet in-sidelines of $M_1M_2....M_n$ at $A_1, A_2,....,A_n$. The rays meet in-sidelines of $N_1N_2...N_n$ at $B_1, B_2, B_3,....,B_n$. Then show that: $$\sum_{1}^{n}{ PB_i} = \sec{\frac{\pi}{n}}\sum_{1}^{n}{ PA_i} $$","['circles', 'plane-geometry', 'polygons', 'geometry']"
1876250,"Understanding the ""physical significance"" of Tr$(A^{-1}B)<1$ for $A,B$ positive definite trace n matrices","Let $A$ and $B$ be positive definite, $n\times n$ matrices, such that $\frac1n\operatorname{tr}(A)=\frac1n\operatorname{tr}(B)=1$. Consider the situation where $$\operatorname{tr}(A^{-1}B)<1.$$ My question is: physically, what does this mean? If $A=UDU^T$ and $B=U\Sigma U^T$ were simultaneously diagonalizable, this would mean that 
$$\frac1n\sum_{1}^n\frac{\sigma_i}{d_i}<1,$$ so that the average quotient of eigenvalues is less than 1. I believe this is equivalent to the energy of $B$ concentrating in the same places as $A$. (Am I missing the best way of interpreting this case?) What about the general case? Does this tell us anything about the relationship between the eigenstructure of $B$ and $A$? My intuition is that this would mean that $B$ correlates weakly with the weakest eigenspaces of $A$, and therefore most of the energy of $B$ concentrates on the dominant eigenspaces of $A$. Can this be made more rigorous? I would love to read a good exposition of some of the subtleties here.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1876259,Baby Rudin Chapter 1 Exercise 9: Complex Field ordering,"Problem: Suppose $z=a+bi$, $w=c+di$. Define $z<w$ if $a<c$, and also if $a=c$ but $b<d$. Prove that this turns the set of all complex numbers into an ordered set. Does this ordered set has the least-upper-bound property? In my attempt, I've been using Rudin's definition of an ordered field as a proof structure: Definition 1.17: An ordered field is a field $F$ which is also an ordered set , such that: $(1)$ $x+y<x+z$ if $x, y, z \in F$ and $y<z$ $(2)$ $xy>0$ if $x\in F, y \in F, x> 0,$ and $y>0$. There are some propositions about ordered fields Rudin provides as well which I can prove, but I'm stuck on a part of proving $(2)$. My attempt on $(2)$: Let $x =a+bi$, $y=c+di$. Since $0$ = $(0,0)$ or $0 + 0i$, by the problem's defined ordering, $x>0$ if $a > 0$ OR $a = 0$ and $b>0$. And $y>0$ if the same properties hold for $c, d$. $xy = (ac-bd) + (ad+bc)i$. There are four cases to consider if both $x >0$ and $y>0$: (1) $a >0, c>0$. What if $ac-bd<0$? $ac > 0$ but if $bd > ac$, then $xy$ is not greater than $0 + 0i$. (2) $a > 0, c = 0$ but $d > 0$. $ac = 0, bc = 0$, so $xy = -bd + (ad)i$. If $-bd < 0$, then $xy$ is not greater than $0 + 0i$. And so forth. Would appreciate any clarification on this. For the least-upper-bound property, does it suffice to state than for any non-empty subset $E = \{(c, d)\ :\ c, d \in \mathbb{R}\}$ of the set of complex numbers, we observe that all complex numbers are an ordered pair of real numbers and the real field has the least-upper-bound property so we can always find a complex number $(a, b)$, $a, b \in \mathbb{R}$ such that $a$ and $b$ are supremums of the respective sets $\{c\}$ and $\{d\}$ as defined for subset $E$?",['real-analysis']
1876305,"Probability of 20 heads without getting 2 tails in a row, versus 10 heads without getting 1 tail",Are the odds of flipping a coin and getting 20 heads without getting 2 tails in a row the same as flipping a coin and getting 10 heads without getting 1 tail?,['probability']
1876306,Second series solution to Differential Equation,"Find a series solution about the point $x=0$ for $\gamma\ne0$ of the
  differential equation $x^2y''-xy'+(1-\gamma x)y=0$ and write down the
  form of a second independent solution. So far I have identified $x=0$ as a regular singular point and so multipled through by $x$ to obtain an equidimenisonal equation. Let $y=\sum^{\infty}_{n=0} a_nx^{n+\sigma}$ and eventually arrived at $a_n=\frac{\gamma}{n^2}a_{n-1}$ and so $y=a_0(1+\frac{\gamma}{1^2}x+\frac{\gamma^2}{1^22^2}x^2+...+\frac{\gamma^n}{(n!)^2}x^n+...)$. Firstly I would ask if there is any way to simplify this sum, I don't recognise it and I can't see how to evaluate the series. Secondly, I don't know how to easily find the second solution, the wording 'write down' suggests that no additional working is required, I had a similar problem on an earlier question with finding a second solution, so this is the more important one to be answered. Thank you.",['ordinary-differential-equations']
1876318,Notation for working with multiple probability measures,"Say I have a random variable $X$, and two probability measures $\mathbb P$ and $\mathbb Q$. The standard notation for the pdf of $X$ is just $f_X(x)$. Does there exist any notation, or could anyone suggest some notation, for distinguishing between the pdf of $X$ under $\mathbb P$ and $\mathbb Q$ respectively?","['probability-theory', 'measure-theory', 'notation']"
1876322,The orthogonal group $O(q)$ associated with a quadratic form $A$,"Let $q \colon \mathbb{R}^n \to \mathbb{R}$ be a quadratic form with associated matrix $A \in $ Sym$(n, \mathbb{R})$, meaning that $q(x)=x^TAx$ for all $x$. Let us first assume that $q$ is non-degenerate, i.e. $A$ is invertible. My first question is topological: 1) How does one show that $O(q):=\{h \in GL(n,\mathbb{R}) \mid h^TAh=A\}$ is compact if and only if $q$ is positive or negative definite? My second question is of differential geometry: 2) I can show that $O(q)$ is a submanifold of $GL(n,\mathbb{R})$ of dimension $\frac{n(n-1)}{2}$, but what happens if $q$ is degenerate? Is $O(q)$ still a submanifold, and of what dimension? Any hint would be appreciated.","['general-topology', 'differential-geometry', 'lie-groups']"
1876324,What is the value of $a^2+b^2$?,"If the range of the function $$f(x)=\frac{x^2+ax+b}{x^2+2x+3}$$ is
  $[-5,4]$ then what is the value of $a^2+b^2$ ? [$a,b$ are natural numbers] What will the correct approach to this problem? I tried using the general method as follows:
$({x^2+2x+3})y={x^2+ax+b}$ or,$x^2(y-1)+x(2y-a)+(b-3y)=0$ Then since $x$ is real the discriminant should be greater than equal to 0. But this method isn't very efficient and quick for this problem.Any shortcuts possible?","['algebra-precalculus', 'functions', 'quadratics']"
1876325,Negation of statements,"I have a statement; for all integers $t$, $j$ (if $t$ is odd and $j$ is even, then $t+j$ is odd). $$
\forall t,j \in\mathbb{Z}:(O(t) \wedge E(j) )\implies O(t+j)
$$
where $E(z)$: $z$ is even $O(z)$: $z$ is odd. I am asked to prove this by negation, I am having trouble figuring out the negation that I should prove. Is it any of these? $$
\forall x,y: p \wedge q \implies r.
$$ Would I be taking the negation in parts (negation $p$ and $q$ then of $p$ implies $r$) which would be 
$$
\exists x,y: (\neg p \vee \neg q )\wedge \neg r
$$
""for some integers $x$ and $y$, if $x$ is not odd, or $y$ is not even and $x+y$ is not odd"" Or would i only follow the negation of $$ p \implies q $$ which would be: $$
\exists x,y: (p \wedge q) \wedge \neg r
$$ ""for some integers $x$ and $y$, $x$ is odd and $y$ is even and $x+y$ is not odd"" Are any of the above correct? I am having a hard time because making them back into english is weird.","['predicate-logic', 'logic', 'discrete-mathematics']"
1876336,Prove $ \sin x + \frac{ \sin3x }{3} + ... + \frac{ \sin((2n-1)x) }{2n-1} >0 $,"Prove that for $ 0<x< \pi $, $$ \quad S_n(x) = \sin x + \frac{ \sin3x }{3} + ... + \frac{ \sin((2n-1)x) }{2n-1} >0 \quad \forall n = 1,2,... $$ Having trouble with this problem. This is an olympiad-style question, so an answer that doesn't use calculus or analysis would be preferred. A possible approach is induction, but for this we need to find a function in terms of $n$ and $x$ so that we can actually use the inductive step. If anyone has any ideas they would be appreciated. If you really want to go down the calculus route (at this point I don't mind), then $ S_n' (x) = \cos x + \cos 3x + ... +\cos((2n-1)x) $ , which you can find a closed form for, but I don't know how useful that is.","['algebra-precalculus', 'contest-math', 'sequences-and-series']"
1876350,Ways of showing $\sum_\limits{n=1}^{\infty}\ln(1+1/n)$ to be divergent,"Show that the following sum is divergent
  $$\sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)$$ I thought to do this using Taylor series using the fact that
$$
\ln\left(1+\frac1n\right)=\frac1n+O\left(\frac1{n^2}\right)
$$
Which then makes it clear that
$$
\sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)\sim \sum_{n=1}^{\infty}\frac1n\longrightarrow \infty
$$
But I feel like I overcomplicated the problem and would be interested to see some other solutions. Also, would taylor series be the way you would see that this diverges if you were not told?","['real-analysis', 'logarithms', 'sequences-and-series', 'calculus', 'convergence-divergence']"
1876434,Central limit theorem for random variables with exactly a 2nd moment,"Consider $X_m$ which are independent, identically distributed random variables which have moments exactly up to order $2$ and no higher. This can be done in numerous ways. One is the following: let $a_n$ be a sequence of positive numbers that goes to zero and let $b_n$ be a sequence of positive numbers such that $\sum_{n=1}^\infty \frac{b_n}{a_n}$ converges. (For example, $a_n=1/n,b_n=2^{-n}$ does the job.) Let $f_n(x)=(2+a_n) 1_{[1,\infty)}(x) |x|^{-3-a_n}$. Then let $f(x)=\sum_{n=1}^\infty \frac{b_n}{\sum_{k=1}^\infty b_k} f_n(x)$. Then $f$ is a pdf of a r.v. which has no higher moments than the second (since each $f_n$ has exactly moments of order strictly less than $2+a_n$). The assumption about $b_n$ ensures that it actually does have a finite second moment. (In effect the trick here is $\bigcap_{n=1}^\infty [1,2+a_n)=[1,2]$.) Despite this somewhat pathological moment property, the classical central limit theorem still tells us that $\frac{\overline{X}_m-\mu}{\sigma/\sqrt{m}}$ converges in distribution to a $N(0,1)$ random variable. However, the most common quantitative estimates for the convergence rate, e.g. the Berry-Esseen theorem, require the existence of a moment higher than order $2$. What can be said about the convergence rate in cases like the one in the previous paragraph?","['probability-theory', 'central-limit-theorem']"
1876467,"How many turns, on average, does it take for a perfect player to win Concentration?","Let's say a person with perfect memory is playing the card-matching game Concentration . He/she randomly lays out 2n cards (ie. n card pairs) sorted randomly and repeatedly takes turns consisting of flipping over two cards, without looking at the first card . When the cards match they are removed from play. When cards do not match, they are turned back over. The player wins when all pairs are found and all cards are removed. A perfect game is played in just n turns, but even a perfect player (one with perfect memory) couldn't hope to come close to a perfect game as there would still be a lot of guessing required. How to you calculate the expected number of turns required to win for player with perfect memory? EDIT 2 -- clarifying the rules Since all the early answers seem to read this question as you CANNOT look at the first card, I'm editing that into the rules. I originally was thinking you CAN look at the first card (since that's how I'd learned the game and, to my knowledge, that is how it is usually played. For that rule adjustment, I've posted a new question here: How many turns, on average, does it take for a perfect player to win Concentration (adjusted)? EDIT -- Checking answers against a simulation The strategy proposed in AlgorithmsX's answer seems to make sense to me - that is, the game takes place in two distinct stages. Stage 1: flip all cards to learn where they are, and... Stage 2: clean up/perfectly find all matches (since you know where they are). Thus improving on a $2n$ turn game is pure chance; how many matches do you find in Stage 1? To check a given answer, I wrote the below code to simulate the problem. It just randomly shuffles an array with pairs from $0$ to $n-1$ and checks for adjacent pairs that would come up in the sweep during Stage 1 (so a 0-1 pair is a match, but 1-2 is not). This seems like a good way to check answers because (for me personally, at least) it's easier to reason about the simulation than the math. Currently, AlgorithmsX's answer for $n=3$ results in $5.6$, but I would expect it to be $5.4$ barring any errors in my simulation. Some simulated results (1 million trials) n     |  2   |   3   |   4   |   5   |    6    | turns | 3.33 |  5.40 |  7.43 |  9.44 |  11.45  | Code to simulate answer given $n$ function shuffle(array) {
  var currentIndex = array.length, temporaryValue, randomIndex;

  // While there remain elements to shuffle...
  while (0 !== currentIndex) {

    // Pick a remaining element...
    randomIndex = Math.floor(Math.random() * currentIndex);
    currentIndex -= 1;

    // And swap it with the current element.
    temporaryValue = array[currentIndex];
    array[currentIndex] = array[randomIndex];
    array[randomIndex] = temporaryValue;
  }

  return array;
}

var simulation = function(n, nTrials){

  // make array from 0 to n-1 with pairs
  var cards = Array.apply(null, {length: n}).map(Number.call, Number);
  cards = cards.concat(cards);

  var totalTurns = 0; 
  var trialsLeft = nTrials;

  while(trialsLeft>0){
    cards = shuffle(cards)
    var matches = 0;
    for(var i=0; i<n; i++){
      if(cards[2*i]===cards[2*i+1]){
        matches++
      }
    }
    totalTurns += 2*n-matches;
    trialsLeft--
  }
  return totalTurns/nTrials;
}","['probability', 'card-games']"
1876483,Exact Matrix Inversion,"I have the following matrix: $$
\left( \begin{array}{ccc}
13 & 9.1 & 8.19 & 8.281 & 8.9271\\
9.1 & 8.19 & 8.281 & 8.9271 & 10.02001\\
8.19 & 8.281 & 8.9271 & 10.02001 & 11.562759\\
8.281 & 8.9271 & 10.02001 & 11.562759 & 13.6147921\\
8.9271 & 10.02001 & 11.562759 & 13.6147921 & 16.27802631\end{array} \right) 
$$ When I find the inverse of this matrix on Excel, I get: $$
\left( \begin{array}{ccc}
5.657342657 & -48.60139861 & 124.4172494 & -122.3776224 & 40.79254079\\
-48.6013986 & 467.9916897 &-1266.958328 & 1288.221582 & -438.9197404\\
124.4172494 & -1266.958328 & 3565.198078 & -3723.319165 & 1293.334933\\
-122.3776224 & 1288.221582 & -3723.319165 & 3967.84588 & -1399.744047\\
40.79254079 & -438.9197404 & 1293.334933 & -1399.744047 & 499.9085881\end{array} \right) 
$$ These values are not exact values. I can indefinitely keep adding more and more decimal places on Excel - However,  I need exact values (in the form of a fraction). I tried converting the values in the initial matrix into fractions before inverting it (the following matrix), but had no luck: $$
\left( \begin{array}{ccc}
130/10 & 91/10 & 819/10^2 & 8281/10^3 & 89271/10^4\\
91/10 & 819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5\\
819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6\\
8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7\\
89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7 & 1627802631/10^8\end{array}\right) 
$$ How can I get an exact inverse? I'd like to do some matrix multiplication with the exact inverse, and so a method to  do that would also be much appreciated.",['matrices']
1876489,"joint, marginal density function","Let $X$ , $Y$ be two independent, random variables each follows the exponential law with parameter $\lambda$ a. Find the joint density function $f(u,v)$ where $V=X+Y$ and $U=X$? b. Find the marginal density function $f(v)$? Solution: a. I got $f(u,v)= {\lambda}^2 e^{-v}$ is this the right answer ? if yes should the double integral be 1 in this case ... I don't think it is 1. b. the joint density function doesn't depend on $u$ explicitly, what should we do in this case?","['probability-theory', 'probability', 'statistics']"
1876495,Convex domains are domains of holomorphy,If we know that the unit ball $\mathbb B_n\subset \mathbb C^n$ is a domain of holomorphy. Can we use this fact to conclude the fact that very convex domain in $\mathbb C^n$ is a domain of holomorphy?,"['complex-analysis', 'several-complex-variables', 'complex-geometry']"
1876503,Prove that $n^n$ is greater than $1\cdot3\cdot5\cdots(2n-1)$,"Prove that $\ n^n \ge (1)(3)(5)\cdots(2n-1)$ I can't think of how to start answering this question and it would be great help if someone could explain how I should go about doing it. Note:It is a sequence and series question(AP,GP,HP)",['sequences-and-series']
1876516,"Show that if $f=g\ \text{a.e.}$ on $[a,b]$ implies that $f=g$ on $[a,b]$.","Suppose that $f,g$ are continuous functions on $[a,b]$ . Show that if $f=g\ \text{a.e.}$ on $[a,b]$ then in fact $f=g$ on $[a,b]$ . Is the result true if $[a,b]$ is replaced by any measurable set? My effort : Let $A=\{x\in [a,b]:f(x)\neq g(x)\}$ . It is given that $\mu(A)=0$ where $\mu$ denotes measure of a set. To show that $A=\emptyset$ . Suppose that $A\neq \emptyset $ . Let $p\in A$ . Then $f(p)\neq g(p)$ and also $f,g$ are continuous at $p$ . As $\mu(A)=0$ then there exists a sequence of intervals $\{I_n\}$ such that $A\subset \bigcup_{n=1}^\infty I_n$ and $\sum_{n=1}^\infty l(I_n)<\epsilon$ for any $\epsilon>0$ . But I am unable to use the continuity of $f,g$ . Please give some hints.","['measure-theory', 'analysis']"
1876522,Showing that $\int_0^\infty x^{-x} \mathrm{d}x \leq 2$.,"This integral is very closely related to the sophmores dream that states $$ \int_0^1 x^{-x}\mathrm{d}x = \sum_{n=1}^\infty n^{-n} = 1.27\ldots $$ For example here http://en.wikipedia.org/wiki/Sophomore%27s_dream Now I want to bound the integral, and showing that is less that 2. 
For the interval $[0,1]$ a good bound is rewriting it to $\exp(x\log x)$ and using the expansion $$ 1 - x \log(x) + \frac12 (-x \log(x))^2$$ but how does one handle $[1,\infty)$ ? In this answer here How to evaluate $ \int_0^\infty {1 \over x^x}dx$ in terms of summation of series? gives bounds to the integral, but they are not tight enough.. So to taste my question again, how does one prove that $$ \int_0^\infty \frac{\mathrm{d}x}{x^x} \leq 2 $$","['real-analysis', 'approximation']"
1876524,Defining a uniform distribution of points in the plane,"I saw this question , and was wondering what is the best way to describe a random distribution of points in the plane such that the expected number of points in any region of unit area is $c$, where $c$ is a given positive constant, and such that the number of points in every pair of disjoint regions are independent.
$\def\lfrac#1#2{{\large\frac{#1}{#2}}}$ I can mimic the derivation of the Poisson distribution in the following manner: Take any $n \in \mathbb{N}_{>c}\,\,.$ Partition each unit square of the plane into an $n \times n$ grid of square cells. With probability $\lfrac{c}{n^2}$ put a point into each cell, distributed uniformly within the cell, independently of other cells. Then every region comprising $n^2$ cells has expected number of points $c$. Also every pair of disjoint regions have independent numbers of points. Now if $n \to \infty$, I presume that this will more and more accurately resemble the desired distribution of points. But I have no idea how to take such a limit, nor what the limit distribution is!. I suspect it might be important that the cells are getting smaller in both dimensions, but I don't know. (If each square is partitioned into $n$ horizontal rectangles instead, each rectangle comprising $1 \times n$ cells, and the probability changed to $\lfrac{c}{n}$, the number of points in two vertical regions each comprising $n^2 \times 1$ cells will not be independent. This dependence might not affect the final limiting distribution, if such a limit exists at all, but a priori it is not clear to me.) So how can we rigorously define such a distribution, and is it really the limit of the above concept, in which case how do we rigorously define this limit?","['probability-theory', 'poisson-distribution', 'probability-distributions']"
1876539,"Prove that if $ a,b,c > 0 $, then $ [(1 + a) (1 + b) (1 + c)]^{7} > 7^{7} (a^{4} b^{4} c^{4}) $.","Problem. Prove that if $ a,b,c > 0 $, then $ [(1 + a) (1 + b) (1 + c)]^{7} > 7^{7} (a^{4} b^{4} c^{4}) $. I donâ€™t know how to solve this problem... What I can think of is to just simplify this inequality:
$$
\left[ \frac{(1 + a) (1 + b) (1 + c)}{7} \right]^{7} > a^{4} b^{4} c^{4}.
$$
How can I proceed with solving this problem? Note: This is a question of sequence and series, specifically AM-GM-HM inequality...",['sequences-and-series']
1876559,What is an intuition behind total differential in two variables function?,"As the definition, the total differential of a differentiable function with two variables equal to:
$$
dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}dy
$$
Since there are innumerable derivable directions , I confuse it now. I have two confusion in follow: Why the total differential equal to sum of just two partial differentials? For a differentiable function, the total differential equal to sum of any two different direction's partial differentials?","['derivatives', 'differential-geometry']"
1876569,Sheaf Theory: Structure sheaf of fiber product scheme,"Let $X = Spec(A)$ and $Y = Spec(B)$ be affine schemes where $A$ and $B$ are $k$-algebras. Then it is known that their fiber product is given by 
\begin{equation}
X\times_{Spec(k)} Y = Spec(A\otimes_k B).
\end{equation}
Then I guess the structure sheaf $\mathscr{O}_{X\times_{Spec(k)} Y}$ is just a usual structure sheaf defined on each basic open sets $D(h) \subset X\times_{Spec(k)} Y, h \in A\otimes_k B$ by $\mathscr{O}_{X\times_{Spec(k)} Y}(D(h)) = (A\otimes_k B)_h$ and extended to arbitrary open subset $U \subset X\times_{Spec(k)} Y$ by the projective limit $\mathscr{O}_{X\times_{Spec(k)} Y}(U) = \varprojlim_{D(h) \subset U} \mathscr{O}_{X\times_{Spec(k)} Y}(D(h))$. Then is it true or not that $\mathscr{O}_{X\times_{Spec(k)} Y} = p^{-1}_X \mathscr{O}_X \otimes_k p^{-1}_Y\mathscr{O}_Y$? ($p_{X,Y}: X\times_{Spec(k)} Y \rightarrow X,Y$ are the projections). The basis of the topology of $X\times_{Spec(k)} Y$ is given by $\{D(f)\times_{spec(k)}D(g) = Spec(A_f\otimes_k B_g)|f \in A, g \in B\}$? How do I show that these are true, or otherwise, what is the nicest way to relate $\mathscr{O}_{X\times_{Spec(k)} Y}$ to $\mathscr{O}_X$ and $\mathscr{O}_Y$? I was hoping that (1.) is true and it seems to me that if (2.) were true it wouldn't be too hard to proof (1.). My thought on this is that for a basic open sets of type $h = f\otimes_k g$ it is quite clear that $D(f\otimes_k g) = Spec((A \otimes_k B)_{f\otimes_k g}) = Spec(A_f\otimes_k B_g) = D(f)\times_{Spec(k)}D(g)$. But I have no idea what do say when $h$ is not a pure tensor.","['sheaf-theory', 'algebraic-geometry']"
1876575,Is there a formula for the number of ways an $N\times N$ grid can be filled with squares?,How many ways are there to fill an $n\times n$ board with squares with varying sizes?,"['combinatorics', 'combinatorial-geometry']"
1876590,Constructing the 11-gon by splitting an angle in five,"In "" Angle Trisection, the Heptagon, and the Triskaidecagon "", published in the American Mathematical Monthly in March 1988, Andrew Gleason discusses what regular polygons can be constructed with compass, straightedge and angle trisector. At the end of that article he notes that the angle p -sectors required for a regular n -gon are the odd primes p dividing $\varphi(n)$. For the heptagon, which only requires an angle trisector, he gives the minimal polynomial of $2\cos(2\pi/7)$
$$x^3+x^2-2x-1$$
and transforms it into the Chebyshev polynomial expression
$$7\sqrt{28}(4\cos^3\theta-3\cos\theta)=7$$
leading to the final identity
$$\sqrt{28}\cos\left(\frac{\cos^{-1}(1/\sqrt{28})}{3}\right)=1+6\cos(2\pi/7).$$ I am interested in the hendecagon (11 sides), which requires an angle quinsector (that splits an angle into five equal parts). Is there a similar transformation between the minimal polynomial for $2\cos(2\pi/11)$
  $$x^5+x^4-4x^3-3x^2+3x+1$$
  and the relevant Chebyshev polynomial
  $$\cos 5\theta=16\cos^5\theta-20\cos^3\theta+5\cos\theta$$
  and how do I find it? If I had such a transformation, I could construct an exact hendecagon with the quinsector. I have tried Tschirnhaus transforms on the former polynomial, without success.","['polynomials', 'trigonometry', 'polygons', 'minimal-polynomials']"
1876618,On sum of determinants,$(1)$ Are there any special non-trivial classes of $n\times n$ square matrices where $$\det(A)=\sum_{i=1}^m\det(A_i)$$ at some (not necessarily any ) $m$ satisfying $2\leq m\leq n$ where $A=\sum_{i=1}^mA_i$ holds? $(2)$ Supposing if $A_i$ are symmetric and positive definite is it true that $$\det(A)\geq\sum_{i=1}^m\det(A_i)$$ holds at any $n\geq1$ if $A=\sum_{i=1}^mA_i$ holds (if true or not are there any other classes of matrices for which this holds)? $(3)$ Are there any classes of non-trivial matrices for which we can have $$\det(A)>\sum_{i=1}^m\det(A_i)$$ holding true if $A=\sum_{i=1}^mA_i$ holds?,"['matrices', 'positive-definite', 'determinant', 'symmetric-matrices', 'linear-algebra']"
1876634,To find last two digits of $2^{100}$,To find last two digits  of $2^{100}$ I have just learned about modular arithmetic and I wanted to solve this problem. I only know about equivalence classes and about  $a=b \pmod n$. I have also learned about multiplication and addition of classes.  Can someone explain to me step by step on how to apply modular arithmetic to this problem? Thanks,"['decimal-expansion', 'group-theory', 'number-systems', 'modular-arithmetic', 'elementary-number-theory']"
1876639,"$\tan^{-1}x$, $\tan^{-1}y$, $\tan^{-1}z$ are in arithmetic progression, as are $x$, $y$, $z$. Show ...","$\tan^{-1}x, \tan^{-1}y, \tan^{-1}z $ are in arithmetic progression, as are $x$ , $y$ , $z$ . (We assume $y \ne 0,1,-1$ .) Show: $x$ , $y$ , $z$ are in geometric progression $x$ , $y$ , $z$ are in harmonic progression. $x=y=z$ $(x-y)^2 +(y-z)^2+(z-x)^2 =0$ My attempt: $$\text{A}=\tan^{-1}x \qquad \text{B}=\tan^{-1}y \qquad \text{C}=\tan^{-1}z$$ $$x=\tan A \qquad y=\tan B \qquad  z=\tan C $$ $$x+z=2y$$ $$A+C=2B$$ $$\tan(A + B + C)=\frac{\tan A +\tan B +\tan C - \tan A\tan B\tan C }{1-\tan A\tan B -\tan B\tan C -\tan C\tan A}$$ $$\tan(3B)=\frac{x +y +z - xyz }{1-xy -yz -zx}$$ How do I continue from here?",['trigonometry']
1876645,Functional equation: $f(x)=x+\dfrac{f(2x)}{f(3x)}$,"Consider the functional equation: $$f(x)=x+\dfrac{f(2x)}{f(3x)}$$ where $x>0$, together with the boundary condition $f(x)=x+O(1)$ as $x\to\infty$ (that is, $f(x)-x$ is bounded). Furthermore suppose that $f(x)$ is continuous. What is the value of $f(1)$? One approach is to expand $f(x)$ as a Laurent series of the form
$$
f(x)=x+a_0 + \frac{a_1}{x}+\frac{a_2}{x^2}+\cdots
$$
Set $f_n(x)=x+a_0+\cdots+a_n x^{-n}$, and observe that $f_{n+1}(x)=x+\frac{f_n(2x)}{f_n(3x)}+o(x^{-n-1})$. This allows one to recursively compute the coefficients of the Laurent series very easily; using this technique I obtained that
$$
f(x)=x+\frac{2}{3}+\frac{2}{27x}-\frac{7}{729x^2}+\frac{227}{236196 x^3}-\frac{34925}{459165024 x^4}+o(x^{-4}).
$$
While I was unable to discern a pattern for these coefficients, it does appear that the $3$-adic valuation of $a_n$ is $n(n+1)/2$. Source: This question is a variant of a question posed by Jacob Lance on the mathriddles subreddit involving a continued fraction. He asked whether $f(1)<\sqrt{3}$. (One may show that in fact $f(1)\approx \sqrt{3}-2\cdot 10^{-5}$.)","['continued-fractions', 'power-series', 'functions', 'functional-equations']"
1876659,Show that $AB-BA\ne C$ for every real $3\times 3$ matrices $A$ and $B$ and some specific $3\times3$ matrix $C$,"Prove that there cannot exist real $3 \times 3$ matrices $A$ and $B$ such that $$AB-BA= \begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 0 \\
0 & 1 & 1\end{bmatrix}$$ I have no idea to solve it as both $A$ and $B$ are unknown.","['matrices', 'matrix-equations', 'linear-algebra']"
1876670,Finding Side Length of Square Adjacent to Square Inscribed in Unit Circle,"I was working on this problem I came up with where you have a unit circle with a square inscribed in it.  Finding the side lengths of this square is trivial ($\sqrt{2}$).  However, I wanted to extend this problem.  I thought, what if I placed a square adjacent to the previous one such that its left side touches the previous square's right side, it is centered on the $x$-axis, and it has its two right corners touching the circle?  Here's a diagram I made of the problem below. I've come as far as setting up the equation, but I cannot see a way to go about solving it.  If I traced a line from the center of the circle to the top right corner of the square, I have a right triangle.  I can use trigonometry to find the side lengths of such a triangle. $$
x = \cos \theta \\
y = \sin \theta
$$ I can deduce that the side length $s$ of the square is equal to twice the height of this triangle and also equal to the difference between the base of this triangle and half the side length of the previous square. $$
s = \cos \theta - \frac{\sqrt{2}}{2} \\
s = 2 \sin \theta
$$ By setting these two equations equal to each other, I can attempt to solve for $\theta$, thereby allowing me to solve for the side length of the square. $$
\cos \theta - \frac{\sqrt{2}}{2} = 2 \sin \theta
$$ After reaching this point, I am not sure how to continue.  I can immediately see that I cannot solve for $\theta$ easily by just rearranging the equation.  I've thought about substituting $\cos \frac{\pi}{4}$ back in for $\frac{\sqrt{2}}{2}$ to make use of some obscure trigonometric identity, but nothing I've tried gets me anywhere near solving the equation.  Any hints or solutions would be greatly appreciated.  Thank you for taking the time to look at this.","['trigonometry', 'geometry']"
1876685,Polynomial roots related question,If a polynomial $P(x)$ with integer coefficients has degree $d\ge 0$ Then prove that $P^2(x)-1$ can have atmost $(d+2)$ integer roots. I am not able to solve this. Basically I was trying to show that if $P(x)=1$ has $d$ distinct integer roots then $P(x)=-1$ can have maximum two distinct integer roots,"['polynomials', 'linear-algebra']"
1876706,Jacobian elliptic surface $J(\mathcal{E})$ for a given elliptic surface $\mathcal{E}$ without a section.,Let $\mathcal{E}$ be an elliptic surface over $\mathbb{P}^1$ without a section. What is a right definition of its Jacobian elliptic surface $J(\mathcal{E})$? Is it unique up to birational isomorphisms over $\mathbb{P}^1$? How can I find its Weierstrass equation $\mathcal{W}$?,"['elliptic-curves', 'algebraic-geometry']"
1876715,Proving that the sum of any two sides of the triangle is greater than the third side,"I know it's easy to prove with the help of linear inequalities, but this time I want to prove it with the help of trigonometry. Is it possible? If yes, then how?","['inequality', 'trigonometry', 'triangles', 'geometry']"
1876716,How to simply: $\prod_{k=1}^{100}(1+2cos\frac{2\pi.3^k}{3^{100} +1})$?,$$\prod_{k=1}^{100}\left(1+2\cos\left(\frac{2\pi.3^k}{3^{100} +1}\right)\right)$$ equals .I tried do this problem many time but i don't figure outhow to do this problem .,['trigonometry']
1876737,A property of continuous bijective maps in the plane.,"Take a non-empty open connected bounded set $D$ of $\mathbb{R}^2$ and let $f:D\to D$ be a continuous bijective map. It is well known that $f$ could have no fixed points i. e. $f(p)\not=p$ for all $p\in D$. I wonder if the following weaker property holds:
there is a point $p\in D$ such that $d(f(p),\partial D)=d(p,\partial D)$ where $d(z,\partial D)$ is the euclidean distance from a point $z\in \mathbb{R}^2$ to the boundary of $D$. Any existing reference in the literature will be appreciated.","['fixed-point-theorems', 'real-analysis']"
1876746,Equality of two definitions of the Drinfeld Double,"While studying the Drinfeld Double of a Hopf algebra, I came across two different definitions used for the multipliaction. For a finite dimensional Hopf algebra $H$ (over a field $K$) we define $D(H)=H^{*cop}\otimes H$ (the Drinfeld double of H) as a Hopf algebra with the following structure:
    \begin{align*}
	&(\varphi \otimes g)(\psi \otimes h)= \psi_{(1)}(S^{-1}(g_{(3)}))\psi_{(3)}(g_{(1)})\varphi\psi_{(2)}\otimes g_{(2)}h		&& 1_{D(H)}=\varepsilon\otimes 1_H\\
	& \Delta ( \varphi \otimes h)=(\varphi_{(2)}\otimes h_{(1)}) \otimes (\varphi_{(1)}\otimes h_{(2)}) && \varepsilon(\varphi \otimes h)=\varphi(1)\varepsilon(h) \\
	& S(\varphi \otimes h)=(\varepsilon \otimes S(h))(S^{-1}(\varphi)\otimes 1).
	\end{align*} In addition I came across the following definition of the multiplication:
$$ (\varphi \otimes g)(\psi \otimes h)=\varphi(g_{(1)} \rightharpoonup \psi \leftharpoonup S^{-1}(g_{(3)}))\otimes g_{(2)}h .$$
The notation we use here is defined like this: Let $A$ be an algebra. For $a \in A$, $\varphi \in A^*$ we define $a \rightharpoonup \varphi, \varphi \leftharpoonup a \in A^*$ with $(a \rightharpoonup \varphi)(b)=\varphi(ba)$ and $(\varphi \leftharpoonup a)(b)=\varphi(ab)$ for every $b \in A$. My goal is to show that both multiplications are equal. This is my approach: \begin{align*}
(\varphi \otimes g)(\psi \otimes h) & = \psi_{(1)}(S^{-1}(g_{(3)}))\psi_{(3)}(g_{(1)})\varphi\psi_{(2)}\otimes g_{(2)}h \\
& = (\psi_{(1)} \leftharpoonup S^{-1}(g_{(3)}))(g_{(1)} \rightharpoonup \psi_{(3)})\varphi\psi_{(2)} \otimes g_{(2)}h
\end{align*}
On the other hand we have:
\begin{align*}
(\varphi \otimes g)(\psi \otimes h) & = \varphi(g_{(1)} \rightharpoonup \psi \leftharpoonup S^{-1}(g_{(3)}))\otimes g_{(2)}h  \\
& = \varphi(g_{(1)} \rightharpoonup \psi_{(2)})\psi_{(1)}(S^{-1}(g_{(3)})) \otimes g_{(2)}h \\
& = \varphi\psi_{(2)}(g_{(1)})\psi_{(1)}(S^{-1}(g_{(3)})) \otimes g_{(2)}h \\
& = \psi_{(1)}(S^{-1}(g_{(3)})) \psi_{(2)}(g_{(1)})  \varphi \otimes g_{(2)}h
\end{align*}
As you can see, I am trying to show the equality by starting from both sides. But with my attempt at the bottom I am still missing a $\psi$ and have to make sure while adding it that it gets the right index. On the upper attempt I have to remove $\psi_{(2)}$ but do not see how it is done.","['abstract-algebra', 'hopf-algebras']"
1876753,The derivative of $\int_0^{\sin x} \sqrt{1-t^2} dt$ is given as $\lvert \cos x\rvert \cos x$. But why the absolute value?,"The given problem: Find the derivative of $$ F(x)=\int_0^{\sin x}\sqrt{1-t^2}dt $$ I know the answer is supposed to be $\lvert \cos x\rvert \cos x$, but I don't understand where the absolute value comes in. Using the fundamental theorem, my working out was as follows: \begin{align}
F'(x) & =\cos x \sqrt{1-sin^2x}\\
& = \cos x \sqrt{\cos^2x} \\
& = \cos x (\pm \cos x)\\
& = \pm \cos^2x
\end{align} Which was evidently wrong, but I don't understand why the sqrt of $\cos^2x$ must be an absolute value. It is a square, so what would be the problem if it were negative? Am I missing something very simple? I'm a long distance student so I can't exactly raise my hand in class and ask what's what. Afterthought: Perhaps I'm grabbing the wrong end of the stick and it's the other cos term that's been made an absolute value, not the sqrt...even if that were the case though I'd still be confused.","['integration', 'trigonometry', 'calculus']"
1876803,"If a series converges, then the sequence of terms converges to 0, Proof by contradiction.","I know there is an elegant proof of this theorem,
But I've been ask to prove it in the test and the only thing I was thinking of was to prove it by contradiction.I want to know if I did right, or maybe I was assuming something wrong: ${S_n} = \sum _{n=0}^{\infty }\:a_n$ converge and assume that $\lim _{x\to \infty }\left(a_n\right)\:=\:L\:\in \mathbb{R}$    (L isn't zero) So there is $N\in \mathbb{N}\:\:\:\:\:\forall \:n>\:N\:\:\rightarrow \:a_n>\:L-\epsilon$ So if  $a_n>0 \Rightarrow \sum _{n=0}^{\infty }\:a_n>\sum _{n=0}^{\infty }\:L-\epsilon =\:\lim _{n\to \infty }\left(n\cdot \left(L-\epsilon \right)\right)=\infty \:$ and that's mean the series diverge. And if $a_{n\:}$ can be positive and negative, let's look at both $\left(S_{n_k}\right)$ the series of the non-negative number $\left(S_{n_r}\right)$ the series of the negative number 
that will converge to  $\infty \:,\:-\infty $ So that means ${S_n}$ doesn't have a limit What do you say guys? Is it legit prove  ?","['infinity', 'calculus', 'limits']"
1876848,Find all functions on the non-zero reals to itself satisfying $f(xy)=f(x+y)(f(x)+f(y))$,"Find all functions $f\colon\mathbb{R}\setminus\{0\}\to\mathbb{R}\setminus\{0\}$ such that $f(xy)=f(x+y)(f(x)+f(y))$. I'm reasonably confident that the solutions are $f(x)=\frac{1}{x}$ and $f(x)=\frac{1}{2}$. Here's some progress: Let $P(x,y)$ be the assertion $f(x+y)(f(x)+f(y))=f(xy)$, and let $a=f(1)$. $P(1,1)\implies f(2)=\frac{1}{2}$. $P(2,1)\implies f(3)=\frac{1}{2a+1}$. $P(3,1)\implies f(4)=\frac{1}{2a^2+a+1}$. $P(4,1)\implies f(5)=\frac{1}{2a^3+a^2+a+1}$. $P(5,1)\implies f(6)=\frac{1}{2a^4+a^3+a^2+a+1}$. $P(2,3)\implies (2a-1)(a-1)(a+1)(2a^2+a+1)=0$ and so $\boxed{a\in\left\{-1,\frac{1}{2},+1\right\}}$. If $a=1$, then inductively $f(n)=\frac{1}{n}$ $\forall n\in\mathbb{N}$. Let $x>0$. Then if $n\in\mathbb{N}$, considering $P(x+n,1)$ inductively, we get $f(x+n)=\frac{f(x)}{nf(x)+1}<\frac{1}{n}$, and so $P(x,n)\implies f(nx)=\frac{f(x)}{n}$. Thus $f(x)\leq\frac{1}{n}$ if $x\geq n$, so for $x\geq1$, we have $f(x)\leq\frac{1}{\lfloor x\rfloor}$. Then for $x>1$, inductively we have $$f(x)^{2^n}=f\left(x^{2^n}\right)\leq\frac{1}{\left\lfloor x^{2^n}\right\rfloor},$$ so taking the $2^n$th root and $n\to\infty$, we get $f(x)\leq\frac{1}{x}$ for all $x\geq1$ (Could someone verify that this part is rigorous?) . But since $f(2^nx)=\frac{f(x)}{2^n}$, we get $f(x)\leq\frac{1}{x}$ for all $x\geq\frac{1}{2^n}$. Again, taking $n\to\infty$, $f(x)\leq\frac{1}{x}$ for all $x>0$. Now $P\left(x,\tfrac{1}{x}\right)\implies1=f(1)=f(x+1/x)(f(x)+f(1/x))\leq1$, so $f(x)=\frac{1}{x}$ $\forall x>0$. However, I'm struggling first to extend this to $x<0$, and secondly to deal with the cases $a=\frac{1}{2}$, $a=-1$. EDIT: I've solved the problem. If $x<0$, then $f(x)^2=f\left(x^2\right)\implies f(x)=\pm\frac{1}{x}$. Choosing $n>|x|$, we have $$\frac{1}{x+n}=f(x+n)=\frac{f(x)}{nf(x)+1},$$and it is clear then that $f(x)=\frac{1}{x}$ $\forall x\in\mathbb R\setminus\{0\}$, which is indeed a solution. If $f(1)=\frac{1}{2}$, then we claim that $\boxed{f(x)=\frac{1}{2}\;\forall x\in\mathbb{R}\setminus\{0\}}$ is the unique solution. Inductively, $f(n)=\frac{1}{2}$ $\forall n\in\mathbb N$. $P(x,1)\implies f(x+1)=\frac{2f(x)}{2f(x)+1}$, and thus $f(x+2)=\frac{4f(x)}{6f(x)+1}$ and $f(x+4)=\frac{16f(x)}{30f(x)+1}$. $P(x,2)\implies f(2x)=\frac{2f(x)(2f(x)+1)}{6f(x)+1}$. $P(2x,2)\implies f(4x)=\frac{4f(x)(2f(x)+1)(8f(x)^2+10f(x)+1)}{(6f(x)+1)(24f(x)^2+18f(x)+1)}$. $P(x,4)\implies f(4x)=\frac{8f(x)(2f(x)+1)}{30f(x)+1}$. Equating the two expressions for $f(4x)$, we get $(2f(x)-1)^2(2f(x)+1)(12f(x)+1)=0$. Thus $f(x)=\frac{1}{2}$ $\forall x\in\mathbb R\setminus\{0\}$. If $f(1)=-1$, then we claim that there are no solutions. $P(x,1)\implies f(x+1)=\frac{f(x)}{f(x)-1}$, so $P(x+1,1)\implies f(x+2)=f(x)$. Thus $f(4)=f(2)=\frac{1}{2}$. But
\begin{align*}P(x,2)&\implies f(2x)=f(x+2)\left(f(x)+\tfrac{1}{2}\right)=f(x)\left(f(x)+\tfrac{1}{2}\right)\\
P(x,4)&\implies f(4x)=f(x+4)\left(f(x)+\tfrac{1}{2}\right)=f(x)\left(f(x)+\tfrac{1}{2}\right)\end{align*}Thus $f(x)=f(2x)=f(x)\left(f(x)+\tfrac{1}{2}\right)\implies f(x)=\frac{1}{2}$ $\forall x\in\mathbb R\setminus\{0\}$, a contradiction.","['algebra-precalculus', 'functions', 'functional-equations']"
1876865,Polynomial ring: $X$ capital or small letters,"For a polynomial ring $F[X]$ where $F$ is a field, what is the usual convention for capital or small letters for $X$? Q1) I.e., should it be $F[X]$ or $F[x]$? I guess theoretically it doesn't matter, just curious to know what is the usual convention. Q2) Also, just curious why when it comes to polynomial rings, people suddenly use capital letters to denote polynomials, e.g. $X^2+1$, instead of $x^2+1$ which is more common in other branches of math? Is there a deep reason? This trivial question has been bugging me for some time, asking it now that I remember it. Thanks.","['abstract-algebra', 'soft-question']"
1876867,Evaluation of given limit when $f(x)=\sum^{n}_{k=1} \frac{1}{\sin 2^kx}$ and $g(x)=f(x)+\frac{1}{\tan 2^nx}$,"Question Statement:- If $\displaystyle f(x)=\sum^{n}_{k=1} \frac{1}{\sin 2^kx}$ and $g(x)=f(x)+\dfrac{1}{\tan 2^nx}$ , then find the value of $$\lim_{x\to 0} \bigg( (\cos x)^{g(x)}+\bigg(\frac{1}{\cos x} \bigg)^{\frac{1}{\sin x}} \bigg)$$ I am not able to find value of $g(x)$ . Could someone help me as how to calculate value of $g(x)?$","['calculus', 'limits']"
1876881,How to understand the change of basis in a differential equation,"Let $w:\mathbb{R} \times \mathbb{R}^n \rightarrow Mat(n,\mathbb{R})$ be a smooth function, $R_{ij}$ be a fixed skew-symmetric $n\times n$ real matrix, and $A\in\mathbb{R}$. Consider the equation $$\frac{\partial w}{\partial t} - \sum_{i=1}^n\left(\frac{\partial}{\partial x^i} + \frac{1}{4}\sum_j {R}_{ij} x^j\right)^2 w + Aw = 0.$$ My question is, if one wants to solve for $w$, why is it sufficient to take $R_{ij}$ to be block-diagonal, consisting of $2\times 2$ skew-symmetric blocks? I know that a skew-symmetric matrix can always be block-diagonalised this way, but am having a hard time writing down rigorously what the solution to the equation would be if $R$ was changed to a block-diagonal form, say $P^{-1} RP$. I'm looking for a proof that shows something like, if $w$ solves the above equation, then $P^{-1}wP$ solves the equation with $R$ replaced by $P^{-1}RP$. Thanks for any help.","['ordinary-differential-equations', 'linear-algebra']"
1876926,Concrete example of the Lie derivative of a one-form,"Let $\alpha$ be a one-form and $X$ a vector field. For example take: \begin{align*}
\alpha &= y^2 dx + x^2 dy\\
&\\
X &= \frac{\partial}{\partial x}+xy \frac{\partial}{\partial y}.
\end{align*} I'm trying to understand how the Lie derivative works on concrete examples. In particular I want to apply the formula: $$\mathcal{L}_X\alpha = \left(X^j\frac{\partial \alpha_i}{\partial \phi^j} + \alpha_j \frac{\partial X^j}{\partial \phi^i}\right)d\phi^i.$$ If I understand the definitions correctly, it should be: \begin{align*}
\mathcal{L}_X\alpha =&\ \left(1\frac{\partial y^2}{\partial x} + y^2 \frac{\partial 1}{\partial x}\right)dx
+\left(xy\frac{\partial y^2}{\partial y} + x^2 \frac{\partial xy}{\partial x} \right)dx\\ 
&\ +\left(1\frac{\partial x^2}{\partial x} + y^2 \frac{\partial 1}{\partial y}\right)dy + \left(xy\frac{\partial x^2}{\partial y} + x^2 \frac{\partial xy}{\partial y}\right)dy\\
=&\ (0+0)dx+(2xy^2+x^2y)dx+(2x+0)dy+(0+x^3)dy\\
=&\ (2xy^2+x^2y)dx+(2x+x^3)dy.
\end{align*} Is this correct at all ? If not, I'd appreaciate any tip or hint as to what I'm doing wrong.","['differential-forms', 'lie-derivative', 'differential-geometry', 'analysis']"
1876960,Number of ways to distribute 10 balls into 4 urns such that one urn has at-least 3 balls?,How many ways are of distributing 10 balls into 4 urns such that one urn has at-least 3 balls? My basic approach is to choose 1 urn in which I am gonna place 3 balls before hand. I can choose i urn from 4 in 4 ways. Now I need to distribute 7 balls into 4 urns. I solve this using bars and stars. So number of ways are 10C3. So answer is 4+10C3. What's wrong with my approach?,"['combinations', 'balls-in-bins', 'permutations', 'combinatorics', 'discrete-mathematics']"
1877014,A series $10^{12} + 10^7 - 45\sum_{k=1}^{999}\csc^4\frac{k\pi}{1000}.$,"There's a math clock with formulas for each of $1,\ldots,12$, most of which are easy. Number 11, however, intrigues me: $$10^{12} + 10^7 - 45\sum_{k=1}^{999}\csc^4\frac{k\pi}{1000}.$$ Wolfram Alpha agrees the answer is (around) 11. How does one prove this? How does one come up with this?","['summation', 'trigonometry']"
1877015,"If the primes were different, how many numbers would there be?","Given a set $S=\{s_1,s_2,\ldots\}$ of pairwise coprime positive integers greater than 1, define $T$ as the set of products of zero or more elements of $S$ so $T$ contains $1, s_1, s_2, s_1^2, s_1s_2,$ etc. If the growth of $S$ is $f(x)$, i.e.,
$$
\lim_{n\to\infty}\frac{\#(S\cap\{1,2,\ldots,n\})}{f(n)} = 1
$$
where $f$ is sufficiently nice (monotone, smooth, etc.), what can be said about the growth $g(x)$ of $T$? Example 1: If $S$ is the set of prime numbers, then $f(x)\sim x\log x$ and $g(x)\sim x$ since $T$ is just the set of positive integers. Example 2: If $S$ is the set of primes congruent to 1 mod 4 together with 2 and the squares of the primes congruent to 3 mod 4, then $T$ is the set of numbers that are the sum of two squares and $f(x)\sim 2x\log x$ and $g(x)\sim kx\sqrt{\log x}$ with a constant $k$ (the inverse of the Landau-Ramanujan constant, to be precise). Example 3: If $S$ is finite with $n$ elements, then $f(x)=n$ for large enough $x$ and $\log g(x) \sim \log^nx$. Example 4: Bending the rules to allow non-coprime elements, if $S$ is the set of squares and cubes of primes, then $f(x) \sim x^2\log^2x$ and $g(x) \sim kx^2$ with $k=\zeta(3)/\zeta(3/2)$ and $T$ is the set of 2-full or powerful numbers.","['number-theory', 'asymptotics', 'prime-numbers']"
1877029,Let $f = x^5 + x^4 + x^3 -2 x^2 + x + 1$. Does $\text{Gal}(f)$ equal to $A_5$?,"Let $f = x^5 + x^4 + x^3 -2 x^2 + x + 1$. Does $\text{Gal}(f)$ equal to $A_5$? By calculating the discriminant of $f$, I get $\text{Disc}(f)=42849=207^2$. So $\text{Gal}(f)$ is a subgroup of $A_5$. Also, since $5$ divides $|\text{Gal}(f)|$, $|\text{Gal}(f)|$ could be $5,15,20,60$. I also noticed that $\text{Gal}(f)$ should have a 3-cycle. The reason is that $f$ modulo $3$ has exactly two roots: $\bar{1},\bar{2}$. As a result, $\bar{f} = (x^3 + x^2 + 2x + 2)(x+2)(x+1)$. So that rules out $5$ and $20$. How should I proceed from here?","['abstract-algebra', 'galois-theory', 'group-theory']"
1877032,Finding all primes $p$ for which $p\mid x^2-x-1$ for some $x\in \mathbb Z$.,I would like to characterize all primes $p$ for which the polynomial $x^2-x-1$ has roots in $\mathbb Z_p$ If this is not possible I would like to find a family of primes for which it has roots or a family of primes for which it does not have roots. (preferably a family for which there is a rapid test to determine pertinence to the family).,"['number-theory', 'congruences', 'modular-arithmetic', 'elementary-number-theory']"
1877034,Finding the coefficient of $x^r$ in an expansion.,Suppose that the summation of the infinite series $$1+nx+\frac{n(n-1)}{2} x^2+\cdots+\frac{n(n-1)\cdots(n-r+1)}{r}x^r+\cdots$$ is equal to $(1+x)^n$ for $|x|<1$. Show that the coefficient of $x^r$ in the expansion of $\frac{1+x+x^2}{(1-x)^2}$  is $3r $. Hence show that $(217)^\frac{1}{3} \simeq 6.0092$ My attempt : $$\frac{1+x+x^2}{(1-x)^2}=\frac{(1+x+x^2)(1-x)}{(1-x)^3}$$ $$\frac{1+x+x^2}{(1-x)^2}=\frac{1-x^3}{(1-x)^3}$$ $$=\frac{1}{(1-x)^3}-\frac{x^3}{(1-x)^3}$$ $$=\frac{1}{\left(1+(-x)\right)^3}+\frac{1}{\left(1+\left(-\frac{1}{x}\right)\right)^3}$$ $$=(1+(-x))^{-3}+\left(1+\left(-\frac{1}{x}\right)\right)^{-3}$$ How can I proceed after this ? Is there another method ? Is my method correct ?,"['derivatives', 'taylor-expansion', 'binomial-coefficients']"
1877047,Prove that if $g$ is integrable $f$ is integrable,"Let $\|x\|= \biggl( \sum_{k=1}^n |x_k|^p \biggr)^{\!1/p\;}$ for a given $p>1$ and $f:\mathbb{R}^n\to [0,\infty)$ of the form $f(x)=g(\|x\|)$ for a given $g:[0,\infty)\to[0,\infty)$ Prove that if $g$ is integrable then $f$ is integrable and $\int_{\mathbb{R}^n} f =nV_1\int_0^\infty g(r)r^{n-1} \, dr$ where $V_a$ is the
volume of $B_a=\{x;\|x\|\leq a\}$ . My work: So I started by taking $g$ to be $\mathbb{1}_{[0,a]}$ for some $a>0$ . In this case $f=\mathbb{1}_{B_a}$ and if $g$ is integrable then $f$ is too, then I took $g$ to be a step function which is a linear combination of indicator functions so again if $g$ is integrable so is $f$ . Now because every integrable function is sandwiched between two step functions with integrals that differ by $\epsilon$ we conclude that if $g$ is integrable $f$ is as well. (Not too sure about this part) Now to prove the integral equality above, I checked it on $n=2$ and $p=2$ and it was true but I am pretty lost on how to do an induction here..Any ideas?","['multivariable-calculus', 'improper-integrals', 'integration', 'analysis']"
1877093,Existence of additive transformation of random variables,"Suppose we have a random variable $W$ and we want to transform it to a random variable $V$ by using additive transformation, as follows
\begin{align}
V=U+W
\end{align}
where $U$ is independent of $W$. My question is: We are interested in knowing whether such random variable $U$  exists or not and what methods can we used to check this. Known method: For me, the most obvious way is via characteristic functions. \begin{align}
\phi_V(t)=\phi_U(t) \cdot \phi_W(t)  \to  \phi_U(t)=\frac{\phi_V(t)}{\phi_W(t)}
\end{align} 
and we have to check if $\phi_U(t)$ is a proper characteristic function. Motivation: In order to show that there is no such transformation all we need is to check that  $\phi_U$ violates one of the properties of characteristic function.  For example, by showing that $|\phi_U(t)|>1$ for some $t$. To show that such a transformation indeed exists one has to check  that $\phi_U$ is a proper charcterstic function, which can be done via the following set of theorems ( see here ). Unfortunately, these theorems can be difficult to check and I was wondering if there is another method (not through characteristic functions) that can be used to prove or disprove existence of such transformations. For me, the more important direction is how to disprove the existence, which I guess amounts to showing some necessary condition and then checking if it holds. Proposed Example I know that the question I am asking can be fairly difficult to answer. So it might be a good idea to focus on a specific example: \begin{align}
2V=U+W
\end{align}
where
\begin{align}
V &\sim c_ve^{-v^{1.5}}, v\ge0\\
W & \sim c_We^{-w^{1.5-\epsilon_W}}, w\ge0
\end{align}
for some  $\epsilon_W \in (-\infty, 1.5)$ and
\begin{align}
 c_W= \frac{1.5-\epsilon_W}{\Gamma \left(\frac{1}{1.5-\epsilon_W}\right)}
\end{align} Why did I pick this example: On the one hand, computing anaylytic expression for the ChF's of this example might be impossible and motivates searching for new methods.  On the other hand, becuse pdf has an exponential which usually has some nice properties, make me think that there might be hope of answering this question. To solve the above question we distinguess three case: $\epsilon_W <0$,  $\epsilon_W =0$, $1.5\ge \epsilon_W >0$. Solution for $1.5\ge \epsilon_W >0$. Via a method outlined in one of the anwer we have that  for any $n$
\begin{align}
2 \ge  \left(\frac{E[V^n]}{E[W^n]} \right)^{1/n}.
\end{align}
For $1.5\ge \epsilon_W >0$ we can show that the RHS of the above equation can be made as larege as possible by choosing appropriate $n$ and we reach a contradiction. So, for the case of  $1.5\ge \epsilon_W >0$ transformation $2V=U+W$ is impossible. At this point, however, I do not know how to apporach the case of $\epsilon_W <0$ and the case of  $\epsilon_W =0$. To avoid a really long question. The case of $\epsilon_W =0$ was asked here .","['characteristic-functions', 'probability-theory', 'random-variables']"
1877137,Order in Set theory and Logic,"In ZFC ordered pairs are often defined in terms of Kuratowski pairs $(x, y)=\{\{x\}, \{x, y\}\}$ or some other such construction to avoid introducing additional primitive notions. The definition is then usually followed by apologies because it implies artificial properties like $\{x\}\in (x, y)$ etc. which are then immediately ignored. This then leads to even weirder and more arbitrary relations down the line. Meanwhile the notion of an ordered strings is taken granted in the underlying logic e.g. $x\rightarrow y$. 
 Since strings are automatically equipped with order why not drop the requirement that order pairs are sets altogether and instead try to arrange something like  $(x, y)=xy$ for sets $x$ and $y$? (If we can use strings of length 1 to represent sets why not strings of length 2 to represent ordered pairs?) $(x, y)$ would then presumably be an urelement but not a set. This would have the advantage that all objects are constructed from sets even though not all are sets.",['elementary-set-theory']
1877138,Find all rational solutions to $x^2+y^2=2$,Find all rational solutions to $x^2+y^2=2$ I rewrote the equation to $x= \sqrt{2-y^2}$ and thought that $x$ is rational if and only if $2-y^2$ is a square. So the only solution to the first problem is $x=1$ or $-1$ and $y=1$ or $-1$. Is there another approach? Thank you. Edit: Another question: Is there maybe a way to show this using elliptic curves?,"['abstract-algebra', 'elementary-number-theory']"
1877143,Check if $\vec F$ is a conservative field,I'm trying to check if $R/r$ is conservative feild where $R=-x\hat i+y\hat j$ and $r=\sqrt{x^2+y^2}$ Attempt: $$R/r=\underbrace{\frac{-x}{\sqrt{x^2+y^2}}}_{M}i+\underbrace{\frac{y}{\sqrt{x^2+y^2}}}_{N}j$$ $$\frac{\partial M}{\partial y}=\frac{xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ $$\frac{\partial N}{\partial x}=\frac{\color{red}-xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ Since $$\frac{\partial M}{\partial y}\ne \frac{\partial N}{\partial x}$$ $R/r$ is not conservative field I think that I'm wrong,"['multivariable-calculus', 'vector-fields']"
1877148,Is $x^{-0}$ defined?,"In the form of mathematics that most of humanity is taught, the following operation is undefined: $\Large{\frac{x}{0}}$ But, how about the following operation? $\Large{x^{-0}}$ Is the following statement true? $\Large{x^{-0}=\frac{1}{x^0}=\frac{1}{1}=1}$","['notation', 'analysis', 'proof-verification']"
1877149,Some subtleties on zerodivisors in polynomial rings,"This question is based on two answers I saw. I want to know if my reasoning is correct in each case. For me, $A$ is always a commutative ring with $1 \ne 0$. In the first answer this proposition was given: Let $f_1,\dots,f_t\in A[X]$ be nonzero. If there is $0 \ne g\in A[X]$ such that $gf_1=\cdots=gf_t=0$, then there is $0 \ne a\in A$ such that $af_1=\cdots=af_t=0$. Assuming the case $t = 1$ which is McCoy's theorem, I think I can prove this as follows: Proof: Let $\displaystyle F := \sum_{i=1}^t f_i X^{\sum_{j < i} (1 + \deg f_j)}$. Then $gf_1 = \ldots = gf_t = 0 \implies gF = 0 \implies aF = 0$ for some $0 \ne a \in A \implies af_1 = \ldots = af_t = 0$. The last implication is because every coefficient of each $f_i$ appears exactly once as a coefficient of $F$.$\newcommand{\N}{\mathbb{N}}$ Question 1: Is the proof above correct? Moving on to the next question, in the other answer this proposition was given: (*) Let $A$ be a commutative $\N$-graded ring. Let $f \in A$ be a zerodivisor. Then there is some $0 \ne a \in A$ homogeneous such that $af = 0$. I think the proof given in that answer has a fatal flaw, which was pointed out as a comment but with no response. Specifically $\deg f_ig < \deg g$ may not be true. For polynomial rings in $1$ variable it's ok because $f_i = a_ix^i$ where $a_i$ has degree $0$ and $x^i$ is a nonzerodivisor, so $\deg a_ig < \deg g$ and $a_ig \ne 0$. So my second question is: Question 2: Is (*) true? If not, what is a counterexample? One note is that (*) is true if $A$ is Noetherian (since then every zerodivisor is in some associated prime, which is the annihilator of a homogeneous element). So a counterexample would have to be non-Noetherian.","['polynomials', 'abstract-algebra', 'proof-verification', 'graded-rings', 'commutative-algebra']"
1877180,Evaluating the sum $\sum_{n=2}^{\infty}\ln[1-1/n^2]$,"For convenience the sum is again
$$
\sum_{n=2}^{\infty}\ln[1-1/n^2]=\sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}
$$
I first tried solving using a definite integral, since this seems to make telescoping easier to see,
$$
\sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}=\sum_{n=2}^{\infty}\ln(n^2-1)-\ln(n^2)=
\sum_{n=2}^{\infty}\int_{n^2}^{n^2-1}\frac{dx}{x}
$$
But writing out the first few terms doesn't make any cancellation obvious because of the $n^2$ terms. I also tried futzing around with log rules and got things down to 
$$
\sum_{n=2}^{\infty}\ln\frac{(n^2-1)}{(n^2)}=
\sum_{n=2}^{\infty}\ln((n-1)(n+1))-\ln(n^2)=
\sum_{n=2}^{\infty}\ln(n-1)+\ln(n+1)-\ln(n^2)
$$
The first few terms of which are
$$
\sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=[\ln1+\ln3-2\ln 2]+[\ln2+\ln4-2\ln 3]+[\ln3+\ln5-2\ln 4]+...\\
=\ln 2+\ln 5-2\ln 4
$$
Which leads to the guess that 
$$
\sum_{n=2}^{4}\ln\frac{(n^2-1)}{(n^2)}=\ln (N-1)+\ln(N+2)-2\ln(N)\\
=\ln(\frac{(N-1)(N+2)}{N^2})\rightarrow 0
$$
Which means I'm wrong. Should I soldiering on looking for a pattern through more computation, or is there a more expedient/elegant way to evaluate the sum?","['sequences-and-series', 'calculus']"
1877193,"Why order topology is defined with open intervals, not closed ones?","In Wikipedia order topology is defined by the subbase consisting of $(a,\infty)$ and $(-\infty,b)$. Why is not it defined by intervals $[a,\infty)$ and $(-\infty,b]$ instead? Is this Wikipedia definition (with open intervals) accepted by all or absolute most of mathematicians? Consider an one-point ordered set (call this point $0$). With the Wikipedia definition we have an empty subbase. Isn't it better to have the subbase consisting of the set $\{0\}$?","['general-topology', 'order-theory']"
1877202,Why is the Operator Norm so hard to calculate?,"I recently took a better look at the operator norm defined on a matrix $\mathbf A \in \Bbb{K}^{n\times n}$ as follows: $$
\|\mathbf A\|_p=\sup\{\|\mathbf Ax\|_p \mid x\in\Bbb{K}^n\land\|x\|=1\}
$$ The first time I looked at this I thought ""ok, lets calculate it for a few example matrices"". I started with $n = 3$ and $p = 2$, just to start ""simple"". Let
$$
\mathbf A = \left[\begin{matrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{matrix}\right]\quad a_{ij}\in\Bbb{K}^n
$$
Now if we're going to minimize $\|\mathbf Ax\|_2$ ($ = \|\mathbf Ax\|$), we might as well make it easy on ourselves and only minimize $\|\mathbf Ax\|^2$ so as to not worry about that annoying radical. We get
$$
\begin{align}
\|\mathbf Ax\|^2 & = (a_{11}x_1 + a_{12}x_2 + a_{13}x_3)^2 + (a_{21}x_1 + a_{22}x_2 + a_{23}x_3)^2 + (a_{31}x_1 + a_{32}x_2 + a_{33}x_3)^2 \\
& = Ax_1^2 + Bx_2^2 + Cx_3^2 + Dx_1x_2 + Ex_1x_3 + Fx_2x_3
\end{align}
$$
where
$$
\begin{align}
A & = a_{11}^2+a_{21}^2+a_{31}^2 \\
B & = a_{12}^2+a_{22}^2+a_{32}^2 \\
C & = a_{13}^2+a_{23}^2+a_{33}^2 \\
D & = 2(a_{11}a_{12} + a_{21}a_{22} + a_{31}a_{33}) \\
E & = 2(a_{11}a_{13} + a_{21}a_{23} + a_{31}a_{33}) \\
F & = 2(a_{12}a_{13} + a_{22}a_{23} + a_{32}a_{33})
\end{align}
$$
Now lets define 
$$
G(x_1,\ x_2,\ x_3) = Ax_1^2+Bx_2^2+Cx_3^2+Dx_1x_2+Ex_1x_3+Fx_2x_3
$$
So if we want to minimize $||\mathbf Ax||^2$, we're either going to have to minimize
$$
N(x_1,\ x_2,\ x_3) = \frac{G(x_1,\ x_2,\ x_3)}{x_1^2+x_2^2+x_3^2}
$$
or simply minimize $G$ with the constraint $g(x_1,\ x_2,\ x_3) = x_1^2 + x_2^2 + x_3^2 = 1$. The latter seemed easier to me, so I gave it a shot using Lagrange multipliers. As usual, I defined 
$$
\mathcal{L}(x_1,\ x_2,\ x_3,\ \lambda) = G(x_1,\ x_2,\ x_3)-\lambda g(x_1,\ x_2,\ x_3)
$$
setting it's gradient to zero gives
$$
\nabla \mathcal L = 0 \implies \begin{cases}
2(A - \lambda)x_1 + Dx_2 + Ex_3 & = 0 \\
Dx_1 + 2(B - \lambda)x_2 + Fx_3 & = 0 \\
Ex_1 + Fx_2 + 2(C - \lambda)x_3 & = 0 \\
x_1^2 + x_2^2 + x_3^2 - 1 & = 0
\end{cases}
$$
Now this is where I really started to get stuck. I tried solving the first three equations for $x_1,\ x_2,$ and $x_3$ but didn't end up with anything I could use. I tried solving for $x_1$ in terms of $x_2,\ x_3,$ and $\lambda$, then $x_2$ in terms of $x_3$ and $\lambda$, and then subbing that all into the third equation, but ended up with either $x_3 = 0$ or 
$$
4\lambda^3 - 4\lambda^2(A+B+C) + \lambda(4AB+4AC+4BC-D2+E^2+F^2)-4ABC-AF^2-BE^2+CD^2+DEF = 0
$$
which, although technically solvable for $\lambda$ via the cubic equation, would be incredibly messy. Now, I probably created my own roadblock for this problem, because I didn't want to think about the system of equations logically and just wanted to bash it out. Regardless of my approach, it seems like the operator norm is a very difficult thing to calculate, and I only analyzed the case where $n = 3$ and $p = 2$. What about the general case? What if $n = 75$ and $p = 9/4$? How on earth would you calculate it then? The questions above are rhetorical, however, and my actual question is as follows: Why define such an ordinary norm for matrices which is so difficult to calculate in general? I see the operator norm everywhere, and it seems like the standard norm for a lot of theorems (unless I'm mistaken and ||A|| just means any matrix norm). So why would we define such a standard norm in a way that is so difficult to calculate? What's the point? Is it easy to work with in theorems? I get that it intuitively makes sense as a norm, but it can't possibly be that easy to work with, especially in comparison to things like the Frobenius norm. So why do we care about this definition?","['matrices', 'normed-spaces', 'matrix-norms', 'definition']"
1877252,If $\lim_{x\to 0}f(x)=L$ then $\lim_{x\to 0}f(cx)=L$ for any nonzero constant $c$.,"I was just wondering if this proof is correct. I'm trying to prove that if $\lim_{x\to 0}f(x)=L$ then  $\lim_{x\to 0}f(cx)=L$ for any nonzero constant $c$. Proof: If  $\lim_{x \to 0}f(cx)=L$ then there exists some $\delta$ such that $0<|x|<\delta \implies |f(cx)-L|<\epsilon$. We have$ |f(cx)-L|=|f(cx)-L+f(x)-f(x)|$. Applying the triangle inequality gives $|f(cx)-L|=|f(cx)-L+f(x)-f(x)|\leq|f(cx)-f(x)|+|f(x)-L|$. So it suffices to find some $\delta$ such that $0<|x|<\delta \implies |f(cx)-f(x)|+|f(x)-L|<\epsilon$ Since $\lim_{x \to 0}f(x)=L$ there exists some $\delta_{1}$ such that $0<|x|<\delta_{1} \implies|f(x)-L|<\epsilon$. Since this must be true for any $\epsilon>0$, it must be true for some $\epsilon>|f(cx)-f(x)|+|f(x)-L|$. So, there exists some $\delta_{1}$ such that $0<|x|<\delta_{1} \implies |f(x)-L|\leq|f(cx)-f(x)|+|f(x)-L|<\epsilon$. Letting $\delta=\delta_{1}$ gives the desired $0<|x|<\delta \implies |f(cx)-f(x)|+|f(x)-L|<\epsilon$. Please tell me if I did anything invalid. Also I'm new to Calculus so please explain as simply as possible. Thanks","['epsilon-delta', 'solution-verification', 'calculus', 'limits']"
1877276,Under which $T$ axiom can you separate disjoint closed and compact set? Compact and compact set?,"I am trying to figure out what is the lowest separation axiom needed that allows you to separate disjoint closed and compact set via disjoint open sets , and separate disjoint compact and compact sets via disjoint open sets . Claim : Disjoint closed sets and compact sets in $T_3$ space and above can be
  separated via disjoint open sets Pf. Take disjoint closed and compact sets $C,K$ respectively. Then since the space is $T_3$ (more importantly, regular), we can take disjoint open sets $V_k, U_k$ such that $k \in V_k$, where $k \in K$, and $C \subset U_k$ Then finite union of $V_k$ covers $K$, and finite intersection of $U_k$ covers $C$, those are disjoint open sets by construction. What about compact sets? Is Hausdorfness enough? Claim : Disjoint compact sets in $T_2$ space and above can be
  separated via disjoint open sets Let $K_1,K_2$ be disjoint compact sets in Hausdorff space. Then $\forall x \in K_1, y \in K_2$, there exists disjoint open sets $U_x, V_x$ such that $x \in U_x, y \in V_x$. Then $\{U_x|x \in X\}, \{V_x| x \in X\}$ are open covers of $K_1, K_2$ with finite subcovers $\{U_{x_i}|x_i \in X, i \in {1, \ldots, n}\}, \{V_{x_i}|x_i \in X, i \in {1, \ldots, n}\}$. We need to show that they are disjoint, let $z \in \cup U_{x_i} \cap \cup V_{x_i}$, then $z \in U_{x_i}, \forall x_i \in X$, and $V_{x_i}, \forall x_i \in X$. This is impossible since $U_{x_i}$ and $V_{x_i}$ are disjoint $\forall x_i \in X$ Are the above characterizations correct? Are there more generalizations?","['general-topology', 'compactness', 'separation-axioms']"
1877288,Equivalence of definitions of Sobolev space,"I'm reading through Steinbach's book on Eliptic Boundary Value Problems and struggling with understanding of the definition of Sobolev space He defines it as $$H^s(\mathbb R^n) := \{u \in \mathcal S^*(\mathbb R^n) \colon \mathcal J^s u \in L_{2}(\mathbb R^n) \}$$ Where $S(\mathbb R^n)$ is Schwartz space, $\mathcal J^s$ is a Bessel Operator  and $$S^*(\mathbb R^n):= \{T:S(\mathbb R^n) \mapsto \mathbb C : \text{ T linear functional}\}$$ But it can be also defined as:
  $$H^s(\mathbb R^n) := W_{2}^{s}(\mathbb R^n) := \{u \in L_{2}(\mathbb R^n) : D^{a}u \in L_{2}(\mathbb R^n),|a| \leq s \} $$ I am struggling to understand how these definitions are the same, since from the first definition each element is a complex functional from Schwartz Space to C, whereas from the second definition each element is an integrable measurable function with integrable measurable weak derivatives. Thank you.","['functional-analysis', 'sobolev-spaces']"
1877289,"A function ""almost not"" in $L^1$","Let $f$ be a measurable function on a finite measure space. If $f \notin L^p$ for all $p > 1$, is it true that $f \notin L^1$? Unfortunately the inequality I am using to prove that $f \notin L^p$ does not work for $p = 1$, but I'm hoping for some kind of ""continuity of the norm"" argument whereby $\lVert f \rVert_1 = \lim_{p \to 1} \lVert f \rVert_p$ would imply that $\lVert f \rVert_1 = \infty$, but obviously the limit argument is not defined.","['functional-analysis', 'lp-spaces', 'examples-counterexamples', 'partial-differential-equations']"
1877296,Derivation of $y=a(x-h)^2+k$ from $y=ax^2+bx+c$ given a vertex and a point,"Derive $y=a(x-h)^2+k$ from $y=ax^2+bx+c$ given a vertex and a point. Recently I have been solving a problem to which I could not find a solution. Google search of ""quadratic equation given vertex and a point,"" yielded what I have been looking for. However, although I have already solved the problem, I am still wondering how to derive the equation I was looking for before I googled it. I was given a vertex $V(-3, -2)$ and a point $P(-4, 0)$ of a parabola. Using $y=ax^2+bx+c$, I have derived an equation for finding the vertex for each parabola with $V(\frac{-b}{2a}, -a(\frac{b}{2a})^2+c)$. I knew that given the same vertex, the parabola $y=x^2+6x+7$ was close to what I've been looking for. However, this parabola didn't go through $P(-4, 0)$, because it was too wide. At this point it seemed as if I had enough information to derive an equation, that given a vertex and a point I would obtain a parabola according to the restrictions. However, from this point on I didn't know how to proceed further.",['algebra-precalculus']
1877311,Finding the range of the function $f(x)=\frac{\sqrt{x^{2}-4}}{\sqrt{x-2}}$,"My question regards finding the range of the following function:
$$f(x)=\frac{\sqrt{x^{2}-4}}{\sqrt{x-2}}.$$ I have found the domain of the function to be $[-2, 2)$ $\cup$ $(2, \infty)$. According to my teacher's notes, this is correct. However, I have almost no clue how to go about finding the range without simply plugging in values of $x$ from the domain and seeing a general pattern of values of $f(x)$ from which to educatedly estimate the range. Could someone please help me find the range of this function?","['algebra-precalculus', 'functions']"
1877323,"SchrÃ¶der-Bernstein, check proof","Theorem: if $r:A\to B, s:B\to A$ are injections, there is a bijection $t:A\to B$. I will prove this lemma: ""if exist $f:A\to B\subset A$ injective, then exist $h:A\to B$ bijective. With this lemma, consider the injective function $f=s\circ r:A\to s(B)\subset A$, and exist $h:A\to s(B)$ bijective. Now, $s_{|B}:B\to s(B)$ is bijective. Finally, $t= s_{|B}^{-1}\circ h:A\to B$ is a bijection. I want check the proof of lemma:
Put $Y=A-B$, $X=Y\cup (\bigcup_{i\in \mathbb{N}}f^i(Y))$, where $ f^i(Y)=f(f(...f(Y)...)), i$ times. Because $Y\cap B=\emptyset$, $ f^k(Y)\subset B$ and $Y$ are disjoint. Because $f$ is injective, $Y\cap f^k(Y)=\emptyset \to f^m(Y)\cap f^{m+k}(Y)=\emptyset $ for all $k,m$. Note in the definition of $X$ that the union is disjoint, and $X=Y\cup f(X)$. Finally, Note that $A-X=(Y\cup B)-(Y\cup f(X))=B-f(X)$. If we define $h:A\to B$ as $h=f$ in $X$ and $h=id$ in $A-X$, is bijective.","['cardinals', 'elementary-set-theory', 'proof-verification']"
1877350,"Stirling's Formula, Remainder Term","With reference to the Series and Product Development chapter of Ahlfors' complex analysis text (Page 201-205), after deriving Stirling's formula $$\Gamma(z)=\sqrt{2\pi}z^{z-\frac{1}{2}}e^{-z}e^{J(z)},$$ where $$J(z)=\frac{1}{\pi}\int_{0}^{\infty}\frac{z}{\eta^2+z^2}\log\frac{1}{1-e^{-2\pi\eta}}d\eta,$$ it is noted that the remainder $J(z)$ can be expressed as $$J(z)=\frac{C_1}{z}+\frac{C_2}{z^3}+...+\frac{C_k}{z^{2k-1}}+J_k(z)$$ with coefficients $$C_{\nu}=(-1)^{\nu-1}\frac{1}{\pi}\int_0^{\infty}\eta^{2\nu-1}\log\frac{1}{1-e^{-2\pi\eta}}d\eta.$$ Subsequently, it states that the coefficients $C_\nu$ are connected with the Bernoulli numbers (Asserting that this can be proved by means of residues) by $$C_\nu=(-1)^{\nu-1}\frac{1}{(2\nu-1)2\nu}B_\nu.$$ How is the last statement proved using the calculus of residues? Thanks in advance.","['complex-analysis', 'residue-calculus', 'gamma-function']"
1877362,Generalization of the q-Binomial Theorem,"The following is the well-known q-Binomial Theorem: For all $n \geq 1$, we have:
  $$ \prod_{j=1}^{n}(1+xq^j) = \sum_{k = 0}^{n} q^{k(k+1)/2} {n \choose k}_q x^k $$ I am not too familiar with the proof of the theorem and I was wondering if the following generalization also holds or if the equality does not hold term by term but only by the sums: Generalization: For all $n \geq 1$ and $1 \leq m \leq n$, we have:
 $$ \prod_{j = m}^{n}(1+xq^j) = \sum_{k = m}^{n} q^{k(k+1)/2} {n \choose k}_q x^k $$ (EDIT) Thanks to the commenter, I now realize that the generalization is false. Is there a similar formula which expresses $\prod_{j = m}^{n}(1+xq^j)$ as a power series of $x$? I would appreciate any suggestions to relevant literature.","['number-theory', 'binomial-theorem', 'q-analogs', 'binomial-coefficients']"
1877364,Why isn't Axiom of Choice a trivial result? Is it needed to prove existence of this recursive function?,"I mean, if we have the collection of sets $\mathcal{A}$ and we choose the set $B$ to contain only one element from each set in $\mathcal{A}$, then is quite trivial that $B\subseteq \bigcup_{A\in\mathcal{A}}A$. And this holds even if the sets in $\mathcal{A}$ are not pairwaise disjoint. Then it wouldn't be needed to assume it as an axiom. What is the error of this reasoning? In the other side, in Munkres' Topology 2nd Ed, he introduces this axiom when needs to proove the existence of an injection $f: \mathbb{Z}^{+} \rightarrow A$, for the infinite set $A$. So, let $a_1$ be an arbitrary element in $A$; when defining $f$ through induction as following
$$
\begin{array}{l}
f(1) = a_1 \\
f(n) = \textrm{arbitrary element of } A-f(\{1,2,...,n-1\}) \quad \textrm{; for }n>1
\end{array}
$$
we can see there are infinite possible choices of the images $a_i$ that meets the above formula, then there exists at least one function $f$, which is injective because of its definition. Munkres uses the axiom of choice to satisfy the hyphotheses for the principle of recursive definition, so he can ensure such function (one which he defines with help of the function of choice) is unique. Although I cannot ensure this, I would say what I did is enough to prove the impication. Is this a valid argument, or is there any lack of rigor in it? Thanks in advance!","['alternative-proof', 'elementary-set-theory', 'axiom-of-choice']"
1877369,Proving the convergence of the series $\sum_{j=1}^\infty \frac{(2^j)^2}{j!}$ without root or ratio test,"I am given the series: 
$\displaystyle \sum_{j=1}^\infty \frac{(2^j)^2}{j!}$ and I can show that it converges by using the ratio test, but I'm not sure how to approach to prove its convergence without it.","['real-analysis', 'sequences-and-series', 'calculus']"
1877414,Is the set of symmetric positive-definite matrices open in the set of symmetric matrices?,"I'm not sure if the set of symmetric positive-definite matrices is open in the set of symmetric matrices.  I'm almost certain that it is not open in the larger set of n by n matrices.  So far most of the proofs I've seen don't use symmetry, so I'm not sure if they are correct.  Any hints?  I've been trying to use that the set of positive-definite matrices is path connected, but I can't come up with a solution.  Thanks in advance.","['matrices', 'general-topology', 'linear-algebra']"
1877444,Proving that a group homomorphism preserves the identity element,"Assume that $(G,*)$ and $(H,o)$ are groups and that $f:(G,*) \rightarrow (H,o)$ is a homomorphism. Let $e_G$ and $e_H$ denote the identity elements of $G$ and $H$, respectively. Show that $f(e_G)=e_H$. Approach: $f(e_G)=f(a*a^{-1})$ for $a,a^{-1} \in G$,
so $f(a*a^{-1})=f(a)of(a^{-1})$. If thatâ€™s true, then how do we know that  $f(a^{-1})$ is the inverse of $f(a)$?","['abstract-algebra', 'group-theory']"
1877465,$Q$ is an injective module iff injections from $Q$ always split,"Clarification of terminology: We say an injection $A \xrightarrow{i} B$ splits iff the induced short exact sequence $0 \to A \xrightarrow{i} B \to B/i(A) \to 0$ splits. Similarly, we say a surjection $B \xrightarrow{p} C$ splits iff $0 \to \ker p \to B \xrightarrow{p} C \to 0$ splits. Here are the definitions of injective and projective modules I'm using: $Q$ is injective if, and only if, $\operatorname{Hom}(\ast, Q)$ takes injections to surjections ($0 \to A \to B$ exact implies $\operatorname{Hom}(B,Q) \to \operatorname{Hom}(A,Q) \to 0$ exact). $P$ is projective if and only if $\operatorname{Hom}(P, \ast)$ takes surjections to surjections. Want to show: $Q$ is injective if and only if injections from $Q$ always split. The $\Longrightarrow$ direction is straightforward. The $\Longleftarrow$ direction, not so much. (Dummit and Foote relegates this to the exercises and uses the nontrivial fact that every module is contained in an injective module. I imagine this is to parallel their proof that $P$ is projective iff surjections to $P$ always split, which uses the fact that every module is a quotient of a free, hence projective, module.) It bothered me that these dual concepts don't have ""dual"" proofs, so I came up with one using pushouts and pullbacks (D&F introduce these in exercise 27, if you have the book). The argument is rather straightforward, so I feel like something has to be wrong, but I can't figure out what it is. Here it is: Suppose injections from $Q$ always split, suppose $i: A \to B$ injects, and suppose $f \in \operatorname{Hom}(A,Q)$. Consider the pushout of $i$ and $f$: $$M = B \oplus Q/\{(i(a), -f(a)): a \in A\}.$$ Then we get the following maps ""for free"": $j: B \to M$ and $g: Q \to M$, with $ji = gf$. (At this point, we haven't used any of our hypotheses.) Now, it's straightforward to check that $g$ injects because $i$ does, hence $g$ splits and we get a map $h: M \to Q$ with $hg = 1$. Thus, $hj$ lifts $f$. Verification that $g$ injects because $i$ does: If $g(q) = 0$, then $(0,q) = (i(a), -f(a))$ for some $a$. But $i(a) = 0$ implies that $ a = 0$, and this in turn implies that $-f(a) = 0$, so that $q = 0$. (A completely analogous proof using the pullback shows that: projections to $P$ always split $\implies P$ is projective.) Where is the flaw in this proof?","['injective-module', 'abstract-algebra', 'projective-module', 'homological-algebra', 'proof-verification']"
1877592,How can the probability that a number contains the digit 3 be 1?,"Based on this Numberphile video which claims almost all integers contain a $3$, I have a few questions on the reasoning behind recurring decimal numbers like $0.9999\ldots =1$ What they have shown is that $$\lim_{n \to + \infty} \frac{10^n-9^n}{10^n} = 1$$ this basically means that probability of eg. a $3$ occurring in a set of numbers like for $1-10, 1-100,$ increases as the upper bound gets large. So you are more likely to see a $3$ when you take $1-100000$, than $1-10$ as the probability gets higher. So what I would like to know is as '$n$' approaches $âˆž$ does probability of seeing a '$3$' equals $0.99999....$? But since $0.9999... = 1$ wouldn't this not make sense, since there are infinitely many numbers that do not have a '$3$'? All I need is for an explanation as to why this logic is wrong. Simpler answers are most appreciated. Note I am not looking for the reason as to why 0.9999...=1.","['probability', 'limits']"
1877614,Is $\mathbb{R}$ and $\mathbb{R}\backslash \mathbb{Q}$ homeomorphic?,"Quick question: Is $\mathbb{R}$ and $\mathbb{R}\backslash \mathbb{Q}$ homeomorphic? Assuming equipped with the usual topology. I am guessing no, because a homeomorphism is a cardinality preserving closed map. Singletons are closed in $\mathbb{R}$, but they are neither closed nor open in $\mathbb{R} \backslash \mathbb{Q}$. So $f(\{a\})$ is not closed. Not homeomorphic, bad! Right?",['general-topology']
1877713,"When finding $\sin\theta$ and $\cos\theta$ given $\tan\theta = \frac35$, shouldn't I get positive and negative answers?","Q: Find the exact value of $\cos\theta$ and $\sin\theta $ if $ \tan\theta=\frac{3}{5}$ To solve it I got told to draw the triangle and label the sides and then find cos and sin. The answer from the answer sheet gives both cos and sin as positive solutions. However, I thought that, since tan is positive in both the 1st and 3rd quadrants, then you could draw the triangle in those two quadrants. So, Why don't you get plus-minus solutions for $\sin\theta$ and $\cos\theta$ ?",['trigonometry']
1877728,Can linear connections other than Levi-Civita connections be useful?,"Consider a smooth Riemann manifold such that a Levi-Civita connection is defined.
I am wondering whether there are examples in mathematics or physics where the use of other linear connections is useful despite the fact that a Levi-Civita connection is available.",['differential-geometry']
1877764,"Orthonormal basis of $\{(x_1, \dots, x_n) \mid x_1+x_2+\cdots+x_n=0\}$ [duplicate]","This question already has answers here : Find an orthonormal basis for the subspace of $\mathbb R^4$ (2 answers) Closed 7 years ago . How can we find the orthonormal basis of $\{(x_1,\dots,x_n) \in \Bbb R^n \mid x_1+x_2+\cdots+x_n=0\}$? It is easy to find a basis, but using Gram-Schmidt procedure seems difficult to obtain an orthonormal one.","['orthonormal', 'linear-algebra']"
1877766,What is the Definition of Linear Algebra?,"For introducing a field of science, usually we require a definition which summarizes the goal which is intended to be achieved by this field. Linear Algebra is one of the most important parts of mathematics which has not only interesting pure mathematical ideas but also a lot of applications in physics and engineering. So it seems reasonable to have a Complete , Clear , Brief , and Delicate definition for this important branch of mathematics. I am wondering that what is the best fit for such a definition. For example, the very first sentence that the Linear Algebra Done Right by Sheldon Axler starts with is Linear Algebra is the study of linear maps on finite dimensional vector spaces. What is your definition of Linear Algebra in few sentences?","['linear-algebra', 'soft-question', 'definition']"
