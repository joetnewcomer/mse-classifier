question_id,title,body,tags
3935988,Showing that if $3x$ is even then $3x+5$ is odd,"I'm learning the absolute basics of how to do proofs, and am really struggling. If 3x is even then 3x+5 is odd. This is the solution: I get that even numbers are 2n and odd numbers are 2n+1. For the life of me, I CANNOT get it into that form shown below. I feel so dumb. I tried looking up other answers before posting, but   nothing I found is this basic. Work:
-Assumptions-
3x = 2n
3x+5 = 2k+1 -Trying to make sense of 3x-
3x+5 = 2k+1
3x = 2k-4 -Plugging in 2k-4 for 3x-
2k-4 = 2n
2k = 2n+4
k = n+2 -Plugging in n+2 for k-
3x+5 = 2(n+2)+1 ...This is where I gave up. I don't know where I'm going with this anymore.",['algebra-precalculus']
3935998,Solving non-autonomous system of differential equations,"Given the following 2x2 system of differential equations, $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y \end{cases}$ A solution is easy to fins by computing eigenvalues and eigenvectors of the associated matrix A $A = \begin{bmatrix} 4 & -1 \\ 2 & 1 \end{bmatrix}$ . Therefore, knowing that eigenvalues are 2 and 3, and the related eigenvectors are $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ respectively, the solution is of the form $Ae^{2t}\begin{bmatrix} 1 \\ 2 \end{bmatrix}+Be^{3t}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ . What about if I make the initial system of equations non-autonomous? Resulting in something as follows $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y +f(t) \end{cases}$ where f(t) can be either a polynomial on t, an exponential ( $e^{at}$ ) or a trigonometric function ( $sin(at)$ or $cos(at)$ ). What is now the procedure to follow this system of differential equations? For example, let $f(t)=t^2$ . How do I solve the following system of differential equations? $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y+t^2 \end{cases}$","['systems-of-equations', 'ordinary-differential-equations']"
3936055,Show that the genus of the projective curve $x^n_1 = x_2x^{n-1}_0 - x^n_2$ is $\frac{(n-1)(n-2)}{2}$.,"Suppose that we have a curve $X \subset \mathbb{P}^2$ given by $x^n_1 = x_2x^{n-1}_0 - x^n_2$ . How do we show that the curve has genus $\frac{(n-1)(n-2)}{2}$ (whenever the curve is smooth). I should use the fact that the sequence $$
0 \to \mathcal{F}(X) \stackrel{\iota}{\to} \mathcal{F}(U_1) \oplus \mathcal{F}(U_2)  \stackrel{\delta}{\to} \mathcal{F}( U_1 \cap U_2)
$$ is exact. Where $\mathcal{F} =\mathcal{O}_X(D)$ for some divisor $D$ on $X$ and $\{ U_1, U_2\}$ an affine cover of $X$ (however, $X$ should be irreducible I think).  Moreover $\iota(f) =( f|_{U_1}, f|_{U_2})$ and $ \delta(f_1,f_2) = f_1|_{U_1 \cap U_2} - f_2|_{U_1 \cap U_2}$ . There is a hint: compute the cokernel of $\delta$ using bases for the infinite-dimensional vector spaces $\mathcal{O}_X(X_1)$ , $\mathcal{O}_X(X_2)$ and $\mathcal{O}_X(X_{12})$ that are as simple as possible. (As a sanity check on your computation: the kernel of $\delta$ should be one-dimensional!) Any help on how to proceed or how to solve it would really be appreciated!","['curves', 'algebraic-geometry', 'projective-geometry']"
3936064,find a matrix $A_{n \times n}$ which satisfy $A^{n}=0$ and $A^{n-1}\ne0$,"I asking a question for my brother. He just started his algebra course not long ago and he got a question he is stuck on (he just started the course so all his knoweldge is based on matrices and their properties). I need to find a Matrix $A_{2x2} \not=0$ which satisfies $A^2=0$ . Also, I need to find a Matrix $A_{3x3}$ which satisfies $A^3=0$ and $A^2\not=0$ . In the end I need to generalize the problem to $A_{n \times n}$ (find a matrix $A_{n \times n}$ which satisfy $A^{n}=0$ and $A^{n-1}\ne0$ ). In the first question I wrote $A$ as \begin{bmatrix} a & b \\ c & d \end{bmatrix} after multiplying $A$ with itself I found out that every matrix $2x2$ in the form of \begin{bmatrix} x & y \\ -\frac{x^2}{y} & -x \end{bmatrix} when multiplied with itself equal zero. In the second question I tried the same thing but it got to long to soon so I figured I am doing something wrong. also, the the equations I get when I am trying to find conditions for $A$ are to messy to deal with (9 in total). I did manage to find some matrices that satisfies $A^3=0$ such as \begin{bmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} but I still wish to know how should I find the conditions for it to happen. About the third question, I guess I need to finish the second one to even begin thinking about the solution but I still can't see where should I start.","['matrices', 'linear-algebra']"
3936115,Is my explanation to this derivative problem correct?,"Basically, I am trying to prove that number 13 relates to the graph of B, but I was wondering if my explanation was correct. Here is the problem and here is what I wrote. From the interval of - ∞ to zero, the function is decreasing, so f ' < 0. The interval from zero to + ∞, the function is increasing, so f ' > 0. The graph that corresponds to this is B because the interval from - ∞ to zero, f ' is below zero on the y-axis and the interval from zero to + ∞, f ' is above zero on the y-axis. Also, for this problem, is there a more efficient way of saying where the function is increasing or decreasing, rather than saying ""in this interval, the function is increasing/decreasing,"" since there are 5 intervals and doesn't give the points on the axis? ,","['calculus', 'derivatives']"
3936134,Does an integrable function vanish at infinity in a suitable sense?,"I'm searching for a generalization of the following observation: If $f\in L^2(\mathbb R)$ , then \begin{equation}\begin{split}\int_{[-n,\:n+1]}f(x)\:{\rm d}x&=\left\langle1_{[-n,\:n+1]},1_{\mathbb R\setminus[-n,\:n)}g\right\rangle_{L^2(\mathbb R)}\\&\le\left\|f-1_{[-n,\:n)}f\right\|_{L^2(\mathbb R)}\xrightarrow{n\to\infty}0\end{split}\tag1\end{equation} by the Cauchy-Schwarz inequality and Lebesgue's dominated convergence theorem. $(1)$ captures the idea that an integrable function should ""vanish at infinity"" in a suitable sense. Can we generalize this result to $f\in L^p(\mu)$ , where $(E,\mathcal E,\mu)$ is an arbitrary measure space and $p\ge1$ ? Maybe assuming that $\mu(E)=\infty$ and/or that $E$ is a metric/normed space with $\mathcal E=\mathcal B(E)$ ? In the case of a metric/normed space, the intuition tells me that an integrable function should ""essentially be supported in a compact set"". What's obviously true is that $$\forall f\in L^p(\mu):\not\exists B\in\mathcal E:\mu(B)=\infty\text{ and }\inf_{x\in B}|f(x)|>0\tag2.$$","['measure-theory', 'lp-spaces', 'lebesgue-integral']"
3936187,Hard power series problem,"Consider the differential equation $$(1+t)y''+2y=0$$ with the variabel coefficient $(1+t)$ , with $t\in \mathbb{R}$ . Set $y(t)=\sum_{n=0}^{\infty}a_nt^n$ . What are the first 4 terms in the associated power series? An Attempt We write $$(1+t)y''=\sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1}$$ $$2y=\sum_{n=0}^{\infty}2a_nt^{n} $$ Combining these three gives us $$\sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1}+ \sum_{n=0}^{\infty}2a_nt^{n}$$ We now want all the powers to be $t^n$ . $$\sum_{n=0}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=0}^{\infty}2a_nt^{n} $$ We finally want the index of summation to all be the same. $$2a_0+2a_2+ \sum_{n=1}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=1}^{\infty}2a_nt^{n}$$ Which simplifies to $$2a_0+2a_2+ \sum_{n=1}^{\infty}\big[a_{n+2}(n+2)(n+1)+a_{n+1}n(n+1)+2a_n \big]t^n$$ Clearly, I'm not getting anywhere close to any of the answers given. I just can't find what I'm doing wrong. I have also tried with Maple, but that didn't give me any real result either. I hope someone can help me out.","['power-series', 'calculus', 'summation']"
3936228,Reference for quotient stacks,"I would like to understand stacks, but the general theory seems pretty abstract to me and I would be happy to limit myself to the case of a quotient stack $[X/G]$ . Are there introductory notes that focus on such stacks ?","['algebraic-stacks', 'algebraic-geometry', 'reference-request']"
3936332,If $f$ is a function differentiable at $a$ find: $\underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h}$,If $f$ is a function differentiable at $a$ find: $\underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h}$ I am struggling to understand what to do. I tried brute forcing this question and I get $\frac{f(a)-f(a)}{0}$ which makes no sense to me. Any clarification would be appreciated.,['derivatives']
3936335,"Given that $g$ is only continuous at $0$, not on $[0,1]$, show $\lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0)$.","Important Notice : Observe that although this question is very similar to many other questions such as the one in proving $\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$ when f is continous on [0,1] , this problem only assumes continuity at $0$ , but not on $[0,1]$ . Here is the question: Assume $g$ is (Riemann) integrable on $[0, 1]$ and continuous at $0$ . Show $\lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0)$ . My attempt at this question: I split up the integral into two parts from $[0,1-\alpha]$ and $[1-\alpha,1]$ . For $[0,1-\alpha]$ , there is uniform continuity, so the limit $n \to \infty$ can be shifted inside the integral to get $g(0)$ . But how do I keep the integral over $[1-\alpha,1]$ to be small? Note: This is Exercise $7.4.10$ in Abbott, Understanding Analysis, 2nd edition.","['continuity', 'riemann-integration', 'real-analysis']"
3936379,"Approximating compactly supported $L^2$ functions with Schwartz functions ""from within""?","Crossposted from MathOverflow. It is well known that the class of Schwartz functions $\mathcal{S}$ in dense in all $L^p$ spaces therefore for each $f \in L^2$ there exists a sequence of Schwartz functions $(f_k)$ such that $\lVert f - f_k \rVert_{L^2} \to 0$ as $k \to \infty$ . If we suppose further that $f$ has compact support can we find a sequence of Schwartz functions $(f_k)$ such that $\lVert f - f_k \rVert_{L^2} \to 0$ as $k \to \infty$ (as above) and additionally $\operatorname{supp}(f_k) \subseteq \operatorname{supp}(f)$ for all $k$ ? If so, what argument can I appeal to?","['schwartz-space', 'functional-analysis', 'approximation-theory']"
3936389,Asymptotic behavior of $\sum\limits_{n \leq x}\frac{\phi(n)}{n}$,"I'm working with the asymptotic behavior of square-free integers and have already proven that if $f$ is an arithmetic function with $F(n) := \sum\limits_{d \mid n}f(d)$ , then \begin{align*}
\sum\limits_{n \leq x}F(n)=x\sum\limits_{d \leq x}\frac{f(d)}{d}+O\left(\sum\limits_{d \leq x}|f(d)|\right).
\end{align*} I'm trying to use this fact to prove that $\sum\limits_{n \leq x}\frac{\phi(n)}{n}=\frac{6}{\pi^2}x+O(\log{x})$ . I know that $\phi=\mu \ast E$ where $\ast$ is the usual Dirichlet convolution $f \ast g(n)=\sum\limits_{d \mid n}f(d)g\left(\frac{n}{d}\right)$ , so I can write \begin{align*}
    \phi(n)=\sum\limits_{d \mid n}\mu(d)E\left(\frac{n}{d}\right)=\sum\limits_{d \mid n}\mu(d)\frac{n}{d}=n\sum\limits_{d \mid n}\frac{\mu(d)}{d},
\end{align*} leading me to conclude that $\frac{\phi(n)}{n}=\sum\limits_{d \mid n}\frac{\mu(d)}{d}$ . Using my identity from before, I can write \begin{align*}
    \sum\limits_{n \leq x}\frac{\phi(n)}{n}&=x\sum\limits_{d \leq x}\frac{\frac{\mu(d)}{d}}{d}+O\left(\sum\limits_{d \leq x}\left|\frac{\mu(d)}{d}\right|\right) \\
    &=x\sum\limits_{d \leq x}\frac{\mu(d)}{d^2}+O\left(\sum\limits_{d \leq x}\left|\frac{\mu(d)}{d}\right|\right) \\
&=x\sum\limits_{d \geq 1}\frac{\mu(d)}{d^2}-x\sum\limits_{d \geq x}\frac{\mu(d)}{d^2}+O\left(\sum\limits_{d \leq x}\left|\frac{\mu(d)}{d}\right|\right) \\
&=x\cdot\frac{1}{\zeta(2)}-x\sum\limits_{d \geq x}\frac{\mu(d)}{d^2}+O\left(\sum\limits_{d \leq x}\left|\frac{\mu(d)}{d}\right|\right) \\
&=\frac{6}{\pi^2}x-x\sum\limits_{d \geq x}\frac{\mu(d)}{d^2}+O\left(\sum\limits_{d \leq x}\left|\frac{\mu(d)}{d}\right|\right).
\end{align*} My problem is that I don't understand how to get the $O(\log{x})$ . Can someone walk me through this?","['number-theory', 'asymptotics', 'arithmetic-functions']"
3936448,$f(x+y) +y$ does not exceed $f(f(f(x)))$,"Find all real to real functions for which $f(x+y) +y$ does not exceed $f(f(f(x)))$ for all real $x,y$ I don't feel like substituting values is that useful, except probably substituting $x=0$ , which gives a really nice bound on $f(y)$ , but other than that, I can't quite find anything else useful, at least not in my opinion.","['functional-equations', 'functions', 'functional-inequalities']"
3936560,number of positive integer solutions of $x+y+z+w=24$,"Determine the number of positive interger solutions of $x+y+z+w=24$ such that $x\leq 6, y \leq 7, z\leq 8, w\leq 9$ My try:
I used generating polynomial as $$f(x)=(x+x^2+\cdots+x^6)(x+x^2+\cdots+x^7)(x+x^2+\cdots+x^8)(x+x^2+\cdots+x^9$$ $\implies$ $$f(x)=x^4(1-x^6)(1-x^7)(1-x^8)(1-x^9)(1-x)^{-4}$$ We need to collect the coefficient of $x^{20}$ in $(1-x^6)(1-x^7)(1-x^8)(1-x^9)(1-x)^{-4}$ So we can write $$(1-x^6)(1-x^7)(1-x^8)(1-x^9)(1-x)^{-4}=(1-x)^{-4}(1-(x^6+x^7+x^8+x^9)+(x^{13}+x^{14}+2x^{15}+x^{16}+x^{17})-..)$$ Also using $$(1-x)^{-4}=\sum_{k=0}^{\infty}\binom{k+3}{k}x^k$$ We get required coefficient of $x^{20}$ as $$\binom{23}{3}-\left(\binom{14}{11}+\binom{15}{12}+\binom{16}{13}+\binom{17}{14}\right)+\left(\binom{6}{3}+\binom{7}{3}+2\binom{8}{3}+\binom{9}{3}+\binom{10}{3}\right)=83$$ But the answer is not matching.","['permutations', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
3936627,Why the graph of $\sin(\sin^{-1}(x))$ is the identity?,"I just made the following plots on Mathematica: This may be really silly but I don't understand why the plot of $\sin(\sin^{-1}(x))$ is the identity. Isn't $\sin^{-1}(x)$ is defined only on the interval $[-1,1]$ ? How come it has values outside of that interval? Here on wikipedia , it says: The trigonometric functions are periodic, and hence not injective, so strictly speaking, they do not have an inverse function. However, on each interval on which a trigonometric function is monotonic, one can define an inverse function, and this defines inverse trigonometric functions as multivalued functions. To define a true inverse function, one must restrict the domain to an interval where the function is monotonic, and is thus bijective from this interval to its image by the function. The common choice for this interval, called the set of principal values, is given in the following table. So is Mathematica using more than one set of principal values? I'm confused.","['functions', 'mathematica', 'inverse-function']"
3936635,"I have a function $f(x,y,z)$. Can this be rewritten as $f(y,z)$ if $x$ is not present in the function?","Let's say that I have $f(x,y,z)$ which is derived from another function g. And this is true for all cases. For a certain case, $f(x,y,z) = yz$ Is this the same as writing/ defining the function as $f(y,z) = yz$ ? Or is it imperative that I write it as the $f(x,y,z)=yz$ as its definition?","['multivariable-calculus', 'functions']"
3936647,Limit $\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t$,"Determine whether or not the limit below exists. $$\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t$$ I tried to use the Mean value theorem integrals to prove the limit exists, but it does not exist for $\lim\limits_{x\rightarrow0}\sin \frac{1}{x}$ . So I guessed the limit does not exist and used the Cauchy principle to prove it, but I failed. Any idea will be helpful.","['integration', 'limits', 'real-analysis']"
3936654,"Help to show that $\bigoplus_{i\in\mathbb{N}}\mathbb{Z}\cong\operatorname{Hom}(\prod_{i\in\mathbb{N}}\mathbb{Z},\mathbb{Z})$.","I am trying to show that $\bigoplus_{i\in\mathbb{N}}\mathbb{Z}\cong\operatorname{Hom}(\prod_{i\in\mathbb{N}}\mathbb{Z},\mathbb{Z})$ , as Abelian groups. I know that there is the map $f:\bigoplus_{i\in\mathbb{N}}\mathbb{Z}\rightarrow\operatorname{Hom}(\prod_{i\in\mathbb{N}}\mathbb{Z},\mathbb{Z})$ that takes an element $a=(a_1,a_2,...)$ to the function $f_a:\prod_{i\in\mathbb{N}}\mathbb{Z}\to\mathbb{Z}$ , where $f_a(x_1,x_2,...)=\sum_{i=1}^\infty a_ix_i$ . By definition, every element of $\bigoplus_{i\in\mathbb{N}}\mathbb{Z}$ has only finitely many non-zero entries. This means that the sum in $f_a$ will be of only finitely many non-zero terms and hence always converge, so $f_a$ is well-defined. I am able to show that each $f_a$ is a group homomorphism for every $a$ in $\bigoplus_{i\in\mathbb{N}}\mathbb{Z}$ , and that $f$ defines an injective group homomorphism, but I can't at all see why $f$ is surjective...","['modules', 'ring-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
3936665,Can conservative systems have unstable equilibrium points?,"In general, can conservative systems have a equilibrium(fixed) point which is unstable?
If so, I don't know why so in spite of these systems have conserved quantity.","['ordinary-differential-equations', 'dynamical-systems']"
3936676,Limit of $0^0$ (Indeterminate form),"Well this format of a limit $0^0$ is an indeterminate form. I claim that whatever this limit is (which depends on the exact question) should always be in between $[0,1]$ . Is my claim correct? I have no mathematical proof for it but just a basic idea, that any number base $0$ should try to pull towards $0$ , whereas any number power $0$ should pull towards $1$ .",['limits']
3936678,Efficiency of the prime generating constant $2.920050977316 \dots$ for the purpose of compressing a list of primes.,"The constant $c \approx 2.920050977316 \dots$ has the property that if $\{a_n\}$ is the sequence defined by $a_1 = c, a_{n+1} = \lfloor a_n \rfloor (a_n - \lfloor a_n \rfloor + 1),$ then $\lfloor a_n \rfloor$ is the $n$ th prime. We need to calculate primes in order to calculate the constant rather than the other way around, so this does not yield a method of generating new primes. However, we might be able to use the constant to compress an already known list of primes. The question then becomes, how many digits of $c$ are needed to generate the first $n$ primes? Note that it takes $\sim n \log n$ digits to store the first $n$ primes naively and $\sim n \log \log n$ digits by storing the differences between consecutive primes.","['number-theory', 'prime-numbers']"
3936710,On a Theorem of Lee's book on Differential Geometry,"The following theorem is from Lee ""Introduction to Smooth Manifolds"". Suppose $\widetilde{M}$ is a connected smooth manifold with a discrete group $\Gamma$ acting smoothly, freely and properly on $\widetilde{M}$ . Then the quotient map $\pi:\widetilde{M}\to \widetilde{M}/\Gamma$ is a smooth normal covering map. My question is if the connected assumption on $\widetilde{M}$ necessary? I do not see if this assumption plays any role in the proof. Secondly, how to show that $\Gamma$ acts trasitively on the fibers of $\pi$ ? My third question is if the covering group of $\widetilde{M}/\Gamma$ and $\Gamma$ are isomorphic? Clearly, each element of $\Gamma$ acts as a Deck transformation on $\widetilde{M}$ . Is the other way also true?","['covering-spaces', 'smooth-manifolds', 'algebraic-topology', 'differential-geometry']"
3936753,Solving for a differential equation Gompertz growth equation,"What is the general solution of this differential equation? $$ \frac{dy}{dt} = k \enspace  y \enspace \ln(\frac{a}{y})$$ where $a$ and $k$ are positive constants. So far, my solution is: $$ \frac{dy}{y \enspace \ln(\frac{a}{y})} = k \enspace dt$$ When I let $u=lny$ , $$ \int \frac{1}{y(\ln a-\ln y)} \,dy =\int k dt $$ $$ \int \frac{1}{\ln a-u} \,du= kt + C $$ How to continue this?","['integration', 'calculus', 'derivatives', 'differential']"
3936767,Finding integrating factor for non exact differential equation,"I am trying to solve non exact differential equation but I am unable to find the integration factor. I solve the following question $$-3y\frac{dy}{dx}+2x=0 $$ and I get the integrationg factor $x^{-5/2}$ and I use the proper way to find the integrating factor, but in our text book the integrating facot for above differential equation is finded and that is $y/x^4$ . Now I am completely confused that why please help me out with this.","['integration', 'ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'algebra-precalculus']"
3936773,Is $V= W\oplus W^{\perp}$ always?,"Is $V= W\oplus W^{\perp}$ always? Here, $W$ is a subspace of the vector space $V$ . I think this holds if $V$ is finite-dimensional, but what if it is not? Could someone help me prove $V= W\oplus W^{\perp}$ in the infinite-dimensional case, or produce a counterexample? I'm really not sure where to start. The thing is, I am not able to see how moving from finite dimensions to infinite dimensions changes anything. Shouldn't it be the case that a vector lies in $W$ or $W^\perp$ ? Where else can it lie? P.S. This isn't part of a homework exercise or something, it just came to my mind. I'm taking a class in Linear Algebra right now that deals mostly with finite-dimensional vector spaces only, so I don't know where to start in this case.","['linear-algebra', 'vector-spaces']"
3936782,A problem in discrete optimization,"I am interested in the following problem and its generalizations. Say we are on the real axis, and we have $n$ points $x_1, \ldots, x_n$ on this line. I would like to minimize/maximize the length someone would walk to go through all of them in a certain order. Intuitively, I feel: to minimize, we can pass by all of them from the leftmost one to the rightmost one; to maximize, we would need to do many ""round-trips"", going from a point more on the ""left side"" to a point on the ""right side"", etc. but I don't see in what order it is better to do it. Is there a convenient way to write a proof of the minimization? And how to determine if the heuristic strategy I suggest in the second case works or can be improved. Less trivially maybe, I am interested in an equivalent question in a two-dimensional disk.","['discrete-optimization', 'optimization', 'discrete-mathematics', 'computational-geometry']"
3936786,Prove that $p(n)$ is the number of ways of partition of $n$,"For a positive integer $n$ , define $p(n)$ to be the number of sets of positive integers $\{ x_1, x_2, \dots, x_k \}$ such that $x_1 > x_2 > \dots > x_k > 0$ and $n = x_1 + x_3 + x_5 + \dots$ (sum of all odd indices element of the set). For example, $p(6) = 11$ since we could have $$ \{ 6 \}, \{ 6, 5 \}, \{ 6, 4 \}, \{ 6, 3 \}, \{ 6, 2 \}, \{ 6, 1 \}, \{ 5, 4, 1\}, \{ 5, 3, 1 \}, \{ 5, 2, 1 \}, \{ 4, 3, 2 \}, \{ 4, 3, 2, 1\} $$ Prove that $p(n)$ is the number of partition of integer $n$ for all $n \in \mathbb{N}$ This is from a selection test that already ended. I think it can be done by bijection (maybe generating function), but I have no idea how to continue.","['integer-partitions', 'combinatorics']"
3936822,Evaluate $\lim\limits_{n\to\infty} \frac{\sin(1)+\sin^2(\frac{1}{2})+\ldots+\sin^n(\frac{1}{n})}{\frac{1}{1!}+\frac{1}{2!}+\ldots+\frac{1}{n!}}$,"This was a recent problem on the Awesome Math Problem Column. The solution is given as follows: We shall use Stolz-Cesaro Lemma . We have: $$\lim_{n\to\infty} \frac{\sin(1)+\sin^2(\frac{1}{2})+\ldots+\sin^n(\frac{1}{n})}{\frac{1}{1!}+\frac{1}{2!}+\ldots+\frac{1}{n!}}$$ $$=\lim_{n\to\infty}\frac{\sin^n(\frac{1}{n})}{\frac{1}{n!}}=\lim_{n\to\infty}\frac{\sin^n(\frac{1}{n})}{\frac{1}{n^n}}\cdot\lim_{n\to\infty} \frac{n!}{n^n}.$$ Since $$\frac{n!}{n^n}<\frac{1}{n}$$ we have $$\lim_{n\to\infty}\frac{n!}{n^n}=0$$ So, the required limit is $0$ . I have two problems with the given proof that I can’t wrap my head around. Firstly, from everything I have read so far the Stolz-Cesaro limit only applies to cases where the limit is of the form $\frac{0}{0}$ and the form $\frac{\cdot}{\infty}$ . However, just from the wording of the theorem and the way the author presented this solution both seem to imply the following: $$a_n=\sum_{k=1}^n \sin^k\Big(\frac{1}{k}\Big), \ \ \  \text{and} \ \ \ b_n=\sum_{k=1}^n \frac{1}{k!}$$ Then when “applying” the Stolz-Cesaro theorem you would have: $$\lim_{n\to\infty} \frac{a_n-a_{n-1}}{b_n-b_{n-1}}=\lim_{n\to\infty} \frac{\sum\limits_{k=1}^{n} \sin^k(\frac{1}{k})-\sum\limits_{k=1}^{n-1} \sin^k(\frac{1}{k})}{\sum\limits_{k=1}^n \frac{1}{k!}-\sum\limits_{k=1}^{n-1} \frac{1}{k!}}=\lim_{n\to\infty}\frac{\sin^n(\frac{1}{n})}{\frac{1}{n!}}$$ The problem that I see is when $n\to\infty$ we know that $a_n$ converges by the root test, and $b_n$ converges to $e-1$ . So by basic limit rules this would converge also. So it seems that $b_n$ does not satisfy either of the sequence requirements of the Stolz-Cesaro Theorem. Secondly, according to wolfram $\displaystyle\sum_{n=1}^\infty \sin^n\Big(\frac{1}{n}\Big)\approx 1.11043$ . So intuitively it seems to me that this limit would be approximately $\frac{1.11043}{e-1}\approx 0.646244$ . Does anyone know what is going on here?","['real-analysis', 'closed-form', 'sequences-and-series', 'limits', 'convergence-divergence']"
3936873,"$2n$ cells were marked on an infinite triangular grid, can you always find a triangle that contains exactly $n$ marked cells?","The problem: (Solve the problem for each positive integer $n$ separately) $2n$ cells were marked on an infinite triangular grid, is it always possible to find a triangle (made by the grid lines) that contains exactly $n$ marked cells? My progress: I managed to solve the problem for every odd integer and for $n=4,6,8,10,12$ My inspiration to create the problem: my inspiration","['combinatorics', 'open-problem']"
3936907,"If $X$ is binomial, $2X$ isn't binomial","My question is really simple. I Don't understand why if a random variable $X \sim \text{Bin}(n,p)$ , then $2X$ isn't binomial. I know every value of $2X$ is even, but I can't prove $2X$ isn't binomial using this fact.","['statistics', 'binomial-distribution', 'random-variables']"
3936952,How to solve the following geometry problem?,"The diameters of the circumcircle of triangle $ABC$ drawn from $A$ , $B$ and $C$ meet $BC$ , $CA$ and $AB$ , respectively, at $L$ , $M$ and $N$ . Prove that: $$\frac{1}{ AL} + \frac{1}{BM}+\frac{1}{CN}=\frac{2}{R}$$ I tried with formulas based on properties of triangle and some with geometry properties with circumcircle but at the end not able to reach the result, I earnestly urge for your help.","['triangles', 'trigonometry', 'geometry']"
3936995,"If $\langle f,\varphi\rangle$ is integrable for every functional $φ$, is there an $x$ with $\langle x,\varphi\rangle=\int\langle f,φ\rangle$?","Let $(\Omega,\mathcal A,\mu)$ be a measure space, $E$ be a normed space and $f:\Omega\to E$ be strongly $^1$ $\mathcal A$ -measurable with $$\langle f,\varphi\rangle\in\mathcal L^1(\mu)\;\;\;\text{for all }\varphi\in E'.$$ Are we able to show that $$\int\langle f,\varphi\rangle\:{\rm d}\mu=\langle x,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\tag1$$ for some $x\in E$ ? If not, what are sufficient conditions to ensure that? If $f$ has finite range, the claim is obviously true. If not, there are $\mathcal A$ -measurable $f_n:\Omega\to\mathbb R$ with finite range for $n\in\mathbb N$ satisfying $\left\|f_n\right\|_E\le\left\|f\right\|_E$ and $\left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0$ . So, there are $(x_n)_{n\in\mathbb N}\subseteq E$ with $$\int\langle f_n,\varphi\rangle\:{\rm d}\mu=\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\text{ and }n\in\mathbb N\tag2.$$ Using Lebesgue’s dominated convergence theorem, it's now easy to see that $$\langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag3\;\;\;\text{for all }\varphi\in E',$$ which at least implies that $(x_n)_{n\in\mathbb N}$ is weakly Cauchy. However, we know that a weak Cauchy sequence doesn't need to weakly converge. So, maybe the problem I'm asking here is a particular instance of the fact that if $(x_n)_{n\in\mathbb N}$ is any weakl Cauchy sequence, then $$L\varphi:=\lim_{n\to\infty}\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E'$$ is a well-defined linear functional (since the field over which $E$ is a vector space is complete), but $L$ is not necessarily of the form $$L\varphi=\langle x,\varphi\rangle\tag4$$ for some $x\in E$ . Maybe all of this boils down to the properties of the canonical embedding $$\iota:E\to E''\;,\;\;\;x\mapsto\left(E'\ni\varphi\mapsto\langle x,\varphi\rangle\right).$$ In any case, I would really appreciate if someone could show under which conditions we are able to prove the desired result; but I'm also interested in results related to the general case for the functional $L$ . EDIT As David Mitra pointed out in the comments, it seems like the question boils down to find conditions under which a Dunford integrable function is even Pettis integrable. Let me fix the terminology: $f:\Omega\to E$ to is called Dunford $\mu$ -integrable if $\langle f,\varphi\rangle\mathcal L^1(\mu)$ . In that case, $$\int f\:{\rm d}\mu:E'\to\mathbb K\;,\;\;\;\varphi\mapsto\int\langle f,\varphi\rangle\:{\rm d}\mu\tag5$$ is a bounded linear functional on $E'$ , i.e. $$\int f\:{\rm d}\mu\in E''.\tag6$$ Now $f$ is called Pettis $\mu$ -integrable if $f$ is Dunford $\mu$ -integrable and $$\int f\:{\rm d}\mu\in\iota E.\tag7$$ First approach for a positive result : I've often seen the assertion that the desired claim is true when $\Omega=[a,b]$ for some $a<b$ , $\mathcal A=\mathcal B([a,b])$ , $\mu$ is the restriction of the Lebesgue measure on $\mathcal B(\mathbb R)$ to $[a,b]$ and $f$ is continuous. The crucial argument is that $[a,b]$ is compact, hence $f$ is even uniformly continuous, which allows us to find $(x_n)_{n\in\mathbb N}\subseteq E$ such that $$\langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag8$$ uniformly with respect to $\left\|\varphi\right\|_{E'}\le1$ . By Lemma 1 below, this is enough to conclude, but I'm quite sure that this result holds in way more generality. Lemma 1 : In general, $(x_n)_{n\in\mathbb N}\subseteq E$ is norm convergent if and only if $$\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|\xrightarrow{m,\:n\to\infty}0.$$ Second approach for a positive result : Let me continue with the setting I've described at the beginning of this post, i.e. the situation where $f$ is strongly $\mathcal A$ -measurable. Most probably I'm missing something, but if $(x_n)_{n\in\mathbb N}$ and $(f_n)_{n\in\mathbb N}$ are as in $(2)$ and $(3)$ , then we should have \begin{equation}\begin{split}\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|&=\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\int\langle f_m-f_n,\varphi\rangle\:{\rm d}\mu\right|\\&\le\int\left\|f_m-f_n\right\|_E\:{\rm d}\mu\xrightarrow{m,\:n\:\to\:\infty}0\end{split}\tag9.\end{equation} So, by Lemma 1, we should be able to conclude. Am I missing something? If not and this is correct, is this somehow a generalization of what I've described in the ""first approach""? Therein $f$ is continuous (hence $\mathcal A$ -measurable) and, since $[a,b]$ is separable, $f([a,b])$ is separable. So, $f$ is strongly $\mathcal A$ -measurable. Maybe that's the crucial point. $^1$ i.e. $f$ is $\mathcal A$ -measurable and $f(\Omega)$ is separable. The crucial point is that a function $f:\Omega\to E$ is strongly $\mathcal A$ if and only if there are $\mathcal A$ -measurable $f_n:\Omega\to\mathbb R$ with finite range for $n\in\mathbb N$ satisfying $\left\|f_n\right\|_E\le\left\|f\right\|_E$ and $\left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0$ .","['banach-spaces', 'measure-theory', 'functional-analysis', 'dual-spaces']"
3937014,"Lebesgue measure coincides on boxes, with measure $m'$ that is translation invariant, and that holds the following equation: $m′([0,1]^d)=1$","I saw this question in measure theory course: Let $m′:L(R^d)→[0,∞]$ be a measure on the Lebesgue measurable sets, satisfying: • Unit box measure: $m′([0,1]^d)=1$ . • Translation invariance: $m′(x+A)=m′(A)$ for all $A∈L(R^d)$ and $x∈R^d$ . Show that $m′$ agrees with the Lebesgue measure on boxes, i.e., $m′(B)=m(B)$ for any box $B⊆R^d$ . I was able to show that for any box with integer length edges, I just used the 2 properties of $m'$ and the additivity property of a measure. But how can I show it for general boxes?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3937097,Infinite sums and intersections of subsets of $\mathbb{R}^2$,"In this exercise I am asked to find $\bigcup_a \bigcap_b X_{a,b}$ (as well as other variations of sums and intersections of $X_{a,b}$ ) where $X_{a,b}=\{(x,y)\in \mathbb{R}^2 \vert y \leq ax(x-b)\} a,b \in \mathbb{R}, a,b >0 $ . I drew the graph of $y =ax(x-b)$ but it's really hard for me to visualize what $\bigcup_a \bigcap_b X_{a,b}$ is going to look like by sheer looking at the graph. I also plugged the formula into desmos to see how it behaves when $a$ and $b$ are changing, but this didn't help either as the results I got were different from the answers provided by the teacher. Is there any systematic approach to a problem like that? Working on exercises of this type becomes dreadful without any good method as the sets get more complicated - what about this monstrosity for instance: $X_{a,b}=\{(x,y)\in \mathbb{R}^2 \vert ax^2 < y \leq \sqrt[3]{ab^2} a,b \in \mathbb{R}^2 a,b>0\}$ ?!","['elementary-set-theory', 'geometry']"
3937138,Methods of solving an initial-value problem for a separable first-order ODE,"In Schaum's outline of Differential Equations it is said that the initial-value problem of $A(x) d x+B(y) d y=0 $ ; $y\left(x_{0}\right)=y_{0}$ can be solved either $(1)$ integrating and applying the initial condition to evaluate $c$ or by using $(2)$ : $$
\int A(x) d x+\int B(y) d y=c; \quad y\left(x_{0}\right)=y_{0} \tag{1}
$$ $$
\int_{x_{0}}^{x} A(x) d x+\int_{y_{0}}^{y} B(y) d y=0 \tag{2}
$$ It is also said that this second method may not determine the solution of the initial-value problem uniquely; thal is, it may have many solutions, of which only one will satisfy the initial-value problem. As I see it, both methods are equivalent: if $\widetilde A(x)$ and $\widetilde B(y)$ are the primitive functions of $A(x)$ and $B(y)$ , then $$(1) \rightarrow \widetilde A(x) + \widetilde B(y) =c \rightarrow c= \widetilde A(x_0) + \widetilde B(y_0)\rightarrow \\\widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0$$ Which is the same result we get from $(2)$ : $$(2) \rightarrow \widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0$$ Why does then the book say that the second method may may not determine the solution of the initial-value problem uniquely ?","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
3937185,Divergence and curl of field proportional to itself.,"I have a system of equations that can be put in the form, $\begin{align}\nabla\cdot \mathbf{n}=\mathbf{T}\cdot  \mathbf{n}\end{align}\\\nabla\times \mathbf{n}=\mathbf{T}\times  \mathbf{n},$ with $\mathbf{T}$ a vector such that $\nabla\cdot \mathbf{T}=0,\: \mathbf{T}=(f(y),g(x),0)$ , and $\mathbf{n}$ has to lie on the plane, that is, $\mathbf{n}=(n_x(x,y),n_y(x,y),0)$ . Also some regular data on a closed curve on the plane can be provided. The question is how to find $\mathbf{n}$ ? In terms of $\mathbf{T}$ , course. The system is well posed, so it has solution, but so far that's all I can say. I'd like to have a explicit integral. The most similar problem that I've seen is that of $\nabla\times f=f$ , but here I'm much more constrained. Thanks in advance,","['vector-fields', 'multivariable-calculus', 'vector-analysis', 'partial-differential-equations', 'differential-geometry']"
3937248,Automorphism of Riemann Sphere without Picard's Theorem,"When I am trying to get familiar with the contributions that Gauss made to the quaternions , the following question comes to my mind: Question: Is there a known proof that every 1-1 comformal mapping of Riemann Sphere to itself is a Mobius Transformation, without using Picard's Theorem about essential singularities ?",['complex-analysis']
3937325,Cholesky decomposition to price asymmetric derivatives - Thank you all for your attention,"I am new here. My first post concerns the evaluation of multiasset options in Robust Programming like you see from papers below Robust Option Pricing (page 848, 5.1) Robust Option Pricing (page 31, 6.1) Option Pricing with LP (page 76, 6.2). I write this post because I am trying to understand how the authors of these works arrive to the condition $ \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma $ where $\Gamma$ is a predefined parameter whose value reflects the degree of risk-tolerance of the investor, but for purposes of question it doesn't matter knowing what it means. $\widetilde{R}_1=\begin{bmatrix}
\frac{\tilde{S}_1^1}{S_0^1} & \frac{\tilde{S}_1^2}{S_0^2} & \cdots  & \frac{\tilde{S}_1^M}{S_0^M}
\end{bmatrix}^T$ represents the vector of random returns for the first period (i.e. $t=1$ ) given the current price of $m$ -th stock included in the underlying basket of multiasset option, for $m=1,...,M$ . $\check{R}_1=\begin{bmatrix}
\frac{\check{S}_1^1}{S_0^1} & \frac{\check{S}_1^2}{S_0^2} & \cdots  & \frac{\check{S}_1^M}{S_0^M}
\end{bmatrix}^T$ represents the vector of expected returns for the first period (i.e. $t=1$ ) given the current price of $m$ -th stock included in the underlying basket of multiasset option, for $m=1,...,M$ . $C=\sum^{-1/2}$ is a matrix obtained with Cholesky decomposition. Text say that $C^{T}C=\sum^{-1}$ (see page 76 third paper) where $\sum$ is the variance-covariance matrix of returns. $||x||$ is (I quote from page 31 second link that I inserted) a general norm of a factor, depending on the modeler's preference, it can be $L_1,L_2,L_{\infty}$ or $D$ -norm . Paper below Robust Linear Optimization under General Norms elaborates on the meaning of $||x||$ but I don’t quite understand what it says. At page 513, authors say to consider an uncertainty set for $\widetilde{A}$ with $U=\begin{Bmatrix}
\widetilde{A}|\left \| M(vec(\widetilde{A})-vec(\check{A}))\right \|\leq \Delta
\end{Bmatrix},\Delta \geq 0$ , and for Proposition 1. of page 512 they say that we have to apply the definition of $||x||_p \doteq (\sum_{j=1}^{n}|x_j|^p)^{\frac{1}{p}}$ , with $M$ a diagonal matrix containing the inverses of the ranges of coefficient variation . This way to model the uncertainty results from the fact that "" Let S be a closed, bounded convex set and consider an uncertainty set in which the uncertain coefficients are allowed to vary in such a way that the deviations from their nominal values fall in a convex set. [...] We consider uncertainty sets that arise from the requirement that the distance (as measured by an arbitrary norm) between uncertain coefficients and their nominal values is bounded. "" ** Fog. ** This is what I've understood, I hope you can help me to make clear my reasoning. Since with a multiasset option we need to consider the correlation between asset prices included in the underlying basket of option, authors sugget (for independence and identically distribution, i.e. to say second order stationarity, that characterize log-returns, which implies the normality for CLT theorem: we are in a Black-Scholes environment) to study correlation between asset returns given the condition $R_t=\frac{S_t}{S_0}$ that binds prices and returns. Clearly correlation should be evaluated between each $m$ -th asset included in the basket (i.e. for each couple of asset on a set of $M$ elements) and for each $t$ -th instant of time. Obviously it's not the case to have such tremendous amount of information, moreover subject to uncertainty, so instead to constructing a random variance-covariance matrix for each $t$ ... [i.e. to say considering, for each $t$ , one by one, all the possible couples of assets and for any $j$ -th possible realization of assets returns (for $j=1,...,n$ and $n$ the sample size) calculating the sum of cross products of deviations of these possibile realizations from their estimations] ...we use the properties of $\sum$ . I know that $\sum$ is square, symmetric and positive definite, i.e. to say not-singular, i.e. to say invertible, so exists a $\sum^{-1}$ . Since $\sum^{-1}$ is also symmetric and positive definite I can factor it with Cholesky, that is an LU decomposition for symmetric and positive definite matrices. So, I should have an upper triangular matrix $C$ with positive elements above the diagonal and a lower triangular normed matrix $C^T$ (i.e. with all ones under under the diagonal). And because the intersection between $t$ lower triangular matrices and $t$ upper triangular matrices gives us $t$ diagonal matrices, we should get $M$ matrix. Right? Now I am confused. A) How do we know that $M$ contains the inverses of the ranges of coefficient variation ? And what does it mean this proposition? B) Authors say that modelling the uncertainty set of returns in that way ensure , regardless of future realization of returns, that the difference with their expectations will be always lower than a not-negative Delta. But why multiply $M$ with that diff? What does it mean, basically, that matrix product? C) In the second paper (page 31), author gives us an example with the $L_1$ -norm. I quote: If $L_1$ is used, $ \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma $ is equivalent to: $\begin{matrix}
\sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq \sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M\\ -\sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq -\sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M
\\ \sum_{m=1}^{M}s^m \leq \Gamma
\end{matrix}$ Does he only apply the definition of norm? D) Authors say that the choice of a norm is discretional, but what does it involves choice a norm instead another one? What are the implications on the number of constraints of linear programming problem? Thanks in advance just for reading.","['normed-spaces', 'matrices', 'linear-algebra', 'optimization', 'probability']"
3937361,"Interesting real integrals from $ I=\oint\limits_{C}\sin(\frac{1}{z})\,\mathrm{dz} = 2\pi i $","I have separated and equated Real and Imaginary Parts of the following equation by plugging $\mathrm{z}=e^{it}$ : $$I=\oint\limits_{C}\sin\frac{1}{z}\,\mathrm{dz} = 2\pi i,$$ where $C$ is the boundary of unit circle centered at origin. which gives $$ K=\int_{-\pi}^{+\pi}e^{it}\sin(e^{-it})\mathrm{dt} = 2\pi$$ simplifying gives the following:- $\int_{0}^{\frac{\pi}{2}}\cos(t)\sin(\cos(t))\cosh(\sin(t))+\sin(t)\cos(\cos(t))\sinh(\sin(t))\mathrm{dt} = \frac{\pi}{2}$ the first and second part of the above integral translates to $I_1$ and $I_2$ respectively under a simple substitution for each. $$ I_1 = \int_{0}^1 \cosh(x)\sin(\sqrt{1-x^2})\mathrm{dx}\\ I_2 = \int_{0}^1 \cos(x)\sinh(\sqrt{1-x^2})\mathrm{dx}$$ which implies, $I_1 + I_2 = \frac{\Large\pi}{2}$ Interestingly, I have found that $I_1 $ and $ I_2$ are equal (using a calculator), but stuck proving them.
Any ideas how they are equal or finding another method to calculate $I_1 $ or $ I_2$ ? EXTRA: some integrals that show up but eventually cancel out( as they are odd functions, $f(x)=f(-x)$ ) in calculating Imaginary part of equation $K$ :- $$ I_3 = \int_{0}^1 \sin(x)\cosh(\sqrt{1-x^2})\mathrm{dx}\\ I_4 = \int_{0}^1 \sinh(x)\cos(\sqrt{1-x^2})\mathrm{dx}$$ however $I_3 $ and $ I_4$ are not equal and their numerical values are approx. 0.584 and 0.418 , can $I_3 $ and $ I_4$ be calculated in closed form?","['integration', 'definite-integrals', 'complex-analysis', 'contour-integration', 'complex-integration']"
3937439,Differential geometry in graph theory,"is there a free book or publication with some applications of differential geometry (especially 2-manifolds) in graph theory.
At that moment I know that graph which are not planar on plane, can be planar on manifolds. But that does not use differential geometry, only treats manifolds as subsets of R^3. I know that every class of graphs which is closed under taking minors, is defined by finite subset of ""forbidden"" minors (it is powerful Robertson-Seymour theorem). I need to write an essay for difgeo course and I thought about something like that, but I can't see anything in my range. I mean something that uses curves, 2-manifolds, easy tensors (in particular metrics).
E. g. where I can find a proof that every graph Kn is planar on manifold being sphere with big enough number of ears (torus is sphere with 1 ear) - maybe it would be cool part.","['graph-theory', 'planar-graphs', 'differential-geometry']"
3937457,Solving finite and infinite nested square roots of 2 - yet another interesting approach,"Consider the following consecutive equalities: $\sqrt2=2\cos(\frac{1}{4})\pi$ $\sqrt{2-\sqrt2}=2\cos(\frac{3}{8})\pi$ $\sqrt{2-\sqrt{2-\sqrt{2}}}=2\cos(\frac{5}{16})\pi$ $\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2}}}}=2\cos(\frac{11}{32})\pi$ $\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2}}}}}=2\cos(\frac{21}{64})\pi$ $\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2-\sqrt{2}}}}}}=2\cos(\frac{43}{128})\pi$ Here is the beautiful part $\frac{3}{8} = \frac{1}{4}+\frac{1}{8}$ $\frac{5}{16} = \frac{3}{8}-\frac{1}{16}$ = $\frac{1}{4}+\frac{1}{8}-\frac{1}{16}$ $\frac{11}{32} = \frac{5}{16}+\frac{1}{32}$ = $\frac{1}{4}+\frac{1}{8}-\frac{1}{16}+\frac{1}{32}$ $\frac{21}{64} = \frac{11}{32}-\frac{1}{64}$ = $\frac{1}{4}+\frac{1}{8}-\frac{1}{16}+\frac{1}{32}-\frac{1}{64}$ $\frac{43}{128} = \frac{21}{64}+\frac{1}{128}$ = $\frac{1}{4}+\frac{1}{8}-\frac{1}{16}+\frac{1}{32}-\frac{1}{64}+\frac{1}{128}$ If extended to infinity $\sqrt{2-\sqrt{2-\sqrt{2...}}} = 2cos(\frac{1}{3})\pi$ (we can calculate the sum of infinite Geometric progression and we get there result as $1\over3$ ) We can easily guess that if a nested square root has $n$ $2$ ’s then a denominator of the corresponding cosine argument is $2^{n+1}$ . Most beautiful part is getting the numerator which is $2^{n+1} \over 3$ approximated to nearest integer.[or approximation to floor and making it as very next odd number(if the floor is even number)--> and this is better ] Subsequent numbers having numerator are as follows $\frac{2^3}{3}=3;\frac{2^4}{3}=5;\frac{2^5}{3}=11;\frac{2^6}{3}=21;\frac{2^7}{3}=43;\frac{2^8}{3}=85;\frac{2^9}{3}=171;\frac{2^{10}}{3}=341...$ Therefore for the finite number of nested square roots of 2 like this, it is the easy way to evaluate the angle inside the cosine function! One more exciting part is association with integer sequence of type Jacobsthal sequence $a(n-1) + 2a(n-2)$ , with $a(0) = 0, a(1) = 1$ . Still more interesting part is Fibonacci like pattern from above integer sequence My question is can we link geometric progression to simplify different patterns of the finite or infinite nested square roots of 2, like this? To expand my question to understand Let us consider finite nested square roots of 2 in simplified way $\sqrt{2-\sqrt2}$ as $n\sqrt2(1-)$ and $\sqrt{2-\sqrt{2+\sqrt2}}$ as $n\sqrt2(1-1+)$ If we have finite nested square roots of 2 such as $n\sqrt2(1-2+)$ repeated signs inside nested radical 'n' times and infinitely, how can we get the angle?. What I have shown is simplest example. Let us consider $n\sqrt2(3-1+7-)$ or $n\sqrt2(2-3+4-1+)$ and so on. Is there any easier way to find the cosine angle?","['nested-radicals', 'fibonacci-numbers', 'integers', 'trigonometry', 'geometric-series']"
3937521,"Example of knot that ""becomes unknotted in a different 3-manifold""?","Suppose I had a knot $K \subset \mathbb{R}^3$ , and an open ball $U$ inside a $3$ -manifold $M$ . If I now consider $K \subset U$ (via some homeomorphism of $\mathbb{R}^3$ with $U$ ), could it be possible that $K$ is unknotted in $M$ ? That is to say, $K$ becomes isotopic to the unknot in $U$ ; somehow ""using the additional geometry that $M$ provides to make unknotting easier""? If so, are there any easy examples I can write down?","['knot-theory', 'manifolds', 'general-topology']"
3937546,derivative for the following matrix [duplicate],"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I have a question about the following derivative. Let us have $X\in\mathbb{R}^{m\times n}, z\in\mathbb{R}^{n}$ and I would like to find the derivative $$\frac{\partial (z^{T}X^{T}Xz)}{\partial z}. $$ Any idea? It gives me a hard time. Thank you.","['matrices', 'multivariable-calculus', 'calculus', 'linear-algebra', 'derivatives']"
3937562,Two cowboys A and B decide to solve a dispute with a duel. [duplicate],"This question already has an answer here : Two players shooting a target alternately (1 answer) Closed 3 years ago . Two cowboys $A$ and $B$ decide to solve a dispute with a duel.
Cowboy $A$ hits his target $\frac13$ of the time.
Cowboy $B$ hits the target $\frac23$ of the time. It is decided that $A$ will take the first shot, cowboy $B$ will take the second shot (if still alive).
This will continue until there is only one left alive. Also, a cowboy can not shoot two times in a row. What are cowboy $A$ chances of winning the duel? My logic is to simulate the cases where $A$ wins: I Case: $A$ wins: $\frac13$ II Case: $A$ loses, $B$ loses, $A$ wins: $\frac23\cdot\frac13\cdot\frac13$ ... But I don't know what to do from here. Any help?","['discrete-mathematics', 'probability-theory', 'probability']"
3937575,Are my examples of False $\implies$ False being true correct?,"I know that $p \implies q$ is true if $p$ is false. I am trying to wrap my brain around this by thinking about examples. I have two examples that I would like to understand. Example 1 : Consider the set $$
X = \{x\in \mathbb{R}: x\geq 0 \implies x^2\leq 0\}.
$$ This set is equal to the interval $(-\infty, 0]$ because, for example, $-1 \geq 0 \implies -1\leq 0$ is a true statement. Is that correct? Example 2 : I know that the definition of limit is $$
\forall \epsilon>0 \exists \delta > 0: 0<\lvert x-a\rvert < \delta \implies \lvert f(x) - L\rvert < \epsilon.
$$ Would this make this a true statement for all functions $f$ ? Say $\epsilon$ is given, then pick $\delta = -1$ . Then $0<\lvert x- a\rvert < \delta$ is false, so it is true that it does imply that $\lvert f(x) - L\rvert < \epsilon$ . Have I just misunderstood the definition? EDIT: I just realized that in my second example the requirement is that $\delta$ be positive. I guess I am down to the first example.","['logic', 'discrete-mathematics']"
3937585,Solving a Secret List (Inspired by 'Are you the One?'),"Let's say there's a list of the numbers $1$ through $N$ in some sequence, unknown to us. This list is the ""correct"" order. You are able to arrange those numbers in any way, and after each step you are told how many of the items on your list are in the correct position, but not which ones. The goal is to guess the order of the secret list in as few steps as possible.
The 2 questions I've been thinking of are these: Is there a strategy that guarantees you find the correct order in $M<N!$ steps? Given a maximum number of guesses $G<M$ , is there a strategy that maximizes your probability of finding the right order in $\leq G$ steps? So far I haven't been able to make much headway. I though I could do something by keeping a fixed order and rotating it $N-2$ times, which would put every item in all but one spot once without double counting, and the $N-1$ st could be deduced given the number of correct spaces would have to add up to $N$ . I don't think it gives useful information though. I imagine in order to find out which positions are correct you'd have to have that spot repeated at least once. Thoughts?","['permutations', 'combinatorics']"
3937609,Can an irregular tetrahedron be inscribed in a cuboid or prism?,A regular tetrahedron can be inscribed in a cube in a way that the tetrahedron edges are diagonals of the cube faces. Is it possible to similarly inscribe any irregular tetrahedron in a cuboid or prism? Image source: http://mathworld.wolfram.com/RegularTetrahedron.html,"['euclidean-geometry', 'geometry', 'simplex', 'platonic-solids']"
3937610,Proving $\sum_{k=1}^n\frac{\sin(k\pi\frac{2m+1}{n+1})}k>\sum_{k=1}^n\frac{\sin(k\pi\frac{2m+3}{n+1})}k$,"A few days ago I asked a question about an interesting property of the partial sums of the series $\sum\sin(nx)/n$ .
Here's the link: Bound the absolute value of the partial sums of $\sum \frac{\sin(nx)}{n}$ The proof given there left some details for me, in particular I have to prove that $$
\sum_{k = 1}^{n}{1 \over k}
\sin\left(k\pi\frac{2m+1}{n+1}\right) >
\sum_{k=1}^{n}{1 \over k}\sin\left(k\pi\frac{2m+3}{n+1}\right)
$$ for every natural $n$ and $m$ such that $m = 0, 1, \ldots, \left\lfloor\left(n-1\right)/2\right\rfloor$ . I don't think this is relatively simple; anyway I tried a few expansions with sum-to-product and viceversa formulas, but everything I tried leads to $$\sum_{k=1}^{n}\frac{\cos(2(m+1)x_k)\sin(x_k)}{k} < 0$$ where $x_k = k\pi/(n+1)$ . From here I don't see any further semplification. Also I noted that $\sin(x_k)$ is always positive since $k$ is at most $n$ , but we can't say much about $\cos$ .
Then I tried to pair terms since for $k' = n+1-k$ we have $\sin(x_k) = \sin(x_k')$ , which gives $$\sum_{k=1}^{n/2}\left[\frac{\cos(2(m+1)x_k)}{k}+\frac{\cos(2(m+1)x_{k'})}{n+1-k}\right]\sin(x_k) < 0$$ for $n$ even. Then I hoped that the term between square brackets were always negative, but it's not always the case... How can I do?","['trigonometry', 'summation', 'inequality', 'real-analysis']"
3937657,"Proving that a function doesn't have a limit given $|a-b| = |f(a)-f(b)|$ for all $a,b\in \mathbb R$","For all $a,b\in \mathbb R$ , $|a-b| = |f(a)-f(b)|$ . Prove that $\;\lim\limits_{a\to \infty}|f(a)|=\infty$ What I was trying to do is show that if we assume that there is a limit, $\;\lim\limits_{a\to \infty}|f(a)|=L\;$ and therefore $\big||f(a)|-L\big|=\big||f(a)|-|L|\big| \leq |f(a)-L| < \epsilon$ Then we set $\epsilon$ to some arbitrary value and try to get a contradiction. I also tried proving directly but couldn't get much out of it. Is there a better approach for proving it?","['limits', 'calculus']"
3937669,"How to solve recurrence relation $a_k = ba_{k−1} + cr^k$, assuming $b \neq r$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Solve for $a_k$ in terms of $a_0$ and the other parameters in the following recurrence relation: $a_k = ba_{k−1} + cr^k$ , assuming $b \neq r$ .","['recurrence-relations', 'discrete-mathematics']"
3937683,"Proof that $\int_0^\infty\frac{\ln x}{x^3 - 1} \, dx = \frac{4 \pi^2}{27}$","I realise this question was asked here , but I'm not able to work with any of the answers. The hint given by my professor is Integrate around the boundary of an indented sector of aperture $\frac{2 \pi}{3}$ but when I try that I can't figure out how to deal with the (divergent) integral along the radial line at angle $2 \pi / 3$ . My issue with the accepted answer is that it uses the residue theorem where it doesn't apply, at least as we've learned it, since $$z \mapsto \frac{\log^2z}{z^3 - 1}$$ has non-isolated singularities on the closed region bounded by the proposed contour (due to branch cuts), and I am not sure how to relate the integral along the real axis to one over a contour modified to avoid the branch cut. For a fixed $\varepsilon > 0$ , and for any $\delta \in (0, 1 - \varepsilon)$ , we could let $\log_{-\delta / 2}$ be the branch of the logarithmic function with a cut along the ray $\operatorname{arg}z = -\delta / 2$ and define a contour which goes along the positive real axis from $\varepsilon$ to $1 - \delta$ , a semicircle in the upper half plane of radius $\delta$ around $1$ , the positive real axis from $1 + \delta$ to $2$ , an arc of radius $2$ around $0$ with central angle $2 \pi - \delta$ , the ray $\operatorname{arg}z = 2 \pi - \delta$ from $r = 2$ to $r = \varepsilon$ , and finally an arc of radius $\varepsilon$ around $0$ back to $\varepsilon$ . But then, for example, I don't know how to calculate the limit of integral along the arc of radius $\varepsilon$ $$\lim_{\delta \to 0}\int_0^{2 \pi - \delta}\frac{\log_{-\delta / 2}^2(\varepsilon e^{i \theta})}{\varepsilon^3 e^{3 i \theta} - 1} \varepsilon i e^{i \theta} \, d\theta.$$ If I instead try to first use the substitution $x = e^u$ on the real integral and then compute a contour integral, I still get a divergent integral that I don't know how to handle, this time along the top of an indented rectangle.","['improper-integrals', 'definite-integrals', 'complex-analysis', 'contour-integration', 'multivalued-functions']"
3937692,"f is continuous on [0,1]. f(0) = f(1). Show that f'(1 - c) = f'(c)","Let $f$ be continuous on $[0,1]$ and differentiable in $(0,1)$ s.t. $f(0) = f(1)$ . Prove that $\exists c \ where \ 0 < c < 1 \ s.t. \ f'(1 - c) = f'(c)$ . Here is my attempt Since $f(0) = f(1)$ , Rolle's Theorem applies. This means $\exists c \in (0,1) \ s.t. \ f'(c) = 0$ Since $c \in (0,1)$ , we can see that $1 - c \in (0,1)$ . To show this is also $0$ , examine the original function. If $0 = 1 - c$ , then we see $c = 1$ . This will satisfy the inequality giving us $f(0) = f(1)$ . If $1 = 1 - c$ , then $c = 0$ . This will also satisfy the inequality giving us $f(1) = f(0)$ . So whether we use $c$ or $1 - c$ , Rolle's Theorem will be satisfied at both points. I was told this is incorrect, and I can see why. Where I try to show that $1 - c$ works as well, I am only given information about the function (f(0) = f(1)). Not the derivative. So I can see why this doesn't work. I was told as a hint to examine the function $g(x) = f(x) + f(1 - x)$ . However, I am not so sure how to apply this to the problem and proceed. Can anyone give me some guidance?","['rolles-theorem', 'derivatives', 'real-analysis']"
3937720,How to prove a matrix $[a_{ij}]$ with $a_{ji} = 1/a_{ij}$ has no complex eigenvalues?,"I've been given a matrix of the form $$\begin{pmatrix}
-1 & 1/a_{21} & \cdots & 1/a_{n1}
\\
a_{21} & -1 &\cdots & 1/a_{n2}
\\
\vdots & \vdots & \ddots & \vdots
\\
a_{n1} & a_{n2} & \cdots & -1
\end{pmatrix}$$ which satisfies $$
\sum_{j=1}^n a_{ij} = 0,\space\space\forall\space i
$$ and I've been asked to prove whether it can have complex eigenvalues when all its coefficients are real. I've been trying to throw properties of matrices and eigevalues to it for a week but I haven't gotten anything. For $n = 2$ , I know its eigenvalues are $0$ and $-2$ , its determinant is $0$ and its trace is $-2$ . For $n = 3$ it also has a $0$ determinant, but this time only after applying the extra property that the sum of all the coefficients of any column must be $0$ . I haven't been able to prove whether the determinant should be $0$ for any natural $n$ or not. I don't know if there's an easy way to prove this, maybe it's possible to prove they don't even without that extra constraint. Any hint or idea I could try would be very welcome!","['matrices', 'eigenvalues-eigenvectors']"
3937772,Upper Triangular Matrix with zeros up,"I have a linear algebra homework, but honestly I'vee been trying to solve this problem for some days without success: Suppose T $\in$ $M_{5 \times 5}$ (K) is a triangulable matrix. Be $\beta = \{ \vec{v_1}, \vec{v_2}, \vec{v_3}, \vec{v_4}, \vec{v_5} \}$ a base of $K^5$ such that: $$[T]_\beta = \begin{pmatrix}
4 & * & a & b & c \\
0 & 4 & d & e & f \\
0 & 0 & 5 & * & * \\
0 & 0 & 0 & 5 & * \\
0 & 0 & 0 & 0 & 5
\end{pmatrix}$$ And I must prove that exists a base $\beta'$ such that: $$[T]_{\beta'} = \begin{pmatrix}
4 & * & 0 & 0 & 0 \\
0 & 4 & 0 & 0 & 0 \\
0 & 0 & 5 & * & * \\
0 & 0 & 0 & 5 & * \\
0 & 0 & 0 & 0 & 5
\end{pmatrix}$$ How can I prove it? Thoughts: If a=b=c=d=e=f=0 means that there is nothing in the coordinates corresponding to the vectors related to 4, but I don't know how to prove that there is a basis with vectors that satisfy this.",['matrices']
3937778,How to find the side of the triangle?,"I need to find the value on the $v_{1}$ side of the triangle, the book's answer is $v_{1} = V cos(\psi) + v'_{1} cos (\theta - \psi)$ , but I couldn't understand how you get to that result. I used the cosine law but it didn't work, how can I solve it? The main exercise is about elastic collisions, in which I need to relate the initial kinetic energy and the final kinetic energy of the particle in the LAB framework : $$
\frac{T_{1}}{T_{0}}=\frac{m^2}{(m_{1}+m_{2})^2}.S^2
$$ $$
S= \cos(\psi)+\frac{\cos(\theta-\psi)}{(m_{1}/m_{2})}
$$ thereby $$
T_{0}=\frac{m_{1}u_{1}^2}{2}
$$ $$
T_{1}=\frac{m_{1}v_{1}^2}{2}
$$ $$
\frac{T_{1}}{T_{0}}=\frac{v_{1}^2}{u_{1}^2}
$$ and the book says that by the triangle diagram you get the relationship $v_{1} = V cos(\psi) + v'_{1} cos (\theta - \psi)$ The figure is the final state of mass $m_{1}$ for the elastic collision of two particles for the case $V<v'_{1}$ for which there is one trajectory. $V$ is the velocity of the center of mass in the LAB framework. That is question 9.38 from the book Classical Dynamics of Particles and Systems by Marion, Thornton.","['classical-mechanics', 'geometry', 'triangles', 'physics', 'trigonometry']"
3937820,What is meant by a flat morphism of relative dimension n?,"I am trying to learn about etale morphisms, and one definition that is given often is a smooth morphism of relative dimension $0$ . I know that relative dimension can be defined generally in the case of flat morphisms, but there seems to be some confusion in the literature about the definition. Can anyone tell me definitively which of the following is actually meant by ""relative dimension $n$ "": Every non-empty fiber has dimension $n$ , every non-empty fiber has pure dimension $n$ , every fiber has dimension $n$ , or every fiber has pure dimension $n$ . I have noticed some sources do say literally every fiber, but then a corollary of this is that any flat morphism is surjective, since otherwise some fibers would be empty. So it seems more likey that it is only referring to non-empty fibers.","['algebraic-geometry', 'schemes']"
3937842,Are the elements of a Lie algebra separated by its finite-dimensional representations?,"Let $\mathbb{k}$ be some field and let $\mathfrak{g}$ be a $\mathbb{k}$ -Lie algebra. Does there exist for every nonzero element $x$ of $\mathfrak{g}$ a finite-dimensional representation of $\mathfrak{g}$ on which $x$ acts nonzero? In other words, do the finite-dimensional representations of $\mathfrak{g}$ separate the elements of $\mathfrak{g}$ ? If $\mathfrak{g}$ is finite-dimensional, then this is true by Ado’s theorem. As pointed out in an answer to a similar question , the finite-dimensional representations of $\mathfrak{g}$ even separate the points in the universal enveloping algebra $\operatorname{U}(\mathfrak{g})$ if $\mathfrak{g}$ is finite-dimensional and $\mathbb{k}$ is of characteristic zero.
(This seems to be Theorem 2.5.7 in Dixmier’s Enveloping Algebras .) Both of the above arguments show even stronger assertions, but also need some additional assumptions and quite a bit of work.","['abstract-algebra', 'representation-theory', 'lie-algebras']"
3937860,"Prove that $\sum a_n$ converges iff $\sum 2^n a_{2^n}$ converges, but with another function instead of $2^{n}$.","Let $\{a_n\}$ be a positive, decreasing sequence of real numbers.
I know how to prove that $\sum_{n=1}^\infty a_n$ converges iff $\sum_{n=1}^\infty 2^n a_{2^n}$ converges, but is it possible that this is true for another function from naturals to naturals instead of $2^n$ ? In case there is no function why is it?","['convergence-divergence', 'soft-question', 'sequences-and-series', 'real-analysis']"
3937868,Want to show $\sum_{n>N}n^{-1/2} \ll N^{-1/2}$,"I feel like I can use the result $$\sum_{n=x}^N \frac{a_n}{n^s} = A(N)N^{-s} + s \int_x^N A(t)t^{-s-1}dt$$ where $s=1/2$ to verify the $\ll$ approximation. If I pick $A(n)=\sum_{x\leq t\leq n}\chi(t)$ , then I know $|A(n)|\ll 1$ . I think that will make the first term become $O(N^{-1/2})$ , but am not sure how I should go about handling the integral term. Thanks! Thanks for the help in advance!","['analytic-number-theory', 'number-theory', 'dirichlet-character', 'dirichlet-series']"
3937882,Proof that $S^1$ is a manifold using projections,"I'm trying to understand what seems like it should be a simple proof in Spivak's text on differential geometry. In order to show that the circle, $S^1$ , is indeed a manifold, he projects $S^1 \setminus \{(0,1)\}$ onto $\Bbb{R} \times \{-1\}$ , as described in this image: I can intuitively see how this gives a homeomorphism between $S^1 \setminus \{(0,1)\}$ and $\Bbb{R} \times \{-1\}$ . Next, he says that the point $(0,1)$ can be handled similarly by projecting it on $\Bbb{R} \times \{1\}$ . How does this projection work? Is the point $(0,1)$ just sent to itself? My bigger question is, how are these two arguments combined to show that $S^1$ is indeed a 1-manifold? The first part shows ""most"" of $S^1$ being homeomorphic to $\Bbb{R}^1,$ and the second part shows the ""rest"" of $S^1$ being homeomorphic to... something? Then what? For reference, here's the definition of manifold in the text:
A manifold is a metric space $M$ with the following property:
If $x \in M,$ then there is some neighborhood $U$ of $x$ and some integer $n \ge 0$ such that $U$ is homeomorphic to $\mathbb{R}^n.$","['manifolds', 'differential-geometry']"
3937906,Does every scheme admit an affine open subset whose sections are the ring global sections?,"The title basically says it all. Given an arbitrary scheme $X$ with global sections $\mathcal{O}_X(X)$ , is there an affine open subset $U$ of $X$ such that $\mathcal{O}_X(U) = \mathcal{O}_X(X)$ ? Obviously the scheme $Y=\operatorname{Spec}\mathcal{O}_X(X)$ has the same global sections as $X$ , but it's not clear to me that you can embed $Y$ in $X$ . I have been unable to produce a counter example though.","['affine-schemes', 'algebraic-geometry', 'schemes']"
3937982,"Partition 100 people, 4 from each country into 4 groups with conditions","This is a problem from the $2005$ All-Russian Olympiad. Problem is as follows: $100$ people from $25$ countries, four from each country, sit in a circle.
Prove that one may partition them onto $4$ groups in such way that no
two countrymen, nor two neighboring people in the circle, are in the
same group. My approach was to try and find an algorithm that can monotonically reduce the number of bad states. However, I haven't been able to figure out a way to do this for every possibility. Does anyone have any hints on how to approach this problem? I've tried to start with an arbitrary partition where each partition only has 1 person from each country, and preserve that with each move. I also claim that there is always a country for which not all of its representatives' neighbors are in a single partition. Proof idea: Suppose not. WLOG, say that country $1$ has all of its representatives' neighbors in the first partition. So if $1_i$ is the representative from country $1$ in partition $i$ , we have $1_1$ and its neighbor, WLOG from country $2$ , is also in group $1$ . So we have $1_1, 2_1$ . Then $2_1$ 's other neighbor must also be in partition $1$ or we are done. Continuing in this way, we find that all the representatives are in partition $1$ , contradiction. Using this, I try to decrease the number of ""bad"" pairs (i.e. neighbors or same country) in each partition monotonically. However, I haven't been able to figure out how to do so. In particular I'm stuck on cases like the following: country $1$ has its representatives in partitions $1-4$ s/t the neighbors of $1_2,1_3,1_4$ are all in the first partition. $1_1$ 's neighbors are in partition $1$ and another partition, say $2$ . Trying to swap the placements of country $1$ representatives seems to only increase number of ""bad"" pairs, and trying to swap the neighbors may also increase the number of bad pairs. Is there an observation that I'm missing here, or is this approach not the right way to go about it?","['contest-math', 'graph-theory', 'invariance', 'combinatorics', 'algorithms']"
3938007,Limit of $n^{\frac{1}{n}}$.,"We have to prove that $n^{\frac{1}{n}}$ converges to $1$ . I have proved it using the binomial theorem where we can substitute $(1+t)$ in place of $n$ and proceed forward. However along with the question another approach was mentioned where we can make use of the Monotone Convergence Theorem. For this approach, I proved that $a_n=\left(1+\frac{1}{n}\right)^n$ is increasing and using $a_n<a_{n+1}$ I proved that $n^{\frac{1}{n}}$ is decreasing after the third term. Also it is bounded as all terms are greater than $0$ . So by MCT it should be convergent. But I am not able to evaluate the limit and am only able to prove its existence here. Please help.",['real-analysis']
3938027,Can someone explain this “visual proof” of $(1 + 2 + 3 +…+ n)^2 = 1^3 + 2^3 + 3^3 +…+ n^3$?,"I see a lot of people saying this visual proof is beautiful and I really want to be able to understand it. If anyone could help me out, I'd really appreciate it! Thanks in advance!","['number-theory', 'proof-without-words']"
3938054,How to evaluate $\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$,"I want to evaluate $$\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$$ But I've not been successful in doing so, what I tried is $$\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx=\int _0^1\left(x\right)'\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$$ $$\overset{\operatorname{IBP}}=\ln \left(\zeta (2)\right)+\int _0^1\frac{\ln \left(1-x\right)}{\operatorname{Li}_2\left(x\right)}\:dx$$ I also tried using identities for the dilogarithm but they dont do much. I'm not sure what to do now, any help will be very well regarded, thank you.","['integration', 'definite-integrals', 'logarithms', 'polylogarithm', 'riemann-zeta']"
3938078,Intuition for the Lax-Milgram Theorem,"I have seen the proof of the Lax-Milgram theorem (and can replicate it with most of the details), I've also applied it for a bi-linear form defined as the weak formulation of a PDE to show the existence of a solution. I still don't get the idea behind it. Is there an equivalent in finite dimensional linear algebra to give me some intuition? Lax milgram says: $a:H \times H \rightarrow \mathbb{R}$ continuous, coercive, symmetric, then: $\forall \phi \in H^{\ast}, \exists ! u \in H$ such that $a(u,v) = \phi(v)$ for all $v \in H$ . This $u$ is the only minimiser of $H$ of the problem $$ \inf_{v \in H}\left\{\frac{1}{2}a(v,v)-\phi(v)\right\}$$ More concretely, my questions are: What is (1), some kind of dimension reduction? How did (2) come about? It's so useful in the context of PDEs but its mysterious to me. What's the finite dimensional intuition I can draw from? EDIT: I had some time to think about this. Take $H:=\mathbb{R}^n$ . In this simple finite dimensional context, $a(u,v) := \langle u,v\rangle$ is the natural choice for a continuous, coercive, and symmetric bilinear form. Then the dual is the space of all row vectors. So Lax-Milgram tells us that $\forall r \in (\mathbb{R}^n)^{\ast}$ there exists a unique $u \in \mathbb{R}^n$ such that $\langle u,v\rangle = r v$ for all $v \in \mathbb{R}$ , i.e. the only one is $r=u^T$ . It just tells us a different way to write the dot product using elements in the dual. As for the minimisation problem, Lax-Milgram says that the solution to this minimisation problem would be $$\frac{1}{2} \langle u, u \rangle - \langle u,u\rangle = -\frac{1}{2}\|u\|^2$$ I am at a loss for what this represents, but I now have some intuition for the first result! Hopefully someone can comment on this minimisation.",['functional-analysis']
3938082,Showing a solution to this differential equation is never zero,"Say I want to find a differentiable function $f: \mathbb{R} \rightarrow \mathbb{R}$ such that for some $k \neq 0$ , $f' = kf$ . Every time I have seen a book solve this, the expression is rewritten  as $$\frac{1}{f}f' = k$$ and then solved using u-substitution. To be technical though, must one first know that $f$ is never zero? If so, how could I go about showing that $f$ is never zero? I know that $f$ must be infinitely differentiable and if for some $a \in \mathbb{R}$ , $f(a) = 0$ , then for each $n \in \mathbb{N}_0$ , $f^{(n)}(a) = 0$ . I think I remember from complex analysis, if $f$ is holomorphic with this property, then $f$ is zero everywhere, but I'm not sure about the usual case of $\mathbb{R} \rightarrow \mathbb{R}$ .","['calculus', 'ordinary-differential-equations', 'real-analysis']"
3938150,Is my method of solving equation correct?,"The problem in question is $$\sqrt[5]{16+\sqrt{x}}+\sqrt[5]{16-\sqrt{x}}=2$$ using $$a+b=2$$ where $a=\sqrt[5]{16+\sqrt{x}}$ and $b=\sqrt[5]{16-\sqrt{x}}$ $$(a+b)^5=32$$ $$(a+b)^2(a+b)^3=32$$ $$a^5+5a^4b+10a^3b^2+10a^2b^3+5ab^4+b^5=32$$ $$a^5+b^5+5ab(a^3+b^3)+10a^2b^2(a+b)=32$$ $$a^5+b^5+5ab\biggl(\frac{32}{(a+b)^2}\biggr)+10a^2b^2(a+b)=32$$ Got $\frac{32}{(a+b)^2}$ from the fact that $(a+b)^2(a+b)^3=32$ and $a+b=2$ $$a^5+b^5+5ab\biggl(\frac{32}{(2)^2}\biggr)+10a^2b^2(2)=32$$ $$a^5+b^5+40ab+20a^2b^2=32$$ From when I defined a and b earlier, I substitute and get $$\left(\sqrt[5]{16+\sqrt{x}}\right)^5+\left(\sqrt[5]{16-\sqrt{x}}\right)^5+40\sqrt[5]{\left(16-\sqrt{x}\right)\left(16+\sqrt{x}\right)}+20\sqrt[5]{\left(16-\sqrt{x}\right)^2\left(16+\sqrt{x}\right)^2}=32$$ $$\require{cancel}\cancel{16}\cancel{+\sqrt{x}}+\cancel{16}\cancel{-\sqrt{x}}+40\sqrt[5]{256-x}+20\sqrt[5]{\left(256-x\right)\left(256-x\right)}=\cancel{32} 0$$ $$40\sqrt[5]{256-x}+20\sqrt[5]{\left(256-x\right)\left(256-x\right)}=0$$ $$20\biggl(2\sqrt[5]{256-x}+\sqrt[5]{\left(256-x\right)\left(256-x\right)\biggr)}=0$$ Then let $u=\sqrt[5]{256+{x}}$ , $$20(2u+u^2)=0$$ $$u(u+2)=0$$ $$u=0,-2$$ Substituting u to get x from $u=\sqrt[5]{256+{x}}$ , I get $$x=\cancel{-288},256$$ However, since the original equation has a $\sqrt{x}$ , which can't be negative, I eliminate $x=-288$ , leaving just $$x=256$$ as my answer. So, this is how I arrived on my answer. Did I perform any mathematical errors or any illegal mathematical maneuvers? Please let me know.
Thank you!","['radicals', 'algebra-precalculus', 'problem-solving']"
3938162,Ito formula to asian option,"I was reading ""2001The Integral of Geometric Brownian Motion"" am getting confused whether $A_t^{(\mu)}=\int_0^t e^{2 \mu \tau + 2 B_\tau} d\tau$ is a function of $t$ only or a function of $t$ and $B_t$ , where $B_t$ is a standard Brownian motion. If the latter is the case, what is partial derivative of $A_t^{(\mu)}$ with respect to $B_t$ ? I suppose the author consider this as a function of $t$ only based on Eq.(2.7) and Eq.(2.8). My understanding is that stochastic differential equation only makes sense in the integral form. Since $A_0^{(\mu)}=0$ , we have $$dA_t^{(\mu)}=e^{2 \mu t + 2 B_t} dt=e^{2 \mu t + 2 B_t} dt+0 dB_t.$$ Can I interpret this as $$\frac{\partial^n A_t^{(\mu)}}{\partial B_t^n}=0$$ for $n \in \mathbb{N}$ ? However, from the second mean value theorem: if $f$ is integrable on $[a,b]$ and $g$ is monotone, then there exists $\eta \in [a,b]$ such that $$\int_a^bf(x)g(x)dx=g(a)\int_a^\eta f(x)dx+g(b)\int_\eta^bf(x)dx.$$ It reminds me that the integral could be represented as a function of integrand at $b$ for some $g$ . Do we have other results for g not monotone? Looking for your help.","['stochastic-integrals', 'stochastic-pde', 'derivatives', 'stochastic-calculus']"
3938269,"Prove $1^r+2^r+.....\equiv 0\pmod p$ , given that p is odd prime and $0<r<p-1$","An exercise reads as follows:
If $p$ is an odd prime and $0<r<p-1$ then prove that $s=1^r+2^r+...(p-1)^r\equiv 0\mod p$ I know one clever way to prove it is to use a primitive root $g$ and write the given sum as: $g^r+g^{2r}+g^{3r} +...+g^{(p-1)r}$ which is a geometric process easy to handle and deduce the result. But I am looking for a more primitive way right now (suppose I know nothing about primitive roots). My thought is: the polynomial $$f=x^{p-1}-1$$ has $p-1$ roots distinct roots in the field $\Bbb{Z_p}$ so one can also write $$f= (x-1)(x-2)(x-3)...(x-(p-1))$$ . By inspection of the coefficients of the 2 equal polynomials we get $t_1=1+2+3+...+(p-1)\equiv0 \mod p$ $t_2=1\cdot 2+1\cdot 3+...+(p-2)\cdot( p-1)\equiv 0 \mod p$ $t_3=1\cdot 2\cdot3+1\cdot 2\cdot4+...+(p-3)\cdot (p-2)\cdot(p-1)\equiv 0\mod p$ and so on .... now suppose $r=2$ then $s=t^2_1 -2t_2$ , where we deduce that $p$ divides $s$ But how in general to compute $s$ by the $t_i$ mentioned above? Is there an algorithm for that? Thanks a lot.","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3938400,Computing the Hardy-Ramanujan asymptotic formula using method of steepest descent/saddle point method,"I am trying to obtain and prove the Hardy-Ramanujan asymptotic approximation formula given by: $$p(n) \sim \frac{1}{4n\sqrt{3}}e^{\pi\sqrt{\frac{2n}{3}}},$$ by using Dedekind's eta function $$\eta(z)=e^{\frac{i\pi z}{12}}\prod_{n=1}^{\infty}(1-e^{2\pi inz})=\prod_{n=1}^{\infty}\frac{1}{1-z^{n}}$$ and the method of steepest descent (or saddle point method). I am stuck on formulating the integral to which this method can be applied. I understand that we can use the generating function: $$f(z)=1+\sum_{n=1}^{\infty}p(n)z^{n}$$ to represent $p(n)$ as the integral $$p(n)=\frac{1}{2\pi i}\int_{C}\frac{f(z)}{z^{n+1}} \, dz$$ around a closed path $C$ entirely within the unit circle enclosing the origin. Using Dedekind's eta function gives me that $$f(z)=e^{\frac{i\pi z}{12}}(\eta(z))^{-1}.$$ Since $\eta(-\frac{1}{z})=\sqrt{\frac{z}{i}}\eta(z)$ I have that $$f(z)=\sqrt{\frac{z}{i}}e^{\frac{i\pi}{12z}}e^{\frac{i\pi z}{12}}f\left(-\frac{1}{z}\right).$$ However, if $z$ is restricted appropriately and $z \to 0$ , then $\Im(-\frac{1}{z}) \to \infty$ implies that $f(-\frac{1}{z}) \to 1$ since $f(z)=1+\mathcal{O}(e^{-2\pi y})$ , where $z=x+iy$ , $y \geq 1$ . So now the integral I am interested in is $$p_{1}(n)=\int_{\gamma}\sqrt{\frac{z}{i}}e^{\frac{i\pi}{12z}}e^{\frac{i\pi z}{12}}e^{-2\pi inz} \, dz.$$ Is this the correct integral to consider? How do I now apply the method of steepest descent/saddle point method to obtain the Hardy-Ramanujan asymptotic approximation formula?","['integer-partitions', 'laplace-method', 'number-theory', 'complex-analysis', 'combinatorics']"
3938405,Differential of the product smooth map from arbitrary manifold,"Suppose $f:M\to N\times P$ defined by $f(x)=(f_1(x),f_2(x))$ where $f_1:M\to N$ and $f_2:M\to P$ are smooth maps. Then show that $df=df_1\times df_2$ i.e. $df(X)=(df_1(X), df_2(X))$ . ( $df$ is the differential of $f$ & $M,N,P$ are smooth manifolds) I was trying by the formula $df(X)(g)=X(g\circ f)$ where $X$ is a tangent vector and $g$ is a real valued function. But did not able to complete the proof. Please help to do this.","['manifolds', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3938483,$O$ is not dense in $\mathbb R^n$,"$A$ is a matrix of order $n$ , and $x$ is an $n$ -dimensional column vector. The set $$
O=\{x,Ax,A^2x,\cdots,A^mx,\cdots\}
$$ Prove that, $O$ is not dense in $\mathbb R^n$ . This claim seems to be trivial: it just looks like $\mathbb N$ in $\mathbb R$ . I recalled how we verify the denseness of $\mathbb Q$ in $\mathbb R$ , but it seeems that the same method doesn't work. Can anyone help?","['matrices', 'linear-algebra', 'vector-spaces']"
3938509,"$\underset{x\to 1}{\text{lim}}\int_0^x \frac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \, \mathrm dt=\frac{ \pi }{\sqrt{2}}$","Define $f(x)=\dfrac{x+1}{(x-1)^2}$ . Prove $\lim\limits_{x \to
 1}\displaystyle\int_0^x \dfrac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \,
 {\rm d}t=\dfrac{\pi }{\sqrt{2}}$ . We can obtain $$\lim_{x \to 1}\int_0^x\frac{\sqrt{t}f(t)}{\sqrt{f(x)-f(t)}}{\rm d}t=\lim_{x\to 1}\int_0^1\frac{x\sqrt{xu}f(xu)}{\sqrt{f(x)-f(xu)}}{\rm d}u,$$ but how to go on ?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'limits']"
3938521,Count number of maps from $2$ object set to $S_n$ up to equivalence,"I need to find a way to count the number of maps from $\{0,1\}$ to $S_{n}$ the symmetric group on n symbols, where 2 maps $f,g$ are equivalent if $\exists \sigma \in S_{n}$ , $\sigma f \sigma^{-1} = g$ (that is, $\sigma f(0) \sigma^{-1} = g(0)$ and $\sigma f(1) \sigma^{-1} = g(1)$ . specifically im interested in the case $n=3$ but if there is some generalization for $n$ and maybe a set larger than $\{0,1\}$ I would love to hear it. What I tried: I was thinking that it was enough to count the number of maps from $\{0,1\}$ to the set of conjugacy classes of $S_{n}$ , but obviously the condition imposed is slightly stronger, since it isnt enough to ask that $g(0)$ is conjugate to $f(0)$ and same for $f(1),g(1)$ . Perhaps there's a way to slightly modify this argument?","['elementary-set-theory', 'group-theory', 'combinatorics', 'permutations']"
3938527,Why is this function not differentiable?,"Why is the function $$\begin{align}
f(x,y)=\begin{cases}\frac{y^3}{x^2+y^2},&(x,y)\ne(0,0)\\\\
0,&(x,y)=(0,0)
\end{cases}
\end{align}$$ not differentiable at $(0,0)$ ? By setting $x=0$ or $y=0$ I would get $f_x(0,0)=0$ , $f_y(0,0)=-2$ and when I look at the 3-D Surface plot, $f(x,y)$ looks quite ""normal"". I gaze already hours on it and cannot identify the problem. Obviously the partial derivates exist, but are not continuous, but I cannot see where. Is there an analytic way, I can obtain the result from? How can I come to the conclusion, without gazing at the curve?","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
3938632,"A quadrilateral $ABCD$ with $AB = 10$ , $BC = 6$ , $CD = 8$ and $DA = 2$ has diagonals $AC$ and $BD$ meeting at $O$.","A quadrilateral $ABCD$ with $AB = 10$ , $BC = 6$ , $CD = 8$ and $DA = 2$ has diagonals $AC$ and $BD$ meeting at $O$ , such that $\angle COB = 45^\circ$ . Find the area of $ABCD$ . What I Tried : Here is a picture :- The only thing missing to find the area is one of the diagonals, and I can find the area easily by Heron's Formula. Also I thought that since some angles are there, I might have to use trigonometry, and I tried but without success. We have that $\angle DOC = \angle AOB = 135^\circ$ . I know that the area of a triangle can be found from the formula :- $$\rightarrow \frac{1}{2}ab\sin \gamma$$ Where $\gamma$ is the angle between $a$ and $b$ , but I cannot use it sadly as I don't know any of the lengths of $AO$ , $BO$ , $CO$ and $DO$ . Next I thought of the formula :- $$\rightarrow \frac{a}{\sin a} = \frac{b}{\sin b} = \frac{c}{\sin c}$$ But this dosen't work here either due to the same reason. I think that Law of Cosines might work, but I am actually new to Trigonometry and not sure if it will really work or not, and then I have to consider $4$ equations for $4$ sides, which will be complicated. Can anyone help me? Thank You. Note :- Solutions without Trigonometry and mostly welcome. I hate using Trigonometry.","['trigonometry', 'problem-solving', 'geometry', 'polygons']"
3938643,"Find all the cosets of $H$ in $G$ , where $G = \{ z\in \mathbb C \mid z \neq 0 \}$ , $H = \{ z \in\mathbb C \mid z^n = 1 , n\in\mathbb N \}$","Let, $G = \{z\in\mathbb C \mid z \neq 0\}$ , $H = \{z\in\mathbb C \mid z^n = 1 , n\in\mathbb N\}$ . Find the cosets of $H$ in $G$ . I have got the pictorial idea that how the cosets will look like concentric circle in complex plane, but I could not understant how to express those cosets and how to solve this problem in a rigorous way.","['group-theory', 'abstract-algebra']"
3938707,Fano surface vs. del Pezzo surface,"This article from wikipedia defines a Fano variety as a complete variety whose anticanonical bundle is ample. It also states that: A Fano surface is also called a del Pezzo surface. Every del Pezzo surface is isomorphic to either P1 × P1 or to the projective plane blown up in at most 8 points, which must be in general position. As a result, they are all rational. After looking it up, I've found that some authors define a del Pezzo surface as either $\Bbb{P}^1\times\Bbb{P}^1$ or the blowup of $\Bbb{P}^2$ at $8$ points at most, while others define it as a projective surface with ample anticanonical divisor (i.e. Fano, as in wikipedia). This appears to be an equivalence, and I'm trying to understand it. I was able to prove that $-K_{\Bbb{P}^1\times\Bbb{P}^1}$ is ample, and if $S$ is the blowup of $r$ points ( $1\leq r \leq 8$ ) I can show that $(-K_S)^2=9-r>0$ . This last result is expected from an ample divisor, but I still can't prove that $-K_S$ is ample. Conversely, I still can't see how $-K_S$ being ample implies it must be some blowup of $r\leq 8$ points. How do I do that? Any help will be appreciated, thank you!","['algebraic-geometry', 'blowup', 'surfaces', 'line-bundles']"
3938716,Is $\limsup_{t} \frac{a_t}{ h(t+1) - h(t) }$ finite when $0 \leq \sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha}$?,"Consider a monotone decreasing sequence $\{a_i\}_{i=1}^\infty$ , where $a_i  \geq 0$ for all $i$ . This sequence is assumed to satisfy the bound on its residual series $$
\sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha} \;\;\;\forall t\in\mathbb{N}
\;,
$$ where $\alpha \geq 1$ . From the above bound, it is clear that $\{a_i\}$ is summable since we have $\sum_{i=1}^\infty a_i \leq 1 < \infty$ . My objective is to determine whether $$\limsup_{i\rightarrow \infty} \frac{a_i}{-\Delta h_i} < \infty \,,$$ where $\Delta h_i = h(i+1) - h(i)$ . Thus far, I have only been able to prove the weaker property that $$ \liminf_{i\rightarrow\infty}  \frac{a_i}{-\Delta h_i} \leq 1 \;,$$ which is done by showing that $a_i \leq -\Delta h_i$ is true for infinitely many $i$ (see below for proof). I have found that the answer may be related to the Stolz-Cesaro Theorem or its converse , but I have been unable to make the exact requirements of the theorem line up with what I need. I have also found that this may be connected to the following post . Any help on this would be appreciated. I am also curious about the more general case when we have some convex function $h:(0,\infty)\rightarrow (0,\infty)$ satisfying $\lim_{i\rightarrow\infty} h(i) = 0$ . Proof that $a_i \leq -\Delta h_i$ for infinitely many $i$ : Assume the converse for all but finitely many $i$ . Then there exists a $T\in\mathbb{N}$ such that for all $t \geq T$ , $$
\sum_{i=t}^\infty a_i > - \sum_{i=t}^\infty \Delta h_i = h(t)
\;,
$$ which is a contradiction. Hence we must have that $a_i \leq -\Delta h_i$ holds infinitely often.","['sequences-and-series', 'upper-lower-bounds', 'real-analysis']"
3938775,How to prove this asymptotic behavior of the expectation of the maximum of the independent standard normal variables?,"How to prove this famous theorem? If $G_1,G_2,...,G_n$ are independent standard normal random variables( $\mathcal{N}(0,1)$ ), then: $$\lim_{n\rightarrow\infty}\frac{\mathbb{E}[\max_{i=1,2,...,n}G_i]}{\sqrt{2\log n}}=1$$ I know the first step: $\Psi_G(\lambda)=\mathbb{E}[e^{\lambda G}]=\frac{\lambda^2}{2}$ then according to the Jensen's inequality, for all $\lambda>0$ \begin{equation*}
\begin{split}
\exp(\lambda\mathbb{E}[\max_{i=1,2,...,n}G_i]&\leq\mathbb{E}[\exp(\lambda\max_{i=1,2,...,n}G_i)]\\
&=\mathbb{E}[\max_{i=1,2,...,n}\exp(\lambda G_i)]\\
&\leq \sum_{i=1}^n\mathbb{E}[\exp(\lambda G_i)]\leq ne^{\frac{\lambda^2}{2}}
\end{split}
\end{equation*} then $$\mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\frac{\log n}{\lambda}+\frac{\lambda}{2}$$ the right term is minimised with $\lambda=\sqrt{2\log n}$ we have $$\mathbb{E}[\max_{i=1,2,...,n}G_i]\leq\sqrt{2\log n}$$ What's the next step?","['statistics', 'probability-distributions', 'integral-inequality', 'probability']"
3938818,foliation of a globally hyperbolic spacetime by Cauchy hypersurfaces,"While studying semi-Riemannian geometry I thought: There is a globally hyperbolic spacetime $\zeta^{3,1}:=\zeta^{1,0}\times \zeta^{1,0} \times \zeta^{1,1}$ , where $\zeta^{1,0}\simeq \Bbb R^{1,0},$ and $f:\Bbb R^{1,0}\to \zeta^{1,0}$ via $f(x)=e^x.$ I'm interested in defining a foliation of Cauchy surfaces of $\big(\zeta^{3,1},g\big).$ Needed is a smooth Cauchy temporal function (the gradient is everywhere timelike, not just causal, and each level set is a Cauchy surface that is necessarily spacelike). What is a foliation of $\big(\zeta^{3,1},g\big)$ by Cauchy surfaces? The metric of $\zeta^{1,1}$ is $g=\frac{dxdy}{xy}.$ The metric of $\zeta^{3,1}$ is $g=\frac{dudv}{uv}-\frac{dr^2}{r^2}-\frac{dw^2}{w^2}.$ For $\zeta^{1,1}$ I have $\textbf{Grad}\big(f(x,y)\big)=\textbf{Grad}\big(\frac{x^2}{2}\ln x - \frac{x^2}{4} - \frac{y^2}{2}\ln y + \frac{y^2}{4} + C\big)=\big\langle x\ln x,-y \ln y \big\rangle$ for some constant $C.$ This I think gives the projection of the vector field from the gradient function onto the x-y plane in $(0,1)^2.$ Then the integral curves of this vector field foliate $\zeta^{1,1}.$ I'm stuck on how to generalize this to higher dimensions. related: extrapolate vector 3-flow in $(0,1)^3$ from boundary vector flows","['foliations', 'semi-riemannian-geometry', 'vector-analysis', 'mathematical-physics', 'differential-geometry']"
3938868,Discuss Series of funtion is Uniformly convergent or not,"Discuss the uniform convergence of this series over $\mathbb R$ : $$\sum_{n=1}^\infty\frac{(-1)^n x^n}{n(1+x^n)}.$$ For $0<x\leq 1$ , $\frac{x^n}{n(1+x^n)}$ is monotonically decreasing and coverges to zero as $n$ tends to $\infty$ . Similarly for $x > 1$ , $\frac{x^n}{n(1+x^n)}$ monotonically decreses for $x$ and converges to zero. In both case they are bounded uniformly between $0$ and $1$ , and $\sum_n{(-1)^n}$ oscillates finately between $0,1$ as $n$ is even or odd, by Abel Test we conclude that series is Uniformly convergence over $(0,+\infty)$ . Doubt what about when $x \leq 0$ , I am confused as I am getting alternating series i.e. $$\frac{-x}{1+x}$ + $\frac{x^2}{2(1+x^2)}$-$\frac{x^3}{3(1+x^³)}\dots$$ Here as $x$ tends to $-1$ , denominator turns $0$ . Please help.","['functions', 'uniform-convergence', 'sequences-and-series']"
3938933,Finding the probability that at most n events take place on any interval during a given time period,"I am interested in the general case, but let us start with a smaller example. Suppose that cars arrive to a street with intensity $\lambda$ per minute. We would like to know the probability that at least two cars have arrived on the street during any five minute interval in the next hour. How can we find this? My initial thought was to use complementary event: 1 - the probability that at most one car is on the street in any five minute period during the next hour. Hence my reasoning was something like $1 - \int_0^{55}\mathbb{P}(N(s + 5) - N(s) \leq 1)ds - \int_{55}^{60}\mathbb{P}(N(60) - N(55 + s) \leq 1)ds$ . But I quickly realized that finding this probability might not be that easy, since the intervals we are considering overlap, namely $N(5)$ and $N(5 + s)$ overlap except for the infinitesimal point $s$ . So then, is the correct way to integrate just $\mathbb{P}(N(s) \leq 1$ over the region? Moreover, I think that my line of reasoning is missing something critical, since I do not see a reason, why the summation would not end up being negative.","['conditional-probability', 'probability-distributions', 'poisson-process', 'probability']"
3938946,"Construction of a $C^1$ function, which is (locally) larger than a given continous function","Let $u: \mathbb{R}^n \rightarrow \mathbb{R}$ be a continous function which is differentiable at the point $x_o \in \mathbb{R^n}$ . Find a function $v \in C^1(\mathbb{R^n})$ such that $v(x_o)=u(x_o)$ , $\nabla v(x_o)=\nabla u(x_o)$ and $g=u-v$ has a strict (local) maximum at $x_o$ . I am not aware of any extension theorems I could apply to this problem Would appreciate any help/hints.","['extreme-value-analysis', 'functions', 'analysis', 'real-analysis']"
3938963,Interpretation of a certain general theorem used by Gauss in his work on theta functions.,"I'm trying to understand the meaning of a general proposition stated by Gauss in a posthomous paper (this paper is in pp. 470-481 of volume 3 of Gauss's werke) on theta functions, a proposition which seems to serve as a guiding and organizing principle of the vast amount of relations among theta functions that he found. Gauss's notation and definitions Denote by $P(x,y),Q(x,y),R(x,y)$ the following functions: $$P(x,y)=1+x(y+\frac{1}{y})+x^4(y^2+\frac{1}{y^2})+x^9(y^3+\frac{1}{y^3})+...$$ $$Q(x,y)= 1-x(y+\frac{1}{y})+x^4(y^2+\frac{1}{y^2})-x^9(y^3+\frac{1}{y^3})+...$$ $$R(x,y)=x^{\frac{1}{4}}(y^{\frac{1}{2}}+y^{-\frac{1}{2}})+x^{\frac{9}{4}}(y^{\frac{3}{2}}+y^{-\frac{3}{2}})+x^{\frac{25}{4}}(y^{\frac{5}{2}}+y^{-\frac{5}{2}})+...$$ These functions include Jacobi theta functions in their usual meaning as special cases; if $y$ is a complex number whose absolute value is $1$ , and $z$ is defined to be a real number such that $y = e^{2iz}$ , then we have: $$P(x,y)=1+2cos(2z)x+2cos(4z)x^4+2cos(6z)x^9+...=\vartheta_3(z,x)$$ which follows from the identity $cos(2nz)= \frac{e^{2inz}+e^{-2inz}}{2}$ . In paticular, we have: $$P(x,1)=1+2x+2x^4+2x^9+...=\vartheta_3(0,x)$$ ,
So one can understand $P(x,y),Q(x,y),R(x,y)$ as a generalization of Jacobi theta function $\vartheta(z,x)$ from purely real $z$ to a complex $z$ (non-zero imaginary part of z), so that $|y| \ne 1$ . Remark: I'm not very familiar with Jacobi's publications, so it's quite possible that Jacobi's original definition of his theta functions includes also the case when $z$ is complex, so Gauss's functions $P(x,y),Q(x,y),R(x,y)$ are nothing else than simply Jacobi's theta functions with different notation. Gauss's theorem On August 6, 1827, Gauss stated the following ""general theorem"": $$P(x,ty)\cdot P(x,\frac{y}{t}) = P(x^2,t^2)P(x^2,y^2) + R(x^2,t^2)R(x^2,y^2) $$ and then goes on to derive a multitude of relations from it. For more comprehensive background on this question, please look at the answer to HSM stackexchange post https://hsm.stackexchange.com/questions/6256/did-gauss-know-jacobis-four-squares-theorem . Therefore, i'd like to know how to interpret the general theorem stated by Gauss.","['complex-analysis', 'theta-functions', 'math-history', 'sequences-and-series']"
3938980,Understanding the definition of primitive recursion.,"I'm a little (truthfully really) lost with the definition of primitive recursion. Here is the definition for primitive recursive functions that we have in our course (translated from German): The class of primitive recursive functions is the smallest class of functions of $\mathbb{N}^l \to \mathbb{N}, l \geq 0$ , which: contains the following basic functions: a. The constant function $f : \mathbb{N}^k \to \mathbb{N}, f(n_1, ..., n_k) = c$ ; b. The projection $\pi_i^k : \mathbb{N}^k \to \mathbb{N}, \pi_i^k(n_1, ..., n_k) = n_i$ ; c. The successor function $\text{succ}:\mathbb{N} \to \mathbb{N}, \text{succ}(n) = n+1$ ; And is closed under the following operations: a. Composition: If $f_1, ..., f_m : \mathbb{N}^k \to \mathbb{N}, g : \mathbb{N}^m \to \mathbb{N}$ are primitive recursive, then $g \circ (f_1, ..., f_m). \mathbb{N}^k \to \mathbb{N}$ , $(n_1, ..., n_k) \mapsto g(f_1(n_1, ..., n_k), ..., f_m(n_1, ..., n_k))$ is also primitive recursive; b. Primitive Recursive: If $g : \mathbb{N}^k \to \mathbb{N}, h : \mathbb{N}^{k+2} \to \mathbb{N}$ are primitive recursive then so if $f : \mathbb{N}^{k+1} \to \mathbb{N}$ with $f(0, n_1, ..., n_k) = g(n_1, ..., n_k)$ and $f(n + 1, n_1, ..., n_k) = h(n, f(n,n_1,...,n_k),n_1,...,n_k)$ . I don't understand what is happening in definition 2.b. Why do $f,g,h$ have different domains? What does $f(n + 1, n_1, ..., n_k) = h(n, f(n,n_1,...,n_k),n_1,...,n_k)$ do? What is the goal with this method?","['computer-science', 'logic', 'recursion', 'discrete-mathematics', 'computability']"
3939017,Is the matrix $A − 2I$ invertible?,"Let $A$ be a $4 \times 4$ matrix with eigenvalues $1, 2, 3,$ and $4$ . Is matrix $A − 2I_4$ invertible? I tried to tackle this by constructing a matrix with eigenvalues $1, 2, 3,$ and $4$ : $$
\begin{bmatrix} 
1 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 3 & 0 \\
0 & 0 & 0 & 4 \\
\end{bmatrix}
\quad
$$ Now, $$A − 2I_4 =
\begin{bmatrix} 
-1 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 2 \\
\end{bmatrix}$$ Since the determinant of this matrix is $0$ , it is not invertible. But this is a very specific case. How can I generalize it to all $4 \times 4$ matrices with eigenvalues $1, 2, 3,$ and $4$ ? Any help is greatly appreciated!","['matrices', 'determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
3939022,"Which comes first?, the distribution $P_X$ or the measure $\text{Prob}$?","I'm currently re-learning probability through measure theory, and I came across these definitions: A probability space is a measurable space $(\Omega, \mathcal{F})$ with a $\sigma$ -additive measure $\text{Prob}\colon\mathcal{F}\to[0,+\infty)$ that obeys $\text{Prob}(\Omega) = 1$ . The distribution of a random variable $X\colon\Omega\to\mathbb{R}$ is the measure in $\mathbb{R}$ given by $P_X(A) = \text{Prob}(X\in A)$ . However, in practice, most probability calculations arise in the context of random variables. If we have e.g. $X \sim \text{Poisson}(\lambda)$ , then we usually do computations like $\text{Prob}(X = k)$ by using the distribution of $X$ (and the fact that it has a density). $$\text{Prob}(X=k) = p_X(k) = e^{-\lambda}\frac{\lambda^k}{k!}$$ My question is: what is $\text{Prob}$ here? (knowing that the sample space is $\Omega = \mathbb{N}_0$ ), it feels as if we are using the distributions of random variables to define $\text{Prob}$ instead of computing it. We almost never specify the sample space when we say things like $X\sim \text{Poisson}(\lambda)$ , or any distribution for that matter. Is there a standard for choosing e.g. the counting measure in finite spaces and some other measure for the continuous case?",['probability-theory']
3939096,Exercise 6.17 from Isaac's Character Theory of Finite Groups.,"I am trying to solve the following problem in Isaac's Character Theory of Finite Groups and I am really sorry if this is a very simple question. 6.17: Let $N$ be a normal subgroup of $G$ with $G/N$ cyclic. Let $\eta\in \operatorname{Irr}(N)$ be invariant in $G$ and assume that $(\eta(1),|G:N|)=1.$ Show that $\eta$ is extendible to $G.$ Q1: The hint is to show that $G/\ker(\det\eta)$ is abelian. But I don't know how to prove the hint or how to solve the problem using it. I think proving that $\det \eta$ would be enough but I don't know how to do that. Q2: There's a note right below the problem saying that this is true even without the assumption $(\eta(1),|G:N|)=1.$ Could someone please give me a hint for this? My only experience in Character Theory is reading up to chapter 6 in Isaac's book so I'm not sure if I know enough material to solve this without this assumption. Thank you!!","['group-theory', 'finite-groups', 'characters']"
3939165,What are some examples of incomplete nonextendible manifolds?,"A connected Riemannian manifold $(M,g)$ is said to be extendible if it is isometric
to a proper open subset of a larger connected Riemannian manifold (this is the definition given in Lee's Introduction to Riemannian manifolds ). I was able to show that a complete Riemannian manifold is nonextendible. I would like to find an example of incomplete Riemannian manifold which is not extendible. According to Wikipedia such manifolds exist, but no reference is given. What are some (relatively simple) examples of connected incomplete nonextendible Riemannian manifolds ? I tried looking at the case of the real numbers $\mathbf{R}$ , without great success. More specifically I tried to find conditions on a metric on $\mathbf{R}$ to be extendible. Suppose that there is a metric $g$ on $\mathbf{R}$ and an isometric embedding $\varphi:(\mathbf{R},g)\hookrightarrow (M,g_0)$ with $M$ a connected 1-manifold and with $\varphi(\mathbf{R})\subsetneq M$ . For example we can look at the case $M=\mathbf{R}$ and $\varphi(\mathbf{R})=(-\infty,0)$ with $\varphi$ increasing. Writte $g_0 = f(x)dx^2,~f>0$ . Then, $\varphi$ bieng an isometry, we get $$g = \underbrace{f\circ\varphi(x)\varphi^\prime(x)^2}_{h(x):=}dx^2.$$ Now $\varphi$ is strictly increasing and $\lim_{x\to+\infty} \varphi(x)=0$ so there is a sequence $(x_n)_n\in\mathbf R^\mathbf N$ converging to $+\infty$ such that $$\lim_{n\to\infty}\varphi^\prime(x_n)=0.$$ Since $f$ in bounded near $0$ , we get $$\lim_{n\to\infty}h(x_n)=0.$$ Therefore if we want to construct nonextendible metric $g=h(x)dx^2$ on $\mathbf R$ ""by hand"", we might want to assume $h$ is bounded bellow by a positive constant $\varepsilon>0$ (so that there is no sequence $(x_n)_n\in\mathbf R^\mathbf N$ with $\lim_{n\to\infty} \vert x_n\vert = +\infty$ and $\lim_{n\to\infty}h(x_n)=0$ ). However I am pretty sure that these metrics are all complete (even if I don't have an actual proof of this fact), so I don't think I am gonna find an example in this way. In dimension two, the only incomplete manifolds I can think of are punctured surfaces, but these are all extendible. Any kind of help will be greatly appreciated.","['riemannian-geometry', 'differential-geometry']"
3939169,"Is there a closed form for $\int_0^1\,_3F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx$?","Is there a closed form evaluation for the integral $$J=\int_0^1 {_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx?$$ Context:
I have been investigating integrals of the form $$e_{p,q}^{n,m}\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}}\right)=\int_0^1x^n\left[{_p}F_q\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}};x\right)\right]^mdx.$$ Obviously there is no reason to expect a general closed form, but I have found the following: $$E_1=e_{2,1}^{1,2}(\tfrac12,1;2)=12-16\ln2,\tag1$$ and $$E_2=e_{2,1}^{1,3}(\tfrac13,\tfrac23;\tfrac32)=\frac{27}{32}.\tag2$$ I found these through applying the Lagrange inversion theorem to the functions $x^2-x$ and $x^3-x$ , respectively. The proofs are below. Theorem. We have the explicit evaluation $$\int_0^1x\left[{_2}F_1(\tfrac12,1;2;x)\right]^2dx=12-16\ln2.\tag{1'}$$ Proof. Using the Lagrange inversion theorem, the function $g(x)$ , satisfying $$g(x)^2-g(x)=x,$$ is given by the hypergeometric series $g(x)=-x\,{_2}F_1(\tfrac12,1;2;-4x)$ , for $x\in[-1/4,\infty)$ . Thus, the function $F(x)={_2}F_1(\tfrac12,1;2;x)$ satisfies $$xF(x)^2=4(F(x)-1),$$ for $x\in(-\infty,1]$ . Thus $$E_1=\int_0^1xF(x)^2dx=-4+4\int_0^1F(x)dx.$$ Then using $$_2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1\frac{t^{b-1}(1-t)^{c-b-1}}{(1-zt)^a}dt,$$ we have $$F(x)=\frac2\pi\int_0^1\sqrt{\frac{1-t}{t}}\frac{dt}{1-xt}=\frac4\pi\int_0^\infty\frac{t^2dt}{(t^2+1)(t^2+1-x)}=\frac{2}{1+\sqrt{1-x}}.$$ It is then not too difficult to show that $$\int_0^1F(x)dx=\int_0^1\frac{2dx}{1+\sqrt{1-x}}=4-4\ln2,$$ which gives $(1')$ and thus $(1)$ . $\square$ Theorem. We have the explicit evaluation $$\int_0^1 x\left[{_2}F_1(\tfrac13,\tfrac23;\tfrac32;x)\right]^3dx=\frac{27}{32}.\tag{2'}$$ Proof. The Lagrange inversion theorem gives $g(x)^3-g(x)=x$ , for $$g(x)=-x{_2}F_1(\tfrac13,\tfrac23;\tfrac32;\tfrac{27}{4}x^2),\qquad |x|<\frac{2}{3\sqrt3}.$$ Setting $F(x)={_2}F_1(\tfrac13,\tfrac23;\tfrac32;x)$ , we have $$4xF(x)^3=27(F(x)-1),$$ and thus $$E_2=\int_0^1xF(x)^3dx=\frac{27}{4}\left(-1+\int_0^1F(x)dx\right).$$ Then from here and here , we have $$\int_0^1F(x)dx=\left(\frac32\right)^2\left(\frac{\Gamma(\tfrac12)\Gamma(\tfrac32)}{\Gamma(\tfrac56)\Gamma(\tfrac76)}-1\right)=\frac{9}{8},$$ which is equivalent to $(2)$ and $(2')$ . $\square$ Here is my work on the current problem. As you may have guessed, we use the Lagrange inversion theorem to see that $g(x)^4-g(x)=x$ , where $$g(x)=-x{_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;-\tfrac{4^4}{3^3}x^3).$$ Setting $F(x)={_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)$ , we have $$xF(x)^4=\frac{4^4}{3^3}(F(x)-1),$$ so that $$e_{1,4}^{3,2}(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43)=\int_0^1xF(x)^4dx=\frac{4^4}{3^3}(J-1),$$ where $J$ is the integral in the title. It may or may not help, but we can use integral representations of $_pF_q$ to get $$J=\int_0^1 F(x)dx=\frac{\Gamma(\tfrac43)\Gamma(\tfrac23)}{\pi\Gamma(\tfrac7{12})}\int_0^1\int_0^1\int_0^1\frac{dtdxdz}{x^{1/2}(1-x)^{1/2}t^{1/4}(1-t)^{5/12}(1-txz)^{1/4}}.$$ According to Desmos, the value of $J$ is roughly $J\approx 1.08494289471$ , but for some reason I can't get wolfram alpha to get me anything better. Is there any way to evaluate $J$ ? Thanks :)","['integration', 'special-functions', 'real-analysis', 'sequences-and-series', 'hypergeometric-function']"
3939177,Find the angle in the drawing,"In the below drawing, we are given:
Triangle ABC is right. D is the midpoint of AB. Angle $ACD = φ$ and $BCE = φ$ .
Angle $EAB = φ$ . $ED=α$ and $CD=4a$ . We are looking for angle φ (to be solved by using Geometry, not trigonometry). What I have tried so far: Since angles EAB and ECB are both φ, and AB is vertical to BC, then their other sides must also be vertical, so AEC is right. Therefore the quadrilateral AEBC is inscribed in a semicircle (which I have drawn to see if I get any clue out of it) and AC is its diameter. If G is the center of the circle, then EG and BG are equal to the circle radius.
Also angle $BDC = φ + BAC = EAC$ so triangles EAC and BDC are similar. But I don't know how to use $ED=a$ and how to combine all this, to calculate angle φ.
By the way, I have found it in Geogebra to be ~19 degrees.",['geometry']
3939180,An eely function $\mu (n):\;\;\prod\limits_{k = 0}^{n - 1} {\left( {\mu (n) - \mu (k)} \right)} = 1$,"Time ago, dealing with a generalization of the Stirling numbers, I stumbled on the following implicit recurrence $$
\mu (n):\;\;\prod\limits_{k = 0}^{n - 1} {\left( {\mu (n) - \mu (k)} \right)}  = 1\quad \left| \matrix{
  \,0 \le n \in Z \hfill \cr 
  \,0 \le \mu (n) \in R \hfill \cr 
  \,\mu (0) = 0 \hfill \cr}  \right.
$$ Using a good CAS it is not difficult to compute the first few values and plot them Clearly, the sequence is monotonically increasing, and its finite difference is monotonically decreasing (1) . Such a regular behaviour leads me to expect that it might be extended over the reals and that $\mu (x)$ might be expressible through a combination of conventional functions. From time to time I am returning to this challenge with some inspiration for a new approach, but
the combination difference & product has frustrated all my attempts. I couldn't even succeed to establish its asymptotic behaviour (now, thanks to @AMarino answer I know it's logarithmic) . So I am asking for hints, suggestions. -- addendum  -- Putting $\rho _{\,n,\,m}  = \mu _{\,n + 1}  - \mu _{\,m} $ , an alternative way to express the problem is $$
\left\{ \matrix{
  \prod\limits_{0\,\, \le \,k\, \le \,n} {\rho _{\,n,\,k} }  = 1 \hfill \cr 
  \rho _{\,n,\,k}  - \rho _{\,n - 1,\,k}  = \mu _{\,n + 1}  - \mu _{\,n}  \hfill \cr}  \right.
$$ which means to find a family of functions whose product wrt $k$ is $1$ and whose difference wrt $n$ is constant, as shown My last tentative has been to take two discrete pmf's on the support $[0,n]$ and put $$
\rho \left( {n,m} \right) = e^{\,h(n)\left( {p(m\,|n) - q(m\,|n)} \right)} 
$$ which by definition gives the unitary product, but cannot go  yet through keeping the difference constant. -- note 1 -- That the difference is monotonically decreasing comes from being $$
\eqalign{
  & \prod\limits_{k = 0}^{n - 1} {\left( {\mu (n) - \mu (k)} \right)}
  = \prod\limits_{k = 0}^{n - 1} {\left( {\mu (n) - \mu (n - 1) + \mu (n - 1) - \mu (k)} \right)}  =   \cr 
  &  = \prod\limits_{k = 0}^{n - 1} {\left( {x + \left( {\mu (n - 1) - \mu (k)} \right)} \right)}
  = p_n (x)\quad  \Rightarrow   \cr 
  &  \Rightarrow \quad \left\{ \matrix{
  p_n (0) = 0 \hfill \cr 
  p_n (x) < p_{n + 1} (x)\quad \left| {\,0 < x} \right. \hfill \cr 
  1 = p_n (\mu (n) - \mu (n - 1)) < p_{n + 1} (\mu (n + 1) - \mu (n)) \hfill \cr}  \right.\quad  \Rightarrow   \cr 
  &  \Rightarrow \quad \mu (n + 1) - \mu (n) < \mu (n) - \mu (n - 1) \cr} 
$$ which tells the interesting fact that $\Delta \mu (n-1)$ comes as the root of $p_n(x)=1$ , which in turn is added
to its zeros, shifting them to the left as to start from $0$ and making them the zeros of $p_{n+1}(x)$ .","['functional-equations', 'functional-analysis', 'recursion']"
3939207,Runge Kutta Fehlberg goes crazy when using adaptive h in python,"So basically what i'm trying to do here is try to define a function that integrates for runge kutta. The most ""physical"" aspects of it i have already controlled for (it's not a problem of the values of the functions , or the physical parameters (initial conditions, etc), since i have controlled for those and they don't change the result) .
The problem i'm solvin is an ODE of second order (harmonic oscilator), x''-μ(1-x^2 )x'+x=0 + CI x(0)=2, x'(0)=0, for t in [0,25], and μ=4 which i have separated into two first-order ODES; μ*(1-x**2)*v-x = v'
v = x' and we have inital conditions x0,v0,t0 and a interval on which to integrate t0,tf, and a step size h with which to integrate.
ε is my error tolerance.
The error I have is that when i activate the adaptivetoggle, so that i should use an adaptive h, x just goes to a value and v goes to 0, instead of oscilating like a harmonic oscilator should. I suspect the problem is only about that bit of code, because when i deactivate the adaptive toggle , everything runs just fine.
I am not sure of what is happening, my values should be oscilating, and small, and instead the error just goes to 0 ( it shouldn't, i think ) , v does too and x tends to something instead of oscilating. The code i'm running is: def rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle):

x = x0; t = t0 
valuesx = []
valuesv = []
while t<tf :
#We define the runge kutta functions on which to iterate while t is in [t0,tf], and they are of the kind (t,x,v)   
    f1v = f(t,               x,                                                                                    v0                                                                                       )
    f1  = v(t,               x,                                                                                    v0                                                                                       )
    
    f2v = f(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )
    f2  = v(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )
    
    f3v = f(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )
    f3  = v(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )
    
    f4v = f(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )
    f4  = v(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )
     
    f5v = f(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )
    f5  = v(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )

    f6v = f(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )
    f6  = v(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )
                         
   #Now we calculate the positions and velocities, for the fourth order runge kutta aproximation. Commented we have the fifth order approxiation results, which we use to estimate the error ( Error = abs(order5approximation-order4approximation)
    x4 = x + h*((25/216)*f1  + (1408/2565)*f3   + (2197/4104)*f4     -(1/5)*f5              )
    #x5 = x + h*((16/135)*f1  + (6656/12825)*f3  + (28561/56430)*f4   -(9/50)*f5  +(2/55)*f6 )   

    v4 = v0 + h*((25/216)*f1v + (1408/2565)*f3v  + (2197/4104)*f4v    -(1/5)*f5v             )
    #v5 = v0 + h*((16/135)*f1v + (6656/12825)*f3v + (28561/56430)*f4v  -(9/50)*f5v +(2/55)*f6v)        
   
  #If we want to use an adaptive h for our calculations,
    if adaptivetoggle == True :
       #We calculate error in x using fs, eror in v using fvs, and we take the smaller hnew of the two 
       Errorx = abs((1/360)*f1  - (128/4275)*f3  - (2197/75240)*f4  + (1/50)*f5  + (2/55)*f6  )
       Errorv = abs((1/360)*f1v - (128/4275)*f3v - (2197/75240)*f4v + (1/50)*f5v + (2/55)*f6v )
 
       
       hnewx  = 0.9*h*((ε)/Errorx)**(0.25)
       hnewv  = 0.9*h*((ε)/Errorv)**(0.25)
       
       hnew = min(hnewx,hnewv)
       
       print(hnew)
       #if hnewx < hnewv:
        #   hnew = hnewx
       #if hnewx > hnewv:
         #  hnew = hnewv

       #After calculating hnew, we compare it to the integration step h we have used, to decide if we can keep our calculation or we have to repeat it using hnew.

       if hnew >= h :
          #we increment the loop,and take hnew for the next loop
          t += h
          h = hnew
          x = x4
          v0 = v4
          valuesx.append(x4)
          valuesv.append(v4)
      
       elif hnew < h: 
          h  = hnew #we don't increment t , the loop variable, when we repeat the integration step using the new h integration step

    else :#if we don't want an adaptive h ( this works just fine)
       valuesx.append(x4)
       valuesv.append(v4)
       x = x4
       v0 = v4
       t+=h #increment the loop
return valuesx, valuesv
#Then we implement the function
#We define the two functions ( of the ODEs) to feed the RK solver
def f(t,x,v):
    return μ*(1-x**2)*v-x
def v(t,x,v):
    return v

#we feed it the parameters
μ  = 4; x0 = 2;  v0 = 0; t0 = 0; tf = 25; h  = 1; ε  = 10**-3 
adaptivetoggle = True

solution = rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle)
print(solution)","['python', 'ordinary-differential-equations', 'recursion', 'runge-kutta-methods', 'numerical-methods']"
3939270,Divergence of a sequence proof,Let $(x_n)$ be a sequence with $x_1=1 $ and $x_{n+1}=x_n+\frac{1}{x_n^2}$ for all integers n. I want to formally prove that this sequence is not limited. I started with showing that $x_n$ is strictly monotonically increasing because: $x_{n+1}-x_n=\frac{1}{x_n^2} > 0$ Now I want to show that for all real numbers $a$ I will find an integer $n_0$ thus $x_n>a$ for all integers $n_0$ . But I am stuck at this point. Can someone give me a hint how to go on? :),"['analysis', 'sequences-and-series']"
3939373,What happened to bornological/uniform spaces?,"When reading older papers, I often see references to bornological or uniform spaces, which encode the notions of ""boundedness"" or ""uniformness"". In this way, they seem to sit between topological spaces and metric spaces. It seems like these used to be of some amount of interest (they are explicitly mentioned in Rudin's ""Functional Analysis"" as topics he's omitting) so it seems odd to me that I've never heard them mentioned in a class, talk, etc. It might just be uncommon in my department, but I don't think I've ever seen them mentioned on MSE either, so they must be somewhat uncommon. Looking at the axioms, they certainly look rather unwieldy, but you could say the same about lots of other mathematical objects which are quite well studied. What, then, happened to bornological spaces and uniform spaces? Does anyone still do work in this area? Has it been subsumed by some other topic? This mystery haunts me. Thanks in advance! ^_^","['uniform-spaces', 'functional-analysis', 'general-topology', 'math-history', 'soft-question']"
3939381,"Is this problem solvable as stated, or is there an error?","I asked this problem, and one of the users suggests the problem is ill posed. Here is the statement of the problem, copied exactly from the homework. Is there really an error in the problem, which makes it unable to be solved? The problem is as follows: Suppose $W : [-1,1] \to \Bbb R$ is a continuous function. Show that $$f(t) = \begin{cases}
 W(t) & \text{if } t \in [-1,0] \\
 1+W(t) & \text{if } t \in (0,1] \end{cases}$$ is integrable on $[-1,1]$ .","['integration', 'calculus', 'functions']"
3939392,"If $x_{n+1}=x_n+x_n^{-m}, x_1>0, m \ge 2 $ then $x_{n}^{m+1} = (m+1)(n+\ln(n))+O_m(1) $","This is a generalization of Divergence of a sequence proof Show that
if $x_{n+1}=x_n+\dfrac{1}{x_n^m},
x_1>0, m \ge 2
$ then $x_{n}^{m+1}
= (m+1)(n+\ln(n))+O_m(1)
$ . (Note:
The notation $O_m(...)$ means that the
constant implied by the
big-oh
depends on $m$ .) I can show that $x_{n}^{m+1}
= (m+1)n+O_m(\ln(n))
$ for integer $m$ ,
and I am pretty sure that
in this case
I can show that $x_{n}^{m+1}
= (m+1)(n+\ln(n))+O_m(1)
$ . I don't have an
explicit expression for the $O_m(1)$ and I don't have a proof
for real $m \ge 2$ .","['limits', 'nonlinear-dynamics', 'recurrence-relations', 'sequences-and-series']"
3939410,How does WolframAlpha simplify $\sum_{k=0}^{n-1} {k+n-1 \choose n-1}$,"Working through a Discrete Math proof I was trying to simplify my equation, but I didn't know how to deal with this summation of a selection: $\sum_{k=0}^{n-1} {k+n-1 \choose n-1}$ I put it into wolframalpha and it simplified it for me, saying: $$\sum_{k=0}^{n - 1} {k + n - 1 \choose n - 1} = {2 n - 1 \choose n - 1}$$ ( This link should show you the result I got) That let me simplify enough to finish the problem, but I don't know how wolframalpha got to that answer. We've just begun the basics of this 'choose notation,' but if I need to do some background research to understand why this simplification is valid I'm willing to. My Question: WolframAlpha simplified my equation, but I'd like to understand the work necessary to get that answer. P.S. I'm new here, I read through the rules but if I'm doing anything wrong be sure to tell me :)","['summation', 'combinatorics', 'discrete-mathematics']"
