question_id,title,body,tags
4183046,The magic behind MATLAB's powerful matrix decomposition algorithms [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 3 years ago . Improve this question I have always wondered how MATLAB's built-in function for matrix decomposition are crafted and optimized. Examples of MATLAB's built-in function for matrix decompositions are $\mathsf{qr},\mathsf{svd},\mathsf{chol}$ I always wondered how do these functions have their algorithm written with optimized speed and stability performance. Moreover, does there exist algorithms that out performs these built-in functions?","['matrices', 'numerical-linear-algebra', 'matlab', 'matrix-decomposition']"
4183074,Galois theory converts problems in field theory to problems in group theory. Can we go the other way?,"When I studied Galois theory, we exclusively used the correspondence between field extensions and automorphism groups to turn field theory problems into analogous group theory problems. Are there any situations where it's useful to do this in reverse, i.e. using field theory to prove results about groups?","['field-theory', 'galois-theory', 'group-theory']"
4183081,"Behavior of $f(n)=\sum_{i,j<n} \left(\frac{1}{i}-\frac{1}{j}\right)^2$","Does the following sum over positive integers have a name/asymptotic closed form? $$f(n)=\sum_{i,j<n} \left(\frac{1}{i}-\frac{1}{j}\right)^2$$ In particular I'm wondering if it eventually grows linearly or sublinearly","['limits', 'sequences-and-series']"
4183166,"If $\sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}=0$ for all $x\in [-1,1]$ what can we say about coefficients $b_{s,m}$?","Consider the following power series: \begin{align}
f(x)=\sum_{s=0}^\infty \sum_{m=0}^{\infty} b_{s,m}x^{2s}(1-x^2)^{m}.
\end{align} Suppose that  we know that $f(x)=0$ for all $x\in [-1,1]$ .
My question is, under the above assumption, what can we conclude about the coefficients $b_{s,m}$ ? Some Thoughts $f$ can be re-written in terms of a single sum as \begin{align}
\sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}x^{2s}(1-x^2)^{m}= \sum_{k=0}^\infty a_k x^{2k}.
\end{align} The expression for $a_k$ in terms $b_{s,m}$ was found here and is given by \begin{align}
a_k =\sum_{s=0}^k \sum_{m \geq (k-s)} b_{s,m} {m \choose (k-s)} (-1)^{k-s} .
\end{align} Since $f(x)=0,x \in [-1,1]$ , this implies that $a_k=0$ for all $k$ . However, since the mapping between $b_{s,m}$ and $a_k$ is not simple I am not sure what we can conclude about the coefficients $b_{s,m}$ and what $a_k$ 's equal zero imply. Edit 1: As suggest in the comment section, it might be usefull to transform the problem by setting $x=\sin(t)$ in which case we get \begin{align}
0= \sum_{s=0}^\infty \sum_{m=0}^\infty b_{s,m}\sin^{2s}(t)\cos^{2m}(t),
\end{align} for all $t\in [-\pi,\pi]$ . Edit 2: If assumptions are needed, then we can assume that $\sum b_{s,m}^2 <\infty$ .","['power-series', 'taylor-expansion', 'real-analysis']"
4183186,Calculating full shade coverage,"Apologies in advance, I'm neither a student nor a mathematician (I'm not even sure how to tag this :P ) Basically, I'm using some home automation software (Home Assistant) to alert me when to close/open the blinds and windows on hot summer days.  This is very easy for my East/West windows (in the example below, Window a and Window c ), as the software provides updating Azimuth values for my location. My problem, however, is that I have a fairly critical window (the bedroom window) that is nestled inside a corner (see the example image).  Based on the example image, What equation would tell me when Window b will be in full shade? I have measured: The length of the wall casting the shadow (Length A) The length of the wall with the window (Length B) The distance from the roof to the top of the window (Length C) The distance from the edge of the window to the edge of the wall (Length D) The Home Assistant software I'm using provides minute-by-minute updates on the Sun's elevation (let's call it X) azimuth (Let's call it Y) This is where my highschool math skills from 20 years ago fall apart.  How to I determine what values of the sun's Elevation (X) and Azimuth (Y) will put my window in full shade given the lengths A, B, C, and D? Help! Please and Thank you.","['trigonometry', 'geometry']"
4183203,Proofs of Karpelevich's results about eigenvalues of nonnegative matrices.,"Are there any books or papers written in English that contain proofs of the results obtained in the following paper? F.I. Karpelevich, On the characteristic roots of matrices with nonnegative elements , Izv. Akad. Nauk SSSR Ker. Mat. , 1951, v.15, issue 4, 361-383. The statement without proof of Karpelevich's main result can be found in theorem 5.1 of the paper On $p$ th roots of stochastic matrices by Higham and Lin. According to the authors, more details of Karpelevich's results can be found in Minc's book Nonnegative Matrices , but again, without proofs.","['linear-algebra', 'nonnegative-matrices', 'reference-request']"
4183212,How do I prove or disprove that the equation $\cos({\sin{x}})=1/x$ has only one real root?,"How do I prove or disprove that the equation $\cos({\sin{x}})=1/x$ has only one real root? It's easy to do graphically, but I'm more interested in the algebraic method. My attempt: First, we can show that $\cos (1)=0.5403023 \ldots \leq \cos (\sin x) \leq 1$ . This implies that the root(s) must be such that $x \in[1 ; 1 / \cos (1)=1.8508157 \ldots]$ . Second, we can show that if $x \in[1 ; \pi / 2]$ we have $\cos (\sin x)<1 / x$ . This implies that the root(s) must be such that $x \in[\pi / 2 ; 1 / \cos (1)]$ . Finally, we show that in this interval the function $\cos({\sin{x}})$ is increasing but the function $1/x$ is decreasing, and that implies that there is only one root. Finding the root of $f(x)=\cos (\sin x)-1 / x$ can be done using the derivative $f^{\prime}(x)=-(\cos x) \sin (\sin x)+1 / x^{2}$ .","['calculus', 'roots', 'trigonometry']"
4183223,"Is this function bounded? $g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p})^t i^{-p} \, di =\varepsilon$","Let $p>1,t>1,\varepsilon>0$ . Assuming the function below exists, when is it bounded? $$g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p} )^t i^{-p} \,di =\varepsilon$$ From simulations I suspect it's bounded for all $p>1$ and $\varepsilon>0$ , can this be shown analytically? For instance, for $p=2$ and $\varepsilon=0.01$ the graph obtained numerically","['integration', 'limits', 'sequences-and-series']"
4183229,Seeming ambiguity when evaluating a function involving arctan? What am I missing?,"I'm trying to evaluate the following exactly: $$\cos(\tan^{-1}(2)+\tan^{-1}(3))$$ There's a few ways to get the answer of $-\frac{\sqrt2}{2}$ .  I'm interested in this particular path using the $\tan^{-1}(a)+\tan^{-1}(b) $ addition identity .  Here are the first few steps without issue: $$\cos(\tan^{-1}(2)+\tan^{-1}(3))=\cos(\tan^{-1}(\frac{2+3}{1-2*3}))$$ $$=\cos(\tan^{-1}(-1))$$ At this point it seems straight forward to me to finish the evaluation, $-1$ can be written as $\tan(-\frac{\pi}{4})$ so we continue: $$\cos(\tan^{-1}(\tan(-\frac{\pi}{4}))=\cos(-\frac{\pi}{4})=+\frac{\sqrt2}{2}$$ Oops...Note that we can force the ""right"" answer if we shift things around a bit: $$\cos(\tan^{-1}(-1))=\cos(\tan^{-1}(-\tan(\frac{\pi}{4})))=\cos(\tan^{-1}(\tan(\pi-\frac{\pi}{4}))$$ $$\cos(\pi-\frac{\pi}{4})=-\frac{\sqrt2}{2}$$ Where did I go wrong here?  What needs to be specified at the beginning to avoid the ambiguity or what convention am I missing? Thanks!","['trigonometry', 'inverse-function']"
4183243,Proof verification for Casorati-Weierstrass theorem,"I posted a question really simillar to this one few days/week(s) ago. But after some thoughts and some further elaboration, I wrote my proof ""more"" formally and thus I wanted to post it again to clarify some details. So, here it goes: Theorem. Let $a$ be a essential singularity of $f$ , then we have that, for any $A \in \mathbb{C}$ there exists a sequence $\left(z_k\right)_{k=0}^{+\infty}$ such that this sequence converges to $a$ and such that $f(z_k) \rightarrow A$ . Proof so far. By contradition: Suposse that $\exists A \in \mathbb{C}$ such that the conclusion of the theorem doesn't hold, i.e., $\exists \epsilon >0$ such that $|z-A|<\epsilon$ doesn't agree with the image of $f$ . Thus, $\forall z \neq a$ , we have that $|f(z)-A|\geq\epsilon$ . Now, consider the particular case of $g(z) = \frac{1}{f(z)-A}$ in a deleted neighbourhood of $a$ . It obviously comes that $|g(z)|\leq \frac{1}{\epsilon}$ . Since $g(z)$ is analytic in a deleted neighbourhood of $a$ , $a$ is also a singular point of $g$ and $a$ is also a removable singularity (using Riemann's Theorem), once it's limited near $a$ . From this, it urges that $\lim_{z\rightarrow a}g(z)$ exists, call it $g(a)$ , and we also have that $g$ extends to a analytic function in $a$ . Now, if $g(a)=0$ , we have that, we have that $\lim_{z\rightarrow a}f(z) = \infty$ and so, $a$ would be a pole of $f$ , which is a contradiction. If $g(a) \neq 0$ , we have that $\lim_{z\rightarrow a}f(z) = A + \frac{1}{g(a)}$ which means, again using the Rimeann's Theorem, that $f$ has a removable singularity, which is (once again) a contradiction. NOTE. Proving that there exists a sequence $\left(z_k\right)_{k=0}^{+\infty}$ such that $f(z_k) \rightarrow A$ is equivalent to proving that the image of $f$ is dense in $\mathbb{C}$ . My final doubts and needs of clarification. I can see clearly where this proves that $f(z_k)\rightarrow A$ . What I am having trouble with is the first part of the theorem, i.e, seeing where this proves that $z_k \rightarrow a$ . If I am missing something and someone could clarify it to me I would be really thankfull. Equivalentelly, if this proof doesn't prove the first part of theorem I would also be thankfull if someone justified it! Thanks for all the help in advance.","['proof-explanation', 'singularity', 'complex-analysis', 'solution-verification', 'sequences-and-series']"
4183263,Tychonoff's theorem vs closed ball,"If Tychonoff's theorem is true, why closed ball in $\mathbb{R}^n$ is not compact? The theorem says that if $X_i$ is compact, for every $i\in I$ , so $\prod_{i\in I}X_i$ is compact. Then take $n\in\mathbb{N}$ and we have $\prod_{n\in\mathbb{N}}[-1,1]_i$ in $\mathbb{R}^\infty$ is not compact. But, what??","['general-topology', 'functional-analysis']"
4183273,Function equal to its own arclength.,"Does there exist a differentiable function $f(x)$ such that $$f(x) = \int_0^x \sqrt{1+[f'(t)]^2}dt?$$ I'm quite interested in finding if there is such a function, as its value at any point $t$ would be the length of the curve from $x=0$ to $x=t$ and I think such a function must necessarily be beautiful in some way. Any help is appreciated!","['calculus', 'arc-length', 'analysis']"
4183281,Critique of this proof of $\frac{1}{\Gamma(s)} = s \prod_{n = 1}^\infty \frac{\left(1 + \frac{s}{n}\right)}{\left(1+ \frac{1}{n}\right)^s}$,"Recently I took a complex analysis exam, and one of the problems was to prove that $$\frac{1}{\Gamma(s)} = s \prod_{n = 1}^\infty \frac{\left(1 + \frac{s}{n}\right)}{\left(1+ \frac{1}{n}\right)^s}$$ I was allowed to use the Stein-Shakarchi text, in which chapter 6, theorem 1.7 states that for all $ s \in \mathbb{C}$ , $$\frac{1}{\Gamma (s)} = e^{\gamma s}s \prod_{n = 1}^\infty \left(1 + \frac{s}{n}\right)e^{ - s/n},$$ where $\gamma$ is the Euler-Mascheroni constant. My professor took away points due to lack of rigor in one of the equalities, and said to ask other mathematicians what they thought about the proof. My proof is as follows: We have that $$\frac{1}{\Gamma (s)} = e^{\gamma s}s \prod_{n = 1}^\infty \left(1 + \frac{s}{n}\right)e^{ - s/n} = e^{\lim_{N \to \infty} \sum_{k = 1}^N s/k - s\log N} s \prod_{n = 1}^\infty \left(1 + \frac{s}{n}\right) e^{- s/n}$$ The exponential terms cancel out as in the limit as $N \to \infty$ as we have that each term in the product is matched by a term from the Euler-Mascheroni term. Thus this equals $$\lim_{N \to \infty} e^{\log N^{-s}} s \prod_{n = 1}^\infty \left(1 + \frac{s}{n}\right) = \lim_{N \to \infty} s \prod_{n = 1}^\infty \frac{1}{N^s} \left(1 + \frac{s}{n}\right)$$ $N = N/(N - 1) \cdot (N - 1)/(N - 2) \cdots 3/2 \cdot 2/1 = \prod_{n = 1}^N (1 + 1/n)$ , therefore as $N \to \infty$ we have that this equals $$s \prod_{n = 1}^\infty \frac{\left(1 + \frac{s}{n}\right)}{\left(1 + \frac{1}{n}\right)^s},$$ as desired. My professor said that this equality $$\lim_{N \to \infty} e^{\log N^{-s}} s \prod_{n = 1}^\infty \left(1 + \frac{s}{n}\right) = \lim_{N \to \infty} s \prod_{n = 1}^\infty \frac{1}{N^s} \left(1 + \frac{s}{n}\right)$$ simply shows that $0 = 0$ and thus my argument is not rigorous. I was confused as to why this was because I only used standard rules involving limits and continuous functions, and it seems like the problems with the limit of $1/N^s$ approaching infinity are resolved in the proof of the equality that I started with (ch. 6, theorem 1/7) via the Hadamard factorization theorem.  Is this rigorous or not? If so, what is my mistake? Thank you very much.","['complex-analysis', 'limits', 'solution-verification', 'gamma-function']"
4183282,How to indefinitely integrate $\ln \ln \ln x$ in terms of special functions or a series of special functions?,"It's well known that $$ \int \ln(x) dx = x \ln(x) - x + C $$ The next one can't be done in elementary functions but can still be expressed as $$ \int \ln \ln (x) dx   = x \ln \ln x - \text{Li}(x) + C $$ Where ""Li"" is the non elementary Logarithmic integral. The next natural question then is: $$ \int \ln \ln \ln (x)  dx = ...?$$ My playing around: So my guess is that the answer should look something like $$ x \ln \ln \ln(x) -...$$ Assuming that form i'm trying to then use special functions to crack open $$ \int \frac{1}{\ln(x) \ln \ln(x) } dx$$ The expression $g(x) = \text{Li}(\ln(x))$ seems an interesting thing to look at, it differentiates to $$ g'(x)  = \frac{1}{x \ln \ln(x)} $$ But I haven't been able to create series out of this quite yet.","['integration', 'complex-analysis', 'indefinite-integrals']"
4183326,Find the number of bijections $g:S\to S$ such that $g^{g(x)}(x)=x$ $\forall x\in S$,"I found a question which asks For any function $f$ , define $f^1(x)=f(x)$ , and for $n\geq 2$ , $f^n(x)=f(f^{n-1}(x))$ . Let $S=\{1,2,3\dots ,10\}$ .
Find the number of bijections $g:S\to S$ such that $g^{g(x)}(x)=x$ $\forall x\in S$ . I know how to deal with these kind of questions if we have a specific integer at the power of $g(x)$ instead of $g(x)$ itself. That is, I can find the number of bijections which satisfy $g(x)=x$ or $g^2(x)=x$ by trying to count cycles. But, I don't have any idea how to approach this one. Any help would be appreciated.","['combinations', 'functions', 'combinatorics']"
4183339,"Number of Prime Numbers Such $p$ such that $p, 10 + p, 10p + 1$ and their sum are primes.","I was intrigued by the reputation I had a few days before. I had a reputation of 1209 a few days before, and out of silly curiosity I thought of just checking in Numbermatics about the specialties of the number and ended up seeing its prime factorisation : $3 \times 13 \times 31$ . What I found interesting is that these prime numbers add up to another prime : $47$ . This made me think : are there any more prime numbers $p$ such that $p, 10 + p, 10p + 1$ are prime and their sum (i.e., $11(p+1) + p$ ) is also a prime? The apparent solution at first sight is $3$ , but I doubt if more solutions exist. If I look at the final digit of the sum, it is $2p + 1 \pmod{10}$ which clearly implies that the last digit is always odd. But also note that the addends must also be also be prime, so $5$ can be eliminated. The addends are prime when $p = 7$ as well, but the sum is composite. $p = 11$ is invalid as the sum will be divisible by $11$ . Thus when $p$ is $13$ , the sum and the addends are prime again. Based in this observation, we can conjecture that primes of the form $10x + 3$ satisfy the conditions, but $23$ is a contradiction as one of the addends will turn composite. And so is $43$ - the sum turns composite. I am not an advanced mathematician and has not enough background in number theory (w.r.t this problem), so please give me an answer (a sufficiently detailed one is most invited) to how I can solve this problem.","['number-theory', 'prime-numbers']"
4183406,An elementary way to show that the determinant is non zero,"Show that the determinant of the matrix \begin{pmatrix} a && -c && -b \\
  b && a - 2c && -c -2b \\ c && b && a -2c
 \end{pmatrix} is non zero for all integers $a,b,c$ where $abc \ne 0$ There is an interesting way to do this by using integral domains. It is easy to see that the polynomial $t^3 + 2t + 1$ is prime in $\mathbb{Z}[t]$ so the ring $\mathbb{Z}[t]/\langle t^3+2t+1\rangle$ is an integral domain and isomorphic to the ring $\{a + bu + cu^2 \vert \text{$a,b,c$ are integers}\}$ where $u$ is a root of the equation $t^3 +2t + 1 =0$ . Now consider integers $a,b,c$ and $x,y,z$ such that $abc \ne  0$ and $(a +bu + cu^2)(x + yu + zu^2) = 0$ . These are element of an integral domain so we must have that $x = y = z = 0$ since at least one of $a,b$ and $c$ are nonzero. But expanding the above equation we get a system of linear equations in terms of $x,y$ and $z$ . $$
\begin{aligned} 
ax- cy -bz &= 0 \\ 
bx+ (a-2c)y + (-c-2b)z &= 0\\ 
c x+b y+ (a-2c)z &= 0 
\end{aligned} 
$$ If the determinant is $0$ then this equation will have a nontrivial solution which is not possible. This strategy requires the use of integral domains which is an abstract tool. Are there any elementary ways to solve this? It gets messy when we try to do row operations or try to expand the determinant. Update : After  Carl Schildkraut's and JimmyK4542's answer. A curious observation: We can also prove that the element $t^3 + rt + 1$ is prime in $\mathbb{Z}[t]$ for $r \ne 0, -2$ . Therefore by similar arguments
we can say that the determinant of the matrix \begin{pmatrix} a && -c && -b \\
  b && a - rc && -c -rb \\ c && b && a -rc
 \end{pmatrix} is non zero for all integers $a,b,c,r$ where $abc \ne 0$ and $r \ne 0,-2$ .","['determinant', 'elementary-number-theory', 'matrices', 'abstract-algebra', 'linear-algebra']"
4183454,What is the derivative of Cantor function on Cantor set?,"Although on Cantor set $C$ , the Cantor function $f|_C$ is non-differentiable, but I wonder for $x,y\in C$ , the limit: $$\lim_{y\to x}\frac{f(y)-f(x)}{y-x}=$$ $$\begin{cases}
=+\infty\\\mathrm{or}\\\text{does not exist?}
\end{cases}$$ I have tried to use the property that $f=\lim\limits_{n\to \infty} F_n$ and $\lim\limits_{n\to\infty}\frac{F_n(y)-F_n(x)}{y-x}=+\infty$ ,but since $\phi_n(y)= \frac{F_n(y)-F_n(x)}{y-x}$ is not uniformly convergent, we cannot say: $$\lim\limits_{y\to x} \lim\limits_{n\to \infty }\phi_n(y)=\lim\limits_{n\to \infty}\lim\limits_{y\to x}\phi_n(y)$$ so I do not know – is there any ways to research the property of the derivative of $f|_C$ .","['limits', 'cantor-set', 'derivatives', 'real-analysis']"
4183490,Does a continuous map from $\mathbb{R}^n$ into an infinite dimensional Hilbert space define a manifold?,"Let $H$ be an infinite dimensional Hilbert space with inner product $\langle \cdot, \cdot\rangle$ and norm $\|\cdot\|_H$ . This norm induces a distance on $H$ given by $$
\forall p,q\in H \ d_H(q,p)=\|q-p\|_H
$$ Let $\Omega \subset \mathbb{R}^n$ be compact and $f$ be a map $$
f: \Omega \longrightarrow H
$$ such that it is continuous with respect to the euclidean standard metric $d_E$ in $\mathbb{R}^n$ and the metric $d_H$ in $H$ . Moreover, the partial derivatives of $f$ with respect to each component of the coordinates in $\Omega$ exists and $$
\frac{\partial f}{\partial x_i} \in H
$$ Under which circumstances $f$ defines a finite dimensional manifold in $H$ with respect to the topology induced by the metric $d_H$ ? If $f$ is neither an injection or a surjection, can the image of $\Omega$ under $f$ be endowed with a manifold structure? Is there a more convenient topology on $H$ that allows to define a manifold? Motivation Interpolating polynomials are an important tool in several Engineering applications. An important case is when the domain of such polynomials is a fixed interval $I\subset \mathbb{R}$ and their codomain is a compact set $\Omega\subset\mathbb{R}^n$ . If we set a fixed sequence of $N+1$ points in the domain $\{t_i\}_{i=0}^{N}\subset I$ then any sequence of $N+1$ points on the codomain $\{w_i\}_{i=0}^{N}\subset \Omega$ defines uniquely a polynomial $p:I\longrightarrow\mathbb{R}^n$ $$
p(t) = w_0 \ell_0(t) + \ldots + w_N \ell_N(t),
$$ where $\ell_i$ are the Lagrange polynomials generated by the points $\{t_i\}_{i=0}^{N}\subset I$ .
Because polynomials with compact domain $I$ are square integrable, the interpolation process defines a map $f:\Omega^{(N+1)}\longrightarrow L^2(I, \mathbb{R}^n)$ . If the interpolating map $f$ induces a manifold on $L^2$ then it would be possible to solve problems associated with the interpolation problem using the tools of differential geometry.","['vector-spaces', 'linear-algebra', 'functional-analysis', 'differential-geometry']"
4183501,unit tangent bundle is connected if and only if $M$ is connected.,"Let $M$ be a Riemannian manifold,and $UTM  = \{(p,v) \in TM\mid g(v,v) = 1\}$ be the unit tangent bundle,assume we have proved that is a embedded submanifold in $TM$ .Needs to prove that $UTM$ is connected if and only if $M$ is connected (provided that dim of $M$ is greater than 2 as here shows) One direction is easy that is $p:UTM \to M$ is continuous hence if $UTM$ is connected then $M$ is connected. I try to prove the other direction,and I hope there is some better idea: We only need to prove that $UTM$ is path connected if $M$ is since manifold is locally path connected and locally connected. To do this consider two point $(p,v)$ and $(q,u)$ on $UTM$ ,needs to find a continuous path.To do this it may be helpful to transport $v$ by some curve $\gamma$ to some other place that still preserved the length 1 that is $d\gamma (v) $ has lengith 1. But I have no idea how to construct such a curve,I try to do like this,first there is a finite chain of the neighborhood(which is given as local trivialization domain) that connect $p$ and $q$ (by the chain characterization of ""connectedness"" ) denote them $U_{x_1},...,U_{x_N}$ such that exist some $y_i \in U_{x_i} \cap U_{x_{i+1}} $ Then we construct local pieces of curve from $$p\to y_1\to x_2\to y_2\to....\to x_{N} \to q$$ Where $p^{-1}(U) \cong U\times S^{n-1} $ that is locally looks like $U\times S^{n-1}$ where we can construct the desired curve If $n\ge 2$ since then it's connected. Is there some better solution?","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4183528,Expressing complexified tangent bundle of a spin 4-manifold as a Hom bundle,"I am reading Moore's book Lectures on Seiberg-Witten Invariants , section 2.2. First here are some defintions that the book uses. The group $\operatorname{Spin}(4)$ is defined to be the product group $SU(2)\times SU(2)$ . Let $V$ be the 4-dimensional $\Bbb R$ -algebra with $\Bbb R$ -basis $\textbf{1}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \textbf{i}=\begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix}, \textbf{j}=\begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix}, \textbf{k}=\begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} $ . Thus $V$ consists of $2\times 2$ complex matrices of the form $\begin{bmatrix} a & -\bar{b} \\ b & \bar{a} \end{bmatrix}$ . Define $\rho:\text{Spin}(4)\to GL(V)$ by $\rho(A,B)(Q)=BQA^{-1}$ . If we identify $V$ and $\Bbb R^4$ (as $\Bbb R$ -vector spaces) by $\textbf{1}\leftrightarrow (1,0,0,0), \textbf{k}\leftrightarrow (0,1,0,0), \textbf{i}\leftrightarrow (0,0,1,0), \textbf{j}\leftrightarrow (0,0,0,1) $ , it can be seen that $\rho$ maps $\text{Spin}(4)$ into $SO(4)$ . Also let $\rho_+, \rho_-:\text{Spin}(4)\to SU(2)$ be the projections onto first and second components, respectively. Lastly, here is the definition of a spin structure on a 4-manifold. Suppose $M$ is an oriented Riemannian manifold of dimension $4$ . Then we can choose trivializing open cover $\{U_\alpha\}$ for $TM$ such that the corresponding transition functions $g_{\alpha\beta}$ take values in $SO(4)$ . A spin structure on $M$ is collection of maps $\tilde{g}_{\alpha\beta}:U_\alpha \cap U_\beta \to \text{Spin}(4)$ such that $\rho \circ \tilde{g}_{\alpha\beta}=g_{\alpha\beta}$ and $\tilde{g}_{\alpha\beta}\tilde{g}_{\beta\gamma}=\tilde{g}_{\alpha\gamma}$ . Now here is my question. Suppose we are given a spin structure on $M$ . Considering $\{\rho_+\circ \tilde{g}_{\alpha\beta}:U_\alpha\cap U_\beta \to SU(2)\}$ and $\{\rho_-\circ \tilde{g}_{\alpha\beta}:U_\alpha\cap U_\beta \to SU(2)\}$ , we get two $\Bbb C^2$ -bundles $W_+$ and $W_-$ over $M$ . The book is claiming that the complexified tangent bundle $TM\otimes \Bbb C$ is isomorphic to the bundle $E:=\text{Hom}_{\Bbb C}(W_+,W_-)$ , but I can't see why. Why are these two bundles are isomorphic? I think comparing transition functions is an appropriate approach. First, the bundle $TM\otimes \Bbb C$ has transition functions $g_{\alpha\beta}$ . If we write $\tilde{g}_{\alpha\beta}=(A_{\alpha\beta},B_{\alpha\beta})$ , then $W_+$ and $W_-$ have transition functions $A_{\alpha\beta}$ and $B_{\alpha\beta}$ , respectively, so $E$ has transition functions $((A_{\alpha\beta})^{-1})^T \otimes B_{\alpha\beta}$ . Since $A_{\alpha\beta}$ take values in $SU(2)$ , $((A_{\alpha\beta})^{-1})^T=(A_{\alpha\beta}^*)^T=\overline{A_{\alpha\beta}}$ . Thus $E$ has transition functions $\overline{A_{\alpha\beta}}\otimes B_{\alpha\beta}$ . How can we show that this is equivalent to $g_{\alpha\beta}$ ?","['riemannian-geometry', 'spin-geometry', '4-manifolds', 'tangent-bundle', 'differential-geometry']"
4183534,$\varphi(A(x))$ continuous $\Rightarrow$ $A$ continuous?,"Let's define linear operator: $$A \colon X \rightarrow Y$$ between normed spaces $X$ and $Y$ . Also consider linear, continuous operator: $$B\colon Y' \rightarrow X'$$ such that $B(\varphi(x)) = \varphi(A(x))$ for $\varphi \in Y', \; x \in X$ . I want to prove that if $B$ is well defined then $A$ is continuous and $\|A\| = \|B\|$ . My work so far Continuity of $B$ : $$\|B(\varphi(x))\| \le \|B\| \cdot \|\varphi(x)\| \le \|B\| \cdot \|\varphi\| \cdot \|x\|$$ Continuity of $\varphi$ : $$\|B(\varphi(x))\| = \|\varphi(A(x))\| \le \|\varphi\|\cdot \|A(x)\|$$ And here out of these two inequalities I tried somehow to obtain that $\|A(x)\| \le \|B\| \cdot\|x\|$ , and $\|B\|$ is the smallest constant $M \ge 0$ for such $\|A(x)\| \le M \|x\|$ . Also my other idea was to use Hahn-Banach theorem, becuase in thesis there is continuity and equality of norms, however I wasn't able to use it. Could you please give me a hint how to end this proof?","['operator-theory', 'normed-spaces', 'functional-analysis', 'real-analysis']"
4183603,Is there a method to determine whether two algebraic curves are related by an euclidean isometry?,"In differential geometry we have the fundamental theorem of curves for two curves given parametrically: If two plane curves, parametrized by their arc length, have the same curvature (as a function of the parameter), then the curves are congruent - there is an isometry of the plane that takes one curve to the other. This theorem gives an (almost) effective procedure for determining whether two curves, given parametrically by functions which are computable, are congruent. Calculating curvature involves only derivatives and so is always tractable. So the only difficulty applying this method is that we need to parameterize both curves in terms of their arc length, which involves an integral. But, if we can somehow solve integrals, then we have a method for always determining whether two parametric plane curves are congruent. Generalizations of the above theorem also exist for space curves and for surfaces. Algebraic curves however are also interesting, and they can rarely be parametrized, so we can't apply the above method. Curvature formulas exist for algebraic curves, but they also give the curvature implicitly (as a function of both x and y), so again, we can't apply the above theorem. So, is there a method to determine whether two algebraic plane curves are related by a euclidean isometry? I'm also fine with ""partial"" methods that depend on our ability to solve another common problem, such as solving integrals or finding roots of polynomials (although if somehow a complete method exists, that's obviously preferable). If such a method exists, a generalization for surfaces or space curves is also welcome.","['algebraic-curves', 'plane-curves', 'curves', 'curvature', 'algebraic-geometry']"
4183605,"Introduction to Statistical Learning, Chapter 3,Ex 3.7.7: Proofing equality of $ R^2=Cor^2$ in simple linear regression. Figuring out the algebra","I am a hobby mathematican without any formal training.
Currently I am chewing through 'An Introduction to Statistical Learning', 1st Ed. (abbreviated: ISLR, https://www.statlearning.com/ ). Now I am stuck on an algebraic proof in the linear regression exercise chapter 3, specifically 3.7.7. As this is a somewhat 'uphill' difficulty, I have not made much progress on the desired proof. The excercise states: ""It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$ , the $ R^2 $ statistic (3.17) is equal to the square of the correlation between $X$ and $Y $ (3.18). Prove that this is the case. For simplicity, you may assume that $ \bar{x}=\bar{y}=0 $ "". I have come this far up to now: Excercise 3.7.7: Prove that $ R^2 = Cor^2 $ DEFINITIONS: $ \bar{x} \equiv \frac{1}{n} \sum_{i=1}^n { x_i }, $ $ \bar{y} \equiv \frac{1}{n} \sum_{i=1}^n { y_i }, $ $
\hat{\beta}_1 =
  \frac
  { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
  { \sum_{i=1}^n{ (x_i - \bar{x})^2               } },
$ $ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}, $ $
\hat{y}_i
=
\hat{\beta}_0 + \hat{\beta}_1 x_i
=
\hat{\beta}_0 +
  \frac
  { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
  { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  x_i
=
\bar{y} -
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  \bar{x} +
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  x_i,
$ $ RSS = \sum_{i=1}^n { (y_i - \hat{y}_i)^2 }, $ $ TSS = \sum_{i=1}^n { (y_i - \bar{y}  )^2 }, $ $
R^2
=
\frac{TSS - RSS}{TSS}
=
1 - \frac{RSS}{TSS}
=
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } },
$ $
Cor
=
\frac
  { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
  {
    \sqrt{ \sum_{i=1}^n { (x_i-\bar{x})^2 } }
    \sqrt{ \sum_{i=1}^n { (y_i-\bar{y})^2 } }
  }
=
\frac
  { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
  {
    \sqrt{
      \sum_{i=1}^n { (x_i-\bar{x})^2 }
      \sum_{i=1}^n { (y_i-\bar{y})^2 }
    }
  }
$ $
Cor^2
=
\left(
  \frac
    { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
    {
      \sqrt{
        \sum_{i=1}^n { (x_i-\bar{x})^2 }
        \sum_{i=1}^n { (y_i-\bar{y})^2 }
      }
    }
\right)^2
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }
  {
    \left( \sqrt{
      \sum_{i=1}^n { (x_i-\bar{x})^2 }
      \sum_{i=1}^n { (y_i-\bar{y})^2 }
    } \right)^2
  }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x}) (y_i-\bar{y}) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-\bar{x})^2 }
    \sum_{i=1}^n { (y_i-\bar{y})^2 }
  }
$ TO PROVE: $ R^2 = Cor^2 $ , with $ \bar{x} = \bar{y} = 0 $ . Initial expansion: $
\hat{y}_i
=
\bar{y} -
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }
  \bar{x} +
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }
  x_i
=
0 -
  \frac
    { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }
    { \sum_{i=1}^n{ (x_i - 0)^2     } }
  0 +
  \frac
    { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }
    { \sum_{i=1}^n{ (x_i - 0)^2     } }
  x_i
=
\frac
  { \sum_{i=1}^n{ x_i y_i   } }
  { \sum_{i=1}^n{ x_i^2 } }
x_i
$ $
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-\bar{x})^2 }
    \sum_{i=1}^n { (y_i-\bar{y})^2 }
  },
$ $
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - 0         )^2 } }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-0)(y_i-0) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-0)^2 }
    \sum_{i=1}^n { (y_i-0)^2 }
  },
$ $
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { y_i^2 } }
=
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  },
$ $
\boxed{
1 - \frac
  { \sum_{i=1}^n {
    \left(
      y_i -
      \frac
        { \sum_{i=1}^n{ x_i y_i } }
        { \sum_{i=1}^n{ x_i^2   } }
      x_i
    \right)^2 }
  }
  { \sum_{i=1}^n { y_i^2 } }
=
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
}
$ Ok, so now I want to prove the equality of the boxed formula.
And that is were I start to stumble. I manage to make a couple of transformations, however I can only achieve a common denominator between the two. The top part of the vulgar fraction escapes me. Here is what I have tried: RHS equals to: $
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  { \sum_{i=1}^n { x_i^2 } \sum_{i=1}^n { y_i^2 } }
$ Transformations for LHS: T0: $
1 - \frac
  { \sum_{i=1}^n {
    \left(
      y_i -
      \frac
        { \sum_{i=1}^n{ x_i y_i } }
        { \sum_{i=1}^n{ x_i^2   } }
      x_i
    \right)^2 }
  }
  { \sum_{i=1}^n { y_i^2 } }
$ T1: $
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        y_i -
        \frac
          { \sum_{i=1}^n{ x_i y_i } }
          { \sum_{i=1}^n{ x_i^2   } }
        x_i
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
$ T2: $
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        y_i -
        \frac
          { x_i \sum_{i=1}^n{ x_i y_i } }
          {     \sum_{i=1}^n{ x_i^2   } }
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
$ T3: $
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        \frac
          {
            \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
            -
            \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
          }
          { \sum_{i=1}^n{ x_i^2 } }
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
$ T4: $
\frac
  {
    \sum_{i=1}^n { x_i^2 }
    \left(
      \left( \sum_{i=1}^n { y_i^2 } \right)
      -
      \sum_{i=1}^n {
        \left(
          \frac
            {
              \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
              -
              \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
            }
            { \sum_{i=1}^n{ x_i^2 } }
        \right)^2
      }
    \right)
  }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
$ T5: $
\frac
  {
    \sum_{i=1}^n { x_i^2 }
    \left(
      \left( \sum_{i=1}^n { y_i^2 } \right)
      -
      \sum_{i=1}^n {
        \left(
          \left(
            \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
            -
            \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
          \right)
          \frac
            { 1 }
            { \sum_{i=1}^n{ x_i^2 } }
        \right)^2
      }
    \right)
  }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
$ ... and here I am running massively out of ideas. I can see the term $ \left( \sum_{i=1}^n { x_i y_i } \right)^2 $ in the top, but no idea how to isolate it and remove the rest. Does anyone have any pointers to get me back on the right road? -terminal","['linear-regression', 'statistics']"
4183699,Free product not abelian,"If $(G_i)$ , $i \in I$ with $|I| > 1$ and $|G_i| > 1$ is the free product $G$ of $(G_i)$ , $i \in I$ always not-commutative? The free product of a family of groups $(G_i)$ , $i \in I$ is defined
by a tuple $(G, (\phi_i)_{i \in I})$ where $G$ is a group and $\phi_i : G_i \rightarrow G$ are homomorphisms, with the following property: For every tuple $(U, (g_i)_{i \in I})$ where $U$ is a group and $g_i : G_i \to U$ are homomorphisms, there exists a unique homomorphism $f : G \to U$ with $g_i = f \circ \phi_i$ for all $i \in I$ .
I thought that since $|G_i| > 1$ and $|I| > 1$ I can choose $a_i \in G_i$ and $a_j \in G_j$ with $i \neq j$ , $a_i \neq e_i $ with $a_j \neq e_j$ . I wanted to show this by contraposition assuming $\phi_i(a_i) \phi_j(a_j) = \phi_j(a_j) \phi_i(a_i)$ for all $i \neq j$ and wanted to show that $a_i = e_i$ or $a_j = e_j$ but I can't really come to this conclusion since all Groups $G_i$ can be commutative and the only think that comes in my mind is that $G$ is generated by \begin{equation}
\bigcup_{i \in I}{\phi_i(G_i)}
\end{equation} which doesn't really help. I would be thankful for every hint I can get.","['group-theory', 'abstract-algebra', 'free-product']"
4183713,"Given an integer sequence $a_{n+1}=a_{n}^{2}-a_{n}-1$, prove that $\forall n\in \mathbb{Z}$, $a_{n+1}$ and $2n+1$ are coprime","This is a very interesting infinite integer sequence problem I came across. The crux of the matter lies in comparing the values and the indices - we are supposed to check whether $a_{n+1}$ and $2n+1$ are coprime as opposed to any two members of the sequence itself. I tried plugging in a few numbers. In the cases when $a_{1}$ is $-1$ , $0$ , $1$ or $2$ the sequence starts oscillating between $-1$ and $1$ and the proof becomes trivial. In general, however, the numbers obviously become large really fast. What kind of approach should be taken? Thank you for the help","['coprime', 'sequences-and-series']"
4183723,complex curve integral interpretation and calculation: $F(z) = \dfrac{z^2}{z^k}$,"The task asks me to calculate the curve integral of that function $F(z)$ over the curve $\gamma(t) = \{z \in\mathbb{C}: \:\vert z\vert=1\}$ . Apparently this is a circle in the complex plane. Before I proceed to my pure calculation process I actually wonder what
that calculation will mean. If I were to look closer at $F(z)$ I
should notice how it maps a function from 2 Dimensions to 4 Dimensions
(each plugged in complex number is matched with another complex
number, both depending on 2 variables). Now how does this plot touch
the curve $\gamma$ ? Is this similar to ordinary curve integrals, where
all Function values of the plot along the curve are added together? Anyway, here my calculations, that I came up with just by intuition: $$\begin{align}
&\text{parametrisation of $\gamma(t)$ by}\: e^{i\,t}\,\quad t\in[0,2\,\pi] \\\\
&\text{ into the usual definitionn of curve integral:}\\\\
&\int_C F(z)\,\mathrm{dz} = \int_0^{2\,\pi} F(\gamma(t))\,\gamma'(t)\,\mathrm{dt} = \\\\
& \int_0^{2\,\pi}e^{i\,t\,(2-k)}\,i\,e^{i\,t}\,\mathrm{dt} = \left[\dfrac{1\,i}{i\,(3-k)}\,e^{i\,t(3-k)}\right]_0^{2\,\pi} = \\\\
&\dfrac{1}{(3-k)}\,\left[\cos\bigr((3-k)\,2\,\pi\bigl)+i\sin\bigr((3-k)\,2\,\pi\bigl)-1\right] = \\\\
&\dfrac{1}{3-k}\left(1-1\right) = 0 \quad \text{is this the answer?}
\end{align}$$ Edit: coming back to this I still need help for $k = 3$ , that peculiar case. I thought it'd be easy to show the integrals also equals $0$ , instead: $\displaystyle{\int_{0}^{2\,\pi}e^{i\,t(3-3)}\,i\,\mathrm{dt} = \left[i\,t\right]_0^{2\,\pi}} = 2\,\pi\,i$ ? Also having access to the Residue Theorem now it seems to agree: $\displaystyle{\int_{C}F(z)}=\displaystyle{\int_{C}\dfrac{z^2}{z^k}} = (\text{Res$(0)$} + \cdots +  \text{Res$(0)$})\,2\,\pi\,i$ for each k from $1 \cdots n$ hence $\displaystyle{\int_{C}F(z) = (\lim_{z \to 0}\dfrac{z^2}{z^1}\,z^1 + \lim_{z \to 0}\dfrac{d}{dz}\dfrac{z^2}{z^2}\,z^2\,\dfrac{1}{1!}+ \lim_{z \to 0}\dfrac{d^2}{dz^2}\dfrac{z^2}{z^3}\,z^3\,\dfrac{1}{2!}+\cdots)\,2\,\pi\,i} $ Here likewise for $k = 3$ the integral seems to become non zero: $\int_C F(z) =2\,\pi\,i$ . The rest vanishes. It just happens to disagree with taking the limit of the very first expression: $$\displaystyle{\lim_{k\to 3}\dfrac{1}{3-k}\left(1-1\right) = 0 ?}$$","['complex-analysis', 'vector-analysis']"
4183734,How do you determine the sign of $\sin 3 + \sin 4$ without using a calculator? Is it even possible to have a sensible answer?,"I just started trigonometry, and I came across this problem. I know that this problem would be very simple with a calculator, but without one, I'm lost. How would you determine if $\sin 4$ is bigger than $\sin 3$ in negativity or positivity? How would you even determine if they are positive or negative in the first place, and how would you know which one is bigger?","['algebra-precalculus', 'trigonometry']"
4183759,When does $\sum_a^b k | \prod_a^b k$?,"When does the sum $\sum_a^b k$ of $b-a+1$ consecutive positive integers divide the product $\prod_a^b k$ ? I know that the sum of the first $n$ natural numbers divides the product of the first $n$ natural numbers for all $n$ except when $n=p-1$ , $p$ being any odd prime. But what about the more general case? I tried a number of the particular cases, but from there it seems not clear how the general case would look like.",['number-theory']
4183774,Coherent Sheaves on Smooth Curves,"Let $X$ be a locally noetherian integral non-singular scheme  of dimension 1 (I'm really interested in the case $X=\mathbb{P}^1_K$ ) and $\mathcal{E}$ a coherent sheaf over $X$ , I've already shown that $\mathcal{E}$ is either locally free (of finite rank) or has torsion (this use the theorem of the structure of finitely generated modules over PIDs on stalks). Let $\mathcal{T} \subseteq \mathcal{E}$ the torsion subsheaf, then $\mathcal{F} = \mathcal{E}/\mathcal{T}$ is locally free and we have a short exact sequence $$
0 \longrightarrow
\mathcal{T} \longrightarrow
\mathcal{E} \longrightarrow
\mathcal{F} \longrightarrow
0
$$ My question is: I want this sequence to split, so every coherent sheaf over $\mathbb{P}^1_K$ would be a sum of a vector bundle and a torsion coherent sheaf. I know this sequence is split on stalks or even on affine opens, but it is not clear to me how to glue sections/retractions together since there is no canonical one. I saw somewhere that torsion sheaves over $X$ have finite support, but I don't know why. Does anyone have a hint or a reference to it? Context: I already know Birkhoff–Grothendieck theorem about the classification of vector bundles over $\mathbb{P}^1_K$ . Now I want a sort of classification to coherent sheaves. It sounds like every coherent sheaf is a finite sum of twists of $\mathcal{O}$ (lines bundles) and skyscraper sheaves over closed points.","['vector-bundles', 'algebraic-geometry', 'coherent-sheaves', 'projective-space']"
4183777,"Why does continuity imply $P(X_i = X_j) = 0$, without integrals","In the book A probability path , at chapter 4 integrals were not yet defined. At 4.3.1, it says Let $\{X_n, n\geq 1\}$ be iid (real random variables) with common distribution function $F(x)$ . The continuity of $F$ implies $$P[X_i = X_j] = 0$$ I'm guessing they mean for some $i\neq j$ . Intuitively sounds like that set is going to be a curve and therefore of null measure in $\mathbb{R}^2$ . Is there a simple way to prove this without using integrals?","['probability', 'random-variables']"
4183836,Proper morphism and base change,"I am little confused about properness for schemes. Let $X \rightarrow \operatorname{Spec}(k)$ be a proper morphism. If $K/k$ is a field extension, then $X_K= X \times_k K \rightarrow \operatorname{Spec}(K)$ is proper. But what about the morphism $X_K \rightarrow X$ ? Is it also proper? Or are some conditions needed to get properness?","['algebraic-geometry', 'schemes']"
4183846,Solutions to the equation $x! + y! = (p-1)^{p}$,"So I was messing around with equations involving factorials and I ended up with this: Find all natural numbers $x,y,p$ where $p$ is a prime such that: $$x! + y! = (p-1)^{p}$$ Through trial and error the only solutions I could find are $(x,y,p) = (2,3,3), (3,2,3)$ so I'm very interested to know whether or not there exist any other solutions.",['number-theory']
4183854,Functional derivative and arbitrary function,"I have a questions regarding the definition of the functional derivative. Unfortunately a lot of text books give not a proper formal definition. Wikipedia gives the following definition \begin{align}
 \int \frac{\delta F}{\delta\rho}(x) \phi(x) \; dx 
&= \lim_{\varepsilon\to 0}\frac{F[\rho+\varepsilon \phi]-F[\rho]}{\varepsilon} \\
&= \left [ \frac{d}{d\varepsilon}F[\rho+\varepsilon \phi]\right ]_{\varepsilon=0},
\end{align} with $\phi$ an arbitrary function, $M$ be a manifold of continous functions $\rho$ and $F:M\to \mathbb{R}$ If $\phi$ is arbitrary then how do I know the left integral exists? Are there no constraints on $\phi$ like it has to be integrable and in $C_c^{\infty}$ ?","['functional-analysis', 'calculus-of-variations']"
4183886,Curvature of a Vector Bundle as a 2-form,"Let $(E,\mathcal{M},\pi)$ be a smooth $\mathbb{R}$ -vector bundle over some smooth manifold $\mathcal{M}$ with or without boundary.Furthermore, let $$\nabla:\mathfrak{X}(\mathcal{M})\times\Gamma^{\infty}(\mathcal{M},E)\to \Gamma^{\infty}(\mathcal{M},E)$$ denote an arbitrary connection on $(E,\mathcal{M},\pi)$ . Then the curvature of the bundle $E$ with respect to the connection $\nabla$ is a map $$F_{\nabla}:\mathfrak{X}(\mathcal{M})\times \mathfrak{X}(\mathcal{M})\times \Gamma^{\infty}(\mathcal{M},E)\to \Gamma^{\infty}(\mathcal{M},E)$$ defined as $$F_{\nabla}(X,Y,s):=\nabla_{X}\nabla_{Y}s-\nabla_{Y}\nabla_{X}s-\nabla_{[X,Y]}s$$ Now, my goal is to proove that $F_{\nabla}\in\Omega^{2}(\mathcal{M},\mathrm{End}(E))$ , where $\mathrm{End}(E)$ denotes the endomorphism bundle of $E$ , i.e. the bundle with fibres given by $\mathrm{End}(E_{p})$ . First of all, we can use that there is a $C^{\infty}(\mathcal{M})$ -module isomorphism $$\Omega^{2}(\mathcal{M},\mathrm{End}(E))\cong \mathrm{L}_{\mathrm{alt}}^{2}(\mathfrak{X}(\mathcal{M}),\Gamma^{\infty}(\mathcal{M},\mathrm{End}(E)))$$ Furthermore, it is easy to prove that the map $$F_{\nabla}:\mathfrak{X}(\mathcal{M})\times \mathfrak{X}(\mathcal{M})\to\Gamma^{\infty}(\mathcal{M},E), (X,Y)\mapsto F(X,Y,s)$$ for some fixed $s\in\Gamma^{\infty}(\mathcal{M},E)$ is $C^{\infty}(\mathcal{M})$ -bilinear and antisymmetric. But how to continue? In the end we need to proof that $$F(X,Y)\in \Gamma^{\infty}(\mathcal{M},\mathrm{End}(E))$$ How is this map defined and how to show smoothness?","['connections', 'curvature', 'vector-bundles', 'differential-topology', 'differential-geometry']"
4183912,Convergence of conditional distributions implies convergence of unconditional distributions,"Let $X,X_1,X_2,\dots$ be real random variables defined on a probability space $(\Omega,\mathcal A ,P)$ . Given a sub- $\sigma$ -algebra $\mathcal F\subset\mathcal A$ , let $P_{X}$ denote the distribution of $X$ and let $P_{X|\mathcal F}$ denote the conditional distribution of $X$ given $\mathcal F$ , i.e. a Markov kernel from $(\Omega,\mathcal F)$ to $(\mathbb R ,\mathcal B(\mathbb R))$ such that $\omega\mapsto P_{X|\mathcal F}(\omega,B)$ is a version of $P(X\in B|\mathcal F)$ for each $B\in \mathcal B(\mathbb R)$ . Suppose that $$P_{X_n|\mathcal F}(\omega,\cdot)\Rightarrow P_{X|\mathcal F}(\omega,\cdot) \quad P\text{-almost surely},$$ where $\Rightarrow$ denotes convergence in distribution (weak convergence). (The above definition makes sense since conditional distributions are almost surely unique). Is it then also the case that $P_{X_n}\Rightarrow P_X$ ? I tried the following: Let $f\in C_b(\mathbb R)$ . From the definition of weak convergence we have $$\int f dP_{X_n|\mathcal F}(\omega,\cdot)\to \int f dP_{X|\mathcal F}(\omega,\cdot)\quad \text{as } n\to \infty, \quad P\text{-almost surely.}  $$ Moreover, from the properties of conditional distributions we have $$E[f\circ X|\mathcal F](\omega)=\int f  dP_{X|\mathcal F}(\omega,\cdot)  \quad P\text{-almost surely, } $$ for each $n$ . Therefore $E[f\circ X_n|\mathcal F]\to E[f\circ X|\mathcal F]$ $P$ -almost surely. From the dominated convergence theorem we get $$E[f\circ X_n]=\int f dP_{X_n}\to \int f dP_X \quad \text{as } n\to \infty.$$ As $f\in C_b(\mathbb R)$ was arbitrary, we indeed have $P_{X_n}\Rightarrow P_X$ . Is this correct? Thanks a lot for your help.","['measure-theory', 'weak-convergence', 'conditional-probability', 'conditional-expectation', 'probability-theory']"
4183964,Solve $yy' = \sqrt{y^2+y'^2}y''-y'y''$,Solve $$yy' = \sqrt{y^2+y'^2}y''-y'y''$$ First I set $p = y'$ and $p' = \frac{dp}{dy}p$ to form: $$yp=\sqrt{y^2+p^2}\frac{dp}{dy}p-p\frac{dp}{dy}p \rightarrow y=\sqrt{y^2+p^2}\frac{dp}{dy}-\frac{dp}{dy}p$$ I am trying to come up with a clever substitution to deal with the square root and $p$ of : $$y=\frac{dp}{dy}(\sqrt{y^2+p^2}-p)$$,['ordinary-differential-equations']
4183972,Finding a metric of constant negative curvature on cylinder over a torus ($\mathbb{S}^1 \times \mathbb{S}^1 \times \mathbb{R}$),"I read that a counterexample to show that compacity as a hypothesis in Preissman's theorem is a necessary condition is the manifold $\mathbb{S}^1 \times \mathbb{S}^1 \times \mathbb{R}$ , which admits a complete metric of constant negative sectional curvature. I realize for this to be true, we must be able to see it as a quotient of $\mathbb{H}^3$ , but I don't know why that's true. Can anyone shed some light on this example? What metric is this, explicity?","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4183973,Intuitive explanation of outer product,The inner product between two vectors is the product of length of first vector and the length of projection of second vector on to the first vector. When I take an outer product its result is a matrix. I understand how to calculate it but I am not able to find out what it represents intuitively and why would it be useful. I have searched about it but have not found some simple explanation of it for myself. So any easy to understand explanation of it would be much appreciated. Many thanks!,"['matrices', 'outer-product']"
4184000,"Suppose $f^{\prime\prime}(y) < 0$ at a point $y$, when it is true that $f^{\prime\prime}(z) < 0$ for all $z$ in an interval around $y$?","Consider some function $f: \mathbb{R} \to \mathbb{R}$ . Suppose I computed the second derivative of $f$ and then proceeded to evaluate at a point $y$ , in which I found, $$f^{\prime\prime}(y) < 0$$ When it is true that $f^{\prime\prime}(z) < 0$ for all $z$ in an interval around $y$ ? I am thinking of a $\cap$ looking function. If $f^{\prime\prime}$ is continuous and differentiable everywhere, then I can move some epsilon distance away from that point $y$ and the concavity condition still holds, is this right? Are there any other conditions (possibly more relaxed than continuous differentiability of the second derivative) such that this holds.","['calculus', 'derivatives', 'analysis']"
4184005,How do I solve this ominous integral?,"Let $ n\ge 1 $ be a positive integer. How do I prove the generalization: $$ \int_0^1\frac{\arctan(x)\log^{2n}(x)}{1+x} \, dx =\frac{\pi}{4}\left(1-2^{-2 n}\right) \zeta(2 n+1)(2 n)!+\frac{1}{2} \beta(2 n+2)(2 n) !-\frac{\pi}{16} \lim _{s \rightarrow 0}\left(\frac{d^{2 n}}{d s^{2 n}}\left(\csc \left(\frac{\pi s}{2}\right)\left(\psi\left(\frac{3}{4}-\frac{s}{4}\right)-\psi\left(\frac{1}{4}-\frac{s}{4}\right)\right)\right.\right.$$ The integral was offered to me by my good friend and it looks very difficult, I managed to solve only for $n=1$ and $n=2$ . $$\int_0^1 \frac{\arctan(x)\log^2(x)}{1+x} \, dx=\frac{21}{64}\pi\zeta(3)-\frac{\pi^3}{32}\log(2)-\frac{\pi^2}{24}G;$$ and $$\int_0^1 \frac{\arctan(x)\log^4(x)}{1+x} \, dx=\frac{1395}{256}\pi\zeta(5)-\frac{9}{128}\pi^3\zeta(3)-\frac{7}{480}\pi^4G-\frac{5}{128}\pi^5\log(2)+\frac{\pi^6}{192}-\frac{\pi^2}{1536}\psi^{(3)}\left( \frac{1}{4} \right).$$ where $G$ represents the Catalan’s constant.","['integration', 'definite-integrals', 'calculus', 'catalans-constant', 'zeta-functions']"
4184039,"Why does the technique to find derivative of $y=\frac{ax^2+bx+c}{a'x^2+b'x+c'}$ quickly, works?","I just watched a video about a very good technique to evaluate derivative of $ y=\dfrac{ax^2+bx+c}{a'x^2+b'x+c'} ,\quad a'\neq0$ quickly: $$\large y'=\large\dfrac{{ \begin{vmatrix}{a} && {b} \\ {a'} && {b'}\end{vmatrix} }x^2+\color{red}2{ \begin{vmatrix}{a} && {c} \\ {a'} && {c'}\end{vmatrix} }x+{ \begin{vmatrix}{b} && {c} \\ {b'} && {c'}\end{vmatrix} }}{(a'x^2+b'x+c')^2}$$ I wonder is it possible to prove this is true? Of course we can calculate derivative of $y$ and expanding the terms and compare it with the formula that I mentioned. But is it possible to check this is true intuitively Or without expanding? ( it is obvious that denominator should be $(a'x^2+b'x+c')^2$ )","['calculus', 'derivatives']"
4184059,Confusion regarding intersection of diagonals,"In a heptagon not more than two diagonals intersect at any point other than the vertices, then what should be the number of points of intersection of the diagonals is (excluding the vertices of this heptagon)? Is the answer $35$ or $49$ $35$ approach will be using $nC4 = 7C4 = 35$ $49$ approach will be $14C2 - 7\cdot4C2$ $14C2$ total possibilities of diagonals Total no of diagonal of heptagon $= 14$ No of intersection of diagonal other than at vertices $= 14C2$ But 4 diagonals arises from a single vertex . They will never intersect
So we subtract $4C2$ And there are total 7 vertices. So we subtract $7× 4C2$ Answer $= 14C2-7×4C2 = 49$ Which one is correct please help","['combinatorics', 'geometry']"
4184113,The direct limit of roots of unity,"Let $\mu_n$ denote the group of $n$ -th roots of unity. In $\mathbb{C}$ , this group has exactly $n$ elements. For a positive integer $n$ and a prime number $p$ , using the canonical isomorphisms $\mu_{p^n} \cong \mathbb{Z}/p^n\mathbb{Z}$ for all $n$ as a direct system of factor groups, and the multiplication-by- $p$ homomorphisms $$\mathbb{Z}/p^n\mathbb{Z} \rightarrow \mathbb{Z}/p^{n+1}\mathbb{Z},$$ one sees that the direct limit of this system is the Prüfer group $$\mathbb{Z}(p^\infty) = \{x \in \mathbb{C}^\times: x^{p^n} = 1\,\, \mathrm{for\,\,some} \,\,n\}.$$ Similarly, as seen in this question Union of all finite cyclic groups , we can consider a direct system of cyclic group $\mathbb{Z}/n\mathbb{Z}$ and homomorphisms $\mathbb{Z}/n\mathbb{Z} \rightarrow \mathbb{Z}/m\mathbb{Z}$ if $n|m$ . The resulting direct limit is seen to be $\mathbb{Q}/\mathbb{Z}$ , the set of rational numbers under addition modulo $1$ . We have $$\mathbb{Q}/\mathbb{Z} = \bigoplus_p \mathbb{Z}(p^\infty).$$ Question. Can we view $\mathbb{Q}/\mathbb{Z}$ as being isomorphic to $\mu_\infty = \{x \in \mathbb{C}^\times:x^n = 1\,\,\mathrm{for\,\,some}\,\, n\in \mathbb{Z}^+\}$ , i.e., the subgroup of all roots of unity? If so, is there any way to define the isomorphism?","['group-theory', 'abstract-algebra', 'limits-colimits']"
4184149,Difference of $f$ and $f^{-1}$,"This just came to my mind, and I wonder if there is some criterion for this. Suppose $f$ is differentiable, strictly increasing function and $f^{-1}$ is its inverse. Then, can $f-f^{-1}$ be any differentiable function? In other words, can we find function $f$ such that the difference between $f$ and $f^{-1}$ is exactly the given differentiable function? For example, is there an increasing function $f$ such that $f(x)-f^{-1}(x)=lnx+e^{x^2}$ ?","['functions', 'inverse-function']"
4184166,How to think about the canonical map $i^{-1}i_*\mathcal{F} \to \mathcal{F}$ and similar maps of sheaves defined by adjunction,"Let $i \colon Z \to X$ be the inclusion of a closed subset $Z$ into $X.$ Let $\mathcal{F}$ be a sheaf on $Z.$ I would like to show that the canonical map $i^{-1}i_*\mathcal{F} \to \mathcal{F}$ is an isomorphism. (This is part of the proof of Stacks Project, Lemma 6.32.1 .) There are a few things that are not clear to me, so perhaps I will bold them as I go along. I understand that $$(i^{-1}i_*\mathcal{F})_z = (i_*\mathcal{F})_z = \mathcal{F}_z.$$ However, we know that just because two sheaves have isomorphic stalks does not necessarily mean that the two sheaves are isomorphic. We need to show that the above isomorphism on the stalks is induced by a morphism $i^{-1}i_*\mathcal{F} \to \mathcal{F}.$ Is there a reason why the isomorphism on stalks is induced by the canonical map $i^{-1}i_*\mathcal{F} \to \mathcal{F}?$ What even does the canonical map look like? So if I try to unpack the definitions, I get the following. Suppose $U$ is an open set of $X,$ and so $U \cap Z$ is an open set of $Z.$ Then $i^{-1}i_*\mathcal{F}(U \cap Z)$ is the sheaf associated to \begin{align*}
U \cap Z
&\mapsto \varinjlim_{V \supseteq U\cap Z}i_*\mathcal{F}(V)\\
&= \varinjlim_{V \supseteq U\cap Z} \mathcal{F}(i^{-1}(V))\\
&= \varinjlim_{V \supseteq U\cap Z} \mathcal{F}(V \cap Z).
\end{align*} But the last limit is just taken over all the open sets of $Z$ that contains $U \cap Z,$ so does that mean that this is exactly $\mathcal{F}(U \cap Z)?$ Then by the universal proeprty of sheafification, does this directly show that $i^{-1}i_*\mathcal{F} = \mathcal{F}?$ Another way I tried to think about this is the following. We know that we have a bijection of sets $$\operatorname{Hom}(i^{-1}i_*\mathcal{F},\mathcal{F}) \cong \operatorname{Hom}(i_*\mathcal{F},i_*\mathcal{F}).$$ I would like to guess that our canonical map $\colon i^{-1}i_*\mathcal{F} \to \mathcal{F}$ is induced by the identity map $i_*\mathcal{F} \to i_*\mathcal{F}.$ But does this identification preserve the stalks? More generally, if we have an identification between $\phi \colon f^{-1}\mathcal{G} \to \mathcal{F}$ and $\psi \colon \mathcal{G} \to f_*\mathcal{F},$ where $\mathcal{F}$ is a sheaf on $X$ and $\mathcal{G}$ is a sheaf on $Y.$ If $f(x) = y,$ is there some sort of correspondence between $\phi_x$ and $\psi_y?$ And finally, in case the answers to my previous questions won't answer this, how should I think about the other canonical maps defined by adjunction (for example, $i^*i_*\mathcal{F} \to \mathcal{F}$ and $\mathcal{G} \to i_*i^*\mathcal{G}$ for sheaves of modules)? Edit: I didn't want to expand on my last question because that would make this post a bit too long, but perhaps it is better to include it for the sake of completeness. In particular, let us consider the map $i_*i^*\mathcal{G} \to \mathcal{G}.$ Let $\mathcal{H} = i^*\mathcal{G}.$ Then $\mathcal{H}$ is the sheafification of the presheaf $\mathcal{H}^{\text{pre}}$ defined by $$V \mapsto \mathcal{O}_Z(V) \otimes_{i^{-1}\mathcal{O}_X(V)} \varinjlim_{W \supseteq i(V)}\mathcal{G}(W).$$ Suppose $U$ is an open set of $X.$ Let $V = i^{-1}(V) = U \cap Z.$ Then $i_*i^*\mathcal{G}(U)$ is just $\mathcal{H}(V).$ But note that $U \supseteq i(V),$ and so we have a canonical morphism $$\mathcal{G}(U) \to \varinjlim_{W \supset i(V)}\mathcal{G}(W).$$ But then we also have canonical morphisms $$\varinjlim_{W \supset i(V)}\mathcal{G}(W) \to \mathcal{O}_Z(V) \otimes_{i^{-1}\mathcal{O}_X(V)} \varinjlim_{W \supseteq i(V)}\mathcal{G}(W) = i_*\mathcal{H}^{\text{pre}}(U) \to i_*\mathcal{H}(U).$$ (Is everything correct up to here?) So this defines a map $\mathcal{G} \to i_*i^*\mathcal{G}.$ And if we were to check the stalks, can we just follow the above composition of maps (at the level of stalks) because stalks behave nicely with direct limits and tensor products?","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
4184194,"What is $\lim_{(x,y) \to (0,0)} \dfrac{1 - \cos x}{x+y}$?","I want to understand this multivariate limit. WolframAlpha says $\lim_{(x,y) \to (0,0)} \dfrac{1 - \cos x}{x+y} = 0$ But what if I take a curve $y = -x$ , then the limit doesn't exist, right? Wouldn't this make the limit inexistent? I tried to prove using sandwich theorem and definition, but didn't get anything good. Any help would be appreciated.","['multivariable-calculus', 'limits', 'calculus']"
4184234,Recognizing semidirect products in classification of groups of a given order,"I am reading about classifications of groups of order $30$ and $12$ in Dummit and Foote. I understand the general procedure for such classifications. However, I have trouble recognizing the resulting semidirect products as ""its more descriptive form""; for example, as a direct product of a cyclic group and a dihedral group. Below are the specific examples: Let $G$ be a group of order $30$ . Let $H = \langle a\rangle \times \langle b \rangle \cong Z_5 \times Z_3$ be normal subgroup of $G$ of order $15$ . Let $K = \langle k \rangle$ be the Sylow $2$ - subgroup of $G$ . Let $\phi_1: K \to \mbox{Aut}(H)$ that maps $k$ to $\{a \mapsto a, b \mapsto b^{-1}\}$ . Then $G_1 = H\rtimes_{\phi_1}K \cong Z_5 \times D_6$ (note that in this semidirect product $k$ centralizes the element $a$ of $H$ of order $5$ , so the factorization as a direct product is $\langle a \rangle \times \langle b,k \rangle$ ). Let $\phi_2: K \to \mbox{Aut}(H)$ that maps $k$ to $\{a \mapsto a^{-1}, b \mapsto b\}$ . Then $G_2 = H\rtimes_{\phi_2}K \cong Z_3 \times D_{10}$ (note that in this semidirect product $k$ centralizes the element $b$ of $H$ of order $3$ , so the factorization as a direct product is $\langle b \rangle \times \langle a,k \rangle$ ). Let $\phi_3: K \to \mbox{Aut}(H)$ that maps $k$ to $\{a \mapsto a^{-1}, b \mapsto b^{-1}\}$ . Then $G_3 = H\rtimes_{\phi_2}K \cong D_{30}$ . Let $G$ be a group of order $12$ . Let $V$ be a Sylow $2$ -subgroup of $G$ and $T$ be a Sylow $3$ -subgroup of $G$ . Suppose $T \trianglelefteq G$ and $V = \langle a\rangle \times \langle b \rangle \cong Z_2 \times Z_2$ . Put $\mbox{Aut}(T) = \langle \lambda \rangle \cong Z_2$ . Then there are three nontrivial homomorphisms from $V$ into $\mbox{Aut}(T)$ determined by specifying their kernels as one of the three subgroups of order $2$ in $V$ . For example, $\phi_1(a) = \lambda$ and $\phi_1(b) = \lambda$ has kernel $\langle ab \rangle$ . If $\phi_2$ and $\phi_3$ have kernels $\langle a \rangle$ and $\langle b \rangle$ , respectively, then the resulting three semidirect products are all isomorphic to $S_3 \times Z_2$ , where the $Z_2$ direct factor is the kernel of $\phi_i$ . For example, $T \rtimes_{\phi_1} V = \langle a, T \rangle \times \langle ab \rangle$ . Can someone offer a more detailed explanation for the bold parts? Are there any results that I am not aware of that make the conclusion follow easily?","['direct-product', 'dihedral-groups', 'semidirect-product', 'sylow-theory', 'group-theory']"
4184249,How to prove $\sqrt {75025} + \sqrt {121393} + \sqrt {196418} + \sqrt{317811} \approx \sqrt {514229} + \sqrt {832040}$?,"Let $a  = \sqrt {75025} + \sqrt {121393} + \sqrt {196418} + \sqrt{317811}$ and $b = \sqrt  {514229} + \sqrt {832040}$ . By using SageMath we can see that $$ a - b \approx 2.95301560981898 \cdot 10^{-9} $$ That means almost nine digits of accuracy! To investigate any particular reason why these surprising digits of accuracy come I considered the function $$ f(x)=  \sqrt{x +317811} + \sqrt{x + 196418} + \sqrt{x + 121393} + \sqrt{x + 75025} -\sqrt{x + 832040} - \sqrt{x + 514229}$$ The Taylor series of $f$ around $x =0$ with approximate coefficients looks like $$ f(x) = f(0) + 0.00403020948350145x  -2.13362649294736 \cdot 10^{-8}\frac12 x^2 + O(x^3) $$ If $\alpha$ is a root of the equation $f(x) = 0$ where $\alpha $ is very close to $0$ (definitely there is a root between $-1$ and $0$ ) then $$0 = f(\alpha) = f(0) + 0.00403020948350145 \alpha  -2.13362649294736 \cdot 10^{-8}\frac12 \alpha^2 +\text{higher error terms} $$ Of course, we can use computer programs to find a bound on $\alpha$ but the whole process is not mathematically elegant. Certainly, $\alpha$ is a root of some polynomial of higher degree, so it may be difficult to find an expression in terms of radicals for $\alpha$ or may not be possible at all. Are there any other reasons for this level of accuracy?","['radicals', 'calculus', 'functions', 'approximation']"
4184299,How to show associativity with set of only two elements.,"Considering the set $S= \{-1, 1\}$ , I need to show if associativity holds under binary operation multiplication. We used to take any three elements to verify the associative property, but here only two are given. Can I consider any one twice ? like $1\cdot ((-1)\cdot (-1))= 1\cdot 1= 1$ and also $(1\cdot (-1))\cdot (-1)= (-1)\cdot (-1)= 1$ , now in this case associativity holds, but I'm not sure if this is a valid approach, or should I say since the set has only two elements, we can't verify associativity.","['elementary-set-theory', 'self-learning', 'binary-operations', 'abstract-algebra']"
4184315,$E(X^+) < \infty$ if and only if $\int_{\alpha}^{\infty}(−\log F(u)) du < \infty$ for some $\alpha > 0.$ where $F$ is distribution function of $X$,Let $X$ be a random variable with distribution function $F$ . Show that $E(X^+) < \infty$ if and only if $\int_{\alpha} ^{\infty}(−\log F(u)) du < \infty$ for some $\alpha > 0$ . Here $X^+ = X\mathbb{1}_{(X>0)}$ I could prove $\int_{\alpha} ^{\infty}(−\log F(u)) du < ∞$ for some $\alpha > 0 $ implies $E(X^+)<\infty$ . Any hints on how to prove the other direction will be helpful.,"['measure-theory', 'probability-theory', 'probability']"
4184345,Stability of the null solution in an autonomous system,"I am supossed to study the instalibity of the null solution of the following (autonomous) system: \begin{equation*}
\begin{cases}
x' = -x+y-x^2 \\[5pt]
y' =3x-x^2-2y
\end{cases}
\end{equation*} My resolution so far and my problems. Clearly we are working with an autonomous system with the form $z' = f(z)$ , where $z' = [x'\hspace{.3cm} y']^T$ and $f(z) = f(x,y) = (-x+y-x^2,3x-x^2-2y)$ . Obviously, $f \in C^1$ in $\mathbb{R^2}$ and $f(0,0)=(0,0)$ ( $(0,0)$ is an equilibruim point). Let's compute the Jacobian of $f$ . \begin{equation*}
J_f(x,y) = \begin{pmatrix}-1-2x \quad \quad 1 \\[5pt]
3-2x \quad \quad -2
\end{pmatrix}
\Rightarrow J_f(0,0) = \begin{pmatrix}-1 \quad \quad1 \\[5pt] 3 \quad \quad -2
\end{pmatrix}
\end{equation*} Now we compute the eigenvalues of $J_f(0,0)$ and we get the following result: \begin{equation*}
\lambda_1 = \frac{-3+\sqrt{13}}{2} \quad \vee \quad \lambda_2 = \frac{-3-\sqrt{13}}{2}  
\end{equation*} Which make us conclude that $J_f(0,0)$ isn't hurwitz (it has a positive real eigenvalue). So, I can't conclude nothing about the stability of the null solution using this method. I would now try to find a Lupyanov function and use its direct method since these are the two methods I've been taught. I have tried multiple Lupyanov functions and I can't figure one out that actually works for this case. So this is my question, if someone can help me finding a Lupyanov function I would be really thankfull.","['matrices', 'stability-theory', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4184407,An Interesting Property Of Fibonacci Numbers,"I gave an exam today that had a question which goes like- a. We will call a binary sequence 'sparse' if there are no two consecutive $1$ 's in it. Calculate number of sparse strings of length $n$ . b. Prove that every integer can be expressed as the sum of non-consecutive distinct Fibonacci numbers. c. Prove that the sum in (b) is unique. d. Either derive (a) using (b) and (c) or derive (c) using (a) and (b) . Now, I know very well how to find the recurrence relation in (a) . I also studied the proofs of (b) and (c) a couple of years back, and didn't have much difficulty in constructing them from memory. But, I got stuck in (d) . I was trying to understand how to connect the results of (a) , (b) and (c) . After giving it a thought, I came up with this expression \begin{equation*}
S(x_1,x_2,\dots ,x_n)=x_1F_1+x_2F_2+\dots +x_nF_n
\end{equation*} for a given $k$ where $F_n$ is the largest Fibonacci number before $k$ and $x_i\in \{0,1\}\;\forall i\in\{1,2,\dots ,n\}$ . Now, if we want $S=k$ , then the sequence $x_1,x_2,\dots ,x_n$ must be sparse. But, from (a) , we know that there are only $F_{n+2}$ such sparse sequences. Also, by (b) , there exists a choice $(y_1,y_2,\dots ,y_n)=(x_1,x_2,\dots ,x_n)$ with $y_i\in \{0,1\}\;\forall i\in\{1,2,\dots ,n\}$ such that $S(y_1,y_2,\dots ,y_n)=k$ . Also, it is clear that the values of $S$ that we can get by changing the choices of $x_i$ 's are arranged in a strict order (i.e., any two of them cannot be equal). So, there is exactly one choice of $x_i$ 's such that $S=k$ . But, there seems to be something fishy about my proof. I have not used the fact that there are $F_{n+1}$ choices of $x_i$ 's anywhere in here. So, is my proof correct? If not, then is my approach correct? Can anybody please help me with this?","['elementary-number-theory', 'fibonacci-numbers', 'sequences-and-series']"
4184457,$5^x - y^2 = 4$ Diophantine Equation,"I wrote a Diophantine equation and tried solving it. Then I got stuck at a stage of the solution. Problem: Find all $(x,y)$ positive integer pairs that satisfy the equation $5^x - y^2=4$ . My Partial Solution: If $x$ is an even number then $x=2n\quad$ ( $n \geq 1$ an integer). Therefore $$ (5^n - y)(5^n + y) = 4$$ and we can write \begin{equation*}
\left\{
\begin{split}
 5^x-y & = 1  \\
 5^x + y & = 4
\end{split}
\right.
\end{equation*} Thus, $2\cdot 5^x = 5$ and there is no positive integer solution in this case. If $x$ is an odd number, $\bullet$ For $x=1$ ; $\quad 5^1 -y^2=4 \implies y=1$ . $\bullet$ For $x=3$ ; $\quad 5^3 -y^2=4 \implies y=11$ . $\bullet$ For $x\geq 5$ ; I thought of finding a contradiction using modular arithmetic. For example; $x=2k + 3 , \quad$ ( $k\geq 1 $ an integer) $125\cdot 25^k - y^2 = 4$ . In $\mod 24$ , $$5 - y^2 \equiv 4 \pmod{24}$$ But this is not a contradiction. So, I failed. How can I tell if the equation has a solution for $x>3$ or not? Thanks for your interest.","['number-theory', 'diophantine-equations']"
4184503,symplectic geometry: help showing the cotangent lift of an action to a symplectic manifold is a symplectic action,"I am following da Silva's lectures on symplectic geometry. She defines the lift of a diffeomorphism as follows: Let $X_1$ and $X_2$ be $n$ -dimensional manifolds with cotangent bundles $M_1=T^*X_1$ and $M_2=T^*X_2$ and suppose $f:X_1 \rightarrow X_2$ is a diffeomorphism. Then the lift $f_\#:M_1 \rightarrow M_2$ is defined $$f_\#(p_1) = p_2 = (x_2, \xi_2) = (f(x_1), \xi_2)
$$ where $\xi_1 = (f_{x_1})^*\xi_2 = \xi_{2}\circ df_{x_1}$ and $\xi_2 \in (T^*X_2)_{f(x_1)}$ $f_\#\rvert_{T^*X_1}$ is therefore the inverse map of $(f_{x_1})^*$ and so $\xi_2 = [(f_{x_1})^*]^{-1}\xi_1 = (f^{-1}_{x_1})^*$ Now let $(M, \omega)$ be the symplectic manifold obtained by equipping the tangent bundle $M=T^*N$ with the canonical 2-form $\omega = \sum_i dx^i \wedge d\xi^i$ and let the Lie Group G act on N: $$\psi: G \rightarrow \text{Diff(M)}, \quad g \rightarrow \psi_g \\
$$ I am trying to prove the contangent lift of the action is symplectic (and hamiltonian) We must have $$(\psi_g)_\#(p) = (\psi_g)_\#(x, \xi) = (\psi_g(x), (\psi_g^{-1})^*(\xi)) = (\psi_g(x), (\psi_{g^{-1}})^*(\xi))
$$ This must preserve the symplectic form, i.e. $$((f_\#)^*\omega)_p (u, v) = \omega_{f_\#(p)}(df_p(u), df_p(v))
$$ I am unsure what to do from here. I can see the symplectic form takes in two coordinates that transform in seemingly inverse ways. Could someone point me in the right direction ? It would also help if anyone spots any errors in the way I state the problem, I have studied physics so far and am slightly out my depth with the mathematics. EDIT: Solution using tautological one-form (this method is in da Silva's notes) We want to show $(f_\#)^*\alpha_2 = \alpha_1$ The tautological form on $M_1$ , $M_2$ is defined: $$(\alpha_1)_{p_1} = \pi^*_{p_1}\xi_1, \quad \quad (\alpha_2)_{p_2} = \pi^*_{p_2}\xi_2$$ So we have $$\begin{align}(f_\#)^*_{p_1}(\alpha_2)_{p_2} &= (f_\#)^*_{p_1}\pi^*_{p_2}(\xi_2)] \\
&= (\pi_2 \circ f_\#)^*_{p_1} \xi_2 \\
&= (f \circ \pi_1)^*_{p_1} \xi_2 \quad\quad \text{(The lift is constructed as such)} \\
&= (\pi_1)^*_{p_1}f^*_{x_1}\xi_2 \\
&= (\pi_1)^*_{p_1}\xi_1 \quad\quad \text{(By definition of the lift)}\\
&= (\alpha_1)_{p_1}
\end{align}
$$ Therefore $f_\#$ preserves the tautological form as well as $d\alpha_1$ , the canonical 2-form which is symplectic on $M_1$ Therefore the lift of the diffeomorphism $\psi_g:X_1 \rightarrow X_2$ is: $(\psi_\#)_g:M_1 \rightarrow M_2$ and it is a symplectomorphism. The lift of the action $\psi_\#$ is therefore symplectic. It can also be shown the action is hamiltonian. It seems obvious since we are preserving the tautological one-form, although I'm not sure on the proof.","['symplectic-geometry', 'co-tangent-space', 'manifolds', 'group-actions', 'differential-geometry']"
4184539,Solve $y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x)$,"When $y = \frac{1}{2}$ , $y' = 1$ , for $x=1$ Solve $$y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x)$$ First I converted it to $$p' +\frac{1}{x(1+\ln(x))}y' = \frac{2+\ln(x)}{1+\ln(x)} $$ Which looks like a nice first order differential equation. However, when $I = 1+ \ln(x)$ you solve for $p$ you get the following DE: $$y' = \frac{2x+\frac{1}{x}+C}{1+\ln(x)}$$ Which does not produce the right result when I put it through a calculator. Where is the mistake?",['ordinary-differential-equations']
4184573,Hilbert-Schmidt integral operator for a positive definite kernel is positive,"Let $(X, \mathcal{A}, \mu)$ be a measure space and $k \colon X \times X \to \mathbb{R}$ be a measurable positive-definite kernel such that $\int_{X} \int_X k^2(x,y) \,\mathrm{d}\mu(x) \, \mathrm{d}\mu(y) < \infty$ . Then it is well known that the operator $T \colon L^2(\mu) \to L^2(\mu)$ defined by $$
Tf(x) := \int_X k(x,y) f(y) \, \mathrm{d}\mu(y)
$$ is a Hilbert-Schmidt operator. Since $k(x,y) = k(y,x)$ , it is easy to see from Fubini's theorem that $$
\langle Tf, g \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) g(x) \, \mathrm{d} \mu(x) = \int_X \left( \int_X k(y,x) g(x) \, \mathrm{d} \mu(x) \right) f(y) \, \mathrm{d} \mu(y) = \langle f, Tg \rangle
$$ and hence $T$ is self-adjoint. How do I show that $T$ is positive? That is, I want to show that for every $f \in L^2(\mu)$ $$
\langle Tf, f \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) f(x) \, \mathrm{d} \mu(x) \ge 0
$$ We need to somehow use the positive-definiteness of $k$ , but I am unable to do that unless I assume $\mu$ is something simple like a counting measure. Ultimately, I want to be able to say that the eigenvalues of $T$ are such that $\lambda_1 \ge \lambda_2 \ge \cdots > 0$ .","['measure-theory', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'integral-transforms']"
4184643,Inclusion of Sobolev spaces (from a proof by Bogachev - Krylov - Röckner - Shaposhnikov),"In a book by Bogachev-Krylov-Rockner-Shaposhnikov, I found the following statement that concludes a proof but I do not understand. I underlined in red the critical parts. $\rho(\cdot,t)>0$ is a probability density function solving a certain parabolic PDE in a weak sense. The function $f_\epsilon$ approximates $\rho(\cdot,t)$ as $\epsilon\to0$ , precisely: $$ f_\epsilon(x,t) \,:=\, \big(\rho(\cdot,t)*w_\epsilon\big)(x) \,+\, \epsilon\,\max(1,|x|)^{-d-1}$$ where $w_\epsilon(x)=\frac{1}{(2\pi\epsilon^2)^{d/2}}\,e^{-|x|^2/(2\epsilon^2)}\,$ .
The Sobolev space $W^{2,1}$ denotes those functions with weak derivatives up to the first order belonging to $L^2$ (notice that the indices of differentiability and integrability are reversed with respect to ""usual"" notation). How can I deduce the last red statement ? The first red part is clear, since $$\nabla \sqrt{\rho(\cdot,t)} \,=\, \frac{1}{2}\,\frac{\nabla\rho(\cdot,t)}{\sqrt{\rho(\cdot,t)}}$$ hence the approximation argument shows that this weak gradient exists and belongs to $L^2(\mathbb R^d)$ . Now, since $\rho(\cdot,t)>0$ is a probability density function (by hypothesis), the second red part follows from the first red part by Cauchy-Schwarz inequality: $$\int_{\mathbb R^d} |\nabla\rho(x,t)|\,d x \,\leq\, \int_{\mathbb R^d} \frac{|\nabla\rho(x,t)|^2}{\rho(x,t)}\,d x \ \underbrace{\int_{\mathbb R^d} \rho(x,t)\,d x}_{=\,1} \ <\infty $$ for almost all $t\in(0,\tau)$ , hence $\rho(\cdot,t)\in W^{1,1}(\mathbb R^d)$ for almost all $t\in(0,\tau)$ . I don't understand why the last red statement holds true. $\sqrt{\rho}\in\mathbb H^{2,2}$ means -by definition of this space- that $\partial_i\partial_j \sqrt{\rho(\cdot,t)}\in L^2(\mathbb R^d)$ for almost all $t\in(0,\tau)$ and $$\int_0^\tau \int|\partial_i\partial_j \sqrt{\rho(x,t)}|^2 d x\, dt <\infty\,.$$ Why this is the case? Why the weak second derivatives even exist? Edit. The full reference is Theorem 7.4.1 in the book ""Fokker-Planck-Kolmogorov equations"" by Bogachev, Krylov, Röckner, Shaposhnikov. Here's the statement:","['weak-derivatives', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4184656,Derivative Product rule for non-commutative objects?,"lets say I have objects $f$ and $g$ , for which one can define a derivative with the typical properties. The product rule would be expected to be $$
d(fg)=(df)g+f\,dg
$$ But what if $f$ and $g$ are not commutative? $$
[f,g]\neq 0\implies d(fg)=\text{?}
$$ Does the product rule get modified?",['derivatives']
4184662,Projections of the Product Topology,"I've recently been studying product topologies from Schaum's General Topology and it appears that this is where topology begins to get fairly abstract (i.e. working in several dimensions). In the Product Topology chapter, the author defines A projection is a function $\pi_{j_0}: X \to X_{j_0}$ where $X=\prod X_i$ is the product set and $X_{j_0}$ is a coordinate space. The function is given by $\pi_{j_0}(\langle a_i: i \in I\rangle)= a_{j_0}$ The author also goes on to say that the product topology is the topology generated by these projections. I'd like to pose two questions: Is it safe to say that a projection is merely a function that picks the $k^{th}$ element from an $n$ -dimensional point in a product space and returns the value from $X_k$ ? e.g. let $\hat{x} \in \mathbb{R}^3$ with $\hat{x}= (4,5,6),$ then $\pi_2(\hat{x}) = 5$ . Would that be sensible? I'm having trouble understanding how the projections generate a topology. By ""generate,"" surely we mean ""take arbitrary unions of,"" but I am failing to see how $\displaystyle\bigcup \pi_i(\hat{x})$ is a base for $\prod X_i$ .","['general-topology', 'projection']"
4184678,Is true that $\det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B$ when $B$ is a symmetric matrix?,"If $B$ is a square symmetric matrix of order $n\times n$ then is true that $$
\det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B
$$ where $A$ is a matrix of order $n\times m$ ? Unfortunately I did not find a counterexample: in particular I tried to show that $$
A^T\cdot B\cdot A=A\cdot A^T\cdot B
$$ so that the statement follows directely applying the Binet formula. So could someone help me, please?","['determinant', 'matrices', 'solution-verification', 'linear-algebra', 'symmetric-matrices']"
4184681,Clairaut differential equations and elliptic discriminants,"I was solving this math.SE question , which was asking to solve the Clairaut differential equation $y= xy' - (y')^3$ . Just to have nicer signs, I then looked at the equivalent equation $$ y= xy' + (y')^3 .$$ The main trajectories of this differential equation are: the algebraic curve given by $27y^2 + 4x^3 = 0$ ; all the lines tangent to this curve. Many of us have already seen this curve somewhere: in fact, the discriminant of the elliptic curve (in Weierstrass form) $$ y^2 = x^3 +ax+b $$ is $$ \Delta = 4a^3+27b^2.$$ My question is: What is the relation between this Clairaut's differential equation and the discriminant of elliptic curves in Weierstrass form? In other words, I ask if this is just a coincidence, or  there is indeed some natural construction that relates the two. On one hand, it is easy to verify that the Clairaut equation is satisfied by the discriminant. It should be easy to verify this without using the discriminant formula, in this fashion: ""Suppose that an elliptic Weierstrass equation with parameters $(a,b)$ is singular (this is a reformulation of $\Delta =0$ ); then if we move the parameters in a curvy special direction, dictated by the Clairaut differential equation, then the cubic Weierstrass curve remains singular. "" This approach is a bit sketchy, because actually also the tangent lines to the zero-discriminant locus are solutions. Then, I guess that one should study what happens to the Weierstrass curve when the parameters move along these lines, and identify some ""quasi-invariant"" or ""property"" along these lines. This property should be, in some sense, some kind of generalization of the property ""the weierstrass curve is singular"". Then the Clairaut differential equation should be a differential equation valid ""along these quasi-invariants"". For completeness, here is the general equation of the tangent lines to the curve $4x^3+27y^2=0$ : $$y = mx+m^3,$$ where $m$ is some parameter. As a follow-up, it would be great if there were some relationship between Clairaut differential equations (or some other class of differential equations) and discriminants of families of algebraic cuves (or better, these ""generalized discriminants"" that include somehow the lines tangents to the ""zero-discriminant locus""). But this would be perhaps a separate question.","['geometric-invariant', 'discriminant', 'elliptic-curves', 'ordinary-differential-equations', 'algebraic-geometry']"
4184704,Source for Peano article written in symbolic logic,"L.E.J. Brouwer states in his 1912 address titled ""Intuitionism and Formalism"" that Peano published one of his most important discoveries concerning the existence of integrals of real differential equations in the Mathematische Annalen in the language of symbolic logic; the result was that it could only be read by a few of the initiated and that it did not become generally available until one of these had translated the article into German. I can only find this version but this is a French translation. Does anyone know where I can find Peano's original article written ""in the language of symbolic logic"" referenced by Brouwer?","['ordinary-differential-equations', 'logic', 'reference-request', 'online-resources', 'math-history']"
4184720,Proving radius of injectivity goes to zero if volume goes to zero,"Let $n_0 \in \mathbb{N}$ be an odd number. Prove there exists a sequence of compact manifolds, $M_k, k \in \mathbb{N}$ , all with the same dimension $n_0$ and the same constant sectional curvature $K_{M_k} \equiv 1$ whose volume satisfies $$\lim_{k \to \infty} \operatorname{Vol}(M_k) = 0$$ Conclude that the radius of injectivity also goes to zero, that is, $\displaystyle{\lim_{k \to \infty}} i(M_k) = 0$ . Since $n_0 = 2m - 1$ for some $m \in \mathbb{N}$ , we can consider the unit sphere $\mathbb{S}^{2m-1} \subset \mathbb{R}^{2n} = \mathbb{C}^n$ and the quotient manifold $\displaystyle{\mathbb{S}^{2m-1} \over \mathbb{Z}_p}$ (a lens space). It's straightforward to show the volume of this quotient is the volume of the sphere over $p$ , so letting $p \to \infty$ we get the desired sequence of manifolds. But how can we conclude the injectivity radius also goes to zero? I think maybe we could show that $i(M_k) = \dfrac{\pi}{k}$ , but I haven't been able to prove this. I'd appreciate any help.","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4184731,"Given both foci, a point on an ellipse and a random secant line how can one construct the meeting of the curves","We are given three points: $F_1, F_2$ that are the two foci of the ellipse $\mathcal E$ and point $A$ that belongs to $\mathcal E$ . We are also given a random line $\ell$ (let's assume it does not contain $A$ ). How can we find points $B$ and $C$ such that $\{B,C\} = \mathcal E \cap \ell \neq \emptyset$ using straight edge and compass? We can easily get: center of $\mathcal E$ as it is the midpoint of $F_1F_2$ axes of symetry of $\mathcal E$ major axis: $X = \odot(A,AF_1) \cap \overleftrightarrow{AF_2}$ (with $X$ and $F_2$ in disjointed halfplanes of $AF_1$ ). if $M$ is the midpoint of $XF_2$ , then $a = F_2M$ so we have the vertices of the ellipse.","['euclidean-geometry', 'conic-sections', 'geometry']"
4184747,Is every algebraically closed field an algebraic extension of a nontrivial subfield?,"I was wondering if every algebraically closed field is an algebraic extension of a nontrivial subfield. Suppose $L$ is an algebraically closed field. By nontrivial subfield, I mean a subfield $K \subset L, K \neq L$ . I ask this because I was wondering if every algebraically closed field was the algebraic closure of some subfield. This does seem like the case sometimes. Suppose we take some $\alpha \in L$ such that $\alpha$ has nontrivial degree over the base field of $L$ . Take the maximal subfield of $L$ that does not contain $\alpha$ , $K'$ . Then $K$ ' has finite degree under $L$ and we're done. The obvious problem with this is when every element of $L$ is transcendental over the base field, in which case the claim seems less straightforward. But honestly I doubt that the claim is true because otherwise we would probably equate the two terms. Any guidance would be greatly appreciated.","['field-theory', 'abstract-algebra']"
4184763,Delightful integral: $\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx.$,"A friend of mine proposed me this integral which I find to be very interesting. I managed to find with the help of software that: $$\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\ln ^2\left(\cos \left(x\right)\right)\:dx=\frac{155}{128}\zeta \left(5\right)+\frac{13}{32}\zeta \left(2\right)\zeta \left(3\right)-\operatorname{Li}_5\left(\frac{1}{2}\right)$$ $$-\frac{49}{32}\ln \left(2\right)\zeta \left(4\right)-\frac{5}{6}\ln ^3\left(2\right)\zeta \left(2\right)+\frac{1}{120}\ln ^5\left(2\right).$$ Where $\zeta \left(z\right)$ denotes the Riemann zeta function and $\operatorname{Li}_n\left(z\right)$ denotes the Polylogarithm function. If $x=\tan\left(t\right)$ is used it yields: $$\frac{1}{4}\int _0^{\infty }\frac{\arctan \left(x\right)\ln \left(x\right)\ln ^2\left(1+x^2\right)}{1+x^2}\:dx-\frac{1}{8}\int _0^{\infty }\frac{\arctan \left(x\right)\ln ^3\left(1+x^2\right)}{1+x^2}\:dx.$$ I know of ways to evaluate the latter integral but the former is very difficult and the techniques that work for the $2$ nd do not work for the $1$ st. Integrating by parts gives: $$-\frac{1}{2}\int _0^{\frac{\pi }{2}}x^2\cot \left(x\right)\ln ^2\left(\cos \left(x\right)\right)\:dx-\int _0^{\frac{\pi }{2}}x^2\tan \left(x\right)\ln \left(\sin \left(x\right)\right)\ln \left(\cos \left(x\right)\right)\:dx.$$ And I find myself in the same situation where I can evaluate the $1$ st integral but the techniques that work for it aren't as effective for the $2$ nd, are there any better approaches for the main integral? Please do not post results without proving them since that is not what I'm looking for, thanks.","['integration', 'definite-integrals', 'real-analysis', 'complex-analysis', 'calculus']"
4184772,What does multiplying a pdf mean?,"Suppose I have two (independent) continuous random variables $X$ and $Y$ with pdfs $f(x)$ and $g(x)$ respectively.  It is well-known that $f(x)g(x)$ is not the pdf of $XY$ ; in fact, $f(x)g(x)$ may not be a pdf at all (see Appendix). On the other hand, (assuming $X$ and $Y$ have common support — h/t Thomas Andrews) it's easy enough to make $f(x)g(x)$ a pdf: just rescale by $\left(\int{f(x)g(x)\,dx}\right)^{-1}$ .  Then we have the following interesting facts: If $f(x)=g(x)=1[0\leq x\leq 1]$ (uniform distribution), then $f(x)g(x)$ is also the pdf of the uniform distribution. If $f(x)=\lambda_fe^{-\lambda_fx}\cdot1[0\leq x]$ , $g(x)=\lambda_ge^{-\lambda_gx}\cdot1[0\leq x]$ (exponential distribution), then $f(x)g(x)\propto(\lambda_f+\lambda_g)e^{-(\lambda_f+\lambda_g)x}\cdot1[0\leq x]$ , also the pdf of the exponential distribution. Same for two normal distributions. Multiplying a normal and an exponential gives a normal. If $X$ is as in the appendix ( $2x1[0\leq x\leq 1]$ ) and $Y$ is supported on $[0,1]$ with standard deviation $\sigma$ , then $f(x)g(x)$ after rescaling has mean $2\sigma^2$ . So clearly there's something going on here. Is there a probabilistic interpretation for $f(x)g(x)$ ? Appendix For example, let $$f(x)=g(x)=2x\cdot 1[0\leq x\leq1]$$ (where $1[A]$ is the indicator function of $A$ ).  Then $$\int_{\mathbb{R}}{f(x)g(x)\,dx}=\int_0^1{4x^2\,dx}=\frac{4}{3}\neq1$$ Thus $f(x)g(x)$ isn't even a pdf. For completeness, the law of $XY$ is as follows: \begin{align*}
\mathbb{P}[XY\leq x]&=\int_{\mathbb{R}}{f(s)\mathbb{P}\left[Y\leq\frac{x}{s}\right]\,ds} \\
&=\int_0^1{2s\min{(1,(x/s)^2)}\,ds} \\
&=\int_0^x{2s\,ds}+\int_x^1{2x/s\,ds} \\
&=x^2-2x\ln{(x)}
\end{align*} To get the pdf, differentiate; the result is precisely $2(x-\ln{(x)}-1)1[0\leq x\leq1]$ .","['probability-distributions', 'probability']"
4184814,Quotient of $\mathbb{R^*}$ by cyclic group product,"For any $k \in \mathbb{R^*}, \langle k\rangle$ is a normal subgroup. Consider the case $k \neq 1, k>0$ . Then, via the surjective homomorphism $$\varphi: \mathbb{R^*} \to \mathbb{T}\times\{\pm1\}, \quad x\mapsto (e^{2\pi i\cdot \log_{k}(|x|)}, \: \textrm{sign}(x))$$ $$\implies \mathbb{R^*}/\langle k\rangle \simeq \mathbb{T} \times\{\pm1\}.$$ by the First Isomorphism Theorem (where $\mathbb{T}$ is the circle of unit radius in the complex plane). But for any $M, N \trianglelefteq  G$ , $$\langle M, N\rangle = MN \trianglelefteq G$$ $$\implies \langle k_1,...,k_n\rangle \trianglelefteq \mathbb{R^*}. $$ There turns out to be a few different cases for $n=1$ , so suppose that $k_1, ... k_n \in \mathbb{R^*},$ with $k_i \neq 1, k_i >0$ . To which group is $\mathbb{R^*}/\langle k_1,...,k_n\rangle$ isomorphic? I can't spot a friendly homomorphism for this one so easily.","['group-theory', 'quotient-group', 'infinite-groups']"
4184823,Proving $\int_{0}^{\pi/2} \ln \sin x ~ \ln \cos x~ dx=-\frac{\pi^3}{48}+\frac{\pi}{2}\ln^22$,"Interestingly the integral $$I=\int_{0}^{\pi/2} \ln \sin x ~ \ln \cos x~ dx~~~~~~~~~(1)$$ is doable by hand by using Fourier series: $$ \ln \sin x=-\sum_{j=1}^{\infty} \frac{\cos 2j x}{j}-\ln 2,\quad \ln \cos x=\sum_{k=1}^{\infty} (-1)^{k+1} \cfrac{\cos 2kx}{k}-\ln 2, ~x\in [0,\pi/2].~~~~~~~~~~(2)$$ $$I=\sum_{j=1}^{\infty} \sum_{k=1}^{\infty} (-1)^{k}\int_{0}^{\pi/2} \frac{\cos 2jx \cos 2kx}{jk} dx-\ln 2 \int_{0}^{\pi/2}\left (\sum_{j=1}^{\infty} \frac{\cos 2jx}{j}+\sum_{k=1}^{\infty} (-1)^k \frac{\cos 2kx}{k}\right) dx+\frac{\pi}{2}\ln^22~~~~~~~~~~~(3)$$ The second and third integrals vanish, then $$\implies I=\frac{1}{2} \sum_{j=1}^{\infty} \sum_{k=1}^{\infty} (-1)^k \int_{0}^{\pi/2}\frac{\cos2(j+k)x+\cos2(j-k)x}{jk} dx+\frac{\pi}{2}\ln^22~~~~~~~~~~~~~~~~(4)$$ $$\implies I=\sum_{j=1}^{\infty} \sum_{k=1}^{\infty}(-1)^k \frac{\sin2(j-k)\pi}{4jk(j-k)}+\frac{\pi}{2}\ln^22~~~~~~~~~~~~~~~~(5)$$ Taking limit $(j-k)\to 0$ , we get $$I=\frac{\pi}{4} \sum_{k=1}^{\infty} \frac{(-1)^k }{k^2}+\frac{\pi}{2}\ln^22=
-\frac{\pi^3}{48}+\frac{\pi}{2}\ln^22.~~~~~~~~~~~~~~~(6)$$ The question is: How else we can get this interesting integral (1)?","['integration', 'summation', 'definite-integrals', 'sequences-and-series']"
4184824,The number of elements of order $2$ in an infinite group,"I've just proved that for $|G|$ finite, the number of elements of order $2$ is odd. But how about the case $|G|$ infinite? (Here, there is assumption that the set of elements of order $2$ is nonempty.)","['group-theory', 'abstract-algebra', 'infinite-groups']"
4184882,Generalization of weak law of large numbers,"In Exercise 5.2.12 of Chung Kai-lai's A Course in Probability Theory, there is a question: Let $\{X_n\}$ be pairwise independent with a common distribution function $F$ (namely, identically distributed) such that (i) $\displaystyle \int_{|x|\leq n}x \, dF(x)=o(1)$ , (ii) $\displaystyle n \int_{|x|>n} \, dF(x)=o(1)$ . Then $\displaystyle\frac{\sum_{j=1}^{n}X_{j}}{n}\rightarrow 0 $ in probability. From condition (i) we can get $\operatorname E(X_j)=0$ . My idea is to get a bound of $\operatorname E(X_j^2)$ but failed, and another thought is to try to construct an equivalent sequence of random variables to $\{X_n\}$ by truncating $X_n$ at $n$ as the author did in this section but also failed. The problem is that I don't know how to apply condition (ii) and find no way to understand it. Can someone give me some hint on the interpretation of (ii) or some method to get the result? Thanks in advance!","['statistics', 'law-of-large-numbers', 'probability']"
4184958,Prove that the expression $(p^2-q^2)^2-4p^2q^2$ cannot be a perfect square,"I was solving a different question which came down to- Prove that the expression $(p^2-q^2)^2-4p^2q^2$ cannot be a perfect
square. In other words, prove that the equation $$(p^2-q^2)^2-4p^2q^2=x^2$$ does not have any solutions in $\mathbb{N}$ . I checked on Wolfram Alpha and found no solutions (except those with $p=0$ or $q=0$ which I'm not interested in). I also feel like it's not very hard to prove it, especially if you look at an alternate form $$p^4+q^4=6p^2q^2+x^2$$ which looks similar to $a^4+b^4=c^2$ (with another extra term), about which we know that no solutions exist (in $\mathbb{N}$ ). But, these are all just talking in air. Can anyone give me a complete proof of this fact? Thanks in advance.","['number-theory', 'algebra-precalculus', 'elementary-number-theory']"
4184979,When does a matrix have a positive eigenvector?,"The generalised problem is as follows: Is there a condition on a symmetric positive-semi-definite matrix $A$ that ensures that it has a positive eigenvector, i.e. an eigenvector $Av = \lambda v$ such that $ v_i > 0$ for all $i$ ? Or, indeed, even a weakly positive eigenvector, i.e. an eigenvector $Av = \lambda v$ such that $ v_i \geq 0$ for all $i$ ? I am aware of the Perron-Frobenius theorem, but the assumptions for that theorem are too strong. The problem I actually want to solve is more specific, in case the extra information helps. If $B$ is the incidence matrix of a directed graph $G$ , then $B$ can be viewed as the linear transformation whose input is vertex weightings and whose output is the corresponding edge weightings obtained by the vertex weight differences. Then the matrix that I want to understand is $A := BVB^T$ , for a diagonal matrix with positive entries $V$ (for comparison, the graph Laplacian is $\Delta = B^TB$ ). This is a map from edge weightings to edge weightings. When does this admit a positive eigenvector? Thanks to everyone in advance.","['spectral-graph-theory', 'eigenvalues-eigenvectors', 'graph-theory', 'matrices', 'linear-algebra']"
4185027,Good argument that $\Omega_{X/k} = 0$ implies that $X$ is finite.,"Let $X$ be a scheme of finite type over a field $k$ . I'm pretty sure that $\Omega_{X/k} = 0$ implies that $\dim X = 0$ , i.e. $X$ is finite. In other words, if $\dim X > 0$ , then $\Omega_{X/k} \neq 0$ . Here is my argument: Since $\Omega$ commutes with base change, we may assume that $k$ is algebraically closed. We may also assume that $X$ is reduced, because $X_{\operatorname{red}}$ is a closed subscheme, and hence $\Omega_{X_{\operatorname{red}}}$ is a quotient of $\Omega_X|_{X_{\operatorname{red}}}$ (by the 2nd fundamental sequence of Kähler differentials). And now $X$ contains a dense open subset $U$ which is regular, and hence smooth over $k$ . Now $\Omega_{U/k} = \Omega_{X/k}|_U$ is a free $\mathcal O_U$ -module of dimension $\dim U = \dim X > 0$ , hence $\Omega_{X/k} \neq 0$ . Did I make a mistake anywhere? Is there a ""more algebraic"" proof which shows for a finitely generated $k$ -algebra $A$ that if $\Omega_{A/k} = 0$ , then $A$ is a finite $k$ -algebra? I know that if we choose a quotient $$ 0 \to (f_1, \dotsc, f_m) \to k[x_1, \dotsc, x_n] \to A \to 0,$$ then there is an exact sequence $$\sum_i A \cdot \left(\sum_j \frac{\partial f_i}{\partial x_j} dx_j\right) \to \bigoplus_i A \cdot dx_i \to \Omega_{A/k} \to 0,$$ but I didn't know how to use the fact that the $\sum_j \frac{\partial f_i}{\partial x_j} dx_j$ generate $\bigoplus_i A \cdot dx_i$ .","['algebraic-geometry', 'commutative-algebra']"
4185028,"Is the Lie algebra generated by $e_1, e_2$ spanned by $e_1, e_2, [e_1, e_2]$?","The Lie algebra generated by $e_1, e_2$ is spanned by the iterated Lie brackets of $e_1$ and $e_2$ ( $e_1, e_2, [e_1, e_2], [e_1, [e_1, e_2]], [e_2, [e_1, e_2]], \ldots$ ). Is it also spanned by just $e_1, e_2$ and $[e_1, e_2]$ ? The reason I ask is that in this Wikipedia article the parabolic Hörmander condition is stated just with $n+1$ iterations of the Lie bracket when considering $n+1$ vector fields and in other sources it is stated with an arbitrary number of iterations of the Lie bracket. I would like to show that a particular SDE $dx = A_0(x)dt + A_1(x) dB_t$ does not satisfy the parabolic Hörmander condition and I was wondering if it would be enough to show that $A_1(x_0)$ and $[A_0, A_1](x_0)$ do not span the tangent space at a particular point $x_0$ like the Wikipedia article suggests.","['stochastic-differential-equations', 'lie-algebras', 'differential-geometry']"
4185061,Complete bipartite subgraph problem,"$A,B$ are the sides of a bipartite graph with $\frac{|A| \cdot |B|}{k}$ edges, where $k$ is an integer. Prove that there are $A' \subset A$ and $B' \subset B$ such that $|A'| \geq |A|/k, |B'| \geq |B|/2^{|A|}$ and the subgraph formed by $A', B'$ is a complete bipartite graph. The problem can be solved if one proves that $\sum_{b \in B} {\deg(b) \choose a/k} > b$ , which could be proved by Jensen, but unfortunately the function $f(x) = {x \choose t}$ is not always convex.","['graph-theory', 'optimization', 'combinatorics', 'bipartite-graphs']"
4185099,"If $a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}}$ and $\lim\limits_{n\to\infty}a_n=\ell$, prove $\lim_{n\to\infty}[(\ell-a_n)^{1/n}n^{1/2}]=\frac{\sqrt e}2$","$$a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}}$$ We can prove that $\{a_n\}$ is convergent (using mathematical induction, $\sqrt {k+\sqrt{k+1+\cdots\sqrt{n}}}\leq k-1, for \ k\geq3$ ). If $$
\lim\limits_{n\to\infty} a_n=\ell,
$$ prove: $$\lim\limits_{n\to\infty} \left[\,(\ell-a_n)^{1/n}\cdot n^{1/2}\,\right]=\frac{\sqrt e}{2}$$","['analysis', 'real-analysis', 'functions', 'sequences-and-series', 'limits']"
4185128,Hausdorff's maximality principle lemma in Rudin Real and Complex analysis,"Here's the lemma that is needed before proving the Hausdorff's maximality principle (in Rudin's real and complex analysis appendix): Lemma Suppose $\mathcal{F}$ is a nonempty collection of subsets of a set $X$ such that the union of every subchain of $\mathcal{F}$ belongs to $\mathcal{F}$ . Suppose $g$ is a function which associates to each $A \in \mathcal{F}$ a set $g(A) \in \mathcal{F}$ such that $A \subset g(A)$ and $g(A) - A$ consists of at most one element. Then there exists an $A \in \mathcal{F}$ for which $g(A) = A$ . In the proof, it first defines a tower : Fix $A_0 \in \mathcal{F}$ . Call a subcollection $\mathcal{F}'$ of $\mathcal{F}$ a tower if $\mathcal{F}'$ has the following three properties: (a) $A_0 \in \mathcal{F}'$ (b) The union of every subchain of $\mathcal{F}'$ belongs to $\mathcal{F}'$ (c) If $A \in \mathcal{F}'$ , then also $g(A) \in \mathcal{F}'$ In the proof, we let $\mathcal{F}_0$ be the intersection of all towers which is also a tower (proof is trivial). Then we go on to define two more collection of sets: Let $\Gamma$ be the collection of all $C \in \mathcal{F}_0$ such that every $A \in \mathcal{F}_0$ satisfies either $A \subset C$ or $C \subset A$ . For each $C \in \Gamma$ , let $\Phi(C)$ be the collection of all $A \in \mathcal{F}_0$ such that either $A \subset C$ or $g(C) \subset A$ . In other words \begin{align*}
\Gamma &= \{ C \in \mathcal{F}_0 : A \subset C \text{ or } C \subset A \; \text{ for every } A \in \mathcal{F}_0 \}\\
\Phi(C) &= \{ A \in \mathcal{F}_0: A \subset C \text{ or } g(C) \subset A \}
\end{align*} Now in the proof, it proves that $\Phi(C)$ is a tower by showing that it meets all three properties shown above. The first two properties can be shown easily. For the third property, we do the following: If $A \in \Phi(C)$ there are three possibilities: Either $A \subset C$ and $A \neq C$ , or $A = C$ , or $g(C) \subset A$ . If $A$ is a proper subset of $C$ , then $C$ cannot be a proper subset of $g(A)$ , otherwise $g(A) - A$ would contain at least two elements; since $C \in \Gamma$ , it follows that $g(A) \subset C$ . If $A = C$ , then $g(A) = g(C)$ . If $g(C) \subset A$ , then also $g(C) \subset g(A)$ since $A \subset g(A)$ . Thus $g(A) \in \Phi(C)$ , and we have proved that $\Phi(C)$ is a tower. The minimality of $\mathcal{F}_0$ implies now that $\Phi(C) = \mathcal{F}_0$ , for every $C \in \Gamma$ I am stuck on the last claim in the proof where it says The minimality of $\mathcal{F}_0$ implies now that $\Phi(C) = \mathcal{F}_0$ , for every $C \in \Gamma$ . What exactly does the minimality mean in this context and how does that lead to the equality? My guess is that $\Phi(C)$ is the smallest tower containing $C$ , and since $\mathcal{F}_0$ is also the smallest tower containing $C$ , they must be equal? Also the proof says that In other words, if $A \in \mathcal{F}_0$ and $C \in \Gamma$ , then either $A \subset C$ or $g(C) \subset A$ . But this says that $g(C) \in \Gamma$ . Hence $\Gamma$ is a tower, and the minimality of $\mathcal{F}_0$ shows that $\Gamma = \mathcal{F}_0$ . It follows from the definition of $\Gamma$ that $\mathcal{F}_0$ is totally ordered. Here, since $\Phi(C) = \mathcal{F}_0$ , for any $A \in \mathcal{F}_0$ , either $A \subset C$ or $g(C) \subset A$ . Since $C \subset g(C)$ , we can say that for all $A \in \mathcal{F}_0$ , either $A \subset g(C)$ or $g(C) \subset A$ , which implies that $g(C) \in \Gamma$ . Hence this satisfies property (c) of the definition of tower. Here they again use the minimality condition to show that $\Gamma = \mathcal{F}_0$ which I'm not sure how they did. And how does this result in $\mathcal{F}_0$ being totally ordered?",['elementary-set-theory']
4185184,Why doesn't $\int\lfloor {x}\rfloor~dx=x\lfloor x\rfloor +C$?,"Why doesn't $$\int\lfloor {x}\rfloor~dx=x\lfloor x\rfloor +C?$$ When I tried integrating $\lfloor {x}\rfloor$ initially, I thought of the integral as representing the area beneath the graph and so was successful in finding its indefinite integral. However, when I tried thinking about it from a 'formulaic' perspective, ie without thinking about what the integral really meant, I've become confused: If we try integrating by parts, we seem to get $$\int\lfloor {x}\rfloor~dx=x\cdot\lfloor {x}\rfloor-\int x\cdot\frac{d}{dx}(\lfloor {x}\rfloor)~dx=x\lfloor {x}\rfloor+C$$ since I would think that $\frac{d}{dx}(\lfloor {x}\rfloor)=0$ . Please can you explain why my result is wrong? I would guess that it has something to do with the derivative of the floor function being undefined at places where there is jump discontinuity, but I'm not sure.","['integration', 'ceiling-and-floor-functions', 'fake-proofs', 'calculus', 'indefinite-integrals']"
4185190,What is the fastest method to compute the $nth$ number in Lucas sequences?,"Lucas sequences $U_n(P,Q)$ and $V_n(P,Q)$ are defined by the following relations: $U_0(P,Q)=0,$ $U_1(P,Q)=1,$ $U_n(P,Q)=P\cdot U_{n-1}(P,Q)-Q\cdot U_{n-2}(P,Q)$ and $V_0(P,Q)=2,$ $V_1(P,Q)=P,$ $V_n(P,Q)=P\cdot V_{n-1}(P,Q)-Q\cdot V_{n-2}(P,Q).$ I knew that we can use the following method to compute whether $U_n(P,Q)$ or $V_n(P,Q)$ : $\begin{pmatrix}
P & -Q \\ 1 & 0 
\end{pmatrix}^{n-1} \begin{pmatrix}
U_1 & V_1 \\ U_0 & V_0 
\end{pmatrix} = \begin{pmatrix}
U_n & V_n \\ U_{n-1} & V_{n-1} 
\end{pmatrix}$ Now, my question is: is this method is the fastest method to compute the $nth$ number in Lucas sequences ? can we use a method similar to (the fast doubling method) to compute the $nth$ number in Lucas sequences instead of the above method (lucas sequences matrix method) ? Note that the fast doubling method here is the fastest method to compute the $n$ -th Fibonacci number and it is an alternative of the following method (matrix method of Fibonacci numbers): $\begin{pmatrix}
1 & 1 \\ 1 & 0 
\end{pmatrix}^{n} = \begin{pmatrix}
F_{n+1} &  F_{n} \\ F_{n} & F_{n-1} 
\end{pmatrix}$ Support your answer with an example please.","['lucas-numbers', 'fibonacci-numbers', 'sequences-and-series']"
4185193,Basis is uncountable or countable when $X$ is separable?,"The following is from Functional Analysis book by Conway: Let $X$ be a separable infinite-dimensional Banach space and let $\{e_i : i \in I\}$ be a Hamel basis for $X$ with $\| e_i \| = 1$ for all $i$ . Note that a Baire Category argument shows that I is uncountable. I am confused with two things: 1- How Baire Category Thm is related? 2- I think the book is wrong because $I$ must be countable not uncountable? See this , for example.","['hamel-basis', 'banach-spaces', 'baire-category', 'functional-analysis']"
4185221,Let $P$ be a Sylow $p$-subgroup of $G$ and suppose $x\in P$ has order $p$ and $C_G(x)$ has cyclic Sylow $p$-subgroups. Prove $P$ is cyclic.,"I can't seem to make any progress in the following question. Let $P$ be a Sylow $p$ -subgroup of a group $G$ , and suppose that $x\in P$ has order $p$ and $C_G(x)$ has cyclic Sylow $p$ -subgroups. Prove that $P$ is cyclic. I was given the following hint: ( $Z(P)\subseteq C_G(x)$ , and that $P$ is cyclic if and only if $x\in Z(P)$ ) My current thought process: I suppose that I am suppose to show that $x\in Z(P)$ . But I dont how to even begin doing this. Perhaps $Z(P)=C_G(x)$ ? Also, I can't seem to see why $x\in Z(P)$ implies that $P$ is cyclic. Any help provided will be appreciated.","['group-theory', 'cyclic-groups', 'sylow-theory']"
4185256,Local coordinate expression for the equations of motion in gauge theory,"Let's assume $P$ is a principal bundle, $F^A \in \Omega^2(M,Ad(P))$ the curvature 2-form, $Ad(P)$ the adjoint bundle. $d_A$ the covariant differential. For sections in the associated bundle $E=P \times_{(G, \rho)} V$ , $d_A$ is just the covariant differential. In local coordinates it is of the form $d_A \rightarrow \partial_{\mu}+\rho_*(A_{\mu})$ . $\phi$ is a section in the associated bundle and in local coordinates takes the form $[s(x),\varphi(x)]$ where $s:U \rightarrow P$ is a section in the principal bundle and $\varphi:U \rightarrow V$ . The Yang-Mills-Higgs action is \begin{equation}
\mathcal{S}_{Y K}: \mathcal{C}(P) \times \Gamma(E) \rightarrow \mathbb{R}, \quad \mathcal{S}_{Y K}[A, \phi]=\int_{M}\left(-\frac{1}{2}\left\langle F^{A}, F^{A}\right\rangle_{\mathrm{Ad}(P)}+\left\langle d_{A} \phi, d_{A} \phi\right\rangle_{E}-m^{2}\langle\phi, \phi\rangle_{E}\right) d \nu_{g}
\end{equation} The variation $A\mapsto A+\omega$ gives the equations of motion \begin{equation}
\delta_{A} F^{A}=j
\end{equation} \begin{equation}
\delta_{A} d_{A} \phi + m^{2} \phi=0
\end{equation} with the codifferential $\delta_A$ and $j \in \Omega^{1}(M , \operatorname{Ad}(P))$ implicitly defined by \begin{equation}
\langle j, \omega\rangle_{\mathrm{Ad}(P)}=-2 \operatorname{Re}\left(\left\langle d_{A} \phi, \rho_{*}(\omega) \phi\right\rangle_{E}\right)\quad\text{for all }\omega.
\end{equation} In physics, the current is defined by \begin{equation}
j_{\nu}^{a}=-i\left(\left(D_{\nu} \varphi_{i}\right)^{\dagger}\left(T_{a}^{r} \varphi\right)_{i}-\left(T_{a}^{r} \varphi\right)_{i}^{\dagger} D_{\nu} \varphi^{j}\right)
\end{equation} where $T_a$ is a basis of the Lie algebra and $T_a^r=\rho_*(T_a)$ , $D_{\nu}=\partial_{\nu}+A_{\nu}^aT_a^r$ . $\varphi_i$ is just the $i$ -th component of $\varphi$ . The $i$ 's come into play due to the definition of physicists that every Lie algebra element is multiplied with $I$ . $\mathbf{Question}$ : How exactly can one derive the physical local coordinate expression from the mathematical definition?","['gauge-theory', 'differential-topology', 'mathematical-physics', 'differential-geometry']"
4185278,"Application of the fundamental theorem of calculus in $n$ dimensions: Stokes', divergence or gradient theorem?","Consider the following linear integral solved by the Fundamental Theorem of Calculus: $$\int_{a_1}^{a_2}dx\, e^{ixk}=\int_{a_1}^{a_2}dx\, \frac{d}{dx}\left(\frac{e^{ixk}}{ik}\right)= \frac{e^{ixk}}{ik}\Biggr\rvert_{a_1}^{a_2}=\frac{e^{ia_2 k}- e^{i a_1 k}}{ik}\,.$$ Is there a way to make use of a generalization of this theorem in $n$ dimensions (maybe the Gradient Theorem, or Divergence Theorem, or Stokes' theorem?) such that the multidimensional integral defined over a $n$ -dimensional generic volume $V$ $$\int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}}$$ is solved in terms of the integrand function evalutated at the $(n-1)$ -dimensional boundary surface $\partial V$ of the volume $V$ : $$\int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}}=\int_V d\mathbf{x}\, \nabla_{\mathbf{x}}(e^{i\mathbf{x}\cdot \mathbf{k}})\cdot \frac{\mathbf{k}}{i k^2} \propto e^{i\mathbf{x}\cdot \mathbf{k}}\rvert_{\mathbf{x}\in \partial V}\, ?$$ Indeed in the linear example above, the integral solution is expressed in terms of the exponential function appearing in the argument, evaluated at the two points corresponding to the borders $x=a_1, a_2$ of the linear domain $[a_1,a_2]$ .","['divergence-theorem', 'definite-integrals', 'multivariable-calculus', 'calculus', 'stokes-theorem']"
4185314,"Largest divisor of all $ n(n^2-1)(5n+2)\, $ [gcd of values of recurrence]",My Solution: $$ n(n^2-1)(5n+2) = (n-1)n(n+1)(5n+2) $$ This number is divisible by 6 (as at least one of 2 consecutive integers is divisible by 2 and one of 3 consecutive integers is divisible by 3. $ 5n+2 \equiv 5n \equiv n \mod 2 $ then $n$ and $5n+2$ have the same pairness and at least one of $n+1$ and $5n+2$ is divisible by 2. $ n \equiv 5n \equiv 5n+4 \mod 4 \to $ if $ 2\ | \ n+1 \to n - 1 $ or $ n + 1 $ is divisible by 4 if $ 2\ | \ 5n+2 \to n $ or $ 5n + 2 $ is divisible by 4 The expression is divisible by 6 and has 2 even integers and one of them is divisible by 4 $\to$ is divible by 24.,"['elementary-number-theory', 'divisibility', 'recurrence-relations', 'discrete-mathematics']"
4185344,Mistaken proof that every bounded linear operator on $L^2$ has an integral kernel,"I have been staring at this for several hours already, still unable to find my own error. I know, it is embarassing, but I need your help to spot it, please. Thank you. Claim: Every bounded linear operator on $L^2$ has an integral kernel (and in fact is Hilbert-Schmidt). Proof: Let $(X,m)$ be a space with measure, and let $U : L^2(X) \to L^2(X)$ be a bounded linear operator. Define $I : L^2(X) \otimes _{alg} L^2(X) \to \mathbb C$ (the tensor product being the algebraic one, not the topological one) by $$I(f \otimes g) = \int _X f \ Ug \ \mathrm d m \ .$$ Notice that $I$ is linear and that, using the Cauchy-Schwarz inequality, $$|I (f \otimes g)| \le \| f \| _{L^2} \ \| Ug \| _{L^2} \le \| U \| \ \| f \| _{L^2} \ \| g \| _{L^2} = \| U \| \ \| f \otimes g \| _{L^2(X \times X)} \ ,$$ which means that we may extend $I$ by continuity to the whole of $L^2(X \times X)$ . By Riesz's theorem, it follows that there exists $k \in L^2 (X \times X)$ such that $I (F) = \int _{X \times X} \bar k \ F \ \mathrm d (m \times m)$ . In particular, if $F = f \otimes g$ , it follows that $$ \int _X f(x) \ (Ug)(x) \ \mathrm d m (x) = \int _X f(x) \left( \int _X \overline {k(x,y)} \ g(y) \ \mathrm d m (y) \right) \mathrm d m (x) \ ,$$ whence, since $f$ is arbitrary, it follows that $$ (Ug)(x) = \int _X \overline {k(x,y)} \ f(y) \ \mathrm d m (y) \ ,$$ for almost all $x$ , which is obviously not true. Where am I losing it?","['measure-theory', 'operator-theory', 'hilbert-spaces', 'lp-spaces', 'functional-analysis']"
4185348,Collatz conjecture but with $\ 3n-1\ $ instead of $\ 3n+1.\ $ Do any sequences go off to $\ +\infty\ $?,"Collatz conjecture but with $\ 3n-1\ $ instead of $\ 3n+1.\ $ Do any
sequences go off to $\ +\infty\ $ ? $$$$ Background (not necessary to answer my question): Considering the following operation on an arbitrary positive integer: If the number is even, divide it by two. If the number is odd, triple it and add one. The Collatz conjecture is: This process will eventually reach the number $1$ , regardless of which positive integer is chosen initially. If the Collatz conjecture is false, then either there will be cycles that don't contain the number $\ 1,\ $ or there will be a (at least one) sequence that goes off to $\ +\infty.$ My question: Considering the following operation on an arbitrary positive integer: If the number is even, divide it by two. If the number is odd, triple it and take away one. An analogue to the Collatz conjecture with these rules fails, because $\ 5\to 14\to 7\to 20\to 10\to\ 5\ $ is a cycle that does not contain $\ 1.\ $ In fact, there are lots of cycles that don't contain $\ 1\ $ that I found with the Python code below. My question is do any sequences with this $\ 3n-1\ $ rule go off to $\ +\infty,\ $ or not? It seems ""less likely"" than the likelihood Collatz sequences will go off to $\ +\infty,\ $ but proving such a thing seems hard. Edit: I have checked all numbers up to $\ 5000\ $ using the code below and every sequence either goes to $\ 1\ $ or is in a loop. Also, there are no really long sequences (relative to number size) as opposed to some small starting numbers in the Collatz conjecture, like $\ n=27,\ $ which has $\ 111\ $ steps. This seems to suggest that no sequence goes off to infinity, and there should be some (relatively simple?) number theory proof for this. $$$$ def collatz2(n):
    if n % 2 == 0: return int(n/2)
    else:          return 3*n-1

def collatz_sequence2(n):
    sequence = [n]
    while n != 1:
        n = collatz2(n)
        sequence += [n]
        if n in sequence[:-1]:
            print(sequence[0], ""is in a loop not containing 1:"",)
            break
    return sequence

for i in range(1,100):
    print(i, ':', collatz_sequence2(i))","['collatz-conjecture', 'number-theory', 'sequences-and-series']"
4185381,"What are the domain and codomain of an arrow in Rel, the category of relations?","I will first state my question and then say why it is causing me a problem. Question :
In the definitions of Rel, the category of relations, that I found (nLab, Wikipedia, and Awodey's book), the objects of Rel are said to be sets, and the arrows $A \to B$ are relations from A to B, that is, subsets of $A\times B$ . This definition seems problematic to me. If that is the case, $R \subseteq A\times B$ implies $ R \subseteq (A\cup \{*\}) \times B$ , where $ * \notin A$ . So, what is true: $dom(R)=A$ or $dom(R)=A\cup \{*\}$ ? The same question applies to codomains, and with any other set larger than A in its place. Why this question :
The reason why this is causing me a problem is that in the second problem of Awodey's book's first chapter, he asks us to check whether $ Sets \cong Sets^{op} $ . I couldn't solve it, and when I checked the solutions in the back, he says that, in Sets, there is only 1 arrow from the empty set $\emptyset$ . Now, if things are like I was thinking, then each relation (and so the functions, since they are also relations) of type $A \to B$ is not simply a subset of $A\times B$ but also carries some label to connect it specifically to $A$ and $B$ . The problem is that such a label would allow many functions from the empty set to itself and other sets by changing the label. Each one would be a differently labelled empty set like $\emptyset = \emptyset \times A$ . If the empty set doesn't count as a function, then Sets doesn't have an identity arrow for $\emptyset$ , so it must count and the problem follows. I appreciate any answer, but if you could also connect it to the solution to this problem, I would be very happy.","['relations', 'definition', 'functions', 'category-theory']"
4185393,Question about FOAG exercise 11.3.I,"This is an exercise in Ravi Vakil's AG notes. My question is about part b.  Namely, in the induction step recommended in the hint, why can't I just choose $g_d$ to be any element outside $\cup_j \mathfrak{ p}_j$ ? Why do I need $g_d$ to be in all of the $\mathfrak{ q}_i$ ? Maybe the dimension of $A/(g_d)$ will go down by more than $1$ , but that just means I would need fewer than $d$ generators in the statement of the proposition (and in fact that is prohibited by part a).  It makes me think I am misunderstanding something fundamental. Edit: I have typed the exercise below. Let $(A, \frak m)$ be a Noetherian local ring. b) Let $d= \dim A$ .  Show that there exist $g_1,...,g_d\in A$ such that $V(g_1,...,v_d) = \{[\frak m]\}$ . Hint: use induction on $d$ . Find an equation $g_d$ knocking the dimension down by 1, i.e., $\dim A/(g_d) = \dim A -1$ . Suppose $\frak p_1,...,p_n$ correspond to the irreducible components of $Spec A$ of dimension $d$ , and $\frak p_i \subset q_i$ are prime ideals corresponding to irreducible closed susbsets of codimension $1$ and dimension $d-1$ . Use prime avoidance to find $h_i \in \frak q_i- \cup_{j=1}^n p_j$ . Let $g_d= \Pi_{i=1}^n h_i.$","['algebraic-geometry', 'commutative-algebra']"
4185394,Rational expressibility of polynomials which are symmetric under the permutation of all variables except one fixed variable,"I am trying to prove the following statement of early group theory / classic galois theory: Let $\sigma_1,\ldots,\sigma_n$ be the elementary symmetric polynomials
of $x_1,\ldots,x_n$ . If $f$ is a rational function of $x_1,\ldots,x_n$ that is symmetric
under all permutations of the $x_i$ ’s that fix $x_1$ , then it is
expressible as a rational function of $\sigma_1,\ldots,\sigma_n$ and $x_1$ . The theorem is stated in the paper The fundamental theorem on symmetric polynomials: History’s first whiff of galois theory as theorem 7. But no proof is given. In the paper Galois for 21st-Century Readers , footnote 4, I found the hint that every symmetric polynomial in the roots $x_2,\ldots,x_n$ can be expressed rationally in terms of $x_1$ . This is explained by showing that the elementary symmetric polynomials of $x_2,\ldots,x_n$ can be written as polynomials of $x_1$ . However I do not completely understand, why this is the case. I get that if you multiply out the left side of the equation $$(x-x_2)\cdot\ldots\cdot(x-x_n)=\frac{f(x)}{(x-x_1)}$$ you get a polynomial where the coefficients are exactly the elementary symmetric polynomials of $x_2,\ldots,x_n$ . But I do not understand why the coefficients of the polynomial on the right side can be expressed as polynomials of $x_1$ . What does the polynomial reminder theorem have to do with this? Yes, $(x-x_1)$ divides $f(x)$ without reminder. $f(x_1)=0$ . So what?","['galois-theory', 'abstract-algebra', 'symmetric-polynomials', 'polynomials', 'group-theory']"
4185467,Rate of eigenvalue decay for Erdős–Rényi random graph,"How fast do eigenvalues decay in a large Bernoulli random graph for a fixed large $n$ (number of nodes) and fixed $p$ (probability of dropping an edge)? Also known as Erdős–Rényi random graph. Empirically, if we sort eigenvalue in decreasing order, their magnitudes seem to decay exponentially with $i$ =position in the sorted list. Can someone point me to a reference or a way to justify this? This graph plots log of absolute value of eigenvalues of a large Erdős–Rényi random graph, arranged in decreasing order. ListLogPlot@Rest@Reverse@Sort@Abs@Eigenvalues@N@AdjacencyMatrix@RandomGraph@BernoulliGraphDistribution[2000, 0.5] Edit eigenvalues follow the shape of semicircle CDF notebook","['graph-theory', 'random-matrices', 'random-graphs', 'probability-theory']"
4185477,"Solving the system $x^4=y+z$, $y^4=x+z$, $z^4=x+y$ with high school math [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $$\begin{align}
x^4=y+z\\[4pt]
y^4=x+z\\[4pt]
z^4=x+y
\end{align}$$ I need to solve this system of equations for all possible real triplets $(x,y,z)$ but with high school math (it's from an university's admission test's past papers). Every path I try to follow seems to be a dead end. Any help, suggestion or tip would be appreciated. Thank you all.","['algebra-precalculus', 'systems-of-equations']"
4185486,"When does the fact that $\underset{(k,k)}{\sum\sum} a_{k,k}$ converges imply convergence for $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$?","I have come to a problem in a multivariate calculus book that I'm having some trouble with.
The book is ""A Course in Multivariate Calculus and Analysis"" by Ghorpade and Limaye.
The problem goes : Let $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ be a double series whose terms are schematically given by : \begin{equation}
\begin{matrix}
            1 &  2 		&  4 		&  8 & \dots \\
 -\frac{1}{2} & -1  		& -2 		& -4 & \dots \\
 -\frac{1}{4} & -\frac{1}{2}  	& -1 		& -2 & \dots \\
 -\frac{1}{8} & -\frac{1}{4}  	& -\frac{1}{2}  & -1 & \dots \\
\vdots & \vdots & \vdots & \vdots & \; 
\end{matrix}
\end{equation} and let $A_{m,n}$ denote its $(m,n)$ th partial double sum. Show that each row-series is divergent, but each column-series converges to $0$ . Also, show that $A_{m,m} \rightarrow 2$ as $m \rightarrow \infty$ . Is $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ convergent ? I am able to solve all of the problem except for the last part, namely determining
if $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ is convergent. My question really is under what conditions for $(a_{k,\ell})$ does : \begin{equation}
(A_{m,m}) \text{ is convergent } \Rightarrow (A_{m,n}) \text{ is convergent }
\end{equation} Could someone maybe provide an example where the above implication is not true ?","['multivariable-calculus', 'convergence-divergence', 'sequences-and-series']"
4185509,Limit of the ratio of a nowhere differentiable function to a polynomial,"Let $f(x)$ be a continuous function that— maps the closed interval [0, 1] to [0, 1], equals 0 at 0, does not equal 0 anywhere except at 0, and is nowhere differentiable on its domain. Let $g(x)$ be a polynomial that— maps the closed interval [0, 1] to [0, 1], bounds $f$ from above, and equals 0 at 0. My question is: Does the limit $\lim_{x\to 0^+} f(x)/g(x)$ exist?  If not, what are the weakest conditions required on $f$ for the limit to exist? I know that by L'Hôpital's rule, the limit exists when $f(x)$ is differentiable on some interval $(0, \epsilon)$ , but I don't know whether the limit still exists in this case when $f$ is not required to be differentiable.  This question is neither homework nor a self-study assignment, nor is this coursework.","['limits', 'real-analysis']"
4185535,Castelnuovo's contractibility criterion in $\text{char}(k)>0$?,"Let $S$ be a smooth, projective surface over an algebraically closed field $k$ of any characteristic. I'm trying to prove/disprove the following: There cannot be a sequence of curves $\{E_n\}_{n\in\Bbb{N}}$ on $S$ such that: (i) each $E_i$ is isomorphic to $\Bbb{P}^1$ with $E_i^2=-1$ . (ii) $E_i\cdot E_j=0$ whenever $i\neq j$ . I know a proof for this when $k=\Bbb{C}$ . Using Castelnuovo's conctractibility criterion, we can successively contract $E_1,E_2,...$ through blowdowns $S=:S_0\to S_1\to S_2\to...$ After we contract $E_n$ , we still have $(-1)$ -curves $E_{n+1},E_{n+2},...$ on $S_n$ , so we never obtain a minimal model, which is absurd. Is it possible to use a similar argument for arbitrary algebraically closed fields, including positive characteristic? (I don't know if this will make a difference, but in the original context of this, $S$ is rational) Thanks you!","['algebraic-geometry', 'surfaces', 'positive-characteristic']"
4185541,A wrong argument for $\mathbb{R}$ being countable [duplicate],"This question already has answers here : Cantor's proof and Zorn's lemma (2 answers) Why does the set of countable sets of $\mathbb R$ does not satisfy the conditions of Zorn's lemma? (3 answers) Closed 3 years ago . We assume $A$ is the set of all countable subsets of the set of real numbers. We know $A$ is a partially ordered set $(A, \subseteq)$ . Suppose $$A_1 \subseteq A_2 \subseteq \ldots \subseteq A_n \subseteq A_{n+1} \subseteq \ldots$$ is a chain in $A$ . We can prove $B=\bigcup_{n \in \Bbb{N}} A_n$ is a countable set. For each natural number $m$ , we have $A_m \subseteq B$ . So $B$ is an upper bound for $A$ . This shows each chain in $A$ has an upper bound according to Zorn's lemma. $A$ has a maximal element $X$ , and we know $X$ is a countable set. Now we prove $X = \Bbb{R}$ . If $X \neq \Bbb{R}$ , then there is an $x \in \Bbb{R}$ such that $x \notin X$ . Let $Y=X \cup \{x\}$ . It's obvious that $Y$ is a countable subset of the real numbers and $X \subsetneq Y$ . This contradicts $X$ being a maximal element. Thus, $X = \Bbb{R}$ and $\Bbb{R}$ is a countable set. What is wrong with this argument?","['elementary-set-theory', 'cardinals']"
4185558,"Is there any ""nice"" representation for algebraic numbers of degree 3 (or higher)?","Every decimal number have a finite decimal expansion ! Every rational number has its decimal expansion becomes periodic after some digits. Every rational number has a finite continued fraction representation. Now every algebraic number of degree 2 has a periodic continued fraction representation
after some digits. Is there another representation of numbers such that an algebraic number of degree 3
or more is  periodic after some digits?","['number-theory', 'continued-fractions']"
4185564,Is theory of equations a dead field today?,"Is theory of equations a dead field today? By theory of equations I mean, specially, the study of polynomials and solving algebraic equations through radicals. There seem to be very few journals on such subjects. Also most of the great results are at least 100 years old, there are not many new results out there. Any journals on that topics would be appreciated as well. Thanks","['galois-theory', 'abstract-algebra', 'soft-question', 'polynomials']"
4185591,Evaluating $\lim_{x\to0}\frac{2\cot 2x-\cot x}{\sin 2x}$,What is the value of $\lim_{x\to0}\frac{2\cot 2x-\cot x}{\sin 2x}$ ? $1)\text{zero}\qquad\qquad2)\frac12\qquad\qquad3)-\frac12\qquad\qquad4)-1$ Here is my work: $$\lim_{x\to0}\frac{2\cot 2x-\cot x}{\sin 2x}=\lim_{x\to0}\left(\frac{2\cos2x}{\sin^22x} -\frac{\cos x}{\sin x\sin 2x}\right)$$ Second fraction can be simplified to $\frac{\cos x}{2\sin^2 x\cos x}=\frac{1}{2\sin^2x}$ . But when $x\to0$ I get $\infty-\infty$,"['limits', 'calculus', 'trigonometry']"
