question_id,title,body,tags
3980800,Derivative of Arctangent with definition of derivative,"I can't find the derivative of arctangent with definition of derivative.
Here's my way: $\displaystyle\lim_{\Delta x\to\ 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}$ is a definition of derivative with limit. If we define $f(x)=\arctan(x)$ , then we get: $$\left[\lim_{\Delta x\to\ 0} \frac{\arctan(x+\Delta x)-\arctan(x)}{\Delta x}\right]=\left[\lim_{\Delta x\to\ 0} \frac{\arctan\left(\dfrac{\Delta x}{1+x(x+\Delta x)}\right)}{\Delta x}\right].$$ But I don't know how to continue. Thanks.","['limits', 'definition', 'derivatives']"
3980834,"No. of ways of selecting 3 squares when they do not lie in same row, column, or diagonal","Total number of ways of selecting 3 small squares on a normal chess board so that they don’t belong to the same row, column or diagonal line, is equal to: No. of ways of selecting 3 squares when they do not lie in same row or same column $=(64×49×36)×\frac{1}{3!}=18816$ Total ways of selecting the squares when they lie in the same diagonal line $=2({8 \choose 3} + {7 \choose 3} +{6 \choose 3}+{5 \choose 3} + {4 \choose 3}+ {3 \choose 3})=392$ But this only counts the cases when all three are in the same diagonal. How to count the cases when two of the three lie in the same diagonal?","['chessboard', 'combinations', 'combinatorics']"
3980835,Is the following an example of a non-measurable event?,"Goodmorning everyone. I started reading DeGroot and Schervish's 'Probability and Statistics' (4th editon) and wondered,  is there an uncountable union of events, that is not an event? This question has been answered on this site.
The topic of question is ""What is an uncountable union of events?"" (sorry ba I can't report the code or the link of the question that has already been answered). The responses were very illuminating,
however, re-reading the example of an uncountable union of events that is not an event, I'm referring to Arthur's answer, I wonder if the example is adequate. This confuses me. I report the example referred to in the aforementioned answer and then I ask my question. Example: Assuming the Axiom of Choice (which is a very reasonable and common thing to do, but not universal), you can construct unions of events which, if allowed to be events themselves, will have a problematic probability of ocurring. Basically, it can't be 0 and it can't be positive. To see it in action, let's say your experiment is to pick a point uniformly at random on a circle. Then any point is an event. Using the AoC, we can construct (or more correctly, we can prove the existence of) a set of points A0 on the circle with a special property: Rotating the set along the circle by any rational angle α∈(0∘,360∘) results in a new set of points Aα. None of these Aα have any points in common with any of the others, but together they cover the entire circle. So, if we were to assign some probability p to picking a point in A0, then by rotational symmetry the same probability should apply to any of the Aα. And since they are all pairwise disjoint, and they together cover the circle, the sum of all those p's should be 1. Thus we have
∑α∈[0,360)∩Qp=1 But if p is 0, the sum is 0, and if p is positive, then the sum is infinite. So it is impossible to assign a probability to this union A0, and therefore we are better off not calling it an event. Question Why can't I consider the set of rational points to have zero measure? If I imagine the circle composed of rational and irrational points, why not assign a measure of nothing on the first (rational) and measure equal to 1 on the second (irrational)? so, having admitted the above, am I wrong if I consider the circle as a union of rational and irrational points? Thanks for any answer or clarification. Francesco.",['measure-theory']
3980845,Calculate how $\mathbb{E}[V|X]$ distributed.,"Given $X,Y$ i.i.d where $\mathbb{P}(X>x)=e^{-x}$ for $x\geq0$ and $\mathbb{P}(X>x)=1$ for all $x<0$ and $V=\min(X,Y)$ Calculate how $\mathbb{E}[V|X]$ distributed. I've found that $F_{V|X=x}(v)=\left\{\begin{array}{rcl} 0&t\leq0\\1-e^{-t}&0\leq t\leq x\\1&else\end{array}\right.$ And I've tried using the formula $\mathbb{E}[V|X]=\int_{\infty}^{\infty}vf_{V|X=x}dv$ and I got that $\mathbb{E}[V|X]=-xe^{-x}-e^{-x}+1$ and in the answer I had to compare with they got $\mathbb{E}[V|X]\sim U(0,1)$ Not sure how to get to this distribution any help?","['probability-distributions', 'probability-theory', 'probability']"
3980874,Mean curvature of graph over its tangent plane,"Let $S$ be a regular surface in $\mathbb{R}^3$ and $p\in S$ a point on the surface.
By the implicit function theorem $S$ can be locally written as a graph of a function, e.g. $V\cap S = \{ (x,y,z) \in \mathbb{R}^3 : (x,y)\in U, z=f(x,y)\}$ for some open neighbourhood $V$ of $p$ , open set $U\subset \mathbb{R}^2$ and some smooth function $f: U \rightarrow \mathbb{R}.$ By choosing local coordinates we can identify $U$ as part of the tangent plane of $S$ at $p$ , furthermore we can set $f^{-1}(p)=(0,0)$ . In this case, the mean curvature at $p$ is given by $H=\frac{f_{xx}\;\;+f_{yy}}{2}$ (average of second derivatives at $p$ ) and principal curvatures are $f_{xx},f_{yy}$ . Is this correct? If it is, how can one describe this more precisely than ""choosing local coordinates...""? If it is not, how could I achieve a similar result (surface as graph over its tangent plane and easy formula of $H$ )? Thank you.","['tangent-spaces', 'surfaces', 'curvature', 'differential-geometry']"
3980879,Intuition behind stolz caesaro theorem [duplicate],"This question already has answers here : How to understand intuitively the Stolz-Cesaro Theorem for sequences? (3 answers) Closed 3 years ago . The stolz caesaro theorem seems to be a discrete analogue of L'hopitals rule. I can understand l'hopitals rule via the taylor series, that is: $$ \lim_{x \to a} \frac{P(x)}{Q(x)} = \frac{ P(a) + \frac{dP}{dx}|_a (x-a) + O ( (x-a)^2)  }{  Q(a) + \frac{dQ}{dx}|_a (x-a) + O ( (x-a)^2)}$$ Now if $P(a) = Q(a) = 0$ , the limit becomes ratio of first order term/ the ratio of values nearby to the point of interest. So, Is there a similar 'series' intuition for stolz caesaro theorem?",['derivatives']
3980882,Show that two angles are equal,"Acute-angled triangle ABC is inscribed in circle $c$ . In the smaller arc BC we choose a random point F and draw a line parallel to AC which intersects side AB in point Q. Then from point Q we draw a parallel to FC which intersects side AC to point H. From vertex B we draw a line parallel to FC which intersects the circle at point S. Show that angle SHC is equal to BAC. I have tried the following: I have extended line SH until it meets the circle at point which I name G. Clearly G lies also on the line QF but we don't know it yet. Since FC and BS are parallel chords of the circle, then BFCS must be an isosceles trapezoid, hence arcs BC and FS are equal. Therefore angle CAB is equal to angle SGF (inscribed angles of the same circle, having equal arcs). Then obviously SHE is equal to SGF. But how can I prove that point G also lies on the parallel from F? Or, is there any other way to prove that the required angles are equal? Thank you in advance.","['euclidean-geometry', 'geometry']"
3980896,Prove the polynomial $x^4+4 x^3+4 x^2-4 x+3$ is positive,"Given the following polynomial $$
x^4+4 x^3+4 x^2-4 x+3
$$ I know it is positive, because I looked at the graphics and I found with the help of Mathematica that the following form $$
(x + a)^2 (x + b)^2 + c^2(x + d)^2 + e^2
$$ can represent the polynomial with the following values for the constants $$
\left(x-\frac{1}{2}\right)^2
   \left(x+\frac{5}{2}\right)^2+\frac{5}{2} \left(x+\frac{1}{5}\right)^2+\frac{107}{80}
$$ I suppose there are simpler ways to prove that the polynomial is positive, perhaps by using some inequalities. Please, advice.","['inequality', 'a.m.-g.m.-inequality', 'polynomials', 'sum-of-squares-method', 'algebra-precalculus']"
3980898,Finding $\mathbb{P}(\max_{t\leq 1} (W_t+t)\geq 1)$,"I was trying to find $\mathbb{P}(\max_{t\leq 1}  (W_t+t)\geq 1),$ where $W_t$ is a one-dimensional Brownian motion for $t\in [0,1].$ I first applied the Girsanov's theorem to get $$\mathbb{P}\Big(\max_{t\leq 1}  (W_t+t)\geq 1\Big)=\mathbb{E}\Big[\mathbb{1}_{\max_{t\leq 1}  (W_t)\geq 1} \exp\Big(W_1-\frac{1}{2}\Big)\Big],$$ where $\mathbb{1}_A$ is the indicator function of set $A$ . A given hint here is to use $\mathbb{P}(\max_{t\leq 1} W_t\geq 1, W_1\leq b)$ and use the fact that if $\mathbb{P}(\xi\leq a ,\eta\leq b)=\int_{-\infty}^b f(x) \ dx$ for every $b$ , then $\mathbb{E}[g(\eta)\mathbb{1}_{\xi\leq a}]=\int_\mathbb{R}g(x)f(x) \ dx.$ Hence, I've found $$
\mathbb{P}(\max_{t\leq 1} W_t\geq 1, W_1\leq b) = \left\{
        \begin{array}{ll}
            \mathbb{P}(W_1\geq 2-b) & \quad b \leq 1 \\
            2\mathbb{P}(W_1\geq 1)-\mathbb{P}(W_1\geq b) & \quad b \geq 1
        \end{array}
    \right.
$$ and we have $g(x)=\exp(x-\frac{1}{2})$ (using the same notation as the hint), since we want to compute $\mathbb{E}\Big[\mathbb{1}_{\max_{t\leq 1}  (W_t)\geq 1} \exp\Big(W_1-\frac{1}{2}\Big)\Big].$ Thus, by using the above result, we get for $b\leq 1$ $$\mathbb{P}(\max_{t\leq 1} W_t\geq 1, W_1\leq b)=\int_{-\infty}^b \frac{1}{\sqrt{2\pi}} \ e^{-(2-x)^2/2} \ dx.$$ However, I could not find an integral of this form for the case $b\geq 1$ . Can someone offer me guidance on how to proceed from here? The answer, by the way, is $$ \mathbb{P}(\max_{t\leq 1}  (W_t+t)\geq 1)=\int_1^\infty e^{x-\frac{1}{2}}\frac{1}{\sqrt{2\pi}} \ e^{-x^2/2} \ dx + \int_{-\infty}^1 e^{x-\frac{1}{2}} \frac{1}{\sqrt{2\pi}} \ e^{-(2-x)^2/2} \ dx.$$","['stochastic-processes', 'probability-distributions', 'brownian-motion', 'probability-theory']"
3980946,Possessing derivative for a complex function of a complex variable vs Being differentiable for a corresponding $\Bbb R^2\to\Bbb R^2$ function,"Assume $f(z)$ is a complex function of a complex variable $z$ . The definition of $f$ possessing a derivative at $z$ is the existence of the limit $$\lim\limits_{h\to0}\frac{f(z+h)-f(z)}{h}.$$ See, e.g., $\S$ 1.1 of ""Complex Analysis (3rd Ed.)"" by Lars V. Ahlfors. A multivariate vector-valued function $\mathbf{f}$ mapping from $\Bbb R^n\to\Bbb R^m$ is differentiable at $\bf{x}$ if there is a linear transformation $A$ of $\Bbb R^n$ into $\Bbb R^m$ such that $$\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}=0,$$ which is equivalent to $$\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{r}}({\bf{h}})|}{|{\bf{h}}|}=0,$$ where ${\bf{r}}({\bf{h}})={\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}$ . See, e.g., ""DIFFERENTIATION"" Section in Chapter 9 of Rudin's ""Principles of Mathematical Analysis (3rd Ed.)"". I've been being tempted to think of complex functions of a complex variable $f(z)=u(z)+iv(z)$ as a $\Bbb R^2\to\Bbb R^2$ function $\mathbf{f}(\mathbf{x})$ where $\mathbf{x}=(x,y)$ corresponds to $z$ and $\mathbf{f}=(u,v)$ , and subsequently the condition that $f(z)$ possesses derivative at $z$ (first paragraph) is equivalent to (i.e., can imply each other) $\mathbf{f}$ being differentiable at $\mathbf{x}$ with $n=m=2$ (second paragraph). But today, when I thought more about this, I found myself standing in front of an obvious contradiction that arises from the following theorem (P26 of Ahlfors' ""Complex Analysis (3rd Ed.)""): If $u(x,y)$ and $v(x,y)$ have continuous first-order partial derivatives which
satisfy the Cauchy-Riemann differential equations, then $f(z) = u(z) + iv(z)$ is analytic with continuous derivative $f'(z)$ , and conversely. If the above equivalence claim were correct, the continuity of first-order partial derivatives will guarantee differentiability of $\mathbf{f}$ (by, e.g., Theorem 9.21 on P219 of Rudin's text) and in turn that $f$ possesses derivative. Then the Cauchy-Riemann differential equations would be useless in the proof. Since the Ahlfors' text is so classical, I'm sure this condition is indispensable (I guess it is needed in line 11 and 12 of P26 in Ahlfors' text, right?). As a result, the equivalence claim in the above paragraph must be wrong. Then I tried to figure out why on earth they are not equivalent. Later I came to a point that (1) $\Rightarrow$ (2) while (2) $\not\Rightarrow$ (1), where (1) stands for $f$ possessing derivative and (2) for the corresponding $\Bbb R^2\to\Bbb R^2$ function $\mathbf{f}$ being differentiable. That is, (1) is stronger than (2). I guess the strongerness comes from the fact that $\Bbb C$ defines a multiplication operation, which is not available in $\Bbb R^2$ . The contradiction can be solved by this relation: the continuity of first-order partial derivatives establishes (2), but we need additional conditions (the Cauchy-Riemann differential equations) to arrive at a stronger conclusion (1). To show the above relation, First, I gave a proof of (1) $\Rightarrow$ (2): Assume the derivative is a complex number $\alpha$ and we define linear transformation according to complex number multiplication: if ${\bf{h}}=(x,y)$ , $A{\bf{h}}\cong\alpha(x+iy)$ (an isometry between $\Bbb R^2$ and the complex plane). Since $\lim\limits_{z\to0}|f(z)|=0$ iff $\lim\limits_{z\to0}f(z)=0$ , we have $$\begin{eqnarray}
\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}&=&\lim\limits_{h\to0}\frac{|f(x+h)-f(x)-\alpha h|}{|h|}\\
&=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)-\alpha h}{h}\\
&=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h}-\alpha\\
&=&\alpha-\alpha=0
\end{eqnarray}$$ For (2) $\not\Rightarrow$ (1): From the equivalent formulation of differentiability, we have $f(z+h)-f(z)=A{\bf{h}}+r(h)$ where $r(h)=o(h)$ . If we can write $A{\bf{h}}\cong\alpha h$ for some complex number $\alpha$ as in the last paragraph, then the derivative of $f$ is immediate. But the thing is, we can't (at least I can't). $A{\bf{h}}/h$ is not necessarily a constant complex number for a given general linear transformation $A$ . That's why (2) does not imply (1). Now comes my question: 1) Is it true that possessing derivative for a complex function of a complex variable is not equivalent to its corresponding $\Bbb R^2\to\Bbb R^2$ function being differentiable? 2) Is my relation that (1) $\Rightarrow$ (2) while (2) $\not\Rightarrow$ (1) a correct argument to explain this inequivalence? 3) Is there anything wrong (or not rigorous) throughout my argument? Thank you.","['complex-analysis', 'multivariable-calculus', 'derivatives']"
3980977,What is the derivative of $(\mathbf{u}(x))^TA\mathbf{v}(x)$ w.r.t. $x$?,"What is $\frac{d}{dx}(\mathbf{u}^TA\mathbf{v})$ where $\mathbf{u}$ and $\mathbf{v}$ are column-vector-valued functions of $x$ , and $A$ is a matrix? Is it $\frac{d\mathbf{u}^T}{dx}A\mathbf{v}+\mathbf{u}^TA\frac{d\mathbf{v}}{dx}$ ?","['derivatives', 'matrix-calculus', 'linear-algebra']"
3981050,When is the glueing of affine schemes again affine?,"Let $(R_i)_{i \in I}$ be a family of rings and suppose that we have compatible isomorphisms of subschemes of the spectra of the $R_i$ so that we are able to glue the corresponding affine schemes $\text{Spec}(R_i)$ . Question: Are there sufficient condition on the rings $R_i$ and on the given maps such that the glued scheme is again affine. I have the feeling that this works out in very special situations and this is also my motivation for the question: Let $A$ be an integral domain with quotient field $K$ . The space $\text{Zar}(K \mid A)$ is the set of all valuation overrings of $A$ endowed with the Zariski topology. It is known that $\text{Zar}(K\mid A)$ is a spectral space, that is, it is homeomorphic to the spectrum of a ring. Indeed, there are explicit constructions of Kronecker function rings, which are Bézout domains and have spectrum homeomorphic to $\text{Zar}(K\mid A)$ for given $A$ . But these constructions are quite out-of-the-hat, in my opinion, so here is my idea to this problem: The subspace $\text{Zar}(K\mid V)$ is totally ordered by set-theoretical inclusion and homeomorphic to $\text{Spec}(V)$ by localization, for each $V \in \text{Zar}(K\mid A)$ . Moreover, $\text{Zar}(K\mid A)$ is covered by those. So if we glue them at their overlapping subschemes, we get a scheme whose underlying space is homeomorphic to $\text{Zar}(K\mid A)$ . If this scheme would be affine, then we have reached the goal. Thank you for your help!","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'valuation-theory', 'schemes']"
3981052,"If $\mathbb E[X^nY^m]=\mathbb E[X^n]\mathbb E[Y^m]$ for all $n,m$, then $X$ and $Y$ are independent.","I had an exam this morning, and I had to prove that for $X$ and $Y$ bounded, if for all $n$ and all $m$ , $$\mathbb E[X^nY^m]=\mathbb E[X^n]\mathbb E[Y^m],$$ then $X$ and $Y$ are independents. Using Characteristic function I think that I could justify it using the fact that $$\cos(tX)=\sum_{k=0}^\infty \frac{(-1)^kt^{2k}X^{2k}}{(2k!)}\quad \text{and}\quad \sin(tX)=\sum_{k=0}^\infty \frac{(-1)^kt^{2k+1}X^{2k+1}}{(2k+1)!},$$ but how can I do without characteristic function, and out measure theory ? (it's a lecture of introduction of probability, so we don't have tools of measure theory as DCT or MCT, neither characteristic function). My definition of the expectation is : for $X$ s.t. $\mathbb E|X|<\infty $ , $$\mathbb E[X]=\lim_{n\to \infty }\mathbb E[X_n]=\sum_{k\in\mathbb Z}\frac{k}{2^n}\mathbb P\left(X_n=\frac{k}{2^n}\right),$$ where $X_n=2^{-n}\lfloor 2^nX\rfloor$ .","['expected-value', 'independence', 'probability-theory']"
3981098,"How to cut a cube out of a tree stump, such that a pair of opposing vertices are in the center?","I saw this picture of a cube cut out of a tree stump. I've been trying to craft the same thing out of a tree stump, but I found it hard to figure out how to do it. One of the opposing vertices pair is on the center of the tree stump: I've been struggling to find the numbers and angle needed to make the cuts. Any kind of advice would be greatly appreciated. Thanks for taking your time to read this and I'm sorry for my bad english.",['geometry']
3981110,Calculate empty labeled boxes in labeled boxes-balls problem,"The problem is to count the number of times that K boxes are empty when n labeled balls are distributed into m labeled boxes. So, if we have n balls and m boxes, we have $m^{n}$ possibilities. I would like to count the number of times that box 1 and/or box 2 for example are empty. For example, how many times box 1 or box 4 ( or both ) are empty when we distribute 10 balls between 10 boxes?. There are $10^{10}$ possibilities. How can it be calculated?","['permutations', 'combinations', 'combinatorics', 'balls-in-bins']"
3981122,What is the Kronecker Product of two vectors?,"In my numerical methods course we got a homework problem that has a definition of a function $\phi(x) = vec(M) - x \otimes x $ where $x\otimes x$ is the kronecker product of an n-vector and $ M $ is an $n\times n$ Matrix that is vectorized (flattened) in column-major by the $vec()$ operator. I got confused as I thought the kronecker product would produce an $ n\times n$ matrix.
But according to the instructor it's a vector? How would I compute the kronecker product of two vectors?
I thought it would be the entries of the first vector times the second vector appended in a matrix. Thanks for anyone who can share some enlightenment. I tried to find some on Wikipedia but the examples there confirm my confusion. I looked at this already: Kronecker product and outer product confusion Thank you!","['matrices', 'linear-algebra', 'kronecker-product']"
3981136,Integral with residue theorem,"I have to resolve the following integral for a proof of theorem. It is used the residue theorem. However I am not in confidence with this argument. I would like to have more detailed step. The integral is $$\int_{-\infty}^{\infty}\frac{{\rm d}k_{1}}{2\pi} \frac{e^{{\rm i}k_{1}\,x_{1}}}{{\rm i}k_{0} + k_{1}}$$ The integral can be solved in the comples plane in the half circle with radius $R\rightarrow \infty$ that rests on the real axis and lies in the upper half plane. $$\int_{-\infty}^{\infty}\frac{dk_1}{2\pi} \frac{e^{ik_1 x_1}}{ik_0+k_1}=\int_{half\,circle}\frac{dk_1}{2\pi} \frac{e^{ik_1 x_1}}{ik_0+k_1}
$$ The integral on the circumnference goes to zero, infact, using the change of variable $k_1=Re^{i\theta}$ , one has: \begin{align}
&\int_{\smallfrown R}\frac{dk_1}{2\pi} \frac{e^{ik_1 x_1}}{ik_0+k_1}
\\[5mm] = &\
\int_{0}^{\pi} \frac{d\theta i Re^{i\theta}}{2\pi}\frac{e^{ix_1Rcos\theta}e^{-x_1Rsin\theta}}{ik_0+Re^{i\theta}}\rightarrow 0 \,\,\,\mbox{for}\,R\rightarrow\infty
\end{align} And here the first question: how can it goes to zero? At this point one can resolves using the residue theorem. The unique pole is in $k_1=-ik_0$ so to have a non zero result, one needs $k_0<0$ . Why is the integral equal to zero in the case $k_0>0$ ? So in the hypothesis that $x_1>0$ and $k_0<0$ one has: $$\int_{-\infty}^{\infty}\frac{dk_1}{2\pi} \frac{e^{ik_1 x_1}}{ik_0+k_1}=ie^{k_0x_1}$$","['integration', 'complex-analysis', 'complex-integration', 'residue-calculus']"
3981156,Let $X_t$ be a solution of a SDE. Does the set $\{X_t \in \{p\}\}$ has null measure?,"I think this question is easy. However, I have not been able to solve it. Let $a,\sigma:\mathbb{R}\times \mathbb R\to\mathbb{R}$ , smooth functions such that $\sigma>0$ . Consider the 1-dimensional SDE, $$dX_t = a(X_t,t) dt + \sigma(X_t,t) dW_t$$ $$X_0 = x_0\in\mathbb{R}. $$ where $W_t$ is a Brownian motion. Fixing $y\in\mathbb R$ and $t>0$ , I was interested in showing that $$\mathbb{P}\left(\{\omega \in \Omega;\ X_t = y\}\right)=0.$$ Where $(\Omega,\mathcal F, \mathbb P)$ , is the probability space being considered. Does anyone know if the above equation is true? A reference would be enough for me.","['stochastic-processes', 'measure-theory', 'stochastic-differential-equations', 'probability-theory']"
3981161,An estimate involving Polar Coordinates,"I encountered the following computation in a paper: The highlighted $t$ is mysterious to me, how did it end up there? For some guidance: the first line is fundamental theorem of calculus and change of variables. The last equality is change of variables and fubini. Thanks!","['multivariable-calculus', 'partial-differential-equations', 'polar-coordinates', 'differential-geometry']"
3981235,"$W_0^{1,p} \cap W^{1,q} = W_0^{1,q}$?","Let $\Omega \subset \mathbb R^n$ be an open set. We denote by $W^{1,p}(\Omega)$ the usual Sobolev spaces and $W_0^{1,p}(\Omega)$ is the closure of $C_c^\infty(\Omega)$ in $W^{1,p}(\Omega)$ . Let $1 \le p < q \le \infty$ . Do we have $W_0^{1,p}(\Omega) \cap W^{1,q}(\Omega) = W_0^{1,q}(\Omega)$ ? Of course, "" $\supset$ "" is clear, but "" $\subset$ "" is hard.
I do not see that this inclusion can be tackled by applying the definitions directly. If the boundary of $\Omega$ possesses some regularity (e.g. Lipschitz boundary), then we can define a trace operator (which is independent of the regularity exponent) and the conclusion follows from $$W_0^{1,r}(\Omega) = \{ u \in W^{1,r}(\Omega) \;\mid\; u = 0 \text{ on }\partial\Omega\}.$$ Is there some easier argument? In particular, I would like to see what happens in case that $\partial\Omega$ has low regularity (or ""no regularity"" at all).","['sobolev-spaces', 'functional-analysis', 'trace-map']"
3981318,"Can any positive real be approximated as $2^m/3^n$ with $(m,n)$ large enough?","Conjecture. There exist positive integers $(m,n)$ large enough, such that for any positive real number $r$ and a given error $\epsilon$ : $$
   \left| r - \frac{2^m}{3^n} \right| < \epsilon
$$ There is numerical evidence for this conjecture. I have tried $r = \sqrt{2}$ and $\epsilon = 10^{-3}$ . Below is a little Delphi Pascal program with accompanying output. But .. can somebody prove the conjecture? program apart; procedure test(r : double; eps : double);
var
  a : double;
  m,n : integer;
begin
  a := 1;
  m := 0; n := 0;
  while true do
  begin
    if a < r then
    begin
      m := m + 1;
      a := a * 2;
    end else begin
      n := n + 1;
      a := a / 3;
    end;
    if abs(r-a) < eps then Break;
  end;
  Writeln(r,' = 2^',m,'/3^',n,' =',a);
end; begin
  test(sqrt(2),1.E-3);
end. Output: 1.41421356237310E+0000 = 2^243/3^153 = 1.41493657935359E+0000 UPDATE. The answer by lhf does look like a very concise proof. But for me - as an retired physicist by education - it's a bit beyond comprehension. Furthermore, it leaves a few issues untouched. One might ask for example whether there are estimates for $m$ and $n$ when $r$ and $\epsilon$ are given. Note. The question can also be formulated as: Can any positive real be approximated as $3^m/2^n$ with $(m,n)$ large enough? Which is the same as allowing negative integers with the original formulation. In this form, it shows some resemblance to the (in)famous Collatz problem . EDIT. As suggested by the answers, an approach with logarithms could be more effective: program anders; procedure proef(r : double; eps : double);
var
  a,l2,l3,lr : double;
  m,n : integer;
begin
  l2 := ln(2); l3 := ln(3);
  lr := ln(r); a := 0;
  m := 0; n := 0;
  while true do
  begin
    a := m*l2 - n*l3 - lr;
    if abs(a) < eps then Break;
    if a < 0 then m := m + 1 else n := n + 1;
  end;
  Writeln(r,' = 2^',m,'/3^',n,' =',exp(a)*r);
end; begin
  proef(sqrt(2),1.E-3);
  proef(sqrt(2),1.E-9);
end. Output: 1.41421356237310E+0000 = 2^243/3^153 = 1.41493657935356E+0000
 1.41421356237310E+0000 = 2^911485507/3^575083326 = 1.41421356125035E+0000 The first line in the output is almost identical to the result obtained previously . The last line in the output shows that the latter approach indeed is more effective. The error plays the same role in both approaches. Oh well, almost.
Let's take a look at the places where the 'Break's are. First program: $$
\left| r - \frac{2^m}{3^n} \right| < \epsilon
$$ Second program: $$
-\epsilon < m\ln(2) - n\ln(3) - \ln(r) < +\epsilon \\
\ln(1-\epsilon) < \ln\left(\frac{2^m/3^n}{r}\right) < \ln(1+\epsilon) \\
-\epsilon < \frac{2^m/3^n}{r} - 1 < +\epsilon \\
\left| r - \frac{2^m}{3^n} \right| < \epsilon.r
$$ So $\epsilon$ in the first program is an absolute error,
while $\epsilon$ in the second program is a relative error. Continuing story at: Can the Stern-Brocot tree be employed for better convergence of $2^m/3^n$ ?","['limits', 'irrational-numbers', 'rational-numbers']"
3981325,$\arctan x$ vs $1/\tan x$,"Why is $\arctan x = (\tan x)^{-1} = \tan^{-1}x$ but is not equal to $1/\tan x$ , even though $\tan^{2}x = (\tan x) \cdot (\tan x)$ ? Is there not another way to write $1/\tan x$ ?","['notation', 'trigonometry', 'inverse-function']"
3981346,How to rigorously define the parametric derivative?,I'm trying to understand the parametric derivative identity $$ \frac{dy(t)}{dt} = \frac{dy}{dx} \frac{dx}{dt} \tag{1}$$ I feel this is not rigorous because we are say that $y(t)$ can be written as $y(x(t) )$ what conditions are required on $y$ for this to be true?,"['parametric', 'derivatives']"
3981366,Details of the proof of the Exit Time Theorem,"Theorem 1.29. Let $V_{A}=\inf \left\{n \geq 0: X_{n} \in A\right\} .$ Suppose $C=S-A$ is finite, and that $P_{x}\left(V_{A}<\infty\right)>0$ for any $x \in C .$ If $g(a)=0$ for all $a \in A,$ and for $x \in C$ we have $$
g(x)=1+\sum_{y} p(x, y) g(y)
$$ Then $g(x)=E_{x}\left(V_{A}\right)$ Proof. It follows from Lemma 1.3 that $E_{x} V_{A}<\infty$ for all $x \in C .(1.27)$ implies that $g(x)=1+E_{x} g\left(X_{1}\right)$ when $x \notin A .$ The Markov property implies $$
g(x)=E_{x}\left(V_{A} \wedge n\right)+E_{x} g\left(X_{V_{A} \wedge n}\right)
$$ We have to stop at time $T$ because the equation is not valid for $x \in A .$ It follows from the definition of the expected value that $E_{x}\left(V_{A} \wedge n\right) \uparrow E_{x} V_{A} .$ Since $g$ is bounded and $g(a)=0$ for $a \in A,$ we have $E_{x} g\left(X_{V_{A}  \wedge n}\right) \rightarrow 0$ This theorem 1.29 is from Richard Durrett's 《Essentials of Stochastic the Processes Third Edition》(Page 62-63).In this theorem, g(x) is expected exit time with initial state x.The hint is simply to use the Markov property,but I finally can't prove it.Could someone show me the details of the proof that $g(x)=1+E_{x} g\left(X_{1}\right)$ is equivalent to $
g(x)=E_{x}\left(V_{A} \wedge n\right)+E_{x} g\left(X_{V_{A} \wedge n}\right)
$ ?Thank you very much！","['stochastic-analysis', 'stochastic-processes', 'probability-theory', 'probability']"
3981371,"Is the identity map the only map from the positive integers to itself that simultaneously preserves multiplication, order, and primes?","Is the identity map the only map $f$ from the positive integers $\mathbb{Z}_{+}$ to itself that satisfies all three of the following properties? $f(mn)=f(m)f(n)$ for all positive integers $m$ and $n$ (which implies that $f(1)=1$ ) $f(m)<f(n)$ whenever $m<n$ (which implies that $f$ is injective) $f(p)$ is prime for any prime number $p$ The above three properties (actually, for property 2, the weaker assumption that $f$ is injective suffices) imply that applying $f$ to any positive integer will not change the prime signature, because all that happens with $f(n)$ is that each prime $p$ dividing $n$ is replaced with $f(p)$ while maintaining the same exponent. Here are some examples that suggest that the answer might be ""Yes"" (because they only satisfy properties 1 and 3 and injectivity, but are not order-preserving on all positive integers, although they are order-preserving on prime numbers): Let $f(p)$ be the next prime following $p$ , with $f$ extended to composite numbers using (complete) multiplicativity. Then, $8<9$ , while $f(8)=f(2^3)=f(2)^3=3^3=27>25=5^2=f(3)^2=f(3^2)=f(9)$ . Let $f$ (the $n$ th prime) be the $(2n)$ th prime. Then, $27<32$ , while $f(27)=f(3^3)=f(3)^3=7^3=343>243=3^5=f(2)^5=f(2^5)=f(32)$ . Let $f(p)$ be the $p$ th prime. Then, $f(8)=27>25=f(9)$ , as in the first example. Let $f$ (the $n$ th prime) be the $(n^2)$ th prime. Then, $3<4$ , while $f(3)=7>4=2^2=f(2)^2=f(2^2)=f(4)$ . But of course, specific examples (using inductive reasoning) are not enough to conclude that the answer is in fact ""Yes"".",['number-theory']
3981401,Slutsky's theorem and the asymptotic normality of MLE,"I understand that $$\left[I(\theta_0)\right]^{1/2}(\hat{\theta}-\theta_0)\overset{d}{\rightarrow}N(0,1),$$ where $I(\theta_0)$ is the Fisher information at $\theta_0$ , $\hat{\theta}$ is a MLE of $\theta$ . My note says $I(\theta_0)$ can be replaced by $I(\hat{\theta})$ justified by Slutsky's theorem. Here is my proof: $$I(\hat{\theta})^{1/2}(\hat{\theta}-\theta_0)=I(\hat{\theta})^{1/2}I(\theta_0)^{-1/2}I(\theta_0)^{1/2}(\hat{\theta}-\theta_0)\overset{d}{\rightarrow}N(0,1),$$ since $I(\hat{\theta})^{1/2}I(\theta_0)^{-1/2}\overset{p}{\rightarrow}1$ , we have $$\left[I(\hat{\theta})\right]^{1/2}(\hat{\theta}-\theta_0)\overset{d}{\rightarrow}N(0,1)$$ Is this correct? If not, what part is wrong? In this proof, I used $I(\hat{\theta})^{1/2}I(\theta_0)^{-1/2}\overset{p}{\rightarrow}1$ . This hold when $\hat{\theta}\overset{p}{\rightarrow}\theta_0\implies I(\hat{\theta})^{1/2}\overset{p}{\rightarrow}I(\theta_0)^{1/2}$ . Is $I(\theta)$ always continuous for it to hold, or the continuity of $I(\theta)$ is an assumption we need to make?","['statistics', 'asymptotics', 'maximum-likelihood', 'fisher-information', 'probability-theory']"
3981405,Is every direct product of (finite) cyclic groups abelian?,"Long story short I'm halfway through a proof and have hit a step where I show that a group order $p^2$ is abelian. I did this by splitting into the $C_{p^2}$ case and the $C_p\times C_p$ case, the first being trivial as all cyclic groups are abelian. The second case I proved by using the definition of the direct product and showing it directly from the definition of the two $C_p$ groups. However this feels odd. Something feels off - especially added to the fact that my proof seemingly holds for any two cyclic groups, implying the direct product of any two cyclic groups is abelian, and yet I can't find anything online talking about this. Is it just so trivial nobody mentions it? The only thing I consistently see brought up is that ""every cyclic group is an abelian group"" (trivial), and ""every finitely generated abelian group is a direct product of cyclic groups"", which is the converse of what I'm trying to show.","['cyclic-groups', 'finite-groups', 'group-theory', 'p-groups', 'abelian-groups']"
3981410,Show $\overline{\oint_{\gamma}f(z)dz} =- \oint_{\gamma}\overline{f(z)}\cdot z^{-2}dz$,"Let $U \subset \Bbb C$ and $f:U \to \Bbb C$ continuous. Furthermore let $\gamma: [0,2 \pi] \to U$ be a curve given by $\gamma(t)=exp(it)$ . To show is: $$\overline{\oint_{\gamma}f(z)dz} =- \oint_{\gamma}\overline{f(z)}\cdot z^{-2}dz$$ I could show: $\overline{\int_\gamma \! f(z) \,dz}=\overline{\int_0^{2\pi} \! f(e^{it})\cdot ie^{it} \,dt}=\int_0^{2\pi} \! \overline{f(e^{it})\cdot ie^{it}} \,dt$ but now I can't end the proof. Can someone helps me?","['integration', 'complex-analysis']"
3981454,"Difficulties solving this integral: $ \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x $ by differentiation under the integral sign","So in the book Advanced Calculus Explored, by Hamza E. Asamraee. The next integral appears as an exercise to solve by differentiating under the integral sign: $$ \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x $$ I have solved this integral before by substitution and change in the limits of integration, but in this chapter the book asks to solve it by differentiation under the integral sign. I have tried several ways of solving this, but the only one that i thought it was leading me somewhere was: $$f(a) = \int_0^1 \frac{\ln(x+a)} {x^2 + 1} \, \mathrm{d}x $$ So that: $$f'(a) = \int_0^1 \frac{1} {(x+a)(x^2 + 1)} \, \mathrm{d}x $$ Then i tried to separate this last integral by partial fractions, my result on this was: $$\frac {1} {(x+a)(x^2 + 1)} = \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1}\right)$$ And the integral reduces to: $$f'(a) = \int_0^1 \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1} \right) \, \mathrm{d}x $$ Then this last expression evaluates to: $$f'(a) = \frac{1} {a^2 + 1} (\ln(a+1) - \ln(a) - \ln(4)+ \frac{π}{4} a)$$ Then integrating from 0 to 1 with respect to $a$ we will get: $$f(1) - f(0) = \int_0^1 \frac{\ln(a+1)} {a^2 + 1} \, \mathrm{d}a - \int_0^1 \frac{\ln(a)} {a^2 + 1} \, \mathrm{d}a $$ (The last two terms of $f'(a)$ cancel each other after the integration so i didn't wrote them) But then the two integrals on the right hand side are equal to $f(1) - f(0)$ so the differentiation under the integral led nowhere. Do i need some other approach? Or did i made any mistake?
Any help is appreciated.","['integration', 'calculus', 'definite-integrals', 'derivatives']"
3981617,Is the posterior always a compromise between the prior and the data?,"Suppose that we are interested in learning the proportion of the population $\theta$ with a particular property (for instance, the fraction of the population who are male). Suppose that we randomly sample $n$ members of this population (with replacement, to make things easier) and observe that $y$ of them have the property (so the fraction of the sample with the property is $y/n$ ). We start with a continuous prior $p(\theta)$ with full support $[0, 1]$ and update this using Bayes rule. Question: does the expected value of the posterior always lie between the prior expectation and the sample fraction $y/n$ ? Comments :
I know this is true in the case where my prior takes the form of a beta distribution (with parameters $\alpha$ , $\beta$ ). In that case, we know that the prior expectation is $$
\mathbb{E}[\theta] = \frac{\alpha}{\alpha + \beta}
$$ Due to random sampling, the probability that $y$ of the $n$ draws have the property is $$
P(\text{data}\mid\theta) = \binom{n}{y} \theta^y (1 - \theta)^{n-y}
$$ which means (see, e.g., here ) that the posterior $P(\theta\mid\text{data})$ is also beta distributed and has expected value $$
\mathbb{E}[\theta\mid\text{data}] = \frac{\alpha + y}{\alpha + \beta + n}
$$ Moreover, it can be shown that $$
\frac{\alpha + y}{\alpha + \beta + n} \in \left[\frac{y}{n}, \frac{\alpha}{\alpha + \beta}\right]
$$ where both inequalities are strict provided that $y/n \neq \alpha / (\alpha+\beta)$ . So the posterior expectation does indeed lie between the prior expectation and the sample fraction in this instance. To now examine the more general case, consider an arbitrary smooth prior $p(\theta)$ with full support on $[0, 1]$ and expected value $\mathbb{E[\theta]}$ . By Bayes theorem, the posterior is \begin{split}
p(\theta\mid\text{data}) &= \frac{p(\text{data}\mid\theta)P(\theta)}{P(\text{data})} \\
&= \frac{\binom{n}{y} \theta^y (1 - \theta)^{n-y}P(\theta)}{\int_0^1 P(\text{data}\mid\theta)P(\theta) d\theta}  \\
&= \frac{\binom{n}{y} \theta^y (1 - \theta)^{n-y}P(\theta)}{\int_0^1 \binom{n}{y} \theta^y (1 - \theta)^{n-y}P(\theta) \, d\theta} \\
&= \frac{\theta^y (1 - \theta)^{n-y}P(\theta)}{\int_0^1 \theta^y (1 - \theta)^{n-y}P(\theta) \, d\theta} 
\end{split} and so the posterior expectation is \begin{split}
\mathbb{E[\theta}\mid\text{data}] &= \int_0^1 p(\theta\mid\text{data}) \theta \, d\theta \\
&= \int_0^1 \frac{\theta^y (1 - \theta)^{n-y}P(\theta)}{\int_0^1 \theta^y (1 - \theta)^{n-y}P(\theta) \, d\theta}  \theta \, d\theta \\
&= \frac{\int_0^1  \theta^y (1 - \theta)^{n-y}P(\theta) \theta \, d\theta}{\int_0^1 \theta^y (1 - \theta)^{n-y}P(\theta) \, d\theta}   \\
&= \frac{\mathbb{E}[\theta^{y+1} (1 - \theta)^{n-y}]}{\mathbb{E}[\theta^y (1 - \theta)^{n-y}]}
\end{split} It remains to show that $$
\frac{\mathbb{E}[\theta^{y+1} (1 - \theta)^{n-y}]}{\mathbb{E}[\theta^y (1 - \theta)^{n-y}]} \in \left[ \mathbb{E}[\theta], \frac{y}{n} \right];
$$ but this is where things begin to get tricky. Very heuristically (apologies for what is to come!), one might try to obtain one bound using an argument like \begin{split}
\frac{\mathbb{E}[\theta^{y+1} (1 - \theta)^{n-y}]}{\mathbb{E}[\theta^y (1 - \theta)^{n-y}]} &\geq
\frac{\mathbb{E}[\theta^{y+1}] \mathbb{E}[(1 - \theta)^{n-y}]}{\mathbb{E}[\theta^y] \mathbb{E}[(1 - \theta)^{n-y}]} \\ &\geq
\frac{\mathbb{E}[\theta]^{y+1} \mathbb{E}[(1 - \theta)]^{n-y}}{\mathbb{E}[\theta]^y \mathbb{E}[(1 - \theta)]^{n-y}} \\ &= 
\mathbb{E}[\theta]
\end{split} ...although, quite aside from the very dubious feel to this argument, it doesn't give us the $y/n$ bound. Any help would be much appreciated!","['bayes-theorem', 'probability']"
3981633,Orthogonal projection onto a subvector space of $L^2(\mathbb{R})$,"Consider the subvectorspace $K\subset L^2(\mathbb{R})$ defined as $$K=\left\{f\in L^2(\mathbb{R}) | \forall n\in\mathbb{Z}:\int_n^{n+1}f(x)=0\right\}.$$ I'm trying to find the orthogonal projection $p_K:L^2(\mathbb{R})\rightarrow K$ . I know I can find $p_K$ by finding an orthonormal basis $(e_k)_{k\in\mathbb{Z}}$ , for $K$ . Since $K$ is closed we hopefully can conclude that $K=\text{closure(span}\{e_k|k\in\mathbb{Z}\})$ (I will not state a proof here). Then, for every $f\in L^2(\mathbb{R})$ , we would have $$p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k.$$ I found an orthonormal set $(e_k)_{k\in\mathbb{Z}}$ where $e_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto e^{2\pi ixk}\chi_{[k,k+1]}(x)$ . It is easy to check that this set is indeed orthonormal. I now claim that this set is maximal orthonormal in $K$ , meaning that if for $f\in K$ we have that $\langle f,e_k\rangle=0$ for all $k\in\mathbb{Z}$ , then $f=0$ . Take an $f\in K$ such that $\langle f,e_k\rangle=0$ for all $k\in\mathbb{Z}$ and consider the inner product: $$\langle f,e_k\rangle = \int_\mathbb{R}f(x)e^{-2\pi ikx}\chi_{[k,k+1]}(x)dx=0.$$ Define the functions $h_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto f(x)\chi_{[k,k+1]}(x)$ . We then get $\langle f,e_k\rangle=\widehat{h_k}(k)=0$ for all $k\in\mathbb{Z}$ , where $\widehat{h_k}$ denotes the Fouriertransform of $h_k$ . Since the Fouriertransform of $h_k$ is always $0$ we find that $h_k$ is zero almost everywhere. Now $f=\sum_{k\in\mathbb{Z}h_k}$ so $f$ is zero almost everywhere and thus $f=0$ in $L^2(\mathbb{R})$ . So $\text{span}\{e_k|k\in\mathbb{Z}\}$ is dense in $K$ meaning for every $f\in L^2(\mathbb{R})$ the orthogonal projection onto $K$ is given by $$p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k,$$ where $e_k(x)=e^{2\pi ikx}\chi_{[k,k+1]}(x)$ and $\langle f,e_k\rangle=\int_k^{k+1}f(x)e^{-2\pi ikx}dx$ . Is this reasoning correct?","['orthogonality', 'vector-spaces', 'analysis', 'hilbert-spaces', 'solution-verification']"
3981637,Calculating of derivative,I'm a little confused with using derivatives. I understand that derivatives respond to question how result of function changes when input of function changes However I do not understand why we calculate derivative of $f(x) = x^2$ instead of calculating $f(2)$ and $f(2.001)$ and subtracting these two values of $f(2.001)$ from $f(2.0)$ Could someone explain that?,['derivatives']
3981639,"Double integral over $f(x,y) = y^2x+x^3$","Compute the integral $$\int_D f \ dx \ dy$$ where $f : D\to \mathbb{R}, f(x,y) = y^2x+x^3$ and $D= \{(x,y) \mid \sqrt{x^2+y^2} \leqslant 5, y >0\}$ Converting to polar coordinates I got $f(r,\theta) = r^3\cos(\theta)$ and $D = \{(r,\theta) \mid 0 \leqslant r \leqslant 5, 0\leqslant \theta \leqslant 2\pi \} $ So the integral would become $$\int_{0}^{2\pi}\int_{0}^{5} r^4\cos(\theta) \ dr \ d\theta = \int _0^{2\pi }\cos \left(θ\right)\cdot \:625 \ d\theta = 0.$$ I guess I have a mistake here since isn't this just a cone whose projection on to the $xy$ plane is a circle of radius $5$ and the area shouldn't evaluate to zero?","['integration', 'multivariable-calculus']"
3981677,Review a solution for second order partial derivative.,"Check please my solution for second derivative. If it's not correct, what are my mistakes? z=ctg(y/x) $\frac{dz}{dx} = \frac{y}{sin^2(\frac{y}{x})*x^2}$ $\frac{dz}{dy} = \frac{-1}{sin^2(\frac{y}{x})*x}$ A solution for second order derivatives $
z''xx = y * (\frac{1}{sin^2(\frac{y}{x})*x^2})' = y* \frac{0-sin^2(y/x)*x^2}{(sin^2(\frac{y}{x})*x^2)^2}- = \frac{y*(-2*sin(y/x)*2x)}{sin^4(\frac{y}{x})*x^4} =
\frac{-y*4x*sin(\frac{y}{x})}{sin^4(\frac{y}{x})*x^4}
$","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
3981703,Proof regarding a colored tree,"Let $T$ be a tree with $n$ vertices. Vertices of $T$ are colored in white and black such that the number of black vertices is always less than white ones and white and black vertices are not adjacent. Prove that the white vertices can be numbered with { 1 , 0 , -1 } such that the sum of numbers of the adjacent white vertices to every black vertex is zero considering that we have at least one none-zero vertex. It's easy to prove that there is at least one white leaf. And also if there is a black vertex connected to more that one white leaf then those leafs can be numbered in the way described in the question and the rest of the white vertices can be zero. But I don't know how to prove this for trees that are not like that.","['graph-theory', 'trees', 'induction', 'discrete-mathematics']"
3981835,Differentiability of $f(x)$ at $0$ provided that $\lim_{x\to0} [f(2x)-f(-x)]/x$ exists? [duplicate],"This question already has answers here : Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$ (3 answers) Closed 3 years ago . Assume that $f:\mathbb{R}\to\mathbb{R}$ is continuous at $x=0$ , and the limit \begin{equation}
\lim_{x\to0}\frac{f(2x)-f(-x)}{x}
\end{equation} exists. Is $f$ necessarily differentiable at 0? I was able to deduce that if $f$ is continuous in a neighbordhood of $0$ and has an isolated zero at $x=0$ , it must be differentiable (unless I made an error). I believe that this result holds if there exists some $\delta>0$ such that $f(x)$ does not change sign in the neiborhoods $-\delta<x<0$ and $0<x<\delta$ . But for the life of me I can't figure out a general proof (or a counterexample). Any help would be greatly appreicated!","['calculus', 'derivatives', 'real-analysis']"
3981849,About uniqueness of lifted paths in the phath lifting theorem in fiber bundles,"Im trying to understand the path lifting theorem (in the context of locally trivial fiber bundles presented in the book ""Manifolds, Tensor Analysis and Applications"", by Jerrold E. Marsden et. al. ), stated as follows: Let $\pi : E \to B$ a $C^{0}$ locally trivial fiber bundle and let $c: [0,1] \to B $ be a continuous path starting at $c(0)=b_0$ . Then for each $c_0 \in \pi^{-1}(b_0)$ there is a unique continuous path $\tilde{c}:[0,1] \to E $ such that $\tilde{c}(0)=c_0$ and $\pi \circ \tilde{c} = c$ . I think a conuterexample of this statement is obtained considering the trivial bundle $\mathbb{R}^{2} \times \mathbb{R}$ with the path $c:[0,1] \to \mathbb{R}^{2}$ , given by $c(t)=(\cos{2 \pi t}, \sin{2 \pi t})$ . Taking $b_0 = (1,0)$ and $c_0 = (1,0,a) \in \mathbb{R}$ , any lifted path $\tilde{c}(t)=(\cos{2 \pi t}, \sin{2 \pi t},f(t))$ , with $f: [0,1] \to \mathbb{R}$ continuous and $f(0)=a$ would fulfill $\pi \circ \tilde{c}$ and $\tilde{c}(0)=c_0$ . In this example the lifting is not unique, and I would appreciate if someone can show me what am I missing or if there is some property or condition missing in the path lifting theorem I quoted above. Thank you for your attention.","['manifolds', 'fiber-bundles', 'differential-geometry', 'real-analysis']"
3981856,Use integration by parts in cylindrical coordinates to turn these second derivatives into first derivatives,"I have a complex function $\psi(\rho,z)$ in cylindrical coordinates, which has the conditions $\psi=0$ as $z\rightarrow\pm\infty$ and $\rho\rightarrow +\infty$ . The quantity $E$ given by the double integral $$
E = 2\pi\int_{z=-\infty}^{z=+\infty}\int_{\rho=0}^{\rho=+\infty} \Bigg( \psi^*\frac{\partial^2\psi}{\partial\rho^2} + \psi^*\frac{1}{\rho}\frac{\partial\psi}{\partial\rho} + \psi^*\frac{\partial^2\psi}{\partial z^2} \Bigg) \rho \;\textrm{d}\rho\, \textrm{d}z \tag{1}
$$ where $\psi^*$ denotes the complex conjugate of $\psi$ , and the $2\pi$ appears because I have integrated out the angular coordinate, since $\psi$ is cylindrically-symmetric. (The quantity $E$ expresses the kinetic energy of a quantum mechanical wavefunction). I would like to simplify this expression so that it contains first derivatives instead of second derivatives. I know that in one-dimension in a variable $x$ , I can use integration by parts to write the following $$
\int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x = \Bigg[ \psi^* \frac{\partial\psi}{\partial x} \Bigg]^{+\infty}_{-\infty} - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x
$$ and if $\psi=0$ at $\pm\infty$ the first term vanishes to give $$
\begin{align}
\int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x &= - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x\\
&= - \int_{x=-\infty}^{x=+\infty} \bigg(\frac{\partial\psi^*}{\partial x}\bigg)^* \bigg(\frac{\partial\psi}{\partial x}\bigg) \,\textrm{d}x \\
&= - \int_{x=-\infty}^{x=+\infty} \Bigg|\frac{\partial\psi}{\partial x}\Bigg|^2\,\textrm{d}x .
\end{align}
$$ Is it possible to use a similar trick to write Eq. (1) in terms of first derivatives? I am struggling with the order of integration and how to apply the integration by parts in 2D here, given that there is an extra factor of $\rho$ from the volume element in cylindrical coordinates. If someone could take me through it that would be great, thank you!","['integration', 'cylindrical-coordinates', 'calculus', 'quantum-mechanics', 'derivatives']"
3981861,"Prove/disprove $(\int_0^{2 \pi} \!\!\cos f(x) \, d x)^2+(\int_0^{2 \pi}\!\!\! \sqrt{(f'(x))^2+\sin ^2 f(x)} \, dx)^2\ge 4\pi^2$","Let $f(x)$ be a differentiable function on $[0,2\pi]$ s.t. $0\leq f(x)\leq 2\pi$ and $f(0)=f(2\pi)$ . Prove or disprove that $$
\left(\int_0^{2 \pi} \cos f(x) \,d x\right)^2+\left(\int_0^{2 \pi} \sqrt{(f'(x))^2+\sin^2 f(x)} \, d x\right)^2 \geq(2 \pi)^2
$$ It seems that when $f$ is an arbitrary constant, the left side equals $(2\pi)^2$ and seems to be the minimum. But how can I show that there's no other $f$ that makes the left side equal (or be less than) $(2\pi)^2$ ? A geometric interpretation of the inequality has been found: Consider a closed curve on a sphere: $C=\{(\cos x\cdot\sin f(x),\,\sin x\cdot\sin f(x),\,\cos f(x))\mid x\in[0,2\pi)\}$ , we have its perimeter $\displaystyle L=\int_0^{2\pi}\sqrt{(f'(x))^2+\sin^2 f(x)}\,dx$ and its area $\displaystyle S=2\pi-\int_0^{2\pi}\cos f(x)\,dx$ . From spherical isoperimetric inequality $L^2\ge S(4\pi-S)$ , we have $(2\pi-S)^2+L^2\ge(2\pi)^2$ , and the equality holds iff $C$ is any circle on the sphere. In this way we get the original inequality in the sense of geometry. Now the question is, how to prove the inequality with only pure analysis methods ?","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'inequality']"
3981877,Rudin RCA Problem 6.4,"I'm trying to solve the following Problem 6.4 from Rudin's RCA: Suppose that $1\leq p\leq \infty$ , and $q$ is the exponent conjugate to $p$ . Suppose that $\mu$ is a $\sigma$ -finite measure and $g$ is a measurable function such that $fg\in L^1(\mu)$ for every $f\in L^p(\mu)$ . Prove that then $g\in L^q(\mu)$ . I can get to the point where $\int_{E}|g|d\mu<\infty$ for every finite measure set $E$ , but I'm not sure where to proceed. I would appreciate any suggestions. Thanks!","['measure-theory', 'analysis', 'riesz-representation-theorem']"
3981911,Looking for a Banach space in which projection on a closed convex set is empty?,"It is well-known that projection on closed convex sets is nonempty in Hilbert spaces. I am interested in seeing a Banach space where the projection is empty on a closed convex set. Definition of Projection of $x $ on set $C$ : $$   P_{C} (x) := \{ y \in C ~| ~ d(x , C ) = \|x -y\|  \}  $$","['convex-analysis', 'functional-analysis', 'real-analysis']"
3981943,"Is the structure of ""solid angle"" conformal maps more interesting in higher dimensions than just conformal maps?","In the complex plane there exists a very large set of smooth conformal maps , (The famous Riemann Mapping Theorem states that between any two simply connected open sets in $\mathbb{C}$ there exists a biholomorphic mapping which is also conformal). The situation is much more bleak as soon you go up to 3 dimensions and higher where the only smooth conformal maps are the mobius transformations. This is the famous Liouville's Theorem I was thinking about trying to find a generalization of the concept of an ""angle"" so that the ""set of all generalized-angle preserving maps"" in $\mathbb{R}^3$ would have an interesting structure larger than merely mobius transformations. There is the obvious generalization, solid angles . In 3 dimensions these are defined for collections of 4 points at a time (we select 1 anchor point and consider the 3 vectors formed by the anchor to the 3 other non collinear points. And then take the area of the spherical triangle spanned by these 3 vectors). The solid angle can be calculated explicitly using a formula given here . So that leads to our question.... is the set of all smooth solid-angle-conformal functions from $\mathbb{R}^3 \rightarrow \mathbb{R}^3$ a larger set than the set of mobius transformations? And does there exist a generalization of the Riemann Mapping Theorem for them?","['complex-analysis', 'conformal-geometry', '3d']"
3981983,Ballistic Motion in Space with One Gravitating Body.,"I had an interesting math problem presented to me some time ago by a friend (he stated it in non-mathematical terms). At what angle would you launch a projectile from a spaceship/satellite such that it left that object and went on to hit another orbiting object? Then as a supplemental question he asked at what angle would you launch that projectile to hit the other orbiting object in the least amount of time? I assumed that the objects were only acted upon by a single spherically symmetric mass distribution such that I could treat it as a one-body problem for each object. Further, I assumed it all took place in the plane with a polar coordinate system so that I ended up with this simple system of nonlinear autonomous ODE's, $$ \begin{bmatrix} \frac{d \theta}{dt}  \\ \frac{dv}{dt}\\ \frac{dr}{dt} \end{bmatrix} =  \begin{bmatrix} \frac{h}{r^{2}} \\ \frac{h^{2}}{r^{3}} -\frac{\mu}{r^{2}} \\ v \end{bmatrix}.$$ Where the inital conditions for the projectile would be $\{ \theta_{i} , r_{i}, v_{\beta}\cos(\phi - \theta_{i}) + v_{i}\}$ with an $h_{\phi} = r_{i} (v_{\beta}\sin(\phi - \theta_{i}) + v_{\theta i})$ and the target objects' initial conditions are $\{ \theta_{i}^{'} , r_{i}^{'}, v_{i}^{'}\}$ with $h'= r^{'}_{i} v_{\theta i}^{'}$ . Where $\phi$ is the launch angle from the polar axis while $v_{\beta}$ is the magnitude of the projectiles velocity (which I assume to not change, only its launch direction) and $\mu$ is a constant relating to the gravitational field strength of the attracting object. Below is a picture depicting the general initial and final conditions. After all these preliminaries, i'm basically asking if there is a variational calculus or other simpler solution to solving this problem as perhaps a boundary problem of sorts mixed with an initial ODE problem. That is, aside from computationally pouring on through thousands of trajectories with minutely differing $\phi$ 's then numerically guessing at the appropriate approximate launch angle or angles that solve my question(s). Which. . . isn't what I exactly want to do and I desire to know if there is an equation, a single or system of ODE's, that I could solve themselves for this launch angle which gives a least time of travel or gives launch angles that would lead to a hit (irrespective of the time of travel). If you can help in anyway, this would be most appreciated. I'm a sophomore College student with little knowledge of solving robustly ODE's or even programming solvers for them.","['numerical-methods', 'ordinary-differential-equations', 'calculus-of-variations']"
3982080,"Let $a$ and $n$ be integers with $n>0$.
Show that the additive order of $a$ modulo $n$ is $\frac{n}{\gcd(a,n)}$.","Let $a$ and $n$ be integers with $n>0$ .
Show that the additive order of $a$ modulo $n$ is $\frac{n}{\gcd(a,n)}$ . attempt: Let $a$ and $n$ be integers with $n>0$ .
Let $m=\frac{n}{\gcd(a,n)}$ .
Then, \begin{equation*}
ma = \left(\frac{n}{\gcd(a,n)}\right)a =
n\left(\frac{a}{\gcd(a,n)}\right)
\equiv 0 \pmod{n}.
\end{equation*} Since $d=\gcd(a,n)$ is the greatest positive integer such that $d\mid a$ and $d \mid n$ , then $m$ is the smallest positive integer such that $n \mid ma$ . Hence, $m$ is the additive order of $a$ modulo $n$ . Am I true?","['ordered-groups', 'group-theory']"
3982090,Intersection of a downward directed family of sets,"Let $X$ be a set and $Y\subseteq X$ a nonempty subset. Let $R$ be an equivalence relation on $X$ and let $K=\{K_i: i\in I\}$ be a directed family of subsets of $X$ containing $Y$ . That is, for any $i,j\in I$ there exist $l\in I$ such that $K_l\subseteq K_i\cap K_j$ . I may add a comment to clarify my notation: For a subset $H\subseteq X$ , $H/R=\{ [x]_R: x\in H\}$ , that is, the set of $R$ -classes of elements in $H$ . It is also important that I am not assuming that any of the $K_i$ is the union of $R$ -classes, that is, it may happen that $[x]_R=[y]_R$ and $x\in K_i$ but $y\notin K_i$ . This is the real problem because otherwise the answer to the question would be trivial. My question is the following: Does $(\bigcap\limits_{i\in I}K_i)/R=\bigcap\limits_{i\in I}(K_i/R)$ ? (and does the cardinality of $I$ matter?) For me it is clear that $(\bigcap\limits_{i\in I}K_i)/R\subseteq\bigcap\limits_{i\in I}(K_i/R)$ . However, I am not able to prove the other inclusion. I can find examples where the equality does not hold if the family $K$ is not directed even for finite $I$ Edit 2: I posted this question thinking that it might be a set-theoretic property, in the context I was working $R$ is a $\emptyset$ -type-definable equivalence relation and all $K_i$ are $\emptyset$ -type-definable sets. In that case, one can use compactness theorem to prove the equality.","['elementary-set-theory', 'set-theory']"
3982170,Schröder–Bernstein Theorem proof help,"In Understanding Analysis, there's is a guided Schröder–Bernstein Theorem proof as an exercise. I'm trying to prove it on my own but I believe I'm confused on several points and would appreciate some aid and corrections where necessary. Here's what I have so far: a) By partitioning the domains, if we prove that $f$ from $A$ to $B$ and $g$ from $B'$ to $A'$ are bijective, then we can write a function: $$h(x)=\begin{cases} f(x)\text{ if}\ x\in A \\ g^{-1}(x)\text { if}\ x\in A'\\ \end{cases}$$ that is bijective and concludes the proof. b) We know that $A_1=X\backslash g(Y)$ and all subsequent $A_n$ s are $A_n\subseteq g(Y)$ . Thus $A_1\ne A_2$ and inductively, as $f$ is 1-1 and so $f(Z)\ne f(Z')$ when $Z\ne Z'$ and the same goes for $g$ , we have $g(f(Z))\ne g(f(Z'))$ when $Z\ne Z'$ and so $g(f(A_n))\ne g(f(A_{n-1}))$ .
Thus the $A$ s are disjoint pairwise and with the same logic the $B$ s are also disjoint. c) I don't get that, it seems true by definition to me... d)For every $x\in B'$ , $x$ can't be in any $f(A_n)$ for all $n$ and for all $g(x)$ , as one generation of $A$ feeds the next and $g(x)$ not in $A_1$ , then $g(x)$ not in any $A_n$ .
(I'm not very happy about d. to be honest).","['elementary-set-theory', 'solution-verification', 'discrete-mathematics', 'real-analysis']"
3982179,Approximating functions of Brownian motion variation,"Consider the unit interval $[0,1]$ and the equi-spaced partition $t_j=\frac{j}{n}$ , with $j=0,\dots,n$ . Suppose that $\sigma_t$ is a positive bounded cadlag process and $W_t$ a Brownian motion. I want to do the following approximation $$
g\left(\sqrt{n}\,\sigma_{\left(j-1\right)/n}\,\left(W_{(j+1)/n}-W_{j/n}\right)\right) = g\left(\sqrt{n}\,\sigma_{\left(j-2\right)/n}\,(W_{j/n}-W_{(j-1)/n})\right)+o_P(n^{-\alpha})
$$ where $g$ is a regular enough function and $\alpha>0$ . I'v used ""regular enough"" because in some papers the above approximation is claimed to hold for any function with at most polynomial growth. The idea is that on the left-hand side we have a $\mathcal{F}_{(j+1)/n}$ -measurable quantity which is approximated by shifting the time backward of one unit, giving a $\mathcal{F}_{j/n}$ -measurable, plus an error. Any idea is welcome.","['approximation', 'brownian-motion', 'probability-theory']"
3982195,What are left and right singular vectors in SVD?,"Let $USV^T$ be a singular value decomposition of matrix $A$ . In the textbook ""Linear Algebra and Its Applications"" by D. C. Lay et. al., where SVD is introduced, it says that ""the columns of $U$ in such a decomposition are called left singular vectors of $A$ , and the columns of $V$ are called right singular vectors of $A$ ."" But it does not make any connections with the eigenvectors of $A^T\!A$ . It also says that ""the matrices $U$ and $V$ are not uniquely determined by $A$ . But in this web page it says that ""the eigenvectors of $A^T\!A$ make up the columns of $V$ , the eigenvectors of $AA^T$ make up the columns of $U$ ."" I'm confused about the relationship between the left and right singular vectors (that is columns of $U$ and $V$ ) and the eigenvectors of $A^T\!A$ and $AA^T$ . Any clarification is appreciated.","['linear-algebra', 'svd', 'eigenvalues-eigenvectors']"
3982248,Convincing reason to define subschemes using immersions and not monomorphisms,"In category theory there is a general and very simple notion of subobject: a subobject of $X$ is a class of monomorphisms $A\to X$ , where we identify $A\to X$ and $B\to X$ when they are related by an isomorphism $A\to B$ . This is quite easy to use and understand, and works very well in a lot of categories. But this is not what we do when we define subschemes: instead of using all monomorphisms, we use a somewhat ad hoc subclass of monomorphisms, namely immersions. Immersions are defined using two base cases, open immersions and closed immersions, which have a quite different flavor and behavior (not at all like in topology where open/closed subspaces can be easily defined in terms of each other). There are historical reasons for this choice, and, of course, it works, but it has always bugged me on some level. Could someone give a clear and convincing mathematical argument for defining subschemes with immersions and not general monomorphisms? What I am looking for is something that would become true with this more general definition and makes you think ""Oh yeah, we clearly don't want that to be true for what we call a subscheme."" (Or of course equivalently something that becomes false and makes you think ""We clearly wanted that to be true for subschemes"".) For instance, things like ""a subscheme would not always be locally closed on a topological level"" would not do it for me, since... so what? Subspaces in topology are not locally closed in general and no one is complaining.","['algebraic-geometry', 'schemes', 'category-theory']"
3982252,Best strategy to optimize probability of winning in card drawing game with opponent,"I am curious about the solution to a probability question that was asked in a trading interview: You and your opponent choose a suit (of a standard 52 cards deck). Then one card after another is drawn (without replacement) until either your suit or your opponents suit have appeared 5 times (not necessarily in a row). You win if your suit is the one which was drawn 5 times first. Now, your opponent lets you decide if you want to choose your suit before the game starts, after 1, or after 2 cards are revealed and he will choose his suit afterwards. Which strategy gives you the best chances to win? So as an example, I decide to choose after the first card, it comes hearts, I choose hearts, he chooses any of the other suits and from that point I will only need 4 more hearts whereas he still needs 5 of his suit. This makes it obvious that choosing before the game starts is not optimal, as you get an advantage by choosing the suit of the first card. But I am not sure how to determine whether choosing after the first or after the second card is better. In the scenario where I choose after the second there are two possibilities: Either the first 2 cards have the same suit, then I will only need 3 more or they have a different suit. In that case I choose one of the two suits and my opponent chooses the other and our chances will be equal again. To summarize, my chances of winning if I choose after the second card are $$ P(\textrm{first 2 cards have same suit}) P(\textrm{3 out of 11 before 5 out of 13}) \\
+ \frac{1}{2}P(\textrm{first 2 cards have different suit}) \\
= \frac{12}{51}P(\textrm{3 out of 11 before 5 out of 13}) + \frac{1}{2}\frac{39}{51}
$$ And if I choose after the first card my chances are simply $$
P(\textrm{4 out of 12 before 5 out of 13})
$$ But in both cases I have no idea how to come up with solutions for the missing probabilities, especially because the game is played without replacement and I can not use a binomial distribution approach as it were possible if the game was for example played with coin tosses and you can choose head or tails.","['gambling', 'game-theory', 'card-games', 'probability']"
3982275,Compute this integral (multivar. calc.),"So i have the following question: Compute $$\int_{C}(x^2+y)dx + (z+x)dy + (x+2y)dz$$ where $C$ is the intersection of the cylinder $x^2+y^2=4 $ and the plane $x+y=z$ So my thoughts are to parametrise it to make get $$r(t) = 2cos(t) i + 2sin(t)j + (2cos(t) + 2sin(t) ) k$$ and then use this along with $r'(t)$ to calculate $$\int_{0}^{2\pi} F(r(t)) \cdot r'(t) dt $$ Which when I computed it all, came out to be $$-8\pi$$ I was hoping someone could either verify this method, along with the answer or help me to find out the solution (through tips) Thank you","['multivariable-calculus', 'surface-integrals', 'stokes-theorem']"
3982285,Question regarding solving multivariable function continuity problems,"$$f(x,y)=\begin{Bmatrix}
\frac{x+y}{x^2+y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix}$$ Let's say I have this problem. I use $y = mx$ , solving this I get $\frac{1+m}{x(1+m^2)}$ . Calculating the limit I get $\frac{1+m}{0}$ . Could this prove that the limit doesn't exist? Some follow-up questions: When can I use th $y=mx$ method, if I can? What is the rule of paths? I guess I can't just plug in $(1,2)$ or $(2,3)$ , but can I plug in let's say $(\frac{1}{n}, \frac{1}{n})$ ? If yes, what should I look for when plugging in values? The 'n' has to go, but how? If I need to prove the continuity of a function, what is the simplest way to prove it? I heard about the Squeeze Theorem, but does that work outside of sin, cos functions? At seminars we used $a^2 + b^2 >= 2ab$ , but I don't find that at all clear. EDIT: $$f(x,y)=\begin{Bmatrix}
\frac{x^2y3}{x^2+y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix}$$ I have this function. Using $a^2 + b^2 >= 2ab$ : $$x^2 + y^2 >= |x^2| + |y^2| >= 2|x||y|$$ $|f(x,y)| = \frac{x^2 |y^3|}{x^2 + y^2} = \frac{x^2 |y^3|}{2|x||y|} = \frac{x y^2}{2}$ ---> this tending to 0 would prove continuity? if yes, does this method work all the time? EDIT 2: $$f(x,y)=\begin{Bmatrix}
\frac{3x^2y}{2x^2+5y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix}$$ $$|f(x,y)| = \frac{3x^2|y|}{2x^2+5y^2} <= \frac{3x^2|y|}{2\sqrt{10}|x||y|} = \frac{3}{2\sqrt{10}}*x$$ $$(a^2 + b^2 >= 2ab)  ==> 2x^2 + 5y^2 >= (|x|\sqrt{2})^2 + (|y|\sqrt{5})^2 >= 2\sqrt{10}*x$$ $f(x,y) <= \frac{3}{2\sqrt{10}}*x$ (1) $x --> 0, y --> 0$ (2) (1)(2) --> f(x,y) --> 0 = f(0, 0).","['multivariable-calculus', 'calculus', 'continuity']"
3982407,Calculate the limit of an integration domain,"Consider the set $D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<x^2+y^2\leq\varepsilon^2\}$ , which is the filled disk of radius $\varepsilon$ with a hole at the origin. Calculate the limit $$\lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}\dfrac{1+\sin x+\sin y}{\sqrt{x^2+y^2}}d(x,y).$$ My first instinct was to do a change of variables into polar coordinates which would give $D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<r^2\leq\varepsilon^2; 0\leq\theta\leq2\pi\}$ and $$\lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}[1+\sin(r\cos\theta)+\sin(r\sin\theta)]d(r,\theta),$$ But this does need seem any simpler than before. Any hints are welcome.","['integration', 'limits', 'multivariable-calculus', 'analysis']"
3982572,A differentiable function with $df = 0$ is locally constant,"Let $U$ be an open set and $f : U \subset\mathbb{R}^d \to \mathbb{R}^m$ be a differentiable function with $df=0, \forall x \in U.$ Show that $f$ is locally constant. So my idea was, given some arbitrary $x_0 \in U$ , first taking some ball $B_{\varepsilon}(x_0) \subset U$ which exists since $U$ is open, we can assume $f$ isn't constant there i.e there is $y_o$ s.t $f(x_0) \neq f(y_0)$ . Then defining $\gamma(t) = (1-t)x_0 +t y_0$ , we have $f(\gamma(0)) \neq f(\gamma(1))$ meaning they are different in at least one coordiante, so without loss of generality $f^1(\gamma(0)) \neq f^1(\gamma(1))$ . and then using lagrange for an $\mathbb{R} \to \mathbb{R}$ function we have $(f^1(\gamma(c)))' \neq 0$ . But on the other hand using chain rule $(f^1(\gamma(c)))' = d(f^1(\gamma(c))) =
\underbrace{df^1(\gamma(c))}_\text{0 since $df=0$} 
  \circ d \gamma(c) = 0$ I don't really like my solution since it relies on coordinates, if someone would like to share a different solution it would be great.",['multivariable-calculus']
3982608,How is this property equivalent to the Reiter Property?,"We have the Reiter Property $(R_2)$ for an action of a group G on a set X:
For any $\epsilon>0$ , any finite subset $S$ of G, there exists $\phi\in{\ell^2(X)}$ such that $\|s\phi-\phi\|_{\ell^2}<\epsilon{\|\phi\|_{\ell^2}}$ for all $s\in{S}$ .
I am trying to show this is equivalent to the alternative property $(R_2)'$ : for any $\epsilon>0$ , any finite subset $S$ of G, there exists $\phi\in{\ell^2(X)}$ such that $$\left\|\frac{1}{|S|}\sum_{s\in{S}}{s\phi}\right\|_{\ell^2}>(1-\epsilon)\|\phi\|_{\ell^2}$$ but I am completely stuck.
I have tried using some uniform convexity since $\ell^2$ has an inner product, but can only get anything out of it when $|S|=2$ , I have heard from someone else that this is related to adjoint operators, so I have tried defining $T:\ell^2(X)\rightarrow\ell^2(X)$ by $T(\phi)=\frac{1}{|S|}\sum_{s\in{S}}{s\phi}$ and can deduce that it has norm 1 and, if we extend S to also contain the inverses of all its elements, is self adjoint, but I can't see how this could be helpful to solve the problem. Many thanks.","['fixed-point-theorems', 'group-theory', 'functional-analysis', 'amenability', 'group-actions']"
3982619,Binomial distribution with weights but constant probability,"What is the name of the distribution and/or distribution function of random variable $X$ ? $X=a_1 x_1+a_2 x_2+...+a_n x_n$ $x_1, ..., x_n$ are independent binomial variables (Bernoulli). $x_i=1$ with probability $p$ and $0$ else. $a_1, \dots, a_n$ are given deterministic integers (weights) $\geq 1$ . How do I show that $P(X>\frac{1}{2}\sum a_i)+\frac{1}{2}P(X=\frac{1}{2}\sum a_i) \geq p$ for $p \geq \frac{1}{2}$ ? This is not a Poisson binomial distribution. It is a generalized Poisson binomial but with the nice property that $p$ is constant. This question gives an almost identical problem. In the spirit of the answer to this question I can calculate probabilities as $P(s_k)=(1-p)^n \sum_{i_1,...,i_m |\sum a_{i_k}=s_k} \Bigl( \frac{p}{1-p}\Bigr)^{\sum i_k}$ . However, this does not seem to get me closer to the proof. It might also be possible to cleverly get the desired proof without a complete distribution function.","['probability-distributions', 'binomial-distribution', 'probability']"
3982620,Is changing limits in this fashion okay? What is the justification for it?,"I am currently working through chapter $5$ on limits of Spivaks calculus book. I have come to exercise $15$ ix. I feel like I can answer this question correctly, but there is one small step I know to be true but can't seem to show it concisely. I would like to note, I am aware similar questions have been asked, but I am querying a specific part of this question. The question: Assume that: $$\lim_{x\to{0}}{}\frac{\sin(x)}{x}=\alpha.$$ Find, in terms of $\alpha$ , the following limit: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x-1}.$$ My attempt: Firstly, multiplying bottom and top by $x+1$ yields: $$\lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{(x-1)(x+1)}=\lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{x^2-1}$$ Now, IF $(*):=\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}$ exists, we can split the above into: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}\lim_{x\to1}{x+1},$$ using algebra of limits. Which gives us: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}2.$$ So I'm essentially left to show $(*)$ exists and is equal to $\alpha$ . Which means the original limit is equivalent to $2\alpha$ . Now clearly as $x\to{1}$ we have that $(x^2-1)\to{0},$ so can we write this limit as: $$\lim_{x\to0}\frac{\sin(x)}{x}\text{ or }\lim_{x^2-1\to0}\frac{\sin(x^2-1)}{x^2-1}?$$ If so, why exactly? It just feels a bit imprecise. I guess what I'm asking exactly is: $$\lim_{x\to0}\frac{\sin(x)}{x}\overset{?}{=}\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}$$","['limits', 'calculus', 'real-analysis']"
3982681,An attempt to revisit the criticality of the Navier-Stokes Equation,"I am looking for feedback on this attempt to revisit the criticality of the Navier-Stokes equation (NSE) for incompressible fluids in 3 dimensions. It has been said, that the NSE is supercritical (see  T. Tao, Why proving global regularity conjecture for Navier-Stokes is hard (terrytao.wordpress.com, March 18, 2007)), because the rescaling that preserves the equation is $$\vec{u} \rightarrow \frac{1}{k}\vec{u}$$ while the energy is an integral of $|\vec{u}|^2$ and thus the rescaling that preserves the energy is given by $$\vec{u} \rightarrow \frac{1}{k^{3/2}} \vec{u}$$ thereby showing that for the NSE in 3 dimensions energy is being rescaled more severely; this amplifies the rescaling at small scales $k\ll1$ . Hence, the NSE appears to be energy-supercritical. The attempt here is to show that in this reasoning, the assumption is only true in part; becaise the rescaling that preserves the NSE is not $\vec{u} \rightarrow \frac{1}{k}\vec{u}$ , but it is actually $$\vec{u}^\prime = k^{\alpha_x-\alpha_t}\vec{u},$$ where $$
\begin{split}
(x,y,z)^\prime &= k^{\alpha_x}(x,y,z)\\
t^\prime & = k^{\alpha_t}t
\end{split}
$$ and these are identical with the above rescaling laws only when $\alpha_x=1$ , $\alpha_t=2$ but not in general. This fact was discovered only recently, see A. Ercan and M. L. Kavvas, Chaos 25, 123126 (2015). But this changes everything, because to keep the NSE invariant under scaling transformation, the velocity and energy transform as $$
\begin{split}
\vec{u}^\prime & = k^{\alpha_x-\alpha_t}\vec{u} \\
E^\prime & = k^{5\alpha_x-2\alpha_t} E
\end{split}
$$ and the energy $E$ at a given moment $t$ , would be scale-invariant if $\vec{u}$ transformed according to the law $$\vec{u}^\prime = k^{-\frac{3}{2}\alpha_x}\vec{u}.$$ Then, appllying T. Tao's definitions of critical, subcritical and supercritical (see T. Tao, Current developments in mathematics 2006 , 255(2008))  we find that the NSE is energy-subcritical, when $\frac{\alpha_t}{\alpha_x}>\frac{5}{2}$ , energy-critical, when $\frac{\alpha_t}{\alpha_x}=\frac{5}{2}$ , energy-supercritical, when $\frac{\alpha_t}{\alpha_x}<\frac{5}{2}$ . Do you find this reasoning correct/credible? NSE criticality is quite important and relates to the smoothness of solutions Your kind thoughts and suggestions are welcome. It is better to have them put together as an answer, pointing mistakes or showing an alternative explanations, rather than having extended discussion/comments Thank you","['fluid-dynamics', 'multivariable-calculus', 'symmetry', 'partial-differential-equations']"
3982704,Is the space of smooth functions on a noncompact manifold nuclear?,"I have two questions, really: Q1) Suppose $M$ is a noncompact, finite-dimensional smooth manifold. Is it true that $C^{\infty}(M)$ , by which I will always mean $C^{\infty}(-; \mathbb{R})$ , is a nuclear Frechet space? Reason for asking: if you quickly google for ``examples of nuclear spaces'', it's usually cited that $C^{\infty}(M)$ is nuclear if $M$ is compact. But compactness seems like rather serious overkill; indeed, Treves, Theorem 51.5 + subsequent Corollaries, suggests $C^{\infty}(U)$ is nuclear for $U \subset \mathbb{R}^n$ open. In this case, I would suspect that if, say, $M$ may be covered by finitely many charts (as is certainly the case if $M$ is compact but is true far more generally), then $C^{\infty}(M)$ is nuclear by embedding $C^{\infty}(M) \hookrightarrow \prod C^{\infty}(U_{\alpha})$ as a closed subspace for $\{U_{\alpha}\}$ a finite open cover of $M$ by charts. This same embedding trick seems to suggest that $C^{\infty}(M)$ is nuclear far more generally still -- in which case, what are the natural conditions on $M$ such that this holds? (e.g., paracompactness, second countability, ...?) Q2) And then secretly my real reason for asking is to know conditions on manifolds $M, N$ such that $C^{\infty}(M \times N) \simeq C^{\infty}(M) \otimes C^{\infty}(N)$ for some reasonable tensor product. Indeed, if $\otimes$ is taken to be the projective tensor product $\hat{\otimes}_{\pi}$ , this statement seems to be commonly asserted - and I think I understand this in the case that at least one space is nuclear so that, e.g., projective and injective tensor products agree. But, right, does this hold for reasonably general noncompact manifolds $M, N$ ? How about if $M, N$ are in some reasonable class of infinite-dimensional manifolds?",['functional-analysis']
3982706,Restrictions on the number of edges in a graph,"I have been wondering, can you prove that if we suppose in a graph (connected or not) there are no cycles of length 3 (so at least 4) and each vertex has a degree 4 or less, then the number of edges will be less than $\frac{3}{2}*n - 2$ where n is the number of vertices. I tried using Euler's Formula, namely $v - e + f = k + 1$ . If we then take into consideration the restriction on cycle length we get $2f < m$ so we can substitute and get $m < 2n - 4$ , however that is not enough and also is not using the fact that each vertex has a degree 4 or less. I'm sorry, I forgot to tell the graph has to be planar.","['graph-theory', 'combinatorics', 'discrete-mathematics', 'planar-graphs', 'discrete-optimization']"
3982721,Coloring a $10×10$ grid,"Given a $10×10$ grid with $9$ red blocks and $91$ white blocks, in each step we color one red block black and after that one white block red until there are $91$ black blocks and $9$ red blocks. Prove that there exits a step in which a black block shares an edge with a white block. I ran a few trials and couldn't get past the fifth step without adjacency. But I don't know how to build a proof.","['invariance', 'coloring', 'discrete-mathematics']"
3982738,Hard olympiad geometry problem to prove ratio,"Let $D$ be a point inside an acute triangle $ABC$ such that $\angle ADC = \angle A +\angle B$ , $\angle BDA = \angle B + \angle C$ and $\angle CDB = \angle C + \angle A$ . Prove that $\frac{AB \cdot CD}{AD} = \frac{AC \cdot  CB} {AB}$ .
Can somebody help me to prove it synthetically My approach- let tanget from $B C ,  A B, A C$ meet at $X, Y, Z$ resp that gives $\angle BDC = \angle ACX$ and so on. Now extend $AD$ to meet $CX$ at $J$ we get $BDJC$ is cyclic (same can be done at other sides too); at this point I tried using power of point but it didn't help... Plz provide some help","['contest-math', 'triangles', 'geometry']"
3982752,Can we combine this two Lyapunov functions (which imply local stability by separate) to conclude global stability?,"Let $x(t)\in\mathbb{R}^n$ constrained to a dynamical system $$
\dot{x}(t) = f(x(t))
$$ for some vector field $f:\mathbb{R}^n\to\mathbb{R}^n$ . Moreover, the dynamical system has a unique equilibrium point at the origin. My goal is to conclude that for this system (with a particular $f$ I have) the origin is globally asymptotically stable. So far, I have found two Lyapunov function candidates $V_1(x),V_2(x)$ which both are positive definite and radially unbounded. Moreover, I managed to show that they comply $\dot{V}_1(x)<0$ $\ \forall x : \|x\|<r_1$ $\dot{V}_2(x)<-c$ $\ \forall x : \|x\|>r_2$ and some constant $c>0$ $0<r_1<r_2$ If it was the case that $r_1>r_2>0$ instead of item 3), we would be done: global stability follows since item 2) ensures that $x$ reaches $\|x\|\leq r_2$ in finite time from any initial condition, and then one can use 1) to conclude $x$ reaches the origin asymptotically from here. (as pointed out in the comments, this argument requires nuance, nevertheless, the important part is the following) . However, from item 3) this is not the case. Thus, I have this disk $r_1<\|x\|<r_2$ in which my Lyapunov stability test is inconclusive. My hope is to show that trajectories $x(t)$ cannot stay in the disk $r_1<\|x\|<r_2$ forever but ultimately reach $\|x\|<r_1$ . My question is: from the information given here, do you think is possible to conclude global asymptotic stability either by constructing a new Lyapunov function from $V_1,V_2$ or by checking some condition on $f$ to conclude trajectories cannot stay in $r_1<\|x\|<r_2$ ? Indeed, there are many counterexamples in which this is not possible. But my question here is if there is a way to check if it is possible: what properties do you suggest me to check for $f$ . I'm looking for suggestions, ideas or relevant bibliography that you think can help me. EDIT: In case this problem is easier, or adds useful information, I can also show the following instead of the original items: $\dot{V}_1(x)<0$ $\ \forall x \in M_1 = \{x : V_1(x)<r_1'\}$ $\dot{V}_2(x)<-c$ $\ \forall x \in M_2=\{ V_2(x)>r_2'\}$ and some constant $c>0$ $D:=\mathbb{R}^n\setminus(M_1\cup M_2)\neq \emptyset$ We are left to show that there are no invariants in $D$ . EDIT 2: After some discussion in the comments, I think the main essence of my question, is more on what happens when the Lyapunov arguments are inconclusive on this non empty set $D$ (bounded, but not containing the origin). A concrete diagram for the state space of what $r_1,r_2,M_1,M_2,D$ looks in the case I am interested is the following:","['ordinary-differential-equations', 'lyapunov-functions', 'control-theory', 'stability-in-odes', 'stability-theory']"
3982761,(Proof Verification) Indecomposable (Limit) Ordinals,"This is problem 5 in Kunen's Chapter 1 exercises. Suppose $\alpha$ is a limit ordinal. I want to show the following equivalences: (1) $\forall \beta, \gamma < \alpha$ , $\beta + \gamma < \alpha$ (2) $\forall \beta < \alpha$ , $\beta + \alpha = \alpha$ (3) $\forall A \subseteq \alpha$ , $ot(A) = \alpha$ or $ot(\alpha \setminus A) = \alpha$ ( $ot$ means order type). (4) $\exists \delta \alpha = \omega^{\delta}$ (1) $\implies$ (2) is fairly easy: just pass to the limit on $\gamma$ .
(2) $\implies$ (1) is also pretty easy: if $\beta + \gamma = \alpha$ then $\beta + (\gamma + 1) > \alpha$ and since $\gamma + 1 < \alpha$ as $\alpha$ is a limit, this contradicts (2). (3) $\implies$ (2) is fairly easy. Just take $A = \beta$ . I can finish the equivalences by showing (4) $\iff$ (1) + (2) and (4) $\implies$ (3) (4) $\implies$ (3): An induction argument. Suppose $\alpha = \omega^{\delta + 1}$ . Then we may express the situation as $X_1 \cup X_2 = \alpha$ . Then by induction we have that for every $n \in \omega$ , $ot(X_i \cap [\omega^{\delta} \cdot n, \omega^{\delta} \cdot (n+1))) = \omega^{\delta}$ for at least one of $i = \{1, 2\}$ . Then one of the $i$ has infinitely many $n \in \omega$ such that $ot(X_i \cap [\omega^{\delta} \cdot n, \omega^{\delta} \cdot (n+1))) = \omega^{\delta}$ and then for this $i$ we have that $ot(X_i) \geq \omega^{\delta}\cdot n$ for every $n \in \omega$ , and thus $ot(X_i) = \omega^{\delta + 1}$ . If $\delta$ is a limit, then apply the inductive hypothesis on all $\zeta < \delta$ . Then let $A_i = \{\zeta < \delta : ot(X_i \cap \omega^{\zeta}) = \omega^{\zeta} \}$ for $i = \{1, 2\}$ . As $A_1 \cup A_2= \delta$ , we have for some $i$ that $sup(A_i) = \delta$ . Then again we see that $ot(X_i) \geq \omega^{\zeta}$ for all $\zeta < \delta$ and thus $ot(X_i) = \omega^{\delta}$ . (4) $\implies (1) + (2)$ : I suppose I don't have to do this since I have that $(4) \implies (3) \implies (2)$ so I'll omit it to save space. $(1) + (2) \implies (4)$ : Let $A = \{\delta \in OR : \omega^{\delta} \leq \alpha \}$ . Then take $\eta = sup(A)$ . We have that $\omega^{\eta} \leq \alpha$ as well. Suppose for contradiction that $\omega^{\eta} < \alpha$ . Then $\omega^{\eta} + \alpha = \alpha$ by assumption. Then it is easy to see by induction that $\omega^{\eta}\cdot n + \alpha = \alpha$ for every $n \in \omega$ . Here's a step I'm not sure of: this implies that $\omega^{\eta}\cdot \omega + \alpha = \alpha$ . I think such limits can be passed only on the right side of the addition by definition. So it would have been fine if it was $\alpha + \omega^{\eta} \cdot n$ but I'm not sure about $\omega^{\eta} \cdot n + \alpha$ . In other words, is $sup_{n \in \omega}(\omega^{\eta} \cdot n + \alpha) = \omega^{\eta} \cdot \omega + \alpha$ ? Assuming this is true however, this gives us $\omega^{\eta}\cdot \omega + \alpha = \omega^{\eta + 1} + \alpha = \alpha$ which means that $\eta \in A$ contradicting $\eta = sup(A)$ .","['elementary-set-theory', 'ordinals', 'well-orders', 'set-theory']"
3982840,An intuitive explanation for the ecological fallacy,"I believe that it is called the ecological fallacy. People say that one cannot apply population-wide statistics to individuals of that population. So, just because some trait exists in a higher proportion in population A than it does in B, that doesn't mean that an individual from pop. A is more likely to have that trait than an individual from pop. B. Now, this doesn't make sense to me. An individual coming from a population where a certain trait is more common, is more likely to have that trait than an individual coming from a population where said trait is less common, right? I guess this depends on how one mathematically defines ""common"". The mean and the average are a bit scary statistical quantifications. I see how ""the average"" is problematic, but a weighted average is surely quite illuminatory of the real likelihoods?","['intuition', 'statistics', 'probability']"
3983005,Challenging Integrals for High School Students,"I am now in my last year of high school. We have covered all the techniques useful for indefinite integration that are included in our Maths and Further Maths courses. This includes: Integration by parts, inspection, substitution, partial fraction decomposition Integration of regular and inverse trigonometric, regular and inverse hyperbolic, exponential, logarithmic, polynomial functions I would like to have some challenging integrals to attack that are possible for me to solve at my current level of knowledge. By challenging, I mean integrals similar to the ones in this document. They were generally enjoyable and very satisfying to solve. If you have an integral that you think I could do that is more challenging than those in the aforementioned link, so much the better. Thank you for your suggestions.","['integration', 'indefinite-integrals', 'big-list', 'soft-question']"
3983035,Finding non-primes for $n^2+n+17$,"If I'm given $n^2+n+17$ and I'm asked to find some $n\in \mathbb N$ such that the polynomial is not prime, is there any reasoning I can use to quickly find a counter-example? It took a good minute for me to guess one counter-example, $n=17$ . But is there a way I could have found a counter example without some lucky trial-and-error?","['elementary-number-theory', 'algebra-precalculus', 'discrete-mathematics']"
3983052,"Are all commutative, associative binary operations isomorphic to addition?","Addition and multiplication are the two classic commutative, associative binary operations on the reals. They satisfy a striking property: they are equivalent up to unary operations. By taking a logarithm of the arguments to addition and exponentiating the result, we get multiplication: $xy = \exp(\log x + \log y)$ In fact, we can create a new commutative, associative operation from every invertible unary function $f$ in the same way: $(x,y) \mapsto f^{-1} (f(x) + f(y))$ I actually haven't been able to come up with a commutative, associative operation that doesn't follow this form. Are all commutative, associative operations on the reals isomorphic to addition in a similar way? EDIT - The answers and comments below give several good counterexamples to the problem as stated. Requiring $f$ to be invertible is too strong a condition. What if we allow this slightly more general functional form $(x,y) \mapsto g (f(x) + f(y))$ where $f$ maps reals to some field and $g$ maps that field to the reals? This seems to capture the counterexamples. For example: $(x, y) \mapsto 0$ matches with arbitrary $f$ and $g(z) = 0$ $(x, y) \mapsto x + y - \lfloor{x+y}\rfloor$ matches with $f(x) = x$ and $g(z) = z - \lfloor{z}\rfloor$ I think $(x, y) \mapsto max(x,y)$ matches with $f(x) = (x + a)^n$ and $g(z) = z^{1/n} - a$ , where $a, n$ are constants we take to infinity. That choice is inspired by the relationship between the max function and the infinity norm. I get that including limits in $f$ and $g$ is pretty suspect, but even with that hack, it's interesting to me that it's possible. Modifying the question slightly, can every commutative, associative binary operation be cast in the form $(x,y) \mapsto g (f(x) + f(y))$ ?","['binary-operations', 'abstract-algebra', 'semigroups']"
3983068,isomorphic $\operatorname{Proj}(S)$ with nonisomorphic $S$,"Let $S$ be a graded ring. To understand $\operatorname{Proj}(S)$ better, I want to know under what changes of $S$ will $\operatorname{Proj}(S)$ remains unchanged. All answers from a proposition or general point of view are welcome. I will start with some examples: Fix an integer $k$ . scaling:  define a new graded ring with $S'_d = S_{dk}$ . Then $\operatorname{Proj}(S)\cong \operatorname{Proj}(S')$ replace a finite number of terms by $0$ and remain the grading. $S'_{n} = 0$ for $0\leq  n \leq k$ and $S'_{n} = S_{n}$ for $n \geq k+1.$","['algebraic-geometry', 'graded-rings', 'projective-schemes']"
3983127,What exactly is a function or mapping? Is it an object?,"What exactly is a function or mapping? Is it an object?  I'm not talking about the domain or the codmain which I know are Sets and therefore objects. The mapping itself, what is it? From what I understand everything is a set, but a function is made of two sets plus something else. What is that something else?  As far as I understand in ZFC everything is a Set. From Abstract Algebra  by Dummit and Foote ""A group is an ordered pair (G, *) where G is a set and * is a binary operation on G satisfying the following axioms:  ..."" ""A binary operation * on a set G is a function $* : G \to G$ ."" What is confusing is that I just finished learning ZFC Set Theory and it shows that an ordered-pair is the Set: (x,y) = { {x}, {x, y}} When the y is not set a but a function now I'm confused of how ""(G, *)"" can be an ordered pair when the * is not simply a Set.","['elementary-set-theory', 'group-theory', 'functions', 'definition']"
3983151,Can infinite unions in some sense be handled like limits?,"I'm stuck on a question regarding sets that are covered by an infinite union of sets. If a set $S$ is a subset of an infinite union of sets, $$S\subset\bigcup_{k=1}^{\infty}{U_k}$$ is it then correct to say that if $x_0\in S$ then $\exists N\in \mathbb{N}$ such that: $$x_0\in\bigcup_{k=1}^{N}{U_k}$$ similar to what we would say if a limit exists?",['elementary-set-theory']
3983165,What is the connection between sum of tuple products and permutations by cycle?,"From my previous question Disjoint sets in a combinatoral sum (continued) , let $X = \{x_1, \dots, x_n\}$ be a set and $$f_X(m) = \sum_{X' \in \binom{X}{m}} \prod_{x \in X'} x$$ Let $S_k = \sum x_i^k$ .  We can calculate with inclusion-exclusion: $$f_X(2) = \frac{1}{2!} (S_1^2 - S_2)$$ $$f_X(3) = \frac{1}{3!} (S_1^3 - 3 S_2 S_1 + 2 S_3) $$ and so on. In the previous answer I discovered a connection between permutations by cycle type and the coefficients of the expression $f_X(m)$ (table at https://oeis.org/A181897 ), and came up with a tenuous explanation. Can someone confirm my answer and elucidate if my reasoning is correct? Also what is the connection to integer partitions? I am not that experienced with combinatorics or group theory so an answer that is as elementary as possible (possibly at the expense of generality) is appreciated.
I'm looking for references to this problem which has surely been explored in the past.","['integer-partitions', 'combinatorics', 'permutation-cycles', 'reference-request']"
3983181,Functions with mutually orthogonal gradients at every point,"I am not completely sure if the question is too broad, but here goes. If we are given a function $f:\mathbb{R}^2 \to \mathbb{R}$ , is there any way to find other non-constant functions $g:\mathbb{R}^2 \to \mathbb{R}$ such that at every point their gradients are orthogonal? In other words, given $f$ can we find a non-constant $g$ such that $\nabla f\cdot \nabla{g}=0$ for every $(x,y) \in \mathbb{R}^2$ ? For given $f$ I am interested in a criterion if such functions $g$ exist, and if so how would be a way to find them. As an example, if $f(x,y)=x$ , then $g(x,y)=y$ satisfies that property. If $f(x,y)=x^2+y^2$ ? It would have to be a solution of the PDE $xf_x+yf_y=0$ , which unfortunately I don't remember my PDEs now to try to solve. The broader question is essentially if we can say anything about the following PDE for a given $f$ . $$f_xg_x+f_yg_y=0$$ Assume any nice conditions you may want about $f$ , such as $f\in C^{\infty}$ etc. If the topic is too broad for an answer, I would at least appreciate any pointers for a direction to read further about it. This also relates to products of harmonic functions, specifically a product of two harmonic functions is also harmonic if the above condition on the gradients is satisfied.","['analysis', 'partial-differential-equations']"
3983203,"""Old-Fashioned"" O-level Maths Book","I'm looking for recommendations for an ""old-fashioned"" style of maths textbook for O-level/GCSE, i.e., one which is concise (not full of pictures) with plenty of exercises. Ideally something which is available in PDF format online. I don't necessarily mean an old book, more a book written in a style which is typical of the early 20th century or earlier. (For instance, Gwynne's Latin is a modern book for learning Latin which I would consider to fall under this category.) I am quite fond of this one which I own a physical copy of, but am struggling to find anywhere online in PDF format. Also I would appreciate if it is just one book rather than many volumes. A lot of maths textbooks made for school come in many volumes with titles like ""School Maths 1A, 1B, 2A, 2B"", etc. To clarify, by ""O-level/GCSE"", I mean a book with a table of contents which should at least tackle all of the following:","['book-recommendation', 'reference-request', 'education', 'algebra-precalculus', 'soft-question']"
3983207,A vector space $V$ such that $V\otimes V \not \cong B(V^*)$,"Let $V$ be a vector space (not necessarily finite-dimensional) over a field $\mathbb F$ . Let $B(V^*)$ denote the vector space of all bilinear forms $V^*\times V^* \rightarrow \mathbb F$ . It is ""well-known"" that the linear map $\phi: V\otimes V \rightarrow B(V^*)$ such that $$\phi(v \otimes w)(f,g) = f(v)g(w)$$ for all $v,w\in V$ and $f,g\in V^*$ establishes an isomorphism $V\otimes V \cong B(V^*)$ when $V$ is finite-dimensional. I want to prove that this hyphotesis cannot be dropped , that is, I'm looking forward a vector space $V$ of infinite dimension such that $V\otimes V \not \cong B(V^*).$ I am not familiar at all with infinite-dimensional vector spaces, besides the classical ones $\mathbb F[x]$ and $\mathscr C([0,1],\mathbb R)$ , for instance. Which vector space should I look for? Is this problem more related to the cardinality of basis, in the sense that $\dim B(V^*)$ is also uncountable if $\dim V^*$ is uncountable? Any help is very much appreciated.","['linear-algebra', 'examples-counterexamples']"
3983270,"Spivak Chapter 10, Exercise 19 solution verification. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists.","This question is asked here and here . I'm going to try to answer it for my own edification, and for the next poor soul that comes across it and has trouble :) I'm using Spivak's Answer Book solution, but adding notes to hopefully clarify things. Here goes: From Calculus by Michael Spivak 3rd Edition, Chapter 10, Exercise 19. 10-19. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. A little experimentation should convince you that it is unwise to seek a formula for $(f\circ g)^{(n)}(a)$ . In order to prove that $(f\circ g)^{(n)}(a)$ exists you will therefore have to devise a reasonable assertion about $(f\circ g)^{(n)}(a)$ which can be proved by induction. Try something like: "" $(f\circ g)^{(n)}(a)$ exists and is a sum of terms each of which is a product of terms of the form..."" First, let's look at the first few derivatives of $(f\circ g)$ . (These are calculated via straightforward application of the Chain Rule): $$(f\circ g)^{\prime}(x) = f^{\prime}(g(x)) \cdot g^{\prime}(x)$$ $$(f\circ g)^{\prime\prime}(x) = f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)^2 + f^{\prime}(g(x)) \cdot g^{\prime\prime}(x)$$ $$(f\circ g)^{\prime\prime\prime}(x) = f^{\prime\prime\prime}(g(x)) \cdot g^{\prime}(x)^3 + 3f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)g^{\prime\prime}(x) + f^{\prime}(g(x)) \cdot g^{\prime\prime\prime}(x)$$ These suggest a general form for derivatives of $(f\circ g)$ : the $n$ -th derivative of $f\circ g$ seems to be made up of a sum of terms that are each the product of some constant, times some derivative of $f$ at $g(x)$ , times some derivatives of $g$ at $x$ , with these derivatives of $g$ perhaps raised to some power. None of the derivatives (of $f$ or $g$ ) are of order higher than $n$ . Conjecture: If $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ for some number $c$ , nonnegative integers $m_1,\dots,m_n$ and a natural number $k \leq n$ . (Aside: The conjecture concerns the existence of the derivative at a single point , $a$ . It says, ""if $g$ is $n$ -times differentiable at $a$ and $f$ is $n$ -times differentiable at $g(a)$ , then $f\circ g$ is $n$ -times differentiable at $a$ . If there's a second point $b$ such that $g$ is $n$ -times differentiable at $b$ and $f$ is $n$ -times differentiable at $g(b)$ , then $f\circ g$ is $n$ -times differentiable at $b$ , and $(f\circ g)^{(n)}(b)$ is the sum of terms of the form $$c\cdot[g^{\prime}(b)]^{m_1}\cdot[g^{\prime\prime}(b)]^{m_2}\cdots[g^{(n)}(b)]^{m_n}\cdot f^{(k)}(g(b))$$ where $c$ , $m_1, \dots, m_n$ , $k$ in each term are all identical to the corresponding term for $(f\circ g)^{(n)}(a)$ .) Proof: Let's restate the conjecture we're trying to prove. Conjecture:
If $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ Proof of the conjecture is by induction on $n$ Case for $n = 1$ If $f^{\prime}(g(a))$ and $g^{\prime}(a)$ both exist, then the Chain rule states that $(f\circ g)^{\prime}(a)$ exists and is equal to $f^{\prime}(g(a)) \cdot g^{\prime}(a)$ . Thus our conjecture is true for $n = 1$ (with $c = m_1 = k = 1$ ). Case for $n + 1$ We will assume the conjecture is true for $n$ , that is, if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ Note that we are not assuming that $(f\circ g)^{(n)}(a)$ exists. Our assumption is that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. (This distinction is important and sets this proof apart from inductive arguments I've previously encountered. Our assumption isn't simply ""thing is true"". Our assumption is that "" if this condition is met, thing is true."" This is in agreement with the theorem we're trying to prove. The theorem doesn't say the $n$ -th derivative of $f\circ g$ exists. Only that, if certain conditions are met, the derivative exists.) Now we need to prove the conjecture for the $n + 1$ case. We need to show that if $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist, then $(f\circ g)^{(n+1)}(a)$ exists, and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a))$$ with $k \leq n+1$ . Suppose $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist. This means that $f^{(n)}(y)$ exists for all $y$ in some interval around $g(a)$ , and likewise $g^{(n)}(x)$ exists for all $x$ in some interval around $a$ . Why? Because, by definition the $(n+1)$ derivatives are $$f^{(n+1)}(g(a)) = \lim_{y \to g(a)}\frac{f^{(n)}(y) - f^{(n)}(g(a))}{y-g(a)}$$ $$g^{(n+1)}(a) = \lim_{x \to a}\frac{g^{(n)}(x) - g^{(n)}(a)}{x-a}$$ and in order for these limits to exist, the $n$ -order derivatives must exist in some intervals around these points. Furthermore, $f^{(k)}$ and $g^{(k)}$ must also exist in these intervals for all $1\leq k < n$ . Because $f^{(n)}$ exists at and around $g(a)$ , there is some $\varepsilon_f > 0$ such that for all $y$ if $|y - g(a)| < \varepsilon_f$ , $f^{(n)}(y)$ exists. Likewise, since $g^{(n)}$ exists at and around $a$ there is some $\delta_g >0$ such that for all $x$ if $|x-a| < \delta_g$ , $g^{(n)}(x)$ exists. We know that $g$ is continuous at $a$ (it's differentiable). As such there exists some $\delta_f > 0$ such that for all $x$ if $|x-a| < \delta_f$ , then $|g(x) - g(a)| < \varepsilon_f$ . If we use $\delta_{min} = \min(\delta_g,\delta_f)$ then for all $x$ if $|x-a| < \delta_{min}$ , both $g^{(n)}(x)$ and $f^{(n)}(g(x))$ will exist. In this interval both $g^{(n)}(x)$ and $f^{(n)}(g(x))$ exist, so the $n$ -case assumption tells us $(f\circ g)^{(n)}(x)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(x)]^{m_1}\cdot[g^{\prime\prime}(x)]^{m_2}\cdots[g^{(n)}(x)]^{m_n}\cdot f^{(k)}(g(x))$$ Worth noting: we're no longer only talking about the value of $(f\circ g)^{(n)}$ at a single point $a$ . $(f\circ g)^{(n)}(x)$ exists for all $x$ in this interval around $a$ . $(f\circ g)^{(n)}$ is a function that's defined at and around $a$ . Furthermore, $(f\circ g)^{(n)}(x)$ is the sum of products of constants and derivatives of $f$ and $g$ and these derivatives are all themselves differentiable at $a$ Therefore, the derivative of $(f\circ g)^{(n)}$ at $a$ exists and can be calculated using the standard, sum, product and chain rules for derivatives. Doing so will result in terms that look like either this $$c\cdot[g^{\prime}(a)]^{m_1}\cdots m_{\alpha}[g^{(\alpha)}(a)]^{m_{\alpha}-1}\cdot[g^{(\alpha+1)}(a)]^{m_{\alpha+1}+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ this $$c\cdot[g^{\prime}(a)]^{m_1}\cdots m_n[g^{(n)}(a)]^{m_n-1}\cdot[g^{(n+1)}(a)]\cdot f^{(k)}(g(a))$$ or this $$c\cdot[g^{\prime}(a)]^{m_1+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k+1)}(g(a))$$ all of which fit the required form for terms of the $n+1$ case. Therefore, if $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist, then $((f\circ g)^{(n)})^{\prime}(a) = (f\circ g)^{(n+1)}(a)$ exists, and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a))$$ with $k \leq n+1$ . This completes the proof.
I hope.","['calculus', 'solution-verification', 'induction', 'derivatives', 'chain-rule']"
3983302,Extension of Bayes' Theorem coin flipping example - finding a fair coin in a bag of biased coins,"There's a fairly common example of Bayes' Theorem in which a coin is drawn from a population with a known amount of biased coins and fair coins. A coin is drawn, flipped a certain amount of times, and then one is challenged to determine the probability that the flipped coin was biased, resulting in a standard implementation of Bayes' Theorem. I was thinking of an extension of this question, where the goal is to find a fair coin drawn from a population. As a concrete example, suppose we have $N$ coins, of which $n$ are fair coins and the remainder will always flip heads. Our goal then is essentially to flip a tails so that we know we have a fair coin. Beforehand we decide after $m$ heads flipped from an individual coin, we will draw a new coin. How would I approach the problem of calculating how many coins I would need to draw before reaching a certain probability threshold $p$ of having at some point flipped a tails? In this case Bayes' Theorem tells us that the probability the first randomly chosen coin is biased after $m$ heads is given by: $$\frac{\frac{N-n}{N}}{\frac{N-n}{N}+\frac{1}{2}^m*\frac{n}{N}}$$ My initial thought was to approach the problem in the vein of ""how many coin flips do I need to have a probability $p$ of having flipped a heads?""-type problems and using Bayes' Theorem between each flip, but the examples of those questions that I've encountered require mutually independent events so I don't think that applies to this problem. Additionally, would the question of finding an ""optimal"" $m$ be related to stopping time? edit : After coming back to this question with a fresh mind, I had the thought that in the case there is only one fair coin (i.e. $n=1$ ) then we could say that the probability we would flip $i$ coins and not see a tails would be: $$\frac{i}{N}*\frac{1}{2}^m+\frac{N-i}{N}$$ which would compute the probability that the fair coin was in the first $i$ coins drawn and that it was flipped $m$ times without seeing a tails, and add that to the probability that the fair coin hasn't yet been drawn. If this is valid, then I think I see how to extend it to $n$ fair coins (by summing over the different amounts of possibly drawn fair coins) as well as how to extend it to individual flips, but I'm also worried I'm violating some assumption in probability and intuiting something incorrectly. Would this work?","['bayes-theorem', 'bayesian', 'probability']"
3983368,the minimum of the sum of two convex functions,"Context: Here is a question jump into my mind, when I met a problem in 1-dim case, I'm wondering if there is a generalization to higher dimensional $\mathbb R^n$ space: Is the following statement true or false? If it's false, please give counter-example, if it's true, please prove that: Suppose $f, g$ are two convex function $\mathbb R^n \rightarrow \mathbb R$ with unique minimum in $\mathbb R^n$ . The minimum of function $f$ is denoted as $\vec{x^{(1)}}$ ; the minimum of function $g$ is denoted as $\vec{x^{(2)}}$ . Suppose $\vec{x^*}$ is the minimum of $f + g$ , then we have \begin{align}
\min \{\vec{x^{(1)}_j}, \vec{x^{(2)}_j} \} \leq \vec{x^*_j} \leq \max \{\vec{x^{(1)}_j}, \vec{x^{(2)}_j} \} ~~~~~~~~~~\forall 1 \leq j \leq n.
\end{align} For $1$ dim case, the above statement is of course true; but what if in the high dimensions case $n \geq 2$ ?","['convex-optimization', 'optimization', 'functional-analysis', 'analysis']"
3983404,Gauss theorem and potential,"Let $V(r) = qe^{-\mu r}/r$ if R is any region containig the origin, show that: $\int_{dR} \nabla V dS = \mu^2 \int_{R} Vd^3r - 4\pi q$ dr is the boundary of R I applied Gauss theorem here, so that $\int_{dR} \nabla V dS = \int \Delta V d³r = \mu² \int_{R} Vd³r$ Now i can guess that the difference between my answer and the book answer is due the singularity at origin, but how to deal with this? In case like that i can't apply Gauss theorem?","['multivariable-calculus', 'calculus', 'vector-fields']"
3983491,Two triangles of equal area,"Claim. Given a convex quadrilateral $ABCD$ that isn't parallelogram. The Newton line intersects sides $AD$ and $BC$ of the quadrilateral at the points $H$ and $G$ , respectively . Then $Area(\triangle HGD)=Area(\triangle HBG)$ and $Area(\triangle HGC)=Area(\triangle HAG)$ . GeoGebra applet that demonstrates this claim can be found here . Proof . Observe that diagonal $HG$ of the quadrilateral $HBGD$ lie on the Newton line of the quadrilateral $HBGD$ . If we apply Anne's theorem on point $H$ and quadrilateral $HBGD$ we can write $Area(\triangle HGD)+0=Area(\triangle HBG)+0$ , hence $Area(\triangle HGD)=Area(\triangle HBG)$ . Similarlly we can show that $Area(\triangle HGC)=Area(\triangle HAG)$ . Question . Is this proof acceptable? Can we use Anne's theorem in the case when the point lies on the side of the quadrilateral? Can you provide an alternative proof?","['quadrilateral', 'euclidean-geometry', 'solution-verification', 'geometry']"
3983524,Laplace eigenvalues on unit disc intermediate step,"If I try for a separable solution $u(r,\theta)=R(r)\Theta(\theta)$ to the equation $\Delta u=-ku$ where $k\geq0$ I run into some problems before getting to the Bessel functions part. After separation the equation simplifies to $\frac{r^2}{R}R''+\frac{r}{R}R'+kr^2=-\frac{1}{\Theta}\Theta''$ . Now I would set both sides to be a constant $c$ . But in every solution I have seen, they have set this constant to be $n^2$ (so that $c=n^2$ in my case) because $c\geq0$ . But I don't understand how we can say $c\geq0$ . Why is this true?","['laplacian', 'eigenfunctions', 'ordinary-differential-equations', 'partial-differential-equations']"
3983624,Existence of point with high curvature.,"When working on an exercise, I stumbled upon the following ""intuitive fact"" which I couldn't prove: Let $\alpha, \beta:[a, b] \to \mathbb{R}^{2}$ be two smooth curves with $\alpha(s) = \lambda(s)\beta(s)$ where $\lambda(s) \geq 1$ and $\lambda(a) = \lambda(b) = 1$ . The trace of $\alpha$ is an arc of a circle of radius $r$ . The tangent vectors of both cuves coincide at the ends of the domain. Conjecture : There exists a point $s\in[a, b]$ with curvature $\kappa_{\beta}(s) \geq \frac{1}{r}$ The intuition is that $\beta$ has to ""bend"" more in order to get to the same point. My attempt I was trying to use an angular function for the curve $\beta$ , $\theta'(s) = \kappa_{\beta}(s)$ since it traslates to $\theta(a) - \theta(b) = $ "" measure of the arc described by $\alpha$ "", and seems to relate both givens and the curvature which I'm trying to bound. However, the inequalities come out the other way, giving me an upper bound instead of a lower bound, which is a clear sign that there's some property that I'm not exploiting. Edit : In my exercise, the curvature of $\beta$ does not vanish. Use that hypothesis to show the result if necessary. As pointed out in the comments, degenerate cases are avoided with a little stronger but still reasonable condition that I've added in the statement. For example, in my exercise the curve $\beta$ is also simple.","['curvature', 'differential-geometry']"
3983698,$ G/H \cong K $ and $ G/K \cong H\implies G \cong H \times K $,"Let $ (G, *) $ be a group. Say, $ H, K \lhd G $ such that $ G/H \cong K $ and $ G/K \cong H $ Example: $\ G = Z_{4}, \ H = \{ 0, 2\}, \ K = \{ 0, 2\}  \ $ satisfies the above condition, but $ G \not\cong \{(0,0),(0,2),(2,0)(2,2) \} = H \times K$ , Here we observe that $H \cap K \not= \{(0,0)\}$ Example: $ \ G = GL(n,R), H = SL(n,R), K = Z(G)$ also satisfies the above condition, and $G = H \times K$ , Here we observe that $H \cap K = \{I_{n \times n}\}$ So, intutively I can feel that $ \ \ G \cong H \times K \iff H \cap K = \{ e \} $ Can anyone prove or counter it? (In case the statement is wrong, please provide an iff condition for $ G \cong H \times K$ where $ H, K \lhd G $ such that $ G/H \cong K $ and $ G/K \cong H $ )","['group-theory', 'abstract-algebra']"
3983701,Leaves of a tree made up of internal nodes with either 3 or 2 children,"I'm currently struggling with solving the following problem: given a tree T made up of n internal nodes, where n-k nodes have 2 children and k nodes have 3 children, demonstrate that T has n+k+1 leaves. Can anyone help me or point me to the right direction? Thank you in advance!","['graph-theory', 'trees', 'discrete-mathematics']"
3983727,Show the following for $f(x)=\dfrac{1}{1+e^{-x}}$,"$f(x)=\dfrac{1}{1+e^{-x}}$ is sigmoid function. Clearly $f(x)$ is bijective Show that for an arbitrary interval $(a,b)$ , by using the composition of $f$ with an appropriate function $g: (0,1) \to (a,b)$ , show that $|\mathbb{R}|=|(a,b)|$ and use this to prove that $[a,b]\approx(a,b)$ This is an exercise in my book. But I couldnt solve it. Please help Edited: Considering $g: (0,1) \to (a,b), $ $g(x)=(b-a)x+a$ this function is bijective. So $gof$ is also bijective. So can we say that his shows $|\mathbb{R}|=|(a,b)|$ For the second part  I think we need a function lets say $h$ such that $h:[a,b]→(a,b)$ is a bijection.",['elementary-set-theory']
3983745,Finite groups schemes over a field are killed by the order,"I am trying to prove that a finite group scheme over a field is killed by the order. I think I can do the case where the field $k$ is perfect, here's a poorly written synopsis: First, by definition a finite group scheme over a field is affine. For a finite group scheme $G = \text{Spec}A$ where $A$ is a $k$ -algebra, which is finitely generated over $k$ as a module. It's clear to see that $G$ is just a disjoint union of points. Since $k$ is perfect, $G_\text{red}$ is actually a closed subgroup, and so we can assume WLOG that $G$ is reduced. Further, by base extension with $\overline{k}$ , we can assume $k$ is algebraically closed. Now we can just use the fact that for any finite extension $K$ of $k$ , $K\otimes_k \overline{k}$ is isomorphic to the sum of injections of $K$ into $\overline{k}$ to see that the orders of the group schemes are comparable, and so since $G$ is a finite group scheme over an algebraically closed field, it's a constant group scheme so the result follows from group theory. Here we used twice that $k$ is perfect: Using $G_{red}$ as a subgroup In the base extension with $\overline{k}$ which is why I don't think this proof generalizes to the case where $k$ is not perfect. I'm stuck trying to prove the case where $k$ is not perfect. I know that somehow I am meant to use Frobenius morphisms, but I am not quite sure how to do this. The only material that I can find online is regarding Deligne's theorem which as far as I can tell only applies to commutative group schemes. Any help or hints would be appreciated.","['group-schemes', 'algebraic-geometry', 'arithmetic-geometry']"
3983799,Mistake in paper? Subgroup of order $4$ in group of order $16$.,"I'm reading this proof from this article and I don't see why one argument works. In the Lemma, $n_4(G)$ is the number of elements of order $4$ in $G$ . In the first step of the induction we have a group $G$ with $|G|=16$ and $exp(G)=4$ , three maximal subgroups $A,B,C \leq G$ , s.t. $A \cap B \cap C =H$ with $|H|=4$ . Now the author says, that if $C_G(H)=G$ , then $G$ is abelian. I don't see, why this is true and I think I found a counterexample: I looked up $SmallGroup(16,3)=C_2^2 \rtimes C_4$ in which exist three maximal subgroups with intersection $H=Z(G) \cong C_2^2$ . Clearly $C_G(H)=G$ , but $G$ is not abelian. What am I missing? SmallGroup(16,3) Edit: I think the next argument is wrong aswell. If $C_G(H)\not = G$ , then $C_G(H)=A$ is indeed abelian, but its not true, that $B\cong C$ . See $SmallGroup(16,13)$ and chose $Q_8$ as a maximal subgroup. It will always lead to $(A,B,C)=(C_4 \times C_2,Q_8,D_8)$ (up to ordering). Is there any quick way to see that all groups of order $16$ contain a number of elements of order 4, that is divisible by $4$ ?","['group-theory', 'abstract-algebra', 'finite-groups']"
3983855,What books would you recommend to understand gauge theories?,I am a math student who got interested in the topics above also I want to learn about the Dirac matter the spinors the Einsteins GR and the Yang- Mills Maxwell Anderson -Higgs theories and models and general gauge theories and wants to understand them. What are some good introductory books for people like me where they have some mathematical understanding but not physics background. I need these books to describe them mathematical and not just explain them in general.,"['gauge-theory', 'physics', 'differential-topology', 'differential-geometry']"
3983857,How prove $a_{n}>0$ with Komal 661 problem,"Let $K$ be a fixed positive integer,Let $(a_{0},a_{1},\cdots )$ be the sequence of real numbers that satisfies $a_{0}=-1$ and $$\sum_{i_{0},i_{1},\cdots,i_{K}\ge 0,i_{0}+i_{1}+\cdots+i_{K}=n}\dfrac{a_{i_{1}}a_{i_{2}}\cdots a_{i_{K}}}{i_{0}+1}=0$$ for every postive integer $n$ ,show that $a_{n}>0$ for $n\ge 1$ This problem is from Komal problem 661. https://www.komal.hu/feladat?a=feladat&f=A661&l=en",['sequences-and-series']
3983860,Showing how many times 3 shows up in compositions of n greater than or equal to 4,"Suppose $n \geq  4$ . Show that in a list of all $2^{n−1}$ compositions of $n$ , the integer 3 occurs exactly $n\cdot 2^{n-5}$ times.
(Hint: As in lectures, look at ways of drawing lines between $n$ dots.) I understand that for 4, say, you draw a line between the 1st and 2nd dot and then you have the partition (1,3) and also drawing a line between the 3rd and 4th dot gives you the partition (3,1). There's the two examples for 4. It's when you get to the examples for $n \geq 4$ that I struggle. I know that there will be a sum ...+3+... and that the numbers on either side of the 3 are important but how this then gets to $n \cdot 2^{n-5}$ is where I lose track. I don't have an example like this in my lecture notes. Only the formulas to say that the number of compositions of n is $2^{n-1}$ and a diagram explaining how the dots and bars works to find total compositions of n but nothing on how these could be used to find the number of compositions containing a specific integer.","['integer-partitions', 'combinatorics']"
3983880,"Computing the integral $ \int_{1}^{\infty} \frac{x \,\mathrm{d} x}{\sqrt{2x^6+2x^3+1}}$","Compute the following integral $$ \int_{1}^{\infty} \frac{x \,\mathrm{d} x}{\sqrt{2x^6+2x^3+1}}.$$ This integral obviously converges. However, is it computable?","['integration', 'calculus', 'analysis']"
3983882,Stationary Distribution of a Stochastic Processes,"A Markov Chain with states 0,1,... has transition probabilities $$p_{jk}=e^{-a} \sum_{r=0}^k \left( \begin{matrix} j \\ r \end{matrix} \right) p^r (1-p)^{j-r} a^{k-r} / (k-r)!$$ Show that the limiting distribution $\{ v_k\}$ is Poisson with parameter $a/(1-p)$ . My attempt was using: $$ \sum_{k=0}^\infty p_{jk} = 1 $$ and $$ v_k = \sum_{j=0}^\infty v_j p_{jk} $$ But I am really out of order on how to simplify $$ v_k = \sum_{j=0}^\infty v_j e^{-a} \sum_{r=0}^k \left( \begin{matrix} j \\ r \end{matrix} \right) p^r (1-p)^{j-r} a^{k-r} / (k-r)! $$","['statistics', 'stationary-processes', 'markov-chains', 'stochastic-processes', 'markov-process']"
3983914,Show that the solution of this (nonlinear) ODE cannot remain bounded as $t\to\infty$,"Preliminary properties : Let the state vector $x(t)=[x_1(t),\dots,x_n(t)]^T\in\mathbb{R}^n$ be constrained to the dynamical system $$
\dot{x} = Ax + 
\begin{bmatrix}
\phi_1(x_1) \\
\vdots \\
\phi_n(x_1) \\
\end{bmatrix}, \ \ \ \ x(0) = x_0
$$ where $A$ is defined by: $$
A = 
\begin{bmatrix}
\lambda_1 & 1 & 0 &\cdots& 0\\
0 & \lambda_2 & 1 &\ddots&\vdots\\
\vdots&\ddots&\ddots&\ddots&0\\
0&\cdots&0&\lambda_{n-1}& 1\\
0&\cdots&0&0&\lambda_n 
\end{bmatrix}
$$ with $\lambda_i>0$ , and $\phi_i(x_1) = \beta_i |x_1|^{\alpha_i}\text{sign}(x_1), \beta_i>0$ , $0<\alpha_i<1$ . Question: Is it possible to show that for any initial condition $x_0\neq 0$ , the solution $x(t)$ either converge to the origin, or $
\lim_{t\to\infty}\|x(t)\| = +\infty
$ , but cannot remain in a bounded trajectory different from staying at the origin? Concretelly, what additional structure or conditions on the system or the initial condition do we require to show this? In case you find this useful, here are my attempts to understand/solve the problem. Attempt 1 : I was trying to use results such as the ones from here which can conclude what I want, but require to find a Lyapunov-like function (not necesarilly positive definite) for which $\ddot{V}\neq 0, x\neq 0$ . However, I haven't been able to come up with a suitable such function. Attempt 2 : The differential equation have ""explicit"" solution (not precisely explicit but can be expressed as) $$
x(t) = e^{At}x_0 + e^{At}\int_0^se^{-As}\Phi(x_1(s))ds
$$ where $\Phi(x_1) = [\phi_1(x_1),\dots,\phi_n(x_1)]^T$ . So I wanted to proceed by contradiction: assume that there exists $b,B>0$ and $T>0$ such that $b\leq \|x(t)\|\leq B$ for all $t\geq T$ . Hence, $$
b\leq \left\|e^{At}x_0 + e^{At}\int_0^se^{-As}\Phi(x_1(s))ds\right\|\leq B
$$ And noticing that in this case there should be $c,C>0$ such that $0<c\leq\|\Phi(x_1(t))\|\leq C $ , for all $t\geq T$ . Thus, try to obtain a contradiction, for example by using $C\geq\|\Phi(x_1(t))\|$ to show that $B\leq\|x(t)\|$ . But unfortunately I haven't obtained anything positive in this direction neither. Attempt 3 : Can Bendixon's/Dulac criterion (see Theorem 11 here ) be used to conclude something for this system? It is easy to verify that if we write this system as $\dot{x} = f(x)$ , we obtain $\nabla\cdot f(x)>0$ . I know that neither my attempts nor my exposition here are perfect. However, I'm looking for suggestions/references or any idea which might help me understand more this problem.","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'stability-theory', 'nonlinear-system']"
3983937,Understanding the second condition in Fritz John's Theorem,"Fritz John's Theorem: Each convex body $K$ contains a unique ellipsoid of maximal volume. This ellipsoid is $B^n_2$ iff: $B^n_2 \subset K$ and (for some $m$ ), there are Euclidean unit vectors $(u_i)_1^m$ on the boundary of $K$ and positive numbers $(c_i)_1^m$ satisfying \begin{equation}
    \sum_{i=1}^m c_i u_i = 0
\end{equation} and \begin{equation}
    \sum_{i=1}^m c_i \langle x,u_i\rangle^2 = \|x\|^2 \text{ for each }x\in\mathbb{R}^n
\end{equation} This is an excerpt from Keith Ball's notes. He goes on to say that the second condition (involving $\|x\|^2$ ) is equivalent to $$x = \sum c_i\langle x,u_i\rangle u_i$$ for all $x\in\mathbb{R}^n$ .
This is easily seen so I've skipped the proof. ""...The $(u_i)$ behave rather like an orthonormal basis in that we can resolve the Euclidean norm as a weighted sum of squares of inner products..."" I think this is because for two vectors $x,y$ we can write $$\langle x,y\rangle = \sum_i c_i\langle x,u_i\rangle \langle y,u_i\rangle$$ ""This guarantees that the $(u_i)$ do not all lie close to a proper subspace of $\mathbb{R}^n$ . If they did, we could shrink the ball a little in this subspace and expand it in an orthogonal direction, to obtain a larger ellipsoid inside $K$ ."" What does it mean to lie close to a proper subspace of $\mathbb{R}^n$ ? Thanks for reading, I would appreciate any help! I hope to better understand this theorem and its consequences. Edit: Later in the notes, the author says to interpret the second condition as a rigidity condition: One may get a bit more of a feel for the second condition in John’s Theorem by interpreting it as a rigidity condition. A sequence of unit vectors $(u_i)$ satisfying the condition (for some sequence $(c_i)$ has the property that if $T$ is a linear map of determinant $1$ , not all the images $Tu_i$ can have Euclidean norm less than $1$ . How do we prove this? I am unable to come up with a proof by contradiction (as follows).
Assume $\|Tu_i\| < 1$ for all $1\le i\le m$ . Now I'm trying to put $x = u_i$ in the second condition but haven't gotten anything fruitful yet. Thanks! Update: I tried putting $x = Tu_j$ in condition $2$ and got something redundant. If you're interested, you may read ahead: $$\|Tu_j\|^2 = \sum_i c_i \langle Tu_j,u_i\rangle^2 \le \sum_i c_i \|Tu_j\|^2 \implies \|Tu_j\|^2 (1 - \sum_i c_i) \le 0$$ which is nothing new since we already have $\sum_i c_i = n$ . Nothing special about $Tu_j$ here, we would have gotten this from any $x$ . Update 2: I'm trying to use Hadamard's inequality but haven't gotten anywhere yet. The bounty is about to end and I hope someone gives it a shot?","['ellipsoids', 'volume', 'convex-geometry', 'geometry', 'optimization']"
3983949,Lyapunov Exponents vs Lyapunov Function,"Suppose I have a system of differential equations $\dot {\vec x} = \vec f(\vec x)$ , and that there is an equilibrium point $\vec x = 0$ . Moreover, suppose I know the Lyapunov exponents of the equilibrium $\vec x = 0$ . Is there a way to calculate the Lyapunov function $V(\vec x)$ given the Lyapunov exponents?","['stability-in-odes', 'lyapunov-functions', 'ordinary-differential-equations']"
3983961,"prove that $3x - x^3 < 2 \cdot \sin(\pi/2 \cdot x)$ $\forall x \in (0, 1)$","I faced problems of such kind before and usually I would just chech that on the ends of segment values of compared functions are equal and then find their derivatives and make sure that derivative of one function is always greater on some interval or segment. However, in this case it just didn't work. I found derivative but on some interval $A \subset (0, 1)$ one is greater than the other and on another interval $B \subset (0, 1)$ everything is vice versa. Do you have any other meaningful approaches to this problem?","['calculus', 'derivatives', 'inequality']"
3983965,Visualizing lower bound for the Happy Ending Problem (small cases).,"I'm studying the Happy Ending Problem now, which states that For any positive integer $s$ , any sufficiently large finite set of points in the plane in general position has a subset of $s$ points that form the vertices of a convex polygon. Let $g(s)$ denote the minimum number of points in general position whose set must contain a convex $s-$ gon. It's known that $g(s) = 2^{s-2}+1$ for $s\le 6$ , but the exact value for $s>6$ is unknown. However, it has been shown that $g(s) > 2^{s-2}$ for all $s$ through an example of a set of $2^{s-2}$ points without any convex $s-$ gon. I've seen the construction of this example, but I have a hard time visualizing it because it involves organizing sets of points very distant from each other recursively. I'm looking for a geometric visualization of sets like that for small cases of $s$ . It is easy to find the setting for $s = 4$ and $s = 5$ (which are as follows) in the online articles on the subject, but I was unable to find any illustration for $s\ge 6$ . An example for the case $s = 7$ would satisfy me a lot. Above, examples for $s=4$ (violet) with $4$ points for and $s=5$ (green) with $8$ points. Edit: Going with my intuition, I have found the following set of $16$ points which seems to not contain any convex hexagon. There are too many polygons to check, though, despite of all the symmetries, so I'm not sure about it. In the figure I'm using a polar grid with an angle of $\dfrac{\pi}{40}$ , in case someone wants to extract the exact coordinates. I think those examples for $4\le s\le 6$ (if the last one is correct) consisting of concentric $s$ -gons are pretty neat, but my hopes for this pattern to continue are not very high.","['euclidean-geometry', 'geometry', 'examples-counterexamples', 'combinatorics', 'ramsey-theory']"
3983984,Relation between Morse number and Heegaard number,"Given a closed 3-manifold $M$ , a self-indexing Morse function $f:M\to[0,3]$ produces a Heegaard splitting by : $$M=f^{-1}([0,3/2])\cup f^{-1}([3/2,3]).$$ ( This question discussed a bit on the topic.) Now, we define : Definition : Let $M$ be a closed 3-manifold. The Morse number $\mathfrak{m}(M)$ of $M$ is the minimal number of critical points of Morse functions. The Heegard number $\mathfrak{h}(M)$ of $M$ is the smallest genus of any Heegard splitting of $M$ . We have a theorem that Milnor used to create exotic spheres : Theorem : (Reeb) Let $M$ be a closed $n$ -manifold such that $\mathfrak{m}(M)=2$ . Then $M$ is a topological $n$ -sphere. Moreover, by a result from Alexander, we have that $\mathbb{S}^3$ is the only manifold whose Heegaard number is $0$ , that is we have, for a closed 3-manifold : $$\mathfrak{h}(M)=0\iff\mathfrak{m}(M)=2\iff M\cong\mathbb{S}^3.$$ This suggests that the two may be related. My questions are the following : Are $\mathfrak{m}(M)$ and $\mathfrak{h}(M)$ related ? Given a self-indexing Morse function $f:M\to[0,3]$ , is the genus of the associated Heegaard splitting related to the number of critical points of $f$ ? About the first question, I am wondering whether there is a somewhat- nice formula, like a linear relation $\mathfrak{m}(M)=A\mathfrak{h}(M)+B$ ? About the second question, I found no literature about this construction for a Heegaard splitting, the one I knew about was using Moise's triangulation theorem. As a side note, about the first question, if a linear relation existed, taking $M=\mathbb{S}^3$ or $\mathbb{S}^1\times\mathbb{S}^2$ would yield (unless I made mistakes in my computations) $\mathfrak{m}(M)=2[\mathfrak{h}(M)+1]$ . However, I didn't manage to compute the two numbers for a 3-torus $\mathbb{T}^3$ to check whether there is a contradiction. In any case, a polynomial relation $\mathfrak{m}(M)=P(\mathfrak{h}(M))$ would be respectable ! Edit : I've been reading some stuff here and there, and I have noticed : Given a self-indexing Morse function $f:M\to[0,3]$ , assuming that there are only one index zero and one index three critical points , then the genus of the associated Heegaard splitting is $g=\# f^{-1}(1)$ the number of index one critical points. It is always possible to construct a Morse function satisfying the hypotheses of the previous bullet. However, my concern is that the function constructed this way is not necessarily minimal (in the sense that its number of critical points is not necessarily the Morse number of the manifold). Moreover, this construction is certainly not natural. At last, I am not sure that given a self-indexing Morse function $f:M\to[0,3]$ , the construction of the new Morse function $\tilde{f}:M\to[0,3]$ with $\# f^{-1}(0)=\# f^{-1}(3)=1$ preserves the total number of critical points -- I haven't yet looked at the very details of this construction (see Milnor, Lectures on the h- cobordism theorem ). In any case, this partially answers my second question, at least in that special setting where we assume only one index zero/three critical point. It would be interesting to see what you think about this !","['general-topology', 'differential-topology', 'morse-theory']"
3984013,Example for Carleman Linearization resulting in a linear system,"The Carleman linearization came to my attention due to this article . I tried to understand this method but so far i wasn't succesful i tried to understand page 39 of this presentation however the example didn't make sense for me. Could someone demonstrate how to compute a Carleman linearization and demonstrate/argue that the resulting linear ODE behaves similar to the non linear system? I would prefer a demonstration that $\frac{dx}{dt} = f(x,t)$ has a multidimensional $x$ $f(x,t)$ is non linear in $x$ It would be nice if the nonlinear system is well understood.  Examples would be the single or double inverse pendulum, Dubins car, SIR model, Lotka-Volterra  but that is not a requirement shows what role initial conditions play If there is some visualization (for example a phase portrait) that shows how a finite approximation of the infinite dimensional linear equation breaks and how it gets better if a larger finite dimensional approximation is used please also add that. Are there some well understood conditions when a Carleman linearization will be non successful in reproducing the dynamics?","['nonlinear-system', 'linearization', 'analysis', 'ordinary-differential-equations']"
3984049,"If two convex polygons tile the plane, how many sides can one of them have?","The set of convex polygons which tile the plane is, as of $2017$ , known: it consists of all triangles, all quadrilaterals, $15$ families of pentagons , and three families of hexagons . Euler's formula rules out strictly convex $n$ -gons with $n\ge 7$ . (The pentagonal case is by far the most difficult one.) I am interested in pairs of convex polygons that can collectively tile the plane. Specifically, I am curious how many sides can be in a polygon which is part of such a tiling. Here are some conditions to impose on such a tiling, from weakest to strongest: There is at least one copy of each tile. (Without this condition, one can trivially take a pair consisting of a tiling polygon and any other convex polygon, and just never use the latter shape.) There are at least $k$ copies of each tile. There are infinitely many of each tile. Every tile borders a tile of the other type. The tiling is $2$ -isohedral, i.e., every tile can be carried to any other tile of the same shape by a symmetry of the tiling. Each of these conditions implies those above it. In the weakest case, the number of sides is unbounded, as exhibited by the following example: (The tiling is constructed by decomposing ""wedges"" of central angle $2\pi/N$ into congruent isosceles triangles, and then combining the central triangles to yield an $N$ -gon in the center.) Requiring at least $k$ of each tile still yields arbitrarily high numbers of sides, by taking the above construction for $N=M\cdot k$ and subdividing the $N$ -gon into $k$ ""wedges"" which are $(M+2)$ -gonal. On the other end of the spectrum, I have found a $2$ -isohedral tiling using regular $18$ -gons, shown below: After consulting this paper , it seems that the tiling pictured above is of type $4_2 18_{12}-1\text{a}\ \text{MN}\ \text{p}6\text{m}$ in their classification scheme (shown at the bottom of page 109); there are no $2$ -isohedral tilings which allow for any higher number of contacts between different shapes, although type $3_1 18_{12}-1\text{a}\ \text{MN}\ \text{p}6\text{m}$ also works (and can be obtained from the above construction by cutting each kite-shaped tile in two). Thus, it is maximal among $2$ -isohedral tilings. What are the maximal tilings under weaker conditions? The maximal number of sides under each successively stronger restriction is a weakly decreasing sequence which goes $\infty, \infty, ?, ?, 18$ . So far, I have no bounds on the missing two terms except that they are each at least $18$ . Some notes on this problem: It is not necessarily the case that one of the tiles may tile the plane on its own; see this math.SE question for a counterexample. If convexity is relaxed for either piece, the number of sides is unbounded even in the $2$ -isohedral case (in fact, both pieces can simultaneously have arbitrarily many sides). Edit: Crossposted to Math Overflow here .","['convex-geometry', 'geometry', 'polygons', 'plane-geometry', 'tiling']"
