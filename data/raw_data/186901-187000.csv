question_id,title,body,tags
3469939,"Counterexamples to: If $f:X \to Y$ is continuous, $Y$ is compact, then $f^{-1}$ is continuous.","It is a theorem that if $f:X \to Y$ is a continuous bijection, $X$ is compact, then $g = f^{-1}$ is continuous. My professor asked us to find a counterexample to If $f:X \to Y$ is continuous, $Y$ is compact, then $g = f^{-1}$ is continuous. I do not like my counterexample so much because it uses the discrete metric. Are there other counterexamples? My Counterexample: Let $X = [0, 1]$ with the Discrete metric, $Y = [0, 1]$ with the Euclidean metric, and let $f$ be the identity function.","['compactness', 'metric-spaces', 'analysis', 'real-analysis']"
3469967,Finding eigenfunction(s),"I am stuck in a part of a proof perhaps somebody can help me. I've got the following eigenvalue problem: $$u'' + \lambda u = 0 \quad 0 <x < L$$ With boundary conditions: $$ -u'(0) + \sigma_1 u(0) = 0, \quad u'(L) + \sigma_2 u(L) = 0$$ Where both $ \sigma_1$ and $ \sigma_2$ are constants. By writing $\lambda = k^2$ , for real $k$ , the general solution is : $$u(x) = A \cos kx + B \sin kx$$ and imposing the boundary conditions: $u(0) = A$ , $u(L) =A \cos kL + B \sin kL$ , $u'(x)= -kA \sin kx +kB \cos kx$ Now substituting the given boundary values: $u'(0) = kB$ , and $u'(L) = -kA \sin  kL +kB  \cos kL$ Which gives: $$-kB + \sigma_1A = 0 $$ $$-kA \sin  kL +kB  \cos kL + \sigma_2(A \cos kL + B \sin kL)=0$$ The last equation can be re-arranged to: $$(-kA+\sigma_2B) \sin KL + (kB + \sigma_2A)\cos kL = 0 $$ Giving: $$\frac{(-kA+\sigma_2B)}{(kB + \sigma_2A)}\tan kL = -1$$ Now solving for $\tan kL$ gives: $$\tan kL = -\frac {(kB + \sigma_2A)}{(-kA+\sigma_2B)}$$ By using the first BC we obtain: $$\tan kL =\frac {(\sigma_1 + \sigma_2)k}{(k^2-\sigma_1 \sigma_2)}$$ But how do I get to the corresponding eigen function: $$u_n(x) = u(x;k_n) \quad u(x;k) = K(k) \sin(kx + θ(k))$$ where: $$\tan(θ(k))= \frac {k}{\sigma_1}$$ For some undetermined normalisation constant $K(k)$","['boundary-value-problem', 'ordinary-differential-equations', 'eigenfunctions']"
3469986,"If $f(17) = 17$, calculate $A(x) = \int_{1}^{x} f(t)dt$ for $x>0$.","Let $f: (0, \infty) \to \mathbb{R} $ be a continuous function. For $x>0$ , $y>0$ and any $x'$ in $(0, \infty)$ we have that $$\int_{x}^{xy} f(u) \ du = \int_{x'}^{x'y} f(u) \ du.$$ If $f(17) = 17$ , calculate $A(x) = \int_{1}^{x} f(t)dt$ for $x>0$ . I got this: $$\begin{align}
A(xy) &= \int_{1}^{xy} f(t)dt \\
&=\int_{1}^{x} f(t)dt + \int_{x}^{xy} f(t)dt \\
&= \int_{1}^{x} f(t)dt + \int_{1}^{y} f(t)dt \\
&= A(x) + A(y)
\end{align}$$ and $$\frac{dA(xy)}{dy} = \frac{dA(xy)}{dy} y = A'(x).$$ With this we have that $A'
(17y)= \frac{f(17)}{y}$ . Here I got stuck. What else I can do?","['integration', 'calculus', 'real-analysis']"
3470057,What is the exact definition of an inverse function?,"I recently started reading a book which has challenged my understanding of functions and inverse functions quite a bit (mostly the format of the notations that are used); I know that we write a function $( f )$ that maps elements of its domain, $X$ , to elements of its co-domain, $Y$ , as: $f:X\rightarrow Y \qquad x \mapsto y  \qquad \text{where} \quad x\in X \quad \text{and} \quad y=f(x) \in Y $ $\\$ This is the point where my understanding starts to deviate from the book's content. The book goes on to define the inverse of the function $f$ as the map $g:Y\rightarrow X \qquad y \mapsto x \quad \text{where} \quad y \in Y \quad x=g(y) \in X $ and then mentions that, since the map $g$ maps EVERY element of its domain ( $Y$ ) onto its co-domain ( $X$ ), it must be injective and since the same holds for the map $f$ , we conclude that both $f$ and $g$ are bijective maps. The problem with this statement, I feel like, is that it excludes the possibility of the co-domain being larger than the image of $f$ (it restricts the co-domain into being the image of $f$ ) $\\$ $f:[-\pi/2,\pi/2] \rightarrow \mathbb{R}    \quad \text{where} \quad x\mapsto f(x)=\sin x \qquad [1]$ this function is not invertible as it's not bijective (The co-domain isn't the same as the image of $f$ ) whereas the function $f:[-\pi/2,\pi/2]  \rightarrow [-1,1]    \quad \text{where} \quad x\mapsto f(x)=\sin x \qquad [2]$ is invertible. Is this correct? If it is, then, are we to say that the functions [1] and [2] are different functions? If it isn't, then doesn't that go against our definition and conditions for inverse functions?","['functions', 'inverse-function']"
3470061,What is the domain of the function $\tan\theta_{1}(\theta)=\frac{\sin\theta}{\cos\theta+1}$ and what is $\theta_1^{\max}$?,"I have some questions about the lecture that I took today on Physics. Consider the cosine function defined below, $\cos\theta=-\frac{M_2}{M_1}$ $M_1$ : Mass of the first object. $M_2$ : Mass of the second object. (Sorry for the physical terms, I was undecided between open this question here or physics stackexchange. I will not jump into the formulas, don't worry.) We have a general formula to use in different situations that is defined as, $\tan\theta_{1}(\theta)=\frac{\sin\theta}{\cos\theta+\frac{M_1}{M_2}}$ Now, consider that $M_1=M_2$ . The mass ratio will be $1$ . $M_1=M_2\Rightarrow \cos\theta=-\frac{M_2}{M_1}=-1$ $\tan\theta_{1}(\theta)=\frac{\sin\theta}{\cos\theta+1}$ Everything is OK until here. Now, as I remember my teacher said when, $\tan\theta_{1}\to\infty$ $\cos\theta+1=0$ $\cos\theta=-1$ $\theta=\pi$ Here, how does the function $\tan\theta_{1}(\theta)$ go to $\infty$ ? I mean it doesn't have a exact result when it goes to positive infinity as I know. The trigonometric tangent function $f(x)=\tan{x}$ doesn't approach to a exact result although. What does my teacher mean by this limit? Another question is how he found $\theta=\pi$ . Because, $$(\arccos)[\cos\theta=-1]$$ $$\theta=\arccos{(-1)}$$ $$\theta=\frac{\pi}{2}$$ Got stuck... Later on that, he found $0\le\tan\theta_{1}\lt\infty$ $0\le\theta_{1}\le\frac{\pi}{2}$ and ${\theta_{1}}^{\max}=\frac{\pi}{2}$ . As I think the function $\tan\theta_{1}$ has $-\infty\lt\tan\theta_{1}\lt\infty$ domain. Am I wrong?",['trigonometry']
3470075,Solving a 2nd order DE (non-constant coefficients),"I am having trouble solving the following differential equation: $$y''+(x^2-1)y'+2xy=0$$ To start off I employed the power-series method, so let $$y=\sum_{n=0}^{\infty}a_nx^n$$ $$y'=\sum_{n=1}^{\infty}na_nx^{n-1}$$ $$y''=\sum_{n=2}^{\infty}n(n-1)a_nx^{n-2}$$ Next I plugged my power-series into my original DE: $$\sum_{n=2}^{\infty}n(n-1)a_nx^{n-2}+\sum_{n=1}^{\infty}na_nx^{n+1}-\sum_{n=1}^{\infty}na_nx^{n-1}+\sum_{n=0}^{\infty}2a_nx^{n+1}=0$$ When I matched the powers of $x$ I ended up with the following equation: $$\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n+\sum_{n=2}^{\infty}(n-1)a_{n-1}x^n-\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n+\sum_{n=1}^{\infty}2a_{n-1}x^n=0$$ Next I tried to match the indeces, so naturally I took out the first 2 terms from $\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^n$ , the first 2 terms from $\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n$ , and the first term from $\sum_{n=1}^{\infty}2a_{n-1}x^n$ . At this point I end up with 2 recurrence relations: $$2a_2+6a_3x-a_0-2a_2x+2a_0x=0$$ $$(n+2)(n+1)a_{n+2}+(n-1)a_{n-1}-(n+1)a_{n+1}+2a_{n-1}=0$$ And here I am completely lost, normally what I would do is solve for one of the first $a_n$ terms but since I've got $x$ 's in the first recurrence I can't really see how that would work. Perhaps there is a much simpler method that I can use? This is just practice for my ODE final exam.","['power-series', 'ordinary-differential-equations']"
3470103,Quotients of lattices and their duals,"Let $V$ be a finite-dimensional complex vector space and let $$
\Lambda_1\subseteq\Lambda_2\subseteq V
$$ be full-rank lattices (discrete subgroups of $V$ such that $\mathrm{span}(\Lambda_i)=V$ ). Let $$
\Lambda_i^*:=\{\xi\in V^*:\xi(\Lambda_i)\subseteq\mathbb{Z}\}
$$ be the dual lattices. Then, we can consider the abelian groups $\Lambda_2/\Lambda_1$ and $\Lambda_1^*/\Lambda_2^*$ . Question. Is it true that $\Lambda_2/\Lambda_1\cong\Lambda_1^*/\Lambda_2^*$ as abelian groups?","['integer-lattices', 'group-theory', 'linear-algebra', 'discrete-mathematics']"
3470116,particular solution guessing of a Riccati equation,I can construct the general solution of a Riccati equation if a particular solution is known. Is there any guessing trick do exist for example: $$\text{For }y'+y^2=\frac{2}{x^2} \text{ seek for a P.S. in the form }y=\frac{c}{x}$$ $$\text{For }x^3y'+x^2y-y^2=2x^4 \text{ seek for a P.S. in the form }y=cx^2$$ $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\vdots$ Why seeking for a particular solution $(\text{P.S.})$ in that form $?$ And how the knew that will be work $?$ I really appreciated any kind of help.,"['analysis', 'ordinary-differential-equations']"
3470123,Prove that limited increasing or decreasing successions have a limit.,"Be $u_n$ an increasing sequence of numbers ${(u_1,u_2,u_3, \ldots)}$ such that for all $\ n$ , $u_{n-1}\le u_n$ Prove that if $u_n\le a, a \in \mathbb R$ , for all n then $\lim_{n\rightarrow\infty}u_n =b $ for some number $b\le a$ . I am using as definition of limit of a succession the following: $\lim_{n\rightarrow\infty}u_n =b \Leftrightarrow  ∀\delta \gt 0, ∃p \in \mathbb N :n\gt p \Rightarrow |u_n -b|\lt \delta$","['limits', 'limits-without-lhopital', 'sequences-and-series']"
3470167,Expected value of number of steps until range reduced to a given fraction,"First time questioner here.  I'm working on a problem from a recreational math website, but trying to understand an easier version first.  Most such problems are discrete, and the continuous nature of this one is confusing me. The simpler problem: we start with the range $[0,1]$ .  Pick a random number uniformly from that range, and remove everything above your pick.  Do that repeatedly, picking a random number from the range that's left, until the remaining range drops below some fraction $1/x$ .  What's the average number of picks required? Consider a sequence of rvs $X_1, X_2, \dots$ , where $X_1 \sim U(0,1)$ and $X_i \sim U(0,X_{i-1})$ for $i \ge 2$ .  We want to know the expected number of steps it will take until an $X_n$ is picked below some fraction $1/x$ .  That is, given $1 \gt X_1 \gt X_2 \gt \dots \gt X_{n-1} \gt \frac 1 x \gt X_n$ , what's the expected value of $n$ ? I know the closed form for this simpler problem, thanks to Monte Carlo simulations.  Playing around with various $x$ and running for billions of simulations, the counts for each possible $n$ are easy to pick out in closed form, and shows (with Wolfram Alpha's help) that $$\mathbb{E}[n] = \sum_{n=1}^\infty n \frac{(\log x)^{n-1}} {(n-1)! x} = \log x + 1$$ So $P(n = 1) = \frac 1 x$ , which is obvious. $P(n = 2) = \frac {\log x} x$ , which I think I can get as follows.  The PDF for $X_2$ is $1/{X_1}$ , so the CDF is $P(X_2 \lt t) = t/X_1$ .  I want to know $P(X_2 \lt \frac 1 x)$ , so I need to integrate that over the range of possible $X_1$ : $$P(X_2 < 1/x < X_1) = \int_{1/x}^1 \frac{dX_1}{x X_1} = \frac{\log x} x$$ But now, how do I get that $P(X_3 < 1/x < X_2 < X_1 < 1) = (\log x)^2/(2x)$ ?  I suspect I need a double integral on $dX_2\ dX_1$ , but what?  The way the log appears in $\mathbb{E}[n]$ makes it look like the $\log x$ gets introduced via one of the integration bounds, but that can't be right, can it? My last college stats class was 40 years ago, and my knowledge since then is mostly driven by solving these website questions.  So that knowledge is really spotty.  The fact that $\mathbb{E}[n]$ looks like a power series suggests this has to do with moment generating functions, I guess, but I'm really out of my depths there. I've actually ""solved"" the full problem by using a discrete analogue, starting with $4\cdot 10^{10}$ stones and finding $\mathbb{E}[n]$ to reduce that to 1/40th the original count.  That used logic related to 2025645 , where harmonic numbers are in play.  But that took too long and was off by 8 in the required 10th decimal place.  A fast answer that doesn't involve guessing the final digits needs a continuous method, which is eluding me.","['expected-value', 'uniform-distribution', 'probability']"
3470171,How to integrate $\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta$,How to integrate $$\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta$$ THis question arises from an earlier question I was working on which I posted here: Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta$ I tried a few different things all to no avail...Which Trig trick am I missing to apply to this question?,"['integration', 'multivariable-calculus', 'derivatives']"
3470196,Reference Request - Functional Derivatives,"I am looking for some good references that introduce functional derivatives in a quick but rigorous way. Any suggestions? In addition, I saw somewhere that functional derivatives are related to Frechét derivatives. Is this accurate? How are they related?","['calculus-of-variations', 'functional-analysis', 'analysis', 'reference-request']"
3470217,"Let $\alpha \in (0,1)$. Find a Borel subset $E$ of $[-1,1]$ s.t. $\lim_{r\to 0^{+}} \frac{m(E\cap [-r,r])}{2r}=\alpha.$","Let $\alpha \in (0,1)$ . Find a Borel subset $E$ of $[-1,1]$ s.t. $$\lim_{r\to 0^{+}} \frac{m(E\cap [-r,r])}{2r}=\alpha.$$ In fact, for a similar result in $\mathbb{R}^d$ also holds: $$D_{E}(x)=\lim_{r\to 0} \frac{m(E\cap B(r,x))}{m(B(r,x))}$$ we just consider the unit circle and $E$ is a sector with angle $2\pi \theta$ where $\theta\in(0,1)$ , then $$D_{E}(x)=\frac{2\pi \theta }{2\pi}=\theta$$ But how about the $\mathbb{R}$ ?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3470227,Behavior of a gradient system,"Let $\frac{d}{dt}(x,y) = -\nabla g(x,y)$ be a planar system, where $g(x,y)=x^{2}y^{4}$ . Show that for $t \to \infty$ , $\Phi(t,(x,y))\to 0$ for all $(x,y)\in \mathbb{R}^{2}$ \ $(\{x=0\} \cup \{y=0\})$ . Al right, what I've tried is: lets write this as a system, $\dot x = -2xy^{4}$ $\dot y = -4x^{2}y^{3}$ Is it enough to consider the Lyapunov function $V(x,y)=x^{2}+y^{2}$ for $(x,y)=(0,0)$ to say that all trajectories converge to $0$ for all $(x,y)\in \mathbb{R}^{2}$ \ $(\{x=0\} \cup \{y=0\})$ when $t\to \infty$ ? (I guess that $\Phi(t,(x,y))$ is the function for trajectories given certain $t$ and certain $(x,y)$ , but not so sure really).","['stability-in-odes', 'calculus', 'ordinary-differential-equations', 'dynamical-systems']"
3470241,Alexander duality and simplicial complexes,"A consequence of Alexander duality is the fact that an $n$ -dimensional simplicial complex $K$ embedded in $\mathbb{R}^{n+1}$ separates $\mathbb{R}^{n+1}$ into $\beta_n + 1$ connected components where $\beta_n$ is the dimension of $H_n(K)$ . When $K$ is a 1-dimensional complex embedded in $\mathbb{R}^2$ , $K$ is a planar graph. It is well known that the edges in a planar embedding belong to at most two faces of the embedding. That is, the 1-simplices in $K$ belong to the boundaries of at most two connected components of $\mathbb{R}^2 \setminus K$ . Does the statement for planar graphs hold in higher dimensions? If $K$ is an $n$ -dimensional complex embedded in $\mathbb{R}^{n+1}$ is it true that the $n$ -simplices of $K$ belong to the boundaries of at most two connected components of $\mathbb{R}^{n+1} \setminus K$ ?","['graph-theory', 'general-topology', 'algebraic-topology', 'planar-graphs']"
3470256,Finding a 2D Fourier transform with a Contour Integral,"I am reading a paper which has a number of two-dimensional Fourier transforms in the appendix.  For example, $$F\left(\frac{x^2}{r^3}\right)=\frac{k_2^2-k_1^2 z k}{k^3}e^{-kz},$$ where $r^2 = x^2 + y^2 + z^2$ , $k^2 = k_1^2 + k_2^2$ and the 2D Fourier transform is defined as $$\hat{f}(k_1, k_2, z)=F(f)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y,z) e^{ik_1x +ik_2 y} \: dx \: dy.$$ I suspect that these were obtained by treating the Fourier transform as a contour integral in the upper half-plane and was wondering if someone could carry out the integral and show how to do this to get the above result as an example of the method. Edit: I see how an integral from $-\infty$ to $\infty$ of a function of $x$ with respect to $x$ can be written as a contour integral along the real axis and a semicircle in the upper half plane if we assume $f$ to be analytic in the upper half-plane except for a finite number of poles. Here we have a 2D Fourier transform which is defined to be an integral of a function of $3$ variables where the integral is with respect to the first $2$ variables, so how do you write this as a contour integral in the upper half-plane?","['complex-analysis', 'contour-integration', 'fourier-analysis', 'fourier-transform']"
3470290,Sphere inside of a Sphere,"I have a very interesting question. I am given the fact that people like cookies the most when the ratio of cookie dough to chocolate is 1:1. The cookie is first placed on a sheet and has a diameter of one. The cookie is then covered in a layer of chocolate that's volume most match that of the cookie dough. It is a sphere inside of another sphere, with them having matching volumes. Please answer in fractional form
Thanks!","['spheres', 'circles', 'geometry', 'probability']"
3470382,Is a f.g. $B_f$ algebra also a f.g. $B$ algebra?,"All rings are unital commutative. Let $\pi:B \rightarrow A$ be a ring homomoprhism. Let the image of $f \in B$ be $f'$ . Then $A_{f'}$ is a $B_f$ -algebra. It seems to me that the following is true. $A_{f'}$ is f.g. as a $B_f$ -algebra iff it is f.g. as a $B$ -algebra. My argument:  Suppose it is f.g. as $B_f$ -algebra. Let the generators be $a_i/(f')^{k_i}$ . The way $B_f$ acts on an element $A_{f'}$ is via $$ \frac{b}{f^k} \cdot \frac{a}{{f'}^l} = \frac{\pi(b) \cdot a}{{f'}^{k+l}}  .$$ In other words $A_{f'}$ is in fact generated by $a_i, \frac{1}{f'}$ as both $B$ -algebra. Equally, if it is f.g. as a $B$ -algebra, then the same argument shows it is f.g. as a $B_f$ -algebra. I would be happy to know if the claim is true and if my argument is correct, if not, what went wrong.","['algebraic-geometry', 'commutative-algebra']"
3470406,"If u,g and f satisfy this condition, $p_1$ that fit the equation will also be the exact solution of the DE f at point $(v_0+h)$ if $g_pf+g_v=0$","Tried to propose this statement before but it looks like i have problem in writing the correct mathematical symbol especially dealing v and p making it very confusing. Please help to edit the symbol if the idea sound comprehensible. Given $u(v,p)$ , $g(v,p)$ and $f(v,p)$ satisfy the relationship such that $f=\frac{dp}{dv}=\frac{g-u_v}{u_p}$ with initial condition $p(v_0)=p_0$ . While $p_1$ is a solution that satisfy below equation $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ The solution of $p_1$ that satisfy the above equation will also be the exact solution of the DE $\frac{dp}{dv}=f(v,p)$ at point $p(v_0+h)$ if $g(v,p)$ satisfy the condition $g_pf+g_v=0$ Example $u(v,p)=pv^{2.4}$ and $g(v,p)=pv^{1.4}$ then $\frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{pv^{1.4}-2.4pv^{1.4}}{v^{1.4}}=-1.4\frac{p}{v}$ Solve this DE and will obtain $pv^{1.4}$ =constant, consider $p(v_0)=p_0$ hence the value of p at point $(v_0+h)$ will be $p(v_0+h)=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}}$ while the value obtain directly from this formulae $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ $p_0v_0^{2.4}+hp_0v_0^{1.4}=p_1(v_0+h)^{2.4}$ $p_0v_0^{1.4}(v_0+h)=p_1(v_0+h)^{2.4}$ $p_1=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}}$ Can try with u(v,p) and g(v,p) with other function such that it yield the same DE $\frac{dp}{dv}=-1.4\frac{p}{v}$ but it wont
give the same result by direct application of this formulae $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ ie $u(v,p)=pv^a$ and $g(v,p)=(-1.4+a)pv^{a-1}$ It produce the same $f(v,p)$ with all value of a but only $a=2.4$ give the above result Is the proof below able to verify both sequence yield the same value of $p_n$ when n tend to infinity? This equation is actually a one step numerical integration which is approximate using an Euler method but a=2.4 give the most accurate result since it is the exact solution. Does this apply the same for all u,g and f if it satisfy above conditions? Example 2 u(v,p)=pv and g(v,p)=p then $\frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{p-p}{v}=0$ assume $p(v_0)=p_0$ while from this formula $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ $p_0v_0+p_0(h)=p_1(v_0+h)$ $p_1=p_0$","['numerical-methods', 'taylor-expansion', 'ordinary-differential-equations', 'partial-differential-equations']"
3470440,How do we know that the divergence of a vector field exists?,"How do we prove that the limit of $${\displaystyle \left.\operatorname {div} \mathbf {F} \right|_{\mathbf {x_{0}} }=\lim _{V\rightarrow 0}{1 \over |V|}} \unicode{x222F}_{\displaystyle \scriptstyle S(V)} {\displaystyle \mathbf {F} \cdot \mathbf {\hat {n}} \,dS}$$ exists? $$$$ I understand how to intuitively derive the formula for divergence in Cartesian coordinates given that the limit exists (since we can then choose an easy shape for the volume), but I don't know how to prove that the limit exists in the first place.","['divergence-operator', 'multivariable-calculus', 'surface-integrals', 'vector-analysis']"
3470510,Prove that the integral of $e^{k\cos t}\cos(k\sin t)$ from $0$ to $2\pi$ equals $2\pi$,"Prove that $$I=\int_0^{2\pi}e^{k\cos t}\cos(k\sin t)\,dx=2\pi$$ and prove that $$J=\int_0^{2\pi}e^{k\cos t}\sin(k\sin t)\,dx=0$$ With the help of the following integral: $$H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx$$ I proved that the  : $$H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx=2i\pi$$ and I proved that $$I+iJ=\frac{H}{i}=2\pi$$ but what should I do next to prove that $I=2\pi$ or to prove that $J=0$ ? Is it by using $Re(z)=\cos t$ ?","['integration', 'definite-integrals', 'complex-analysis', 'complex-numbers', 'bessel-functions']"
3470539,Why is $\frac{1}{\sqrt 5}\left[\left(\frac{1+\sqrt 5}{2}\right)^n-\left(\frac{1-\sqrt 5}{2}\right)^n\right]$ an integer? [duplicate],"This question already has answers here : The number $\frac{1}{\sqrt{5}}\left[\left(\frac{1+\sqrt{5}}{2}\right)^{n}-\left(\frac{1-\sqrt{5}}{2}\right)^n\right]$ is always an integer (2 answers) How to prove that Fibonacci number is integer? [duplicate] (1 answer) Closed 4 years ago . I am looking for a proof on why $$\frac{1}{\sqrt 5}\left[\left(\frac{1+\sqrt 5}{2}\right)^n-\left(\frac{1-\sqrt 5}{2}\right)^n\right]$$ an integer. I have seen many proofs on this, but they all refer to a properties of Fibonacci numbers, which shouldn't be necessary. I am trying to see why it is true using purely elementary results such as the binomial formula. Clearly this reduces to $$\frac{1}{2^n\sqrt 5}\sum_{k=0}^n {n\choose k }\left(1-(-1)^k\right) 5^{k/2}$$ I am looking for a ""divisibility"" argument to see why this is an integer.","['number-theory', 'combinatorics', 'elementary-number-theory']"
3470552,Exercise 5.40 John Lee ISM. $S \subset M$ is a level set of a smooth map $\Phi : M \to N$ with constant rank then $T_pS = {\rm Ker} d\Phi_p$ .,"The following is Exercise 5.40 from John Lee's ISM. Suppose $S \subset M$ is a level set of a smooth map $\Phi : M \to N$ with constant rank. Show that $T_pS = {\rm Ker} d\Phi_p$ for each $p \in S$ . I am having a hard time proving this result. From Theorem 5.12 (Constant-Rank Level Set Theorem) of the txt, I know that $S$ is a properly embedded submanifold. I think the proof should be similar to Proposition 5.38.
Using the fact that the inclusion map $\iota: S \hookrightarrow M$ is a smooth immersion, I can show that since $\Phi \circ \iota$ is constant on $S$ , so $d\Phi_p \circ d \iota_p$ is the zero map from $T_pS$ to $T_{\Phi(P)}N$ , and therefore ${\rm Im} d \iota_p \subset {\rm Ker} d\Phi_p$ . Up to here, is identical to the proof of Proposition 5.38. However, I cannot use that $d\Phi_p$ is surjective as in the proof, so I cannot conclude that ${\rm Im} d \iota_p = {\rm Ker} d \Phi_p$ . I am lost here. I would greatly appreciate any help.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
3470574,spin structure on $\Gamma \backslash S^n$,"Let $Γ \subset SO(n + 1)$ be a finite group acting freely on the unit sphere $S^n \subset \mathbb{R}^{n+1}$ . Show that the quotient $\Gamma \backslash S^n$ admits a spin structure if and only if there is
a subgroup $\tilde{\Gamma} \subset \operatorname{Spin}(n+1)$ such that the covering homomorphism $\tilde{Ad}: \operatorname{Spin}(n+1)\to SO(n+1)$ restricts to an isomorphism $\tilde{\Gamma} \cong Γ$ . My question: For the ""only if"" part, the map $\tilde{\Gamma} \backslash \operatorname{Spin}(n+1) \to \Gamma \backslash S^n$ gives a spin structure. However I am stuck at the ""if"" part. How would I construct this $\tilde{\Gamma}$ ? Thank you very much!","['principal-bundles', 'spin-geometry', 'differential-geometry']"
3470575,Wrapping a Christmas Tree with Even Spacing and Constant Slope,"Last week, when I was wrapping strings of beads around my Christmas tree, I initially had the following design in mind: I wanted each pass around the tree to be evenly spaced (e.g. exactly one foot below each part of the string would be another part of the string), and I wanted the slope of the string to be constant (with the exception of the vertex of the tree, since slope isn't defined there). I quickly realized that this was mathematically impossible, since a constant slope along the entire string of beads would imply increased spacing between each pass down the tree due to the tree's increasing diameter at lower heights. That said, if we only look at the points at which the string of beads intersects a given ray down the side of the tree, then it is possible for the string to be at the same spacing and slope for each point on that ray by flattening or steepening the string in that neighborhood. My intuition says that this cannot apply to all rays at once, no matter how varied or strange we make the path, but I'm struggling to prove why that is. My thought is that flattening or steepening the string around whatever point we're looking at has to be compensated for in other neighborhoods, but I'm not sure how to formalize this. Any ideas?",['differential-geometry']
3470583,There are 3 red balls and 2 white balls in a box. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Now consider an experiment in which a ball is taken out from the box each time, then put back after its color observed, and one ball is put into again whose color is the same as the taking out one. if we repeat the experiment 4 times in succession, try to find the probability that we obtain a white ball in the 1st, 2nd times, and obtain a red ball in the 3rd, 4th times.","['statistics', 'probability']"
3470595,Solve $x^{2}y''-3xy'+4y=x^{2}lnx$.,Solve $x^{2}y''-3xy'+4y=x^{2}lnx$ . I wanted to try and follow my professor's notes but I was confused when she wrote $\eta=lnx$ and $\frac{dy}{dx}=\frac{1}{x}\frac{dy}{d\eta}$ . Something about changing the equation to have constant coefficients. Is there a step-by-step way of solving such a differential equation?,['ordinary-differential-equations']
3470600,Is there an efficient way of solving $y^{(4)}+2y''+y=0$?,"Is there an efficient way of solving $y^{(4)}+2y''+y=0$ ? I wanted to use the following theorem: However, the characteristic equation is unfactorable when its highest factor is 3! So I was thinking about using Laplace transform, by first finding $\hat\Phi(s)=(sI-A)^{-1}$ , where $A$ is the companion matrix, and transforming it back into the t-domain. But this is a $4\times 4$ matrix I have to find the inverse to and the row operations get super messy with the $s$ variable involved. So I am wondering...is there a better way that is around this scope of difficulty? Or do I just fight it out with the algebra and find $\hat\Phi(s)$ ?","['laplace-transform', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
3470679,Prove Fubini's Theorem for non-negative functions,"The objective is to prove the complete Fubini's Theorem $$\int_\mathbb{R}\left(\int_\mathbb{R}f(x,y)dy\right)dx=\int_\mathbb{R^2}f$$ where $f : \mathbb{R^2} \rightarrow \mathbb{R}$ is a Lebesgue absolutely integrable function. Since the Lebesgue integration is defined by positive and negative parts (i.e. $\int_\Omega f=\int_\Omega f^+ - \int_\Omega f^-$ where $f^+=max(f,0)$ and $f^-=-min(f,0)$ ). It suffices to prove the theorem for non-negative functions, and use the linearity of non-negative integration to prove the general case. Next, it suffices to prove the theorem for non-negatively supported functions $f$ on $[-N,N]\times[-N,N]$ for some positive integer N then take suprema $$f=\sup_{N>0}f1_{[-N,N]\times[-N,N]}$$ and apply the monotone convergence theorem ( $\sup\int f_n=\int\sup f_n$ where $f_n(x)$ is monotone increasing on $n$ for any fixed $x$ ). Similarly, the integration of non-negative functions is defined as $$\int_\Omega f=\sup\left\{\int_\Omega s:\text{s is a simple function and minorizes f}\right\}$$ I think I could use the monotone convergence theorem again. But it applies to sequences of functions, not sets of functions. If I were to construct a sequence, the subscript would depend on $x$ and I don't know how to proceed.","['analysis', 'real-analysis']"
3470682,"Show $\max\{X_1,...,X_n\}/n$ converges in probability to $0$","Suppose $X_n\geq0$ are any sequence of random variables and $\dfrac{X_1+...+X_n}{n}\stackrel{p}{\to}1$ . Then show that $\dfrac{\max(X_1,...,X_n)}{n}\stackrel{p}{\to}0$ and for any $r>1$ , $\dfrac{1}{n^r}\sum_{k=1}^n X_k^r\stackrel{p}{\to}0$ . I could prove the almost sure version of this, that is, replacing convergence in probability by almost sure convergence everywhere in the statement. I thought of a subsequence argument (first showing for a subsequence that almost surely the results hold) but things don't seem to go through. I have obtained that $X_n/n$ converges in probability to $0$ . If this was an almost sure statement then I could immediately say that this implies $\max(X_1,...,X_n)/n$ also converges almost surely to $0$ .","['probability-limit-theorems', 'probability-theory', 'random-variables']"
3470684,Isometry between $L^\infty$ and $(L^1)^*$,"I know that for $p \in (1,\infty] \,$ , $L^q(\Omega,\mathcal{M},\mu)$ is isometric to $(L^p(\Omega,\mathcal{M},\mu))^*$ with $\frac{1}{p}+\frac{1}{q}=1$ , namely that the operator: $$ T:L^q \to (L^p)^*, \;\; T: g \mapsto L_{g}, \;\; L_{g}f = \int_{\Omega}{fg \:d\mu} \;\; \forall f\in L^p$$ is an isometry. Indeed: $ ||L_{g}||_{*} \leq ||g||_{L^q} $ and we can find $f_{0} \in L^p \,$ s.t. $ |L_{g}f_{0}| = ||g||_{L^q} $ So $ ||Tg||_*  = ||L_{g}||_{*} = ||g||_{L^q} $ and we have an isometry. Moreover, if $p \in (1,\infty)$ , $T$ is surjective and thus it is an isomorphism. This is also the case for $p=1$ if $\mu$ is $\sigma$ -finite. I 'm trying to understand if $L^\infty$ is isometric to $(L^1)^*$ even if $\mu$ is not $\sigma$ -finite. Clearly: $ ||L_{g}||_{*} \leq ||g||_{L^\infty} $ , I'm struggling in finding $f_{0} \in L^1 $ s.t. $ |L_{g}f_{0}| \geq ||g||_{L^\infty} $ , and I'm questioning if such $f_{0}$ even exists.","['operator-theory', 'lp-spaces', 'functional-analysis', 'dual-spaces', 'isometry']"
3470698,Wedge product of $\beta \wedge dx$,"I'm still struggling to understand the wedge product (and also differential forms in general) and am therefore trying to find / come up with actual examples. Say I have differential forms $\alpha = dx + dy + dz$ and $\beta = 2dx - dy + dz$ , how would I wedge $\alpha \wedge \beta \wedge dz$ . Sorry if this is not a good question, I'm just really lacking good intuition, yet.","['differential-geometry', 'multivariable-calculus', 'intuition', 'differential-forms', 'exterior-algebra']"
3470705,Minimal sufficient statistic implies any complete statistic is also minimal sufficient,"I am reading about data reduction of Casella book. There is a theorem establishing the next: If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic. A proof is no provided by the author; is there a reference for a formal proof of this fact? Any kind of help is thanked in advanced.","['statistical-inference', 'statistics', 'reference-request']"
3470800,Proving the Lebesgue measure space completes the Borel measure space,"I am trying to complete Exercise 1.4.27 page 78 of Terence Tao's book ""An Introduction to Measure Theory"", but is caught up in some troublesome details. The exercise is as follows: $\bullet$ Show that the Lebesgue measure space $(\mathbb{R}^d,\mathcal{L}[\mathbb{R}^d],m)$ is the completion of the Borel measure space $(\mathbb{R}^d,\mathcal{B}[\mathbb{R}^d],m)$ I know that the completion of a measure space $(X,\mathcal{B},\mu)$ is the measure space $(X,\overline{\mathcal{B}},\overline{\mu})$ where $\overline{\mathcal{B}}=\{B\cup N |B \in \mathcal{B} \land N \in \mathcal{N}  \}$ , where $\mathcal{N}$ is the collection of subsets of the null sets of $\mathcal{B}$ , and $\overline{\mu}:B\cup N \mapsto \mu(B)$ . Claim: $\mathcal{L}[\mathbb{R}^d] \subseteq \{B \cup N | B \in \mathcal{B}[\mathbb{R}^d] \land N \in \mathcal{N}\}$ , where $\mathcal{N}:$ Borel subnull-sets Proof attempt: Part 1: We show that any $S \in \mathcal{L}[\mathbb{R}^d]$ is of the form $B \cup N$ , where $B$ is a Borel set and $N$ is a (Lebsegue) null set. Let $S \in \mathcal{L}[\mathbb{R}^d]$ . Suppose that $m(S)=\delta > 0$ . Then there is a closed set $E \subseteq S$ such that $m(S \setminus E)\leq \delta$ . Since any closed set is a Borel-set, this proves that $\Lambda=\{U\subseteq S| U \in \mathcal{B}[\mathbb{R}^d]\}$ is nonempty. Therefore, let $B=\bigcup_{A \in \Lambda} A$ be the largest Borel set contained in $S$ . Then we also have $B \in \mathcal{L}[\mathbb{R^d}]$ , and accordingly $S \setminus B \in \mathcal{L}[\mathbb{R^d}]$ . We claim that $m(S \setminus B)=0$ . Suppose not, and that $m(S \setminus B)=\gamma >0$ . Then we can once again find a closed set $F \subseteq S \setminus B$ , contradictiong the maximality of $B$ . Therefore $m(S\setminus B)=0.$ Part 2: We show that any (Lebesgue) null set is a Borel subnull set. Let $N$ be any (Lebsegue) null set. Then, given any $n \in \mathbb{N}$ , there is an open set $U_n$ containing $N$ , such that $m(U_n \setminus N)\leq 2^{-n}$ . Let $U=\cap_{n=1}^{\infty}U_n$ . Then $U$ is an open set containing $N$ , furtheremore $m(U)\leq 2^{-n}$ for any $n$ , so $m(U)=0$ . Thus, $U$ is a (Borel) null set, showing that $N \in \mathcal{N}$ . The problem however, is in part 1 of my attempted proof, as $B$ is defined as an (possibly uncountable) union of Borel sets, so I cannot guarantee that it is even well defined. I tried to change the definition of $B$ to the union of all open sets contained in $S$ , i.e. $B:=S^\circ$ , but this makes matters worse, as $m(S)>0 $ does not imply that $ S^\circ \neq \emptyset$ . Can anyone see a way around this problem?","['measure-theory', 'lebesgue-measure', 'borel-measures']"
3470850,Coordinate change to make function linear on a neighborhood,"The following was stated over at Mathoverflow when discussing applications of the Implicit function theorem. By changing coordinates you can make a simple function appear complicated. Have you ever asked yourself if the opposite is true: given a ""complicated"" function, can I make it look simpler in a neighborhood of a point by changing the coordinates near that point? The implicit function theorem states that if that point is not a
critical point, then you can find coordinates near that point such
that, in these coordinates the function is a linear function. However, I do not quite see how this follows from the theorem I know: Let $F:\mathbb{R}^{n+m}\to\mathbb{R}^m $ be continuously differentiable, then at a point $(\mathbf{a},\mathbf{b})$ such that $F(\mathbf{a},\mathbf{b})=\mathbf{0}$ , the Jacobian $${\displaystyle J_{F,\mathbf {y} }(\mathbf {a} ,\mathbf {b} )=\left[{\frac {\partial F_{i}}{\partial y_{j}}}(\mathbf {a} ,\mathbf {b} )\right]}$$ tells us if locally near $(\mathbf{a},\mathbf{b})\in U$ an implicit function $f:U\subset \mathbb{R}^n\to\mathbb{R}^m$ exists such that $F(\mathbf{x},f(\mathbf{x}))=\mathbf{0}$ for all $x$ in the neighborhood $U$ . Additionally, it provides a formula for the Jacobian of the implicit function $f$ . Now, I cannot wrap my head around how you use this to show that you can make any multivariable function locally linear with respect to some coordinates.","['coordinate-systems', 'multivariable-calculus', 'implicit-function-theorem']"
3470886,Visualising the divergence of a vector field,"My main doubt is : ""How do you visualize finding out the divergence of a vector field?"" I will first tell you what I think can be a nice way of visualizing it. (There is obviously a mistake in my method as we get a contradiction which I will discuss  below) Step 1 : Sketch the vector field (length of the vector is proportional to its magnitude) Step 2 : Place a particle at the tail of all the vectors visible in the sketch Step 3 : Assume that the vectors indicate the velocity of the particles Step 4 : Let the particles move for one second. At the end of unit time(one second), the particles will reach the arrow head of the vector Step 5 : See whether the density has increased in the surrounding of the point, or has decreased If the density has increased, it has negative divergence and vice versa. 1-D case I hope you can understand my one dimensional (1/r^2) vector field sketch and my method of calculating the divergence at a point p More particles enter the rectangular surrounding of p than leave. 2-D case Here I have made the diagram for 2 dimensions. As we can see from the diagrams, more particles come into the neighbourhood of p. This suggests negative divergence at p and that is indeed the case. The math checks out. Now, doing the same for 3 dimensional vector field (1/r^2) gives us trouble. We know by calculating that the divergence of this field is 0, but it does not look like that according to my method of visualisation. 3-D case The dotted circular line shows the spherical neighbourhood of the point p. The solid ellipse depicts a circular intersection when sliced by a plane passing through origin . We can break the entire spherical surrounding as infinite such circular discs. Now we know the 2-D case. More points enter the disc than leave. This means more points enter the sphere than leave. This should make the divergence at p negative, but it turns out to be 0. SO MY QUESTION IS, ""WHAT IS THE MISTAKE IN MY METHOD OF VISUALISATION? IS THERE A BETTER WAY TO VISUALISE THE DIVERGENCE AT A POINT?""","['vector-fields', 'multivariable-calculus', 'divergence-operator', 'vector-analysis']"
3470910,How to fit ordinary differential equations to empirical data?,"For some biological systems, there exists ordinary or partial differential equations that allow one to simulate their activity/behavior over time. Some of these models even produce data that is very difficult to tell apart from real data. What I haven't been able to figure out is how were those equations found? Suppose I have some empirical time-series data, that has very little noise in it. How could I ""fit"" or find ODEs or PDEs that mimic them? Are there any paper and pen based methods for this? Or is this something that you would do numerically; say measure the difference between the output of a given ODE and empirical data and optimize the parameters? Thanks for any help!","['ordinary-differential-equations', 'empirical-processes', 'partial-differential-equations', 'time-series', 'optimization']"
3470926,Removing countable no. of lines from $R^3 $,"What will happen to connectedness after removing countable no. of lines from $ R^3$ . Will it be connected or will it be disconnected? I had read that removing finite number of points from $R^2$ is still connected. But I am unable to get any proof for this fact. How do I prove/disprove it? Edit Choose two points in $R^3$ , say $a, b \in \mathbb{R} ^3$ Then there are uncountably many spherical surfaces passing through those two points. Now any straight line and a sphere intersect in atmost two points. Hence there exists a sphere containing the two points but not any lines which has been deleted.
So join those two points by a continous map, which exists as spheres are connected.
Hence it is Path Connected, and hence connected. Hence proved.","['connectedness', 'general-topology', 'real-analysis']"
3470993,Matrix norms and the matrix transpose,"There are three parts to this question, and I'm not sure how they link together to provide answers. $A$ is a linear mapping from Euclidean space $X$ to Euclidean space $U$ , and the norm $\| \cdot \|$ is the Euclidean norm for matrices. (i) Show that $\|A^{T}\| = \|A\|$ . (ii) Let $v \in \mathbb{R}^{n}$ be a unit vector, and $\sigma u = Av$ , with $\sigma = \|Av\|$ . Therefore, $u\in \mathbb{R}^{m}$ is also a unit vector. Does it follow that $\sigma v = A^{T}u$ ? (iii) Now if $v$ is as above, but $\sigma = \|A\|$ . Show that $\sigma v = A^{T}u$ .","['transpose', 'linear-algebra', 'matrix-norms']"
3471005,Alice and Bob card game - a different version,"Alice chooses with uniform distribution two real numbers between 0 and 100 and write them down, each on a card. Then, Alice decides which card Bob will see. Bob looks at the card and then decides which of the cards he wants. If he get the higher number he wins, otherwise he loses. As not as the version in the question Alice and Bob card game , here Alice can play a strategy such that Bob won't be able to guarantee winning strictly more than half the time. The question is how Alice can do that. I tried to reflect the strategy that Bob uses in the original question, for example, decide that if both numbers are greater than 50, Bob will get the lower number to see, etc. In all these variation, the original strategy of Bob still guarantees to win strictly more than half the time. Any other ideas?","['card-games', 'probability']"
3471011,Proving monotonicity of this ratio of Hypergeometric functions,"Let $n\in\Bbb N$ , $\omega=0,1,\dots,n$ , and $\nu,z>0$ . We define $$
\tilde g_{n,\omega}(z,\nu):=\frac{z^{n-\omega}\partial_z^n z^\omega {_1F_0}(1;-;z)_\nu}{{_1F_0}(1;-;z)_\nu},
$$ where $$
{_1F_0}(1;-;z)_\nu:=\frac{1-z^{\nu+1}}{1-z}=(\nu+1)z^{\nu+1}\mathbf F(1,\nu+2,2,1-z),
$$ which is a continuous interpolation of the truncated geometric series and $\mathbf F(a,b,c,z)=F(a,b,c,z)/\Gamma(c)$ is the regularized Gauss hypergeometric function. Conjecture: Under the specified restrictions on the parameters, $|\tilde g_{n,\omega}(z,\nu)|$ is nondecreasing in $z$ and is strictly increasing in $z$ when $n-\omega-\nu\notin\Bbb N$ . I am seeking to solve this conjecture but have been unsuccessful so far and am turning to the SE math community for help in finishing the proof.  See below for what I have tried as a potential path forward.","['interpolation', 'monotone-functions', 'real-analysis', 'gamma-function', 'hypergeometric-function']"
3471018,How can I tell the expected value of a random variable looking at its density function's graph?,"There's an intuition in me that whenever I look at a graph of a random variable's graph, its expected value should be that specific $x$ that has the maximum function value. That was the case at normal distribution. But I encoutnered other distributions, like the gamma distribution, and it wasn't always the case with that. I read a Wikipedia that is has the expected value of $k\theta$ and for like $k = 2$ and $\theta=5$ case, the corresponding expected value would be $10$ , but the function has its maximum value at around $5$ . I thought the density function says how probable certain values are, although the value not displaying its probability of course, but where the function has bigger values, the probability also goes higher. For example: sampling a random variable, then looking at the sample's histogram clearly shows that more numbers were generated near points where the function had bigger values. Also, I thought the expected value names the value which is the most likely to be drawn of a random variable. So I had a general intuition that where the function has its maximum value, that's what its expected value is.","['expected-value', 'statistics', 'probability-theory', 'probability']"
3471048,Why is shear missing in Helmholtz theorem?,"Let $F_i$ be a well behaved vector field in $\mathbb{R}^3$ which rapidly vanishes at infinity. (I am using here the index notation.) By the Helmholtz theorem , knowledge of divergence $\partial_i F_i$ , and the curl $\partial_iF_j - \partial_j F_i$ of $F_i$ provides enough information to reconstruct the original vector field $F_i$ . I am wondering how is it that one can reconstruct $F_i$ only from divergence and curl, while the complete Jacobian $J_{ij} = \partial_j F_i$ corresponds additional information. Divergence is the trace of $J_{ij}$ , curl is skew-symmetric part, while the symmetric part $\partial_j F_i + \partial_i F_j$ , sometimes known as shear, does not participate in the Helmholtz theorem. I would like to intuitively understand this behavior. In the case of a scalar function $f$ , one needs to know all of its derivatives $\partial_i f$ in order to reconstruct $f$ via line integral. However, in the case of Helmholtz theorem, we are reconstructing $F_i$ as a volume integral. Does this make any difference?","['vector-fields', 'jacobian', 'multivariable-calculus', 'vector-analysis', 'partial-derivative']"
3471393,When does $A \cap C \subseteq B \cap D \Longrightarrow A \subseteq B$ and $C \subseteq D$,"Basically, we know that $A\subseteq B$ and $C\subseteq D$ implies that $A \cap C \subseteq B \cap D$ . I was wondering under which (additional) conditions (such as convexity, nonemptyness or being on a specific class of sets) can we establish the converse. Is this possible? Thanks!",['elementary-set-theory']
3471447,What's the generalization for $\zeta(2n)$?,"We know that $$\zeta(2)=\frac{\pi^2}{6}\tag1$$ $$\zeta(4)=\frac{\pi^4}{90}
\tag2$$ $$\zeta(6)=\frac{\pi^6}{945}\tag3$$ $$.$$ $$.$$ $$.$$ My question is there a generalization for $\zeta(2n)$ in term of $\pi^{2n}$ ? Thank you","['real-analysis', 'calculus', 'closed-form', 'sequences-and-series', 'riemann-zeta']"
3471477,Sard's Theorem with whatever function,"A lot of staetments of Sard's Theorem require a certain order of smoothness to the function $f$ . At least it is $C^1$ . I want to prove a more general version of this theorem which holds (clearly) only with $f : \mathbb{R}^n \to \mathbb{R}^n$ but $f$ could be literally what you want (no regularity assumption). The statement sounds like that: Let $f : \mathbb{R}^n \to \mathbb{R}^n$ and \begin{align}
Z &= \{ x \in \mathbb{R^n} | f \text{ is differentiable at $x$ and }  \det(J f(x)) =0 \}
\end{align} Then $\mathcal{L}^n (f(Z))=0$ Here $\mathcal{L}^n$ is the Lebesgue measure. I proved the claim for $n=1$ . I'm reporting it here: Firstly assume wlog $f :[a,b] \to \mathbb{R}$ . Let $x \in Z$ , then by definition of differentiability, $\forall \epsilon >0$ there exists $r_{\epsilon,x}>0$ such that $\forall y \in \mathbb{R}^n$ with $0 < \vert y-x\vert<r_{\epsilon,x}$ , then $\Big\rvert \dfrac{f(y)-f(x)}{y-x} -0\Big\lvert < \epsilon$ . Thus $\vert f(x) - f(y) \vert < \epsilon |y-x| < r \epsilon $ for all $r \in (0,r_{\epsilon,x}) \qquad (1)$ . Let me call $I_{x,r} := [x-r,x+r] $ and $A_M:=f(Z) \cap [-M,M]$ . I have $f(Z) = \bigcup_{M=1}^{\infty}A_M$ and $\mathcal{L}(f(I_{x,r}))< 2r\epsilon$ by $(1)$ . Claim: $\mathcal{L}(A_M)=0$ I need Vitali covering Lemma Let $A \subset \mathbb{R}^n$ bounded and $\mathcal{B}$ a fine covering of $A$ (i.e. a family of balls such that for all $x \in A$ , then $\inf \{r>0 | B(x,r) \in \mathcal{B} \} = 0$ ). Then there exists a subfamily $\mathcal{B}' \subset \mathcal{B}$ of disjoint balls (at most countable) such that $\mathcal{L}^n(A) < \mathcal{L}^n(\bigcup_{B \in \mathcal{B}'}B) + \epsilon$ Fix $\epsilon >0$ . I choose $\mathcal{B} = \{[f(x) - r \epsilon,f(x)+r\epsilon] | x \in Z, r \in (0,r_{\epsilon,x}) \text{ such that  if } y \in [x-r,x+r] \text{ then } \vert f(y)-f(x) \vert < r \epsilon \}$ Clearly this is a fine covering, hence, by Vitali, there exists a subfamily of disjoint intervals $I_i:= I_{f(x),\epsilon r_i}$ such that $\mathcal{L}(A_M) < \mathcal{L}(\bigcup I_i)  + \bar{\epsilon}$ . Now, I take $\epsilon  = \bar{\epsilon} = \dfrac{1}{100} \dfrac{\mathcal{L}(A_M)}{(1+b-a)}$ . I observe that $\mathcal{L}(\bigcup I_i) \leq 2 \sum \epsilon r_i \leq \epsilon (b-a)$ . We put all these stuffs together and we get $\mathcal{L}(A_M) < \dfrac{1}{100} \mathcal{L}(A_M)$ which implies $\mathcal{L}(A_M)=0$ . Finally $\mathcal{L}(f(Z)) \leq \sum \mathcal{L}(A_M)=0$ NOW, I want to simulate this proof for the cases $n \geq 2$ but the main isssue is that I have more than one direction to control. My idea is to prove that for every $\epsilon >0$ and every $x \in Z$ there exists $r_{\epsilon,x} >0$ such that $f(B(x,r))$ is contained in a parallelepiped with all sides controlled by $r$ and one side controlled by $\epsilon r$ for every $r \in (0, r_{\epsilon,x})$ . Then I will use Vitali and finally discover $\mathcal{L}^n(f(B(x,r)) < c \epsilon r^n$ . Wlog I can suppose that the first column of Jacobian matrix is full of zeros and this leads  me to prove that along $x_1$ -axis my image is controlled by $\epsilon r$ , but what about the other axis? Can you give me some hints or references?","['lebesgue-measure', 'derivatives', 'real-analysis']"
3471633,A combinatorial interpretation for $n$-ary trees for negative $n$,"The ordinary generating function $T_n=T_n(x)$ for the $n$ -ary trees satisfies the functional equation $$T_n=1+xT_n^n.$$ This usually applies for $n\ge 0$ , but the functional equation can be extended to negative $n$ . Writing $$T_{-n}=1+xT_{-n}^{-n},$$ we obtain, by dividing through by $T_{-n}$ , that $$T_{-n}^{-1}=1-x(T_{-n}^{-1})^{n+1},$$ i.e. $$T_{-n}(x)=\frac{1}{T_{n+1}(-x)}.$$ What would be a natural way to interpret this combinatorially? I.e. what are "" $n$ -ary trees"" for negative $n$ , why do we get the extra 1 degree, etc.","['combinatorics', 'combinatorial-proofs', 'reference-request']"
3471711,General solution of $x^{\prime}(t)=A x(t)$ [repeated real/complex eigenvalues case],"Could anybody give me the general solution of autonomous linear systems $X^{\prime}(t)=A X(t)$ in the case where there are repeated real eigenvalues and some complex repeated complex eigenvalues? The case where they are all distinct is done by the following theorem but in the case where the eigenvalues are repeated, I didn't find something that could make me satisfied. Theorem: Consider the system $X^{\prime}=A X$ where $A$ has distinct eigenvalues $\lambda_{1}, \ldots, \lambda_{k_{1}} \in \mathbb{R}$ and $\alpha_{1}+i \beta_{1}, \ldots, \alpha_{k_{2}}+i \beta_{k_{2}} \in \mathbb{C} .$ Let $T$ be the matrix that puts
A in the canonical form $T^{-1} A T=\left(\begin{array}{cccccc}{\lambda_{1}} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{k_{1}}} & {} \\ {} & {} & {} & {B_{1}} \\ {} & {} & {} & {} & {\ddots} \\ {} & {} & {} & {} & {} & {B_{k_{2}}}\end{array}\right)$ where $$
B_{j}=\left(\begin{array}{cc}{\alpha_{j}} & {\beta_{j}} \\ {-\beta_{j}} & {\alpha_{j}}\end{array}\right)
$$ Then the general solution of $X^{\prime}=A X$ is TY (t) where $Y(t)=\left(\begin{array}{c}{c_{1} e^{\lambda_{1} t}} \\ {\vdots} \\ {\vdots} \\ {a_{1} e^{\alpha_{1} t} \cos \beta_{1} t+b_{1} e^{\alpha_{1} t} \sin \beta_{1} t} \\ {-a_{1} e^{\alpha_{1} t} \sin \beta_{1} t+b_{1} e^{\alpha_{1} t} \cos \beta_{1} t} \\ {\vdots} \\ {a_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t} \\ {-a_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t}\end{array}\right)$ Thanks for your help","['matrix-exponential', 'linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
3471843,Show that every covering map is a local homeomorphism.,"We know the fact that If $p:E\longrightarrow B$ is a covering map, then $p$ is a local homeomorphism. However, it is hard to find the proof of this statement. I only found one proof here: https://topospaces.subwiki.org/wiki/Covering_map_implies_local_homeomorphism , but it seems that it uses a really weird definition of covering map. I tried to prove this statement but I got stuck. Below is my attempt: Let $e\in E$ and set $x=p(e)\in B$ . Since $p$ is a covering map, we can choose a neighborhood $U$ of $x$ that is evenly covered by $p$ . Let $(V_{\alpha})$ be a partition of $p^{-1}(U)$ into slices, that is $p^{-1}(U)$ is a disjoint union of $V_{\alpha}$ and $p|_{V_{\alpha}}:V_{\alpha}\longrightarrow U$ is a homeomorphism onto $U$ for each $\alpha$ . Then, how could I argue that $e$ must be in one of $V_{\alpha}$ ? If this was true, then it follow immediately since then the point $e$ has a neighborhood $V_{\alpha}$ that is mapped homomorphically by $p$ on to an open subset $U$ of $B$ . Thank you!","['general-topology', 'algebraic-topology', 'covering-spaces']"
3471858,is there a real solution to this differential equation?,"This is the equation: $\frac {d^4y}{dx^4}=-y$ I know the solution to this equation: $\frac {d^2y}{dx^2}=-y$ is this: $y(x)=a\cdot \cos(x)+b\cdot \sin(x)$ and it involves $e^{ix}$ , but I wondered if there was any solution to a higher-order version of this.",['ordinary-differential-equations']
3471869,Sum of two integrals is zero,"this sum is said to be equal to $0$ : $\int_{\mathbb{R}}\int_{-\infty}^x \frac{h(x)h(t)}{t-x}dt dx +\int_{\mathbb{R}}\int_{x}^{\infty} \frac{h(x)h(t)}{t-x}dt dx$ and I'm trying to understand why. As reason I got said ""because of reflection"" but I still don't see it. So I tried something: $\int_{\mathbb{R}}\int_{-\infty}^x \frac{h(x)h(t)}{t-x}dt dx +\int_{\mathbb{R}}\int_{x}^{\infty} \frac{h(x)h(t)}{t-x}dt dx$ $=\int_{\mathbb{R}}\int_{-\infty}^x \frac{h(x)h(t)}{t-x}dt dx -\int_{\mathbb{R}}\int_{x}^{\infty} \frac{h(x)h(t)}{x-t}dt dx$ where I changed the sign of the second integral and therefore, I swapped $x$ and $t$ in the denominator. $=\int_{\mathbb{R}}\int_{-\infty}^x \frac{h(x)h(t)}{t-x}dt dx +\int_{\mathbb{R}}\int_{\infty}^{x} \frac{h(x)h(t)}{x-t}dt dx$ where I swapped the integrationbounds of the second integral and therefore, the sign changes again. $=\int_{\mathbb{R}}\int_{-\infty}^x \frac{h(x)h(t)}{t-x}dt dx -\int_{\mathbb{R}}\int_{\infty}^{x} \frac{h(x)h(t)}{t-x}dt dx$ where I swapped $t$ and $x$ in the denominator of the second integral back again and therefore, the sign of the second integral is negative again. Did I do a mistake? I don't see why this should be equal to $0$ , since in the second integral we have $\infty$ and $x$ as integrationbounds and in the first $-\infty$ and $x$ or should it also be $-\infty$ in the second integral? Really thankful for any help!!","['integration', 'reflection']"
3471882,Some applications of Gronwall's inequality.,"I've been trying to solve this problem: Problem: Lets consider the equation $\dot x = f(x) + g(x)$ , lets suppose that $|f(x)|<1$ and that are some $\epsilon >0$ and $L>0$ such that $|g(x)|\leq \epsilon$ . Lets suppose too, that $|f(x)-f(y)|\leq L|x-y|$ . If $x_{1}$ and $x_{2}$ are two solutions s.t $x_{1}(0) = x_{2}(0)$ , prove that $|x_{1}(t)-x_{2}(t)| \leq \frac{2\epsilon}{L}(e^{Lt}-1)$ . What I've done by now: Well, $$x_{1}(t) = x_{1}(0) + \int_{0}^{t} f(x_{1}(s))+g(x_{1}(s))ds$$ $$x_{2}(t) = x_{2}(0) + \int_{0}^{t} f(x_{2}(s))+g(x_{2}(s))ds$$ Then $x_{1}(t)-x_{2}(t)\leq x_{1}(0)-x_{2}(0) + \int_{0}^{t} f(x_{1}(s))-f(x_{2}(s))+g(x_{1}(s))-g(x_{2}(s))ds$ , and finally, $$|x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} |f(x_{1}(s))-f(x_{2}(s))|+ \int_{0}^{t} |g(x_{1}(s))-g(x_{2}(s))|ds$$ $$|x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} L|x_{1}(s)-x_{2}(s)|ds+ 2\epsilon tds$$ And since $2\epsilon t$ is non-decreasing we get to $$|x_{1}(t)-x_{2}(t)| \leq 2\epsilon t e^{Lt}$$ But I couldnt get a better bound :( Any help is really appreciated, thanks so much :)","['calculus', 'ordinary-differential-equations']"
3471886,Quotient map preserves deformation retraction,"For the statement: Suppose the subspace $A$ is a deformation retract of the space $X$ , and the quotient map $q: X \rightarrow Y$ is given. Then, $q(A)$ is deformation retract of the space $Y$ . My attempt starts with defining a continuous function $H$ on $X \times I$ to $X$ satisfying $$H(x, 0)=x,\  H(x, 1) \in A,\ H(a, t)=a $$ for any $x \in X, a \in A$ , and $t\in I$ . Since the composite $q\circ H$ is continuous, and $q: X \rightarrow Y$ is a quotient map, using the universal property, I want to define a continuous map $G: Y \times I \rightarrow Y$ that gives a deformation retraction between $Y$ and $q(A)$ . However, $q \circ H$ is not constant on $q^{-1} (z)$ for each $z \in Y$ , because the pre-image of $z$ could lie in $X-A$ . Is this approach right to solve the problem?","['general-topology', 'algebraic-topology']"
3471923,"Proving John Lee's ISM proposition 5.47. For a smooth real function, each regular sublevel set is a regular domain.","I have a hard time proving the following proposition from John Lee's Introduction to Smooth Manifolds. I have found this question : If $b$ is a regular value of $f$, $f^{-1}(-\infty,b]$ is a regular domain? but I cannot understand the answer. I think I need to show that $f^{-1}(-\infty, b]$ satiesfies the local $m$ -slice condition for submanifolds with boundary, where $m= \dim M$ . Since $f^{-1}(-\infty,b)$ , as an open subset of $M$ , is an embedded submanifold, it must satisfy the local $m$ -slice condition. The problem is $f^{-1}(b)$ , which I predict satisfies the local half-slice condition. My attempt so far has been : since for all $p \in f^{-1}(b)$ , $df_p$ is surjective, by Theorem 4.1 of the text, $p$ has a neighborhood $U$ such that $f|U$ is a smooth submersion. Then by the rank theorem, for $p$ , there exist smooth charts $(W, \phi)$ for $U$ centered at $p$ and $(V, \psi)$ for $\mathbb{R}$ centered at $f(p)=b$ such that $f(W) \subset V$ , in which $f$ has a coordinate representation of the form $f(x^1, \dots, x^m) = x^m$ . How can I use this to show that $p$ is contained in the domain of a smooth chart $(A, \varphi =(x^i))$ such that $f^{-1}(b) \cap A$ is a $m$ -dimensional half-slice, i.e. $\{(x^1, \dots, x^m) \in \varphi(A): x^m \ge 0\}$ ?","['manifolds', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3471930,What is the probability that two hyperspheres of radii $x$ overlap within a bigger hypersphere?,"Having a sphere of dimension $N = 5$ , its volume can be calculated as $$\frac{8}{15}\pi^2R^5.$$ If two smaller spheres of radii $x$ are dropped randomly inside of the bigger sphere, how can we find the probability of both spheres overlapping? Can someone point me towards a direction? I am trying to understand the probability of different sizes of smaller spheres.","['geometric-probability', 'geometry', 'probability']"
3471956,"Number of permutations of $\{1,2,\dots,n\}$ so that there are no $i<j<k$ with $\pi(j)<\pi(i)<\pi(k)$.","What is the number of permutations $\pi$ of $\{1,\ldots,n\}$ so that there is no triple $i<j<k$ with $\pi(j)<\pi(i)<\pi(k)$ ? My understanding: We have triple $i\ge j\ge k$ such that $\pi(j)\ge\pi(i)\ge\pi(k)$ . Then, the number of such permutations is equivalent to the number of monotone mappings. $$
\varphi: \{1,2,\ldots,n\}\rightarrow \{1,2,\ldots,n\},~1 \le\varphi(i)\le i.
$$ But I don't know how it leads to $\frac{1}{n+1}{2n\choose n}$ .","['permutations', 'catalan-numbers', 'combinatorics']"
3472003,Control of fusion of p-elements in a subgroup containing a Sylow p-subgroup,"I need help on the following problem - my first attempt came to a dead end and I'm not sure where else to go. First, a definition: Definition (control of fusion) : Suppose $P \le H \le G$ . Then $H$ controls fusion in $P$ with respect to $G$ if whenever two elements of $P$ are conjugate in $G$ then they are already conjugate in $H$ . Another way to phrase this: whenever $x,g \in G$ satisfy $x,x^g \in P$ then $g=ch$ for some $c \in C_G(x)$ and $h \in H$ . Problem : Let $H \le G$ . Suppose that $H$ contains a Sylow $p$ -subgroup $P$ of $G$ and that $H$ controls fusion in $P$ with respect to $G$ . Then any two $p$ -elements of $H$ that are conjugate in $G$ are already conjugate in $H$ . Idea for proof : Since $H$ controls fusion in $P$ with respect to $G$ , this is already true for $p$ -elements in $P$ . So, consider $s \in H \setminus P$ where $s$ is a $p$ -element and let $g \in G$ be an element such that $s,s^g \in H$ . Since all Sylow $p$ -subgroups of $G$ are conjugate, there exists some $k \in G$ such that $s^k \in P$ and some $\ell$ such that $(s^g)^\ell \in P$ . Now I want to send both $s$ and $s^g$ into $P$ where we know $H$ controls fusion, use the second version of the definition above to rewrite that element as a product of an element of $C_G(s)$ and $H$ , then conjugate again to pull them back into $H \setminus P$ and show that this factorization still holds there. The problem is that there isn't necessarily one element that conjugates $s$ and $s^g$ into $P$ . Can this outline of an idea be made to work, and if not what other path can be taken toward a proof?","['group-theory', 'abstract-algebra', 'finite-groups']"
3472017,Find a fundamental set of solutions for $t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0$.,Find a fundamental set of solutions for $t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0$ . I haven't learned any techniques on how to solve an $n$ th order DE with nonconstant coefficients besides the Euler-Cauchy equation. Any ideas will be helpful!,['ordinary-differential-equations']
3472042,Finite groups $A$ and $B$. Does existence of surjective group homomorphism $f:A\to B$ imply existence of an injective homomorphism $g:B\to A$?,"For finite groups $A$ and $B$ . Does existence of a surjective group homomorphism $f:A\to B$ imply the existence of an injective group homomorphism $g:B\to A$ ? I know that since $f$ is surjective we have $\frac{A}{\ker(f)}\cong B$ If I take an element $b\in B$ , I know $b=f(a)$ for some $a\in A$ . I want to send $f(a)\mapsto a$ , but of course there are multiple options since the pre-image of $f(a)$ may be larger than just the set $\{a\}$ . Is there a more natural way to find an injection? or is the claim false? Thanks","['group-theory', 'abstract-algebra']"
3472106,Relating the order of the class of an ideal in $\operatorname{Cl}_K$ to the class number of a finite extension $L$,"Let $K$ be a number field and let $\mathfrak{a}$ be a non-zero ideal of $\mathcal{O}_K$ such that $\mathfrak{a}^m = (a)$ for some $a \in \mathcal{O}_K$ . Does there exist a finite extension $L/K$ such that $\mathfrak{a}\mathcal{O}_L$ is principal? The answer is yes; one can take $L = K(\sqrt[m]{a})$ (or indeed the Hilbert class field of $K$ ). Consider instead the induced homomorphism $\varphi: \operatorname{Cl}_K \to \operatorname{Cl}_L$ . Then the image of $[\mathfrak{a}^m]$ is obviously $[\mathcal{O}_L]$ under $\varphi$ . If, however, $(m , h_L) = 1$ then this guarantees that $\varphi([\mathfrak{a}]) = [\mathcal{O}_L]$ , motivating the question Given a number field $K$ and a positive integer $m$ , is there a finite extension $L/K$ such that $(m, h_L) = 1$ and $h_L \neq 1$ ? Obviously if I pick $m$ as the order of the class of an ideal in $\mathcal{O}_K$ then this answers the original question in a cool way, but I don't expect that I'd be able to prove this with the tools I have (if it's not a stupid question).","['number-theory', 'abstract-algebra', 'algebraic-number-theory']"
3472141,Correlation vs Derivative,"Is a statistical correlation and derivative (in calculus) the same thing? I ask because a teacher gave an example in which the derivative between happiness and coffee was positive. In other words, as coffee consumption goes up so should happiness. This sounds like a correlation but I've never heard anyone mention the connection between statistics and calculus.","['calculus', 'statistics']"
3472151,Two definitions of the half-derivative of $e^x$ don't match. Who's right?,"I find two main sources on how to compute the half-derivative of $e^x$ . Both make sense to me, but they give different answers. Firstly, people argue, that $$\begin{align}
\frac{\mathrm{d}}{\mathrm{d} x} e^{k x} &= k e^{k x} \\[4pt]
\frac{\mathrm{d}^2}{\mathrm{d} x^2} e^{k x} &= k^2 e^{k x} \\[4pt]
\frac{\mathrm{d}^n}{\mathrm{d} x^n} e^{k x} &= k^n e^{k x}
\end{align}$$ Therefore, it seems very reasonable, that $$\frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \sqrt{k} e^{k x}$$ But this is not what the usual formula gives $$
\frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \int \limits_0^x \mathrm{d} t \frac{e^{k t}}{\sqrt{x-t}} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} e^{k x} \int \limits_0^x \mathrm{d} u \frac{e^{- k u}}{\sqrt{u}} = \\
= \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \frac{e^{k x}}{\sqrt{k}} \int \limits_0^{\sqrt{k x}} \mathrm{d} s \, e^{-s^2}
$$ We can already see that this is not equal to $\sqrt{k} e^{k x}$ . So who is right? Why do we use the latter formula in almost all cases but somehow we settle for the simpler formula when it comes to the exponential? Note that both satisfy that if we apply the half-derivative twice, we get the usual first derivative. (In the first case, it's simply because $\sqrt{k} \sqrt{k} = k$ ; in the second case, there's a proof on Wiki using the properties of beta and gamma functions - plus I verified this numerically even though I couldn't express the integrals in a closed-form.) I also have hard time accepting this second, complicated, formula, mainly because any integer derivative of the exponential gives exponential, but for the half-derivative we get this weird monstrosity. On the other hand, it should be consistent with the formulas for the half-derivative for all powers of $x$ when it's put together in the infinite series for $e^{k x}$ . Can anyone shed some light on this issue for me, please?","['derivatives', 'fractional-calculus']"
3472158,Show that $2(\sin y + 1)(\cos y + 1) = (\sin y + \cos y + 1)^2$,The question states: Show that: $$2(\sin y + 1)(\cos y + 1) = (\sin y + \cos y + 1)^2$$ This is what I have done $2(\sin y + 1)(\cos y + 1) = 2(\sin y + \cos y + 1)^2$ L. H. S.                = R. H. S. From L. H. S. $2(\sin y +1)(\cos y + 1) = 2(\sin y\cos y + \sin y + \cos y + 1)$ $= 2(\sin y\cos y + \sin y + \cos y + \sin^2 y + \cos^2 y)  (\sin^2 y + \cos^2 y = 1)$ $= 2(\sin^2 y + \sin y\cos y + \sin y + \cos^2 y + \cos y)$ I got stuck here. I do not know what to do from here. I have tried and tried several days even contacted friends but all to no avail.,['trigonometry']
3472231,How to detailed derivation of multiplicative update rules for Nonnegative Matrix Factorization?,"How to detailed derivation of multiplicative update rules for Nonnegative Matrix Factorization? Minimize $\left \| V - WH \right \|^2$ with respect to $W$ and $H$ , subject to the constraints $W,H \geq 0.$ The multiplicative update rules are as follows: \begin{equation}
W_{i,j} \leftarrow W_{ij}   \frac{(VH^T)_{ij}}{(WHH^T)_{ij}}
\end{equation} \begin{equation}
H_{i,j} \leftarrow H_{ij}  \frac{(W^TV)_{ij}}{(W^TWH)_{ij}}
\end{equation} the Lagrange $\mathcal{L}$ is: $$\mathcal{L}(W,H) =\left \| V - WH \right \|^2-Tr(\Psi W^T)-Tr(\Phi H^T)$$ The derivatives with respect to H can computed similarly. Thus, $$\nabla_W f(W,H) = -2VH^T + 2WHH^T-\Psi$$ $$\nabla_H f(W,H) = -2W^TV + 2W^TWH- \Phi$$ According to KKT conditions, $\Psi_{ij}W_{ij}=0$ and $\Phi_{ij}H_{ij}=0$ : $$(-2VH^T + 2WHH^T)\circ W=0$$ $$(-2W^TV + 2W^TWH)\circ H=0$$ The question is: Why the results are as follows: \begin{equation}
W_{i,j} \leftarrow W_{ij}   \frac{(VH^T)_{ij}}{(WHH^T)_{ij}}
\end{equation} \begin{equation}
H_{i,j} \leftarrow H_{ij}  \frac{(W^TV)_{ij}}{(W^TWH)_{ij}}
\end{equation} Not are as follows: \begin{equation}
W_{i,j} \leftarrow W_{ij}   \frac{(WHH^T)_{ij} }{(VH^T)_{ij}}
\end{equation} \begin{equation}
H_{i,j} \leftarrow H_{ij}  \frac{(W^TWH)_{ij}}{(W^TV)_{ij}}
\end{equation} In addition, Why should the learning rate be set like as follows: $\eta_W = \frac{W}{WHH^T}$ and $\eta_H = \frac{H}{W^TWH}$ Click here , you can see this paper.","['real-analysis', 'matrices', 'matrix-calculus', 'optimization', 'derivatives']"
3472235,Inverse of any element in a group of matrices [duplicate],"This question already has an answer here : Prove that matrices of the form $\begin{pmatrix} x & x \\ x & x \end{pmatrix}$ are a group under matrix multiplication. (1 answer) Closed 4 years ago . I came across the following question in an under graduate course examination: Let $G$ be a group of all matrices of the form $  \pmatrix{x&x\\x&x}$ , where $0\neq x\in \Bbb R$ under matrix multiplication. Find the inverse of any element in $G$ . It appears to me that the question is wrongly framed because any matrix of the given form doesn't have inverse due to the fact that its determinant is zero.","['matrices', 'group-theory']"
3472247,"How to find a solution ""by inspection"" w.r.t. the method of reduction of order for homogeneous linear ODE's?","I'm currently studying ODE's and ran into an example problem that I'm having trouble understanding. The problem is from a section dealing with the method of reduction of order and is as follows: Find a basis of solutions for the ODE $$(x^2 - x)y'' - xy' + y = 0$$ I'm aware that reduction of order applies when we know one solution $y_1$ and we find the other by putting $y_2 = uy_1$ where $y_i$ and $u$ are both functions of $x$ . The solution for this ODE simply states that ""by inspection"" we can see that $y_1 = x$ is one solution. Indeed, if we put $y_2 = uy_1 = ux$ then we can solve the ODE, but I'm puzzled as to how one is to find out that $y_1 = x$ is supposedly such an obvious solution? Is there some sort of intuition that I should be accustomed to or are there actual methods? Thanks in advance.",['ordinary-differential-equations']
3472250,"Prove that the midpoints of $A$, $B$, $C$ and the orthocenters of $\triangle HKA$, $\triangle HKB$, $\triangle HKC$ are collinear.","$H$ and $K$ are respectively the orthocenter of and a point inside $\triangle ABC$ $(H \not\equiv K)$ . $M$ , $N$ and $P$ are respectively the orthocenters of $\triangle HKA$ , $\triangle HKB$ and $\triangle HKC$ . $D$ , $E$ and $F$ are respectively the midpoints of $AM$ , $BN$ and $CP$ . Prove that $D$ , $E$ and $F$ are collinear. It can be observed straight away that $AM \parallel BN \parallel CP$ $($ since they are all perpendicular to $HK)$ . It can also be deduced that $KM \parallel BC$ , $KN \parallel CA$ and $KP \parallel AB$ . Also, if I let $A'$ , $B'$ and $C'$ respectively be the midpoint of $AH$ , $BH$ and $CH$ then $(A'B'C')$ is the Euler circle of $\triangle ABC$ . And I'm thinking of a transformation that could do $(A'B'C') \longleftrightarrow \overline{D, E, F}$ potentially.",['geometry']
3472301,Determine the Maximum Value,"Given $ n $ positive integer determine the maximum value of $ x ^ 3_1
 + x ^ 3_2 + ... + x ^ 3_n $ for $ x_j $ , with $ 1 \leq j \leq n $ , real numbers satisfying $ x_1 + x_2 +. ... + x_n = 0 $ and $ x ^ 2_1 +
 x ^ 2_2 + ... + x ^ 2_n = 1 $ I think it works by using Lagrange
 multipliers. But I tried to use sums of Newton: $ p (x) = (x-x_1) \cdots (x-x_n) $ Then there are things like Vieta like $ x_1 + x_2 + \cdots + x_n $ etc. But their product appears two by two, three by thre.But I can't keep going","['algebra-precalculus', 'roots', 'polynomials']"
3472345,The calculation does not use complex variable function theory $\int _0^{2\pi }e^{\cos \left(x\right)}\cos \left(\sin \left(x\right)\right)dx$,Problem: calculate $\int _0^{2\pi }e^{\cos \left(x\right)}\cos \left(\sin \left(x\right)\right)dx$ This is a problem which post in How to evaluate $\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$? I wish to find a calculation that does not use complex function theory. I want to prove the most elementary way. Please watch it for me. I sincerely thank you.,"['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3472385,"For positive $a$, $b$, $c$, show $\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}}$","Prove that for every three positive numbers $a, b, c$ : $$\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}}$$ I tried using $\sum_{\mathrm{cyc}}$ but I haven't got far. I also tried: $$\sqrt[3]{abc}=\frac{1}{3}\sqrt[3]{3a \cdot 3b \cdot 3c}$$ and according to the HM-GM inequality we can replace the root by: $$\frac13 \frac{3}{\frac1{3a} +\frac1{3b}+\frac1{3c}}=\frac{1}{\frac13\left(\frac1a+\frac1b+\frac1c\right)}=\frac{3}{ \frac1a+\frac1b+\frac1c}$$ And that's it. Can you give me a hint or a solution for the question?","['contest-math', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'inequality']"
3472391,"$X_n\xrightarrow{P} X$ and $Y_n\xrightarrow{P} Y$, then $X_n Y_n\xrightarrow{P}X Y$","How to prove if $X_n\xrightarrow{P} X$ and $Y_n\xrightarrow{P} Y$ , then $X_n Y_n\xrightarrow{P}X Y$ ? Proof Original I try to finish it as below. $P(|X_nY_n-XY|>\epsilon)=P(|X_nY_n-X_nY+X_nY-XY|>\epsilon)=P(|X_n(Y_n-Y)+(X_n-X)Y|>\epsilon)=P(|(X_n-X)(Y_n-Y)+X(Y_n-Y)+(X_n-X)Y|>\epsilon)\leq P(|(X_n-X)(Y_n-Y)|>\epsilon/3)+P(|X(Y_n-Y)|>\epsilon/3)+P(|(X_n-X)Y|>\epsilon/3)$ We need to show $P(|X(Y_n-Y)|>\epsilon/3)\rightarrow 0$ as $n \rightarrow \infty$ ， that is $Y_n\rightarrow^{P} Y$ and $X$ is a random variable, we want to show $Y_n X\rightarrow^{P} YX$ I use truncate to see this part $P(|X(Y_n-Y)|>\epsilon/3)\leq P(|X|> M, |X(Y_n-Y)|>\epsilon/3)+P(|X|\geq M, |X(Y_n-Y)|>\epsilon/3) \leq P(|X|> M, |X(Y_n-Y)|>\epsilon/3)+P( |M(Y_n-Y)|>\epsilon/3) \leq P(|(Y_n-Y)|>\epsilon/(3M))+P( |M(Y_n-Y)|>\epsilon/3)\rightarrow 0$ as $n\rightarrow \infty$ The similar to show, $P(|(X_n-X)Y|>\epsilon/3)\rightarrow 0$ as $n\rightarrow \infty$ As , we have $P(|(X_n-X)(Y_n-Y)|>\epsilon/3)\leq P(|(X_n-X)|>\epsilon/3)+P(|Y_n-Y)|>\epsilon/3)$ if n is large enough, we can bound $|Y_n-Y|$ and  |X_n-X| by $1$ .  Thus we also have $P(|(X_n-X)(Y_n-Y)|>\epsilon/3) \rightarrow 0$ Thus we prove the inequality.  I only know the definition of convergence in probability. Can any one give some suggestion, either for the proof based on the basic definition or other much simpler proof. The proof according to suggestion. Since $X_n\xrightarrow{P} X$ and $Y_n\xrightarrow{P} Y$ for every subsequence $n_m$ , there is a further subsequence $n_{m_k}$ , such that $X_{n(m_k)}\xrightarrow{a.s} X$ , and $Y_{n(m_k)}\xrightarrow{a.s} Y$ ,  thus $X_{n(m_k)} Y_{n(m_k)}\xrightarrow{a.s} XY$ (still need further proof) According to the suggestions from my other question. We can show that $\mathbb{P}(X_n Y_n \not\to XY) \leq \mathbb{P}(\{X_n \not\to X\}\cup \{Y_n \not\to Y\}) \leq \mathbb{P}(X_n \not\to X) + \mathbb{P}(Y_n \not\to Y) = 0 + 0 = 0$ Thus $X_{n(m_k)} Y_{n(m_k)}\xrightarrow{a.s} XY$ Thus , for every subsequence $n_m$ , there is a further subsequence $n_{m_k}$ , such that $X_{n(m_k)}\xrightarrow{a.s} X$ ,(for the only if part of the theorem statement, we can conclude that), $X_n Y_n\xrightarrow{P} XY$ . (According to the theorem: $X_n\xrightarrow{P} X$ if and only if for every subsequence $n_m$ , there is a further subsequence $n_{m_k}$ , such that $X_{n(m_k)} \xrightarrow{a.s} X$ )","['measure-theory', 'probability-theory']"
3472411,"Find a primitive Pythagorean right triangle such that the difference of two shorter sides is 1, and every side is at least 100.","I am asked to find a primitive Pythagorean triple $(x, y, z)$ such that $x^{2}+y^{2}=z^{2}$ and $|x-y|=1$ , and $x\geq100$ and $y\geq 100$ . I know the result should be x = 119, y = 120 and z = 169, but I don't know how to ""find"" it systematically.
By the theorem, we can have $x=r^{2}-s^{2}$ $y=2rs$ $z=r^{2}+s^{2}$ I am stuck. Ps: I found this question prove that there are infinitely many primitive pythagorean triples $x,y,z$ such that $y=x+1$ But where does the hint come from?","['number-theory', 'pythagorean-triples', 'elementary-number-theory']"
3472413,How to calculate Carmichael function of a large number.,"How would I go about calculating $\lambda(49392)$ by hand? I have $\lambda$ defined in a book as $$ \lambda(n) = max{\{\bar{a} | \bar{a} \in Z_n^*\}} \ \cdot \ ord_{Z_n^{*}} (\bar{a}) $$ where $Z_n^*$ is the set of residue classes of $a$ such that $a$ and $n$ are co-prime. If I follow this correctly, then the max residue class would be $\overline{49391}$ , since $49391, 49392$ are coprime. Then I would have to multiply this by the order of $49391$ in the group $Z_m^*$ , meaning I should find  a minimal power $k$ such that $49391^k \equiv 1 \ \bmod \ 49392$ . I have found this $k$ to be $2$ . However, this is not in line with what I have found online about the definition of $\lambda$ . Elsewhere I have found that it is defined as ""the lowest power $k$ such that every (coprime of $n$ ) to the power $k \equiv 1 \ \bmod \ n$ "". In that case, I have found $k = 588$ by several online calculators, and it seems to be the case. Do you have any idea as to what I am missing? Any idea/hint appreciated.","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
3472459,What is the radial function of a Minkowski sum of a circle and an ellipse?,"Let $C$ denote the unit circle in the two-dimensional plane, centered at the origin (the blue circle in the pic). Let $E$ an ellipse whose equation is $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ . The set $C+E$ , known as the Minkowski sum of $C$ and $E$ , is defined by $$C+E=\{\vec{\alpha}+\vec{\beta}: \vec{\alpha}\in C,\ \ \vec{\beta}\in E\}$$ In the picture above, the orange oval-shaped enclosing figure is the boundary of the sum $C+E$ ,
(where for the sake of this example, $E$ is the ellipse with $a=2,b=1$ ). The green ellipse in the picture is $Q+E$ , where for the sake of example, $Q=(1/\sqrt{2},1/\sqrt{2})$ . The point $P$ represents a typical boundary point of the sum $C+E$ , which is the orange oval, which is the geometric location of all points obtained by rotating around the green ellipse with its center varying along the blue circle.
The point $O$ is the origin $(0,0)$ and the point $M$ is the orthogonal projection of $P$ onto the $x$ axis. Problem: Express the length of $\vec{OP}$ in terms of the angle POM. I can prove that if $$F(\varphi)=\arctan\left(\tan\varphi \frac{\sqrt{a^2\cos^2\varphi+b^2\sin^2\varphi}+b^2}{\sqrt{a^2\cos^2\varphi+b^2\sin^2\varphi}+a^2}\right),\quad (0\leq\varphi\leq \pi/2)$$ Then if the angle POM is $\theta$ , then the angle QOM is $F^{-1}(\theta)$ (the inverse function), and then it is not too difficult to express the $\vec{QP}$ in terms of $F^{-1}(\theta)$ and consequently the length of $\vec{OP}$ in terms of $\theta$ (and $a,b$ of course).  However, the result seems to be too complicated; I suspect there might be a simpler geometric argument that escapes me.","['convex-geometry', 'geometry']"
3472467,"On the determination of ambiguous ideal class of the extension $\mathbb{Q}(\zeta_5,\sqrt[5]{m})/\mathbb{Q}(\zeta_5))$","let $L=\mathbb{Q}(\sqrt[5]{n},\zeta_5)$ and $K=\mathbb{Q}(\zeta_5)$ the $5^{th}$ cyclotomic fields, we now that $[L:K] = 5$ and $\textrm{Gal}(L/K) = \langle \sigma \rangle$ so we call $\mathcal{A}$ an ambigous ideal class of the extension $L/K$ if and only if $\mathcal{A}^{\sigma}= \mathcal{A}$ . My question is how to prove using that $\sigma^4+\sigma^3+\sigma^2+\sigma+1 = 0$ that it exist a non trivial ambiguous ideal class??","['class-field-theory', 'number-theory', 'abstract-algebra', 'algebraic-number-theory']"
3472511,Minima of $f(x)=\frac{x^2-1}{x^2+1}$,"If $f(x)=\dfrac{x^2-1}{x^2+1}$ for every real $x$ then find the minimum value of $f$ $$
f'(x)=\frac{4x}{(x^2+1)^2}=0\implies x=0\\
f'(-0.5)<0\quad\&\quad f'(0.5)>0
$$ Seems to me like $x=0$ is a point of inflexion. But, $$
f''(x)=\frac{4(1-3x^2)}{(x^2+1)^3}\\
f''(0)>0\implies x=0\text{ is a minima}
$$ Am I making some stupid mistake here, since it doesnt make sense to me ?","['maxima-minima', 'derivatives']"
3472529,minimal average distances between $n$ nodes in a directed graph,"I have a directed graph with $n$ nodes. for any paired of nodes $A,B$ , there is a directed edge that goes in between, but it can't go both ways. i.e. $A \rightarrow B$ and $B \rightarrow A$ cannot exist at the same time. Let the distance between $A$ to $B$ be the number of edges we travel. How do we arrange the edges, so that the average distance, between all ordered pairs of $A,B$ in the graph, is minimized? This frankly seems like an simple setup that should come with some known answer, but googling couldn't give me anything useful. I found http://mathworld.wolfram.com/WienerIndex.html though, but not sure where my example would fall..","['graph-theory', 'puzzle', 'combinatorics', 'discrete-mathematics']"
3472621,"Meaning of $\frac{1}{y}dy=x\,dx$.","Consider the differential equation $\frac{dy}{dx}=xy$ , If we go on mechanically we will follow the steps below: $\frac{dy}{y}=x\,dx$ ,then integrate it to get $\ln(y)=x^2/2+C$ . Fine, but what does $\frac{dy}{y}=x\,dx$ mean. Does it make any sense. Loosely speaking one can say $dy$ and $dx$ are infinitesimal change and all that rubbish things. But I want to understand is the any meaning of writing this or it is just a notation?Also there is another doubt, how can we divide both sides by $y$ . I am looking for a rigorous understanding of these things. Can someone guide me?","['integration', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3472642,How do I define $\theta$?,"I need to calculate $\iint f(x,y) \,dx \,dy$ with the given region $D$ where $$f(x,y) = xy$$ and $$D = \{(x,y)\in \mathbb{R^2} \space | \space 0 \leq y \leq x, x^2+y^2 \leq 4 \}$$ I have used cylindrical coordinates trying to calculate this. So $f(x,y) = r \cos(\theta) r \sin(\theta)$ and because $x^2+y^2 \leq 2^2$ is a circle, I know that $ 0 \leq r \leq 2$ . Now, how do I know the values for $\theta$ ? I can't get anything out of $0 \leq y \leq x$ .","['integration', 'multivariable-calculus', 'definite-integrals', 'surfaces']"
3472653,Every polytope is orthogonal projection of some regular simplex [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Needs details or clarity Add details and clarify the problem by editing this post . Improve this question I have a statement and I am unable to prove it. Can you help me please? Prove that every polytope $P \subset \mathbb{R}^d$ is an orthogonal projection of some $k$ -dimensional regular simplex in $\mathbb{R}^n$ for suitable $k$ , $n$ . (An orthogonal projection
  is a mapping $\pi$ from the space $\mathbb{R}^n$ to a subspace $M\cong\mathbb{R}^d$ embedded in $\mathbb{R}^n$ such that for every $x\in\mathbb{R}^n$ the vector $\pi(x) − x$ is orthogonal to $M$ . A simplex
  is regular if all its edges have the same length.)","['combinatorics', 'geometry', 'discrete-mathematics']"
3472703,What are the exact limits of validity of the Abel-Plana summation formula?,"Recently, there was some discussion $[1]$ , $[2]$ on certain infinite sums, like these $$f = \sum_{k=1}^\infty \left(\frac{1}{k(k^2+1)}\right)$$ $$g = \sum_{k=1}^\infty \left( \frac{1}{k^2+1}\right)$$ $$h_0=\sum_{k=1}^\infty \frac{\sin(k)}{k}$$ $$h_1=\sum_{k=1}^\infty \frac{\sin(k^2)}{k}$$ $$h_2=\sum_{k=1}^\infty \frac{\sin(k^2)}{k^2}$$ and I wondered if the elegant summation formula of Abel and Plana could be applied to these sums. In summary, AP works fine for $f$ , $g$ , and $h_0$ but does not for $h_1$ and $h_2$ . This led me to the question in the heading. But let us start from the beginning. Quoting $[3]$ : ""In mathematics, the Abel-Plana formula (AP) is a summation formula discovered independently by Niels Henrik Abel (1823) and Giovanni Antonio Amedeo Plana (1820). It states that $$\sum _{n=0}^{\infty } f(n)=\frac{1}{2}f(0)+\int_0^{\infty } f(x) \, dx+i \int_0^{\infty } \frac{f(i y)-f(-i y)}{e^{2 \pi  y}-1} \, dy\tag{1}$$ It holds for functions f that are holomorphic in the region Re(z) >= 0, and satisfy a suitable growth condition in this region; for example it is enough to assume that |f| is bounded by $C/|z|^{1+\epsilon}$ in this region for some constants C, $\epsilon > 0$ , though the formula also holds under much weaker bounds."" Ok, the growth condition for applicability of AP, $$|f(z)| < C / |z|^{1+\epsilon}\text{, with } C>0, \epsilon >0\tag{2}$$ is fulfilled for $f$ and $g$ above, and is violated by the $h$ -sums because $\sin(z)$ is not limited in the complex plane. I have obtained the following results: AP works for $f$ , $g$ , and $h_0$ and doesn't for the remaining $h_{1,2}$ . The surprising case is $h_0$ where the growth condition $(2)$ not met but AP still gives the correct result. Examples Example $g$ . -> sucess The sum has a closed form $$g = \frac{1}{2} (\pi  \coth (\pi )-1)\simeq 1.0766740474685811741$$ Now we to AP: Here $f(n)=\frac{1}{1+(n+1)^2}$ to let the index start at $n=0$ . $f(z)$ is meromorhpic (it has two simple poles at $-1+\pm i$ ) and it goes to zero like $1/|z|^2 for $ |z|\to\infty $ so that condition $ (2)$ is fulfilled. We write the r.h.s. of $(1)$ as a sum of three parts, a, b, and c, and calculate $$a = \frac{1}{2}f(0)= \frac{1}{2}= 0.5$$ $$b = \int_0^{\infty } f(x) \, dx=\frac{\pi}{4}\simeq 0.785398$$ the integrand of the last integral is $$c_i = i \frac{f(i y)-f(-i y)}{e^{2 \pi  y}-1} = \frac{i (f(i y)-f(-i y))}{e^{2 \pi  y}-1}= \frac{4 y}{\left(e^{2 \pi  y}-1\right) \left(y^4+4\right)}$$ hence the integral is $$c = \int_0^\infty \frac{4 y}{\left(e^{2 \pi  y}-1\right) \left(y^4+4\right)}$$ The integral is convergent and the numerical value is $$c_n = 0.041275695056277776209$$ Hence $a+b+c$ is equivalent to the closed expression for $g$ .
Success! Example $h_0$ -> success $$f(n) = \frac{\sin(n+1)}{n+1}$$ The sum has the closed form $$\frac{1}{2} (\pi -1)\simeq 1.0707963267948966192$$ Condition $(2)$ is not fulfilled because for complex $z$ we have $f(z) \sim \sinh(|z|)$ . Let us nevertheless apply AP. $$a = \frac{1}{2}f(0)=\frac{\sin (1)}{2}\simeq 0.420735$$ $$b = \int_0^{\infty } f(x) \, dx=\frac{1}{2} (\pi -2 \text{Si}(1))\simeq 0.624713$$ $$c_i = i \frac{f(i y)-f(-i y)}{e^{2 \pi  y}-1} = \frac{2 (y \sin (1) \cosh (y)-\cos (1) \sinh (y))}{\left(e^{2 \pi  y}-1\right) \left(y^2+1\right)}$$ $$c = \int_0^\infty \frac{2 (y \sin (1) \cosh (y)-\cos (1) \sinh (y))}{\left(e^{2 \pi  y}-1\right) \left(y^2+1\right)}$$ The integral is convergent and the numerical value is $$c_n = 0.0253475$$ Hence we have $a+b+c \simeq 1.070796$ in good agreement with $h_0$ . 
So in spite of violating the growing condition AP gives the correct result. Example $h_1$ -> failure $$f(n) = \frac{\sin((n+1)^2)}{n+1}$$ Numerically we have $$h_1 \simeq 0.167924$$ Condition $(2)$ is not fulfilled because for complex $z$ we have $f(z) \sim \sinh(|z|^2)$ . Let us nevertheless formally apply AP. $$a = \frac{1}{2}f(0)=\frac{\sin (1)}{2}\simeq 0.420735$$ $$b = \int_0^{\infty } f(x) \, dx=\frac{1}{4} (\pi -2 \text{Si}(1))\simeq 0.312357$$ $$c_i = i \frac{f(i y)-f(-i y)}{e^{2 \pi  y}-1} = \frac{\left(-(y+i) \sin \left((y-i)^2\right)-(y-i) \sin \left((y+i)^2\right)\right) (\coth (\pi  y)-1)}{2 \left(y^2+1\right)}$$ $$c = \int_0^\infty c_i \,dy\simeq -0.0304174$$ and we have $a+b+c \simeq 0.758439693815265076542$ whic siginifantly deviates from the value of the sum $h_1$ . References $[1]$ Find Harmonic Numbers for Imaginary and Complex Values $[2]$ Convergence of $\sum_{k=1}^\infty \frac{\sin(k(k-1))}{k}$ $[3]$ https://en.wikipedia.org/wiki/Abel%E2%80%93Plana_formula","['summation-method', 'complex-integration', 'sequences-and-series']"
3472796,Find all connected covering space of $\mathbb RP^2\vee \mathbb RP^2$,"This is exercise 1.3.14 in page 80 of Hatcher's book Algebraic topology . It's equivalent to consider subgroups of $\pi_1(X_1\vee X_2)=\mathbb Z_2 * \mathbb Z_2 =\langle a \rangle *\langle b \rangle$ . To move this question out of the unanswered list, I put my solution in answer.","['general-topology', 'covering-spaces', 'algebraic-topology', 'group-theory']"
3472813,Reference: unique differentiable structure on $\mathbb{R}$?,"There have been at least two questions on MathOverflow asking about the uniqueness of differentiable structures on $\mathbb{R}^n$ ( $n \neq 4$ ). However, on neither page is an explicit reference or proof of the case I'm interested in: $n=1$ . In the comments of one of them, the following solution is given: ""one can put a Riemannian metric on any smooth $\mathbb{R}^1$ and then the exponential map from a given point defines a diffeomorphism from the standard $\mathbb{R}^1$ to the smooth $\mathbb{R}^1$ "". Unfortunately, I'm not very familiar with either Riemannian metrics or the exponential map. Does anyone have a proof, or a reference to a proof, of this result that is more elementary? Or is that as simple as it gets?","['manifolds', 'reference-request', 'smooth-manifolds', 'differential-geometry']"
3472838,"$X_n\stackrel{a.s.}\rightarrow X, Y_n \stackrel{a.s.}{\rightarrow} Y \implies X_n Y_n \stackrel{a.s.}\rightarrow XY$","How to proof $X_n\rightarrow^{a.s} X, Y_n\rightarrow^{a.s} Y$ , thus $X_n Y_n\rightarrow^{a.s} XY$ (2) The proof One: I try to show $P(\bigcup_{n=m}^{+\infty}|X_nY_n-XY|>\epsilon)=0$ as $m\rightarrow\infty$ , then we can conclude that $X_n Y_n\rightarrow^{a.s} XY$ $P(\bigcup_{n=m}^{+\infty}|X_nY_n-XY|>\epsilon)=P(\bigcup_{n=m}^{+\infty}|X_nY_n-X_nY+X_nY-XY|>\epsilon)=P(\bigcup_{n=m}^{+\infty}|X_n(Y_n-Y)+(X_n-X)Y|>\epsilon)=P(\bigcup_{n=m}^{+\infty}|(X_n-X)(Y_n-Y)+X(Y_n-Y)+(X_n-X)Y|>\epsilon)\leq P(\bigcup_{n=m}^{+\infty}|(X_n-X)(Y_n-Y)|>\epsilon/3)+P(|X(Y_n-Y)|>\epsilon/3)+P(|(X_n-X)Y|>\epsilon/3)$ We need to show $P(\bigcup_{n=m}^{+\infty}|X(Y_n-Y)|>\epsilon/3)\rightarrow 0$ as $m \rightarrow \infty$ ， that is $Y_n\rightarrow^{a.s} Y$ and $X$ is a random variable, we want to show $Y_n X\rightarrow^{a.s.} YX$ I use truncate to see this part $P(\bigcup_{n=m}^{+\infty}|X(Y_n-Y)|>\epsilon/3)\leq P(|X|> M, \bigcup_{n=m}^{+\infty}|X(Y_n-Y)|>\epsilon/3)+P(|X|\geq M, \bigcup_{n=m}^{+\infty}|X(Y_n-Y)|>\epsilon/3) \leq P(|X|> M, \bigcup_{n=m}^{+\infty}|X(Y_n-Y)|>\epsilon/3)+P(\bigcup_{n=m}^{+\infty} |M(Y_n-Y)|>\epsilon/3) \leq P(\bigcup_{n=m}^{+\infty}|(Y_n-Y)|>\epsilon/(3M))+P(\bigcup_{n=m}^{+\infty} |M(Y_n-Y)|>\epsilon/3)\rightarrow 0$ as $m\rightarrow \infty$ The similar to show, $P(\bigcup_{n=m}^{+\infty}|(X_n-X)Y|>\epsilon/3)\rightarrow 0$ as $m\rightarrow \infty$ As , we have $P(\bigcup_{n=m}^{+\infty}|(X_n-X)(Y_n-Y)|>\epsilon/3)\leq P(\bigcup_{n=m}^{+\infty}|(X_n-X)|>\epsilon/3)+P(\bigcup_{n=m}^{+\infty}|Y_n-Y)|>\epsilon/3)$ if n is large enough, we can bound $|Y_n-Y|$ and  |X_n-X| by $1$ .  Thus we also have $P(\bigcup_{n=m}^{+\infty}|(X_n-X)(Y_n-Y)|>\epsilon/3) \rightarrow 0$ as $m\rightarrow \infty$ Thus we prove the inequality.  I only know the definition of convergence in probability and almost surely. Can anyone give some suggestions, either for the proof based on the basic definition or other much simpler proof? I am a self-learner. These questions are from Chung Kai-lai's book. Thanks a lot! (2) The proof two:
The suggestion proof is very simple and clear.  It shows as below. $$\mathbb{P}(X_n Y_n \not\to XY) \leq \mathbb{P}(\{X_n \not\to X\}\cup \{Y_n \not\to Y\}) \leq \mathbb{P}(X_n \not\to X) + \mathbb{P}(Y_n \not\to Y) = 0 + 0 = 0$$ Here the first inequality follows because if $X_n(\omega) \to X(\omega)$ and $Y_n(\omega) \to Y(\omega)$ for some $\omega $ , then $X_n(\omega)Y_n(\omega) \to X(\omega) Y(\omega)$ . By contraposition, we get $$\{X_nY_n \not\to XY\} \subseteq \{X_n \not\to X\}\cup \{Y_n \not\to Y\}$$","['measure-theory', 'probability-theory']"
3472866,Are a variance and mean themselves RVs?,"Suppose that we have a RV $$X\sim N(\mu,\sigma^2).$$ Are $\sigma^2$ and $\mu$ itself another random variables ? If yes what are their mean and variance as RV ?","['normal-distribution', 'probability-theory', 'random-variables']"
3472899,"Let $T$ a compact operator in $L^2([0,1])$, show that $\lim_{n\to \infty }\sqrt{n}\|Tx^n\|_2=0$","Im stuck with this exercise: Let $T$ a compact operator in $L^2([0,1])$ , show that $\lim_{n\to \infty }\|Tf_n\|_2=0$ for $f_n(x):=\sqrt{n}x^n$ . I know that $(f_n)$ is linearly independent and that if $(e_n)$ is an orthonormal sequence then $\lim_{n\to \infty }Te_n=0$ . Thus I tried to connect the sequence $(f_n)$ with it orthonormalization using the Gram-Schmidt process but I dont find something useful. Another way would be show that $$
\lim_{n\to \infty }\langle Tf_n,v \rangle=\lim_{n\to \infty }\langle  f_n,T^*v\rangle=0
$$ for all $v\in L^2([0,1])$ . But I dont see how I can show that. Some help will be appreciated. Note: in any case I cannot use the fact that the span of $(f_n)$ is dense in $L^2([0,1])$ because this is not presented in the book where this exercise comes from.","['hilbert-spaces', 'compact-operators', 'functional-analysis']"
3472901,"Let $f(0) = -1, f(x+y) \leq -f(x)f(y)$, show that $f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0$","Let $f:\mathbb{R} \rightarrow \mathbb{R}, \\ f(0)=-1, \\ f(x+y) \leq -f(x)f(y).$ Show that $f \text{ continuous in } \mathbb{R} \iff f \text{ continuous in } 0$ . $\Rightarrow$ is trivial, as $0 \in \mathbb{R}$ . $\Leftarrow$ is pretty hard for me. You could begin with saying that for all $\varepsilon > 0$ , there is a $\delta > 0$ such that $|f(x)-f(0)|=|f(x)+1| < \varepsilon$ for all $|x| < \delta$ . But how do you go on? Or is there simply a better solution? Thanks in advance!","['continuity', 'functions', 'analysis', 'epsilon-delta']"
3472904,"Existence of $f$ such that $f(X,Y)$ independent of $Z$, where $(X,Y,Z)$ are...","Let $(X,Y,Z)$ be three random variables such that $X$ is not independent of $Y$ , $X$ is not independent of $Z$ , while $Y$ is independent of $Z$ . Is it possible to construct a function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f(X,Y)$ is independent of $Z$ , yet $f$ is different from $f(X,Y)=g(Y)$ for some $g:\mathbb{R}\to\mathbb{R}$ ? I have difficulties finding a counterexample. If the answer is positive, I'm interested in finding the largest possible class of functions/distributions for which this is not the case.","['independence', 'probability-theory', 'random-variables']"
3472929,Integral representation of Hermite polynomials,"I would like to go from the generating function, $g(x,t)$ of the Hermite polynomials $H_n(x)$ , $$ g(x,t) = e^{-t^2 + 2tx} = \sum_{n=0}^\infty H_n(x) \frac{t^n}{n!}$$ to the following representation, $$H_n(x) = \frac{n!}{2\pi i} e^{x^2} \oint  \frac{e^{-z^2}}{(z+x)^{n+1}}\mathrm{d}z $$ where the contour encloses the point $z=-x$ . I know that I am supposed to use the Cauchy integral formula, but I am having troubles. Can someone push me in the right direction?","['integration', 'complex-analysis', 'special-functions']"
3472977,Maximal subgroup of $S_n$,"Let $S_n$ denote the symmetric group on $\{1,\ldots,n\}$ . Let $M$ be the subgroup $\{\sigma \in S_n \mid \sigma(1) = 1\}$ . Show that $M$ is a maximal subgroup of $S_n$ . Here is what I've come up with so far: Suppose we have a subgroup $H$ of $S_n$ such that $M \subseteq H \subseteq S_n$ . We must show that $H = M$ or $H = S_n$ . Thus, it suffices to show that if $H$ is a subgroup of $S_n$ that contains $M$ together with at least one element of $S_n$ that is not in $M$ , then $H$ must be all of $S_n$ . To this point, suppose $H$ contains $M$ and a permutation $\beta$ such that $\beta(1) \neq 1$ . Is there a reason why $H$ must then be all of $S_n$ ? I'm hoping there's a clever way to see why that must be true, and I need some help hashing this out. Thanks!","['permutations', 'maximal-subgroup', 'abstract-algebra', 'symmetric-groups', 'group-theory']"
3472981,Is there non-trivial examples for groups $G$ satisfying $G = \mathrm{Aut}(G)$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question As the question says. Are there non-trivial groups isomorphic to their automorphisms group? (By the way i am İbrahim İpek but i lost the password of my account and also mail :/ Is there anything anyone can do?)","['automorphism-group', 'group-theory']"
3473051,If $\lim_{x\to 0} \frac{f(x)}{\sin(x)} = 2$ then find $\lim_{x\to 0} \frac{\ln(1+3x)}{f(x)}$,If $\lim_{x\to 0} \frac{f(x)}{\sin(x)} = 2$ then find $$\lim_{x\to 0} \frac{\ln(1+3x)}{f(x)}$$ I tried to solve this problem but I only got that $f(x)>\sin x$ (I hope). But I can't procceed further. Any hints?,"['limits', 'calculus']"
3473057,$ \lim\limits_{n \to \infty} \frac1n\sqrt[n]{n\cdot(n+1)\cdots(2n)}$,"It tried to solve this limit $$    \lim_{n \to \infty} \frac{\sqrt[n]{n\cdot(n+1)\cdots(2n)}}{n}$$ $   \frac{\sqrt[n]{n\cdot(n+1)\cdots(2n)}}{n} = \sqrt[n]{\frac {2n!n}{n!}} \frac{1}{n} \sim \sqrt[n]{\frac { 
 \sqrt {2 \pi  2 n} (\frac {2n}{e})^ {2n}n }{\sqrt {2 \pi   n} (\frac {n}{e})^ {n} }} \frac{1}{n} = 
\sqrt[n]{\frac { 
 \sqrt {2 } (\frac {2n}{e})^ {n}(\frac {2n}{e})^ {n}n }{ (\frac {n}{e})^ {n} }} \frac{1}{n} = 2^{\frac{1}{2n}} \frac{4}{e}n^{\frac{1}{n}} \rightarrow \frac{4}{e}$ Is it right?",['real-analysis']
3473097,Showing that there is a real $a$ such that $\frac{\sin(e^x)}{x^2+1} \leq \frac{\sin(e^a)}{a^2 + 1}$ for all real $x$,"How do we show that there exists $ a \in \mathbb{R} $ , for which every $x \in \mathbb{R}$ $$\frac{\sin(e^x)}{x^2+1} \leq \frac{\sin(e^a)}{a^2 + 1}$$ holds true? This ended up being rather complicated. There was a hint stating that there would be no need for any derivatives here.","['calculus', 'trigonometry', 'analysis']"
3473114,Married Couple Probability Question [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question There are $n$ husband and wife couples at a party. If the $n$ men and $n$ women are randomly paired with one another, what is the expected number of pairings that are actual husband-wife couples? Had this on a test earlier and cannot figure it out. Anyone have the answer? FYI, the answer choices were 1, n/5, n/2, none of the above","['combinatorics', 'probability']"
3473115,"Let $f(x)$ be a continuous function defined on $[0,1]$ with range $[0,1]$. Show that there is some $c$ in $[0,1]$ such that $f(c)=1-c$","Let $f(x)$ be a continuous function defined on $[0,1]$ with range $[0,1]$ . Show that there is some $c$ in $[0,1]$ such that $f(c)=1-c$ My attempt is as follows:- $$f(c)=1-c$$ $$f(c)-1+c=0$$ $$g(c)=f(c)-1+c$$ $g(c)$ will be continuous function in $[0,1]$ as sum of two continuous functions will also be continuous. If we can prove that there $g(c)$ will have atleast one root in $[0,1]$ , then we are done. $$g(0)=f(0)-1$$ $$g(0)\in [0,1]-1$$ $$g(0)\in[-1,0]\tag{1}$$ $$g(1)=f(1)-1+1$$ $$g(1)=f(1)$$ $$g(1)\in[0,1]\tag{2}$$ Case $1$ : If $g(0)$ or $g(1)$ goes $0$ , then we are done because then $g(c)$ will be having at least one root in $[0,1]$ Case $2$ : If $g(0)$ is negative and $g(1)$ is positive, then what we can say? By intermediate value theorem, we can say that as $g(0)<0<g(1)$ , so there will exists some c in $(0,1)$ for which $g(c)$ is zero. But I have a counter, what if $g(c)$ is like following:- Here we are getting no $c$ in $(0,1)$ for which $g(c)$ is zero.","['continuity', 'functions']"
3473120,What is the sufficient condition for the consistency and normality of MLE?,"What is the sufficient condition for the consistency and normality of MLE? I read several such regular conditions for the consistency and normality of MLE. The latest one I have read state is one the by Ferguson ""a course in the large sample"" Sufficient Condition 1 Let $X_1, X_2, \dots$ be i.i.d with density $f(x|\theta)$ (with respect t0 dv), and let $\theta_0$ denoate the true value of the parameter. If (1) $\Theta$ is an open subset of $R^k$ (2)second partial derivatives of $f(x|\theta)$ with respect to $\theta$ exist and are continuous for all x and may be passed under the integral sign in $\int f(x|\theta)dv(x)$ (3)There exists a function K(x) such that $E_{\theta_0}K(X)<\infty$ and each $\frac{d^2\theta f(x|\theta)}{d\theta^2 }$ is  bounded in absolute value by K(x) uniformly in some neigborhood of $\theta_0$ (4) $I(\theta_0)=-E_{\theta_0}\frac{d\theta f(x|\theta)}{d\theta}$ is positive definite (5) $f(x|\theta)=f(x|\theta_0)$ a.e.dv, thus $\theta=\theta_0$ Then there exists a strongly consistent sequence of $\hat{\theta}$ of roots of the likelihood equation such that $\sqrt{n}(\hat{\theta}_n-{\theta}_0)\rightarrow ^{d} N(0,I(\theta_0)^{-1})$ Can not make sure the condition (3)? The details of sufficient conditions are never stated in some elementary statistics courses. Some books may state that For consistency, some books may state that $(1)X=(X_1,X_2,\cdots,X_n)$ follows $ f(x|\theta)$ i.i.d with respect to u with some common support (2)The true parameter value $\theta_0$ is an interior point of the parameter space Similar situation also happens in asymptotic normality: They told me the MLE can be asymptotic normality or can be not? And then they just show some examples. I am not so good at real analysis and mathematics. So it is not easy for me to understand the proof in Ferguson. (Especially inconsistent part, it use Uniform Law of Large Number, and based on some semiconscious and compact assumption) Can anyone give much more simple read proof? And also a simple sufficient statement for asymptotic normality?  I know some advanced probability and measure theory. Though I am a beginner. I try to find a much clear statement for this consistency of MLE. One note from Stanford looks like that (see page 2) https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture14.pdf Sufficient Condition 2 (1)all pdfs /pmfs $f(x|\theta)$ in the model have the same support, (2) $\theta_0$ is an interior point(not one the boundary) (3)The loglikelihood $l(\theta)$ is differentiable in $\theta$ (4) $\hat{\theta}$ is the unique value of $\theta \in \Omega$ that solves the equation $0=l'(\theta)$ My question: (1) It seems that in the sencond item of first sufficient condition :second partial derivatives of $f(x|\theta)$ with respect to $\theta$ exist and are continuous for all x and may be passed under the integral sign in $\int f(x|\theta)dv(x)$ This statement seems very important. Though I never find it in Sufficient Condition 2. (2) It can be shown that Let $X_1,\cdots, X_n$ follow $U(0,\theta)$ . The MLE is $\hat{\theta}=X_{(n)}$ .
Though, $\hat{\theta}$ is asymptotic exp(1). Which items in sufficient conditions are not satisfied, both in sufficient conditions  one and two? Many thanks!","['statistical-inference', 'statistics', 'measure-theory']"
3473150,Riemann Sphere as extended complex plane,"I was reading some Complex Analysis, and came across this concept that; says you have the extended complex plane which is illustrated as the $x,y$ -plane as the complex plane, and the $z$ -plane is added, so that $(0,0,1)$ is $\infty$ . The book says, "" the hemisphere $z<0$ corresponds to the disk $|z|<1$ and the hemisphere $z>0$ to its outside $|z|>1$ . "" I don't understand why we are considering the entire sphere, when I understood that the extended complex plane was just the x-y-plane, and then we add on the point $(0,0,1)$ . Even if we considered everything in $3$ dimensions, how is it that the top of the sphere corresponds to the inside of a circle on the complex plane, and the bottom half to the outside? Thank you for your explanations/clarifications!","['complex-analysis', 'riemann-surfaces', 'riemann-sphere']"
3473153,Identifying correlation coefficients for each graph,"The figure below has six scatter diagrams for hypothetical data.   The  correlation coefficients are given alongside the figure. While the solution is given, someone can ""easily"" identify them by first by checking if they are negative or positive. then, by how the dots are spread. Since I am thinking if there is a better way since I can't identify them without looking at the solution. Wonder if anyone can give a hand, how they would approach them in a more systematical way.","['statistics', 'covariance', 'correlation']"
3473170,Flawed reasoning in proving $A\subseteq B$ iff $A-B=\emptyset$,"Suppose $A$ and $B$ are sets. Prove $A\subseteq B$ iff $A-B=\emptyset$ . Assume $A\subseteq B$ . This means that if $a\in A$ then $a\in B$ . Since $A-B$ means all $a\in A$ such that $a\not\in B$ , we can see that $A-B=\emptyset$ . Now assume $A-B=\emptyset$ . This means that there are no $a\in A$ such that $a\not\in B$ . This implies that if $a\in A$ then $a\in B$ . Therefore $A\subseteq B$ . Since we have shown that $(A\subseteq B)\Rightarrow (A-B=\emptyset )\wedge (A-B=\emptyset )\Rightarrow (A\subseteq B)$ , we can conclude that $A\subseteq B$ iff $A-B=\emptyset$ . $\blacksquare$ Though I can't pinpoint anywhere I went wrong here, I can't help but feel there's a hole in my reasoning somewhere... is this a valid proof?","['elementary-set-theory', 'proof-explanation', 'proof-writing', 'proof-verification']"
