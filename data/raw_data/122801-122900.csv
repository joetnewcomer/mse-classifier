question_id,title,body,tags
1848574,Stable resolution of a $2\times2$ linear system,"Cramer’s method for the resolution of linear systems is known to be unstable, even in the $2\times2$ case. For general systems, stability can be improved by partial or full pivoting. When you transpose the full pivoting principle to a $2\times2$ , the procedure essentially amounts to finding the L.H.S. coefficient with the largest magnitude, let it be $a_{11}$ W.L.O.G.; computing $x_2$ by determinants*, computing $x_1$ by elimination of $x_2$ from equation $1$ . Can this improve stability? Is there a more stable solution? *Whatever the choice of the pivot, the formula amounts to a ratio of $2\times2$ determinants. I wonder if first normalizing the pivot coefficient to $1$ makes any difference. $$\frac{b_1-b_2\dfrac{a_{21}}{a_{11}}}{a_{22}-a_{12}\dfrac{a_{21}}{a_{11}}}\text{ vs. }\frac{b_1a_{11}-b_2a_{21}}{a_{11}a_{22}-a_{12}a_{21}}$$","['numerical-methods', 'linear-algebra']"
1848602,local minimum is isolated,"Let $f:\mathbb{R}^2\to \mathbb{R}$, $f(x,y)=(y-x^2)(y-2x^2)$. Claim: For all $(a,b)\in\mathbb{R}^2\setminus\{(0,0)\}$ the function $\varphi(t)=f(at,bt)=b^2t^2-3a^2bt^3+2a^4t^4$ has an isolated local minimum at $t=0$. I proved that $\varphi$ has an local minimum at $t=0$ (because $\varphi'(0)=0$ and $\varphi''(0)>0$) but I don't know why this is an isolated minimum. Isolated means that there is a $\epsilon >0$ such that for all $t\neq 0$ with  $|t|<\epsilon$ $\Rightarrow$ $\varphi(0)<\varphi(t)$. But how to define $\epsilon$? Can you help me? Edit: If for example $b=0$ (see the hint below), then $\varphi(t)=2a^4t^4$ has a local minimum in $t=0$ too. But what does that have to do with ""isolated""?","['derivatives', 'real-analysis', 'calculus']"
1848644,Existence of the natural density of the strictly-increasing sequence of positive integer?,Let $A=\{a_n\}$ is a strictly-increasing sequence of positive integer. The natural density of this sequence is defined by $\delta(A)=\lim_{n\rightarrow \infty} \frac{A(n)}{n}$ whenever the limit exists and where $A(n)$ is the number of elements of $A$ not exceeding $n$. Is there a strictly-increasing sequence of positive integer $A=\{a_n\}$ such that $\delta(A)$ does not exists?,"['number-theory', 'elementary-number-theory']"
1848671,Discrete Logarithm vs Integer Factorization,"Can anyone please tell me if finding discrete logarithm is considered more difficult than integer factorization?
We have very advanced methods to find factors of large composite numbers like Number Field Sieve. Do any such advanced method exist for finding discrete logarithm modulo prime?
This argument is also linked directly with saying which one among RSA or Elgamal cryptosystems is more strong.","['number-theory', 'cryptography', 'prime-factorization', 'discrete-mathematics']"
1848696,Definition of a derivative of differential form,"While reading a paper I encountered the following: Let $(\mathbf{q,p}) \in \mathbb{R}^{2n}$ be canonical coordinates and
  let $H: \mathbb{R}^{2n} \to \mathbb{R}$ be a smooth function. The continuous-time Hamiltonian system $\mathbf{q}_t = + \nabla_p H(q, p)$ $\mathbf{p}_t = - \nabla_q H(q, p)$ preserves exactly the symplectic form $\mathbf{\omega = dp \wedge dq}$, that is $(d/dt)\omega = 0$ Here $\mathbf{p} , \mathbf{q} \in \mathbb{R}^n$ How is $(d/dt)\omega $ even defined? I have seen exterior derivatives of differential forms, but this is evidently something else. Also, how can we prove that (for the given Hamiltonian system) $(d/dt)\omega = 0$? Any help will be greatly appreciated.","['hamilton-equations', 'ordinary-differential-equations', 'differential-forms']"
1848706,"Stationary distribution of ""probabilistic geometric series"" with two alternative ratios","I have an iterative process starting at $X_0=2$. In each iteration $i=1,2,\ldots$, the value of $X_i$ is determined based on the value of $X_{i-1}$ as follows: With probability 0.5, $X_i=qX_{i-1}+1$, otherwise, $X_i=(1-q)X_{i-1}+1$, with $0<q<1$. I am interested in the stationary distribution of this process, i.e. $\lim_{i\rightarrow\infty}P(X_i=x)$. Specifically, I am interested in an explanation why this distribution seems so ""rough"" and ""repetitive"" (at least when numerically solving it, see below). Alternatively, I would be grateful for any reference to the literature on this distribution, the name of the process, or anything else to find out more. The image shows the numerically obtained distribution by running the above described algorithm n=100000 times for 10000 iterations, each, with $q=\pi/5$. Dash dotted lines are the two limits $\frac{1}{1-q}$ and $\frac{1}{q}$ corresponding to the solution of the deterministic geometric series with coefficient $q$ and $(1-q)$. The numerically obtained distribution seems not to change significantly when increasing the number of iterations. Edit: Preliminary ideas Based on the reference from Did, I managed to get some preliminary ideas. I am not completely sure if the following is correct, thus, if you find an error please tell. My process is an iterated random function:
\begin{align*}
X_{n+1}=f_{\theta_{n+1}}(X_n),\,X_0=2
\end{align*}
with $\theta_k$ iid random variables being either $1$ or $2$ with probability $1/2$, and
\begin{align*}
f_1(x)&=qx+1\\
f_2(x)&=(1-q)x+1.
\end{align*} The first thing I learned from Diaconis & Freedman (1999) is that the forward iteration
\begin{align*}
X_{n+1}=(f_{\theta_{n+1}}\circ\ldots\circ f_{\theta_{1}})(x_0)
\end{align*}
has the same distribution as the backward iteration
\begin{align*}
Y_{n+1}=(f_{\theta_{1}}\circ\ldots\circ f_{\theta_{n+1}})(x_0).
\end{align*} This is useful because the forward process is ergodic, while the backward process converges. Specifically,
\begin{align*}
Y_\infty=1+a_1+a_1a_2+\ldots = \sum_{j=0}^\infty\prod_{i=1}^{j}a_i,
\end{align*}
with $a_k$ either $q$ or $1-q$ with probability $1/2$. Denoting the first $N$ terms of $Y_\infty$ as $Z_N$, and letting $Z^q_N$ be the first $N$ terms with $a_{N-1}=q$, we obtain:
\begin{align*}
Z_N^q&=1+\ldots+\prod_{i=1}^{N-2}a_i+q\prod_{i=1}^{N-2}a_i\\
Z_N^{1-q}&=1+\ldots+\prod_{i=1}^{N-2}a_i+(1-q)\prod_{i=1}^{N-2}a_i.
\end{align*}
Then, we define $R_N^q=Z_\infty^q-Z_N^q$. By doing some calculations, we find out that
\begin{align*}
\frac{q^2}{1-q}\prod_{i=1}^{N-2}a_i\leq R_N^q\leq (1-q)\prod_{i=1}^{N-2}a_i\\
q\prod_{i=1}^{N-2}a_i\leq R_N^{1-q}\leq \frac{(1-q)^2}{q}\prod_{i=1}^{N-2}a_i,
\end{align*}
and, thus, $Z_N^q+R_N^q\leq Z_N^{1-q}+R_N^{1-q}$, meaning that an iteration of the backward iteration ""splits"" the realizations in two groups, such that each group ends up in a given interval, with the overlap between the intervals having measure zero. Furthermore, both groups have the same mass, and it also follows (I don't do all steps here) that the stationary distributions in the interior of both intervals have to be the same (except scaling of axis), and to be the same as the whole distribution (Note: I have to check this again). This again implies, if I didn't make some mistake, that every interval contains the information about the whole distribution. I guess with a few more arguments it follows that the distribution cannot be uniformly continuous, explaining its roughness.","['power-series', 'sequences-and-series', 'probability-distributions']"
1848739,A topology on the set of lines?,"Of course any set $X$ can have a topology, but are there more natural topologies, metrics or similar on the set of straight lines in $\mathbb R^2$?","['reference-request', 'real-numbers', 'geometry', 'general-topology', 'metric-spaces']"
1848792,Fundamental group of a compact space with compact universal covering space,"I have this problem for Riemannian manifold, but think that it is just a topological problem. I know that this is probably a silly question, but it is since a while that I don't study general topology and algebraic topology.. Let $X$ be a compact topological space and assume that its universal covering space $\tilde{X}$ is also compact.  How can I prove that the fundamental group of $X$ is finite? Thanks!","['general-topology', 'fundamental-groups']"
1848797,Cohomology of a group of order two with coefficients in a finite abelian group of odd order,"I am looking for an elementary proof that the cohomology groups in the title are trivial in the positive degrees. In more detain, let $G=\{1,s\}$ be a group of order two, and let $A$ be an abelian group with an action of $G$.
I need an elementary proof that if $A$ is finite of odd order, then
$H^1(G,A)=0$ and $H^2(G,A)=0$. A non-elementary proof goes as follows. Since $|G|=2$, both $H^1(G,A)$ and $H^2(G,A)$ are killed by the multiplication by $2$. Clearly they are also killed by the multiplication by $|A|$. Since $2$ and $|A|$ are coprime, these cohomology groups are killed by the multiplication by $1$, hence they both are trivial. I give an elementary formulation of the desired assertion. 
Let $A$ be an abelian group and let $s\colon A\to A$ be an automorphism such that $s^2=1$.
Set
$$ N=s+1, \qquad T=s-1. $$
Then $TS=0$ and $ST=0$,
hence
$$ \mathrm{im\ } N\subseteq \ker T\quad\text{and}\quad \mathrm{im\ } T\subseteq \ker N. $$
I need an elementary proof of the following assertion: Theorem. If $A$ is a finite abelian group of odd order , then $\ker T=\mathrm{im\ } N$ and $\ker N=\mathrm{im\ } T$. Motivation. Let $X$ be a quasiprojective variety with additional structure over ${\mathbb{C}}$.
Write $G=\mathrm{Gal}({\mathbb{C}}/{\mathbb{R}})$, then $G=\{1,s\}$, where $s$ is the complex conjugation.
Let $sX$ denote the variety with additional structure over ${\mathbb{C}}$
obtained from $X$ by the action of the complex conjugation $s$ on the coefficients of the equations defining $X$.
Assume that $sX$ is isomorphic to $X$. 
We would like to know whether $X$ admits a real form. 
Let $A=\mathrm{Aut}(X)$, and assume that $A$ is an abelian group.
Then one can construct an obstruction $\eta(X)\in H^2(G,A)$ to the existence of a real form of $X$,
see my question .
Now assume that $A$ is a finite abelian group of odd order .
Then by our theorem we have $H^2(G,A)=0$, hence $\eta(X)=0$ and therefore, $X$ admits a real form $X_{\mathbb{R}}$.
The set of isomorphism classes of such real forms is a principal homogeneous space of the abelian group $H^1(G,A)$,
see again my question .
By our theorem we have $H^1(G,A)=0$, hence this real form is unique. Motivation for an elementary proof: A potential reader of my paper comes from analysis and would prefer an elementary proof.","['algebraic-geometry', 'finite-groups', 'arithmetic-geometry', 'group-cohomology', 'group-theory']"
1848819,Find all rational numbers $\frac p q$ such that $0 < p < q$ are relatively prime and $pq=25!$,"Find all the rational numbers $\frac p q$ such that all the below three conditions are satisfied. $$0<\frac{p}q<1,$$
  $$p  \hspace{4 mm}  \text{and} \hspace{5 mm}q \hspace{5 mm}\text{are relatively prime, and}$$
  $$pq=25!$$ My try What i feel that answer should be $\sum_{r=1}^{9}\binom{9}{r}$
I have arranged the 9 primes between 1 to 25.
but i guess not correct. i still think that i am doing some silly mistake. Can you please provide me the answer ?",['number-theory']
1848838,Why does $\vec{V_1}\times\vec{V_2}\cdot \overrightarrow{M_1M_2}\neq0$ imply that the two lines with $V_1$ and $V_2$ as direction vectors are skew?,"How come that when we want to prove that two lines are skew (that is that they don't intersect nor that they are parallel) we show that $C:=\vec{V_{1}} \times \vec{V_{2}} \cdot \overrightarrow{M_{1}M_{2}}\neq0 $? I understand that the main intent is to show that the parallelepiped these vectors create has volume, but still it is somewhat vague to me. ($V_{1} , V_{2}$ direction vectors, $M_{1},M_{2}$ dots on the two lines).","['algebra-precalculus', 'vectors']"
1848840,Question about oblique co-ordinate system,"Definitions: Let the following figure show an oblique $2$ dimensional co-ordinate system, where $O$ is the origin and the parallelogram $OQRP$ is called the fundamental parallelogram. Rest of the infinite parallelograms ( those with sides parallel to $OQ$ and $OP$ ) formed are said to be based on parallelogram $OQRP$. Two points $A$ and $B$  are called equivalent if they are inside ( or on the boundary ) of two different parallelograms based on $OQRP$, and when the first parallelogram is made to coincide with the second ( the first parallelogram is moved along lines parallel to $OQ$ and $OP$  ), the two points coincide, for example in the figure the two red points are equivalent. Also the points of intersection in the figure above are called lattice points . Question: Let $R_{O}$ be a parallelogram containing the origin $O$ ( not necessarily a parallelogram based on $OQRP$ ). And let us denote $R_{P}$ to be a parallelogram congruent to $R_O$ and similarly situated about lattice point $P$. What I have to prove is that iff $R_{O}$ does not contain two equivalent points inside it, then none of the parallelograms $R_{P}^*$ coincide ( $R_P^*$ denotes the set of parallelograms around all lattice points ). What I tried: Just using the fact that a parallelogram is a convex figure, I was able to prove that if $R_{O}$ contains two equivalent points inside of it, the parallelograms $R_P^*$ will coincide. But I am unable to prove the reverse direction, that is if $R_P^*$ overlap then each of them has two equivalent points inside of them. PS: The above is one of theorems related to Minkowski's theorems regarding Farey series given in the book by Hardy and Wright ( Introduction to theory of numbers ). The concerning theorem is If $R_O$ is a parallelogram containing the origin such that its area is equal to that of the fundamental parallelogram $OQRP$ and there are no two equivalent points ( that is the parallelograms $R_P^*$ don't overlap  ) inside $R_O$ then the parallelograms $R_P^*$ cover the $2D$ space, where the terms are defined above in Definitions.","['elementary-number-theory', 'geometry']"
1848854,How many digits of the googol-th prime can we calculate (or were calculated)?,"Here , a lower and upper bound for the $n$-th prime are given. Applying the given bounds $$n(\ln(n\cdot\ln(n))-1)<p_n<n\cdot\ln(n\cdot\ln(n))$$ and the approximation $$p_n\approx n(\ln(n\cdot\ln(n))-1)+\frac{n(\ln(\ln(n))-2)}{\ln(n)}$$ we get that $p_{10^{100}}$ is somewhere between $2.346977\cdot 10^{102}$ and $2.35698\cdot 10^{102}$ and approximately $2.3471\cdot 10^{102}$ , so it
has $103$ digits. How many digits can we determine of the googol-th prime with the known methods ? It is likely that this calculation was already done. In this case, a reference would be nice. (Please present also the result, not only the link).","['number-theory', 'reference-request', 'big-numbers', 'prime-numbers']"
1848855,"A log arctan integral $\int_0^1 \log x \arctan^2 x \, dx$","Here is an integral that arose while solving another problem. Can we express $$\mathcal{J}=\int_0^1 \log x \arctan^2 x \, {\rm d}x$$ in terms of known mathematical constants or special functions? I had a couple of ideas for this one. For example begin by parts , that is: \begin{align*}
\int_{0}^{1} \log x \arctan^2 x\, {\rm d}x &= \left [ \left ( x \log x - x \right )\arctan^2 x \right ]_0^1 - \int_{0}^{1}\left ( x \log x - x \right )\arctan^2 x \, {\rm d}x \\ 
 &=-\frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x + \int_{0}^{1}x \arctan^2 x \, {\rm d}x \\ 
 &= - \frac{\pi^2}{16} - \int_{0}^{1}x \log x \arctan^2 x \, {\rm d}x -\frac{\pi}{4}+\frac{\pi^2}{16} +\frac{\log 256}{16}
\end{align*} since the RHS integral is trivial. We can even find an elementary antiderivative. The problem is with the second. An idea that pumped to me while writing down my thoughts is that probably the easiest way of computing is by trying to evaluate the integral $$\int_{0}^{1} t^{s-1} \arctan^2 x \, {\rm d}x$$ Of course $s$ is imposed on restrictions that are yet unknown to me. Playing around I see that this method is fruitless since: $$\int_{0}^{1}x^{s-1} \arctan^2 x \, {\rm d}x = \left [ \frac{x^s}{s} \arctan^2 x  \right ]_0^1  - \frac{2}{s}\int_{0}^{1} \frac{x^{s-1} \arctan x}{x^2+1} \, {\rm d}x$$ and if someone tries to apply parts again at the second integral beginning with the rational function then he encounters hypergeometrics. I don't know any of that. So, any help is welcome!","['real-analysis', 'definite-integrals']"
1848907,Constructibility of the $17$-gon,"Comment : I greatly shortened and simplified the question. As a drawback, some comments/answers might not make any sense anymore. Assume we are using this set of axioms $A$ for plane euclidean geometry and some sensible definition of the length $\overline{ab}$ between two points $a$ and $b$ . Then we can define the set $R$ to be a regular $n$ -gon iff $R = \{x_j \mid j \in \mathbb{Z}_n \}$ (has $n$ elements) $\forall k \in \mathbb{Z}_n : ~\overline{x_{k-1}x_{k}} = \overline{x_{k}x_{k+1}}$ (is equilateral) $\forall k \in \mathbb{Z}_n: \angle ~x_{k-1}x_{k}x_{k+1} = \angle~ x_{k}x_{k+1}x_{k+2} $ (is equiangular) Now imagine someone simply presented you the following construction of a $17$ -gon, with an instruction of what he did. The construction yields $17$ points of interest you collect in a set $R$ . Can you prove (or is there a known proof) by only using the Axioms of $A$ , that $R$ is a regular $17$ -gon? Comment: The linked construction is one by Herbert William Richmond which I found here , but my question would be the same for any other known construction which does the same job. The origins of the construction are of algebraic nature. Independantly of the origin, I want to know if the answer to my question is positiv, negative or not known.","['polygons', 'euclidean-geometry', 'field-theory', 'geometry']"
1848940,"If $f(1-x)-2f(x) = 1-x$, then solve for $f(\sin x-\cos x) = \frac{\sqrt{2}-4}{6}.$","If $f(1-x)-2f(x) = 1-x$, then solve for 
  $$f(\sin x-\cos x) = \frac{\sqrt{2}-4}{6}.$$ How do I do this? I'm a bit confused with notations function notations; what's the difference between $f(x), f(x+2), f(2x)$? etc... For example if $f(x)= x+1$, then $f(x+1)= (x+1)+1$, right? $f(x)$ is the base function where $x$ is the input; $f(x+1)$ is the same function machine but with $x+1$ as the input. Could you please elaborate... :D sorry for the bad phrasing...","['trigonometry', 'functions']"
1848949,How was the integral for Zeta Function created,"How was the zeta function integrated from $$\zeta(s) = \sum_{n=1}^{\infty}\frac{1}{n^{s}}$$ To $$\zeta(s) = \frac{1}{\Gamma (s)}\int_{0}^{\infty}\frac{x^{s-1}}{e^{x}-1}dx$$ I've tried googling this and surprisingly can't find much on it, even the wikipedia article for zeta function doesn't explain how this integral is derived or cite a source anywhere. I have no doubt it's true; I am just curious how it was obtained. I know very little about converting infinite sums to integrals.","['zeta-functions', 'summation', 'integration', 'riemann-zeta']"
1848959,"Constant such that $\max\left(\frac{5}{5-3c},\frac{5b}{5-3d}\right)\geq k\cdot\frac{2+3b}{5-c-2d}$","What is the greatest  constant $k>0$ such that $$\max\left(\frac{5}{5-3c},\frac{5b}{5-3d}\right)\geq k\cdot\frac{2+3b}{5-c-2d}$$ for all $0\leq b\leq 1$ and $0\leq c\leq d\leq 1$? The right-hand side looks like a weighted sum of the two terms on the left-hand side, but not quite. If we plug in $b=1$ and $c=d$, then all three terms are equal, so $k\leq 1$. On the other hand, we have $k\geq 3/5$. Indeed, we will show that $$\frac{5}{5-3c}\geq\frac35\cdot\frac{2+3b}{5-c-2d}.$$
 Note that $d\leq 1$ and $2+3b\leq 5$, so it suffices to show $$\frac{1}{5-3c}\geq\frac35\cdot\frac{1}{3-c},$$
or $$5(3-c)\geq 3(5-3c)$$
or $$15-5c\geq 15-9c$$
which is true. But the bound is not tight here, since we must have $b=d=1$ and $c=0$, and we have $\max(5, 5/2)\geq 5/3$. (The term $\frac{5b}{5-3d}$, which we did not use at all, is large.) Update : By dividing the cases into whether $b\leq 3/5$ (and compare with the first term in the $\max$ if so) or $b\geq 3/5$ (and compare with the second term in the $\max$ if so), we can show that $k\geq 15/19$. Moreover, as Aravind pointed out in the comments, we have $k\leq 15/16$. So the gap is now between $15/19$ and $15/16$. Update 2 : WolframAlpha confirms that $k=15/16$ is the right answer. The question is now how to prove it: http://www.wolframalpha.com/input/?i=find+minimum+of+max(5%2F(5-3c),(5b)%2F(5-3d)) *(5-c-2d)%2F(2%2B3b)+for+0%3C%3Db%3C%3D1+and+0%3C%3Dc%3C%3Dd%3C%3D1","['algebra-precalculus', 'inequality', 'optimization']"
1848960,Values of $a$ for Which $|||x-1|-3|-a|=k$ Has 8 Distinct Real Roots,"Question $|||x-1|-3|-a|=k$, $a \in \mathbb{N}$ has 8 distinct real roots for some $k$, then find the number of such values of $a$. My Thought Sorry I cannot show any work this time because I don't understand how to proceed. Please provide some guidance.","['algebra-precalculus', 'calculus', 'functions']"
1848980,"Showing $L(1,\chi)$ is positive given that it's nonzero","Let me first provide context for this question. There is a series of four exercises in Ireland & Rosen's book (in second edition it's exercises 14-17 in chaprer 16), aim of which is (although this is not relevant to my question) to establish that in the interval $0<x<\frac{p}{2}$ there are more quadratic residues than nonresidues $\mod p$ for $p\equiv 3\pmod 4$. The content of exercise 17, when restated, is essentially the following: Let $\chi$ be the Dirichlet character $\mod 2p$ such that $\chi(n)=\left(\frac{n}{p}\right)$ for odd $n$. Given that $L(1,\chi)\neq 0$, show $L(1,\chi)>0$. Since I am asking this question in the context of mentioned book, let me briefly go through what was established in the chapter (for the same of people who don't own the book and want to give an answer at the adequate level): Definitions of: zeta function, Dirichlet characters and L-functions was given and their basic properties were established Nonvanishing of L-functions has been proven by standard method for complex characters and for real characters using Landau's lemma (concerning Dirichlet series with nonnegative coefficients) I suppose these aren't related to my question, but just in case I mention these: Dirichlet's theorem was proven, it was shown that L-functions can be continued to the whole (punctured) complex plane, and L-functions have been evaluated at negative integers with help of generalized Bernoulli numbers. The exercise so restated seems to be quite difficult, since sign of L-function value was at no point dealt with in the chapter. At this point you can guess what my question: How to solve this exercise without using tools outside the ones provided by Ireland & Rosen in their book? I know from other places that the value of $L(1,\chi)$ for real $\chi$ is always positive. So, extending my question: Is there an easy way to show, for general real Dirichlet character $\chi$, that $L(1,\chi)>0$, given that we know it's not zero? Let me also note that I am aware of a proof of nonvanishing of L-functions which at the same time proves that the value is positive, but this wouldn't be a valid answer to my question - as far as I can recall, the proof was by itself pretty nontrivial, and I'm looking for something that, in theory, could've been figured out by a reader of the book who, say, has not encountered analytic number theory before. Thanks in advance.","['number-theory', 'analytic-number-theory', 'dirichlet-series', 'l-functions']"
1848984,"If the entries of a positive semidefinite matrix shrink individually, will the operator norm always decrease?","Given a positive semidefinite matrix $P$, if we scale down its entries individually , will its operator norm always decrease? Put it another way: Suppose $P\in M_n(\mathbb R)$ is positive semidefinite and $B\in M_n(\mathbb R)$ is a $[0,1]$-matrix, i.e. $B$ has all entries between $0$ and $1$ (note: $B$ is not necessarily symmetric). Let $\|\cdot\|_2$ denotes the operator norm (i.e. the largest singular value). Is it always true that
   $$\|P\|_2\ge\|P\circ B\|_2?\tag{$\ast$}$$ Background. I ran into this inequality in another question . Having done a numerical experiment, I believed the inequality is true, but I hadn't been able to prove it. If $(\ast)$ turns out to be true, we immediately obtain the analogous inequality $\rho(P)\ge\rho(P\circ B)$ for the spectral radii because $\rho(P)=\|P\|_2\ge\|P\circ B\|_2\ge\rho(P\circ B)$. Remarks. There is much research on inequalities about spectral radii or operator norms of Hadamard products. Often, either all multiplicands in each product are semidefinite or all of them are nonnegative. Inequalities like those two here, which involve mixtures of  semidefinite matrices with nonnegative matrices, are rarely seen. I have tested the inequality for $n=2,3,4,5$ with 100,000 random examples for each $n$. No counterexamples were found. The semidefiniteness condition is essential. If it is removed, counterexamples with symmetric $P$s can be easily obtained. The inequality is known to be true if $P$ is also entrywise nonnegative. So, if you want to carry out a numerical experiment to verify $(\ast)$, make sure that the $P$s you generate have both positive and negative entries. One difficulty I met in constructing a proof is that I couldn't make use of the submultiplicativity of the operator norm. Note that tie occurs if $B$ is the all-one matrix, which has spectral norm $n\,(>1)$. If you somehow manage to extract a factor like $\|B\|_2$ from $\|P\circ B\|_2$, that factor may be too large. For a similar reason, the triangle inequality also looks useless.","['inequality', 'hadamard-product', 'matrices', 'normed-spaces', 'linear-algebra']"
1849047,"There is only one structure of ring (with identity) on the abelian group $(\mathbb{Z},+)$. Prove that a certain ring homomorphism is surjective.","This is an exercise from the textbook ""Algebra: Chapter 0"" by Paolo Aluffi. First, I state necessary facts from the book used in the exercise: For every abelian group $G, End_{Ab}(G)$ ( the set of group homomorphisms from $G$ to $G$ ) is a ring with operations: $+: (f+g)(a) = f(a) + g(a)$ $.: (fg)(a) = f(g(a))$ and zero $0_{End_{Ab}(G)} = $ trivial map $0: \ \ im0 = 0$ , identity $1_{End_{Ab}(G)} = 1_G \in End_{Ab}(G)$ Proposition. $End_{Ab}(\mathbb{Z}) \cong \mathbb{Z}$ as rings. Let $R$ be a ring. For $r \in R$ , define left-multiplication by $r$ by $\lambda_r$ . $\lambda_r$ is an endomorphism of the underlying abelian group $(R,+)$ . Proposition. Let $R$ be a ring. Then the function $\lambda: R \to End_{Ab}(R)$ defined by $\lambda(r) = \lambda_r$ is an injective ring homomorphism. Now, here goes the exercise: it's on the page $138$ in book, numbered $2.16$ . Prove that there is (up to isomorphism) only one structure of ring(with identity) on the abelian group $(\mathbb{Z},+)$ . Let $R$ be a ring whose underlying group is $\mathbb{Z}$ . Then there is an injective ring homomorphism $\lambda: R \to End_{Ab}(R)$ , and the latter is isomorphic to the ring $\mathbb{Z}$ . Prove that $\lambda$ is surjective is this case. So, assume $R$ is some ring, whose underlying abelian group is $(\mathbb{Z}, +)$ . We may denote it as $(\mathbb{Z}, + , o , 1_R = k)$ where $o$ is multiplication in $R$ , and $k$ is some integer serving and identity in this ring. We need to prove that every group homomorphism $\phi:(\mathbb{Z},+) \to (\mathbb{Z},+)$ may be defined by formula $\phi(n) = d \ o \ n$ for some integer $d$ . But every group endomorphism $\phi$ of $(\mathbb{Z},+)$ is uniquely determined by the image of $1$ . Since $\forall n \in \mathbb{Z} \ \ \phi(n) = \phi(n1) = n\phi(1)$ . And $1_{(\mathbb{Z},+)}$ is determined as $\forall z \in \mathbb{Z} \ \ 1_{(\mathbb{Z},+)}(z) = k(=1_{(\mathbb{Z},+,o)}) \ o \ z = z$ . What can be done next to show $\lambda: (\mathbb{Z}, +, o, k) \to End_{Ab}(\mathbb{Z})$ is surjective?","['abstract-algebra', 'ring-theory']"
1849081,"If you take the reciprocal in an inequality, would it change the $>/< $ signs?","Example:$$-16<\frac{1}{x}-\frac{1}{4}<16$$ In the example above, if you take the reciprocal of $$\frac{1}{x}-\frac{1}{4} = \frac{x}{1}-\frac{4}{1}$$ would that flip the $<$  to $>$ or not? In another words, if you take the reciprocal of $$-16<\frac{1}{x}-\frac{1}{4}<16$$   would it be like this: $$\frac{1}{-16}>\frac{x}{1}-\frac{4}{1}>\frac{1}{16}$$","['algebra-precalculus', 'inequality', 'fractions']"
1849115,"Relationship between $C_c^\infty(\Omega,\mathbb R^d)'$ and $H_0^1(\Omega,\mathbb R^d)'$","Let $d\in\mathbb N$ $\Omega\subseteq\mathbb R^d$ be open $\langle\;\cdot\;,\;\cdot\;\rangle$ denote the inner product on $L^2(\Omega,\mathbb R^d)$ $\mathcal D:=C_c^\infty(\Omega,\mathbb R^d)$ and $$H:=\overline{\mathcal D}^{\langle\;\cdot\;,\;\cdot\;\rangle_H}\color{blue}{=H_0^1(\Omega,\mathbb R^d)}$$ with $$\langle\phi,\psi\rangle_H:=\langle\phi,\psi\rangle+\sum_{i=1}^d\langle\nabla\phi_i,\nabla\psi_i\rangle\;\;\;\text{for }\phi,\psi\in\mathcal D$$ How are the topological dual spaces $\mathcal D'$ and $H'$ of $\mathcal D$ and $H$ related? Let me share my thoughts and please correct me, if I'm wrong somewhere (and feel free to leave a comment, if everything is correct): Let $f\in\mathcal D'$. If we equip $\mathcal D$ with the restriction $\left\|\;\cdot\;\right\|_{\mathcal D}$ of the norm induced by $\langle\;\cdot\;,\;\cdot\;\rangle_H$, then $f$ is a bounded, linear operator from $(\mathcal D,\left\|\;\cdot\;\right\|_{\mathcal D})$ to $\mathbb R$. Thus, since $\mathcal D$ is a dense subspace of $(H,\langle\;\cdot\;,\;\cdot\;\rangle_H)$, we can apply the bounded linear transform theorem and obtain the existence of a unique bounded, linear operator $F:(H,\langle\;\cdot\;,\;\cdot\;\rangle_H)\to\mathbb R$ (i.e. $F\in H'$) with $$\left.F\right|_{\mathcal D}=f\tag 1$$ and $$\left\|F\right\|_{H'}=\left\|f\right\|_{\mathcal D'}\tag 2$$ where $\left\|\;\cdot\;\right\|_{\mathcal D'}$ denotes the operator norm on $(\mathcal D,\left\|\;\cdot\;\right\|_{\mathcal D})'$. On the other hand, if $F\in H'$ and $$f:=\left.F\right|_{\mathcal D}\;,$$ then we can show that $f\in\mathcal D'$ where $\mathcal D'$ is equipped with the usual topology . It's clear that $(2)$ is verified too. Is there any mistake in my argumentation? And what's meant if $\nabla\pi$ with $\pi\in C_c^\infty(\Omega)'$ is claimed to be an element of $H$?","['hilbert-spaces', 'distribution-theory', 'operator-theory', 'functional-analysis', 'sobolev-spaces']"
1849191,Why doesn't this argument show the Möbius bundle is trivial?,"I wrote the following argument to prove that $S^1$ is parallelizable, that is, to show that the tangent bundle is trivial. It looks fairly reasonable to me. Let $\tau=2\pi$. We define a map $\varphi:S^1\times\Bbb R\to TS^1$ by
  $\varphi((e^{i\tau\theta},
 t))=(e^{i\tau\theta},t\frac{\partial}{\partial
 x^1}\Big|_{e^{i\tau\theta}})$. This map is clearly a bijection:
  injectivity follows trivially and surjectivity follows since the
  tangent space $T_{e^{i\tau\theta}}$ is one-dimensional. It remains to be shown that it is a diffeomorphism. We will, in fact,
  show that it is the identity map on suitably chosen coordinates. For
  any point $(e^{i\tau\theta},t)\in S^1\times\Bbb R$, we can choose the
  coordinate chart such that
  $(e^{i\tau(\theta+\varepsilon)},(t+\delta))$, for sufficiently small
  $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$. Similarly, for any point $(e^{i\tau\theta},t\frac{\partial}{\partial
 x^1}\Big|_{e^{i\tau\theta}})\in TS^1$, we can choose the coordinate
  chart such that
  $(e^{i\tau(\theta+\varepsilon)},(t+\delta)\frac{\partial}{\partial
 x^1}\Big|_{e^{i\tau(\theta+\varepsilon)}})$, for sufficiently small
  $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$.  Then,
  we have that $\widehat\varphi$ (the function with respect to these two
  coordinates) sends $(\varepsilon,\delta)\mapsto (\varepsilon,\delta)$,
  as desired. However, I don't see where this is using the tangent bundle construction in any meaningful way. It seems that this would work just as well for any rank-1 vector bundle over the sphere. That concerns me, because of course the conclusion is not true: for instance the Möbius strip is (diffeomorphic to) a vector bundle over the sphere. Since it is not orientable, we know that the bundle is nontrivial. So the question is: Where does this break for a general vector bundle? Or if the argument is simply invalid, can it be fixed without much trouble?","['vector-bundles', 'differential-geometry', 'differential-topology']"
1849230,"Subspaces of $\mathbb{R}^{(0,3)}$","I'm working my way through Axler's ""Linear Algebra Done Right"", and I've run into a proposition that I don't understand.  Namely: The set of differentiable real-valued functions $f$ on the interval (0, 3) such that $f'(2)=b$ is a subspace of $\mathbb{R}^{(0,3)}$ if and only if $b=0$ . My intution is that this has something to do with the fact that a subspace requires an additive identity, but that because the domain of the differentiable functions is open and exludes 0, we have to make some concessions (i.e. $f'(2) = 0$ ), though I'm unsure of how to work up a proof that backs this. Why must $f'(2) = 0$ to form a subspace of $\mathbb{R}^{(0,3)}$ ?",['linear-algebra']
1849251,Limit of a summation involving fractional parts,"Working with some problems on the floor function, I noticed that the sum
 $$\frac {1}{n}\sum_{{\sqrt{n}}\leq x\leq n}\left\{\sqrt {x^2-n}\right\} $$ where $n$ and $x$ are integers, $\left\{f(x)\right\}$ denotes the fractional part of $f(x)$, and $n$ tends to $\infty$, seems to converge to $\approx 0.44...$. For example, for $n=10^6$, the sum gives $0.4414959...$. I would be interested to know whether there is a closed expression for this value. 
I tried to solve this starting from commonly used formulas involving the floor function, but failed to prove it.","['number-theory', 'fractional-part', 'summation']"
1849271,Homeomorphism between evenly spaced integer topology and the rationals,"The evenly spaced integer topology is countable, metrizable, and has no isolated points, and hence is homeomorphic to the rationals with the order topology. But what is an explicit construction for this homeomorphism?","['general-topology', 'order-topology']"
1849280,Derivative of remainder function wrt denominator,"Given $f(x,y) = x \mathbin{\%} y = x - y \lfloor \frac{x}{y} \rfloor$, I want to find the partials ${{\partial f}\over{\partial y}}$ and ${{\partial f}\over{\partial x}}$ I understand there will be discontinuities in both. Intuitively ${{\partial f}\over{\partial x}} = 1$ where $\frac{x}{y} \notin \mathbb{Z}$ However, ${{\partial f}\over{\partial y}}$ seems more tricky. If I try to use intuition and define it piecewise, it seems like it may be: $$
{{\partial f}\over{\partial y}} = \left\{\begin{aligned}
&0 &&: 0 < x < y\\
&\text{undefined} &&: \frac{x}{y} \in \mathbb{Z} \\
&-1 &&: \text{otherwise}
\end{aligned}
\right.$$ Or something similar. However, if I derive it from the above definition: $$
\begin{aligned}
{{\partial f}\over{\partial y}} &= -\frac{\partial}{\partial y} \operatorname{floor}\left(\frac{x}{y}\right)\\[6pt]
&= \frac{\partial}{\partial y} \operatorname{floor} \left(\frac{x}{y}\right)\frac{x}{y^{2}}\\
\end{aligned}
$$ But it's my understanding that $\frac{\partial}{\partial x}\operatorname{floor}(x) = 0, \forall x \notin \mathbb{Z}$ which implies $\frac{\partial f}{\partial y} = 0$. This just feels like an incomplete picture and I hope I've made some error or poor assumption somewhere. Would really appreciate some insight into alternative definitions of modulo that I could use here.","['derivatives', 'partial-derivative', 'modular-arithmetic']"
1849281,Average distance between two points on a unit square. [duplicate],"This question already has an answer here : Average distance between two random points in a square (1 answer) Closed 7 years ago . Consider the unit square $S =[0,1]\times[0,1]$.  I'm interested in the average distance between random points in the square. Let $ \mathbf{a} = \left< x_1,y_1 \right>$ and  $ \mathbf{b} = \left< x_2,y_2 \right>$ be random points in the unit square.  By random, I mean that $x_i$ and $y_i$ are uniformly distributed on $[0,1]$. The normal approach is to use multiple integration to determine the average value of the distance between $\mathbf{b}$ and $\mathbf{a}$.  I would like to try another approach. $\mathbf{a}$ and $\mathbf{b}$ are random vectors, and each element has known distribution. So, the vector between them also has known distribution.  The difference between two uniformly random variables has triangular distribution. So $\mathbf{c} = \mathbf{b} - \mathbf{a}$.  Then, the average distance is the expectation of $\lVert \mathbf{c} \rVert$.  Perhaps it would be easier to calculate the expectation of $\lVert \mathbf{c} \rVert^2$. In any case, I am not sure how to calculate the expectation for $\lVert \mathbf{c} \rVert^2$. Can someone guide me in the right direction?",['probability']
1849304,Are S.I. Euler and Verlet the same thing?,"Verlet as given by Wikipedia : set $\vec x_1=\vec x_0+\vec v_0\,\Delta t+\frac12 A(\vec x_0)\,\Delta t^2$ for ''n=1,2,...'' iterate $\vec x_{n+1}=2 \vec x_n- \vec x_{n-1}+ A(\vec x_n)\,\Delta t^2.$ S.I. Euler as given by Wikipedia : $v_{n+1} = v_n + g(t_n, x_n) \, \Delta t\\
x_{n+1} = x_n + f(t_n, v_{n+1}) \, \Delta t$ Starting with S.I. Euler (and simplifying the notation): $$
x_{n+1} = x_n + hv_{n+1} \implies v_n = \frac{x_n - x_{n-1}}{h}\\
x_{n+1} = x_n + hv_{n+1} = x_n + h(v_n + ha_n) = x_n + h\left(\frac{x_n - x_{n-1}}{h} + ha_n\right) = 2x_n - x_{n-1} + h^2a_n
$$ OK so they're the same thing, cool. That matches my tests (some simple projectile stuff + wind resistance + weird acceleration functions) where Verlet and SI Euler perform to within floating point error. However the Verlet page says, "" The global error of all Euler methods is of order one, whereas the global error of [Verlet] is, similar to the midpoint method, of order two. "" It also says, "" The global error in position, in contrast, is $O(\Delta t^{2})$ and the global error in velocity is $O(\Delta t^{2})$. "" The SI Euler page says "" The semi-implicit Euler is a first-order integrator, just as the standard Euler method. "" How can they have different orders when they are the same method and seem to produce identical results?","['numerical-methods', 'eulers-method', 'ordinary-differential-equations']"
1849315,Find the bounds for the norm $T:l_2 \to l_2$.,"Given $T:l_2 \to l_2$ define as $T((x_1,x_2,\ldots,x_n,\ldots))=(x_2-x_1,x_3-x_2,\ldots,x_{n+1}-x_n,\ldots)$ then which of the following is true, $\|T\|=1$ $\|T\|\geq2$ $1<\|T\|\leq2$ None of above. What I did- I used $\|T\|^2=\langle T,T\rangle$, so after calculation I got $\|T(x)\| = \left( \sum_{i=1}^\infty (x_{i+1}-x_i)^2 \right)^{1/2}$. Now I got stuck. How to proceed further? Please help.",['functional-analysis']
1849323,"How do we prove that $\int_0^1 \ln x\left({1\over \ln{x}}+{1\over 1-x}\right)^2\,dx=\gamma-1?$","How do we prove that: $$\int_{0}^{1}\ln{x}\left({1\over \ln{x}}+{1\over 1-x}\right)^2\, dx =\color{blue}{\gamma-1}?\tag1$$ The only idea came to mind was this series $$\sum_{n=1}^{\infty}{1\over 2^k(1+x^{-1/2^k})}={x\over 1-x}-{1\over \ln{x}}\tag2$$ Or expanded $(1)$ $$\int_0^1 \left({1\over \ln x} + {2 \over 1-x}+{\ln x \over (1-x)^2} \right)\,dx=\gamma-1\tag3$$ $$\int_0^1 {\ln x \over (1-x)^2}\,dx=\sum_{n=0}^\infty (1+n)\int_0^1 x^n\ln x \,dx = \sum_{n=0}^\infty (1+n)\cdot{-1\over (1+n)^2}\tag4$$ But $(4)$ diverges! $\int {1\over 1-x} \, dx=-\ln(1-x)$ $\int_0^1 {2\over 1-x} \, dx$ also diverges $\int{1\over \ln x} dx = \ln(\ln x )+\ln x +{\ln^2 x\over 2\cdot2!}+{\ln^3 x \over 3\cdot 3!}+\cdots$ $\int_0^1 {1\over \ln x} \, dx$ diverges too. How do we go about integrating $(1)$? Help needed, thanks!","['integration', 'definite-integrals', 'calculus', 'proof-verification']"
1849325,Projection from a point on a curve,"Let $k$ be an algebraically closed field, char $k=0$, and let $C\subset\mathbb{P}_k^2$ be a nonsingular projective plane curve of degree $d$. Let $O\in C$, $L\subset\mathbb{P}_k^2$ a line not containing $O$, and consider the map $\varphi:C\to L$ given by projection away from $O$ onto $L$.  This is a priori only a map $C-\{O\}\to L$, and then we extend it using that $C$ is a curve.  It is a map of degree $d-1$. I want to check this claim: the ramification index of $\varphi$ at $O$ is the order of vanishing of the tangent line to $C$ at $O$ (call if $f$) minus 1. For, suppose that $\varphi(O)=P$.  Then $\varphi^{-1}(O)=\{Q_1,\dots,Q_{r},P\}$, and $Q_1,\dots,Q_r$ are points on $C\cap OP$, where $OP$ is line between $O$ and $P$, and there are $d$ points on this line, counting multiplicities.  If $e_i$ is the ramification index of $\varphi$ at $Q_i$, and $e$ is the ramification index of $\varphi$ at $P$, then we have $e+\sum_{i=1}^re_i=d-1$.  On the other hand, $e_i$ is equal to the order of vanishing of $OP$ at $Q_i$, and we have that $d=f+\sum_{i=1}^re_i$.  So we get $e=f-1$. Now that I've written this I'm pretty sure I believe it, but I'd appreciate anyone who wants to check it!",['algebraic-geometry']
1849335,Why is continuous differentiability important?,"In calculus, I would presume that the notion of continuous differentiability is important, which is why we have classes $C^1, C^2,\ldots,C^n$ which are defined in terms of having a continuous $n$th derivative. But why? Why is the derivative being continuous relevant at all? What is the motivation for defining $C^n$ in terms of not merely being $n$ times differentiable, but $n$ times continuously differentiable? For which (important) theorems in single and multivariable calculus is the hypothesis of continuous differentiability absolutely required? It is not required for either the fundamental theorem of calculus or integration by substitution, though it is often presented as being such.","['derivatives', 'calculus']"
1849340,A very tricky pseudo-proof of $0=-1$ through series and integrals,"Dealing with a recent question I spotted a very nice exercise for Calc-2 students, i.e. to find the mistake in the following lines. Lemma 1. For any $n\in\mathbb{N}$, we have:  $$ \int_{0}^{1} x^n\left(1+(n+1)\log x\right)\,dx = 0. $$ Lemma 2. For any $x\in(0,1)$ we have: $$ \frac{1}{1-x}=\sum_{n\geq 0}x^n,\qquad \frac{\log x}{(1-x)^2}=\sum_{n\geq 0}(n+1) x^n\log(x). $$
   By Lemmas 1 and 2 it follows that:
   $$\begin{eqnarray*}(\text{Lemma 1})\quad\;\;\color{red}{0}&=&\int_0^1 \sum_{n\geq0} x^n\left(1+(n+1)\log x\right)\,dx\\[0.2cm](\text{Lemma 2})\qquad&=&\int_0^1 \left(\frac{1}{1-x} + \frac{\log x}{(1-x)^2}\right)\,dx\\[0.2cm](x\mapsto 1-x)\qquad&=&\int_0^1 \left(\frac{1}{x}+\frac{\log(1-x)}{x^2}\right)\,dx\\[0.2cm](\text{Taylor series of }x+\log(1-x))\qquad&=&-\int_0^1 \frac{1}{x^2} \sum_{k\geq2}\frac{x^k}k \,dx\\[0.2cm](\text{termwise integration})\qquad&=&-\sum_{k\geq 2} \frac{1}{k(k-1)}\\[0.2cm](\text{telescopic series})\qquad&=&-\sum_{m\geq 1} \left(\frac{1}{m}-\frac{1}{m+1}\right)=\color{red}{-1}.
 \end{eqnarray*}$$ Now the actual questions: were you able to locate the fatal flaw at first sight ? Do you think it is a well-suited exercise for Calculus-2 (or Calculus-X) students ?","['integration', 'sequences-and-series', 'soft-question']"
1849345,Graph of the function $\cos(x)\cos(x+2)-\cos^2(x+1)$ will be?,Graph of the function $\cos(x)\cos(x+2)-\cos^2(x+1)$ will be? (A)A straight line (B)A parabola Give the corresponding equation too. Source:JEE 1997. Can someone suggest how should I proceed?Can't think of any way to reduce the the given equation to a straight line or a parabola's equation.Please guide me! JEE 1997 paper PDF version,"['trigonometry', 'calculus', 'analytic-geometry', 'algebra-precalculus', 'functions']"
1849380,The equation $\sqrt{x+1}-\sqrt{x-1}=\sqrt{4x-1}$ has how many solutions? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The equation $\sqrt{x+1}-\sqrt{x-1}=\sqrt{4x-1}$ has how many solutions? What would be the correct approach to this problem?Squaring seems to make it even more complicated! P.S:Sorry,at first I wanted to avoid invalid roots.But thanks for your help.Its a good idea to check at the end.",['algebra-precalculus']
1849389,Application of tensor product of graphs in real life.,"I was going through the book HANDBOOK OF PRODUCT GRAPHS by Richard Hammack, Wilfried Imrich, and Sandi Klavzar. In the preface section, application of direct product of graphs is mentioned. I am interested in gaining more information about the real life applications of other graph products. Can anyone suggest me a link or good book as a reference? This will be very helpful to me. Thanks a lot for giving time.","['combinatorics', 'graph-theory', 'tensor-products', 'discrete-mathematics']"
1849443,"Is there a connection between the concepts of limits in ordinals, functions and categories?",In set theory there is the concept of a limit ordinal: Nonzero ordinals that are the supermum of all ordinals below them. In functional analysis there are the concepts of limits of functions (and sequences) a value that the function comes arbitrarily close to at a point. And in category theory there is a concept of a limit which is a universal cone. Is there something common about all these ideas that justifies them all being called limits or is it a coincidence of language ?,"['real-analysis', 'limits', 'functional-analysis', 'elementary-set-theory', 'category-theory']"
1849462,An operator has closed range iff some condition holds,"If $T:X \rightarrow Y$ is a bounded linear operator that $T^{-1}(0)=\{0\}$ then $\mathcal{R}(T)=\{Tx\}_{x \in X}$ is closed $\Leftrightarrow$ there is
no sequence $(x_n)$ with $||x_n||=1$ such that $Tx_n \rightarrow 0$ I have $\Rightarrow$ but no idea for $\Leftarrow$ .",['functional-analysis']
1849467,"Given $A+B+C=180^{\circ}$, find value of $\tan A\cdot\tan B+\tan B\cdot\tan C+\tan A\cdot \tan C-\sec A\cdot\sec B\cdot\sec C$","Given $A+B+C=180^{\circ}$, find value of $$\tan A\cdot\tan B+\tan B\cdot\tan C+\tan A\cdot \tan C-\sec A\cdot\sec B\cdot\sec C$$ I know about some basic conditional identities but don't know how to use them here.",['trigonometry']
1849502,Determine if $A\subset B$ and $A\subset C\Leftrightarrow A\subset (B\cup C)$ is true. [duplicate],"This question already has an answer here : Verify if the proposed equivalence $A \subset B \land A \subset C \iff A \subset (B \cup C)$ holds (1 answer) Closed 2 years ago . My opinion is $A\subset B$ and $A\subset C\Rightarrow A\subset (B\cup C)$ , BUT converse is NOT true. My proof: ($\Rightarrow$) Suppose $A\subset B$ and $A\subset C$ Let $x\in A$ Then $ x\in B$ and $x\in C$ $\Rightarrow x\in (B\cap C)$ $\Rightarrow x\in (B\cup C)$ ($\because (B\cap C) \subset (B\cup C))$ Therefore, $A\subset B$ and $A\subset C\Rightarrow A\subset (B\cup C)$ However, I don't know how to show that the converse is NOT true. Should I find a counterexample? If so, can someone please show me how to write a counterexample to disprove the converse?","['real-analysis', 'elementary-set-theory']"
1849541,How to compute the sine of huge numbers,"For several days, I've been wondering how it would be possible to compute the sine of huge numbers like 100000! (radians). I obviously don't use double but cpp_rational from the boost multiprecision library. But I can't simply do 100000! mod 2pi and then use the builtin function sinl (I don't need more than 10 decimal digits..) as I'd need several million digits of pi to compute this accurately. Is there any way to achieve this?","['trigonometry', 'programming']"
1849544,What is KL-Divergence? Why Do I need it? How do I use it?,"I am currently studying KL Divergence. But It seems very confusing that I don't maybe understand why do I ever need it and what is that for? As I have been reading stuff about Mutual Information, it looks like it is about the amount of Entropy between two probability distributions, for example, P(A) and P(B|A), especially for the conditional probability situation. Can somebody give me a clear explanation about KL-Divergence?","['data-analysis', 'mathematical-physics', 'machine-learning', 'statistics', 'data-mining']"
1849546,Evaluate $\sum_ {n=1}^{\infty} \cot^{-1}(2n^2)$,"I was trying to solve up this equation but couldn't move ahead.
$$\sum_ {n=1}^{\infty} \cot^{-1}(2n^2)$$
I wrote the expression as $$\sum_ {n=1}^{\infty} \tan^{-1}\left( \frac{1}{2n^2}\right)$$ I wanted to change the expression into such a form such that it can take up the form of $\tan^{-1}A-\tan^{-1}B$ so that all the terms except the second one get cancelled up but I am unable to think of any manipulation through which I can get the thing done. Can anybody give me a hint on how to go ahead?",['trigonometry']
1849552,Spectrum in functional-analysis and algebraic geometry,"Why do we use the notion ""spectrum"" both in functional-analysis and in algebraic geometry? Are there any analogies?","['functional-analysis', 'soft-question', 'algebraic-geometry']"
1849575,Prove a matrix expression leads to an invertible matrix?,"I want to prove matrix $C$ is invertible:
$$C=I-A^TB(B^TB)^{-1}B^TA(A^TA)^{-1},$$
where $I$ is an identity matrix of appropriate dimensions, and $(A^TA)^{-1}$ and $(B^TB)^{-1}$ imply both $A$ and $B$ have linearly independent columns. I tried to achieve my goal by proving $\det(C)\neq0$ but got stuck.","['matrices', 'inverse', 'linear-algebra', 'determinant']"
1849577,A celestial topology?,I recently asked for natural topologies on the set of lines in $\mathbb R^2$. Now I'm aiming for a similar question on the set $S_p$ of conic sections in $\mathbb R^2$ sharing the same focus $p$ (but not necessary having the same major axis). The situation is an idealization of the ecliptic plane and all stuff in the solar system. Are there natural topologies on this set?,"['algebraic-geometry', 'geometry', 'celestial-mechanics', 'general-topology', 'differential-geometry']"
1849588,Can I define the set of natural numbers without the axiom of infinity?,"The exercise asks if exists a definition of $Nat(x)$ such that $Nat(x) \Rightarrow Nat(S(x))$, and $\exists x $such that $ Nat(x)$ is false without the use of axiom of infinity. Here $Nat(x) \Leftrightarrow x$ is a natural number, and $S(x)$ is the successor of $x$. I tried to define $$S(a)=\{a\}.$$ Then I define $$a \in C_a \Leftrightarrow (a\in C_a \Rightarrow S(s) \in C_a).$$
So $$\emptyset\in \cap C_i \forall i. $$
At the end I define $$Nat (a) \Leftrightarrow a\in C_\emptyset.$$ 
Can you ckech my idea or suggest another one?",['elementary-set-theory']
1849603,The Greatest Common Divisor of All Numbers of the Form $n^a-n^b$,"For fixed nonnegative integers $a$ and $b$ such that $a>b$, let $$g(a,b):=\underset{n\in\mathbb{Z}}{\gcd}\,\left(n^a-n^b\right)\,.$$   Here, $0^0$ is defined to be $1$.  (Technically, we can also set $g(a,b):=0$ if $a=b$, and $g(a,b):=g(b,a)$ if $a<b$.)     For a nonzero integer $m$ and a prime integer $p$, let $v_p(m)$ denote the maximum power $k\in\mathbb{N}\cup\{0\}$ such that $p^k$ divides $m$.  The set $D(a,b)$ is given by $$D(a,b):=\big\{p\in\mathbb{N}\,\big|\,p\text{ is an odd prime and }p-1\mid a-b\big\}\,.$$
  (a)  Show that
  $$g(a,b)=\left\{\begin{array}{ll}1\,,&\text{if }b=0\text{ and }a\text{ is odd}\,,
\\
2\,,&\text{if }b>0\text{ and }a-b\text{ is odd}\,,
\\
\displaystyle 2^{\min\big\{v_2(a-b)+2,b\big\}}\,\prod_{p\in D(a,b)}\,p^{\min\big\{v_p(a-b)+1,b\big\}}\,,&\text{if }a-b\text{ is even}\,.\end{array}\right.$$
  (b)  What is the minimum value $\ell(a,b)$ of $l\in\mathbb{N}\cup \{0\}$ such that there exists a subset $S\subseteq\mathbb{Z}$ of size $l$ for which
  $$g(a,b)=\underset{n\in S}{\gcd}\,\left(n^a-n^b\right)\,?$$
  (With the convention $\gcd(\emptyset)=0$, we can say that $\ell(a,b)=0$ for $a=b$.) For examples, 
$$g(a,0)=1$$
for all integers $a>0$,
$$g(13,1)=2\cdot 3\cdot 5\cdot 7\cdot 13=2730\,,$$
and
$$g(8,2)=2^{2}\cdot 3^2\cdot 7=252\,.$$
This post is inspired by Prove that $2730$ divides $n^{13} - n$ for all integers $n$. .  Part (a) is known.  Part (b) is still open, and is related to The GCD of a Univariate Integer-Valued Polynomial over a Set .  We have the following bound:
$$\ell(a,b)\leq a+1\,.$$
However, I believe that this inequality holds:
$$\ell(a,b)\leq 2\,.$$","['prime-factorization', 'gcd-and-lcm', 'number-theory', 'prime-numbers', 'elementary-number-theory']"
1849606,How evaluate this integral in cartesian coordinates?,"I can evaluate this with polar coordinates, but is it possible in cartesian coordinates? $$
    \int\limits_{z=-1}^{1} \int\limits_{y=-\sqrt{1-x^2}}^{y=\sqrt{1-x^2}} \int\limits_{x=-\sqrt{1-y^2}}^{x=\sqrt{1-y^2}} 1 dx\,dy\,dz$$ Inner integral:
$$\int\limits_{x=-\sqrt{1-y^2}}^{x=\sqrt{1-y^2}} 1 dx= 2\sqrt{1-y^2}$$ Second integral:
$$
\int\limits_{y=-\sqrt{1-x^2}}^{y=\sqrt{1-x^2}}2\sqrt{1-y^2}
$$ Substitution:
$$
t=\sqrt{1-y^2}
$$ $$
\frac{dt}{dy}=\frac{-y}{\sqrt{1-y^2}}
$$ $$
dt=\frac{-y}{\sqrt{1-y^2}}dy
$$
And I'm stuck here....","['multivariable-calculus', 'integration', 'definite-integrals', 'calculus']"
1849613,Evaluate $\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right)$ (possible textbook mistake - James Stewart 7th),"I was working on a few problems from James Stewart's Calculus book (seventh edition) and I found the following: Find $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right)$$ Since there's a $|x|$ on the limit and knowing that $|x| = -x$ for any value less than zero, we have $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x}$$ So far so good. Continuing, $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x} = - \infty$$ since the denominator becomes smaller and smaller. When checking the textbook's answer I've found the following: Am I missing something or should the limit really be $- \infty$ ?","['absolute-value', 'calculus', 'limits']"
1849674,Solving $\cos^{-1}x+\cos^{-1}2x=-\pi$ gives the invalid answer $x=0$,"To solve the inverse trigonometric equation $$\cos^{-1}x+\cos^{-1}2x=-\pi,$$ I use the normal cosine addition \begin{align}
\cos^{-1}(2x^2 -\sqrt {1-x^2} \sqrt{1-4x^2})&=-\pi\\
2x^2 -\sqrt {1-x^2} \sqrt{1-4x^2}&=\cos(-\pi)\\
2x^2 -\sqrt {1-x^2} \sqrt{1-4x^2}&=-1\\
(2x^2+1)^2&=(1-x^2)(1-4x^2)\\
4x^4+1+4x^2&=1-4x^2-x^2+4x^4\\
9x^2&=0\\
x&=0.
\end{align} Putting $x=0$ in the equation gives LHS $\ne$ RHS: $$\cos^{-1}0+\cos^{-1}0=-\pi \\\pi=-\pi.$$ Since the equation has no solution, why does solving it give zero as a solution? Is my method wrong or is there something else which gives up one solution (i.e. $x=0$ on solving algebraically)?",['trigonometry']
1849697,whats the proof for $\lim_{x → 0} [(a_1^x + a_2^x + .....+ a_n^x)/n]^{1/x} = (a_1.a_2....a_n)^{1/n}$,"This equation is directly given in my book and I am don't know anything about its proof.I tried L'Hospital rule by differentiating the both numerator as well as denominator(division rule), but the result is still coming in indeterminate forms.I am a beginner , and haven't practiced limits that much. This formula is really confusing me.",['limits']
1849698,How to calculate the sine cosine or tangent of an angle(Simply Explained),"I wanted to know how a calculator finds the sine Or any other trig function with only knowing the value of the angle. 
I have been looking on the internet for answers because i was really interested in how Archimedes found out what the value of pi is. 
Then that led me to calculating the sides of right triangles but i wanted to know since Archimedes didn't have a calculator how did he find the length of the opposite side on  the triangle. I searched a lot of pages but they were all so complex(i'm in middle school)to me. So basically can anyone simplify the way you(or Archimedes) can find the length of the opposite side of a right triangle? Sorry, if this is a broad/vague question in any way or if i have made any mistakes.
This is my first time asking a question online I thank you in advance for anyone who can help!","['trigonometry', 'pi']"
1849712,Imaginary eigenvalues,I'm still developing my skills in LinearAlgebra and I ponder just what are the main differences between effects of imaginary and real eigenvalues on linear operations specially taking into account their geometric interpretation ? Is there somewhere the list of these differences? Is it true that if we have imaginary eigenvalues then it is necessary for some  subspace of a space generated by a matrix $A$  that we have no preserved directions of vectors in this subspace as for example it is in the case of 2D i 3D rotations? If so how to apply these imaginary eigenvalues for generating this subspace? (in the case of rotations to generate the plane),"['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1849730,How can I solve $y''=\frac{a}{y^2}$ where a is a (positive) constant?,"Actually, I found out a way to solve that, but I can't get rid of complex numbers. And it does not make sense when it comes to complex numbers as the original question that involves this differential equation has nothing to do with complex numbers, as this is from a physics question.","['physics', 'ordinary-differential-equations', 'complex-numbers']"
1849763,Pointwise convergence of Fourier series in two dimensions,"By Carleson's Theorem, we know that for every $f\in L^2(\mathbb{T})$
$$ f(x)=\lim_{N\rightarrow\infty}\sum_{k=-N}^N\hat{f}(k)e^{2\pi ikx}\;\text{ a.e.} $$
Suppose now that $f\in L^2(\mathbb{T}^2)$. Using Carleson's Theorem, which would be the easiest way to prove
$$ f(x,y)=\lim_{N\rightarrow\infty}\sum_{k,l=-N}^N\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$
Is it also true that
$$ f(x,y)=\lim_{M,N\rightarrow\infty}\sum_{|k|\leq M,\,|l|\leq N}\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$
(by $\lim_{M,N\rightarrow\infty}$ I mean the limit of a double sequence: given $\{a_{m,n}\}\subseteq\mathbb{C}$, we say that $\lim_{m,n\rightarrow\infty}a_{m,n}=L$ if for all $\epsilon>0$ there exists an $N_{\epsilon}\in\mathbb{N}$ such that $|L-a_{m,n}|<\epsilon$ for every $m,n\geq N_{\epsilon}$).","['harmonic-analysis', 'fourier-series', 'fourier-analysis', 'limits']"
1849784,Calculate miter points of stroked vectors in Cartesian plane,"I have two vectors CA and CB which I 'stroked' with lines of width a and b . I need to calculate D and E points to draw miter joint between two stroked vectors. 
What I know is: A point coordinates B point coordinates C point coordinates β angle a length b length What I'm looking are coordinates of points D and E . I need to find universal formula that lets me to calculate those points at any β. (please see pic.1 below) I can calculate those points if use stroke with same length for both vectors ( a == b ). I'm doing it by reflecting point C over vector AC with distance 0.5*a . Then I have right angle triangle, on which γ angle at point C equals 90° - (0.5 * β) angle. Therefore I have all three angles for the triangle and length of CF (half of a ) which lets my to calculate coordinates of point D . (please see pic.2 below). I use triangle CGE to calculate E coordinates in the same way as above. My problems start when I need to use different width for the vector's stroke ( a != b ) (please see pic.3 below). In that case when I draw CFD triangle I cannot calculate γ angle as it is not 90° - (0.5 * β) anymore and I have no idea how to calculate D and E coordinates. Can someone point me in the right direction how to find γ angle or if there is any other (better) way to calculate coordinates of D and E ?",['geometry']
1849797,What benefits do real numbers bring to the theory of rational numbers?,"Complex numbers make it easier to find real solutions of real polynomial equations. Algebraic topology makes it easier to prove theorems of (very) elementary topology (e.g. the invariance of domain theorem). In that sense, what are theorems purely about rational numbers whose proofs are greatly helped by the introduction of real numbers? By ""purely"" I mean: not about Cauchy sequences, Dedekind cuts, etc. of rational numbers. (This is of course a meta-mathematical statement and therefore imprecise by nature.) ""No, there is no such thing, because..."" would also be a valuable answer.","['real-analysis', 'real-numbers', 'soft-question']"
1849809,"Entries of the inverse of $\left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}}$ are polynomials in $x$.","Let $n$ be a positive integer.  Define $$\textbf{A}_n(x):= \left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}}$$ as a matrix over the field $\mathbb{Q}(x)$ of rational functions over $\mathbb{Q}$ in variable $x$. (a) Prove that the Hilbert matrix $\textbf{A}_n(0)$ is an invertible matrix over $\mathbb{Q}$ and all entries of the inverse of $\textbf{A}_n(0)$ are integers. (b) Determine the greatest common divisor (over $\mathbb{Z}$) of all the entries of $\big(\textbf{A}_n(0)\big)^{-1}$. (c) Show that $\textbf{A}_n(x)$ is an invertible matrix over $\mathbb{Q}(x)$ and every entry of the inverse of $\textbf{A}_n(x)$ is a polynomial in $x$. (d) Prove that $x+n$ is the greatest common divisor (over $\mathbb{Q}[x]$) of all the entries of $\big(\textbf{A}_n(x)\big)^{-1}$. Parts (a) and (c) are known. Parts (b) and (d) are open. Now, Part (d) is known (see i707107's solution below), but Part (b) remains open, although it seems like the answer is $n$. Recall that 
$$\binom{t}{r}=\frac{t(t-1)(t-2)\cdots(t-r+1)}{r!}$$
for all $t\in\mathbb{Q}(x)$ and $r=0,1,2,\ldots$.  According to i707107, the $(i,j)$-entry of $\big(\textbf{A}_n(x)\big)^{-1}$ is given by
$$\alpha_{i,j}(x)=(-1)^{i+j}\,(x+n)\,\binom{x+n+i-1}{i-1}\,\binom{x+n-1}{n-j}\,\binom{x+n+j-1}{n-i}\,\binom{x+i+j-2}{j-1}\,.\tag{*}$$
This means that, for all integers $k$ such that $k\notin\{-1,-2,\ldots,-2n+1\}$, the entries of $\big(\textbf{A}_n(k)\big)^{-1}$ are integers. I now have a new conjecture, which is the primary target for the bounty award. Conjecture: The greatest common divisor $\gamma_n(k)$ over $\mathbb{Z}$ of the entries of $\big(\textbf{A}_n(k)\big)^{-1}$, where $k$ is an integer not belonging in the set $\{-1,-2,\ldots,-2n+1\}$, is given by $$\gamma_n(k)=\mathrm{lcm}(n,n+k)\,.$$ It is clear from (*) that $n+k$ must divide $\gamma_n(k)$.  However, it is not yet clear to me why $n$ should divide $\gamma_n(k)$.  I would like to have a proof of this conjecture, or at least a proof that $n \mid \gamma_n(k)$. Let $M_n$ denote the (unitary) cyclic $\mathbb{Z}[x]$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)$.  Then, the (unitary)  $\mathbb{Z}[x]$-module $N_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}[x]$-submodule of $M_n$. We also denote by $\tilde{M}_n$ for the (unitary)  $\mathbb{Z}$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)\,x^l$ for $l=0,1,2,\ldots,2n-2$.  Then, the (unitary) $\mathbb{Z}$-module $\tilde{N}_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}$-submodule of $\tilde{M}_n$. For example, $M_2/N_2$ is isomorphic to the (unitary) $\mathbb{Z}[x]$-module $\mathbb{Z}/2\mathbb{Z}$ (in which $x$ acts trivially), and $\tilde{M}_2/\tilde{N}_2$ is isomorphic to the (unitary) $\mathbb{Z}$-module $\mathbb{Z}/2\mathbb{Z}$.  Hence, $\left|M_2/N_2\right|=2=\left|\tilde{M}_2/\tilde{N}_2\right|$.  For $n=3$, Mathematica yields
$$\tilde{M}_3/\tilde{N}_3\cong (\mathbb{Z}/2\mathbb{Z})\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/4\mathbb{Z})^{\oplus 3}\,,$$
as abelian groups.  That is, $\left|\tilde{M}_3/\tilde{N}_3\right|=1152$.  On the other hand, 
$$M_3/N_3\cong \mathbb{Z}[x] \big/\left(12,2x^2+6x+4,x^4-x^2\right)$$
as $\mathbb{Z}[x]$-modules, which gives $\left|M_3/N_3\right|=576$. Question: Describe the factor $\mathbb{Z}[x]$-module $M_n/N_n$ and the factor $\mathbb{Z}$-module $\tilde{M}_n/\tilde{N}_n$.  It is easily seen that $\left|M_n/N_n\right|\leq\left|\tilde{M}_n/\tilde{N}_n\right|$.  What are $\left|M_n/N_n\right|$ and $\left|\tilde{M}_n/\tilde{N}_n\right|$?  It can be shown also that the ratio $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ is an integer, provided that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite.  Compute $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ for all integers $n>0$ such that $\left|\tilde{M}_n/\tilde{N}_n\right|<\infty$.  Is it always the case that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite? Apart from the conjecture above, this question is also eligible for the bounty award. I have not yet fully tried to deal with any case involving $n>3$.  However, for $n=4$, the module $\tilde{M}_4/\tilde{N}_4$ is huge:
$$  \tilde{M}_4/\tilde{N}_4\cong (\mathbb{Z}/2\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 3}\oplus(\mathbb{Z}/8\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/9\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/16\mathbb{Z})\oplus(\mathbb{Z}/27\mathbb{Z})$$
as abelian groups.","['modules', 'polynomials', 'matrices', 'gcd-and-lcm', 'rational-functions']"
1849821,Proving $ a^4 \equiv 1 \pmod d$,"I need to prove the following statements: Prove the following statements: (a) if $a$ is odd then $a^4 ≡ 1 \pmod 4$, (b) if $5$ does not divide a, then $a^4 \equiv 1 \pmod 5$. Can I do this inductively? Or should I be adopting another approach? I know for (a), if $a$ is odd, $a^4$ will also be odd, as the product of odd numbers is always odd. This would mean that $4 \mid (a^4 - 1)$, which would always be an even number, but of course not all even numbers are divisible by 4 (or rather $(a^4 - 1)$ would always be an even number). I also know you can obtain all the congruence cases of $1 \pmod 5$ by adding or subtracting $5$, beginning at $1$.","['modular-arithmetic', 'discrete-mathematics']"
1849839,Jordan form of a power of Jordan block?,"How, in general, does one find the Jordan form of a power of a Jordan block? Because of the comments on this question I think there is a simple answer.","['matrices', 'jordan-normal-form', 'linear-algebra', 'exponentiation']"
1849851,General solution of a nonlinear differential equation,"Nonlinear differential equation gone beyond my field of expertise but I'd like to know the details of a problem and to do that I should know the general solution of the following nonlinear differential equation: $$y'(x) = \alpha\beta e^{-\frac{x}{\gamma}} - \delta \sqrt{y(x)}$$ with $\alpha, \gamma, \delta > 0$ and $0 \leq \beta \leq 1$. Since $x$ represents the time is it also possible to assume $x \geq 0$. I tried to solve it with Wolfram Alpha and Wolfram Mathematica but I didn't get any result due to computational time excedeed. Is it possible to find an analytical form of $y(x)$?","['nonlinear-analysis', 'ordinary-differential-equations']"
1849855,"From Halmos' ""Finite-Dimensional Vector Spaces"": Similar matrices and transformations paradox","From section 47 called Similarity: (In the following I will represent matrices like $[A]$ and linear transforms as $A$ and also sorry if I am not rigorous enough) Halmos proves that when we have one linear transformation $T:V\longrightarrow V$ with matrix $[B]$ in a basis $X$ (vectors $\ \vec x_1, \vec x_2, .., \vec x_n$)  and matrix $[C]$ in basis $Y$ (vectors $\ \vec y_1, \vec y_2, .., \vec y_n$) and the two bases are related by $[A]x_i=y_i$, then the two matrices are related by $[C]=[A]^{-1}[B][A]$. He also proves that when we have two linear transformations $B$ and$C$ and $[B]=(β_{ij})$ is a matrix and the two transformations are defined as $B\vec x_j= \sum_{i=0}^nβ_{ij}\vec x_i$ and $C\vec y_j= \sum_{i=0}^nβ_{ij}\vec y_i$, the two transformations are related by $C=ABA^{-1} $. While I have proved these things, I can't intuitively(geometrically) understand why the difference in the relation of the transformations with the relation with the matrices , since a matrix is a way to express the transformation on a coordinates system(please correct me if I am wrong as I am not a mathematician). Also, which of the two relations are used when somebody deals with change of basis? Trying to understand these concepts through a rotation matrix $[A]$ and a projection matrix $[B]$, I figured out that in the first case(relation between matrices), the matrix $[C]$ is presented this way in order to again project to the same plane as $[B]$ did but it just has to have different matrix elements in order for it to work in the new basis $Y$ and I suppose that is why Halmos calls the two matrices similar. But, I can't figure out such an intuitive and geometrical explanation or example of how the second case with the relation between linear transformations works and thus, I can't explain why the two transformations are called similar. EDIT: I understood why $[C]=[A]^{-1}[B][A]$ but I didn't understand why $C=ABA^{-1} $ and why this difference between the two relation exists.","['matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
1849857,Nonexistence of a continuous injection $f:S^2 \rightarrow \mathbb{R^2}$,"What is the ""easiest"" way to show that there is no continuous injection $f:S^2 \rightarrow \mathbb{R^2}$? Sure the Borsuk-Ulam theorem implies that result, but this may be a ""difficult"" way.","['algebraic-topology', 'differential-topology', 'analysis']"
1849875,Minimize $\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1})$ by solving an eigenproblem?,"My optimization problem is
$$\min_X\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1}),$$
where $P$ is a projection matrix. I was told this could be solved as an eigenproblem: columns of $X^*$ (the solution) are eigenvectors of $(A^TA)^{-1}(I-P)$, but I failed to see why. The form looks so familiar to me that I feel I am just lacking one last bit to reach the solution. Update -- Thanks to @AWashburn, I realize my projection matrix $P$ is symmetric, and equivalently, the projection is orthogonal .","['matrices', 'eigenvalues-eigenvectors', 'optimization']"
1849877,Number of ways to arrange $n$ numbers based on their relative values to each other,"EDIT I've found a formula to solve this question, but I don't understand the reasoning behind it. Can someone explain this formula? $s(n - 1, x + y - 2) \times C(x + y - 2, x - 1)$ $s$ being Stirling number of the first kind $C$ being the combinations For x, y and n in the example question below: $s(5 - 1, 2 + 3 - 2) \times C(2 + 3 - 2, 2 - 1) = s(4, 3) \times C(3, 1) = 6 \times 3 = 18$ ORIGINAL POST I have two main questions about this Which topics do I need to know to be able to solve this question? How to apply those topics to solve this question? It is fine if you just know the answer to the 1st question. Just point me in the right direction. Abstract question How many ways are there to arrange $n$ distinct numbers so when counted from left to right there are only $x$ increasing numbers (doesn't have to be adjacent) and when counted from right to left there are only $y$ increasing numbers (doesn't have to be adjacent)? $n \geq 1$ $1 \leq x \leq n$ $1 \leq y \leq n$ Example question How many ways are there to arrange n=5 stakes of different heights in a line so when looked from the left of the line you can only see x=2 stakes, when looked from the right of the line you can only see y=3 stakes (because they get behind of the long ones)? Explanation Let's say stakes are the height of 1cm, 2cm, 3cm, 4cm and 5cm . Arranging the stakes like this 1cm 5cm 2cm 4cm 3cm would meet the given condition. Because if you look from the left of the line you can only see 1cm and 5cm (hence x=2) stakes. If you look from the right of the line you can only see 3cm, 4cm and 5cm (hence y=3) stakes. The answer of this question is 18 and the possible arrangements are: 1, 5, 2, 4, 3 1, 5, 3, 4, 2 1, 5, 4, 2, 3 2, 1, 5, 4, 3 2, 5, 1, 4, 3 2, 5, 3, 4, 1 2, 5, 4, 1, 3 3, 1, 5, 4, 2 3, 2, 5, 4, 1 3, 5, 1, 4, 2 3, 5, 2, 4, 1 3, 5, 4, 1, 2 4, 1, 5, 3, 2 4, 2, 5, 3, 1 4, 3, 5, 2, 1 4, 5, 1, 3, 2 4, 5, 2, 3, 1 4, 5, 3, 1, 2 Related questions Number of ways to arrange people in two rows, so that nobody stands behind or to the right of someone taller How many ways to line up n objects with distinct heights","['permutations', 'combinatorics', 'stirling-numbers', 'combinations']"
1849878,How did Euler prove the partial fraction expansion of the cotangent function: $\pi\cot(\pi z)=\frac1z+\sum_{k=1}^\infty(\frac1{z-k}+\frac1{z+k})$?,"As far as we know, Euler was the first to prove $$ \pi \cot(\pi z) = \frac{1}{z} + \sum_{k=1}^\infty \left( \frac{1}{z-k} + \frac{1}{z+k} \right).$$ I've seen several modern proofs of it and they all seem to rely either on the Herglotz trick or on the residue theorem. I recon Euler had neither nor at his disposal, so how did he prove it? Added : Did Euler prove it for complex $z$ or just reals?","['math-history', 'partial-fractions', 'reference-request', 'sequences-and-series', 'analysis']"
1849888,The cardinality of the classic Hilbert space,"Question. The classic Hilbert space consists of all infinite sequences $(x_n)$ of real numbers, called points, for which the series $x_1^2+x_2^2+\cdots $ converges. Show that the classic Hilbert space contains just as many points as the real in $\mathbb R$. Proof says $c\le \vert H\vert \le c^{\aleph_0} =(2^{\aleph_0})^{\aleph_0}=2^{\aleph_0 \aleph_0}=c. $ Why is $c\le \vert H\vert \le c^{\aleph_0}$ so trivial that the book adds no explanation to it?",['elementary-set-theory']
1849899,"Problem 5, Chapter 3 from Stein and Shakarchi's ""Real Analysis"" on a version of the FTC","The problem reads: Suppose that $F$ is continuous on $[a,b]$, $F'(x)$ exists for every $x\in(a,b)$, and $F'(x)$ is integrable. Then $F$ is absolutely continuous and
  $$F(b)-F(a)=\int_a^b F'(x)\,dx.$$
  [Hint: Assume $F'(x)\geq 0$ for a.e. $x$. We want to conclude that $F(b)\geq F(a)$. Let $E$ be the set of measure zero of those $x$ such that $F'(x)<0$. Then according to Exercise 25, there is a function $\Phi$ which is increasing, absolutely continuous, and for which $(D^+\Phi)(x)=+\infty$, $x\in E$. Consider $F+\delta\Phi$ for each $\delta$ and apply the result (a) in Exercise 23.] Result (a) in Exercise 23 is that if $F$ is continuous on $[a,b]$ and $(D^+F)(x)\geq 0$ for every $x\in[a,b]$ then $F$ is increasing on $[a,b]$. S&S have defined
$$(D^+F)(x)=\limsup_{h\to 0^+} \frac{F(x+h)-F(x)}{h}.$$ My question: I am able to prove what they suggest in the hint, but I don't see how it connects to the original problem. Why can I assume that $F'(x)\geq 0$ for a.e. $x$? And even then, knowing $F(b)-F(a)\geq 0$ is a far cry from knowing its exact value. Rudin proves this theorem in Chapter 7 of his ""Real and Complex Analysis"" by an entirely different proof, but I would like to understand what S&S are suggesting here. Any reference or hint is welcome.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1849932,Is every relation which is transitive and symmetric also reflexive? [duplicate],"This question already has answers here : Examples and Counterexamples of Relations which Satisfy Certain Properties (2 answers) Closed 3 years ago . I have seen a proof that every relation which is symmetric and transitive is also reflexive. if $A=\{1,2,3\}$ Then if $R=\{(1,2)(2,1)(1,1)\color{blue}{(2,2)}\}$ here $R$ is symmetric and transitive on $A$ but not reflexive right? Can anyone clear up this confusion for me?","['algebra-precalculus', 'relations', 'elementary-set-theory']"
1849965,Complex inverse function,"I've got a problem when solving an inverse function. Usually when I have a basic function and trying to find its inverse is not a problem. I just solve for X and find it. But now I've got a more complicated one and I don't know how to start. If the inverse function is $$f^{-1}(x+2) = \frac{3x-2}{2x+3}$$ I have to find $$f\left( \frac{1}{x-4} \right).$$ I really don't know where to start. I would know to solve it if it was simple as just f(x) but this is more complex. If someone could show me step by step it would really help me.
Thank you ! :D","['functions', 'inverse']"
1849974,"Between $2$ consecutive roots of $f'$, there is at least one root of $f$","Prove that between $2$ consecutive roots of $f'$, there is at least one root of $f$. I understand that a root of $f'$ represents an extreme point. But, for example, $f(x) = \sin(x)+2$ has no roots, but its derivative, $\cos(x)$, has lots of consecutive roots. Ok, while I was writing this, I realized that its no ""at least"" but ""there is at most "" one root of $f$. So, I understand that, between $2$ consecutive maximum points, for example, there can be one root, but if that function tries to come back and make another root between the two max poits, it's gotta create a local maximum point between them. But how do I write this mathematically? Let me try: By Rolle's, between $2$ consecutive roots $f(a) = f(b)$, there must be a point $c\in [a,b]$ where $f'(c) = 0$, which is a maximum point. Or maybe, can I say the following: between two roots of $f'(x)$, let's say, $f'(m) = f'(n)$ by rolles theorem we have: $$\exists c\in [m,n] / f''(c) = 0$$ so there's a maximum point between the roots, but I don't know how to prove that this is the only maximum point, and that this maximum point leads to only one root.","['algebra-precalculus', 'real-analysis', 'calculus', 'functions']"
1849979,8 people in 4 teams with different pairs in each team each day for 7 days without repeated pairs or anyone being in the same within 3 days,"Ok I am a Scout Leader and on our 7 day summer camp we have 8 Leaders and will have the Scouts in 4 different patrols or teams. I want to set up a rota for the Leaders so that they can be assigned to help the Scout Patrols for a day in pairs . I want all the Leaders to work with every other Leader for a day which I know is possible as there are 28 different pairings with 8 people and helpfully there are 7 days and 4 patrols making 28 spaces to allocate to the 28 pairs. I also want to make sure each Leader helps with each Patrol at least once, again this is not too hard to achieve (its basically an 8 team round robin tournament at 4 grounds) but the stumbling block I am having is that ideally I do not want any Leader to help with the same Patrol without at least a 2 day gap (e.g. help Monday and not back there until Thursday). I am not sure this is possible but I certainly do not want any Leader helping with the same patrol on consecutive days if i can help it. I do have a Maths degree but its well over 10 years since I completed it now and I do not work in Mathematics so I am unfortunately very rusty on this kind of thing but I am sure there must be a logical way of solving this rather than just trial and error. Sorry if I have posted in the wrong place or not made my question clear enough",['combinatorics']
1850003,"Evaluating $\int_0^1\int_0^1 e^{\max\{x^2,y^2\}\,}\mathrm dx\,\mathrm dy$","The integral again for convenience is
$$
I=\int_0^1\int_0^1 e^{\max\{x^2,y^2\}}\,\mathrm dx\,\mathrm dy
$$
My thoughts:
Ignoring for a moment that the region is a rectangle, I hoped moving to polar coordinates might help. This gives
$$
I=\int_0^1\int_0^{2\pi}re^{r^2\max\{\cos^2 t,\sin^2t\}} \, \mathrm dt \, \mathrm dr
$$
Then since $|\cos t|\geq |\sin t|$ for $t\in D_1=[-\frac{\pi}{4},\frac{\pi}{4}]\cup [\frac{3\pi}{4},\frac{5\pi}{4}]$ but not for $t\in D_2=[\frac{\pi}{4},\frac{3\pi}{4}]\cup [\frac{5\pi}{4},\frac{7\pi}{4}]$
I think we can break $I$ into 
$$
I=\left[\int_0^1\int_{D_1}re^{r^2\cos^2 t}\,\mathrm dt\,\mathrm dr\right] \left[\int_0^1\int_{D_2}re^{r^2\sin^2 t}\,\mathrm dt\,\mathrm dr\right]
$$
Aside from the problem of the region not being the same, I am stuck here. Is the work above on the right track? How do I evaluate for $[0,1]\times[0,1]$? Thanks!","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
1850034,derivative transpose,"I'm reading the book ""The Elements of Statistical Learning - Data Mining, Inference, and Prediction"" chapter 3 and there comes a simple derivation that I don't understand: We have:                   $$(1): RSS(\beta) = (y-X\beta)^T(y-X\beta)$$ with input X is N × (p + 1) matrix, (p+1) column is 1, $X_1$,$X_2$, ... $X_p$; y is N-vector output. The question is when differentiating with  respect to $\beta$ why we obtain this ?
$$(2):  \frac{\partial RSS}{\partial \beta} = -2X^T(y-X\beta)  $$ 
An answer that explain how to differentiate RSS with respect to $\beta$ will be highly appreciated","['matrices', 'transpose', 'linear-regression', 'derivatives']"
1850049,Intuitive explanation of a stochastic PDE,"Lindgren et al 2011 connects Gaussian Markov Random Fields (which have fast calculation properties due to the Markov attribute) and Gaussian Processes (which can model many types of data). The connection rests upon the fact (from Whittle 1954) that solutions to a certain stochastic partial differential equation (SPDE) defined below have a Matérn covariance (common in Gaussian Processes). They then show that some models with defined Markov properties (like on a lattice, but they extend it to off-lattice data) are solutions to that SPDE and so all the fast calculations (such as the precision matrix) that can be done on Markov models lead to desired covariance properties. So we can do some GP calculations very quickly with this technique. My questions are about the SPDE itself:
$$
(\kappa^2 - \Delta)^{\alpha/2}x(\mathbf{u}) = \mathcal{W}(\mathbf{u})
$$ where $\Delta = \sum \frac{\delta^2}{\delta x_i^2}$ is the Laplacian,  $\mathcal{W}$ is a white-noise process, $\alpha/2$ is an integer, and $\kappa$ is a constant that represents the inverse ""range"" of the covariance (something like a persistence length). What does this very weird equation represent? I hate to pull an ""I don't get it"" so here are some specific questions: On the LHS we have the Laplacian operator, which is the divergence of the gradient. What does a PDE with this operator imply about the solution? E.g. ""$dx/dt = a$ means that x changes with speed $a$."" On the RHS we have a stochastic white noise process $\mathcal{W}$. How is this different from putting something deterministic here? In the paper they call this ""driving the SPDE with white noise"" but I don't know what driving means in this context. They mention in the paper the relationship of this equation to diffusion. It would be helpful to flesh out that connection. They further extend this model to non-stationary fields with a slightly modified SPDE: $$
(\kappa^2(\mathbf{u}) - \Delta)^{\alpha/2}\left\{\tau(\mathbf{u})x(\mathbf{u})\right\} = \mathcal{W}(\mathbf{u})
$$ Where functions $\kappa^2(\mathbf{u})$ and $\tau(\mathbf{u})$ vary throughout space. They show this ALSO has ""local"" Matérn covariance but globally could be a dense covariance with interesting global correlations. How does this relate to the intuitive picture from the simpler equation?","['stochastic-processes', 'stochastic-pde', 'ordinary-differential-equations', 'partial-differential-equations']"
1850050,Infinitely Concatenated Sine and Cosine,"Using a graphing calculator, if one concatenates sine and cosine repeatedly, i.e. $$y=\sin(\cos(\sin(\cos(x))))$$ the graph appears to approach a horizontal line, suggesting that at infinite concatenation, there is a single value of the function for all $x$.  Is this correct?  If so is this value known?","['algebra-precalculus', 'limits']"
1850069,Prove on Incenter and mid point.,"Let the incircle (with center $I$) of $\triangle{ABC}$ touch the side $BC$ at $X$, and let $A'$ be the midpoint of this side. Then prove that line $A'I$ (extended) bisects $AX$.","['circles', 'euclidean-geometry', 'triangles', 'geometry']"
1850073,Prove: $||x|-|y||\leq |x+y|$ [duplicate],"This question already has answers here : Prove that $||x|-|y||\le |x-y|$ (7 answers) Closed 7 years ago . I have to prove: $||x|-|y||\leq |x+y|\leq |x|+|y|$ I have already proved $|x+y|\leq |x|+|y|$. I saw proofs for $||x|-|y||\leq |x-y|$, can I use the proof for  $||x|-|y||\leq |x-y|$ and just add that $|x-y|\leq|x+y|$?",['algebra-precalculus']
1850082,Deriving of formula for finding the length of median,In the below image $AD$ is the median of $\triangle ABC$ We know that $m_A = \frac 1 2 \sqrt{2b^2 + 2c^2 - a^2} $ But can someone tell me how it's derived !! I am just unable to think of it !! But I assume it muste be linked with the Appolonius Theorem !!,"['triangles', 'geometry']"
1850087,"If the limit of a $L^2$ sequence is in $L^\infty$, is the sequence bounded in $L^\infty$?","Let $f_n \to f$ in $L^2$ on a bounded domain. We know that $f \in L^\infty$. Does it follow that $\lVert f_n \rVert \leq A$ for a constant $A$ independent of $n$, for a subsequence if necessary? I think it is true since the functions get closer to the limit $f$, and at worst, $f_1$ is the furthest away. The sequence cannot oscillate due to the pointwise a.e. convergence for a subsequence.","['functional-analysis', 'lp-spaces']"
1850101,Function that is both midpoint convex and concave: $f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$,"Which functions $f:\mathbb{R} \to \mathbb{R}$ do satisfy Jensen's functional equation $$f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$$ for all $x,y \in \mathbb{R}$ ? I think the only ones are of type $f(x) = c$ for some constant $c\in \mathbb{R}$ and the solutions of the Cauchy functional equation $f(x+y) = f(x)+f(y)$ and the sums and constant multiples of these functions. Are there other functions which are both midpoint convex and concave?","['real-analysis', 'functions', 'functional-equations']"
1850110,Solve a nonlinear system of coupled differential equations,"I have this system of differential equations which describes the motion of a missile launcher model with 5 degrees of freedom: (1)$$(m_w +m_v +m_p)\ddot{y}_w - (m_v + m_p)h_v\ddot{\vartheta}_w \sin(\vartheta_w) 
+ m_p \xi_p \ddot{\vartheta}_v \cos(\vartheta_v) \\
- (m_v + m_p)h_v\dot{\vartheta}^2_w \cos(\vartheta_w) 
- m_p\xi_p \dot{\vartheta}^2_v \sin(\vartheta_v) + m_p \dot{\xi}_p \dot{\vartheta}_v \cos(\vartheta_v) 
+k_{w11}\lambda_{w11} \\
+ k_{w12}\lambda_{w12} -c_{w11}\dot{\lambda}_{w11} -c_{w12}\dot{\lambda}_{w12} = -(m_w +m_v +m_p)g$$ (2)$$[I_w+I_{p_{zp}}+I_v+(m_v+m_p)h^2_v]\ddot{\vartheta}_w 
+ [I_{p_{zp}}+m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\
- \sin\vartheta_w\cos\vartheta_v)]\ddot\vartheta_v 
- (m_v + m_p)h_v\ddot{y}_w \sin\vartheta_w - m_p h_v \ddot{\xi}_p(\cos\vartheta_w\cos\vartheta_v \\
+ \sin\vartheta_w\sin\vartheta_v) 
+ 2m_p h_v \dot{\xi}_p \dot{\vartheta}_w \sin\vartheta_w\cos\vartheta_v \\
-2m_p h_v \dot{\xi}_p \dot{\vartheta}_v \sin\vartheta_w\cos\vartheta_v 
+m_p h_v \xi_p \dot{\vartheta}^2_v(\cos\vartheta_w\cos\vartheta_v + \sin\vartheta_w\sin\vartheta_v)\\ +k_{w11}l_{w1}\lambda_{w11} + k_{w12}l_{w2}\lambda_{w12} - k_{w31}\lambda_{w31} 
-c_{w11}l_{w1}\dot{\lambda}_{w11}+c_{w12}l_{w2}\dot{\lambda}_{w12} \\
+ c_{w31}\dot{\lambda}_{w31} = (m_v + m_p)gh_v\sin(\vartheta_w+\vartheta_{wst})$$ (3)$$(I_v + I_{p_{zp}} + m_p\xi^2_p)\ddot{\vartheta}_v + m_p \xi_p \ddot{y}_w\cos\vartheta_v 
+ [I_v + I_{p_{zp}} + m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\
- \sin\vartheta_w\cos\vartheta_v)]\ddot{\vartheta}_w + 2m_p\xi_p\dot{\xi}_p\dot{\vartheta}_v 
+ m_p h_v\xi_p\dot{\vartheta}^2_w(\sin\vartheta_w\sin\vartheta_v \\
+ \cos\vartheta_w\cos\vartheta_v)+k_{w31}\lambda_{w31}-c_{w31}\dot{\lambda}_{w31} = 
-m_pg\xi_p\cos(\vartheta_v + \vartheta_{vst})$$ (4)$$ m_p\ddot{\xi}_p-m_ph_v\ddot{\vartheta}_v(\cos\vartheta_w\cos\vartheta_v 
+ \sin\vartheta_w\sin\vartheta_v) \\
+ m_p\ddot{y}_w\sin\vartheta_w + m_ph_v\dot{\vartheta}^2_w(\sin\vartheta_w\cos\vartheta_v - \cos\vartheta_w\sin\vartheta_v) \\
+ m_p\xi_p\dot{\vartheta}^2_v = -m_pg\sin(\vartheta_v + \vartheta_{vst}) + P_p$$ (5)$$I_{p_{xp}}\ddot{\varphi}_p = M_p$$. The scheme of the model is this: scheme of the launcher . I know all the coefficients but I want to see the trend in the first 2 seconds of the variables $y_w, \vartheta_w, \vartheta_v, \xi_p, \phi_p$. Unfortunately I tried with Matlab ode45 but the system is coupled and nonlinear, can you suggest me a numerical method, a code, or anything that may help me solve this system? After understanding the procedure I can do some more advanced calculations but I need to figure out the approach to solve it first. Thanks, really thanks, to anybody who wants to help. Loris EDIT: same equations but with numbers and with linearised sines and cosines so that you can clearly see where is the difficulty: (1.1)$$\ddot{y}_w = 0.0917\ddot{\vartheta_w}\vartheta_w -0.1667\xi_p\ddot{\vartheta_w} +0.0917\dot{\vartheta_w}^2 + 
    0.1667\xi_p\dot{\vartheta_v}^2\vartheta_v -0.1667\dot{\xi_p}\dot{\vartheta_v} -4166.67(2y_w +0.3916\vartheta_w)+
    4.1667*(2\dot{y}_w +0.3916\dot{\vartheta_w}) + 9.81$$ (2.1)$$\ddot{\vartheta_w} = -0.3217\ddot{\vartheta_v} -0.4531\xi_p\ddot{\vartheta_v}(\vartheta_v-\vartheta_w)
    +1.2082\ddot{y}_w\vartheta_w +0.4531\ddot{\xi}_p(1+\vartheta_w\vartheta_v) 
    -0.9602\dot{\xi}_p\vartheta_w(\vartheta_w-\vartheta_v) -0.4531\xi_p\dot{\vartheta_v}^2(1+\vartheta_w\vartheta_v)
    -4.393\cdot 10^4y_w -1.72\cdot 10^4\vartheta_w +366(\vartheta_v-\vartheta_w) +21.5\dot{y}_w +21.78\dot{\vartheta_w} +36.6(\dot{\vartheta_w}-\dot{\vartheta_v}) +11.8529\vartheta_w$$ (3.1)$$\ddot{\vartheta_v} = -3.9734(\xi_p^2\ddot{\vartheta_v}+\xi_p\ddot{y}_w) -\ddot{\vartheta_w}
    -0.8197\xi_p(\vartheta_v-\vartheta_w)\ddot{\vartheta_w} -7.9468\xi_p\dot{\xi}_p\dot{\vartheta_v} -0.8197\xi_p\dot{\vartheta_w}^2(1+\vartheta_w\vartheta_v)
    -662.23(\vartheta_v-\vartheta_w) -66.223(\dot{\vartheta_v}-\dot{\vartheta_w}) +38.98\xi_p$$ (4.1) $$\ddot{\xi}_p = 0.2063\ddot{\vartheta_v}(1+\vartheta_w\vartheta_v) -\ddot{y}_w\vartheta_w
    -0.2063\dot{\vartheta_w}^2(\vartheta_w-\vartheta_v) -\xi_p\dot{\vartheta_v}^2 -9.81\vartheta_v +333$$ (5.1) $$\ddot{\phi}_p = 81$$","['coupling', 'ordinary-differential-equations', 'nonlinear-system', 'systems-of-equations']"
1850231,Complex representation of a quaternionic matrix,"It is evident that right module $\mathbb{H}^n$ is $\mathbb{C}$-linearly isomorphic to $\mathbb{C^{2n}}$ with corresponding isomorphism   $\nu :  \mathbb{C^{2n}} \to\mathbb{H}^n   $ given by $ \nu(a,b) = a + b\mathrm{j}$. This naturaly gives  representation for any quaternionic matrix $M \in \mathcal{M}^{n \times m}(\mathbb{H}) $ with two complex matrices $A,B \in \mathcal{M}^{n \times m}(\mathbb{C})$ as $M = A + B\mathrm{j}$. It's assumed that  complex matrix representing $\nu^{-1}M\nu$ in parallel with complex representation of quaternion numbers can be written in  the form $$
\theta_{n,m}(M) = \theta_{n,m}(A+B\mathrm{j}) =
 \left[\begin{matrix} A & B \\ -\overline B & \overline{A} \end{matrix}\right]$$
where $\overline{A}$ is a complex conjugate. However, what i don't understand is there this conjugation came from and I need your help. When I write
$$ \nu^{-1}M\nu(a,b) = \nu^{-1}(A +B\mathrm{j})(a + b\mathrm j) =\nu^{-1}\left(Aa + Ab\mathrm{j} + B\overline{a}\mathrm{j} -B\overline b\right) = \left( Aa - B\overline{b}, Ab + B\overline a \right) $$
I don't have any idea what to do with conjugates to show that this map even linear.","['quaternions', 'modules', 'linear-algebra', 'lie-groups']"
1850242,"$x_1 = 2$, $x_{n + 1} = {{x_n(x_n + 1)}\over2}$, what can we say about $x_n \text{ mod }2$?","Let$$x_1 = 2, \quad x_{n + 1} = {{x_n(x_n + 1)}\over2}.$$What can we say about the behavior of $x_n \text{ mod }2$? Is there an exact formula for $x_n \text{ mod }2$?","['number-theory', 'modular-arithmetic', 'sequences-and-series', 'elementary-number-theory']"
1850251,When is the contraction of a maximal ideal still maximal?,"Let $k$ be an algebraically closed field, $F$ a subfield of $k$, $A$ a reduced, finitely generated $k$-algebra, and $A_0$ a finitely generated $F$-subalgebra of $A$ for which $A = k \otimes_F A_0$. Let $\mathfrak m$ be a maximal ideal of $A$.  Then $\mathfrak m \cap A_0$ is a prime ideal of $A$.  Here is my very general question: When is $\mathfrak m \cap A_0$ a maximal ideal of $A$? Another point of view would be to use the fact that since $k$ is algebraically closed, there is a unique $k$-algebra homomorphism $\phi: A \rightarrow k$ whose kernel is $\mathfrak m$.  Then $\mathfrak m \cap A_0$ is the kernel of the restriction of $\phi$ to $A_0$.  So equivalently the question I'm asking is when is $\phi(A_0)$ a field (necessarily a finite extension of $F$, by Zariski's lemma). Here is a small result: Let $A_0 = F[X], A = k[X]$.  Let $\mathfrak m = (X - x)$ be any maximal ideal of $A$.  Then $\mathfrak m \cap A_0$ is a maximal ideal of $A$ if and only if $x$ is algebraic over $F$.  Otherwise, $\mathfrak m \cap A_0$ is zero. Proof: If $x$ is algebraic over $F$ with minimal polynomial $\mu$, then $\mu A_0$ is a maximal ideal of $A_0$ contained in $\mathfrak m \cap A_0$, so the inclusion $\mu A_0 \subseteq \mathfrak m \cap A_0$ is an equality. Conversely, suppose that $\mathfrak m \cap A_0$ is a maximal ideal of $A_0$.  Since $A_0$ is a PID, and we know that $\mathfrak m \cap A_0$ is prime, this is the same as saying that $\mathfrak m \cap A_0 \neq 0$.  So there exists a polynomial $f(X) \in k[X]$ such that $(X-x)f(X) \in F[X]$.  This is a polynomial in $F[X]$ of which $x$ is a root, so $x$ is algebraic over $F$. $\blacksquare$ The next step would be to consider $A_0 = F[X_1, ... , X_n], A = k[X_1, ... , X_n]$, and $\mathfrak m = (X_1 - x_1, ... , X_n - x_n)$.  A natural thing to expect would be that $\mathfrak m \cap A$ is a maximal ideal of $A_0$ if and only if all the $x_i$ are algebraic over $F$.  I have not been successful in proving this yet, but it may follow from using induction and some facts about flat extensions of rings.  Maybe a more general result to expect would be that the height of $\mathfrak m \cap A_0$ is $$ n - \textrm{tr. deg}(F[x_1, ... , x_n]/F)$$ The final step would be to consider quotients $$A = k[X_1, ... , X_n]/I, A_0 = F[X_1, ... , X_n]/I \cap F[X_1, ... , X_n]$$ where $I$ is an ideal of $A$ which is extended from an ideal of $F[X_1, ... , X_n]$.  Up to noncanonical isomorphism, I believe such extensions $A_0 \rightarrow A$ cover all pairs $(A,A_0)$ as I described in the beginning of the question.","['algebraic-geometry', 'commutative-algebra']"
1850253,A few questions on the Gaussian integers,"I have a few questions surrounding the Gaussian integers, which I hope can be answered together in one fell swoop. The Gaussian integers are defined as $\mathbb{Z}[i] = \{x + iy : x, y \in \mathbb{Z}\}$. What is the intuition for working with them, and why should we care about them? What is arithmetic like in $\mathbb{Z}[i]$? Are there ""prime numbers"" in $\mathbb{Z}[i]$? Do Gaussian integers factor into primes? If so, do they factor uniquely?","['gaussian-integers', 'abstract-algebra', 'algebraic-number-theory', 'number-theory', 'ring-theory']"
1850260,Abstract algebraic definition of dual tangent spaces,"I know that if $(M,\mathcal{A})$ is a smooth manifold, the dual tangent space at $p\in M$ can be defined as $$ T^*_pM=I_p/I_p^2, $$ where $I_p$ is the ideal of the ring $C^\infty(M)$ consisting of smooth functions that vanish at $p$ and $I_p^2$ is the second power of this ideal. I understand that this definition is useful, because, unlike other definitions, this one can be generalized to situations when you don't have the smooth structure $\mathcal{A}$, however this definition is so unintuitive I have a hard time grasping it. According to wikipedia, the product of ideals $A$ and $B$ is defined as $$ AB=\{a_1b_1+...a_nb_n|\ a_i\in A,b_i\in B,n\in\mathbb{N}\}, $$ I assume the point of this definition is that by demanding that the functions vanish, we make sure that the functions' Taylor expansion has no zeroth order terms, and since the elements of the product ideal are second order expressions, I assume the quotient is needed to get rid of second order terms. Seems logical, since by the usual definition we can infer that the contangent space is generated by differentials of functions, which are, of course, the first order part. Am I correct in this? If I am correct on the first point, what about higher than second order terms? Shouldn't we need to quotient those out too? Thinking about it as I write this post, elements of $I_p^2$ are second order algebraic expressions from $C^\infty(M)$, but they are not polynomials, a Taylor expansion, however is polynomial. I think I don't understand it now. This line of thought seems to depend on the functions having Taylor expansions. However I know this definition is generalizable to algebraic varieties as well as locally ringed spaces in general. Without having a structure that allows Taylor expansions, how do we know that this grasps the concept of ""functions having the same first order behaviour at $p$"" for those cases as well? How can we see that this quotient space is finite $n$ dimensional? If the proof is not particularily pleasant, I don't expect anyone to actually post it here, but a reference to a manifold theory textbook that utilizes this definition heavily enough to also contain related proofs would be nice. I would appreciate any response, even if it doesn't address these points directly, if it can help me see intuitively why this particular quotient space has the same meaning as the usual definition of dual tangent space. EDIT: Since there have been misunderstandings, I wish to clarify: In my last bullet point I am not asking how to prove that the tangent/cotangent space is $n$ dimensional for an $n$ dimensional manifold. I am asking how to see that the space $I_p/I_p^2$ is $n$ dimensional, or alternatively, how to see that it is isomorphic to the cotangent space (defined by any alternative means).","['sheaf-theory', 'differential-geometry', 'algebraic-geometry']"
1850268,$P^{-1}A^{-1}B P$ diagonal implies $P^\top A P$ and $P^\top B P$ diagonal?,"Consider two symmetric positive-definite real matrices $A,B$. $A$ is hence invertible. Denote by $P$ the matrix of the column-eigenvectors of $A^{-1}B$ such that $P^{-1}A^{-1}BP=D$ where $D$ is a diagonal matrix of the eigenvalues of $A^{-1}B$. $A^{-1}B$ is symmetric and real, hence can be diagonalized in an orthogonal basis, but this does not mean that $P$ is necessarily orthogonal. It seems that $P^\top A P$ and $P^\top B P$ are diagonal. Is it always true, and if so, why? What is not clear to me is the meaning of $P^\top$ instead of $P^{-1}$. I wonder whether this is related to a change of basis (for example, the change of basis of the bilinear forms defined by $A$ and $B$). Edit I think this is true only if $A^{-1}B$ has distinct eigenvalues. Indeed, let $v_i$ denote an eigenvector of $A^{-1}B$ for $\lambda_i>0$ (because it is PD), then $A^{-1}B v_i=\lambda_i v_i$ or $Bv_i=\lambda A v_i$ so that $v_j^\top B v_i=\lambda_i v_j^\top A v_i$ and similarly $v_i^\top B v_j=\lambda_j v_i^\top A v_j$. Substracting one equality to the other:
$$ (\lambda_j-\lambda_i)v_i^\top A v_j=(\lambda_j-\lambda_i)v_i^\top \dfrac{1}{\lambda_j}  B v_j.$$ Hence, if $\lambda_i\neq\lambda_j$, necessarily $v_i^\top A v_j=v_i^\top  B v_j=0$ or $V^\top A V$ and $V^\top B V$ are diagonal, where $V$ is a matrix of columns the $v_i$. Does it seem correct?","['matrices', 'symmetric-matrices', 'bilinear-form', 'linear-algebra']"
1850320,Exact value of $\cos^2(\frac{\pi}{8})+\sin^2(\frac{15\pi}{8})?$,I tried separating it into $\cos^2(\frac{\pi}{8})+\sin^2(\frac{\pi}{8}+\frac{14\pi}{8})?$ and using the angle sum identity but it didn't help.,"['algebra-precalculus', 'trigonometry']"
1850341,How do we prove that $\int_{0}^{1}x^n\left({1\over \ln{x}}+{1\over 1-x}\right)dx={\gamma+\ln(1+n)-H_n}$,"How do we prove that n $\in$ $\Re$ $$\int_{0}^{1}x^n\left({1\over \ln{x}}+{1\over 1-x}\right)dx=\color{red}{\gamma+\ln(1+n)-H_n}.\tag1$$ $$J=\int_{0}^{1}{x^n\over 1-x}dx=\sum_{k=0}^{\infty}(-1)^k\int_{0}^{1}x^{n+k}dx=\sum_{k=0}^{\infty}{(-1)^k\over n+k+1}\tag2$$ This integral seem difficult to evaluate, help needed. Thank! $$\int_{0}^{1}{x^n\over \ln{x}}dx\tag3$$","['integration', 'definite-integrals', 'sequences-and-series', 'calculus']"
1850372,nef Line bundles over Kähler manifolds,"I am trying to understand a particular property of the first Chern class of a nef line bundle over a  Kähler manifold. We know in general, let $X$ be a complete complex projective variety, and $L$ a line bundle over it. Then $L$ is called nef if $$\int_Cc_1(L)\geq0$$ where $C$ is any irreducible complete curve on $X$. Then is it true that $c_1(L)$ can is a limit of Kähler forms in $H^2(X,\mathbb{Q})$? I appreciate any comments and answers. Thanks in advance!","['algebraic-topology', 'kahler-manifolds', 'differential-geometry', 'algebraic-geometry']"
1850376,Coloring the pentagonal hexecontahedron,"So, I'd like to color the pentagonal hexecontahedron in a way that is satisfying aesthetically and mathematically. For me this equates to, in order of priority -
1. No same-colored faces can share an edge
2. We must use as few colors as possible
3. We must have as much symmetry as possible Here's a 5-colored pentagonal hexecontahedron - 5 colors is a lot, but it's nice that there are apparently no adjacent faces with the same color. Not clearly very symmetrical though (if it is, it's obscured by the large number of colors). By the four-color theorem, we know it can be done with 4. I know it can't be done with 3, proof below. So, the aesthetics question boils down to ""how symmetrical a 4-coloring can we make?"". By the way, it is possible to color the deltoid hexecontahedron with only three colors, without same colored faces touching, and with chiral octahedral symmetry. Here's a lovely piece of origami designed by Tomoko Fuse that does this . Answer this question and you will be credited in a documentary about virus structure! I'm coloring a Satellite Tobacco Mosaic Virus EDIT so the origami one I show is actually quite symmetrical! But still not hugely useable since you (I) have to stare at it for a while to see it.","['polyhedra', 'combinatorics', 'coloring', 'biology']"
1850381,Differential Geometry for General Relativity,"I'm going to start self-studying General Relativity from Sean Caroll's Spacetime and Geometry: An Introduction to General Relativity. I'd like to have a textbook on Differential Geometry/Calculus on Manifolds for me on the side. I do like mathematical rigor, and I'd like a textbook whose focus caters to my need. Having said that, I don't want a exchaustive mathematics textbook (although I'd appreciate one) that'll hinder me from going back to the physics in a timely manner. I looked for example at Lee's textbook but it seemed too advanced. I have done courses on Single and Multivariable Calculus, Linear Algebra, Analysis I and II and Topology but I'm not sure what book would be the most useful for me given that I have a knack of seeing all results formally. P.S: I'm a student of physics with a mathematical leaning.","['reference-request', 'book-recommendation', 'differential-geometry', 'general-relativity']"
1850382,"$\frac{\phi(m)}{m}$ is dense in $[0,1]$","Let $n$ be a natural number, $n \geq 2$, and let $\phi$ be Euler's function; i.e. $\phi(n)$ is the number of positive integers not exceeding $n$ and coprime to $n$. Given any two real numbers $\alpha$ and $\beta$, $0 \leq \alpha < \beta \leq 1$, prove that there exists a natural number $m$ such that $$\alpha<\frac{\phi(m)}{m}<\beta.$$ We know that $$\dfrac{\phi(m)}{m} = \prod_{p \mid m}\left(1-\dfrac{1}{p}\right).$$ How should we choose $m$ so that we can satisfy the above?",['number-theory']
1850401,Reducing an indicator function summation into a simpler form.,I want to simplify the following: $\sum_{n=0}^x 1 - (1 - I[a \ne 0] * I[a \ge b]) * (1 - I[b \ne 0] * I[b \ge c]) * (1 - I[c \ne 0] * I[c \ge d]) * (1 - I[d = 0])$,"['algebra-precalculus', 'characteristic-functions', 'number-theory', 'summation']"
1850426,Laplace Transform for Solving Differential Equation,"I solved the following task, but since I am new in this field I need to check if it is correct or if there is anything I am missing or doing wrong. Task : Solve differential equation using Laplace transform.
$$y^{''}-y^{'}-2y=2t+1 \\y^\;(0)=1, \; y^{'}(0)=2$$ First i got the following equation : $$\mathcal{L}(y)=\frac{s^3+s^2+s+2}{s^2(s^2-s-2)}$$ Now this is the part that was kinda tricky. When i fractioned equation i got this : $$\frac{A}{s}+\frac{B}{s^2}+\frac{C}{s+1}+\frac{D}{s-2}$$ The fractions were : $$A=0,\;B=-1,\;C=1,\;D=0$$ Finally the result is : $$y(t)=-t + e^{-t}$$","['ordinary-differential-equations', 'fractions', 'laplace-transform']"
