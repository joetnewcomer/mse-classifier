question_id,title,body,tags
3801191,Can the trace of a positive matrix increase under a projection?,"The question is concerned with symmetric matrices $\mathbb{S}_n$ as a real vector space. Let $X$ be a positive semidefinite symmetric matrix, and let $P : \mathbb{S}_n \to \mathcal{V}$ be a projection onto some subspace $\mathcal{V} \subset \mathbb{S}_n$ . Is it always the case that $\mathrm{trace}(P(X)) \leq \mathrm{trace}(X)$ ? I can see this being true when $\mathcal{V}$ has an orthonormal basis $\{A_i\}$ consisting of matrices which are all either trace $0$ or positive and trace $\leq 1$ . But in general I don't know anything about what kind of basis $\mathcal{V}$ would admit. Is it possible to come up with a counterexample?","['matrices', 'matrix-analysis', 'linear-algebra', 'symmetric-matrices']"
3801203,Let $G$ be a bipartite graph. Prove that $\alpha(G) = |V(G)|/2$ if and only if G has a perfect matching.,"Let $G$ be a bipartite graph. Prove that $\alpha(G) = |V(G)|/2$ if and only if $G$ has a perfect matching. $\alpha(G)$ is the independence number of $G$ , i.e. , the size of the maximum independent set of $G$ . I can't figure it out how to start this proof. I can see that this is true by looking at the picture in https://i.sstatic.net/lV4b9.png , but I can't formulate the proof. If anyone could help me writing the complete formal proof, I would be very grateful.","['graph-theory', 'matching-theory', 'bipartite-graphs', 'discrete-mathematics']"
3801214,Concerning the Proof that a Union of a sequence of Countable Sets is Countable,"Theorem 2.12 of Principles of Mathematical Analysis by Rudin states that the union S of a sequence of countable sets (E n ) is countable. In the proof, Rudin constucts the following array: and he then iterates through it to obtain the following: and he concluded by stating that if any two sets have elements in common, then they appear more than once in the above sequence therefore we take a subset T of the set of all postitive integers and that subset T is equivalent to the union. Hence it is countable. To my understanding, to establish a set is countable, we must construct a 1-1 onto map from the set of positive integers(or a subset of it, T in this case) to that set. What I assume the author does here is he constructs the following function: $$f: T ↦ S$$ $$f(n_1)=x_{11}$$ $$f(n_2)=x_{21}$$ $$f(n_3)=x_{12}$$ $$.$$ $$.$$ $$.$$ where the n's are elements of T in order. Is what I said correct? Is the function above valid? Thank you.","['elementary-set-theory', 'solution-verification', 'real-analysis']"
3801229,"Affine transformations technique (Putnam 2001, A-4)","I am trying to learn the technique of affine transformations from this article. The first question covered is question A4 on the Putnam of 2001. (Putnam 2001, A4) $\triangle ABC$ has area one. Point $E$ , $F$ , $G$ lie on $BC$ , $CA$ , and $AB$ respectively such that $AE$ bisects $BF$ at point $R$ , $BF$ bisects $CG$ at $S$ , and $CG$ bisects $AE$ at $T$ . Find the area of $\triangle RST$ . By affine transformations we can take $\triangle ABC$ to be equilateral or right-isosceles as we see fit. When $\triangle ABC$ is right, we have that $\frac{AG}{AB} = \frac{BE}{BC} = \frac{CF}{CA} = r$ . This makes sense. But then it gets crazy. Apparently, in the right-isosceles case, ""we can use the fact that $CG$ bisects $AE$ to obtain the identity $(1 - r)(1 - \frac{r}{2}) = 1/2$ "". Why is this? (Later on there are other perplexing claims such as: $\frac{CT}{CG} = \frac{1}{2(1-r)}$ and $BS = SG$ , but hopefully if I can understand how the author comes up with one of them then the others will become more apparent). I looked up the official Putnam solution and they seemed to have use the affine technique slightly differently. Solution two (of six) uses affine transformation to take $\triangle ABC$ into a particular triangle with area one (namely the one with vertices $(0,1)$ , $(1,0)$ , and $(-1,0)$ . By co-linearity of subsets of these points, we can further come up with three equations in three unknowns (these equations are not linear, but are still solvable). Plugging the values in for our three unknowns gives us the coordinates of the points $R$ , $S$ , and $T$ . We can use the Shoelace Lemma to find the area of $\triangle RST$ . Since the area of $\triangle ABC$ is already one, any affine transformation will leave the ratio of the areas of $\triangle ABC$ to $\triangle RST$ fixed. The only issue with the approach is the shear amount of computation required if one where to solve the problem by hand.","['contest-math', 'geometry', 'affine-geometry']"
3801346,Calculating the volume under the surface $z = x^2 + y^2$ and above $D$,"Given: $$D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{x \sqrt{3}}{3} \leq y \leq x \sqrt{3} \right\}$$ Calculate the Volume under the surface $$z = x^2 + y^2$$ And above $D$ . My try: $$I = \iint_D x^2 + y^2 \,dx\,dy$$ We can write $D$ as: $$D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{\sqrt{3}}{3} \leq \frac{y}{x} \leq  \sqrt{3} \right\}$$ And change to $u,v$ as the following: $$\left\{\begin{matrix}
 u = x^2 + y^2 \\ v = \frac{y}{x}
\end{matrix}\right.$$ So the set $D$ would be: $$D = \left\{(u,v) | 1 \leq u \leq 100, ~~ \frac{1}{\sqrt{3}} \leq v \leq  \sqrt{3} \right\}$$ Now, because we changed the variables, we need to calculate the Jacobian: $$ J = \frac{ D(x,y)}{D(u,v)} = \frac{1}{\frac{D(u,v)}{D(x,y)}} = \frac{1}{\begin{vmatrix}
u_x & u_y\\ 
v_x & v_y
\end{vmatrix}} = \frac{1}{\begin{vmatrix}
2x & 2y\\ 
-\frac{y}{x^2} & \frac1x
\end{vmatrix}} = \frac{1}{2+ 2 (\frac{y}{x})^2}$$ $$J = \frac{1}{2(1+v^2)}$$ And we can calculate the integral as so: $$\iint_{D_{uv}} u \cdot \frac{1}{2} \cdot \frac{1}{1+v^2} \,du\,dv = \frac{1}{2} \int_1^{100} ( \int_{ \frac{1}{\sqrt{3}}}^{\sqrt{3}} \frac{u}{1+v^2} du)dv = \frac{1}{2} \int_1^{100} \frac{1}{1+v^2} ( \frac{ (\sqrt{3})^2 - (\frac{1}{ \sqrt{3}})^2}{2})dv = \frac23 \int_1^{100} \frac{1}{1+v^2} dv = \frac23 ( \arctan{v} |_1^{100}) \approx 0.5169 \dots$$ Some people I talked with got an answer above $10,000$ and my answer is not close at all! So I ask for your help, if you can review my work, I would be so thankful!
Thanks for helping!","['integration', 'volume', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
3801361,Characterization of Plane Curves *via* Curvature $\kappa(s)$ or Equal Curvature Curves are Congruent,"My engagement with this topic was piqued by this question , in which the OP MathMan was seeking help in applying the principle that two plane curves with identical curvature function (I will make this more precise in what follows) are themselves identical ""except for probably their position in $\Bbb R^2$ "" ( sic ).  As I attempted to answer MathMan's concerns, I became more and more aware that the underlying concept was worthy of address in and of itself.  Specifically, I began to wonder just how it might be proved.  In particular, I wanted, and still want, an analysis/proof of the assertion that two curves the curvature functions of which are the same are ""congruent"" in the sense that one may be made pointwise identical to the other by a rigid motion of $\Bbb R^2$ .  In formulating a precise statement of this result, I searched math.stackexchange.com in the usual manner for related questions, but found nothing which seemed exactly on point, so I am proceeding to ask it here. Having said these things, I turn to my Question: Let $I \subseteq \Bbb R \tag 1$ be an open interval, not necesarily bounded, and let $\alpha, \beta: I \to \Bbb R^2 \tag 2$ be regular, arc-length parametrized curves with curvatures $\kappa_\alpha, \kappa_\beta: I \to \Bbb R^+ = \{r \in \Bbb R, \; r > 0 \}, \tag 3$ as defined in the Frenet-Serret equations , $\dot T_\alpha(s) = \kappa_\alpha(s) N_\alpha(s), \; \dot T_\beta(s) = \kappa_\beta(s) N_\beta(s), \tag 4$ where $N_\alpha(s)$ and $N_\beta(s)$ are the unit normal fields to $\alpha(s)$ and $\beta(s)$ , respectively.  Then if $\kappa_\alpha(s) = \kappa_\beta(s), \; \forall s \in I, \tag 5$ it follows that there is an orthogonal transformation $O$ of $\Bbb R^2$ and a vector $\vec v \in \Bbb R^2 \tag 6$ such that $\alpha(s) = O\beta(s) + \vec v, \; \forall s \in I. \tag 7$","['curvature', 'curves', 'ordinary-differential-equations', 'differential-geometry']"
3801405,The limit and asymptotic analysis of $a_n^2 - n$ from $a_{n+1} = \frac{a_n}{n} + \frac{n}{a_n}$,"I came up with the following question which is the follow up of How to prove that for $a_{n+1}=\frac{a_n}{n} + \frac{n}{a_n}$ , we have $\lfloor a_n^2 \rfloor = n$? Problem : Let $a_1 = 1,\quad a_{n+1} = \frac{a_n}{n} + \frac{n}{a_n},\quad n\ge 1$ . Prove that $\lim_{n\to \infty} (a_n^2 - n) = \frac{1}{2}$ ; Give the asymptotic analysis of $a_n^2 - n - \frac{1}{2}$ . Edit (2021/02/16) I also posted in https://mathoverflow.net/questions/384047/asymptotic-analysis-of-x-n1-fracx-nn2-fracn2x-n-2 For 1), I use the mathematical induction to prove the claim $$n + \frac{1}{2} - \frac{2}{n} < a_n^2 < n + \frac{1}{2} + \frac{13}{4n} + \frac{13}{8n^2} + \frac{157}{16n^3}, \quad n \ge 22. \tag{1}$$ However, we need to verify it for $n = 22$ (a computer is required). Are there simpler solutions ? $\color{blue}{\textbf{Edit}}$ 2021/02/15: For 1), there is a solution in [1] (I know it from @haidangel's post The variation of a Ukrainian Olympiad problem: 10982 ). The authors proved that $\frac{n^2}{n-1/2} \le a_n^2 \le \frac{(n-1/2)^2}{n-3/2}$ for all $n\ge 3$ . [1] Yuming Chen, Olaf Krafft and Martin Schaefer, “Variation of a Ukrainian Olympiad Problem: 10982”,
The American Mathematical Monthly, Vol. 111, No. 7 (Aug. - Sep., 2004), pp. 631-632 For 2), I have no idea currently. I want to find something like: for example, for the recurrence relation $b_0 = 1, b_{n+1} = b_n + \frac{1}{b_n}, n\ge 0$ ,
we have $b_n \sim \sqrt{2n} + \frac{\sqrt{2}}{8\sqrt{n}}\ln n + o(\frac{\ln n}{\sqrt{n}})$ . (Thank @ Diger for pointing out the mistake. See the comment.) About how to construct the claim (1): I want to find $d_n, c_n$ such that, for sufficiently large $n$ , $$n + \frac{1}{2} - d_n < a_n^2 < n + \frac{1}{2} + c_n.$$ To use the the mathematical induction, we need $$a_{n+1}^2 = \frac{a_n^2}{n^2} + \frac{n^2}{a_n^2} + 2
< \frac{n + \frac{1}{2} + c_n}{n^2} + \frac{n^2}{n + \frac{1}{2} - d_n} + 2
< n + 1 + \frac{1}{2} + c_{n+1},$$ $$a_{n+1}^2 = \frac{a_n^2}{n^2} + \frac{n^2}{a_n^2} + 2
> \frac{n + \frac{1}{2} - d_n}{n^2} + \frac{n^2}{n + \frac{1}{2} + c_n} + 2
> n + 1 + \frac{1}{2} - d_{n+1}$$ which results in $$c_{n+1} - \frac{c_n}{n^2} > \frac{n + \frac{1}{2}}{n^2} + \frac{n^2}{n + \frac{1}{2} - d_n} + \frac{1}{2} - n,$$ $$c_n < \frac{n^2}{n - \frac{1}{2} - d_{n+1} - \frac{n + \frac{1}{2} - d_n}{n^2}} - n - \frac{1}{2}.$$ We first choose $d_n$ , then determine $c_n$ .
For example, $d_n = \frac{2}{n}$ and $c_n = \frac{13}{4n} + \frac{13}{8n^2} + \frac{157}{16n^3}$ .","['recurrence-relations', 'asymptotics', 'sequences-and-series', 'limits', 'induction']"
3801429,"How many subsets of $\{1,2,...,n\}$ do not contain three consecutive integers?","My attempt. Let us assume $n$ is a large positive integer. ${n \choose 0},{n \choose 1},{n \choose 2}$ are the numbers of such subsets having $0,1,2$ elements respectively, which is trivial. For $3$ elements, the number of such subsets is ${n \choose 3}-(n-2)$ . Starting from $4$ elements, my brain begins muddling up; I have no idea how to proceed further systematically. Any hint or idea would be appreciated. Remark. Thanks to the partial solutions by VIVID and Masacroso, I have completely solved this problem. Following VIVID's answer, I have posted an answer which completes the solution primarily for future reference. I am going to give the accepted answer to VIVID who has been very dedicated to this problem seen from his number of edits. Also, most importantly, VIVID was the first person who posted the core part of the solution. Masacroso, hope you would not mind. Last, though this problem has been completely resolved, any new approach is always welcome.","['elementary-number-theory', 'combinatorics']"
3801432,The derivative $\frac{\mathrm d}{\mathrm dx} x^x=x^x\left(\ln x+1\right)$ is problematic for $x<0$,"To take the derivative of $x ^ x$ , we write $$\dfrac {\mathrm d}{\mathrm dx} x^x=\dfrac {\mathrm d}{\mathrm dx} e^{\ln x^x}=\dfrac {\mathrm d}{\mathrm dx} e^{x\ln x}= e^{x\ln x}× \dfrac {\mathrm d}{\mathrm dx}(x\ln x)=x^x\left(\ln x+1\right)$$ Here is my problem: If $x\in\mathbb{Z^-}$ , then $x^x\in\mathbb {R}$ . But, $\ln x \not\in\mathbb {R}.$ Because, $\ln x$ is defined only in the set of positive real numbers. If, $x \not\in\mathbb {Z^{-}}$ and $x\in\mathbb{R^{-}}$ , then $x^x\in\mathbb {C}$ and $\ln x \in\mathbb {C}.$ But, the problem occurs if $x\in\mathbb{Z^-}.$ So, $x^x=e^{x\ln x}$ doesn't hold for all real numbers. This makes the derivative result suspicious. Where is the problem?","['real-numbers', 'calculus', 'algebra-precalculus', 'derivatives', 'complex-numbers']"
3801459,"Weierstrass Factorization Theorem, infinite polynomial/infinite power series","As we know from basic complex analysis, any finite polynomial (infinite power series) $P(z)$ can be represented as a product of its zeroes. $P(z)=\Pi_n(z-z_n)$ (when $z_n $ are the zeros). And as we know, the ""Weierstrass Factorization Theorem"" is a super inclusion of the theorem that I mentioned before, and it is pretty hard to prove the Weierstrass Factorization Theorem. I tried to prove the Weierstrass Factorization Theorem when the function is an infinite power series, but  it seems more complicated than I thought. Does anyone know if there is a nice (or not) proof to Weierstrass Factorization Theorem when the function is an infinite power series?",['complex-analysis']
3801476,Prove 'dual to the dual norm is the original norm' without using Hahn-Banach theorem?,"Let $\|\cdot\|$ be a norm on $\mathbb{R}^{n} .$ The associated dual norm, denoted $\|\cdot\|_{*},$ is defined as $$
\|z\|_{*}=\sup \left\{z^{\top} x \mid\|x\| \leq 1\right\}
$$ I'm trying to prove $$\|x\|_{**} = \|x\|$$ here says we can prove it by Hahn-Banach theorem. i.e. $$\|y\|=\max _{x \neq 0} \frac{x^{T} y}{\|y\|_{*}}$$ But I think by definition, this is what we need to prove. So it seems says 'because this is correct,
this is correct'. I think it proves nothing. Are there any proof without using Hahn-Banach theorem?","['normed-spaces', 'duality-theorems', 'functional-analysis', 'dual-spaces']"
3801478,A periodic function with no fundamental period and continous at one point is constant.,"Theorm : Let $f:\mathbb{R} \to \mathbb{R}$ be a periodic function and suppose $f$ is continous at some $\zeta \in \mathbb{R}$ and that $f$ has no fundamental period then prove that $f$ is constant . My trial proof using sequences Let $\{p_n\}$ be a decreasing sequence of periods of $f$ converging to $0$ . If $f$ is not constant then $\exists $ a point $a$ such that $f(a) \neq f(\zeta)$ . Let $ a\gt \zeta$ . There exist $ m\in \mathbb{N}$ such that $0\lt p_n \lt a-\zeta, \forall n \gt m$ We choose $x_1, x_2 , ..., x_m$ as the same real number $a$ For $n\gt m$ , we select $x_n \in (\zeta, \zeta+p_n) $ such that $f(x_n)=f(a)$ which is possible by the periodicity of $f$ Clearly $x_n \to \zeta$ as $n\to \infty$ but the corresponding functional sequence $f(x_n)=f(a)\to f(a)\neq f(\zeta) $ as $n\to \infty$ thus contradicting that $f$ is continous at $\zeta$ Similar technique for $a\lt \zeta$ Thus there is no such $a$ and so the result follows. I know there are several questions like this posted here but as far as I have seen none of them use sequences. My proof looks too simple . Is everything correct or am I overlooking something? Thanks for your time.","['periodic-functions', 'real-analysis', 'continuity', 'solution-verification', 'sequences-and-series']"
3801480,"Function $f$ such that $|f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R.$","Let $f$ be a real function such that  such that $|f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R.$ Does this condition imply that $f$ will be differentiable ? If Lipschitz order is greater than $1$ then function is constant so differentiable . If Lipschitz order is equal to $1$ then $ |x|$ is counterexample for differentiablity . For this question I have no idea . Please suggest me a counterexample or proof of differentiablity hint . Thank you.","['holder-spaces', 'derivatives', 'lipschitz-functions', 'real-analysis']"
3801481,"Is this substitution, a sufficient condition to find the range of the function?","I have this function ( $x>0$ ) $$f (x)=\frac{\sqrt{g (x)}+4 x \left(x^2+1\right) \sin (\pi  x) \cos ((3+\pi ) x)}{x^4+2 x^2+1+\left(4 x^2+\left(x^2-1\right)^2 \cos (2 \pi  x)\right)}$$ where $g(x)=4 x^2+\left(x^2-1\right)^2 \cos (2 \pi  x)-\left(x^2+1\right)^2 \cos (2 (3+\pi ) x)\;$ . I want to show that the range of function $f(x)$ for those values of $x$ in which $g(x)=0\;$ is $[-1,1]$ . From $g(x)=0\;$ we have $\;4 x^2+\left(x^2-1\right)^2 \cos (2 \pi  x)=\left(x^2+1\right)^2 \cos (2 (3+\pi ) x)\;$ . The LHS is the same as one of the term in denominator of $f$ . Then, is it sufficient to substitute this term in $f$ and then check the range? Does this give the range of the function for only those values of $x$ for which $g(x)=0$ ?  If not, how can I prove that the range of function $f(x)$ for those values of $x$ in which $g(x)=0\;$ is $[-1,1]$ ? P.S. I did this way, but I see that that the range of function for
some $x_0$ is greater than one, but when I check $g(x_0)\;$ , it is
not zero!","['alternative-proof', 'functions', 'real-analysis']"
3801483,Does any inconsistency arise from these two interpretations of $\frac{df}{dx}$?,"Using limits, $\dfrac{df(z)}{dx}$ is defined as what the slope of a line joining $(x,f(x))$ and $(x+h,f(x+h))$ approaches as $h \to 0$ which can be rephrased as precisely the slope of the tangent to $f$ at $(x,f(x))$ as the secant would approach the tangent as $h \to 0$ . So : $$\dfrac{d}{dx}f(x) = \lim_{h \to 0} \dfrac{f(x+h)-f(x)}{h}$$ Now, we also say that for a very small $\Delta x$ : $$\dfrac{\Delta y}{\Delta x} = \dfrac{df}{dx} \implies \Delta y = \dfrac{df}{dx}\Delta x$$ Which is actually an approximation but the error is so ""negligibly negligible"" that it can simply be ignored. So, the two ways of interpreting $\dfrac{df(x)}{dx}$ are : Precisely the slope of the tangent to $f$ at $(x,f(x))$ [ $0$ error] The ratio of change in $f(x)$ with an extremely small change in $x$ . The second interpretation is commonly used in Physics and also helps to understand partial derivatives better (for beginners like me, at least). I want to know if these two interpretations go hand in hand or at some point, they give contradicting results and cause inconsistencies. Also, which interpretation should be followed to derive rules like the sum rule, product rule, power rule, quotient rule etc? Thanks!","['calculus', 'soft-question', 'derivatives']"
3801537,SIR model exact solution,"I try to understand how the exact solution for infectious from this article works, in order to prove that we cannot have ""two waves of infectious"" with fixed parameters. The exact solution for the infected is $ i(t)=\dfrac{\lambda}{\beta+\lambda \left(  \dfrac{\lambda-i_0\beta}{\lambda i_0 e^{\dfrac{\beta (s_0 + i_0 -1)}{\mu}}} )\right) \cdot e^{-\lambda t + \dfrac{\beta (s_0+i_0-1)}{\mu}} }$ I'm confused because it seems to me that $\beta,\lambda,\mu,i_0,s_0$ are fixed so the only term not fixed is $e^{-\lambda t +\dfrac{\beta (s_0+i_0-1)}{\mu}}$ according to $t$ . But I feel that am wrong because the graph of inctious is as a wave, not as an exponential function. My question is about where am I wrong and do you have any clue in order to prove that simple fixed SIR model infection has only one wave ? edit: typo in exponential, missed ""+""","['multivariable-calculus', 'statistics', 'proof-writing']"
3801548,Intuition behind inversions of triangles in circles leading to similar triangles,"In here the inversion of triangle $qab$ produces triangle $q \tilde{a} \tilde{b}$ but I find it a bit weird that $\angle qab = \angle q \tilde{b} \tilde{a}$ and $\angle q ab = \angle q \tilde{a} \tilde{b}$ , why exactly does the angles of triangle switch under inversion? is there something deeper that I may be missing? Picture from Tristan_Needham's visual complex analysis book","['mobius-inversion', 'geometry']"
3801549,"Minimal Manifold, $\mathbb{R}^4$",We know that all the minimal surfaces of revolution in $\mathbb{R}^3$ are the Catenoid and the euclidean plane $\mathbb{R}^2$ in $\mathbb{R}^3$ . Do we know what are the minimal surfaces of revolution in $\mathbb{R}^4$ ?,"['minimal-surfaces', 'riemannian-geometry', 'differential-geometry']"
3801641,Find $\mathbb P(\sqrt{V} \cos(\pi U)\leq c)$,"Find $$IC=\mathbb P(\sqrt{V} \cos(\pi U)\leq c),$$ where $V\sim \Gamma(\alpha , \lambda)$ and $U\sim Beta(a,b)$ , that is, $f_V(v)=\frac{1}{\Gamma(\alpha) \lambda^\alpha} v^{\alpha-1} e^{-\frac{v}{\lambda}}1_{v>0}$ and $f_U(u)=\frac{1}{Beta(a,b)}u^{a-1}(1-u)^{b-1}1_{(0,1)}(u)$ . $V$ and $U$ are independent. 1) For a special case $a=b=1$ , $\alpha=1$ and $\lambda=2$ the distribution of $\sqrt{V} \cos (\pi U)$ is standard normal. 2) For a special case $a=b=1$ .  Let $c>0$ \begin{eqnarray}
IC &=& \mathbb P(\sqrt{V} \cos(\pi U)\leq c) 
\\ &=&  \int_0^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) f_U(u) du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du
+\int_{0.5}^{1} \mathbb P(\sqrt{V} \cos(\pi u)\leq c)  du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \cos(\pi u)\leq c) du
+\int_{0.5}^{1} \mathbb 1 \quad  du
\\ &=&  \int_0^{0.5} \mathbb P(\sqrt{V} \leq \frac{c}{\cos(\pi u)}) du +\frac{1}{2}
\\ &=&  \int_0^{0.5} \frac{\gamma \left(\alpha , \frac{c^2}{\lambda^2 \cos^2(\pi u)} \right)}{\Gamma(\alpha)} du +\frac{1}{2} \tag{2}
\end{eqnarray} Where $\gamma$ is Incomplete_gamma_function . For $c<0$ the solution is similar. Is it possible to simplify (2) or have a better solution ? According to square-root-of-a-gamma-distribution , the distribution of $\sqrt{V}$ is Nakagami_distribution Any  special case is also useful. Thanks in advance for any help you are able to provide.","['definite-integrals', 'probability']"
3801646,"Solving the system $3-(y+1)^2 = \sqrt{x-y}$, $\;x+8y = \sqrt{x-y-9}$","How to solve these equations? $$\begin{cases}
3-(y+1)^2 = \sqrt{x-y}\\
x+8y = \sqrt{x-y-9}
\end{cases}$$ I've tried solving this using the substitution and elimination methods without any success. I also tried plotting these equations and I got $x = 8$ , $y = -1$ . Can someone show me the steps required to solve this? Thanks in advance.","['algebra-precalculus', 'systems-of-equations']"
3801666,"Given four real numbers $a,b,c,d$ so that $1\leq a\leq b\leq c\leq d\leq 3$. Prove that $a^2+b^2+c^2+d^2\leq ab+ac+ad+bc+bd+cd.$","Given four real numbers $a, b, c, d$ so that $1\leq a\leq b\leq c\leq d\leq 3$ . Prove that $$a^{2}+ b^{2}+ c^{2}+ d^{2}\leq ab+ ac+ ad+ bc+ bd+ cd$$ My solution $$3a- d\geq 0$$ $$\begin{align}\Rightarrow d\left ( a+ b+ c \right )- d^{2}= d\left ( a+ b+ c- d \right ) & = d\left ( 3a- d \right )+ d\left ( \left ( b- a \right )+ \left ( c- a \right ) \right )\\ 
 & \geq b\left ( b- a \right )+ c\left ( c- a \right ) \\ 
 & \geq \left ( b- a \right )^{2}+ \left ( c- a \right )^{2} \\ 
 & \geq \frac{1}{2}\left ( \left ( b- a \right )^{2}+ \left ( c- a \right )^{2}+ \left ( c- a \right )^{2} \right )\\
 & \geq \frac{1}{2}\left ( \left ( b- a \right )^{2}+ \left ( c- b \right )^{2}+ \left ( c- a \right )^{2} \right )\\
 & = a^{2}+ b^{2}+ c^{2}- ab- bc- ca
\end{align}$$ How about you ?","['inequality', 'alternative-proof', 'symmetric-polynomials', 'algebra-precalculus', 'convexity-inequality']"
3801678,What's the name for this star-like shape formed by joining rectangles to the edges of a (regular) polygon?,"Is there a proper name for this shape - these examples are all the same, just with different 'arm' counts. I'd describe the second one as a cross , but they presumably can't all be called crosses? All but the first two could qualify as an asterisk ... Would they be truncated stars ? stargons ? x-agons ? polystars ? I'm thinking they have a proper name somewhere...","['geometry', 'polygons', 'terminology']"
3801683,Prove that $\triangle ABC=\left(\triangle DEF \cdot \triangle XYZ\right)^{1/2}$,"In $\triangle ABC$ , $D$ , $E$ , $F$ are points on the sides $BC$ , $CA$ , $AB$ . Also, $A$ , $B$ , $C$ are points on $YZ$ , $ZX$ , $XY$ of $\triangle XYZ$ for which $EF \parallel YZ$ , $FD \parallel ZX$ , $DE \parallel XY$ . Prove that area of $$\triangle ABC=\left(\triangle DEF \cdot \triangle XYZ\right)^{1/2}$$ I really have no idea how to approach this question. Any help would be greatly appreciated. The only thing that I know is $\triangle DEF \sim \triangle XYZ$ . I do not know about homothety and am expected to only solve this problem using elementary techniques such as similarity, Menelaus theorem, Ceva Theorem etc. Trigonometry is also allowed.","['area', 'geometry']"
3801711,Why $\boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})\neq\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u}$?,"It is well known that for some vector field $\mathbf{u}$ the following holds: $$ \boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})=\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u}.$$ Let's consider the following vector field in cylindrical coordinates with the unit vectors $\hat{\mathbf{r}}, \hat{\boldsymbol\phi}, \hat{\mathbf{z}}$ : $$\mathbf{A}=0\hat{\mathbf{r}}+1\hat{\boldsymbol\phi}+0\hat{\mathbf{z}}.$$ For cylindrical coordinate frame we know that divergence, curl, and Laplacian are written as respectively: $$\boldsymbol\nabla\cdot\mathbf{A} = \frac{1}{r}\frac{\partial(rA_r)}{\partial r} + \frac{1}{r}\frac{\partial A_\phi}{\partial \phi} + \frac{\partial A_z}{\partial z},$$ $$\boldsymbol\nabla\times\mathbf{A}=\left(\frac{1}{r}\frac{\partial A_z}{\partial\phi}-\frac{\partial A_\phi}{\partial z}\right)\hat{\mathbf{r}}+\left(\frac{\partial A_r}{\partial z}-\frac{\partial A_z}{\partial r}\right)\hat{\boldsymbol\phi}+\left(\frac{1}{r}\frac{\partial(rA_\phi)}{\partial r}-\frac{1}{r}\frac{\partial A_r}{\partial \phi}\right)\hat{\mathbf{z}},$$ $$\nabla^2 =\frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial}{\partial r}\right)+\frac{1}{r^2}\frac{\partial^2}{\partial\phi^2}+\frac{\partial^2}{\partial z^2}.$$ On the one hand, for the left side using these formulae we have: $$\boldsymbol\nabla\times\hat{\boldsymbol{\phi}} = \frac{\hat{\mathbf{z}}}{r},$$ and $$\boldsymbol\nabla\times\frac{\hat{\mathbf{z}}}{r}=\frac{\hat{\boldsymbol\phi}}{r^2}.$$ However, on the other hand, for the right side we shall have zeros, because all partials of $1$ are equal to zero. Where am I wrong?","['multivariable-calculus', 'vector-analysis']"
3801716,How can one construct the pair of three dimensional representations of $PSL_2(\mathbb{F}_7)$ explicitly?,"The group $PSL_2(\mathbb{F}_7)\cong GL_3(\mathbb{F}_2)$ has a pair of (complex) three dimensional representations, and I am wondering if anyone knows an explicit construction of these. To clarify what I mean by explicit, we can recognise the $6$ and $7$ irreducible dimensional representations being associated to the $2$ -transitive actions on the fano plane and projective line over $\mathbb{F}_7$ respectively, and the $8$ dimensional representation is induced from a nontrivial linear rep of the index $8$ subgroup $C_7\rtimes C_3$ . Its also the Steinberg representation, which I am not really familiar with, but this also counts as an ""explicit description"". From this, one can fill out the character table and find the characters of the three dimensional reps, but given that their dimension is small, I would hope there's a nice construction of them.","['group-theory', 'representation-theory', 'characters', 'reference-request']"
3801728,"Bounds on the norm $|J'|$, where $J$ is a Jacobi field","I am trying to understand the well known formula for the norm of Jacobi fields $J$ along a geodesic $\gamma(t)$ with $J(0)=0$ , i.e. $$
J''(t)+ R(J,\gamma'),\gamma'=0.
$$ The formula $$
 (\ast) \quad g(J(t),J(t)) = O(t^2)
$$ holds on a Riemannian manifold $(M,g)$ (see e.g. here ). In this link the Jacobi equations are used to compute the first two derivatives in $t=0$ and then use a Taylor expansion to argue that the remainder is of higher order. But what I do not understand in this proof is how to make sense of the remainder because it involves $J(t)$ again: $$
(g(J(t),J(t)))''= 2(g(J''(t),J(t))+ g(J'(t),J'(t)).
$$ So a Taylor expansion yields for some $s_t \in [0,t]$ $$
g(J(t),J(t))= 0+ t\cdot 0 + t^2 \cdot   (g( R(J,\gamma'(s_t)),\gamma'(s_t),J(s_t)))+ g(J'(s_t),J'(s_t)).
$$ How do we estimate the term $g(J'(s_t),J'(s_t))$ ? For example, how do rule out that $$
g(J'(s_t),J'(s_t)) \geq \frac{1}{t}? 
$$","['taylor-expansion', 'riemannian-geometry', 'differential-geometry']"
3801735,The numbers 49/1; 49/2...49/97 are written on the board.,"The numbers $\frac{49}{1},\frac{49}{2},...,\frac{49}{97}$ are written on the board. Every time we make a move we erase two numbers from the board ( $a$ and $b$ ) and instead we write: $$2\cdot ab-a-b+1$$ After $96$ moves there is only one number left, what is it? Been struggling with this one for a while now. I thought maybe it's useful to count the sum or the sum of the reverse numbers. Or it could be done with induction but I didn't come up with anything. Thanks in advance 🙂 Also I really don't know how to tag this so please help","['algebra-precalculus', 'induction', 'discrete-mathematics']"
3801757,Solution to trigonometric system of equations?,"I am given the following system of trigonometric equations for parameters $x,y \in \mathbb{R}$ , with $p:= \sqrt3/2$ and $q:=1/2$ : $$\begin{align}
\cos y 
- \sin\left(\frac\pi6 + px - qy\right) 
- \sin\left(\frac\pi6 + px + qy\right) &= 0 \\[6pt]
\sin y 
+ \cos\left(\frac\pi6 + px - qy\right) 
- \cos\left(\frac\pi6 + px + qy\right) &=0
\end{align}$$ According to WolframAlpha , one can even find some explicit solutions, but I don't quite see how to derive these/all solutions from scratch?","['algebra-precalculus', 'trigonometry']"
3801759,"$L^2$ convergence over $[0,T] \times \Omega$ for all $T>0$ implies convergence a.e. for a subsequence over $[0,\infty) \times \Omega$","While reading Karatzas and Shreve's Brownian Motion and Stochastic Calculus, I have a question on the extraction of a convergent subsequence from $L^2$ convergence. My question is regarding the final sentence below. Let $M$ be a continuous square integrable martingale and $\langle M \rangle$ be the quadratic variation process. For each $t>0$ , and a measurable adapted process $X$ define $$[X]_T^2 = E\int_0^T X_t^2 d\langle M \rangle_t.$$ Let $\mathcal{L}$ denote the set of equivalence classes of all measurable $\mathscr{F}_t$ -adapted processes $X$ , for which $[X]_T<\infty$ for all $T>0.$ We define a metric on $\mathcal{L}$ by $[X-Y]$ , where $$[X] = \sum_{n=1}^\infty 2^{-n} (1 \wedge [X]_n).$$ Now, in the proof of Proposition 2.6, it states that: If $X \in \mathcal{L}$ is bounded, then Lemma 2.4 guarantees the existence of a bounded sequence $\{X^{(m)}\}$ of simple processes satisfying $$\sup_{T>0} \lim_{m\to \infty} E \int_0^T |X_t^{(m)} - X_t|^2 dt = 0.$$ It says that
from these we extract a subsequence $\{X^{(m_k)}\},$ such that the set $$\{(t,\omega)\in [0,\infty)\times \Omega; \lim_{k \to \infty} X_t^{(m_k)}(\omega) \neq X_t (\omega)\}$$ has product measure zero. I know that $L^2$ convergence implies convergence a.e. for a subsequence. But here, we have the $L^2$ space on the product space $[0,\infty) \times \Omega$ , and we are given that for each $T>0$ , $X^{(m)}$ converges in $L^2([0,T] \times \Omega)$ to $X$ . So how do we get a convergent subsequence that works for all $(t,\omega) \in [0,\infty) \times \Omega$ ?","['measure-theory', 'lebesgue-integral', 'real-analysis', 'lp-spaces', 'probability-theory']"
3801783,"If $P(x)$ is a polynomial of degree three in $x$, and $y^2 = P(x)$, show that $\frac{D(y^3D^2y)}{y^2}$ is constant","If $P(x)$ is a polynomial of degree three in $x$ , and $y^2 = P(x)$ , show that $$\frac{D(y^3D^2y)}{y^2}$$ is a constant, where D denotes the derivative operator. I have tried expressing the expression above in terms of $P$ and its derivatives (in the hopes of showing the derivative is $0$ ) but I couldn't manage to do that.","['derivatives', 'ordinary-differential-equations']"
3801808,"Is that possible to partition $(\Bbb R,+)$ into 4 additively closed subsets?","Is that possible to partition $(\Bbb R,+)$ into 4 additively closed subsets? It is easy to partition it to 1 and 2 and 3 additively closed subsets. Fore example for 2 subsets we have: $(\Bbb R^{\geq 0},+)\cup(\Bbb R^{< 0},+)$ . But Is that possible for 4 additively closed subsets? What about $k$ additively closed subsets?","['real-numbers', 'group-theory', 'set-partition']"
3801818,"solve for $x$, $(\sqrt{a+ \sqrt{a^2-1}})^x+(\sqrt{a- \sqrt{a^2-1}})^x=2a$","Find the value of $x$ when $$(\sqrt{a+ \sqrt{a^2-1}})^x+(\sqrt{a- \sqrt{a^2-1}})^x=2a.$$ See, by hit and trial method it is clear that $x=2$ is a solution. But I failed to solve this explicitly to get the solutions. My Attempt: \begin{align*} &(\sqrt{a+ \sqrt{a^2-1}})^x+(\sqrt{a- \sqrt{a^2-1}})^x=2a \\ \implies \> & (a+ \sqrt{a^2-1})^{x/2}+(a- \sqrt{a^2-1})^{x/2}=2a\\ \implies \>& (a+ \sqrt{a^2-1})^x+(a- \sqrt{a^2-1})^x+2(a+ \sqrt{a^2-1})^{x/2}(a- \sqrt{a^2-1})^{x/2} = 4a^2.\end{align*} I have no idea how to proceed after this. Also I tried by multiplying conjugate up and down, but I failed. Please help me to solve this.
Thanks in advance.","['exponentiation', 'algebra-precalculus', 'functions']"
3801829,What is the probability that $P(Y>X)$ when $Y$ and $X$ are independent?,"Let $Y\sim N(8.30;0.02^2)$ and $X\sim N(6.60;0.01^2)$ . What is the probability that $P(Y>X)$ when $Y$ and $X$ are independent? My solution so far: Since $Y$ and $X$ are independent, $$Y+X\sim N(\mu_Y+\mu_X,\sigma^2_Y+\sigma^2_X).$$ Now $$Y-X\sim N(1.7;0.02236^2).$$ The probability can be written $$P(Y>X)=P(Y-X>0)=1-P(Y-X\le0)$$ and since the probability has to be greater or equal to zero, $$1-P(Y-X=0).$$ I don't know if I have done correctly so far and I don't know how to go on from here.","['statistics', 'probability-distributions', 'normal-distribution', 'probability']"
3801869,Equivalent definitions of $\beta$-smoothness,"Suppose that the gradient of $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ is $\beta$ -Lipschitz, for some $\beta \geq 0$ , i.e. \begin{equation}
      \|\nabla f(x)-\nabla f(y)\|_2\leq \beta\|x-y\|_2,
\end{equation} for all $x,y\in\mathbb{R}^n$ . Show that this is equivalent to \begin{align*}
        f(\theta x+(1-\theta)y)&\geq \theta f(x)+(1-\theta)f(y)-\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2,\\
        f(\theta x+(1-\theta)y)&\leq \theta f(x)+(1-\theta)f(y)+\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2.
\end{align*} for all $x,y\in\mathbb{R}^n$ and all $\theta \in [0,1]$ , Moreover, show that it is equivalent to \begin{align*}
        f(y)&\geq f(x)+ \nabla f(x)^{T}(y-x) - \tfrac{\beta}{2}\|x-y\|^2,\\
        f(y)&\leq f(x) + \nabla f(x)^{T}(y-x) + \tfrac{\beta}{2}\|x-y\|^2,
\end{align*} for all $x,y\in\mathbb{R}^n$ .","['optimization', 'calculus', 'derivatives', 'real-analysis']"
3801882,Complexity of partitions of $\mathbb{R}$ into subsemigroups,"In an answer to this question it was shown that for each finite $k \geq 1$ it is possible to partition $\mathbb{R}$ into exactly $k$ parts, each of which is additively closed (i.e., is a subsemigroup of $(\mathbb{R},+)$ ).  The argument given there is an induction argument relying on the fact that $\mathbb{R} \cong \mathbb{R}^2$ as groups, and so the partitions obtained for $k \geq 4$ are unlikely to be very nice. My question is: For a given $k \geq 4$ , is it possible to partition $\mathbb{R}$ into $k$ Borel sets, each of which is closed under addition?  If so, I'd also be interested to know how complicated the Borel sets are, and in particular if that complexity changes with $k$ .","['group-theory', 'descriptive-set-theory', 'set-theory']"
3801884,Help me understand Riemannian Manifolds,"I understand differentiable manifolds and the need of them. Thanks to the atlas structure, we can develop a differential calculus on spaces that look locally like $\mathbb R^n$ . Now, what is the need of introducing a metric on the tangent spaces of a smooth manifold (that is what is called a Riemannian metric, and a smooth manifold equipped with a Riemannian metric is called a Riemannian manifold) ? What does it allow us to do and why would we want to do that ? I feel everything that can be done in $\mathbb R^n$ can already be done in a smooth manifold by using coordinates charts. So what is the need of this extra Riemannian structure ? What is the difference between a (smooth) metric space and a Riemannian manifold ? Why do we equip a metric on the tangent spaces and not on the space itself ? What is the intuition behind this structure ? Maybe those are very broad questions, if needed, you can restrict to what really interest me: geodesics, ie the shortest continuous way of going from a point A to a point B. When I read about geodesics, it's all about Riemannian manifolds. But I do not understand why ? Geodesics need only a notion of distance: why not work in metric spaces then ? Are Riemannian manifolds a subset of metric spaces ?","['general-topology', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
3801916,"Working with infinitesimals of the form d(f(x)), for example d(ax), and relating them to dx (integration, delta function)","I am trying to get a better understanding on how we can manipulate the infinitesimal dx in an integral $$\int f(x) dx$$ I have come across the following $$ d(\cos (x)) = -\sin(x) dx$$ Therefore $$\int^{x=2\pi}_{x=0} dx \sin(x) \cos(x) = - \int^{x = 2\pi}_{x=0} d(\cos(x)) \cos(x) = - \dfrac{1}{2} [ \cos^{2}(x)]^{x=2\pi}_{x=0} = -\dfrac{1}{2}[1-1] = 0$$ This looks to me like the chain rule can be applied to infinitesimals in analogy to differentiation. However, today I'm trying to solve the following problem : prove $$\delta(ax) = \dfrac{\delta(x)}{|a|}$$ Following the hint I looked at $$\int d(ax)\delta(ax) = 1 = \int d(ax)\delta(-ax)$$ Since $$\int d(ax)\delta(ax) = 1 \quad \text{and} \quad \delta(x) = \delta(-x)$$ From this it would seem $$d(ax) = |a|dx$$ giving $$\int d(ax)\delta(ax) = |a|\int dx \delta(ax) = |a|\int dx \delta(-ax) = \int dx \delta(x) = 1$$ as expected. I would have naively assumed $d(ax) = a \space dx$ In summary, I have no idea how to treat d(f(x)), and I'm not sure where to look for information. Could someone help me gain a better understanding ? Unfortunately I have only taken a few undergraduate maths courses so far, so I couldn't understand anything too complex.","['integration', 'measure-theory', 'dirac-delta', 'derivatives', 'differential-forms']"
3801933,Explanation for Weight multiplied in Posterior Probability Sum,"Let, $P(\theta|x)$ is the posterior probability. It describes $\textbf{how certain or confident we
are that hypothesis $\theta$ is true, given that}$ we have observed data $x$ . Calculating posterior probabilities is the main goal of Bayesian statistics! $P(\theta)$ is the prior probability, which describes $\textbf{how sure we were that}$ $\theta$ was true,
before we observed the data $x$ . $P(x|\theta)$ is the likelihood. $\textbf{If you were to assume that $\theta$ is true, this is the
probability}$ that you would have observed data $x$ . $P(x)$ is the marginal likelihood. This is the probability that you would have observed data $x$ , whether $\theta$ is true or not. So, $P (\theta|x) = \frac{P (\theta) P(x|\theta)}{P (x)}$ The following part is an excerpt from the same text - In the Bayesian framework, our predictions are always in the form of
probabilities or (later) probability distributions. They are usually
calculated in three stages. First, you pretend you actually know the true value of the parameters,
and calculate the probability based on that assumption. Then, you do this for all possible values of the parameter $\theta$ (alternatively, you can calculate the probability as a function of $\theta$ ). Finally, you combine all of these probabilities in a particular way to
get one final probability which tells you how confident you are of
your prediction. Suppose we knew the true value of $\theta$ was $0.3$ . Then, we would
know the probability of catching the right bus tomorrow is $0.3$ . If
we knew the true value of $\theta$ was $0.4$ , we would say the
probability of catching the right bus tomorrow is 0.4. The problem is, we don’t know what the true value is. We only have the
posterior distribution. Luckily, the sum rule of probability (combined
with the product rule) can help us out. We are interested in whether I will get the good bus tomorrow. There
are $11$ different ways that can happen. Either $\theta=0$ and I get
the good bus, or $\theta=0.1$ and I get the good bus, or $\theta=0.2$ and I get the good bus, and so on. These 11 ways are all mutually
exclusive. That is, only one of them can be true (since $\theta$ is
actually just a single number). Mathematically, we can obtain the posterior probability of catching
the good bus tomorrow using the sum rule: $$P(\text{good bus tomorrow}|x) = \sum_{\theta} p(\theta|x) \times P(\text{good bus tomorrow}|\theta, x) $$ $$=  \sum_{\theta} p(\theta|x) \times \theta$$ This says that the total probability for a good bus tomorrow (given
the data, i.e. using the posterior distribution and not the prior
distribution) is given by going through each possible $\theta$ value, working out the probability assuming the $\theta$ value you are considering is true, multiplying by the probability (given the data)
this $\theta$ value is actually true, and summing. In this particular problem, because $P\text{(good bus  tomorrow}|\theta, x) = θ$ , it just so happens that the probability for
tomorrow is the expectation value of $\theta$ using the posterior
distribution. To three decimal places, the result for the probability tomorrow is $0.429$ . Interestingly, this is not equal to $2/5 = 0.4$ . The problem on page $26, 7$ of the text of Introduction to Bayesian Statistics by Brendon J. Brewer is written as following  - QUESTION Now to calculate posterior probability (of catching
the good bus tomorrow) $P(\text{good bus tomorrow}|x)$ why do the author multiplied $p(\theta|x)$ by $P(\text{good bus tomorrow}|\theta, x) $ in the $\sum_{\theta}$ ? To me, $P(\text{good bus tomorrow}|x) = \sum_{\theta} p(\theta|x) $ is correct, so what am I missing? In this comment I have been told, $p(\theta|x)$ itself is a weight, which confused me more, so please explain, thanks.","['statistics', 'bayesian', 'bayes-theorem']"
3801950,"How many unordered pairs of $\{A,B\}$ of subsets are possible under given conditions?","Let $S=\{1,2,\ldots,n\}$ . Find the number of unordered pairs $\{A,B\}$ of subsets of $S$ usch that $A$ and $B$ are disjoint, where $A$ and $B$ or both may be empty. This question here [1] addreses a similar kind of problem but it doesn't take empty sets into consideration. My Approach: Let us select subset $A$ first which would mean that the selection of subset $B$ depends on $A$ . Let $n(A)$ denote the number of elements in subset $A$ . Suppose $n(A)=k$ , then the subset $B$ has control over the selection of remaining $(n-k)$ elements which means that the number of such possible pairs $=\binom{n}{k}2^{n-k}$ $$\therefore \text {Total unordered pairs} =\sum_{k=0}^{n}\binom{n}{k}2^{n-k}=\sum_{k=0}^{n}\binom{n}{n-k}2^{n-k}$$ $$=(1+2)^n=3^n$$ I just want to verify my solution. So please check for any mistakes in my solution and please offer suggestions. THANKS Correction : These are the total number of ORDERED pairs including both sets empty case. As stated by Phicar in the answer, to keep the both sets empty case in the unordered pairs as well, it may be excluded before and can be added later, hence giving the total number of unordered pairs to be $\dfrac{3^n-1}{2}+1$ .","['elementary-set-theory', 'solution-verification', 'combinatorics']"
3802013,Explicit Hodge decomposition on $T^2$,"Given a general compact Riemann manifold $(M,g)$ , we have the well-known Hodge decomposition $$
\Omega^*(M)\cong d\Omega^*(M) \oplus \delta\Omega^*(M)\oplus \mathcal H_{\Delta}(M)
$$ where $\delta$ is the dual of $d$ with respect to the metric and $\mathcal H_{\Delta}(M)$ is the solution space of Laplacian equation $\Delta\alpha=0$ , i.e. the space of harmonic forms. Question: So far, I can only theoretically understand this decomposition. That is, we know the existence of such decomposition, but I was wondering if we can get some intuition about it by finding some explicit decomposition. Now, we equip the torus $T^2$ with the flat metric $g$ induced from $\mathbb R^2\to \mathbb R^2/\mathbb Z^2\equiv T^2$ . Let $\alpha= f(x_1,x_2)dx_1+g(x_1,x_2)dx_2$ be an arbitrary one-form. Can we write down explicitly the Hodge decomposition of $\alpha$ with respect to the flat metric?","['hodge-theory', 'differential-geometry']"
3802053,Proof of $\lim_{n \to \infty}(1+\frac{1}{n})^n=e$,"I have this messy proof of $\lim_{n \to \infty}(1+\frac{1}{n})^n=e$ in my notebook. I can't find it anywhere else, but I need it since the professor accepts only this version at the exam. At the time I am only stuck with the first part, so I will write the proof only to that point. Also, there is a few steps with missing reasoning between them. Proof. We are given two sequences: $$x_n=1+\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+\dots+\frac{1}{n!}$$ $$y_n=\left(1+\frac{1}{n}\right)^n$$ If $x_n$ converges, then $\lim_{n \to \infty}x_n=e$ . (This is given.) Let's prove that $x_n$ converges. To prove that, the Monotone Convergence Theorem will be used. Since $x_{n+1}>x_{n}$ the sequence is increasing. (I skipped the unnecessary reasoning for this.) Now let's prove that it is bounded from above. $$n!=1\cdot 2\cdot 3\cdots n>2^{n-1}\tag{1.1}$$ $$\frac{1}{n!}\leq \frac{1}{2^{n-1}}\tag{1.2}$$ $$\underbrace{2}_{\text{?}}\leq x_n\leq 1+1+\frac{1}{2}+\frac{1}{2^2}+\dots+\frac{1}{2^{n-1}}\tag{1.3}$$ $$1+q+q^2+\dots+q^{n-1}=\underbrace{\overbrace{\frac{1-q^n}{1-q}}^{?}=\frac{q^n-1}{q-1}}_{\text{?}}\tag{1.4}$$ $$q=\frac{1}{2}$$ $$\underbrace{1}_{\text{?}}+\frac{1-\frac{1}{q^n}}{1-\frac{1}{q}}=1+2\left(1-\frac{1}{2^n}\right)<1+2=3\tag{1.5}$$ And so $2\leq x_n\leq3$ , $x_n$ is bounded and increasing, hence it converges. Therefore $\lim_{n \to \infty}x_n=e$ . The proof proceeds with proving that $\lim_{n\to \infty}y_n=Y$ , and then showing $y\geq e$ and $y \leq e$ , and thereby $y=e$ . This part is very long and already looking messy in the notebook. Because of that I will skip it, since it isn't important for the question. My questions: Is there any obvious reason why $2^{n-1}$ is a good (necessary?) choice for the inequality (1.1)? How does (1.2) follow from (1.1)? Why is it $\leq$ and not $ < $ ? I see that $\leq$ is the right choice since for $n=1$ , we have $\frac{1}{1!}=\frac{1}{2^{1-1}}$ , but is there any way to know this without evaluating for choices of $n$ ? Why is that $2$ there in inequality (1.3)? Where did it come from ? Considering inequality (1.4) there is few things; a) How was the overbraced fraction derived from the geometric series? b) The underbraced equality at first didn't make sense to me, but when I evaluated for few $n$ s, I realized it actually holds. Is this a known thing, that the order of subracting isn't important for two fractions to be equal, as long as both the numerator and the denominator of the fraction are both $<0$ or $>0$ ? Also what was the role of the equality in the proof, or was it just a remark ? EDIT: I just realized that we can do that since the fraction is always positive. But it still doesn't seem so obvious to notice. Should it be obvious? Why is that $1$ there in the inequality (1.5) ? Where did it come from ? If you have a source for this proof please send me a link or tell me where I can find it. Thanks","['proof-explanation', 'proof-writing', 'real-analysis', 'sequences-and-series', 'limits']"
3802057,using a bijection to prove an equality involving generating functions,"A parity-preserving subset $\{\alpha_1,\cdots, \alpha_k\}$ of $\{1,\cdots, n\}$ satisfies that $\alpha_i \cong i \mod 2$ and $\alpha_i < \alpha_{i+1}\forall i.$ Let $p_n$ be the number of parity-preserving subsets of $\{1,\cdots, n\}, n\geq 0.$ Let $P_n$ be the set of parity-preserving subsets of $\{1,\cdots, n\}$ and $D_n$ be the set of differences of $\{1,\cdots, n\},$ whose elements are the ordered pairs of the differences between consecutive elements that start with the first element of the subset. Let $\{\alpha_1,\cdots, \alpha_k\} := \sigma$ be a parity-preserving subset. Define $d(\sigma) := (\alpha_1,\alpha_2-\alpha_1,\cdots, \alpha_k - \alpha_{k-1}).$ Then $d^{-1}(\alpha_1,\alpha_2,\cdots, \alpha_k) = \{\alpha_1, \alpha_1 + \alpha_2,\cdots, \alpha_1 + \alpha_2+\cdots \alpha_k\}.$ This is called the difference-partial sum bijection. Show that $\sum_{n\geq 0} a_nx^n = \dfrac{1+x}{1-x-x^2}$ using the difference-partial sum bijection. Let $b_n$ denote the number of subsets of $\{1,\cdots, n\}$ without consecutive elements. Show that $\sum_{n\geq 0} b_nx^n = \sum_{n\geq 0} a_nx^n$ using the difference-partial sum bijection. The two equalities should be similar to show. Given an element in $B_n$ (i.e. a subset of $\{1,\cdots, n\}$ without any consecutive elements), the difference vectors are either empty vectors or they start with a positive integer. Each subsequent difference should be at least $2$ , resulting in the series $\dfrac{\frac{x}{1-x}}{1-\frac{x^2}{1-x}} = \frac{x}{1-x-x^2},$ but this is not equal to $\frac{1+x}{1-x-x^2}.$ Similarly, given an element in $P_n$ , the differences are always odd (due to differing parities), which gives the series $\dfrac{\frac{x}{1-x^2}}{1-\frac{x}{1-x^2}} = \dfrac{x}{1-x-x^2},$ which is also incorrect. What am I doing wrong? How can I account for the fact that empty sets are elements in $A_n$ and $B_n$ for all $n$ in my generating functions?","['elementary-set-theory', 'combinatorics', 'generating-functions']"
3802069,Finding bounds in double integral,"Consider the unit circle $x^2 + y^2 = 1$ and the line $y = 3x$ . I want to compute the area of the portion of the circle enclosed above the line $y = 0$ but to the left of the line $y = 3x$ . In other words, I require $y > 3x$ and $y > 0$ to hold. NOTE: Although the shaded blue area in the image above goes outside of the circle, I only want the area of the portion enclosed inside of the circle. Is there any nice geometric way to solve this problem? I tried to solve it with a double integral, but I can't quite figure out the bounds. I'm happy with any solution. I know that the entire second quadrant has area $\pi/4$ , but figuring out the last bit is difficult. I thought about finding the angle from the line $x = 0$ to $y = 3x$ by drawing a right triangle with side-lengths $3$ and $1$ . This means that the angle is $\theta = \arctan(1/3)$ , but how can I figure out the area from here?","['euclidean-geometry', 'circles', 'geometry', 'multivariable-calculus', 'polar-coordinates']"
3802135,Geometric proof that a Pythagorean triple has a number divisible by $3$?,"Can I infer from “If $a,b \in \mathbb{N}$ such that $(a,3)=(b,3)=1$ then $a^2+b^2\neq c^2$ for any $c\in\mathbb{N}$ ” that any pythagorean triple has a number which is divisible by $3$ ? If so, how can this relationship be proved geometrically?","['elementary-number-theory', 'pythagorean-triples', 'geometry', 'divisibility']"
3802158,"If $I$ and $J$ are ideals and I know an algorithm to find $I : J$, can I use this to find the saturation of $I$ by $J$?",I know a pair of ideals $I$ and $J$ in a Noetherian ring and have an algoritm to find a generating set of the quotient ideal $I : J$ . If I keep applying this algorithm successively finding generating sets for $I : J$ $(I : J) : J$ $((I : J) : J) : J$ $(((I : J) : J) : J) : J$ and so on then this will at some point stabilise by the ascending chain condition. My question is whether it is guaranteed to stabilise at the saturation $I : J ^\infty$ ?,"['noetherian', 'algebraic-geometry', 'polynomial-rings', 'ideals', 'commutative-algebra']"
3802279,Examples and Counterexamples of Relations which Satisfy Certain Properties,"Definition: Given a set $X$ , a relation $R$ on $X$ is any subset of $X\times X$ .  A relation $R$ on $X$ is said to be reflexive if $(x,x) \in R$ for all $x \in X$ , irreflexive if $(x,x) \not\in R$ for all $x \in X$ , transitive if $(x,y) \in R$ and $(y,z) \in R$ implies that $(x,z)\in R$ , intransitive (or antitransitive) if $(x,y) \in R$ and $(y,z) \in R$ implies that $(x,z)\not\in R$ , symmetric if $(x,y) \in R$ implies that $(y,x) \in R$ , antisymmetric if $(x,y) \in R$ and $(y,x) \in R$ implies that $x=y$ . Given any combination of the properties listed above, is there a nontrivial (i.e. nonempty) relation which satisfies that combination of properties?","['elementary-set-theory', 'relations', 'examples-counterexamples', 'faq']"
3802293,How does $3\cos x + 4\sin x$ become $5\cos(x - \arctan\frac{4}{3})$?,I'm not sure how any rule is being applied to manipulate the $4\sin x$ None of the double angle/compound angle formulas have the trig functions in this layout,['trigonometry']
3802366,How to prove $\frac{a}{b}=\frac{c}{d} \implies \frac{a+c}{b+d}=\frac{a-c}{b-d}$?,"My question: How do I prove the following property of ratio? $$\frac{a}{b}=\frac{c}{d} \implies \frac{a+c}{b+d}=\frac{a-c}{b-d} \tag{1}$$ $a,b,c,d \in \mathbb{R} \backslash \{ 0 \}$ I want to use the result in argument below. Use the sine rule to establish the following identities for triangles: $$\frac{a+b}{c}=\frac{\sin(A) +\sin(B)}{\sin(C)}$$ and \begin{equation}\frac{a-b}{c}=\frac{\sin(A) -\sin(B)}{\sin(C)}\end{equation} To prove both these identities the equations rearrange to the following $$\frac{\sin(C)}{c}=\frac{\sin(A)+\sin(B)}{a+b}=\frac{\sin(A)-\sin(B)}{a-b} $$ From here, $(1)$ would complete the argument. Thanks in advance","['algebra-precalculus', 'trigonometry']"
3802404,Is this a correct application of the distributive law to $ (\neg P \wedge \neg Q \wedge R) \vee (\neg P \wedge Q \wedge \neg R)$?,"Is this a correct application of the distributive law? $$\begin{align}
(\neg P \wedge \neg Q \wedge R) \vee (\neg P \wedge Q \wedge \neg R) &\equiv \phantom{\wedge}(\neg P \vee \neg P) \wedge (\neg P \vee Q) \wedge (\neg P \vee \neg R) \\
&\phantom{\equiv}\wedge (\neg Q \vee \neg P) \wedge (\neg Q \vee Q) \wedge (\neg Q \vee \neg R) \\
&\phantom{\equiv}\wedge (\phantom{\neg}R \vee \neg P) \wedge (\phantom{\neg}R \vee Q) \wedge (\phantom{\neg}R \vee \neg R)
\end{align}$$","['propositional-calculus', 'logic', 'discrete-mathematics']"
3802441,Five roots of $x^5+x+1=0$ and the value of $\prod_{k=1}^{5} (2+x_k^2)$,"Here, $x_{k}$ are five roots of $x^{5} + x + 1 = 0$ . I know two roots  are $\omega, \omega^{2}$ and next I can find a cubic dividing it by $x^{2} + x + 1$ and using the connection of $3$ roots with the coefficients of this cubic( Vieta's formulas ). But the calculation becomes very tedious, where I do not get the required value of $\prod_{k = 1}^{5}\left(2 + x_{k}^{2}\right) = 51$ . Can there be a simpler way of doing this ?.","['algebra-precalculus', 'polynomials']"
3802485,"Density of $Y = \left\lbrace f \in [0,1]^{[0,1]} : \operatorname{supp}(f) \leq \left\lvert \mathbb{N} \right\rvert \right\rbrace$","I'd like to prove that given $X = [0,1]^{[0,1]}$ has a subspace, which is indeed $Y$ , that is sequentially compact but not compact. To prove that I need to prove that $Y$ is dense in $X$ , where I denote $\operatorname{supp}(f) = \left\lbrace x \in [0,1] : f(x) \ne 0 \right\rbrace$ . My effort : given $Y = \left\lbrace f \in X : \operatorname{supp}(f) \leq \left\lvert \mathbb{N} \right\rvert \right\rbrace$ it is sufficient to prove that $Y \cap U \ne \emptyset$ just for $U$ of the form $U = \prod\limits_{t \in [0,1]} U_{t}$ where $U_{t} \ne [0,1]$ just for finite $t \in [0,1]$ . Then if $f(t) = \hspace{0.1cm} \begin{cases} 0 & U_{t} = [0,1] \\ a \in [0,1] & U_{t} \ne [0,1]\end{cases}$ But I'm not sure if I can conclude that $f \in U \cap Y$ which implies my thesis. Any help or hint would be appreciated.","['continuity', 'general-topology', 'compactness']"
3802487,What is the second derivative of the absolute function $\left|\frac{x+1}{x+2}\right|$?,"I calculated the derivative of $\left|\frac{x+1}{x+2}\right|$ in the same way that I would do with $ \frac{x+1}{x+2}$ in order to study the function. But when I verified on wolfram, I noticed it is all wrong. Wolfram uses the chain rule as you can see here . I don't get it. The only rule I've been taught as far as absolute function derivatives are concerned, is $|x|' = \frac{x}{|x|}$ . Does a similar rule apply for $f(x)$ ? And why does wolfram uses chain rule? Edit I calculated the derivatives as there is no absolute and then, at the result, I applied the absolute. My answers are $|(\frac{x+1}{x+2})|' = |(\frac{x+1}{x+2})'| = \frac{1}{\left(x+2\right)^2}$ and $|(\frac{x+1}{x+2})|'' = |(\frac{x+1}{x+2})''| = \frac{2}{\left(x+2\right)^3}$ Wolfram's answer is $\left(\left|\frac{x+1}{x+2}\right|\right)'\:=\frac{\left|x+2\right|\left(x+1\right)}{\left|x+1\right|\left(x+2\right)^3}$","['calculus', 'soft-question', 'derivatives']"
3802506,Interpreting the meaning of change in a variable,"If $z$ is a function depending on some other variables, let's say $x$ and $y$ , I have learnt that we can write $$\delta z=\frac{\partial{z}}{\partial{x}}\delta{x}+\frac{\partial{z}}{\partial{y}}\delta{y}$$ Or if a function simply depends on a single variable, for example if $y$ depends on $x$ , then we can write $$\delta y=\frac{\mathrm{d}y}{\mathrm{d}x}\delta x$$ I want to know why it can be written like this. I know that $\delta y$ represents some finite change in $y$ and $\mathrm{d}y$ also represents infinitesimal change in $y$ , but how are they related is confusing me. Any help would be appreciated!","['partial-derivative', 'calculus', 'change-of-variable', 'derivatives']"
3802517,"Residue theorem for $ I=\int_{-\infty}^{+\infty}\frac{e^{\mathrm{i}\,t\,z}}{(z-z_1)(z-z_2)} \, \mathrm{d}z$","If I use the residue theorem to evaluate the integral $$ I(t)=\int_{-\infty}^{+\infty}\frac{e^{\mathrm{i}\,t\,z}}{(z-z_1)(z-z_2)} \, \mathrm{d}z$$ with $t>0$ , $\mathrm{Im}(z_1)>0$ and $\mathrm{Im}(z_2)<0$ , I would have thought to get $$ I(t)=2\,\pi\,\mathrm{i}\,\frac{e^{\,\mathrm{i}\,t\,z_1}}{z_1-z_2}$$ since only the pol in the upper half plane contributes to the integral. If I solve the integral with Mathematica 12.0 it evaluates to $$ I(t)=2\,\pi\,\mathrm{i}\,\frac{e^{\,\mathrm{i}\,t\,z_1}-e^{\,\mathrm{i}\,t\,z_2}}{z_1-z_2}$$ even though I set the correct assumptions on $z_1$ and $z_2$ and allowed for the calculatoin of the Cauchy principal value. Now I am wondering if I misunderstood the residue theorem or Mathematica evaluates the integral incorrectly.","['integration', 'improper-integrals', 'contour-integration', 'residue-calculus', 'exponential-function']"
3802538,derivative of the $L^p$-norm with respect to $p$,"Let $\Omega$ be a probability space (or a finite measure space) and $f : \Omega \to \mathbb{R}$ be a positive function of $L^p(\Omega)$ with $f>0$ . We suppose that $\|f\|_{L^1(\Omega)}=1$ . What is the derivative $\frac{d}{d p}|_{p=1} \|f\|_p$ at the point 1 of the function $p \mapsto \|f\|_p$ defined on $[1,+\infty)$ ? My feeling is that the answer is maybe $-\int_{\Omega} f\log f$ . Same question with $\ell^p$ instead of $L^p(\Omega)$ with a sequence $(a_k)$ of $\ell^1$ with $a_k >0$ for any $k$ and $\|a\|_{\ell^1}=1$ . I believe that there is a difference between the two cases since if $q<p$ we have $L^p \subset L^q$ in the first case and $\ell^q \subset \ell^p$ in the second case.","['measure-theory', 'entropy', 'lp-spaces', 'functional-analysis', 'derivatives']"
3802547,Prove that the composition map is continuous with respect to the metric topology on $\operatorname{Iso}(M)$,"Let $M$ be a finite dimensional Riemannian manifold and $\operatorname{Iso}(M)$ be its set of isometries. It can be shown that $\operatorname{Iso}(M)$ is a finite dimensional manifold with a metric as defined below: Consider $(n + 1)$ points on $M$ so close together that $n$ of them lie in an normal neighborhood of the other, and if the points are linearly independent (i.e. not in the same $(n-1)$ -dimensional geodesic hypersurface). Then the distance $d(f, \tilde f)$ between two isometries $f$ and $\tilde f$ will be defined as the maximum of the distance $d_i[f(x), \tilde f(x)]$ as $x$ ranges over the given set of $n+1$ points. This distance can be shown to satisfy the usual metric axioms. Here $d_i$ is of course the induced metric on $M$ (Riemannian distance fucntion) Given $\operatorname{Iso}(M)$ is now a metric space with metric $d$ as defined, we thus get a natural metric topology for $\operatorname{Iso}(M)$ . That is open sets are all subsets that can be realized as the unions of open balls of form $B(f_0, r) = \{f \in \operatorname{Iso}(M): d(f_0,f)< r\}$ where $f_0 \in \operatorname{Iso}(M)$ and $r>0$ . $\ $ I am trying to prove that $$\mathscr M: \operatorname{Iso}(M) \times \operatorname{Iso}(M) \rightarrow \operatorname{Iso}(M), \, (f,g) \mapsto f \circ g$$ is continuous in the metric topology of $\operatorname{Iso}(M)$ . Attempt: Munkres Topology Section 46 Page 287 Let $Y$ be locally compact Hausdorff, and $X$ and $Z$ general spaces. Also let $\mathscr{C}(X,Y),\,\mathscr{C}(Y,Z),$ and $\mathscr{C}(X,Z)$ denote the spaces of continuous functions from the respective spaces with the compact open topology. Then the composition map $$\mathscr M: \mathscr{C}(X,Y) \times\mathscr{C}(Y,Z)\rightarrow\mathscr{C}(X,Z)$$ is continuous. The above is a proven statement, and can be assumed for now. In the statement, $X$ and $Z$ can be replaced with $M$ which has metric topology (and hence manifold topology) and thus is a general space. Further, $Y$ can also be replaced with $M$ as it is locally compact Hausdorff as a manifold. So we end up with $\mathscr{C}(M,M)$ for all 3. Furthermore, as isometries are continuous, we get that $\operatorname{Iso}(M) \subset \mathscr{C}(M,M)$ . Thus we end up with the following: $\mathscr M: \operatorname{Iso}(M) \times \operatorname{Iso}(M) \rightarrow \operatorname{Iso}(M)$ is continuous Idea : The compact-open topology and metric topology are the same in case of $\operatorname{Iso}(M)$ under these conditions because the topologies of every space involved here comes from same $d_i$ (Riemannian distance function as defined previously) $\ $ Q) So I'm looking for a proof that the CO Topology and metric topology are the same for $\operatorname{Iso}(M)$ . Alternatively (and preferably) Q) Is there a direct way to show continuity of $\mathscr M$ in the metric topology of $\operatorname{Iso}(M)$ (i.e. showing inverse of an open set in metric topology of $\operatorname{Iso}(M)$ is always open in $\operatorname{Iso}(M)\times\operatorname{Iso}(M)$ , or any of the equivalent definitions of metric continuity) ? $\ $","['riemannian-geometry', 'topological-groups', 'continuity', 'manifolds', 'general-topology']"
3802588,Are lattice operations continuous in the Lipschitz norm?,"Denote by $Lip_0(X)$ the set of all Lipschitz functions on a metric space $X$ vanishing at some base point $e \in X$ . The norm in $Lip_0$ is defined as fololows $$
\|f\|_{Lip_0} := Lip(f),
$$ where $Lip(f)$ denotes the Lipschitz constant. With pointwise operations $f \vee g := \max\{f,g\}$ and $f \wedge g := \min\{f,g\}$ the space $Lip_0$ becomes a Lipschitz lattice , in which the following condition holds $$
\|f \vee g\|_{Lip_0} \leq \max\{\|f\|_{Lip_0},\|g\|_{Lip_0}\}.
$$ The Banach lattice condition $|f| \leq |g| \implies \|f\| \leq \|g\|$ , however, fails. (Nik Weaver. Lipschitz Algebras, 2nd ed.) Question . Are operations $f_+ := f \vee 0$ , $f_- := (-f) \vee 0$ and $|f| := f \vee (-f)$ continuous in the $Lip_0$ norm, i.e. does, e.g., $$
\|f_+ - g_+\|_{Lip_0} \leq C\|f - g\|_{Lip_0}
$$ hold? I have searched a lot for either a proof or a counterexample, but couldn't find anything. Any help will be appreciated.","['vector-lattices', 'functional-analysis', 'lipschitz-functions']"
3802617,What is the difference between $\mathbb{R}$ and $\mathbb{R}^1$?,"I am wondering that $\mathbb{R}$ and $\mathbb{R}^1$ are same or not. $\mathbb{R}$ is the real numbers, and $\mathbb{R}^1$ is a set of 1-tuples. I am so stucked on this. Thanks for the support.","['real-numbers', 'definition', 'analysis', 'real-analysis']"
3802633,"Evaluate $3\int_{0}^{2\pi} \sin(t) \cos(t) \,{\rm d}t$","$$3\int_{0}^{2\pi} \sin(t) \cos(t) \,{\rm d}t$$ My calculus is a bit rusty and I can not find where I get it wrong. Setting $u= \sin(t)$ , I get ${\rm d} u=\cos(t) \,{\rm d} t$ and, thus, $$3\int_{u=0}^{u=0}u \,{\rm d}u=0$$","['integration', 'calculus', 'definite-integrals']"
3802673,Is every isometric immersion between surfaces of equal area injective?,"Let $M,N$ be smooth connected, compact two-dimensional Riemannian manifolds, such that $M$ has a non-empty Lipschitz boundary. Suppose that $\operatorname{Vol}(M)=\operatorname{Vol}(N)$ . Question: Let $f:M \to N$ be a smooth isometric immersion (i.e $df_p$ is an isometry for every $p \in M$ ). Must $f$ be surjective? This equivalent to $f$ being injective ""a.e. in the image""- i.e. $|f^{-1}(q)| \le 1$ for a.e. $q \in N$ . (see below). The argument given here shows that if $\partial M=\emptyset$ , then $f$ is surjective. Proof of the equivalence: By the area formula $$
\text{Vol}(M) = \int_M 1=\int_M \det df  = \int_N |f^{-1}(y)|=\int_{f(M)} |f^{-1}(y)|.
$$ So, if $|f^{-1}(y)| \le 1$ a.e. on $N$ , then $
\text{Vol}(N)=\text{Vol}(M) = \text{Vol}(f(M))$ . On the other hand, if $\text{Vol}(f(M))=\text{Vol}(M)$ , then $$\text{Vol}(f(M))=\text{Vol}(M)= \int_{f(M)} |f^{-1}(y)| \ge \int_{f(M)} 1= \text{Vol}(f(M)),
 $$ so $|f^{-1}(y)| \le 1$ a.e. on $f(M)$ , hence also on $N$ . We proved that $|f^{-1}(y)| \le 1$ a.e. on $N$ if and only if $\text{Vol}(f(M))=\text{Vol}(N)$ . Since $f(M) $ is compact, being of full measure in $N$ is equivalent to being equal to $N$ . Comment: Some amount of non-injectivity is clearly possible: Take for example $M=[-1,1]^2$ , and let $N=M/\sim$ be the flat $2$ -torus with $\sim$ the standard equivalence relation. Then the quotient map $\pi:M\to N$ is not everywhere injective.","['riemannian-geometry', 'geometric-measure-theory', 'real-analysis', 'isometry', 'differential-geometry']"
3802696,Conditions (allegedly..) sufficient on a nonabelian finite group -- involving the number of conjugacy classes -- for the centre to be nontrivial,"I recently encountered a -- may I say -- ''cute'' looking problem in a textbook on group theory. In my own notations, here is the hypothesis. For arbitrary group $F$ the (cardinal) number of conjugacy classes of $F$ is denoted by $\mathrm{c}(F)$ and for arbitrary subset $X \subseteq F$ the centraliser of $X$ in $F$ is written as $\mathrm{C}_{F}(X)$ . For arbitrary set $M$ the full symmetric group on $M$ shall be denoted by $\Sigma(M)$ . $\mathbf{Gr}$ refers to the category of groups. Let $G$ be a finite nonabelian group and let us denote the largest prime divisor of the order $|G|$ by $p$ . Given the relation $\mathrm{c}(G)>\frac{|G|}{p}$ , prove that $\mathrm{Z}(G)$ is not trivial. The problem follows the chapter which introduces group actions, representations by permutations, elementary properties of conjugation etc, so it should not require tools too sophisticated to solve (such as character theory). My thoughts are to attempt Reductio ad absurdum by assuming the centre were trivial, to the effect that the number $\mathrm{c}(G)-1$ of nontrivial conjugacy classes is at least equal to $\frac{|G|}{p}$ . This further means that at least one of these nontrivial classes -- say $C$ -- has cardinality strictly less than $p$ , otherwise the union over all these classes -- expressible as $G \setminus \mathrm{Z}(G)=G \setminus \{1_G\}$ by virtue of our assumption and hence of cardinality $|G|-1$ -- has at the same time cardinality $\geqslant |G|$ , which is absurd. Let us write $m=|C|$ . Thus, $G$ acts transitively (by conjugation) on $C$ , action which induces a permutation representation $\rho \in \mathrm{Hom}_{\mathbf{Gr}}(G, \Sigma(C))$ . The kernel of this representation is clearly given by $\mathrm{Ker} \rho=\mathrm{C}_G(C)$ , so it follows that $(G:\mathrm{C}_G(C))|\ m!$ . If it were the case that $p$ divides this index we would derive $p|m!$ , which is equivalent to $p \leqslant m$ and contradicts one of the previous conclusions. Hence we gather that $p|\ |\mathrm{C}_G(C)|$ .....and what of it?! The reasoning above doesn't take into account the special property that $p$ has (of being the maximum among the set of prime divisors of $|G|$ ), which is indication that it is perhaps not the way to go searching for a proof... Attempting to reason by contradiction by means of assuming the existence of a minimal (in the sense of order) counterexample also doesn't seem too promising, since the behaviour of either centres or conjugacy classes is rather hard to control when passing on to subgroups, in general. All in all the problem doesn't seem to conceal anything too profound, but alas I see myself lacking in ideas to approach it. Any word of advice shall be greatly appreciated!","['representation-theory', 'group-theory', 'finite-groups']"
3802700,Finding result of composing operations many times,"Consider the operator given by, $$ P = x \frac{d}{dx}$$ with, $$ P^2 = x \frac{d}{dx} ( x\frac{d}{dx}) = x \frac{d}{dx} + x^2 \frac{d^2}{dx^2}$$ or, $$ P^2 = x \frac{d}{dx} ( x\frac{d}{dx}) = x \frac{d}{dx} + x^2 \frac{d^2}{dx^2}$$ and on another application of the operator, $$ P^3 = [x \frac{d}{dx}] P^2 = x\frac{d}{dx}(x \frac{d}{dx}) + x \frac{d}{dx}( x^2 \frac{d^2}{dx^2}) = x \frac{d}{dx} + 3x^2 \frac{d^2}{dx^2} + x^3 \frac{d^3}{dx^3}  $$ I tried writing more iterations but I can't find / a general form of what $P^k$ should be. So my question is if you are given an operator as the one shown, is there any standard procedure to find the a formula for the kth iteration of the operator? The actual reason why I want to know about this Any help would be appreciated :D","['operator-theory', 'sequences-and-series']"
3802726,Regarding definition of Linear Convexity in $\mathbb{C}^n$ and reference request,"In the book Notions of Convexity by Lars Hörmander page 290, section 4.6 Linear convexity is defined as follows.
An open set $X\in \mathbb{C}^n$ is called linearly convex if for every $z\in \mathbb{C}^n\setminus X $ there exists an affine complex hyperplane $\Pi$ such that $z\in \Pi\subset \mathbb{C}^n\setminus X$ . Proposition 4.6.2 says. If $X$ is an open set in $\mathbb{C}^n$ , then the union $F$ Of all affine complex hyperplanes $\Pi\subset\mathbb{C}^n\setminus X$ is a closed set and $\hat{X}= \mathbb{C}^n\setminus F$ is linearly convex. If $V$ is a complex vector space we shall denote by $P(V)$ the projective space consisting of all complex lines through the origin in $V$ . And $V^*$ is the dual space of $V$ . I wanted to know that why did we need to go to the projective geometry set up to define $X^{**}$ ? Can we not define it in vector space set up itself? Do we have a similar definition of $X^{**}$ where we need not consider the projective space (something’s similar to the bipolar of a set in Banach space). I have no background in projective geometry, but have started learning and have understood the real projective plane. Since I am not very sure of my concepts in projective geometry, I wanted to know if we have similar definition of $X^{**}$ in vector space set up. Reference request:
The most standard known books to study the different notions of convexity in several complex variables are Complex Convexity and Analytic Functionals by Mats Andersson and Notions of Convexity by Lars Hörmander. Are there any other references/texts with all equivalent definitions to study the same?","['complex-analysis', 'several-complex-variables', 'convex-analysis', 'reference-request']"
3802767,Why cone of revolution is not a submanifold?,"I just begin to learn sub-manifold. I want to prove the cone of revolution $$\{(x,y,z) \in \mathbb{R}^3|x^2+y^2−z^2=0)\}$$ is not a submanifold.
Why we cannot find a submersion (like $f(x,y,z) =x^2+y^2−z^2$ ) to express the cone as an equation? I see there may be some problem on $0$ but I cannot express it explicitly and link it to some classical contrary example (like $[0,1]$ not homeomorphic to $(0,1)$ ).","['manifolds', 'submanifold', 'differential-geometry']"
3802779,Evaluate $\int x^2 \sin(7x^3)dx$,"Evaluate $$\int x^2 \sin(7x^3)dx.$$ Can somebody check my solution? Thanks! Let $u = 7x^3$ , then $du = 21x^2dx$ and so $\dfrac{du}{21x^2}=dx$ . Thus we have: $$\int x^2 \sin(7x^3)dx=\frac{1}{21} \int \sin(u)du = \frac{-1}{21}\cos(u)+C = \frac{-1}{21}\cos(7x^3)+C$$","['integration', 'calculus', 'solution-verification']"
3802790,Question about affine plane minus origin not being affine,"I am confused about the example given in Vakil's algebraic geometry notes that states $U:=\mathbb{A}_k^2-\{(x,y)\}$ is not an affine scheme. Questions about this example have been asked on here before, but none seem to have the same confusion as I do (nor is my confusion relieved by the answers provided). We start by assuming that $U$ is affine. Say $(U,\mathcal{O}_{\mathbb{A}_k^2|U})=(\operatorname{Spec}A,\mathcal{O}_{\operatorname{Spec}A})$ for some ring $A$ . Then we can recover $A$ by global setions: $A=\Gamma(U,\mathcal{O}_{\mathbb{A}_2|U})$ , which we have previously identified to be $k[x,y]$ . So by assuming $U$ is affine, we have that $U\cong\mathbb{A}^2_k$ . Now, the next part is what I have been struggling with. Vakil writes "" But this bijection between prime ideals in a ring and points of the spectrum is more constructive than that: given the prime ideal $I$ , you can recover the point as the geometric point of the closed subset cut out by $I$ , i.e., $V(I)$ , and given a point $p$ , you can recover the ideal as those functions vanishing at p, i.e., $I(p)$ . In particular, the prime ideal $(x,y)$ of $A$ should cut out a point of $\operatorname{Spec}A$ "". I am not sure what he means here. Don't both sides of $U\cong\mathbb{A}^2_k$ consist of prime ideals (which can also be interpreted as point on $\operatorname{Spec}A$ ), except $U$ has one fewer point (prime ideal), namely $(x,y)$ ? Grammatically, this paragraph has pronouns for which the antecedent is unclear. (For example, does ""this bijection"" refer to $U\cong\mathbb{A}^2_k$ or to the general association of prime ideals with points on a spectrum?)Perhaps if I had more intuition, I'd be able to figure it out from context. Unfortunately, I don't. Could someone please explain more explicitly what is going on here?","['algebraic-geometry', 'abstract-algebra']"
3802818,Binary Operations and Inverse elements in groups and functions,"Suppose we have group $G$ in relation to binary operation $*$ and $a\in G$ and function $f$ from $G$ to $G$ ( $f:G\to G$ ), and for each $x\in G$ $f(x)=a^{-1} * x * a$ . Prove that if $b,c\in G$ are inverse elements to each other, then $f(b)$ and $f(c)$ are inverse elements to each other as well. My way of answer: $b$ and $c$ are inverse elements, meaning that $f(b)=a^{-1} * b * a$ is the inverse of $f(c)=a^{-1} * c * a$ , but I am not quite sure.","['functions', 'group-theory', 'abstract-algebra', 'binary-operations']"
3802836,Explicit equations for $Y(N)$ for small $N$,"Consider the congruence subgroup $$\Gamma(N) = \left\{\left(\begin{array}{cc}
                            a & b \\
                             c & d\end{array} \right) \in SL_2(\mathbb{Z})\ ;\ \left(\begin{array}{cc}
                            a & b \\
                             c & d\end{array} \right) \equiv  \left(\begin{array}{cc}
                            1 & 0 \\
                             0 & 1\end{array} \right)\mod N\right\}$$ acting on the upper half-plane $\mathfrak{h} = \{\tau \in \mathbb{C} \ ;\  \Im(\tau)>0\}$ by fractional linear transformations: $$ 
\left(\begin{array}{cc}
                            a & b \\
                             c & d\end{array} \right) \cdot \tau = \frac{a\tau + b}{c\tau + d}
$$ It is known that the quotient $Y(N) = \Gamma(N)\backslash\mathfrak{h}$ is an algebraic curve for every $N$ . For instance, $$Y(2) = \mathbb{P}^1 \setminus \{0,1,\infty\}$$ which can be proved by considering modular function $\lambda : \mathfrak{h} \to \mathbb{C}$ . It is also well known that the compactification $X(7)$ of $Y(7)$ is isomorphic to Klein's quartic in $\mathbb{P}^2$ . What about the other small values of $N$ ? How do I compute $Y(N)$ for $N=3,4,5,6$ ? I know I can look at the genus of the compactification $X(N)$ , but is there a way of getting the precise form of $Y(N)$ as in the example for $N=2$ ? Bonus: What about the universal elliptic curve over $Y(N)$ , is it also computable in these small cases?","['algebraic-curves', 'modular-function', 'algebraic-geometry', 'modular-group', 'modular-forms']"
3802842,"Let $f : (0,2)\cup(4,6) \to R$. Suppose also that $ f'(x)=1$ . Which of the following is ALWAYS true $?$","Let $f : (0,2)\cup(4,6) \to R$ be a differentiable function. Suppose also that $ f'(x)=1$ for all $x \in (0,2)\cup(4,6)$ . Which of the following is ALWAYS true $?$ a.) $f$ is increasing b.) $f$ is one-to-one c.) $f=x$ for all $x \in (0,2)\cup(4,6)$ d.) $f(5.5)- f(4.5) = f(1.5)- f(0.5)$ Source Only one option is correct. My approach: By looking at the options, I can deduce that option-D is correct. As it is given that function is differentiable and $(0.5,1.5,4.5,5.5) \in (0,2)\cup(4,6)$ . So it is correct by using Lagrange’s mean value theorem. But option- A seems to be correct too as $ f'(x)>0$ , hence function should be increasing. Also when we integrate the given differential equation the solution is $$f(x)=x+c$$ and it seems it is injective too. where am I going wrong?","['calculus', 'derivatives']"
3802863,$M\setminus \mathrm{Cut}(L)$ deforms to $L$,"Let $M$ be a connected complete Riemannian manifold, $p\in M$ and $\mathrm{Cu}(p)$ denotes the cut locus of a point . This is a standard result that $M\setminus\mathrm{Cu}(p)$ deforms to $p$ . Now if $L$ is a compact submanifold of $M$ and $\mathrm{C}u(L)$ denotes the cut locus of $L$ then is it true that $M\setminus \mathrm{Cu}(L)$ deforms to $L$ . I checked some of the examples and it worked. But I am unable to prove this fact. Any reference or proof will be appreciated. Edit We say $q\in \mathrm{Cu}(L)$ if any distance minimal geodesic joining $L$ to $q$ is no longer distance minimal beyond $q$ . By deformation, I mean to find $$H:M\setminus \mathrm{Cu}(L)\times [0,1]\to M\setminus \mathrm{Cu}(L)$$ such that $H(x,0)= x,~ H(q,1)\in L$ and $H(q,t)=q$ (if $q\in L$ ). Thanks!","['riemannian-geometry', 'differential-geometry']"
3802869,Probability of majority vote to be correct,"Let $X$ be a random variable taking values from $[k] = \{1, 2, ..., k\}$ with probabilities $p_1, ..., p_k$ , respectively. Suppose that $X$ is slightly more likely to be 1: there exists some $\epsilon > 0$ such that for all $1 < i \leq k$ , $p_1 - p_i \geq \epsilon$ . Now, suppose we have $n$ independent copies of $X$ : $X_1, X_2, ..., X_n$ . For each $j \in [k]$ , define the random variable $Y_j$ to be the ""number of votes"" for $j$ : $Y_j := |\{t \in [n] : X_t = j\}|$ . Define the majority random variable $M$ to be the ""winner candidate"", i.e. the arg-max of $Y_1,...,Y_k$ (if there is more than a single maximizer, $M$ equals one of them arbitrarily. In order to make $M$ well defined, assume it equals the smallest such index). I want to bound the probability that $M \neq 1$ . For $k=2$ the problem is well-known and an exponential bound is not difficult to obtain. My attempt I am not sure about it at all, but this is what I tried. From the union bound, $$\Pr[M \neq 1] \leq \Pr[\exists i\neq 1: Y_i > Y_1] \leq \sum_{i=2}^k \Pr[Y_i > Y_1] \text{ ,}$$ And by the law of total probability, $$\Pr[Y_i > Y_1] = \sum_{t=0}^n \Pr[Y_i >Y_1 | Y_i + Y_1 = t]\Pr[Y_1 + Y_i = t]$$ Now $\Pr[Y_i+Y_1 = t]$ is like Binomial random variable with success probability $p_1 + p_i$ , which is smaller than $2p_1 -\epsilon$ by the assumption on $X$ . Thus, $\Pr[Y_1 + Y_i = t] \leq {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t}$ . Furthermore, $\Pr[Y_i >Y_1 | Y_i + Y_1 = t] = \Pr[Y_1 \leq t/2 - 1 | Y_1 + Y_i = t]$ . I think that this is like asking what is the probability that a Binomial random variable $B(t, p_1)$ is smaller than $t/2$ . I can bound it using Hoeffding's inequality: $$ \Pr[Y_i >Y_1 | Y_i + Y_1 = t] \leq e^{-2t(p_1 - 1/2)^2}.$$ Then I can combine the two results and conclude that $$\Pr[M \neq 1] \leq (k-1)  \sum_{t=0}^n e^{-2t(p_1 - 1/2)^2} {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t}. $$ My issue with this solution (beyond just not being sure if that's right) is that if $p_1 = 1/2$ I would expect the majority to be $1$ with overwhelming probability, but this bound does not capture this behaviour, which makes me trust it even less.","['binomial-distribution', 'binomial-coefficients', 'probability-theory', 'probability', 'random-variables']"
3802960,"Prove $\frac{\mathbb Z[X,Y]}{(5,X^{2}-Y,XY+X+1)}$ is a field","Prove $\frac{\mathbb Z[X,Y]}{(5,X^{2}-Y,XY+X+1)}$ is a field. I thought to prove that this is isomorphic with $\mathbb{\mathbb Z_{5}(X)}$ , and because $5$ is prime it will follow that it's a field. I wanted to use the first isomorphic theorem.
I wanted to use the map $\phi: Z[X,Y]\mapsto\mathbb{Z}_{5}(X)$ , $f(x,y)\mapsto f(x,x^{2})$ .
Now I'm proving that 1) $\phi$ is a morphism 2) $\phi$ is surjective 3) $\ker\phi=(y-x^{2},x^{3}+x+1,5)$ take $x,y \in \mathbb{Z[X,Y]}$ random then: $\phi(x+y)$ = $\phi(\sum((a_{i1}a_{i2}+b_{i1}b_{i2})X^{i1}Y^{i2})$ = $(\sum((a_{i1}a_{i2}+b_{i1}b_{i2})X^{i1}Y^{i2})$ = $\sum((a_{i1}a_{i2}X^{i1}Y^{i2})+\sum(b_{i1}b_{i2})X^{i1}Y^{i2})$ = $\phi(x)$ + $\phi(y)$ $\phi(xy)$ = $\phi(\sum((a_{i1}a_{i2}b_{i1}b_{i2})X^{i1}Y^{i2})$ = $(\sum((a_{i1}a_{i2}b_{i1}b_{i2})X^{i1}Y^{i2})$ = $\sum((a_{i1}a_{i2}X^{i1}Y^{i2})\sum(b_{i1}b_{i2})X^{i1}Y^{i2})$ = $\phi(x)$$\phi(y)$ i don't know how to prove this let's prove two inlcusions. first let $f\in ker\phi$ then $f\in (Z[X,Y])([Z])$ . We use the division algorithm, then there exist an $q(x,y)$ and a $r(x,y)$ so that $f(x,y)$ = $q(x,y)(x^{3}+x+1)$ + $r(y-x^{2})$ + $5$ met $deg(x)<deg(x^{3}+x+1)=3$ I'm not sure how to prove those things but this is what i already have. Can someone help me further. EDIT: my answer that i tried to prove is wrong. Some of you write a solution down. But i still need to prove that it's isomorphic with your solution and i'm still struggeling with the same question how to do that exactly EDIT:
So the people who answered my question (thank you for that) don't really see my problem now.
Well now after you guys helpt me I want to prove that $\frac{Z[X,Y]}{5,X^{2}-Y,XY+X+1}$ is isomorphic with $\frac{F_{5}[X]}{(X^{3}+X+1)}$ .
So I need to prove that for the map the map $\phi$ : $Z[X,Y]$ $\mapsto$$\frac{F_{5}(X)}{X^{3}+X+1}$ : $f(x,y)$$\mapsto$$f(x,x^{2})$ .
Now I'm proving that 1) $\phi$ is a morphisme 2) $\phi$ is surjective 3)ker $\phi$ = $(y-x^{2},x^{3}+x+1,5)$ I'm stuck with proving these three things correctly","['ring-isomorphism', 'abstract-algebra']"
3802983,Do lifts of maps from extremally disconnected compact Hausdorff spaces into certain colimits exist?,"Let $T$ be a extremally disconnected compact Hausdorff space. Further let $X$ and $Y$ be two ind-compact Hausdorff spaces, i.e. both are the filtered colimit of compact Hausdorff spaces with injective transition maps. Assume that $f \colon X \to Y$ is a continuous surjective map and $h \colon T \to Y$ is continuous. My question is wether there exists a continuous lift $g \colon T \to X$ of $h$ such that $h= f \circ g$ . I know that this is true if $X$ and $Y$ are compact Hausdorff spaces themselves since extremally disconnected compact Hausdorff spaces are the projective  objects in the category of compact Hausdorff spaces. I might have to add, that $X$ and $Y$ share the same underlying filtrated category $\mathbb{R}_{>0}$ and whenever $f$ sends an element $x \in X_c$ to an element $f(x) \in Y_d$ then $d \leq c$ .","['general-topology', 'category-theory']"
3802997,For $\omega$ and $\eta$ k-forms exist a $C^{1}$ function $f: \mathbb{R}^{3} \to \mathbb{R}$ such that $\eta = f\omega$.,"Let's consider a $k$ - form $\omega$ , $$\omega = \sum_{i_{1} < ... < i_{k}} \omega_{i_{1}, ..., i_{k}} dx^{i_{1}} \wedge ... \wedge dx^{i_{k}}$$ $\omega$ is $C^{r}$ if $\omega_{i_{1},... ,i_{k}}$ is $C^{r}$ . Consider $\Omega^k(U)$ the set of $C^{\infty}$ $k$ -forms in $U$ . My question is the following: Let $\omega,\ \eta\ \in \Omega^{1}(\mathbb{R}^{3})$ . If $\omega(x) \neq 0$ for all $x \in \mathbb{R}^{3}$ and $\omega \wedge \eta = 0$ , then exist a $C^{1}$ function $f: \mathbb{R}^{3} \to \mathbb{R}$ such that $\eta = f\omega$ . My attempt: consider $\omega = \omega_{1}dx + \omega_{2}dy + 
\omega_{3}dz$ and $\eta = \eta_{1}dx + \eta_{2}dy + \eta_{3}dz$ , thus $$\omega \wedge \eta = (\omega_{1}\eta_{2} - \omega_{2}\eta_{1}) dx \wedge dy + (\omega_{1}\eta_{3} - \omega_{3}\eta_{1}) dx \wedge dz +\\ (\omega_{2}\eta_{3} - \omega_{3}\eta_{2}) dy \wedge dz$$ Using that $\omega \wedge \eta = 0$ and $\{dx \wedge dy, dx \wedge dz, dy \wedge dz \}$ is L.I. we have that $\omega_{1}\eta_{2} = \omega_{2}\eta_{1}$ ; $\omega_{1}\eta_{3} = \omega_{3}\eta_{1}$ ; $\omega_{2}\eta_{3}  = \omega_{3}\eta_{2}$ . Remember that we want a function $f$ such that $f\omega_{i} = \eta_{i}$ . Suppose that exist $p \in \mathbb{R}^{3}$ such that $\omega_{3}(p) = 0$ , then $\omega_{1}\eta_{3} = 0$ and $\omega_{2}\eta_{3} = 0$ and so $\omega_{1}(p) = 0$ or $\omega_{2}(p) = 0$ or $\eta_{3}(p) = 0$ . If $\eta_{3}(p) \neq 0$ , then $\omega(p) = 0$ , contradiction. So, we conclude that $\eta_{3}(p) = 0$ . Hence, we can define $f$ like $$f(p) = \begin{cases} \frac{\eta_{3}}{\omega_{3}}(p), &\text{ if }\omega_{3}(p) \neq 0, \\ 0, & \text{ if }\omega_{3}(p) = 0.\end{cases}$$ My problem is prove that $f$ is a $C^{1}$ function. Someone can help me?","['multivariable-calculus', 'differential-forms', 'exterior-algebra']"
3803025,Why does the equation with $2 \arctan(x)$ and other Inverse Trigonometric functions have weird conditions?,"I have these three equations: $$2\arctan(x) = \arcsin(\frac{2x}{1+x^2}),\left |x\right|\leq1$$ $$2\arctan(x) = \arccos(\frac{1-x^2}{1+x^2}),x\geq0$$ $$2\arctan(x) = \arctan(\frac{2x}{1-x^2}),\left |x\right|<1 $$ I can verify these by simple substitution taking $x = \tan\theta$ for some $\theta$ but I could not understand why these conditions for $x$ are given.
I can see their graphs makes sense but is there an algebraic way of proving/verifying these conditions? Also the graph is:","['trigonometry', 'inverse-function']"
3803042,Show that : $f(x)+f(1-x)\leq 2$,"I'm very proud to show one of my dream in term of inequalities . Claim Let $0.25\leq x\leq 0.75$ and $x\neq \frac{2k+1}{100}$ with $12\leq k\leq 37$ and $k$ a natural number then define the function : $$f(x)=x^{\frac{1}{\cos^2(x50\pi)}}+x^{\cos^2(x50\pi)}$$ then we have : $$f(x)+f(1-x)\leq 2$$ First we have $50$ (limit) equality cases as $x=\frac{25}{100},\frac{26}{100},\frac{27}{100},\cdots,\frac{73}{100},\frac{74}{100},\frac{75}{100}$ To prove it I have tried Bernoulli's inequality as we have : $$x^{\frac{1}{\cos^2(x50\pi)}}\leq \frac{1}{1+\Big(\frac{1}{x}-1\Big)\frac{1}{\cos^2(x50\pi)}}$$ And : $$x^{\cos^2(x50\pi)}\leq 1+(x-1)\cos^2(x50\pi)$$ But it doesn't work . I add a graph to convince you : Update as partial answer : It's an heavy method but it works numerically speaking . Well we show that the inequality is true for $x\in[0.307,0.31)$ and $x\in(0.31,0.313]$ . Firstly on these intervals we have : $$(1-x)^{\cos((1-x)50\pi)^2}+x^{\frac{1}{\cos(x50\pi)^2}}\leq 1\quad (1)$$ And $$x^{\cos(x50\pi)^2}+(1-x)^{\frac{1}{\cos((1-x)50\pi)^2}}\leq 1\quad(2)$$ Now we use the method used here General trick to factorize an inequality of the kind $a+b\leq 1$ . The problem becomes : $$\sin\Big(x^{\frac{1}{\cos(x50\pi)^2}}\frac{\pi}{2}\Big)\leq \cos\Big((1-x)^{\cos((1-x)50\pi)^2}\frac{\pi}{2}\Big)$$ Or : $$\ln\Big(x^{\frac{1}{\cos(x50\pi)^2}}\frac{\pi}{2}\Big)\leq \ln \Big(\sin^{-1}\Big(\cos\Big((1-x)^{\cos((1-x)50\pi)^2}\frac{\pi}{2}\Big)\Big)\Big)$$ We study the function : $$h(x)= \ln \Big(\sin^{-1}\Big(\cos\Big((1-x)^{\cos((1-x)50\pi)^2}\frac{\pi}{2}\Big)\Big)\Big)-\ln\Big(x^{\frac{1}{\cos(x50\pi)^2}}\frac{\pi}{2}\Big)$$ The derivative is here Studing this function we see that for $x\in[0.307,0.31)$ the function is increasing and decreasing for $x\in(0.31,0.313]$ But : $$f(0.307)>0 \quad \operatorname{and} \quad f(0.313)>0$$ Happy ending ! Question How to show my claim ? Thanks in advance ! Regards Max .","['limits', 'trigonometry', 'exponentiation', 'inequality']"
3803138,question relating to the Euler's totient function,"I just cam across a question in number theory which relates to Euler's totient function. The question is the following: We have a positive integer $n>1$ . Find the sum of all numbers $x$ , such that $x\in {1, 2, ..., n}$ , which are relatively prime with n. I solved it in the following fashion: We have number $d$ which is relatively prime with $n$ , we also have that $n-d$ is relatively prime with $n$ . So the total addition, is $\frac{n*\phi(n)}{2}$ However, I know that there exists a solution with the use of the inclusion exclusion principle. Could you please explain to me how I could solve it using PIE?","['contest-math', 'number-theory', 'inclusion-exclusion', 'combinatorics', 'totient-function']"
3803141,Total Variation in multidimension,"A function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ has bounded variation if \begin{eqnarray}
TV_{\mathbb{R}^2}(f):=\sup\limits_{\phi \in C_c^1(\mathbb{R}^2), ||\phi||_{L^{\infty}(\mathbb{R}^2)}\leq 1}\, \int\limits_{\mathbb{R}^n}f(x,y)\operatorname{div} \phi(x,y)\, dy dx < \infty.
\end{eqnarray} I am trying to prove the following characterization of the above definition of total variation of a function of two variable in terms of the total variation of the one variable function. \begin{eqnarray}
TV_{\mathbb{R}^2}(f) = \int\limits_{\mathbb{R}}TV_{\mathbb{R}}(f(\cdot,y))\, dy+ \int\limits_{\mathbb{R}}TV_{\mathbb{R}}(f(x,\cdot))\, dx.
\end{eqnarray} P.S: I could prove that \begin{eqnarray}
TV_{\mathbb{R}^2}(f)\leq \int\limits_{\mathbb{R}}TV_{\mathbb{R}}(f(\cdot,y))\, dy+ \int\limits_{\mathbb{R}}TV_{\mathbb{R}}(f(x,\cdot))\, dx.
\end{eqnarray} How to prove the other way round?","['bounded-variation', 'measure-theory', 'total-variation', 'real-analysis']"
3803170,How could have random numbers been generated hundreds of years ago?,"Though I can't provide a concrete example, in the history of science
there must have been numerous cases where scientists built complicated
probabilistic models, for which the knew the probabilities of some
simple events, but for which it was very difficult to formally compute
the probabilities of various other, more complicated events that were
composed of these simple ones (e.g. take a graph with particles on
it, that jump at times $t=1,2,3,\ldots$ with some probability $p\in(0,1)$ to any adjacent node and merge to a new particle of two particles
meet; these are very simple starting probabilities, but it can become
a research-level problem to compute probabilities of events like the
time until only one sole particle randomly jumps around on the graph). In that case, by doing simulations, it is easy to estimate the probabilities.
How did scientists estimated these probabilities by simulation, if
they did not have a computer or books with look-up tables of random
numbers that quickly supplied them with a list of (uniformly) random
number (from which almost any other distribution can then be constructed)?
Many very sophisticated ways of generating pseudo-random numbers have
recently been devised, but I'm interested what people did in, e.g.,
1800, when such methods were not yet available? Did they devise some clever ad-hoc physical experiment and hoped that
the numbers obtained that were sufficiently random, in order to be
able to carry out their simulation? If so, what experiment was that?","['math-history', 'probability-theory', 'sampling']"
3803187,Find a parametric equation of a plane curve,"The task is to find a parametric equation of a curve, if you are given its curvature in arclength parametrisation. I know that I need to integrate but I don’t get the idea of angle theta(what’s it?) and plus there are constants of integration. Would be very grateful if someone explained how to solve it. For example: $$k(s)=\frac1{as\sqrt2}, \text{where }a=\text{constant}$$","['differential', 'plane-curves', 'curvature', 'derivatives', 'differential-geometry']"
3803246,"An optimization problem related to parabolas, yields a hard to solve derivative","Hello, I have came up with what I think is a unique optimization problem.
We are given the positive real parameters $k,t$ . $t$ is the height of the rectangle, and $k$ is half of its width (see the picture).
Let the parabola intersect the $x$ -axis at $x=p, -p$ and pass through the top vertices: $(k,t), (-k,t)$ of the rectangle. The equation of the parabola is: $$y=ax^2+c$$ $c > t > 0$ , $a < 0$ . We want to find $a,c$ such that the length of the parabolic curve from $-p$ to $p$ is minimized (note that $p$ is dependent on $a$ and $c$ ). My work on the problem so far:
notice that $(k,t)$ is a point on the parabola. So: $$ak^2+c=t$$ $$c=t-ak^2$$ So know we really just need to find $a$ .
let us solve for $p$ ( $x$ intersection): $$ap^2+c=0$$ $$p=\sqrt{\frac{-c}{a}}=\sqrt{\frac{ak^2-t}{a}}=\sqrt{\frac{t-ak^2}{-a}}=\frac{\sqrt{t-ak^2}}{\sqrt{-a}}$$ recall that the length of a curve from $-p$ to $p$ is given by: $$\int _{-p}^p\:\sqrt{1+\left(\frac{dy}{dx}\right)^2}dx$$ By symmetry, the length of the parabola from $-p$ to $p$ will be 2*length of the parabola from $0$ to $p$ .
Subsituting: $$2\int _{x=0}^{x=p}\:\sqrt{1+\left(2ax\right)^2}dx$$ This integral is quite easy to solve with trig substitution. After solving I got that: $$f\left(a\right)=\frac{2ap\sqrt{1+4a^2p^2}+ln\left(2ap+\sqrt{1+4a^2p^2}\right)}{2a}$$ Note that $p$ is not a constant, because it is dependant on $a$ .
It seems as though we have made progress, we have managed to build a function, dependant on only one variable, that returns the length of the curve. The function works and returns a correct result (I have verefied it with a couple of examples) but the derivative is very long and i doubt that it is solvable. I am quite stuck here with my current knowledge. But I am positive because when you graph the function on desmos there is indeed one local minimum for $a < 0$ . An easier variant is trying to minimize the area blocked by the parabola and the $x$ axis. It was much easier to solve...","['conic-sections', 'derivatives', 'calculus', 'optimization', 'quadratics']"
3803294,Where's the catch? Game involving taking turns choosing $n \times n$ matrices over $\mathbb{F}_p$ that all commute with each other.,"Let $S$ be the group of invertible $n \times n$ matrices over $\mathbb{F}_p.$ Alice and Bob take turns choosing elements of $S$ such that no element is repeated and every element must commute with all previous matrices. Whoever cannot move loses. Who wins? My solution: Let $A = \{ X \in S : X^2 = I\}, B = \{ X \in S : X^2 = -I\}, C = S \setminus (A \cup B).$ If $p \ne 2,$ then $X \ne -X$ for all $X \in S.$ Thus, Bob may win by playing $f(X)$ if Alice plays $X,$ where $$f(X) = \begin{cases} -X, \, X \in A \text{ or } X \in B \text{ and n odd}\\ X^{-1}, \text{ else} \end{cases}.$$ Suppose Alice just played $X$ and previously played $Y \ne X.$ To show Bob never repeats a move, it suffices to show that $f(X) \notin \{Y, f(Y)\}.$ Since $f$ is an involution, $f(X) \in \{Y,f(Y)\} \Rightarrow X \in \{f(Y), Y\},$ which means Alice repeated a move and already lost. Thus, Bob wins if $p \ne 2.$ Suppose $p = 2$ and $X \in A.$ The minimal polynomial of $X$ divides $t^2 - 1 = (t-1)^2 \mod 2,$ so the only eigenvalue of $X$ is $1.$ Let $X = PJP^{-1}$ and let $J_i = I + N$ where $N$ is the matrix with ones on the diagonal right above the main one be a Jordan block of $J.$ We must have $I = J_i^2 = I + N^2 \Rightarrow N^2 = 0 \Rightarrow J_i$ has size $1$ or $2.$ I suspect that in the case $p = 2,$ Alice wins by playing $I,$ then playing $X^{-1}$ whenever Bob plays $X \notin A,$ but I have no idea what she should do in the case $X \in A$ is played. For $n=1,2,$ we can see that Alice wins easily, but trying to analyze all $n$ choices for $J$ is unfeasible for larger $n$ since we have tons of choices for $P$ as well. I believe the catch is that this final case is much harder than the rest of the problem, and I've only scratched the surface of the tip of the iceberg when it comes to solving it. How should I proceed? Any hints, approaches, or ideas? If we look for a strategy involving just polynomials, we will observe that $\mathbb{F}_2[X]/\langle X^2 - 1 \rangle = \{ O, I, X, X+I\}.$ However, $(X+I)^2 = X^2 + I = 2I = O$ and the other choices are not viable either. Thus, our strategy (which ostensibly must rely on performing some sensible operations on the set of previously played matrices) has to rely on the products of matrices. But which matrices shall we multiply?","['group-theory', 'linear-algebra', 'combinatorial-game-theory']"
3803349,Existence of a special vertex in a special DAG,Suppose we're given a Directed Acyclic Graph $G$ with $L$ leaves such that every non-leaf vertex has out-degree $2$ . Let $r(v)$ the number of leaves reachable starting at vertex $v$ . $G$ is such that it has a root vertex $v_0$ with $r(v_0) = L$ . How does one prove that given these conditions $G$ also has a vertex $v$ such that $\dfrac{L}{3}\leq r(v)\leq\dfrac{2L}{3}$ ? As we go from the root $v_0$ towards any one leaf $l_0$ we trace out a path along which $r(\cdot)$ either decreases or stays the same and overall it decreases from $L$ to $1$ . This is true for all such paths. But how does one prove the existence of a vertex with $\dfrac{L}{3}\leq r(v)\leq\dfrac{2L}{3}$ ?,"['graph-theory', 'combinatorics', 'discrete-mathematics']"
3803370,Why is this method wrong on calculating this multivariable limit?,"So, I was studying Apostol's book while studying on the site ""Brilliant"" methods of calculating multivariable limits... In particular, in $R^2$ we have polar coordinates to switch on and we have: $\lim_{{(x,y)}\to(0,0)}f(x,y) = L$ iff $\lim_{r\to0^+}f(r\cos(\theta),r\sin(\theta)) = L$ since the statement $0\lt\sqrt{x^2+y^2}\lt\delta$ can be translated into $0\lt r \lt \delta$ from the $\epsilon-\delta$ definition of the limit while $x = r\cos(\theta)$ and $y = r\sin(\theta)$ (so the limit exists iff the limit exists in polar coordinates and it's $\theta-independent$ ) (taken from Brilliant) But then Apostol came with the following function: $f(x,y) = \frac{xy^2}{x^2+y^4}$ if $x\neq 0$ and $f(0,y) = 0$ and things got messy in my mind because, if we switch to polar coordinates, it becomes $f(r\cos(\theta),r\sin(\theta)) = \frac{r\cos(\theta)\sin^2(\theta)}{\cos^2(\theta)+\sin^4(\theta)}$ if $r$ is different from $0$ and if we make $r\to0$ we'd have have $\lim_{r\to0^+}f(r\cos(\theta),r\sin(\theta)) = 0$ But, if you choose the curve $x = y^2$ , we have $f(y^2,y) = \frac{1}{2}$ and so if we approach the origin by that curve we'd have $\lim_{y\to0}f(y^2,y) = \frac{1}{2}$ and by such we'd have the limit approaching $2$ different values which would mean the limit actually doesn't exist So my doubt is about what is wrong about the procedure using polar coordinates instead of trying different curves, why the polar coordinate method didn't show me that the limit is ""angle dependent"" (and it doesn't exist in practice)? Did I make any mistakes in the procedure?","['limits', 'multivariable-calculus', 'polar-coordinates']"
3803377,If $\phi$ and $\sigma$ are solutions of the ODE then $\phi(t)=\sigma(t)$,"Let $\Omega$ be a subset of $\mathbb{R}\times \mathbb{R}^n$ , $f:\Omega\rightarrow \mathbb{R}^n$ a continuos function. Suppose that for all $(t_0,x_0) \in \Omega$ there exists unique local solution $x:I_{t_0,x_0}\rightarrow \mathbb{R}^n$ of \begin{equation*}
x'=f(t,x) \quad x(t_0)=x_0
\end{equation*} Prove that if $\phi:I\rightarrow \mathbb{R}^n$ and $\sigma:J\rightarrow \mathbb{R}^n$ are solutions of the ODE $x'=f(t,x)$ such that $\phi$ and $\sigma$ have a coincide point, then $\phi(t)=\sigma(t)$ for all $t\in I\cap J$ . Can somebody give a hint of what I have to do in this problem.",['ordinary-differential-equations']
3803388,"If $f$ is twice differentiable at $a$, prove $f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2}$","Let $f: U \subset \mathbb{R}^m \rightarrow \mathbb{R}^n$ twice differentiable at the point $a \in U$ .  suppose that $\lim_{k \rightarrow \infty} t_k=0$ in $\mathbb{R}$ and that $\lim_{k \rightarrow \infty} v_k=v$ ; $\lim_{k \rightarrow \infty} w_k=w$ prove that $$f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2}$$ We know that since $f$ is differentiable there is a linear transformation $$T_a: \mathbb{R}^m \rightarrow \mathbb{R }^n$$ such that $f(a+h)-f(a)=T_a h + r(h)$ where $\lim_{h \rightarrow \infty} \frac{\|r(h)\|}{\|h\|}=0$ , I don't know how to proceed, any idea please.","['multivariable-calculus', 'derivatives', 'hessian-matrix', 'real-analysis']"
3803424,How to find $\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}$ and $\sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2}$ using real methods?,"How to calculate $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}$$ and $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2}$$ by means of real methods? This question was suggested by Cornel the author of the book, Almost Impossible Integrals, Sums and Series . The way I would approach the problem is to use the series property: $$\sum_{n=1}^\infty (-1)^n f(2n)=\Re \sum_{n=1}^\infty i^n f(n),$$ namely $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}=8\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{(2n)^3}=8\Re\sum_{n=1}^\infty\frac{i^n H_n}{n^3}$$ then we use the well-known generating function $\sum_{n=1}^\infty\frac{x^nH_n}{n^3}$ . But this method is really tedious as we will need to use $\Re\{\text{Li}_2(1+i), \text{Li}_3(1+i),\text{Li}_4(1+i),\text{Li}_4(\frac{1+i}{2})\}.$ By the way, I have not seen a rigorous proof of the following equality : $$\operatorname{Re} \operatorname{Li}_4 (1 + i)= -\frac{5}{16} \operatorname{Li}_4 \left (\frac{1}{2} \right ) + \frac{97}{9216} \pi^4 + \frac{\pi^2}{48} \ln^2 2 - \frac{5}{384} \ln^4 2\tag1$$ So solving this sum in a different way would be considered a new rigorous proof of $(1)$ . For the second series, I would follow the same approach. Any idea by real methods? Thanks","['integration', 'real-analysis', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
3803440,"Cramer's theorem requires MGF finite on $[0,\delta)$ for some $\delta>0$","Let $X_n$ be i.i.d random variables with mean $0$ such that $\mathbb{E}e^{\lambda X_1}=+\infty$ . Then for all $t>0$ $$\frac1n \log\mathbb{P}(S_n\geq nt)\rightarrow 0$$ where $S_n=X_1+\cdots+X_n$ . I feel like this should be a straightforwards exercise but it's destroying my sanity for some reason. I tried to exploit the subadditivity of $\log\mathbb{P}(S_n\geq nt)$ , but that wasn't particularly useful.  I also suspect that the norm $\|X\|_{\psi_{1}}=\inf\{\lambda>0:\mathbb{E}e^{\frac{|X|}{\lambda}}\}$ and its connection to sub-exponential decay of $\mathbb{P}(X>t)$ might be useful. I would appreciate any input. Thanks in advance.","['moment-generating-functions', 'large-deviation-theory', 'concentration-of-measure', 'probability-theory']"
3803487,Is this proof to the existence of a set that contains all subsets of another set right?,"The problem comes from Exercise 3.4.6 of Terence Tao's Analysis I . In the book, there is a hint regarding the problem. However, my approach is quiet different from this hint, so I am not sure if my proof is right. Could you please help verify it? Lemma 3.4.9. Let $X$ be a set. Then the set $$\{Y : Y\ \text{is a subset of }X\}$$ is a set. My proof: (Axioms used) Axiom 3.6 (Replacement). Let A be a set. For any object $x \in A$ , and
any object $y$ , suppose we have a statement $P(x, y)$ pertaining to $x$ and $y$ , such that for each $x \in A$ there is at most one y for which $P(x, y)$ is
true. Then there exists a set $\{y : P(x, y)\text{ is true for some }x \in A\}$ , such
that for any object $z$ , $$z \in \{y : P(x, y)\text{ is true for some }x \in A\} \Longleftrightarrow
 P(x, z)\text{ is true for some }x \in A$$ Axiom 3.10 (Power set axiom). Let $X$ and $Y$ be sets. Then there exists
a set, denoted $Y^X$ , which consists of all the functions from $X$ to $Y$ , thus $f \in Y^X \Longleftrightarrow (f$ is a function with domain $X$ and range $Y$ ). According to the power set axiom, we have the set $X^X$ . Apply the axiom
of replacement to each element of $X^X$ , we construct a set $Z$ such that $$
\forall x(x \in Z \equiv \exists f(f \in X^X \wedge x = f(X)))
$$ Let $Y = \{\varnothing\} \cup Z$ . Now we prove that $Y$ is the set we want. On one hand, for any $S \subseteq X$ , if $S = \varnothing$ , then $S \in Y$ , as $Y = \{\varnothing\} \cup Z$ . If $S \neq \varnothing$ , there exists ( Is this assertion right? ) a surjective function $g: X \rightarrow S$ . $g\in X^X$ , and $g(X) = S$ , so $S \in Z$ , and thus $S \in Y$ . On the other hand, for any $S' \nsubseteq X$ , $\exists a(a \in S' \wedge a \notin X)$ . To prove that $S' \notin Y$ , we need to show that $\nexists f(f \in X^X \wedge S' = f(X))$ . We know that for any function $f: X \rightarrow X$ , $\nexists x(x \in X \wedge f(x) = a)$ , so $a \notin f(X)$ . Therefore $S' \neq f(X)$ , so $S' \notin Y$ . Thus, $Y$ is the set we want. $\square$ Is my proof right?","['elementary-set-theory', 'solution-verification']"
3803516,"Is there a function from $(0, 1)$ to $\mathbb{R}$ that is surjective but not injective?","I know there are injections [e.g. $\tan(x)$ ] and bijections [e.g. the classic $\tan(\pi (x-\frac{1}{2}))$ ] from $(0, 1)$ to $\mathbb{R}$ , but I had been stuck after I tried to construct a function from $(0, 1)$ to $\mathbb{R}$ that is only surjective but not injective. So I wonder if there is any example. Thanks in advance!","['elementary-set-theory', 'functions']"
3803529,Distribution of $(X_1-\mu)^T\Sigma^{-1}(X_1-\mu)$ is chi-squared? [duplicate],"This question already has an answer here : Finding the distribution of $\|X-\mu \|_\Sigma^2$ with $X \sim N(\mu,\Sigma)$ (1 answer) Closed 3 years ago . If $(X_i)_{i=1}^{20}\sim N_6(\mu,\Sigma),$ then find the distribution of $$ (X_1-\mu)^T\Sigma^{-1}(X_1-\mu)$$ The solution is $\chi^2_6,$ but could someone show why? I only know that the sum of standard normal variables is itself chi-squared but I'm not sure how to approach this.","['chi-squared', 'statistics', 'probability-distributions', 'normal-distribution']"
3803547,Does the derivative of a differentiable function have to be Lebesgue integrable in some interval?,"I know that the derivative of a differentiable function doesn't have to be continuous. How discontinuous can a derivative be? . Inspired by Limits and continuity of a derivative , I was thinking of defining the notion of pseudo-continuous: $f:(a,b) \to \mathbb R$ is pseudo-continuous at $x \in (a,b)$ if $$ f(x) = \lim_{y\to x} \frac1{y-x} \int_x^y f(t) \, dt .$$ And then I wanted to show that a function is the derivative of a differentiable function if and only if it is pseudo-continuous. But then I realized that the derivative doesn't have to be Lebesgue integrable, for example $$ f(x) = \frac x{\log|x|} \sin\left(\frac1x\right) , \quad x \in (-\tfrac12,\tfrac12) ,$$ or $$ f(x) = x^2 \sin\left(\frac1{x^2}\right)  ,$$ Does there exist a differentiable function $f:(0,1) \to \mathbb R$ such that its derivative restricted to any subinterval of $(0,1)$ fails to be in $L^1$ ?","['lebesgue-integral', 'derivatives', 'real-analysis']"
3803550,"How many numbers are there between two real numbers n2 and n1? Does't depend on value of |n1−n2|? (Why or why not, and what is most precise notation?)","How many numbers are there between 1 and 4 inclusive? That is, what is the number of distinct numbers found within the closed set [1,4]? How many numbers are there between -2 and 7? That is, what is the number distinct numbers found within the closed set [-2,7]? Wouldn't the answer to the latter be an infinity equal to exactly three times the former? If not, then why? Similarly, wouldn't the answer to the number of numbers between two real numbers exclusive (an open set, (n1,n2)) be equal to exactly 2 fewer than inclusive closed cardinal set [n1,n2] (which is exactly one more than either (n1,n2] or [n1,n2))? Extended to all real numbers, wouldn't the number of different numbers between (-∞,-∞) be greater than those restricted to domain of, say, (-4,-1)? If so; then by what degree, respectively notated how exactly? If not; then why not, and what would the most correct notation of the infinitude be? I have read about different types of numbers, bijections, and a little of set theories, but none has proven to me why this would not be an intuitively clear example of varying scale of infinitude, in fact to the contrary. If I am mistaken, please explain to me why. If my intuition is correct, please substantiate.","['elementary-set-theory', 'real-numbers', 'infinity']"
3803602,"Given that $2017$ is prime, how do I prove this statement?","I'm asked to prove the following statement: Let $N=(1008!)^2+1$ . Prove that $N$ is divisible by $2017$ . (Hint: $2017$ is prime.) I don't know how to go about proving this statement, since there seems to be nothing particularly special about this number except the factorial, which may point to some usage of Wilson's theorem. However, I don't know how to continue from there. Any hints? I would appreciate them a lot. (Please don't write full solutions, as I want to gain the intuition myself on how to solve these.)","['number-theory', 'factorial', 'big-numbers']"
3803615,Existence of a random variable $X$ such that the moment generating function of $X$ is given by $\exp(t^3c)$ for some number $c$?,"We know that for a normal random variable $N(0,a^{2})$ the moment generating function (MGF) $E \exp(tX)$ is given by $\exp(t^2a^2/2)$ . I am trying to find a random variable $X$ such that the MGF of $X$ is given by $\exp(t^3 c)$ for some number $c$ , and more generally $\exp(t^{n} c)$ for an integer $n \geq 3$ and a number $c$ .","['moment-generating-functions', 'measure-theory', 'probability-distributions', 'probability-theory']"
3803620,What's this matrix transformation called?,"Given an augmented matrix $$M = [A \mid X] = \left[\begin{array}{cc|c}  
 a & b & x\\  
 c & d & y  
\end{array}\right],$$ there's an associated augmented matrix defined like so $$M' = [A' \mid X'] = \left[\begin{array}{cc|c}  
 \dfrac{ax}{ax+cy} & \dfrac{cy}{ax+cy} & ax+cy\\  
 \dfrac{bx}{bx+dy} & \dfrac{dy}{bx+dy} & bx+dy
\end{array}\right],$$ as long as neither of the denominators vanish. It's probably best to assume $x,y>0$ and $a,b,c,d \geq 0$ together with $a + b = 1$ and $c + d = 1$ to prevent division by zero. Anyway, this function $M \mapsto M'$ has good theoretical properties; it's analytic, involutive, and so long as $A$ is row-stochastic and $X$ is column-stochastic, it follows that $A'$ is row stochastic and $X'$ is column stochastic. Going beyond theoretical considerations, I'll note that this transform shows up in a lot of real-world applications, whenever you're trying to perform binary classification based on an imperfect test. Example. If $M(1,1)$ is the probability of the test coming up positive assuming you have the disease $M(1,2)$ is the probability of the test coming up negative assuming you have the disease $M(1,3)$ is the probability of having the disease $M(2,1)$ is the probability of the test coming up positive assuming you don't have the disease $M(2,2)$ is the probability of the test coming up negative assuming you don't have the disease $M(2,3)$ is the probability of not having the disease then $M'(1,1)$ is the probability you have the disease assuming that the test comes up positive $M'(1,2)$ is the probability you don't have the disease assuming that the test comes up positive $M'(1,3)$ is the probability that the test comes up positive $M'(2,1)$ is the probability you have the disease assuming that the test comes up negative $M'(2,2)$ is the probability you don't have the disease assuming that the test comes up negative $M'(2,3)$ is the probability that the test comes up negative It's straightforward to generalise the function $(M \mapsto M')$ so that it can be applied to any augmented square matrix in which the denominators don't vanish. Question. Does this transform (and/or the matrix it produces) have a name? Addendum 5th Septemeber, 2020. I think I'll call it the conditional dual for now. I remain very interested in this notion, and would love to hear from anyone who knows more about it.","['conditional-probability', 'matrices', 'linear-algebra', 'stochastic-matrices', 'probability']"
3803670,Motivation and Application of the Fourier Series,"I am currently reading some material on the Fourier series. The main motivation of course is to write a periodic function as a series involving cosine and sine functions, which we understand much about. It turns out that we have some nice convergence results regarding the Fourier series and the function which we wish to compute the series for. Now the texts I am reading show that it is possible to derive solutions for PDE's by applying Fourier series methods, however, the text does not mention when such an application will work. So I am curious to find out when can someone use Fourier series methods to find solutions of PDE's? Also given a PDE is it always possible to re-write the PDE for the Fourier series? For example, suppose we consider $-\Delta u+u=f$ in $\Omega\subset\mathbb{R}^{N}$ with zero boundary conditions. What conditions must be satisfied so that $\hat{-\Delta u}+\hat{u}=\hat{f}$ in $\Omega\subset\mathbb{R}^{N}$ makes sense, with appropriate boundary conditions? Is it sufficient to take the periodic extension of, $\tilde{u}$ , on the appropriate domain $\tilde{\Omega}$ , and hence we can always consider the ""transformed"" PDE? Is it necessary for the differential operator to have eigenfunctions which form an orthonormal basis of $L^{2}(\Omega)$ ? I understand that for any periodic function one can always take the Fourier series. However, it is not clear to me under what conditions one can consider solutions of a PDE  as a Fourier series and more specifically, when one can consider the ""transformed"" PDE.","['fourier-series', 'fourier-analysis', 'functional-analysis', 'partial-differential-equations']"
3803681,Non-zero probability of hitting a convex hull of $d+1$ i.i.d. points in $\mathbb{R}^d$,"Let $X_1, \ldots, X_{d+1}$ be $d + 1$ i.i.d. random points in $\mathbb{R}^d$ sampled from a continuous probability $\mu$ of density $f$ . Let $x_0 \in \mathbb{R}^d$ . Is it true that almost surely with respect to $\mu$ , $$\mathbb{P}(x_0 \in \mathrm{Conv}(X_1, \ldots, X_{d+1})) > 0$$ where $\mathrm{Conv}$ denotes the convex hull of the points? This assertion is part of an answer to a mathoverflow question that was given to me in comment. I suspect that am not familiar enough with the regularity properties of approximate continuity in order to conclude. My current reasoning is as follows: if we suppose that $f$ is almost surely positive in a local neighbourhood around $x_0$ , then we can find neighbourhoods of the $d+1$ vertices of a simplex having $x_0$ in its interior in the local neighbourhood of $x_0$ , and the $X_i$ will be sampled in these neighbourhoods with non-zero probability. However, I am not sure if with probability 1, we can find neighbourhoods of $f$ that are almost surely positive.","['lebesgue-measure', 'geometric-probability', 'convex-hulls', 'probability-theory', 'probability']"
3803699,Evaluate $\int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta$,"In the Hardy's book Divergent series is proved the equality $$\frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi)$$ With $m\in\mathbb{N}$ . Then integrating that equality $0<\theta<\pi$ (he says ""ignoring any difficulties about the range of $\theta$ over which it may be expected to be valid) we obtain $$\int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=\pi \frac{\sin m\phi}{\sin \phi}$$ How that result is reached? I've tried, in a sloppy way (because I didn't proved the possibility to exchange series and integral), this approach with Werner identities $$\int_0^\pi 2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi) \text{d}\theta=$$ $$2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos n\theta \cos m\theta-\cos n\theta\cos m \phi) \text{d}\theta=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos (n\theta+m\theta)+\cos (n\theta-m\theta) -\cos (n\theta+m\phi) -\cos (n\theta-m\phi)) \text{d}\theta=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\theta+m\theta)}{n+m}+\frac{\sin(n\theta-m\theta)}{n-m} -\frac{\sin(n\theta+m\phi)}{n} -\frac{\sin(n\theta-m\phi)}{n}\right]_{\theta=0}^{\theta=\pi}=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\pi+m\pi)}{n+m}+\frac{\sin(n\pi-m\pi)}{n-m} -\frac{\sin(n\pi+m\phi)}{n} -\frac{\sin(n\pi-m\phi)}{n}\right]$$ And here I'm stuck. Any help is appreciated.","['definite-integrals', 'sequences-and-series', 'real-analysis']"
3803719,Surjectivity and composition in functions,"Sorry for this pretty dumb question, but I couldn't find any answer to it. Supposing we have a set $A$ and functions $f,g:A\to A$ . Prove that if $f$ isn't surjective then $f \circ g$ isn't surjective. Basically as I think, In order for a composite function to be surjective' both the functions that are getting composited (In this case functions $f$ and $g$ ) should be surjective. Is that right or wrong? And how can I prove that? EDIT: Question is solved, Check my answer.","['elementary-set-theory', 'functions']"
