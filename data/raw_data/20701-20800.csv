question_id,title,body,tags
174779,"If $\phi \in C^1_c(\mathbb R)$ then $ \lim_n \int_\mathbb R \frac{\sin(nx)}{x}\phi(x)\,dx = \pi\phi(0)$.","Let $\phi \in C^1_c(\mathbb R)$. Prove that $$
\lim_{n \to +\infty} \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx = \pi\phi(0).
$$ Unfortunately, I didn't manage to give a complete proof. First of all, I fixed $\varepsilon>0$. Then there exists a $\delta >0$ s.t. 
$$
\vert x \vert < \delta \Rightarrow \vert \phi(x)-\phi(0) \vert < \frac{\varepsilon}{\pi}.
$$
Now, I would use the well-known fact that 
$$
\int_\mathbb R \frac{\sin x}{x} \, dx = \pi.
$$
On the other hand, by substitution rule, we have also
$$
\int_\mathbb R \frac{\sin(nx)}{x} \, dx = \int_\mathbb R \frac{\sin x}{x} \, dx = \pi.
$$
Indeed, I would like to estimate the quantity
$$
\begin{split}
& \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \pi \phi(0) \right\vert = \\ 
& = \left\vert \int_\mathbb R \frac{\sin(nx)}{x}\phi(x) \, dx - \phi(0)\int_\mathbb R \frac{\sin{(nx)}}{x}dx \right\vert \le \\ 
& \le \int_\mathbb R \left\vert \frac{\sin(nx)}{x}\right\vert \cdot \left\vert  \phi(x)-\phi(0) \right\vert dx
\end{split}
$$
but the problem is that $x \mapsto \frac{\sin(nx)}{x}$ is not absolutely integrable over $\mathbb R$. Another big problem is that I don't see how to use the hypothesis $\phi$ has compact support. I think that I should use dominated convergence theorem, but I've never done exercises about this theorem. Would you please help me? Thank you very much indeed.","['real-analysis', 'limits']"
174783,How to solve this system of fractional equations?,"I have to complete a summer packet of 90 Algebra 2 questions. I have completed 89 of them, the only one I could not get was this. I know the answer is $y = \frac {47}2$ , $\frac 17$ according to WolframAlpha, but I have no idea how to reach that answer, my Algebra 2 Honors teacher couldn't figure it out. Use substitution or linear combination to solve the following system: $$\begin{cases}\dfrac 3{x-1} + \dfrac 4{y+2} = 2\\
\dfrac 6{x-1} - \dfrac 7{y+2} = -3 \end{cases}$$","['algebra-precalculus', 'systems-of-equations']"
174806,Sum of truncated normals,"Suppose $X_1, \dots, X_n$ are truncated standard normal variables, truncated so that $X_i \geq 0$ (that is, $X_i$ is drawn as a standard normal, conditional on $X_i \geq 0$) Let $c_1, \dots, c_n$ be non-negative coefficients. What does the distribution of $\sum_i c_i Y_i$ look like? Does it have, or approximately have, a standard distribution, such as a truncated normal distribution? Original question : Suppose $X_1, \dots, X_n$ are iid Normal random variables, with mean 0 and variances $\sigma_1, \dots, \sigma_n$. Let $Y_i = \max(0,X_i)$. (So $Y_i$ is a truncated normal random variable). What does the distribution of $\sum_i Y_i$ look like? Does it have, or approximately have, a standard distribution?","['statistics', 'probability']"
174807,Closure of image of diagonal morphism of S-scheme,"Let $X$ be an $S$-scheme with structural morphism given by $f : X \to S$.  The image of the diagonal morphism $\Delta : X \to X \times_S X$ is contained in the subset $Z := \{ z \in X \times_S X : p(z) = q(z) \} \subset X \times_S X$ where $p, q$ are the projection maps. Is $Z$ closed in general?  Is it furthermore the closure of $\Delta(X)$?","['algebraic-geometry', 'schemes']"
174810,Does this qualify as a proof of $\sum _{k=0}^{l}\binom{n}{k}\binom{m}{l-k} = \binom{n+m}{l}$? (Spivak's 'Calculus'),"I'm working through Spivak's 'Calculus' at the moment, and a question about series confused me a bit. I think I have the solution, but I'm not sure if my ""proof"" holds. The question is: Prove that $\sum\limits_{k=0}^{l}\binom{n}{k}\binom{m}{l-k} = \binom{n+m}{l}$ Hint: Apply the binomial theorem to $(1+x)^{n}(1+x)^{m}$ So I expanded $(1+x)^{n}(1+x)^{m}$, then expanded that again, and compared that to $(1+x)^{n+m}$, to get the equality: $$\begin{align*}
&\binom{n}{0}\binom{m}{0} + \left [ \binom{n}{0}\binom{m}{1} + \binom{n}{1}\binom{m}{0} \right ]x \\
&+ \left [ \binom{n}{0}\binom{m}{2} + \binom{n}{1}\binom{m}{1} + \binom{n}{2}\binom{m}{0} \right ]x^{2} + \cdots\\
&\qquad = \binom{n+m}{0} + \binom{n+m}{1}x + \binom{n+m}{2}x^{2} + \cdots
\end{align*}$$ Now, my question is, is the original statement proven, just because the terms match up on both sides of the equality, or is this insufficient? Thanks.","['summation', 'algebra-precalculus', 'binomial-coefficients', 'solution-verification']"
174812,A trigonometric identity,"If one sees the simplification done in equation $5.3$ (bottom of page 29) of this paper it seems that a trigonometric identity has been invoked of the kind, $$\ln(2) + \sum _ {n=1} ^{\infty} \frac{\cos(n\theta)}{n} = - \ln \left\vert \sin\left(\frac{\theta}{2}\right)\right\vert $$ Is the above true and if yes then can someone help me prove it?","['mathematical-physics', 'trigonometry', 'fourier-series']"
174816,Discontinuous functions that are continuous on every line in ${\bf R}^2$,"This is exercise 7 from chapter 4 of Walter Rudin Principles of Mathematical Analysis , 3rd edition. (Page 99) Define $f$ and $g$ on ${\bf R}^2$ by: $$f(x,y) = \cases {0,&if $(x,y)=(0,0)$\\
xy^2/(x^2+y^4) &otherwise}$$ $$g(x,y) = \cases {0,&if $(x,y)=(0,0)$\\
xy^2/(x^2+y^6) &otherwise}$$ Prove that: $f$ is bounded on ${\bf R}^2$ $g$ is unbounded in every neighborhood of $(0,0)$ $f$ is not continuous at $(0,0)$ Nevertheless, the restrictions of both $f$ and $g$ to every straight line in ${\bf R}^2$ are continuous! This is one of the few specific problems I remember from my university career, which ended some time ago. I remember it because I toiled over it for so long and when I finally found the answer it seemed so simple.","['continuity', 'real-analysis']"
174821,Selfadjoint compact operator with finite trace,"I have a compact selfadjoint operator $T$ on a separable Hilbert space. For some fixed orthonormal basis, the operator's diagonal is in $\ell^1(\mathbb{N})$. Can we conclude that $T$ is trace class?","['operator-theory', 'functional-analysis']"
174828,A (probably trivial) induction problem: $\sum_2^nk^{-2}\lt1$,"So I'm a bit stuck on the following problem I'm attempting to solve. Essentially, I'm required to prove that $\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2} < 1$ for all $n$. I've been toiling with some algebraic gymnastics for a while now, but I can't seem to get the proof right. Proving it using calculus isn't a problem, but I'm struggling hither.","['inequality', 'induction', 'summation', 'discrete-mathematics']"
174832,Zeros of a complex polynomial,"The question is: Show that $$ P(z) = z^4 + 2z^3 + 3z^2 + z +2$$ has exactly one root in each quadrant of the complex plane. My initial thought was to use Rouche's Theorem (since that's generally what I use to find how many roots a complex polynomial has), but the more I think about it, the more I'm not sure how to make it work. Here is my attempt: First, pick a radius for a circle that can encompass all four roots of the polynomial. For simplicity sake (in my opinion), I went with |z| = 5. Setting $$f(z) = z^4$$ and $$g(z) = 2z^3 + 3z^2 + z + 2$$ I get |f(z)| = 625 and |g(z)| = 332, so by Rouche's Theorem we have four roots in the disc. Now, my thought was that I could somehow seperate the quadrants by breaking up my circle into quarter-circles (like four slices of pie) and applying Rouche's Theorem again on each of these new domains. However, finding the place on the boundary where the value hits its max for some f(z) or g(z) would be messy (at best), since if I juse use |z| = 5, I'm right back where I started. There's also the issue that these zeroes may occur on the boundary, so in the real/imaginary axis, which wouldn't be what I'm trying to show. So now, I'm just stuck, so if anyone can see how to tackle this, it would be greatly appreciated.","['complex-analysis', 'polynomials']"
174854,Pointwise order of the Cartesian product of two preordered chains,"Definitions: ( From Categories for Types by Roy L. Crole. ) A preorder on a set $X$ is a binary relation $\leq$ on $X$ which is reflexive and transitive. A preordered set $(X, \leq)$ is a set equipped with a preorder.... Where confusion cannot result, we refer to the preordered set $X$ or sometimes just the preorder $X$. If $x \leq y$ and $y \leq x$ then we shall write $x \cong y$ and say that $x$ and $y$ are isomorphic elements. Given two preordered sets $A$ and $B$, the point-wise order on the Cartesian product $A \times B$ is defined as $(a,b) \le (a',b')$ if and only if $a \le a'$ and $b \le b'$). The result is a preorder. A subset $C$ of a preorder $X$ is called a chain if for every $x,y \in C$ we have $x \leq y$ or $y \leq x$.... We shall say that a preorder $X$ is a chain ... if the underlying set $X$ is such. ( p.8 ) Exercise: Let $C$ and $C'$ be chains. Show that the set of pairs $(c, c')$, where $c \in C$ and $c' \in C'$, with the pointwise order is also a chain just in case at most one of $C$ or $C'$ has more than one element. ( p.9 ) Proposed Solution: Suppose $C$ is a preorder with more than one element such that for every $a, b \in C, a \cong b$. Then by the definition given above, $C$ is a chain. Now suppose that $C'$ is a chain (without any additional properties). I claim that $C \times C'$ is a chain. Proof Let $(c_1, c'_1), (c_2, c'_2) \in C \times C'$. Then $c_1 \cong c_2$ and ($c'_1 \le c'_2$ or $c'_2 \le c'_1$). So $c_1 \le c_2$ and $c_2 \le c_1$. If $c'_1 \le c'_2$, then $(c_1, c'_1) \le (c_2, c'_2)$. If $c'_2 \le c'_1$, then $(c_2, c'_2) \le (c_1, c'_1)$. Question: This seems to be a counterexample to the statement I am asked to prove. Is the question in error or am I missing something here?","['relations', 'elementary-set-theory', 'order-theory']"
174856,Triangulate rectangular parallelepiped in $\mathbb{R}^{n}$,I need to triangulate the n-dimensioned rectangular parallelepiped in $\mathbb{R}^{n}$ into a set of $n$-simplices. Could you suggest me any known algorithm for that or maybe an extension of Delaunay triangulation for $n > 2$ cases?,"['geometry', 'simplicial-stuff', 'triangulation']"
174871,Does multiplying by $dt$ have any meaning?,"Consider, for example, the equation $x'=x$, then it is usually solved
by writing $\frac{dx}{dt}=x\implies\frac{dx}{x}=dt\implies\int\frac{dx}{x}=\int dt$
... I know that there is a theorem in ODE that justify $x'=x\implies\int\frac{dx}{x}=\int dt$
,but my question is about the intermediate step: $x'$ at some point
$x_{0}$ is defined via a limit. $\frac{dx}{dt}$ is, as far as I
understand, a notation for the function $x'$ - so we can not multiply
by $dt$ since it has no meaning, it is a part of the notation. My question is as follows: Although the last step is indeed correct
and can be justified, does the intermediate step (multiplying by $dt$)
have any meaning, or is it just an easy way to remember and get to
the last step ?","['ordinary-differential-equations', 'calculus']"
174875,How to find all polynomials $P(x)$ with real coefficents satisfying $P^2(x)-1=4P(x^2-4x+1)$,"Find all polynomials $P(x)$ with real coefficents satisfying
  $P^2(x)-1=4P(x^2-4x+1)$. My solution: Let the first term of $P(x)$ be $ax^n$. We see first term of left side is easily $a^2x^{2n}$ and right side's is
$4ax^{2n}$ . As we see there's absolutely no solution. If $n=0$ then easily, $a^2-4a-1=0$ and then we can find only 2 values for $P(x)=a$. So the solution is $2$.","['algebra-precalculus', 'functional-equations', 'polynomials']"
174877,Evaluation of $\sum_{x=0}^\infty e^{-x^2}$,"Most of us are aware of the classic Gaussian Integral $$\int_0^\infty e^{-x^2}\, dx=\frac{\sqrt{\pi}}{2}$$ I would be interested in evaluating the similar sum $$\sum_{x=0}^\infty e^{-x^2}$$ Now, because $\exp(-\lfloor x \rfloor^2) \ge \exp(-x)$, we find $$\sum_{x=0}^\infty e^{-x^2}= \int_0^\infty e^{-\lfloor x \rfloor^2}\, dx \ge \int_0^\infty e^{-x^2}\, dx=\frac{\sqrt{\pi}}{2}$$ Does a closed form for this sum exist?  If so, what would it be?  I would be very interested in how a closed form would be found for this function.","['special-functions', 'closed-form', 'sequences-and-series', 'integration']"
174911,Does $z ^k+ z^{-k}$ belong to $\Bbb Z[z + z^{-1}]$?,"Let $z$ be a non-zero element of $\mathbb{C}$.
Does $z^k + z^{-k}$ belong to $\mathbb{Z}[z + z^{-1}]$ for every positive integer $k$? Motivation: I came up with this problem from the following question. Maximal real subfield of $\mathbb{Q}(\zeta )$","['ring-theory', 'abstract-algebra']"
174945,"Why are $\sin$ and $\cos$ (and perhaps $\tan$) ""more important"" than their reciprocals?","(My personal ""feel"" is that $\sin$ and $\cos$ are first-class citizens, $\tan$ is ""1.5th-class,"" and the rest are second-class; I'm sure there are others who feel the same.) Main question(s): From a purely high-school-geometric/""ninth-century-geometer's"" standpoint, is there any reason why this should be so? Given the usual elementary knowledge of triangles when one is first introduced to these functions, I think it appears pretty arbitrary. How should I convince a high school student that $\sin$, $\cos$, and $\tan$, instead of their reciprocals, should be our main objects of study? How did history decide on their superiority? Of course, with real analysis goggles, things look quite a bit different: $\sin$ and $\cos$ are the only ones that are continuous everywhere; $f'' = -f$ characterizes all their linear combinations; they have much nicer series representations; etc. But I suspect this is all hindsight. (I don't pretend to know enough about complex analysis, but I suspect even more nice things happen there with $\sin$ and $\cos$, and even more ugly things happen with the other four. In any case, I doubt history chose $\sin$ and $\cos$ to be first-class citizens because of their complex properties.) Secondary questions: Is there any reason why $\sin$ is the ""main"" function and $\cos$ is ""only"" its complement, or is this arbitrary as well? Is there any reason why $\tan$ is preferred to $\cot$?","['trigonometry', 'terminology']"
174948,Proportional to 2 Separate Variables vs. Proportional to Product of 2 Variables,"I've commonly seen the following in physics and math textbooks, but never understood how it is mathematically deduced: $A \propto B$ $\space$ and $\space$ $A \propto C \space\space\space \implies \space\space\space A \propto BC.$ Could someone walk me through how this is done? This has been bothering me for a while now. Update : Here 's something I found that explains how this works. (Page 387; ""Proof"" section). Still, this proof takes the two statements one after the other . The author uses $x \propto y$ when $z$ is constant, and then takes care of $x \propto z$ when $y$ is constant, where it left off from the first (going from $x$ to $x'$ and then $x_1$ ). Is this the only way it can be done?",['algebra-precalculus']
174950,Poincaré Inequality - Product Of Measures,"I'm given two euclidean spaces $ \mathbb{R}_1 , \mathbb{R}_2 $ , with probability measures on them , that satisfy the Poincaré's inequality:
$ \lambda^2 \int_{\mathbb{R}^k} |f - \int_{\mathbb{R}^k} f d\mu | ^2 d\mu \leq
\int_{\mathbb{R}^k} | \nabla f | ^2 d\mu $ for some constants $C_1 , C_2 $ respectively. How can I prove that the space $ (\mathbb{R}_1 \times \mathbb{R}_2 , \mu_1 \otimes \mu_2 )$ also satisfies the Poincaré inequality? Thanks in advance ! My attempt:
denote: $ g(x) = \int_{\mathbb{R}_2 } f(x,y) d\mu_2  $ . We can then apply the inequality in order to get that:
$$ C_1 ^2 \cdot \int_{\mathbb{R}_1 } \left| \int_{\mathbb{R}_2} f(x,y) d\mu_2 - \int_{\mathbb{R}_1} \int_{\mathbb{R}_2} f(x,y) d\mu_2 d\mu_1 \right| ^2 d\mu_1  \leq \int_{\mathbb{R_1}} \left| \nabla \int_{\mathbb{R}_2} f(x,y) d\mu_2 \right|^2 d\mu_1 $$ and we can even say that the RHS is 
$ \leq \int_{\mathbb{R}_1 }  \int_{\mathbb{R}_2 } | \nabla f | ^2 d\mu_1 d\mu_2$ where our function is nice enough...But how can I finish the proof?","['sobolev-spaces', 'measure-theory', 'inequality']"
174973,When is the limit in $y$ of a Taylor expansion in $x$ a valid expansion?,"I'd be interested to know when, if
$$f(x,y)=g(x,y)+O(x^n)$$
we have that
$$\lim_{y\rightarrow c}=\lim_{y\rightarrow c}g(x,y)+O(x^n).$$
Are there conditions of $f$ and/or $g$ that make sure that this is satisfied? In particular, assume that we have a function $f(x,y)$ and that we Taylor expand it as a function of $x$ around $a$:
$$f(a+x,y)=f(a,y)+\frac{\partial f}{\partial x}(a,y)\cdot x+\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$ I wish to study the behaviour of $f(x,y)$ as $y$ tends to a value on the boundary of the domain of $f(x,y)$, say $y\rightarrow c$. I therefore wonder under what conditions on $f$
$$\lim_{y\rightarrow c}f(a+x,y)=\lim_{y\rightarrow c}f(a,y)+\lim_{y\rightarrow c}\frac{\partial f}{\partial x}(a,y)\cdot x+\lim_{y\rightarrow c}\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$
In other words, when is the $O(x^n)$ term still $O(x^n)$ as $y\rightarrow c$? I've been hoping to find some smoothness constraints on $f$ that would guarantee that the expansion still is $O(x^n)$, but so far I haven't been able to find any.","['taylor-expansion', 'multivariable-calculus', 'real-analysis', 'analysis']"
174974,complex torus has topological genus one,"could any one give me a hint how to show a complex torus has topological genus one by constructing an explicit homeomorphism to $S^1\times S^1$?
Complex Torus: $\mathbb{C}/L$, where $L=\{\mathbb{Z}\omega_1+\mathbb{Z}\omega_2\}$.
Thank you.","['general-topology', 'riemann-surfaces']"
174991,Compute $ \sum\limits_{m=1}^{\infty} \sum\limits_{n=1}^{\infty} \sum\limits_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p}$,"How would you compute this sum? It's not a problem I need to immediately solve, but a problem that came to my mind today. I think that the generalization to more than three nested sums would be interesting as well. $$ \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \sum_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p}$$","['sequences-and-series', 'calculus', 'real-analysis']"
174997,Cancellation law for Minkowski sums,"Let $(X,\|\cdot\|)$ be a Banach space and $A,B,C\subset X$ closed bounded non-empty convex subsets. Let $+$ denote the Minkowski symbol for addition . Does the $+$ satisfy: $$A+C\subset B+C\implies A\subset B$$","['convex-analysis', 'functional-analysis', 'real-analysis', 'banach-spaces']"
175010,When is the integral of a periodic function periodic?,"I'm attempting some questions from Zwiebach - A First Course in String Theory, and have got stuck. I've proved that a function $h'(u)$ is periodic. The question then asks me to show that $h(u)=au+f(u)$ where $a$ is a constant and $f(u)$ a periodic function. I can't see how to do this directly from the periodicity of $h'$. Is this possible, or true? Many thanks!","['functions', 'periodic-functions']"
175042,Opposite Clifford-Algebra,"for a symmetric bilinearform $\beta$ on a $\mathbb{K}$-vectorspace $V$ the associated Clifford Algebra $Cl(\beta)$ is the associative algebra with unit subject to the relations $$v\cdot v=\beta(v,v)\cdot 1\qquad\forall v\in V.$$It is then often said that $Cl(-\beta)$ is isomorphic to the opposite Algebra $Cl(\beta)^\text{op}$. Why is that? Cheers, Robert","['clifford-algebras', 'abstract-algebra']"
175048,A compact operator in $L^2(\mathbb R)$,"Let $g \in L^{\infty}(\mathbb R)$. Consider the operator 
  $$
\begin{split}
T_g\colon & L^2(\mathbb R)\to L^2(\mathbb R) \\
& f \mapsto gf
\end{split}
$$
  Prove that $T_g$ is compact (i.e., the image under $T_g$ of bounded closed sets is compact) if and only if $g=0$ a.e. I do not know how to start and I'm very puzzled. I know very little about compactness in $L^p$: of course they are complete metric spaces, therefore a subspace is compact if and only if it is closed (complete) and totally bounded. A singleton is of course totally bounded and I think it is closed: therefore I can say that if $g=0$ a.e. then the image of every subspace is $\{0\}$ which is compact, so the operator is compact. What about the inverse direction? It seems hard to prove. 
Would you help me, please? Thanks.","['operator-theory', 'compact-operators', 'functional-analysis', 'real-analysis']"
175050,Simple simple Euler Lagrange Equation,"Just starting a course on Lagrangian Mechanics and I'm just wondering what about the Euler-Lagrange equation, and more specifically what I'm meant to be trying to do .. One of the questions from my textbook reads : Solve the Euler-Lagrange equation for the following function
\begin{align*}
f(y,y') = y^2+y'^2
\end{align*}
Looks simple enough... But where should I be headed to start? The Euler-Lagrange Equation as we have it is
\begin{align*}
\frac{\partial f}{\partial y} = \frac{d}{dx} \frac{\partial f}{\partial y'}
\end{align*} Do I just find the partial derivatives of f treating y and y' as independent variables as follows: $$\frac{\partial f}{\partial y} = 2y,$$
$$\frac{\partial f}{\partial y'} = 2y$$
so then we have :
$$2y = \frac{d}{dx}(2y')$$
$$y = \frac{d}{dx}(y')$$
$$yx+c = y'$$
$$\frac{yx^2}{2}+cx+d = y$$ And then simplify with the y isolated on the right so it looks nice .. or at least thats what I thought.. Any comments ? right track, wrong track? 
Also, what is this euler equation actually solving for? Is it just a nice way of solving differential equations in this form ? When we did this in class, we did a monster proof showing it minimizes the path between 2 points or finds the extrema for the function $I(x) = \int^a_b \ F(y(x),y'(x),x) dx,$ which is all great but I'm not too sure how this relates to the equation we solve in the question .. Thanks for all","['calculus-of-variations', 'ordinary-differential-equations', 'physics']"
175078,Is there a known closed form number for $\prod\limits_{k=2}^{ \infty } \sqrt[k^2]{k}$,"$f(x)=\sum\limits_{k =  2 }^ \infty e^{-kx} \ln(k)  $ $\int\limits_0^{\infty}\int\limits_x^{\infty}\, f(\gamma)\, d\gamma dx=\sum\limits_{k =  2 }^ \infty \frac{1}{k^2}  \ln(k)  $ $\int\limits_0^{\infty}\int\limits_x^{\infty} f(\gamma)\, d\gamma dx=\sum\limits_{k=2}^ \infty\ln(k^{\frac{1}{k^2}})=\ln(\prod\limits_{k=2}^{\infty}k^{\frac{1}{k^2}})  $ $\prod\limits_{k=2}^{ \infty }k^{\frac{1}{k^2}}=\prod\limits_{k=2}^{ \infty } \sqrt[k^2]{k}=e^{\int\limits_0^{\infty}\int\limits_x^{\infty}  f(\gamma) \,d\gamma dx}$ $f(x)=\sum\limits_{k =  2 }^ \infty e^{-kx} \ln(k)  $ $f(x)=\sum\limits_{k =  1 }^ \infty e^{-(k+1)x} \ln(k+1)  $ $f(x)=e^{-x}\sum\limits_{k =  1 }^ \infty e^{-kx} \ln(k+1)  $ $f(x)=e^{-x}\sum\limits_{n =  1 }^ \infty \frac{(-1)^{n+1}}{n} \sum\limits_{k =  1 }^ \infty k^n e^{-kx}$ We know that $\sum\limits_{k =  1 }^ \infty e^{-kx}= \frac{1}{e^{x}-1} $ $\sum\limits_{k =  1 }^ \infty k^n e^{-kx}= (-1)^n\frac{d^n}{dx^n}(\frac{1}{e^{x}-1}) $ $f(x)=e^{-x}\sum\limits_{n =  1 }^ \infty \frac{(-1)^{n+1}}{n} \sum\limits_{k =  1 }^ \infty k^n e^{-kx} =  e^{-x}\sum\limits_{n =  1 }^ \infty \frac{(-1)^{n+1}}{n} (-1)^n\frac{d^n}{dx^n}(\frac{1}{e^{x}-1})$ $f(x)=-e^{-x}\sum\limits_{n =  1 }^ \infty \frac{1}{n} \frac{d^n}{dx^n}(\frac{1}{e^x-1})$ $\int\limits_0^{\infty}\int\limits_x^{\infty}  f(\gamma) \,d\gamma dx= -\int\limits_0^{\infty}\int\limits_x^{\infty} e^{-\gamma}\sum\limits_{n =  1 }^ \infty \frac{1}{n} \frac{d^n}{d\gamma^n}(\frac{1}{e^{\gamma}-1})\, d\gamma dx$ I have lost my way after that. Is it possible to find a closed form in my way? or I need to follow a different way.
I need your mathematical sense. 
Thanks a lot for answers and advice.","['sequences-and-series', 'products']"
175092,Median from probability generating function,"How can I find a median, or an approximation of a median, from the probability generating function?",['probability']
175131,Irreducibility of $x^5 -x -1$ by reduction mod 5,"Is there a quick way of deducing that $x^5-x-1 \in \mathbb{Z}[x]$ is irreducible by reducing it mod 5, other than verifying that it has no roots in $\mathbb{Z}_5$ and no factorization as the product of a factor of order 2 and a factor of order 3?","['abstract-algebra', 'polynomials']"
175143,Prove $ \sin(A+B)\sin(A-B)=\sin^2A-\sin^2B $,"How would I verify the following double angle identity.
$$
\sin(A+B)\sin(A-B)=\sin^2A-\sin^2B
$$
So far I have done this.
$$
(\sin A\cos B+\cos A\sin B)(\sin A\cos B-\cos A\sin B) 
$$But I am not sure how to proceed.",['trigonometry']
175148,Prove $\frac{\sin(A+B)}{\cos(A-B)}=\frac{\tan A+\tan B}{1+\tan A\tan B}$,"How would I solve the following double angle identity. 
$$
\frac{\sin(A+B)}{\cos(A-B)}=\frac{\tan A+\tan B}{1+\tan A\tan B}
$$
So far my work has been.
$$
\frac{\sin A\cos B+\cos A\sin B}{\cos A\cos B+\sin A\sin B}
$$
But what would I do to continue.",['trigonometry']
175157,Can a meromorphic function be written as ratio of holomorphic function?,"Well, I want to know whether a meromorphic function can be written as ratio of two holomorphic function on $\mathbb{C}$ or on a Riemann surface .
Thank you for help.","['riemann-surfaces', 'complex-analysis']"
175173,$\lim_{n\to\infty}\frac{a_{n-1}}{a_{n}}$,"If $\{a_{n}\}_{n\geq 1}$ is a decreasing  sequence of real numbers, $a_{n}\in (0,1)$ and $\lim_{n\to \infty} a_{n}=0$. What we can say about $$\lim_{n\to\infty}\frac{a_{n-1}}{a_{n}}$$","['sequences-and-series', 'real-analysis', 'limits']"
175180,Expected value and life,"Let $e_{x} = \int_{0}^{\infty} p_{x}(t) \ dt$ where $p_{x}(t)$ is the probability that a person aged $x$ will survive at least $t$ more years. Why is $e_{x} \leq e_{x+1}+1$? We know that $e_{x} \geq e_{x+1}$ since the expected future lifetime of a younger person is greater than that of an older person. So maybe we can show that $e_{x}-e_{x+1} \leq 1$ or $$\int_{0}^{\infty} p_{x}(t) - p_{x+1}(t) \ dt \leq 1$$ Edit. We know that for a discrete random variable, $$\mathbb{E}(Y|X) = \int_{\Omega} Y(\omega) \mathbb{P}(dw|X=x)$$ $$= \frac{\int_{X=x} Y(\omega)\mathbb{P}(dw)}{\mathbb{P}(X=x)}$$ $$= \frac{\mathbb{E}(Y \mathbb{1}_{(X=x)})}{\mathbb{P}(X=x)}$$ What does $w$ above represent?","['probability', 'actuarial-science']"
175191,Normal to Ellipse and Angle at Major Axis,"I've tried to detail my question using the image shown in this post. Consider an ellipse with 5 parameters $(x_C, y_C, a, b, \psi)$ where $(x_C, y_C)$ is the center of the ellipse, $a$ and $b$ are the semi-major and semi-minor radii and $\psi$ is the orientation of the ellipse. Now consider a point $(x,y)$ on the circumference of the ellipse. The normal at this point on the circumference of the ellipse intersects the major axis at a point $(x_D, y_D)$. This normal makes an angle $/phi$ with the major axis. However, the angle subtended by this point at the center of the ellipse is $\theta$. For a circle, $\theta = \phi$ for all points on its circumference because the normal at the circle is the radial angle subtended by the point on the circumference. Is there a relationship between the angles $\theta$ and $\phi$ for an ellipse. For some context, I am trying to ""extract"" points from the circumference of an ellipse given its parameters $(x_C, y_C, a, b, \psi)$. For such an ellipse, I start from $(x_C, y_C$) and with angle $\theta = 0^\circ$ and I start sweeping until $360^\circ$. Using the equation $\left[\begin{array}{c} x \\ y\end{array}\right] = \left[\begin{array}{c c} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{array}\right] \left[\begin{array}{c} a\cos(\psi) \\ b\sin(\psi) \end{array}\right]$, I get the $(x,y)$ location of the point that is supposed to be on the ellipse circumference. I then look up this location in a list of ""edge"" points. Along with this list of edge points, I also have gradient angle information for each edge point. This corresponds to the angle $\phi$. Here is the crux of the question, for a circle, I am confident that the edge point lies on the circumference of the circle if $|\theta - \phi| < \text{threshold}$. But, for an ellipse, how do I get a similar relationship ?","['geometry', 'conic-sections']"
175192,one to one mapping between the floor function and the Riemann prime counting function,"We have the following 'transform' of a real valued, piecewise continuous function $f(x)$ : $$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$
$$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$
$$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ and we wish to recover $f(x)$ from $T[f(x)]$. What kind of mathematics should be used to study this problem? EDIT: $$J(x)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\ln\zeta(s)\frac{x^{s}}{s}ds$$ where $J(x)$ is the Riemann prime counting function . 
 The above relation is the well known Perron's formula for Dirichlet series. This induced me to express $\zeta(s)$ as an exponential series expansion in terms of $\ln\zeta(s)$: $$\zeta(s)=\sum_{n=0}^{\infty}\frac{(\ln\zeta(s))^{n}}{n!}$$ Applying Perron's formula : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\alpha-i\infty}^{\alpha+i\infty}\zeta(s)\frac{x^{s}}{s}ds=\frac{1}{2\pi i}\sum_{n=0}^{\infty}\frac{1}{n!}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds$$ The first two terms corresponding to $n=0,n=1$ are trivial. The other terms starting at $n=2$ could be done using Mellin convolution. Namely, if two functions, say $F(s)$ and $G(s)$ are given by: $$F(s)=\int_{0}^{\infty}f(x)x^{-s-1}dx$$ 
$$G(s)=\int_{0}^{\infty}g(x)x^{-s-1}dx$$ 
  then the following holds : $$F(s)G(s)=\int_{0}^{\infty}f(x)\bigstar g(x) x^{-s-1}dx$$
 Where the star stands for Mellin convolution, and is defined by :
 $$f(x)\bigstar g(x)=\int_{0}^{\infty}f\left( \frac{x}{u}\right)g(u)\frac{du}{u}$$ Another property of the Mellin transform we will need is that, if a function, say $h(x)$ is a Mellin inverse of of $H(s)$, then :
 $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}sH(s)x^{s}ds=x\frac{d}{dx}h(x)$$ Using these facts about the Mellin transform, we can evaluate the integrals : $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds=J(x)\bigstar \left( x\frac{d}{dx}J(x)\right)^{\bigstar n-1}$$ Where the star and the power $n-1$ mean repeated convolution for $n-1$ times . Furthermore, the Mellin convolution has the property : $$\left(x\frac{d}{dx}\right)^{n}(f(x)\bigstar g(x))=f(x)\bigstar \left(x\frac{d}{dx}\right)^{n}g(x)=g(x) \bigstar\left(x\frac{d}{dx}\right)^{n}f(x) $$
Therefore : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\zeta(s)\frac{x^{s}}{s}ds$$ Therefore : $$\left \lfloor x \right \rfloor=1+  \sum_{n=0}^{\infty}\frac{J(x)^{\bigstar n}}{(n+1)!} \bigstar \left(x\frac{d}{dx}\right)^{n}J(x)$$ This relation could be given more explicitly by: $$\left \lfloor x \right \rfloor=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}J\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}J(u_{i}) \right )\Theta^{n-1}_{u_{n}}J(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$
where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$
$$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$
$$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ We define the following operator acting on a distribution $f(x)$:
$$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ if $f(x)$ is given by : $$f(x)=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\ln g(s)\frac{x^{s}}{s}ds$$ then, the following holds: $$T[f(x)]=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty} g(s)\frac{x^{s}}{s}ds$$ The plan here is to investigate the geometric and algebraic properties of the mapping above. One thing that comes in mind is trying to find an inverse of the operator $T[f(x)]$, such that : 
$$T^{-1}T[f(x)]=f(x)$$ assuming such an operator exists, and applying the operator to $\left \lfloor x \right \rfloor$: $$T^{-1}[\left \lfloor x \right \rfloor]=J(x)$$
 and the prime counting function $\pi(x)$ could be given by: $$\pi(x)=\sum_{n=1}^{\infty}\frac{\mu(n)}{n}T^{-1}\left[\left \lfloor x^{1/n} \right \rfloor\right]$$","['analytic-number-theory', 'real-analysis']"
175208,What's the analogue of Sierpinski triangle to disk?,What's the (closest) analogue of Sierpinski triangle to disk?,"['geometry', 'fractals']"
175220,optimality of 2 as a coefficient in a continued fraction theorem,"I'm giving some lectures on continued fractions to high school and college students, and I discussed the standard theorem that, for a real number $\alpha$ and integers $p$ and $q$ with $q \not= 0$, if $|\alpha-p/q| < 1/(2q^2)$ then $p/q$ is a convergent in the continued fraction expansion of $\alpha$. Someone in the audience asked if 2 is optimal: is there a positive number $c < 2$ such that, for every $\alpha$ (well, of course the case of real interest is irrational $\alpha$), when $|\alpha - p/q| < 1/(cq^2)$ it is guaranteed that $p/q$ is a convergent to the continued fraction expansion of $\alpha$? Please note this is not answered by the theorem of Hurwitz, which says that an irrational $\alpha$ has $|\alpha - p_k/q_k| < 1/(\sqrt{5}q_k^2)$ for infinitely many convergents $p_k/q_k$, and that $\sqrt{5}$ is optimal: all $\alpha$ whose cont. frac. expansion ends with an infinite string of repeating 1's fail to satisfy such a property if $\sqrt{5}$ is replaced by any larger number. For the question the student at my lecture is asking, an optimal parameter is at most 2, not at least 2.","['continued-fractions', 'number-theory']"
175223,Simple dice game: Optimal strategy?,"Here's the description of a dice game which puzzles me since quite some time (the game comes from a book which offered a quite unsatisfactory solution — but then, its focus was on programming, so this is probably excusable). The game goes as follows: Two players play against each other, starting with score 0 each. Winner is the player to first reach a score of 100 or more. The players play in turn. The score added in each round is determined as follows: The player throws a die. If the die does not show an 1, he has the option to stop and have the points added to his score, or to continue throwing until either he stops or gets an 1. As soon as he gets an 1, his turn ends and no points are added to his score, any points he has accumulated in this round are lost. Afterward it is the second player's turn. The question is now what is the best strategy for that game. The book suggested to try out which of the following two strategies gives better result: Throw 5 times (if possible), then stop. If the accumulated points in this round add up to 20 or more, stop, otherwise continue. The rationale is that you want the next throw to increase the expected score. Of course it doesn't need testing to see that the second strategy is better: If you've accumulated e.g. 10 points, it doesn't matter whether you accumulated them with 5 times throwing a 2, or with 2 times throwing a 5. However it is also easy to see that this second strategy isn't the best one either: After all, the ultimate goal is not to maximize the increase per round, but to maximize the probability to win.; both are related, but not the same. For example, imagine you have been very unlucky and are still at a very low score, but your opponent has already 99 points. It's your turn, and you've already accumulated some points (but those points don't get you above 100) and have to decide whether to stop, or to continue. If you stop, you secure the points, but your opponent has a 5/6 chance to win in the next move. Let's say that if you stop, the optimal strategy in the next move will be to try to get 100 points in one run, and that the probability to reach that is $p$. Then if you stop, since your opponent then has his chance to win first, your total probability to win is just $1/6(p + (1-p)/6 (p + (1-p)/6 (p + ...))) = p/(p+5)$. On the other hand, if you continue to 100 points right now, you have the chance $p$ to win this round before the other has a chance to try, but a lower probability $p'$ to win in later rounds, giving a probability $p + p'/(5+p')$. It is obvious that even if we had $p'=0$ (i.e. if you don't succeed now, you'll lose), you'd still have the probability $p>p/(p+5)$ to win by continuing, so you should continue no matter how slim your chances, and even if your accumulated points this round are above 20, because if you stop, you chances will be worse for sure. Since at some time, the optimal strategy will have a step where you try to go beyond 100 (because that's where you win), by induction you can say that if your opponent has already 99 points, your best strategy is, unconditionally, to try to get 100 points in one run. Of course this ""brute force rule"" is for that specific situation (it also applies if the opponent has 98 points, for obvious reasons). If you'd play that brute-force rule from the beginning, you'd lose even against someone who just throws once each round. Indeed, if both are about equal, and far enough from the final 100 points, intuitively I think the 20 points rule is quite good. Also, intuitively I think if you are far advanced against your opponent, you should even play more safe and stop earlier. As the current game situation is described by the three numbers your score ($Y$), your opponent's score ($O$) and the points already collected in this round ($P$), and your decision is to either continue ($C$) or to stop ($S$), a strategy is completely given by a function
$$s:\{(Y, O, P)\}\to \{C,S\}$$
where the following rules are obvious: If $Y+S\ge 100$ then $s(Y,O,P)=S$ (if you already have collected 100 points, the only reasonable move is to stop). $s(Y, O, 0)=C$ (it doesn't make sense to stop before you threw at least once). Also, I just above derived the following rule: $f(Y,98,P)=f(Y,99,P)=C$ unless the first rule kicks in. I believe the following rule should also hold (but have no idea how to prove it): If $f(Y,98,P)=S$ then also $f(Y,98,P+1)=S$ If that believe is true, then the description of a strategy can be simplified to a function $g(Y,O)$ which gives the smallest $P$ at which you should stop. However, that's all I've figured out. What I'd really like to know is: What is the optimal strategy for this game?","['game-theory', 'probability']"
175225,second order ODE via variation of parameters [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I don't understand this step on this article .
$$A'(x)u_1(x) + B'(x)u_2(x) = 0$$
why we desire A=A(x) and B=B(x) to be of this form? What is the basis that this form is valid?",['ordinary-differential-equations']
175228,Suppose A is an n-by-n matrix with its diagonal entries are n and other entries are one. Find determinant of A.,"For $n \geq 2$, find the determinant of
$A_{n}=\begin{bmatrix}
n &  1  & 1 &\ldots &1 \\
1 & n  & 1 &\ldots &1 \\
1 & 1  & n &\ldots &1 \\
\vdots & \vdots &\vdots & \ddots & \vdots\\
1 & 1 & 1 &\ldots  & n
\end{bmatrix}_{n \times n}
$ One can deduce the determinant of $A_{n}=(n-1)^{n-1}(2n-1)$ by taking $n=2,3,4...$. I solve it as follows but I found it is not elegant. Any better approach ? Here is my attempt: Let's find the eigenvalues of $A_{n}$ by solving $det(\lambda I-A_{n})=0$ because $det(A_{n})=\lambda_{1}\lambda_{2}...\lambda_{n}$ where $\lambda_{i}$ are eigenvalues of $A_{n}$. Express $A_{n}=(n-1)I+J$ where $I$ is the n-by-n identity matrix while $J$ is an n-by-n matrix with all entries equal to 1.
Denote $B=(n-1)I$ so that $$\lambda I-A_{n}=\lambda I-(B+J)=(\lambda I - B)(I-(\lambda I - B)^{-1}J)$$
Since $det(XY)=det(X)det(Y)$ for square matrices $X$ and $Y$
$$det(\lambda I-A_{n})=det(\lambda I - B)det(I-(\lambda I - B)^{-1}J)$$
Since $det(kX)=k^{n}det(X)$ for a scalar k and $det(I)=1$
$$det(\lambda I-B)=\det(\lambda I - (n-1)I)=(\lambda-(n-1))^{n}det(I)=(\lambda-(n-1))^{n}$$
If $u$ be a column vector of one's in $\mathbb{R}^{n}$, then $uu^{T}=J$ so that:
$$det(I-(\lambda I - B)^{-1}J)=det(I-(\lambda I - B)^{-1}uu^{T}) $$
By Sylvester's Determinant Theorem: 
\begin{equation*}
\begin{split}
det(I-(\lambda I - B)^{-1}J)&=&det(I-(\lambda I - B)^{-1}uu^{T})=det(I-u^{T}(\lambda I - B)^{-1}u)\\
&=&1-u^{T}(\lambda I - B)^{-1}u=1-u^{T}(\lambda-(n-1))^{-1}Iu\\
&=&1-(\lambda-(n-1))^{-1}u^{T}Iu\\
&=&1-\frac{u^{T}u}{\lambda-(n-1)}=1-\frac{n}{\lambda-(n-1)}\\
&=&\frac{\lambda-(2n-1)}{\lambda-(n-1)}
\end{split}
\end{equation*}
This yields $$det(\lambda I-A_{n})=(\lambda-(n-1))^{n}\frac{\lambda-(2n-1)}{\lambda-(n-1)}=(\lambda-(n-1))^{n-1}(\lambda -(2n-1))=0$$
The eigenvalues of $A_{n}$ are $n-1$ (with algebraic multiplicity of $n-1$) and $2n-1$  (with algebraic multiplicity of 1). Thus, $det(A_{n}=(n-1)^{n-1}(2n-1)$ I wonder if it is possible to obtain the determinant without solving for the eigenvalues. Or at least without using the Sylvester's determinant theorem....","['matrices', 'linear-algebra', 'determinant']"
175229,Possible Calibration Equations [Misc],"this is my first post so hopefully this topic is considered OK. Background: In class we were using a laser (mounted on a planar robot) to measure various profiles of a sample underneath. The system was already calibrated when I used it but I started wondering how this calibration would be performed (i.e. the math behind it). Here is what I came up with for a setup (simplified): Setup :
Assume the calibration gauge is a flat plate (blue, at angle $\alpha$ from x-axis and angle $\beta$ from y-axis, ideally it would be parallel to the x-y plane), then there will be: some initial height offset at the robot's origin ($\vec{Z_0}$, unknown), the laser's position attached to the planar robot ($\vec{P}$, known), the laser's measurement vector ($\vec{D_m}$, |$\vec{D_m}|$ known), the ""true"" height directly below the laser/robot ($\vec{D_t}$, unknown), and the error between the two ($\vec{E}$, unknown). Ideally, the laser is to be mounted such that the output beam is parallel to the z-axis but I'm sure there are some mounting angle errors (error angle $\phi$ from the z-axis in x-z plane and angle $\psi$ from the z-axis in the y-z plane). $i$ is the point where the laser makes contact with the plate (x,y,z). The ""known"" variables are |$\vec{D_m}$|, P_x, and P_y. The unknown variables I need to ""calibrate"" the system (so I can calculate $\vec{D_t}$) are $\vec{Z_0}$ and the angles $\alpha, \beta, \phi$, and $\psi$. My Attempt to Figure It Out :
First, I found where the laser would intersect the plate/plane based on the robot's (x,y) position:
$i = ( P_x + \Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, P_y + \Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ where $A$ is the slope of the plane along the x-direction, $B$ is the slope of the plane along the y-direction, $\Phi$ is the slope of $\vec{D_m}$'s x-component along the z-direction, and $\Psi$ is the slope of $\vec{D_m}$'s y-component along the z-direction. Using these instead of the angles made the equations cleaner. The measurement vector: $\vec{D_m} = (\Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi},\Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ The Calibration Equations (need 5) Take measurements along x-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_x}$. Take measurements along y-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_y}$. Move to origin ($P_x = P_y = 0$ to eliminates variables), take measurement set equal to |$\vec{D_m}$|. Have two points on the plane ($p_1, p_2$) with a known distance between them ($\Delta$). Move the robot so that the laser dot hits the first point, save ($P_{x1}, P_{y1}$), move the laser to the second point, save that position information. Use $P_{x1}, P_{y1}, P_{x2}, P_{y2}$ in the equation: $\Delta = |i(P_{x1}, P_{y1}) - i(P_{x2}, P_{y2})|$. ...? I'm stuck. Final Thoughts I thought this was pretty interesting and not being able to solve it has been bugging me. :) I have no idea how the system was actually calibrated but this is how I set it up. If there is a far easier way or if someone knows how these types of systems are actually calibrated please let me know (it would be an interesting read) but I would also like to figure out that fifth equation using my way. I'm not a math major so I figured this would be the place to go! Thanks!",['multivariable-calculus']
175230,Interior points of Affine Variety in $\mathbb{C}^2$,"I've just read that no non-trivial affine variety in $\mathbb{C}^2$ has an interior point. I can't see why this is obviously true. Could somebody give me a hint? In particular, is this something special about $\mathbb{C^2}$, or is it true generally in $\mathbb{A}^n$? Many thanks!",['algebraic-geometry']
175240,Solve system of nonlinear differential equations,"I am trying to solve a large system of differential equations. Ideally, I would like to solve it exactly, but if not, can anyone suggest me a numerical method? In all its generality, the system I am trying to solve is like this: (here, $x = x(t) \in R^n$, and $\dot x = dx/dt$) $$
(a_i + P_ix/\Vert P_ix \Vert)^T \dot x = -\Vert P_ix \Vert
$$ for $i = 1,\ldots,n$. Here all $P_i$ are positive definite matrices, and the set of $a_i$ is linearly independent. Also, $\Vert . \Vert$ is the 2-norm. It would help me a great deal if someone can help me to solve even a highly restricted special case of it, where $n=2$, $a_i = e_i$ (the $i$-th vector of the canonical basis), and $P_i = I$ for all $i$. Namely, this system: $$
(e_i + x/\Vert x \Vert)^T \dot x = -\Vert x \Vert
$$
for all $i$. Thanks a lot,
Daniel.",['ordinary-differential-equations']
175250,Computing a complex integral potentially using residues,"The question is: Compute: $$\mbox{p.v.}\int_{-\infty}^{\infty}\frac{x\sin4x}{{x^2}-1}dx$$ Initially I thought it was straight forward and I could just use residues. However, the Residue Theorem requires the poles to be in the upper plane ($y > 0$), and in this case, that is not the case. So, now I have no idea what to do since I cannot use residues.",['complex-analysis']
175251,How to show that this set is compact in $\ell^2$,"Let $(a_n)_{n}\in\ell^2:=\ell^2(\mathbb{R})$ be a fixed sequence. Consider the subspace $$C=\{(x_n)_{n}\in\ell^2 : |x_n|\le a_n\text{ for all }n\in\mathbb{N}\}.$$ According to the book [Dunford and Schwartz, Linear operators part I , page 453] $C$ is compact in the $\ell^2$-norm, but there is no proof. How can I show that $C$ is indeed compact in $\ell^2$ ?","['hilbert-spaces', 'functional-analysis', 'banach-spaces', 'compactness']"
175263,Gradient And Hessian Of General 2-Norm,"Given $f(\mathbf{x}) = \|\mathbf{Ax}\|_2 = (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}$, $\nabla f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {\|\mathbf{Ax}\|_2} = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}}$ $\nabla^2 f(\mathbf{x}) = \frac { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2} \cdot \mathbf{A}^\mathrm{T} \mathbf{A} - (\mathbf{A}^\mathrm{T} \mathbf{Ax})^\mathrm{T} (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{-1/2} \mathbf{A}^\mathrm{T} \mathbf{Ax} } {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} ) } = \frac { \mathbf{A}^\mathrm{T} \mathbf{A} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}} - \frac {\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{Ax} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{3/2} }$ I guess I am looking for confirmation that I have done the above correctly. The dimensions match up except for the second term of the Hessian is a scalar, which makes me think that something is missing. Edit: Also, the last equality reduces to $\nabla^2 f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{A} - \nabla f(\mathbf{x})^\mathrm{T} \nabla f(\mathbf{x})} {\|\mathbf{Ax}\|_2}$",['multivariable-calculus']
175264,"If $p$ is an element of $\overline E$ but not a limit point of $E$, then why is there a $p' \in E$ such that $d(p, p') < \varepsilon$?","I don't understand one of the steps of the proof of Theorem 3.10(a) in Baby Rudin. Here's the theorem and the proof up to where I'm stuck: Relevant Definitions The closure of the subset $E$ of some metric space is the union of $E$ with the set of all its limit points. The diameter of the subset $E$ of some metric space is the supremum of the set of all pairwise distances between its elements. For the points $x$ and $y$ in some metric space, $d(x, y)$ denotes the distance between $x$ and $y$. Theorem 3.10(a) If $\overline{E}$ is the closure of a set $E$ in a metric space $X$, then $$
\text{diam} \ \overline{E} = \text{diam} \ E.
$$ Proof. Since $E \subseteq \overline{E}$, we have $$\begin{equation*}
\text{diam} \ E \leq \text{diam} \ \overline{E}.
\end{equation*}$$ Let $\epsilon > 0$, and pick $p, q \in \overline{E}$. By the definition of $\overline{E}$, there are points $p', q' \in E$ such that $d(p,p') < \epsilon$ and $d(q, q') < \epsilon$... I see that this works if $p$ and $q$ are limit points of $E$. But how does this work if, say, $p$ isn't a limit point of $E$? What if $E$ is some region in $\mathbb{R}^2$ along with the point $p$ by itself way off somewhere?","['general-topology', 'real-analysis']"
175271,The group of roots of unity in an algebraic number field,"Is the following proposition true? If yes, how would you prove this? Proposition. Let $K$ be an algebraic number field.
The group of roots of unity in $K$ is finite.
In other words, the torsion subgroup of $K^*$ is finite. Motivation. Let $A$ be the ring of algebraic integers in $K$ .
A root of unity in $K$ is a unit (i.e. an invertible element of $A$ ). It is important to determine the structure of the group of units in $K$ to investigate the arithmetic properties of $K$ . Remark. Perhaps, the following fact can be used in the proof.
Every conjugate of a root of unity in $K$ has absolute value 1. Related question: The group of roots of unity in the cyclotomic number field of an odd prime order Is an algebraic integer all of whose conjugates have absolute value 1 a root of unity?","['algebraic-number-theory', 'abstract-algebra']"
175344,Composition of Relations,"I'm in a bit confusion of understanding ""Composition of Relations "". can someone help me up with an example. i have basic knowledge about relations, good explanation from some expert on this topic would get me through this topic.","['relations', 'elementary-set-theory', 'function-and-relation-composition']"
175348,The Star Trek Problem in Williams's Book,"This problem is from the book Probability with martingales by Williams. It's numbered as Exercise 12.3 on page 236. It can be stated as follows: The control system on the starship has gone wonky. All that one can do is to set a distance to be traveled. The spaceship will then move that distance in a randomly chosen direction and then stop. The object is to get into the Solar System, a ball of radius $r$ . Initially, the starship is at a distance $R>r$ from the sun. If the next hop-length is automatically set to be the current distance to the Sun.(""next"" and ""current"" being updated in the obvious way). Let $R_n$ be the distance from Sun to the starship after $n$ space-hops. Prove $$\sum_{n=1}^\infty \frac{1}{R^2_n}<\infty$$ holds almost everywhere. It has puzzled me a long time. I tried to prove the series has a finite expectation, but in fact it's expectation is infinite. Does anyone has a solution?",['probability-theory']
175350,"$\sum a_n$ converges absolutely, does $\sum (a_n + \cdots + a_n^n)$ converge","Suppose $\sum_{n=1}^\infty a_n$ converges absolutely. Does this imply that the series $$\sum_{n=1}^\infty (a_n + \cdots + a_n^n)$$ converges? I believe the answer is yes, but I can't figure out how to prove it. Any help would be appreciated. Thanks.","['sequences-and-series', 'analysis']"
175356,$p$-adic completion of integers,"I'm trying to do the following exercise: Let $p$ be a prime and for $n\geq 1$ let $\alpha_n :\mathbb Z/p \mathbb Z \to \mathbb Z/p^n \mathbb Z$ be the injection of  abelian groups given by $1 \mapsto  p^{n−1}$. Consider the direct sum $\alpha  : A \to B$ of these maps where $A$ is a countable direct sum of copies of $\mathbb Z/p \mathbb Z$ and $B$ is the direct sum of the groups $\mathbb Z/p^n \mathbb Z$. Show that the $p$-adic completion of $A$ is just $A$ but that the completion of $A$ for the topology induced from the $p$-adic topology on $B$ is the direct product of the $\mathbb Z/p \mathbb Z$. Deduce that $p$-adic completion is not a right exact functor on the category of all $\mathbb Z$-modules. At first I thought $A$ was just the normal integers but it's not since for example for $p=2$, $-1 = 01111\dots$ is not in the space. The direct sum are all things with only finitely many non-zero terms, so for example the sequence $a_0 = 10000\dots a_1 = 110000\dots, a_2 = 111000\dots$ is a sequence in $A$ with a limit not in $A$. I guess I am confused about what ""$p$-adic completion"" means: I assumed it meant that I take the equivalence classes of Cauchy sequences (Cauchy with respect to $|\cdot|_p$) where two sequences are equivalent if their difference tends to zero. But if that was what ""$p$-adic completion"" really meant then the sequence $a_k$ I gave above would be Cauchy and didn't have a limit in $A$ which is a counter example to what the exercise asks me to show. Would someone explain to me what ""$p$-adic completion"" means? Thanks. Edit I'm bumping this question because the answerer is on holiday and I still have a bunch of questions. Thanks for your help.","['general-topology', 'commutative-algebra', 'metric-spaces']"
175368,what is the behaviour of moving dot with 50% chance to go left or right?,"If a dot is moving (from zero) left or right, by one, with 50% chance to go left or right - is it going to go to the +inf or -inf when it has infinite moves?","['random-walk', 'probability']"
175369,How follows the Strong Law of Large Numbers from Birkhoff's Ergodic Theorem?,"We want to prove the strong law of large numbers with Birkhoff's ergodic theorem. Let $X_k$ be an i.i.d. sequence of $\mathcal{L}^1$ random variables. This is a stochastic process with measure-preserving operation $\theta$ (the shift operator). From Birkhoff's ergodic theorem, we obtain $\frac{X_0 + \dotsb + X_{n-1}}{n} \to Y$ a.s., with $Y=\mathbb{E}[X_1 \mid \mathcal{J}_{\theta}]$ a.s. Now, if $Y$ constant a.s., $Y= \mathbb{E}[X_1]$ a.s., and we would have the desired result. But why is $Y$ constant a.s.?","['probability-theory', 'ergodic-theory', 'probability']"
175375,"Uniform convergence of functions, Spring 2002","The question I have in mind is (see here , page 60 , the solution is at page 297 ): Assume $f_{n}$ is a sequence of functions from a metric space $X$ to $Y$. Suppose $f_{n}\rightarrow f$ uniformly and has inverse $g_{n}$. Now assume $f$'s inverse $g$ is uniformly continuous on $Y$. Prove that $g_{n}\rightarrow g$ uniformly. I could not prove it using standard techniques as I do not know how to bound $|g_{n}(y)-g(y)|$ when $n$ becomes very large. The authors argue that the convergence of $g_{n}(y)\rightarrow g(y)$ is similar to $f(g_{n}(y))\rightarrow f(g(y))$ because the mapping by a uniformly convergent function series keeps uniform convergence. Thus they give the following argument that $$d(f(g(y)),f(g_{n}(y)))=d(y,f(g_{n}(y)))\le d(y,f_{n}(g_{n}(y)))+d(f_{n}(g_{n}(y)),f(g_{n}(y)))=d(f_{n}(g_{n}(y)),f(g_{n}(y)))$$ So since $f_{n}\rightarrow f$ uniformly by hypothesis the statement is proved. My question is: Is the step of substituting $|g_{n}(y)-g(y)|$ by $|f(g(y))-f(g_{n}(y))|$ really justified? I could not get the ""keep uniform convergence"" thing the author is talking about. But I also could not come up with a better proof.","['general-topology', 'convergence-divergence', 'sequences-and-series', 'metric-spaces']"
175376,Is there a Definite Integral Representation for $n^n$?,"The factorial $n!$ has a nice representation as definite integral:
$$
    n!=\Gamma(n+1)=\int_0^\infty t^{n} e^{-t}\, \mathrm{d}t. \!  
$$
Is it possible to write down such an integral for $n^n$ as well? I tried to come up with an integral that reproduces a $n$ factor, $n$-times, but without success. I don't see a way to stop the partial integration process like in the $n!$ case. So this might not work here and I currently can't think of another way. If it helps to restrict $n$, feel free to do so. The only thing a found online so far is the Lambert's $W$ function , which is involved when solving $x^x=z$, but I'm not sure if this helps. EDIT: Answers with integrals of the form $\displaystyle n^n=\int_0^\infty \cdots dt$ are preferred.","['definite-integrals', 'exponentiation', 'number-theory']"
175380,Solution af a system of 2 quadratic equations,"I have a system of two quadratic equations with unknowns $x$ and $y$: $$a_{1 1} x y + a_{1 2} x^2 + a_{1 3} y^2 + a_{1 4} x + a_{1 5} y + a_{1 6} = 0,\\
a_{2 1} x y + a_{2 2} x^2 + a_{2 3} y^2 + a_{2 4} x + a_{2 5} y + a_{2 6} = 0,$$ where $a_{i j}$ are arbitrary scalars. Is there an algebraic solution of the above system?","['quadratics', 'algebra-precalculus']"
175387,P[random x is composite | $2^{x-1}$ mod $x = 1$ ]?,"Select a uniformly random integer $n$ between $2^{1024}$ and $2^{1025}$ (Q) What is the probability that n is composite given that $2^{n-1}$ mod $n = 1$ ? How did you calculate this? More info : One way to calculate this would be if you had the following two variables: $$P_Q(n) = 1 - { P_{prime}(n) \over P_{cong}(n) }$$ Where: $P_{prime}(n)$ is the probability n is prime $P_{cong}(n)$ is the probability that $2^{n-1}$ mod $n = 1$ So answering the following two questions would be sufficient to answer the main one: What is $P_{prime}(n)$ equal to? What is $P_{cong}(n)$ equal to ? (This holds because the probability that n is prime if the congruence is false is 0.) Based on the PouletNumber forumale given below: exp((ln(2^1025))^(5/14))-exp((ln(2^1024))^(5/14))

= 123 and (2^1025)*exp(-ln(2^1025)*ln(ln(ln(2^1025))) / (2*ln(ln(2^1025)))) -
(2^1024)*exp(-ln(2^1024)*ln(ln(ln(2^1024))) / (2*ln(ln(2^1024))))

= 9.82e263 So its between 123 < x < 9.82e263 ?? And so $P_Q$ is: 3.29e-306 < P_Q < 2.63e-44","['prime-numbers', 'primality-test', 'probability', 'number-theory']"
175391,Question about $L^1$-$L^2$ integrable functions,"Can somebody tell me what's wrong with the following argument? If $f$ is $L^1$ Lebesgue-integrable, say $f$ positive, then it is bounded almost everywhere by some bound $M$. Then $f^2 < M\cdot f$ which is in $L^1$, then $f$ is in $L^2$ and $L^1$ lies in $L^2$.
It seems to me that the map $x^{-1/2}$ is $L^1$ but not $L^2$ on $(0,1)$, hence a counterexample... So I'm a bit confused.","['proof-writing', 'measure-theory', 'integration']"
175392,Finite family of infinite sets / A.C.,"Let $\{A_i\mid i\in n\}$ be a finite family of infinite sets. ( That is, $A_i$ is infinite for every $i\in n$ and $n\in \mathbb{N}$) Here, we can choose representative $a_i$ from each $A_i$ and construct $\{a_i\mid i\in n\}$. This process really doesn't use Axiom of Choice? How do I write this process down formally (in mathematical language)","['elementary-set-theory', 'axiom-of-choice']"
175406,Structures on torus,"Quotienting $\mathbb R^2$ by different lattices isomorphic to $\mathbb Z^2$, we get different tori. Somehow I think of the tori as having different ""structures"", but thinking more about it, I am not quite sure what different structures I am really thinking of. Two structures I am guessing at are complex structures and metrics. Could someone explain how these differ? Also, am I thinking of the right kind of structure? Are there other structures which vary with the lattice chosen?",['differential-geometry']
175425,How to properly use GAGA correspondence,"currently studying algebraic surfaces over the complex numbers. Before i did some algebraic geometry (I,II,start of III of Hartshorne) and a course on Riemann surfaces. Now i understood that by GAGA, a lot of results transfer from complex analytic geometry to algebraic geometry over $\mathbb{C}$.
Question: in my RS surface course the genus for a compact RS was defined as the number of holes, formally half of the dimension of the first de rham cohomology group (or singular with coefficients in some field of characteristic zero).
Now i encountered the algebraic definition: the dimension of the first cohomology group of the structure sheaf. I expect them to be the same, but did not find a quick proof. Can anyone show me how this works? I learned that de rham cohomology with coefficients in $\mathbb{C}$ corresponds to sheaf cohomology of the constant sheaf $\mathbb{C}$. However i'm not sure, can someone either confirm this or tell me why it's not true? If it is true, i find it hard to work with, since up till now i've been basically using Serre duality and Riemann Roch for bundles to reduce computing homology to computing spaces of global sections, however since the constant sheaf is not locally free i cannot apply this trick (or can i?).
Also, the constant sheaf makes sense both algebraically and analytically, so which one should i take in the de rham <-> constant sheaf correspondence? Or do both work? (with GAGA in mind, i expect them to have the same cohomology, but this might not be true) So summarizing Do both definitions of genus agree? (of course assuming the algebraic curve to be smooth, so it is a RS) If they do, can you show me a proof? (preferably using some sheaf cohomology) Is it true that de rham cohomology corresponds to sheaf cohomology of the constant sheaf, if so is it the the analytic one, the algebraic one, or both? Am i limiting myself unnecessarily by using Riemann Roch and Serre duality just for bundles, i.e. can i use them for all sheaves? Lastly, when answering my questions, it would be immensely appreciated if you could elaborate a little on how to use GAGA in general. Hope this makes sense, i suspect them to be silly questions once i understand them, but right now it's a fuzz to me.. Joachim Edit: i just found a result from Hodge theory for surfaces in Beauville, stating $h^{0}(S,\Omega^1_S) = \frac{1}{2}h^{1}(S,\mathbb{R})$, all cohomology being analytic. So assuming (1) the same thing holds for curves and (2) GAGA identifies the algebraic and analytic cotangent bundle, Serre duality states that the left hand side is equal to $h^{1}(C,\mathcal{O}_C)$, i.e. the genus found in my AG textbook and i proved one thingwhat i wanted to proof. I was trying to find a reference on Hodge theory so i could verify (1) but a quick scan yielded no results. Does anyone have good idea on this? Also i was wondering about (2) for a longer time, i'd be really happy with any comments on this.","['sheaf-theory', 'complex-geometry', 'algebraic-geometry', 'algebraic-curves', 'riemann-surfaces']"
175428,Probability with card game,"I need help calculating the chances of winning this strange game that I'm going to explain right now: You have a deck of 52 cards(4 suits,Ace to King). All you have to do is turning
 the cards one by one counting one,two,three while you turn them. If you get an 
 Ace when you count one or a two when you count two or a three when you count
 three you lose. For example if you get: 2(one),K(two),6(three),3(one),Q(two),3(three) You lose,because you get a 3 when you counted three. The only way I could think to resolve this problem is to calculate the chances of losing and then:
\begin{equation}
P(W)=1-P(L)
\end{equation}
where $ P(W) $ is chances of winning and $ P(L) $ is chances of losing.
But how do I calculate $ P(L) $ ? I've tried this,but I'm almost sure that's wrong:
$P(L)=$chances of getting an ace in first position or chances of getting a 2 in second position or chances of gettin a 3 in third position or chances of getting an ace in fourth position and so on... So:
\begin{equation}
P(L)=\frac{4}{52}+\frac{4}{51}+\frac{4}{50}+\frac{3}{49}+\frac{3}{48}+\frac{3}{47}+\frac{2}{46}+\frac{2}{45}+\frac{2}{44}+\frac{1}{43}+\frac{1}{42}+\frac{1}{41}
\end{equation}
Thanks everybody:)","['card-games', 'probability']"
175443,Proving that the general linear group is a differentiable manifold,"We know that the the general linear group is defined as the set $\{A\in M_n(R): \det A \neq 0\}$. I have a homework on how to prove that it is a smooth manifold. So far my only idea is that we can think of each matrix, say $A$, in that group as an $n^2-$dimensional vector. So i guess that every neighborhood of $A$ is homeomorphic to an open ball in $\mathbb{R}^{n^2}$ (However, i don't know how to prove this.) Now, I'm asking for help if anyone could give me a hint on how to prove that the general linear group is a smooth manifold since I really don't have an idea on how to do this. (By the way, honestly, I don't really understand what a $C^{\infty}-$smooth structure means which is essential to the definition of a smooth manifold.) Your help will be greatly appreciated. :)",['differential-geometry']
175452,How to simplify an expression like this: $(x^2+x^{-2}-2)^{1/2}$,"Sorry, I am not sure how to do the maths mark-up on this site but hopefully the question will make sense. I should know how to do this, but I have got myself stuck! Can anyone help? $(x^2+x^{-2}-2)^{1/2}$",['algebra-precalculus']
175462,What is this topic (competition questions)?,"I discovered the following question by accident, and found it interesting, but I only resolved it by brute force: Problem: Let f: $\mathbb{R} \rightarrow \mathbb{R}$ have the property $(\pi)$ iff they satisfy the equation $$f(s^2 + f(t)) = t + f(s)^2 $$ Determine the family of all functions with the property $(\pi)$. Solution: If we can show $f(0) := y$ to be zero, then letting $t$ run over all real values, with $s = 0$, shows $f(f(x)) = x$ $\forall x \in \mathbb{R}$, from which it is clear that $\mathrm{Id}(x)$ has the property $(\pi)$. Taking $s = 0, t = 0$ shows (1) $f(y) = y^2$. Taking $s = y, t = 0$ shows (2) $f(y^2 + y) = (f(y))^2$, so that substitution from (1) gives (3) $f(y^2 + y) = y^4$. However, letting $s = 0, t = y$ shows (4) $f(y^2) = y + y^2$, so that $f(f(y^2)) = y^4$ by substituting the LHS of (4) in (3) , while letting $s= 0, t = y^2$ shows directly that (5) $f(f(y^2)) = 2y^2$. Equating (4) and (5) says $2y^2 = y^4$, so $y = -\sqrt{2}, 0,$ or $\sqrt{2}.$ Assume $y = \sqrt{2}.$ Letting $t = f(y^2), s = y$ says (6) $f(f(f(y^2)) + y^2) = f(3y^2)$ $ = f(y^2) + (f(y))^2$; but $s = \sqrt{2}y, t = y$ says (7) $f(3y^2) = (f(\sqrt{2}y))^2 + y = (f(y^2))^2 + y$. Since $(f(y))^2 = y^4$, $f(y^2) = y + y^2$, equating the RHS of (6) and (7) leaves $$ f(y^2) + (f(y))^2 = (f(y^2))^2 + y \implies y + y^2 + y^4 = (y^4 + 2y^3 + y^2) + y \implies 2y^3 = 0$$ which is absurd. Assuming $y = - \sqrt{2}$ and setting $s = -\sqrt{2}y, t = y$ in (7) similarly shows that $y \neq - \sqrt{2}.$ We conclude that $f(0) = 0$. That $\forall x \in \mathbb{R} (f(f(x)) = x)$ says that $f(x) = f^{-1}(x);$ this requires that $f$ be one-to-one. Since $f(0) = 0$, we have $t \neq 0 \implies f(t) \neq 0$; we can now show that $f$ is increasing. Pick $x, y \in \mathbb{R}, x < y,$ and set $\delta = y - x.$ By choosing $t = f(x), s = \sqrt{\delta}$, we have $$f(\delta + f(f(x))) = f(\delta + x) = f(x) + f(\sqrt{\delta})^2 \implies f(x + \delta) - f(x) = f(\sqrt{\delta})^2$$
$$\therefore f(y) - f(x) > 0$$ since we established that $f(\sqrt{\delta}) \neq 0$. This proves that $f$ is increasing.
Since, in fact, $\mathrm{Id}(x)$ is the only $f: \mathbb{R} \rightarrow \mathbb{R}$ which is both increasing and an involution, this says that $f(x) = x$ is the sole function with the property ($\pi$). So several things: 1) What field(?) of study does this question come from? Is it relevant to quotidian mathematics? 2) This was tricky. It took a lot of messing around with various combinations to get the main result, and I had the uncomfortable feeling that I was being inefficient. (I filled up a page and a half of notebook!) Does anybody know any books/have any advice on how to improve on questions like these? 3) If this is its own field, what other fields are best connected with it? (i.e., furnish handy tools)","['functions', 'functional-equations']"
175477,"If $f: \mathbb Q\to \mathbb Q$ is a homomorphism, prove that $f(x)=0$ for all $x\in\mathbb Q$ or $f(x)=x$ for all $x$ in $\mathbb Q$.","If $f: \mathbb Q\to \mathbb Q$ is a homomorphism, prove that $f(x)=0$ for all $x\in\mathbb Q$ or $f(x)=x$ for all $x$ in $\mathbb Q$. I'm wondering if you can help me with this one?","['ring-theory', 'abstract-algebra']"
175484,"Principles of Mathematical Analysis, Dedekind Cuts, Multiplicative Inverse","At the top of the page 20 of Rudin's book ''Principles of Mathematical Analysis'' he writes:
''The proofs (of the multiplication axioms) are so similar to the ones given in detail in Step 4 (proof of the addition axioms) that we omit them''. I tried to prove them but I got stuck in the proof of 
\begin{equation}\alpha \cdot {\alpha }^{-1}=1^*\end{equation}
 where $\alpha$ is positive cut and ${\alpha }^{-1}=\mathbb{Q}_{-}\bigcup\left\{0\right\}\bigcup\left\{t\in \mathbb{Q}:0<t<r\text{ for some }r\in \mathbb{Q}:\frac{1}{r}\notin \alpha\right\}$ is the candidate for the multiplicative inverse of $\alpha$. I have already proved that ${\alpha }^{-1}$ is a cut and $\alpha \cdot {\alpha }^{-1}\le 1^*$. My question is how do we prove the opposite direction similarly to the proof Rudin gives for $\alpha +(-\alpha) \le 0^*$. A proof completely different to that one can be found here: Dedekind cut multiplicative inverse Here is what I have tried thus far: Let $p\in 1^*$. If $p\le 0$ then obviously $p\in \alpha\cdot \alpha^{-1}$. Suppose $0<p<1$ and $q=q(p)\in \mathbb{Q}_{+}$. By the Archimedean Property of Rational numbers
\begin{equation}\exists n\in \mathbb{N}:nq\in \alpha\text{ and }(n+1)q\notin \alpha\end{equation} 
We must find a $u \in \alpha^{-1}$ such as that $p=(nq)\cdot u$ or equivalenty, $u=\frac{p}{nq}$ In order for $u \in \alpha^{-1}$ we must have that $0<u<r$ and $\frac{1}{r}\notin \alpha$ for some rational $r$. The only reasonable choice for $r$ would be $\frac{1}{(n+1)q}$. But then,
\begin{equation}u<r\Leftrightarrow \frac{p}{nq}<\frac{1}{(n+1)q}\Leftrightarrow p<\frac{n}{n+1}\end{equation} which may not be true for some values of $n$ (like $0$). Where can we derive a restriction for these values of $n$? EDIT: Found another proof here: http://mypage.iu.edu/~sgautam/m413.33418.11f/Dedekind.pdf STill nothing similar to Rudin's...","['elementary-set-theory', 'real-analysis']"
175488,How does the Siamese method to construct any size of n-odd magic squares work?,"A Magic Square of order n is an arrangement of $n^2$ numbers, usually distinct integers, in a square, such that the n numbers in all rows, all columns, and both diagonals sum to the same constant. To construct Magic Squares of n-odd size, a method known as Siamese method is given on Wikipedia , the method is :: starting from the central box of the first row with the number 1 (or
the first number of any arithmetic progression), the fundamental 
movement for filling the boxes is diagonally up and right (↗), one step
at a time. When a move would leave the square, it is wrapped around to 
the last row or first column, respectively.

If a filled box is encountered, one moves vertically down one box (↓)    
instead, then continuing as before. How does this method work?","['matrices', 'recreational-mathematics', 'magic-square']"
175500,3xy + 14x + 17y + 71 = 0 need some advice,"$$3xy + 14x + 17y + 71 = 0$$ Need to find both $x$ and $y$. If there was only one variable then this is easy problem. Have tried: $$\begin{align}3xy &= -14x - 17y - 71 \\
x &= \frac{-14x - 17y - 71}{3y}\end{align}$$ Then tried to put this expression everywhere instead of $x$ but it tooks forever to find both $x$ and $y$. I don't even know how to get on right track.
Please give any advice. Thanks.",['algebra-precalculus']
175525,What are differences between semidirect product and direct product?,"Given two groups $A, B$, we can construct direct product $A \times B$ whose elements are of the form $(a, b), a \in A, b\in B$. If $A, B$ are subgroups of a group $G$ and $A \cap B =\{1\}$, then we can construct semidirect product $A \rtimes B$ whose elements are of the form $ab, a\in A, b\in B$. In this case, is the semidirect product $A \rtimes B$ the same as direct product $A \times B$? Is the order of $AB$ equal to $|A||B|$? For example, is the order of $(\mathbb{Z}/2\mathbb{Z})^n \rtimes S_n$ equal to $2^n * n!$? Here $S_n$ is the symmetrical group of order $n$. Thank you very much.","['semidirect-product', 'group-theory', 'abstract-algebra']"
175553,Every multiplicative linear functional on $\ell^{\infty}$ is the limit along an ultrafilter.,"It is well-known that for any ultrafilter $\mathscr{u}$ in $\mathbb{N}$, the map\begin{equation}a\mapsto \lim_{\mathscr{u}}a\end{equation} is a multiplicative linear functional, where $\lim_{\mathscr{u}}a$ is the limit of the sequence $a$ along $\mathscr{u}$. I vaguely remember someone once told me that every multiplicative linear functional on $\ell^{\infty}$ is of this form. That is, given a multiplicative linear functional $h$ on $\ell^{\infty}$, there is an ultrafilter $\mathscr{u}$ such that \begin{equation}
h(a)=\lim_{\mathscr{u}}a
\end{equation} for all $a\in\ell^{\infty}$. However, I cannot find a proof to this. I can show that if $h$ is the evaluation at $n$, then $h$ corresponds to the principal ultrafilter centered at $n$, but there are other kinds of multiplicative functionals (all these must vanish on any linear combinations of point masses though). Can somebody give a hint on how to do this latter case?
Thanks!","['functional-analysis', 'filters', 'real-analysis', 'analysis', 'banach-spaces']"
175554,Prove $\frac{1+\cos{(2A)}}{\sin{(2A)}}=\cot{A}$,"I am sorry to ask so many of these questions in such as short time span. But how would I prove this following trigonometric identity.
$$
\frac{1+\cos(2A)}{\sin(2A)}=\cot A
$$
My work thus far is
$$
\frac{1+\cos^2A-\sin^2A}{2\sin A\cos A}
$$
I know $1-\sin^2A=\cos^2A$ So I do
$$
\frac{\cos^2A+\cos^2A}{2\sin A\cos A}
$$
I know not what I do next.",['trigonometry']
175559,"A set that it is uncountable, has measure zero, and is not compact","I want a example of a set that it is uncountable and has measure zero and not compact?
Cantor set has these properties except not compactness.","['measure-theory', 'examples-counterexamples', 'real-analysis']"
175561,Is the following matrix invertible?,"$$\begin{bmatrix} 1235 &2344 &1234 &1990\\
2124 & 4123& 1990& 3026 \\
1230 &1234 &9095 &1230\\
1262 &2312& 2324 &3907 
\end{bmatrix}$$ Clearly, its determinant is not zero and, hence, the matrix is invertible. Is there a more elegant way to do this? Is there a pattern among these entries?","['matrices', 'linear-algebra', 'inverse']"
175570,Exponential Distribution - memoryless,"A post office has 2 clerks. Alice enters the post office while 2 other customers,
Bob and Claire, are being served by the 2 clerks. She is next in line. Assume
that the time a clerk spends serving a customer has the Exponential(x)
distribution. (a) What is the probability that Alice is the last of the 3 customers to be done
being served Hint: no integrals are needed. (b) What is the expected total time that Alice needs to spend at the post
office? I understand that no matter who leaves the office first, the remainer has the same expo(x) like Alice, but don't have intuitive thinking why the answer for the 1st is 1/2.",['probability']
175575,Show matrix $A+5B$ has an inverse with integer entries given the following conditions,"Let $A$ and $B$ be 2×2 matrices with integer entries such that
each of $A$, $A + B$, $A + 2B$, $A + 3B$, $A + 4B$ has an inverse with integer
entries. Show that the same is true for $A + 5B$.","['matrices', 'linear-algebra', 'inverse']"
175600,The count of functions from $X$ to $Y$?,"Let $X$ be a set with $N$ elements, and $Y$ a set with $M$ elements. What is the count of possible functions from $X$ to $Y$? If answer is $M^N$, then why can't it be $N \times M$? That is, why not count for each  $x \in X$, the possible mappings from $x$ to each $y\in Y$?","['functions', 'combinatorics']"
175607,"Differentiable function, not constant, $f(x+y)=f(x)f(y)$, $f'(0)=2$","Let $f: \mathbb R\rightarrow \mathbb R$ a derivable function but not
  zero, such that $f'(0) = 2$ and  $$ f(x+y)= f(x)\cdot \ f(y)$$ for all
  $x$ and $y$ belongs $\mathbb R$. Find $f$. My first answer is $f(x) = e^{2x}$, and I proved that there are not more functions like $f(x) = a^{bx}$ by Existence-Unity Theorem (ODE), but I don't know if I finished. What do you think about this sketch of proof's idea? Thanks, I'll be asking more things.","['calculus', 'functions']"
175613,sum with permutations,"Let $a$ be vector in $R^{2m}$. And let $S_{2m}$ be group of all permutations on the set $\{1,\dots,2m\}$. I would like to calculate
 $$
\sup_{\pi\in S_{2m}}\sum_{d(\sigma, \pi)=2}\left(\left|\sum_{k=1}^ma_{\sigma(k)}-\sum_{k=m+1}^{2m}a_{\sigma(k)}\right|-\left|\sum_{k=1}^ma_{\pi(k)}-\sum_{k=m+1}^{2m}a_{\pi(k)}\right|\right)^2.
$$ Here $\sigma(\cdot),\pi(\cdot)$ are a permutations on the set $\{1,...,2m\}$ with uniform distribution. Of course, I can open square: \begin{align}
\sum_{d(\sigma, \pi)=2}\Big(\left|\sum_{k=1}^ma_{\sigma(k)}-\sum_{k=m+1}^{2m}a_{\sigma(k)}\right|^2&-2\left|\sum_{k=1}^ma_{\pi(k)}-\sum_{k=m+1}^{2m}a_{\pi(k)}\right|\left|\sum_{k=1}^ma_{\sigma(k)}-\sum_{k=m+1}^{2m}a_{\sigma(k)}\right|\\
&+\left|\sum_{k=1}^ma_{\pi(k)}-\sum_{k=m+1}^{2m}a_{\pi(k)}\right|^2\Big),
\end{align}
and now for the first and for the last terms
$$
\left|\sum_{k=1}^ma_{\pi(k)}-\sum_{k=m+1}^{2m}a_{\pi(k)}\right|^2=\sum_{i=1}^{2m}a^2_{\pi_(i)}+2\sum_{i=1}^{2m}\sum_{i\neq k}a_{\pi(i)}a_{\pi(k)}-2\sum_{i=1}^{2m}\sum_{k=1}^{2m}a_{\pi(i)}a_{\pi(k)}.
$$ But I don't know what to do with the second term of the sum and how to sum everything over $d(\pi, \sigma)$. Note here, $d(\sigma, \pi)=2$ means $\sigma=\pi \tau$, where $\tau$ is a transposition.
Thank you for the help.","['statistics', 'permutations', 'summation', 'combinatorics']"
175615,original source for the Borel-Kolmogorov paradox,"Does anyone know the original source for the Borel-Kolmogorov paradox?  Is it online somewhere?  Kolmogorov doesn't give a precise citation.  (He does list three works by Borel in his bibliography, but it is not easy for me to check them.)","['probability-theory', 'math-history', 'reference-request']"
175624,Finding shortest distance between a point and a surface,"Consider the surface $S$ (in $\mathbb R^3$) given by the equation $z=f(x,y)=\frac32(x^2+y^2)$. How can I find the shortest distance from a point $p=(a,b,c)$ on $S$ to the point $(0,0,1)$. This is what I have done: Define $d(a,b,c)=a^2+b^2+(c-1)^2$, for all points $p=(a,b,c)\in S$. Then $\sqrt d$ is the distance from $S$ to $(0,0,1)$. I think that the method of Lagrange multipliers is the easiest way to solve my question, but how can I find the Lagrangian function? Or is there an easier way to find the shortest distance?","['multivariable-calculus', 'calculus']"
175657,Quantitative Analysis of Structure of Gaussian Mixture Model,"I am fitting a Gaussian Mixture Model to high-dimensional data (40 dimensions). I have trained the model using EM, learned the parameters and now I want to know quantitatively: What is most important in capturing the structure of the data, the
  means or the covariance matrices? Currently, I can think of measuring the Euclidean distance between different means or the cosine of the principal eigenvectors of the different covariance matrices to measure if the direction of variability each covariance matrix captures is similar or different to the rest. Any ideas ?","['statistics', 'machine-learning']"
175666,What function satisfies $x^2 f(x) + f(1-x) = 2x-x^4$?,What function satisfies $x^2 f(x) + f(1-x) = 2x-x^4$? I'm especially curious if there is both an algebraic and calculus-based derivation of the solution.,"['calculus', 'algebra-precalculus', 'functional-equations']"
175674,Cauchy-Schwarz matrix inequality for random vectors,"If $X$ and $Y$ are random scalars, then Cauchy-Schwarz says that
$$| \mathrm{Cov}(X,Y) | \le \mathrm{Var}(X)^{1/2}\mathrm{Var}(Y)^{1/2}.$$
If $X$ and $Y$ are random vectors, is there a way to bound the covariance matrix $\mathrm{Cov}(X,Y)$ in terms of the matrices $\mathrm{Var}(X)$ and $\mathrm{Var}(Y)$? In particular, is it true that
$$\mathrm{Cov}(X,Y) \le \mathrm{Var}(X)^{1/2}\mathrm{Var}(Y)^{1/2},$$
where the square roots are Cholesky decompositions, and the inequality is read as meaning that the right hand side minus the left hand side is positive semidefinite?","['statistics', 'matrices', 'inequality', 'probability']"
175675,Intuitionistic Banach-Tarski Paradox,"While the Banach-Tarski paradox is a counter-intuitive result which requires the Axiom of Choice, leading some people to argue specifically against Choice, and others to argue for constructive mathematics, as the use of Choice is the only non-constructive step in the proof, and traditional accounts of constructive logic do not contain choice. However, newer frameworks of constructive logic such as intensional type theory (ITT) do admit Choice as a rather trivial theorem. Does this mean that the Banach Tarski theorem is also a theorem of ITT?","['logic', 'type-theory', 'measure-theory', 'axiom-of-choice']"
175692,Is composition of piecewise linear functions again a piecewise linear function?,"I have some piecewise linear (not necessarily continuous) functions (also, in case it matters, in my specific case 'a' is larger than 0 in all functions). Is every the composition of those functions again a piecewise linear (not necessarily continuous) function? If yes, how about the slightly more complicated scenario. The functions are still piecewise, but now the pieces are not ideal linear functions anymore but include a small amount of 'noise': f(x)= a*x+b+randomNoise (randomNoise is different for every call to f(), but always smaller than a). Is the composition of such functions again a piecewise linear function of the form f(x)= a*x+b+randomNoise for all pieces?",['functions']
175727,Proof of Gauss's Lemma (Riemannian Geometry version),"I was self-learning Do Carmo's Riemannian Geometry, there is a step in the proof of Gauss's Lemma what I can't quite figure out. Since $d\,\exp_p$ is linear and, by the definition of $\exp_p$,
  $$
\langle (d\,\exp_p)_v(v),(d\,\exp_p)_v(w_T)\rangle=\langle v,w_T\rangle. 
$$ So I went on wikipedia hoping to find something that can help me figure this out. I did find something. HERE . It says that $(d\,\exp_p)_v(v)=v$. In order to do that, it constructs that curve $\alpha(t)$ with $\alpha(0)=v$, $\alpha'(0)=v$. And it gives $\alpha(t)=(t+1)v$. I agree with all these. Then it argues that you can view $\alpha (t)=vt$ since it's just a shift of parametrization. Okay. I am okay with that. But in this case, should $(d\,\exp_p)_v(v)={d\over dt}(\exp_po \alpha(t))|_{t=1}$, instead of evaluating the derivative at $t=0$ as argued in wiki? Also, I found myself really uncomfortable with all the abuse of notations in Differential Geometry. Like here, the identification of the $T_p M$ and $T_v(T_p M)$ freaks me out. Does this mean that when a local coordinate system is picked, $T_p M$ under the natural basis will be the same as $R^n$, and so does $T_v (T_p M)$?","['riemannian-geometry', 'differential-geometry']"
175729,On the element orders of finite group,"Let $G$ be a finite group. Suppose that $G$ has a normal Sylow $p-$subgroup $P$ such that $|P|=p^2$ where $p\neq 2$, but $P$ does not contain an element of order $p^2$ or equivalently $P$ is not cyclic. Is it the case that if $g \in G$ has order $k$ such that $\gcd(k,p)=1$ then there is an element of $G$ of order $kp$? Thanks in advance.",['group-theory']
175736,Evaluate $\tan^{2}(20^{\circ}) + \tan^{2}(40^{\circ}) + \tan^{2}(80^{\circ})$,Evaluate $\tan^{2}(20^{\circ}) + \tan^{2}(40^{\circ}) + \tan^{2}(80^{\circ})$. Can anyone help me with this? Thank You!,['trigonometry']
175750,"The definition of metric space,topological space","I have read some books in analysis. All of them define metric space, topological space or vector space directly, without any reason. Therefore, I want to know the background of the definition - the problem the space aim to solve - is there any reference? Thanks a lot.","['functional-analysis', 'general-topology', 'reference-request', 'soft-question', 'intuition']"
175768,Show $1 + 2 \sum_{n=1}^N \cos n x = \frac{ \sin (N + 1/2) x }{\sin \frac{x}{2}}$ for $x \neq 0$ [duplicate],"This question already has answers here : Finite Sum $\sum\limits_{k=0}^{n}\cos(kx)$ (7 answers) Closed 10 years ago . For $x \neq 0$, $$ 1 + 2 \sum_{n=1}^N \cos n x = \frac{ \sin (N + 1/2) x }{\sin \frac{x}{2}} $$","['trigonometry', 'summation']"
175773,Evaluating $\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2}dx$,I have the following integral $$\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2} dx$$ that I do not know how to evaluate. Could you please give me a hint? Thanks in advance.,"['calculus', 'integration']"
175791,Show $ \int_{-\pi}^{\pi}  \frac{1 - \cos (n+1) x}{1- \cos x} dx = (n+1) 2 \pi$ for $n \in \mathbb N$ [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: Compute the trigonometric integrals For $n \in \mathbb N$, $$ \int_{-\pi}^{\pi}  \frac{1 - \cos (n+1) x}{1- \cos x} dx = (n+1) 2 \pi$$","['definite-integrals', 'calculus', 'integration']"
175794,Jacobian of $A^{-1}b$,"I need to calculate the Jacobian $\frac{df}{dx}$ of $f=A^{-1}b$ where $A$ and $b$ are a function of $x$, the variable towards to differentiate. I thought $$\frac{df}{dx} = \frac{dA^{-1}}{dx} b  + A^{-1}\frac{db}{dx}$$ by the product rule, 
and since $A^{-1}A=I$, $$\frac{dA^{-1}}{dx} = A^{-1} \frac{dA}{dx}  A^{-1}.$$ Now the last thing i thought is $$\frac{dA}{dx} = \frac{dA}{dx_1} + \frac{dA}{dx_2} + \frac{dA}{dx_3} + \cdots $$ The last step is to calculate the Jacobian of a matrix. However, if I try this for a simple example, I get a wrong answer. Can anyone see where I make the mistake? How can I calculate the Jacobian of $f=A^{-1}b$ correct if I cannot analytically invert $A$ (I can only do that numerically)?","['matrices', 'multivariable-calculus']"
175807,"""Push-Pull"" Morphisms of Higher Direct Image Sheaves","(This is 20.7.B in Ravi Vakil's notes) Suppose $f:Y \to Z$ is any morphism, and $\pi: X\to Y$ is quasicompact and quasiseparated. Suppose $\mathcal{F}$ is a quasicoherent sheaf on $X$. Let $$\require{AMScd}\begin{CD}
W @>{f'}>> X \\
@V{\pi'}VV @VV{\pi}V \\
Z @>>{f}>  Y
\end{CD}$$ be a fiber diagram. Describe a natural morphism $f^*(R^i\pi_*\mathscr{F}) \to R^i\pi'_*(f')^* \mathscr{F}$ of sheaves on $Z$. I'm not sure how to work with higher direct image sheaves at all, so help would be appreciated.","['algebraic-geometry', 'schemes']"
175810,Do such sequences exist?,"I wish to know if there are real sequences $(a_k)$, $(b_k)$ (and if there are, how to construct such sequences) such that: $b_k<0$ for each $k \in \mathbb{N}$ with  $\lim\limits_{k \rightarrow \infty} b_k=-\infty,$ $$\sum_{k=1}^\infty |a_k| |b_k|^n< \infty, \space\forall n\in\mathbb{N}\cup\{0\}$$ $$\sum_{k=1}^\infty a_k b_k^n=1, \space \forall n\in\mathbb{N}\cup\{0\}$$","['sequences-and-series', 'analysis']"
175837,How do I set up a function as a graph so I can find the range?,"$$h(x) = \frac{6x^2}{3x^2-2x-1}$$ I know that the domain is excluding $-\frac{1}{3}$ and $1$. The range is $(2, \infty)$ and $(-\infty, 0)$ How many minimum values need to be plotted in order to find the range of a function? Like say there's an equation and its range is all values up to 1000. It would be a lot of work to do that on a graph by hand. Is there a shortcut?","['graphing-functions', 'algebra-precalculus', 'functions']"
