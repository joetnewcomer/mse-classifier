question_id,title,body,tags
3319129,Can we tile the board by L trominos? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I tried to cover this, but there is no way I can fill it. 
The black square is a removed square. Is there a way to prove that it cannot be filled with L trominos?",['combinatorics']
3319132,Is $\lim_{x\to \infty} -\frac{2}{\sqrt{x-2} +\sqrt{2}} = 0$?,"I was solving following limit problem: $$\lim_{x\to \infty} (\sqrt{x-2} - \sqrt{x})$$ Since (If I get this correctly) $\infty - \infty$ tells us nothing about the solution of the limit problem, we need do something. After multiplying $(\sqrt{x-2} - \sqrt{x})$ by $\frac{\sqrt{x-2}+\sqrt{x}}{\sqrt{x-2}+\sqrt{x}}$ , I got: $$\frac{x-2 - x}{\sqrt{x-2}+\sqrt{x}}  =\frac{-2 }{\sqrt{x-2}+\sqrt{x}} $$ I suppose that if it was something like this $\frac{-2}{\sqrt{x}}$ , it would evalute to zero, but what about fractional function like the one above? And while I am at it, I will ask one more question: suppose it was $\lim_{x\to \infty}\frac{-2 }{\sqrt{x-2}-\sqrt{x}}$ , what would that limit evaluate to?","['limits', 'calculus']"
3319147,Colors and corresponding numbers,"The 10 colors Green, Blue, Violet, Red, Orange, Yellow,  Cyan, Magenta, Fuchsia, Brown are associated to each of the numbers 1, 2,…, 10 but we don’t know which color corresponds to each number. In a large box there are infinite sealed envelopes, each containing one card of the above colors. 
  Our target is to find which number corresponds to each color.
  There is a keypad with the 10 numbers outside this box. Each time I type a sequence of 5 numbers (10 is considered as ONE number), and 5 envelopes come out of the box (but not in the order I typed the numbers). 
  We can open the envelopes and see the colors but we will still not know which is which. We can repeat this process only 3 times. Is it possible to determine the correspondence of colors with the numbers? What combinations of numbers must we use each of the 3 times? Let's say we first type 11223. Then we get 5 envelopes, of which, 2 + 2 will have the same color of cards. So now we know the color that corresponds to number 3 and also we know that 1 & 2 correspond to two other colors (that we also know - but we don't know which is which). We repeat the same process with 44556. Again we know 6, and 4 & 5.
In our last turn, we type 1, 4, 7, 8, 9. 
It the 5 envelopes, we will see 2 of the colors we have already seen in the first two draws and we will now know 1, 4 and 7 & 8 & 9, but not which is which. We will also know number 10. We can also do 11223, 14456, 57789 but I am still missing one number :(",['combinatorics']
3319149,Find $ \prod_{i=1}^{1903} (2^i + 5) \mod 1000 $,Find $$ \prod_{i=1}^{1903} (2^i + 5) \mod 1000 $$ my try I tried to find remainder $\mod 8$ and $\mod 125$ and use Chinese remainder theorem. Mod 8 Let see that for each $2^k + 5$ where $k\ge 3$ we have $2^k + 5 \equiv 5$ . So $$ \prod_{i=1}^{1903} (2^i + 5) \equiv 7 \cdot 9 \cdot 5^{1901} \equiv 35 \equiv 3 $$ Mod 125 Unfortunately I stucked at calculating mod $8$ . I want to say that officially (that means: from lecture) I don't know Carmichael function but I know Euler function if it can be helpful there.,"['number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3319175,Separate equations of the pair of lines $x^2+2xy\sec\theta+y^2=0$,"Find the separate equations of the lines $x^2+2xy\sec\theta+y^2=0$ Attempt 1 $$
x^2+2xy\sec\theta+y^2=0\\
\frac{x^2}{y^2}+2\frac{x}{y}\sec\theta+1=0\\
\frac{x}{y}=\frac{-2\sec\theta\pm\sqrt{4\sec^2\theta-4}}{2}=-\sec\theta\pm\tan\theta=\frac{-1\pm\sin\theta}{\cos\theta}\\
x\cos\theta=-y(1\pm\sin\theta)\\
x\cos\theta+y(1\pm\sin\theta)=0
$$ Attempt 2 $$
x^2+2xy\sec\theta+y^2=0\\
\frac{y^2}{x^2}+2\frac{y}{x}\sec\theta+1=0\\
\frac{y}{x}=\frac{-2\sec\theta\pm\sqrt{4\sec^2\theta-4}}{2}=-\sec\theta\pm\tan\theta=\frac{-1\pm\sin\theta}{\cos\theta}\\
y\cos\theta=-x(1\pm\sin\theta)\\
y\cos\theta+x(1\pm\sin\theta)=0
$$ Why do I seems to get different solutions for the lines in attempts 1 and 2 ?","['trigonometry', 'geometry']"
3319189,Proving the Generalized Bezout Theorem,"I am trying to prove the higher-dimensional analogue of Bezout theorem. It states the following: Suppose $F_1,...,F_n\in k[X_1,...,X_{n+1}]$ are homogenous polynomials
of degrees $d_1,...,d_n$ , and let $C_1,...,C_n\subseteq \mathbb{P}_n$ be their zero loci. Suppose in addition that they have no factor in
common. Then their intersection number is given by: \begin{align*}
\dim \Gamma(C_1\cap C_2\cap ...\cap C_n)=d_1...d_n\end{align*} For $n=2$ (which is the Bezout theorem we know and love), a fairly well known way to prove it is as follows. Suppose $C_1\cap C_2\subseteq\mathbb{A}_2$ , so none of their intersection happens in $l_{\infty}=Z(X_3)$ . This
can be accomplished through coordinate transformations. Letting, $f_1,f_2$ be the dehomogenized versions of $F_1$ and $F_2$ it suffices
to prove that: \begin{align*}\dim
k[x_1,x_2]/(f_1,f_2)=d_1d_2\end{align*} For $t\geqslant d_1+d_2$ , let $R_t\subseteq k[x_1,x_2]$ be the set of polynomials of degree at most $t$ . If we prove that $\dim R_t/(f_1,f_2)=d_1d_2$ for all $t\geqslant d_1+d_2$ , then it follows immediately that $\dim k[x_1,x_2]/(f_1,f_2)=d_1d_2$ as well. This on its turn can be accomplished by proving $\dim R_t/(f_1,f_2)$ only depends on the degrees of $f_1$ and $f_2$ , and compute the dimension using easy examples (like $f_1=x_1^{d_1}$ and $f_2=x_2^{d_2}$ ). In order to do so, we prove that there exist an exact sequence: \begin{align*}
0\xrightarrow{}R_{t-d_1-d_2}\xrightarrow{\alpha_1}R_{t-d_1}\oplus
R_{t-d_2}\xrightarrow{\alpha_2}R_t\xrightarrow{\pi}
R_t/(f_1,f_2)\xrightarrow{}0\end{align*} Define the maps by $\alpha_1(g)=(f_2g,f_1g)$ and $\alpha_2(g_1,g_2)=f_1g_1-f_2g_2$ , and let $\pi$ be the projection. It remains to prove we obtain an exact sequence. It is immediate that $\alpha_1$ is injective, $\pi$ is surjective, and $\alpha_2\alpha_1=0$ and $\pi\alpha_2=0$ . To prove $\ker(\alpha_2)\subseteq\text{im}(\alpha_1)$ , let $(g_1,g_2)\in\ker (\alpha_2)$ . Then $f_1g_1-f_2g_2=0$ , so $f_1g_1=f_2g_2$ . Since $f_1$ and $f_2$ have no prime factors in common, for each prime factor $\pi\mid f_1$ , we must have $\pi\mid g_2$ . As such, $f_1\mid g_2$ . Likewise, $f_2\mid g_1$ . Obviously, $g_2/f_1=g_1/f_2$ . As such, there exist a $h$ such that $(g_1,g_2)=(hf_2,hf_1)$ . We also have $\deg(h)\leqslant t-d_1-d_2$ , otherwise $\deg (g_1)>t-d_1$ . So $(g_1,g_2)\in\text{im}(\alpha_1)$ . To prove $\ker(\pi)\subseteq \text{im}(\alpha_2)$ , let $g\in \ker(\pi)$ . Then there exist $u_1,u_2\in k[x_1,x_2]$ such that $h_1f_1+h_2f_2=g$ . Assume that $\deg(h_1), \deg(h_2)$ are minimal . It remains to prove $\deg(h_1)\leqslant t-d_1$ and $\deg(h_2)\leqslant t-d_2$ . Suppose $\deg(h_1)>t-d_1$ . Then the lead terms of $h_1f_1$ and $-h_2f_2$ are the same. Letting $H_1, H_2$ be the homogenized versions of $h_1,h_2$ , and for any $p\in k[X_1,X_2,X_3]$ , let $p^*=p[X_1,X_2,0]$ . Then we have: $F_1^*H_1^*+F_2^*H_2^*=0$ . As $F_1,F_2$ don't intersect on $l_{\infty}$ , $F_1^*$ and $F_2^*$ are coprime, so $F_1^*\mid H_2^*$ and $F_2^*\mid H_1^*$ . Letting $Q=H_1^*/F_2^*$ , we have $-Q=H_2^*/F_1$ . Write $H_1=QF_2+R_1$ and $H_2=-QF_1+R_2$ . Then $R_1$ and $R_2$ are $0$ on $l_{\infty}$ . As such, $X_3\mid R_1,R_2$ . But that means that for their restrictions $r_1,r_2$ on $\textbf{A}_2$ , we have: \begin{align*} \deg(r_1)<\deg(h_1); \deg(r_2)<\deg(h_2)\end{align*} In addition, we have $R_1F_1+R_2F_2=H_1F_1+H_2F_2$ . That means that $g=r_1f_1+r_2f_2$ . This is a contradiction to the minimality of the degrees of $h_1,h_2$ . So I want to repeat this technique for $n>2$ . Take the assumption that $C_1\cap ...\cap C_n\subseteq \textbf{A}_n$ , so it suffices to prove that $\dim k[x_1,...,x_n]/(f_1,...,f_n)=d_1...d_n$ . Letting $R_t=k[x_1,...,x_n]_{\deg\leqslant t}$ , we want to prove $\dim R_t/(f_1,...,f_n)=d_1...d_n$ for $t\geqslant d_1+...+d_n$ . In order to do so, for any $S\subseteq \{1,...,n\}$ , let $R_{t,S}=R_{t_S}$ with $t_S=t-\sum_{i\in \{1,...,n\}\setminus S}d_i$ . We obtain the following sequence: \begin{align*} 0\xrightarrow{} \prod_{|S|=0} R_{t,S}\xrightarrow{\alpha_1}\prod_{|S|=1}R_{t,S}\xrightarrow{\alpha_2}...\xrightarrow{\alpha_{n}}\prod_{|S|=n}R_{t,S}=R_t\xrightarrow{\pi}R_t/(f_1,...,f_n)\xrightarrow{}0\end{align*} Again, $\pi$ is just the projection. For each $i$ , $\alpha_i$ is the sum of maps in the form $\beta_{S,k}$ with $|S|=i-1$ and $k\notin S$ . Such $\beta_{S,k}$ is given by: \begin{align*} \beta_{S,k}: R_{S,t}\to R_{S\cup\{k\},t}; g\mapsto\begin{cases} -gf_k&\text{ if }|S_{<k}|\text{ is odd}\\ gf_k&\text{ if }|S_{<k}|\text{ is even}\end{cases}\end{align*} The question is: how do I prove that this sequence is exact? It is again obvious that $\alpha_1$ is injective, $\pi$ is surjective and $\pi\alpha_n=0$ . It is also fairly easy to see $\alpha_{i+1}\alpha_{i}=0$ since $\beta_{S\cup\{k\},l}\beta_{S,k}+\beta_{S\cup\{l\},k}\beta_{S,l}=0$ .","['algebraic-geometry', 'abstract-algebra', 'intersection-theory', 'commutative-algebra']"
3319201,4 distinct integers with prime sum for each triple,"Here is a nice high school olympiad math problem: Can you choose 4 distinct positive integers so that the sum of each 3 of
them is prime? How about 5? It looks that just by looking at reminders mod 2,3,6 is not enough.","['contest-math', 'elementary-number-theory', 'combinatorics', 'prime-numbers']"
3319203,unit circle problem - tan ratio in the third quadrant,"I am convinced that the answer is E because as $x<0$ and $y>0$ , therefore $\tan(\theta)=\Bigg|\dfrac{-y}{x}\Bigg|=\dfrac{y}{-x}$ . Can anyone please confirm? Thank you!","['trigonometry', 'circles']"
3319228,Minimize KL divergence + linear function,"I am looking at the following problem: $$
\min_q \underbrace{KL \left[ q(x) ~||~ p(x) \right]}_{=: A} + \underbrace{\mathbb{E}_{x \sim q} \left [ f(x) \right ]}_{=: B}.
$$ For $A$ , the solution is $q(x) = p(x)$ . For $B$ , the solution is a point mass/dirac/delta distribution putting all its mass at $arg,\min f(x)$ . Further I know that $A$ should be strictly convex (accd to a comment in this question .) My questions are the following: Is there any hope of finding the minimizer in closed form, making use of the two respective solutions? Something like ""the minimizer lies on a line between the two respective solutions."" ?","['convex-optimization', 'probability', 'information-theory']"
3319241,Proof that $E[SS_E] = (n-2)\sigma^2$,"From Probability and Statistics in Engineering by Hines et. al: Let $y_i = \beta_0 + \beta _1 x_i + \epsilon$ , where $\epsilon$ has mean $0$ and variance $\sigma^2$ with all $\epsilon_i$ uncorrelated. Let $SS_E = \sum y_i^2  - n\bar y^2 - \hat \beta_1 S_{xy}$ , where $S_{xy} = \sum x_iy_i - \frac{1}{n}(\sum x_i^2)(\sum y_i^2)$ and $\hat \beta_1 = \frac{S_{xy}}{S_{xx}}$ . $E[\hat \beta _1] = \beta_1$ and $V(\hat \beta_1) = \frac{\sigma^2}{S_{xx}}$ . Then $E(SS_E) = (n-2) \sigma^2$ How is this derived?  I can't figure out a way to show this. $E[\sum y_i^2] = n\sigma^2 + (E[y_i])^2$ $E[\hat \beta_1 S_{xy}] = E[S_{xx}\hat \beta_1 ^2] = S_{xx}(\frac{\sigma^2}{S_{xx}} + \beta_1) = \sigma^2 + S_{xx} \beta_1$ $E[n \bar y^2] = nE[\bar y^2] = n (\frac{\sigma^2}{n} +(\frac{1}{n}\sum E[y_i])^2 )$ But I can't see a way to show equality with these.","['proof-explanation', 'statistics', 'probability']"
3319265,Maximum run in binary digit expansions,"For numbers between $2^{k-1}$ and $2^{k}-1$ , how many have a maximum run of $n$ identical digits in base $2$ ? For instance, $1000110101111001$ in base $2$ has a maximum run of 4. See picture below showing the number of numbers with a maximum run equal to $2$ , between $1$ and $2^{k}-1$ , for various values of $k$ . Clearly, it is a simple function of Fibonacci numbers. This seems to generalize to maximum run equal to $3, 4, 5$ and so on. See this previous question on the subject . However, the people who answered that question provide no reference and no explanation. It is also said (in that same question) that in a random string of $0/1$ of length $k$ , one expects the longest sequence of zeros to be roughly of length $\log k$ . I am also very interested in that statement (if you replace ""longest sequence of zero"" by ""longest sequence whether zero or one""), but where can I find a proof? The goal is to construct a sub-sequence of integers that has a max run less than (say) $\sqrt{k}$ for all $k$ so that as $k$ increases, and you divide the numbers in the sub-sequence by a power of two so that each number becomes a fraction between $0.5$ and $1$ , you end up at the limit with an irrational number that has a specific proportion of zero and one in its binary expansion. The final goal is to find a mathematical constant that we know for sure, based on the aforementioned construction, to be either normal or not normal.","['binary', 'number-theory', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
3319273,"Which value is larger, $f(1/2)$ vs $f(1/\pi)$, given $f(x)=x^x$","I have $f(x)=x^{x}$ and the question, Which value is larger, $f(1/2)$ or $f(1/\pi)$ ? It is easy to find the answer numerically. But, I am keen on an analytical one, with only elementary argument.","['calculus', 'functions', 'algebra-precalculus', 'inequality']"
3319274,"If $\sec A-\cos A=1$, then determine the value of $\tan^2\frac A2$","This is what I tried $\sec A=\frac{1}{\cos A}$ , so the equation becomes $1-\cos^2A=\cos A$ If we solve the above quadratic equation, we the values of $\cos A$ as $\frac{-1\pm \sqrt5}{2}$ Therefore, $\tan\frac A2$ becomes $$\sqrt \frac{3-\sqrt 5}{1+\sqrt 5}$$ Squaring that value, the answer remains meaningless The options are A) $\sqrt 5+ 2$ B) $\sqrt 5-2$ C) $2-\sqrt5$ D) $0$ Since the options are not matching, where am I going wrong?",['trigonometry']
3319283,Trigonometry: Value of constants,"I have this equation where $A\sin b=B\sin(a-b)$ I have to find the value of A and B. Is there any other way except for assuming $A=\sin(a-b)$ (if it's even correct in the first place) even though it is giving me the answer I want?
Note the $\sin$ values are not zero neither the constants. The a and b are specific points, nos. radians,...
edit: $sin(a)$ and $sin(a-b)$ is a constant value except zero.",['trigonometry']
3319287,Completeness of subsets in $\mathbb{R}$,"Let $S,T \subseteq \mathbb{R}$ be given by $S= \{ x \in \mathbb {R} : 2 x^2 \cos\frac1x =1\}$ and $T= \{ x \in \mathbb {R} : 2 x^2 \cos\frac1x \leq1\} \cup \{0\}$ . Then ,under the usual metric on $\mathbb R$ , which one is complete? My guess is both should be complete. As both sets are closed any cauchy sequence converges inside. But the answer given is only S. Am I making mistake?","['general-topology', 'metric-spaces']"
3319290,Ways to sit 5 kids in 12 chairs lined up in a row such that none of them are next to each other,"How many ways are there to sit 5 kids in 12 chairs lined up in a row such that none of them are next to each other? My first thought was to sit the kids with an empty chair next to them: $x_1|x_2|x_3|x_4|x_5$ (where $x_i$ are the kids and | are the chairs). by then I have used 9 chairs. Of course I can put a chair next to $x_1$ and another chair next to $x_5$ . There are $5!$ ways to sit the kids in 5 chairs, and then I have 3 chairs left, which can be put in any of the 6 spaces left so I arrange them as ${6}\choose{3}$ . So that would be $5!$${6}\choose{3}$$=2400$ which is wrong, because the correct answer to this would be $6720$ .","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
3319338,"Why doesn't the ""actual"" path matter for line integrals?","We have the following definition given in our textbook: Let $U \subseteq \mathbb{R}^n$ be open and $F: U \rightarrow \mathbb{R}$ be continuously partial differentiable. If $a, b \in U$ and $\gamma$ is a piecewise differentiable path from a to b, that lies completely in $U$ ( $[a,b]\in U$ ), then: $$\int_\gamma (\operatorname{grad} F) \cdot dx = F(b)-F(a)$$ This is obviously super useful for solving line integrals $$\int_\gamma f\,dx$$ where we can find $F$ such that $\operatorname{grad} F = f$ . My question is: why doesn't the path matter in these cases? If I have two paths $\gamma$ and $\gamma^*$ with the same origin/destination but with completely different paths, this tells me the line integral is the same. Why does this make sense?","['path-connected', 'vector-fields', 'multivariable-calculus', 'line-integrals']"
3319339,Help trying to prove the existence of sequence limit. [duplicate],"This question already has answers here : Proving the general limit $\lim_{n\to\infty} \frac{an+c}{bn+d} = \frac{a}{b}$ (4 answers) Closed 4 years ago . So I'm trying to prove that $$\lim_{n \to +\infty} \frac{2n - 4}{3n - 7} = \frac{2}{3}$$ using the formal definition of the limit of a sequence. Applying it I've got $$\left|\frac{2n - 4}{3n - 7} - \frac{2}{3}\right| \lt \varepsilon,$$ $$\frac{2}{\left|9n-21\right|} \lt \varepsilon.$$ From here I don't know how to continue, I guess I have to get ripped of the absolute value but I don't know how should I do it. Thank you so much.","['limits', 'sequences-and-series', 'epsilon-delta', 'real-analysis']"
3319376,From a set of N elements how many reflexive relations are out of the anti symmetric relations,"As I know in a set of N elements there are $2^{n^2}$ (two to the power of $n$ squared) relations, in which there are $3^{\frac12(n^2-n)}$ anti-symmetric relations.
How can I find out of those anti-symmetric relations the amount that is also reflexive?","['logic', 'discrete-mathematics']"
3319404,Integral $\int_0^1 \frac{\ln(1-x)\ln(1+x^2)}{x}dx$,"I am trying to solve by a different approach the fourth sum from here ,  namely: $$S= \sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{nm(4n+m)} =\int_0^1 \frac{\ln(1-x)\ln(1-x^4)}{x}dx= \frac{67}{32} \zeta(3) -\frac{\pi}{2}G$$ One way to solve it is similarly to my answer from there: $$S=\int_0^1 \frac{\ln(1-x)\ln(1-x^2)}{x}dx+\int_0^1 \frac{\ln(1-x)\ln(1+x^2)}{x}dx$$ From here we know that: $$\small \int_0^1 \frac{[m\ln(1+x)+n\ln(1-x)][q\ln(1+x)+p\ln(1-x)]}{x}dx=\left(\frac{mq}{4}-\frac{5}{8}(mp+nq)+2np\right)\zeta(3)$$ Thus by setting $m=0,n,p,q=1$ in the first integral we get that: $$S=\frac{11}{8}\zeta(3)+\int_0^1 \frac{\ln(1-x)\ln(1+x^2)}{x}dx=\frac{11}{8}\zeta(3)+I$$ $$I=\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n} \int_0^1 x^{2n-1} \ln(1-x)dx=\frac12\sum_{n=1}^\infty \frac{(-1)^n H_{2n}}{n^2}=\frac{23}{32}\zeta(3)-\frac{\pi}{2}G$$ And the result for $S$ follows. The last sum appears to be known, see $(659)$ from here , or alternatively since $I=2\Re\left( S(i)\right)$ just use the following identity: $$S(x)=\sum_{n=1}^\infty \frac{x^n}{n^2}H_n=\operatorname{Li}_3(x)-\operatorname{Li}_3(1-x)+\operatorname{Li}_2(1-x)\ln(1-x)+\frac{1}{2}\ln x \ln^2(1-x)+\zeta(3)$$ However I am trying to find a different method since the result is quite nice and I believe there's a nicer way to solve the integral without using such sums. Thus I would appreciate to get some help with the following problem: Prove without using Euler's sum or polylogs that $$\int_0^1 \frac{\ln(1-x)\ln(1+x^2)}{x}dx=\frac{23}{32}\zeta(3)-\frac{\pi}{2}G$$ I also tried to consider the following integral: $$J=\int_0^1 \frac{\ln(1+x)\ln(1+x^2)}{x}dx$$ $$\Rightarrow I+J=\int_0^1 \frac{\ln(1-x^2)\ln(1+x^2)}{x}dx\overset{x^2=t}=\frac12 \int_0^1\frac{\ln(1-t)\ln(1+t)}{t}dt=-\frac{5}{16}\zeta(3)$$ So now I am after the following integral: $$I-J=\int_0^1 \frac{\ln\left(\frac{1-x}{1+x}\right)\ln(1+x^2)}{x}dx=\frac74 \zeta(3)-\pi G$$","['integration', 'alternative-proof', 'definite-integrals']"
3319418,Why did we study “defect” in the late 1800s?,"I just began reading Weibel’s An introduction to Homological Algebra and stuck at the first page, where the author attempts to give a historical motivation for the homology construction: Homological algebra is a tool used in several branches of mathematics: algebraic topology, group theory, commutative ring theory, and algebraic geometry come to mind. It arose in the late 1800s in the following manner. Let $f$ and $g$ be matrices whose product is zero. If $g\cdot v=0$ for some column vector $v$ , say, of length $n$ , we cannot always write $v=f\cdot u$ . This failure is measured by the defect $$d=n-\text{rank}(f)-\text{rank}(g).$$ In modern language, $f$ and $g$ represent linear maps $$U\xrightarrow{f} V\xrightarrow{g} W$$ with $gf=0$ , and $d$ is the dimension of the homology module $$H=\ker (g)/f(U).$$ I am not seeing any motivation for studying the “defect” of the above phenomenon. Could anyone give a brief explanation for why did the mathematicians study how to write a column vector of $f$ by product of $f$ and another column vector? Much appreciated.","['matrices', 'linear-algebra', 'homological-algebra']"
3319421,Conjecture $\sum_{n=0}^\infty a_n= \frac{1}{2}-\frac{7 \zeta(3)}{2 \pi^2}$,"Working with some integrals I stumbled upon the following slowly converging series: $$
S =
\sum_{n = 0}^{\infty}\left(-1\right)^{n} \left[n + \frac{3}{2} + \left(n + 1\right)\left(n + 2\right) \log\left(1 - \frac{1}{n + 2}\right)\right]
$$ I have reasons to suspect that the series has a closed form: $$S=\frac{1}{2}-\frac{7 \zeta(3)}{2 \pi^2}=0.073721601182494209 \ldots$$ The actual proof eludes me so far. Can you prove or disprove this conjecture? Writing the logarithm as a series we have: \begin{align}
&\left(n + 2\right)
\log\left(1 - \frac{1}{n+2}\right) =
-\sum_{k = 1}^{\infty}
\frac{1}{k\left(n + 2\right)^{k - 1}}
\\ = &\
-1-\sum_{k=1}^\infty \frac{1}{(k+1) (n+2)^k}
\end{align} Which turns the series into: $$S=\sum_{n=0}^\infty (-1)^n \left(\frac{1}{2}-(n+1) \sum_{k=1}^\infty \frac{1}{(k+1) (n+2)^k} \right)$$ I can provide the way I came to this expression, but it's very long and complicated, as usual. I'd like some clear proof, if possible. To get the sense of how slowly the series converges, for $20000$ terms the result agrees with the stated closed form in $4$ first significant digits. The integral from which this series was obtained is (again, conjectured): $$\int_0^1 {_2 F_1} (1,-t;2-t;-1) dt = \frac{7 \zeta(3)}{\pi^2}+\frac{1}{2}$$ I don't think it's very useful, except for numerical confirmation.","['riemann-zeta', 'conjectures', 'logarithms', 'sequences-and-series']"
3319444,$(x^{2022}+1)(1+x^2+x^4+...+x^{2020})=2022\cdot x^{2021}$,"Let $S$ denote the set of all real values of $x$ for which $(x^{2022}+1)(1+x^2+x^4+...+x^{2020})=2022\cdot x^{2021}$ , then the number of elements in $S$ is $0/1/2/$ infinite? My attempt: $$(x^{2022}+1)(\frac{(x^2)^{1011}-1}{x^2-1})=2022\cdot x^{2021}$$ $$x^{4044}-1=2022\cdot x^{2023}-2022\cdot x^{2021}$$ Don't know how to proceed next.",['sequences-and-series']
3319477,Alternative Proof of Burnside's Lemma,"I studied group theory a long time ago. Back then, I didn't understand how to use the group theory-specific idioms to write short proofs. I still don't. Below is a proof of Burnside's Lemma using as little group theory as possible, by which I mean it uses few commonly-known lemmas. The basic thrust of the argument is showing that both sides count the number of fixed points of $\varphi(g)$ for each $g$ in the group $G$ . What are some good ways of proving Burnside's lemma without spending much ink and idiomatically using other results that are ""more basic"" than Burnside's Lemma? I'm attaching below my proof of the lemma. I looked at Wikipedia to get the statement of the theorem, but did not read the proof section until I completed the proof. Proof of Burnside's lemma. First a word on notation. The notation $[\psi]$ for a proposition $\psi$ is $1$ if the expression is true and $0$ if the expression is false. It is called an Iverson bracket . A group $G$ acts on a set $X$ . Equivalently, there exists a function $\varphi : G \to (X \to X) $ that sends each $g$ to a function from $X$ to itself. $\varphi$ is not required to be injective. $\varphi$ is not completely arbitrary; it satisfies some laws that I won't enumerate here. Let $\langle g, x \rangle$ denote the group action. $$ \langle g,x \rangle \stackrel{\mathrm{def}}{=} (\, \varphi(g)\,)(x) $$ Let $G(x)$ denote the orbit of $x$ in $G$ . $$ G(x) \stackrel{\mathrm{def}}{=} \{ g \in G \;|\; \langle g, x \rangle \} $$ Let $\simeq_G$ be a binary predicate that is true if and only if there exists a $g$ that sends the left argument to the right argument. $$ x \simeq_G y \stackrel{\mathrm{def}}{\iff} \left(\exists g \in G \mathop. \langle g, x \rangle = y \right) $$ Note that $$ x \simeq_G y \iff x \in G(y) $$ and $$ x \simeq_G y \iff y \in G(x) $$ Let's show that the negation of Burnside's Lemma is absurd. $$ |X/G|\cdot|G| \ne \sum_{g \in G} |X^g| $$ $$ |X/G|\cdot|G| \ne \sum_{g \in G} \left|\left\{ x \in X | \langle g, x \rangle = x \right\}\right| $$ $$ |X/G|\cdot|G| \ne \sum_{g \in G}\sum_{x \in X} [\langle g, x \rangle = x ] $$ $$ |G|\cdot|X/G| \ne \sum_{g \in G}\sum_{x \in X} [\langle g, x \rangle = x ] $$ $$ |G|\cdot\sum_{x \in X} \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X} [\langle g, x \rangle = x ] $$ $$ |G|\cdot\sum_{x \in X} \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ][ x = y ] $$ If we relax the restriction that $x = y$ and insist instead that $x \simeq_G y$ , then we can count each $x$ at $\frac{1}{|G(x)|} = \frac{1}{|G(y)|}$ value. $$ |G|\cdot\sum_{x \in X} \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{[x \simeq_G y]}{|G(x)|} $$ However, the condition $x \simeq_G y$ is redundant if we already know that $\langle g, x \rangle = y$ for some particular $g \in G$ . $$ |G|\cdot\sum_{x \in X} \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ $$ \sum_{x \in X} |G| \cdot \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ Replace $|G|$ with a sum counting $1$ for each item in $G$ . $$ \sum_{x \in X} \left( \sum_{g \in G} 1 \right) \cdot \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ The group element $g$ acting on the set $X$ sends a particular $x \in X$ to exactly one destination. $$ \sum_{x \in X} \left( \sum_{g \in G} \sum_{y \in X} [\langle g, x \rangle = y] \right) \cdot \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ $$ \sum_{x \in X} \sum_{g \in G} \sum_{y \in X} [\langle g, x \rangle = y] \cdot \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ All sub-expressions are positive, we can reorder. $$ \sum_{g \in G} \sum_{x \in X} \sum_{y \in X} [\langle g, x \rangle = y] \cdot \frac{1}{|G(x)|} \ne \sum_{g \in G}\sum_{x \in X}\sum_{y \in X} [\langle g, x \rangle = y ]\cdot\frac{1}{|G(x)|} $$ $$ \bot $$ Therefore $ | X/ G | \cdot |G| = \sum_{g \in G} |X^g| $ as desired.","['alternative-proof', 'group-theory']"
3319576,Complex symmetric matrices are normal,"In the taxonomy at https://en.wikipedia.org/wiki/List_of_matrices , it says that complex symmetric matrices are normal. For real symmetric matrices, you can prove that they are hermitian hence normal. How would you prove the normality for a complex symmetric matrix ?","['matrices', 'linear-algebra', 'symmetric-matrices']"
3319587,Is Calculus necessary for computer science student? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 4 years ago . Improve this question I'm a freshman in university and I'm studying Computer science and engineering. This will be my second year of studying. We don't have Calculus as a mandatory class but I can take it from elective classes. More senior students are telling me calculus is a very hard class and I shouldn't take it. Should I take easier classes just to pass? Or should I take it anyway even if I'm really bad at math but enjoy math a lot? Is calculus necessary for my future as a student and would it help me in data science or AI? That's what I'm really interested in and I want to work for either of them. Would Calculus make my education easier in the future and in my work or should I just take something just to pass? In my next semesters, I want to take just AI and DS classes (Data mining, Data Science, Mechanical Learning, etc). Thanks for your time reading and answering my question.","['calculus', 'soft-question', 'education', 'computer-science']"
3319611,"Determine if $p: \mathbb N \times \mathbb N \to \mathbb N$, $p(a,b) = \frac {ab(b+1)}2$ is injective and/or surjective.","Determine if $$p: \mathbb N \times \mathbb N \to \mathbb N\quad \quad p(a,b) = \frac {ab(b+1)}2$$ is injective and/or surjective. Just from looking at it and plugging in a few numbers I can already tell that this function is a bijection. However, I have no clue how to begin to prove that this is the case. I am comfortable proving injectivity and surjectivity for functions with 1 variable but I get very confused when there is more than 1. I could use some help generalizing a way of how to do this comfortably. Thanks.","['functions', 'discrete-mathematics']"
3319669,How to use Liouville's theorem to prove the formula of $\frac{\pi}{\sin \pi z}$?,"In my complex analysis textbook the author claims that the identity $$\frac{\pi }{ \sin \pi z} = \frac{1}{z} + \sum_{n \geq 1} (-1)^n \left[ \frac{1}{z-n} + \frac{1}{z+n} \right]$$ with $z \in \mathbb{C} - \mathbb{N}$ can be directly proved using Liouvilles's theorem, but I find it hard to verify that LHS-RHS is bounded. Can anyone give some ideas?","['complex-analysis', 'trigonometry', 'power-series']"
3319703,Help in proof from Riemannian Geometry by Docarmo.,"I have been working on ${\it Lemma\,5.2}$ from Riemannian Geometry by DoCarmo which establishes the existence and uniqueness of the vector field $Zf=(XY-YX)f$ , given $X$ and $Y$ as differenciable vector fields. On this proof we have expressions for $XYf$ and $YXf$ as follows: $XYf=\sum_{i,j}a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}$ $YXf=\sum_{i,j}b_{j}\frac{\partial a_{i}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{j}\partial x_{i}}$ Where $Xf=\sum_{i}a_{i}\frac{\partial f}{\partial x_{i}}$ and $Yf=\sum_{j}b_{j}\frac{\partial f}{\partial x_{j}}$ . If I substract expressions of the items I obtain $$Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}-b_{j}\frac{\partial a_{i}}{\partial x_{j}}\frac{\partial f}{\partial x_{i}}\right).$$ But DoCarmos says that this turns out to be $$Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}-b_{i}\frac{\partial a_{j}}{\partial x_{i}}\right)\frac{\partial f}{\partial x_{j}}$$ as if $\frac{\partial f}{\partial x_{i}}$ and $\frac{\partial f}{\partial x_{j}}$ were the same.",['differential-geometry']
3319708,Prove that $(p\oplus q)\oplus r$ is logically equivalent to $p\oplus (q \oplus r)$ [duplicate],"This question already has answers here : Proving $(p \oplus q) \oplus r=p \oplus (q \oplus r)$ (3 answers) Closed 4 years ago . I am reviewing for my upcoming Discrete Mathematics class and I'm clueless on the aforementioned question. I've tried expounding the XOR operator in hopes of transforming the problem but still I cannot find the solution to the problem. The title is pretty self-explanatory, how do I prove their logical equivalence without using a truth table? The truth table of the problem looks like this: Problem truth table",['discrete-mathematics']
3319746,Is every $1$-dimensional vector space a field?,"We say that every field $F$ ""is"" a $1$ -D vector space over itself. By this we mean that if we consider the elements of $F$ as both vectors and scalars, then we get a vector space by using the addition and multiplication from $F$ . It seems just as easy to go in the other direction and interpret any $1$ -D vector space as a field. But I've never seen it written that every $1$ -D vector space ""is"" a field. Why?","['field-theory', 'abstract-algebra', 'vector-spaces', 'terminology']"
3319754,How to find each equation for a set of five equidistant points around a unit circle,"Given a unit circle I want to make five points around it that are the same distance from each other. I know that in order to make four I can do $(1,0),(0,1),(-1,0),(0,-1)$ . But how can I do this on a five point algorithm?",['trigonometry']
3319762,How can one show that a matrix is nilpotent or not?,"Context : In linear algebra, a nilpotent matrix is a square matrix $N$ such that $$
N^{k}=0\,
$$ for some positive integer $k$ . (The smallest such $k$ is sometimes called the index of $N$ .) Question : How can one show that a matrix is nilpotent or not? Direct matrix multiplication for checking the definition is an obvious choice but seems not efficient. Is there any other method to do it?","['matrices', 'nilpotence', 'linear-algebra']"
3319790,Question regarding convergence not defined by a metric.,"I am trying to understand how to do the following question and I am confused about several issues:
What does it mean for convergence to not be defined by a metric (not satisfies the three axiom of metric space or something else entirely)? Am I suppose to show that the convergence defined in the question, even though it can not be defined by a metric, it nonetheless still satisfies conditions (1) and (2)? Lastly, how is this different than topology of point-wise convergence?  I am having trouble trying to have visual representation of this sequence like what is usually shown in an introductory calculus text for uniform convergence of sequences of functions.
Also, the question assumes a another question which I have also stated below. Question:
Let $X$ be the space of all infinite sequences $\{x_n\}$ of real numbers such that $x_{n}=0$ for all but a finite number of $n.$ Define a convergence in $X$ as follows: A sequence $a_n =(\alpha_{n,1},\alpha_{n,2},\alpha_{n,3}, \ldots  )$ converges to $a=(\alpha_1,\alpha_2,\alpha_3, \ldots)$ if the following two conditions are satisfied: (1) $|\alpha_{n,k} - \alpha_{k}|\rightarrow 0$ as $n \rightarrow \infty$ for every $k \in N,$ (2) there exists $k_{0} \in N$ such that $\alpha_{n,k}=0$ for all $n\in N$ and all $k \geq k_0.$ Prove that this convergence cannot be defined by a metric. Assume the following: Let $(X,d)$ be a metric space and let $x_{n,k} \in X$ for $k,n \in N$ .  Prove that if $$x_{1,1},x_{1,2},x_{1,3}, \ldots \rightarrow x$$ $$x_{2,1},x_{2,2},x_{2,3}, \ldots \rightarrow x$$ $$x_{3,1},x_{3,2},x_{3,3}, \ldots \rightarrow x$$ $$\vdots$$ $$x_{n,1},x_{n,2},x_{n,3}, \ldots \rightarrow x$$ $$\vdots$$ then there exists an increasing sequence of natural numbers $p_n$ such that $x_{n,p_n} \rightarrow x.$ Thank you in advance.","['general-topology', 'analysis']"
3319822,"Two people drawing cards, probability of both having pairs","I have been working working on the following probability problem: Alice has a set of 52 perfectly shuffled cards, hands the first 2 cards to Bob, Alice takes the next 2 cards. Let: B:= Bob has a pair A:= Alice has a pair Calculate: $\text{Pr}[A]$ , $\text{Pr}[B]$ , $\text{Pr}[A\cap B]$ $\text{Pr}[B]$ was straight forward, $\frac{3}{51}$ . $\text{Pr}[A]$ took some thinking, but as far as I understand, the point is that handing cards to Bob and not looking at them is equal to not drawing them in the first place. Hence, $\text{Pr}[A]=\frac{3}{51}$ I am now stuck with $\text{Pr}[A\cap B]$ . The way I approached it was to use conditional probability: $\text{Pr}[A\mid B]\cdot \text{Pr}[B]$ , splitting $\text{Pr}[A\mid B]$ Alice draws the same pair as Bob Alice draws a different pair For (1), I calculated: $\frac{2}{52} \cdot \frac{1}{52}$ , because Bob has already drawn the first 2 cards, so Alice has to get exactly the remaining 2 For(2), I calculated: $12 \cdot \frac{4}{52} \cdot \frac{3}{51}$ because there are 12 other possible pairs to make s.t. it is different from Bob's pair. Hence, I am left with: $$\text{Pr}[A\mid B]\cdot \text{Pr}[B]=\left(\frac{2}{52} \cdot \frac{1}{52} + 12 \cdot \frac{4}{52} \cdot \frac{3}{51}\right) \cdot \frac{3}{51}.$$ Unfortunately, this does not agree with the solution of $\frac{73}{20825}$ . I simply cannot find the mistake, help is greatly appreciated! Thanks!","['binomial-coefficients', 'combinatorics', 'probability']"
3319903,Generating function of ordered partitions,What is exactly generating function of ordered partitions and how can I get number of ordered partitions from that? Example: $$ 4 = 1+1+1+1 \\ = 2+2  \\ = 1+1+2 \\ = 1+2+1 \\ = 2+1+1 \\ = 1+3 \\ = 3+1 \\ = 4 $$ so we have $8$ partitions. I was thinking about exponential generating function: $$(1+t+\frac{t^2}{2!} + ...)(1+\frac{t^2}{2!} +\frac{t^4}{4!} +...)...(1+\frac{t^n}{n!} + ... ) = e^x e^{2x} e^{3x} \cdots e^{nx} = e^{n(n+1)/2} = \sum_{k \ge 0}\frac{\left(\frac{n(n+1)}{2}x\right)^k}{k!} $$ so the number of ordered partitions of $n$ is $$\left(\frac{n(n+1)}{2}\right)^n $$ bot for $n=4$ it is $$10^4$$ It seems to be completely wrong.,"['integer-partitions', 'combinatorics', 'generating-functions']"
3319927,Series of $\frac{1}{2}(e^{3x} - e^{2x} - e^x + 1)$,I am reading slides from my lecture and saw this equality: $$  \frac{1}{2}(e^{3x} - e^{2x} - e^x +   \color{red}{1}) = \sum_{r \ge 0}\frac{1}{2}(3^r-2^r-1) \frac{x^r}{r!} $$ but in my opinion it should be $$  \frac{1}{2}(e^{3x} - e^{2x} - e^x + 1) = \sum_{r \ge 0}\frac{1}{2}(3^r-2^r-1) \frac{x^r}{r!} + \color{red}{\frac{1}{2}} $$ where $  \color{red}{1}$ has been lost?,"['summation', 'sequences-and-series']"
3319932,Which elements are in $\mathcal{P}(\emptyset)\backslash \emptyset$,"We have $\mathcal{P}(\emptyset)\ = \{\emptyset\}$ . And $x \in \mathcal{P}(\emptyset)\backslash \emptyset$ means that $x \in \{\emptyset\} $ and $x \notin \emptyset$ . I think that $\emptyset \in \{\emptyset\}$ and $\emptyset \notin \emptyset$ . Hence $\mathcal{P}(\emptyset)\backslash \emptyset = \{\emptyset \}$ , but the solution for this task says that $|\mathcal{P}(\emptyset)\backslash \emptyset| = 0$ .",['elementary-set-theory']
3319948,Does $f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|}$ converge pointwise or uniformly?,"I want to check whether the sequence below converges pointwise or uniformly $$f_{n}(z):\{z\in\mathbb{C}:|z| = 1\}\to\mathbb{R}$$ $$\qquad\qquad f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|}$$ I have tried using the triangle inequality $||z|-|w||\leq|z-w|$ but that gives me $f_{n}(z)\leq 1$ . Intuitively, I don't even know whether the sequence is meant to converge pointwise because while $n^{2}\to\infty$ , $e^{in}$ is dense on the unit circle, so it seems to me that $|z-e^{in}|$ should be close to $0$ infinitely many times. How can I prove/disprove this sequence converges pointwise and/or uniformly?","['convergence-divergence', 'pointwise-convergence', 'uniform-convergence', 'real-analysis']"
3319950,"How ""visualize"" negative-dimensional space?","The concept of negative-dimensional space was introduced in many  branches of geometry (topology, algebraic-geometry, derived-geometry, fractals, etc)
for example: http://forthelukeofmath.com/documents/Wolcott-McTernan-workshop.pdf Or https://en.wikipedia.org/wiki/Negative-dimensional_space How ""visualize"" this? For example a (-2)-sphere","['dimension-theory-analysis', 'geometry', 'algebraic-topology']"
3319960,How to find the area of a region bounded by a simple closed curve?,"I have the following equation: $$
\frac{p}{(a-x)^2+y^2}+\frac{1-p}{(b-x)^2+y^2}=1 \text{ where } 0\leq p\leq 1
$$ Which represent a simple close curve. Obviously, when $p=0,p=1$ or $a=b$ we recover a unit circle. However the shape of the curve is more interesting in the other cases. Here is an example for $p=0.2$ , $a=1$ and $b=0$ : The area inside the curve is supposed to represent an uniform distribution, thus I suspect the area to be equal to one (unless I did a mistake!). However I don't know how to compute it. My attempt was to shift to polar coordinates and compute a double integral. However I have trouble determining the boundaries of these integrals. Any hints or general advice are appreciated !","['integration', 'curves', 'polar-coordinates', 'area']"
3320014,Limit distribution of Bernoulli variance,"Let $X_1 \dots X_n \sim B(1, p)$ be i.i.d. random variables. Then the variance of $X_i$ can be approximated through $Y_n = \bar{X}_n(1 - \bar{X}_n)$ . What is the limit distribution of $Y_n$ as $n \to \infty$ ? I tried to solve this as follows. Let $q = 1 - p$ , then from the CLT follows $$\sqrt{\frac{n}{pq}}(\bar{X}_n - p) \to Z \sim \text{N}(0, 1) \text{ as } n \to \infty$$ Through the delta method with $g(x) = x(1 - x)$ , we can conclude that $$ \sqrt{\frac{n}{pq}}(g(\bar{X}_n) - g(p)) \to g'(p) \cdot Z $$ or equivalently $$\sqrt{n}(Y_n - pq) \to \sqrt{pq}(q - p) \cdot Z \sim \text{N}(0, pq(q - p)^2) \text{ as } n \to \infty$$ And then I was unsure on how to proceed finding the limit distribution of $Y_n$ . The big problem, at least to me, seems to be how to eliminate the occurrence of $\sqrt{n}$ on the left. I had a bit of an ad-hoc idea but I'm not sure if it's justifiable. It goes as follows: Let $F_n : \mathbb{R} \to \mathbb{R}$ be the cdf and $P_n : \mathbb{R} \to \mathbb{R}$ the probability function of $Y_n$ , and $\phi : \mathbb{R} \to \mathbb{R}$ be the cdf of $Z \sim N(0, 1)$ . Then, using the definition of convergence by distribution, the previous result can be rewritten as follows. $$ \lim_{n \to \infty} P_n(\sqrt{n}(Y_n - pq) \le x) = \phi\left(\frac{x}{\sqrt{pq}(q-p)}\right)$$ or equivalently $$ \lim_{n \to \infty} P_n(Y_n \le \frac{x}{\sqrt{n}} + pq) = \phi\left(\frac{x}{\sqrt{pq}(q-p)}\right)$$ Letting $y = \frac{x}{\sqrt{n}} + pq$ we can conclude that \begin{align}
\lim_{n \to \infty} F_n(y) &= \lim_{n \to \infty} P_n(Y_n \le y) \\
&= \lim_{n \to \infty}\phi\left(\frac{\sqrt{n}y - pq}{\sqrt{pq}(q-p)}\right)
\end{align} If we denote the unknown limit distribution as $F : \mathbb{R} \to \mathbb{R}$ , then this means we can define it as $$
F(y) = \left\{\begin{array}{ll}
0 \quad &\text{ if } \frac{y}{p - q} < 0 \\
\phi\left(\frac{\sqrt{pq}}{p - q}\right) \quad &\text{ if } y = 0 \\
1 \quad &\text{ if } \frac{y}{p - q} > 0
\end{array}\right.
$$ I can already tell this argument fails specifically in the case when $p = 1/2$ , but disregarding that, does this argument make any sense?","['statistics', 'central-limit-theorem', 'probability-distributions', 'delta-method', 'random-variables']"
3320026,A set of Lebesgue measure zero,"I've been reading through some exercises and solutions and came across a student's argument that seemed rather odd, though it was crucial for the proof and his answer got a full mark. It was claimed that if $f \in {L_\infty }\left[ {0,1} \right]$ then the following set is of Lebesgue measure zero: $$E = \left\{ {x \in \left[ {0,1} \right]|f\left( x \right) \ge {{\left\| f \right\|}_\infty }} \right\}$$ I just figured that if $f\left( x \right) \equiv 1$ then $E = \left[ {0,1} \right]$ and then $m\left( E \right) = 1$ . So I'm quite baffled","['measure-theory', 'lebesgue-measure']"
3320030,Find the maximum angle $X$ in the range $0^\circ \leq x < 360^\circ$ which satisfies the equation $\cos^2(2x)+\sqrt{3}\sin(2x)-\frac{7}{4}=0$,"Find the maximum angle $X$ in the range $0^\circ \leq x < 360^\circ$ which satisfies the equation $\cos^2(2x)+\sqrt{3}\sin(2x)-\frac{7}{4}=0$ $\cos^2(2x)=\sin^2(2x)-1$ , so we can substitute $t$ for $\sin(2x)$ , and we have the equation $t^2+\sqrt{3}t-\frac{7}{4}-1=0$ . Solving for $t$ , $t$ is either $\frac{-\sqrt{3}+\sqrt{14}}{2}$ or $\frac{-\sqrt{3}-\sqrt{14}}{2}$ $x= \frac{\arcsin(t)}{2}$ How can I find the maximum angle $X$ ? Edit: $\cos^2(2x) = -\sin^2(2x)+1$ , not $\cos^2(2x)=\sin^2(2x)-1$",['trigonometry']
3320032,Recognizing an adjunction space,"I am currently studying Adjunction spaces using Brown's Topology and Groupoids . I am having trouble understanding exercise 4.5.3: Let $B$ be a closed subspace of $Q$ . For each $λ=1,\dots,n$ , let $f_λ:X_λ→Q$ be a map, and let $A_λ$ be a closed subspace of $X_λ$ such that $f_λ[A_λ]⊆B$ , $f_λ|X_λ\setminus A_λ$ is injective, the sets $f_λ[X_λ\setminus A_λ]$ are disjoint and cover $Q\setminus B$ , $f_λ|X_λ$ , $f_λ[X_λ]$ is an identification map. Prove that a function $g:Q→Y$ is continuous if and only if $g|B$ , $gf_λ$ is continuous, $λ=1,\dots,n$ . Prove also that there is a homeomorphism $Q→B_{f1}⊔X1\dots_{fn}⊔Xn$ which is the identity on $B$ . I tried to prove that $Q$ has the final topology with respect to the inclusion $i_B:B\rightarrow Q$ and the maps $f_\lambda$ . For that I take a subset $C$ in $Q$ whose inverse image for all these maps are closed, and I try to show that $C$ is closed. However, I am not able to prove it without adding the assumption that $f_\lambda[X_\lambda]$ is closed in $Q$ for each $\lambda$ , and I have the feeling that it is necessary. For example, take $Q=[0,2]$ and the closed subspace $B=[0,1]$ in $Q$ . Take $X=(0,1)\cup (1,2]$ and the closed subspace $A=(0,1)$ in $X$ . Let $f:X\rightarrow Q$ be the identity map. Then these spaces and maps satisfy the conditions above (do they, actually?). But the subset $C=(1,2]$ of $Q$ is not closed in $Q$ although $i_B^{-1}[C]$ is closed in $B$ and $f^{-1}[C]$ is closed in $X$ . I cannot find where is the mistake in this example. How can I solve this exercise without the additional assumption? Is my example wrong?",['general-topology']
3320041,Proving a Simple Differential Equality by Induction,"I would like to prove the following statement. For $\phi (r) \in C^{k+1}(\mathbb{R})$ , and $k \in \mathbb{N}$ : $(d_{r}^{2}) (\frac{1}{r} d_{r})^{k-1} (r^{2k - 1} \phi(r)) = (\frac{1}{r}d_{r})^{k} (r^{2k}d_{r} \phi(r))$ Equality was easy to show for the $k=1$ case. I would then like to assume true for $k = n-1$ case and show that the $k=n$ case follows. However, I have been unable to do this. Could someone please help me with the induction step? Here is my attempt: Assume true for $k = n-1, \ n \in \mathbb{N}$ . Consider the $k = n$ case. We can rewrite the left-hand side of the equality as follows: $(d_{r}^{2}) (\frac{1}{r} d_{r})^{n-1} (r^{2n - 1} \phi(r))$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} d_{r} [r^{2k - 1} \phi(r)])$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} [(2k - 1)r^{2k - 2} \phi(r) + r^{2k - 1} d_{r}\phi(r)])$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} [(2k - 1)r^{2k - 3} \phi(r) + r^{2k - 2} d_{r}\phi(r)]$ Since the $k = n-1$ case is true, we can say: $= (2k - 1) (\frac{1}{r} d_{r})^{k-1} (r^{2(k-1)} d_{r}\phi) + (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (r^{2(k-1)} d_{r} \phi)$ We now try to rewrite the right-hand side: $(\frac{1}{r} d_{r})^{k} (r^{2k} d_{r} \phi) $ $= (\frac{1}{r} d_{r})^{k-1} (\frac{1}{r} [2kr^{2k-1} d_{r}\phi + r^{2k} d_{r}^{2} \phi ] )$ $= (\frac{1}{r} d_{r})^{k-1} (2kr^{2k-2} d_{r}\phi + r^{2k - 1} d_{r}^{2} \phi) $ Unfortunately, this is about as far as I can go! I have tried manipulating these in slightly different ways, but I just can't seem to get the desired result using the $k = n-1$ case. Could someone try to fix/point out a mistake I've made?","['induction', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3320042,How does 'AND' distribute over 'OR' (Set Theory)?,"In my textbook, there is a solved example: Prove that $A \cup (B \cap C) = (A \cup B)\cap(A\cup C).$ Solution Let $x$ be an arbitrary element of $A \cup (B\cap C)$ . Then, $$
\begin{aligned}
&x\in A\cup (B\cap C)\\
\implies &x \in A \lor (x\in B\cap C)\\
\implies &x \in A \lor (x\in B \land x\in C)\\
\implies &(x \in A \lor x\in B) \land (x\in A \lor x\in C)\\
\implies &x\in(A \cup B) \land x\in(A\cup C)\\
\implies &x\in((A\cup B)\cap(A\cup C))\\
\therefore\ A\cup(B\cap C) \subseteq (A\cup B)\cap (A\cup C)
\end{aligned}
$$ Similarly, $(A\cup B)\cap (A\cup C) \subseteq A\cup(B\cap C)$ . Hence, $A\cup (B\cap C) = (A\cup B)\cap(A\cup C)$ . The book didn't prove $(A\cup B)\cap (A\cup C) \subseteq A\cup(B\cap C)$ . So, I tried to do it: Let $y$ be an arbitrary element of $(A\cup B)\cap(A\cup C)$ . Then, $$
\begin{aligned}
&y\in(A\cup B) \land y\in(A\cup C)\\
\implies &(y\in A \lor y\in B) \land (y\in A \lor y\in C)\\
\implies &((y\in A \lor y\in B)\land y\in A) \lor ((y\in A \lor y\in B)\land y\in C)\\
\implies &((y\in A \land y\in A)\lor (y\in B\land y\in A))\lor ((y\in A \land y\in C) \lor (y\in B \land y\in C))
\end{aligned}
$$ I don't know how to proceed further. There could be a better way to prove this, but I just want to simplify this expression into something that could enable me to solve the problem.","['elementary-set-theory', 'logic']"
3320102,"Limit by polar coordinates $\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0$","I need to demonstrate the following limit $$\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0$$ By polar coordinates. I apply the substitution $x=1+\rho\cos(\theta)$ , $y=\rho\sin(\theta)$ . $$\lim_{\rho\to0} \frac{\rho^2\sin^2(\theta)\log(1+\rho\cos(\theta))}{\rho^2}=\lim_{\rho\to0} \sin^2(\theta)\log(1+\rho\cos(\theta))$$ Here I have some issues with the following steps. $$|\sin^2(\theta)\log(1+\rho\cos(\theta))|\leq\rho|\sin^2(\theta)\cos(\theta)|=\rho\to0$$ I'm not sure about if that step is correct $$|\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta)$$ My thought process was that I observed that $$0\leq|\log(x)|\leq x-1$$ for $x\ge1$ . Thus, by setting $t=x-1 \to x=t+1$ , I conclude that $$0\leq|\log(t+1)|\leq t$$ For $t\ge0$ . Hence since $\cos(\theta)$ is a bounded value I conclude that $$|\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta)$$ For $\rho\ge0$ . And with this inequality I conclude that the limit is $0$ as I've show above. Is the demonstration correct?","['multivariable-calculus', 'limits', 'calculus', 'proof-verification']"
3320192,How do I evaluate $\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right)$ without using L'Hopital or series?,"How do I evaluate $\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right)$ without using L'Hopital or series? I've tried expanding the variable such as $x = 2y$ or $x = 3y$ , but seemed to still get stuck.","['limits', 'calculus', 'limits-without-lhopital']"
3320193,"If given $P(B\mid A) =4/5$, $P(B\mid A^\complement)= 2/5$ and $P(B)= 1/2$, what is the probability of $A$?","If given $P(B\mid A) =4/5$ , $P(B\mid A^\complement)= 2/5$ and $P(B)= 1/2$ , what is the probability of $A$ ? I know I need to apply Bayes theorem here to figure this out, but I'm struggling a bit to understand how. So far I've considered this formula: $$P(B\mid A) = \dfrac{P (B \cap A) }{ P (B \cap A) + P(B^\complement \cap A)}$$ From this formula, I understand that $P(B \cap A) = P(A) \cdot P(B\mid A)$ so I plug in the given values but then only find that $P(B^\complement |A)$ is $2/25$ . But this does not get me any closer to my goal, $P(A)$ . I imagine my understanding of this is quite backward. Any pointers would be helpful. Thank you","['conditional-probability', 'bayes-theorem', 'probability']"
3320274,What is the length of pair of wires after twisting them around each other?,"Given two wires each of diameter D and length L, what will be the new length after you twist the wires around each other 1 Turn? I mean that when you start with the pair of straight wires and put them against a scale, the length is L. When you twist them around each other tightly, and place the straightened twisted wire against the scale, its new length will be reduced. And what I mean by 1 Turn is that looking at the cross-section, if you begin with the Red wire on top, Black at bottom then after half Turn the Red wire is at bottom and after another half Turn (or Total 1 Turn) the Red is on top again. And both wires are twisting in same direction, say clockwise, when looking at the cross-section. Successive cross-sections will look like the number 8 rotating. I suppose we should also give the pitch? But when you play with an actual mouse wire, twisting it around itself, you will realize that you can not reduce the pitch beyond a certain extent. So what is that minimum pitch when twisting the wires? What is causing that restriction? Can it be quantified? Perhaps the question is not too clear, so please feel free to edit/clarify.",['geometry']
3320316,"How can I show that $\int_0^T f(s)dW_s\sim \mathcal N\left(0, \int_0^T f(s)^2ds\right)$?","Let $f$ a determinisitic functio in $L^2(0,T)$ . How can I show that $$\int_0^T f(s)dW_s\sim \mathcal N\left(0,\int_0^T f(s)^2ds\right) \ \ ?$$ First of all, it's clear that $$\mathbb E\int_0^T f(s)dW_s=0$$ and $$\mathbb E\left(\int_0^T f(s)dW_s\right)^2=\int_0^T f(s)^2ds.$$ So, if $\int_0^T f(s)dW_s$ is normally distributed, then the claim follow. To prove it's normally distributed, I tried to use Itô formula with $g(x,t)=xf(t)$ . This gives (as far as $f$ derivable) $$W_tf(t)=\int_0^t W_s f'(s)ds+\int_0^t f(s)dW_s,$$ but unfortunately, I can't conclude. Any idea ?","['stochastic-integrals', 'normal-distribution', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3320317,Noise Bottleneck of Nassim Taleb,"In several of Nassim Taleb's books he mentions a phenomenon referred to as a noise bottleneck where more sampling of something actually decreases your signal to noise relationship: ""Assume further that for what you are observing, at a yearly frequency, the ratio of signal to noise is about one to one (half noise, half signal)—this means that about half the changes are real improvements or degradations, the other half come from randomness. This ratio is what you get from yearly observations. But if you look at the very same data on a daily basis, the composition would change to 95 percent noise, 5 percent signal. And if you observe data on an hourly basis, as people immersed in the news and market price variations do, the split becomes 99.5 percent noise to 0.5 percent signal."" - Antifragile pg. 126 All of the situations I can think of have the opposite effect, where longer sampling times (more data) average out the noise and improve your signal to noise ratio.  Can anyone give a mathematical example of this noise bottleneck?","['statistics', 'probability']"
3320339,constant evaluation when using differential equations.,"This is regards to constant evaluation when using differential equations. A solution is given to be: $$y=(e^{2x}+e^x )  \ln⁡(1+e^{-x} )-(c_1+1) e^x+(c_2-1) e^{2x}$$ A simplified solution in an answer book is given as: $$y=(e^{2x}+e^x )  \ln⁡(1+e^{-x} )+(c_1 ) e^x+(c_2 ) e^{2x}$$ There is a change in sign of $c_1$ in the third term. $C_1$ is a constant and not specified to be positive or negative or is it supposed to be positive and that information is simply not specified. I never know how to interpret this kind of results. Can someone explain, please? Thank you. Sincerely,
Mary A. Marion",['ordinary-differential-equations']
3320371,Proposition 3.6 part 1 - Do Carmo's Riemannian Geometry,"Proposition 3.6 Let $p \in \mathcal{M}$ , $U$ a normal neighborhood of $p$ , $B \subset U$ a normal ball of center $p$ . Let $\gamma : [0,1] \to B$ be a geodesic segment with $\gamma(0) = p$ . If $c : [0,1]\to \mathcal{M}$ is any piecewise differentiable curve joining $\gamma(0)$ to $\gamma(1)$ then $l(\gamma) \leq l(c)$ and if equality holds then $\gamma([0,1]) = c([0,1])$ . For the proof Suppose initially that $c([0,1]) \subset B$ . Since $\exp_p$ is a diffeomorphism on $U$ , the curve $c(t)$ , for $t \neq 0$ , can be written uniquely as $\exp_p (r(t) \cdot v(t)) = f(r(t),t)$ where $t \to v(t)$ is a curve in $T_p\mathcal{M}$ with $\left| v(t) \right| = 1$ and $r : (0,1] \to \mathbb{R}$ is a positive piecewise differentiable function. The first question is why can $c(t)$ be written in such a way?
My first guess is because in $B$ by definition of exponential map there's a unique geodesic such that $$
\exp_p(v) = \alpha(1,p,v)
$$ And I can write $v$ in the form stated in the theorem and the equation $$
c(t) = \exp_p(v)
$$ Is well defined. Carrying on with the prof It follows that, except for a finite number of points, $$
\frac{dc}{dt} = \frac{\partial f}{\partial r} r'(t) + \frac{\partial f}{\partial t}
$$ I think it's clear the rule applied is essentially the chain rule. However I do struggle to derive the formula using the definition of differentials in manifolds (rigorously). My attempt was to decompose $f$ as $$
t \to (r(t),t) \to f(r(t),t)
$$ So $f = f_2 \circ f_1$ , where $f_1 : \mathbb{R} \to \mathbb{R}^2$ and $f_2 : \mathbb{R}^2 \to \mathcal{M}$ , for $f_1$ the differential is simply a derivative wrt $t$ componentwise. For $f_2$ I would assume I can use $$
d f_{2_{(r,t)}} = \left[ d f_{2_{(r,t)}} \left( \frac{\partial}{\partial r} \right) \; d f_{2_{(r,t)}} \left(\frac{\partial}{\partial t} \right) \right]
$$ And using the author notation I'll end up with the same expression, the question is whether my derivation is correct. Finally From the Gauss lemma, $\left\langle \frac{\partial f}{\partial r}, \frac{\partial f}{\partial t} \right\rangle = 0$ I'd assume from the Gauss Lemma more specifically it follows that $$\left\langle \frac{\partial f}{\partial r}, \frac{\partial f}{\partial t} \right\rangle = \left\langle \frac{\partial}{\partial r}, \frac{\partial}{\partial t} \right\rangle$$ And the two vectors $\frac{\partial}{\partial r}$ and $\frac{\partial}{\partial t}$ and these are orthogonal. Again, is this correct?
Very last bit Since $\left| \frac{\partial f}{\partial r} \right| = 1$ , ...
  I'm not quite sure I understand why is 1, could you explain? I'm still reading through the rest of the proof it seems ok, but I'll potentially ask a different question. Also just to clarify shouldn't the expression $$
\frac{dc}{dt} = \frac{\partial f}{\partial r} r'(t) + \frac{\partial f}{\partial t}
$$ actually be $$
\frac{dc}{dt} = r'(t) \frac{\partial f}{\partial r} + \frac{\partial f}{\partial t}
$$ Mostly because $r'(t)$ is considered an element of a scalar field, while $\frac{\partial f}{\partial r}$ is a vector. Thank you","['manifolds', 'riemannian-geometry', 'differential-geometry']"
3320454,"Question about ""Approaching Zero and Limits"" in the Intuitive Proof of the Derivative of Sine","I'm a high school student hoping to self-study some introductory calculus over the summer. While studying, I came across this intuitive proof of the derivative of the sine function, using trig and the unit circle... As in the picture, as dθ approaches zero, angles A and B will approach 90° -- allowing triangle ABC to be ""approaching"" similar to triangle BDE, but this would mean you would never get the exact angles for triangle ABC; thus, never the exact ratio of sine and cosine in order to complete the proof. Is this small (even negligible) inaccuracy inherent to calculus, or is there a flaw in my understanding? P.S. Please forgive me if this is a stupid/far-too-basic question.","['calculus', 'definition', 'algebra-precalculus']"
3320485,Question about a corollary to the theorem on the dimension of fibres.,"The highlighted sections are taken  from Volume 1 of ""Basic Algebraic Geometry"" by Igor Shafarevich. Theorem : Let $ f : X \rightarrow Y $ be a regular map between irreducible varieties. Suppose that $ f $ is surjective, and that $ \text{dim}(X) = n, \text{dim}(Y) = m. $ Then $ m \leq n, $ and $ \text{dim}(F) \geq n-m $ for any $ y \in Y $ and for any component $ F $ of the fibre $ f^{-1}(y). $ there exists a nonempty open subset $ U \subset Y $ such that $ \text{dim}f^{-1}(y) = n-m $ for $ y \in U. $ Corollary: The sets $ Y_{k} = \lbrace y \in Y \; | \text{dim}f^{-1}(y) \geq k \rbrace $ are closed in $ Y. $ The proof of the corollary is as follows(paraphrasing): We know that $ Y_{n-m} = Y, $ and there is some closed subset $ Y' \subsetneqq Y $ such that $ Y_{k} \subset Y' $ if $ k > n-m. $ It is clear that $ Y_{n-m} = Y. $ I'm not entirely sure why there must exist such a closed set $ Y' $ however. The proof continues: If $ Z_{i} $ are the irreducible components of $ Y' $ and $ f_{i}: f^{-1}(Z_{i}) \rightarrow Z_{i} $ the restrictions of $ f $ to $ Z_{i}. $ Then $ \text{dim}(Z_{i}) < \text{dim}(Y), $ and we prove the corollary by induction on $ \text{dim}Y. $ I don't see why $ \text{dim}(Z_{i}) < \text{dim}(Y). $ Why is it not possible for $ \text{dim}(Z_{i}) = \text{dim}(Y)? $ I'm also unclear on how the induction is done.",['algebraic-geometry']
3320495,Existence of weak limit of measures,"Suppose $\{\mu_n\}$ is a sequence of Borel probability measures on $\mathbb{C}$ such that $$
\lim_{n \to \infty} \int fd\mu_n
$$ exists for all $f \in C_b(\mathbb{C}, \mathbb{R})$ . Is it true that there exists a measure $\mu$ such that $$
\int f d\mu = \lim_{n \to \infty} \int fd\mu_n?
$$ In other words, is there a measure $\mu$ such that $\mu_n \to \mu$ weakly? In my problem, we can assume the support of the $\mu_n$ is contained in the unit ball, if it helps.","['measure-theory', 'probability-theory', 'analysis']"
3320509,Two matrices that are not similar have (almost) same eigenvalues,"I have two matrices $$
A=\begin{pmatrix} 
a & 0 & 0  \\
0 & b & 0  \\
0 & 0 & c 
\end{pmatrix}
\quad
\text{  and  }
\quad
B=\begin{pmatrix} 
d & e & f  \\
d & e & f  \\
d & e & f 
\end{pmatrix}
$$ In reality mine are more like 1000 x 1000 matrices but the only thing that is important for now is that the left matrix is diagonal and the right one has one row that repeats itself. Obviously the eigenvalues of the left matrix are its diagonal components. I want to create a new matrix C $$C = A+B=\begin{pmatrix} 
a & 0 & 0  \\0 & b & 0  \\0 & 0 & c \end{pmatrix}+\begin{pmatrix} d & e & f \\d & e & f  \\d & e & f \end{pmatrix}=\begin{pmatrix} a+d & e & f  \\d & b+e & f  \\d & e & c+f \end{pmatrix}$$ I am now wondering how the eigenvalues of this new matrix C are related to the eigenvalues of the diagonal matrix A. Can I use an argument that uses row reduction in order to relate the eigenvalues of both matrices? The reason why I am asking is that my 1000 x 1000 matrix (implemented in mathematica) that is described as above gives me almost the same eigenvalues as the corresponding diagonal matrix (only a few eigenvalues differ) and I really cannot think of any reason why that should be the case. EDIT: I implemented a simple code in mathematica to illustrate what I mean. One can see that every eigenvalue of the diagonal matrix A appears in C: dim = 50;

    A = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];

    mat = RandomReal[{0, 100}, {1, dim}];
    B = ArrayFlatten[ConstantArray[{mat}, dim]];

    c = A + B;

    Abs[Eigenvalues[A]]
    Round[Abs[Eigenvalues[c]], 0.01]

    (*{10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 7, 7, 7, 7, 7, 
    6, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 
    1, 1, 1, 0, 0, 0}*)

    (*{2084.89, 10., 10., 10., 10., 10., 9.71, 9., 9., 9., 9., 9., 8.54, 
    8., 8., 8., 7.72, 7., 7., 7., 7., 6.61, 6., 6., 6., 5.44, 5., 5., 5., 
    5., 4.29, 4., 4., 4., 3.51, 3., 3., 3., 3., 2.28, 2., 2., 2., 2., 
    1.21, 1., 1., 0.33, 0., 0.}*)","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3320520,Some elementary properties of Formal Power Series from Cartan' book,"I'm referring to Cartan's ""Elementary Theory of Analytic Functions of One or Several Complex Variables"". Recall that the order of a formal series $\sum_{n\ge0}a_nX^n$ is the smallest (if it exists) index $n\ge 0$ s.t. $a_n\ne0$ ; otherwise, the order is infinity, by convention. Definition (p.11) : A family $(S_i(X))_{i\in I}$ of series is said to be summable iff for any integer $k$ , only a finite number of series of the family have order $< k$ . In this case, the sum is defined to be the series $$S(X)=\sum_{i\in I}S_i(X)=\sum_{n\ge 0}(\sum_{i\in I}a_{n,i})X^n$$ which makes sense because the family is summable. Two questions: (Q1) : the author claims (pp.11) that this ""generalized addition is commutative and associative in a sense which the reader should specify"". I'm looking for a ""sense"", but I'm not sure at all what the author means. The main problem, at least for me, is that commutativity and associativity are properties defined for operations on some set, e.g. $+\colon \Bbb{R}\times \Bbb{R}\to \Bbb{R}$ . But the generalized sum takes possibly infinitely many arguments.
Thus, how should those properties look like in this generalized situation? (Q2) : For a formal series $S(X)=\sum_{n\ge0}a_nX^n$ , define the formal derivative to be $S'(X)=\sum_{n\ge0}na_nX^{n-1}$ . Clearly, we get a linear map $S\mapsto S'$ . The author claims (pp.15) that a Leibniz-type rule holds, i.e. $(ST)'=S'T+ST'$ , by saying that ""it is sufficient to verify this formula in the particular case when $S,T$ are monomials, and it is clearly true then.""
Here, I'm not sure I understood the meaning of ""clearly"": I think it is a consequence of the fact that two power series are equal iff have the same coefficients of $X^n$ for every $n\ge0$ .
But for fixed $n\ge0$ , we have to prove equality of products of polynomial, which follows from linearity and what we have already proved for monomials.
Is this reasoning right or not? Thank you in advance for your help.","['complex-analysis', 'power-series']"
3320522,De Montmort's Matching Problem strategy,"Context: Introduction to Probability -  Hwang and Blitzstein Pg. 24. Example 1.6.4 de Montmort's matching problem.
The problem states: Consider a well shuffled deck of n cards labeled 1 through n. You flip over the cards one by one, saying the numbers 1 through n as you do so. You win the game if, at some point, the number you say aloud is the same as the number on the number on the card being flipped over (for example, if the 7th card in the deck has the label 7). What is the probability of winning? My Questions: Part 1: The problem does not state that the player keeps playing the game even after they win the game. The solution uses the inclusion-exclusion principle to calculate the probability of winning. For this, the authors calculate the probability of event $A_i$ that the $i$ -th flipped card has the number $i$ on it, then the probability that $A_i \cap A_j$ occur and so on and string them together using inclusion-exclusion. Why is $A_i \cap A_j$ important at all? would not the game end as soon as $A_i$ occurs? Part 2: Here is my approach to calculate the solution, and I would like to understand what is wrong with it (and in-case nothing is wrong then how to develop it further): $P(\text{win}) = P(\text{win on 1st card}) + P(\text{win on 2nd card}) + ... + P(\text{win on the n-th card}) = \cup_{\forall i} P(A_i | \cap_{\forall j<i} A(j))$ In the above equation I assume that winning on the 1st card is disjoint with winning on the second card and so on.","['derangements', 'combinatorics', 'probability-theory', 'discrete-mathematics']"
3320573,What is distance of ship from coastline(AB) if it is 167° and 205° from A and B respectively?,"Two points A and B on a straight coastline are 1km apart, B being due east of A. If a ship is observed on bearings 167° and 205° from A and B respectively, what is its distance from the coastline? Any tips? I ended up with at triangle containing the degrees 142°, 13° and 25° and then using the sin rule to find sides BC and AC. But it already feels like I'm going in the wrong direction since I'd have to get the perpendicular of C which would make two triangles and then use pythagoras AND simultaenous equations. Seems wrong.",['trigonometry']
3320709,$f$ is injective and $f(U) \subset f(V)$ then $U \subset V$?,I wonder the question If $f$ is injective and $f(U) \subset f(V)$ then can I conclude $U \subset V$ ?,['elementary-set-theory']
3320725,Squeeze Theorem to evaluate the limit of $\arctan(x)$ as $x$ approaches $0$.,I need to use $$\frac{x}{1+x^2} < \arctan(x) < x$$ to evaluate the limit of $\arctan(x)$ as $x$ approaches $0$ . I have attempted to take the limit as $x$ approaches $0$ for each expression in the equality. So $$\lim_{x\to 0}\frac{x}{1+x^2} < \lim_{x\to 0} \arctan(x)) < \lim_{x\to 0} x$$ as $x$ approaches $0$ but it just leave me with $$0 < \lim_{x\to 0} \arctan(x) < 0$$ which doesn't make sense. What do I do?,"['calculus', 'trigonometry']"
3320733,The extreme points of $\overline{\mathrm{conv}(A)}$ are in $\overline{A}$,"Let $A$ be a subset of $\mathbb{R}^n$ and let $e$ be an extreme point of $\overline{\mathrm{conv}(A)}$ .
Prove that $e \in \overline{A}$ . The problem is intuitive, but I cannot prove it.
Thanks in advance for any help!","['convex-analysis', 'geometry', 'analysis', 'convex-hulls']"
3320753,Show that $n^4-6n^3+11n^2-6n$ is divisible by $4$ for every integer $n$.,"So I am trying to refresh my memory when it comes to modular arithmetic but I am having some difficulties. The problem as states in the title is: Show that $n^{4}-6n^{3}+11n^{2}-6n$ is divisible by $4$ för every
  integer $n$ . So what I first did was reformulating the problem in terms of congruences, in the following way: $$4\vert (n^{4}-6n^{3}+11n^{2}-6n) \iff  4\vert (n^{4}+11n^{2}-(6n^{3}+6n))\\ \hspace{6cm} \iff  n^{4}+11n^{2}\equiv 6n^{3}+6n\pmod 4$$ Now, one initial question I have is if it is correct that the only cases I need to check is: $$n^{4}+11n^{2}\equiv 0\equiv 6n^{3}+6n\pmod 4 \\ n^{4}+11n^{2}\equiv 1\equiv 6n^{3}+6n\pmod 4\\ n^{4}+11n^{2}\equiv 2\equiv 6n^{3}+6n\pmod 4\\n^{4}+11n^{2}\equiv 3\equiv 6n^{3}+6n\pmod 4,$$ since $0,1,2,3$ are the possible remainders when considering division by $4$ ? If this is true, it still seams like a really tedious approach to solving the problem. So I am wondering if there is some other way of solving it? Thanks in advance.","['number-theory', 'proof-verification', 'modular-arithmetic', 'divisibility']"
3320770,"Given an arbitrary ordered field (F,<), is there always a sequence of strictly positive elements of F that tends to zero?","Let $(F,<)$ be an ordered field. Is there always a sequence $(a_n)_{n\in\mathbb N}$ of strictly positive elements of $F$ such that $\lim_{n\to\infty}a_n=0$ ? (To define limits, I'm using the topology induced by the total ordering of $F$ ). If the field is archimedean, than setting $a_n=\frac{1}{n+1}~\forall n\in\mathbb N$ will work just fine. However, if the field is non-archimedean, then there is a strictly positive $\epsilon\in F$ such that $\forall n\in\mathbb N:\epsilon<\frac{1}{n+1}$ , thus the sequence defined above is not eventually in the interval $(-\epsilon,\epsilon)$ , and it does not approach zero. So the problem seems to be the existence of infinitesimal elements.","['field-theory', 'ordered-fields', 'sequences-and-series']"
3320804,"Find all $(x,y)$ of positive integers s.t. $x^3-y^3=xy+61$ [duplicate]","This question already has answers here : Solving $x^3-y^3=xy+61$ in integers (3 answers) Closed 4 years ago . Given $x^3-y^3=xy+61$ .
Find all pairs (x,y) of positive integers which satisfies the given equation. I tried solving this by following method- $(x-y)(x^2+y^2+xy)-xy=61$ $(x-y)[(x-y)^2+3xy]-xy=61$ $(x-y)^3+3xy(x-y)-xy=61$ I don't know if doing this was in any means fruitful or not. Remember $x>y$ , only then a solution is possible.",['number-theory']
3320826,"Equivalence of Continuous Monotonic Functions $[0, 1] \to [0, 1]$","A couple of later clarifications to my original post ... the monotonic non-decreasing functions mapping $[0, 1] \to [0, 1]$ are surjective - it appears from the comments that this was misunderstood. Prof. Raussen has kindly assisted me in understanding the proofs in the paper, and I will at some point post an answer giving the proof. In outline, the result depends on three others...... a) Given a countable set of points in [0, 1] - ""stop values"" - one can construct an element of $\mathscr M$ with non-trivial (closed) intervals - ""stop intervals"" - that map to these points. b) an element of $\mathscr M$ has at most countably many stop values. c) If the stop values of $\phi \subset $ stop values of $\eta$ then there is $\psi$ such that $\eta = \phi \circ \psi$ (all functions being elements of $\mathscr M$ ). Let $\mathscr M$ be the set of continuous monotonic non-decreasing functions mapping $[0, 1] \to [0, 1]$ . Then under the operation of composition $\mathscr M$ is a monoid - a group without inverse: it is easily seen that it is closed, associative and has an identity ( $i: [0, 1] \to [0, 1], i(t) = t$ ). One also sees that all such functions are surjective (e.g. intermediate value theorem). In the paper ""Reparametrizations of continuous paths - Ulrich Fahrenberg and Martin Raussen"" https://arxiv.org/pdf/0706.3560.pdf it seems to be proven that given any $f, g \in \mathscr M$ there are $\mu, \nu \in \mathscr M$ such that $f \circ \mu = g \circ \nu$ . I must admit the paper is a little beyond me, and I'm looking for a simple proof for this . Background. The paper shows among other things that re-parameterisation of paths is an equivalence relation, which then formalizes the definition of a curve as an equivalence class of paths. Showing symmetry and reflexivity is easy - the proof above is needed to show transitivity. (The paper also shows that every path is equivalent to a regular path - i.e. one which does not ""stop"" at any point. An alternate proof for this can be found here https://math.stackexchange.com/q/3317511 .)","['alternative-proof', 'curves', 'general-topology', 'real-analysis']"
3320837,"Representations of $\mathfrak{sl}(3,\mathbb{C})$ and the symmetric group","let $V$ be the fundamental 3 dimensional representation of $\mathfrak{sl}(3,\mathbb{C})$ and consider the product $V^{\otimes N}$ . The action of any representation $\rho$ of $\mathfrak{sl}(3,\mathbb{C})$ commutes with the action of the symmetric group $S_N$ that permutes the vector in the tensor product. I read that this implies the Schur Weyl duality $$V^{\otimes N}=\bigoplus_{\lambda}V_\lambda\otimes S_\lambda $$ where $V_\lambda$ are irreducible representations of $\mathfrak{sl}(3,\mathbb{C})$ and $S_\lambda$ are irreducible representations of $S_N$ . I'm confused by how this decomposition works, usually the following example is provided $$V^{\otimes 2}=S^2V\oplus\Lambda^2V $$ where $S^2V$ is the symmetric part of the tensor product and $\Lambda^2V$ the antisymmetric part. I don't see the promised decomposition in this example, I see a direct sum of two spaces that are irreducible representations of both $\mathfrak{sl}(3,\mathbb{C})$ and $S_2$ , not a direct sum of tensor products of representations of $\mathfrak{sl}(3,\mathbb{C})$ and $S_2$ . Could somebody help me understand some concrete examples of this decomposition?","['symmetric-groups', 'group-theory', 'representation-theory', 'lie-algebras']"
3320890,Help me out with the sum $\sum_{n= 0}^{N-1} \frac{ \left(a-b \cos{\left(\frac{2 \pi n}{N} \right)} \right)^2}{a^2 + b^2 -2ab\cos{\frac{2\pi n}{N}}}$,"I am trying to find an analytical expression for the summation below $$
\sum_{n= 0}^{N-1} \frac{ \left(a-b \cos{\left(\frac{2 \pi n}{N} \right)} \right)^2}{a^2 + b^2 -2ab\cos{\frac{2\pi n}{N}}}
$$ with $a>b$ . By trying it in MATLAB, I found that for large $N$ it gives a number which is
proportional to $N$ . Therefore I am certain there must be analytical solution for this summation. Can anyone help out? Thank you","['summation', 'sequences-and-series']"
3320892,Higher powers of a matrix's relation with its trace,"Let $A=[a_{ij}]$ , where $a_{ij}=u_{i}v_{j}, 1 \leq i \leq n$ and $1\leq j \leq n$ and $u_i,v_j$ belong to $R$ satisfies $A^5=16A$ . Find trace(A). I denoted U as a column matrix having values u1,u2,...,un. And V a row matrix having values v1,v2,...,vn. So that A=UV. But I am not able to proceed further. Evaluating A^5 would be very tedious so I think that I am missing the trick in this question. Also backtracking from the answer, I feel $A^5=(trace(A))^4A$ . Is there any easy way to prove this?","['matrices', 'linear-algebra']"
3320911,Rationality of $Y^2Z = X^3 + X^2Z$,"I came across the following example when reading  Silverman: Example 3.7 Let $V$ be the variety $$V : Y^2Z = X^3 + X^2Z$$ and consider the rational maps $$\psi : \mathbb{P}_1 → V, \psi = [(S^2 − T^2)T, (S^2 − T^2)S, T^3],$$ $$\varphi : V → \mathbb{P}_1, \varphi= [Y,X].$$ Here $\psi$ is a morphism, while $\varphi$ is not regular at $[0, 0, 1]$ . Not coincidentally, the
point $[0, 0, 1]$ is a singular point of $V$ ; see (II.2.1). We emphasize that although the
compositions $\varphi \circ \psi$ and $\psi \circ \varphi$ are the identity map wherever they are defined, the
maps $\varphi$ and $\psi$ are not isomorphisms, because $\varphi$ is not a morphism. It seems to me that although $\varphi$ is not a morphism, the two maps defined above imply that $\mathbb{P}_1$ is birational to $V$ (since they define isomorphisms  between $\mathbb{P}_1$ and $V\[0,0,1]$ ). On the other hand, $K(\mathbb{P}_1)=K(X)$ is purely transcendental, but $K(V)\cong K(X, \sqrt{X^3+X^2})$ is not, so $\mathbb{P}_1$ cannot be birational to $V$ . I would appreciate it if someone could point out the mistake in my argument. Thanks!","['algebraic-geometry', 'birational-geometry']"
3320950,"Integrating $\int \frac{-\sin x}{1+\cos x}\, dx$, I get $\ln(1 + \cos x)$. WolframAlpha gives $2 \ln(\cos \frac x 2)$. Is WA wrong?","So, I'm watching a tutorial on differential equations, where I encountered this little trick: $$\int \frac{y'}{y}\, dx = \ln(y)$$ It seems perfectly logical and easy to justify, but something fishy happens to this integral: $$\int \frac{-\sin x}{1+\cos x}\, dx$$ The trick gives $\int \frac{-\sin x}{1+\cos x}\, dx = \ln(1 + \cos x)$ while WolframAlpha gives $\int \frac{-\sin x}{1+\cos x}\, dx = 2 \ln(\cos \frac x 2)$ . You guys who know this stuff - does WolframAlpha mess up here or is it something I've missed? Taking the derivative of $2 \ln(\cos \frac x 2)$ gives me $-\tan \frac x 2$ , so I don't see how WA may be right.","['integration', 'derivatives', 'ordinary-differential-equations']"
3321004,Second Kummer Function,"Background: I am trying to solve the radial Schroedinger equation in the form: \begin{align}
\frac{\partial^2 P}{\partial r^2} + 2 \left(E + \frac{Z}{r} - \frac{l(l+1)}{2r^2}\right) P = 0
\end{align} Where $E = \frac{1}{2n^2}$ , $n = 1,2,3, \dotsc$ , $Z=1$ and $l = 0,1,2, \dotsc, n-1$ . In the derivation of the radial equation one finds that the solutions can have the following asymptotic behavior: \begin{align}
\lim_{r \rightarrow 0} P(r) &= \begin{cases} r^{-l} \\
r^{l+1}
\end{cases} \\
\lim_{r\rightarrow \infty} P(r) &= \begin{cases} e^{\lambda r} \\
e^{-\lambda r}
\end{cases}, \lambda = \sqrt{-2E} = \frac{1}{n}
\end{align} Question: To find the physical (regular) solution, which is normalizable, one usually proceeds with the ansatz $P(r) = r^{l+1} e^{-\lambda r} F(r)$ , $\lambda = \sqrt{-2E} = \frac{1}{n}$ . This leads to a Kummer differential equation for F(z): \begin{align}
&z F''(z) + (c-z)F'(z) - a F(z) = 0\\ &a = (l+1)-n, c = 2(l+1), z=\frac{2}{n} r  
\end{align} Solutions to this equation are called confluent hypergeometric functions or Kummer functions of the first M(a,c,z) and second kind U(a,c,z) (which is sometimes also referred to as Tricomi's functions). I am concerned with the irregular solution, which should not be normalizable at it diverges at the origin (and for large r). However, the chosen ansatz leads to negative or zero parameter $a$ . From my understanding, for example the irregular solution corresponding to the 1s-orbital $(n=1, l=0)$ should be expressible as $Q(r) := r  e^{-r}  U(0, 2, 2r)$ . But $U(0, 2, 2r)  = 1$ . Hence, this is not a linearly independent solution to $X(r) = r e^{-r} M(0,2,2r) = r e^{-r}$ . How can one construct the second linear independent solution to this parameters of the Kummer equation? I thought the choice of the ansatz, using the asymptotic behaviour, should be arbitrary (despite a particular choice might be advantageous) as in the end there is a second order ode which has two solutions. Other choices lead to equally problematic (?!) parameters, except the choice $P(r) = r^{l+1} e^{-\lambda r} F(r)$ , for which one gets $a=n+l+1$ , $b=2(l+1)$ and $z = -\frac{2}{n}r$ . For this choice I found a irregular solution, however, the negative argument is kind of problematic. Despite not being implemented in a lot of numerical packages to evaluate the U function, consulting Wolfram Alpha or for example the mpmath package in python the U function is imaginary in the regime of interest (1s: $U(2,2,-2) \approx  -0.3 + i 1.155$ ). I consulted Morse & Feshbach [1] for an expression, but the derivation of a formula for integer $c$ starts with formula (5.3.59) under the restriction $0<\phi<\pi$ which I interpreted as being defined as $z=|z| e^{i\phi}$ and, hence, the exclusion of negative arugments (?!). The formula in the end leads to something similar to DLMF 13.2.9. where I can not find such restrictions. What is the right expression for irregular function U(a,c,z) in this parameter regime of the Kummer equation? Why is it complex for integer a (I suspect the logarithm) and how does it satisfy the Wronskian $PQ' - P' Q = 1$ with this non zero imaginary part (I did a short test and it seemed not to hold, but maybe I was not careful enough)?","['complex-analysis', 'wronskian', 'hypergeometric-function']"
3321025,What p.d.f. over angles is equivalent to a uniform distribution over a hypersphere?,"Eric Weisstein's Sphere Point Picking points out that sampling uniformly from each angle $\phi$ and $\theta$ in spherical coordinates does not sample from the uniform sphere because it clusters near the poles. I am interested in which distribution over the angles does sample uniformly over the area element. For the spherical case, he notes that the random variables $\phi$ and $\theta$ that do correspond to sampling from the uniform sphere are: $\theta = 2\pi u \\
\phi = \cos^{-1}(2v -1)$ where $u$ and $v$ are random variables uniformly distributed over [0, 1]. I would like to know how this extends to n-dimensional hyperspheres. Is there a similar expression for the distribution of the angles $\boldsymbol{\theta}$ when sampling from a uniform hypersphere? Very grateful for any help! (I'm aware that there are simpler ways to sample from the unit hypersphere such as this . I'm specifically interested in the probability density function of the angles.)","['probability-theory', 'statistics', 'geometry', 'probability']"
3321058,A particular vanishing integral,"While dealing with a definite integral on AoPS I discovered (I have to admit by pure chance) the following relation $$\int_0^1\log\left(\frac{(x+1)(x+2)}{x+3}\right)\frac{\mathrm dx}{1+x}~=~0\tag1$$ The proof is quite easy, but feels kind of contrived. Indeed, just apply a self-similar substitution - $x\mapsto\frac{1-x}{1+x}$ - to the auxiliary integral $\mathcal I$ given by $$\mathcal I=\int_0^1\log\left(\frac{x^2+2x+3}{(x+1)(x+2)}\right)\frac{\mathrm dx}{1+x}.$$ And the result follows. However, to consider precisely this integral seems highly unnatural to me (in fact, as I mentioned earlier, this integral was just a by-product while evaluating something quite different and I discovered $(1)$ when experimenting with various substitutions). The crucial point to notice concerning $\mathcal I$ is the invariance of the polynomial $f(x)=x^2+2x+3$ regarding the self-similar substitution which allows us to deduce $(1)$ . Additionally for myself I am quite surprised by the special structure of $(1)$ since we have factors of the form $(x+1)$ , $(x+2)$ and $(x+3)$ combined which calls for a generalization (although I found none yet). It there a more elementary approach, not relying on such an ""accident"" like examining the integral $\mathcal I$ for proving $(1)$ ? Additionally, can this particular pattern be further generalized? Answers to both questions (also separately) are highly appreciated! Thanks in advance!","['integration', 'calculus', 'definite-integrals']"
3321082,List of definitions of the various types of ODE stability?,"I am studying numerical methods for solving ordinary differential equations (ODEs) and I keep on coming across different kinds of stability. I'm struggling to find the common thread between them. I was hoping that someone who knows this topic better than I do could draft a list of different kinds of stability of numerical ODE solvers along with their definitions. I'm guessing some are synonyms of each other. And others may be relevant only to the model problem. I'm just looking to discriminate which are which. Examples of types of stability I've come across include the following. However, this list is probably not exhaustive. Stability Zero-Stability A-Stability Absolute Stability Relative Stability Weak Stability Thank you!","['stability-in-odes', 'numerical-methods', 'stability-theory', 'ordinary-differential-equations']"
3321100,"Prove that convex real functions on [0,1] form a meet-semilattice","Let C be the set of all continuous strictly convex real valued functions on the interval [0,1]. For f,g $\in$ C, define f $\leq$ g IFF f(x) $\leq$ g(x) for all x $\in$ [0,1]. Prove this is a meet-semmilattice, but not a join-semilattice. My intuition at first was that this is meet-semilattice because a linear function, which is convex and concave, would be the GLB and there is not LUB for all subsets because I could define an infinite amount of functions to have a greater slope, so there would be a lattice with a linear function on the bottom with an infinite amount of functions above it growing infinitely upward. but the problem calls for strictly convex functions, so I'm not sure that linear functions are allowed. I don't really know how to approach this or even if I'm thinking about it correctly. This is an exercise from Gratzer's Lattice Theory: First Concepts and Distributive Lattices in the first chapter. I'm working through it on my own, it's not a homework assignment or anything. If it wouldn't be too much of a burden, I'd rather someone gently point me in the right direction to think about the problem rather than outright solve it for me. Thank you!","['order-theory', 'functions', 'convex-analysis', 'lattice-orders']"
3321139,"How to split D to calculate $\int_Df(x,y)d(x,y)$ for $f(x,y)=xe^{x^2+y^2}$","Let $$D=\{(x,y)\in \mathbb{R}^2: 1\le x^2+y^2 \le 4 ,\quad y\ge0\}$$ Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined as $$f(x,y)=xe^{x^2+y^2}$$ Calculate $\int_Df(x,y)d(x,y)$ I believe the order of integration must be $dxdy$ . However, I can't clearly express $x$ as a function of $y$ . What is the correct way to split the set $D$ so we can integrate?","['integration', 'multivariable-calculus', 'calculus']"
3321172,Is there a matrix that can be used to find the transpose of a matrix?,"Let $A$ be a general $n\times n$ invertible matrix. Let $T^A$ be the ""transposer"" matrix i.e. $T^A A = A'$ . (Does that $T^A$ multiplied by $A$ equal the transpose of $A$ ?) Then does $T^A$ depend on the matrix $A$ : is $T^A = T^B$ for all invertible Matrices? Prove your claim. So can you take a matrix times the given matrix in order to find its transpose? If so, is that a general form that can be used for all invertible matrices? 
I'm guessing it does not, but I am not totally sure or know how to go about proving that and I cannot find anything online about it.","['matrices', 'linear-algebra']"
3321194,Soft Question: Textbook for first graduate course in Functional Analysis,"I will be studying Functional Analysis for the first time, at the first-year grad school level.  My relevant background is $1$ basic course in Topology and $1$ basic course in Measure Theory. I was given some recommendations for what textbooks may be suitable to a person in my position (sufficient detail to learn at the graduate level, but easy enough for a complete beginner to hit the ground running), and I now have to make a decision that will haunt me for a semester. My question is Based on your experience, can you give me any insight into the pros and cons of any of these books : W. Rudin, Functional Analysis G.K. Pedersen, Analysis Now R.J. Zimmer, Essential results of functional analysis J.B. Conway, A course in functional analysis C.D. Aliprantis, K. Border, Infinite Dimensional Analysis: A Hitchhiker's Guide (a comprehensive text, but don't feel intimidated) B. Simon, Convexity: An analytic approach (as you can guess by the title, not exactly what we are doing but this text is an excellent supplement)","['book-recommendation', 'soft-question', 'functional-analysis']"
3321212,Finding the Gradient Matrix for the given expression,"Let $\rho$ be a matrix, and let $\rho_A$ be the partial trace of the matrix $\rho$ . For simplicity, let us assume $\rho$ is a $4 \times 4$ matrix. The partial trace is defined as follows: If $$\rho = \begin{bmatrix}a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p \end{bmatrix}$$ Then $$\rho_A = \begin{bmatrix} a+f & c+h \\i+n & k + p \end{bmatrix}$$ I would like to calculate the following: $$\nabla_\rho (Tr(\rho log \rho) - Tr(\rho_A log \rho_A))$$ which is the derivative of a continuous scalar quantity with respect to a matrix, so I should be able to calculate it right? Note that each of the entries are complex. I tried a few things, but I'm not sure whether chain rule works, because you end up getting gradients of matrices with respect to matrices, which is not defined? Any help on how to go about such a thing would be very appreciated.","['matrices', 'matrix-calculus', 'linear-algebra', 'quantum-information', 'derivatives']"
3321216,How to find the coordinate of $B'$ and $C'$ in an efficient way?,"where $B'C'$ is parallel to $BC$ and the distance between them is $d$ . I have the coordinate of $A, B,$ and $C$ and the value of $d$ ;","['triangles', 'geometry']"
3321219,"Integrate ${\int\sqrt{1 + \sin\frac{x}2}\,\mathrm{d}x}$","So I was doing a integral question and I stumbled upon this question. $\displaystyle{\int\sqrt{1 + \sin\left(\frac x2\right)}\,dx}$ In order to solve it I did the following: I took $u = \frac12x$ Then $\frac {du}{dx}$ Which gave me $2 du = dx$ . After that I substituted u in the equation to get $2\int \sqrt{1 + \sin(u)} du$ . After this I was stuck as I am new to integration of trigonometry so I checked my textbook which did the same just the same but the step after this was this one $2\int{\sqrt{\sin^2 \frac12u + \cos^2\frac12u + 2\sin \frac 12u\cos \frac12u}\text{ du}}$ I am in a complete awe how the textbook got $2\int{\sqrt{\sin^2 \frac12u + \cos^2\frac12u + 2\sin \frac 12u\cos \frac12u}\text{ du}}$ from $2\int \sqrt{1 + \sin(u)} \text{ du} $ Can someone please explain me how the this is achieved? I am totally stuck","['integration', 'indefinite-integrals', 'calculus', 'analysis']"
3321235,"Two random variables $U$, $V$ such that the conditional distribution of $U$ given $V = v$ is $ve^{-u}$ for $u > ln(v)$. Prove the following:","Given two random variables $U$ and $V$ such that the conditional PDF of $U$ given $V = v$ is $ve^{-u}$ for $u > ln(v)$ , prove $U - V$ conditional on $V = v$ follows $Exp(1)$ . Intuitively, why is this true? My attempt: Let $Z = U - V$ . $P(Z <= z | V = v) = P(U - v <= z | V = v) = P(U <= z + v | V = v)$ Differentiating with respect to $z$ , we see that the desired PDF is $ve^{-(z+v)}$ . But this is not an exponential distribution with parameter $1$ .. what have I done wrong?","['conditional-probability', 'statistics', 'proof-verification', 'probability-distributions']"
3321265,Prove the zeros of a polynomial all lie in an annulus.,"I am working on a problem. It has two parts: (a) Let $c_{0}>c_{1}>\cdots c_{n}>0$ . Show that the polynomial $P(z):=c_{0}+c_{1}z+\cdots+c_{n}z^{n}$ has no zeros inside the closed unit disc. (b) Show that the zeros of polynomial $P_{n}(z):=1+\frac{z}{2}+\frac{z^{2}}{3}+\cdots+\frac{z^{n}}{n+1}$ all lie in an annulus $\{1<|z|<1+\delta_{n}\}$ where $\delta_{n}\rightarrow 0$ as $n\rightarrow\infty$ . I've proved part (a), but I am stuck in part (b). I think the proof of part (b) may be similar to part (a), so I state part (a) above and give my proof below, then I will give my attempt for part (b). Part (a): Suppose there exists $z_{0}\in\mathbb{C}$ such that $P(z_{0})=0$ and $|z_{0}|<1$ . Since $P(z_{0})=0$ , we also have $$(1-z_{0})P(z_{0})=0, $$ where $LHS=c_{0}+(c_{1}-c_{0})z_{0}+(c_{2}-c_{1})z_{0}^{2}+\cdots+ (c_{n}-c_{n-1})z_{0}^{n}-c_{n}z_{0}^{n+1}.$ Thus, we have $$c_{0}=(c_{0}-c_{1})z_{0}+(c_{1}-c_{2})z_{0}^{2}+\cdots (c_{n-1}-c_{n})z_{0}^{n}+c_{n}z_{0}^{n+1}.$$ Now, taking norm to both side, and recalling that $c_{0}>c_{1}>\cdots>c_{n}>0$ and $|z_{0}|<1$ , we have \begin{align*}
c_{0}&<c_{0}-c_{1}+c_{1}-c_{2}+\cdots+c_{n-1}-c_{n}+c_{n}\\
&=c_{0}
\end{align*} which is a contradiction. Thus, there is no zero of $P(z)$ that is inside the closed unit disc. Part (b): For part (b), I mimic what I've done in part (a). Let $z\in\mathbb{C}$ be a zero of $P_{n}(z)$ , then $P_{n}(z)=0$ implies that $$(1-z)P_{n}(z)=0.$$ Thus, we have $$\Big(1-\dfrac{z^{n+1}}{n+1}\Big)-\dfrac{z}{2}-\dfrac{z^{2}}{6}-\cdots-\dfrac{z^{n}}{n(n+1)}=0.$$ Set $Q_{n}(z):=-\dfrac{z}{2}-\dfrac{z^{2}}{6}-\cdots-\dfrac{z^{n}}{n(n+1)}.$ Then if $|z|<1$ , we have \begin{align*}
1&\leq|Q_{n}(z)|+\Big|\dfrac{z^{n+1}}{n+1}\Big|\\
&\leq\dfrac{|z|}{2}+\dfrac{|z|^{2}}{6}+\cdots+\dfrac{|z|^{n}}{n(n+1)}+\dfrac{|z|^{n+1}}{n+1}\\
&<\dfrac{1}{2}+\dfrac{1}{6}+\cdots+\dfrac{1}{n(n+1)}+\dfrac{1}{n+1}\\
&=1-\dfrac{1}{2}+\dfrac{1}{2}-\dfrac{1}{3}+\cdots+\dfrac{1}{n}-\dfrac{1}{n+1}+\dfrac{1}{n+1}\\
&=1,
\end{align*} which is a contradiction. Thus, zeros of $P_{n}(z)$ must lie in $|z|\geq 1$ . Then, I try to get rid of $|z|=1$ by using the same techniques, but I found something else interesting. If $|z|=1$ , then by definition $$|Q_{n}(z)|\leq 1-\dfrac{1}{2}+\dfrac{1}{2}-\dfrac{1}{3}+\cdots+\dfrac{1}{n}-\dfrac{1}{n+1}=\dfrac{n}{n+1}.$$ On the other hand, since we assume $z$ is a zero, we have $$|Q_{n}(z)|=\Big|1-\dfrac{z^{n+1}}{n+1}\Big|\geq \Big|1-\dfrac{1}{n+1}\Big|=\dfrac{n}{n+1}.$$ Thus, if $|z|=1$ , we have $$\dfrac{n}{n+1}\leq |Q_{n}(z)|\leq\dfrac{n}{n+1},$$ and thus $$|Q_{n}(z)|=\dfrac{n}{n+1}.$$ This did not give me any contradiction, but an idea of $\delta_{n}$ . By the problem itself, we can see that if $\delta_{n}\rightarrow 0$ , then the annulus will become to a unit circle $|z|=1$ , which is exactly our case. Also, by my argument above, I want my $\delta_{n}$ to be related to $|Q_{n}(z)|$ , so I tried to set $$1+\delta_{n}=\dfrac{n}{n+1},$$ which gives us $$\delta_{n}=-\dfrac{1}{n+1},$$ which tends to be $0$ as $n\rightarrow\infty$ . But then I don't know how to proceed, what should I do now? Thank you!","['complex-analysis', 'proof-verification']"
3321289,Intuition for non-convergence of Cauchy sequence in $\mathbb{Q}$,"Suppose we were standing on the rational line at the point 3. Then we took a step to the point 3.1, then to 3.14, etc. (Cauchy sequence of decimal approximations of $\pi$ ). Suppose, also, that it takes us $\frac{1}{2^n}$ seconds to take the n'th step (so that we'll have ""completed"" the process after 1 second). Given that this sequence obviously doesn't converge in $\mathbb{Q}$ , where would we be after continuing this process for 1 second? I get this is kind of a weird question to ask, but I feel like it's hard for me to intuitively grasp what it means for a Cauchy sequence to not converge (since it feels like it is going somewhere).","['general-topology', 'rational-numbers', 'cauchy-sequences', 'real-analysis']"
3321317,"If $a + b = \frac{1}{4}$, what is $a^3 + b^3$?","I just want to know how to solve this problem. If $a + b = \dfrac{1}{4}$ , what is $a^3 + b^3$ equals to? My work - but still doesn't give me an answer. $a^2 + b^2 = (a + b)^2 - 2ab = \dfrac{1}{16} - 2ab$ $a^3 + b^3 = (a + b)^3 - 3ab(a + b) = \dfrac{1}{64} - \dfrac{3ab}{4}$ . How can I get the value for $ab$ here?",['algebra-precalculus']
3321361,Why is flipping a head then a tail a different outcome than flipping a tail then a head?,"In either case, one coin flip resulted in a head and the other resulted in a tail. Why is {H,T} a different outcome than {T,H}? Is this simply how we've defined an ""outcome"" in probability? My main problem with {H,T} being a different outcome than {T,H} is that we apply binomial coefficients (i.e. we count subsets of sets) in some common probability problems. But if we take {H,T} and {T,H} to be different outcomes, then our ""sets"" are ordered, but sets are by definition unordered... I feel as though the fact that I'm confused about something so basic means that I am missing something fundamental. Any help or insight whatsoever is greatly appreciated!","['discrete-mathematics', 'binomial-coefficients', 'combinatorics', 'probability']"
3321377,Law of the iterated logarithm in higher dimensions,"The law of the iterated logarithm states that $\limsup_{t \rightarrow 0} \frac{B_t}{\sqrt{2t\log \log 1/t}} = 1$ almost surely, and has the natural generalization to $d$ -dimensions that $$\limsup_{t \rightarrow 0} \frac{|B_t|}{\sqrt{2t\log \log 1/t}} = 1$$ almost surely, but I'm having a hard time finding a proof of this.  Everywhere that I've looked has simply said that it's an easy generalization, so it seems like there has to be some trick that's way easier than what I've done.  I have so far that $$\limsup_{t \rightarrow 0} \frac{|B_t|}{\sqrt{2t\log \log 1/t}} \ge \limsup_{t \rightarrow 0} \frac{|B_t^{(1)}|}{\sqrt{2t\log \log 1/t}} = 1,$$ but showing the reverse inequality is somewhat harder.  Based on a hint in Revuz and Yor I've tried using the fact that we can find a sequence $(e_n)$ in $\mathbb{R}^d$ with $|e_n|=1$ such that $|B_t| = \sup_{n} \langle e_n,B_t \rangle$ , so we need to show $$\limsup_{t \rightarrow 0}\left( \sup_{n \in \mathbb{N}} \frac{\langle e_n, B_t \rangle}{\sqrt{2t\log \log 1/t}}\right) \le 1.$$ Since $\langle e_n,B_t \rangle$ is a 1-dimensional brownian motion for each $n$ by Levy's characterization, we have that $\limsup_{t \rightarrow 0} \frac{\langle e_n B_t\rangle}{\sqrt{2t\log \log 1/t}} =1$ for each $n$ almost surely, but I don't think this is enough to get the result because there's no clear way to interchange taking the limsup over $t$ and the sup over $n$ . Is this the right idea so far?  If so, how do you finish from here?  And is there a simpler way to do this proof?","['stochastic-processes', 'brownian-motion', 'probability-theory']"
3321398,Graphs for which a calculus student can reasonably compute the arclength,"Given a differentiable real-valued function $f$ , the arclength of its graph on $[a,b]$ is given by $$\int_a^b\sqrt{1+\left(f'(x)\right)^2}\,\mathrm{d}x$$ For many choices of $f$ this can be a tricky integral to evaluate, especially for calculus students first learning integration. I've found a few choices of $f$ that make the computation pretty easy: Letting $f$ be linear is super easy, but then you don't even need the formula. Taking $f$ of the form $(\text{stuff})^{\frac{3}{2}}$ might work out nicely if $\text{stuff}$ is chosen carefully. Calculating it for $f(x) = \sqrt{1-x^2}$ is alright if you remember that $\int\frac{1}{x^2+1}\,\mathrm{d}x$ is $\arctan(x)+C$ . Letting $f(x) = \ln(\sec(x))$ results in $\int\sec(x)\,\mathrm{d}x$ , which classically sucks. But it looks like most choices of $f$ suggest at least a trig substitution $f'(x) \mapsto \tan(\theta)$ , and will be computationally intensive, and unreasonable to ask a student to do. Are there other examples of a function $f$ such that computing the arclength of the graph of $f$ won't be too arduous to ask a calculus student to do?","['integration', 'arc-length', 'big-list', 'calculus', 'education']"
3321399,"$\lim\limits_{n \to \infty} \cos^2\left(\pi \sqrt[3]{n^3+n^2+2n}\right) $, where $n \in \mathbb{N}$","$$
\lim\limits_{n \to \infty} \cos^2\left(\pi \sqrt[3]{n^3+n^2+2n}\right)
$$ where $n \in \mathbb{N}$ . In this question, what I thought was, since $n \to \infty$ and $\cos ^2x$ is periodic , all I need is actually the fractional part of this. And it easy to say that $n+1> \sqrt[3]{n^3+n^2+2n}> n$ . But evaluation of $\sqrt[3]{n^3+n^2+2n} - n$ , is getting tricky. I'm sure there must be a short way to solve it. Can someone help me with it?","['limits', 'calculus']"
3321413,what is formula for $\sum_{i=0}^{b-1} (-1)^{i}(b-i)^{m}\binom{b-1}i$ and Real Roots Polynomials,"Definition let $b$ and $m$ are non negative integers with $b\ge 1$ $$D_{m,b}=\sum_{i=0}^{b-1} (-1)^{i}(b-i)^{m}\binom{b-1}i$$ I observed $$D_{m,m+1}=m!$$ $$D_{m,m}=(m+1)!/2$$ $$D_{m,m-1}= (m+1)!(3m-2)/24$$ $$D_{m,m-2}= (m+1)!(m-1)(m-2)/48$$ $$\dots$$ what is general formula for $D_{m,m-k}$ ? Let define $F_k=D_{m,m-k}=0$ where $k\in \mathbb{N}$ Example $F_1 = 3m-2 = 0  \implies m = 2/3$ $F_2 = (m-1)(m-2) = 0  \implies m = 2,1$ Prove or disprove $F_k \implies m \in \mathbb{R} \ \ \forall k$","['number-theory', 'combinatorics']"
3321429,Find total area of circles within polygon,"Let's say I have a polygon defined by a set of coordinates. I also have the radius and (x, y) coordinates of the centre of $n$ circles. I am looking for a function to find the total area of the circles within the polygon, i.e if there are circle segments outside of the polygon, they are not counted towards the area. My initial thought was to simply find the area of union of the circles, but I am stuck in subtracting the areas of the segments outside of the main polygon. I know I can find the area using the shoelace method, but I am unable to see how this will assist me in getting a result. Here's an image to demonstrate - I want to find the area of the green space, using only the labelled points and distances as input:","['intersection-theory', 'circles', 'geometry', 'polygons']"
3321442,Evaluating an integral with divergence theorem,Evaluate the integral $\int_S(x^2+y^2)dS$ where S is the unit sphere in $\mathbb{R}^3$ I'm being tripped up when the question asks me to evaluate this integral with the divergence theorem because I keep getting $2\pi/3$ but I should get $8\pi/3$ since I got that for the integral.,"['multivariable-calculus', 'calculus']"
3321446,Find all entire functions $f$ satisfying $\Re(f(z))\leq\frac{2}{|z|}$ whenever $|z|>1\iff \frac{1}{|z|}< 1$,"I've been working in the following: Find all the entire function $f$ satisfying $$\Re (f(z))\leq \frac{2}{|z|}\quad\textrm{  whenever } \quad|z|>1\iff \frac{1}{|z|}< 1$$ Solution: Suppose that $f$ is entire with under these conditions. First, $\Re f\leq 2$ . Second, consider $g(z)=\exp(f(z))$ , then $g$ is entire too and $$
|g(z)|=|\exp(f(z))|=\exp(\Re(f(z)))\leq e^2.
$$ Hence, by  Liouville’s theorem, $g(z)=C$ . 
Finally, we observe $$g(z)=\exp(f(z))\Longrightarrow g'(z)=f'(z)\exp(f(z))\Longrightarrow 0=f'(z)$$ meaning that $f(z)$ must be a constant. I just wonder if I miss steps or I'm wrong. Thanks",['complex-analysis']
3321455,Do Carmo differential geometry book: section 2.2 proposition 4 proof clarification,"I am confused about this proposition: Prop: Let $p \in S$ be a point of a regular surface $S$ , and let ${\bf x}: U \subset R^2 \rightarrow R^3$ be a map with $p \in x(U)$ such that the conditions 1 and 3 of the definition 1 hold. Assume $x$ is one to one. Then ${\bf x}^{-1}$ is continuous. Definition 1 is: A subset $S \subset R^2$ is a regular surface if, for each $p \in S$ , there exists a neighborhood $V$ in $R^3$ and a map ${\bf x}: U \rightarrow V \cap S$ of an open set $U$ in $R^2$ such that: 1) ${\bf x}$ is differentiable. 2) ${\bf x}$ is a homeomorphism. 3) For each $q \in U$ , the differential $d{\bf x}_q : R^2 \rightarrow R^3$ is one to one. Essentially they claim that the continuity of the inverse map is automatic once other conditions are satisfied once we know that $S$ is already a surface. But I do not see how they make use of the fact that $S$ is a surface anywhere in the proof. Also in their proof, they show that each point in the image of ${\bf x}$ has a set around it (no claim on it being open) on which the inverse function is continuous, and conclude that it is enough.  Is it? EDIT : (adding the actual proof they present) Write ${\bf x}(u, v) = (x(u, v), y(u, v), z(u, v))$ , for $(u, v) \in U$ , and let $q \in U$ .
  By conditions 1 and 3, we can assume, interchanging the coordinate axis
  if necessary, that $(\partial(x, y)/\partial(u, v))(q) \neq 0$ . Let $\pi: R^3 → R^2$ be the projection $\pi(x, y, z) = (x, y)$ . By the inverse function theorem, we obtain neighborhoods $V_1$ of $q$ in $U$ and $V_2$ of $\pi ◦ {\bf x}(q)$ in $R^2$ such that $\pi ◦ {\bf x}$ maps $V_1$ diffeomorphically onto $V_2$ . Assume now that ${\bf x}$ is one-to-one. Then, restricted to ${\bf x}(V_1)$ , ${\bf x}^{−1} = (\pi ◦ {\bf x})^{−1} \circ \pi$ . Thus ${\bf x}^{−1}$ , as a composition of continuous maps, is continuous. Since $q$ is arbitrary, ${\bf x}^{-1}$ is continuous in ${\bf x}(U)$ .","['surfaces', 'differential-geometry']"
3321456,How can I Solve $y^{\prime\prime}+\frac2{x}y^{\prime}+y^3=0$?,"The equation below is a non-linear second order differential equation. $$y^{\prime\prime}+\dfrac{2}{x}y^{\prime}+y^3=0$$ I don't know how can I solve it?
Would somebody help me?",['ordinary-differential-equations']
3321478,Evaluating $\lim_{k\to\infty}\sum_{n=1}^{\infty} \frac{\sin\left(\pi n/k\right)}{n}$,"Recently, I was asked by a friend to compute the limit of the following series $$\displaystyle{\lim_{k\to\infty}}\sum_{n=1}^{\infty} \frac{\sin\left(\frac{\pi n}{k}\right)}{n}$$ Having seen a similar problem to this before, Difficult infinite trigonometric series , I used the same complex argument approach as seen in that problem. Ultimately, for this problem, I obtained $\frac{\pi}{2}$ as my answer. However, this limit can also be interpreted as a Riemann Sum, except the answer to the Riemann Sum differs from what I obtained as my answer, and according to Wolfram Alpha, the answer is expressed in terms of $Si$ , where $Si$ is the sine integral. I'm wondering, does the limit invalidate the argument approach, or is there something else I'm missing, because this limit if I'm not mistaken is a Riemann Sum after all?","['riemann-sum', 'sequences-and-series', 'real-analysis']"
