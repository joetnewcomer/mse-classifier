question_id,title,body,tags
4023109,$\lim_{n \rightarrow \infty}\left(1+\dfrac{2}{n}\right)^{n^2}e^{-2n}$ [duplicate],This question already has answers here : $\lim_{n\to\infty} \left(1+\frac{2}{n}\right)^{n^2} e^{-2n}$ (2 answers) Closed 3 years ago . $\lim_{n \rightarrow \infty}\left(1+\dfrac{2}{n}\right)^{n^2}e^{-2n}$ $\lim_{n \rightarrow \infty}\left(e^{-2}\left(1+\dfrac{2}{n}\right)^n\right)^{n}$ It is indeterminate form $1^{\infty}$ I solve this like this $e^{\lim_{n \rightarrow \infty}}\frac{\ln(e^{-2}(1+\frac{2}{n})^n)}{\frac{1}{n}}$ I can't apply L'Hôpital's rule now how can I solve this problem quickly?,['limits']
4023134,"Proof $gcd(2^{2x},2x)=2gcd(2^{x},x)$","How do I proof $\operatorname{gcd}(2^{2x},2x)=2\operatorname{gcd}(2^{x},x)$ , I have managed to proof $\operatorname{gcd}(2^{2x},2x)=2\operatorname{gcd}(2^{2x-1},x)$ but haven't managed to get anywhere form there.","['number-theory', 'gcd-and-lcm', 'functions', 'exponential-function']"
4023172,Prove that $f(x)>f(y)$ if $x>y$,"I am working on Spivaks calculus book and have the following question: We have a function with properties (Chapter 3, Ex. 17, (d)): $$f(x+y)=f(x)+f(y)$$ $$f(xy)=f(x)f(y)$$ and we are trying to prove that $f(x)=x$ for $x$ a real number.
In this step (d) we want to prove that $f(x)>f(y)$ if $x>y$ and here I am struggling. My attempt was to do the following: $x>y$ is the same as $x-y>0$ .
Then $$f(x-y) = f(x)+f(-y) = f(x)+f(-1)f(y)=f(x)+f(y)$$ because from (a) we know that $$f(1)=1=f((-1)(-1))=f(-1)^2$$ therefore $$f(-1)=1$$ From (c) we know that $f(x)>0 $ if $x>0$ so $$f(x-y)=f(x)+f(y)>0$$ but i want $f(x)-f(y)>0$ .
The solution manual says instantly $f(x-y)=f(x)-f(y)$ , but why is that? And what is wrong in my reasoning?",['functions']
4023263,Given that $\cos\left(\dfrac{2\pi m}{n}\right) \in \mathbb{Q}$ prove $\cos\left(\dfrac{2\pi}{n}\right) \in \mathbb{Q}$,"Given that $\cos\left(\dfrac{2\pi m}{n}\right) \in \mathbb{Q}$ , $\gcd(m,n) = 1$ , $m \in \mathbb{Z}, \, n \in \mathbb{N}$ prove that $\cos\left(\dfrac{2\pi}{n}\right) \in \mathbb{Q}.$ I know nothing about how to attack the problem. I believe I need to suppose that $\cos\left(\dfrac{2\pi}{n}\right) \not \in \mathbb{Q}$ and somehow show that $\cos\left(\dfrac{2\pi m}{n}\right) \not \in \mathbb{Q}$ what would have been a contradiction. Could you give me some hints?","['trigonometry', 'abstract-algebra', 'rational-numbers']"
4023287,Does this differential equation have a unique solution?,"Consider the Initial Value Problem $\dfrac{dy}{dx} = \sqrt{y+a}$ where $a > 0$ and $y(0) = 0$ Does this have a unique solution ? If I try to solve it : $\dfrac{dy}{\sqrt{y+a}} = dx$ gives, $2\sqrt{y+a} = x + c  \ldots (1)$ Now, here using Initial Condition $2\sqrt{y+a} = x + 2\sqrt{a}$ or, $y+a = (x+ 2\sqrt{a})^2$ But. in $(1)$ if I simplify the equation $y+a= (\dfrac{x}{2} + \dfrac{c}{2})^2$ and here via initial condition we get two values $c = 2\sqrt{a}$ and $c = -2\sqrt{a}$ and thus this has two different solutions. So, can anyone explain does this Differential Equation has unique solution or multiple solutions ? Thank you.","['ordinary-differential-equations', 'real-analysis']"
4023298,$A \cup C \sim B \cup C$ with $A \cap C = \emptyset$ and $B \cap C = \emptyset$ such that $A \sim B?$,"Let $A,B$ and $C$ be sets. Suppose that $A \cup C \sim B \cup C$ and that $A \cap C = \emptyset$ and $B \cap C = \emptyset$ . Is it necessarily the case that $A \sim B?$ Give a proof or counterexample.
Where $\sim$ means bijection. Actually the reciprocal is true and I already tried it, but my intuition tells me that this is not true. I would like to know if I am wrong or the counterexample is very exotic. Thanks",['elementary-set-theory']
4023352,Confusion about simple probability,"This Question stems from Ronald Fishers Tea problem in how to design experiments which I have reworded. Anyway, suppose you have 2 different items in front of you, 5 of each which we can just call red and blue. So you have 10 items total,
and you want to randomly select the 5 red ones. The probability of getting this correct is $\frac{1}{10 \choose 5} $ but I can't shake the intuition that it would be $ \frac{5}{10} $ and I can't reason with myself as to why it wouldn't be. Can someone explain intuitively and not mathematically why it wouldn't be because I am able to conclude mathematically that it's not.","['intuition', 'combinatorics', 'probability']"
4023484,"Are there ""close"" solutions to Hilbert's third problem?","Hilbert's third problem (or a modern formulation thereof) asks whether two polyhedra $P,Q$ of equal volume are equidecomposable by cutting $P$ into finitely many polyhedral pieces and rearranging them to obtain $Q$ . (In the two-dimensional case, this can always be done with polygons of equal area.) With the use of Dehn invariants , one can show that this is impossible; however, the proof focuses on specific equivalence classes of dihedral angles, and does not seem to rule out ""approximate"" solutions. To be more precise: Given polyhedra $P,Q$ of identical volume, here are some notions of a ""close"" solution to Hilbert's third problem: For all $\epsilon>0$ , $P$ may be cut into finitely many polyhedra which can be reassembled to form a polyhedron which contains a copy of $Q$ scaled down by $1-\epsilon$ and is contained in a copy of $Q$ scaled up by $1+\epsilon$ . For all $\epsilon>0$ , $P$ may be cut into finitely many polyhedra which can be reassembled to form a polyhedron whose boundary differs from that of $Q$ over a region of surface area less than $\epsilon$ . For all $\epsilon>0$ and points $x$ on the boundary of $Q$ , $P$ may be cut into finitely many polyhedra which can be reassembled to form a polyhedron which is identical to $Q$ except within a ball of radius $\epsilon$ centered at $x$ . The first condition is easily satisfied by chopping up $P$ into very very small cubes; I do not see how to show the latter two conditions, though I expect they are probably true. Is it known whether dissections meeting the latter two conditions can always be performed?","['dissection', 'polyhedra', 'geometry', 'reference-request']"
4023501,Does this weighted sum of Gaussians converge to $1/x^2$?,"By coincidence I discovered that a weighted sum of several Gaussians seems to converge to $1/x^2$ . Now I wonder whether that's a well-known property, just a coincidence or if this could be proven. This is a normal Gaussian with an additional weight factor $1/\sigma$ : $$
f(x, \sigma) = \frac{1}{\sigma} \cdot \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2}
$$ If I now accumulate several of these weighted Gaussians with exponentially increasing $\sigma$ , I get a curve which closely resembles $1/x^2$ . The additional factor $\sqrt{3}$ was determined by trial and error and may not be the correct value. $$
g(x, n) = \sqrt{3} \cdot \sum_{k=0}^n f\left(x, \frac{2^k}{n}\right)
$$ $$
\lim\limits_{n \to \infty} g(x, n) \stackrel{?}{=} \frac{1}{x^2}
$$ Does anybody have an idea how this could be proven?","['limits', 'convergence-divergence', 'gaussian']"
4023528,How do I compute the number of intersection points of these three surfaces in $\mathbb A^3$,"The setting (ambient space) for this question is the projective space $\mathbb P^3$ over an algebraically closed field $k$ , with homogeneous coordinates $x,y,z,w$ . We choose $w = 0$ as the plane at infinity. Let $P, Q, R$ be surfaces of degree $d+1$ in $\mathbb P^3$ , whose set-theoretic intersection is the union of a finite number of points in $\mathbb A^3$ and a curve $C$ of degree $d$ on the plane at infinity. Assume none of $P, Q, R$ contains the plane at infinity. My goal is to find the number of intersection points in $\mathbb A^3$ , counted with multiplicity. I expect this number to be exactly $$d^3 + d^2 + d + 1 = (d^2 + 1)(d + 1),$$ and this can be proven to be the case when $k = \mathbb C$ , using complex analytic methods. We shall identify $P, Q, R$ with their defining polynomials in $k[x,y,z,w]$ . Similarly, we shall identify $C$ with its defining polynomial in $k[x,y,z]$ . Then we may write $$P = xC - wF, \qquad Q = yC - wG, \qquad R = zC - wH,$$ where $F, G, H$ are homogeneous polynomials of degree $d$ . FAILED ATTEMPT: My original strategy was to proceed as follows: Replace $R$ with a sufficiently general surface $S$ , also of degree $d+1$ . Assume $P \cap Q \cap R$ and $P \cap Q \cap S$ have the same number of points in $\mathbb A^3$ . Count how many points $P \cap Q \cap S$ has on the plane at infinity (i.e., on $C$ ), and subtract them from $(d + 1)^3$ . Unfortunately, this does not work. Using numerical experiments, I have found that $P \cap Q \cap S$ might have up to $d^3 + 2d^2 + 2d + 1 = (d^2 + d + 1) (d + 1)$ points in $\mathbb A^3$ , violating assumption 2. CURRENT STRATEGY: From now on, consider $A = k[x,y,z,w]$ , the homogeneous coordinate ring of $\mathbb P^3$ . $\mathfrak a = (P)$ , the ideal of the first surface. $\mathfrak b = (P, Q)$ , the ideal of the intersection of the first two surfaces. $\mathfrak c = (P, Q, R)$ , the ideal of the intersection of all three surfaces. First we show that $P, Q, R$ are pairwise coprime. By assumption, none of them is a multiple of $w$ . Now suppose two of them, say $P$ and $Q$ , had a common prime factor. This factor would be an irreducible component of $P \cap Q$ that is mostly contained in $\mathbb A^3$ . Hence $P \cap Q \cap R$ would contain a curve mostly contained in $\mathbb A^3$ , which we have assumed is not the case. Since $A/\mathfrak a$ and $A/\mathfrak b$ are complete intersections, we have the exact sequences $$
0 \longrightarrow A(-d-1)
  \longrightarrow A
  \longrightarrow A/\mathfrak a
  \longrightarrow 0
$$ and $$
0 \longrightarrow A/\mathfrak a(-d-1)
  \longrightarrow A/\mathfrak a
  \longrightarrow A/\mathfrak b
  \longrightarrow 0.
$$ On the other hand, $A/\mathfrak c$ is not a complete intersection, so $R$ must be a zero divisor in $A/\mathfrak b$ . Then we have a short exact sequence of the form $$
0 \longrightarrow K
  \longrightarrow A/\mathfrak b(-d-1)
  \longrightarrow A/\mathfrak b
  \longrightarrow A/\mathfrak c
  \longrightarrow 0,
\DeclareMathOperator \Hilb {Hilb}
$$ where $K$ is not trivial. Taking Hilbert polynomials, we have $\Hilb_A(n) = \binom {n+3} 3$ $\Hilb_{A/\mathfrak a}(n) = \Hilb_A(n) - \Hilb_A(n-d-1)$ $\Hilb_{A/\mathfrak b}(n) = \Hilb_{A/\mathfrak a}(n) - \Hilb_{A/\mathfrak a}(n-d-1)$ $\Hilb_{A/\mathfrak c}(n) = \Hilb_{A/\mathfrak b}(n) - \Hilb_{A/\mathfrak b}(n-d-1) + \Hilb_K(n) = (d + 1)^3 + \Hilb_K(n)$ Now consider a prime filtration of the form $$0 = M_0 \subset M_1 \subset \dots \subset M_r = A/\mathfrak c,$$ and let $\mathfrak p_i$ be the prime ideals such that $M_i/M_{i-1} = A/\mathfrak p_i(d_i)$ . Each $\mathfrak p_i$ is one of the following: The prime ideal of the curve $C$ , i.e., $\mathfrak p$ . The prime ideal of an embedded point on $C$ . The prime ideal of an isolated intersection point in $\mathbb A^3$ . The irrelevant ideal $\mathfrak m = (x,y,z)$ . We may ignore any occurrences of $\mathfrak m$ , because they do not contribute to $\Hilb_{A/\mathfrak c}(n)$ . So we need to compute The $A$ -module $K$ , or at least enough information to recover $\Hilb_K(n)$ . The number of embedded points in $C$ , counted with multiplicity. The degree of the Serre twist applied to $A/\mathfrak p$ , wherever it might occur in the prime filtration. How do I even begin to do this? BAD NEWS: Using even more numerical experiments, I have found that I was careless when stating the problem. The actual problem that I am trying to solve is the following. I have an algebraic foliation of $\mathbb P^3$ by curves, defined in $\mathbb A^3$ by the polynomial vector field $$\mathcal F : P^\flat \frac \partial {\partial x} + Q^\flat \frac \partial {\partial y} + R^\flat \frac \partial {\partial z},$$ where $P^\flat, Q^\flat, R^\flat$ denote the dehomogenizations (i.e., setting $w = 1$ ) of the polynomials $P, Q, R$ at the beginning of this post. We further assume that $\mathcal F$ contains only isolated singularities. If the plane at infinity is chosen generically, then all the singularities are in $\mathbb A^3$ , and the curve $C$ mentioned above is the set of points on the plane at infinity where $\mathcal F$ is tangent to this plane. It follows from these assumptions that $P, Q, R$ have the form stated above: $$P = xC - wF, \qquad Q = yC - wG, \qquad R = zC - wH.$$ However, the implication is strict. It does not follow from these three equations that $\mathcal F$ does not have singularities on the plane at infinity! Could someone help me find an algebraically precise way to state, in terms of $P, Q, R, C, F, G, H$ , that $\mathcal F$ does not have singularities on the plane at infinity?","['algebraic-geometry', 'commutative-algebra']"
4023546,Is this math game always winnable?,"Kent Haines describes the game of Integer Solitaire , which I find to be excellent for young kids learning arithmetic. I'm sure they will be motivated by this game to get a lot of practice. Kent asks a question about his game, which I find very interesting, and so I am asking here, in the hopes that Math.SE might be able to answer. The child draws 18 cards from an ordinary deck of cards, and then regards the cards to have values Ace = 1, 2, 3, ..., Jack = 11, Queen = 12, King = 13, except that Black means a positive value and Red means a negative value. Using 14 of the 18, the child seeks to find solutions of four equations: For example, a successful solution would look like: Question. Does every set of 18 cards admit a solution? Kent Haines says, ""I have no idea whether all combinations of 18 cards are solvable in this game. But I have played this game for five years with dozens of students, and I have yet to see a combination of 18 cards that is unsolvable."" Follow up Question. In the event that the answer is negative, what is the probability of having a winning set? For the follow up question, it may be that an exact answer is out of reach, but bounds on the probability would be welcome.","['combinatorics', 'card-games']"
4023597,Exponent equation with common power,"Solve for $x$ in $$\log_{2}(2^{x-1}+3^{x+1}) = 2x-\log_{2}(3^x)$$ I simplified by doing: $$\log_{2}(3^x \cdot 2^{x-1} + 3^{2x+1}) = 2x$$ $$\frac{6^x}{2} + 3^{2x+1} = 2^{2x}$$ $$6^x + 2 \cdot 3^{2x+1} = 2^{2x+1}$$ Where do I go from here? I tried moving more numbers around, but I haven't been able to get any closer to solving for x. Any help is appreciated.","['algebra-precalculus', 'problem-solving', 'logarithms']"
4023626,How does $\lim_{x \to \infty} (f(x) - 2x) = 0$ imply that $\lim_{y \to \infty} (g(y) - \frac{y}{2}) = 0$ without doing a calculation? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The original function is $f(x) = \sqrt{4x^2 + 1}$ for $x \in [0,\infty)$ and the inverse is $g(y)=\frac{\sqrt{x^2 - 1}}{2}$","['continuity', 'calculus', 'functions']"
4023641,The operator $U$ is unitary,"If $A$ is a selft-adjoint operator, then the operator $U$ defined by \begin{eqnarray*}
U=(A-iI)(A+iI)^{-1}
\end{eqnarray*} Is unitary. I know if $U$ is unitary then $U^{*}U=UU^{*}=I$ But when i tried to compute: \begin{eqnarray*}
U^{*}U&=&{((A+iI)^{-1})}^{*}(A-iI)^{*}(A-iI)(A+iI)^{-1}\\\
&=&{((A+iI)^{-1})}^{*}(A^{*}-iI^{*})(A-iI)(A+iI)^{-1}
\end{eqnarray*} or I tried to compute $UU^{*}$ : \begin{eqnarray*}
UU^{*}&=&{(A-iI)(A+iI)^{-1}((A+iI)^{-1})}^{*}(A-iI)^{*}\\\
&=&{(A-iI)(A+iI)^{-1}((A+iI)^{-1})}^{*}(A^{*}-iI^{*})
\end{eqnarray*} I don't know how can I continue with the hypothesis that $A$ is selft-adjoin operator and the way that I have to treat the adjoint inverse. Can you give some hint to continue? Thank you","['self-adjoint-operators', 'operator-theory', 'functional-analysis', 'analysis']"
4023651,conditions for which matrix has all eigenvalues inside unit circle or on the unit circle,"$\mathbf{M}=\begin{bmatrix}
\mathbf{y}&0&0&\cdots&0&0&\mathbf{x}\\
\mathbf{I}&0&0&\cdots&0&0&0\\
0&\mathbf{I}&0&0&0&0&0\\
0&0&\mathbf{I}&0&0&0&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
0&0&0&\cdots&\mathbf{I}&0&0\\
0&0&0&0&0&\mathbf{I}&0\\
\end{bmatrix}$ Given that $\mathbf{y}=\operatorname{diag}\left(y_i\right)$ and $\mathbf{x}=\operatorname{diag}\left(x_i\right) $ where each $i=1,2,\dots, N$ , $y_i$ are $N\times N$ matrices whose eigenvalues are strictly less inside unit circle. $I$ are $N\times N$ . For each $i$ , $x_i$ are also $N\times N$ . Could anyone help me to find out under what condition(s) this matrix $\mathbf{M}$ has the property that modulus of its all eigenvalues strictly less than $1$ given that each block matrix in $\mathbf{y}$ are given that they all have eigenvalues less than $1$ , for all $i$ ? Thank you.","['matrices', 'numerical-linear-algebra', 'stability-theory', 'eigenvalues-eigenvectors']"
4023666,"If $D=\{(x,y):x^2+y^2\leq 1\}$. Show there is $p_0\in D$ such that $T(p_0)=(0,0)$","Let $D=\{(x,y):x^2+y^2\leq 1\}$ , $T$ transformation of class $C'$ on an open containing $D$ , $$T:\left\lbrace \begin{array}{rcl} u &=& f(x,y) \\ v&=&g(x,y)  \end{array}\right.  $$ whose Jacobian is never $0$ in $D$ , and $|T(p)-p|\leq \frac{1}{3}$ for all $p\in D$ . Show there is $p_0\in D$ such that $T(p_0)=(0,0)$ Idea: Seems like fixed point theorem can help, for example if  I define $H(p)=T(p)-p$ , then $H(p)=p$ iff $T(p)=0$ , so it's enough to find such point for $H$ , by hypothesis $|H(p)|\leq \frac{1}{3}$ . Usually the fixed theorem is stated as: if $f:D\to D$ is continuous then there is a fixed point, or also there is no retraction of class $C'$ from $D$ onto the unit circle (only the boundary of $D$ ). Here I know that the jacobian of $T$ , $J(p)\neq 0$ for all $p\in D$ so there is a neighborhood of any $p\in D$ where $T$ is $1$ -to- $1$ , i.e., in where $T^{-1}$ exists. But at least for now I can't have a precise idea. Any hints are welcome. For context, this is a problem #17 section 7.5 from Buck's Advance Calculus. In this section we have the results: For $T:D\subset\mathbb{R}^n \to \mathbb{R}^n$ of class $C'$ , $D$ open, with $J(p)\neq 0$ for all $p\in D$ then $T$ is locally $1$ -to- $1$ in $D$ . If $J(p_0)\neq 0$ only for a specific value, then $T$ is $1$ -to- $1$ only in a small neighborhood of $p_0$ , where $T^{-1}$ exists. If conditions as in 1.,then $T(D)$ is an open set.",['multivariable-calculus']
4023702,Nested sums - reciprocal of a sum : Find exact value,"$$\text{Find:} ~~~~~~ \sum_{k=1}^{\infty} \frac{1}{ \left (
 \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2}$$ (Beware of the bounds of $j$ , it does not always start from $1$ , but starts at $k$ and goes to $k^2$ ) At first I thought it has something to do with Riemann's zeta function $\zeta$ and thus the solution is: $$ \zeta \left( \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}} \right ) $$ However this totally seems incorrect because we cannot find this sum as we have an unknown, $k$ . Which also is being summed to $\infty$ . I thought (however was unsure) of the fact that: $$ \left ( \sum f \right )^2 \ge  \sum f^2  $$ Thus (maybe) it is trivial the sum is converging to some value, which according to a little program I wrote is about: $$ \sum_{k=1}^{\infty} \frac{1}{ \left (
 \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2} \approx 1.596$$ But this is not a rigorous proof, is there a way to solve this so we get a solution by hand - calculating the exact value of this sum if it is finite (if not, why? ) Any mathematical tools (not programs) are acceptable, because this is not a question from a specific course ( I don't have context for this).","['riemann-zeta', 'calculus', 'summation', 'sequences-and-series']"
4023717,Are there general procedures to find group isomorphisms?,"This question is actually a series of related questions that have been bothering me while studying group theory. It seems to me that if we have two groups $G,H$ of same order and $G$ has a subgroup of order $n$ and $H$ has no subgroup of order $n$ , then we can't have an isomorphism. Now, suppose that we have two subgroups $G,H$ and they have the same quantity of subgroups $G_1,\dots,G_n$ and $H_1,\dots,H_n$ and $|G_i|=|H_i|$ does this means we have an isomorphism? Or are there groups where these conditions are true but there is no isomorphism? I've been thinking about mappings from generating sets of groups, if we can map each generator of $G$ to another generator of $H$ such that each generator from $H$ and $G$ have the same order, do we have an isomorphism? Perhaps this is equivalent to the previous one but I am not really sure. This is bothering me because in my lectures, we speak about what is an isomorphism but we never go in depth to actually talk about general procedures to find isomorphisms. Perhaps it's too early? I don't know.","['group-theory', 'abstract-algebra']"
4023733,Compute $\lim_{n \rightarrow \infty} \left(1 + \frac{i}{n^2 + in}\right)^n$,"I have a feeling that $$\lim_{n \rightarrow \infty} \left( 1 + \frac{i}{n^2 + in}\right)^n = 1,$$ but I don't know how to justify it. If $i$ was a real number $r > 0$ , I know how to compute this: take logs and then solve $$\lim_{n \rightarrow \infty} n \ln \left( 1 + \frac{r}{n^2 + rn}\right)$$ which after applying L'Hopital's rule gives $0$ , and hence an original limit of $e^0 = 1$ .
But since complex numbers are involved I'm don't even know if I'm allowed to apply the same logic. What theorems / results can be applied to efficiently solve the limit, now that complex numbers are involved?","['limits', 'complex-numbers']"
4023760,Kernel of Quadratic Field Norm,"Let $D > 0$ be squarefree and consider the field $\mathbb{Q}(\sqrt{D})$ . Then the field norm $N : \mathbb{Q}(\sqrt{D}) \to \mathbb{Q}^\times$ is given by $N(a + b\sqrt{D}) = a^2 - D b^2$ . Let $K$ be the kernel of this map, consisting of elements of norm $+1$ . I'm looking for an example where $K$ is not isomorphic to $\mathbb{Z}/2\mathbb{Z} \times \bigoplus_{i = 0}^\infty \mathbb{Z}$ as abelian groups. The motivation is the Pell conic group $C$ defined on the curve $x^2 - Dy^2 = 1$ . If $D$ is a square of a rational number, then $x \mapsto (\frac{x+x^{-1}}{2},\frac{x-x^{-1}}{2\sqrt{D}})$ gives an isomorphism $\mathbb{Q}^{\times} \cong C$ , but otherwise we have the isomorphism $K \cong C$ where $K$ is the kernel of the norm map. I am wondering whether the group structure of $C$ alone can detect the irrationality of a square root of a natural number. Below I've included my current thoughts on this question. Let's consider $D = 2$ . Then the group of units of $\mathbb{Z}[\sqrt{2}]$ is generated by powers of $\pm(1 + \sqrt{2})$ , so it is isomorphic $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}$ . Since $\mathbb{Z}[\sqrt{2}]$ is a UFD, we have that any element of $\mathbb{Q}(\sqrt{2})^\times$ can be written as a unique product of primes and units of $\mathbb{Z}[\sqrt{2}]$ , so $\mathbb{Q}(\sqrt{2})^\times \cong (\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}) \times \bigoplus_{p \text{ prime}} \mathbb{Z}$ . Now consider the map $h : \mathbb{Q}(\sqrt{2})^\times \to K$ given by $h(a) = \frac{a^*}{a}$ where $a^*$ denotes the conjugate. My understanding is that Hilbert's Theorem 90 implies this map is surjective. Its kernel is $\mathbb{Q}^\times$ , so $K$ should be something like $K \cong (\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}) \times \bigoplus_{p \text{ prime, } \notin \mathbb{Q}} \mathbb{Z}$ . This paper uses a similar map to obtain the analogous result for $\mathbb{Q}(i)$ . It seems like similar reasoning should apply to any UFD. Now consider a non-UFD, say $D = 10$ . Here we have $2 \cdot 5 = \sqrt{10} \cdot \sqrt{10}$ which at first seems like it might change the structure of $\mathbb{Q}(\sqrt{10})^\times$ compared to $\mathbb{Q}(\sqrt{2})^\times$ . However, I think you can throw out $5$ as a generator because you can get it from the other irreducibles as as $(\sqrt{10})^2/2$ . It seems like $\mathbb{Q}(\sqrt{10})^\times$ has a chance of being isomorphic to $\mathbb{Z}/2\mathbb{Z} \times \bigoplus_{i = 0}^\infty \mathbb{Z}$ despite not being a UFD. Then applying $h$ should then just kill the factors corresponding to primes in $\mathbb{Q}$ , so you might get the same thing for $K$ as with $D = 2$ .","['algebraic-number-theory', 'number-theory', 'field-theory', 'abstract-algebra', 'group-theory']"
4023762,Generalization of Schwarz's Lemma,"I am reading Lectures on Riemann Surfaces by Otto Forster. He says: (p.110) The following lemma may be viewed as a generalization of Schwarz's lemma. Let $D,D'$ be a pair of open subsets of $\mathbb{C}$ , where $D$ is a relatively compact subset of $D'$ . For any $\varepsilon>0$ , there is a closed vector space $A\subset L^2(D,\mathcal{O})$ , of finite codimension, with $$ \lVert f\rVert_{L^2(D')}\leq \varepsilon \lVert f\rVert_{L^2(D)}$$ He has already shown that $L^2(D,\mathcal{O})$ , the space of holomorphic functions on $D$ , forms a Hilbert space under the inner product $\iint f\overline{g}dxdy$ thus the ""closed"" comment. What does he mean when he says this generalizes Schwarz's lemma? How is this related to Schwarz's lemma?",['complex-analysis']
4023794,How to solve this question,"prove that if $R\leq S$ and $S\leq N$ then $P(N,S)$ is divisible by $P(N,R)$ Let $s = r +k$ since s is greater than r by some value but we don’t we don’t how much so I used k $\frac{n!}{(n-(r+k))!}$ / $\frac{n}{n-r}!$ $\frac{n!}{(n-r-k))!}$ / $\frac{n}{n-r}!$ where you can cancel out n! I am not getting how to solve farther than this. Now , I also don’t the meaning behind this question . Because $r$ can be just any value. There may be many values of $P(N,S)$ divisible by $P(N,R)$ . Many different values for $N , R$ and $S$ . so , from this way of solving . Can we find at least which value is $n,r$ and $s$ also. Because i think just by dividing variables , how can I find the real values when they are just not given.","['combinations', 'combinatorics']"
4023801,Intuition for meagre sets,"So I'm putting together a presentation in topology on BCT and I have the following theorems: Let $D$ be the set of somewhere differentiable functions $[0,1] \to \mathbb R$ . Then $D$ is meagre in $C[0,1]$ . Let $f \colon [0,1] \to \mathbb R$ be a differentiable function with derivative $f'$ . Let $X$ be the set of points for which $f'$ is discontinuous. Then $X$ is meagre in $[0,1]$ . My temptation to rationalise these intuitively (as a taster in the introduction) as follows: Most continuous functions are nowhere differentiable. Derivatives are continuous at most points. This is based off my understanding that a set $A$ is meagre in $B$ if it is small relative to $B$ in a precise sense. But there are instances where a meagre set can be quite large (even of full measure), and I've been told that it's not really correct to say ""most"" in this context. However, I've seen quite a lot of posts on here giving similar intuitive restatements. I thought I'd ask here for clarification - is it proper/well-understood to say ""most"" here? I could say ""many"" but that doesn't really feel as punchy. If it's not proper to say most here, is there any better way to put it? My topological intuition is still fairly weak.","['general-topology', 'derivatives', 'functional-analysis', 'real-analysis']"
4023817,Period of a linear recurrence mod p,"$\newcommand{\FF}{\mathbb{F}} $ $\newcommand{\bs}{\mathbf{s}}$ Given linear recurrence $$
x_n = a_{1}x_{n-1} + a_{2}x_{n-2} + \cdots + a_kx_{n-k}\pmod p,
$$ for $x_0, x_1, \ldots$ we define its state vector $\bs_{m}=(x_{m+k-1}, \ldots, x_m)\in \FF_p^k$ . The sequence is determined by choosing $\bs_0$ . We define its characteristic polynomial to be $\phi(x) = x^k - a_1x^{k-1} - \cdots - a_k$ . We say a recurrence is maximal if it has period $p^k-1$ for any nonzero choice of initial state $\bs_0$ . Clearly this is maximal because no nontrivial sequence can pass through the zero state. I am trying to prove the following (The Art of Computer Programming Vol 2, sec 3.2.2) The recurrence is maximal iff $\phi$ , considered as an element of $\FF_{p^k}[x]$ , has a primitive element of $\FF_{p^k}$ as a root. I am unsure how to attempt either direction.","['number-theory', 'finite-fields', 'recurrence-relations']"
4023906,I can't wrap my head around Peano's 5'th axiom,"I'm currently reading about Peano's axiom and the construction of the natural numbers. The fifth Axiom of Peano does not make sense to me: If a subset $T$ contains $0$ as an element and for all $n \in T$ , $s(n)$ is also in $T$ , then $T$ is the set of Natural numbers $\mathbb{N}$ . I know that this axiom exists in order to exclude elements which could have the following property: $$s(a) = b$$ $$s(b) = a$$ These elements would not violate axioms $1$ to $4$ however they don't have the desired ""natural number"" properties hence axiom $5$ has been created to allegedly clean these elements up. I also know that axiom $5$ is supposed to work as follows: If $0$ is in our set $T$ then $s(0)$ is also in $T$ but if $s(0)$ is in $T$ then $s(s(0))$ is also in $T$ and so on. Thus we declare this set to be the natural numbers. Now to my actual question: suppose we throw in $a$ and $b$ and $0$ into $T$ . Since we do not yet know what the natural numbers are, we can't exclude $a,b$ either (if my thinking is correct). Then for all $n$ in $T$ the succesor $s(n)$ is also in $T$ . As previously stated $s(0), s(s(0)), ...$ are all in in $T$ (the actual natural numbers) but if such elements $a$ and $b$ are also in T then their successor $s(a)$ and $s(b)$ are also in $T$ which is  a true statement since $s(a) = b$ and $s(b) =a$ . Axiom $5$ should exclude such elements but the way I am stating it, it seems like these elements do not violate axiom $5$ .","['elementary-set-theory', 'logic', 'analysis', 'real-analysis']"
4023908,Prove $\int_0^\infty\frac{\ln x}{x^3-1}\mathrm{d}x=\frac{4\pi^2}{27}$,"Proof of the integral $$\int_0^\infty\frac{\ln x}{x^3-1}\mathrm{d}x=\frac{4\pi^2}{27}$$ I try to substitute $u = \ln x$ . Then $x = e^u,\>\mathrm{d}x = e^u\mathrm{d}u$ and the limits $(0,\infty)\to (-\infty,\infty)$ .
The integral becomes $$\int_{-\infty}^\infty \frac{ue^u}{e^{3u}-1}\mathrm{d}u.$$","['integration', 'calculus', 'improper-integrals']"
4023947,Application of implicit theorem,"Find conditions on the function $f$ and $g$ which permit you to solve the equations $$f(xy)+g(yz)=0\ \ \textrm{and} \ \ g(xy)+f(yz)=0 $$ for $y$ and $z$ as functions of $x$ , near the point $x=y=z=1$ and $f(1)=g(1)=0$ . Attempt: This problem seems to be an application of the implicit function theorem. Usually on this kind of problems we define special transformations, but I don't now how to define it in this case since I have several variables $x,y,z$ and two functions $f$ and $g$ . Any idea on how to start?",['multivariable-calculus']
4024002,$\lim_{n \to \infty} (u_{n+1}-u_n)$ does not converge then $\lim_{n \to \infty} u_n$ does not converge,"Prove that if $\lim_{n \to \infty} (u_{n+1}-u_n)$ does not converge then $\lim_{n \to \infty} u_n$ does not converge. My try -
Contrapositive statement - If $\lim_{n \to \infty} u_n$ converges then $\lim_{n \to \infty} (u_{n+1}-u_n)$ converges. Let , $\lim_{n \to \infty} u_n=L$ Then $\lim_{n \to \infty} u_{n+1}=L$ $\lim_{n \to \infty} (u_{n+1}-u_n)=L-L=0$ Is the proof correct? Any other proof is also desirable.","['real-numbers', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4024017,Intuition regarding Euler’s formula,"In every beginner’s class on differential geometry, we learn Euler’s formula , which tells us about the normal curvatures at a point on a surface. I know how to prove this formula, and I have even taught it in classes, but it seems entirely unbelievable, to me. It says that at any point on any surface, the variation of normal curvature as a function of angle is given by an absurdly simple little formula. My intuition says that this variation should be different for different surfaces. If I choose a point on a highly convoluted surface, I would expect the variation of normal curvature to be complicated, but it’s not. I can’t get my head around this at all. It’s astonishing. Can anyone elucidate? I have seen the standard proofs, and those don’t seem to help very much. I’m looking for intuition, please, rather than manipulation of symbols.",['differential-geometry']
4024042,Can dimension of manifolds be understood similarly to dimension of schemes?,"I’m only beginning to learn about schemes, but I know that at least in some cases, the dimension of a scheme (or variety) is 1 less than the length of the longest chain of irreducible closed subsets. For (let’s say real, smooth) manifolds, this definition doesn’t work: all closed sets are reducible unless they are points. But what if we replace “irreducible closed sets” with “connected compact (c.c.) submanifolds (without boundary)”? It seems that c.c. manifolds are somewhat similar to irreducible closed sets in that they cannot be written as finite unions of other c.c. (proper)submanifolds not equal to themselves (I think). Does this idea lead anywhere interesting / is there a source that discusses dimension from this perspective (or can the validity thereof be disproven)? The obvious follow up is: given a smooth manifold X, does the ideal of smooth, real valued functions on X which vanish on a given c.c. submanifold Y have any “nice” properties analogous to prime ideals (of course in the ring of smooth functions on X, this ideal itself is unfortunately not actually prime... but maybe something comparable but weaker? Or maybe this works better in the real analytic case?)","['compact-manifolds', 'krull-dimension', 'algebraic-geometry', 'smooth-manifolds']"
4024064,Proving that the intersection of $(a-4)^{2}+(b-1)^{2}<4$ and $(a-1)^{2}+(b-5)^{2}<4$ is empty,"I´m trying to prove that these two inequalities intersection is empty, but i don´t know how to proceed i know they are circles, but i´m trying to do it with only algebra Let $a,b\in\mathbb{R}$ and the inequalities are $$(a-4)^{2}+(b-1)^{2}<4 \quad\land\quad (a-1)^{2}+(b-5)^{2}<4$$ Any insight would be appreciated.",['algebra-precalculus']
4024146,"Given a recursion $a_{n+ 1}= \sqrt{na_{n}+ 2n+ 1}$ with $a_{1}\geq 1.$ Prove that $a_{n}\sim n\,{\rm as}\,n\rightarrow\infty$","Given a recursion $a_{n+ 1}= \sqrt{na_{n}+ 2n+ 1}$ with $a_{1}\geq 1.$ Prove that $$a_{n}\sim n\,{\rm as}\,n\rightarrow\infty$$ Let $b_{n}= \dfrac{a_{n}}{n},$ we need to prove $\lim b_{n}= 1,$ then we find a value $\beta\in\left ( 0, 1 \right )$ satisfying $\left | b_{n+ 1}- 1 \right |\leq\beta\left | b_{n}- 1 \right |$ so that $$\beta\rightarrow 0\,{\rm aut}\,n\rightarrow\infty$$ I can't predict the relationship among them, I need the the help to go to the induction, thank you.","['laurent-series', 'recurrence-relations', 'recursion', 'alternative-proof', 'limits']"
4024158,How did Gauss come up with this function?,"I have been studying statistics lately, and after having my first brush with normal distributions, I was curious to know more about it. I have researched about the history of normal distributions and found the following information: When gauss published his monograph, Theoria motus corporum coelestium in sectionibus conicis solem ambientium , he discussed among other things the normal distribution (interestingly, the book was on how celestial objects have orbits that are conic sections). According to Wikipedia , he did the following to arrive at the distribution: There is an unknown quantity $V$ . $M, M',M"",\dots$ are the measurements of $V$ . $\varphi$ is the function that governs the law of probability. The aim was to  find $\varphi$ . It says on Wikipedia that the most likely estimator of $V$ would be the one that maximizes: $$\varphi(M-V)\varphi(M'-V)\varphi(M""-V)\dots\;\;\;\;\;\;\;\;\;\;-(1)$$ Gauss guessed that the answer to the $V$ maximizing the above equation must be the mean of the measured values. Then gauss demonstrated that the only function $\varphi$ that gives the mean as the answer was: $$\varphi\Delta=\frac{h}{\sqrt{\pi}}e^{-h^2{\Delta}^2}$$ where, $\Delta$ is the measurement of errors. $h$ is the measure of precision of the observation A number of questions pop up in my head. Why should the most likely estimate be the one that maximizes equation 1? How did Gauss' derive $\varphi$ using the assumed solution to be the mean? What does $\Delta$ actually mean? MY ANSWERS/ATTEMPTS I actually feel that it should not be the maximum. It should rather be the minimum as when the expression is minimized it means that $V$ is closer to each of the values $M,M',M""\dots$ which are the actual measurements. Therefore such a value should be the most likely estimate. From my knowledge of statistical inference, in the procedure of maximum likelihood estimator, we maximize the probability density function of the distribution. The value of random variable at the maxima is the estimate. I do not know the answer to this one. I tried looking up Gauss' original work but it would take me too off topic as Gauss' has treated these ideas with respect to orbits of celestial objects being conic sections. I do not wish to look into the applications right now (But I do understand that reading the book right from the start will be wonderful learning experience). I once again do not fully understand what $\Delta$ means here. According to me, $\varphi$ should have been a function of $V$ and not $\Delta$ . It would be really helpful if someone could answer my questions and also provide an explanation of what actually is being done here as I am getting confused with so many ideas in my head!!","['statistical-inference', 'statistics', 'probability-distributions', 'normal-distribution', 'gaussian']"
4024173,Poset vs Set and Some Notations,"I apologize if I am splitting hairs here. First of all, is a poset a set ? The official definition on my textbook says, If $X$ is a set and $R$ is an ordering on $X$ , the pair $(X, R)$ is called an ordered set. I mean a poset is a set with a partial order , whereas a set itself, by definition, does not have order. So I guess a poset is not a set? But if this is true, then I have another confusion: There's a line from the book that says, A finite linearly ordered set has a (unique) largest element. So if $x \in X$ and this $x$ is this largest element, is it ok to also write $x \in (X, R)$ ?","['elementary-set-theory', 'order-theory', 'notation']"
4024212,An Inequalities related to power sum,"Let $S_m(n)=1^m+2^m+\cdots+n^m$ Show that the following inequalities are true for all $m,k\in\mathbb{Z}_+$ [1] $k\cdot(2km+m)^{2m-1}\le S_{2m-1}(2km+m-1)$ [2] $k\cdot(2km+m+k+1)^{2m}\le S_{2m}(2km+m+k)$ Example: for $m=k=1$ ; gives [1] $ 3\le3$ and [2] $25\le 30$ . both claims are verified up to $m,k\le50$ . source code Pari GP for(k=1,50,for(m=1,50,if(k*(2*k*m+m)^(2*m-1)>sum(t=1,2*k*m+m-1,t^(2*m-1)),print([k,m])))) Apologize for deleting all previous updates. update #1: (more on claim) (1) $k\cdot(2km+m)^{2m-1}\le S_{2m-1}(2km+m-1)\le (k+1)\cdot(2km+m)^{2m-1}$ (2) $k\cdot(2km+m+k+1)^{2m}\le S_{2m}(2km+m+k)\le (k+1)\cdot(2km+m+k+1)^{2m}$","['number-theory', 'elementary-number-theory']"
4024276,Functions and Infinity,"I have been reading a lot about real analysis and infinite sets lately. Infinite sets I mean the most common ones ( $\mathbb{N}$ and $\mathbb{R}$ ). I have come across many problems and there are two which look very similar in terms of solving technique but I could not seem to figure them out. Problem 1: Is there a function ( $f:$ $\mathbb{R} \rightarrow  \mathbb{R}$ ) whose limit at every point is $ \infty $ ? Problem 2: Can a function ( $f:$ $\mathbb{R} \rightarrow  \mathbb{R}$ ) have continuum many local min/max values ?
(We are looking for local extrema values not points, so the constant function only has 1 in this regard) So both problems ask questions about some properties maximum cardinality. Obviously countably finite is achieveable for both questions ( $\tan{x}$ and $x\sin{(1/x)}$ ). I think the answer is negative for both questions. For Problem 1 I think all values have to be 'large' for this to happen, namely $\infty$ but could not exactly prove this. I also tried to make a bijection with rational numbers on how many points can have a limit at infinity, but also could not finish. For problem 2 I tried bijection with rationals but I could not make it so that for every rational I pair it with one Local min/max. I feel like the thought process should be roughly similar for both but I could not find it. I may be completely on the wrong foot either by guessing that the answer must be negative. Can someone tell me if my ideas can be finished, or if not then what way would work for proving these, and also if my guess about the answer is right.","['functions', 'infinity']"
4024298,'Mean' appearance of $\pi$...,"I just came across a strange result, where $\pi$ unexpectedly pops up. Consider two real numbers $a$ and $b$ , now repeat the following steps- Replace $a$ by the arithmetic mean of the two numbers. Replace $b$ by the geometric mean of the two numbers. In mathematical terms, $\forall a,b \in R$ , We define a sequence $a_i$ and $b_i$ such that $a_n=\dfrac{a_{n-1}+b_{n-1}}{2}$ and $b_n=\sqrt{a_{n}b_{n-1}}$ . After sufficient numbers of iterations of the same steps, it is observed that $a\text{~}b$ , that is $a_\infty=b_\infty$ , which we can easily prove. At $a=1$ and $b=2$ , I get the following value of convergence: 1.6539866862653758 , which turns out to be equal to $\frac{3\sqrt3}{\pi}$ ! Python code below: a=1
b=2
while(a!=b):
     a=(a+b)/2 #Arithmatic mean
     b=pow((a*b),0.5) #Geometric mean
print(a)
#print(3*pow(3,0.5)/a) which equals pi Here is a method which explains, and generalizes the case for all values of $a$ and $b$ , but I am still looking for a better intuition... $""Let (a=b\cos\theta)\\ Given(a_{1}=\displaystyle \dfrac{a+b}{2}) \\(\displaystyle { a }_{ 1 }=\dfrac { b\cos { \theta +b }  }{ 2 } =b\cos ^{ 2 }{ \dfrac { \theta  }{ 2 }  } ) \\(b_{ 1 }=\sqrt { a_{ 1 }b } =\sqrt { { b }^{ 2 }\cos ^{ 2 }{ \dfrac { \theta  }{ 2 }  }  } =b\cos { \dfrac { \theta  }{ 2 }  }) Given \\(\displaystyle a_{ 2 }=\dfrac { a_{ 1 }+b_{ 1 } }{ 2 } =\dfrac { b\cos ^{ 2 }{ \dfrac { \theta  }{ 2 } +b\cos { \dfrac { \theta  }{ 2 }  }  }  }{ 2 } =\dfrac{b \cos \dfrac{\theta}{2}(\cos \dfrac{\theta}{2}+1)}{2}=b\cos { \dfrac { \theta  }{ 2 }  } \cos ^{ 2 }{ \dfrac { \theta  }{ 4 }  } ) \\( \displaystyle b_{2}=\sqrt{a_{2}b_{1}}) \\(\displaystyle =\sqrt { { b }^{ 2 }\cos ^{ 2 }{ \frac { \theta  }{ 2 }  } \cos ^{ 2 }{ \frac { \theta  }{ 4 }  }  } ) \\(\displaystyle b_{2}=b\cos { \frac { \theta  }{ 2 }  } \cos { \frac { \theta  }{ 4 }  } )Similarly, \\(\displaystyle b_{ 3 }=b\cos { \frac { \theta  }{ 2 }  } \cos { \frac { \theta  }{ 4 }  } \cos { \frac { \theta  }{ 8 }  } )So,\\(b_{ n }=b\cos { \dfrac { \theta  }{ 2 }  } \cos { \dfrac { \theta  }{ 4 }  } \cos { \dfrac { \theta  }{ 8 }  } ......\cos { \dfrac { \theta  }{ { 2 }^{ n } }  } )Now, \\(b_{ \infty  }=\displaystyle \lim _{ n\rightarrow \infty  }{ { b }_{ n } } )\text{we can reduce} \\(b_n=\dfrac {b\cos { \dfrac { \theta  }{ 2 }  } \cos { \dfrac { \theta  }{ 4 }  } \cos { \dfrac { \theta  }{ 8 }  } ......2 \sin { \dfrac { \theta  }{ { 2 }^{ n } }} \cos { \dfrac { \theta  }{ { 2 }^{ n } }  }}{2 \sin { \dfrac { \theta  }{ { 2 }^{ n } }}}=\dfrac {b\cos { \dfrac { \theta  }{ 2 }  } \cos { \dfrac { \theta  }{ 4 }  } \cos { \dfrac { \theta  }{ 8 }  } ...... \cos { \dfrac { \theta  }{ { 2 }^{ n-1 } }  }}{2 \sin { \dfrac { \theta  }{ { 2 }^{ n } }}} )\text{and thus reducing so on, we get}\\(=\displaystyle \lim _{ n\rightarrow \infty  }{ \displaystyle \dfrac { (\dfrac { \theta  }{ { 2 }^{ n } } )b\sin { \theta  }  }{ \theta \sin { (\displaystyle\frac { \theta  }{ { 2 }^{ n } }  } ) }  } =\dfrac { b\sin { \theta  }  }{ \theta  }  =\dfrac { \sqrt { 1-\dfrac { { a }^{ 2 } }{ { b }^{ 2 } }  }  }{ \cos ^{ -1 }{ (\dfrac { a }{ b } ) }  }  =\dfrac { \sqrt {{ { b }^{ 2 }-{ a }^{ 2 } }}  }{ \cos ^{ -1 }{ (\dfrac { a }{ b } ) }  } )""$ Hence, at $a=1$ and $b=2$ , we get the $\pi$ through the inverse of cos term.
It is counterintuitive that $\pi$ turns up here through mere substitution, and I am hopeful that there exists a more natural, better understanding of this problem than the one mentioned above, which connects together the seemingly unreleated areas of maths. Can anyone please help me with any alternative, more elegant, or visual solution? P.S.-- This doesn't answer my question, and just is another form of the proof given above. I am looking for a more intuitive/visual/elegant proof for why pi turns up here.... EDIT-- have tried adding an image of the convergence table of the sequence","['trigonometry', 'pi', 'means']"
4024355,About the definition of directional derivative in the clousure of an open set,"Let be $X$ a normed space and thus we prove that if $x_0\in\overset{\circ}Y\subseteq X$ then for any $\vec v\in X$ such that $\lVert\vec v\lVert=1$ the set $$
V:=\{h\in\Bbb R:(x_0+h\vec v)\in\overset{\circ}Y\}
$$ is a not empty open set that has $0$ as a cluster point. So first of all we observe that if $x_0\in\overset{\circ}Y$ then $0\in V$ and thus it is not empty. Anyway if $x_0\in\overset{\circ}Y$ then  there exists $\delta>0$ such that $B(x_0,\delta)\subseteq\overset{\circ} Y$ and thus $(x_0+h\vec v)\in \overset{\circ}Y$ for any $h\in(-\delta,\delta)$ so that $0$ is a cluster point of $V$ . Then if $(x_0+h\vec v)\in\overset{\circ}Y$ there exists $\delta>0$ such that $B(x_0+h\vec v,\delta)\subseteq\overset{\circ}Y$ and thus $(x_0+t\vec v)\in\overset{\circ}Y$ for any $t\in(h-\delta,h+\delta)$ so that $(h-\delta,h+\delta)\subseteq V$ and thus this it is open. Now if $\overset{\circ}Y$ is not empty and so that for any $x_0\in\overset{\circ}Y$ there exists $\delta>0$ such that $B(x_0,\delta)\subseteq\overset{\circ}Y$ then $V\neq\emptyset$ for any $\vec v\in V$ such that $\lVert\vec v\lVert=1$ (indeed an open ball is a convex set) and so there exist at least a net in $V$ converging to $0$ so that if $f:Y\rightarrow\Bbb R^n$ is a function then the limit $$
\lim_{h\rightarrow 0}\frac{f(x_0+h\vec v)-f(x)}h
$$ is well defined and we call it the deriviative of $f$ in the direction $\vec v$ . Anyway if $x_0\notin\overset{\circ}Y$ it could happen that one of the limits $$
\lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec v)-f(x)}h\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\lim_{h\rightarrow 0^-}\frac{f(x_0+h\vec v)-f(x)}h
$$ could be defined and we can think to exted the notion of directional derivative: e.g. the upper half space $H^k:=\{x\in\Bbb R^k:x_k\ge0\}$ is closed and if $x_0\in\partial H^k=\{x\in\Bbb R^k:x_k=0\}$ the limit $$
\lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec e_k)-f(x)}h
$$ is well defined for any function $f$ whose domain is the upper half space. So I ask if $x_0$ is a cluster point of $\overset{\circ}Y$ $\Biggl($ and thus $x_0\in\overline{\overset{\circ\,}Y}\Biggl)$ then at least for one $\vec v\in X$ such that $\lVert\vec v\lVert=1$ the set $V$ as above defined is a not empty open set that has $0$ as cluster point so that it is possible to exted the notion of directional derivative in the clousure of an open set. So could someone help me, please?","['normed-spaces', 'calculus', 'vector-analysis', 'general-topology', 'derivatives']"
4024360,Link between two gradient definitions,"Let $M$ a $n$ dimensional embedded Riemannian manifold of $\mathbb{R}^{n+1}$ . There are two definitions of $\nabla_M f$ that seem different, but I think they should be related: Def 1: $\nabla_M f = \nabla f - \langle\nabla f, n\rangle n$ , where $n$ is the outer unit normal. Def 2: Definition through parametrization. By Def 1, one sees that $\nabla_M f \in \mathbb{R}^{n+1}$ , while using Def 2 (in practice), we get $\nabla_M f \in \mathbb{R}^{n}$ . How to relate this two definitions? (Maybe, behind my confusion there is some hidden identification). Edit 1: Take, e.g., $f(x,y,z)=\frac{1}{2}x^2$ on $\mathbb{R}^3$ . Then
by Def 1: $\nabla_{S^2} f(x,y,z)=(x-x^3, -x^2y,-x^2z)$ and by Def 2: $\nabla_{S^2} f(\phi(\theta,\varphi))=(-\frac{1}{2} \sin(2\theta),\frac{1}{2} \sin(2\varphi) \cos^2 \theta)$ . But in Riemannian geometry, they said that they are the same, they even prove one from the other.","['derivatives', 'riemannian-geometry', 'differential-geometry']"
4024418,"If $x+y+z+a+b+c=1$, $xyz+abc=1/36$, what is the max value of $abz+bcx+cay$?","Question : If $x+y+z+a+b+c=1,xyz+abc=1/36,$ What is the max value of $abz+bcx+cay$ ?  ( $x, y, z, a, b, c ∈ R$ (non-negative)) I tried $abz+bcx+cay≥3abc$ (because of AM-GM) to make it similar to $xyz+abc=1/36$ , but I'm stuck with expanding.","['contest-math', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
4024519,what is the derivative of $\sec^2x$,"I'm using the quotient rule and so I rewrite $\sec^2x$ as $\frac{1}{\cos^2x}$ Then set $u = 1,$ $u'= 0$ then $v = \cos^2x$ and $v'= -2\sin x \cos x$ I then get $\frac{2\sin x \cos x}{\cos^4x}$ but when I simplify that I don't get $2\sec^2x\tan x$ which is the correct answer?
I'm not really sure what I'm doing wrong","['trigonometry', 'derivatives']"
4024521,"Is $\inf \{P(|X| > \varepsilon) : E[X] = 0, \operatorname{Var}(X) = 1\} = 0$ for all $\varepsilon > 0$?","If $\varepsilon > 0$ , is it true that the infimum of $\{P(|X| > \varepsilon) : E[X] = 0, \operatorname{Var} (X) = 1\}$ is zero? That is, for every $a > 0$ , can we always find a random variable $X$ with zero mean and unit variance that satisfies $P(|X| > \varepsilon) < a$ ? My first attempt to prove this statement was by using the Chebyshev's inequality: if $E[X] = 0$ and $\operatorname{Var} (X) = 1$ , then $E[X^2] = \operatorname{Var} (X) + E[X]^2 = 1$ , so we have $$P(|X| > \varepsilon) \leq \frac{1}{\varepsilon^2}.$$ However, this inequality seems to be unhelpful, especially if $\varepsilon \leq 1$ . Moreover, the right hand side does not even depend on $X$ or any other variable, so I cannot make the upper bound of $P(|X| > \varepsilon)$ to approach zero in this inequality. Next, I tried to construct a sequence $\{X_n\}_{n\in\mathbb{N}}$ of random variables based on the uniform and U quadratic distributions. The idea is to reduce the support of $X_n$ to be less than $[-\varepsilon, \varepsilon]$ as $n \to \infty$ , so that $P(|X_n| > \varepsilon) = 0$ eventually. More precisely, each $X_n$ has a probability density function defined by $$f_{X_n} = \frac{2n+1}{2a_n^{2n+1}}x^{2n} \quad (x \in [-a_n,a_n]),$$ where $a_n = \sqrt{1+\frac{2}{2n+1}}$ , and zero otherwise (this formulation ensures that $E[X_n] = 0$ and $\operatorname{Var}(X_n) = 1$ for all $n$ ). Unfortunately, $a_n$ converges to $1$ in this case, and $P(|X_n| > \varepsilon) \neq 0$ for $\varepsilon < 1$ . After these attempts, I began to doubt the validity of this proposition. After all, wouldn't a random variable with variance of $1$ surely ""spread over"" to the value of $1$ if it is centered at $0$ ? This is only my intuition and I have yet to prove this. But if that's the case, then $P(|X| > \varepsilon)$ surely can't approach $0$ for $0 < \varepsilon < 1$ . Any help on proving or disproving the statement in my question is appreciated.","['statistics', 'probability-distributions', 'probability', 'random-variables']"
4024530,Proving if a function is continuous and not one-one then it has many such points.,"Let $g$ be a continuous function on an interval $A$ and let $F$ be the set of points where $g$ fails to be one-to-one; that is $$F = \{x \in  A : f(x)=f(y) \text{ for some $y \neq x$ and $y \in A$} \}$$ Show that if $F$ is non-empty then it is uncountable. An attempt at the solution. Since F is non-empty there exists some $c_1,c_2$ $\in A$ such that $c_1\neq c_2 $ and $g(c_1)=g(c_2)$ .
Choose a point $z$ between $c_1$ and $c_2$ with $g(z) \neq g(c_1)$ Now $\forall$ $L$ satisfying $g(z)<L<g(c_1)$ or $g(z)>L>g(c_1)$ by $IVT$ there will be a point $K_1$ $\in (z,c_1)$ satisfying $g(K_1)=L$ . Notice that $g(z)<L<g(c_2)$ or $g(z)>L>g(c_2)$ is also true for the same value of $L$ which by $IVT$ leads to the conclusion that there is also another point $K_2 \in (z,c_2)$ satisfying $g(K_2)=L$ . Now how can i prove that there are uncountable number of points? Am i correct upto here?","['real-numbers', 'continuity', 'functions', 'real-analysis']"
4024542,support vs state space,"In one of my examples, I saw that the state space for a binomial distribution is $({0,1,...,n})$ . I then thought that for $X_1, X_2,...,X_n$ iid RVs, where $f_\theta(x_i)=\theta x_i^{\theta -1}$ for $x \in (0,1)$ , and $0$ otherwise, the state space is $(0,1)^n$ . However, it was said to be $\Bbb R^n$ , meaning other values outside $(0,1)$ are also counted in the state space (which would make sense, since the state space is the set of values assumed by our RV), and that $(0,1)^n$ is rather the support. But, following this logic, how comes the state space for the binomial distribution isn't $\Bbb N$ , where for all $N>n,$ the probability that the RV is equal to that value is zero? Or ss it because the binomial distribution isn't defined for $n+1$ onwards?","['statistics', 'probability-distributions']"
4024548,Show that the sequence defined as $x_{n+1}=\frac{1}{1+x_n}$ converge,"Show that the sequence $(x_n)$ defined as $x_{n+1}=\frac{1}{1+x_n},x_0=0$ converge. My attempt is to show that $(x_n)$ is Cauchy so i would like to have a feedback on my proof,please. First of all we show by induction that $2>x_n\ge0$ . The previous inequality is true for $n=0$ so we can suppose that it works up to a certain $n$ . We want to proof right now that it holds fot $n+1$ and so show that $2>x_{n+1}\ge0$ : $2>x_{n+1}\ge0 \iff 2>\frac{1}{1+x_n}\ge0$ . It is trivial that $x_{n+1}\ge0$ by induction hypothesis (because $x_n\ge0$ ). Moreover, as $2>x_n\ge0$ , we have that $x_{n+1}=\frac{1}{1+x_n}\le1<2$ . So we can conclude that $2>x_n\ge0 \ \forall n\ge0$ Then, $\forall p\ge2$ we have: $|x_p-x_{p-1}|=\Big|\frac{x_{p-2}-x_{p-1}}{(1+x_{p-1})(1+x_{p-2})}\Big|\le 1 \cdot|x_{p-2}-x_{p-1}|=...=1^{p-1}|x_0-x_1|=1^{p-1}$ We show now that $(x_n)$ is Cauchy. So, by définition of Cauchy sequence, we have to show that $\forall \epsilon>0 \ \exists N \ \forall m,n>N$ : $|x_m-x_n|<\epsilon$ . But, $|x_m-x_n|\le|x_m-x_{m-1}|+|x_{m-1}+x_{m-2}|+...+|x_{n+1}-x_n|\le(m-n-1)$ So $(x_n)$ is Cauchy I am not reall sure if this proof holds, because i think that in case where $m=n-1$ it doesn't work,or...?","['limits', 'convergence-divergence', 'proof-writing', 'sequences-and-series']"
4024620,Can a series $\sum_{n=1}^{\infty} (\frac{1}2)^n=1$ be infinitely substituted into itself?,"Example $$\Large\sum_{n=1}^{\infty} \left(\frac{\sum_{n=1}^{\infty} \left(\frac{\sum_{n=1}^{\infty} \left(\frac{\sum_{n=1}^{\infty} \left(\frac{\sum_{n=1}^{\infty} \left(\frac{...}2\right)^n}2\right)^n}2\right)^n}2\right)^n}2\right)^n$$ Motivation Recently did a screen share on gather.town with itself and was greeted with this image below. The idea popped into my head, ""What if this infinite mirror concept was applied to an infinite series, would it be possible to create a series that has a known result but is not calculable?""","['algebra-precalculus', 'sequences-and-series']"
4024626,Inverse of a continuous increasing function is continuous (solution verification),"Show that for a continuous, increasing function $f : \mathbb R \rightarrow [0,1]$ that is onto $(0,1)$ it's inverse $f^{-1}$ is also continuous. What I've tried My first approach: $f^{-1}$ is continuous if $f$ maps open sets to open sets. Since $f$ is increasing it is a one-to-one mapping and it suffices to show that $f$ maps open intervals to open intervals. Let $V \subset [0,1]$ so that $f^{-1}(V)$ is open (since $f$ is continuous) and $f(f^{-1}(V))=V$ which is also open. All that remains to show is that every open interval $U = f^{-1}(V)$ for some open interval $V$ . This is true since $f$ is a one-to-one mapping. My second approach: $f^{-1}$ is continuous if there exists a $\delta$ such that when: $$|\alpha - \alpha'| < \delta$$ we have: $$|f^{-1}(\alpha) - f^{-1}(\alpha')| < \epsilon$$ Call the first event $A$ and the second event $B$ . To show $A \implies B$ it is sufficient to show that $B^c \implies A^c$ . Let $\alpha = f(x),\, \alpha' = f(x')$ for some $x,x'$ so that we need to show for each $\epsilon$ : $|x-x'| \geq \epsilon \implies |f(x) - f(x')| \geq \delta$ for some $\delta$ . But this is clear since $f$ is strictly increasing. I could use some feedback on my reasoning for both approaches.","['real-analysis', 'continuity', 'functions', 'solution-verification', 'inverse']"
4024641,Do repeated roots (and Real Jordan form) for ODE's come up in real world applications of ODE's,"An equation like $y^{\prime \prime} + 2 y^{\prime} + y = 0$ has repeated roots: The characteristic polynomial is $r^2 + 2r + 1$ which has repeated roots $(-1,-1)$ . Two basic solutions of the ODE are then $e^{-t}$ and $t e^{-t}$ . More general examples exist with systems of ODE's and involve real Jordan form (aka real cannonical form) as explained in the textbook by Hirsch and Smale. My question is whether this has real applications. In order to get repeated roots, the ""true"" ODE would have to be exactly a certain way and any slight deviation would destroy the repeated roots. I suppose people call non-repeated roots ""generic"". So this would seem very rare. Does this come up real applications? I could maybe see it in physics or somethings where there are well-defined laws governing a system but hard to see it in a setting where the ODE is really just a model of what is in reality a more complex system.","['applications', 'mathematical-modeling', 'ordinary-differential-equations']"
4024664,Does $\sum^{\infty}_{n=1}\left(\frac{p_{n+1}}{p_n}-1\right)^2 $ converge?,"There appears to be a ""prime constant"" $\kappa$ , generated from the sequence of primes: $$\kappa = \sum^{\infty}_{n=1}\left(\frac{p_{n+1}}{p_n}-1\right)^2 \approx 1.653$$ Where $p_n$ is the nth prime. However, how does one prove that such constant, does in fact, exist, that is, how does one prove that the above series converges?","['elementary-number-theory', 'prime-numbers', 'sequences-and-series']"
4024678,Can the idea of a 'function of a variable' be made rigorous?,"Suppose that $y=x^2$ . Very often people describe this relationship by saying that ' $y$ is a function of $x$ '. It seems that there are several logical problems with this statement: A function is simply a set of ordered pairs of numbers. It does not matter how you denote the input of a function, and there is no doubt that $x \mapsto x^2$ and $y \mapsto y^2$ are the same function. Indeed, in both cases $y$ and $x$ are simply dummy variables used to illustrate what happens when you plug in an arbitrary number into the function. $x$ and $y$ are often described as variables that are related in some way; in this case, with $y$ being the square of $x$ . However, in this Math Overflow post  Mike Shulman states that the idea of a variable is 'not a standard part of modern formalizations of mathematics'. Strangely, the idea of a dummy variable makes much more sense to me. For instance, when we say 'consider the function $f$ defined by $f(x)=x^2$ for all $x$ ', it is clear that the only purpose of the letter $x$ is to declare that the second entry of the ordered pair is the square of the first. If $y=x^2$ and $x \geq 0$ , then we just as well might write $x=\sqrt{y}$ . This flexibility is not really allowed when speaking of functions: $x \mapsto x^2$ and $x \mapsto \sqrt{x}$ are certainly not the same function. Perhaps it is possible evade this by treating $x$ as the independent variable and $y$ as the dependent variable. Some authors get around this ambiguity by saying that 'the function $y=x^2$ ' is just a shorthand for 'the function $y(x)=x^2$ ', which in turn is just a shorthand for 'the function $y$ defined by $y(x)=x^2$ for all x'. However, this doesn't seem to align with how people treat the relationship between $y$ and $x$ in practice. Even if we accept that $y=x^2$ is simply a shorthand for $y(x)=x^2$ , it still seems that there is a tendency towards treating $x$ as an independent variable representing the input of $y$ , as opposed to simply a dummy variable that can be replaced by any other letter. So I ask, in formal mathematics, is it possible to interpret $y=f(x)$ in such a way that $x$ and $y$ are variables representing the inputs and outputs of a function? And if so, how should the statement ' $y$ is a function of $x$ ' be understood?","['notation', 'functions', 'real-analysis']"
4024737,"If a two variable smooth function has two global minima, will it necessarily have a third critical point?","Assume that $f:\mathbb{R}^2\to\mathbb{R}$ a $C^{\infty}$ function that has exactly two minimum global points. Is it true that $f$ has always another critical point? A standard visualization trick is to imagine a terrain of height $f(x,y)$ at the point $(x,y)$ , and then imagine an endless rain pouring with water level rising steadily on the entire plane. Because there are only two global minima, they must both be isolated local minima also. Therefore, initially the water will collect into two small lakes around the minima. Those two points are connected by a compact line segment $K$ . As a continuous function, $f$ attains a maximum value $M$ on the set $K$ . This means that when the water level has reached $M$ , the two lakes will have been merged. The set $S$ of water levels $z$ such that two lakes are connected is thus non-empty and bounded from below. Therefore it has an infimum $m$ . It is natural to think that at water height $m$ there should be a critical point. A saddle point is easy to visualize. For example the function (originally suggested in a deleted answer) $f(x,y)=x^2+y^2(1-y)^2$ has a saddle point at the midway point between the two local minima at $(0,0)$ and $(0,1)$ . But, can we prove that one always exists? Follow-ups: Does the answer change, if we replace $\Bbb{R}^2$ with a compact domain? What if $f$ is a $C^\infty$ function on a torus ( $S^1\times S^1$ ) or the surface of a sphere ( $S^2$ ). Ok, on a compact domain the function will have a maximum, but if we assume only isolated critical points, what else is implied by the presence of two global minima? Similarly, what if we have local minima instead of global? If it makes a difference you are also welcome to introduce an extra condition (like when the domain is not compact you could still assume the derivatives to be bounded - not sure that would be at all relevant, but who knows).","['morse-theory', 'multivariable-calculus', 'differential-topology', 'real-analysis']"
4024749,Is a well-ordered set always countable?,"Suppose that $A$ is a well-ordered set. We can find $a_{0} = \min{A}$ . Then, if $A \setminus \left\{a_{0}\right\} \neq \emptyset$ , we can find $a_{1} = \min{\left(A \setminus \left\{a_{0}\right\}\right)}$ , etc. Intuitively, this seems to be a counting process. Then my hypothesis is, every well-ordered set is countable. Is this a valid guess? In my understanding, if it is valid, then for finite well-ordered sets, we can use mathematical induction to prove it. Then what about infinite cases?",['elementary-set-theory']
4024755,Why does adding an extra column to a matrix not change the RREF of the first columns?,"This resource (2.2.7) points out that deleting a column from a matrix would not change the RREF of the initial undeleted columns. For example, the RREF form of this $3\times3$ matrix is the $3\times3$ identity matrix. \begin{bmatrix}1&1&1\\1&0&1\\1&2&3\end{bmatrix} The RREF form of this matrix (the above with third column deleted) would be the $3\times 3$ identity matrix with the third column deleted, i.e. the RREF form of the first two colums are preserved. \begin{bmatrix}1&1\\1&0\\1&2\end{bmatrix} Is there a way to geometrically, or numerically indicate why the RREF form of individual columns are not changed by adding/deleting other columns? Intuitively, I believe this has something to do with the fact that adding another column to a matrix doesn't change the linear independence of the other columns with respect to each other. Furthermore, how does this connect to deleting or adding rows? Would messing with rows change the RREF form of columns?","['matrices', 'linear-algebra']"
4024767,Classification of analytic involutions,"Does there exist a classification of all analytic involutions? I only know $x\mapsto x$ , $x\mapsto -x$ , $x\mapsto\frac{1}{x}$ , and ""shifts"" thereof.","['involutions', 'functions', 'analytic-functions']"
4024798,Limit of a function defined at a single point,"Let's suppose we have a function defined at a single point $f : \left\{ 1 \right\} \to \left\{ 1 \right\}$ defined by $f(x) = x$ . Its graph is, therefore, composed of a single point $(1,1)$ . Does the following limit exist? $$\lim_{x \to 1} \ f(x)$$ I've read this post but it addresses the question in a topology perspective. I'd like to answer this considering a typical first semester Calculus course. Any help is highly appreciated.",['limits']
4024849,Number of ways to get a full house in Poker,"I'm trying to understand how to represent the number of ways to get a full house in poker without using the combinations method. I read that this is true: $\frac{52\times3\times2\times48\times3}{3!2!}=3744$ I understand where the numerator comes from but I'm not sure why we divide by "" $3!2!$ "". I'm assuming it's because we have 3 cards of the same value and 2 others with the same value? But I don't really understand why the division... I am also questioning why we couldn't start by doing $52\times51\times...$ because we could really pick any card to start with and then pick any of the remaining 51 as a second card (either same value as the first or any other)? I guess the third number in the calculation would depend on if we picked two same values or two different values but I'm all a bit confused about this. Thanks for your help!","['poker', 'combinatorics', 'discrete-mathematics']"
4024913,Quotienting $\mathbb Z[x]$ by a maximal ideal gives a finite field.,"Let $R = \mathbb Z[x]$ , a polynomial ring over $\mathbb Z$ , and $\mathfrak m$ be any maximal ideal of $R$ . How do we show that $R / \mathfrak m$ is a finite field? I know that the fact that it is a finite field directly follows, but not sure about the finite part.","['field-theory', 'ring-theory', 'abstract-algebra', 'polynomials']"
4024938,"Probability of $X > \max\{Y,Z\}$ exponentials","Let $X,$ $Y$ and $Z$ be exponentially distributed with $E[X] = 1/\lambda_X$ , $E[Y] = 1/\lambda_Y$ , $E[Z] = 1/\lambda_Z$ respectively. How do I calculate $P(X > \max\{Y,Z\})$ ? I have A, B, C and D for 3 cash boxes in a queueing system. Consider D the one waiting. I want to find the probability D is the last one to leave. The service times of the C.Boxes are indep. Exp with mean 5 (same mean for c.box 1, 2 and 3) That's how I'm trying to solve this: P(D last one) = P(T1 > max{T2,T3})*P(T1 < min{T2,T3}) + P(T2 > max{T1,T3})*P(T2 < min{T1,T3}) + P(T3 > max{T1,T2})*P(T3 < min{T1,T2}) And then: $$(\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4 = = 3(\frac{1}{2})^4 = 3\frac{1}{16} = \frac{3}{16}$$","['statistics', 'probability-distributions', 'exponential-distribution', 'probability']"
4024940,convex hull of roots of a polynomial.,"This question deals with some consequences of the Gauss-Lucas theorem. Suppose that $f:\mathbb{C}\rightarrow \mathbb{C}$ is a polynomial. If I know that $f^{\prime}$ has precisely two positive real roots, for instance, what exactly can I deduce from the Gauss-Lucas theorem about the roots of $f$ ? The only conclusion I can draw is that the convex hull of the roots of $f$ must enclose a portion of the positive real axis. My wish is to be able to say something about the roots of $f$ - how many exist, and whether they are (real) positive as well. It would also be nice to be able to completely describe the convex hull of the roots of $f$ .","['complex-numbers', 'polynomials', 'analysis', 'real-analysis']"
4024951,Basic Proof of Properties of Kravchuk Polynomials,"Fix $n$ and consider the family of polynomials $$
K_s(\ell) = \sum_{k=0}^s(-1)^k\binom{\ell}{k}\binom{n-\ell}{s-k} \quad \text{defined for} \quad 0 \leq s \leq n
$$ They are called Kravchuk (or Krawtchouk) polynomials. They are used in a paper I am trying to understand. The key properties of the polynomials that are used are that if you define the inner product $$
\langle f, g \rangle = \mathbf{E}_{k \leftarrow Bin(n,\frac{1}{2})}\big[f(k)g(k)\big]
$$ where $Bin(n,\frac{1}{2})$ is the binomial distribution,
on the vector space of functions $\mathbb R^{\{0,1,\ldots,n\}}$ then $$
\langle K_s, K_r\rangle = \begin{cases} \binom{n}{s} & r=s\\ 0 & r \neq s\end{cases}
$$ so that they form an orthogonal basis, and $$
\binom{n}{r}K_s(r) = \binom{n}{s}K_r(s)
$$ which allows some computations to be simplified. However, there seem to be no simple proofs of these facts anywhere; any proof I have found proves this as a special case of some much more general theory and it is too complicated for me to understand.
Can someone give a basic proof of these identities?","['orthogonal-polynomials', 'probability-theory', 'discrete-mathematics']"
4024970,expected value of minimum of sum random variable uniformly distributed variables,"Let $U_1$ , $U_2$ , $U_3$ ...  random variables uniformly distributed over the interval [0,1].
Define $n(x)$ as $n(x)$ = $\min$ $\{$$n$ | $\sum_1^n$ $U_i>x$$\}$ . I need to find $\Bbb E[n(x)]$ I tried to find $\Bbb E[n(x)]$ conditioning on the value of $U_1$ than i use the fact $\tag{2}\Bbb E \left[ n(x) | U_1=y\right] = \cases{1,& $y>x$\cr 1+\Bbb E(x-y),& $y\le x $}.$ but I'm struggling to build the integral, I don't know how to proceed any further than that. I appreciate any help. Thank you in advance.","['expected-value', 'probability-theory', 'probability']"
4025015,Why doesn't $f(x) = x^2$ generate a counter-example to $f(-x) = -f'(x)$?,"According to the chain rule, the derivative of $f(-x) = -f'(x)$ , since $$
\frac{d}{dx}(f(-x)) = \frac{df(u)}{du} \frac{du}{dx} = f'(-x)*-1 = -f'(-x)
$$ where $u = -x$ .  But doesn't the case of $f(x) = (x)^2$ generate a counter-example to this? $$
f'(-x) = \frac{d}{dx}(-x)^2 = \frac{d}{dx}x^2 = 2x \ne -(2x) = -f'(-x)
$$","['derivatives', 'chain-rule']"
4025024,What would be the graph of $P(x)=(x-x_1)(x-x_2)(x-x_3)(x-x_4)\cdots$ (with infinitely-many factors)?,"Suppose A polynomial $P(x)$ , such that, $$P(x)=(x-x_1)(x-x_2)(x-x_3)(x-x_4)\cdots$$ having infinite roots. Where $x_1$ , $x_2$ , $x_3$ , $x_4$ , $\ldots$ are roots of the polynomial. My question is what would be the graph of this function? Will it be just the $x$ -axis or there will some other curve?","['graphing-functions', 'roots', 'calculus', 'functions', 'polynomials']"
4025052,How to get the angle between the inner common tangents to two general ellipse,"I have been trying to find the angle made by inner tangents to two general ellipse by following the method described by Futurologist to find four homogeneous equations of tangents, this method is presented here and uses the concept of dual conics and degenerate conics. Using that method, I am able to get four tangent equations in homogeneous coordinates. However, I am looking to find the angle between the inner common tangents. For that purpose, currently, I am solving for the intersection of the four tangent lines with the ellipse and then tracing those points back to get the angle made by inner common tangents. This process is a bit computationally expensive and I am looking for a less expensive algorithm. My question here is that if it is possible to obtain that angle directly instead of going through the hassle of finding the intersection points? I am not interested in getting the equation of tangents. I just need the angle between them. Graphically in the diagram below, that angle can be represented as $\theta$ : Note: Using the slopes of the lines obtained from above method do not return correct answer because we do not know if that angle is greater one or smaller one  (two angle made by exterior or interior to those line), it depends on the relative position of the two conics. I want a generalize solution which will work for every configuration of two given conics anywhere in the 2D space.","['projective-geometry', 'conic-sections', 'geometry', 'differential-geometry']"
4025053,Consistency of the minimizer of the convex loss,"This is an exercise from the Stanford Stat 300B course, which I am auditing and don't have the solutions for. It is taken from the following source (Q2.6): https://web.stanford.edu/class/stats300b/Exercises/all-exercises.pdf . I have been stuck on the question for quite a while and think that I am missing some important part. Let $\mathcal{X}$ be a measurable space and $X_i \sim P$ be i.i.d, where $P$ is a probability distribution on $\mathcal{X}$ . Let $\Theta \subset \mathbb{R}^d$ be an open set and let $l :\Theta \times \mathcal{X} \to \mathbb{R}^{+}$ be a loss function. Define the risk functional $R(\theta) := E_P [l(\theta, X)]$ , which is the expected loss of a vector $\theta$ . Let $ \theta^* := \arg \min_{\theta\in\Theta} R(\theta)$ . Define the empirical risk functional as $\hat{R}_n := \frac{1}{n}\sum_1^n l(\theta, X_i)$ . Let $ \hat{\theta}_n := \arg \min_{\theta\in\Theta} \hat{R}_n(\theta)$ . We also have the following assumptions: $\theta \to l(\theta, x)$ is convex $\nabla^2 R(\theta^*) \succ 0$ i.e. the Hessian of the risk is positive definite at $\theta^*$ $\theta^* \in \text{interior of }\Theta$ There is a function $H : X → R^+$ such that $E[H^2(X)] < \infty$ and the Hessian $\nabla^2l(\theta, x)$ is $H(x)$ -Lipschitz in $\theta$ , that is, $$|| \nabla^2l(\theta, x) - \nabla^2l(\theta', x)||_{op} \leq H(x)||\theta - \theta'||~~\forall \theta,\theta'\in\Theta$$ Assume that gradients and Hessians can be passed through all expectations and integrals and as many moments of $\nabla l$ as you need. Also assume the following theorem holds true. If $f$ is convex and satisfies $\nabla^2 f(x) 
 - \lambda I\succ 0$ for all $x$ such that $||x - x_0|| \leq c$ , then: $$ f(x) \geq f(x_0) + \nabla f(x_0)(x - x_0) + \frac{\lambda}{2}\min(||x-x_0||^2, c||x-x_0||)$$ Need to show that under these assumptions: $\hat{\theta}_n \to \theta^*$ in probability. I feel hopelessly overwhelmed by all the details. I am not sure how to approach the problem.","['probability-distributions', 'convex-analysis', 'probability-theory', 'asymptotics']"
4025058,Envelope of a real function consisting of a complex function and its conjugate,"For a real function $f(x)=A(x)e^{ix}+\overline{A}(x)e^{-ix}$ , where $\overline{A}$ represents complex conjugate of $A$ . Note that $A(x)$ itself is a complex function $A(x)=A_r(x)+i A_i(x)$ . It seems that the envelope of $f(x)$ should be $\pm 2\vert A \vert$ , where $\vert \cdot \vert$ repersents the modulus of a complex number. That is, the envelope of $f(x)$ is double amplitude of $A$ if $f$ can be written as $A(x)e^{ix}+c.c.$ Now, I am confusing with the envelope of the real function $g=ff_x$ since it cannot be written in a similar form.
Substituting $f(x)$ into $g=ff_x$ , one obtains $g=A(A_x+iA)e^{2ix}+A\overline{A}_x+c.c.$ , where c.c. repersents the complex conjugate of the preceding terms.
How can I express the amplitude of the envelope of $g$ with $A$ , $\overline{A}$ and their derivatives? For the 1st term and its conjugate, the envelope should be $\pm 2 \vert A(A_x+iA)\vert$ . But I do not know how to include the 2nd term and its c.c. Can anyone please give me some suggestions? Thank you in advance!","['envelope', 'several-complex-variables', 'derivatives', 'complex-numbers']"
4025070,Strange triangle perimeter problem - Russian Olympiad 2010,"This problem came from Crux Mathematicorum - Canadian Mathematical Society. The perimeter of triangle $\triangle ABC$ is equal to $4$ . Points $X$ and $Y$ are marked on the rays $AB$ and $AC$ in such a way that $AX = AY = 1$ . The segments $BC$ and $XY$ intersect at point $M$ in their interior. Prove that the perimeter of one of the triangles $\triangle ABM$ or $\triangle AMC$ is equal to $2$ . I created $MN$ parallel to $AB$ , $N$ is on $AC$ . Then I calculated the two pairs of similar triangles introduced by this parallel segment. I also used Stewart to calculate $AM$ It didn't go anywhere.","['contest-math', 'euclidean-geometry', 'geometry']"
4025114,Derivative of $(A+BC^{-T}B^T)^{-1}BC^{-1}$,"Suppose $A:\mathbb{R}\rightarrow\mathbb{R}^{m\times m}$ , $B:\mathbb{R}\rightarrow\mathbb{R}^{m\times n}$ , and $C:\mathbb{R}\rightarrow\mathbb{R}^{n\times n}$ . I'm trying to find the derivative with respect to a scalar $x$ of \begin{equation}(A+BC^{-T}B^T)^{-1}BC^{-1}\end{equation} My matrix calculus is very rusty. I guess we can set \begin{equation}\phi=A+BC^{-T}B^T\end{equation} so that we have \begin{align}
\frac{\partial(\phi^{-1} B C^{-1})}{\partial x}&=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} + \phi^{-1}B\frac{\partial C^{-1}}{\partial x}\\
&=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} - \phi^{-1}BC^{-1}\frac{\partial C}{\partial x}C^{-1}.
\end{align} Now, we only need to find $\frac{\partial\phi^{-1}}{\partial x}$ . That is, \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}\end{equation} Is the following correct? \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}=-\left(A+BC^{-T}B^T\right)^{-1}\left[\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)\right] \left(A+BC^{-T}B^T\right)^{-1}\end{equation} where \begin{align}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)&=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T+B\frac{\partial C^{-T}}{\partial x}B^T+BC^{-T}\frac{\partial B^T}{\partial x}\\
&=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T-BC^{-T}\left(\frac{\partial C}{\partial x}\right)^{T}C^{-T}B^T+BC^{-T}\left(\frac{\partial B}{\partial x}\right)^T\end{align}","['matrices', 'matrix-calculus', 'derivatives']"
4025116,compute $ \iiint_Kxyz\ dxdydz$,"The question is: $$ \iiint_Kxyz\ dxdydz\quad k:=\{(x,y,z):x^2+y^2+z^2\leq1, \  \ x^2+y^2\leq z^2\leq 3(x^2+y^2), \ x,y,z\geq 0\} $$ Here how i have tried to solve this: $$\iint_{x^2+y^2\leq1}\int_{\sqrt{x^2+y^2}}^{\sqrt{3(x^2+y^2)}}xyz \ dz\ dxdy=\frac{1}{2}\iint xy\left(3(x^2+y^2)-(x^2+y^2)\right)=...=0$$ But the answer that i got is zero which is obviously wrong what is wrong with my solution?
any suggestion would be great, Thanks","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4025136,"Find the $\mathcal{O}\left(\frac{1}{n}\right)$ so that $a_n-1=\mathcal{O}\left(\frac{1}{n}\right)\,{\rm as}\,n\rightarrow\infty$","Given a recursion $a_{n+ 1}= \dfrac{a_{n}^{2}+ 1}{2}$ with $a_{1}= \dfrac{1}{2}.$ Find the $\mathcal{O}\left ( \dfrac{1}{n} \right )$ so that $$a_{n}- 1\sim\mathcal{O}\left ( \dfrac{1}{n} \right )\,{\rm as}\,n\rightarrow\infty$$ Remark. By this recurrence sequence, Ji Chen proposed 2 inequalities on AoPS, which I can't prove, of course $$1- \frac{2}{n}+ \frac{2}{n^{2}}\ln\frac{n}{3}+ \frac{417}{128n^{2}}\leq a_{n}\leq 1- \frac{2}{n}+ \frac{5\ln n+ 3}{n^{2}}$$ and for non-negative $x_{1\div n}$ quite not related $$\sum_{i= 1}^{n}\frac{x_{i}}{\left ( 1+ \sum_{j= 1}^{i}x_{j} \right )^{2}}\leq a_{n}^{2}$$ Return to the OP, I realise that $$a_{n+ 1}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ a_{n}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ \omega\left ( \left ( a_{n}- 1 \right )^{2} \right )$$ As same as what I told on this topic  * Find the $\mathcal{o}\left(n\right)$ so that $a_n\sim\frac{\mathcal{o}\left(n\right)}{n}+\mathcal{O}\left(\frac{1}{n}\right){\rm as}\lim a_n=\infty$ *, I still can't find such an $\mathcal{o}$ satisfying my wish. I need to your help, thanks.","['laurent-series', 'recurrence-relations', 'recursion', 'alternative-proof', 'limits']"
4025243,Bound of the Wave Equation in 3 dimensions,"For n=3, $u(x,t)\in C^2$ and $\Box u=0$ , $x\in\mathbb{R}^3,\ t\geq0$ . Assume U(t)= $\sum_{|a|\leq2}\int_{\mathbb{R}^3} |D^au(x,t)|dx<\infty$ for $t=0$ . a) Show that there exists a constant K independent of u such that: $|u(x,t)|\leq \frac{K}{t}U(0)$ for $t>0$ Hint: Write the integrand in Kirchoff's formula as: $\sum_i[\frac{1}{ct}(th(y)+g(y))(y_i-x_i)+ctg_{y_i}(y)]\xi_i $ where $\xi_i=\frac{y_i-x_i}{ct}$ are the directions cosines of the exterior surface normal. Convert the integral to one extended over the solid sphere $|y-x|<ct$ . b)Show that $\lim_{t\rightarrow\infty}\frac{U(t)}{t}=0$ Implies that u vanishes identically. Hint: Apply (a) to the function $v(x,t,T)=u(x,T-t)$ for large T. Exercise 4, page 133 of Partial Differential Equations, John F. My attempt was: $u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{\partial B}\frac{1}{2ct}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2 \xi_i+ctD_{y_i}g(y)\xi_i dS(y)$ . Then using Green's identities to transform it into: $u(x,t)= \frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{2ct}(th(y)+
g(y))\Delta_{y_i}\lvert y-x\rvert^2+ \frac{1}{2ct}D_{y_i}(th(y)+g(y))D_{y_i}\lvert y-x\rvert^2+ct \Delta_{y_i}g(y)\ dy$ $u(x,t)=\frac{1}{4\pi c^2 t^2}\sum\int_{B}\frac{1}{ct}(th(y)+g(y))+ \frac{1}{ct}D_{y_i}(th(y)+g(y))( y_i-x_i)+ct\Delta_{y_i}g(y)\ dy$ . $u(x,t)\leq\frac{1}{4\pi c^2 t}\int_R |\Delta g| + \frac{1}{t}|Dg| + \frac{3}{c^2t^2}|g| + |Dh| +\frac{3}{ct}|h|dy$ . For $ct>1$ there is a constant K such as $|u(x,t)|\leq\frac{K}{t}U(0)$ . Based on page 17 $U(0)=\int_R|g|+|h|+|Dg|+|Dh|+|D^2g|dy$ . But how is it computed? b) $v(x,t,T)=u(x,T-t)\implies v(x,T-t,T)=u(x,t)$ $|u(x,t)|=v(x,T-t,T)\leq\frac{K}{T-t}U(T)$ , therefore for $T\rightarrow\infty\implies |u(x,t)=0|$","['multivariable-calculus', 'partial-differential-equations', 'analysis', 'inequality']"
4025257,Showing a set is Borel and a function is Borel measurable.,"I am stuck in the following question: Suppose $f: B \rightarrow \mathbb{R}$ be a Borel measurable function. Let $g(x)=\begin{cases}
	f(x), & \text{if } x \in B,\\
	0,    & \text{if } x \in B^c
\end{cases}$ . I would like to first show that $B$ is a Borel set and that the function $g$ is Borel measurable. I know the required definitions, but I can't figure out how to put things together to solve this. Would someone please help?","['borel-sets', 'measure-theory', 'borel-measures']"
4025290,Is every open and connected set in $\mathbb C$ the continuous image of the open unit disk?,"Let $$
\mathbb D=\{z\in\mathbb C\ |\ |z|<1\}
$$ be the open unit disk in $\mathbb C$ . It is well known that an open (nonempty) set $U\subseteq\mathbb C$ is simply connected if and only if it is homeomorphic to the unit disk. One can show that this is equivalent to the fact that there is a continuous injective function $f:\mathbb D\to\mathbb C$ such that $f(\mathbb D)=U$ . Therefore, my question is: If $U$ is a (nonempty) open and connected subset of $\mathbb C$ , is there always a continuous (but not necessarily injective) function $f:\mathbb D\to\mathbb C$ such that $f(\mathbb D)=U$ ? It sufficices to assume $U\subseteq\mathbb D$ . If not, how do images of such functions look like? I know that any compact connected and locally connected subset of $\mathbb C$ is the image of a continuous function defined on $[0,1]$ and hence also the image of a continuous function on $\overline{\mathbb D}$ . However, the closure of an open and connected set does not have to be locally connected. I believe that the answer to my first question is negative (although I hope that it is not ;) ). Any help is highly appreciated. Thank you very much in advance!","['complex-analysis', 'connectedness', 'locally-connected']"
4025291,Finding length of diagonal in parallelogram given two sides and other diagonal,"A parallelogram has sides of length 11 cm and 13 cm and has one diagonal 20 cm long the length of the other diagonal is what I am supposed to find. Now my answer came out to be 13.2 cm. Finding the area of the first triangle: Perimeter = $11 + 13 + 20 = 44$ cm Semi-Perimeter = $44 ÷ 2 = 22$ cm Area = $\sqrt{p(p - a)(p - b) (p - c)} = \sqrt{22(22 - 11)(22 - 13)(22 - 20)} = \sqrt{4356} = 66 \text{cm}^2$ Find the area of the parallelogram: Area of parallelogram = $2 \times \ $ area of triangle Area = $66 * 2 = 132 cm²$ Find the other diagonal: Let the other diagonal be x $1/2 (diagonal 1 * diagonal 2) = Area $ $1/2 (20x) = 132 $ $10x = 132 $ $x = 13.2 cm $ However, surprisingly my answer wasn't in the options. Now my friend sent me this attachment from the website topper and claimed that 20 is the correct answer so can you guys pls tell me whether 13.2 cm is the correct answer or 20? Do pls also tell whether what is the mistake done by the other one? Thanks in advance for your help.","['euclidean-geometry', 'area', 'geometry', 'median']"
4025319,Applying Zorn's lemma in the proposition about cardinality of sets,"In Real Analysis book by Folland G., the following proposition is given: 0.7 Proposition. For any sets $X$ and $Y$ , either $\mathrm{card}(X)\leq\mathrm{card}(Y)$ or $\mathrm{card}(Y)\leq\mathrm{card}(X)$ . Proof. Consider the set $\mathcal{J}$ of all injections from subsets of $X$ to $Y$ . The members of $\mathcal{J}$ can be regarded as subsets of $X\times Y$ , so $\mathcal{J}$ is partially ordered by inclusion. It is easily verified that Zorn's lemma applies, so $\mathcal{J}$ has a maximal element $f$ , with (say) domain $A$ and range $B$ . If $x_0\in X\setminus A$ and $y_0\in Y\setminus B$ , then $f$ can be extended to an injection from $A\cup\{x_0\}$ to $Y\cup\{y_0\}$ by setting $f(x_0)=y_0$ , contradicting maximality. Hence either $A=X$ , in which case $\mathrm{card}(X)\leq\mathrm{card}(Y)$ , or $B=Y$ , in which case $f^{-1}$ is an injection from $Y$ to $X$ and $\mathrm{card}(Y)\leq\mathrm{card}(X)$ . $\qquad\square$ I know that Zorn's lemma is as follows: Zorn's lemma: Suppose a partially ordered set P has the property that every chain in P has an upper bound in P. Then the set P contains at least one maximal element. However, I am having difficulty in seeing how that quickly one can see that Zorn's lemma applies there as the textbook mentions. How can I show that any chain of $X\times Y$ has an upper bound? Could I say $X \times Y$ is itself the upper bound? More explanation on the above proof would be greatly appreciated.","['order-theory', 'discrete-mathematics', 'real-analysis']"
4025330,Find last remaining person in the game,"In a game, $N$ people are lined up in a circle and numbered from $0$ to $N-1$ . At each turn, the next $K$ th person, starting with the last eliminated person, is eliminated from the game. The first person to be eliminated is the $K$ th.
Find the last remaining person in the game.
Example: $N$ = 5, $K$ = 3 0 1 2 3 4 0 1 3 4 1 3 4 1 3 3 The last person is the 3rd one.",['discrete-mathematics']
4025343,"Number of solution sets to $\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}=1,~~a_1,a_2,\ldots,a_n\in\mathbb{N}$","Consider the Diophantine equation $$\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}=1$$ I'll give you the solutions sets $(a_1,a_2,\ldots,a_n)$ for $n=1$ up to $4$ , because beyond $n=4$ the number of solutions seem to explode. (These can be found quite easily by considering the smallest value of the $n$ variables and seeing just how large/small it can be, and then doing the same for the next smallest variable, and so on.) I will not consider permutations of each set as a separate solution set. $$\begin{align}
n=1:&~~~(1)\\
n=2:&~~~(2,2)\\
n=3:&~~~(3,3,3)~(2,3,6)~(2,4,4)\\
n=4:&~~~(4,4,4,4)~(3,3,4,12)~(3,3,6,6)~(3,4,4,6)~(2,3,7,42)\\&~~~(2,3,8,24)
~(2,3,9,18)~(2,3,10,15)~(2,3,12,12)~(2,4,5,20)\\
&~~~(2,4,6,12)~(2,4,8,8)~(2,5,5,10)~(2,6,6,6)
\end{align} $$ Denote by $a_n$ the number of solution sets we have for the Diophantine equation above for $n$ variables. So we have $$a_n=1,~~a_2=1,~~a_3=3,~~a_4=14$$ (Thanks for the correction @player3236).
And I suppose you could include $a_0=0$ if you really wanted, although I'm not sure how much sense that makes. My question is: Is there a formula that tells us the number of solutions sets for the Diophantine equation above? If so, how can I find and prove it? Thank you for your help.","['number-theory', 'elementary-number-theory', 'diophantine-equations', 'closed-form', 'algebra-precalculus']"
4025373,"Given $a,b\in \mathbb N$, can $a^2+a b+b^2$ and $a^2- a b+b^2$ both be square numbers?","The question arises if you search for two integer triangles ABC and AB'C with common vertices A and C, CA=b, CB=a, CB'=a+b and $\measuredangle ACB$ = $\measuredangle ACB'= 60°$ . With c=AB and c'=AB' you get $c^2=a^2- a b+b^2$ and $c'^2=a^2+ a b+b^2$ . I cannot find any $a,b\in \mathbb N$ such that $c,c'\in \mathbb N$ . Is there an argument that shows that this is impossible?","['number-theory', 'geometry']"
4025374,What is $\mathbb E[X\mid \boldsymbol 1_B]$?,"Let $B$ measurable. I know that $$\mathbb E[X\mid B]:=\frac{\mathbb E[X\boldsymbol 1_B]}{\mathbb P(B)}.$$ So, I guess that $$\mathbb E[X\mid \boldsymbol 1_B]=\mathbb E[X\mid B]\boldsymbol 1_B+\mathbb E[X\mid B^c]\boldsymbol 1_{B^c},$$ but I'm not so sure. My idea is the following : $$\mathbb E[X\mid \boldsymbol 1_B=1]=\mathbb E[X\mid B]\quad \text{and}\quad \mathbb E[X\mid \boldsymbol 1_{B^c}]=\mathbb E[X\mid B^c].$$ But in the other side, if this would be true, we would have $\mathbb E[X\mid \boldsymbol 1_B]=\mathbb E[X\mid \boldsymbol 1_{B^c}],$ and I really don't think it's true. So, any idea about $\mathbb E[X\mid \boldsymbol 1_B]$ ?","['expected-value', 'conditional-expectation', 'probability-theory']"
4025402,"Prove that for all bijective functions $\pi:\mathbb{N} \rightarrow \mathbb{N} \times \mathbb{N}$, $\sum_{n=1}^{\infty} a_{\pi (n)}$ is convergent.","I am tasked with this question: Let $$\sum_{n=1}^{\infty} a_{(i, n)} = a_{(i, 1)}+a_{(i, 2)}+a_{(i, 3)}+\dots$$ Such that $\sum_{n=1}^{\infty} a_{(i, n)}$ is absolutely convergent for each $i$ , and $\sum_{n=1}^{\infty} |a_{(i, n)}| = b_i.$ Furthermore, $\sum_{n=1}^{\infty} b_i$ is also convergent. Prove that for all bijective functions $\pi:\mathbb{N} \rightarrow \mathbb{N} \times \mathbb{N}$ , $\sum_{n=1}^{\infty} a_{\pi (n)}$ is convergent. My attempt: Since $\pi$ is bijective, every $a_{(i,n)}$ is represented in $\sum a_{\pi (n)}$ . Thus $\sum a_{\pi (n)} = \sum a_{(i,n)}$ . We take $\sum |a_{\pi (n)}| = \sum b_i$ , which is convergent. Since every absolutely convergent series is also just convergent, it follows that $\sum a_{(i,n)} = \sum a_{\pi (n)}$ is convergent. Is that just it? The weightage of the question is rather high, and I can't help but feel that I've made a mistake in reasoning. Anyway I can further 'flesh out' the proof in case something is missing?","['solution-verification', 'sequences-and-series', 'real-analysis']"
4025403,concentration inequality question,"Suppose that $g_1,\dots,g_m$ are i.i.d. $N(0,I_n)$ vectors. Let $$
X = \sum_{i\neq j} (\langle g_i,g_j \rangle^2 - n)
$$ It is known that when $m\to \infty$ , $n\to\infty$ , $m/n\to 0$ , $$
\frac{1}{2mn}X \to N(0,1) \quad \text{in distribution}.
$$ My question is, how can we obtain a concentration bound for $X$ , maybe something like $\Pr\{X > tmn\} \leq \exp(-ct^2)$ ? Is this possible?","['concentration-of-measure', 'probability-theory']"
4025428,Do all of the orbits have the same dimensions?,"Let $G$ be an algebraic group and let $X$ be a $G$ -variety. It's stated in the paper (pg. 13) that all orbits are closed and have the same dimensions if the graph $$\Gamma_{X}:=\{(g x, x) \mid g \in G, x \in X\}=\bigcup_{x \in X} G x \times G x \subset X \times X$$ is closed. They say that the statement about the dimensions is true because $$G x \times\{x\}=p_{2}^{-1}(x) \text { where } p_{2}: \Gamma_{X} \rightarrow X \text { is the second projection. }$$ But I don't understand this reasoning. Can someone explain it to me, please?","['algebraic-geometry', 'representation-theory', 'invariant-theory']"
4025495,Any memory efficient mapping function from a square 2d space to 1d space? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question I do programming questions but many a times I come across some problem in which I think I should make an effort to reduce both time and space complexity.
For example, I want to convert given coordinates (0,0 to n,n). Consider a square matrix. Now I want my program to mark the coordinates that I have already visited. For that I have to save the x,y coordinates in a pair<int,int> object. I was wondering if I could save them in a single primitive type variable.
E.g. x,y ---(Function 1)----> z   and  z--(Function 2)--> x,y Is this possible?
Here we should get a unique value of z for every different pair of x,y and for each value of z we should get unique coordinates x,y.
I want to implement these 2 functions, that do the forward and backward mapping.","['functions', 'abstract-algebra', 'special-functions', 'graphing-functions']"
4025534,"(X,X) independent and moments","I have a question regarding a random variable X being independent of itself.
I have to proof, knowing that X has no moment, that the following real number exists: $$ t_{0} = \inf_{} \{t\in \mathbb{R} | F_X(t) = 1 \}$$ Please, if my question is ambiguous or unclear, do not hesitate to edit it !
Thank you so much !","['statistics', 'probability', 'random-variables']"
4025548,Can I reduce fraction under integral?,Can I perform such kind of fraction reduction under integral? $$g(x)=\int \frac{d f(x)} {dx} dx = \int d f(x) = f(x) +C $$,"['integration', 'fractions', 'derivatives']"
4025559,How do I prove $\exp(1)=e$ if $\exp$ is defined in a series expansion?,"Right now I'm on a journey to clear up my confusion about exponential functions.
Thanks to your help in this stack , I was able to derive the basic properties of the ln function from the ""position of defining ln in terms of integrals"". In this stack, the definitions of exp and e are as follows; $$\exp(x):= 1+\frac{x}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\ldots,\ $$ $$e:=\lim_{n\to \infty} \left( 1+\frac{1}{n} \right)^{n} , n\in \mathbb{N}.$$ When defined in this way, the relationship between $\exp(x)$ and $e$ seems to be non-trivial .
We further assume that; The theory on limit operations such as the scissors-out theorem and the theory on convergence conditions for power series are assumed to exist separately and independently. For any integer $a>0$ where $n$ is an integer, $a^n$ is assumed to be defined in an inductive way. My question. Under these assumptions, how can we mathematically prove the following? $$\exp(1)=e$$ There is a similar stack , but it does not seem to go into the definition of Napier-number. If possible, we focus my questions on the following ""How to evaluate from the lower side. As far as I tried, I was probably able to suppress it from the upper side.
In other words, I believe I was able to prove the following. $$e\le \exp(1)$$ Therefore, perhaps the question is how to find the series to suppress from below. This means that the most desirable answer is to show $$e\ge \exp(1)$$ Proof of $e\le exp(1)$ From the binomial theorem, $$\left( 1+\frac{1}{n} \right)^{n}
=\sum_{k=0}^{n} {}_{n} \mathrm{C}_{k} \frac{1}{n^k}\\
=\sum_{k=0}^{n} \frac{1}{k!} \left( 1- \frac{1}{n} \right)\cdots \left( 1- \frac{1-k}{n} \right)\\
\le \sum_{k=0}^{n} \frac{1}{k!} 
\le \exp(1)
$$","['calculus', 'exponential-function', 'eulers-number-e']"
4025569,Analyse the asymptotic behavior of $\sum_{p\leq x} \frac{\log^2(p)}{p}$,"This question showed up in a number theory test from previous semesters so I might not know all the material needed to solve this question but here is my try: The function $f(x)=\frac{\log^2(x)}{x}$ is bounded in $(1,\infty)$ so for some upped bound $C$ on $f(x)$ we have $$\sum_{p\leq x, p\, \text{prime}} \frac{\log^2(p)}{p}\leq C\cdot \pi(x)$$ on the other hand for large enough $x$ we have $\min\left\{\frac{\log^2(p)}{p}|p\leq x, p\, \text{prime}\right\}\geq \frac{\log^2(x)}{x}$ and so: $$\sum_{p\leq x, p\, \text{prime}} \frac{\log^2(p)}{p}\geq \frac{\log^2(x)}{x} \pi(x)\stackrel{PNT}{=}O(\log(x))$$ Both bounds dont give a tight bound and here I am pretty much stuck.","['analytic-number-theory', 'number-theory']"
4025586,"$\lim_{n\to\infty} \frac{10^n}{\sqrt{(n+1)!} + \sqrt{n!}}\,.$","$\lim_{n\to\infty} \frac{10^n}{\sqrt{(n+1)!} + \sqrt{n!}}\,.$ I used the ratio criterion for the calculation and I got to this, can I say now that it is zero or is it still an undefined expression? $\frac{10+(10/\sqrt{(n+1)})}{\sqrt{(n+2)}+1}$ 10/inf < 1 ----> lim an=0",['limits']
4025644,Show that the sequence defined as $x_{n+1}=\sqrt[3]{x_n+x_{n-1}}$ converges,"Show that the sequence $(x_n)$ defined as $x_{n+1}=\sqrt[3]{x_n+x_{n-1}}, x_0=3,x_1=2$ converges. Here i wil try to prove that $(x_n)$ converges considering all details. So. i would like to have a critical feedback on my proof and would like to know if i missed something or did much more than it should be.. Here $\mathbf{R}^*$ will denote $\mathbf{R}\ \ \not \ \{0\}$ . Consider a function $f(x)=\sqrt[3]{x}$ . First, we show that it's defined on $\mathbf{R}$ and then we will show that $f(x)$ is an increasing function on $\mathbf{R}$ . We remark that $f(x)$ is differntiable on $\mathbf{R}^*$ . Let $a \in \mathbf{R}^*$ . By definition of differentiable function the $\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$ msut exist: $\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=\frac{1}{3\sqrt[3]{a^2}}$ and so it exists $\forall a \in \mathbf{R}^*$ as we chose an arbitrary $a$ . So $f(x)$ is differentiable on $\mathbf{R}^*$ and so is continious on $\mathbf{R}^*$ . We show now that $f(x)$ is continious on $0$ . To show that we pass by the definition : $\forall \varepsilon>0 \ \exists \delta>0 \ \forall x \in \mathbf{R}$ : $|x|<\delta \implies |f(x)|=|\sqrt[3]{x}|<\varepsilon$ To satisfy the definition, one can take $\delta=\varepsilon^3$ , so it holds and we can conclude that $f(x)$ is continious on $\mathbf{R}$ . Let $a,b \in \mathbf{R}$ such that $a<b$ . As $f(x)$ is continious on $\mathbf{R}$ we can write: $a<b \iff \sqrt[3]{a}<\sqrt[3]{b} \iff f(a)<f(b)$ As $a,b$ are arbitrary numbers (satisfying $a<b$ ), we can conclude that $f(x)$ is strictly incrasing function. Now, as all preliminary results are proven, we can show that $(x_n)$ is a convergent sequence. For that, we show that $(x_n)$ is monotonic and bounded. $\bullet$ $(x_n)$ is decreasing: Suppose $x_{n+1}<x_n$ . The previous inequality holds for $n=1$ so we can suppose that it works up to a certain $n\in \mathbf{N}^*$ . We show nowthat it holds for $n+1$ : $x_{n+2}<x_{n+1} \iff \sqrt[3]{x_{n+1}+x_n}<\sqrt[3]{x_n+x_{n-1}}$ . But, as $f(x)=\sqrt[3]{x}$ is strictly increasing function and by induction hypothesis $x_{n+1}<x_n$ , we can conclude that $x_{n+2}<x_{n+1}$ and so $x_{n+1}<x_n$ holds $\forall n\in\mathbf{N}^*$ . So $(x_n)$ is decreasing. $\bullet$ $(x_n)$ is bounded below: Suppose $x_n>1$ . It holds for $n=1$ so we can suppose that it works up to a certain $n\in \mathbf{N}^*$ . We want to show now that t holds for $n+1$ : $x_{n+1}>1 \iff \underbrace{x_{n}}_{>1}+\underbrace{x_{n-1}}_{>1}>1$ which is true, because $(x_n)$ is decreasing. So we conclude that $x_n>1 \ \forall n\in \mathbf{N}^*$ . So, we have shown that $(x_n)$ converges.","['limits', 'convergence-divergence', 'proof-writing', 'sequences-and-series']"
4025645,Are the orbits of this discrete dynamical system bounded?,"Is this a known result? If no such result (or similar result) is known, a proof or counterexample will suffice. Concise Version Conjecture: Let $f\colon \Bbb Z\to\Bbb Z$ be a function.
Assume that for some index set $I$ , there are $a_i,b_i,c_i,d_i\in\Bbb Z$ such that $\Bbb Z=\bigcup_{i\in I}(a_i+b_i\Bbb N_0)$ and $\Bbb Z=\bigcup_{i\in I}(c_i+d_i\Bbb N_0)$ For all $i\in I$ , $n\in\Bbb N_0$ , we have $f(a_i+b_in)=c_i+d_in$ For all $i\in I$ , we have $a_ib_i\ge 0$ and $c_id_i\ge 0$ For all $i\in I$ , if $b_i\ne d_i$ , then $\frac{a_i-c_i}{b_i-d_i}>0$ For all $i\in I$ , the set $\{\,f^{\circ k}(a_i)\mid k\in\Bbb N_0\,\}$ is finite Then for all $x\in \Bbb Z$ , the set $\{\,f^{\circ k}(x)\mid k\in\Bbb N_0\,\}$ is finite. Descriptive Version Given an iterative piecewise linear function, $f$ , with domain $\mathbb{Z}$ and range $\mathbb{Z}$ : under iteration by $f$ , if the trajectories/iterates of the first integers in all the subdomains converge to a cycle; then, the trajectories of all integers in the domain of $f$ will converge to a cycle, that is, no trajectory diverges.
- Special Conditions: (1) Zero can be included or excluded from $f$ . (2) It is assumed that the subdomains and subranges are in arithmetic progression containing non-negative or non-positive integers but not a mix. (3) It is also assumed that if $f$ has only two subfunctions then the cycle must be of length 1 and, of course, $f(x)\neq x$ unless $x$ is the element to which all the trajectories of the first elements converge. (4) Importantly, if one element in a subdomain is greater than its corresponding element in the subrange, then all elements in that subdomain must be greater than  their corresponding elements in the subrange. The same is true if one element in a subdomain is less than its corresponding element in the subrange. $\color{black}{\text{Example 1}}$ $$f(x) =
\begin{cases}
x+1,  & \text{subdoman: 1, 3, 5, 7,...Subrange: 2, 4, 6, 8,...} \\
-x+2, & \text{subdoman: 2, 4, 6, 8,...Subrange: 0,-2,-4,-6...}\\
-x, & \text{subdoman: -1, -3, -5,...Subrange: 1, 3, 5, ...}\\
x+1, & \text{subdoman: -2, -4, -6,...Subrange: -1, -3, -5,...}\\
0, & \text{subdoman: 0 $\hspace{1.6cm}$ Subrange: 0}
\end{cases}$$ Note that the trajectories of the first elements $(1, 2, -1$ and $-2)$ of the subdomains all converge to $0$ implying that no trajectory will diverge.
For example, the trajectory of $6$ under $f$ is: $6\to-4 \to-3\to3\to4\to-2\to-1\to1\to2\to0.$ $\color{black}{\text{Example 2}}$ Here is a second example in which the first elements of the subdomains converge to the cycle $-1, 1$ and $0$ is excluded: $$g(x) =
\begin{cases}
\frac{3}{2}x,  & \text{subdoman:2, 4, 6, 8,...Subrange: 3, 6, 9, 12...} \\
\frac{-3}{2}x+\frac{1}{2}, & \text{subdoman:1, 3, 5, 7,...Subrange:-1,-4,-7, -10...}\\
\frac{-1}{2}x+\frac{1}{2}, & \text{subdoman:-1, -3, -5,...Subrange:1, 2, 3, ...}\\
\frac{1}{2}x, & \text{subdoman:-2, -4, -6,...Subrange:-1, -2, -3,...}
\end{cases}$$ In this example, the trajectory of 6 is $6\to9\to-13\to7\to-10\to-5\to3\to-4\to-2\to-1\to1.$","['number-theory', 'convergence-divergence', 'vector-fields', 'dynamical-systems']"
4025651,Solve the equation in natural numbers $x^3+2x+1=y^2$,"The Question Solve the equation in natural numbers: $x^3+2x+1=y^2,x,y\in N$ The solution is: $x=1,y=2;x=8,y=23$ My Understanding The given equation can be written as $x(x^2 +2)=(y-1)(y+1)$ , as we're working in $\mathbb{N}$ we've the following cases: Case 1: $x=y-1$ and $x^2 +2 =y+1$ using these we get $y^2 -3y +2=(y-2)(y-1)=0$ therfore it follows that $(x,y)=(1,2)$ is the only solution in this case. Case 2: $x=y+1$ , and $x^2+2 =y-1$ , using these we get $y^2 +y+4=0$ . As the quadratic is irreducible over $\mathbb{Z}$ we can conclude the non existence of solutions in this case. Therefore we can conclude that the only solution in $\mathbb{N}^{2}$ is $(x,y)=(1,2)$ What about $(8,23)$ ?","['contest-math', 'number-theory', 'problem-solving', 'diophantine-equations']"
4025675,Ideal Class Group of $ \mathbb{Q}(\sqrt[3]{3}) $,"I'm trying to compute the ideal class group of $\mathbb Q(\sqrt[3]{3})$ , and I would like to know if my calculations are right and if I could improve my arguments. Let $K=\mathbb Q (\sqrt[3]{3}),$ its ring of integers $\mathcal O_K$ is $ \mathbb Z[\sqrt[3]{3}]$ , since Disc $(1,\sqrt[3]{3},\sqrt[3]{3^2})=-3^5 \quad $ and $f(x)=x^3-3 \ $ is Eisenstein for $3$ . The Minkowski bound is $\frac{4}{\pi}\cdot\frac{6}{27}\cdot \sqrt{3^5}<5$ . Thus we need to check ideals of norm less than $5$ . Set $a=\sqrt[3]{3}$ to facilitate notation. The only ideal of norm 1 is $\mathcal{O}_K$ Every ideal contains its norm, thus ideals of norm $2$ are prime factors of $(2). \quad $ $ (2)=(a-1)(a^2+a+1) \ $ , since $2=3-1=a^3-1=(a-1)(a^2+a+1). N(a-1)=2, N(a^2+a+1)=4 $ . $(3)=(a)^3$ and $(a)$ is principal. Since $(2)=(a-1)(a^2+a+1),$ ideals of norm 4 are: $(a-1)^2, \ (a^2+a+1).$ All of the ideals above are principal, thus the ideal class group is trivial and $\mathcal O_K$ is a PID.","['number-theory', 'ring-theory', 'abstract-algebra', 'algebraic-number-theory']"
4025692,"PDE: solve IVP where $u_{t} +3u u _{x} = 0$, $u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\ 0, x>1 \end{matrix} \right.$","I just started learning PDEs and I am at the very beginning of the book by Peter J. Olver. Now when I search in the stackexchange I found similar problems, all relating to Cauchy or Burger equations. The book so far has not spoken of these, so I guess I should be able to solve this without knowledge of them. I am asked to solve the IVP where $$u_{t} +3u u _{x} = 0, \quad \quad u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\  0, x>1 \end{matrix} \right.$$ My reasoning (likely to be wrong) so far is the following: we have a non-linear transport equation, where the speed of the wave is dictated by $3u$ (i.e. the wave is moving faster the bigger it is). We also know the solution is constant along the characteristic curve. We can see the wave being constant at $x>1$ but moving to the right when $x<1$ . This leads me to the following: $$
\begin{align}
\frac{ \partial x}{\partial t} &= 3u \\
\frac{\partial u }{\partial t} &= 0 \\
u(0,x) &= f(x),
\end{align}
$$ Then $u(t,x) = f(x_{0})$ and now I am stuck. I do not know how to proceed. Any suggestions or comments on my train of thought or on how to proceed would be much appreciated.","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
4025717,Let $x_n$ be a bounded sequence. Show that $y_n = n(x_{n+1} - x_n)$ can't have limit $+ \infty$,"I have tried to solve this problem by supposing by contradiction that the limit was $ + \infty$ . Then, you'd have $\forall M > 0, \exists N \in \mathbb{N}: \forall n > N, n(x_{n+1} - x_n) > M$ . This would mean that $(x_n)$ is increasing after a certain order $N$ , which means that it must be convergent. I don't know how to continue past this point.","['limits', 'sequences-and-series', 'real-analysis']"
4025748,"If $E$ has positive measure, then prove that there exists $h \in \mathbb{R}$ such that $|(E+h) \cap E| > 0$.","If $E$ has positive measure, then prove that there exists $h \in \mathbb{R}$ such that $|(E+h) \cap E| > 0$ . I tried doing this by contradiction but cannot seem to find my way about the argument. Any hints or help would be appreciated. Thanks.","['measure-theory', 'real-analysis']"
4025800,Bounding the difference between consecutive terms in the sequence $a_n=(1+1/n)^n$,"I'm pretty sure the following estimate holds for $C>1$ , but I can't prove it: $$a_n:=\left(1+\frac{1}{n}\right)^n-\left(1+\frac{1}{n-1}\right)^{n-1}\leq\frac{C}{n^2}$$ For context, I was looking for some such bound in order to prove that $na_n\to0$ . I tried using the fact that $$\left(1+\frac{1}{n}\right)^n\leq\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}+\cdots+\color{red}{\frac{1}{n!}}:=S_n$$ because, by some algebra with the binomial theorem, $$\left(1+\frac{1}{n}\right)^n=\color{red}{1}+\color{red}{1}+\color{red}{\frac{1}{2}}\color{blue}{\left(1-\frac{1}{n}\right)}+\cdots+\color{red}{\frac{1}{n!}}\color{blue}{\left(1-\frac{1}{n}\right)\cdots\left(1-\frac{n-1}{n}\right)}$$ where the blue factors are all less than one. But I also need a good lower bound on the terms $(1+n^{-1})^{n}$ , and I can't see how to pick the right lower bound to get, say, $$a_n\leq S_n-\text{lower bound applied to the term $\left(1+\frac{1}{n-1}\right)^{n-1}$}\leq\frac{C}{n^2}$$","['limits', 'inequality', 'upper-lower-bounds', 'real-analysis']"
4025883,Confused about partial derivatives,"I am having some issues understanding what should I keep constant and what not in certain cases when I take partial derivatives. Specifically in this kind of situation: say we have a function $$f(x,y) = x^3+7y^2$$ and we also know that $y=2x+1$ and we need to find the partial derivative with respect to $x$ at a given point, say $\frac{\partial f(2,3)}{\partial x}$ . From what I understood, when taking partial derivatives with respect to a variable, you need to keep the other constant, in which case, if I do that above (keeping $y$ constant) I would get, $\frac{\partial f(x,y)}{\partial x}=3x^2$ , so I get $12$ . However, if I plug $x$ in $y$ explicitly I would get $$f(x,y) = x^3+7(2x+1)^2$$ so, $$\frac{\partial f(x,y)}{\partial x}=3x^2+28(2x+1)$$ So, I get $152$ . What should I do? Thank you!","['partial-derivative', 'multivariable-calculus']"
4025888,Finding $\frac{DK}{DI}$,"In triangle $ABC,$ where $AB = 8, AC = 7,$ and $BC = 10,$ $I$ is the incenter. If $AI$ intersects $BC$ at $K$ and the circumcircle of $\triangle ABC$ at $D,$ find $\frac{DK}{DI}.$ I first drew a diagram, but I was unsure where to go from here.",['geometry']
4025912,How define the exterior derivative over a manifold with boundary?,"What shown below is a reference from the text Analysis on Manifolds by James Munkres Definition Let $S$ be a subset of $\Bbb R^k$ ; let $f:S\rightarrow\Bbb R^n$ . We say that $f$ is of class $C^r$ on $S$ if $f$ may be extended to a function $g:U\rightarrow\Bbb R^n$ that is of class $C^r$ on an open set $U$ of $\Bbb R^k$ containing $S$ . Lemma Let $U$ be open in $H^k$ but not in $\Bbb R^k$ ; let $\alpha:U\rightarrow\Bbb R^k$ be of class $C^r$ . Let $\beta:U'\rightarrow\Bbb R^n$ and $\gamma:U''\rightarrow\Bbb R^n$ be two $C^r$ extensions of $\alpha$ defined on an open subsets $U'$ and $U''$ of $\Bbb R^k$ . Then the derivatives of $\beta$ and $\gamma$ are equal in $U$ and in $\overset{\circ} U$ are equal to the derivative of $\alpha$ . Definition Given $x\in\Bbb R^n$ we define a tangent vector to $\Bbb R^n$ at $x$ to be a pair $(x;\vec v)$ where $\vec v\in\Bbb R^n$ . The set of all tangent vectors to $\Bbb R^n$ at $x$ forms a vector space if we define $$
(x;\vec v)+(x;\vec w):=(x;\vec v+\vec w)\\
c(x;\vec v):=(x;c\vec v)
$$ It is called the tangent space to $\Bbb R^n$ at $x$ and it is denoted $\mathcal T_x(\Bbb R^n)$ . Definition Let be $A$ open in $\Bbb R^k$ or in $H^k$ ; let $\alpha: A\rightarrow\Bbb R^n$ be of class $C^r$ . Let $x\in A$ , and let $p=\alpha(x)$ . We define a linear transformation $$
\alpha_*\mathcal T_x(\Bbb R^k)\rightarrow\mathcal T_p(\Bbb R^n)
$$ by the equation $$
\alpha_*(x;\vec v):=\big(p;D\alpha(x)\cdot\vec v\big)
$$ and we call it pushforward of $\alpha$ . Definition Let $M$ be a $k$ -manifold of class $C^r$ in $\Bbb R^n$ . If $p\in M$ chose a coordinate patch $\alpha:U\rightarrow V$ about $p$ , where $U$ is open in $\Bbb R^k$ or $H^k$ . Let $x$ be the point of $U$ such that $\alpha(x)=p$ . The set of all vectors of the form $\alpha_*(x;\vec v)$ where $\vec v$ is a vector in $\Bbb R^k$ is called the tangent space to $M$ at $p$ and it is denoted $\mathcal T_p(M)$ . Said differently $$
\mathcal T_p(M):=\alpha_*\big(\mathcal T_x(\Bbb R^k)\big)
$$ Definition Let $A$ be an open set $\Bbb R^n$ . A $k$ -tensor field in $A$ is a function $\omega$ assigning to each $x\in A$ a $k$ -tensor defined on the vector space $\mathcal T_x(\Bbb R^n)$ . That is, $$
\omega(x)\in\mathcal L^k\big(\mathcal T_x(\Bbb R^n)\big)
$$ for each $x\in A$ . Thus $\omega(x)$ is a function mapping $k$ -tuples of tangent vectors to $\Bbb R^n$ at $x$ into $\Bbb R$ ; as such, its value on a given $k$ -tuple can be written in the form $$
\big[\omega(x)\big]\big((x;\vec v_1),...,(x;\vec v_k)\big)
$$ We require this function to be continuous as a function of $(x,v_1,..,v_k)$ ; if it is of class $C^r$ we say that $\omega$ is a tensor field of class $C^r$ . If it happens that $\omega(x)$ is an alternanting $k$ -tensor for each $x$ , then $\omega$ is called a differential form of order $k$ on $A$ . More generally, if $M$ is an $m$ -manifold in $\Bbb R^n$ then we define a $k$ -tensor field on $M$ to be a function $\omega$ assigning to each $p\in M$ an element of $\mathcal L^k\big(\mathcal T_p(M)\big)$ . If in fact $\omega(p)$ is alternanting for each $p$ then $\omega$ is called a differential form on $M$ . In particular any tensor field on $M$ can be extended to a tensor field defined on an open set of $\Bbb R^n$ containing $M$ but the proof is decidedly non-trivial. Definition Let $A$ be open in $\Bbb R^k$ ; let $\alpha:A\rightarrow\Bbb R^n$ be of class $C^\infty$ ; let $B$ be an open set of $\Bbb R^n$ containing $\alpha[A]$ . We define the pull-back $\alpha^*$ of $\alpha$ to be a linear transformation $$
\alpha^*:\Omega^l(B)\rightarrow\Omega^l(A)
$$ such that if $f$ is a zero form on $B$ then $$
[\alpha^*f](x):=f\big(\alpha(x)\big)
$$ for each $x\in A$ and if $\omega$ is a $l$ -formon $B$ with $l>0$ then $$
\big[(\alpha^*\omega)(x)\big]\big((x;\vec v_1),...,(x;\vec v_l)\big):=\big[\omega\big(\alpha(x)\big)\big]\big(\alpha_*(x;\vec v_1),...,\alpha_*(x;\vec v_l)\big)
$$ for each $x\in A$ and for each $\vec v_1,..,\vec v_k\in\Bbb R^k$ . Now having defined the differential operator $d$ for forms defined over an open set let's go to define it for forms defined over a manifold. So previously we observe that the pushforward of a coordinate patch is a injective function and thus if $\omega$ is a $l$ -form defined over a $k$ -manifold we define a $(l+1)$ -form $d\omega$ by letting that $$
\big[d\omega(y)\big]\big((y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\big):=\Big[d(\alpha^*\omega)\big(\alpha^{-1}(y)\big)\Big]\big((\alpha_*)^{-1}(y;\vec v_1),...,(\alpha_*)^{-1}(y;\vec v_l),(\alpha_*)^{-1}(y;\vec v_{l+1})\big)
$$ for any $y\in M$ and for any $\vec v_1,..,\vec v_k,\vec v_{l+1}\in\Bbb R^n$ such that $(y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\in\mathcal T_y(M)$ . So since the domain of a coordinate patch of an interior point is a open subset of $\Bbb R^k$ the above position is consistent with what is above defined; anyway if $y$ is a boundary poin of $M$ then the domain of any coordinate patch about $y$ is not open in $\Bbb R^k$ so that the pull-back of this cordinate system is not defined and moreover if it was defined apparently it would carry a differential forms defined over a manifold to differetial forms defined over an open subset in $H^k$ that is not open in $\Bbb R^k$ so that the form $d(\alpha^*\omega)$ would not be defined. So could someone help me, please?","['exterior-derivative', 'manifolds-with-boundary', 'multivariable-calculus', 'differential-forms', 'differential-geometry']"
4025937,Proof of Clairaut’s theorem in Terence Tao Analysis 2,"I'm self-studying the book Real Analysis 2 by Terence Tao. I'm having some trouble working with vector-valued functions. Particularly, I'm stuck at his proof of Clairaut's theorem for a vector-valued function while having no math professor to ask :( screenshot 1 Theorem 6.5.4 (Clairaut's theorem). Let $E$ be an open subset of $\mathbf{R}^{n}$ , and let $f: E \rightarrow \mathbf{R}^{m}$ be a twice continuously differentiable function on
E. Then we have $\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}\left(x_{0}\right)=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}\left(x_{0}\right)$ for all $1 \leq i, j \leq n$ . In his proof, he proves for the case $\fbox{m=1}$ and $x_0 = 0$ . The beginning of the proof is easily to understood: screenshot 2 Let $a$ be the number $a:=\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(0),$ and $a^{\prime}$ denote the quantity $a^{\prime}:=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(0) .$ Our task is to show that $a^{\prime}=a$ .
Let $\varepsilon>0$ . Because the double derivatives of $f$ are continuous, we can find a $\delta>0$ such that $$
\left|\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(x)-a\right| \leq \varepsilon
$$ and $$
\left|\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(x)-a^{\prime}\right| \leq \varepsilon
$$ whenever $|x| \leq 2 \delta$ . Then comes the difficult part that I can't understand his notation: screenshot 3 Now we consider the quantity $$
X:=f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{i}\right)-f\left(\delta e_{j}\right)+f(0)
$$ From the fundamental theorem of calculus in the $e_{i}$ variable, we have $$
f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{j}\right)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right) d x_{i}
\qquad\color{red}{(*)}$$ and $$
f\left(\delta e_{i}\right)-f(0)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right) d x_{i}
$$ and hence $$
X=\int_{0}^{\delta}\left(\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right)-\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right)\right) d x_{i}.
$$ Where $e_i$ is a $n-dimensional$ vector with all zeros except the $1$ in the $i^{th}$ position. My question concerns the $\color{red}{(*)}$ equations: How to use the Fundamental theorem of calculus to lead to the (*) equation? Why the limit of the integral is from $0$ $\to$ $\delta$ and NOT from ( $\delta \, e_j$ $\to$ $\delta \, e_i + \delta \, e_j$ ) ? (like in the case of single variable integration: $F(b) - F(a) = \int \limits_{(a,b)} f $ ) ? ( $i.e$ In my opinion, we consider $b = \delta \, . e_i + \delta \, .e_j$ and $a= \delta \, .e_j$ , so the limit of the integration should be from $a$ to $b$ which is different from what is written in the text: $0$ $\to$ $\delta$ ) Why the ""main"" function to integrate is $\frac{\partial f}{\partial x_j}$ ? Why is it evaluated at the point ( $x_i*e_i + \delta * e_j$ )? Why the differential part in the integral is $dx_i$ ? Could you please suggest to me a $\fbox{document}$ or some $\fbox{keywords}$ so that I can read more to further understand the things related to the $\color{red}{(*)}$ equation? In the case where $m > 1$ , does the equation $\color{red}{(*)}$ still hold ? Is there any definition of integration of a vector-valued function? ( $i.e.$ $\int f dx$ where $f : \mathbb{R^n} \to \mathbb{R}^m$ with $m > 1$ and $x$ is a single variable. In other words, does an integration always take scalar- value or it can be a vector) ? Thank you very much for your help!","['integration', 'multivariable-calculus', 'vector-analysis', 'real-analysis']"
