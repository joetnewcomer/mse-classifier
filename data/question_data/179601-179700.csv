,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving a set of $2\times 3$ matrices is a manifold?,Proving a set of  matrices is a manifold?,2\times 3,"The way I have always been told to check if something is a manifold (I haven't had a whole lot of experience with them), is to check if the derivative of the function representing the loci of the graph satisfies the implicit function theorem. For example, the circle is a manifold because it can be expressed as $F(x,y)=x^2+y^2-1=0$ and this has a derivative of $D_F=[2x\  2y]$ which satisfies the implicit function theorem since it is onto. However, how could one show something more abstract is a manifold? Specifically, how can you show (even just hints are fine) that the set of all 2x3 matrices with rank one (subset of $\text{Mat}(2, 3)$) form a manifold in $\mathbb{R}^4$? Also, the notion of manifold for me I think is properly called an embedded submanifold or something of the sort","The way I have always been told to check if something is a manifold (I haven't had a whole lot of experience with them), is to check if the derivative of the function representing the loci of the graph satisfies the implicit function theorem. For example, the circle is a manifold because it can be expressed as $F(x,y)=x^2+y^2-1=0$ and this has a derivative of $D_F=[2x\  2y]$ which satisfies the implicit function theorem since it is onto. However, how could one show something more abstract is a manifold? Specifically, how can you show (even just hints are fine) that the set of all 2x3 matrices with rank one (subset of $\text{Mat}(2, 3)$) form a manifold in $\mathbb{R}^4$? Also, the notion of manifold for me I think is properly called an embedded submanifold or something of the sort",,"['matrices', 'multivariable-calculus', 'manifolds']"
1,Evaluate the path integral and interpret it geometrically?,Evaluate the path integral and interpret it geometrically?,,"Let $f(x,y)=2x-y$, and consider the path $x=t^4, y=t^4,-1 \leq t \leq 1$. (a) Compute the integral of $f$ along the path. What does this mean geometrically? My attempt: Let $\textbf c(t) = (t^4,t^4)$. We wish to integrate $f(x,y)=2x-y$ over this path: $$\int_{\textbf c}f\space ds = \int_a^bf\big(x(t),y(t)\big)\|\textbf c'(t)\| \space dt.$$ $$\int_{-1}^1 (2t^4-t^4)\sqrt{32t^6} \space dt = \sqrt{32}\int_{-1}^1 t^7 \space dt = 0.$$ But this is not the correct answer. I think I'm misinterpreting what $x=y=t^4$ means. (b) Evaluate the arc-length function $s(t)$ and redo part (a) in terms of $s$. What does the question mean by ""redo (a) in terms of $s$""?","Let $f(x,y)=2x-y$, and consider the path $x=t^4, y=t^4,-1 \leq t \leq 1$. (a) Compute the integral of $f$ along the path. What does this mean geometrically? My attempt: Let $\textbf c(t) = (t^4,t^4)$. We wish to integrate $f(x,y)=2x-y$ over this path: $$\int_{\textbf c}f\space ds = \int_a^bf\big(x(t),y(t)\big)\|\textbf c'(t)\| \space dt.$$ $$\int_{-1}^1 (2t^4-t^4)\sqrt{32t^6} \space dt = \sqrt{32}\int_{-1}^1 t^7 \space dt = 0.$$ But this is not the correct answer. I think I'm misinterpreting what $x=y=t^4$ means. (b) Evaluate the arc-length function $s(t)$ and redo part (a) in terms of $s$. What does the question mean by ""redo (a) in terms of $s$""?",,['multivariable-calculus']
2,The differential is NOT the Jacobi Matrix?,The differential is NOT the Jacobi Matrix?,,"In the book Analysis II by C.T. Michaels the differential is introduced as the Jacobi-Matrix. In class we had the following definition: Definition : Let $U \subset \mathbb{R}^m$ be open, $f: U \to \mathbb{R}^n$ , $x_0 \in U$ $f$ is at $x_0$ differentiable $ \iff \exists A: \mathbb{R}^m \to \mathbb{R}^n$ such that $ \displaystyle \lim_{x \to x_0}_{x \neq 0} \displaystyle \frac{f(x)-f(x_0)-A(x-x_0)}{||x-x_0||}=0$ Note : We call $A \in \hom(\mathbb{R}^m, \mathbb{R}^n)$ the differential of $f$ at point $x_0$ and we also write $A=df(x_0)$ My tutor said there are many books where the Jacobi-Matrix and the differential are said to be equal, but he mentioned they really are not. Oddly enough when I returned home after this Colloquium in Mathematics I did try to get some practice and just found the definition of the differential being equal to the Jacobi-Matrix as in C.T. Michaels. Now consider the following exercise (found in a paper by Salamon) Exercise : Show that $f$ is differentiable and compute the differential $df$ for all points in the domain: $$ f: \mathbb{R} \longrightarrow \mathbb{R}^2, \ f(x)= (ye^{ix}, xe^{iy}) $$ My approach : Showing that $f$ is differentiable is easy, I compute the Jacobi Matrix $$J_f= \begin{pmatrix}iye^{ix} \\ e^{iy} \end{pmatrix} \in \text{Mat}_{2,2}( \mathbb{C})  $$ And see that all the partial derivatives exists and are continuous $ \implies f$ is differentiable. Questions : How do I find the differential? If I plugin the Jacobi-Matrix into the definition above I can't seem to come up with the correct result Is it wrong to treat the Jacobi-Matrix and the differential as equal? (considering the definition as given in my class of course)","In the book Analysis II by C.T. Michaels the differential is introduced as the Jacobi-Matrix. In class we had the following definition: Definition : Let be open, , is at differentiable such that Note : We call the differential of at point and we also write My tutor said there are many books where the Jacobi-Matrix and the differential are said to be equal, but he mentioned they really are not. Oddly enough when I returned home after this Colloquium in Mathematics I did try to get some practice and just found the definition of the differential being equal to the Jacobi-Matrix as in C.T. Michaels. Now consider the following exercise (found in a paper by Salamon) Exercise : Show that is differentiable and compute the differential for all points in the domain: My approach : Showing that is differentiable is easy, I compute the Jacobi Matrix And see that all the partial derivatives exists and are continuous is differentiable. Questions : How do I find the differential? If I plugin the Jacobi-Matrix into the definition above I can't seem to come up with the correct result Is it wrong to treat the Jacobi-Matrix and the differential as equal? (considering the definition as given in my class of course)","U \subset \mathbb{R}^m f: U \to \mathbb{R}^n x_0 \in U f x_0  \iff \exists A: \mathbb{R}^m \to \mathbb{R}^n  \displaystyle \lim_{x \to x_0}_{x \neq 0} \displaystyle \frac{f(x)-f(x_0)-A(x-x_0)}{||x-x_0||}=0 A \in \hom(\mathbb{R}^m, \mathbb{R}^n) f x_0 A=df(x_0) f df  f: \mathbb{R} \longrightarrow \mathbb{R}^2, \ f(x)= (ye^{ix}, xe^{iy})  f J_f= \begin{pmatrix}iye^{ix} \\ e^{iy} \end{pmatrix} \in \text{Mat}_{2,2}( \mathbb{C})    \implies f","['real-analysis', 'analysis', 'multivariable-calculus', 'definition', 'self-learning']"
3,Line integral with Stokes,Line integral with Stokes,,"Let $C$ be curve $(x-1)^2 + (y-2)^2 =4$ and $z=4$ orientated counterclockwise when viewed from high on the z-axis. Let $$\mathbf{F}(x,y,z)=(z^2 +y^2 +\sin x^2)\mathbf{i}+(2xy+xz)\mathbf{j}+(xz+2yz)\mathbf{k} $$ Evaluate $\oint_C \mathbf{F}\bullet d\mathbf{r}$ My work: Normal $\mathbf{\hat{N}}$ is $\mathbf{k}$, $dS=dA$ and the projection to $xy$-plane is a circle with area of $4\pi$ and curl $\mathbf{F}=(2z-x)\mathbf{i}+z\mathbf{j}+\mathbf{k} $. By Stokes theorem, $$\begin{align} \oint_C \mathbf{F}\bullet d\mathbf{r}&=\iint_S ((2z-x)\mathbf{i}+z\mathbf{j}+z\mathbf{k})\bullet\mathbf{k}\; dA \\ &=\int_0^{2\pi}\int_0^2 z r \;drd\phi,\; \text{$z=4$ on $S$ } \\ &=4\times4\pi=16\pi\end{align}$$ Does my work seem correct? Im not that sure if the fact that the circle is not origo centered changes something.","Let $C$ be curve $(x-1)^2 + (y-2)^2 =4$ and $z=4$ orientated counterclockwise when viewed from high on the z-axis. Let $$\mathbf{F}(x,y,z)=(z^2 +y^2 +\sin x^2)\mathbf{i}+(2xy+xz)\mathbf{j}+(xz+2yz)\mathbf{k} $$ Evaluate $\oint_C \mathbf{F}\bullet d\mathbf{r}$ My work: Normal $\mathbf{\hat{N}}$ is $\mathbf{k}$, $dS=dA$ and the projection to $xy$-plane is a circle with area of $4\pi$ and curl $\mathbf{F}=(2z-x)\mathbf{i}+z\mathbf{j}+\mathbf{k} $. By Stokes theorem, $$\begin{align} \oint_C \mathbf{F}\bullet d\mathbf{r}&=\iint_S ((2z-x)\mathbf{i}+z\mathbf{j}+z\mathbf{k})\bullet\mathbf{k}\; dA \\ &=\int_0^{2\pi}\int_0^2 z r \;drd\phi,\; \text{$z=4$ on $S$ } \\ &=4\times4\pi=16\pi\end{align}$$ Does my work seem correct? Im not that sure if the fact that the circle is not origo centered changes something.",,"['multivariable-calculus', 'vector-analysis']"
4,Question with Stokes theorem,Question with Stokes theorem,,"Show with Stokes that $\oint_C (y\mathbf{i}+z\mathbf{j}+x\mathbf{k})\bullet d\mathbf{r}=\sqrt{3}\pi a^2$ when $C$ is intersection of $x^2+y^2+z^2=a^2$ and $x+y+z=0$. My work: $$z=g(x,y)=-x-y$$ $$\mathbf{N}=\frac{-\frac{\partial g}{\partial x}\mathbf{i}-\frac{\partial g}{\partial y}\mathbf{j}+\mathbf{k}}{\sqrt{1+\left(\frac{\partial g}{\partial x}\right)^2+\left(\frac{\partial g}{\partial y}\right)^2}}=\pm\frac{1}{\sqrt 3}(\mathbf{i}+\mathbf{j}+\mathbf{k})$$ $$dS=\sqrt{1+\left(\frac{\partial g}{\partial x}\right)^2+\left(\frac{\partial g}{\partial y}\right)^2}dA=\pm\sqrt 3 dA$$ So stokes says that: $$\oint_C (y\mathbf{i}+z\mathbf{j}+x\mathbf{k})\bullet d\mathbf{r}=\iint_R \nabla \times \mathbf{F}\bullet\mathbf{N}dS$$ $$\begin{align} \iint_R \nabla \times \mathbf{F}\bullet\mathbf{N}dS&=\iint_R (-\mathbf{i}-\mathbf{j}-\mathbf{k})\bullet(\pm\frac{1}{\sqrt 3}(\mathbf{i}+\mathbf{j}+\mathbf{k}))\pm\sqrt{3}dA \\ &=\pm\sqrt{3} \iint_R \pm\frac{3}{\sqrt{3}} dA \\ &=3 \iint_R 1 dA \\ &=3\int_0^{2\pi}d\phi\int_0^{a}rdr=3\pi r^2\end{align}$$ Soo... I got it wrong. Can someone explain what my mistake was?","Show with Stokes that $\oint_C (y\mathbf{i}+z\mathbf{j}+x\mathbf{k})\bullet d\mathbf{r}=\sqrt{3}\pi a^2$ when $C$ is intersection of $x^2+y^2+z^2=a^2$ and $x+y+z=0$. My work: $$z=g(x,y)=-x-y$$ $$\mathbf{N}=\frac{-\frac{\partial g}{\partial x}\mathbf{i}-\frac{\partial g}{\partial y}\mathbf{j}+\mathbf{k}}{\sqrt{1+\left(\frac{\partial g}{\partial x}\right)^2+\left(\frac{\partial g}{\partial y}\right)^2}}=\pm\frac{1}{\sqrt 3}(\mathbf{i}+\mathbf{j}+\mathbf{k})$$ $$dS=\sqrt{1+\left(\frac{\partial g}{\partial x}\right)^2+\left(\frac{\partial g}{\partial y}\right)^2}dA=\pm\sqrt 3 dA$$ So stokes says that: $$\oint_C (y\mathbf{i}+z\mathbf{j}+x\mathbf{k})\bullet d\mathbf{r}=\iint_R \nabla \times \mathbf{F}\bullet\mathbf{N}dS$$ $$\begin{align} \iint_R \nabla \times \mathbf{F}\bullet\mathbf{N}dS&=\iint_R (-\mathbf{i}-\mathbf{j}-\mathbf{k})\bullet(\pm\frac{1}{\sqrt 3}(\mathbf{i}+\mathbf{j}+\mathbf{k}))\pm\sqrt{3}dA \\ &=\pm\sqrt{3} \iint_R \pm\frac{3}{\sqrt{3}} dA \\ &=3 \iint_R 1 dA \\ &=3\int_0^{2\pi}d\phi\int_0^{a}rdr=3\pi r^2\end{align}$$ Soo... I got it wrong. Can someone explain what my mistake was?",,"['multivariable-calculus', 'vector-analysis']"
5,"Problem using Stokes's Theorem : Boundary Curve, Unit Normal Vector","Problem using Stokes's Theorem : Boundary Curve, Unit Normal Vector",,"Source: Stewart, James. Calculus: Early Transcendentals (6 edn 2007) . p. 1097. ยง16.8, Exercise #5. $\Large{1.}$ How does one determine the boundary curve, called C, to be the plane $z = -1$? I don't understand, as $S$ here is given as bottomless. $\Large{2.}$ Why isn't the unit normal vector for $S_2$ be $-\mathbf{k}$ ? How does one determine which direction, either green or purple, is correct for the red person, without flipping the figure vertically? Can someone please explain more informally than Christian Blatter's answer that looks too complicated?","Source: Stewart, James. Calculus: Early Transcendentals (6 edn 2007) . p. 1097. ยง16.8, Exercise #5. $\Large{1.}$ How does one determine the boundary curve, called C, to be the plane $z = -1$? I don't understand, as $S$ here is given as bottomless. $\Large{2.}$ Why isn't the unit normal vector for $S_2$ be $-\mathbf{k}$ ? How does one determine which direction, either green or purple, is correct for the red person, without flipping the figure vertically? Can someone please explain more informally than Christian Blatter's answer that looks too complicated?",,['multivariable-calculus']
6,Question about Implicit function theorem,Question about Implicit function theorem,,"I was asked a simple question, show that $y+\sin y=x$ sets in the neighborhood of $(0,0)$ $y$ as a function of $x$, and find $\dfrac{dy}{dx}(0,0)$ Firstly, my naive solution would be: Since $lim_{y \to 0} \frac{\sin y}{y} = 1$ I want to say that in the vicinity of $(0,0)$ $y=\sin y$ and then we get the equation $2y=x$, $y=\dfrac{1}{2}x$, and so $\dfrac{dy}{dx}(0,0)=\dfrac{1}{2}$ But if I was to do this question using the implicit fucntion theorem I have a problem. Let's define $f(x,y)=y+\sin y-x$ Let's look at the matrix of the partial derivatives: $\begin{pmatrix} \dfrac{df}{dx}(0,0) & \dfrac{df}{dy}(0,0)\end{pmatrix} = \begin{pmatrix} -1 & 2 \end{pmatrix}$ and of course this matrix is not invertible, it is not even a square matrix. So my questions are: 1) Why is my naive solution wrong? 2) Why isn't the jacobi matrix a square matrix? 3) Even if it was an invertible matrix, how would I find $y$ as a function of $x$ in order to find $\dfrac{dy}{dx}$?","I was asked a simple question, show that $y+\sin y=x$ sets in the neighborhood of $(0,0)$ $y$ as a function of $x$, and find $\dfrac{dy}{dx}(0,0)$ Firstly, my naive solution would be: Since $lim_{y \to 0} \frac{\sin y}{y} = 1$ I want to say that in the vicinity of $(0,0)$ $y=\sin y$ and then we get the equation $2y=x$, $y=\dfrac{1}{2}x$, and so $\dfrac{dy}{dx}(0,0)=\dfrac{1}{2}$ But if I was to do this question using the implicit fucntion theorem I have a problem. Let's define $f(x,y)=y+\sin y-x$ Let's look at the matrix of the partial derivatives: $\begin{pmatrix} \dfrac{df}{dx}(0,0) & \dfrac{df}{dy}(0,0)\end{pmatrix} = \begin{pmatrix} -1 & 2 \end{pmatrix}$ and of course this matrix is not invertible, it is not even a square matrix. So my questions are: 1) Why is my naive solution wrong? 2) Why isn't the jacobi matrix a square matrix? 3) Even if it was an invertible matrix, how would I find $y$ as a function of $x$ in order to find $\dfrac{dy}{dx}$?",,"['matrices', 'multivariable-calculus', 'implicit-function-theorem']"
7,How to intuitivly think of graphing a function in $\Bbb{R}^3$,How to intuitivly think of graphing a function in,\Bbb{R}^3,"How to intuitivly think of graphing a function in $\mathbb R^3$? Let there be $f: \mathbb R \rightarrow \mathbb R^3$ $$f(t)=\begin{bmatrix}         \cos(t) \\         \sin(t) \\         t \\         \end{bmatrix}$$ Graph $f$. How do I even think about it? I thought that it's a cylinder with made of unit circles at different levels of $t$, but that doesn't seem right anymore... Also, just a question about a term: What's a parameterized curve and what is it different from other curves? Thanks!","How to intuitivly think of graphing a function in $\mathbb R^3$? Let there be $f: \mathbb R \rightarrow \mathbb R^3$ $$f(t)=\begin{bmatrix}         \cos(t) \\         \sin(t) \\         t \\         \end{bmatrix}$$ Graph $f$. How do I even think about it? I thought that it's a cylinder with made of unit circles at different levels of $t$, but that doesn't seem right anymore... Also, just a question about a term: What's a parameterized curve and what is it different from other curves? Thanks!",,"['multivariable-calculus', 'graphing-functions']"
8,Analytic functions and antiderivative,Analytic functions and antiderivative,,"Suppose $g(z)$ is analytic on all of $\mathbb{C}$ except at finitely many points $z_1, ..., z_n$ satisfying $|z_i| < 1$ for all $i$. Show that $g(z)$ has an analytic antiderivative on $|z| > 1$ if and only if $\int_{|z|=1} g(z) dz = 0$.","Suppose $g(z)$ is analytic on all of $\mathbb{C}$ except at finitely many points $z_1, ..., z_n$ satisfying $|z_i| < 1$ for all $i$. Show that $g(z)$ has an analytic antiderivative on $|z| > 1$ if and only if $\int_{|z|=1} g(z) dz = 0$.",,"['real-analysis', 'complex-analysis', 'multivariable-calculus']"
9,Show that $\nabla [f(r)]=f'(r)\frac {\mathbf{r}}{r}$,Show that,\nabla [f(r)]=f'(r)\frac {\mathbf{r}}{r},"Let $\mathbf{r} = xi+yj+zk$, write $r= \|\mathbf{r}\|$ and let $f:\mathbb{R}\to\mathbb{R}$ be a function of class $C^1$ So from what I know, we can derive the function at least once and we know gradients are just the derivative of the function with respect to each variable . Anyways $$r=\sqrt{x^2+y^2+z^2}$$ now replacing $$\nabla f\left(\sqrt{x^2+y^2+z^2}\right)$$ where do I go from here to get the proof? I feel like I'm overthinking this. The follow up is to use the answer from the above to calculate $\nabla \left(\frac{r}{\sin r}\right)$. I am guessing $$\nabla f(r)=\nabla f\left(\frac{r}{\sin r}\right)=f'\left(\frac{r}{\sin r}\right) \frac{\|\frac{r}{\sin r}\|}{\frac{r}{\sin r}}$$","Let $\mathbf{r} = xi+yj+zk$, write $r= \|\mathbf{r}\|$ and let $f:\mathbb{R}\to\mathbb{R}$ be a function of class $C^1$ So from what I know, we can derive the function at least once and we know gradients are just the derivative of the function with respect to each variable . Anyways $$r=\sqrt{x^2+y^2+z^2}$$ now replacing $$\nabla f\left(\sqrt{x^2+y^2+z^2}\right)$$ where do I go from here to get the proof? I feel like I'm overthinking this. The follow up is to use the answer from the above to calculate $\nabla \left(\frac{r}{\sin r}\right)$. I am guessing $$\nabla f(r)=\nabla f\left(\frac{r}{\sin r}\right)=f'\left(\frac{r}{\sin r}\right) \frac{\|\frac{r}{\sin r}\|}{\frac{r}{\sin r}}$$",,['multivariable-calculus']
10,Reading advice for Advanced Calculus by Loomis and Sternberg,Reading advice for Advanced Calculus by Loomis and Sternberg,,"I am a math major currently in my sophomore year. I have a sound base in one variable calculus and basic linear algebra. I am currently doing a course in multivariable calculus. I have completed reading Calculus by Apostol Vol 1 and 2 and reached nearly half-way through Baby Rudin. I am currently thinking about reading Advanced Calculus by Loomis and Sternberg mainly because of its vast table of contents and applications in Classical Mechanics (I am a physics minor too). After going through the first few chapters lightly, I am certain that it would take a whole semester to complete this book. My question is: Is it worth spending the whole semester (5 months) in this book?","I am a math major currently in my sophomore year. I have a sound base in one variable calculus and basic linear algebra. I am currently doing a course in multivariable calculus. I have completed reading Calculus by Apostol Vol 1 and 2 and reached nearly half-way through Baby Rudin. I am currently thinking about reading Advanced Calculus by Loomis and Sternberg mainly because of its vast table of contents and applications in Classical Mechanics (I am a physics minor too). After going through the first few chapters lightly, I am certain that it would take a whole semester to complete this book. My question is: Is it worth spending the whole semester (5 months) in this book?",,"['calculus', 'multivariable-calculus']"
11,Differentiability of a multivariable function,Differentiability of a multivariable function,,"I want to study differentiability of $f$ at the origin: $$ f(x,y) = ( x^3 + y^3)^{1/3} $$ MY attempt: I claim $f$ is not differentiable at the origin because its partial derivatives are not defined at the origin. Is this correct?","I want to study differentiability of $f$ at the origin: $$ f(x,y) = ( x^3 + y^3)^{1/3} $$ MY attempt: I claim $f$ is not differentiable at the origin because its partial derivatives are not defined at the origin. Is this correct?",,['calculus']
12,Local maximality implies global maximality?,Local maximality implies global maximality?,,"Let $S$ be the unit sphere in $\mathbb R^n.$ For a given $A\in\operatorname{M}_n(\mathbb R),$ define $f:\mathbb R^n\mapsto\mathbb R$ as $f(x)=\langle x,Ax\rangle.$ Suppose $a\in S$ is an element such that $\exists \delta\gt0,$ with $f(a)\ge f(x), \forall x\in\mathbb R^n$ such that $\|x-a\|\lt\delta.$ Then is it true that $f(a)\ge f(x), \forall x\in S?$ Intuitively, I think this must fail, as we are only given a local maximiser. However, I failed to find a counter-example, nor a proof. If I can calculate the derivative of $f,$ maybe I can conclude that $f$ has no more than one local maximiser, which thus must be a global maximiser. But I do not know about $f'$, either. Thanks very much in advance for any hint or reference.","Let $S$ be the unit sphere in $\mathbb R^n.$ For a given $A\in\operatorname{M}_n(\mathbb R),$ define $f:\mathbb R^n\mapsto\mathbb R$ as $f(x)=\langle x,Ax\rangle.$ Suppose $a\in S$ is an element such that $\exists \delta\gt0,$ with $f(a)\ge f(x), \forall x\in\mathbb R^n$ such that $\|x-a\|\lt\delta.$ Then is it true that $f(a)\ge f(x), \forall x\in S?$ Intuitively, I think this must fail, as we are only given a local maximiser. However, I failed to find a counter-example, nor a proof. If I can calculate the derivative of $f,$ maybe I can conclude that $f$ has no more than one local maximiser, which thus must be a global maximiser. But I do not know about $f'$, either. Thanks very much in advance for any hint or reference.",,"['matrices', 'multivariable-calculus']"
13,Show that the natural map between vectors and linear functions on $\mathbb R^n$ is bijective,Show that the natural map between vectors and linear functions on  is bijective,\mathbb R^n,"I'm doing this problem in Spivak's Calculus on Manifolds and I need help with what the second statement is trying to say and how it is proven. *Let $(\mathbb{R}^n)^*$ denote the dual space of the vector space $\mathbb{R}^n$ . If $x \in \mathbb{R}^n$, define $\varphi_x \in (\mathbb{R}^n)^*$ by $\varphi_x(y) =\langle x,y\rangle$.  Define $T:\mathbb{R}^n \rightarrow (\mathbb{R}^n)^*$ by $T(x) = \varphi_x$. Show that $T$ is a $1 - 1$ linear transformation and conclude that every $\varphi$ $\in$ $(\mathbb{R}^n)^*$ is $\varphi_{x}$ for a unique $x \in \mathbb{R}^n$. I understand how to do the first part in proving that T is a linear transformation, but the second statement confuses me even when I look at the proof. I know this may be trivial, but I would appreciate any help in clarifying what the statement is trying to say as well as why its proved by showing that  $\varphi_{ax + by}$ = $a\varphi_{x} + b\varphi{y}$. Thanks!","I'm doing this problem in Spivak's Calculus on Manifolds and I need help with what the second statement is trying to say and how it is proven. *Let $(\mathbb{R}^n)^*$ denote the dual space of the vector space $\mathbb{R}^n$ . If $x \in \mathbb{R}^n$, define $\varphi_x \in (\mathbb{R}^n)^*$ by $\varphi_x(y) =\langle x,y\rangle$.  Define $T:\mathbb{R}^n \rightarrow (\mathbb{R}^n)^*$ by $T(x) = \varphi_x$. Show that $T$ is a $1 - 1$ linear transformation and conclude that every $\varphi$ $\in$ $(\mathbb{R}^n)^*$ is $\varphi_{x}$ for a unique $x \in \mathbb{R}^n$. I understand how to do the first part in proving that T is a linear transformation, but the second statement confuses me even when I look at the proof. I know this may be trivial, but I would appreciate any help in clarifying what the statement is trying to say as well as why its proved by showing that  $\varphi_{ax + by}$ = $a\varphi_{x} + b\varphi{y}$. Thanks!",,"['linear-algebra', 'multivariable-calculus']"
14,"proof of$\frac{\partial^2 f(x,y)}{\partial x\partial y}$=$\frac{\partial^2 f(x,y)}{\partial y\partial x}$",proof of=,"\frac{\partial^2 f(x,y)}{\partial x\partial y} \frac{\partial^2 f(x,y)}{\partial y\partial x}","I was at my physics class(electrodynamics).I saw a relation which frequently uses in my course.Relation is that $$\frac{\partial^2 f(x,y)}{\partial x\partial y}=\frac{\partial^2 f(x,y)}{\partial y\partial x}$$.I saw this relations frequently used in the derivation of different formula for multiple gradient,curl etc. which is fundamental for many other important theorems.My doubt is that is it true for all function?.What is the original proof for this relation.My thought is that it is not true for any function.there may be some function which does not satisfy this relation.If there is such function why we uses this relation in many physical problem.any link or answer would be appreciated.","I was at my physics class(electrodynamics).I saw a relation which frequently uses in my course.Relation is that $$\frac{\partial^2 f(x,y)}{\partial x\partial y}=\frac{\partial^2 f(x,y)}{\partial y\partial x}$$.I saw this relations frequently used in the derivation of different formula for multiple gradient,curl etc. which is fundamental for many other important theorems.My doubt is that is it true for all function?.What is the original proof for this relation.My thought is that it is not true for any function.there may be some function which does not satisfy this relation.If there is such function why we uses this relation in many physical problem.any link or answer would be appreciated.",,"['calculus', 'multivariable-calculus', 'proof-writing', 'mathematical-physics']"
15,Integration limits of the double integral after conversion to the polar coordinates,Integration limits of the double integral after conversion to the polar coordinates,,"I want to solve the following double integral: $$\int_0^{\infty}dx\int_{-\infty}^{\infty}dy\,f(x,y).$$ And for example I made a conversion to the polar coordinates, $x=r\cos{\theta}$ and $y=r\sin{\theta}$ and get a new integral where I plugged in new variables and multiplied the Integral with the Jacobian factor, in this case $r$. How could I find new integration limits in terms of $r$ and $\theta$ based on the integration limits in terms of $x$ and $y$?","I want to solve the following double integral: $$\int_0^{\infty}dx\int_{-\infty}^{\infty}dy\,f(x,y).$$ And for example I made a conversion to the polar coordinates, $x=r\cos{\theta}$ and $y=r\sin{\theta}$ and get a new integral where I plugged in new variables and multiplied the Integral with the Jacobian factor, in this case $r$. How could I find new integration limits in terms of $r$ and $\theta$ based on the integration limits in terms of $x$ and $y$?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
16,Using Lagrange multipliers to maximize function,Using Lagrange multipliers to maximize function,,"Use Lagrange multipliers to maximize function $$f(x,y)=6xy,$$ subject to the constraint $$2x+3y=24.$$ $$F(x,y,\lambda)=6xy+\lambda(2x+3y-24)$$ $$F_{x}=6y+2\lambda=0$$ $$F_{y}=6x+3\lambda=0$$ $$y = - \frac{1}{3} \lambda$$ $$x = - \frac{1}{2} \lambda$$ $$\frac{3}{2}y= - \frac{1}{3} \left( \frac{3}{2} \right) \lambda = - \frac{1}{2} \lambda=x$$ I'm not sure where the $(3y)/2$ came from. Can someone help me with this? I know what to do after. Thank you so much.","Use Lagrange multipliers to maximize function $$f(x,y)=6xy,$$ subject to the constraint $$2x+3y=24.$$ $$F(x,y,\lambda)=6xy+\lambda(2x+3y-24)$$ $$F_{x}=6y+2\lambda=0$$ $$F_{y}=6x+3\lambda=0$$ $$y = - \frac{1}{3} \lambda$$ $$x = - \frac{1}{2} \lambda$$ $$\frac{3}{2}y= - \frac{1}{3} \left( \frac{3}{2} \right) \lambda = - \frac{1}{2} \lambda=x$$ I'm not sure where the $(3y)/2$ came from. Can someone help me with this? I know what to do after. Thank you so much.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
17,Prove $(\vec A \times \vec B) \cdot (\vec C \times \vec D) = (\vec A \cdot \vec C)(\vec B \cdot \vec D) - (\vec A \cdot \vec D)(\vec C \cdot \vec B)$,Prove,(\vec A \times \vec B) \cdot (\vec C \times \vec D) = (\vec A \cdot \vec C)(\vec B \cdot \vec D) - (\vec A \cdot \vec D)(\vec C \cdot \vec B),"Prove that $(\vec A \times \vec B) \cdot (\vec C \times \vec D) = (\vec A \cdot \vec C)(\vec B \cdot \vec D) - (\vec A \cdot \vec D)(\vec C \cdot \vec B)$. The problem asks to prove this only using the properties: $ \text{(i)}\space (\vec a \times \vec b) \times \vec c = (\vec a \cdot \vec c)\vec b - (\vec b \cdot \vec c)\vec a \\ \text{(ii)}\space \vec a \times (\vec b \times \vec c) = (\vec a \cdot \vec c)\vec b - (\vec a \cdot \vec b)\vec c \\ \text{(iii)}\space \vec u \cdot (\vec v \times \vec w) = \vec v \cdot (\vec w \times \vec u) = \vec w \cdot (\vec u \times \vec v) = -\vec u \cdot (\vec w \times \vec v) = -\vec w \cdot (\vec v \times \vec u) = -\vec v \cdot (\vec u \times \vec w)$ I've tried manipulating the left hand side in all the ways I could think of, and I can't seem to reach the right hand side. Can someone please point me in the right direction?","Prove that $(\vec A \times \vec B) \cdot (\vec C \times \vec D) = (\vec A \cdot \vec C)(\vec B \cdot \vec D) - (\vec A \cdot \vec D)(\vec C \cdot \vec B)$. The problem asks to prove this only using the properties: $ \text{(i)}\space (\vec a \times \vec b) \times \vec c = (\vec a \cdot \vec c)\vec b - (\vec b \cdot \vec c)\vec a \\ \text{(ii)}\space \vec a \times (\vec b \times \vec c) = (\vec a \cdot \vec c)\vec b - (\vec a \cdot \vec b)\vec c \\ \text{(iii)}\space \vec u \cdot (\vec v \times \vec w) = \vec v \cdot (\vec w \times \vec u) = \vec w \cdot (\vec u \times \vec v) = -\vec u \cdot (\vec w \times \vec v) = -\vec w \cdot (\vec v \times \vec u) = -\vec v \cdot (\vec u \times \vec w)$ I've tried manipulating the left hand side in all the ways I could think of, and I can't seem to reach the right hand side. Can someone please point me in the right direction?",,['multivariable-calculus']
18,Is the hessian negative semi-definite if we have an interior maximum?,Is the hessian negative semi-definite if we have an interior maximum?,,"Is it true that given a smooth scalar field f on a domain D , if f attains a maximum (minimum) on the interior of D then the hessian of f evaluated at this max (min)  is negative (positive) semi-definite? I have seen this quoted as a fact but usually when i try to find a proof all that comes up is the second derivative test (given hessian is negative definite then we have max) is this actually just equivalent to the test? EDIT: if we consider an approximation via taylor's theorem we have (near $ a $ ) : $$ f(x) = f(a) + Df(a)^{T} (x-a)  + (x-a)^{T} D^2 f(a) (x-a) + o(|x-a|^3)$$ now if $a$ is max then$ f(x) \le f(a) $ so $ D^2 f(a) $ is negative semidef ?","Is it true that given a smooth scalar field f on a domain D , if f attains a maximum (minimum) on the interior of D then the hessian of f evaluated at this max (min)  is negative (positive) semi-definite? I have seen this quoted as a fact but usually when i try to find a proof all that comes up is the second derivative test (given hessian is negative definite then we have max) is this actually just equivalent to the test? EDIT: if we consider an approximation via taylor's theorem we have (near $ a $ ) : $$ f(x) = f(a) + Df(a)^{T} (x-a)  + (x-a)^{T} D^2 f(a) (x-a) + o(|x-a|^3)$$ now if $a$ is max then$ f(x) \le f(a) $ so $ D^2 f(a) $ is negative semidef ?",,"['multivariable-calculus', 'optimization']"
19,Continuity in multivariable calculus,Continuity in multivariable calculus,,"I want to find out the points, where the function $f(x,y)=\dfrac{xy}{x-y}$ if $x\neq y$ and $f(x,y)=0$ otherwise, is continuous. I have shown that at all the points $(x,y)$, where $x\neq y$, $f$ is continuous. Also at all those points $(x,y)\in \mathbb R^2\setminus \{(0,0)\}$ such that $x=y$, $f$ is not continuous. But what would happen at $(0,0)$? I couldn't do. Please give a hint.","I want to find out the points, where the function $f(x,y)=\dfrac{xy}{x-y}$ if $x\neq y$ and $f(x,y)=0$ otherwise, is continuous. I have shown that at all the points $(x,y)$, where $x\neq y$, $f$ is continuous. Also at all those points $(x,y)\in \mathbb R^2\setminus \{(0,0)\}$ such that $x=y$, $f$ is not continuous. But what would happen at $(0,0)$? I couldn't do. Please give a hint.",,"['real-analysis', 'multivariable-calculus', 'continuity']"
20,Finding critical points of multivariable function,Finding critical points of multivariable function,,"Find the critical points of $f(x,y)=x^y+4xy-y^2-8x-6y$ I found the derivative of the function and got $$f^\prime_x=yx^{y-1}+4y-8  \\ f^\prime_y=\ln x\, x^y+4x-2y-6 $$. I want to find point $(x_0,y_0)$ s.t $f^\prime_x(x_0,y_0)=f^\prime_y(x_0,y_0)=0$. I isolated $x^y$ in both equations and got $x^y=\dfrac{2y+6-4x}{\ln x}=\dfrac{8x-4xy}{y}$, but I can't proceed any further (I get implicit function).  How can I find the critical points?","Find the critical points of $f(x,y)=x^y+4xy-y^2-8x-6y$ I found the derivative of the function and got $$f^\prime_x=yx^{y-1}+4y-8  \\ f^\prime_y=\ln x\, x^y+4x-2y-6 $$. I want to find point $(x_0,y_0)$ s.t $f^\prime_x(x_0,y_0)=f^\prime_y(x_0,y_0)=0$. I isolated $x^y$ in both equations and got $x^y=\dfrac{2y+6-4x}{\ln x}=\dfrac{8x-4xy}{y}$, but I can't proceed any further (I get implicit function).  How can I find the critical points?",,[]
21,How would you prove $E = -\vec{\nabla} V$ from the electric potential's line integral?,How would you prove  from the electric potential's line integral?,E = -\vec{\nabla} V,"Basically, we were given an equation: $$V_1 - V_2 = \int_{r_1}^{r_2}\vec{E}\cdot d\vec{r}$$ where $\vec{E}$ is the electric field distribution and $d\vec{r}$ is the displacement vector of the charge. Suppose that, in a 3D space, the electric field is $$\vec{E} = E_x \hat{i} + E_y \hat{j} + E_z \hat{k}$$ and the displacement vector is $$d\vec{r} = d_x \hat{i} + d_y \hat{j} + d_z \hat{k}$$ Given $V = V(x, y, z)$, we have to prove that $$E = -\vec{\nabla} V$$ Which make a whole lot of sense, considering that the equations for gradient theorem and line integral state so. However, assuming I have no clue what the gradient theorem is, how would I go about proving $E = -\vec{\nabla} V$ using only the given equations above? I know that if $V$ and $E$ were merely functions of $x$: $$V(x) = \int_{x_o}^{x} E(x)dx$$ we can differentiate both sides with respect to the upper limit to get: $$\frac{\partial V}{\partial x} = -E(x)$$ But how would I go about doing that if it was a 3D space? Do I need to use some electrostatic concepts, or is this all just basic calculus that I've long forgotten? I think I'm really overthinking this because my brain says it's simple, yet I just can't think of a way.","Basically, we were given an equation: $$V_1 - V_2 = \int_{r_1}^{r_2}\vec{E}\cdot d\vec{r}$$ where $\vec{E}$ is the electric field distribution and $d\vec{r}$ is the displacement vector of the charge. Suppose that, in a 3D space, the electric field is $$\vec{E} = E_x \hat{i} + E_y \hat{j} + E_z \hat{k}$$ and the displacement vector is $$d\vec{r} = d_x \hat{i} + d_y \hat{j} + d_z \hat{k}$$ Given $V = V(x, y, z)$, we have to prove that $$E = -\vec{\nabla} V$$ Which make a whole lot of sense, considering that the equations for gradient theorem and line integral state so. However, assuming I have no clue what the gradient theorem is, how would I go about proving $E = -\vec{\nabla} V$ using only the given equations above? I know that if $V$ and $E$ were merely functions of $x$: $$V(x) = \int_{x_o}^{x} E(x)dx$$ we can differentiate both sides with respect to the upper limit to get: $$\frac{\partial V}{\partial x} = -E(x)$$ But how would I go about doing that if it was a 3D space? Do I need to use some electrostatic concepts, or is this all just basic calculus that I've long forgotten? I think I'm really overthinking this because my brain says it's simple, yet I just can't think of a way.",,"['multivariable-calculus', 'physics']"
22,combining Gauss and Stokes theorems leads to nonsense,combining Gauss and Stokes theorems leads to nonsense,,"Gauss Theorem: $$\int_S \vec{a} \, d\vec{S}=\int_V\operatorname{div}(\vec{a})\,dV$$ Stokes theorem: $$\int_C \vec{a}\,d\vec{l}=\int_S\operatorname{curl}(\vec{a})\,d\vec{S}$$ Combining together: $$\int_C \vec{a}\,d\vec{l}=\int_S \operatorname{curl}(\vec{a}) d\vec{S} = \int_V \operatorname{div}(\operatorname{curl}(\vec{a}))\,dV=0$$ since $\operatorname{div}(\operatorname{curl}(\vec{a}))=0$, whats wrong here?","Gauss Theorem: $$\int_S \vec{a} \, d\vec{S}=\int_V\operatorname{div}(\vec{a})\,dV$$ Stokes theorem: $$\int_C \vec{a}\,d\vec{l}=\int_S\operatorname{curl}(\vec{a})\,d\vec{S}$$ Combining together: $$\int_C \vec{a}\,d\vec{l}=\int_S \operatorname{curl}(\vec{a}) d\vec{S} = \int_V \operatorname{div}(\operatorname{curl}(\vec{a}))\,dV=0$$ since $\operatorname{div}(\operatorname{curl}(\vec{a}))=0$, whats wrong here?",,"['calculus', 'multivariable-calculus']"
23,Deriving an equation for solid of revolution,Deriving an equation for solid of revolution,,"I was wondering, if there is any generic method that will help me find an explicit formula for a region bounded by a solid of revolution. For example: If I am given $z=x^2 $ which is a parabola, and rotate it by the $z$ axis, I obviously get the paraboloid $ z=x^2 + y^2 $ . But, how can I get the equation $z=x^2 + y^2 $ apriori, only by knowing that the resulted figure will be the solid of revolution of $z=x^2 $ . In general, given a figure of the form $ z=f(x) $ that rotates about the $z$ axis for example. How can I find an explicit formula that will represent the obtained solid of revolution ? Hope you'll be able to help Thanks in advance","I was wondering, if there is any generic method that will help me find an explicit formula for a region bounded by a solid of revolution. For example: If I am given $z=x^2 $ which is a parabola, and rotate it by the $z$ axis, I obviously get the paraboloid $ z=x^2 + y^2 $ . But, how can I get the equation $z=x^2 + y^2 $ apriori, only by knowing that the resulted figure will be the solid of revolution of $z=x^2 $ . In general, given a figure of the form $ z=f(x) $ that rotates about the $z$ axis for example. How can I find an explicit formula that will represent the obtained solid of revolution ? Hope you'll be able to help Thanks in advance",,"['integration', 'multivariable-calculus']"
24,"Vector Calculus (Gradients, Potential Functions, and Equipotential Curves)","Vector Calculus (Gradients, Potential Functions, and Equipotential Curves)",,"This is my first question on the Mathematics section of StackExchange, so please forgive me if I don't follow all the rules or things like that. Here's my question: Consider the following potential function and graph of its equipotential curves: $\phi(x,y)=x^2+2y^2$ Find the associated gradient field $F=\nabla\phi$ Show that the vector field is orthogonal to the equipotential curve at the point $(1,1)$. Illustrate this result on the picture. Show that the vector field is orthogonal to the equipotential curve at all points $(x,y)$. Sketch two flow curves representing $F$ that are everywhere orthogonal to the equipotential curves. Here's some of the equipotential curves: Thank you so much in advance for helping me!","This is my first question on the Mathematics section of StackExchange, so please forgive me if I don't follow all the rules or things like that. Here's my question: Consider the following potential function and graph of its equipotential curves: $\phi(x,y)=x^2+2y^2$ Find the associated gradient field $F=\nabla\phi$ Show that the vector field is orthogonal to the equipotential curve at the point $(1,1)$. Illustrate this result on the picture. Show that the vector field is orthogonal to the equipotential curve at all points $(x,y)$. Sketch two flow curves representing $F$ that are everywhere orthogonal to the equipotential curves. Here's some of the equipotential curves: Thank you so much in advance for helping me!",,['multivariable-calculus']
25,Deduce 1d wave equation from 2d by method of descent,Deduce 1d wave equation from 2d by method of descent,,"Suppose we take as given that the formula for the wave equation on $\mathbb{R}^2$ is $$u(x,t) = \frac{1}{2\pi} \int_{|y-x|<t} \frac{g(y)}{\sqrt{t^2-|y-x|^2}}dy+ \frac{d}{dt}\left\{  \frac{1}{2\pi} \int_{|y-x|<t} \frac{f(y)}{\sqrt{t^2-|y-x|^2}}dy  \right\},$$ where $u=f(x)$ at $t=0$, and $u_t=g(x)$ at $t=0$. We want to deduce the 1d wave equation by the method of descent. This is how I understand the method of descent: suppose $u(x,t)$ solves $u_{xx}=u_{tt}$ on $\mathbb{R}$ such that $u=f(x)$ at $t=0$, and $u_t=g(x)$ at $t=0$. Extend $f$ and $g$ to $\mathbb{R}^2$ by making their values independent of the $x_2$ coordinate. Then I end up with $$u(x,t) = \frac{1}{2\pi} \int_{|y-x|<t} \frac{g(y_1)}{\sqrt{t^2-\left( y_2^2+(y_1-x)^2 \right)}}dy \\+ \frac{d}{dt}\left\{  \frac{1}{2\pi} \int_{|y-x|<t} \frac{f(y_1)}{\sqrt{t^2-\left( y_2^2+(y_1-x)^2 \right)}}dy  \right\}.$$ The ideas I have for this are Changing variables so I am integrating in the variable $x+ry$ over $y\in B_1(0)$ in $0\leq r \leq t$ (might make it easier to differentiate under the integral). But still the integral is not working. Converting to polar somehow, in which case the denominator becomes $\sqrt{t^2-r^2}$, but then the numerator becomes $g(r\cos\theta)$, which is screwy. Somehow using Green's theorem to change it into an integral over the boundary $\{|y-x|=t\}$ and then parameterizing (as in the 3d--> 2d descent). But it's hard to find a primitive. Don't work with this formula at all, and instead try to derive a solution some other way Any ideas? Of course we should end up with $$u(x,t) = \frac12 [f(x+t)+f(x-t)] + \frac12 \int_{x-t}^{x+t}g(\xi) d\xi.$$","Suppose we take as given that the formula for the wave equation on $\mathbb{R}^2$ is $$u(x,t) = \frac{1}{2\pi} \int_{|y-x|<t} \frac{g(y)}{\sqrt{t^2-|y-x|^2}}dy+ \frac{d}{dt}\left\{  \frac{1}{2\pi} \int_{|y-x|<t} \frac{f(y)}{\sqrt{t^2-|y-x|^2}}dy  \right\},$$ where $u=f(x)$ at $t=0$, and $u_t=g(x)$ at $t=0$. We want to deduce the 1d wave equation by the method of descent. This is how I understand the method of descent: suppose $u(x,t)$ solves $u_{xx}=u_{tt}$ on $\mathbb{R}$ such that $u=f(x)$ at $t=0$, and $u_t=g(x)$ at $t=0$. Extend $f$ and $g$ to $\mathbb{R}^2$ by making their values independent of the $x_2$ coordinate. Then I end up with $$u(x,t) = \frac{1}{2\pi} \int_{|y-x|<t} \frac{g(y_1)}{\sqrt{t^2-\left( y_2^2+(y_1-x)^2 \right)}}dy \\+ \frac{d}{dt}\left\{  \frac{1}{2\pi} \int_{|y-x|<t} \frac{f(y_1)}{\sqrt{t^2-\left( y_2^2+(y_1-x)^2 \right)}}dy  \right\}.$$ The ideas I have for this are Changing variables so I am integrating in the variable $x+ry$ over $y\in B_1(0)$ in $0\leq r \leq t$ (might make it easier to differentiate under the integral). But still the integral is not working. Converting to polar somehow, in which case the denominator becomes $\sqrt{t^2-r^2}$, but then the numerator becomes $g(r\cos\theta)$, which is screwy. Somehow using Green's theorem to change it into an integral over the boundary $\{|y-x|=t\}$ and then parameterizing (as in the 3d--> 2d descent). But it's hard to find a primitive. Don't work with this formula at all, and instead try to derive a solution some other way Any ideas? Of course we should end up with $$u(x,t) = \frac12 [f(x+t)+f(x-t)] + \frac12 \int_{x-t}^{x+t}g(\xi) d\xi.$$",,"['multivariable-calculus', 'partial-differential-equations']"
26,Determine continuity and differentiability,Determine continuity and differentiability,,"Could you tell me how to determine continuity and differentiability of functions with two or more variables? I know how to do it for fairly simple functions, such as $f(x,y)=\sqrt{x^k + y^k}$, $f(x,y)=\sin (x^k + y^k)$ and for polynomials continuity is obvious. We check if the function and its partial derivatives are continuous. For functions for which it is evident that they are a composition of continuous functions, it is easy to establish continuity. I have problems determining continuity of more ambitious functions, say $f(x,y)  = \sqrt{xy}(\sqrt{x^2+y^2})^{-1}$ for $(x,y)$ in the domain of this function and $f(x,y) = 0$  elsewhere. I've also seen functions of this type $(|x|^ky) \cdot (x^2 + x^4)^{-1} $, and I also don't know how to deal with them. How to calculate $\lim _{(x,y)\rightarrow (0,0)}f(x,y)$ here? I would appreciate it if you explained to me how to solve such problems on an example of the functions I've mentioned above. Thank you.","Could you tell me how to determine continuity and differentiability of functions with two or more variables? I know how to do it for fairly simple functions, such as $f(x,y)=\sqrt{x^k + y^k}$, $f(x,y)=\sin (x^k + y^k)$ and for polynomials continuity is obvious. We check if the function and its partial derivatives are continuous. For functions for which it is evident that they are a composition of continuous functions, it is easy to establish continuity. I have problems determining continuity of more ambitious functions, say $f(x,y)  = \sqrt{xy}(\sqrt{x^2+y^2})^{-1}$ for $(x,y)$ in the domain of this function and $f(x,y) = 0$  elsewhere. I've also seen functions of this type $(|x|^ky) \cdot (x^2 + x^4)^{-1} $, and I also don't know how to deal with them. How to calculate $\lim _{(x,y)\rightarrow (0,0)}f(x,y)$ here? I would appreciate it if you explained to me how to solve such problems on an example of the functions I've mentioned above. Thank you.",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
27,Laplacian operator,Laplacian operator,,"I need to find A,$\alpha$,C $\in \mathbb{R}$ so that $u:\mathbb{R}^n \to \mathbb{R}$ of the form $u(x) = A||x||^\alpha + C $ satisfies $\Delta u(x) = ||x||^2$ and $u(0) = -1$. This question gets very messy. I end up with terms that can potentially cancel but they both depend on A and $\alpha$ and i cannot satisfy what is required.","I need to find A,$\alpha$,C $\in \mathbb{R}$ so that $u:\mathbb{R}^n \to \mathbb{R}$ of the form $u(x) = A||x||^\alpha + C $ satisfies $\Delta u(x) = ||x||^2$ and $u(0) = -1$. This question gets very messy. I end up with terms that can potentially cancel but they both depend on A and $\alpha$ and i cannot satisfy what is required.",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
28,Partial derivative with respect to a function?,Partial derivative with respect to a function?,,"I have $f(x, y)$ is a class $C^2$ function where $x = u + v$, and $y = u - v$ How do I get $$\frac{\partial^2 f}{\partial x^2}$$ Well before we even talk about that one, I don't even know how to get the first partial. I'm confused about how to go about solving it when its with respect to x. Any hints? Edit: I don't have a specific equation for $f(x, y)$. The full question was to show  $$\frac{\partial^2f}{\partial{u}\ \partial{v}} = \frac{\partial^2f}{\partial{x^2}} - \frac{\partial^2f}{\partial{y^2}}$$","I have $f(x, y)$ is a class $C^2$ function where $x = u + v$, and $y = u - v$ How do I get $$\frac{\partial^2 f}{\partial x^2}$$ Well before we even talk about that one, I don't even know how to get the first partial. I'm confused about how to go about solving it when its with respect to x. Any hints? Edit: I don't have a specific equation for $f(x, y)$. The full question was to show  $$\frac{\partial^2f}{\partial{u}\ \partial{v}} = \frac{\partial^2f}{\partial{x^2}} - \frac{\partial^2f}{\partial{y^2}}$$",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
29,"Solve $\lim_{(x, y) \to (4, 0)} x^2y \ln(x y)$",Solve,"\lim_{(x, y) \to (4, 0)} x^2y \ln(x y)","I got this question: $$\lim_{(x, y) \to (4, 0)} x^2y \ln(x y)$$ I know this limit does not exist by using Wolfram but I get 0 using all method. How to prove this limit does not exist?","I got this question: $$\lim_{(x, y) \to (4, 0)} x^2y \ln(x y)$$ I know this limit does not exist by using Wolfram but I get 0 using all method. How to prove this limit does not exist?",,"['limits', 'multivariable-calculus']"
30,"Implicit function theorem ""submersion version""","Implicit function theorem ""submersion version""",,"In class we learned the following variant of the Implicit Function Theorem: Suppose $f:U \to \mathbb{R}^{n-k}$, where $U \subseteq \mathbb{R}^n$, is such that $Df(p)$ has full row rank for all $p \in U$. Then there exist diffeomorphisms $\alpha, \beta$ s.t. $\alpha: U' \to U$ and $\beta: F(U) \to W$, and $\beta \circ f \circ \alpha = \pi$, the normal orthogonal projection $\mathbb{R}^n \to \mathbb{R}^{n-k}$. My Question: Why do we need $\beta$? WLOG suppose that the first $n-k$ columns of $Df(p)$ are linearly independent. Then the function $F:(x_1, \dotsc, x_n) \mapsto (f_1, \dotsc, f_{n-k}, x_{n-k+1}, \dotsc, x_n)$ has an inverse in a neighborhood of $p$, so $$f \circ F^{-1}: (f_1, \dotsc, f_{n-k}, x_{n-k+1}, \dotsc, x_n) \mapsto (f_1, \dotsc, f_{n-k}).$$ So why do we need $\beta$?","In class we learned the following variant of the Implicit Function Theorem: Suppose $f:U \to \mathbb{R}^{n-k}$, where $U \subseteq \mathbb{R}^n$, is such that $Df(p)$ has full row rank for all $p \in U$. Then there exist diffeomorphisms $\alpha, \beta$ s.t. $\alpha: U' \to U$ and $\beta: F(U) \to W$, and $\beta \circ f \circ \alpha = \pi$, the normal orthogonal projection $\mathbb{R}^n \to \mathbb{R}^{n-k}$. My Question: Why do we need $\beta$? WLOG suppose that the first $n-k$ columns of $Df(p)$ are linearly independent. Then the function $F:(x_1, \dotsc, x_n) \mapsto (f_1, \dotsc, f_{n-k}, x_{n-k+1}, \dotsc, x_n)$ has an inverse in a neighborhood of $p$, so $$f \circ F^{-1}: (f_1, \dotsc, f_{n-k}, x_{n-k+1}, \dotsc, x_n) \mapsto (f_1, \dotsc, f_{n-k}).$$ So why do we need $\beta$?",,"['multivariable-calculus', 'derivatives', 'implicit-differentiation']"
31,Find where the limit does not exist for the function,Find where the limit does not exist for the function,,"Given the function: $f(x,y) = \frac{xy^4}{x^2+y^8}$, find a path where the limit does not exist at the origin. I am having problems with this because of lot of paths go to $0$ but I know the limit does not exist. I have tried things like $y^2$, $\sqrt(y)$ but I am getting nowhere.  Thank you!","Given the function: $f(x,y) = \frac{xy^4}{x^2+y^8}$, find a path where the limit does not exist at the origin. I am having problems with this because of lot of paths go to $0$ but I know the limit does not exist. I have tried things like $y^2$, $\sqrt(y)$ but I am getting nowhere.  Thank you!",,"['limits', 'multivariable-calculus']"
32,Unit speed reparametrization of curve,Unit speed reparametrization of curve,,I am learning Elementary Differential Geometry by O'Neill and having a hard time with this exercise. Suppose that $\beta_1$ and $\beta_2$ are unit-speed reparametrizations of the same curve $\alpha$. Show that there is a number $s_0$ such that $\beta_2(s)=\beta_1(s+s_0)$ for all $s$. My questions are 1 The two curves may not be defined on the same interval and each of them may traverse part of the curve $\alpha$. What is the meaning of for all $s$? 2 Why must the two beta curves have the same orientation? 3 How to prove the statement rigorously and completely?,I am learning Elementary Differential Geometry by O'Neill and having a hard time with this exercise. Suppose that $\beta_1$ and $\beta_2$ are unit-speed reparametrizations of the same curve $\alpha$. Show that there is a number $s_0$ such that $\beta_2(s)=\beta_1(s+s_0)$ for all $s$. My questions are 1 The two curves may not be defined on the same interval and each of them may traverse part of the curve $\alpha$. What is the meaning of for all $s$? 2 Why must the two beta curves have the same orientation? 3 How to prove the statement rigorously and completely?,,"['multivariable-calculus', 'differential-geometry']"
33,$r'(t)=C \times r(t)$ - what is special about $r(t)$?,- what is special about ?,r'(t)=C \times r(t) r(t),"So $r'(t) = C \times r(t)$, where $r(t)$ is vector function and C is constant vector and $\times$ is cross product. What is then special about $r(t)$?","So $r'(t) = C \times r(t)$, where $r(t)$ is vector function and C is constant vector and $\times$ is cross product. What is then special about $r(t)$?",,['multivariable-calculus']
34,Deriving equations of motion in spherical coordinates,Deriving equations of motion in spherical coordinates,,"OK, we've been asked to derive the equations of motion in spherical coordinates. According to the assignment, we should end up with this: $$ \bf \vec{v} \rm = \frac{d \bf \vec{r} \rm}{dt} = \dot{r} \bf \hat{r} \rm + r \dot{\theta}\hat{\boldsymbol \theta} \rm + r \dot{\phi}\sin \theta \bf \hat{\boldsymbol\phi}\rm $$ In this case ฮธ is the angle from the z axis and phi is the angle in the x-y plane. Now, if I take it that position $$\bf \vec{r} \rm = r \bf \hat{r}$$ and say $$ \bf \hat{r} \rm = \bf \hat{x} \rm \sin\theta \cos\phi + \bf \hat{y} \rm \sin \theta \sin \phi + \bf \hat{z}\rm \cos\theta \\ \hat{\boldsymbol\theta} = \bf \hat{x} \rm \cos\theta \cos\phi + \bf \hat{y} \rm \cos \theta \sin \phi - \bf \hat{z}\rm\sin\theta\\ \hat{\boldsymbol\phi} = \bf \hat{x} \rm (-\sin\phi) + \bf \hat{y} \rm \cos \phi\\ $$ now maybe I am making this more complex than it is. And maybe it's just a notation problem (I really hate the dot notation sometimes because I feel it obscures things, but I need to know it, I know). If we assume that when r changes, $\phi$ and $\theta$ and their unit vectors stay the same, then we can safely say that $\frac{d \hat{\boldsymbol\phi}}{dr} = 0$ and $\frac{d \hat{\boldsymbol\theta}}{d r} = 0.$ (someone please tell me if i am wrong). If we do the same thing with changing ฮธ and $\phi$ though, the result is different. hen we change ฮธ, r has to change because it changes direction, and when we change $\phi$ $r$ has to change because it changes direction in that case also. When I take the derivative of $\hat{r}$ with respect to $\theta$, I get the following: $$\frac{d \bf \hat{r}}{d\theta} = \bf \hat{x} \rm \cos\theta \cos\phi + \bf \hat{y} \rm \cos \theta \sin \phi - \bf \hat{z}\rm\sin\theta $$ which as it happens also is equal to $\hat{\boldsymbol{\theta}}$ Now, if I look at $\bf \vec{r} \rm = r \bf \hat{r}$ and take the derivative w/r/t time, I should get $\frac{d \bf \vec{r} \rm}{dt} = r \frac{d\bf \hat{r}}{dt} + \frac{dr}{dt}\bf \hat{r} \rm$ I notice that this happens (and some of this is just seeing the notation): $$ \frac{d \bf \vec{r} \rm}{dt} = r \frac{d\bf \hat{r}}{dt} + \frac{dr}{dt}\bf \hat{r} \rm = \dot{r} \bf \hat{r} \rm + r \hat{\boldsymbol \theta} \rm $$ ANd I feel like I am almost there. But I am having trouble making that last step. I am getting a bit frustrated because I can't seem to make the differentiation work the way it does in the text and I haven't found a derivation online that matches up with anything I have seen in class. Again, maybe it's just the notation used. But I am trying to understand where the $\dot{\theta}$ term comes up, and how to get that extra dimension in. Any help is most appreciated. Thanks. EDIT: fixed the unit vector phi expression.","OK, we've been asked to derive the equations of motion in spherical coordinates. According to the assignment, we should end up with this: $$ \bf \vec{v} \rm = \frac{d \bf \vec{r} \rm}{dt} = \dot{r} \bf \hat{r} \rm + r \dot{\theta}\hat{\boldsymbol \theta} \rm + r \dot{\phi}\sin \theta \bf \hat{\boldsymbol\phi}\rm $$ In this case ฮธ is the angle from the z axis and phi is the angle in the x-y plane. Now, if I take it that position $$\bf \vec{r} \rm = r \bf \hat{r}$$ and say $$ \bf \hat{r} \rm = \bf \hat{x} \rm \sin\theta \cos\phi + \bf \hat{y} \rm \sin \theta \sin \phi + \bf \hat{z}\rm \cos\theta \\ \hat{\boldsymbol\theta} = \bf \hat{x} \rm \cos\theta \cos\phi + \bf \hat{y} \rm \cos \theta \sin \phi - \bf \hat{z}\rm\sin\theta\\ \hat{\boldsymbol\phi} = \bf \hat{x} \rm (-\sin\phi) + \bf \hat{y} \rm \cos \phi\\ $$ now maybe I am making this more complex than it is. And maybe it's just a notation problem (I really hate the dot notation sometimes because I feel it obscures things, but I need to know it, I know). If we assume that when r changes, $\phi$ and $\theta$ and their unit vectors stay the same, then we can safely say that $\frac{d \hat{\boldsymbol\phi}}{dr} = 0$ and $\frac{d \hat{\boldsymbol\theta}}{d r} = 0.$ (someone please tell me if i am wrong). If we do the same thing with changing ฮธ and $\phi$ though, the result is different. hen we change ฮธ, r has to change because it changes direction, and when we change $\phi$ $r$ has to change because it changes direction in that case also. When I take the derivative of $\hat{r}$ with respect to $\theta$, I get the following: $$\frac{d \bf \hat{r}}{d\theta} = \bf \hat{x} \rm \cos\theta \cos\phi + \bf \hat{y} \rm \cos \theta \sin \phi - \bf \hat{z}\rm\sin\theta $$ which as it happens also is equal to $\hat{\boldsymbol{\theta}}$ Now, if I look at $\bf \vec{r} \rm = r \bf \hat{r}$ and take the derivative w/r/t time, I should get $\frac{d \bf \vec{r} \rm}{dt} = r \frac{d\bf \hat{r}}{dt} + \frac{dr}{dt}\bf \hat{r} \rm$ I notice that this happens (and some of this is just seeing the notation): $$ \frac{d \bf \vec{r} \rm}{dt} = r \frac{d\bf \hat{r}}{dt} + \frac{dr}{dt}\bf \hat{r} \rm = \dot{r} \bf \hat{r} \rm + r \hat{\boldsymbol \theta} \rm $$ ANd I feel like I am almost there. But I am having trouble making that last step. I am getting a bit frustrated because I can't seem to make the differentiation work the way it does in the text and I haven't found a derivation online that matches up with anything I have seen in class. Again, maybe it's just the notation used. But I am trying to understand where the $\dot{\theta}$ term comes up, and how to get that extra dimension in. Any help is most appreciated. Thanks. EDIT: fixed the unit vector phi expression.",,"['multivariable-calculus', 'physics', 'spherical-coordinates']"
35,Evaluating two integrals having logarithmic and trig functions,Evaluating two integrals having logarithmic and trig functions,,"Evaluate   $\displaystyle \int\limits_{0}^{\pi/2} \dfrac{\ln(1+\cos a\cos x)}{\cos x} dx$ where $0<a<\pi$. $\displaystyle \int\limits_{0}^{\pi} \dfrac{\ln(1+x\cos y)}{\cos y}dy$ where $-1<x<1$. One way is to use Leibniz's theorem for derivative under the integral sign for $f:(0,\pi)\times [0,\frac{\pi}2]\to \mathbb{R}, \\ \displaystyle f(a,x) = \frac{\ln(1+\cos a \cos x)}{\cos x}$ and $f:(-1,1)\times [0,\pi]\to \mathbb{R}, \\ \displaystyle f(x,y) = \frac{\ln(1+x\cos y)}{\cos y}$ but I'm looking for other methods, preferably more elementary.","Evaluate   $\displaystyle \int\limits_{0}^{\pi/2} \dfrac{\ln(1+\cos a\cos x)}{\cos x} dx$ where $0<a<\pi$. $\displaystyle \int\limits_{0}^{\pi} \dfrac{\ln(1+x\cos y)}{\cos y}dy$ where $-1<x<1$. One way is to use Leibniz's theorem for derivative under the integral sign for $f:(0,\pi)\times [0,\frac{\pi}2]\to \mathbb{R}, \\ \displaystyle f(a,x) = \frac{\ln(1+\cos a \cos x)}{\cos x}$ and $f:(-1,1)\times [0,\pi]\to \mathbb{R}, \\ \displaystyle f(x,y) = \frac{\ln(1+x\cos y)}{\cos y}$ but I'm looking for other methods, preferably more elementary.",,"['calculus', 'multivariable-calculus', 'definite-integrals']"
36,Finding a vector that has 0 curl and 0 div,Finding a vector that has 0 curl and 0 div,,"SO as stated, I am trying to find a vector $\vec F$such that $$\nabla \times \vec F=0$$ $$\nabla \cdot \vec F=0$$ The way I go about it is: Becasue curl is 0, we know that $\vec F=\nabla f$ so the divergence equation then becomes $$\nabla ^2f=0$$ Which I then say $f=A(x)B(y)C(z)$, which results in me getting the following function f: $$f=\cos(x)\cosh(y)\cos(z)$$ so $\vec F=\nabla f$,  Which has 0 curl, but nonzero div. Sad face","SO as stated, I am trying to find a vector $\vec F$such that $$\nabla \times \vec F=0$$ $$\nabla \cdot \vec F=0$$ The way I go about it is: Becasue curl is 0, we know that $\vec F=\nabla f$ so the divergence equation then becomes $$\nabla ^2f=0$$ Which I then say $f=A(x)B(y)C(z)$, which results in me getting the following function f: $$f=\cos(x)\cosh(y)\cos(z)$$ so $\vec F=\nabla f$,  Which has 0 curl, but nonzero div. Sad face",,['multivariable-calculus']
37,"Find an equation for the line that is parallel to the plane $2x - 3y + 5z - 10 = 0$ and passes through the point (-1, 7, 4)","Find an equation for the line that is parallel to the plane  and passes through the point (-1, 7, 4)",2x - 3y + 5z - 10 = 0,"""Find an equation for the line that is parallel to the plane $2x - 3y + 5z - 10 = 0$ and passes through the point (-1, 7, 4)"" I'm just learning this and am pretty confused on how to do this problem. In class we went over distance between vectors, orthogonal projection, but I still don't know how to proceed with this question. How does this deal with vectors?","""Find an equation for the line that is parallel to the plane $2x - 3y + 5z - 10 = 0$ and passes through the point (-1, 7, 4)"" I'm just learning this and am pretty confused on how to do this problem. In class we went over distance between vectors, orthogonal projection, but I still don't know how to proceed with this question. How does this deal with vectors?",,['multivariable-calculus']
38,Finding local and global extrema even when the determinant of the Hessian zero,Finding local and global extrema even when the determinant of the Hessian zero,,"I am trying to solve the following problem: Let $f: \mathbb R^2\rightarrow\mathbb R$  be a function defined by   $$ f(x,y) = x^{2n} + y^{2n} - nx^2 + 2nxy - ny^2, $$   where $n$ is a natural number greater than 1.   Decide whether $f$ has a (global) minimum.  Also find all the points at which $f$ attains its local maxima and local minima. I calculated the partial derivatives $f_x = 2nx^{2n-1}-2nx+2ny$, $f_{xx} = 2n(2n-1)x^{2n-2}-2n$, $f_{xy} = 2n$, and so on.  But I got an equation system $f_x = f_y = 0$ of degree three, which I had hard times solving it.  I found out that $(x,y) = (0,0)$ is one of its solutions, but at that point the the determinant of the Hessian is zero, from which I could not conclude whether it was a local extremum. I was not sure about how to prove that the minimum value of $f$ exists, either. I would be most grateful if you could help me solve this problem.,","I am trying to solve the following problem: Let $f: \mathbb R^2\rightarrow\mathbb R$  be a function defined by   $$ f(x,y) = x^{2n} + y^{2n} - nx^2 + 2nxy - ny^2, $$   where $n$ is a natural number greater than 1.   Decide whether $f$ has a (global) minimum.  Also find all the points at which $f$ attains its local maxima and local minima. I calculated the partial derivatives $f_x = 2nx^{2n-1}-2nx+2ny$, $f_{xx} = 2n(2n-1)x^{2n-2}-2n$, $f_{xy} = 2n$, and so on.  But I got an equation system $f_x = f_y = 0$ of degree three, which I had hard times solving it.  I found out that $(x,y) = (0,0)$ is one of its solutions, but at that point the the determinant of the Hessian is zero, from which I could not conclude whether it was a local extremum. I was not sure about how to prove that the minimum value of $f$ exists, either. I would be most grateful if you could help me solve this problem.,",,"['calculus', 'multivariable-calculus']"
39,Why in this proof we get $\alpha \geq 0$?,Why in this proof we get ?,\alpha \geq 0,"I've solved the following problem: ""Let $u,v \in \mathbb{R}^n$ with $u \neq 0$ be such that $|u+v|=|u|+|v|$ (euclidean norm), show that there's $\alpha \in \mathbb{R}$ with $\alpha \geq 0$ such that $v = \alpha u$"". My approach was the following: By definition of the euclidean norm, we have that $$|u+v|^2=\left\langle u+v,u+v\right\rangle=|u|^2+2\left\langle u,v\right\rangle+|v|^2,$$ but by hypothesis $|u+v|^2=(|u|+|v|)^2=|u|^2+2|u||v|+|v|^2$ so that equating those two we must have $\left\langle u,v\right\rangle = |u||v|$ and so $|\left\langle u,v\right\rangle| =|u||v|$ so that by the Cauchy-Schwarz inequality there must be $\alpha \in \mathbb{R}$ such that $v=\alpha u$. Since I've not shown that $\alpha \geq 0$ in this proof, I thought it was wrong, but the answer in the book gives the same proof and says that all of this implies $\alpha \geq 0$. I've gone again through the proof of Cauchy-Schwarz Inequality and it doesn't seem to be there the reason for this. Indeed, if $x,y \in \mathbb{R}^2$ are simply $x=(1,1)$ and $y=(-2,-2)$ then $y=-2x$ and indeed $|\left\langle x,y\right\rangle| = |x||y|$, so by this example we see that Cauchy-Schwarz Inequality doesn't imply that the scalar that multiply one vector to get the other must be positive. So, how this $\alpha$ is shown to be greater or equal to zero? Thanks very much in advance! EDIT: I think I've found the answer. I've shown that $\left\langle u,v\right\rangle =|u||v|$ and thus $|\left\langle u,v\right\rangle| = |u||v|$ by Cauchy-Schwarz Inequality we must have $v=\alpha u$, but since $\alpha = \left\langle u,v\right\rangle /|u|^2$, since the inner product itself is a positive number (because it's a product of two norms), then $\alpha$ must be positive. Is this right?","I've solved the following problem: ""Let $u,v \in \mathbb{R}^n$ with $u \neq 0$ be such that $|u+v|=|u|+|v|$ (euclidean norm), show that there's $\alpha \in \mathbb{R}$ with $\alpha \geq 0$ such that $v = \alpha u$"". My approach was the following: By definition of the euclidean norm, we have that $$|u+v|^2=\left\langle u+v,u+v\right\rangle=|u|^2+2\left\langle u,v\right\rangle+|v|^2,$$ but by hypothesis $|u+v|^2=(|u|+|v|)^2=|u|^2+2|u||v|+|v|^2$ so that equating those two we must have $\left\langle u,v\right\rangle = |u||v|$ and so $|\left\langle u,v\right\rangle| =|u||v|$ so that by the Cauchy-Schwarz inequality there must be $\alpha \in \mathbb{R}$ such that $v=\alpha u$. Since I've not shown that $\alpha \geq 0$ in this proof, I thought it was wrong, but the answer in the book gives the same proof and says that all of this implies $\alpha \geq 0$. I've gone again through the proof of Cauchy-Schwarz Inequality and it doesn't seem to be there the reason for this. Indeed, if $x,y \in \mathbb{R}^2$ are simply $x=(1,1)$ and $y=(-2,-2)$ then $y=-2x$ and indeed $|\left\langle x,y\right\rangle| = |x||y|$, so by this example we see that Cauchy-Schwarz Inequality doesn't imply that the scalar that multiply one vector to get the other must be positive. So, how this $\alpha$ is shown to be greater or equal to zero? Thanks very much in advance! EDIT: I think I've found the answer. I've shown that $\left\langle u,v\right\rangle =|u||v|$ and thus $|\left\langle u,v\right\rangle| = |u||v|$ by Cauchy-Schwarz Inequality we must have $v=\alpha u$, but since $\alpha = \left\langle u,v\right\rangle /|u|^2$, since the inner product itself is a positive number (because it's a product of two norms), then $\alpha$ must be positive. Is this right?",,"['linear-algebra', 'multivariable-calculus']"
40,Integration formula with wedge product.,Integration formula with wedge product.,,"On $\mathbb{R}^3$, consider a compactly supported $2$-form $$\omega = f_1 \, dx_2 \wedge dx_3 + f_2 \, dx_3 \wedge dx_1 + f_3 \, dx_1 \wedge dx_2.$$ Choose for $S$ the parametrization $h: \mathbb{R}^2 \to S$ defined by $$h(x_1, x_2) = (x_1, x_2, G(x_1, x_2)).$$ The I start to lost at why we are doing this: Compute $$h^* dx_1 \wedge dx_2 = dx_1 \wedge dx_2$$ $$h^* dx_2 \wedge dx_3 = -\frac{\partial G}{\partial x_1} dx_1 \wedge dx_2$$ $$h^* dx_3 \wedge dx_1 = -\frac{\partial G}{\partial x_2} dx_1 \wedge dx_2$$ So I get totally lost here And we emerge with the formula   $$\int_s \omega = \int_{\mathbb{R}^2}(n_1 f_1 + n_2 f_2 + n_3 f_3) dx_1 dx_2,$$   where   $$\vec{n} = (n_1, n_2, n_3) = (-\frac{\partial G}{\partial x_1},-\frac{\partial G}{\partial x_2}, 1).$$ Can someone give me some explanation/intuition here? Thank you.","On $\mathbb{R}^3$, consider a compactly supported $2$-form $$\omega = f_1 \, dx_2 \wedge dx_3 + f_2 \, dx_3 \wedge dx_1 + f_3 \, dx_1 \wedge dx_2.$$ Choose for $S$ the parametrization $h: \mathbb{R}^2 \to S$ defined by $$h(x_1, x_2) = (x_1, x_2, G(x_1, x_2)).$$ The I start to lost at why we are doing this: Compute $$h^* dx_1 \wedge dx_2 = dx_1 \wedge dx_2$$ $$h^* dx_2 \wedge dx_3 = -\frac{\partial G}{\partial x_1} dx_1 \wedge dx_2$$ $$h^* dx_3 \wedge dx_1 = -\frac{\partial G}{\partial x_2} dx_1 \wedge dx_2$$ So I get totally lost here And we emerge with the formula   $$\int_s \omega = \int_{\mathbb{R}^2}(n_1 f_1 + n_2 f_2 + n_3 f_3) dx_1 dx_2,$$   where   $$\vec{n} = (n_1, n_2, n_3) = (-\frac{\partial G}{\partial x_1},-\frac{\partial G}{\partial x_2}, 1).$$ Can someone give me some explanation/intuition here? Thank you.",,"['multivariable-calculus', 'differential-topology']"
41,Use triple integrals to find the volume of...,Use triple integrals to find the volume of...,,"The solid enclosed by the parabaloid $$x=y^2+z^2$$ and the plane $$x=6$$ I wanted to make sure that I'm setting up the correct integral before I start to integrate it. $$\int_{-\sqrt{6}}^{\sqrt{6}}\int_{-\sqrt{6-y^2}}^{\sqrt{6-y^2}}\int_{y^2+z^2}^6dxdydz$$ If this is incorrect, would someone be able to help guide me in the right direction?","The solid enclosed by the parabaloid $$x=y^2+z^2$$ and the plane $$x=6$$ I wanted to make sure that I'm setting up the correct integral before I start to integrate it. $$\int_{-\sqrt{6}}^{\sqrt{6}}\int_{-\sqrt{6-y^2}}^{\sqrt{6-y^2}}\int_{y^2+z^2}^6dxdydz$$ If this is incorrect, would someone be able to help guide me in the right direction?",,"['multivariable-calculus', 'integration']"
42,How do I define the limits of a double integral in polar coordinates over an annulus?,How do I define the limits of a double integral in polar coordinates over an annulus?,,"Evaluate the double integral by re-writing them in polar coordinates: $\displaystyle\iint\limits_{R}\frac{y^2}{x^2}\ dA$, where $R$ is part of the annulus (ring) $9\leq x^2+y^2\leq 25$ lying in the first quadrant and below the line $y=x$. So from this, I gather (assuming I understood correctly) that $R=\{(x,y)\mid9\leq x^2+y^2\leq 25,\ 0\leq y\leq x\}$. There are two circles, one where $r=5$ and one where $r=3$. I would guess that what I'm looking for is the larger circle minus the smaller circle, but doing that the only way I can think of yields the following: \begin{gather} (x^2+y^2)-(x^2+y^2)=25-9\\ 0=16 \end{gather} I'm clearly way off track here; how do I look at this so I can define the bounds of the double integral in polar coordinates?","Evaluate the double integral by re-writing them in polar coordinates: $\displaystyle\iint\limits_{R}\frac{y^2}{x^2}\ dA$, where $R$ is part of the annulus (ring) $9\leq x^2+y^2\leq 25$ lying in the first quadrant and below the line $y=x$. So from this, I gather (assuming I understood correctly) that $R=\{(x,y)\mid9\leq x^2+y^2\leq 25,\ 0\leq y\leq x\}$. There are two circles, one where $r=5$ and one where $r=3$. I would guess that what I'm looking for is the larger circle minus the smaller circle, but doing that the only way I can think of yields the following: \begin{gather} (x^2+y^2)-(x^2+y^2)=25-9\\ 0=16 \end{gather} I'm clearly way off track here; how do I look at this so I can define the bounds of the double integral in polar coordinates?",,"['multivariable-calculus', 'polar-coordinates']"
43,How to integrate over polar coordinates,How to integrate over polar coordinates,,"Evaluate the following double integral by rewriting it in polar coordinates: $\displaystyle\iint\limits_Dxy\,dA$, where $D$ is the disc with center at the origin and radius 5 I have very little understanding about how to do this. The most I know right now is the following: $x=r\cos(\theta)$ $y=r\sin(\theta)$ $dA=r\,dr\,d\theta$ $D=\{(x,y)\mid x^2+y^2\leq 25\}$ or $D=\{(r,\theta)\mid r\leq 5\}$ It's given in the problem that $r=5$, so that's a start. I'm assuming then that my limits for $r$ is $0\leq r\leq 5$. But I have no idea how to define the limits for $\theta$. My guess would be $0\leq\theta\leq 2\pi$, but several examples with different regions seem to use $0\leq\theta\leq\pi$. So here's part of the integral with missing limits on $\theta$: $$\int\limits_{\alpha}^{\beta}\int\limits_{0}^{5}r^3\sin{\theta}\cos{\theta}\,dr\,d\theta$$ Is my limited understanding correct so far? How do I fill in the holes of this problem? I know how to integrate after I have the proper limits; I just don't know how to define the limits given the information I have.","Evaluate the following double integral by rewriting it in polar coordinates: $\displaystyle\iint\limits_Dxy\,dA$, where $D$ is the disc with center at the origin and radius 5 I have very little understanding about how to do this. The most I know right now is the following: $x=r\cos(\theta)$ $y=r\sin(\theta)$ $dA=r\,dr\,d\theta$ $D=\{(x,y)\mid x^2+y^2\leq 25\}$ or $D=\{(r,\theta)\mid r\leq 5\}$ It's given in the problem that $r=5$, so that's a start. I'm assuming then that my limits for $r$ is $0\leq r\leq 5$. But I have no idea how to define the limits for $\theta$. My guess would be $0\leq\theta\leq 2\pi$, but several examples with different regions seem to use $0\leq\theta\leq\pi$. So here's part of the integral with missing limits on $\theta$: $$\int\limits_{\alpha}^{\beta}\int\limits_{0}^{5}r^3\sin{\theta}\cos{\theta}\,dr\,d\theta$$ Is my limited understanding correct so far? How do I fill in the holes of this problem? I know how to integrate after I have the proper limits; I just don't know how to define the limits given the information I have.",,"['multivariable-calculus', 'polar-coordinates']"
44,Tetrahedral Law of Cosines Proof,Tetrahedral Law of Cosines Proof,,"Given a tetrahedral $\rho$ with faces $\Xi, \Pi, \Gamma, \Delta$ with areas $\xi , \pi, \gamma , \delta$, respectively, assign a normal vector to each face such that $\mid \mid \hat{\xi} \mid \mid = \xi, \mid \mid \hat{\pi} \mid \mid = \pi, \mid \mid \hat{\gamma} \mid \mid = \gamma, \mid \mid \hat{\delta} \mid \mid = \delta$ Prove that the sum of the vectors is the zero using vector cross product.","Given a tetrahedral $\rho$ with faces $\Xi, \Pi, \Gamma, \Delta$ with areas $\xi , \pi, \gamma , \delta$, respectively, assign a normal vector to each face such that $\mid \mid \hat{\xi} \mid \mid = \xi, \mid \mid \hat{\pi} \mid \mid = \pi, \mid \mid \hat{\gamma} \mid \mid = \gamma, \mid \mid \hat{\delta} \mid \mid = \delta$ Prove that the sum of the vectors is the zero using vector cross product.",,['multivariable-calculus']
45,Derivative of $f(x) = e^{a^Tx}$,Derivative of,f(x) = e^{a^Tx},"Let $f(x) = e^{a^Tx}$ where $a,x \in \mathbb{R}^N$ Is $\frac {\partial f}{\partial x} = e^{a^Tx}a$ ?","Let $f(x) = e^{a^Tx}$ where $a,x \in \mathbb{R}^N$ Is $\frac {\partial f}{\partial x} = e^{a^Tx}a$ ?",,['multivariable-calculus']
46,Evaluate the double integral using substitution:,Evaluate the double integral using substitution:,,"Evaluate: $$\int_0^1\int_0^\sqrt{1-x^2}e^{-(x^2+y^2)}\,\mathrm dy\;\mathrm dx$$ I am trying to use substitution on this problem by making $x^2+y^2 = u$ and $\sqrt{1-x^2} = v$. I then tried to add $u$ and $v$ to get something like $v + u = x^2 + y^2 + \sqrt{1-x^2}$ and I'm trying to set it equal to $x$, and then $y$, but I'm not sure if I'm doing this correctly. Any suggestions as to how to make the substitution process less tedious? Thank you so much.","Evaluate: $$\int_0^1\int_0^\sqrt{1-x^2}e^{-(x^2+y^2)}\,\mathrm dy\;\mathrm dx$$ I am trying to use substitution on this problem by making $x^2+y^2 = u$ and $\sqrt{1-x^2} = v$. I then tried to add $u$ and $v$ to get something like $v + u = x^2 + y^2 + \sqrt{1-x^2}$ and I'm trying to set it equal to $x$, and then $y$, but I'm not sure if I'm doing this correctly. Any suggestions as to how to make the substitution process less tedious? Thank you so much.",,"['calculus', 'integration', 'multivariable-calculus']"
47,extrema and saddle points,extrema and saddle points,,"Examine the following function for relative extrema and saddle points: $$f(x, y) = 9x^2-5y^2-54x-40y+4.$$ I did this and got that the point should be at $(3, -4, 3)$. Is that right? Also, how do I know if it is a saddle point or a minimum?","Examine the following function for relative extrema and saddle points: $$f(x, y) = 9x^2-5y^2-54x-40y+4.$$ I did this and got that the point should be at $(3, -4, 3)$. Is that right? Also, how do I know if it is a saddle point or a minimum?",,['multivariable-calculus']
48,Dot products in the context of linear algebra and matrix multiplication,Dot products in the context of linear algebra and matrix multiplication,,"I've been self-teaching myself linear algebra from Linear Algebra and its Applications 4th from D.ยLay. I'm about 8 sections deep and I've had this bothersome feeling regarding the section describing the process of multiplying matrix $A$ and vector $\mathbf x$: The first entry in the product $A \mathbf x$ is a sum of products (sometimes called a dot product ), using the first row of $A$ and the entries in $\mathbf x$. That is,   $$\begin{bmatrix} 2 & 3 & 4 \\ \phantom{0} \\ \phantom{0} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 2x_1 + 3x_2 + 4x_3 \\ \phantom{0} \\ \phantom{0} \end{bmatrix}$$ This rolled into some examples: $\begin{bmatrix}   1 & 2 & -1 \\   0 & -5 & 3 \end{bmatrix} \begin{bmatrix}   4 \\ 3 \\ 7 \end{bmatrix} = \begin{bmatrix}   1 \cdot 4 + 2 \cdot 3 + (-1) \cdot 7 \\   0 \cdot 4 + (-5) \cdot 3 + 3 \cdot 7 \end{bmatrix} = \begin{bmatrix}   3 \\ 6 \end{bmatrix}$ $\begin{bmatrix}   2 & -3 \\   8 & 0 \\   -5 & 2 \end{bmatrix} \begin{bmatrix}   4 \\ 7 \end{bmatrix} = \begin{bmatrix}   2 \cdot 4 + (-3) \cdot 7 \\   8 \cdot 4+ 0 \cdot 7 \\   (-5) \cdot 4 + 2 \cdot 7 \end{bmatrix} = \begin{bmatrix}   -13 \\ 32 \\ -6 \end{bmatrix}$ $\begin{bmatrix}   1 & 0 & 0 \\   0 & 1 & 0 \\   0 & 0 & 1 \end{bmatrix} \begin{bmatrix}   r \\ s \\ t \end{bmatrix} = \begin{bmatrix}   1 \cdot r + 0 \cdot s + 0 \cdot t \\   0 \cdot r + 1 \cdot s + 0 \cdot t \\   0 \cdot r + 0 \cdot s + 1 \cdot t \\ \end{bmatrix} = \begin{bmatrix}   r \\ s \\ t \end{bmatrix}$ I don't know if it's the radical change in syntax or if I'm just plain missing something. The dot products (multivar calc/physics) I'm familiar with result in a scalar answer at the end of the day. As far as I understand, a matrix in itself is a โbundleโ of vectors each occupying a column. I get the feeling I'm trying to relate a concept when there isn't a relationship at all I guess. Maybe someone can clarify.","I've been self-teaching myself linear algebra from Linear Algebra and its Applications 4th from D.ยLay. I'm about 8 sections deep and I've had this bothersome feeling regarding the section describing the process of multiplying matrix $A$ and vector $\mathbf x$: The first entry in the product $A \mathbf x$ is a sum of products (sometimes called a dot product ), using the first row of $A$ and the entries in $\mathbf x$. That is,   $$\begin{bmatrix} 2 & 3 & 4 \\ \phantom{0} \\ \phantom{0} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 2x_1 + 3x_2 + 4x_3 \\ \phantom{0} \\ \phantom{0} \end{bmatrix}$$ This rolled into some examples: $\begin{bmatrix}   1 & 2 & -1 \\   0 & -5 & 3 \end{bmatrix} \begin{bmatrix}   4 \\ 3 \\ 7 \end{bmatrix} = \begin{bmatrix}   1 \cdot 4 + 2 \cdot 3 + (-1) \cdot 7 \\   0 \cdot 4 + (-5) \cdot 3 + 3 \cdot 7 \end{bmatrix} = \begin{bmatrix}   3 \\ 6 \end{bmatrix}$ $\begin{bmatrix}   2 & -3 \\   8 & 0 \\   -5 & 2 \end{bmatrix} \begin{bmatrix}   4 \\ 7 \end{bmatrix} = \begin{bmatrix}   2 \cdot 4 + (-3) \cdot 7 \\   8 \cdot 4+ 0 \cdot 7 \\   (-5) \cdot 4 + 2 \cdot 7 \end{bmatrix} = \begin{bmatrix}   -13 \\ 32 \\ -6 \end{bmatrix}$ $\begin{bmatrix}   1 & 0 & 0 \\   0 & 1 & 0 \\   0 & 0 & 1 \end{bmatrix} \begin{bmatrix}   r \\ s \\ t \end{bmatrix} = \begin{bmatrix}   1 \cdot r + 0 \cdot s + 0 \cdot t \\   0 \cdot r + 1 \cdot s + 0 \cdot t \\   0 \cdot r + 0 \cdot s + 1 \cdot t \\ \end{bmatrix} = \begin{bmatrix}   r \\ s \\ t \end{bmatrix}$ I don't know if it's the radical change in syntax or if I'm just plain missing something. The dot products (multivar calc/physics) I'm familiar with result in a scalar answer at the end of the day. As far as I understand, a matrix in itself is a โbundleโ of vectors each occupying a column. I get the feeling I'm trying to relate a concept when there isn't a relationship at all I guess. Maybe someone can clarify.",,"['linear-algebra', 'multivariable-calculus']"
49,Area of $\left( \frac{x^2}{9}+\frac{y^2}{25} \right)^2 \le x^2 + y^2$,Area of,\left( \frac{x^2}{9}+\frac{y^2}{25} \right)^2 \le x^2 + y^2,"I've used the modified polar coordinates: $x = 3r \cos \theta$, $y =5r \sin \theta$, which got me to $$r^2 \le 9 \cos^2 \theta + 25 \sin^2 \theta$$ What now?","I've used the modified polar coordinates: $x = 3r \cos \theta$, $y =5r \sin \theta$, which got me to $$r^2 \le 9 \cos^2 \theta + 25 \sin^2 \theta$$ What now?",,"['calculus', 'multivariable-calculus', 'polar-coordinates']"
50,How to define intrinsic curvature?,How to define intrinsic curvature?,,"I have been exploring differential geometry slightly... And i'm trying to grasp how to define intrinsic curvature, from a visual/geometric viewpoint... One formulation I got was that Intrinsic curvature can be thought in terms how the $n$-volume of an $N$-sphere in your particular $n$-dimensional space (example: a the area of a circle drawn on a $2$d space) differs from the $n$-volume of a euclidean N-sphere... But here is the big problem I found... That definition is all wonderful and whatnot when talking about the curvature of a certain ""area"" of a surface... Example, If I had a big sphere in $3$ dimensional space. I can start from a point in the sphere... walk $10$ meters off of the point and draw a circle of radius $10$... then measure the area using tiles and compare its ratio to that of a euclidean circle... But as I reduce the radius that I'm measuring... The circle I draw out will get increasingly closer to the area of euclidean circle for the same radius... Clearly the point curvature given this definiton will be $0$... At any given point on a sphere the curvature, by this definition, is equivalent to a flat plane... I can't seem to find a way to define intrinsic curvature now. Anyone care to help?","I have been exploring differential geometry slightly... And i'm trying to grasp how to define intrinsic curvature, from a visual/geometric viewpoint... One formulation I got was that Intrinsic curvature can be thought in terms how the $n$-volume of an $N$-sphere in your particular $n$-dimensional space (example: a the area of a circle drawn on a $2$d space) differs from the $n$-volume of a euclidean N-sphere... But here is the big problem I found... That definition is all wonderful and whatnot when talking about the curvature of a certain ""area"" of a surface... Example, If I had a big sphere in $3$ dimensional space. I can start from a point in the sphere... walk $10$ meters off of the point and draw a circle of radius $10$... then measure the area using tiles and compare its ratio to that of a euclidean circle... But as I reduce the radius that I'm measuring... The circle I draw out will get increasingly closer to the area of euclidean circle for the same radius... Clearly the point curvature given this definiton will be $0$... At any given point on a sphere the curvature, by this definition, is equivalent to a flat plane... I can't seem to find a way to define intrinsic curvature now. Anyone care to help?",,"['differential-geometry', 'multivariable-calculus', 'riemannian-geometry', 'curvature']"
51,"How to determine the limit of $ f(x, y)=\frac{9xy}{x^2 + y^2}$?",How to determine the limit of ?," f(x, y)=\frac{9xy}{x^2 + y^2}","Given $$f(x,y) = \frac{9xy}{x^2 + y^2},$$ determine the limit of $f(x,y)$ as $(x,y) \to (2,1)$. The answer for this would be $\dfrac{18}{5}$ right?","Given $$f(x,y) = \frac{9xy}{x^2 + y^2},$$ determine the limit of $f(x,y)$ as $(x,y) \to (2,1)$. The answer for this would be $\dfrac{18}{5}$ right?",,"['limits', 'multivariable-calculus']"
52,Solving Lagrange multipliers system,Solving Lagrange multipliers system,,I need help solving this system: $$ \begin{cases} 2(x-1) = \lambda2x \\ 2(y-2) = \lambda2y  \\ 2(z-2) = \lambda2z \\x^2 + y^2+z^2 = 1 \end{cases} $$ I can find $$ \lambda = (x-1)/x $$ but can't go further. Any help?,I need help solving this system: $$ \begin{cases} 2(x-1) = \lambda2x \\ 2(y-2) = \lambda2y  \\ 2(z-2) = \lambda2z \\x^2 + y^2+z^2 = 1 \end{cases} $$ I can find $$ \lambda = (x-1)/x $$ but can't go further. Any help?,,"['multivariable-calculus', 'lagrange-multiplier']"
53,Show that the area vectors for a general $n$-sided closed shape sum to zero,Show that the area vectors for a general -sided closed shape sum to zero,n,"It is possible to show that the sum of the area vectors for a general, closed, $n$-sided figure in $\mathbb{R}^3$ (3-space) is zero. Hint: it may be easiest to consider orientable and non-orientable surfaces separately.","It is possible to show that the sum of the area vectors for a general, closed, $n$-sided figure in $\mathbb{R}^3$ (3-space) is zero. Hint: it may be easiest to consider orientable and non-orientable surfaces separately.",,"['geometry', 'multivariable-calculus', 'vector-spaces']"
54,Using implicit function theorem without using the inverse function theorem.,Using implicit function theorem without using the inverse function theorem.,,"Let $f:U\rightarrow \mathbb{R}$ defined on the open set $U \subset \mathbb{R}^m$. It the function $g(x):U\rightarrow \mathbb{R}$, given by the expression $$g(x)= \int_{0}^{f(x)} (t^2 + 1)dt,$$ of class $C^{\infty}$, then f is $C^{\infty}$.Explicitly using the implicit function theorem, knowing that the inverse function is equivalent.","Let $f:U\rightarrow \mathbb{R}$ defined on the open set $U \subset \mathbb{R}^m$. It the function $g(x):U\rightarrow \mathbb{R}$, given by the expression $$g(x)= \int_{0}^{f(x)} (t^2 + 1)dt,$$ of class $C^{\infty}$, then f is $C^{\infty}$.Explicitly using the implicit function theorem, knowing that the inverse function is equivalent.",,"['real-analysis', 'multivariable-calculus', 'integration']"
55,derivative of square root of $x_1$ and $x_2$,derivative of square root of  and,x_1 x_2,"I am confusing with calculation of derivative of $$\sqrt{x_1x_2}$$ I am very thankful if anyone help me out of this problem. my question is that, (1) $x_1$ and $x_2$ are two different variable i just want to calculate the derivative with respect to $x$. (2) if $x_1=x_2$ what would be the value of derivative. Thanks in advance","I am confusing with calculation of derivative of $$\sqrt{x_1x_2}$$ I am very thankful if anyone help me out of this problem. my question is that, (1) $x_1$ and $x_2$ are two different variable i just want to calculate the derivative with respect to $x$. (2) if $x_1=x_2$ what would be the value of derivative. Thanks in advance",,"['multivariable-calculus', 'derivatives']"
56,Finding the range of a vector valued function,Finding the range of a vector valued function,,"For a single valued function, I can infer if the function is monotone from its derivative. For a vector valued function, is it possible to infer monotonicity from the directional derivative? For example, define  $$ D=[1,2]\times[1,2], $$  and $$ f(x,y)=\left( \frac{2}{1/x+1/y},\sqrt{xy} \right). $$ Is it possible to show that $f$ maps $D$ to $D$ from its gradient $\nabla f$? The gradient is $$ \nabla f = \begin{pmatrix} \frac{2}{\left(1+x/y\right)^2} & \frac{2}{\left(1+y/x\right)^2} \\ \frac{y^{1/2}}{2x^{1/2}} & \frac{y^{1/2}}{2x^{1/2}} \end{pmatrix}, $$ whence, for $(x,y)$ in $D$, the directional derivative $$ \left(\nabla f(x,y)\right)\begin{pmatrix} x \\ y \end{pmatrix}, $$ is always positive and I would like to conclude that, on $D$, $$ \text{$f$ is minimal at $(1,1)$},\\ \text{$f$ is maximal at $(2,2)$}. $$ Is it the right way to proceed? The graph of each component of $f$ looks like this","For a single valued function, I can infer if the function is monotone from its derivative. For a vector valued function, is it possible to infer monotonicity from the directional derivative? For example, define  $$ D=[1,2]\times[1,2], $$  and $$ f(x,y)=\left( \frac{2}{1/x+1/y},\sqrt{xy} \right). $$ Is it possible to show that $f$ maps $D$ to $D$ from its gradient $\nabla f$? The gradient is $$ \nabla f = \begin{pmatrix} \frac{2}{\left(1+x/y\right)^2} & \frac{2}{\left(1+y/x\right)^2} \\ \frac{y^{1/2}}{2x^{1/2}} & \frac{y^{1/2}}{2x^{1/2}} \end{pmatrix}, $$ whence, for $(x,y)$ in $D$, the directional derivative $$ \left(\nabla f(x,y)\right)\begin{pmatrix} x \\ y \end{pmatrix}, $$ is always positive and I would like to conclude that, on $D$, $$ \text{$f$ is minimal at $(1,1)$},\\ \text{$f$ is maximal at $(2,2)$}. $$ Is it the right way to proceed? The graph of each component of $f$ looks like this",,[]
57,Help with Taylor polynomial,Help with Taylor polynomial,,"I need to find the Taylor polynomial of order $2n$ of the function $$f(x,y)=\frac{1}{1+xy}$$ on  $(x_0,y_0)=(0,0)$. Can anyone give me a hand please? Sorry, i make a mistake, their should say ""polynomial"" instead of ""series, i have corrected now.","I need to find the Taylor polynomial of order $2n$ of the function $$f(x,y)=\frac{1}{1+xy}$$ on  $(x_0,y_0)=(0,0)$. Can anyone give me a hand please? Sorry, i make a mistake, their should say ""polynomial"" instead of ""series, i have corrected now.",,['multivariable-calculus']
58,Ask a question about an example in a course note on optimization problem with equality constraint,Ask a question about an example in a course note on optimization problem with equality constraint,,"I have two difficulties on understanding the solution to an example in a course I took this semester on optimization. This example is given to illustrate the usage of Lagrange multiplier method (please see Example 1 in the image below: $${\Large ?}\;\left\{\begin{align*}(x_1-x_2)(x_2-x_3)(x_3-x_1)&=0\\x_1^2+x_2^2+x_3^2&=4\\x_1+x_2+x_3&=1\end{align*}\right.$$   One solution is $$x_1=x_2,\;x_3=1-2x_1,\;2x_1^2+(1-2x_1)^2=4 \\ \left(\dfrac13+\dfrac{\sqrt{22}}6,\;\dfrac13+\dfrac{\sqrt{22}}6,\;\dfrac13-\dfrac{\sqrt{22}}3\right) \\ \left(\dfrac13-\dfrac{\sqrt{22}}6,\;\dfrac13-\dfrac{\sqrt{22}}6,\;\dfrac13+\dfrac{\sqrt{22}}3\right)$$ and permutations of these. $\underline{\text{Example 2}:}$ $$\begin{array}{ll}\text{minimize}&ax\\\text{subject to}&x_1x_2+x_1x_3+x_2x_3=0\\&x_1^2+x_2^2+x_3^2=1\end{array}$$   Necessary conditions for optimality: $$\eqalign{&{\Large\Rightarrow}\;\;\;\left\{\begin{array}{l}a_1+\lambda_1(x_2+x_3)+2\lambda_2x_1=0\\a_2+\lambda_1(x_3+x_1)+2\lambda_2x_2=0\\a_3+\lambda_1(x_1+x_2)+2\lambda_2x_3=0\\x_1x_2+x_1x_3+x_2x_3=0\\x_1^2+x_2^2+_3^2=1\end{array}\right.\\ &{\Large\Rightarrow}\;\;\;\left|\begin{matrix}a_1 & x_2+x_3 & x_1 \\ a_2 & x_3+x_1 & x_2 \\ a_3 & x_1+x_2 & x_3\end{matrix}\right|=0\\ &{\Large\Rightarrow}\;\;\;\underbrace{(x_1+x_2+x_3)}_{\neq0}\left|\begin{matrix}a_1 & 1 & x_1 \\ a_2 & 1 & x_2 \\ a_3 & 1 & x_3\end{matrix}\right|=0.}$$ I marked my question with a red ""?"" in the image. I can not understand how these two steps comes out. Could you please help me? I also have the same difficulty for the second example, I hope, after I solve the first example with your help, I can understand the second one myself. But if I still fail to understand it, I'll post it and ask for help here. Sorry for my dullness in math and thank you for any help.","I have two difficulties on understanding the solution to an example in a course I took this semester on optimization. This example is given to illustrate the usage of Lagrange multiplier method (please see Example 1 in the image below: $${\Large ?}\;\left\{\begin{align*}(x_1-x_2)(x_2-x_3)(x_3-x_1)&=0\\x_1^2+x_2^2+x_3^2&=4\\x_1+x_2+x_3&=1\end{align*}\right.$$   One solution is $$x_1=x_2,\;x_3=1-2x_1,\;2x_1^2+(1-2x_1)^2=4 \\ \left(\dfrac13+\dfrac{\sqrt{22}}6,\;\dfrac13+\dfrac{\sqrt{22}}6,\;\dfrac13-\dfrac{\sqrt{22}}3\right) \\ \left(\dfrac13-\dfrac{\sqrt{22}}6,\;\dfrac13-\dfrac{\sqrt{22}}6,\;\dfrac13+\dfrac{\sqrt{22}}3\right)$$ and permutations of these. $\underline{\text{Example 2}:}$ $$\begin{array}{ll}\text{minimize}&ax\\\text{subject to}&x_1x_2+x_1x_3+x_2x_3=0\\&x_1^2+x_2^2+x_3^2=1\end{array}$$   Necessary conditions for optimality: $$\eqalign{&{\Large\Rightarrow}\;\;\;\left\{\begin{array}{l}a_1+\lambda_1(x_2+x_3)+2\lambda_2x_1=0\\a_2+\lambda_1(x_3+x_1)+2\lambda_2x_2=0\\a_3+\lambda_1(x_1+x_2)+2\lambda_2x_3=0\\x_1x_2+x_1x_3+x_2x_3=0\\x_1^2+x_2^2+_3^2=1\end{array}\right.\\ &{\Large\Rightarrow}\;\;\;\left|\begin{matrix}a_1 & x_2+x_3 & x_1 \\ a_2 & x_3+x_1 & x_2 \\ a_3 & x_1+x_2 & x_3\end{matrix}\right|=0\\ &{\Large\Rightarrow}\;\;\;\underbrace{(x_1+x_2+x_3)}_{\neq0}\left|\begin{matrix}a_1 & 1 & x_1 \\ a_2 & 1 & x_2 \\ a_3 & 1 & x_3\end{matrix}\right|=0.}$$ I marked my question with a red ""?"" in the image. I can not understand how these two steps comes out. Could you please help me? I also have the same difficulty for the second example, I hope, after I solve the first example with your help, I can understand the second one myself. But if I still fail to understand it, I'll post it and ask for help here. Sorry for my dullness in math and thank you for any help.",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'optimization']"
59,Understanding some differential notations...,Understanding some differential notations...,,"What this notation mean? (I know that is a partial derivative, but I don't understand the meaning of the evaluation bar at the right) $$\frac{\partial g}{\partial T}\Big|_{SA,p}$$ Is this relation true? $$\frac{\partial g}{\partial T}\Big|_{SA,p}=\left(\frac{\partial g}{\partial T}\right) _{SA,p}$$ Also used here $$d\rho=\left(\frac{\partial \rho}{\partial T}\right)_{S,p}+dT\left(\frac{\partial \rho}{\partial S}\right)_{T,p}+dS\left(\frac{\partial \rho}{\partial p}\right)_{T,S}dp$$ And another thing, the $$d\rho$$ must be intended as the differential of the function $$\rho$$? So if I have understood well, the equality can be rewritten as: \begin{equation*} \frac{1}{\rho}\left(\frac{\partial \rho}{\partial T}dT+\frac{\partial \rho}{\partial S}dS+\frac{\partial \rho}{\partial p}dp\right)=-\rho\left(\frac{\partial \rho^{-1}}{\partial T}dT+\frac{\partial \rho^{-1}}{\partial S}dS+\frac{\partial \rho^{-1}}{\partial p}dp\right) \end{equation*} \begin{equation*} \frac{1}{\rho}\sum_{i=1}^3\frac{\partial \rho}{\partial x_i}dx_i=-\rho\sum_{i=1}^3\frac{\partial \rho^{-1}}{\partial x_i}dx_i \end{equation*} Proof: $$ \frac{d\rho}{\rho}=-\frac{d\alpha}{\alpha}=-\rho d\rho^{-1}=-\rho\sum_{i=1}^3\frac{\partial \rho^{-1}}{\partial x_i}dx_i $$ Where $$ \alpha=\frac{1}{\rho}\\ \rho=f(S,T,p)\\ $$ $\Box$","What this notation mean? (I know that is a partial derivative, but I don't understand the meaning of the evaluation bar at the right) $$\frac{\partial g}{\partial T}\Big|_{SA,p}$$ Is this relation true? $$\frac{\partial g}{\partial T}\Big|_{SA,p}=\left(\frac{\partial g}{\partial T}\right) _{SA,p}$$ Also used here $$d\rho=\left(\frac{\partial \rho}{\partial T}\right)_{S,p}+dT\left(\frac{\partial \rho}{\partial S}\right)_{T,p}+dS\left(\frac{\partial \rho}{\partial p}\right)_{T,S}dp$$ And another thing, the $$d\rho$$ must be intended as the differential of the function $$\rho$$? So if I have understood well, the equality can be rewritten as: \begin{equation*} \frac{1}{\rho}\left(\frac{\partial \rho}{\partial T}dT+\frac{\partial \rho}{\partial S}dS+\frac{\partial \rho}{\partial p}dp\right)=-\rho\left(\frac{\partial \rho^{-1}}{\partial T}dT+\frac{\partial \rho^{-1}}{\partial S}dS+\frac{\partial \rho^{-1}}{\partial p}dp\right) \end{equation*} \begin{equation*} \frac{1}{\rho}\sum_{i=1}^3\frac{\partial \rho}{\partial x_i}dx_i=-\rho\sum_{i=1}^3\frac{\partial \rho^{-1}}{\partial x_i}dx_i \end{equation*} Proof: $$ \frac{d\rho}{\rho}=-\frac{d\alpha}{\alpha}=-\rho d\rho^{-1}=-\rho\sum_{i=1}^3\frac{\partial \rho^{-1}}{\partial x_i}dx_i $$ Where $$ \alpha=\frac{1}{\rho}\\ \rho=f(S,T,p)\\ $$ $\Box$",,"['multivariable-calculus', 'notation', 'partial-derivative']"
60,Quadratic surface maximization and Hessians,Quadratic surface maximization and Hessians,,"If we have that the contours of a response surface are elliptical and the response is given by the following function: $$\large \exp\left(-\left(w^2 + \frac{1}{4}l^2 -\frac{1}{4} \cdot w \cdot l\right)\right)$$ then if we maximize this function w.r.t $l$ holding $w$ fixed at $1/2$. And if we call the maximizer l-star, then holding l-star fixed, maximize over w. How to show that the overall max isn't achieved? My approach: I got the partial of the above function w.r.t. $l$ and then tried to evaluate it at $1/2$, but got stuck. It most likely will involve some analysis of Hessians.","If we have that the contours of a response surface are elliptical and the response is given by the following function: $$\large \exp\left(-\left(w^2 + \frac{1}{4}l^2 -\frac{1}{4} \cdot w \cdot l\right)\right)$$ then if we maximize this function w.r.t $l$ holding $w$ fixed at $1/2$. And if we call the maximizer l-star, then holding l-star fixed, maximize over w. How to show that the overall max isn't achieved? My approach: I got the partial of the above function w.r.t. $l$ and then tried to evaluate it at $1/2$, but got stuck. It most likely will involve some analysis of Hessians.",,"['calculus', 'real-analysis', 'linear-algebra', 'analysis', 'multivariable-calculus']"
61,Laplacian $\Delta u$ in spherical coordinates,Laplacian  in spherical coordinates,\Delta u,"The Laplacian $\Delta u$ in spherical coordinates is  $$\Delta u=\frac{\partial^2u}{\partial\rho^2}+\frac{2}{\rho}\frac{\partial u}{\partial\rho}+\frac{1}{\rho^2}\frac{\partial^2u}{\partial\phi^2}+\frac{\cot\phi}{\rho^2}\frac{\partial u}{\partial\phi}+\frac{1}{\rho^2\sin^2\phi}\frac{\partial^2u}{\partial\theta^2}$$  The way\tag 1 I proved this is a bit tricky. I wonder if there is a simpler way to do this, e.g. using vectors or matrices , so the formula can be produced in a Computer Algebra System.","The Laplacian $\Delta u$ in spherical coordinates is  $$\Delta u=\frac{\partial^2u}{\partial\rho^2}+\frac{2}{\rho}\frac{\partial u}{\partial\rho}+\frac{1}{\rho^2}\frac{\partial^2u}{\partial\phi^2}+\frac{\cot\phi}{\rho^2}\frac{\partial u}{\partial\phi}+\frac{1}{\rho^2\sin^2\phi}\frac{\partial^2u}{\partial\theta^2}$$  The way\tag 1 I proved this is a bit tricky. I wonder if there is a simpler way to do this, e.g. using vectors or matrices , so the formula can be produced in a Computer Algebra System.",,"['complex-analysis', 'analysis', 'multivariable-calculus', 'partial-differential-equations']"
62,"Using Green's Theorem,calculate the integral....","Using Green's Theorem,calculate the integral....",,"Using Green's Theorem, calculate the closed integral over $C$ $$\oint_C(3x+4y)dx+(2x-3y)dy$$ where $C$ is the circle of radius $4$ units,with its centre at the origin of the $x,y$-plane. My approach: From Green's Theorem, $$\oint_C(3x+4y)dx+(2x-3y)dy = \iint_A  (\frac{\partial}{\partial y}(3x+4y) + \frac{\partial}{\partial x}(2x-3y)) dx \,dy$$ $\frac{\partial}{\partial y}(3x+4y)= 4$ and $\frac{\partial}{\partial x}(2x-3y)=2$, which combines to form $\iint 6\,dx\,dy$. Now my question is that from the question what is the limit of the two integrals I am going to take, is it one integral from $0$ to $2 \pi$ and another $0$ to $4$ or something else... Please see my approach and tell me whether  I am going or wrong or right??","Using Green's Theorem, calculate the closed integral over $C$ $$\oint_C(3x+4y)dx+(2x-3y)dy$$ where $C$ is the circle of radius $4$ units,with its centre at the origin of the $x,y$-plane. My approach: From Green's Theorem, $$\oint_C(3x+4y)dx+(2x-3y)dy = \iint_A  (\frac{\partial}{\partial y}(3x+4y) + \frac{\partial}{\partial x}(2x-3y)) dx \,dy$$ $\frac{\partial}{\partial y}(3x+4y)= 4$ and $\frac{\partial}{\partial x}(2x-3y)=2$, which combines to form $\iint 6\,dx\,dy$. Now my question is that from the question what is the limit of the two integrals I am going to take, is it one integral from $0$ to $2 \pi$ and another $0$ to $4$ or something else... Please see my approach and tell me whether  I am going or wrong or right??",,"['calculus', 'multivariable-calculus']"
63,Sketch of the ordinate set of $f$,Sketch of the ordinate set of,f,"Let $f$ be defined on $[0,1] \times [0,1]$ as follows: $f(x,y)= \begin{cases} x+y \mbox{ if } x^2 \leq y \leq 2x^2 \\ 0 \mbox{ otherwise} \end{cases}$ I want to make a sketch of the ordinate set of $f$ over $[0,1] \times [0,1]$  and compute the volume of this ordinate set by double integration (Assuming the integral exists)","Let $f$ be defined on $[0,1] \times [0,1]$ as follows: $f(x,y)= \begin{cases} x+y \mbox{ if } x^2 \leq y \leq 2x^2 \\ 0 \mbox{ otherwise} \end{cases}$ I want to make a sketch of the ordinate set of $f$ over $[0,1] \times [0,1]$  and compute the volume of this ordinate set by double integration (Assuming the integral exists)",,"['calculus', 'integration']"
64,Boundary of the Difference,Boundary of the Difference,,"I'm trying to prove or give a counterexample for the following: given $A, B \subset \mathbb{R}^n$ such that $A\cap B \neq \emptyset$. Let $A'=A-B$, then $\partial A' = \partial A - \partial B$. I'm not sure if that's true, but I'm trying to prove or give a counterexample. As I didn't find any point to start the proof, I don't have anything to show here, however I just want a hint on how to start. I don't want any detailed proof, I just want a little hint to get started with this proof. Thanks in advance for your help. And sorry if this is too basic.","I'm trying to prove or give a counterexample for the following: given $A, B \subset \mathbb{R}^n$ such that $A\cap B \neq \emptyset$. Let $A'=A-B$, then $\partial A' = \partial A - \partial B$. I'm not sure if that's true, but I'm trying to prove or give a counterexample. As I didn't find any point to start the proof, I don't have anything to show here, however I just want a hint on how to start. I don't want any detailed proof, I just want a little hint to get started with this proof. Thanks in advance for your help. And sorry if this is too basic.",,"['real-analysis', 'multivariable-calculus']"
65,Product rule for inner products using the 3 conditions,Product rule for inner products using the 3 conditions,,"I understand there are multiple ways of of proving the product rule for the derivative of an inner product, though I cannot figure out how to do this one specifically: let $\alpha,\beta :R \rightarrow R^n$ be differentiable functions. If $ f(t)=\langle \alpha(t),\beta(t) \rangle,$ using only these three rules (IE the conditions for inner product): $\langle x,x \rangle > 0$ if $x \not= 0$ $\langle x,y \rangle = \langle y,x \rangle $ $ \langle ax+by,z \rangle = a\langle x,z \rangle + b\langle y,z \rangle $ show that $f'(t) = \langle \alpha(t), \beta'(t) \rangle + \langle \alpha'(t), \beta(t) \rangle $ Basically I understand how to do it using summations, the normal product rule and and the linearity of summations but I do not understand how to get this result another way. There was another question asked that did it by taking the limits, which also makes sense to me. thanks for any suggestions/help.","I understand there are multiple ways of of proving the product rule for the derivative of an inner product, though I cannot figure out how to do this one specifically: let $\alpha,\beta :R \rightarrow R^n$ be differentiable functions. If $ f(t)=\langle \alpha(t),\beta(t) \rangle,$ using only these three rules (IE the conditions for inner product): $\langle x,x \rangle > 0$ if $x \not= 0$ $\langle x,y \rangle = \langle y,x \rangle $ $ \langle ax+by,z \rangle = a\langle x,z \rangle + b\langle y,z \rangle $ show that $f'(t) = \langle \alpha(t), \beta'(t) \rangle + \langle \alpha'(t), \beta(t) \rangle $ Basically I understand how to do it using summations, the normal product rule and and the linearity of summations but I do not understand how to get this result another way. There was another question asked that did it by taking the limits, which also makes sense to me. thanks for any suggestions/help.",,"['real-analysis', 'linear-algebra', 'multivariable-calculus']"
66,Arc length parametrisation is reparametrisation,Arc length parametrisation is reparametrisation,,"I was trying to show that if $\gamma : [a,b] \to \mathbb R^3$ is a curve and $$ p(t) = \int_{t_0}^t |\gamma'(\tau)|d\tau$$ where $t_0 \in [a,b]$ then $p^{-1}: [c,d] \to [a,b]$ is a reparametrisation of $\gamma$. This is exercise 2 here . But I am not even sure what I need to show: $p^{-1}$ is a bijection and $(p^{-1})'(s) \neq 0$ for all $s$ and $\gamma ( p^{-1}(s)) = \gamma (s)$ for all $s$? I tried to show 3. but the problem is that it does not even have same domain on both side. How to show that $p^{-1}$ is a reparametrisation of $\gamma$? Thank you for help.","I was trying to show that if $\gamma : [a,b] \to \mathbb R^3$ is a curve and $$ p(t) = \int_{t_0}^t |\gamma'(\tau)|d\tau$$ where $t_0 \in [a,b]$ then $p^{-1}: [c,d] \to [a,b]$ is a reparametrisation of $\gamma$. This is exercise 2 here . But I am not even sure what I need to show: $p^{-1}$ is a bijection and $(p^{-1})'(s) \neq 0$ for all $s$ and $\gamma ( p^{-1}(s)) = \gamma (s)$ for all $s$? I tried to show 3. but the problem is that it does not even have same domain on both side. How to show that $p^{-1}$ is a reparametrisation of $\gamma$? Thank you for help.",,['multivariable-calculus']
67,Constrained maximization problem,Constrained maximization problem,,"I need help with the following optimization problem $$ \max\;\alpha\ln(x(1-y^2))+(1-\alpha)\ln(z) $$ where the maximization is with respect to $x,y,z$, subject to \begin{align} \alpha x+(1-\alpha)z&=C_1\\ \alpha y\sqrt{x(x+\gamma)}-\alpha x&=C_2 \end{align} where $0\leq\alpha\leq1$, $\gamma>0$, and $x,z\geq0$, and $|y|\leq1$. Generally, one can substitute the constraints in the objective function and maximize with respect to one parameter. The problem is that in this way things become algebraically complicated, and I believe that there is a simple solution. Thank you","I need help with the following optimization problem $$ \max\;\alpha\ln(x(1-y^2))+(1-\alpha)\ln(z) $$ where the maximization is with respect to $x,y,z$, subject to \begin{align} \alpha x+(1-\alpha)z&=C_1\\ \alpha y\sqrt{x(x+\gamma)}-\alpha x&=C_2 \end{align} where $0\leq\alpha\leq1$, $\gamma>0$, and $x,z\geq0$, and $|y|\leq1$. Generally, one can substitute the constraints in the objective function and maximize with respect to one parameter. The problem is that in this way things become algebraically complicated, and I believe that there is a simple solution. Thank you",,"['calculus', 'multivariable-calculus', 'convex-optimization']"
68,Vector Calculus proof bogus or not?,Vector Calculus proof bogus or not?,,Consider an arbitrary vector real valued function smooth and continuous $\mathbf{r}$ and $\mathbf{r} \cdot \mathbf{r} = \| \mathbf{r} \|^2$ Given $ (\| \mathbf{r} \|^2)'  = (\mathbf{r} \cdot \mathbf{r})' = \mathbf{r}' \cdot \mathbf{r} + \mathbf{r} \cdot \mathbf{r}' = 2\mathbf{r} \cdot \mathbf{r}' = 0$ Does it follow that $\|\mathbf{r}\|$ is a constant because $(\| \mathbf{r} \|^2)' = 0$?,Consider an arbitrary vector real valued function smooth and continuous $\mathbf{r}$ and $\mathbf{r} \cdot \mathbf{r} = \| \mathbf{r} \|^2$ Given $ (\| \mathbf{r} \|^2)'  = (\mathbf{r} \cdot \mathbf{r})' = \mathbf{r}' \cdot \mathbf{r} + \mathbf{r} \cdot \mathbf{r}' = 2\mathbf{r} \cdot \mathbf{r}' = 0$ Does it follow that $\|\mathbf{r}\|$ is a constant because $(\| \mathbf{r} \|^2)' = 0$?,,['multivariable-calculus']
69,"Equality of two integrals on $C[0,1]$, does it always mean the equality of the integrands?","Equality of two integrals on , does it always mean the equality of the integrands?","C[0,1]","Is it true that : If for $f,g \in C[0,1]$, $k\in \mathbb{N}$  it holds : $$\int_0^1 f(x)x^kdx= \int_0^1g(x)x^kdx$$ Then $$f=g$$ What have I tried : One can rewrite: $$ \int_0^1 (f(x)-g(x))x^kdx=0$$ f,g are continuous on $C[0,1]$ and from the approximation theorem of Stone-Weierstrass one knows that there is a sequence $\{p_n\}$ of polynomials that converges uniformly to $f(x)$ for $n\rightarrow \infty$ and also one $\{p_t\}$ which converges uniformly to $g(x)$  for $t\rightarrow \infty $ so: $$\int_0^1  \lim_{n\rightarrow \infty} \{p_n\}x^kdx = \int_0^1 \lim_{t\rightarrow \infty} \{p_t\}x^kdx =\lim_{n\rightarrow \infty}\int_0^1\{p_n\}x^kdx=\lim_{t\rightarrow \infty}\int_0^1\{p_t\}x^k dx $$ this doesnt work How does one go about showing it?","Is it true that : If for $f,g \in C[0,1]$, $k\in \mathbb{N}$  it holds : $$\int_0^1 f(x)x^kdx= \int_0^1g(x)x^kdx$$ Then $$f=g$$ What have I tried : One can rewrite: $$ \int_0^1 (f(x)-g(x))x^kdx=0$$ f,g are continuous on $C[0,1]$ and from the approximation theorem of Stone-Weierstrass one knows that there is a sequence $\{p_n\}$ of polynomials that converges uniformly to $f(x)$ for $n\rightarrow \infty$ and also one $\{p_t\}$ which converges uniformly to $g(x)$  for $t\rightarrow \infty $ so: $$\int_0^1  \lim_{n\rightarrow \infty} \{p_n\}x^kdx = \int_0^1 \lim_{t\rightarrow \infty} \{p_t\}x^kdx =\lim_{n\rightarrow \infty}\int_0^1\{p_n\}x^kdx=\lim_{t\rightarrow \infty}\int_0^1\{p_t\}x^k dx $$ this doesnt work How does one go about showing it?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
70,"Is $(C_{0}(X), ||.||_{\infty})$ a Banach Algebra?",Is  a Banach Algebra?,"(C_{0}(X), ||.||_{\infty})","Is $(C_{0}(X), ||.||_{\infty})$ a Banach Algebra? Given $$C_{0}(\mathbb{R}^{n})=\{f\in C(\mathbb{R^n} \ | \ \ \exists R \ge 0 \ \text{such that } f(x)=0 \ \text{for} \ ||x||\ge R  \}$$ and $$||f(x)||_{\infty} = \max_{x\in R^n}|f(x)|  $$ What I have tried: $C_{0}(\mathbb{R}^n)$  is the space of all continous functions whose values can be restricted by a constant R by restricting the largest possible coordinate in a vector from the domain set. Let $(f_n)_{n\in \mathbb{N}}$ be a cauchy sequence in $C_{0}(\mathbb{R}^n)$: $$|| f_n-f_m||_{\infty} < \frac{\epsilon}{2} \\ \text{for n,m greater than an index N}$$ because of the completeness of $\mathbb{R}^n$ there exists a $N_x \ge N$ for each $x\in \mathbb{R^n}$ so that: $$|f_{N_x}-f(x)| < \frac{\epsilon}{2}$$ this leads to : $$|f_n-f| \le ||f_n-f_{N_x}||_\infty+|f_{N_x}-f | \le \epsilon $$ and if one takes the max norm of it: $$|| f_n- f ||_\infty \le \epsilon$$ which is uniform convergence of $f_n$ to $f$ . $(C_{0}(X),||.||_\infty)$ is a Banach space. To  be a Banach Algebra, closure under multiplication for all elements of $C_{0}(X)$ is also needed (otherwise cant check the Algebra multiplication conditions), if one function has $R_1=2$ and the other $R_2=4$ , then multiplying f(x)f(y) it would not work with the closure under multiplication, so it can not be a Banach Algebra ... Does $C_{0}(X)$ have a common name ?","Is $(C_{0}(X), ||.||_{\infty})$ a Banach Algebra? Given $$C_{0}(\mathbb{R}^{n})=\{f\in C(\mathbb{R^n} \ | \ \ \exists R \ge 0 \ \text{such that } f(x)=0 \ \text{for} \ ||x||\ge R  \}$$ and $$||f(x)||_{\infty} = \max_{x\in R^n}|f(x)|  $$ What I have tried: $C_{0}(\mathbb{R}^n)$  is the space of all continous functions whose values can be restricted by a constant R by restricting the largest possible coordinate in a vector from the domain set. Let $(f_n)_{n\in \mathbb{N}}$ be a cauchy sequence in $C_{0}(\mathbb{R}^n)$: $$|| f_n-f_m||_{\infty} < \frac{\epsilon}{2} \\ \text{for n,m greater than an index N}$$ because of the completeness of $\mathbb{R}^n$ there exists a $N_x \ge N$ for each $x\in \mathbb{R^n}$ so that: $$|f_{N_x}-f(x)| < \frac{\epsilon}{2}$$ this leads to : $$|f_n-f| \le ||f_n-f_{N_x}||_\infty+|f_{N_x}-f | \le \epsilon $$ and if one takes the max norm of it: $$|| f_n- f ||_\infty \le \epsilon$$ which is uniform convergence of $f_n$ to $f$ . $(C_{0}(X),||.||_\infty)$ is a Banach space. To  be a Banach Algebra, closure under multiplication for all elements of $C_{0}(X)$ is also needed (otherwise cant check the Algebra multiplication conditions), if one function has $R_1=2$ and the other $R_2=4$ , then multiplying f(x)f(y) it would not work with the closure under multiplication, so it can not be a Banach Algebra ... Does $C_{0}(X)$ have a common name ?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
71,Generalized divergence theorem,Generalized divergence theorem,,"In the common divergence theorem, shall the boundary (surface) not be smooth everywhere? Is there a version of this theorem where the boundary is nowhere differentiable?","In the common divergence theorem, shall the boundary (surface) not be smooth everywhere? Is there a version of this theorem where the boundary is nowhere differentiable?",,['multivariable-calculus']
72,Surface Integral - Intersection of cone and circle,Surface Integral - Intersection of cone and circle,,"I am now looking at the following exercise: Calculate:  $\int \int_S (2z^2 - x^2 - y^2) dS $ where $S $ is $ z=\sqrt{x^2+y^2} $ intersected with $x^2 + y^2 =2x$  (i.e. $ (x-1)^2 + y^2 =1 $ ) . Now, as far as definitions: $\int \int_S (2z^2 - x^2 - y^2) dS  = \int \int_D f(x(u,v),y(u,v),z(u,v)) ||\phi_u \times \phi_v || dudv $  where $\phi:D \to \mathbb{R}^3 $ is the parameterization of $S$ . But, how can I parameterize $S$ ? It is the intersection of the cone and the cylinder, but how can I do it? Thanks in advance","I am now looking at the following exercise: Calculate:  $\int \int_S (2z^2 - x^2 - y^2) dS $ where $S $ is $ z=\sqrt{x^2+y^2} $ intersected with $x^2 + y^2 =2x$  (i.e. $ (x-1)^2 + y^2 =1 $ ) . Now, as far as definitions: $\int \int_S (2z^2 - x^2 - y^2) dS  = \int \int_D f(x(u,v),y(u,v),z(u,v)) ||\phi_u \times \phi_v || dudv $  where $\phi:D \to \mathbb{R}^3 $ is the parameterization of $S$ . But, how can I parameterize $S$ ? It is the intersection of the cone and the cylinder, but how can I do it? Thanks in advance",,"['calculus', 'multivariable-calculus']"
73,"Finding local minima / maxima of multivariable function $f(x,y) = x^3 + 2y^2+3xy+y$",Finding local minima / maxima of multivariable function,"f(x,y) = x^3 + 2y^2+3xy+y","Suppose we have a function $f(x,y) = x^3 + 2y^2+3xy+y$ To find critical points of $f$, we compute its gradient: $\nabla f=(3x^2+3y,\ 3x+4y+1)$ Solving this pair of equations, we see that the critical points of $f$ are $[1, -1]$ and $ [ {-1 \over 4}, {-1 \over 16} ]$. The Hessian matrix for function $f$ is: $$ \nabla^2 f =  \begin{pmatrix} 6x & 3 \\ 3 & 4 \\ \end{pmatrix} $$ Since the determinant of this self-adjoint matrix is in the form: $det(\nabla^2 f) = 24x - 9$ Then the determinant of $\nabla^2 f$ is positive when $x > {9 \over 24}$ Looking at each of the critical points, using Sylvester's criterion we see that $det(\nabla^2 f(1,-1)) = 15$, which is positive, therefore $[1,-1]$ is a local minimum $f$ and $det(\nabla^2 f({-1 \over 4}, {-1 \over 16})) = {6 \over 4} * 4 - 9 = -3$, which is negative, therefore $[{-1 \over 4}, {-1 \over 16}]$ is a local maximum of $f$ Is this result correct?","Suppose we have a function $f(x,y) = x^3 + 2y^2+3xy+y$ To find critical points of $f$, we compute its gradient: $\nabla f=(3x^2+3y,\ 3x+4y+1)$ Solving this pair of equations, we see that the critical points of $f$ are $[1, -1]$ and $ [ {-1 \over 4}, {-1 \over 16} ]$. The Hessian matrix for function $f$ is: $$ \nabla^2 f =  \begin{pmatrix} 6x & 3 \\ 3 & 4 \\ \end{pmatrix} $$ Since the determinant of this self-adjoint matrix is in the form: $det(\nabla^2 f) = 24x - 9$ Then the determinant of $\nabla^2 f$ is positive when $x > {9 \over 24}$ Looking at each of the critical points, using Sylvester's criterion we see that $det(\nabla^2 f(1,-1)) = 15$, which is positive, therefore $[1,-1]$ is a local minimum $f$ and $det(\nabla^2 f({-1 \over 4}, {-1 \over 16})) = {6 \over 4} * 4 - 9 = -3$, which is negative, therefore $[{-1 \over 4}, {-1 \over 16}]$ is a local maximum of $f$ Is this result correct?",,"['multivariable-calculus', 'optimization']"
74,"Prove that $f\circ g$ is differentiable in t=0, but the chain rule doesn't work.","Prove that  is differentiable in t=0, but the chain rule doesn't work.",f\circ g,"Given $f(x,y)=\begin{cases}\frac{x^2y}{x^2+y^2} &\text{ if }(x,y) \neq (0,0)\\\\ 0&\text{ if }(x,y)=(0,0)\end{cases}$ and let $g(t)=(t,-2t).$ Prove that $f\circ g$ is differentiable in t=0, but the chain rule doesn't work.","Given $f(x,y)=\begin{cases}\frac{x^2y}{x^2+y^2} &\text{ if }(x,y) \neq (0,0)\\\\ 0&\text{ if }(x,y)=(0,0)\end{cases}$ and let $g(t)=(t,-2t).$ Prove that $f\circ g$ is differentiable in t=0, but the chain rule doesn't work.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
75,Having Difficuilty With Evaluating A Triple Integral,Having Difficuilty With Evaluating A Triple Integral,,Here's the question $$\int_{2}^{4}\int_{1}^{2}\int_{0}^{4}xy(z+2)dxdydz$$ Is it right to distribute $xy$ to $z$ and $2$ obtaining $$\int_{2}^{4}\int_{1}^{2}\int_{0}^{4}(xyz+2xy)dxdydz ?$$ and then start with the inner most integral i.e $$\int_{0}^{4}(xyz + 2xy)dx$$ and integrate only with respect to $x$ keeping the rest constant and evaluating between $0$ and $4$. $$i.e \left (\frac{x^2}{2}yz + x^2y  \right )_{0}^{4}$$ $$=8yz + 16y $$ Does the next step invlove integrating the above expression $w.r.t$ $y$ keeping $z$ constant?i.e$$\int_{1}^{2}(8yz + 16y)dy$$ If i follow the above step i get the final answer to be $180$ which is obviously wrong.As per the text book that i'am using it's $120$.,Here's the question $$\int_{2}^{4}\int_{1}^{2}\int_{0}^{4}xy(z+2)dxdydz$$ Is it right to distribute $xy$ to $z$ and $2$ obtaining $$\int_{2}^{4}\int_{1}^{2}\int_{0}^{4}(xyz+2xy)dxdydz ?$$ and then start with the inner most integral i.e $$\int_{0}^{4}(xyz + 2xy)dx$$ and integrate only with respect to $x$ keeping the rest constant and evaluating between $0$ and $4$. $$i.e \left (\frac{x^2}{2}yz + x^2y  \right )_{0}^{4}$$ $$=8yz + 16y $$ Does the next step invlove integrating the above expression $w.r.t$ $y$ keeping $z$ constant?i.e$$\int_{1}^{2}(8yz + 16y)dy$$ If i follow the above step i get the final answer to be $180$ which is obviously wrong.As per the text book that i'am using it's $120$.,,"['multivariable-calculus', 'definite-integrals']"
76,Mass and center of mass of lamina in polar coordinates,Mass and center of mass of lamina in polar coordinates,,"I need some help with the following problem which is question number 15.5.4 in the seventh edition of Stewart Calculus.  Here is the problem definition: ""Find the mass and center of mass of the lamina that occupies the region D and has the given density function $\rho$, where:  $D={(x,y) | 0\le x \le a, 0 \le y \le b}$ and $\rho (x,y) =1+x^2+y^2 $"" I did this in rectangular coordinates, but the work and answer are too complicated.  I need help doing this in polar coordinates. I see that $z=1+x^2 +y^2=1+r^2$, the graph of which is easy to visualize. I need help getting started in converting the following into polar coordinates: $m=\int\int_D \rho(x,y) dA =\int_0^a\int_0^b(1+x^2+y^2)dy dx$ $\bar{x}=\frac{1}{m}\int\int_Dx\rho(x,y)dA$ $\bar{y}=\frac{1}{m}\int\int_Dy\rho(x,y)dA$ Then solve for center of mass $(\bar{x},\bar{y})$ It would seem obvious that $m=\int\int_D \rho(x,y) dA =\int\int_D (1+r^2)r dr d\theta$, but the range of integration is what I do not understand.  I tried using $0\le r\le \frac{b}{sin{\theta}}$ and $0\le \theta \le \arcsin{\frac{b}{r}}$ , but got an undefined result from my TI-89 calculator. If someone can show me how to set up these integrals in polar coordinates, I think I could do the integration myself.  However, I would hope to have someone check my answers to the integrals so that I make sure to geth the mass and center of mass correct.","I need some help with the following problem which is question number 15.5.4 in the seventh edition of Stewart Calculus.  Here is the problem definition: ""Find the mass and center of mass of the lamina that occupies the region D and has the given density function $\rho$, where:  $D={(x,y) | 0\le x \le a, 0 \le y \le b}$ and $\rho (x,y) =1+x^2+y^2 $"" I did this in rectangular coordinates, but the work and answer are too complicated.  I need help doing this in polar coordinates. I see that $z=1+x^2 +y^2=1+r^2$, the graph of which is easy to visualize. I need help getting started in converting the following into polar coordinates: $m=\int\int_D \rho(x,y) dA =\int_0^a\int_0^b(1+x^2+y^2)dy dx$ $\bar{x}=\frac{1}{m}\int\int_Dx\rho(x,y)dA$ $\bar{y}=\frac{1}{m}\int\int_Dy\rho(x,y)dA$ Then solve for center of mass $(\bar{x},\bar{y})$ It would seem obvious that $m=\int\int_D \rho(x,y) dA =\int\int_D (1+r^2)r dr d\theta$, but the range of integration is what I do not understand.  I tried using $0\le r\le \frac{b}{sin{\theta}}$ and $0\le \theta \le \arcsin{\frac{b}{r}}$ , but got an undefined result from my TI-89 calculator. If someone can show me how to set up these integrals in polar coordinates, I think I could do the integration myself.  However, I would hope to have someone check my answers to the integrals so that I make sure to geth the mass and center of mass correct.",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
77,Double integral problem [closed],Double integral problem [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Evaluate the double integral $$ I = \iint_D x y \ dx dy $$ where $D$ is the closed triangular region with vertices $(0, 0)$, $(4, 0)$, $(0, 3)$. Appreciate any and all help!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Evaluate the double integral $$ I = \iint_D x y \ dx dy $$ where $D$ is the closed triangular region with vertices $(0, 0)$, $(4, 0)$, $(0, 3)$. Appreciate any and all help!",,['multivariable-calculus']
78,Does the limit exist if the values are different?,Does the limit exist if the values are different?,,"I got  $$ \lim_{(x,y)\to (1,0)} \ln\left(\frac{1+{y^2}}{{x^2} +xy}\right) $$ and I get $\lim =  0,$ when $y = 0,$ $\lim = 0,$ when $y = x,$ but when $y = -x$ I get undefined. So the limit doesn't exist?","I got  $$ \lim_{(x,y)\to (1,0)} \ln\left(\frac{1+{y^2}}{{x^2} +xy}\right) $$ and I get $\lim =  0,$ when $y = 0,$ $\lim = 0,$ when $y = x,$ but when $y = -x$ I get undefined. So the limit doesn't exist?",,"['calculus', 'limits', 'multivariable-calculus']"
79,Taking the exterior derivative of a 0-form,Taking the exterior derivative of a 0-form,,"I'm attempting to show that $dg(\vec{x})=\alpha$, where $\alpha=\Sigma^n_{i=1}f_idx_i$ and $g(\vec x)={1\over{p+1}}\Sigma_{i=1}^nx_if_i(\vec x)$...and $d$ is the exterior derivative.  The $f_i$ are all smooth and homogeneous of the same ($p\neq-1)$) degree, i.e. $f_i(t\vec x)=t^pf_i(\vec x)$. This latter fact gives us that $\large\Sigma_{j=1}^nx_j{\partial f_i\over \partial x_j}(\vec x)=pf_i(\vec x)$. I don't know whether it's all the summations involved or what, but I can't seem to get this to work out.  It so happens that I always have the j's and i's mixed in the wrong way...This might indicate that the question isn't true as stated...or more likely that I'm failing at some fairly basic bookkeeping. In the end I keep ending up with $\large dg={1\over{p+1}}\Sigma^n_{j=1}\Sigma^n_{i=1}x_i{\partial f_i\over \partial x_j}dx_j+f_i$, which won't really allow me to use any of the nice identities I've earned.","I'm attempting to show that $dg(\vec{x})=\alpha$, where $\alpha=\Sigma^n_{i=1}f_idx_i$ and $g(\vec x)={1\over{p+1}}\Sigma_{i=1}^nx_if_i(\vec x)$...and $d$ is the exterior derivative.  The $f_i$ are all smooth and homogeneous of the same ($p\neq-1)$) degree, i.e. $f_i(t\vec x)=t^pf_i(\vec x)$. This latter fact gives us that $\large\Sigma_{j=1}^nx_j{\partial f_i\over \partial x_j}(\vec x)=pf_i(\vec x)$. I don't know whether it's all the summations involved or what, but I can't seem to get this to work out.  It so happens that I always have the j's and i's mixed in the wrong way...This might indicate that the question isn't true as stated...or more likely that I'm failing at some fairly basic bookkeeping. In the end I keep ending up with $\large dg={1\over{p+1}}\Sigma^n_{j=1}\Sigma^n_{i=1}x_i{\partial f_i\over \partial x_j}dx_j+f_i$, which won't really allow me to use any of the nice identities I've earned.",,"['multivariable-calculus', 'differential-forms', 'exterior-algebra']"
80,"Let $f(x,y)=x+y^2$ and $P = (1,1)$. Find a unit vector $u$ such that the directional derivative $D_uf(x,y)$ is zero.",Let  and . Find a unit vector  such that the directional derivative  is zero.,"f(x,y)=x+y^2 P = (1,1) u D_uf(x,y)","Let $f(x,y)=x+y^2$ and $P = (1,1)$. Find a unit vector $u$ such that the directional derivative $D_uf(x,y)$ is zero. $$ \nabla{f(x,y)} = \left\langle1, 2y\right\rangle\\ \nabla{f(1,1)} = \left\langle11, 2\right\rangle\\ D_uf(1,1) = \left\langle11,2\right\rangle\cdot u\\ $$ $u$ must be a unit vector so $u = \sqrt{x^2+y^2} = 1$. So we must solve the system of equations: $$ \sqrt{x^2+y^2} = 1\\ x + 2y = 0 $$ Then we simply solve the system of equations. Is this the correct direction for this problem?","Let $f(x,y)=x+y^2$ and $P = (1,1)$. Find a unit vector $u$ such that the directional derivative $D_uf(x,y)$ is zero. $$ \nabla{f(x,y)} = \left\langle1, 2y\right\rangle\\ \nabla{f(1,1)} = \left\langle11, 2\right\rangle\\ D_uf(1,1) = \left\langle11,2\right\rangle\cdot u\\ $$ $u$ must be a unit vector so $u = \sqrt{x^2+y^2} = 1$. So we must solve the system of equations: $$ \sqrt{x^2+y^2} = 1\\ x + 2y = 0 $$ Then we simply solve the system of equations. Is this the correct direction for this problem?",,['multivariable-calculus']
81,convert a double integral to a triple integral,convert a double integral to a triple integral,,"Let $m,n,p\in (0,+\infty)$ be constants and let $D$ be the region in the plane enclosed by lines $x=0,y=0$ and $x+y=1$. Calculate the following integral: $$\iint\limits_{D}x^{m-1}y^{n-1}๏ผ1-x-y)^{p-1}\mathrm{d}x\mathrm{d}y$$ Note: I know how to calculate it by the routine procedure, but the answer computed it in another way: It turned the simplex $D$ into the region $\Omega$ in $\mathbb{R}^3$ which is enclosed by planes $x=0, y=0, z=0$ and $x+y+z=1$, it then claimed that the original integral is equal to $$(p-1)\iiint\limits_{\Omega}x^{m-1}y^{n-1}z^{p-2}\mathrm{d}x\mathrm{d}y\mathrm{d}z $$ But I could not get why the triple integral is equivalent to the original double integral?(I know how to compute the triple integral, but I just can't see how it's equivalent to the double integral.)","Let $m,n,p\in (0,+\infty)$ be constants and let $D$ be the region in the plane enclosed by lines $x=0,y=0$ and $x+y=1$. Calculate the following integral: $$\iint\limits_{D}x^{m-1}y^{n-1}๏ผ1-x-y)^{p-1}\mathrm{d}x\mathrm{d}y$$ Note: I know how to calculate it by the routine procedure, but the answer computed it in another way: It turned the simplex $D$ into the region $\Omega$ in $\mathbb{R}^3$ which is enclosed by planes $x=0, y=0, z=0$ and $x+y+z=1$, it then claimed that the original integral is equal to $$(p-1)\iiint\limits_{\Omega}x^{m-1}y^{n-1}z^{p-2}\mathrm{d}x\mathrm{d}y\mathrm{d}z $$ But I could not get why the triple integral is equivalent to the original double integral?(I know how to compute the triple integral, but I just can't see how it's equivalent to the double integral.)",,"['multivariable-calculus', 'definite-integrals']"
82,Where should I start to determine the limit of a multivariate function?,Where should I start to determine the limit of a multivariate function?,,"The textbook I'm using only contains examples for very simple functions, like $$\lim_{(x,y) \rightarrow (0, 1)}{x^2 + y^2 + 2}$$ In this case, I can just break up the function into its composite parts: $$\lim_{(x, y) \rightarrow (0, 1)}{x^2} + \lim_{(x, y) \rightarrow (0, 1)}{y^2} + \lim_{(x, y) \rightarrow (0, 1)}{2} = 0 + 1 + 2 = 3$$ For more complex functions, it has some examples for how to verify a given limit. But none for how to compute it in the first place. Suppose I am given something like one of the following problems: $$\lim_{(x,y) \rightarrow (0,0)} \frac{e^{xy} - 1}{y}$$ $$\lim_{(x,y) \rightarrow (0,0)} \frac{\cos {(xy)} - 1}{x^2y^2}$$ $$\lim_{(x,y) \rightarrow (0,0)} \frac{xy}{x^2 + y^2 + 2}$$ How do I begin if I want to compute the limit? What is the general approach?","The textbook I'm using only contains examples for very simple functions, like $$\lim_{(x,y) \rightarrow (0, 1)}{x^2 + y^2 + 2}$$ In this case, I can just break up the function into its composite parts: $$\lim_{(x, y) \rightarrow (0, 1)}{x^2} + \lim_{(x, y) \rightarrow (0, 1)}{y^2} + \lim_{(x, y) \rightarrow (0, 1)}{2} = 0 + 1 + 2 = 3$$ For more complex functions, it has some examples for how to verify a given limit. But none for how to compute it in the first place. Suppose I am given something like one of the following problems: $$\lim_{(x,y) \rightarrow (0,0)} \frac{e^{xy} - 1}{y}$$ $$\lim_{(x,y) \rightarrow (0,0)} \frac{\cos {(xy)} - 1}{x^2y^2}$$ $$\lim_{(x,y) \rightarrow (0,0)} \frac{xy}{x^2 + y^2 + 2}$$ How do I begin if I want to compute the limit? What is the general approach?",,"['limits', 'multivariable-calculus']"
83,Area of twisted torus,Area of twisted torus,,"I am trying to determine the surface area of a torus that has been twisted.  I know the non-twisted case can be solved by either using Pappus's Theorem or a method along the lines of this post . To be more precise, suppose we take a circle of radius $1$ sitting in the $xz$-plane with center at $(2,0,0)$ and revolve it about the $z$-axis so that the center of the revolving circle always rests in the $xy$-plane. Right before attaching this rotated object back up upon itself to form a torus, we rotate the circle by some amount $\theta$ and then attach the starting circle to the rotating circle. From the outside, this torus looks just like a normal torus, but it certainly is not because if we consider where the point $(3,0,0)$ goes in the process of the revolution, it is clear that some nonzero rotation will increase the length of the path sweeped out by the point. My question is, how (if?) does the area change?  Moreso, how do you calculate this new area?  (I think this may depend upon the ""fervor"" in which which one twists the torus. For example, if we hope to acheive a twist by $\pi/2$,  we can do it in the last $25\%$ of the revolution or the last $50\%$ of the revolution (actually, I have no idea if the area would change in this case!).","I am trying to determine the surface area of a torus that has been twisted.  I know the non-twisted case can be solved by either using Pappus's Theorem or a method along the lines of this post . To be more precise, suppose we take a circle of radius $1$ sitting in the $xz$-plane with center at $(2,0,0)$ and revolve it about the $z$-axis so that the center of the revolving circle always rests in the $xy$-plane. Right before attaching this rotated object back up upon itself to form a torus, we rotate the circle by some amount $\theta$ and then attach the starting circle to the rotating circle. From the outside, this torus looks just like a normal torus, but it certainly is not because if we consider where the point $(3,0,0)$ goes in the process of the revolution, it is clear that some nonzero rotation will increase the length of the path sweeped out by the point. My question is, how (if?) does the area change?  Moreso, how do you calculate this new area?  (I think this may depend upon the ""fervor"" in which which one twists the torus. For example, if we hope to acheive a twist by $\pi/2$,  we can do it in the last $25\%$ of the revolution or the last $50\%$ of the revolution (actually, I have no idea if the area would change in this case!).",,"['calculus', 'general-topology', 'multivariable-calculus']"
84,Change of Variable for Lebesgue Integral on $\mathbb{R}^n$,Change of Variable for Lebesgue Integral on,\mathbb{R}^n,"Let $A=(a_{ij})$ be a real symmetric, positive definite $n \times n$ matrix and set $$ F(x_1,x_2,\ldots, ,x_n)= \sum_{i,j}a_{ij}x_ix_j.$$ I am trying to show that for any non-negative measurable function $\alpha$ on the real line $$ \int_{\mathbb{R}^n} \alpha(F(x_1,x_2,\ldots, ,x_n))\,dm=  \frac{1}{\sqrt{\det A}}\int_{\mathbb{R}^n} \alpha(x_1^2+x_2^2+\ldots +x_n^2)\,dm,$$ where $dm$ is the Lebesgue measure on $\mathbb{R}^n$. I thought in the following sense: Somehow, change of variables theorem for $\mathbb{R}^n$ should be used and here we need the Jacobian matrix for the transformation $F$. Is $A$ the matrix representation of $F$? How to write Jacobian for $F$? Moreover, $A$ is diagonalizable and all of its eigenvalues, $\lambda_1\lambda_2\ldots\lambda_n$, are positive. Therefore, $D= P^{-1}A P$, where $D$ is the diagonal $n \times n$ matrix with eigenvalues on the diagonal and $P$ is the orthogonal $n \times n$ matrix consisting of eigenfunctions as column vectors. Then, it follows that  $$\det A=\det D=\lambda_1\lambda_2\ldots\lambda_n.$$ How can we combine all these data? Can you please help me? Thank you!","Let $A=(a_{ij})$ be a real symmetric, positive definite $n \times n$ matrix and set $$ F(x_1,x_2,\ldots, ,x_n)= \sum_{i,j}a_{ij}x_ix_j.$$ I am trying to show that for any non-negative measurable function $\alpha$ on the real line $$ \int_{\mathbb{R}^n} \alpha(F(x_1,x_2,\ldots, ,x_n))\,dm=  \frac{1}{\sqrt{\det A}}\int_{\mathbb{R}^n} \alpha(x_1^2+x_2^2+\ldots +x_n^2)\,dm,$$ where $dm$ is the Lebesgue measure on $\mathbb{R}^n$. I thought in the following sense: Somehow, change of variables theorem for $\mathbb{R}^n$ should be used and here we need the Jacobian matrix for the transformation $F$. Is $A$ the matrix representation of $F$? How to write Jacobian for $F$? Moreover, $A$ is diagonalizable and all of its eigenvalues, $\lambda_1\lambda_2\ldots\lambda_n$, are positive. Therefore, $D= P^{-1}A P$, where $D$ is the diagonal $n \times n$ matrix with eigenvalues on the diagonal and $P$ is the orthogonal $n \times n$ matrix consisting of eigenfunctions as column vectors. Then, it follows that  $$\det A=\det D=\lambda_1\lambda_2\ldots\lambda_n.$$ How can we combine all these data? Can you please help me? Thank you!",,"['multivariable-calculus', 'integration']"
85,When is the limit in $y$ of a Taylor expansion in $x$ a valid expansion?,When is the limit in  of a Taylor expansion in  a valid expansion?,y x,"I'd be interested to know when, if $$f(x,y)=g(x,y)+O(x^n)$$ we have that $$\lim_{y\rightarrow c}=\lim_{y\rightarrow c}g(x,y)+O(x^n).$$ Are there conditions of $f$ and/or $g$ that make sure that this is satisfied? In particular, assume that we have a function $f(x,y)$ and that we Taylor expand it as a function of $x$ around $a$: $$f(a+x,y)=f(a,y)+\frac{\partial f}{\partial x}(a,y)\cdot x+\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$ I wish to study the behaviour of $f(x,y)$ as $y$ tends to a value on the boundary of the domain of $f(x,y)$, say $y\rightarrow c$. I therefore wonder under what conditions on $f$ $$\lim_{y\rightarrow c}f(a+x,y)=\lim_{y\rightarrow c}f(a,y)+\lim_{y\rightarrow c}\frac{\partial f}{\partial x}(a,y)\cdot x+\lim_{y\rightarrow c}\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$ In other words, when is the $O(x^n)$ term still $O(x^n)$ as $y\rightarrow c$? I've been hoping to find some smoothness constraints on $f$ that would guarantee that the expansion still is $O(x^n)$, but so far I haven't been able to find any.","I'd be interested to know when, if $$f(x,y)=g(x,y)+O(x^n)$$ we have that $$\lim_{y\rightarrow c}=\lim_{y\rightarrow c}g(x,y)+O(x^n).$$ Are there conditions of $f$ and/or $g$ that make sure that this is satisfied? In particular, assume that we have a function $f(x,y)$ and that we Taylor expand it as a function of $x$ around $a$: $$f(a+x,y)=f(a,y)+\frac{\partial f}{\partial x}(a,y)\cdot x+\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$ I wish to study the behaviour of $f(x,y)$ as $y$ tends to a value on the boundary of the domain of $f(x,y)$, say $y\rightarrow c$. I therefore wonder under what conditions on $f$ $$\lim_{y\rightarrow c}f(a+x,y)=\lim_{y\rightarrow c}f(a,y)+\lim_{y\rightarrow c}\frac{\partial f}{\partial x}(a,y)\cdot x+\lim_{y\rightarrow c}\frac{\partial^2 f}{\partial x^2}(a,y)\cdot \frac{x^2}{2}+\cdots+O(x^n).$$ In other words, when is the $O(x^n)$ term still $O(x^n)$ as $y\rightarrow c$? I've been hoping to find some smoothness constraints on $f$ that would guarantee that the expansion still is $O(x^n)$, but so far I haven't been able to find any.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'taylor-expansion']"
86,Significance of Hessian Matrix [duplicate],Significance of Hessian Matrix [duplicate],,This question already has answers here : How to interpret Hessian of a function (3 answers) Closed 10 years ago . I want to know the geometrical significance of Hessian Matrix. Please could anyone have any idea about it?,This question already has answers here : How to interpret Hessian of a function (3 answers) Closed 10 years ago . I want to know the geometrical significance of Hessian Matrix. Please could anyone have any idea about it?,,"['multivariable-calculus', 'derivatives']"
87,"Is it true that $\max\limits_D |f(x)|=\max\limits\{|\max\limits_D f(x)|, |\min\limits_D f(x)|\}$?",Is it true that ?,"\max\limits_D |f(x)|=\max\limits\{|\max\limits_D f(x)|, |\min\limits_D f(x)|\}","I came across an equality, which states that If $D\subset\mathbb{R}^n, n\geq 2$ is compact, for each $ f\in C(D)$, we have the following equality   $$\max\limits_D |f(x)|=\max\limits\{|\max\limits_D f(x)|, |\min\limits_D f(x)|\}.$$ Actually I can not judge if it is right. Can anyone tell me if it is right, and if so, how to prove it? Thanks a lot.","I came across an equality, which states that If $D\subset\mathbb{R}^n, n\geq 2$ is compact, for each $ f\in C(D)$, we have the following equality   $$\max\limits_D |f(x)|=\max\limits\{|\max\limits_D f(x)|, |\min\limits_D f(x)|\}.$$ Actually I can not judge if it is right. Can anyone tell me if it is right, and if so, how to prove it? Thanks a lot.",,['multivariable-calculus']
88,Derivative of matrix exponential away from the identity,Derivative of matrix exponential away from the identity,,"Suppose $X$ and $Y$ are $n\times n$ matrices (assume they are symmetric if you want), is there a reasonable formula for $$ Z=\frac{d}{dt}|_{t=0} e^{X+tY}? $$ Obviously if we assume that $[X,Y]=0$ then there is a simple answer. I assume if there is an reasonable closed form then it should follow from the Baker-Campbell-Hausdorff identity but I am having a bit of a hard time at it.","Suppose $X$ and $Y$ are $n\times n$ matrices (assume they are symmetric if you want), is there a reasonable formula for $$ Z=\frac{d}{dt}|_{t=0} e^{X+tY}? $$ Obviously if we assume that $[X,Y]=0$ then there is a simple answer. I assume if there is an reasonable closed form then it should follow from the Baker-Campbell-Hausdorff identity but I am having a bit of a hard time at it.",,"['multivariable-calculus', 'lie-groups']"
89,Describing multivariable functions,Describing multivariable functions,,"So I am presented with the following question: Describe and sketch the largest region in the $xy$-plane that corresponds to the domain of the function: $$g(x,y) = \sqrt{4 - x^2 - y^2} \ln(x-y).$$ Now to be I can find different restrictions like $4 - x^2 - y^2 \geq 0$... but I'm honestly not even sure where to begin this question! Any help?","So I am presented with the following question: Describe and sketch the largest region in the $xy$-plane that corresponds to the domain of the function: $$g(x,y) = \sqrt{4 - x^2 - y^2} \ln(x-y).$$ Now to be I can find different restrictions like $4 - x^2 - y^2 \geq 0$... but I'm honestly not even sure where to begin this question! Any help?",,"['multivariable-calculus', 'plane-curves']"
90,Find region of integration where triple integral has maximum value,Find region of integration where triple integral has maximum value,,"I have to find the region $E$ where $$ \iiint_E (1-x^2 - 2y^2 - 3z^2) dV $$ has maximum value, but I'm not sure how to start. I was thinking of getting the derivative of the integral and then finding the extrema with the usual $f_x = 0, f_y = 0$ equation system, but even if I get the second derivative of it, I'm stuck with a single integral and I'm not sure if that's right.","I have to find the region $E$ where $$ \iiint_E (1-x^2 - 2y^2 - 3z^2) dV $$ has maximum value, but I'm not sure how to start. I was thinking of getting the derivative of the integral and then finding the extrema with the usual $f_x = 0, f_y = 0$ equation system, but even if I get the second derivative of it, I'm stuck with a single integral and I'm not sure if that's right.",,"['multivariable-calculus', 'optimization', 'integration']"
91,About $\nabla$'s Property.,About 's Property.,\nabla,"For a scalar function $g$, and a vector function $f$, $$ | \nabla ( (\nabla g) \cdot f ) | \leqslant |f| \cdot \text{Something} $$ Is this inequality possible? If possible, what would ""$\text{Something}$"" be?","For a scalar function $g$, and a vector function $f$, $$ | \nabla ( (\nabla g) \cdot f ) | \leqslant |f| \cdot \text{Something} $$ Is this inequality possible? If possible, what would ""$\text{Something}$"" be?",,['multivariable-calculus']
92,How to draw a family of curves and its envelope?,How to draw a family of curves and its envelope?,,"Given a family of curves $$F=(x-t)^2+y^2-\frac{1}{2}t^2,$$ I am trying to compute the envelope of this family. The envelope is described by the equations $$F=0, \\ \dfrac{\partial F}{\partial t} = -2(x-t)-t = 0.$$ To eliminate $t$, I computed a Grรถbner basis for $I=\left\langle F,\dfrac{\partial F}{\partial t}\right\rangle.$ Namely, $$ \{ g_1=x^2-y^2,g_2=t-2x \}.$$ So $I\cap \mathbb R[x,y]= \langle g_1 \rangle$, and the envelope lies on the curve $x^2-y^2=0.$ Now, I want to draw a picture to illustrate my answer by Mathematica , (I also cannot imagine what the picture of that family looks like), but I don't know how to write the code. Could you give me a hand?","Given a family of curves $$F=(x-t)^2+y^2-\frac{1}{2}t^2,$$ I am trying to compute the envelope of this family. The envelope is described by the equations $$F=0, \\ \dfrac{\partial F}{\partial t} = -2(x-t)-t = 0.$$ To eliminate $t$, I computed a Grรถbner basis for $I=\left\langle F,\dfrac{\partial F}{\partial t}\right\rangle.$ Namely, $$ \{ g_1=x^2-y^2,g_2=t-2x \}.$$ So $I\cap \mathbb R[x,y]= \langle g_1 \rangle$, and the envelope lies on the curve $x^2-y^2=0.$ Now, I want to draw a picture to illustrate my answer by Mathematica , (I also cannot imagine what the picture of that family looks like), but I don't know how to write the code. Could you give me a hand?",,"['geometry', 'multivariable-calculus', 'plane-curves', 'mathematica']"
93,"$f:(x,y)\mapsto \frac{x\sin(y)-y\sin(x)}{x^2+y^2}$ is a $C^1$-function",is a -function,"f:(x,y)\mapsto \frac{x\sin(y)-y\sin(x)}{x^2+y^2} C^1","I would like to show that the function: $$f:(x,y)\mapsto \frac{x\sin(y)-y\sin(x)}{x^2+y^2}$$ is a $C^1$-function. $$ \frac{\partial f}{\partial x}(x,y)=\frac{\sin(y)-y\cos(x)}{x^2+y^2}+\frac{2x(y\sin(x)-x\sin(y))}{(x^2+y^2)^2}$$ $$ \frac{\partial f}{\partial y}(x,y)=-\frac{\partial f}{\partial y}(y,x)=... $$ So I just have to show that: $$ \frac{\partial f}{\partial x}(x,y)\rightarrow_{(0,0)}0$$ When $y\geq0$ : $$ -\frac{y^3}{6(x^2+y^2)}+\frac{x^2y}{x^2+y^2}-\frac{x^4y}{4!(x^2+y^2)} \leq \frac{\sin(y)-y\cos(x)}{x^2+y^2} \leq \frac{yx^2}{2(x^2+y^2)}$$ When $y<0$ : $$ -\frac{y^3}{6(x^2+y^2)}+\frac{y^5}{5!(x^2+y^2)}+\frac{x^2y}{2(x^2+y^2)} \leq \frac{\sin(y)-y\cos(x)}{x^2+y^2} \leq \frac{yx^2}{2(x^2+y^2)}-\frac{yx^4}{4!(x^2+y^2)}$$ So $$ \frac{\sin(y)-y\cos(x)}{x^2+y^2}\rightarrow_{(0,0)}0 $$ How can I directly find an upper bound of $$ \left| \frac{2x(y\sin(x)-x\sin(y))}{(x^2+y^2)^2} \right|$$ that tends to 0 ?","I would like to show that the function: $$f:(x,y)\mapsto \frac{x\sin(y)-y\sin(x)}{x^2+y^2}$$ is a $C^1$-function. $$ \frac{\partial f}{\partial x}(x,y)=\frac{\sin(y)-y\cos(x)}{x^2+y^2}+\frac{2x(y\sin(x)-x\sin(y))}{(x^2+y^2)^2}$$ $$ \frac{\partial f}{\partial y}(x,y)=-\frac{\partial f}{\partial y}(y,x)=... $$ So I just have to show that: $$ \frac{\partial f}{\partial x}(x,y)\rightarrow_{(0,0)}0$$ When $y\geq0$ : $$ -\frac{y^3}{6(x^2+y^2)}+\frac{x^2y}{x^2+y^2}-\frac{x^4y}{4!(x^2+y^2)} \leq \frac{\sin(y)-y\cos(x)}{x^2+y^2} \leq \frac{yx^2}{2(x^2+y^2)}$$ When $y<0$ : $$ -\frac{y^3}{6(x^2+y^2)}+\frac{y^5}{5!(x^2+y^2)}+\frac{x^2y}{2(x^2+y^2)} \leq \frac{\sin(y)-y\cos(x)}{x^2+y^2} \leq \frac{yx^2}{2(x^2+y^2)}-\frac{yx^4}{4!(x^2+y^2)}$$ So $$ \frac{\sin(y)-y\cos(x)}{x^2+y^2}\rightarrow_{(0,0)}0 $$ How can I directly find an upper bound of $$ \left| \frac{2x(y\sin(x)-x\sin(y))}{(x^2+y^2)^2} \right|$$ that tends to 0 ?",,"['real-analysis', 'multivariable-calculus']"
94,How to find the directional derivative of the following function?,How to find the directional derivative of the following function?,,"I would like to find all directional derivatives of the function $$f(x,y) = (3x^4 + y^4)^{1/4} , $$ (where $ (x,y) \in \mathbb{R}^2 $), in the point $(0,0)$. I tried to do this by calculating $$\nabla f(x,y) = f_1 (x,y) e_1 + f_2 (x,y) e_2 $$, where $e_n$ is the $n$'th unit vector and $f_n$ is the partial derivative with respect to the $n$'th variable. I found that $f_1 (x,y) = (1/4)\cdot 12x^3 (3x^4 + y^4)^{-3/4} = \frac{3x^3}{(3x^4+y^4)^{3/4}} $, and that $f_2 (x,y) = \frac{y^3}{(3x^4 + y^4)^{3/4}} $. When filling in $(0,0)$ to find $\nabla f(0,0)$, however, I get two expressions involving a $\frac{0}{0}$ - fraction. I do think I need to do this though, since $$D_{v/|v|} f(a,b) = \langle (\frac{v}{|v|}, \nabla f(a,b)  \rangle . $$  Can you please help me with this?","I would like to find all directional derivatives of the function $$f(x,y) = (3x^4 + y^4)^{1/4} , $$ (where $ (x,y) \in \mathbb{R}^2 $), in the point $(0,0)$. I tried to do this by calculating $$\nabla f(x,y) = f_1 (x,y) e_1 + f_2 (x,y) e_2 $$, where $e_n$ is the $n$'th unit vector and $f_n$ is the partial derivative with respect to the $n$'th variable. I found that $f_1 (x,y) = (1/4)\cdot 12x^3 (3x^4 + y^4)^{-3/4} = \frac{3x^3}{(3x^4+y^4)^{3/4}} $, and that $f_2 (x,y) = \frac{y^3}{(3x^4 + y^4)^{3/4}} $. When filling in $(0,0)$ to find $\nabla f(0,0)$, however, I get two expressions involving a $\frac{0}{0}$ - fraction. I do think I need to do this though, since $$D_{v/|v|} f(a,b) = \langle (\frac{v}{|v|}, \nabla f(a,b)  \rangle . $$  Can you please help me with this?",,['multivariable-calculus']
95,Is the Hessian of the 'generalized least square function' positive semi-definite?,Is the Hessian of the 'generalized least square function' positive semi-definite?,,"Let  $F:\mathbb{R}^n\rightarrow \mathbb{R}$ a scalar field which has a quadratic from, $$F(\mathbf{x}) = \frac{1}{2}\mathbf{d}(\mathbf{x})^\top\Lambda\mathbf{d}(\mathbf{x})$$  with $\mathbf{d}:\mathbb{R}^n\rightarrow\mathbb{R}^m$ being a twice differentiable vector field, and $\Lambda\in \mathbb{R}^{m\times m}$ being a symmetric, positive semi-definite matrix. Thus, $F$ is the objective function in generalized least square problems such as Gauss-Newton. Let $\mathtt{J}_\mathbf{d}$ be the Jacobian of $\mathbf{d}$ and $\mathtt{H}_\mathbf{d}$  its second derivative, the $n\times m \times m$ Hessian tensor. Due to the product rule, the  first derivative of $F$ becomes: \begin{equation} \nabla F = \frac{1}{2}( \mathbf{d}(\mathbf{x})^{\top}  \Lambda \mathtt{J}_ \mathbf{d}(\mathbf{x})  )^\top + \frac{1}{2}( \mathtt{J}_ \mathbf{d} (\mathbf{x})^{\top} \Lambda  \mathbf{d}(\mathbf{x}) = \mathtt{J}_\mathbf{d}(\mathbf{x})^{\top} \Lambda  \mathbf{d}(\mathbf{x}), \end{equation} using the fact that $\Lambda$ is symmetric. Again by means of the product rule, the second derivation or Hessian of $F$ is  \begin{equation} \mathtt{H}_F (\mathbf{x})  = \mathtt{J} _\mathbf{d}(\mathbf{x})^\top \Lambda \mathtt{J} _  \mathbf{b}(\mathbf{x})  + \mathtt{H}_\mathbf{d}(\mathbf{x})  \Lambda \mathbf{d}(\mathbf{x}), \end{equation} right? Question: Is $\mathtt{H}_F$ is positive semi-definite?","Let  $F:\mathbb{R}^n\rightarrow \mathbb{R}$ a scalar field which has a quadratic from, $$F(\mathbf{x}) = \frac{1}{2}\mathbf{d}(\mathbf{x})^\top\Lambda\mathbf{d}(\mathbf{x})$$  with $\mathbf{d}:\mathbb{R}^n\rightarrow\mathbb{R}^m$ being a twice differentiable vector field, and $\Lambda\in \mathbb{R}^{m\times m}$ being a symmetric, positive semi-definite matrix. Thus, $F$ is the objective function in generalized least square problems such as Gauss-Newton. Let $\mathtt{J}_\mathbf{d}$ be the Jacobian of $\mathbf{d}$ and $\mathtt{H}_\mathbf{d}$  its second derivative, the $n\times m \times m$ Hessian tensor. Due to the product rule, the  first derivative of $F$ becomes: \begin{equation} \nabla F = \frac{1}{2}( \mathbf{d}(\mathbf{x})^{\top}  \Lambda \mathtt{J}_ \mathbf{d}(\mathbf{x})  )^\top + \frac{1}{2}( \mathtt{J}_ \mathbf{d} (\mathbf{x})^{\top} \Lambda  \mathbf{d}(\mathbf{x}) = \mathtt{J}_\mathbf{d}(\mathbf{x})^{\top} \Lambda  \mathbf{d}(\mathbf{x}), \end{equation} using the fact that $\Lambda$ is symmetric. Again by means of the product rule, the second derivation or Hessian of $F$ is  \begin{equation} \mathtt{H}_F (\mathbf{x})  = \mathtt{J} _\mathbf{d}(\mathbf{x})^\top \Lambda \mathtt{J} _  \mathbf{b}(\mathbf{x})  + \mathtt{H}_\mathbf{d}(\mathbf{x})  \Lambda \mathbf{d}(\mathbf{x}), \end{equation} right? Question: Is $\mathtt{H}_F$ is positive semi-definite?",,"['linear-algebra', 'multivariable-calculus', 'numerical-methods']"
96,How do I find the volume of this sliced cylinder?,How do I find the volume of this sliced cylinder?,,"This is hard to explain but I'll do my best.  I hope I'm clear. Imagine you have a donut.  You want to find the volume of it and the method you want to use is to imagine slicing one side of that donut and opening it out into a cylinder.  Only, it's not exactly a cylinder as it has two pointed ends: one side of the cylinder has the length of the inner circumference of the donut, the other side has the length of the outer circumference. The 'middle part' is a simple cylinder. I want to find the volume of each end part. There's an easy trick to it, but that's not the solution I'm looking for; the easy trick being putting the two end parts together to make a smaller cylinder. But, how I want to do this is to find the volume of one of these pointy cylindrical endparts with an integral.  However, I can't seem to hit the right answer. Let's say my stretched out donut has a left side of length $2\pi(R-r)$ and a right side of length $2\pi(R+r)$ and a radius of $r$.  Let's say I slice the top and bottom pointed end parts off.  I now have the middle part, a cylinder with a radius of $r$ and a height of $2\pi(R-r)$ and two pointed endparts.  Each endpart has a radius of $r$, and a height of $2\pi r$. If I cut an endpart into triangle wedge-shaped cross sections, each wedge will have a length of $2\sqrt{r^2-y^2}$, a height of $2\pi\sqrt{r^2-y^2}$ and a depth of $dy$.  So the volume of each wedge is $2\pi(r^2-y^2)dy$ If I integrate this with limits $-r$ and $r$ I get $2\pi\int_{-r}^r (r^2-y^2)dy = 2\pi (4r^3/3)$ Assuming that's right, I can double that and add it to the volume of my cylinder to get the donut volume.  So: $4\pi (\frac{4r^3}{3}) + 2\pi^2r^2(R-r) = \frac{16\pi r^3}{3} + 2\pi^2 r^2(R-r)$ The volume of the donut is actually $2\pi^2 r^2R$ (using the solid of revolution approach) so looks like I picked up some extra dough somewhere... $16\pi \frac{r^3}{3} - 2\pi^2 r^3$ cubic units of extra dough to be precise. Is my arithmetic wrong?  Is my method wrong?  Can you spot what I messed up? Sketch:","This is hard to explain but I'll do my best.  I hope I'm clear. Imagine you have a donut.  You want to find the volume of it and the method you want to use is to imagine slicing one side of that donut and opening it out into a cylinder.  Only, it's not exactly a cylinder as it has two pointed ends: one side of the cylinder has the length of the inner circumference of the donut, the other side has the length of the outer circumference. The 'middle part' is a simple cylinder. I want to find the volume of each end part. There's an easy trick to it, but that's not the solution I'm looking for; the easy trick being putting the two end parts together to make a smaller cylinder. But, how I want to do this is to find the volume of one of these pointy cylindrical endparts with an integral.  However, I can't seem to hit the right answer. Let's say my stretched out donut has a left side of length $2\pi(R-r)$ and a right side of length $2\pi(R+r)$ and a radius of $r$.  Let's say I slice the top and bottom pointed end parts off.  I now have the middle part, a cylinder with a radius of $r$ and a height of $2\pi(R-r)$ and two pointed endparts.  Each endpart has a radius of $r$, and a height of $2\pi r$. If I cut an endpart into triangle wedge-shaped cross sections, each wedge will have a length of $2\sqrt{r^2-y^2}$, a height of $2\pi\sqrt{r^2-y^2}$ and a depth of $dy$.  So the volume of each wedge is $2\pi(r^2-y^2)dy$ If I integrate this with limits $-r$ and $r$ I get $2\pi\int_{-r}^r (r^2-y^2)dy = 2\pi (4r^3/3)$ Assuming that's right, I can double that and add it to the volume of my cylinder to get the donut volume.  So: $4\pi (\frac{4r^3}{3}) + 2\pi^2r^2(R-r) = \frac{16\pi r^3}{3} + 2\pi^2 r^2(R-r)$ The volume of the donut is actually $2\pi^2 r^2R$ (using the solid of revolution approach) so looks like I picked up some extra dough somewhere... $16\pi \frac{r^3}{3} - 2\pi^2 r^3$ cubic units of extra dough to be precise. Is my arithmetic wrong?  Is my method wrong?  Can you spot what I messed up? Sketch:",,"['calculus', 'integration', 'multivariable-calculus']"
97,Directional derivatives,Directional derivatives,,"For a function of one variables $f(x)$, $f'(x)>1$ implies that the rate of increase of $f(x) $ at $x$ is more than 1. How can we define the analogous notion for a function of two variables $f(x,y)$, i.e. whenever $f(x,y)$ increases, the rate of increase is more than 1?","For a function of one variables $f(x)$, $f'(x)>1$ implies that the rate of increase of $f(x) $ at $x$ is more than 1. How can we define the analogous notion for a function of two variables $f(x,y)$, i.e. whenever $f(x,y)$ increases, the rate of increase is more than 1?",,['multivariable-calculus']
98,Derivative under a double integral,Derivative under a double integral,,"How does one find ${\partial y\over \partial t}$ and ${\partial^2 y\over \partial t^2}$ of a double integral $$y(x,t)=\int\limits_0^t \int\limits_{x-t+\xi}^{x+t-\xi} F(\eta)\,\,\,d\eta \,\,\,d\xi$$?","How does one find ${\partial y\over \partial t}$ and ${\partial^2 y\over \partial t^2}$ of a double integral $$y(x,t)=\int\limits_0^t \int\limits_{x-t+\xi}^{x+t-\xi} F(\eta)\,\,\,d\eta \,\,\,d\xi$$?",,"['integration', 'multivariable-calculus', 'derivatives']"
99,Multivariate integrals involving Dirac delta functions,Multivariate integrals involving Dirac delta functions,,"I'm interested in the behavior of Dirac deltafunctions within multivariate integrals. Here is a simple example to which I do not know the answer: $$\iint\limits_{[0,1]\times [0,1]} \delta\left(x - y\right)\, dA$$ This integrates the constant function $1$ over the line $y=x$. Is there a prefactor involed though due to the delta? I can imagine this evaluating either to $1$ or to $\sqrt2$. The information from the relevant Wikipedia page suggests the value $\sqrt2$ while WolframAlpha suggests the value $1$. How is the behavior of a Dirac deltafunction within a multivariate integral defined? My goal is to create an algorithm to solve a broad class of these problems in general.","I'm interested in the behavior of Dirac deltafunctions within multivariate integrals. Here is a simple example to which I do not know the answer: $$\iint\limits_{[0,1]\times [0,1]} \delta\left(x - y\right)\, dA$$ This integrates the constant function $1$ over the line $y=x$. Is there a prefactor involed though due to the delta? I can imagine this evaluating either to $1$ or to $\sqrt2$. The information from the relevant Wikipedia page suggests the value $\sqrt2$ while WolframAlpha suggests the value $1$. How is the behavior of a Dirac deltafunction within a multivariate integral defined? My goal is to create an algorithm to solve a broad class of these problems in general.",,"['multivariable-calculus', 'integration', 'special-functions']"
