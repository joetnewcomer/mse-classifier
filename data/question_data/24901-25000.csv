,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Finding the number of symmetric, positive definite $10 \times 10$ matrices having...","Finding the number of symmetric, positive definite  matrices having...",10 \times 10,"I was looking at old exam papers and I was stuck with the following problem: What is the number of symmetric, positive definite $10 \times 10$ matrices having trace equal to $10$ and determinant equal to $1$ ? The options are: $0$ $1$ greater than $1$ but finite Infinite. After getting positive feedback,I am posting my attempt as a separate answer.","I was looking at old exam papers and I was stuck with the following problem: What is the number of symmetric, positive definite matrices having trace equal to and determinant equal to ? The options are: greater than but finite Infinite. After getting positive feedback,I am posting my attempt as a separate answer.",10 \times 10 10 1 0 1 1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
1,"How to prove that sequence spaces $l^{p}, l^{\infty}$ and function space $C[a. b]$ are of infinite dimension",How to prove that sequence spaces  and function space  are of infinite dimension,"l^{p}, l^{\infty} C[a. b]","I am studying about the sequence space $l^{p}, l^{\infty}$ and function space $C[a. b]$. It is mentioned in the book that all of these spaces are of infinite dimension. I want to prove that these spaces are of infinite dimension. I am not finding a way to proceed. Any help and suggestions would be helpful to me. Thanks","I am studying about the sequence space $l^{p}, l^{\infty}$ and function space $C[a. b]$. It is mentioned in the book that all of these spaces are of infinite dimension. I want to prove that these spaces are of infinite dimension. I am not finding a way to proceed. Any help and suggestions would be helpful to me. Thanks",,"['linear-algebra', 'analysis', 'vector-spaces', 'banach-spaces']"
2,square root of a real matrix,square root of a real matrix,,"I want to compute the square root of a real symmetric positive definite matrix $S\in \mathcal{M}_{m,m}$ such that $S^{1/2}S^{1/2}=S$ and it's well known that this decomposition is unique. My question is if I have $S$ a real matrix will its square root be real too or it could be a complex matrix. In case that it could be complex then $S$ could have infinitely many square roots. Any comments?","I want to compute the square root of a real symmetric positive definite matrix $S\in \mathcal{M}_{m,m}$ such that $S^{1/2}S^{1/2}=S$ and it's well known that this decomposition is unique. My question is if I have $S$ a real matrix will its square root be real too or it could be a complex matrix. In case that it could be complex then $S$ could have infinitely many square roots. Any comments?",,"['linear-algebra', 'matrices']"
3,Describe all solutions of $Ax=0$.,Describe all solutions of .,Ax=0,"Describe all solutions of $Ax=0$ in parametric vector form, where $A$ is row equivalent to the given matrix. $$  \begin{bmatrix}     1 & -2 & 3 & -6 & 5 & 0\\     0 & 0 & 0 & 1 & 4&-6\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ I know that I should get this into row reduced echelon form, but I'm having trouble doing so.  I attempted it below. $$  \begin{bmatrix}     1 & -2 & 3 & -6 & 5 & 0\\     0 & 0 & 0 & 1 & 4&0\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ $$  \begin{bmatrix}     1 & -2 & 3 & 0 & 29 & 0\\     0 & 0 & 0 & 1 & 4&0\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ I'm not quite sure where to go from here, also I don't know how I would describe all solutions of $Ax=0$.","Describe all solutions of $Ax=0$ in parametric vector form, where $A$ is row equivalent to the given matrix. $$  \begin{bmatrix}     1 & -2 & 3 & -6 & 5 & 0\\     0 & 0 & 0 & 1 & 4&-6\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ I know that I should get this into row reduced echelon form, but I'm having trouble doing so.  I attempted it below. $$  \begin{bmatrix}     1 & -2 & 3 & -6 & 5 & 0\\     0 & 0 & 0 & 1 & 4&0\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ $$  \begin{bmatrix}     1 & -2 & 3 & 0 & 29 & 0\\     0 & 0 & 0 & 1 & 4&0\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0   \end{bmatrix}$$ I'm not quite sure where to go from here, also I don't know how I would describe all solutions of $Ax=0$.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
4,How to show that an $n$-dimensional vector space over field $\mathbb{F}_p$ has $p^n$ elements.,How to show that an -dimensional vector space over field  has  elements.,n \mathbb{F}_p p^n,"Suppose $V$ is an $n$-dimensional vector space over the finite field $\mathbb{F}_p$ for some prime $p$. How do I show that $V$ has $p^n$ elements? I was thinking that considering $n$ basis elements one can show that each element of the basis spans its own subspace which doesn't have any other basis elements, but how do I show that there are exactly $p^n$ elements overall?","Suppose $V$ is an $n$-dimensional vector space over the finite field $\mathbb{F}_p$ for some prime $p$. How do I show that $V$ has $p^n$ elements? I was thinking that considering $n$ basis elements one can show that each element of the basis spans its own subspace which doesn't have any other basis elements, but how do I show that there are exactly $p^n$ elements overall?",,['linear-algebra']
5,"Find $C$, if $A=CBC$, where $A$,$B$,$C$ are symmetric matrices.","Find , if , where ,, are symmetric matrices.",C A=CBC A B C,"If $A=CBC$, where $A$,$B$,$C$ are symmetric matrices and $A$,$B$ are given find $C$. $A$,$B$,$C$ are assumed to be real valued and $B$ is positive definite matrix. Does the unique solution always exist ? What additional assumptions are needed to obtain unique solution.","If $A=CBC$, where $A$,$B$,$C$ are symmetric matrices and $A$,$B$ are given find $C$. $A$,$B$,$C$ are assumed to be real valued and $B$ is positive definite matrix. Does the unique solution always exist ? What additional assumptions are needed to obtain unique solution.",,"['linear-algebra', 'matrices', 'quadratics', 'matrix-equations']"
6,prove change of basis matrix is unitary,prove change of basis matrix is unitary,,"As the title, let $(V,\langle,\rangle)$ be a complex inner product space and assume $S_1=(u_1,\ldots,u_n)$, $S_2=(v_1,\ldots,v_n)$ are orthonormal bases of $V$. Prove that the change of basis matrix $M_ IV(S_2,S_1)$ is a unitary matrix. (There is a hint that let $S$ be the operator s.t. $S(u_i)=v_i$ and prove this is a unitary operator.)","As the title, let $(V,\langle,\rangle)$ be a complex inner product space and assume $S_1=(u_1,\ldots,u_n)$, $S_2=(v_1,\ldots,v_n)$ are orthonormal bases of $V$. Prove that the change of basis matrix $M_ IV(S_2,S_1)$ is a unitary matrix. (There is a hint that let $S$ be the operator s.t. $S(u_i)=v_i$ and prove this is a unitary operator.)",,['linear-algebra']
7,How to project a n-dimensional point onto a 2-D subspace?,How to project a n-dimensional point onto a 2-D subspace?,,"Here is a n-dimensional space: There's a point $P (p_1,p_2,\dots,p_n)$ And two orthogonal vectors that determines a 2-D plane/subspace D $v (v_1,v_2, \dots ,v_n)$ $w (w_1,w_2, \dots ,w_n)$ How do I project the point $P$ onto the 2-D subspace D that is determined by vector $v$ and $w$?","Here is a n-dimensional space: There's a point $P (p_1,p_2,\dots,p_n)$ And two orthogonal vectors that determines a 2-D plane/subspace D $v (v_1,v_2, \dots ,v_n)$ $w (w_1,w_2, \dots ,w_n)$ How do I project the point $P$ onto the 2-D subspace D that is determined by vector $v$ and $w$?",,"['linear-algebra', 'geometry']"
8,How to show unitary decomposition is continuous,How to show unitary decomposition is continuous,,"It is a well-known fact that, for $A \in GL(n,\mathbb{C})$ with polar decomposition $A=U_AP_A$ for $U_A$ unitary and $P_A$ positive definite and Hermitian, the map $GL(n,\mathbb{C}) \rightarrow U(n)$ by $A \mapsto U_A$ is a homotopy equivalence. This is a fact I have seen many times. However, I have recently realized that I have no idea how to prove this map is continuous. It's pretty clear that since $U_A=A P_A^{-1}=A (\sqrt{A^*A})^{-1}$, where * denotes the conjugate transpose, all one needs to prove is continuity for the matrix square root map which sends a positive-definite matrix to its principal square root since continuity for multiplication and inverses are well-known and continuity for * is obvious. Can anyone provide a good proof for continuity of the matrix square-root map on the space of positive definite matrices? Of course if you can prove $A \mapsto U_A$ is continuous without that fact, that would also be a fine answer.","It is a well-known fact that, for $A \in GL(n,\mathbb{C})$ with polar decomposition $A=U_AP_A$ for $U_A$ unitary and $P_A$ positive definite and Hermitian, the map $GL(n,\mathbb{C}) \rightarrow U(n)$ by $A \mapsto U_A$ is a homotopy equivalence. This is a fact I have seen many times. However, I have recently realized that I have no idea how to prove this map is continuous. It's pretty clear that since $U_A=A P_A^{-1}=A (\sqrt{A^*A})^{-1}$, where * denotes the conjugate transpose, all one needs to prove is continuity for the matrix square root map which sends a positive-definite matrix to its principal square root since continuity for multiplication and inverses are well-known and continuity for * is obvious. Can anyone provide a good proof for continuity of the matrix square-root map on the space of positive definite matrices? Of course if you can prove $A \mapsto U_A$ is continuous without that fact, that would also be a fine answer.",,"['linear-algebra', 'continuity']"
9,Commutativity between diagonal and unitary matrices?,Commutativity between diagonal and unitary matrices?,,"Quick questions: if you have a diagonal matrix $A$ and a unitary matrix $B$. Do $A$ and $B$ commute? if $A$ and $B$ are positive definite matrices. if $a$ is an eigenvalue of $A$ and $b$ is an eigenvalue of $B$, does it follow that $a+b$ is an eigenvalue of $A+B$?","Quick questions: if you have a diagonal matrix $A$ and a unitary matrix $B$. Do $A$ and $B$ commute? if $A$ and $B$ are positive definite matrices. if $a$ is an eigenvalue of $A$ and $b$ is an eigenvalue of $B$, does it follow that $a+b$ is an eigenvalue of $A+B$?",,"['linear-algebra', 'matrices']"
10,"If $T\alpha=c\alpha$, then there is a non-zero linear functional $f$ on $V$ such that $T^{t}f=cf$","If , then there is a non-zero linear functional  on  such that",T\alpha=c\alpha f V T^{t}f=cf,"I am self-studying Hoffman and Kunze's book Linear Algebra . This is exercise $4$ from page $115$. It is in the section of The transpose of a Linear Transformation . Let $V$ be a finite-dimensional vector space over the field   $\mathbb{F}$ and let $T$ be a linear operator on $V$. Let $c$  be a   scalar and suppose there is a non-zero vector $\alpha$ in $V$ such   that $T\alpha=c\alpha.$ Prove that there is a non-zero linear   functional $f$ on $V$ such that $T^{t}f=cf.$ ($T^{t}$ is the transpose of $T$.) I tried to solve this question by induction on $\dim V$. I was able to show the base case, that is, when $\dim V=1$, but I got stuck in the  inductive step. If $\alpha $ is a non-zero vector, then we can find a base $\mathcal{B}=\{\alpha,\alpha_{1},\ldots\alpha_{m}\}$ of $V$. We can write $V=W_{1}\oplus W_{2}$, where $W_{1}=\langle \alpha \rangle$ and $W_{2}=\langle \alpha_{1},\ldots,\alpha_{m}\rangle.$ I can not show that $T(W_{2})\subset W_{2}.$ Anyway, $\alpha \notin W_{2}.$ EDIT: If $T$ is a linear transformation from $V$ to $W$, then $T^{t}$ is the linear transformation from $W^{\star}$ into $V^{\star}$ such that $$(T^{t}f)(\alpha)=g(T\alpha)$$ for every $f\in W^{\star}$ and $\alpha \in V$.","I am self-studying Hoffman and Kunze's book Linear Algebra . This is exercise $4$ from page $115$. It is in the section of The transpose of a Linear Transformation . Let $V$ be a finite-dimensional vector space over the field   $\mathbb{F}$ and let $T$ be a linear operator on $V$. Let $c$  be a   scalar and suppose there is a non-zero vector $\alpha$ in $V$ such   that $T\alpha=c\alpha.$ Prove that there is a non-zero linear   functional $f$ on $V$ such that $T^{t}f=cf.$ ($T^{t}$ is the transpose of $T$.) I tried to solve this question by induction on $\dim V$. I was able to show the base case, that is, when $\dim V=1$, but I got stuck in the  inductive step. If $\alpha $ is a non-zero vector, then we can find a base $\mathcal{B}=\{\alpha,\alpha_{1},\ldots\alpha_{m}\}$ of $V$. We can write $V=W_{1}\oplus W_{2}$, where $W_{1}=\langle \alpha \rangle$ and $W_{2}=\langle \alpha_{1},\ldots,\alpha_{m}\rangle.$ I can not show that $T(W_{2})\subset W_{2}.$ Anyway, $\alpha \notin W_{2}.$ EDIT: If $T$ is a linear transformation from $V$ to $W$, then $T^{t}$ is the linear transformation from $W^{\star}$ into $V^{\star}$ such that $$(T^{t}f)(\alpha)=g(T\alpha)$$ for every $f\in W^{\star}$ and $\alpha \in V$.",,[]
11,Orthogonal set vs. orthogonal basis,Orthogonal set vs. orthogonal basis,,"For some reason my book distinguishes the two names. If a set is an orthogonal set, doesn't that make it immediately a basis for some subspace $W$ since all the vectors in the orthogonal set are linearly independent anyways? So why do we have two different words for the same thing?","For some reason my book distinguishes the two names. If a set is an orthogonal set, doesn't that make it immediately a basis for some subspace $W$ since all the vectors in the orthogonal set are linearly independent anyways? So why do we have two different words for the same thing?",,['linear-algebra']
12,First Course in Linear algebra books that start with basic algebra? [duplicate],First Course in Linear algebra books that start with basic algebra? [duplicate],,"This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm 30 years old, and the only math I can remember from college is basic algebra and some probabilities. Next month, I have a machine learning project I'd like to work on, but I'll need a solid footing in linear algebra first. Are there any books or tutorials that can take me from the spotty math knowledge I have now to linear algebra? Your help would be appreciated.","This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm 30 years old, and the only math I can remember from college is basic algebra and some probabilities. Next month, I have a machine learning project I'd like to work on, but I'll need a solid footing in linear algebra first. Are there any books or tutorials that can take me from the spotty math knowledge I have now to linear algebra? Your help would be appreciated.",,"['linear-algebra', 'algorithms', 'self-learning', 'machine-learning']"
13,Determining the minimal polynomial,Determining the minimal polynomial,,"How do you find the minimal polynomial, $\mu_{M^{-1}}(x)$ given  $\mu_{M}(x)$? My guess is since if $\lambda$ is an eigenvalue of $M$, then $1\over \lambda$ is an eigenvalue of $M^{-1}$, we might have something like $\mu_{M^{-1}}(x)=\mu_{M}({1\over x})$? But then I am not sure that that is the minimal polynomial... Thanks","How do you find the minimal polynomial, $\mu_{M^{-1}}(x)$ given  $\mu_{M}(x)$? My guess is since if $\lambda$ is an eigenvalue of $M$, then $1\over \lambda$ is an eigenvalue of $M^{-1}$, we might have something like $\mu_{M^{-1}}(x)=\mu_{M}({1\over x})$? But then I am not sure that that is the minimal polynomial... Thanks",,"['linear-algebra', 'matrices']"
14,Analogues between the adjoint and conjugation,Analogues between the adjoint and conjugation,,"I have been told that the adjoint of an operator behaves much like complex conjugation, and so self-adjoint operators are like real numbers. Can someone explain this analogy more in depth, or give a reference? For example, a corollary on p. 129 of Axler's ""Linear Algebra Done Right"" says that the proposition $T$ is self-adjoint iff $(Tv, v) \in \mathbb{R}$ for every $v \in V$. is an example of how self-adjoint operators behave like real numbers. I don't see what he's talking about here, and am wondering if there are other propositions that illustrate parallels between self-adjoint operators and real numbers.","I have been told that the adjoint of an operator behaves much like complex conjugation, and so self-adjoint operators are like real numbers. Can someone explain this analogy more in depth, or give a reference? For example, a corollary on p. 129 of Axler's ""Linear Algebra Done Right"" says that the proposition $T$ is self-adjoint iff $(Tv, v) \in \mathbb{R}$ for every $v \in V$. is an example of how self-adjoint operators behave like real numbers. I don't see what he's talking about here, and am wondering if there are other propositions that illustrate parallels between self-adjoint operators and real numbers.",,"['linear-algebra', 'functional-analysis']"
15,Why eigenvectors basis then transformation matrix is $\Lambda$?,Why eigenvectors basis then transformation matrix is ?,\Lambda,"Why is it that in a transformation, if the eigenvectors are chosen as the basis vectors, the transformation matrix $A$ would turn out to be that diagonal eigenvalues matrix $\Lambda$ and the transformation becomes $\Lambda \vec{x}=\vec{c}$ ? What is the reason or cause that the transformation matrix could easily just be the eigenvalues matrix $\Lambda$? I tried to prove it to show myself but somehow, it doesn't look very right. Let $\vec{b_i}$ be the eigenvectors that form the basis for the vector $\vec{v}$ and the $c_n$ are combination to the eigenvectors... $\vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + ... + c_n \vec{b_n} $ $ T(\vec{v})= c_1 T(\vec{b_1}) + c_2 T(\vec{b_2}) + ... + c_n T(\vec{b_n}) $ $ T(\vec{v})= c_1 A\vec{b_1} + c_2 A\vec{b_2} + ... + c_n A\vec{b_n} $ $ T(\vec{v})= c_1 \lambda_1 \vec{b_1} + c_2 \lambda_2\vec{b_2} + ... + c_n \lambda_n\vec{b_n} $ $ T(\vec{v})= \begin{bmatrix}  &  & \\  \vec{b_1} & \vec{b_2} &...& \vec{b_n}\\   &  &  \end{bmatrix} \begin{bmatrix} \lambda_1 &  &  & \\   & \lambda_2 &  & \\   &  & \ddots  & \\   &  &  & \lambda_n \end{bmatrix} \begin{bmatrix} c_1 &  &  & \\   & c_2 &  & \\   &  & \ddots  & \\   &  &  & c_n \end{bmatrix} $ The last line is wrong but I'm trying to force out that $\Lambda$ which I couldn't.","Why is it that in a transformation, if the eigenvectors are chosen as the basis vectors, the transformation matrix $A$ would turn out to be that diagonal eigenvalues matrix $\Lambda$ and the transformation becomes $\Lambda \vec{x}=\vec{c}$ ? What is the reason or cause that the transformation matrix could easily just be the eigenvalues matrix $\Lambda$? I tried to prove it to show myself but somehow, it doesn't look very right. Let $\vec{b_i}$ be the eigenvectors that form the basis for the vector $\vec{v}$ and the $c_n$ are combination to the eigenvectors... $\vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + ... + c_n \vec{b_n} $ $ T(\vec{v})= c_1 T(\vec{b_1}) + c_2 T(\vec{b_2}) + ... + c_n T(\vec{b_n}) $ $ T(\vec{v})= c_1 A\vec{b_1} + c_2 A\vec{b_2} + ... + c_n A\vec{b_n} $ $ T(\vec{v})= c_1 \lambda_1 \vec{b_1} + c_2 \lambda_2\vec{b_2} + ... + c_n \lambda_n\vec{b_n} $ $ T(\vec{v})= \begin{bmatrix}  &  & \\  \vec{b_1} & \vec{b_2} &...& \vec{b_n}\\   &  &  \end{bmatrix} \begin{bmatrix} \lambda_1 &  &  & \\   & \lambda_2 &  & \\   &  & \ddots  & \\   &  &  & \lambda_n \end{bmatrix} \begin{bmatrix} c_1 &  &  & \\   & c_2 &  & \\   &  & \ddots  & \\   &  &  & c_n \end{bmatrix} $ The last line is wrong but I'm trying to force out that $\Lambda$ which I couldn't.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
16,How to convert a plane (e.g. $4x - 3y + 6z = 12$) into parametric vector form?,How to convert a plane (e.g. ) into parametric vector form?,4x - 3y + 6z = 12,"I can convert something in the 2nd dimension fine, but I'm having difficulty with something like $4x - 3y + 6z = 12$. Any help? EDIT: Solve using only algebra, no matrices yet.","I can convert something in the 2nd dimension fine, but I'm having difficulty with something like $4x - 3y + 6z = 12$. Any help? EDIT: Solve using only algebra, no matrices yet.",,"['linear-algebra', 'parametric']"
17,Tips for writing math solutions for others,Tips for writing math solutions for others,,"I am working a bit on a collection of Linear Algebra examples,  as well as some examples on induction. This is what is taught freshman year at our university. I intend to release this to the public, either by selling printed copies or releasing it online. Since I do not have experience using such material myself, there are some questions I would like some opinions on: How much theory should I include? Is references to course litterature enough? Is there a format preference? Small text, so that the collection is more enviromental-friendly, or with big marginals for notes? Best way to deal with misprints? Should induction and Linear algeba be separate pieces? Please share your experience if you have done something similar. EDIT: An answer I seek is something along the lines of: ""I am a ""something"" stident, and I prefer ""something"", and would like to see more of ""something"".","I am working a bit on a collection of Linear Algebra examples,  as well as some examples on induction. This is what is taught freshman year at our university. I intend to release this to the public, either by selling printed copies or releasing it online. Since I do not have experience using such material myself, there are some questions I would like some opinions on: How much theory should I include? Is references to course litterature enough? Is there a format preference? Small text, so that the collection is more enviromental-friendly, or with big marginals for notes? Best way to deal with misprints? Should induction and Linear algeba be separate pieces? Please share your experience if you have done something similar. EDIT: An answer I seek is something along the lines of: ""I am a ""something"" stident, and I prefer ""something"", and would like to see more of ""something"".",,"['linear-algebra', 'examples-counterexamples']"
18,Find a vertex of a tetragon where three vertices are given,Find a vertex of a tetragon where three vertices are given,,"Suppose that $V,W,U$ are three 3D points and $L,K$ are given positive values. Let $dist(A,B)$ represents euclidean distance between $A,B$ . Morover, assume that $M$ is a plane that passes through $V,W,U$ . What I need is a point $P$ where: $$dist(V,P)=L \\ dist(U,P)=K \\ P\in M $$ Obviously, we must find the equations of 2 circles, $C_1(V,L), C_2(U,K)$ where $C_1\subset M, C_2\subset M$ and then find their intersections. These possible intersections would be our points. Of course $L,K$ are given such a way that the intersections exist.","Suppose that are three 3D points and are given positive values. Let represents euclidean distance between . Morover, assume that is a plane that passes through . What I need is a point where: Obviously, we must find the equations of 2 circles, where and then find their intersections. These possible intersections would be our points. Of course are given such a way that the intersections exist.","V,W,U L,K dist(A,B) A,B M V,W,U P dist(V,P)=L \\
dist(U,P)=K \\
P\in M  C_1(V,L), C_2(U,K) C_1\subset M, C_2\subset M L,K","['linear-algebra', 'geometry', 'analytic-geometry', 'vector-analysis']"
19,Upper bound on ratio of probability sums,Upper bound on ratio of probability sums,,"Let us say I have some vector of probabilities $\vec{V}$ such that $\sum_i v_i = 1$ .  I am trying to find a sharp upper bound on $\frac{\sum_i v_i^3}{(\sum_i v_i^2)^2}$ .  I can currently prove via Cauchy-Schwarz a very bad upper bound - this ratio must be less than the number of entries in the vector.  But I have numerical evidence that leads me to believe that something like 1.5 should be a sharp upper bound, not dependent on the vector length.","Let us say I have some vector of probabilities such that .  I am trying to find a sharp upper bound on .  I can currently prove via Cauchy-Schwarz a very bad upper bound - this ratio must be less than the number of entries in the vector.  But I have numerical evidence that leads me to believe that something like 1.5 should be a sharp upper bound, not dependent on the vector length.",\vec{V} \sum_i v_i = 1 \frac{\sum_i v_i^3}{(\sum_i v_i^2)^2},"['linear-algebra', 'probability', 'inequality']"
20,Criteria for $3 \times 3$ matrix to positive definite,Criteria for  matrix to positive definite,3 \times 3,"Here it is said that a $2\times 2$ matrix $A$ is positive definite if and only if $tr(A) >0$ and $det(A)>0$ . This will not work if $A$ is $3\times 3$ . But is there any way to enforce the positive definiteness of the matrix $A$ via the trace and determinant of $A$ , if $A$ is of size $3\times 3$ ?","Here it is said that a matrix is positive definite if and only if and . This will not work if is . But is there any way to enforce the positive definiteness of the matrix via the trace and determinant of , if is of size ?",2\times 2 A tr(A) >0 det(A)>0 A 3\times 3 A A A 3\times 3,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'positive-definite']"
21,When do Linear Transformations NOT preserve angles between vectors? Doesn't the SVD tell us all linear transformations preserve angles?,When do Linear Transformations NOT preserve angles between vectors? Doesn't the SVD tell us all linear transformations preserve angles?,,"From searching on the internet, I learned only a subset of linear transformations preserve angles between vectors. But - Learning about the SVD - we can geometrically understand as breaking down some matrix A into a three matrices. These matrices can be understood geometrically as a rotation step, then a scaling  step, and then another rotation. Since any matrix can be broken down into these three steps (Since SVD applies to all matrices A?) doesn't that mean that all transformations are simply a rotation, a scaling, and then a rotation, which means the angles are preserved? Why is this not true? And when do linear transformations preserve angles, and when do they not? Thanks, A","From searching on the internet, I learned only a subset of linear transformations preserve angles between vectors. But - Learning about the SVD - we can geometrically understand as breaking down some matrix A into a three matrices. These matrices can be understood geometrically as a rotation step, then a scaling  step, and then another rotation. Since any matrix can be broken down into these three steps (Since SVD applies to all matrices A?) doesn't that mean that all transformations are simply a rotation, a scaling, and then a rotation, which means the angles are preserved? Why is this not true? And when do linear transformations preserve angles, and when do they not? Thanks, A",,"['linear-algebra', 'linear-transformations', 'matrix-decomposition', 'svd', 'singular-values']"
22,Is every orthogonal matrix orthogonally diagonalizable?,Is every orthogonal matrix orthogonally diagonalizable?,,"Is every orthogonal matrix orthogonally diagonalizable? If so, how do you prove it? And is it true that the entries of every orthogonal matrix is real? Since every unitary matrix is unitarily diagonalizable, so is true that every orthogonal matrix orthogonally diagonalizable? Thank you.","Is every orthogonal matrix orthogonally diagonalizable? If so, how do you prove it? And is it true that the entries of every orthogonal matrix is real? Since every unitary matrix is unitarily diagonalizable, so is true that every orthogonal matrix orthogonally diagonalizable? Thank you.",,"['linear-algebra', 'orthogonal-matrices']"
23,What square matrices cannot be expressed as the sum of symmetric and skew-symmetric parts,What square matrices cannot be expressed as the sum of symmetric and skew-symmetric parts,,"I thought I knew that any square matrix can be written as the sum of symmetric and antisymmetric matrices since we have the property that any $n\times n$ matrix $A$ can be expressed as $A=\frac{1}{2}(A-A^T)+\frac{1}{2}(A+A^T)$ . However, I have come across the following statement and was told this is not always true and asked if I could find a counterexample where this isn't true but I have been struggling to find a counterexample (perhaps when $\mathbb{F}=\mathbb{Z}_2$ ?). Here is the statement: Let $\mathbb{F}$ be a field. Then any $A\in \mathbb{F}(n,n)$ may be written as a sum of a symmetric and an anti-symmetric matrices in $\mathbb{F}(n,n)$ . I would greatly appreciate if someone could show me a counterexample to this.","I thought I knew that any square matrix can be written as the sum of symmetric and antisymmetric matrices since we have the property that any matrix can be expressed as . However, I have come across the following statement and was told this is not always true and asked if I could find a counterexample where this isn't true but I have been struggling to find a counterexample (perhaps when ?). Here is the statement: Let be a field. Then any may be written as a sum of a symmetric and an anti-symmetric matrices in . I would greatly appreciate if someone could show me a counterexample to this.","n\times n A A=\frac{1}{2}(A-A^T)+\frac{1}{2}(A+A^T) \mathbb{F}=\mathbb{Z}_2 \mathbb{F} A\in \mathbb{F}(n,n) \mathbb{F}(n,n)","['linear-algebra', 'matrices']"
24,Prove that the intersection of 2 generalised eigenspaces is the zero space,Prove that the intersection of 2 generalised eigenspaces is the zero space,,"I have searched for my above question and came across the ""Trivial intersection of generalised eigenspaces"" post on math stack exchange but I do not understand the proof using coprime polynomials. How do I proof such a statement (below) using just the definition of eigenvalues/generalised Eigenspaces? I have seen/proven that if $\lambda \neq \mu $ . then the intersection between $ E_\lambda(T) \cap K_mu(T) = \{ \mathbf{0} \} $ (where $E_\lambda(T) $ is the Eigenspaces corresponding the eigenvalue $\lambda$ . (not sure whether this information is required for the proof) Let $ T: V \rightarrow V$ be a linear operator where $V$ is a finite dimensional vector space over $ \mathbb{C} $ . I want to prove that $$ \text{If } \lambda \neq \mu, \text{then } K_\mu(T) \ \cap \ K_\lambda(T) = \{\bf{0}\}  $$ where $$ K_\lambda(T) = \{ \mathbf{v} \in V : (T-\lambda I_V)^m(\mathbf{v})=\mathbf{0}\} $$ Currently, the lecturer has only gone through the above definition of generalised Eigenspaces (he currently assumes that m need not be the same for different $\mathbf{v} \in K_\lambda(T)$ , he has not gone through the prove that m can be chosen to satisfy all $\mathbf{v}$ in the generalised eigenspace yet) Anyway, I tried to prove the above statement by contradiction but I got stuck: Let $ \lambda \neq \mu $ and assume $$ \exists_{non-zero \ vector \ \mathbf{v} \in V}\  \text{such that } v \in K_\mu(T)  \cap K_\lambda(T) $$ Then $$ (T-\mu I_V)^m(\mathbf{v}) = \mathbf{0} = (T-\lambda I_V)^n(\mathbf{v}) $$ $$ (T-\mu I_V)^m(\mathbf{v}) = (T-\lambda I_V)^n(\mathbf{v})$$ $$ (T-\mu I_V)^m(\mathbf{v}) - (T-\lambda I_V)^n(\mathbf{v}) =\mathbf{0} $$ And I'm not sure how to proceed. Thank you for your time!!","I have searched for my above question and came across the ""Trivial intersection of generalised eigenspaces"" post on math stack exchange but I do not understand the proof using coprime polynomials. How do I proof such a statement (below) using just the definition of eigenvalues/generalised Eigenspaces? I have seen/proven that if . then the intersection between (where is the Eigenspaces corresponding the eigenvalue . (not sure whether this information is required for the proof) Let be a linear operator where is a finite dimensional vector space over . I want to prove that where Currently, the lecturer has only gone through the above definition of generalised Eigenspaces (he currently assumes that m need not be the same for different , he has not gone through the prove that m can be chosen to satisfy all in the generalised eigenspace yet) Anyway, I tried to prove the above statement by contradiction but I got stuck: Let and assume Then And I'm not sure how to proceed. Thank you for your time!!","\lambda \neq \mu   E_\lambda(T) \cap K_mu(T) = \{ \mathbf{0} \}  E_\lambda(T)  \lambda  T: V \rightarrow V V  \mathbb{C}   \text{If } \lambda \neq \mu, \text{then } K_\mu(T) \ \cap \ K_\lambda(T) = \{\bf{0}\}    K_\lambda(T) = \{ \mathbf{v} \in V : (T-\lambda I_V)^m(\mathbf{v})=\mathbf{0}\}  \mathbf{v} \in K_\lambda(T) \mathbf{v}  \lambda \neq \mu   \exists_{non-zero \ vector \ \mathbf{v} \in V}\  \text{such that } v \in K_\mu(T)  \cap K_\lambda(T)   (T-\mu I_V)^m(\mathbf{v}) = \mathbf{0} = (T-\lambda I_V)^n(\mathbf{v})   (T-\mu I_V)^m(\mathbf{v}) = (T-\lambda I_V)^n(\mathbf{v})  (T-\mu I_V)^m(\mathbf{v}) - (T-\lambda I_V)^n(\mathbf{v}) =\mathbf{0} ","['linear-algebra', 'generalized-eigenvector']"
25,$T:\Bbb{R}^2\rightarrow\Bbb{R}^2$ has 2 distinct eigenvalues. Showing that $v$ or $T(v)− \lambda_1v$ are eigenvectors of $T$,has 2 distinct eigenvalues. Showing that  or  are eigenvectors of,T:\Bbb{R}^2\rightarrow\Bbb{R}^2 v T(v)− \lambda_1v T,"$T:\Bbb{R}^2\rightarrow\Bbb{R}^2$ which is diagonalizable with 2 distinct eigenvalues. Showing that either $v$ is an eigenvector for $\lambda_1$ or else $T(v)− \lambda_1v$ is an eigenvector for $\lambda_2$ . I tried to use the matrix representation of $T$ and the Cayley-Hamilton theorem, but I can't reach the conclusion. Is there a way to prove it without the matrix representation? I would prefer to get guidance rather than a full solution.","which is diagonalizable with 2 distinct eigenvalues. Showing that either is an eigenvector for or else is an eigenvector for . I tried to use the matrix representation of and the Cayley-Hamilton theorem, but I can't reach the conclusion. Is there a way to prove it without the matrix representation? I would prefer to get guidance rather than a full solution.",T:\Bbb{R}^2\rightarrow\Bbb{R}^2 v \lambda_1 T(v)− \lambda_1v \lambda_2 T,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
26,How to get a linear map with specified kernel and image,How to get a linear map with specified kernel and image,,"This is more of an abstract question. Let $U, W$ be subspaces of a vector space $V$ . I’m thinking about how one might get these subspaces to “interact” or be “paired up” in some way. The natural thought I had was to find a linear map $T$ from $V$ to itself so that $\ker T = U$ and $\text{im} \, T = W$ . Then the rank-nullity theorem implies that we need the dimensions of $U$ and $W$ to add up to $\dim V$ . So this is a constraint. One reason I was considering such a map is that, by the First Isomorphism Theorem, $V/U \cong W$ , so we can think of $T$ as partitioning $V$ into identical copies/pieces of $U$ , and each of these pieces is uniquely associated with a vector in $W$ . So in a sense $U$ is sent into every point of $W$ . How might we find such a linear map? Does it matter whether or not $U, W$ intersect at a point other than zero, i.e. does it matter whether $U + W$ is a direct sum?","This is more of an abstract question. Let be subspaces of a vector space . I’m thinking about how one might get these subspaces to “interact” or be “paired up” in some way. The natural thought I had was to find a linear map from to itself so that and . Then the rank-nullity theorem implies that we need the dimensions of and to add up to . So this is a constraint. One reason I was considering such a map is that, by the First Isomorphism Theorem, , so we can think of as partitioning into identical copies/pieces of , and each of these pieces is uniquely associated with a vector in . So in a sense is sent into every point of . How might we find such a linear map? Does it matter whether or not intersect at a point other than zero, i.e. does it matter whether is a direct sum?","U, W V T V \ker T = U \text{im} \, T = W U W \dim V V/U \cong W T V U W U W U, W U + W","['linear-algebra', 'abstract-algebra']"
27,Cross-product identity,Cross-product identity,,"This page of vector identities lists the following (among many other identities): $$ (\mathbf{A}\cdot(\mathbf{B}\times\mathbf{C}))\,\mathbf{D}= (\mathbf{A}\cdot\mathbf{D} )\left(\mathbf{B}\times\mathbf{C}\right)+\left(\mathbf{B}\cdot\mathbf{D}\right)\left(\mathbf{C}\times\mathbf{A}\right)+\left(\mathbf{C}\cdot\mathbf{D}\right)\left(\mathbf{A}\times\mathbf{B}\right) $$ which is presumably supposed to hold for vectors $\mathbf{A,B,C,D} \in \Bbb R^3$ . Unlike the other identities, this one is given without justification or citation.  With this in mind, my questions are: Is the identity true? (proven in answers below) Is the identity well-known? Is there a citation that can be used here? How can we prove it? Some answers have been given, but alternate approaches would be interesting to see. Thank you for your consideration. Quick thoughts on the problem: $\mathbf{A}\cdot(\mathbf{B}\times\mathbf{C})$ is a scalar-triple product and can be rewritten as $$  \det \pmatrix{\mathbf{A}& \mathbf{B} & \mathbf{C}} $$ I have a hunch Cauchy-Binet can be applied here somehow This amounts to a statement about the map $$ D \mapsto [(A \times B)(C\cdot D) + (B \times C)(A\cdot D) + (C \times A)(B\cdot D)] $$ A proof in Levi-Cevita notation might be quick.","This page of vector identities lists the following (among many other identities): which is presumably supposed to hold for vectors . Unlike the other identities, this one is given without justification or citation.  With this in mind, my questions are: Is the identity true? (proven in answers below) Is the identity well-known? Is there a citation that can be used here? How can we prove it? Some answers have been given, but alternate approaches would be interesting to see. Thank you for your consideration. Quick thoughts on the problem: is a scalar-triple product and can be rewritten as I have a hunch Cauchy-Binet can be applied here somehow This amounts to a statement about the map A proof in Levi-Cevita notation might be quick.","
(\mathbf{A}\cdot(\mathbf{B}\times\mathbf{C}))\,\mathbf{D}= (\mathbf{A}\cdot\mathbf{D} )\left(\mathbf{B}\times\mathbf{C}\right)+\left(\mathbf{B}\cdot\mathbf{D}\right)\left(\mathbf{C}\times\mathbf{A}\right)+\left(\mathbf{C}\cdot\mathbf{D}\right)\left(\mathbf{A}\times\mathbf{B}\right)
 \mathbf{A,B,C,D} \in \Bbb R^3 \mathbf{A}\cdot(\mathbf{B}\times\mathbf{C})  
\det \pmatrix{\mathbf{A}& \mathbf{B} & \mathbf{C}}
 
D \mapsto [(A \times B)(C\cdot D) + (B \times C)(A\cdot D) + (C \times A)(B\cdot D)]
","['linear-algebra', 'matrices', 'multivariable-calculus', 'reference-request', 'cross-product']"
28,"How do one view a linear transformation as a (1,1) Tensor?","How do one view a linear transformation as a (1,1) Tensor?",,"I'm relatively new in tensor space theory, and while reading some materials i've came across authors describing a inner product as a $(0,2)$ tensor. I'm not sure why it is, but i think if i write a map $f$ as $$f: P \times P \rightarrow \mathbb{R} \\(p,q) \mapsto \int_{-T}^{T}p(x)q(x) \mathrm{d}x$$ This clearly defines a inner product , and here i'm taking two elements from $P$ which the integral eats and spits out something in $R$ , so it's like a $(0,2)$ tensor, can i think like this ? But i can't picture it for a linear transform as a (1,1) tensor. For a linear map $T$ defined between two finite dimensional vector spaces i.e $$T:V \rightarrow W$$ How is it tensor ? because the target is in $W$ which is not in $\mathbb{R}$ and by definition of tensor it eats $r$ copies of $V^{*}$ and $s$ copies of $V$ and spits out a real number . Also what happens if T is a endomorphism ? i'm having hard time imagining it.","I'm relatively new in tensor space theory, and while reading some materials i've came across authors describing a inner product as a tensor. I'm not sure why it is, but i think if i write a map as This clearly defines a inner product , and here i'm taking two elements from which the integral eats and spits out something in , so it's like a tensor, can i think like this ? But i can't picture it for a linear transform as a (1,1) tensor. For a linear map defined between two finite dimensional vector spaces i.e How is it tensor ? because the target is in which is not in and by definition of tensor it eats copies of and copies of and spits out a real number . Also what happens if T is a endomorphism ? i'm having hard time imagining it.","(0,2) f f: P \times P \rightarrow \mathbb{R} \\(p,q) \mapsto \int_{-T}^{T}p(x)q(x) \mathrm{d}x P R (0,2) T T:V \rightarrow W W \mathbb{R} r V^{*} s V","['linear-algebra', 'functional-analysis', 'tensors', 'multilinear-algebra']"
29,"Real-world applications of fields, rings and groups in linear algebra.","Real-world applications of fields, rings and groups in linear algebra.",,"Real-world applications of fields, rings and groups in linear algebra. A friend of mine asked me where one could use the definitions of rings, groups, fields etc. I was very embarrassed of the fact that I could only mention cyber security - nothing more (I'm studying IT). That's why I would like to get some more detailed suggestions. I would really appreciate that. Thank you in advance!","Real-world applications of fields, rings and groups in linear algebra. A friend of mine asked me where one could use the definitions of rings, groups, fields etc. I was very embarrassed of the fact that I could only mention cyber security - nothing more (I'm studying IT). That's why I would like to get some more detailed suggestions. I would really appreciate that. Thank you in advance!",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'field-theory', 'applications']"
30,Finding eigenvector only knowing others eigenvectors.,Finding eigenvector only knowing others eigenvectors.,,"The matrix $A \in M_3(\mathbb{R})$ satisfy $A^t=A$ and $(1,2,1), (-1,1,0)$ are eigenvectors of $A$ . Which vector is also an eigenvector of $A$ ? Alternatives: $(0,0,1)$ ; $(1,1,-3)$ ; $(1,1,3)$ ; There is no other eigenvector . The problem with this exercise is that I don't know the matrix $A$ , and I don't have any eigenvalue to start with. I can get a matrix with less variables using $A = A^t$ , but there's still 6 variables. Any tips or guidance is appreciated.","The matrix satisfy and are eigenvectors of . Which vector is also an eigenvector of ? Alternatives: ; ; ; There is no other eigenvector . The problem with this exercise is that I don't know the matrix , and I don't have any eigenvalue to start with. I can get a matrix with less variables using , but there's still 6 variables. Any tips or guidance is appreciated.","A \in M_3(\mathbb{R}) A^t=A (1,2,1), (-1,1,0) A A (0,0,1) (1,1,-3) (1,1,3) A A = A^t","['linear-algebra', 'eigenvalues-eigenvectors']"
31,What happens if matrix $A^2$ has a zero column?,What happens if matrix  has a zero column?,A^2,"Let me assume that $A$ is a square matrix and the matrix $A^2$ has a column of zeros. Is it possible for me to prove that A has a column of zeros. I know that the determinants are both zero, however it did not help me. I assumed that jth column is the zero row and had [i,j] term $(a_{i1}*a_{1j}+a_{i2}*a_{2j}+...+a_{ij}*a_{ij}+...+a_{in}*a_{nj}=0)$ equal to zero. I summed it for all $i$ in range 1 and $n$ , and tried to prove it. But I was not able to do it. Are there anything I am missing?","Let me assume that is a square matrix and the matrix has a column of zeros. Is it possible for me to prove that A has a column of zeros. I know that the determinants are both zero, however it did not help me. I assumed that jth column is the zero row and had [i,j] term equal to zero. I summed it for all in range 1 and , and tried to prove it. But I was not able to do it. Are there anything I am missing?",A A^2 (a_{i1}*a_{1j}+a_{i2}*a_{2j}+...+a_{ij}*a_{ij}+...+a_{in}*a_{nj}=0) i n,"['linear-algebra', 'matrices', 'proof-verification']"
32,Is the matrix $A = I - \frac{J}{n+1}$ idempotent?,Is the matrix  idempotent?,A = I - \frac{J}{n+1},"I am supposed to figure out if the following statement is false or true. If $I$ is the $n \times n$ identity matrix, and $J$ is an $n \times n$ matrix consisting entirely of ones, then the matrix $$A = I - \frac{J}{n+1}$$ is idempotent (i.e., $A^{2} = A$ ). I understand obviously what $I$ and $J$ are, my issue is with the $A = I - \frac{J}{n+1}$ . I searched my textbook and found no reference to it. What does it mean?","I am supposed to figure out if the following statement is false or true. If is the identity matrix, and is an matrix consisting entirely of ones, then the matrix is idempotent (i.e., ). I understand obviously what and are, my issue is with the . I searched my textbook and found no reference to it. What does it mean?",I n \times n J n \times n A = I - \frac{J}{n+1} A^{2} = A I J A = I - \frac{J}{n+1},"['linear-algebra', 'matrices', 'idempotents']"
33,Orbit of a matrix can generates a basis?,Orbit of a matrix can generates a basis?,,"Let A be a matrix $n\times n$, given a n-vector $v$, what conditions over $v$ and $A$ are necessaries for $[v, Av,..., A^{n-1}v]$ will be linearly independent? For example if $v$ is a eigenvector or $A^k=Id$ $(k<n)$,  they are not linearly independent. In other words, when the orbit of a matrix over a vector space $V$ (finite dimensional) can  generates a basis of $V$?","Let A be a matrix $n\times n$, given a n-vector $v$, what conditions over $v$ and $A$ are necessaries for $[v, Av,..., A^{n-1}v]$ will be linearly independent? For example if $v$ is a eigenvector or $A^k=Id$ $(k<n)$,  they are not linearly independent. In other words, when the orbit of a matrix over a vector space $V$ (finite dimensional) can  generates a basis of $V$?",,"['linear-algebra', 'matrices', 'vector-spaces', 'group-actions', 'topological-vector-spaces']"
34,Linear algebra change of basis always makes a linear map diagonal.,Linear algebra change of basis always makes a linear map diagonal.,,Prove that there exist bases $\alpha $ and $\beta$ for V such that $ [T]_{\alpha}^{\beta} $ is a diagonal matrix with each diagonal entry equal to either 0 or 1. Originally i thought that T=I was the only solution to this i realize that is not the case now but i am still lost what if T forms an non-invertable matrix? i really dont understand how we can always know that this is true cause if the map is non-invertable isn't it not diagonalizable?,Prove that there exist bases $\alpha $ and $\beta$ for V such that $ [T]_{\alpha}^{\beta} $ is a diagonal matrix with each diagonal entry equal to either 0 or 1. Originally i thought that T=I was the only solution to this i realize that is not the case now but i am still lost what if T forms an non-invertable matrix? i really dont understand how we can always know that this is true cause if the map is non-invertable isn't it not diagonalizable?,,['linear-algebra']
35,Matrices restricted to a subspace,Matrices restricted to a subspace,,"Let $Q$ be an $n\times n$ stochastic matrix. Let $\mathcal S$ be the following subspace of $\mathbb R^n$ : $$\mathcal S:=\left\{x\in\mathbb R^n: \sum_{i=1}^nx_i=0 \right\}\, .$$ In a paper that I'm reading, there is a concept that I do not know: the restriction of $Q$ to $\mathcal S$ , (denoted by $Q|_{\mathcal S}$ ). What does it mean? For example, if I have a given matrix $Q$ , how could I calculate $Q|_{\mathcal S}$ ?","Let be an stochastic matrix. Let be the following subspace of : In a paper that I'm reading, there is a concept that I do not know: the restriction of to , (denoted by ). What does it mean? For example, if I have a given matrix , how could I calculate ?","Q n\times n \mathcal S \mathbb R^n \mathcal S:=\left\{x\in\mathbb R^n: \sum_{i=1}^nx_i=0 \right\}\, . Q \mathcal S Q|_{\mathcal S} Q Q|_{\mathcal S}",['linear-algebra']
36,Eigenvalues of a matrix multiplied by its transpose,Eigenvalues of a matrix multiplied by its transpose,,I recall being told that the eigenvalues of the matrix formed by multiplying a matrix by its transpose are the squares of the eigenvalues of the original matrix. Is that true for any matrix? Surely not.,I recall being told that the eigenvalues of the matrix formed by multiplying a matrix by its transpose are the squares of the eigenvalues of the original matrix. Is that true for any matrix? Surely not.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'transpose']"
37,Is the real part of a positive semi-definite matrix positive semi-definite over $\mathbb C^n$,Is the real part of a positive semi-definite matrix positive semi-definite over,\mathbb C^n,"Let $P$ be a positive semi-definite Hermitian matrix (i.e. $P^\dagger=P$, $x^\dagger Px \geq 0$ for all $x \in \mathbb C^n$). Then the matrix can be decomposed as $P= R+iM$ where $R$ and $M$ are strictly real matrices. Clearly $R$ is symmetric and $M$ is skew-symmetric. I call $R$ the real part of $P$. I am almost certain that the real part $R$ must be positive semi-definite as well as I cannot find a counter-example even through numerical simulation. Is this assumption correct? Is there a way to prove the positivity of $R$ over $\mathbb C^n$ or does somebody have a counter-example?","Let $P$ be a positive semi-definite Hermitian matrix (i.e. $P^\dagger=P$, $x^\dagger Px \geq 0$ for all $x \in \mathbb C^n$). Then the matrix can be decomposed as $P= R+iM$ where $R$ and $M$ are strictly real matrices. Clearly $R$ is symmetric and $M$ is skew-symmetric. I call $R$ the real part of $P$. I am almost certain that the real part $R$ must be positive semi-definite as well as I cannot find a counter-example even through numerical simulation. Is this assumption correct? Is there a way to prove the positivity of $R$ over $\mathbb C^n$ or does somebody have a counter-example?",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
38,To show a function is bilinear symmetric non degenerate form,To show a function is bilinear symmetric non degenerate form,,"Let $V$ be vector space of set of $n×n$ matrices over $R$. Define $\langle A,B \rangle = \mathrm{trace}(AB)$, $A$, $B$ in $V$. show that $\langle \ \ ,\  \rangle$ is a non degenerate symmetric bilinear form. Now succeeded in showing that the function  $\langle \ \ ,\  \rangle$ is a symmetric bilinear form by checking the properties of bilinear form but now to show that it is non degenerate I need help. I know a function is nondegenerate if left radical or right radical of  $\langle \ \ ,\  \rangle$ is zero. That means for a bilinear form $B$ on $V$ , if  I get $S=\{y\in V \ |\  B(x,y)=0, \mbox{for all } x\in V\}=\{0\}$ then $B$ is nondegenerate. So if I start with  $\langle A, B \rangle =0$ Then $\mathrm{trace}(AB)=0$ for all $A\in V$ My claim is to show $B=0$? Am I going right?","Let $V$ be vector space of set of $n×n$ matrices over $R$. Define $\langle A,B \rangle = \mathrm{trace}(AB)$, $A$, $B$ in $V$. show that $\langle \ \ ,\  \rangle$ is a non degenerate symmetric bilinear form. Now succeeded in showing that the function  $\langle \ \ ,\  \rangle$ is a symmetric bilinear form by checking the properties of bilinear form but now to show that it is non degenerate I need help. I know a function is nondegenerate if left radical or right radical of  $\langle \ \ ,\  \rangle$ is zero. That means for a bilinear form $B$ on $V$ , if  I get $S=\{y\in V \ |\  B(x,y)=0, \mbox{for all } x\in V\}=\{0\}$ then $B$ is nondegenerate. So if I start with  $\langle A, B \rangle =0$ Then $\mathrm{trace}(AB)=0$ for all $A\in V$ My claim is to show $B=0$? Am I going right?",,['linear-algebra']
39,Show $\text{det}(A)$ is divisible by $2^{n-1}$,Show  is divisible by,\text{det}(A) 2^{n-1},Let $A_{n \times n}$ be a $n$ by $n$ matrix with all entries equal to $\pm 1$. I want to show $\text{det}(A)$ is divisible by $2^{n-1}$. How can I appraoch this?,Let $A_{n \times n}$ be a $n$ by $n$ matrix with all entries equal to $\pm 1$. I want to show $\text{det}(A)$ is divisible by $2^{n-1}$. How can I appraoch this?,,"['linear-algebra', 'matrices', 'determinant']"
40,We know that $A^{23} = 0$. What are the eigenvalues of $A$? [duplicate],We know that . What are the eigenvalues of ? [duplicate],A^{23} = 0 A,"This question already has answers here : Prove that the only eigenvalue of a nilpotent operator is 0? (2 answers) Closed 7 years ago . Let $A$ be an $n \times n$ matrix. We know that $A^{23} = 0$. What are the eigenvalues of $A$? I think it's just $0$, but I'm not sure. How should I do this problem?","This question already has answers here : Prove that the only eigenvalue of a nilpotent operator is 0? (2 answers) Closed 7 years ago . Let $A$ be an $n \times n$ matrix. We know that $A^{23} = 0$. What are the eigenvalues of $A$? I think it's just $0$, but I'm not sure. How should I do this problem?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
41,Sylvester's law of inertia,Sylvester's law of inertia,,"Considering the bilinear form on $R$, $\phi$, defined by the matrix   \begin{bmatrix}{-1}&{-1}&{-1}\\{-1}&{1}&{0}\\{-1}&{0}&{1}\end{bmatrix}   Classify, according to the Sylvester's law of inertia this bilinear   form and specify if it is a definite positive. After reading in various books and sites about the Sylvester's law of inertia, I still don't understand why it is used and, more concretely in this problem, how to classify a bilinear form. Thanks in advance for explaining the Syvester's law of inertia and how should approach this problem.","Considering the bilinear form on $R$, $\phi$, defined by the matrix   \begin{bmatrix}{-1}&{-1}&{-1}\\{-1}&{1}&{0}\\{-1}&{0}&{1}\end{bmatrix}   Classify, according to the Sylvester's law of inertia this bilinear   form and specify if it is a definite positive. After reading in various books and sites about the Sylvester's law of inertia, I still don't understand why it is used and, more concretely in this problem, how to classify a bilinear form. Thanks in advance for explaining the Syvester's law of inertia and how should approach this problem.",,"['linear-algebra', 'bilinear-form']"
42,When does a matrix admit a Jordan canonical form?,When does a matrix admit a Jordan canonical form?,,"If a matrix over the field $\mathbb R$ has as elementary divisors: $x-4$, $x^2 + 2$, does it then admit a Jordan canonical form? Am I right in thinking that a matrix has a Jordan canonical form only when its elementary divisors are all of the form $(x-a)^n$? sorry if my question is too basic. I am learning these stuff on my own and I am having some difficulty understanding them","If a matrix over the field $\mathbb R$ has as elementary divisors: $x-4$, $x^2 + 2$, does it then admit a Jordan canonical form? Am I right in thinking that a matrix has a Jordan canonical form only when its elementary divisors are all of the form $(x-a)^n$? sorry if my question is too basic. I am learning these stuff on my own and I am having some difficulty understanding them",,['linear-algebra']
43,"Does it follow that the hermetian part of a matrix is positive definite, that the matrix itself is invertible? [duplicate]","Does it follow that the hermetian part of a matrix is positive definite, that the matrix itself is invertible? [duplicate]",,"This question already has an answer here : Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues (1 answer) Closed 8 years ago . I came across this in a paper and I was wondering whether it is true. We have a complex matrix $M$ such that $(M+M^H)$ is positive definite. Now, it is clear that $(M+M^H)$ is invertible, but does that hold for $M$? Is there (in general) some connections between the hermetian part of a matrix and the hermetian part of its inverse? Thanks guys!","This question already has an answer here : Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues (1 answer) Closed 8 years ago . I came across this in a paper and I was wondering whether it is true. We have a complex matrix $M$ such that $(M+M^H)$ is positive definite. Now, it is clear that $(M+M^H)$ is invertible, but does that hold for $M$? Is there (in general) some connections between the hermetian part of a matrix and the hermetian part of its inverse? Thanks guys!",,"['linear-algebra', 'matrices', 'operator-theory', 'matrix-decomposition']"
44,Transpose of $(X'X)^{-1}$,Transpose of,(X'X)^{-1},"I am taking a Phd class in econometrics, and the following is used constantly, for $X$ a $n\times k$ matrix, $n \neq k$: $$(X'X)^{-1} = ((X'X)^{-1})'$$ with ""$'$"" standing for transpose. Having a rather weak background in linear algebra, I cannot understand why this is true. For example: ${\underset{k\times1}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)}}\underset{1\times k}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)'}}}={\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times n}{\underbrace{X'}}\underset{n\times n}{\underbrace{uu'}}\underset{n\times k}{\underbrace{X}}\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}}$ To my understanding the far right expression on the RHS should be different.","I am taking a Phd class in econometrics, and the following is used constantly, for $X$ a $n\times k$ matrix, $n \neq k$: $$(X'X)^{-1} = ((X'X)^{-1})'$$ with ""$'$"" standing for transpose. Having a rather weak background in linear algebra, I cannot understand why this is true. For example: ${\underset{k\times1}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)}}\underset{1\times k}{\underbrace{\left(\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times1}{\underbrace{X'u}}\right)'}}}={\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}\underset{k\times n}{\underbrace{X'}}\underset{n\times n}{\underbrace{uu'}}\underset{n\times k}{\underbrace{X}}\underset{k\times k}{\underbrace{\left(X'X\right)^{-1}}}}$ To my understanding the far right expression on the RHS should be different.",,"['linear-algebra', 'matrices', 'regression', 'transpose']"
45,Prove That $M^2+xM+yI$ and $M^2-xM+yI$ are non-singular,Prove That  and  are non-singular,M^2+xM+yI M^2-xM+yI,"Let $M$ be an Invertible Hermitian matrix and let $x,y\in\Bbb R$ such that $x^2\lt 4y$,Then Prove That $M^2+xM+yI$ and $M^2-xM+yI$ are non-singular. My Attempt: $$(M^2+xM+yI)(M^2-xM+yI)=(M^2+yI)^2-(xM)^2$$ Now I Don't Know How to proceed further, I know that all the eigen values of Hermitian matrix are real. Help is needed. Thank you.","Let $M$ be an Invertible Hermitian matrix and let $x,y\in\Bbb R$ such that $x^2\lt 4y$,Then Prove That $M^2+xM+yI$ and $M^2-xM+yI$ are non-singular. My Attempt: $$(M^2+xM+yI)(M^2-xM+yI)=(M^2+yI)^2-(xM)^2$$ Now I Don't Know How to proceed further, I know that all the eigen values of Hermitian matrix are real. Help is needed. Thank you.",,"['linear-algebra', 'matrices']"
46,"If $\sqrt{\frac{n+15}{n+1}}\in\mathbb Q$, then $n=17$","If , then",\sqrt{\frac{n+15}{n+1}}\in\mathbb Q n=17,"How can one show that : If $\sqrt{\frac{n+15}{n+1}}\in\mathbb Q$ so $n=17$ I tried using the fact that any number $a\in\mathbb Q$ so $a=\frac{x}{y}$ such that $\gcd(x,y)=1$ So $\frac{n+15}{n+1}=\frac{x^2}{y^2}$ But here I'm stuck.","How can one show that : If $\sqrt{\frac{n+15}{n+1}}\in\mathbb Q$ so $n=17$ I tried using the fact that any number $a\in\mathbb Q$ so $a=\frac{x}{y}$ such that $\gcd(x,y)=1$ So $\frac{n+15}{n+1}=\frac{x^2}{y^2}$ But here I'm stuck.",,"['linear-algebra', 'number-theory']"
47,About the elements of a finite subgroup of $\mathrm{GL}(\mathbb{R}^{n})$,About the elements of a finite subgroup of,\mathrm{GL}(\mathbb{R}^{n}),"Let $G$ be a finite subgroup of $\mathrm{GL}(\mathbb{R}^{n})$. I would like to prove that for every $g \in G$, $\det(g) \in \lbrace -1,1 \rbrace$. Here are my ideas : since $G$ is a finite subgroup of $\mathrm{GL}(\mathbb{R}^{n})$, the elements of $G$ satisfy to : $X^{e} - \mathrm{Id} = 0$ (for $e \in \mathbb{N}^{\ast}$). Therefore, the eigenvalues of the elements of $G$ are roots of unity in $\mathbb{C}$. For a given element $g \in G$, we can also note that if $\lambda \in \mathbb{C}$ is an eigenvalue for $g$, then $\overline{\lambda}$ is also an eigenvalue for $g$. Therefore, the determinant of $g$ will be either $-1$ or $1$. Is this correct ?","Let $G$ be a finite subgroup of $\mathrm{GL}(\mathbb{R}^{n})$. I would like to prove that for every $g \in G$, $\det(g) \in \lbrace -1,1 \rbrace$. Here are my ideas : since $G$ is a finite subgroup of $\mathrm{GL}(\mathbb{R}^{n})$, the elements of $G$ satisfy to : $X^{e} - \mathrm{Id} = 0$ (for $e \in \mathbb{N}^{\ast}$). Therefore, the eigenvalues of the elements of $G$ are roots of unity in $\mathbb{C}$. For a given element $g \in G$, we can also note that if $\lambda \in \mathbb{C}$ is an eigenvalue for $g$, then $\overline{\lambda}$ is also an eigenvalue for $g$. Therefore, the determinant of $g$ will be either $-1$ or $1$. Is this correct ?",,"['linear-algebra', 'finite-groups']"
48,Should I use set notation or list notation when writing out a basis of vectors?,Should I use set notation or list notation when writing out a basis of vectors?,,"I think in Sheldon Axler's Linear Algebra Done Right , he makes a comment about why the technically correct way is to write vectors in lists, such as $(v_1, ... v_n)$, while many books use set notation, such as $\{v_1, ... , v_n\}$. I believe set notation just includes the distinct vectors, while lists allow repeat vectors, such as this list $(v_1, v_2, v_2, ..., v_1, v_n)$. Or is it not important and both are acceptable?","I think in Sheldon Axler's Linear Algebra Done Right , he makes a comment about why the technically correct way is to write vectors in lists, such as $(v_1, ... v_n)$, while many books use set notation, such as $\{v_1, ... , v_n\}$. I believe set notation just includes the distinct vectors, while lists allow repeat vectors, such as this list $(v_1, v_2, v_2, ..., v_1, v_n)$. Or is it not important and both are acceptable?",,"['linear-algebra', 'vector-spaces', 'notation']"
49,Interesting examples of non-normal operators?,Interesting examples of non-normal operators?,,"I am currently learning spectral aspects of linear algebra. At first sight, it seems like normality is very narrow restriction. But, I can not think up any examples of non-normal operators. There is example on Wiki, but it is just matrix, and I want to find out some broad and intuitive class of examples. What are examples of non-normal operators (especially finite-dimensional)?","I am currently learning spectral aspects of linear algebra. At first sight, it seems like normality is very narrow restriction. But, I can not think up any examples of non-normal operators. There is example on Wiki, but it is just matrix, and I want to find out some broad and intuitive class of examples. What are examples of non-normal operators (especially finite-dimensional)?",,"['linear-algebra', 'examples-counterexamples', 'spectral-theory']"
50,Is it possible to have an $n\times n$ real matrix $A$ such that $A^TA$ has an eigenvalue of $-1$?,Is it possible to have an  real matrix  such that  has an eigenvalue of ?,n\times n A A^TA -1,"Question: Is it possible to have an $n\times n$ real matrix $A$ such that $A^TA$ has an eigenvalue of $-1$? I can prove that it is not possible for $n=1,2$, but I am not sure for the general case. Case $n=1$: $a^2v=-v$ $\implies$ $(a^2+1)v=0$ $\implies$ $v=0$. Case $n=2$: Write $A=\begin{pmatrix}a&b \\ c&d\end{pmatrix}$. Then, $A^T A=\begin{pmatrix}a^2+c^2&ab+cd\\ ab+cd&b^2+d^2\end{pmatrix}$, so $$ \begin{align} \det(A^TA+1)  &= \begin{vmatrix}a^2+c^2+1&ab+cd\\ ab+cd&b^2+d^2+1\end{vmatrix}\\ &= 1+a^2+b^2+c^2+d^2+(ad-bc)^2\\ &\neq 0. \end{align} $$ Now, can we generalize to all $n$?","Question: Is it possible to have an $n\times n$ real matrix $A$ such that $A^TA$ has an eigenvalue of $-1$? I can prove that it is not possible for $n=1,2$, but I am not sure for the general case. Case $n=1$: $a^2v=-v$ $\implies$ $(a^2+1)v=0$ $\implies$ $v=0$. Case $n=2$: Write $A=\begin{pmatrix}a&b \\ c&d\end{pmatrix}$. Then, $A^T A=\begin{pmatrix}a^2+c^2&ab+cd\\ ab+cd&b^2+d^2\end{pmatrix}$, so $$ \begin{align} \det(A^TA+1)  &= \begin{vmatrix}a^2+c^2+1&ab+cd\\ ab+cd&b^2+d^2+1\end{vmatrix}\\ &= 1+a^2+b^2+c^2+d^2+(ad-bc)^2\\ &\neq 0. \end{align} $$ Now, can we generalize to all $n$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
51,How can I prove that two vectors in $ℝ^3$ are linearly independent iff their cross product is nonzero?,How can I prove that two vectors in  are linearly independent iff their cross product is nonzero?,ℝ^3,"Here's my attempt: Let $𝒙 = (x_1, x_2, x_3)$ and $𝒚 = (y_1, y_2, y_3)$ The cross product of $𝒙, 𝒚$ is $𝒙⨯𝒚=(x_2y_3-x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)$ And linear independence of $𝒙, 𝒚$ means that if $a𝒙 + b𝒚 = 0$, then $a = b = 0$, i.e. $a(x_1, x_2, x_3) + b(x_1, x_2, x_3) = 0 ⇒ a = b = 0$ So if cross product is nonzero, then $ x_2y_3-x_3y_2 + x_3y_1 - x_1y_3 + x_1y_2 - x_2y_1 ≠ 0, i.e. x_1(y_2-y_3)+x_2(y_3-y_1) + x_3(y_1 - y_2) ≠ 0$ And then I'm just getting confused. I can't seem to connect the two in a formal proof.","Here's my attempt: Let $𝒙 = (x_1, x_2, x_3)$ and $𝒚 = (y_1, y_2, y_3)$ The cross product of $𝒙, 𝒚$ is $𝒙⨯𝒚=(x_2y_3-x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)$ And linear independence of $𝒙, 𝒚$ means that if $a𝒙 + b𝒚 = 0$, then $a = b = 0$, i.e. $a(x_1, x_2, x_3) + b(x_1, x_2, x_3) = 0 ⇒ a = b = 0$ So if cross product is nonzero, then $ x_2y_3-x_3y_2 + x_3y_1 - x_1y_3 + x_1y_2 - x_2y_1 ≠ 0, i.e. x_1(y_2-y_3)+x_2(y_3-y_1) + x_3(y_1 - y_2) ≠ 0$ And then I'm just getting confused. I can't seem to connect the two in a formal proof.",,"['linear-algebra', 'cross-product']"
52,If $\lambda^n $ is an eigenvalue of $A^n$ then $\lambda $ is an eigenvalue of $A$?,If  is an eigenvalue of  then  is an eigenvalue of ?,\lambda^n  A^n \lambda  A,"I'm trying to figure out whether the following statement is correct: If $\lambda^n $ is an eigenvalue of $A^n$ associated to a certain   eigenvector $v$ then $\lambda $ is an eigenvalue of $A$ associated to   the same eigenvector $v$. I know that the opposite statement is correct, that is, if $\lambda $ is an eigenvalue of $A$ associated to a certain eigenvector $v$ then $\lambda^n $ is an eigenvalue of $A^n$ associated to the same eigenvector $v$. I tried to guess some matrices to find out a counter example but I couldn't find anything. It seems that if we take $A$ to be diagonal matrix then the statement is always correct. In particular if we take $A$ such that $A^n$=$\lambda^n I $ then it must be that $A=\lambda I$.","I'm trying to figure out whether the following statement is correct: If $\lambda^n $ is an eigenvalue of $A^n$ associated to a certain   eigenvector $v$ then $\lambda $ is an eigenvalue of $A$ associated to   the same eigenvector $v$. I know that the opposite statement is correct, that is, if $\lambda $ is an eigenvalue of $A$ associated to a certain eigenvector $v$ then $\lambda^n $ is an eigenvalue of $A^n$ associated to the same eigenvector $v$. I tried to guess some matrices to find out a counter example but I couldn't find anything. It seems that if we take $A$ to be diagonal matrix then the statement is always correct. In particular if we take $A$ such that $A^n$=$\lambda^n I $ then it must be that $A=\lambda I$.",,"['linear-algebra', 'matrices']"
53,Proof that inverse of a matrix is unique [duplicate],Proof that inverse of a matrix is unique [duplicate],,"This question already has answers here : Proof that the inverse of a square matrix is unique (3 answers) Closed 9 years ago . If B and C are both inverses of the matrix A,then B=C . Can't i prove it in following way ? Proof: AB=BA=I and AC=CA=I ,then BA=CA=I By postmultiplication $\Rightarrow (BA)(A^{-1})=(CA)(A^{-1})=(I)(A^{-1})\Rightarrow B=C=A^{-1}$, or by premultiplication $AB=AC=I\Rightarrow (A^{-1})(AB)=(A^{-1})(AC)=(A^{-1})(I)\Rightarrow B=C=A^{-1}$.","This question already has answers here : Proof that the inverse of a square matrix is unique (3 answers) Closed 9 years ago . If B and C are both inverses of the matrix A,then B=C . Can't i prove it in following way ? Proof: AB=BA=I and AC=CA=I ,then BA=CA=I By postmultiplication $\Rightarrow (BA)(A^{-1})=(CA)(A^{-1})=(I)(A^{-1})\Rightarrow B=C=A^{-1}$, or by premultiplication $AB=AC=I\Rightarrow (A^{-1})(AB)=(A^{-1})(AC)=(A^{-1})(I)\Rightarrow B=C=A^{-1}$.",,"['linear-algebra', 'matrices', 'inverse']"
54,What is the matrix norm induced by weighted vector norm?,What is the matrix norm induced by weighted vector norm?,,"I denote vector norms with doulbe bars and matrix norms with triple bars. It is well known that the vector norm $L_2$ i.e. $\| x \|_2 = \sqrt{x^\top x}$ induces the matrix norm $||| \cdot  |||_2$, which is the largest singular value of a matrix. Consider the weighted norm, i.e. $\| x \|_W = \sqrt{x^\top W x} = \| W^\frac12 x\|_2$, where $W$ is some diagonal matrix of positive weights. What is the matrix norm induced by the vector norm $\| \cdot \|_W$ ? Does it have a formula like $||| \cdot |||_W = |||F \cdot |||_2$ for some matrix $F$?","I denote vector norms with doulbe bars and matrix norms with triple bars. It is well known that the vector norm $L_2$ i.e. $\| x \|_2 = \sqrt{x^\top x}$ induces the matrix norm $||| \cdot  |||_2$, which is the largest singular value of a matrix. Consider the weighted norm, i.e. $\| x \|_W = \sqrt{x^\top W x} = \| W^\frac12 x\|_2$, where $W$ is some diagonal matrix of positive weights. What is the matrix norm induced by the vector norm $\| \cdot \|_W$ ? Does it have a formula like $||| \cdot |||_W = |||F \cdot |||_2$ for some matrix $F$?",,"['linear-algebra', 'matrices', 'normed-spaces']"
55,Sum of k-largest eigenvalues of a symmetric matrix as an SDP,Sum of k-largest eigenvalues of a symmetric matrix as an SDP,,"I found the following statement from a google search. If $S_k(\mathbf{X})$ is the sum of the $k$ largest eigenvalues of a symmetric $m\times m$ matrix $\mathbf{X}$, then,$$S_k(\mathbf{X}) \leq t$$ is true iff $$ t - k s - \mathrm{trace}(\mathbf{Z}) \geq 0 \\ \mathbf{Z} \geq 0 \\ \mathbf{Z} - \mathbf{X} + s\mathbf{I}_m \geq 0$$, where $s$ hasn't been described. I tried with $s$ as a positive variable in CVX and it worked and I could calculate the $k$ largest eigenvalues of $\mathbf{X}$. I seem to be going around in circles trying to prove this one. I would like to prove it on my own. Can anyone provide a starting direction or the first few lines of the proof. Thank you.","I found the following statement from a google search. If $S_k(\mathbf{X})$ is the sum of the $k$ largest eigenvalues of a symmetric $m\times m$ matrix $\mathbf{X}$, then,$$S_k(\mathbf{X}) \leq t$$ is true iff $$ t - k s - \mathrm{trace}(\mathbf{Z}) \geq 0 \\ \mathbf{Z} \geq 0 \\ \mathbf{Z} - \mathbf{X} + s\mathbf{I}_m \geq 0$$, where $s$ hasn't been described. I tried with $s$ as a positive variable in CVX and it worked and I could calculate the $k$ largest eigenvalues of $\mathbf{X}$. I seem to be going around in circles trying to prove this one. I would like to prove it on my own. Can anyone provide a starting direction or the first few lines of the proof. Thank you.",,"['linear-algebra', 'convex-analysis', 'convex-optimization']"
56,Eigenvalues of $\left[\begin{smallmatrix}1 & 1 & 2 \\1 & 2 & 1 \\2 & 1 & 1 \\ \end{smallmatrix}\right]$,Eigenvalues of,\left[\begin{smallmatrix}1 & 1 & 2 \\1 & 2 & 1 \\2 & 1 & 1 \\ \end{smallmatrix}\right],"I am trying to find the eigenvalues of $A$ where $$A=         \begin{bmatrix}         1 & 1 & 2 \\         1 & 2 & 1 \\         2 & 1 & 1 \\         \end{bmatrix} $$ I'm stuck after writing out the equation $(1-λ)(2-λ)(1-λ)-6(1-λ) = 0$. I have tried solving this in two different ways (using characteristic equation and otherwise, both result in the above equation), but the solutions on the eigenvalue calculator for 3x3 matrices on WolframAlpha differ from mine. What am I doing wrong here? Edit: My bad, I forgot to include the solution I have written. I took a picture of it: I also have another solution where I used the formula for the determinant of a 3x3 matrix and I got the same answer. WolframAlpha says that the eigenvalues for this are $λ = 1, -1$, and $4$.","I am trying to find the eigenvalues of $A$ where $$A=         \begin{bmatrix}         1 & 1 & 2 \\         1 & 2 & 1 \\         2 & 1 & 1 \\         \end{bmatrix} $$ I'm stuck after writing out the equation $(1-λ)(2-λ)(1-λ)-6(1-λ) = 0$. I have tried solving this in two different ways (using characteristic equation and otherwise, both result in the above equation), but the solutions on the eigenvalue calculator for 3x3 matrices on WolframAlpha differ from mine. What am I doing wrong here? Edit: My bad, I forgot to include the solution I have written. I took a picture of it: I also have another solution where I used the formula for the determinant of a 3x3 matrix and I got the same answer. WolframAlpha says that the eigenvalues for this are $λ = 1, -1$, and $4$.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
57,Find whether vector w belongs in the span,Find whether vector w belongs in the span,,"$$v_1=[1,0,1,2]$$ $$v_2 = [0,1,1,3]$$ $$v_3 = [2,1,3,7]$$ $$w = [1,2,3,4]$$ We are supposed to determine if $w$ is in $\operatorname{span}(v_1,v_2,v_3)$.","$$v_1=[1,0,1,2]$$ $$v_2 = [0,1,1,3]$$ $$v_3 = [2,1,3,7]$$ $$w = [1,2,3,4]$$ We are supposed to determine if $w$ is in $\operatorname{span}(v_1,v_2,v_3)$.",,['linear-algebra']
58,The determinant of adjugate matrix,The determinant of adjugate matrix,,Why does $\det(\text{adj}(A)) = 0$  if  $\det(A) = 0$? (without using the formula $\det(\text{adj}(A)) = \det(A)^{n-1}.)$,Why does $\det(\text{adj}(A)) = 0$  if  $\det(A) = 0$? (without using the formula $\det(\text{adj}(A)) = \det(A)^{n-1}.)$,,"['linear-algebra', 'determinant']"
59,The number of $n\times n$ matrix over integer modulo $p$ field with determinant equal $1$,The number of  matrix over integer modulo  field with determinant equal,n\times n p 1,"How to count the number of $n\times n$ matrix over integer modulo $p$ field with determinant equal $1$? I know that the number of invertible matrices is GL$(n,p)$.  Have any ideas?","How to count the number of $n\times n$ matrix over integer modulo $p$ field with determinant equal $1$? I know that the number of invertible matrices is GL$(n,p)$.  Have any ideas?",,"['linear-algebra', 'matrices', 'ring-theory', 'finite-groups', 'determinant']"
60,"Are all inner products in $\mathbb R^n$ of the form $\langle X,Y\rangle=\sum_i c_i x_i y_i$?",Are all inner products in  of the form ?,"\mathbb R^n \langle X,Y\rangle=\sum_i c_i x_i y_i","I know that in $\mathbb{R}^n$ (and in general $F^n$) you can define an Inner Product in the following way: Let $X,Y \in \mathbb{R}^n$.  Let $C \in \mathbb{R}^n$, and let all the components of $C$ be positive. Then: $\langle 'X,Y \rangle = \sum_{i=1}^n c_ix_iy_i$ is an Inner Product. I was wondering  if this is the only type of Inner Product in $\mathbb{R}^n$? Can you define an inner product in $\mathbb{R}^n$ that doesn't take this form? Thanks.","I know that in $\mathbb{R}^n$ (and in general $F^n$) you can define an Inner Product in the following way: Let $X,Y \in \mathbb{R}^n$.  Let $C \in \mathbb{R}^n$, and let all the components of $C$ be positive. Then: $\langle 'X,Y \rangle = \sum_{i=1}^n c_ix_iy_i$ is an Inner Product. I was wondering  if this is the only type of Inner Product in $\mathbb{R}^n$? Can you define an inner product in $\mathbb{R}^n$ that doesn't take this form? Thanks.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
61,Inverse of Cartan matrix,Inverse of Cartan matrix,,"The Cartan matrix of the root system $A_n$ looks like, denote it by $A'_n$ $$A'_n= \begin{bmatrix}        2 & -1 & 0 & 0&\ldots & 0          \\[0.3em]        -1 & 2 & -1 & 0 & \ldots & 0 \\[0.3em]        0 &-1 &2  & -1 & \ldots & 0   \\[0.3em]        \vdots & & &\ddots & 2& -1  \\[0.3em] 0 & \ldots & & & -1&2      \end{bmatrix}$$ I need to find the inverse matrix explicitly. I know that $det A'_n=n+1$, using this and the fact that $$b_{ij}:=(A'_n)_{ij}^{-1}=(-1)^{i+j}\frac{\text{det}(A''_n)_{ij}}{\text{det}(A'_n)_{ij}}$$(Where $(A''_n)_{ij}$ is the matrix $A'_n$ with i-th row and j-th column removed) For the first row I could find that $$ b_{1k} =  \frac{n-k+1}{n+1} $$ But this nice pattern does not continue for other rows.... Is there an easier way to approach this problem ? I would appreciate any hints/suggestions/answers for this problem. Thank you,","The Cartan matrix of the root system $A_n$ looks like, denote it by $A'_n$ $$A'_n= \begin{bmatrix}        2 & -1 & 0 & 0&\ldots & 0          \\[0.3em]        -1 & 2 & -1 & 0 & \ldots & 0 \\[0.3em]        0 &-1 &2  & -1 & \ldots & 0   \\[0.3em]        \vdots & & &\ddots & 2& -1  \\[0.3em] 0 & \ldots & & & -1&2      \end{bmatrix}$$ I need to find the inverse matrix explicitly. I know that $det A'_n=n+1$, using this and the fact that $$b_{ij}:=(A'_n)_{ij}^{-1}=(-1)^{i+j}\frac{\text{det}(A''_n)_{ij}}{\text{det}(A'_n)_{ij}}$$(Where $(A''_n)_{ij}$ is the matrix $A'_n$ with i-th row and j-th column removed) For the first row I could find that $$ b_{1k} =  \frac{n-k+1}{n+1} $$ But this nice pattern does not continue for other rows.... Is there an easier way to approach this problem ? I would appreciate any hints/suggestions/answers for this problem. Thank you,",,"['linear-algebra', 'inverse', 'root-systems']"
62,Matrix $A=B+C$ with $B$ symmetric and $C$ antisymmetric,Matrix  with  symmetric and  antisymmetric,A=B+C B C,"I am stumped on a question and am looking for some guidance on how to get it done. The problem gives you: $x_1 = \begin{bmatrix}9&-4&-2 \\-9&6&-3 \\10&-3&9\end{bmatrix}$ $x_1 = x_2 + x_3$. $x_2$ is a symmetric matrix and $x_3$ is an antisymmetric matrix. That is that part that confuses me. The only thing Ive gotten is that the $x_2$ has the $9,6,9$ on the diagonal and $x_3$ has $0$ for its diagonal. Can anyone help me down the right path. Thanks","I am stumped on a question and am looking for some guidance on how to get it done. The problem gives you: $x_1 = \begin{bmatrix}9&-4&-2 \\-9&6&-3 \\10&-3&9\end{bmatrix}$ $x_1 = x_2 + x_3$. $x_2$ is a symmetric matrix and $x_3$ is an antisymmetric matrix. That is that part that confuses me. The only thing Ive gotten is that the $x_2$ has the $9,6,9$ on the diagonal and $x_3$ has $0$ for its diagonal. Can anyone help me down the right path. Thanks",,"['linear-algebra', 'matrices', 'symmetry']"
63,Inverse of diagonalizable matrix is diagonalizable,Inverse of diagonalizable matrix is diagonalizable,,"Let $A \in M_n(\mathbb C)$ be invertible. Prove that $A$ is diagonalizable if and only if $A^{-1}$ is diagonalizable. This is what I have for one direction of the proof: Suppose $A$ is diagonalizable.  Then there exists a diagonal matrix $D \in M_n(\mathbb C)$ and an invertible matrix $S \in M_n(\mathbb C)$ such that $A=SDS^{-1}$.  So $$A=SDS^{-1}$$ $$A^{-1}A=A^{-1}SDS^{-1}$$ $$I_n=A^{-1}SDS^{-1}$$ $$S=A^{-1}SD$$ $$*SD^{-1}=A^{-1}S*$$ $$SD^{-1}S^{-1}=A^{-1}$$ In my deduction above, I assumed that $D$ is invertible. I know this is the case. $D$ is the diagonal matrix with entries that are just the eigenvalues of $A$. Since $A$ is invertible, $\lambda \neq 0$, so $det(D) \neq 0$ and therefore $D$ is invertible. But how can I show that the entries of $D$ are just the eigenvalues of $A$? I already proved that for an invertible matrix $A$, $\lambda$ is an eigenvalue of $A$ if and only if $1/ \lambda$ is an eigenvalue of $A^{-1}$. Can I use this to somehow prove that the entries in $D$ are the eigenvalues of $A$?","Let $A \in M_n(\mathbb C)$ be invertible. Prove that $A$ is diagonalizable if and only if $A^{-1}$ is diagonalizable. This is what I have for one direction of the proof: Suppose $A$ is diagonalizable.  Then there exists a diagonal matrix $D \in M_n(\mathbb C)$ and an invertible matrix $S \in M_n(\mathbb C)$ such that $A=SDS^{-1}$.  So $$A=SDS^{-1}$$ $$A^{-1}A=A^{-1}SDS^{-1}$$ $$I_n=A^{-1}SDS^{-1}$$ $$S=A^{-1}SD$$ $$*SD^{-1}=A^{-1}S*$$ $$SD^{-1}S^{-1}=A^{-1}$$ In my deduction above, I assumed that $D$ is invertible. I know this is the case. $D$ is the diagonal matrix with entries that are just the eigenvalues of $A$. Since $A$ is invertible, $\lambda \neq 0$, so $det(D) \neq 0$ and therefore $D$ is invertible. But how can I show that the entries of $D$ are just the eigenvalues of $A$? I already proved that for an invertible matrix $A$, $\lambda$ is an eigenvalue of $A$ if and only if $1/ \lambda$ is an eigenvalue of $A^{-1}$. Can I use this to somehow prove that the entries in $D$ are the eigenvalues of $A$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
64,Non-degenerate symmetric bilinear form; dimension formulae.,Non-degenerate symmetric bilinear form; dimension formulae.,,"Let $E$ be a vector space endowed with a non-degenerate symmetric bilinear form. Show $\dim F+\dim F^{\perp}=\dim E=\dim\left(F+F^{\perp}\right)+\dim\left(F\cap F^{\perp}\right)$ Lang uses this formula in proposition 1.2, page 573 of his book ""Algebra"" (Graduate). I did not find a straight forward argument to show it, the only argument I found was really long, and not very enlightening. But I assume if he stated it without proof Here is what I want to show: Let $E$ be a finite-dimensional vector space over a field K. Let $g$ be a symmetric/alternating/hermitian form (it suffices to me if you prove it for symmetric and hopefuly I can go from there). Assume g is non-degenerate. Let F be a subspace of E. I want to show the following: $ \dim F+\dim F^{\perp}=\dim E=\dim\left(F+F^{\perp}\right)+\dim\left(F\cap F^{\perp}\right) $ It may be that not all hypothesis are necessary, he just uses this formula in a proof but doesn't derive it. Thanx to anyone who helps! cheers","Let $E$ be a vector space endowed with a non-degenerate symmetric bilinear form. Show $\dim F+\dim F^{\perp}=\dim E=\dim\left(F+F^{\perp}\right)+\dim\left(F\cap F^{\perp}\right)$ Lang uses this formula in proposition 1.2, page 573 of his book ""Algebra"" (Graduate). I did not find a straight forward argument to show it, the only argument I found was really long, and not very enlightening. But I assume if he stated it without proof Here is what I want to show: Let $E$ be a finite-dimensional vector space over a field K. Let $g$ be a symmetric/alternating/hermitian form (it suffices to me if you prove it for symmetric and hopefuly I can go from there). Assume g is non-degenerate. Let F be a subspace of E. I want to show the following: $ \dim F+\dim F^{\perp}=\dim E=\dim\left(F+F^{\perp}\right)+\dim\left(F\cap F^{\perp}\right) $ It may be that not all hypothesis are necessary, he just uses this formula in a proof but doesn't derive it. Thanx to anyone who helps! cheers",,"['linear-algebra', 'vector-spaces', 'bilinear-form']"
65,Number of elementary multiplications for multiplying 4x4 matrices,Number of elementary multiplications for multiplying 4x4 matrices,,Applying recursively Strassen's algorithm on 4x4 matrices results in 49 elementary multiplications. Are there methods tailored for 4x4 matrices which can do better? Links to articles are highly appreciated.,Applying recursively Strassen's algorithm on 4x4 matrices results in 49 elementary multiplications. Are there methods tailored for 4x4 matrices which can do better? Links to articles are highly appreciated.,,"['linear-algebra', 'matrices']"
66,Compute Left Eigenvectors,Compute Left Eigenvectors,,"How does one compute the left eigenvectors of a matrix? I cannot seem to quite get the answer.. I don't care what the matrix is. Let's just say I have matrix $A$ and have found the 'right' eigenvectors $e$ and I want to compute the left eigenvectors. Do we $A^Te = c$ Then the left eigenvector is $c^T$? I did this for a 2x2 and it seemed to work, but for a 3x3 it did not (according to the answers in the book). Thanks in advance!","How does one compute the left eigenvectors of a matrix? I cannot seem to quite get the answer.. I don't care what the matrix is. Let's just say I have matrix $A$ and have found the 'right' eigenvectors $e$ and I want to compute the left eigenvectors. Do we $A^Te = c$ Then the left eigenvector is $c^T$? I did this for a 2x2 and it seemed to work, but for a 3x3 it did not (according to the answers in the book). Thanks in advance!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
67,non linear transformation that satisfies $T(cx) = cT(x)$,non linear transformation that satisfies,T(cx) = cT(x),"I am just curious if there is a transformation that does not satisfy  $\;T(x+y) = T(x) + T(y),\;$  but satisfies $\;T(cx)=cT(x).\;$ I cannot think of any. Thanks for any help people.","I am just curious if there is a transformation that does not satisfy  $\;T(x+y) = T(x) + T(y),\;$  but satisfies $\;T(cx)=cT(x).\;$ I cannot think of any. Thanks for any help people.",,"['linear-algebra', 'transformation']"
68,$T$ is normal if and only if exist polynomial $p$ s.t $T^{*}=p(T)$,is normal if and only if exist polynomial  s.t,T p T^{*}=p(T),"Let $V$ be an inner product space over $\mathbb{C}$, and let $T:v\to V$ be a linear transformation. Need proving that $T$ is normal if and only if  exist polynomial $p\in\mathbb{C}[x]$ s.t $T^{*}=p(T)$. Thanks!","Let $V$ be an inner product space over $\mathbb{C}$, and let $T:v\to V$ be a linear transformation. Need proving that $T$ is normal if and only if  exist polynomial $p\in\mathbb{C}[x]$ s.t $T^{*}=p(T)$. Thanks!",,['linear-algebra']
69,Adjoint matrix eigenvalues and eigenvectors,Adjoint matrix eigenvalues and eigenvectors,,"I just wanted to make sure that the following statement is true: Let $A$ be a normal matrix with eigenvalues $\lambda_1,...,\lambda_n$ and eigenvectors $v_1,...,v_n$. Then $A^*$ has the same eigenvectors with eigenvalues $\bar{\lambda_1},..,\bar{\lambda_n}$, correct?","I just wanted to make sure that the following statement is true: Let $A$ be a normal matrix with eigenvalues $\lambda_1,...,\lambda_n$ and eigenvectors $v_1,...,v_n$. Then $A^*$ has the same eigenvectors with eigenvalues $\bar{\lambda_1},..,\bar{\lambda_n}$, correct?",,"['linear-algebra', 'matrices']"
70,$A$ and $B$ are different matrices satisfying $A^3=B^3$ and $A^2B=B^2A$,and  are different matrices satisfying  and,A B A^3=B^3 A^2B=B^2A,I found the following problem interesting but do not know how to tackle it. If $A$ and $B$ are different  matrices satisfying $A^3=B^3$ and $A^2B=B^2A$.Then find $\det (A^2+B^2)=?.$ Can someone point me in the right direction?,I found the following problem interesting but do not know how to tackle it. If $A$ and $B$ are different  matrices satisfying $A^3=B^3$ and $A^2B=B^2A$.Then find $\det (A^2+B^2)=?.$ Can someone point me in the right direction?,,['linear-algebra']
71,Sum of two quadratic forms,Sum of two quadratic forms,,"Suppose I have two quadratic forms $Q_i(x)=(x-a_i)^T A_i(x-a_i)+c_i$, $i=1,2$ where $x,a_i \in \Bbb{R}^n$ and $A_i$ are positive-definite $n\times n$ matrices. Then $Q(x)=Q_1(x)+Q_2(x)$ is also a quadratic form, $Q(x)=(x-a)^T A(x-a)+c$, with $A=A_1+A_2$ (easy to see by considering just the quadratic terms). How do I find $a$ and, especially, $c$?","Suppose I have two quadratic forms $Q_i(x)=(x-a_i)^T A_i(x-a_i)+c_i$, $i=1,2$ where $x,a_i \in \Bbb{R}^n$ and $A_i$ are positive-definite $n\times n$ matrices. Then $Q(x)=Q_1(x)+Q_2(x)$ is also a quadratic form, $Q(x)=(x-a)^T A(x-a)+c$, with $A=A_1+A_2$ (easy to see by considering just the quadratic terms). How do I find $a$ and, especially, $c$?",,"['linear-algebra', 'quadratic-forms']"
72,"If $v$ is an eigenvalue of $A$ and $c$ is an eigenvalue of $B$, must $vc$ be an eigenvalue of $AB$?","If  is an eigenvalue of  and  is an eigenvalue of , must  be an eigenvalue of ?",v A c B vc AB,Let $v$ be an eigenvalue of $A$ and $c$ be an eigenvalue of $B$. Is the product of $v$ and $c$ equal to an eigenvalue of $AB$?,Let $v$ be an eigenvalue of $A$ and $c$ be an eigenvalue of $B$. Is the product of $v$ and $c$ equal to an eigenvalue of $AB$?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
73,Does there exist a unique definition of dot product in $\mathbb R^n$ such that the standard basis is orthonormal?,Does there exist a unique definition of dot product in  such that the standard basis is orthonormal?,\mathbb R^n,"Does there exist a unique definition of scalar product in $\mathbb R^n$ so that the standard basis is orthonormal? I can't find a definition of dot product different from the usual definition. Is this true or false?. Thanks! Edit Let $\langle ,\rangle_1$ y $\langle ,\rangle_2$ be two different dot products such that the standard basis is orthonormal with those dot products. Both dot products are associated to Gram matrices $G, G'$ respectively, but since they make the standard basis orthonormal,   necessarily $G = G' = I_n$. Hence dot products are the same since given two vectores $x,y\in\mathbb R^n$ we have that $x^t G   y = x^t G' y = x^t y $. Sorry for my english, it is terrible. Is this valid? Thanks again!","Does there exist a unique definition of scalar product in $\mathbb R^n$ so that the standard basis is orthonormal? I can't find a definition of dot product different from the usual definition. Is this true or false?. Thanks! Edit Let $\langle ,\rangle_1$ y $\langle ,\rangle_2$ be two different dot products such that the standard basis is orthonormal with those dot products. Both dot products are associated to Gram matrices $G, G'$ respectively, but since they make the standard basis orthonormal,   necessarily $G = G' = I_n$. Hence dot products are the same since given two vectores $x,y\in\mathbb R^n$ we have that $x^t G   y = x^t G' y = x^t y $. Sorry for my english, it is terrible. Is this valid? Thanks again!",,"['linear-algebra', 'vector-spaces']"
74,Dual Spaces and Natural maps,Dual Spaces and Natural maps,,"(I'll explain what I know first and then I'll ask the questions). Given a finite dimensional vector space $V$, it is often remarked that there is no ""natural"" isomorphism from $V$ to $V^*$ (I guess this means a basis independent isomorphism?). I understand that one typically constructs an isomorphism $V \to V^*$ by fixing a basis for $V$, call this $B = \{v_1, v_2, \ldots, v_n\}$, and then mapping $v_i \mapsto \delta_i$ where $\delta_i$ is the linear functional given by $\delta_i(v_i) = \delta_{ij}$. Here are my questions: (1) Why is there no natural isomorphism $V \to V^*$? (2) If $V$ is a finite dimensional inner product space, we can map each $v \mapsto \langle v, \cdot \rangle$. Is this not ""natural""? (I know we can consider sesquilinear forms, but let's keep the discussion to this inner product for simplicity).","(I'll explain what I know first and then I'll ask the questions). Given a finite dimensional vector space $V$, it is often remarked that there is no ""natural"" isomorphism from $V$ to $V^*$ (I guess this means a basis independent isomorphism?). I understand that one typically constructs an isomorphism $V \to V^*$ by fixing a basis for $V$, call this $B = \{v_1, v_2, \ldots, v_n\}$, and then mapping $v_i \mapsto \delta_i$ where $\delta_i$ is the linear functional given by $\delta_i(v_i) = \delta_{ij}$. Here are my questions: (1) Why is there no natural isomorphism $V \to V^*$? (2) If $V$ is a finite dimensional inner product space, we can map each $v \mapsto \langle v, \cdot \rangle$. Is this not ""natural""? (I know we can consider sesquilinear forms, but let's keep the discussion to this inner product for simplicity).",,"['linear-algebra', 'abstract-algebra']"
75,Show that $\alpha_1u+\alpha_2v+\alpha_3w=0\Rightarrow\alpha_1=\alpha_2=\alpha_3=0$,Show that,\alpha_1u+\alpha_2v+\alpha_3w=0\Rightarrow\alpha_1=\alpha_2=\alpha_3=0,"Let $u, v, w$ be three points in $\mathbb R^3$ not lying in any plane containing the origin. Would you help me to prove or disprove : $\alpha_1u+\alpha_2v+\alpha_3w=0\Rightarrow\alpha_1=\alpha_2=\alpha_3=0.$ I think this is wrong since otherwise Rank of the Coefficient Matrix have to be 3. But for$u_1=(1,1,0),u_2=(1,2,0),u_3=(1,3,0)$, ( Rank of the corresponding Coefficient Matrix )$\neq 3$. Am I right?","Let $u, v, w$ be three points in $\mathbb R^3$ not lying in any plane containing the origin. Would you help me to prove or disprove : $\alpha_1u+\alpha_2v+\alpha_3w=0\Rightarrow\alpha_1=\alpha_2=\alpha_3=0.$ I think this is wrong since otherwise Rank of the Coefficient Matrix have to be 3. But for$u_1=(1,1,0),u_2=(1,2,0),u_3=(1,3,0)$, ( Rank of the corresponding Coefficient Matrix )$\neq 3$. Am I right?",,['linear-algebra']
76,"Lower bound for $\|A-B\|$ when $\operatorname{rank}(A)\neq \operatorname{rank}(B)$, both $A$ and $B$ are idempotent","Lower bound for  when , both  and  are idempotent",\|A-B\| \operatorname{rank}(A)\neq \operatorname{rank}(B) A B,"Let's first focus on $k$-by-$k$ matrices. We know that rank is a continuous function for idempotent matrices, so when we have, say, $\operatorname{rank}(A)>\operatorname{rank}(B)+1$, the two matrices cannot be close in norm topology. But I wonder whether there is an explicit lower bound of the distance between two idempotent matrices in terms of their difference in their ranks. Thanks!","Let's first focus on $k$-by-$k$ matrices. We know that rank is a continuous function for idempotent matrices, so when we have, say, $\operatorname{rank}(A)>\operatorname{rank}(B)+1$, the two matrices cannot be close in norm topology. But I wonder whether there is an explicit lower bound of the distance between two idempotent matrices in terms of their difference in their ranks. Thanks!",,"['linear-algebra', 'operator-theory', 'operator-algebras']"
77,"Is the set of all $n\times n$ matrices, such that for a fixed matrix B  AB=BA, a subspace of the vector space of all $n\times n$ matrices?","Is the set of all  matrices, such that for a fixed matrix B  AB=BA, a subspace of the vector space of all  matrices?",n\times n n\times n,"Is the set of all $n\times n$ matrices, such that for any fixed matrix B AB=BA, a subspace of the vector space of all $n\times n$ matrices? Alright, I understand the question and I know what I have to do, basically. I need to show that additive closure and multiplicative closure are satisfied. The problem is, I can't figure out how to do this generally. I tried playing around with $2\times 2$ matrices but that seemed like a dead end. Obviously two such matrices are the 0 matrix and the identity matrix, and those form a subspace, but that doesn't really tell me about all the matrices. Any ideas for how I should be tackling this? I feel like I'm not thinking generally enough.","Is the set of all $n\times n$ matrices, such that for any fixed matrix B AB=BA, a subspace of the vector space of all $n\times n$ matrices? Alright, I understand the question and I know what I have to do, basically. I need to show that additive closure and multiplicative closure are satisfied. The problem is, I can't figure out how to do this generally. I tried playing around with $2\times 2$ matrices but that seemed like a dead end. Obviously two such matrices are the 0 matrix and the identity matrix, and those form a subspace, but that doesn't really tell me about all the matrices. Any ideas for how I should be tackling this? I feel like I'm not thinking generally enough.",,"['linear-algebra', 'vector-spaces']"
78,Is there a simple argument for why a random symmetric matrix has distinct eigenvalues?,Is there a simple argument for why a random symmetric matrix has distinct eigenvalues?,,"Lets generate a random symmetric matrix $A$ by generating the entries of a random matrix $Z$ iid from some continuous distribution, and setting $A=(1/2)(Z+Z^T)$. I think its true that $A$ should have distinct eigenvalues with probability $1$, and it seems like there should be a really simple argument to this effect, but I am not seeing it.","Lets generate a random symmetric matrix $A$ by generating the entries of a random matrix $Z$ iid from some continuous distribution, and setting $A=(1/2)(Z+Z^T)$. I think its true that $A$ should have distinct eigenvalues with probability $1$, and it seems like there should be a really simple argument to this effect, but I am not seeing it.",,"['linear-algebra', 'probability-theory', 'random-matrices']"
79,How can I find equivalent Euler angles?,How can I find equivalent Euler angles?,,"I have a rotation over time represented as a series of Euler angles (heading, pitch and bank).  I'd like to represent the individual rotation curves as continuously as possible.  An anyone help me with how I would calculate the equivalent Euler angle representations for a single rotation, from where I could derive the ""closest"" equivalent to the prior rotation in time? Thanks! I realize that half the problem may be my inability to properly express the problem in the first place.  Perhaps a concrete example might make it a bit clearer.  I have two rotations around a common $xyz$ axis expressed in degrees as Euler angles: $(-224.782, 265, 214.25)$ and $(-44.782, -85, 34.215)$.  These produce an equivalent orientation.  Supposing that I started with the latter, which is ""normalized"" to the range to $-180 \leq x \leq 180$, $-90\leq y\leq 90$, $-180 \leq z \leq 180$, how would I arrive at the former? Apologies for the layman explanation.","I have a rotation over time represented as a series of Euler angles (heading, pitch and bank).  I'd like to represent the individual rotation curves as continuously as possible.  An anyone help me with how I would calculate the equivalent Euler angle representations for a single rotation, from where I could derive the ""closest"" equivalent to the prior rotation in time? Thanks! I realize that half the problem may be my inability to properly express the problem in the first place.  Perhaps a concrete example might make it a bit clearer.  I have two rotations around a common $xyz$ axis expressed in degrees as Euler angles: $(-224.782, 265, 214.25)$ and $(-44.782, -85, 34.215)$.  These produce an equivalent orientation.  Supposing that I started with the latter, which is ""normalized"" to the range to $-180 \leq x \leq 180$, $-90\leq y\leq 90$, $-180 \leq z \leq 180$, how would I arrive at the former? Apologies for the layman explanation.",,"['linear-algebra', 'geometry']"
80,Sum of squared maximization with a norm constraint,Sum of squared maximization with a norm constraint,,"I have the following optimization \begin{align} \max_{\|x\|^2\le1} \|Lx - y\| \end{align} where $L$ is a lower triangular and $y$ is a given vector. Does it admit a closed-form solution? I am interested in the general case where $L$ may be not invertible and even not square, but any progress under some assumptions will be muchnappreciated. The motivation is to understand the standard least squares method but as a game from the ""adversarial"" point of view: $y$ is a measurement sequence and $x$ is the set of possible states from $y = Lx + n$ .","I have the following optimization where is a lower triangular and is a given vector. Does it admit a closed-form solution? I am interested in the general case where may be not invertible and even not square, but any progress under some assumptions will be muchnappreciated. The motivation is to understand the standard least squares method but as a game from the ""adversarial"" point of view: is a measurement sequence and is the set of possible states from .","\begin{align}
\max_{\|x\|^2\le1} \|Lx - y\|
\end{align} L y L y x y = Lx + n","['linear-algebra', 'optimization', 'convex-optimization', 'least-squares']"
81,"Is $A - AB(B + BAB)^+ BA = A(A + ABA)^+A$ true for positive semidefinite $A, B$?",Is  true for positive semidefinite ?,"A - AB(B + BAB)^+ BA = A(A + ABA)^+A A, B","From extensive numerical simulation it seems that the following identity holds for two symmetric positive semidefinite matrices: $$ A - AB(B + BAB)^+ BA = A(A + ABA)^+A. $$ I tried to prove this, and was able to verify it in the case $A, B$ are rank one. However, as mentioned above, it holds at least in simulation for general positive semidefinite matrices. Is there an easy proof? Here $P^+$ is the generalized inverse of $P$ .","From extensive numerical simulation it seems that the following identity holds for two symmetric positive semidefinite matrices: I tried to prove this, and was able to verify it in the case are rank one. However, as mentioned above, it holds at least in simulation for general positive semidefinite matrices. Is there an easy proof? Here is the generalized inverse of .","
A - AB(B + BAB)^+ BA = A(A + ABA)^+A.
 A, B P^+ P","['linear-algebra', 'matrices', 'positive-semidefinite', 'pseudoinverse']"
82,"Is the ""Its transpose is its inverse"" definition of an orthogonal matrix equivalent to the ""It preserves the dot product"" definition?","Is the ""Its transpose is its inverse"" definition of an orthogonal matrix equivalent to the ""It preserves the dot product"" definition?",,"A long time ago, I was taught that a real $n\times n$ matrix $A$ is called orthogonal if $AA^t=I$ . But recently I learned from a DG book that $A$ is said to be orthogonal if it preserves the dot product: $$(Ax)\cdot(Ay)=x\cdot y\quad\text{for all }x,y\in\mathbb{R}^n.$$ Are these two definitions equivalent? It is easy to see that the former implies the latter: $$(Ax)\cdot(Ay)=(Ay)^t(Ax)=(y^t A^t)(Ax)=y^t x=x\cdot y$$ But I have a hard time going from the latter to the former. Is it possible? Thank you for your time.","A long time ago, I was taught that a real matrix is called orthogonal if . But recently I learned from a DG book that is said to be orthogonal if it preserves the dot product: Are these two definitions equivalent? It is easy to see that the former implies the latter: But I have a hard time going from the latter to the former. Is it possible? Thank you for your time.","n\times n A AA^t=I A (Ax)\cdot(Ay)=x\cdot y\quad\text{for all }x,y\in\mathbb{R}^n. (Ax)\cdot(Ay)=(Ay)^t(Ax)=(y^t A^t)(Ax)=y^t x=x\cdot y","['linear-algebra', 'matrices', 'orthogonal-matrices']"
83,"Can a ""dimension reducing"" function be bijective?","Can a ""dimension reducing"" function be bijective?",,"Can a function whose image has lower dimensionality than its domain (alas I don't know if there is a special name for that kind of functions) ever be bijective? Consider e. g. $$f:X\to Y:\begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} \to\begin{pmatrix} y_1\\y_2 \end{pmatrix}=\begin{pmatrix} x_1 +x_2\\x_2+x_3 \end{pmatrix}$$ which is a function from $\Bbb{R}^3$ into $\Bbb{R}^2$ and is obviously not bijective. My problem is: On the one hand, I think it should be possible to have a bijective function from a ""cube"" into a ""plane"", on the other, it's hard to imagine because it effectively means calculating one result variable from multiple input variables, which would imply that the same image element can always result from different domain elements, thus rendering bijectivity impossible.","Can a function whose image has lower dimensionality than its domain (alas I don't know if there is a special name for that kind of functions) ever be bijective? Consider e. g. which is a function from into and is obviously not bijective. My problem is: On the one hand, I think it should be possible to have a bijective function from a ""cube"" into a ""plane"", on the other, it's hard to imagine because it effectively means calculating one result variable from multiple input variables, which would imply that the same image element can always result from different domain elements, thus rendering bijectivity impossible.",f:X\to Y:\begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} \to\begin{pmatrix} y_1\\y_2 \end{pmatrix}=\begin{pmatrix} x_1 +x_2\\x_2+x_3 \end{pmatrix} \Bbb{R}^3 \Bbb{R}^2,['linear-algebra']
84,Prove $(AB)^{k} = A^{k}B^{k}$,Prove,(AB)^{k} = A^{k}B^{k},"Apologies if this question was posed already. I need to prove that $(AB)^{k} = A^{k}B^{k}$ holds if $AB=BA$ . After trying it myself, I looked at the solution and found it quite strange. ""Use induction: $(AB)^{k} = A^{k}B^{k}$ is true for $k=1$ since since $AB=BA$ . Assume $(AB)^{k-1}=(BA)^{k-1}$ and prove it true for k: $$ (AB)^{k} = (AB)^{k-1} AB $$ $$  = A^{k-1}B^{k-1} AB $$ $$  = A^{k-1}A B^{k-1} B $$ $$  = A^{k} B^{k} $$ "" I don't understand how this really proves anything. We had to assume $(AB)^{k-1}=(BA)^{k-1}$ , how does if follow then that $(AB)^{k} = A^{k}B^{k}$ holds?","Apologies if this question was posed already. I need to prove that holds if . After trying it myself, I looked at the solution and found it quite strange. ""Use induction: is true for since since . Assume and prove it true for k: "" I don't understand how this really proves anything. We had to assume , how does if follow then that holds?",(AB)^{k} = A^{k}B^{k} AB=BA (AB)^{k} = A^{k}B^{k} k=1 AB=BA (AB)^{k-1}=(BA)^{k-1}  (AB)^{k} = (AB)^{k-1} AB    = A^{k-1}B^{k-1} AB    = A^{k-1}A B^{k-1} B    = A^{k} B^{k}  (AB)^{k-1}=(BA)^{k-1} (AB)^{k} = A^{k}B^{k},"['linear-algebra', 'matrices', 'exponentiation']"
85,"Solve the system of equations: $32y+32x^3=6x+17$, $16z+32y^3=6y+9$, $8x+32z^3=6z+5$ where $x,y,z\in \mathbb{R}$","Solve the system of equations: , ,  where","32y+32x^3=6x+17 16z+32y^3=6y+9 8x+32z^3=6z+5 x,y,z\in \mathbb{R}","Solve the system of equations: $$\begin{cases} 32y+32x^3=6x+17 \\16z+32y^3=6y+9 \\8x+32z^3=6z+5 \end{cases}$$ where $x,y,z\in \mathbb{R}$ (Bulgaria 1960) I attempted to solve this question as follows: $32y+32x^3=6x+17$ $y=-x^3+\frac{6x}{32}+\frac{17}{32}$ $y-\frac{1}{2}=-x^3+\frac{3x}{16}+\frac{17}{32}-\frac{1}{2}$ Here it started getting very complex, and hence I don't think it can be solved this way. After this I tried doing something similar with the other two equations, but once again it was ending up way too complex. It is obvious that the solution is $x=y=z=\frac{1}{2}$ , but I can't manage to prove it. Could you please explain to me how to solve this question?","Solve the system of equations: where (Bulgaria 1960) I attempted to solve this question as follows: Here it started getting very complex, and hence I don't think it can be solved this way. After this I tried doing something similar with the other two equations, but once again it was ending up way too complex. It is obvious that the solution is , but I can't manage to prove it. Could you please explain to me how to solve this question?","\begin{cases} 32y+32x^3=6x+17 \\16z+32y^3=6y+9 \\8x+32z^3=6z+5 \end{cases} x,y,z\in \mathbb{R} 32y+32x^3=6x+17 y=-x^3+\frac{6x}{32}+\frac{17}{32} y-\frac{1}{2}=-x^3+\frac{3x}{16}+\frac{17}{32}-\frac{1}{2} x=y=z=\frac{1}{2}","['linear-algebra', 'algebra-precalculus', 'contest-math', 'systems-of-equations', 'real-numbers']"
86,On the imaginary and real part of the eigenvalues of a real normal matrix.,On the imaginary and real part of the eigenvalues of a real normal matrix.,,"Let $A\in\mathbb{R}^{d\times d}$ be a real normal matrix . We can write $A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)$ . Can we show that the real part of $A$ 's eigenvalues are the eigenvalues of the symmetric $A+A^T$ and the imaginary part of $A$ 's eigenvalues are the eigenvalues of the skew symmetric $A-A^T$ ? It is claimed in another post that ""the real part of a normal matrix $A$ 's eigenvalues are $A+A^T$ "", however, there is no proof, and there is no claim on what happens with the imaginary part of the eigenvalues. Furthermore, it concerns complex normal matrices, I'm hoping the real case has a simpler proof.","Let be a real normal matrix . We can write . Can we show that the real part of 's eigenvalues are the eigenvalues of the symmetric and the imaginary part of 's eigenvalues are the eigenvalues of the skew symmetric ? It is claimed in another post that ""the real part of a normal matrix 's eigenvalues are "", however, there is no proof, and there is no claim on what happens with the imaginary part of the eigenvalues. Furthermore, it concerns complex normal matrices, I'm hoping the real case has a simpler proof.",A\in\mathbb{R}^{d\times d} A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T) A A+A^T A A-A^T A A+A^T,"['linear-algebra', 'complex-numbers', 'eigenvalues-eigenvectors']"
87,Determinant of matrix with constant lines apart diagonal,Determinant of matrix with constant lines apart diagonal,,"I would like to compute the determinant of a matrix with the following structure: \begin{equation} \begin{pmatrix} D_1 & l_1 & l_1 &\cdots & l_1 \\ l_2 & D_2 & l_2 &\cdots & l_2 \\ l_3 & \cdots & D_3  &\cdots & l_3 \\ l_4 & \cdots & l_4 & D_4  & l_4 \\ l_5 & \cdots & \cdots & l_5 & D_5  \\ \end{pmatrix} \end{equation} That is, it is constant on each line apart from the diagonal. $l_i, D_i \in \mathbb R^+$ . Is there a way to make use of such symmetric structure to simplify the calculation of the determinant?","I would like to compute the determinant of a matrix with the following structure: That is, it is constant on each line apart from the diagonal. . Is there a way to make use of such symmetric structure to simplify the calculation of the determinant?","\begin{equation}
\begin{pmatrix}
D_1 & l_1 & l_1 &\cdots & l_1 \\
l_2 & D_2 & l_2 &\cdots & l_2 \\
l_3 & \cdots & D_3  &\cdots & l_3 \\
l_4 & \cdots & l_4 & D_4  & l_4 \\
l_5 & \cdots & \cdots & l_5 & D_5  \\
\end{pmatrix}
\end{equation} l_i, D_i \in \mathbb R^+","['linear-algebra', 'matrices', 'determinant']"
88,If $A^2 = A$ then $A$ is diagonalizable,If  then  is diagonalizable,A^2 = A A,"I've stumbled upon this question in my assignment: Prove if $A_{nxn}(\mathbb C)$ with $A^2 = A$ , then $A$ is diagonalizable My first thought is to solve for $p(A)$ where $p(x)  = x^2 - x$ and you get real roots. Would that be sufficient in showing that $A$ is diagonalizable given you get real roots?","I've stumbled upon this question in my assignment: Prove if with , then is diagonalizable My first thought is to solve for where and you get real roots. Would that be sufficient in showing that is diagonalizable given you get real roots?",A_{nxn}(\mathbb C) A^2 = A A p(A) p(x)  = x^2 - x A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'idempotents']"
89,Maximum cardinality of a subspace of a vector space on a finite field,Maximum cardinality of a subspace of a vector space on a finite field,,Let $k$ be a field with five elements. Let $V$ be the $k$ -vector space of $5\times1$ matrices with entries in $k$ . Let $S$ be a subset of $V$ such that $u^t v=0$ for all $u$ and $v$ in $S$ . What is the maximum possible cardinality of $S$ ? The question seems to be asking the maximum number of mutually orthogonal vectors over $k$ . How do we compute it? Is it same as the number of orthogonal matrices over $k$ ? Thanks beforehand.,Let be a field with five elements. Let be the -vector space of matrices with entries in . Let be a subset of such that for all and in . What is the maximum possible cardinality of ? The question seems to be asking the maximum number of mutually orthogonal vectors over . How do we compute it? Is it same as the number of orthogonal matrices over ? Thanks beforehand.,k V k 5\times1 k S V u^t v=0 u v S S k k,"['linear-algebra', 'vector-spaces', 'finite-fields']"
90,Positive matrix with integer eigenvalues,Positive matrix with integer eigenvalues,,"Is there any way of creating a positive matrix which has integer eigenvalues? Each entry $a_{ij}$ of the matrix must be strictly greater than $0$ . I get how to create a matrix with certain eigenvalues using diagonal matrices, but I do not know how to make sure the matrix is strictly positive","Is there any way of creating a positive matrix which has integer eigenvalues? Each entry of the matrix must be strictly greater than . I get how to create a matrix with certain eigenvalues using diagonal matrices, but I do not know how to make sure the matrix is strictly positive",a_{ij} 0,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
91,Matrix with even integers entries doesn't have odd eigenvalue,Matrix with even integers entries doesn't have odd eigenvalue,,Let $A \in M_n(\mathbb{Z})$ with even entries. Prove that $A$ doesn't have odd eigenvalue.,Let with even entries. Prove that doesn't have odd eigenvalue.,A \in M_n(\mathbb{Z}) A,"['linear-algebra', 'eigenvalues-eigenvectors', 'integers']"
92,"If $A$ is $4\times2$ and $B$ is $2\times4$, prove that $AB$ is not invertible","If  is  and  is , prove that  is not invertible",A 4\times2 B 2\times4 AB,"My attempt : If $A$ is a matrix of size $4\times2$ and $B$ a matrix of size $2\times4$ . Then $AB$ is a matrix of size $4\times4$ . If $(AB)^{(-1)}$ exists then it would be of the form $$B^{-1}A^{-1}.$$ We see that $B^{-1}$ doesn't exist as the size of left inverse and right inverse will not be equal, so they are not equal.  Similarly $A^{-1}$ does not exist. Am I correct?","My attempt : If is a matrix of size and a matrix of size . Then is a matrix of size . If exists then it would be of the form We see that doesn't exist as the size of left inverse and right inverse will not be equal, so they are not equal.  Similarly does not exist. Am I correct?",A 4\times2 B 2\times4 AB 4\times4 (AB)^{(-1)} B^{-1}A^{-1}. B^{-1} A^{-1},"['linear-algebra', 'matrices', 'proof-verification']"
93,"1-Torus as finite dimensional $\mathbb{R}$-vector space is one dimensional, yet not isomorphic to $\mathbb{R}$","1-Torus as finite dimensional -vector space is one dimensional, yet not isomorphic to",\mathbb{R} \mathbb{R},"I know that the 1-torus, given by its presentation as rotation matrixes: $\mathbb{T}=\{R_{\theta}=\begin{pmatrix} cos(\theta) & -sin(\theta) \\ sin(\theta) & cos(\theta) \end{pmatrix}: \theta \in \mathbb{R}\}$ , forms a vector space over $\mathbb{R}$ , with the following addition and scalar multiplication. $R_{\theta}  \oplus R_{\alpha} = R_{\theta}R_{\alpha}=R_{\theta + \alpha}$ , this works because of sine and cosine sum of angles formulae. And scalar multiplication given by $r\odot R_{\theta} = R_{r\theta}$ . My doubt is the following, I'm quite sure given a finite dimensional vector space $V$ over a field $\mathbb{K}$ , we can say that: $$(1)\text{    }\text{    }\text{    }\text{    }\text{    }\text{    }V\cong \bigoplus_{j=1}^{dim(V)}\mathbb{K}$$ Where the isomorphism is given by mapping the scalar multiplying each basis element to it's own coordinate on the direct sum. The thing here, is that we have an epimorphism given by: $$\psi:\mathbb{R}\to \mathbb{T}$$ $${\theta}\mapsto R_{\theta}$$ This epimorphism has clearly a nontrivial kernel, given the periodicity of sine and cosine functions, and the space defined as $\mathbb{T}$ is clearly one dimensional. Also, no linear mapping between these spaces can be ever an isomorphism, given that $\mathbb{T}$ is compact, and every linear function is continuous on $\mathbb{R}$ . How come this isn't a contradiction, am i missing something and $\mathbb{T}$ isn't really a vector space? What I'm sure has to be true is, given a vector space $V$ , with an ordered basis $\mathcal{B}=\{b_{i}\}_{i=1}^{n}$ , and the mapping: $$\phi: \bigoplus_{j=1}^{n}\mathbb{K} \to V$$ $$(\lambda_{i})_{i=1}^{n} \mapsto  \sum\limits_{i=1}^{n} \lambda_{i}b_{i}$$ Is an epimorphism, so because of the first isomorphism theorem for modules, we can conclude: $$V\cong \bigoplus_{j=1}^{n} (\mathbb{K}/Ker(\phi_{i}))$$ Where $\phi_{i}:\mathbb{K}\to V$ , given by $\phi_{i}(\lambda)=\lambda b_{i}$ . Is this what I should think about when talking about finite dimensional vector spaces, or is indeed (1) true, and I'm missing something fundamental about the structure of $\mathbb{T}$ , making it NOT a vector space of finite dimension over $\mathbb{R}$ ?","I know that the 1-torus, given by its presentation as rotation matrixes: , forms a vector space over , with the following addition and scalar multiplication. , this works because of sine and cosine sum of angles formulae. And scalar multiplication given by . My doubt is the following, I'm quite sure given a finite dimensional vector space over a field , we can say that: Where the isomorphism is given by mapping the scalar multiplying each basis element to it's own coordinate on the direct sum. The thing here, is that we have an epimorphism given by: This epimorphism has clearly a nontrivial kernel, given the periodicity of sine and cosine functions, and the space defined as is clearly one dimensional. Also, no linear mapping between these spaces can be ever an isomorphism, given that is compact, and every linear function is continuous on . How come this isn't a contradiction, am i missing something and isn't really a vector space? What I'm sure has to be true is, given a vector space , with an ordered basis , and the mapping: Is an epimorphism, so because of the first isomorphism theorem for modules, we can conclude: Where , given by . Is this what I should think about when talking about finite dimensional vector spaces, or is indeed (1) true, and I'm missing something fundamental about the structure of , making it NOT a vector space of finite dimension over ?",\mathbb{T}=\{R_{\theta}=\begin{pmatrix} cos(\theta) & -sin(\theta) \\ sin(\theta) & cos(\theta) \end{pmatrix}: \theta \in \mathbb{R}\} \mathbb{R} R_{\theta}  \oplus R_{\alpha} = R_{\theta}R_{\alpha}=R_{\theta + \alpha} r\odot R_{\theta} = R_{r\theta} V \mathbb{K} (1)\text{    }\text{    }\text{    }\text{    }\text{    }\text{    }V\cong \bigoplus_{j=1}^{dim(V)}\mathbb{K} \psi:\mathbb{R}\to \mathbb{T} {\theta}\mapsto R_{\theta} \mathbb{T} \mathbb{T} \mathbb{R} \mathbb{T} V \mathcal{B}=\{b_{i}\}_{i=1}^{n} \phi: \bigoplus_{j=1}^{n}\mathbb{K} \to V (\lambda_{i})_{i=1}^{n} \mapsto  \sum\limits_{i=1}^{n} \lambda_{i}b_{i} V\cong \bigoplus_{j=1}^{n} (\mathbb{K}/Ker(\phi_{i})) \phi_{i}:\mathbb{K}\to V \phi_{i}(\lambda)=\lambda b_{i} \mathbb{T} \mathbb{R},"['linear-algebra', 'vector-spaces', 'vector-space-isomorphism']"
94,Is every eigenvector of AA an eigenvector of A?,Is every eigenvector of AA an eigenvector of A?,,"Let $V$ be a (finite-dimensional) vector space and $A \colon V \to V$ a linear map. Is it true that, if $v$ is an eigenvector of $A\circ A$ , then $v$ is an eigenvector of $A$ ? I know the converse statement is true.","Let be a (finite-dimensional) vector space and a linear map. Is it true that, if is an eigenvector of , then is an eigenvector of ? I know the converse statement is true.",V A \colon V \to V v A\circ A v A,"['linear-algebra', 'matrices']"
95,A matrix of order 8 over $\mathbb{F}_3$,A matrix of order 8 over,\mathbb{F}_3,"What is an example of an invertible matrix of size 2x2 with coefficients in $\mathbb{F}_3$ that has exact order 8? I have found by computation that the condition that the 8th power of a matrix $\begin{bmatrix}a & b\\c & d\end{bmatrix}$ is the identity is $$ b c (a + d)^2 (a^2 + 2 b c + d^2)^2 + ((a^2 + b c)^2 +     b c (a + d)^2)^2=1, \qquad b (a + d) (a^2 + 2 b c + d^2) (a^4 + 4 a^2 b c + 2 b^2 c^2 +     4 a b c d + 4 b c d^2 + d^4)=0, \qquad c (a + d) (a^2 + 2 b c + d^2) (a^4 + 4 a^2 b c + 2 b^2 c^2 +     4 a b c d + 4 b c d^2 + d^4)=0, \qquad b c (a + d)^2 (a^2 + 2 b c +      d^2)^2 + (b c (a + d)^2 + (b c + d^2)^2)^2=1 $$ and the condition for invertibility is $ad\neq bc$ . If the 4th power is not the identity, then no power that is not a multiple of 8 is not the identity (because we could cancel out to either get that the first power is the identity or that the second power is the identity, both lead to contradiction). That is another cumbersome condition to write out. I hope somebody can suggest a nicer way.","What is an example of an invertible matrix of size 2x2 with coefficients in that has exact order 8? I have found by computation that the condition that the 8th power of a matrix is the identity is and the condition for invertibility is . If the 4th power is not the identity, then no power that is not a multiple of 8 is not the identity (because we could cancel out to either get that the first power is the identity or that the second power is the identity, both lead to contradiction). That is another cumbersome condition to write out. I hope somebody can suggest a nicer way.","\mathbb{F}_3 \begin{bmatrix}a & b\\c & d\end{bmatrix} 
b c (a + d)^2 (a^2 + 2 b c + d^2)^2 + ((a^2 + b c)^2 + 
   b c (a + d)^2)^2=1, \qquad b (a + d) (a^2 + 2 b c + d^2) (a^4 + 4 a^2 b c + 2 b^2 c^2 + 
   4 a b c d + 4 b c d^2 + d^4)=0, \qquad c (a + d) (a^2 + 2 b c + d^2) (a^4 + 4 a^2 b c + 2 b^2 c^2 + 
   4 a b c d + 4 b c d^2 + d^4)=0, \qquad b c (a + d)^2 (a^2 + 2 b c + 
    d^2)^2 + (b c (a + d)^2 + (b c + d^2)^2)^2=1
 ad\neq bc",[]
96,"Is ""assignment"" a canonical term in math?","Is ""assignment"" a canonical term in math?",,"Wikipedia says : In vector calculus and physics, a vector field is an assignment of a   vector to each point in a subset of space. 1 A vector field in the   plane (for instance), can be visualised as: a collection of arrows   with a given magnitude and direction, each attached to a point in the   plane. Vector fields are often used to model, for example, the speed   and direction of a moving fluid throughout space, or the strength and   direction of some force, such as the magnetic or gravitational force,   as it changes from one point to another point. Is ""assignment"" a canonical term in math? What does ""assignment"" mean here?","Wikipedia says : In vector calculus and physics, a vector field is an assignment of a   vector to each point in a subset of space. 1 A vector field in the   plane (for instance), can be visualised as: a collection of arrows   with a given magnitude and direction, each attached to a point in the   plane. Vector fields are often used to model, for example, the speed   and direction of a moving fluid throughout space, or the strength and   direction of some force, such as the magnetic or gravitational force,   as it changes from one point to another point. Is ""assignment"" a canonical term in math? What does ""assignment"" mean here?",,"['linear-algebra', 'vector-spaces', 'terminology']"
97,An Example of a Torsion Module,An Example of a Torsion Module,,"I'm working on a problem (in a graduate Linear Algebra text) as follows. The author ask us to find a module $M$ which is finitely generated by torsion elements but $\text{Ann}(M)=\{0\}$ . I can't seem to find such an example. I did find module with finitely generated by its torsion elements but $\text{Ann}(M)\neq \{0\}$ , also examples of module with $\text{Ann}(M) = \{0\}$ but not finitely generated. What i had in mind is that the ring $R$ should not be an integral domain, otherwise $\text{ann}(M)\neq \{0\}$ . I quite new to this and i think this is the point where i need some help. Any help will be appreciated. Thanks.","I'm working on a problem (in a graduate Linear Algebra text) as follows. The author ask us to find a module which is finitely generated by torsion elements but . I can't seem to find such an example. I did find module with finitely generated by its torsion elements but , also examples of module with but not finitely generated. What i had in mind is that the ring should not be an integral domain, otherwise . I quite new to this and i think this is the point where i need some help. Any help will be appreciated. Thanks.",M \text{Ann}(M)=\{0\} \text{Ann}(M)\neq \{0\} \text{Ann}(M) = \{0\} R \text{ann}(M)\neq \{0\},"['linear-algebra', 'abstract-algebra', 'ring-theory', 'modules', 'examples-counterexamples']"
98,"Prove that if $f(x)=f(y)$ for all $f\in X^{*},$ then $x=y$",Prove that if  for all  then,"f(x)=f(y) f\in X^{*}, x=y","Can you check if my proof is correct? Let $X$ be a normed linear space. Prove that if $f(x)=f(y)$ for all $f\in X^{*},$ then $x=y$ Let $f\in X^{*}$ , then $f$ is a bounded linear functional. Assume that $x,\in X$ such that \begin{align}f(x)=f(y)&\iff f(x)-f(y)=0, \\& \iff f(x-y)=0, \;\text{since}\;f \;\text{is a linear functional}\;\\& \iff x-y\in \ker f =\{0\}\\& \iff x=y\end{align}","Can you check if my proof is correct? Let be a normed linear space. Prove that if for all then Let , then is a bounded linear functional. Assume that such that","X f(x)=f(y) f\in X^{*}, x=y f\in X^{*} f x,\in X \begin{align}f(x)=f(y)&\iff f(x)-f(y)=0, \\& \iff f(x-y)=0, \;\text{since}\;f \;\text{is a linear functional}\;\\& \iff x-y\in \ker f =\{0\}\\& \iff x=y\end{align}","['linear-algebra', 'functional-analysis', 'analysis']"
99,Example of 2 matrices similar but not row equivalent,Example of 2 matrices similar but not row equivalent,,"If two matrices are row equivalent, they may not be similar because all invertible matrices are row equivalent to $I$ , yet not all invertible matrices have the same trace, eigenvalues etc. Is it also true that if two matrices are similar, they may not be row equivalent? My instinct is that there is no reason that 2 similar matrices need to be row equivalent since having the same rank, eigenvalues, determinant etc does not necessarily make them row equivalent. Any suggestions as to how to find a counter-example? Thanks for help.","If two matrices are row equivalent, they may not be similar because all invertible matrices are row equivalent to , yet not all invertible matrices have the same trace, eigenvalues etc. Is it also true that if two matrices are similar, they may not be row equivalent? My instinct is that there is no reason that 2 similar matrices need to be row equivalent since having the same rank, eigenvalues, determinant etc does not necessarily make them row equivalent. Any suggestions as to how to find a counter-example? Thanks for help.",I,"['linear-algebra', 'matrices']"
