,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Doubt about the definition of limit in two variables,Doubt about the definition of limit in two variables,,"In this discussion Finding $\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^3+y}$ I found that we can consider paths that don't belogs to the domain of $f(x,y)$ to prove that a limit doesn't exist, but my teacher would not agree. I propose to you the definition of limit that I know. Let $f:\mbox{dom}(f)\subset\mathbb{R}^2\to\mathbb{R}$ and $(x_0, y_0)$ an accumulation point of $\mbox{dom}(f)$. We say that $$\lim_{(x,y)\to (x_0, y_0)}f(x,y)=L$$ if and only if $\forall\varepsilon>0, \ \exists\delta>0$ such that if $(x,y)\in \left(B_{\delta}(x_0,y_0)-\{(x_0,y_0)\}\right)\cap\mbox{dom}(f)$ than $|f(x,y)-L|<\varepsilon$ To show that a limit doesn't exist, I have to find two path $P_1(x,y), P_2(x,y)$ such that $$P_1(x,y), \ P_2(x,y)\in\mbox{dom}(f)\ \ \ \mbox{locally}$$ and $$\lim_{(x,y)\to (x_0, y_0)}P_1(x,y)=(x_0, y_0)\wedge \lim_{(x,y)\to (x_0, y_0)}P_2(x,y)=(x_0, y_0)$$ but $$\lim_{(x,y)\to (x_0, y_0)}f(P_1(x,y))=\ell_1\wedge \lim_{(x,y)\to (x_0,y_0)}f(P_2(x,y))=\ell_2$$ with $\ell_1\ne \ell_2$. In the discussion that i linked, I discovered that I can choose all possible path... but this is strange to me, and I'm now confused. Please help me to understand. Thank you.","In this discussion Finding $\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^3+y}$ I found that we can consider paths that don't belogs to the domain of $f(x,y)$ to prove that a limit doesn't exist, but my teacher would not agree. I propose to you the definition of limit that I know. Let $f:\mbox{dom}(f)\subset\mathbb{R}^2\to\mathbb{R}$ and $(x_0, y_0)$ an accumulation point of $\mbox{dom}(f)$. We say that $$\lim_{(x,y)\to (x_0, y_0)}f(x,y)=L$$ if and only if $\forall\varepsilon>0, \ \exists\delta>0$ such that if $(x,y)\in \left(B_{\delta}(x_0,y_0)-\{(x_0,y_0)\}\right)\cap\mbox{dom}(f)$ than $|f(x,y)-L|<\varepsilon$ To show that a limit doesn't exist, I have to find two path $P_1(x,y), P_2(x,y)$ such that $$P_1(x,y), \ P_2(x,y)\in\mbox{dom}(f)\ \ \ \mbox{locally}$$ and $$\lim_{(x,y)\to (x_0, y_0)}P_1(x,y)=(x_0, y_0)\wedge \lim_{(x,y)\to (x_0, y_0)}P_2(x,y)=(x_0, y_0)$$ but $$\lim_{(x,y)\to (x_0, y_0)}f(P_1(x,y))=\ell_1\wedge \lim_{(x,y)\to (x_0,y_0)}f(P_2(x,y))=\ell_2$$ with $\ell_1\ne \ell_2$. In the discussion that i linked, I discovered that I can choose all possible path... but this is strange to me, and I'm now confused. Please help me to understand. Thank you.",,"['calculus', 'real-analysis', 'limits']"
1,"Prob. 19, Chap. 5, in Baby Rudin: If $f$ is defined in $(-1, 1)$, $f^\prime(0)$ exists, $-1<\alpha_n<\beta_n<1$, and . . .","Prob. 19, Chap. 5, in Baby Rudin: If  is defined in ,  exists, , and . . .","f (-1, 1) f^\prime(0) -1<\alpha_n<\beta_n<1","Here is Prob. 19, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined in $(-1, 1)$ and $f^\prime(0)$ exists. Suppose $-1 < \alpha_n < \beta_n < 1$, $\alpha_n \to 0$, and $\beta \to 0$ as $n \to \infty$. Define the difference quotients $$ D_n = \frac{f\left(\beta_n\right)-f\left(\alpha_n\right)}{\beta_n-\alpha_n}.$$    Prove the following statements: (a) If $\alpha_n < 0 < \beta_n$, then $\lim D_n = f^\prime(0)$. (b) If $0 < \alpha_n < \beta_n$ and $\left\{ \beta_n / (\beta_n-\alpha_n) \right\}$ is bounded, then $\lim D_n = f^\prime(0)$. (c) If $f^\prime$ is continuous in $(-1, 1)$, then $\lim D_n = f^\prime(0)$. Give an example in which $f$ is differentiable in $(-1, 1)$ (but $f^\prime$ is not continuous at $0$) and in which $\alpha_n$, $\beta_n$ tend to $0$ in such a way that $\lim D_n$ exists but is different from $f^\prime(0)$. My Attempt: Part (a): Since $$f^\prime(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x-0}$$ exists, so, given any real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that    $$ \left| \frac{f(x) - f(0)}{x-0} - f^\prime(0) \right| < \frac{\varepsilon}{4} \tag{1} $$   for all $x \in (-1, 1)$ for which $0 < |x| < \delta$. Now as $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, so we can find natural numbers $M$ and $N$ such that    $$ \left| \alpha_n \right| < \delta \ \mbox{ for all } n > M,$$    and    $$ \left| \beta_n \right| < \delta  \ \mbox{ for all } n > N,$$    So    $$ \left| \alpha_n \right| < \delta  \ \mbox{ and } \left| \beta_n \right| < \delta \ \mbox{ for all } n > \max \left\{ M, N \right\}. \tag{2} $$ As $\alpha_n < 0 < \beta_n$, so we have $0 < - \alpha_n < \beta_n - \alpha_n$ and $ 0 < \beta_n < \beta_n - \alpha_n$, which imply    $$ 0 < \frac{\beta_n }{ \beta_n - \alpha_n } < 1 \ \mbox{ and } \ 0 < \frac{ - \alpha_n }{ \beta_n - \alpha_n } < 1. \tag{3} $$    Now $$ \begin{align} D_n &=  \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } \\ &= \frac{ f\left( \beta_n \right) - f(0) + f(0) -  f \left( \alpha_n \right) }{ \beta_n - \alpha_n } \\ &= \frac{f\left( \beta_n \right) - f(0) }{ \beta_n - \alpha_n} + \frac{ f(0) -  f \left( \alpha_n \right)}{\beta_n - \alpha_n} \\ &= \frac{\beta_n }{ \beta_n - \alpha_n } \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } + \frac{ - \alpha_n }{ \beta_n - \alpha_n} \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n }. \end{align} $$   So, for all $n > \max \left\{ M, N \right\}$ [Refer to (2) above.], we obtain $$  \begin{align} & \left| D_n - f^\prime(0) \right| \\ &= \left| \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } + \frac{ - \alpha_n }{ \beta_n - \alpha_n} \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \left[ \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right]  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left[ \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right] \right| \\ &\leq \frac{\beta_n }{ \beta_n - \alpha_n } \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right|  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0)  \right| \qquad \mbox{ [ using (3) ] } \\ &\leq \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right|  + \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0)  \right| \qquad \mbox{ [ using (3) again ] }  \\ &= \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \qquad \mbox{ [ using (1) and (2) ] } \\ &< \varepsilon.  \end{align} $$    Thus, for every real number $\varepsilon > 0$, we can find a natural number $K \colon= \max\left\{ M, N \right\}$ such that $$ \left| D_n - f^\prime(0) \right| < \varepsilon $$ for all natural numbers $n > K$. Hence $$\lim_{n \to \infty} D_n = 0.$$ Am I right? Part (b): If $0 < \alpha_n < \beta_n$, then $$0 < \frac{\alpha_n}{\beta_n - \alpha_n} < \frac{\beta_n}{\beta_n - \alpha_n}; \tag{4}$$ furthermore if there is a real number $r > 0$ such that $$\frac{\beta_n }{\beta_n - \alpha_n } \leq r,$$ then we also have $$ \frac{\alpha_n }{\beta_n - \alpha_n } \leq r.$$   Now as $f^\prime(0) = \lim_{x \to 0 } \frac{ f(x) - f(0) }{x-0}$ exists, so, for every real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that $$\left| \frac{ f(x) - f(0) }{x-0} - f^\prime(0) \right| < \frac{\varepsilon}{4r} \ \mbox{ for all real numbers } x \in (-1, 1) \ \mbox{ for which } \ 0 < | x | < \delta. \tag{5} $$    And, since $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, therefore we can find natural numbers $M$ and $N$ such that    $$ \left| \alpha_n \right| < \delta \ \mbox{ for all natural numbers } n > M, \tag{6} $$   and    $$ \left| \beta_n \right| < \delta \ \mbox{ for all natural numbers } n > N. \tag{7} $$   So, for all natural numbers $n > \max\left\{ M, N \right\}$, we see that    $$  \begin{align} & \left| D_n - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \left[ \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right]  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left[ \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right] \right| \\ &\leq \frac{\beta_n }{ \beta_n - \alpha_n } \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } \right| + \frac{  \alpha_n }{ \beta_n - \alpha_n}  \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \qquad \mbox{ [ using (4) ] } \\ &\leq r \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } \right| + r \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \qquad \mbox{ [ using our hypothesis ] } \\ &< r \frac{\varepsilon}{4r} + r \frac{\varepsilon}{4r} \qquad \mbox{ [ using (5), (6), and (7) above ] } \\ &< \varepsilon. \end{align}  $$   Since $\varepsilon$ was an arbitrary positive real number, therefore it follows that $$\lim_{n \to \infty} D_n = f^\prime(0),$$    as required. Am I right? Part (c): For each $n \in \mathbb{N}$, as $-1 < \alpha_n < \beta_n < 1$ and as $f^\prime$ is continuous in $(-1, 1)$, so $f$ satisfies the hypothesis of the mean value theorem on the interval $\left[ \alpha_n, \beta_n \right]$. So, for every $n \in \mathbb{N}$, we can find a real number $\gamma_n \in \left( \alpha_n, \beta_n \right)$ such that $$ D_n = \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } = f^\prime\left(\gamma_n\right). $$   Now as $\alpha_n < \gamma_n < \beta_n$ for all $n \in \mathbb{N}$ and as $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, so by the sandwiching theorem we can conclude that $\gamma_n \to 0$ as $n \to \infty$, and since $f^\prime$ is continuous in $(-1, 1)$ and hence at $x=0$, therefore we can conclude that $f^\prime\left(\gamma_n\right) \to f^\prime(0)$ as $n \to \infty$; that is, $D_n \to f^\prime(0)$ as $n \to \infty$. Am I right? Let $f$ be defined on $(-1, 1)$ by $$ f(x) = \begin{cases} x^2 \sin \left( \frac{1}{x} \right) \ & \ (x \neq 0), \\ 0 \ & \ (x=0). \end{cases} $$ Let $$\alpha_n = \frac{1}{2\pi (n + 1/4)}, \qquad \beta_n = \frac{1}{2 \pi n}  \qquad \mbox{ for all } n \in \mathbb{N}.$$    Then $$ f^\prime(x) = \begin{cases} 2x \sin \left( \frac{1}{x} \right) + x^2 \cos \left( \frac{1}{x} \right) \ & \ (x \neq 0), \\ 0 \ & \ (x=0). \end{cases} $$    Moreover, $$0 < \alpha_n < \beta_n < 1 \ \mbox{ for all } n \in \mathbb{N},$$ and $$ \lim_{n \to \infty} \alpha_n = 0, \qquad \lim_{n \to \infty} \beta_n = 0.$$    Now    $$ \begin{align} D_n &= \frac{ f\left( \beta_n \right) - f\left( \alpha_n \right)   }{\beta_n - \alpha_n } \\  &= \frac{ - \frac{1}{4 \pi^2 (n+1/4)^2 } }{\frac{1}{2 \pi n}  -  \frac{1}{2\pi (n + 1/4)} } \\ &= - \frac{ 1}{2 \pi } \frac{  \frac{1}{(n+1/4)^2} }{ \frac{1}{n} - \frac{1}{n+1/4}  } \\ &= - \frac{ 1}{2 \pi } \frac{ \frac{1}{n+1/4} }{ \frac{1/4}{ n }  } \\ &= - \frac{ 1}{2 \pi }\frac{16 n}{4n + 1} \\ &= - \frac{ 8 }{ \pi } \frac{1}{ 4+1/n }. \end{align} $$   So    $$\lim_{n \to \infty} D_n = - \frac{2}{\pi} \neq f^\prime(0).$$ Am I right? Are my proofs correct and rigorous enough for Rudin? If not, then where are the pitfalls? Is my example appropriate in the situation described by Rudin? If so, is my calculation correct also?","Here is Prob. 19, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined in $(-1, 1)$ and $f^\prime(0)$ exists. Suppose $-1 < \alpha_n < \beta_n < 1$, $\alpha_n \to 0$, and $\beta \to 0$ as $n \to \infty$. Define the difference quotients $$ D_n = \frac{f\left(\beta_n\right)-f\left(\alpha_n\right)}{\beta_n-\alpha_n}.$$    Prove the following statements: (a) If $\alpha_n < 0 < \beta_n$, then $\lim D_n = f^\prime(0)$. (b) If $0 < \alpha_n < \beta_n$ and $\left\{ \beta_n / (\beta_n-\alpha_n) \right\}$ is bounded, then $\lim D_n = f^\prime(0)$. (c) If $f^\prime$ is continuous in $(-1, 1)$, then $\lim D_n = f^\prime(0)$. Give an example in which $f$ is differentiable in $(-1, 1)$ (but $f^\prime$ is not continuous at $0$) and in which $\alpha_n$, $\beta_n$ tend to $0$ in such a way that $\lim D_n$ exists but is different from $f^\prime(0)$. My Attempt: Part (a): Since $$f^\prime(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x-0}$$ exists, so, given any real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that    $$ \left| \frac{f(x) - f(0)}{x-0} - f^\prime(0) \right| < \frac{\varepsilon}{4} \tag{1} $$   for all $x \in (-1, 1)$ for which $0 < |x| < \delta$. Now as $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, so we can find natural numbers $M$ and $N$ such that    $$ \left| \alpha_n \right| < \delta \ \mbox{ for all } n > M,$$    and    $$ \left| \beta_n \right| < \delta  \ \mbox{ for all } n > N,$$    So    $$ \left| \alpha_n \right| < \delta  \ \mbox{ and } \left| \beta_n \right| < \delta \ \mbox{ for all } n > \max \left\{ M, N \right\}. \tag{2} $$ As $\alpha_n < 0 < \beta_n$, so we have $0 < - \alpha_n < \beta_n - \alpha_n$ and $ 0 < \beta_n < \beta_n - \alpha_n$, which imply    $$ 0 < \frac{\beta_n }{ \beta_n - \alpha_n } < 1 \ \mbox{ and } \ 0 < \frac{ - \alpha_n }{ \beta_n - \alpha_n } < 1. \tag{3} $$    Now $$ \begin{align} D_n &=  \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } \\ &= \frac{ f\left( \beta_n \right) - f(0) + f(0) -  f \left( \alpha_n \right) }{ \beta_n - \alpha_n } \\ &= \frac{f\left( \beta_n \right) - f(0) }{ \beta_n - \alpha_n} + \frac{ f(0) -  f \left( \alpha_n \right)}{\beta_n - \alpha_n} \\ &= \frac{\beta_n }{ \beta_n - \alpha_n } \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } + \frac{ - \alpha_n }{ \beta_n - \alpha_n} \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n }. \end{align} $$   So, for all $n > \max \left\{ M, N \right\}$ [Refer to (2) above.], we obtain $$  \begin{align} & \left| D_n - f^\prime(0) \right| \\ &= \left| \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } + \frac{ - \alpha_n }{ \beta_n - \alpha_n} \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \left[ \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right]  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left[ \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right] \right| \\ &\leq \frac{\beta_n }{ \beta_n - \alpha_n } \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right|  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0)  \right| \qquad \mbox{ [ using (3) ] } \\ &\leq \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right|  + \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0)  \right| \qquad \mbox{ [ using (3) again ] }  \\ &= \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \qquad \mbox{ [ using (1) and (2) ] } \\ &< \varepsilon.  \end{align} $$    Thus, for every real number $\varepsilon > 0$, we can find a natural number $K \colon= \max\left\{ M, N \right\}$ such that $$ \left| D_n - f^\prime(0) \right| < \varepsilon $$ for all natural numbers $n > K$. Hence $$\lim_{n \to \infty} D_n = 0.$$ Am I right? Part (b): If $0 < \alpha_n < \beta_n$, then $$0 < \frac{\alpha_n}{\beta_n - \alpha_n} < \frac{\beta_n}{\beta_n - \alpha_n}; \tag{4}$$ furthermore if there is a real number $r > 0$ such that $$\frac{\beta_n }{\beta_n - \alpha_n } \leq r,$$ then we also have $$ \frac{\alpha_n }{\beta_n - \alpha_n } \leq r.$$   Now as $f^\prime(0) = \lim_{x \to 0 } \frac{ f(x) - f(0) }{x-0}$ exists, so, for every real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that $$\left| \frac{ f(x) - f(0) }{x-0} - f^\prime(0) \right| < \frac{\varepsilon}{4r} \ \mbox{ for all real numbers } x \in (-1, 1) \ \mbox{ for which } \ 0 < | x | < \delta. \tag{5} $$    And, since $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, therefore we can find natural numbers $M$ and $N$ such that    $$ \left| \alpha_n \right| < \delta \ \mbox{ for all natural numbers } n > M, \tag{6} $$   and    $$ \left| \beta_n \right| < \delta \ \mbox{ for all natural numbers } n > N. \tag{7} $$   So, for all natural numbers $n > \max\left\{ M, N \right\}$, we see that    $$  \begin{align} & \left| D_n - f^\prime(0) \right| \\ &= \left| \frac{\beta_n }{ \beta_n - \alpha_n } \left[ \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } - f^\prime(0) \right]  +  \frac{ - \alpha_n }{ \beta_n - \alpha_n} \left[ \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right] \right| \\ &\leq \frac{\beta_n }{ \beta_n - \alpha_n } \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } \right| + \frac{  \alpha_n }{ \beta_n - \alpha_n}  \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \qquad \mbox{ [ using (4) ] } \\ &\leq r \left| \frac{ f\left( \beta_n \right) - f(0) }{ \beta_n } \right| + r \left| \frac{ f \left( \alpha_n \right) - f(0) }{ \alpha_n } - f^\prime(0) \right| \qquad \mbox{ [ using our hypothesis ] } \\ &< r \frac{\varepsilon}{4r} + r \frac{\varepsilon}{4r} \qquad \mbox{ [ using (5), (6), and (7) above ] } \\ &< \varepsilon. \end{align}  $$   Since $\varepsilon$ was an arbitrary positive real number, therefore it follows that $$\lim_{n \to \infty} D_n = f^\prime(0),$$    as required. Am I right? Part (c): For each $n \in \mathbb{N}$, as $-1 < \alpha_n < \beta_n < 1$ and as $f^\prime$ is continuous in $(-1, 1)$, so $f$ satisfies the hypothesis of the mean value theorem on the interval $\left[ \alpha_n, \beta_n \right]$. So, for every $n \in \mathbb{N}$, we can find a real number $\gamma_n \in \left( \alpha_n, \beta_n \right)$ such that $$ D_n = \frac{ f\left( \beta_n \right) - f \left( \alpha_n \right) }{ \beta_n - \alpha_n } = f^\prime\left(\gamma_n\right). $$   Now as $\alpha_n < \gamma_n < \beta_n$ for all $n \in \mathbb{N}$ and as $\alpha_n \to 0$ and $\beta_n \to 0$ as $n \to \infty$, so by the sandwiching theorem we can conclude that $\gamma_n \to 0$ as $n \to \infty$, and since $f^\prime$ is continuous in $(-1, 1)$ and hence at $x=0$, therefore we can conclude that $f^\prime\left(\gamma_n\right) \to f^\prime(0)$ as $n \to \infty$; that is, $D_n \to f^\prime(0)$ as $n \to \infty$. Am I right? Let $f$ be defined on $(-1, 1)$ by $$ f(x) = \begin{cases} x^2 \sin \left( \frac{1}{x} \right) \ & \ (x \neq 0), \\ 0 \ & \ (x=0). \end{cases} $$ Let $$\alpha_n = \frac{1}{2\pi (n + 1/4)}, \qquad \beta_n = \frac{1}{2 \pi n}  \qquad \mbox{ for all } n \in \mathbb{N}.$$    Then $$ f^\prime(x) = \begin{cases} 2x \sin \left( \frac{1}{x} \right) + x^2 \cos \left( \frac{1}{x} \right) \ & \ (x \neq 0), \\ 0 \ & \ (x=0). \end{cases} $$    Moreover, $$0 < \alpha_n < \beta_n < 1 \ \mbox{ for all } n \in \mathbb{N},$$ and $$ \lim_{n \to \infty} \alpha_n = 0, \qquad \lim_{n \to \infty} \beta_n = 0.$$    Now    $$ \begin{align} D_n &= \frac{ f\left( \beta_n \right) - f\left( \alpha_n \right)   }{\beta_n - \alpha_n } \\  &= \frac{ - \frac{1}{4 \pi^2 (n+1/4)^2 } }{\frac{1}{2 \pi n}  -  \frac{1}{2\pi (n + 1/4)} } \\ &= - \frac{ 1}{2 \pi } \frac{  \frac{1}{(n+1/4)^2} }{ \frac{1}{n} - \frac{1}{n+1/4}  } \\ &= - \frac{ 1}{2 \pi } \frac{ \frac{1}{n+1/4} }{ \frac{1/4}{ n }  } \\ &= - \frac{ 1}{2 \pi }\frac{16 n}{4n + 1} \\ &= - \frac{ 8 }{ \pi } \frac{1}{ 4+1/n }. \end{align} $$   So    $$\lim_{n \to \infty} D_n = - \frac{2}{\pi} \neq f^\prime(0).$$ Am I right? Are my proofs correct and rigorous enough for Rudin? If not, then where are the pitfalls? Is my example appropriate in the situation described by Rudin? If so, is my calculation correct also?",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
2,Limit of a sum 1: $\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k} \cdot \sin \frac{2k\pi }{n}$,Limit of a sum 1:,\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k} \cdot \sin \frac{2k\pi }{n},"$$\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k} \cdot \sin \frac{2k\pi }{n}$$ It looks like a Riemann sum, but I don't know how to approach it. Any hint would be appreciated. (Without Taylor expansion) EDIT: The answer is $- \frac{1}{4 \pi}$","$$\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k} \cdot \sin \frac{2k\pi }{n}$$ It looks like a Riemann sum, but I don't know how to approach it. Any hint would be appreciated. (Without Taylor expansion) EDIT: The answer is $- \frac{1}{4 \pi}$",,"['limits', 'riemann-sum']"
3,Infinite sum of recursive integrals,Infinite sum of recursive integrals,,"Given, $$I_n = \int_0^1 e^{x}x^{n}dx\ $$ Find:  $$\left\lceil \lim_{n\to \infty}\sum_{k=1}^n\frac{I_{k+1}}{I_k} \right\rceil$$ I tried to define $I_n$ using a recursive pattern in the following way: \begin{align} I_n &= \int_0^1e^{x}x^{n}\text{d}x\\ &=  e - \int_0^1 e^{x}\cdot n\cdot x^{n-1}\text{d}x\  \text{(using integration by parts)}\\ &= e - n\cdot  I_{n-1} \end{align} Equivalently, $ \frac{I_{n+1}}{I_n} = \frac{e}{I_n} - (n+1) $ So, the sum becomes:  \begin{align} &\ e(\frac{1}{I_{n}} + \frac{1}{I_{n-1}} + \frac{1}{I_{n-2}} +\cdots + \frac{1}{I_{1}}) - ((n+1) + n + (n-1) + \cdots+2)\\ &=\ e(\frac{1}{I_{n}} + \frac{1}{I_{n-1}} + \frac{1}{I_{n-2}} +\cdots + \frac{1}{I_{1}}) +1 - (\frac{(n+2)(n+1)}{2}) \end{align} But now I am stuck and don't know how to proceed, I also found out that $ I_0 = e-1 $ and $ I_1 = 1 $ and few more $I_n$ but that did not lead me anywhere, or at least, I could not get anything from there. Thank you in advance!","Given, $$I_n = \int_0^1 e^{x}x^{n}dx\ $$ Find:  $$\left\lceil \lim_{n\to \infty}\sum_{k=1}^n\frac{I_{k+1}}{I_k} \right\rceil$$ I tried to define $I_n$ using a recursive pattern in the following way: \begin{align} I_n &= \int_0^1e^{x}x^{n}\text{d}x\\ &=  e - \int_0^1 e^{x}\cdot n\cdot x^{n-1}\text{d}x\  \text{(using integration by parts)}\\ &= e - n\cdot  I_{n-1} \end{align} Equivalently, $ \frac{I_{n+1}}{I_n} = \frac{e}{I_n} - (n+1) $ So, the sum becomes:  \begin{align} &\ e(\frac{1}{I_{n}} + \frac{1}{I_{n-1}} + \frac{1}{I_{n-2}} +\cdots + \frac{1}{I_{1}}) - ((n+1) + n + (n-1) + \cdots+2)\\ &=\ e(\frac{1}{I_{n}} + \frac{1}{I_{n-1}} + \frac{1}{I_{n-2}} +\cdots + \frac{1}{I_{1}}) +1 - (\frac{(n+2)(n+1)}{2}) \end{align} But now I am stuck and don't know how to proceed, I also found out that $ I_0 = e-1 $ and $ I_1 = 1 $ and few more $I_n$ but that did not lead me anywhere, or at least, I could not get anything from there. Thank you in advance!",,"['calculus', 'integration', 'limits', 'summation']"
4,Calculating Euler Number limit,Calculating Euler Number limit,,"Please, so far I did $$\lim_{x\to +\infty}\left(\frac{x^2-x+1}{x+2}\right)^{\frac{1}{x-1}},$$ but I can write $$\frac{x^2-x+1}{x+2}=1+\frac{x^2-2x-1}{x+2}=1+\frac{1}{\frac{x+2}{x^2-2x-1}}.$$ But $$\lim_{x\to +\infty}\frac{x+2}{x^2-2x-1}=0,$$ so I can not use  $$e =\lim_{N\to \infty}(1+\frac{1}{N})^N$$","Please, so far I did $$\lim_{x\to +\infty}\left(\frac{x^2-x+1}{x+2}\right)^{\frac{1}{x-1}},$$ but I can write $$\frac{x^2-x+1}{x+2}=1+\frac{x^2-2x-1}{x+2}=1+\frac{1}{\frac{x+2}{x^2-2x-1}}.$$ But $$\lim_{x\to +\infty}\frac{x+2}{x^2-2x-1}=0,$$ so I can not use  $$e =\lim_{N\to \infty}(1+\frac{1}{N})^N$$",,"['calculus', 'limits']"
5,Find $ \lim_{n \rightarrow \infty} \left(n \int^{\frac{\pi}{4}}_0 (\cos(x)-\sin(x))^n \right)$,Find, \lim_{n \rightarrow \infty} \left(n \int^{\frac{\pi}{4}}_0 (\cos(x)-\sin(x))^n \right),"Find $$ \lim_{n \rightarrow \infty}  \left(n \int^{\frac{\pi}{4}}_0 (\cos(x)-\sin(x))^n \right)$$ I've managed to prove that the limit is in $(0,1]$ and I believe it is $1$ but I don't know how to prove it. Could you help me?","Find $$ \lim_{n \rightarrow \infty}  \left(n \int^{\frac{\pi}{4}}_0 (\cos(x)-\sin(x))^n \right)$$ I've managed to prove that the limit is in $(0,1]$ and I believe it is $1$ but I don't know how to prove it. Could you help me?",,['limits']
6,What is $\lim\limits_{n\to\infty}\frac{n^\sqrt n}{n!}$?,What is ?,\lim\limits_{n\to\infty}\frac{n^\sqrt n}{n!},"I am stuck at this question where I have to calculate what is big O of what, $n!$ and $n^\sqrt n$ I tried replacing n! by it's equivalent formula but it makes everything more complicated, I can't even think about doing it by induction. Any help would be appreciated","I am stuck at this question where I have to calculate what is big O of what, $n!$ and $n^\sqrt n$ I tried replacing n! by it's equivalent formula but it makes everything more complicated, I can't even think about doing it by induction. Any help would be appreciated",,"['limits', 'factorial', 'radicals']"
7,Limit $\lim\limits_{a \to 0}( a\lfloor\frac{x}{a}\rfloor)$,Limit,\lim\limits_{a \to 0}( a\lfloor\frac{x}{a}\rfloor),"Obviously the limit either does not exist or converges to $x$ . I'm partial towards the latter, and have an incomplete argument involving Fourier Series which corroborates my inclination. Feel free to disprove me/affirm my hunch. $$\lim\limits_{a \to 0}( a\lfloor\frac{x}{a}\rfloor)$$","Obviously the limit either does not exist or converges to . I'm partial towards the latter, and have an incomplete argument involving Fourier Series which corroborates my inclination. Feel free to disprove me/affirm my hunch.",x \lim\limits_{a \to 0}( a\lfloor\frac{x}{a}\rfloor),"['calculus', 'real-analysis', 'limits', 'ceiling-and-floor-functions']"
8,Find the asymptotes,Find the asymptotes,,"Find the asymptotes of $ f:\mathbb{R}\rightarrow \mathbb{R},f(x)=\sqrt[3]{e^{x}-e^{2x}+e^{4x}\ln^{2}(1+e^{-x})}. $ I found that $y=0$ is an asymptote when $ x\rightarrow -\infty $, but how do I calculate $ \lim_{x\to\infty }f(x) $ ?","Find the asymptotes of $ f:\mathbb{R}\rightarrow \mathbb{R},f(x)=\sqrt[3]{e^{x}-e^{2x}+e^{4x}\ln^{2}(1+e^{-x})}. $ I found that $y=0$ is an asymptote when $ x\rightarrow -\infty $, but how do I calculate $ \lim_{x\to\infty }f(x) $ ?",,"['real-analysis', 'limits', 'asymptotics']"
9,Limits tricky problem,Limits tricky problem,,"Let $f: \mathbb{R} \to \mathbb{R}$ be a function that is twice differentiable. We know that: $$\lim_{x\to-\infty}\ f(x) = 1$$ $$\lim_{x\to\infty}\ f(x) = 0$$ $$f(0) = \pi$$ We have to prove that there exist at least two points of the function in which $f''(x) = 0$. How could we do it in a rigorous way? It is pretty intuitive, but in a rigorous way it isn't that simple for me...","Let $f: \mathbb{R} \to \mathbb{R}$ be a function that is twice differentiable. We know that: $$\lim_{x\to-\infty}\ f(x) = 1$$ $$\lim_{x\to\infty}\ f(x) = 0$$ $$f(0) = \pi$$ We have to prove that there exist at least two points of the function in which $f''(x) = 0$. How could we do it in a rigorous way? It is pretty intuitive, but in a rigorous way it isn't that simple for me...",,"['calculus', 'limits']"
10,limit of arithmetic weighted mean ($\lim_\limits{n\to\infty}\frac{\sum_\limits{k=1}^n{t_ka_n}}{\sum_\limits{k=1}^n{t_k}} = L$),limit of arithmetic weighted mean (),\lim_\limits{n\to\infty}\frac{\sum_\limits{k=1}^n{t_ka_n}}{\sum_\limits{k=1}^n{t_k}} = L,"Given that: $t_n>0 \\ \lim_\limits{n\to\infty}\sum_\limits{k=1}^n{t_k} = \infty$ $\lim_\limits{n\to\infty}a_n = L$ (either $L=\pm\infty$ or $L\in\mathbb{R}$) I'm trying to prove that: $\lim_\limits{n\to\infty}\frac{\sum_\limits{k=1}^n{t_ka_n}}{\sum_\limits{k=1}^n{t_k}} = L$ Of course, it seems very similar Cesaro mean, but I've had trouble trying to apply a similar method here. Any idea? Thanks!","Given that: $t_n>0 \\ \lim_\limits{n\to\infty}\sum_\limits{k=1}^n{t_k} = \infty$ $\lim_\limits{n\to\infty}a_n = L$ (either $L=\pm\infty$ or $L\in\mathbb{R}$) I'm trying to prove that: $\lim_\limits{n\to\infty}\frac{\sum_\limits{k=1}^n{t_ka_n}}{\sum_\limits{k=1}^n{t_k}} = L$ Of course, it seems very similar Cesaro mean, but I've had trouble trying to apply a similar method here. Any idea? Thanks!",,"['calculus', 'limits']"
11,How do you find the limit as $x$ approaches infinity of $x! / a^x$ for some integer $a$,How do you find the limit as  approaches infinity of  for some integer,x x! / a^x a,The limit as $n$ approaches infinity of the following functions diverges: $n! / a^n$ $n! / n^a$ For any fixed integer $a$. How can I show this? EDIT: The original question was the second bullet point; however I really meant to type the first bullet.,The limit as $n$ approaches infinity of the following functions diverges: $n! / a^n$ $n! / n^a$ For any fixed integer $a$. How can I show this? EDIT: The original question was the second bullet point; however I really meant to type the first bullet.,,"['calculus', 'limits', 'factorial']"
12,"Let $\sum_{n=0}^\infty \frac{(-1)^{n+1}}{3 n+6 (-1)^n}$, does it converge or does it diverge?","Let , does it converge or does it diverge?",\sum_{n=0}^\infty \frac{(-1)^{n+1}}{3 n+6 (-1)^n},"Let $\displaystyle \sum_{n=0}^\infty  \frac{(-1)^{n+1}}{3 n+6 (-1)^n}$ , does it converge or does it diverge? I'm not completely sure that my calculation is correct, check it please. $$\begin{align}\sum_{n\ge 0} \frac{(-1)^{n+1}}{3 n+6 (-1)^n}&=-\frac13\sum_{n\ge 0} \frac{(-1)^{n}}{n+2 (-1)^n}\cdot\frac{(-1)^n}{(-1)^n}\\&=-\frac13\sum_{n\ge 0} \frac1{n(-1)^n+2}\\&=-\frac13\lim_{m\to\infty}\left(\sum_{n=0\\ 2\mid n}^m\frac1{n+2}+\sum_{n=0\\ 2\nmid n}^m\frac1{-n+2}\right)\\&=-\frac13\lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)\end{align}$$ If $m$ is even then $$\begin{align}\lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)&=\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m-2}\frac{(-1)^{n}}n\right)+\frac1m+\frac1{m+2}+1\right)\\&=-\log (2)+1\end{align}$$ If $m$ is odd then $$\lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)=\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m-1}\frac{(-1)^{n}}n\right)+\frac1{m+1}+1\right)=-\log (2)+1$$ Then finally $$\bbox[2pt,border:yellow solid 2px]{\sum_{n=0}^\infty\frac{(-1)^{n+1}}{3 n+6 (-1)^n}=\frac{\log(2)-1}{3}}$$ Are my calculations correct?","Let , does it converge or does it diverge? I'm not completely sure that my calculation is correct, check it please. If is even then If is odd then Then finally Are my calculations correct?","\displaystyle \sum_{n=0}^\infty  \frac{(-1)^{n+1}}{3 n+6 (-1)^n} \begin{align}\sum_{n\ge 0} \frac{(-1)^{n+1}}{3 n+6 (-1)^n}&=-\frac13\sum_{n\ge 0} \frac{(-1)^{n}}{n+2 (-1)^n}\cdot\frac{(-1)^n}{(-1)^n}\\&=-\frac13\sum_{n\ge 0} \frac1{n(-1)^n+2}\\&=-\frac13\lim_{m\to\infty}\left(\sum_{n=0\\ 2\mid n}^m\frac1{n+2}+\sum_{n=0\\ 2\nmid n}^m\frac1{-n+2}\right)\\&=-\frac13\lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)\end{align} m \begin{align}\lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)&=\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m-2}\frac{(-1)^{n}}n\right)+\frac1m+\frac1{m+2}+1\right)\\&=-\log (2)+1\end{align} m \lim_{m\to\infty}\left(\sum_{n=2\\ 2\mid n}^{m+2}\frac1{n}-\sum_{n=-2\\ 2\nmid n}^{m-2}\frac1{n}\right)=\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m-1}\frac{(-1)^{n}}n\right)+\frac1{m+1}+1\right)=-\log (2)+1 \bbox[2pt,border:yellow solid 2px]{\sum_{n=0}^\infty\frac{(-1)^{n+1}}{3 n+6 (-1)^n}=\frac{\log(2)-1}{3}}","['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'closed-form']"
13,How to calculate the limit of $\frac{a_n}{n^2}$ for the sequence $a_{n+1}=a_n+\frac{2 a_{n-1}}{n+1}$?,How to calculate the limit of  for the sequence ?,\frac{a_n}{n^2} a_{n+1}=a_n+\frac{2 a_{n-1}}{n+1},"Assume $\{a_n\}$is a sequence which  satisfies the following recursion formula: $$a_{n+1}=a_n+\frac{2a_{n-1}}{n+1}~(n\geqslant 1).$$ and $a_0=\pi,a_1=\pi^2 $. How to compute $\displaystyle\lim_{n\to \infty}\frac{a_n}{n^2}$?","Assume $\{a_n\}$is a sequence which  satisfies the following recursion formula: $$a_{n+1}=a_n+\frac{2a_{n-1}}{n+1}~(n\geqslant 1).$$ and $a_0=\pi,a_1=\pi^2 $. How to compute $\displaystyle\lim_{n\to \infty}\frac{a_n}{n^2}$?",,"['sequences-and-series', 'limits']"
14,Why can we not use L'Hopitals' rule to prove $\lim_{x \to 0} \dfrac{x^2 \sin \frac{1}{x}}{\sin x}=0$?,Why can we not use L'Hopitals' rule to prove ?,\lim_{x \to 0} \dfrac{x^2 \sin \frac{1}{x}}{\sin x}=0,"This is a problem in Schaum's outline -- Advanced Calculus, page 89, problem 4.84. Why we cannot use L'Hopitals' rule to prove $$\lim_{x \to 0} \dfrac{x^2 \sin \dfrac{1}{x}}{\sin x}=0\;?$$ I thought the numerator $x^2 \sin \dfrac{1}{x}$ goes to $0$ as $x$ goes to $0$, since $x^2$ goes to 0 and $\sin \dfrac{1}{x}$is bounded. And the denominator goes to 0 too. But why we can not use L'Hopitals' rule here?","This is a problem in Schaum's outline -- Advanced Calculus, page 89, problem 4.84. Why we cannot use L'Hopitals' rule to prove $$\lim_{x \to 0} \dfrac{x^2 \sin \dfrac{1}{x}}{\sin x}=0\;?$$ I thought the numerator $x^2 \sin \dfrac{1}{x}$ goes to $0$ as $x$ goes to $0$, since $x^2$ goes to 0 and $\sin \dfrac{1}{x}$is bounded. And the denominator goes to 0 too. But why we can not use L'Hopitals' rule here?",,"['calculus', 'limits']"
15,basic concepts of limits,basic concepts of limits,,"Suppose I have a function: $ f(x)=x$ Now I want to calculate the limits: $\lim\limits_{x\rightarrow1}f(x)=f(1)$ As wiki_limits said, the limit of $f$ of $x$, as $x$ approaches 1, is $f(1)$. So my question here is 1 Does it mean the limit calculation are actually is a approximated kind of calculation? It is not normal precise calculation such as $1+1=2$ 2 If all limit calculation are approximated results, does it mean that Caculus are about approximated calculation? (Based on my current knowledge, just begin to learn single calculus)","Suppose I have a function: $ f(x)=x$ Now I want to calculate the limits: $\lim\limits_{x\rightarrow1}f(x)=f(1)$ As wiki_limits said, the limit of $f$ of $x$, as $x$ approaches 1, is $f(1)$. So my question here is 1 Does it mean the limit calculation are actually is a approximated kind of calculation? It is not normal precise calculation such as $1+1=2$ 2 If all limit calculation are approximated results, does it mean that Caculus are about approximated calculation? (Based on my current knowledge, just begin to learn single calculus)",,"['calculus', 'limits']"
16,Geometrical proof of the limit $\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1$ using sandwich theorem.,Geometrical proof of the limit  using sandwich theorem.,\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1,"I am studying about sandwich theorem and its applications by deriving some well-known limits such as this- $$\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1$$ while I found some proofs of this result by first defining $e$ and then using that definition such as here( Proof of $ f(x) = (e^x-1)/x = 1 \text{ as } x\to 0$ using epsilon-delta definition of a limit )( which I agree sounds a lot easier because if one is using the Taylor series expansion of $e^x$ then it becomes very easy )but my book tries to do this in a different manner by using this inequality $$\frac{1}{1+|x|}≤\left(\frac{e^x-1}{x}\right)≤ 1 + (e – 2) |x|$$( holds for all $x$ in $[–1, 1]-[0]$) and then just using sandwich theorem the limit is easily calculated , but the book does not explain as to from where this inequality came from.And I am not able to get it by myself ,as I am not able to see how can this result be so obvious, and even though the graph does make it a bit clear( which I have attached below ) still I am not able to get the given inequality(any hints there?) I tried to search it on this site but couldn't find it,but still I found some very neat applications of sandwich theorem such as here ( How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? ) so which makes me think that maybe this inequality can be derived easily by looking at it's geometrical interpretation such as in the link given. So can someone please help me in understanding the geometrical meaning of this inequality ( just like in the limit given in the link above ) or help to derive it using its geometrical implications? 1)And even if that is not possible can someone help me in understanding the inequality intuitively because I honestly haven't got any idea as to how such a weird looking inequality can be related to the given limit, 2)And how that inequality is derived ? 3)Also what can possibly be the motivation behind this complicated inequality for deriving this limit, are there other such wierd inequalities also for finding this limit?","I am studying about sandwich theorem and its applications by deriving some well-known limits such as this- $$\lim_{(x \to 0)}\left(\frac{e^x-1}{x}\right)=1$$ while I found some proofs of this result by first defining $e$ and then using that definition such as here( Proof of $ f(x) = (e^x-1)/x = 1 \text{ as } x\to 0$ using epsilon-delta definition of a limit )( which I agree sounds a lot easier because if one is using the Taylor series expansion of $e^x$ then it becomes very easy )but my book tries to do this in a different manner by using this inequality $$\frac{1}{1+|x|}≤\left(\frac{e^x-1}{x}\right)≤ 1 + (e – 2) |x|$$( holds for all $x$ in $[–1, 1]-[0]$) and then just using sandwich theorem the limit is easily calculated , but the book does not explain as to from where this inequality came from.And I am not able to get it by myself ,as I am not able to see how can this result be so obvious, and even though the graph does make it a bit clear( which I have attached below ) still I am not able to get the given inequality(any hints there?) I tried to search it on this site but couldn't find it,but still I found some very neat applications of sandwich theorem such as here ( How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? ) so which makes me think that maybe this inequality can be derived easily by looking at it's geometrical interpretation such as in the link given. So can someone please help me in understanding the geometrical meaning of this inequality ( just like in the limit given in the link above ) or help to derive it using its geometrical implications? 1)And even if that is not possible can someone help me in understanding the inequality intuitively because I honestly haven't got any idea as to how such a weird looking inequality can be related to the given limit, 2)And how that inequality is derived ? 3)Also what can possibly be the motivation behind this complicated inequality for deriving this limit, are there other such wierd inequalities also for finding this limit?",,"['calculus', 'limits']"
17,Calculate the limit $\lim_{x \to 2} \frac{x^2\sqrt{x+2}-8}{4-x^2}$,Calculate the limit,\lim_{x \to 2} \frac{x^2\sqrt{x+2}-8}{4-x^2},"Calculate the limit $$\lim_{x \to 2} \frac{x^2\sqrt{x+2}-8}{4-x^2}$$ I tried to factorise and to simplify, but I can't find anything good. $$\lim_{x \to 2} \frac{\frac{x^2(x+2)-8\sqrt{x+2}}{\sqrt{x+2}}}{(4-x^2)}$$","Calculate the limit $$\lim_{x \to 2} \frac{x^2\sqrt{x+2}-8}{4-x^2}$$ I tried to factorise and to simplify, but I can't find anything good. $$\lim_{x \to 2} \frac{\frac{x^2(x+2)-8\sqrt{x+2}}{\sqrt{x+2}}}{(4-x^2)}$$",,['limits']
18,Prove $\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi$,Prove,\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi,"For a beginning calculus student , prove $\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi$ I'm guessing this means something like Allowed : Pre-university maths, precalculus, basic calculus up to techniques of integration, Bernoulli's rule for sequences, two police persons theorem for sequences, $\lim_{n \to \infty} f(a_n) = f(\lim_{n \to \infty} a_n)$ if $f$ is continuous Not allowed : Monotone Convergence Theorem, Taylor series, polar coordinates and advanced stuff in real and complex analysis and the like (eg limsup, liminf, Stolz–Cesàro theorem, Cauchy sequences) What I tried: $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \le \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi$$ To use the two police persons theorem, I need to find some $a_n$ s.t. $$\lim_{n \to \infty} a_n = 2\pi$$ $$\frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \ge a_n $$ $$a_n \ne \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n})$$ Questions: What $a_n$ can I use? How else can I approach this problem? Might I be able to say that $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \color{red}{=} \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi$$ because $\lim \sin(\pi/n) = \lim \pi/n$ and for the same reason that justifies step 3 here ? So it's like $$\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \lim_{n \to \infty} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = 2\pi (1) = 2\pi$$ ? What is the technique here exactly? Usually whenever I see a trigonometric function (eg $\sin$ or $\cos$ ), my instinct is to ignore the argument and focus on the range if possible (eg $[-1,1]$ ). However, that would seem to give me $-\infty < L < \infty$","For a beginning calculus student , prove I'm guessing this means something like Allowed : Pre-university maths, precalculus, basic calculus up to techniques of integration, Bernoulli's rule for sequences, two police persons theorem for sequences, if is continuous Not allowed : Monotone Convergence Theorem, Taylor series, polar coordinates and advanced stuff in real and complex analysis and the like (eg limsup, liminf, Stolz–Cesàro theorem, Cauchy sequences) What I tried: To use the two police persons theorem, I need to find some s.t. Questions: What can I use? How else can I approach this problem? Might I be able to say that because and for the same reason that justifies step 3 here ? So it's like ? What is the technique here exactly? Usually whenever I see a trigonometric function (eg or ), my instinct is to ignore the argument and focus on the range if possible (eg ). However, that would seem to give me","\lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = 2\pi \lim_{n \to \infty} f(a_n) = f(\lim_{n \to \infty} a_n) f \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \le \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi a_n \lim_{n \to \infty} a_n = 2\pi \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \ge a_n  a_n \ne \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) a_n \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) \color{red}{=} \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} = 2\pi \lim \sin(\pi/n) = \lim \pi/n \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \sin(\frac{\pi}{n}) = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = \lim_{n \to \infty} \frac{4n^3}{2n^2+1} \frac{\pi}{n} \lim_{n \to \infty} \frac{\sin(\frac{\pi}{n})}{\frac{\pi}{n}} = 2\pi (1) = 2\pi \sin \cos [-1,1] -\infty < L < \infty","['calculus', 'sequences-and-series', 'limits']"
19,Sequence bounded away from $0$ and $2$,Sequence bounded away from  and,0 2,"Suppose I have a sequence of real numbers $\{a_n\}_n$ and I'm told that $\{a_n\}_n$ is bounded away from $0$ and $2$. (1) What does it mean exactly? My thinking is that it means $a_n\neq 0$ and $a_n \neq 2$ $\forall n$. (2) Does it imply that $\lim_{n\rightarrow \infty}a_n \neq 0$ and $\lim_{n\rightarrow \infty}a_n \neq 2$ (assuming that the limit exists)? Question (2) is related to the discussion on rescaling rates at p.211 of van der Vaart ""Asymptotic Statistics"" point (iii) here where it seems that the answer to (2) is ""Yes""","Suppose I have a sequence of real numbers $\{a_n\}_n$ and I'm told that $\{a_n\}_n$ is bounded away from $0$ and $2$. (1) What does it mean exactly? My thinking is that it means $a_n\neq 0$ and $a_n \neq 2$ $\forall n$. (2) Does it imply that $\lim_{n\rightarrow \infty}a_n \neq 0$ and $\lim_{n\rightarrow \infty}a_n \neq 2$ (assuming that the limit exists)? Question (2) is related to the discussion on rescaling rates at p.211 of van der Vaart ""Asymptotic Statistics"" point (iii) here where it seems that the answer to (2) is ""Yes""",,['sequences-and-series']
20,Spivak's Limit Example,Spivak's Limit Example,,"Consider the function $f: (0, 1) \to \mathbb{Q}$ defined by $$f(x) = \begin{cases} 0, & x\text{ irrational} \\ 1/q, & x = p/q\text{ in lowest terms.} \end{cases}$$ The problem is to show that $\lim\limits_{x \to a}f(x) = 0$, using $\delta$-$\epsilon$, for all $a \in (0, 1)$. That is, for all $\epsilon >0$ there is a $\delta > 0$ such that for all $x$ satisfying $0 < |x - a| < \delta$, $|f(x)-0| <\epsilon$. If $x$ is irrational, this is trivial . Edit : as discussed in the comments, this isn't as trivial as I thought it was. But what if $x$ is rational? Then $f(x) = 1/q$ as shown above, so $$\left|\dfrac{1}{q}\right| < \epsilon\text{.}$$ I'm used to $\delta$-$\epsilon$ problems where I can ""work backwards"" to find $\delta$, but obviously $1/q$ isn't a function of $x$, and I'm lost as to what to do here. I sort of understand the discussion in Spivak, but I'm not quite getting how the discussion helps me find $\delta$, so I thought someone here could enlighten me. As mentioned in the comments, I don't agree that this is a duplicate.","Consider the function $f: (0, 1) \to \mathbb{Q}$ defined by $$f(x) = \begin{cases} 0, & x\text{ irrational} \\ 1/q, & x = p/q\text{ in lowest terms.} \end{cases}$$ The problem is to show that $\lim\limits_{x \to a}f(x) = 0$, using $\delta$-$\epsilon$, for all $a \in (0, 1)$. That is, for all $\epsilon >0$ there is a $\delta > 0$ such that for all $x$ satisfying $0 < |x - a| < \delta$, $|f(x)-0| <\epsilon$. If $x$ is irrational, this is trivial . Edit : as discussed in the comments, this isn't as trivial as I thought it was. But what if $x$ is rational? Then $f(x) = 1/q$ as shown above, so $$\left|\dfrac{1}{q}\right| < \epsilon\text{.}$$ I'm used to $\delta$-$\epsilon$ problems where I can ""work backwards"" to find $\delta$, but obviously $1/q$ isn't a function of $x$, and I'm lost as to what to do here. I sort of understand the discussion in Spivak, but I'm not quite getting how the discussion helps me find $\delta$, so I thought someone here could enlighten me. As mentioned in the comments, I don't agree that this is a duplicate.",,"['calculus', 'limits', 'epsilon-delta']"
21,"If $f(x.y)=f(x).f(y)$ for all $x,y$ and $f(x)$ is continuous at $x=1$,then show that $f(x)$ is continuous for all x except at $x=0$.Given $f(1)\neq 0$","If  for all  and  is continuous at ,then show that  is continuous for all x except at .Given","f(x.y)=f(x).f(y) x,y f(x) x=1 f(x) x=0 f(1)\neq 0","If $f(x.y)=f(x).f(y)$ for all $x,y$ and $f(x)$ is continuous at $x=1$,then show that $f(x)$ is continuous for all x except at $x=0$.Given $f(1)\neq 0$ In the functional equation $f(x.y)=f(x).f(y)$,put $x=1,y=1$ $f(1)=f(1).f(1)\Rightarrow f(1)=1$ because $f(1)\neq 0$ As $f(x)$ is continuous at $x=1$,so $\lim_{x\to 1}=f(1)=1$ Now take any number $x_0\neq 0$ Put $x=x_0$ in the equation $f(x.y)=f(x).f(y)$ $\lim_{x\to1}f(x_0.x)=f(x_0).f(x)=f(x_0)\lim_{x\to1}f(x)=f(x_0).1=f(x_0)$ So i proved $\lim_{x\to1}f(x_0.x)=f(x_0)$ Therefore $f(x)$ is continuous at all non zero numbers. But i do not know how to prove that $f(x)$ is not continuous at $x=0.$Please help me.Thanks.","If $f(x.y)=f(x).f(y)$ for all $x,y$ and $f(x)$ is continuous at $x=1$,then show that $f(x)$ is continuous for all x except at $x=0$.Given $f(1)\neq 0$ In the functional equation $f(x.y)=f(x).f(y)$,put $x=1,y=1$ $f(1)=f(1).f(1)\Rightarrow f(1)=1$ because $f(1)\neq 0$ As $f(x)$ is continuous at $x=1$,so $\lim_{x\to 1}=f(1)=1$ Now take any number $x_0\neq 0$ Put $x=x_0$ in the equation $f(x.y)=f(x).f(y)$ $\lim_{x\to1}f(x_0.x)=f(x_0).f(x)=f(x_0)\lim_{x\to1}f(x)=f(x_0).1=f(x_0)$ So i proved $\lim_{x\to1}f(x_0.x)=f(x_0)$ Therefore $f(x)$ is continuous at all non zero numbers. But i do not know how to prove that $f(x)$ is not continuous at $x=0.$Please help me.Thanks.",,"['limits', 'continuity']"
22,How to show that $x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k}$ converges?,How to show that  converges?,x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k},"How to show that $x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k}$ converges? My solution: I tried to use Cauchy convergence theorem. For any $\epsilon>0$, I need to find $N$ such that for all $n \geq m \geq N$, the inequality  $$  \left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|<\epsilon  $$ holds. We have $$  \left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|\\  =\left| -2\sum_{k=m+1}^{n} \frac{\sin(\frac{k}{2}+1)x \sin \frac{1}{2}x}{k} \right| \\ \leq \left| -2\sum_{k=m+1}^{n} \frac{1}{k} \right|\\ = \sum_{k=m+1}^{n} \frac{1}{k}. $$ But I am not it seems that only when $n$ is close to $m$, $\sum_{k=m+1}^{n} \frac{1}{k}$ is small. Thank you very much.","How to show that $x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k}$ converges? My solution: I tried to use Cauchy convergence theorem. For any $\epsilon>0$, I need to find $N$ such that for all $n \geq m \geq N$, the inequality  $$  \left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|<\epsilon  $$ holds. We have $$  \left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|\\  =\left| -2\sum_{k=m+1}^{n} \frac{\sin(\frac{k}{2}+1)x \sin \frac{1}{2}x}{k} \right| \\ \leq \left| -2\sum_{k=m+1}^{n} \frac{1}{k} \right|\\ = \sum_{k=m+1}^{n} \frac{1}{k}. $$ But I am not it seems that only when $n$ is close to $m$, $\sum_{k=m+1}^{n} \frac{1}{k}$ is small. Thank you very much.",,"['calculus', 'sequences-and-series', 'limits', 'trigonometry']"
23,The rule for evaluating limits of rational functions by dividing the coefficients of highest powers,The rule for evaluating limits of rational functions by dividing the coefficients of highest powers,,"I have a Limit problem as below: Connor claims: "" $\lim_{x\to \infty} \left(\frac{6x^2 + 7x +3}{2x^3 + x^2 -2x -1}\right) = 3$ because my high school calculus teacher told us the limit of ratio of polynomials is always the quotient of the coefficients of the highest power terms"" If correct, show in detail how to use algebra and the limit theorems to evaluate this limit and get the same answer If wrong, 1, use algebra and limit theorems to correctly evaluate the limit and 2, write a paragraph that explain why Connor shouldn't expect the rule he remember from high school to work in this particular problem I am able to correctly evaluate the limit ( lim = 0) but since my first language is not English, I don't understand what Connor claimed and how to explain it. Can anyone help? Thanks in advance","I have a Limit problem as below: Connor claims: "" $\lim_{x\to \infty} \left(\frac{6x^2 + 7x +3}{2x^3 + x^2 -2x -1}\right) = 3$ because my high school calculus teacher told us the limit of ratio of polynomials is always the quotient of the coefficients of the highest power terms"" If correct, show in detail how to use algebra and the limit theorems to evaluate this limit and get the same answer If wrong, 1, use algebra and limit theorems to correctly evaluate the limit and 2, write a paragraph that explain why Connor shouldn't expect the rule he remember from high school to work in this particular problem I am able to correctly evaluate the limit ( lim = 0) but since my first language is not English, I don't understand what Connor claimed and how to explain it. Can anyone help? Thanks in advance",,"['calculus', 'limits', 'rational-functions']"
24,Is the formula $\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)}$ a standard result?,Is the formula  a standard result?,\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)},Is the formula for the form $1^{\infty}$ that is  $$\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)}$$ a standard formula? Does it have any special name? Note:In the above formula $f(x)\to 0$ as $x\to a$ and $g(x)\to\infty$ as $x\to a$ I'm asking because I'm not sure whether it will be allowed in a subjective maths exam(it sometimes greatly simplifies calculations).Thanks.,Is the formula for the form $1^{\infty}$ that is  $$\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)}$$ a standard formula? Does it have any special name? Note:In the above formula $f(x)\to 0$ as $x\to a$ and $g(x)\to\infty$ as $x\to a$ I'm asking because I'm not sure whether it will be allowed in a subjective maths exam(it sometimes greatly simplifies calculations).Thanks.,,['limits']
25,Prove that $ \lim_\limits{x \to \infty} x(\sqrt{x^2+1}-x) $ = $ \frac{1}{2}$?,Prove that  = ?, \lim_\limits{x \to \infty} x(\sqrt{x^2+1}-x)   \frac{1}{2},How can you prove that $$ \lim_\limits{x \to \infty}  x(\sqrt{x^2+1}-x) =  \frac{1}{2} \text{ ?}$$ I can not find a way to calculate this. This is one idea : $$ \lim_{x \to \infty}  x(\sqrt{x^2+1}-x) \approx \lim_\limits{x \to \infty}  x(\sqrt{x^2}-x) = 0 $$ but that is wrong.,How can you prove that $$ \lim_\limits{x \to \infty}  x(\sqrt{x^2+1}-x) =  \frac{1}{2} \text{ ?}$$ I can not find a way to calculate this. This is one idea : $$ \lim_{x \to \infty}  x(\sqrt{x^2+1}-x) \approx \lim_\limits{x \to \infty}  x(\sqrt{x^2}-x) = 0 $$ but that is wrong.,,"['limits', 'radicals', 'limits-without-lhopital']"
26,Limiting value of $\frac{x^n e^x}{n!}$ as $n\to\infty$,Limiting value of  as,\frac{x^n e^x}{n!} n\to\infty,"For the Taylor Series the remainder is of the form $$R_n = \frac{(x-a)^n}{n!} f^{(n)}(\xi) $$ with $a \leq \xi \leq x$ For the series of $e^x$ about $0$ (that is, the Maclaurin series) the remainder is  $R_n = \frac{x^n}{n!}e^{\xi}$ with $0 \leq \xi \leq x$. Now, I have to prove that as $n \to \infty$ this $R_n \to 0$. How do I go about it? My problem is that I know nothing about $x$ and whether it is bigger or smaller than $n$. In short, how do I prove  $$\lim_{n\to\infty}\frac{x^n}{n!}e^{\xi}=0$$","For the Taylor Series the remainder is of the form $$R_n = \frac{(x-a)^n}{n!} f^{(n)}(\xi) $$ with $a \leq \xi \leq x$ For the series of $e^x$ about $0$ (that is, the Maclaurin series) the remainder is  $R_n = \frac{x^n}{n!}e^{\xi}$ with $0 \leq \xi \leq x$. Now, I have to prove that as $n \to \infty$ this $R_n \to 0$. How do I go about it? My problem is that I know nothing about $x$ and whether it is bigger or smaller than $n$. In short, how do I prove  $$\lim_{n\to\infty}\frac{x^n}{n!}e^{\xi}=0$$",,"['sequences-and-series', 'limits', 'taylor-expansion', 'exponential-function']"
27,Why are the two limits equal?,Why are the two limits equal?,,"I want to show that if $g$ is continuous at $a$ and $f$ at $g(a)$, then $$\lim_{x \to a}{\frac{f(g(x))-f(g(a))}{g(x)-g(a)}} = \lim_{x \to g(a)}{\frac{f(x)-f(g(a))}{x-g(a)}}$$ Now I know that continuity implies that  $$\lim_{x \to a}{f(g(x))} = \lim_{x \to g(a)}{f(x)} = f(g(a))$$ and $$\lim_{x \to a}{g(x)} = \lim_{x \to g(a)}{x} = g(a)$$ so it is quite easy to see that the two original limits are equal. How do I prove this? I cannot repeatedly use the limit laws since I get a limit that is zero ($\lim_{x \to a}{(g(x)-g(a))}$) and so cannot apply the quotient rule.","I want to show that if $g$ is continuous at $a$ and $f$ at $g(a)$, then $$\lim_{x \to a}{\frac{f(g(x))-f(g(a))}{g(x)-g(a)}} = \lim_{x \to g(a)}{\frac{f(x)-f(g(a))}{x-g(a)}}$$ Now I know that continuity implies that  $$\lim_{x \to a}{f(g(x))} = \lim_{x \to g(a)}{f(x)} = f(g(a))$$ and $$\lim_{x \to a}{g(x)} = \lim_{x \to g(a)}{x} = g(a)$$ so it is quite easy to see that the two original limits are equal. How do I prove this? I cannot repeatedly use the limit laws since I get a limit that is zero ($\lim_{x \to a}{(g(x)-g(a))}$) and so cannot apply the quotient rule.",,"['calculus', 'limits']"
28,Counter exchanging limit and integral,Counter exchanging limit and integral,,"Background I came across this answer on Math SE which claimed it made a lot of sense to switch limit and integral. In response I came up with the following counter-examples: $\lim_{w \to 0} \int_0^\infty we^{-wt}\ dt = 1$ but $\int_0^\infty \lim_{w \to 0} we^{-wt}\ dt = 0$. $\lim_{n \to \infty} \int_0^\infty \frac{t^{n+1}}{n!} e^{-t}\ dt = \infty$ but $\int_0^\infty \lim_{n \to \infty} \frac{t^{n+1}}{n!} e^{-t}\ dt = 0$. $\lim_{n \to \infty} \int_0^1 n^3 t^n (1-t)\ dt = \infty$ but $\int_0^1 \lim_{n \to \infty} n^3 t^n (1-t)\ dt = 0$. $\lim_{r \to \infty} \int_{-\infty}^\infty \frac{r(rx)^3}{(rx)^4-(rx)^3+1}\ dx = \lim_{r \to \infty} \int_{-\infty}^\infty \frac{y^3}{y^4-y^3+1}\ dy \approx 2$ and $\lim_{r \to \infty} \frac{r(rx)^3}{(rx)^4-(rx)^3+1} = \frac{1}{x}$ for any $x \ne 0$. [Newly added] Unlike the claimed method in that post, the first three counter-examples have continuous limit functions. The third has the advantage of having the integral over a finite interval, to show that interchanging is still not valid without additional assumptions on the function sequence. The fourth uses exactly the same technique as the claimed method (with a discontinuous limit function) and also gives an integral that is constant and independent of the limit! Of course, these examples do not satisfy the dominated convergence theorem in various ways. Question Do you have any good counter-examples that actually show up in real-world problems and have an experimentally measurable implication for the real world? I somehow get the impression that the functions in the real-world might be too nice (wave functions are infinitely differentiable when you really factor everything in; in particular there are no such things as square potential wells because it is impossible to have a discontinuous change in potential!) I would be satisfied with examples in classical mechanics if the effect is measurable.","Background I came across this answer on Math SE which claimed it made a lot of sense to switch limit and integral. In response I came up with the following counter-examples: $\lim_{w \to 0} \int_0^\infty we^{-wt}\ dt = 1$ but $\int_0^\infty \lim_{w \to 0} we^{-wt}\ dt = 0$. $\lim_{n \to \infty} \int_0^\infty \frac{t^{n+1}}{n!} e^{-t}\ dt = \infty$ but $\int_0^\infty \lim_{n \to \infty} \frac{t^{n+1}}{n!} e^{-t}\ dt = 0$. $\lim_{n \to \infty} \int_0^1 n^3 t^n (1-t)\ dt = \infty$ but $\int_0^1 \lim_{n \to \infty} n^3 t^n (1-t)\ dt = 0$. $\lim_{r \to \infty} \int_{-\infty}^\infty \frac{r(rx)^3}{(rx)^4-(rx)^3+1}\ dx = \lim_{r \to \infty} \int_{-\infty}^\infty \frac{y^3}{y^4-y^3+1}\ dy \approx 2$ and $\lim_{r \to \infty} \frac{r(rx)^3}{(rx)^4-(rx)^3+1} = \frac{1}{x}$ for any $x \ne 0$. [Newly added] Unlike the claimed method in that post, the first three counter-examples have continuous limit functions. The third has the advantage of having the integral over a finite interval, to show that interchanging is still not valid without additional assumptions on the function sequence. The fourth uses exactly the same technique as the claimed method (with a discontinuous limit function) and also gives an integral that is constant and independent of the limit! Of course, these examples do not satisfy the dominated convergence theorem in various ways. Question Do you have any good counter-examples that actually show up in real-world problems and have an experimentally measurable implication for the real world? I somehow get the impression that the functions in the real-world might be too nice (wave functions are infinitely differentiable when you really factor everything in; in particular there are no such things as square potential wells because it is impossible to have a discontinuous change in potential!) I would be satisfied with examples in classical mechanics if the effect is measurable.",,"['integration', 'limits', 'physics', 'examples-counterexamples']"
29,Calculating the radius of convergence of a series.,Calculating the radius of convergence of a series.,,"Let $d_n$ denote the number of divisors of $n^{50}$ then determine the radius of convergence of the series $\sum\limits_{n=1}^{\infty}d_nx^n$. So obviously we need to calculate the limit of $\frac{d_{n+1}}{d_n}$. I am guessing I need some information about the asymptotic behavior of $d_n$. Any help? The options given are $1 ,0 , 50 ,\frac{1}{50}$","Let $d_n$ denote the number of divisors of $n^{50}$ then determine the radius of convergence of the series $\sum\limits_{n=1}^{\infty}d_nx^n$. So obviously we need to calculate the limit of $\frac{d_{n+1}}{d_n}$. I am guessing I need some information about the asymptotic behavior of $d_n$. Any help? The options given are $1 ,0 , 50 ,\frac{1}{50}$",,"['calculus', 'real-analysis', 'elementary-number-theory', 'limits', 'power-series']"
30,For what value of $\alpha$ is the limit $\lim{x\to 0} \frac{\arctan(x) \cdot \log(\sin(x)) - x \cdot \log(x) }{x^\alpha}$ finite and non-zero?,For what value of  is the limit  finite and non-zero?,\alpha \lim{x\to 0} \frac{\arctan(x) \cdot \log(\sin(x)) - x \cdot \log(x) }{x^\alpha},"My brother asked me this question and I found it very difficult. For which value of $\alpha$ is the limit  $$ \lim_{x\to0} \frac{\arctan(x) \cdot \log(\sin(x)) - x \cdot \log(x) }{x^\alpha} $$ finite and different from zero? The thing that confuses me is that I can't use McLaurin for $\log x$, because it is not defined in 0. Is the question really hard or am I missing something?","My brother asked me this question and I found it very difficult. For which value of $\alpha$ is the limit  $$ \lim_{x\to0} \frac{\arctan(x) \cdot \log(\sin(x)) - x \cdot \log(x) }{x^\alpha} $$ finite and different from zero? The thing that confuses me is that I can't use McLaurin for $\log x$, because it is not defined in 0. Is the question really hard or am I missing something?",,"['calculus', 'limits']"
31,Show that this integral is finite $\lim_n \int_0^n x^p (\ln x)^r \left(1 - \frac{x}{n} \right)^n dx$,Show that this integral is finite,\lim_n \int_0^n x^p (\ln x)^r \left(1 - \frac{x}{n} \right)^n dx,"Let $p > -1$ and $r \in \mathbb{N}$ , show that $$\lim_n \int_0^n x^p (\ln x)^r \left(1 - \frac{x}{n} \right)^n dx =  \int_0^\infty x^p (\ln x)^r e^{-x} dx$$ and that this integral is finite. To solve that problem, I wanted to use Lebesgue's dominated convergence theorem. Therefore, I set $f_n(x) = x^p (\ln x )^r \left( 1 - \frac{x}{n} \right)^n$ . Then we clearly have that $\lim_n f_n(x) = x^p (\ln x)^r e^{-x}$ . Plus $f_n$ are measurable for all $n$ . Thus the remaining problem is to find an integrable function $g$ such that $|f_n(x)| \leq g(x)$ almost everywhere (I don't know if it is said like that in english). That is where I'm stuck, I can't find such a function. At first I wanted to use that $\ln(x) \leq x\ \forall x \geq 0$ , thus $f(x) \leq x^{p+r}e^{-x}$ where $f(x) = \lim_n f_n(x)$ but it doesn't seem to lead anywhere. Any help would be appreciated. Thanks in advance.","Let and , show that and that this integral is finite. To solve that problem, I wanted to use Lebesgue's dominated convergence theorem. Therefore, I set . Then we clearly have that . Plus are measurable for all . Thus the remaining problem is to find an integrable function such that almost everywhere (I don't know if it is said like that in english). That is where I'm stuck, I can't find such a function. At first I wanted to use that , thus where but it doesn't seem to lead anywhere. Any help would be appreciated. Thanks in advance.",p > -1 r \in \mathbb{N} \lim_n \int_0^n x^p (\ln x)^r \left(1 - \frac{x}{n} \right)^n dx =  \int_0^\infty x^p (\ln x)^r e^{-x} dx f_n(x) = x^p (\ln x )^r \left( 1 - \frac{x}{n} \right)^n \lim_n f_n(x) = x^p (\ln x)^r e^{-x} f_n n g |f_n(x)| \leq g(x) \ln(x) \leq x\ \forall x \geq 0 f(x) \leq x^{p+r}e^{-x} f(x) = \lim_n f_n(x),"['real-analysis', 'integration', 'limits', 'lebesgue-integral']"
32,Prove $\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0$ directly from the definition of limit.,Prove  directly from the definition of limit.,\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0,"One of my homework questions is: Prove $\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0$ directly   from the definition of limit. In trying to follow: Prove that $\lim \limits_{n\to\infty}\frac{n}{n^2+1}  = 0$ from the definition , i have so far: Proof: It must be shown that for any $\epsilon>0$, there exists an integer $N$ such that $\left|\frac{2n^2+2}{3n^3+1}-0\right| <\epsilon$ whenever $n>N$. $\left|\frac{2n^2+2}{3n^3+1}-0\right|=\frac{2n^2+2}{3n^3+1}>\frac{2n^2}{3n^3}=\frac{2}{3n}$. Because in the problem im given, the left side of the inequality that im trying to simplify i guess, is instead greater than the right side, im stuck. And i dont understand the how or why behind the algebraic manipulations that the original author and one of the answers performs, in the linked question: $$\left|\frac{n}{n^2 + 1}\right| < \epsilon \text{ whenever }x \gt M.$$ $$ n \lt \epsilon(n^2 + 1) $$ $$n \lt  \epsilon n^2 + \epsilon$$ Truly do feel clueless at this point!","One of my homework questions is: Prove $\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0$ directly   from the definition of limit. In trying to follow: Prove that $\lim \limits_{n\to\infty}\frac{n}{n^2+1}  = 0$ from the definition , i have so far: Proof: It must be shown that for any $\epsilon>0$, there exists an integer $N$ such that $\left|\frac{2n^2+2}{3n^3+1}-0\right| <\epsilon$ whenever $n>N$. $\left|\frac{2n^2+2}{3n^3+1}-0\right|=\frac{2n^2+2}{3n^3+1}>\frac{2n^2}{3n^3}=\frac{2}{3n}$. Because in the problem im given, the left side of the inequality that im trying to simplify i guess, is instead greater than the right side, im stuck. And i dont understand the how or why behind the algebraic manipulations that the original author and one of the answers performs, in the linked question: $$\left|\frac{n}{n^2 + 1}\right| < \epsilon \text{ whenever }x \gt M.$$ $$ n \lt \epsilon(n^2 + 1) $$ $$n \lt  \epsilon n^2 + \epsilon$$ Truly do feel clueless at this point!",,"['real-analysis', 'limits', 'epsilon-delta']"
33,Trying to solve a limit without Taylor series [don't put on hold] [closed],Trying to solve a limit without Taylor series [don't put on hold] [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question For instance in my recent post: I have this limit to find $$\lim_{n\to \infty }\left(1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots+\left(-1\right)^{n-1}\cdot \frac{1}{2n-1}\right)=\text{ ?}$$ and we know too this integral $$I_n=\int _0^1\:\frac{x^n}{x^2+1}dx$$ and that relation for recurrence: $$I_{2n}\:\cdot \:\left(-1\right)^{n-1}\:=\:\frac{\left(-1\right)^{n-1}}{2n-1}-\:\left(-1\right)^{n-1}\cdot I_{2n-2}$$ Okay and now how we can continue, if we know recurrence relation because my teacher adviced me to use this and I don't know what helps me, because if I put value for n>2, I'll find some terms and I need to find the sums... okay so tell me someone if that recurrence relation can helps me and if not put on hold...","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question For instance in my recent post: I have this limit to find $$\lim_{n\to \infty }\left(1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots+\left(-1\right)^{n-1}\cdot \frac{1}{2n-1}\right)=\text{ ?}$$ and we know too this integral $$I_n=\int _0^1\:\frac{x^n}{x^2+1}dx$$ and that relation for recurrence: $$I_{2n}\:\cdot \:\left(-1\right)^{n-1}\:=\:\frac{\left(-1\right)^{n-1}}{2n-1}-\:\left(-1\right)^{n-1}\cdot I_{2n-2}$$ Okay and now how we can continue, if we know recurrence relation because my teacher adviced me to use this and I don't know what helps me, because if I put value for n>2, I'll find some terms and I need to find the sums... okay so tell me someone if that recurrence relation can helps me and if not put on hold...",,"['calculus', 'real-analysis', 'integration', 'limits']"
34,Limit $\lim_{x \to +\infty}\left(x^\frac{7}{6}-x^\frac{6}{7}\cdot \ln^2( x) \right)$ using L'Hôpital's rule.,Limit  using L'Hôpital's rule.,\lim_{x \to +\infty}\left(x^\frac{7}{6}-x^\frac{6}{7}\cdot \ln^2( x) \right),"$$\lim_{x \to +\infty}\left(x^\frac{7}{6}-x^\frac{6}{7}\cdot \ln^2( x) \right)$$ I can not decide the limit. I understand that it is necessary to apply L'Hôpital's rule, when there will be a fraction. But to start, how to make this shot in this example? Please help me to solve it L'Hospital's rule!","$$\lim_{x \to +\infty}\left(x^\frac{7}{6}-x^\frac{6}{7}\cdot \ln^2( x) \right)$$ I can not decide the limit. I understand that it is necessary to apply L'Hôpital's rule, when there will be a fraction. But to start, how to make this shot in this example? Please help me to solve it L'Hospital's rule!",,['limits']
35,Proving infinity limit of a multivariable function,Proving infinity limit of a multivariable function,,"I have a function $f(x,y,z): \mathbb{R}^3\rightarrow \mathbb{R}$ which is $f(x,y,z)=3x^2+z^2+y^2-2xy+14$. I'm trying to show that $f(x,y,z)\rightarrow \infty$ when $||(x,y,z)|| \rightarrow \infty$. (Or formally: $\forall_{M>0}\exists_{R>0}\forall_{(x,y,z) \in \mathbb{R}^3}:||(x,y,z)||>R \rightarrow f(x,y,z)>M $) So in the process of building the proof I'm trying to find a suitable $R$, and for that I'm trying to express $f(x,y,z)$ in terms of $||(x,y,z)||$ or $||(x,y,z)||^2$. Try 1 : Let $(x,y,z)$ be such that $||(x,y,z)||>R$, then $f(x,y,z) = 3x^2+z^2+y^2-2xy+14$ $= x^2+z^2+y^2 +2x^2-2xy+14 \geq R^2+2(x^2-xy)+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2})+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2+z^2})+14 $ $\geq R^2-2(\sqrt{2x^2} R)+14 = R^2-2\sqrt{2}|x|R+14$. From which I can't continue since if I try to bound $x$ by $R$ I get a negative expression. Try 2 : Using $xy \leq \frac{x^2+y^2}{2}$, we get that $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 \geq 2x^2+z^2+14$, from which I can't proceed since I can't express $R$ without $y$. Try 3 : Let $R=\sqrt{3}M$, and since $||(x,y,z)||>R$ then $x^2+y^2+x^2>R^2=3M^2$. Meaning that at least one of $x^2$, $y^2$, $z^2$ has to be greater or equal $M^2$. $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 = 2x^2+z^2+(y-x)^2+14$. If $x^2>M^2$ or $z^2>M^2$ then obviously $f(x,y,z)>M$ and we're done. But I don't know how to handle the case of $y^2>M^2$. Any help would be appreciated!","I have a function $f(x,y,z): \mathbb{R}^3\rightarrow \mathbb{R}$ which is $f(x,y,z)=3x^2+z^2+y^2-2xy+14$. I'm trying to show that $f(x,y,z)\rightarrow \infty$ when $||(x,y,z)|| \rightarrow \infty$. (Or formally: $\forall_{M>0}\exists_{R>0}\forall_{(x,y,z) \in \mathbb{R}^3}:||(x,y,z)||>R \rightarrow f(x,y,z)>M $) So in the process of building the proof I'm trying to find a suitable $R$, and for that I'm trying to express $f(x,y,z)$ in terms of $||(x,y,z)||$ or $||(x,y,z)||^2$. Try 1 : Let $(x,y,z)$ be such that $||(x,y,z)||>R$, then $f(x,y,z) = 3x^2+z^2+y^2-2xy+14$ $= x^2+z^2+y^2 +2x^2-2xy+14 \geq R^2+2(x^2-xy)+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2})+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2+z^2})+14 $ $\geq R^2-2(\sqrt{2x^2} R)+14 = R^2-2\sqrt{2}|x|R+14$. From which I can't continue since if I try to bound $x$ by $R$ I get a negative expression. Try 2 : Using $xy \leq \frac{x^2+y^2}{2}$, we get that $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 \geq 2x^2+z^2+14$, from which I can't proceed since I can't express $R$ without $y$. Try 3 : Let $R=\sqrt{3}M$, and since $||(x,y,z)||>R$ then $x^2+y^2+x^2>R^2=3M^2$. Meaning that at least one of $x^2$, $y^2$, $z^2$ has to be greater or equal $M^2$. $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 = 2x^2+z^2+(y-x)^2+14$. If $x^2>M^2$ or $z^2>M^2$ then obviously $f(x,y,z)>M$ and we're done. But I don't know how to handle the case of $y^2>M^2$. Any help would be appreciated!",,"['limits', 'multivariable-calculus', 'functions']"
36,Picking a $\delta$ for a convenient $\varepsilon$?,Picking a  for a convenient ?,\delta \varepsilon,"I'm studying some proofs in a book from the library  and  unlike my own book, it doesn't prove by showing that a certain expression is striclty less than $\varepsilon$ (given an  appropiate $\delta$ and $\vert x-a\vert < \delta$, etc). Instead, it shows that it's less than some expression involving $\varepsilon$. E.g. Given $\varepsilon > 0$, we can pick a $\delta > 0$ so $|f(x) - g(a)| <  \text{insert stuff} < k \cdot \varepsilon$ when $|x-a| < \delta$, and thus $f(x) - g(a) \to 0 $ when $x \rightarrow a$. How does this make sense formally? (Intuitively, I am convinced.) I don't have much experience with $\varepsilon$-$\delta$ -proofs, so I might be missing something obvious.","I'm studying some proofs in a book from the library  and  unlike my own book, it doesn't prove by showing that a certain expression is striclty less than $\varepsilon$ (given an  appropiate $\delta$ and $\vert x-a\vert < \delta$, etc). Instead, it shows that it's less than some expression involving $\varepsilon$. E.g. Given $\varepsilon > 0$, we can pick a $\delta > 0$ so $|f(x) - g(a)| <  \text{insert stuff} < k \cdot \varepsilon$ when $|x-a| < \delta$, and thus $f(x) - g(a) \to 0 $ when $x \rightarrow a$. How does this make sense formally? (Intuitively, I am convinced.) I don't have much experience with $\varepsilon$-$\delta$ -proofs, so I might be missing something obvious.",,"['analysis', 'limits', 'epsilon-delta']"
37,Prove $\lim\limits_{n \to \infty} \sup \left ( \frac{(2n - 1)^{2n - 1}}{2^{2n} (2n)!)} \right ) ^ {\frac 1 n} = \frac {e^2} 4$,Prove,\lim\limits_{n \to \infty} \sup \left ( \frac{(2n - 1)^{2n - 1}}{2^{2n} (2n)!)} \right ) ^ {\frac 1 n} = \frac {e^2} 4,"This is a problem in Heuer (2009) ""Lerbuch der Analysis Teil 1"" on page 366. I assume that the proof should use $e = \sum\limits_{k = 0}^{\infty} \frac 1 {k!}$, but I cannot come further.","This is a problem in Heuer (2009) ""Lerbuch der Analysis Teil 1"" on page 366. I assume that the proof should use $e = \sum\limits_{k = 0}^{\infty} \frac 1 {k!}$, but I cannot come further.",,"['real-analysis', 'limits', 'power-series', 'self-learning']"
38,"Prove the sequence determined by $a_{n+1}={a_n\over \sin a_n}$ is convergent, and found its limit.","Prove the sequence determined by  is convergent, and found its limit.",a_{n+1}={a_n\over \sin a_n},"Let $\{a_n\}$ be a sequence defined by $0<a_1<{\pi \over 2}$, $a_{n+1}={a_n\over \sin a_n}$. $Attempt:$  $a_1>0$ and $\sin a_1>0$  and therefore the sequence begins positive and remains positive since sine preserves sign. $a_{n+1}-a_n={a_n\over \sin a_n}-a_n=a_n({1-\sin a_n\over \sin a_n})$. By showing $0<a_n<{\pi \over 2}$ I can prove that the sequence is bounded, monotonically increasing and its supremum, ${\pi\over 2}$ is also the limit. Except I don't know how to show those steps. I would appreciate your reply.","Let $\{a_n\}$ be a sequence defined by $0<a_1<{\pi \over 2}$, $a_{n+1}={a_n\over \sin a_n}$. $Attempt:$  $a_1>0$ and $\sin a_1>0$  and therefore the sequence begins positive and remains positive since sine preserves sign. $a_{n+1}-a_n={a_n\over \sin a_n}-a_n=a_n({1-\sin a_n\over \sin a_n})$. By showing $0<a_n<{\pi \over 2}$ I can prove that the sequence is bounded, monotonically increasing and its supremum, ${\pi\over 2}$ is also the limit. Except I don't know how to show those steps. I would appreciate your reply.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
39,Evaluating $\lim_{x\to0} \frac{(1+x)^{1/x}-e}{x}$ [duplicate],Evaluating  [duplicate],\lim_{x\to0} \frac{(1+x)^{1/x}-e}{x},This question already has answers here : Need hint for $\lim_{x\to 0} \frac{(x+1)^\frac{1}{x}-e}{x}$ [duplicate] (3 answers) Closed 9 years ago . Evaluate $$\lim_{x\to0}  \frac{(1+x)^{1/x}-e}{x}$$,This question already has answers here : Need hint for $\lim_{x\to 0} \frac{(x+1)^\frac{1}{x}-e}{x}$ [duplicate] (3 answers) Closed 9 years ago . Evaluate $$\lim_{x\to0}  \frac{(1+x)^{1/x}-e}{x}$$,,"['calculus', 'limits']"
40,"Limit $ \lim\limits_{n\to\infty}\Bigl[\frac{1}{2^{n/2}\Gamma(n/2)} \int_{n-\sqrt{2n}}^{\infty} t^{\frac{n}{2}-1}e^{\frac{-t}{2}}\,dt\Bigr]$",Limit," \lim\limits_{n\to\infty}\Bigl[\frac{1}{2^{n/2}\Gamma(n/2)} \int_{n-\sqrt{2n}}^{\infty} t^{\frac{n}{2}-1}e^{\frac{-t}{2}}\,dt\Bigr]","Find $ \lim\limits_{n\to\infty}\left[\dfrac{1}{2^{n/2}\Gamma(n/2)} \displaystyle \int_{n-\sqrt{2n}}^{\infty} t^{\frac{n}{2}-1}e^{\frac{-t}{2}}\,dt\right]$ This looks like the p.d.f. of a chi-square distribution. With limit $n\to \infty$, this should be a normal distribution. So, should this be 1 (like the area under the curve) ? What does $n-\sqrt{2n}$ mean here ? Please help.","Find $ \lim\limits_{n\to\infty}\left[\dfrac{1}{2^{n/2}\Gamma(n/2)} \displaystyle \int_{n-\sqrt{2n}}^{\infty} t^{\frac{n}{2}-1}e^{\frac{-t}{2}}\,dt\right]$ This looks like the p.d.f. of a chi-square distribution. With limit $n\to \infty$, this should be a normal distribution. So, should this be 1 (like the area under the curve) ? What does $n-\sqrt{2n}$ mean here ? Please help.",,"['limits', 'statistics', 'probability-distributions', 'probability-limit-theorems']"
41,Tree of Primitive Pythagorean Triples graph scale infinite series,Tree of Primitive Pythagorean Triples graph scale infinite series,,"Consider the tree of primitive Pythagorean triples as seen here: https://en.wikipedia.org/wiki/Tree_of_primitive_Pythagorean_triples Consider the values for c in the triples (a, b, c) and their density in each generation layer of the tree. Taking the number of c's in any generation and dividing by the difference between the maximum and minimum values for c in that generation gives the density of that generation. The 'scale' of the graph is given by the ratio of densities as the generations go on to infinity. I have calculated the maximum, minimum and number of c's in increasing generations of the tree and the 'scale' asymptotes to 0.5147186257… by calculation. Note: This is a graph scaling question and is not the same question as the simple density of primitive triples as solved by Lehmer (1900). I postulate that this graph scaling value is in fact 3(3-2sqrt(2)). Can this be proved?","Consider the tree of primitive Pythagorean triples as seen here: https://en.wikipedia.org/wiki/Tree_of_primitive_Pythagorean_triples Consider the values for c in the triples (a, b, c) and their density in each generation layer of the tree. Taking the number of c's in any generation and dividing by the difference between the maximum and minimum values for c in that generation gives the density of that generation. The 'scale' of the graph is given by the ratio of densities as the generations go on to infinity. I have calculated the maximum, minimum and number of c's in increasing generations of the tree and the 'scale' asymptotes to 0.5147186257… by calculation. Note: This is a graph scaling question and is not the same question as the simple density of primitive triples as solved by Lehmer (1900). I postulate that this graph scaling value is in fact 3(3-2sqrt(2)). Can this be proved?",,"['number-theory', 'limits', 'pythagorean-triples']"
42,"Is it true for every sequence $a_n$ that if $\sum a_n$ is absolutely convergent, then $\sum (-1)^n a_n$ is convergent?","Is it true for every sequence  that if  is absolutely convergent, then  is convergent?",a_n \sum a_n \sum (-1)^n a_n,The problem is in the title. I must answer the question whether it's true for every sequence $\{a_n\}_{n\geq 1}$ that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent then $\sum_{n=1}^{\infty} (-1)^n a_n$ is convergent. Here is what I came up with: $\sum_{n=1}^{\infty} |(-1)^n a_n|=\sum_{n=1}^{\infty} |a_n|$ which is convergent by assumption. So  $\sum_{n=1}^{\infty} (-1)^n a_n$ is absolutely convergent which implies that it is convergent as well. Is it correct?,The problem is in the title. I must answer the question whether it's true for every sequence $\{a_n\}_{n\geq 1}$ that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent then $\sum_{n=1}^{\infty} (-1)^n a_n$ is convergent. Here is what I came up with: $\sum_{n=1}^{\infty} |(-1)^n a_n|=\sum_{n=1}^{\infty} |a_n|$ which is convergent by assumption. So  $\sum_{n=1}^{\infty} (-1)^n a_n$ is absolutely convergent which implies that it is convergent as well. Is it correct?,,"['calculus', 'sequences-and-series', 'limits']"
43,How find this sum $\sum_{k=1}^{\infty}\frac{1}{1+a_{k}}$,How find this sum,\sum_{k=1}^{\infty}\frac{1}{1+a_{k}},"Let $\{a_{n}\}$ be the sequence of real numbers defined by $a_{1}=3$ and for all $n\ge 1$,   $$a_{n+1}=\dfrac{1}{2}(a^2_{n}+1)$$ Evaluate    $$\sum_{k=1}^{\infty}\dfrac{1}{1+a_{k}}$$ My idea 1: since  $$2a_{n+1}=a^2_{n}+1$$ so we have $$2(a_{n+1}-1)=(a_{n}+1)(a_{n}-1)\Longrightarrow \dfrac{1}{1+a_{n}}=2\cdot\dfrac{a_{n}-1}{a_{n+1}-1}$$ so we must find this sum $$\sum_{n=1}^{\infty}\dfrac{1}{1+a_{n}}=\sum_{n=1}^{\infty}\dfrac{2(a_{n}-1)}{a_{n+1}-1}$$ then I can't find this sum other idea: maybe we can find this $a_{n}$ closed form?  $$2a_{n+1}-1=(a_{n})^2$$ I want let $a_{n}=\cos^2{b_{n}}$,so  $$\cos{2b_{n+1}}=(\cos{b_{n}})^4$$ then I can't follow works.","Let $\{a_{n}\}$ be the sequence of real numbers defined by $a_{1}=3$ and for all $n\ge 1$,   $$a_{n+1}=\dfrac{1}{2}(a^2_{n}+1)$$ Evaluate    $$\sum_{k=1}^{\infty}\dfrac{1}{1+a_{k}}$$ My idea 1: since  $$2a_{n+1}=a^2_{n}+1$$ so we have $$2(a_{n+1}-1)=(a_{n}+1)(a_{n}-1)\Longrightarrow \dfrac{1}{1+a_{n}}=2\cdot\dfrac{a_{n}-1}{a_{n+1}-1}$$ so we must find this sum $$\sum_{n=1}^{\infty}\dfrac{1}{1+a_{n}}=\sum_{n=1}^{\infty}\dfrac{2(a_{n}-1)}{a_{n+1}-1}$$ then I can't find this sum other idea: maybe we can find this $a_{n}$ closed form?  $$2a_{n+1}-1=(a_{n})^2$$ I want let $a_{n}=\cos^2{b_{n}}$,so  $$\cos{2b_{n+1}}=(\cos{b_{n}})^4$$ then I can't follow works.",,"['calculus', 'analysis', 'limits', 'summation']"
44,Finding $\lim_{y\to -2}\left(\frac{y^3+8}{y+2}\right)$,Finding,\lim_{y\to -2}\left(\frac{y^3+8}{y+2}\right),Here's my work. $$\begin{align} \lim_{y\to -2} \;\dfrac{y^3+8}{y+2} &= \lim_{y \to -2}\;\require{cancel}\dfrac{(\cancel{y +2})(y^2 - 2y + 4)}{\cancel{y + 2}}\\ \\ & = \lim_{y \to -2}\;\; y^2 - 2y + 4 \\ \\ & = 4 + 4 + 4 \\ \\ & = 12\end{align}$$ The answer book says that the correct answer is 4 What did I do wrong??,Here's my work. $$\begin{align} \lim_{y\to -2} \;\dfrac{y^3+8}{y+2} &= \lim_{y \to -2}\;\require{cancel}\dfrac{(\cancel{y +2})(y^2 - 2y + 4)}{\cancel{y + 2}}\\ \\ & = \lim_{y \to -2}\;\; y^2 - 2y + 4 \\ \\ & = 4 + 4 + 4 \\ \\ & = 12\end{align}$$ The answer book says that the correct answer is 4 What did I do wrong??,,"['calculus', 'limits', 'solution-verification']"
45,Evaluate $\lim_{x\to\infty}\left[x - \sqrt[n]{(x - a_1)(x - a_2)\ldots(x - a_n)}\right]$ [duplicate],Evaluate  [duplicate],\lim_{x\to\infty}\left[x - \sqrt[n]{(x - a_1)(x - a_2)\ldots(x - a_n)}\right],This question already has answers here : How to find $\lim_{n \rightarrow +\infty } \left(\sqrt[m]{\prod_{i=1}^{m}(n+{a}_{i})}-n\right)$? (3 answers) Closed 8 years ago . Evaluate the following the limit: $$\lim_{x\to\infty}\left[x - \sqrt[n]{(x - a_1)(x - a_2)\ldots(x - a_n)}\right]$$ I tried expressing the limit in the form $f(x)g(x)\left[\frac{1}{f(x)} - \frac{1}{g(x)}\right]$ but it did not help.,This question already has answers here : How to find $\lim_{n \rightarrow +\infty } \left(\sqrt[m]{\prod_{i=1}^{m}(n+{a}_{i})}-n\right)$? (3 answers) Closed 8 years ago . Evaluate the following the limit: $$\lim_{x\to\infty}\left[x - \sqrt[n]{(x - a_1)(x - a_2)\ldots(x - a_n)}\right]$$ I tried expressing the limit in the form $f(x)g(x)\left[\frac{1}{f(x)} - \frac{1}{g(x)}\right]$ but it did not help.,,['limits']
46,Evaluating $\lim_{x \to 0}\frac{(1+x)^{1/x} - e}{x}$ [duplicate],Evaluating  [duplicate],\lim_{x \to 0}\frac{(1+x)^{1/x} - e}{x},This question already has answers here : How to solve this limit: $\lim\limits_{x\to0}\frac{(1+x)^{1/x}-e}x$? [duplicate] (4 answers) Closed 4 years ago . How to evaluate the following limit? $$\lim_{x \to 0}\frac{(1+x)^{1/x} - e}{x}.$$,This question already has answers here : How to solve this limit: $\lim\limits_{x\to0}\frac{(1+x)^{1/x}-e}x$? [duplicate] (4 answers) Closed 4 years ago . How to evaluate the following limit? $$\lim_{x \to 0}\frac{(1+x)^{1/x} - e}{x}.$$,,"['calculus', 'limits', 'taylor-expansion', 'closed-form']"
47,The limit of iterated integrals,The limit of iterated integrals,,"Let $f_0>0$ be integrable on $[0,1]$, define $$f_n(x)=\sqrt{\int_0^x f_{n-1}(t)dt},\ n=1,2,\cdots.$$ Find the limit $$\lim_{n\to\infty}f_n(x),\ x\in [0,1].$$ I could suspect that the limit is $0$, but I could not prove it...","Let $f_0>0$ be integrable on $[0,1]$, define $$f_n(x)=\sqrt{\int_0^x f_{n-1}(t)dt},\ n=1,2,\cdots.$$ Find the limit $$\lim_{n\to\infty}f_n(x),\ x\in [0,1].$$ I could suspect that the limit is $0$, but I could not prove it...",,['limits']
48,Finding the limit without using L'Hôpital's rule or a Taylor/Maclaurin series.,Finding the limit without using L'Hôpital's rule or a Taylor/Maclaurin series.,,"Can this limit be found without using L'Hôpital's rule or Taylor/Maclaurin series?-- $$L=\displaystyle\lim_{x \rightarrow 0} \dfrac{e^x-x-1}{x^2}$$ I came up to the right answer..just that the method is not foolproof. -- Let $L$ be the limit. So, $$L=\lim_{x \rightarrow 0}\dfrac{e^x-x-1}{x^2}$$ Now, let $x=2y$. So, $$L=\lim_{x \rightarrow 0} \dfrac{e^{2y}-2y-1}{4y^2}$$So, the limit can be rewritten as $$L=\lim_{y \rightarrow 0} \dfrac{e^{2y}-2e^y+1+2e^y-2y-2}{4y^2}$$ which is $$L=\dfrac{1}{4}\lim_{y \rightarrow 0} \left(\left(\dfrac{e^y-1}{y}\right)^2+2\dfrac{e^y-y-1}{y^2}\right)$$ Now comes what i was saying. What i did was i separated the limit across the two terms. We can do that only if the two limits exist individually and finitely. The first, i am sure, exists.But notice the second term is twice the limit we desire to find. So, this method is only applicable when the limit exists.If we do substitute the second term with 2L, we have $$L=\dfrac{1}{4} (1+2L)=L$$ Solving for $L$ we get $\boxed{L=\dfrac{1}{2}}$. But as I said this method is not foolproof. Is there any?","Can this limit be found without using L'Hôpital's rule or Taylor/Maclaurin series?-- $$L=\displaystyle\lim_{x \rightarrow 0} \dfrac{e^x-x-1}{x^2}$$ I came up to the right answer..just that the method is not foolproof. -- Let $L$ be the limit. So, $$L=\lim_{x \rightarrow 0}\dfrac{e^x-x-1}{x^2}$$ Now, let $x=2y$. So, $$L=\lim_{x \rightarrow 0} \dfrac{e^{2y}-2y-1}{4y^2}$$So, the limit can be rewritten as $$L=\lim_{y \rightarrow 0} \dfrac{e^{2y}-2e^y+1+2e^y-2y-2}{4y^2}$$ which is $$L=\dfrac{1}{4}\lim_{y \rightarrow 0} \left(\left(\dfrac{e^y-1}{y}\right)^2+2\dfrac{e^y-y-1}{y^2}\right)$$ Now comes what i was saying. What i did was i separated the limit across the two terms. We can do that only if the two limits exist individually and finitely. The first, i am sure, exists.But notice the second term is twice the limit we desire to find. So, this method is only applicable when the limit exists.If we do substitute the second term with 2L, we have $$L=\dfrac{1}{4} (1+2L)=L$$ Solving for $L$ we get $\boxed{L=\dfrac{1}{2}}$. But as I said this method is not foolproof. Is there any?",,['limits']
49,$\lim_{n\rightarrow \infty} n(x^{1/n}-1)$ [duplicate],[duplicate],\lim_{n\rightarrow \infty} n(x^{1/n}-1),This question already has answers here : The Limit of $x\left(\sqrt[x]{a}-1\right)$ as $x\to\infty$. [duplicate] (3 answers) Closed 9 years ago . How to find limit of $$\lim_{n\rightarrow \infty} n(x^{1/n}-1)$$ Via L'Hopital's rule $$\lim_{n\rightarrow \infty} n(x^{1/n}-1) =  \lim_{n\rightarrow \infty} \frac{1}{n} x^{\frac{1}{n}-1} / \frac{-1}{n^2} =\lim_{n\rightarrow \infty} x^{\frac{1}{n}-1} / \frac{-1}{n}$$ but that doesn't help.,This question already has answers here : The Limit of $x\left(\sqrt[x]{a}-1\right)$ as $x\to\infty$. [duplicate] (3 answers) Closed 9 years ago . How to find limit of $$\lim_{n\rightarrow \infty} n(x^{1/n}-1)$$ Via L'Hopital's rule $$\lim_{n\rightarrow \infty} n(x^{1/n}-1) =  \lim_{n\rightarrow \infty} \frac{1}{n} x^{\frac{1}{n}-1} / \frac{-1}{n^2} =\lim_{n\rightarrow \infty} x^{\frac{1}{n}-1} / \frac{-1}{n}$$ but that doesn't help.,,"['calculus', 'limits']"
50,How to find the following limit? $\lim\limits_{t \to {\pi}/{2}}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx}.$,How to find the following limit?,\lim\limits_{t \to {\pi}/{2}}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx}.,"I am trying to solve the limit $$\lim\limits_{t \to \pi/2}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx}$$ My first method was to try with L'Hopital, i derived using Leibniz rule: $$\eqalign{\frac{\partial}{\partial t} \left(\int_{\sin t}^{1}e^{x^2\sin t}dx\right)&=\int_{\sin t}^{1}e^{x^2 \sin t}x^2 \cos tdx-e^{\sin ^3 t}\cos t\\ &= \cos t\left(\int_{\sin t}^{1}x^2e^{x^2 \sin t}dx-e^{ \sin ^3 t}\right).\\}$$ In the same manner, we can see that $$\frac{\partial}{\partial t} \left(\int_{\cos t}^{0}e^{x^2 \cos t}dx\right) = -\sin t\left(\int_{\cos t}^{0}x^2e^{x^2 \cos t}dx-e^{\cos ^3 t}\right).$$ So overall we have: $$\lim\limits_{t \to \pi/2}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx} = \lim\limits_{t \to { \pi}/{2}} \frac{\cos t\left(\int_{\sin t}^{1}x^2e^{x^2 \sin t}dx-e^{ \sin ^3 t}\right)}{-\sin t\left(\int_{\cos t}^{0}x^2e^{x^2 \cos t}dx-e^{\cos ^3 t}\right)}.$$ But where do we go from here? Could we say that because $\lim\limits_{t \to \pi/2} \frac{\cos t}{\sin t} =0$ then the entire limit goes to $0$? I don't think we can... Would appreciate any input. Perhaps L'Hopital was not the way.","I am trying to solve the limit $$\lim\limits_{t \to \pi/2}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx}$$ My first method was to try with L'Hopital, i derived using Leibniz rule: $$\eqalign{\frac{\partial}{\partial t} \left(\int_{\sin t}^{1}e^{x^2\sin t}dx\right)&=\int_{\sin t}^{1}e^{x^2 \sin t}x^2 \cos tdx-e^{\sin ^3 t}\cos t\\ &= \cos t\left(\int_{\sin t}^{1}x^2e^{x^2 \sin t}dx-e^{ \sin ^3 t}\right).\\}$$ In the same manner, we can see that $$\frac{\partial}{\partial t} \left(\int_{\cos t}^{0}e^{x^2 \cos t}dx\right) = -\sin t\left(\int_{\cos t}^{0}x^2e^{x^2 \cos t}dx-e^{\cos ^3 t}\right).$$ So overall we have: $$\lim\limits_{t \to \pi/2}\frac{ \int_{\sin t}^{1}e^{x^2\sin t}dx}{\int_{\cos t}^{0}e^{x^2 \cos t}dx} = \lim\limits_{t \to { \pi}/{2}} \frac{\cos t\left(\int_{\sin t}^{1}x^2e^{x^2 \sin t}dx-e^{ \sin ^3 t}\right)}{-\sin t\left(\int_{\cos t}^{0}x^2e^{x^2 \cos t}dx-e^{\cos ^3 t}\right)}.$$ But where do we go from here? Could we say that because $\lim\limits_{t \to \pi/2} \frac{\cos t}{\sin t} =0$ then the entire limit goes to $0$? I don't think we can... Would appreciate any input. Perhaps L'Hopital was not the way.",,"['calculus', 'integration', 'limits', 'multivariable-calculus']"
51,$f'$ strictly increases and $f'(c)=0$. There exist $x_1 < c < x_2$ such that $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$,strictly increases and . There exist  such that,f' f'(c)=0 x_1 < c < x_2 f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1},"Question: Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Assume that $f'$ is strictly increasing. Show that for any $c\in(a,b)$ such that $f'(c)=0$, there exist $x_1, x_2 \in [a,b], x_1 < c < x_2$ such that $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. What I've done: We argue by contradiction. Assume that for all $x_1<c<x_2$, we have $f'(c) \ne \frac{f(x_2)-f(x_1)}{x_2 - x_1}$. Without losing generality, we assume that $f'(c) < \frac{f(x_2)-f(x_1)}{x_2 - x_1}$. If we take limit on both side, we have $\displaystyle \lim_{x_2 \to x_1} f'(c) \le \lim_{x_2\to x_1}\frac{f(x_2)-f(x_1)}{x_2 - x_1} $. ( I am not sure if this step is correct or make any sense ) So we have $f'(c) \le f'(x_1)$. But since $f'$ is strictly increasing, and we know that $x_1<c$, so this cannot be the case. Contradiction.","Question: Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Assume that $f'$ is strictly increasing. Show that for any $c\in(a,b)$ such that $f'(c)=0$, there exist $x_1, x_2 \in [a,b], x_1 < c < x_2$ such that $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. What I've done: We argue by contradiction. Assume that for all $x_1<c<x_2$, we have $f'(c) \ne \frac{f(x_2)-f(x_1)}{x_2 - x_1}$. Without losing generality, we assume that $f'(c) < \frac{f(x_2)-f(x_1)}{x_2 - x_1}$. If we take limit on both side, we have $\displaystyle \lim_{x_2 \to x_1} f'(c) \le \lim_{x_2\to x_1}\frac{f(x_2)-f(x_1)}{x_2 - x_1} $. ( I am not sure if this step is correct or make any sense ) So we have $f'(c) \le f'(x_1)$. But since $f'$ is strictly increasing, and we know that $x_1<c$, so this cannot be the case. Contradiction.",,"['real-analysis', 'limits', 'proof-writing', 'proof-verification']"
52,Is it possible to find the limit of a sequence using the definition of a limit?,Is it possible to find the limit of a sequence using the definition of a limit?,,"Suppose we have a sequence $x_n = (\frac{1}{n})$ and we want to find $$\lim_{n \to \infty} x_n = \ ?$$ by the definition of limit. Clearly, the limit is $0$ but if we were not able to determine this by intuition (as is the case with more complex sequences), is there a way to find that the limit of this sequence is $0$ by using the definition of a limit? Or is the only option to make an educated guess and attempt to prove it using the definition?","Suppose we have a sequence $x_n = (\frac{1}{n})$ and we want to find $$\lim_{n \to \infty} x_n = \ ?$$ by the definition of limit. Clearly, the limit is $0$ but if we were not able to determine this by intuition (as is the case with more complex sequences), is there a way to find that the limit of this sequence is $0$ by using the definition of a limit? Or is the only option to make an educated guess and attempt to prove it using the definition?",,['limits']
53,Exponentiel function,Exponentiel function,,"Choose correct options , more than one may be correct . Let f be the function defined by  $f(x)=e^{\sqrt{x+\sqrt{1+x}}-\sqrt{x}}$ we've: (1) $\lim_{x\to\infty}f(x)=+\infty$ (2) $\lim_{x\to\infty}f(x)=0$ (3) $\lim_{x\to\infty}f(x)=e^{\dfrac{1}{2}}$ (4) $\lim_{x\to\infty}f(x)=\sqrt{e}$ her graph I think the correct answer is (3) Indeed : $$ \begin{align*} &\lim_{x\to +\infty} e^{\sqrt{x+\sqrt{1+x}}-\sqrt{x}}\\ =& \lim_{x\to +\infty} e^{\dfrac{\sqrt{1+x}}{\sqrt{x+\sqrt{1+x}}+\sqrt{x}}}\\ =&\lim_{x\to +\infty}e^{ \dfrac{\sqrt{x}\sqrt{\dfrac{1}{x}+1}}{\sqrt{x}(1+\sqrt{1+\dfrac{ \sqrt{1+x}}{x}})}}=\lim_{x\to +\infty}e^{ \dfrac{\sqrt{\dfrac{1}{x}+1}}{(1+\sqrt{1+\dfrac{ \sqrt{1+X}}{x}})}}=e^{\dfrac{1}{2}} \end{align*} $$ because of $\lim_{x\to +\infty}\dfrac{ \sqrt{1+x}}{x}=\lim_{x\to +\infty}\dfrac{ 1}{x}=0$ I wonder if there are  some other short ways to calculate that limit ?","Choose correct options , more than one may be correct . Let f be the function defined by  $f(x)=e^{\sqrt{x+\sqrt{1+x}}-\sqrt{x}}$ we've: (1) $\lim_{x\to\infty}f(x)=+\infty$ (2) $\lim_{x\to\infty}f(x)=0$ (3) $\lim_{x\to\infty}f(x)=e^{\dfrac{1}{2}}$ (4) $\lim_{x\to\infty}f(x)=\sqrt{e}$ her graph I think the correct answer is (3) Indeed : $$ \begin{align*} &\lim_{x\to +\infty} e^{\sqrt{x+\sqrt{1+x}}-\sqrt{x}}\\ =& \lim_{x\to +\infty} e^{\dfrac{\sqrt{1+x}}{\sqrt{x+\sqrt{1+x}}+\sqrt{x}}}\\ =&\lim_{x\to +\infty}e^{ \dfrac{\sqrt{x}\sqrt{\dfrac{1}{x}+1}}{\sqrt{x}(1+\sqrt{1+\dfrac{ \sqrt{1+x}}{x}})}}=\lim_{x\to +\infty}e^{ \dfrac{\sqrt{\dfrac{1}{x}+1}}{(1+\sqrt{1+\dfrac{ \sqrt{1+X}}{x}})}}=e^{\dfrac{1}{2}} \end{align*} $$ because of $\lim_{x\to +\infty}\dfrac{ \sqrt{1+x}}{x}=\lim_{x\to +\infty}\dfrac{ 1}{x}=0$ I wonder if there are  some other short ways to calculate that limit ?",,"['calculus', 'limits']"
54,Does $\sin(\sin(\sin\cdots(\sin1)\cdots) \rightarrow 0 $?,Does ?,\sin(\sin(\sin\cdots(\sin1)\cdots) \rightarrow 0 ,"Stuck on homework problem (not this), if I can prove as a lemma that the sequence $$\sin(\sin(\sin\cdots(\sin1)\cdots) \rightarrow  0  $$ then I'm done. It's monotonic and decreasing and bounded by 0 and 1 respectively, so it converges, though is it truly $0$ ?","Stuck on homework problem (not this), if I can prove as a lemma that the sequence $$\sin(\sin(\sin\cdots(\sin1)\cdots) \rightarrow  0  $$ then I'm done. It's monotonic and decreasing and bounded by 0 and 1 respectively, so it converges, though is it truly $0$ ?",,"['sequences-and-series', 'limits', 'trigonometry']"
55,$\lim_{x\rightarrow 0^+} \frac {\ln(x)}{\ln( \sin x)}$ without l'Hôpital's rule,without l'Hôpital's rule,\lim_{x\rightarrow 0^+} \frac {\ln(x)}{\ln( \sin x)},How to calculate $\displaystyle \lim_{x\rightarrow 0^{+}}\frac{\ln x}{\ln (\sin x)}$ without l'Hôpital's rule please? If anybody knows please help I don´t have any idea :-(  I´m looking forward your helps,How to calculate $\displaystyle \lim_{x\rightarrow 0^{+}}\frac{\ln x}{\ln (\sin x)}$ without l'Hôpital's rule please? If anybody knows please help I don´t have any idea :-(  I´m looking forward your helps,,['calculus']
56,Evaluating Dirichlet series,Evaluating Dirichlet series,,"It is well known that $$\eta(s)=\sum\limits_{k=1}^{\infty}\frac{(-1)^{k-1}}{k^s} =(1-2^{1-s})\zeta(s)$$ But I have the wider problem of evaluating the following $$f(s)=\sum\limits_{k=1}^{\infty}\frac{\zeta^{k}}{k^s}$$ where $\zeta$ is a root of unity. So for instance, I would like to know if there is even a closed form solution for $$f(s)=\sum\limits_{k=1}^{\infty}\frac{i^{k}}{k^s}$$ I am interested in these results because I am trying to find the Zeta regularized sum of a particular series which is in a similar form to the ones stated above. Here is the problem in case you are curious. Evaluate $$\lim\limits_{s\to 0} \sum\limits_{k=1}^{\infty}\frac{\zeta^{k}}{k^s}(\sum\limits_{d|k}\frac{(-1)^{d-1}}{d})$$ Any suggestions would be helpful","It is well known that $$\eta(s)=\sum\limits_{k=1}^{\infty}\frac{(-1)^{k-1}}{k^s} =(1-2^{1-s})\zeta(s)$$ But I have the wider problem of evaluating the following $$f(s)=\sum\limits_{k=1}^{\infty}\frac{\zeta^{k}}{k^s}$$ where $\zeta$ is a root of unity. So for instance, I would like to know if there is even a closed form solution for $$f(s)=\sum\limits_{k=1}^{\infty}\frac{i^{k}}{k^s}$$ I am interested in these results because I am trying to find the Zeta regularized sum of a particular series which is in a similar form to the ones stated above. Here is the problem in case you are curious. Evaluate $$\lim\limits_{s\to 0} \sum\limits_{k=1}^{\infty}\frac{\zeta^{k}}{k^s}(\sum\limits_{d|k}\frac{(-1)^{d-1}}{d})$$ Any suggestions would be helpful",,"['limits', 'zeta-functions', 'dirichlet-series']"
57,"How prove that, for every $n$, $ \lim\limits_{x\to\infty}f_{n}(x)=\frac{1}{n!}$","How prove that, for every ,",n  \lim\limits_{x\to\infty}f_{n}(x)=\frac{1}{n!},"Let $$f_{1}(x)=\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}$$ $$f_{2}(x)=\left(\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}-1\right)\ln{x}$$ $$f_{3}(x)=\left(\left(\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}-1\right)\ln{x}-\dfrac{1}{2!}\right)\ln{x}$$ $$\cdots\cdots\cdots\cdots$$ $$f_{n+1}(x)=\left(f_{n}(x)-\dfrac{1}{n!}\right)\ln{x}$$ Find the limit $$\lim_{x\to +\infty}f_{n}(x)$$ I know $$\lim_{x\to+\infty}f_{1}(x)=1,\lim_{x\to+\infty}f_{2}(x)=\dfrac{1}{2!}$$ so I guess $$\lim_{x\to\infty}f_{n}(x)=\dfrac{1}{n!}$$ But I can't prove it,Thank you","Let $$f_{1}(x)=\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}$$ $$f_{2}(x)=\left(\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}-1\right)\ln{x}$$ $$f_{3}(x)=\left(\left(\left(\left(\dfrac{\ln{(1+x)}}{\ln{x}}\right)^x-1\right)\ln{x}-1\right)\ln{x}-\dfrac{1}{2!}\right)\ln{x}$$ $$\cdots\cdots\cdots\cdots$$ $$f_{n+1}(x)=\left(f_{n}(x)-\dfrac{1}{n!}\right)\ln{x}$$ Find the limit $$\lim_{x\to +\infty}f_{n}(x)$$ I know $$\lim_{x\to+\infty}f_{1}(x)=1,\lim_{x\to+\infty}f_{2}(x)=\dfrac{1}{2!}$$ so I guess $$\lim_{x\to\infty}f_{n}(x)=\dfrac{1}{n!}$$ But I can't prove it,Thank you",,['limits']
58,Does $\lim_{n \to \infty} \sum_{i=1}^{\infty} \frac {i}{n^2}=\frac 12?$,Does,\lim_{n \to \infty} \sum_{i=1}^{\infty} \frac {i}{n^2}=\frac 12?,"I was working on something on some physics and this came up: $\lim_{n \to \infty} [\sum_{i=1}^{\infty} \dfrac {i}{n^2}]$ Is it possible to do the following manipulation? $\lim_{n \to \infty} [\dfrac {1}{n^2} \sum_{i=1}^{\infty} {i}]$ $\lim_{n \to \infty} [\dfrac {1}{n^2} \cdot \dfrac {n(n+1)}{2}]$ $\dfrac 12$ The reason why I think this might be wrong is because more generally, $\lim_{n \to \infty} \dfrac {1}{n^2} \sum_{i=1}^{\infty} {i}=\lim_{n \to \infty} [ \dfrac {1}{n^2} \cdot \lim_{r \to \infty} \dfrac {r(r+1)}{2}]$ In the manipulation above, we assumed that $r=n$ but I don't see anything to suggest that $r$ ""grows"" as fast as $n$. Although $r$ and $n$ both represent natural numbers, I think they still might be able to ""grow"" at different ""speeds."" Does thus ambiguity allow us to select any relationship between $r$ and $n$ is most convenient, such as $r=n$?","I was working on something on some physics and this came up: $\lim_{n \to \infty} [\sum_{i=1}^{\infty} \dfrac {i}{n^2}]$ Is it possible to do the following manipulation? $\lim_{n \to \infty} [\dfrac {1}{n^2} \sum_{i=1}^{\infty} {i}]$ $\lim_{n \to \infty} [\dfrac {1}{n^2} \cdot \dfrac {n(n+1)}{2}]$ $\dfrac 12$ The reason why I think this might be wrong is because more generally, $\lim_{n \to \infty} \dfrac {1}{n^2} \sum_{i=1}^{\infty} {i}=\lim_{n \to \infty} [ \dfrac {1}{n^2} \cdot \lim_{r \to \infty} \dfrac {r(r+1)}{2}]$ In the manipulation above, we assumed that $r=n$ but I don't see anything to suggest that $r$ ""grows"" as fast as $n$. Although $r$ and $n$ both represent natural numbers, I think they still might be able to ""grow"" at different ""speeds."" Does thus ambiguity allow us to select any relationship between $r$ and $n$ is most convenient, such as $r=n$?",,"['calculus', 'sequences-and-series', 'limits']"
59,How find this $\lim_{x\to 0}\frac{(1+x)^{\frac{1}{x}}-(1+2x)^{\frac{1}{2x}}}{x}$,How find this,\lim_{x\to 0}\frac{(1+x)^{\frac{1}{x}}-(1+2x)^{\frac{1}{2x}}}{x},Evaluate   $$I=\lim_{x\to 0}\dfrac{(1+x)^{\frac{1}{x}}-(1+2x)^{\frac{1}{2x}}}{x}$$ My try: Use L'Hôpital's rule.  We  have  $$I=\lim_{x\to0}(1+x)^{1/x}\left(\dfrac{x}{x+1}-\ln{(x+1)}\right)-(1+2x)^{1/2x}\left(\dfrac{4x}{1+2x}-2\ln{(1+2x)}\right)$$ I'm stuck.  Thank you very much for you help,Evaluate   $$I=\lim_{x\to 0}\dfrac{(1+x)^{\frac{1}{x}}-(1+2x)^{\frac{1}{2x}}}{x}$$ My try: Use L'Hôpital's rule.  We  have  $$I=\lim_{x\to0}(1+x)^{1/x}\left(\dfrac{x}{x+1}-\ln{(x+1)}\right)-(1+2x)^{1/2x}\left(\dfrac{4x}{1+2x}-2\ln{(1+2x)}\right)$$ I'm stuck.  Thank you very much for you help,,['limits']
60,Proving that $L \le M$ as limits of $f$ and $g$ when $f(x) \le g(x)$,Proving that  as limits of  and  when,L \le M f g f(x) \le g(x),"While doing some tasks for my next calculus course, I ran across this task: ""Let $a < b <c$, and assume that $f(x) \le g(x)$ for all $x \in [a, c]$. If $\lim_{x \to b}f(x) = L$ and $\lim_{x \to b}g(x) = M$, prove that $L \le M$."" I have done some thinking about this, but I have to admit that I'm hopelessly stuck. A proof by contradiction seems natural here, so I'm assuming, for the sake of contradiciton, that $L > M$. From the limits we know, from the definition, that $|x - b| < \delta(\epsilon)$ and that $|f(x) - L| < \epsilon$ and $|g(x) - M| < \epsilon$. Unconventionally, I tried setting $\epsilon < 1$ (yes, $\epsilon$, not $\delta$), getting $|f(x) - L| < 1$ and $|g(x) - M| < 1$. I'm having a hard time forcing a contradiction from this. We are left with the facts that $f(x) < 1 + L$ and $g(x) < 1 + M$. While writing this, I thought that if I changed $\epsilon < 1$ to ""very small $\epsilon$"", we could force a contradiction from $f(x) < \epsilon + L$ and $g(x) < \epsilon + M$. For sufficiently small $\epsilon$ and $\delta(\epsilon)$, this would imply (I do not have any confidence in this statement whatsoever) $f(x) > g(x)$ for $L > M$, which is the desired contradiction, and we are done. I'm new to these kind of proofs, and I'd highly appreciate to be spoonfed about my mistakes here.","While doing some tasks for my next calculus course, I ran across this task: ""Let $a < b <c$, and assume that $f(x) \le g(x)$ for all $x \in [a, c]$. If $\lim_{x \to b}f(x) = L$ and $\lim_{x \to b}g(x) = M$, prove that $L \le M$."" I have done some thinking about this, but I have to admit that I'm hopelessly stuck. A proof by contradiction seems natural here, so I'm assuming, for the sake of contradiciton, that $L > M$. From the limits we know, from the definition, that $|x - b| < \delta(\epsilon)$ and that $|f(x) - L| < \epsilon$ and $|g(x) - M| < \epsilon$. Unconventionally, I tried setting $\epsilon < 1$ (yes, $\epsilon$, not $\delta$), getting $|f(x) - L| < 1$ and $|g(x) - M| < 1$. I'm having a hard time forcing a contradiction from this. We are left with the facts that $f(x) < 1 + L$ and $g(x) < 1 + M$. While writing this, I thought that if I changed $\epsilon < 1$ to ""very small $\epsilon$"", we could force a contradiction from $f(x) < \epsilon + L$ and $g(x) < \epsilon + M$. For sufficiently small $\epsilon$ and $\delta(\epsilon)$, this would imply (I do not have any confidence in this statement whatsoever) $f(x) > g(x)$ for $L > M$, which is the desired contradiction, and we are done. I'm new to these kind of proofs, and I'd highly appreciate to be spoonfed about my mistakes here.",,"['calculus', 'real-analysis', 'limits', 'inequality']"
61,How to approximate $n \int_{0}^{1} [1 - x^m ]^n x^m dx $ near infinity?,How to approximate  near infinity?,n \int_{0}^{1} [1 - x^m ]^n x^m dx ,"I have a hypothesis that if: $$  I_{n,m} := n \int_{0}^{1} [1 - x^m ]^n x^m dx $$ where $m,n \in \mathbb{N}$ then  $$ \lim_{n \rightarrow \infty} \frac{I_{n,m}}{n^{-\frac{1}{m} } } = c_m $$ But I have no idea how to prove it. For $m = 1$ I can evaluate the integral using integration by parts: $$ \int_{0}^{1} (1 - x)^nx dx =  \left [- \frac{( 1-x )^{n+1} }{n+1} x \right ]_0^1 - \int_{0}^{1}  -\frac{( 1-x )^{n+1} }{n+1} dx=  0 + \frac{1}{n+1}  \int_{0}^{1} {(1 - x)}^{n+1} dx =  \frac{1}{n+1} \left [ -\frac{( 1-x )^{n+2} }{n+2}  \right ]_0^1  =   \frac{1}{(n+1)(n+2)} $$ therefore: $$ I_{n,1} = \frac{n}{(n+1)(n+2)} $$ and this can be approximated by $\frac{1}{n}$ as the hypothesis suggests. But if I try integrating by parts for general $m$ I just transform the integral to an expression containng: $$ \int_{0}^{1} [1 - x^m ]^n  dx $$ which I do not know how to solve. I really do not know if my hypothesis is true. I would just like to know how does $I_{n,m}$ behave for large values of $n$.","I have a hypothesis that if: $$  I_{n,m} := n \int_{0}^{1} [1 - x^m ]^n x^m dx $$ where $m,n \in \mathbb{N}$ then  $$ \lim_{n \rightarrow \infty} \frac{I_{n,m}}{n^{-\frac{1}{m} } } = c_m $$ But I have no idea how to prove it. For $m = 1$ I can evaluate the integral using integration by parts: $$ \int_{0}^{1} (1 - x)^nx dx =  \left [- \frac{( 1-x )^{n+1} }{n+1} x \right ]_0^1 - \int_{0}^{1}  -\frac{( 1-x )^{n+1} }{n+1} dx=  0 + \frac{1}{n+1}  \int_{0}^{1} {(1 - x)}^{n+1} dx =  \frac{1}{n+1} \left [ -\frac{( 1-x )^{n+2} }{n+2}  \right ]_0^1  =   \frac{1}{(n+1)(n+2)} $$ therefore: $$ I_{n,1} = \frac{n}{(n+1)(n+2)} $$ and this can be approximated by $\frac{1}{n}$ as the hypothesis suggests. But if I try integrating by parts for general $m$ I just transform the integral to an expression containng: $$ \int_{0}^{1} [1 - x^m ]^n  dx $$ which I do not know how to solve. I really do not know if my hypothesis is true. I would just like to know how does $I_{n,m}$ behave for large values of $n$.",,"['integration', 'limits', 'definite-integrals', 'approximation']"
62,Finding the limit .,Finding the limit .,,"Consider $f$ is differentiable, $$\lim_{n\to \infty} \frac{1}{n^2} \sum_{k=1}^n \frac{f(a+\frac{k}{n^2}) -f(a)}{\frac{k}{n^2}}$$ .  My idea was ,  Since $f$ is differentiable each term in the sum exists $\forall n$ , hence say $M$ be the max so we have $$\lim_{n\to \infty} \frac{1}{n^2} n.|M|$$  Hence the limit is $0$. Can you guys help me out .","Consider $f$ is differentiable, $$\lim_{n\to \infty} \frac{1}{n^2} \sum_{k=1}^n \frac{f(a+\frac{k}{n^2}) -f(a)}{\frac{k}{n^2}}$$ .  My idea was ,  Since $f$ is differentiable each term in the sum exists $\forall n$ , hence say $M$ be the max so we have $$\lim_{n\to \infty} \frac{1}{n^2} n.|M|$$  Hence the limit is $0$. Can you guys help me out .",,"['real-analysis', 'limits']"
63,Clayton's copula limit to infinity,Clayton's copula limit to infinity,,"This is Clayton's copula: $C(u_1,u_2)=[u_1^{-\alpha} + u_2^{-\alpha} - 1]^{\frac{-1}{\alpha}}$ where $ (u_1,u_2) \in ]0,1]$ and $\alpha>0$ How do you prove the following limit to infinity ? $lim_{\alpha \to \infty}C(u_1,u_2)=min(u_1,u_2) $ What about the other limit, to zero ? $lim_{\alpha \to 0}C(u_1,u_2)=u_1u_2 $ I'm stuck here, help would be appreciated.","This is Clayton's copula: $C(u_1,u_2)=[u_1^{-\alpha} + u_2^{-\alpha} - 1]^{\frac{-1}{\alpha}}$ where $ (u_1,u_2) \in ]0,1]$ and $\alpha>0$ How do you prove the following limit to infinity ? $lim_{\alpha \to \infty}C(u_1,u_2)=min(u_1,u_2) $ What about the other limit, to zero ? $lim_{\alpha \to 0}C(u_1,u_2)=u_1u_2 $ I'm stuck here, help would be appreciated.",,['limits']
64,Finding limit of $\frac{\sin\pi3^x}{x}$ as $x\to 0$,Finding limit of  as,\frac{\sin\pi3^x}{x} x\to 0,I have to find the limit of $\frac{\sin\pi3^x}{x}$ as $x\to 0$ using ONLY notable limits please help me.,I have to find the limit of $\frac{\sin\pi3^x}{x}$ as $x\to 0$ using ONLY notable limits please help me.,,"['calculus', 'limits']"
65,$\limsup_{n\rightarrow\infty} a_n^{1/\log n}<1/e$ and $a_n>0$ then $\sum a_n$ converges,and  then  converges,\limsup_{n\rightarrow\infty} a_n^{1/\log n}<1/e a_n>0 \sum a_n,"If $\limsup_{n\rightarrow\infty} a_n^{1/\log n}<1/e$ and $a_n>0$ then $\sum_{n=1}^{\infty} a_n$ converges. $$0<a_n < e^{-\log n}=\frac{1}{n}$$ Also, $\exp\{\frac{1}{\log n}\log a_n \} \le \exp(-1)$ so $\displaystyle\frac{\log a_n}{\log n}\le -1$. How does the limsup help here?","If $\limsup_{n\rightarrow\infty} a_n^{1/\log n}<1/e$ and $a_n>0$ then $\sum_{n=1}^{\infty} a_n$ converges. $$0<a_n < e^{-\log n}=\frac{1}{n}$$ Also, $\exp\{\frac{1}{\log n}\log a_n \} \le \exp(-1)$ so $\displaystyle\frac{\log a_n}{\log n}\le -1$. How does the limsup help here?",,"['sequences-and-series', 'limits', 'summation']"
66,Calculate limit using Stolz-Cesàro theorem,Calculate limit using Stolz-Cesàro theorem,,Can someone help me calculate this limit using the Stolz-Cesàro theorem? $\lim_{n\to \infty } \frac{1+\frac12+......+\frac1n}{\ln n}$,Can someone help me calculate this limit using the Stolz-Cesàro theorem? $\lim_{n\to \infty } \frac{1+\frac12+......+\frac1n}{\ln n}$,,"['calculus', 'limits', 'harmonic-numbers']"
67,Elegant or elementary evaluation of $\lim\limits_{x\to 0} \left( \frac{1}{x}-\frac{1}{\sin(x)} \right) $ [duplicate],Elegant or elementary evaluation of  [duplicate],\lim\limits_{x\to 0} \left( \frac{1}{x}-\frac{1}{\sin(x)} \right) ,"This question already has answers here : What is the result of $\lim_{x \rightarrow 0}\left(\frac1x - \frac1{\sin x}\right)$? (11 answers) Closed 10 years ago . I give math tutoring and was wondering about the following limit. I found the answer but I was wondering if someone has a nicer explanation than the one I am giving where I use L'Hôpital's rule twice. The limit I am evaluating is: $$\lim\limits_{x\to 0}    \left( \frac{1}{x}-\frac{1}{\sin(x)}   \right)  = \lim\limits_{x\to 0} \frac{\sin(x)-x}{x\sin(x)}  $$ Both numerator and denomitor go to zero here so we may use l'Hôpital's rule here. The limit is thus equal to: $$\lim\limits_{x\to 0}   \frac{\cos(x)-1}{\sin(x)+x\cos(x)}   $$ Again numerator and denominator go to zero and thus by l'Hôpital the limit is equal to: $$\lim\limits_{x\to 0}  \frac{-\sin(x)}{\cos(x) + \cos(x) + x\sin(x)}$$ Now we are looking at the limit of the quotient of two everywhere continuous functions where the denominator is not zero. Thus the function itself is continuous and so the limits is: $$\lim\limits_{x\to 0}  \frac{-\sin(x)}{2 \cos(x) + x\sin(x)} = \frac{0}{2}=0$$ Does anyone know a more nicer, more elementary way of solving this? Thanks! EDIT: Also, if anyone knows a fast, yet less elementary way to solve I would enjoy seeing it so feel free to post :)","This question already has answers here : What is the result of $\lim_{x \rightarrow 0}\left(\frac1x - \frac1{\sin x}\right)$? (11 answers) Closed 10 years ago . I give math tutoring and was wondering about the following limit. I found the answer but I was wondering if someone has a nicer explanation than the one I am giving where I use L'Hôpital's rule twice. The limit I am evaluating is: $$\lim\limits_{x\to 0}    \left( \frac{1}{x}-\frac{1}{\sin(x)}   \right)  = \lim\limits_{x\to 0} \frac{\sin(x)-x}{x\sin(x)}  $$ Both numerator and denomitor go to zero here so we may use l'Hôpital's rule here. The limit is thus equal to: $$\lim\limits_{x\to 0}   \frac{\cos(x)-1}{\sin(x)+x\cos(x)}   $$ Again numerator and denominator go to zero and thus by l'Hôpital the limit is equal to: $$\lim\limits_{x\to 0}  \frac{-\sin(x)}{\cos(x) + \cos(x) + x\sin(x)}$$ Now we are looking at the limit of the quotient of two everywhere continuous functions where the denominator is not zero. Thus the function itself is continuous and so the limits is: $$\lim\limits_{x\to 0}  \frac{-\sin(x)}{2 \cos(x) + x\sin(x)} = \frac{0}{2}=0$$ Does anyone know a more nicer, more elementary way of solving this? Thanks! EDIT: Also, if anyone knows a fast, yet less elementary way to solve I would enjoy seeing it so feel free to post :)",,"['calculus', 'limits', 'alternative-proof']"
68,"A double sum and its relation to a simple sum, is this an identity for any complex number $S=a+i b$ and any integers n and t?","A double sum and its relation to a simple sum, is this an identity for any complex number  and any integers n and t?",S=a+i b,"Does: $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ equal: $$\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for any complex number $S$ and any integers $n$ and $t$? Which as a Mathematica program is: S = 1.23456789 + I*9.87654321 Monitor[Table[   Chop[N[Table[       Sum[Limit[         Zeta[s]*Sum[(1 - If[Mod[k, n] == 0, n, 0])/            k^(s - 1), {k, (m - 1)*n + 1, m*n}], s -> S], {m, 1,          t}], {n, 1, 12}]] -         Table[      Limit[Zeta[s]*        Sum[(1 - If[Mod[k, n] == 0, n, 0])/k^(s - 1), {k, 1, t*n}],        s -> S], {n, 1, 12}]], {t, 1, 6}], t] Some background information on the problem. Mathematica knows that logarithms can be calculated as: $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ Logarithms can be added to give the logarithm of the product of their argument, $\lim_{s\to 1} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 1} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 1} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta$ $\log(6) = \log(2) + \log(3)$ This works only when the limits lets $s \to 1$. Trying the same for example with the limit letting $s \to 2$, it fails, as can be seen in this example: $\lim_{s\to 2} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 2} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 2} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta$ which gives us: $\lim_{s\to 2} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 2} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 2} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta (s)$ which evaluated is: $\frac{5 \pi ^2}{36} = \frac{\pi ^2}{12} + \frac{\pi ^2}{9}$ but then we would have: $\frac{5 \pi ^2}{36} = \frac{7 \pi ^2}{36}$ which is not true. Looking at the logarithm above again, as a zeta function limit, we have: $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ In this question it was shown in the answer that logarithms have these numerators in their Dirichlet series: $$1-\text{If}[k \bmod n=0,n,0]$$ which as a matrix is: $$\begin{bmatrix} 0&0&0&0&0&0&0 \\ 1&-1&1&-1&1&-1&1 \\ 1&1&-2&1&1&-2&1 \\ 1&1&1&-3&1&1&1 \\ 1&1&1&1&-4&1&1 \\ 1&1&1&1&1&-5&1 \\ 1&1&1&1&1&1&-6 \end{bmatrix}$$ which is an infinite matrix where each row is the numerators in the Dirichlet series that converges to $\log(n)$ for $s=1$. In the zeta function limit for logarithms the part within the parentheses $\left(1-\frac{1}{n^{s-1}}\right)$ in $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ will go towards zero in the limit: $\lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) = 0$ In a different matrix: $$\displaystyle T = \begin{bmatrix} +1&+1&+1&+1&+1&+1&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \end{bmatrix}$$ where $\Lambda(n) = \displaystyle \sum\limits_{k=1}^{\infty}\frac{T(n,k)}{k}$, and where $\Lambda(n)$ is the von Mangoldt function as proven in the answer to this question, we have the triangle: $$\displaystyle \begin{bmatrix} +1 \\ +1&-1&=0 \\ +1&+1&-2&=0 \\ +1&-1&+1&-1&=0 \\ +1&+1&+1&+1&-4&=0 \\ +1&-1&-2&-1&+1&+2&=0 \\ +1&+1&+1&+1&+1&+1&-6&=0 \end{bmatrix}$$ where we see that the row sums are also zero. Looking again at the earlier matrix we see the same thing (with exception of the first row, but $\Lambda(0) = 0$ anyways): $$\displaystyle \begin{bmatrix} 0=0 \\ 1&-1=0 \\ 1&1&-2=0 \\ 1&1&1&-3=0 \\ 1&1&1&1&-4=0 \\ 1&1&1&1&1&-5=0 \\ 1&1&1&1&1&1&-6=0 \end{bmatrix}$$ writing zeta function limits of this latter matrix, and we have: $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}-\frac{1}{2^{s-1}}\right) \zeta (s) = \log (2)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}-\frac{2}{3^{s-1}}\right) \zeta (s) = \log \left(\frac{9}{2}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}-\frac{3}{4^{s-1}}\right) \zeta (s) = \log \left(\frac{32}{3}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}-\frac{4}{5^{s-1}}\right) \zeta (s) = \log \left(\frac{625}{24}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}+\frac{1}{5^{s-1}}-\frac{5}{6^{s-1}}\right) \zeta (s) = \log \left(\frac{324}{5}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}+\frac{1}{5^{s-1}}+\frac{1}{6^{s-1}}-\frac{6}{7^{s-1}}\right) \zeta (s) = \log \left(\frac{117649}{720}\right)$$ with the sequence: $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$ looking up numerators and denominators in the oeis we find sequences: http://oeis.org/A036505 and: http://oeis.org/A095996 Multiplying by $n!$ and searching for the sequence: $1, 4, 27, 256, 3125, 46656, 823543...$ we find when we scroll down to sequence https://oeis.org/A177885 the following formula: $$\frac{1}{2}+\frac{2 \pi  i \exp (1) \left(n-\frac{11}{8}\right)}{\exp (1) W\left(\frac{n-\frac{11}{8}}{\exp (1)}\right)}$$ involving the Lambert_W function for a good approximation of the n-th Riemann zeta zero. $W(x)$ or LambertW is undocumented in Mathematica as said in Mathworld, and the ProductLog[x] command is used instead. There is also a functional relation between the sequence from the zeta limits and the $1, 4, 27, 256, 3125, 46656, 823543...$ sequence which is as follows: $$\frac{x}{W(-x)} = x \sum _{n=1}^{\infty } \frac{1}{(W(-x)+1)^n}$$ Changing the argument $-x$ to $x$ we have the function used in the zeta zero approximation formula. This warrants some further investigation of the sequence $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$  from the zeta function limits. Setting the denominators equal to $\exp(n)^{s-1}$: $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}-\frac{1}{\exp ^{s-1}(2)}\right) \zeta (s) =1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}-\frac{2}{\exp ^{s-1}(3)}\right) \zeta (s)=3$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}-\frac{3}{\exp ^{s-1}(4)}\right) \zeta (s)=6$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}-\frac{4}{\exp ^{s-1}(5)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}+\frac{1}{\exp ^{s-1}(5)}-\frac{5}{\exp ^{s-1}(6)}\right) \zeta (s)=15$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}+\frac{1}{\exp ^{s-1}(5)}+\frac{1}{\exp ^{s-1}(6)}-\frac{6}{\exp ^{s-1}(7)}\right) \zeta (s)=21$$ which appear to be the triangular numbers, $\frac{n(n+1)}{2}$. Reversing the triangular matrix: $$\displaystyle \begin{bmatrix} 0=0 \\ 1&-1=0 \\ 1&1&-2=0 \\ 1&1&1&-3=0 \\ 1&1&1&1&-4=0 \\ 1&1&1&1&1&-5=0 \\ 1&1&1&1&1&1&-6=0 \end{bmatrix}$$ into: $$\displaystyle \begin{bmatrix} 0=0 \\ -1&1=0 \\ -2&1&1=0 \\ -3&1&1&1=0 \\ -4&1&1&1&1=0 \\ -5&1&1&1&1&1=0 \\ -6&1&1&1&1&1&1=0 \end{bmatrix}$$ which is the triangle https://oeis.org/A167407 in the OEIS. Taking successive matrix powers of this triangle and reversing back we seem to get the whole Pascal triangle as zeta function limits. $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}-\frac{1}{\exp ^{s-1}(2)}\right) \zeta (s)=1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}-\frac{3}{\exp ^{s-1}(3)}\right) \zeta (s)=4$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}-\frac{6}{\exp ^{s-1}(4)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}-\frac{10}{\exp ^{s-1}(5)}\right) \zeta (s)=20$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}+\frac{5}{\exp ^{s-1}(5)}-\frac{15}{\exp ^{s-1}(6)}\right) \zeta (s)=35$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}+\frac{5}{\exp ^{s-1}(5)}+\frac{6}{\exp ^{s-1}(6)}-\frac{21}{\exp ^{s-1}(7)}\right) \zeta (s)=56$$ More in line with logarithms of $n=1,2,3,4,5...$ is to consider the following zeta function limits: $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+2)}-\frac{1}{\exp ^{s-1}(2+2)}\right) \zeta (s)=1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+3)}+\frac{1}{\exp ^{s-1}(2+3)}-\frac{2}{\exp ^{s-1}(3+3)}\right) \zeta (s)=3$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+4)}+\frac{1}{\exp ^{s-1}(2+4)}+\frac{1}{\exp ^{s-1}(3+4)}-\frac{3}{\exp ^{s-1}(4+4)}\right) \zeta (s)=6$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+5)}+\frac{1}{\exp ^{s-1}(2+5)}+\frac{1}{\exp ^{s-1}(3+5)}+\frac{1}{\exp ^{s-1}(4+5)}-\frac{4}{\exp ^{s-1}(5+5)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+6)}+\frac{1}{\exp ^{s-1}(2+6)}+\frac{1}{\exp ^{s-1}(3+6)}+\frac{1}{\exp ^{s-1}(4+6)}+\frac{1}{\exp ^{s-1}(5+6)}-\frac{5}{\exp ^{s-1}(6+6)}\right) \zeta (s)=15$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+7)}+\frac{1}{\exp ^{s-1}(2+7)}+\frac{1}{\exp ^{s-1}(3+7)}+\frac{1}{\exp ^{s-1}(4+7)}+\frac{1}{\exp ^{s-1}(5+7)}+\frac{1}{\exp ^{s-1}(6+7)}-\frac{6}{\exp ^{s-1}(7+7)}\right) \zeta (s)=21$$ where we get the triangular numbers again. In other words there seems to be an invariance property. Converting back to unexponentiated denominators: $$\lim_{s\to 1} \, \left(\frac{1}{(1+2)^{s-1}}-\frac{1}{(2+2)^{s-1}}\right) \zeta (s)=\log \left(\frac{4}{3}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+3)^{s-1}}+\frac{1}{(2+3)^{s-1}}-\frac{2}{(3+3)^{s-1}}\right) \zeta (s)=\log \left(\frac{9}{5}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+4)^{s-1}}+\frac{1}{(2+4)^{s-1}}+\frac{1}{(3+4)^{s-1}}-\frac{3}{(4+4)^{s-1}}\right) \zeta (s)=\log \left(\frac{256}{105}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+5)^{s-1}}+\frac{1}{(2+5)^{s-1}}+\frac{1}{(3+5)^{s-1}}+\frac{1}{(4+5)^{s-1}}-\frac{4}{(5+5)^{s-1}}\right) \zeta (s)=\log \left(\frac{625}{189}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+6)^{s-1}}+\frac{1}{(2+6)^{s-1}}+\frac{1}{(3+6)^{s-1}}+\frac{1}{(4+6)^{s-1}}+\frac{1}{(5+6)^{s-1}}-\frac{5}{(6+6)^{s-1}}\right) \zeta (s)=\log \left(\frac{1728}{385}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+7)^{s-1}}+\frac{1}{(2+7)^{s-1}}+\frac{1}{(3+7)^{s-1}}+\frac{1}{(4+7)^{s-1}}+\frac{1}{(5+7)^{s-1}}+\frac{1}{(6+7)^{s-1}}-\frac{6}{(7+7)^{s-1}}\right) \zeta (s)=\log \left(\frac{117649}{19305}\right)$$ where we have: $1,4/3, 9/5, 256/105, 625/189, 1728/385, 117649/19305,...$ compared to the earlier: $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$ in other words, no invariance like with the triangular numbers. Drawing this further we generalize and write: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+1}^{n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+n+1}^{n+n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for which we get the sequences for $n=1,2,3,4,5,6,7,...$: $$\left\{0,\log (2),\log \left(\frac{9}{2}\right),\log \left(\frac{32}{3}\right),\log \left(\frac{625}{24}\right),\log \left(\frac{324}{5}\right),\log \left(\frac{117649}{720}\right),...\right\}$$ $$\left\{0,\log \left(\frac{4}{3}\right),\log \left(\frac{9}{5}\right),\log \left(\frac{256}{105}\right),\log \left(\frac{625}{189}\right),\log \left(\frac{1728}{385}\right),\log \left(\frac{117649}{19305}\right),...\right\}$$ $$\left\{0,\log \left(\frac{6}{5}\right),\log \left(\frac{81}{56}\right),\log \left(\frac{96}{55}\right),\log \left(\frac{16875}{8008}\right),\log \left(\frac{19683}{7735}\right),\log \left(\frac{3176523}{1033600}\right),...\right\}$$ $$\left\{0,\log \left(\frac{8}{7}\right),\log \left(\frac{72}{55}\right),\log \left(\frac{2048}{1365}\right),\log \left(\frac{5000}{2907}\right),\log \left(\frac{331776}{168245}\right),\log \left(\frac{15059072}{6660225}\right),...\right\}$$ $$\left\{0,\log \left(\frac{10}{9}\right),\log \left(\frac{225}{182}\right),\log \left(\frac{4000}{2907}\right),\log \left(\frac{390625}{255024}\right),\log \left(\frac{4500}{2639}\right),\log \left(\frac{367653125}{193666176}\right),...\right\}$$ What is the relationship between these numbers? Trying to describe the fractions within the logarithms I wrote: $$\frac{((j+1) n)^j}{\prod _{k=1}^j ((j+1) (n-1)+k)}$$ $$n=1,2,3,4,5$$ $$j=0,1,2,3,4,5,6$$ which gives: $$\left( \begin{array}{ccccc}  1 & 1 & 1 & 1 & 1 \\  2 & \frac{4}{3} & \frac{6}{5} & \frac{8}{7} & \frac{10}{9} \\  \frac{9}{2} & \frac{9}{5} & \frac{81}{56} & \frac{72}{55} & \frac{225}{182} \\  \frac{32}{3} & \frac{256}{105} & \frac{96}{55} & \frac{2048}{1365} & \frac{4000}{2907} \\  \frac{625}{24} & \frac{625}{189} & \frac{16875}{8008} & \frac{5000}{2907} & \frac{390625}{255024} \\  \frac{324}{5} & \frac{1728}{385} & \frac{19683}{7735} & \frac{331776}{168245} & \frac{4500}{2639} \\  \frac{117649}{720} & \frac{117649}{19305} & \frac{3176523}{1033600} & \frac{15059072}{6660225} & \frac{367653125}{193666176} \end{array} \right)$$ which are the sequences within the logarithms above but transposed. Here  I tried to permute indexes $j$,$n$ and $k$ in an attempt to transpose the matrix but I did not succeed. In addition to the known (conjectured): $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\frac{n^{n-1}}{(n-1)!}$$ a oeis search and some manipulation gives the following: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\frac{n^{n-1}}{\frac{(n-1)! (2 n-1)\text{!!}}{n!}}$$ The following appears to be an example of a relationship: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+1}^{n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for $n$ an integer $\geq 1$. Also the limit $s \to 1$ seems be possible to put to any complex number $S=a+i b$, $s \to S$. So the question repeated is: Does: $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ equal: $$\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for any complex number $S=a+i b$ and integer values of $n \geq 1$ and $t \geq 1 $? The question seems to simplify to the following identity $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1}{k^{s-1}}=\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1}{k^{s-1}}$$ for $S$ a complex number not equal to $1$ and $n=1,2,3,4,5,...$ and $t=1,2,3,4,5,...$ S = 1.23456789 + I*9.87654321 Monitor[Table[   Chop[N[Table[       Sum[Limit[Zeta[s]*Sum[1/k^(s - 1), {k, (m - 1)*n + 1, m*n}],          s -> S], {m, 1, t}], {n, 1, 12}]] -      Table[Limit[Zeta[s]*Sum[1/k^(s - 1), {k, 1, t*n}], s -> S], {n, 1,        12}]], {t, 1, 6}], t] Simpler still: $$\sum _{m=1}^t \left(\sum _{k=(m-1) n+1}^{m n} \frac{1}{k^{s-1}}\right)=\sum _{k=1}^{n t} \frac{1}{k^{s-1}}$$ for $S=a+i b$ and $n=1,2,3,4,5,..$ $t=1,2,3,4,5,..$ Can anyone simplify the indexes in the double sum?","Does: $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ equal: $$\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for any complex number $S$ and any integers $n$ and $t$? Which as a Mathematica program is: S = 1.23456789 + I*9.87654321 Monitor[Table[   Chop[N[Table[       Sum[Limit[         Zeta[s]*Sum[(1 - If[Mod[k, n] == 0, n, 0])/            k^(s - 1), {k, (m - 1)*n + 1, m*n}], s -> S], {m, 1,          t}], {n, 1, 12}]] -         Table[      Limit[Zeta[s]*        Sum[(1 - If[Mod[k, n] == 0, n, 0])/k^(s - 1), {k, 1, t*n}],        s -> S], {n, 1, 12}]], {t, 1, 6}], t] Some background information on the problem. Mathematica knows that logarithms can be calculated as: $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ Logarithms can be added to give the logarithm of the product of their argument, $\lim_{s\to 1} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 1} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 1} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta$ $\log(6) = \log(2) + \log(3)$ This works only when the limits lets $s \to 1$. Trying the same for example with the limit letting $s \to 2$, it fails, as can be seen in this example: $\lim_{s\to 2} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 2} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 2} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta$ which gives us: $\lim_{s\to 2} \, \left(1-\frac{1}{6^{s-1}}\right) \zeta (s) = \lim_{s\to 2} \, \left(1-\frac{1}{2^{s-1}}\right) \zeta (s) + \lim_{s\to 2} \, \left(1-\frac{1}{3^{s-1}}\right) \zeta (s)$ which evaluated is: $\frac{5 \pi ^2}{36} = \frac{\pi ^2}{12} + \frac{\pi ^2}{9}$ but then we would have: $\frac{5 \pi ^2}{36} = \frac{7 \pi ^2}{36}$ which is not true. Looking at the logarithm above again, as a zeta function limit, we have: $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ In this question it was shown in the answer that logarithms have these numerators in their Dirichlet series: $$1-\text{If}[k \bmod n=0,n,0]$$ which as a matrix is: $$\begin{bmatrix} 0&0&0&0&0&0&0 \\ 1&-1&1&-1&1&-1&1 \\ 1&1&-2&1&1&-2&1 \\ 1&1&1&-3&1&1&1 \\ 1&1&1&1&-4&1&1 \\ 1&1&1&1&1&-5&1 \\ 1&1&1&1&1&1&-6 \end{bmatrix}$$ which is an infinite matrix where each row is the numerators in the Dirichlet series that converges to $\log(n)$ for $s=1$. In the zeta function limit for logarithms the part within the parentheses $\left(1-\frac{1}{n^{s-1}}\right)$ in $\log(n) = \lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$ will go towards zero in the limit: $\lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) = 0$ In a different matrix: $$\displaystyle T = \begin{bmatrix} +1&+1&+1&+1&+1&+1&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \end{bmatrix}$$ where $\Lambda(n) = \displaystyle \sum\limits_{k=1}^{\infty}\frac{T(n,k)}{k}$, and where $\Lambda(n)$ is the von Mangoldt function as proven in the answer to this question, we have the triangle: $$\displaystyle \begin{bmatrix} +1 \\ +1&-1&=0 \\ +1&+1&-2&=0 \\ +1&-1&+1&-1&=0 \\ +1&+1&+1&+1&-4&=0 \\ +1&-1&-2&-1&+1&+2&=0 \\ +1&+1&+1&+1&+1&+1&-6&=0 \end{bmatrix}$$ where we see that the row sums are also zero. Looking again at the earlier matrix we see the same thing (with exception of the first row, but $\Lambda(0) = 0$ anyways): $$\displaystyle \begin{bmatrix} 0=0 \\ 1&-1=0 \\ 1&1&-2=0 \\ 1&1&1&-3=0 \\ 1&1&1&1&-4=0 \\ 1&1&1&1&1&-5=0 \\ 1&1&1&1&1&1&-6=0 \end{bmatrix}$$ writing zeta function limits of this latter matrix, and we have: $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}-\frac{1}{2^{s-1}}\right) \zeta (s) = \log (2)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}-\frac{2}{3^{s-1}}\right) \zeta (s) = \log \left(\frac{9}{2}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}-\frac{3}{4^{s-1}}\right) \zeta (s) = \log \left(\frac{32}{3}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}-\frac{4}{5^{s-1}}\right) \zeta (s) = \log \left(\frac{625}{24}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}+\frac{1}{5^{s-1}}-\frac{5}{6^{s-1}}\right) \zeta (s) = \log \left(\frac{324}{5}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{1^{s-1}}+\frac{1}{2^{s-1}}+\frac{1}{3^{s-1}}+\frac{1}{4^{s-1}}+\frac{1}{5^{s-1}}+\frac{1}{6^{s-1}}-\frac{6}{7^{s-1}}\right) \zeta (s) = \log \left(\frac{117649}{720}\right)$$ with the sequence: $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$ looking up numerators and denominators in the oeis we find sequences: http://oeis.org/A036505 and: http://oeis.org/A095996 Multiplying by $n!$ and searching for the sequence: $1, 4, 27, 256, 3125, 46656, 823543...$ we find when we scroll down to sequence https://oeis.org/A177885 the following formula: $$\frac{1}{2}+\frac{2 \pi  i \exp (1) \left(n-\frac{11}{8}\right)}{\exp (1) W\left(\frac{n-\frac{11}{8}}{\exp (1)}\right)}$$ involving the Lambert_W function for a good approximation of the n-th Riemann zeta zero. $W(x)$ or LambertW is undocumented in Mathematica as said in Mathworld, and the ProductLog[x] command is used instead. There is also a functional relation between the sequence from the zeta limits and the $1, 4, 27, 256, 3125, 46656, 823543...$ sequence which is as follows: $$\frac{x}{W(-x)} = x \sum _{n=1}^{\infty } \frac{1}{(W(-x)+1)^n}$$ Changing the argument $-x$ to $x$ we have the function used in the zeta zero approximation formula. This warrants some further investigation of the sequence $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$  from the zeta function limits. Setting the denominators equal to $\exp(n)^{s-1}$: $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}-\frac{1}{\exp ^{s-1}(2)}\right) \zeta (s) =1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}-\frac{2}{\exp ^{s-1}(3)}\right) \zeta (s)=3$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}-\frac{3}{\exp ^{s-1}(4)}\right) \zeta (s)=6$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}-\frac{4}{\exp ^{s-1}(5)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}+\frac{1}{\exp ^{s-1}(5)}-\frac{5}{\exp ^{s-1}(6)}\right) \zeta (s)=15$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{1}{\exp ^{s-1}(2)}+\frac{1}{\exp ^{s-1}(3)}+\frac{1}{\exp ^{s-1}(4)}+\frac{1}{\exp ^{s-1}(5)}+\frac{1}{\exp ^{s-1}(6)}-\frac{6}{\exp ^{s-1}(7)}\right) \zeta (s)=21$$ which appear to be the triangular numbers, $\frac{n(n+1)}{2}$. Reversing the triangular matrix: $$\displaystyle \begin{bmatrix} 0=0 \\ 1&-1=0 \\ 1&1&-2=0 \\ 1&1&1&-3=0 \\ 1&1&1&1&-4=0 \\ 1&1&1&1&1&-5=0 \\ 1&1&1&1&1&1&-6=0 \end{bmatrix}$$ into: $$\displaystyle \begin{bmatrix} 0=0 \\ -1&1=0 \\ -2&1&1=0 \\ -3&1&1&1=0 \\ -4&1&1&1&1=0 \\ -5&1&1&1&1&1=0 \\ -6&1&1&1&1&1&1=0 \end{bmatrix}$$ which is the triangle https://oeis.org/A167407 in the OEIS. Taking successive matrix powers of this triangle and reversing back we seem to get the whole Pascal triangle as zeta function limits. $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}-\frac{1}{\exp ^{s-1}(2)}\right) \zeta (s)=1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}-\frac{3}{\exp ^{s-1}(3)}\right) \zeta (s)=4$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}-\frac{6}{\exp ^{s-1}(4)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}-\frac{10}{\exp ^{s-1}(5)}\right) \zeta (s)=20$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}+\frac{5}{\exp ^{s-1}(5)}-\frac{15}{\exp ^{s-1}(6)}\right) \zeta (s)=35$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1)}+\frac{2}{\exp ^{s-1}(2)}+\frac{3}{\exp ^{s-1}(3)}+\frac{4}{\exp ^{s-1}(4)}+\frac{5}{\exp ^{s-1}(5)}+\frac{6}{\exp ^{s-1}(6)}-\frac{21}{\exp ^{s-1}(7)}\right) \zeta (s)=56$$ More in line with logarithms of $n=1,2,3,4,5...$ is to consider the following zeta function limits: $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+2)}-\frac{1}{\exp ^{s-1}(2+2)}\right) \zeta (s)=1$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+3)}+\frac{1}{\exp ^{s-1}(2+3)}-\frac{2}{\exp ^{s-1}(3+3)}\right) \zeta (s)=3$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+4)}+\frac{1}{\exp ^{s-1}(2+4)}+\frac{1}{\exp ^{s-1}(3+4)}-\frac{3}{\exp ^{s-1}(4+4)}\right) \zeta (s)=6$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+5)}+\frac{1}{\exp ^{s-1}(2+5)}+\frac{1}{\exp ^{s-1}(3+5)}+\frac{1}{\exp ^{s-1}(4+5)}-\frac{4}{\exp ^{s-1}(5+5)}\right) \zeta (s)=10$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+6)}+\frac{1}{\exp ^{s-1}(2+6)}+\frac{1}{\exp ^{s-1}(3+6)}+\frac{1}{\exp ^{s-1}(4+6)}+\frac{1}{\exp ^{s-1}(5+6)}-\frac{5}{\exp ^{s-1}(6+6)}\right) \zeta (s)=15$$ $$\lim_{s\to 1} \, \left(\frac{1}{\exp ^{s-1}(1+7)}+\frac{1}{\exp ^{s-1}(2+7)}+\frac{1}{\exp ^{s-1}(3+7)}+\frac{1}{\exp ^{s-1}(4+7)}+\frac{1}{\exp ^{s-1}(5+7)}+\frac{1}{\exp ^{s-1}(6+7)}-\frac{6}{\exp ^{s-1}(7+7)}\right) \zeta (s)=21$$ where we get the triangular numbers again. In other words there seems to be an invariance property. Converting back to unexponentiated denominators: $$\lim_{s\to 1} \, \left(\frac{1}{(1+2)^{s-1}}-\frac{1}{(2+2)^{s-1}}\right) \zeta (s)=\log \left(\frac{4}{3}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+3)^{s-1}}+\frac{1}{(2+3)^{s-1}}-\frac{2}{(3+3)^{s-1}}\right) \zeta (s)=\log \left(\frac{9}{5}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+4)^{s-1}}+\frac{1}{(2+4)^{s-1}}+\frac{1}{(3+4)^{s-1}}-\frac{3}{(4+4)^{s-1}}\right) \zeta (s)=\log \left(\frac{256}{105}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+5)^{s-1}}+\frac{1}{(2+5)^{s-1}}+\frac{1}{(3+5)^{s-1}}+\frac{1}{(4+5)^{s-1}}-\frac{4}{(5+5)^{s-1}}\right) \zeta (s)=\log \left(\frac{625}{189}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+6)^{s-1}}+\frac{1}{(2+6)^{s-1}}+\frac{1}{(3+6)^{s-1}}+\frac{1}{(4+6)^{s-1}}+\frac{1}{(5+6)^{s-1}}-\frac{5}{(6+6)^{s-1}}\right) \zeta (s)=\log \left(\frac{1728}{385}\right)$$ $$\lim_{s\to 1} \, \left(\frac{1}{(1+7)^{s-1}}+\frac{1}{(2+7)^{s-1}}+\frac{1}{(3+7)^{s-1}}+\frac{1}{(4+7)^{s-1}}+\frac{1}{(5+7)^{s-1}}+\frac{1}{(6+7)^{s-1}}-\frac{6}{(7+7)^{s-1}}\right) \zeta (s)=\log \left(\frac{117649}{19305}\right)$$ where we have: $1,4/3, 9/5, 256/105, 625/189, 1728/385, 117649/19305,...$ compared to the earlier: $1, 2, 9/2, 32/3, 625/24, 324/5, 117649/720...$ in other words, no invariance like with the triangular numbers. Drawing this further we generalize and write: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+1}^{n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+n+1}^{n+n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for which we get the sequences for $n=1,2,3,4,5,6,7,...$: $$\left\{0,\log (2),\log \left(\frac{9}{2}\right),\log \left(\frac{32}{3}\right),\log \left(\frac{625}{24}\right),\log \left(\frac{324}{5}\right),\log \left(\frac{117649}{720}\right),...\right\}$$ $$\left\{0,\log \left(\frac{4}{3}\right),\log \left(\frac{9}{5}\right),\log \left(\frac{256}{105}\right),\log \left(\frac{625}{189}\right),\log \left(\frac{1728}{385}\right),\log \left(\frac{117649}{19305}\right),...\right\}$$ $$\left\{0,\log \left(\frac{6}{5}\right),\log \left(\frac{81}{56}\right),\log \left(\frac{96}{55}\right),\log \left(\frac{16875}{8008}\right),\log \left(\frac{19683}{7735}\right),\log \left(\frac{3176523}{1033600}\right),...\right\}$$ $$\left\{0,\log \left(\frac{8}{7}\right),\log \left(\frac{72}{55}\right),\log \left(\frac{2048}{1365}\right),\log \left(\frac{5000}{2907}\right),\log \left(\frac{331776}{168245}\right),\log \left(\frac{15059072}{6660225}\right),...\right\}$$ $$\left\{0,\log \left(\frac{10}{9}\right),\log \left(\frac{225}{182}\right),\log \left(\frac{4000}{2907}\right),\log \left(\frac{390625}{255024}\right),\log \left(\frac{4500}{2639}\right),\log \left(\frac{367653125}{193666176}\right),...\right\}$$ What is the relationship between these numbers? Trying to describe the fractions within the logarithms I wrote: $$\frac{((j+1) n)^j}{\prod _{k=1}^j ((j+1) (n-1)+k)}$$ $$n=1,2,3,4,5$$ $$j=0,1,2,3,4,5,6$$ which gives: $$\left( \begin{array}{ccccc}  1 & 1 & 1 & 1 & 1 \\  2 & \frac{4}{3} & \frac{6}{5} & \frac{8}{7} & \frac{10}{9} \\  \frac{9}{2} & \frac{9}{5} & \frac{81}{56} & \frac{72}{55} & \frac{225}{182} \\  \frac{32}{3} & \frac{256}{105} & \frac{96}{55} & \frac{2048}{1365} & \frac{4000}{2907} \\  \frac{625}{24} & \frac{625}{189} & \frac{16875}{8008} & \frac{5000}{2907} & \frac{390625}{255024} \\  \frac{324}{5} & \frac{1728}{385} & \frac{19683}{7735} & \frac{331776}{168245} & \frac{4500}{2639} \\  \frac{117649}{720} & \frac{117649}{19305} & \frac{3176523}{1033600} & \frac{15059072}{6660225} & \frac{367653125}{193666176} \end{array} \right)$$ which are the sequences within the logarithms above but transposed. Here  I tried to permute indexes $j$,$n$ and $k$ in an attempt to transpose the matrix but I did not succeed. In addition to the known (conjectured): $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\frac{n^{n-1}}{(n-1)!}$$ a oeis search and some manipulation gives the following: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\frac{n^{n-1}}{\frac{(n-1)! (2 n-1)\text{!!}}{n!}}$$ The following appears to be an example of a relationship: $$\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^n \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}=\lim_{s\to 1} \, \zeta (s) \sum _{k=1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+1}^{n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+1}^{n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}+\lim_{s\to 1} \, \zeta (s) \sum _{k=n+n+n+1}^{n+n+n+n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for $n$ an integer $\geq 1$. Also the limit $s \to 1$ seems be possible to put to any complex number $S=a+i b$, $s \to S$. So the question repeated is: Does: $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ equal: $$\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1-\text{If}[k \bmod n=0,n,0]}{k^{s-1}}$$ for any complex number $S=a+i b$ and integer values of $n \geq 1$ and $t \geq 1 $? The question seems to simplify to the following identity $$\sum _{m=1}^t \lim_{s\to \text{S}} \, \zeta (s) \sum _{k=(m-1) n+1}^{m n} \frac{1}{k^{s-1}}=\lim_{s\to \text{S}} \, \zeta (s) \sum _{k=1}^{n t} \frac{1}{k^{s-1}}$$ for $S$ a complex number not equal to $1$ and $n=1,2,3,4,5,...$ and $t=1,2,3,4,5,...$ S = 1.23456789 + I*9.87654321 Monitor[Table[   Chop[N[Table[       Sum[Limit[Zeta[s]*Sum[1/k^(s - 1), {k, (m - 1)*n + 1, m*n}],          s -> S], {m, 1, t}], {n, 1, 12}]] -      Table[Limit[Zeta[s]*Sum[1/k^(s - 1), {k, 1, t*n}], s -> S], {n, 1,        12}]], {t, 1, 6}], t] Simpler still: $$\sum _{m=1}^t \left(\sum _{k=(m-1) n+1}^{m n} \frac{1}{k^{s-1}}\right)=\sum _{k=1}^{n t} \frac{1}{k^{s-1}}$$ for $S=a+i b$ and $n=1,2,3,4,5,..$ $t=1,2,3,4,5,..$ Can anyone simplify the indexes in the double sum?",,"['limits', 'complex-numbers', 'logarithms', 'riemann-zeta']"
69,$\limsup$ of indicator function of converging sets,of indicator function of converging sets,\limsup,"Let $\{A_n\}_{n=1}^{\infty}$ be a sequence of bounded sets of $\mathbb{R}^n$, such that $A_n \rightarrow A \subset \mathbb{R}^n$. Let $\mathbb{1}: \mathbb{R}^n \rightarrow \{0,1\}$ be the indicator function . 1) I am wondering if the following statement holds. Assume $A_n$'s compact. For all $x \in \mathbb{R}^n$, it holds that $$\limsup_{n \rightarrow \infty} \mathbb{1}_{A_n}(x) \leq \mathbb{1}_{A}(x). $$ 2) What if the $A_n$'s are open? Comments. I tried to follow this post . I am not clear whether the compactness is actually needed.","Let $\{A_n\}_{n=1}^{\infty}$ be a sequence of bounded sets of $\mathbb{R}^n$, such that $A_n \rightarrow A \subset \mathbb{R}^n$. Let $\mathbb{1}: \mathbb{R}^n \rightarrow \{0,1\}$ be the indicator function . 1) I am wondering if the following statement holds. Assume $A_n$'s compact. For all $x \in \mathbb{R}^n$, it holds that $$\limsup_{n \rightarrow \infty} \mathbb{1}_{A_n}(x) \leq \mathbb{1}_{A}(x). $$ 2) What if the $A_n$'s are open? Comments. I tried to follow this post . I am not clear whether the compactness is actually needed.",,"['real-analysis', 'limits', 'convergence-divergence']"
70,one-sided continuity and one-sided derivative?,one-sided continuity and one-sided derivative?,,"A continuous function is continuous at an $x$ value (call the $x$ value that we're interested in $c$ ) if both of these conditions are met and are true: $f(c)= \text{some real number}$ $\lim_{x\to c} = \text{that same real number}$ So, when we state this definition, we referring to a function being continuous on the open interval $(a,b)$ , not the closed interval $[a,b]$ , correct? Because, an open interval would allow a left and right limit to exist since the limit can approach from both sides, correct? Because for any point that is in an open interval, you can always mark off a little interval around it where that interval is still within the original open interval. So for any point in a given open interval, we have a little ""space"" on either side for our left and right limits to form. But endpoints on $[a,b]$ cannot be approached from both sides, so a function defined on this interval is right-continuous at $a$ and left-continuous at $b$ and has only one-sided limits at endpoints $a$ and $b$ ? So then a function on $[a,b]$ has only one-sided continuity, correct? Because how can an endpoint $a$ be approached from the left since it's an $endpoint$ , it could only make the function right-continuous, not totally continuous. And a derivative is usually defined on some differentiable interval $(a,b)$ , but it could also be differentiable on a closed interval $[a,b]$ , but in this case it would be a one-sided derivative, correct? Why is continuity defined mostly on closed intervals, when closed intervals mean that it is only continuous from one-side, and open intervals mean that it's both right-continuous and left-continuous and hence has total continuity?","A continuous function is continuous at an value (call the value that we're interested in ) if both of these conditions are met and are true: So, when we state this definition, we referring to a function being continuous on the open interval , not the closed interval , correct? Because, an open interval would allow a left and right limit to exist since the limit can approach from both sides, correct? Because for any point that is in an open interval, you can always mark off a little interval around it where that interval is still within the original open interval. So for any point in a given open interval, we have a little ""space"" on either side for our left and right limits to form. But endpoints on cannot be approached from both sides, so a function defined on this interval is right-continuous at and left-continuous at and has only one-sided limits at endpoints and ? So then a function on has only one-sided continuity, correct? Because how can an endpoint be approached from the left since it's an , it could only make the function right-continuous, not totally continuous. And a derivative is usually defined on some differentiable interval , but it could also be differentiable on a closed interval , but in this case it would be a one-sided derivative, correct? Why is continuity defined mostly on closed intervals, when closed intervals mean that it is only continuous from one-side, and open intervals mean that it's both right-continuous and left-continuous and hence has total continuity?","x x c f(c)= \text{some real number} \lim_{x\to c} = \text{that same real number} (a,b) [a,b] [a,b] a b a b [a,b] a endpoint (a,b) [a,b]","['calculus', 'limits', 'derivatives', 'continuity', 'self-learning']"
71,Calculating $\lim_{x\to+\infty}(\sqrt{x^2-3x}-x)$,Calculating,\lim_{x\to+\infty}(\sqrt{x^2-3x}-x),"Let $f(x) = \sqrt{x^2-3x}$ and $g(x) = x$. Calculate the following limits, showing all working. I've done the first two - $$\lim_{x\to0}f(x)\\ =\lim_{x\to0}\sqrt{x^2-3x}=0$$ $$\lim_{x\to-\infty}\frac{f(x)}{g(x)}\\ =\lim_{x\to-\infty}\frac{\sqrt{x^2-3x}}{x}=\sqrt{1-\frac{3}{x}}=1$$ How do I calculate this one? $$\lim_{x\to+\infty}(f(x)-g(x))$$","Let $f(x) = \sqrt{x^2-3x}$ and $g(x) = x$. Calculate the following limits, showing all working. I've done the first two - $$\lim_{x\to0}f(x)\\ =\lim_{x\to0}\sqrt{x^2-3x}=0$$ $$\lim_{x\to-\infty}\frac{f(x)}{g(x)}\\ =\lim_{x\to-\infty}\frac{\sqrt{x^2-3x}}{x}=\sqrt{1-\frac{3}{x}}=1$$ How do I calculate this one? $$\lim_{x\to+\infty}(f(x)-g(x))$$",,"['limits', 'radicals']"
72,Simplify a radical expression for a limit,Simplify a radical expression for a limit,,"I'm supposed to find the slope of the tangent of the function $$D(p)=\frac{20}{\sqrt{p-1}}$$ at $p=5$. Using the definition of slope of a tangent at a point, I get $$\lim_{h \to 0}\frac{\frac{20}{\sqrt{(5+h)-1}}-10}{h}$$ Which, using ""regular"" (high school) algebra I simplified to $$\frac{10(2-\sqrt{h+4})}{h\sqrt{h+4}}.$$ Rationalizing the numerator or denominator doesn't help in terms of removing $h$ as a factor in the denominator of the slope. How do you find the limit algebraically in this case (or can you)?","I'm supposed to find the slope of the tangent of the function $$D(p)=\frac{20}{\sqrt{p-1}}$$ at $p=5$. Using the definition of slope of a tangent at a point, I get $$\lim_{h \to 0}\frac{\frac{20}{\sqrt{(5+h)-1}}-10}{h}$$ Which, using ""regular"" (high school) algebra I simplified to $$\frac{10(2-\sqrt{h+4})}{h\sqrt{h+4}}.$$ Rationalizing the numerator or denominator doesn't help in terms of removing $h$ as a factor in the denominator of the slope. How do you find the limit algebraically in this case (or can you)?",,"['calculus', 'limits']"
73,Finding the limit with three variables,Finding the limit with three variables,,"Here is the original problem:  $$\lim_{(x,y,z)\to (0,0,0)}{(\cos x-1)\sin(2y)(e^{3z}-1)\over x^2yz}$$ I was thinking about splitting up the limit like this: $$\lim_{(x,y,z)\to (0,0,0)}{(\cos x-1)\over x^2}\cdot\lim_{(x,y,z)\to (0,0,0)}{\sin(2y)\over y}\cdot\lim_{(x,y,z)\to (0,0,0)}{e^{3z}-1\over z}$$ And then break it down some more:$$\lim_{x\to 0}{(\cos x-1)\over x^2}\cdot\lim_{y\to 0}{\sin(2y)\over y}\cdot\lim_{z\to 0}{e^{3z}-1\over z}$$ And then I want to use L'Hospital's rule. If someone could let me know if I'm heading in the right direction or not, that would be great. Thanks!","Here is the original problem:  $$\lim_{(x,y,z)\to (0,0,0)}{(\cos x-1)\sin(2y)(e^{3z}-1)\over x^2yz}$$ I was thinking about splitting up the limit like this: $$\lim_{(x,y,z)\to (0,0,0)}{(\cos x-1)\over x^2}\cdot\lim_{(x,y,z)\to (0,0,0)}{\sin(2y)\over y}\cdot\lim_{(x,y,z)\to (0,0,0)}{e^{3z}-1\over z}$$ And then break it down some more:$$\lim_{x\to 0}{(\cos x-1)\over x^2}\cdot\lim_{y\to 0}{\sin(2y)\over y}\cdot\lim_{z\to 0}{e^{3z}-1\over z}$$ And then I want to use L'Hospital's rule. If someone could let me know if I'm heading in the right direction or not, that would be great. Thanks!",,"['limits', 'multivariable-calculus']"
74,Interchange of partial derivative and limit,Interchange of partial derivative and limit,,"Consider the following expression: $$\frac{\partial}{\partial m} \lim_{T \rightarrow \infty} \gamma(T,m)$$ where $\gamma$ is a function of $T$ and $m$. My question is just: can I permute the partial derivative and the limit operators ? I suppose that I can, given that the concerned variable is different for each operator but I still need a confirmation.","Consider the following expression: $$\frac{\partial}{\partial m} \lim_{T \rightarrow \infty} \gamma(T,m)$$ where $\gamma$ is a function of $T$ and $m$. My question is just: can I permute the partial derivative and the limit operators ? I suppose that I can, given that the concerned variable is different for each operator but I still need a confirmation.",,"['real-analysis', 'limits', 'multivariable-calculus', 'partial-derivative']"
75,Can the integral of $x^x$ be found?,Can the integral of  be found?,x^x,"I'm interested in knowing if the indefinite integral of $x^x$ can be found in terms of elementary functions. I am under the impression (be it correct or incorrect) that it can be found. This is why: the derivative of $x^x$ has $x^x$ in it ($d/dx[x^x] = x^x(\ln(x) + 1)$). The derivative is quite easy to find with logarithmic differentiation, but the integral – not so much. If the indefinite integral cannot be defined in terms of elementary functions, why? If the indefinite integral can be found, would you please work it out? I find myself lost here, I've tried multiple methods.","I'm interested in knowing if the indefinite integral of $x^x$ can be found in terms of elementary functions. I am under the impression (be it correct or incorrect) that it can be found. This is why: the derivative of $x^x$ has $x^x$ in it ($d/dx[x^x] = x^x(\ln(x) + 1)$). The derivative is quite easy to find with logarithmic differentiation, but the integral – not so much. If the indefinite integral cannot be defined in terms of elementary functions, why? If the indefinite integral can be found, would you please work it out? I find myself lost here, I've tried multiple methods.",,"['calculus', 'limits', 'integration', 'derivatives']"
76,Show that $\displaystyle\lim_{x\rightarrow 0}\frac{5^x-4^x}{x}=\log_e\left({\frac{5}{4}}\right)$,Show that,\displaystyle\lim_{x\rightarrow 0}\frac{5^x-4^x}{x}=\log_e\left({\frac{5}{4}}\right),Show that $\displaystyle\lim_{x\rightarrow 0}\frac{5^x-4^x}{x}=\log_e\left({\frac{5}{4}}\right)$ If $0<\theta < \frac{\pi}{2} $ and $\sin 2\theta=\cos 3\theta~~$ then find the value of $\sin\theta$,Show that $\displaystyle\lim_{x\rightarrow 0}\frac{5^x-4^x}{x}=\log_e\left({\frac{5}{4}}\right)$ If $0<\theta < \frac{\pi}{2} $ and $\sin 2\theta=\cos 3\theta~~$ then find the value of $\sin\theta$,,['limits']
77,$\lim_{x\to\infty}\sqrt [n] {p_n(x)}-\sqrt [m] {p_m(x})$ where $p_k(x)$ is polynomial of order k.,where  is polynomial of order k.,\lim_{x\to\infty}\sqrt [n] {p_n(x)}-\sqrt [m] {p_m(x}) p_k(x),"What is the easiest way to evaluate  $$ \lim_{x\to\infty}\sqrt [n] {p(x)}-\sqrt [m] {q(x}) $$ where $p,q\in\mathbb{R}[x]$ with $\deg p= n$, $\deg q=m$ .","What is the easiest way to evaluate  $$ \lim_{x\to\infty}\sqrt [n] {p(x)}-\sqrt [m] {q(x}) $$ where $p,q\in\mathbb{R}[x]$ with $\deg p= n$, $\deg q=m$ .",,"['real-analysis', 'limits', 'polynomials']"
78,How to prove this limit composition theorem?,How to prove this limit composition theorem?,,"If $$\displaystyle \lim_{x \rightarrow c}f(x)=l$$ and $$\displaystyle  \lim_{x \rightarrow l}g(x)=L$$ and, in every punctured neighbourhood of $c,\;f(x) \neq l,$ then $\displaystyle \lim_{x \rightarrow c}g(f(x))=L$ . Note that the first two conditions are insufficient for the conclusion to hold. (For a clear example, see the first comment in Is there a better counter-example? (problem involving limit of composition of functions) ) But if all three conditions are satisfied, the conclusion may be drawn. This is because the 3rd condition ensures that near $c$ , $gf$ won't be forced to take the value of $g$ at $l$ . I.e. $\displaystyle \lim_{x \rightarrow c}gf(x)$ will be equal to $\displaystyle \lim_{x \rightarrow l}g(x)$ instead of necessarily being equal to $g(l)$ . (Note that $\displaystyle \lim_{x \rightarrow l}g(x)=g(l)$ iff. $g$ is continuous at $l$ .) So that's my informal explanation for the theorem. But I'm having difficulty writing down a proper proof. Please use the sequential definition of limits (instead of epsilon-delta), if required.","If and and, in every punctured neighbourhood of then . Note that the first two conditions are insufficient for the conclusion to hold. (For a clear example, see the first comment in Is there a better counter-example? (problem involving limit of composition of functions) ) But if all three conditions are satisfied, the conclusion may be drawn. This is because the 3rd condition ensures that near , won't be forced to take the value of at . I.e. will be equal to instead of necessarily being equal to . (Note that iff. is continuous at .) So that's my informal explanation for the theorem. But I'm having difficulty writing down a proper proof. Please use the sequential definition of limits (instead of epsilon-delta), if required.","\displaystyle \lim_{x \rightarrow c}f(x)=l \displaystyle
 \lim_{x \rightarrow l}g(x)=L c,\;f(x) \neq l, \displaystyle \lim_{x \rightarrow c}g(f(x))=L c gf g l \displaystyle \lim_{x \rightarrow c}gf(x) \displaystyle \lim_{x \rightarrow l}g(x) g(l) \displaystyle \lim_{x \rightarrow l}g(x)=g(l) g l","['real-analysis', 'limits', 'functions', 'continuity']"
79,How can I write $e^{ki\theta}/k$ as a nice integral?,How can I write  as a nice integral?,e^{ki\theta}/k,"I am trying to evaluate the sum $$\sum_{k=1}^{n}\frac{\cos\left(k\theta\right)}{k}$$ using $\sum_{k=1}^{n}e^{ki\theta}/k$ as a first step. I recognize this as being similar to $\log\left(1-e^{i\theta}\right)$ plus some terms for sufficiently large $n;$ however, I am tempted to write $$i\sum_{k=1}^{n}\frac{e^{ki\theta}}{ik}=i\sum_{k=1}^{n}\left(-\int^{+\infty}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha\right)$$ and then replace the sum and the integral with justification, but I'm not sure the improper integral converges. A careful analysis on the improper integral yields $$-\int^{+\infty}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha=-\lim_{b\rightarrow+\infty}\int^{b}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha=\frac{1}{ik}\left[\lim_{b\rightarrow+\infty}\left(e^{ki\theta}-e^{kib}\right)\right]=\frac{1}{ik}\left[e^{ki\theta}-\lim_{b\rightarrow+\infty}\left(e^{kib}\right)\right]$$ and I'm stuck here. I'm not sure how to evaluate $$\lim_{b\rightarrow+\infty}\left(e^{kib}\right)$$ for a couple of reasons. First, I know that if $k\in\mathbb{N}$ and $b\in\mathbb{R},$ then $\left|e^{kib}\right|\leq1.$ It seems that whenever $b$ is close enough to $2t\pi$ ($t$ integer), $\left|e^{kib}\right|$ approches $1.$ In my mind, a graph of $f(x)=\left|e^{kib}\right|$ would sporadically get close to one. For my assumption to behold, I need $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)=0.$ My guts say maybe it is right, but I don't know how to properly justify it. It seems like the limit can be $1,$ another complex number or simply doesn't exist. So, how can I formally show what $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)$ is?  If $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)\neq0,$ is there another integral that nicely fits this situation?","I am trying to evaluate the sum $$\sum_{k=1}^{n}\frac{\cos\left(k\theta\right)}{k}$$ using $\sum_{k=1}^{n}e^{ki\theta}/k$ as a first step. I recognize this as being similar to $\log\left(1-e^{i\theta}\right)$ plus some terms for sufficiently large $n;$ however, I am tempted to write $$i\sum_{k=1}^{n}\frac{e^{ki\theta}}{ik}=i\sum_{k=1}^{n}\left(-\int^{+\infty}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha\right)$$ and then replace the sum and the integral with justification, but I'm not sure the improper integral converges. A careful analysis on the improper integral yields $$-\int^{+\infty}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha=-\lim_{b\rightarrow+\infty}\int^{b}_{\theta}e^{ki\alpha}\,\mathrm{d}\alpha=\frac{1}{ik}\left[\lim_{b\rightarrow+\infty}\left(e^{ki\theta}-e^{kib}\right)\right]=\frac{1}{ik}\left[e^{ki\theta}-\lim_{b\rightarrow+\infty}\left(e^{kib}\right)\right]$$ and I'm stuck here. I'm not sure how to evaluate $$\lim_{b\rightarrow+\infty}\left(e^{kib}\right)$$ for a couple of reasons. First, I know that if $k\in\mathbb{N}$ and $b\in\mathbb{R},$ then $\left|e^{kib}\right|\leq1.$ It seems that whenever $b$ is close enough to $2t\pi$ ($t$ integer), $\left|e^{kib}\right|$ approches $1.$ In my mind, a graph of $f(x)=\left|e^{kib}\right|$ would sporadically get close to one. For my assumption to behold, I need $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)=0.$ My guts say maybe it is right, but I don't know how to properly justify it. It seems like the limit can be $1,$ another complex number or simply doesn't exist. So, how can I formally show what $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)$ is?  If $\displaystyle\lim_{b\rightarrow+\infty}\left(e^{kib}\right)\neq0,$ is there another integral that nicely fits this situation?",,"['limits', 'improper-integrals']"
80,Integration limits when integrating both sides,Integration limits when integrating both sides,,"I have been working on solving differential equations and this is really cracking me up. I obtained the following equation: dz/dr = r and I wish to obtain z in terms of r, given that we know that z = 0 when r = 0; is this: $dz = r dr$ $\int_0^z 1\,dz = \int_0^r r \,dr$ $==> z = r^2/2$ the right way of doing this(are the limits of the integration correct)? thank you","I have been working on solving differential equations and this is really cracking me up. I obtained the following equation: dz/dr = r and I wish to obtain z in terms of r, given that we know that z = 0 when r = 0; is this: $dz = r dr$ $\int_0^z 1\,dz = \int_0^r r \,dr$ $==> z = r^2/2$ the right way of doing this(are the limits of the integration correct)? thank you",,"['limits', 'integration']"
81,Evaluating $\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n}$,Evaluating,\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n},"We know that one of the characterizations of the exponential function is: $$e^x=\lim_{n\rightarrow\infty}\left(1+\frac{x}{n}\right)^{n}$$ Trivially, it follows that $\lim_{n\rightarrow\infty}\left(1-\frac{x}{n}\right)^{n}=e^{-x}$ I am wondering about $$\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n}$$ where $a$ is a real number.  Is the following evaluation of the expression correct? $$\begin{array}{rcl}\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n}&=&\lim_{n\rightarrow\infty}\left(1-\frac{xn^{-a}}{n}\right)^{n}\\ &=&\lim_{n\rightarrow\infty}e^{-xn^{-a}}\\ &=&\left\{\begin{array}{rl}1,&a>0\\e^{-x},&a=0\\0,&a<0\end{array}\right. \end{array}$$ I am uncomfortable taking the second equality, not sure what the justification is...","We know that one of the characterizations of the exponential function is: $$e^x=\lim_{n\rightarrow\infty}\left(1+\frac{x}{n}\right)^{n}$$ Trivially, it follows that $\lim_{n\rightarrow\infty}\left(1-\frac{x}{n}\right)^{n}=e^{-x}$ I am wondering about $$\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n}$$ where $a$ is a real number.  Is the following evaluation of the expression correct? $$\begin{array}{rcl}\lim_{n\rightarrow\infty}\left(1-\frac{x}{n^{1+a}}\right)^{n}&=&\lim_{n\rightarrow\infty}\left(1-\frac{xn^{-a}}{n}\right)^{n}\\ &=&\lim_{n\rightarrow\infty}e^{-xn^{-a}}\\ &=&\left\{\begin{array}{rl}1,&a>0\\e^{-x},&a=0\\0,&a<0\end{array}\right. \end{array}$$ I am uncomfortable taking the second equality, not sure what the justification is...",,['limits']
82,Evaluate a limit by using squeeze theorem,Evaluate a limit by using squeeze theorem,,"We're supposed to use the Squeeze Theorem to prove that $$\lim_{x\to 0} {1-\cos x\over x^2} = \frac12$$ I tried this: $$-1\le \cos x \le 1$$  $$-1\le -\cos x \le 1$$ $$0\le 1-\cos x \le 2$$ $$0\le {1-\cos x\over x^2} \le {2\over x^2}$$ Then using limits we have: $$\lim_{x\to 0}0\le \lim_{x\to 0} {1-\cos x\over x^2} \le \lim_{x\to 0}{2\over x^2}$$ And for obvious reasons the first limit is $\Bbb {0}$, and the third limit is $\Bbb \infty$ What do I do now? Or what am I doing wrong? Thanks in advance","We're supposed to use the Squeeze Theorem to prove that $$\lim_{x\to 0} {1-\cos x\over x^2} = \frac12$$ I tried this: $$-1\le \cos x \le 1$$  $$-1\le -\cos x \le 1$$ $$0\le 1-\cos x \le 2$$ $$0\le {1-\cos x\over x^2} \le {2\over x^2}$$ Then using limits we have: $$\lim_{x\to 0}0\le \lim_{x\to 0} {1-\cos x\over x^2} \le \lim_{x\to 0}{2\over x^2}$$ And for obvious reasons the first limit is $\Bbb {0}$, and the third limit is $\Bbb \infty$ What do I do now? Or what am I doing wrong? Thanks in advance",,['limits']
83,Cesàro mean for a divergent sequence,Cesàro mean for a divergent sequence,,"Given a real sequence $(a_n)_n$ converging to a finite value $a$, a property of the Cesàro mean, defined as the arithmetic mean $$ b_n=\frac{a_1+\ldots+a_n}{n}, $$ is $$ \lim_{n\to\infty}b_n=a,\tag1 $$ so that, supposing $a_n\neq0$ $\forall\,n$ and $a\neq0$, we can also deduce $$ \lim_{n\to\infty}\frac{b_n}{a_n}=1.\tag2 $$ Is result $(2)$ also valid for $a=0$? And are results $(1)$ and/or $(2)$ also valid for $a=+\infty$?","Given a real sequence $(a_n)_n$ converging to a finite value $a$, a property of the Cesàro mean, defined as the arithmetic mean $$ b_n=\frac{a_1+\ldots+a_n}{n}, $$ is $$ \lim_{n\to\infty}b_n=a,\tag1 $$ so that, supposing $a_n\neq0$ $\forall\,n$ and $a\neq0$, we can also deduce $$ \lim_{n\to\infty}\frac{b_n}{a_n}=1.\tag2 $$ Is result $(2)$ also valid for $a=0$? And are results $(1)$ and/or $(2)$ also valid for $a=+\infty$?",,"['real-analysis', 'sequences-and-series', 'limits']"
84,Evaluating $\lim_{y \to 0^+} (\cosh (3/y))^y$,Evaluating,\lim_{y \to 0^+} (\cosh (3/y))^y,"Evaluating $$\lim_{y \to 0^+} (\cosh (3/y))^y$$ This is what I have tried: $L = (cosh(3/y))^y$ $\ln L = \frac{\cosh(3/y)}{1/y}$, applying L'Hopital's rule, I get: $\ln L = \frac{-3y^2(\sinh (3/y))}{(y^2)}$ $\ln L = 3\sinh (3/y)$ Now I seem to be stuck in a loop between $\sinh$ and cosh. I know the answer is supposed to be $e^3$, but how do I proceed from here? Any help would be greatly appreciated!","Evaluating $$\lim_{y \to 0^+} (\cosh (3/y))^y$$ This is what I have tried: $L = (cosh(3/y))^y$ $\ln L = \frac{\cosh(3/y)}{1/y}$, applying L'Hopital's rule, I get: $\ln L = \frac{-3y^2(\sinh (3/y))}{(y^2)}$ $\ln L = 3\sinh (3/y)$ Now I seem to be stuck in a loop between $\sinh$ and cosh. I know the answer is supposed to be $e^3$, but how do I proceed from here? Any help would be greatly appreciated!",,"['trigonometry', 'limits']"
85,A limit related to a recurrence sequence,A limit related to a recurrence sequence,,"Let be the following sequence $x_{n}$, $n\geq0$, $0<x_{0}<1$ that satisfies the following recurrence: $$ x_{n+1}=x_{n}-x_{n}^2+x_{n}^3-x_{n}^4+x_{n}^5-x_{n}^6$$ Having given this, i'm required to calculate: $$\lim_{n\rightarrow\infty} n x_{n} $$","Let be the following sequence $x_{n}$, $n\geq0$, $0<x_{0}<1$ that satisfies the following recurrence: $$ x_{n+1}=x_{n}-x_{n}^2+x_{n}^3-x_{n}^4+x_{n}^5-x_{n}^6$$ Having given this, i'm required to calculate: $$\lim_{n\rightarrow\infty} n x_{n} $$",,"['real-analysis', 'sequences-and-series', 'limits']"
86,How to find the limit of a markov chain,How to find the limit of a markov chain,,"Given a markov chain where the next state is related to the previous state by the following matrix: $$\begin{array}{c|ccc} & A & B & C\\ \hline A & p_1 & q_1 & r_1\\ B & p_2 & q_2 & r_2\\ C & p_3 & q_3 & r_3\\ \end{array}$$ Where the system of equations is given by A$_{n+1} = p_1A_n + p_2B_n+p_3C_n$ B$_{n+1} = q_1A_n + q_2B_n+q_3C_n$ C$_{n+1} = r_1A_n + r_2B_n+r_3C_n$ How can the final relationships be found? (i.e. the limit of each function as n $\rightarrow \infty$, assuming A$_0 = B_0 = C_0$).","Given a markov chain where the next state is related to the previous state by the following matrix: $$\begin{array}{c|ccc} & A & B & C\\ \hline A & p_1 & q_1 & r_1\\ B & p_2 & q_2 & r_2\\ C & p_3 & q_3 & r_3\\ \end{array}$$ Where the system of equations is given by A$_{n+1} = p_1A_n + p_2B_n+p_3C_n$ B$_{n+1} = q_1A_n + q_2B_n+q_3C_n$ C$_{n+1} = r_1A_n + r_2B_n+r_3C_n$ How can the final relationships be found? (i.e. the limit of each function as n $\rightarrow \infty$, assuming A$_0 = B_0 = C_0$).",,"['limits', 'markov-chains']"
87,Interchanging the order of limits of a particular double sequence,Interchanging the order of limits of a particular double sequence,,"I would like to interchange the iterated limits of a particular double sequence $a_{n,m}$, that is I would like that $\lim_{m\to\infty}\lim_{n\to\infty}a_{n,m}=\lim_{n\to\infty}\lim_{m\to\infty}a_{n,m}$. The sequence has the following properties. Firstly, it is actually formed by a sum over one of the variables of a double sequence $b_{n,i}$, so that $a_{n,m}=\sum_{i=1}^mb_{n,i}$, where $0\leq b_{n,i}\leq 1$. Hence for each fixed $n$, $a_{n,m}$ is an increasing sequence in $m$. For each fixed $n$ the limit $\lim_{m\to\infty}a_{n,m}=l_n$ exists, i.e. the series $\sum_{i=1}^\infty b_{n,i}$ converges. The iterated limit $\lim_{n\to\infty}\lim_{m\to\infty}a_{n,m}=\lim_{n\to\infty}\sum_{i=1}^\infty b_{n,i}$ exists. For each fixed $m$ the limit $\lim_{n\to\infty}a_{n,m}=\lim_{n\to\infty}\sum_{i=1}^m b_{n,i}=\sum_{i=0}^m\lim_{n\to\infty}b_{n,i}=p_m$ exists. I know that if it were the case that $a_{n,m}\to l_n$ uniformly in $n$ then the double limit $\lim_{n,m\to\infty}a_{n,m}=a$ exists, and then since $\lim_{n\to\infty}a_{n,m}=p_m$ exists, the iterated limits would commute and equal $a$ (as in When can you switch the order of limits? ), but I don't have uniformity. So are the above properties sufficient for the the iterated limits to commute, or do I need something more?","I would like to interchange the iterated limits of a particular double sequence $a_{n,m}$, that is I would like that $\lim_{m\to\infty}\lim_{n\to\infty}a_{n,m}=\lim_{n\to\infty}\lim_{m\to\infty}a_{n,m}$. The sequence has the following properties. Firstly, it is actually formed by a sum over one of the variables of a double sequence $b_{n,i}$, so that $a_{n,m}=\sum_{i=1}^mb_{n,i}$, where $0\leq b_{n,i}\leq 1$. Hence for each fixed $n$, $a_{n,m}$ is an increasing sequence in $m$. For each fixed $n$ the limit $\lim_{m\to\infty}a_{n,m}=l_n$ exists, i.e. the series $\sum_{i=1}^\infty b_{n,i}$ converges. The iterated limit $\lim_{n\to\infty}\lim_{m\to\infty}a_{n,m}=\lim_{n\to\infty}\sum_{i=1}^\infty b_{n,i}$ exists. For each fixed $m$ the limit $\lim_{n\to\infty}a_{n,m}=\lim_{n\to\infty}\sum_{i=1}^m b_{n,i}=\sum_{i=0}^m\lim_{n\to\infty}b_{n,i}=p_m$ exists. I know that if it were the case that $a_{n,m}\to l_n$ uniformly in $n$ then the double limit $\lim_{n,m\to\infty}a_{n,m}=a$ exists, and then since $\lim_{n\to\infty}a_{n,m}=p_m$ exists, the iterated limits would commute and equal $a$ (as in When can you switch the order of limits? ), but I don't have uniformity. So are the above properties sufficient for the the iterated limits to commute, or do I need something more?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
88,Infinite limits on integration: acceptable notation,Infinite limits on integration: acceptable notation,,"So you have an integral like $$  \int_{-\infty}^\infty{ \frac{dx}{1+4 x^2} }  $$ Schaum's Calculus 5e recommends you write this as $$ \lim_{a \to -\infty} \int_a^b{ \frac{dx}{1+4 x^2} } + \lim_{c \to \infty} \int_b^c{ \frac{dx}{1+4 x^2} } $$ Where b is chosen as a point where f(x) is defined. Choosing b=0, you then get $$ \lim_{a \to -\infty} \frac{1}{4} \int_a^0{ \frac{dx}{\frac{1}{4}+x^2} } + \lim_{c \to \infty} \frac{1}{4} \int_0^c{ \frac{dx}{\frac{1}{4}+x^2} } $$ $$ \lim_{a \to -\infty} \frac{1}{2} \tan^{-1}{ 2x } |_a^0 + \lim_{c \to \infty} \frac{1}{2} \tan^{-1}{ 2x } |_0^c $$ $$ 0 - \lim_{a \to -\infty} \frac{1}{2} \tan^{-1}{ 2a } + \lim_{c \to \infty} \frac{1}{2} \tan^{-1}{ 2c } - 0 $$ $$ = \frac{\pi}{4} + \frac{\pi}{4} = \frac{\pi}{2} $$ Would it be correct to shortcut this as Take the indefinite integral with no limits $$ \frac{1}{4} \int{ \frac{dx}{ \frac{1}{4} + x^2} } = \frac{1}{2} \tan^{-1}{ 2x } $$ Evaluate $$ = \frac{1}{2} \lim_{a \to -\infty} \lim_{b \to \infty} \tan^{-1}{ 2x } |_a^b $$ $$ = \frac{1}{2} \lim_{a \to -\infty} \lim_{b \to \infty} \left( \tan^{-1}{ 2b } - \tan^{-1}{ 2a } \right) $$ $$ = \frac{1}{2} \left( \frac{\pi}{2} + \frac{\pi}{2} \right) = \frac{ \pi }{2} $$ So my question is surrounding limit notation and evaluation.  Is the above notation ok, or is it completely necessary to break it up into 2 limits?","So you have an integral like $$  \int_{-\infty}^\infty{ \frac{dx}{1+4 x^2} }  $$ Schaum's Calculus 5e recommends you write this as $$ \lim_{a \to -\infty} \int_a^b{ \frac{dx}{1+4 x^2} } + \lim_{c \to \infty} \int_b^c{ \frac{dx}{1+4 x^2} } $$ Where b is chosen as a point where f(x) is defined. Choosing b=0, you then get $$ \lim_{a \to -\infty} \frac{1}{4} \int_a^0{ \frac{dx}{\frac{1}{4}+x^2} } + \lim_{c \to \infty} \frac{1}{4} \int_0^c{ \frac{dx}{\frac{1}{4}+x^2} } $$ $$ \lim_{a \to -\infty} \frac{1}{2} \tan^{-1}{ 2x } |_a^0 + \lim_{c \to \infty} \frac{1}{2} \tan^{-1}{ 2x } |_0^c $$ $$ 0 - \lim_{a \to -\infty} \frac{1}{2} \tan^{-1}{ 2a } + \lim_{c \to \infty} \frac{1}{2} \tan^{-1}{ 2c } - 0 $$ $$ = \frac{\pi}{4} + \frac{\pi}{4} = \frac{\pi}{2} $$ Would it be correct to shortcut this as Take the indefinite integral with no limits $$ \frac{1}{4} \int{ \frac{dx}{ \frac{1}{4} + x^2} } = \frac{1}{2} \tan^{-1}{ 2x } $$ Evaluate $$ = \frac{1}{2} \lim_{a \to -\infty} \lim_{b \to \infty} \tan^{-1}{ 2x } |_a^b $$ $$ = \frac{1}{2} \lim_{a \to -\infty} \lim_{b \to \infty} \left( \tan^{-1}{ 2b } - \tan^{-1}{ 2a } \right) $$ $$ = \frac{1}{2} \left( \frac{\pi}{2} + \frac{\pi}{2} \right) = \frac{ \pi }{2} $$ So my question is surrounding limit notation and evaluation.  Is the above notation ok, or is it completely necessary to break it up into 2 limits?",,"['notation', 'limits', 'improper-integrals']"
89,Are limits on exponents in moduli possible?,Are limits on exponents in moduli possible?,,"Suppose I show that: $$x^{f(z)/g(z)} = y \pmod{4}$$ is impossible for some given positive integers $x$ and $y$, where, \begin{align*} f(z) &= \phi(4)  k_1(z) + 1 \\ &= 2 k_1(z) + 1\\ g(z) &= \phi(4) k_2(z) + 1 \\ &= 2  k_2(z) + 1 \end{align*} and $k_1(z)$ and $k_2(z)$ are integer functions, that approach infinity, such that $f(z)/g(z)$ approaches some irrational number. Can I then say, the equation: $$x^{f(z)/g(z)} = y$$ has no solutions integer solutions, with the same $x$ and $y$, as $z$ goes to infinity as well? That is, if I let, $$d = \lim_{z->\infty}\frac{f(z)}{g(z)}$$ be the irrational number in the limit, then would it be true that, $$x^d \neq y$$ for the same $x$ and $y$ ?","Suppose I show that: $$x^{f(z)/g(z)} = y \pmod{4}$$ is impossible for some given positive integers $x$ and $y$, where, \begin{align*} f(z) &= \phi(4)  k_1(z) + 1 \\ &= 2 k_1(z) + 1\\ g(z) &= \phi(4) k_2(z) + 1 \\ &= 2  k_2(z) + 1 \end{align*} and $k_1(z)$ and $k_2(z)$ are integer functions, that approach infinity, such that $f(z)/g(z)$ approaches some irrational number. Can I then say, the equation: $$x^{f(z)/g(z)} = y$$ has no solutions integer solutions, with the same $x$ and $y$, as $z$ goes to infinity as well? That is, if I let, $$d = \lim_{z->\infty}\frac{f(z)}{g(z)}$$ be the irrational number in the limit, then would it be true that, $$x^d \neq y$$ for the same $x$ and $y$ ?",,"['number-theory', 'limits']"
90,Accumulation points of the sequence $c_n = \lfloor \cos(\sqrt{n}) \rfloor$,Accumulation points of the sequence,c_n = \lfloor \cos(\sqrt{n}) \rfloor,"I'm trying to find the accumulation points for the sequence $c_n = \lfloor \cos(\sqrt{n}) \rfloor, n \in \mathbb{N}_0$ . I know what the points are, but I'm having trouble coming up with an explicit proof. Here's what I have so far: Since $\cos(x) \in [-1,1] \; \forall x$ there are only three cases for $\lfloor \cos(\sqrt{n}) \rfloor$ and we can rewrite $c_n$ as: $$ c_n = \left \lfloor{\cos(\sqrt{n})}\right \rfloor = \begin{cases}      1 & \cos(\sqrt{n}) = 1 \\     0 & 0 \leq \cos(\sqrt{n}) < 1 \\     -1 & -1 \leq \cos(\sqrt{n}) < 0    \end{cases} $$ Thus, the only candidates for accumulation points are $\{-1,0,1\}$ , since that's the only values $c_n$ can assume. Regarding 1, we have: \begin{align*}     c_n &= 1\\     \Leftrightarrow \cos(\sqrt{n}) &= 1\\     \Leftrightarrow \sqrt{n} &= 2k \pi \text{ for et $k \in \mathbb{N}_0$} \\     \Leftrightarrow n &= 4k^2 \pi^2 \text{ for et $k \in \mathbb{N}_0$} \end{align*} We see that this equality only holds for $n=0$ , since $\pi^2$ is irrational and $n \in \mathbb{N}_0$ . So 1 is not an accumulation point, since the sequence only assumes the value 1 once. I know both 0 and -1 are accumulation points, corresponding to $\sqrt{n} \in (0, \frac{\pi}{2}) \cup (\frac{3\pi}{2}, 2\pi)$ and $\sqrt{n} \in (\frac{\pi}{2}, \frac{3\pi}{2})$ respectively. But I'm not sure how to prove that $\sqrt{n}$ ""visits"" both of these regions infinitely often. This is my first question on here, so I'm sorry if there is any formatting weirdness. Thanks in advance for your help!","I'm trying to find the accumulation points for the sequence . I know what the points are, but I'm having trouble coming up with an explicit proof. Here's what I have so far: Since there are only three cases for and we can rewrite as: Thus, the only candidates for accumulation points are , since that's the only values can assume. Regarding 1, we have: We see that this equality only holds for , since is irrational and . So 1 is not an accumulation point, since the sequence only assumes the value 1 once. I know both 0 and -1 are accumulation points, corresponding to and respectively. But I'm not sure how to prove that ""visits"" both of these regions infinitely often. This is my first question on here, so I'm sorry if there is any formatting weirdness. Thanks in advance for your help!","c_n = \lfloor \cos(\sqrt{n}) \rfloor, n \in \mathbb{N}_0 \cos(x) \in [-1,1] \; \forall x \lfloor \cos(\sqrt{n}) \rfloor c_n  c_n = \left \lfloor{\cos(\sqrt{n})}\right \rfloor = \begin{cases} 
    1 & \cos(\sqrt{n}) = 1 \\
    0 & 0 \leq \cos(\sqrt{n}) < 1 \\
    -1 & -1 \leq \cos(\sqrt{n}) < 0
   \end{cases}  \{-1,0,1\} c_n \begin{align*}
    c_n &= 1\\
    \Leftrightarrow \cos(\sqrt{n}) &= 1\\
    \Leftrightarrow \sqrt{n} &= 2k \pi \text{ for et k \in \mathbb{N}_0} \\
    \Leftrightarrow n &= 4k^2 \pi^2 \text{ for et k \in \mathbb{N}_0}
\end{align*} n=0 \pi^2 n \in \mathbb{N}_0 \sqrt{n} \in (0, \frac{\pi}{2}) \cup (\frac{3\pi}{2}, 2\pi) \sqrt{n} \in (\frac{\pi}{2}, \frac{3\pi}{2}) \sqrt{n}","['real-analysis', 'sequences-and-series', 'limits', 'ceiling-and-floor-functions']"
91,"How do I evaluate $\lim_{(x,y)\to (0,0)}\frac{x^2\sin(xy^2)}{x^4+9y^4}$",How do I evaluate,"\lim_{(x,y)\to (0,0)}\frac{x^2\sin(xy^2)}{x^4+9y^4}","I have tried multiple paths to see if they have differing limits, but they all lead to 0, so I'm assuming that is the limit. Then, I tried using Squeeze Theorem, but it doesn't seem to work. This is my current progress: $$x^4+9y^4\geq x^4$$ $$\frac{x^2}{x^4+9y^4}\leq\frac{1}{x^2}$$ $$0\leq\left|\frac{x^2\sin(xy^2)}{x^4+9y^4}\right|\leq\left|\frac{\sin(x^2y)}{x^2}\right|$$ and I'm stuck in evaluating the limit of the upper bound.","I have tried multiple paths to see if they have differing limits, but they all lead to 0, so I'm assuming that is the limit. Then, I tried using Squeeze Theorem, but it doesn't seem to work. This is my current progress: and I'm stuck in evaluating the limit of the upper bound.",x^4+9y^4\geq x^4 \frac{x^2}{x^4+9y^4}\leq\frac{1}{x^2} 0\leq\left|\frac{x^2\sin(xy^2)}{x^4+9y^4}\right|\leq\left|\frac{\sin(x^2y)}{x^2}\right|,"['limits', 'multivariable-calculus']"
92,Limit of quotient of integrals,Limit of quotient of integrals,,"I was working with the next problem: Let $f:[1,\infty)\to\mathbb{R}$ be an increasing function. Consider $F(x)=\displaystyle\int_{1}^{x}\dfrac{f(t)}{t}\, dt$ . Calculate the limit $$\lim\limits_{x\to 1^{+}} \dfrac{F(x)}{\ln(x)}=\lim\limits_{x\to 1^{+}} \dfrac{\displaystyle\int_{1}^{x}\dfrac{f(t)}{t}\, dt}{\displaystyle\int_{1}^{x}\dfrac{1}{t}\, dt}$$ Clearly, if we evaluate in the limit, we obtain a quotient of the from $\frac{0}{0}$ but L'Hopital's rule can't be used because we don't know if $F(x)$ is derivable. If we take $f(x)=x$ then the limit is equal to $$\lim\limits_{x\to 1^{+}}\dfrac{\displaystyle\int_{1}^{x}\dfrac{1}{t}\, dt}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{\ln(x)}{\ln(x)}=1$$ If $f(x)=nx$ with $n>0$ , then $$\lim\limits_{x\to 1^{+}} \dfrac{\displaystyle\int_{1}^{x}\dfrac{nt}{t}\, dt}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{n(x-1)}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{n}{\frac{1}{x}}=\lim\limits_{x\to 1^{+}}nx=n$$ The last step is by L'Hopital's rule. But $n=f(1)$ . Therefore, we claim that $$\lim\limits_{x\to 1^{+}}\dfrac{F(x)}{\ln(x)}=f(1)$$ How can I prove it? Any hint? Thanks!","I was working with the next problem: Let be an increasing function. Consider . Calculate the limit Clearly, if we evaluate in the limit, we obtain a quotient of the from but L'Hopital's rule can't be used because we don't know if is derivable. If we take then the limit is equal to If with , then The last step is by L'Hopital's rule. But . Therefore, we claim that How can I prove it? Any hint? Thanks!","f:[1,\infty)\to\mathbb{R} F(x)=\displaystyle\int_{1}^{x}\dfrac{f(t)}{t}\, dt \lim\limits_{x\to 1^{+}} \dfrac{F(x)}{\ln(x)}=\lim\limits_{x\to 1^{+}} \dfrac{\displaystyle\int_{1}^{x}\dfrac{f(t)}{t}\, dt}{\displaystyle\int_{1}^{x}\dfrac{1}{t}\, dt} \frac{0}{0} F(x) f(x)=x \lim\limits_{x\to 1^{+}}\dfrac{\displaystyle\int_{1}^{x}\dfrac{1}{t}\, dt}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{\ln(x)}{\ln(x)}=1 f(x)=nx n>0 \lim\limits_{x\to 1^{+}} \dfrac{\displaystyle\int_{1}^{x}\dfrac{nt}{t}\, dt}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{n(x-1)}{\ln(x)}=\lim\limits_{x\to 1^{+}}\dfrac{n}{\frac{1}{x}}=\lim\limits_{x\to 1^{+}}nx=n n=f(1) \lim\limits_{x\to 1^{+}}\dfrac{F(x)}{\ln(x)}=f(1)","['calculus', 'integration', 'limits']"
93,Find the limit of this large expression,Find the limit of this large expression,,Find $$\lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{(n+1)\left(n+\frac{1}{2}\right)\left(n+\frac{1}{2^2}\right)\cdots\left(n+\frac{1}{2^{n-1}}\right)\right\}^n$$ where $()$ and $\{\}$ are nothing except brackets. I wrote the above expression as $$\lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{\prod_{i=0}^{n-1}\left(n+\frac{1}{2^i}\right)\right\}^n$$ I can't apply L $'$ Hopital's rule here. I'm stuck. Any help is greatly appreciated. I honestly don't know what $(a)$ means and imho I guess $[a]$ means greatest integer less than or equal to $a$ .,Find where and are nothing except brackets. I wrote the above expression as I can't apply L Hopital's rule here. I'm stuck. Any help is greatly appreciated. I honestly don't know what means and imho I guess means greatest integer less than or equal to .,\lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{(n+1)\left(n+\frac{1}{2}\right)\left(n+\frac{1}{2^2}\right)\cdots\left(n+\frac{1}{2^{n-1}}\right)\right\}^n () \{\} \lim_{n\rightarrow\infty}\frac{1}{n^2}\left\{\prod_{i=0}^{n-1}\left(n+\frac{1}{2^i}\right)\right\}^n ' (a) [a] a,"['calculus', 'limits']"
94,Troubles understanding when certain quantity is bounded :,Troubles understanding when certain quantity is bounded :,,"I'm trying to study the following function : $$f(x,y) = \begin{cases}\dfrac{x^2y^2}{x^2+y^4} & (x,y)\neq (0,0)\\ 0 & (x,y)=(0,0) \end{cases}$$ I started by showing it's continuous at $(0,0)$ : \begin{align*} \lim_{(x,y)\to (0,0)} f(x,y) &= \lim_{(x,y)\to (0,0)} \dfrac{x^2y^2}{x^2+y^4}\\ &= \lim_{r\to 0 } r^2 \dfrac{\cos^2 \theta \sin^2 \theta}{\cos^2 \theta + r^2 \sin^4\theta}\\ &= 0 \end{align*} It's $0$ because the $r^2 \sin^4\theta\to 0$ so as the whole expression, and I checked my answer using Wolfram alpha and it is true. Well when it comes to check if the function is $\mathcal{C}^1$ , I must prove that the partial derivatives exist and they're continuous : $$\partial_x f(0,0) = \lim_{x\to 0 } \dfrac{f(x,0)-f(0,0)}{x}=0$$ But : $$\lim_{(x,y)\to (0,0)} \partial_x f(x,y) = \lim_{(x,y)\to (0,0)} \dfrac{2xy^6}{(x^2+y^4)^2} $$ Its limit doesn't exist as Wolfram Alpha says, because : $$\dfrac{2r^5\cos\theta \sin^6\theta}{(\cos^2\theta+r^2\sin^4\theta)^2}$$ Isn't bounded, but according to my philosophy that I used to solve the continuity it is since $r^2\sin^4 \theta\to 0$ and the limit as well. Any help to overcome this confusion ?","I'm trying to study the following function : I started by showing it's continuous at : It's because the so as the whole expression, and I checked my answer using Wolfram alpha and it is true. Well when it comes to check if the function is , I must prove that the partial derivatives exist and they're continuous : But : Its limit doesn't exist as Wolfram Alpha says, because : Isn't bounded, but according to my philosophy that I used to solve the continuity it is since and the limit as well. Any help to overcome this confusion ?","f(x,y) = \begin{cases}\dfrac{x^2y^2}{x^2+y^4} & (x,y)\neq (0,0)\\ 0 & (x,y)=(0,0) \end{cases} (0,0) \begin{align*}
\lim_{(x,y)\to (0,0)} f(x,y) &= \lim_{(x,y)\to (0,0)} \dfrac{x^2y^2}{x^2+y^4}\\
&= \lim_{r\to 0 } r^2 \dfrac{\cos^2 \theta \sin^2 \theta}{\cos^2 \theta + r^2 \sin^4\theta}\\
&= 0
\end{align*} 0 r^2 \sin^4\theta\to 0 \mathcal{C}^1 \partial_x f(0,0) = \lim_{x\to 0 } \dfrac{f(x,0)-f(0,0)}{x}=0 \lim_{(x,y)\to (0,0)} \partial_x f(x,y) = \lim_{(x,y)\to (0,0)} \dfrac{2xy^6}{(x^2+y^4)^2}  \dfrac{2r^5\cos\theta \sin^6\theta}{(\cos^2\theta+r^2\sin^4\theta)^2} r^2\sin^4 \theta\to 0","['limits', 'multivariable-calculus']"
95,To Evaluate $\sum_{r=1}^\infty\frac{1}{r^2}$ using limit as a sum method,To Evaluate  using limit as a sum method,\sum_{r=1}^\infty\frac{1}{r^2},"I have recently learnt to find some limits and values of some series using integration. This is called as evaluating series and limits using limit as a sum in my textbook. Now, I tried to derive this series using what I learnt. (It is not part of my textbook. I was just doing this for my own fun): $$\sum_{r=1}^\infty\frac{1}{r^2}= \frac{π^2}{6}$$ Here is what I did : Let $S = \lim_{n\to\infty}\sum_{r=1}^n\frac{1}{r^2}$ Using change of variable of $n\to n^2$ and $r\to r^2$ , call $n^2 = x$ and $r^2 = y$ , we have : $$S=\lim_{x\to\infty}\frac{1}{x}\sum_{y=1}^x\frac{x}{y}$$ Now using limit as a sum formula, we have : $$S = \int_0^1\frac{1}{x}dx=\infty$$ Of course this is wrong. Where did I went wrong? Can you provide a correct proof of this series using this limit as a sum integration method ? (There is a famous question of this series on this site but on the answers to that, I didn't find a solution involving limit as a sum method. Hence my question is different.)","I have recently learnt to find some limits and values of some series using integration. This is called as evaluating series and limits using limit as a sum in my textbook. Now, I tried to derive this series using what I learnt. (It is not part of my textbook. I was just doing this for my own fun): Here is what I did : Let Using change of variable of and , call and , we have : Now using limit as a sum formula, we have : Of course this is wrong. Where did I went wrong? Can you provide a correct proof of this series using this limit as a sum integration method ? (There is a famous question of this series on this site but on the answers to that, I didn't find a solution involving limit as a sum method. Hence my question is different.)",\sum_{r=1}^\infty\frac{1}{r^2}= \frac{π^2}{6} S = \lim_{n\to\infty}\sum_{r=1}^n\frac{1}{r^2} n\to n^2 r\to r^2 n^2 = x r^2 = y S=\lim_{x\to\infty}\frac{1}{x}\sum_{y=1}^x\frac{x}{y} S = \int_0^1\frac{1}{x}dx=\infty,"['calculus', 'integration', 'sequences-and-series', 'limits']"
96,"Spivak, Calculus, Ch. 22: How do we compute the limit $\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$?","Spivak, Calculus, Ch. 22: How do we compute the limit ?",\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n},"My question is simply how do we compute the limit $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$$ I know the limit is $1$ , both from the context below and because I checked in Maple. Here is the context in which this limit arose. In Chapter 22 of Spivak's Calculus , Problem 13 asks us to show first that if $f$ is increasing on $[1,\infty)$ then $$f(1)+...+f(n-1)<\int_1^n f(x)dx<f(2)+...+f(n)$$ When we apply this result to the function $f(x)=\log{(x)}$ we easily obtain the relationship $$\frac{n^n}{e^{n-1}}<n!<\frac{(n+1)^{n+1}}{e^n}\tag{1}$$ Spivak concludes for us that given this result we can say that $$\lim\limits_{n\to\infty} \frac{\sqrt[n]{n!}}{n}=\frac{1}{e}\tag{2}$$ I am interested in going from $(1)$ to $(2)$ . Starting at $(1)$ , if we take the n-th root and divide by $n$ we have $$0<\frac{1}{e^{\frac{n-1}{n}}}<\frac{\sqrt[n]{n!}}{n}<\frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e}$$ Now, $$\lim\limits_{n\to\infty} \frac{1}{e^{\frac{n-1}{n}}}=\frac{1}{e}$$ I would like to compute the limit $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e}$$ Which necessitates computing the limit that gave rise to the current question $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$$","My question is simply how do we compute the limit I know the limit is , both from the context below and because I checked in Maple. Here is the context in which this limit arose. In Chapter 22 of Spivak's Calculus , Problem 13 asks us to show first that if is increasing on then When we apply this result to the function we easily obtain the relationship Spivak concludes for us that given this result we can say that I am interested in going from to . Starting at , if we take the n-th root and divide by we have Now, I would like to compute the limit Which necessitates computing the limit that gave rise to the current question","\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n} 1 f [1,\infty) f(1)+...+f(n-1)<\int_1^n f(x)dx<f(2)+...+f(n) f(x)=\log{(x)} \frac{n^n}{e^{n-1}}<n!<\frac{(n+1)^{n+1}}{e^n}\tag{1} \lim\limits_{n\to\infty} \frac{\sqrt[n]{n!}}{n}=\frac{1}{e}\tag{2} (1) (2) (1) n 0<\frac{1}{e^{\frac{n-1}{n}}}<\frac{\sqrt[n]{n!}}{n}<\frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e} \lim\limits_{n\to\infty} \frac{1}{e^{\frac{n-1}{n}}}=\frac{1}{e} \lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e} \lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}","['calculus', 'limits']"
97,evaluate $\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}$,evaluate,\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)},"Evaluate $\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}$ where $\ln$ denotes the natural logarithm. Let $A_N = \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}.$ Clearly $A_N \ge \dfrac{\ln^2 N}N \cdot \dfrac{N-3}{\ln^2 N} = 1-3/N$ for all $N$ so the main question is whether $A_N$ converges to 1 as $N\to\infty.$ Fix $2\leq M < N/2.$ We want to find an upper bound for $A_N$ in terms of $N$ and $M$ that converges to 1 as $N\to\infty$ when $M$ is chosen carefully enough. Note that by differentiating $f(x)=\dfrac{1}{\ln x\cdot \ln (N-x)},$ one can conclude that it is decreasing on $(1,N/2]$ and increasing on $[N/2,N-2]$ (one can equivalently analyze the behaviour of $\dfrac{1}{f(x)}$ by differentiating to simplify the calculations).  Using this observation, we have $\sum_{k=2}^{N-2} \dfrac{1}{\ln k\cdot \ln(N-k)} = (\sum_{k=2}^{M} + \sum_{k=M+1}^{N-M-1} + \sum_{k=N-M}^{N-2} )\dfrac{1}{\ln k\cdot \ln(N-k)}\leq \dfrac{2(M-1)}{\ln 2 \ln (N-2)} +\sum_{k=M+1}^{N-M-1} \dfrac{1}{\ln k\cdot \ln(N-k)}.$ But I'm not sure how to simplify the above sum.","Evaluate where denotes the natural logarithm. Let Clearly for all so the main question is whether converges to 1 as Fix We want to find an upper bound for in terms of and that converges to 1 as when is chosen carefully enough. Note that by differentiating one can conclude that it is decreasing on and increasing on (one can equivalently analyze the behaviour of by differentiating to simplify the calculations).  Using this observation, we have But I'm not sure how to simplify the above sum.","\lim\limits_{N\to\infty} \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)} \ln A_N = \dfrac{\ln^2 N}{N}\sum_{k=2}^{N-2} \dfrac{1}{\ln k \cdot \ln (N-k)}. A_N \ge \dfrac{\ln^2 N}N \cdot \dfrac{N-3}{\ln^2 N} = 1-3/N N A_N N\to\infty. 2\leq M < N/2. A_N N M N\to\infty M f(x)=\dfrac{1}{\ln x\cdot \ln (N-x)}, (1,N/2] [N/2,N-2] \dfrac{1}{f(x)} \sum_{k=2}^{N-2} \dfrac{1}{\ln k\cdot \ln(N-k)} = (\sum_{k=2}^{M} + \sum_{k=M+1}^{N-M-1} + \sum_{k=N-M}^{N-2} )\dfrac{1}{\ln k\cdot \ln(N-k)}\leq \dfrac{2(M-1)}{\ln 2 \ln (N-2)} +\sum_{k=M+1}^{N-M-1} \dfrac{1}{\ln k\cdot \ln(N-k)}.","['calculus', 'sequences-and-series', 'limits', 'derivatives', 'inequality']"
98,"How to ""explain in words"" these $3$ conditions at infinity and zero?","How to ""explain in words"" these  conditions at infinity and zero?",3,"Let $f:\mathbb{R}^N\setminus \{0\}\to\mathbb R$ and $g:\mathbb{R}^N\to\mathbb R$ be functions satisfying $$a)\qquad f(x)\le \frac{1}{|x|} \text{ when } |x|\sim 0,$$ $$b)\qquad \nabla f(x)\to 0\quad\text{ as } |x|\to +\infty,$$ $$c)\quad|g(x)|+\sum_{i=1}^N \left\vert\frac{\partial}{\partial x_i} g(x)\right\vert\to 1\quad\text{ as } |x|\to +\infty. $$ I am trying to ""traduce in words"" these three conditions. As on the one hand $a)$ can be said as $f$ grows less that $1/|x|$ for $x$ small and $b)$ as the gradient of $f$ is infinitesimal at infinity respectively, I have no idea how to explain condition $c)$ . Could someone please help me with that? Also, there is a better way to explain conditions $a)$ and $b)$ , too? Thank you in advance.","Let and be functions satisfying I am trying to ""traduce in words"" these three conditions. As on the one hand can be said as grows less that for small and as the gradient of is infinitesimal at infinity respectively, I have no idea how to explain condition . Could someone please help me with that? Also, there is a better way to explain conditions and , too? Thank you in advance.","f:\mathbb{R}^N\setminus \{0\}\to\mathbb R g:\mathbb{R}^N\to\mathbb R a)\qquad f(x)\le \frac{1}{|x|} \text{ when } |x|\sim 0, b)\qquad \nabla f(x)\to 0\quad\text{ as } |x|\to +\infty, c)\quad|g(x)|+\sum_{i=1}^N \left\vert\frac{\partial}{\partial x_i} g(x)\right\vert\to 1\quad\text{ as } |x|\to +\infty.  a) f 1/|x| x b) f c) a) b)","['real-analysis', 'calculus', 'limits', 'functions', 'notation']"
99,Is my understanding of limits correct?,Is my understanding of limits correct?,,"I want to explain the basic concept of limits to see if my understanding is correct or not. If we have a function in the form of a fraction and for a value of $x$ the numerator and denominator $=0$ ,  the graph of the function will have a perforation. In order to find the coördinates of the perforation we can use limits to rewrite the function in a form so that we can put in the value of $x$ that would have resulted in $0$ . Because of the limits we can rewrite, for example, $f(x)= \frac {x^2 - 5x + 6}{x-2}$ = $\frac {(x-3)(x-2)}{x-2}$ to $\displaystyle \lim_{x \to 2} x-3 = -1$ We normally are not allowed to divide by $x-2$ because we do not know if $x-2$ could equal $0$ . In this case we know $x=2$ will result in $0$ , so the limit basically says that we take a $x$ that is very close to $2$ , like $1.9999999999999$ but does not equal $2$ and therefore we can divide by $x-2$ and simplify the function to a form where we can input $x=2$ Please correct me if I am wrong and apologies for some of the formatting, I do not know how to format the limit correctly.","I want to explain the basic concept of limits to see if my understanding is correct or not. If we have a function in the form of a fraction and for a value of the numerator and denominator ,  the graph of the function will have a perforation. In order to find the coördinates of the perforation we can use limits to rewrite the function in a form so that we can put in the value of that would have resulted in . Because of the limits we can rewrite, for example, = to We normally are not allowed to divide by because we do not know if could equal . In this case we know will result in , so the limit basically says that we take a that is very close to , like but does not equal and therefore we can divide by and simplify the function to a form where we can input Please correct me if I am wrong and apologies for some of the formatting, I do not know how to format the limit correctly.",x =0 x 0 f(x)= \frac {x^2 - 5x + 6}{x-2} \frac {(x-3)(x-2)}{x-2} \displaystyle \lim_{x \to 2} x-3 = -1 x-2 x-2 0 x=2 0 x 2 1.9999999999999 2 x-2 x=2,['limits']
