,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Table of pdfs, cdfs, moments","Table of pdfs, cdfs, moments",,"Is there a table online (or perhaps in a book) that you would recommend for looking up probability density functions, cumulative distribution functions, moments, etc. for known distributions?","Is there a table online (or perhaps in a book) that you would recommend for looking up probability density functions, cumulative distribution functions, moments, etc. for known distributions?",,"['reference-request', 'statistics']"
1,sub martingales and more,sub martingales and more,,"This is a problem on sub-martingales. Given : $X_n = X_0 \mathrm{e}^{\mu  S_n}$, $n= 1,2,3,\ldots$, where $X_0 > 0$ and where $S_n$ is a symmetric random walk and $\mu$ is greater than zero. We have to prove that $\{X_n\}$ is a sub martingale. How do we go about this? I understand that, if $Z_n$ is a stochastic process, with $\mathbb{E}(Z_n) < \infty$ , then it is a sub martingale if $\mathbb{E}(Z_{n+1} | Z_1,Z_2,Z_3,\ldots,Z_n) \geqslant Z_n$. So how do I proceed with the problem?","This is a problem on sub-martingales. Given : $X_n = X_0 \mathrm{e}^{\mu  S_n}$, $n= 1,2,3,\ldots$, where $X_0 > 0$ and where $S_n$ is a symmetric random walk and $\mu$ is greater than zero. We have to prove that $\{X_n\}$ is a sub martingale. How do we go about this? I understand that, if $Z_n$ is a stochastic process, with $\mathbb{E}(Z_n) < \infty$ , then it is a sub martingale if $\mathbb{E}(Z_{n+1} | Z_1,Z_2,Z_3,\ldots,Z_n) \geqslant Z_n$. So how do I proceed with the problem?",,"['statistics', 'probability-theory', 'stochastic-processes']"
2,Maximum Likelihood Estimation,Maximum Likelihood Estimation,,"For iid random variables from a distribution with p.d.f. $$f(x;\theta_1,\theta_2)=\frac{1}{\theta_2}\exp\bigg(-\frac{(x-\theta_1)}{\theta_2}\bigg), \quad x>\theta_1, \quad(\theta_1,\theta_2)\in\mathbb{R}\times\mathbb{R}^{+}$$ how can we find maximum likelihood estimators for $\theta_1$ and $\theta_2$? I don't think finding the log-likelihood and performing partial deffierentiation will help for determining the MLE of $\theta_1$ because of the $x>\theta_1$ condition. Any help would be greatly appreciated. Regards, MM.","For iid random variables from a distribution with p.d.f. $$f(x;\theta_1,\theta_2)=\frac{1}{\theta_2}\exp\bigg(-\frac{(x-\theta_1)}{\theta_2}\bigg), \quad x>\theta_1, \quad(\theta_1,\theta_2)\in\mathbb{R}\times\mathbb{R}^{+}$$ how can we find maximum likelihood estimators for $\theta_1$ and $\theta_2$? I don't think finding the log-likelihood and performing partial deffierentiation will help for determining the MLE of $\theta_1$ because of the $x>\theta_1$ condition. Any help would be greatly appreciated. Regards, MM.",,"['statistics', 'parameter-estimation']"
3,"Chi-square approximation to standard normal (0,1)","Chi-square approximation to standard normal (0,1)",,"Supose that $S_n$ has a $\chi^2$ distribution with $n$ degrees of freedom.  Show that  $$ \mathbb{P}(S_n \le x) = f\left(\sqrt{2x}-\sqrt{2n}\right) $$ where $f(u)$ is the normal distribution. I tried this: $\mathbb{P}(S_n \le x)= \mathbb{P}(\sqrt{2 S_n}-\sqrt{2 n} <= \sqrt{2x}-\sqrt{2n})$ now I am trying to show that  $\sqrt{2 S_n} - \sqrt{2n}$ converges in law to $\mathcal{N}(0,1)$.","Supose that $S_n$ has a $\chi^2$ distribution with $n$ degrees of freedom.  Show that  $$ \mathbb{P}(S_n \le x) = f\left(\sqrt{2x}-\sqrt{2n}\right) $$ where $f(u)$ is the normal distribution. I tried this: $\mathbb{P}(S_n \le x)= \mathbb{P}(\sqrt{2 S_n}-\sqrt{2 n} <= \sqrt{2x}-\sqrt{2n})$ now I am trying to show that  $\sqrt{2 S_n} - \sqrt{2n}$ converges in law to $\mathcal{N}(0,1)$.",,"['statistics', 'probability-distributions', 'convergence-divergence', 'normal-distribution']"
4,"Given $X$ is distributed normally, is $Y=aX$ normal? (for some constant a)","Given  is distributed normally, is  normal? (for some constant a)",X Y=aX,"I assumed it was true, and then found $f_Y(y) = f_x(\frac{y}{a}).a^{-1} = a^{-1}\frac{1}{\sqrt{2\pi\sigma^2}}.\exp\{-\frac{(\frac{y}{a}-\mu)}{2\sigma^2}\} = \frac{1}{\sqrt{a^2.2\pi\sigma^2}}.\exp\{\frac{(y-\frac{\mu}{a})}{2a\sigma^2}\} $ which is almost of the form of a normal pdf, but there isn't a consistent value for $\sigma^2_Y$, unless $a=\pm1$","I assumed it was true, and then found $f_Y(y) = f_x(\frac{y}{a}).a^{-1} = a^{-1}\frac{1}{\sqrt{2\pi\sigma^2}}.\exp\{-\frac{(\frac{y}{a}-\mu)}{2\sigma^2}\} = \frac{1}{\sqrt{a^2.2\pi\sigma^2}}.\exp\{\frac{(y-\frac{\mu}{a})}{2a\sigma^2}\} $ which is almost of the form of a normal pdf, but there isn't a consistent value for $\sigma^2_Y$, unless $a=\pm1$",,"['statistics', 'normal-distribution']"
5,Probability of G Good elements and B Bad elements.,Probability of G Good elements and B Bad elements.,,"A box contains 3 red balls, 4 blue balls, and 6 green balls. Balls are drawn one-by-one without replacement until all the red balls are drawn. Let $D$ be the number of draws made. Calculate: a)$P(D\le 9)$ b)$P(D=9)$ c)$E(D)$ For part a) I know the answer should be $\binom{10}{6}/\binom{13}{9}$ but not sure how to get this. And I am wondering the relationship between (a) and (b). And how to calculate hypergeometric distribution in general. For part c) I could use the basic definition to get the answer but I am sure there is an easier way to do this.","A box contains 3 red balls, 4 blue balls, and 6 green balls. Balls are drawn one-by-one without replacement until all the red balls are drawn. Let $D$ be the number of draws made. Calculate: a)$P(D\le 9)$ b)$P(D=9)$ c)$E(D)$ For part a) I know the answer should be $\binom{10}{6}/\binom{13}{9}$ but not sure how to get this. And I am wondering the relationship between (a) and (b). And how to calculate hypergeometric distribution in general. For part c) I could use the basic definition to get the answer but I am sure there is an easier way to do this.",,"['probability', 'statistics']"
6,Probability distribution question: Please verify answer,Probability distribution question: Please verify answer,,"Question A k-out-of-n system is one that will function if and only if at least $k$ out of the $n$ individual components in the system function. Assume that individual components function independently of each other. Assume also each individual component functions with probability $0.9$ Determine the long-run proportion of 3-out-of-5 systems that will function. My Working The binomial distribution looks to be a good fit here. Summarising the data: $p$ (the probability of success) $= 0.9$ $n$ (the total number of trials) $= 5$ $x$ (the number of successes needed) $= 3$ 3-out-of-5 means the system needs at least $3$ components to function which means the system will function at $3$ components, $4$ components $5$ components. Which in turn means the answer will be $1 - prob$($0$ components + $1$ components + $2$ components) Therefore $1 - 0.0815$ = $0.9185$ Is this correct?","Question A k-out-of-n system is one that will function if and only if at least $k$ out of the $n$ individual components in the system function. Assume that individual components function independently of each other. Assume also each individual component functions with probability $0.9$ Determine the long-run proportion of 3-out-of-5 systems that will function. My Working The binomial distribution looks to be a good fit here. Summarising the data: $p$ (the probability of success) $= 0.9$ $n$ (the total number of trials) $= 5$ $x$ (the number of successes needed) $= 3$ 3-out-of-5 means the system needs at least $3$ components to function which means the system will function at $3$ components, $4$ components $5$ components. Which in turn means the answer will be $1 - prob$($0$ components + $1$ components + $2$ components) Therefore $1 - 0.0815$ = $0.9185$ Is this correct?",,"['statistics', 'probability-distributions']"
7,Unbiased estimator and Variance,Unbiased estimator and Variance,,"I am having a hard time trying to solve this problem. I don't know how to start it. Any help would be greatly appreciated. Let T be any unbiased estimator of $\tau(\theta),$ and let $W$ be a sufficient statistic for theta. Define $\phi(W)=E[T|W]$. Show that $\phi(W)$ is an unbiased estimator of $\tau(\theta),$ and $\mathrm{Var}(T)=\mathrm{Var}\left(\phi(W)\right) + E\left[\mathrm{Var}(T|W)\right]$. The only thing I know is that this may be a part of the Rao-Blackwell Thm.","I am having a hard time trying to solve this problem. I don't know how to start it. Any help would be greatly appreciated. Let T be any unbiased estimator of $\tau(\theta),$ and let $W$ be a sufficient statistic for theta. Define $\phi(W)=E[T|W]$. Show that $\phi(W)$ is an unbiased estimator of $\tau(\theta),$ and $\mathrm{Var}(T)=\mathrm{Var}\left(\phi(W)\right) + E\left[\mathrm{Var}(T|W)\right]$. The only thing I know is that this may be a part of the Rao-Blackwell Thm.",,['statistics']
8,"Poisson random variable with parameter lambda~uniform(0, 10)","Poisson random variable with parameter lambda~uniform(0, 10)",,"Suppose $N\sim Poisson(\lambda)$ and $\lambda\sim Unif(0, 10)$ what is the expected value $E(N)$? I feel like I should just plug in the expected value of Unif(0, 10) but that seems too easy.  Anyone have any thoughts?","Suppose $N\sim Poisson(\lambda)$ and $\lambda\sim Unif(0, 10)$ what is the expected value $E(N)$? I feel like I should just plug in the expected value of Unif(0, 10) but that seems too easy.  Anyone have any thoughts?",,['statistics']
9,Calculate Line Of Best Fit Using Exponential Weighting?,Calculate Line Of Best Fit Using Exponential Weighting?,,I know how to calculate a line of best fit with a set of data. I want to be able to exponentially weight the data that is more recent so that the more recent data has a greater effect on the line. How can I do this?,I know how to calculate a line of best fit with a set of data. I want to be able to exponentially weight the data that is more recent so that the more recent data has a greater effect on the line. How can I do this?,,"['statistics', 'regression']"
10,Likelihood Function,Likelihood Function,,"How do we write the likelihood function of a problem that consists of more than one x variable? (In my case it is x1, x2 and x3)","How do we write the likelihood function of a problem that consists of more than one x variable? (In my case it is x1, x2 and x3)",,"['statistics', 'probability-theory']"
11,"For a covariance matrix, what would be the properties associated with the eigenvectors space of this matrix?","For a covariance matrix, what would be the properties associated with the eigenvectors space of this matrix?",,"I want to know, since the covariance matrix is symmetric, positive, and semi-definite, then if I calculate its eigenvectors what would be the properties of the space constructed by those eigenvectors (corresponds to non-close-zero eigenvalues), is it orthogonal or anything else special? Suppose this eigenvector matrix is called U, then what would be the properties with U*transpose(U)？","I want to know, since the covariance matrix is symmetric, positive, and semi-definite, then if I calculate its eigenvectors what would be the properties of the space constructed by those eigenvectors (corresponds to non-close-zero eigenvalues), is it orthogonal or anything else special? Suppose this eigenvector matrix is called U, then what would be the properties with U*transpose(U)？",,"['linear-algebra', 'statistics']"
12,Sufficient statistics,Sufficient statistics,,Suppose $T_1(X)$ is a sufficient statistic for $\theta$ for a family of distributions indexed by $\theta$ and $T_2(X)$ is another statistic that induces a “finer”(what is meant by a finer ? ) partition than $T_1$. How do I know that $T_2$ is also sufficient  ?,Suppose $T_1(X)$ is a sufficient statistic for $\theta$ for a family of distributions indexed by $\theta$ and $T_2(X)$ is another statistic that induces a “finer”(what is meant by a finer ? ) partition than $T_1$. How do I know that $T_2$ is also sufficient  ?,,['statistics']
13,Sufficient statistics,Sufficient statistics,,"I'm learning about sufficient statistics and I understand basic stuff that it is the minimum information we need to represent a statistic (very vague definition, I know. I'm learning). I'm trying to solve problems in that. The problem is suppose a sample of size $n$ is taken from the geometric distribution with parameter $\pi$. How can I find a real valued sufficient statistic for this ?","I'm learning about sufficient statistics and I understand basic stuff that it is the minimum information we need to represent a statistic (very vague definition, I know. I'm learning). I'm trying to solve problems in that. The problem is suppose a sample of size $n$ is taken from the geometric distribution with parameter $\pi$. How can I find a real valued sufficient statistic for this ?",,['statistics']
14,Expansion of a function with a negative fractional power,Expansion of a function with a negative fractional power,,How would you expand a moment generating function with a negative fractional power as a power series?? What method would you be able to use?,How would you expand a moment generating function with a negative fractional power as a power series?? What method would you be able to use?,,['statistics']
15,a simple argument?,a simple argument?,,"This argument appeared in a proof in a paper (I am paraphrasing), and I am not sure why it is true. It should be rather simple. Let $(\Omega,P)$ be some sample space. Let $X$ be a random variable between $0$ and $1$. Let $U \subseteq \Omega$ and let $U^\prime = ${$ \omega \colon X(\omega) \ge E[X \mid U] - 1/m$}. Then $P(U^\prime) \ge P(U)/m$. Can't see exactly why this is true.","This argument appeared in a proof in a paper (I am paraphrasing), and I am not sure why it is true. It should be rather simple. Let $(\Omega,P)$ be some sample space. Let $X$ be a random variable between $0$ and $1$. Let $U \subseteq \Omega$ and let $U^\prime = ${$ \omega \colon X(\omega) \ge E[X \mid U] - 1/m$}. Then $P(U^\prime) \ge P(U)/m$. Can't see exactly why this is true.",,"['probability', 'statistics']"
16,"Given a function $f(x)$ where $x$ is uniformly distributed between $a$ and $b$, how do I find the probability density function of $f$?","Given a function  where  is uniformly distributed between  and , how do I find the probability density function of ?",f(x) x a b f,"For example, if $f(x) = \sin x$ and $x$ is uniformly distributed on $[0, \pi]$, how is the equation found that satisfies the probability distribution function of $f(x)$? I imagine the distribution function will be greater when the derivative of $f(x)$ is closer to zero, but this is just a guess. I apologize if this question is vague or not advanced enough, but I can't find the answer anywhere.","For example, if $f(x) = \sin x$ and $x$ is uniformly distributed on $[0, \pi]$, how is the equation found that satisfies the probability distribution function of $f(x)$? I imagine the distribution function will be greater when the derivative of $f(x)$ is closer to zero, but this is just a guess. I apologize if this question is vague or not advanced enough, but I can't find the answer anywhere.",,"['probability', 'statistics']"
17,Is it possible to calculate the standard deviation without the samples?,Is it possible to calculate the standard deviation without the samples?,,Imagine I had two sets of samples. The only information I have about them is their Size Average Standard deviation Is it possible to calculate the standard deviation of a third set that is composed by the union of all samples in both sets? And what if the original sets had the same size?,Imagine I had two sets of samples. The only information I have about them is their Size Average Standard deviation Is it possible to calculate the standard deviation of a third set that is composed by the union of all samples in both sets? And what if the original sets had the same size?,,"['statistics', 'standard-deviation']"
18,"can KL-divergence converge to $0,$ but variance of log odds not converge to $0?$",can KL-divergence converge to  but variance of log odds not converge to,"0, 0?","Let $p(x)$ be a fixed distribution. Let $A, C > 0$ be constants. Let $\epsilon > 0$. Can we find an example of a distribution $q_{\epsilon}$ such that $\mathrm{KL}(p||q_{\epsilon}) < \epsilon$, but $E[(\log (p(x)/q(x)))^2] \ge A * \epsilon^C$? If $p$ and $q_{\epsilon}$ would be a coin toss or a geometric distribution, that would be even better. I hope I am making myself clear. This is a rather hard question. Perhaps I could also get a clue if someone could tell me whether there is a name for the quantity $E[(\log (p(x)/q(x)))^2]$. (KL divergence is defined as $E[\log (p(x)/q(x))]$.) All expectations are taken with respect to $p$. So, for example, $E[(\log (p(x)/q(x)))^2] = \sum_x p(x) \left(\log (p(x)/q(x))\right)^2$","Let $p(x)$ be a fixed distribution. Let $A, C > 0$ be constants. Let $\epsilon > 0$. Can we find an example of a distribution $q_{\epsilon}$ such that $\mathrm{KL}(p||q_{\epsilon}) < \epsilon$, but $E[(\log (p(x)/q(x)))^2] \ge A * \epsilon^C$? If $p$ and $q_{\epsilon}$ would be a coin toss or a geometric distribution, that would be even better. I hope I am making myself clear. This is a rather hard question. Perhaps I could also get a clue if someone could tell me whether there is a name for the quantity $E[(\log (p(x)/q(x)))^2]$. (KL divergence is defined as $E[\log (p(x)/q(x))]$.) All expectations are taken with respect to $p$. So, for example, $E[(\log (p(x)/q(x)))^2] = \sum_x p(x) \left(\log (p(x)/q(x))\right)^2$",,"['probability', 'statistics']"
19,Simple algorithm effectiveness (Least-Squares Regression Line in Terms of Sample Variances),Simple algorithm effectiveness (Least-Squares Regression Line in Terms of Sample Variances),,"I hope I'm at right place, as I asked this question at stackoverflow.com where the question was closed and suggested I go ask here. I'll try to be more comprehensive also, looking at couple of replies I got there. The question is simple for those who know how algorithms work, and as I'm physics student I'm not sure about the answer. I had exam in Statistics where professor suggested that formula I used (red outlined) in determining ""Least-Squares Regression Line"" was computationally inferior to the one derived below (green outline). My simple thinking was that first one uses n squares while derived (n+1) squares, but supposedly that's not enough. Can someone point why could derived formula be computationally more effective? Thanks","I hope I'm at right place, as I asked this question at stackoverflow.com where the question was closed and suggested I go ask here. I'll try to be more comprehensive also, looking at couple of replies I got there. The question is simple for those who know how algorithms work, and as I'm physics student I'm not sure about the answer. I had exam in Statistics where professor suggested that formula I used (red outlined) in determining ""Least-Squares Regression Line"" was computationally inferior to the one derived below (green outline). My simple thinking was that first one uses n squares while derived (n+1) squares, but supposedly that's not enough. Can someone point why could derived formula be computationally more effective? Thanks",,['statistics']
20,Bar chart or histogram,Bar chart or histogram,,What's the difference between a bar chart and a histogram?,What's the difference between a bar chart and a histogram?,,[]
21,Determine speed of the object at the current time by the non-uniform time sample,Determine speed of the object at the current time by the non-uniform time sample,,"Here is a time sample: $Q = \{(t_i, x_i) | 0 \leq x_i \leq x_{i+1}, 1 \leq i \leq n\}$ and rules: (1) $T_1 \leq t_{i+1} - t_i < T_2$ where  $T_1, T_2 > 0$ (2) $x_i$ comes with error: $x_i = \lfloor x(t_i) \rfloor$ where $\lfloor x \rfloor$ - whole part of $x$, $x(t_i)$ - voluntary unknown law of object motion. How we can find (approximately but sufficiently accurate) speed of the object at the given time?","Here is a time sample: $Q = \{(t_i, x_i) | 0 \leq x_i \leq x_{i+1}, 1 \leq i \leq n\}$ and rules: (1) $T_1 \leq t_{i+1} - t_i < T_2$ where  $T_1, T_2 > 0$ (2) $x_i$ comes with error: $x_i = \lfloor x(t_i) \rfloor$ where $\lfloor x \rfloor$ - whole part of $x$, $x(t_i)$ - voluntary unknown law of object motion. How we can find (approximately but sufficiently accurate) speed of the object at the given time?",,"['sequences-and-series', 'statistics', 'numerical-methods', 'approximation']"
22,Probability of the Maximum of $6$ die rolls,Probability of the Maximum of  die rolls,6,"Someone please explain to me why this is wrong. Here is the question: If I roll $6$ different fair $6$ -sided dice, what is the probability that the maximum of the rolls is $5$ ? My approach was to find the the PDF of the Maximum, then evaluate at $x = 5$ : Let $f (\text{Max})$ be the PDF of the maximum, then $f (\text{Max}) = \frac{6!}{5!} \frac{1}{6} F (\text{Max})^5$ , which is $\left( \frac{\text{Max}}{6} \right)^5$ . Evaluating at $\text{Max} = 5$ , gives us $\left( \frac{5}{6} \right)^5$ . I know this is actually the CDF of the maximum here, but where did I go wrong in my derivation of the PDF?","Someone please explain to me why this is wrong. Here is the question: If I roll different fair -sided dice, what is the probability that the maximum of the rolls is ? My approach was to find the the PDF of the Maximum, then evaluate at : Let be the PDF of the maximum, then , which is . Evaluating at , gives us . I know this is actually the CDF of the maximum here, but where did I go wrong in my derivation of the PDF?",6 6 5 x = 5 f (\text{Max}) f (\text{Max}) = \frac{6!}{5!} \frac{1}{6} F (\text{Max})^5 \left( \frac{\text{Max}}{6} \right)^5 \text{Max} = 5 \left( \frac{5}{6} \right)^5,"['probability', 'combinatorics', 'statistics', 'order-statistics', 'actuarial-science']"
23,"Mean squared error of MLE of $\theta$ where $f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x)$",Mean squared error of MLE of  where,"\theta f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x)","Given $f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x)$ I want to find the MSE of the MLE estimator for $\theta$ . I've found that $\hat{\theta} = \frac{n}{\sum_{i=1}^n X_i^3} = \frac{1}{\bar{X^3}}$ . I know that $MSE(\hat{\theta}) = Var(\hat{\theta}) + Bias(\hat{\theta})^2$ but I am having trouble finding $E(\hat{\theta})$ and $E(\hat{\theta}^2)$ here. How should I proceed?",Given I want to find the MSE of the MLE estimator for . I've found that . I know that but I am having trouble finding and here. How should I proceed?,"f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x) \theta \hat{\theta} = \frac{n}{\sum_{i=1}^n X_i^3} = \frac{1}{\bar{X^3}} MSE(\hat{\theta}) = Var(\hat{\theta}) + Bias(\hat{\theta})^2 E(\hat{\theta}) E(\hat{\theta}^2)","['probability', 'statistics', 'expected-value', 'maximum-likelihood', 'mean-square-error']"
24,Does the kurtosis need to be finite for the sample variance to be consistent?,Does the kurtosis need to be finite for the sample variance to be consistent?,,"It is known (see this answer ) that if $\mu_4$ is the fourth central moment of a distribution and $\sigma$ is the standard deviation, then we can write $$\operatorname{Var}(S^2_n)=\frac{1}{n}\left[\mu_4-\frac{n-3}{n-1}\sigma^4\right].$$ This goes to $0$ as $n\to\infty$ , hence if the kurtosis (and therefore the fourth moment) is finite, then the sample variance $S^2$ is consistent for the population variance (since $\mathbb{E}(S^2_n)=\sigma^2)$ . Does the converse hold? Does the kurtosis need to be finite for $S^2_n$ to converge in probability to $\sigma^2$ ? I have tried simulating the sampling distribution of $S^2$ for various distributions; I tried looking at the sampling distribution for a $t_4$ population (which only has up to the third moment) as well as a $t_5$ population (which has finite kurtosis), and this seems to support the idea that the kurtosis does in fact need to exist (but obviously there could be a counterexample with another distribution).","It is known (see this answer ) that if is the fourth central moment of a distribution and is the standard deviation, then we can write This goes to as , hence if the kurtosis (and therefore the fourth moment) is finite, then the sample variance is consistent for the population variance (since . Does the converse hold? Does the kurtosis need to be finite for to converge in probability to ? I have tried simulating the sampling distribution of for various distributions; I tried looking at the sampling distribution for a population (which only has up to the third moment) as well as a population (which has finite kurtosis), and this seems to support the idea that the kurtosis does in fact need to exist (but obviously there could be a counterexample with another distribution).",\mu_4 \sigma \operatorname{Var}(S^2_n)=\frac{1}{n}\left[\mu_4-\frac{n-3}{n-1}\sigma^4\right]. 0 n\to\infty S^2 \mathbb{E}(S^2_n)=\sigma^2) S^2_n \sigma^2 S^2 t_4 t_5,"['probability', 'statistics', 'probability-distributions', 'variance', 'parameter-estimation']"
25,Asymptotic Gambler's Ruin Probability with Unequal Gain/Loss with Zero-Mean Payoff Distribution,Asymptotic Gambler's Ruin Probability with Unequal Gain/Loss with Zero-Mean Payoff Distribution,,"The gambler's ruin problem with unequal gain/loss with a payoff distribution whose support is a finite subset of $\mathbb Z$ is an old problem; for example, see Feller (1968, Vol.1, Section XIV.8) and this old MSE question . The simple case of the problem where the payoff distribution takes $-1$ and $1$ with probabilities $p$ and $1-p$ , has been studied in many introductory text books (it can be easily adjusted for the case the payoff distribution takes $-1$ , $1$ , and $0$ with probabilities $p$ , $q$ , and $1-p-q>0$ ). The ruin probability for the general case is a linear combination of the roots of the following equation: $$P_X(z)=1 \Leftrightarrow M_X(t)=1$$ where $P_X(z)=\mathbb E(z^X)$ and $M_X(t)=\mathbb E(e^{tX})$ are the generating function and moment generating function of $X$ , respectively. Inspired by the heuristic method used in this answer , I guess if $\mathbb E(X)=0$ and the range of $X$ is fixed, we have $$\color{blue}{\lim_{N,M \to \infty} \mathbb P_\text{ruin}(M,N)= \lim_{N,M \to \infty} \frac{N-M}{N}}. $$ Here, $M$ is the gambler's initial wealth, and he is ruined if he loses all of it. If the gambler’s fortune exceeds $N>M$ , the gambler wins and his opponent or the casino is ruined. For any $M$ and $N$ , the relation holds for the simple case of the problem described earlier. This is also supported by the method used to adjust the simple case of the problem for the case where $0$ payoff appears with non-zero probability. The above can be a useful result (if it is proven generally), because in practice the initial moneys of the players ( $M$ and $N-M$ ) can be relatively large compared to the payoff $X$ gained in each round of the game, and the condition $\mathbb E(X)=0$ is somehow shows that the game is balanced and fair. I am seeking a counterexample, or a rigorous proof for the above conjecture. This can be first studied for the simple payoff distribution that takes two integer values $-r$ and $k$ with probabilities $\frac{k}{r+k}$ and $\frac{r}{r+k}$ .","The gambler's ruin problem with unequal gain/loss with a payoff distribution whose support is a finite subset of is an old problem; for example, see Feller (1968, Vol.1, Section XIV.8) and this old MSE question . The simple case of the problem where the payoff distribution takes and with probabilities and , has been studied in many introductory text books (it can be easily adjusted for the case the payoff distribution takes , , and with probabilities , , and ). The ruin probability for the general case is a linear combination of the roots of the following equation: where and are the generating function and moment generating function of , respectively. Inspired by the heuristic method used in this answer , I guess if and the range of is fixed, we have Here, is the gambler's initial wealth, and he is ruined if he loses all of it. If the gambler’s fortune exceeds , the gambler wins and his opponent or the casino is ruined. For any and , the relation holds for the simple case of the problem described earlier. This is also supported by the method used to adjust the simple case of the problem for the case where payoff appears with non-zero probability. The above can be a useful result (if it is proven generally), because in practice the initial moneys of the players ( and ) can be relatively large compared to the payoff gained in each round of the game, and the condition is somehow shows that the game is balanced and fair. I am seeking a counterexample, or a rigorous proof for the above conjecture. This can be first studied for the simple payoff distribution that takes two integer values and with probabilities and .","\mathbb Z -1 1 p 1-p -1 1 0 p q 1-p-q>0 P_X(z)=1 \Leftrightarrow M_X(t)=1 P_X(z)=\mathbb E(z^X) M_X(t)=\mathbb E(e^{tX}) X \mathbb E(X)=0 X \color{blue}{\lim_{N,M \to \infty} \mathbb P_\text{ruin}(M,N)= \lim_{N,M \to \infty} \frac{N-M}{N}}.  M N>M M N 0 M N-M X \mathbb E(X)=0 -r k \frac{k}{r+k} \frac{r}{r+k}","['real-analysis', 'probability', 'statistics', 'stochastic-processes']"
26,Wasserstein-2 distance between $\mu_1$ and $\mu_2$ equals the $L^2$ distance between the Fourier transforms $\hat{\mu}_1$ and $\hat{\mu}_2$,Wasserstein-2 distance between  and  equals the  distance between the Fourier transforms  and,\mu_1 \mu_2 L^2 \hat{\mu}_1 \hat{\mu}_2,"Plancherel's theorem states that the Fourier transform preserves the $L^2$ norm, meaning that if $f$ and $g$ are functions whose Fourier transforms are denoted by $\hat{f}$ and $\hat{g}$ respectively, then: $$ \| f \|_{L^2} = \| \hat{f} \|_{L^2} $$ Similarly, is it true that: if $\mu_1$ and $\mu_2$ are probability measures and $\hat{\mu}_1$ and $\hat{\mu}_2$ are their respective Fourier transforms, then the $L^2$ distance between $\hat{\mu}_1$ and $\hat{\mu}_2$ is equal to the Wasserstein-2 distance between $\mu_1$ and $\mu_2$ ? Mathematically, this can be expressed as: $$ W_2(\mu_1, \mu_2) = \left( \int_{X \times X} |x - y|^2 d\pi(x,y) \right)^{1/2} = \left( \int_{\mathbb{R}} |\hat{\mu}_1(\xi) - \hat{\mu}_2(\xi)|^2 d\xi \right)^{1/2} $$","Plancherel's theorem states that the Fourier transform preserves the norm, meaning that if and are functions whose Fourier transforms are denoted by and respectively, then: Similarly, is it true that: if and are probability measures and and are their respective Fourier transforms, then the distance between and is equal to the Wasserstein-2 distance between and ? Mathematically, this can be expressed as:","L^2 f g \hat{f} \hat{g} 
\| f \|_{L^2} = \| \hat{f} \|_{L^2}
 \mu_1 \mu_2 \hat{\mu}_1 \hat{\mu}_2 L^2 \hat{\mu}_1 \hat{\mu}_2 \mu_1 \mu_2 
W_2(\mu_1, \mu_2) = \left( \int_{X \times X} |x - y|^2 d\pi(x,y) \right)^{1/2} = \left( \int_{\mathbb{R}} |\hat{\mu}_1(\xi) - \hat{\mu}_2(\xi)|^2 d\xi \right)^{1/2}
","['complex-analysis', 'probability-theory', 'measure-theory', 'statistics', 'fourier-analysis']"
27,Probability that the defendant is guilty. [closed],Probability that the defendant is guilty. [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 months ago . Improve this question I've just come across this question on Isaac Physics and I have no idea where to begin. The question is as follows: You are serving on a jury in the Crown Court. The defendant has been accused of a serious crime, however the only evidence is that their DNA is a perfect match to the perpetrator's DNA found at the crime scene. The expert in genetic analysis tells you that the chance of a false positive (i.e. an innocent person matching that DNA) is 1 in 3000000. The prosecution lawyer says in their summing up speech that this means that as the defendant matches the DNA, the chance that they are innocent is less than 0.00004%, This means that there is a 99.99996% chance that the person is guilty, and as this is beyond reasonable doubt you and your jury colleagues should decide that the person is guilty. Back in the jury room, the other jurors know that you have mathematical knowledge and ask you for your view on the matter. If you think it relevant, you may also assume that the crime was definitely committed by a British person, and that the population of Britain is 67000000. Based on the information given, what is your best estimate of the probability that the defendant is guilty? Give your answer to two significant figures. As I said, I'm stuck in the mud with this question. I feel something to do with calculating how many innocent people in Britain would match the perpetrator's DNA would be a logical first step, although I'm not sure how I would go about doing this.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 months ago . Improve this question I've just come across this question on Isaac Physics and I have no idea where to begin. The question is as follows: You are serving on a jury in the Crown Court. The defendant has been accused of a serious crime, however the only evidence is that their DNA is a perfect match to the perpetrator's DNA found at the crime scene. The expert in genetic analysis tells you that the chance of a false positive (i.e. an innocent person matching that DNA) is 1 in 3000000. The prosecution lawyer says in their summing up speech that this means that as the defendant matches the DNA, the chance that they are innocent is less than 0.00004%, This means that there is a 99.99996% chance that the person is guilty, and as this is beyond reasonable doubt you and your jury colleagues should decide that the person is guilty. Back in the jury room, the other jurors know that you have mathematical knowledge and ask you for your view on the matter. If you think it relevant, you may also assume that the crime was definitely committed by a British person, and that the population of Britain is 67000000. Based on the information given, what is your best estimate of the probability that the defendant is guilty? Give your answer to two significant figures. As I said, I'm stuck in the mud with this question. I feel something to do with calculating how many innocent people in Britain would match the perpetrator's DNA would be a logical first step, although I'm not sure how I would go about doing this.",,"['probability', 'statistics', 'conditional-probability']"
28,Covariance Operator corresponding to multivariate covariance function,Covariance Operator corresponding to multivariate covariance function,,"The usual definition of a covariance operator on $L_2(D)$ is: $$ C : L_2(D) \to L_2(D), \qquad (C \psi)(x) = \int_D c(x,y) \psi(y) dy \qquad \forall x\in D, ~~\psi \in L_2(D), $$ where $c(x,y): D \times D \to \mathbb{R}$ is the corresponding covariance function. The operator norm is then $$ \|C\|= \sup_{\|\psi\|_{L_2}=1} \sqrt{\int_D \left(\int_D c(x,y)\psi(y) dy\right)^2dx} $$ I am interested in the covariance operator corresponding to a multivariate covariance function. For example in this paper the author describes the multivariate covariance function $K: D \times D \to \mathbb{R}^p \times \mathbb{R}^p$ , so now we have a matrix valued covariance function $K(x,y) = [c_{ij} (x,y)]$ where $c_{ij}$ is a univariate covariance function defined as above. My question is about the operator corresponding to $K$ , and the definition of the operator norm. Is it correct to write this as: $$ C_K: (L_2(D))^p \to (L_2(D))^p, \qquad  (C_K \psi)(x) =\int_{D} (K(x,y))^T \psi(y) dy, \qquad \forall x \in D, ~~ \psi \in (L_2(D))^p $$ and $$ \|C_K\|= \sup_{\| \psi \|_{(L_2(D))^p}=1} \sqrt{\int_{D} \left ( \int_{D} (K(x,y))^T\psi(y) dy \right )^2dx} $$ I am also wondering if there are any good references that discuss this setting in more detail.","The usual definition of a covariance operator on is: where is the corresponding covariance function. The operator norm is then I am interested in the covariance operator corresponding to a multivariate covariance function. For example in this paper the author describes the multivariate covariance function , so now we have a matrix valued covariance function where is a univariate covariance function defined as above. My question is about the operator corresponding to , and the definition of the operator norm. Is it correct to write this as: and I am also wondering if there are any good references that discuss this setting in more detail.","L_2(D) 
C : L_2(D) \to L_2(D), \qquad (C \psi)(x) = \int_D c(x,y) \psi(y) dy \qquad \forall x\in D, ~~\psi \in L_2(D),
 c(x,y): D \times D \to \mathbb{R} 
\|C\|= \sup_{\|\psi\|_{L_2}=1} \sqrt{\int_D \left(\int_D c(x,y)\psi(y) dy\right)^2dx}
 K: D \times D \to \mathbb{R}^p \times \mathbb{R}^p K(x,y) = [c_{ij} (x,y)] c_{ij} K 
C_K: (L_2(D))^p \to (L_2(D))^p, \qquad 
(C_K \psi)(x) =\int_{D} (K(x,y))^T \psi(y) dy, \qquad \forall x \in D, ~~ \psi \in (L_2(D))^p
 
\|C_K\|= \sup_{\| \psi \|_{(L_2(D))^p}=1}
\sqrt{\int_{D} \left ( \int_{D} (K(x,y))^T\psi(y) dy \right )^2dx}
","['functional-analysis', 'statistics', 'reference-request', 'stochastic-processes', 'covariance']"
29,What is the intuition behind the single-pass algorithm (Welford's method) for the corrected sum of squares?,What is the intuition behind the single-pass algorithm (Welford's method) for the corrected sum of squares?,,"The corrected sum of squares is the sum of squares of the deviations of a set of values about its mean. $$ S = \sum_{i=1}^k\space\space(x_i - \bar x)^2 $$ We can calculate the mean in a streaming fashion, as follows: $$ m_n = \frac{n-1}{n}m_{(n-1)} + \frac{1}{n}x_n $$ I understand the intuition behind this: the sum of the previous $n-1$ values was divided by $n-1$ , so by multiplying those values by $\frac{n-1}{n}$ , we down-weight the sum properly. However, we can also calculate the full corrected sum of squares as follows: $$ S_n = S_{n-1} + \frac{n-1}{n}\left( x_n - m_{n-1}\right)^2 $$ However, I don't have a good intuition for why this works. It looks like we use the previous corrected sum of squares value, and then add the square of the current value's deviation from the mean of all the previous values. But, this algorithm doesn't make sense to me, even if it was derived logically. These formulas are from ""Note on a Method for Calculating Corrected Sums of Squares and Products"" by B.P Welford.","The corrected sum of squares is the sum of squares of the deviations of a set of values about its mean. We can calculate the mean in a streaming fashion, as follows: I understand the intuition behind this: the sum of the previous values was divided by , so by multiplying those values by , we down-weight the sum properly. However, we can also calculate the full corrected sum of squares as follows: However, I don't have a good intuition for why this works. It looks like we use the previous corrected sum of squares value, and then add the square of the current value's deviation from the mean of all the previous values. But, this algorithm doesn't make sense to me, even if it was derived logically. These formulas are from ""Note on a Method for Calculating Corrected Sums of Squares and Products"" by B.P Welford.","
S = \sum_{i=1}^k\space\space(x_i - \bar x)^2
 
m_n = \frac{n-1}{n}m_{(n-1)} + \frac{1}{n}x_n
 n-1 n-1 \frac{n-1}{n} 
S_n = S_{n-1} + \frac{n-1}{n}\left( x_n - m_{n-1}\right)^2
","['statistics', 'variance', 'average', 'means', 'sampling-theory']"
30,Doubt about sampling without replacement,Doubt about sampling without replacement,,"A problem consists of five numbers $2,3,6,8 and 11$ . Consider all possible sample space of size 2 that can be drawn with and without replacement from this population then find (a) mean of the population (b) the std deviation of the population (c) the mean of  the sampling distributon of means I was dealing with this problem.. Here for  possible sample with replacement of size 2, they have considered both {4,3} and {3,4}.. My question is, aren't they both same sample set? If I believe in first set we got '4' first and then '3' nd. And in second set they got '3' first and then '4'... in that sense both sets has to be different so different sample space.But in that case, for  sampling without replacement also they should have considered {4,3} and {3,4} both...like for first sample they got '4' first and then '3' and for the second sample they got '3' first and then '4'. Please clear my this doubt... May be  I am wrong with my definitions of with and without replacement... for me with replacement means after selecting one element for the sample you have put it again in the population set to choose the other, so there is chance of getting the same element again...so repetition is allowed in samples...But this is not the case with without replacement thanks in advance... I dont know why I am  not getting answer, may be I used some pic instead of writing... so I written the question myself and reposting","A problem consists of five numbers . Consider all possible sample space of size 2 that can be drawn with and without replacement from this population then find (a) mean of the population (b) the std deviation of the population (c) the mean of  the sampling distributon of means I was dealing with this problem.. Here for  possible sample with replacement of size 2, they have considered both {4,3} and {3,4}.. My question is, aren't they both same sample set? If I believe in first set we got '4' first and then '3' nd. And in second set they got '3' first and then '4'... in that sense both sets has to be different so different sample space.But in that case, for  sampling without replacement also they should have considered {4,3} and {3,4} both...like for first sample they got '4' first and then '3' and for the second sample they got '3' first and then '4'. Please clear my this doubt... May be  I am wrong with my definitions of with and without replacement... for me with replacement means after selecting one element for the sample you have put it again in the population set to choose the other, so there is chance of getting the same element again...so repetition is allowed in samples...But this is not the case with without replacement thanks in advance... I dont know why I am  not getting answer, may be I used some pic instead of writing... so I written the question myself and reposting","2,3,6,8 and 11","['statistics', 'sampling-theory']"
31,"From the Central-Limit Theorem, what does a sum of IID random variables converge in distribution to?","From the Central-Limit Theorem, what does a sum of IID random variables converge in distribution to?",,"From [1], we have the well known Lindeberg–Levy Central-Limit Theorem: [Lindeberg–Levy Central-Limit Theorem] Suppose $X_1, X_2, \ldots, X_n$ is a sequence of  independent and identically distributed  random variables with $E[X_i] = \mu$ and $\operatorname{Var}[X_i] = \sigma^2 < \infty.$ Then, as $n$ approaches infinity, the random variable $\sqrt{n}(\bar{X}_n - \mu)$ converges in distribution  to a  normal distribution $\mathcal{N}(0, \sigma^2)$ : $$ \sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right) .$$ I am able to go through the proof of this found in [1] without issue. What I am curious about is how to manipulate the equation $$ \sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right)   $$ to determine, for example, what $$  \sum_{i=1}^n  X_i  \ \xrightarrow{d}\ ?  $$ Imitating Alecos Papadopoulos from [2], I write that ""by abusing notation and asymptotics [2]"", I have that $$ \sum_{i=1}^n X_i  \ \approx\  \mathcal{N}\left(n\mu, \left(\sqrt{n} \sigma\right)^2 \right). $$ Ok, I have a solution. Yet, I do not know if its correct; nor do I know how to obtain the result on my own. That's not where I would like to be. Question Can you show---without abusing notation or asymptotics---that  I can infer $$ \sum_{i=1}^n X_i  \ \xrightarrow{d}\  \mathcal{N}\left(n\mu, \left(\sqrt{n} \sigma\right)^2 \right)  $$ from $$ \sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right)  ? $$ Bibliography [1] https://en.wikipedia.org/wiki/Central_limit_theorem# [2] Alecos Papadopoulos ( https://math.stackexchange.com/users/87400/alecos-papadopoulos ), Why does the central limit theorem imply that the standard deviation approaches $\frac{\sigma}{\sqrt{n}}$ ?, URL (version: 2018-07-14): https://math.stackexchange.com/q/515040","From [1], we have the well known Lindeberg–Levy Central-Limit Theorem: [Lindeberg–Levy Central-Limit Theorem] Suppose is a sequence of  independent and identically distributed  random variables with and Then, as approaches infinity, the random variable converges in distribution  to a  normal distribution : I am able to go through the proof of this found in [1] without issue. What I am curious about is how to manipulate the equation to determine, for example, what Imitating Alecos Papadopoulos from [2], I write that ""by abusing notation and asymptotics [2]"", I have that Ok, I have a solution. Yet, I do not know if its correct; nor do I know how to obtain the result on my own. That's not where I would like to be. Question Can you show---without abusing notation or asymptotics---that  I can infer from Bibliography [1] https://en.wikipedia.org/wiki/Central_limit_theorem# [2] Alecos Papadopoulos ( https://math.stackexchange.com/users/87400/alecos-papadopoulos ), Why does the central limit theorem imply that the standard deviation approaches ?, URL (version: 2018-07-14): https://math.stackexchange.com/q/515040","X_1, X_2, \ldots, X_n E[X_i] = \mu \operatorname{Var}[X_i] = \sigma^2 < \infty. n \sqrt{n}(\bar{X}_n - \mu) \mathcal{N}(0, \sigma^2) 
\sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right) . 
\sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right)  
 
 \sum_{i=1}^n  X_i  \ \xrightarrow{d}\ ? 
 
\sum_{i=1}^n X_i  \ \approx\ 
\mathcal{N}\left(n\mu,
\left(\sqrt{n}
\sigma\right)^2
\right).
 
\sum_{i=1}^n X_i  \ \xrightarrow{d}\ 
\mathcal{N}\left(n\mu,
\left(\sqrt{n}
\sigma\right)^2
\right) 
 
\sqrt{n}\left(\bar{X}_n - \mu\right)\ \xrightarrow{d}\ \mathcal{N}\left(0,\sigma^2\right)  ?
 \frac{\sigma}{\sqrt{n}}","['statistics', 'probability-distributions', 'central-limit-theorem']"
32,Expected waiting time for $n$ successive successful tasks,Expected waiting time for  successive successful tasks,n,"Let's assume we have $n$ assignments $a_1$ , $a_2$ , $\cdots$ , $a_n$ . Each assignment $a_i$ can be done with probability $0<p_i<1$ in a time $t_i>0$ . But if you fail in the $i$ -th task ( $i < n$ ), then you have to begin from the start. Thus, for the complete process to be consider a success, the last $n$ final tasks must be successful for the first time. For example, for $n=1$ , succeeding in the complete task in $k$ steps corresponds to failing $k-1$ times and then managing to do the task $t_1$ . Because the expected number of steps is $\frac{1}{p}$ and each of the failed tasks takes $\frac{t_1}{2}$ (using a uniform law), I think the expected time is: $$\left(\frac{1}{p_1}-1 \right) \cdot \frac{t_1}{2} +t_1 = \frac{t_1(1+p_1)}{2p_1}$$ My first attempt was to find the law of the waiting time and then calculate the expected value : $$\sum_{k=1}^{+\infty} \left(\frac{(k-1)t}{2}+t \right)(1-p)^{k-1}p = \frac{t(1+p)}{2p} $$ I wasn't able to find a formula for $n\geq 2$ .","Let's assume we have assignments , , , . Each assignment can be done with probability in a time . But if you fail in the -th task ( ), then you have to begin from the start. Thus, for the complete process to be consider a success, the last final tasks must be successful for the first time. For example, for , succeeding in the complete task in steps corresponds to failing times and then managing to do the task . Because the expected number of steps is and each of the failed tasks takes (using a uniform law), I think the expected time is: My first attempt was to find the law of the waiting time and then calculate the expected value : I wasn't able to find a formula for .",n a_1 a_2 \cdots a_n a_i 0<p_i<1 t_i>0 i i < n n n=1 k k-1 t_1 \frac{1}{p} \frac{t_1}{2} \left(\frac{1}{p_1}-1 \right) \cdot \frac{t_1}{2} +t_1 = \frac{t_1(1+p_1)}{2p_1} \sum_{k=1}^{+\infty} \left(\frac{(k-1)t}{2}+t \right)(1-p)^{k-1}p = \frac{t(1+p)}{2p}  n\geq 2,"['probability', 'statistics', 'expected-value']"
33,Variance recursion formula in Galton-Watson process,Variance recursion formula in Galton-Watson process,,"Consider a Galton-Watson process with expected offspring $\mathbb{E}[\xi]=\mu<\infty$ and variance $\text{Var}(\xi)=\sigma^2<\infty$ where the offspring in generation $t\in\mathbb{N}$ is given by $Z_t$ . Suppose I introduce a new random variable $W_t$ given by $W_t=\mu^{-t}Z_t$ . Noting this setup, in the proof of Proposition $1.4$ on page $7$ of this source they say: ""a straightforward calculation yields $$\text{Var}(W_t)=\frac{\sigma^2}{\mu^{t+1}}+\text{Var}(W_{t-1})""$$ but I am unable to see what straightforward calculation this follows from. Any help would be appreciated.","Consider a Galton-Watson process with expected offspring and variance where the offspring in generation is given by . Suppose I introduce a new random variable given by . Noting this setup, in the proof of Proposition on page of this source they say: ""a straightforward calculation yields but I am unable to see what straightforward calculation this follows from. Any help would be appreciated.","\mathbb{E}[\xi]=\mu<\infty \text{Var}(\xi)=\sigma^2<\infty t\in\mathbb{N} Z_t W_t W_t=\mu^{-t}Z_t 1.4 7 \text{Var}(W_t)=\frac{\sigma^2}{\mu^{t+1}}+\text{Var}(W_{t-1})""","['probability', 'statistics', 'stochastic-processes', 'random-variables', 'variance']"
34,How to do derivative of expectation (characteristic function)?,How to do derivative of expectation (characteristic function)?,,"I type a lot of stuff here, which may looks like daunting. But most of them are just background of my question and are not relevant on my question. Basically, my question is how to get derivative of expectation (here it is characteristic function)? You can skip the background and directly solve my question at last. Suppose $X$ is integrable. Then the following statements are equivalent: for all $u,v \in \mathbb{R}^p, u \perp v$ , we have $E(u^TX|v^T X)=0$ ; $X$ has a spherical distribution; Proof: $1 \Rightarrow 2$ . Step 1. It suffices to show that, for any $t_1, t_2 \in \mathbb{R}^p$ such that $||t_1||=||t_2||$ , we have $\phi_X(t_1)=\phi_X(t_2)$ . (Background: it uses a lemma that is a random vector $X$ has a spherical distribution if and only if its characteristic function is a function of $t^Tt$ ; that is, $   \phi_X(t)=g_0 (t^Tt)$ for some function $g_0$ . Here $\phi_X(t)=E(e^{it^T X})$ , where $i=\sqrt{-1}, t \in \mathbb{R}^p$ ) Step 2. Let $\mathcal{S} = \{t: ||t|| =||t_1|| \}$ . By another lemma (see below), there is a differential function $c: (0, 2\pi) \to \mathcal{S}$ that passes through $t_1$ and $t_2$ . It then suffices to show that $\phi_X$ is constant on $\mathcal{C}$ . Lemma: Let $\mathcal{S}=\{ x:||x||=r\}$ , where $r>0$ , and let $t_1,t_2\in \mathcal{S}$ . Then there is a differentiable mapping $c:(0, 2\pi) \mapsto \mathcal{S}$ that passes through $t_1$ and $t_2$ ; that is, there exist $\alpha_1,\alpha_2 \in (0, 2\pi)$ such that $c(\alpha_1)=t_1$ and $c(\alpha_2)=t_2$ . Proof: Let $\mathcal{H}$ be a hyperplane that passes through $t_1,t_2,0$ in $\mathbb R^p$ . Then $\mathcal{C}=\mathcal{H} \cap \mathcal{S}$ is a circle centered at the origin with radius $r$ . Pick an arbitrary point $t_3$ in $\mathcal{C}$ that is not $t_1$ and $t_2$ , and let $t_4$ be the point on $\mathcal{C}$ obtained by rotating $t_3$ counterclockwise 90 degree along $\mathcal{C}$ . Then any point $t$ in $\mathcal{C}$ that is not $t_3$ can be written as $t=\cos(\alpha)t_3+\sin(\alpha)t_4$ for some $\alpha \in (0, 2\pi)$ . By construction, $\{  c(\alpha): \alpha \in (0, 2\pi) \}$ is a differentiable curve that passes through $t_1$ and $t_2$ . Step 3. A sufficient condition for this is $$\partial \phi_X[c(\alpha)]/\partial \alpha=0$$ for all $\alpha \in (0, 2\pi)$ . The left hand side is $$\partial \phi_X[c(\alpha)]/\partial \alpha=\partial E(e^{ic(\alpha)^T X})/\partial   \alpha\stackrel{?}{=}  E[(e^{ic(\alpha)^T X} iX^T\dot{c}(\alpha))]  \stackrel{?}{=}   E\{ e^{ic(\alpha)^T X} iE[X^T \dot{c}(\alpha)|c(\alpha)^T X]   \}$$ Question: how to get the last two equality signs? I don't know how to get derivative with expectation.","I type a lot of stuff here, which may looks like daunting. But most of them are just background of my question and are not relevant on my question. Basically, my question is how to get derivative of expectation (here it is characteristic function)? You can skip the background and directly solve my question at last. Suppose is integrable. Then the following statements are equivalent: for all , we have ; has a spherical distribution; Proof: . Step 1. It suffices to show that, for any such that , we have . (Background: it uses a lemma that is a random vector has a spherical distribution if and only if its characteristic function is a function of ; that is, for some function . Here , where ) Step 2. Let . By another lemma (see below), there is a differential function that passes through and . It then suffices to show that is constant on . Lemma: Let , where , and let . Then there is a differentiable mapping that passes through and ; that is, there exist such that and . Proof: Let be a hyperplane that passes through in . Then is a circle centered at the origin with radius . Pick an arbitrary point in that is not and , and let be the point on obtained by rotating counterclockwise 90 degree along . Then any point in that is not can be written as for some . By construction, is a differentiable curve that passes through and . Step 3. A sufficient condition for this is for all . The left hand side is Question: how to get the last two equality signs? I don't know how to get derivative with expectation.","X u,v \in \mathbb{R}^p, u \perp v E(u^TX|v^T X)=0 X 1 \Rightarrow 2 t_1, t_2 \in \mathbb{R}^p ||t_1||=||t_2|| \phi_X(t_1)=\phi_X(t_2) X t^Tt    \phi_X(t)=g_0 (t^Tt) g_0 \phi_X(t)=E(e^{it^T X}) i=\sqrt{-1}, t \in \mathbb{R}^p \mathcal{S} = \{t: ||t|| =||t_1|| \} c: (0, 2\pi) \to \mathcal{S} t_1 t_2 \phi_X \mathcal{C} \mathcal{S}=\{ x:||x||=r\} r>0 t_1,t_2\in \mathcal{S} c:(0, 2\pi) \mapsto \mathcal{S} t_1 t_2 \alpha_1,\alpha_2 \in (0, 2\pi) c(\alpha_1)=t_1 c(\alpha_2)=t_2 \mathcal{H} t_1,t_2,0 \mathbb R^p \mathcal{C}=\mathcal{H} \cap \mathcal{S} r t_3 \mathcal{C} t_1 t_2 t_4 \mathcal{C} t_3 \mathcal{C} t \mathcal{C} t_3 t=\cos(\alpha)t_3+\sin(\alpha)t_4 \alpha \in (0, 2\pi) \{  c(\alpha): \alpha \in (0, 2\pi) \} t_1 t_2 \partial \phi_X[c(\alpha)]/\partial \alpha=0 \alpha \in (0, 2\pi) \partial \phi_X[c(\alpha)]/\partial \alpha=\partial E(e^{ic(\alpha)^T X})/\partial 
 \alpha\stackrel{?}{=}  E[(e^{ic(\alpha)^T X} iX^T\dot{c}(\alpha))]  \stackrel{?}{=}   E\{ e^{ic(\alpha)^T X} iE[X^T \dot{c}(\alpha)|c(\alpha)^T X]   \}","['calculus', 'linear-algebra', 'probability', 'statistics']"
35,Triangle Inequality for Le Cam's Metric?,Triangle Inequality for Le Cam's Metric?,,"Let $P, Q$ be probability distributions (say absolutely continuous with respect to the Lebesgue measure for simplicity). The Le Cam metric (also sometimes called the Triangular Discrimination) is defined as $$\Delta(P, Q)^2 = \frac{1}{2}\int \frac{(dP-dQ)^2}{dP+dQ}$$ In Le Cam's book Asymptotic Methods in Statistical Decision Theory , it is stated (page 48) that the non-trivial part of verifying $\Delta(P, Q)$ is a metric reduces to the following inequality Let $c \geq b\geq a\geq 0$ . Then $$ \frac{c-a}{\sqrt{c+a}} \leq \frac{c-b}{\sqrt{c+b}}+\frac{b-a}{\sqrt{b+a}}. $$ This itself follows from convexity of $x\mapsto 1/\sqrt{x}$ . I am trying to verify this myself. I see how an inequality of the above form may be useful, in particular if the condition $c\geq b\geq a\geq 0$ may be ignored, then one can define $f(P,Q) = \frac{dP-dQ}{\sqrt{dP+dQ}}$ , a signed measure with respect to the Lebesgue measure. The above inequality (ignoring the condition on $a,b,c$ ) can then be used to prove that $f(P,Q) \leq f(P,R)+f(R,Q)$ . One can then argue that $$\Delta(P,Q) = \sqrt{1/2}\lVert f(P,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)+f(R,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)\rVert_2+\sqrt{1/2}\lVert f(R,Q)\rVert_2.$$ E.g. the result from sub-additivity of $\lVert \cdot\rVert_2$ . I'm stuck when trying to extend the proof to the case the condition on $(c,b,a)$ does not hold. I've tried extending the proof of the above inequality to (for example) when $b\geq c\geq a\geq 0$ , but this goes nowhere. Can someone help me fill in this proof sketch?","Let be probability distributions (say absolutely continuous with respect to the Lebesgue measure for simplicity). The Le Cam metric (also sometimes called the Triangular Discrimination) is defined as In Le Cam's book Asymptotic Methods in Statistical Decision Theory , it is stated (page 48) that the non-trivial part of verifying is a metric reduces to the following inequality Let . Then This itself follows from convexity of . I am trying to verify this myself. I see how an inequality of the above form may be useful, in particular if the condition may be ignored, then one can define , a signed measure with respect to the Lebesgue measure. The above inequality (ignoring the condition on ) can then be used to prove that . One can then argue that E.g. the result from sub-additivity of . I'm stuck when trying to extend the proof to the case the condition on does not hold. I've tried extending the proof of the above inequality to (for example) when , but this goes nowhere. Can someone help me fill in this proof sketch?","P, Q \Delta(P, Q)^2 = \frac{1}{2}\int \frac{(dP-dQ)^2}{dP+dQ} \Delta(P, Q) c \geq b\geq a\geq 0 
\frac{c-a}{\sqrt{c+a}} \leq \frac{c-b}{\sqrt{c+b}}+\frac{b-a}{\sqrt{b+a}}.
 x\mapsto 1/\sqrt{x} c\geq b\geq a\geq 0 f(P,Q) = \frac{dP-dQ}{\sqrt{dP+dQ}} a,b,c f(P,Q) \leq f(P,R)+f(R,Q) \Delta(P,Q) = \sqrt{1/2}\lVert f(P,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)+f(R,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)\rVert_2+\sqrt{1/2}\lVert f(R,Q)\rVert_2. \lVert \cdot\rVert_2 (c,b,a) b\geq c\geq a\geq 0","['real-analysis', 'probability', 'measure-theory', 'statistics', 'triangle-inequality']"
36,Gamma Distribution parameters as function of n,Gamma Distribution parameters as function of n,,"I have a line with $n$ spaces initially empty, and in each iteration, I randomly choose one of them to insert an element. If it already has 1, the iteration doesn't add anything to the space. Eventually, after enough amount of iterations, all spaces will be full. Then, to visualize the number of iterations needed to finish the process (fill all spaces), I repeat the process with a fixed number of spaces and come up with the following histogram: The above graph is the result of 1 million process repetitions with $n=10$ , showing a distribution of the number of iterations for the specific size. As you can see, I tried to fit the histogram with a Gamma Distribution. My question is, how can I create an expression that outputs the parameters $k$ and $\theta$ of the gamma distribution as a function of $n$ ? I also tried to fit the histogram with a Negative Binomial distribution too, as the probability of success (inserting element) in each iteration changes over time, but it seems a better option to continue with the Gamma distribution since it has a continuous nature.","I have a line with spaces initially empty, and in each iteration, I randomly choose one of them to insert an element. If it already has 1, the iteration doesn't add anything to the space. Eventually, after enough amount of iterations, all spaces will be full. Then, to visualize the number of iterations needed to finish the process (fill all spaces), I repeat the process with a fixed number of spaces and come up with the following histogram: The above graph is the result of 1 million process repetitions with , showing a distribution of the number of iterations for the specific size. As you can see, I tried to fit the histogram with a Gamma Distribution. My question is, how can I create an expression that outputs the parameters and of the gamma distribution as a function of ? I also tried to fit the histogram with a Negative Binomial distribution too, as the probability of success (inserting element) in each iteration changes over time, but it seems a better option to continue with the Gamma distribution since it has a continuous nature.",n n=10 k \theta n,"['probability', 'statistics', 'discrete-mathematics', 'probability-distributions', 'coupon-collector']"
37,Best Strategy for Predicting $p$ of a coin based after $n$ flips,Best Strategy for Predicting  of a coin based after  flips,p n,"Consider a coin with probability of landing on heads, $p$ , where $p$ is sampled from a truncated normal distribution from [0,1]. You are offered to play a game where you can bet on a 12% interval containing $p$ of a coin after observing the outcome of $n$ flips, i.e. if you estimate an interval of [0.6, 0.72] and $p$ is contained in this interval you win the game. What is your optimal strategy for playing this game? What value of $n$ lets you confidently play this game (assuming a 1:1 payout structure, at what point does your strategy net you a win percentage greater than 50%)? The answer will of course depend on the standard deviation of the distribution, which for my simulation purposes I have just used a placeholder of $\sigma=0.2$ . I am wondering if anyone has thoughts on how to obtain a closed form solution or way of estimating what relations between $n$ and $\sigma$ make it worth playing this game. Initial Thoughts A trivial strategy is to have the 12% interval centered about 0.5 as this would be the interval containing the most of the distribution and would have probability of success $P(X\le0.56)-P(X\le0.44)$ . Another strategy I considered is to construct an interval centered about the observed probability of the coin, but with a bias headed toward 0.5. For example, after 4 flips I observe 3 heads, and thus construct an interval of [0.69, 0.81], and then consider adding an offset to my bounds to shift them closer to the center. I have simulated a scenario with $N(0.5, 0.2)$ to varying the number of observed flips, $n$ , and what is the best offset strategy at each $n$ . As predicted, the benefit of offsetting the interval becomes less significant as the number of flips you get to observe increases. From my simulations, it seems that $n=20$ leads is the cutoff for when it becomes worth playing this game. Is there anyway to approximate this result without doing simulation?","Consider a coin with probability of landing on heads, , where is sampled from a truncated normal distribution from [0,1]. You are offered to play a game where you can bet on a 12% interval containing of a coin after observing the outcome of flips, i.e. if you estimate an interval of [0.6, 0.72] and is contained in this interval you win the game. What is your optimal strategy for playing this game? What value of lets you confidently play this game (assuming a 1:1 payout structure, at what point does your strategy net you a win percentage greater than 50%)? The answer will of course depend on the standard deviation of the distribution, which for my simulation purposes I have just used a placeholder of . I am wondering if anyone has thoughts on how to obtain a closed form solution or way of estimating what relations between and make it worth playing this game. Initial Thoughts A trivial strategy is to have the 12% interval centered about 0.5 as this would be the interval containing the most of the distribution and would have probability of success . Another strategy I considered is to construct an interval centered about the observed probability of the coin, but with a bias headed toward 0.5. For example, after 4 flips I observe 3 heads, and thus construct an interval of [0.69, 0.81], and then consider adding an offset to my bounds to shift them closer to the center. I have simulated a scenario with to varying the number of observed flips, , and what is the best offset strategy at each . As predicted, the benefit of offsetting the interval becomes less significant as the number of flips you get to observe increases. From my simulations, it seems that leads is the cutoff for when it becomes worth playing this game. Is there anyway to approximate this result without doing simulation?","p p p n p n \sigma=0.2 n \sigma P(X\le0.56)-P(X\le0.44) N(0.5, 0.2) n n n=20","['probability', 'statistics', 'probability-distributions', 'game-theory']"
38,"If n independent trials are made from a discrete uniform distr. with parameter n, what is the probability that $(n+1)^\text{th}$ is the least of all?","If n independent trials are made from a discrete uniform distr. with parameter n, what is the probability that  is the least of all?",(n+1)^\text{th},"My solution is that since the expected value of the discrete uniform distribution is $\frac{n+1}{2}$ , the probability that the outcome of the second trial is the least of any two trials is $\frac{1}{2}$ . Then the probability that the outcome of the $(n+1)^\text{th}$ trial being less than each of the outcomes of all the previous $n$ trials is $(\frac{1}{2})^n$ . But simulations of the experiment (in python) suggest that the correct answer is different. I have simulated the experiment for $n = 2 \text{ to } 100$ , $10000$ times each, and summarized the final probabilities for various $n's$ in this graph (with $n$ on the x-axis and $p$ on the y-axis). The closest formula I could find for these results is $\frac{1}{2n}$ , whose graph looks like this . This is the python code that I ran the simulations with: import numpy as np  def experiment(trials, n):     count_min = 0          for _ in range(trials):         # Generating n random observations from a discrete uniform distribution         observations = np.random.randint(1, n+1, size=n)  # Discrete uniform distribution from 1 to n                  # Finding the minimum value among the observations         min_observation = np.min(observations)                  # Generating the next observation from the same discrete uniform distribution         next_observation = np.random.randint(1, n+1)                  # Checking if the next observation is the minimum among the previous ones         if next_observation < min_observation:             count_min += 1          probability = count_min / trials     return probability  # Setting the number of trials trials = 10000  # Number of trials results = [] for n in range(3,100):     results.append(experiment(trials, n)) results = np.array(results) print(results) But how do you arrive at the right answer?","My solution is that since the expected value of the discrete uniform distribution is , the probability that the outcome of the second trial is the least of any two trials is . Then the probability that the outcome of the trial being less than each of the outcomes of all the previous trials is . But simulations of the experiment (in python) suggest that the correct answer is different. I have simulated the experiment for , times each, and summarized the final probabilities for various in this graph (with on the x-axis and on the y-axis). The closest formula I could find for these results is , whose graph looks like this . This is the python code that I ran the simulations with: import numpy as np  def experiment(trials, n):     count_min = 0          for _ in range(trials):         # Generating n random observations from a discrete uniform distribution         observations = np.random.randint(1, n+1, size=n)  # Discrete uniform distribution from 1 to n                  # Finding the minimum value among the observations         min_observation = np.min(observations)                  # Generating the next observation from the same discrete uniform distribution         next_observation = np.random.randint(1, n+1)                  # Checking if the next observation is the minimum among the previous ones         if next_observation < min_observation:             count_min += 1          probability = count_min / trials     return probability  # Setting the number of trials trials = 10000  # Number of trials results = [] for n in range(3,100):     results.append(experiment(trials, n)) results = np.array(results) print(results) But how do you arrive at the right answer?",\frac{n+1}{2} \frac{1}{2} (n+1)^\text{th} n (\frac{1}{2})^n n = 2 \text{ to } 100 10000 n's n p \frac{1}{2n},"['probability', 'statistics', 'probability-distributions']"
39,Calculating the probability of a set of 4 players with at least one hand with more than D doubles (in double 6 dominoes),Calculating the probability of a set of 4 players with at least one hand with more than D doubles (in double 6 dominoes),,"I'm new when it comes to statistics, so apologies if my math is wrong or even not relatable. Assuming each player gets 7 dominoes in a field of 28. Player 1 picks 7, then player 2, and so on... I suspect, to calculate the probability of a single person receiving ""D"" doubles in ""P"" picks is: $$(\frac{7!}{(7-D)!})(\frac{21!}{(21-P+D)!})(\frac{(28-P)!}{28!})$$ When P = 7 and D = 7, I get the probability of you getting all doubles which is 1 in 1184040. I believe 28C7 is 1184040 so this appears to be right since there's only 1 hand that has all doubles. My instinct was to find the probability of 21 picks and generate the table below (P = 21) ""D"" Doubles 1 in # Hands 7 1184040 6 169148.6 5 56382.86 4 33829.71 3 33829.71 2 56382.86 1 169148.6 0 1184040 I find the data a bit suspicious. I find it strange that it is as likely for you to be left with a hand of non-doubles after the three players have picked theirs, as it is to be left with the only set of doubles. While I think the calculation makes sense, I'm not 100% sure I'm getting the calculation I want. As the title states I'm looking for the probability of at least one player getting D doubles.","I'm new when it comes to statistics, so apologies if my math is wrong or even not relatable. Assuming each player gets 7 dominoes in a field of 28. Player 1 picks 7, then player 2, and so on... I suspect, to calculate the probability of a single person receiving ""D"" doubles in ""P"" picks is: When P = 7 and D = 7, I get the probability of you getting all doubles which is 1 in 1184040. I believe 28C7 is 1184040 so this appears to be right since there's only 1 hand that has all doubles. My instinct was to find the probability of 21 picks and generate the table below (P = 21) ""D"" Doubles 1 in # Hands 7 1184040 6 169148.6 5 56382.86 4 33829.71 3 33829.71 2 56382.86 1 169148.6 0 1184040 I find the data a bit suspicious. I find it strange that it is as likely for you to be left with a hand of non-doubles after the three players have picked theirs, as it is to be left with the only set of doubles. While I think the calculation makes sense, I'm not 100% sure I'm getting the calculation I want. As the title states I'm looking for the probability of at least one player getting D doubles.",(\frac{7!}{(7-D)!})(\frac{21!}{(21-P+D)!})(\frac{(28-P)!}{28!}),"['probability', 'statistics']"
40,Most likely configuration of coins flips given their value,Most likely configuration of coins flips given their value,,"Suppose I have 500 silver coins and 500 gold coins, and all are fair. If a heads on a silver coin gives me 1 dollar, and a heads on a gold coin gives me 3 dollars. Then we know the expected number of dollars received after flipping all 1000 coins is $1\cdot250 + 3\cdot250=1000$ . The question: Suppose I flipped all the coins and got 1100 dollars. What is the most likely configurations of the number of silvers and number of golds? Thoughts: I know there are many possible ways this could happen. The extra \$100 outside of the expected 1000 could come from, say, 100 extra silver heads, or 33 golds and 1 silver. Intuitively, both of these configurations are unlikely because there is only way of scoring 100 extra silvers, e.g. So something in between all silvers and (almost) all golds should be more likely. I could brute force this problem by using the binomial distribution to calculate the probability of any number of heads: $P(k \text{ heads}) = (500 \text{ choose } k)\left(\frac12\right)^{500} $ . So we could find the probability of any configuration ( $j$ silver heads, $k$ gold heads). But I would have to plug in every possible combination that adds up to 100. Note: this was an interview question, and was meant to be solved intuitively rather than with equations.","Suppose I have 500 silver coins and 500 gold coins, and all are fair. If a heads on a silver coin gives me 1 dollar, and a heads on a gold coin gives me 3 dollars. Then we know the expected number of dollars received after flipping all 1000 coins is . The question: Suppose I flipped all the coins and got 1100 dollars. What is the most likely configurations of the number of silvers and number of golds? Thoughts: I know there are many possible ways this could happen. The extra \$100 outside of the expected 1000 could come from, say, 100 extra silver heads, or 33 golds and 1 silver. Intuitively, both of these configurations are unlikely because there is only way of scoring 100 extra silvers, e.g. So something in between all silvers and (almost) all golds should be more likely. I could brute force this problem by using the binomial distribution to calculate the probability of any number of heads: . So we could find the probability of any configuration ( silver heads, gold heads). But I would have to plug in every possible combination that adds up to 100. Note: this was an interview question, and was meant to be solved intuitively rather than with equations.",1\cdot250 + 3\cdot250=1000 P(k \text{ heads}) = (500 \text{ choose } k)\left(\frac12\right)^{500}  j k,"['probability', 'statistics', 'puzzle']"
41,Is the sample mean absolute deviation unbiased for a normally distributed population?,Is the sample mean absolute deviation unbiased for a normally distributed population?,,"Let $X_1,\ldots,X_n$ be a random sample from a normally distributed population. Is the sample mean average deviation $$\frac{\sum_{i=1}^n|X_i-\bar{X}|}{n}$$ an unbiased estimator of the population mean average deviation?",Let be a random sample from a normally distributed population. Is the sample mean average deviation an unbiased estimator of the population mean average deviation?,"X_1,\ldots,X_n \frac{\sum_{i=1}^n|X_i-\bar{X}|}{n}","['statistics', 'statistical-inference', 'descriptive-statistics']"
42,Is the zero expectation of the residuals in Gauss-Markov theorem really an hypothesis?,Is the zero expectation of the residuals in Gauss-Markov theorem really an hypothesis?,,"Wikipedia says The Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. Alright. But the fact is that, when I compute the expectation of the residuals taking into account the OLS estimator, I naturally obtain $0$ . So my question is: do we need to explictly consider the zero-expectation of the residuals as an hypothesis ? Isn't it already induced by the computation of the estimator through OLS ?","Wikipedia says The Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. Alright. But the fact is that, when I compute the expectation of the residuals taking into account the OLS estimator, I naturally obtain . So my question is: do we need to explictly consider the zero-expectation of the residuals as an hypothesis ? Isn't it already induced by the computation of the estimator through OLS ?",0,"['statistics', 'statistical-inference', 'regression', 'parameter-estimation', 'least-squares']"
43,About basic statistical definitions,About basic statistical definitions,,"I'm having a hard time trying to understand some basic statistical definitions presented in the section II.9 of the book ""Mathematical Statistics"" written by Wiebe R. Pestman. Below I'll write the excerpt of the book that I'm trying to understand. Let $\Theta$ be an arbitrary set and let $f:\mathbb{R}\times\Theta \to  [0,+\infty )$ be a function. For every fixed $\theta\in\Theta$ denote the function $x\mapsto f(x,\theta)$ by $f(\bullet ,\theta)$ . If for all $\theta\in\Theta$ this function presents a (possibly discrete) probability density, then one talks about a family of probability densities with parameter $\theta\in\Theta$ . [...] Now, once and for all, let $\big(f(\bullet,\Theta)\big)_{\theta\in\Theta}$ be any family of probability densities. It is assumed that the population from which the samples are drawn has a probability density $f(\bullet,\theta)$ , where $\theta\in\Theta$ . [...] Definition II.9.1. A function $\kappa:\Theta\to\mathbb{R}$ is said to be a characteristic of the population. [...] Definition II.9.2: Let $X_1,\cdots,X_n$ be a sample from a population with probability density $f(\bullet,\theta)$ , where $\theta\in\Theta$ . Suppose $\kappa$ is a characteristic of the population. A statistic $T=g(X_1,\cdots,X_n)$ , the outcome of which is used as an estimate of $\kappa(\theta)$ , is said to be an estimator of $\kappa$ (based on the sample $X_1,\cdots,X_n$ ). Note that the expectation value of a statistic $T=g(X_1,\cdots,X_n)$ will, if existing, in general depend on the parameter $\theta$ . [...]. If in some context one has to count with this, one could write $\mathbb{E}_\theta(T)$ instead of $\mathbb{E}(T)$ . Definition II.9.3: A statistic $T$ is said to be an unbiased estimator of $\kappa$ if $\mathbb{E}_\theta(T)=\kappa(\theta)$ for all $\theta \in\Theta$ . My question is (EDITED): What does the definition II.9.3 mean? Does it mean that given any $\theta\in \Theta$ , any $n\in\mathbb{N}$ and any sample $X_1,\cdots,X_n$ with distribution $f(\bullet,\theta)$ there's a Borel measurable function $g:\mathbb{R}^n\to\mathbb{R}$ such that $T=g(X_1,\cdots,X_n)$ and $\mathbb{E}_\theta (T)=\kappa(\theta)$ ? I tried to interpret the content of that excerpt with the following definitions. Definition 1: We say that $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ is a statistical model if $(\Omega,\Sigma,\mathbb{P})$ is probability space, $\Theta$ is a non-empty set and $f_\theta:\mathbb{R}\to\mathbb{R}$ is a density function for all $\theta\in\Theta$ . Definition 2: Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model. We say that random variables $X_1,\cdots,X_n:\Omega\to\mathbb{R}$ is a $\theta$ -sample if $\theta\in\Theta$ , $X_1,\cdots,X_n$ are iid with respect to $(\Omega,\Sigma,\mathbb{P})$ and have common distribution $f_\theta$ . Definition 3: Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model and $X_1,\cdots,X_n$ be a $\theta$ -sample. We say that $T:\Omega\to \mathbb{R}$ is a statistic (with respect to that sample) if there's a Borel measurable function $g:\mathbb{R}^n\to\mathbb{R}$ such that $T=g(X_1,\cdots,X_n)$ . Definition 4 (EDITED): Let $(\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta)$ be a statistical model and $\kappa:\Omega\to\mathbb{R}$ be any function. We say that a Borel function $g:\mathbb{R}^n\to \mathbb{R}$ is unbiased estimator of $\kappa$ if for any $\theta\in \Theta$ and any $\theta$ -sample $X_1,\cdots,X_n$ we have $\mathbb{E}[g(X_1,\cdots,X_n)]=\kappa (\theta)$ Another question: Are these definitions really a good interpretation of that excerpt? Also, are they in line with standard statistical definitions? Thank you for your attention!","I'm having a hard time trying to understand some basic statistical definitions presented in the section II.9 of the book ""Mathematical Statistics"" written by Wiebe R. Pestman. Below I'll write the excerpt of the book that I'm trying to understand. Let be an arbitrary set and let be a function. For every fixed denote the function by . If for all this function presents a (possibly discrete) probability density, then one talks about a family of probability densities with parameter . [...] Now, once and for all, let be any family of probability densities. It is assumed that the population from which the samples are drawn has a probability density , where . [...] Definition II.9.1. A function is said to be a characteristic of the population. [...] Definition II.9.2: Let be a sample from a population with probability density , where . Suppose is a characteristic of the population. A statistic , the outcome of which is used as an estimate of , is said to be an estimator of (based on the sample ). Note that the expectation value of a statistic will, if existing, in general depend on the parameter . [...]. If in some context one has to count with this, one could write instead of . Definition II.9.3: A statistic is said to be an unbiased estimator of if for all . My question is (EDITED): What does the definition II.9.3 mean? Does it mean that given any , any and any sample with distribution there's a Borel measurable function such that and ? I tried to interpret the content of that excerpt with the following definitions. Definition 1: We say that is a statistical model if is probability space, is a non-empty set and is a density function for all . Definition 2: Let be a statistical model. We say that random variables is a -sample if , are iid with respect to and have common distribution . Definition 3: Let be a statistical model and be a -sample. We say that is a statistic (with respect to that sample) if there's a Borel measurable function such that . Definition 4 (EDITED): Let be a statistical model and be any function. We say that a Borel function is unbiased estimator of if for any and any -sample we have Another question: Are these definitions really a good interpretation of that excerpt? Also, are they in line with standard statistical definitions? Thank you for your attention!","\Theta f:\mathbb{R}\times\Theta \to
 [0,+\infty ) \theta\in\Theta x\mapsto f(x,\theta) f(\bullet ,\theta) \theta\in\Theta \theta\in\Theta \big(f(\bullet,\Theta)\big)_{\theta\in\Theta} f(\bullet,\theta) \theta\in\Theta \kappa:\Theta\to\mathbb{R} X_1,\cdots,X_n f(\bullet,\theta) \theta\in\Theta \kappa T=g(X_1,\cdots,X_n) \kappa(\theta) \kappa X_1,\cdots,X_n T=g(X_1,\cdots,X_n) \theta \mathbb{E}_\theta(T) \mathbb{E}(T) T \kappa \mathbb{E}_\theta(T)=\kappa(\theta) \theta \in\Theta \theta\in \Theta n\in\mathbb{N} X_1,\cdots,X_n f(\bullet,\theta) g:\mathbb{R}^n\to\mathbb{R} T=g(X_1,\cdots,X_n) \mathbb{E}_\theta (T)=\kappa(\theta) (\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta) (\Omega,\Sigma,\mathbb{P}) \Theta f_\theta:\mathbb{R}\to\mathbb{R} \theta\in\Theta (\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta) X_1,\cdots,X_n:\Omega\to\mathbb{R} \theta \theta\in\Theta X_1,\cdots,X_n (\Omega,\Sigma,\mathbb{P}) f_\theta (\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta) X_1,\cdots,X_n \theta T:\Omega\to \mathbb{R} g:\mathbb{R}^n\to\mathbb{R} T=g(X_1,\cdots,X_n) (\Omega,\Sigma,\mathbb{P},f_\theta :\theta\in\Theta) \kappa:\Omega\to\mathbb{R} g:\mathbb{R}^n\to \mathbb{R} \kappa \theta\in \Theta \theta X_1,\cdots,X_n \mathbb{E}[g(X_1,\cdots,X_n)]=\kappa (\theta)","['statistics', 'definition']"
44,Significance of t-tests and outlieres,Significance of t-tests and outlieres,,"I was performing paired t-tests and came across the example from the R function. They show that addition of strong outlier can make the test more compliant about differences: t.test(1:10, y = c(7:20))      # P = .00001855  t.test(1:10, y = c(7:20, 200)) # P = .1245    -- NOT significant anymore How is this possible ? Does that mean that a t-test shouldn't be used in the case of outliers presence in the data ?","I was performing paired t-tests and came across the example from the R function. They show that addition of strong outlier can make the test more compliant about differences: t.test(1:10, y = c(7:20))      # P = .00001855  t.test(1:10, y = c(7:20, 200)) # P = .1245    -- NOT significant anymore How is this possible ? Does that mean that a t-test shouldn't be used in the case of outliers presence in the data ?",,['statistics']
45,"A decomposition of Fisher Information: is $I_{\mathbf X,T(\mathbf X)}=I_{\mathbf X}$ true?",A decomposition of Fisher Information: is  true?,"I_{\mathbf X,T(\mathbf X)}=I_{\mathbf X}","Suppose $\mathbf X=(X_1,\dots,X_N)$ where $X_n\sim f_X(\theta)$ are iid random variables parameterized in terms of $\theta$ . Furthermore, denote the Fisher information by $$ I_{\mathbf X}(\theta)=-\mathsf E(\nabla_\theta\nabla_\theta^T\log f_X(\mathbf X|\theta)). $$ and $$ T(\mathbf X)=\left(\frac{1}{N}\sum_{n=1}^NX_n,\frac{1}{N}\sum_{n=1}^N(X_n-\bar X)^2\right) $$ to be the sample mean and variance. Can we decompose $I_{\mathbf X}$ as $$ \tag{1} I_{\mathbf X}(\theta)=I_{T(\mathbf X)}(\theta)+I_{\mathbf X|T(\mathbf X)}(\theta)? $$ If so, how would this decomposition be derived? I am aware of the decomposition for jointly distributed variables $(X,Y)$ : $$ I_{X,Y}(\theta)=I_{Y}(\theta)+I_{X|Y}(\theta), $$ which is easily derived since the information is based on a logarithm.  However, in the contect of my question, shouldn't $I_{\mathbf X,T(\mathbf X)}=I_{\mathbf X}$ since knowledge of $T(\mathbf X)$ does not add additional information?","Suppose where are iid random variables parameterized in terms of . Furthermore, denote the Fisher information by and to be the sample mean and variance. Can we decompose as If so, how would this decomposition be derived? I am aware of the decomposition for jointly distributed variables : which is easily derived since the information is based on a logarithm.  However, in the contect of my question, shouldn't since knowledge of does not add additional information?","\mathbf X=(X_1,\dots,X_N) X_n\sim f_X(\theta) \theta 
I_{\mathbf X}(\theta)=-\mathsf E(\nabla_\theta\nabla_\theta^T\log f_X(\mathbf X|\theta)).
 
T(\mathbf X)=\left(\frac{1}{N}\sum_{n=1}^NX_n,\frac{1}{N}\sum_{n=1}^N(X_n-\bar X)^2\right)
 I_{\mathbf X} 
\tag{1}
I_{\mathbf X}(\theta)=I_{T(\mathbf X)}(\theta)+I_{\mathbf X|T(\mathbf X)}(\theta)?
 (X,Y) 
I_{X,Y}(\theta)=I_{Y}(\theta)+I_{X|Y}(\theta),
 I_{\mathbf X,T(\mathbf X)}=I_{\mathbf X} T(\mathbf X)","['probability', 'statistics', 'fisher-information']"
46,The logic of P values and rejecting,The logic of P values and rejecting,,Could someone please explain the logic of the reasoning behind why we reject the $H_0$ if the $\alpha$ exceeds the $p$ value What I understand is that $\alpha$ is the probability of making a type 1 error if $H_0$ is true. $p$ value is the likelihood of getting a value more to the right or left of your test stat if the $H_0$ is true (probability of getting a test stat that favors $H_a$ more) So if $\alpha>p$ value means we reject $H_0$ Then it must mean that if the probability of making an error while $H_0$ is true is larger than the probability of getting a value more the to the right of your test statistic then reject $H_0$ . How does that logic add up? Could someone explain the reason behind the why we reject $H_0$ ? Thank you,Could someone please explain the logic of the reasoning behind why we reject the if the exceeds the value What I understand is that is the probability of making a type 1 error if is true. value is the likelihood of getting a value more to the right or left of your test stat if the is true (probability of getting a test stat that favors more) So if value means we reject Then it must mean that if the probability of making an error while is true is larger than the probability of getting a value more the to the right of your test statistic then reject . How does that logic add up? Could someone explain the reason behind the why we reject ? Thank you,H_0 \alpha p \alpha H_0 p H_0 H_a \alpha>p H_0 H_0 H_0 H_0,['statistics']
47,"What are some calculus, linear algebra and probability and statistics books you would recommend that focus on applications with minimum proofs?","What are some calculus, linear algebra and probability and statistics books you would recommend that focus on applications with minimum proofs?",,"I am a machine learning engineer and I decided I wanted to brush up on the topics relevant for machine learning: calculus linear algebra probability and statistics I will be doing this in my free time, alongside my full-time job. My college education entailed calculus (although I didn't do any multiple integrals, for example). I also had linear algebra and discrete mathematics. I had statistics, but not probability, although I did self-study probability, but I never did probability with calculus for example (like calculating the surface under a probability density function using an integral). I want to emphasize that the books should be focused on applications and not proofs. Actually, the less theory oriented the book, the better. I don't mind going through a few crucial proofs, but that's it. I want to focus on the ""mechanics"". Maybe in a later pass through these topics I will find myself reading some more theory oriented books, but for now, my plan is to read application-oriented books with minimal or no focus on theory and try to solve as many exercises as I can. Having said this, what are your recommendations for books on calculus, linear algebra and probability and statistics that focus on applications with minimum proofs?","I am a machine learning engineer and I decided I wanted to brush up on the topics relevant for machine learning: calculus linear algebra probability and statistics I will be doing this in my free time, alongside my full-time job. My college education entailed calculus (although I didn't do any multiple integrals, for example). I also had linear algebra and discrete mathematics. I had statistics, but not probability, although I did self-study probability, but I never did probability with calculus for example (like calculating the surface under a probability density function using an integral). I want to emphasize that the books should be focused on applications and not proofs. Actually, the less theory oriented the book, the better. I don't mind going through a few crucial proofs, but that's it. I want to focus on the ""mechanics"". Maybe in a later pass through these topics I will find myself reading some more theory oriented books, but for now, my plan is to read application-oriented books with minimal or no focus on theory and try to solve as many exercises as I can. Having said this, what are your recommendations for books on calculus, linear algebra and probability and statistics that focus on applications with minimum proofs?",,"['calculus', 'linear-algebra', 'probability', 'statistics', 'reference-request']"
48,Independence of random pairs and conditional expectation,Independence of random pairs and conditional expectation,,"Let $(X_1,Y_1)$ and $(X_2,Y_2)$ be two independent and identically distributed random pairs. Does it hold that $$E(Y_1Y_2|X_1,X_2)=E(Y_1|X_1)E(Y_2|X_2)?$$ Why? Comments By hypothesis, the two sigma-algebras are independent $\sigma(X_1,Y_1)\perp \sigma(X_2,Y_2)$ . It implies that $\sigma(Y_1)\perp \sigma(X_2,Y_2)$ and $\sigma(Y_2)\perp \sigma(X_1,Y_1)$ . So $E(Y_1Y_2|X_2)=E(Y_1)E(Y_2|X_2)$ and $E(Y_1Y_2|X_1)=E(Y_2)E(Y_1|X_1)$ . Is everything correct so far? If so, my intuition says that $E(Y_1Y_2|X_2,X_1)=E(Y_1|X_1)E(Y_2|X_2)$ but I'm not quite sure how to show it.","Let and be two independent and identically distributed random pairs. Does it hold that Why? Comments By hypothesis, the two sigma-algebras are independent . It implies that and . So and . Is everything correct so far? If so, my intuition says that but I'm not quite sure how to show it.","(X_1,Y_1) (X_2,Y_2) E(Y_1Y_2|X_1,X_2)=E(Y_1|X_1)E(Y_2|X_2)? \sigma(X_1,Y_1)\perp \sigma(X_2,Y_2) \sigma(Y_1)\perp \sigma(X_2,Y_2) \sigma(Y_2)\perp \sigma(X_1,Y_1) E(Y_1Y_2|X_2)=E(Y_1)E(Y_2|X_2) E(Y_1Y_2|X_1)=E(Y_2)E(Y_1|X_1) E(Y_1Y_2|X_2,X_1)=E(Y_1|X_1)E(Y_2|X_2)","['statistics', 'conditional-expectation', 'independence']"
49,"What does it mean that a random variable has ""Lebesgue density""?","What does it mean that a random variable has ""Lebesgue density""?",,"My question is basically what the title says. I am solving some exercises in statitics and was told no knowledge of measure theory is required but I am not sure if that is the case. I've been looking into Lebesgue measures but I cannot find a definition of what it means that a random variable has Lebesgue density given by some formula. For example a question where I've been stuck because of this: Let $Y$ be a random variable with a Lebesgue density, and let $Z$ be a discrete random variable with (probable) probabilities $\{ z_k \mid k \}$ . Furthermore, let $Y$ and $Z$ be independent. Justify that $X = Y + Z$ also has a Lebesgue density and calculate it. Thank you!","My question is basically what the title says. I am solving some exercises in statitics and was told no knowledge of measure theory is required but I am not sure if that is the case. I've been looking into Lebesgue measures but I cannot find a definition of what it means that a random variable has Lebesgue density given by some formula. For example a question where I've been stuck because of this: Let be a random variable with a Lebesgue density, and let be a discrete random variable with (probable) probabilities . Furthermore, let and be independent. Justify that also has a Lebesgue density and calculate it. Thank you!",Y Z \{ z_k \mid k \} Y Z X = Y + Z,"['statistics', 'lebesgue-measure']"
50,Sum of integrals of gaussian function,Sum of integrals of gaussian function,,"The background for this question is inferential statistics. I am trying to derive some characteristics about estimators in the context of design-based inference. Let us assume I have a population of $N$ elements. Each element is associated to a value $\beta_i$ . I do not know the individual values of $\beta_i$ for each population element, but I know their mean (denoted as $\mu_\beta$ ) and variance (denoted as $\sigma_\beta^2$ ). To each population element, I apply the function: $$\int_{2-\beta_i}^{\infty} \, e^{-t^2/2} \, dt. $$ Now, I want to sum the result of this function over each population element. I believe I can formally write this problem in the following format: $$ \sum_{i=1}^{N} \int_{2-\beta_i}^{\infty} \,  e^{-t^2/2}  \, dt. $$ Do you think this can be simplified, expressed, or solved as a function of the only valid information I have, namely the mean and variance of $\beta_i$ ? EDIT:   After the very good answers from Henry and Eduardo, there doesn't appear to be any further simplification possible. However, what would happen if I knew the distribution of values of βi, in addition to their mean and variance? Could this potentially lead to a multivariable calculus problem? BTW I hope this edit in within the forum policy, I can create a new post if necessary....","The background for this question is inferential statistics. I am trying to derive some characteristics about estimators in the context of design-based inference. Let us assume I have a population of elements. Each element is associated to a value . I do not know the individual values of for each population element, but I know their mean (denoted as ) and variance (denoted as ). To each population element, I apply the function: Now, I want to sum the result of this function over each population element. I believe I can formally write this problem in the following format: Do you think this can be simplified, expressed, or solved as a function of the only valid information I have, namely the mean and variance of ? EDIT:   After the very good answers from Henry and Eduardo, there doesn't appear to be any further simplification possible. However, what would happen if I knew the distribution of values of βi, in addition to their mean and variance? Could this potentially lead to a multivariable calculus problem? BTW I hope this edit in within the forum policy, I can create a new post if necessary....","N \beta_i \beta_i \mu_\beta \sigma_\beta^2 \int_{2-\beta_i}^{\infty} \, e^{-t^2/2} \, dt.   \sum_{i=1}^{N} \int_{2-\beta_i}^{\infty} \,  e^{-t^2/2}  \, dt.  \beta_i","['calculus', 'statistics', 'normal-distribution', 'statistical-inference', 'gaussian-integral']"
51,Why does CDF tend to be a power-law around 0?,Why does CDF tend to be a power-law around 0?,,"If we look at CDF of various famous random variables around 0, they tend to be well approximated by a power law. Why? Below are 24 examples, notable exceptions are log-normal, Frechet, Inverse Gamma and Inverse Normal, the rest appear to be fit with a power law. $$\left( \begin{array}{cc}  \text{ChiSquared} & \sqrt{\frac{2}{\pi }} \sqrt{x} \\  \text{Marchenko-Pastur} & \frac{2 \sqrt{x}}{\pi } \\  \text{Beta(1/2,1)} & \sqrt{x} \\  \text{ArcSin} & \frac{2 \sqrt{x}}{\pi } \\  \text{Weihbul(1/2,2)} & \frac{\sqrt{x}}{\sqrt{2}} \\  \text{Exponential} & x \\  \text{Chi} & \sqrt{\frac{2}{\pi }} x \\  \text{Student-T} & \frac{2 x}{\pi } \\  \text{Normal} & \sqrt{\frac{2}{\pi }} x \\  \text{Cauchy} & \frac{2 x}{\pi } \\  \text{Semicircle} & \frac{2 x}{\pi } \\  \text{Gumbel} & x \\  \text{F-ratio(2,2)} & x \\  \text{Gamma(1,2)} & \frac{x}{2} \\  \text{Extreme Value} & \frac{x}{e-1} \\  \text{Logistic} & \frac{x}{2} \\  \text{Uniform} & x \\  \text{Inverse Normal} & -\frac{e x^2}{16}-\frac{x^2}{16}+\sqrt{\frac{2}{\pi }} e^{-\frac{(x-2)^2}{8 x}} \sqrt{x} \\  \text{Erlang(2,2)} & 2 x^2 \\  \text{Triangle} & 2 x^2 \\  \text{Kumaraswamy(2,3)} & 3 x^2 \\  \text{Bates(3)} & \frac{9 x^3}{2} \\  \text{LogNormal} & -\frac{e^{-\frac{1}{2} \log ^2(x)}}{\sqrt{2 \pi } \log (x)} \\  \text{InverseGamma} & e^{-2/x} \\  \text{Frechet(2,1,0)} & e^{-\frac{1}{x^2}} \\  \text{Pareto(1,2)} & 0 \\ \end{array} \right)$$ Notebook","If we look at CDF of various famous random variables around 0, they tend to be well approximated by a power law. Why? Below are 24 examples, notable exceptions are log-normal, Frechet, Inverse Gamma and Inverse Normal, the rest appear to be fit with a power law. Notebook","\left(
\begin{array}{cc}
 \text{ChiSquared} & \sqrt{\frac{2}{\pi }} \sqrt{x} \\
 \text{Marchenko-Pastur} & \frac{2 \sqrt{x}}{\pi } \\
 \text{Beta(1/2,1)} & \sqrt{x} \\
 \text{ArcSin} & \frac{2 \sqrt{x}}{\pi } \\
 \text{Weihbul(1/2,2)} & \frac{\sqrt{x}}{\sqrt{2}} \\
 \text{Exponential} & x \\
 \text{Chi} & \sqrt{\frac{2}{\pi }} x \\
 \text{Student-T} & \frac{2 x}{\pi } \\
 \text{Normal} & \sqrt{\frac{2}{\pi }} x \\
 \text{Cauchy} & \frac{2 x}{\pi } \\
 \text{Semicircle} & \frac{2 x}{\pi } \\
 \text{Gumbel} & x \\
 \text{F-ratio(2,2)} & x \\
 \text{Gamma(1,2)} & \frac{x}{2} \\
 \text{Extreme Value} & \frac{x}{e-1} \\
 \text{Logistic} & \frac{x}{2} \\
 \text{Uniform} & x \\
 \text{Inverse Normal} & -\frac{e x^2}{16}-\frac{x^2}{16}+\sqrt{\frac{2}{\pi }} e^{-\frac{(x-2)^2}{8 x}} \sqrt{x} \\
 \text{Erlang(2,2)} & 2 x^2 \\
 \text{Triangle} & 2 x^2 \\
 \text{Kumaraswamy(2,3)} & 3 x^2 \\
 \text{Bates(3)} & \frac{9 x^3}{2} \\
 \text{LogNormal} & -\frac{e^{-\frac{1}{2} \log ^2(x)}}{\sqrt{2 \pi } \log (x)} \\
 \text{InverseGamma} & e^{-2/x} \\
 \text{Frechet(2,1,0)} & e^{-\frac{1}{x^2}} \\
 \text{Pareto(1,2)} & 0 \\
\end{array}
\right)","['probability', 'statistics', 'probability-distributions', 'soft-question', 'asymptotics']"
52,"How to calculate the probabilities of outcomes, of 3d6 - drop lowest dice","How to calculate the probabilities of outcomes, of 3d6 - drop lowest dice",,"I am currently doing a spot of game-design but I am held back by being only a novice in the arts of mathematics. Here is the problem: I wish to calculate the probabilities of the eleven outcomes of rolling 3d6 but dropping the lowest dice. So it is like rolling 2d6 but with a significant bias for higher rolls. When rolling 3d6 , there are 216 different dice combinations. In order to get the outcome 2 (with 3d6-drop lowest), all 3 dice must be 1 (1, 1, 1) (Orelse we would be dropping a 1 in favour of a dice that doesn't show 1) the probability of getting the outcome 2, is therefore 1/216. I can also tell that the probability of getting the outcome 12 , is 16/216 since there are 16 different outcomes that result in 12. When rolling 3d6 there are 16 different outcomes that contain at least 2 6'es. That said, counting out all the possible combinations seem like a very manual way to get these probabilities, that will be nearly impossible when it comes to the more likely outcomes, such as 7, 8 and 9... What is a good method to actually figuring out what I want to know? Outcomes: 2: 1/216 3: ?? 4: ?? 5: ?? 6: ?? 7: ?? 8: ?? 9: ?? 10 ?? 11: ?? 12: 16/216 Edit: cleaning some small mistakes in my explanation Edit2: Thank you Henry, there are 11 outcomes of course, not 12.","I am currently doing a spot of game-design but I am held back by being only a novice in the arts of mathematics. Here is the problem: I wish to calculate the probabilities of the eleven outcomes of rolling 3d6 but dropping the lowest dice. So it is like rolling 2d6 but with a significant bias for higher rolls. When rolling 3d6 , there are 216 different dice combinations. In order to get the outcome 2 (with 3d6-drop lowest), all 3 dice must be 1 (1, 1, 1) (Orelse we would be dropping a 1 in favour of a dice that doesn't show 1) the probability of getting the outcome 2, is therefore 1/216. I can also tell that the probability of getting the outcome 12 , is 16/216 since there are 16 different outcomes that result in 12. When rolling 3d6 there are 16 different outcomes that contain at least 2 6'es. That said, counting out all the possible combinations seem like a very manual way to get these probabilities, that will be nearly impossible when it comes to the more likely outcomes, such as 7, 8 and 9... What is a good method to actually figuring out what I want to know? Outcomes: 2: 1/216 3: ?? 4: ?? 5: ?? 6: ?? 7: ?? 8: ?? 9: ?? 10 ?? 11: ?? 12: 16/216 Edit: cleaning some small mistakes in my explanation Edit2: Thank you Henry, there are 11 outcomes of course, not 12.",,"['probability', 'statistics', 'dice']"
53,Show that $\sigma_n^2$ is an unbiased estimator for $\sigma^2$,Show that  is an unbiased estimator for,\sigma_n^2 \sigma^2,"Let $X_n$ be a sequence of i.i.d. random variables with $E(X_i)=\mu,Var(X_i)=\sigma^2$ and define $\mu_n:=\frac{1}{n} \sum_{i=1}^n X_i,\sigma_n^2:=\sum_{i=1}^n(X_i-\mu_n)^2.$ I have already shown that $Var(\mu_n)=\frac{\sigma^2}{n}$ . Now to show that $\sigma_n^2$ is an unbiased estimator for $\sigma^2$ I need to show $E(\sigma_n^2)=\sigma^2$ . What I have so far is: \begin{align} E(\sum_{i=1}^n(X_i-\mu_n)^2)&=\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^nVar(X_i-\mu_n)-(E(X_i-\mu_n))^2\\&=\sum_{i=1}^nVar(X_i)+Var(\mu_n)-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^n \sigma^2+\sigma^2/n-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=n\sigma^2+\sigma^2-E(\sum_{i=1}^n(X_i-\mu_n)^2) \end{align} Here I have used the fact that the variance of the sum of two independent variables is the sum of the single variances, the product of expectations of independent variables is the expectation of the product and the sum of the expectations is the expectation of the sum. Adding the identical sums on both sides togehter I get $2E(\sum_{i=1}^n(X_i-\mu_n)^2)=n\sigma^2+\sigma^2$ . Dividing by $2$ unfortunately doesn't give me $\sigma^2$ due to the $n$ on the right side which makes me think that some of my conclusions were wrong. Any help would be appreciated!","Let be a sequence of i.i.d. random variables with and define I have already shown that . Now to show that is an unbiased estimator for I need to show . What I have so far is: Here I have used the fact that the variance of the sum of two independent variables is the sum of the single variances, the product of expectations of independent variables is the expectation of the product and the sum of the expectations is the expectation of the sum. Adding the identical sums on both sides togehter I get . Dividing by unfortunately doesn't give me due to the on the right side which makes me think that some of my conclusions were wrong. Any help would be appreciated!","X_n E(X_i)=\mu,Var(X_i)=\sigma^2 \mu_n:=\frac{1}{n} \sum_{i=1}^n X_i,\sigma_n^2:=\sum_{i=1}^n(X_i-\mu_n)^2. Var(\mu_n)=\frac{\sigma^2}{n} \sigma_n^2 \sigma^2 E(\sigma_n^2)=\sigma^2 \begin{align}
E(\sum_{i=1}^n(X_i-\mu_n)^2)&=\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^nVar(X_i-\mu_n)-(E(X_i-\mu_n))^2\\&=\sum_{i=1}^nVar(X_i)+Var(\mu_n)-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^n \sigma^2+\sigma^2/n-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=n\sigma^2+\sigma^2-E(\sum_{i=1}^n(X_i-\mu_n)^2)
\end{align} 2E(\sum_{i=1}^n(X_i-\mu_n)^2)=n\sigma^2+\sigma^2 2 \sigma^2 n","['probability', 'statistics', 'expected-value', 'variance']"
54,Relationship between sum of squares and square of sums in a recursive way,Relationship between sum of squares and square of sums in a recursive way,,"In the proof of Theorem 2 of Sparse projections onto the simplex authors mention the following equality for any $\mathbf{b} \in \mathbb{R}^k$ and $\lambda \in \mathbb{R}$ : $$ \begin{aligned} &\sum_{i=1}^k b_i^2 - \frac{((\sum_{i=1}^k b_i) - \lambda)^2}{k} \\ &= \\ &\lambda(2b_1-\lambda)+\sum_{j=2}^k \frac{j-1}{j} \Big( b_j - \frac{(\sum_{i=1}^{j-1}b_i)-\lambda}{j-1} \Big)^2. \end{aligned} $$ Question Is there an elegant way of showing the above equality? Probably it is possible to show it using induction but I am looking to find a way other than induction. My thoughts If we look at the way terms are given on the left hand side, one can think of sampling $k$ number of $b_i$ 's with uniform probability of $\frac{1}{k}$ where each $b_i$ has a fix distribution $\mathcal{D}$ . Then the sums can be written as expected value of k samples of $b_i$ and $b_i^2$ . Looking in this way, can we give another proof to that other than algebraic proof? Another observation is that, the right hand side some how tries to relate sum of squares and square of sums in a recursive way.","In the proof of Theorem 2 of Sparse projections onto the simplex authors mention the following equality for any and : Question Is there an elegant way of showing the above equality? Probably it is possible to show it using induction but I am looking to find a way other than induction. My thoughts If we look at the way terms are given on the left hand side, one can think of sampling number of 's with uniform probability of where each has a fix distribution . Then the sums can be written as expected value of k samples of and . Looking in this way, can we give another proof to that other than algebraic proof? Another observation is that, the right hand side some how tries to relate sum of squares and square of sums in a recursive way.","\mathbf{b} \in \mathbb{R}^k \lambda \in \mathbb{R} 
\begin{aligned}
&\sum_{i=1}^k b_i^2 - \frac{((\sum_{i=1}^k b_i) - \lambda)^2}{k}
\\
&=
\\
&\lambda(2b_1-\lambda)+\sum_{j=2}^k
\frac{j-1}{j}
\Big(
b_j - \frac{(\sum_{i=1}^{j-1}b_i)-\lambda}{j-1}
\Big)^2.
\end{aligned}
 k b_i \frac{1}{k} b_i \mathcal{D} b_i b_i^2","['linear-algebra', 'statistics', 'optimization', 'sum-of-squares-method']"
55,Maximum likelihood estimator of Cauchy distribution,Maximum likelihood estimator of Cauchy distribution,,"The following exercise is from Bickel and Doksum, volume one. Let $g(x) = 1/[\pi (1+x^2)]$ , $x \in \mathbb{R}$ , be the Cauchy density, let $X_1$ and $X_2$ be i.i.d. with density $g(x-\theta)$ , $\theta \in \mathbb{R}$ . Let $x_1$ and $x_2$ be observations and set $\Delta = \frac{1}{2} (x_1 - x_2)$ . Let $\hat \theta = \arg \max L_X (\theta) $ be ""the"" MLE. Show that if $|\Delta| \leq 1$ , then the MLE exists and is unique. Give the MLE when $|\Delta| \leq 1$ . The likelihood function is $$L(\theta) = \frac{1}{\pi^2 \cdot \prod_{i = 1}^{2} (1 + (x_i - \theta)^2)  }$$ I tried taking the log-likelihood and differentiating it w.r.t $\theta$ , which resulted in $\hat \theta = \overline{X}$ . I do not understand, however, why we need $|\Delta| \leq 1$ for uniqueness of the maximum likelihood estimator, or how to establish uniqueness of the MLE.","The following exercise is from Bickel and Doksum, volume one. Let , , be the Cauchy density, let and be i.i.d. with density , . Let and be observations and set . Let be ""the"" MLE. Show that if , then the MLE exists and is unique. Give the MLE when . The likelihood function is I tried taking the log-likelihood and differentiating it w.r.t , which resulted in . I do not understand, however, why we need for uniqueness of the maximum likelihood estimator, or how to establish uniqueness of the MLE.",g(x) = 1/[\pi (1+x^2)] x \in \mathbb{R} X_1 X_2 g(x-\theta) \theta \in \mathbb{R} x_1 x_2 \Delta = \frac{1}{2} (x_1 - x_2) \hat \theta = \arg \max L_X (\theta)  |\Delta| \leq 1 |\Delta| \leq 1 L(\theta) = \frac{1}{\pi^2 \cdot \prod_{i = 1}^{2} (1 + (x_i - \theta)^2)  } \theta \hat \theta = \overline{X} |\Delta| \leq 1,"['statistics', 'statistical-inference']"
56,Doubt about exponential distribution [closed],Doubt about exponential distribution [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm given a certain seed number, lambda and k number of observations. I'm suppossed to find sum(k), divide it in unit intervals, check how many events occured in each interval, calculate the mean of events and finally calculate the absolute deviation between the experimental value I got of the mean and the theoretical expected value. In exponential distribution, lambda is the expected value. Does this still apply in each interval? Is my theoretical expected value equal to lambda? Here follows my code in R set.seed(4731)     k <- 4979     lambda <- 9     data <- rexp(k, lambda)     sum <- cumsum(data)     T <- ceiling(sum[k])     subintervals <- cut(sum, breaks = seq(0, T, 1))     num_events <- table(subintervals)     mean <- mean(num_events)     expected <- lambda     absolute_deviation <- abs(mean - expected)     round(absolute_deviation, 4)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm given a certain seed number, lambda and k number of observations. I'm suppossed to find sum(k), divide it in unit intervals, check how many events occured in each interval, calculate the mean of events and finally calculate the absolute deviation between the experimental value I got of the mean and the theoretical expected value. In exponential distribution, lambda is the expected value. Does this still apply in each interval? Is my theoretical expected value equal to lambda? Here follows my code in R set.seed(4731)     k <- 4979     lambda <- 9     data <- rexp(k, lambda)     sum <- cumsum(data)     T <- ceiling(sum[k])     subintervals <- cut(sum, breaks = seq(0, T, 1))     num_events <- table(subintervals)     mean <- mean(num_events)     expected <- lambda     absolute_deviation <- abs(mean - expected)     round(absolute_deviation, 4)",,['statistics']
57,Why is this constant eliminated in the u-substitution in this proof about the normal distribution?,Why is this constant eliminated in the u-substitution in this proof about the normal distribution?,,"I'm working through a statistical theory text and have this proof that if $X$ is a normally distributed random variable then the random variable $Z = (X - \mu)/\sigma$ has a n(0,1) distribution. The proof is as follows: $$P(Z\leq z) = P(\frac{X-\mu}{\sigma}\leq z)$$ $$= P(X \leq z\sigma + \mu)$$ $$=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{z\sigma + \mu}e^{-(x-\mu)^2/(2\sigma^2)}dx$$ $$=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}e^{-t^2/2}dt$$ (The book uses t instead of u for the u-substitution, I'm guessing because it would look too similar to $\mu$ ) I understand all of the moves in this, except the removal of the sigma constant outside of the integral going from steps 3 to 4. The substitution is $t=\frac{x-\mu}{\sigma}$ , which makes sense to me within the integrand and in changing the upper parameter of the integral, but I can't figure out at all why or how it would cancel out the sigma outside of the integral. I appreciate any help.","I'm working through a statistical theory text and have this proof that if is a normally distributed random variable then the random variable has a n(0,1) distribution. The proof is as follows: (The book uses t instead of u for the u-substitution, I'm guessing because it would look too similar to ) I understand all of the moves in this, except the removal of the sigma constant outside of the integral going from steps 3 to 4. The substitution is , which makes sense to me within the integrand and in changing the upper parameter of the integral, but I can't figure out at all why or how it would cancel out the sigma outside of the integral. I appreciate any help.",X Z = (X - \mu)/\sigma P(Z\leq z) = P(\frac{X-\mu}{\sigma}\leq z) = P(X \leq z\sigma + \mu) =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{z\sigma + \mu}e^{-(x-\mu)^2/(2\sigma^2)}dx =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}e^{-t^2/2}dt \mu t=\frac{x-\mu}{\sigma},"['calculus', 'integration', 'statistics', 'normal-distribution']"
58,Painted Cube and the Law of Total Probability,Painted Cube and the Law of Total Probability,,"Problem: Suppose you are given a white cube that is broken into $27$ pieces.  This cube was originally $3$ units by $3$ units, and has been broken into $27$ smaller cubes, witch each smaller cube having dimensions $1 \times 1 \times 1$ .  Before this cube was broken, all $6$ of its faces were painted green.  You randomly pick a small cube and see that $5$ faces are white.  What is the probability that the bottom face is also white? In this conditional probability problem, I am having trouble determining $P(A)$ and $P(A^c)$ , which are critical to determining $P(B)$ via the Law of Total Probability. $P(B|A)$ is relatively straightforward, since it is the probability of picking a mini-cube with 5 White Faces given that the Bottom Face is White.  Only 1 mini-cube satisfies that, the Core Cube, so it's probability ought to be $1/27$ . Likewise, $P(B|A^c)$ is pretty easy, its the probability of 5 White Faces given that the Bottom Face is Not White (Green).  Only 6 mini-cubes satisfy that, the ones in the centers of the 6 faces of the original cube.  This makes the probability $6/27$ . But what of $P(A)$ , the probability of ""Bottom White""?  Originally I thought it was $1/27$ , since there is only 1 cube that meets that configuration, but upon checking my answer it seems $P(A) = 1$ . $P(A^c)$ is the probability that the Bottom is not White, so originally I went with $6/27$ , the number of mini-cubes that could have 5 white faces and still have a bottom face that is not white.  However, the solution maintains $P(A^c) = 1/6$ . What is the reasoning that leads to $P(A) = 1$ and $P(A^c) = 1/6$ ?","Problem: Suppose you are given a white cube that is broken into pieces.  This cube was originally units by units, and has been broken into smaller cubes, witch each smaller cube having dimensions .  Before this cube was broken, all of its faces were painted green.  You randomly pick a small cube and see that faces are white.  What is the probability that the bottom face is also white? In this conditional probability problem, I am having trouble determining and , which are critical to determining via the Law of Total Probability. is relatively straightforward, since it is the probability of picking a mini-cube with 5 White Faces given that the Bottom Face is White.  Only 1 mini-cube satisfies that, the Core Cube, so it's probability ought to be . Likewise, is pretty easy, its the probability of 5 White Faces given that the Bottom Face is Not White (Green).  Only 6 mini-cubes satisfy that, the ones in the centers of the 6 faces of the original cube.  This makes the probability . But what of , the probability of ""Bottom White""?  Originally I thought it was , since there is only 1 cube that meets that configuration, but upon checking my answer it seems . is the probability that the Bottom is not White, so originally I went with , the number of mini-cubes that could have 5 white faces and still have a bottom face that is not white.  However, the solution maintains . What is the reasoning that leads to and ?",27 3 3 27 1 \times 1 \times 1 6 5 P(A) P(A^c) P(B) P(B|A) 1/27 P(B|A^c) 6/27 P(A) 1/27 P(A) = 1 P(A^c) 6/27 P(A^c) = 1/6 P(A) = 1 P(A^c) = 1/6,"['probability', 'statistics', 'conditional-probability', 'bayes-theorem']"
59,How to calculate relative performance (non-finance) when lower is better?,How to calculate relative performance (non-finance) when lower is better?,,"Example: Car 1 goes from 0-60 in 6 seconds. Car 2 goes from 0-60 in 8 seconds. Car 3 goes from 0-60 in 9 seconds. Now, Car 1 is obviously the fastest, so I want to make it my reference and express other cars' performance relative to it. If higher was better this would be a simple value/reference * 100 , however, if I express Car 1 as 100% this formula makes the other two as 133% and 150% respectively, which is mathematically correct, but makes no sense on a scale of 1-100%. I've wasted the last two hours looking for an answer and only come across two: (1 - value/reference) + 1 or 2 - value/reference . Problem with this is: if a car takes 12 seconds the result would be zero. Again no sense. (1/value)/(1/reference) or reference/value . I'm not sure whether it's correct, but it seems to achieve what I want.","Example: Car 1 goes from 0-60 in 6 seconds. Car 2 goes from 0-60 in 8 seconds. Car 3 goes from 0-60 in 9 seconds. Now, Car 1 is obviously the fastest, so I want to make it my reference and express other cars' performance relative to it. If higher was better this would be a simple value/reference * 100 , however, if I express Car 1 as 100% this formula makes the other two as 133% and 150% respectively, which is mathematically correct, but makes no sense on a scale of 1-100%. I've wasted the last two hours looking for an answer and only come across two: (1 - value/reference) + 1 or 2 - value/reference . Problem with this is: if a car takes 12 seconds the result would be zero. Again no sense. (1/value)/(1/reference) or reference/value . I'm not sure whether it's correct, but it seems to achieve what I want.",,['statistics']
60,"Find $P(X+Y>s+t \mid X>s)$ where $X$,$Y$ are independent exponential distribution","Find  where , are independent exponential distribution",P(X+Y>s+t \mid X>s) X Y,"Let $X$ , $Y$ be independent exponential distribution with parameter $\lambda$ I want to find $P(X+Y>s+t \mid X>s)$ where $X$ , $Y$ are independent exponential distribution. My attempt: \begin{aligned} P(X+Y>s+t, X>s) &= \int \int \mathbb{1}_{x>s} \mathbb{1}_{x+y>s+t} f_{X}(x)f_{Y}(y) \,dx\,dy \\&= \int_{s}^{\infty} \int_{s+t-x}^{\infty} f_{X}(x)f_{Y}(y) \,dx\,dy \end{aligned} But I stuck since \begin{equation} \int_{s+t-x}^{\infty} f_{Y}(y) \,dy = \int_{s+t-x}^{\infty} \lambda e^{-\lambda y} \,dy = e^{-\lambda (s+t-x)} \end{equation} gives \begin{equation} \int_{s}^{\infty} \lambda e^{-\lambda x} e^{-\lambda (s+t-x)} \,dx = \int_{s}^{\infty} \lambda e^{-\lambda (s+t)} \,dx = \lambda e^{-\lambda (s+t)}x |_{s}^{\infty} = \infty \end{equation} What did I miss?","Let , be independent exponential distribution with parameter I want to find where , are independent exponential distribution. My attempt: But I stuck since gives What did I miss?","X Y \lambda P(X+Y>s+t \mid X>s) X Y \begin{aligned}
P(X+Y>s+t, X>s) &= \int \int \mathbb{1}_{x>s} \mathbb{1}_{x+y>s+t} f_{X}(x)f_{Y}(y) \,dx\,dy \\&= \int_{s}^{\infty} \int_{s+t-x}^{\infty} f_{X}(x)f_{Y}(y) \,dx\,dy
\end{aligned} \begin{equation}
\int_{s+t-x}^{\infty} f_{Y}(y) \,dy = \int_{s+t-x}^{\infty} \lambda e^{-\lambda y} \,dy = e^{-\lambda (s+t-x)}
\end{equation} \begin{equation}
\int_{s}^{\infty} \lambda e^{-\lambda x} e^{-\lambda (s+t-x)} \,dx = \int_{s}^{\infty} \lambda e^{-\lambda (s+t)} \,dx = \lambda e^{-\lambda (s+t)}x |_{s}^{\infty} = \infty
\end{equation}","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
61,KL divergence for distribution representing sums of iid random variables,KL divergence for distribution representing sums of iid random variables,,"Sorry if my description is inaccurate, I hope it's understandable. Given $X_1,...,X_n$ , a series of $n$ iid Bernoulli RVs with means $p$ , and a similar series $Y_1,...,Y_n$ with means $q$ , we know that the KL divergence between the probability measure $P$ corresponding to the sum $X=X_1+...+X_n$ and $Q$ that corresponds to $Y=Y_1+...+Y_n$ is (since $X$ and $Y$ are binomial) $$KL(P,Q)=nd(p,q),$$ where $d(p,q)$ stands for the KL divergence between measures of Bernoulli RVs with means $p$ and $q$ . A similar principle also holds for the divergence between measures corresponding to sums of independent Gaussians. My question is whether we can claim this holds in general. Meaning, given i.i.d $X_1,...,X_n$ such that each RV has a measure $P_x$ and similarly for $Y_1,...,Y_n$ and $Q_y$ , define $X=X_1+...+X_n$ and $Y=Y_1+...+Y_n$ where measure $P$ corresponds to $X$ and $Q$ to $Y$ . Can we say that $$KL(P,Q)=nKL(P_x,Q_y)?$$ Thank you in advance!","Sorry if my description is inaccurate, I hope it's understandable. Given , a series of iid Bernoulli RVs with means , and a similar series with means , we know that the KL divergence between the probability measure corresponding to the sum and that corresponds to is (since and are binomial) where stands for the KL divergence between measures of Bernoulli RVs with means and . A similar principle also holds for the divergence between measures corresponding to sums of independent Gaussians. My question is whether we can claim this holds in general. Meaning, given i.i.d such that each RV has a measure and similarly for and , define and where measure corresponds to and to . Can we say that Thank you in advance!","X_1,...,X_n n p Y_1,...,Y_n q P X=X_1+...+X_n Q Y=Y_1+...+Y_n X Y KL(P,Q)=nd(p,q), d(p,q) p q X_1,...,X_n P_x Y_1,...,Y_n Q_y X=X_1+...+X_n Y=Y_1+...+Y_n P X Q Y KL(P,Q)=nKL(P_x,Q_y)?","['probability', 'statistics', 'information-theory', 'entropy']"
62,Best strategy for a 'linearized' coupon collector problem,Best strategy for a 'linearized' coupon collector problem,,"The following problem is motivated by a certain aggravating side-quest in an old video game. The side-quest is itself a game which follows a sequence of rounds; each round can be a success or a failure, and the game ends after $N$ successes. In its simplest form, we take the probability of failure after $k$ successes to be $k/N$ , in which case we recover the coupon collector's problem . (In the original setting the failure of probability is rounded up to the nearest whole percentage less than 100%, e.g., 84.5% rounds to 85% but 99.5% rounds to 99%; I chose not to include this detail for simplicity.) Thus far there is nothing for the player to do besides keep drawing until success. To add a strategic element, we include a base cost for each round (one ""token""). Furthermore, we allow the player to bet multiple tokens: each token past the first reduces the probability of failure by 1%, to a minimum of 0%. Here the analogy with the coupon collector's problem breaks down: drawing multiple coupons can lead to more than one new coupon being drawn, and the probability to get any new coupons would moreover not be a linear function of the number already found. So this is a coupon collector's problem where the probabilities have been 'linearized'. The question is then what strategy to pursue, and this will very much depend on what goal we pursue. If what we want to minimize is the number of rounds, then we should always bet enough tokens to eliminate the chance of failure and thus finish in $N$ rounds. But these bets grow linearly and thus the total number of tokens required scales as $N^2$ . On the other hand, we can simply bet the one-token minimum each round. In that case the number of tokens is the same as the number of rounds, which scales as $N\log N$ per the coupon collector's problem. This strategy therefore minimizes the token count at the cost of more rounds, especially near the end when the probability of failure is quite high. This last point suggests that a reasonable tradeoff between rounds and tokens can be achieved by a strategy which interpolates between the two outlined above: Bet the minimum initially, but transition towards higher bets as the success probability shrinks. But it's not at all clear to me what an 'optimal' strategy is, or even how to properly formulate the tradeoff, and I would appreciate some perspective on how this might best be framed.","The following problem is motivated by a certain aggravating side-quest in an old video game. The side-quest is itself a game which follows a sequence of rounds; each round can be a success or a failure, and the game ends after successes. In its simplest form, we take the probability of failure after successes to be , in which case we recover the coupon collector's problem . (In the original setting the failure of probability is rounded up to the nearest whole percentage less than 100%, e.g., 84.5% rounds to 85% but 99.5% rounds to 99%; I chose not to include this detail for simplicity.) Thus far there is nothing for the player to do besides keep drawing until success. To add a strategic element, we include a base cost for each round (one ""token""). Furthermore, we allow the player to bet multiple tokens: each token past the first reduces the probability of failure by 1%, to a minimum of 0%. Here the analogy with the coupon collector's problem breaks down: drawing multiple coupons can lead to more than one new coupon being drawn, and the probability to get any new coupons would moreover not be a linear function of the number already found. So this is a coupon collector's problem where the probabilities have been 'linearized'. The question is then what strategy to pursue, and this will very much depend on what goal we pursue. If what we want to minimize is the number of rounds, then we should always bet enough tokens to eliminate the chance of failure and thus finish in rounds. But these bets grow linearly and thus the total number of tokens required scales as . On the other hand, we can simply bet the one-token minimum each round. In that case the number of tokens is the same as the number of rounds, which scales as per the coupon collector's problem. This strategy therefore minimizes the token count at the cost of more rounds, especially near the end when the probability of failure is quite high. This last point suggests that a reasonable tradeoff between rounds and tokens can be achieved by a strategy which interpolates between the two outlined above: Bet the minimum initially, but transition towards higher bets as the success probability shrinks. But it's not at all clear to me what an 'optimal' strategy is, or even how to properly formulate the tradeoff, and I would appreciate some perspective on how this might best be framed.",N k k/N N N^2 N\log N,"['probability', 'statistics', 'coupon-collector']"
63,"Find var$(\overline{X_n}^2)$ given that $EX_1=\mu,E(X_1-\mu)^k=\alpha_k$",Find var given that,"(\overline{X_n}^2) EX_1=\mu,E(X_1-\mu)^k=\alpha_k","$X_i\sim F$ are $n$ iid observations. $\overline{X_n}$ is their mean. I want to find var $(\overline{X_n}^2)$ given that $EX_1=\mu,E(X_1-\mu)^k=\alpha_k$ . What I found out are: var $(\overline{X_n})=\frac{\alpha_2}{n}$ var $(\overline{X_n}^2)=E(\overline{X_n}^4)-[E(\overline{X_n}^2)]^2$ $E(\overline{X_n}^2)=\frac{\alpha_2}{n}+\mu^2$ I found a hint that $E[\overline{X}_n^4]={1\over n^4}\sum E[X_i X_j X_k X_\ell]$ but I can't figure how to get this or how to use this. I can also find $E(\overline{X_1}^4)$ in terms of $\alpha_k$ 's and $\mu$ but how do I use these to get $E(\overline{X_n}^4)$ ? Any help?",are iid observations. is their mean. I want to find var given that . What I found out are: var var I found a hint that but I can't figure how to get this or how to use this. I can also find in terms of 's and but how do I use these to get ? Any help?,"X_i\sim F n \overline{X_n} (\overline{X_n}^2) EX_1=\mu,E(X_1-\mu)^k=\alpha_k (\overline{X_n})=\frac{\alpha_2}{n} (\overline{X_n}^2)=E(\overline{X_n}^4)-[E(\overline{X_n}^2)]^2 E(\overline{X_n}^2)=\frac{\alpha_2}{n}+\mu^2 E[\overline{X}_n^4]={1\over n^4}\sum E[X_i X_j X_k X_\ell] E(\overline{X_1}^4) \alpha_k \mu E(\overline{X_n}^4)","['statistics', 'expected-value', 'variance', 'means']"
64,Alternates to the Neyman-Pearson Lemma?,Alternates to the Neyman-Pearson Lemma?,,"I was reading about the Neyman-Pearson Lemma ( https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma ). Let $X_1, X_2, \ldots, X_n$ be a random sample from a distribution with unknown parameter $\theta$ . Consider two hypotheses: Null hypothesis $H_0$ : $\theta = \theta_0$ Alternative hypothesis $H_1$ : $\theta = \theta_1$ ( $\theta_1 \neq \theta_0$ ) Let $f(x|\theta)$ denote the probability density function (or probability mass function) of $X_1, X_2, \ldots, X_n$ . The Neyman-Pearson lemma states that the likelihood ratio test (LRT) is the most powerful test at level $\alpha$ , where the LRT rejects $H_0$ if and only if: $$\frac{L(\theta_1 | \mathbf{x})}{L(\theta_0 | \mathbf{x})} > k,$$ where $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ is the observed sample, $L(\theta|\mathbf{x}) = \prod_{i=1}^n f(x_i|\theta)$ is the likelihood function, and $k$ is a constant chosen such that the test has level $\alpha$ . My Question: This might sound like a rhetorical question - but what other possible tests  could exist? The Neyman-Pearson Lemma is said to be very important as it shows us what is the most powerful statistical test - but it seems to me that the test indicated by the Neyman-Pearson is practically the only test that exists. What other tests can be considered as competitors? Thanks!","I was reading about the Neyman-Pearson Lemma ( https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma ). Let be a random sample from a distribution with unknown parameter . Consider two hypotheses: Null hypothesis : Alternative hypothesis : ( ) Let denote the probability density function (or probability mass function) of . The Neyman-Pearson lemma states that the likelihood ratio test (LRT) is the most powerful test at level , where the LRT rejects if and only if: where is the observed sample, is the likelihood function, and is a constant chosen such that the test has level . My Question: This might sound like a rhetorical question - but what other possible tests  could exist? The Neyman-Pearson Lemma is said to be very important as it shows us what is the most powerful statistical test - but it seems to me that the test indicated by the Neyman-Pearson is practically the only test that exists. What other tests can be considered as competitors? Thanks!","X_1, X_2, \ldots, X_n \theta H_0 \theta = \theta_0 H_1 \theta = \theta_1 \theta_1 \neq \theta_0 f(x|\theta) X_1, X_2, \ldots, X_n \alpha H_0 \frac{L(\theta_1 | \mathbf{x})}{L(\theta_0 | \mathbf{x})} > k, \mathbf{x} = (x_1, x_2, \ldots, x_n) L(\theta|\mathbf{x}) = \prod_{i=1}^n f(x_i|\theta) k \alpha","['probability', 'statistics']"
65,Applicability of Glivenko-Cantelli theorem to functions other than cdf,Applicability of Glivenko-Cantelli theorem to functions other than cdf,,"Glivenko-Cantelli theorem states that approximation function $F_n(t):=\frac{1}{n} \sum_{i=1}^{n} [X_i < t]$ uniformly converges to actual cumulative distribution function $F$ . That is, $\mathrm{sup}_{t} ||F_n(t) - F(t)|| \to 0$ , a.s. Reading through the proof of the theorem, I found that the proof uses only the three properties of $F_n, F$ pair: Property 1: $\forall t, F_n(t) \to F(t)$ , a.s. Property 2: $F$ is bounded function Property 3: $F$ and $F_n$ is monotonically increasing function Thus, I think any function-approx-function-pair $(f_n, f)$ satisfies the all three properties, then Glivenko-Cantelli theorem can also be applied to that pair, and conclude that $\mathrm{sup}_t||f_n(t) - f(t)|| \to 0$ , a.s. Is this correct? Or, is there other important properties that $(f, f_n)$ must have? Proof of Glivenko-Cantelli For your information, here is that proof of Glivenko-Cantelli fetched from Thm. 19.1 of [1] *By the strong law of large numbers, both $F_{n} \to F$ , a.s., and $F_n(t-) \to F(t-)$ , a.s for given $t$ . ( Property 1 used ). Given a fixed $\epsilon > 0$ , there exists a partition $-\infty = t_0 < t_1 < \cdots < t_k = \infty$ such that $F(t_i-)-F(t_{i-1}) < \epsilon$ for every $i$ . ( Property 2 and 3 used ) Now for $t_{i-1}\leq t < t_i$ , $ F_n(t) - F(t) \leq F_n(t_i -) - F(t_i -) + \epsilon$ ( Property 3 used ) $ F_n(t) - F(t) \geq F_n(t_{i-1}) - F(t_{i-1}) - \epsilon$ ( Property 3 used ) The onvergene of $F_n(t)$ and $F_n(t-)$ for every fixed $t$ is certainly uniform for $t$ in the finite set $\{t_1,\ldots, t_n\}$ . Conclude that $\mathrm{limsup} (\mathrm{sup}_{t}||F_n(t) - F(t)|) \leq \epsilon$ almost surely. This is true for every $\epsilon$ and hence the limit superior is zero.* [1] Van der Vaart, Aad W. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.","Glivenko-Cantelli theorem states that approximation function uniformly converges to actual cumulative distribution function . That is, , a.s. Reading through the proof of the theorem, I found that the proof uses only the three properties of pair: Property 1: , a.s. Property 2: is bounded function Property 3: and is monotonically increasing function Thus, I think any function-approx-function-pair satisfies the all three properties, then Glivenko-Cantelli theorem can also be applied to that pair, and conclude that , a.s. Is this correct? Or, is there other important properties that must have? Proof of Glivenko-Cantelli For your information, here is that proof of Glivenko-Cantelli fetched from Thm. 19.1 of [1] *By the strong law of large numbers, both , a.s., and , a.s for given . ( Property 1 used ). Given a fixed , there exists a partition such that for every . ( Property 2 and 3 used ) Now for , ( Property 3 used ) ( Property 3 used ) The onvergene of and for every fixed is certainly uniform for in the finite set . Conclude that almost surely. This is true for every and hence the limit superior is zero.* [1] Van der Vaart, Aad W. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.","F_n(t):=\frac{1}{n} \sum_{i=1}^{n} [X_i < t] F \mathrm{sup}_{t} ||F_n(t) - F(t)|| \to 0 F_n, F \forall t, F_n(t) \to F(t) F F F_n (f_n, f) \mathrm{sup}_t||f_n(t) - f(t)|| \to 0 (f, f_n) F_{n} \to F F_n(t-) \to F(t-) t \epsilon > 0 -\infty = t_0 < t_1 < \cdots < t_k = \infty F(t_i-)-F(t_{i-1}) < \epsilon i t_{i-1}\leq t < t_i  F_n(t) - F(t) \leq F_n(t_i -) - F(t_i -) + \epsilon  F_n(t) - F(t) \geq F_n(t_{i-1}) - F(t_{i-1}) - \epsilon F_n(t) F_n(t-) t t \{t_1,\ldots, t_n\} \mathrm{limsup} (\mathrm{sup}_{t}||F_n(t) - F(t)|) \leq \epsilon \epsilon","['probability-theory', 'statistics', 'probability-limit-theorems']"
66,Is there a rigorous way to show it is not a sufficient statistic?,Is there a rigorous way to show it is not a sufficient statistic?,,"Source of the question: the exercise 6.22(a) of Statistical Inference Book by Casella and Berger. Let $X_1,...,X_n$ be a random sample from a population with pdf $$f(x|\theta)=\theta x^{\theta-1},0<x<1, \theta>0.$$ Is $\Sigma X_i$ sufficient for $\theta$ ? I know how to find a sufficient statistics. I can use either factorization theorem or exponential family way. One sufficient statistics is $\prod_i X_i$ . I also know how to find other sufficient statistics by using any function of $\prod_i X_i$ . But I am considering whether there exists a more rigorous way to conclude $\Sigma X_i$ is not sufficient.",Source of the question: the exercise 6.22(a) of Statistical Inference Book by Casella and Berger. Let be a random sample from a population with pdf Is sufficient for ? I know how to find a sufficient statistics. I can use either factorization theorem or exponential family way. One sufficient statistics is . I also know how to find other sufficient statistics by using any function of . But I am considering whether there exists a more rigorous way to conclude is not sufficient.,"X_1,...,X_n f(x|\theta)=\theta x^{\theta-1},0<x<1, \theta>0. \Sigma X_i \theta \prod_i X_i \prod_i X_i \Sigma X_i","['statistics', 'statistical-inference']"
67,"Is there any book ""economics for mathematicians""? [closed]","Is there any book ""economics for mathematicians""? [closed]",,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed last year . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question If you search economics book there are plenty of them on the market. We may say the say about ""mathematics for economists"" or ""application of mathematics in economics"". But I've never seen book which tells me about economics based on statistics, optimization and other field of mathematics. Where every law, curve and so on is based on some statistical experiment or real analysis. Example: I'm currently reading https://www.amazon.com/Principles-Microeconomics-N-Gregory-Mankiw/dp/1305971493 and the problem here is that there're no proof for principles. Author just says ""people always  have trade-offs"". But from the other hand there's optimization theory, and if we care only about revenue function - we may find maximum. (So in that case we don't have any trade-offs). Let me know, please, if you know any book ""economics for mathematicians"". Thanks! PS. There's similar question on site, but it's outdated. (But have a little different idea) ( Textbooks of economics for mathematicians )","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed last year . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question If you search economics book there are plenty of them on the market. We may say the say about ""mathematics for economists"" or ""application of mathematics in economics"". But I've never seen book which tells me about economics based on statistics, optimization and other field of mathematics. Where every law, curve and so on is based on some statistical experiment or real analysis. Example: I'm currently reading https://www.amazon.com/Principles-Microeconomics-N-Gregory-Mankiw/dp/1305971493 and the problem here is that there're no proof for principles. Author just says ""people always  have trade-offs"". But from the other hand there's optimization theory, and if we care only about revenue function - we may find maximum. (So in that case we don't have any trade-offs). Let me know, please, if you know any book ""economics for mathematicians"". Thanks! PS. There's similar question on site, but it's outdated. (But have a little different idea) ( Textbooks of economics for mathematicians )",,"['statistics', 'book-recommendation', 'economics']"
68,"If an event can happen with probability $p$, but I don't see it in $n$ tries, what is $P(p < p_0)$?","If an event can happen with probability , but I don't see it in  tries, what is ?",p n P(p < p_0),"How can I calculate the following: Suppose I have an event (in my case, a computer bug) that can happen with unknown probability $p$ . I run $n$ experiments and don't see the bug. What is the probability that $p$ is below some fixed value $p_0$ ? (I am not well versed in this type of statistical analysis. From a bit of research I gather this is a Bernoulli process, and I saw some formulas for determining things like the probability of $k$ successes in $n$ runs, but not exactly what I am interested in. This question occurred to me as I am considering how often one should replicate a bug to make sure it is a consistent bug and not a sporadic bug. Or how sure you can be a bug is fixed if you don't see it anymore.)","How can I calculate the following: Suppose I have an event (in my case, a computer bug) that can happen with unknown probability . I run experiments and don't see the bug. What is the probability that is below some fixed value ? (I am not well versed in this type of statistical analysis. From a bit of research I gather this is a Bernoulli process, and I saw some formulas for determining things like the probability of successes in runs, but not exactly what I am interested in. This question occurred to me as I am considering how often one should replicate a bug to make sure it is a consistent bug and not a sporadic bug. Or how sure you can be a bug is fixed if you don't see it anymore.)",p n p p_0 k n,"['probability', 'statistics', 'bernoulli-distribution']"
69,Showing sufficiency of a statistic (the hard way),Showing sufficiency of a statistic (the hard way),,"Given $X_1,\dots,X_n$ i.i.d. from $P_\theta$ , the usual way of showing that a statistic $T$ is sufficient is to use the factorization lemma as opposed to computing the distribution of $(X_1,\dots,X_n)|T$ and showing that it is independent of the parameter of interest $\theta$ . Suppose that $X_1,\dots,X_n$ are i.i.d. samples from the uniform distribution of $(0,\theta)$ . I want to compute the distribution of $X|X_{(n)}$ and show that it is independent of $\theta$ . I am struggling with the rigorous justifications here because $X|X_{(n)}$ does not have a density with respect to Lebesgue measure. What is the correct way to derive the distribution here? The following is an attempt: \begin{align*} &P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t)\\ &= \sum_{i=1}^nP(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) P(X_{(n)}=i|X_{(n)}=t)\\ &= \frac{1}{n } \sum_{i=1}^n P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) \end{align*} where the last equality holds by the symmetry of the problem, each $X_i$ is as likely to be the largest as any of the others. Now from the discussion here , we have that $$ X_1,\dots, X_{i-1}, X_{i+1},\dots, X_n|X_{(n)}=X_i= t $$ is distributed as $n-1$ i.i.d. uniform random variables on $[0,t]$ . So can we then write: \begin{align*} &P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i)\\ &P(X_1\le x_1,\dots, X_{i-1}\le x_{i-1}, X_{i +1}\le x_{i+1} \le \dots \le X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) \times \mathbf{1}\{x_i = t= X_{(n)}\}\\ &= \mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \}. \end{align*} Plugging this back in to the above expression yields \begin{align*} P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t) &= \frac{1}{n } \sum_{i=1}^n  \mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \} \end{align*} I'm not completely convinced by the derivation, and I'm not sure what to do next. Intuitively the first indicator in the expression will only be equal to 1 for one of the indices in $i=1,\dots, n$ .","Given i.i.d. from , the usual way of showing that a statistic is sufficient is to use the factorization lemma as opposed to computing the distribution of and showing that it is independent of the parameter of interest . Suppose that are i.i.d. samples from the uniform distribution of . I want to compute the distribution of and show that it is independent of . I am struggling with the rigorous justifications here because does not have a density with respect to Lebesgue measure. What is the correct way to derive the distribution here? The following is an attempt: where the last equality holds by the symmetry of the problem, each is as likely to be the largest as any of the others. Now from the discussion here , we have that is distributed as i.i.d. uniform random variables on . So can we then write: Plugging this back in to the above expression yields I'm not completely convinced by the derivation, and I'm not sure what to do next. Intuitively the first indicator in the expression will only be equal to 1 for one of the indices in .","X_1,\dots,X_n P_\theta T (X_1,\dots,X_n)|T \theta X_1,\dots,X_n (0,\theta) X|X_{(n)} \theta X|X_{(n)} \begin{align*}
&P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t)\\
&=
\sum_{i=1}^nP(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) P(X_{(n)}=i|X_{(n)}=t)\\
&=
\frac{1}{n }
\sum_{i=1}^n P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i)
\end{align*} X_i 
X_1,\dots, X_{i-1}, X_{i+1},\dots, X_n|X_{(n)}=X_i= t
 n-1 [0,t] \begin{align*}
&P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i)\\
&P(X_1\le x_1,\dots, X_{i-1}\le x_{i-1}, X_{i +1}\le x_{i+1} \le \dots \le X_n \le x_n|X_{(n)}=t, X_{(n)}=X_i) \times \mathbf{1}\{x_i = t= X_{(n)}\}\\
&= \mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \}.
\end{align*} \begin{align*}
P(X_1\le x_1,\dots, X_n \le x_n|X_{(n)}=t)
&=
\frac{1}{n }
\sum_{i=1}^n 
\mathbf{1}\{x_i = t= X_{(n)}\} \prod_{j \neq i} \frac{x_j}{\theta} \mathbf{1}\{ 0 \le x_j \le t \}
\end{align*} i=1,\dots, n","['probability', 'statistics', 'probability-distributions', 'statistical-inference']"
70,Negative values in Poisson process,Negative values in Poisson process,,"I'm trying to make some sense of the following definition: A collection of random variables $\{ N_t \}$$_{t \geq 0}$ is called a Poisson process with rate parameter $\lambda > 0$ if $N_0 = 0$ $N_{t_2} - N_{t_1}$ and $N_{t_4} - N_{t_3}$ are independent for all $0 \leq t_1 < t_2 \leq t_3 < t_4$ For all $t \geq 0$ and $h > 0$ , we have $P(N_{t+h} - N_t = 1) = \lambda h + o(h)$ and $P(N_{t+h} - N_t \geq 2) = o(h)$ Now my question is, does this definition imply that $N_t \geq 0$ for all $t$ ? I know that this must obviously be true since they represent counts, but how do you actually get this from the definition?","I'm trying to make some sense of the following definition: A collection of random variables is called a Poisson process with rate parameter if and are independent for all For all and , we have and Now my question is, does this definition imply that for all ? I know that this must obviously be true since they represent counts, but how do you actually get this from the definition?",\{ N_t \}_{t \geq 0} \lambda > 0 N_0 = 0 N_{t_2} - N_{t_1} N_{t_4} - N_{t_3} 0 \leq t_1 < t_2 \leq t_3 < t_4 t \geq 0 h > 0 P(N_{t+h} - N_t = 1) = \lambda h + o(h) P(N_{t+h} - N_t \geq 2) = o(h) N_t \geq 0 t,"['probability-theory', 'statistics', 'stochastic-processes', 'random-variables', 'poisson-process']"
71,Is this result correct?,Is this result correct?,,"I have the following property of summation $e^{x}=\sum_{k=0}^{∞}\frac{x^{k}}{k!}$ , then $$\sum _{k=61} ^{\infty} \frac{e^{-66}\left(66\right)^{k}}{k!} = \frac{1}{e^{66}} \sum_{k=61} ^{\infty} \frac{\left(66\right)^{k}}{k!}=\frac{e^{66}}{e^{66}}=1$$ The correct answer is supposed to be $0.747$ but I have not found a structure or a recursive method to know where the result comes from. Thank you very much for anyone who can give me a good indication.","I have the following property of summation , then The correct answer is supposed to be but I have not found a structure or a recursive method to know where the result comes from. Thank you very much for anyone who can give me a good indication.",e^{x}=\sum_{k=0}^{∞}\frac{x^{k}}{k!} \sum _{k=61} ^{\infty} \frac{e^{-66}\left(66\right)^{k}}{k!} = \frac{1}{e^{66}} \sum_{k=61} ^{\infty} \frac{\left(66\right)^{k}}{k!}=\frac{e^{66}}{e^{66}}=1 0.747,"['probability', 'analysis', 'statistics', 'discrete-mathematics', 'summation']"
72,A problem from Mathematical Statistics and Data Analysis [closed],A problem from Mathematical Statistics and Data Analysis [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question This problem is from Ch. 5, Mathematical Statistics and Data Analysis, 3rd edition, stated as follows, In addition to limit theorems that deal with sums, there are limit theorems that deal with extreme values such as maxima or minima. Here is an example. Let $U_1, \ldots, U_n$ be independent uniform random variables on $[0,1]$ , and let $U_{(n)}$ be the maximum. Find the cdf of $U_{(n)}$ and a standardized $U_{(n)}$ , and show that the cdf of the standardized variable tends to a limiting value. Below is my derivation. The cdf of $U_{(n)}$ is as follows, $$ F_{(n)} = \left\{     \begin{array}{lr}         x^n, & \text{if } x \in [0,1]\\         0, & \text{otherwise}     \end{array} \right\} $$ and pdf $$ f_{(n)} = \left\{     \begin{array}{lr}         nx^{n-1}, & \text{if } x \in [0,1]\\         0, & \text{otherwise}     \end{array} \right\} $$ With pdf we can calculate mean and variance, which is $$ E(U_{(n)}) = \frac{n}{n+1}, \text{Var}(U_{(n)}) = \frac{n}{(n+1)^2(n+2)} $$ The standardized variable $Z_n$ is $$ Z_{n} = \frac{U_{(n)}-\frac{n}{n+1}}{\sqrt{\frac{n}{(n+1)^2(n+2)}}} $$ However, the key is quite different from mine. The standardized variable it gives is $Z_n = n(U_{(n)}-1)$ . Did I miss something? Many thanks!","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question This problem is from Ch. 5, Mathematical Statistics and Data Analysis, 3rd edition, stated as follows, In addition to limit theorems that deal with sums, there are limit theorems that deal with extreme values such as maxima or minima. Here is an example. Let be independent uniform random variables on , and let be the maximum. Find the cdf of and a standardized , and show that the cdf of the standardized variable tends to a limiting value. Below is my derivation. The cdf of is as follows, and pdf With pdf we can calculate mean and variance, which is The standardized variable is However, the key is quite different from mine. The standardized variable it gives is . Did I miss something? Many thanks!","U_1, \ldots, U_n [0,1] U_{(n)} U_{(n)} U_{(n)} U_{(n)} 
F_{(n)} = \left\{
    \begin{array}{lr}
        x^n, & \text{if } x \in [0,1]\\
        0, & \text{otherwise}
    \end{array}
\right\}
 
f_{(n)} = \left\{
    \begin{array}{lr}
        nx^{n-1}, & \text{if } x \in [0,1]\\
        0, & \text{otherwise}
    \end{array}
\right\}
 
E(U_{(n)}) = \frac{n}{n+1}, \text{Var}(U_{(n)}) = \frac{n}{(n+1)^2(n+2)}
 Z_n 
Z_{n} = \frac{U_{(n)}-\frac{n}{n+1}}{\sqrt{\frac{n}{(n+1)^2(n+2)}}}
 Z_n = n(U_{(n)}-1)","['probability', 'statistics', 'order-statistics']"
73,How to measure the “Skewness” of a Probability Distribution?,How to measure the “Skewness” of a Probability Distribution?,,"I looked at the ""Fisher Measure"" formula for ""skewness of a probability distribution"" ( https://en.wikipedia.org/wiki/Skewness ) - this is related to the expectation of the third moment for some transformed probability distribution. How does this seemingly arbitrary formula using the expectation of the third moment specifically measure how skewed the probability distribution is? In other words, what is the motivation behind it? Why does this not incorporate, for example, the second or fourth moment? Why even involve moments in this calculation to characterize the skewness of a distribution?","I looked at the ""Fisher Measure"" formula for ""skewness of a probability distribution"" ( https://en.wikipedia.org/wiki/Skewness ) - this is related to the expectation of the third moment for some transformed probability distribution. How does this seemingly arbitrary formula using the expectation of the third moment specifically measure how skewed the probability distribution is? In other words, what is the motivation behind it? Why does this not incorporate, for example, the second or fourth moment? Why even involve moments in this calculation to characterize the skewness of a distribution?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
74,Does maximum likelihood estimation maximize probability really?,Does maximum likelihood estimation maximize probability really?,,"In maximum likelihood estimation (for continuous rv) we would like to maximize some parameter of our density function such that at some fixed observations our density function is bigger or equal for same density function bur for all other parameters. I would like to say that if we maximize the likelihood function, the probability that our observations occur is maximized as well. But, I can’t really produce a proof of this (I can’t convince myself that around each sample point I can maximize its probability by maximizing the density function). If someone could explain this I would appreciate it","In maximum likelihood estimation (for continuous rv) we would like to maximize some parameter of our density function such that at some fixed observations our density function is bigger or equal for same density function bur for all other parameters. I would like to say that if we maximize the likelihood function, the probability that our observations occur is maximized as well. But, I can’t really produce a proof of this (I can’t convince myself that around each sample point I can maximize its probability by maximizing the density function). If someone could explain this I would appreciate it",,"['probability', 'probability-theory', 'statistics', 'approximation', 'parameter-estimation']"
75,Convergence of a confidence interval for the variance,Convergence of a confidence interval for the variance,,"It's known that a confidence interval for the variance with $1-\alpha$ confidence is as it follows $$\sigma^2\in\Biggl(\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,\alpha/2}},\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,1-\alpha/2}}\Bigg)=(A_{n},B_{n}). $$ I'm asked to compute $$\lim_{n \to \infty}P\Bigl(\sigma^2\in(A_{n},B_{n})\Bigr),$$ taking into account that $X_{1},\dots,X_{n},\dots,$ are i.i.d. (but no necessarily normal) and $\mathbb{E}(X_{1}^{4})<\infty$ . I know that $\sqrt{n} (S_n^2 - \sigma^2) \rightarrow_{d} \text{N}(0, \sigma^4 (\kappa - 1)),$ where $\kappa=\mu_{4}/\sigma^{4}$ and $\mu_4 = E(X_i -\mu)^4$ , but I have no clue in how to continue. Could you give me some ideas?","It's known that a confidence interval for the variance with confidence is as it follows I'm asked to compute taking into account that are i.i.d. (but no necessarily normal) and . I know that where and , but I have no clue in how to continue. Could you give me some ideas?","1-\alpha \sigma^2\in\Biggl(\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,\alpha/2}},\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,1-\alpha/2}}\Bigg)=(A_{n},B_{n}).  \lim_{n \to \infty}P\Bigl(\sigma^2\in(A_{n},B_{n})\Bigr), X_{1},\dots,X_{n},\dots, \mathbb{E}(X_{1}^{4})<\infty \sqrt{n} (S_n^2 - \sigma^2) \rightarrow_{d} \text{N}(0, \sigma^4 (\kappa - 1)), \kappa=\mu_{4}/\sigma^{4} \mu_4 = E(X_i -\mu)^4","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
76,Markov Chains: From Theory to Application,Markov Chains: From Theory to Application,,"A question that I have always wondered about is that how are the Transition Probabilities within a Markov Chain estimated in real-world applications? I tried to learn more about this online and found the following link ( https://hesim-dev.github.io/hesim/articles/mlogit.html ). Over here, the following equation and explanation is provided: Procedurally, I was trying to understand how this works. Suppose there is a dataset with 3 States (State A, State B, State C). Each row in this dataset is an individual medical patient and contains some information on their covariates (e.g. height, weight, blood pressure, etc.), the state they were in and the state they transitioned to (let's assume that transitions can only take place at fixed discrete times, e.g. the first of January) : patient_id initial_state final_state   height age   weight 1          1             A           B 147.9283  49 85.03746 2          2             B           B 147.6188  50 98.37848 3          3             A           C 146.0570  51 87.79418 4          4             C           A 147.0269  56 86.38467 5          5             C           A 158.2545  47 83.81863 Suppose we want to fit a DISCRETE TIME MARKOV CHAIN to this data and estimate the transition probabilities - My understanding is the following: Isolate a subset of all rows where the initial state was State = State A Fit a Multinomial Logistic Regression to this subset of rows - doing this will provide you with general equations to calculate the probability of anyone within the population transitioning to any of the 3 States based on their covariate vector. This will also tell you the effect of different covariates on the transition probability and if these effects were statistically significant Repeat these two steps from the other two states (i.e. isolate the subset where initial state = State B, etc.) and fit a Multinomial Logistic Regression. In the end, you will have a 3 x 3 transition matrix which equations (as provided above) that estimate the transition probabilities based on a given vector of covariates Based on these transition probabilities, you can now perform standard calculations as is done with Markov Chains - for example, given an initial probability distribution vector, what is the probability that this Markov Chain will be State B after ""k"" iterations? I am not sure if in this transition matrix I described above, transition probabilities within a given row will sum to 1? Is my understanding of the above correct? Thanks!","A question that I have always wondered about is that how are the Transition Probabilities within a Markov Chain estimated in real-world applications? I tried to learn more about this online and found the following link ( https://hesim-dev.github.io/hesim/articles/mlogit.html ). Over here, the following equation and explanation is provided: Procedurally, I was trying to understand how this works. Suppose there is a dataset with 3 States (State A, State B, State C). Each row in this dataset is an individual medical patient and contains some information on their covariates (e.g. height, weight, blood pressure, etc.), the state they were in and the state they transitioned to (let's assume that transitions can only take place at fixed discrete times, e.g. the first of January) : patient_id initial_state final_state   height age   weight 1          1             A           B 147.9283  49 85.03746 2          2             B           B 147.6188  50 98.37848 3          3             A           C 146.0570  51 87.79418 4          4             C           A 147.0269  56 86.38467 5          5             C           A 158.2545  47 83.81863 Suppose we want to fit a DISCRETE TIME MARKOV CHAIN to this data and estimate the transition probabilities - My understanding is the following: Isolate a subset of all rows where the initial state was State = State A Fit a Multinomial Logistic Regression to this subset of rows - doing this will provide you with general equations to calculate the probability of anyone within the population transitioning to any of the 3 States based on their covariate vector. This will also tell you the effect of different covariates on the transition probability and if these effects were statistically significant Repeat these two steps from the other two states (i.e. isolate the subset where initial state = State B, etc.) and fit a Multinomial Logistic Regression. In the end, you will have a 3 x 3 transition matrix which equations (as provided above) that estimate the transition probabilities based on a given vector of covariates Based on these transition probabilities, you can now perform standard calculations as is done with Markov Chains - for example, given an initial probability distribution vector, what is the probability that this Markov Chain will be State B after ""k"" iterations? I am not sure if in this transition matrix I described above, transition probabilities within a given row will sum to 1? Is my understanding of the above correct? Thanks!",,"['probability', 'statistics']"
77,"Finding a pro disc golfer's chance of winning a tournament, knowing their chance of beating each other player","Finding a pro disc golfer's chance of winning a tournament, knowing their chance of beating each other player",,"I'm working on a strength of field metric for disc golf tournaments, and I'd first like to come up with a way to determine a player's likelihood of winning an event, given their player rating and the ratings of other players in the event. For anyone not familiar (probably most reading this), competitive disc golf is played very similar to golf. The competitors play a set number of rounds on the tournament course, and the player with the lowest stroke total at the end of the event wins. The governing body calculates player ratings based on performance at these competitive events, which allows any player with enough rated rounds to be compared to another player, even if they haven't competed directly. I compiled data from every tournament in the 2022 disc golf pro tour, including the entrants, their finishes, and their ratings at the start of the event. I separated every player at every tournament into buckets of 5 ratings points each, and for all players at all tournaments calculated the probability of an average player of rating x beating an average player of rating y (beating defined simply as placing higher/scoring lower in the tournament). For instance, a player who enters a tournament with a rating between 1040-1045 has a 33% chance of placing higher than a 1045-1050 rated player - but a 74% chance of placing higher than a player rated 1025-1030. My thought was that I could use this matrix to find a player's likelihood of winning a tournament. Once I know their likelihood of placing higher than each other player in the event individually, I can multiply all of those probabilities to find the chance of that player finishing higher than every other player at the event. These should all be independent events I assume, as each player is playing against the course - their individual scores have no effect on one another's scores. However, after running this calculation, the resulting event-win probabilities I get are way too low. They do make sense directionally - higher rated players in my sample had higher win probabilities - but they are still far too low. For example, one of these events had 131 entrants - so if I were to assume every player entered had the same skill, they should all have about a 0.8% chance of winning. In my calculation, with skill difference taken into account, the highest rated player at the event had a .0004% likelihood of winning. I thought floating point error from my probability matrix might be to blame, so I tried rounding any value below .005 in the matrix to 0. This did not turn out to be the source of the issue however. Am I missing something in my approach, or my assumptions? I'm a bit of an amateur at all of this, so I wouldn't be surprised if the math just doesn't work out the way I think it does. Thanks!","I'm working on a strength of field metric for disc golf tournaments, and I'd first like to come up with a way to determine a player's likelihood of winning an event, given their player rating and the ratings of other players in the event. For anyone not familiar (probably most reading this), competitive disc golf is played very similar to golf. The competitors play a set number of rounds on the tournament course, and the player with the lowest stroke total at the end of the event wins. The governing body calculates player ratings based on performance at these competitive events, which allows any player with enough rated rounds to be compared to another player, even if they haven't competed directly. I compiled data from every tournament in the 2022 disc golf pro tour, including the entrants, their finishes, and their ratings at the start of the event. I separated every player at every tournament into buckets of 5 ratings points each, and for all players at all tournaments calculated the probability of an average player of rating x beating an average player of rating y (beating defined simply as placing higher/scoring lower in the tournament). For instance, a player who enters a tournament with a rating between 1040-1045 has a 33% chance of placing higher than a 1045-1050 rated player - but a 74% chance of placing higher than a player rated 1025-1030. My thought was that I could use this matrix to find a player's likelihood of winning a tournament. Once I know their likelihood of placing higher than each other player in the event individually, I can multiply all of those probabilities to find the chance of that player finishing higher than every other player at the event. These should all be independent events I assume, as each player is playing against the course - their individual scores have no effect on one another's scores. However, after running this calculation, the resulting event-win probabilities I get are way too low. They do make sense directionally - higher rated players in my sample had higher win probabilities - but they are still far too low. For example, one of these events had 131 entrants - so if I were to assume every player entered had the same skill, they should all have about a 0.8% chance of winning. In my calculation, with skill difference taken into account, the highest rated player at the event had a .0004% likelihood of winning. I thought floating point error from my probability matrix might be to blame, so I tried rounding any value below .005 in the matrix to 0. This did not turn out to be the source of the issue however. Am I missing something in my approach, or my assumptions? I'm a bit of an amateur at all of this, so I wouldn't be surprised if the math just doesn't work out the way I think it does. Thanks!",,"['probability', 'statistics']"
78,4th moment of the sample mean estimator,4th moment of the sample mean estimator,,"I got a following setup: $(X_i)_{i \geq 1}$ are iid random variables with values in $\mathbb{R}$ and finite second moment. By the weak law of large numbers: $\sqrt{n}(\bar{X} - E(X))$ converges in distribution to $N(0, Var(X))$ , so we could deploy the delta method with a transformation $g(x) = x^4$ , implying that $\sqrt{n}g(\bar{X} - E(X))$ should be normally distributed. I was trying to convince myself about it: $$ \sqrt{n}g(\bar{X} - E(X)) = \sqrt{n}(\bar{X} - E(X))^4 = \sqrt{n}(Var(\bar{X}))^2 = \sqrt{n} \text{Var}(X)^2 \to \infty \text{ for } n \to \infty$$ so $\sqrt{n}g(\bar{X} - E(X))$ can't be normally distributed Am I right? If I am, why does the delta method doesn't work in this case? Thanks!","I got a following setup: are iid random variables with values in and finite second moment. By the weak law of large numbers: converges in distribution to , so we could deploy the delta method with a transformation , implying that should be normally distributed. I was trying to convince myself about it: so can't be normally distributed Am I right? If I am, why does the delta method doesn't work in this case? Thanks!","(X_i)_{i \geq 1} \mathbb{R} \sqrt{n}(\bar{X} - E(X)) N(0, Var(X)) g(x) = x^4 \sqrt{n}g(\bar{X} - E(X)) 
\sqrt{n}g(\bar{X} - E(X)) = \sqrt{n}(\bar{X} - E(X))^4 = \sqrt{n}(Var(\bar{X}))^2 = \sqrt{n} \text{Var}(X)^2 \to \infty \text{ for } n \to \infty \sqrt{n}g(\bar{X} - E(X))","['probability', 'statistics', 'asymptotics', 'delta-method']"
79,Weighted Mean of 3 Measurements: Too Little Information?,Weighted Mean of 3 Measurements: Too Little Information?,,"Suppose I only have the following summary data: ""City A"" has an adult population of ""n1"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""x dollars"" ""City B"" has an adult population of ""n2"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""y dollars"" ""City C"" has an adult population of ""n3"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""z dollars"" Based on only this information, I want to estimate the average income of all 3 cities and the standard deviation. Originally, I had thought that the ""Weighted Mean"" ( https://en.wikipedia.org/wiki/Weighted_arithmetic_mean ) was a good approach for this problem. That is, cities with larger populations should have more of an influence on the final estimate, and cities with smaller populations should have less of an influence on the final estimate. I could then calculate the standard deviation as well. I started looking at references to perform this calculation (e.g. http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf), and it appears that individual measurements might be required for this calculation. For example, I would need to have the income of every person interviewed in City A, City B and City C. However, I am only provided with the average income from each of these cities - and furthermore, I am not even provided with the standard deviation of these averages. In such a problem, does it still make sense to calculated the Weighted Mean on essentially three measurements? Or in such cases, is it better to refrain from calculating any statistics, seeing that any estimate generated in such a context is likely to be inherently flawed? Note: these 3 measurements I am provided with are themselves ""means"" of other measurements - I am not sure if this further complicates things. E.g. since I am not provided with the variance of each individual mean - will it still be published to calculate the variance of the weighted mean?","Suppose I only have the following summary data: ""City A"" has an adult population of ""n1"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""x dollars"" ""City B"" has an adult population of ""n2"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""y dollars"" ""City C"" has an adult population of ""n3"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""z dollars"" Based on only this information, I want to estimate the average income of all 3 cities and the standard deviation. Originally, I had thought that the ""Weighted Mean"" ( https://en.wikipedia.org/wiki/Weighted_arithmetic_mean ) was a good approach for this problem. That is, cities with larger populations should have more of an influence on the final estimate, and cities with smaller populations should have less of an influence on the final estimate. I could then calculate the standard deviation as well. I started looking at references to perform this calculation (e.g. http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf), and it appears that individual measurements might be required for this calculation. For example, I would need to have the income of every person interviewed in City A, City B and City C. However, I am only provided with the average income from each of these cities - and furthermore, I am not even provided with the standard deviation of these averages. In such a problem, does it still make sense to calculated the Weighted Mean on essentially three measurements? Or in such cases, is it better to refrain from calculating any statistics, seeing that any estimate generated in such a context is likely to be inherently flawed? Note: these 3 measurements I am provided with are themselves ""means"" of other measurements - I am not sure if this further complicates things. E.g. since I am not provided with the variance of each individual mean - will it still be published to calculate the variance of the weighted mean?",,"['probability', 'statistics']"
80,Link between sum of squares and sum of absolutes,Link between sum of squares and sum of absolutes,,"If I have a real-valued vector $\mathbf{x} = (x_1, \cdots, x_p)$ . Is there a way I can obtain $\sum_{i=1}^p \left|x_i \right|$ from $\sum_{i=1}^p \left|x_i \right|^2$ ?",If I have a real-valued vector . Is there a way I can obtain from ?,"\mathbf{x} = (x_1, \cdots, x_p) \sum_{i=1}^p \left|x_i \right| \sum_{i=1}^p \left|x_i \right|^2","['real-analysis', 'linear-algebra', 'statistics', 'vector-spaces', 'vectors']"
81,Properties of Moment Functions,Properties of Moment Functions,,"I was watching this video here https://www.youtube.com/watch?v=ZLJqjiI0aHM/ From 5:00 - 5:20, the author of this video states that the "" variance of higher order moments tends to increase"". However, no real explanation in the video is provided as to why this is. I tried to search online but I could not find any explanation behind this. For instance, how can I be mathematically certain that the variance of higher order moments does in fact increase? Is there some mathematical proof for this? Thanks! Note: The only reason (non mathematical) I could think of is that since the Variance is ""additive"" - perhaps higher order moments have more terms and functions with more terms will likely have larger variances ... but I am not sure if this is the case.","I was watching this video here https://www.youtube.com/watch?v=ZLJqjiI0aHM/ From 5:00 - 5:20, the author of this video states that the "" variance of higher order moments tends to increase"". However, no real explanation in the video is provided as to why this is. I tried to search online but I could not find any explanation behind this. For instance, how can I be mathematically certain that the variance of higher order moments does in fact increase? Is there some mathematical proof for this? Thanks! Note: The only reason (non mathematical) I could think of is that since the Variance is ""additive"" - perhaps higher order moments have more terms and functions with more terms will likely have larger variances ... but I am not sure if this is the case.",,"['probability', 'statistics']"
82,Probability of the intersection of the complement of two events,Probability of the intersection of the complement of two events,,"Suppose that $A$ and $B$ are events for which $P(A)=x, P(B)=y$ , and $P(A\cap B)=z.$ Express the following probability in terms of $x, y, \text{and} \,z$ . $P(\bar A\cup \bar B$ ). Answer: $1-z$ . This is the Exercise 1.16 from (Paul Meyer, ""Introductory Probability and Statistical Applications"", 2ed) My attempt: $P(\bar A\cup \bar B) = P(\bar A) + P(\bar B) - P(\bar A\cap \bar B)$ $\qquad=1-x +1-y-[(1-x)(1-y)]$ $\qquad=2-x-y-[1-y-x+xy]$ $\qquad=1-xy$ What was my mistake here? I don't get why $z$ appeared in the answer since we are taking the interjection between the complement events of $A$ and $B$ .","Suppose that and are events for which , and Express the following probability in terms of . ). Answer: . This is the Exercise 1.16 from (Paul Meyer, ""Introductory Probability and Statistical Applications"", 2ed) My attempt: What was my mistake here? I don't get why appeared in the answer since we are taking the interjection between the complement events of and .","A B P(A)=x, P(B)=y P(A\cap B)=z. x, y, \text{and} \,z P(\bar A\cup \bar B 1-z P(\bar A\cup \bar B) = P(\bar A) + P(\bar B) - P(\bar A\cap \bar B) \qquad=1-x +1-y-[(1-x)(1-y)] \qquad=2-x-y-[1-y-x+xy] \qquad=1-xy z A B","['probability', 'statistics']"
83,Can we get the concentration inequality of the inner product of two unit vectors distributed on the sphere?,Can we get the concentration inequality of the inner product of two unit vectors distributed on the sphere?,,"Let $u$ and $v$ be two random vectors on $R^n$ that are independent and uniformly distributed on the unit sphere. That means we can represent it as Gaussian random vectors $g\sim N(0, I_n)$ , $$u=\frac{g}{\|g\|_2}. $$ But do we have the concentration inequality of the $u\cdot v$ ? That looks like $$P(u\cdot v\le x)\ge 1-Ce^{-c x}$$","Let and be two random vectors on that are independent and uniformly distributed on the unit sphere. That means we can represent it as Gaussian random vectors , But do we have the concentration inequality of the ? That looks like","u v R^n g\sim N(0, I_n) u=\frac{g}{\|g\|_2}.
 u\cdot v P(u\cdot v\le x)\ge 1-Ce^{-c x}","['probability', 'analysis', 'statistics']"
84,"Correlation, Linear Regression, and Minimizing Cost","Correlation, Linear Regression, and Minimizing Cost",,"If $X$ and $Y$ are random variables with correlation coefficient $\rho$ , then linear regression tells us that, is we wish to minimize the mean square error, then the best linear approximation $\hat{Y}$ (resp $\hat{X}$ ) of $Y$ ( $X$ ) as a function of $X$ ( $Y$ ) satisfies $$\frac{\hat{Y}-\mu_Y}{\sigma_Y} = \rho\frac{X-\mu_X}{\sigma_X},$$ $$\frac{\hat{X}-\mu_X}{\sigma_X} = \rho\frac{Y-\mu_Y}{\sigma_Y}.$$ I want to highlight here that in BOTH cases, the slope (as a function of the normalized variables) is $\rho$ , which reflects the fact that $\rho$ is a symmetric function of $X$ and $Y$ . Now, if someone were to ask me for the equation of the line of best fit between $X$ and $Y$ , WITHOUT mentioning that we want to minimize mean squared error, my naive guess would be that whatever slope the line of best fit has for $Y$ vs $X$ , the line of best fit for $X$ vs $Y$ should have the inverse slope, that is (assuming $X$ and $Y$ are normalized for simplicity): $$\hat{Y} = aX \quad \iff \quad \hat{X} = \frac{1}{a}Y,$$ with the case $a=0$ being left undetermined. This was my first naive attempt, but after some reflection I realized why this doesn't need to be the case, both on a conceptual level and on an algebraic level. Basically, my confusion was resting on the following logic: $$(\hat{Y} = \rho X \quad \& \quad \hat{Y} \approx Y) \implies  Y \approx \rho X \iff X \approx \frac{1}{\rho}Y \implies \hat{X} = \frac{1}{\rho}Y,$$ and written this way it is clear that the mistake is at the $\hat{Y} \approx Y$ step. Now, I'm saying all of this to finally get to the following question: is there a (natural, useful) way of defining $\hat{X}$ and $\hat{Y}$ so that they do satisfy the naive logic I presented? Perhaps by changing the cost function from mean squared error to something else, and/or by replacing the correlation coefficient by some other quantity $\rho'$ satisfying $\rho'(X,Y) = \frac{1}{\rho'(Y,X)}$ ? $\textbf{tldr:}$ Can we reformulate linear regression to make $\hat{Y} = \rho{X} \iff \hat{X} = \frac{1}{\rho}Y$ ?","If and are random variables with correlation coefficient , then linear regression tells us that, is we wish to minimize the mean square error, then the best linear approximation (resp ) of ( ) as a function of ( ) satisfies I want to highlight here that in BOTH cases, the slope (as a function of the normalized variables) is , which reflects the fact that is a symmetric function of and . Now, if someone were to ask me for the equation of the line of best fit between and , WITHOUT mentioning that we want to minimize mean squared error, my naive guess would be that whatever slope the line of best fit has for vs , the line of best fit for vs should have the inverse slope, that is (assuming and are normalized for simplicity): with the case being left undetermined. This was my first naive attempt, but after some reflection I realized why this doesn't need to be the case, both on a conceptual level and on an algebraic level. Basically, my confusion was resting on the following logic: and written this way it is clear that the mistake is at the step. Now, I'm saying all of this to finally get to the following question: is there a (natural, useful) way of defining and so that they do satisfy the naive logic I presented? Perhaps by changing the cost function from mean squared error to something else, and/or by replacing the correlation coefficient by some other quantity satisfying ? Can we reformulate linear regression to make ?","X Y \rho \hat{Y} \hat{X} Y X X Y \frac{\hat{Y}-\mu_Y}{\sigma_Y} = \rho\frac{X-\mu_X}{\sigma_X}, \frac{\hat{X}-\mu_X}{\sigma_X} = \rho\frac{Y-\mu_Y}{\sigma_Y}. \rho \rho X Y X Y Y X X Y X Y \hat{Y} = aX \quad \iff \quad \hat{X} = \frac{1}{a}Y, a=0 (\hat{Y} = \rho X \quad \& \quad \hat{Y} \approx Y) \implies  Y \approx \rho X \iff X \approx \frac{1}{\rho}Y \implies \hat{X} = \frac{1}{\rho}Y, \hat{Y} \approx Y \hat{X} \hat{Y} \rho' \rho'(X,Y) = \frac{1}{\rho'(Y,X)} \textbf{tldr:} \hat{Y} = \rho{X} \iff \hat{X} = \frac{1}{\rho}Y","['probability', 'statistics', 'correlation', 'linear-regression', 'mean-square-error']"
85,Rough path expected signature vs cumulant-generating function / characteristic function,Rough path expected signature vs cumulant-generating function / characteristic function,,"What is the point of using rough path expected signature to characterize the law of а stochastic process when the cumulant generating function is known ( $\log\mathbb{E}[e^{i\theta X(t)}]$ )? Since an average of group-like elements is not a group-like element, an expected signature (of a sample of paths) is not a signature, which seems to imply that expected signatures do not enjoy a nice geometric interpretation as signatures do. It seems that two main applications of expected signatures are the characterization of the law of a process (if moments completely describe the distribution) and the approximation of continuous functions on a collection of paths as linear functions on the expected signature of that collection of paths. If one doesn't need the latter use case, how is the description of a stochastic processes in terms of expected signatures qualitatively different from the classic characteristic function description? Is there something else that expected signatures can tell besides the moments? For example, in case of Levy processes, characteristic function of a process has some very useful information about the process besides moments in light of Levy–Khintchine representation. It tells what parts this Levy process is composed of, namely its linear drift, Brownian motion, and Lévy jump process. By expected signature I mean $ \Big( \mathbb{E} \Big[ \int dX^{\otimes n} \Big]  \Big)_{n\geq 0} = \Big( \mathbb{E} \Big[\int_0^T \int_0^{r_n} \cdots \int_0^{r_2} dX_{r_1} \otimes \cdots  \otimes dX_{r_n} \Big]\Big)_{n\geq 0} $ described in [LM22] (Chapters 6 and 13), [CO22] , [LSDBL21] , [BDMN20] . END OF THE ORIGINAL POST EDIT: Reply to JeremyFR Thank you, @JeremyFR, for your response! However, I still struggle to see advantages of expected signatures vis-à-vis CGF when applied to common 1-D Levy processes. Let us start with Brownian motion. Let B be a standard one-dimensional Brownian motion calculated up to some fixed time T > 0. Then B has the following expected signature (Fawcett’s formula): \begin{align*} 	\mathbb{E}[S(B_{[0,T]})] &= \exp(\frac{T}{2}  e_i \otimes e_i ) \\        &= 1+ \frac{T}{2} e_i ^ {\otimes 2}   +    \frac{1}{2!} \frac{T^2}{4} e_i ^ {\otimes 4}  + \dots  \end{align*} It is also known that it’s moment-generating function is \begin{align*} 	M_{B_T}(\theta) &= \exp( \frac{T}{2}  \theta^2) \\ 		             &= 1 + \frac{T}{2} \theta^2 + \frac{1}{2!} \frac{T^2}{4} \theta^4 + \dots \end{align*} It seems that both expressions carry identical information once we take apart the notation. In the case of Brownian motion, it seems that one hardly can argue that one approach is better than the other because the results are semantically equal. Now let’s move on to a more general case of d-dimensional Lévy process with triplet (a, b, K). It is shown in [FS14] (Theorem 43), that the expected signature of such process is awfully similar to Lévy–Kintchine formula, \begin{align*} \mathbb{E}[S(X)_{[0,T]}]  =\exp\Big [  T \Big (b + \frac{1}{2} a + \int (exp(y) – 1 – y 1 _{|y| < 1}) K(dy) \Big ) \Big] \in T((\mathbb{R}^d)). \end{align*} In light of this result, one cannot help but wonder what is the point of using expected signatures for time series analysis from the practitioner’s perspective in applications involving common types of time series, which are usually 1D Levy Processes, that frequently occur in finance and energy / load research, given that the information in expected signature is essentially identical to the one provided by characteristic function? Granted, there are more exotic cases that require more general expression for expected signature [FHT21] but that hardly looks like something arising frequently in day-to-day applications.","What is the point of using rough path expected signature to characterize the law of а stochastic process when the cumulant generating function is known ( )? Since an average of group-like elements is not a group-like element, an expected signature (of a sample of paths) is not a signature, which seems to imply that expected signatures do not enjoy a nice geometric interpretation as signatures do. It seems that two main applications of expected signatures are the characterization of the law of a process (if moments completely describe the distribution) and the approximation of continuous functions on a collection of paths as linear functions on the expected signature of that collection of paths. If one doesn't need the latter use case, how is the description of a stochastic processes in terms of expected signatures qualitatively different from the classic characteristic function description? Is there something else that expected signatures can tell besides the moments? For example, in case of Levy processes, characteristic function of a process has some very useful information about the process besides moments in light of Levy–Khintchine representation. It tells what parts this Levy process is composed of, namely its linear drift, Brownian motion, and Lévy jump process. By expected signature I mean described in [LM22] (Chapters 6 and 13), [CO22] , [LSDBL21] , [BDMN20] . END OF THE ORIGINAL POST EDIT: Reply to JeremyFR Thank you, @JeremyFR, for your response! However, I still struggle to see advantages of expected signatures vis-à-vis CGF when applied to common 1-D Levy processes. Let us start with Brownian motion. Let B be a standard one-dimensional Brownian motion calculated up to some fixed time T > 0. Then B has the following expected signature (Fawcett’s formula): It is also known that it’s moment-generating function is It seems that both expressions carry identical information once we take apart the notation. In the case of Brownian motion, it seems that one hardly can argue that one approach is better than the other because the results are semantically equal. Now let’s move on to a more general case of d-dimensional Lévy process with triplet (a, b, K). It is shown in [FS14] (Theorem 43), that the expected signature of such process is awfully similar to Lévy–Kintchine formula, In light of this result, one cannot help but wonder what is the point of using expected signatures for time series analysis from the practitioner’s perspective in applications involving common types of time series, which are usually 1D Levy Processes, that frequently occur in finance and energy / load research, given that the information in expected signature is essentially identical to the one provided by characteristic function? Granted, there are more exotic cases that require more general expression for expected signature [FHT21] but that hardly looks like something arising frequently in day-to-day applications.","\log\mathbb{E}[e^{i\theta X(t)}]  \Big( \mathbb{E} \Big[ \int dX^{\otimes n} \Big]  \Big)_{n\geq 0} = \Big( \mathbb{E} \Big[\int_0^T \int_0^{r_n} \cdots \int_0^{r_2} dX_{r_1} \otimes \cdots  \otimes dX_{r_n} \Big]\Big)_{n\geq 0}  \begin{align*}
	\mathbb{E}[S(B_{[0,T]})] &= \exp(\frac{T}{2}  e_i \otimes e_i ) \\
       &= 1+ \frac{T}{2} e_i ^ {\otimes 2}   +    \frac{1}{2!} \frac{T^2}{4} e_i ^ {\otimes 4}  + \dots 
\end{align*} \begin{align*}
	M_{B_T}(\theta) &= \exp( \frac{T}{2}  \theta^2) \\
		             &= 1 + \frac{T}{2} \theta^2 + \frac{1}{2!} \frac{T^2}{4} \theta^4 + \dots
\end{align*} \begin{align*}
\mathbb{E}[S(X)_{[0,T]}]  =\exp\Big [  T \Big (b + \frac{1}{2} a + \int (exp(y) – 1 – y 1 _{|y| < 1}) K(dy) \Big ) \Big] \in T((\mathbb{R}^d)).
\end{align*}","['probability', 'statistics', 'stochastic-processes', 'rough-path-theory']"
86,Optimizing a Chernoff bound with Bernstein's condition,Optimizing a Chernoff bound with Bernstein's condition,,"Let $X$ be a real valued, zero mean random variable satisfying berstein's condition with parameter $b$ so that the moment generating function satisfies: $$\mathbb E\left [ \exp \left ( \lambda X\right )\right ] \leq \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} \right )$$ for all $\lambda \in (0, 1/b)$ where $\sigma^2$ is the variance of $X$ . Using this to evaluate a chernoff bound for $X$ : $$\mathbb P (X \geq t) \leq \inf_{\lambda \in (0, 1/b)} \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} -\lambda t\right )$$ All that remains is to choose an appropriate value of $\lambda$ . The literature usually chooses $\lambda^* = t/(bt + \sigma^2)$ but it's not clear to me how this value is chosen or why it is a good choice. Interestingly, the argument of the exponential is $0$ when $\lambda =  2t/(2bt + \sigma^2)$ , which looks like the choice of $\lambda^*$ from the literature (but with a scaling on $t$ ). Is the process to identify $\lambda^*$ basically: 1. find a root, 2. choose a scaling of $t$ that minimizes the value of the RHS but still satisfies the constraint $\lambda \in (0, 1/b)$ ?","Let be a real valued, zero mean random variable satisfying berstein's condition with parameter so that the moment generating function satisfies: for all where is the variance of . Using this to evaluate a chernoff bound for : All that remains is to choose an appropriate value of . The literature usually chooses but it's not clear to me how this value is chosen or why it is a good choice. Interestingly, the argument of the exponential is when , which looks like the choice of from the literature (but with a scaling on ). Is the process to identify basically: 1. find a root, 2. choose a scaling of that minimizes the value of the RHS but still satisfies the constraint ?","X b \mathbb E\left [ \exp \left ( \lambda X\right )\right ]
\leq \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} \right ) \lambda \in (0, 1/b) \sigma^2 X X \mathbb P (X \geq t) \leq \inf_{\lambda \in (0, 1/b)} \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} -\lambda t\right ) \lambda \lambda^* = t/(bt + \sigma^2) 0 \lambda =  2t/(2bt + \sigma^2) \lambda^* t \lambda^* t \lambda \in (0, 1/b)","['probability', 'statistics', 'inequality']"
87,What does it mean for a distribution to not have a power law tail?,What does it mean for a distribution to not have a power law tail?,,"In this paper , while generating a random data set (on origin spacing), it is written ""All we require is to define an average separation between adjacent origins and the existence of an associated standard deviation (i.e. we assume the distribution has no power law tail)"" What does this mean, exactly?","In this paper , while generating a random data set (on origin spacing), it is written ""All we require is to define an average separation between adjacent origins and the existence of an associated standard deviation (i.e. we assume the distribution has no power law tail)"" What does this mean, exactly?",,"['probability', 'statistics', 'probability-distributions', 'random']"
88,Conditional expectation between lognormal random variables,Conditional expectation between lognormal random variables,,"My question is whether the conditional expectation between lognormal random variables $Y$ and $X$ , i.e $\mathbb{E}(Y|X)$ has a closed form linear (or non-linear) expression similar to Gaussian random variables. Recall that if $(Z,W)$ are jointly normal, then $\mathbb{E}(Z|W)=\beta_0 +\beta_1W$ , where $\beta_0=\mathbb{E}(Z)-\beta_1\mathbb{E}(W)$ and $\beta_1=\frac{Cov(Z,W)}{Var(W)}$ . Can we use this result by transforming lognormals to normals and then after using the result for normals, transform it back to logs?","My question is whether the conditional expectation between lognormal random variables and , i.e has a closed form linear (or non-linear) expression similar to Gaussian random variables. Recall that if are jointly normal, then , where and . Can we use this result by transforming lognormals to normals and then after using the result for normals, transform it back to logs?","Y X \mathbb{E}(Y|X) (Z,W) \mathbb{E}(Z|W)=\beta_0 +\beta_1W \beta_0=\mathbb{E}(Z)-\beta_1\mathbb{E}(W) \beta_1=\frac{Cov(Z,W)}{Var(W)}","['statistics', 'probability-distributions', 'normal-distribution', 'conditional-probability', 'conditional-expectation']"
89,Chi-squared Test for the Variance,Chi-squared Test for the Variance,,"An example of Chi-squared Test for the Variance on NIST website ( https://www.itl.nist.gov/div898/handbook/eda/section3/eda358.htm ) goes as follows: The observed variance for the 100 measurements of gear diameter is 0.00003969  (the standard deviation is 0.0063).  We will test the null hypothesis that the true variance is equal to 0.01.  H0:  σ2 = 0.01  Ha:  σ2 ≠ 0.01  Test statistic:  T = 0.3903 How was this T calculated to be 0.3903? According to the formula, the value of the statistic should be: $T=(N-1)(s/σ)^2=99(0.00003969/0.01)=0.392931$ Could someone point out anything that I missed here?","An example of Chi-squared Test for the Variance on NIST website ( https://www.itl.nist.gov/div898/handbook/eda/section3/eda358.htm ) goes as follows: The observed variance for the 100 measurements of gear diameter is 0.00003969  (the standard deviation is 0.0063).  We will test the null hypothesis that the true variance is equal to 0.01.  H0:  σ2 = 0.01  Ha:  σ2 ≠ 0.01  Test statistic:  T = 0.3903 How was this T calculated to be 0.3903? According to the formula, the value of the statistic should be: Could someone point out anything that I missed here?",T=(N-1)(s/σ)^2=99(0.00003969/0.01)=0.392931,['statistics']
90,Integral of borel function wrt to measure,Integral of borel function wrt to measure,,"Let $(\Omega,\mathscr{F},v)$ be some measure space. Consider the non-negative simple function $l:\Omega \rightarrow R$ given by: $$l(\omega)=\sum _{i=1}^k a_i I_{A_{i}}(\omega)$$ Where $a_i\in R_+, A_i\in\mathscr{F}$ , and $I_{A_{i}}$ is the indicator function for $A_i$ Definition 1 : Integral of a non-negative simple function $l$ wrt to a measure $v$ is given by: $$\int l~dv = \sum_{i=1}^k a_i v(A_i)$$ Definition 2 : Integral of some non-negative Borel measurable function $f:\Omega\rightarrow R$ with respect to measure $v$ is given by: $$\int f~dv= \sup\left\{\int l~dv: l \in S_f \right\}$$ Where $S_f$ is a collection of non-negative simple functions satisfying $l(\omega) \leq f(\omega)$ $\forall\omega\in\Omega$ . Consider some non-negative simple function $l'$ . I am aware all simple functions as defined above are Borel measurable. So how do I reconcile definition 1 and 2? That is, how do I prove that: $$\sup\left\{\int l~dv: l \in S_{l'} \right\} = \sum_{i=1}^{k'} a_i' v(A_i')$$ Where $l'(\omega)=\sum _{i=1}^{k'} {a_i'} I_{A_{i}'}(\omega), a_i'\in R_+, A_i'\in\mathscr{F}$ and $S_{l'}$ is the collection of non-negative simple functions satisfying $l(\omega)\leq l'(\omega)$ $\forall \omega\in \Omega$","Let be some measure space. Consider the non-negative simple function given by: Where , and is the indicator function for Definition 1 : Integral of a non-negative simple function wrt to a measure is given by: Definition 2 : Integral of some non-negative Borel measurable function with respect to measure is given by: Where is a collection of non-negative simple functions satisfying . Consider some non-negative simple function . I am aware all simple functions as defined above are Borel measurable. So how do I reconcile definition 1 and 2? That is, how do I prove that: Where and is the collection of non-negative simple functions satisfying","(\Omega,\mathscr{F},v) l:\Omega \rightarrow R l(\omega)=\sum _{i=1}^k a_i I_{A_{i}}(\omega) a_i\in R_+, A_i\in\mathscr{F} I_{A_{i}} A_i l v \int l~dv = \sum_{i=1}^k a_i v(A_i) f:\Omega\rightarrow R v \int f~dv= \sup\left\{\int l~dv: l \in S_f \right\} S_f l(\omega) \leq f(\omega) \forall\omega\in\Omega l' \sup\left\{\int l~dv: l \in S_{l'} \right\} = \sum_{i=1}^{k'} a_i' v(A_i') l'(\omega)=\sum _{i=1}^{k'} {a_i'} I_{A_{i}'}(\omega), a_i'\in R_+, A_i'\in\mathscr{F} S_{l'} l(\omega)\leq l'(\omega) \forall \omega\in \Omega","['measure-theory', 'statistics']"
91,BLUE in correlated case,BLUE in correlated case,,"Let $X_1,...,X_n$ be real-valued observations such that $E(X_j)=\mu$ , $V(X_j)=\sigma^2$ and the correlation coefficient between $X_i$ and $X_j$ is $r_{ij}$ , $1 \le i < j \le n$ . Assume that $r_{ij}$ 's are known. Derive the Best Linear Unbiased Estimator (BLUE) $\hat{\mu}$ of $\mu$ . My approach so far Suppose $T = \sum_{j=1}^{n} c_j X_j$ is the BLUE. Then $E(T)=\mu \implies \sum_{j=1}^{n} c_j =1$ . Also, we need to minimize $V(T) = \sum_{j=1}^{n} c_j ^2 V(X_j) + 2 \sum \sum_{i<j} c_i c_j \sigma^2 r_{ij}$ with respect to $c_1,...,c_n$ . I tried Lagrange Multipliers but the entire thing becomes too much tedious and I could not get a closed-form solution for the $c_j $ 's. Is there any other approach to this?","Let be real-valued observations such that , and the correlation coefficient between and is , . Assume that 's are known. Derive the Best Linear Unbiased Estimator (BLUE) of . My approach so far Suppose is the BLUE. Then . Also, we need to minimize with respect to . I tried Lagrange Multipliers but the entire thing becomes too much tedious and I could not get a closed-form solution for the 's. Is there any other approach to this?","X_1,...,X_n E(X_j)=\mu V(X_j)=\sigma^2 X_i X_j r_{ij} 1 \le i < j \le n r_{ij} \hat{\mu} \mu T = \sum_{j=1}^{n} c_j X_j E(T)=\mu \implies \sum_{j=1}^{n} c_j =1 V(T) = \sum_{j=1}^{n} c_j ^2 V(X_j) + 2 \sum \sum_{i<j} c_i c_j \sigma^2 r_{ij} c_1,...,c_n c_j ","['probability', 'probability-theory', 'statistics', 'statistical-inference', 'estimation']"
92,Probability Convergence of an average of random variables where $n \to \infty$ and $d \to \infty$,Probability Convergence of an average of random variables where  and,n \to \infty d \to \infty,"Suppose $X_1^{(d)}, X_2^{(d)}, \cdots, X_n^{(d)} \stackrel{\text{i.i.d.}}{\sim} \mathbf{F}_d$ are $d$ -dimensional random variables such that there exists a function $g_d(\cdot,\cdot)$ taking values in $[-2,+2]$ , and satisfying $g_d(X_1^{(d)},X_2^{(d)}) \to 0$ in probability as $d \to \infty$ . Also, $\mathbb{E}[g_d(X_i^{(d)}, X_j^{(d)})] = 0$ , for all $i,j = 1,2,\cdots,n ~(i \neq j)$ . Define the following random variable: $$S_{n,d} = \frac{1}{\binom{n}{2}} \sum_{1 \leqslant i \neq j \leqslant n} g_d(X_i^{(d)},X_j^{(d)})$$ Can we comment on the probability convergence of $S_{n,d}$ as $n \to \infty$ and $d \to \infty$ simultaneously? NOTE: $g_d(\cdot,\cdot)$ is a function which doesn't vary over $d$ . e.g. $g_d(a^{(d)},b^{(d)})= \frac{||a^{(d)}+b^{(d)}||_2}{||a^{(d)}||_2+||b^{(d)}||_2}$ . The problem is that I am facing difficulty understanding whether the different $g_d(X_i^{(d)},X_j^{(d)})$ 's are independent or not. If they are independent, it would have been a straightforward application of WLLN which is not the case here. However, we have an additional information that each summand goes to $0$ in probability as $d \to \infty$ . I have a hunch that $S_{n,d} \to 0$ in probability as $n \to \infty$ and $d \to \infty$ . Any help will be appreciated.","Suppose are -dimensional random variables such that there exists a function taking values in , and satisfying in probability as . Also, , for all . Define the following random variable: Can we comment on the probability convergence of as and simultaneously? NOTE: is a function which doesn't vary over . e.g. . The problem is that I am facing difficulty understanding whether the different 's are independent or not. If they are independent, it would have been a straightforward application of WLLN which is not the case here. However, we have an additional information that each summand goes to in probability as . I have a hunch that in probability as and . Any help will be appreciated.","X_1^{(d)}, X_2^{(d)}, \cdots, X_n^{(d)} \stackrel{\text{i.i.d.}}{\sim} \mathbf{F}_d d g_d(\cdot,\cdot) [-2,+2] g_d(X_1^{(d)},X_2^{(d)}) \to 0 d \to \infty \mathbb{E}[g_d(X_i^{(d)}, X_j^{(d)})] = 0 i,j = 1,2,\cdots,n ~(i \neq j) S_{n,d} = \frac{1}{\binom{n}{2}} \sum_{1 \leqslant i \neq j \leqslant n} g_d(X_i^{(d)},X_j^{(d)}) S_{n,d} n \to \infty d \to \infty g_d(\cdot,\cdot) d g_d(a^{(d)},b^{(d)})= \frac{||a^{(d)}+b^{(d)}||_2}{||a^{(d)}||_2+||b^{(d)}||_2} g_d(X_i^{(d)},X_j^{(d)}) 0 d \to \infty S_{n,d} \to 0 n \to \infty d \to \infty","['probability-theory', 'statistics', 'law-of-large-numbers']"
93,Determine $\alpha$ that satisfies $P(|X|<1) = P(|X|>1)$,Determine  that satisfies,\alpha P(|X|<1) = P(|X|>1),"Suppose that $X$ is uniformly distributed $[-\alpha, +\alpha]$ , where $\alpha > 0$ . Whenever possible, determine $\alpha$ so that the following are satisfied. $$P(|X| < 1) = P(|X| > 1)$$ Answer: $\alpha = 2$ . This is the question 4.20 from (Paul Meyer's ""Introductory probability and Statistics"", 2nd ed.) What I've managed to do was find the density function, which is: $\frac{1}{2\alpha}$ . And from there I applied in the given equation using the absolute value definition, obtaining: $$P(-1<X<1) = P(X>1) + P(X<-1)$$ $$\int_{-1}^{1}\frac{1}{2\alpha}d\alpha = 1 - P(X≤1) + P(X<-1)$$ But the outcome of this integral it's not a real value. What was my mistake here?","Suppose that is uniformly distributed , where . Whenever possible, determine so that the following are satisfied. Answer: . This is the question 4.20 from (Paul Meyer's ""Introductory probability and Statistics"", 2nd ed.) What I've managed to do was find the density function, which is: . And from there I applied in the given equation using the absolute value definition, obtaining: But the outcome of this integral it's not a real value. What was my mistake here?","X [-\alpha, +\alpha] \alpha > 0 \alpha P(|X| < 1) = P(|X| > 1) \alpha = 2 \frac{1}{2\alpha} P(-1<X<1) = P(X>1) + P(X<-1) \int_{-1}^{1}\frac{1}{2\alpha}d\alpha = 1 - P(X≤1) + P(X<-1)","['probability', 'statistics', 'probability-distributions', 'random-variables', 'uniform-distribution']"
94,gradient of KL-Divergence,gradient of KL-Divergence,,"Let $X$ be a finite set and $A$ be a set of probability distributions. Then KL-Divergence between two probability distributions $P(x)$ and $Q(x)$ $\in$ A  is $$D(P(x)\vert \vert Q(x))=\sum P(x)\operatorname{log}\left(\frac{P(x)}{Q(x)}\right)$$ for all x $\in$ X. Since KL-divergence is the distance between two probability distributions, therefore for fixed $Q(x)$ we can talk about its derivative with respect to $P(x).$ Can anyone tell what will be its gradient w.r.t $P(x)$ ?","Let be a finite set and be a set of probability distributions. Then KL-Divergence between two probability distributions and A  is for all x X. Since KL-divergence is the distance between two probability distributions, therefore for fixed we can talk about its derivative with respect to Can anyone tell what will be its gradient w.r.t ?",X A P(x) Q(x) \in D(P(x)\vert \vert Q(x))=\sum P(x)\operatorname{log}\left(\frac{P(x)}{Q(x)}\right) \in Q(x) P(x). P(x),"['probability', 'statistics', 'information-theory']"
95,MGF from a conditional distribution,MGF from a conditional distribution,,"I'm having a difficulty with this problem, it is not for submission. Let Y~U(0,1) be a continuous random variable, and X is another random variable such that given Y=p: $X|_{Y=p}$ ~B(n,p). I need to find the moment generating function of X. I honestly don't have much here, but I will try to add some context. It's weird to me that they ask about Y=p while being a continuous random variable, but my guess is that this will be relevant later. I have the MGF formula of the Unified distribution: $$M_x(t) = \frac{e^{tb}-e^{ta}}{t\left(b-a\right)} $$ And I thought about using that, but I still don't really see the way. Another direction I thought of going through but couldn't advance with it is going to the definition: $$M_x(t) = \sum _x\:e^{tx}P\left(X_{|Y=p}=x\right) $$ but I couldn't really solve it. Thanks in advance for any help provided.","I'm having a difficulty with this problem, it is not for submission. Let Y~U(0,1) be a continuous random variable, and X is another random variable such that given Y=p: ~B(n,p). I need to find the moment generating function of X. I honestly don't have much here, but I will try to add some context. It's weird to me that they ask about Y=p while being a continuous random variable, but my guess is that this will be relevant later. I have the MGF formula of the Unified distribution: And I thought about using that, but I still don't really see the way. Another direction I thought of going through but couldn't advance with it is going to the definition: but I couldn't really solve it. Thanks in advance for any help provided.",X|_{Y=p} M_x(t) = \frac{e^{tb}-e^{ta}}{t\left(b-a\right)}  M_x(t) = \sum _x\:e^{tx}P\left(X_{|Y=p}=x\right) ,"['statistics', 'conditional-probability']"
96,Calculating odds of Trading Cards [closed],Calculating odds of Trading Cards [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to calculate the odds of a box of trading cards containing less than $100$ unique cards. From what I can gather, there's $11$ common cards per pack of $159$ unique common cards, $1$ item card of $13$ , $1$ rare of $79$ , one card that may be rare or higher (very rare - $1$ in $12$ packs, out of $31$ v.rare cards, or super rare - $1$ in $96$ packs, out of $6$ s.rare cards) and one special edition that can be of any of the common cards. There are $16$ packs to a box. I've not done statistics much since high school some $15$ years ago so I'm a little overwhelmed as to where to start. Thanks!","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to calculate the odds of a box of trading cards containing less than unique cards. From what I can gather, there's common cards per pack of unique common cards, item card of , rare of , one card that may be rare or higher (very rare - in packs, out of v.rare cards, or super rare - in packs, out of s.rare cards) and one special edition that can be of any of the common cards. There are packs to a box. I've not done statistics much since high school some years ago so I'm a little overwhelmed as to where to start. Thanks!",100 11 159 1 13 1 79 1 12 31 1 96 6 16 15,['statistics']
97,Why is Matt Parker approximation of the average of the highest value of M N-sided dice more and more wrong for higher M?,Why is Matt Parker approximation of the average of the highest value of M N-sided dice more and more wrong for higher M?,,"In his video about the unexpected logic behind rolling multiple dice and picking the highest , Matt Parker, says that the formula in the end is $$\frac{m}{m+1}\times n+0.5$$ where $m$ is the number of dice, and $n$ the number of sides on the dice. I wrote a program that computes the actual values of the averages by simulating all the possible rolls for 20-sided dice (up to 8 dice because it takes too much more time after that). The values I get from my program are also very similar to Matt's values when he simulates the rolls, but those values differ by a greater value at every increase of $m$ . $m$ Matt's formula result Computed result Difference 1 10.500 10.500 0 2 13.833 13.825 0.008 3 15.500 15.487 0.013 4 16.500 16.483 0.017 5 17.166 17.145 0.021 6 17.642 17.617 0.025 7 18.000 17.970 0.030 8 18.277 18.244 0.033 Furthermore, if we take much higher values like $m=99$ , we get an average of $20.3$ , which is absurd because the average is then higher than the maximum rollable value. So to me, the $+0.5$ part is totally hand-waived and isn't correct from a mathematical point of view. So where is Matt wrong with the $+0.5$ part, and how to fix his formula so that it's accurate?","In his video about the unexpected logic behind rolling multiple dice and picking the highest , Matt Parker, says that the formula in the end is where is the number of dice, and the number of sides on the dice. I wrote a program that computes the actual values of the averages by simulating all the possible rolls for 20-sided dice (up to 8 dice because it takes too much more time after that). The values I get from my program are also very similar to Matt's values when he simulates the rolls, but those values differ by a greater value at every increase of . Matt's formula result Computed result Difference 1 10.500 10.500 0 2 13.833 13.825 0.008 3 15.500 15.487 0.013 4 16.500 16.483 0.017 5 17.166 17.145 0.021 6 17.642 17.617 0.025 7 18.000 17.970 0.030 8 18.277 18.244 0.033 Furthermore, if we take much higher values like , we get an average of , which is absurd because the average is then higher than the maximum rollable value. So to me, the part is totally hand-waived and isn't correct from a mathematical point of view. So where is Matt wrong with the part, and how to fix his formula so that it's accurate?",\frac{m}{m+1}\times n+0.5 m n m m m=99 20.3 +0.5 +0.5,"['statistics', 'average']"
98,"If vectors are maxima of $\frac{a^T a}{a^T \Sigma^{-1} a}$, then they are eigenvectors of $\Sigma$.","If vectors are maxima of , then they are eigenvectors of .",\frac{a^T a}{a^T \Sigma^{-1} a} \Sigma,"Let $\Sigma \in M_q$ be symmetric and positive definite matrix. If vectors $a_1,\dots, a_q \in \mathbb R^q$ satisfy: $(i)$ $$i \not = j \implies a_i^T a_j = 0$$ $(ii)$ $$\frac{a_1^T a_1}{a_1^T \Sigma^{-1} a_1} =\sup_{a \not = 0} \frac{a^T a}{a^T \Sigma^{-1} a} $$ $(iii)$ $$i \gt 1 \implies \frac{a_i^T a_i}{a_i^T \Sigma^{-1} a_i} = \sup\left\{ \frac{a^T a}{a^T \Sigma^{-1} a} : a \not = 0, a^T a_j = 0, j=1,\dots, i-1 \right\}$$ then $a_1,\dots, a_q$ are eigenvectors of $\Sigma$ . I am stuck trying to prove this claim. The text gives the hint to use induction, but I am not sure how to proceed. The claim is part of the proof that $a_1^T Y, \dots, a_q^T Y$ are main components of $Y$ . Given this result, rest of the proof is simple. Edit Attempt : Let $A, \Lambda$ be the square matrices such that $$\Sigma A = A\Lambda, \quad A^T \Sigma^{-1} A = I$$ where $\Lambda = \text{diag}(\phi_1,\dots, \phi_q)$ has eigenvalues of $\Sigma$ in descending order. Let: $$F(a) = \frac{a^T a}{a^T \Sigma^{-1} a}.$$ Then $$R(c) := F(Ac) = \frac{c^T A^T A c}{c^T A^T \Sigma^{-1} A c} = \frac{c^T A^T\Sigma^{-1} \Sigma A c}{c^T c} = \frac{c^T A^T\Sigma^{-1} A \Lambda c}{c^T c} = \frac{c^T \Lambda c}{c^T c}.$$ It is clear that $a$ is supremum of $F$ is and only if $A^{-1}a$ is supremum of $R$ . Let $c_i = A^{-1} a_i, i = 1,\dots q$ . Then: $$a_i^T a_j = c_i^T A^T A c_j = c_i^T A^T \Sigma^{-1}\Sigma A c_j =c_i^T A^T \Sigma^{-1} A  \Lambda c_j = c_i^T \Lambda c_j.$$ Therefore the $c_1,\dots, c_q$ satisfy: $(i')$ $$i \not = j \implies c_i^T \Lambda c_j = 0$$ $(ii')$ $$\frac{c_1^T \Lambda c_1}{c_1^T  c_1} =\sup_{c \not = 0} \frac{c^T \Lambda c}{c^T  c} $$ $(iii')$ $$i \gt 1 \implies \frac{c_i^T \Lambda c_i}{c_i^T  c_i} = \sup\left\{ \frac{c_i^T \Lambda c_i}{c_i^T  c_i} : c \not = 0, c^T \Lambda c_j = 0, j=1,\dots, i-1 \right\}$$ which seem somewhat simpler since $\Lambda$ is diagonal, but i am still unsure how to proceed. Attempt 2. Thanks to Mason's answer i feel i am closer to the solution: $$\frac{\partial F}{\partial a}(a) = \frac{2a \cdot (a^T \Sigma^{-1} a)-2\Sigma^{-1} a (a^T a)}{(a^T \Sigma^{-1}a)^2} = 0 \implies \Sigma^{-1}a = \frac{a^T\Sigma^{-1}a}{a^Ta} a$$ which implies every critial value of $F$ must be eigenvalue, and that $a_1$ is the eigenvector. Now that this is done, my idea is to look at the function: $$F:\{a_1\}^\perp \to \{a_1\}^{\perp}$$ and doing the same procedure shows that critical values of $F$ must be eigenvectors of $\Sigma^{-1}$ . However I am not sure i can conclude that the minimum and maximum of $F$ on the set $\{a_1\}^\perp$ are critical points of $F$ since $\{a_1\}^\perp$ is not open.","Let be symmetric and positive definite matrix. If vectors satisfy: then are eigenvectors of . I am stuck trying to prove this claim. The text gives the hint to use induction, but I am not sure how to proceed. The claim is part of the proof that are main components of . Given this result, rest of the proof is simple. Edit Attempt : Let be the square matrices such that where has eigenvalues of in descending order. Let: Then It is clear that is supremum of is and only if is supremum of . Let . Then: Therefore the satisfy: which seem somewhat simpler since is diagonal, but i am still unsure how to proceed. Attempt 2. Thanks to Mason's answer i feel i am closer to the solution: which implies every critial value of must be eigenvalue, and that is the eigenvector. Now that this is done, my idea is to look at the function: and doing the same procedure shows that critical values of must be eigenvectors of . However I am not sure i can conclude that the minimum and maximum of on the set are critical points of since is not open.","\Sigma \in M_q a_1,\dots, a_q \in \mathbb R^q (i) i \not = j \implies a_i^T a_j = 0 (ii) \frac{a_1^T a_1}{a_1^T \Sigma^{-1} a_1} =\sup_{a \not = 0} \frac{a^T a}{a^T \Sigma^{-1} a}  (iii) i \gt 1 \implies \frac{a_i^T a_i}{a_i^T \Sigma^{-1} a_i} = \sup\left\{ \frac{a^T a}{a^T \Sigma^{-1} a} : a \not = 0, a^T a_j = 0, j=1,\dots, i-1 \right\} a_1,\dots, a_q \Sigma a_1^T Y, \dots, a_q^T Y Y A, \Lambda \Sigma A = A\Lambda, \quad A^T \Sigma^{-1} A = I \Lambda = \text{diag}(\phi_1,\dots, \phi_q) \Sigma F(a) = \frac{a^T a}{a^T \Sigma^{-1} a}. R(c) := F(Ac) = \frac{c^T A^T A c}{c^T A^T \Sigma^{-1} A c} = \frac{c^T A^T\Sigma^{-1} \Sigma A c}{c^T c} = \frac{c^T A^T\Sigma^{-1} A \Lambda c}{c^T c} = \frac{c^T \Lambda c}{c^T c}. a F A^{-1}a R c_i = A^{-1} a_i, i = 1,\dots q a_i^T a_j = c_i^T A^T A c_j = c_i^T A^T \Sigma^{-1}\Sigma A c_j =c_i^T A^T \Sigma^{-1} A  \Lambda c_j = c_i^T \Lambda c_j. c_1,\dots, c_q (i') i \not = j \implies c_i^T \Lambda c_j = 0 (ii') \frac{c_1^T \Lambda c_1}{c_1^T  c_1} =\sup_{c \not = 0} \frac{c^T \Lambda c}{c^T  c}  (iii') i \gt 1 \implies \frac{c_i^T \Lambda c_i}{c_i^T  c_i} = \sup\left\{ \frac{c_i^T \Lambda c_i}{c_i^T  c_i} : c \not = 0, c^T \Lambda c_j = 0, j=1,\dots, i-1 \right\} \Lambda \frac{\partial F}{\partial a}(a) = \frac{2a \cdot (a^T \Sigma^{-1} a)-2\Sigma^{-1} a (a^T a)}{(a^T \Sigma^{-1}a)^2} = 0 \implies \Sigma^{-1}a = \frac{a^T\Sigma^{-1}a}{a^Ta} a F a_1 F:\{a_1\}^\perp \to \{a_1\}^{\perp} F \Sigma^{-1} F \{a_1\}^\perp F \{a_1\}^\perp","['linear-algebra', 'statistics', 'matrix-calculus']"
99,"Let $X_{1}, X_{2}, X_{3},...$ be a sequence of independent and identically distributed random variables, prove the following","Let  be a sequence of independent and identically distributed random variables, prove the following","X_{1}, X_{2}, X_{3},...","Let $X_{1}, X_{2}, X_{3},...$ be a sequence of independent and identically distributed random variables with common (finite) mean $\mu$ . Prove that there exists and event A such that $P(A) = 1$ and for all $w \in \Omega$ , the quantity $\lim_{n \to \infty}(X_{1}(w)X_{2}(w)...X_{n}(w))^\frac{1}{n}$ converges and determine this limit. I get the part where these random variables are identically distributed, so the expectation of the sequence is also $\mu$ . But how do I proceed further? Will central limit theorem help here?","Let be a sequence of independent and identically distributed random variables with common (finite) mean . Prove that there exists and event A such that and for all , the quantity converges and determine this limit. I get the part where these random variables are identically distributed, so the expectation of the sequence is also . But how do I proceed further? Will central limit theorem help here?","X_{1}, X_{2}, X_{3},... \mu P(A) = 1 w \in \Omega \lim_{n \to \infty}(X_{1}(w)X_{2}(w)...X_{n}(w))^\frac{1}{n} \mu",['statistics']
