,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lax's proof of a property of principal minors,Lax's proof of a property of principal minors,,"There's a section in Peter Lax's Linear Algebra (2nd edition) that I am struggling to understand. I think it involves at least one typo, so let me write it out here exactly as it is in the book. Lax has just established that if $\lambda$ is a simple root of $\chi_A(x)$, then one of the principal minors of $A-\lambda I$ has nonzero determinant. Then he continues (p. 131): ""Let A be (a matrix with $a$ as a simple root of its characteristic polynomial) and take the eigenvalue $a$ to be zero. Then one of the principal $(n-1)\times (n-1)$ minors of $A$, say the $i$th, has nonzero determinant. We claim that the $i$th component of an eigenvector $h$ of $A$ pertaining to the eigenvalue $a$ is nonzero. Suppose it were denote (sic) by $h^{(i)}$ the vector obtained from $h$ by omitting the $i$th component, and by $A_ji$ (sic) the $i$th principal minor of $A$. Then $h^{(i)}$ satisfies $$A_{ii}h^{(i)}=0.""$$ Huh? Why should $A_{ii}h^{(i)}=0$? If I am to understand by $A_{ii}$ the $i$th principal minor of $A$, which results from omitting the $i$th row and $i$th column, then this statement seems false. For a dumb counterexample, let $A=\left[\begin{matrix}1&-1 \\ 2&-2\end{matrix}\right]$ and $h=\left[\begin{matrix}1\\1\end{matrix}\right]$. Then $A_{ii}h^{(i)}\neq 0$. What am I not understanding?","There's a section in Peter Lax's Linear Algebra (2nd edition) that I am struggling to understand. I think it involves at least one typo, so let me write it out here exactly as it is in the book. Lax has just established that if $\lambda$ is a simple root of $\chi_A(x)$, then one of the principal minors of $A-\lambda I$ has nonzero determinant. Then he continues (p. 131): ""Let A be (a matrix with $a$ as a simple root of its characteristic polynomial) and take the eigenvalue $a$ to be zero. Then one of the principal $(n-1)\times (n-1)$ minors of $A$, say the $i$th, has nonzero determinant. We claim that the $i$th component of an eigenvector $h$ of $A$ pertaining to the eigenvalue $a$ is nonzero. Suppose it were denote (sic) by $h^{(i)}$ the vector obtained from $h$ by omitting the $i$th component, and by $A_ji$ (sic) the $i$th principal minor of $A$. Then $h^{(i)}$ satisfies $$A_{ii}h^{(i)}=0.""$$ Huh? Why should $A_{ii}h^{(i)}=0$? If I am to understand by $A_{ii}$ the $i$th principal minor of $A$, which results from omitting the $i$th row and $i$th column, then this statement seems false. For a dumb counterexample, let $A=\left[\begin{matrix}1&-1 \\ 2&-2\end{matrix}\right]$ and $h=\left[\begin{matrix}1\\1\end{matrix}\right]$. Then $A_{ii}h^{(i)}\neq 0$. What am I not understanding?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
1,Does this complex remain exact after I restrict the maps?,Does this complex remain exact after I restrict the maps?,,"$R$ is a commutative ring with unity. Assume you have two matrices $A:R^n\rightarrow R^m$ and $B:R^m\rightarrow R^n$ such that they form an exact complex in the obvious way, i.e., $$\cdots\rightarrow R^n\stackrel{A}\rightarrow R^m\stackrel{B}\rightarrow R^n\rightarrow\cdots.$$ Call $S$ the subring of $R$ generated by the entries of $A$ and $B$ (I mean $S=\mathbb{Z}/n\mathbb{Z}[a_{ij},b_{ij}]$, where $n$ is the characteristic of $R$), and restrict the matrices to the ring $S$. The restricted maps still form a complex. Is it exact? I'm pretty sure it is not, but I cannot find any counterexample. Any help?","$R$ is a commutative ring with unity. Assume you have two matrices $A:R^n\rightarrow R^m$ and $B:R^m\rightarrow R^n$ such that they form an exact complex in the obvious way, i.e., $$\cdots\rightarrow R^n\stackrel{A}\rightarrow R^m\stackrel{B}\rightarrow R^n\rightarrow\cdots.$$ Call $S$ the subring of $R$ generated by the entries of $A$ and $B$ (I mean $S=\mathbb{Z}/n\mathbb{Z}[a_{ij},b_{ij}]$, where $n$ is the characteristic of $R$), and restrict the matrices to the ring $S$. The restricted maps still form a complex. Is it exact? I'm pretty sure it is not, but I cannot find any counterexample. Any help?",,"['matrices', 'commutative-algebra', 'ring-theory', 'modules']"
2,Question about Noncommutative Rings by I. N. Herstein,Question about Noncommutative Rings by I. N. Herstein,,"Here's an example that I do not quite understand it fully, it's on page 6 of the book. And here's what it says: Let $F$ be a field, and $F_n$ be a ring of all $n\times n$ matrices over $F$ . We consider $F_n$ as the ring of all linear transformations on the vector space $V$ of $n-$ tuples of elements of $F$ . If $A$ is a subset of $F_n$ , let $\overline{A}$ be the sub-algebra generated by $A$ over $F$ . Clearly, $V$ is a faithful $F_n-$ module, and so, $\overline{A}$ -module . $V$ is in addition, both a unitary, and irreducible $F_n-$ module. We say the set of matrices $A \subset F_n$ is irreducible if $V$ is an irreducible $\overline{A}-$ module. In matrix terms, it merely says that there is no invertible matrix $S$ in $F_n$ so that $$S^{-1}aS = \left( \begin{array}{c|c} a_1 & 0 \\ \hline * & a_2  \end{array} \right)$$ for all $a \in A$ . The bolded parts are what I don't understand Firstly, why must $V$ be a faithfull $F_n-$ module in order to be an $\overline{A}-$ module? What will happen if $V$ is not faithful? Secondly, I don't understand the bolded part start with ""in matrix terms..."", say I have $V$ is an irreducible $\overline{A}-$ module. How can I deduce the last part? And I don't get the star in the expression $S^{-1}aS$ as well, what does it stand for, is it a wild-card? I think this has to do with some matrix knowledge that I'm missing, so if the explanation is pretty long, please just give me a reference on what I should read. Thank you guys very much, And have a good day,","Here's an example that I do not quite understand it fully, it's on page 6 of the book. And here's what it says: Let be a field, and be a ring of all matrices over . We consider as the ring of all linear transformations on the vector space of tuples of elements of . If is a subset of , let be the sub-algebra generated by over . Clearly, is a faithful module, and so, -module . is in addition, both a unitary, and irreducible module. We say the set of matrices is irreducible if is an irreducible module. In matrix terms, it merely says that there is no invertible matrix in so that for all . The bolded parts are what I don't understand Firstly, why must be a faithfull module in order to be an module? What will happen if is not faithful? Secondly, I don't understand the bolded part start with ""in matrix terms..."", say I have is an irreducible module. How can I deduce the last part? And I don't get the star in the expression as well, what does it stand for, is it a wild-card? I think this has to do with some matrix knowledge that I'm missing, so if the explanation is pretty long, please just give me a reference on what I should read. Thank you guys very much, And have a good day,",F F_n n\times n F F_n V n- F A F_n \overline{A} A F V F_n- \overline{A} V F_n- A \subset F_n V \overline{A}- S F_n S^{-1}aS = \left( \begin{array}{c|c} a_1 & 0 \\ \hline * & a_2  \end{array} \right) a \in A V F_n- \overline{A}- V V \overline{A}- S^{-1}aS,"['abstract-algebra', 'matrices', 'modules', 'noncommutative-algebra']"
3,Counting 0-1 matrices up to symmetry,Counting 0-1 matrices up to symmetry,,"I'm interested in counting the number of n×n 0-1 matrices with a given number of 1s up to rotation and reflection. What is the best way to do this if n is not too small? For example, consider 4×4 matrices with 7 1s. There are ${16\choose7}=11440$ such matrices in total, but most equivalence classes under reflection and rotation have 8 members, so the total is closer to 11440/8 = 1430. In fact if I am correct the total is 1465. One method would be enumerating all such matrices, counting the number of distinct matrices resulting from each of the eight symmetries. But this takes too long, even for a computer, for n > 6 or so. Burnside's lemma seems promising, but this requires counting fixed points and I do not know how to do this rapidly. In any case I have very little familiarity with group theory and so pointers, even basic ones, would be appreciated. Perhaps there are other methods which can be used as well. I have seen this described as an easy problem, so I must be missing something (unless it was mischaracterized!).","I'm interested in counting the number of n×n 0-1 matrices with a given number of 1s up to rotation and reflection. What is the best way to do this if n is not too small? For example, consider 4×4 matrices with 7 1s. There are ${16\choose7}=11440$ such matrices in total, but most equivalence classes under reflection and rotation have 8 members, so the total is closer to 11440/8 = 1430. In fact if I am correct the total is 1465. One method would be enumerating all such matrices, counting the number of distinct matrices resulting from each of the eight symmetries. But this takes too long, even for a computer, for n > 6 or so. Burnside's lemma seems promising, but this requires counting fixed points and I do not know how to do this rapidly. In any case I have very little familiarity with group theory and so pointers, even basic ones, would be appreciated. Perhaps there are other methods which can be used as well. I have seen this described as an easy problem, so I must be missing something (unless it was mischaracterized!).",,"['combinatorics', 'group-theory', 'matrices', 'symmetry', 'dihedral-groups']"
4,Is this map a known one?,Is this map a known one?,,"Let $A$ be a $2\times2$ real matrix, then define $f:S^1\to S^1$ by $f(\phi)=\arctan(A\cdot(\cos \phi,\sin \phi))$. This can be viewed as a discrete dynamical system on $\mathbb{S}^1$ and I am trying to figure out its behaviour. Are there any results about it? Does it have a name? EDIT: What actually $f$ does, is the following. Takes a normalized vector, applies the matrix on the vector and renormalizes it. Then it can be considered as a map $S^1\to S^1$. Then obviously an eigenvector corresponds to a fixed point.","Let $A$ be a $2\times2$ real matrix, then define $f:S^1\to S^1$ by $f(\phi)=\arctan(A\cdot(\cos \phi,\sin \phi))$. This can be viewed as a discrete dynamical system on $\mathbb{S}^1$ and I am trying to figure out its behaviour. Are there any results about it? Does it have a name? EDIT: What actually $f$ does, is the following. Takes a normalized vector, applies the matrix on the vector and renormalizes it. Then it can be considered as a map $S^1\to S^1$. Then obviously an eigenvector corresponds to a fixed point.",,"['matrices', 'dynamical-systems']"
5,Characteristic and minimal polynomial of a special matrix,Characteristic and minimal polynomial of a special matrix,,"$H = \begin{bmatrix} 1 & w^{-1} & w^{-2} & ... & w^{1-n}\\  w & 1 & w^{-1} & ... & w^{2-n} \\  w^{2} & w^1 & 1 & ... & w^{3-n} \\  ... & ... & ... & ... & ...\\  w^{n-1} & w^{n-2} & w^{n-3} & ... & 1 \end{bmatrix}$ I'm looking for the characteristic polynomial $p_{H}(\lambda)$ and the minimal polynomial $m_{H}(\lambda)$ of this matrix. I got $p_{H}(\lambda) =(-\lambda)^{n-1}({(n-2)+\lambda)}$, but I'm not sure it's okay.","$H = \begin{bmatrix} 1 & w^{-1} & w^{-2} & ... & w^{1-n}\\  w & 1 & w^{-1} & ... & w^{2-n} \\  w^{2} & w^1 & 1 & ... & w^{3-n} \\  ... & ... & ... & ... & ...\\  w^{n-1} & w^{n-2} & w^{n-3} & ... & 1 \end{bmatrix}$ I'm looking for the characteristic polynomial $p_{H}(\lambda)$ and the minimal polynomial $m_{H}(\lambda)$ of this matrix. I got $p_{H}(\lambda) =(-\lambda)^{n-1}({(n-2)+\lambda)}$, but I'm not sure it's okay.",,['matrices']
6,Values for a and b Diagonalizable Over C,Values for a and b Diagonalizable Over C,,"For which values of $a$ and $b$ is the matrix $$ \begin{pmatrix} 0 & a\\ b & 0 \end{pmatrix} $$ diagonalizable over $\mathbb{C}$? I know that if $a = -1$ and $b = 1$, then the matrix is diagonalizable. However, I am not sure if that is the only solution or how to go about finding solutions in general. Please help.","For which values of $a$ and $b$ is the matrix $$ \begin{pmatrix} 0 & a\\ b & 0 \end{pmatrix} $$ diagonalizable over $\mathbb{C}$? I know that if $a = -1$ and $b = 1$, then the matrix is diagonalizable. However, I am not sure if that is the only solution or how to go about finding solutions in general. Please help.",,"['linear-algebra', 'matrices', 'diagonalization']"
7,Is this a matrix norm?,Is this a matrix norm?,,"In wikipedia , the operator norm of a matrix is given by (assume: real, $n$-dimensional) $$ ||A||= \max \left\{ \frac{|Ax|}{|x|}:x \in \mathbb{R}^n, x\neq 0 \right\}$$ (I'm not sure why it is not a supremum, maybe the set is closed.) Then is it true that the following is also a norm? $$ ||A||= \inf \left\{ \frac{|Ax|}{|x|}:x \in \mathbb{R}^n, x\neq 0 \right\}$$ (Actually I want to say something like $|Ax|\ge||A||\,|x|$)","In wikipedia , the operator norm of a matrix is given by (assume: real, $n$-dimensional) $$ ||A||= \max \left\{ \frac{|Ax|}{|x|}:x \in \mathbb{R}^n, x\neq 0 \right\}$$ (I'm not sure why it is not a supremum, maybe the set is closed.) Then is it true that the following is also a norm? $$ ||A||= \inf \left\{ \frac{|Ax|}{|x|}:x \in \mathbb{R}^n, x\neq 0 \right\}$$ (Actually I want to say something like $|Ax|\ge||A||\,|x|$)",,"['matrices', 'normed-spaces']"
8,Find the value of lambda for which the system of equations can be solved?,Find the value of lambda for which the system of equations can be solved?,,"I'm having some trouble with a problem here. I have the following system of equations: $$         \begin{bmatrix}         1 & 1 & 2 \\         1 & 2 & 1 \\         2 & 1 & 5 \\         \end{bmatrix}         \begin{bmatrix}         x \\         y \\         z \\         \end{bmatrix} =         \begin{bmatrix}         3 \\         4 \\         \lambda \\         \end{bmatrix} $$ I need to find the value of $\lambda$ that allows the system to be solved. I can't just premultiply the inverse of the 3x3 matrix over, since it's singular, so I multiplied it out to get: $$ x+y+2z=3 $$$$ x+2y+z=4 $$$$ 2x+y+5z=\lambda $$ I understand that if $\lambda=5$, then the third equation is not independent of the other two. I could just set $\lambda$ to anything but $5$, but that doesn't sound like what I'm supposed to do, and plus, the problem says ""the value of $\lambda$,"" not ""a value of $\lambda$."" Am I confused here, or does this not make sense? Thanks. -Zane","I'm having some trouble with a problem here. I have the following system of equations: $$         \begin{bmatrix}         1 & 1 & 2 \\         1 & 2 & 1 \\         2 & 1 & 5 \\         \end{bmatrix}         \begin{bmatrix}         x \\         y \\         z \\         \end{bmatrix} =         \begin{bmatrix}         3 \\         4 \\         \lambda \\         \end{bmatrix} $$ I need to find the value of $\lambda$ that allows the system to be solved. I can't just premultiply the inverse of the 3x3 matrix over, since it's singular, so I multiplied it out to get: $$ x+y+2z=3 $$$$ x+2y+z=4 $$$$ 2x+y+5z=\lambda $$ I understand that if $\lambda=5$, then the third equation is not independent of the other two. I could just set $\lambda$ to anything but $5$, but that doesn't sound like what I'm supposed to do, and plus, the problem says ""the value of $\lambda$,"" not ""a value of $\lambda$."" Am I confused here, or does this not make sense? Thanks. -Zane",,"['linear-algebra', 'matrices']"
9,Linear Algebra Matrix Question,Linear Algebra Matrix Question,,I am having trouble showing that $e^AX = Xe^A$ for all $n$ by $n$ matrices $X$ where $A$ is an invertible $n$ by $n$ matrix iff $AX = XA$ for all $X$. Any help will be appreciated.  Thank you,I am having trouble showing that $e^AX = Xe^A$ for all $n$ by $n$ matrices $X$ where $A$ is an invertible $n$ by $n$ matrix iff $AX = XA$ for all $X$. Any help will be appreciated.  Thank you,,"['linear-algebra', 'matrices', 'power-series']"
10,"If $H$ has an $a\times b$ submatrix of all $1$s, please prove that $ab\le n$.","If  has an  submatrix of all s, please prove that .",H a\times b 1 ab\le n,"Let $H$ be an $n\times n$ matrix with entries $\pm1$. Its rows are mutually orthogonal. If $H$ has an $a\times b$ submatrix of all $1$s, please prove that $ab\le n$.","Let $H$ be an $n\times n$ matrix with entries $\pm1$. Its rows are mutually orthogonal. If $H$ has an $a\times b$ submatrix of all $1$s, please prove that $ab\le n$.",,"['linear-algebra', 'matrices', 'inequality', 'contest-math']"
11,Matrices to model 3D object,Matrices to model 3D object,,I'm toying around with an algorithm to determine placement of 3D objects into a larger 3D space. I immediately thought of using matrices. It's been some years since my Linear Algebra courses. I was hoping someone could provide some references or even examples to help get me started. Bonus points for: Collision detection Object rotation Alternatives to matrices,I'm toying around with an algorithm to determine placement of 3D objects into a larger 3D space. I immediately thought of using matrices. It's been some years since my Linear Algebra courses. I was hoping someone could provide some references or even examples to help get me started. Bonus points for: Collision detection Object rotation Alternatives to matrices,,"['linear-algebra', 'matrices']"
12,Why is there always a Householder transformation that maps one specific vector to another?,Why is there always a Householder transformation that maps one specific vector to another?,,"I'm wondering why, if I'm given two vectors $u$ and $v$, I can always find a Householder transformation that maps $u$ to $v$. (This is needed in QR factorisation with Householder transformations). Thanks in advance for the answers!","I'm wondering why, if I'm given two vectors $u$ and $v$, I can always find a Householder transformation that maps $u$ to $v$. (This is needed in QR factorisation with Householder transformations). Thanks in advance for the answers!",,"['linear-algebra', 'matrices']"
13,Is there any procedure to compute the root of a matrix equation??,Is there any procedure to compute the root of a matrix equation??,,"Actually I want to know the procedure, how could we calculate the value of $A$ where $A^2$= $\begin{pmatrix} x & y\\ z & w\\ \end{pmatrix}$","Actually I want to know the procedure, how could we calculate the value of $A$ where $A^2$= $\begin{pmatrix} x & y\\ z & w\\ \end{pmatrix}$",,"['linear-algebra', 'matrices']"
14,Raising a matrix to a large power when the values are fractions (precision problem),Raising a matrix to a large power when the values are fractions (precision problem),,"I have a matrix $M$ where various elements may be in the form of $x/y$. If I use the decimal form of that number, I lose precision if I raise $M$ to a large power. My question: is it possible to do two exponentiations? One for the numerator only, and then one for the denominator, and then somehow combine the results?","I have a matrix $M$ where various elements may be in the form of $x/y$. If I use the decimal form of that number, I lose precision if I raise $M$ to a large power. My question: is it possible to do two exponentiations? One for the numerator only, and then one for the denominator, and then somehow combine the results?",,"['matrices', 'numerical-methods']"
15,"What type of division is possible in 1, 2, 4, and 8 but not the 3rd dimension?","What type of division is possible in 1, 2, 4, and 8 but not the 3rd dimension?",,"In this article that talks about some history of hamilton http://plus.maths.org/content/curious-quaternions There is a snippet that says this: Multiplication is very sneaky. You can only set up rules for   multiplication that let you divide in dimensions 1, 2, 4 and 8. This   is just a mysterious fact about the universe. Well, if you study maths   it's not mysterious because you can see exactly why, but it's   mysterious in the sense that when you hear about it first it just   sounds completely crazy! What are they talking about here? What type of division is possible in 2 dimensions but not 3? Could you give me an example of the division in two dimensions?","In this article that talks about some history of hamilton http://plus.maths.org/content/curious-quaternions There is a snippet that says this: Multiplication is very sneaky. You can only set up rules for   multiplication that let you divide in dimensions 1, 2, 4 and 8. This   is just a mysterious fact about the universe. Well, if you study maths   it's not mysterious because you can see exactly why, but it's   mysterious in the sense that when you hear about it first it just   sounds completely crazy! What are they talking about here? What type of division is possible in 2 dimensions but not 3? Could you give me an example of the division in two dimensions?",,"['linear-algebra', 'matrices', 'complex-numbers', 'quaternions']"
16,Matrix representation of a linear transformation between vector spaces,Matrix representation of a linear transformation between vector spaces,,"Let $v$ be an $n$-dimensional vector space over a field $F$ and $\psi: V \to V$ and isomorphism. Show that there exist bases $B_1$, $B_2$ (possibly different) such that the matrix representation of $\psi$ with respect to the two bases is precisely the $n \times n$ identity matrix. I know that isomorphisms map bases to bases. I just cannot give the explicit two bases. I am assuming that one should fix a basis $\mathbf{v_1, v_2 \cdots v_n} $ and then use the fact that $\psi$ is an isomorphism to get the second basis. But I don't know how to proceed from here. Any suggestions?","Let $v$ be an $n$-dimensional vector space over a field $F$ and $\psi: V \to V$ and isomorphism. Show that there exist bases $B_1$, $B_2$ (possibly different) such that the matrix representation of $\psi$ with respect to the two bases is precisely the $n \times n$ identity matrix. I know that isomorphisms map bases to bases. I just cannot give the explicit two bases. I am assuming that one should fix a basis $\mathbf{v_1, v_2 \cdots v_n} $ and then use the fact that $\psi$ is an isomorphism to get the second basis. But I don't know how to proceed from here. Any suggestions?",,['linear-algebra']
17,Are there any maps which preserve addition and multiplication over Matrics?,Are there any maps which preserve addition and multiplication over Matrics?,,"I don't know whether the title is correct, cause English is not my native language.  What I mean is: Suppose there is a function, say $f$, which maps Matrix $A$ into Matrix $A'$, and satisfies $$f(A+B)=f(A)+f(B)$$ This is what I mean by saying preserving addition. Of course, something like $f(A)=QA$ is a solution, but I prefer non-linear solutions. And preserving multiplication means  $$f(AB)=f(A)f(B)$$ Are there any examples of this kind of mapping, which preserve addition or multiplication or both? Thanks a lot!","I don't know whether the title is correct, cause English is not my native language.  What I mean is: Suppose there is a function, say $f$, which maps Matrix $A$ into Matrix $A'$, and satisfies $$f(A+B)=f(A)+f(B)$$ This is what I mean by saying preserving addition. Of course, something like $f(A)=QA$ is a solution, but I prefer non-linear solutions. And preserving multiplication means  $$f(AB)=f(A)f(B)$$ Are there any examples of this kind of mapping, which preserve addition or multiplication or both? Thanks a lot!",,"['linear-algebra', 'matrices']"
18,Armijo's rule line search,Armijo's rule line search,,"I have read a paper ( http://www.seas.upenn.edu/~taskar/pubs/aistats09.pdf ) which describes a way to solve an optimization problem involving Armijo's rule, cf. p363 eq 13. The variable is $\beta$ which is a square matrix. If $f$ is the objective function, the paper states that Armijo's rule is the following: $f(\beta^{new})-f(\beta^{old}) \leq \eta(\beta^{new}-\beta^{old})^T \nabla_{\beta} f$ where $\nabla_{\beta} f$ is the vectorization of the gradient of $f$. I am having problems with this as the expression on the right of the inequality above does not make sense due to dimension mismatch. I am unable to find an analogue of the above rule elsewhere. Can anyone help me as to figure out what the right expression means? The left expression is a scalar while the right expression suffers from a dimension mismatch...","I have read a paper ( http://www.seas.upenn.edu/~taskar/pubs/aistats09.pdf ) which describes a way to solve an optimization problem involving Armijo's rule, cf. p363 eq 13. The variable is $\beta$ which is a square matrix. If $f$ is the objective function, the paper states that Armijo's rule is the following: $f(\beta^{new})-f(\beta^{old}) \leq \eta(\beta^{new}-\beta^{old})^T \nabla_{\beta} f$ where $\nabla_{\beta} f$ is the vectorization of the gradient of $f$. I am having problems with this as the expression on the right of the inequality above does not make sense due to dimension mismatch. I am unable to find an analogue of the above rule elsewhere. Can anyone help me as to figure out what the right expression means? The left expression is a scalar while the right expression suffers from a dimension mismatch...",,"['linear-algebra', 'matrices', 'optimization', 'linear-programming', 'convex-optimization']"
19,smallest singular value,smallest singular value,,"I know this question is a difficult one, but any advice/tip/reference/heuristic is welcome. Is there any good lower bound (other than $0$) on the smallest singular value of a matrix? It is easy to get an upper bound, but any ideas for a lower bound? An answer for the smallest eigenvalue of a positive matrix is also appreciated.","I know this question is a difficult one, but any advice/tip/reference/heuristic is welcome. Is there any good lower bound (other than $0$) on the smallest singular value of a matrix? It is easy to get an upper bound, but any ideas for a lower bound? An answer for the smallest eigenvalue of a positive matrix is also appreciated.",,"['matrices', 'eigenvalues-eigenvectors']"
20,Question on generalized eigenspaces of commuting matrices,Question on generalized eigenspaces of commuting matrices,,"The following question came up as a though while I was reading. I cannot see how to proceed on it. Let us have $M_1,\ldots,M_n$ be commuting matrices. I know that that the generalized eigenspaces are the same across the $M_i$. However, is it true that for each generalized eigenspace, $V$, where $V$ is the generalized eigenspace of $\lambda_i$ for $M_i$, there exists $v\in V$ such that $v$ is an eigenvector for all the $M_i$. A related question is if it is even true that the $M_i$ need have a common eigenvector (I see how an answer to this question implies an answer to my first question, so these seem equivalent) I have tried to come up with examples for which what I said is not true, but this has been to no avail. From the generalized eigenspaces I do not see how to proceed. Thank you for any help.","The following question came up as a though while I was reading. I cannot see how to proceed on it. Let us have $M_1,\ldots,M_n$ be commuting matrices. I know that that the generalized eigenspaces are the same across the $M_i$. However, is it true that for each generalized eigenspace, $V$, where $V$ is the generalized eigenspace of $\lambda_i$ for $M_i$, there exists $v\in V$ such that $v$ is an eigenvector for all the $M_i$. A related question is if it is even true that the $M_i$ need have a common eigenvector (I see how an answer to this question implies an answer to my first question, so these seem equivalent) I have tried to come up with examples for which what I said is not true, but this has been to no avail. From the generalized eigenspaces I do not see how to proceed. Thank you for any help.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'eigenvalues-eigenvectors']"
21,solving linear equations whose matix is strictly diagonally dominant and element values decrease rapidly as they locate far from the pivots,solving linear equations whose matix is strictly diagonally dominant and element values decrease rapidly as they locate far from the pivots,,"For a certain problem I am modeling, I have a set of linear equations whose matrix $M$ is symmetric and strictly diagonally dominant ($M_{ii}=1$, for all $i$). In addition, for the upper triangular part, the elements $M_{ij}>M_{in}$ for $j<n$ as well as $M_{ij}>M_{mj}$ for $i>m$, which means the values of matrix elements decrease (in fact, rapidly) as they locate far away the pivots. I intend to neglect these elements below certain threshold (for instant $10^{-3}$) and set them to be zero. Numerical results show the accuracy but I need to prove it from equations. Could anybody give me a clue to estimate the error?","For a certain problem I am modeling, I have a set of linear equations whose matrix $M$ is symmetric and strictly diagonally dominant ($M_{ii}=1$, for all $i$). In addition, for the upper triangular part, the elements $M_{ij}>M_{in}$ for $j<n$ as well as $M_{ij}>M_{mj}$ for $i>m$, which means the values of matrix elements decrease (in fact, rapidly) as they locate far away the pivots. I intend to neglect these elements below certain threshold (for instant $10^{-3}$) and set them to be zero. Numerical results show the accuracy but I need to prove it from equations. Could anybody give me a clue to estimate the error?",,"['linear-algebra', 'matrices']"
22,Computing exponential of a $2\times 2$ matrix using only its trace and determinant.,Computing exponential of a  matrix using only its trace and determinant.,2\times 2,"I want to compute the exponential of an arbitrary $2\times 2$ matrix over $\mathbb{R}$ only using its trace and determinant. I've shown that for a traceless matrix $A$ there is the following formula: $$ \exp(A)=(\cos\sqrt{\det A})\mathbb{I}+\frac{\sin\sqrt{\det A}}{\sqrt{\det A}}A $$ Suppose $X$ is an arbitrary matrix, say $X=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$, then $X$ can be written as a sum: $$ \begin{pmatrix} a & b \\ c & d \end{pmatrix}=\underbrace{\begin{pmatrix} \frac{a-d}{2} & b \\ c & -\frac{a-d}{2}\end{pmatrix}}_{T}+\underbrace{\frac{a+d}{2}\mathbb{I}}_{S} $$ Notice that $TS=ST$, so $\exp(X)=\exp(T)\exp(S)$. Since $T$ is traceless I can use the formula and write $\exp(T)$ in terms of its determinant, but my final result needs to depend on determinant and trace of $X$ only, so $$ \det T=\frac{a-d}{2}\cdot\frac{d-a}{2}-bc=-\frac{a^2+d^2}{2}+ad-bc=-\frac{a^2+d^2}{2}+\det X $$ This is where I'm stuck, as I want to write $a^2+d^2$ in terms of $\det X=ad-bc$ and $\mathrm{tr} X=a+d$, but cannot do that.","I want to compute the exponential of an arbitrary $2\times 2$ matrix over $\mathbb{R}$ only using its trace and determinant. I've shown that for a traceless matrix $A$ there is the following formula: $$ \exp(A)=(\cos\sqrt{\det A})\mathbb{I}+\frac{\sin\sqrt{\det A}}{\sqrt{\det A}}A $$ Suppose $X$ is an arbitrary matrix, say $X=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$, then $X$ can be written as a sum: $$ \begin{pmatrix} a & b \\ c & d \end{pmatrix}=\underbrace{\begin{pmatrix} \frac{a-d}{2} & b \\ c & -\frac{a-d}{2}\end{pmatrix}}_{T}+\underbrace{\frac{a+d}{2}\mathbb{I}}_{S} $$ Notice that $TS=ST$, so $\exp(X)=\exp(T)\exp(S)$. Since $T$ is traceless I can use the formula and write $\exp(T)$ in terms of its determinant, but my final result needs to depend on determinant and trace of $X$ only, so $$ \det T=\frac{a-d}{2}\cdot\frac{d-a}{2}-bc=-\frac{a^2+d^2}{2}+ad-bc=-\frac{a^2+d^2}{2}+\det X $$ This is where I'm stuck, as I want to write $a^2+d^2$ in terms of $\det X=ad-bc$ and $\mathrm{tr} X=a+d$, but cannot do that.",,"['linear-algebra', 'matrices', 'exponentiation']"
23,Relation between trace and Ky Fan norm,Relation between trace and Ky Fan norm,,"As we know that the Ky Fan k Norm is the sum of k-th largest singular values. On the other hand, the trace of a matrix is the sum of its eigenvalues. For a N by N symmetric matrix $M$, its Ky Fan N-Norm is equal to the trace of $M$. Yet how about the matrix $M$ is square but not symmetric? Is there any relation between the trace and the Ky Fan $N$ norm?","As we know that the Ky Fan k Norm is the sum of k-th largest singular values. On the other hand, the trace of a matrix is the sum of its eigenvalues. For a N by N symmetric matrix $M$, its Ky Fan N-Norm is equal to the trace of $M$. Yet how about the matrix $M$ is square but not symmetric? Is there any relation between the trace and the Ky Fan $N$ norm?",,"['matrices', 'eigenvalues-eigenvectors', 'svd']"
24,Matrix proof using norms,Matrix proof using norms,,I have a linear algebra question I need help with. Let $A$ be an $m\times m$ matrix with $\|A\|_2 < 1$ where $\|A\|_2$ is the $2$-norm of $A$. Show that $I - A$ is invertible where $I$ is the identity matrix. I know that $\|Ax\|_2 \leq C\|x\|_2$ for some constant $C$ and a vector $x$. However I don't know the definition of $\|x\|_2$. I also don't see how this definition can help solve this problem.,I have a linear algebra question I need help with. Let $A$ be an $m\times m$ matrix with $\|A\|_2 < 1$ where $\|A\|_2$ is the $2$-norm of $A$. Show that $I - A$ is invertible where $I$ is the identity matrix. I know that $\|Ax\|_2 \leq C\|x\|_2$ for some constant $C$ and a vector $x$. However I don't know the definition of $\|x\|_2$. I also don't see how this definition can help solve this problem.,,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'normed-spaces']"
25,Minimal polynomial and Cayley hamiliton theorem,Minimal polynomial and Cayley hamiliton theorem,,"My Classmate asked me two following question this morning. But I still can't figure out proper solutions... (1) If $A \in R^{nxn}$ , satisfy $A^{3}+A+I_n=0$ , find $a,b,c\in R$ such that $(A^{2}-A-I_n)^{-1}=aI_n+bA+cA^{2}$ My Approach By Cayley Hamilton Theorem , $$0=A^{3}+A+I_n=(A^{2}-A-I_n)(A+I_n)+3A+2I_n$$ then I don't know how to reorder the equation to fit the $(A^{2}-A-I_n)^{-1}$ approach. (2) If $A\in R^{nxn}$ and $(A-I_n)(A-9I_n)(A-16I_n)=0$ , find a matrix $B=aI_n+bA+cA^2(a,b,c\in R)$ such that $B^{2}=A$ My Approach Either I try to solve it by (1) brute-force or (2) minimal polynomial , I have no idea how to get the proper solution. The only thing I know is that minimal polynomial is only satisfied while A is diagonalizable. Is there any proper solution? or which step should start with?","My Classmate asked me two following question this morning. But I still can't figure out proper solutions... (1) If , satisfy , find such that My Approach By Cayley Hamilton Theorem , then I don't know how to reorder the equation to fit the approach. (2) If and , find a matrix such that My Approach Either I try to solve it by (1) brute-force or (2) minimal polynomial , I have no idea how to get the proper solution. The only thing I know is that minimal polynomial is only satisfied while A is diagonalizable. Is there any proper solution? or which step should start with?","A \in R^{nxn} A^{3}+A+I_n=0 a,b,c\in R (A^{2}-A-I_n)^{-1}=aI_n+bA+cA^{2} 0=A^{3}+A+I_n=(A^{2}-A-I_n)(A+I_n)+3A+2I_n (A^{2}-A-I_n)^{-1} A\in R^{nxn} (A-I_n)(A-9I_n)(A-16I_n)=0 B=aI_n+bA+cA^2(a,b,c\in R) B^{2}=A","['linear-algebra', 'matrices']"
26,"Dim E, set of linear transformations","Dim E, set of linear transformations",,"Suppose $U\subset W \subset V$ are three linear spaces with respective dimensions 3, 6 and 10. Let $E\subset L(V,V)$ be the set of linear transformations $f:V\rightarrow V$ such that $f(U)\subset U$ and $f(W)\subset W$. Calculate $\dim\, E$.","Suppose $U\subset W \subset V$ are three linear spaces with respective dimensions 3, 6 and 10. Let $E\subset L(V,V)$ be the set of linear transformations $f:V\rightarrow V$ such that $f(U)\subset U$ and $f(W)\subset W$. Calculate $\dim\, E$.",,"['linear-algebra', 'matrices']"
27,Matrix norm inequality implying eigenvector norm inequality,Matrix norm inequality implying eigenvector norm inequality,,"For a matrix $A$ let $\|A\|$ be the norm given by $\|A\|=\sup_{v \neq 0}\frac{\|Av\|}{\|v\|}$ where $\|v\|$ is the Euclidian norm on the vector $v$. Suppose we have matrices $M$ and $S$ with leading eigenvectors $u$ and $\hat{u}$ respectively. If we have  $\|M-S\| \leq \theta\|M\|$ does this imply $\|\hat{u}-u\|\leq 2\theta$, and if so, how? This comes from the paragraph containing equation (1) on page 2 of this paper http://www.stanford.edu/~montanar/RESEARCH/FILEPAP/GossipPCA.pdf but I don't see how the result is ""immediate"" as they claim. In response to user1551, $M$ is an $n \times n$ symmetric matrix, $u$ is ""the"" eigenvector of $M$ corresponding to the eigenvalue of largest magnitude. $S$ is some sparsification of $M$ obtained by setting each entry of $M$ to $0$ independently with probability $1-p$ then rescaling non-zero entries by $1/p$. $\hat{u}$ is ""the leading eigenvector"" of $S$. Let's assume the entries of $M$ are all positive, then by Perron-Frobenius say that $u$ and $\hat{u}$ are non-negative unit eigenvectors, if that makes it easier.","For a matrix $A$ let $\|A\|$ be the norm given by $\|A\|=\sup_{v \neq 0}\frac{\|Av\|}{\|v\|}$ where $\|v\|$ is the Euclidian norm on the vector $v$. Suppose we have matrices $M$ and $S$ with leading eigenvectors $u$ and $\hat{u}$ respectively. If we have  $\|M-S\| \leq \theta\|M\|$ does this imply $\|\hat{u}-u\|\leq 2\theta$, and if so, how? This comes from the paragraph containing equation (1) on page 2 of this paper http://www.stanford.edu/~montanar/RESEARCH/FILEPAP/GossipPCA.pdf but I don't see how the result is ""immediate"" as they claim. In response to user1551, $M$ is an $n \times n$ symmetric matrix, $u$ is ""the"" eigenvector of $M$ corresponding to the eigenvalue of largest magnitude. $S$ is some sparsification of $M$ obtained by setting each entry of $M$ to $0$ independently with probability $1-p$ then rescaling non-zero entries by $1/p$. $\hat{u}$ is ""the leading eigenvector"" of $S$. Let's assume the entries of $M$ are all positive, then by Perron-Frobenius say that $u$ and $\hat{u}$ are non-negative unit eigenvectors, if that makes it easier.",,"['matrices', 'inequality', 'normed-spaces']"
28,"What is $\lim_{t\to +\infty} \exp(-A-tB)$ for $0\leq A,B\in M_n(\mathbb C)$?",What is  for ?,"\lim_{t\to +\infty} \exp(-A-tB) 0\leq A,B\in M_n(\mathbb C)","Questions Let $\mathcal B(\mathbb C^n)$ be the space of linear operator on $\mathbb C^n$. Let $A$ and $B\in \mathcal B(\mathbb C^n)$ self-adjoint, i.e., $A=A^*$ and $B=B^*$, non-negative, i.e., $A\geq 0$ and $B\geq 0$ ($A\geq 0\, :\Leftrightarrow \, \big(\forall x\in \mathbb C^n\,,\quad\big\langle x,Ax\big\rangle\geq0\big)$). Questions Does the limit $$\lim_{t\to \infty} \exp(-A-tB)$$ exist? If this limit exists, what is this limit in terms of $A$ and $B$? What I did up to now In the case $AB=BA$, $A$ and $A$ can be simultaneously diagonalized and with $$\mathcal H_2:=span(B)\,,\quad \mathcal H_1:=\mathcal H_2^\perp$$ we get $$\lim_{t\to +\infty} \exp(-A-tB)=\exp(-A\big|_{\mathcal H_1})$$ with $A\big|_{\mathcal H_1}$ the endomorphism induced on $\mathcal H_1$ by $A$. If I don't suppose $AB=BA$ I expect that the limit still exists, but $\mathcal H_1$ is not preserved by $A$ anymore. With $\tilde A\in \mathcal B(\mathcal H_1)$ defined by $$\forall x_1,y_1 \in \mathcal H_1\,,\quad \big\langle x_1,\tilde A y_1\big\rangle=\big\langle x_1,Ay_1\big\rangle$$ I expect that $$\lim_{t\to +\infty} \exp(-A-tB)=\exp(-\tilde A)\oplus 0 \in \mathcal B(\mathcal H_1 \oplus\mathcal H_2)=\mathcal B(\mathbb C^n)\,,$$ but I have no idea how to prove it.","Questions Let $\mathcal B(\mathbb C^n)$ be the space of linear operator on $\mathbb C^n$. Let $A$ and $B\in \mathcal B(\mathbb C^n)$ self-adjoint, i.e., $A=A^*$ and $B=B^*$, non-negative, i.e., $A\geq 0$ and $B\geq 0$ ($A\geq 0\, :\Leftrightarrow \, \big(\forall x\in \mathbb C^n\,,\quad\big\langle x,Ax\big\rangle\geq0\big)$). Questions Does the limit $$\lim_{t\to \infty} \exp(-A-tB)$$ exist? If this limit exists, what is this limit in terms of $A$ and $B$? What I did up to now In the case $AB=BA$, $A$ and $A$ can be simultaneously diagonalized and with $$\mathcal H_2:=span(B)\,,\quad \mathcal H_1:=\mathcal H_2^\perp$$ we get $$\lim_{t\to +\infty} \exp(-A-tB)=\exp(-A\big|_{\mathcal H_1})$$ with $A\big|_{\mathcal H_1}$ the endomorphism induced on $\mathcal H_1$ by $A$. If I don't suppose $AB=BA$ I expect that the limit still exists, but $\mathcal H_1$ is not preserved by $A$ anymore. With $\tilde A\in \mathcal B(\mathcal H_1)$ defined by $$\forall x_1,y_1 \in \mathcal H_1\,,\quad \big\langle x_1,\tilde A y_1\big\rangle=\big\langle x_1,Ay_1\big\rangle$$ I expect that $$\lim_{t\to +\infty} \exp(-A-tB)=\exp(-\tilde A)\oplus 0 \in \mathcal B(\mathcal H_1 \oplus\mathcal H_2)=\mathcal B(\mathbb C^n)\,,$$ but I have no idea how to prove it.",,"['linear-algebra', 'matrices', 'limits']"
29,Generating random commuting hermitian matrices,Generating random commuting hermitian matrices,,"How can I generate random commuting hermitian matrices ? EDIT: Another question: given a certain hermitian matrix, how can I generate a random hermitian matrix which commutes with it?","How can I generate random commuting hermitian matrices ? EDIT: Another question: given a certain hermitian matrix, how can I generate a random hermitian matrix which commutes with it?",,"['matrices', 'algorithms', 'random-matrices', 'hermitian-matrices']"
30,Similar matrices - find a matrix $T$,Similar matrices - find a matrix,T,"The matrices $A=\begin{pmatrix}5 & -3 \\ 4 & -2\end{pmatrix}$ and $B=\begin{pmatrix}-1 & 1\\-6 & 4\end{pmatrix}$ are similar. By knowing that similar matrices have the same eigenvalues, find a matrix $T$ such that $A=TBT^{-1}.$ any idea or proof is welcome :) thanks .","The matrices $A=\begin{pmatrix}5 & -3 \\ 4 & -2\end{pmatrix}$ and $B=\begin{pmatrix}-1 & 1\\-6 & 4\end{pmatrix}$ are similar. By knowing that similar matrices have the same eigenvalues, find a matrix $T$ such that $A=TBT^{-1}.$ any idea or proof is welcome :) thanks .",,['matrices']
31,Derivative of a trace,Derivative of a trace,,"I'm new here, so ""Hi"" to everyone :D I got the following problem.  I have the matrices $A$, $B$, $C$, $X$ and $Y$. All matrices are square (say n-by-n). In particular: - $A$ is full rank - $B$ is symmetric and (semi)definite positive; - $C$ is diagonal and definite positive; - $Y$ is diagonal and definite positive; - $X$ is diagonal ($X = \operatorname{diag}\{x_1, \ldots,x_n\}$) and it is the unknown matrix; Then I have the following function: $f(X) = (A(B+X^{T}YX)^{-1}A^{T} + C)^{-1}$ (it may seem dumb to write $X^{T}$ since it is diagonal, but I think this is the best way to write it). I would like to evaluate the derivative of the trace of $f(X)$ with respect to each $x_i$. Any idea?","I'm new here, so ""Hi"" to everyone :D I got the following problem.  I have the matrices $A$, $B$, $C$, $X$ and $Y$. All matrices are square (say n-by-n). In particular: - $A$ is full rank - $B$ is symmetric and (semi)definite positive; - $C$ is diagonal and definite positive; - $Y$ is diagonal and definite positive; - $X$ is diagonal ($X = \operatorname{diag}\{x_1, \ldots,x_n\}$) and it is the unknown matrix; Then I have the following function: $f(X) = (A(B+X^{T}YX)^{-1}A^{T} + C)^{-1}$ (it may seem dumb to write $X^{T}$ since it is diagonal, but I think this is the best way to write it). I would like to evaluate the derivative of the trace of $f(X)$ with respect to each $x_i$. Any idea?",,"['linear-algebra', 'matrices', 'derivatives', 'trace']"
32,Finding an idempotent that satisfies certain conditions in a matrix ring.,Finding an idempotent that satisfies certain conditions in a matrix ring.,,"I've been stuck on a problem, and I was wondering if anyone could help me out. The problem is: Let $R$ be the $2 \times 2$ matrix ring over the reals $\mathbb{R}$ of the form   $$ \begin{bmatrix}a & b \\0 & c\end{bmatrix}, $$   where $a, b, c \in \mathbb{R}$. Find an idempotent $e$ in $R$ such that $eRe$ is a field, but the right ideal $eR$ is not minimal. I was thinking of using $e=\begin{bmatrix}0 & 1 \\0 & 1\end{bmatrix}$, which is idempotent. I also showed $eRe$ is a field, but I'm not sure how to show the right ideal $eR$ is not minimal. If this $e$ doesn't work, I also tried $e=\begin{bmatrix}1 &0 \\0 & 0\end{bmatrix}$, but once again, I'm not sure how to show $eR$ is not minimal. Any help would be greatly appreciated. Thanks!","I've been stuck on a problem, and I was wondering if anyone could help me out. The problem is: Let $R$ be the $2 \times 2$ matrix ring over the reals $\mathbb{R}$ of the form   $$ \begin{bmatrix}a & b \\0 & c\end{bmatrix}, $$   where $a, b, c \in \mathbb{R}$. Find an idempotent $e$ in $R$ such that $eRe$ is a field, but the right ideal $eR$ is not minimal. I was thinking of using $e=\begin{bmatrix}0 & 1 \\0 & 1\end{bmatrix}$, which is idempotent. I also showed $eRe$ is a field, but I'm not sure how to show the right ideal $eR$ is not minimal. If this $e$ doesn't work, I also tried $e=\begin{bmatrix}1 &0 \\0 & 0\end{bmatrix}$, but once again, I'm not sure how to show $eR$ is not minimal. Any help would be greatly appreciated. Thanks!",,"['abstract-algebra', 'matrices', 'ring-theory', 'ideals']"
33,Characterize triangular matrices by its eigenvalues?,Characterize triangular matrices by its eigenvalues?,,"For a triangular matrix, its diagonal entries are eigenvalues repeated with algebraic multiplicities. I wonder if the reverse is true. In other words,  a matrix whose diagonal entries are eigenvalues repeated with algebraic multiplicities must be triangular? Thanks!","For a triangular matrix, its diagonal entries are eigenvalues repeated with algebraic multiplicities. I wonder if the reverse is true. In other words,  a matrix whose diagonal entries are eigenvalues repeated with algebraic multiplicities must be triangular? Thanks!",,['matrices']
34,Characterize unitary matrices by their eigenvalues and/or eigenvectors?,Characterize unitary matrices by their eigenvalues and/or eigenvectors?,,Every eigenvalue of a unitary matrix has absolute value 1. I was     wondering whether a matrix whose eigenvalues all have absolute value 1     must be unitary? Thanks!,Every eigenvalue of a unitary matrix has absolute value 1. I was     wondering whether a matrix whose eigenvalues all have absolute value 1     must be unitary? Thanks!,,['matrices']
35,Is the generalized eigenvalue problem only for square matrices?,Is the generalized eigenvalue problem only for square matrices?,,"From Wikipedia : A generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeys   $$     A\mathbf{v} = \lambda B \mathbf{v} \quad \quad $$   where $A$ and $B$ are matrices. I was wondering if $A$ and $B$ are required to be square matrices? The definition doesn't seem to require this, but the next sentence does The possible values of $λ$ must obey the following equation   $$     \det(A - \lambda B)=0.\,  $$ Thanks!","From Wikipedia : A generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeys   $$     A\mathbf{v} = \lambda B \mathbf{v} \quad \quad $$   where $A$ and $B$ are matrices. I was wondering if $A$ and $B$ are required to be square matrices? The definition doesn't seem to require this, but the next sentence does The possible values of $λ$ must obey the following equation   $$     \det(A - \lambda B)=0.\,  $$ Thanks!",,"['linear-algebra', 'matrices']"
36,jordan Canonical Form for 8x8 matrix.,jordan Canonical Form for 8x8 matrix.,,"This is one of the prelim questions I am having trouble with. Also I need some Jordan Canonical examples similar to the following question. Thank you an advance. We need to find a Jordan Canonical Form for an 8x8 matrix A such that $(A-I) $ has nullity 2, $(A-I)^2 $ has nullity $4$, $ (A-I)^k $ has nullity $5$ for $ k \ge 3 $, and $(A+2I)^j $ has nullity $3$ for  $ j \ge 1 $ I just know, A has $2$ Jordan blocks for eigenvalue $1$ since $\dim(\operatorname{ker}(A-I))=2$ and $3$ Jordan blocks for eigenvalue $-2$. I need help to understand this question. Thank you.","This is one of the prelim questions I am having trouble with. Also I need some Jordan Canonical examples similar to the following question. Thank you an advance. We need to find a Jordan Canonical Form for an 8x8 matrix A such that $(A-I) $ has nullity 2, $(A-I)^2 $ has nullity $4$, $ (A-I)^k $ has nullity $5$ for $ k \ge 3 $, and $(A+2I)^j $ has nullity $3$ for  $ j \ge 1 $ I just know, A has $2$ Jordan blocks for eigenvalue $1$ since $\dim(\operatorname{ker}(A-I))=2$ and $3$ Jordan blocks for eigenvalue $-2$. I need help to understand this question. Thank you.",,"['linear-algebra', 'matrices']"
37,Are Trace of product of matrices- distributive/associative?,Are Trace of product of matrices- distributive/associative?,,"Is $\operatorname{Tr}(X^TAX)-\operatorname{Tr}(X^TBX)$ equal to $\operatorname{Tr}(X^TCX)$, where $C=A-B$ and $A$, $B$, $X$ have real entries and also $A$ and $B$ are p.s.d.","Is $\operatorname{Tr}(X^TAX)-\operatorname{Tr}(X^TBX)$ equal to $\operatorname{Tr}(X^TCX)$, where $C=A-B$ and $A$, $B$, $X$ have real entries and also $A$ and $B$ are p.s.d.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'trace']"
38,Matrix factorization as a product of triangular matrices,Matrix factorization as a product of triangular matrices,,"Is it true that any square matrix $A $ can be factorized as a product of only triangular matrices?  That is, can we write $A $ as $\prod_{i=1}^k B_i$, where every $B_i$ is either a lower or an upper triangular matrix (for some natural $ k $)? Note $ k $ is not assumed to be 2 above. So the question is not (directly) about the $LU$ decomposition. Comment : We know that for any square matrix $A$ we have: $A=PLU$ where $P$  is a permutation matrix, $L$ is a lower triangular matrix and $U$ is an upper triangular matrix. So the question possibly boils down to whether any permutation matrix can be factorized to triangular matrices.","Is it true that any square matrix $A $ can be factorized as a product of only triangular matrices?  That is, can we write $A $ as $\prod_{i=1}^k B_i$, where every $B_i$ is either a lower or an upper triangular matrix (for some natural $ k $)? Note $ k $ is not assumed to be 2 above. So the question is not (directly) about the $LU$ decomposition. Comment : We know that for any square matrix $A$ we have: $A=PLU$ where $P$  is a permutation matrix, $L$ is a lower triangular matrix and $U$ is an upper triangular matrix. So the question possibly boils down to whether any permutation matrix can be factorized to triangular matrices.",,"['linear-algebra', 'matrices']"
39,"Why for any doubly stochastic matrix, there exist a permutation $\pi$, such that $A_{[i,\pi(i)]}\ne 0$?","Why for any doubly stochastic matrix, there exist a permutation , such that ?","\pi A_{[i,\pi(i)]}\ne 0","Or put it simply, for any $n$-by-$n$ doubly stochastic matrix $A$, you can always find $n$ non-zero entries in $A$, none of them lies in the same row or column. Why is that?","Or put it simply, for any $n$-by-$n$ doubly stochastic matrix $A$, you can always find $n$ non-zero entries in $A$, none of them lies in the same row or column. Why is that?",,['matrices']
40,Dominant Eigenvalues,Dominant Eigenvalues,,"I'm trying to understand dominant eigenvalues and I found this website that has a explanation of it (Definition 9.2). In the example, the power method is used to find the dominant eigenvector which correspondes to the eigenvalue of 1. When I calculate the eigenvalues and vectors of the matrix in the example, I got this result: The first line are the eigenvalues and the second row the eigenvectors. As you can see, the eigenvector that corresponde to to the eigenvalue of 1 is {-0.577, -0.577, -0.577} If I calculate the powers of the matrix, I find that after M^9, it converges as shown in the website I don't understand what is the difference between the eigenvector that I found that corresponde to to the eigenvalue of 1 and the eigenvector that is found after elevating the matrix many times, and that the website described also as the eigenvector of eigenvalue 1.","I'm trying to understand dominant eigenvalues and I found this website that has a explanation of it (Definition 9.2). In the example, the power method is used to find the dominant eigenvector which correspondes to the eigenvalue of 1. When I calculate the eigenvalues and vectors of the matrix in the example, I got this result: The first line are the eigenvalues and the second row the eigenvectors. As you can see, the eigenvector that corresponde to to the eigenvalue of 1 is {-0.577, -0.577, -0.577} If I calculate the powers of the matrix, I find that after M^9, it converges as shown in the website I don't understand what is the difference between the eigenvector that I found that corresponde to to the eigenvalue of 1 and the eigenvector that is found after elevating the matrix many times, and that the website described also as the eigenvector of eigenvalue 1.",,"['matrices', 'eigenvalues-eigenvectors']"
41,Characterizing the eigenvalues of matrix powers,Characterizing the eigenvalues of matrix powers,,"I can see that if $c$ is an eigenvalue of a matrix $A$, then $c^k$ will be an eigenvalue for the matrix $A^k$, but I'm curious about the reverse case. I checked for a simple rotation matrix counterclockwise by $180$ with single eigenvalue $-1$ that the rotation matrix counterclockwise by $\pi/2$ has eigenvalues $\pm i$. I don't expect it to be the case, though, that if $c$ is an eigenvalue for $A^k$ ($k > 1$), that $c^{1/k}$ are eigenvalues of $A$. The example I think of to contradict this is taking a $2 \times 2$ rotation matrix by $\pi / 3$, $A$, whose cube will have eigenvalue $-1$, which has three cube roots, which exceeds the possible number of eigenvalues of $A$. Is there, however, a more intuitive way to see this, or a weaker result which holds relating the eigenvalues in the reverse direction?","I can see that if $c$ is an eigenvalue of a matrix $A$, then $c^k$ will be an eigenvalue for the matrix $A^k$, but I'm curious about the reverse case. I checked for a simple rotation matrix counterclockwise by $180$ with single eigenvalue $-1$ that the rotation matrix counterclockwise by $\pi/2$ has eigenvalues $\pm i$. I don't expect it to be the case, though, that if $c$ is an eigenvalue for $A^k$ ($k > 1$), that $c^{1/k}$ are eigenvalues of $A$. The example I think of to contradict this is taking a $2 \times 2$ rotation matrix by $\pi / 3$, $A$, whose cube will have eigenvalue $-1$, which has three cube roots, which exceeds the possible number of eigenvalues of $A$. Is there, however, a more intuitive way to see this, or a weaker result which holds relating the eigenvalues in the reverse direction?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
42,Is product NM positive definite when N is a diagonal positive definite matrix and M is an asymmetric positive definite matrix,Is product NM positive definite when N is a diagonal positive definite matrix and M is an asymmetric positive definite matrix,,"I have the following question: Matrix $N$ is a diagonal matrix with all entries strictly positive (hence, $N$ is positive definite since it satisfies $x^T N x > 0$). Matrix $M$ is an asymmetric positive definite matrix with all entries non-negative. Since $NM \neq MN$, it does not follow that the product $NM$ is positive definite. However, given the special structure of $N$, can we still show that $NM$ is positive definite? Or maybe, under certain additional conditions?","I have the following question: Matrix $N$ is a diagonal matrix with all entries strictly positive (hence, $N$ is positive definite since it satisfies $x^T N x > 0$). Matrix $M$ is an asymmetric positive definite matrix with all entries non-negative. Since $NM \neq MN$, it does not follow that the product $NM$ is positive definite. However, given the special structure of $N$, can we still show that $NM$ is positive definite? Or maybe, under certain additional conditions?",,['matrices']
43,Safe use of generalized inverses,Safe use of generalized inverses,,"Suppose I'm given a linear system $$Ax=b,$$ with unknown $x\in\mathbb{R}^n$, and some symmetric $A\in\mathbb{R}^{n\times n}$ and $b=\in\mathbb{R}^n$. Furthermore, it is known that $A$ is not full-rank matrix, and that its rank is $n-1$; therefore, $A$ is not invertible. However, to compute the ""solution"" $x$, one may use $x=A^+b$, where $A^+$ is a generalized inverse of $A$, i.e., Moore-Penrose inverse. What is the characteristic of such solution? More precisely, under which conditions will $x=A^+b$ give the exact solution to the system (supposing the exact solution exists)? Could one state that in the above case, with additional note that $b$ is orthogonal to null-space of $A$, the generalized inverse will yield the exact solution to the system?","Suppose I'm given a linear system $$Ax=b,$$ with unknown $x\in\mathbb{R}^n$, and some symmetric $A\in\mathbb{R}^{n\times n}$ and $b=\in\mathbb{R}^n$. Furthermore, it is known that $A$ is not full-rank matrix, and that its rank is $n-1$; therefore, $A$ is not invertible. However, to compute the ""solution"" $x$, one may use $x=A^+b$, where $A^+$ is a generalized inverse of $A$, i.e., Moore-Penrose inverse. What is the characteristic of such solution? More precisely, under which conditions will $x=A^+b$ give the exact solution to the system (supposing the exact solution exists)? Could one state that in the above case, with additional note that $b$ is orthogonal to null-space of $A$, the generalized inverse will yield the exact solution to the system?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
44,How to find matrix form of an operator on a vector space V which is direct sum of its two subspaces?,How to find matrix form of an operator on a vector space V which is direct sum of its two subspaces?,,"I am studying a lecture notes where I found this result: Let $A$ be an operator in vector space $V$ . If $V$ is equal to direct sum of its two subspace $U$ and $W$ ie $V = U\bigoplus W$, and if the basis $\{e_1,. . . . .e_n\}$of $V$  is such that the first $k$ vectors $\{e_1,. . . . .e_k\}$ is basis of $U$ and vectors $\{e_{k+1},...,e_n\}$ is a basis of $W$, Then the matrix of $A$ has the following form $\{\left(     \begin{array}{cc}       B & 0 \\       0 & C \\     \end{array}   \right)\}$ It has not mentioned anything about matrix $B$ and $C$. I am finding it difficult to understand this. Could anybody help me with this. Thanks a ton","I am studying a lecture notes where I found this result: Let $A$ be an operator in vector space $V$ . If $V$ is equal to direct sum of its two subspace $U$ and $W$ ie $V = U\bigoplus W$, and if the basis $\{e_1,. . . . .e_n\}$of $V$  is such that the first $k$ vectors $\{e_1,. . . . .e_k\}$ is basis of $U$ and vectors $\{e_{k+1},...,e_n\}$ is a basis of $W$, Then the matrix of $A$ has the following form $\{\left(     \begin{array}{cc}       B & 0 \\       0 & C \\     \end{array}   \right)\}$ It has not mentioned anything about matrix $B$ and $C$. I am finding it difficult to understand this. Could anybody help me with this. Thanks a ton",,"['linear-algebra', 'matrices', 'functional-analysis']"
45,Intersection of the sets of generalized doubly stochastic matrices and of orthogonal matrices,Intersection of the sets of generalized doubly stochastic matrices and of orthogonal matrices,,"The definition of a doubly stochastic matrix can be found here . We say a square matrix $A$ is a generalized doubly stochastic matrix if the sums of each rows and columns of $A$ all equal $1$ , but $A$ doesn't have to be non-negative. An interesting fact (which is also easy to prove) about doubly stochastic matrices is: if $A$ is doubly stochastic and orthogonal, then $A$ is actually a permutation matrix. What is the intersection set for a generalized doubly stochastic matrix set and orthogonal matrix set? More specifically, can any one give me an example of an $N \times N$ matrix $A$ , which satisfy the following constraints: $AA^T=I$ $A 1=1$ $A^T 1=1$ there exists at least one entry $A_{i,j}$ , satisfying $A_{i,j}<0$","The definition of a doubly stochastic matrix can be found here . We say a square matrix is a generalized doubly stochastic matrix if the sums of each rows and columns of all equal , but doesn't have to be non-negative. An interesting fact (which is also easy to prove) about doubly stochastic matrices is: if is doubly stochastic and orthogonal, then is actually a permutation matrix. What is the intersection set for a generalized doubly stochastic matrix set and orthogonal matrix set? More specifically, can any one give me an example of an matrix , which satisfy the following constraints: there exists at least one entry , satisfying","A A 1 A A A N \times N A AA^T=I A 1=1 A^T 1=1 A_{i,j} A_{i,j}<0","['linear-algebra', 'matrices', 'orthogonal-matrices']"
46,Linear algebra exam questions,Linear algebra exam questions,,"Pick out the true statements: There exist $n\times n$ matrices $A$ and $B$ with real entries such that $(I-(AB-BA)^n) = 0$. If $A$ is symmetric and positive definite matrix then $tr(A)^n\geq n^n \det(A)$.  :( I am stucked, unable to solve this problem.","Pick out the true statements: There exist $n\times n$ matrices $A$ and $B$ with real entries such that $(I-(AB-BA)^n) = 0$. If $A$ is symmetric and positive definite matrix then $tr(A)^n\geq n^n \det(A)$.  :( I am stucked, unable to solve this problem.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
47,If $null(A) \subset null(B)$ can we draw any conclusion about range spaces of $A$ and $B$?,If  can we draw any conclusion about range spaces of  and ?,null(A) \subset null(B) A B,$A$ and $B$ are given $n\times m$ matrices. If $null(A) \subset null(B)$ what conclusion can we draw about range space of $A$ and $B$ ? Can we conclude that range space of $B$ is contained in a range space of $A$ ?,and are given matrices. If what conclusion can we draw about range space of and ? Can we conclude that range space of is contained in a range space of ?,A B n\times m null(A) \subset null(B) A B B A,"['linear-algebra', 'matrices']"
48,Derivation of the derivative of a square matrix w.r.t. a vector,Derivation of the derivative of a square matrix w.r.t. a vector,,"So I have gotten stumped on something that seems like it (should?) be easy. I am trying to find the following derivative shown below. I have scoured the wiki link on matrix derivatives , and I think my answer is correct but I want to make sure. So let us say we have a square matrix $\boldsymbol{A}$, and a vector $\boldsymbol{\theta}$. (I am assuming here that the dimensionality is 2 for ease. So: $$\boldsymbol{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} &a_{22}\end{bmatrix}, \boldsymbol{\theta}= \begin{bmatrix} \theta_0 \\ \theta_1\end{bmatrix}$$ I am trying to derive how we get: $$ \frac{\delta \boldsymbol{A}\boldsymbol{\theta}}{\delta \boldsymbol{\theta}}= \boldsymbol{A} $$ So first I tried to 'open up' the matrix-vector product, so I now have the following matrix: $$ \begin{bmatrix} a_{11}\theta_{0} + a_{12}\theta_{1} \\ a_{21}\theta_{0} + a_{22}\theta_{1} \end{bmatrix}_{2x1} $$ ... and this is where I am stuck. How do I show from here, that the derivative of the above is indeed equal to $\boldsymbol{A}$? I know that I have to take the partials, but I cannot seem to find a rule governing in what ordering of columns/rows those partials must be taken.","So I have gotten stumped on something that seems like it (should?) be easy. I am trying to find the following derivative shown below. I have scoured the wiki link on matrix derivatives , and I think my answer is correct but I want to make sure. So let us say we have a square matrix $\boldsymbol{A}$, and a vector $\boldsymbol{\theta}$. (I am assuming here that the dimensionality is 2 for ease. So: $$\boldsymbol{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} &a_{22}\end{bmatrix}, \boldsymbol{\theta}= \begin{bmatrix} \theta_0 \\ \theta_1\end{bmatrix}$$ I am trying to derive how we get: $$ \frac{\delta \boldsymbol{A}\boldsymbol{\theta}}{\delta \boldsymbol{\theta}}= \boldsymbol{A} $$ So first I tried to 'open up' the matrix-vector product, so I now have the following matrix: $$ \begin{bmatrix} a_{11}\theta_{0} + a_{12}\theta_{1} \\ a_{21}\theta_{0} + a_{22}\theta_{1} \end{bmatrix}_{2x1} $$ ... and this is where I am stuck. How do I show from here, that the derivative of the above is indeed equal to $\boldsymbol{A}$? I know that I have to take the partials, but I cannot seem to find a rule governing in what ordering of columns/rows those partials must be taken.",,"['matrices', 'derivatives']"
49,How to solve a linear algebra homework problem?,How to solve a linear algebra homework problem?,,"Let $K$ be a field, suppose that $D\colon M_{n\times n}(K) \to K$ is a function such that $D(AB)=D(A)\cdot D(B)$ and $D(I) \neq D(0)$, where $0$ is the zero matrix. Show that if $\operatorname{rank}(A) < n$, then $D(A)=0$. My consideration is that: first by $D(0)=D(0)D(I)$ and $D(0)\neq D(I)$, I can show $D(0)=0$ and $D(I)=1$. then I want to show $D(I_k)=0$, where $I_k $ is $n\times n$ diagonal matrix with k diagonal entries equal to $1$ and others $0$. Then $D(A)=D(P^{-1}I_kP)=0$. However, I fail to prove $D(I_k)=0$. Any suggestions? Thanks a lot","Let $K$ be a field, suppose that $D\colon M_{n\times n}(K) \to K$ is a function such that $D(AB)=D(A)\cdot D(B)$ and $D(I) \neq D(0)$, where $0$ is the zero matrix. Show that if $\operatorname{rank}(A) < n$, then $D(A)=0$. My consideration is that: first by $D(0)=D(0)D(I)$ and $D(0)\neq D(I)$, I can show $D(0)=0$ and $D(I)=1$. then I want to show $D(I_k)=0$, where $I_k $ is $n\times n$ diagonal matrix with k diagonal entries equal to $1$ and others $0$. Then $D(A)=D(P^{-1}I_kP)=0$. However, I fail to prove $D(I_k)=0$. Any suggestions? Thanks a lot",,"['linear-algebra', 'matrices']"
50,Hermitian positive semi-definite-Square root,Hermitian positive semi-definite-Square root,,"Problem: Let $A$ be a Hermitian positive semi-definite $n$ by $n$ matrix (The field of scalars is $\mathbb{C}$). Let $B$ be an $n$ by $n$ matrix that commutes with A. Prove that $B$ and $\sqrt{A}$ commute. I started to solve the problem like this: Since $A$ is a Hermitian positive semi-definite matrix, then there exits a unitary matrix $J$ such that: $$J^{-1}AJ=D=diag(\lambda _{1},\lambda _{2},...,\lambda _{n})$$ where: $\lambda _{i}\geq 0$ and $\lambda _{i}$ are eigenvalues of $A$, and since $A$ is Hermitian, then the eigenvalues are real. Let $$F=\sqrt{A}=JCJ^{-1}$$ where $C=\sqrt{D}=diag\left ( \sqrt{\lambda _{1}},\sqrt{\lambda _{2}},...,\sqrt{\lambda _{n}} \right )$, and $\sqrt{\lambda _{i}}$ are eigenvalues of $F=\sqrt{A}$. $AB=BA\Rightarrow JDJ^{-1}B=BJDJ^{-1}$ So, I need to use the above equality to prove this one: $$FB=BF\Rightarrow JCJ^{-1}B=BJCJ^{-1}\Rightarrow ...$$ Please let me know how I should finish my proof. Also, if anyone has a different solution, please share. Thanks","Problem: Let $A$ be a Hermitian positive semi-definite $n$ by $n$ matrix (The field of scalars is $\mathbb{C}$). Let $B$ be an $n$ by $n$ matrix that commutes with A. Prove that $B$ and $\sqrt{A}$ commute. I started to solve the problem like this: Since $A$ is a Hermitian positive semi-definite matrix, then there exits a unitary matrix $J$ such that: $$J^{-1}AJ=D=diag(\lambda _{1},\lambda _{2},...,\lambda _{n})$$ where: $\lambda _{i}\geq 0$ and $\lambda _{i}$ are eigenvalues of $A$, and since $A$ is Hermitian, then the eigenvalues are real. Let $$F=\sqrt{A}=JCJ^{-1}$$ where $C=\sqrt{D}=diag\left ( \sqrt{\lambda _{1}},\sqrt{\lambda _{2}},...,\sqrt{\lambda _{n}} \right )$, and $\sqrt{\lambda _{i}}$ are eigenvalues of $F=\sqrt{A}$. $AB=BA\Rightarrow JDJ^{-1}B=BJDJ^{-1}$ So, I need to use the above equality to prove this one: $$FB=BF\Rightarrow JCJ^{-1}B=BJCJ^{-1}\Rightarrow ...$$ Please let me know how I should finish my proof. Also, if anyone has a different solution, please share. Thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inner-products']"
51,Pointwise product of real symmetric nonnegative matrices,Pointwise product of real symmetric nonnegative matrices,,"I have encoutered several times the following claim : Let $A$ and $B$ be two real symmetric matrices (of dimension $n\times n$) with nonnegative coefficients and such that their eigenvalues are nonnegative. Let $C$ be their pointwise product, i.e. the $n\times n$ matrix with coefficients $C_{ij}=A_{ij}B_{ij}$. Then the matrix $C$ has nonnegative eigenvalues. It seems to be true, but I can't find a proof. Does anybody know how to show this ? Also, this result implies that if we denote by $A^{[n]}$ the matrix with coefficients $A^{[n]}_{ij}=(A_{ij})^n$ with $n\in \mathbb{N}$, then $A^{[n]}$ has nonnegative eigenvalues. Does this result still hold if we only suppose that $n\in\mathbb{R}^+$ ? If it doesn't hold anymore, is there a sufficient condition weaker than $n\in\mathbb{N}$ under which the result holds ?","I have encoutered several times the following claim : Let $A$ and $B$ be two real symmetric matrices (of dimension $n\times n$) with nonnegative coefficients and such that their eigenvalues are nonnegative. Let $C$ be their pointwise product, i.e. the $n\times n$ matrix with coefficients $C_{ij}=A_{ij}B_{ij}$. Then the matrix $C$ has nonnegative eigenvalues. It seems to be true, but I can't find a proof. Does anybody know how to show this ? Also, this result implies that if we denote by $A^{[n]}$ the matrix with coefficients $A^{[n]}_{ij}=(A_{ij})^n$ with $n\in \mathbb{N}$, then $A^{[n]}$ has nonnegative eigenvalues. Does this result still hold if we only suppose that $n\in\mathbb{R}^+$ ? If it doesn't hold anymore, is there a sufficient condition weaker than $n\in\mathbb{N}$ under which the result holds ?",,"['linear-algebra', 'matrices']"
52,Positive semi-definite matrix,Positive semi-definite matrix,,"Suppose a square symmetric matrix $V$ is given $V=\left(\begin{array}{ccccc}  \sum w_{1s} &   &    &  & \\    & \ddots &   & -w_{ij} \\    &  & \ddots & & \\    & -w_{ij} &   & \ddots & \\    & & & & \sum w_{ns}  \end{array}\right) \in\mathbb{R}^{n\times n},$ with values $w_{ij}> 0$, hence with only positive diagonal entries. Since the above matrix is diagonally dominant, it is positive semi-definite. However, I wonder if it can be proved that $a\cdot diag(V)-V~~~~~a\in[1, 2]$ is also positive semi-definite. ($diag(V)$ denotes a diagonal matrix whose entries are those of $V$, hence all positive) In case of $a=2$, the resulting $2\cdot diag(V)-V$ is also diagonally dominant (positive semi-definite), but is it possible to prove for $a\in[1,2]$? ......................................... Note that the above proof would facilitate my actual problem; is it possible to prove $tr[(X-Y)^T[a\cdot diag(V)-V](X-Y)]\geq 0$, where $tr(\cdot)$ denotes matrix trace, for $X, Y\in\mathbb{R}^{n\times 2}$ and $a\in[1,2]$ ? Also note that $tr(Y^TVY)\geq tr(X^TVX)$ and $tr(Y^Tdiag(V)Y)\geq tr(X^Tdiag(V)X)$. (if that facilitates the quest., assume $a=1$) ..................................................... Since the positive semi-definiteness could not generally be guaranteed for $a<2$, the problem casts to: for which restrictions on a does the positive semi-definiteness of a⋅diag(V)−V still hold? Note the comment from DavideGiraudo, and his claim for case $w_{ij}=1$, for all $i,j$. Could something similar be derived for general $w_{ij}$≥0?","Suppose a square symmetric matrix $V$ is given $V=\left(\begin{array}{ccccc}  \sum w_{1s} &   &    &  & \\    & \ddots &   & -w_{ij} \\    &  & \ddots & & \\    & -w_{ij} &   & \ddots & \\    & & & & \sum w_{ns}  \end{array}\right) \in\mathbb{R}^{n\times n},$ with values $w_{ij}> 0$, hence with only positive diagonal entries. Since the above matrix is diagonally dominant, it is positive semi-definite. However, I wonder if it can be proved that $a\cdot diag(V)-V~~~~~a\in[1, 2]$ is also positive semi-definite. ($diag(V)$ denotes a diagonal matrix whose entries are those of $V$, hence all positive) In case of $a=2$, the resulting $2\cdot diag(V)-V$ is also diagonally dominant (positive semi-definite), but is it possible to prove for $a\in[1,2]$? ......................................... Note that the above proof would facilitate my actual problem; is it possible to prove $tr[(X-Y)^T[a\cdot diag(V)-V](X-Y)]\geq 0$, where $tr(\cdot)$ denotes matrix trace, for $X, Y\in\mathbb{R}^{n\times 2}$ and $a\in[1,2]$ ? Also note that $tr(Y^TVY)\geq tr(X^TVX)$ and $tr(Y^Tdiag(V)Y)\geq tr(X^Tdiag(V)X)$. (if that facilitates the quest., assume $a=1$) ..................................................... Since the positive semi-definiteness could not generally be guaranteed for $a<2$, the problem casts to: for which restrictions on a does the positive semi-definiteness of a⋅diag(V)−V still hold? Note the comment from DavideGiraudo, and his claim for case $w_{ij}=1$, for all $i,j$. Could something similar be derived for general $w_{ij}$≥0?",,"['linear-algebra', 'matrices', 'vector-spaces']"
53,Get eigenvector and eigenvalues from symmetrical Matrix,Get eigenvector and eigenvalues from symmetrical Matrix,,"I used a Lapack library to compute Singular Value Decomposition of symmetrical Matrix in c# language like: $\left( \begin{array}{ccc} 1 & 2 & 5 \\ 2 & 1 & 7 \\ 5 & 7 & 1 \end{array} \right)   =  \left( \begin{array}{ccc} -0.467 & -0.328 & -0.821 \\ -0.582 & -0.586 & 0.564 \\ -0.666 & 0.741 & 0.083 \end{array} \right)  \left( \begin{array}{ccc} 10.62 & 0 & 0 \\ 0 & 6.74 & 0 \\ 0 & 0 & 0.88 \end{array} \right)   \left( \begin{array}{ccc} -0.467 & -0.582 & -0.666 \\ 0.328 & 0.586 & -0.741 \\ 0.821 & -0.564 & -0.083 \end{array} \right)     $ but How do I get eigenvectors and eigenvalues from $USV^T$, I was reading that $S$ are the eigenvalues but like the sign must be changed... also the $U$ gives the eigenvectors but there is an issue with the sign, is there a rule to get the correct sign?","I used a Lapack library to compute Singular Value Decomposition of symmetrical Matrix in c# language like: $\left( \begin{array}{ccc} 1 & 2 & 5 \\ 2 & 1 & 7 \\ 5 & 7 & 1 \end{array} \right)   =  \left( \begin{array}{ccc} -0.467 & -0.328 & -0.821 \\ -0.582 & -0.586 & 0.564 \\ -0.666 & 0.741 & 0.083 \end{array} \right)  \left( \begin{array}{ccc} 10.62 & 0 & 0 \\ 0 & 6.74 & 0 \\ 0 & 0 & 0.88 \end{array} \right)   \left( \begin{array}{ccc} -0.467 & -0.582 & -0.666 \\ 0.328 & 0.586 & -0.741 \\ 0.821 & -0.564 & -0.083 \end{array} \right)     $ but How do I get eigenvectors and eigenvalues from $USV^T$, I was reading that $S$ are the eigenvalues but like the sign must be changed... also the $U$ gives the eigenvectors but there is an issue with the sign, is there a rule to get the correct sign?",,"['matrices', 'eigenvalues-eigenvectors']"
54,How do you find the matrix representation of a linear transformation?,How do you find the matrix representation of a linear transformation?,,I am having trouble with this problem. I have to find the matrix representation of a linear transformation. The example in my book got me my answer below but I do not feel that it is right/sufficient. Can someone explain matrix representation of a linear transformation? Given $P_2(x)$ and $P_3(x)$ and the linear transformation: $L:P_2(x)\rightarrow P_3(x)$ defined by $L(p(x)) =  \displaystyle \int p(x)dx$. Find the matrix representation $A$ of the linear transformation $L$. Then find the rank of $A$ and the null space of $A$. Here is what I have: $$A =  \begin{bmatrix}0&1&0\\ 0&0&2\\ 0&0&0\end{bmatrix}$$ $R(A)$ = 2 $N(A)$ = 1,I am having trouble with this problem. I have to find the matrix representation of a linear transformation. The example in my book got me my answer below but I do not feel that it is right/sufficient. Can someone explain matrix representation of a linear transformation? Given $P_2(x)$ and $P_3(x)$ and the linear transformation: $L:P_2(x)\rightarrow P_3(x)$ defined by $L(p(x)) =  \displaystyle \int p(x)dx$. Find the matrix representation $A$ of the linear transformation $L$. Then find the rank of $A$ and the null space of $A$. Here is what I have: $$A =  \begin{bmatrix}0&1&0\\ 0&0&2\\ 0&0&0\end{bmatrix}$$ $R(A)$ = 2 $N(A)$ = 1,,"['linear-algebra', 'matrices']"
55,Is my reasoning/understanding of definitions right?,Is my reasoning/understanding of definitions right?,,"Does the following reasoning make sense? I have an $n\times n$ matrix $M$ acting on $\mathbb C^n$, Then, The eigenspace of matrix $M$ is $\ker(M-\lambda I)$. The eigenvalues of  $M^2$ are $\lambda^2$, where $\lambda$ are eigenvalues of $M$. $Mv=\lambda v \implies M^2 v=\lambda^2v \implies$ eigenvectors of $M$ are eigenvectors of $M^2$ Now $M^2v=\lambda^2 v\implies Mv=\pm\lambda v$ so eigenvectors of $M^2$ are also eigenvectors of $M$ Hence the eigenspaces of $M$ and $M^2$ are the same? (Check: To have the same eigenspace does that mean just having the same set of eigenvectors?) Thanks.","Does the following reasoning make sense? I have an $n\times n$ matrix $M$ acting on $\mathbb C^n$, Then, The eigenspace of matrix $M$ is $\ker(M-\lambda I)$. The eigenvalues of  $M^2$ are $\lambda^2$, where $\lambda$ are eigenvalues of $M$. $Mv=\lambda v \implies M^2 v=\lambda^2v \implies$ eigenvectors of $M$ are eigenvectors of $M^2$ Now $M^2v=\lambda^2 v\implies Mv=\pm\lambda v$ so eigenvectors of $M^2$ are also eigenvectors of $M$ Hence the eigenspaces of $M$ and $M^2$ are the same? (Check: To have the same eigenspace does that mean just having the same set of eigenvectors?) Thanks.",,"['matrices', 'eigenvalues-eigenvectors']"
56,evaluating determinants of matricies with unknowns given,evaluating determinants of matricies with unknowns given,,My homework question says given: $$\left|\begin{matrix}a & b& c\\ d& e& f\\ g& h& i\end{matrix}\right| = -6$$ evaluate the determinant $$\left|\begin{matrix}a+d & b+e& c+f\\ -d& -e& -f\\ g& h& i\end{matrix}\right|$$ It says I have to do this by row reduction. I know how to row reduce but I don't know where to start for this question. I just can't grasp how the two matrixes/determinants can be used together to solve the question. Any help would be greatly appricated,My homework question says given: $$\left|\begin{matrix}a & b& c\\ d& e& f\\ g& h& i\end{matrix}\right| = -6$$ evaluate the determinant $$\left|\begin{matrix}a+d & b+e& c+f\\ -d& -e& -f\\ g& h& i\end{matrix}\right|$$ It says I have to do this by row reduction. I know how to row reduce but I don't know where to start for this question. I just can't grasp how the two matrixes/determinants can be used together to solve the question. Any help would be greatly appricated,,"['linear-algebra', 'matrices', 'determinant']"
57,How to obtain a possible state space representation of this 2nd order transfer function?,How to obtain a possible state space representation of this 2nd order transfer function?,,"I have this 2nd order transfer function: $$G(s) = \frac{2}{s} + \frac{1}{s+2}$$ And I need to find a possible state space representation in the form of: $$ \frac{dx}{dt} = Ax + bu $$  $$y = c^Tx$$ Matrix A Matrix A is the system matrix, and relates how the current state affects the state change x' . If the state change is not dependent on the current state, A will be the zero matrix. The exponential of the state matrix, eAt is called the state transition matrix. Matrix B Matrix B is the control matrix, and determines how the system input affects the state change. If the state change is not dependent on the system input, then B will be the zero matrix. Matrix C Matrix C is the output matrix, and determines the relationship between the system state and the system output. I can see the eigenvalues, they are $s_1 = 0$ and $s_2 = -2$. So I can write down a diagonal matrix like that I think: $$ A = \begin{pmatrix} 0 & 0  \\0 & -2  \end{pmatrix} $$ But know I am stuck. Is there some sort of trick I can use?","I have this 2nd order transfer function: $$G(s) = \frac{2}{s} + \frac{1}{s+2}$$ And I need to find a possible state space representation in the form of: $$ \frac{dx}{dt} = Ax + bu $$  $$y = c^Tx$$ Matrix A Matrix A is the system matrix, and relates how the current state affects the state change x' . If the state change is not dependent on the current state, A will be the zero matrix. The exponential of the state matrix, eAt is called the state transition matrix. Matrix B Matrix B is the control matrix, and determines how the system input affects the state change. If the state change is not dependent on the system input, then B will be the zero matrix. Matrix C Matrix C is the output matrix, and determines the relationship between the system state and the system output. I can see the eigenvalues, they are $s_1 = 0$ and $s_2 = -2$. So I can write down a diagonal matrix like that I think: $$ A = \begin{pmatrix} 0 & 0  \\0 & -2  \end{pmatrix} $$ But know I am stuck. Is there some sort of trick I can use?",,"['matrices', 'dynamical-systems', 'control-theory']"
58,"Can the Baker-Campbell-Hausdorff formula for $\ln(AB)$ be simplified for similar, diagonizable matrices?","Can the Baker-Campbell-Hausdorff formula for  be simplified for similar, diagonizable matrices?",\ln(AB),"Given two similar, diagonizable square matrices $A$ and $B$ that do not commute, can the Baker-Campbell-Hausdorff formula be simplified exploiting the similarity to obtain a nice expression for $\ln(AB)$? (It's probably not simply $\ln A + \ln B$ due to the lack of commutivity)","Given two similar, diagonizable square matrices $A$ and $B$ that do not commute, can the Baker-Campbell-Hausdorff formula be simplified exploiting the similarity to obtain a nice expression for $\ln(AB)$? (It's probably not simply $\ln A + \ln B$ due to the lack of commutivity)",,"['matrices', 'logarithms']"
59,a property of log determinant,a property of log determinant,,"Let $X$ be a symmetric positive definite matrix, and $D$ be a symmetric matrix satisfying $\operatorname{tr}(X^{-1}DX^{-1}D) < 1$. How to show that $$f(X+D)\le f(X)+\operatorname{tr}(f'(X)D)+\frac{\operatorname{tr}(X^{-1}DX^{-1}D)}{2\left(1-\sqrt{\operatorname{tr}(X^{-1}DX^{-1}D)}\right)^2},$$ where $f(X)=-\ln\det X$?","Let $X$ be a symmetric positive definite matrix, and $D$ be a symmetric matrix satisfying $\operatorname{tr}(X^{-1}DX^{-1}D) < 1$. How to show that $$f(X+D)\le f(X)+\operatorname{tr}(f'(X)D)+\frac{\operatorname{tr}(X^{-1}DX^{-1}D)}{2\left(1-\sqrt{\operatorname{tr}(X^{-1}DX^{-1}D)}\right)^2},$$ where $f(X)=-\ln\det X$?",,"['matrices', 'optimization']"
60,Linear Algebra: An explanation on a simplification,Linear Algebra: An explanation on a simplification,,"Could someone please explain to me what property was used in simplifying this, or how this was achieved? Thank you.","Could someone please explain to me what property was used in simplifying this, or how this was achieved? Thank you.",,['linear-algebra']
61,Why is the identity component of a matrix group a subgroup?,Why is the identity component of a matrix group a subgroup?,,"I'm working through Stillwell's ""Naive Lie Theory"".  I'm supposed to show that the identity component of a matrix group is a subgroup in two steps.  I'm allowed to assume that ""matrix multiplication is a continuous operation"".  First question- what does this mean?  Does this mean multiplying matrices by a fixed matrix is continuous, or multiplying two matrices which vary? In the first step, I'm supposed to prove that if there are continuous paths in the group $G$ from 1 to $A \in G$ and  to $B \in G$ then there is a path in G from $A$ to $AB$. I did this by assuming that matrix multiplication by a fixed matrix was continuous.  I presume that this will get us closure under group operation by concatenating the path from 1 to $A$ with the path from $A$ to $AB$. Second, and where I am stuck, is in proving that if there is a continuous path in $G$ from 1 to $A$ there is also a continuous path from $A^{-1}$ to 1 .  If I knew that the map that sends $A$ to $A^{-1}$ was continuous, I think I would be done, but I don't know how to get this easily.","I'm working through Stillwell's ""Naive Lie Theory"".  I'm supposed to show that the identity component of a matrix group is a subgroup in two steps.  I'm allowed to assume that ""matrix multiplication is a continuous operation"".  First question- what does this mean?  Does this mean multiplying matrices by a fixed matrix is continuous, or multiplying two matrices which vary? In the first step, I'm supposed to prove that if there are continuous paths in the group $G$ from 1 to $A \in G$ and  to $B \in G$ then there is a path in G from $A$ to $AB$. I did this by assuming that matrix multiplication by a fixed matrix was continuous.  I presume that this will get us closure under group operation by concatenating the path from 1 to $A$ with the path from $A$ to $AB$. Second, and where I am stuck, is in proving that if there is a continuous path in $G$ from 1 to $A$ there is also a continuous path from $A^{-1}$ to 1 .  If I knew that the map that sends $A$ to $A^{-1}$ was continuous, I think I would be done, but I don't know how to get this easily.",,"['matrices', 'lie-groups']"
62,Finding Jordan form base for $\left(\begin{smallmatrix} 2 &1 &2 \\ -1 &0 &2 \\ 0&0 & 1 \end{smallmatrix}\right)$,Finding Jordan form base for,\left(\begin{smallmatrix} 2 &1 &2 \\ -1 &0 &2 \\ 0&0 & 1 \end{smallmatrix}\right),"I need your help for understanding how to compute jordan base for this matrix: $\begin{pmatrix} 2 &1  &2 \\  -1 &0  &2 \\   0&0  & 1 \end{pmatrix}$ This is what I tried to do: I found the Minimal polynomial: $(x-1)^{3}$, so I know that the Jordan normal form is: $\begin{pmatrix} 1 &0  &0 \\   1& 1 & 0\\   0&1  & 1 \end{pmatrix}$ Now I remember that I need to find: $\ker (A-I), \ker (A-I)^{2}, \ker(A-I)^{3}$. I found that $\ker (A-I)$ is: $\begin{pmatrix} -1\\  1 \\  0 \end{pmatrix}$ and $\ker (A-I)^{2}$ is: $\begin{pmatrix} 1\\  0 \\  0 \end{pmatrix}$ , $\begin{pmatrix} 0\\  1 \\  0 \end{pmatrix}$ and obivously $\ker(A-I)^{3}$ is all  $\mathbb{R}^{3}$, What should I do from here? I can't remember the whole algorithem, and I can't find it anywhere. Thank you, Have a great week-end.","I need your help for understanding how to compute jordan base for this matrix: $\begin{pmatrix} 2 &1  &2 \\  -1 &0  &2 \\   0&0  & 1 \end{pmatrix}$ This is what I tried to do: I found the Minimal polynomial: $(x-1)^{3}$, so I know that the Jordan normal form is: $\begin{pmatrix} 1 &0  &0 \\   1& 1 & 0\\   0&1  & 1 \end{pmatrix}$ Now I remember that I need to find: $\ker (A-I), \ker (A-I)^{2}, \ker(A-I)^{3}$. I found that $\ker (A-I)$ is: $\begin{pmatrix} -1\\  1 \\  0 \end{pmatrix}$ and $\ker (A-I)^{2}$ is: $\begin{pmatrix} 1\\  0 \\  0 \end{pmatrix}$ , $\begin{pmatrix} 0\\  1 \\  0 \end{pmatrix}$ and obivously $\ker(A-I)^{3}$ is all  $\mathbb{R}^{3}$, What should I do from here? I can't remember the whole algorithem, and I can't find it anywhere. Thank you, Have a great week-end.",,['linear-algebra']
63,Cross product for matrices,Cross product for matrices,,"I have been contemplating extending the definition of cross product for matrices, and I wonder if this has been done before. Basically my definition is, given two 3x3 matrices: $A=(a_{ij})_{i,j=1} ^ 3$ and $B=(b_{ij})$ then $A\times B=(A_i \times B_j)_{i,j=1}^3$ where $A_i=(a_{i1},a_{i2},a_{i3})$ the same with B (just replace A with B a with b and i with j). Now I checked that it's linear with regard to simple matrix addition $$A\times (B+C)= A\times B + A\times C$$ And obviously it's antisymmetric, I am not sure if there's a nice connection with matrix multiplcation, I mean $A\times (BC)=?$ not sure if there's a simple identity here. Has this been done already, where may I read on this? Thanks.","I have been contemplating extending the definition of cross product for matrices, and I wonder if this has been done before. Basically my definition is, given two 3x3 matrices: $A=(a_{ij})_{i,j=1} ^ 3$ and $B=(b_{ij})$ then $A\times B=(A_i \times B_j)_{i,j=1}^3$ where $A_i=(a_{i1},a_{i2},a_{i3})$ the same with B (just replace A with B a with b and i with j). Now I checked that it's linear with regard to simple matrix addition $$A\times (B+C)= A\times B + A\times C$$ And obviously it's antisymmetric, I am not sure if there's a nice connection with matrix multiplcation, I mean $A\times (BC)=?$ not sure if there's a simple identity here. Has this been done already, where may I read on this? Thanks.",,['matrices']
64,complexity cost for which one is greater : determinant or eigen values?,complexity cost for which one is greater : determinant or eigen values?,,what is complexity cost for determining all of eigen values? what is complexity cost for calculating determinant ?,what is complexity cost for determining all of eigen values? what is complexity cost for calculating determinant ?,,"['matrices', 'matlab', 'eigenvalues-eigenvectors', 'computational-complexity', 'determinant']"
65,Gauss Jordan elimination - count of steps for $N \times M$ equation,Gauss Jordan elimination - count of steps for  equation,N \times M,"I am having some problem wrapping my head around an assignment. I have to find out how many additions, subtractions, multiplications and divisions are used while solving an $N \times M$ linear equation, while using Gauss-Jordan elimination. Could anyone point me in the right direction?","I am having some problem wrapping my head around an assignment. I have to find out how many additions, subtractions, multiplications and divisions are used while solving an $N \times M$ linear equation, while using Gauss-Jordan elimination. Could anyone point me in the right direction?",,"['linear-algebra', 'matrices', 'algorithms', 'computational-complexity']"
66,Constructing an Extrusion between two non-parallel planes,Constructing an Extrusion between two non-parallel planes,,"I am crossposting this question from Stack Overflow since it is more math then programming related. In Reference to my question about projecting a planar Polygon to a plane, i came to another problem. I want to project a base contour which is aligned to the x-y-plane along a projection vector from from a point P1 to a point P2. This already works, but now i also need to change the alignment of my base contour using the position of P1  and two vectors defining the new alignement base's x and y should be transformed to. I want to use a transformation matrix M for this, which premultiplied to my projecton matrix p should give me the transformation matrix from a vertex v in the base contour to a vertex v' on the projection plane such as v' = (P x M) * v So, my question is: How to create a transformation matrix which transforms a vertex v relative to a coordinate system with O = (0,0,0), xdir = (1,0,0), and ydir = (0,1,0) to a vertex relative to a coordinate system with O'=(ox, oy, oz) and xdir and ydir as two arbitrary perpendicular unit vectors? Clarifications I use 4x4 matrices","I am crossposting this question from Stack Overflow since it is more math then programming related. In Reference to my question about projecting a planar Polygon to a plane, i came to another problem. I want to project a base contour which is aligned to the x-y-plane along a projection vector from from a point P1 to a point P2. This already works, but now i also need to change the alignment of my base contour using the position of P1  and two vectors defining the new alignement base's x and y should be transformed to. I want to use a transformation matrix M for this, which premultiplied to my projecton matrix p should give me the transformation matrix from a vertex v in the base contour to a vertex v' on the projection plane such as v' = (P x M) * v So, my question is: How to create a transformation matrix which transforms a vertex v relative to a coordinate system with O = (0,0,0), xdir = (1,0,0), and ydir = (0,1,0) to a vertex relative to a coordinate system with O'=(ox, oy, oz) and xdir and ydir as two arbitrary perpendicular unit vectors? Clarifications I use 4x4 matrices",,"['matrices', 'transformation']"
67,Diagonalization/Eigenvalues of block matrices,Diagonalization/Eigenvalues of block matrices,,"Suppose I have two square $2^n \times 2^n$ matrices $A$ and $B$ . Suppose furthermore that $A$ and $B$ are both symmetric matrices with real coefficients. Therefore their eigenvectors can be chosen to be orthonormal, and the matrices can be diagonalized with orthonormal matrices $R_a, R_b$ : $$A=R_A^{T} \Lambda_A R_A, $$ $$B=R_B^{T} \Lambda_B R_B.$$ Now I form the block-matrix $M$ of size $2^{n+1} \times 2^{n+1}$ : $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix} $$ Clearly, $M$ will also be symmetric with real coefficients, and so its eigenvectors can be chosen to be orthonormal, and $M$ can be diagonalized with another orthonormal matrix: $$M=R_M^{T} \Lambda_M R_M.$$ Question: Is there any formula for the eigenvalues of $M$ in terms of the eigenvalues of $A$ and $B$ ? Is there any formula for $R_M$ in terms of $R_A$ and $R_B?$ Attempt: Taking inspiration from the $2 \times 2$ case we have $$\begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{\sqrt{2}}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix} \frac{1}{\sqrt{2}} \begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix} $$ $$\begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix}\begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix} $$ We can observe that this also works in block form: $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} A-B & 0 \\ 0 & A+B \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix} $$ Now, this is not diagonalized yet, but the upper left and lower right block behave independently, and so: $$M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} R^T_{A-B} & 0 \\ 0 & R^T_{A+B} \end{pmatrix} \begin{pmatrix} \Lambda_{A-B} & 0 \\ 0 & \Lambda_{A+B} \end{pmatrix} \begin{pmatrix} R_{A-B} & 0 \\ 0 & R_{A+B} \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix} $$ Therefore, we could answer our original question if instead we answered: Question: Is there any formula for the eigenvalues of $A+B$ and $A-B$ in terms of the eigenvalues of $A$ and $B$ ? Is there any formula for $R_{A+B}$ and $R_{A-B}$ in terms of $R_A$ and $R_B?$ Context: Knowing how to do this would be useful for this problem: Sums of Matrices; 2-Color an $m \times k$ grid so that no $2 \times 2$ squares of the same color occur .","Suppose I have two square matrices and . Suppose furthermore that and are both symmetric matrices with real coefficients. Therefore their eigenvectors can be chosen to be orthonormal, and the matrices can be diagonalized with orthonormal matrices : Now I form the block-matrix of size : Clearly, will also be symmetric with real coefficients, and so its eigenvectors can be chosen to be orthonormal, and can be diagonalized with another orthonormal matrix: Question: Is there any formula for the eigenvalues of in terms of the eigenvalues of and ? Is there any formula for in terms of and Attempt: Taking inspiration from the case we have We can observe that this also works in block form: Now, this is not diagonalized yet, but the upper left and lower right block behave independently, and so: Therefore, we could answer our original question if instead we answered: Question: Is there any formula for the eigenvalues of and in terms of the eigenvalues of and ? Is there any formula for and in terms of and Context: Knowing how to do this would be useful for this problem: Sums of Matrices; 2-Color an $m \times k$ grid so that no $2 \times 2$ squares of the same color occur .","2^n \times 2^n A B A B R_a, R_b A=R_A^{T} \Lambda_A R_A,  B=R_B^{T} \Lambda_B R_B. M 2^{n+1} \times 2^{n+1} M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}  M M M=R_M^{T} \Lambda_M R_M. M A B R_M R_A R_B? 2 \times 2 \begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{\sqrt{2}}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix} \frac{1}{\sqrt{2}} \begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix}  \begin{pmatrix} a & b \\ b & a \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a-b & 0 \\ 0 & a+b \end{pmatrix}\begin{pmatrix}-1 & 1 \\ 1 & 1 \end{pmatrix}  M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} A-B & 0 \\ 0 & A+B \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix}  M=\begin{pmatrix} A & B \\ B & A \end{pmatrix}= \frac{1}{2}\begin{pmatrix} -I & I \\ I & I \end{pmatrix} \begin{pmatrix} R^T_{A-B} & 0 \\ 0 & R^T_{A+B} \end{pmatrix} \begin{pmatrix} \Lambda_{A-B} & 0 \\ 0 & \Lambda_{A+B} \end{pmatrix} \begin{pmatrix} R_{A-B} & 0 \\ 0 & R_{A+B} \end{pmatrix}\begin{pmatrix}-I & I \\ I & I \end{pmatrix}  A+B A-B A B R_{A+B} R_{A-B} R_A R_B?","['linear-algebra', 'matrices']"
68,"What is in the image of the exponential of $\frak{sl}(n,\mathbb{R}$)? What do you need to get all of $\mathrm{SL}(n,\mathbb{R}$)?",What is in the image of the exponential of )? What do you need to get all of )?,"\frak{sl}(n,\mathbb{R} \mathrm{SL}(n,\mathbb{R}","This question discusses how $\mathrm{S}L(2,\mathbb{R}$ ) coincides with $\pm\exp(z)$ with $z\in \frak{sl}(n,\mathbb{R}$ ) (the real traceless matrices). Is it known what happens for $n>2$ ? Namely, one can represent all invertible matrices with determinant $1$ and trace greater than or equal to $-2$ . Also, just by providing a sign (more precisely, up to the center of the group, which is $\{ I, -I\}$ ) one is able to obtain all of $\mathrm{SL}(2,\mathbb{R})$ ). Is such a result known for the general case of $\mathrm{SL}(n,\mathbb{R})$ , which characterizes the image of the exponential and what one needs to ""add"" in order to obtain the entire group (i.e. what is the ""minimal"" set $A\subseteq G$ such that $\mathrm{SL}(n,\mathbb{R}) = A \exp(\frak{sl}(n,\mathbb{R}))$ )? I would also be interested in the more general question regarding classical Lie groups, although this would already be a great help towards a greater understanding of what happens in the exponential (even just for $n=3$ ).","This question discusses how ) coincides with with ) (the real traceless matrices). Is it known what happens for ? Namely, one can represent all invertible matrices with determinant and trace greater than or equal to . Also, just by providing a sign (more precisely, up to the center of the group, which is ) one is able to obtain all of ). Is such a result known for the general case of , which characterizes the image of the exponential and what one needs to ""add"" in order to obtain the entire group (i.e. what is the ""minimal"" set such that )? I would also be interested in the more general question regarding classical Lie groups, although this would already be a great help towards a greater understanding of what happens in the exponential (even just for ).","\mathrm{S}L(2,\mathbb{R} \pm\exp(z) z\in \frak{sl}(n,\mathbb{R} n>2 1 -2 \{ I, -I\} \mathrm{SL}(2,\mathbb{R}) \mathrm{SL}(n,\mathbb{R}) A\subseteq G \mathrm{SL}(n,\mathbb{R}) = A \exp(\frak{sl}(n,\mathbb{R})) n=3","['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras', 'matrix-exponential']"
69,What is the exact form of the matrix algebra generated by a Jordan matrix?,What is the exact form of the matrix algebra generated by a Jordan matrix?,,"In a paper it is claimed that the matrix algebra generated by a matrix $$J=\oplus_i  J_{\lambda_i,m_i}$$ in Jordan normal form ""is the algebra of block diagonal matrices with the same block partition, where each block contains an arbitrary upper triangular Toeplitz matrix "". I take issue with the arbitrary Toeplitz blocks - an explicit calculation for any polynomial $$p(J)=\sum_k p_k J^k=p\left(\bigoplus_i J_{\lambda_i,m_i}\right)=\bigoplus_i p\left(J_{\lambda_i,m_i}\right) \ ,$$ and thus any element of the generated matrix algebra yields $$\sum\limits_k\begin{bmatrix}p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}&\ddots&\ddots &\ddots&\ddots\\0&p_k\lambda^k&p_kk\lambda^{k-1}&\ddots&\ddots& p_kk!\lambda^{k-l}/(l!(k-l)!)&\ddots\\0&0&p_k\lambda^k&\ddots&\ddots&\ddots&\ddots\\0&0&0&\ddots&\ddots&\ddots&\ddots\\0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}\\0&0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}\\0&0&0&0&0&0&p_k\lambda^k\end{bmatrix}$$ for the Toeplitz blocks. But the same polynomial has to be applied to all blocks and so how can the full matrix be composed of arbitrary upper triangular matrix blocks ? For example, I fail to see how one would obtain a single diagonal block with all other blocks being trivial zero Toeplitz submatrices. What am I missing here?","In a paper it is claimed that the matrix algebra generated by a matrix in Jordan normal form ""is the algebra of block diagonal matrices with the same block partition, where each block contains an arbitrary upper triangular Toeplitz matrix "". I take issue with the arbitrary Toeplitz blocks - an explicit calculation for any polynomial and thus any element of the generated matrix algebra yields for the Toeplitz blocks. But the same polynomial has to be applied to all blocks and so how can the full matrix be composed of arbitrary upper triangular matrix blocks ? For example, I fail to see how one would obtain a single diagonal block with all other blocks being trivial zero Toeplitz submatrices. What am I missing here?","J=\oplus_i  J_{\lambda_i,m_i} p(J)=\sum_k p_k J^k=p\left(\bigoplus_i J_{\lambda_i,m_i}\right)=\bigoplus_i p\left(J_{\lambda_i,m_i}\right) \ , \sum\limits_k\begin{bmatrix}p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}&\ddots&\ddots &\ddots&\ddots\\0&p_k\lambda^k&p_kk\lambda^{k-1}&\ddots&\ddots& p_kk!\lambda^{k-l}/(l!(k-l)!)&\ddots\\0&0&p_k\lambda^k&\ddots&\ddots&\ddots&\ddots\\0&0&0&\ddots&\ddots&\ddots&\ddots\\0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}\\0&0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}\\0&0&0&0&0&0&p_k\lambda^k\end{bmatrix}","['linear-algebra', 'matrices', 'linear-transformations', 'jordan-normal-form']"
70,What's relationship between ST and TS's eigenvalues,What's relationship between ST and TS's eigenvalues,,"this comes from a comment of this If 𝑆 and 𝑇 are 𝑛×𝑚 and 𝑚×𝑛 matrices, 𝑆𝑇 and 𝑇𝑆 have the same nonzero eigenvalues: 𝑢 is an eigenvector of 𝑆𝑇 iff 𝑇𝑢 is an eigenvector of 𝑇𝑆 , with the same nonzero eigenvalue. I try to proof this if 𝑢 is an eigenvector of 𝑆𝑇 => 𝑇𝑢 is an eigenvector of 𝑇𝑆 $\begin{align} STu &= \lambda u \tag{1} \end{align}$ left multiply by T, then $\begin{align} TSTu &= \lambda Tu \tag{2} \end{align}$ but I'm blocked (2)=>(1) if $TSTu=\lambda Tu$ , I can't multiply left with $T^-1$ since T is not square matrix, even T is square matrix, it still may not be invertible. and also why OP in that thread says have the same nonzero eigenvalues , from (1)=>(2) it seemed the zero eigenvalues are also corresponding?","this comes from a comment of this If 𝑆 and 𝑇 are 𝑛×𝑚 and 𝑚×𝑛 matrices, 𝑆𝑇 and 𝑇𝑆 have the same nonzero eigenvalues: 𝑢 is an eigenvector of 𝑆𝑇 iff 𝑇𝑢 is an eigenvector of 𝑇𝑆 , with the same nonzero eigenvalue. I try to proof this if 𝑢 is an eigenvector of 𝑆𝑇 => 𝑇𝑢 is an eigenvector of 𝑇𝑆 left multiply by T, then but I'm blocked (2)=>(1) if , I can't multiply left with since T is not square matrix, even T is square matrix, it still may not be invertible. and also why OP in that thread says have the same nonzero eigenvalues , from (1)=>(2) it seemed the zero eigenvalues are also corresponding?","\begin{align}
STu &= \lambda u \tag{1}
\end{align} \begin{align}
TSTu &= \lambda Tu \tag{2}
\end{align} TSTu=\lambda Tu T^-1","['matrices', 'eigenvalues-eigenvectors']"
71,Question on The Theorem of Existence and Uniqueness of The Solutions to Sylvester Equations,Question on The Theorem of Existence and Uniqueness of The Solutions to Sylvester Equations,,"Background I am self-studying linear algebra, and I got stuck on some steps of the proof of the following theorem: Theorem $\quad$ Let $A$ be an $n \times n$ complex matrix and $B$ be an $m \times m$ complex matrix. For any $n \times m$ complex matrix $C$ , the Sylvester equation $AX+XB=C$ has a unique solution $X$ , which is an $n \times m$ complex matrix, if and only if $A$ and $-B$ do not share any eigenvalue. Here is the proof: Proof $\quad$ The equation $AX+XB=C$ is a linear system with $mn$ unknowns and $mn$ equations. Hence, it is uniquely solvable for any given $C$ if and only if the homogeneous equation $AX+XB=0$ admits only the trivial solution. First suppose that $A$ and $-B$ do not share any eigenvalue. Let $X$ be a solution to the homogeneous system $AX+XB=0$ . Then $AX=X(-B)$ . Since \begin{align*} &A(AX) = A(X(-B))\\ \implies\ &(AA)X = (AX)(-B) = (X(-B))(-B) = X((-B)(-B))\\ \implies\ &A^2X = X(-B)^2, \end{align*} by mathematical induction, we have $A^kX=X(-B)^k$ for each $k\in\mathbb{N}$ . Then, $p(A)X = Xp(-B)$ for any polynomial $p$ . In particular, let $p$ be the characteristic polynomial of $A$ ; that is, let $p(r) = det(A-rI)$ . Then, $p(A) = 0$ . If $E$ is any square matrix, let $\sigma(E)$ denote the set of eigenvalues of $E$ . Then, $\sigma(p(-B)) = p(\sigma(-B))$ . Since $A$ and $-B$ do not share any eigenvalue, $p(\sigma(-B))$ does not contain zero, and hence, $p(-B)$ is nonsigular. Thus, $X$ is the zero matrix. Conversely, suppose that $A$ and $-B$ share an eigenvalue $\lambda$ . Let $\mathbf{u}$ be a corresponding right eigenvector for $A$ and $\mathbf{v}$ be a corresponding left eigenvector for $-B$ ; that is $A\mathbf{u}=\lambda\mathbf{u}$ and $\mathbf{v}(-B) = \lambda\mathbf{v}$ . Let $X=\mathbf{u}\mathbf{v}^*$ . Then, $X$ is not the zero matrix, and $AX+XB = A(\mathbf{u}\mathbf{v}^*) - (\mathbf{u}\mathbf{v}^*)(-B) = \lambda\mathbf{u}\mathbf{v}^* - \lambda\mathbf{u}\mathbf{v}^* = 0$ . Therefore, $X$ is a nontrovial solution to the homogeneous system $AX+XB=0$ . My Questions I could not understand certain steps in the above proof. For the ""if"" direction (the second paragraph in the proof): After it proved that $A^kX=X(-B)^k$ , it claimed that $p(A)X=Xp(-B)$ for any polynomial. Why is that? After it defined $p$ to be the characteristic polynomial, it said that $p(A)=0$ . Is this because one might just plug in $A$ to $p(r)$ to get $p(A) = det(A-AI) = det(0) = 0$ ? I have difficulties understanding the notation of $p(\sigma(-B))$ . While the $\sigma(-B)$ is the set of eigenvalues of $-B$ , what is $p(\sigma(-B))$ then? Is it the set of the image of the eigenvalues of $-B$ under the polynomial $p$ ? If so, does $\sigma(p(-B)) = p(\sigma(-B))$ mean that the two sets are equal? Why does the fact that $A$ and $-B$ do not share any eigenvalue imply that $p(\sigma(-B))$ does not contain zero, and how does it implies that $p(-B)$ is nonsingular? Why does the nonsigularity of $p(-B)$ imply that $X$ is the zero matrix? For the ""only if"" part (the third paragraph in the above proof): I am not familiar with the notation $\mathbf{v}^*$ . Is it the conjugate transpose of $\mathbf{v}$ ? If so, why cannot $\mathbf{u}\mathbf{v}^*$ be the zero matrix? I apologize for my lousy beginner linear algebra background. I would really appreciate it if someone could help me with these questions!","Background I am self-studying linear algebra, and I got stuck on some steps of the proof of the following theorem: Theorem Let be an complex matrix and be an complex matrix. For any complex matrix , the Sylvester equation has a unique solution , which is an complex matrix, if and only if and do not share any eigenvalue. Here is the proof: Proof The equation is a linear system with unknowns and equations. Hence, it is uniquely solvable for any given if and only if the homogeneous equation admits only the trivial solution. First suppose that and do not share any eigenvalue. Let be a solution to the homogeneous system . Then . Since by mathematical induction, we have for each . Then, for any polynomial . In particular, let be the characteristic polynomial of ; that is, let . Then, . If is any square matrix, let denote the set of eigenvalues of . Then, . Since and do not share any eigenvalue, does not contain zero, and hence, is nonsigular. Thus, is the zero matrix. Conversely, suppose that and share an eigenvalue . Let be a corresponding right eigenvector for and be a corresponding left eigenvector for ; that is and . Let . Then, is not the zero matrix, and . Therefore, is a nontrovial solution to the homogeneous system . My Questions I could not understand certain steps in the above proof. For the ""if"" direction (the second paragraph in the proof): After it proved that , it claimed that for any polynomial. Why is that? After it defined to be the characteristic polynomial, it said that . Is this because one might just plug in to to get ? I have difficulties understanding the notation of . While the is the set of eigenvalues of , what is then? Is it the set of the image of the eigenvalues of under the polynomial ? If so, does mean that the two sets are equal? Why does the fact that and do not share any eigenvalue imply that does not contain zero, and how does it implies that is nonsingular? Why does the nonsigularity of imply that is the zero matrix? For the ""only if"" part (the third paragraph in the above proof): I am not familiar with the notation . Is it the conjugate transpose of ? If so, why cannot be the zero matrix? I apologize for my lousy beginner linear algebra background. I would really appreciate it if someone could help me with these questions!","\quad A n \times n B m \times m n \times m C AX+XB=C X n \times m A -B \quad AX+XB=C mn mn C AX+XB=0 A -B X AX+XB=0 AX=X(-B) \begin{align*}
&A(AX) = A(X(-B))\\
\implies\ &(AA)X = (AX)(-B) = (X(-B))(-B) = X((-B)(-B))\\
\implies\ &A^2X = X(-B)^2,
\end{align*} A^kX=X(-B)^k k\in\mathbb{N} p(A)X = Xp(-B) p p A p(r) = det(A-rI) p(A) = 0 E \sigma(E) E \sigma(p(-B)) = p(\sigma(-B)) A -B p(\sigma(-B)) p(-B) X A -B \lambda \mathbf{u} A \mathbf{v} -B A\mathbf{u}=\lambda\mathbf{u} \mathbf{v}(-B) = \lambda\mathbf{v} X=\mathbf{u}\mathbf{v}^* X AX+XB = A(\mathbf{u}\mathbf{v}^*) - (\mathbf{u}\mathbf{v}^*)(-B) = \lambda\mathbf{u}\mathbf{v}^* - \lambda\mathbf{u}\mathbf{v}^* = 0 X AX+XB=0 A^kX=X(-B)^k p(A)X=Xp(-B) p p(A)=0 A p(r) p(A) = det(A-AI) = det(0) = 0 p(\sigma(-B)) \sigma(-B) -B p(\sigma(-B)) -B p \sigma(p(-B)) = p(\sigma(-B)) A -B p(\sigma(-B)) p(-B) p(-B) X \mathbf{v}^* \mathbf{v} \mathbf{u}\mathbf{v}^*","['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors', 'determinant']"
72,$(A+B)^2\|B^{-1}\|-A$ is positive definite?,is positive definite?,(A+B)^2\|B^{-1}\|-A,"If $A$ and $B$ are positive definite matrix, $(A+B)^2\|B^{-1}\|-A$ is also positive definite? When $A$ and $B$ are real numbers, then $(A+B)^2\|B^{-1}\|-A$ is obviously positive definite. When $A$ and $B$ are matrix, I can use orthonormal matrix $O$ to transform $B$ an diagonal matrix $\operatorname{diag}\{\lambda_1,\cdots,\lambda_n\}$ . Then I can transform the original problem to $(A+B)^2-\lambda_n A$ is it also positive definite?","If and are positive definite matrix, is also positive definite? When and are real numbers, then is obviously positive definite. When and are matrix, I can use orthonormal matrix to transform an diagonal matrix . Then I can transform the original problem to is it also positive definite?","A B (A+B)^2\|B^{-1}\|-A A B (A+B)^2\|B^{-1}\|-A A B O B \operatorname{diag}\{\lambda_1,\cdots,\lambda_n\} (A+B)^2-\lambda_n A","['linear-algebra', 'matrices', 'analysis', 'matrix-decomposition']"
73,Generating consecutive numbers from matrix multiplication.,Generating consecutive numbers from matrix multiplication.,,"I'm new to matrix multiplication and was trying to generate a sequence of consecutive numbers from the product of two $2 \times 2$ matrices and noticed this pattern. Let $$ A = \begin{pmatrix} n & n+1 \\ n+2 & n-1 \end{pmatrix}, \quad B = \begin{pmatrix} m & m-1 \\ m-1 & m \end{pmatrix},  \quad C = AB $$ Compute \begin{align*} C &= \begin{pmatrix} n & n+1 \\ n+2 & n-1 \end{pmatrix} \begin{pmatrix} m & m-1 \\ m-1 & m \end{pmatrix} \\ &= \begin{pmatrix} nm + (n+1)(m-1) & n(m-1) + (n+1)m \\ (n+2)m + (n-1)(m-1) & (n+2)(m-1) + (n-1)m \end{pmatrix} \\ &= \begin{pmatrix} 2nm - n + m - 1 & 2nm + m - n \\ 2nm + m - n + 1 & 2nm + m - n - 2 \end{pmatrix} \end{align*} Let $k = 2nm + m - n$ , then $$ C = \begin{pmatrix} k - 1 & k \\ k + 1 & k - 2 \end{pmatrix} $$ which yields consecutive numbers $k-2,k-1,k,k+1$ . Example Let $n=150, m=16$ . Then multiplying $$ \begin{pmatrix} 150 & 151 \\ 152 & 149 \\ \end{pmatrix} \cdot \begin{pmatrix} 16 & 15 \\ 15 & 16 \\ \end{pmatrix} = \begin{pmatrix} 4665 & 4666 \\ 4667 & 4664 \\ \end{pmatrix} $$ gives $k=4666$ with consecutive numbers $4664, 4665, 4666, 4667$ . WolframAlpha Questions Can that method be generalized to two $n \times n$ matrix products, resulting in $n^2$ consecutive numbers? Aside, is there a way to ensure the $n^2$ consecutive numbers are always composite?","I'm new to matrix multiplication and was trying to generate a sequence of consecutive numbers from the product of two matrices and noticed this pattern. Let Compute Let , then which yields consecutive numbers . Example Let . Then multiplying gives with consecutive numbers . WolframAlpha Questions Can that method be generalized to two matrix products, resulting in consecutive numbers? Aside, is there a way to ensure the consecutive numbers are always composite?","2 \times 2 
A = \begin{pmatrix} n & n+1 \\ n+2 & n-1 \end{pmatrix}, \quad B = \begin{pmatrix} m & m-1 \\ m-1 & m \end{pmatrix},  \quad C = AB
 \begin{align*}
C &= \begin{pmatrix} n & n+1 \\ n+2 & n-1 \end{pmatrix} \begin{pmatrix} m & m-1 \\ m-1 & m \end{pmatrix} \\
&= \begin{pmatrix} nm + (n+1)(m-1) & n(m-1) + (n+1)m \\ (n+2)m + (n-1)(m-1) & (n+2)(m-1) + (n-1)m \end{pmatrix} \\
&= \begin{pmatrix} 2nm - n + m - 1 & 2nm + m - n \\ 2nm + m - n + 1 & 2nm + m - n - 2 \end{pmatrix}
\end{align*} k = 2nm + m - n 
C = \begin{pmatrix} k - 1 & k \\ k + 1 & k - 2 \end{pmatrix}
 k-2,k-1,k,k+1 n=150, m=16 
\begin{pmatrix}
150 & 151 \\
152 & 149 \\
\end{pmatrix}
\cdot
\begin{pmatrix}
16 & 15 \\
15 & 16 \\
\end{pmatrix}
=
\begin{pmatrix}
4665 & 4666 \\
4667 & 4664 \\
\end{pmatrix}
 k=4666 4664, 4665, 4666, 4667 n \times n n^2 n^2",['matrices']
74,X and Y are both $N\times N$ matrices. How many of the $N^2$ equations in $XY=YX$ regarding the elements of X and Y are independent?,X and Y are both  matrices. How many of the  equations in  regarding the elements of X and Y are independent?,N\times N N^2 XY=YX,"I tried the case of $N=2$ : Let $X=\begin{pmatrix}a&b \\ c&d\end{pmatrix},Y=\begin{pmatrix}e&f \\ g&h\end{pmatrix}$ . $XY=YX$ means that $$ \begin{pmatrix}  a e+b g & a f+b h \\  c e+d g & c f+d h \\ \end{pmatrix}= \begin{pmatrix}  a e+c f & b e+d f \\  a g+c h & b g+d h \\ \end{pmatrix}$$ which is $N^2=4$ equations about $a,b,c,d,e,f,g,h$ , But only $3$ of them are independent. $tr(XY-YX)=0$ , so at most $N^2-1$ equations are independent. Is $N^2-1$ the answer?","I tried the case of : Let . means that which is equations about , But only of them are independent. , so at most equations are independent. Is the answer?","N=2 X=\begin{pmatrix}a&b \\ c&d\end{pmatrix},Y=\begin{pmatrix}e&f \\ g&h\end{pmatrix} XY=YX 
\begin{pmatrix}
 a e+b g & a f+b h \\
 c e+d g & c f+d h \\
\end{pmatrix}=
\begin{pmatrix}
 a e+c f & b e+d f \\
 a g+c h & b g+d h \\
\end{pmatrix} N^2=4 a,b,c,d,e,f,g,h 3 tr(XY-YX)=0 N^2-1 N^2-1","['linear-algebra', 'matrices']"
75,Invertible matrices acting on set of subspaces of the same dimension,Invertible matrices acting on set of subspaces of the same dimension,,"I'm reading a paper and have stumbled upon the following phrases (everything over an arbitrary field $\mathbb{K}$ ): ""As the natural action of $\operatorname{GL}_n(\mathbb{K})$ on the set of linear hyperplanes of $\mathbb{K}^n$ is transitive..."" ""Let $H$ be a subspace of $\mathbb{K}^n$ of codimension $2$ . Since $\operatorname{GL}_n(\mathbb{K})$ acts transitively on the set of $(n − 2)$ -dimensional linear subspaces of $\mathbb{K}^n$ , we may also assume that $H$ is the subspace defined by the following system of two (independent) linear equations... "" What is this action exactly? The first thing I could think of was that it was simply permutation matrices permuting the basis vectors of each subspace in the set, and my initial reaction was to choose (e.g. for the hyperplane case) the orbit of the vectors with last coordinate $0$ , whence (by permutation), we obtain the subspaces with $0$ each of the $n$ coordinates. However, I stumbled upon the following obstacles: I think (I may be wrong on this) that, for this to be valid, we first need to choose a basis for each subspace, and I don't know if this is permitted. Even after choosing a basis, I can see how to include subspaces, whose basis is a subset of the standard basis, in the orbit, but there are infinitely many hyperplanes and permutting a subset of the standard basis to get a not-so-nice looking basis for another subspace is not clear to me. As I'm writing 2), I realise that I have probably made a choice of basis implicitly when I'm speaking of permutation matrices: They contain $0$ 's and $1$ 's, so they are supposed to permute the vectors of the standard basis, so now I'm asking myself how would we go about permuting vectors of a non-standard basis. Off the top of my head, I can think of Gram-Schmidt, but this does not feel like it solves the problem completely. I'm sorry about the mumbo-jumbo, but I'm really trying to understand this intuitively, so I may be overthinking quite a bit. Is there any merit in my thoughts, or the action does not have anything to do with permutation matrices whatsoever?","I'm reading a paper and have stumbled upon the following phrases (everything over an arbitrary field ): ""As the natural action of on the set of linear hyperplanes of is transitive..."" ""Let be a subspace of of codimension . Since acts transitively on the set of -dimensional linear subspaces of , we may also assume that is the subspace defined by the following system of two (independent) linear equations... "" What is this action exactly? The first thing I could think of was that it was simply permutation matrices permuting the basis vectors of each subspace in the set, and my initial reaction was to choose (e.g. for the hyperplane case) the orbit of the vectors with last coordinate , whence (by permutation), we obtain the subspaces with each of the coordinates. However, I stumbled upon the following obstacles: I think (I may be wrong on this) that, for this to be valid, we first need to choose a basis for each subspace, and I don't know if this is permitted. Even after choosing a basis, I can see how to include subspaces, whose basis is a subset of the standard basis, in the orbit, but there are infinitely many hyperplanes and permutting a subset of the standard basis to get a not-so-nice looking basis for another subspace is not clear to me. As I'm writing 2), I realise that I have probably made a choice of basis implicitly when I'm speaking of permutation matrices: They contain 's and 's, so they are supposed to permute the vectors of the standard basis, so now I'm asking myself how would we go about permuting vectors of a non-standard basis. Off the top of my head, I can think of Gram-Schmidt, but this does not feel like it solves the problem completely. I'm sorry about the mumbo-jumbo, but I'm really trying to understand this intuitively, so I may be overthinking quite a bit. Is there any merit in my thoughts, or the action does not have anything to do with permutation matrices whatsoever?",\mathbb{K} \operatorname{GL}_n(\mathbb{K}) \mathbb{K}^n H \mathbb{K}^n 2 \operatorname{GL}_n(\mathbb{K}) (n − 2) \mathbb{K}^n H 0 0 n 0 1,"['linear-algebra', 'matrices', 'group-theory', 'group-actions', 'permutation-matrices']"
76,Prove that if $AB = BA$ then $\log{(AB)} = \log{(A)} + \log{(B)}$,Prove that if  then,AB = BA \log{(AB)} = \log{(A)} + \log{(B)},"Background I am solving an assignment on the fundamentals of Quantum Computing which mostly has questions on Linear Algebra. All matrices and vectors are over complex domains . Question Define the exponent of a matrix as $$e^{\lambda A} = \sum_{i = 0}^{\infty}\frac{\lambda^i}{i!}A^i$$ where $A^0 = \mathbb{I}$ or the identity operator. Given a matrix $B$ , the matrix logarithm of $B$ is defined as the matrix $A$ such that $e^A = B$ . Prove that $\log$ is a non-unique function (i.e. for every $B$ , there are several $A$ that satisfy the matrix formula given). Prove that for two matrices, if $AB = BA$ , then $\log(AB) = \log(A) + \log(B)$ . My approach For this part, I am firstly not sure about the existence of an $A$ , it doesn't seem obvious to me why such an $A$ should always exist and if so, how to compute it. Nonetheless, assuming that we have a solution to $\log(B) = A$ , then I try to prove that I can derive a family of solutions from it (this is my intuition) $$\therefore B = \mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots $$ Assume we have an $A' = A + 2\pi i \mathbb{I}$ such that $$ e^{A'} = \mathbb{I} + (A+2\pi i)\mathbb{I} + \frac{({A + 2\pi i\mathbb{I}})^2}{2!} + \frac{({A + 2\pi i\mathbb{I}})^3}{3!} + \ldots $$ Since $A$ and $\mathbb{I}$ always commute, we can write the sum as (the proof is just algebraic manipulation and application of the binomial theorem, so I've skipped it) $$e^{A'} = \left(\mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots\right)\left(\mathbb{I} + \frac{2\pi i}{1!}\mathbb{I} + \frac{(2\pi i)^2}{2!}\mathbb{I} + \frac{(2 \pi i)^3}{3!} \mathbb{I} + \ldots \right)$$ $$\therefore e^{A'} = e^A \mathbb{I} \left(1 + \frac{2\pi i}{1!}+ \frac{(2\pi i)^2}{2!}\ + \frac{(2 \pi i)^3}{3!} + \ldots \right)$$ $$\therefore e^{A'} = e^A \mathbb{I} e^{2 \pi i} = B$$ Similarly $\forall n \in \mathbb{Z}, \exp{(A + 2n \pi i \mathbb{I})} = B$ This finishes the proof Assume $e^X = AB, e^Y = A, e^Z = B$ To prove that $X = Y + Z$ We write out the expansions of $A$ and $B$ and then use the fact that $AB = BA$ $$AB = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2YZ + Z^2}{2!} + \frac{Y^3 + 3Y^2Z + 3YZ^2 + Z^3}{3!} + \ldots$$ $$BA = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2ZY + Z^2}{2!} + \frac{Y^3 + 3ZY^2 + 3Z^2Y + Z^3}{3!} + \ldots$$ Put $AB - BA = \mathbb{O}$ $$\therefore (YZ - ZY) + \frac{1}{2!} (Y^2Z - ZY^2) + \frac{1}{2!}(YZ^2 - Z^2Y) + \ldots = 0$$ This is where I am stuck Obviously if $Y$ and $Z$ commute, then the above expression equates to 0 and thus from the expansions of $AB$ and $BA$ , if $Y$ and $Z$ would commute, $AB = BA = e^{Y+Z} = e^X$ and thus our proof is complete. The question is: how do I prove that this is the only case where the summation is 0? If $Y$ and $Z$ don't commute, then is it possible to proceed with the proof? Are there any alternate approaches?","Background I am solving an assignment on the fundamentals of Quantum Computing which mostly has questions on Linear Algebra. All matrices and vectors are over complex domains . Question Define the exponent of a matrix as where or the identity operator. Given a matrix , the matrix logarithm of is defined as the matrix such that . Prove that is a non-unique function (i.e. for every , there are several that satisfy the matrix formula given). Prove that for two matrices, if , then . My approach For this part, I am firstly not sure about the existence of an , it doesn't seem obvious to me why such an should always exist and if so, how to compute it. Nonetheless, assuming that we have a solution to , then I try to prove that I can derive a family of solutions from it (this is my intuition) Assume we have an such that Since and always commute, we can write the sum as (the proof is just algebraic manipulation and application of the binomial theorem, so I've skipped it) Similarly This finishes the proof Assume To prove that We write out the expansions of and and then use the fact that Put This is where I am stuck Obviously if and commute, then the above expression equates to 0 and thus from the expansions of and , if and would commute, and thus our proof is complete. The question is: how do I prove that this is the only case where the summation is 0? If and don't commute, then is it possible to proceed with the proof? Are there any alternate approaches?","e^{\lambda A} = \sum_{i = 0}^{\infty}\frac{\lambda^i}{i!}A^i A^0 = \mathbb{I} B B A e^A = B \log B A AB = BA \log(AB) = \log(A) + \log(B) A A \log(B) = A \therefore B = \mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots  A' = A + 2\pi i \mathbb{I}  e^{A'} = \mathbb{I} + (A+2\pi i)\mathbb{I} + \frac{({A + 2\pi i\mathbb{I}})^2}{2!} + \frac{({A + 2\pi i\mathbb{I}})^3}{3!} + \ldots  A \mathbb{I} e^{A'} = \left(\mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots\right)\left(\mathbb{I} + \frac{2\pi i}{1!}\mathbb{I} + \frac{(2\pi i)^2}{2!}\mathbb{I} + \frac{(2 \pi i)^3}{3!} \mathbb{I} + \ldots \right) \therefore e^{A'} = e^A \mathbb{I} \left(1 + \frac{2\pi i}{1!}+ \frac{(2\pi i)^2}{2!}\ + \frac{(2 \pi i)^3}{3!} + \ldots \right) \therefore e^{A'} = e^A \mathbb{I} e^{2 \pi i} = B \forall n \in \mathbb{Z}, \exp{(A + 2n \pi i \mathbb{I})} = B e^X = AB, e^Y = A, e^Z = B X = Y + Z A B AB = BA AB = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2YZ + Z^2}{2!} + \frac{Y^3 + 3Y^2Z + 3YZ^2 + Z^3}{3!} + \ldots BA = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2ZY + Z^2}{2!} + \frac{Y^3 + 3ZY^2 + 3Z^2Y + Z^3}{3!} + \ldots AB - BA = \mathbb{O} \therefore (YZ - ZY) + \frac{1}{2!} (Y^2Z - ZY^2) + \frac{1}{2!}(YZ^2 - Z^2Y) + \ldots = 0 Y Z AB BA Y Z AB = BA = e^{Y+Z} = e^X Y Z","['linear-algebra', 'matrices', 'exponentiation', 'quantum-mechanics']"
77,Is the dual norm of an induced norm an induced norm?,Is the dual norm of an induced norm an induced norm?,,"I think this Q&A gets close to answering this, but does not provide a full response. If you have some induced matrix norm $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}$ induced by the vector norm $\|\cdot\|'$ , I'm wondering if is it the dual of this norm (i.e. $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ ) also an induced norm? Furthermore, if $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ is an induced norm, is there a relationship between the vector norm $\|\cdot\|'$ and the vector norm which would induce $\|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D$ ? I was considering trying to work with what I think is just an unnamed Theorem, $$ \|A\|_{\|\cdot\|, \|\cdot\|} = \|A^*\|_{\|\cdot\|^D, \|\cdot\|^D}, $$ but that doesn't seem to be getting me anywhere.","I think this Q&A gets close to answering this, but does not provide a full response. If you have some induced matrix norm induced by the vector norm , I'm wondering if is it the dual of this norm (i.e. ) also an induced norm? Furthermore, if is an induced norm, is there a relationship between the vector norm and the vector norm which would induce ? I was considering trying to work with what I think is just an unnamed Theorem, but that doesn't seem to be getting me anywhere.","\|\cdot\|_{\|\cdot\|', \|\cdot\|'} \|\cdot\|' \|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D \|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D \|\cdot\|' \|\cdot\|_{\|\cdot\|', \|\cdot\|'}^D 
\|A\|_{\|\cdot\|, \|\cdot\|} = \|A^*\|_{\|\cdot\|^D, \|\cdot\|^D},
","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
78,Linear transformation of $A^T A$,Linear transformation of,A^T A,"Suppose that we have linear transformation $T:V \to U$ , then $T$ induces a map on dual spaces $T^*: U^* \to V^*$ via the pullback and suppose further that they are given by the matrices $A$ and $A^T$ respectively. A matrix that is often of interest is $A^TA$ . Since matrix multiplication corresponds to composition of linear maps, I would naively think that $A^TA$ corresponds to $T^* \circ T$ but this is not a valid composition (codomain and domain do not match)! So my question is what linear map $A^TA$ corresponds to? Thank you!","Suppose that we have linear transformation , then induces a map on dual spaces via the pullback and suppose further that they are given by the matrices and respectively. A matrix that is often of interest is . Since matrix multiplication corresponds to composition of linear maps, I would naively think that corresponds to but this is not a valid composition (codomain and domain do not match)! So my question is what linear map corresponds to? Thank you!",T:V \to U T T^*: U^* \to V^* A A^T A^TA A^TA T^* \circ T A^TA,"['linear-algebra', 'matrices', 'linear-transformations', 'dual-spaces']"
79,How to use the minimal polynomial theorem for this block matrix,How to use the minimal polynomial theorem for this block matrix,,"Let $M$ be a matrix made up of two diagonal blocks: $M = \begin{pmatrix} A & 0 \\ 0 & D\\ \end{pmatrix}$ Prove that $M$ is diagonalizable if and only if A and D are diagonalizable. I know I'd have to use the minimal polynomial theorem, I just learnt it with one diagonal matrix in the diagonal, so for some reason I can't think of a way to implement it here Is the minimal polynomial equation $M_{min} = A_{min} * D_{min}$ ? And after that can I say $A_{min} * D_{min} = 0$ ?","Let be a matrix made up of two diagonal blocks: Prove that is diagonalizable if and only if A and D are diagonalizable. I know I'd have to use the minimal polynomial theorem, I just learnt it with one diagonal matrix in the diagonal, so for some reason I can't think of a way to implement it here Is the minimal polynomial equation ? And after that can I say ?",M M = \begin{pmatrix} A & 0 \\ 0 & D\\ \end{pmatrix} M M_{min} = A_{min} * D_{min} A_{min} * D_{min} = 0,"['linear-algebra', 'matrices', 'diagonalization', 'minimal-polynomials', 'block-matrices']"
80,Universal ring with zero matrix product is an integral domain?,Universal ring with zero matrix product is an integral domain?,,"Let $n \geq 0$ . The universal commutative ring which has two $n \times n$ -matrices whose product is zero is $$\textstyle R := \mathbb{Z}[(X_{ij})_{1 \leq i,j \leq n}, (Y_{ij})_{1 \leq i,j \leq n}] / \langle \sum_{k} X_{ik} Y_{kj} = 0 : 1 \leq i,j \leq n \rangle$$ So we have $2n^2$ generators and $n^2$ relations. If $n = 1$ , then $R$ is just $\mathbb{Z}[u,v] / \langle uv = 0 \rangle$ , hence not an integral domain. Question. If $n > 1$ , is $R$ an integral domain? And if not, can we at least show that $R$ is reduced? Unfortunately I don't have an idea how to attack this problem, already for $n=2$ . I vaguely remember that there are methods from algebraic geometry to show that a ring is an integral domain, but I don't know if this applies here.","Let . The universal commutative ring which has two -matrices whose product is zero is So we have generators and relations. If , then is just , hence not an integral domain. Question. If , is an integral domain? And if not, can we at least show that is reduced? Unfortunately I don't have an idea how to attack this problem, already for . I vaguely remember that there are methods from algebraic geometry to show that a ring is an integral domain, but I don't know if this applies here.","n \geq 0 n \times n \textstyle R := \mathbb{Z}[(X_{ij})_{1 \leq i,j \leq n}, (Y_{ij})_{1 \leq i,j \leq n}] / \langle \sum_{k} X_{ik} Y_{kj} = 0 : 1 \leq i,j \leq n \rangle 2n^2 n^2 n = 1 R \mathbb{Z}[u,v] / \langle uv = 0 \rangle n > 1 R R n=2","['abstract-algebra', 'matrices', 'algebraic-geometry', 'commutative-algebra']"
81,Matrix exponentials of Jordan blocks,Matrix exponentials of Jordan blocks,,"Let $f:\Bbb C→\Bbb C$ be an analytic function1.  Let $\lambda \in\Bbb C$ and let $$ J=\begin{pmatrix} \lambda & 1 &  & &\\  &\ddots& \ddots \\  &  & \ddots& 1\\ & & &\lambda\end{pmatrix} = \lambda\begin{pmatrix} 1& 0 &  & &\\  &\ddots& \ddots \\  &  & \ddots& 0\\ & & &1\end{pmatrix} +\begin{pmatrix} 0 & 1 &  & &\\  &\ddots& \ddots \\  &  & \ddots& 1\\ & & &0\end{pmatrix}=\lambda I+N\in \Bbb C ^{l×l}$$ (Such matrices J appear as Jordan blocks in the Jordan canonical form of a matrix.)Consider the Taylor expansion of f around $\lambda∈\Bbb C$ , i.e., $f(a) =\sum_{k=0}^n\frac{f^{(k)}(\lambda)}{k!}(a−\lambda)^k$ .We define a matrix valued function via the formal power series $F(A) =\sum_{k=0}^\infty \frac{f^{(k)}(\lambda)}{k!} (A−\lambda I)k\in\Bbb C^{l×l}$ .(By definition, $A^0=I$ for any square matrix A.)One can show that the matrix function F(A) is well-defined, i. e., the above power series converges (absolutely) for any matrix $A\in\Bbb C^{l×l}$ , because f is analytic. Establish the following explicit finite formula for the matrix $F(J)\in \Bbb C^{l×l}$ in terms of f and its derivatives in $\lambda$ : $$ F(J) =\begin{pmatrix} f(\lambda) &\frac{f′(\lambda)}{1!} &\frac{f′′(\lambda)}{2!} &\cdots&\frac{f^{(l−1)}(\lambda)}{(l−1)!} &\\ 0 &f(\lambda)& \frac{f′(\lambda)}{1}&\ddots&\vdots\\  0& 0 & \ddots& \ddots&\frac{f′′(\lambda)}{2!}\\ \vdots &\cdots&\ddots&f(\lambda)& \frac{f′(\lambda)}{1!}\\ 0 &\cdots&\cdots&0&f(\lambda)\end{pmatrix}$$ Answer: To establish the finite formula for the matrix F(J), we first note that J can be expressed as J = A + N, where A is a diagonal matrix with the eigenvalue $\lambda$ and N is a nilpotent matrix $(i.e., N^n = 0)$ as  mentioned. Now, we want to compute the powers of N inductively: $N^1= N$ $N^2 = N ✕ N = 0$ (since N is nilpotent) $N^3 = N ✕ N^2 = 0$ ... So, we see that $N^k = 0$ for $ k \ge 2$ . Now, let's compute F(J) using the Taylor expansion: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]^* (J - \lambda^*I)^k$ Since $N^2 = 0$ , the expansion simplifies: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A + N - \lambda I)^k$ Now, we can apply the binomial theorem, which states that for any matrices A and B: $(A + B)^k = \sum_{j=0}^k (k choose j) A^{(k-j)} B^j$ In our case, $A = A - \lambda I$ and B = N, and k can be any non-negative integer. So, the above expression becomes: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]$ $\sum_{j=0}^k (k  choose  j)  (A - \lambda I)^{(k-j)}  N^j$ Now, since $N^2 = 0$ , the terms with $j \ge 2$ in the inner sum vanish, and we're left with: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-0)}  N^0] + [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-1)}  N^1]$ Since $N^0$ is the identity matrix I, we have: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - λI)^k + \sum_{k=0}^\infty[\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)}  N$ Now, we can separate the terms involving A and N: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A - \lambda I)^{(k-1)}$ The second sum involving N can be simplified using the property of the derivative of the matrix function: $N * \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = N  f'(\lambda)  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)}$ Now, we can recognize that the second sum is the derivative of the matrix function with respect to A evaluated at $\lambda$ : $\sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = f'(\lambda)  ∂F(A)/∂A|_{(\lambda)}$ So, we have: $F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  f'(\lambda)  ∂F(A)/∂A|_{(λ)}$ Now, the formula for F(J) in terms of f and its derivatives in X is established. I want to know is my answer correct for this question as i am in the new place , and i don’t know anyone here to ask. Or can anyone provide me hints.","Let be an analytic function1.  Let and let (Such matrices J appear as Jordan blocks in the Jordan canonical form of a matrix.)Consider the Taylor expansion of f around , i.e., .We define a matrix valued function via the formal power series .(By definition, for any square matrix A.)One can show that the matrix function F(A) is well-defined, i. e., the above power series converges (absolutely) for any matrix , because f is analytic. Establish the following explicit finite formula for the matrix in terms of f and its derivatives in : Answer: To establish the finite formula for the matrix F(J), we first note that J can be expressed as J = A + N, where A is a diagonal matrix with the eigenvalue and N is a nilpotent matrix as  mentioned. Now, we want to compute the powers of N inductively: (since N is nilpotent) ... So, we see that for . Now, let's compute F(J) using the Taylor expansion: Since , the expansion simplifies: Now, we can apply the binomial theorem, which states that for any matrices A and B: In our case, and B = N, and k can be any non-negative integer. So, the above expression becomes: Now, since , the terms with in the inner sum vanish, and we're left with: Since is the identity matrix I, we have: Now, we can separate the terms involving A and N: The second sum involving N can be simplified using the property of the derivative of the matrix function: Now, we can recognize that the second sum is the derivative of the matrix function with respect to A evaluated at : So, we have: Now, the formula for F(J) in terms of f and its derivatives in X is established. I want to know is my answer correct for this question as i am in the new place , and i don’t know anyone here to ask. Or can anyone provide me hints.","f:\Bbb C→\Bbb C \lambda \in\Bbb C 
J=\begin{pmatrix}
\lambda & 1 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 1\\
& & &\lambda\end{pmatrix} = \lambda\begin{pmatrix}
1& 0 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 0\\
& & &1\end{pmatrix} +\begin{pmatrix}
0 & 1 &  & &\\
 &\ddots& \ddots \\
 &  & \ddots& 1\\
& & &0\end{pmatrix}=\lambda I+N\in \Bbb C ^{l×l} \lambda∈\Bbb C f(a) =\sum_{k=0}^n\frac{f^{(k)}(\lambda)}{k!}(a−\lambda)^k F(A) =\sum_{k=0}^\infty \frac{f^{(k)}(\lambda)}{k!} (A−\lambda I)k\in\Bbb C^{l×l} A^0=I A\in\Bbb C^{l×l} F(J)\in \Bbb C^{l×l} \lambda 
F(J) =\begin{pmatrix}
f(\lambda) &\frac{f′(\lambda)}{1!} &\frac{f′′(\lambda)}{2!} &\cdots&\frac{f^{(l−1)}(\lambda)}{(l−1)!} &\\
0 &f(\lambda)& \frac{f′(\lambda)}{1}&\ddots&\vdots\\
 0& 0 & \ddots& \ddots&\frac{f′′(\lambda)}{2!}\\
\vdots &\cdots&\ddots&f(\lambda)& \frac{f′(\lambda)}{1!}\\
0 &\cdots&\cdots&0&f(\lambda)\end{pmatrix} \lambda (i.e., N^n = 0) N^1= N N^2 = N ✕ N = 0 N^3 = N ✕ N^2 = 0 N^k = 0  k \ge 2 F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]^* (J - \lambda^*I)^k N^2 = 0 F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A + N - \lambda I)^k (A + B)^k = \sum_{j=0}^k (k choose j) A^{(k-j)} B^j A = A - \lambda I F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] \sum_{j=0}^k (k  choose  j)  (A - \lambda I)^{(k-j)}  N^j N^2 = 0 j \ge 2 F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-0)}  N^0] + [\frac{f^{(k)}(\lambda)}{k!}]  [(A - \lambda I)^{(k-1)}  N^1] N^0 F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - λI)^k + \sum_{k=0}^\infty[\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)}  N F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}] (A - \lambda I)^{(k-1)} N * \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = N  f'(\lambda)  \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} \lambda \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^{(k-1)} = f'(\lambda)  ∂F(A)/∂A|_{(\lambda)} F(J) = \sum_{k=0}^\infty [\frac{f^{(k)}(\lambda)}{k!}]  (A - \lambda I)^k + N  f'(\lambda)  ∂F(A)/∂A|_{(λ)}","['linear-algebra', 'matrices', 'solution-verification', 'matrix-equations', 'jordan-normal-form']"
82,Geometric Interpretation of Matrices with Repeated Nonzero Singular Values,Geometric Interpretation of Matrices with Repeated Nonzero Singular Values,,"Let $P\in \mathbb{R}^{d\times n},d<n$ be a $d$ -dimensional coordinate matrix with $\text{rank}(P)=d$ . And let singular value decomposition $P = U\Sigma V^T$ where $U,V$ are orthogonal matrix and $\Sigma\in \mathbb{R}^{d\times n}$ is a diagonal matrix with non-negative real numbers as its diagonal entries. I am particularly interested in understanding the geometric and symmetry implications of having repeated nonzero singular values for $P$ . Specifically, if $\Sigma_{ii} = \Sigma_{jj} > 0$ , it is clear that there exists an infinite number of corresponding unit vectors in $U$ that can perform the orthogonal transformation. In other words, these dimensions form an invariant subspace under some actions of $O(d)$ . However, I am uncertain if this is directly related to the geometry or symmetry of $P$ . Are there any theorems or properties that can shed light on the direct geometric or symmetry implications of repeated nonzero singular values in $P$ ? Any insights, references, or suggestions for further reading would be greatly appreciated.","Let be a -dimensional coordinate matrix with . And let singular value decomposition where are orthogonal matrix and is a diagonal matrix with non-negative real numbers as its diagonal entries. I am particularly interested in understanding the geometric and symmetry implications of having repeated nonzero singular values for . Specifically, if , it is clear that there exists an infinite number of corresponding unit vectors in that can perform the orthogonal transformation. In other words, these dimensions form an invariant subspace under some actions of . However, I am uncertain if this is directly related to the geometry or symmetry of . Are there any theorems or properties that can shed light on the direct geometric or symmetry implications of repeated nonzero singular values in ? Any insights, references, or suggestions for further reading would be greatly appreciated.","P\in \mathbb{R}^{d\times n},d<n d \text{rank}(P)=d P = U\Sigma V^T U,V \Sigma\in \mathbb{R}^{d\times n} P \Sigma_{ii} = \Sigma_{jj} > 0 U O(d) P P","['matrices', 'group-theory', 'reference-request', 'matrix-decomposition']"
83,When is row equivalence equivalent to the equivalence of systems of equations?,When is row equivalence equivalent to the equivalence of systems of equations?,,"Theorem 3 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 3: If $A$ and $B$ are row-equivalent $m \times n$ matrices, the homogeneous systems of linear equations $AX = 0$ and $BX = 0$ have exactly the same solutions. The proof intimately uses that the systems in question are homogeneous, but I'm not sure I can quite see how. I am also led to wonder about how this homogeneous assumption can be relaxed (perhaps I am jumping the gun and HK will discuss this). I wonder about these two questions at any rate: Can someone explicate exactly how the homogeneous assumption is used? In the crucial step, Hoffman and Kunze write (after reducing the proof by induction to equivalence of systems differing by just one row operation) No matter which of the three types the row operation is, each equation in the system $A_{j+1}X$ = 0 will be a linear combination of the equations in the system $AX = 0$ . Now my confusion is that row operations only act on the matrix $A$ of coefficients, not the vector $Y$ of inhomogeneous scalars. Is the point that, in general, we would also have to do the row operations on $Y$ in order to obtain a system of equations which is a linear combination of the original equations, but that here since $Y = 0$ this action is trivial? Theorem 3 can perhaps be rewritten as ""if two systems of equations are homogeneous then row equivalence of their coefficient matrices implies that the systems are equivalent"" (here we recall that systems of equations being equivalent means that their constituent equations can be written as linear combinations of one another). My question here is (a) does the converse hold (""if two systems of equations are homogeneous then their equivalence implies row equivalence of their coefficient matrices"") and (b) if (a) is true, is there an if and only if statement to be similarly made if we relax homogeneity (presumably we'd need to add a hypothesis about the $Y$ vector)? If it's possible for any answer to be really clear about all the possible cases at play here I would greatly appreciate it, as I know there are some pathological cases involving inconsistent systems which sometimes confuse me.","Theorem 3 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 3: If and are row-equivalent matrices, the homogeneous systems of linear equations and have exactly the same solutions. The proof intimately uses that the systems in question are homogeneous, but I'm not sure I can quite see how. I am also led to wonder about how this homogeneous assumption can be relaxed (perhaps I am jumping the gun and HK will discuss this). I wonder about these two questions at any rate: Can someone explicate exactly how the homogeneous assumption is used? In the crucial step, Hoffman and Kunze write (after reducing the proof by induction to equivalence of systems differing by just one row operation) No matter which of the three types the row operation is, each equation in the system = 0 will be a linear combination of the equations in the system . Now my confusion is that row operations only act on the matrix of coefficients, not the vector of inhomogeneous scalars. Is the point that, in general, we would also have to do the row operations on in order to obtain a system of equations which is a linear combination of the original equations, but that here since this action is trivial? Theorem 3 can perhaps be rewritten as ""if two systems of equations are homogeneous then row equivalence of their coefficient matrices implies that the systems are equivalent"" (here we recall that systems of equations being equivalent means that their constituent equations can be written as linear combinations of one another). My question here is (a) does the converse hold (""if two systems of equations are homogeneous then their equivalence implies row equivalence of their coefficient matrices"") and (b) if (a) is true, is there an if and only if statement to be similarly made if we relax homogeneity (presumably we'd need to add a hypothesis about the vector)? If it's possible for any answer to be really clear about all the possible cases at play here I would greatly appreciate it, as I know there are some pathological cases involving inconsistent systems which sometimes confuse me.",A B m \times n AX = 0 BX = 0 A_{j+1}X AX = 0 A Y Y Y = 0 Y,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
84,Intriguing Tridiagonal Matrix,Intriguing Tridiagonal Matrix,,"Given a positive sequence $\{a_n\}_{n=1}^N$ , please consider the matrix $$  \begin{bmatrix} a_1 & -a_1 & & & &   \\  -a_1 & a_2 + a_1 & -a_2 & & & \\   & - a_2 & a_3 + a_2 & \ddots & &   \\   & & \ddots& \ddots & \ddots  &    \\   & & & \ddots & a_{N-1} + a_{N-2}&  -a_{N-1}  \\   & & & & -a_{N-1}&  a_{N-1}   \end{bmatrix}$$ where blank areas denote null matrix elements. Does this type of matrix have a name, or resembles others from contexts you know of? It is real, symmetric, tridiagonal. All its rows sum to zero. All its columns sum to zero as well. Can anything nontrivial be said on its eigenvalues? I believe (correct me if I'm wrong) that the kernel is one-dimensional, generated by the vector $(1,1,\ldots$ ). In the 3d case, I see that the nonzero eigenvalues are simply $a_1 + a_2 \pm \sqrt{a_1^2 - a_1 a_2 + a_2^2}$ . So I am trying to guess a generalization of this formula - of which several come to mind. But there may be smarter way to go about it? Thank you very much for any tip you may have.","Given a positive sequence , please consider the matrix where blank areas denote null matrix elements. Does this type of matrix have a name, or resembles others from contexts you know of? It is real, symmetric, tridiagonal. All its rows sum to zero. All its columns sum to zero as well. Can anything nontrivial be said on its eigenvalues? I believe (correct me if I'm wrong) that the kernel is one-dimensional, generated by the vector ). In the 3d case, I see that the nonzero eigenvalues are simply . So I am trying to guess a generalization of this formula - of which several come to mind. But there may be smarter way to go about it? Thank you very much for any tip you may have.","\{a_n\}_{n=1}^N   \begin{bmatrix}
a_1 & -a_1 & & & &   \\ 
-a_1 & a_2 + a_1 & -a_2 & & & \\ 
 & - a_2 & a_3 + a_2 & \ddots & &   \\ 
 & & \ddots& \ddots & \ddots  &    \\ 
 & & & \ddots & a_{N-1} + a_{N-2}&  -a_{N-1}  \\ 
 & & & & -a_{N-1}&  a_{N-1}  
\end{bmatrix} (1,1,\ldots a_1 + a_2 \pm \sqrt{a_1^2 - a_1 a_2 + a_2^2}","['matrices', 'eigenvalues-eigenvectors', '3d', 'diagonalization', 'tridiagonal-matrices']"
85,"Each $A\in SO(3,\mathbb{R})$ satisfies $A^3-\alpha A^2+\alpha A-I_3=0$ with $-1\leq \alpha \leq 3$",Each  satisfies  with,"A\in SO(3,\mathbb{R}) A^3-\alpha A^2+\alpha A-I_3=0 -1\leq \alpha \leq 3","Question: Let $G=SO(3,\mathbb{R})=\{A\in GL(3,\mathbb{R}):A^TA=I_3, \det(A)=1\}$ . a) Show that for any element $A$ in $G$ , there exists a real number $\alpha$ with $-1\leq\alpha\leq 3$ such that $A^3-\alpha A^2+\alpha A-I_3=0$ . b) For which real numbers $\alpha$ with $-1\leq \alpha\leq 3$ does there exist an element $A$ in $G$ whose minimal polynomial is $x^3-\alpha x^2+\alpha x-1$ ? Answer: What I know is that since the dimensions of the matrices are three, the characteristic polynomial of each $A$ must be a three dimensional polynomial. Also, since each three dimensional polynomial has at least one real root, every matrix $A$ must have an eigenvector. Moreover, any matrix $A$ in $G$ has the property that sum of the squares of the entries in each row of $A$ must be 1, i.e. $A_{i1}^2+A_{i2}^2+A_{i3}^2=1$ . However, I could not relate this knowledge to solve the questions. Any help/hint would be appreciated. Thanks in advance...","Question: Let . a) Show that for any element in , there exists a real number with such that . b) For which real numbers with does there exist an element in whose minimal polynomial is ? Answer: What I know is that since the dimensions of the matrices are three, the characteristic polynomial of each must be a three dimensional polynomial. Also, since each three dimensional polynomial has at least one real root, every matrix must have an eigenvector. Moreover, any matrix in has the property that sum of the squares of the entries in each row of must be 1, i.e. . However, I could not relate this knowledge to solve the questions. Any help/hint would be appreciated. Thanks in advance...","G=SO(3,\mathbb{R})=\{A\in GL(3,\mathbb{R}):A^TA=I_3, \det(A)=1\} A G \alpha -1\leq\alpha\leq 3 A^3-\alpha A^2+\alpha A-I_3=0 \alpha -1\leq \alpha\leq 3 A G x^3-\alpha x^2+\alpha x-1 A A A G A A_{i1}^2+A_{i2}^2+A_{i3}^2=1","['linear-algebra', 'matrices']"
86,Involutory matrix under prime modulo,Involutory matrix under prime modulo,,"A matrix $A \neq I $ is called involutory modulo $m$ if $A^2≡I (\mod m).$ Prove or disprove that if $A$ is a $2 × 2 $ involutory matrix modulo $m$ , then $ \det(A) ≡ ±1 (\mod m)$ . I already disprove this statement but I have doubt about what happens if $m=p$ be a prime. I assume a matrix $A$ is involutory modulo $p$ . Therefore it is the self-inverse matrix now, $\det(A)^2=\Delta ^2=1(\mod p)$ from that I got $\Delta =\pm1 (\mod p)$ Is this corect?","A matrix is called involutory modulo if Prove or disprove that if is a involutory matrix modulo , then . I already disprove this statement but I have doubt about what happens if be a prime. I assume a matrix is involutory modulo . Therefore it is the self-inverse matrix now, from that I got Is this corect?","A \neq I  m A^2≡I (\mod m). A 2 × 2  m  \det(A) ≡
±1 (\mod m) m=p A p \det(A)^2=\Delta ^2=1(\mod p) \Delta =\pm1 (\mod p)","['matrices', 'number-theory', 'elementary-number-theory', 'prime-numbers']"
87,A small proportion of the non-zero eigenvalues lies near zero,A small proportion of the non-zero eigenvalues lies near zero,,"Problem: We consider a sequence $(A_n)_{n\ge 1}$ of matrices such that $A_n$ is a $n\times n$ matrix with integer co-efficients, The sum of the absolute values of entries of any row is at-most $M$ . For every $\delta>0$ , let $N_{\delta}(A_n)$ denote the number of eigenvalues of $A_n$ of modulus strictly between $0$ and $\delta$ . Prove that If $\lambda$ is an eigenvalue of $A_n$ , then $|\lambda|\le M$ . Show that for every $\epsilon>0$ , there exists $\delta>0$ such that for every $n\ge 1$ , $$N_{\delta}(A_n)\le \epsilon n $$ My attempt: I could prove the first part using triangle inequality and stuff like that. But am completely stumped on the second part. I tried to proceed by contradiction and constructing sequence of eigenvalues, but that didn't seem to lead anywhere. I would appreciate some hints towards the correct direction, rather than a complete solution.","Problem: We consider a sequence of matrices such that is a matrix with integer co-efficients, The sum of the absolute values of entries of any row is at-most . For every , let denote the number of eigenvalues of of modulus strictly between and . Prove that If is an eigenvalue of , then . Show that for every , there exists such that for every , My attempt: I could prove the first part using triangle inequality and stuff like that. But am completely stumped on the second part. I tried to proceed by contradiction and constructing sequence of eigenvalues, but that didn't seem to lead anywhere. I would appreciate some hints towards the correct direction, rather than a complete solution.",(A_n)_{n\ge 1} A_n n\times n M \delta>0 N_{\delta}(A_n) A_n 0 \delta \lambda A_n |\lambda|\le M \epsilon>0 \delta>0 n\ge 1 N_{\delta}(A_n)\le \epsilon n ,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
88,Can one find an injective-matrix-valued approximation of a given matrix-valued continuous function?,Can one find an injective-matrix-valued approximation of a given matrix-valued continuous function?,,"This question is basically an extension of a previous question by replacing a matrix with a continuous matrix-valued function: Given a matrix $A \in \mathbb{R}^{m \times n}$ ($m > n$), can one always find an injective matrix $B$ close to $A$? Suppose that $X \subset \mathbb{R}^{n}$ is compact and $U \subset \mathbb{R}^{m}$ is convex compact. A continuous matrix-valued function $A: X \to \mathbb{R}^{l \times m}$ is given with $l > m$ . Given $\epsilon > 0$ , can one always find an injective-matrix-valued continuous function $B: X \to \mathbb{R}^{l \times m}$ such that for any $x \in X$ , $\lVert A(x)-B(x) \rVert < \epsilon$ and $u_{1} \neq u_{2}$ $\Rightarrow$ $B(x)u_{1} \neq B(x)u_{2}$ ? Note from the comments of the previous question that one can construct an injective matrix $B$ for a given matrix $A$ by singular value decomposition. My understanding is something like this: SVD of $A \in \mathbb{R}^{l \times m}$ ; $A = U \Sigma V^{*}$ . Define $B := U \left( \Sigma + t\begin{bmatrix} I \\ 0 \end{bmatrix} \right) V^{*}$ , which satisfies the requirement of the previous question with arbitrarily small $t \in \mathbb{R}_{>0}$ . However, this approach is not easily extended for this question as the SVD of $A(x)$ is not uniquely determined in general for given $x \in X$ and it seems hard to guarantee that there exists $U(x)$ , $V^{*}(x)$ , and $\Sigma(x)$ continuous w.r.t. $x$ .","This question is basically an extension of a previous question by replacing a matrix with a continuous matrix-valued function: Given a matrix $A \in \mathbb{R}^{m \times n}$ ($m > n$), can one always find an injective matrix $B$ close to $A$? Suppose that is compact and is convex compact. A continuous matrix-valued function is given with . Given , can one always find an injective-matrix-valued continuous function such that for any , and ? Note from the comments of the previous question that one can construct an injective matrix for a given matrix by singular value decomposition. My understanding is something like this: SVD of ; . Define , which satisfies the requirement of the previous question with arbitrarily small . However, this approach is not easily extended for this question as the SVD of is not uniquely determined in general for given and it seems hard to guarantee that there exists , , and continuous w.r.t. .",X \subset \mathbb{R}^{n} U \subset \mathbb{R}^{m} A: X \to \mathbb{R}^{l \times m} l > m \epsilon > 0 B: X \to \mathbb{R}^{l \times m} x \in X \lVert A(x)-B(x) \rVert < \epsilon u_{1} \neq u_{2} \Rightarrow B(x)u_{1} \neq B(x)u_{2} B A A \in \mathbb{R}^{l \times m} A = U \Sigma V^{*} B := U \left( \Sigma + t\begin{bmatrix} I \\ 0 \end{bmatrix} \right) V^{*} t \in \mathbb{R}_{>0} A(x) x \in X U(x) V^{*}(x) \Sigma(x) x,"['matrices', 'approximation', 'svd']"
89,Getting the original matrix from the projection,Getting the original matrix from the projection,,"If I have the projection matrix of $X$ , $$ P = X\,{(X^TX)}^{-1} X^T,  $$ how can I recover $X$ by only knowing $P$ ? Is there a way to do that?","If I have the projection matrix of , how can I recover by only knowing ? Is there a way to do that?","X 
P = X\,{(X^TX)}^{-1} X^T, 
 X P","['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition', 'projection-matrices']"
90,Relation between the level and the determinant of a matrix,Relation between the level and the determinant of a matrix,,"Let $ \mathcal{D}_k$ be the set of $k \times k$ integer , positive definite matrices with even diagonal . For $A \in \mathcal{D}_k$ we define the level of $A$ as the smallest integer $n_A \in \mathbb{N}$ such that $n_A A^{-1} \in \mathcal{D}_k$ . I was reading the chapter on theta functions on Iwaniec's ""Topics in classical automorphic forms"" and it seems like he uses the following property of $n_A$ (e.g. in Theorem 10.9): If a prime $p$ divides det $(A)$ , then $p$ divides $n_A$ . Although it seems like something elementary, I have not been able to prove it. I tried to use the formula $$A^{-1} = \frac{1}{\det (A)} \operatorname{adj} (A)$$ but didn't arrive anywhere. I have also worked some concrete examples, but all of them have $n_A = 2 \det (A)$ , for example: $$ A =\begin{pmatrix}  2 & 1 & 0\\ 1 & 2 & 1 \\ 0 & 1 & 2 \end{pmatrix}$$ so is not very helpful. Any help would be appreciated.","Let be the set of integer , positive definite matrices with even diagonal . For we define the level of as the smallest integer such that . I was reading the chapter on theta functions on Iwaniec's ""Topics in classical automorphic forms"" and it seems like he uses the following property of (e.g. in Theorem 10.9): If a prime divides det , then divides . Although it seems like something elementary, I have not been able to prove it. I tried to use the formula but didn't arrive anywhere. I have also worked some concrete examples, but all of them have , for example: so is not very helpful. Any help would be appreciated."," \mathcal{D}_k k \times k A \in \mathcal{D}_k A n_A \in \mathbb{N} n_A A^{-1} \in \mathcal{D}_k n_A p (A) p n_A A^{-1} = \frac{1}{\det (A)} \operatorname{adj} (A) n_A = 2 \det (A)  A =\begin{pmatrix} 
2 & 1 & 0\\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}","['matrices', 'quadratic-forms', 'symmetric-matrices', 'positive-definite']"
91,Finding a matrix and its inverse in terms of another one.,Finding a matrix and its inverse in terms of another one.,,I have a symmetric matrix $A$ . I wanted to modify its first row (thus it is first column). Let's call the new row $c^T$ . How do I write the modified matrix $B$ in terms of $A$ and $c$ . What is the inverse of $B$ in terms of $A$ and $c$ ?,I have a symmetric matrix . I wanted to modify its first row (thus it is first column). Let's call the new row . How do I write the modified matrix in terms of and . What is the inverse of in terms of and ?,A c^T B A c B A c,"['linear-algebra', 'matrices', 'inverse']"
92,Using a rotation matrix to rotate the graph of a function?,Using a rotation matrix to rotate the graph of a function?,,"I understand where the following matrices come from and how they geometrically rotate a point (x, y) counterclockwise/clockwise by $θ$ $\begin{bmatrix}\cosθ&-\sinθ\\\sinθ&\cosθ\end{bmatrix} $ for counterclockwise and $\begin{bmatrix}\cosθ&\sinθ\\-\sinθ&\cosθ\end{bmatrix} $ for clockwise For example, I want to rotate the graph ${y} = x^{2}$ by 45° counterclockwise. By appying this matrix to any point (x, y), $\begin{bmatrix}\cos45°&-\sin45°\\\sin45°&\cos45°\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y\\\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y\end{bmatrix}$ So, $x$ moves to $(\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y)$ and $y$ moves to $(\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y)$ If I plug these values into ${y} = x^{2}$ , it yields the following graph: As you can see, despite plugging in the counterclockwise-rotated points, the graph was rotated clockwise by 45°. So my question is: Why does plugging in the counterclockwise-rotated points result in a graph that is rotated clockwise? I know that in order to rotate the graph counterclockwise you just have to input the points rotated counterclockwise, but I am having trouble understanding why, conceptually.","I understand where the following matrices come from and how they geometrically rotate a point (x, y) counterclockwise/clockwise by for counterclockwise and for clockwise For example, I want to rotate the graph by 45° counterclockwise. By appying this matrix to any point (x, y), So, moves to and moves to If I plug these values into , it yields the following graph: As you can see, despite plugging in the counterclockwise-rotated points, the graph was rotated clockwise by 45°. So my question is: Why does plugging in the counterclockwise-rotated points result in a graph that is rotated clockwise? I know that in order to rotate the graph counterclockwise you just have to input the points rotated counterclockwise, but I am having trouble understanding why, conceptually.",θ \begin{bmatrix}\cosθ&-\sinθ\\\sinθ&\cosθ\end{bmatrix}  \begin{bmatrix}\cosθ&\sinθ\\-\sinθ&\cosθ\end{bmatrix}  {y} = x^{2} \begin{bmatrix}\cos45°&-\sin45°\\\sin45°&\cos45°\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y\\\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y\end{bmatrix} x (\frac{1}{\sqrt{2}}x-\frac{1}{\sqrt{2}}y) y (\frac{1}{\sqrt{2}}x+\frac{1}{\sqrt{2}}y) {y} = x^{2},"['linear-algebra', 'matrices', 'linear-transformations', 'graphing-functions', 'rotations']"
93,How to prove that a $n\times n$ matrix has distinct eigenvalues?,How to prove that a  matrix has distinct eigenvalues?,n\times n,"I am trying to solve the following question: Let A be an $n \times n$ matrix, and suppose that the only eigenvalues of $A$ are $0,$ $1,$ and $2.$ (a) Prove that $\dim E_1(A) + \dim E_2(A) \leq \operatorname{rank} A.$ Where $E_1$ and $E_2$ is the eigenspace corresponding to eigenvalues $1$ and $2$ respectively. I have found a way to prove this if all of the eigenvalues are distinct, because then the geometric multiplicity must be 1 for each of them, and thus the dimension there must be less than or equal to Rank(A). That's the approach I've taken to the problem so far. My assumption of the eigenvalues being distinct also allows for an easy proof that A is diagonalizable. However, I have no way to prove my assumption, given the information in the question, that each of the eigenvalues is actually distinct. I am lost on how I would prove that, if I even can. There must be a way, because I don't really see another approach to showing that dim E1(A) + dim E2(A) ≤ rank A","I am trying to solve the following question: Let A be an matrix, and suppose that the only eigenvalues of are and (a) Prove that Where and is the eigenspace corresponding to eigenvalues and respectively. I have found a way to prove this if all of the eigenvalues are distinct, because then the geometric multiplicity must be 1 for each of them, and thus the dimension there must be less than or equal to Rank(A). That's the approach I've taken to the problem so far. My assumption of the eigenvalues being distinct also allows for an easy proof that A is diagonalizable. However, I have no way to prove my assumption, given the information in the question, that each of the eigenvalues is actually distinct. I am lost on how I would prove that, if I even can. There must be a way, because I don't really see another approach to showing that dim E1(A) + dim E2(A) ≤ rank A","n \times n A 0, 1, 2. \dim E_1(A) + \dim E_2(A) \leq \operatorname{rank} A. E_1 E_2 1 2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,What are the eigenvalues of this arrowhead matrix?,What are the eigenvalues of this arrowhead matrix?,,"Suppose $p_0,\ p_1,\ \dots,\ p_q$ are positive such that $p_0+p_1+\dots+p_q=1$ . I am wondering how to find the eigenvalues of the following arrowhead matrix $$A=\begin{bmatrix} 1 & p_1 & \dots & p_q\\ p_1 & p_1 & & \\ \vdots & & \ddots & \\ p_q & & & p_q \end{bmatrix} \in \mathbb{R}^{(q+1)\times (q+1)}.$$ where the empty parts in $A$ are all zeros and $p_0$ does not show up in $A$ . Since $p_0>0$ , we have $0<p_1+\dots+p_q<1$ and $A$ is a positive definite matrix. I used the method $|\lambda I - A|$ to find the eigenvalues: $$\lambda I - A=\begin{bmatrix} \lambda-1 & -p_1 & \dots & -p_q\\ -p_1 & \lambda-p_1 & & \\ \vdots & & \ddots & \\ -p_q & & & \lambda-p_q \end{bmatrix}.$$ We can multiply the second column of $\lambda I - A$ by $p_1/(\lambda-p_1)$ which could be added to the first column. Then we can eliminate the second entry in the first column. However, this method moves $\lambda-p_1$ in the denominator. It is still hard to find those eigenvalues of $A$ .","Suppose are positive such that . I am wondering how to find the eigenvalues of the following arrowhead matrix where the empty parts in are all zeros and does not show up in . Since , we have and is a positive definite matrix. I used the method to find the eigenvalues: We can multiply the second column of by which could be added to the first column. Then we can eliminate the second entry in the first column. However, this method moves in the denominator. It is still hard to find those eigenvalues of .","p_0,\ p_1,\ \dots,\ p_q p_0+p_1+\dots+p_q=1 A=\begin{bmatrix}
1 & p_1 & \dots & p_q\\
p_1 & p_1 & & \\
\vdots & & \ddots & \\
p_q & & & p_q
\end{bmatrix} \in \mathbb{R}^{(q+1)\times (q+1)}. A p_0 A p_0>0 0<p_1+\dots+p_q<1 A |\lambda I - A| \lambda I - A=\begin{bmatrix}
\lambda-1 & -p_1 & \dots & -p_q\\
-p_1 & \lambda-p_1 & & \\
\vdots & & \ddots & \\
-p_q & & & \lambda-p_q
\end{bmatrix}. \lambda I - A p_1/(\lambda-p_1) \lambda-p_1 A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'characteristic-polynomial']"
95,Find a basis for set of matrices,Find a basis for set of matrices,,"Let $M_{2n}$ denote the set of all square matrices of size $(2n)\times (2n)$ . I managed to show that the following set is a subvector space of $M_{2n}(K)$ : $$ N_{2n}:=\left\{A\in M_{2n}(K): QA+A^\top Q=0\right\} $$ with $Q\in M_{2n}(K)$ being the block matrix given by $$ Q:=\begin{pmatrix}0 & E_n\\ -E_n & 0\end{pmatrix} $$ where $E_n$ is the $n$ -dimensional identity matrix. I am asked to find a basis for $N_{2n}$ and I do not really know how to do that. I started with the special case $n=1$ . In this case, if I let $A=\begin{pmatrix}a & b\\ c& d\end{pmatrix}$ be in $N_2$ , then I get the condition $$ a = -d $$ while $c$ and $b$ have no restrictions. So I think that a basis is given by $$ \left\{\begin{pmatrix}1 & 0\\ 0 & -1\end{pmatrix},\begin{pmatrix}0 & 1\\0 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\1 & 0\end{pmatrix}\right\} $$ In the general case, if I write $A\in N_{2n}$ in block form $A=\begin{pmatrix}B & C\\D & F\end{pmatrix}$ , I get the condition $$ B=-F $$ but I don't know if this is helpful to find a basis...","Let denote the set of all square matrices of size . I managed to show that the following set is a subvector space of : with being the block matrix given by where is the -dimensional identity matrix. I am asked to find a basis for and I do not really know how to do that. I started with the special case . In this case, if I let be in , then I get the condition while and have no restrictions. So I think that a basis is given by In the general case, if I write in block form , I get the condition but I don't know if this is helpful to find a basis...","M_{2n} (2n)\times (2n) M_{2n}(K) 
N_{2n}:=\left\{A\in M_{2n}(K): QA+A^\top Q=0\right\}
 Q\in M_{2n}(K) 
Q:=\begin{pmatrix}0 & E_n\\ -E_n & 0\end{pmatrix}
 E_n n N_{2n} n=1 A=\begin{pmatrix}a & b\\ c& d\end{pmatrix} N_2 
a = -d
 c b 
\left\{\begin{pmatrix}1 & 0\\ 0 & -1\end{pmatrix},\begin{pmatrix}0 & 1\\0 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\1 & 0\end{pmatrix}\right\}
 A\in N_{2n} A=\begin{pmatrix}B & C\\D & F\end{pmatrix} 
B=-F
","['linear-algebra', 'matrices']"
96,Proof of n-times matrix multiplication,Proof of n-times matrix multiplication,,"I want to proof the following: $$   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n=  \left[ {\begin{array}{cc}     1 & 0 \\     2^n-1 & 2^n \\   \end{array} } \right] $$ I betitle the matrix on the left with $A$ and the matrix on the right with $B$ . I am not finding the right idea. I started with stating that the resulting matrix of $   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n $ has to be a $2\times2$ matrix because of the definition of matrix multiplication. Then I wrote the following $$  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n=  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]\times  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]=  \left[ {\begin{array}{cc}     1\times 1+0\times 1 & 1\times 0 + 0\times 2 \\     1\times 1 + 2\times 1 & 1\times 0 + 2\times 2 \\   \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]=  \left[ {\begin{array}{cc}     1 & 0 \\     3 & 4 \\   \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]$$ My idea behind this was that this should show that the first line will never change, because we always just multiply with the same matrix again. I think this is quite easy to see. For the second line it's in my opinion quite easy to see that the second number ( $b_{22}$ ) is always the old second number ( $a_{22}$ ) times 2 and because the first number with wich we started ( $a_{22}$ ) is the number 2 it's just always powers of 2. And the first number ( $b_{21}$ ) is always the old first number ( $a_{21}$ ) plus the old second number ( $a_{22}$ ). And because the first number with wich we started ( $a_{21}$ ) is on less than the second number with wich we started ( $a_{22}$ ) it's always one less than the power of 2. I have problems with explaining my proof and I am not quite sure if it's correct, so can maybe someone recommend a more exact way of proofing this, because I don't know if my sentences are exact enough. Induction Try Thanks to the comment from @Jaap Sherphuis I now tried to proof the statement with induction and started with the basecase $n=1$ . $$  \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^1=\left[ {\begin{array}{cc}     1 & 0 \\     2^1-1 & 2^1 \\   \end{array} } \right]$$ This is obviously true. Now I take $   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n=  \left[ {\begin{array}{cc}     1 & 0 \\     2^n-1 & 2^n \\   \end{array} } \right] $ as given and try to show with it the statement for $n=n+1$ . $$   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^{n+1}=  \left[ {\begin{array}{cc}     1 & 0 \\     2^{n+1}-1 & 2^{n+1} \\   \end{array} } \right]\Leftrightarrow   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n \times   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]=  \left[ {\begin{array}{cc}     1 & 0 \\     2^n-1 & 2^n \\   \end{array} } \right] \times   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right] $$ Because $   \left[ {\begin{array}{cc}     1 & 0 \\     1 & 2 \\   \end{array} } \right]^n=  \left[ {\begin{array}{cc}     1 & 0 \\     2^n-1 & 2^n \\   \end{array} } \right] $ is true the whole statement is true for $n=n+1$ and thus the whole statement is true for every $n\in\mathbb{N}$ . Is this the correct approach?","I want to proof the following: I betitle the matrix on the left with and the matrix on the right with . I am not finding the right idea. I started with stating that the resulting matrix of has to be a matrix because of the definition of matrix multiplication. Then I wrote the following My idea behind this was that this should show that the first line will never change, because we always just multiply with the same matrix again. I think this is quite easy to see. For the second line it's in my opinion quite easy to see that the second number ( ) is always the old second number ( ) times 2 and because the first number with wich we started ( ) is the number 2 it's just always powers of 2. And the first number ( ) is always the old first number ( ) plus the old second number ( ). And because the first number with wich we started ( ) is on less than the second number with wich we started ( ) it's always one less than the power of 2. I have problems with explaining my proof and I am not quite sure if it's correct, so can maybe someone recommend a more exact way of proofing this, because I don't know if my sentences are exact enough. Induction Try Thanks to the comment from @Jaap Sherphuis I now tried to proof the statement with induction and started with the basecase . This is obviously true. Now I take as given and try to show with it the statement for . Because is true the whole statement is true for and thus the whole statement is true for every . Is this the correct approach?","
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
 A B 
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n
 2\times2   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1\times 1+0\times 1 & 1\times 0 + 0\times 2 \\
    1\times 1 + 2\times 1 & 1\times 0 + 2\times 2 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1 & 0 \\
    3 & 4 \\
  \end{array} } \right]\times\dots\times  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right] b_{22} a_{22} a_{22} b_{21} a_{21} a_{22} a_{21} a_{22} n=1   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^1=\left[ {\begin{array}{cc}
    1 & 0 \\
    2^1-1 & 2^1 \\
  \end{array} } \right] 
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
 n=n+1 
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^{n+1}=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^{n+1}-1 & 2^{n+1} \\
  \end{array} } \right]\Leftrightarrow
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n \times   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right] \times   \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]
 
  \left[ {\begin{array}{cc}
    1 & 0 \\
    1 & 2 \\
  \end{array} } \right]^n=  \left[ {\begin{array}{cc}
    1 & 0 \\
    2^n-1 & 2^n \\
  \end{array} } \right]
 n=n+1 n\in\mathbb{N}","['linear-algebra', 'matrices', 'solution-verification']"
97,Change of basis vector,Change of basis vector,,"I don't have a problem, I just need help with the theory. I feel like I keep misunderstanding things when it comes to solving problems that involves changing from a basis to another. Lets say we are given 3 linearly independent vectors $v_1, v_2, v_3$ . If we are to build a basis of these vectors, we would put them in a bracket right, but they are said to be a new basis that has it's coordinates with respect to the standard basis? But what if we wanted to write these new basis with its own coordinates? Also how can we write the standard basis with respect to this new basis? I'd really appreciate it if someone can help me out, or give me a YT video recommendation or another website. I would also really appreciate it if someone could give me questions to work with to understand it better. I tried googling myself but I only found what I already know, which is how to change from one base to another etc.","I don't have a problem, I just need help with the theory. I feel like I keep misunderstanding things when it comes to solving problems that involves changing from a basis to another. Lets say we are given 3 linearly independent vectors . If we are to build a basis of these vectors, we would put them in a bracket right, but they are said to be a new basis that has it's coordinates with respect to the standard basis? But what if we wanted to write these new basis with its own coordinates? Also how can we write the standard basis with respect to this new basis? I'd really appreciate it if someone can help me out, or give me a YT video recommendation or another website. I would also really appreciate it if someone could give me questions to work with to understand it better. I tried googling myself but I only found what I already know, which is how to change from one base to another etc.","v_1, v_2, v_3","['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'change-of-basis']"
98,"Prove that for any nonsingular complex matrix $A$ and for any positive integer $k$, the equation $X^k = A$ has a solution","Prove that for any nonsingular complex matrix  and for any positive integer , the equation  has a solution",A k X^k = A,"Prove that for any nonsingular complex matrix $A$ and for any positive integer $k$ , the equation $X^k = A$ has a solution. Any tips or solution?","Prove that for any nonsingular complex matrix and for any positive integer , the equation has a solution. Any tips or solution?",A k X^k = A,"['linear-algebra', 'matrices', 'proof-writing', 'matrix-equations']"
99,Curious how to retrieve original vector $x$ from an $x^Tx$ operation,Curious how to retrieve original vector  from an  operation,x x^Tx,"I am doing some analysis on a signal $x(t)$ which is a vector of length $m$ . After performing the operation $x^Tx$ of the vector on itself, I get a matrix $Y$ of size $m\times m$ . Now, how can I retrieve the original $x(t)$ ? Should this be the elements in the diagonal of $Y$ ? I did some basic plotting and while the diagonal looks similar to original signal vector, they are not equal. Looks straightforward but I am missing something, probably need to normalize it somehow. In summary, Is it always possible to retrieve the original signal without information loss? If it is possible, how is it done?","I am doing some analysis on a signal which is a vector of length . After performing the operation of the vector on itself, I get a matrix of size . Now, how can I retrieve the original ? Should this be the elements in the diagonal of ? I did some basic plotting and while the diagonal looks similar to original signal vector, they are not equal. Looks straightforward but I am missing something, probably need to normalize it somehow. In summary, Is it always possible to retrieve the original signal without information loss? If it is possible, how is it done?",x(t) m x^Tx Y m\times m x(t) Y,"['linear-algebra', 'matrices', 'signal-processing']"
