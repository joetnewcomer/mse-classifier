,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve real ode $\ddot x + x = 0$ by introducing $i$?,Solve real ode  by introducing ?,\ddot x + x = 0 i,"It is said that we can solve differential equ $\ddot x + x = 0$ by writing it as $(d/dt + i) (d/dt - i)x=0$ . Why can we do this? Certainly we cannot simply say $\frac{d^2}{dt^2} = \left(\frac{d}{dt}\right)^2$ , since the linear operators, such as $d/dt$ , are not the same as numbers. For example, the former is usually not commutative (though in this special case it is), and even possibly non-associative. $\\$ Perhaps it is because that (1) $\frac{d^2}{dt^2} = \frac{d}{dt} \frac{d}{dt}$ according to the def of 2nd-order differentiation (2) the operators $d/dt$ , though non-commutative, obey distribution law, similar to the matrix? $\\$ Anyway despite the confusion about laws that linear operators follow, if I try to expand the expression $(d/dt + i) (d/dt - i)x$ from right to left then it makes sense: $(d/dt + i) (d/dt - i)x = (d/dt + i) (dx/dt - ix)= d/dt (dx/dt - ix)  + i (dx/dt - ix) \\ = \ddot x + x$ And so we can solve the ode from left to right: From $(d/dt + i)f(t) = 0$ we get $f(t)$ , then from $(d/dt - i)x(t) = f(t)$ we get $x(t)$ . $\\$ More generally speaking, are there some rules that the addition and multiplication of (partial) differentiation operators ( $d/dt$ and $\partial/\partial x$ , etc.) and numbers would follow? it seems that (partial) differentiation operators and matrices, both of which can be regarded as linear operators, share similarities. But they are still seemingly different, e.g. $d/dt$ and $\partial/\partial x$ may not obey associative laws while matrices do. Is there an article or book discussing the similarities and dissimilarities between the two? is there any chapter or essay that discusses in general the way of solving a higher order ode by factorizing the operators?","It is said that we can solve differential equ by writing it as . Why can we do this? Certainly we cannot simply say , since the linear operators, such as , are not the same as numbers. For example, the former is usually not commutative (though in this special case it is), and even possibly non-associative. Perhaps it is because that (1) according to the def of 2nd-order differentiation (2) the operators , though non-commutative, obey distribution law, similar to the matrix? Anyway despite the confusion about laws that linear operators follow, if I try to expand the expression from right to left then it makes sense: And so we can solve the ode from left to right: From we get , then from we get . More generally speaking, are there some rules that the addition and multiplication of (partial) differentiation operators ( and , etc.) and numbers would follow? it seems that (partial) differentiation operators and matrices, both of which can be regarded as linear operators, share similarities. But they are still seemingly different, e.g. and may not obey associative laws while matrices do. Is there an article or book discussing the similarities and dissimilarities between the two? is there any chapter or essay that discusses in general the way of solving a higher order ode by factorizing the operators?","\ddot x + x = 0 (d/dt + i) (d/dt - i)x=0 \frac{d^2}{dt^2} = \left(\frac{d}{dt}\right)^2 d/dt \\ \frac{d^2}{dt^2} = \frac{d}{dt} \frac{d}{dt} d/dt \\ (d/dt + i) (d/dt - i)x (d/dt + i) (d/dt - i)x = (d/dt + i) (dx/dt - ix)= d/dt (dx/dt - ix)  + i (dx/dt - ix) \\
= \ddot x + x (d/dt + i)f(t) = 0 f(t) (d/dt - i)x(t) = f(t) x(t) \\ d/dt \partial/\partial x d/dt \partial/\partial x","['ordinary-differential-equations', 'linear-transformations']"
1,How to calculate the rotation number of this system,How to calculate the rotation number of this system,,"Consider the equation $\frac{dx}{dt}=\sin 2\pi x+c \sin 2\pi t$ with $|c|<1$ , I want to show that $\lim\limits_{t\to\infty} \frac{x(t)}{t}=0$ . For a given $x\in \mathbb R$ , there is a unique trajectory passing through $(0,x)$ . Let $A(x)$ be the value of this trajectory at $t=1$ . Since $\sin 2\pi x+c \sin 2\pi t$ is a double periodical function. $A:\mathbb R\to\mathbb R$ is a lifting of a map $S^1\to S^1$ . Therefore the limit we want to calculate is just the rotation number of $A$ . But I have no idea how to calculate this number.","Consider the equation with , I want to show that . For a given , there is a unique trajectory passing through . Let be the value of this trajectory at . Since is a double periodical function. is a lifting of a map . Therefore the limit we want to calculate is just the rotation number of . But I have no idea how to calculate this number.","\frac{dx}{dt}=\sin 2\pi x+c \sin 2\pi t |c|<1 \lim\limits_{t\to\infty} \frac{x(t)}{t}=0 x\in \mathbb R (0,x) A(x) t=1 \sin 2\pi x+c \sin 2\pi t A:\mathbb R\to\mathbb R S^1\to S^1 A","['ordinary-differential-equations', 'dynamical-systems', 'rotations']"
2,How to solve this quadratic nonhomogenius differential equation,How to solve this quadratic nonhomogenius differential equation,,"I have this differential equation $r'(t) = \sqrt{1-\left(\dfrac{a}{1+a^2t^2}\right)^2 r(t)^2}$ where $r(0)=0, r'(0)=1$ I have no clue how to solve it, my usual techniques fail (such as separation of variables, direct integration, and a lot of things that don't apply because it is nonhomogenius and nonlinear). I also tried to find some substitutions to separate the variables without success. With the rescaling $r\rightarrow r/a,t\rightarrow t/a$ as suggested by JohnBarber and a subsitution $t=\tan(u)$ (where the time domain $[0,\infty)$ maps to $[0,\pi/2)$ ) this can be simplified to $r'(u)^2+r(u)^2=\dfrac{1}{\cos(u)^4}, r(0)=0, r'(0)=1$ Any suggestions would be highly appreciated. As it was asked in the comments: the background is the following geometrical problem: A point $A$ moves along a the line $y=1/2$ with position $x(t)=a⋅t$ with constant velocity $a>0$ . A second point $B$ starting at the origin $O$ tries to ""stick to"" the (moving) line $AO$ while using its remaining velocity component of total velocity $|v|=1$ to move away from $A$ . The movement of $B$ in polar coordinates $(r,\varphi)$ is such that $\varphi(t)$ follows trivially from the problem while $r(t)$ satisfies the above equation. More detailed, one derives that $\varphi'(t)=2a/(1+(2at)^2)$ and plugs it into $|v|=1$ which leads to the above equation. The geometric view suggests that with inreasing distance $r$ there might be a point in time where $B$ cannot compensate the rotational motion of $A$ anymore which is when the term under the squareroot becomes negative.","I have this differential equation where I have no clue how to solve it, my usual techniques fail (such as separation of variables, direct integration, and a lot of things that don't apply because it is nonhomogenius and nonlinear). I also tried to find some substitutions to separate the variables without success. With the rescaling as suggested by JohnBarber and a subsitution (where the time domain maps to ) this can be simplified to Any suggestions would be highly appreciated. As it was asked in the comments: the background is the following geometrical problem: A point moves along a the line with position with constant velocity . A second point starting at the origin tries to ""stick to"" the (moving) line while using its remaining velocity component of total velocity to move away from . The movement of in polar coordinates is such that follows trivially from the problem while satisfies the above equation. More detailed, one derives that and plugs it into which leads to the above equation. The geometric view suggests that with inreasing distance there might be a point in time where cannot compensate the rotational motion of anymore which is when the term under the squareroot becomes negative.","r'(t) = \sqrt{1-\left(\dfrac{a}{1+a^2t^2}\right)^2 r(t)^2} r(0)=0, r'(0)=1 r\rightarrow r/a,t\rightarrow t/a t=\tan(u) [0,\infty) [0,\pi/2) r'(u)^2+r(u)^2=\dfrac{1}{\cos(u)^4}, r(0)=0, r'(0)=1 A y=1/2 x(t)=a⋅t a>0 B O AO |v|=1 A B (r,\varphi) \varphi(t) r(t) \varphi'(t)=2a/(1+(2at)^2) |v|=1 r B A",['ordinary-differential-equations']
3,"Solve the differential equation. $y^\prime = y^2+\frac{1}{x^4}$, $y=\frac{1}{x^2}\text{ctg}(\frac{1}{x}+c) - \frac{1}{x}$ and $y(+\infty)=0$","Solve the differential equation. ,  and",y^\prime = y^2+\frac{1}{x^4} y=\frac{1}{x^2}\text{ctg}(\frac{1}{x}+c) - \frac{1}{x} y(+\infty)=0,Solve the differential equation. $$y^\prime = y^2+\frac{1}{x^4}.$$ I am given this as a solution.Need to choose one which satisfies given condition.( $y(+\infty)=0$ ) $$y=\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x}$$ and $$y(+\infty)=0.$$ We need to find $c$ and show that it is solution for differential equation. So $$\lim_{x\to +\infty}\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x} =0 \\ \lim_{x\to+\infty}\frac{1}{x^2}\frac{\cos(\frac{1}{x}+c)}{\sin(\frac{1}{x}+c)} =0$$ From here I don't know how to find $c$ . Will be glad if you can help me.,Solve the differential equation. I am given this as a solution.Need to choose one which satisfies given condition.( ) and We need to find and show that it is solution for differential equation. So From here I don't know how to find . Will be glad if you can help me.,"y^\prime = y^2+\frac{1}{x^4}. y(+\infty)=0 y=\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x} y(+\infty)=0. c \lim_{x\to +\infty}\frac{1}{x^2}\cot(\frac{1}{x}+c) - \frac{1}{x} =0
\\
\lim_{x\to+\infty}\frac{1}{x^2}\frac{\cos(\frac{1}{x}+c)}{\sin(\frac{1}{x}+c)} =0 c","['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations', 'definite-integrals']"
4,How can I derive $~\frac{d}{dx}\left(\exp\left(\int f\left(x\right)dx\right)\right)=\exp\left(\int f\left(x\right)dx\right)\cdot f\left(x\right)~$?,How can I derive ?,~\frac{d}{dx}\left(\exp\left(\int f\left(x\right)dx\right)\right)=\exp\left(\int f\left(x\right)dx\right)\cdot f\left(x\right)~,"$$  P:=\text{function which only contains } ~x~ \text{as variable}  $$ $$  I:= \exp\left(\int P dx\right) $$ I want to derive the below equation . $$  \frac{  d  }{ dx   } \left( \exp\left(\int P dx\right)  \right) = I \cdot P = \exp\left(\int P dx\right)  \cdot P $$ By the way I assumed that $~ P ~$ can be a constant function of $~ x ~$ . For instance $~ P= 1 ~$ is allowed . The following info are the things which I know (WIP). $$  a \in\mathbb{R}  $$ $$ \frac{  d  }{ dx   } \left( \exp\left(ax\right)  \right) = a \cdot \exp\left(a x\right)  $$ Resetted info of $~ a ~$ $$  \frac{  d  }{ dx   }  \left( \int_{a }^{x } f(t) \,dt   \right)  $$ $$ = \frac{  d  }{ dx   }  \left[ F(t) \right]_{a}^{x}  $$ $$ = \frac{  d  }{  dx  } \left\{ F(x)-F(a) \right\}  $$ $$ = \frac{  d  }{ dx   } F(x) -  \underbrace{\frac{  d  }{ dx   } F(a)}_\text{ constant}  $$ $$ = f(x) $$",I want to derive the below equation . By the way I assumed that can be a constant function of . For instance is allowed . The following info are the things which I know (WIP). Resetted info of,"  P:=\text{function which only contains } ~x~ \text{as variable}     I:= \exp\left(\int P dx\right)    \frac{  d  }{ dx   } \left( \exp\left(\int P dx\right)  \right) = I \cdot P = \exp\left(\int P dx\right)  \cdot P  ~ P ~ ~ x ~ ~ P= 1 ~   a \in\mathbb{R}    \frac{  d  }{ dx   } \left( \exp\left(ax\right)  \right) = a \cdot \exp\left(a x\right)   ~ a ~   \frac{  d  }{ dx   }  \left( \int_{a }^{x } f(t) \,dt   \right)    = \frac{  d  }{ dx   }  \left[ F(t) \right]_{a}^{x}    = \frac{  d  }{  dx  } \left\{ F(x)-F(a) \right\}    = \frac{  d  }{ dx   } F(x) -  \underbrace{\frac{  d  }{ dx   } F(a)}_\text{ constant}    = f(x) ","['ordinary-differential-equations', 'derivatives', 'exponential-function']"
5,"ODE: $y''y+ax+by+c=0,y=k\pm\sqrt2\int\sqrt{a\int\ln(y)dx-(ax+c)\,\ln(y)-by+K}dx,\int\frac{dy}{\sqrt{K-(ax+c)\,\ln(y)+a\int\ln(y)dx-by}}=k\pm\sqrt2x$",ODE:,"y''y+ax+by+c=0,y=k\pm\sqrt2\int\sqrt{a\int\ln(y)dx-(ax+c)\,\ln(y)-by+K}dx,\int\frac{dy}{\sqrt{K-(ax+c)\,\ln(y)+a\int\ln(y)dx-by}}=k\pm\sqrt2x","Imagine we had a differential equation like: $$y’’-\frac xy=0$$ Now let’s standardize the signs. Note we do not need a constant for the first term because of the zero product property. We can generalize any way we want, but this way of adding constants will generalize linearly and intuitively: $$y’’+\frac {ax+c}{y}+b=0\implies y’’y+ax+by+c=0\implies y=y(a,b,c;x)\ne 0$$ where a,b,c are  any constants Notice that we can then do: $$y’’+a\frac {x+\frac ca}{y}+b=0\implies y’’y+a\left(x+\frac ca\right)+by=0$$ It can be shown that an implicit solution for $y=y(0,b,c;x)$ is the function $y(x)$ which satisfies the following functional integral equation. Be careful with the squared terms when solving here: $$y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2$$ This might remind you of an inverse gamma-type function which I did not see coming. Another nice thing is a closed form for $y(0,0,c;x)$ which uses the Inverse Error function : $$y’’y+c=0\implies y(x)=e^{\frac{c_1-2c\left(\operatorname{erf}^{-1}\left(\pm \sqrt{\frac 2\pi}\sqrt{ce^{-\frac{c_1}{c}}(x+c_2)^2}\right)\right)^2}{2c}} \implies \left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2c\ln(t))}}\right)^2=(x+c_2)^2$$ Here is what the sample solution family for $y’’y+x+y+1=0\ $ looks like: Here is what the sample solution family for $y’’y-x-y-1=0\ $ looks like: Related problems: Approach Zero Search Results This almost looks like the Airy Differential Equation , but it is not related : $$y’’-xy=0\implies y=\operatorname{Ai}(x)+i\operatorname {Bi}(x)$$ Unfortunately, we cannot use the Principle of Superposition to find a more general solution as the equation is nonlinear. @Eli showed that the following is a particular solution: $$y=-\frac{ax+c}b\ne 0\implies -\frac{ax+c}b \frac{d^2}{dx^2} \frac{-ax-c}b +ax-b\frac{ax+b}b +c=0+ax-ax-c+c=0$$ The problem is that this is not the general solution. One attempt at finding such a general solution will use our aforementioned a=0 using a Generalized Puisex Series , with machine help, at t=0, but the convergence may be a problem. I have listed out a few of many known terms: $$y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2\implies \int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t))}}=\int_1^{y(x)}\left(\frac 1{\sqrt{k - 2 c \ln(t)}}+ \frac{b t}{(k - 2 c \ln(t))^\frac32} + \frac{3 b^2 t^2}{(2 (k - 2 c \ln(t))^\frac52)} +O\left(t^3\right)\right) $$ Then one may use an inversion theorem to find the inverse and find $y(x)$ with correct convergence. My question is how to either find a closed form for $y(x)=y(a,b,c;x)$ or a general series representation for the general case without any initial values. Please do not give any implicit solutions as the goal is to find $y(x)$ . You can even use an inversion theorem like the Lagrange Inversion Theorem . Please correct me and give me feedback! Here is a general integral solution for which the main integral we need to put in terms of x. Here is a reference problem for the technique used by @Ron Gordon to solve the general solution and somehow integrate a general $y’y’’$ : Solution to Differential Equation: $y'' = \frac{c_1}{y} - \frac{c_2}{y^2} $ . $$y’’y+ax+by+c=0\mathop \implies^{y\ne0} y’’+a\frac x{y’}+b+ \frac cy=0\implies -y’’y’= a\frac {x y’}{y}+by’+ c\frac {y’}y$$ Now we integrate and use the referenced integration technique along with Logarithmic Differentiation . One also uses Integration by Parts of x and $\frac {y’}{y}$ : $$ \int -y’’y’dx= a\int \frac {x y’}{y}dx+b\int y’dx+ c\int \frac {y’}y dx\implies c_1-\frac{y’^{\,2}}2=ax\,\ln(y)-\int \ln(y) dx+c_2+by+c_3+c\,\ln(y)+c_4\implies y’^{\,2}=-2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)+c_1\implies y’=\pm\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} \implies y=c_2\pm \sqrt 2\int\sqrt{a\int \ln(y)dx -ax\,\ln(y)-by-c\,\ln(y)+c_1} dx$$ Here is another way to solve: $$\frac{y’}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} }=\pm1\implies \int \frac{\frac{dy}{dx}}{\sqrt{c_1 -2ax\,\ln(y)+2\int \ln(y)dx-2by-2c\,\ln(y)}}dx=\pm\int dx  \implies \int \frac{dy}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)}}=c_2\pm x\implies \int \frac{dy}{\sqrt{c_1 -ax\,\ln(y)+a\int \ln(y)dx-by-c\,\ln(y)}}=c_2\pm \sqrt2 x$$ This is where I got stuck. If we could integrate, by substitution in terms of x maybe, with a series expansion and use a series reversion, then it should output $y(x)$ explicitly.","Imagine we had a differential equation like: Now let’s standardize the signs. Note we do not need a constant for the first term because of the zero product property. We can generalize any way we want, but this way of adding constants will generalize linearly and intuitively: where a,b,c are  any constants Notice that we can then do: It can be shown that an implicit solution for is the function which satisfies the following functional integral equation. Be careful with the squared terms when solving here: This might remind you of an inverse gamma-type function which I did not see coming. Another nice thing is a closed form for which uses the Inverse Error function : Here is what the sample solution family for looks like: Here is what the sample solution family for looks like: Related problems: Approach Zero Search Results This almost looks like the Airy Differential Equation , but it is not related : Unfortunately, we cannot use the Principle of Superposition to find a more general solution as the equation is nonlinear. @Eli showed that the following is a particular solution: The problem is that this is not the general solution. One attempt at finding such a general solution will use our aforementioned a=0 using a Generalized Puisex Series , with machine help, at t=0, but the convergence may be a problem. I have listed out a few of many known terms: Then one may use an inversion theorem to find the inverse and find with correct convergence. My question is how to either find a closed form for or a general series representation for the general case without any initial values. Please do not give any implicit solutions as the goal is to find . You can even use an inversion theorem like the Lagrange Inversion Theorem . Please correct me and give me feedback! Here is a general integral solution for which the main integral we need to put in terms of x. Here is a reference problem for the technique used by @Ron Gordon to solve the general solution and somehow integrate a general : Solution to Differential Equation: . Now we integrate and use the referenced integration technique along with Logarithmic Differentiation . One also uses Integration by Parts of x and : Here is another way to solve: This is where I got stuck. If we could integrate, by substitution in terms of x maybe, with a series expansion and use a series reversion, then it should output explicitly.","y’’-\frac xy=0 y’’+\frac {ax+c}{y}+b=0\implies y’’y+ax+by+c=0\implies y=y(a,b,c;x)\ne 0 y’’+a\frac {x+\frac ca}{y}+b=0\implies y’’y+a\left(x+\frac ca\right)+by=0 y=y(0,b,c;x) y(x) y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2 y(0,0,c;x) y’’y+c=0\implies y(x)=e^{\frac{c_1-2c\left(\operatorname{erf}^{-1}\left(\pm \sqrt{\frac 2\pi}\sqrt{ce^{-\frac{c_1}{c}}(x+c_2)^2}\right)\right)^2}{2c}} \implies \left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2c\ln(t))}}\right)^2=(x+c_2)^2 y’’y+x+y+1=0\  y’’y-x-y-1=0\  y’’-xy=0\implies y=\operatorname{Ai}(x)+i\operatorname {Bi}(x) y=-\frac{ax+c}b\ne 0\implies -\frac{ax+c}b \frac{d^2}{dx^2} \frac{-ax-c}b +ax-b\frac{ax+b}b +c=0+ax-ax-c+c=0 y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2\implies \int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t))}}=\int_1^{y(x)}\left(\frac 1{\sqrt{k - 2 c \ln(t)}}+ \frac{b t}{(k - 2 c \ln(t))^\frac32} + \frac{3 b^2 t^2}{(2 (k - 2 c \ln(t))^\frac52)} +O\left(t^3\right)\right)  y(x) y(x)=y(a,b,c;x) y(x) y’y’’ y'' = \frac{c_1}{y} - \frac{c_2}{y^2}  y’’y+ax+by+c=0\mathop \implies^{y\ne0} y’’+a\frac x{y’}+b+ \frac cy=0\implies -y’’y’= a\frac {x y’}{y}+by’+ c\frac {y’}y \frac {y’}{y}  \int -y’’y’dx= a\int \frac {x y’}{y}dx+b\int y’dx+ c\int \frac {y’}y dx\implies c_1-\frac{y’^{\,2}}2=ax\,\ln(y)-\int \ln(y) dx+c_2+by+c_3+c\,\ln(y)+c_4\implies y’^{\,2}=-2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)+c_1\implies y’=\pm\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} \implies y=c_2\pm \sqrt 2\int\sqrt{a\int \ln(y)dx -ax\,\ln(y)-by-c\,\ln(y)+c_1} dx \frac{y’}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} }=\pm1\implies \int \frac{\frac{dy}{dx}}{\sqrt{c_1 -2ax\,\ln(y)+2\int \ln(y)dx-2by-2c\,\ln(y)}}dx=\pm\int dx  \implies \int \frac{dy}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)}}=c_2\pm x\implies \int \frac{dy}{\sqrt{c_1 -ax\,\ln(y)+a\int \ln(y)dx-by-c\,\ln(y)}}=c_2\pm \sqrt2 x y(x)","['sequences-and-series', 'ordinary-differential-equations', 'special-functions', 'error-function', 'integro-differential-equations']"
6,Omega Limit set containing only equilibria,Omega Limit set containing only equilibria,,"I am struggling to come to an answer the satisfies myself with the current problem. I have a Lipschitz vector field, restricted to an hyperbox $\mathcal{B}= [a_i,b_i] \times[a_2,b_2]\times\dots\times[a_N,b_N]$ . Such a set $\mathcal{B}$ is forward invariant, therefore for every initial condition $\xi \in \mathcal{B}$ the corresponding forward orbit is contained in a compact set. For my particular system I can show that: For each $\xi$ the corresponding omega limit set $\omega(\xi)$ is made only of equilibria, i.e. every point in $\omega(\xi)$ is an equilibrium. Is 1. enough to conclude that every solution converges to a certain, single equilibrium, even without assuming that the equilibria are isolated? If not, why not?","I am struggling to come to an answer the satisfies myself with the current problem. I have a Lipschitz vector field, restricted to an hyperbox . Such a set is forward invariant, therefore for every initial condition the corresponding forward orbit is contained in a compact set. For my particular system I can show that: For each the corresponding omega limit set is made only of equilibria, i.e. every point in is an equilibrium. Is 1. enough to conclude that every solution converges to a certain, single equilibrium, even without assuming that the equilibria are isolated? If not, why not?","\mathcal{B}= [a_i,b_i] \times[a_2,b_2]\times\dots\times[a_N,b_N] \mathcal{B} \xi \in \mathcal{B} \xi \omega(\xi) \omega(\xi)","['ordinary-differential-equations', 'dynamical-systems', 'vector-fields', 'nonlinear-dynamics']"
7,The Sequences $\{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty$ are Linearly Independent over $\mathbb{C}$,The Sequences  are Linearly Independent over,\{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty \mathbb{C},"I am studying the book ""Galois' Dream: Group Theory and Differential Equations"" by Michio Kuga. I have some trouble in proving the Lemma 18.3: Let $\alpha_1, \ldots, \alpha_n$ be complex numbers such that $\alpha_t - \alpha_s$ is not an integer for $t \neq s$ . Then for every positive integer $n$ , the following $(N+1)n$ sequences are linearly independent over $\mathbb{C}$ : $$ \{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty, $$ where $j = 1, \ldots, n$ and $k = 0, \ldots, N$ . In other words, if the equation $$ \sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^ke^{2\pi i \alpha_j m} = 0 $$ ( $m = 1,2,\ldots$ ) holds for complex numbers $C_{j,k}$ , then $C_{j,k} = 0$ for all $j = 1, \ldots, n$ and $k = 0, \ldots, N$ . Here is my process of thought: First, to have a simpler notation, let $\beta_j := e^{2\pi i \alpha_j} \neq 0 $ . Then, consider the linear dependence relation: $$ \sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^k \beta_j^m = 0, \hspace{10pt}  m = 1, 2, \ldots $$ But I couldn't figure out how should I proceed next. I tried to do induction on one of the indicies, eg. on $j$ as: For $j = 1$ the relation becomes $$ \sum_{k = 0} ^N C_{0,k}m^k \beta_0^m = 0 $$ But even here, I couldn't conclude that all $C_{0,k}$ s are zero. Since this $C$ has two indicies, I am not sure if finding all $C_{0,k}$ s zero would imply that all $C_{j,k}$ s are zero. So I am stuck. The book says this proof was easy, but I couldn't catch that ""easy"". I will appreciate any help. Thank you in advance.","I am studying the book ""Galois' Dream: Group Theory and Differential Equations"" by Michio Kuga. I have some trouble in proving the Lemma 18.3: Let be complex numbers such that is not an integer for . Then for every positive integer , the following sequences are linearly independent over : where and . In other words, if the equation ( ) holds for complex numbers , then for all and . Here is my process of thought: First, to have a simpler notation, let . Then, consider the linear dependence relation: But I couldn't figure out how should I proceed next. I tried to do induction on one of the indicies, eg. on as: For the relation becomes But even here, I couldn't conclude that all s are zero. Since this has two indicies, I am not sure if finding all s zero would imply that all s are zero. So I am stuck. The book says this proof was easy, but I couldn't catch that ""easy"". I will appreciate any help. Thank you in advance.","\alpha_1, \ldots, \alpha_n \alpha_t - \alpha_s t \neq s n (N+1)n \mathbb{C} 
\{m^ke^{2\pi i \alpha_j m}\}_{m=1}^\infty,
 j = 1, \ldots, n k = 0, \ldots, N 
\sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^ke^{2\pi i \alpha_j m} = 0
 m = 1,2,\ldots C_{j,k} C_{j,k} = 0 j = 1, \ldots, n k = 0, \ldots, N \beta_j := e^{2\pi i \alpha_j} \neq 0  
\sum_{j=1}^n\sum_{k=0}^NC_{j,k} m^k \beta_j^m = 0, \hspace{10pt}  m = 1, 2, \ldots
 j j = 1 
\sum_{k = 0} ^N C_{0,k}m^k \beta_0^m = 0
 C_{0,k} C C_{0,k} C_{j,k}","['linear-algebra', 'sequences-and-series', 'ordinary-differential-equations', 'singularity']"
8,Differential Equation Intuition - Wolfram Alpha Incorrect?,Differential Equation Intuition - Wolfram Alpha Incorrect?,,"Consider the differential equation $$f'(x)=\frac{f(x)}{2x} - \sqrt{f(x)}. $$ Per Wolfram Alpha , its solution is $$f(x)=\frac{1}{9}(4x^2-12cx^{5/4}+9c^2 x^{1/2}).$$ From the differential equation, we can see that when $x$ and $f(x)$ are large, the function has a negative slope. However, we see from the solution that when $x$ is large, so is $f(x)$ , yet the function has a positive slope. What explains this apparent contradiction in intuition? Here is an example with concrete values. Setting $c=1$ , $f(20) = 125$ . From the solution, $f$ slopes upwards at $x=20$ . Yet according to the differential equation, $f'(20) = 125/40-125^{0.5} = -8$ , which suggests $f$ slopes downwards. What explains this disconnect? Here is an interactive Desmos graph showing these functions and calculations. As you can see, the derivative of $f$ as calculated by Desmos is markedly different from the slope given by the differential equation. Is there any chance Wolfram Alpha got this wrong? Or is there some other explanation for the seeming contradiction? Any feedback or guidance would be greatly appreciated.","Consider the differential equation Per Wolfram Alpha , its solution is From the differential equation, we can see that when and are large, the function has a negative slope. However, we see from the solution that when is large, so is , yet the function has a positive slope. What explains this apparent contradiction in intuition? Here is an example with concrete values. Setting , . From the solution, slopes upwards at . Yet according to the differential equation, , which suggests slopes downwards. What explains this disconnect? Here is an interactive Desmos graph showing these functions and calculations. As you can see, the derivative of as calculated by Desmos is markedly different from the slope given by the differential equation. Is there any chance Wolfram Alpha got this wrong? Or is there some other explanation for the seeming contradiction? Any feedback or guidance would be greatly appreciated.",f'(x)=\frac{f(x)}{2x} - \sqrt{f(x)}.  f(x)=\frac{1}{9}(4x^2-12cx^{5/4}+9c^2 x^{1/2}). x f(x) x f(x) c=1 f(20) = 125 f x=20 f'(20) = 125/40-125^{0.5} = -8 f f,"['ordinary-differential-equations', 'functions', 'derivatives']"
9,Finding a general solution of pde,Finding a general solution of pde,,"$(x^2-y^2-u^2)\cdot u_x+(2xy)\cdot u_y=2xu$ how can I solve this partial dif. equation. I try to use Langarange methods This is my solution; $$\frac {dx}{x^2-y^2-u^2}=\frac {dy}{2xy}=\frac {du}{2xu}= λ$$ $$\frac {dy}{2xy}=\frac {du}{2xu}$$ $$\frac {du}{u}-\frac {dy}{y}=0$$ $$\ln u-\ln y=\ln c_1$$ $$\frac {u}{y}=c_1=w$$ then i know that we now need to ﬁnd another function. but i am stuck. $$F(w,v)=0$$ $$w=f(v)$$ can u help me to find $v$ with steps, please?  i would appreciate it if you help.","how can I solve this partial dif. equation. I try to use Langarange methods This is my solution; then i know that we now need to ﬁnd another function. but i am stuck. can u help me to find with steps, please?  i would appreciate it if you help.","(x^2-y^2-u^2)\cdot u_x+(2xy)\cdot u_y=2xu \frac {dx}{x^2-y^2-u^2}=\frac {dy}{2xy}=\frac {du}{2xu}= λ \frac {dy}{2xy}=\frac {du}{2xu} \frac {du}{u}-\frac {dy}{y}=0 \ln u-\ln y=\ln c_1 \frac {u}{y}=c_1=w F(w,v)=0 w=f(v) v","['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
10,Finding the integral curve of a vector field on cylinder,Finding the integral curve of a vector field on cylinder,,"Find the integral curve of the vector field $X = \{-y, x, 1\}$ that is given on circular cylinder. I have looked through the definition of integral curve . But I cannot find any beginner-friendly explanation of finding the curve itself. After a some research on MSE I found this post but still I don't know what to do with the cylinder. I quite new to all of these and self-learner. Some hints and explanations are greatly appreciated. So, am I to solve just $$\begin{cases}\dot x = -y \\ \dot y = x \\ \dot z = 1\end{cases}$$ ?","Find the integral curve of the vector field that is given on circular cylinder. I have looked through the definition of integral curve . But I cannot find any beginner-friendly explanation of finding the curve itself. After a some research on MSE I found this post but still I don't know what to do with the cylinder. I quite new to all of these and self-learner. Some hints and explanations are greatly appreciated. So, am I to solve just ?","X = \{-y, x, 1\} \begin{cases}\dot x = -y \\ \dot y = x \\ \dot z = 1\end{cases}","['ordinary-differential-equations', 'differential-geometry', 'vector-fields']"
11,Neural ODEs - Adjoint Sensitivity Method for multiple time points,Neural ODEs - Adjoint Sensitivity Method for multiple time points,,"I'm trying to understand the math behind the backward pass in Neural ODEs . I understand it boils down to solving an IVP backwards in time, for a state $\mathbf a(t)$ (the so called adjoint state) such that $$\mathbf a(t) = \frac{dL}{d\mathbf z(t)},$$ where $L(.)$ is some loss function. From this, it's possibe to derive $$\frac{d\mathbf a(t)}{dt} = -\mathbf a(t)\frac{\partial f(\mathbf z(t), t, \theta)}{\partial t}.$$ Assuming $L$ only depends on the hidden state at the final time point, z( $t_N$ ), the gradients w.r.t $z(t_N)$ are trivial. To get gradients w.r.t $z(t_0)$ , we must have stored $z(t_N)$ from the forward pass, and we solve the backwards IVP: $$\mathbf a(t_0) = \mathbf a(t_N) + \int_{t_N}^{t_0} \frac{d\mathbf a(t)}{dt}dt = \mathbf a(t_N) - \int_{t_N}^{t_0} \mathbf a(t)^T\frac{\partial f(\mathbf z(t), t, \theta)}{\partial \mathbf z(t)}dt. $$ To get gradients w.r.t $t$ and $\theta$ , the adjoint state is augmented with $\theta(t)$ (constant), and $t(t)$ , which have trivial derivatives $\frac{d\theta(t)}{dt} = 0$ and $\frac{dt(t)}{dt} = 1$ . We obtain (omitting some steps, which can be found in Appendix B in the paper): $$ \frac{d\mathbf a_{aug}}{dt} = -[\mathbf a(t) \ \mathbf a_\theta(t) \ \mathbf a_t(t)]\frac{\partial f_{aug}}{\partial [\mathbf z, \theta, t]}(t) = - [\mathbf a \frac{\partial f}{\partial \mathbf z} \  \mathbf a \frac{\partial f}{\partial \theta} \  \mathbf a \frac{\partial f}{\partial t}],$$ Where the first element is our adjoint state before augmentation, the second element gives us gradients w.r.t $\theta$ and the third element gives us gradients w.r.t $t$ . This is all straightforward to me, as long as $L$ only depends (directly) on $\mathbf z(t_N)$ . If instead we have a loss that depends directly on $\mathbf z(.)$ at additional intermediate time points, the authors state that one should simply repeat this procedure for each pair of consecutive time points and sum up the obtained gradients. My intuition is a bit lost as to why this is the case. I think part of it is the fact that future points can't influence past time points, but from Figure 2 in the paper it looks as though when we're going (backwards) from $\mathbf z(t_{i+1})$ to $\mathbf z(t_{i})$ and then ""jump"" there to repeat the procedure from $\mathbf z(t_{i})$ to $\mathbf z(t_{i-1})$ , we ""keep"" $\mathbf z_{i-1}$ from influencing $\mathbf z_{i+1}$ . Bluntly and to summarize: the paper states that ""If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation."", but I lack some intuition as to why that is the case.","I'm trying to understand the math behind the backward pass in Neural ODEs . I understand it boils down to solving an IVP backwards in time, for a state (the so called adjoint state) such that where is some loss function. From this, it's possibe to derive Assuming only depends on the hidden state at the final time point, z( ), the gradients w.r.t are trivial. To get gradients w.r.t , we must have stored from the forward pass, and we solve the backwards IVP: To get gradients w.r.t and , the adjoint state is augmented with (constant), and , which have trivial derivatives and . We obtain (omitting some steps, which can be found in Appendix B in the paper): Where the first element is our adjoint state before augmentation, the second element gives us gradients w.r.t and the third element gives us gradients w.r.t . This is all straightforward to me, as long as only depends (directly) on . If instead we have a loss that depends directly on at additional intermediate time points, the authors state that one should simply repeat this procedure for each pair of consecutive time points and sum up the obtained gradients. My intuition is a bit lost as to why this is the case. I think part of it is the fact that future points can't influence past time points, but from Figure 2 in the paper it looks as though when we're going (backwards) from to and then ""jump"" there to repeat the procedure from to , we ""keep"" from influencing . Bluntly and to summarize: the paper states that ""If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation."", but I lack some intuition as to why that is the case.","\mathbf a(t) \mathbf a(t) = \frac{dL}{d\mathbf z(t)}, L(.) \frac{d\mathbf a(t)}{dt} = -\mathbf a(t)\frac{\partial f(\mathbf z(t), t, \theta)}{\partial t}. L t_N z(t_N) z(t_0) z(t_N) \mathbf a(t_0) = \mathbf a(t_N) + \int_{t_N}^{t_0} \frac{d\mathbf a(t)}{dt}dt = \mathbf a(t_N) - \int_{t_N}^{t_0} \mathbf a(t)^T\frac{\partial f(\mathbf z(t), t, \theta)}{\partial \mathbf z(t)}dt.  t \theta \theta(t) t(t) \frac{d\theta(t)}{dt} = 0 \frac{dt(t)}{dt} = 1  \frac{d\mathbf a_{aug}}{dt} = -[\mathbf a(t) \ \mathbf a_\theta(t) \ \mathbf a_t(t)]\frac{\partial f_{aug}}{\partial [\mathbf z, \theta, t]}(t) = - [\mathbf a \frac{\partial f}{\partial \mathbf z} \  \mathbf a \frac{\partial f}{\partial \theta} \  \mathbf a \frac{\partial f}{\partial t}], \theta t L \mathbf z(t_N) \mathbf z(.) \mathbf z(t_{i+1}) \mathbf z(t_{i}) \mathbf z(t_{i}) \mathbf z(t_{i-1}) \mathbf z_{i-1} \mathbf z_{i+1}","['ordinary-differential-equations', 'neural-networks']"
12,ODE: The Lyapunov stability of a set,ODE: The Lyapunov stability of a set,,"When talking about the Lyapunov stability of a dynamical system, we usually take some point in the domain to test its stability. For example, for the ODE $\dot{x} = -x$ , the origin $ x = 0 $ is asymptotically stable. But, I saw a paper about the stability of a dynamical system where the stability is defined for a set rather than a point. They defined the distance $$ {\rm{dist}}(x,A) = \inf_{y \in A}{\Vert x - y \Vert} $$ and also defined that a closed set $ A $ is asymptotically stable if there exists a class $ \mathcal{KL} $ function $ \beta $ such that for the solution $ x(t) $ , $$ \forall t \ge t_{0}:~{\rm{dist}}(x(t),A) \le \beta({\rm{dist}}(x_{0},A), t - t_{0} ) $$ where $ t_{0} $ and $ x_{0} $ are the initial time and state, respectively. I think it is a quite reasonable generalization of stability. I wonder if there are good references to such a definition. Is this a common definition of stability of a dynamical system? If so, I want some representative references (textbook or paper) introducing it.","When talking about the Lyapunov stability of a dynamical system, we usually take some point in the domain to test its stability. For example, for the ODE , the origin is asymptotically stable. But, I saw a paper about the stability of a dynamical system where the stability is defined for a set rather than a point. They defined the distance and also defined that a closed set is asymptotically stable if there exists a class function such that for the solution , where and are the initial time and state, respectively. I think it is a quite reasonable generalization of stability. I wonder if there are good references to such a definition. Is this a common definition of stability of a dynamical system? If so, I want some representative references (textbook or paper) introducing it.","\dot{x} = -x  x = 0  
{\rm{dist}}(x,A) = \inf_{y \in A}{\Vert x - y \Vert}
  A   \mathcal{KL}   \beta   x(t)  
\forall t \ge t_{0}:~{\rm{dist}}(x(t),A) \le \beta({\rm{dist}}(x_{0},A), t - t_{0} )
  t_{0}   x_{0} ","['ordinary-differential-equations', 'reference-request', 'stability-theory']"
13,How to prove that the biggest solution of $\dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon$ is $x(t)=(ct)^{1/(1-\varepsilon)}$?,How to prove that the biggest solution of  is ?,\dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon x(t)=(ct)^{1/(1-\varepsilon)},"Let $x:[0,\infty[\to[0,\infty[$ be a differentiable function satisfying $$\dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon$$ for all $t\in[0,\infty[$ , a fixed $c\ge 0$ and a fixed $\varepsilon\in[0,1[$ . Furthermore, assume that $x(0)=0$ . I want to prove that $x(t)\le (ct)^{1/(1-\varepsilon)}$ for all $t\ge 0$ . Note: Morally speaking, there are only the solutions $x=0$ and $x(t)=(ct)^{1/(1-\varepsilon)}$ . The latter solution is obtained by separating variables, i.e. by writing $$\dot x(t)x(t)^{-\varepsilon}=\frac{c}{1-\varepsilon}$$ and then by integrating. However, that approach requires $x(t)\neq 0$ , which I can't assume in my case. Note 2: The usual uniqueness result for ODEs does not hold in this case since $x\mapsto x^\varepsilon$ is not Lipschitz-continuous around $x=0$ .","Let be a differentiable function satisfying for all , a fixed and a fixed . Furthermore, assume that . I want to prove that for all . Note: Morally speaking, there are only the solutions and . The latter solution is obtained by separating variables, i.e. by writing and then by integrating. However, that approach requires , which I can't assume in my case. Note 2: The usual uniqueness result for ODEs does not hold in this case since is not Lipschitz-continuous around .","x:[0,\infty[\to[0,\infty[ \dot x(t)=\frac{c}{1-\varepsilon} x(t)^\varepsilon t\in[0,\infty[ c\ge 0 \varepsilon\in[0,1[ x(0)=0 x(t)\le (ct)^{1/(1-\varepsilon)} t\ge 0 x=0 x(t)=(ct)^{1/(1-\varepsilon)} \dot x(t)x(t)^{-\varepsilon}=\frac{c}{1-\varepsilon} x(t)\neq 0 x\mapsto x^\varepsilon x=0","['ordinary-differential-equations', 'initial-value-problems']"
14,Second order linear non-homogenous ODE - checking my solution,Second order linear non-homogenous ODE - checking my solution,,"$$xy'' + 2y' - xy = e^x$$ is the equation. Here's what I did: Divide by x First, solve the homogenous equation. $$y'' + \frac{2}{x}y' - y =0$$ Perform a substitution $y=u(x) \cdot z(x)$ , where $u(x) = e^{-\frac{1}{2}\int{p(x)dx}}$ In this case, $p(x) = \frac{2}{x}$ , and $q(x) = -1$ Find $u', u''$ After plugging in the necessary values, we get an equation: $$z'' - z = 0$$ , the solution of which is $$C_1e^x + C_2e^{-x}$$ We plug this in $y=u\cdot z$ and get: $$y=C_1 \frac{e^x}{x} + C_2 \frac{e^{-x}}{x}$$ I am fairly sure I made no mistakes up until this point. Next, I tried to solve the equation by variation of constants, where I get the Wronskian $-\frac{2}{x^2}$ . Thus, $C_1' = \frac{1}{2}$ , so $C_1 = \frac{x}{2} + C_3$ And $C_2' = -\frac{e^{2x}}{2}$ , so $C_2 = -\frac{e^{2x}}{4} + C_4$ When I plug in these values into $y$ , I get $$y=\frac{e^x}{2} + C_3 \frac{e^x}{x} - \frac{e^x}{4x} + C_4\frac{e^{-x}}{x}$$ Now, when I check the solution on WolframAlpha, the third term in my solution is extra. Can anyone spot my mistake because I am unable to do so? I have been trying for at least several hours. NOTE : It is imperative for this exercise to be done with variation of constants.","is the equation. Here's what I did: Divide by x First, solve the homogenous equation. Perform a substitution , where In this case, , and Find After plugging in the necessary values, we get an equation: , the solution of which is We plug this in and get: I am fairly sure I made no mistakes up until this point. Next, I tried to solve the equation by variation of constants, where I get the Wronskian . Thus, , so And , so When I plug in these values into , I get Now, when I check the solution on WolframAlpha, the third term in my solution is extra. Can anyone spot my mistake because I am unable to do so? I have been trying for at least several hours. NOTE : It is imperative for this exercise to be done with variation of constants.","xy'' + 2y' - xy = e^x y'' + \frac{2}{x}y' - y =0 y=u(x) \cdot z(x) u(x) = e^{-\frac{1}{2}\int{p(x)dx}} p(x) = \frac{2}{x} q(x) = -1 u', u'' z'' - z = 0 C_1e^x + C_2e^{-x} y=u\cdot z y=C_1 \frac{e^x}{x} + C_2 \frac{e^{-x}}{x} -\frac{2}{x^2} C_1' = \frac{1}{2} C_1 = \frac{x}{2} + C_3 C_2' = -\frac{e^{2x}}{2} C_2 = -\frac{e^{2x}}{4} + C_4 y y=\frac{e^x}{2} + C_3 \frac{e^x}{x} - \frac{e^x}{4x} + C_4\frac{e^{-x}}{x}",['ordinary-differential-equations']
15,Second-order linear homogenous ODE - I can't see any substitution!,Second-order linear homogenous ODE - I can't see any substitution!,,"$$y'' - y' + e^{2x}y=0$$ At first it looks like it's an easy ODE, but I haven't been able to solve it. For example, if I let $y=u(x)\cdot z, z(x)$ and accordingly choose $u$ so that the coefficient of $z'$ (after performing the necessary derivations and grouping all terms by the order of derivation of $z$ ) is zero, I get the following equation: $$z'' + (e^{2x} - \frac{1}{2})z = 0$$ which isn't any easier to solve than the first one. I'm having trouble with this equation because I don't think there's an apparent substitution to be done. Can anyone help? Thanks!","At first it looks like it's an easy ODE, but I haven't been able to solve it. For example, if I let and accordingly choose so that the coefficient of (after performing the necessary derivations and grouping all terms by the order of derivation of ) is zero, I get the following equation: which isn't any easier to solve than the first one. I'm having trouble with this equation because I don't think there's an apparent substitution to be done. Can anyone help? Thanks!","y'' - y' + e^{2x}y=0 y=u(x)\cdot z, z(x) u z' z z'' + (e^{2x} - \frac{1}{2})z = 0",['ordinary-differential-equations']
16,"Integrating $\int \sqrt{2me-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}} \,dr$",Integrating,"\int \sqrt{2me-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}} \,dr","I was trying to find Hamiltons principle function, $S$ ,  for the Hamiltonian: $$ H = \frac{1}{2} m \left( P_{r}^2 + \frac{P_{\theta}^2}{r^2} \right) + \frac{1}{2}kr^2 + \frac{1}{4} b r^4$$ After considering the form $S = f(r, \theta) - Et = R(r) + \Theta(\theta) - Et$ and identifying the canonical momenta with the partial derivatives of $S$ I got to: $$ \left(\frac{dR}{dr} \right)^2 r^2 + mkr^4 + \frac{1}{2}mbr^6 - 2mEr^2 = -\left(\frac{d \Theta}{d \theta} \right)^2 $$ Each side must be a constant, $-a^2$ , and so: $$ \frac{dR}{dr} =  \sqrt{2mE-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}} $$ which leads to: $$ R(r)=  \int \sqrt{2mE-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}} \,dr$$ I've tried to find a substitution of variables to make it simpler but to no luck. How can I solve this?","I was trying to find Hamiltons principle function, ,  for the Hamiltonian: After considering the form and identifying the canonical momenta with the partial derivatives of I got to: Each side must be a constant, , and so: which leads to: I've tried to find a substitution of variables to make it simpler but to no luck. How can I solve this?","S  H = \frac{1}{2} m \left( P_{r}^2 + \frac{P_{\theta}^2}{r^2} \right) + \frac{1}{2}kr^2 + \frac{1}{4} b r^4 S = f(r, \theta) - Et = R(r) + \Theta(\theta) - Et S  \left(\frac{dR}{dr} \right)^2 r^2 + mkr^4 + \frac{1}{2}mbr^6 - 2mEr^2 = -\left(\frac{d \Theta}{d \theta} \right)^2  -a^2  \frac{dR}{dr} =  \sqrt{2mE-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}}   R(r)=  \int \sqrt{2mE-mkr^2-\frac{1}{2}m br^4 - \frac{a^2}{r^2}} \,dr","['integration', 'ordinary-differential-equations', 'derivatives', 'substitution', 'hamilton-equations']"
17,How to evaluate the definite integral of a derivative?,How to evaluate the definite integral of a derivative?,,"I am looking through some old material on first order differential equations and realized I didn't really understand how the bounds on the integral are found. I found this question from a couple years ago which gives an answer but doesn't really go into much detail. I'm currently just thinking about first order linear diff eq's of the form: $$\frac{dy}{dx}+P(x)y=f(x)$$ Finding the integrating factor is pretty straightforward: $$\mu(x)=e^{\int{P(x)dx}}$$ Which is multiplied to both sides: $$\mu(x)\left[\frac{dy}{dx}+P(x)y\right]=\mu(x)f(x)$$ And this simplifies to: $$\frac{d}{dx}(\mu(x)y)=\mu(x)f(x)$$ And this is where the trouble starts. This equation should be integrated on both sides over the same bounds to maintain equality: $$\int_a^b{\frac{d}{dx}(\mu(x)y)dx}=\int_a^b{\mu(x)f(x)dx}$$ The RHS is just a straghtforward definite integral. The LHS however is a little more complicated. The question I linked above has a couple responses suggesting an integration by substitution so doing something like $u=\mu(x)y$ and I guess turning the integral into $\int_{u(a)}^{u(b)}{u}du$ but I'm not sure if that is right. This question was inspired partially by a practical issue I'm trying to solve involving an op-amp integrator and following the u substitution route leads to some things that don't make sense like the initial value of the output growing to infinity which shouldn't happen since I'm modeling a circuit that already was made and I know works as intended in real life. The LHS in my model looks like this: $$\int_0^T{\frac{d}{dt}\left[e^{-\frac{t}{R_2C}}V_{out}(t)\right]dt}$$ And applying $$u=e^{-\frac{t}{R_2C}}V_{out}(t)$$ and the bounds: $$a=u(0)=V_{out}(0)\quad\text{and}\quad b=u(t)=e^{-\frac{t}{R_2C}}V_{out}(t)$$ gives: $$\int_{u(0)}^{u(t)}{du}=u(t)-u(0)=e^{-\frac{t}{R_2C}}V_{out}(t) - V_{out}(0)$$ This is problematic because when $V_{out}(0)$ is added over to the RHS and the exponential term is multiplied across it will then have a positive exponent and grow this initial condition to infinity. I don't believe I made a mistake up to this point but regardless of whether my derivation in this particular problem is correct or not the question still remains as to what the proper way to deal with the limits on integrals of this type is, both determining what appropriate limits are and how to evaluate them. Any help here is appreciated. Edit: I realized I did make a mistake and lost a negative sign earlier causing that weird behavior. Corrected the model now has a decaying initial value which is in line with what I've seen from the actual circuit. I've left this up however because I am still interested in whether there is a more formal way of doing this integration since I'm aware that a lot of the methods taught in differential equations play fast and loose. If there is any nuances that I'm missing in this post or any corrections to anything I said I would still like to hear. Thanks.","I am looking through some old material on first order differential equations and realized I didn't really understand how the bounds on the integral are found. I found this question from a couple years ago which gives an answer but doesn't really go into much detail. I'm currently just thinking about first order linear diff eq's of the form: Finding the integrating factor is pretty straightforward: Which is multiplied to both sides: And this simplifies to: And this is where the trouble starts. This equation should be integrated on both sides over the same bounds to maintain equality: The RHS is just a straghtforward definite integral. The LHS however is a little more complicated. The question I linked above has a couple responses suggesting an integration by substitution so doing something like and I guess turning the integral into but I'm not sure if that is right. This question was inspired partially by a practical issue I'm trying to solve involving an op-amp integrator and following the u substitution route leads to some things that don't make sense like the initial value of the output growing to infinity which shouldn't happen since I'm modeling a circuit that already was made and I know works as intended in real life. The LHS in my model looks like this: And applying and the bounds: gives: This is problematic because when is added over to the RHS and the exponential term is multiplied across it will then have a positive exponent and grow this initial condition to infinity. I don't believe I made a mistake up to this point but regardless of whether my derivation in this particular problem is correct or not the question still remains as to what the proper way to deal with the limits on integrals of this type is, both determining what appropriate limits are and how to evaluate them. Any help here is appreciated. Edit: I realized I did make a mistake and lost a negative sign earlier causing that weird behavior. Corrected the model now has a decaying initial value which is in line with what I've seen from the actual circuit. I've left this up however because I am still interested in whether there is a more formal way of doing this integration since I'm aware that a lot of the methods taught in differential equations play fast and loose. If there is any nuances that I'm missing in this post or any corrections to anything I said I would still like to hear. Thanks.",\frac{dy}{dx}+P(x)y=f(x) \mu(x)=e^{\int{P(x)dx}} \mu(x)\left[\frac{dy}{dx}+P(x)y\right]=\mu(x)f(x) \frac{d}{dx}(\mu(x)y)=\mu(x)f(x) \int_a^b{\frac{d}{dx}(\mu(x)y)dx}=\int_a^b{\mu(x)f(x)dx} u=\mu(x)y \int_{u(a)}^{u(b)}{u}du \int_0^T{\frac{d}{dt}\left[e^{-\frac{t}{R_2C}}V_{out}(t)\right]dt} u=e^{-\frac{t}{R_2C}}V_{out}(t) a=u(0)=V_{out}(0)\quad\text{and}\quad b=u(t)=e^{-\frac{t}{R_2C}}V_{out}(t) \int_{u(0)}^{u(t)}{du}=u(t)-u(0)=e^{-\frac{t}{R_2C}}V_{out}(t) - V_{out}(0) V_{out}(0),"['ordinary-differential-equations', 'definite-integrals']"
18,Stability of Explicit midpoint method,Stability of Explicit midpoint method,,"I am trying to determine the stability region of the well known explicit midpoint method $$y_{i+1} = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right)$$ and after following the links Determine a stability region? and Calculating stability and order of implicit midpoint scheme , I managed to apply the numerical method on the test equation and got $$\begin{align*}y_{i+1} & = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right) \\ & =   y_i + \left( \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i \\  & = \left( 1 + \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i\end{align*}$$ Stability: $\big| 1 + \lambda h +\frac{\lambda^2h^2}{2}\big|<1$ and simplifying, $-2 < (1+ \lambda h)^2<0$ is the stability. I don't know if the stability region is correct. How do I find it?","I am trying to determine the stability region of the well known explicit midpoint method and after following the links Determine a stability region? and Calculating stability and order of implicit midpoint scheme , I managed to apply the numerical method on the test equation and got Stability: and simplifying, is the stability. I don't know if the stability region is correct. How do I find it?","y_{i+1} = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right) \begin{align*}y_{i+1} & = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right) \\ & = 
 y_i + \left( \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i \\ 
& = \left( 1 + \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i\end{align*} \big| 1 + \lambda h +\frac{\lambda^2h^2}{2}\big|<1 -2 < (1+ \lambda h)^2<0","['ordinary-differential-equations', 'numerical-methods', 'stability-in-odes', 'stability-theory', 'runge-kutta-methods']"
19,Heat Equation With Boundary Value Problem,Heat Equation With Boundary Value Problem,,"I have a heat equation $$ \frac{\partial u}{\partial t} = a^2 \frac {\partial^2 u}{\partial x^2} + b \frac{\partial u}{\partial x}  \ $$ with boundary conditions $$ u(0,x) = 300 \qquad u(t,0) = 100 \qquad  u_{x}(t, l) = 300e^{-t} $$ I understand perfectly how to solve this equation with zero boundary conditions, but I can't figure out what to do with the boundary conditions. Can u help me please?","I have a heat equation with boundary conditions I understand perfectly how to solve this equation with zero boundary conditions, but I can't figure out what to do with the boundary conditions. Can u help me please?","
\frac{\partial u}{\partial t} = a^2 \frac {\partial^2 u}{\partial x^2} + b \frac{\partial u}{\partial x}  \
 
u(0,x) = 300 \qquad u(t,0) = 100 \qquad  u_{x}(t, l) = 300e^{-t}
","['ordinary-differential-equations', 'boundary-value-problem', 'heat-equation']"
20,Solving an integral equation by converting it to a differential equation [closed],Solving an integral equation by converting it to a differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have to solve an integral equation given as $$ x(y)=\sin(y)+\varepsilon \int_0^\infty e^{-s} x(y+s) \mathrm{d}s$$ I know that I have to differentiate it, but I am doing hard with it. Can anyone please help me?!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have to solve an integral equation given as I know that I have to differentiate it, but I am doing hard with it. Can anyone please help me?!", x(y)=\sin(y)+\varepsilon \int_0^\infty e^{-s} x(y+s) \mathrm{d}s,"['ordinary-differential-equations', 'integral-equations']"
21,Show that no values of $b$ can make the equation $b\sin(bx)-2\cos(bx) = 0$ true.,Show that no values of  can make the equation  true.,b b\sin(bx)-2\cos(bx) = 0,"I am trying to prove that $𝑓(𝑥)=\cos(𝑏𝑥)$ is a solution to the DE $𝑦'''+2𝑦''+𝑦'+2𝑦=0$ , and by substituting $f$ and its derivatives, I have simplified the equation to $$(𝑏^2−1)(𝑏\sin(𝑏𝑥)−2\cos(𝑏𝑥))=0.$$ It is easy to show $𝑏^2−1=0$ has appropriate values for $b$ , but I am not so sure about $b\sin(bx)−2\cos(bx)=0$ . If I have the equation $b\sin(bx)-2\cos(bx) = 0$ , how do I prove that no values of $b \in \mathbb{R}$ , can make the equation true? It it enough to say that $2\cot(bx) \neq b$ for all $x \in \mathbb{R}$ , hence $b$ does not exists?","I am trying to prove that is a solution to the DE , and by substituting and its derivatives, I have simplified the equation to It is easy to show has appropriate values for , but I am not so sure about . If I have the equation , how do I prove that no values of , can make the equation true? It it enough to say that for all , hence does not exists?",𝑓(𝑥)=\cos(𝑏𝑥) 𝑦'''+2𝑦''+𝑦'+2𝑦=0 f (𝑏^2−1)(𝑏\sin(𝑏𝑥)−2\cos(𝑏𝑥))=0. 𝑏^2−1=0 b b\sin(bx)−2\cos(bx)=0 b\sin(bx)-2\cos(bx) = 0 b \in \mathbb{R} 2\cot(bx) \neq b x \in \mathbb{R} b,['ordinary-differential-equations']
22,Is the difference between stable nonlinear systems Lyapunov stable?,Is the difference between stable nonlinear systems Lyapunov stable?,,"If there are two nonlinear systems with stable equilibria $x_1 = x_2 = 0$ $$\dot x_1 = f(x_1, u) \qquad \dot x_2 = g(x_2, u)$$ with identical inputs $u$ , is the difference between the system states $x_1, x_2 \in \mathbb{R}^n$ $$e = x_1 - x_2$$ stable? Trivial case If the systems are LTI with identical system and input matrices $A$ and $B$ $$f(x_1 = x, u) = g(x_2 = x, u) = Ax + Bu,$$ the system dynamics is $$\dot e = Ae.$$ A Lyapunov function $$V(e) = e^T P e$$ results in $$\dot V(e) = \dot e P e^T + e^T P \dot e = e^T (A^T P + P A) e.$$ If $A$ is Hurwitz, then $A^T P + P A = -Q$ with a positive definite real symmetric $P$ and a positive definite $Q$ . As $\dot V(e) = -e^TQe < 0\ \forall x \neq 0$ , the difference between the two asymptotically stable LTI systems is asymptotically stable. My Question Can this be generalized to arbitrary nonlinear globally/locally/asymptotically/... stable systems, where $f = g$ ? (1) where $f \neq g$ ? (2) Thought experiment : An equilibrium is stable if for each $\epsilon > 0$ , there is a $\delta$ such that with $|| x || < \delta$ at $t = t_0$ the state remains within $|| x || < \epsilon$ . Wouldn't that imply that the vector $e$ connecting two $x_1, x_2$ remains within a hypersphere $\mathcal{B}$ that includes both $|| x_1 || < \epsilon_1$ and $|| x_2 || < \epsilon_2$ ? Then for $|| e || < \min(\delta_1, \delta_2)$ at $t = t_0$ , it would hold $|| e || < 2r_\mathcal{B}$ with $r_\mathcal{B}$ radius of $\mathcal{B}$ ? At the same time : Starting from $$V(e) = \frac{1}{2} e^T e,$$ I quickly get stuck at $$\dot V(e) = e^T \dot e = e^T \left(f\left(x_1, u\right) - g\left(x_2, u\right)\right) =\ ...?$$ Update Arastas answer has a counterexample for $f \neq g$ (loosely quoting): $\dot x_1 = u$ and $\dot x_2 = -u$ leads to $\dot e = 2u$ . With $u=1$ , we get $e(t) \rightarrow \infty$ for $t \rightarrow \infty$ . In this SISO example, the derivatives of $f$ and $g$ w.r.t. $u$ differ: $$\frac{\partial}{\partial u} f(x_1, u) = 1 \neq -1 = \frac{\partial}{\partial u} g(x_2, u).$$ Speaking more generally, the (transposed) gradient matrices of $f$ and $g$ w.r.t. $u$ differ: $$(\nabla_u f)^T = \frac{\partial}{\partial u} f(x_1, u) \neq \frac{\partial}{\partial u} g(x_2, u) = (\nabla_u g)^T.$$ What can we say about the stability of $e$ for arbitrary $f, g$ , but allowing for restrictions on the gradients $\nabla_u f$ , $\nabla_u g$ ? (3) If the restriction were made $\nabla_u f = \nabla_u g$ , (3) would capture case (1).","If there are two nonlinear systems with stable equilibria with identical inputs , is the difference between the system states stable? Trivial case If the systems are LTI with identical system and input matrices and the system dynamics is A Lyapunov function results in If is Hurwitz, then with a positive definite real symmetric and a positive definite . As , the difference between the two asymptotically stable LTI systems is asymptotically stable. My Question Can this be generalized to arbitrary nonlinear globally/locally/asymptotically/... stable systems, where ? (1) where ? (2) Thought experiment : An equilibrium is stable if for each , there is a such that with at the state remains within . Wouldn't that imply that the vector connecting two remains within a hypersphere that includes both and ? Then for at , it would hold with radius of ? At the same time : Starting from I quickly get stuck at Update Arastas answer has a counterexample for (loosely quoting): and leads to . With , we get for . In this SISO example, the derivatives of and w.r.t. differ: Speaking more generally, the (transposed) gradient matrices of and w.r.t. differ: What can we say about the stability of for arbitrary , but allowing for restrictions on the gradients , ? (3) If the restriction were made , (3) would capture case (1).","x_1 = x_2 = 0 \dot x_1 = f(x_1, u) \qquad \dot x_2 = g(x_2, u) u x_1, x_2 \in \mathbb{R}^n e = x_1 - x_2 A B f(x_1 = x, u) = g(x_2 = x, u) = Ax + Bu, \dot e = Ae. V(e) = e^T P e \dot V(e) = \dot e P e^T + e^T P \dot e = e^T (A^T P + P A) e. A A^T P + P A = -Q P Q \dot V(e) = -e^TQe < 0\ \forall x \neq 0 f = g f \neq g \epsilon > 0 \delta || x || < \delta t = t_0 || x || < \epsilon e x_1, x_2 \mathcal{B} || x_1 || < \epsilon_1 || x_2 || < \epsilon_2 || e || < \min(\delta_1, \delta_2) t = t_0 || e || < 2r_\mathcal{B} r_\mathcal{B} \mathcal{B} V(e) = \frac{1}{2} e^T e, \dot V(e) = e^T \dot e = e^T \left(f\left(x_1, u\right) - g\left(x_2, u\right)\right) =\ ...? f \neq g \dot x_1 = u \dot x_2 = -u \dot e = 2u u=1 e(t) \rightarrow \infty t \rightarrow \infty f g u \frac{\partial}{\partial u} f(x_1, u) = 1 \neq -1 = \frac{\partial}{\partial u} g(x_2, u). f g u (\nabla_u f)^T = \frac{\partial}{\partial u} f(x_1, u) \neq \frac{\partial}{\partial u} g(x_2, u) = (\nabla_u g)^T. e f, g \nabla_u f \nabla_u g \nabla_u f = \nabla_u g","['ordinary-differential-equations', 'control-theory', 'nonlinear-system', 'stability-in-odes', 'lyapunov-functions']"
23,Asymptotic matching of a first order ODE,Asymptotic matching of a first order ODE,,"Consider the following ODE: \begin{equation} \frac{\mathrm{d}y(t)}{\mathrm{d}t} = \varepsilon y(1-y)+y^2-\varepsilon \frac{y^2}{1-y} \quad \quad \text{for} \quad \quad t>0, \tag{1} \end{equation} where $y(0)=c$ and $\varepsilon\ll1.$ I would like to find an asymptotic solution that is valid for all $t>0.$ I have attached a figure of the numerical solution for $\varepsilon = 0.01$ and $c=0.5.$ For the outer solution (the solution that satisfies the initial conditions), I take $y(t) \sim y_0(t) +\varepsilon y_1(t)$ and substitute this into equation (1), which at leading order, gives \begin{equation} \frac{\mathrm{d}y_0(t)}{\mathrm{d}t} = y_0^2, \quad \quad \text{where} \quad \quad y_0(0)=c. \tag{2} \end{equation} Solving yields \begin{equation} y_0(t) = \frac{c}{1-ct}. \tag{3} \end{equation} Im relatively comfortable that this is correct outer solution. Now, I need to find the inner solution which matches with this outer solution. Taking $y_{\infty}$ to be the steady state solution to (1), I make the variable transform \begin{equation} \tau = \frac{t-t_0}{\varepsilon^{\alpha}} \quad \quad \text{which provides} \quad \quad \frac{\mathrm{d}}{\mathrm{d}t}= \varepsilon^{-\alpha}\frac{\mathrm{d}}{\mathrm{d}\tau},\tag{4} \end{equation} for $\alpha>0.$ Here $t_0$ is the overlap between the outer and inner solutions; this can be estimated by seeing when $y_0(t)$ first meets $y_{\infty}$ , meaning we just solve $y_0(t_0)=y_{\infty}.$ Doing so yields $t_0 = (y_{\infty}-c)/y_{\infty}c.$ For the inner region, the numerical simulations suggest that we take $Y(\tau) \sim y_{\infty}-\varepsilon Y_1(\tau). $ Substituting this into (1) yields \begin{equation} -\varepsilon^{1-\alpha}\frac{\mathrm{d}Y_1}{\mathrm{d}\tau} = -2\varepsilon y_{\infty} Y_1+\mathcal{O}(\varepsilon^2). \tag{5} \end{equation} This is where I'm stuck. There is no possible balance except from taking $\alpha = 0$ , which is not allowed. Furthermore, say if $\alpha=0$ , equation (5) solves to give $Y_1 = k \exp (2y_{\infty}\tau),$ which is clearly secular. Am I using the wrong variable transform? I'm clearly doing something wrong, since $Y_1\rightarrow \infty$ as $\tau \rightarrow \infty$ . Thanks for any help in advanved.","Consider the following ODE: where and I would like to find an asymptotic solution that is valid for all I have attached a figure of the numerical solution for and For the outer solution (the solution that satisfies the initial conditions), I take and substitute this into equation (1), which at leading order, gives Solving yields Im relatively comfortable that this is correct outer solution. Now, I need to find the inner solution which matches with this outer solution. Taking to be the steady state solution to (1), I make the variable transform for Here is the overlap between the outer and inner solutions; this can be estimated by seeing when first meets , meaning we just solve Doing so yields For the inner region, the numerical simulations suggest that we take Substituting this into (1) yields This is where I'm stuck. There is no possible balance except from taking , which is not allowed. Furthermore, say if , equation (5) solves to give which is clearly secular. Am I using the wrong variable transform? I'm clearly doing something wrong, since as . Thanks for any help in advanved.","\begin{equation}
\frac{\mathrm{d}y(t)}{\mathrm{d}t} = \varepsilon y(1-y)+y^2-\varepsilon \frac{y^2}{1-y} \quad \quad \text{for} \quad \quad t>0, \tag{1}
\end{equation} y(0)=c \varepsilon\ll1. t>0. \varepsilon = 0.01 c=0.5. y(t) \sim y_0(t) +\varepsilon y_1(t) \begin{equation}
\frac{\mathrm{d}y_0(t)}{\mathrm{d}t} = y_0^2, \quad \quad \text{where} \quad \quad y_0(0)=c. \tag{2}
\end{equation} \begin{equation}
y_0(t) = \frac{c}{1-ct}. \tag{3}
\end{equation} y_{\infty} \begin{equation}
\tau = \frac{t-t_0}{\varepsilon^{\alpha}} \quad \quad \text{which provides} \quad \quad \frac{\mathrm{d}}{\mathrm{d}t}= \varepsilon^{-\alpha}\frac{\mathrm{d}}{\mathrm{d}\tau},\tag{4}
\end{equation} \alpha>0. t_0 y_0(t) y_{\infty} y_0(t_0)=y_{\infty}. t_0 = (y_{\infty}-c)/y_{\infty}c. Y(\tau) \sim y_{\infty}-\varepsilon Y_1(\tau).  \begin{equation}
-\varepsilon^{1-\alpha}\frac{\mathrm{d}Y_1}{\mathrm{d}\tau} = -2\varepsilon y_{\infty} Y_1+\mathcal{O}(\varepsilon^2). \tag{5}
\end{equation} \alpha = 0 \alpha=0 Y_1 = k \exp (2y_{\infty}\tau), Y_1\rightarrow \infty \tau \rightarrow \infty","['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
24,Modelling with Higher-order DEs,Modelling with Higher-order DEs,,"I would like to know if there are any models that make use of higher-order (5th, 6th and so on...) linear differential equations. The highest that I know is the Euler-Bernoulli beam equation. Do you know any? Or how to model with higher-order ones? Thanks","I would like to know if there are any models that make use of higher-order (5th, 6th and so on...) linear differential equations. The highest that I know is the Euler-Bernoulli beam equation. Do you know any? Or how to model with higher-order ones? Thanks",,"['ordinary-differential-equations', 'mathematical-modeling']"
25,Using Calculus to Solve for a Differential Equation,Using Calculus to Solve for a Differential Equation,,"The question I have is: $x''(t)+yx'(t)+z^2x(t)=0$ , where y and z $\in \mathbb{R}$ without $0$ . $t \geq 0$ by the way. The total energy of the system is $$\frac{1}{2}(x'(t))^2+ \frac{1}{2}z^2x(t)^2$$ The question is, for t > 0, under what conditions is the total energy of the system always increasing ? Since the only things that can change are y and z, it certainly has to do with these constants. I'm just not sure what it is, though.","The question I have is: , where y and z without . by the way. The total energy of the system is The question is, for t > 0, under what conditions is the total energy of the system always increasing ? Since the only things that can change are y and z, it certainly has to do with these constants. I'm just not sure what it is, though.",x''(t)+yx'(t)+z^2x(t)=0 \in \mathbb{R} 0 t \geq 0 \frac{1}{2}(x'(t))^2+ \frac{1}{2}z^2x(t)^2,"['calculus', 'integration', 'ordinary-differential-equations', 'partial-differential-equations']"
26,What do you take as the Particular Integral when is cancels out [closed],What do you take as the Particular Integral when is cancels out [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm trying to solve the second order ODE $\ddot{x} + \omega^2x = A\cos(\omega t)$ and when I try to solve the PI it cancels out to be just $0 = A\cos(\omega t)$ .  What do I take for the PI?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm trying to solve the second order ODE and when I try to solve the PI it cancels out to be just .  What do I take for the PI?",\ddot{x} + \omega^2x = A\cos(\omega t) 0 = A\cos(\omega t),['ordinary-differential-equations']
27,"Regarding Picard's iteration its relation to numerical methods, such as Euler's method.","Regarding Picard's iteration its relation to numerical methods, such as Euler's method.",,"I've been told numerical methods in solving ODEs, such as Euler's method and Runge-Kutta, are all in some way approximations to Picard's iteration, and I'm trying to understand how. Suppose we have a differential equation on an interval $[x_0,x_L]$ : $$\frac{dy}{dx}=f(x,y)$$ with initial condition $y(x_0)=y_0$ I would like to numerically solve the equation on a set of points $\{x_0<x_1<\dots<x_n\}$ , i.e. obtain approximations $y_i$ to the true solution $y(x_i)$ for each $x_i$ . Picard's iteration works as follows: $$y_{0,0}=y_0$$ $$y_{0,k}(x_1)=y_0+\int_{x_0}^{x_1} f(x,y_{0,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1$$ suppose we stop for $k=m$ , then take $y_1=y_{0,m}(x_1)$ We then repeat the process for $i \geq 1$ . $$y_{i,0}=y_i$$ $$y_{i,k}(x_{i+1})=y_i+\int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1$$ $$y_{i+1}=y_{i,m}(x_{i+1})$$ So is the idea of a numerical method (e.g. Euler's method) to replace the integral $\int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx$ with an approximation such as $(x_{i+1}-x_{i})f(x_i,y_i)$ (Euler's method)? What I don't understand is why numerical methods only iterate once for each point $x_i$ (in other words, $m=1$ ) but Picard's iteration suggests you should iterate multiple (potentially many) times for each $x_i$ ?","I've been told numerical methods in solving ODEs, such as Euler's method and Runge-Kutta, are all in some way approximations to Picard's iteration, and I'm trying to understand how. Suppose we have a differential equation on an interval : with initial condition I would like to numerically solve the equation on a set of points , i.e. obtain approximations to the true solution for each . Picard's iteration works as follows: suppose we stop for , then take We then repeat the process for . So is the idea of a numerical method (e.g. Euler's method) to replace the integral with an approximation such as (Euler's method)? What I don't understand is why numerical methods only iterate once for each point (in other words, ) but Picard's iteration suggests you should iterate multiple (potentially many) times for each ?","[x_0,x_L] \frac{dy}{dx}=f(x,y) y(x_0)=y_0 \{x_0<x_1<\dots<x_n\} y_i y(x_i) x_i y_{0,0}=y_0 y_{0,k}(x_1)=y_0+\int_{x_0}^{x_1} f(x,y_{0,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1 k=m y_1=y_{0,m}(x_1) i \geq 1 y_{i,0}=y_i y_{i,k}(x_{i+1})=y_i+\int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1 y_{i+1}=y_{i,m}(x_{i+1}) \int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx (x_{i+1}-x_{i})f(x_i,y_i) x_i m=1 x_i","['ordinary-differential-equations', 'numerical-methods', 'numerical-calculus']"
28,"PDE: solve IVP where $u_{t} +3u u _{x} = 0$, $u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\ 0, x>1 \end{matrix} \right.$","PDE: solve IVP where ,","u_{t} +3u u _{x} = 0 u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\ 0, x>1 \end{matrix} \right.","I just started learning PDEs and I am at the very beginning of the book by Peter J. Olver. Now when I search in the stackexchange I found similar problems, all relating to Cauchy or Burger equations. The book so far has not spoken of these, so I guess I should be able to solve this without knowledge of them. I am asked to solve the IVP where $$u_{t} +3u u _{x} = 0, \quad \quad u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\  0, x>1 \end{matrix} \right.$$ My reasoning (likely to be wrong) so far is the following: we have a non-linear transport equation, where the speed of the wave is dictated by $3u$ (i.e. the wave is moving faster the bigger it is). We also know the solution is constant along the characteristic curve. We can see the wave being constant at $x>1$ but moving to the right when $x<1$ . This leads me to the following: $$ \begin{align} \frac{ \partial x}{\partial t} &= 3u \\ \frac{\partial u }{\partial t} &= 0 \\ u(0,x) &= f(x), \end{align} $$ Then $u(t,x) = f(x_{0})$ and now I am stuck. I do not know how to proceed. Any suggestions or comments on my train of thought or on how to proceed would be much appreciated.","I just started learning PDEs and I am at the very beginning of the book by Peter J. Olver. Now when I search in the stackexchange I found similar problems, all relating to Cauchy or Burger equations. The book so far has not spoken of these, so I guess I should be able to solve this without knowledge of them. I am asked to solve the IVP where My reasoning (likely to be wrong) so far is the following: we have a non-linear transport equation, where the speed of the wave is dictated by (i.e. the wave is moving faster the bigger it is). We also know the solution is constant along the characteristic curve. We can see the wave being constant at but moving to the right when . This leads me to the following: Then and now I am stuck. I do not know how to proceed. Any suggestions or comments on my train of thought or on how to proceed would be much appreciated.","u_{t} +3u u _{x} = 0, \quad \quad u(0,x) = \left \{ \begin{matrix} 2, x < 1 \\  0, x>1 \end{matrix} \right. 3u x>1 x<1 
\begin{align}
\frac{ \partial x}{\partial t} &= 3u \\
\frac{\partial u }{\partial t} &= 0 \\
u(0,x) &= f(x),
\end{align}
 u(t,x) = f(x_{0})","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
29,Uniqueness and existence of solution $\frac{dy}{dx} = \sqrt{x-y}$ given $y(2)=2$,Uniqueness and existence of solution  given,\frac{dy}{dx} = \sqrt{x-y} y(2)=2,"I want to determine if there exists a solution and the uniqueness of it for: $\frac{dy}{dx} = \sqrt{x-y}$ given $y(2)=2$ A solution said that we cannot determine if there exists a solution since it was discontinuous at (2,2). This solution seems to imply that it is possible for x to be a number for example 1.99 and y be 2.00 or x be 2.00 and y be 2.01. I'm confused as to why the textbook makes this sort of assumption - I thought we were just determining the continuity of the point. If we were to look at the neighborhood then why then if we are given $y(2)=1$ as the condition that we know that the above has a solution? It seems arbitrary as to how far we can go left and right or above and down a certain point to say that it was not continuous?","I want to determine if there exists a solution and the uniqueness of it for: given A solution said that we cannot determine if there exists a solution since it was discontinuous at (2,2). This solution seems to imply that it is possible for x to be a number for example 1.99 and y be 2.00 or x be 2.00 and y be 2.01. I'm confused as to why the textbook makes this sort of assumption - I thought we were just determining the continuity of the point. If we were to look at the neighborhood then why then if we are given as the condition that we know that the above has a solution? It seems arbitrary as to how far we can go left and right or above and down a certain point to say that it was not continuous?",\frac{dy}{dx} = \sqrt{x-y} y(2)=2 y(2)=1,"['calculus', 'ordinary-differential-equations', 'continuity']"
30,Laplace eigenvalues on unit disc intermediate step,Laplace eigenvalues on unit disc intermediate step,,"If I try for a separable solution $u(r,\theta)=R(r)\Theta(\theta)$ to the equation $\Delta u=-ku$ where $k\geq0$ I run into some problems before getting to the Bessel functions part. After separation the equation simplifies to $\frac{r^2}{R}R''+\frac{r}{R}R'+kr^2=-\frac{1}{\Theta}\Theta''$ . Now I would set both sides to be a constant $c$ . But in every solution I have seen, they have set this constant to be $n^2$ (so that $c=n^2$ in my case) because $c\geq0$ . But I don't understand how we can say $c\geq0$ . Why is this true?","If I try for a separable solution to the equation where I run into some problems before getting to the Bessel functions part. After separation the equation simplifies to . Now I would set both sides to be a constant . But in every solution I have seen, they have set this constant to be (so that in my case) because . But I don't understand how we can say . Why is this true?","u(r,\theta)=R(r)\Theta(\theta) \Delta u=-ku k\geq0 \frac{r^2}{R}R''+\frac{r}{R}R'+kr^2=-\frac{1}{\Theta}\Theta'' c n^2 c=n^2 c\geq0 c\geq0","['ordinary-differential-equations', 'partial-differential-equations', 'laplacian', 'eigenfunctions']"
31,Ballistic Motion in Space with One Gravitating Body.,Ballistic Motion in Space with One Gravitating Body.,,"I had an interesting math problem presented to me some time ago by a friend (he stated it in non-mathematical terms). At what angle would you launch a projectile from a spaceship/satellite such that it left that object and went on to hit another orbiting object? Then as a supplemental question he asked at what angle would you launch that projectile to hit the other orbiting object in the least amount of time? I assumed that the objects were only acted upon by a single spherically symmetric mass distribution such that I could treat it as a one-body problem for each object. Further, I assumed it all took place in the plane with a polar coordinate system so that I ended up with this simple system of nonlinear autonomous ODE's, $$ \begin{bmatrix} \frac{d \theta}{dt}  \\ \frac{dv}{dt}\\ \frac{dr}{dt} \end{bmatrix} =  \begin{bmatrix} \frac{h}{r^{2}} \\ \frac{h^{2}}{r^{3}} -\frac{\mu}{r^{2}} \\ v \end{bmatrix}.$$ Where the inital conditions for the projectile would be $\{ \theta_{i} , r_{i}, v_{\beta}\cos(\phi - \theta_{i}) + v_{i}\}$ with an $h_{\phi} = r_{i} (v_{\beta}\sin(\phi - \theta_{i}) + v_{\theta i})$ and the target objects' initial conditions are $\{ \theta_{i}^{'} , r_{i}^{'}, v_{i}^{'}\}$ with $h'= r^{'}_{i} v_{\theta i}^{'}$ . Where $\phi$ is the launch angle from the polar axis while $v_{\beta}$ is the magnitude of the projectiles velocity (which I assume to not change, only its launch direction) and $\mu$ is a constant relating to the gravitational field strength of the attracting object. Below is a picture depicting the general initial and final conditions. After all these preliminaries, i'm basically asking if there is a variational calculus or other simpler solution to solving this problem as perhaps a boundary problem of sorts mixed with an initial ODE problem. That is, aside from computationally pouring on through thousands of trajectories with minutely differing $\phi$ 's then numerically guessing at the appropriate approximate launch angle or angles that solve my question(s). Which. . . isn't what I exactly want to do and I desire to know if there is an equation, a single or system of ODE's, that I could solve themselves for this launch angle which gives a least time of travel or gives launch angles that would lead to a hit (irrespective of the time of travel). If you can help in anyway, this would be most appreciated. I'm a sophomore College student with little knowledge of solving robustly ODE's or even programming solvers for them.","I had an interesting math problem presented to me some time ago by a friend (he stated it in non-mathematical terms). At what angle would you launch a projectile from a spaceship/satellite such that it left that object and went on to hit another orbiting object? Then as a supplemental question he asked at what angle would you launch that projectile to hit the other orbiting object in the least amount of time? I assumed that the objects were only acted upon by a single spherically symmetric mass distribution such that I could treat it as a one-body problem for each object. Further, I assumed it all took place in the plane with a polar coordinate system so that I ended up with this simple system of nonlinear autonomous ODE's, Where the inital conditions for the projectile would be with an and the target objects' initial conditions are with . Where is the launch angle from the polar axis while is the magnitude of the projectiles velocity (which I assume to not change, only its launch direction) and is a constant relating to the gravitational field strength of the attracting object. Below is a picture depicting the general initial and final conditions. After all these preliminaries, i'm basically asking if there is a variational calculus or other simpler solution to solving this problem as perhaps a boundary problem of sorts mixed with an initial ODE problem. That is, aside from computationally pouring on through thousands of trajectories with minutely differing 's then numerically guessing at the appropriate approximate launch angle or angles that solve my question(s). Which. . . isn't what I exactly want to do and I desire to know if there is an equation, a single or system of ODE's, that I could solve themselves for this launch angle which gives a least time of travel or gives launch angles that would lead to a hit (irrespective of the time of travel). If you can help in anyway, this would be most appreciated. I'm a sophomore College student with little knowledge of solving robustly ODE's or even programming solvers for them."," \begin{bmatrix} \frac{d \theta}{dt}  \\ \frac{dv}{dt}\\ \frac{dr}{dt} \end{bmatrix} =  \begin{bmatrix} \frac{h}{r^{2}} \\ \frac{h^{2}}{r^{3}} -\frac{\mu}{r^{2}} \\ v \end{bmatrix}. \{ \theta_{i} , r_{i}, v_{\beta}\cos(\phi - \theta_{i}) + v_{i}\} h_{\phi} = r_{i} (v_{\beta}\sin(\phi - \theta_{i}) + v_{\theta i}) \{ \theta_{i}^{'} , r_{i}^{'}, v_{i}^{'}\} h'= r^{'}_{i} v_{\theta i}^{'} \phi v_{\beta} \mu \phi","['ordinary-differential-equations', 'numerical-methods', 'calculus-of-variations']"
32,Writes in polar coordinates $x''-(1-x^2-(x')^2)x'+4x=0$.,Writes in polar coordinates .,x''-(1-x^2-(x')^2)x'+4x=0,"I need to determine a periodic solution for : $x''-(1-x^2-(x')^2)x'+4x=0$ . We have the equivalent system: $$\begin{cases}         x'=y \\        y'=(1-x^2-y^2)y-4x.    \end{cases} $$ We determined the stationary points for the equivalent system: $(0,0)$ . I saw that this solution is unstable. But now I have to turn this system into polar coordinates. Unfortunately, I tried in all possibilities and I failed to bring the system to a beautiful shape depending on the polar coordinates. I present what I tried: Polar coordinates: $\begin{cases}         x(r,\theta)=r\cos(\theta) \\        y(r,\theta)=r\sin(\theta)    \end{cases} $ We have that $x^2+y^2=r^2$ and $\tan(\theta)=\frac{y}{x}$ . So $r'=\frac{xx'+yy'}{r}$ and $\theta'=\frac{xy'-x'y}{r^2}.$ So $$r'=\frac{xy+y[(1-x^2-y^2)y-4x]}{r}=\frac{xy+y^2-x^2y^2-y^4-4xy}{r}.$$ I tried to calculate, but I have no idea how to bring the system into polar coordinates to continue my work. Thanks!","I need to determine a periodic solution for : . We have the equivalent system: We determined the stationary points for the equivalent system: . I saw that this solution is unstable. But now I have to turn this system into polar coordinates. Unfortunately, I tried in all possibilities and I failed to bring the system to a beautiful shape depending on the polar coordinates. I present what I tried: Polar coordinates: We have that and . So and So I tried to calculate, but I have no idea how to bring the system into polar coordinates to continue my work. Thanks!","x''-(1-x^2-(x')^2)x'+4x=0 \begin{cases} 
       x'=y \\
       y'=(1-x^2-y^2)y-4x.
   \end{cases}
 (0,0) \begin{cases} 
       x(r,\theta)=r\cos(\theta) \\
       y(r,\theta)=r\sin(\theta)
   \end{cases}
 x^2+y^2=r^2 \tan(\theta)=\frac{y}{x} r'=\frac{xx'+yy'}{r} \theta'=\frac{xy'-x'y}{r^2}. r'=\frac{xy+y[(1-x^2-y^2)y-4x]}{r}=\frac{xy+y^2-x^2y^2-y^4-4xy}{r}.","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'polar-coordinates']"
33,What kind of ODE is this,What kind of ODE is this,,What kind of ODE is the following one: $y^{(n)}(t) = \dfrac{x^{(n)}(t)}{f(n)}$ I have never seen such type. Any references would be very much appreciated. Thanks,What kind of ODE is the following one: I have never seen such type. Any references would be very much appreciated. Thanks,y^{(n)}(t) = \dfrac{x^{(n)}(t)}{f(n)},"['calculus', 'ordinary-differential-equations']"
34,Proving the solution of an ODE exists for $t \geq 0$,Proving the solution of an ODE exists for,t \geq 0,"Consider the system $$x'(t) = y(t)$$ $$y'(t) = -y(t)^3-p(x(t)),$$ where $p(x(t))$ is a polynomial of odd degree whose leading coefficient is positive. I'm interested in proving that the solution exists for all $t \geq 0$ . I was thinking of using the Lyapunov function $L(x,y)= \frac{1}{2}y^2+P(x)$ , where $P(x)$ is the antiderivative of $p(x)$ . However, I'm not sure how to prove it. Any help would be appreciated.","Consider the system where is a polynomial of odd degree whose leading coefficient is positive. I'm interested in proving that the solution exists for all . I was thinking of using the Lyapunov function , where is the antiderivative of . However, I'm not sure how to prove it. Any help would be appreciated.","x'(t) = y(t) y'(t) = -y(t)^3-p(x(t)), p(x(t)) t \geq 0 L(x,y)= \frac{1}{2}y^2+P(x) P(x) p(x)","['ordinary-differential-equations', 'dynamical-systems']"
35,Integrating factor for the ODE $(3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0$,Integrating factor for the ODE,(3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0,"I was asked to find an integrating factor $\mu = \mu(x+y^2)$ for the ODE $$(3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0.$$ So the natural approach was define $P = 3x+2y+y^2$ and $Q = x+4xy+5y^2.$ Then, $$\partial_yP = 2+2y,~~~\partial_xQ = 1+4y$$ and hence $\partial_yP - \partial_xQ = 1-2y.$ Let us consider then $$\frac{\partial_yP-\partial_xQ}{Q} = \frac{1-2y}{x+4xy+5y^2}$$ and define $z = x+y^2.$ Thus, $$\frac{\partial_yP - \partial_xQ}{Q} = \frac{1-2y}{(z-y^2)(1+4y)+5y^2}.$$ But from here I really don't know how to proceed. Any hint?","I was asked to find an integrating factor for the ODE So the natural approach was define and Then, and hence Let us consider then and define Thus, But from here I really don't know how to proceed. Any hint?","\mu = \mu(x+y^2) (3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0. P = 3x+2y+y^2 Q = x+4xy+5y^2. \partial_yP = 2+2y,~~~\partial_xQ = 1+4y \partial_yP - \partial_xQ = 1-2y. \frac{\partial_yP-\partial_xQ}{Q} = \frac{1-2y}{x+4xy+5y^2} z = x+y^2. \frac{\partial_yP - \partial_xQ}{Q} = \frac{1-2y}{(z-y^2)(1+4y)+5y^2}.","['calculus', 'ordinary-differential-equations']"
36,"If $|f''| \leq M|f |$, how to prove that $f(x) \equiv 0$ [closed]","If , how to prove that  [closed]",|f''| \leq M|f | f(x) \equiv 0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This time I come across a problem. A function $f(x)$ is defined on an  interval $[a,b]$ , and given that $f(a) = f'(a) = 0$ , there exists a also const $M$ which satisfies $$|f''(x)| \le M|f(x)|$$ How to obtain $f(x) \equiv 0$ ? Edit : the content of the related section is about the differential form of Grönwall's inequality here : If $f(x)$ is differential on $[a,b]$ , $f(a)=0$ and there exists a const $M$ which satisfies $$|f'(x)| \le M|f(x)|$$ we can easily obtain $$f(x) \equiv 0$$ . I tried to use a similar method in the book to prove the second order derivative situation, but without success.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This time I come across a problem. A function is defined on an  interval , and given that , there exists a also const which satisfies How to obtain ? Edit : the content of the related section is about the differential form of Grönwall's inequality here : If is differential on , and there exists a const which satisfies we can easily obtain . I tried to use a similar method in the book to prove the second order derivative situation, but without success.","f(x) [a,b] f(a) = f'(a) = 0 M |f''(x)| \le M|f(x)| f(x) \equiv 0 f(x) [a,b] f(a)=0 M |f'(x)| \le M|f(x)| f(x) \equiv 0","['real-analysis', 'ordinary-differential-equations']"
37,Intuition behind the general form of the solution in Frobenius Method?,Intuition behind the general form of the solution in Frobenius Method?,,"When we solve the equation $z^{2}u''+p(z)zu'+q(z)u=0$ using Frobenius method , we first find zeros of the indicial polynomial $r_1>r_2$ to get a solution $u_1$ . In the case that the difference of roots $r_1-r_2$ is an integer, we might need to find a second linearly independent solution using a different method. It is given on the Wikipedia page (and many other places) that $$ u_{2}=Cu_{1}\ln x+\sum _{{k=0}}^{\infty }B_{k}x^{{k+r_{2}}} $$ gives the desired second solution. My question is: how do we come up with this general form of solution? I try to derive it in various ways, but find it hard to achieve - I have tried to write $u_2 =vu_1$ , which reduces the order of the DE and gives $$ v'= u_1^{-2}\exp\left(- \int \frac{p(z)}{z} dz\right), $$ but this looks completely different from the formula above. I do see some vague intuition about the logarithmic term - we are basically saying: if multiplying by $x^{-r}$ does not make the solution analytic, then something stronger like $\ln x$ will. In other words, we create an essential singularity by adding $\ln x$ . I wish to see a more satisfying explanation of this.","When we solve the equation using Frobenius method , we first find zeros of the indicial polynomial to get a solution . In the case that the difference of roots is an integer, we might need to find a second linearly independent solution using a different method. It is given on the Wikipedia page (and many other places) that gives the desired second solution. My question is: how do we come up with this general form of solution? I try to derive it in various ways, but find it hard to achieve - I have tried to write , which reduces the order of the DE and gives but this looks completely different from the formula above. I do see some vague intuition about the logarithmic term - we are basically saying: if multiplying by does not make the solution analytic, then something stronger like will. In other words, we create an essential singularity by adding . I wish to see a more satisfying explanation of this.","z^{2}u''+p(z)zu'+q(z)u=0 r_1>r_2 u_1 r_1-r_2 
u_{2}=Cu_{1}\ln x+\sum _{{k=0}}^{\infty }B_{k}x^{{k+r_{2}}}
 u_2 =vu_1 
v'= u_1^{-2}\exp\left(- \int \frac{p(z)}{z} dz\right),
 x^{-r} \ln x \ln x","['ordinary-differential-equations', 'analysis', 'analytic-functions', 'frobenius-method']"
38,Solving a differential equation using an integrating factor,Solving a differential equation using an integrating factor,,"I'm trying the solve the following equation: $ \left\{\begin{matrix} (x^B+y^B)(xdy-ydx)=(1+x)x^9dx \\  y(-1)=A \end{matrix}\right. $ for $A=1$ and $A=0$ . $B\in2\mathbb{N}_0+1$ My solution is the following, but I got stuck: $(x^B+y^B)(xdy-ydx)=(1+x)*x^9dx$ $x^B*xdy-y*x^Bdx+y^B*xdy-y*y^Bdx-(1+x)*x^9dx=0$ $x^B*xdy+y^B*xdy-y*x^Bdx-y*y^Bdx-(1+x)*x^9dx=0$ $xdy(x^B+y^B)+[-y(x^B+y^B)-(1+x)*x^9]dx=0$ where: $Q(x,y)=x(x^B+y^B)dy$ $P(x,y)=[-y(x^B+y^B)-(1+x)*x^9]$ Then I made partial derivation: $\frac{∂Q}{∂x}:(x^B+y^B)x(Bx^{B-1})$ $\frac{∂P}{∂y}: -(x^B+y^B)-y(By^{B-1})$ And then subtract: $\frac{∂P}{∂y}-\frac{∂Q}{∂x}=-(x^B+y^B)-y(By^{B-1})-(x^B+y^B)-x(Bx^{B-1})$ But probably I did something wrong and I'm stuck and not sure where made I mistake.. Can you help me please?","I'm trying the solve the following equation: for and . My solution is the following, but I got stuck: where: Then I made partial derivation: And then subtract: But probably I did something wrong and I'm stuck and not sure where made I mistake.. Can you help me please?"," \left\{\begin{matrix}
(x^B+y^B)(xdy-ydx)=(1+x)x^9dx \\  y(-1)=A \end{matrix}\right.  A=1 A=0 B\in2\mathbb{N}_0+1 (x^B+y^B)(xdy-ydx)=(1+x)*x^9dx x^B*xdy-y*x^Bdx+y^B*xdy-y*y^Bdx-(1+x)*x^9dx=0 x^B*xdy+y^B*xdy-y*x^Bdx-y*y^Bdx-(1+x)*x^9dx=0 xdy(x^B+y^B)+[-y(x^B+y^B)-(1+x)*x^9]dx=0 Q(x,y)=x(x^B+y^B)dy P(x,y)=[-y(x^B+y^B)-(1+x)*x^9] \frac{∂Q}{∂x}:(x^B+y^B)x(Bx^{B-1}) \frac{∂P}{∂y}: -(x^B+y^B)-y(By^{B-1}) \frac{∂P}{∂y}-\frac{∂Q}{∂x}=-(x^B+y^B)-y(By^{B-1})-(x^B+y^B)-x(Bx^{B-1})",['ordinary-differential-equations']
39,reduce a differential equation $y'+\dfrac{x}{y}=0$,reduce a differential equation,y'+\dfrac{x}{y}=0,"I want to reduce a differential equation. $y'+\dfrac{x}{y}=0$ I reduced this. But my answer don't much with Wolfram alpha . Please tell me what is wrong. $y'=-\dfrac{x}{y}=-\dfrac{1}{\left( \dfrac{y}{x}\right) }$ When I put, $u=\dfrac{y}{x}$ , $\dfrac{dy}{dx}=-\dfrac{1}{u}$ differentiate both sides with respect to x $\dfrac{du}{dx}=-\dfrac{1+u^{2}}{ux}$ $\dfrac{u}{1+u^{2}}\dfrac{du}{dx}=-\dfrac{1}{x}$ Integrate both sides with respect to x $\dfrac{1}{2}\log \left| u^{2}+1\right| =-\log \left| x\right| +C'$ Organize the formula $u^{2}+1=\dfrac{C}{x}$ $\begin{aligned}\dfrac{y^{2}}{x}+1=\dfrac{C}{x}\\ y=\pm \sqrt{Cx-x^{2}}\end{aligned}$","I want to reduce a differential equation. I reduced this. But my answer don't much with Wolfram alpha . Please tell me what is wrong. When I put, , differentiate both sides with respect to x Integrate both sides with respect to x Organize the formula",y'+\dfrac{x}{y}=0 y'=-\dfrac{x}{y}=-\dfrac{1}{\left( \dfrac{y}{x}\right) } u=\dfrac{y}{x} \dfrac{dy}{dx}=-\dfrac{1}{u} \dfrac{du}{dx}=-\dfrac{1+u^{2}}{ux} \dfrac{u}{1+u^{2}}\dfrac{du}{dx}=-\dfrac{1}{x} \dfrac{1}{2}\log \left| u^{2}+1\right| =-\log \left| x\right| +C' u^{2}+1=\dfrac{C}{x} \begin{aligned}\dfrac{y^{2}}{x}+1=\dfrac{C}{x}\\ y=\pm \sqrt{Cx-x^{2}}\end{aligned},['ordinary-differential-equations']
40,Solving the heat equation in spherical polars with nonhomogeneous boundary conditions,Solving the heat equation in spherical polars with nonhomogeneous boundary conditions,,"Trying to find the series solution of $r\rho''+2\rho'-\lambda r \rho = f$ with certain ICs & BCs. Question: what is wrong with solution strategy (numerical check does not confirm result). My question: What function describes the internal temperature change of a spherical body with some initial temperature profile when its surroundings is heated up? Plan Set up equation governing heat transfer Put it into spherical polar coordinates Set up Initial & Boundary Conditions Obtain two separated equations using separation of variables, as here, in a different setup Obtain solutions for those: [THIS IS WHERE IT GOES WRONG] Temporal one hopefully be a first order ODE, some exponential as solution Spatial one probably will be more complicated, use: series solution Apply IC & BC to fix constants in the general solutions, get final result Execution $\dot{u} = \alpha \Delta u$ , $R \geq r \geq 0$ , $R$ being the radius of the sphere, $t \geq 0$ . Since we expect a spherically symmetric solution, $u=u(r,t)$ . $u$ it does not depend on $\theta$ & $\phi$ . $\dot{u} = \alpha \frac{1}{r^2} \frac{\partial}{\partial r}(r^2\frac{\partial u}{\partial r})$ . Since $u$ is only a function of $r$ & $t$ , we can ignore the $\theta$ & $\phi$ dependence of $\Delta$ . IC: $u(r \leq R,t=0) = f(r)$ (ie initial internal temperature of the spherical object) BC: $u(r=R,t>0) = T_f$ (ie temperature of heated up surroundings , heating up taken as instantaneous) Let $u=T(t)\rho(r)$ . Separated equations: temporal: $T' = \lambda \alpha T$ spatial: $r\rho''+2\rho'-\lambda r \rho = 0$ (similarly to this & this ) Solutions: Temporal: $T(t)=Ae^{\lambda \alpha t}$ Spatial: let $\rho(r) = \sum_0^{\infty}c_n r^n$ . (Not allowing negative powers because we would like to have a real solution at $r=0$ as well.) Substitute this in to the equation for $\rho$ : $$r\sum_2^{\infty}n(n-1)c_nr^{n-2}+2\sum_1^{\infty}nc_nr^{n-1}-\lambda r\sum_0^{\infty}c_n r^n=0$$ which is equal to: $$\sum_2^{\infty}n(n-1)c_nr^{n-1}+2\sum_1^{\infty}nc_nr^{n-1}-\lambda \sum_0^{\infty}c_n r^{n+1}=0$$ In the first sum, change every $n$ to $n+2$ & lower the lower limit of summation by 2. In the second sum, change every $n$ to $n+1$ & lower the lower limit of summation by 1. Obtain: $$\sum_0^{\infty}(n+2)(n+1)c_{n+2}r^{n+1}+2(n+1)c_{n+1}r^{n}-\lambda c_{n}r^{n+1} = 0$$ The above equation can only be true if it is true for each powers of $r$ . Consider the $r^{n+1}$ terms: $$(n+2)(n+1)c_{n+2} - \lambda c_n = -2(n+2)c_{n+2}$$ Rearrange: $$(n+4)(n+1)c_{n+2}=\lambda c_n$$ Rewrite: $$(n+2)(n-1)c_n = \lambda c_{n-2}$$ Obtain recurrence relation: $$c_n = \frac{\lambda c_{n-2}}{(n+2)(n-1)}$$ To not to get into trouble with zero division, lets set $c_{-1}$ to $0$ , rendering all $c_{n=odd}$ zero. We are free to choose $c_0$ , determining all even coefficients. Let's plot this & check that we are right! Using $c_0=\lambda=1$ , computed an approximation to $\rho$ using $c_n$ s up to $n=100$ . Also computed an approximation to $r\rho''+2\rho'-\lambda r \rho$ , which should be zero. (Notebook available here , the code is also on the bottom of this post.) Unfortunately, it isn't: (Note that this post has a very similar homogeneous equation to mine, but it doesn't use series solutions, but claims that this equation ""integrates into"" the solution.) We obtain: $u=T(t,A)\rho(c_0,r)$ . Even if my $\rho$ function was correct, BC hardly seem satisfiably for general $f$ with these constants. Am I missing something? I recall in school (/uni) we solved the homogeneous case, when $f$ is $0$ , obtained a general solution, then changed $f$ to something nonzero, found a particular solution, then added the general & the particular solutions. My plan for this: Find orthonormal eigenfunctions $y_n$ & eigenvalues $\sigma_n$ for $Ly_n=\sigma_n y_n$ , with $L = \frac{\partial^2}{\partial r^2}+\frac{2}{r}\frac{\partial}{\partial r}$ Fix $T(t=0)=1$ , rewrite IC as $rL\rho=r(L\rho_g + L\rho_p) = rL\rho_p = f$ , ie $L\rho_p=f/r$ where $\rho_g$ & $\rho_p$ are the general & particular solutions in the $u$ , where $u=T(t)\rho(r)=T(t)(\rho_{general}(r)+\rho_{particular}(r))$ , $u$ solving $\dot{u} = \alpha \Delta u$ . Let $\rho_p=\sum_0^{\infty}s_n y_n$ Since $$\langle L\rho_p | y_m \rangle = \langle L\sum_{n=0}^{\infty}s_n y_n | y_m \rangle = \langle \sum_{n=0}^{\infty}s_n L y_n | y_m \rangle = \langle \sum_{n=0}^{\infty}s_n \sigma_n y_n | y_m \rangle = s_n \sigma_n \langle  \sum_{n=0}^{\infty}y_n | y_m \rangle = s_m \sigma_m \implies {s_m=\frac{\langle L\rho_p | y_m \rangle}{\sigma_m}},$$ where $\langle f | g \rangle = \int_{all space} f g dV = \int_r=0^{\infty} \int_{\theta=0}^{2\pi} \int_{\phi=0}^{\pi} f(r, \phi, \theta) g(r, \theta, \phi) r^2 \sin(\phi) dr d\phi d\theta$ . Therefore, I can compute all $s_n$ s, therefore obtain $\rho_p$ , giving me a $\rho=\rho_g+\rho_p$ , with which I can satisfy the BC & IC: $\rho_g$ will satisfy BC, while $\rho_p$ will satisfy IC. Question What goes wrong in the 5th step of the plan and is the outline for the last, 6th step is correct, or am I missing something? The code (Notebook here ) import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm  LAMBDA = 1 c1=1 def Cn(n, LAMBDA = LAMBDA, c1=c1):     if n%2==1:         return 0     elif n==0:         return 1     else:         return LAMBDA*Cn(n-2)/ ((n+2)*(n-1))  start=-1 stop=10 step=0.0001 rs = np.arange(start, stop+step, step)  rho = [sum([Cn(n)*(r**n) for n in range(0,100+1)]) for r in tqdm(rs)]  def deriv(arr, dx=step):     return np.gradient(arr, dx)  rhoderiv = deriv(rho) rhoderivderiv = deriv(rhoderiv)  this_is_rather_be_zero = [r*rhoderivderiv[i] + 2*rhoderiv[i] - LAMBDA * r * rho[i] for i, r in enumerate(rs)]  plt.plot(rs, rho,c='k',label='rho') plt.plot(rs, this_is_rather_be_zero,c='g',label='shouldBeZero') plt.axvline(c='r') plt.axhline(c='r') plt.xlim([-1,5]) plt.ylim([-1,5]) plt.legend()","Trying to find the series solution of with certain ICs & BCs. Question: what is wrong with solution strategy (numerical check does not confirm result). My question: What function describes the internal temperature change of a spherical body with some initial temperature profile when its surroundings is heated up? Plan Set up equation governing heat transfer Put it into spherical polar coordinates Set up Initial & Boundary Conditions Obtain two separated equations using separation of variables, as here, in a different setup Obtain solutions for those: [THIS IS WHERE IT GOES WRONG] Temporal one hopefully be a first order ODE, some exponential as solution Spatial one probably will be more complicated, use: series solution Apply IC & BC to fix constants in the general solutions, get final result Execution , , being the radius of the sphere, . Since we expect a spherically symmetric solution, . it does not depend on & . . Since is only a function of & , we can ignore the & dependence of . IC: (ie initial internal temperature of the spherical object) BC: (ie temperature of heated up surroundings , heating up taken as instantaneous) Let . Separated equations: temporal: spatial: (similarly to this & this ) Solutions: Temporal: Spatial: let . (Not allowing negative powers because we would like to have a real solution at as well.) Substitute this in to the equation for : which is equal to: In the first sum, change every to & lower the lower limit of summation by 2. In the second sum, change every to & lower the lower limit of summation by 1. Obtain: The above equation can only be true if it is true for each powers of . Consider the terms: Rearrange: Rewrite: Obtain recurrence relation: To not to get into trouble with zero division, lets set to , rendering all zero. We are free to choose , determining all even coefficients. Let's plot this & check that we are right! Using , computed an approximation to using s up to . Also computed an approximation to , which should be zero. (Notebook available here , the code is also on the bottom of this post.) Unfortunately, it isn't: (Note that this post has a very similar homogeneous equation to mine, but it doesn't use series solutions, but claims that this equation ""integrates into"" the solution.) We obtain: . Even if my function was correct, BC hardly seem satisfiably for general with these constants. Am I missing something? I recall in school (/uni) we solved the homogeneous case, when is , obtained a general solution, then changed to something nonzero, found a particular solution, then added the general & the particular solutions. My plan for this: Find orthonormal eigenfunctions & eigenvalues for , with Fix , rewrite IC as , ie where & are the general & particular solutions in the , where , solving . Let Since where . Therefore, I can compute all s, therefore obtain , giving me a , with which I can satisfy the BC & IC: will satisfy BC, while will satisfy IC. Question What goes wrong in the 5th step of the plan and is the outline for the last, 6th step is correct, or am I missing something? The code (Notebook here ) import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm  LAMBDA = 1 c1=1 def Cn(n, LAMBDA = LAMBDA, c1=c1):     if n%2==1:         return 0     elif n==0:         return 1     else:         return LAMBDA*Cn(n-2)/ ((n+2)*(n-1))  start=-1 stop=10 step=0.0001 rs = np.arange(start, stop+step, step)  rho = [sum([Cn(n)*(r**n) for n in range(0,100+1)]) for r in tqdm(rs)]  def deriv(arr, dx=step):     return np.gradient(arr, dx)  rhoderiv = deriv(rho) rhoderivderiv = deriv(rhoderiv)  this_is_rather_be_zero = [r*rhoderivderiv[i] + 2*rhoderiv[i] - LAMBDA * r * rho[i] for i, r in enumerate(rs)]  plt.plot(rs, rho,c='k',label='rho') plt.plot(rs, this_is_rather_be_zero,c='g',label='shouldBeZero') plt.axvline(c='r') plt.axhline(c='r') plt.xlim([-1,5]) plt.ylim([-1,5]) plt.legend()","r\rho''+2\rho'-\lambda r \rho = f \dot{u} = \alpha \Delta u R \geq r \geq 0 R t \geq 0 u=u(r,t) u \theta \phi \dot{u} = \alpha \frac{1}{r^2} \frac{\partial}{\partial r}(r^2\frac{\partial u}{\partial r}) u r t \theta \phi \Delta u(r \leq R,t=0) = f(r) u(r=R,t>0) = T_f u=T(t)\rho(r) T' = \lambda \alpha T r\rho''+2\rho'-\lambda r \rho = 0 T(t)=Ae^{\lambda \alpha t} \rho(r) = \sum_0^{\infty}c_n r^n r=0 \rho r\sum_2^{\infty}n(n-1)c_nr^{n-2}+2\sum_1^{\infty}nc_nr^{n-1}-\lambda r\sum_0^{\infty}c_n r^n=0 \sum_2^{\infty}n(n-1)c_nr^{n-1}+2\sum_1^{\infty}nc_nr^{n-1}-\lambda \sum_0^{\infty}c_n r^{n+1}=0 n n+2 n n+1 \sum_0^{\infty}(n+2)(n+1)c_{n+2}r^{n+1}+2(n+1)c_{n+1}r^{n}-\lambda c_{n}r^{n+1} = 0 r r^{n+1} (n+2)(n+1)c_{n+2} - \lambda c_n = -2(n+2)c_{n+2} (n+4)(n+1)c_{n+2}=\lambda c_n (n+2)(n-1)c_n = \lambda c_{n-2} c_n = \frac{\lambda c_{n-2}}{(n+2)(n-1)} c_{-1} 0 c_{n=odd} c_0 c_0=\lambda=1 \rho c_n n=100 r\rho''+2\rho'-\lambda r \rho u=T(t,A)\rho(c_0,r) \rho f f 0 f y_n \sigma_n Ly_n=\sigma_n y_n L = \frac{\partial^2}{\partial r^2}+\frac{2}{r}\frac{\partial}{\partial r} T(t=0)=1 rL\rho=r(L\rho_g + L\rho_p) = rL\rho_p = f L\rho_p=f/r \rho_g \rho_p u u=T(t)\rho(r)=T(t)(\rho_{general}(r)+\rho_{particular}(r)) u \dot{u} = \alpha \Delta u \rho_p=\sum_0^{\infty}s_n y_n \langle L\rho_p | y_m \rangle = \langle L\sum_{n=0}^{\infty}s_n y_n | y_m \rangle = \langle \sum_{n=0}^{\infty}s_n L y_n | y_m \rangle = \langle \sum_{n=0}^{\infty}s_n \sigma_n y_n | y_m \rangle = s_n \sigma_n \langle  \sum_{n=0}^{\infty}y_n | y_m \rangle = s_m \sigma_m
\implies {s_m=\frac{\langle L\rho_p | y_m \rangle}{\sigma_m}}, \langle f | g \rangle = \int_{all space} f g dV = \int_r=0^{\infty} \int_{\theta=0}^{2\pi} \int_{\phi=0}^{\pi} f(r, \phi, \theta) g(r, \theta, \phi) r^2 \sin(\phi) dr d\phi d\theta s_n \rho_p \rho=\rho_g+\rho_p \rho_g \rho_p","['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'eigenfunctions', 'sturm-liouville']"
41,How do these two solutions of $\frac{dn}{dt}=rn(t)+m$ correspond/relate to each other?,How do these two solutions of  correspond/relate to each other?,\frac{dn}{dt}=rn(t)+m,"I attempted to solve the following first-order linear ODE: $$ \frac{dn}{dt} = rn(t)+m $$ This is what I managed to come up with: \begin{align} \int \frac{1}{rn(t)+m}dn & = \int 1dt \\ \frac{\ln(rn(t)+m)}{r} & = t+C \\ rn(t)+m & = e^{r(t+C)}\\ rn(t)+m & = e^{rt}e^{rC} \end{align} $$ rn(0)+m = e^{rC} $$ \begin{align} rn(t)+m & = e^{rt}(rn(0)+m) \\ n(t) & = \frac{e^{rt}rn(0)+e^{rt}m-m}{r} \\ n(t) & = \frac{e^{rt}rn(0)+(e^{rt}-1)m}{r} \\ n(t) & = e^{rt}n(0)-\frac{m}{r}(1-e^{rt}) \end{align} This solution should be correct based on the book where I got the ODE from. (Otto, S.P.; Day, T., 2007. A Biologist's Guide to Mathematical Modeling in Ecology and Evolution. Princeton University Press. (p. 200.: Recipe 6.3)) My purpose was to work the solution out by a separation of variables by myself. But when I give $\frac{dn}{dt}=rn(t)+m$ to WolframAlpha, its solution is the following: $$ n(t) = c_1e^{rt}-\frac{m}{r} $$ Where does this come from? And how do these two solutions correspond/relate to each other? I've tried but could not work it out.","I attempted to solve the following first-order linear ODE: This is what I managed to come up with: This solution should be correct based on the book where I got the ODE from. (Otto, S.P.; Day, T., 2007. A Biologist's Guide to Mathematical Modeling in Ecology and Evolution. Princeton University Press. (p. 200.: Recipe 6.3)) My purpose was to work the solution out by a separation of variables by myself. But when I give to WolframAlpha, its solution is the following: Where does this come from? And how do these two solutions correspond/relate to each other? I've tried but could not work it out."," \frac{dn}{dt} = rn(t)+m  \begin{align}
\int \frac{1}{rn(t)+m}dn & = \int 1dt \\
\frac{\ln(rn(t)+m)}{r} & = t+C \\
rn(t)+m & = e^{r(t+C)}\\
rn(t)+m & = e^{rt}e^{rC}
\end{align}  rn(0)+m = e^{rC}  \begin{align}
rn(t)+m & = e^{rt}(rn(0)+m) \\
n(t) & = \frac{e^{rt}rn(0)+e^{rt}m-m}{r} \\
n(t) & = \frac{e^{rt}rn(0)+(e^{rt}-1)m}{r} \\
n(t) & = e^{rt}n(0)-\frac{m}{r}(1-e^{rt})
\end{align} \frac{dn}{dt}=rn(t)+m  n(t) = c_1e^{rt}-\frac{m}{r} ",['ordinary-differential-equations']
42,Why does solving this equation with differential equations and related rates yield different results?,Why does solving this equation with differential equations and related rates yield different results?,,"Sorry for the figure being so large I was unsure how to shrink it. The question asked is about a beam anchored at angle $\theta$ to two perpendicular axes (at points a and b). The beam slides along them at a constant speed $-V$ in the $x$ direction and $V_b$ in the $y$ direction. The goal is to solve for $V_b$ in terms of $\theta$ and $V$ . I solved this equation in two different ways, the first using related rates and the second using a differential equation, and don't understand why they yield different results, and I was hoping someone could shed some light on it for me, as I think I violated some mathematical rule when solving with related rates. Solve attempt 1 using related rates: \begin{align*} \frac{\mathrm{d}x}{\mathrm{d}t} &= -v\\ \frac{\mathrm{d}y}{\mathrm{d}t} &= v_b\\ y &=x\tan\theta\\ \frac{\mathrm{d}y}{\mathrm{d}x} &=\tan\theta\\ \frac{\mathrm{d}y}{\mathrm{d}t} &=\frac{\mathrm{d}y}{\mathrm{d}x}\frac{\mathrm{d}x}{\mathrm{d}t}\\ \frac{\mathrm{d}y}{\mathrm{d}t} &=-v\tan\theta\\ v_b &=-v\tan\theta \end{align*} Solve attempt 2 using differential equations (where $L$ is the beam) \begin{align*} x^2+y^2 &=L^2\\ 2x\frac{\mathrm{d}x}{\mathrm{d}t}+2y\frac{\mathrm{d}y}{\mathrm{d}t} &=0\\ -2xv+2yv_b &=0\\ v_b &=\frac{x}{y}v\\ v_b &=\frac{v}{\tan\theta} \end{align*}","Sorry for the figure being so large I was unsure how to shrink it. The question asked is about a beam anchored at angle to two perpendicular axes (at points a and b). The beam slides along them at a constant speed in the direction and in the direction. The goal is to solve for in terms of and . I solved this equation in two different ways, the first using related rates and the second using a differential equation, and don't understand why they yield different results, and I was hoping someone could shed some light on it for me, as I think I violated some mathematical rule when solving with related rates. Solve attempt 1 using related rates: Solve attempt 2 using differential equations (where is the beam)","\theta -V x V_b y V_b \theta V \begin{align*}
\frac{\mathrm{d}x}{\mathrm{d}t} &= -v\\
\frac{\mathrm{d}y}{\mathrm{d}t} &= v_b\\
y &=x\tan\theta\\
\frac{\mathrm{d}y}{\mathrm{d}x} &=\tan\theta\\
\frac{\mathrm{d}y}{\mathrm{d}t} &=\frac{\mathrm{d}y}{\mathrm{d}x}\frac{\mathrm{d}x}{\mathrm{d}t}\\
\frac{\mathrm{d}y}{\mathrm{d}t} &=-v\tan\theta\\
v_b &=-v\tan\theta
\end{align*} L \begin{align*}
x^2+y^2 &=L^2\\
2x\frac{\mathrm{d}x}{\mathrm{d}t}+2y\frac{\mathrm{d}y}{\mathrm{d}t} &=0\\
-2xv+2yv_b &=0\\
v_b &=\frac{x}{y}v\\
v_b &=\frac{v}{\tan\theta}
\end{align*}","['ordinary-differential-equations', 'derivatives', 'related-rates']"
43,"Consider the ODE $y'(x)=a(x)y(x),\; x>0, \;y(0)=y_{0}\neq 0$. Which of the following statements are true?",Consider the ODE . Which of the following statements are true?,"y'(x)=a(x)y(x),\; x>0, \;y(0)=y_{0}\neq 0","Assume that $a:[0,\infty) \to \mathbb{R}$ is a continuous function. Consider the ODE $$y'(x)=a(x)y(x),\quad x>0, \;y(0)=y_0 \neq 0$$ Which of the following statements are true? If $\int_{0}^{\infty}\vert a(x)\vert dx<\infty$ , then $y$ is bounded. If $\int_{0}^{\infty}\vert a(x)\vert dx<\infty$ , then $\lim_{x \to \infty}y(x)$ exists. If $\lim_{ x \to \infty}a(x)=1$ , then $\lim_{x \to \infty}\vert y(x) \vert=\infty$ If $\lim_{ x \to \infty}a(x)=1$ , then $y$ is monotone. My attempt: $f(x,y)=a(x)y(x)$ then $f(x,y)$ is continuous and $\frac{\partial f}{\partial y}=a(x)$ which is continuous at $(0,y_{0})$ . So, it follows that there exists unique solution using Picard uniqueness theorem. I didn't get how to explicitly solve for option 1 and 2. Also for option 3, i solved through an example. So, if $\lim_{ x \to \infty}a(x)=1$ , then choose $a(x)=1$ , clearly satisfies the hypothesis. Then the solution is $y=y_{0}e^x$ and it satisfies options $3$ and $4$ . But can someone please show me how to work on these questions explicitly, without using some examples etc? Thanks in advance. (Also can someone please edit the tags, there does not exists tag for uniqueness-existence of solution and the famous picard's theorem for existsence and uniqueness).","Assume that is a continuous function. Consider the ODE Which of the following statements are true? If , then is bounded. If , then exists. If , then If , then is monotone. My attempt: then is continuous and which is continuous at . So, it follows that there exists unique solution using Picard uniqueness theorem. I didn't get how to explicitly solve for option 1 and 2. Also for option 3, i solved through an example. So, if , then choose , clearly satisfies the hypothesis. Then the solution is and it satisfies options and . But can someone please show me how to work on these questions explicitly, without using some examples etc? Thanks in advance. (Also can someone please edit the tags, there does not exists tag for uniqueness-existence of solution and the famous picard's theorem for existsence and uniqueness).","a:[0,\infty) \to \mathbb{R} y'(x)=a(x)y(x),\quad x>0, \;y(0)=y_0 \neq 0 \int_{0}^{\infty}\vert a(x)\vert dx<\infty y \int_{0}^{\infty}\vert a(x)\vert dx<\infty \lim_{x \to \infty}y(x) \lim_{ x \to \infty}a(x)=1 \lim_{x \to \infty}\vert y(x) \vert=\infty \lim_{ x \to \infty}a(x)=1 y f(x,y)=a(x)y(x) f(x,y) \frac{\partial f}{\partial y}=a(x) (0,y_{0}) \lim_{ x \to \infty}a(x)=1 a(x)=1 y=y_{0}e^x 3 4",['ordinary-differential-equations']
44,Observability of a Matrix Pair,Observability of a Matrix Pair,,"Let $A\in\mathbb{R}^{m \times m}$ and $C\in\mathbb{R}^{n\times m}$ . The pair $(A,C)$ is observable if $Cx \ne 0$ for every right eigenvector $x$ of $A$ . Therefore, if the pair $(A,C)$ is NOT observable, then \begin{align} 	\exists \lambda \in \mathbb{C}, x \in \mathbb{C}^m, x\ne 0  \: \text{ such that } Ax = \lambda x \text{ and } Cx = 0. \tag{1} \end{align} In the above case that ( $1$ ) holds, is there any concept or result that study the multiplicity of the eigenvalue $\lambda$ through which the observability is violated? For instance, what is the difference between the case that $\lambda$ in ( $1$ ) is a simple eigenvalue and the case where $\lambda$ is not simple? Is there a way to identify the multiplicity of $\lambda$ for example through observability matrix? Thank you for your comments and thoughts.","Let and . The pair is observable if for every right eigenvector of . Therefore, if the pair is NOT observable, then In the above case that ( ) holds, is there any concept or result that study the multiplicity of the eigenvalue through which the observability is violated? For instance, what is the difference between the case that in ( ) is a simple eigenvalue and the case where is not simple? Is there a way to identify the multiplicity of for example through observability matrix? Thank you for your comments and thoughts.","A\in\mathbb{R}^{m \times m} C\in\mathbb{R}^{n\times m} (A,C) Cx \ne 0 x A (A,C) \begin{align}
	\exists \lambda \in \mathbb{C}, x \in \mathbb{C}^m, x\ne 0  \: \text{ such that } Ax = \lambda x \text{ and } Cx = 0. \tag{1}
\end{align} 1 \lambda \lambda 1 \lambda \lambda","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'linear-control']"
45,solving a particular form of Chini's equation,solving a particular form of Chini's equation,,"Solve the initial value problem $$u''(t)+u'(t)=\sin u(t)$$ with initial conditions $u(0)=1,u'(0)=0$ , and hence show that $u(t),u'(t)$ is bounded for all $t>0$ . Find $\displaystyle\lim_{t\to\infty}u(t)$ . I started with the substitution $u'(t)=p(t)$ . Then by chain rule $\displaystyle{u''(t)=\frac{dp}{dt}=p\frac{dp}{du}}$ . So our equation becomes $$p\frac{dp}{du}+p=\sin u \\ \implies \frac{dp}{du}=-1+\frac{\sin u}{p}$$ which (after a thorough search online) I identified as a particular form of Chini's equation . I calculated the Chini invariant as $C=\sec u$ , which is not independent of $u$ and hence can not be solved easily. I wonder whether this equation is solvable by any form of algebraic transformation or not. In case this equation is not solvable analytically, or at least in some closed form, how to check for boundedness of the solution? Any help is appreciated.","Solve the initial value problem with initial conditions , and hence show that is bounded for all . Find . I started with the substitution . Then by chain rule . So our equation becomes which (after a thorough search online) I identified as a particular form of Chini's equation . I calculated the Chini invariant as , which is not independent of and hence can not be solved easily. I wonder whether this equation is solvable by any form of algebraic transformation or not. In case this equation is not solvable analytically, or at least in some closed form, how to check for boundedness of the solution? Any help is appreciated.","u''(t)+u'(t)=\sin u(t) u(0)=1,u'(0)=0 u(t),u'(t) t>0 \displaystyle\lim_{t\to\infty}u(t) u'(t)=p(t) \displaystyle{u''(t)=\frac{dp}{dt}=p\frac{dp}{du}} p\frac{dp}{du}+p=\sin u \\ \implies \frac{dp}{du}=-1+\frac{\sin u}{p} C=\sec u u",['ordinary-differential-equations']
46,Find number of solutions of an ODE with a property,Find number of solutions of an ODE with a property,,"I'm stucked on the following exercise: How many solution of $x'(t)=x(t) -e ^{-t^2}$ have the following property: $\lim_{t \rightarrow \pm \infty} x(t)=0$ ? I tried to integrate in $[0,t]$ , obtaining $$x(t)= c e^t -  e^{t} \int_0^t e^{-s^2} e^{-s} ds$$ Now I'd like to take the limit, in order to note that $x(t)$ is unbounded, by I don't know how to handle that integral. How can I move?","I'm stucked on the following exercise: How many solution of have the following property: ? I tried to integrate in , obtaining Now I'd like to take the limit, in order to note that is unbounded, by I don't know how to handle that integral. How can I move?","x'(t)=x(t) -e ^{-t^2} \lim_{t \rightarrow \pm \infty} x(t)=0 [0,t] x(t)= c e^t -  e^{t} \int_0^t e^{-s^2} e^{-s} ds x(t)","['integration', 'ordinary-differential-equations']"
47,Solving a non linear differential equation,Solving a non linear differential equation,,"This is a first order differential equation: $$ \frac{df_1}{dx} + \frac{(f_1)^2}{h^2} - \frac{2m}{h^2} \lambda \delta(x-pa)=-\frac{2mE_1}{h^2} $$ Where, h, $\lambda $ and $E_1$ are constants and and pa lies in [0,a] as 0<p<1 . I have not been taught how to handle differential equations with a Dirac Delta function in it. Moreover, this is a non linear one. I came across this in a research paper and the answer is given but the method to solve it isn't. I have tried learning to use Laplace transform to solve this, but got stuck again because I didn't know how to do Laplace transform of the second term of the equation. Any help will be appreciated. Please, help me out. The answer is: $f_1=√2mE_1[cot (\frac{√2mE_1}{h} (x-b))]$ Where b is constant of integration P.s.: I know this might be rude but please don't vote this as a homework question because it isn't one. If you can't help just ignore.","This is a first order differential equation: Where, h, and are constants and and pa lies in [0,a] as 0<p<1 . I have not been taught how to handle differential equations with a Dirac Delta function in it. Moreover, this is a non linear one. I came across this in a research paper and the answer is given but the method to solve it isn't. I have tried learning to use Laplace transform to solve this, but got stuck again because I didn't know how to do Laplace transform of the second term of the equation. Any help will be appreciated. Please, help me out. The answer is: Where b is constant of integration P.s.: I know this might be rude but please don't vote this as a homework question because it isn't one. If you can't help just ignore.","
\frac{df_1}{dx} + \frac{(f_1)^2}{h^2} - \frac{2m}{h^2} \lambda \delta(x-pa)=-\frac{2mE_1}{h^2}
 \lambda  E_1 f_1=√2mE_1[cot (\frac{√2mE_1}{h} (x-b))]","['ordinary-differential-equations', 'laplace-transform', 'dirac-delta', 'nonlinear-analysis']"
48,"""Differential Operator"" Over Polynomial Space","""Differential Operator"" Over Polynomial Space",,"Let's suppose we are given the differential operator $T \colon \mathcal{P}_2(\mathbb{C}) \longrightarrow \mathcal{P}_3(\mathbb{C})$ , over the space of quadratic polynomials with complex coefficients, such that $T(p(t)) := p(t) + t^2\,p'(t)$ , and we are asked to find it's kernel. Of course, setting $p(t) := a_0 + a_1\,t + a_2\,t^2$ , where $a_0,\,a_1,\,a_2 \in \mathbb{C}$ , one can easily find that $\text{Ker}(T) = \left\{0\right\}$ , the zero polynomial, with the polynomial equality. But, can we actualy solve the equation $p(t) + t^2\,p'(t) = 0$ ? I know we're going to find an exponential solution of the form $k\exp(1/t)$ , but can we take $k = 0$ (and hence $p(t) = 0$ ) to solve this problem? Thanks in advance!","Let's suppose we are given the differential operator , over the space of quadratic polynomials with complex coefficients, such that , and we are asked to find it's kernel. Of course, setting , where , one can easily find that , the zero polynomial, with the polynomial equality. But, can we actualy solve the equation ? I know we're going to find an exponential solution of the form , but can we take (and hence ) to solve this problem? Thanks in advance!","T \colon \mathcal{P}_2(\mathbb{C}) \longrightarrow \mathcal{P}_3(\mathbb{C}) T(p(t)) := p(t) + t^2\,p'(t) p(t) := a_0 + a_1\,t + a_2\,t^2 a_0,\,a_1,\,a_2 \in \mathbb{C} \text{Ker}(T) = \left\{0\right\} p(t) + t^2\,p'(t) = 0 k\exp(1/t) k = 0 p(t) = 0","['linear-algebra', 'ordinary-differential-equations', 'linear-transformations', 'differential-operators']"
49,For which $a$ the solution is defined on the interval,For which  the solution is defined on the interval,a,"For which $a$ the solution of $$\begin{cases} \frac{z'}{z^2}= e^{-x^2} \\ z(0)=a \end{cases}$$ is defined on the interval $[0,\infty)$ My try: $$\frac{z'}{z^2}= e^{-x^2}$$ $$\int \frac{z'}{z^2} \,dx=\int e^{-x^2}\,dx$$ $$-\frac{1}{z(x)}+C=\int e^{-x^2}\,dx$$ $$z(x)=\frac{1}{\frac 1a -\int e^{-x^2}\,dx}$$ I think that this equation has the solution when exist $z'$ . $z'(x)=\frac{e^{-x^2}}{(\frac 1a - \int e^{-x^2}\,dx)^2}$ so exist when $(\frac 1a - \int e^{-x^2}\,dx)$ doesn't converge  to $0$ However, I do not know how it matters for the range given in the task. Can you help me finish this?","For which the solution of is defined on the interval My try: I think that this equation has the solution when exist . so exist when doesn't converge  to However, I do not know how it matters for the range given in the task. Can you help me finish this?","a \begin{cases} \frac{z'}{z^2}= e^{-x^2} \\ z(0)=a \end{cases} [0,\infty) \frac{z'}{z^2}= e^{-x^2} \int \frac{z'}{z^2} \,dx=\int e^{-x^2}\,dx -\frac{1}{z(x)}+C=\int e^{-x^2}\,dx z(x)=\frac{1}{\frac 1a -\int e^{-x^2}\,dx} z' z'(x)=\frac{e^{-x^2}}{(\frac 1a - \int e^{-x^2}\,dx)^2} (\frac 1a - \int e^{-x^2}\,dx) 0",['ordinary-differential-equations']
50,Any insight into this ODE?,Any insight into this ODE?,,"I wonder if anyone has analytical insights into the following nonlinear ODE $$(y^\prime)^2-{3\over2}y^2=-t^2$$ which occurs in cosmology (so called Hamilton-Jacobi equation). I tried solving this numerically with RK4 (e.g. with some random initial condition, say $f(1)=1$ ). It is numerically sensitive, stemming from having to take the square-root (one can assume, say, that $y>0$ and $y^\prime>0$ ). Does anyone have any analytical insights into simplifying this ODE, e.g., some substitution? I'd very much appreciate your input. Thank you.","I wonder if anyone has analytical insights into the following nonlinear ODE which occurs in cosmology (so called Hamilton-Jacobi equation). I tried solving this numerically with RK4 (e.g. with some random initial condition, say ). It is numerically sensitive, stemming from having to take the square-root (one can assume, say, that and ). Does anyone have any analytical insights into simplifying this ODE, e.g., some substitution? I'd very much appreciate your input. Thank you.",(y^\prime)^2-{3\over2}y^2=-t^2 f(1)=1 y>0 y^\prime>0,"['ordinary-differential-equations', 'nonlinear-dynamics']"
51,Can a trajectory pass through a critical point for a plane autonomous system? (Differential equations),Can a trajectory pass through a critical point for a plane autonomous system? (Differential equations),,"Set up: I am considering a plane autonomous system where there exists two ODEs, $\frac{dx}{dt}=X(x,y),\frac{dy}{dt}=Y(x,y)$ . We then usually draw trajectories on the phase plane to indicate the solutions on the plane. Question : I often see trajectories sometimes cross at critical points, however I was wondering, if a trajectory does contain a critical point, wouldn't this trajectory just stay at that critical point rather than passing through that critical point since at that point $X=Y=0$ and so just becomes stationary? I am not too sure if I made much sense with my question but any help is appreciated!","Set up: I am considering a plane autonomous system where there exists two ODEs, . We then usually draw trajectories on the phase plane to indicate the solutions on the plane. Question : I often see trajectories sometimes cross at critical points, however I was wondering, if a trajectory does contain a critical point, wouldn't this trajectory just stay at that critical point rather than passing through that critical point since at that point and so just becomes stationary? I am not too sure if I made much sense with my question but any help is appreciated!","\frac{dx}{dt}=X(x,y),\frac{dy}{dt}=Y(x,y) X=Y=0","['ordinary-differential-equations', 'stationary-point']"
52,Commutation relation between covariant and Lie derivatives,Commutation relation between covariant and Lie derivatives,,"I am currently working on extrinsic riemannian geometry and I am looking for a sort of commutation relation between the covariant and Lie derivatives. To be more precise : considering an hypersurface $H \subset M$ of a riemannian manifold, $\nu$ a vector field normal to $H$ and $S$ its shape operator (or Wiengarten operator ) defined by $SX = \nabla_X \nu$ , you can consider normal geodesics emanating from $H$ as geodesics veryfing $\gamma(0) \in H$ , $\dot\gamma(0) = \nu$ . Writing the parameters of these geodesics $r$ , you get a vector field $\partial_r = \dot\gamma$ . If $(x^1,\ldots,x^n)$ are local coordinates on $H$ , then you have Fermi coordinates $(r,x^1,\ldots,x^n)$ on $M$ . We have the Ricatti equation, where $R_{\partial_r} = R(\partial_r,\cdot)\partial_r$ : \begin{align*} \mathcal{L}_{\partial_r}S=\partial_r S = -S^2 - R_{\partial_r} \end{align*} (in fact, the equation is still true while replacing $\mathcal{L}_{\partial_r}$ by $\nabla_{\partial_r}$ , it's a property of the shape operator). I want to find a differential equation for $\nabla_{\partial_j}S$ where $\partial_j = \frac{\partial}{\partial x^j}$ . My idea is to differentiate the Ricatti equation with respect to $\nabla_{\partial_j}$ and use a sort of commutation relation to get a differential equation involving $S$ , $\nabla_{\partial_j}S$ , $R$ , etc . with variable $r$ . So, my question is : do we have a nice relation between $\nabla_{\partial_j} \mathcal{L}_{\partial_r} S$ and $\mathcal{L}_{\partial_r}\nabla_{\partial_j}S$ ? Thank you for reading me.","I am currently working on extrinsic riemannian geometry and I am looking for a sort of commutation relation between the covariant and Lie derivatives. To be more precise : considering an hypersurface of a riemannian manifold, a vector field normal to and its shape operator (or Wiengarten operator ) defined by , you can consider normal geodesics emanating from as geodesics veryfing , . Writing the parameters of these geodesics , you get a vector field . If are local coordinates on , then you have Fermi coordinates on . We have the Ricatti equation, where : (in fact, the equation is still true while replacing by , it's a property of the shape operator). I want to find a differential equation for where . My idea is to differentiate the Ricatti equation with respect to and use a sort of commutation relation to get a differential equation involving , , , etc . with variable . So, my question is : do we have a nice relation between and ? Thank you for reading me.","H \subset M \nu H S SX = \nabla_X \nu H \gamma(0) \in H \dot\gamma(0) = \nu r \partial_r = \dot\gamma (x^1,\ldots,x^n) H (r,x^1,\ldots,x^n) M R_{\partial_r} = R(\partial_r,\cdot)\partial_r \begin{align*}
\mathcal{L}_{\partial_r}S=\partial_r S = -S^2 - R_{\partial_r}
\end{align*} \mathcal{L}_{\partial_r} \nabla_{\partial_r} \nabla_{\partial_j}S \partial_j = \frac{\partial}{\partial x^j} \nabla_{\partial_j} S \nabla_{\partial_j}S R r \nabla_{\partial_j} \mathcal{L}_{\partial_r} S \mathcal{L}_{\partial_r}\nabla_{\partial_j}S","['ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry']"
53,Are there solutions to this coupled system of ODE's?,Are there solutions to this coupled system of ODE's?,,"While working on a variational problem, I have reached to the following question. Let $0<\lambda < \frac{1}{2}$ be a parameter. Are there smooth strictly increasing surjective maps $\phi:[0,1] \to [0,\lambda]$ satisfying $\phi(0)=0$ and the coupled system of ODE's given by $$\phi'=\frac{1}{2}\big(1 + \sqrt{1-4\frac{\phi \phi'}{r}}),\frac{\phi}{r}=\frac{1}{2}\big(1 - \sqrt{1-4\frac{\phi \phi'}{r}})\,\,\,\,\,?$$ Note that taking the product of two these equations result in a consistent tautology. In particular, I want the ODE to be defined everywhere, i.e. $\frac{\phi \phi'}{r} \le \frac{1}{4}$ for every $r$ . Are there such solutions $\phi$ that also satisfy $\phi^{2k}(0)=0$ for every natural $k$ ? Note that for the crucial value $\lambda=\frac{1}{2}$ (which is out of scope here), $\phi(r)=\lambda r$ does the job. However, $r \mapsto \lambda r$ does not satisfy the ODE for $\lambda < \frac{1}{2}$ .","While working on a variational problem, I have reached to the following question. Let be a parameter. Are there smooth strictly increasing surjective maps satisfying and the coupled system of ODE's given by Note that taking the product of two these equations result in a consistent tautology. In particular, I want the ODE to be defined everywhere, i.e. for every . Are there such solutions that also satisfy for every natural ? Note that for the crucial value (which is out of scope here), does the job. However, does not satisfy the ODE for .","0<\lambda < \frac{1}{2} \phi:[0,1] \to [0,\lambda] \phi(0)=0 \phi'=\frac{1}{2}\big(1 + \sqrt{1-4\frac{\phi \phi'}{r}}),\frac{\phi}{r}=\frac{1}{2}\big(1 - \sqrt{1-4\frac{\phi \phi'}{r}})\,\,\,\,\,? \frac{\phi \phi'}{r} \le \frac{1}{4} r \phi \phi^{2k}(0)=0 k \lambda=\frac{1}{2} \phi(r)=\lambda r r \mapsto \lambda r \lambda < \frac{1}{2}","['real-analysis', 'calculus', 'ordinary-differential-equations', 'symmetry']"
54,How does the author derive these relations?,How does the author derive these relations?,,"I am still stuck to the differential equation: $$r(r-1)\partial_r^2f+\partial_rf-\left[  \dfrac{\rho^2r^3}{r-1}+l(l+1)-\dfrac{3}{r} \right]f=0$$ According to the author of this paper, the solution has the form $$f(r)=(r-1)^\rho r^{-2\rho} \exp(-\rho(r-1))\sum_n a_n\left( \dfrac{r-1}{r} \right)^n.$$ The author susbtitutes this expression into the diff. eqn. and gets a three term recursion relation: $$\alpha_0 a_1+\beta_0 a_0=0$$ $$\alpha_n a_{n+1}+\beta_n a_n+\gamma_n a_{n-1}=0$$ where the coefficients are $$\alpha_n=n^2+(2+2\rho)n+2\rho+1$$ $$\beta_n=-(n^2+(8\rho+2)n+8\rho^2+4\rho+l(l+1)-3)$$ $$\gamma_n=n^2+4\rho n+4\rho^2 -4$$ What I do not understand is how the author derived these three relations, since substituting $f(r)$ into the diff. eqn. I get a three terms recurrence relation, but with different coefficients. I use as $\alpha_n$ etc. the coefficients of the powers of $r$ : is this correct? Could someone show me the correct derivation?","I am still stuck to the differential equation: According to the author of this paper, the solution has the form The author susbtitutes this expression into the diff. eqn. and gets a three term recursion relation: where the coefficients are What I do not understand is how the author derived these three relations, since substituting into the diff. eqn. I get a three terms recurrence relation, but with different coefficients. I use as etc. the coefficients of the powers of : is this correct? Could someone show me the correct derivation?",r(r-1)\partial_r^2f+\partial_rf-\left[  \dfrac{\rho^2r^3}{r-1}+l(l+1)-\dfrac{3}{r} \right]f=0 f(r)=(r-1)^\rho r^{-2\rho} \exp(-\rho(r-1))\sum_n a_n\left( \dfrac{r-1}{r} \right)^n. \alpha_0 a_1+\beta_0 a_0=0 \alpha_n a_{n+1}+\beta_n a_n+\gamma_n a_{n-1}=0 \alpha_n=n^2+(2+2\rho)n+2\rho+1 \beta_n=-(n^2+(8\rho+2)n+8\rho^2+4\rho+l(l+1)-3) \gamma_n=n^2+4\rho n+4\rho^2 -4 f(r) \alpha_n r,"['ordinary-differential-equations', 'recurrence-relations', 'frobenius-method']"
55,A function satisfying a series equation,A function satisfying a series equation,,"We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function $f$ satisfying the series equation $$f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*} $$ The source: I interested to solve the equation $$f'(x)=f(x+1)$$ and I found out $$f^{(n)}(x)=f(x+n)$$ thus: $f^{(n)}(0)=f(n)$ and if we consider $f$ having a taylor series, then the expression $(*)$ will be the case. But I have no idea how $f$ would be. Is such a function exists? Thanks.","We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function satisfying the series equation The source: I interested to solve the equation and I found out thus: and if we consider having a taylor series, then the expression will be the case. But I have no idea how would be. Is such a function exists? Thanks.",f f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*}  f'(x)=f(x+1) f^{(n)}(x)=f(x+n) f^{(n)}(0)=f(n) f (*) f,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'taylor-expansion', 'delay-differential-equations']"
56,Maximum principle of a differential Equation,Maximum principle of a differential Equation,,"Consider  the boundary value problem $$g(x) - g’’(x) = f(x), \qquad x \in (0,1),$$ with boundary conditions $g(0)=g(1)=0$ and $f$ beeing a function in $C([0,1])$ . My question is, how can I show from the information above that $$||g||_{\infty}\le \frac{1}{7}||f||_{\infty},$$ and that this implies that there is an unique solution for $f\in C([0,1])$ ? ( $||\cdot||_{\infty}$ denotes the $\infty$ -norm) I know there is a proposition that applies for the Poisson’s equation on the form $$-g’’(x) = f(x) \quad x \in (0,1) \quad g(0)=g(1)=0$$ That states that $||g||_{\infty}\le \frac{1}{8}||f||_{\infty}$ . I did not however manage to apply this in order to solve the given equation. Anyone who can help me?","Consider  the boundary value problem with boundary conditions and beeing a function in . My question is, how can I show from the information above that and that this implies that there is an unique solution for ? ( denotes the -norm) I know there is a proposition that applies for the Poisson’s equation on the form That states that . I did not however manage to apply this in order to solve the given equation. Anyone who can help me?","g(x) - g’’(x) = f(x), \qquad x \in (0,1), g(0)=g(1)=0 f C([0,1]) ||g||_{\infty}\le \frac{1}{7}||f||_{\infty}, f\in C([0,1]) ||\cdot||_{\infty} \infty -g’’(x) = f(x) \quad x \in (0,1) \quad g(0)=g(1)=0 ||g||_{\infty}\le \frac{1}{8}||f||_{\infty}","['real-analysis', 'ordinary-differential-equations']"
57,How to rearrange a second order differential equation?,How to rearrange a second order differential equation?,,How would I go about rearranging a second order ODE? For instance if I wanted to rearrange the below for $\theta$ how would I go about it? $$\ddot{\theta}+\frac{g}{l}\sin \theta=0$$,How would I go about rearranging a second order ODE? For instance if I wanted to rearrange the below for how would I go about it?,\theta \ddot{\theta}+\frac{g}{l}\sin \theta=0,['ordinary-differential-equations']
58,Differential equation satisfying $f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}$,Differential equation satisfying,f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x},"Find the differential equation satisfying $$f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}$$ It is solved in my reference (also in this video ) as: $$ f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}=\frac{1}{1-\cos^2\theta}=\csc^2\theta\\ f'(\theta)=-2\csc^2\theta\cot\theta\\ f'(\theta)+2f(\theta)\cot\theta=0 $$ As per my knowledge the Leibniz rule is $$ \frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}g(\theta,x)dx=g(\theta,b(\theta))\frac{d}{d\theta}b(\theta)-g(\theta,a(\theta))\frac{d}{d\theta}a(\theta) $$ iff $g(x,\theta)=g(x)$ So isn't it wrong to approach the problem as in my reference ?",Find the differential equation satisfying It is solved in my reference (also in this video ) as: As per my knowledge the Leibniz rule is iff So isn't it wrong to approach the problem as in my reference ?,"f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x} 
f(\theta)=\frac{d}{d\theta}\int_0^\theta\frac{dx}{1-\cos\theta\cos x}=\frac{1}{1-\cos^2\theta}=\csc^2\theta\\
f'(\theta)=-2\csc^2\theta\cot\theta\\
f'(\theta)+2f(\theta)\cot\theta=0
 
\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}g(\theta,x)dx=g(\theta,b(\theta))\frac{d}{d\theta}b(\theta)-g(\theta,a(\theta))\frac{d}{d\theta}a(\theta)
 g(x,\theta)=g(x)","['integration', 'ordinary-differential-equations', 'definite-integrals']"
59,Is $\frac{\Delta y}{\Delta x}$ equal to $f'(x) + \varepsilon$?,Is  equal to ?,\frac{\Delta y}{\Delta x} f'(x) + \varepsilon,"I was trying to understand the relationship between differential and derivative, but I couldn't understand where the first equation comes from. When, $$\Delta y = f(x + \Delta x) - f(x),\ \ \ \ \lim_{\Delta x \to 0} \varepsilon = 0$$ Is this equation true? $$ \frac{\Delta y}{\Delta x} = f'(x) + \varepsilon $$","I was trying to understand the relationship between differential and derivative, but I couldn't understand where the first equation comes from. When, Is this equation true?","\Delta y = f(x + \Delta x) - f(x),\ \ \ \ \lim_{\Delta x \to 0} \varepsilon = 0 
\frac{\Delta y}{\Delta x} = f'(x) + \varepsilon
",['ordinary-differential-equations']
60,Find a function that makes a differential form exact [duplicate],Find a function that makes a differential form exact [duplicate],,"This question already has an answer here : Find a function that makes this differential form exact (1 answer) Closed 4 years ago . We have $A=\mathbb{R^3}\backslash \left\{ (0,0,z):z\in \mathbb{R}\right\}$ and $\omega$ the differential form: $$\omega :=\left(\frac{4x^2+2zx}{x^2+y^2}+2A(x,y)\right)dx+\left(\frac{2y}{x^2+y^2}(2x+z)\right)dy+A(x,y)\,dz,$$ where $A\in C^1(\mathbb{R^2}\backslash(0,0); \mathbb{R})$ 1) Find a function $A$ that makes the differential form exact. 2) Find all the primitives of $\omega$ . My idea was calculating $d\omega=0$ because $\omega$ is closed (necessary condition for exactness), and finding $A(x,y)$ who will depend on a costant $C$ . And then, putting $\int_{\gamma}\omega=0$ in order to prove the exactness of $\omega$ for some curves $\gamma$ .","This question already has an answer here : Find a function that makes this differential form exact (1 answer) Closed 4 years ago . We have and the differential form: where 1) Find a function that makes the differential form exact. 2) Find all the primitives of . My idea was calculating because is closed (necessary condition for exactness), and finding who will depend on a costant . And then, putting in order to prove the exactness of for some curves .","A=\mathbb{R^3}\backslash \left\{ (0,0,z):z\in \mathbb{R}\right\} \omega \omega :=\left(\frac{4x^2+2zx}{x^2+y^2}+2A(x,y)\right)dx+\left(\frac{2y}{x^2+y^2}(2x+z)\right)dy+A(x,y)\,dz, A\in C^1(\mathbb{R^2}\backslash(0,0); \mathbb{R}) A \omega d\omega=0 \omega A(x,y) C \int_{\gamma}\omega=0 \omega \gamma","['calculus', 'ordinary-differential-equations', 'analysis', 'differential-forms']"
61,Calculating differential equation with bessel function in it with ln,Calculating differential equation with bessel function in it with ln,,How can I solve that: $$\frac{d}{dx}\ln \left(\frac{xJ_{-\frac14}\left(\frac{x^2}{2}\right)}{2} \right)$$ without using the fact $$2J^{'}_{\nu}(z)=J_{\nu-1}(z)-J_{\nu+1}(z)$$ only the definition of the Bessel function,How can I solve that: without using the fact only the definition of the Bessel function,\frac{d}{dx}\ln \left(\frac{xJ_{-\frac14}\left(\frac{x^2}{2}\right)}{2} \right) 2J^{'}_{\nu}(z)=J_{\nu-1}(z)-J_{\nu+1}(z),"['ordinary-differential-equations', 'derivatives', 'bessel-functions']"
62,"How to find a solution ""by inspection"" w.r.t. the method of reduction of order for homogeneous linear ODE's?","How to find a solution ""by inspection"" w.r.t. the method of reduction of order for homogeneous linear ODE's?",,"I'm currently studying ODE's and ran into an example problem that I'm having trouble understanding. The problem is from a section dealing with the method of reduction of order and is as follows: Find a basis of solutions for the ODE $$(x^2 - x)y'' - xy' + y = 0$$ I'm aware that reduction of order applies when we know one solution $y_1$ and we find the other by putting $y_2 = uy_1$ where $y_i$ and $u$ are both functions of $x$ . The solution for this ODE simply states that ""by inspection"" we can see that $y_1 = x$ is one solution. Indeed, if we put $y_2 = uy_1 = ux$ then we can solve the ODE, but I'm puzzled as to how one is to find out that $y_1 = x$ is supposedly such an obvious solution? Is there some sort of intuition that I should be accustomed to or are there actual methods? Thanks in advance.","I'm currently studying ODE's and ran into an example problem that I'm having trouble understanding. The problem is from a section dealing with the method of reduction of order and is as follows: Find a basis of solutions for the ODE I'm aware that reduction of order applies when we know one solution and we find the other by putting where and are both functions of . The solution for this ODE simply states that ""by inspection"" we can see that is one solution. Indeed, if we put then we can solve the ODE, but I'm puzzled as to how one is to find out that is supposedly such an obvious solution? Is there some sort of intuition that I should be accustomed to or are there actual methods? Thanks in advance.",(x^2 - x)y'' - xy' + y = 0 y_1 y_2 = uy_1 y_i u x y_1 = x y_2 = uy_1 = ux y_1 = x,['ordinary-differential-equations']
63,Some applications of Gronwall's inequality.,Some applications of Gronwall's inequality.,,"I've been trying to solve this problem: Problem: Lets consider the equation $\dot x = f(x) + g(x)$ , lets suppose that $|f(x)|<1$ and that are some $\epsilon >0$ and $L>0$ such that $|g(x)|\leq \epsilon$ . Lets suppose too, that $|f(x)-f(y)|\leq L|x-y|$ . If $x_{1}$ and $x_{2}$ are two solutions s.t $x_{1}(0) = x_{2}(0)$ , prove that $|x_{1}(t)-x_{2}(t)| \leq \frac{2\epsilon}{L}(e^{Lt}-1)$ . What I've done by now: Well, $$x_{1}(t) = x_{1}(0) + \int_{0}^{t} f(x_{1}(s))+g(x_{1}(s))ds$$ $$x_{2}(t) = x_{2}(0) + \int_{0}^{t} f(x_{2}(s))+g(x_{2}(s))ds$$ Then $x_{1}(t)-x_{2}(t)\leq x_{1}(0)-x_{2}(0) + \int_{0}^{t} f(x_{1}(s))-f(x_{2}(s))+g(x_{1}(s))-g(x_{2}(s))ds$ , and finally, $$|x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} |f(x_{1}(s))-f(x_{2}(s))|+ \int_{0}^{t} |g(x_{1}(s))-g(x_{2}(s))|ds$$ $$|x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} L|x_{1}(s)-x_{2}(s)|ds+ 2\epsilon tds$$ And since $2\epsilon t$ is non-decreasing we get to $$|x_{1}(t)-x_{2}(t)| \leq 2\epsilon t e^{Lt}$$ But I couldnt get a better bound :( Any help is really appreciated, thanks so much :)","I've been trying to solve this problem: Problem: Lets consider the equation , lets suppose that and that are some and such that . Lets suppose too, that . If and are two solutions s.t , prove that . What I've done by now: Well, Then , and finally, And since is non-decreasing we get to But I couldnt get a better bound :( Any help is really appreciated, thanks so much :)",\dot x = f(x) + g(x) |f(x)|<1 \epsilon >0 L>0 |g(x)|\leq \epsilon |f(x)-f(y)|\leq L|x-y| x_{1} x_{2} x_{1}(0) = x_{2}(0) |x_{1}(t)-x_{2}(t)| \leq \frac{2\epsilon}{L}(e^{Lt}-1) x_{1}(t) = x_{1}(0) + \int_{0}^{t} f(x_{1}(s))+g(x_{1}(s))ds x_{2}(t) = x_{2}(0) + \int_{0}^{t} f(x_{2}(s))+g(x_{2}(s))ds x_{1}(t)-x_{2}(t)\leq x_{1}(0)-x_{2}(0) + \int_{0}^{t} f(x_{1}(s))-f(x_{2}(s))+g(x_{1}(s))-g(x_{2}(s))ds |x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} |f(x_{1}(s))-f(x_{2}(s))|+ \int_{0}^{t} |g(x_{1}(s))-g(x_{2}(s))|ds |x_{1}(t)-x_{2}(t)|\leq \int_{0}^{t} L|x_{1}(s)-x_{2}(s)|ds+ 2\epsilon tds 2\epsilon t |x_{1}(t)-x_{2}(t)| \leq 2\epsilon t e^{Lt},"['calculus', 'ordinary-differential-equations']"
64,Solve $2xyy^\prime=x^2+2y^2$,Solve,2xyy^\prime=x^2+2y^2,Solve $2xyy^\prime=x^2+2y^2$ I tried to solve with substitution $v=y^2$ then $\frac{dy}{dx}=\frac{1}{2}v^{-1/2}\frac{dv}{dx}$ and substituting the given equation I get $$\frac{x^2+v}{2x\sqrt{v}}=\frac{1}{2\sqrt{v}}\frac{dv}{dx}$$ $$\frac{x^2+v}{x}=\frac{dv}{dx}$$ $$\frac{dv}{dx}=x+\frac{v}{x}$$ But now it seems like I just need to make another substitution to solve this.,Solve I tried to solve with substitution then and substituting the given equation I get But now it seems like I just need to make another substitution to solve this.,2xyy^\prime=x^2+2y^2 v=y^2 \frac{dy}{dx}=\frac{1}{2}v^{-1/2}\frac{dv}{dx} \frac{x^2+v}{2x\sqrt{v}}=\frac{1}{2\sqrt{v}}\frac{dv}{dx} \frac{x^2+v}{x}=\frac{dv}{dx} \frac{dv}{dx}=x+\frac{v}{x},['ordinary-differential-equations']
65,Show that $x(t)<0$ for all $t>0$,Show that  for all,x(t)<0 t>0,"Show that if a function $x(t)$ satisfies $0\leq \frac{dx}{ dt}\leq x^2$ for all $t$ , and $x(0) = -1$ , then $x(t) < 0$ for all $t \in [0,\infty)$ . I tried integrating by separation of variables but I'm not sure if I am allowed to do this since there's a discontinuity at $0$ .","Show that if a function satisfies for all , and , then for all . I tried integrating by separation of variables but I'm not sure if I am allowed to do this since there's a discontinuity at .","x(t) 0\leq \frac{dx}{
dt}\leq x^2 t x(0) = -1 x(t) < 0 t \in [0,\infty) 0",['ordinary-differential-equations']
66,"Given the following Riccati equation and the 4 solutions $y_{1}$ , $y_{2}$ , $y_{3}$ , $y_{4}$ . prove that the following expression is constant.","Given the following Riccati equation and the 4 solutions  ,  ,  ,  . prove that the following expression is constant.",y_{1} y_{2} y_{3} y_{4},"Riccati equation: $$y'(x)=c(x)+b(x)\,y(x)+a(x)\,y^{2}(x)$$ with the solutions: $y_{1}$ , $y_{2}$ , $y_{3}$ , $y_{4}$ Prove that $q$ is constant: $$ q=\frac{\ (y_{3} -y_{1})(y_{4} - y_{2})}{(y_{3}-y_{2})(y_{4}-y_{1})}$$","Riccati equation: with the solutions: , , , Prove that is constant:","y'(x)=c(x)+b(x)\,y(x)+a(x)\,y^{2}(x) y_{1} y_{2} y_{3} y_{4} q  q=\frac{\ (y_{3} -y_{1})(y_{4} - y_{2})}{(y_{3}-y_{2})(y_{4}-y_{1})}",['ordinary-differential-equations']
67,Solve differential equation $(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2}$,Solve differential equation,(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2},Help me to solve differential equation: $$(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2}$$ I can't think of a way to substitute a variable,Help me to solve differential equation: I can't think of a way to substitute a variable,(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2},['ordinary-differential-equations']
68,Numerical method for steady-state solution to viscous Burgers' equation,Numerical method for steady-state solution to viscous Burgers' equation,,"I am reading a paper in which a specific partial differential equation (PDE)  on the space-time domain $[-1,1]\times[0,\infty)$ is studied. The authors are  interested in the steady-state solution. They design a finite difference method (FDM) for the PDE. As usual, there are certain discretizations in time-space, $U_j^n$ , that approximate the solution $u$ at the mesh points, $u(x_j,t_n)$ . The  authors conduct the FDM method on $[-1,1]\times [0,T]$ , for $T$ sufficiently large  such that $$ \left|\frac{U_j^N-U_j^{N-1}}{\Delta t}\right|<10^{-12},\quad \forall j, $$ where $t_N=T$ is the last point in the time mesh and $\Delta t$ is the  distance between the points in the time mesh. The approximations for the steady-state  solution are given by $\{U_j^N\}_j$ . I wonder why the authors rely on the PDE to study the steady-state solution. As  far as I know, the steady-state solution comes from equating the derivatives  with respect to time to $0$ in the PDE. The remaining equation is thus an ordinary  differential equation (ODE) in space. To approximate the steady-state solution,  one just needs to design a FDM for this ODE, which is easier than dealing with  the PDE for sure. Is there anything I am not understanding properly? For completeness, I am referring to the paper Supersensitivity due to  uncertain boundary conditions . The authors deal with the PDE $u_t+uu_x=\nu u_{xx}$ , $x\in (-1,1)$ , $u( -1,t)=1+\delta$ , $u(1,t)=-1$ , where $\nu,\delta>0$ . They employ a FDM for this  PDE for large times until the steady-state is reached. Why not considering the  ODE $uu'=\nu u''$ , $u(-1)=1+\delta$ , $u(1)=-1$ , instead?","I am reading a paper in which a specific partial differential equation (PDE)  on the space-time domain is studied. The authors are  interested in the steady-state solution. They design a finite difference method (FDM) for the PDE. As usual, there are certain discretizations in time-space, , that approximate the solution at the mesh points, . The  authors conduct the FDM method on , for sufficiently large  such that where is the last point in the time mesh and is the  distance between the points in the time mesh. The approximations for the steady-state  solution are given by . I wonder why the authors rely on the PDE to study the steady-state solution. As  far as I know, the steady-state solution comes from equating the derivatives  with respect to time to in the PDE. The remaining equation is thus an ordinary  differential equation (ODE) in space. To approximate the steady-state solution,  one just needs to design a FDM for this ODE, which is easier than dealing with  the PDE for sure. Is there anything I am not understanding properly? For completeness, I am referring to the paper Supersensitivity due to  uncertain boundary conditions . The authors deal with the PDE , , , , where . They employ a FDM for this  PDE for large times until the steady-state is reached. Why not considering the  ODE , , , instead?","[-1,1]\times[0,\infty) U_j^n u u(x_j,t_n) [-1,1]\times [0,T] T  \left|\frac{U_j^N-U_j^{N-1}}{\Delta t}\right|<10^{-12},\quad \forall j,  t_N=T \Delta t \{U_j^N\}_j 0 u_t+uu_x=\nu u_{xx} x\in (-1,1) u(
-1,t)=1+\delta u(1,t)=-1 \nu,\delta>0 uu'=\nu u'' u(-1)=1+\delta u(1)=-1","['ordinary-differential-equations', 'partial-differential-equations']"
69,Why is the solution to $\dot x = \sin x$ written in $t = ... $ form?,Why is the solution to  written in  form?,\dot x = \sin x t = ... ,"I'm a high school senior currently taking AP Calculus AB. I'm trying to learn a little about dynamical systems in my free time (yes, I know the pre-requisites are supposed to be multi-variable calculus and linear algebra, but I was eager!) Basically, I'm starting Strogatz's Nonlinear Dynamics and Chaos and came across the equation $\dot x = \sin x$ , which I presume means the same thing as $x'(t)=\sin(x(t))$ since my understanding is that $x$ is a function of $t$ . The separation of variables seems pretty straightforward -- you get $t = -\ln|\csc x + \cot x| + C$ . I agree with Strogatz on that much. But then I wondered -- wasn't a solution to a differential equation supposed to be a function $x(t)$ , not a variable $t$ ? What the heck does it even mean to have a solution expressed as $t = ...$ ? Indeed, when I told Wolfram Alpha to solve $x'(t) = \sin(x(t))$ , it returned $x(t) = 2\cot^{-1}(e^{c_1 - t})$ -- quite different from the solution given above, although I have no idea how Wolfram Alpha got it or how their process differred from Strogatz's. Below is an image of the Strogatz page in question (section 2.1). Having not understood why solving for $t$ was useful, I am of course completely (even more) lost at the part where he says $x = x_0$ and does some stuff with initial conditions or whatever. Basically, isn't the point of solving a differential equation generally to find the function $x(t)$ that satisfies it? So why isn't Strogatz doing that here, and what else is it that he's doing instead?","I'm a high school senior currently taking AP Calculus AB. I'm trying to learn a little about dynamical systems in my free time (yes, I know the pre-requisites are supposed to be multi-variable calculus and linear algebra, but I was eager!) Basically, I'm starting Strogatz's Nonlinear Dynamics and Chaos and came across the equation , which I presume means the same thing as since my understanding is that is a function of . The separation of variables seems pretty straightforward -- you get . I agree with Strogatz on that much. But then I wondered -- wasn't a solution to a differential equation supposed to be a function , not a variable ? What the heck does it even mean to have a solution expressed as ? Indeed, when I told Wolfram Alpha to solve , it returned -- quite different from the solution given above, although I have no idea how Wolfram Alpha got it or how their process differred from Strogatz's. Below is an image of the Strogatz page in question (section 2.1). Having not understood why solving for was useful, I am of course completely (even more) lost at the part where he says and does some stuff with initial conditions or whatever. Basically, isn't the point of solving a differential equation generally to find the function that satisfies it? So why isn't Strogatz doing that here, and what else is it that he's doing instead?",\dot x = \sin x x'(t)=\sin(x(t)) x t t = -\ln|\csc x + \cot x| + C x(t) t t = ... x'(t) = \sin(x(t)) x(t) = 2\cot^{-1}(e^{c_1 - t}) t x = x_0 x(t),"['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
70,"Solve $(A- \operatorname{diag}(x) ) \,\nabla_x f(x) - c f(x)=0, \, f(0)=1$",Solve,"(A- \operatorname{diag}(x) ) \,\nabla_x f(x) - c f(x)=0, \, f(0)=1","Let $f : \mathbb{R}^n \to \mathbb{R}$ . How to solve the following differential equation $$ (A-  \operatorname{diag}(x) ) \nabla_x f(x) - c f(x)=0,  \qquad  f(0)=1. $$ where $\operatorname{diag}(x)$ is a diagonal matrix with vector $x$ on the main diagonal, $A$ is some $n \times n$ matrix  and $ c \in \mathbb{R}^n$ ? In the scalar case, this is easy to solve since it is a first-order linear ordinary differential equation (ODE) whose solution is given by $$f(x) = a^{c} (a -  x)^{-c}.$$ I don't have have much experience solving matrix differential equations and would appreciate some references on this topic. Edit: Using the approach of NN2 the above PDE can be reformulated as: \begin{align} (A-  \operatorname{diag}(x) )  \nabla_x g(x)=c,  \qquad g(0)=0. \end{align}","Let . How to solve the following differential equation where is a diagonal matrix with vector on the main diagonal, is some matrix  and ? In the scalar case, this is easy to solve since it is a first-order linear ordinary differential equation (ODE) whose solution is given by I don't have have much experience solving matrix differential equations and would appreciate some references on this topic. Edit: Using the approach of NN2 the above PDE can be reformulated as:","f : \mathbb{R}^n \to \mathbb{R} 
(A-  \operatorname{diag}(x) ) \nabla_x f(x) - c f(x)=0,  \qquad  f(0)=1.
 \operatorname{diag}(x) x A n \times n  c \in \mathbb{R}^n f(x) = a^{c} (a -  x)^{-c}. \begin{align}
(A-  \operatorname{diag}(x) )  \nabla_x g(x)=c,  \qquad g(0)=0.
\end{align}","['linear-algebra', 'ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'vector-analysis']"
71,"differential equation, Fourier coefficients are equal","differential equation, Fourier coefficients are equal",,"Consider the differential equation: $$(2-\cos(\pi x))y''+y=1, -1\le x \le 1, y(-1)=y(1).$$ Let the Fourier series of $y$ be $y(x)=\displaystyle\sum_{n=-\infty}^{n=\infty}\hat{y}_ne^{i\pi n x}$ , where $\hat{y}_n=\frac{1}{2}\displaystyle\int_{-1}^{1}y(x)e^{-i\pi n x} dx$ . I need to prove that $\hat{y}_n=\hat{y}_{-n}$ . My attempts: Another way of showing that $\hat{y}_n=\hat{y}_{-n}$ , is to show that the imaginary part of $\hat{y}_n$ is zero, or equivalently that $\displaystyle\int_{-1}^{1}y(x)\sin{(\pi n x)} dx=0$ . By substituting the Fourier series of $y$ into the differential equation we get: $$-2 \pi^2 n^2 \hat{y}_n+\hat{y}_n+\pi^2(\frac{(n-1)^2}{2}\hat{y}_{n-1}+\frac{(n+1)^2}{2}\hat{y}_{n+1})=0$$ which can be used to prove recursively that $\hat{y}_n=\hat{y}_{-n}$ , if we know that $\hat{y}_1=\hat{y}_{-1}$ for example. Would anyone please help me solving this. Thank you.","Consider the differential equation: Let the Fourier series of be , where . I need to prove that . My attempts: Another way of showing that , is to show that the imaginary part of is zero, or equivalently that . By substituting the Fourier series of into the differential equation we get: which can be used to prove recursively that , if we know that for example. Would anyone please help me solving this. Thank you.","(2-\cos(\pi x))y''+y=1, -1\le x \le 1, y(-1)=y(1). y y(x)=\displaystyle\sum_{n=-\infty}^{n=\infty}\hat{y}_ne^{i\pi n x} \hat{y}_n=\frac{1}{2}\displaystyle\int_{-1}^{1}y(x)e^{-i\pi n x} dx \hat{y}_n=\hat{y}_{-n} \hat{y}_n=\hat{y}_{-n} \hat{y}_n \displaystyle\int_{-1}^{1}y(x)\sin{(\pi n x)} dx=0 y -2 \pi^2 n^2 \hat{y}_n+\hat{y}_n+\pi^2(\frac{(n-1)^2}{2}\hat{y}_{n-1}+\frac{(n+1)^2}{2}\hat{y}_{n+1})=0 \hat{y}_n=\hat{y}_{-n} \hat{y}_1=\hat{y}_{-1}","['ordinary-differential-equations', 'fourier-analysis']"
72,Differential equation for quantity of salt at a certain time,Differential equation for quantity of salt at a certain time,,"Hi I'm having a lot of trouble with the following problem, I tried using the equation dQ/dt = 3Q/2006-3t but it doesn't work. A tank contains 90 kg of salt and 2000 L of water. Pure water enters a tank at the rate 6 L/min. The solution is mixed and drains from the tank at the rate 3 L/min. How much salt is left in the tank after 5 hours. Thank you","Hi I'm having a lot of trouble with the following problem, I tried using the equation dQ/dt = 3Q/2006-3t but it doesn't work. A tank contains 90 kg of salt and 2000 L of water. Pure water enters a tank at the rate 6 L/min. The solution is mixed and drains from the tank at the rate 3 L/min. How much salt is left in the tank after 5 hours. Thank you",,"['integration', 'ordinary-differential-equations', 'derivatives']"
73,Dynamical Systems / Differentiatial Equations Problem [closed],Dynamical Systems / Differentiatial Equations Problem [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I need help with my Dynamical systems homework, I honestly don't even really know where to start on it. Any help would be appreciated Show that if $x(t)$ is a solution to $x'(t) = x(t)^2 - 1 ,$ then so is $z(t) = -x(t)^{-1}$ and $y(t) = -x(-t).$ What type of symmetries are these and how are they reflected in the phase plane?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I need help with my Dynamical systems homework, I honestly don't even really know where to start on it. Any help would be appreciated Show that if is a solution to then so is and What type of symmetries are these and how are they reflected in the phase plane?","x(t) x'(t) = x(t)^2 - 1 , z(t) = -x(t)^{-1} y(t) = -x(-t).","['ordinary-differential-equations', 'dynamical-systems']"
74,Find solution for $|x| + |y| \frac{dy}{dx}=0$,Find solution for,|x| + |y| \frac{dy}{dx}=0,"I want to solve following differential equation $|x| + |y| \frac{dy}{dx}=0$ with initial condition $y(2)=-1$ . @Robert Z,  since the it pass through $(2,-1)$ \begin{align}   x - y \frac{dy}{dx}=0  \end{align} \begin{align}    x dx = y dy   \end{align} with the initial condition $y(2)=-1$ ,  I have $y^2 = x^2 - 3 $ . so \begin{align} y= - \sqrt{x^2-3} \end{align}","I want to solve following differential equation with initial condition . @Robert Z,  since the it pass through with the initial condition ,  I have . so","|x| + |y| \frac{dy}{dx}=0 y(2)=-1 (2,-1) \begin{align}
  x - y \frac{dy}{dx}=0 
\end{align} \begin{align}
   x dx = y dy 
 \end{align} y(2)=-1 y^2 = x^2 - 3  \begin{align}
y= - \sqrt{x^2-3}
\end{align}",['ordinary-differential-equations']
75,Difficult ODE with Heaviside as coefficient,Difficult ODE with Heaviside as coefficient,,"So In my research I stumbled upon a difficult ODE, It goes like this $$ y''(x)-[(\operatorname{Heaviside}(ax)+b]y(x)=0, $$ (a,b are the respective constants) I tried approximating Heaviside with analytical terms, which gave me these ODE's $$ y''(x)-[b/2*(1+2/π * \arctan⁡(x/ϵ) )+a]y(x)=0,  $$ or $$ y''(x)-[a/(1+e^x\epsilon]+b)y(x)=0$$ The epsilon should multiply the power as well, it doesn't work for some reason. The mighty wolfram gives me answers that I barely understand, How would you suggest I go about it? Should I try a different approach? is it even solvable? Laplace transform also didn't help me. Thanks a lot!","So In my research I stumbled upon a difficult ODE, It goes like this (a,b are the respective constants) I tried approximating Heaviside with analytical terms, which gave me these ODE's or The epsilon should multiply the power as well, it doesn't work for some reason. The mighty wolfram gives me answers that I barely understand, How would you suggest I go about it? Should I try a different approach? is it even solvable? Laplace transform also didn't help me. Thanks a lot!"," y''(x)-[(\operatorname{Heaviside}(ax)+b]y(x)=0,   y''(x)-[b/2*(1+2/π * \arctan⁡(x/ϵ) )+a]y(x)=0,    y''(x)-[a/(1+e^x\epsilon]+b)y(x)=0","['calculus', 'ordinary-differential-equations', 'step-function']"
76,Mistake in calculating Green's function,Mistake in calculating Green's function,,"Solve the BVP $$y'' = f(x) = x, \qquad y(0) = 0, \qquad y'(1) = 0$$ with the usage of Green's function. First of all, the solution should be $$y = \frac{x^3}{6} - \frac{x}{2}.$$ According to this explanation, for calculating $G$ , we first need a fundamental system of the associated homogenous equation $y'' = 0$ . We take $u_1 = x$ and $u_2 = 1$ . According to the boundary values, we now choose $y_1 = x$ and $y_2 = 1$ , then we have $y_1(0) = 0$ and $y_2'(1) = 0$ . Furthermore, $W(y_1, y_2) = -1$ , so it follows $$G(x,s) = \begin{cases} -s \qquad &0 \leq s \leq x \\ -x \qquad &x \leq s \leq 1. \end{cases}$$ Then I obtain for the solution $$y = \int_0^1 G(x,s) f(s) \ \mathrm{d}s = -\frac{1}{3},$$ which is obviously not the correct answer. Where is my mistake?","Solve the BVP with the usage of Green's function. First of all, the solution should be According to this explanation, for calculating , we first need a fundamental system of the associated homogenous equation . We take and . According to the boundary values, we now choose and , then we have and . Furthermore, , so it follows Then I obtain for the solution which is obviously not the correct answer. Where is my mistake?","y'' = f(x) = x, \qquad y(0) = 0, \qquad y'(1) = 0 y = \frac{x^3}{6} - \frac{x}{2}. G y'' = 0 u_1 = x u_2 = 1 y_1 = x y_2 = 1 y_1(0) = 0 y_2'(1) = 0 W(y_1, y_2) = -1 G(x,s) = \begin{cases} -s \qquad &0 \leq s \leq x \\
-x \qquad &x \leq s \leq 1.
\end{cases} y = \int_0^1 G(x,s) f(s) \ \mathrm{d}s = -\frac{1}{3},","['real-analysis', 'calculus', 'ordinary-differential-equations', 'boundary-value-problem', 'greens-function']"
77,Integrating factor mistakes when solving 1 order ODE,Integrating factor mistakes when solving 1 order ODE,,"I have an ODE: $$\frac{dy}{dx} + 3x^{2}y = x^{2}$$ . I got the following integrating factor: $$e^{x^3}$$ Then I multiplied both sides, but didn't come up with the right answer. It should be: $$y = c~e^{-x^3} + \frac{1}{3}. $$ When I come up with the following: $$y = \left(\frac{1}{3}e^{x^3} + c\right) e^{x^3}$$ What am I doing wrong?","I have an ODE: . I got the following integrating factor: Then I multiplied both sides, but didn't come up with the right answer. It should be: When I come up with the following: What am I doing wrong?",\frac{dy}{dx} + 3x^{2}y = x^{2} e^{x^3} y = c~e^{-x^3} + \frac{1}{3}.  y = \left(\frac{1}{3}e^{x^3} + c\right) e^{x^3},"['ordinary-differential-equations', 'integrating-factor']"
78,Finding the solution to $xy'' +2y' +xy=0$ around $x_{0}=0$using the method of Frobenius.,Finding the solution to  around using the method of Frobenius.,xy'' +2y' +xy=0 x_{0}=0,"We know that the solution of this ODE is like: $$ y=\sum_{n=0}^{\infty}C_nx^{n+r}$$ Them derivative $y$ and $y'$ . $$y'=\sum_{n=0}^{\infty}(n+r)C_nx^{n+r-1}$$ $$y''=\sum_{n=0}^{\infty}(n+r-1)(n+r)C_nx^{n+r-2}$$ Replace $y$ , $y'$ and $y''$ in the ODE. $$\sum_{n=0}^{\infty}(n+r-1)(n+r)C_nx^{n+r-1}+\sum_{n=0}^{\infty}2(n+r)C_nx^{n+r-1} +\sum_{n=0}^{\infty}C_nx^{n+r+1}=0 $$ Now in the first and second summations if $k=n-1$ and in the third summation if $k=n+1$ . We have: $$\sum_{k=-1}^{\infty}(k+r)(k+r+1)C_{k+1}x^{k+r}+\sum_{n=-1}^{\infty}2(k+r+1)C_{k+1}x^{k+r} +\sum_{n=1}^{\infty}C_{k-1}x^{k+r}=0 $$ Now I separed the $k=-1$ and $k=0$ in the first and second summantions. $$r(r-1)C_0x^{r-1}+(r+1)(r)C_1x^{r} + 2rC_0x^{r-1} +2(r+1)C_1x^r + \sum_{k=1}^{\infty}[(k+r)(k+r+1)C_{k+1}x^{k+r}+2(k+r+1)C_{k+1}x^{k+r}+C_{k-1}x^{k+r}]=0$$ After that I equalized the right side with the left and I have this 2 equation to find $r$ : $$(r^2 +r)C_0=0$$ and $$(r^2+3r+2)C_{1}=0$$ If you solve the quadratic equations you have: $$r_1=0$$ $$r_{2,3}=-1$$ $$r_4=-2$$ And that's all that I did please help me.","We know that the solution of this ODE is like: Them derivative and . Replace , and in the ODE. Now in the first and second summations if and in the third summation if . We have: Now I separed the and in the first and second summantions. After that I equalized the right side with the left and I have this 2 equation to find : and If you solve the quadratic equations you have: And that's all that I did please help me."," y=\sum_{n=0}^{\infty}C_nx^{n+r} y y' y'=\sum_{n=0}^{\infty}(n+r)C_nx^{n+r-1} y''=\sum_{n=0}^{\infty}(n+r-1)(n+r)C_nx^{n+r-2} y y' y'' \sum_{n=0}^{\infty}(n+r-1)(n+r)C_nx^{n+r-1}+\sum_{n=0}^{\infty}2(n+r)C_nx^{n+r-1} +\sum_{n=0}^{\infty}C_nx^{n+r+1}=0  k=n-1 k=n+1 \sum_{k=-1}^{\infty}(k+r)(k+r+1)C_{k+1}x^{k+r}+\sum_{n=-1}^{\infty}2(k+r+1)C_{k+1}x^{k+r} +\sum_{n=1}^{\infty}C_{k-1}x^{k+r}=0  k=-1 k=0 r(r-1)C_0x^{r-1}+(r+1)(r)C_1x^{r} + 2rC_0x^{r-1} +2(r+1)C_1x^r + \sum_{k=1}^{\infty}[(k+r)(k+r+1)C_{k+1}x^{k+r}+2(k+r+1)C_{k+1}x^{k+r}+C_{k-1}x^{k+r}]=0 r (r^2 +r)C_0=0 (r^2+3r+2)C_{1}=0 r_1=0 r_{2,3}=-1 r_4=-2","['ordinary-differential-equations', 'frobenius-method']"
79,Find function with inequality for derivative,Find function with inequality for derivative,,"Let $C\geq 2$ and $L>0$ . Does there exist $g \in C^1([0,L])$ such that \begin{equation*} g(x)>0, \qquad g'(x)>0, \qquad g'(x) > (g(L) - g(x))C \end{equation*} holds for any $x \in [0,L]$ ? How large can the length $L$ of the interval be chosen? First example: if \begin{equation*} g(x) = ax + b \end{equation*} with $a, b>0$ , then $$g'(x) > (g(L) - g(x)) C$$ is equivalent to $L < \frac{1}{C}$ . Second example: let $f \colon [0,L] \rightarrow [\varepsilon, \frac\pi2]$ be defined by \begin{equation*} f(x) = \frac{\pi x}{2L} + \varepsilon \Big(1 - \frac{x}{L}\Big).  \end{equation*} Then, if we choose \begin{equation*} g(x) = -\cot(f(x)) + \cot(\varepsilon) + \delta \end{equation*} with $\delta>0$ and $\varepsilon \in (0, \frac\pi2)$ , we have $$g'(x) = \Big(\frac{\pi}{2} - \varepsilon\Big) \frac{1}{L \sin^2(f(x))},$$ while $$g(L) - g(x) = \cot(f(x)).$$ Hence $g'(x) > (g(L) - g(x)) C$ is equivalent to $L < \frac{\pi - 2\varepsilon}{C}$ . But is there a function $g$ allowing $L$ to be larger?","Let and . Does there exist such that holds for any ? How large can the length of the interval be chosen? First example: if with , then is equivalent to . Second example: let be defined by Then, if we choose with and , we have while Hence is equivalent to . But is there a function allowing to be larger?","C\geq 2 L>0 g \in C^1([0,L]) \begin{equation*}
g(x)>0, \qquad g'(x)>0, \qquad g'(x) > (g(L) - g(x))C
\end{equation*} x \in [0,L] L \begin{equation*}
g(x) = ax + b
\end{equation*} a, b>0 g'(x) > (g(L) - g(x)) C L < \frac{1}{C} f \colon [0,L] \rightarrow [\varepsilon, \frac\pi2] \begin{equation*}
f(x) = \frac{\pi x}{2L} + \varepsilon \Big(1 - \frac{x}{L}\Big). 
\end{equation*} \begin{equation*}
g(x) = -\cot(f(x)) + \cot(\varepsilon) + \delta
\end{equation*} \delta>0 \varepsilon \in (0, \frac\pi2) g'(x) = \Big(\frac{\pi}{2} - \varepsilon\Big) \frac{1}{L \sin^2(f(x))}, g(L) - g(x) = \cot(f(x)). g'(x) > (g(L) - g(x)) C L < \frac{\pi - 2\varepsilon}{C} g L","['ordinary-differential-equations', 'functional-inequalities']"
80,Second order linear inhomogeneous ODE with characteristic function,Second order linear inhomogeneous ODE with characteristic function,,"I'd like to calculate the solution $y : [0, \infty) \longrightarrow \mathbb{R}$ of the IVP $$\ddot{y}(t) -4\dot{y}(t)+4y(t) = t\mathrm e^{2t} \chi_{[0,1]}(t), \quad y(0) = 1,\ \dot{y}(0) = 0,$$ where $\chi_{[0,1]}(t)$ is the indicator function on the interval $[0, 1].$ What I was able to do is to solve the homogenous equation $\ddot{y}(t) -4\dot{y}(t)+4y(t) = 0$ . After this I wanted to find a particular solution of the inhomogeneous equation. My idea was, that I use the ansatz $y_{\mathrm p}(t) = f(t) \mathrm e^{2t}$ , where $f$ is $\mathcal{C}^2$ , plug this into the ODE and multiplicate the resulting solution with the characteristic function. That lead me to the particular solution $y_{\mathrm p}(t) = \frac{t^3}{6} \mathrm e^{2t} \chi_{[0,1]}(t)$ , but I was told that this is wrong. What is my mistake and how can one solve this problem?","I'd like to calculate the solution of the IVP where is the indicator function on the interval What I was able to do is to solve the homogenous equation . After this I wanted to find a particular solution of the inhomogeneous equation. My idea was, that I use the ansatz , where is , plug this into the ODE and multiplicate the resulting solution with the characteristic function. That lead me to the particular solution , but I was told that this is wrong. What is my mistake and how can one solve this problem?","y : [0, \infty) \longrightarrow \mathbb{R} \ddot{y}(t) -4\dot{y}(t)+4y(t) = t\mathrm e^{2t} \chi_{[0,1]}(t), \quad y(0) = 1,\ \dot{y}(0) = 0, \chi_{[0,1]}(t) [0, 1]. \ddot{y}(t) -4\dot{y}(t)+4y(t) = 0 y_{\mathrm p}(t) = f(t) \mathrm e^{2t} f \mathcal{C}^2 y_{\mathrm p}(t) = \frac{t^3}{6} \mathrm e^{2t} \chi_{[0,1]}(t)",['ordinary-differential-equations']
81,Singular solution of $y^2(y - xp) = x^4p^2$,Singular solution of,y^2(y - xp) = x^4p^2,"Given differential equation, $y^2(y - xp) = x^4p^2$ where { $p = dy/dx$ } To find the singular solution, I have extracted the p-discriminant relation which is, $y^3x^2(y + 4x^2) = 0$ From here it is evident that all $x = 0, y = 0$ and $y + 4x^2 = 0$ are the singular solutions when tested by putting back in the differential equation but my text book doesn't mention anything about $x=0$ as singular solution. Am I doing any mistake?","Given differential equation, where { } To find the singular solution, I have extracted the p-discriminant relation which is, From here it is evident that all and are the singular solutions when tested by putting back in the differential equation but my text book doesn't mention anything about as singular solution. Am I doing any mistake?","y^2(y - xp) = x^4p^2 p = dy/dx y^3x^2(y + 4x^2) = 0 x = 0, y = 0 y + 4x^2 = 0 x=0","['ordinary-differential-equations', 'singular-solution']"
82,Example of unstable attractor,Example of unstable attractor,,"While defining the notion of assymptotically stable solution of an ODE (stable + atractor), my notes warn that there are unstable attractors and accompany this with the following diagram: What is a simple example of an unstable attractor.","While defining the notion of assymptotically stable solution of an ODE (stable + atractor), my notes warn that there are unstable attractors and accompany this with the following diagram: What is a simple example of an unstable attractor.",,"['ordinary-differential-equations', 'dynamical-systems']"
83,Find the composite solution to this problem,Find the composite solution to this problem,,"I want to find a composite solution to the boundary value problem: $$ \epsilon y'' - y' + y^2 = 1, \text{ for }0<x<1 ,\text{ where }y(0) = 1/3,\,y(1) = 1 $$ where $\epsilon\ll 1$ . My approach: I know that I can find a composite solution in four steps: Find an outer solution. Find a boundary-layer solution. Apply matching so that the outer solution and the boundary-layer solution both approximate the same function correctly. Find the composite solution, by adding the outer solution and the boundary-layer solution and subtracting the part where they are equal. Step 1: Outer solution: I assume that the solution has an expansion in powers of $\epsilon$ . So that $$ y\sim y_0 + \epsilon^\alpha y_1 + \epsilon^\beta y_2 + \ldots $$ with $0 < \alpha < \beta <\ldots$ If we substitute this into the equation we get $$ \epsilon(y_0 + \epsilon^\alpha y_1 + \ldots )'' - (y_0 + \epsilon^\alpha y_1 + \ldots)' + (y_0 + \epsilon^\alpha y_1 + \ldots )^2 = 1 $$ If we only look at the $\mathcal{O}(1)$ terms we get $$ -y_0' + y_0^2 = 1 $$ so that the outer solution becomes $$ y_0 = \dfrac{1 - e^{2c_1 + 2x}}{e^{2c_1 + 2x} + 1} $$ Step 2: Boundary layer solution: Let's assume that there is a boundary layer at $x = 0$ . I introduce the boundary layer coordinate $$ \bar{x} = \dfrac{x}{\epsilon^\alpha} \Leftrightarrow \dfrac{d}{dx} = \dfrac{1}{\epsilon^\alpha}\dfrac{d}{d\bar{x}}, \dfrac{d^2}{dx^2} = \dfrac{1}{\epsilon^{2\alpha}}\dfrac{d^2}{d\bar{x}^2} $$ If we let $Y(\bar{x})$ denote the solution of the problem when using this boundary layer coordinate, the original equation becomes $$ \epsilon^{1 - 2\alpha}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots) - \epsilon^{-\alpha}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots ) = 1 $$ To balance this equation we need to look at the different terms. We already used the second and third term in part 1 for the outer solution. We can try balancing the first and the second term so that the third term becomes higher order. We need $1 - 2\alpha = -\alpha\Leftrightarrow \alpha = 1$ . With $\alpha = 1$ the equation with the boundary layer coordinate becomes $$ $$\dfrac{1}{\epsilon}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots ) - \dfrac{1}{\epsilon}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots)^2 = 1 $$ If we now look at the order $\mathcal{O}\big(\dfrac{1}{\epsilon}\big)$ terms we get: $$ Y_0''(\bar{x}) - Y_0'(\bar{x}) = 1, \, Y_0(0) = 1/3 $$ so that $Y_0(\bar{x}) = c_1e^{\bar{x}} + c_2 - \bar{x}$ . We need $Y_0(0) = 1/3$ so we have $c_1 + c_2 = 1/3$ . If there is a boundary layer at $x = 0$ then we need the outer solution to satisfy the boundary condition at $x = 1$ so that we have $\dfrac{1 - e^{2c_1 + 2}}{e^{2c_1 + 2} + 1} = 1$ . This last expression can be rewritten to \begin{align} 1 - e^{2c_1+2} = e^{2c_1 + 2} + 1\\ \Leftrightarrow -2e^{2c_1 + 2} = 0 \end{align} Since there is no finite $c_1$ for which this expression holds I should probably look if the outer solution should instead satisfy the boundary condition at $x = 0$ . In that case we need to find $c_1$ such that $$ -2e^{2c_1 + 2} = 1/3 $$ but this results in a complex value for $c_1$ and I don't think that I should end up with such a solution. Question: What am I doing wrong? How can I find a correct outer solution and a correct boundary layer solution so that I can start with the matching process?","I want to find a composite solution to the boundary value problem: where . My approach: I know that I can find a composite solution in four steps: Find an outer solution. Find a boundary-layer solution. Apply matching so that the outer solution and the boundary-layer solution both approximate the same function correctly. Find the composite solution, by adding the outer solution and the boundary-layer solution and subtracting the part where they are equal. Step 1: Outer solution: I assume that the solution has an expansion in powers of . So that with If we substitute this into the equation we get If we only look at the terms we get so that the outer solution becomes Step 2: Boundary layer solution: Let's assume that there is a boundary layer at . I introduce the boundary layer coordinate If we let denote the solution of the problem when using this boundary layer coordinate, the original equation becomes To balance this equation we need to look at the different terms. We already used the second and third term in part 1 for the outer solution. We can try balancing the first and the second term so that the third term becomes higher order. We need . With the equation with the boundary layer coordinate becomes $$ If we now look at the order terms we get: so that . We need so we have . If there is a boundary layer at then we need the outer solution to satisfy the boundary condition at so that we have . This last expression can be rewritten to Since there is no finite for which this expression holds I should probably look if the outer solution should instead satisfy the boundary condition at . In that case we need to find such that but this results in a complex value for and I don't think that I should end up with such a solution. Question: What am I doing wrong? How can I find a correct outer solution and a correct boundary layer solution so that I can start with the matching process?","
\epsilon y'' - y' + y^2 = 1, \text{ for }0<x<1 ,\text{ where }y(0) = 1/3,\,y(1) = 1
 \epsilon\ll 1 \epsilon 
y\sim y_0 + \epsilon^\alpha y_1 + \epsilon^\beta y_2 + \ldots
 0 < \alpha < \beta <\ldots 
\epsilon(y_0 + \epsilon^\alpha y_1 + \ldots )'' - (y_0 + \epsilon^\alpha y_1 + \ldots)' + (y_0 + \epsilon^\alpha y_1 + \ldots )^2 = 1
 \mathcal{O}(1) 
-y_0' + y_0^2 = 1
 
y_0 = \dfrac{1 - e^{2c_1 + 2x}}{e^{2c_1 + 2x} + 1}
 x = 0 
\bar{x} = \dfrac{x}{\epsilon^\alpha} \Leftrightarrow \dfrac{d}{dx} = \dfrac{1}{\epsilon^\alpha}\dfrac{d}{d\bar{x}}, \dfrac{d^2}{dx^2} = \dfrac{1}{\epsilon^{2\alpha}}\dfrac{d^2}{d\bar{x}^2}
 Y(\bar{x}) 
\epsilon^{1 - 2\alpha}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots) - \epsilon^{-\alpha}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots ) = 1
 1 - 2\alpha = -\alpha\Leftrightarrow \alpha = 1 \alpha = 1 \dfrac{1}{\epsilon}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon^\gamma Y_1 + \ldots ) - \dfrac{1}{\epsilon}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon^\gamma Y_1 + \ldots) + (Y_0 + \epsilon^\gamma Y_1 + \ldots)^2 = 1
 \mathcal{O}\big(\dfrac{1}{\epsilon}\big) 
Y_0''(\bar{x}) - Y_0'(\bar{x}) = 1, \, Y_0(0) = 1/3
 Y_0(\bar{x}) = c_1e^{\bar{x}} + c_2 - \bar{x} Y_0(0) = 1/3 c_1 + c_2 = 1/3 x = 0 x = 1 \dfrac{1 - e^{2c_1 + 2}}{e^{2c_1 + 2} + 1} = 1 \begin{align}
1 - e^{2c_1+2} = e^{2c_1 + 2} + 1\\
\Leftrightarrow -2e^{2c_1 + 2} = 0
\end{align} c_1 x = 0 c_1 
-2e^{2c_1 + 2} = 1/3
 c_1","['ordinary-differential-equations', 'asymptotics', 'approximation', 'boundary-value-problem']"
84,Stability of equilibrium of a nonlinear system of ODE's,Stability of equilibrium of a nonlinear system of ODE's,,"Suppose we have the nonlinear system of ODE's $$\begin{cases} \dot{x_1} = -\beta x_1 x_2 \\ \dot{x_2} = \beta x_1 x_2 - \gamma x_2 \end{cases} $$ Where we take $\beta, \gamma > 0$ arbitrary for now. In particular I am interested in the equilibrium point $(x_1, x_2) = (1, 0)$ . I first linearized the system around the point $(1, 0)$ by using the Jacobian $$J(x_1, x_2) = \begin{pmatrix} -\beta x_2 & -\beta x_1 \\ \beta x_2 & \beta x_1 - \gamma \end{pmatrix}.$$ So the linearized system around $(1, 0)$ is given by $$\begin{pmatrix} \dot{x_1} \\ \dot{x_2} \end{pmatrix} = \begin{pmatrix} 0 & -\beta \\ 0 & \beta - \gamma \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}.$$ Hence, it follows we have eigenvalues $\lambda_1 = 0$ and $\lambda_2 = \beta - \gamma$ . Now if $\beta > \gamma$ we know that the nonlinear system is unstable. However, if we let $\beta \leq \gamma$ we can not determine the stability of the nonlinear system by linearization. The system seems relatively simple and I would expect the equilibrium to be stable or even asymptotically stable in the case $\beta \leq \gamma$ , but how would one prove this when linearization fails to provide a conclusive answer? Or did I make some error in my reasoning?","Suppose we have the nonlinear system of ODE's Where we take arbitrary for now. In particular I am interested in the equilibrium point . I first linearized the system around the point by using the Jacobian So the linearized system around is given by Hence, it follows we have eigenvalues and . Now if we know that the nonlinear system is unstable. However, if we let we can not determine the stability of the nonlinear system by linearization. The system seems relatively simple and I would expect the equilibrium to be stable or even asymptotically stable in the case , but how would one prove this when linearization fails to provide a conclusive answer? Or did I make some error in my reasoning?","\begin{cases}
\dot{x_1} = -\beta x_1 x_2 \\
\dot{x_2} = \beta x_1 x_2 - \gamma x_2
\end{cases}
 \beta, \gamma > 0 (x_1, x_2) = (1, 0) (1, 0) J(x_1, x_2) = \begin{pmatrix} -\beta x_2 & -\beta x_1 \\ \beta x_2 & \beta x_1 - \gamma \end{pmatrix}. (1, 0) \begin{pmatrix} \dot{x_1} \\ \dot{x_2} \end{pmatrix} = \begin{pmatrix} 0 & -\beta \\ 0 & \beta - \gamma \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}. \lambda_1 = 0 \lambda_2 = \beta - \gamma \beta > \gamma \beta \leq \gamma \beta \leq \gamma","['ordinary-differential-equations', 'systems-of-equations', 'nonlinear-system', 'stability-in-odes', 'stability-theory']"
85,Find solution to second order linear differential equation in 3 parts.,Find solution to second order linear differential equation in 3 parts.,,Say I have this equation: $$y'' - 4y' + 4y = x - \sin{x}$$ My process is to: - find complementary solution - find the particular solution in two parts - add them together to find general solution. Is this process and answer correct? complementary solution find auxiliary: $$r^2 - 4r + 4 = 0$$ $$(r-2)(r-2) = 0$$ so complementary is: $y_c = c_1e^{2x} + c_2xe^{2x}$ part 1 of particular: $$y_p1 = y'' - 4y' + 4y = x$$ the particular is in the form: $Ax + B$ $$y'p = A$$ $$y''p = 0$$ so via substitution: $$- 4A + 4Ax + 4B = x$$ setting coefficients equal: $-4A = 1$ and $A = \frac{1}{4}$ and $B = \frac{1}{4}$ so this part of this particular solution is $$y_p1 = \frac{1}{4}x + \frac{1}{4}$$ the other part of the particular is this: $$y_p2 = A\sin{x} + B\cos{x}$$ $$y'_p2 = A\cos{x} - B\sin{x}$$ $$y''_p2 = -A\sin{x} - B\cos{x}$$ so substituting into $y'' - 4y' + 4y = x$ : $$-A\sin{x} - B\cos{x} - 4A\cos{x} + 4B\sin{x} + 4A\sin{x} + 4B\cos{x} = -\sin{x}$$ so for the sines: $$3A + 4B = -1$$ for the cosines: $$-3B - 4A = 0$$ solving: $$-3B = 4A$$ $$\frac{-3}{4}B = A$$ $$\frac{-9}{4}B + 4B = -1$$ $$\frac{7}{4}B = -1$$ $$B = \frac{-4}{7}$$ $$A = \frac{12}{28}$$ so summing all together: general solution $$y = c_1e^{2x} + c_2xe^{2x} + \frac{1}{4}x + \frac{1}{4} + \frac{12}{28}\sin x  -\frac{4}{7}\cos x$$ Is this right?,Say I have this equation: My process is to: - find complementary solution - find the particular solution in two parts - add them together to find general solution. Is this process and answer correct? complementary solution find auxiliary: so complementary is: part 1 of particular: the particular is in the form: so via substitution: setting coefficients equal: and and so this part of this particular solution is the other part of the particular is this: so substituting into : so for the sines: for the cosines: solving: so summing all together: general solution Is this right?,y'' - 4y' + 4y = x - \sin{x} r^2 - 4r + 4 = 0 (r-2)(r-2) = 0 y_c = c_1e^{2x} + c_2xe^{2x} y_p1 = y'' - 4y' + 4y = x Ax + B y'p = A y''p = 0 - 4A + 4Ax + 4B = x -4A = 1 A = \frac{1}{4} B = \frac{1}{4} y_p1 = \frac{1}{4}x + \frac{1}{4} y_p2 = A\sin{x} + B\cos{x} y'_p2 = A\cos{x} - B\sin{x} y''_p2 = -A\sin{x} - B\cos{x} y'' - 4y' + 4y = x -A\sin{x} - B\cos{x} - 4A\cos{x} + 4B\sin{x} + 4A\sin{x} + 4B\cos{x} = -\sin{x} 3A + 4B = -1 -3B - 4A = 0 -3B = 4A \frac{-3}{4}B = A \frac{-9}{4}B + 4B = -1 \frac{7}{4}B = -1 B = \frac{-4}{7} A = \frac{12}{28} y = c_1e^{2x} + c_2xe^{2x} + \frac{1}{4}x + \frac{1}{4} + \frac{12}{28}\sin x  -\frac{4}{7}\cos x,"['ordinary-differential-equations', 'proof-verification']"
86,Solving Thiele's differential equation.,Solving Thiele's differential equation.,,"Consider Thiele's differential equation for $t\in[0,\infty)$ (all the other functions are continuous on $[0,\infty)$ , too.) $$ \begin{align} V'(t)&=\Big(\phi(t)+\lambda(t)\Big)V(t)+\pi(t)-\lambda(t)A(t)\\ V(0)&=0 \end{align} $$ I am reading a proof about the unique solution being $$V(t)=\int_0^t \big(\pi(s)-\lambda(s)A(s)\big)\exp\Big(\int_s^t \big(\phi(u)+\lambda(u)\big)du\Big)ds$$ So the first thing happening in the proof is that the author solves the equation $$V'(t)=\big(\phi(t)+\lambda(t)\big)V(t)$$ and finding the solution by variation of the constant afterwards. The last equation is equivalent to $$\frac{V'(t)}{V(t)}=\big(\phi(t)+\lambda(t)\big)$$ and therefore $$\int_0^t\frac{V'(s)}{V(s)}ds=\int_0^t \big(\phi(s)+\lambda(s)\big)ds$$ Now he states something I do not understand: $$\log V(t)=\int_0^t \big(\phi(s)+\lambda(s)\big)ds + c$$ In my opinion, using that $\log' V(t)= \frac{V'(t)}{V(t)}$ , it should be $$\log V(t) -\log V(0)=\int_0^t \big(\phi(s)+\lambda(s)\big)ds,$$ which seems to be not well defined, since $V(0)=0$ . Is this some sort of method to solve this equation or is this just wrong? What is the procedure here? Thanks in advance for any help!","Consider Thiele's differential equation for (all the other functions are continuous on , too.) I am reading a proof about the unique solution being So the first thing happening in the proof is that the author solves the equation and finding the solution by variation of the constant afterwards. The last equation is equivalent to and therefore Now he states something I do not understand: In my opinion, using that , it should be which seems to be not well defined, since . Is this some sort of method to solve this equation or is this just wrong? What is the procedure here? Thanks in advance for any help!","t\in[0,\infty) [0,\infty) 
\begin{align}
V'(t)&=\Big(\phi(t)+\lambda(t)\Big)V(t)+\pi(t)-\lambda(t)A(t)\\
V(0)&=0
\end{align}
 V(t)=\int_0^t \big(\pi(s)-\lambda(s)A(s)\big)\exp\Big(\int_s^t \big(\phi(u)+\lambda(u)\big)du\Big)ds V'(t)=\big(\phi(t)+\lambda(t)\big)V(t) \frac{V'(t)}{V(t)}=\big(\phi(t)+\lambda(t)\big) \int_0^t\frac{V'(s)}{V(s)}ds=\int_0^t \big(\phi(s)+\lambda(s)\big)ds \log V(t)=\int_0^t \big(\phi(s)+\lambda(s)\big)ds + c \log' V(t)= \frac{V'(t)}{V(t)} \log V(t) -\log V(0)=\int_0^t \big(\phi(s)+\lambda(s)\big)ds, V(0)=0","['real-analysis', 'ordinary-differential-equations']"
87,Using a Lyapunov function to determine stability of equilibria,Using a Lyapunov function to determine stability of equilibria,,"Given $$\left\{\begin{aligned} x' &= -x^3 + 7xy^2\\ y' &= -3x^2y+y^3\end{aligned}\right.$$ find $a, b > 0$ such that $L(x,y) = a x^2 + b y^2$ obeys $\frac{d}{dt}L \neq 0$ whenever $(x,y) \neq (0,0)$ . State whether the origin is a stable or unstable equilibrium. So we want to find $a,b$ where $L(x,y)$ is a Lyapunov function, and then determine the stability of the equilibria. First differentiate: $\frac{dL}{dt}=2axx'+2byy'$ Now plug in system: $2ax(-x^3+7xy^2)+2by(-3x^2y+y^3)$ By checking the equilibria, we see that both $x'$ and $y'$ equal zero only when $(x,y)=(0,0)$ . Therefore we need to find when $2ax(-x^3+7xy^2)= -2by(-3x^2y+y^3)$ This is where I am confused, as I am not sure how to find $a$ or $b$ purely in terms of $x,y$ and what restrictions I may need to impose. Any help is appreciated. Thank you.","Given find such that obeys whenever . State whether the origin is a stable or unstable equilibrium. So we want to find where is a Lyapunov function, and then determine the stability of the equilibria. First differentiate: Now plug in system: By checking the equilibria, we see that both and equal zero only when . Therefore we need to find when This is where I am confused, as I am not sure how to find or purely in terms of and what restrictions I may need to impose. Any help is appreciated. Thank you.","\left\{\begin{aligned} x' &= -x^3 + 7xy^2\\
y' &= -3x^2y+y^3\end{aligned}\right. a, b > 0 L(x,y) = a x^2 + b y^2 \frac{d}{dt}L \neq 0 (x,y) \neq (0,0) a,b L(x,y) \frac{dL}{dt}=2axx'+2byy' 2ax(-x^3+7xy^2)+2by(-3x^2y+y^3) x' y' (x,y)=(0,0) 2ax(-x^3+7xy^2)= -2by(-3x^2y+y^3) a b x,y","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'lyapunov-functions']"
88,A proof for solution of first order differential equation,A proof for solution of first order differential equation,,"Let $f\left( x \right)$ and $g\left( x \right)$ be continuous functions on $\left[ 0,\infty  \right)$ satisfying $f\left( x \right)>0$ for all $x\ge 0$ , $$ \int_{0}^{\infty }{f\left( x \right)dx=\infty \quad and\quad }M=\int_{0}^{\infty }{\frac{{{g}^{2}}\left( x \right)}{f\left( x \right)}dx<\infty }. $$ My Question : I would like to show that a solution of the differential equation: $$ {y}'\left( x \right)=f\left( x \right)y\left( x \right)+g\left( x \right) $$ Satisfying $$ \int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<\infty } $$ is uniquely determined and satisfies: $$  \int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<M}. $$ Note :I think the proof of this statement is based on the formula which gives the solution of first order differential equation and the Cauchy-Schwarz inequality, however i don't know how to continue. Any ideas?","Let and be continuous functions on satisfying for all , My Question : I would like to show that a solution of the differential equation: Satisfying is uniquely determined and satisfies: Note :I think the proof of this statement is based on the formula which gives the solution of first order differential equation and the Cauchy-Schwarz inequality, however i don't know how to continue. Any ideas?","f\left( x \right) g\left( x \right) \left[ 0,\infty  \right) f\left( x \right)>0 x\ge 0 
\int_{0}^{\infty }{f\left( x \right)dx=\infty \quad and\quad }M=\int_{0}^{\infty }{\frac{{{g}^{2}}\left( x \right)}{f\left( x \right)}dx<\infty }.
 
{y}'\left( x \right)=f\left( x \right)y\left( x \right)+g\left( x \right)
 
\int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<\infty }
 
 \int_{0}^{\infty }{f\left( x \right){{y}^{2}}\left( x \right)dx<M}.
","['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations']"
89,Green's Function for Differential Equation $t\dfrac{d^2}{dt^2} - \dfrac{d}{dt}$,Green's Function for Differential Equation,t\dfrac{d^2}{dt^2} - \dfrac{d}{dt},"I have a Second Order Differential Equation $tu'' - u' = 1-t^2$ . How do you find the Green's function $G_a(t)$ for any $a\in(-1,1)$ ? We define the Green's function $G_a(t)$ as a solution to $tG_a'' - G_a' = \delta(t-a)$ My Attempt I found the Homogeneous solution $u(t) = At^2 + B$ for constants $A,B$ . For a fixed $a$ the Green's function $G_a(t)$ should obey the Homogeneous solution on either side of $a$ . So \begin{equation} G_a(t) = \begin{cases}At^2 + B &t<a \\Ct^2 + D &t>a\end{cases} \end{equation} Integrating the equation from $a-\epsilon$ to $a+\epsilon$ as $\epsilon\to 0$ lead to $D-B = \dfrac{1}{2}$ . At this point I've tried to enforce continuity of the Green's function and the Jump in Derivative but no luck. Any help is appreciated.",I have a Second Order Differential Equation . How do you find the Green's function for any ? We define the Green's function as a solution to My Attempt I found the Homogeneous solution for constants . For a fixed the Green's function should obey the Homogeneous solution on either side of . So Integrating the equation from to as lead to . At this point I've tried to enforce continuity of the Green's function and the Jump in Derivative but no luck. Any help is appreciated.,"tu'' - u' = 1-t^2 G_a(t) a\in(-1,1) G_a(t) tG_a'' - G_a' = \delta(t-a) u(t) = At^2 + B A,B a G_a(t) a \begin{equation} G_a(t) = \begin{cases}At^2 + B &t<a \\Ct^2 + D &t>a\end{cases} \end{equation} a-\epsilon a+\epsilon \epsilon\to 0 D-B = \dfrac{1}{2}","['ordinary-differential-equations', 'greens-function']"
90,Help on population differential equation,Help on population differential equation,,"$$\frac{\mathrm dr}{\mathrm dt} = kr \left(1-\frac{r}{r_*}\right) -\alpha fr,$$ where $k>0$ is a constant representing the rabbit breeding rate, $r_*>0$ is the (constant) maximun sustainable rabbit population size in the absence of predation, $f>0$ is the population of foxes, and $\alpha>0$ is the (constant) rate or predation of rabbits by foxes. I am told that the fox population is constant. I'm trying to solve this differential equation. I tried using the Bernoulli equation method, however it did not work out, which is shown here (habbit of mine is I accidently made $r_*$ that was for the maximum sustainable rabbit population size in the absence of predation later into $A$ since it was easier:) $$\frac{\mathrm dr}{\mathrm dt} = kr -\frac{kr^2}{r_*} -\alpha fr$$ Define $w(t)$ by $w=r^{1-2}=\frac{1}{r}$ , so that $$\frac{\mathrm dw}{\mathrm dt} = -\frac{1}{r^2} \frac{\mathrm dr}{\mathrm dt}$$ $$\implies \frac{\mathrm dw}{\mathrm dt} +kw = \frac{k}{r_*} +\alpha fw,$$ where this would become $$e^{kt} w(t) = \frac{e^{kt}}{A} +\frac{\alpha fwe^{kt}}{k} +c.$$ As I made $m(t)=k$ , leading to $e^{\int k} = e^{kt}$ and the left hand side was inverse product rule and the right hand side was integration $$ \implies w(t) =\frac{1}{A} +\frac{\alpha fw}{k} +ce^{-kt}$$ $$\implies r(t) = A +\frac{k}{\alpha fw} +\frac{e^{kt}}{c}$$","where is a constant representing the rabbit breeding rate, is the (constant) maximun sustainable rabbit population size in the absence of predation, is the population of foxes, and is the (constant) rate or predation of rabbits by foxes. I am told that the fox population is constant. I'm trying to solve this differential equation. I tried using the Bernoulli equation method, however it did not work out, which is shown here (habbit of mine is I accidently made that was for the maximum sustainable rabbit population size in the absence of predation later into since it was easier:) Define by , so that where this would become As I made , leading to and the left hand side was inverse product rule and the right hand side was integration","\frac{\mathrm dr}{\mathrm dt} = kr \left(1-\frac{r}{r_*}\right) -\alpha fr, k>0 r_*>0 f>0 \alpha>0 r_* A \frac{\mathrm dr}{\mathrm dt} = kr -\frac{kr^2}{r_*} -\alpha fr w(t) w=r^{1-2}=\frac{1}{r} \frac{\mathrm dw}{\mathrm dt} = -\frac{1}{r^2} \frac{\mathrm dr}{\mathrm dt} \implies \frac{\mathrm dw}{\mathrm dt} +kw = \frac{k}{r_*} +\alpha fw, e^{kt} w(t) = \frac{e^{kt}}{A} +\frac{\alpha fwe^{kt}}{k} +c. m(t)=k e^{\int k} = e^{kt}  \implies w(t) =\frac{1}{A} +\frac{\alpha fw}{k} +ce^{-kt} \implies r(t) = A +\frac{k}{\alpha fw} +\frac{e^{kt}}{c}","['calculus', 'ordinary-differential-equations']"
91,Non-Exact differential Equation,Non-Exact differential Equation,,I am trying to solve a non-exact differential equation $\frac{s-b}{s}ds - \frac{b(2\beta-b)}{8{\beta}^2}db =0$ where $\beta >0$ is a constant. Boundary conditions are given by $b(s=0)=0$ . A reformulation of the above equation using $s =\phi(b)$ yields $\frac{d ln(F(\phi(b))}{db} = \frac{g(b)}{\phi(b)-b}$ . I am interested in finding an equation for $\phi(b)$ . How should I go about it? Thank you very much for your help.,I am trying to solve a non-exact differential equation where is a constant. Boundary conditions are given by . A reformulation of the above equation using yields . I am interested in finding an equation for . How should I go about it? Thank you very much for your help.,\frac{s-b}{s}ds - \frac{b(2\beta-b)}{8{\beta}^2}db =0 \beta >0 b(s=0)=0 s =\phi(b) \frac{d ln(F(\phi(b))}{db} = \frac{g(b)}{\phi(b)-b} \phi(b),"['ordinary-differential-equations', 'integrating-factor']"
92,Solving a PDE in $1$D,Solving a PDE in D,1,"I want to solve the below equation $\partial_t f(x,t)=D \partial_x^2 f(x,t)$ with initial and boundary conditions: $f(x,0)=\delta(x-x_0)$ $D\partial_x f(x,t)=0$ at $x=L$ and $x=0$ ; Can anyone help me with solving it?",I want to solve the below equation with initial and boundary conditions: at and ; Can anyone help me with solving it?,"\partial_t f(x,t)=D \partial_x^2 f(x,t) f(x,0)=\delta(x-x_0) D\partial_x f(x,t)=0 x=L x=0","['ordinary-differential-equations', 'partial-differential-equations']"
93,Second-order non-linear differential equation containing sgn function encountered in literature on the SYK model.,Second-order non-linear differential equation containing sgn function encountered in literature on the SYK model.,,"I'm trying to solve the following differential equation: $$J^2\frac{1}{2^{q-1}}\operatorname{sgn}(\tau)e^{g(\tau)}=\partial_\tau^2\Big(\frac{1}{2q}\operatorname{sgn}(\tau)g(\tau)\Big).$$ Here $J^2$ and $q$ are constants and I want to solve for $e^{g(\tau)}$ . I encountered this equation in a paper by Gábor Sárosi et al . called ""AdS $_2$ holography and the SYK model"" (p.38) and ""Comments on the Sachdev-Ye-Kitaev model"" (p.12) by Juan Maldacena and Douglas Stanford. According to them the general solution to this differential equation is given by: $$e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))}$$ where $$\mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}}$$ I am unable to reach the same result unfortunately. What I tried to do is solve the equation for $$|\tau|>0$$ so that I loose the sgn function (this could potentially be the problem) since I'm not sure how to deal with it otherwise. I get: $$\frac{d^2g(\tau)}{d\tau^2}=J^2\frac{q}{2^{q-2}}e^{g(\tau)}$$ It would be nice if we could make this a first order differential equation so multiplying both sides by $$\frac{dg(\tau)}{d\tau}$$ and integrating w.r.t. $\tau$ gives: $$\int\frac{dg(\tau)}{d\tau}\frac{d^2g(\tau)}{d\tau^2}d\tau=J^2\frac{q}{2^{q-2}}\int e^{g(\tau)}\frac{dg(\tau)}{d\tau}d\tau\\$$ $$\frac{1}{2}\Big(\frac{dg(\tau)}{d\tau}\Big)^2=J^2\frac{q}{2^{q-2}}(e^{g(\tau)}+c_1)$$ Here I used integration by parts to rewrite the lhs. Rearranging a bit gives: $$\frac{dg(\tau)}{d\tau}=J\sqrt{\frac{q}{2^{q-3}}}\sqrt{(e^{g(\tau)}+c_1)}$$ Now that the equation is a separable first order differential equation we can integrate to find the general solution: $$\int\frac{dg(\tau)}{\sqrt{(e^{g(\tau)}+c_1)}} =J\sqrt{\frac{q}{2^{q-3}}}\int d\tau$$ $$-\frac{2}{\sqrt{c_1}} \operatorname{artanh}\Big(\frac{\sqrt{e^{g(\tau)}+c_1}}{\sqrt{c_1}}\Big) =J\sqrt{\frac{q}{2^{q-3}}}(\tau+c_2)$$ So that: $$e^{g(\tau)} =c_1\bigg(\tanh^2\Big(\mathcal{J}\sqrt{c_1}(\tau+c_2)\Big)-1\bigg)$$ where again $$\mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}}$$ Obviously this is not the same as the general solution they state in their paper. Hopefully someone can help me see where I go wrong. If I check the solution they give in the paper $\Big(e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))}\Big)$ it does work out: $$\frac{d^2g(\tau)}{d\tau^2}=\mathcal{J}^2e^{g(\tau)}$$ $$\frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))}=\frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))}$$ But I can think of no way of finding the constants of integration ( $c_1$ and $c_2$ ) using boundary conditions $$g(0)=g(\beta)=0$$","I'm trying to solve the following differential equation: Here and are constants and I want to solve for . I encountered this equation in a paper by Gábor Sárosi et al . called ""AdS holography and the SYK model"" (p.38) and ""Comments on the Sachdev-Ye-Kitaev model"" (p.12) by Juan Maldacena and Douglas Stanford. According to them the general solution to this differential equation is given by: where I am unable to reach the same result unfortunately. What I tried to do is solve the equation for so that I loose the sgn function (this could potentially be the problem) since I'm not sure how to deal with it otherwise. I get: It would be nice if we could make this a first order differential equation so multiplying both sides by and integrating w.r.t. gives: Here I used integration by parts to rewrite the lhs. Rearranging a bit gives: Now that the equation is a separable first order differential equation we can integrate to find the general solution: So that: where again Obviously this is not the same as the general solution they state in their paper. Hopefully someone can help me see where I go wrong. If I check the solution they give in the paper it does work out: But I can think of no way of finding the constants of integration ( and ) using boundary conditions",J^2\frac{1}{2^{q-1}}\operatorname{sgn}(\tau)e^{g(\tau)}=\partial_\tau^2\Big(\frac{1}{2q}\operatorname{sgn}(\tau)g(\tau)\Big). J^2 q e^{g(\tau)} _2 e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))} \mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}} |\tau|>0 \frac{d^2g(\tau)}{d\tau^2}=J^2\frac{q}{2^{q-2}}e^{g(\tau)} \frac{dg(\tau)}{d\tau} \tau \int\frac{dg(\tau)}{d\tau}\frac{d^2g(\tau)}{d\tau^2}d\tau=J^2\frac{q}{2^{q-2}}\int e^{g(\tau)}\frac{dg(\tau)}{d\tau}d\tau\\ \frac{1}{2}\Big(\frac{dg(\tau)}{d\tau}\Big)^2=J^2\frac{q}{2^{q-2}}(e^{g(\tau)}+c_1) \frac{dg(\tau)}{d\tau}=J\sqrt{\frac{q}{2^{q-3}}}\sqrt{(e^{g(\tau)}+c_1)} \int\frac{dg(\tau)}{\sqrt{(e^{g(\tau)}+c_1)}} =J\sqrt{\frac{q}{2^{q-3}}}\int d\tau -\frac{2}{\sqrt{c_1}} \operatorname{artanh}\Big(\frac{\sqrt{e^{g(\tau)}+c_1}}{\sqrt{c_1}}\Big) =J\sqrt{\frac{q}{2^{q-3}}}(\tau+c_2) e^{g(\tau)} =c_1\bigg(\tanh^2\Big(\mathcal{J}\sqrt{c_1}(\tau+c_2)\Big)-1\bigg) \mathcal{J}=J\sqrt{\frac{q}{2^{q-1}}} \Big(e^{g(\tau)} =\frac{c_1^2}{\mathcal{J}^2}\frac{1}{\sin^2(c_1(|\tau|+c_2))}\Big) \frac{d^2g(\tau)}{d\tau^2}=\mathcal{J}^2e^{g(\tau)} \frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))}=\frac{c_1^2}{\sin^2(c_1(|\tau|+c_2))} c_1 c_2 g(0)=g(\beta)=0,"['calculus', 'ordinary-differential-equations']"
94,Homogeneous or non - homogeneous $?$,Homogeneous or non - homogeneous,?,"The second order differential equation is given by - $ \frac{d^{2}y}{dx^{2}} + \sin (x+y) = \sin x$ Is this a homogeneous differential equation $?$ Well, I guess this is not a homogeneous differential equation since the form of this equation is not $a(x)y'' + b(x)y' +c(x)y = 0$ . But the answer is given that it's homogeneous.  How can this equation be homogeneous?","The second order differential equation is given by - Is this a homogeneous differential equation Well, I guess this is not a homogeneous differential equation since the form of this equation is not . But the answer is given that it's homogeneous.  How can this equation be homogeneous?", \frac{d^{2}y}{dx^{2}} + \sin (x+y) = \sin x ? a(x)y'' + b(x)y' +c(x)y = 0,['ordinary-differential-equations']
95,Torricelli's law modelling Sphere,Torricelli's law modelling Sphere,,"Struggling connecting Water-volume to time. Could anyone be kind and guide me in the right direction? A spherical tank is filled with liquid. A tap in the bottom of the   tank opens, and the liquid starts to drain. After one hour is The tank   emptied halfway. How long does it take before it is completely empty?   (The flow is assumed to follow Torricelli's law: its is proportional   to the square root of the surface So far I'm mostly grasping at straws. I've come up with $V'(x) = -k\sqrt{h}$ Assuming r=1 => $V(h) = \int_0^h\pi(1-y^2)dy $ $V(h)$ is a primitive to $V'(h) = \pi (1-h^2)$ $V(h=1)= \frac{2\pi}{3}$ $V(h=\frac{1}{2})= \frac{2\pi}{6}$ $ t = 0 \iff h = 1$ , $ t = 1 \iff h = \frac{1}{2}$ Not sure how to connect this to time further to figure out when $\int_0^tV'(t)dt$ = 0","Struggling connecting Water-volume to time. Could anyone be kind and guide me in the right direction? A spherical tank is filled with liquid. A tap in the bottom of the   tank opens, and the liquid starts to drain. After one hour is The tank   emptied halfway. How long does it take before it is completely empty?   (The flow is assumed to follow Torricelli's law: its is proportional   to the square root of the surface So far I'm mostly grasping at straws. I've come up with Assuming r=1 => is a primitive to , Not sure how to connect this to time further to figure out when = 0",V'(x) = -k\sqrt{h} V(h) = \int_0^h\pi(1-y^2)dy  V(h) V'(h) = \pi (1-h^2) V(h=1)= \frac{2\pi}{3} V(h=\frac{1}{2})= \frac{2\pi}{6}  t = 0 \iff h = 1  t = 1 \iff h = \frac{1}{2} \int_0^tV'(t)dt,"['integration', 'ordinary-differential-equations']"
96,How to deal with the absolute value sign in the process of solving differential equation?,How to deal with the absolute value sign in the process of solving differential equation?,,"Imagine we are given a differential equation as follows $$y'\sin x-y\cos x=0,$$ which seems to be very simple. We can solve it like this: \begin{align*} &y'\sin x-y\cos x=0\\ \implies &\frac{{\rm d}y}{y}=\cot x{\rm d}x\\ \implies &\int \frac{{\rm d}y}{y}=\int \cot x{\rm d}x\\ \implies &\ln y=\ln\sin x+C'\\ \implies &y=e^{\ln\sin x+C'}\\ \implies &y=C\sin x. \end{align*} Right? May be. But wait! Notice the transformation or operation from the the 3rd line to the 4th line. In general, we ought to write as these: $$\int \frac{{\rm d}y}{y}=\ln |y|+C_1,~~~\int \cot x{\rm d}x=\ln|\sin x|+C_2,$$ when we are finding the indefinite integral, namely, we should add an absolute value sign in the result. Of course, sometimes, you can cancel the sign in the final result again, indeed. But some books boldly omit the sign in the process. This is irresponsible negligence, or a valid trick?","Imagine we are given a differential equation as follows which seems to be very simple. We can solve it like this: Right? May be. But wait! Notice the transformation or operation from the the 3rd line to the 4th line. In general, we ought to write as these: when we are finding the indefinite integral, namely, we should add an absolute value sign in the result. Of course, sometimes, you can cancel the sign in the final result again, indeed. But some books boldly omit the sign in the process. This is irresponsible negligence, or a valid trick?","y'\sin x-y\cos x=0, \begin{align*}
&y'\sin x-y\cos x=0\\
\implies &\frac{{\rm d}y}{y}=\cot x{\rm d}x\\
\implies &\int \frac{{\rm d}y}{y}=\int \cot x{\rm d}x\\
\implies &\ln y=\ln\sin x+C'\\
\implies &y=e^{\ln\sin x+C'}\\
\implies &y=C\sin x.
\end{align*} \int \frac{{\rm d}y}{y}=\ln |y|+C_1,~~~\int \cot x{\rm d}x=\ln|\sin x|+C_2,",['ordinary-differential-equations']
97,Continuous vs. Discrete State-Space Model,Continuous vs. Discrete State-Space Model,,"Time-invariant continuous model: $\dot{x}(t) = Ax(t)+Bu(t)$ $y(t) = Cx(t)+Du(t)$ Time-invariant discrete model: $x_{k+1} = Ax_{k}+Bu_{k}$ $y_{k} = Cx_{k}+Du_{k}$ Why does the continuous model result in a rate of change $\dot{x}$ , while the discrete model results in a new state $x_{k+1}$ ?","Time-invariant continuous model: Time-invariant discrete model: Why does the continuous model result in a rate of change , while the discrete model results in a new state ?",\dot{x}(t) = Ax(t)+Bu(t) y(t) = Cx(t)+Du(t) x_{k+1} = Ax_{k}+Bu_{k} y_{k} = Cx_{k}+Du_{k} \dot{x} x_{k+1},"['ordinary-differential-equations', 'control-theory']"
98,Solve $(1-x^2)y''-xy'-y=e^{\arcsin x}$. [closed],Solve . [closed],(1-x^2)y''-xy'-y=e^{\arcsin x},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Problem Solve $$(1-x^2)y''-xy'-y=e^{\arcsin x}.$$ Can it be solved to get the general solution?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Problem Solve Can it be solved to get the general solution?",(1-x^2)y''-xy'-y=e^{\arcsin x}.,['ordinary-differential-equations']
99,Proof that the operator $\Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega})$ is positive and self-adjoint,Proof that the operator  is positive and self-adjoint,\Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega}),"I have the following partial differential equation (eigenvalue equation) \begin{equation} \Theta \vec{\omega}=\alpha\vec{\omega} \end{equation} where I have defined the linear partial differential operator $\Theta$ as $$\Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega}).$$ I would like to show that this operator is positive self-adjoint with respect to the $L^2$ scalar product $(u,v)=\int_B u^*\cdot v~ d^3 r$ . In a paper which I found ( link , doi ) they said it is obvious, however, I would like to prove this explicitly but I struggle a lot. Can anyone give me a hint or some intuition how to prove that? In my case $B$ denotes the body which is a sphere with radius $R$ and $\lambda,\mu$ are the Lamé parameters. This equation one encounters often in linear and isotropic elasticity. If I could show this, than an immediate consequence would be that the eigenvalues are real and positive as well as the eigenfunctions to different eigenvalues are orthogonal, that is clear an easy to prove if the operator is self-adjoint. Is it also possible to prove that the eigenfunction form a complete set of a basis, so that I can write every vector field defined on $B$ as a linear combination of the eigenfunctions?","I have the following partial differential equation (eigenvalue equation) where I have defined the linear partial differential operator as I would like to show that this operator is positive self-adjoint with respect to the scalar product . In a paper which I found ( link , doi ) they said it is obvious, however, I would like to prove this explicitly but I struggle a lot. Can anyone give me a hint or some intuition how to prove that? In my case denotes the body which is a sphere with radius and are the Lamé parameters. This equation one encounters often in linear and isotropic elasticity. If I could show this, than an immediate consequence would be that the eigenvalues are real and positive as well as the eigenfunctions to different eigenvalues are orthogonal, that is clear an easy to prove if the operator is self-adjoint. Is it also possible to prove that the eigenfunction form a complete set of a basis, so that I can write every vector field defined on as a linear combination of the eigenfunctions?","\begin{equation}
\Theta \vec{\omega}=\alpha\vec{\omega}
\end{equation} \Theta \Theta\vec{\omega}=-\mu\Delta\vec{\omega}-(\lambda+\mu)\nabla(\nabla\cdot\vec{\omega}). L^2 (u,v)=\int_B u^*\cdot v~ d^3 r B R \lambda,\mu B","['mathematical-physics', 'ordinary-differential-equations']"
