,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving on the equation $\frac1{x+1}+\frac1{2(x+2)}+\frac1{3(x+3)}+\cdots+\frac1{n(x+n)}=1$,Proving on the equation,\frac1{x+1}+\frac1{2(x+2)}+\frac1{3(x+3)}+\cdots+\frac1{n(x+n)}=1,"Consider the equation: \begin{gather*} \frac{1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + \cdots + \frac{1} {n(x+n)} = 1.\tag1 \end{gather*} Prove that: For every $n \geq 2$ , equation $(1)$ always has a unique positive solution $x_n$ . The sequence $(x_n)$ has a finite limit with $n \to \infty+$ . Find that limit. Here's all I did: $\frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)} = 1.$ $\Leftrightarrow 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) = n! (x+1)(x+2)...(x+n) $ Consider the polynomial $P(x) = 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) - n! (x+1)(x+2)...(x+n) $ Consider the highest coefficient of the polynomial: $ -n! < 0 $ Consider the lowest coefficient of the polynomial: $ (2.3...n)^2+(1.3...n)^2+...(1.2...(n-1))^2  - n! = (1.3...n)^2+...(1.2...(n-1))^2  > 0$ Let $x_1,x_2,x_3....$ be the solutions of $ P(x) $ Assume that equation (1) has no positive solution. According to Viette's theorem: $x_1 x_2 x_3....x_n = (-1)^n \frac{a_0}{a_n} $ $\frac {a_0}{a_n} < 0 \Rightarrow $ if $n$ is an odd number , then $x_1 x_2 x_3....x_n < 0$ , but $(-1)^n \frac{a_0}{a_n} >0 \Rightarrow $ (nonsense) . Same with $n $ being an even number. So the polynomial must have at least one integer root. Suppose the polynomial has at least 2 positive roots $x$ and $y$ . $\frac {1}{y+1} + \frac{1}{2(y+2)} + \frac{1}{3(y+3)} + .... + \frac {1} {n(y+n)} =\frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)}  .$ $\Rightarrow (y-x)( \frac {1}{(x+1)(y+1)}+ \frac {1}{2(x+2)(y+2)} +...+\frac {1}{n(x+n)(y+n)} ) = 0 $ $\Rightarrow x=y \Rightarrow$ (no sense) So for every $n \geq 2$ , equation $( 1 )$ always has a unique positive solution $x_n$ That's all I did , and I have no idea what to do next , I want to establish a relationship between $x_n$ but for $n \geq 3$ the polynomial's solution is a "" bad "" solution . "" so it's very difficult to build. Hope to get help from everyone. Thanks very much .","Consider the equation: Prove that: For every , equation always has a unique positive solution . The sequence has a finite limit with . Find that limit. Here's all I did: Consider the polynomial Consider the highest coefficient of the polynomial: Consider the lowest coefficient of the polynomial: Let be the solutions of Assume that equation (1) has no positive solution. According to Viette's theorem: if is an odd number , then , but (nonsense) . Same with being an even number. So the polynomial must have at least one integer root. Suppose the polynomial has at least 2 positive roots and . (no sense) So for every , equation always has a unique positive solution That's all I did , and I have no idea what to do next , I want to establish a relationship between but for the polynomial's solution is a "" bad "" solution . "" so it's very difficult to build. Hope to get help from everyone. Thanks very much .","\begin{gather*}
\frac{1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + \cdots + \frac{1} {n(x+n)} = 1.\tag1
\end{gather*} n \geq 2 (1) x_n (x_n) n \to \infty+ \frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)} = 1. \Leftrightarrow 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) = n! (x+1)(x+2)...(x+n)  P(x) = 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) - n! (x+1)(x+2)...(x+n)   -n! < 0   (2.3...n)^2+(1.3...n)^2+...(1.2...(n-1))^2  - n! = (1.3...n)^2+...(1.2...(n-1))^2  > 0 x_1,x_2,x_3....  P(x)  x_1 x_2 x_3....x_n = (-1)^n \frac{a_0}{a_n}  \frac {a_0}{a_n} < 0 \Rightarrow  n x_1 x_2 x_3....x_n < 0 (-1)^n \frac{a_0}{a_n} >0 \Rightarrow  n  x y \frac {1}{y+1} + \frac{1}{2(y+2)} + \frac{1}{3(y+3)} + .... + \frac {1} {n(y+n)} =\frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)}  . \Rightarrow (y-x)( \frac {1}{(x+1)(y+1)}+ \frac {1}{2(x+2)(y+2)} +...+\frac {1}{n(x+n)(y+n)} ) = 0  \Rightarrow x=y \Rightarrow n \geq 2 ( 1 ) x_n x_n n \geq 3","['sequences-and-series', 'limits']"
1,"let $a_1=1$ and $a_n=\sin(a_{n-1})$, $n>1$ and $n \in N$ The Calculate the following Limit","let  and ,  and  The Calculate the following Limit",a_1=1 a_n=\sin(a_{n-1}) n>1 n \in N,"Let $a_1=1$ and $a_n=\sin(a_{n-1})$ , $n>1$ and $n \in N$ If $$\lim_{n\to\infty} \frac{2^{2{a_n}}-2^{1+a_n} \cdot 3^{a_n}+3^{2a_n}}{\cos(a_n)+1-e^{a_n}-e^{-a_n}}=l\quad$$ Then calculate $l$ How to solve it without Using L'Hopital's Rule. My Approach: $$\begin{align} a_2 &=\sin(a_1)\\ \implies a_2 &=\sin(1)\\ \implies a_3 &=\sin(\sin1)\\ \end{align} $$ Similarly $a_n=\sin(\sin(\sin\sin(....(sin1))))$ Which will Approach to Zero for $n \to \ \infty$ $\implies$ $\lim_{n\to\infty} a_n=0$ Later I Replaced $a_n$ with $x$ So question changed to $$\lim_{x\to 0} \frac{2^{x}-2^{1+x} \cdot 3^{x}+3^{2x}}{\cos(x)+1-e^{x}-e^{-x}}=l \implies \lim_{x\to 0} \frac{(2^x-3^x)^2}{1+\cos(x)-e^x-e^{-x}}$$ How to solve it further without using L'Hopital's Rule.","Let and , and If Then calculate How to solve it without Using L'Hopital's Rule. My Approach: Similarly Which will Approach to Zero for Later I Replaced with So question changed to How to solve it further without using L'Hopital's Rule.","a_1=1 a_n=\sin(a_{n-1}) n>1 n \in N \lim_{n\to\infty} \frac{2^{2{a_n}}-2^{1+a_n} \cdot 3^{a_n}+3^{2a_n}}{\cos(a_n)+1-e^{a_n}-e^{-a_n}}=l\quad l \begin{align}
a_2 &=\sin(a_1)\\
\implies a_2 &=\sin(1)\\
\implies a_3 &=\sin(\sin1)\\
\end{align}
 a_n=\sin(\sin(\sin\sin(....(sin1)))) n \to \ \infty \implies \lim_{n\to\infty} a_n=0 a_n x \lim_{x\to 0} \frac{2^{x}-2^{1+x} \cdot 3^{x}+3^{2x}}{\cos(x)+1-e^{x}-e^{-x}}=l \implies \lim_{x\to 0} \frac{(2^x-3^x)^2}{1+\cos(x)-e^x-e^{-x}}","['calculus', 'limits', 'limits-without-lhopital']"
2,"Exist or Not ? $\displaystyle \lim_{x\to\infty} \dfrac{1}{x} - \dfrac{1}{x-k}$, $1 \leq k \leq x-1$","Exist or Not ? ,",\displaystyle \lim_{x\to\infty} \dfrac{1}{x} - \dfrac{1}{x-k} 1 \leq k \leq x-1,"I was trying to evaluate $$ \lim_{n\to \infty} \left(\dfrac{n!}{n^n}\right)^{\dfrac{1}{n}} $$ without resorting to a certain theorem that states : $$ \limsup_{n\to \infty}\; a_n\,^{\frac{\Large 1}{\Large n}} = \lim_{n\to \infty} \left(\dfrac{a_{n+1}}{a_n}\right). $$ With my own idea: By using the discreet definition of factorial using the product notation (also converting $n^n$ into discreet product) : $$ n! = \prod_{0\leq k \leq n-1} (n-k)= n\cdot\left(\prod_{1\leq k \leq n-1} (n-k)\right) $$ $$ n^n = \prod_{1\leq k \leq n} n = n\cdot\left(\prod_{1\leq k \leq n-1} n \right) $$ And by dividing $n-k$ by $n$ and duplicating the limit (one for evaluating the limit inside product notation and another for reminding myself that $x$ goes to $\infty$ ) into two I get this limit : $\displaystyle \lim_{n\to\infty} \prod_{1\leq k \leq n-1} \lim_{n\to\infty}\left(1-\dfrac{k}{n}\right)^{\dfrac{1}{n}}$ (1) Evaluating the inner limit by logarithm and L'Hopital get me into this point : $a =  \displaystyle \lim_{n\to\infty} \dfrac{1}{n} - \dfrac{1}{n-k}$ , $1 < k < n-1$ where $y =  \displaystyle \lim_{n\to\infty} \left(1-\dfrac{k}{n}\right)^{\dfrac{1}{n}}$ , and $a = \ln(y)$ My Question : $1$ ) Does $a$ exist as $k$ varies and goes to $n-1$ ? (Continue to Q $2$ ) $2$ ) How to evaluate it ? (Continuation of Q $1$ ) $3$ ) Is my way of doing the limit at equation (1) valid (Duplicating the limit) ?? $4$ ) Any other way to do $\displaystyle \lim_{n\to \infty} \left(\dfrac{n!}{n^n}\right)^{\dfrac{1}{n}}$ without that sequence theorem I mentioned earlier My opinion about Q $1$ : because the main limit (at the very beginning) exist and has a value, I think somehow $a$ exist, but what confuse me is that $a$ varies between $0$ and $-1$ ( $a = -1$ when $k$ goes to $n-1$ ). Q $2$ : I don't have any idea as $k$ value is varying (at least I know that when $k$ is small then $a$ goes to $0$ , and when $k$ value is approaching $n-1$ then $a$ goes to $-1$ )","I was trying to evaluate without resorting to a certain theorem that states : With my own idea: By using the discreet definition of factorial using the product notation (also converting into discreet product) : And by dividing by and duplicating the limit (one for evaluating the limit inside product notation and another for reminding myself that goes to ) into two I get this limit : (1) Evaluating the inner limit by logarithm and L'Hopital get me into this point : , where , and My Question : ) Does exist as varies and goes to ? (Continue to Q ) ) How to evaluate it ? (Continuation of Q ) ) Is my way of doing the limit at equation (1) valid (Duplicating the limit) ?? ) Any other way to do without that sequence theorem I mentioned earlier My opinion about Q : because the main limit (at the very beginning) exist and has a value, I think somehow exist, but what confuse me is that varies between and ( when goes to ). Q : I don't have any idea as value is varying (at least I know that when is small then goes to , and when value is approaching then goes to )","
\lim_{n\to \infty} \left(\dfrac{n!}{n^n}\right)^{\dfrac{1}{n}}
 
\limsup_{n\to \infty}\; a_n\,^{\frac{\Large 1}{\Large n}} = \lim_{n\to \infty} \left(\dfrac{a_{n+1}}{a_n}\right).
 n^n 
n! = \prod_{0\leq k \leq n-1} (n-k)= n\cdot\left(\prod_{1\leq k \leq n-1} (n-k)\right)
 
n^n = \prod_{1\leq k \leq n} n = n\cdot\left(\prod_{1\leq k \leq n-1} n \right)
 n-k n x \infty \displaystyle \lim_{n\to\infty} \prod_{1\leq k \leq n-1} \lim_{n\to\infty}\left(1-\dfrac{k}{n}\right)^{\dfrac{1}{n}} a =  \displaystyle \lim_{n\to\infty} \dfrac{1}{n} - \dfrac{1}{n-k} 1 < k < n-1 y =  \displaystyle \lim_{n\to\infty} \left(1-\dfrac{k}{n}\right)^{\dfrac{1}{n}} a = \ln(y) 1 a k n-1 2 2 1 3 4 \displaystyle \lim_{n\to \infty} \left(\dfrac{n!}{n^n}\right)^{\dfrac{1}{n}} 1 a a 0 -1 a = -1 k n-1 2 k k a 0 k n-1 a -1","['real-analysis', 'limits', 'asymptotics', 'products']"
3,How to see $\lim_{x \to 0}\frac{1-\cos x}{x^2}=\frac12$ by looking at the graphs of $1-\cos x$ and $x^2$,How to see  by looking at the graphs of  and,\lim_{x \to 0}\frac{1-\cos x}{x^2}=\frac12 1-\cos x x^2,"We know $$\lim_{x \to 0}\frac{\sin x}{x} = 1$$ because $\sin x$ behaves like $x$ locally. It can be seen from the following graph: We can see as $x$ approaches $0$ , $\sin x$ behaves more and more like $y = x$ which makes the limit $\lim_{x \to 0}\frac{\sin x}{x}$ equal to $1$ . Now, let's see another limit. This time, we have $$\lim_{x \to 0}\frac{1-\cos x}{x^2}$$ which is equal to $\frac12$ . Here is the graph of $(1 - \cos x)$ and $x^2$ : How is this limit approaching $\frac12$ ? How to see it approaching $\frac12$ by the graph as in the case of $\lim_{x \to 0}\frac{\sin x}{x}$ ?","We know because behaves like locally. It can be seen from the following graph: We can see as approaches , behaves more and more like which makes the limit equal to . Now, let's see another limit. This time, we have which is equal to . Here is the graph of and : How is this limit approaching ? How to see it approaching by the graph as in the case of ?",\lim_{x \to 0}\frac{\sin x}{x} = 1 \sin x x x 0 \sin x y = x \lim_{x \to 0}\frac{\sin x}{x} 1 \lim_{x \to 0}\frac{1-\cos x}{x^2} \frac12 (1 - \cos x) x^2 \frac12 \frac12 \lim_{x \to 0}\frac{\sin x}{x},"['limits', 'graphing-functions']"
4,"$ f(x)=\begin{cases} 0 & \text{if $x$ is irrat. } \\ \sin |x| &\text{if $x$ is rat. }\end{cases} $, Show $ \lim_{x \to x_0 }f(x) =0 $ by Heine def.","xx, Show  by Heine def.", f(x)=\begin{cases} 0 & \text{if   is irrat. } \\ \sin |x| &\text{if   is rat. }\end{cases}   \lim_{x \to x_0 }f(x) =0 ,"Problem : $ f(x)=\begin{cases} 0  & \text{if $x$ is irrational} \\ \sin |x|  &\text{if $x$ is rational}\end{cases} $ Let $ x_0 \in \{ \pi n : n \in \Bbb Z \} $ . Show that $ \lim_{x \to x_0 }f(x) $ exists or not, if it exists, find it. Attempt: Let $ \epsilon > 0$ be arbitrary.  Since we know that $ \sin |x|  $ is continuous then there exists $ \delta >0 $ . Let $ x \in \Bbb R $ be arbitrary. Suppose $ | x- x_0 | < \delta $ . Then we know $ | \sin|x| | <\epsilon $ ( since $ \lim_{x \to 0 } \sin |x| = 0 $ ). Now, If x is irrational, $ f(x) = 0  $ and so $ |f(x)| = 0 < \epsilon $ . If x is rational, $f(x) = \sin|x| $ and so $ |f(x)| = |\sin |x| | < \epsilon $ . Since $ \epsilon>0, x \in \Bbb R  $ were arbitrary, we showed that $ \lim_{x \to x_0 }f(x) = 0 $ . $ \square $ Question: Initially I tried to solve the problem using Heine's definition of limit but I found it to be very difficult. How would one prove the limit above using Heine's definition?","Problem : Let . Show that exists or not, if it exists, find it. Attempt: Let be arbitrary.  Since we know that is continuous then there exists . Let be arbitrary. Suppose . Then we know ( since ). Now, If x is irrational, and so . If x is rational, and so . Since were arbitrary, we showed that . Question: Initially I tried to solve the problem using Heine's definition of limit but I found it to be very difficult. How would one prove the limit above using Heine's definition?"," f(x)=\begin{cases} 0  & \text{if x is irrational}
\\ \sin |x|  &\text{if x is rational}\end{cases}   x_0 \in \{ \pi n : n \in \Bbb Z \}   \lim_{x \to x_0 }f(x)   \epsilon > 0  \sin |x|    \delta >0   x \in \Bbb R   | x- x_0 | < \delta   | \sin|x| | <\epsilon   \lim_{x \to 0 } \sin |x| = 0   f(x) = 0    |f(x)| = 0 < \epsilon  f(x) = \sin|x|   |f(x)| = |\sin |x| | < \epsilon   \epsilon>0, x \in \Bbb R    \lim_{x \to x_0 }f(x) = 0   \square ","['real-analysis', 'limits']"
5,$f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}}$ and $g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}}$ then evaluate $\lim_{x\to\infty}g(x)=$,and  then evaluate,f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}} g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}} \lim_{x\to\infty}g(x)=,"Let $$f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R$$ and $$g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R$$ Also, $g(0)=0,f(0)=10$ and $\lim_{x\to\infty}f(x)=0$ , then find value of $\lim_{x\to\infty}g(x)$ My Attempt: I could do till here only $(f'(x))^2+(g'(x)-1)^2=1$","Let and Also, and , then find value of My Attempt: I could do till here only","f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R g(0)=0,f(0)=10 \lim_{x\to\infty}f(x)=0 \lim_{x\to\infty}g(x) (f'(x))^2+(g'(x)-1)^2=1","['calculus', 'limits', 'derivatives']"
6,"Verification of my attempt to calculate $\lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}(1-\frac{\cos^2{t}}{t})\, dt$",Verification of my attempt to calculate,"\lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}(1-\frac{\cos^2{t}}{t})\, dt","For calculating $$\lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt$$ I have thought to apply the mean value theorem. In fact thanks to the continuity of $1-\frac{\cos^2{t}}{t}$ I can say that $\exists c\in[2, x]:$ $\int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt=f(c)(x-2)$ . So $$\lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt=\lim_{x\to \infty} \frac{1}{x}f(c)(x-2)=1$$ since $f(c)=(1-\frac{\cos^2{c}}{c})=1$ because $c\to \infty$ when $x\to\infty$ . Is it right my work?",For calculating I have thought to apply the mean value theorem. In fact thanks to the continuity of I can say that . So since because when . Is it right my work?,"\lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt 1-\frac{\cos^2{t}}{t} \exists c\in[2, x]: \int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt=f(c)(x-2) \lim_{x\to \infty} \frac{1}{x}\int_{2}^{x}\left(1-\frac{\cos^2{t}}{t}\right)\, dt=\lim_{x\to \infty} \frac{1}{x}f(c)(x-2)=1 f(c)=(1-\frac{\cos^2{c}}{c})=1 c\to \infty x\to\infty","['real-analysis', 'integration', 'limits', 'solution-verification']"
7,When to simply plug in infinity when evaluating limits to infinity.,When to simply plug in infinity when evaluating limits to infinity.,,"Please don't destroy me as I am teaching myself calculus. Anyway, my question pertains to this problem: The problem with solution. This is the solution to the limit of $x-$ $\sqrt{x^2-7}$ as $x$ approaches - $\infty$ . My question is, when is it okay to just plug in - $\infty$ like they did in the solution? Also, when I did the problem, I got an answer of 0. Here is my step by step solution: My solution Feel free to critique it and give me and tips, tricks, and advice. Thank you! Edit: I made an error. The denominator in my solution should be a 0 as -1 +1 = 0.My final result based on the work I did should be 0/0.  Nevertheless, my answer is still incorrect.","Please don't destroy me as I am teaching myself calculus. Anyway, my question pertains to this problem: The problem with solution. This is the solution to the limit of as approaches - . My question is, when is it okay to just plug in - like they did in the solution? Also, when I did the problem, I got an answer of 0. Here is my step by step solution: My solution Feel free to critique it and give me and tips, tricks, and advice. Thank you! Edit: I made an error. The denominator in my solution should be a 0 as -1 +1 = 0.My final result based on the work I did should be 0/0.  Nevertheless, my answer is still incorrect.",x- \sqrt{x^2-7} x \infty \infty,"['calculus', 'limits', 'soft-question', 'infinity']"
8,If $a_{n_k}$ denotes a subsequence of $a_n$. Why is $n_k \geq k$ for all $k$.?,If  denotes a subsequence of . Why is  for all .?,a_{n_k} a_n n_k \geq k k,"I am trying to prove that If a sequence converges, then every subsequence converges to the same limit. Now going in the backwards direction, I know it is true because the sequence is a subsequence of itself. But what about proving if a sequence converges then every subsequence converges to the same limit. I do not understand the notation $a_{n_k}$ , $a_n$ , $n_k$ and $k$ I know that subsequence need to be in order just like in the original sequence. Can someone please clarify the notation and explain why $n_k \geq k$ for all $k$ .?","I am trying to prove that If a sequence converges, then every subsequence converges to the same limit. Now going in the backwards direction, I know it is true because the sequence is a subsequence of itself. But what about proving if a sequence converges then every subsequence converges to the same limit. I do not understand the notation , , and I know that subsequence need to be in order just like in the original sequence. Can someone please clarify the notation and explain why for all .?",a_{n_k} a_n n_k k n_k \geq k k,"['sequences-and-series', 'limits', 'solution-verification']"
9,Why is my proof of the integral of the dirac function being equal to 1 incorrect?,Why is my proof of the integral of the dirac function being equal to 1 incorrect?,,"I know that there are questions on this website on how to prove that the integral of the Dirac function is $1$ . However, I am interested in knowing why my proof is wrong and if there is a way to salvage it. So here's the question : The Dirac delta function can be defined by the limit of a short pulse: $$\delta(t-t_0) = \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)})$$ where $f_{\Delta}(t) = \frac{1}{\Delta}$ for $t_0\leq t\leq t_0 + \Delta$ and $f_{\Delta}(t) = 0$ otherwise. Convince yourself that the integral $\int_{t_1}^{t_2} \delta(t-t_0) dt = 1$ if $t_1\leq t_0 \leq t_2 $ Here's my approach: $$\int_{t_1}^{t_2} \delta(t-t_0) dt = \int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)}) dt$$ $$=\int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})dt$$ $$=\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})\int_{t_1}^{t_2}dt$$ $$=\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta}) \cdot (t_2-t_1)$$ $$=\lim_{\Delta \rightarrow 0}(\frac{t_2-t_1}{\Delta})$$ If we let $t_1 = t_0$ and $t_2 = t_0 + \Delta$ , then we get $$\lim_{\Delta \rightarrow 0}(\frac{\Delta}{\Delta})$$ $$ = 1$$ Here's why I doubt my proof. First, in the third line of my development, I took out the limit from the integral. My reasoning behind the maneuver is that this limit does not depend on $t$ and therefore I can treat it as a constant. However, upon re-reading my proof, I am wondering if I am really ""allowed"" to do this. Since I am treating the limit as a constant, I am assuming that it exists and is finite, which is not the case, right? (since $\lim_{x \rightarrow 0} (1/x)$ does not exist). Secondly, to get to the 6th line, I defined $t_2 = t_0 + \Delta$ . Am I allowed to do this? I defined it that way only because it was mathematically convenient, not because it actually makes sense, which is why I doubt its validity. I personally think that the proof is incorrect, but I'm not sure. Perhaps one could argue otherwise and I would be interested to know how. So, can you confirm to me that my proof is incorrect, and can you please let me know if there's anything I can do to ""salvage"" it, or if I need to try another approach","I know that there are questions on this website on how to prove that the integral of the Dirac function is . However, I am interested in knowing why my proof is wrong and if there is a way to salvage it. So here's the question : The Dirac delta function can be defined by the limit of a short pulse: where for and otherwise. Convince yourself that the integral if Here's my approach: If we let and , then we get Here's why I doubt my proof. First, in the third line of my development, I took out the limit from the integral. My reasoning behind the maneuver is that this limit does not depend on and therefore I can treat it as a constant. However, upon re-reading my proof, I am wondering if I am really ""allowed"" to do this. Since I am treating the limit as a constant, I am assuming that it exists and is finite, which is not the case, right? (since does not exist). Secondly, to get to the 6th line, I defined . Am I allowed to do this? I defined it that way only because it was mathematically convenient, not because it actually makes sense, which is why I doubt its validity. I personally think that the proof is incorrect, but I'm not sure. Perhaps one could argue otherwise and I would be interested to know how. So, can you confirm to me that my proof is incorrect, and can you please let me know if there's anything I can do to ""salvage"" it, or if I need to try another approach",1 \delta(t-t_0) = \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)}) f_{\Delta}(t) = \frac{1}{\Delta} t_0\leq t\leq t_0 + \Delta f_{\Delta}(t) = 0 \int_{t_1}^{t_2} \delta(t-t_0) dt = 1 t_1\leq t_0 \leq t_2  \int_{t_1}^{t_2} \delta(t-t_0) dt = \int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)}) dt =\int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})dt =\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})\int_{t_1}^{t_2}dt =\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta}) \cdot (t_2-t_1) =\lim_{\Delta \rightarrow 0}(\frac{t_2-t_1}{\Delta}) t_1 = t_0 t_2 = t_0 + \Delta \lim_{\Delta \rightarrow 0}(\frac{\Delta}{\Delta})  = 1 t \lim_{x \rightarrow 0} (1/x) t_2 = t_0 + \Delta,"['integration', 'limits', 'dirac-delta']"
10,Epsilon-delta proofs,Epsilon-delta proofs,,"$\newcommand{\absval}[1]{\left\lvert #1 \right\rvert}$ I am self-learning Real Analysis from Understanding Analysis by Stephen Abott. I am getting back to $\delta-\epsilon$ arguments after a break, so I'd like ask if my $\delta$ -response to the below $\epsilon$ -challenges is technically correct and rigorous. Problem 4.2.2. For each stated limit, find the largest possible $\delta-$ neighbourhood that is a proper response to the given $\epsilon$ -challenge. (a) $\lim_{x \to 3}(5x - 6) = 9$ , where $\epsilon = 1$ (b) $\lim_{x \to 4}\sqrt{x} = 2$ , where $\epsilon = 1$ (c) $\lim_{x \to \pi} [[x]] = 3$ , where $\epsilon = 1$ . (The function $[[x]]$ returns the greatest integer less than or equal to $x$ ) (d) $\lim_{x \to \pi} [[x]] = 3$ , where $\epsilon = 0.01$ Proof. (a) We would like to make the distance $\absval{(5x - 6) - 9}<1$ , so $\absval{5x - 15} < 1$ , thus $\absval{x - 3} < 1/5$ . Thus, $\delta = 1/5$ . (b) The expression $\absval{\sqrt{x} - 2}$ can be written as, \begin{align*} 	\sqrt{x} - 2 &= (\sqrt{x} - 2) \times \frac{(\sqrt{x} + 2)}{(\sqrt{x} + 2)}\\ 	&= \frac{\absval{(x - 4)}}{\absval{\sqrt{x} + 2}}\\ 	&\le \frac{\absval{(x - 4)}}{2}\\ 	&< \frac{\delta}{2} \end{align*} So, if we pick $\delta = 2$ , the distance $\absval{\sqrt{x} - 2}$ would be smaller than $\epsilon = 1$ . (c) Consider the expression $\absval{[[x]] - 3}$ . We are interested to make the distance $\absval{[[x]] - 3}$ smaller than $\epsilon = 1$ . So, $2 < [[x]] < 4$ . Therefore, $[[x]]=3$ . This is true, if and only if, $3 \le x < 4$ . Consequently, $3 - \pi \le x - \pi < 4 - \pi$ . This inequality is satisfied, if the distance $\absval{x - \pi} < \pi - 3$ . So, $\delta = \pi - 3$ . (d) Again, $\delta = \pi - 3$ is a proper response to the given $\epsilon-$ challenge.","I am self-learning Real Analysis from Understanding Analysis by Stephen Abott. I am getting back to arguments after a break, so I'd like ask if my -response to the below -challenges is technically correct and rigorous. Problem 4.2.2. For each stated limit, find the largest possible neighbourhood that is a proper response to the given -challenge. (a) , where (b) , where (c) , where . (The function returns the greatest integer less than or equal to ) (d) , where Proof. (a) We would like to make the distance , so , thus . Thus, . (b) The expression can be written as, So, if we pick , the distance would be smaller than . (c) Consider the expression . We are interested to make the distance smaller than . So, . Therefore, . This is true, if and only if, . Consequently, . This inequality is satisfied, if the distance . So, . (d) Again, is a proper response to the given challenge.","\newcommand{\absval}[1]{\left\lvert #1 \right\rvert} \delta-\epsilon \delta \epsilon \delta- \epsilon \lim_{x \to 3}(5x - 6) = 9 \epsilon = 1 \lim_{x \to 4}\sqrt{x} = 2 \epsilon = 1 \lim_{x \to \pi} [[x]] = 3 \epsilon = 1 [[x]] x \lim_{x \to \pi} [[x]] = 3 \epsilon = 0.01 \absval{(5x - 6) - 9}<1 \absval{5x - 15} < 1 \absval{x - 3} < 1/5 \delta = 1/5 \absval{\sqrt{x} - 2} \begin{align*}
	\sqrt{x} - 2 &= (\sqrt{x} - 2) \times \frac{(\sqrt{x} + 2)}{(\sqrt{x} + 2)}\\
	&= \frac{\absval{(x - 4)}}{\absval{\sqrt{x} + 2}}\\
	&\le \frac{\absval{(x - 4)}}{2}\\
	&< \frac{\delta}{2}
\end{align*} \delta = 2 \absval{\sqrt{x} - 2} \epsilon = 1 \absval{[[x]] - 3} \absval{[[x]] - 3} \epsilon = 1 2 < [[x]] < 4 [[x]]=3 3 \le x < 4 3 - \pi \le x - \pi < 4 - \pi \absval{x - \pi} < \pi - 3 \delta = \pi - 3 \delta = \pi - 3 \epsilon-","['real-analysis', 'limits', 'solution-verification', 'epsilon-delta']"
11,Estimations of some new recurrence sequences,Estimations of some new recurrence sequences,,"Problem 1 . Given the recursion $a_{n+ 1}= \sqrt{a_{n}^{2}+ a_{n}}$ with $a_{1}= 1.$ Prove that $$a_{n}\sim\frac{n}{2}+ \frac{1}{2n}\,{\rm as}\,n\rightarrow\infty$$ For problem 1 , I only can prove $a_{n}\sim\left ( n+ 1 \right )/2$ by using the Laurent series for the increasing $a_{n} :$ $$a_{n+ 1}= a_{n}+ \dfrac{1}{2}+ \mathcal{O}\left ( \dfrac{1}{a_{n}} \right )$$ Problem 2 . Given the recursion $a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ a_{n}}$ with $a_{1}= 1.$ Prove that $$a_{n+ 1}\sim\sqrt{\dfrac{2n}{3}}\,{\rm as}\,n\rightarrow\infty$$ What if we changed the given one by $a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ 3a_{n}^{2}+ a_{n}}$ ? For problem 2 , similarly, I saw that $a_{n+ 1}\fallingdotseq a_{n}+ \dfrac{1}{3a_{n}}$ , then how can we find the relative equations like ${y}'= \dfrac{1}{3y}$ by $n$ times ?","Problem 1 . Given the recursion with Prove that For problem 1 , I only can prove by using the Laurent series for the increasing Problem 2 . Given the recursion with Prove that What if we changed the given one by ? For problem 2 , similarly, I saw that , then how can we find the relative equations like by times ?","a_{n+ 1}= \sqrt{a_{n}^{2}+ a_{n}} a_{1}= 1. a_{n}\sim\frac{n}{2}+ \frac{1}{2n}\,{\rm as}\,n\rightarrow\infty a_{n}\sim\left ( n+ 1 \right )/2 a_{n} : a_{n+ 1}= a_{n}+ \dfrac{1}{2}+ \mathcal{O}\left ( \dfrac{1}{a_{n}} \right ) a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ a_{n}} a_{1}= 1. a_{n+ 1}\sim\sqrt{\dfrac{2n}{3}}\,{\rm as}\,n\rightarrow\infty a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ 3a_{n}^{2}+ a_{n}} a_{n+ 1}\fallingdotseq a_{n}+ \dfrac{1}{3a_{n}} {y}'= \dfrac{1}{3y} n","['limits', 'recurrence-relations']"
12,"what does the professor mean by ""the convergence to the limit point is fast?""","what does the professor mean by ""the convergence to the limit point is fast?""",,"I am considering the following facility location problem: Let $S$ be a subset of interval $[0,1]$ , which denotes the set of positions of agents . With a little abuse of notation, $S$ also denotes the set of all agents. For instance, $S=\{1, \frac{1}{2}, \frac{1}{3}, \cdots\}$ indicates that there is an agent in $1, \frac{1}{2}, \frac{1}{3}, \cdots$ , respectively. Now we need to locate a facility on the real line(to serve all the agents). Suppose the $y\in \mathbb{R}$ is the location of the facility, then the cost of agent $x\in S$ is definte as $cost(x, y):=|x-y|$ . The total cost is the sum of all agents' cost, i.e., $\sum_{x\in S} |x-y|$ . In a discussion with a professor by email, he made the following three observations and asked me to check their correctness: If $y$ is not a limit point of $S$ then the total cost is infinite. The total cost is finite iff $y$ is the only limit point of $S$ and the convergent to the limit point is fast . Suppose there are $m$ facilities, define the cost of an agent to be the distance to the closest facility. Then the total cost is bounded iff each facility is in a limit point of $S$ and the convergence to each facility is fast . I understood his observations except the ""convergence to the limit point is fast"". And I proposed $S=\{1, \frac{1}{2}, \frac{1}{3},\cdots\}$ as an counter example to observation $2$ since the only limit point here is $0$ but the total cost $\lim_{n=1}^{\infty}\frac{1}{n}$ is unbounded. He argued that it is not a counter example because the convergence is slow . I've learnt the concepts of the rate of convergence and the order of convergence , but what does he mean by the convergence is fast or slow? I think there must be some missing prerequisite knowledge I should know first.","I am considering the following facility location problem: Let be a subset of interval , which denotes the set of positions of agents . With a little abuse of notation, also denotes the set of all agents. For instance, indicates that there is an agent in , respectively. Now we need to locate a facility on the real line(to serve all the agents). Suppose the is the location of the facility, then the cost of agent is definte as . The total cost is the sum of all agents' cost, i.e., . In a discussion with a professor by email, he made the following three observations and asked me to check their correctness: If is not a limit point of then the total cost is infinite. The total cost is finite iff is the only limit point of and the convergent to the limit point is fast . Suppose there are facilities, define the cost of an agent to be the distance to the closest facility. Then the total cost is bounded iff each facility is in a limit point of and the convergence to each facility is fast . I understood his observations except the ""convergence to the limit point is fast"". And I proposed as an counter example to observation since the only limit point here is but the total cost is unbounded. He argued that it is not a counter example because the convergence is slow . I've learnt the concepts of the rate of convergence and the order of convergence , but what does he mean by the convergence is fast or slow? I think there must be some missing prerequisite knowledge I should know first.","S [0,1] S S=\{1, \frac{1}{2}, \frac{1}{3}, \cdots\} 1, \frac{1}{2}, \frac{1}{3}, \cdots y\in \mathbb{R} x\in S cost(x, y):=|x-y| \sum_{x\in S} |x-y| y S y S m S S=\{1, \frac{1}{2}, \frac{1}{3},\cdots\} 2 0 \lim_{n=1}^{\infty}\frac{1}{n}","['real-analysis', 'limits']"
13,Limit of fourier series,Limit of fourier series,,"I want to compute the limit $f(x)$ of $$ f(n,x)=\sum_{k=1}^n\frac{1}{k^2+1}\sin(kx) $$ and didn't succeed. I tried with the means used to compute the >>easy<< limits of trigonometric series like those listed in (e.g.) Bronstein, however, the above series is not in Bronstein. I plotted the graph with sagemath (see my post on ask.sagemath.org: https://ask.sagemath.org/question/55596/limit-of-fourier-series/ ). At least this suggests that f(x) exists and is not a polynomial. Is it possible to compute the above limit and if so, how?","I want to compute the limit of and didn't succeed. I tried with the means used to compute the >>easy<< limits of trigonometric series like those listed in (e.g.) Bronstein, however, the above series is not in Bronstein. I plotted the graph with sagemath (see my post on ask.sagemath.org: https://ask.sagemath.org/question/55596/limit-of-fourier-series/ ). At least this suggests that f(x) exists and is not a polynomial. Is it possible to compute the above limit and if so, how?","f(x) 
f(n,x)=\sum_{k=1}^n\frac{1}{k^2+1}\sin(kx)
","['limits', 'fourier-series']"
14,Prove that: $ \sum_{i=0}^{\infty} \frac{\tan \frac{\theta}{2^i}}{2^i}= \frac{1}{\theta} - 2 \cot 2 \theta$,Prove that:, \sum_{i=0}^{\infty} \frac{\tan \frac{\theta}{2^i}}{2^i}= \frac{1}{\theta} - 2 \cot 2 \theta,"My attempt: Consider the following series: $$ S = \sum_{i=0}^n \ln( \sec \frac{x}{2^i} )$$ Notice that $ \lim_{n \to \infty} \frac{dS}{dx}$ is the required sum. Simplfying S, $$ S = - \ln \left(  \cos x \cdot \cos \frac{x}{2} ... \cos \frac{x}{2^n} \right)$$ or, $$ S = - \ln \left(  \frac{ \sin(2x)}{2^{n+1} \sin (\frac{x}{2^n})} \right)= \ln(2^{n+1}) + \ln( \sin \frac{x}{2^n}) - \ln( \sin(2x) )$$ Now, $$ \frac{dS}{dx}  = \frac{1}{2^n} \cot \frac{x}{2^n} - 2 \cot(2x)$$ The problem I'm having is proving that $$ \lim_{ n \to \infty} \frac{1}{2^n} \cot \frac{x}{2^n} = \frac{1}{x}$$","My attempt: Consider the following series: Notice that is the required sum. Simplfying S, or, Now, The problem I'm having is proving that", S = \sum_{i=0}^n \ln( \sec \frac{x}{2^i} )  \lim_{n \to \infty} \frac{dS}{dx}  S = - \ln \left(  \cos x \cdot \cos \frac{x}{2} ... \cos \frac{x}{2^n} \right)  S = - \ln \left(  \frac{ \sin(2x)}{2^{n+1} \sin (\frac{x}{2^n})} \right)= \ln(2^{n+1}) + \ln( \sin \frac{x}{2^n}) - \ln( \sin(2x) )  \frac{dS}{dx}  = \frac{1}{2^n} \cot \frac{x}{2^n} - 2 \cot(2x)  \lim_{ n \to \infty} \frac{1}{2^n} \cot \frac{x}{2^n} = \frac{1}{x},"['sequences-and-series', 'limits']"
15,"Is this proof of, that there is no limit of lim 1/x as x approaches 0, correct?","Is this proof of, that there is no limit of lim 1/x as x approaches 0, correct?",,"I just wanted to ask, if the following proof can be used to proof, that there is no l, such as $\lim\limits_{x \to 0} \dfrac{1}{x} = l$ . If we assume such l exists, then for every $\epsilon>0$ there is some $\delta>0$ , such that for all $x$ , if $\left|x \right|< \delta$ , then $\left| \dfrac{1}{x}-l\right|<\epsilon$ . Then $\left|\dfrac{1-xl}{x} \right|<\epsilon$ , so $\left|\dfrac{xl-1}{x} \right|<\epsilon$ , then $\dfrac{\left|xl-1 \right|}{\left|x \right|}<\epsilon$ , and finally $\left| xl-1 \right|<\epsilon\cdot\left| x \right|$ . For contradiction, we need to find $\left|x\cdot l -1 \right| \geq \epsilon\cdot \left|x \right| $ . If we choose $\epsilon =1$ , then we have, for all $\delta$ that $\dfrac{1}{n}< \delta$ , as $n\in N$ , then we can assume $x=\dfrac{1}{n}$ . From this we have $\left|\dfrac{l}{n}-1 \right|=\left|xl-1 \right|<\epsilon\cdot \left| x \right|=\left| \dfrac{1}{n}\right|$ , furthermore $\left|l-n \right|<1$ , but there is certainly some combination of l,n for which $\left|x-l \right|\geq 1 = \epsilon$ , which contradicts our assumption about limits. In conclusion, there is no such l for which $\lim\limits_{x \to 0}\dfrac{1}{x}=l$ works.","I just wanted to ask, if the following proof can be used to proof, that there is no l, such as . If we assume such l exists, then for every there is some , such that for all , if , then . Then , so , then , and finally . For contradiction, we need to find . If we choose , then we have, for all that , as , then we can assume . From this we have , furthermore , but there is certainly some combination of l,n for which , which contradicts our assumption about limits. In conclusion, there is no such l for which works.",\lim\limits_{x \to 0} \dfrac{1}{x} = l \epsilon>0 \delta>0 x \left|x \right|< \delta \left| \dfrac{1}{x}-l\right|<\epsilon \left|\dfrac{1-xl}{x} \right|<\epsilon \left|\dfrac{xl-1}{x} \right|<\epsilon \dfrac{\left|xl-1 \right|}{\left|x \right|}<\epsilon \left| xl-1 \right|<\epsilon\cdot\left| x \right| \left|x\cdot l -1 \right| \geq \epsilon\cdot \left|x \right|  \epsilon =1 \delta \dfrac{1}{n}< \delta n\in N x=\dfrac{1}{n} \left|\dfrac{l}{n}-1 \right|=\left|xl-1 \right|<\epsilon\cdot \left| x \right|=\left| \dfrac{1}{n}\right| \left|l-n \right|<1 \left|x-l \right|\geq 1 = \epsilon \lim\limits_{x \to 0}\dfrac{1}{x}=l,"['real-analysis', 'calculus', 'limits', 'solution-verification', 'epsilon-delta']"
16,Limit of difference of pseudo-arcsinh and arcsinh,Limit of difference of pseudo-arcsinh and arcsinh,,"$$\DeclareMathOperator{\erf}{erf}$$ $$\DeclareMathOperator{\arcsinh}{arcsinh}$$ Let $f$ a function defined by : $$f(x) = \int_0^x \frac{\erf(u)}{u}du$$ Where $\erf$ is the error-function. This function look like $\arcsinh(x)$ : In black there is $f(x)$ and in red this is $\arcsinh(x)$ . Now, we can see (or at least conjecture) that $$\kappa := \lim_\limits{x \rightarrow +\infty} \left[ \int_0^x \frac{\erf(u)}{u}du - \arcsinh(x)\right]$$ exists and is finite. That would imply that $f(x) \sim \kappa + \arcsinh(x)$ and more accuratly $f(x) \approx \kappa + \arcsinh(x)$ . By using $\arcsinh(x) \sim \ln(2x)$ we would have $$f(x) \sim \kappa + \ln(2x)$$ Now, heres the graph of $f(x)-\arcsinh(x)$ : In black this is $f(x)-\arcsinh(x)$ . We can easily see that it converges toward $\kappa$ (in red). Here, we have $\kappa \approx 0.288607829951$ . My question is, what is the exact value of $\kappa$ ?","Let a function defined by : Where is the error-function. This function look like : In black there is and in red this is . Now, we can see (or at least conjecture) that exists and is finite. That would imply that and more accuratly . By using we would have Now, heres the graph of : In black this is . We can easily see that it converges toward (in red). Here, we have . My question is, what is the exact value of ?",\DeclareMathOperator{\erf}{erf} \DeclareMathOperator{\arcsinh}{arcsinh} f f(x) = \int_0^x \frac{\erf(u)}{u}du \erf \arcsinh(x) f(x) \arcsinh(x) \kappa := \lim_\limits{x \rightarrow +\infty} \left[ \int_0^x \frac{\erf(u)}{u}du - \arcsinh(x)\right] f(x) \sim \kappa + \arcsinh(x) f(x) \approx \kappa + \arcsinh(x) \arcsinh(x) \sim \ln(2x) f(x) \sim \kappa + \ln(2x) f(x)-\arcsinh(x) f(x)-\arcsinh(x) \kappa \kappa \approx 0.288607829951 \kappa,"['calculus', 'limits']"
17,Prove $x_n=\frac{n}{2^n}$ converges to $0$,Prove  converges to,x_n=\frac{n}{2^n} 0,"Prove $x_n=\frac{n}{2^n}$ converges to $0$ . Since $2^n>\frac{n^2}{2}$ for all $n \in \mathbb{N}$ we have $$-\frac{2}{n}<\frac{n}{2^n}<\frac{2}{n}.$$ Since $\frac{1}{n} \rightarrow0$ as $n \rightarrow \infty$ , by Squeeze theoreom $\frac{n}{2^n}$ converges to $0$ . Is there anything wrong this proof?","Prove converges to . Since for all we have Since as , by Squeeze theoreom converges to . Is there anything wrong this proof?",x_n=\frac{n}{2^n} 0 2^n>\frac{n^2}{2} n \in \mathbb{N} -\frac{2}{n}<\frac{n}{2^n}<\frac{2}{n}. \frac{1}{n} \rightarrow0 n \rightarrow \infty \frac{n}{2^n} 0,"['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
18,Function satisfying the relation $f(x+y)=f(x)+f(y)-(e^{-x}-1)(e^{-y}-1)+1$,Function satisfying the relation,f(x+y)=f(x)+f(y)-(e^{-x}-1)(e^{-y}-1)+1,"Let f be the differentiable function satisfying the relation $f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ ; $\forall x,y \in R$ and $\mathop {\lim }\limits_{h \to 0} \frac{{f'\left( {1 + h} \right) + f\left( h \right) - {e^{ - 1}}}}{h}$ exist. The value of $\int\limits_0^1 {f\left( x \right)dx}  = \_\_\_\_\_\_\_$ . My approach is as follow $f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ $\Rightarrow f\left( {x + y} \right) - f\left( x \right) = f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ $ \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}$ $ \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}$ y=h $ \Rightarrow f'\left( x \right) = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + h} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( h \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - h}} - 1} \right) + 1}}{h}$ How will I proceed from here",Let f be the differentiable function satisfying the relation ; and exist. The value of . My approach is as follow y=h How will I proceed from here,"f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1 \forall x,y \in R \mathop {\lim }\limits_{h \to 0} \frac{{f'\left( {1 + h} \right) + f\left( h \right) - {e^{ - 1}}}}{h} \int\limits_0^1 {f\left( x \right)dx}  = \_\_\_\_\_\_\_ f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1 \Rightarrow f\left( {x + y} \right) - f\left( x \right) = f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1  \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}  \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}  \Rightarrow f'\left( x \right) = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + h} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( h \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - h}} - 1} \right) + 1}}{h}","['integration', 'limits']"
19,Calculating the limit of the following series,Calculating the limit of the following series,,"I want to prove that $$\lim_{x\to1^-}(1-x)\sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} = \frac14$$ So far I tried to manipulate the series for instance using $$\sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} =  -\sum_{n=1}^{\infty}(-1)^{n}nx^n\sum_{m=0}^{\infty}\left(x^{2n}\right)^m$$ since $x < 1$ . Interchanging the two sums (not sure if allowed) I obtained, assuming I did not make mistakes, the sum $$\sum_{m=0}^{\infty}\frac{x^{2m+1}}{(1+x^{2m+1})^2}$$ I am unable to continue from this point. Perhaps my work isn't actually useful at all. Can you help me?","I want to prove that So far I tried to manipulate the series for instance using since . Interchanging the two sums (not sure if allowed) I obtained, assuming I did not make mistakes, the sum I am unable to continue from this point. Perhaps my work isn't actually useful at all. Can you help me?",\lim_{x\to1^-}(1-x)\sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} = \frac14 \sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} =  -\sum_{n=1}^{\infty}(-1)^{n}nx^n\sum_{m=0}^{\infty}\left(x^{2n}\right)^m x < 1 \sum_{m=0}^{\infty}\frac{x^{2m+1}}{(1+x^{2m+1})^2},"['real-analysis', 'sequences-and-series', 'limits']"
20,What is the answer of $ \lim_{n\to \infty}\frac{{(-1)}^{(n-1)}\cdot n}{(n+1)!}$,What is the answer of, \lim_{n\to \infty}\frac{{(-1)}^{(n-1)}\cdot n}{(n+1)!},"I found a limit question in my textbook but i could not obtained the result given by answer key. Let me introduce the question : $ \lim_{n\to \infty}\frac{{(-1)}^{(n-1)}\cdot n}{(n+1)!}$ I tried to separate it into two part such that $ \lim_{n\to \infty}{(-1)}^{(n-1)}\cdot\lim_{n\to \infty}\frac{n}{(n+1)!}$ As you see, this partition gave me "" $\text{oscillating (undetermined)}$ "". $0$ However, the answer is zero. What am I missing? Thank you for your helps ...","I found a limit question in my textbook but i could not obtained the result given by answer key. Let me introduce the question : I tried to separate it into two part such that As you see, this partition gave me "" "". However, the answer is zero. What am I missing? Thank you for your helps ...", \lim_{n\to \infty}\frac{{(-1)}^{(n-1)}\cdot n}{(n+1)!}  \lim_{n\to \infty}{(-1)}^{(n-1)}\cdot\lim_{n\to \infty}\frac{n}{(n+1)!} \text{oscillating (undetermined)} 0,['calculus']
21,How to find $\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n$,How to find,\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n,"Let $c \neq 1$ be a postive real number. Find the following limit $$\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n.$$ I know that $\lim_{n \to \infty} \Big( 1 + \frac{c}{n} \Big)^{bn} = e^{bc}$ . But it cannot be used here, since $\ln(n)$ is not a constant.  I also know how to find the $\lim_{n \to \infty} \Big( 1 + \frac{1}{n^2} \Big)^{n}$ , which is maybe closer to our limit. I tried to use some methods (e.g., L'Hopital's rule) that appear in the proofs of the above limits to solve also $\lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n$ but without success. The graphs of $\Big (1 - \frac{c \ln(n)}{n} \Big)^n$ indicate that the limit is $\infty$ if $c < 1$ and the limit is 0 otherwise. I appreciate any suggestions on how to solve this limit. Also, any readings on this topic (I did not find any such example anywhere) are appreciated.","Let be a postive real number. Find the following limit I know that . But it cannot be used here, since is not a constant.  I also know how to find the , which is maybe closer to our limit. I tried to use some methods (e.g., L'Hopital's rule) that appear in the proofs of the above limits to solve also but without success. The graphs of indicate that the limit is if and the limit is 0 otherwise. I appreciate any suggestions on how to solve this limit. Also, any readings on this topic (I did not find any such example anywhere) are appreciated.",c \neq 1 \lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n. \lim_{n \to \infty} \Big( 1 + \frac{c}{n} \Big)^{bn} = e^{bc} \ln(n) \lim_{n \to \infty} \Big( 1 + \frac{1}{n^2} \Big)^{n} \lim_{n \to \infty} \Big (1 - \frac{c \ln(n)}{n} \Big)^n \Big (1 - \frac{c \ln(n)}{n} \Big)^n \infty c < 1,"['real-analysis', 'limits']"
22,Is this approach to the limit solution kosher?$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e}$,Is this approach to the limit solution kosher?, \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e},"Here is a non-rigourous approach to a limit: $$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e}$$ I know the solution is correct. I'm suspicious of my methods. Rewrite the expression: $$ \lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)^{\frac{1}{n}}$$ I know the $\lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)=0$ . So bear with me. $$\frac{n!}{n^n}=\frac{n(n-1)(n-2)...[n-(n-2)][n-(n-1)]}{n^n}$$ Written this way, the numerator is a polynomial of degree $n$ . For large $n$ , the leading term will prevail. I will write that polynomial as a sum of $n^n$ and a polynomial of degree $n-1$ . $$\frac{n!}{n^n}\Rightarrow\frac{n^n+[\text{polynomial}]^{n-1}}{n^n}\Rightarrow 1+\frac{1}{n}$$ As noted in Step 2, this is not the expected result for this limit. But watch: $$ \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{\tfrac{1}{n}}=\lim_{n\rightarrow\infty}\frac{1}{\left(1+\frac{1}{n}\right)^n}=\frac{1}{e}$$ This is the correct solution. But Step 2 is sketchy. Did I make another error which undid this error? Can this derivation be salvaged, or is my proof ""not even wrong""? Please be gentle with me.","Here is a non-rigourous approach to a limit: I know the solution is correct. I'm suspicious of my methods. Rewrite the expression: I know the . So bear with me. Written this way, the numerator is a polynomial of degree . For large , the leading term will prevail. I will write that polynomial as a sum of and a polynomial of degree . As noted in Step 2, this is not the expected result for this limit. But watch: This is the correct solution. But Step 2 is sketchy. Did I make another error which undid this error? Can this derivation be salvaged, or is my proof ""not even wrong""? Please be gentle with me.", \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\frac{1}{e}  \lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)^{\frac{1}{n}} \lim_{n\rightarrow\infty}\left(\frac{n!}{n^n}\right)=0 \frac{n!}{n^n}=\frac{n(n-1)(n-2)...[n-(n-2)][n-(n-1)]}{n^n} n n n^n n-1 \frac{n!}{n^n}\Rightarrow\frac{n^n+[\text{polynomial}]^{n-1}}{n^n}\Rightarrow 1+\frac{1}{n}  \lim_{n\rightarrow\infty}\frac{(n!)^{\tfrac{1}{n}}}{n}=\lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{\tfrac{1}{n}}=\lim_{n\rightarrow\infty}\frac{1}{\left(1+\frac{1}{n}\right)^n}=\frac{1}{e},"['limits', 'solution-verification', 'limits-without-lhopital']"
23,Finding $\lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx$,Finding,\lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx,"I have to calculate the following limits, using a theorem but I don't really know what theorem to use (it is for the subject of measurement and integration, for the unit ""Measurable functions, integration and its properties""). $\space$ $$\lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx$$ Do I have to use the dominated convergence theorem of Lebesgue? I have first of all calculated $\lim\limits_{n \to \infty}\frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)}$ and I've obtained $\frac{1}{2x-3}$ Now, I want to calculate $\int_{2}^{\infty} \frac{1}{2x-3}\,dx$ but $\ln(\infty)$ doesn't exist... so what am I doing wrong?","I have to calculate the following limits, using a theorem but I don't really know what theorem to use (it is for the subject of measurement and integration, for the unit ""Measurable functions, integration and its properties""). Do I have to use the dominated convergence theorem of Lebesgue? I have first of all calculated and I've obtained Now, I want to calculate but doesn't exist... so what am I doing wrong?","\space \lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx \lim\limits_{n \to \infty}\frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} \frac{1}{2x-3} \int_{2}^{\infty} \frac{1}{2x-3}\,dx \ln(\infty)","['integration', 'limits', 'measure-theory']"
24,Limit of the sequence using intermediate value theorem $\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}]$,Limit of the sequence using intermediate value theorem,\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}],"Find the limit of: $$\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}]$$ where: $$\psi(x)=(1+\frac{1}{x})^x, ~~~~~~~\phi(x)=\sqrt[x]{x}$$ I used Lagrange theorem for the intermediate value for $f(x)=\frac{1}{\sin(x)}$ , which is a recommended way of solving the problem, but I am stuck now and I would ask you for some help. $$\frac{f(b)-f(a)}{b-a}=f'(c),~~~~c\in(a,b)\\ f(b)-f(a)=f'(c)\cdot(b-a) \\ \lim_{x\to\infty}\frac{\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}}{x+\psi(x)-x-\phi(x)}=\lim_{x\to\infty}\frac{\cos(\frac{1}{c})}{\sin^2(\frac{1}{c})\cdot x^2}\cdot (\psi(x)-\phi(x))$$ That's the end of my story. I have also tried $\frac{1-\cos(2\alpha)}{2}=\sin^2(\alpha)$ , but it leaded me to nowhere. I would really appreciate your hints.","Find the limit of: where: I used Lagrange theorem for the intermediate value for , which is a recommended way of solving the problem, but I am stuck now and I would ask you for some help. That's the end of my story. I have also tried , but it leaded me to nowhere. I would really appreciate your hints.","\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}] \psi(x)=(1+\frac{1}{x})^x, ~~~~~~~\phi(x)=\sqrt[x]{x} f(x)=\frac{1}{\sin(x)} \frac{f(b)-f(a)}{b-a}=f'(c),~~~~c\in(a,b)\\ f(b)-f(a)=f'(c)\cdot(b-a) \\ \lim_{x\to\infty}\frac{\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}}{x+\psi(x)-x-\phi(x)}=\lim_{x\to\infty}\frac{\cos(\frac{1}{c})}{\sin^2(\frac{1}{c})\cdot x^2}\cdot (\psi(x)-\phi(x)) \frac{1-\cos(2\alpha)}{2}=\sin^2(\alpha)","['real-analysis', 'limits', 'trigonometry']"
25,Limit to infinity rule for fractions?,Limit to infinity rule for fractions?,,"I am reading a book and it says to solve limits to infinity with a fraction such as: $$\frac{5X^2 + 8X - 3}{3X^2 + 2}$$ We divide the numerator and denominator by the highest power of X in the DENOMINATOR so in this case it is $X^2$ .  I get this helps simplify the equation, but what is to prevent someone from dividing by a higher power like $X^3$ ?  All components would evaluate to 0. Is there another rule for limits that I am not aware of? Thanks!","I am reading a book and it says to solve limits to infinity with a fraction such as: We divide the numerator and denominator by the highest power of X in the DENOMINATOR so in this case it is .  I get this helps simplify the equation, but what is to prevent someone from dividing by a higher power like ?  All components would evaluate to 0. Is there another rule for limits that I am not aware of? Thanks!",\frac{5X^2 + 8X - 3}{3X^2 + 2} X^2 X^3,"['limits', 'self-learning']"
26,Limit of function equivalent to limit of a sequence?,Limit of function equivalent to limit of a sequence?,,"When I try to compute $\lim_{n\rightarrow\infty}\frac{(\ln n)^2}{n}$ in my mathematical analysis homework, I may be supposed to use Stolz theorem to compute this limit of a sequence . However, I find it much more easier to compute this limit by treating it as a limit of function , say $\lim_{x\rightarrow\infty}\frac{(\ln x)^2}{x}$ , using L'Hospital's Rule . And I got the right answer. However, I'm still worrying about if I can replace $n$ by $x$ under any circumstances. Here are my questions Can anyone tell me under what circumstances can I replace $n$ by $x$ safely when calculating the limit? What's the relationship between Stolz theorem and L'Hospital's Rule ? (The former looks like a discrete version of the latter theorem)","When I try to compute in my mathematical analysis homework, I may be supposed to use Stolz theorem to compute this limit of a sequence . However, I find it much more easier to compute this limit by treating it as a limit of function , say , using L'Hospital's Rule . And I got the right answer. However, I'm still worrying about if I can replace by under any circumstances. Here are my questions Can anyone tell me under what circumstances can I replace by safely when calculating the limit? What's the relationship between Stolz theorem and L'Hospital's Rule ? (The former looks like a discrete version of the latter theorem)",\lim_{n\rightarrow\infty}\frac{(\ln n)^2}{n} \lim_{x\rightarrow\infty}\frac{(\ln x)^2}{x} n x n x,"['real-analysis', 'limits']"
27,Why is $\lim_{x\rightarrow 1} \sin (x^2 - 1) / (x^2 - 1) = \lim_{x\rightarrow 0} \sin(x)/x$?,Why is ?,\lim_{x\rightarrow 1} \sin (x^2 - 1) / (x^2 - 1) = \lim_{x\rightarrow 0} \sin(x)/x,"I'm confused how to prove that $\lim_{x\rightarrow 1} \sin (x^2 - 1)  / (x^2 - 1) = \lim_{x\rightarrow 0} \sin(x)/x$ . Assuming $\lim_{x\rightarrow 1} \sin (x^2 - 1)  / (x^2 - 1) = l$ , then there's $\delta > 0$ s.t. for all $x$ , if $0 < |x - 1| < \delta$ implies that $ | \sin (x^2 - 1)  / (x^2 - 1) - l| < \epsilon $ for all $\epsilon > 0$ . My approach: if we take $|x| < \delta$ , then $| (x + 1) - 1| < \delta$ this implies that $| \sin (x^2 - 2x)  / (x^2 - 2x) - l| < \epsilon$ , which probably isn't the route I need to take. Intuitively it makes sense, since $x^2 - 1$ approaches $0$ , then we should be able to replace it with its limit.","I'm confused how to prove that . Assuming , then there's s.t. for all , if implies that for all . My approach: if we take , then this implies that , which probably isn't the route I need to take. Intuitively it makes sense, since approaches , then we should be able to replace it with its limit.",\lim_{x\rightarrow 1} \sin (x^2 - 1)  / (x^2 - 1) = \lim_{x\rightarrow 0} \sin(x)/x \lim_{x\rightarrow 1} \sin (x^2 - 1)  / (x^2 - 1) = l \delta > 0 x 0 < |x - 1| < \delta  | \sin (x^2 - 1)  / (x^2 - 1) - l| < \epsilon  \epsilon > 0 |x| < \delta | (x + 1) - 1| < \delta | \sin (x^2 - 2x)  / (x^2 - 2x) - l| < \epsilon x^2 - 1 0,"['calculus', 'limits', 'limits-without-lhopital']"
28,sum of the series $(1+n)^{1/5} - n^{1/5}$ as $n$ tends to infinity,sum of the series  as  tends to infinity,(1+n)^{1/5} - n^{1/5} n,"The sum of the series ${(1+n)}^{1/5} - n^{1/5}$ when $n$ goes from 0 to infinity is (A) less than -1 (B) equal to -1 (C) greater than 1 but less than 2 (D) none of the above When I expand the summation, all the terms of the series cancel each other and what we are finally left with is $(1+n)^{1/5} - 1^{1/5}$ . Here I have assumed $1 = 1^{1/5}$ and the series is $S = (1+n)^{1/5} - 1 $ now $n>0$ so $1+n > 1$ that implies $(1+n)^{1/5} > 1 $ so, $(1+n)^{1/5} - 1 > 0 $ If n goes to infinity then this sum is greater than 0 but we cannot be sure that it is less than 2. so, I think (D) is the correct option. Am I correct ? please help me! Thanks.","The sum of the series when goes from 0 to infinity is (A) less than -1 (B) equal to -1 (C) greater than 1 but less than 2 (D) none of the above When I expand the summation, all the terms of the series cancel each other and what we are finally left with is . Here I have assumed and the series is now so that implies so, If n goes to infinity then this sum is greater than 0 but we cannot be sure that it is less than 2. so, I think (D) is the correct option. Am I correct ? please help me! Thanks.",{(1+n)}^{1/5} - n^{1/5} n (1+n)^{1/5} - 1^{1/5} 1 = 1^{1/5} S = (1+n)^{1/5} - 1  n>0 1+n > 1 (1+n)^{1/5} > 1  (1+n)^{1/5} - 1 > 0 ,"['real-analysis', 'limits', 'telescopic-series']"
29,"Calculate $\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}}$",Calculate,"\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}}","Calculate, $$\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}},$$ if there exist. My attempt: I have tried several paths, for instance: $x=0$ , $y=0$ , $y=x^m$ . In all the cases I got that the limit is $0$ . But I couldn't figure out how to prove it. Any suggestion?","Calculate, if there exist. My attempt: I have tried several paths, for instance: , , . In all the cases I got that the limit is . But I couldn't figure out how to prove it. Any suggestion?","\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}}, x=0 y=0 y=x^m 0","['calculus', 'limits', 'multivariable-calculus']"
30,How to prove that $\lim\limits_{x \to \infty}\frac{\pi\left(\frac{4x}{3}\right)}{\frac{x}{3\ln x}}=4$?,How to prove that ?,\lim\limits_{x \to \infty}\frac{\pi\left(\frac{4x}{3}\right)}{\frac{x}{3\ln x}}=4,"I'm a member of a Facebook-based mathematics group. Recently, one of the members made a post detailing an observation he made in his free time, namely that $\pi(4x/3)-\pi(x)$ (here, $\pi(x)$ denotes the prime-counting function) is approximated rather closely by $\frac{x}{3\ln(x)}$ . Knowing about the prime number theorem, I immediately recognized that this might be due to the fact that $\lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1$ . This guess turned out to be partially correct. It turns out that these two are in fact asymptotic, in the sense that $\lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)}=1$ , but I haven't been able to prove this completely. Here's my reasoning so far: The limit $\lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)}$ can be found by evaluating $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}$ and $\lim_{x\to\infty}-\frac{\pi(x)}{x/(3\ln x)}$ . The latter of these is easy to compute (just multiply the limit $\lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1$ by $-3$ and rewrite $3$ as $\frac{1}{1/3}$ ). Consulting WolframAlpha, one finds that $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4$ , so $$\lim_{x\to\infty}\frac{\pi\left(\frac{4x}{3}\right)-\pi(x)}{\frac{x}{3\ln(x)}}=4-3=1$$ My question is this: how can I prove that $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4$ ? I imagine the prime number theorem will come into play here, but I'm not sure how. In particular, I don't know how the $\frac{4}{3}$ factor that appears in the argument of $\pi(x)$ affects the limit, much less how to deal with it.","I'm a member of a Facebook-based mathematics group. Recently, one of the members made a post detailing an observation he made in his free time, namely that (here, denotes the prime-counting function) is approximated rather closely by . Knowing about the prime number theorem, I immediately recognized that this might be due to the fact that . This guess turned out to be partially correct. It turns out that these two are in fact asymptotic, in the sense that , but I haven't been able to prove this completely. Here's my reasoning so far: The limit can be found by evaluating and . The latter of these is easy to compute (just multiply the limit by and rewrite as ). Consulting WolframAlpha, one finds that , so My question is this: how can I prove that ? I imagine the prime number theorem will come into play here, but I'm not sure how. In particular, I don't know how the factor that appears in the argument of affects the limit, much less how to deal with it.",\pi(4x/3)-\pi(x) \pi(x) \frac{x}{3\ln(x)} \lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1 \lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)}=1 \lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)} \lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)} \lim_{x\to\infty}-\frac{\pi(x)}{x/(3\ln x)} \lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1 -3 3 \frac{1}{1/3} \lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4 \lim_{x\to\infty}\frac{\pi\left(\frac{4x}{3}\right)-\pi(x)}{\frac{x}{3\ln(x)}}=4-3=1 \lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4 \frac{4}{3} \pi(x),"['limits', 'asymptotics']"
31,Why use limit laws to verify continuity instead of direct substitution?,Why use limit laws to verify continuity instead of direct substitution?,,"My textbook (Calculus Early Transcendentals, 8th edition, by James Stewart ) asks to verify a function is continuous at a point using the definition of continuity and the limit laws. However, why would the text explicitly state to use the limit laws to verify continuity? I'm assuming this implies that direct substitution shouldn't be used and looking at solutions online confirms my suspicions as they all use limit laws as well. However, using direct substitution is much faster than using the limit laws and both methods achieve the same result. For example, show that $f(x) = (x+2x^3)^4$ is continuous at $x=-1$ By definition of continuity, we're trying to show that $\lim_{x\to-1}(x + 2x^3)^4 =f(-1)$ Show that $f(-1)$ exists $$f(-1)=(1+2(-1)^3)^4 = 81$$ Now for the limit Using the limit laws: $$\lim_{x\to-1}(x + 2x^3)^4$$ $$=[\lim_{x\to-1}x + 2x^3]^4$$ $$=[\lim_{x\to-1}x + \lim_{x\to-1}2x^3]^4$$ $$=[\lim_{x\to-1}x + 2\lim_{x\to-1}x^3]^4$$ $$=[-1 + 2(-1)^3]^4 = 81$$ Using direct substitution: $$\lim_{x\to-1}(x + 2x^3)^4 = (1+2(-1)^3)^4=81$$ In either method we reach the same result that $\lim_{x\to-1}(x + 2x^3)^4 = 81$ which verifies that $f(x)$ is continuous at $x=-1$ since $\lim_{x\to-1}(x + 2x^3)^4 =f(-1)$ Perhaps I'm missing some important connection between continuity and limits as to why limit laws are used instead of direction substitution to verify continuity?","My textbook (Calculus Early Transcendentals, 8th edition, by James Stewart ) asks to verify a function is continuous at a point using the definition of continuity and the limit laws. However, why would the text explicitly state to use the limit laws to verify continuity? I'm assuming this implies that direct substitution shouldn't be used and looking at solutions online confirms my suspicions as they all use limit laws as well. However, using direct substitution is much faster than using the limit laws and both methods achieve the same result. For example, show that is continuous at By definition of continuity, we're trying to show that Show that exists Now for the limit Using the limit laws: Using direct substitution: In either method we reach the same result that which verifies that is continuous at since Perhaps I'm missing some important connection between continuity and limits as to why limit laws are used instead of direction substitution to verify continuity?",f(x) = (x+2x^3)^4 x=-1 \lim_{x\to-1}(x + 2x^3)^4 =f(-1) f(-1) f(-1)=(1+2(-1)^3)^4 = 81 \lim_{x\to-1}(x + 2x^3)^4 =[\lim_{x\to-1}x + 2x^3]^4 =[\lim_{x\to-1}x + \lim_{x\to-1}2x^3]^4 =[\lim_{x\to-1}x + 2\lim_{x\to-1}x^3]^4 =[-1 + 2(-1)^3]^4 = 81 \lim_{x\to-1}(x + 2x^3)^4 = (1+2(-1)^3)^4=81 \lim_{x\to-1}(x + 2x^3)^4 = 81 f(x) x=-1 \lim_{x\to-1}(x + 2x^3)^4 =f(-1),"['calculus', 'limits', 'continuity']"
32,${f(x) = x^2 + x + 1}$ is continuous on ${\mathbb{R}}$,is continuous on,{f(x) = x^2 + x + 1} {\mathbb{R}},I need help finding the limit of the function ${f(x) = x^2 + x + 1}$ as ${x\rightarrow c}$ for any ${c \in \mathbb{R}}$ . So far I have used the limit definition and triangle inequality up to this point. $${|(x^2+x+1)-(c^2+c+1)|=|x^2-c^2+x-c|\leq|x^2-c^2|+|x-c|}$$ I do not know how to handle the $|x^2-c^2|$ part within the limit definition.,I need help finding the limit of the function as for any . So far I have used the limit definition and triangle inequality up to this point. I do not know how to handle the part within the limit definition.,{f(x) = x^2 + x + 1} {x\rightarrow c} {c \in \mathbb{R}} {|(x^2+x+1)-(c^2+c+1)|=|x^2-c^2+x-c|\leq|x^2-c^2|+|x-c|} |x^2-c^2|,"['real-analysis', 'limits', 'continuity', 'epsilon-delta']"
33,"How do I show $\lim_{n \to \infty} \int_0^\infty \frac{n}{n^2+x}\sin(\frac{1}{x})\, dx = 0\,$?",How do I show ?,"\lim_{n \to \infty} \int_0^\infty \frac{n}{n^2+x}\sin(\frac{1}{x})\, dx = 0\,","How do I show $$\lim_{n \to \infty} \int_0^\infty \frac{n}{n^2+x}\sin\left(\frac{1}{x}\right)\, dx = 0\,\,?$$ I've tried splitting into the cases where $x \leq 1$ and $x \geq 1$ but I am having trouble finding bounds so that I can apply the dominated convergence theorem.",How do I show I've tried splitting into the cases where and but I am having trouble finding bounds so that I can apply the dominated convergence theorem.,"\lim_{n \to \infty} \int_0^\infty \frac{n}{n^2+x}\sin\left(\frac{1}{x}\right)\, dx = 0\,\,? x \leq 1 x \geq 1","['real-analysis', 'limits', 'measure-theory']"
34,Evaluating $\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}}$,Evaluating,\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}},"Evaluate $$\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin({\frac{1}{x}})}{x-\sqrt{1+x^2}}$$ My attempt: $$\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}}=\lim_{x\to+\infty} \frac{x^2\sqrt{x}\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{x\left(1-\sqrt{1+\frac{1}{x^2}}\right)}$$ $$=\lim_{x\to+\infty} x\sqrt{x}\cdot \frac{\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{1-\sqrt{1+\frac{1}{x^2}}}$$ Both numerator and denominator tend to zero, while $x\sqrt{x} \to +\infty$ . Any help is appreciated.","Evaluate My attempt: Both numerator and denominator tend to zero, while . Any help is appreciated.",\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin({\frac{1}{x}})}{x-\sqrt{1+x^2}} \lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}}=\lim_{x\to+\infty} \frac{x^2\sqrt{x}\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{x\left(1-\sqrt{1+\frac{1}{x^2}}\right)} =\lim_{x\to+\infty} x\sqrt{x}\cdot \frac{\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{1-\sqrt{1+\frac{1}{x^2}}} x\sqrt{x} \to +\infty,"['calculus', 'limits', 'limits-without-lhopital']"
35,Find $\lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1}$ (I need a review of my resolution please :) ),Find  (I need a review of my resolution please :) ),\lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1},"Find the limit: $$\lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1}$$ This is what I have, i'm not sure about my answer (I'm just learning limits). $$\lim_{x \to 1} \cos(\pi \cdot x) \cdot \lim_{x \to 1} \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1} $$ And I know that cos is a bounded limit, also its $L=-1$ . The other limit: $$ \lim_{x \to 1} \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1} $$ $$ \lim_{x \to 1} \sqrt{\frac{(x-1) \cdot (x-1)}{(x-1)\cdot (x+1)}} \cdot \frac{1-x}{x^2+x-1} $$ Simplifying: $$ \lim_{x \to 1} \sqrt{\frac{(x-1) \cdot (x-1)}{(x-1)\cdot (x+1)}} \cdot \frac{1-x}{x^2+x-1} $$ $$ \lim_{x \to 1} \sqrt{\frac{(x-1)}{(x+1)}} \cdot \frac{1-x}{x^2+x-1} $$ Evaluating: $$ \lim_{x \to 1} \sqrt{\frac{(1-1)}{(1+1)}} \cdot \frac{1-1}{1^2+1-1} $$ $$ = \sqrt{0} \cdot \frac{0}{1} $$ $$\fbox {= 0}$$ So since I have a bounded limit and the other limit function is zero, the whole limit is $0$ . So; $$\lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1} = 0$$","Find the limit: This is what I have, i'm not sure about my answer (I'm just learning limits). And I know that cos is a bounded limit, also its . The other limit: Simplifying: Evaluating: So since I have a bounded limit and the other limit function is zero, the whole limit is . So;",\lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1} \lim_{x \to 1} \cos(\pi \cdot x) \cdot \lim_{x \to 1} \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1}  L=-1  \lim_{x \to 1} \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1}   \lim_{x \to 1} \sqrt{\frac{(x-1) \cdot (x-1)}{(x-1)\cdot (x+1)}} \cdot \frac{1-x}{x^2+x-1}   \lim_{x \to 1} \sqrt{\frac{(x-1) \cdot (x-1)}{(x-1)\cdot (x+1)}} \cdot \frac{1-x}{x^2+x-1}   \lim_{x \to 1} \sqrt{\frac{(x-1)}{(x+1)}} \cdot \frac{1-x}{x^2+x-1}   \lim_{x \to 1} \sqrt{\frac{(1-1)}{(1+1)}} \cdot \frac{1-1}{1^2+1-1}   = \sqrt{0} \cdot \frac{0}{1}  \fbox {= 0} 0 \lim_{x \to 1} \cos(\pi \cdot x) \cdot \sqrt{\frac{(x-1)^2}{(x^2-1)}} \cdot \frac{1-x}{x^2+x-1} = 0,"['calculus', 'limits', 'solution-verification', 'limits-without-lhopital']"
36,"Find the limit of the sequence of functions: $f_n (x) = \frac{\ln (2^n +x^n)}{n}, x \ge 0$",Find the limit of the sequence of functions:,"f_n (x) = \frac{\ln (2^n +x^n)}{n}, x \ge 0","So, the given function: $f_n (x) = \frac{\ln(2^n +x^n)}{n}, x \ge 0$ . For $|x| < 1$ the limit would be: $\lim_{n \to \infty} \frac{\ln(2^n +x^n)}{n} = \lim_{n \to \infty}\frac{\ln (2^n + 0)}{n} = \frac{\infty}{\infty}$ . Can I apply L'Hospital rule here? Then it would be: $\lim_{n \to \infty} \frac{e^{\ln(2) n} \cdot \ln(2)}{2^n} = \lim_{n \to \infty}\frac{\ln(2)^{n+1}}{2^n}$ It already looks nightmerish, so I assume that the method is not the best one in this case. What would be the best way to deal with this limit problem?","So, the given function: . For the limit would be: . Can I apply L'Hospital rule here? Then it would be: It already looks nightmerish, so I assume that the method is not the best one in this case. What would be the best way to deal with this limit problem?","f_n (x) = \frac{\ln(2^n +x^n)}{n}, x \ge 0 |x| < 1 \lim_{n \to \infty} \frac{\ln(2^n +x^n)}{n} = \lim_{n \to \infty}\frac{\ln (2^n + 0)}{n} = \frac{\infty}{\infty} \lim_{n \to \infty} \frac{e^{\ln(2) n} \cdot \ln(2)}{2^n} = \lim_{n \to \infty}\frac{\ln(2)^{n+1}}{2^n}","['limits', 'sequence-of-function']"
37,Calculate $\lim_{x \rightarrow -\infty} |x \sin x|$,Calculate,\lim_{x \rightarrow -\infty} |x \sin x|,$\lim_{x \rightarrow -\infty} |x \sin x|=\lim_{x \rightarrow -\infty} |x| |\sin x|=+\infty$ because $|\sin x|$ is bounded but the result is that the limit doesn't exist. Why?,because is bounded but the result is that the limit doesn't exist. Why?,\lim_{x \rightarrow -\infty} |x \sin x|=\lim_{x \rightarrow -\infty} |x| |\sin x|=+\infty |\sin x|,"['real-analysis', 'limits']"
38,"Show that $f_{n}(x):=nx(1-x)^{n}$ is uniformly bounded on $[0,1]$ for all $n\geq 1$.",Show that  is uniformly bounded on  for all .,"f_{n}(x):=nx(1-x)^{n} [0,1] n\geq 1","Consider $f_{n}(x):=nx(1-x)^{n}$ defined for $n=1,2,3,\cdots$ and $x\in [0,1]$ . The exercises have two parts: (a) Show that for each $n$ , $f_{n}(x)$ has a unique maximum $M_{n}$ at $x=x_{n}$ . Compute the limit of $M_{n}$ and $x_{n}$ as $n\rightarrow\infty.$ (b) Prove that $f_{n}(x)$ is uniformly bounded in $[0,1]$ . I have computed that for each $n$ , $f_{n}(x)$ has a unique maximum in $[0,1]$ at $$x_{n}=\dfrac{1}{1+n}$$ with the maximum value $$M_{n}=\Big(\dfrac{n}{n+1}\Big)^{n+1}.$$ Thus $$\lim_{n\rightarrow\infty}x_{n}=0\ \ \ \text{and}\ \ \ \lim_{n\rightarrow\infty}M_{n}=\lim_{n\rightarrow\infty}\Big(1-\dfrac{1}{1+n}\Big)^{n+1}=e^{-1}.$$ The solution says that since $|f_{n}(x)|\leq |M_{n}|$ for each $n=1,2,\cdots$ , the above shows that $|f_{n}(x)|\leq e^{-1}$ for all $x\in [0,1]$ and $n=1,2,\cdots$ . I don't understand this. To show the uniform boundedness, don't we need to show $$\sup_{n}|f_{n}(x)|\leq C,\ \ \text{for some constant}\  C\ \text{and for all}\  x\in [0,1]?$$ It is true that since $|f_{n}(x)|\leq M_{n}$ for each $n$ and for all $x\in [0,1]$ , we have $$\sup_{n}|f_{n}(x)|\leq\sup_{n}|M_{n}|,$$ but why does the limit of $M_{n}$ being $e^{-1}$ implies the sup is $e^{-1}$ ? Thank you!","Consider defined for and . The exercises have two parts: (a) Show that for each , has a unique maximum at . Compute the limit of and as (b) Prove that is uniformly bounded in . I have computed that for each , has a unique maximum in at with the maximum value Thus The solution says that since for each , the above shows that for all and . I don't understand this. To show the uniform boundedness, don't we need to show It is true that since for each and for all , we have but why does the limit of being implies the sup is ? Thank you!","f_{n}(x):=nx(1-x)^{n} n=1,2,3,\cdots x\in [0,1] n f_{n}(x) M_{n} x=x_{n} M_{n} x_{n} n\rightarrow\infty. f_{n}(x) [0,1] n f_{n}(x) [0,1] x_{n}=\dfrac{1}{1+n} M_{n}=\Big(\dfrac{n}{n+1}\Big)^{n+1}. \lim_{n\rightarrow\infty}x_{n}=0\ \ \ \text{and}\ \ \ \lim_{n\rightarrow\infty}M_{n}=\lim_{n\rightarrow\infty}\Big(1-\dfrac{1}{1+n}\Big)^{n+1}=e^{-1}. |f_{n}(x)|\leq |M_{n}| n=1,2,\cdots |f_{n}(x)|\leq e^{-1} x\in [0,1] n=1,2,\cdots \sup_{n}|f_{n}(x)|\leq C,\ \ \text{for some constant}\  C\ \text{and for all}\  x\in [0,1]? |f_{n}(x)|\leq M_{n} n x\in [0,1] \sup_{n}|f_{n}(x)|\leq\sup_{n}|M_{n}|, M_{n} e^{-1} e^{-1}","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'proof-explanation']"
39,"Finding $\lim_{n\to\infty} \frac1{3^n}\left(a^{\frac{1}{n}}+b^{\frac{1}{n}}+c^{\frac{1}{n}} \right)^n$ where $a,b,c>0$",Finding  where,"\lim_{n\to\infty} \frac1{3^n}\left(a^{\frac{1}{n}}+b^{\frac{1}{n}}+c^{\frac{1}{n}} \right)^n a,b,c>0","$$\lim_{n\to\infty} \bigg(\frac{a^{1/n}+b^{1/n}+c^{1/n}}{3} \bigg)^n, \quad \textrm{$a>0$, $b>0$ and $c>0$.}$$ I had an idea to present the terms and decompose them as $1+(a-1), 1+(b-1)$ and $1+(c-1)$ , but in this way I got to the answer $\exp(\frac{1}{3}(a+b+c-1))$ while the correct answer is $(abc)^{1/3}$ .","I had an idea to present the terms and decompose them as and , but in this way I got to the answer while the correct answer is .","\lim_{n\to\infty} \bigg(\frac{a^{1/n}+b^{1/n}+c^{1/n}}{3} \bigg)^n, \quad \textrm{a>0, b>0 and c>0.} 1+(a-1), 1+(b-1) 1+(c-1) \exp(\frac{1}{3}(a+b+c-1)) (abc)^{1/3}","['calculus', 'limits']"
40,epsilon-delta proofs: can delta be defined in terms of both epsilon and x?,epsilon-delta proofs: can delta be defined in terms of both epsilon and x?,,"I'm trying to construct a proof for the following limit: $\lim_{x \to -2} \frac{2x+5}{x+4} = \frac{1}{2} $ I know I need to show that for $ \epsilon > 0$ , $\exists \delta > 0$ such that if $0 < |x-(-2)| < \delta$ , $\left|\frac{2x+5}{x+4} - \frac{1}{2}\right| < \epsilon$ Here's the steps I took that led me to my question: $\left|\frac{2x+5 - \frac{1}{2}(x+4)}{x+4}\right| < \epsilon$ $\left|\frac{2x+5 - \frac{1}{2}x - 2}{x+4}\right| < \epsilon$ $\left|\frac{\frac{3}{2}x +3}{x+4}\right| < \epsilon$ $\frac{3}{2} * \left|\frac{x+2}{x+4}\right| < \epsilon$ I know I want to get this value in terms of just $x+2$ so that I can pick my delta value in terms of epsilon, since the limit is defined as $-2$ . Since I have that $x+2$ sitting right in the denominator (and this professor is not sadistic so I don't think he would try to make this overly difficult), I'm wondering if it's ""legal"" for me to pull the whole $|x+4|$ out of the denominator, and do something like: $\frac{3}{2(x+4)} * \left|x+2\right| < \epsilon$ and then make $\delta = \frac{\epsilon}{\frac{3}{2x+8}} = \frac{\epsilon(2x+8)}{3} $ If I'm not allowed to do this, does anyone have suggestions about where I might go with it next? I'm really not sure how else to get rid of the denominator. Thanks very much!","I'm trying to construct a proof for the following limit: I know I need to show that for , such that if , Here's the steps I took that led me to my question: I know I want to get this value in terms of just so that I can pick my delta value in terms of epsilon, since the limit is defined as . Since I have that sitting right in the denominator (and this professor is not sadistic so I don't think he would try to make this overly difficult), I'm wondering if it's ""legal"" for me to pull the whole out of the denominator, and do something like: and then make If I'm not allowed to do this, does anyone have suggestions about where I might go with it next? I'm really not sure how else to get rid of the denominator. Thanks very much!",\lim_{x \to -2} \frac{2x+5}{x+4} = \frac{1}{2}   \epsilon > 0 \exists \delta > 0 0 < |x-(-2)| < \delta \left|\frac{2x+5}{x+4} - \frac{1}{2}\right| < \epsilon \left|\frac{2x+5 - \frac{1}{2}(x+4)}{x+4}\right| < \epsilon \left|\frac{2x+5 - \frac{1}{2}x - 2}{x+4}\right| < \epsilon \left|\frac{\frac{3}{2}x +3}{x+4}\right| < \epsilon \frac{3}{2} * \left|\frac{x+2}{x+4}\right| < \epsilon x+2 -2 x+2 |x+4| \frac{3}{2(x+4)} * \left|x+2\right| < \epsilon \delta = \frac{\epsilon}{\frac{3}{2x+8}} = \frac{\epsilon(2x+8)}{3} ,"['real-analysis', 'limits', 'proof-explanation', 'epsilon-delta']"
41,Prove that if $ \lim_{x\to\infty}f\left(x\right)=L $ then $ \lim_{n\to\infty}\intop_{0}^{1}f\left(n\cdot x\right)dx=L $.,Prove that if  then ., \lim_{x\to\infty}f\left(x\right)=L   \lim_{n\to\infty}\intop_{0}^{1}f\left(n\cdot x\right)dx=L ,"let $ f $ be integrable function in any interval such [0,M]. assume $ \lim_{x\to\infty}f\left(x\right)=L $ for some $ L\in \mathbb{R} $ and prove that $ \lim_{n\to\infty}\intop_{0}^{1}f\left(n\cdot x\right)dx=L $ . I've managed to prove that $ \intop_{0}^{1}f\left(n\cdot x\right)dx=\frac{1}{n}\intop_{0}^{n}f\left(x\right)dx $ . Im not sure how to continue. Thanks in advance","let be integrable function in any interval such [0,M]. assume for some and prove that . I've managed to prove that . Im not sure how to continue. Thanks in advance", f   \lim_{x\to\infty}f\left(x\right)=L   L\in \mathbb{R}   \lim_{n\to\infty}\intop_{0}^{1}f\left(n\cdot x\right)dx=L   \intop_{0}^{1}f\left(n\cdot x\right)dx=\frac{1}{n}\intop_{0}^{n}f\left(x\right)dx ,"['calculus', 'integration', 'limits', 'riemann-integration']"
42,How to convert a limit of a sum (involving composite function) into a definite integral?,How to convert a limit of a sum (involving composite function) into a definite integral?,,"I’m finding it hard to convert a limit of sum which involves a composite function into a definite integral. For normal functions we can make use of this definition $$  \lim_{n\to \infty} \frac{b-a}{n} \sum_{i=1}^{n} f \left(                                                            a+ \left(           \frac{b-a}{n} \right) i                                                 \right)= \int_{a}^{b} f(x) dx$$ But let’s consider I have something like this: $$\lim_{n\to \infty}  \frac{x}{n}\left[ \ln \Bigg(1 + \dfrac{x^2}{n^2} \Bigg) + \ln\Bigg(1+ \dfrac{4x^2}{n^2}\Bigg) + \ln\Bigg(1+ \dfrac{9x^2}{n^2}\Bigg) ... \ln\Bigg(1+ \dfrac{n^2x^2}{n^2}\Bigg)  \right]$$ Well, the above thing is not of the standard form, because it can be written like $$ \lim_{n\to \infty} \frac{x}{n} \sum_{i=1}^{n} \ln \left(                                                            1+ \left(\frac{ix}{n} \right) ^2 \right) $$ Where, in my opinion, $x/n$ is acting as $\frac{b-a}{n}$ but the problem is inside the argument we have $\left(\frac{b-a}{n} i \right)^2$ but according to the standard form we ought to have just $\frac{b-a}{n} i $ . I asked this to someone he said the above said limit can be written as the integral $$\int_{0}^{x} \ln(1 +t^2) dt$$ And I really don’t know how and it’s my question to know how. There is one another example, how can I write $$\int_{0}^{x} \sin (t^2) dt$$ as limit of a sum? The most I can think is $$ \lim_{n\to \infty} \frac{x}{n}  \left[ \sin (x/n) + \sin(2x/n) .... + \sin(nx/n) \right]$$ but I know it’s equal to $$\int_{0}^{x} \sin (t) dt $$ Please help.","I’m finding it hard to convert a limit of sum which involves a composite function into a definite integral. For normal functions we can make use of this definition But let’s consider I have something like this: Well, the above thing is not of the standard form, because it can be written like Where, in my opinion, is acting as but the problem is inside the argument we have but according to the standard form we ought to have just . I asked this to someone he said the above said limit can be written as the integral And I really don’t know how and it’s my question to know how. There is one another example, how can I write as limit of a sum? The most I can think is but I know it’s equal to Please help."," 
\lim_{n\to \infty} \frac{b-a}{n} \sum_{i=1}^{n} f \left(
                                                           a+ \left(
          \frac{b-a}{n} \right) i 
                                               \right)= \int_{a}^{b} f(x) dx \lim_{n\to \infty} 
\frac{x}{n}\left[
\ln \Bigg(1 + \dfrac{x^2}{n^2} \Bigg) + \ln\Bigg(1+ \dfrac{4x^2}{n^2}\Bigg) + \ln\Bigg(1+ \dfrac{9x^2}{n^2}\Bigg) ... \ln\Bigg(1+ \dfrac{n^2x^2}{n^2}\Bigg) 
\right]  \lim_{n\to \infty} \frac{x}{n} \sum_{i=1}^{n} \ln \left( 
                                                          1+ \left(\frac{ix}{n} \right) ^2 \right)  x/n \frac{b-a}{n} \left(\frac{b-a}{n} i \right)^2 \frac{b-a}{n} i  \int_{0}^{x} \ln(1 +t^2) dt \int_{0}^{x} \sin (t^2) dt  \lim_{n\to \infty} \frac{x}{n} 
\left[
\sin (x/n) + \sin(2x/n) .... + \sin(nx/n) \right] \int_{0}^{x} \sin (t) dt ","['calculus', 'integration', 'limits', 'definite-integrals', 'riemann-integration']"
43,"Prove from first principles that if $x_n \rightarrow x$ as $n \rightarrow \infty$, then $|x_n| \rightarrow |x|$.","Prove from first principles that if  as , then .",x_n \rightarrow x n \rightarrow \infty |x_n| \rightarrow |x|,"I'm having trouble proving the limit of the above statement using first principles. Here is what I got so far, any tips you might have on continuing from here, would be much appreciated! We are given $x_n \rightarrow x$ as $n \rightarrow \infty$ . $$n \geq N \rightarrow \big||x_n|-|x|\big| < \epsilon $$ Proving $ \big||x_n|-|x|\big| \leq|x_n - x|$ . $$ \big||x_n|-|x|\big| \leq |x_n-x| \iff\big||x_n|-|x|\big|^2 \leq|x_n-x|^2$$ $$\iff x_n^2 - 2|x_nx|+x^2 \leq  x_n^2 - 2x_nx + x^2 \iff x_nx \leq |x_nx| $$ Which is always true. Therefore... $$\big||x_n|-|x|\big| \leq |x_n - x| < \epsilon$$ I'm not sure what to do after this as I have not practised much with a generalised sequences.","I'm having trouble proving the limit of the above statement using first principles. Here is what I got so far, any tips you might have on continuing from here, would be much appreciated! We are given as . Proving . Which is always true. Therefore... I'm not sure what to do after this as I have not practised much with a generalised sequences.",x_n \rightarrow x n \rightarrow \infty n \geq N \rightarrow \big||x_n|-|x|\big| < \epsilon   \big||x_n|-|x|\big| \leq|x_n - x|  \big||x_n|-|x|\big| \leq |x_n-x| \iff\big||x_n|-|x|\big|^2 \leq|x_n-x|^2 \iff x_n^2 - 2|x_nx|+x^2 \leq  x_n^2 - 2x_nx + x^2 \iff x_nx \leq |x_nx|  \big||x_n|-|x|\big| \leq |x_n - x| < \epsilon,"['real-analysis', 'sequences-and-series', 'limits', 'solution-verification', 'absolute-value']"
44,Find $ \lim\limits_{x\to 0}{\frac{1-\sqrt{1+x}\sqrt[3]{1-x}\cdots\sqrt[2n+1]{1-x}}{x}} $ without using L'Hopital's rule.,Find  without using L'Hopital's rule., \lim\limits_{x\to 0}{\frac{1-\sqrt{1+x}\sqrt[3]{1-x}\cdots\sqrt[2n+1]{1-x}}{x}} ,Let $ n $ be a positive integer greater than $ 1 $ . Find : $$ \lim\limits_{x\to 0}{\frac{1-\sqrt{1+x}\sqrt[3]{1-x}\cdots\sqrt[2n+1]{1-x}}{x}} $$ Without L'Hopital's rule or series expansion. Here is What I did to solve the problem. \begin{aligned}\displaystyle\lim_{x\to 0}{\displaystyle\frac{1-\prod\limits_{k=2}^{2n+1}{\sqrt[k]{1+\left(-1\right)^{k}x}}}{x}}&=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}-\prod\limits_{i=2}^{k}{\sqrt[i]{1+\left(-1\right)^{i}x}}}{x}}}\\&=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{1-\sqrt[k]{1+\left(-1\right)^{k}x}}{x}\displaystyle\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}}}\\ &=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\left(-1\right)^{k+1}}{\sum\limits_{j=0}^{k-1}{\sqrt[k]{1+\left(-1\right)^{k}x}^{j}}}\displaystyle\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}}}\\ &=\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\left(-1\right)^{k+1}}{k}} \\ &=\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k+1}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k}}\\ &=-1+\displaystyle\sum_{k=0}^{n}{\displaystyle\frac{1}{2k+1}}+\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{k}}\\ &=-1+\displaystyle\sum_{k=1}^{2n+1}{\displaystyle\frac{1}{k}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{k}}\\ \displaystyle\lim_{x\to 0}{\displaystyle\frac{1-\prod\limits_{k=2}^{2n+1}{\sqrt[k]{1+\left(-1\right)^{k}x}}}{x}}&=H_{2n+1}-H_{n}-1 \end{aligned} What's your approach to solve the problem ?,Let be a positive integer greater than . Find : Without L'Hopital's rule or series expansion. Here is What I did to solve the problem. What's your approach to solve the problem ?, n   1   \lim\limits_{x\to 0}{\frac{1-\sqrt{1+x}\sqrt[3]{1-x}\cdots\sqrt[2n+1]{1-x}}{x}}  \begin{aligned}\displaystyle\lim_{x\to 0}{\displaystyle\frac{1-\prod\limits_{k=2}^{2n+1}{\sqrt[k]{1+\left(-1\right)^{k}x}}}{x}}&=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}-\prod\limits_{i=2}^{k}{\sqrt[i]{1+\left(-1\right)^{i}x}}}{x}}}\\&=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{1-\sqrt[k]{1+\left(-1\right)^{k}x}}{x}\displaystyle\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}}}\\ &=\displaystyle\lim_{x\to 0}{\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\left(-1\right)^{k+1}}{\sum\limits_{j=0}^{k-1}{\sqrt[k]{1+\left(-1\right)^{k}x}^{j}}}\displaystyle\prod\limits_{i=2}^{k-1}{\sqrt[i]{1+\left(-1\right)^{i}x}}}}\\ &=\displaystyle\sum_{k=2}^{2n+1}{\displaystyle\frac{\left(-1\right)^{k+1}}{k}} \\ &=\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k+1}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k}}\\ &=-1+\displaystyle\sum_{k=0}^{n}{\displaystyle\frac{1}{2k+1}}+\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{2k}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{k}}\\ &=-1+\displaystyle\sum_{k=1}^{2n+1}{\displaystyle\frac{1}{k}}-\displaystyle\sum_{k=1}^{n}{\displaystyle\frac{1}{k}}\\ \displaystyle\lim_{x\to 0}{\displaystyle\frac{1-\prod\limits_{k=2}^{2n+1}{\sqrt[k]{1+\left(-1\right)^{k}x}}}{x}}&=H_{2n+1}-H_{n}-1 \end{aligned},"['sequences-and-series', 'limits', 'limits-without-lhopital']"
45,How to evaluate limits of inverse trigonometric functions without L'Hospital and series expansion?,How to evaluate limits of inverse trigonometric functions without L'Hospital and series expansion?,,"I am studying limits and how to evaluate them without using l'Hospital Rule or series expansion. Most of them aren't that hard, there are some common trick to do, but I have issues when I face limits of some not-so-common functions such as inverse trigonometric functions. An example of such a function is this: And also: I have no idea how to even approach such a limit so I would be happy if you could, besides just solving these two limits explain some approaches to evaluating limits with inverse trigonometric functions in general.","I am studying limits and how to evaluate them without using l'Hospital Rule or series expansion. Most of them aren't that hard, there are some common trick to do, but I have issues when I face limits of some not-so-common functions such as inverse trigonometric functions. An example of such a function is this: And also: I have no idea how to even approach such a limit so I would be happy if you could, besides just solving these two limits explain some approaches to evaluating limits with inverse trigonometric functions in general.",,"['limits', 'analysis', 'trigonometry', 'limits-without-lhopital']"
46,Calculating a limit with Harmonic number,Calculating a limit with Harmonic number,,I am trying to prove that $\lim_{n\to\infty} (\frac{\sum^n_{k=1} \frac{1}{k}} { \ln(n) })^{ \ln(n) } = e^γ$ where $γ$ the Euler-Mascheroni constant. We know that that: $\lim_{n\to\infty} \frac{\sum^n_{k=1} \frac{1}{k}}{ \ln(n) } = 1$ By approximating the sum with integrals $$ \ln(n+1)=\int_1^{n+1}\frac1x\mathrm dx\le\sum_{k=1}^n\frac1k\le 1+\int_1^n\frac1x\mathrm dx=1+\ln n $$ since $1/k$ is decreasing for $k\ge 1$ . We have that $$ \frac{\ln(n+1)}{\ln n}=\frac{\ln n+\ln(1+\frac1n)}{\ln n}\to1 $$ as $n\to\infty$ and we obtain the result. But I can't calculate $\lim_{n\to\infty} (\frac{\sum^n_{k=1} \frac{1}{k}} { \ln(n) })^{ \ln(n) } = e^γ.$ Any help?,I am trying to prove that where the Euler-Mascheroni constant. We know that that: By approximating the sum with integrals since is decreasing for . We have that as and we obtain the result. But I can't calculate Any help?,"\lim_{n\to\infty} (\frac{\sum^n_{k=1} \frac{1}{k}} { \ln(n) })^{ \ln(n) } = e^γ γ \lim_{n\to\infty} \frac{\sum^n_{k=1} \frac{1}{k}}{ \ln(n) } = 1 
\ln(n+1)=\int_1^{n+1}\frac1x\mathrm dx\le\sum_{k=1}^n\frac1k\le 1+\int_1^n\frac1x\mathrm dx=1+\ln n
 1/k k\ge 1 
\frac{\ln(n+1)}{\ln n}=\frac{\ln n+\ln(1+\frac1n)}{\ln n}\to1
 n\to\infty \lim_{n\to\infty} (\frac{\sum^n_{k=1} \frac{1}{k}} { \ln(n) })^{ \ln(n) } = e^γ.","['calculus', 'limits']"
47,"Given $a_n=\sum_{k=1}^n\frac{k}{n^2}e^{k/n^2}$, find $a,b\in\Bbb R$ with $ \lim_{n\to\infty}(na_n-an)=b$.","Given , find  with .","a_n=\sum_{k=1}^n\frac{k}{n^2}e^{k/n^2} a,b\in\Bbb R  \lim_{n\to\infty}(na_n-an)=b","It is given that: $$a_n=\sum_{k=1}^n \frac{k}{n^2} e^{\frac{k}{n^2}}$$ and $\lim\limits_{n\to \infty} a_n=\frac{1}{2}$ . I want to find reals $a,b$ so $ \lim\limits_{n\to \infty} (n a_n-a n)=b $ . Actually I was wondering if there is a theorem such as Stolz-Cesaro to solve it. I can't find something. Thank you",It is given that: and . I want to find reals so . Actually I was wondering if there is a theorem such as Stolz-Cesaro to solve it. I can't find something. Thank you,"a_n=\sum_{k=1}^n \frac{k}{n^2} e^{\frac{k}{n^2}} \lim\limits_{n\to \infty} a_n=\frac{1}{2} a,b  \lim\limits_{n\to \infty} (n a_n-a n)=b ","['sequences-and-series', 'limits']"
48,Let $\lim_{n\to\infty}(a_n)=\infty$ and $(b_n)_{n\in\mathbb N}$ be bounded. Prove that $\lim_{n\to\infty}(a_n+b_n)=\infty$.,Let  and  be bounded. Prove that .,\lim_{n\to\infty}(a_n)=\infty (b_n)_{n\in\mathbb N} \lim_{n\to\infty}(a_n+b_n)=\infty,"Question: Let $(a_n)_{n\in\mathbb N}$ and $(b_n)_{n\in\mathbb N}$ be sequences with real values. Let $\lim_{n\to\infty}(a_n)=\infty$ and $(b_n)_{n\in\mathbb N}$ be bounded. Prove that $\lim_{n\to\infty}(a_n+b_n)=\infty$ . Proof: Since we know: $\lim_{n\to\infty}(a_n)=\infty\space$ $\Leftrightarrow\space$ $\forall K\in\mathbb R^+ \space \space\exists N_1\in\mathbb N $ : $n\gt N_1 \space$ $\Rightarrow a_n\gt 2K.$ and $(b_n)_{n\in\mathbb N}$ is bounded $\Leftrightarrow\space$ $\exists K\in\mathbb R^+ \space $ such that $\space \forall n\in\mathbb N \space$ $ |b_n|\le K.$ Now taking $N=N_1$ : $\Rightarrow$ $a_n\gt 2K$ and $|b_n|\le K$ $\Leftrightarrow$ $2K\lt a_n$ and $-K\le b_n \le K$ $\Rightarrow$ $a_n+b_n \gt 2K-K=K$ $\space \forall n\gt N$ Hence $\lim_{n\to\infty}(a_n+b_n)=\infty$ This is an optional question from my Analysis I course, thought it would be a fun little proof. Be great if anyone could check what I have done -struggling to get to grips with these types of proofs.","Question: Let and be sequences with real values. Let and be bounded. Prove that . Proof: Since we know: : and is bounded such that Now taking : and and Hence This is an optional question from my Analysis I course, thought it would be a fun little proof. Be great if anyone could check what I have done -struggling to get to grips with these types of proofs.",(a_n)_{n\in\mathbb N} (b_n)_{n\in\mathbb N} \lim_{n\to\infty}(a_n)=\infty (b_n)_{n\in\mathbb N} \lim_{n\to\infty}(a_n+b_n)=\infty \lim_{n\to\infty}(a_n)=\infty\space \Leftrightarrow\space \forall K\in\mathbb R^+ \space \space\exists N_1\in\mathbb N  n\gt N_1 \space \Rightarrow a_n\gt 2K. (b_n)_{n\in\mathbb N} \Leftrightarrow\space \exists K\in\mathbb R^+ \space  \space \forall n\in\mathbb N \space  |b_n|\le K. N=N_1 \Rightarrow a_n\gt 2K |b_n|\le K \Leftrightarrow 2K\lt a_n -K\le b_n \le K \Rightarrow a_n+b_n \gt 2K-K=K \space \forall n\gt N \lim_{n\to\infty}(a_n+b_n)=\infty,"['real-analysis', 'limits', 'analysis', 'alternative-proof', 'solution-verification']"
49,Limit of the function $f(x) = \begin{cases} 1 & \text{if } x = 1 \\ 0 & \text{otherwise} \end{cases} $,Limit of the function,f(x) = \begin{cases} 1 & \text{if } x = 1 \\ 0 & \text{otherwise} \end{cases} ,"Consider function $f(x) = \begin{cases}        1 & \text{if } x = 1 \\       0 & \text{otherwise}    \end{cases} $ I am trying to find limit of this function when $x \to 1$ . By two-sided limit theorem, it can concluded $$\lim_{x \to 1^{-}}f(x) = \lim_{x \to1^{+}}f(x) = 0 \implies \lim_{x \to 1}f(x) = 0.$$ But when trying to apply the definition of the limit there is a problem: we want to bound $|f(x) - 0| = |f(x)|$ by $\epsilon$ , $|x - 1| < \delta$ and when $x = 1$ we have $|f(x)| = 1$ which is not bounded for any $\epsilon > 0$ . The question is: does the limit even exist?","Consider function I am trying to find limit of this function when . By two-sided limit theorem, it can concluded But when trying to apply the definition of the limit there is a problem: we want to bound by , and when we have which is not bounded for any . The question is: does the limit even exist?","f(x) = \begin{cases} 
      1 & \text{if } x = 1 \\
      0 & \text{otherwise}
   \end{cases}
 x \to 1 \lim_{x \to 1^{-}}f(x) = \lim_{x \to1^{+}}f(x) = 0 \implies \lim_{x \to 1}f(x) = 0. |f(x) - 0| = |f(x)| \epsilon |x - 1| < \delta x = 1 |f(x)| = 1 \epsilon > 0","['real-analysis', 'limits', 'real-numbers']"
50,What's wrong with my calculation of a limit here?,What's wrong with my calculation of a limit here?,,"So, I have to find the following limit: $\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right)$ I solved it by splitting it into three limits as follows: $$\begin{align} &\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right) \\ &=\lim _{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln \left(8x+1\right)\right)^2}\cdot \frac{sin\left(9x\right)^{14}}{\left(\ln \left(8x+1\right)\right)^{28}} \\ &=\lim_{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \lim_{x\to\:0+}\frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln\left(8x+1\right)\right)^2}\cdot\lim_{x\to 0+}\frac{sin\left(9x\right)^{14}}{\left(\ln\left(8x+1\right)\right)^{28}} \\ &=\left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14} \end{align}$$ Using L'Hospital's rule to solve these separate limits, you get $$\left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14} \\ =0^{14}\cdot 0^2\cdot \left(-\frac{9}{16}\right)^{14} \\ =0$$ However, the automated homework system did not accept 0 as the correct answer. What did I do wrong?","So, I have to find the following limit: I solved it by splitting it into three limits as follows: Using L'Hospital's rule to solve these separate limits, you get However, the automated homework system did not accept 0 as the correct answer. What did I do wrong?","\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right) \begin{align}
&\lim _{x\to \:0+}\left(\frac{\left(1-\cos \left(2x\right)\right)^{14}\left(1-\cos \left(7x\right)\right)^2\sin ^{14}\left(9x\right)}{\tan ^{14}\left(x\right)\left(\ln \left(8x+1\right)\right)^{30}}\right)
\\
&=\lim _{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln \left(8x+1\right)\right)^2}\cdot \frac{sin\left(9x\right)^{14}}{\left(\ln \left(8x+1\right)\right)^{28}}
\\
&=\lim_{x\to 0+}\frac{\left(1-cos\left(2x\right)\right)^{14}}{\tan ^{14}\left(x\right)}\cdot \lim_{x\to\:0+}\frac{\left(1-cos\left(7x\right)\right)^2}{\left(\ln\left(8x+1\right)\right)^2}\cdot\lim_{x\to 0+}\frac{sin\left(9x\right)^{14}}{\left(\ln\left(8x+1\right)\right)^{28}}
\\
&=\left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14}
\end{align} \left(\lim _{x\to \:\:0+}\frac{\left(1-cos\left(2x\right)\right)}{\tan \left(x\right)}\right)^{14}\cdot \left(\lim _{x\to 0+}\frac{\left(1-cos\left(7x\right)\right)}{\:\ln \left(8x+1\right)}\right)^2\cdot \left(\lim _{x\to 0+}\frac{sin\left(9x\right)}{\left(\ln\left(8x+1\right)\right)^2}\right)^{14}
\\
=0^{14}\cdot 0^2\cdot \left(-\frac{9}{16}\right)^{14}
\\
=0","['calculus', 'limits', 'solution-verification']"
51,"Compute: $\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R$",Compute:,"\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R","Compute: $$\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha  x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R$$ My attempt: $$L=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2}=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-1+1-\sqrt[n]{\cos(\beta x)}}{x^2}\\\displaystyle=\lim_{x\to 0}\frac{\left(1+(\cos(\alpha x)-1)\right)^{\frac{1}{m}}-1-\left(\left(1+(\cos(\beta x)-1)\right)^{\frac{1}{n}}-1\right)}{x^2}$$ Transformed expression: $$-\frac{(1+(\cos(\alpha x)-1))^{\frac{1}{m}}-1}{\cos(\alpha x)-1}\cdot\frac{1-\cos(\alpha x)}{\alpha^2x^2}\alpha^2+\frac{(1+(\cos(\beta x)-1))^{\frac{1}{n}}-1}{\cos(\beta x)-1}\cdot\frac{1-\cos(\beta x)}{\beta^2x^2}\beta^2$$ I applied the standard limits from the table: $$\lim_{x\to 0}\frac{(1+x)^a-1}{x}=a\;\&\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2}$$ $$L=-\frac{1}{m}\cdot\frac{1}{2}\alpha^2+\frac{1}{n}\cdot\frac{1}{2}\beta^2=\frac{m\beta^2-n\alpha^2}{2mn}$$ Is this correct?",Compute: My attempt: Transformed expression: I applied the standard limits from the table: Is this correct?,"\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha
 x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2},\;m,n\in\mathbb N,\alpha,\beta\in\mathbb R L=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-\sqrt[n]{\cos{(\beta x)}}}{x^2}=\lim_{x\to 0}\frac{\sqrt[m]{\cos(\alpha x)}-1+1-\sqrt[n]{\cos(\beta x)}}{x^2}\\\displaystyle=\lim_{x\to 0}\frac{\left(1+(\cos(\alpha x)-1)\right)^{\frac{1}{m}}-1-\left(\left(1+(\cos(\beta x)-1)\right)^{\frac{1}{n}}-1\right)}{x^2} -\frac{(1+(\cos(\alpha x)-1))^{\frac{1}{m}}-1}{\cos(\alpha x)-1}\cdot\frac{1-\cos(\alpha x)}{\alpha^2x^2}\alpha^2+\frac{(1+(\cos(\beta x)-1))^{\frac{1}{n}}-1}{\cos(\beta x)-1}\cdot\frac{1-\cos(\beta x)}{\beta^2x^2}\beta^2 \lim_{x\to 0}\frac{(1+x)^a-1}{x}=a\;\&\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2} L=-\frac{1}{m}\cdot\frac{1}{2}\alpha^2+\frac{1}{n}\cdot\frac{1}{2}\beta^2=\frac{m\beta^2-n\alpha^2}{2mn}","['calculus', 'limits', 'limits-without-lhopital', 'solution-verification']"
52,Find the supremum of $h(z)$.,Find the supremum of .,h(z),"We put $$‎h(z) = \frac{|‎\frac{e}{e-1}(e^z-1)|\;-‎\frac{3}{2}e^{|z|}+1‎}{‎\frac{\pi}{\pi^2-1}‎‎‎‎\sinh‎(\pi|z|)\;+‎\frac{1}{\pi^2-1}‎‎‎‎‎(\cosh‎‎(\pi|z|)-1) -e^{|z|}+1},‎\; z\in\mathbb{C}\setminus\{0\}. $$ ‎‎‎‎My question is: What is the $\displaystyle\sup_{z\in\mathbb{C}\setminus\{0\}}h(z)$ (in other words, the least $M\leq \infty$ such that $h(z)\leq M$ , for all $z\neq0$ )? To Find this I firstly checked that the denominator of $h(z)$ is positive. Then using Wolfram Alpha I get it approximately equals to $0.000759$ . Now I want to know that is it correct? Anyone can help me to obtain the supremum (and how to calculate)? thanks a lot.","We put ‎‎‎‎My question is: What is the (in other words, the least such that , for all )? To Find this I firstly checked that the denominator of is positive. Then using Wolfram Alpha I get it approximately equals to . Now I want to know that is it correct? Anyone can help me to obtain the supremum (and how to calculate)? thanks a lot.","‎h(z) = \frac{|‎\frac{e}{e-1}(e^z-1)|\;-‎\frac{3}{2}e^{|z|}+1‎}{‎\frac{\pi}{\pi^2-1}‎‎‎‎\sinh‎(\pi|z|)\;+‎\frac{1}{\pi^2-1}‎‎‎‎‎(\cosh‎‎(\pi|z|)-1) -e^{|z|}+1},‎\; z\in\mathbb{C}\setminus\{0\}.
 \displaystyle\sup_{z\in\mathbb{C}\setminus\{0\}}h(z) M\leq \infty h(z)\leq M z\neq0 h(z) 0.000759","['real-analysis', 'calculus', 'limits', 'inequality', 'supremum-and-infimum']"
53,Evaluating $\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}}$ without l'Hopital's rule or Taylor series,Evaluating  without l'Hopital's rule or Taylor series,\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}},"Can anyone please help me find this limit without l'Hopital's rule, I already used it to evaluate the limit, but I didn't know how to calculate it without l'Hopital's rule. $$\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}}$$ Any tips will be helpful. Sorry, but I don't want to use the Taylor series as well.","Can anyone please help me find this limit without l'Hopital's rule, I already used it to evaluate the limit, but I didn't know how to calculate it without l'Hopital's rule. Any tips will be helpful. Sorry, but I don't want to use the Taylor series as well.",\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}},"['calculus', 'limits', 'limits-without-lhopital']"
54,The Limit of a Recursive Sequence from a Probability Question,The Limit of a Recursive Sequence from a Probability Question,,"If we define a sequence as $e_0=0$ , for any $n\geq 1$ we have $$e_n=\frac{1}{2^n}\left(\sum_{i=0}^{n}C_n^i (e_i +1)\right)$$ How to calculate the limit $$\lim_{n\rightarrow\infty}\frac{e_n}{\log_2 n}$$ which is conjectured to be $1$ from a probability question.","If we define a sequence as , for any we have How to calculate the limit which is conjectured to be from a probability question.",e_0=0 n\geq 1 e_n=\frac{1}{2^n}\left(\sum_{i=0}^{n}C_n^i (e_i +1)\right) \lim_{n\rightarrow\infty}\frac{e_n}{\log_2 n} 1,"['probability', 'sequences-and-series', 'limits']"
55,Why is an exponent of $2n$ necessary in Dirichlet's Function?,Why is an exponent of  necessary in Dirichlet's Function?,2n,"For those unfamiliar, a question explaining the definition is in the question here . The definition itself is $$\lim_{m\to\infty}\lim_{n\to\infty}\cos^{2n}\left(m!\pi x\right)$$ and evaluates to 1 for rational $x$ and 0 for irrational $x$ . The part I don't totally understand is why the exponent is $2n$ as opposed to just $n$ . This is a very-not rigorous question, but my concern stems from the fact that as $n\to\infty$ , $2n$ isn't necessarily even, let alone an integer, which negates the whole point of making rational values become $1$ instead of $\pm1$ . It seems like you could, on the other hand, write it as $$\lim_{m\to\infty}\lim_{n\to\infty}\left(\cos^{2}\left(m!\pi x\right)\right)^n$$ although I'm not entirely sure if this is the same expression. My other question is that if the first equation is 'right', it seems to like it logically follows that $$\lim_{n\to\infty}(-1)^{2n}=1$$ as well. Is this true?","For those unfamiliar, a question explaining the definition is in the question here . The definition itself is and evaluates to 1 for rational and 0 for irrational . The part I don't totally understand is why the exponent is as opposed to just . This is a very-not rigorous question, but my concern stems from the fact that as , isn't necessarily even, let alone an integer, which negates the whole point of making rational values become instead of . It seems like you could, on the other hand, write it as although I'm not entirely sure if this is the same expression. My other question is that if the first equation is 'right', it seems to like it logically follows that as well. Is this true?",\lim_{m\to\infty}\lim_{n\to\infty}\cos^{2n}\left(m!\pi x\right) x x 2n n n\to\infty 2n 1 \pm1 \lim_{m\to\infty}\lim_{n\to\infty}\left(\cos^{2}\left(m!\pi x\right)\right)^n \lim_{n\to\infty}(-1)^{2n}=1,['limits']
56,Find limit of $\frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2}$,Find limit of,\frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2},I have to show that $\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2} = 0$ . But I cannot figure out the trick you need to find an upper estimation which goes to $0$ . Do you have any hints? EDIT : I think I got it: $\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2} =  \lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2}{(x^4+2x^2 y^2 +y^4)} +\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|y^4}{(x^4+2x^2 y^2 +y^4)}\leq \lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2}{x^2 y^2}+\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|y^4}{y^4}=\lim_{{x\choose y}\to { 0 \choose 0}} |x|+ \lim_{{x\choose y}\to { 0 \choose 0}} |x|=0.$ What do you think?,I have to show that . But I cannot figure out the trick you need to find an upper estimation which goes to . Do you have any hints? EDIT : I think I got it: What do you think?,\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2} = 0 0 \lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2+|x|y^4}{(x^2+y^2)^2} =  \lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2}{(x^4+2x^2 y^2 +y^4)} +\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|y^4}{(x^4+2x^2 y^2 +y^4)}\leq \lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|^3 y^2}{x^2 y^2}+\lim_{{x\choose y}\to { 0 \choose 0}} \frac{|x|y^4}{y^4}=\lim_{{x\choose y}\to { 0 \choose 0}} |x|+ \lim_{{x\choose y}\to { 0 \choose 0}} |x|=0.,"['limits', 'multivariable-calculus']"
57,Can we interchange the order of limit and derivative?,Can we interchange the order of limit and derivative?,,Can you explain with an example that $$\frac{d}{dx} \lim_{n\to \infty} f_n(x)= \lim_{n \to \infty} \frac{d}{dx} f_n(x)$$ Is not true in general?,Can you explain with an example that Is not true in general?,\frac{d}{dx} \lim_{n\to \infty} f_n(x)= \lim_{n \to \infty} \frac{d}{dx} f_n(x),"['real-analysis', 'limits', 'derivatives']"
58,"If $\lim_{x\to 0}\frac{ae^x-b}{x}=2$ the find $a,b$",If  the find,"\lim_{x\to 0}\frac{ae^x-b}{x}=2 a,b","If $$\lim_{x\to 0}\frac{ae^x-b}{x}=2$$ the find $a,b$ $$ \lim_{x\to 0}\frac{ae^x-b}{x}=\lim_{x\to 0}\frac{a(e^x-1)+a-b}{x}=\lim_{x\to 0}\frac{a(e^x-1)}{x}+\lim_{x\to 0}\frac{a-b}{x}=\boxed{a+\lim_{x\to 0}\frac{a-b}{x}=2}\\ \lim_{x\to 0}\frac{a-b}{x} \text{ must be finite}\implies \boxed{a=b}\\ $$ Now I think I am stuck, how do I proceed ?","If the find Now I think I am stuck, how do I proceed ?","\lim_{x\to 0}\frac{ae^x-b}{x}=2 a,b 
\lim_{x\to 0}\frac{ae^x-b}{x}=\lim_{x\to 0}\frac{a(e^x-1)+a-b}{x}=\lim_{x\to 0}\frac{a(e^x-1)}{x}+\lim_{x\to 0}\frac{a-b}{x}=\boxed{a+\lim_{x\to 0}\frac{a-b}{x}=2}\\
\lim_{x\to 0}\frac{a-b}{x} \text{ must be finite}\implies \boxed{a=b}\\
","['limits', 'exponential-function']"
59,Prove $\lim_{x \to 2} \frac{ x(x^2-1) }{x+3}=6/5$.,Prove .,\lim_{x \to 2} \frac{ x(x^2-1) }{x+3}=6/5,"Prove by $\epsilon$ - $\delta$ definition that $\displaystyle\lim_{x \to 2}  \frac{ x(x^2-1) }{x+3}=\dfrac{6}{5}$ . I know this is simple but I am stuck, If we assume $\delta <1$ and $|x-2|< \delta$ we can get rid of the factor $x-2$ in the following expression $$\dfrac{ x(x^2-1) }{x+3}-\dfrac{6}5=\dfrac{(x-2)(5x^2+10x+9)}{5x+15}$$ but I don't know how to proceed.","Prove by - definition that . I know this is simple but I am stuck, If we assume and we can get rid of the factor in the following expression but I don't know how to proceed.",\epsilon \delta \displaystyle\lim_{x \to 2}  \frac{ x(x^2-1) }{x+3}=\dfrac{6}{5} \delta <1 |x-2|< \delta x-2 \dfrac{ x(x^2-1) }{x+3}-\dfrac{6}5=\dfrac{(x-2)(5x^2+10x+9)}{5x+15},"['calculus', 'limits']"
60,Justifying $\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7)$,Justifying,\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7),"How do you justify such limit equalities as these? $\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7)$ I am familiar with the $\epsilon-\delta$ definition of the limit, I just don't know what it is that makes these statements equivalent: $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |x| < \delta \implies |f(x) - l| < \epsilon$$ $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |bx| < \delta \implies |f(bx) - l| < \epsilon$$ $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |x - 7| < \delta \implies |f(x - 7) - l| < \epsilon$$","How do you justify such limit equalities as these? I am familiar with the definition of the limit, I just don't know what it is that makes these statements equivalent:",\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7) \epsilon-\delta  \forall \epsilon \ \exists \delta> 0 \ \forall x: |x| < \delta \implies |f(x) - l| < \epsilon  \forall \epsilon \ \exists \delta> 0 \ \forall x: |bx| < \delta \implies |f(bx) - l| < \epsilon  \forall \epsilon \ \exists \delta> 0 \ \forall x: |x - 7| < \delta \implies |f(x - 7) - l| < \epsilon,"['real-analysis', 'calculus', 'limits']"
61,Why is $e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x}$? [duplicate],Why is ? [duplicate],e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x},"This question already has answers here : If $e=\lim\limits_{x\to \infty} (1+x^{-1})^x$, why doesn't $e=1$? (9 answers) Closed 4 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved The definition of the number $e$ that's used in my textbook is $e = \lim\limits_{x\to \infty} (1+\frac {1}{x})^x$ which relates to compound interest. But when trying to calculate the derivative of $e^x$ , I encountered another definition of $e$ : $$e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x}$$ Now, if it were $x$ approached $0$ from the positive direction, it would've made sense to me since $\frac {1}{x}$ would approach infinity and $1+x$ would approach $1$ with the same ""speed""(sorry I don't know the term here) as the first definition. But it's approaching $0$ from both sides. My question is, why are these two equal? EDIT The reason my question isn't a duplicate of the suggested one is that mine isn't really about the limit $\lim\limits_{x\to \infty} (1+\frac {1}{x})^x$ . It's about its alternative form. I totally understand how the former isn't equal to $1$ . Thank you so much in advance! P.S: I graphed the function $(1+x)^\frac {1}{x}$ and can ""see"" the limit exists as $x$ approaches $0$ but I would like a non-visual proof.","This question already has answers here : If $e=\lim\limits_{x\to \infty} (1+x^{-1})^x$, why doesn't $e=1$? (9 answers) Closed 4 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved The definition of the number that's used in my textbook is which relates to compound interest. But when trying to calculate the derivative of , I encountered another definition of : Now, if it were approached from the positive direction, it would've made sense to me since would approach infinity and would approach with the same ""speed""(sorry I don't know the term here) as the first definition. But it's approaching from both sides. My question is, why are these two equal? EDIT The reason my question isn't a duplicate of the suggested one is that mine isn't really about the limit . It's about its alternative form. I totally understand how the former isn't equal to . Thank you so much in advance! P.S: I graphed the function and can ""see"" the limit exists as approaches but I would like a non-visual proof.",e e = \lim\limits_{x\to \infty} (1+\frac {1}{x})^x e^x e e = \lim\limits_{x\to 0} (1+x)^\frac{1}{x} x 0 \frac {1}{x} 1+x 1 0 \lim\limits_{x\to \infty} (1+\frac {1}{x})^x 1 (1+x)^\frac {1}{x} x 0,['limits']
62,How to evaluate the limit for this sequence?,How to evaluate the limit for this sequence?,,"I need to evaluate the following limit : Suppose that $\displaystyle\lim_{n \to \infty} a_n = a$ then find the Limit : $\displaystyle\lim_{n \to \infty} \left(\dfrac{a_n}{1} + \dfrac{a_{n-1}}{2} + \dfrac{a_{n-2}}{2^2} + \ldots + \dfrac{a_1}{2^{n-1}}\right)$ Since term in denominators forms a Geometric sequence,I am trying to solve this by using Sandwich Theorem to get some insights but I haven't achieved any success . Any idea how should I proceed towards the solution ?","I need to evaluate the following limit : Suppose that then find the Limit : Since term in denominators forms a Geometric sequence,I am trying to solve this by using Sandwich Theorem to get some insights but I haven't achieved any success . Any idea how should I proceed towards the solution ?",\displaystyle\lim_{n \to \infty} a_n = a \displaystyle\lim_{n \to \infty} \left(\dfrac{a_n}{1} + \dfrac{a_{n-1}}{2} + \dfrac{a_{n-2}}{2^2} + \ldots + \dfrac{a_1}{2^{n-1}}\right),['real-analysis']
63,Using AM-GM to prove $\lim_{p\to 0}$ of the $p$-mean is equal to the GM.,Using AM-GM to prove  of the -mean is equal to the GM.,\lim_{p\to 0} p,"The problem in question is. Let $a_1,a_2,...,a_n>0$ . Prove that $${\lim_{p\to 0} \left(\frac{{a_1}^p+{a_2}^p+\cdots+{a_n}^p}{n}\right)^{1/p}}=(a_1a_2\cdots a_n)^{1/n}$$ i.e. show that as $p$ approches $0$ , the $p$ -mean approaches the geometric mean. The direct proof of this is simple and can be done using logorithms. However when I started this problem I had the following idea:  Notice first that by the $AM-GM$ inequality, $$\frac{a_1^p+\cdots+a_n^p}{n}\geq(a_1a_2\cdots a_n)^{p/n}$$ Thus, $$\left(\frac{a_1^p+\cdots+a_n^p}{n}\right)^{1/p}\geq(a_1a_2...a_n)^{1/n}$$ Equality is achieved in the above iff $a_1^p=a_2^p=\cdots=a_n^p$ . But when $p$ approaches $0$ , $a_1^p=a_2^p=\cdots=a_n^p=1$ . Of course this does not prove the problem in question but I am trying to use this idea to construct a proof but it is much trickier that expected. Any ideas? The most promising idea I had so far was to create a subsequence of the p-mean and prove that it is decreasing and the geometric mean is the infinimum of this subsequence. But that is very hard to do. So I guess my main question is: Could we find some special case where if the terms $a_1,a_2,\cdots,a_n$ approached some $x$ in some limit $L$ then, $L(AM)$ = $L(GM)$ ?","The problem in question is. Let . Prove that i.e. show that as approches , the -mean approaches the geometric mean. The direct proof of this is simple and can be done using logorithms. However when I started this problem I had the following idea:  Notice first that by the inequality, Thus, Equality is achieved in the above iff . But when approaches , . Of course this does not prove the problem in question but I am trying to use this idea to construct a proof but it is much trickier that expected. Any ideas? The most promising idea I had so far was to create a subsequence of the p-mean and prove that it is decreasing and the geometric mean is the infinimum of this subsequence. But that is very hard to do. So I guess my main question is: Could we find some special case where if the terms approached some in some limit then, = ?","a_1,a_2,...,a_n>0 {\lim_{p\to 0} \left(\frac{{a_1}^p+{a_2}^p+\cdots+{a_n}^p}{n}\right)^{1/p}}=(a_1a_2\cdots a_n)^{1/n} p 0 p AM-GM \frac{a_1^p+\cdots+a_n^p}{n}\geq(a_1a_2\cdots a_n)^{p/n} \left(\frac{a_1^p+\cdots+a_n^p}{n}\right)^{1/p}\geq(a_1a_2...a_n)^{1/n} a_1^p=a_2^p=\cdots=a_n^p p 0 a_1^p=a_2^p=\cdots=a_n^p=1 a_1,a_2,\cdots,a_n x L L(AM) L(GM)","['limits', 'analysis', 'inequality']"
64,Evaluating $\lim_{n \to \infty} \prod_{i=0}^{n-1} \left(\frac{n(n+1)}{2(n-1)} - i\right)^{n-i}$,Evaluating,\lim_{n \to \infty} \prod_{i=0}^{n-1} \left(\frac{n(n+1)}{2(n-1)} - i\right)^{n-i},"I am trying to find this limit. $$\lim_{n \to \infty} \prod_{i=0}^{n-1} \left(\frac{n(n+1)}{2(n-1)} - i\right)^{n-i}$$ I am not sure how to proceed. I tried to check out the values of the product for increasing values of $n$ and it looks like the limit oscillates between $+\infty$ and $-\infty$ for every 4 terms, so I believe the limit does not exist. I am not sure how I can prove that.","I am trying to find this limit. I am not sure how to proceed. I tried to check out the values of the product for increasing values of and it looks like the limit oscillates between and for every 4 terms, so I believe the limit does not exist. I am not sure how I can prove that.",\lim_{n \to \infty} \prod_{i=0}^{n-1} \left(\frac{n(n+1)}{2(n-1)} - i\right)^{n-i} n +\infty -\infty,"['sequences-and-series', 'limits']"
65,Find the limit of $\arctan(y/x)$,Find the limit of,\arctan(y/x),"I need to calculate the limit of function $\arctan(y/x)$ for the arguments $(x,y)\to(0,1)$ . Earlier I thought it's a bit easy but I just cannot get any answer, Instinctively it may look like limit is $\pi/2$ but one could also argue about two paths yielding different limits. Kindly help!! Thanks & regards","I need to calculate the limit of function for the arguments . Earlier I thought it's a bit easy but I just cannot get any answer, Instinctively it may look like limit is but one could also argue about two paths yielding different limits. Kindly help!! Thanks & regards","\arctan(y/x) (x,y)\to(0,1) \pi/2","['calculus', 'limits', 'multivariable-calculus', 'inverse-function']"
66,Prove that $\lim_{y\to0}\lim_{x\to0}(x+y\sin\frac{1}{x}) $ doesn't exist,Prove that  doesn't exist,\lim_{y\to0}\lim_{x\to0}(x+y\sin\frac{1}{x}) ,"Actually the full problem states: given a function $f(x, y) = x + y\sin\frac{1}{x}$ prove that $\lim_{x\to0}\lim_{y\to0}f(x, y)$ and $\lim_{(x, y) \to(0,0)}f(x, y) $ exist and that $\lim_{y\to0}\lim_{x\to0}f(x, y)$ doesn't exist. The first two weren't a problem really, first one is just direct calculation the other one is applying the three limits theorem. The third one confuses me. I remember there being a theorem that states something like the following: $\lim_{x \to x_0}f(x) = A$ if and only if for every sequence of real   numbers $(x_n)$ where $\forall n \quad x_n \neq x_0$ such that $\lim_{n \to \infty}x_n = x_0$ , then $\lim_{n \to \infty}f(x_n) = A$ However, in the textbook they seem to be using some other property that they haven't mentioned so far. I'm quoting now: We will prove that $\lim_{x->0}f(x, y)$ cannot exist, therefore the   entire limit cannot exist. Let's look at the sequences $a_n=(\frac{1}{n\pi})$ and $b_n =(\frac{1}{2n\pi + \frac{\pi}{2}})$ . $$\lim_{n \to > \infty}f(a_n, y) = \lim_{n \to \infty}(\frac{1}{n\pi} +0\cdot y) = 0$$ $$\lim_{n \to \infty}f(b_n, y) = y$$ Therefore the limit cannot exist. So it kind of looks like some application of the previous theorem? But still then, it looks different because the theorem doesn't state anything about the existence of the limit as far as I understood. Is this some separate theorem that I've missed? Thanks.","Actually the full problem states: given a function prove that and exist and that doesn't exist. The first two weren't a problem really, first one is just direct calculation the other one is applying the three limits theorem. The third one confuses me. I remember there being a theorem that states something like the following: if and only if for every sequence of real   numbers where such that , then However, in the textbook they seem to be using some other property that they haven't mentioned so far. I'm quoting now: We will prove that cannot exist, therefore the   entire limit cannot exist. Let's look at the sequences and . Therefore the limit cannot exist. So it kind of looks like some application of the previous theorem? But still then, it looks different because the theorem doesn't state anything about the existence of the limit as far as I understood. Is this some separate theorem that I've missed? Thanks.","f(x, y) = x + y\sin\frac{1}{x} \lim_{x\to0}\lim_{y\to0}f(x, y) \lim_{(x, y) \to(0,0)}f(x, y)  \lim_{y\to0}\lim_{x\to0}f(x, y) \lim_{x \to x_0}f(x) = A (x_n) \forall n \quad x_n \neq x_0 \lim_{n \to \infty}x_n = x_0 \lim_{n \to \infty}f(x_n) = A \lim_{x->0}f(x, y) a_n=(\frac{1}{n\pi}) b_n =(\frac{1}{2n\pi + \frac{\pi}{2}}) \lim_{n \to
> \infty}f(a_n, y) = \lim_{n \to \infty}(\frac{1}{n\pi} +0\cdot y) = 0 \lim_{n \to \infty}f(b_n, y) = y","['limits', 'multivariable-calculus']"
67,Limit of the composition function $\sin(\cos(x))/\cos(x)$ at $\pi/2$ in Apostol Exercises 3.8,Limit of the composition function  at  in Apostol Exercises 3.8,\sin(\cos(x))/\cos(x) \pi/2,"In Apostol calculus exercise there is a bunch of exercises where we need to find a limit of a composite function. One example is this: $\lim_\limits{x \rightarrow \frac{\pi}{2}} \frac{\sin(\cos(x))}{\cos(x)}$ Well, intuitively we simply take $\cos(x) \rightarrow 0$ as $x \rightarrow \pi/2$ . Then $\lim_\limits{x \rightarrow \frac{\pi}{2}} \frac{\sin(\cos(x))}{\cos(x)} = 1$ , since the limit of $\sin(x)/x$ is 1 as $x \rightarrow 0$ . Multiple online solutions suggest the same without any explanation. How to show this rigorously? In Apostol, there is a proof that composition of two continuous functions is continuous at any $x = p$ . However, there is no proof about general limits. Moreover, we know that $\sin(x)/x$ is not even defined at $0$ . How is it allowed to substitute $\cos(x) \rightarrow 0$ into this outer sine then? PS. L'Hopital rule has not been specified yet. The only available option is delta-epsilon arguments, and simplest limit rules: sum, difference, product, and division. Squeezing limit theorem can be used (proved before). Plus sin(x)/x limit at 0 has been proven with squeezing limit theorem.","In Apostol calculus exercise there is a bunch of exercises where we need to find a limit of a composite function. One example is this: Well, intuitively we simply take as . Then , since the limit of is 1 as . Multiple online solutions suggest the same without any explanation. How to show this rigorously? In Apostol, there is a proof that composition of two continuous functions is continuous at any . However, there is no proof about general limits. Moreover, we know that is not even defined at . How is it allowed to substitute into this outer sine then? PS. L'Hopital rule has not been specified yet. The only available option is delta-epsilon arguments, and simplest limit rules: sum, difference, product, and division. Squeezing limit theorem can be used (proved before). Plus sin(x)/x limit at 0 has been proven with squeezing limit theorem.",\lim_\limits{x \rightarrow \frac{\pi}{2}} \frac{\sin(\cos(x))}{\cos(x)} \cos(x) \rightarrow 0 x \rightarrow \pi/2 \lim_\limits{x \rightarrow \frac{\pi}{2}} \frac{\sin(\cos(x))}{\cos(x)} = 1 \sin(x)/x x \rightarrow 0 x = p \sin(x)/x 0 \cos(x) \rightarrow 0,"['real-analysis', 'calculus', 'limits', 'alternative-proof', 'epsilon-delta']"
68,Confusion Over Ratio Limits and Difference Limits,Confusion Over Ratio Limits and Difference Limits,,"I have two functions, $f(x)$ : $$f(x)=\left(H\left(x\right)+\ln\left(H\left(x\right)^{\left(e^{H\left(x\right)}\right)}\right)\right)$$ and $g(x)$ : $$g(x)=e^\gamma x\ln\left(\gamma+\ln x\right)+\frac{x}{\ln\left(\ln\left(x\right)\right)}$$ where $H(x)$ is the harmonic series. What I find confusing is that $$\lim_{x \to \infty}\frac{f(x)}{g(x)}=1$$ but $$\lim_{x \to \infty}f(x)-g(x)\neq 1$$ Is there any way to consolidate these two facts?","I have two functions, : and : where is the harmonic series. What I find confusing is that but Is there any way to consolidate these two facts?",f(x) f(x)=\left(H\left(x\right)+\ln\left(H\left(x\right)^{\left(e^{H\left(x\right)}\right)}\right)\right) g(x) g(x)=e^\gamma x\ln\left(\gamma+\ln x\right)+\frac{x}{\ln\left(\ln\left(x\right)\right)} H(x) \lim_{x \to \infty}\frac{f(x)}{g(x)}=1 \lim_{x \to \infty}f(x)-g(x)\neq 1,"['calculus', 'limits']"
69,Limit with integral - L'Hospital's rule,Limit with integral - L'Hospital's rule,,Find $$ \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)} $$ Let $$ F(t) \mbox{ such that } F'(t) = (1 + \sin(t))^{1/t}  $$ Then we use L'Hospital's rule: $$ \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)} = \frac{F'(x^2) - F'(0) }{\sin(x) + x \cos(x)} $$ but $$ F'(0) $$ is not defined (we have $1/t$ part as exponent),Find Let Then we use L'Hospital's rule: but is not defined (we have part as exponent), \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)}   F(t) \mbox{ such that } F'(t) = (1 + \sin(t))^{1/t}    \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)} = \frac{F'(x^2) - F'(0) }{\sin(x) + x \cos(x)}   F'(0)  1/t,['integration']
70,Polynomials: $f_n \to p$ and $p$ has distinct real roots implies $f_n$ eventually has real roots.,Polynomials:  and  has distinct real roots implies  eventually has real roots.,f_n \to p p f_n,"Recall that a polynomial is hyperbolic if all its roots are real. Let $p$ and $\{f_n\}_{n \geq 1}$ be polynomials of degree $d$ .  Suppose $\lim_{n \to \infty} f_n(x) = p(x)$ where the limit is taken pointwise with respect to $x$ .  Prove that if $p$ is hyperbolic and has distinct roots, then there exists an $N$ such that for all $n \geq N$ , the polynomials $f_n$ are hyperbolic. It is false if $p(x)$ does not have distinct roots, for example $f_n(x)=x^2+\frac{1}{n}.$","Recall that a polynomial is hyperbolic if all its roots are real. Let and be polynomials of degree .  Suppose where the limit is taken pointwise with respect to .  Prove that if is hyperbolic and has distinct roots, then there exists an such that for all , the polynomials are hyperbolic. It is false if does not have distinct roots, for example",p \{f_n\}_{n \geq 1} d \lim_{n \to \infty} f_n(x) = p(x) x p N n \geq N f_n p(x) f_n(x)=x^2+\frac{1}{n}.,"['limits', 'analysis', 'polynomials']"
71,Find sequence limit $x_{n+1} = \frac{3}{4} x_n + \frac{1}{4} \int_0^{|x_n|} f(x) dx$,Find sequence limit,x_{n+1} = \frac{3}{4} x_n + \frac{1}{4} \int_0^{|x_n|} f(x) dx,"I am doing my Analysis class revision, and here is one of the problem I saw in the past exam papers: Let $f: \mathbb{R} \to [0,1]$ be continuous and $x_0 \in (0,1)$ . Define $x_n$ via recurrence: \begin{equation} x_{n+1} = \frac{3}{4} x_n^2 + \frac{1}{4}\int_0^{|x_n|}f(x)dx \end{equation} Prove that $x_n$ is convergent, and find its limit. I think it is not hard to show the convergence. Since $f(x) \geq 0$ , the integral is non-negative. Hence inductively $x_n \geq 0$ for all $n$ . We could drop the absolute value. Also by induction, $x_n \in (0,1)$ , then we have $x_n^2 < x_n$ , and we could conclude that $x_n$ is decreasing, because $x_{n+1} \leq \frac{3}{4} x_n + \frac{1}{4} x_n = x_n$ . A decreasing function with a lower bound (i.e, $x_n \geq 0$ ) is convergent. However, I am not sure how to find the limit. Usually, if we have a sequence defined by recurrence, we could simply make $x_{n+1} = x_n = x$ , and solve for $x$ .   However, here we have an integral there, I am wondering if we need to do some estimations and use the Sandwich Theorem.","I am doing my Analysis class revision, and here is one of the problem I saw in the past exam papers: Let be continuous and . Define via recurrence: Prove that is convergent, and find its limit. I think it is not hard to show the convergence. Since , the integral is non-negative. Hence inductively for all . We could drop the absolute value. Also by induction, , then we have , and we could conclude that is decreasing, because . A decreasing function with a lower bound (i.e, ) is convergent. However, I am not sure how to find the limit. Usually, if we have a sequence defined by recurrence, we could simply make , and solve for .   However, here we have an integral there, I am wondering if we need to do some estimations and use the Sandwich Theorem.","f: \mathbb{R} \to [0,1] x_0 \in (0,1) x_n \begin{equation}
x_{n+1} = \frac{3}{4} x_n^2 + \frac{1}{4}\int_0^{|x_n|}f(x)dx
\end{equation} x_n f(x) \geq 0 x_n \geq 0 n x_n \in (0,1) x_n^2 < x_n x_n x_{n+1} \leq \frac{3}{4} x_n + \frac{1}{4} x_n = x_n x_n \geq 0 x_{n+1} = x_n = x x","['real-analysis', 'sequences-and-series', 'limits', 'analysis']"
72,"If $f(x,y)=9-x^2-y^2$ if $x^2+y^2\leq9$ and $f(x,y)=0$ if $x^2+y^2>9$ study what happens at $(3,0)$",If  if  and  if  study what happens at,"f(x,y)=9-x^2-y^2 x^2+y^2\leq9 f(x,y)=0 x^2+y^2>9 (3,0)","If $$f(x,y)=\begin{cases}9-x^2-y^2&\text{if }x^2+y^2\leq9\\0&\text{if }x^2+y^2>9\end{cases}$$ study the continuity and existence of partial derivative with respect to $y$ at point $(3,0)$ . The graph of the domain of $f$ is: Continuity study at $(3,0)$ : $f(3,0)=9-3^2-0=0$ , but I do not know how to find $$\lim_{(x,y)\to(3,0)}f(x,y).$$ I tried the following: $$\lim_{(x,y)\to(3,0)}f(x,y)=\left\{\begin{array}{l}\displaystyle\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}(9-(x^2+y^2))=9-9=0\\\displaystyle\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}\text{??}=\text{??????}\end{array}\right.$$ but then I realized that the ""curve"" $C_2$ is actually NOT a curve but a set of infinite points, as shown in the previous image. Existence of partial derivative with respect to $y$ at $(3,0)$ : I know that I need to study whether $$\frac\partial{\partial y}f(3,0)=f'((3,0);(0,1))=f_y(3,0)=\lim_{h\to0}\frac{f((3,0)+h(0,1))-f(3,0)}{h}=\lim_{h\to0}\frac{f(3,h)}{h}$$ exists or not, but I am not able to even find that limit. Any help? Thanks!!","If study the continuity and existence of partial derivative with respect to at point . The graph of the domain of is: Continuity study at : , but I do not know how to find I tried the following: but then I realized that the ""curve"" is actually NOT a curve but a set of infinite points, as shown in the previous image. Existence of partial derivative with respect to at : I know that I need to study whether exists or not, but I am not able to even find that limit. Any help? Thanks!!","f(x,y)=\begin{cases}9-x^2-y^2&\text{if }x^2+y^2\leq9\\0&\text{if }x^2+y^2>9\end{cases} y (3,0) f (3,0) f(3,0)=9-3^2-0=0 \lim_{(x,y)\to(3,0)}f(x,y). \lim_{(x,y)\to(3,0)}f(x,y)=\left\{\begin{array}{l}\displaystyle\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}(9-(x^2+y^2))=9-9=0\\\displaystyle\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}\text{??}=\text{??????}\end{array}\right. C_2 y (3,0) \frac\partial{\partial y}f(3,0)=f'((3,0);(0,1))=f_y(3,0)=\lim_{h\to0}\frac{f((3,0)+h(0,1))-f(3,0)}{h}=\lim_{h\to0}\frac{f(3,h)}{h}","['limits', 'multivariable-calculus', 'piecewise-continuity']"
73,Proof that $x\sin(x)$ has infinite accumulation points,Proof that  has infinite accumulation points,x\sin(x),"I have to find a sequence with infinite many accumulation points and intuitively I thought about $\sin(x)$ - since it is periodic, it has points from its codomain that get repeated infinite many times. However, not infinite many points since all the y values that come from a sine function are bounded in the interval [-1,1].  But what about $xsin(x)$ ? The definition of accumuation point is: let $(a_n$ )  be a sequence of real numbers. The number a is said to be an accumulation point of $(a_n)$ if there exists a subsequence $(an_k)$ such that $\lim\limits_{k\to\infty} an_k=a$ . If my intuition is correct from the plots it looks like each point get repeated infinite many times, together with the growth of the x value (in both directions). However I also need to come up with a formal proof to back up (or destroy) my intuition and here is where I got stuck. I thought about trying to describe a subsequence by exploiting the periodical behaviour, like I can say that 0 is an accumulation point of $sin(x)$ by choosing the subsequence $sin (\pi  k),k\in\ \Bbb{Z}$ . Am I on the wrong path or does it make some sense? Any feedback would be greatly appreciated.","I have to find a sequence with infinite many accumulation points and intuitively I thought about - since it is periodic, it has points from its codomain that get repeated infinite many times. However, not infinite many points since all the y values that come from a sine function are bounded in the interval [-1,1].  But what about ? The definition of accumuation point is: let )  be a sequence of real numbers. The number a is said to be an accumulation point of if there exists a subsequence such that . If my intuition is correct from the plots it looks like each point get repeated infinite many times, together with the growth of the x value (in both directions). However I also need to come up with a formal proof to back up (or destroy) my intuition and here is where I got stuck. I thought about trying to describe a subsequence by exploiting the periodical behaviour, like I can say that 0 is an accumulation point of by choosing the subsequence . Am I on the wrong path or does it make some sense? Any feedback would be greatly appreciated.","\sin(x) xsin(x) (a_n (a_n) (an_k) \lim\limits_{k\to\infty} an_k=a sin(x) sin (\pi 
k),k\in\ \Bbb{Z}","['real-analysis', 'limits', 'proof-verification', 'periodic-functions']"
74,Computing the limit of a function given the limit of its derivative,Computing the limit of a function given the limit of its derivative,,"Let $f:\mathbb{R}\to\mathbb{R}$ such that $f\in C^\infty$ and $f'(x)>0$ for all $x\in\mathbb{R}$ . Then, is it true that $$\lim_{x\to+\infty}f'(x)=0\Rightarrow\lim_{x\to+\infty} f(x)=a$$ for some $a\in\mathbb{R}$ ? I believe this is true due to the mean value theorem but I can't seem to properly show it, any help on where to start?","Let such that and for all . Then, is it true that for some ? I believe this is true due to the mean value theorem but I can't seem to properly show it, any help on where to start?",f:\mathbb{R}\to\mathbb{R} f\in C^\infty f'(x)>0 x\in\mathbb{R} \lim_{x\to+\infty}f'(x)=0\Rightarrow\lim_{x\to+\infty} f(x)=a a\in\mathbb{R},"['real-analysis', 'calculus', 'limits', 'derivatives']"
75,Find $\lim\limits_{n\rightarrow\infty} \sum\limits_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n}$,Find,\lim\limits_{n\rightarrow\infty} \sum\limits_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n},"Let $(a_n)_{n\geq1}$ a sequence strictly increasing of real positive numbers such that $\lim\limits_{n\to\infty} \frac{a_{n+1}}{a_n}=1$ . Find $$\lim_{n\to\infty} \sum_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n}$$ I know this should be solved using Riemann integration, but my only significant progress was the finding of the partition $$0\leq\frac{a_1}{a_1+...+a_n}\leq\frac{a_1+a_2}{a_1+...+a_n}\leq...\leq\frac{a_1+...+a_n}{a_1+...+a_n}=1$$ for the interval $[0,1]$ .","Let a sequence strictly increasing of real positive numbers such that . Find I know this should be solved using Riemann integration, but my only significant progress was the finding of the partition for the interval .","(a_n)_{n\geq1} \lim\limits_{n\to\infty} \frac{a_{n+1}}{a_n}=1 \lim_{n\to\infty} \sum_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n} 0\leq\frac{a_1}{a_1+...+a_n}\leq\frac{a_1+a_2}{a_1+...+a_n}\leq...\leq\frac{a_1+...+a_n}{a_1+...+a_n}=1 [0,1]",['sequences-and-series']
76,Are these two definitions of limit actually equivalent?,Are these two definitions of limit actually equivalent?,,"I've seen the following definition of limits in the given books: Laczkovich/Sós: Real Analysis Let $f$ be defined on an open interval containing $a$ , excluding perhaps $a$ itself. The limit of $f$ at $a$ exists and has value $b$ if for all $\epsilon>0$ , there exists a $\delta >0$ such that $$|f(x)-b|<\epsilon, \text{ if } 0<|x-a|<\delta$$ On the other hand: Zorich: Mathematical Analysis 1 On a function $f:E\mapsto \Bbb{R}$ . $$\forall \epsilon>0\;\; \exists \delta >0 \;\; \forall x\in E \;\; (0<|x-a|<\delta \implies |f(x)-A|<\epsilon)$$ And this sides up with Spivak's definition on his Calculus and many other books I've seen. In the second definition, the inclusion of $\forall x\in E$ seems to tell me something very different. I tried to prove the inexistence of a limit in the Dirichlet function yersterday with the first definition and couldn't but when I read Zorich's definition, it was pretty straightforward so, are both definitions actually equivalent? I guess in the first definition, the authors expect us to understand $\forall x \in E$ by pointing to a figure in the page perhaps.","I've seen the following definition of limits in the given books: Laczkovich/Sós: Real Analysis Let be defined on an open interval containing , excluding perhaps itself. The limit of at exists and has value if for all , there exists a such that On the other hand: Zorich: Mathematical Analysis 1 On a function . And this sides up with Spivak's definition on his Calculus and many other books I've seen. In the second definition, the inclusion of seems to tell me something very different. I tried to prove the inexistence of a limit in the Dirichlet function yersterday with the first definition and couldn't but when I read Zorich's definition, it was pretty straightforward so, are both definitions actually equivalent? I guess in the first definition, the authors expect us to understand by pointing to a figure in the page perhaps.","f a a f a b \epsilon>0 \delta >0 |f(x)-b|<\epsilon, \text{ if } 0<|x-a|<\delta f:E\mapsto \Bbb{R} \forall \epsilon>0\;\; \exists \delta >0 \;\; \forall x\in E \;\; (0<|x-a|<\delta \implies |f(x)-A|<\epsilon) \forall x\in E \forall x \in E","['real-analysis', 'limits']"
77,Confused by Baby Rudin chapter $3$ problem $19$,Confused by Baby Rudin chapter  problem,3 19,"Associate to each sequence $a={a_n}$ , in which $a_n$ is $0$ or $2$ , the real number $x(a)=\sum \frac{a_n}{3^n}$ . Prove all the $x(a)$ is precisely the Cantor set. My attempt: If I let $a_n=2$ then $x(a)=1$ , $a_n=0,\text{ then }   x(a)=0$ . So, $x(a)\in [0,1]$ . That problem looks right and I need a more precise proof. I notice when I take $a_n=2$ , I do the sum from $1$ to $n$ it always the right point of interval I know a technology that proof two set are equal if I prove $A\subset B\,\text{ and }B \subset A$ then $A=B$ but it seems like I can’t use that technology.","Associate to each sequence , in which is or , the real number . Prove all the is precisely the Cantor set. My attempt: If I let then , . So, . That problem looks right and I need a more precise proof. I notice when I take , I do the sum from to it always the right point of interval I know a technology that proof two set are equal if I prove then but it seems like I can’t use that technology.","a={a_n} a_n 0 2 x(a)=\sum \frac{a_n}{3^n} x(a) a_n=2 x(a)=1 a_n=0,\text{ then }   x(a)=0 x(a)\in [0,1] a_n=2 1 n A\subset B\,\text{ and }B \subset A A=B","['real-analysis', 'calculus', 'limits', 'analysis']"
78,"Let $\alpha,\beta \in \mathbb{R}$ then find conditions for when $\lim\limits_{(x,y)\rightarrow(0,0)}\frac{x^{\alpha}y^\beta}{x^2+xy+y^2}$ exists.",Let  then find conditions for when  exists.,"\alpha,\beta \in \mathbb{R} \lim\limits_{(x,y)\rightarrow(0,0)}\frac{x^{\alpha}y^\beta}{x^2+xy+y^2}","I don't know how to approach this exercise, I tried looking at directional paths $y=mx$ and found out that if $\alpha+\beta-2=0$ then the limit doesn't exist. From the same directional paths I think that the condition $\alpha+\beta-2<0$ means the limit doesn't exist since if I approach $0$ from the right then the limit must be $+\infty$ and from the left is $-\infty$ . Though I really don't know if I used the definition correctly, is this part right? I'm lost at $\alpha+\beta-2>0$ , I know the limit must be $0$ but I don't know how to get extra conditions so that I get a for sure convergent limit.","I don't know how to approach this exercise, I tried looking at directional paths and found out that if then the limit doesn't exist. From the same directional paths I think that the condition means the limit doesn't exist since if I approach from the right then the limit must be and from the left is . Though I really don't know if I used the definition correctly, is this part right? I'm lost at , I know the limit must be but I don't know how to get extra conditions so that I get a for sure convergent limit.",y=mx \alpha+\beta-2=0 \alpha+\beta-2<0 0 +\infty -\infty \alpha+\beta-2>0 0,"['calculus', 'limits', 'multivariable-calculus']"
79,$\lim_{x\to0}\frac{\arcsin x-\sin x}{x^3}$ without series or L'Hospital,without series or L'Hospital,\lim_{x\to0}\frac{\arcsin x-\sin x}{x^3},$$\lim_{x\to0}\frac{\arcsin x-\sin x}{x^3}$$ without using series or L'Hospital Is there any ohter simpler method? Expansion of $\arcsin$ is not trivial like tha of sine and L'Hospital is too cumbersome here. Source -Question $2.10$,without using series or L'Hospital Is there any ohter simpler method? Expansion of is not trivial like tha of sine and L'Hospital is too cumbersome here. Source -Question,\lim_{x\to0}\frac{\arcsin x-\sin x}{x^3} \arcsin 2.10,"['limits', 'limits-without-lhopital']"
80,How to evaluate $\lim_{n->\infty}\frac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\frac{3}{n}}}{\sqrt[6]{n^2+\sin{\frac{2}{n}}}-\sqrt[3]{n}}?$,How to evaluate,\lim_{n->\infty}\frac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\frac{3}{n}}}{\sqrt[6]{n^2+\sin{\frac{2}{n}}}-\sqrt[3]{n}}?,I tried to get rid off cube root as written below but still can not get throught the next steps. What should be the right step to take after the steps below? Did I start as I should or do I have to take completely different approach? $\lim_{n->\infty}\dfrac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\dfrac{3}{n}}}{\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n}}=$ $=\lim_{n->\infty}\dfrac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\dfrac{3}{n}}}{\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n}}\dfrac{(\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2}{(\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2}$ $=\lim_{n->\infty}\dfrac{1-\cos\dfrac{3}{n}}{(\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n})((\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2)}$,I tried to get rid off cube root as written below but still can not get throught the next steps. What should be the right step to take after the steps below? Did I start as I should or do I have to take completely different approach?,\lim_{n->\infty}\dfrac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\dfrac{3}{n}}}{\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n}}= =\lim_{n->\infty}\dfrac{\sqrt[3]{n+1}-\sqrt[3]{n+\cos{}\dfrac{3}{n}}}{\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n}}\dfrac{(\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2}{(\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2} =\lim_{n->\infty}\dfrac{1-\cos\dfrac{3}{n}}{(\sqrt[6]{n^2+\sin{\dfrac{2}{n}}}-\sqrt[3]{n})((\sqrt[3]{n+1})^2+\sqrt[3]{n+1}\sqrt[3]{n+\cos{}\dfrac{3}{n}}+(\sqrt[3]{n+\cos{}\dfrac{3}{n}})^2)},"['real-analysis', 'calculus', 'limits']"
81,How to find limit $\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2}$,How to find limit,\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2},How can I find limit $$\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2}$$ I solve subproblem: $$\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h} = \lim_{h\rightarrow0^-}\frac{1}{e^{1/|h|}\cdot h} =\lim_{y\rightarrow -\infty}\frac{y}{e^{|y|}}=0$$ but I have no idea how to apply that for main target,How can I find limit I solve subproblem: but I have no idea how to apply that for main target,\lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h^2} \lim_{h\rightarrow0^-}\frac{e^{-1/|h|}}{h} = \lim_{h\rightarrow0^-}\frac{1}{e^{1/|h|}\cdot h} =\lim_{y\rightarrow -\infty}\frac{y}{e^{|y|}}=0,['limits']
82,"Determining the limit of $\sqrt{x^2+y^2} \ln(x^2+y^2)$ as $(x,y)$ approaches $(0,0)$",Determining the limit of  as  approaches,"\sqrt{x^2+y^2} \ln(x^2+y^2) (x,y) (0,0)","I want to evaluate the limit of $\sqrt{x^2+y^2}\ln(x^2+y^2)$ as $(x,y)$ approaches $(0,0)$ . Using polar coordinates, $x=r \cos \theta$ , $y= r \sin \theta$ , I obtain $\sqrt{x^2+y^2}\ln(x^2+y^2)=2r\ln(r)$ but I can not understand why this would be more simple than the original expression, or how I can proceed with this problem.","I want to evaluate the limit of as approaches . Using polar coordinates, , , I obtain but I can not understand why this would be more simple than the original expression, or how I can proceed with this problem.","\sqrt{x^2+y^2}\ln(x^2+y^2) (x,y) (0,0) x=r \cos \theta y= r \sin \theta \sqrt{x^2+y^2}\ln(x^2+y^2)=2r\ln(r)","['limits', 'multivariable-calculus']"
83,Why do the properties of $\lim\limits_{n\rightarrow\infty}(x_n)$ still hold for $\lim\limits_{x\rightarrow x_0}f(x)$,Why do the properties of  still hold for,\lim\limits_{n\rightarrow\infty}(x_n) \lim\limits_{x\rightarrow x_0}f(x),"Where is the transition of sequences and functions? With properties I mean for example rules like the sandwich theorem $$\lim_{n\rightarrow\infty}a_n=K, \lim_{n\rightarrow\infty}c_n=K\text{ and }a_n\leq b_n\leq c_n\forall_{n\in\mathbb{N}}\Rightarrow\lim_{n\rightarrow\infty}b_n=K$$ Or that one can ""pull out"" the constant, i.e $$\lim_{n\rightarrow\infty}a_n=K\Rightarrow \lim_{n\rightarrow\infty}ca_n=c\lim_{n\rightarrow\infty}a_n=cK$$ To name a few.","Where is the transition of sequences and functions? With properties I mean for example rules like the sandwich theorem Or that one can ""pull out"" the constant, i.e To name a few.","\lim_{n\rightarrow\infty}a_n=K, \lim_{n\rightarrow\infty}c_n=K\text{ and }a_n\leq b_n\leq c_n\forall_{n\in\mathbb{N}}\Rightarrow\lim_{n\rightarrow\infty}b_n=K \lim_{n\rightarrow\infty}a_n=K\Rightarrow \lim_{n\rightarrow\infty}ca_n=c\lim_{n\rightarrow\infty}a_n=cK","['sequences-and-series', 'limits']"
84,Proving a strict inequality in the limit,Proving a strict inequality in the limit,,"I want to prove that $$ \lim_{k \to \infty} \left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < e .$$ Using the $AM-GM$ inequality we arrive at $$\left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < \left(\frac{k + 1 - \frac{1}{2^k} }{k} \right)^k = \left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k < \left(1 + \frac{1}{k} \right)^k < e.$$ The first inequality is strict because the terms are different.However, I know that in the limit, strict inequalities can transform into equalities. Since the limit of $\left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k$ when $k$ goes to infinity is also $e$ , how could I prove a strict inequality?","I want to prove that Using the inequality we arrive at The first inequality is strict because the terms are different.However, I know that in the limit, strict inequalities can transform into equalities. Since the limit of when goes to infinity is also , how could I prove a strict inequality?", \lim_{k \to \infty} \left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < e . AM-GM \left( 1 + \frac{1}{2} \right) \left(1 + \frac{1}{4} \right)...\left( 1 + \frac{1}{2^k} \right) < \left(\frac{k + 1 - \frac{1}{2^k} }{k} \right)^k = \left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k < \left(1 + \frac{1}{k} \right)^k < e. \left( 1 + \frac{1}{k} - \frac{1}{k2^{k}}\right)^k k e,"['calculus', 'limits']"
85,Evaluate the limit $\lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right)$,Evaluate the limit,\lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right),"Evaluate the limit: $$ \lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right)\\ a>0\\ a \ne 1 $$ I've started with defining another sequence. Let: $$ y_n = a^{x_n} = \frac{4^nn!}{n^n} $$ Consider the fraction: $$ \frac{y_{n+1}}{y_n} = \frac{4^{n+1}(n+1)!}{(n+1)^{n+1}} \cdot \frac{n^n}{4^nn!}\\ = \frac{4n^n}{(n+1)^n} $$ Consider the limit: $$ \begin{align} \lim_{n\to\infty}\frac{y_{n+1}}{y_n} &= \lim_{n\to\infty}\frac{4n^n}{(n+1)^n} \\ &= \lim_{n\to\infty}4\left(\frac{n}{n+1} \right)^n \\  &= {4\over e} > 1 \end{align} $$ So by this $y_n$ is divergent. Which means: $$ \lim_{n\to\infty}y_n = \infty $$ Now I'm having difficulties translating it in a backward direction. We have that: $$ \lim_{n\to\infty}y_n = \lim_{n\to\infty}a^{x_n} = \infty $$ Or: $$ \log_a \lim_{n\to\infty}a^{x_n} = \log_a(\infty) $$ The answer suggests that: $$ \lim_{n\to\infty}x_n =  \begin{cases} +\infty,\ a > 1\\ -\infty,\ 0 < a < 1 \end{cases} $$ And I don't see where this appears when going backward from $a^{x_n}$ to $x_n$ . Could you please explain that to me?",Evaluate the limit: I've started with defining another sequence. Let: Consider the fraction: Consider the limit: So by this is divergent. Which means: Now I'm having difficulties translating it in a backward direction. We have that: Or: The answer suggests that: And I don't see where this appears when going backward from to . Could you please explain that to me?,"
\lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right)\\
a>0\\
a \ne 1
 
y_n = a^{x_n} = \frac{4^nn!}{n^n}
 
\frac{y_{n+1}}{y_n} = \frac{4^{n+1}(n+1)!}{(n+1)^{n+1}} \cdot \frac{n^n}{4^nn!}\\
= \frac{4n^n}{(n+1)^n}
 
\begin{align}
\lim_{n\to\infty}\frac{y_{n+1}}{y_n} &= \lim_{n\to\infty}\frac{4n^n}{(n+1)^n} \\
&= \lim_{n\to\infty}4\left(\frac{n}{n+1} \right)^n \\ 
&= {4\over e} > 1
\end{align}
 y_n 
\lim_{n\to\infty}y_n = \infty
 
\lim_{n\to\infty}y_n = \lim_{n\to\infty}a^{x_n} = \infty
 
\log_a \lim_{n\to\infty}a^{x_n} = \log_a(\infty)
 
\lim_{n\to\infty}x_n = 
\begin{cases}
+\infty,\ a > 1\\
-\infty,\ 0 < a < 1
\end{cases}
 a^{x_n} x_n","['calculus', 'sequences-and-series', 'limits']"
86,"$f$ is continuous on $[0,1]$, $f(0)=f(1)$,$n\in \mathbb N$ constant $\Rightarrow$ There exists $x\in[0,1-\frac{1}{n}]$ : $f(x+\frac{1}{n})=f(x).$ [duplicate]","is continuous on , , constant  There exists  :  [duplicate]","f [0,1] f(0)=f(1) n\in \mathbb N \Rightarrow x\in[0,1-\frac{1}{n}] f(x+\frac{1}{n})=f(x).","This question already has an answer here : If $f$ is continuous, then there exists $x+\frac{1}{n}\in[0,1]$ and $f\left(x+\frac{1}{n}\right)=f(x)$ (1 answer) Closed 5 years ago . I've been trying to prove this statement: Let $f$ be a continuous function on $[0,1]$ such that $f(0)=f(1):=c$ , and let $n$ be a natural constant. Prove that there exists $x\in[0,1-\frac{1}{n}], x\in \mathbb R$ such that $f(x+\frac{1}{n})=f(x).$ I tried to use the Intermediate Value Theorem by defining $g(x)=f(x+\frac{1}{n})-f(x)$ . I could see that $g(0)=f(\frac{1}{n})-c$ and $g(1-\frac{1}{n})=c-f(1-\frac{1}{n})$ yet I could not figure out how to prove that $\mathrm s\mathrm i\mathrm g\mathrm n (g(0)\cdot g(1-\frac{1}{n}))=-1.$ Edit: Please notice that this statement is not (necessarily) true for every n. The statement is about some constant n. Thank you and have a good day!.","This question already has an answer here : If $f$ is continuous, then there exists $x+\frac{1}{n}\in[0,1]$ and $f\left(x+\frac{1}{n}\right)=f(x)$ (1 answer) Closed 5 years ago . I've been trying to prove this statement: Let be a continuous function on such that , and let be a natural constant. Prove that there exists such that I tried to use the Intermediate Value Theorem by defining . I could see that and yet I could not figure out how to prove that Edit: Please notice that this statement is not (necessarily) true for every n. The statement is about some constant n. Thank you and have a good day!.","f [0,1] f(0)=f(1):=c n x\in[0,1-\frac{1}{n}], x\in \mathbb R f(x+\frac{1}{n})=f(x). g(x)=f(x+\frac{1}{n})-f(x) g(0)=f(\frac{1}{n})-c g(1-\frac{1}{n})=c-f(1-\frac{1}{n}) \mathrm s\mathrm i\mathrm g\mathrm n (g(0)\cdot g(1-\frac{1}{n}))=-1.","['calculus', 'limits', 'functions', 'continuity']"
87,"Proof verification. $\{x_n\}$ is a sequence such that $|x_{n+1} - x_n| \le C\alpha^n$ for $\alpha\in (0, 1), n\in\Bbb N$. Prove $x_n$ converges.",Proof verification.  is a sequence such that  for . Prove  converges.,"\{x_n\} |x_{n+1} - x_n| \le C\alpha^n \alpha\in (0, 1), n\in\Bbb N x_n","Let $\{x_n\}, n\in \Bbb N$ denote a sequence such that: $$ \begin{cases} |x_{n+1} - x_n| \le C\alpha^n \\ 0 < \alpha < 1 \end{cases} $$ Prove $\{x_n\}$ converges. Given the fact $|x_{n+1} - x_n| \le C\alpha^n$ consider the following inequalities: $$ |x_{n+1} - x_n| \le C\alpha^n \\ |x_{n+2} - x_{n+1}| \le C\alpha^{n+1} \\ |x_{n+3} - x_{n+2}| \le C\alpha^{n+2} \\ \dots \\ |x_{n+p+1} - x_{n+p}| \le C\alpha^{n+p} \\ $$ Consider the sum of the inequalities: $$ |x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + |x_{n+3} - x_{n+2}| + \cdots + |x_{n+p+1} - x_{n+p}| \\  \le \sum_{k=0}^p C\alpha^{n+k} = C \sum_{k=0}^p \alpha^{n+k} \tag1 $$ By geometric sum: $$ C \sum_{k=0}^p \alpha^{n+k} = C \cdot \frac{\alpha^n(1-\alpha^{p + 1})}{1-\alpha} \le C \cdot \frac{\alpha^n}{1-\alpha} $$ Lets fix some $\epsilon > 0$ , and $N\in \Bbb N$ such that: $$ C\cdot \frac{\alpha^{N}}{1-\alpha} < \epsilon $$ Rewrite $\alpha$ as: $$ \alpha = \frac{1}{1+r},\ r \in \Bbb R_{>0} $$ Thus: $$ C\cdot \frac{1}{(1-\alpha)(1+r)^N} < \epsilon \\ (1+r)^N > {C\over (1-\alpha)\epsilon} \\ N > \log_{1+r} {C\over (1-\alpha)\epsilon} $$ Returning to $(1)$ we have by triangular inequality: $$ |x_{n}- x_{n+1} + x_{n+1} - x_{n+2} + \cdots + x_{n+p} - x_{n+p+1}| \\ \le |x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + \cdots + |x_{n+p+1} - x_{n+p}| $$ Since the values are telescoping we obtain: $$ |x_{n} - x_{n+p+1}| < C \cdot \frac{\alpha^n}{1-\alpha} < \epsilon $$ Now if we choose: $$ n > N > \log_{1+r} {C\over (1-\alpha)\epsilon} $$ we obtain a regular definition of the Cauchy criteria, which means $\{x_n\}$ is a convergent sequence. Could you please verify the reasoning above? Thank you!","Let denote a sequence such that: Prove converges. Given the fact consider the following inequalities: Consider the sum of the inequalities: By geometric sum: Lets fix some , and such that: Rewrite as: Thus: Returning to we have by triangular inequality: Since the values are telescoping we obtain: Now if we choose: we obtain a regular definition of the Cauchy criteria, which means is a convergent sequence. Could you please verify the reasoning above? Thank you!","\{x_n\}, n\in \Bbb N 
\begin{cases}
|x_{n+1} - x_n| \le C\alpha^n \\
0 < \alpha < 1
\end{cases}
 \{x_n\} |x_{n+1} - x_n| \le C\alpha^n 
|x_{n+1} - x_n| \le C\alpha^n \\
|x_{n+2} - x_{n+1}| \le C\alpha^{n+1} \\
|x_{n+3} - x_{n+2}| \le C\alpha^{n+2} \\
\dots \\
|x_{n+p+1} - x_{n+p}| \le C\alpha^{n+p} \\
 
|x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + |x_{n+3} - x_{n+2}| + \cdots + |x_{n+p+1} - x_{n+p}| \\ 
\le \sum_{k=0}^p C\alpha^{n+k} = C \sum_{k=0}^p \alpha^{n+k} \tag1
 
C \sum_{k=0}^p \alpha^{n+k} = C \cdot \frac{\alpha^n(1-\alpha^{p + 1})}{1-\alpha} \le C \cdot \frac{\alpha^n}{1-\alpha}
 \epsilon > 0 N\in \Bbb N 
C\cdot \frac{\alpha^{N}}{1-\alpha} < \epsilon
 \alpha 
\alpha = \frac{1}{1+r},\ r \in \Bbb R_{>0}
 
C\cdot \frac{1}{(1-\alpha)(1+r)^N} < \epsilon \\
(1+r)^N > {C\over (1-\alpha)\epsilon} \\
N > \log_{1+r} {C\over (1-\alpha)\epsilon}
 (1) 
|x_{n}- x_{n+1} + x_{n+1} - x_{n+2} + \cdots + x_{n+p} - x_{n+p+1}| \\ \le |x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + \cdots + |x_{n+p+1} - x_{n+p}|
 
|x_{n} - x_{n+p+1}| < C \cdot \frac{\alpha^n}{1-\alpha} < \epsilon
 
n > N > \log_{1+r} {C\over (1-\alpha)\epsilon}
 \{x_n\}","['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'cauchy-sequences']"
88,Expressing Riemann sums as integrals,Expressing Riemann sums as integrals,,"$$L_2=\lim_{n→∞}\sum_{k=1}^n\frac{(k-\cos^2(k))^4}{n^5}.$$ My teacher said that when brackets at the numarator is expanded the limit of sum except $\dfrac{k^4}{n^5}$ equals $0$ , so the Riemann sum becomes $\lim\limits_{n→∞}\sum\limits_{k=1}^n\dfrac{k^4}{n^5}$ . I don't understand this point. Please explain this point. Sorry for my bad English.","My teacher said that when brackets at the numarator is expanded the limit of sum except equals , so the Riemann sum becomes . I don't understand this point. Please explain this point. Sorry for my bad English.",L_2=\lim_{n→∞}\sum_{k=1}^n\frac{(k-\cos^2(k))^4}{n^5}. \dfrac{k^4}{n^5} 0 \lim\limits_{n→∞}\sum\limits_{k=1}^n\dfrac{k^4}{n^5},"['limits', 'definite-integrals']"
89,Limit of the composistion of two functions when the limit of $f$ does not exist?,Limit of the composistion of two functions when the limit of  does not exist?,f,"The theorem about the limit of composition of two real functions $f$ and $g$ is proved here . But it is required that the two limits (of $f$ and $g$ ) both exist. I can't understand how to deal with the case in which the limit of $f$ does not exists. In particular I would like to know if the following is correct. Consider $f(x)$ and $g(x)$ (real functions). If I find out that $\lim_{x \to x_0} f(x)$ does not exists, can I conclude that $\lim_{x\to x_0 }g(f(x))$ does not exist? Under what conditions is this correct?","The theorem about the limit of composition of two real functions and is proved here . But it is required that the two limits (of and ) both exist. I can't understand how to deal with the case in which the limit of does not exists. In particular I would like to know if the following is correct. Consider and (real functions). If I find out that does not exists, can I conclude that does not exist? Under what conditions is this correct?",f g f g f f(x) g(x) \lim_{x \to x_0} f(x) \lim_{x\to x_0 }g(f(x)),"['calculus', 'limits', 'function-and-relation-composition']"
90,"Prove that if $a,b\in \mathbb{R}$, $a>0$, then $\lim \frac{n^b}{(1+a)^n}=0$ as $n\to\infty$","Prove that if , , then  as","a,b\in \mathbb{R} a>0 \lim \frac{n^b}{(1+a)^n}=0 n\to\infty","As the title says, i want to show that $$\lim_{n\to \infty} \frac{n^b}{(1+a)^n}=0$$ where $a,b\in \mathbb{R}$ with $a>0$ . I tried to bound the sequence $x_n =  \frac{n^b}{(1+a)^n}$ and use the sandwish theorem, but have no results. My problem is the exponent in the denominator, maybe it could be bounded with Bernoulli inequality ( $(1+a)^n\geq 1+na$ ) but then the problem is de numerator $n^b$ . I'm grateful for any hints for doing this problem, or some steps to clarify.","As the title says, i want to show that where with . I tried to bound the sequence and use the sandwish theorem, but have no results. My problem is the exponent in the denominator, maybe it could be bounded with Bernoulli inequality ( ) but then the problem is de numerator . I'm grateful for any hints for doing this problem, or some steps to clarify.","\lim_{n\to \infty} \frac{n^b}{(1+a)^n}=0 a,b\in \mathbb{R} a>0 x_n =  \frac{n^b}{(1+a)^n} (1+a)^n\geq 1+na n^b","['calculus', 'sequences-and-series', 'limits']"
91,Evaluation of $\lim_{x\to -\infty}(3/4)^x - (5/4)^x$ [closed],Evaluation of  [closed],\lim_{x\to -\infty}(3/4)^x - (5/4)^x,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have to evaluate the following limit and I want to know if the below algebraic manipulation is legitimate. $$\lim_{x\to -\infty}\left(\frac{3}{4}\right)^x - \left(\frac{5}{4}\right)^x$$ Can I change the limit to positive infinity, seeing as that would just be the reciprocal of the fractions as such? $$\lim_{x\to \infty}\left(\frac{4}{3}\right)^x - \left(\frac{4}{5}\right)^x$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have to evaluate the following limit and I want to know if the below algebraic manipulation is legitimate. Can I change the limit to positive infinity, seeing as that would just be the reciprocal of the fractions as such?",\lim_{x\to -\infty}\left(\frac{3}{4}\right)^x - \left(\frac{5}{4}\right)^x \lim_{x\to \infty}\left(\frac{4}{3}\right)^x - \left(\frac{4}{5}\right)^x,"['calculus', 'limits', 'limits-without-lhopital']"
92,"$\lim \frac{x}{\sin x} = +\infty$ or $-\infty $ as $ x \rightarrow (n\pi) , n \neq 0$ is a wrong statement.",or  as  is a wrong statement.,"\lim \frac{x}{\sin x} = +\infty -\infty   x \rightarrow (n\pi) , n \neq 0","$$\lim \frac{x}{\sin x} = +\infty \textrm{ or } -\infty $$ as $ x \rightarrow (n\pi) , n \neq 0$ is a wrong statement. My professor told me this and he told me that the correct statement to write is: $$\lim \frac{x}{\sin x} = +\infty \textrm{ or } -\infty $$ as $ x \rightarrow (n\pi)^+ , n \neq 0$ But I do not understand how this ""+"" above the $(n \pi)$ yields to $ +\infty$ or $-\infty $ , could anyone explain this for me please?","as is a wrong statement. My professor told me this and he told me that the correct statement to write is: as But I do not understand how this ""+"" above the yields to or , could anyone explain this for me please?","\lim \frac{x}{\sin x} = +\infty \textrm{ or } -\infty   x \rightarrow (n\pi) , n \neq 0 \lim \frac{x}{\sin x} = +\infty \textrm{ or } -\infty   x \rightarrow (n\pi)^+ , n \neq 0 (n \pi)  +\infty -\infty ","['calculus', 'real-analysis', 'limits', 'analysis']"
93,Showing a certain limit or an upper bound of it goes to zero,Showing a certain limit or an upper bound of it goes to zero,,"I have the following limit which I know is equal to zero (using Mathematica soft.), however, I can't show it analytically. $\lim_{L \rightarrow \infty} (\ln{L})^2\left[1-\left(1-e^{-\frac{1}{2}(\sqrt{L}-2)}\right)^L\right]$ I would appreciate any help. Thanks","I have the following limit which I know is equal to zero (using Mathematica soft.), however, I can't show it analytically. I would appreciate any help. Thanks",\lim_{L \rightarrow \infty} (\ln{L})^2\left[1-\left(1-e^{-\frac{1}{2}(\sqrt{L}-2)}\right)^L\right],"['calculus', 'limits']"
94,"Compute $\lim \limits_{n \to \infty}\left(\frac {\sqrt[n]a + \sqrt[n]b}2\right)^{n} ~~~ (a, b>0)$",Compute,"\lim \limits_{n \to \infty}\left(\frac {\sqrt[n]a + \sqrt[n]b}2\right)^{n} ~~~ (a, b>0)","$$\lim_{n \to \infty}\left(\frac {\sqrt[n]a + \sqrt[n]b}2\right)^{n}  ~~~ (a, b>0)$$ I extended its domain and applied L'Hopital's rule to get the answer $\sqrt{ab}$ . However, is it possible to avoid using L'Hopital's rule here? (I tried for a few times, but failed.) Or any hint maybe?","I extended its domain and applied L'Hopital's rule to get the answer . However, is it possible to avoid using L'Hopital's rule here? (I tried for a few times, but failed.) Or any hint maybe?","\lim_{n \to \infty}\left(\frac {\sqrt[n]a + \sqrt[n]b}2\right)^{n}  ~~~ (a, b>0) \sqrt{ab}","['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'limits-without-lhopital']"
95,how to prove that this sequence converges to $0$?,how to prove that this sequence converges to ?,0,"$0 \le x_0 \le \frac{1}{2}$ ,  and $x_{n+1}=x_n-\dfrac{4x_n^3}{n+1}$ When I take $x_0=\sqrt{\frac{1}{12}}$ , it converges very very slow. I can see it is monotonic decreasing but don't know how to find its limit.",",  and When I take , it converges very very slow. I can see it is monotonic decreasing but don't know how to find its limit.",0 \le x_0 \le \frac{1}{2} x_{n+1}=x_n-\dfrac{4x_n^3}{n+1} x_0=\sqrt{\frac{1}{12}},"['sequences-and-series', 'limits']"
96,The limit of a function involving the digits in different bases,The limit of a function involving the digits in different bases,,"If we look at the digits in some base $d$ , and divide the numbers made by the string of digits from high to low and low to high, the ratio appears to approach $(d-2)$ as $d$ gets larger. Base 2: 1/1 = 1/1 = 1 Base 3: 21/12 = 7/5 = 1.4 Base 4: 321/123 = 57/27 = 2.11111 Base 10: 987654321/123456789 = 8.000000073 Base 16: FEDCBA987654321/... = 13.99999953 How would you go about proving the limit? The best I can do for a general form is, $$\lim\limits_{d\rightarrow\infty}\frac{\sum\limits_{x=1}^{d-1}{xd^{x-1}}}{\sum\limits_{x=1}^{d-1}(d-x)d^{d-x-1}} - d \stackrel{?}{=} -2$$ But I have no idea how to proceed.","If we look at the digits in some base , and divide the numbers made by the string of digits from high to low and low to high, the ratio appears to approach as gets larger. Base 2: 1/1 = 1/1 = 1 Base 3: 21/12 = 7/5 = 1.4 Base 4: 321/123 = 57/27 = 2.11111 Base 10: 987654321/123456789 = 8.000000073 Base 16: FEDCBA987654321/... = 13.99999953 How would you go about proving the limit? The best I can do for a general form is, But I have no idea how to proceed.",d (d-2) d \lim\limits_{d\rightarrow\infty}\frac{\sum\limits_{x=1}^{d-1}{xd^{x-1}}}{\sum\limits_{x=1}^{d-1}(d-x)d^{d-x-1}} - d \stackrel{?}{=} -2,"['limits', 'limits-without-lhopital']"
97,Interchange of a limit and integral,Interchange of a limit and integral,,How would one show the following equality? Trying standard convergence theorems eg DCT (I can't find a choice of a dominating function which is also integrable) do not appear to be successful. $$\lim_{a_n \to 0^+} \int_\mathbb{R} e^{−a_n|x|}\frac {\sin(x)}x dx=\int_\mathbb{R} \frac {\sin(x)}x dx $$ Thanks,How would one show the following equality? Trying standard convergence theorems eg DCT (I can't find a choice of a dominating function which is also integrable) do not appear to be successful. $$\lim_{a_n \to 0^+} \int_\mathbb{R} e^{−a_n|x|}\frac {\sin(x)}x dx=\int_\mathbb{R} \frac {\sin(x)}x dx $$ Thanks,,"['integration', 'limits']"
98,Proof of convergence for reciprocal of convergent sequence,Proof of convergence for reciprocal of convergent sequence,,"On page - 62 of Bartle and Sherbert's Introduction to Real Analysis, it is proved that- If $Z=(z_n)$ is a sequence of nonzero numbers that converges to nonzero limit z, then the sequence $(1/z_n)$ of reciprocals converges to $(1/z)$. However, i found the given proof somewhat difficult to understand, and made my own attempt at what i hoped was a simpler one. The said proof runs as follows- PROOF: $(z_n)_n\to z\neq0\quad(n\to\infty)$ $\Rightarrow$ $\forall\epsilon\gt0:\exists N_1\in \mathbb{N}:\forall n\geq N_1:\vert z_n -z \vert\lt\epsilon$ Now, for $(1/z_n)$: $$\forall  n \in \mathbb{N}:\vert 1/z_n -1/z \vert = \vert{ z-z_n\over z_n.z}\vert = { (\vert z_n - z\vert) \over (\vert z_n \vert . \vert z \vert) } \lt {\epsilon \over \vert z_n \vert \vert z \vert } = {\delta \over \vert z_n \vert}  \text{, where }\delta ={\epsilon \over \vert z \vert } $$ $\Rightarrow \vert 1/z_n - 1/z \vert \lt \delta / ( \vert z_n \vert)$ , for all $n\in\mathbb{N}$ Now, $\vert z_n \vert \gt 0$ and $\vert z_n \vert \in  \mathbb {R}, \epsilon/(\vert z_n \vert) \gt 0$ $\Rightarrow \vert 1/z_n - 1/z \vert  \lt \epsilon $ for all $n\in\mathbb{N}$ such that $\epsilon \gt 0 $ Thus, $(1/z_n) \to (1/z)$ Q. E. D. However, my fellow- students found it 'quite incomprehendible and jumbled'. MY question is - Is my proof correct ? If not, why ? If yes, how do I make it clearer ? Edit- The earlier edit by zzussee had somewhat changed my intent regarding the last part of the proof. I had actually intended -  $\vert 1/z_n - 1/z \vert \to \upsilon $ , where $\upsilon := { \epsilon \over (\vert z_n \vert )} $, where $\upsilon $ should be position as both $\epsilon $ and $ z_n $ are positive. Of course, this does not invalidate the answer by @Jonas Lenz but it explains both his and @ Lucas' comments about $\epsilon $ in the last part of the proof.","On page - 62 of Bartle and Sherbert's Introduction to Real Analysis, it is proved that- If $Z=(z_n)$ is a sequence of nonzero numbers that converges to nonzero limit z, then the sequence $(1/z_n)$ of reciprocals converges to $(1/z)$. However, i found the given proof somewhat difficult to understand, and made my own attempt at what i hoped was a simpler one. The said proof runs as follows- PROOF: $(z_n)_n\to z\neq0\quad(n\to\infty)$ $\Rightarrow$ $\forall\epsilon\gt0:\exists N_1\in \mathbb{N}:\forall n\geq N_1:\vert z_n -z \vert\lt\epsilon$ Now, for $(1/z_n)$: $$\forall  n \in \mathbb{N}:\vert 1/z_n -1/z \vert = \vert{ z-z_n\over z_n.z}\vert = { (\vert z_n - z\vert) \over (\vert z_n \vert . \vert z \vert) } \lt {\epsilon \over \vert z_n \vert \vert z \vert } = {\delta \over \vert z_n \vert}  \text{, where }\delta ={\epsilon \over \vert z \vert } $$ $\Rightarrow \vert 1/z_n - 1/z \vert \lt \delta / ( \vert z_n \vert)$ , for all $n\in\mathbb{N}$ Now, $\vert z_n \vert \gt 0$ and $\vert z_n \vert \in  \mathbb {R}, \epsilon/(\vert z_n \vert) \gt 0$ $\Rightarrow \vert 1/z_n - 1/z \vert  \lt \epsilon $ for all $n\in\mathbb{N}$ such that $\epsilon \gt 0 $ Thus, $(1/z_n) \to (1/z)$ Q. E. D. However, my fellow- students found it 'quite incomprehendible and jumbled'. MY question is - Is my proof correct ? If not, why ? If yes, how do I make it clearer ? Edit- The earlier edit by zzussee had somewhat changed my intent regarding the last part of the proof. I had actually intended -  $\vert 1/z_n - 1/z \vert \to \upsilon $ , where $\upsilon := { \epsilon \over (\vert z_n \vert )} $, where $\upsilon $ should be position as both $\epsilon $ and $ z_n $ are positive. Of course, this does not invalidate the answer by @Jonas Lenz but it explains both his and @ Lucas' comments about $\epsilon $ in the last part of the proof.",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
99,Does there exist such an interval?,Does there exist such an interval?,,"If $f: \mathbb{[a,b]} \to \mathbb{R}$ and $f$ is twice differentiable at a point $c$.     Does there exist an interval $[p,q]$ in $[a,b]$ where $f$ is differentiable I think here, $f'$ is continuous at $c$.Then by the definition of continuity there must exist such an interval","If $f: \mathbb{[a,b]} \to \mathbb{R}$ and $f$ is twice differentiable at a point $c$.     Does there exist an interval $[p,q]$ in $[a,b]$ where $f$ is differentiable I think here, $f'$ is continuous at $c$.Then by the definition of continuity there must exist such an interval",,"['real-analysis', 'limits', 'continuity']"
