,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\alpha \ln\alpha$,Show that,\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\alpha \ln\alpha,"Show that the improper integral $\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\alpha \ln\alpha$, for $\alpha\in(0,1)$. This is an integral of Riemann . My work: The set of discontinuities of the integral is $$D=\left\{\frac1k:k\in\Bbb N\right\}\cup\left\{\frac{\alpha}{k}:k\in\Bbb N\right\}$$ And when we have that $x>\alpha$ the integral can be simplified to $$\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\int_0^\alpha \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx-\alpha\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx$$ I dont know how to continue from here, it is not clear how to handle the partition $D$ to simplify the integral. What I did here is just see what is the value of $\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx$ to see if I get some clue. If there is no weird mistake somewhere: $$\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx=\int_\alpha^{\frac1{\left\lfloor 1/\alpha\right\rfloor}}\frac{\mathbf 1_{\Bbb N}(1/\alpha)\mathrm dx}{\lfloor 1/\alpha\rfloor}+\sum_{k=1}^{\lfloor1/\alpha\rfloor}\int_{\frac1{k+1}}^{\frac1k}\frac{\mathrm dx}{k}=\\=\mathbf 1_{\Bbb N}(1/\alpha)\frac{1-\alpha\lfloor 1/\alpha\rfloor}{\lfloor 1/\alpha\rfloor^2}+\sum_{k=1}^{\lfloor1/\alpha\rfloor}\frac1{k^2(k+1)}$$ what is not useful at all. So I get stuck with this problem, can you help me to show this identity (not going deeper than a Riemann integral background)? Thank you in advance.","Show that the improper integral $\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\alpha \ln\alpha$, for $\alpha\in(0,1)$. This is an integral of Riemann . My work: The set of discontinuities of the integral is $$D=\left\{\frac1k:k\in\Bbb N\right\}\cup\left\{\frac{\alpha}{k}:k\in\Bbb N\right\}$$ And when we have that $x>\alpha$ the integral can be simplified to $$\int_0^1 \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx=\int_0^\alpha \left(\left\lfloor\frac{\alpha}{x}\right\rfloor-\alpha\left\lfloor\frac{1}{x}\right\rfloor\right)\mathrm dx-\alpha\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx$$ I dont know how to continue from here, it is not clear how to handle the partition $D$ to simplify the integral. What I did here is just see what is the value of $\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx$ to see if I get some clue. If there is no weird mistake somewhere: $$\int_{\alpha}^1 \left\lfloor\frac{1}{x}\right\rfloor\mathrm dx=\int_\alpha^{\frac1{\left\lfloor 1/\alpha\right\rfloor}}\frac{\mathbf 1_{\Bbb N}(1/\alpha)\mathrm dx}{\lfloor 1/\alpha\rfloor}+\sum_{k=1}^{\lfloor1/\alpha\rfloor}\int_{\frac1{k+1}}^{\frac1k}\frac{\mathrm dx}{k}=\\=\mathbf 1_{\Bbb N}(1/\alpha)\frac{1-\alpha\lfloor 1/\alpha\rfloor}{\lfloor 1/\alpha\rfloor^2}+\sum_{k=1}^{\lfloor1/\alpha\rfloor}\frac1{k^2(k+1)}$$ what is not useful at all. So I get stuck with this problem, can you help me to show this identity (not going deeper than a Riemann integral background)? Thank you in advance.",,"['real-analysis', 'integration', 'improper-integrals', 'riemann-integration']"
1,Can a function be differentiable at the end points of its (closed interval) domain?,Can a function be differentiable at the end points of its (closed interval) domain?,,"Assume $f$ has a domain of $[a,b]$. Is it possible that $f$ is differentiable on the closed interval $[a,b]$, or must the maximal domain for $f'$ be $(a,b)$?","Assume $f$ has a domain of $[a,b]$. Is it possible that $f$ is differentiable on the closed interval $[a,b]$, or must the maximal domain for $f'$ be $(a,b)$?",,"['calculus', 'real-analysis', 'derivatives']"
2,"Proving that $f'(x)\le f(x), \forall x\in \mathbb{R}$",Proving that,"f'(x)\le f(x), \forall x\in \mathbb{R}","Let $f:\mathbb{R} \to \mathbb{R}$ be a twice differentiable function such that $0 \le f''(x) \le f(x)$ $f'(x)\ge 0, \forall x\in \mathbb{R}$ . Thus, prove that $f'(x)\le f(x), \forall x\in \mathbb{R}$ . I couldn't make much progress, but I observed that $f$ is increasing and convex while $f'$ is increasing from the hypothesis. Then I tried to evaluate the derivative of $h:\mathbb{R} \to \mathbb{R}$ with $$ h(x)=f(x)-f'(x)$$ and I got that $$h'(x)=f'(x)-f''(x), \forall x\in \mathbb{R}$$ Now, I would be done if I could prove that $h'(x)\ge 0$ for all $x\in \mathbb{R}$ . But I'm not sure yet if this approach is really that straightforward. I also tried to use that $f$ is convex, but to no avail. EDIT: Maybe that we should somehow use the fact that a differentiable function $g:\mathbb{R}\to\mathbb{R}$ is convex if and only if $g(x)\ge g(y)+g'(y)(x-y),\  \forall x,y\in \mathbb{R}$","Let be a twice differentiable function such that . Thus, prove that . I couldn't make much progress, but I observed that is increasing and convex while is increasing from the hypothesis. Then I tried to evaluate the derivative of with and I got that Now, I would be done if I could prove that for all . But I'm not sure yet if this approach is really that straightforward. I also tried to use that is convex, but to no avail. EDIT: Maybe that we should somehow use the fact that a differentiable function is convex if and only if","f:\mathbb{R} \to \mathbb{R} 0 \le f''(x) \le f(x) f'(x)\ge 0, \forall x\in \mathbb{R} f'(x)\le f(x), \forall x\in \mathbb{R} f f' h:\mathbb{R} \to \mathbb{R}  h(x)=f(x)-f'(x) h'(x)=f'(x)-f''(x), \forall x\in \mathbb{R} h'(x)\ge 0 x\in \mathbb{R} f g:\mathbb{R}\to\mathbb{R} g(x)\ge g(y)+g'(y)(x-y),\  \forall x,y\in \mathbb{R}","['real-analysis', 'calculus', 'contest-math']"
3,"Functions $f$ satisfying $ f\circ f(x)=2f(x)-x,\forall x\in\mathbb{R}$.",Functions  satisfying .,"f  f\circ f(x)=2f(x)-x,\forall x\in\mathbb{R}","How to prove that the continuous functions $f$ on $\mathbb{R}$  satisfying  $$f\circ f(x)=2f(x)-x,\forall x\in\mathbb{R},$$ are given by  $$f(x)=x+a,a\in\mathbb{R}.$$ Any hints are welcome. Thanks.","How to prove that the continuous functions $f$ on $\mathbb{R}$  satisfying  $$f\circ f(x)=2f(x)-x,\forall x\in\mathbb{R},$$ are given by  $$f(x)=x+a,a\in\mathbb{R}.$$ Any hints are welcome. Thanks.",,['calculus']
4,Does recursively replacing $\frac1n$ by $\frac1n(\frac12+\dots+\frac1{n+1})$ really converge to $\frac1e$?,Does recursively replacing  by  really converge to ?,\frac1n \frac1n(\frac12+\dots+\frac1{n+1}) \frac1e,"I was thinking of a problem and have an answer through computer programming, but am unable to prove it mathematically. Start with the following:  $$\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg)\approx 0.41666$$  Replace every unit fraction being summed in the parentheses (in this case, the inner 1/2 and 1/3) with  $$\frac{1}{n}\bigg(\frac{1}{2} + \frac{1}{3} + ... + \frac{1}{n+1}\bigg)$$ where $n$ is the denominator of the unit fraction. If we apply this process once, we get $$\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg)+\frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+\frac{1}{4}\bigg)\bigg)\approx 0.3888$$ and if we apply it again, we get $$\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg) + \frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}\bigg)\bigg)+\frac{1}{3}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg) + \frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}\bigg)+\frac{1}{4}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}+ \frac{1}{5}\bigg)\bigg)\bigg)\approx 0.37754$$ As we repeat this process, the result seems to approach $e^{-1}$. If anyone could prove this or direct me to a proof of this, that would be wonderful!","I was thinking of a problem and have an answer through computer programming, but am unable to prove it mathematically. Start with the following:  $$\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg)\approx 0.41666$$  Replace every unit fraction being summed in the parentheses (in this case, the inner 1/2 and 1/3) with  $$\frac{1}{n}\bigg(\frac{1}{2} + \frac{1}{3} + ... + \frac{1}{n+1}\bigg)$$ where $n$ is the denominator of the unit fraction. If we apply this process once, we get $$\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg)+\frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+\frac{1}{4}\bigg)\bigg)\approx 0.3888$$ and if we apply it again, we get $$\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg) + \frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}\bigg)\bigg)+\frac{1}{3}\bigg(\frac{1}{2}\bigg(\frac{1}{2} + \frac{1}{3}\bigg) + \frac{1}{3}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}\bigg)+\frac{1}{4}\bigg(\frac{1}{2} + \frac{1}{3}+ \frac{1}{4}+ \frac{1}{5}\bigg)\bigg)\bigg)\approx 0.37754$$ As we repeat this process, the result seems to approach $e^{-1}$. If anyone could prove this or direct me to a proof of this, that would be wonderful!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
5,Limit of $\lim_{t \to \infty} \frac{ \int_0^\infty \cos(x t) e^{-x^k}dx}{\int_0^\infty \cos(x t) e^{-x^p}dx}$,Limit of,\lim_{t \to \infty} \frac{ \int_0^\infty \cos(x t) e^{-x^k}dx}{\int_0^\infty \cos(x t) e^{-x^p}dx},"Let  \begin{align} f(t,k,p)= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx}, \end{align} My question: How to find the following limit of the function $f(t,k,p)$ \begin{align} \lim_{t \to \infty} f(t,k,p), \end{align}  for any $p>0$ and $k>0$. What is known Some facts about the function Note that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is a fourierier transform of $e^{-{|x|^k}}$. For $0<k \le 2$ we have that $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is non-negative function and has no zeros. See this question . For $k>2$ we know that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ has countable many zeros. See this questions . A related question was asked here . Because for the case of $p>2$ the denominator has countable many zeros I am not sure if $\lim_{t \to \infty} f(t,k,p)$ even exists. It would be nice to show if it exists or not. Other trivial case include  $k=1,p=2$ and $k=2,p=1$ since inverse fourier transforms of $e^{-|x|}$ and $e^{-|x|^2}$ are know in closed form. Clearly, the case of $k=p$ is trivial. So, we would like to analyze $k>p$ and $p<k$. Numerical Simulations: Numerical simulations seem to suggest that \begin{align} \lim_{t \to \infty} f(t,k,p)&=-\infty, \ k>p, \\ \lim_{t \to \infty} f(t,k,p)&=\infty, \ k<p. \end{align} Method of the Steepest Descent: (See the answer in progress via this method by @tired) It has been suggested by @tired that the method of steepest descent might be a possible approach for solving the limit.  That is since \begin{align} \int_0^\infty \cos(x  t) e^{-x^k}dx&= \mathsf{Re}  \int_0^\infty e^{it} e^{-x^k}dx, \\ &=  t^{\frac{1}{k-1}} \mathsf{Re}  \int_0^\infty  e^{ t^{\frac{k}{k-1}} (-u^k+iu)}du, \\ \end{align}  where in the last step we have used substitution $x=u t^{\frac{1}{k-1}}$. Note that this now take the form of $\int_I  e^{ A S(u)}du$ which can be handle by method of steepest desend if $S(u)$ satisfies: $S(u)$ is holomorphic $\mathsf{Re}(S(u))$  has a single maximum: $\max_{ u \in I } \mathsf{Re}(S(u))=\mathsf{Re}(S(u_0))$ for execly one $u_0\in I$. $u_0$ is non-degenare saddle point. That is $S''(u_0)\neq 0$. Adopting this to our case we have that the maximum  of \begin{align} \max_{ u \in I } \mathsf{Re}(S(u))=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k+iu)=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k)= 0, \end{align} where $u_0=0$. Note also that $S''(u)= k (k-1) u^{k-2}$ and therefore $S''(0)=0$ so the maximum is a degenaret saddle point. This violates the third conditon. I really hoped that this method was going to work. Am I making any mistakes in the above? Thank you for any help or suggestions you might have. This the second bounty posted on this question. Things that did not work: Approach with expansion of $e^x$ \begin{align} f(t,k,p)&= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx}\\ &=\frac{ \int_0^\infty \cos(u) e^{-(u/t)^k}du}{\int_0^\infty \cos(u) e^{-(u/t)^p}du}\\ &=\frac{ \int_0^\infty \cos(u) (1- (u/t)^k+O((u/t)^{2k})) du}{\int_0^\infty \cos(u)  (1- (u/t)^p+O((u/t)^{2p}))du}, \end{align} but the integrals, do not converge. Don't think this approach works. Integration by parts approach: Note that  by tntegration by parts \begin{align} \int_0^\infty \cos(x  t) e^{-x^k}dx=  \frac{k}{t^{k+1}} \int_0^\infty \sin(u)  u^{k-1} e^{-(u/t)^k} du, \end{align} using this we have that \begin{align} f(t,k,p)=\frac{k}{p} t^{k-p} \frac{ \int_0^\infty \sin(u) u^{k-1} e^{-(u/t)^k}du}{\int_0^\infty \sin(u) u^{p-1} e^{-(u/t)^p}du}. \end{align} The question is how to proceed next? How do we know how the ratio of the two integrals behaves?","Let  \begin{align} f(t,k,p)= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx}, \end{align} My question: How to find the following limit of the function $f(t,k,p)$ \begin{align} \lim_{t \to \infty} f(t,k,p), \end{align}  for any $p>0$ and $k>0$. What is known Some facts about the function Note that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is a fourierier transform of $e^{-{|x|^k}}$. For $0<k \le 2$ we have that $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is non-negative function and has no zeros. See this question . For $k>2$ we know that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ has countable many zeros. See this questions . A related question was asked here . Because for the case of $p>2$ the denominator has countable many zeros I am not sure if $\lim_{t \to \infty} f(t,k,p)$ even exists. It would be nice to show if it exists or not. Other trivial case include  $k=1,p=2$ and $k=2,p=1$ since inverse fourier transforms of $e^{-|x|}$ and $e^{-|x|^2}$ are know in closed form. Clearly, the case of $k=p$ is trivial. So, we would like to analyze $k>p$ and $p<k$. Numerical Simulations: Numerical simulations seem to suggest that \begin{align} \lim_{t \to \infty} f(t,k,p)&=-\infty, \ k>p, \\ \lim_{t \to \infty} f(t,k,p)&=\infty, \ k<p. \end{align} Method of the Steepest Descent: (See the answer in progress via this method by @tired) It has been suggested by @tired that the method of steepest descent might be a possible approach for solving the limit.  That is since \begin{align} \int_0^\infty \cos(x  t) e^{-x^k}dx&= \mathsf{Re}  \int_0^\infty e^{it} e^{-x^k}dx, \\ &=  t^{\frac{1}{k-1}} \mathsf{Re}  \int_0^\infty  e^{ t^{\frac{k}{k-1}} (-u^k+iu)}du, \\ \end{align}  where in the last step we have used substitution $x=u t^{\frac{1}{k-1}}$. Note that this now take the form of $\int_I  e^{ A S(u)}du$ which can be handle by method of steepest desend if $S(u)$ satisfies: $S(u)$ is holomorphic $\mathsf{Re}(S(u))$  has a single maximum: $\max_{ u \in I } \mathsf{Re}(S(u))=\mathsf{Re}(S(u_0))$ for execly one $u_0\in I$. $u_0$ is non-degenare saddle point. That is $S''(u_0)\neq 0$. Adopting this to our case we have that the maximum  of \begin{align} \max_{ u \in I } \mathsf{Re}(S(u))=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k+iu)=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k)= 0, \end{align} where $u_0=0$. Note also that $S''(u)= k (k-1) u^{k-2}$ and therefore $S''(0)=0$ so the maximum is a degenaret saddle point. This violates the third conditon. I really hoped that this method was going to work. Am I making any mistakes in the above? Thank you for any help or suggestions you might have. This the second bounty posted on this question. Things that did not work: Approach with expansion of $e^x$ \begin{align} f(t,k,p)&= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx}\\ &=\frac{ \int_0^\infty \cos(u) e^{-(u/t)^k}du}{\int_0^\infty \cos(u) e^{-(u/t)^p}du}\\ &=\frac{ \int_0^\infty \cos(u) (1- (u/t)^k+O((u/t)^{2k})) du}{\int_0^\infty \cos(u)  (1- (u/t)^p+O((u/t)^{2p}))du}, \end{align} but the integrals, do not converge. Don't think this approach works. Integration by parts approach: Note that  by tntegration by parts \begin{align} \int_0^\infty \cos(x  t) e^{-x^k}dx=  \frac{k}{t^{k+1}} \int_0^\infty \sin(u)  u^{k-1} e^{-(u/t)^k} du, \end{align} using this we have that \begin{align} f(t,k,p)=\frac{k}{p} t^{k-p} \frac{ \int_0^\infty \sin(u) u^{k-1} e^{-(u/t)^k}du}{\int_0^\infty \sin(u) u^{p-1} e^{-(u/t)^p}du}. \end{align} The question is how to proceed next? How do we know how the ratio of the two integrals behaves?",,"['real-analysis', 'integration', 'limits', 'fourier-analysis', 'laplace-method']"
6,How can we describe the graph of $x^x$ for negative values?,How can we describe the graph of  for negative values?,x^x,"We usually only see the graph $y=x^x$ for $x>0$, because $x^x$ is a complex number for most negative values of $x$.  Yet here is a full graph of $y=x^x$ on the real line: This graph may seem like it's not even a function, failing the vertical line test, but what's actually going on is that the graph contains infinitely many holes.  It's true that $x^x$ is not a real number for almost all values of $x<0$, but it is real in the rare situation when $x$ is a rational number which has an odd denominator when written in simplest form.  In that case, $x^x$ is a positive real number when $x$ can be written as an even number divided by an odd number, and a negative real number when $x$ can be written as an odd number divided by an odd number.  So just those rational points are being graphed, but since the rational numbers with odd denominators are dense in the real numbers it looks like we have continuous curves. My question is, what are the continuous curves that seem to be there?  Any continuous curve defined on a dense subset of the reals can be uniquely extended to a continuous function on all the reals.  So what continuous function passes through all the points $(x,x^x)$ where $x<0$ and $x$ can be written as an even number divided by an odd number?  (I'd ask the analogous question about the second curve, but it seems to be a mirror image of the first.) Any help would be greatly appreciated. Thank You in Advance.","We usually only see the graph $y=x^x$ for $x>0$, because $x^x$ is a complex number for most negative values of $x$.  Yet here is a full graph of $y=x^x$ on the real line: This graph may seem like it's not even a function, failing the vertical line test, but what's actually going on is that the graph contains infinitely many holes.  It's true that $x^x$ is not a real number for almost all values of $x<0$, but it is real in the rare situation when $x$ is a rational number which has an odd denominator when written in simplest form.  In that case, $x^x$ is a positive real number when $x$ can be written as an even number divided by an odd number, and a negative real number when $x$ can be written as an odd number divided by an odd number.  So just those rational points are being graphed, but since the rational numbers with odd denominators are dense in the real numbers it looks like we have continuous curves. My question is, what are the continuous curves that seem to be there?  Any continuous curve defined on a dense subset of the reals can be uniquely extended to a continuous function on all the reals.  So what continuous function passes through all the points $(x,x^x)$ where $x<0$ and $x$ can be written as an even number divided by an odd number?  (I'd ask the analogous question about the second curve, but it seems to be a mirror image of the first.) Any help would be greatly appreciated. Thank You in Advance.",,"['real-analysis', 'complex-analysis', 'graphing-functions', 'exponential-function']"
7,"Continuous $f\colon [0,1]\to \mathbb{R}$ all of whose nonempty fibers are countably infinite?",Continuous  all of whose nonempty fibers are countably infinite?,"f\colon [0,1]\to \mathbb{R}","I have been told it is possible to construct a continuous function $f\colon [0,1]\to \mathbb{R}$ such that $f^{-1}(x)$ is either empty or has cardinality $\aleph_0$ for every $x\in \mathbb{R}$. I've thought about this for a while but can't seem to cook up an example. Does anyone know such a construction?","I have been told it is possible to construct a continuous function $f\colon [0,1]\to \mathbb{R}$ such that $f^{-1}(x)$ is either empty or has cardinality $\aleph_0$ for every $x\in \mathbb{R}$. I've thought about this for a while but can't seem to cook up an example. Does anyone know such a construction?",,"['real-analysis', 'general-topology']"
8,Differentiation under the integral sign for Lebesgue integrable derivative,Differentiation under the integral sign for Lebesgue integrable derivative,,"The problem is the following: Let $a,b,c,d \in \mathbb R$ be given such that $a<b$ and $c<d$ . Suppose $f : [a,b] \times [c,d] \to \mathbb R$ is a function such that $\partial_1 f: [a,b] \times [c,d] \to \mathbb R$ exists and is (Lebesgue-)integrable. Show that for $t\in [a,b]$ we have $$\frac{\mathrm d}{\mathrm dt}\int_c^d f(t,x) \, \mathrm dx = \int_c^d \partial_1 f(t,x)\, \mathrm dx$$ I have tried the following: By the fundamental theorem of calculus, we have $$f(t',x) - f(t,x) = \int_{t}^{t'} \partial_1f(s,x)\, \mathrm ds$$ It follows that $$ \begin{align} \int_c^d f(t',x)-f(t,x) \, \mathrm dx &= \int_c^d \int_{t}^{t'} \partial_1f(s,x)\, \mathrm ds \, \mathrm dx \\ &= \int_{t}^{t'} \int_c^d \partial_1f(s,x)\, \mathrm dx \, \mathrm ds \end{align} $$ so that for almost all $t$ , we have $$ \begin{align} \frac{\mathrm d}{\mathrm dt}\int_c^d f(t,x) \, \mathrm dx  &= \lim_{t'\to t} \frac{1}{t'-t}\int_c^d f(t',x)-f(t,x) \, \mathrm dx \\ &= \lim_{t'\to t} \frac{1}{t'-t}\int_{t}^{t'} \int_c^d \partial_1f(s,x)\, \mathrm dx \, \mathrm ds \\ &= \int_c^d \partial_1f(t,x)\, \mathrm dx \end{align} $$ since $t \mapsto \int_c^d \partial_1f(t,x)\, \mathrm dx$ is an $L^1$ function (hence almost every point $t$ is a Lebesgue-point). Unfortunately equality for almost every $t$ is not good enough in this case. I have been thinking about this for quite a while now, but I cannot figure out the reason for why every point $t\in [a,b]$ has to be a Lebesgue point of $$t\mapsto \int_c^d \partial_1f(t,x)\, \mathrm dx$$ I have also been thinking about possible counterexamples, but couldn't really come up with one. (At least not if one defines $\frac{\mathrm d}{\mathrm dt} \infty = 0$ for the constant function $\infty$ ...) Some help would be appreciated, thanks!","The problem is the following: Let be given such that and . Suppose is a function such that exists and is (Lebesgue-)integrable. Show that for we have I have tried the following: By the fundamental theorem of calculus, we have It follows that so that for almost all , we have since is an function (hence almost every point is a Lebesgue-point). Unfortunately equality for almost every is not good enough in this case. I have been thinking about this for quite a while now, but I cannot figure out the reason for why every point has to be a Lebesgue point of I have also been thinking about possible counterexamples, but couldn't really come up with one. (At least not if one defines for the constant function ...) Some help would be appreciated, thanks!","a,b,c,d \in \mathbb R a<b c<d f : [a,b] \times [c,d] \to \mathbb R \partial_1 f: [a,b] \times [c,d] \to \mathbb R t\in [a,b] \frac{\mathrm d}{\mathrm dt}\int_c^d f(t,x) \, \mathrm dx = \int_c^d \partial_1 f(t,x)\, \mathrm dx f(t',x) - f(t,x) = \int_{t}^{t'} \partial_1f(s,x)\, \mathrm ds 
\begin{align}
\int_c^d f(t',x)-f(t,x) \, \mathrm dx &= \int_c^d \int_{t}^{t'} \partial_1f(s,x)\, \mathrm ds \, \mathrm dx \\
&= \int_{t}^{t'} \int_c^d \partial_1f(s,x)\, \mathrm dx \, \mathrm ds
\end{align}
 t 
\begin{align}
\frac{\mathrm d}{\mathrm dt}\int_c^d f(t,x) \, \mathrm dx 
&= \lim_{t'\to t} \frac{1}{t'-t}\int_c^d f(t',x)-f(t,x) \, \mathrm dx \\
&= \lim_{t'\to t} \frac{1}{t'-t}\int_{t}^{t'} \int_c^d \partial_1f(s,x)\, \mathrm dx \, \mathrm ds \\
&= \int_c^d \partial_1f(t,x)\, \mathrm dx
\end{align}
 t \mapsto \int_c^d \partial_1f(t,x)\, \mathrm dx L^1 t t t\in [a,b] t\mapsto \int_c^d \partial_1f(t,x)\, \mathrm dx \frac{\mathrm d}{\mathrm dt} \infty = 0 \infty","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'leibniz-integral-rule']"
9,$L^{\infty}$ is a Banach Space,is a Banach Space,L^{\infty},"Show that $L^{\infty}$ is a Banach Space with respect to the norm $||.||_{\infty}$ where $||f||_{\infty}=\inf\{ a\ge 0: \mu\left(\{x: |f(x)| \gt a\}\right)=0\}$ I am going to prove the following lemmas and then use them in the proof. Lemma-1: $||.||_{\infty}$ is a norm on $L^{\infty}$ Proof: Suppose that $||f||_{\infty}=0$.  Let $A=\{x \in X : f(x) \ne 0\} $ and $A_n=\{x \in X: |f(x)| \gt \frac{1}{n}\}$ . Then $A=\cup_{n}A_n$. Then there exists $'a'$ such that $\mu\left(\{x: |f(x)| \gt a\}\right)=0$ and $a \lt \frac{1}{n}$ (by the definition of $||f||_{\infty}$). Thus $$\mu\left(\{x: |f(x)| \gt \frac{1}{n}\}\right) \le \mu\left(\{x: |f(x)| \gt a\}\right)=0$$ $\implies \mu(A_n)=0$ and hence $\mu(A)=0$. Thus $f =0 ,\mu$-a.e. Now $$|f(x)+g(x)| \le |f(x)| +|g(x)| \le ||f||_{\infty}|+||g||_{\infty} ,\mu-\text{a.e}$$ Thus $||f+g||_{\infty} \le ||f||_{\infty}+||g||_{\infty}$ Now $$||\lambda f||_{\infty}=\inf\{a \ge 0: \mu\left(\{x: |\lambda f(x)| \gt a\}\right)=0\}$$ $$=\inf\{|\lambda|a \ge 0:\mu\left(\{x: |\lambda||f(x)| \gt |\lambda|a\}\right)=0\}$$ $$=|\lambda|\inf\{a \ge 0:\mu\left(\{x: |f(x)| \gt a\}\right)=0\}=|\lambda|||f||_{\infty}$$ Lemma-2: $||f_n-f||_{\infty} \to 0$ iff there exists $E \in \mathcal{M}$ such that $\mu(E^c)=0$ and $f_n \to f$ uniformly on $E$. Proof: $(\implies)$ For every $k \in \mathbb{N}$, there exists $n_0(k) \in \mathbb{N}$ such that for all $n \ge n_0(k)$, $||f_n-f||_{\infty} \lt \dfrac{1}{k}$. Then for each $n \ge n_0(k)$ there exists $E_n^{k}$ such that $\mu(E_n^k)=0$ and $|f_n(x)-f(x)| \lt \dfrac{1}{k}$ for all $x \in (E_n^k)^{c}$.  Let $$E^k=\cup_{n \ge n_0(k)} E_n^k$$. Then $\mu(E^k)=0$ and for all $x \not \in E^k, |f_n(x)-f(x)| \lt \frac{1}{k}, \forall n \ge n_0(k)$. Now Let $E=\cup_{k=1}^{\infty} E^k$. Then $\mu(E)=0$. Let $\epsilon \gt 0$. Then there exists a $k_0 \in \mathbb{N}$ such that $\frac{1}{k} \lt \epsilon$ for all $k \ge k_0$. Let $x \in E^c$. In Particular $x \not \in E^{k_0}$ . Then for all $n \ge n_0(k_0), |f_n(x)-f(x)| \lt \frac{1}{k_0} \lt \epsilon$ ($\impliedby$) Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0, |f_n(x)-f(x)| \lt \epsilon, x\in E$. Then for all $n \ge n_0$, $$\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right) \subset E^c$$ $$\implies \mu\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right)=0  $$ which in turn gives us that for all $n \ge n_0$, we have $$||f_n-f||_{\infty} \lt \epsilon$$. (Proof that $L^{\infty}$ is a Banach Space) : Let $\{f_n\}_{n \in \mathbb{N}} \in L^{\infty}$ be  a cauchy sequence. Then for $\epsilon \gt 0$, there exists a $n_0(\epsilon) \in \mathbb{N}$ such that for all $n,m \ge n_0(\epsilon)$, we have $||f_n-f_m||_{\infty} \lt \dfrac{\epsilon}{2}$. Then there exists $E^{\epsilon} \in \mathcal{M}$ such that $\mu(E^{\epsilon})=0$ and for $x \in (E^{\epsilon})^{c} ,|f_n(x)-f_m(x)| \lt \dfrac{\epsilon}{2}$. Let $$E= \cup_n E^{\frac{1}{n}}.$$ Then $\mu(E)=0$ and for all $x \in E^c, \{f_n(x)\}$ is a  cauchy sequence. Let $f(x)=\lim_n f_n(x)$ for $x \in E^c$. Let $\epsilon \gt 0$. Then there exists $k_0 \in \mathbb{N}$ such that for all $k \ge k_0,\frac{1}{k} \lt \frac{\epsilon}{2}$. Then for all $n, m \ge n_0(k_0)$, we have $$|f_n(x)-f_m(x)| \lt \frac{1}{k} \lt \frac{\epsilon}{2}$$ Letting $m \to \infty$ we have $$|f_n(x)-f(x)| \lt \epsilon, x\in E^c.$$ By Lemma-2, we have $||f_n-f||_{\infty} \to 0$. From here by use of triangle inequality, we see that $ f\in L^{\infty}$. Is this proof alright? Thanks for the help!!","Show that $L^{\infty}$ is a Banach Space with respect to the norm $||.||_{\infty}$ where $||f||_{\infty}=\inf\{ a\ge 0: \mu\left(\{x: |f(x)| \gt a\}\right)=0\}$ I am going to prove the following lemmas and then use them in the proof. Lemma-1: $||.||_{\infty}$ is a norm on $L^{\infty}$ Proof: Suppose that $||f||_{\infty}=0$.  Let $A=\{x \in X : f(x) \ne 0\} $ and $A_n=\{x \in X: |f(x)| \gt \frac{1}{n}\}$ . Then $A=\cup_{n}A_n$. Then there exists $'a'$ such that $\mu\left(\{x: |f(x)| \gt a\}\right)=0$ and $a \lt \frac{1}{n}$ (by the definition of $||f||_{\infty}$). Thus $$\mu\left(\{x: |f(x)| \gt \frac{1}{n}\}\right) \le \mu\left(\{x: |f(x)| \gt a\}\right)=0$$ $\implies \mu(A_n)=0$ and hence $\mu(A)=0$. Thus $f =0 ,\mu$-a.e. Now $$|f(x)+g(x)| \le |f(x)| +|g(x)| \le ||f||_{\infty}|+||g||_{\infty} ,\mu-\text{a.e}$$ Thus $||f+g||_{\infty} \le ||f||_{\infty}+||g||_{\infty}$ Now $$||\lambda f||_{\infty}=\inf\{a \ge 0: \mu\left(\{x: |\lambda f(x)| \gt a\}\right)=0\}$$ $$=\inf\{|\lambda|a \ge 0:\mu\left(\{x: |\lambda||f(x)| \gt |\lambda|a\}\right)=0\}$$ $$=|\lambda|\inf\{a \ge 0:\mu\left(\{x: |f(x)| \gt a\}\right)=0\}=|\lambda|||f||_{\infty}$$ Lemma-2: $||f_n-f||_{\infty} \to 0$ iff there exists $E \in \mathcal{M}$ such that $\mu(E^c)=0$ and $f_n \to f$ uniformly on $E$. Proof: $(\implies)$ For every $k \in \mathbb{N}$, there exists $n_0(k) \in \mathbb{N}$ such that for all $n \ge n_0(k)$, $||f_n-f||_{\infty} \lt \dfrac{1}{k}$. Then for each $n \ge n_0(k)$ there exists $E_n^{k}$ such that $\mu(E_n^k)=0$ and $|f_n(x)-f(x)| \lt \dfrac{1}{k}$ for all $x \in (E_n^k)^{c}$.  Let $$E^k=\cup_{n \ge n_0(k)} E_n^k$$. Then $\mu(E^k)=0$ and for all $x \not \in E^k, |f_n(x)-f(x)| \lt \frac{1}{k}, \forall n \ge n_0(k)$. Now Let $E=\cup_{k=1}^{\infty} E^k$. Then $\mu(E)=0$. Let $\epsilon \gt 0$. Then there exists a $k_0 \in \mathbb{N}$ such that $\frac{1}{k} \lt \epsilon$ for all $k \ge k_0$. Let $x \in E^c$. In Particular $x \not \in E^{k_0}$ . Then for all $n \ge n_0(k_0), |f_n(x)-f(x)| \lt \frac{1}{k_0} \lt \epsilon$ ($\impliedby$) Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0, |f_n(x)-f(x)| \lt \epsilon, x\in E$. Then for all $n \ge n_0$, $$\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right) \subset E^c$$ $$\implies \mu\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right)=0  $$ which in turn gives us that for all $n \ge n_0$, we have $$||f_n-f||_{\infty} \lt \epsilon$$. (Proof that $L^{\infty}$ is a Banach Space) : Let $\{f_n\}_{n \in \mathbb{N}} \in L^{\infty}$ be  a cauchy sequence. Then for $\epsilon \gt 0$, there exists a $n_0(\epsilon) \in \mathbb{N}$ such that for all $n,m \ge n_0(\epsilon)$, we have $||f_n-f_m||_{\infty} \lt \dfrac{\epsilon}{2}$. Then there exists $E^{\epsilon} \in \mathcal{M}$ such that $\mu(E^{\epsilon})=0$ and for $x \in (E^{\epsilon})^{c} ,|f_n(x)-f_m(x)| \lt \dfrac{\epsilon}{2}$. Let $$E= \cup_n E^{\frac{1}{n}}.$$ Then $\mu(E)=0$ and for all $x \in E^c, \{f_n(x)\}$ is a  cauchy sequence. Let $f(x)=\lim_n f_n(x)$ for $x \in E^c$. Let $\epsilon \gt 0$. Then there exists $k_0 \in \mathbb{N}$ such that for all $k \ge k_0,\frac{1}{k} \lt \frac{\epsilon}{2}$. Then for all $n, m \ge n_0(k_0)$, we have $$|f_n(x)-f_m(x)| \lt \frac{1}{k} \lt \frac{\epsilon}{2}$$ Letting $m \to \infty$ we have $$|f_n(x)-f(x)| \lt \epsilon, x\in E^c.$$ By Lemma-2, we have $||f_n-f||_{\infty} \to 0$. From here by use of triangle inequality, we see that $ f\in L^{\infty}$. Is this proof alright? Thanks for the help!!",,"['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lp-spaces']"
10,If $f$ is uniformly differentiable then $f '$ is uniformly continuous?,If  is uniformly differentiable then  is uniformly continuous?,f f ',"The following theorem is true? Theorem. Let $U\subset \mathbb{R}^m$ (open set) and $f:U\longrightarrow \mathbb{R}^n$ a differentiable function. If $f$  is uniformly differentiable  $ \Longrightarrow$ $f':U\longrightarrow \mathcal{L}(\mathbb{R}^m,\mathbb{R}^n)$ is uniformly continuous. Note that  $f$ is uniformly differentiable if $\forall \epsilon>0\,,\exists \delta>0:|\!|h|\!|<\delta,\color{blue}{[x,x+h]\subset U} \Longrightarrow |\!|f(x+h)-f(x)-f'(x)(h)|\!|<\epsilon |\!|h|\!| $ (edited) $\forall \epsilon>0\,,\exists \delta>0:|\!|h|\!|<\delta,\color{blue}{x,x+h\in U} \Longrightarrow |\!|f(x+h)-f(x)-f'(x)(h)|\!|<\epsilon |\!|h|\!|\qquad  \checkmark$ Any hints would be appreciated.","The following theorem is true? Theorem. Let $U\subset \mathbb{R}^m$ (open set) and $f:U\longrightarrow \mathbb{R}^n$ a differentiable function. If $f$  is uniformly differentiable  $ \Longrightarrow$ $f':U\longrightarrow \mathcal{L}(\mathbb{R}^m,\mathbb{R}^n)$ is uniformly continuous. Note that  $f$ is uniformly differentiable if $\forall \epsilon>0\,,\exists \delta>0:|\!|h|\!|<\delta,\color{blue}{[x,x+h]\subset U} \Longrightarrow |\!|f(x+h)-f(x)-f'(x)(h)|\!|<\epsilon |\!|h|\!| $ (edited) $\forall \epsilon>0\,,\exists \delta>0:|\!|h|\!|<\delta,\color{blue}{x,x+h\in U} \Longrightarrow |\!|f(x+h)-f(x)-f'(x)(h)|\!|<\epsilon |\!|h|\!|\qquad  \checkmark$ Any hints would be appreciated.",,['real-analysis']
11,Can a surjective continuous function from the reals to the reals assume each value an even number of times?,Can a surjective continuous function from the reals to the reals assume each value an even number of times?,,"Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous and onto. Is it possible for $f$ to assume each of its values an even number of times? To clarify, some values might be taken 2 times, some 4, some 6, etc., but always an even (and therefore finite) number. I don't require that there be a value that is assumed any particular number of times.  For example, the function might never take on any value exactly twice. Here is a closely related question with an excellent answer.","Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous and onto. Is it possible for $f$ to assume each of its values an even number of times? To clarify, some values might be taken 2 times, some 4, some 6, etc., but always an even (and therefore finite) number. I don't require that there be a value that is assumed any particular number of times.  For example, the function might never take on any value exactly twice. Here is a closely related question with an excellent answer.",,"['calculus', 'real-analysis', 'continuity']"
12,How do I prove a uniformly continuous function preserves Cauchy sequences?,How do I prove a uniformly continuous function preserves Cauchy sequences?,,"Let $f$ be a uniformly continuous function on A of $\Bbb{R}$. How do I show that if $a_n$ is Cauchy, then $f(a_n)$ is Cauchy. This is what I have worked on, but it does not quite make sense since I feel like I didn't really use the given condition that $f$ is uniformly continuous. Let $\epsilon>0$, $f$ is uniformly continuous, so there exists$\delta>0$ st $|f(x)-f(y)|<\epsilon$ for $|x-y|<\delta$. Since $a_n$ is Cauchy, there exists $N>0$ such that $|a_n-a_m|<\delta$ for $m,n>N$ Hence$|f(a_n)-f(a_m)|<\epsilon$ for $m,n>N$. So $f(a_n)$ is Cauchy","Let $f$ be a uniformly continuous function on A of $\Bbb{R}$. How do I show that if $a_n$ is Cauchy, then $f(a_n)$ is Cauchy. This is what I have worked on, but it does not quite make sense since I feel like I didn't really use the given condition that $f$ is uniformly continuous. Let $\epsilon>0$, $f$ is uniformly continuous, so there exists$\delta>0$ st $|f(x)-f(y)|<\epsilon$ for $|x-y|<\delta$. Since $a_n$ is Cauchy, there exists $N>0$ such that $|a_n-a_m|<\delta$ for $m,n>N$ Hence$|f(a_n)-f(a_m)|<\epsilon$ for $m,n>N$. So $f(a_n)$ is Cauchy",,"['real-analysis', 'uniform-continuity', 'cauchy-sequences']"
13,$\lim_{n\to\infty} f(t/n)=0$ for every $t$ implies $\lim_{t\to0^+}f(t)=0$ (???),for every  implies  (???),\lim_{n\to\infty} f(t/n)=0 t \lim_{t\to0^+}f(t)=0,"The question was this: Suppose $f$ is continuous on $(0,\infty)$ and for every $t>0$ $$\lim_{n\to\infty}f(t/n)=0.$$Does it follow that $f(t)\to0$ as $t\to0$ from above? (Evidently it doesn't go without saying: The question is whether the limit exists - of course it equals $0$ if so.) I couldn't believe I didn't know the answer. Of course if $f$ is not continuous one could concoct a counterexample, but. I believe the answer is yes. So the remaining questions are vague: Is this obvious to someone for some reason I don't see? Is it something everybody knows? Edit: Maybe it's not trivial; assuming just $f(t/n)\to0$ for almost every $t$ is not enough. Here's a proof, I think. Suppose not. Then since $f$ is continuous there exist $\epsilon>0$, $t_k\to0$ and $\delta_k>0$ such that $$|f(t)|\ge\epsilon\quad(|t-t_k|<\delta_k).$$ Let $I_k=(t_k-\delta_k,t_k+\delta_k)$ and $$B_k=\bigcup_{n=1}^\infty nI_k,$$where $nS=\{ns:s\in S\}$. Note that if $0<a<b$ and $t_k<b-a$ then $$[a,b]\cap B_k\ne\emptyset.$$ This allows you to find a nested sequence of intervals $[a_n,b_n]$ such that every element of $[a_n,b_n]$ lies in $B_k$ for at least $n$ values of $k$. So by compactness there exists $t$ such that $t$ lies in infinitely many $B_k$; hence $f(t/k)$ does not tend to $0$. Chuckle: An example showing that assuming $f(t/n)\to0$ for almost every $t$ is not enough: Say $t_k\to0$. Choose $\delta_k>0$ so that the $I_k$ are pairwise disjoint and also $$\sum_km((0,A)\cap B_k)<\infty$$for every $A>0$. Let $f$ be a continuous function on $(0,\infty)$ such that $f=0$ everywhere except on $I_k$, where $f$ has a spike of height $1$. Almost every $t$ lies in only finitely many $B_k$, and for every such $t$ we have $f(t/n)\to0$.","The question was this: Suppose $f$ is continuous on $(0,\infty)$ and for every $t>0$ $$\lim_{n\to\infty}f(t/n)=0.$$Does it follow that $f(t)\to0$ as $t\to0$ from above? (Evidently it doesn't go without saying: The question is whether the limit exists - of course it equals $0$ if so.) I couldn't believe I didn't know the answer. Of course if $f$ is not continuous one could concoct a counterexample, but. I believe the answer is yes. So the remaining questions are vague: Is this obvious to someone for some reason I don't see? Is it something everybody knows? Edit: Maybe it's not trivial; assuming just $f(t/n)\to0$ for almost every $t$ is not enough. Here's a proof, I think. Suppose not. Then since $f$ is continuous there exist $\epsilon>0$, $t_k\to0$ and $\delta_k>0$ such that $$|f(t)|\ge\epsilon\quad(|t-t_k|<\delta_k).$$ Let $I_k=(t_k-\delta_k,t_k+\delta_k)$ and $$B_k=\bigcup_{n=1}^\infty nI_k,$$where $nS=\{ns:s\in S\}$. Note that if $0<a<b$ and $t_k<b-a$ then $$[a,b]\cap B_k\ne\emptyset.$$ This allows you to find a nested sequence of intervals $[a_n,b_n]$ such that every element of $[a_n,b_n]$ lies in $B_k$ for at least $n$ values of $k$. So by compactness there exists $t$ such that $t$ lies in infinitely many $B_k$; hence $f(t/k)$ does not tend to $0$. Chuckle: An example showing that assuming $f(t/n)\to0$ for almost every $t$ is not enough: Say $t_k\to0$. Choose $\delta_k>0$ so that the $I_k$ are pairwise disjoint and also $$\sum_km((0,A)\cap B_k)<\infty$$for every $A>0$. Let $f$ be a continuous function on $(0,\infty)$ such that $f=0$ everywhere except on $I_k$, where $f$ has a spike of height $1$. Almost every $t$ lies in only finitely many $B_k$, and for every such $t$ we have $f(t/n)\to0$.",,"['calculus', 'real-analysis']"
14,Prove that:$f(f(x)) = x^2 \implies \int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13}$,Prove that:,f(f(x)) = x^2 \implies \int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13},"Let $f: [0,\infty) \to [0,\infty)$ be a continuous function such that $f(f(x)) = x^2, \forall x \in [0,\infty)$. Prove that $\displaystyle{\int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13}}$. All I know about this function is that $f$ is bijective, it is strictly increasing*, $f(0) = 0, f(1) = 1, f(x^2) = (f(x))^2, \forall x \in [0, \infty)$ and $f(x) \leq x, \forall x \in [0, 1]$**. With all these, I am not able to show that  $\displaystyle{\int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13}}$. *Suppose that $f$ is strictly decreasing. Then, $\forall x \in (0,1), x^2 < x \implies f(x^2) > f(x) \iff (f(x))^2 > f(x) \iff f(x) > 1$, which is false because, if we substitute $x$ with $0$ and with $1$ in $f(x^2) = (f(x))^2$ we get that $f(0) \in \{0,1\}$ and $f(1) \in \{0,1\}$. So $f$ is strictly increasing. **Suppose that there exists $x_0 \in [0, 1]$ such that $f(x_0) > x_0$. Then, $x_0^2 = f(f(x_0)) > f(x_0) > x_0$, which is false. Then $f(x) \leq x, \forall x \in [0,1]$. Edit: I have come up with an idea to use Riemann sums, but I reach a point where I cannot continue. Let $\epsilon < 1$. Then $f(\epsilon) = x_1$ and $f(x_1) = \epsilon^2$. And now $(f(\epsilon))^2 = f(\epsilon^2) = x_2$ and so on. Now we will use the Riemann sum: We will take the partition $\Delta = (1 > \epsilon > \epsilon ^2 > ... \epsilon ^{2^n} >0 ) $and the intermediate points will be the left margin of each interval. Then we have: $\displaystyle{\int_{0}^{1}{(f(x))^2 dx}}  = \displaystyle{ \lim_{\epsilon \to 1}{\lim_{n \to \infty}{\sum_{k = 0}^{n}{(\epsilon^{2^k} - \epsilon^{2^{k+1}})\epsilon^{2^{k+1}}}}}}$ I do not know how to compute this.","Let $f: [0,\infty) \to [0,\infty)$ be a continuous function such that $f(f(x)) = x^2, \forall x \in [0,\infty)$. Prove that $\displaystyle{\int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13}}$. All I know about this function is that $f$ is bijective, it is strictly increasing*, $f(0) = 0, f(1) = 1, f(x^2) = (f(x))^2, \forall x \in [0, \infty)$ and $f(x) \leq x, \forall x \in [0, 1]$**. With all these, I am not able to show that  $\displaystyle{\int_{0}^{1}{(f(x))^2dx} \geq \frac{3}{13}}$. *Suppose that $f$ is strictly decreasing. Then, $\forall x \in (0,1), x^2 < x \implies f(x^2) > f(x) \iff (f(x))^2 > f(x) \iff f(x) > 1$, which is false because, if we substitute $x$ with $0$ and with $1$ in $f(x^2) = (f(x))^2$ we get that $f(0) \in \{0,1\}$ and $f(1) \in \{0,1\}$. So $f$ is strictly increasing. **Suppose that there exists $x_0 \in [0, 1]$ such that $f(x_0) > x_0$. Then, $x_0^2 = f(f(x_0)) > f(x_0) > x_0$, which is false. Then $f(x) \leq x, \forall x \in [0,1]$. Edit: I have come up with an idea to use Riemann sums, but I reach a point where I cannot continue. Let $\epsilon < 1$. Then $f(\epsilon) = x_1$ and $f(x_1) = \epsilon^2$. And now $(f(\epsilon))^2 = f(\epsilon^2) = x_2$ and so on. Now we will use the Riemann sum: We will take the partition $\Delta = (1 > \epsilon > \epsilon ^2 > ... \epsilon ^{2^n} >0 ) $and the intermediate points will be the left margin of each interval. Then we have: $\displaystyle{\int_{0}^{1}{(f(x))^2 dx}}  = \displaystyle{ \lim_{\epsilon \to 1}{\lim_{n \to \infty}{\sum_{k = 0}^{n}{(\epsilon^{2^k} - \epsilon^{2^{k+1}})\epsilon^{2^{k+1}}}}}}$ I do not know how to compute this.",,"['real-analysis', 'inequality', 'definite-integrals', 'integral-inequality']"
15,Analyticity of the convolution of two functions,Analyticity of the convolution of two functions,,"This question came about when trying to answer this one . Does there exist a function $f\in C^\infty(\Bbb R)$ not identically zero such that: $f$ is supported on $[-1,1]$ , $f$ is analytic on $(-1,1)$ , The convolution $f*f$ is analytic at $0$ ? Typical example of an $f$ satisfying 1. and 2. is $e^{-1/(1-t^2)}\chi_{[-1,1]}$ , where $\chi_A$ is the characteristic function of the set $A$ .","This question came about when trying to answer this one . Does there exist a function not identically zero such that: is supported on , is analytic on , The convolution is analytic at ? Typical example of an satisfying 1. and 2. is , where is the characteristic function of the set .","f\in C^\infty(\Bbb R) f [-1,1] f (-1,1) f*f 0 f e^{-1/(1-t^2)}\chi_{[-1,1]} \chi_A A","['real-analysis', 'fourier-analysis']"
16,Scalar product and uniform convergence of polynomials,Scalar product and uniform convergence of polynomials,,"Given two functions $u$ and $v$, you can compute $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \,\mathrm{d}t$. It resembles the scalar product $\int_{-1}^1 u(t)v(t) \,\mathrm{d}t$, which leads to Legendre polynomials , with a major difference: while the latter is a scalar product over continuous functions, the former is not, since for $f(t)=e^t$, $(f|f)=0$. However, it's obviously a scalar product over polynomials (the solutions of $(f|f)=0$ are not polynomials, and all other defining properties of a scalar product are trivial). We can therefore find an orthonormal basis of polynomials, using Gram-Schmidt orthogonalization on the basis $[1,t,t^2,...]$. Let's call $P_n$ this orthonormal basis. Now, after doing this numerically with a CAS, and after some plots, it looks like $\frac{P_n}{P_n(0)}$ tends uniformly to the function $t \to e^t$ (on $[0,1]$), a solution of $y'-y=0$, which is closely related to the definition of the scalar product above. But I don't know how to prove this. Any idea? Also, if I do the same with $(u|v)=\int_0^1 (u''(t)-u(t))(v''(t)-v(t)) \,\mathrm{d}t$, I'm not absolutely sure, but numerically it looks like, up to a constant factor $P_{2n} \to \cos t$ and $P_{2n+1} \to \sin t$, uniformly on $[0,1]$, so the orthogonal basis of polynomials leads to a basis of solutions of $y''-y=0$. I'd like to know if the process can be generalized to other linear differential equations (provided they don't have polynomial solutions). Does it work with equations with nonconstant coefficients? This is not homework. It's something I discovered while playing with a Ti89 long ago (initially, to test a program doing Gram-Schmidt orthogonalization), and I have never been able to prove or disprove the numerical evidence. $\color{red}{\textbf{Now, why this question?}}$ Apart from mathematical curiosity, there may be some motivation: the method seems quite robust, there is convergence, probably under certain conditions, for several weight functions and differential equations. And we get uniform approximation of solutions of linear differential equations, by polynomials, which may be quite useful in practice. Since many linear ODE give rise to special functions, that would give a cheap way to compute uniform approximations of their solutions, on arbitrary (but bounded) intervals. To help visualize, here are some sample graphics done with Maxima, computed with a straightforward Gram-Schmidt algorithm. The two following are $P_4(x)/P_4(0)-\exp(x)$ and $P_9(x)/P_9(0)-\exp(x)$, in the case $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \,\mathrm{d}t$. The division by $P_n(0)$ is only done in order to ""normalize"" polynomials, since $\exp 0=1$. Now example with ""trigonometric"" differential equation $y''+y=0$. And $(u|v) = \int_0^{2\pi} (u(t)''+u(t))(v(t)''+v(t)) \,\mathrm{d}t$. Notice the integration bound differ from the preceding, and now uniform convergence of $P_n$ takes place on $[0,2\pi]$. The normalization is here $P_{2n}(x)/P_{2n}(0)$ as $\cos 0 = 1$ ans $P_{2n+1}(x)/P_{2n+1}(\pi/2)$, as $\sin \pi/2=1$, and precisely, $P_{8}(x)/P_{8}(0)-\cos x$ and $P_{9}(x)/P_{9}(\pi/2)-\sin x$ are respectively plotted. Notice, it's also possible to add a weighting function to our scalar product, e.g. $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \mathcal{W}(t)\,\mathrm{d}t$. For example, with$\mathcal{W}(t)=\frac{1}{\sqrt{t(1-t)}}$ on $[0,1]$, there is still convergence, and for example $P_9(x)/P_9(0) -\exp x$ yields the following plot.","Given two functions $u$ and $v$, you can compute $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \,\mathrm{d}t$. It resembles the scalar product $\int_{-1}^1 u(t)v(t) \,\mathrm{d}t$, which leads to Legendre polynomials , with a major difference: while the latter is a scalar product over continuous functions, the former is not, since for $f(t)=e^t$, $(f|f)=0$. However, it's obviously a scalar product over polynomials (the solutions of $(f|f)=0$ are not polynomials, and all other defining properties of a scalar product are trivial). We can therefore find an orthonormal basis of polynomials, using Gram-Schmidt orthogonalization on the basis $[1,t,t^2,...]$. Let's call $P_n$ this orthonormal basis. Now, after doing this numerically with a CAS, and after some plots, it looks like $\frac{P_n}{P_n(0)}$ tends uniformly to the function $t \to e^t$ (on $[0,1]$), a solution of $y'-y=0$, which is closely related to the definition of the scalar product above. But I don't know how to prove this. Any idea? Also, if I do the same with $(u|v)=\int_0^1 (u''(t)-u(t))(v''(t)-v(t)) \,\mathrm{d}t$, I'm not absolutely sure, but numerically it looks like, up to a constant factor $P_{2n} \to \cos t$ and $P_{2n+1} \to \sin t$, uniformly on $[0,1]$, so the orthogonal basis of polynomials leads to a basis of solutions of $y''-y=0$. I'd like to know if the process can be generalized to other linear differential equations (provided they don't have polynomial solutions). Does it work with equations with nonconstant coefficients? This is not homework. It's something I discovered while playing with a Ti89 long ago (initially, to test a program doing Gram-Schmidt orthogonalization), and I have never been able to prove or disprove the numerical evidence. $\color{red}{\textbf{Now, why this question?}}$ Apart from mathematical curiosity, there may be some motivation: the method seems quite robust, there is convergence, probably under certain conditions, for several weight functions and differential equations. And we get uniform approximation of solutions of linear differential equations, by polynomials, which may be quite useful in practice. Since many linear ODE give rise to special functions, that would give a cheap way to compute uniform approximations of their solutions, on arbitrary (but bounded) intervals. To help visualize, here are some sample graphics done with Maxima, computed with a straightforward Gram-Schmidt algorithm. The two following are $P_4(x)/P_4(0)-\exp(x)$ and $P_9(x)/P_9(0)-\exp(x)$, in the case $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \,\mathrm{d}t$. The division by $P_n(0)$ is only done in order to ""normalize"" polynomials, since $\exp 0=1$. Now example with ""trigonometric"" differential equation $y''+y=0$. And $(u|v) = \int_0^{2\pi} (u(t)''+u(t))(v(t)''+v(t)) \,\mathrm{d}t$. Notice the integration bound differ from the preceding, and now uniform convergence of $P_n$ takes place on $[0,2\pi]$. The normalization is here $P_{2n}(x)/P_{2n}(0)$ as $\cos 0 = 1$ ans $P_{2n+1}(x)/P_{2n+1}(\pi/2)$, as $\sin \pi/2=1$, and precisely, $P_{8}(x)/P_{8}(0)-\cos x$ and $P_{9}(x)/P_{9}(\pi/2)-\sin x$ are respectively plotted. Notice, it's also possible to add a weighting function to our scalar product, e.g. $(u|v) = \int_0^1 (u(t)'-u(t))(v(t)'-v(t)) \mathcal{W}(t)\,\mathrm{d}t$. For example, with$\mathcal{W}(t)=\frac{1}{\sqrt{t(1-t)}}$ on $[0,1]$, there is still convergence, and for example $P_9(x)/P_9(0) -\exp x$ yields the following plot.",,"['real-analysis', 'ordinary-differential-equations', 'orthogonal-polynomials']"
17,"""Novel"" proofs of ""old"" calculus theorems","""Novel"" proofs of ""old"" calculus theorems",,"Every once in a while some mathematicians publish (mostly on the American Mathematical Monthly ) a new proof of an old (nowadays considered "" basic "") result in analysis (calculus). This article is an example. I would like to collect a "" big list "" of such novel proofs of old results. Note, however, that I am only looking for proofs that represent an improvement ( in some sense ) over standard alternatives which can be found on most textbooks.","Every once in a while some mathematicians publish (mostly on the American Mathematical Monthly ) a new proof of an old (nowadays considered "" basic "") result in analysis (calculus). This article is an example. I would like to collect a "" big list "" of such novel proofs of old results. Note, however, that I am only looking for proofs that represent an improvement ( in some sense ) over standard alternatives which can be found on most textbooks.",,"['calculus', 'real-analysis', 'reference-request', 'big-list', 'alternative-proof']"
18,Expository articles on Analysis and Probability theory,Expository articles on Analysis and Probability theory,,"When I come across a notion from algebra or number theory which I don't know I usually check Keith Conrad's page to see if he has written something about it. Key features of his articles are a very clear exposition and carefully worked-out, well-chosen examples. Furthermore, he explains why the definitions are the way the are (why do we require a subring of a unital ring to have the same multiplicative identy as the original ring?). I am now looking for articles of similar style and quality explaining the basic notions of analysis (both real and complex analysis) and probability theory (so they should be aimed at undergraduate students). More specifically, I mean the notions (and central theorems) you would expect to learn in any undergraduate course on the subject at a German university. The content descriptions of the courses in the following document might give you an idea of what that means: mathematics.uni-bonn.de/study/master/files/MA_QualTest.pdf The courses are: Analysis I & II (page 2), Analysis III (page 3), Introduction to complex analysis (p. 4), Introduction to Probability & Stochastik Processes (both p. 6). I tried to find something using google and the only thing I found is http://www.mtts.org.in/expository-articles . There are articles on theorems from analysis on that site, but they focus on proofs and do not contain enough motivation for definitions or interesting examples (as far as I checked, I did not read them all). The articles should not be too dense (to give you an idea of what that means: They should not be as dense and short as the articles in the Princeton Companion). Note that I am looking for articles which are available freely online. I am aware of the related question Elementary Papers at ArXiv , however, I ask for articles explaining ideas and notions from analysis and probability theory only and do not require the articles to be posted on the arxiv.","When I come across a notion from algebra or number theory which I don't know I usually check Keith Conrad's page to see if he has written something about it. Key features of his articles are a very clear exposition and carefully worked-out, well-chosen examples. Furthermore, he explains why the definitions are the way the are (why do we require a subring of a unital ring to have the same multiplicative identy as the original ring?). I am now looking for articles of similar style and quality explaining the basic notions of analysis (both real and complex analysis) and probability theory (so they should be aimed at undergraduate students). More specifically, I mean the notions (and central theorems) you would expect to learn in any undergraduate course on the subject at a German university. The content descriptions of the courses in the following document might give you an idea of what that means: mathematics.uni-bonn.de/study/master/files/MA_QualTest.pdf The courses are: Analysis I & II (page 2), Analysis III (page 3), Introduction to complex analysis (p. 4), Introduction to Probability & Stochastik Processes (both p. 6). I tried to find something using google and the only thing I found is http://www.mtts.org.in/expository-articles . There are articles on theorems from analysis on that site, but they focus on proofs and do not contain enough motivation for definitions or interesting examples (as far as I checked, I did not read them all). The articles should not be too dense (to give you an idea of what that means: They should not be as dense and short as the articles in the Princeton Companion). Note that I am looking for articles which are available freely online. I am aware of the related question Elementary Papers at ArXiv , however, I ask for articles explaining ideas and notions from analysis and probability theory only and do not require the articles to be posted on the arxiv.",,"['real-analysis', 'complex-analysis', 'soft-question', 'probability-theory', 'big-list']"
19,How smooth can non-nice associative operations on the reals be?,How smooth can non-nice associative operations on the reals be?,,"Suppose ${*}:\mathbb R\times\mathbb R\to\mathbb R$ is $\mathcal C^k$ and associative . Does it necessarily satisfy the identity $a * b * c * d = a * c * b * d$? For $k=0$ the answer is ""no"" -- a counterexample would be to let $x_1*x_2*\cdots*x_n$ to be the product of the $x_i$s up to and including the leftmost negative one (and the product of all of them if they are all nonnegative). However, I can't find any counterexample for $k=1$. Will high enough $k$ guarantee that $a*b*c*d=a*c*b*d$? If not, will $\mathcal C^\infty$? Will ""analytic""? Bonus question: If there is a $k$ that does force the identity, does the same $k$ work with $\mathbb R^n$ instead of $\mathbb R$? (Edit: obviously not; multiplication of $[\,{}^{\strut x}_0\;{}^{\strut y}_1\,]$ matrices is very smooth.) The condition that $a*b*c*d\ne a*c*b*d$ for some $a,b,c,d$ is meant to force $*$ to be noncommutative while at the same time avoiding trivial solutions where $a*b*c$ doesn't depend on $b$. (This is a variant of a question posed by Nikhil Mahajan which unfortunately failed to get interesting answers due to a technicality.)","Suppose ${*}:\mathbb R\times\mathbb R\to\mathbb R$ is $\mathcal C^k$ and associative . Does it necessarily satisfy the identity $a * b * c * d = a * c * b * d$? For $k=0$ the answer is ""no"" -- a counterexample would be to let $x_1*x_2*\cdots*x_n$ to be the product of the $x_i$s up to and including the leftmost negative one (and the product of all of them if they are all nonnegative). However, I can't find any counterexample for $k=1$. Will high enough $k$ guarantee that $a*b*c*d=a*c*b*d$? If not, will $\mathcal C^\infty$? Will ""analytic""? Bonus question: If there is a $k$ that does force the identity, does the same $k$ work with $\mathbb R^n$ instead of $\mathbb R$? (Edit: obviously not; multiplication of $[\,{}^{\strut x}_0\;{}^{\strut y}_1\,]$ matrices is very smooth.) The condition that $a*b*c*d\ne a*c*b*d$ for some $a,b,c,d$ is meant to force $*$ to be noncommutative while at the same time avoiding trivial solutions where $a*b*c$ doesn't depend on $b$. (This is a variant of a question posed by Nikhil Mahajan which unfortunately failed to get interesting answers due to a technicality.)",,"['real-analysis', 'abstract-algebra', 'associativity']"
20,Refinement of a famous inequality,Refinement of a famous inequality,,"I refine a famous inequality this is the following  : Let $x,y>0$ then we have : $$x^n+y^n\leq \Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n+\Big(\frac{x+y}{2}\Big)^n$$ It's equivalent to : $$x^n+1\leq \Big(\frac{x^n+1}{x^{n-1}+1} \Big)^n+\Big(\frac{x+1}{2}\Big)^n$$ Because it's homogeneous . We can't use AM GM  it's too weak so the difficulty  is interesting . I try to derivate this but it's a little bit ugly . I have two questions how interpreting this result and how to solve this one variable inequality  ? Thanks a lot Remark (@Andreas, 2020-10-25) This inequality is rather fine-tuned. Consider as a first term on the RHS $$\Big(\frac{x^n+y^n+z\cdot(\frac{x+y}{2})^n}{x^{n-1}+y^{n-1}+z\cdot(\frac{x+y}{2})^{n-1}}\Big)^n $$ and let $z$ increase from $0$ to $1$ . It is easy to see that the increase of $z$ makes the term smaller. Choosing $z=0$ (this question) makes the term ""just big enough"" for the inequality to be "" $\le$ "". Indeed, for $z=1$ , this inverses to "" $\ge$ "", as this post shows. So, fine tuned upper and lower bounds to $x^n + y^n$ are available. An interesting observation is the following: $$\Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n \ge x^n+y^n - \Big(\frac{x+y}{2}\Big)^n \ge \frac{x^n+y^n}{2} .$$ The first inequality is the one under consideration, the second one is an application of Jensen's inequality for two values of the function $x^n$ , whereas the inequality between the first and the third expression is a direct application of Slater's inequality* (eq. (2) in  this pdf) , so we see here a sharpening of Slater's inequality for the function $x^n$ . *Slater ML, A Companion Inequality to Jensen's Inequality. Jour. of Approximation Theory 1981, 32(2):160166.","I refine a famous inequality this is the following  : Let then we have : It's equivalent to : Because it's homogeneous . We can't use AM GM  it's too weak so the difficulty  is interesting . I try to derivate this but it's a little bit ugly . I have two questions how interpreting this result and how to solve this one variable inequality  ? Thanks a lot Remark (@Andreas, 2020-10-25) This inequality is rather fine-tuned. Consider as a first term on the RHS and let increase from to . It is easy to see that the increase of makes the term smaller. Choosing (this question) makes the term ""just big enough"" for the inequality to be "" "". Indeed, for , this inverses to "" "", as this post shows. So, fine tuned upper and lower bounds to are available. An interesting observation is the following: The first inequality is the one under consideration, the second one is an application of Jensen's inequality for two values of the function , whereas the inequality between the first and the third expression is a direct application of Slater's inequality* (eq. (2) in  this pdf) , so we see here a sharpening of Slater's inequality for the function . *Slater ML, A Companion Inequality to Jensen's Inequality. Jour. of Approximation Theory 1981, 32(2):160166.","x,y>0 x^n+y^n\leq \Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n+\Big(\frac{x+y}{2}\Big)^n x^n+1\leq \Big(\frac{x^n+1}{x^{n-1}+1} \Big)^n+\Big(\frac{x+1}{2}\Big)^n \Big(\frac{x^n+y^n+z\cdot(\frac{x+y}{2})^n}{x^{n-1}+y^{n-1}+z\cdot(\frac{x+y}{2})^{n-1}}\Big)^n
 z 0 1 z z=0 \le z=1 \ge x^n + y^n \Big(\frac{x^n+y^n}{x^{n-1}+y^{n-1}} \Big)^n \ge x^n+y^n - \Big(\frac{x+y}{2}\Big)^n \ge \frac{x^n+y^n}{2} . x^n x^n",['real-analysis']
21,A closed $1$-form on a convex open set is exact,A closed -form on a convex open set is exact,1,"Baby Rudin Exercise 10.24: Let $\omega = \sum a_i(\mathbf x) \, dx_i$ be a $1$-form of class $\mathscr{C}''$ in a convex open set  $E \subset \mathbb{R}^n$. Assume $d \omega = 0$ and show that $\omega$ is exact in $E$, by completing the following outline: Fix $p \in E$, Define $$f(\mathbf{x}) = \int_{\bf [p,x]} \omega $$ and apply Stokes' theorem to affine-oriented 2-simplexes $\bf[p, x, y]$ in E. Deduce that \begin{equation} f(\mathbf y) - f(\mathbf x) = \sum_{i=1}^n (y_i - x_i) \int_0^1 a_i((1-t) \mathbf x + t \mathbf y)\,dt \end{equation} for $\mathbf x \in E, \mathbf y \in E$. Hence $(D_i f)(\mathbf x) = a_i(\mathbf x)$. The boundary of $\bf[p, x, y]$ is $\bf [x, y] - [p, y] + [p, x]$, from which $$f(\mathbf y) - f(\mathbf x) = \int_{\bf [x,y]} \omega= \int_{\bf [x,y]} \sum_{i=1}^n a_i(\mathbf x) \, dx_i$$ follows. Next take $\gamma(t) = (1-t) \mathbf x + t \mathbf y$ for $0 \leq t \leq 1$. This is a $1$-surface. By differentiation of $1$-forms, we have $$\int_\gamma \omega = \int_{\bf [x, y]} \omega = \int_0^1 \sum_{i=1}^n a_i((1-t) \mathbf x + t \mathbf y)(y_i - x_i) \, dt$$ which is the same as what Rudin gives. At this point I'm not sure what to do. I suppose I should partially differentiate both sides with respect to one of the basis vectors of $\mathbb{R}^n$, but I'm not sure how to go forth with that. Edit. I have followed JohnMa's advice and here is the rest of the solution (for verification and archival purposes): Fix any $\mathbf x$ and and take $\mathbf y = \mathbf x + v \mathbf e_k$. Then  $$ f(\mathbf x + v \mathbf e_k) - f(\mathbf x) = v \int_0^1 a_k ((1-t) \mathbf x + t (\mathbf x + v \mathbf e_k))\, dt = v \int_0^1 a_k(\mathbf x + tv \mathbf e_k) \, dt $$ Dividing by $v$ and taking $v \to 0$ gives $$ (D_k f)(\mathbf x) = a_k (\mathbf x) $$ since $a_k$ is continuous. Since $\mathbf{x}$ was arbitrary, we have $D_k f = a_k$ for $1 \leq k \leq n$, hence $$\omega = \sum_{i=1}^n (D_i f)(\mathbf x) \, dx_i = d f$$ Since $f$ is a $0$-form.","Baby Rudin Exercise 10.24: Let $\omega = \sum a_i(\mathbf x) \, dx_i$ be a $1$-form of class $\mathscr{C}''$ in a convex open set  $E \subset \mathbb{R}^n$. Assume $d \omega = 0$ and show that $\omega$ is exact in $E$, by completing the following outline: Fix $p \in E$, Define $$f(\mathbf{x}) = \int_{\bf [p,x]} \omega $$ and apply Stokes' theorem to affine-oriented 2-simplexes $\bf[p, x, y]$ in E. Deduce that \begin{equation} f(\mathbf y) - f(\mathbf x) = \sum_{i=1}^n (y_i - x_i) \int_0^1 a_i((1-t) \mathbf x + t \mathbf y)\,dt \end{equation} for $\mathbf x \in E, \mathbf y \in E$. Hence $(D_i f)(\mathbf x) = a_i(\mathbf x)$. The boundary of $\bf[p, x, y]$ is $\bf [x, y] - [p, y] + [p, x]$, from which $$f(\mathbf y) - f(\mathbf x) = \int_{\bf [x,y]} \omega= \int_{\bf [x,y]} \sum_{i=1}^n a_i(\mathbf x) \, dx_i$$ follows. Next take $\gamma(t) = (1-t) \mathbf x + t \mathbf y$ for $0 \leq t \leq 1$. This is a $1$-surface. By differentiation of $1$-forms, we have $$\int_\gamma \omega = \int_{\bf [x, y]} \omega = \int_0^1 \sum_{i=1}^n a_i((1-t) \mathbf x + t \mathbf y)(y_i - x_i) \, dt$$ which is the same as what Rudin gives. At this point I'm not sure what to do. I suppose I should partially differentiate both sides with respect to one of the basis vectors of $\mathbb{R}^n$, but I'm not sure how to go forth with that. Edit. I have followed JohnMa's advice and here is the rest of the solution (for verification and archival purposes): Fix any $\mathbf x$ and and take $\mathbf y = \mathbf x + v \mathbf e_k$. Then  $$ f(\mathbf x + v \mathbf e_k) - f(\mathbf x) = v \int_0^1 a_k ((1-t) \mathbf x + t (\mathbf x + v \mathbf e_k))\, dt = v \int_0^1 a_k(\mathbf x + tv \mathbf e_k) \, dt $$ Dividing by $v$ and taking $v \to 0$ gives $$ (D_k f)(\mathbf x) = a_k (\mathbf x) $$ since $a_k$ is continuous. Since $\mathbf{x}$ was arbitrary, we have $D_k f = a_k$ for $1 \leq k \leq n$, hence $$\omega = \sum_{i=1}^n (D_i f)(\mathbf x) \, dx_i = d f$$ Since $f$ is a $0$-form.",,"['real-analysis', 'differential-forms']"
22,A question connected with the decomposition of a functional on $C(X)$ on Riesz and Banach functionals,A question connected with the decomposition of a functional on  on Riesz and Banach functionals,C(X),"Let $X$ be a metric space and let $C(X)$ be a family of all bounded and  continuous functions from $X$ in $\mathbb{R}$. We call a positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ the functional of Riesz if there is a borel measure $\mu$ on $X$, such that $\varphi(f)=\int_X f \,d\mu$, for $f\in C(X)$. We call a positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ the functional of Banach if for each borel measure $\nu$ on $X$ the condition:$\int_X f d\nu\leq \varphi(f)$, for $f\in C(X)$ - implies that $\nu$ is trivial. There is a well known theorem : Let $X$ be a polish space. Then, for each positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ there is a unique couple $(\varphi_0,\varphi_*)$ of positive linear functionals defined on $C(X)$, such that $\varphi_0$ is the functional of Riesz, $\varphi_*$ is the functional of Banach and $\varphi=\varphi_*+\varphi_0$. Moreover, the measure $\mu$ related to $\varphi_0$ is defined by: $$\mu(K)=\inf\{\varphi(f): f\in C(X), 1_X\geq f \geq 1_K\},$$  for each compact set $K\subset X$. More pecisely, for the proof, we define: $$\varphi_{\delta}(f)=\sup\{\varphi(h): \mbox{ supp}\,h\in N(\delta), 0\leq h\leq f\},$$ for $\delta>0$, $$\varphi_{0}(f)=\lim\limits_{\delta \to 0^+}\varphi_{\delta}(f),$$ for $f\in C(X), f\geq 0$, and $$\varphi_{0}(f)=\varphi_{0}(f^+)-\varphi_{0}(f^-),$$ for $f \in C(X)$, where $N(\delta)$ is a family of sets that possess a covering composed of finite number of open balls with a radius equal to $\delta$. My question concerns the truth of the following sentence:  Let $X$ be a $\sigma$-compact and polish space. Assume that $\varphi^x:C(X) \rightarrow \mathbb{R}$ is a positive linear functional, for all $x \in X$ and let $((\varphi^x)_0,(\varphi^x)_*)$ be a couple of Banach-Riesz functionals, for $x \in X$. If the mapping $X \ni x \mapsto  \varphi^x(f)$ is continuous for all $f \in C(X)$ and $\varphi^x(1_X)=1$, for $x \in X$, then mapping $X \ni x \mapsto  (\varphi^x)_0(f)$ is continuous for all $f \in C(X)$ (or may be for only $f \in C_c(X)$). I was able to proof only that the mapping $X \ni x \mapsto  (\varphi^x)_0(f)$ is upper semi-continuous, for $f\in C_c(X)$.","Let $X$ be a metric space and let $C(X)$ be a family of all bounded and  continuous functions from $X$ in $\mathbb{R}$. We call a positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ the functional of Riesz if there is a borel measure $\mu$ on $X$, such that $\varphi(f)=\int_X f \,d\mu$, for $f\in C(X)$. We call a positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ the functional of Banach if for each borel measure $\nu$ on $X$ the condition:$\int_X f d\nu\leq \varphi(f)$, for $f\in C(X)$ - implies that $\nu$ is trivial. There is a well known theorem : Let $X$ be a polish space. Then, for each positive linear functional $\varphi: C(X) \rightarrow \mathbb{R}$ there is a unique couple $(\varphi_0,\varphi_*)$ of positive linear functionals defined on $C(X)$, such that $\varphi_0$ is the functional of Riesz, $\varphi_*$ is the functional of Banach and $\varphi=\varphi_*+\varphi_0$. Moreover, the measure $\mu$ related to $\varphi_0$ is defined by: $$\mu(K)=\inf\{\varphi(f): f\in C(X), 1_X\geq f \geq 1_K\},$$  for each compact set $K\subset X$. More pecisely, for the proof, we define: $$\varphi_{\delta}(f)=\sup\{\varphi(h): \mbox{ supp}\,h\in N(\delta), 0\leq h\leq f\},$$ for $\delta>0$, $$\varphi_{0}(f)=\lim\limits_{\delta \to 0^+}\varphi_{\delta}(f),$$ for $f\in C(X), f\geq 0$, and $$\varphi_{0}(f)=\varphi_{0}(f^+)-\varphi_{0}(f^-),$$ for $f \in C(X)$, where $N(\delta)$ is a family of sets that possess a covering composed of finite number of open balls with a radius equal to $\delta$. My question concerns the truth of the following sentence:  Let $X$ be a $\sigma$-compact and polish space. Assume that $\varphi^x:C(X) \rightarrow \mathbb{R}$ is a positive linear functional, for all $x \in X$ and let $((\varphi^x)_0,(\varphi^x)_*)$ be a couple of Banach-Riesz functionals, for $x \in X$. If the mapping $X \ni x \mapsto  \varphi^x(f)$ is continuous for all $f \in C(X)$ and $\varphi^x(1_X)=1$, for $x \in X$, then mapping $X \ni x \mapsto  (\varphi^x)_0(f)$ is continuous for all $f \in C(X)$ (or may be for only $f \in C_c(X)$). I was able to proof only that the mapping $X \ni x \mapsto  (\varphi^x)_0(f)$ is upper semi-continuous, for $f\in C_c(X)$.",,"['real-analysis', 'probability-theory', 'measure-theory']"
23,"Continuous function from $(0, 1]$ onto $(0, 1)$?",Continuous function from  onto ?,"(0, 1] (0, 1)","Is there any continuous function defined on $(0, 1]$ whose range is $(0, 1)$?","Is there any continuous function defined on $(0, 1]$ whose range is $(0, 1)$?",,"['real-analysis', 'examples-counterexamples']"
24,Good Textbooks for Real Analysis and Topology.,Good Textbooks for Real Analysis and Topology.,,"I'm currently in my 3rd year of my undergrad in Mathematics and moving onto my 4th year next year. I took a course in Real Analysis I, but the professor was very confusing and we didn't use a textbook for the class (it was his lecture notes) which was also very confusing as well. Although I got a good mark in the course, I don't think I learned anything from the class since I felt like I memorized how to do the questions rather than actually understood the questions. So I was wondering what would be a good textbook for real analysis. I'm okay with a textbook with rigorous proofs, as long as everything is explained in good detail. At the same time, I also want to teach myself Topology. I wanted to take it this year, but there was a conflict and hence I could not take the course. So I was also looking for a textbook for an introduction to Topology. Any kind of recommendations would be great! I really do want to learn analysis and topology.","I'm currently in my 3rd year of my undergrad in Mathematics and moving onto my 4th year next year. I took a course in Real Analysis I, but the professor was very confusing and we didn't use a textbook for the class (it was his lecture notes) which was also very confusing as well. Although I got a good mark in the course, I don't think I learned anything from the class since I felt like I memorized how to do the questions rather than actually understood the questions. So I was wondering what would be a good textbook for real analysis. I'm okay with a textbook with rigorous proofs, as long as everything is explained in good detail. At the same time, I also want to teach myself Topology. I wanted to take it this year, but there was a conflict and hence I could not take the course. So I was also looking for a textbook for an introduction to Topology. Any kind of recommendations would be great! I really do want to learn analysis and topology.",,"['real-analysis', 'reference-request', 'soft-question', 'self-learning', 'book-recommendation']"
25,"How to ""see"" discontinuity of second derivative from graph of function","How to ""see"" discontinuity of second derivative from graph of function",,"Suppose we have a real function $ f: \mathbb{R} \to \mathbb{R}$ that is two times differentiable and we draw its graph $\{(x,f(x)), x \in \mathbb{R} \} $ . We know, for example, that when the first derivative is not continuous at a point we then have an ""corner"" in the graph. How about a discontinuity of $f''(x)$ at a point $x_0$ though? Can we spot that just by drawing the graph of $f(x)$ -not drawing the graph of the first derivative and noticing it has an ""corner"" at $x_0$ , that's cheating. What about discontinuities of higher derivatives, which will of course be way harder to ""see""?","Suppose we have a real function that is two times differentiable and we draw its graph . We know, for example, that when the first derivative is not continuous at a point we then have an ""corner"" in the graph. How about a discontinuity of at a point though? Can we spot that just by drawing the graph of -not drawing the graph of the first derivative and noticing it has an ""corner"" at , that's cheating. What about discontinuities of higher derivatives, which will of course be way harder to ""see""?"," f: \mathbb{R} \to \mathbb{R} \{(x,f(x)), x \in \mathbb{R} \}  f''(x) x_0 f(x) x_0","['real-analysis', 'functions', 'derivatives']"
26,Showing n! is greater than n to the tenth power,Showing n! is greater than n to the tenth power,,"I'd like to show $n!>n^{10} $ for large enough n ( namely $ n \geq 15 $). By induction, I do not know how to proceed at this step: $$ (n+1)\cdot n!>(n+1)^{10}  $$ As I can't see how to simplify $(n+1)^{10} $. This seems like such a trivial thing (and it probably is), yet I can't do it. Isn't there an easier way to show this? (P.S. I need to refrain from the use of derivatives, integrals etc., I suppose, then you could work something out with the slope of the respective functions)","I'd like to show $n!>n^{10} $ for large enough n ( namely $ n \geq 15 $). By induction, I do not know how to proceed at this step: $$ (n+1)\cdot n!>(n+1)^{10}  $$ As I can't see how to simplify $(n+1)^{10} $. This seems like such a trivial thing (and it probably is), yet I can't do it. Isn't there an easier way to show this? (P.S. I need to refrain from the use of derivatives, integrals etc., I suppose, then you could work something out with the slope of the respective functions)",,"['real-analysis', 'inequality', 'factorial']"
27,Show that unit circle is compact?,Show that unit circle is compact?,,"Quick question. Say we are given the unit circle $\{ (x,y)\in \mathbb{R}^2: x^2+y^2=1 \}$. Is this set compact? How can I prove that this is closed? Bounded? Do I have to take the complement of the set, showing that that set is open (and so unit circle is closed)? Any other trick? In addition, how can I show that $\{(x,y) \in \mathbb{R}^2: x^2+y^2 < 1\}$ is not compact? I have to show that this thing is open, how can I do that? I know that compact is equivalent by saying that the set is bounded and closed, if we are talking about subsets of $\mathbb{R}^n$. I also can see that the unit discs are bounded, because the distance between any two points in the set is bounded. But how to show that those are open/closed? Thanks for your help! :-)","Quick question. Say we are given the unit circle $\{ (x,y)\in \mathbb{R}^2: x^2+y^2=1 \}$. Is this set compact? How can I prove that this is closed? Bounded? Do I have to take the complement of the set, showing that that set is open (and so unit circle is closed)? Any other trick? In addition, how can I show that $\{(x,y) \in \mathbb{R}^2: x^2+y^2 < 1\}$ is not compact? I have to show that this thing is open, how can I do that? I know that compact is equivalent by saying that the set is bounded and closed, if we are talking about subsets of $\mathbb{R}^n$. I also can see that the unit discs are bounded, because the distance between any two points in the set is bounded. But how to show that those are open/closed? Thanks for your help! :-)",,"['real-analysis', 'general-topology', 'metric-spaces']"
28,Derivative of determinant of a matrix,Derivative of determinant of a matrix,,"I would like to know how to calculate: $$\frac{d}{dt}\det \big(A_1(t), A_2(t), \ldots, A_n (t) \big).$$",I would like to know how to calculate:,"\frac{d}{dt}\det \big(A_1(t), A_2(t), \ldots, A_n (t) \big).","['linear-algebra', 'real-analysis', 'matrices', 'derivatives']"
29,when product of irrational numbers = rational number?,when product of irrational numbers = rational number?,,let $a$ and $b$ be irrational numbers. when do we have $ a \cdot b $ = rational number? for example $\sqrt{2} \cdot \sqrt{2}=2$. I was wondering if there some conditions for the product to be a rational number.,let $a$ and $b$ be irrational numbers. when do we have $ a \cdot b $ = rational number? for example $\sqrt{2} \cdot \sqrt{2}=2$. I was wondering if there some conditions for the product to be a rational number.,,['real-analysis']
30,How to show $\lim_{n \to \infty} a_n = \frac{ [x] + [2x] + [3x] + \dotsb + [nx] }{n^2} = x/2$?,How to show ?,\lim_{n \to \infty} a_n = \frac{ [x] + [2x] + [3x] + \dotsb + [nx] }{n^2} = x/2,"This question came from the prelim exam I took last month. I have a proof that seems a bit unwieldy to me (posted as an answer), so I'm opening it up to ask if there are other ways of showing this. Let $x$ be any positive real number, and define a sequence $\{a_n\}$ by   $$ a_n = \frac{ [x] + [2x] + [3x] + \dotsb + [nx] }{n^2} $$   where $[x]$ is the largest integer less than or equal to $x$. Prove that $\displaystyle{\lim_{n \to \infty} a_n = x/2}$.","This question came from the prelim exam I took last month. I have a proof that seems a bit unwieldy to me (posted as an answer), so I'm opening it up to ask if there are other ways of showing this. Let $x$ be any positive real number, and define a sequence $\{a_n\}$ by   $$ a_n = \frac{ [x] + [2x] + [3x] + \dotsb + [nx] }{n^2} $$   where $[x]$ is the largest integer less than or equal to $x$. Prove that $\displaystyle{\lim_{n \to \infty} a_n = x/2}$.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'ceiling-and-floor-functions']"
31,"Prove that the closure of complement, is the complement of the interior","Prove that the closure of complement, is the complement of the interior",,"Let $(X,d)$ a metric space and $A\subset X$ . Prove that $$\overline{(A^c)}=\overset{\circ}{(A)}^c$$ i.e. the closure of complement, is  the complement of the interior. Proof. If $x\in \overline{(A^c)}$ then $x\in A^c$ or $x\in (A^c)'$ . Since $\overset{\circ}{A} \subset A \Rightarrow A^c \subset \overset{\circ}{(A)}^c$ so, if $x\in A^c$ then $x\in \overset{\circ}{(A)}^c$ . Now if $x \not\in A^c$ but $x\in (A^c)'$ then $\forall r>0, B(x;r)-\{x\}\cap A^c \not= \emptyset.$ And now I've to deduce that $x\not\in \overset{\circ}{A}$ so $x\in \overset{\circ}{(A)}^c$ , but I can't figure out how.","Let a metric space and . Prove that i.e. the closure of complement, is  the complement of the interior. Proof. If then or . Since so, if then . Now if but then And now I've to deduce that so , but I can't figure out how.","(X,d) A\subset X \overline{(A^c)}=\overset{\circ}{(A)}^c x\in \overline{(A^c)} x\in A^c x\in (A^c)' \overset{\circ}{A} \subset A \Rightarrow A^c \subset \overset{\circ}{(A)}^c x\in A^c x\in \overset{\circ}{(A)}^c x \not\in A^c x\in (A^c)' \forall r>0, B(x;r)-\{x\}\cap A^c \not= \emptyset. x\not\in \overset{\circ}{A} x\in \overset{\circ}{(A)}^c","['real-analysis', 'general-topology']"
32,Real analysis book suggestion,Real analysis book suggestion,,"I am searching for a real analysis book (instead of Rudin's) which satisfies the following requirements: clear, motivated (but not chatty), clean exposition in definition-theorem-proof style; complete (and possibly elegant and explicative) proofs of every theorem; examples and solved exercises; possibly, the proofs of the theorems on limits of functions should not use series; generalizations of theorem often given for $\mathbb{R}$ to metric spaces and also to topological spaces. Thank you very much in advance for your assistance.","I am searching for a real analysis book (instead of Rudin's) which satisfies the following requirements: clear, motivated (but not chatty), clean exposition in definition-theorem-proof style; complete (and possibly elegant and explicative) proofs of every theorem; examples and solved exercises; possibly, the proofs of the theorems on limits of functions should not use series; generalizations of theorem often given for $\mathbb{R}$ to metric spaces and also to topological spaces. Thank you very much in advance for your assistance.",,"['real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
33,Lebesgue non-measurable function,Lebesgue non-measurable function,,"Can we give an example of Lebesgue non-measurable function, for which set $\{x: f(x)=C\}~\forall C\in\mathbb{R}$ is measurable? Thanks.","Can we give an example of Lebesgue non-measurable function, for which set $\{x: f(x)=C\}~\forall C\in\mathbb{R}$ is measurable? Thanks.",,"['real-analysis', 'analysis', 'measure-theory']"
34,Show $\lim \limits_{n \to \infty} \frac{a_{n+1}}{a_n} = \|f\|_{\infty}$ for $f \in L^{\infty}$,Show  for,\lim \limits_{n \to \infty} \frac{a_{n+1}}{a_n} = \|f\|_{\infty} f \in L^{\infty},"I have a question that I need help with getting started (possibly I would be back for more help). I have a measure space $(X,A,\mu)$ that is finite, and $f \in L^{\infty}(\mu)$. Also, defined is $a_n = \int_X\,|f|^n\,d\mu$. I need to show that the limit is: $$\lim_{n\to\infty}\,\frac{a_{n+1}}{a_n} = \|f\|_{\infty} .$$ I am stuck on getting started, anybody have any suggestions? thanks much","I have a question that I need help with getting started (possibly I would be back for more help). I have a measure space $(X,A,\mu)$ that is finite, and $f \in L^{\infty}(\mu)$. Also, defined is $a_n = \int_X\,|f|^n\,d\mu$. I need to show that the limit is: $$\lim_{n\to\infty}\,\frac{a_{n+1}}{a_n} = \|f\|_{\infty} .$$ I am stuck on getting started, anybody have any suggestions? thanks much",,"['real-analysis', 'measure-theory']"
35,How to deal with lim sup and lim inf?,How to deal with lim sup and lim inf?,,"I am currently taking first course in real analysis following Ross's Elementary Analysis textbook. When I was introduced to lim sup and lim inf, I found it hard to manage to play around or make meaningful conclusions from them because the terms are not in explicit forms but are in form of suprema and infima of a set. For example, how to start tackling this problem:  if $\lim \sup |a_n| > 0$, prove that $\lim \sup |a_n|^\frac{1}{n} \geq 1$.  What might be the first ideas that I should try thinking about Generally, how ones can approach lim sup and lim inf, starting intuitively and working technically?","I am currently taking first course in real analysis following Ross's Elementary Analysis textbook. When I was introduced to lim sup and lim inf, I found it hard to manage to play around or make meaningful conclusions from them because the terms are not in explicit forms but are in form of suprema and infima of a set. For example, how to start tackling this problem:  if $\lim \sup |a_n| > 0$, prove that $\lim \sup |a_n|^\frac{1}{n} \geq 1$.  What might be the first ideas that I should try thinking about Generally, how ones can approach lim sup and lim inf, starting intuitively and working technically?",,"['real-analysis', 'limits']"
36,Definite integral over a simplex,Definite integral over a simplex,,"Let $T^d:=\{(x_1,..,x_d):x_i \geq 0, \sum_{i=1}^{d}x_i \leq 1\}$ be the standard simplex in $\mathbb{R}^d$. Compute the integral $$\int_{T^d} x_1^{\nu_1-1}x_2^{\nu_2-1}...x_d^{\nu_d-1}(1-x_1-...-x_d)^{\nu_0-1}$$ where $\nu_i>0$. Remark: I know the answer is $$\frac{\prod_{i=0}^{d}\Gamma(\nu_i)}{\Gamma(\sum_{i=0}^{d}\nu_i)}.$$ I evaluated for the case $d=2$ by using the transformation $(p-1)\iiint\limits_{T^{3}} x^{m-1}y^{n-1}z^{p-2} \mathrm{d}z\mathrm{d}y\mathrm{d}x=  \iint\limits_{T^{2}} x^{m-1}y^{n-1}(1-x-y)^{p-1}\mathrm{d}y\mathrm{d}x$ and the substitutions $ \left\{\begin{matrix}x=u^2& &\\y=v^2& &\\z=w^2& &\end{matrix}\right.$and    $ \left\{\begin{matrix}u=r\sin\varphi\cos\theta& &\\v=r\sin\varphi\sin\theta& &\\w=r\cos\varphi& &\end{matrix}\right.,$ but this method is complex for computing the general case.","Let $T^d:=\{(x_1,..,x_d):x_i \geq 0, \sum_{i=1}^{d}x_i \leq 1\}$ be the standard simplex in $\mathbb{R}^d$. Compute the integral $$\int_{T^d} x_1^{\nu_1-1}x_2^{\nu_2-1}...x_d^{\nu_d-1}(1-x_1-...-x_d)^{\nu_0-1}$$ where $\nu_i>0$. Remark: I know the answer is $$\frac{\prod_{i=0}^{d}\Gamma(\nu_i)}{\Gamma(\sum_{i=0}^{d}\nu_i)}.$$ I evaluated for the case $d=2$ by using the transformation $(p-1)\iiint\limits_{T^{3}} x^{m-1}y^{n-1}z^{p-2} \mathrm{d}z\mathrm{d}y\mathrm{d}x=  \iint\limits_{T^{2}} x^{m-1}y^{n-1}(1-x-y)^{p-1}\mathrm{d}y\mathrm{d}x$ and the substitutions $ \left\{\begin{matrix}x=u^2& &\\y=v^2& &\\z=w^2& &\end{matrix}\right.$and    $ \left\{\begin{matrix}u=r\sin\varphi\cos\theta& &\\v=r\sin\varphi\sin\theta& &\\w=r\cos\varphi& &\end{matrix}\right.,$ but this method is complex for computing the general case.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
37,Intuitively understanding Fatou's lemma,Intuitively understanding Fatou's lemma,,I learnt Fatou's lemma a while ago. I am able to prove it and use it. I know examples showing that the inequality may be strict. But I don't really have an intuitive way to understand it. Any good thoughts?,I learnt Fatou's lemma a while ago. I am able to prove it and use it. I know examples showing that the inequality may be strict. But I don't really have an intuitive way to understand it. Any good thoughts?,,"['real-analysis', 'lebesgue-integral', 'intuition']"
38,How to prove a sequence of a function converges uniformly?,How to prove a sequence of a function converges uniformly?,,"For $n \in \mathbb{N}$ , define the formula, $$f_n(x)= \frac{x}{2n^2x^2+8},\quad x \in [0,1].$$ Prove that the sequence $f_n$ converges uniformly on $[0,1]$ , as $n \to \infty$ . I know that the definition says $f_n$ converges uniformly to $f$ if given $\forall \epsilon \gt 0$ , $\forall n \geq N$ , such that $|f_n(x) - f(x)| \lt \epsilon, \forall n \geq N$ and $\forall x \in [0,1].$ I looked first at the pointwise convergence and found that $$\lim_{n \rightarrow \infty} \frac{x}{2n^2x^2+8} = 0, \forall x \in [0,1].$$ So how do I use this to choose an $n \geq N$ such that $|f_n(x) - f(x)| \lt \epsilon$ ? Right now, I have ""proof: Let $\epsilon > 0, \exists N \in \mathbb{N}$ such that, n $\geq N \Rightarrow \frac{1}{2n^2+8} \lt \epsilon$ , by $|f_n(x) - 0| = |\frac{x}{2n^2x^2+8}| \leq |\frac{x^2}{2n^2x^2+8}| \leq \frac{1}{2n^2x^2+8} \;\;\;\; \forall x \in [0,1].$ Since $\lim_{n \rightarrow \infty} \frac{x}{2n^2x^2+8} = 0, \forall x \in [0,1]$ , $f_n(x)$ will converge uniformly to $0$ on $[0,1]$ ."" Is this correct? Am I missing something? Is something not correct? I'm unsure about my choice of $N$ . Please & thanks!","For , define the formula, Prove that the sequence converges uniformly on , as . I know that the definition says converges uniformly to if given , , such that and I looked first at the pointwise convergence and found that So how do I use this to choose an such that ? Right now, I have ""proof: Let such that, n , by Since , will converge uniformly to on ."" Is this correct? Am I missing something? Is something not correct? I'm unsure about my choice of . Please & thanks!","n \in \mathbb{N} f_n(x)= \frac{x}{2n^2x^2+8},\quad x \in [0,1]. f_n [0,1] n \to \infty f_n f \forall \epsilon \gt 0 \forall n \geq N |f_n(x) - f(x)| \lt \epsilon, \forall n \geq N \forall x \in [0,1]. \lim_{n \rightarrow \infty} \frac{x}{2n^2x^2+8} = 0, \forall x \in [0,1]. n \geq N |f_n(x) - f(x)| \lt \epsilon \epsilon > 0, \exists N \in \mathbb{N} \geq N \Rightarrow \frac{1}{2n^2+8} \lt \epsilon |f_n(x) - 0| = |\frac{x}{2n^2x^2+8}| \leq |\frac{x^2}{2n^2x^2+8}| \leq \frac{1}{2n^2x^2+8} \;\;\;\; \forall x \in [0,1]. \lim_{n \rightarrow \infty} \frac{x}{2n^2x^2+8} = 0, \forall x \in [0,1] f_n(x) 0 [0,1] N","['real-analysis', 'uniform-convergence']"
39,"Find the limit points of the set $\{ \frac{1}{n} +\frac{1}{m} \mid n , m = 1,2,3,\dots \}$",Find the limit points of the set,"\{ \frac{1}{n} +\frac{1}{m} \mid n , m = 1,2,3,\dots \}","I need to find limit points of the set $\{  \frac{1}{n} +\frac{1}{m} \mid n, m = 1,2,3,\dots \}$. My try : If both $m$ and $n$ tend to very large values say $\infty$ then the value of $\{ \frac{1}{n} +\frac{1}{m} \}$ tends to $0$, and if only one of $m$ or $n$ tends to very large values, then the set $\{ \frac{1}{n} \mid n=1,2,3,\dots\} $ acts as limit points. So is it true that a set of limit points is $\{0\}$ $\cup$  $\{\frac{1}{k} \mid k=1,2,3,\dots \}$? How should I write this proof rigorously?","I need to find limit points of the set $\{  \frac{1}{n} +\frac{1}{m} \mid n, m = 1,2,3,\dots \}$. My try : If both $m$ and $n$ tend to very large values say $\infty$ then the value of $\{ \frac{1}{n} +\frac{1}{m} \}$ tends to $0$, and if only one of $m$ or $n$ tends to very large values, then the set $\{ \frac{1}{n} \mid n=1,2,3,\dots\} $ acts as limit points. So is it true that a set of limit points is $\{0\}$ $\cup$  $\{\frac{1}{k} \mid k=1,2,3,\dots \}$? How should I write this proof rigorously?",,['real-analysis']
40,Evaluate $\lim_{x\to 0}\frac {(\cos(x))^{\sin(x)} - \sqrt{1 - x^3}}{x^6}$,Evaluate,\lim_{x\to 0}\frac {(\cos(x))^{\sin(x)} - \sqrt{1 - x^3}}{x^6},"Evaluate $$ \displaystyle \lim_{x\to 0}\Bigg( \frac {(\cos(x))^{\sin(x)} - \sqrt{1 - x^3}}{x^6}\Bigg).$$ I tried to use L'Hopital's rule but it got very messy. Moreover I also tried to analyze from graphs, but I was getting the limit $= 0$ by observing it. However, the answer given in my book is $\frac{1}{4}$. Is there any method to do without Taylor series and L' Hopital's rule (like using special limits). We are given that the limit exists. Any help will be appreciated. Thanks!","Evaluate $$ \displaystyle \lim_{x\to 0}\Bigg( \frac {(\cos(x))^{\sin(x)} - \sqrt{1 - x^3}}{x^6}\Bigg).$$ I tried to use L'Hopital's rule but it got very messy. Moreover I also tried to analyze from graphs, but I was getting the limit $= 0$ by observing it. However, the answer given in my book is $\frac{1}{4}$. Is there any method to do without Taylor series and L' Hopital's rule (like using special limits). We are given that the limit exists. Any help will be appreciated. Thanks!",,"['calculus', 'real-analysis', 'algebra-precalculus', 'limits', 'limits-without-lhopital']"
41,A log improper integral,A log improper integral,,Evaluate : $$\int_0^{\frac{\pi}{2}}\ln ^2\left(\cos ^2x\right)\text{d}x$$ I found it can be simplified to $$\int_0^{\frac{\pi}{2}}4\ln ^2\left(\cos x\right)\text{d}x$$ I found the exact value in the table of integrals: $$2\pi\left(\ln ^22+\frac{\pi ^2}{12}\right)$$ Anyone knows how to evaluate this?,Evaluate : $$\int_0^{\frac{\pi}{2}}\ln ^2\left(\cos ^2x\right)\text{d}x$$ I found it can be simplified to $$\int_0^{\frac{\pi}{2}}4\ln ^2\left(\cos x\right)\text{d}x$$ I found the exact value in the table of integrals: $$2\pi\left(\ln ^22+\frac{\pi ^2}{12}\right)$$ Anyone knows how to evaluate this?,,"['calculus', 'real-analysis', 'sequences-and-series', 'integration', 'fourier-analysis']"
42,Is there a theorem in Real analysis similar to Cauchy's theorem in Complex analysis?,Is there a theorem in Real analysis similar to Cauchy's theorem in Complex analysis?,,"Is there a theorem in Real Analysis similar to Cauchy's Theorem/Cauchy's Integral Formula from Complex Analysis? If not, then why? What is it about the complex space that makes Cauchy's Theorem true?","Is there a theorem in Real Analysis similar to Cauchy's Theorem/Cauchy's Integral Formula from Complex Analysis? If not, then why? What is it about the complex space that makes Cauchy's Theorem true?",,"['real-analysis', 'calculus']"
43,Prove the existence of limit of $x_{n+1}=x_n+\frac{x_n^2}{n^2}$,Prove the existence of limit of,x_{n+1}=x_n+\frac{x_n^2}{n^2},"The problem is: Let $\{x_n\}$ be a sequence such that $0<x_1<1$ and $x_{n+1}=x_n+\dfrac{x_n^2}{n^2}$. Prove that there exists the limit of $\{x_n\}$. It is easy to show that $x_n$ is increasing, but I cannot prove it is bounded to show the existing of limit. Anyone have any ideas?","The problem is: Let $\{x_n\}$ be a sequence such that $0<x_1<1$ and $x_{n+1}=x_n+\dfrac{x_n^2}{n^2}$. Prove that there exists the limit of $\{x_n\}$. It is easy to show that $x_n$ is increasing, but I cannot prove it is bounded to show the existing of limit. Anyone have any ideas?",,"['real-analysis', 'limits', 'recurrence-relations']"
44,"Can we add an uncountable number of positive elements, and can this sum be finite?","Can we add an uncountable number of positive elements, and can this sum be finite?",,"Can we add an uncountable number of positive elements, and can this sum be finite? I always have trouble understanding mathematical operations when dealing with an uncountable number of elements. Any help would be great.","Can we add an uncountable number of positive elements, and can this sum be finite? I always have trouble understanding mathematical operations when dealing with an uncountable number of elements. Any help would be great.",,['real-analysis']
45,The limit of the derivative of an increasing and bounded function is always $0$?,The limit of the derivative of an increasing and bounded function is always ?,0,"Let $\,f : \mathbb{R} \rightarrow \mathbb{R}$ be a infinitely differentiable function that is  increasing and bounded. Then is it true that $\lim_{x\to \infty}f'(x)=0$?","Let $\,f : \mathbb{R} \rightarrow \mathbb{R}$ be a infinitely differentiable function that is  increasing and bounded. Then is it true that $\lim_{x\to \infty}f'(x)=0$?",,"['calculus', 'real-analysis']"
46,Please explain inequality $|x^{p}-y^{p}| \leq |x-y|^p$,Please explain inequality,|x^{p}-y^{p}| \leq |x-y|^p,"Suppose $x \geq 0$, $y \geq 0$ and $0<p<1$. Why is the following inequality true? $|x^{p}-y^{p}| \leq |x-y|^p$","Suppose $x \geq 0$, $y \geq 0$ and $0<p<1$. Why is the following inequality true? $|x^{p}-y^{p}| \leq |x-y|^p$",,['real-analysis']
47,"Finite dimensional subspace of $C([0,1])$",Finite dimensional subspace of,"C([0,1])","Let linear $S$ be a subspace of $C([0,1])$, i.e., the continuous real-valued   functions on $[0,1]$. Assume that there exists $c>0$, such that   $\|\,f\|_\infty\leq c \|\,f\|_2$, for all $f\in S$. Then show that $S$ is finite-dimensional. This is equivalent to proving that the closed unit ball in $(S,\|\|_\infty)$ or in $(S,\|\|_2)$ is compact, but I can't derive this. Another thought is that the $L^2$ closure of $S$ is a subset of $C([0,1])$. Indeed, if $f_n\to f\in L^2$, then the given relation $\|f_n-f\|_\infty \leq c\|f_n-f\|_2$ implies that $f_n\to f$ in $L^\infty$, so $f$ is continuous. Therefore we have the inclusion $$S\subset \overline{S}^{L^2}\subset C([0,1])\subset L^2$$ If $S$ was infinite dimensional, then $\overline{S}$ would be an infinite dimensional hilbert space, proper subset of $L^2$. Another thought is that all the $L^p$ norms on $S$ are equivalent (by Holder). If ALL norms are equivalent, then $S$ has to be finite dimensional.","Let linear $S$ be a subspace of $C([0,1])$, i.e., the continuous real-valued   functions on $[0,1]$. Assume that there exists $c>0$, such that   $\|\,f\|_\infty\leq c \|\,f\|_2$, for all $f\in S$. Then show that $S$ is finite-dimensional. This is equivalent to proving that the closed unit ball in $(S,\|\|_\infty)$ or in $(S,\|\|_2)$ is compact, but I can't derive this. Another thought is that the $L^2$ closure of $S$ is a subset of $C([0,1])$. Indeed, if $f_n\to f\in L^2$, then the given relation $\|f_n-f\|_\infty \leq c\|f_n-f\|_2$ implies that $f_n\to f$ in $L^\infty$, so $f$ is continuous. Therefore we have the inclusion $$S\subset \overline{S}^{L^2}\subset C([0,1])\subset L^2$$ If $S$ was infinite dimensional, then $\overline{S}$ would be an infinite dimensional hilbert space, proper subset of $L^2$. Another thought is that all the $L^p$ norms on $S$ are equivalent (by Holder). If ALL norms are equivalent, then $S$ has to be finite dimensional.",,"['real-analysis', 'analysis', 'functional-analysis', 'vector-spaces', 'normed-spaces']"
48,Happy $\pi$-day! Is it true that $\sum_{p \;\text{prime} } \frac{1}{{\pi}^p} < \pi -\lfloor \pi \rfloor$?,Happy -day! Is it true that ?,\pi \sum_{p \;\text{prime} } \frac{1}{{\pi}^p} < \pi -\lfloor \pi \rfloor,"Today is a $\pi$ -day and I made this exercise for that purpose (and not only for that!): Let: $$\phi = \sum_{p \;\text{prime} } \frac{1}{{\pi}^p}$$ By applying only knowledge of calculus and, more generally (if needed), real analysis of functions of one variable, and without computational software, determine is it true that we have: $$\phi< \pi - \lfloor\pi\rfloor$$ Where $\lfloor\pi\rfloor=3$ is the floor function of $\pi$ . Is this possible to solve with, for example, some of the formulas for infinite product for $\pi$ or Taylor series for ${\sin}^{-1}$ , without any numerical estimates? Or, if estimates are needed, what is the worst one you need to apply to solve this?","Today is a -day and I made this exercise for that purpose (and not only for that!): Let: By applying only knowledge of calculus and, more generally (if needed), real analysis of functions of one variable, and without computational software, determine is it true that we have: Where is the floor function of . Is this possible to solve with, for example, some of the formulas for infinite product for or Taylor series for , without any numerical estimates? Or, if estimates are needed, what is the worst one you need to apply to solve this?",\pi \phi = \sum_{p \;\text{prime} } \frac{1}{{\pi}^p} \phi< \pi - \lfloor\pi\rfloor \lfloor\pi\rfloor=3 \pi \pi {\sin}^{-1},"['real-analysis', 'calculus']"
49,"Is $f(x)=x\sin(\frac{1}{x})$ with $f(0)=0$ of bounded variation on $[0,1]$?",Is  with  of bounded variation on ?,"f(x)=x\sin(\frac{1}{x}) f(0)=0 [0,1]","I can't figure out whether $f(x)=x\sin(1/x)$ with  $f(0)=0$ is of bounded variation on $[0,1]$ or not. But I think it is not. Can someone suggest a partition to prove it is not of bounded variation is so? Thanks","I can't figure out whether $f(x)=x\sin(1/x)$ with  $f(0)=0$ is of bounded variation on $[0,1]$ or not. But I think it is not. Can someone suggest a partition to prove it is not of bounded variation is so? Thanks",,['real-analysis']
50,Proving that a sequence such that $|a_{n+1} - a_n| \le 2^{-n}$ is Cauchy,Proving that a sequence such that  is Cauchy,|a_{n+1} - a_n| \le 2^{-n},"Suppose the terms of the sequence of real numbers $\{a_n\}$ satisfy $|a_{n+1} - a_n| \le 2^{-n}$ for all $n$. Prove that $\{a_n\}$ is Cauchy. My Work So by the definition of a Cauchy sequence, for all $\varepsilon > 0$ $\exists N$ so that for $n,m \ge N$ we have $|a_m - a_n| \le \varepsilon$. However, questions like this one make me understand that the $2^{-n}$ condition is necessary for this to be a true statement. So I am wondering how to appeal to the Cauchy definition for this proof. Do I prove that every convergent sequence is therefore Cauchy, and then try to prove convergence?","Suppose the terms of the sequence of real numbers $\{a_n\}$ satisfy $|a_{n+1} - a_n| \le 2^{-n}$ for all $n$. Prove that $\{a_n\}$ is Cauchy. My Work So by the definition of a Cauchy sequence, for all $\varepsilon > 0$ $\exists N$ so that for $n,m \ge N$ we have $|a_m - a_n| \le \varepsilon$. However, questions like this one make me understand that the $2^{-n}$ condition is necessary for this to be a true statement. So I am wondering how to appeal to the Cauchy definition for this proof. Do I prove that every convergent sequence is therefore Cauchy, and then try to prove convergence?",,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
51,Prove using Rolle's Theorem that an equation has exactly one real solution.,Prove using Rolle's Theorem that an equation has exactly one real solution.,,"Prove that the equation $x^7+x^5+x^3+1=0$ has exactly one real solution. You should use Rolles Theorem at some point in the proof. Since $f(x) = x^7+x^5+x^3+1$ is a polynomial then it is continuous over all the real numbers, $(-\infty,\infty)$. Since $f(x)$ is continuous on $(-\infty,\infty)$ then it is certainly continuous on $[-1,0]$ and the Intermediate Value Theorem (IVT) applies. The IVT states that since $f(x)$ is continuous on $[-1,0]$ we can let $C$ be any number between $f(-1)=-2$ and $f(0)=1$, namely $C=0$, then there exists a number $c$ with $-1 < c < 0$ such that $f(c)=C=0$. Since $f(x)$ is continuous on $[-1,0]$ and differentiable on $(-1,0)$ then Rolles Theorem applies. Rolles Theorem states that if a function $f\colon [a,b] \to \mathbf{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$ then if $f(a)=f(b)$, there exists a point $c \in (a,b)$ such that $f'(c)=0$. We assume that there is more than one real solution for this equation, namely $f(a)=0=f(b)$. If there exists more than one real solution for $f(x)=0$ then $f(a)=0=f(b)\implies a=b$, and thus there is only one real solution to the equation, as desired. But I feel  I am missing something, and I'm not sure how to show that the equation is differentiable on the interval and that Rolle's Theorem applies.","Prove that the equation $x^7+x^5+x^3+1=0$ has exactly one real solution. You should use Rolles Theorem at some point in the proof. Since $f(x) = x^7+x^5+x^3+1$ is a polynomial then it is continuous over all the real numbers, $(-\infty,\infty)$. Since $f(x)$ is continuous on $(-\infty,\infty)$ then it is certainly continuous on $[-1,0]$ and the Intermediate Value Theorem (IVT) applies. The IVT states that since $f(x)$ is continuous on $[-1,0]$ we can let $C$ be any number between $f(-1)=-2$ and $f(0)=1$, namely $C=0$, then there exists a number $c$ with $-1 < c < 0$ such that $f(c)=C=0$. Since $f(x)$ is continuous on $[-1,0]$ and differentiable on $(-1,0)$ then Rolles Theorem applies. Rolles Theorem states that if a function $f\colon [a,b] \to \mathbf{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$ then if $f(a)=f(b)$, there exists a point $c \in (a,b)$ such that $f'(c)=0$. We assume that there is more than one real solution for this equation, namely $f(a)=0=f(b)$. If there exists more than one real solution for $f(x)=0$ then $f(a)=0=f(b)\implies a=b$, and thus there is only one real solution to the equation, as desired. But I feel  I am missing something, and I'm not sure how to show that the equation is differentiable on the interval and that Rolle's Theorem applies.",,"['real-analysis', 'derivatives', 'roots', 'rolles-theorem']"
52,Can every real number be represented by a (possibly infinite) decimal?,Can every real number be represented by a (possibly infinite) decimal?,,"Does every real number have a representation within our decimal system? The reason I ask is because, from beginning a mathematics undergraduate degree a lot of 'mathematical facts' I had previously assumed have been consistently modified, or altogether stripped away. I'm wondering if my subconscious assumption that every real number can be represented in such a way is in fact incorrect? If so, is there a proof? If not, why not? (Also I'm not quite sure how to tag this question?)","Does every real number have a representation within our decimal system? The reason I ask is because, from beginning a mathematics undergraduate degree a lot of 'mathematical facts' I had previously assumed have been consistently modified, or altogether stripped away. I'm wondering if my subconscious assumption that every real number can be represented in such a way is in fact incorrect? If so, is there a proof? If not, why not? (Also I'm not quite sure how to tag this question?)",,"['real-analysis', 'number-systems']"
53,Essentially bounded function on $\mathbb{R}$,Essentially bounded function on,\mathbb{R},"I don't really know a lot about measure (just finishing my undergrad) so I'm not really on good terms with this. So, let $L^\infty[a,b]$ denote the space of all essentially bounded functions on $[a,b]$ with the norm $\left\| f \right\|_\infty = \operatorname{ess} \sup_{x\in[a,b]} |f(x)|$ . What would exactly be the difference between a bounded and an essentially bounded function (also, the difference between the supremum and essential supremum)? If I'm not actually dealing with measures, but just with real variable functions $f:[a,b]\to\mathbb{R}$ , can I omit the ""essentially"" part and just say that $L^\infty[a,b]$ denotes the space of bounded functions, and, additionally, omit the $\text{ess}$ in the definition of the norm? I understand that this seems like a silly question, but I've tried Googleing this and found nothing useful.","I don't really know a lot about measure (just finishing my undergrad) so I'm not really on good terms with this. So, let denote the space of all essentially bounded functions on with the norm . What would exactly be the difference between a bounded and an essentially bounded function (also, the difference between the supremum and essential supremum)? If I'm not actually dealing with measures, but just with real variable functions , can I omit the ""essentially"" part and just say that denotes the space of bounded functions, and, additionally, omit the in the definition of the norm? I understand that this seems like a silly question, but I've tried Googleing this and found nothing useful.","L^\infty[a,b] [a,b] \left\| f \right\|_\infty = \operatorname{ess} \sup_{x\in[a,b]} |f(x)| f:[a,b]\to\mathbb{R} L^\infty[a,b] \text{ess}","['real-analysis', 'measure-theory']"
54,"Connectivity, Path Connectivity and Differentiability","Connectivity, Path Connectivity and Differentiability",,"I have two questions which pertain to differentiability, connectivity and path connectivity. Ocasionally, I will encounter an author who defines connectivity in the following way: An open subset $U$ of $\mathbb{R}^n$ is said to be connected if and only if given two points $a$ and $b$ of $U$ there exists a differentiable mapping $\phi: \mathbb{R} \rightarrow U$ such that $\phi(0) = a$ and $\phi(1) = b$. This particular example is from Edward's Advanced Calculus of Several Variables p 84. Now, this is obviously not the standard definition we learn from topology which has nothing to do with differentiability but rather whether there exists two nonempty open subsets that comprise a separation. It also seems to me that what the author is really defining what it means for a space to be ""smoothly path connected"", which of course implies connectivity and, it seems to me, considerably more. My first question is: Is ""smoothly path connected"", as defined above actually equivalent to ""connected"", in the topological sense, in $\mathbb{R}^n$? Next, in Vector Calculus by Baxandall and Liebeck on p 150 the authors state the existence of a continuous path $\alpha$ from the closed interval $[0,1]$ to an open subset $D$ of $\mathbb{R}^n$ where $\alpha (0) = a \in D$ and $\alpha(1) = b \in D$ guarantees the existence of a differentiable path with the same properties. This claim is stated without proof. My second question is: Can someone provide a reference to a proof of the above claim or explain why it is so?","I have two questions which pertain to differentiability, connectivity and path connectivity. Ocasionally, I will encounter an author who defines connectivity in the following way: An open subset $U$ of $\mathbb{R}^n$ is said to be connected if and only if given two points $a$ and $b$ of $U$ there exists a differentiable mapping $\phi: \mathbb{R} \rightarrow U$ such that $\phi(0) = a$ and $\phi(1) = b$. This particular example is from Edward's Advanced Calculus of Several Variables p 84. Now, this is obviously not the standard definition we learn from topology which has nothing to do with differentiability but rather whether there exists two nonempty open subsets that comprise a separation. It also seems to me that what the author is really defining what it means for a space to be ""smoothly path connected"", which of course implies connectivity and, it seems to me, considerably more. My first question is: Is ""smoothly path connected"", as defined above actually equivalent to ""connected"", in the topological sense, in $\mathbb{R}^n$? Next, in Vector Calculus by Baxandall and Liebeck on p 150 the authors state the existence of a continuous path $\alpha$ from the closed interval $[0,1]$ to an open subset $D$ of $\mathbb{R}^n$ where $\alpha (0) = a \in D$ and $\alpha(1) = b \in D$ guarantees the existence of a differentiable path with the same properties. This claim is stated without proof. My second question is: Can someone provide a reference to a proof of the above claim or explain why it is so?",,"['real-analysis', 'general-topology', 'multivariable-calculus']"
55,Improved intermediate value theorem,Improved intermediate value theorem,,"Suppose $f\colon [a,b] \to \mathbb{R}$ is a continuous function with $f(a)<0$, $f(b)>0$. Can it be proved that there exists $s_1\leq s_2$ and $\epsilon>0$  such that $f(s)=0$ for all $s\in[s_1,s_2]$, whilst $f(s)<0$ for all $s\in [s_1-\epsilon, s_1)$ and $f(s)>0$ for all $s\in (s_2,s_2+\epsilon]$? If not, what about in the case that one assumes $f$ is $C^1$, or smooth?","Suppose $f\colon [a,b] \to \mathbb{R}$ is a continuous function with $f(a)<0$, $f(b)>0$. Can it be proved that there exists $s_1\leq s_2$ and $\epsilon>0$  such that $f(s)=0$ for all $s\in[s_1,s_2]$, whilst $f(s)<0$ for all $s\in [s_1-\epsilon, s_1)$ and $f(s)>0$ for all $s\in (s_2,s_2+\epsilon]$? If not, what about in the case that one assumes $f$ is $C^1$, or smooth?",,"['real-analysis', 'general-topology']"
56,"If $\sum a_n$ converges and $b_n=\sum\limits_{k=n}^{\infty}a_k $, prove that $\sum \frac{a_n}{b_n}$ diverges","If  converges and , prove that  diverges",\sum a_n b_n=\sum\limits_{k=n}^{\infty}a_k  \sum \frac{a_n}{b_n},"Let $\displaystyle \sum a_n$ be  convergent series of positive terms and set $\displaystyle b_n=\sum_{k=n}^{\infty}a_k$ , then prove that $\displaystyle\sum \frac{a_n}{b_n}$ diverges. I could see that $\{b_n\}$ is monotonically decreasing sequence converging to $0$ and I can write $\displaystyle\sum \frac{a_n}{b_n}=\sum\frac{b_n-b_{n+1}}{b_n}$ , how shall I proceed further?","Let be  convergent series of positive terms and set , then prove that diverges. I could see that is monotonically decreasing sequence converging to and I can write , how shall I proceed further?",\displaystyle \sum a_n \displaystyle b_n=\sum_{k=n}^{\infty}a_k \displaystyle\sum \frac{a_n}{b_n} \{b_n\} 0 \displaystyle\sum \frac{a_n}{b_n}=\sum\frac{b_n-b_{n+1}}{b_n},"['real-analysis', 'sequences-and-series', 'analysis', 'divergent-series']"
57,The Biharmonic Eigenvalue Problem on a Rectangle with Dirichlet Boundary Conditions,The Biharmonic Eigenvalue Problem on a Rectangle with Dirichlet Boundary Conditions,,"I am interested in solving the following biharmonic eigenvalue problem. $$\begin{array}{cccc}   & \Delta ^2  \Psi (x,y) = \lambda \Psi (x,y), & - a \le x \le a &  - b \le y \le b \\   & x = \phantom{-}a & \Psi = 0 &   \dfrac{\partial \Psi }{\partial x} = 0 \\   & x =  - a & \Psi = 0 &  \dfrac{\partial  \Psi }{\partial x} = 0 \\   & y = \phantom{-}b & \Psi = 0 &  \dfrac{\partial  \Psi }{ \partial y} = 0 \\  & y =  - b & \Psi = 0 &  \dfrac{\partial  \Psi }{ \partial y} = 0  \end{array} $$ where $$ \Delta ^2 \Psi  = \frac{\partial ^4 \Psi }{\partial x^4} + 2 \frac{\partial^4 \Psi }{\partial x^2 \partial y^2} + \frac{\partial ^4 \Psi }{\partial y^4}$$ $$\Psi  \in {{\bf{C}}^{\infty}}\left( {[ - a,a] \times [ - b,b]} \right)$$ To describe the problem in words, we are looking for the eigenfunctions of the biharmonic operator over a rectangular domain where all its derivatives are continuous. The boundary conditions are of Dirichlet type, i.e., the function and it's normal derivative are prescribed over the boundary of the rectangular domain. Facts and Motivations This problem occurs in many physical areas. One of the most famous ones is the vibration of a rectangular isotropic elastic clamp plate. It is believed among engineers that the problem doesn't have a closed form solution. It may be asked that even the problem has a solution or not. Numerical evidence shows that such a solution may exists. However, I am looking for some strong theoretical basis to prove the existence of the solution so I planned to ask this question in a society of mathematicians. After the existence is verified, one is definitely interested in looking for methods to compute these eigen-functions. Questions Is there any non-zero solution for this problem? In other words, I am asking an existence or non-existence theorem for this problem. This question is completely answered by TKS. According to TKS, it is an old result firstly proved by K. Friedrichs . Maybe the reason that many people are unaware of this is that the paper by K. Friedrichs is written in German entitled as Die Randwert- und Eigenwertprobleme aus der Theorie der elastischen Platten. (Anwendung der direkten Methoden der Variationsrechnung) The translation in English is The boundary value and eigenvalue problems in the theory of elastic plates. (Application of direct methods of variational calculus) Another short answer to this question is given by Jean Duchon on Math Over Flow . Assuming the existence, how can one compute these eigenvalues and eigenfunctions? Is there a closed form solution for this purpose? This question remained unanswered !","I am interested in solving the following biharmonic eigenvalue problem. where To describe the problem in words, we are looking for the eigenfunctions of the biharmonic operator over a rectangular domain where all its derivatives are continuous. The boundary conditions are of Dirichlet type, i.e., the function and it's normal derivative are prescribed over the boundary of the rectangular domain. Facts and Motivations This problem occurs in many physical areas. One of the most famous ones is the vibration of a rectangular isotropic elastic clamp plate. It is believed among engineers that the problem doesn't have a closed form solution. It may be asked that even the problem has a solution or not. Numerical evidence shows that such a solution may exists. However, I am looking for some strong theoretical basis to prove the existence of the solution so I planned to ask this question in a society of mathematicians. After the existence is verified, one is definitely interested in looking for methods to compute these eigen-functions. Questions Is there any non-zero solution for this problem? In other words, I am asking an existence or non-existence theorem for this problem. This question is completely answered by TKS. According to TKS, it is an old result firstly proved by K. Friedrichs . Maybe the reason that many people are unaware of this is that the paper by K. Friedrichs is written in German entitled as Die Randwert- und Eigenwertprobleme aus der Theorie der elastischen Platten. (Anwendung der direkten Methoden der Variationsrechnung) The translation in English is The boundary value and eigenvalue problems in the theory of elastic plates. (Application of direct methods of variational calculus) Another short answer to this question is given by Jean Duchon on Math Over Flow . Assuming the existence, how can one compute these eigenvalues and eigenfunctions? Is there a closed form solution for this purpose? This question remained unanswered !","\begin{array}{cccc}
  & \Delta ^2  \Psi (x,y) = \lambda \Psi (x,y), & - a \le x \le a &  - b \le y \le b \\
  & x = \phantom{-}a & \Psi = 0 &   \dfrac{\partial \Psi }{\partial x} = 0 \\
  & x =  - a & \Psi = 0 &  \dfrac{\partial  \Psi }{\partial x} = 0 \\
  & y = \phantom{-}b & \Psi = 0 &  \dfrac{\partial  \Psi }{ \partial y} = 0 \\
 & y =  - b & \Psi = 0 &  \dfrac{\partial  \Psi }{ \partial y} = 0 
\end{array}
  \Delta ^2 \Psi  = \frac{\partial ^4 \Psi }{\partial x^4} + 2 \frac{\partial^4 \Psi }{\partial x^2 \partial y^2} + \frac{\partial ^4 \Psi }{\partial y^4} \Psi  \in {{\bf{C}}^{\infty}}\left( {[ - a,a] \times [ - b,b]} \right)","['real-analysis', 'analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
58,How do I prove that a function with a finite number of discontinuities is Riemann integrable over some interval?,How do I prove that a function with a finite number of discontinuities is Riemann integrable over some interval?,,"First, I want to figure out why, for the simple case where $ g:[a,b]\rightarrow\mathbb{R} $ is bounded and continuous except at some point $ x_0\in[a,b], $ $ g$ is Riemann integrable on [a,b]. I know the Riemann integrability condition that there must exist some partition $ P $ for which $ U(P,f) - L(P,f)\leq\epsilon, \forall \epsilon>0 $. For my attempt at a proof, I said: Let $ P=\{x_0,x_1,\cdots,x_n:x_0=a<x_1<\cdots<x_n=b\} $ be a partition of $ [a,b] $. Let $ x_0\in[x_{k-1},x_k]. $ Then normally for an everywhere continuous function, since $ [a,b] $ is a compact set, the function is uniformly continuous and therefore $ \exists\delta(\epsilon):|x-y|<\delta\implies|f(x)-f(y)|<\epsilon,\forall\epsilon>0 $.  Using this, we can make any of the subintervals in the partition arbitrarily small, and eventually make the difference between the upper and lower sums arbitrarily small by using the property that a continuous function will attain its maximum and minimum.  But how do I do something similar for $ g $?  Furthermore, how can I extend that to the case in which $ g $ is discontinuous at $ x_0, x_1, \cdots, x_n $? EDIT: Using ncmathsadist's advice, Let $\epsilon > 0$.  Let $M = \sup_{x\in[a,b]} f$. Let $ D=\{x_0,x_1,\cdots,x_{2n}:x_0<x_1<\cdots<x_{2n}\} $ be a subpartition containing all the points $ x_0,x_1,\cdots,x_n $ where $ g $ is discontinuous, such that $ \sum_{i=1}^{2n}(x_i-x_{i-1}) < \frac{\epsilon}{2M}$ Let $ C $ be a subpartition containing all the other points.  By visiting the proof that a continuous function is Riemann integrable, I can construct a $ C $ so that: $ U(C\bigcup D,f)-L(C\bigcup D,f)< \frac{\epsilon}{2M}\times M+\frac{\epsilon}{2}=\epsilon $ This is because $ g $ is bounded, and any contribution by $ g $ to the sum from the discontinuous point must be less than the maximum, $ M $.","First, I want to figure out why, for the simple case where $ g:[a,b]\rightarrow\mathbb{R} $ is bounded and continuous except at some point $ x_0\in[a,b], $ $ g$ is Riemann integrable on [a,b]. I know the Riemann integrability condition that there must exist some partition $ P $ for which $ U(P,f) - L(P,f)\leq\epsilon, \forall \epsilon>0 $. For my attempt at a proof, I said: Let $ P=\{x_0,x_1,\cdots,x_n:x_0=a<x_1<\cdots<x_n=b\} $ be a partition of $ [a,b] $. Let $ x_0\in[x_{k-1},x_k]. $ Then normally for an everywhere continuous function, since $ [a,b] $ is a compact set, the function is uniformly continuous and therefore $ \exists\delta(\epsilon):|x-y|<\delta\implies|f(x)-f(y)|<\epsilon,\forall\epsilon>0 $.  Using this, we can make any of the subintervals in the partition arbitrarily small, and eventually make the difference between the upper and lower sums arbitrarily small by using the property that a continuous function will attain its maximum and minimum.  But how do I do something similar for $ g $?  Furthermore, how can I extend that to the case in which $ g $ is discontinuous at $ x_0, x_1, \cdots, x_n $? EDIT: Using ncmathsadist's advice, Let $\epsilon > 0$.  Let $M = \sup_{x\in[a,b]} f$. Let $ D=\{x_0,x_1,\cdots,x_{2n}:x_0<x_1<\cdots<x_{2n}\} $ be a subpartition containing all the points $ x_0,x_1,\cdots,x_n $ where $ g $ is discontinuous, such that $ \sum_{i=1}^{2n}(x_i-x_{i-1}) < \frac{\epsilon}{2M}$ Let $ C $ be a subpartition containing all the other points.  By visiting the proof that a continuous function is Riemann integrable, I can construct a $ C $ so that: $ U(C\bigcup D,f)-L(C\bigcup D,f)< \frac{\epsilon}{2M}\times M+\frac{\epsilon}{2}=\epsilon $ This is because $ g $ is bounded, and any contribution by $ g $ to the sum from the discontinuous point must be less than the maximum, $ M $.",,"['calculus', 'real-analysis', 'definite-integrals']"
59,"For an irrational number $a$ the fractional part of $na$ for $n\in\mathbb N$ is dense in $[0,1]$ [duplicate]",For an irrational number  the fractional part of  for  is dense in  [duplicate],"a na n\in\mathbb N [0,1]","This question already has answers here : For $x\in\mathbb R\setminus\mathbb Q$, the set $\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$ is dense on $[0,1)$ (5 answers) Closed 9 years ago . How to prove that the $\{$ fractional part of $n\alpha\mid n \in \mathbb{N}$ $\}$ is dense in $[0,1]$ for an irrational number $\alpha$. NOTICE that $n$ is in $\mathbb{N}$ Also notice that this is not a duplicate of the mentioned question as it does not carry a correct answer and the partially correct answer in the said question is given for integer multiple case, NOT for $n \in \mathbb{N}$","This question already has answers here : For $x\in\mathbb R\setminus\mathbb Q$, the set $\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$ is dense on $[0,1)$ (5 answers) Closed 9 years ago . How to prove that the $\{$ fractional part of $n\alpha\mid n \in \mathbb{N}$ $\}$ is dense in $[0,1]$ for an irrational number $\alpha$. NOTICE that $n$ is in $\mathbb{N}$ Also notice that this is not a duplicate of the mentioned question as it does not carry a correct answer and the partially correct answer in the said question is given for integer multiple case, NOT for $n \in \mathbb{N}$",,"['real-analysis', 'sequences-and-series', 'general-topology', 'irrational-numbers']"
60,Intuition: If $a\leq b+\epsilon$ for all $\epsilon>0$ then $a\leq b$?,Intuition: If  for all  then ?,a\leq b+\epsilon \epsilon>0 a\leq b,I am reading Tom Apostol's Analysis and come across this theorem. Should $a \leq b$ if $a\leq b+\epsilon$ for all $\epsilon >0$? I don't doubt the proof in the book but I don't understand the intuition or geometric explanation behind this. Could somebody shed some light on this equation? I just started studying analysis on my own.$\ \ $,I am reading Tom Apostol's Analysis and come across this theorem. Should $a \leq b$ if $a\leq b+\epsilon$ for all $\epsilon >0$? I don't doubt the proof in the book but I don't understand the intuition or geometric explanation behind this. Could somebody shed some light on this equation? I just started studying analysis on my own.$\ \ $,,"['real-analysis', 'inequality', 'intuition']"
61,Convexity and equality in Jensen inequality,Convexity and equality in Jensen inequality,,"Theorem 3.3 from W. Rudin, Real and complex analysis, says: Let $\mu$ be a probabilistic measure on a $\sigma$ -algebra of subsets of a given set $\Omega$ . If a function $f:X \rightarrow \mathbb R$ is in $L^1(\mu)$ , $a<f(x)<b$ for $x\in \Omega$ and if a function $\phi:(a,b)\rightarrow \mathbb R$ is a convex  then $$ \phi\left(\int_a^b fd \mu\right) \leq \int_a^b (\phi \circ f) d \mu. $$ Is it known when in  this inequality holds equality? Maybe, it is iff $\phi$ is affine a.e. ?","Theorem 3.3 from W. Rudin, Real and complex analysis, says: Let be a probabilistic measure on a -algebra of subsets of a given set . If a function is in , for and if a function is a convex  then Is it known when in  this inequality holds equality? Maybe, it is iff is affine a.e. ?","\mu \sigma \Omega f:X \rightarrow \mathbb R L^1(\mu) a<f(x)<b x\in \Omega \phi:(a,b)\rightarrow \mathbb R 
\phi\left(\int_a^b fd \mu\right) \leq \int_a^b (\phi \circ f) d \mu.
 \phi","['real-analysis', 'convex-analysis']"
62,mystery regarding power series of $\frac{1}{\sqrt{1+x^{x}}}$,mystery regarding power series of,\frac{1}{\sqrt{1+x^{x}}},"In the course of playing around with $\sum_{n=1}^{\infty} \frac{1}{\sqrt{1+n^{n}}}$ , I used w| to obtain the power series for $f(x)=\frac{1}{\sqrt{1+x^{x}}}$ which is \begin{align*} \frac{1}{\sqrt{1+x^{x}}} =& \frac{1}{\sqrt{2}} - \frac{x\log(x)}{4\sqrt{2}} -\frac{x^{2}\log^{2}(x)}{32\sqrt{2}}+ \frac{5x^{3}\log^{3}(x)}{384\sqrt{2}}\\\  \\\ &+ \frac{17x^{4}\log^{4}(x)}{6144\sqrt{2}} - \frac{121x^{5}\log^{5}(x)}{122880\sqrt{2}} - \frac{721x^{6}\log^{6}(x)}{2949120\sqrt{2}} \ldots \end{align*} Before I realized that I couldn't really use this to help me with the sum, I found that the denominators (ignoring the $\sqrt{2}$ , because all of them have it in common) correspond to $4^{n}n!$ , what is baffling is that the numerators appear to correspond to the coefficients in the exponential generating function for $f(x)=e^{\tanh^{-1}(\tan(x))}$ (I believe that the 7th entry should be 1369 and not 6845), and I'm curious what the explanation is, because $f(x)=e^{\tanh^{-1}(\tan(x))}$ is a mighty weird looking function.","In the course of playing around with , I used w| to obtain the power series for which is Before I realized that I couldn't really use this to help me with the sum, I found that the denominators (ignoring the , because all of them have it in common) correspond to , what is baffling is that the numerators appear to correspond to the coefficients in the exponential generating function for (I believe that the 7th entry should be 1369 and not 6845), and I'm curious what the explanation is, because is a mighty weird looking function.","\sum_{n=1}^{\infty} \frac{1}{\sqrt{1+n^{n}}} f(x)=\frac{1}{\sqrt{1+x^{x}}} \begin{align*}
\frac{1}{\sqrt{1+x^{x}}} =& \frac{1}{\sqrt{2}} - \frac{x\log(x)}{4\sqrt{2}} -\frac{x^{2}\log^{2}(x)}{32\sqrt{2}}+ \frac{5x^{3}\log^{3}(x)}{384\sqrt{2}}\\\
 \\\
&+ \frac{17x^{4}\log^{4}(x)}{6144\sqrt{2}} - \frac{121x^{5}\log^{5}(x)}{122880\sqrt{2}} - \frac{721x^{6}\log^{6}(x)}{2949120\sqrt{2}} \ldots
\end{align*} \sqrt{2} 4^{n}n! f(x)=e^{\tanh^{-1}(\tan(x))} f(x)=e^{\tanh^{-1}(\tan(x))}","['real-analysis', 'sequences-and-series']"
63,Why can we interchange summations?,Why can we interchange summations?,,"Suppose we have the following $$ \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij}$$ where all the $a_{ij}$ are non-negative. We know that we can interchange the order of summations here. My interpretation of why this is true is that both this iterated sums are rearrangements of the same series and hence converge to the same value, or diverge to infinity (as convergence and absolute convergence are same here and all the rearrangements of an absolutely convergent series converge to the same value as the series). Is this interpretation correct. Or can some one offer some more insightful interpretation of this result? Please note that I am not asking for a proof but interpretations, although an insightful proof would be appreciated.","Suppose we have the following $$ \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij}$$ where all the $a_{ij}$ are non-negative. We know that we can interchange the order of summations here. My interpretation of why this is true is that both this iterated sums are rearrangements of the same series and hence converge to the same value, or diverge to infinity (as convergence and absolute convergence are same here and all the rearrangements of an absolutely convergent series converge to the same value as the series). Is this interpretation correct. Or can some one offer some more insightful interpretation of this result? Please note that I am not asking for a proof but interpretations, although an insightful proof would be appreciated.",,"['real-analysis', 'sequences-and-series']"
64,Infinitely differentiable function with given zero set?,Infinitely differentiable function with given zero set?,,"For each closed set $A\subseteq\mathbb{R}$, is it possible to construct a real continuous function $f$ such that the zero set, $f^{-1}(0)$, of $f$ is precisely $A$, and $f$ is infinitely differentiable on all of $\mathbb{R}$? Thanks!","For each closed set $A\subseteq\mathbb{R}$, is it possible to construct a real continuous function $f$ such that the zero set, $f^{-1}(0)$, of $f$ is precisely $A$, and $f$ is infinitely differentiable on all of $\mathbb{R}$? Thanks!",,"['real-analysis', 'functions']"
65,Product of two Lebesgue integrable functions not Lebesgue integrable,Product of two Lebesgue integrable functions not Lebesgue integrable,,"I have a homework problem that says; Give Borel functions $f,g: \mathbb{R} \to \mathbb{R}$ that are Lebesgue integrable, but are such that $fg$ is not Lebesgue integrable. I saw this page too: Product of two Lebesgue integrable functions , but the question does not mention boundedness. I also am not sure what to do with the fact that the functions are Borel. (Any help on this would be especially appreciated) I know that if $fg$ were Lebesgue integrable then both $\int (fg)^+\,d\mu$ and $\int (fg)^-\,d\mu$ would be finite. This could lead to utilizing the finiteness of their difference (the function's integral) or their sum (the absolute value). I also know that $f+g$ are Lebesgue integrable if $f$ and $g$ are so I thought of using $$fg = \frac{1}{4}\,\big( (f+g)^2 - (f-g)^2 \big)\longrightarrow \int (fg)\,d\mu = \frac{1}{4}\,\int (f+g)^2\,d\mu - \frac{1}{4}\,\int (f-g)^2\,d\mu,$$ assuming linearity of the integral etc. I also thought of the Hlder inequality, $$\int \mid fg \mid d\mu \leq \bigg( \int \mid f \mid^p d\mu \bigg)^{(1/p)}\,\bigg( \int \mid g \mid^q d\mu \bigg)^{(1/q)},$$ but there was no mention in the question of what $L^p$-space this was in. Maybe by the definition I gave it is such that $p=1$ and $q=1$? Then $$\int \mid fg \mid d\mu \leq \bigg( \int \mid f \mid d\mu \bigg)\,\bigg( \int \mid g \mid d\mu \bigg).$$ However, I still can't seem to think of an approach to show that $fg$ is not Lebesgue integrable, while $f$ and $g$ are. Thanks for any guidance!","I have a homework problem that says; Give Borel functions $f,g: \mathbb{R} \to \mathbb{R}$ that are Lebesgue integrable, but are such that $fg$ is not Lebesgue integrable. I saw this page too: Product of two Lebesgue integrable functions , but the question does not mention boundedness. I also am not sure what to do with the fact that the functions are Borel. (Any help on this would be especially appreciated) I know that if $fg$ were Lebesgue integrable then both $\int (fg)^+\,d\mu$ and $\int (fg)^-\,d\mu$ would be finite. This could lead to utilizing the finiteness of their difference (the function's integral) or their sum (the absolute value). I also know that $f+g$ are Lebesgue integrable if $f$ and $g$ are so I thought of using $$fg = \frac{1}{4}\,\big( (f+g)^2 - (f-g)^2 \big)\longrightarrow \int (fg)\,d\mu = \frac{1}{4}\,\int (f+g)^2\,d\mu - \frac{1}{4}\,\int (f-g)^2\,d\mu,$$ assuming linearity of the integral etc. I also thought of the Hlder inequality, $$\int \mid fg \mid d\mu \leq \bigg( \int \mid f \mid^p d\mu \bigg)^{(1/p)}\,\bigg( \int \mid g \mid^q d\mu \bigg)^{(1/q)},$$ but there was no mention in the question of what $L^p$-space this was in. Maybe by the definition I gave it is such that $p=1$ and $q=1$? Then $$\int \mid fg \mid d\mu \leq \bigg( \int \mid f \mid d\mu \bigg)\,\bigg( \int \mid g \mid d\mu \bigg).$$ However, I still can't seem to think of an approach to show that $fg$ is not Lebesgue integrable, while $f$ and $g$ are. Thanks for any guidance!",,['real-analysis']
66,Injective functions with intermediate-value property are continuous. Better proof?,Injective functions with intermediate-value property are continuous. Better proof?,,"A function $f: \mathbb{R} \to \mathbb{R}$ is said to have the intermediate-value property if for any $a$ , $b$ and $\lambda \in [f(a),f(b)]$ there is $x \in [a,b]$ such that $f(x)=\lambda$ . A function $f$ is injective if $f(x)=f(y) \Rightarrow x=y$ . Now it is the case that every injective function with the intermediate-value property is continuous. I can prove this using the following steps: An injective function with the intermediate-value property must be monotonic. A monotonic function possesses left- and right-handed limits at each point. For a function with the intermediate-value property the left- and right-handed limits at $x$ , if they exist, equal $f(x)$ . I am not really happy with this proof. Particularly I don't like having to invoke the intermediate-value property twice. Can there be a shorter or more elegant proof?","A function is said to have the intermediate-value property if for any , and there is such that . A function is injective if . Now it is the case that every injective function with the intermediate-value property is continuous. I can prove this using the following steps: An injective function with the intermediate-value property must be monotonic. A monotonic function possesses left- and right-handed limits at each point. For a function with the intermediate-value property the left- and right-handed limits at , if they exist, equal . I am not really happy with this proof. Particularly I don't like having to invoke the intermediate-value property twice. Can there be a shorter or more elegant proof?","f: \mathbb{R} \to \mathbb{R} a b \lambda \in [f(a),f(b)] x \in [a,b] f(x)=\lambda f f(x)=f(y) \Rightarrow x=y x f(x)","['real-analysis', 'continuity']"
67,"Continuous function from $(0,1)$ onto $[0,1]$",Continuous function from  onto,"(0,1) [0,1]","While revising, I came across this question(s): A) Is there a continuous function from $(0,1)$ onto $[0,1]$? B) Is there a continuous one-to-one function from $(0,1)$ onto $[0,1]$? (clarification: one-to-one is taken as a synonym for injective) I figured the answer to A is yes, with $\frac{1}{2}\sin(4\pi x)+\frac{1}{2}$ as an example. The answer to part B is no, but what is the reason? Sincere thanks for any help.","While revising, I came across this question(s): A) Is there a continuous function from $(0,1)$ onto $[0,1]$? B) Is there a continuous one-to-one function from $(0,1)$ onto $[0,1]$? (clarification: one-to-one is taken as a synonym for injective) I figured the answer to A is yes, with $\frac{1}{2}\sin(4\pi x)+\frac{1}{2}$ as an example. The answer to part B is no, but what is the reason? Sincere thanks for any help.",,['real-analysis']
68,Does the harmonic series converge if you throw out the terms containing a $9$? [duplicate],Does the harmonic series converge if you throw out the terms containing a ? [duplicate],9,"This question already has answers here : Sum of reciprocals of numbers with certain terms omitted (2 answers) Closed 7 years ago . I found this very amusing comic on the internet the other day: The last frame seems to claim that the harmonic series converges if you throw out all the terms with a $9$ in the denominator. Is this true? Where could I find a proof? Is it only true for terms with a $9$ or is it true for any number? If it's only true for $9$, why is that?","This question already has answers here : Sum of reciprocals of numbers with certain terms omitted (2 answers) Closed 7 years ago . I found this very amusing comic on the internet the other day: The last frame seems to claim that the harmonic series converges if you throw out all the terms with a $9$ in the denominator. Is this true? Where could I find a proof? Is it only true for terms with a $9$ or is it true for any number? If it's only true for $9$, why is that?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
69,Evaluate $\int_0^{\frac{\pi}{2}}\frac{x^2}{1+\cos^2 x}dx$,Evaluate,\int_0^{\frac{\pi}{2}}\frac{x^2}{1+\cos^2 x}dx,Evaluate the following integral $$\int_0^{\frac{\pi}{2}}\frac{x^2}{1+\cos^2 x}dx$$ This function does not have an elementary anti-derivative. How can we solve this?,Evaluate the following integral This function does not have an elementary anti-derivative. How can we solve this?,\int_0^{\frac{\pi}{2}}\frac{x^2}{1+\cos^2 x}dx,"['real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
70,Show that $f(x)=1/ x$ is continuous at any $c\neq 0$,Show that  is continuous at any,f(x)=1/ x c\neq 0,"Can anyone help me to solve this problem? Q :   Show that $f:\mathbb{R}\rightarrow\mathbb{R}$, $f(x)=1/ x$ is continuous at any $c\neq 0$. Notice: (choose your $\delta$ so that you stay away from 0) I hope someone can solve. Thanks","Can anyone help me to solve this problem? Q :   Show that $f:\mathbb{R}\rightarrow\mathbb{R}$, $f(x)=1/ x$ is continuous at any $c\neq 0$. Notice: (choose your $\delta$ so that you stay away from 0) I hope someone can solve. Thanks",,"['real-analysis', 'continuity']"
71,Graph of real continuous function has measure zero,Graph of real continuous function has measure zero,,"Let $f\colon[a,b] \to \mathbb R$ be a continuous function. Show that its graph has measure zero. I've tried with the following idea but I got stuck: Let $\epsilon >0$, since $f$ is uniformly continuous, there exists $\delta>0$ such that $ |x-y|< \delta \implies  |f(x)-f(y)|<\epsilon$. Let $P=\{x_0,\dots,x_n\}$ be a partition of the interval $[a,b]$ with $ |x_i-x_{i-1}|<\delta$. The graph of $f$ is $G(f)=\{(x,f(x)) : x \in [a,b]\}$, and $G(f) \subset \bigcup_{i=1}^n [x_{i-1},x_i]\times[m_i,M_i]$, where $m_i=\min_{x \in [x_{i-1},x_i]} f(x)$,$M_i=\max_{x \in [x_{i-1},x_i]} f(x)$. We have $ |G(f)|_e \leq \sum_{i=1}^nm([x_{i-1},x_i].m([m_i,M_i])<n\delta\epsilon$. At this point I got stuck, I need to arrive to an inequality with $\epsilon$ times a constant or something of the sort. I would appreciate some help.","Let $f\colon[a,b] \to \mathbb R$ be a continuous function. Show that its graph has measure zero. I've tried with the following idea but I got stuck: Let $\epsilon >0$, since $f$ is uniformly continuous, there exists $\delta>0$ such that $ |x-y|< \delta \implies  |f(x)-f(y)|<\epsilon$. Let $P=\{x_0,\dots,x_n\}$ be a partition of the interval $[a,b]$ with $ |x_i-x_{i-1}|<\delta$. The graph of $f$ is $G(f)=\{(x,f(x)) : x \in [a,b]\}$, and $G(f) \subset \bigcup_{i=1}^n [x_{i-1},x_i]\times[m_i,M_i]$, where $m_i=\min_{x \in [x_{i-1},x_i]} f(x)$,$M_i=\max_{x \in [x_{i-1},x_i]} f(x)$. We have $ |G(f)|_e \leq \sum_{i=1}^nm([x_{i-1},x_i].m([m_i,M_i])<n\delta\epsilon$. At this point I got stuck, I need to arrive to an inequality with $\epsilon$ times a constant or something of the sort. I would appreciate some help.",,"['real-analysis', 'measure-theory']"
72,How to understand/remember Hlder's inequality,How to understand/remember Hlder's inequality,,"If $p$ and $q$ are nonnegative numbers such that $\frac{1}{p}+\frac{1}{q}=1$ and if $f \in L^p$ and $g \in L^q$, then $f\cdot g \in L^1$ and $$\int |fg| \leqslant ||f||_p \cdot ||g||_q$$ I think Hlder's inequality is derived in order to prove Minkowski's inequality, which is a generalization of triangle inequality to $L^p$ norm. But is there any intuitive understanding of Hlder's inequality? It's hard for me to remember it. It seems that it's a generalization of the Cauchy-Schwarz inequality, trying to compare $L^2$ inner product to norm, but the power of each term is different, which makes it harder to be understood compared with Minkowski inequality.","If $p$ and $q$ are nonnegative numbers such that $\frac{1}{p}+\frac{1}{q}=1$ and if $f \in L^p$ and $g \in L^q$, then $f\cdot g \in L^1$ and $$\int |fg| \leqslant ||f||_p \cdot ||g||_q$$ I think Hlder's inequality is derived in order to prove Minkowski's inequality, which is a generalization of triangle inequality to $L^p$ norm. But is there any intuitive understanding of Hlder's inequality? It's hard for me to remember it. It seems that it's a generalization of the Cauchy-Schwarz inequality, trying to compare $L^2$ inner product to norm, but the power of each term is different, which makes it harder to be understood compared with Minkowski inequality.",,"['real-analysis', 'inequality']"
73,Prove multi-dimensional Mean Value Theorem,Prove multi-dimensional Mean Value Theorem,,"I've been asked to prove multi-dimensional Mean Value Theorem. I'd be   grateful if someone could give me feedback if it is okay. Proof of Mean Value Theorem: Let $f: [a,b]\rightarrow \mathbb{R}$ be a continuous on $[a,b]$ and differentiable on $(a,b)$. Consider the function: $$g(x)=f(x)-f(a)-\frac{f(b)-f(a)}{b-a}(x-a) \mbox{.}$$ This function is continuous on $[a,b]$, differentiable on $(a,b)$ and $g(a)=g(b)$. Thus there is $c\in (a,b)$ such that $g'(c)=0$. But this means that there is $c\in (a,b)$ such that $$f'(c)=\frac{f(b)-f(a)}{b-a}\mbox{.}$$ Proof of multi-dimensional Mean Value Theorem: Let $f:U\rightarrow\mathbb{R}$ be a differentiable function ($U$ is an open subset of $\mathbb{R}^n)$. Let $\mathbf{a}$ and $\mathbf{b}$ be points in $U$ such that the entire line segment between them is contained in $U$. Define $h:[0,1]\rightarrow U$ in the following way: $$h(t)=(a_1+(b_1-a_1)t,\ldots,a_n+(b_n-a_n)t) \mbox{.}$$ This function is differentiable on $(0,1)$ and continuous on $[0,1]$, so is $f \circ h$. If we apply Mean Value Theorem to $f\circ h$ we get $$(f \circ h )'(c)=(f \circ h )(1)-(f \circ h )(0)$$ where $c\in (0,1)$ and $$f '(h(c))(\mathbf{b}-\mathbf{a})=f(\mathbf{b})-f(\mathbf{a}) \mbox{.}$$ If we set $\zeta=h(c)$ we get $$f '(\zeta)=\frac{f(\mathbf{b})-f(\mathbf{a})}{\mathbf{b}-\mathbf{a}} \mbox{.}$$ (Obviously $f '(\zeta)$ is a gradient vector.)","I've been asked to prove multi-dimensional Mean Value Theorem. I'd be   grateful if someone could give me feedback if it is okay. Proof of Mean Value Theorem: Let $f: [a,b]\rightarrow \mathbb{R}$ be a continuous on $[a,b]$ and differentiable on $(a,b)$. Consider the function: $$g(x)=f(x)-f(a)-\frac{f(b)-f(a)}{b-a}(x-a) \mbox{.}$$ This function is continuous on $[a,b]$, differentiable on $(a,b)$ and $g(a)=g(b)$. Thus there is $c\in (a,b)$ such that $g'(c)=0$. But this means that there is $c\in (a,b)$ such that $$f'(c)=\frac{f(b)-f(a)}{b-a}\mbox{.}$$ Proof of multi-dimensional Mean Value Theorem: Let $f:U\rightarrow\mathbb{R}$ be a differentiable function ($U$ is an open subset of $\mathbb{R}^n)$. Let $\mathbf{a}$ and $\mathbf{b}$ be points in $U$ such that the entire line segment between them is contained in $U$. Define $h:[0,1]\rightarrow U$ in the following way: $$h(t)=(a_1+(b_1-a_1)t,\ldots,a_n+(b_n-a_n)t) \mbox{.}$$ This function is differentiable on $(0,1)$ and continuous on $[0,1]$, so is $f \circ h$. If we apply Mean Value Theorem to $f\circ h$ we get $$(f \circ h )'(c)=(f \circ h )(1)-(f \circ h )(0)$$ where $c\in (0,1)$ and $$f '(h(c))(\mathbf{b}-\mathbf{a})=f(\mathbf{b})-f(\mathbf{a}) \mbox{.}$$ If we set $\zeta=h(c)$ we get $$f '(\zeta)=\frac{f(\mathbf{b})-f(\mathbf{a})}{\mathbf{b}-\mathbf{a}} \mbox{.}$$ (Obviously $f '(\zeta)$ is a gradient vector.)",,"['calculus', 'real-analysis', 'derivatives']"
74,Compute $\sum_{k=1}^{\infty}e^{-\pi k^2}\left(\pi k^2-\frac{1}{4}\right)$,Compute,\sum_{k=1}^{\infty}e^{-\pi k^2}\left(\pi k^2-\frac{1}{4}\right),"How may I evaluate the below series? $$\sum_{k=1}^{\infty}e^{-\pi k^2}\left(\pi k^2-\frac{1}{4}\right)$$ I'm supposed to come up with a solution by only using high school knowledge. Thanks in advance for your hints, suggestions!","How may I evaluate the below series? $$\sum_{k=1}^{\infty}e^{-\pi k^2}\left(\pi k^2-\frac{1}{4}\right)$$ I'm supposed to come up with a solution by only using high school knowledge. Thanks in advance for your hints, suggestions!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
75,Need some help on a non-example of equicontinuity,Need some help on a non-example of equicontinuity,,"In an attempt to better understand the definition of an equicontinuous family of continuous functions, I want to find a simple non-example. My intuition says that the family $\{f_n\colon[0,1]\to\mathbb R\}_{n\in\mathbb N}$ given by $f_n(x)=x^n$ is not equicontinuous, but I do not know how to show this. Any help is appreciated.","In an attempt to better understand the definition of an equicontinuous family of continuous functions, I want to find a simple non-example. My intuition says that the family $\{f_n\colon[0,1]\to\mathbb R\}_{n\in\mathbb N}$ given by $f_n(x)=x^n$ is not equicontinuous, but I do not know how to show this. Any help is appreciated.",,"['real-analysis', 'examples-counterexamples', 'equicontinuity']"
76,Bound on $\sum\limits_{n=0}^{x}{\sin{\sqrt{n}}}$,Bound on,\sum\limits_{n=0}^{x}{\sin{\sqrt{n}}},"Using Desmos and Mathematica, I was able to find a function $g(x)$ that seemingly estimated the function $$f(x)=\sum_{n=0}^{x}{\sin{\sqrt{n}}}$$ I found that $${g(x)=2\sqrt{x}*\sin{\left({{\sqrt{4x+{\pi}^2}-\pi}\over{2}}\right)}}\approx f(x)$$ Furthermore, I found that the difference between these two functions, or $|{f(x)-g(x)}|$ , never seemed to exceed some constant $C\approx 0.464568$ Is there some way to prove the conjecture below? $g(x)<f(x)<(g(x)+C)$ for all $x>14$ Or perhaps a weaker version considering I have no good definition for $C$ : $g(x)<f(x)<(g(x)+{1\over2})$ for all $x>14$ . Further questions: Is there a closed form for $C$ ? Is there a closed form for $f(x)$ ?","Using Desmos and Mathematica, I was able to find a function that seemingly estimated the function I found that Furthermore, I found that the difference between these two functions, or , never seemed to exceed some constant Is there some way to prove the conjecture below? for all Or perhaps a weaker version considering I have no good definition for : for all . Further questions: Is there a closed form for ? Is there a closed form for ?",g(x) f(x)=\sum_{n=0}^{x}{\sin{\sqrt{n}}} {g(x)=2\sqrt{x}*\sin{\left({{\sqrt{4x+{\pi}^2}-\pi}\over{2}}\right)}}\approx f(x) |{f(x)-g(x)}| C\approx 0.464568 g(x)<f(x)<(g(x)+C) x>14 C g(x)<f(x)<(g(x)+{1\over2}) x>14 C f(x),"['real-analysis', 'sequences-and-series', 'upper-lower-bounds']"
77,"Counterexample to ""Measurable in each variable separately implies measurable""","Counterexample to ""Measurable in each variable separately implies measurable""",,"Some fellow classmates are preparing for a qualifying exam on real analysis, and asked me for help on the following question: Let $ \ f:[0,1]^2\longrightarrow\mathbb{R}$ be such that: (i) $\ f(x,\cdot)$ is measurable for each fixed $x\in[0,1]$ ; (ii) $\ f(\cdot,y)$ is continuous for each fixed $y\in[0,1]$ . Show that $\ f$ is measurable.  If we only assume that $\ f(\cdot,y)$ is measurable for each $y\in[0,1]$ , rather than continuous, can we still conclude that $f$ is measurable? I was able to come up with the following simple proof (at least I hope it is a proof) of the first statement using some standard arguments: Proof. $ \ $ Define a sequence of functions $\{f_n:[0,1]^2\longrightarrow\mathbb{R}\}_{n\geq 1}$ by: $$ f_n(x,y)=f\bigg(\frac{i}{n},y\bigg), \qquad \text{for } x\in\bigg[\frac{i}{n},\frac{i+1}{n}\bigg], \ i=1,\cdots,n $$ Moreover, since $ \ f(\cdot,y)$ is continuous at $x=\tfrac{i}{n}$ for each $y$ , for each $\epsilon>0$ there is a $\delta_y>0$ so that $$ \bigg| \ x-\frac{i}{n}\bigg|<\delta_y \qquad\Rightarrow\qquad \bigg| \ f(x,y)-f\bigg(\frac{i}{n},y\bigg)\bigg|<\epsilon; $$ therefore, for any $n>\tfrac{1}{\delta_y}$ and any fixed $(x,y)\in[0,1]^2$ with $x\in\big[\tfrac{i}{n},\tfrac{i+1}{n}\big]$ , we have $$ \bigg| \ x-\frac{i}{n}\bigg|\leq \frac{1}{n}<\delta_y $$ and so $$ | \ f_n(x,y)-f(x,y)|=\bigg| \ f\bigg(\frac{i}{n},y\bigg)-f(x,y)\bigg|<\epsilon. \tag{$\ast$} $$ In other words, $ \ f_n\longrightarrow f$ pointwise.  Furthermore, we clearly have that $$ A_n:=\{(x,y)\in[0,1]^2 \ | \ f_n(x,y)>\alpha\} \qquad\qquad\qquad $$ $$ \qquad\qquad \ =\bigcup_{i=1}^{n-1}\bigg(\bigg[\frac{i}{n},\frac{i+1}{n}\bigg)\times \bigg\{y\in[0,1] \ \bigg| \ \ f\bigg(\frac{i}{n},y\bigg)>\alpha \bigg\}\bigg). $$ Since $ \ f\big(\tfrac{i}{n},\cdot\big)$ is measurable for each $i=1,\cdots,n$ , the latter expression is a union of measurable sets.  This means that $ \ A_n$ is measurable, and therefore $ \ f_n$ is measurable for each $n\geq 1$ .  Finally, as the limit of measurable functions is measurable, we conclude that $ \ f$ is measurable. $\hspace{6.25in}\square$ I'm fairly certain that the above proof is correct ( anyone care to confirm? ); however, that still leaves the following: Question:  If $ \ f$ is merely measurable in each variable separately, is it measurable? The proof I gave above uses the continuity assumption in $(\ast)$ , so obviously the same proof will not work in this case, but I cannot come up with any counterexamples.  Any suggestions?","Some fellow classmates are preparing for a qualifying exam on real analysis, and asked me for help on the following question: Let be such that: (i) is measurable for each fixed ; (ii) is continuous for each fixed . Show that is measurable.  If we only assume that is measurable for each , rather than continuous, can we still conclude that is measurable? I was able to come up with the following simple proof (at least I hope it is a proof) of the first statement using some standard arguments: Proof. Define a sequence of functions by: Moreover, since is continuous at for each , for each there is a so that therefore, for any and any fixed with , we have and so In other words, pointwise.  Furthermore, we clearly have that Since is measurable for each , the latter expression is a union of measurable sets.  This means that is measurable, and therefore is measurable for each .  Finally, as the limit of measurable functions is measurable, we conclude that is measurable. I'm fairly certain that the above proof is correct ( anyone care to confirm? ); however, that still leaves the following: Question:  If is merely measurable in each variable separately, is it measurable? The proof I gave above uses the continuity assumption in , so obviously the same proof will not work in this case, but I cannot come up with any counterexamples.  Any suggestions?"," \ f:[0,1]^2\longrightarrow\mathbb{R} \ f(x,\cdot) x\in[0,1] \ f(\cdot,y) y\in[0,1] \ f \ f(\cdot,y) y\in[0,1] f  \  \{f_n:[0,1]^2\longrightarrow\mathbb{R}\}_{n\geq 1}  f_n(x,y)=f\bigg(\frac{i}{n},y\bigg), \qquad \text{for } x\in\bigg[\frac{i}{n},\frac{i+1}{n}\bigg], \ i=1,\cdots,n   \ f(\cdot,y) x=\tfrac{i}{n} y \epsilon>0 \delta_y>0  \bigg| \ x-\frac{i}{n}\bigg|<\delta_y \qquad\Rightarrow\qquad \bigg| \ f(x,y)-f\bigg(\frac{i}{n},y\bigg)\bigg|<\epsilon;  n>\tfrac{1}{\delta_y} (x,y)\in[0,1]^2 x\in\big[\tfrac{i}{n},\tfrac{i+1}{n}\big]  \bigg| \ x-\frac{i}{n}\bigg|\leq \frac{1}{n}<\delta_y   | \ f_n(x,y)-f(x,y)|=\bigg| \ f\bigg(\frac{i}{n},y\bigg)-f(x,y)\bigg|<\epsilon. \tag{\ast}   \ f_n\longrightarrow f  A_n:=\{(x,y)\in[0,1]^2 \ | \ f_n(x,y)>\alpha\} \qquad\qquad\qquad   \qquad\qquad \ =\bigcup_{i=1}^{n-1}\bigg(\bigg[\frac{i}{n},\frac{i+1}{n}\bigg)\times \bigg\{y\in[0,1] \ \bigg| \ \ f\bigg(\frac{i}{n},y\bigg)>\alpha \bigg\}\bigg).   \ f\big(\tfrac{i}{n},\cdot\big) i=1,\cdots,n  \ A_n  \ f_n n\geq 1  \ f \hspace{6.25in}\square  \ f (\ast)","['real-analysis', 'measure-theory', 'proof-verification', 'examples-counterexamples']"
78,"Evaluating the limit of a sequence given by recurrence relation $a_1=\sqrt2$, $a_{n+1}=\sqrt{2+a_n}$. Is my solution correct? [duplicate]","Evaluating the limit of a sequence given by recurrence relation , . Is my solution correct? [duplicate]",a_1=\sqrt2 a_{n+1}=\sqrt{2+a_n},"This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 6 years ago . Problem The sequence $(a_n)_{n=1}^\infty$ is given by recurrence relation: $a_1=\sqrt2$, $a_{n+1}=\sqrt{2+a_n}$. Evaluate the limit $\lim_{n\to\infty} a_n$. Solution Show that the sequence $(a_n)_{n=1}^\infty$ is monotonic. The statement $$V(n): a_n < a_{n+1}$$ holds for $n = 1$, that is $\sqrt2 < \sqrt{2+\sqrt2}$. Let us assume the statement holds for $n$ and show that $V(n) \implies V(n+1)$. We have that $$a_n < a_{n+1}.$$ Adding 2 to both sides and taking square roots, we have that $$\sqrt{2+a_n} < \sqrt{2+a_{n+1}},$$ that is $a_{n+1} < a_{n+2}$ by definition. Find bounds for $a_n$. The statement $$W(n): 0 < a_n < 2$$ holds for $n=1$, that is $0 < \sqrt2 < 2$. Let us assume the statement holds for $n$ and show that $W(n) \implies W(n+1)$. We have that $$0 < a_n < 2.$$ Adding two and taking square roots, we have that $$0 < \sqrt2 < \sqrt{2+a_n} < \sqrt4 = 2.$$ The limit $\lim_{n\to\infty} a_n$ exists , because $(a_n)_{n=1}^\infty$ is a bounded monotonic sequence. Let $A = \lim_{n\to\infty} a_n$. Therefore the limit $\lim_{n \to\infty} a_{n+1}$ exists as well and $\lim_{n \to\infty} a_{n+1} = A$. (For $(n_k)_{k=1}^\infty = (2,3,4, \dots)$, we have that $(a_{n_k})_{k=1}^\infty$ is a subsequence of $(a_n)_{n=1}^\infty$, from which the statement follows.) We have that $a_{n+1} = f(a_n)$. That means that $A = \lim_{n\to\infty} a_n = \lim_{n \to\infty} {f(a_n)} = f(\lim_{n \to\infty} a_n) = f(A) = \sqrt{2 + A}$. Solving the equation $A = \sqrt{2 + A}$, we get $A = -1 \lor A = 2$. Putting it all together, we get that $A = 2$, because the terms of the sequence are increasing and $a_1 > 0$. Is my solution correct?","This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 6 years ago . Problem The sequence $(a_n)_{n=1}^\infty$ is given by recurrence relation: $a_1=\sqrt2$, $a_{n+1}=\sqrt{2+a_n}$. Evaluate the limit $\lim_{n\to\infty} a_n$. Solution Show that the sequence $(a_n)_{n=1}^\infty$ is monotonic. The statement $$V(n): a_n < a_{n+1}$$ holds for $n = 1$, that is $\sqrt2 < \sqrt{2+\sqrt2}$. Let us assume the statement holds for $n$ and show that $V(n) \implies V(n+1)$. We have that $$a_n < a_{n+1}.$$ Adding 2 to both sides and taking square roots, we have that $$\sqrt{2+a_n} < \sqrt{2+a_{n+1}},$$ that is $a_{n+1} < a_{n+2}$ by definition. Find bounds for $a_n$. The statement $$W(n): 0 < a_n < 2$$ holds for $n=1$, that is $0 < \sqrt2 < 2$. Let us assume the statement holds for $n$ and show that $W(n) \implies W(n+1)$. We have that $$0 < a_n < 2.$$ Adding two and taking square roots, we have that $$0 < \sqrt2 < \sqrt{2+a_n} < \sqrt4 = 2.$$ The limit $\lim_{n\to\infty} a_n$ exists , because $(a_n)_{n=1}^\infty$ is a bounded monotonic sequence. Let $A = \lim_{n\to\infty} a_n$. Therefore the limit $\lim_{n \to\infty} a_{n+1}$ exists as well and $\lim_{n \to\infty} a_{n+1} = A$. (For $(n_k)_{k=1}^\infty = (2,3,4, \dots)$, we have that $(a_{n_k})_{k=1}^\infty$ is a subsequence of $(a_n)_{n=1}^\infty$, from which the statement follows.) We have that $a_{n+1} = f(a_n)$. That means that $A = \lim_{n\to\infty} a_n = \lim_{n \to\infty} {f(a_n)} = f(\lim_{n \to\infty} a_n) = f(A) = \sqrt{2 + A}$. Solving the equation $A = \sqrt{2 + A}$, we get $A = -1 \lor A = 2$. Putting it all together, we get that $A = 2$, because the terms of the sequence are increasing and $a_1 > 0$. Is my solution correct?",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'recurrence-relations']"
79,Interesting integral formula,Interesting integral formula,,"I looked around and found that integrals of the form $$\int_{0}^{\infty} \frac{x^{m-1}}{a+x^n}, a,m,n \in \mathbb{R}, 0<m<n, 0<a$$ seem to occur very often: Just to give a few examples (the formula given below would solve them all right away): How can I compute the integral $\int_{0}^{\infty} \frac{dt}{1+t^4}$? Simpler way to compute a definite integral without resorting to partial fractions? Is this definite integral really independent of a parameter? How can it be shown? Integrating $\int_0^\infty \frac{1}{x^2 + 2x + 2} \mathrm{d} x$ Even more surprising was that there seems to be a (quite beautiful) closed form, namely: $$\int_{0}^{\infty} \frac{x^{m-1}}{a+x^n}=\frac{\pi}{n}  \left(\frac{1}{a}\right)^{1-\frac{m}{n}} \csc \left(m \cdot \frac{\pi     }{n}\right)$$ (This result is from Mathematica). I tried to derive this result by integrating (brute force) but you get hypergeometric functions which I don't like. Therefore I would like to know if there is a straight-forward way to get this by hand.","I looked around and found that integrals of the form $$\int_{0}^{\infty} \frac{x^{m-1}}{a+x^n}, a,m,n \in \mathbb{R}, 0<m<n, 0<a$$ seem to occur very often: Just to give a few examples (the formula given below would solve them all right away): How can I compute the integral $\int_{0}^{\infty} \frac{dt}{1+t^4}$? Simpler way to compute a definite integral without resorting to partial fractions? Is this definite integral really independent of a parameter? How can it be shown? Integrating $\int_0^\infty \frac{1}{x^2 + 2x + 2} \mathrm{d} x$ Even more surprising was that there seems to be a (quite beautiful) closed form, namely: $$\int_{0}^{\infty} \frac{x^{m-1}}{a+x^n}=\frac{\pi}{n}  \left(\frac{1}{a}\right)^{1-\frac{m}{n}} \csc \left(m \cdot \frac{\pi     }{n}\right)$$ (This result is from Mathematica). I tried to derive this result by integrating (brute force) but you get hypergeometric functions which I don't like. Therefore I would like to know if there is a straight-forward way to get this by hand.",,"['real-analysis', 'integration']"
80,"Regulating $\int_0^\infty \sin x \, \mathrm{d} x$",Regulating,"\int_0^\infty \sin x \, \mathrm{d} x","The limit $$\lim_{a \to \infty} \int_0^a \sin x \, \mathrm{d} x$$ does not exist. However, consider that $$ \lim_{\epsilon \to 0} \int_0^\infty e^{- \epsilon x} \sin x \, \mathrm{d} x  = 1 \,.$$ Here I have 'regulated' the integral. What I discovered, and what strikes me as very surprising, is that if, instead of an exponential, I choose a different function $f(x, \epsilon)$ which tends pointwise to $1$ as $\epsilon$ goes to $0$ and tends to $0$ as $x$ goes to $\infty$, I get convergence to exactly the same limit. So if I choose $$ f(x, \epsilon)  = \frac{1}{1 + \epsilon x^2} \quad \text{or} \quad \mathrm{sech}^2(\epsilon x) \quad \text{or} \quad (1 + 2 \epsilon x^2) e^{-\epsilon x^2}\,,$$ Then the integral of $f(x, \epsilon) \sin x$ from 0 to $\infty$ tends to $1$ as $\epsilon$ tends to $0$. Why is this happening? EDIT: I was initially satisfied with the responses given, but on further thought I don't think I follow the logic of tired's answer, which invokes the stationary phase approximation. In particular, my understanding of the stationary phase approximation is that one looks for stationary points of the argument of the exponential since these correspond to the points where the oscillation is slowest  away from this point, the oscillations 'cancel out' because of how rapid they are. However, in this case the argument of the exponential has no stationary points. Further, whilst I can appreciate that (in the case where there are no stationary points) a 'boundary maximum point' would dominate the integral in the real case (that is, for which the argument of the exponential is real), I can't see that this would be relevant in the imaginary case. I am looking for an answer that includes the following three points: A proof that the limit of this integral is independent of regulator (for a suitable class of regulator). Some intuition as to why we might expect this particular integral to be independent of regulator. Information on whether there is some general theory about assigning, perhaps uniquely, a value to non-convergent integrals. In particular, I would like to know whether the fact that the integral at the top of this question is 'almost convergent' (in the sense that it is bounded for all $a$) makes it easier to unambiguously regulate.","The limit $$\lim_{a \to \infty} \int_0^a \sin x \, \mathrm{d} x$$ does not exist. However, consider that $$ \lim_{\epsilon \to 0} \int_0^\infty e^{- \epsilon x} \sin x \, \mathrm{d} x  = 1 \,.$$ Here I have 'regulated' the integral. What I discovered, and what strikes me as very surprising, is that if, instead of an exponential, I choose a different function $f(x, \epsilon)$ which tends pointwise to $1$ as $\epsilon$ goes to $0$ and tends to $0$ as $x$ goes to $\infty$, I get convergence to exactly the same limit. So if I choose $$ f(x, \epsilon)  = \frac{1}{1 + \epsilon x^2} \quad \text{or} \quad \mathrm{sech}^2(\epsilon x) \quad \text{or} \quad (1 + 2 \epsilon x^2) e^{-\epsilon x^2}\,,$$ Then the integral of $f(x, \epsilon) \sin x$ from 0 to $\infty$ tends to $1$ as $\epsilon$ tends to $0$. Why is this happening? EDIT: I was initially satisfied with the responses given, but on further thought I don't think I follow the logic of tired's answer, which invokes the stationary phase approximation. In particular, my understanding of the stationary phase approximation is that one looks for stationary points of the argument of the exponential since these correspond to the points where the oscillation is slowest  away from this point, the oscillations 'cancel out' because of how rapid they are. However, in this case the argument of the exponential has no stationary points. Further, whilst I can appreciate that (in the case where there are no stationary points) a 'boundary maximum point' would dominate the integral in the real case (that is, for which the argument of the exponential is real), I can't see that this would be relevant in the imaginary case. I am looking for an answer that includes the following three points: A proof that the limit of this integral is independent of regulator (for a suitable class of regulator). Some intuition as to why we might expect this particular integral to be independent of regulator. Information on whether there is some general theory about assigning, perhaps uniquely, a value to non-convergent integrals. In particular, I would like to know whether the fact that the integral at the top of this question is 'almost convergent' (in the sense that it is bounded for all $a$) makes it easier to unambiguously regulate.",,"['real-analysis', 'integration']"
81,How to prove that the rank of a matrix is a lower semi-continuous function?,How to prove that the rank of a matrix is a lower semi-continuous function?,,"I need to prove that rank($\mathrm{A}$) is not continuous everywhere but is lower semi-continuous everywhere, where $\mathrm{A}\in \mathbb{C}^{n\times m} $","I need to prove that rank($\mathrm{A}$) is not continuous everywhere but is lower semi-continuous everywhere, where $\mathrm{A}\in \mathbb{C}^{n\times m} $",,"['real-analysis', 'linear-algebra', 'matrices', 'analysis']"
82,Is there a bijection $f: \mathbb{N} \rightarrow \mathbb{N}$ such that the series $\sum\limits_n \frac{1}{n+f(n)} $ is convergent?,Is there a bijection  such that the series  is convergent?,f: \mathbb{N} \rightarrow \mathbb{N} \sum\limits_n \frac{1}{n+f(n)} ,"Is there a bijection $f: \mathbb{N} \rightarrow \mathbb{N}$ such that the series $\sum_1 ^\infty \frac{1}{n+f(n)} $ is convergent? I could not solve this. I tried to proceed in following lines: 1) Tried to provide a contradiction: First let $n \sim m$ iff $\exists k \in \mathbb{Z}$ such that $f^k(n)=m$. This is an equivalence relation. Consider the orbits. For the finite orbits we can compare the series to $\sum_1^\infty \frac{1}{n+n}$. But then I could not figure out how to proceed for infinite orbits. 2) Tried to prove that there is some function: Let $\{k_n\}$ be a subsequence of $\mathbb{N}$ such that $\sum_0^\infty \frac{1}{k_n}$ converges. Set $f(n)=k_{n}, \forall n \in \mathbb{N}\setminus\{k_n\}$. Then images of each $n$ which are not in the subsequence $k_n$ is defined. Now we have to define images of each $k_n$. Define $f(k_n)=n$ $ \forall n \in \mathbb{N}\setminus \{k_n\}.$. Could not proceed further. I think My second attempt was going in right direction. My plan was use the fact that all elements here are positive and to construct the function $f$ in such a manner that $\forall n\in \mathbb{N}$ either $n$ or $f(n)$ is in $\{k_n\}$.","Is there a bijection $f: \mathbb{N} \rightarrow \mathbb{N}$ such that the series $\sum_1 ^\infty \frac{1}{n+f(n)} $ is convergent? I could not solve this. I tried to proceed in following lines: 1) Tried to provide a contradiction: First let $n \sim m$ iff $\exists k \in \mathbb{Z}$ such that $f^k(n)=m$. This is an equivalence relation. Consider the orbits. For the finite orbits we can compare the series to $\sum_1^\infty \frac{1}{n+n}$. But then I could not figure out how to proceed for infinite orbits. 2) Tried to prove that there is some function: Let $\{k_n\}$ be a subsequence of $\mathbb{N}$ such that $\sum_0^\infty \frac{1}{k_n}$ converges. Set $f(n)=k_{n}, \forall n \in \mathbb{N}\setminus\{k_n\}$. Then images of each $n$ which are not in the subsequence $k_n$ is defined. Now we have to define images of each $k_n$. Define $f(k_n)=n$ $ \forall n \in \mathbb{N}\setminus \{k_n\}.$. Could not proceed further. I think My second attempt was going in right direction. My plan was use the fact that all elements here are positive and to construct the function $f$ in such a manner that $\forall n\in \mathbb{N}$ either $n$ or $f(n)$ is in $\{k_n\}$.",,"['real-analysis', 'sequences-and-series', 'analysis']"
83,"For $x\in\mathbb R\setminus\mathbb Q$, the set $\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$ is dense on $[0,1)$","For , the set  is dense on","x\in\mathbb R\setminus\mathbb Q \{nx-\lfloor nx\rfloor: n\in \mathbb{N}\} [0,1)","Let $x\in \mathbb{R}$ an irrational number. Define $X=\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$. Prove that $X$ is dense on $[0,1)$. Can anyone give some hint to solve this problem? I tried contradiction but could not reach a proof. I spend part of the day studying this question Positive integer multiples of an irrational mod 1 are dense and its answers. Only one answer is clear and give clues to solve the problem. This answer is the first one. However, this answer does not answer the question nor directly, nor the proof follows from this answer. This answer has some mistakes, he use that $[(k_1-k_2)\alpha]=[k_1\alpha]-[k_2\alpha]$ which is not true. Consider $k_1=3, k_2=1, \alpha=\sqrt{2}$ we have $[(k_1-k_2)\alpha]=2\not= 3=[k_1\alpha]-[k_2\alpha] $. We only can assure that $[k_2\alpha]-[k_1\alpha]-1\leq [(k_2-k_1)\alpha]\leq[k_2\alpha]-[k_1\alpha]$. Who answered said something interesting about additive subgroups of $\mathbb{R}$, but unfortunately the set $X=\{nx-[nx] : n\in \mathbb{N} \}$ is not a subgroup. Considering the additive subgroup $G=\langle X \rangle$, if we prove the part (a) of the link, we get that indeed $G$ is dense on $\mathbb{R}$ but we can not conclude that $X$ is dense on $[0,1)$. I think this problem has not been solved. Thanks!","Let $x\in \mathbb{R}$ an irrational number. Define $X=\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$. Prove that $X$ is dense on $[0,1)$. Can anyone give some hint to solve this problem? I tried contradiction but could not reach a proof. I spend part of the day studying this question Positive integer multiples of an irrational mod 1 are dense and its answers. Only one answer is clear and give clues to solve the problem. This answer is the first one. However, this answer does not answer the question nor directly, nor the proof follows from this answer. This answer has some mistakes, he use that $[(k_1-k_2)\alpha]=[k_1\alpha]-[k_2\alpha]$ which is not true. Consider $k_1=3, k_2=1, \alpha=\sqrt{2}$ we have $[(k_1-k_2)\alpha]=2\not= 3=[k_1\alpha]-[k_2\alpha] $. We only can assure that $[k_2\alpha]-[k_1\alpha]-1\leq [(k_2-k_1)\alpha]\leq[k_2\alpha]-[k_1\alpha]$. Who answered said something interesting about additive subgroups of $\mathbb{R}$, but unfortunately the set $X=\{nx-[nx] : n\in \mathbb{N} \}$ is not a subgroup. Considering the additive subgroup $G=\langle X \rangle$, if we prove the part (a) of the link, we get that indeed $G$ is dense on $\mathbb{R}$ but we can not conclude that $X$ is dense on $[0,1)$. I think this problem has not been solved. Thanks!",,"['real-analysis', 'irrational-numbers', 'ceiling-and-floor-functions', 'fractional-part']"
84,Convergence of the series $\sum\limits_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)}$,Convergence of the series,\sum\limits_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)},$$\sum_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)}$$,$$\sum_{n=2}^{\infty} \frac{ (-1)^n} { \ln(n) +\cos(n)}$$,,"['real-analysis', 'sequences-and-series']"
85,How does this discontinuity occur in evaluating a nested square root?,How does this discontinuity occur in evaluating a nested square root?,,"This question is based on a comment I made on a question likely to be closed. Let $$y=\sqrt {x+ \sqrt {x+ \sqrt {x+ \sqrt {x+ \sqrt {x+ \dots}}}}}$$ be the classic nested square root which has appeared many times in questions in one form or another. We have $y^2=x+y$ so that $$y=\frac {1\pm\sqrt {1+4x}}2$$ Now if $x\gt 0$ then $y\gt 0$ and we must take the positive square root, and we have $y\gt 1$ and tends to $1$ as $x$ approaches zero from above. If $x=0$ it is obvious that we have $y=0$ and this is achieved by taking the negative square root in the quadratic formula. The positive square root gives the limit $1$ which looks ridiculous as a value of the expression. I am looking for an explanation of how this rather curious discontinuity arises - what are the danger signs that there might be a problem with this value? Note (8 July) - I'm still not satisfied that I have developed a sound intuition of what's going on here, except that nesting square roots need not commute with other limit operations at critical points - it is a nastier operation than meets the eye.","This question is based on a comment I made on a question likely to be closed. Let $$y=\sqrt {x+ \sqrt {x+ \sqrt {x+ \sqrt {x+ \sqrt {x+ \dots}}}}}$$ be the classic nested square root which has appeared many times in questions in one form or another. We have $y^2=x+y$ so that $$y=\frac {1\pm\sqrt {1+4x}}2$$ Now if $x\gt 0$ then $y\gt 0$ and we must take the positive square root, and we have $y\gt 1$ and tends to $1$ as $x$ approaches zero from above. If $x=0$ it is obvious that we have $y=0$ and this is achieved by taking the negative square root in the quadratic formula. The positive square root gives the limit $1$ which looks ridiculous as a value of the expression. I am looking for an explanation of how this rather curious discontinuity arises - what are the danger signs that there might be a problem with this value? Note (8 July) - I'm still not satisfied that I have developed a sound intuition of what's going on here, except that nesting square roots need not commute with other limit operations at critical points - it is a nastier operation than meets the eye.",,"['real-analysis', 'limits', 'continuity', 'nested-radicals']"
86,Evaluate series: $\sum_{k=1}^\infty (-1)^k\left[ k\ln\left(\frac{k^4+2k^3+k^2}{k^4+2k^3+3k^2+2k+2}\right)+\ln\frac{k^2+2k+1}{k^2+1} \right]$,Evaluate series:,\sum_{k=1}^\infty (-1)^k\left[ k\ln\left(\frac{k^4+2k^3+k^2}{k^4+2k^3+3k^2+2k+2}\right)+\ln\frac{k^2+2k+1}{k^2+1} \right],"The series is $$\sum_{k=1}^\infty (-1)^k\left[ k\ln\left(\frac{k^4+2k^3+k^2}{k^4+2k^3+3k^2+2k+2}\right)+\ln\left(\frac{k^2+2k+1}{k^2+1}\right)  \right] $$ You won't believe it: this has a closed form! It's the beautiful $$4\coth^{-1}(e^\pi)$$ and Wolfram agrees. By the way, I tried to prove it: the arguments of the $\ln$ 's all factor, and with lots of simplifications the summand inside the brackets reduces to $$[\ 2k\ln(k)+2(k+1)\ln(k+1)-(k+1)\ln(k^2+1)-k\ln(k^2+2k+2)\ ] $$ With this new representation and some struggle (it's not difficult, just some algebraic manipulations), I managed to equate the original sum to the sum $$\sum_{k=1}^\infty (-1)^k\left[\ -2k \tanh^{-1}\left(\frac{k+1}{k^2+k+1}\right)- (k+1)\ln\left(\frac{k^2+1}{(k+1)^2}\right) \right] $$ but from here, I couldn't proceed any further. At this point, I even think this made it only worse. In addition, notice that $$4\coth^{-1}(e^\pi)=2\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)=2\ln\left(\coth\left(\frac\pi2\right) \right)$$ which follows just from the definition of inverse $\coth$ , and maybe this is a bit easier to work with. If anyone has any idea on how to attack this monster, or comes up with a full solution, please share, I look forward to read it.","The series is You won't believe it: this has a closed form! It's the beautiful and Wolfram agrees. By the way, I tried to prove it: the arguments of the 's all factor, and with lots of simplifications the summand inside the brackets reduces to With this new representation and some struggle (it's not difficult, just some algebraic manipulations), I managed to equate the original sum to the sum but from here, I couldn't proceed any further. At this point, I even think this made it only worse. In addition, notice that which follows just from the definition of inverse , and maybe this is a bit easier to work with. If anyone has any idea on how to attack this monster, or comes up with a full solution, please share, I look forward to read it.",\sum_{k=1}^\infty (-1)^k\left[ k\ln\left(\frac{k^4+2k^3+k^2}{k^4+2k^3+3k^2+2k+2}\right)+\ln\left(\frac{k^2+2k+1}{k^2+1}\right)  \right]  4\coth^{-1}(e^\pi) \ln [\ 2k\ln(k)+2(k+1)\ln(k+1)-(k+1)\ln(k^2+1)-k\ln(k^2+2k+2)\ ]  \sum_{k=1}^\infty (-1)^k\left[\ -2k \tanh^{-1}\left(\frac{k+1}{k^2+k+1}\right)- (k+1)\ln\left(\frac{k^2+1}{(k+1)^2}\right) \right]  4\coth^{-1}(e^\pi)=2\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)=2\ln\left(\coth\left(\frac\pi2\right) \right) \coth,"['real-analysis', 'integration', 'sequences-and-series', 'analysis', 'improper-integrals']"
87,A snappy proof of Fatou's lemma,A snappy proof of Fatou's lemma,,"I'm studying basic real analysis and stumbled across three big results (Fatou's lemma, Lebesgue's Dominated Convergence theorem, and the Monotone Convergence theorem) in the theory of Lebesgue integration. I've seen short and slick proofs of the LDCT and MCT from scratch (in Bogachev's Measure Theory and Bass' Graduate Real Analysis/Rudin's Real and Complex Analysis, respectively). However, I was wondering if such a proof exists for Fatou's lemma. I've seen a couple of proofs that rely on neither the MCT nor the LDCT (in particular, in Royden and Fitzpatrick's Real Analysis, and on the Wikipedia page for Fatou's lemma), and while these proofs aren't too tricky or difficult to understand, they seem considerably longer than the proofs of the other two theorems. So to summarize: Can someone supply me with a short and slick proof, perhaps even an outline of a proof for me to work through, of Fatou's lemma that does not rely on LDCT or MCT? Thank you!","I'm studying basic real analysis and stumbled across three big results (Fatou's lemma, Lebesgue's Dominated Convergence theorem, and the Monotone Convergence theorem) in the theory of Lebesgue integration. I've seen short and slick proofs of the LDCT and MCT from scratch (in Bogachev's Measure Theory and Bass' Graduate Real Analysis/Rudin's Real and Complex Analysis, respectively). However, I was wondering if such a proof exists for Fatou's lemma. I've seen a couple of proofs that rely on neither the MCT nor the LDCT (in particular, in Royden and Fitzpatrick's Real Analysis, and on the Wikipedia page for Fatou's lemma), and while these proofs aren't too tricky or difficult to understand, they seem considerably longer than the proofs of the other two theorems. So to summarize: Can someone supply me with a short and slick proof, perhaps even an outline of a proof for me to work through, of Fatou's lemma that does not rely on LDCT or MCT? Thank you!",,"['real-analysis', 'measure-theory', 'reference-request', 'lebesgue-integral', 'alternative-proof']"
88,"Function $\mathbb{R}\to\mathbb{R}$ that is continuous and bounded, but not uniformly continuous [duplicate]","Function  that is continuous and bounded, but not uniformly continuous [duplicate]",\mathbb{R}\to\mathbb{R},"This question already has answers here : Prove that the function$\ f(x)=\sin(x^2)$ is not uniformly continuous on the domain $\mathbb{R}$. (3 answers) Closed 9 years ago . I found an example of a function $f: \mathbb{R}\to\mathbb{R}$ that is continuous and bounded, but is not uniformly continuous. It is $\sin(x^2)$. I think it's not uniformly continuous because the derivative is bigger and bigger as $x$ increases. But I don't know how to prove this is uniformly continuous. Is $\sin(x^2)$ uniformly continuous then? if it isn't, can you guys think of any other examples? thanks","This question already has answers here : Prove that the function$\ f(x)=\sin(x^2)$ is not uniformly continuous on the domain $\mathbb{R}$. (3 answers) Closed 9 years ago . I found an example of a function $f: \mathbb{R}\to\mathbb{R}$ that is continuous and bounded, but is not uniformly continuous. It is $\sin(x^2)$. I think it's not uniformly continuous because the derivative is bigger and bigger as $x$ increases. But I don't know how to prove this is uniformly continuous. Is $\sin(x^2)$ uniformly continuous then? if it isn't, can you guys think of any other examples? thanks",,['real-analysis']
89,"$C([0, 1])$ is not complete with respect to the norm $\lVert f\rVert _1 = \int_0^1 \lvert f (x) \rvert \,dx$",is not complete with respect to the norm,"C([0, 1]) \lVert f\rVert _1 = \int_0^1 \lvert f (x) \rvert \,dx","Consider $C([0, 1])$, the linear space of continuous complex-valued functions on the interval $[0, 1]$, with the norm $$\displaystyle\lVert f\rVert_1 = \int_0^1 \lvert f(x)\rvert \,dx.$$ I have to show that $C([0, 1])$ is not complete with respect to this norm. I have found the following example from a book. Let $f_n \in C[0,1]$ be given by $$f_n(x) := \begin{cases}  0   & \text{if $0 \le x \le \frac1{2}$}\\  n(x-\frac{1}{2}) & \text{if $\frac {1}{2} < x \le \frac {1}{2} + \frac {1}{n}$}\\  1              & \text{if $ \frac {1}{2} + \frac {1}{n} <x \leq 1 $} \end{cases}$$ How to prove that $f_n$  is a Cauchy sequence with respect to $\lVert \cdot\rVert_1$? If I use basic definition then I have to prove that $\lVert f_n - f_m\rVert_1 < \epsilon$ $\forall n, m > N$. But I am finding it difficult to prove this. Please help me to understand how to prove that $f_n$ is Cauchy sequence in $C([0, 1])$. Thanks","Consider $C([0, 1])$, the linear space of continuous complex-valued functions on the interval $[0, 1]$, with the norm $$\displaystyle\lVert f\rVert_1 = \int_0^1 \lvert f(x)\rvert \,dx.$$ I have to show that $C([0, 1])$ is not complete with respect to this norm. I have found the following example from a book. Let $f_n \in C[0,1]$ be given by $$f_n(x) := \begin{cases}  0   & \text{if $0 \le x \le \frac1{2}$}\\  n(x-\frac{1}{2}) & \text{if $\frac {1}{2} < x \le \frac {1}{2} + \frac {1}{n}$}\\  1              & \text{if $ \frac {1}{2} + \frac {1}{n} <x \leq 1 $} \end{cases}$$ How to prove that $f_n$  is a Cauchy sequence with respect to $\lVert \cdot\rVert_1$? If I use basic definition then I have to prove that $\lVert f_n - f_m\rVert_1 < \epsilon$ $\forall n, m > N$. But I am finding it difficult to prove this. Please help me to understand how to prove that $f_n$ is Cauchy sequence in $C([0, 1])$. Thanks",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
90,Inequality in a bounded real sequence,Inequality in a bounded real sequence,,"Prove or disprove that for any bounded real sequence $\{x_n\}_{n\in\mathbb{N}}$ there exist two distinct natural numbers $u,v$ such that: $$|x_u-x_v|\cdot|u-v|\leq 1.$$","Prove or disprove that for any bounded real sequence $\{x_n\}_{n\in\mathbb{N}}$ there exist two distinct natural numbers $u,v$ such that: $$|x_u-x_v|\cdot|u-v|\leq 1.$$",,"['real-analysis', 'sequences-and-series']"
91,Evaluating by real methods $\int_0^{\pi/2} \frac{x^5}{2-\cos^2(x)}\ dx$,Evaluating by real methods,\int_0^{\pi/2} \frac{x^5}{2-\cos^2(x)}\ dx,"$\def\Li{{\rm{Li}}}$I'm sure you guys can briefly get the result by some methods of complex analysis, but now I'm only interested in real analysis methods of proving the result. What would you propose for that? \begin{align*} \int_0^{\pi/2} \frac{x^5}{2-\cos^2(x)}\ dx=&\,\frac{\pi^6 \sqrt{2}}{768}+\frac{5 \sqrt{2}\pi^4}{64}\Li_2\left(2\sqrt2-3\right)-\frac{15\sqrt{2}\pi^2}{16}\Li_4\left(2\sqrt2-3\right)\\ &+\,\frac{15\sqrt{2}}{8}\bigg[\Li_6\left(2\sqrt2-3\right)-\Li_6\left(3-2\sqrt2\right)\bigg] \end{align*} And a supplementary question for another version, that is $$\int_0^{\pi/2} \frac{x^5}{1+\cos^2(x)}\ dx$$ again, by real analysis methods only.","$\def\Li{{\rm{Li}}}$I'm sure you guys can briefly get the result by some methods of complex analysis, but now I'm only interested in real analysis methods of proving the result. What would you propose for that? \begin{align*} \int_0^{\pi/2} \frac{x^5}{2-\cos^2(x)}\ dx=&\,\frac{\pi^6 \sqrt{2}}{768}+\frac{5 \sqrt{2}\pi^4}{64}\Li_2\left(2\sqrt2-3\right)-\frac{15\sqrt{2}\pi^2}{16}\Li_4\left(2\sqrt2-3\right)\\ &+\,\frac{15\sqrt{2}}{8}\bigg[\Li_6\left(2\sqrt2-3\right)-\Li_6\left(3-2\sqrt2\right)\bigg] \end{align*} And a supplementary question for another version, that is $$\int_0^{\pi/2} \frac{x^5}{1+\cos^2(x)}\ dx$$ again, by real analysis methods only.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
92,Strictly increasing function from reals to reals which is never an algebraic number,Strictly increasing function from reals to reals which is never an algebraic number,,"Let $f:\Bbb R\rightarrow\Bbb R$ have the properties $\forall x,y\in\Bbb R,\space x<y\implies f(x)<f(y)$ and $\forall x\in\Bbb R,\space f(x)\notin\Bbb A$ where $\Bbb A$ is the set of algebraic numbers; i.e. $f$ is strictly increasing, but nowhere is $f(x)$ algebraic. Does such a function exist? And if so, can one be explicitly constructed? My thoughts are that such a function should exist, since the algebraic numbers are ""small"" compared to the reals; we can show that a bijection (or more weakly an injection) must exist from $\Bbb R$ to $\Bbb R\backslash\Bbb A$ because they have the same cardinality, but I'm not entirely sure how to show rigorously that a strictly increasing function exists, even if in principle this is just a special type of injection. Replacing $\Bbb A$ by a set such as $\Bbb Z$ in the definition makes the question trivial, and these sets have the same cardinality, so clearly the difficulty arises because $\Bbb A$ is dense in the reals - any hints or answers would be appreciated.","Let have the properties and where is the set of algebraic numbers; i.e. is strictly increasing, but nowhere is algebraic. Does such a function exist? And if so, can one be explicitly constructed? My thoughts are that such a function should exist, since the algebraic numbers are ""small"" compared to the reals; we can show that a bijection (or more weakly an injection) must exist from to because they have the same cardinality, but I'm not entirely sure how to show rigorously that a strictly increasing function exists, even if in principle this is just a special type of injection. Replacing by a set such as in the definition makes the question trivial, and these sets have the same cardinality, so clearly the difficulty arises because is dense in the reals - any hints or answers would be appreciated.","f:\Bbb R\rightarrow\Bbb R \forall x,y\in\Bbb R,\space x<y\implies f(x)<f(y) \forall x\in\Bbb R,\space f(x)\notin\Bbb A \Bbb A f f(x) \Bbb R \Bbb R\backslash\Bbb A \Bbb A \Bbb Z \Bbb A","['real-analysis', 'order-theory', 'algebraic-numbers']"
93,"If $d$ is a metric, is $d(x,y)/(1+d(x,0)+d(y,0))$ a metric?","If  is a metric, is  a metric?","d d(x,y)/(1+d(x,0)+d(y,0))","I now that one can show that if $d$ is a metric on a vectorspace $X$ then so is $$\varrho(x,y):=\frac{d(x,y)}{1+d(x,y)}.$$ This easily follows from the fact that the function $s \mapsto \frac{s}{1+s}$ is increasing on $\mathbb{R}$. But does the same hold true for $$\nu(x,y):=\frac{d(x,y)}{1+d(x,0)+d(y,0)},$$ i.e. is this a metric, in the case $X=\mathbb{R}$ and $d(x,y)=|x-y|$? Of course one could use the reasoning above to see that $$\frac{d(x,y)}{1+d(x,0)+d(y,0)} \leq \frac{d(x,y)}{1+d(x,y)} \leq \frac{d(x,z)+d(z,y)}{1+d(x,z)+d(z,y)} = \frac{d(x,z)}{1+d(x,z)+d(z,y)}+\frac{d(z,y)}{1+d(x,z)+d(z,y)}$$ but that leads nowhere since the quantity $d(x,z)+d(z,y)$ tells us nothing about the quantity $d(x,0)+d(z,0)$. Can anybody give me a hint on how to do that? Thank you very much in advance!","I now that one can show that if $d$ is a metric on a vectorspace $X$ then so is $$\varrho(x,y):=\frac{d(x,y)}{1+d(x,y)}.$$ This easily follows from the fact that the function $s \mapsto \frac{s}{1+s}$ is increasing on $\mathbb{R}$. But does the same hold true for $$\nu(x,y):=\frac{d(x,y)}{1+d(x,0)+d(y,0)},$$ i.e. is this a metric, in the case $X=\mathbb{R}$ and $d(x,y)=|x-y|$? Of course one could use the reasoning above to see that $$\frac{d(x,y)}{1+d(x,0)+d(y,0)} \leq \frac{d(x,y)}{1+d(x,y)} \leq \frac{d(x,z)+d(z,y)}{1+d(x,z)+d(z,y)} = \frac{d(x,z)}{1+d(x,z)+d(z,y)}+\frac{d(z,y)}{1+d(x,z)+d(z,y)}$$ but that leads nowhere since the quantity $d(x,z)+d(z,y)$ tells us nothing about the quantity $d(x,0)+d(z,0)$. Can anybody give me a hint on how to do that? Thank you very much in advance!",,"['real-analysis', 'metric-spaces', 'euclidean-geometry']"
94,Explanation of the Bounded Convergence Theorem,Explanation of the Bounded Convergence Theorem,,"Theorem (Bounded Convergence Theorem) Let $\{f_n\}$ be a sequence of measurable functions on a set of finite measure $E$.  Suppose $\{f_n\}$ is uniformly pointwise bounded on $E$, that is , there is a number $M\geq 0$ for which $|f_n| \leq M$ for all $n$.  If $\{f_n\} \to f$ pointwise on $E$, then $\lim\limits_{n \to \infty} \int_E f_n = \int_E f.$ Why is it important in this theorem for it to be uniformly pointwise bounded as opposed to just pointwise bounded?  Is this because if it is not uniformly bounded than it is certainly not uniformly convergent.","Theorem (Bounded Convergence Theorem) Let $\{f_n\}$ be a sequence of measurable functions on a set of finite measure $E$.  Suppose $\{f_n\}$ is uniformly pointwise bounded on $E$, that is , there is a number $M\geq 0$ for which $|f_n| \leq M$ for all $n$.  If $\{f_n\} \to f$ pointwise on $E$, then $\lim\limits_{n \to \infty} \int_E f_n = \int_E f.$ Why is it important in this theorem for it to be uniformly pointwise bounded as opposed to just pointwise bounded?  Is this because if it is not uniformly bounded than it is certainly not uniformly convergent.",,['real-analysis']
95,"Are there two functions $f, g$ such that $f(g(x)) = x^3$ and $g(f(x)) = x^5$?",Are there two functions  such that  and ?,"f, g f(g(x)) = x^3 g(f(x)) = x^5","Question. Are there two functions $f, g: \mathbb{R}\rightarrow\mathbb{R}$ that satisfy $f(g(x)) = x^3 \enspace\forall x\in\mathbb{R}$ and $g(f(x)) = x^5\enspace\forall x\in\mathbb{R}$ ? This is an extension to this question , where I proved that there are no two functions such that $f(g(x)) = x^{2018}$ and $g(f(x))=x^{2019}$ (my proof can easily be extended to any two powers where one power is odd and the other power is even, instead of just $2018$ and $2019$ .) Remark $1$ . If there are two such functions, then they satisfy the following properties: $f, g$ are bijective; $f(x^5) = f(x)^3\enspace\forall x\in\mathbb{R}$ ; $g(x^3) = g(x)^5\enspace\forall x\in\mathbb{R}$ ; $f(i), g(i)\in\{-1, 0, 1\}\enspace\forall i\in\{-1, 0, 1\}$ ; $x^9 = f(g(x))^3 = f(g(x^3)) \enspace\forall x\in\mathbb{R}$ ; $g^{-1}(x)=\sqrt[3]{f(x)}, f^{-1}(x)=\sqrt[5]{g(x)}\enspace\forall x\in\mathbb{R}$ . Remark $2$ . A similar question would be if there are two functions such that $f(g(x)) = x^2 \enspace\forall x\in\mathbb{R}$ and $g(f(x)) = x^4 \enspace\forall x\in\mathbb{R}$ , or more generally: For what $i, j\in\mathbb{N}$ are there functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$ such that $f(g(x)) = x^i, g(f(x)) = x^j\enspace\forall x\in\mathbb{R}$ ?","Question. Are there two functions that satisfy and ? This is an extension to this question , where I proved that there are no two functions such that and (my proof can easily be extended to any two powers where one power is odd and the other power is even, instead of just and .) Remark . If there are two such functions, then they satisfy the following properties: are bijective; ; ; ; ; . Remark . A similar question would be if there are two functions such that and , or more generally: For what are there functions such that ?","f, g: \mathbb{R}\rightarrow\mathbb{R} f(g(x)) = x^3 \enspace\forall x\in\mathbb{R} g(f(x)) = x^5\enspace\forall x\in\mathbb{R} f(g(x)) = x^{2018} g(f(x))=x^{2019} 2018 2019 1 f, g f(x^5) = f(x)^3\enspace\forall x\in\mathbb{R} g(x^3) = g(x)^5\enspace\forall x\in\mathbb{R} f(i), g(i)\in\{-1, 0, 1\}\enspace\forall i\in\{-1, 0, 1\} x^9 = f(g(x))^3 = f(g(x^3)) \enspace\forall x\in\mathbb{R} g^{-1}(x)=\sqrt[3]{f(x)}, f^{-1}(x)=\sqrt[5]{g(x)}\enspace\forall x\in\mathbb{R} 2 f(g(x)) = x^2 \enspace\forall x\in\mathbb{R} g(f(x)) = x^4 \enspace\forall x\in\mathbb{R} i, j\in\mathbb{N} f,g:\mathbb{R}\rightarrow\mathbb{R} f(g(x)) = x^i, g(f(x)) = x^j\enspace\forall x\in\mathbb{R}","['real-analysis', 'function-and-relation-composition']"
96,"$b_n$ bounded, $\sum a_n$ converges absolutely, then $\sum a_nb_n$ also","bounded,  converges absolutely, then  also",b_n \sum a_n \sum a_nb_n,"a) Prove that if $\sum a_n$ converges absolutely and $b_n$ is a bounded sequence, then also $\sum a_nb_n$ converges absolutely. I wanted to use the comparison test to show it's true, but I think I got something mixed up. Here's what I have so far. $b_n$ is bounded $\Rightarrow \exists c > 0: |b_k| < c, \forall k \in \mathbb N$ $\sum a_nb_n < \sum a_nc < c\sum a_n$. I'm very tempted to use the comparison test here and say, as $\sum a_n$ converges absolutetly, then so does $c\sum a_n$, but for that I needed the inverted relation, right? I would need $\sum a_n > c\sum a_n$, which is not true. However isn't it obvious that something absolutely convergent multiplied by a constant is also absolutetly convergent? Is there a mathematical way to write this? Thanks a lot in advance. b) Refute with a counter example: if $\sum a_n$ converges and $b_n$ is a bounded sequence, then also $\sum a_nb_n$ converges. Is this even possible? I mean, it has to be, but it doesn't make sense to me. If something converges, it means it's bounded, right? And I thought, well, since bounded + bounded = bounded, then bounded*bounded would also get me something bounded again. Anyway, my idea would be to use for $a_n$ an alternating series that ist only convergent, for example $(-1)^n \frac 1 {n^2}$. But something tells me I'm trying to prove $a_n b_n$ is not absolutely convergent. Thanks a lot in advance guys!","a) Prove that if $\sum a_n$ converges absolutely and $b_n$ is a bounded sequence, then also $\sum a_nb_n$ converges absolutely. I wanted to use the comparison test to show it's true, but I think I got something mixed up. Here's what I have so far. $b_n$ is bounded $\Rightarrow \exists c > 0: |b_k| < c, \forall k \in \mathbb N$ $\sum a_nb_n < \sum a_nc < c\sum a_n$. I'm very tempted to use the comparison test here and say, as $\sum a_n$ converges absolutetly, then so does $c\sum a_n$, but for that I needed the inverted relation, right? I would need $\sum a_n > c\sum a_n$, which is not true. However isn't it obvious that something absolutely convergent multiplied by a constant is also absolutetly convergent? Is there a mathematical way to write this? Thanks a lot in advance. b) Refute with a counter example: if $\sum a_n$ converges and $b_n$ is a bounded sequence, then also $\sum a_nb_n$ converges. Is this even possible? I mean, it has to be, but it doesn't make sense to me. If something converges, it means it's bounded, right? And I thought, well, since bounded + bounded = bounded, then bounded*bounded would also get me something bounded again. Anyway, my idea would be to use for $a_n$ an alternating series that ist only convergent, for example $(-1)^n \frac 1 {n^2}$. But something tells me I'm trying to prove $a_n b_n$ is not absolutely convergent. Thanks a lot in advance guys!",,"['real-analysis', 'sequences-and-series', 'absolute-convergence']"
97,"If $f(x+1/n)$ converges to a continuous function $f(x)$ uniformly on $\mathbb{R}$, is $f(x)$ necessarily uniformly continuous?","If  converges to a continuous function  uniformly on , is  necessarily uniformly continuous?",f(x+1/n) f(x) \mathbb{R} f(x),"Suppose $f:\mathbb{R}\mapsto\mathbb{R}$ is a continuous function. Define $f_n(x)=f(x+1/n)$ for all $n\in\mathbb{N}^+$ . If $f_n(x)\to f(x)$ uniformly on $\mathbb{R}$ , can we conclude that $f$ is actually uniformly continuous? If not, can you give a counterexample of $f$ to be not uniformly continuous but satisfies all above conditions? I know that if $f_n(x)$ 's are all uniformly continuous and $f_n(x)\to f(x)$ uniformly, then $f(x)$ must be uniformly continuous. However, here we do not assume $f(x)$ to be uniformly continuous, and $f_n(x)$ has specific structure related to $f(x)$ , so at least you cannot directly conclude they are uniformly continuous. I guess the statement above is wrong but it's hard for me to find a counter-example. Can anyone help me?","Suppose is a continuous function. Define for all . If uniformly on , can we conclude that is actually uniformly continuous? If not, can you give a counterexample of to be not uniformly continuous but satisfies all above conditions? I know that if 's are all uniformly continuous and uniformly, then must be uniformly continuous. However, here we do not assume to be uniformly continuous, and has specific structure related to , so at least you cannot directly conclude they are uniformly continuous. I guess the statement above is wrong but it's hard for me to find a counter-example. Can anyone help me?",f:\mathbb{R}\mapsto\mathbb{R} f_n(x)=f(x+1/n) n\in\mathbb{N}^+ f_n(x)\to f(x) \mathbb{R} f f f_n(x) f_n(x)\to f(x) f(x) f(x) f_n(x) f(x),"['real-analysis', 'uniform-convergence', 'uniform-continuity']"
98,"A function $f:\mathbb{R}^2\to\mathbb{R}^2$ that is open and closed, but not continuous.","A function  that is open and closed, but not continuous.",f:\mathbb{R}^2\to\mathbb{R}^2,"Does there exist a function $f:\mathbb{R}^2\to\mathbb{R}^2$ that is open and closed, but not continuous? Note that I require $f$ to be defined on the entirety of $\mathbb{R}^2$ . There are a few examples of open functions that are not continuous. Most examples I found were functions $f$ with the property that $f(U)=\mathbb{R}^2$ for all open $U\subseteq\mathbb{R}^2$ . The extra requirement that $f$ needs to be closed makes things a lot harder, though. I believe the answer is no, such a function does not exist. My main motivation is the following observation. Proposition: Let $f:\mathbb{R}^2\to\mathbb{R}^2$ be open and closed. If $x_n\to x$ and $f(x_n)$ is bounded, then $f(x_n)\to f(x)$ . Proof of 1: Let $f(x_{n_i})$ be a convergent subsequence of $f(x_n)$ with limit $p$ . For all $i$ let $y_i=x_{n_i}$ if $f(x_{n_i})\neq p$ . Otherwise, let $r=d(x_{n_i},x)$ . Then $B_r(x_{n_i})$ is open, so $f(B_r(x_{n_i}))$ is open as well. So $f(B_r(x_{n_i}))\cap B_r(p)\setminus\{p\}$ is not empty. So we can choose $y_i\in B_r(x_{n_i})$ such that $f(y_i)\in B_r(p)\setminus\{p\}$ . We find $y_i\to x$ and $f(y_i)\to p$ and $f(y_i)\neq p$ for all $i$ . Let $S=\{y_i:i\in\mathbb{N}\}$ . We find that $p$ is a limit point of $f(S)$ not contained in $f(S)$ . However, $C=S\cup\{x\}$ is closed, so $f(C)$ is also closed, and hence contains $p$ . We conclude $f(x)=p$ , so $f(x_{n_i})\to f(x)$ . Assume for the contrary that $f(x_n)\not\to f(x)$ . Then there is a subsequence of $f(x_n)$ that always stays a certain distance from $f(x)$ . By the Bolzano Weierstrass theorem, this subsequence itself has a convergent subsequence. By the previous observation, this subsequence converges to $f(x)$ . This contradicts the fact that it always stays a certain distance from $f(x)$ . $\square$ If you manage to prove that such a function does not exist, it might be neat to also look at how general the domain and codomain of $f$ can be made. For example, all arguments in the proposition still work for $f:X\to Y$ with any metric space $X$ , and any finite dimensional vector space $Y$ . However, with tweaking the arguments only a bit you find the following. Let $X$ be a first countable Hausdorff topological space, and let $Y$ be a first countable topological space with no isolated points. Let $f:X\to Y$ be open and closed. If $x_n\to x$ and $f(x_n)$ is contained in some sequentially compact set, then $f(x_n)\to f(x)$ . Anyways, please let me know your thoughts.","Does there exist a function that is open and closed, but not continuous? Note that I require to be defined on the entirety of . There are a few examples of open functions that are not continuous. Most examples I found were functions with the property that for all open . The extra requirement that needs to be closed makes things a lot harder, though. I believe the answer is no, such a function does not exist. My main motivation is the following observation. Proposition: Let be open and closed. If and is bounded, then . Proof of 1: Let be a convergent subsequence of with limit . For all let if . Otherwise, let . Then is open, so is open as well. So is not empty. So we can choose such that . We find and and for all . Let . We find that is a limit point of not contained in . However, is closed, so is also closed, and hence contains . We conclude , so . Assume for the contrary that . Then there is a subsequence of that always stays a certain distance from . By the Bolzano Weierstrass theorem, this subsequence itself has a convergent subsequence. By the previous observation, this subsequence converges to . This contradicts the fact that it always stays a certain distance from . If you manage to prove that such a function does not exist, it might be neat to also look at how general the domain and codomain of can be made. For example, all arguments in the proposition still work for with any metric space , and any finite dimensional vector space . However, with tweaking the arguments only a bit you find the following. Let be a first countable Hausdorff topological space, and let be a first countable topological space with no isolated points. Let be open and closed. If and is contained in some sequentially compact set, then . Anyways, please let me know your thoughts.","f:\mathbb{R}^2\to\mathbb{R}^2 f \mathbb{R}^2 f f(U)=\mathbb{R}^2 U\subseteq\mathbb{R}^2 f f:\mathbb{R}^2\to\mathbb{R}^2 x_n\to x f(x_n) f(x_n)\to f(x) f(x_{n_i}) f(x_n) p i y_i=x_{n_i} f(x_{n_i})\neq p r=d(x_{n_i},x) B_r(x_{n_i}) f(B_r(x_{n_i})) f(B_r(x_{n_i}))\cap B_r(p)\setminus\{p\} y_i\in B_r(x_{n_i}) f(y_i)\in B_r(p)\setminus\{p\} y_i\to x f(y_i)\to p f(y_i)\neq p i S=\{y_i:i\in\mathbb{N}\} p f(S) f(S) C=S\cup\{x\} f(C) p f(x)=p f(x_{n_i})\to f(x) f(x_n)\not\to f(x) f(x_n) f(x) f(x) f(x) \square f f:X\to Y X Y X Y f:X\to Y x_n\to x f(x_n) f(x_n)\to f(x)","['real-analysis', 'general-topology', 'continuity']"
99,A natural discovery of $e$ as $\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$ - what is unrigorous here?,A natural discovery of  as  - what is unrigorous here?,e \lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n,"I had an interesting conversation with my teacher today, and we were discussing what the best foot forward for me might be. He asked me to differentiate $e^x$ from first principles, which I did, and then challenged me on: Begin by defining $e$ as: $$\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ He wondered if I found this satisfactory, intuitively speaking. I said: ""this is just one of the fundamental and historic definitions of $e$ "", and he asked me - ""why?"" And I just couldn't answer beyond repeating: ""it's the definition"". On that note he then showed a more natural exploration that leads to a discovery of this definition of $e$ . He did declare this exploration to be ""cheating"", which is the focus of my question. Take $a$ to be a positive real number. Then: $$\begin{align}\lim_{h\to0}\frac{a^{x+h}-a^h}{h}=a^x\lim_{h\to0}\frac{a^h-1}{h}\end{align}$$ And by exploration with graphs, one intuitively sees that the latter term, the derivative of $a^x$ at $0$ , clearly should exist, and that there should exist some base $a$ for which this derivative is $1$ . Exploring this possibility, define: $$(h_n)_{n\in\Bbb N}=\frac{1}{n}$$ And for each $h_n$ , ask which $a$ satisfies: $$\frac{a^{h_n}-1}{h_n}=1$$ And naturally define a sequence $(a_n)$ that solves this equation: $$(a_n)_{n\in\Bbb N}=(1+h_n)^{1/h_n}$$ This leads to the idea that the mystery $a$ for which the derivative of $a^x$ at zero equals $1$ can be found by considering the limits of the sequence $a_n$ , as $h_n$ goes to $0$ . This finds: $$\lim_{n\to\infty}a_n=\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ And we call this number $e$ . The exploration leads one to suspect that $e$ satisfies $\frac{d}{dx}e^x=1$ . However, he did mention that, in the interests of me studying real analysis, that this approach is ""cheating"" a little somehow, and that it is not a fully rigorous derivation of that limit. He hinted that I would see why this is shortly after properly studying real analysis. I have seen a good deal of calculus and analysis around the Internet, in a very erratic way, so I have only some sense of why this approach is unrigorous. However, there are many many people on this forum who have properly studied real analysis, and I am asking for your help in explaining either what's wrong, or how to make it right. My thoughts: We have not shown that $a^x$ is sequentially convergent, nor have we shown that $(a_n)$ converges. Showing that there exists $(a_n),(h_n)$ such that: $$\lim_{n\to\infty}h_n=0,\,\frac{a_n^{h_n}-1}{h_n}=1$$ Is not the same as showing that: $$\lim_{h\to0}\frac{(\lim_{n\to\infty}(a_n))^h-1}{h}=1$$ Any insight into $(1),(2)$ or anything else here would be greatly appreciated - especially any insight into how to make it fully rigorous! I just intuitively feel that $(2)$ is correct, but I cannot know for sure.","I had an interesting conversation with my teacher today, and we were discussing what the best foot forward for me might be. He asked me to differentiate from first principles, which I did, and then challenged me on: Begin by defining as: He wondered if I found this satisfactory, intuitively speaking. I said: ""this is just one of the fundamental and historic definitions of "", and he asked me - ""why?"" And I just couldn't answer beyond repeating: ""it's the definition"". On that note he then showed a more natural exploration that leads to a discovery of this definition of . He did declare this exploration to be ""cheating"", which is the focus of my question. Take to be a positive real number. Then: And by exploration with graphs, one intuitively sees that the latter term, the derivative of at , clearly should exist, and that there should exist some base for which this derivative is . Exploring this possibility, define: And for each , ask which satisfies: And naturally define a sequence that solves this equation: This leads to the idea that the mystery for which the derivative of at zero equals can be found by considering the limits of the sequence , as goes to . This finds: And we call this number . The exploration leads one to suspect that satisfies . However, he did mention that, in the interests of me studying real analysis, that this approach is ""cheating"" a little somehow, and that it is not a fully rigorous derivation of that limit. He hinted that I would see why this is shortly after properly studying real analysis. I have seen a good deal of calculus and analysis around the Internet, in a very erratic way, so I have only some sense of why this approach is unrigorous. However, there are many many people on this forum who have properly studied real analysis, and I am asking for your help in explaining either what's wrong, or how to make it right. My thoughts: We have not shown that is sequentially convergent, nor have we shown that converges. Showing that there exists such that: Is not the same as showing that: Any insight into or anything else here would be greatly appreciated - especially any insight into how to make it fully rigorous! I just intuitively feel that is correct, but I cannot know for sure.","e^x e \lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n e e a \begin{align}\lim_{h\to0}\frac{a^{x+h}-a^h}{h}=a^x\lim_{h\to0}\frac{a^h-1}{h}\end{align} a^x 0 a 1 (h_n)_{n\in\Bbb N}=\frac{1}{n} h_n a \frac{a^{h_n}-1}{h_n}=1 (a_n) (a_n)_{n\in\Bbb N}=(1+h_n)^{1/h_n} a a^x 1 a_n h_n 0 \lim_{n\to\infty}a_n=\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n e e \frac{d}{dx}e^x=1 a^x (a_n) (a_n),(h_n) \lim_{n\to\infty}h_n=0,\,\frac{a_n^{h_n}-1}{h_n}=1 \lim_{h\to0}\frac{(\lim_{n\to\infty}(a_n))^h-1}{h}=1 (1),(2) (2)","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
